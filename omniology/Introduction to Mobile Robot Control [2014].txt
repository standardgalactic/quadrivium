Introduction to Mobile Robot Control

Introduction to Mobile
Robot Control
Spyros G. Tzafestas
School of Electrical and Computer Engineering
National Technical University of Athens
Athens, Greece
AMSTERDAM
G BOSTON
G HEIDELBERG
G LONDON NEW YORK
G OXFORD
PARIS
G SAN DIEGO
G SAN FRANCISCO
G SINGAPORE
G SYDNEY
G TOKYO

Elsevier
32 Jamestown Road, London NW1 7BY
225 Wyman Street, Waltham, MA 02451, USA
First edition 2014
Copyright © 2014 Elsevier Inc. All rights reserved.
No part of this publication may be reproduced or transmitted in any form or by any means,
electronic or mechanical, including photocopying, recording, or any information storage
and retrieval system, without permission in writing from the publisher. Details on how to
seek permission, further information about the Publisher’s permissions policies and our
arrangement with organizations such as the Copyright Clearance Center and the Copyright
Licensing Agency, can be found at our website: www.elsevier.com/permissions.
This book and the individual contributions contained in it are protected under copyright by
the Publisher (other than as may be noted herein).
Notices
Knowledge and best practice in this field are constantly changing. As new research and
experience broaden our understanding, changes in research methods, professional practices,
or medical treatment may become necessary.
Practitioners and researchers must always rely on their own experience and knowledge in
evaluating and using any information, methods, compounds, or experiments described herein.
In using such information or methods they should be mindful of their own safety and the
safety of others, including parties for whom they have a professional responsibility. To the
fullest extent of the law, neither the Publisher nor the authors, contributors,or editors,
assume any liability for any injury and/or damage to persons or property as a matter of
products liability, negligence or otherwise, or from any use or operation of any methods,
products, instructions, or ideas contained in the material herein.
British Library Cataloguing-in-Publication Data
A catalogue record for this book is available from the British Library
Library of Congress Cataloging-in-Publication Data
A catalog record for this book is available from the Library of Congress
ISBN: 978-0-12-417049-0
For information on all Elsevier publications
visit our website at store.elsevier.com
This book has been manufactured using Print On Demand technology. Each copy is
produced to order and is limited to black ink. The online version of this book will show
color figures where appropriate.

Dedication
To the Robotics Teacher and Learner
For the things we have to learn before we can do them, we learn by doing them.
Aristotle
The second most important job in the world, second only to being a parent, is being
a good teacher.
S.G. Ellis
In learning you will teach, and in teaching you will learn.
Phil Collins

Preface
Robotics has been a dominant contributor to the development of the human society
over the years. It is a field that needs the synergy of a variety of scientific areas
such as mechanical engineering, electrical-electronic engineering, control engineer-
ing, computer engineering, sensor engineering, and others. Robots and other auto-
mated machines have to live together with people. In this symbiosis, human needs
and preferences should be predominantly respected, incorporated, and implemen-
ted. To this end, modern robots, especially wheeled or legged mobile robots, incor-
porate and realize in a purposeful and profitable way the perceptionaction cycle
principle borrowed from biological systems and human cognitive and adaptation
capabilities.
The objective of this book is to present in a cohesive way a set of fundamental
conceptual and methodological elements, developed over the years for nonholo-
nomic and omnidirectional wheeled mobile robots. The core of the book (Chapters
5 through 10) is devoted to the analysis and design of several mobile robot control-
lers that include basic Lyapunov-based controllers, invariant manifoldbased con-
trollers, affine modelbased controllers, model reference adaptive controllers,
sliding-mode and Lyapunov-based robust controllers, neural controllers, fuzzy-
logic controllers, vision-based controllers, and mobile manipulator controllers. The
topics of mobile robot drives, kinematics, dynamics, and sensing are covered in the
first four chapters. The topics of path planning, motion planning, task planning,
localization, and mapping are covered in Chapters 11 and 12, including most fun-
damental concepts and techniques at a detail compatible with the purpose and size
of the book. Chapter 13 provides a selection of experimental results obtained by
many of the methods studied in this book. These results were drawn from the
research literature and include some of the author’s results. Chapter 14 provides a
conceptual overview of some generic systemic and software architectures devel-
oped for implementing integrated intelligent control of mobile robots. Finally,
Chapter 15 provides a tour to the applications of mobile robots in the factory and
society at an encyclopedic level.
For the convenience of the reader, the first section of each chapter involves the
required mathematical, mechanics, control, and fixed-robot background concepts
that are used in the chapter. The book is actually complementary to most books in
the field, in the sense that it provides a solid model-based analysis and design of a
large repertory of mobile robot control schemes, not covered in other books.
This book is suitable for senior undergraduate and graduate instructional courses
on general and mobile robotics. It can also be used as an introductory reference

book by researchers and practitioners in the field that need a consolidated methodo-
logical source for their work.
I am grateful to all publishers and authors for granting their permission to
include in the book the requested illustrations and experimental plots.
Spyros. G. Tzafestas
Athens, April 2013
xviii
Preface

List of acknowledged authors and
collaborators
The work on mobile robotics of the following researchers and collaborators, which
was used and compiled for producing the present book, is acknowledged.
Authors
Abdelkader H.H.
Edwards G.
Mu¨hlenfeld A.
Adams M.D.
Engedy I.
Muir P.F.
Adom A.
Espiau B.
Myers K.
Afshar A.
Ettelt E.
Narayankar I.
Agu¨ero C.
Euderde E.
Nayar S.
Aguilar L.T.
Everett H.R.
Nesnas A.
Ahmadabadi M.N.
Fetter Lages W.
Okamoto Jr J.
Aicardi M.
Fierro R.
Olunloyo V.O.S
Albus J.S.
Fischer C.
Oreba¨ck A.
Alsina P.J.
Fourquet J.Y.
Oriolo G.
Al-Zorkany M.
Fraser G.
Osmic N.
Amato N.
Fu L.C.
Ostrowski J.P.
Andreff N.
Fukuda T.
Padois V.
Ang Jr M.H.
Furtwa¨ngler R.
Papadopoulos E.
Araujo A.
Gallina P.
Pascoal A.
Araujo H.
Ganguly P.
Petrinic T.
Arkin R.C.
Gans N.R.
Petrovic I.
Arney T.
Garrido S.
Pimenta L.C.A.
Asheriad M.
Gasparetto A.
Poulakakis J.
Ashoorizad M.
Gerkey B.
Pourboghrat F.
Asmore M.
Geva S.
Puiu D.
Awad H.A.
Geyer C.
Qian J.
Ayari I.
Gholipour A.
Rachid A.
Ayomoh M.K.O.
Gilioli M.
Ramaswamy P.S.A.
Baek S.H.
Giordano P.R.
Rekleitis I.
Bailey T.
Gordon N.J.
Reyhanoglu M.
Baker S.
Grassi Jr V.
Rives P.
Balakrishnan S.N.
Grassi Jr Y.
Rizon M.
Balestrino A.
Grigorescu S.
Grosu V.
Roy N.
Barnes N.
Hager G.
Ruiz-Ayu`car J.
Barreto J.P.
Handeck U.D.
Ruspini E.
Bar-Shalom Y.
Hemerly E.M.
Sablantno¨g S.
Barzamini R.
Hong D.
Safadi H.

Bastin G.
Horvath G.
Saffioti A.
Bayar G.
Howard A.
Saich J.
Beattle B.
Huang C.I.
Salmond D.J.
Bench-Capon T.J.M.
Hutchinson S.
Samson C.
Bian H.J.
Ivanjko E.
Saridis G.N.
Bicchi A.
Izumi K.
Sarkar N.
Blanco D.
Jarvis R.A.
Sato K.
Boles W.W.
Jiang P.
Scheutz M.
Bonert M.
Kalmar-Nagy T.
Schmidt D.C.
Borenstein J.
Karlsson M.P.
Schmidt G.
Bornstedt B.
Kerry M.
Schneider S.
Bretl T.
Khatib O.
Scholl K.U.
Bright G.
Kim B.M
Selmic R.
Brooks R.A.
Kim J.H
Sethian J.A.
Brun Y.
Kim M.
Shave M.J.R.
Buss M.
Kimoto K.
Sheu P.G.Y.
Byrne J.C.
Koku A.B
Sidek N.
Campion G.
Konolige K.
Simmons R.
Campos J.
Konukseven E.I.
Smith A.F.M.
Can˜as J.M.
Koren Y.
Soetanto D.
Cardenas S.
Kraetzschmar G.
Soria C.M.
Carreli R.
Kragic D.
Southall B.
Spinu V.
Casalino G.
Kramer J.
Spletzer J.
Castillo O.
Kru¨ger D.
Steinbauer G.
Ceyer C.M.
Kumar V.
Su J.
Chang C.F.
Kunitake Y.
Suga Y.
Chang K.C.
Lacevic B.
Tadijne M.
Chatti A.
Laengle T.
Tang J.
Chaumette F.
Lages W.F.
Tayebi A.
Chen Y.
Lapierre L.
Taylor C.J.
Cherubini A.
Latombe J.C.
Taylor T.
Chirikjian G.S.
Lavalle S.M.
Thrun S.
Chiron P.
Lee S.
Tian Y.
Choi Y.H.
Lee T.K.
Tsakiris D.
Chong C.Y.
Leonard J.L.
Tsiotras P.
Christensen H.L.
Lewis F.L.
Ullman M.
Chung J.H.
Liang S.
Utz H.
Chung W.
Liang Z.
Velagic J.
Cocias T.
Lietmann T.
Vidal Calleja T.A.
Coenen F.P.
Limsoonthrakul S.
Wang C.
Corke P.
Lindstro¨m M.
Wang L.C.
Coste-Manie`re E.
Lozano-Perez T.
Wang Y.
Cote C.
Lueth T.C.
Wotawa A.
Cuerra P.N.
Ma J.
Xie W.
Cwa D.
Macesanu G.
Xue Q
D’andrea R.
Maeyama S.
Yacacob S.
D’andrea-Novel B.
Mamat M.
Yamamoto T.
Dailey M.N.
Martin F.
Yamamoto Y.
xx
List of acknowledged authors and collaborators

Daniilidis K.
Martinet P.
Yang F.
Das A.K
Matella´n V.
Yang J.M
Das T.
Medeiros A.A.D.
Yang M.
De Oliveira V.M.
Mehrandezh M.
Yaugham R.
De Pieri E.R.
Melchiori G.
Yong L.S.
Dehgam S.M.
Meystel A.M.
Youm Y.
DeLuca A.
Mezouar Y.
Yun X.
DeVilliers M.
Micaelli A.
Yuta S.
Zefran M.
DeVon D.
Milios E.
Zelinsky A.
Diaz B.M.
Moldoveanu F.
Zhang Q.
Ding F.G.
Doroftei I.
Montemerlo M.
Zhang Y.
Du J.
Morales B.
Zouzdani J.
Dudek G.
Moreno L.
Durrant-Whyte H.F.
Moret E.N.
Collaborators
Deliparaschos K.M.
Moustris G.
Tang J.
Fukuda T.
Rigatos G.
Sgouros N.M.
Tzafestas C.S.
Katevas N.
Shiraishi Y.
Tzafestas E.S.
Krikochoritis T.
Skoundrianos E.
Watanabe K.
Melfi A.
Stamou G.
Zavlangas P.
xxi
List of acknowledged authors and collaborators

Principal symbols and acronyms
t; k
Continuous, discrete time
l; s; D; d
Linear distance (length)
p;d;x
Position vector
R
Rotation matrix
Rx; Ry; Rz
Rotation matrix w.r.t. axis x; y; z
n;o;a
Normal, orientation, and approach unit vectors
A;Τ
Homogeneous ð4 3 4Þ matrix
q
Generalized variable (linear, angular)
v;υ
Linear velocity vector
ω; _θ
Angular velocity vector
JðqÞ
Jacobian matrix
J21ðqÞ
Inverse of JðqÞ
JyðqÞ
Generalized inverse (pseudoinverse) of JðqÞ
D  H
DenavitHartenberg
detðÞ
Determinant
F;τ;ðNÞ
Force, torque vector
L
Lagrangian function
K; P
Kinetic energy and potential energy
DðqÞ
Inertial matrix
gðqÞ
Gravity term
HRI
Humanrobot interface
GHRI
Graphical humanrobot interface
IC
Intelligent control
ICA
Intelligent control architecture
NL
Natural language
NL-HRI
Natural language HRI
UI
User interface
Cðq;_qÞ_q
Centrifugal/Coriolis term
Oxyz
Coordinate frame
WMR
Wheeled mobile robot
MM
Mobile manipulator
LS
Least squares
φ; ψ
WMR direction, steering angles
DOF
Degree of freedom
COG
Center of gravity
COM
Center of mass
MðqÞ
Nonholonomic constraint

CS
Configuration space
GPS
Global positioning system
lf
Lens focal length
Kp; Kυ
Position, velocity gain matrix
KF; EKF
Kalman filter, extended Kalman filter
SLAM
Simultaneous localization and mapping
^x; ^θ
Estimate of x;θ
~x; ~θ
Error of the estimate ^x; ^θ
Σx; Σθ
Covariance matrix of ~x; ~θ
AI
Artificial intelligence
RF
Radio frequency
CAD
Computer-aided design
xðsÞ
Laplace transform of xðtÞ
GðsÞ; gðsÞ
Transfer function
ωn
Natural angular frequency
ζ
Damping factor
P; PD
Proportional, proportional plus derivative
PI; PID
Proportional plus integral (plus derivative)
MRAC
Model reference adaptive control
SMC
Sliding mode control
Jim
Image Jacobian
VRSðVRCÞ
Visual robot servoing (control)
FLðFCÞ
Fuzzy logic (fuzzy control)
NN
Neural network
BP
Back propagation
RFN
Radial basis function neural network (RBF-NN)
MLP
Multi-layer perception
NF
Neurofuzzy
MB
Model-based
tg; tan
Trigonometric tangent of an angle
tg21; arctan
Inverse of tg; tan
xxiv
Principal symbols and acronyms

Quotations about robotics
Aristotle If every tool, when ordered, or even of its own accord, could do the work
that benefits it, just as the creations of Daedalus move by themselves. . ., then there
would be no need of apprentices for the master workers or of slaves for their lords.
Allen Newel From where I stand it is easy to see the science lurking in robotics. It
lies in the welding of intelligence to energy. That is, it lies in intelligent perception
and intelligent control of motion.
Rod Grupen At bottom, robotics is about us. It is the discipline of emulating our
lives, of wondering how we work.
Rob Spencer Got a dirty, dangerous, dull job? Let a robot do it and keep your
workers safe.
Marvin Minsky We wanted to solve robot problems and needed some vision,
action, reasoning, planning, and so on. . .. Eventually, robots will make everything.
David Hanson Making realistic robots is going to polarize the market. You will
have people who love it and some people who will really be disturbed.
John McCarthy Every aspect of learning or any other feature of intelligence can
in principle be so precisely described that a machine can be made to simulate it.
No robot has ever been designed that is ever aware of what is doing; but most of
the time, we aren’t either.
John McDermott To be useful, a system has to do more than just correctly per-
form some task.
Chuck Gosdzinski The top two awards don’t even go to robots.

1 Mobile Robots: General Concepts
1.1
Introduction
Mobile robots are robots that can move from one place to another autonomously,
that is, without assistance from external human operators. Unlike the majority of
industrial robots that can move only in a specific workspace, mobile robots have
the special feature of moving around freely within a predefined workspace to
achieve their desired goals. This mobility capability makes them suitable for a large
repertory of applications in structured and unstructured environments. Ground
mobile robots are distinguished in wheeled mobile robots (WMRs) and legged
mobile robots (LMRs) Mobile robots also include unmanned aerial vehicles
(UAVs), and autonomous underwater vehicles (AUVs). WMRs are very popular
because they are appropriate for typical applications with relatively low mechanical
complexity and energy consumption. Legged robots are suitable for tasks in non-
standard environments, stairs, heaps of rubble, etc. Typically, systems with two,
three, four, or six legs are of general interest but many other possibilities also exist.
Single-leg robots find rare applications because they can only move by hopping.
Mobile robots also include mobile manipulators (wheeled or legged robots
equipped with one or more light manipulators to perform various tasks).
The objective of this chapter is to present the fundamental general concepts of
WMRs. In particular, the chapter
G
provides a list of the main historical landmarks of general robotics and mobile robots;
G
discusses the locomotion issues of ground (wheeled, legged) mobile robots;
G
investigates the wheel and drive types of mobile robots (nonholonomic, omnidirectional);
and
G
introduces the concepts of mobile robot degree of mobility, degree of steerability, and
maneuverability.
1.2
Definition and History of Robots
1.2.1
What Is a Robot?
The term “robot” (robota) was used for the first time in 1921 by the Czech writer
Karel Capek and means slave servant or forced labor. In science and technology
there is not a global or unique definition of a robot. When Joseph Engelberger, the
father of modern robotics, was asked to define a robot he said: “I can’t define a
Introduction to Mobile Robot Control. DOI: http://dx.doi.org/10.1016/B978-0-12-417049-0.00001-8
© 2014 Elsevier Inc. All rights reserved.

robot but I know one when I see one.” The Robotics Institute of America (RIA)
defines an industrial robot as “a reprogrammable multi-functional manipulator
designed to move materials, parts, tools, or specialized devices through variable
programmed motions for the performance of a variety of tasks which also acquire
information from the environment and move intelligently in response.” This defini-
tion does not capture mobile robots.
The definition adopted in the European Standard EN775/1992 is as follows:
“Manipulating industrial robot is an automatically controlled reprogrammable
multi-purpose, manipulative machine with several degrees of freedom (DOF), which
may be either fixed in place or mobile for use in industrial automation applications.
Ronald Arkin says: “An intelligent robot is a machine able to extract information
from its environment and use knowledge about its work to move safely in a mean-
ingful and purposive manner.”
Rodney Brooks says: “To me a robot is something that has some physical effect
on the world, but it does it based on how it senses the world and how the world
changes around it.”
In summary, a robot is referred in the literature as a machine that performs an
intelligent connection between perception and action. An autonomous robot is pro-
grammed to work without human intervention, and with the aid of embodied artifi-
cial intelligence can perform and live within its environment. Today’s mobile
robots can move around safely in cluttered surroundings, understand natural
speech, recognize real objects, locate themselves, plan paths, and generally think
by themselves. Intelligent mobile robot design employs the methodologies and
technologies of intelligent, cognitive, and behavior-based control. Mobile robots
must maximize flexibility of performance subject to minimal input dictionary and
minimal computational complexity.
1.2.2
Robot History
The history of robots can be divided in two general periods [1,2]:
G
Ancient and preindustrial period
G
Industrial and robosapien period
1.2.2.1
Ancient and Preindustrial Period
The first robot in the worldwide history (around 25003000 BC) is the Greek
mythodological mechanical creature called Talos (“Τ αλως”) [3]. This name is
attributed both to a human being (the son of Daedalus’ sister Perdika) and a
mechanical artificial entity constructed by Hephaestus, under the order of Zeus,
with bronze body and a single vein from the neck up to the ankle, where a copper
nail blocked it out. Talos was gifted by Zeus to Europe who afterward gave him to
her son Minos to guard Crete. Talos died when the Argonaut Poas removed the
copper nail from his heel. This resulted in the spilling out of the ichor (“the blood
2
Introduction to Mobile Robot Control

of the immortals”) flowing in the Poas’ vein. The name Talos was given to
Asteroid 5786 discovered by Robert McNaught, on September 31, 1991 at Siding
Spring Observatory in Coonabarabran, New South Wales (Australia). Around 350 BC
the friend of Plato Archytas of Tarentum constructed a mechanical bird (“pigeon”)
which was propelled by steam. This represents one of the earlier historic studies of
flight or airplane model:
Around 270 BC: Ktesibios (“Kτησιβιoς”) has discovered the water clock that involves
movable parts, and wrote his book “About Pneumatics” (Περι Πνευματικης) where he
has shown that air is a material entity.
Around 200 BC: Chinese artisans design and construct mechanical automata, such as
orchestra, etc.
Around 100 AD: Heron of Alexandria designs and constructs several regulating mechan-
isms, such as the odometer, the steam boiler (aclopyle), the automatic opening of temples,
and automatic distribution of wine.
Around 1200 AD: The Arab author Al Jazari writes “Automata” which is one of the most
important texts in the study of the history of technology and engineering.
Around 1490: Leonardo Da Vinci constructs a device that looks as an armored knight.
This seems to be the first humanoid robot in Western civilization.
Around 1520: Hans Bullman (Nuernberg, Germany) builds the first real android in robot
history imitating people (e.g., playing musical instruments).
1818: Mary Shelley writes the famous novel Frankenstein based on an artificial life crea-
ture (robot) developed by Dr. Frankenstein. All robots in this novel turned eventually
against human kind in a frightening way.
1921: The Chzech dramatist Karel Capek coins the term robota (robot) in his play named
“Rossum’s Universal Robots” meaning compulsory or slavery work.
1940: The science fiction writer Isaac Asimov used for the first time the terms “robot”
and “robotics.” In 1942, he wrote “Runaround” a story that involved his three laws of
robotics (known as Asimov’s laws).
1.2.2.2
Industrial and Robosapien Period
This period starts in 1954 when George Devol, Jr patented his multijoined robotic
arm (the first modern robot). In 1956, together with Joseph Engelberger founded
the world’s first robot company called Unimation (from Universal Automation):
1961: The first industrial robot called Unimate joined a die-casting production line at
General Motors.
1963: The RanchoArm, the first computer controlled robotic arm, was put in operation at
the Rancho Los Amigos Hospital (Downey, CA). This was a prosthetic arm designed to
aid the handicapped.
1969: The first truly flexible arm, known as the, Stanford Arm, was developed in the
Stanford Artificial Intelligence Laboratory by Victor Scheinman. This arm soon became a
standard, and is still influencing the design of today’s robotic manipulators.
1970: This is the starting year of mobile robotics. The mobile robot Shakey was devel-
oped at the Stanford Research Institute (today known as SRI Technology), controlled by
intelligent algorithms that observe via sensors, and react to their own actions (Figure 1.1).
3
Mobile Robots: General Concepts

Shakey is referred to as “the first electronic person.” The name Shakey is due to its jerky
motion.
1979: The Stanford Cart, originally designed in 1970 as a line follower, is rebuilt by
Hans Moravec and equipped with a more robust 3-D vision that allows more autonomy
(Figure 1.2). In an experiment the Stanford Cart crossed a chair-filled room autonomously
using a TV camera that was taking pictures from several angles. These pictures were pro-
cessed by a computer to analyze the distance of the cart from the obstacles.
19801989: This decade is dominated by the development of advanced Japanese robots,
especially walking robots (humanoids). Among them the WABOT-2 humanoid is men-
tioned (Figure 1.3B) which was developed in 1984, representing the first attempt to
develop a personal robot with a “specialistic purpose” to play a keyboard musical instru-
ment, rather than a versatile robot like WABOT-1 (Figure 1.3A).
Other robots developed in the 1980s are the 1983 British computer controlled micro
robot vehicle Prowler (Figure 1.4), the 1985 Waseda-Hitachi Leg-11 (WHL-11), the 1989
Aquarobot (Figure 1.5), and the 1989 multilegged robot Genghis of MIT (Figure 1.6).
19901999: During this decade the emergence of “explorer robots” took place. These
robots went where the human did not visit before or it was considered too risky or incon-
venient. Examples of such robots are Dante (1993) and Dante II (1994) that explored
Mt. Erebrus in Antarctica and Mt. Spurr in Alaska (Figure 1.7).
An example of NASA planetary missions aiming at studying the climate and geology
of the Red Planet (Mars) is Path Finder. The Mars Observer was to touch down close to
the south pole latitudes, carrying two scientific instruments and a lander (robotic rover).
Figure 1.1 The wheeled mobile robot Shakey of SRI
which is equipped with on-board logic, a camera, a
range finder sensor, and a bump detector.
Source: http://www.thocp.net/reference/robotics/
robotics2.htm
4
Introduction to Mobile Robot Control

The Pathfinder spacecraft landed successfully on Mars’ Ares Vallis region on July 4,
1997. Its robotic rover was named Sojourner and is a 10.6 kg WMR (Figure 1.8).
Sojourner conducted a number of experiments on the surface of Mars and continued to
broadcast data until September 1997.
2000Present: In the 2000s the development of numerous new intelligent mobile robots
capable of almost fully interacting with humans recognizing voices, faces and gestures,
Figure 1.3 The Waseda University robots (A) WABOT-1, (B) WABOT-2.
Source: http://www.humanoid.waseda.ac..jp/booklet/kato_2.html.
Figure 1.2 The Stanford cart.
Source: http://www.thocp.net/reference/robotics/
robotics2.htm.
5
Mobile Robots: General Concepts

and expressing emotions through speech, dexterous walking or performing household
chores, hospital chores, microsurgeries and the like, is continuing with growing rates.
Notable examples are:
G
HONDA humanoid ASIMO (2000) (Figure 1.9)
G
LEGO Robotics Invention System-2 (2000)
G
FDA Cyberknife for treating tumors anywhere in the human body (2001)
Figure 1.4 The expandable
microrobot Prowler
published in Sinclair
projects, August 1983.
Source: http://www.
davidbuckley.net/DB/
Prowler.htm.
Figure 1.5 The AQUA robot can
take pictures of coral reefs and other
aquatic organisms returning home
after completion of its tasks.
Source: http://www.rutgersprep.org/
kendall/7thgrade/cycleA_2008_09/zi/
robo_AQUA.html.
Figure 1.6 The Genghis robot.
Genghis has a special
multilegged walking mode
known as the “Genghis gait.”
It is now retired at the
Smithsonian Air Space
Museum.
Source: http://www.ai.mit.edu/
prohects/genghis.
6
Introduction to Mobile Robot Control

G
SONY AIBO ERS-7: Third generation robotic pet (2003)
G
iROBOT Roomba, a robotic vacuum cleaner (2003)
G
TOMY i-SOBOT entertainment robot, a humanoid robot capable of walking like a
human and performing entertainment actions, such as kicks and punches (2007)
G
SHADOW dextrous hand robot (2008)
G
ROLLIN JUSTIN robot of the German Air Space Agency (2009), a humanoid prepar-
ing and serving drinks (Figure 1.10)
G
FLAME: the Toyota 130 cm running humanoid robot
G
WowWee Roborover, Joebot, and Robosapien robots.
Figure 1.7 The Dante II explorer robot.
Source: http://www.frc.ri.cmu.edu/robots/robs/photos/
1994_DanteII.jpg.
Figure 1.8 The NASA Sojourner robotic
rover.
Source: http://haberlesmeplatformu.
blogspot.com/2010_04_01_archive.html.
7
Mobile Robots: General Concepts

Figure 1.9 The Honda’s humanoid ASIMO.
Source: http://www.gizmag.com/go/1765picture/2029; http://razorrobotics.com/safety.

A comprehensive presentation of the development and current maturity of robosa-
piens and socialized robots is provided in Menzel and D’Aluisio book (Robo Sapiens:
Evolution and New Species, MIT Press, MA, 2000). Three currently commercially
available
mobile
robotic
platforms
for
research
purposes
are
the
following
(Figure 1.11):
Seekur: An all weather large holonomic robot platform for security, inspection, and
research
Pioneer 3-DX: A fully programmable platform equipped with motors-encoders, and 16
ultrasonic (front-facing and rear-facing sonars). It is used for research and rapid develop-
ment (localization, monitoring, navigation, control, etc.)
Figure 1.11 (A) Seekur (350 kg, dimensions 1.4 m 3 1.3 m 3 1.1 m), (B) Pioneer 3D-X, and
(C) PowerBot.
Source: http://mobilerobots.com/ResearchRobots/ResearchRobots.aspx; http://www.
conscious-robots.com/en/reviews/robots/mobilerobots-pioneer-3p3-dx-8.html.
Figure 1.10 The Rollin Justin robot mixing instant
tea.
Source: http://inventors.about.com/od/robotart/ig/
Robots-and-Robotics/Rollin-Justin-Robot.htm.
9
Mobile Robots: General Concepts

PowerBot: A high payload (up to 100 kg) differential drive robot for research and rapid
prototyping in universities and research institutes.
1.3
Ground Robot Locomotion
Locomotion of ground mobile robots is distinguished in [415]1 :
G
Legged locomotion
G
Wheeled locomotion
1.3.1
Legged Locomotion
The wheel is a human invention, but the leg is a biological element. Locomotion of
most of the highly developed animals is through legs. Biological multilegged
organisms can move in diverse and difficult environments with obstacles, rough
grounds, etc. Insects have very small size and weight and possess strong robustness
that cannot be achieved by artificial creatures. To be useful in real-life tasks, a leg-
ged robot must be statically stable. This condition is satisfied if the center of grav-
ity lies always within the polygon defined by the actual contact points with the
ground. This can be achieved only if at each time three feet are in contact with the
floor. Thus to guarantee statical stability four legs are at least needed. If the feet do
not have contact points, but lines or planes of contact, this might not be true, and a
statically stable robot may have only two legs. In practice, the contact of the robot
body with the floor is a small region.
A legged robot is said to be dynamically stable if it is does not fall over, despite the
fact that it is not statically stable. Legged robots are distinguished in two main categories:
G
Two-leg (bipedal) robots
G
Many-leg robots
Bipedal locomotion is standing on two legs, walking and running.
Humanoid robots are bipedal robots with an overall appearance based on that of
the human body, that is, head, torso, legs, arms, and hands. Some humanoids may
model only part of the body, for example, from waist up, such as in the NASA’s
Robonaut, while others have also a “face” with “eyes” and “mouth.”
Robot bipedal locomotion needs complex interaction of mechanical and control
system features. Humans are able to perform bipedal locomotion because their
spines are s-curved and their heels are round. During locomotion the legs need to
be lifted from and return to the ground. The sequence and way of placing and lift-
ing each foot (in time and space), synchronized with the body motion so as to
move from one place to another, is called gait.
The human gait involves the following distinct phases:
G
Rocking back and forth between feet
1 UAVs and AUVs will not be studied in this book.
10
Introduction to Mobile Robot Control

G
Pushing with the toe to sustain speed
G
Combined interruption in rocking and ankle twist to turn
G
Shortening and extending the knees to prolong the “forward fall”
The basic cycle of a gait is called a stride, which describes the complete cycle
from one occurrence of a leg motion to its repetition. The fraction of a stride during
which the foot is in on-state is called the duty factor. For statically stable walking
the minimum duty factor needed is “3: (number of legs),” where 3 is the minimum
number of feet in on-state to assure static stability. A walking gait is one where at
least one foot is on the ground at any time. A running gait is occurring if for some
time periods all feet are in off-state. Robots without static stability need higher
energy to achieve dynamic stability, and extra stabilizing motions to prevent the
body from falling over (in addition to useful/productive motions). The initial
research on multilegged walking robots was focused on robot locomotion design
for smooth or easy rough terrain, by passing simple obstacles, motion on soft
ground, body maneuvering, and so on. These requirements can be realized via peri-
odic gaits and binary (yes/no) contact information with the ground. Newer studies
are concerned with multilegged robots that can move over an impassable road or
Figure 1.12 The quadruped robot
“Kotetsu” (leg length at standing
1822 cm).
Source: http://robotics.mech.kit.ac.jp/
kimura/research/Quadruped/photo-
movie-kotetsu-e.html.
Figure 1.13 DARPA
quadruped robot (LC3).
Source: http://www.gizmag.
com/darpa-lc3-robot-
quadruped/14256/picture/
111087.
11
Mobile Robots: General Concepts

an extremely complex terrain, such as mountain areas, ditches, trenches, earthquake
damaged areas, etc. In these cases, additional capabilities are needed, as well as
detailed support reactions and robot stability prediction. Figures 1.121.15 show
three advanced multilegged robots with capabilities of the above type. The quadru-
ped robot Kotetsu of Figure 1.12 is capable of adaptive walking using phase modu-
lations based on leg loading/unloading.
1.3.2
Wheeled Locomotion
The maneuverability of a WMR depends on the wheels and drives used. The
WMRs that have three DOF are characterized by maximal maneuverability which
is needed for planar motions, such as operating on a warehouse floor, a road, a hos-
pital, a museum, etc. Nonholonomic WMRs have less than three DOF in the plane,
but they are simpler in construction and cheaper because less than three motors are
used. A holonomic vehicle can travel in every direction and function in tight areas.
This capability is called omnidirectionality. Balance is inherently assured in WMRs
with three or more wheels. But in the case of m-wheel WMRs (m $ 3) a suspension
Figure 1.15 A typical example of hexabot
robo-spider (Gadget Lab).
Source: http://www.wired.com/gadgetlab/
2010/04/gallery-spider-robot/2/.
Figure 1.14 Six-legged robot “SLAIR”
capable of operating at “action level.”
Source: http://www.uni-magdeburg.de/
ieat/robotslab/images/Slair/CIMG1059.jpg.
12
Introduction to Mobile Robot Control

system must be used to assure that all wheels can have ground contact in rough ter-
rains. The main problems in WMR design are the traction, maneuverability, stabil-
ity, and control that depend on the wheel types and configurations (drives).
1.3.2.1
Wheel Types
The types of wheels used in WMRs are:
G
Conventional wheels
G
Special wheels
Conventional wheels: These wheels are distinguished in powered fixed wheels,
castor wheels, and powered steering wheels. Powered fixed wheels (Figure 1.16A)
are driven by motors mounted on fixed positions of the vehicle. Their axis of rota-
tion has a fixed direction with respect to the platform’s coordinate frame. Castor
wheels (Figure 1.16B) are not powered but they can also rotate freely about an axis
perpendicular to their axis of rotation.
Powered steering wheels have a driving motor for their rotation and can be
steered about an axis perpendicular to their axis of rotation. They can be without
offset (Figure 1.16C) or with offset (Figure 1.16D) in which case the axes of rota-
tion and steering do not intersect. To achieve omnidirectionality with conventional
castor and powered steering wheels some kind of motion redundancy should be
used, for example, n-wheel drives (n . 2) with all wheels driven and steered.
Conventional wheels have higher load capacities and higher tolerance for ground
irregularities compared to special wheel configurations. But due to their nonholo-
nomic constraints are not truly omnidirectional wheels.
Special wheels: These wheels are designed such that to have activated traction
in one direction and passive motion in another, thus allowing greater maneuverabil-
ity in congested environments. We have three main types of special wheels:
1. Universal wheel
2. Mecanum wheel
3. Ball wheel
The universal wheel provides a combination of constrained and unconstrained
motion during turning. It contains small rollers around its outer diameter which are
Figure 1.16 Conventional wheels (A) fixed wheel, (B) castor wheel, (C) powered steering
wheel without any offset, and (D) power steering wheel with longitudinal offset.
13
Mobile Robots: General Concepts

mounted perpendicular to the wheel’s rotation axis. This way the wheel can roll in
the direction parallel to the wheel axis in addition to the normal wheel rotation
(Figure 1.17).
The mecanum wheel is similar to the universal wheel except that the rollers are
mounted at an angle α other than 90 (usually 6 45) (Figure 1.18).
Figure 1.18A and B shows the omnidirectional wheel as it looks from the bot-
tom (via a glass floor). The force F produced by the rotation of the wheel acts on
the ground via the roller that has contact with the ground (which is assumed suf-
ficiently flat without irregularities). At this roller, the force is decomposed in a
force F1 parallel to the roller axis and a force F2 perpendicular to the roller axis.
The force perpendicular to the roller axis produces a small roller rotation
(speed vr), but the force parallel to the roller axis exerts a force on the wheel and
thereby on the vehicle resulting in the hub speed vh. The actual velocity vt of the
vehicle is the combination of vh and vr. Figure 1.18C shows a practical mecanum
wheel.
45°
–45°
F
F1
F2
(A)
(B)
(C)
Figure 1.18 (A) Mecanum wheel with α 5 45 (left wheel), (B) mecanum wheel with
α 5 245 (right wheel), and (C) an actual mecanum wheel.
Source: http://www.aceize.com/node/562.
Figure 1.17 Three designs of universal wheel.
Source: http://www.generationrobots.com/2-omni-directional-wheel-robot-v.ex-robotics,
us,4,2165-Omni-Directional-Wheel-kit.cfm; http://www.rotacaster.com.au/robot-wheels.html;
http://www-scf.usc.edu/Bcsci445_final_contest/SearchAndRescue/OtherContests/
2004_contest_Fall_RobotSoccer/Locomotion/omni_4wheel_encoders/omni_drive.pdf.
14
Introduction to Mobile Robot Control

The ball (or spherical) wheel places no direct constraints on the motion, that is,
it is omnidirectional like castor or special universal and mecanum wheels. In other
words, the rotational axis of the wheel can have any arbitrary direction. One way to
achieve this is by using an active ring driven by a motor and gearbox to transmit
power to the ball via rollers and friction, which is free to rotate in any direction
instantaneously. Because of its difficult construction the ball wheel is very rarely
used in practice. A type of ball wheel is shown in Figure 1.19.
1.3.2.2
Drive Types
The drives of WMRs are distinguished in:
G
Differential drive
G
Tricycle
G
Omnidirectional
G
Synchro drive
G
Ackerman steering
G
Skid steering
Differential drive: This drive consists of two fixed powered wheels mounted on
the left and right side of the robot platform. The two wheels are independently
driven. One or two passive castor wheels are used for balance and stability.
Differential drive is the simplest mechanical drive since it does not need rotation of
a driven axis. If the wheels rotate at the same speed, the robot moves straight for-
ward or backward. If one wheel is running faster than the other, the robot follows a
curved path along the arc of an instantaneous circle. If both wheels are rotating at
the same velocity in opposite directions, the robot turns about the midpoint of the
two driving wheels. The above locomotion modes are illustrated in Figure 1.20.
Clearly, this type of WMR cannot turn on the spot.
Omni-ball
Chassis
Geared motor
Motor driver
Figure 1.19 A practical implementation of a ball wheel.
Source: http://www-hh.mech.eng.osaka-u.ac.jp/robotics/Omni-Ball_e.html.
15
Mobile Robots: General Concepts

The instantaneous center of curvature (ICC) of the WMR lies at the cross point
of all axes of the wheels. ICC is the center of the circle with radius R depending on
the speeds of the two wheels (Figure 1.21).
The radius R is determined by the relation: ðvl 2 vrÞ=2a 5 vr=ðR 2 aÞ.
Q
(A)
(B)
(C)
(D)
Initial 
configuration
Final configuration
(E)
Figure 1.20 Locomotion possibilities of differential drive. (A) Straight path, (B) curved
path, (C) circular path, (D) obstacle-free maneuvering to go from an initial to a final pose,
and (E) maneuvering to go from an initial to a final pose while avoiding obstacles.
Source: Colored picture, courtesy of N. Katevas.
a
a
vr
vl
ICC
R
Figure 1.21 Calculation of the instantaneous
radius R of WMR rotation.
16
Introduction to Mobile Robot Control

Therefore:
R 5 aðvl 1 vrÞ=ðvl 2 vrÞ;
vl $ vr
ð1:1Þ
When vl 5 vr we have R 5 N (i.e., straight motion, and when vr 5 2 vl we have
R 5 0 (i.e., rotational motion).
Tricycle: This drive has a single wheel which is both driven (powered) and
steered. For stability, two free-running (unpowered) fixed wheels in the back are
used, in order to have always the three point contact required. The linear and angu-
lar velocities of the wheel are fully decoupled. For driving straight, the wheel is
positioned in the middle position and driven at the desired speed (Figure 1.22A).
When the front wheel is at an angle the vehicle follows a curved path
(Figure 1.22B). If the front wheel is positioned at 90, the robot will rotate follow-
ing a circular path the center of which is in the middle point of the rear wheels and
not in the robot’s geometric center (Figure 1.22C). This means that this WMR can-
not turn on the spot. Nonholonomic WMRs (like differential drive or tricycle
Passive wheel
Forward
Backward
Q
(A)
(B)
(C)
(D)
(E)
Figure 1.22 Tricycle WMR locomotion modes (AD), (E) A tricycle example.
Source: http://www.asianproducts.com/product/A12391789884559774_p1240094409205655/
cargo-tricycle(250cc).html.
L
L
R
R
(A)
(B)
Figure 1.23 Omnidirectional
WMRs. (A) Three-wheel case,
(B) four-wheel case with roller
angle different than 90 (typically
α 5 6 45).
Source: Colored picture, courtesy
of Jahobr.
17
Mobile Robots: General Concepts

vehicles) cannot perform parallel parking directly but by a number of maneuvers
with forward and backward movements as shown in Figure 1.22D.
Omnidirectional: This drive can be obtained using three, four, or more omnidi-
rectional wheels as shown in Figure 1.23. WMRs with three wheels use universal
wheels that have a 90 roller angle (Figure 1.17) as shown in Figure 1.23A.
Omnidirectional WMRs with four wheels use mecanum wheels (Figure 1.18) in the
configuration shown in Figure 1.23B.
From the four wheels in Figure 1.24, two are called left-handed (L) wheels and
the other two right-handed (R) wheels. The left-handed wheels have a roller angle
α 5 45 and the right-handed ones an angle α 5 245. Therefore, the four-wheel
omnidirectional WMRs have the typical structure shown in Figure 1.24.
Figure 1.25 shows six basic motions of a four-wheel omnidirectional robot,
namely (A) forward motion, (B) left sliding, (C) clockwise turning (on the spot),
(D) backward motion, (E) right sliding, and (F) anticlockwise turning. The arrows
on the left and the right side of the vehicle show the motion direction of the corre-
sponding wheels.
1
2
3
4
1
2
3
4
1
2
3
4
1
2
3
4
1
2
3
4
1
2
3
4
(A)
(B)
(C)
(D)
(E)
(F)
Figure 1.25 Six basic
locomotion modes of a WMR
with four mecanum wheels.
R
R
L
L
1
2
3
4
R
L
L
R
–45°
–45°
45°
45°
Figure 1.24 Standard setup
of a four-mecanum-wheel
omnidirectional WMR.
Source: http://www.interhopen.
com/download/pdf/pdfs_id/
465/InTech-Omnidirectional_
mobile_robot_design_and_
implementation.pdf.
18
Introduction to Mobile Robot Control

The arrows on the vehicle platform show the respective directions of the WMR
motion, that is, for forward vehicle direction all wheels must move forward
(Figure 1.25A), for left sliding wheels 1, 3 should move forward, and wheels 2, 4
backward, and so on. The locomotions shown in Figure 1.25 are obtained if all
wheels move at the same speed. By varying the speed magnitude of the wheels one
can realize WMR motion in any direction on the 2-D plane. A few cases are shown
in Figure 1.26.
All locomotions of Figures 1.25 and 1.26 can be explained using the force or
velocity diagrams of Figure 1.18A and B). For example, because of the symmetri-
city of left and right wheels (Figure 1.24), if all wheels are driven forward, there
are four vectors pointing forward that are added up and four vectors pointing side-
ways, two to the left and two to the right, that cancel each other out. Thus, in over-
all the WMR moves forward. The L and R wheels may be interchanged (i.e., front
wheels RL, back wheels LR). Again, with proper motion of the wheels all omnidi-
rectional locomotion modes can be obtained with this setup too.
Synchro drive: This drive has three or more wheels that are mechanically cou-
pled such that all of them rotate in the same direction at the same speed and pivot
in unision about their own steering axes when perform a turn. This mechanical
steering synchronization can be realized in several ways, for example, using a
chain, a belt or gear drive. Actually, synchro drive is an extension of a single
driven and steered wheel and so it still has only two DOF. But a synchro drive
WMR is nearly a holonomous vehicle because it can move in any desired direction.
However, it cannot drive and rotate at the same time. To change its driving from
forward to sideways this WMR must stop and realign its wheels. Figure 1.27A
shows pictorially how a three-wheel WMR with synchro drive is moving and
rotated.
Chain- or belt-based synchro drive presents lower steering accuracy and align-
ment. This problem does not occur if a gear drive is used. Actually, two indepen-
dent motor-drive subsystems (operating with chain, belt, and gear) must be used;
one for the steering and one for the driving shaft (Figure 1.27B).
Ackerman steering: This is the standard steering used in automobiles. It consists
of two combined driven rear wheels and two combined steered front wheels. An
Ackerman-steered vehicle can move straight (because the rear wheels are driven by
(A)
(B)
(C)
(D)
(E)
(F)
Figure 1.26 Six more locomotions:
(A) forward-right, (B) forward-left,
(C) curved right, (D) backward-right,
(E) backward-left, and (F) lateral arc.
19
Mobile Robots: General Concepts

a common axis), but cannot turn on the spot (it requires a certain minimum radius).
The rear driving wheels experience slippage (when moving in curves).
Ackerman steering is designed so as to ensure that at turns the wheels of all
axes have a common cross point (instantaneous center of rotation (ICR)), in order
to avoid geometrically caused wheel slippage.
From Figure 1.28, we find the following relations:
ctgφs 5 ða 1 LÞ=D; ctgφo 5 ð2a 1 LÞ=D; ctgφi 5 L=D
which by elimination of L give:
ctgφs 5 a
D 1 ctgφi
Drive motor
Drive belt
Wheel
steering
axis
Rolling axis
Driving pulley
Wheel
Turret pulley
Steering belt
Direction of 
motion
Steering pulley
x
(A)
(B)
(C)
Steering motor
Figure 1.27 (A) Illustration of synchro drive WMR motion, (B) illustration of the two
independent belt subsystems, and (C) a synchro drive WMR example.
Source: http://members.efn.org/Bkirbyf/05syncro/1.jpg.
`
L
D
a
a
φ0
φs
φi
φ0
φs
φi
Q1
(ICR)
x
y
Q2
Figure 1.28 The rotation axes of all wheels intersect at the same point ICR.
20
Introduction to Mobile Robot Control

or
ctgφs 5 ctgφo 2 a
D
ð1:2Þ
where φs is the vehicle’s actual steering angle and φo, φi are the steering angles of
the outer and inner wheel, respectively.
Figure 1.29 illustrates the constraint in the motion of an Ackerman-steered vehi-
cle. There is a circular area on the left and on the right of its current position which
is inaccessible to the vehicle. This is due to that the robot cannot turn (right or left)
following a path with radius smaller than a minimum. Therefore, for parallel park-
ing a considerable maneuvering is required.
Skid steering: This is a special implementation of differential drive and realized
in track form on bulldozers and harmored vehicles. Its difference from a differen-
tial drive vehicle is the increased maneuverability in uneven terrains, and the higher
friction which is due to its tracks and the multiple contact points with the terrain
(rough or even). Figure 1.30A illustrates that the effective point contact for such a
skid-steer robot is roughly constrained on either side by a rectangular uncertainty
area which corresponds to the track footprint.
One can see from the concentric circles that for the vehicle to turn, a consider-
able slippage is needed. Figure 1.30B shows a typical tracked robotic platform that
can carry a manipulator or special exploration equipment.
Figure 1.29 The shaded areas are
inaccessible to the Ackerman-steered robot.
Track 
footprint
lmin
lmax
(A)
(B)
Figure 1.30 (A) Illustration of the effective contact point and (B) a typical tracked platform.
Source: http://www.robotshop.com/Dr-robot-jaguar-tracked-mobile-platform-chassis-motors-
3.html.
21
Mobile Robots: General Concepts

1.3.2.3
WMR Maneuverability
The maneuverability Mw of WMRs is defined as
Mw 5 Dm 1 Ds
ð1:3Þ
where Dm is the degree of mobility, and Ds the degree of steerability.
Degree of mobility: The degree of mobility Dm is determined by the number of
independent constraints that the type of wheels and their configuration impose on
the motion ability of the robot. Constraints on the motion are imposed only by con-
ventional wheels (fixed or steered). Omnidirectional wheels do not impose any con-
straint on the robot’s mobility. The best way to see the independent kinematic
constraints of a WMR is by studying the geometric properties of the robot through
the ICC or ICR. For example, a single conventional wheel cannot move laterally,
that is, along the line determined by its axis of rotation. This line is called the zero
motion line of the wheel. This means that the wheel can only move on an instanta-
neous circle of radius R with its center lying on the zero motion line. A bicycle has
two wheels: the steered front wheel and the fixed wheel on the rear (Figure 1.31).
Each wheel introduces a separate (independent) zero motion line. The two lines
intersect at the ICR. In the case of a differential drive WMR (Figure 1.21) the zero
motion lines of the two (common axis) wheels coincide and so they are not inde-
pendent. This means that there is only one independent kinematic constraint. Any
point on the common zero motion line can be an ICR. In the Ackerman steering,
the WMR has four conventional wheels but two independent kinematic constraints
(Figure 1.28). The two rear wheels impose a single constraint (as in the differential
drive), and also the two front steered wheels impose a second single kinematic con-
straint, because they cross on an ICR lying on the zero motion line determined by
the common axis rear wheels. The maximum degree of mobility Dm is 3, which is
true when no kinematic constraints are imposed. This is the case when all wheels
of the WMR are omnidirectional. In general, the degree of mobility is equal to:
Dm 5 3 2 Nc
ð1:4Þ
where Nc is the number of independent constraints.
Degree of steerability: The degree of steerability Ds depends on the number of
independently controllable steering parameters and lies in the interval 0 # Ds # 2.
If no steerable wheels exist we have Ds 5 0. The case Ds 5 2 holds only if the robot
has no fixed standard wheels. In this case we can have a platform with two separate
ICR
Figure 1.31 The two wheels of a bicycle impose two
independent constraints.
22
Introduction to Mobile Robot Control

steerable conventional wheels (as e.g., in a 2-steer bicycle or 3-wheeled 2-steer
WMR). Actually, Ds 5 2 means that the WMR can place its ICR at any point of
the plane. The most common case is Ds 5 1 which is obtained when the robot con-
figuration has one or more steerable conventional wheels. A steered conventional
wheel can decrease the robot’s mobility, but also can increase the steerability. In
fact, although an instantaneous orientation of the wheel imposes a kinematic con-
straint its capability to change this orientation may allow additional trajectories.
The maneuverability ðMwÞ, the degree of mobility ðDmÞ and the degree of steerabil-
ity ðDsÞ of some typical WMR configurations are shown in Table 1.1.
Two other characteristic parameters of WMRs are the “degrees of freedom” and
the “differential degrees of freedom” (DDOF), which satisfy the relation:
DDOF # Mw # DOF
The DDOF is equal to Dm and represents the number of independent velocities
that can be achieved. DOF represents the ability of a WMR to achieve various
poses ðx; y; φÞ in its environment (work space).
A bicycle can achieve any pose ðx; y; φÞ on the plane by some maneuver and so
it has DOF 5 3, but its DDOF is DDOF 5 Dm 5 1. An omnirobot, with three omni-
directional wheels, has Dm 5 3, that is, DDOF 5 3, and also DOF 5 3. Similarly, a
tricycle has DDOF 5 Dm 5 1 and DOF 5 3, because it can reach any ðx; y; φÞ by
appropriate maneuvering.
A list of possible wheel drive configurations is as follows [7]:
G
One traction wheel in the back, one steering wheel in the front (bicycle, motor cycle)
G
Two-wheel differential drive with the center of mass, below the wheels’ axis (a balance
controller is needed)
G
Two-wheel differential drive centered with an omni wheel for stability (Nomad Scout
robot)
G
Three-wheel differential drive with an unpowered omni wheel, rear or front driven (typi-
cal indoor WMRs)
G
Two connected powered wheels (differential) in the back, one free turning wheel in front
G
One steered and driven wheel in front two free wheels in the back (e.g., Neptune)
Table 1.1 Degree of Mobility and Steerability ðDm; DsÞ of Typical WMRs
Configuration
Dm
Ds
Mw
Notation
Bicycle
1
1
2
(1,1)
Differential drive
2
0
2
(2,0)
Synchro drive
1
1
2
(1,1)
Tricycle
1
1
2
(1,1)
Ackerman steer
1
1
2
(1,1)
Two-steer
1
2
3
(1,2)
Omni-steer
2
1
3
(2,1)
Omnidirectional
3
0
3
(3,0)
23
Mobile Robots: General Concepts

G
Three omnidirectional wheels (universal)
G
Three synchronous powered and steered wheels (synchronous drive)
G
Car-like WMR (rear-wheel driven)
G
Car-like WMR (front-wheel driven)
G
Four-wheel drive, four-wheel steering (Hyperion)
G
Differential wheel drive in the back two omni wheels in front
G
Four Swedish omnidirectional wheels ðα 6¼ 90Þ (Uranus)
G
Four motorized and steered castor wheels (Nomad XR4000)
G
Multi wheel walking drive (rovers, climbing robots).
A small set of modern WMRs falling in the above categories are shown in
Figures 1.321.42.
Figure 1.42 shows the components of the AMiR swarm robot involving the
main board, communication module, kinematic design, power module, and sensory
system.
Figure 1.32 KIVA autonomous mobile robots.
(A) Pallet and case handling, (B) Order
fulfillment.
Source: (A) www.kivasystems.com/solutions/
picking/pick-from-pallets.
(B) www.kivasystems.com/about-us-the-kiva-
approach.
Figure 1.33 (A) SCITOS mobile general platform (SCITOS G5). (B) Robotic manipulator
mounted on SCITOS G5.
24
Introduction to Mobile Robot Control

Figure 1.43 shows the miniature WMR “Khepera” used for research for more
than 15 years. It was developed at the LAM laboratory in EPFL (Lausanne,
Switzerland). The initial version of Khepera (Figure 1.43A) is 55 mm diameter and
30 mm high robot. It has appropriate sensors and actuators to ensure that it can be
Figure 1.34 CORECON automated guided mobile robots (AGVs). (A) Horizontal roll
handling, (B) low lift rear loader, and (C) high lift side loader.
Source: www.coreconagvs.com/products.
Figure 1.35 SCITOS mobile
robot guide (it can provide users
valuable information via speech
or touch screen at any location).
Source: http://www.expo21xx.
com/automation21xx/
13582_st3_mobile-robots/default.
htm.
25
Mobile Robots: General Concepts

programmed to perform a large repertory of tasks. Khepera can run autonomously
or tethered to a host computer. The newer versions of Khepera (K-II Version and
K-III Version) are shown in Figure 1.43B and C).
Khepera is able to move on a table top as well as on a room floor for performing
real-world swarm robotics. To enable fast development of portable applications,
Khepera III supports a standard Linux operating system.
Finally, Figure 1.44 shows the famous mecanum-wheeled omnidirectional
WMR “Uranus.”
Figure 1.36 Mobile robot platform with
spring-loaded castors for physical
interaction with humans.
Source: http://robot.kaist.ac.kr/paper/view.
php?n 5 318.
Figure 1.37 The CMU Rover 1 robot climbing a stair.
Source: http://www.cs.cmu.edu/Bmyrover/Rover1/robot.htm.
26
Introduction to Mobile Robot Control

Figure 1.39 The NASA nBot (two-wheel
balancing robot).
Source: http://www.geology.smu.edu/Bdpa-www/
robot/nbot/nobot2/nb12.jpg.
Figure 1.38 The Nomad robot (CACS
Louisiana University).
Source: http://www.cacs.louisiana.edu/
Bsxg3148/nomad_pics/
Nomad_robot_jpg.
Body
Body
Payload support
Forearm
Touch sensitive
wheel
Arm
Figure 1.40 The EPFL wheeled
climbing “Octopus Robot”
(43 cm 3 42 cm 3 23 cm).
Source: http://www-robot.mes.titech.ac.
jp/robot/walking/rollerwalker/roller_e.
html.
27
Mobile Robots: General Concepts

Main SW
Main board
IR
receiver
IR
emitter
Plastic chassis
Caster
Battery
Motor
Wheel
Figure 1.42 The AMiR swarm
robot.
Source: http://www.
swarmrobotic.com/Robot.htm.
Figure 1.43 The evolution of Khepera WMR. (A) Original version, (B) Version K-II,
(C) Version K-III.
Source: http://mobotica.blogspot.com/2011/08khepera.html; www.k-team.com/mobile-
robotics-products/KheperaII.
Figure 1.41 Robotic swarms showing a “collective behavior.”
Source: http://www.humansinvent.com/#!/8878/swarm-robots-the-droid-workforce-of-the-
future/.
28
Introduction to Mobile Robot Control

References
[1] Freedman J. Robots through history: robotics. New York, NY: Rosen Central; 2011.
[2] Mayr O. The origins of feedback control. Cambridge, MA: MIT Press; 1970.
[3] Lazos C. Engineering and technology in ancient Greece. Athens: Aeolos Editions;
1993.
[4] Campion G, Bastin G, D’Andre´a-Novel B. Structural properties and classification of
kinematic and dynamic models of wheeled mobile robots. IEEE Trans Rob Autom
1996;12(1):4762.
[5] Floreano D, Zufferey J-C. Robots mobiles. EPFL Course-Mobile Robots. ,www.cs.cmu.
edu/Bgwp/robots/Uranus.html..
[6] Bekey G. Autonomous robots. Cambridge, MA: MIT Press; 2005.
[7] Siegwart R, Nourbakhsh I. Autonomous mobile robots. Cambridge, MA: MIT Press;
2005.
[8] Bra¨unl T. Embedded robotics: mobile robot design and applications with embedded
systems. Berlin: Springer; 2006. ,http://newplans.net/RDB..
[9] Salih J, Rizon M, Yacacob S, Adom A, Mamat M. Designing omni-directional mobile
robot with mecanum wheel. Am J Appl Sci 2006;3(5):18315.
[10] West M, Asada H. Design of ball wheel mechanisms for omnidirectional vehicles with
full mobility and invariant kinematics. J Mech Des 1997;119:1537.
[11] Holland J-M. Rethinking robot mobility. Rob Age 1988;7(1):2630.
[12] Duro JR, Santos J, Grana M. Biologically inspired robot behavior engineering. Berlin/
Heidelberg: Springer; 2002.
[13] Katevas N, editor. Mobile robotics in healthcare. Amsterdam: IOS Press; 2001.
[14] Tzafestas SG, editor. Autonomous mobile robots in health care services. J Intell Rob
Syst 1998;22(34):177350 [special issue].
[15] Fong T, Nourbakhsh IR, Dautenhahn K. A survey of socially interactive robots. Rob
Auton Syst 2003;42(34):14366.
Figure 1.44 “Uranus” four-wheel omnidirectional robot.
Source: http://www.cs.cmu.edu/afs/cs/user/gwp/www/robots/Uranus.jpg.
29
Mobile Robots: General Concepts

2 Mobile Robot Kinematics
2.1
Introduction
Robot kinematics deals with the configuration of robots in their workspace, the rela-
tions between their geometric parameters, and the constraints imposed in their trajec-
tories. The kinematic equations depend on the geometrical structure of the robot. For
example, a fixed robot can have a Cartesian, cylindrical, spherical, or articulated
structure, and a mobile robot may have one two, three, or more wheels with or with-
out constraints in their motion [120]. The study of kinematics is a fundamental pre-
requisite for the study of dynamics, the stability features, and the control of the robot.
The development of new and specialized robotic kinematic structures is still a topic
of ongoing research, toward the end of constructing robots that can perform more
sophisticated and complex tasks in industrial and societal applications [120].
The objectives of this chapter are as follows:
G
To present the fundamental analytical concepts required for the study of mobile robot
kinematics
G
To present the kinematic models of nonholonomic mobile robots (unicycle, differential
drive, tricycle, and car-like wheeled mobile robots (WMRs))
G
To present the kinematic models of 3-wheel, 4-wheel, and multiwheel omnidirectional WMRs.
2.2
Background Concepts
As a preparation for the study of mobile robot kinematics the following background
concepts are presented:
G
Direct and inverse robot kinematics
G
Homogeneous transformations
G
Nonholonomic constraints
2.2.1
Direct and Inverse Robot Kinematics
Consider a fixed or mobile robot with generalized coordinates q1; q2; . . .; qn in the
joint (or actuation) space and x1; x2; . . .; xm in the task space. Define the vectors:
q 5
q1
q2
^
qn
2
664
3
775;
p 5
x1
x2
^
xm
2
664
3
775
ð2:1Þ
Introduction to Mobile Robot Control. DOI: http://dx.doi.org/10.1016/B978-0-12-417049-0.00002-X
© 2014 Elsevier Inc. All rights reserved.

The problem of determining p knowing q is called the direct kinematics prob-
lem. In general pARm and qARn (Rn denotes the n-dimensional Euclidean space)
are related by a nonlinear function (model) as:
p 5 fðqÞ;
fðqÞ 5
f1ðqÞ
f2ðqÞ
^
fmðqÞ
2
664
3
775
ð2:2Þ
The problem of solving Eq. (2.2), that is of finding q from p, is called the
inverse kinematic problem expressed by:
q 5 f21ðpÞ
ð2:3Þ
The direct and inverse kinematic problems are pictorially shown in Figure 2.1.
In general, kinematics is the branch of mechanics that investigates the motion of
material bodies without referring to their masses/moments of inertia and the forces/
torques that produce the motion. Clearly, the kinematic equations depend on the
fixed geometry of the robot in the fixed world coordinate frame.
To get these motions we must tune appropriately the motions of the joint vari-
ables, expressed by the velocities _q 5 ½_q1; _q2; . . .; _qnT. We therefore need to find
the differential relation of q and p. This is called direct differential kinematics and
is expressed by:
dp 5 Jdq
ð2:4Þ
where
dq 5
dq1
^
dqn
2
4
3
5; dp 5
dx1
^
dxm
2
4
3
5
and the m 3 n matrix:
J 5
@x1
@q1
@x1
@q2
?
@x1
@qn
?
?
?
?
?
?
?
?
@xm
@q1
@xm
@q2
?
@xm
@qn
2
6666664
3
7777775
5 Jij


ð2:5Þ
Robot direct
kinematic model
p
q
q
f (·)
f –1 (·)
Robot inverse
kinematic model
Figure 2.1 Direct and inverse robot kinematic models.
32
Introduction to Mobile Robot Control

with ði; jÞ element Jij 5 @xi=@qj is called the Jacobian matrix of the robot.1
For each configuration q1; q2; . . .; qn of the robot, the Jacobian matrix represents
the relation of the displacements of the joints with the displacement of the position
of the robot in the task space.
Let _q 5 ½_q1; . . .; _qnT and _p 5 ½_x1; _x2; . . .; _xmT be the velocities in the joint and
task spaces.
Then, dividing Eq. (2.4) by dt we get formally:
dp
dt 5 J dq
dt or _p 5 J_q
ð2:6Þ
Under the assumption that m 5 n (J square) and that the inverse Jacobian matrix
J21 exists (i.e., its determinant is not zero: det J 6¼ 0), from Eq. (2.6) we get:
_q 5 J21 _p
ð2:7Þ
This is the inverse differential kinematics equation, and is illustrated in
Figure 2.2.
If m 6¼ n, then we have two cases:
Case 1 There are more equations than unknowns ðm . nÞ, that is, _q is overspeci-
fied. In this case J21 in Eq. (2.7) is replaced by the generalized inverse Jy
given by:
Jy 5 ðJTJÞ21JT
ð2:8aÞ
under the condition that J is full rank (i.e., rank J 5 min (m.n) 5 n) so as JTJ is
invertible. The expression (2.8a) of Jy follows by minimizing the squared norm of
the difference _p 2 J_q, that is of the function:
V 5 jj_p 2 J_qjj2 5 ð_p2J_qÞTð_p 2 J_qÞ
with respect to _q. The optimality condition is:
@V=@_q 5 2 2ð_p2J_qÞTJ 5 0
1 It is remarked that in many works the Jacobian matrix is defined as the transpose of that defined in
Eq. (2.5).
q
•
q
•
p
•
J
J–1
Figure 2.2 Direct and inverse differential kinematics.
33
Mobile Robot Kinematics

which, if solved for _q, gives:
_q 5 ðJTJÞ21JT _p 5 Jy _p
Case 2 There are less equations than unknowns ðm , nÞ, that is, _q is underspecified
and many choices of _q lead to the same _p. In this case we select _q with the mini-
mum norm, that is, we solve the constrained minimization problem:
minjj_qjj2 subject to _p 2 J_q 5 0;
_qARn
Introducing the Lagrange multiplier vector λ, we get the augmented (uncon-
strained) Lagrangian minimization problem:
min
_q;λ Lð_q; λÞ; Lð_q; λÞ 5 _qT _q 1 λTð_p 2 J_qÞ
The optimality conditions are:
@L=@_q 5 2_q 2 JTλ 5 0;
@L=@λ 5 _p 2 J_q 5 0
Solving the first for _q we get _q 5 ð1=2ÞJTλ.
Introducing this result into @L=@λ 5 0 yields:
λ 5 2ðJJTÞ21 _p
Therefore, finally we find:
_q 5 JTðJJTÞ21 _p 5 Jy _p
where
Jy 5 JTðJJTÞ21
ð2:8bÞ
under the condition that rank J 5 m (i.e., JJT invertible). Therefore, when m , n
the generalized inverse Eq. (2.8b) should be used.
Formally, the generalized inverse Jy of a m 3 n real matrix J is defined to be
the unique n 3 m real matrix that satisfies the following four conditions:
JJyJ 5 J; JyJJy 5 Jy
ðJJyÞT 5 JJy; ðJyJÞT 5 JyJ
34
Introduction to Mobile Robot Control

It follows that Jy has the properties:
ðJyÞy 5 J; ðJTÞy 5 ðJyÞT; ðJJTÞy 5 ðJyÞTJy
All the above relations are useful when dealing with overspecified or underspe-
cified linear algebraic systems (encountered, e.g., in underactuated or overactuated
mechanical systems).
2.2.2
Homogeneous Transformations
The position and orientation of a solid body (e.g., a robotic link) with respect to the
fixed world coordinate frame Oxyz (Figure 2.3) are given by a 4 3 4 transformation
matrix A, called homogeneous transformation, of the type:
A =
R
p
0
1
ð2:9Þ
where p is the position vector of the center of gravity O0(or some other fixed point
of the link) with respect to Oxyz, and R is a 3 3 3 matrix defined as:
R 5 n
^
o
^
a 

ð2:10Þ
p
Q
Rxb
Q
p
End-effector
o
x
y
z
xb
yb
zb
o′
o
a
n
(A)
z
x
y
o
n
a
(B)
o
x
y
z
xb
yb
zb
o′
o
a
n
(C)
A
B
o
xQ
p
Figure 2.3 (A) Position and orientation of a solid body, (B) position and orientation of a
robotic end-effector (a 5 approach vector, n 5 normal vector, o 5 orientation or sliding
vector), and (C) position vectors of a point Q with respect to the frames Oxyz and O0xbybzb.
35
Mobile Robot Kinematics

In Eq. (2.10), n, o and a are the unit vectors along the axes xb, yb, zb of the local
coordinate frame O0xbybzb. The matrix R represents the rotation of O0xbybzb with
respect to the reference (world) frame Oxyz. The columns n, o, and a of R are pair-
wise orthonormal, that is, nTo 5 0; oTa 5 0; aTn 5 0; jnj 5 1; joj 5 1; jaj 5 1
where bT denotes the transpose (row) vector of the column vector b, and jbj
denotes the Euclidean norm of b ðjbj 5 ½b2
x1b2
y1b2
z1=2Þ, with bx, by, and bz being
the x; y; z components of b, respectively.
Thus the rotation matrix R is orthonormal, that is:
R21 5 RT
ð2:11Þ
To work with homogeneous matrices we use 4-dimensional vectors (called
homogeneous vectors) of the type:
XQ 5
xQ
yQ
zQ
?
1
2
66664
3
77775
5
xQ
?
1
2
4
3
5;
XQ
b 5
xQ
b
yQ
b
zQ
b
?
1
2
66664
3
77775
5
xQ
b
?
1
2
4
3
5
ð2:12Þ
Suppose that XQ
b and XQ are the homogeneous position vectors of a point Q in
the coordinate frames O0xbybzb and Oxyz, respectively. Then, from Figure 2.3C we
obtain the following vectorial equation:
~
OQ 5 ~
OO0 1 ~
O0A 1 ~
AB 1 ~
BQ
where
~
OQ 5 xQ;
~
OO0 5 p;
~
O0A 5 xQ
b n;
~
AB 5 yQ
b o;
~
BQ 5 zQ
b a:
Thus:
xQ 5 p 1 xQ
b n 1 yQ
b o 1 zQ
b a 5 p 1 n
o
a


xQ
b
yQ
b
zQ
b
2
64
3
75 5 p 1 RxQ
b
ð2:13aÞ
or
XQ =
n
o
a
p
0
0
0
1 XQ
b = AXQ
b
ð2:13bÞ
where A is given by Eqs. (2.9) and (2.10). Equation (2.13b) indicates that the
homogeneous matrix A contains both the position and orientation of the local coor-
dinate frame O0xbybzb with respect to the world coordinate frame Oxyz.
36
Introduction to Mobile Robot Control

It is easy to verify that:
A21 5
RT
2RTp
0
1


ð2:14Þ
Indeed, from Eq. (2.13a) we have: xQ
b 5 2 R21p 1 R21xQ, which by Eq. (2.11)
gives:
xQ
b
1


5
RT
2RTp
0
1


xQ
1


5 A21 xQ
1


The columns n, o, and a of R consist of the direction cosines with respect to
Oxyz. Thus the rotation matrices with respect to axes x; y; z which are represented as:
nx 5
1
0
0
2
4
3
5;
oy 5
0
1
0
2
4
3
5;
az 5
0
0
1
2
4
3
5
are given by:
RxðφxÞ 5
1
0
0
0
cos φx
2sin φx
0
sin φx
cos φx
2
4
3
5
ð2:15aÞ
RyðφyÞ 5
cos φy
0
2sin φy
0
1
0
sin φy
0
cos φy
2
4
3
5
ð2:15bÞ
RzðφzÞ 5
cos φz
2sin φz
0
sin φz
cos φz
0
0
0
1
2
4
3
5
ð2:15cÞ
where φx, φy, and φz are the rotation angles with respect to x, y, and z, respectively.
In mobile robots moving on a horizontal plane, the robot is rotating only with
respect to the vertical axis z, and so Eq. (2.15c) is used. Thus, for convenience, we
drop the index z.
For better understanding, the upper left block of Eq. (2.15c) is obtained directly
using the Oxy plane geometry shown in Figure 2.4.
Let a point Pðxp; ypÞ in the coordinate frame Oxy, which is rotated about the axis
Oz by the angle φ. The coordinates of P in the frame Ox0y0 are x0
P and y0
P as shown
in Figure 2.4. From this figure we see that:
xP 5 ðOBÞ 5 ðODÞ 2 ðBDÞ 5 x0
P cos φ 2 y0
P sin φ
yP 5 ðOEÞ 5 ðEZÞ 1 ðZOÞ 5 x0
P sin φ 1 y0
P cos φ
ð2:16Þ
37
Mobile Robot Kinematics

that is:
xP
yP


5
cos φ
2sin φ
sin φ
cos φ

 x0
P
y0
P


ð2:17Þ
Similarly, one can derive the respective 2 3 2 blocks RxðφxÞ and RyðφyÞ for the
rotations about the x and y axes, respectively.
Given an open kinematic chain of n links, the homogeneous vector Xn of the
local coordinate frame Onxnynzn of the nth link, expressed in the world coordinate
frame Oxyz can be found by successive application of Eq. (2.13b), that is, as:
X0 5 A0
1A1
2?An21
n
Xn
ð2:18Þ
where Ai21
i
is the 4 3 4 homogeneous transformation matrix that leads from the
coordinate frame of link i to that of link i 2 1. The matrices Ai21
i
can be computed
by the so-called DenavitHartenberg (DH) method (Section 10.2.1). The general
relation (2.18) is of the form (2.2), and provides the robot Jacobian as indicated
in Eq. (2.5).
2.2.3
Nonholonomic Constraints
A nonholonomic constraint (relation) is defined to be a constraint that contains
time derivatives of generalized coordinates (variables) of a system and is not inte-
grable. To understand what this means we first define a holonomic constraint as
any constraint which can be expressed in the form:
Fðq; tÞ 5 0
ð2:19Þ
where q 5 ½q1; q2; . . .; qnT is the vector of generalized coordinates.
Now, suppose we have a constraint of the form:
fðq; _q; tÞ 5 0
ð2:20Þ
If this constraint can be converted to the form:
Fðq; tÞ 5 0
ð2:21Þ
ф
ф
ф
y′
x′
E
y
Z
P(xp,yp)
A
C
B
D
o
x
x′p
yp′
Figure 2.4 Direct trigonometric derivation of the
rotation matrix with respect to axis z.
38
Introduction to Mobile Robot Control

we say that it is integrable. Therefore, although f in Eq. (2.20) contains the time
derivatives _q, it can be expressed in the holonomic form Eq. (2.21), and so it is actu-
ally a holonomic constraint. More specifically we have the following definition.
Definition 2.1 (Nonholonomic constraint)
A constraint of the form (2.20) is said to be nonholonomic if it cannot be ren-
dered to the form (2.21) such that to involve only the generalized variables
themselves.
Typical systems that are subject to nonholonomic constraints (and hence are
called nonholonomic systems) are underactuated robots, WMRs, autonomous under-
water vehicles (AUVs), and unmanned aerial vehicles (UAVs). It is emphasized
that “holonomic” does not necessarily mean unconstrained. Surely, a mobile robot
with no constraint is holonomic. But a mobile robot capable of only translations is
also holonomic.
Nonholonomicity occurs in several ways. For example a robot has only a few
motors, say k , n, where n is the number of degrees of freedom, or the robot has
redundant degrees of freedom. The robot can produce at most k independent
motions. The difference n 2 k indicates the existence of nonholomicity. For exam-
ple, a differential drive WMR has two controls (the torques of the two wheel
motors), that is, k 5 2, and three degrees of freedom, that is, n 5 3. Therefore, it
has one ðn 2 k 5 1Þ nonholonomic constraint.
Definition 2.2 (Pfaffian constraints)
A nonholonomic constraint is called a Pfaffian constraint if it is linear in _q, that
is, if it can be expressed in the form:
μiðqÞ_q 5 0;
i 5 1; 2; . . .; r
where μi are linearly independent row vectors and q 5 q1; q2; . . .; qn
½
T.
In compact matrix form the above r Pfaffian constraints can be written as:
MðqÞ_q 5 0;
MðqÞ 5
μ1ðqÞ
μ2ðqÞ
^
μrðqÞ
2
664
3
775
ð2:22Þ
An example of integrable Pfaffian constraint is:
μðqÞ_q 5 q1 _q1 1 q2 _q2 1 ? 1 qn _qn; qARn
ð2:23Þ
This is integrable because it can be derived via differentiation, with respect to
time, of the equation of a sphere:
sðqÞ 5 q2
1 1 q2
2 1 ? 1 q2
n 2 a2 5 0
39
Mobile Robot Kinematics

with constant radius a. The particular resulting sphere by integrating Eq. (2.23)
depends on the initial state qðtÞt50 5 q0. The collection of all concentric spheres
with center at the origin and radius “a” is called a foliation with spherical leaves.
For example, if n 5 3 the foliation produces a maximal integral manifold ð0; 0; aÞ:
M 5 fqAR3: sðqÞ 5 q2
1 1 q2
2 1 q2
3 2 a2 5 0g
The nonholonomic constraint encountered in mobile robotics is the motion con-
straint of a disk that rolls on a plane without slipping (Figure 2.5). The no-slipping
condition does not allow the generalized velocities _x; _y, and _φ to take arbitrary
values.
Let r be the disk radius. Due to the no-slipping condition the generalized coordi-
nates are constrained by the following equations:
_x 5 r _θ cos φ;
_y 5 r _θ sin φ
ð2:24Þ
which are not integrable. These constraints express the condition that the velocity
vector of the disk center lies in the midplane of the disk. Eliminating the velocity
v 5 r _θ in Eq. (2.24) gives:
v 5 r _θ 5
_x
cos φ 5
_y
sin φ
or
_x sin φ 2 _y cos φ 5 0
ð2:25Þ
This is the nonholonomic constraint of the motion of the disk. Because of the
kinematic
constraints
(2.24),
the
disk
can
attain
any
final
configuration
ðx2; y2; φ2; θ2Þ starting from any initial configuration ðx1; y1; φ1; θ1Þ. This can be
done in two steps as follows:
Step 1: Move the contact point ðx1; y1Þ to ðx2; y2Þ by rolling the disk along a line of length
ð2kπ 1 θ2 2 θ1Þr;
k 5 0; 1; 2; . . ..
Step 2: Rotate the disk about the vertical axis from φ1 to φ2.
ф
θ
y
x
z
y axis
x axis
xy plane
o
Figure 2.5 The generalized coordinates x; y,
and φ.
40
Introduction to Mobile Robot Control

Given a kinematic constraint one has to determine whether it is integrable or
not. This can be done via the Frobenius theorem which uses the differential geome-
try concepts of distributions and Lie Brackets. We will come to this later
(Section 6.2.1).
Two other systems that are subject to nonholonomic constraints are the rolling
ball on a plane without spinning on place, and the flying airplane that cannot
instantaneously stop in the air or move backward.
2.3
Nonholonomic Mobile Robots
The kinematic models of the following nonholonomic WMRs will be derived:
G
Unicycle
G
Differential drive WMR
G
Tricycle WMR
G
Car-like WMR
2.3.1
Unicycle
Unicycle has a kinematic model which is used as a basis for many types of nonho-
lonomic WMRs. For this reason this model has attracted much theoretical attention
by WMR controlists and nonlinear systems workers.
Unicycle is a conventional wheel rolling on a horizontal plane, while keeping its
body vertical (Figure 2.5). The unicycle configuration (as seen from the bottom via
a glass floor) is shown in Figure 2.6.
Its
configuration
is
described
by
a
vector
of
generalized
coordinates:
p 5 ½xQ; yQ; φT, that is, the position coordinates of the point of contact Q with the
ground in the fixed coordinate frame Oxy, and its orientation angle φ with respect
to the x axis. The linear velocity of the wheel is vQ and its angular velocity about
its instantaneous rotational axis is vφ 5 _φ. From Figure 2.6, we find:
_xQ 5 vQ cos φ;
_yQ 5 vQ sin φ;
_φ 5 vφ
ð2:26Þ
Eliminating vQ from the first two equations (2.26) we find the nonholonomic
constraint (2.25):
2 _xQ sin φ 1 _yQ cos φ 5 0
ð2:27Þ
ф
y
yQ
x
xQ
o
vQ
Q
Figure 2.6 Kinematic structure of a unicycle.
41
Mobile Robot Kinematics

Using the notation v1 5 vQ and v2 5 v φ, for simplicity, the kinematic model
(2.26) of the unicycle can be written as:
_p 5
cos φ
sin φ
0
2
4
3
5v1 1
0
0
1
2
4
3
5v2;
_p 5
_xQ
_yQ
_φ
2
4
3
5
ð2:28aÞ
or
_p 5 J_q;
_q 5 ½v1; v2T
ð2:28bÞ
where J is the system Jacobian matrix:
J 5
cos φ
0
sin φ
0
0
1
2
4
3
5
ð2:28cÞ
The linear velocity v1 5 vQ and the angular velocity vφ 5 v2 are assumed to be
the action (joint) variables of the system.
The model (2.28a) belongs to the special class of nonlinear systems, called
affine systems, and described by a dynamic equation of the form (Chapter 6):
_x 5 g0ðxÞ 1
X
m
i51
giðxÞui
ð2:29aÞ
5 g0ðxÞ 1 GðxÞu
ð2:29bÞ
where uiði 5 1; 2; . . .; mÞ appear linearly, and:
x 5 ½x1; x2; . . .; xnTAX;
u 5 ½u1; u2; . . .; umTAU
GðxÞ 5 ½g1ðxÞ^g2ðxÞ^?^gmðxÞ
ð2:30Þ
If m , n the system has a less number of actuation variables (controls) than the
degrees of freedom under control and is known as underactuated system. If m . n
we have an overactuated system. In practice, usually m , n. The vector x is actu-
ally the state vector of the system and u the control vector. The term g0ðxÞ is called
“drift,” and the system with g0ðxÞ 5 0 is called a “driftless” system. The column
vector set:
g1ðxÞ 5
g11ðxÞ
g12ðxÞ
^
g1nðxÞ
2
664
3
775;
g2ðxÞ 5
g21ðxÞ
g22ðxÞ
^
g2nðxÞ
2
664
3
775; . . .;
gmðxÞ 5
gm1ðxÞ
gm2ðxÞ
^
gmnðxÞ
2
664
3
775
ð2:31Þ
42
Introduction to Mobile Robot Control

is referred to as the system’s vector field. It is assumed that the set U contains at
least an open set that involves the origin of Rm. If U does not contain the origin,
then the system is not “driftless.”
The unicycle model (2.28a) is a 2-input driftless affine system with two vector
fields:
g1 5
cos φ
sin φ
0
2
4
3
5;
g2 5
0
0
1
2
4
3
5
ð2:32Þ
The Jacobian formulation (2.28c) organizes the two column vector fields into a
matrix J 5 G. Each action variable uiAR in Eq. (2.29a) is actually a coefficient
that determines how much of giðxÞ is contributing into the result x. The vector field
g1ðφÞ of the unicycle allows pure translation, and the field g2 allows pure rotation.
2.3.2
Differential Drive WMR
Indoor and other mobile robots use the differential drive locomotion type
(Figure 1.20). The Pioneer WMR shown in Figure 1.11 is an example of differen-
tial drive WMR. The geometry and kinematic parameters of this robot are shown
in Figure 2.7. The pose (position/orientation) vector of the WMR and its speed are
respectively:
p 5
xQ
yQ
φ
2
4
3
5;
_p 5
_xQ
_yQ
_φ
2
4
3
5
ð2:33Þ
The angular positions and speeds of the left and right wheels are fθl; _θ lg; fθr; _θ rg,
respectively.
The following assumptions are made:
G
Wheels are rolling without slippage
G
The guidance (steering) axis is perpendicular to the plane Oxy
G
The point Q coincides with the center of gravity G, that is, jj ~
GQjj 5 0.2
Let vl and vr be the linear velocity of the left and right wheel respectively, and
vQthe velocity of the wheel midpoint Q of the WMR. Then, from Figure 2.7A we get:
vr 5 vQ 1 a_φ;
vl 5 vQ 2 a_φ
ð2:34aÞ
Adding and subtracting vr and vl we get
vQ 5 1
2 ðvr 1 vlÞ;
2a_φ 5 vr 2 vl
ð2:34bÞ
2 In Figure 2.7, the points Q and G are shown distinct in order to use the same figure in all configura-
tions with Q and G separated by distance b.
43
Mobile Robot Kinematics

where, due to the nonslippage assumption, we have vr 5 r _θ r and vl 5 r _θ l. As in the
unicycle case _xQ and _yQ are given by:
_xQ 5 vQ cos φ;
_yQ 5 vQ sin φ
ð2:35Þ
and so the kinematic model of this WMR is described by the following relations:
_xQ 5 r
2 ð_θ r cos φ 1 _θ l cos φÞ
ð2:36aÞ
_yQ 5 r
2 ð_θ r sin φ 1 _θ l sin φÞ
ð2:36bÞ
_φ 5 r
2a ð_θ r 2 _θ lÞ
ð2:36cÞ
Analogously to Eq. (2.28a,b) the kinematic model (2.36ac) can be written in
the driftless affine form:
_p 5
ðr=2Þcos φ
ðr=2Þsin φ
r=2a
2
4
3
5_θ r 1
ðr=2Þcos φ
ðr=2Þsin φ
2r=2a
2
4
3
5_θ l
ð2:37aÞ
ф
θr
ф
Q
b
2α
G
y
y
x
x
o
xQ
Passive castor
wheel
Center of 
gravity
(A)
yQ
x
xr
vQ
y
yr
ф
ф
(B)
o
•
•
θl
νl
νQ
νr
•
(yQ)1
yQ
•
•
(xQ)1
•
xQ
•
Figure 2.7 (A) Geometry of
differential drive WMR,
(B) Diagram illustrating the
nonholonomic constraint.
44
Introduction to Mobile Robot Control

or
_p 5 J_q
ð2:37bÞ
where
_p 5
_xQ
_yQ
_φ
2
4
3
5;
_q 5
_θ r_θ l


ð2:37cÞ
and J is the WMR’s Jacobian:
J 5
ðr=2Þcos φ
ðr=2Þcos φ
ðr=2Þsin φ
ðr=2Þsin φ
r=2a
2r=2a
2
4
3
5
ð2:37dÞ
Here, the two 3-dimensional vector fields are:
g1 5
ðr=2Þcos φ
ðr=2Þsin φ
r=2a
2
4
3
5;
g2 5
ðr=2Þcos φ
ðr=2Þsin φ
2r=2a
2
4
3
5
ð2:38Þ
The field g1 allows the rotation of the right wheel, and g2 allows the rotation of
the left wheel. Eliminating vQ in Eq. (2.35) we get as usual the nonholonomic con-
straint (2.25) or (2.27).
2 _xQsin φ 1 _yQcos φ 5 0
ð2:39Þ
which expresses the fact that the point Q is moving along Qxr, and its velocity
along the axis Qyr is zero (no lateral motion), that is (Figure 2.7B):
2ð_xQÞ1 1 ð_yQÞ1 5 0
where ð_xQÞ1 5 _xQ sin φ and ð_yQÞ1 5 _yQ cos φ.
The Jacobian matrix J in Eq. (2.37d) has three rows and two columns, and so it
is not invertible. Therefore, the solution of Eq. (2.37b) for _q is given by:
_q 5 Jy _p
ð2:40Þ
where Jy is the generalized inverse of J given by Eq. (2.8a). However, here Jy can
be computed directly by using Eq. (2.34a), and observing from Figure 2.7B that:
vQ 5 _xQ cos φ 1 _yQ sin φ
45
Mobile Robot Kinematics

Thus, using this equation in Eq. (2.34a) we obtain:
r _θ r5 _xQ cos φ 1 _yQ sin φ 1 a_φ
r _θ l5 _xQ cos φ 1 _yQ sin φ 2 a_φ
ð2:41aÞ
that is:
_θ r_θ l


5 1
r
cos φ
sin φ
a
cos φ
sin φ
2a


_xQ
_yQ
_φ
2
4
3
5
or
_q 5 Jy _p
ð2:41bÞ
where3 :
Jy 5 1
r
cos φ
sin φ
a
cos φ
sin φ
2a


ð2:41cÞ
The nonholonomic constraint (2.39) can be written as:
M_p 5 0;
M 5
2sin φ
cos φ
0 

ð2:42Þ
Clearly, if _θ r 6¼ _θ l, then the difference between _θ r and _θ l determines the robot’s
rotation speed _φ and its direction. The instantaneous curvature radius R is given by
(Eq. 1.1):
R 5 vQ
_φ 5 a vr 1 vl
vr 2 vl


;
vr $ vl
ð2:43aÞ
and the instantaneous curvature coefficient is:
κ 5 1=R
ð2:43bÞ
Example 2.1
Derive the kinematic relations (2.35) using the rotation matrix concept (2.17).
Solution
Here, the point Pðxp; ypÞ of Figure 2.4 is the point QðxQ; yQÞ in Figure 2.7. The WMR veloci-
ties along the local coordinate axes Qxr and Qyr are _xr and _yr. The corresponding velocities
in the world coordinate frame are _xQ and _yQ. Therefore, for a given φ, (2.17) gives:
_xQ
_yQ


5
cos φ
2sin φ
sin φ
cos φ


_xr
_yr


ð2:44Þ
3 As an exercise, the reader is advised to derive Eq. (2.41c) using Eq. (2.8a).
46
Introduction to Mobile Robot Control

Now, the condition of no lateral wheel movement implies that
_yr 5 0 cos φ
2sin φ
sin φ
cos φ
and _xr 5 vQ. Therefore, the above relation gives:
_xQ 5 vQ cos φ and _yQ 5 vQ sin φ
as desired.
Example 2.2
Derive the kinematic equations and constraints of a differential drive WMR by relaxing the
no-slipping condition of the wheels’ motion.
Solution
We will work with the WMR of Figure 2.7. Considering the rotation about the center of
gravity G we get the following relations:
_xG 5 _xQ 1 b_φ sin φ
_yG 5 _yQ 2 b_φ cos φ
Therefore, the kinematic equations (2.41a) and the nonholonomic constraint (2.42)
become:
r _θ r 5 _xG cos φ 1 _yG sin φ 1 a_φ
r _θ l 5 _xG cos φ 1 _yG sin φ 2 a_φ
2 _xG sin φ 1 _yG cos φ 1 b_φ 5 0
Now, assume that the wheels are subject to longitudinal and lateral slip [10]. To
include the slip into the kinematics of the robot, we introduce two variables wr, wl for the
longitudinal slip displacements of the right wheel and left wheel, respectively, and two
variables zr, zl for the corresponding lateral slip displacements. Thus, here:
p 5 ½xG; yG; φ^wr; wl; zr; zlT
The slipping wheels’ velocities are now given by:
vr 5 ðr _θ r 2 _wrÞcos ζr;
vl 5 ðr _θ l 2 _wlÞcos ζl
where ζr and ζl are the steering angles of the wheels.
47
Mobile Robot Kinematics

Using these relations for vr and vl the above kinematic equations are written as:
vr 5 ðr _θ r 2 _wrÞcos ζr 5 _xG cos φ 1 _yG sin φ 1 a_φ
vl 5 ðr _θ l 2 _wlÞcos ζl 5 _xG cos φ 1 _yG sin φ 2 a_φ
and the nonholonomic constraint becomes:
2 _xG sin φ 1 _yG cos φ 1 b_φ 2 _zr cos ζr 5 0
2 _xG sin φ 1 _yG cos φ 1 b_φ 2 _zl cos ζl 5 0
In our WMR the two wheels have a common axis and are unsteered. Therefore,
ζr 5 ζl 5 0. For WMRs with steered wheels we may have ζr 6¼ 0, ζl 6¼ 0. In our
case cos ζr 5 cos ζl 5 1, and so the two kinematic equations, solved for the angular
wheel velocities _θ r and _θ l, give:
_q 5 Jy _p;
_q 5
_θ r_θ l


where the inverse Jacobian is:
Jy 5 1
r
cos φ
sin φ
a
1
0
0
0
cos φ
sin φ
2a
0
1
0
0


The nonholonomic constraints are written in Pfaffian form:
MðpÞ_p 5 0
where
MðpÞ 5
2sin φ
cos φ
b
0
0
21
0
2sin φ
cos φ
b
0
0
0
21


_p 5 ð _xG;
_yG;
_φ
^
_wr;
_wl
_zr
_zl ÞT
In the special case where only lateral slip takes place (i.e., _wr 5 0, _wl 5 0), the
components _wr and _wl are dropped from _p, and the matrices Jy and MðpÞ are reduced
appropriately, having only five columns. Note that here the wheels are fixed and so
_zr 5 _zl 5 _yr where _yr is the lateral slipping velocity of the body of the WMR.
Typically, the slipping variables, which are unknown and nonmeasurable are treated
as disturbances via disturbance rejection and robust control techniques.
48
Introduction to Mobile Robot Control

2.3.3
Tricycle
The motion of this WMR is controlled by the wheel steering angular velocity ωψ
and its linear velocity vw (or its angular velocity ωw 5 _θ w 5 vw=r, where r is the
radius of the wheel) (Figure 2.8).
The orientation angle and angular velocity are φ and _φ, respectively. It is
assumed that the vehicle has its guidance point Q in the back of the powered wheel
(i.e., it has a central back axis). The state of the robot’s motion is:
p 5 xQ;
yQ;
φ;
ψ T

The kinematic variables are:
Steering wheel velocity: vw 5 r _θ w.
Vehicle velocity: v 5 vwcos ψ 5 rðcos ψÞ_θ w
Vehicle orientation velocity: _φ 5 ð1=DÞvwsin ψ
Steering angle velocity: _ψ 5 ωψ
Using the above relations we find:
_p 5
_xQ
_yQ
_φ
_ψ
2
664
3
775 5
v cos φ
v sin φ
_φ
_ψ
2
664
3
775 5
r cos ψ cos φ
r cos ψ sin φ
ðr=DÞsin ψ
0
2
664
3
775_θ w 1
0
0
0
1
2
664
3
775 _ψ 5 J _θ
ð2:45Þ
where _θ 5 ½_θ w; _ψT is the vector of joint velocities (control variables), and
J 5
r cos ψ cos φ
0
r cos ψ sin φ
0
ðr=DÞsin ψ
0
0
1
2
664
3
775
ð2:46Þ
x
D
y
o
yQ
Q
a
a
ф
xQ
R
ψ
ψ
v
vw
r
ICC
ωw
Figure 2.8 Geometry of the tricycle WMR
(ψ is the steering angle).
49
Mobile Robot Kinematics

is the Jacobian matrix. This Jacobian is again noninvertible, but we can find the
inverse kinematic equations directly using the relations:
_φ=v 5 ð1=DÞtgψ or ψ 5 arctgðD_φ=vÞ
ð2:47aÞ
and
_θ w 5 vw
r 5 1
r
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
v2 1 ðD _φÞ2
q
ð2:47bÞ
The instantaneous curvature radius R is given by (Figure 2.8):
R 5 Dtgðπ=2 2 ψðtÞÞ
ð2:48Þ
From Eq. (2.45) we see that the tricycle is again a 2-input driftless affine system
with vector fields:
g1 5
r cos ψ cos φ
r cos ψ sin φ
ðr=DÞsin ψ
0
2
664
3
775;
g2 5
0
0
0
1
2
664
3
775
that allow steering wheel motion _θ w, and steering angle motion _ψ, respectively.
2.3.4
Car-Like WMR
The geometry of the car-like mobile robot is shown in Figure 2.9A and the A.W.E.
S.O.M.-9000
line-tracking
car-like
robot
prototype
(Aalborg
University)
in
Figure 2.9B.
D
P
ψ
vw
v1
xQ
xP
y
(A)
(B)
yP
yQ
o
ζ
x
ф
Q
Figure 2.9 (A) Kinematic structure of a car-like robot, (B) A car-like robot prototype.
Source: http://sqrt-1.dk/robot/robot.php
50
Introduction to Mobile Robot Control

The state of the robot’s motion is represented by the vector [20]:
p 5 ½xQ; yQ; φ; ψT
ð2:49Þ
where xQ, yQ are the Cartesian coordinates of the wheel axis midpoint Q, φ is the
orientation angle of the vehicle, and ψ is the steering angle. Here, we have two
nonholonomic constraints, one for each wheel pair, that is:
2 _xQ sin φ 1 _yQ cos φ 5 0
ð2:50aÞ
2 _xpsinðφ 1 ψÞ 1 _ypcosðφ 1 ψÞ 5 0
ð2:50bÞ
where xp and yp are the position coordinates of the front wheels midpoint P. From
Figure 2.9 we get:
xp 5 xQ 1 D cos φ;
yp 5 yQ 1 D sin φ
Using these relations the second kinematic constraint (2.50b) becomes:
2 _xQsinðφ 1 ψÞ 1 _yQcosðφ 1 ψÞ 1 Dðcos ψÞ_φ
The two nonholonomic constraints are written in the matrix form:
MðpÞ_p 5 0
ð2:51aÞ
where
MðpÞ 5
2sin φ
cos φ
0
0
2sinðφ 1 ψÞ
cosðφ 1 ψÞ
D cos ψ
0


ð2:51bÞ
The kinematic equations for a rear-wheel driving car are found to be
(Figure 2.9):
_xQ 5 v1cos φ
_yQ 5 v1sin φ
_φ 5 1
D vwsin ψ
5 1
D v1tg ψ
_ψ 5 v2
ð2:52Þ
51
Mobile Robot Kinematics

These equations can be written in the affine form:
_xQ
_yQ
_φ
_ψ
2
664
3
775 5
cos φ
sin φ
ð1=DÞtgψ
0
2
664
3
775v1 1
0
0
0
1
2
664
3
775v2
ð2:53Þ
that has the vector fields:
g1 5
cos φ
sin φ
ð1=DÞtgψ
0
2
664
3
775;
g2 5
0
0
0
1
2
664
3
775
allowing the driving motion v1 and the steering motion v2 5 _ψ, respectively. The
Jacobian form of Eq. (2.53) is:
_p 5 Jv;
v 5 ½v1; v2T
ð2:54Þ
with Jacobian matrix:
J 5
cos φ
0
sin φ
0
ðtgψÞ=D
0
0
1
2
664
3
775
ð2:55Þ
Here, there is a singularity at ψ 5 6 π=2, which corresponds to the “jamming”
of the WMR when the front wheels are normal to the longitudinal axis of its body.
Actually, this singularity does not occur in practice due to the restricted range of
the steering angle ψ ð2 π=2 , ψ , π=2Þ.
The kinematic model for the front wheel driving vehicle is (Eqs. 2.45 and 2.46)
[20]:
_p 5 Jv;
J 5
cos φ cos ψ
0
sin φ cos ψ
0
ðsin ψÞ=D
0
0
1
2
664
3
775
ð2:56aÞ
In this case the previous singularity does not occur, since at ψ 56 π=2 the car
can still (in principle) pivot about its rear wheels. Using the new inputs u1 and u2
defined as:
u1 5 v1; u2 5 ð1=DÞsinðζ 2 φÞv1 1 v2
52
Introduction to Mobile Robot Control

the above model is transformed to:
_xp
_yp
_φ
_ζ
2
664
3
775 5
cos ζ
sin ζ
ð1=DÞsinðζ 2 φÞ
0
2
664
3
775u1 1
0
0
0
1
2
664
3
775u2
ð2:56bÞ
where ζ 5 φ 1 ψ is the total steering angle with respect to the axis Ox.
Indeed,
from
xp 5 xQ 1 D cos φ
and
yp 5 yQ 1 D sin φ
(Figure
2.9),
and
Eq. (2.56a) we get:
_xp 5 _xQ 2 Dðsin φÞ_φ 5 ðcos φ cos ψ 2 sin φ sin ψÞv1
5 ½cosðφ 1 ψÞv1 5 ðcos ζÞu1
_yp 5 _yQ 1 Dðcos φÞ_φ 5 ðsin φ cos ψ 1 cos φ sin ψÞv1
5 sinðφ 1 ψÞ
½
v1 5 ðsin ζÞu1
_φ 5 1
D sinðζ 2 φÞ
½
v1 5 1
D sinðζ 2 φÞ


u1
_ζ 5 _φ 1 _ψ 5 1
D sinðζ 2 φÞ


v1 1 v2 5 u2
We observe, from Eq. (2.56b), that the kinematic model for xp, yp, and ζ (i.e.,
the first, second, and fourth equation in Eq. (2.56b)) is actually a unicycle model
(2.28a).
Two special cases of the above car-like model are known as:
G
Reeds-Shepp car
G
Dubins car
The Reeds-Shepp car is obtained by restricting the values of the velocity v1 to
three distinct values 11, 0, and 21. These values appear to correspond to three
distinct “gears”: “forward,” “park,” or “reverse.” The Dubins car is obtained when
the reverse motion is not allowed in the Reeds-Shepp car, that is, the value
v1 5 2 1 is excluded, in which case v1 5 f0; 1g.
Example 2.3
It is desired to find the steering angle ψ which is required for a rear-wheel driven car-like
WMR to go from its present position QðxQ; yQÞ to a given goal Fðxf; yf Þ. The available data,
which are obtained via proper sensors, are the distance L between ðxf ; yfÞ and ðxQ; yQÞ and
the angle ε of the vector ~
QF with respect to the current vehicle orientation.
53
Mobile Robot Kinematics

Solution
We will work with the geometry of Figure 2.10 [19]. The kinematic equations of the WMR
are given by Eq. (2.52). The WMR will go from the position Q to the goal F following a cir-
cular path with curvature:
1
R1
5 1
D tgψ
determined using the bicycle equivalent model, that combines the two front wheels and
the two rear wheels (Figure 2.10, left).
On the other hand, the curvature 1=R2 of the circular path that passes through the
goal, is obtained from the relation (Figure 2.10, right):
L=2 5 R2sinðεÞ
that is:
1
R2
5 2
L sinðεÞ
ð2:57aÞ
To meet the goal tracking requirement the above two curvatures 1=R1 and 1=R2 must be
the same, that is:
1
D tgψ 5 2
L sinðεÞ
Therefore:
ψ 5 tg21 2D
L sinðεÞ


ð2:57bÞ
ф
D
v1
ψ
ε
d
ε
ε
R2
R2
Trajectory of
the goal 
ψ
D
R1
ψ
Goal F(xf,yf)
Q(xQ,yQ)
Circular path
L/2
ICR
L
ICR
Figure 2.10 Geometry of the goal tracking problem.
54
Introduction to Mobile Robot Control

Equation (2.57b) gives the steering angle ψ in terms of the data L and ε, and can be
used to pursuit tracking of goals (targets) that are moving along given trajectories. In
these cases the goal F lies at the intersection of the goal trajectory and the look-ahead
circle. To get a better interpretation of (2.57a), we use the lateral distance d between the
vehicles orientation (heading) vector and the goal point, which is given by (Figure 2.10):
d 5 L sinðεÞ
Then, the curvature 1=R2 in Eq. (2.57a) is given by:
1
R2
5 2d
L2
This indicates that the curvature 1=R1 of the path resulting from the steering angle ψ
should be:
1
R1
5
2
L2


d
ð2:57cÞ
Equation (2.57c) is a “proportional control law” and shows that the curvature 1=R1 of
the robot’s path should be proportional to the cross track error d some look-ahead dis-
tance in front of the WMR with a gain 2=L2.
2.3.5
Chain and Brockett—Integrator Models
The general 2-input n-dimensional chain model (briefly ð2; nÞ-chain model) is:
_x1 5 u1
_x2 5 u2
_x3 5 x2u1
^
_xn 5 xn21u1
ð2:58Þ
The Brockett (single) integrator model is:
_x1 5 u1
_x2 5 u2
_x3 5 x1u2 2 x2u1
ð2:59Þ
and the double integrator model is:
_x1 5 u1
_x2 5 u2
_x3 5 x1 _x2 2 x2 _x1
ð2:60Þ
55
Mobile Robot Kinematics

The nonholonomic WMR kinematic models can be transformed to the above
models. Here, the unicycle model (which also covers the differential drive model)
and the car-like model will be considered.
2.3.5.1
Unicycle WMR
The unicycle kinematic model is given by Eq. (2.26):
_xQ 5 vQ cos φ;
_yQ 5 vQ sin φ;
_φ 5 vφ
ð2:61Þ
Using the transformation:
z1 5 φ
z2 5 xQ cos φ 1 yQ sin φ
z3 5 xQ sin φ 2 yQ cos φ
ð2:62Þ
the unicycle model is converted to the (2,3)-chain form:
_z1 5 u1
_z2 5 u2
_z3 5 z2u1
ð2:63Þ
where u1 5 vφ and u2 5 vQ 2 z3u1.
Defining new state variables:
x1 5 z1;
x2 5 z2;
x3 5 2 2z3 1 z1z2
ð2:64Þ
the (2,3)-chain model is converted to the Brockett integrator:
_x1 5 u1
_x2 5 u2
_x3 5 x1u2 2 x2u1
ð2:65Þ
2.3.5.2
Rear-Wheel Driving Car
The rear-wheel driven car model is given by Eq. (2.52):
_xQ 5 v1 cos φ;
_yQ 5 v1 sin φ;
_φ 5 v1
D tgψ;
_ψ 5 v2
ð2:66Þ
Using the state transformation:
x1 5 xQ;
x2 5
tgψ
D cos3φ ;
x3 5 tgφ;
x4 5 yQ
ð2:67Þ
56
Introduction to Mobile Robot Control

and input transformation:
v1 5 u1=cos φ
ð2:68Þ
v2 5 2 3 sin2ψ sin φ
D cos2φ
u1 1 D cos2ψðcos3φÞu2
for φ 6¼ π=2 6 kπ and ψ 6¼ π=2 6 kπ, the model (2.66) is converted to the
(2,4)-chain form:
_x1 5 u1
_x2 5 u2
_x3 5 x2u1
_x4 5 x3u1
ð2:69Þ
2.3.6
Car-Pulling Trailer WMR
This is an extension of the car-like WMR, where N one-axis trailers are attached to
a car-like robot with rear-wheel drive. This type of trailer is used, for example, at
airports for transporting luggage. The form of equations depend crucially on the
exact point at which the trailer is attached and on the choice of body frames. Here,
for simplicity each trailer will be assumed to be connected to the axle midpoint of
the previous trailer (zero hooking) as shown in Figure 2.11 [20].
The new parameter introduced here is the distance from the center of the
back axle of trailer i to the point at which is hitched to the next body. This is called
the hitch (or hinge-to-hinge) length denoted by Li. The car length is D. Let φi be
D
ψ
ф1
ф0
ф2
фN
L1
L2
Q (xQ,yQ)
Figure 2.11 Geometrical structure of the N-trailer WMR.
57
Mobile Robot Kinematics

the orientation of the ith trailer, expressed with respect to the world coordinate
frame. Then from the geometry of Figure 2.11 we get the following equations:
xi 5 xQ 2
X
i
j51
Lj cos φj
i 5 1; 2; . . .; N
yi 5 yQ 2
X
i
j51
Lj sin φj
which give the following nonholonomic constraints:
_xQ sin φ0 2 _yQ cos φ0 5 0
_xQ sinðφ0 1 ψÞ 2 _yQ cosðφ0 1 ψÞ 2 _φ0D cos ψ 5 0
_xQ sin φi 2 _yQ cos φi 1
X
i
j51
_φjLj cosðφi 2 φjÞ 5 0
for i 5 1; 2; . . .; N.
In analogy to Eq. (2.52) the kinematic equations of the N-trailer are found to be:
_xQ 5 v1 cos φ0
_yQ 5 v1 sin φ0
_φ0 5 ð1=DÞv1tgψ
_ψ 5 v2
_φ1 5 1
L1
sinðφ0 2 φ1Þ
_φ2 5 1
L2
cosðφ0 2 φ1Þsinðφ1 2 φ2Þ
^
_φi 5 1
Li
L
i21
j51
cosðφj21 2 φjÞsinðφi21 2 φiÞ
^
_φN 5 1
LN
L
N21
j51
cosðφj21 2 φjÞsinðφN21 2 φNÞ
ð2:70Þ
which, obviously, represent a driftless affine system with two inputs u1 5 v1 and
u2 5 v2 and N 1 4, states:
_x 5 g1ðxÞu1 1 g2ðxÞu2
We observe that the first four lines of the fields g1 and g2 represent the (pow-
ered) car-like WMR itself.
58
Introduction to Mobile Robot Control

2.4
Omnidirectional WMR Kinematic Modeling
The following WMRs will be considered [2,4,11,12,16]:
G
Multiwheel omnidirectional WMR with orthogonal (universal) wheels
G
Four-wheel omnidirectional WMR with mecanum wheels that have a roller angle 6 45.
2.4.1
Universal Multiwheel Omnidirectional WMR
The geometric structure of a multiwheel omnirobot is shown in Figure 2.12A. Each
wheel has three velocity components [16]:
G
Its own velocity vi 5 r _θ i, where r is the common wheel radius and _θ i its own angular
velocity
G
An induced velocity vi;roller which is due to the free rollers (here assumed of the universal
type; roller angle 6 90)
G
A velocity component vφ which is due to the rotation of the robotic platform about its
center of gravity Q, that is, vφ 5 D_φ, where _φ is the angular velocity of the platform and
D is the distance of the wheel from Q.
Here, the roller angle is 6 90, and so:
v2
h 5 v2
i 1 v2
i;roller
ð2:71aÞ
δ
β
γ ф
vh
vi
vi, roller
Reference axis Ox
o
x
y
xr
yr
Q
D
Rollers
(α=90°)
(A)
(B)
Figure 2.12 (A) Velocity vector of wheel i. The velocity vh is the robot vehicle velocity due
to the wheel motion, (B) An example of a 3-wheel setup.
Source: http://deviceguru.com/files/rovio-3.jpg.
59
Mobile Robot Kinematics

where
vi 5 vh cosðδÞ
5 vh cosðγ 2 βÞ
5 vhðcos γ cos β 1 sin γ sin βÞ
ð2:71bÞ
Thus the total velocity of the wheel i is:
vi 5 vhðcos γ cos β 1 sin γ sin βÞ 1 D_φ
5 vhxcos β 1 vhy sin β 1 D_φ
ð2:72Þ
where vhx and vhy are the x; y components of vh, that is:
vhx 5 vhcos γ;
vhy 5 vhsin γ
Equation (2.72) is general and can be used in WMRs with any number of
wheels.
Thus, for example, in the case of a 3-wheel robot we may choose the angle β for
the wheels 1, 2, and 3 as 0, 120, and 240, respectively, and get the equations:
v1 5 vhx 1 D_φ;
v2 5 2 1
2 vhx 1
ﬃﬃﬃ
3
p
2 vhy 1 D_φ;
v3 5 2 1
2 vhx 2
ﬃﬃﬃ
3
p
2 vhy 1 D_φ
ð2:73Þ
with vi 5 r _θ i. Now, defining the vectors:
_ph 5 vhx;vhy; _φ T; _q 5
_θ 1; _θ 2; _θ 3 T


we can write Eq. (2.73) in the inverse Jacobian form:
_q 5 J21 _ph
ð2:74aÞ
where
J21 5 1
r
1
0
D
21=2
ﬃﬃﬃ
3
p
=2
D
21=2
2
ﬃﬃﬃ
3
p
=2
D
2
4
3
5
ð2:74bÞ
Here det J 6¼ 0, and Eq. (2.74a) can be inverted to give _ph 5 J_q.
It is remarked that using omniwheels at different angles we can obtain an overall
velocity of the WMR’s platform which is greater than the maximum angular
60
Introduction to Mobile Robot Control

velocity of each wheel. For example, selecting in the above 3-wheel case β 5 60
and γ 5 90 we get from Eq. (2.71b):
vi 5
ﬃﬃﬃ
3
p
2 vh; i:e:; vh 5 2ﬃﬃﬃ
3
p vi . vi
The ratio vh=vi is called the velocity augmentation factor (VAF) [16]:
VAF 5 vh=vi
and depends on the number of wheels used and their angular positions on the
robot’s body. As a further example, consider a 4-wheel robot with β 5 45 and
γ 5 90. Then, Eq. (2.71b) gives:
VAF 5
ﬃﬃﬃ
2
p
Example 2.4
We are given the 4-universal-wheel omnidirectional robot of Figure 2.13, where the angles
of the wheels with respect to the axis Qxr of the vehicle’s coordinate frame are
βi ði 5 1; 2; 3; 4Þ. Derive the kinematic equations of the robot in terms of the unit direc-
tional vectors ui ði 5 1; 2; 3; 4Þ of the wheel velocities, with respect to the local coordinate
frame Qxryr.
Solution
Let _φ be the robot’s angular velocity, and vQ its linear velocity with world-frame coordi-
nates _xQ and _yQ.
The unit directional vectors of the wheel velocities are:
u1 5
2sin β1
cos β1


; u2 5
2sin β2
cos β2


; u3 5
2sin β3
cos β3


; u4 5
0
1


where it was assumed that the axis of wheel 4 coincides with axis Qxr.
ф
vQ
Reference
axis Ox
xr
yr
Q
D
β1
β2
β3
u2
u3
u1
u4
y
x
o
Figure 2.13 Four-wheel
omnidirectional robot.
61
Mobile Robot Kinematics

The relation between _xQ, _yQ and _xr, _yr is given by the rotational matrix RðφÞ
(Eq. 2.17), that is:
_xQ
_yQ


5
cos φ
2sin φ
sin φ
cos φ


_xr
_yr


5 RðφÞ
_xr
_yr


or
_xr
_yr


5
cos φ
sin φ
2sin φ
cos φ


_xQ
_yQ


5 R21ðφÞ
_xQ
_yQ


Now, we have:
v1 5 r _θ 1 5 uT
1
_xr
_yr


1 D_φ 5 uT
1R21ðφÞ
_xQ
_yQ
"
#
1 D_φ
v2 5 r _θ 2 5 uT
2
_xr
_yr


1 D_φ 5 uT
2R21ðφÞ
_xQ
_yQ
"
#
1 D_φ
v3 5 r _θ 3 5 uT
3
_xr
_yr


1 D_φ 5 uT
3R21ðφÞ
_xQ
_yQ
"
#
1 D_φ
v4 5 r _θ 4 5 uT
4
_xr
_yr


1 D_φ 5 uT
4R21ðφÞ
_xQ
_yQ
"
#
1 D_φ
or, in compact, form:
_q 5 J21 _pQ
ð2:75aÞ
where
_q 5
_θ 1
_θ 2
_θ 3
_θ 4
2
664
3
775;
_pQ 5
_xQ
_yQ
_φ
2
4
3
5
ð2:75bÞ
J21 5 1
r ðUTR21ðφÞ 1 DÞ
ð2:75cÞ
with:
U 5
u1
^
u2
^
u3
^
u4 ;
D 5 D;
D;
D;
D

T
h
ð2:75dÞ
As usual, this inverse Jacobian equation gives the required angular wheel speeds
_θ iði 5 1; 2; 3; 4Þ that lead to the desired linear velocity ½_xQ; _yQ, and angular velocity
_φ of the robot. A discussion of the modeling and control problem of a WMR with
this structure is provided in Ref. [17].
62
Introduction to Mobile Robot Control

2.4.2
FourWheel Omnidirectional WMR with Mecanum Wheels
Consider the 4-wheel WMR of Figure 2.14, where the mecanum wheels have roller
angle 6 45[2,4].
Here, we have four-wheel coordinate frames Ociði 5 1; 2; 3; 4Þ. The angular
velocity _qi of the wheel i has three components:
1. _θ ix: rotation speed around the hub
2. _θ ir: rotation speed of the roller i
3. _θ iz: rotation speed of the wheel around the contact point.
The wheel velocity vector vci 5
_xci;
_yci;
_φci T

in Oci coordinates is given by:
_xci
_yci
_φci
2
4
3
5 5
0
ri sinαi
0
Ri
2ri cosαi
0
0
0
1
2
4
3
5
_θ ix
_θ ir
_θ iz
2
4
3
5
ð2:76Þ
for i 5 1; 2; 3; 4, where Ri is the wheel radius, ri is the roller radius, and ai the roller
angle. The robot velocity vector _pQ 5 ½_xQ; _yQ; _φQT in the OxQyQ coordinate frame
(Eqs. 2.92.13) is:
_pQ 5
_xQ
_yQ
_φQ
2
4
3
5 5
cos φQ
ci
2sin φQ
ci
dQ
ciy
sin φQ
ci
cos φQ
ci
2dQ
cix
0
0
1
2
4
3
5
_xci
_yci
_φci
2
4
3
5
ð2:77Þ
where φQ
ci denotes the rotation angle (orientation) of the frame Oci with respect to
QxQyQ, and dQ
cix, dQ
ciy are the translations of Oci with respect to QxQyQ. Introducing
Eq. (2.76) into Eq. (2.77) we get:
_pQ 5 Ji _qi
ði 5 1; 2; 3; 4Þ
ð2:78Þ
Oc1
Oc2
Oc3
Oc4
α2=45°
α4=45°
α1=–45°
α3=–45°
yc1
yc2
yc3
yc4
xc1
xc2
xc3
xc4
d1
d2
yQ
Q xQ
(B)
(A)
Figure 2.14 Four-mecanum-wheel WMR (A) Kinematic geometry (B) A real 4-mecanum-
wheel WMR.
Source: http://www.automotto.com/entry/airtrax-wheels-go-in-any-direction.
63
Mobile Robot Kinematics

where _qi 5 _θ ix; _θ ir; _θ iz

T, and
Ji 5
2Risin φQ
ci
risinðφQ
ci 1 αiÞ
dQ
ciy
Ricos φQ
ci
2ricosðφQ
ci 1 αiÞ
2dQ
cix
0
0
1
2
4
3
5
ð2:79Þ
is the Jacobian matrix of wheel i, which is square and invertible. If all wheels are
identical (except for the orientation of the rollers), the kinematic parameters of the
robot in the configuration shown in Figure 2.14 are:
Ri 5 R; ri 5 r; φQ
ci 5 0
jdQ
cixj 5 d1;
jdQ
ciyj 5 d2
ð2:80Þ
α1 5 a3 5 2 45;
α2 5 α4 5 45
Thus, the Jacobian matrices (2.79) are:
J1 5
0
2r
ﬃﬃﬃ
2
p
=2
d2
R
2r
ﬃﬃﬃ
2
p
=2
d1
0
0
1
2
64
3
75;
J2 5
0
r
ﬃﬃﬃ
2
p
=2
d2
R
2r
ﬃﬃﬃ
2
p
=2
2d1
0
0
1
2
64
3
75
J3 5
0
2r
ﬃﬃﬃ
2
p
=2
2d2
R
2r
ﬃﬃﬃ
2
p
=2
2d1
0
0
1
2
64
3
75;
J4 5
0
r
ﬃﬃﬃ
2
p
=2
2d2
R
2r
ﬃﬃﬃ
2
p
=2
d1
0
0
1
2
64
3
75
ð2:81Þ
The robot motion is produced by the simultaneous motion of all wheels.
In terms of _θ ix (i.e., the wheels’ angular velocities around their axles) the veloc-
ity vector _pQ is given by:
_xQ
_yQ
_φQ
2
4
3
5 5 R
4
21
1
21
1
1
1
1
1
1
d1 1 d2
21
d1 1 d2
21
d1 1 d2
1
d1 1 d2
2
6664
3
7775
_θ 1x
_θ 2x
_θ 3x
_θ 4x
2
664
3
775
ð2:82Þ
The robot speed vector _p 5 _x; _y; _φ

T in the world coordinate frame is obtained as:
_x
_y
_φ
2
4
3
5 5
cos φ
2sin φ
0
sin φ
cos φ
0
0
0
1
2
4
3
5
_xQ
_yQ
_φQ
2
4
3
5
ð2:83Þ
where φ is the rotation angle of the platform’s coordinate frame QxQyQ around
the z axis which is orthogonal to Oxy. Inverting Eqs. (2.82) and (2.83) we get the
64
Introduction to Mobile Robot Control

inverse kinematic model, which gives the angular speeds _θ ixði 5 1; 2; 3; 4Þ of the
wheels around their hubs required to get a desired speed ½_x; _y; _φT of the robot:
_θ 1x
_θ 2x
_θ 3x
_θ 4x
2
664
3
775 5 1
R
21
1
ðd1 1 d2Þ
1
1
2ðd1 1 d2Þ
21
1
2ðd1 1 d2Þ
1
1
ðd1 1 d2Þ
2
664
3
775
_xQ
_yQ
_φQ
2
4
3
5
ð2:84Þ
_xQ
_yQ
_φQ
2
4
3
5 5
cos φ
sin φ
0
2sin φ
cos φ
0
0
0
1
2
4
3
5
_x
_y
_φ
2
4
3
5
ð2:85Þ
For historical awareness, we mention here that the mecanum wheel was invented
by the Swedish engineer Bengt Ilon in 1973 during his work at the Swedish com-
pany Mecanum AB. For this reason it is also known as Ilon wheel or Swedish
wheel.
Example 2.5
It is desired to construct a mecanum wheel with n rollers of angle α. Determine the roller
length Dr and the thickness d of the wheel.
Solution
We consider the wheel geometry shown in Figure 2.15, where R is the wheel radius [18].
From this figure we get the following relations:
n 5 2π=φ
ð2:86aÞ
sinðφ=2Þ 5 b=R
ð2:86bÞ
2b 5 Drsinα
ð2:86cÞ
d 5 Drcosα
ð2:86dÞ
From Eq. (2.86ac) we have:
sin π
n
	 
5
Dr
2R


sinα
ð2:87Þ
ф
R
d
Dr
(A)
(B)
α
b
Figure 2.15 (A) Geometry of
mecanum wheel where the
rollers are assumed to be
placed peripherally, (B) A
6-roller wheel example.
Source: http://store.kornylak.
com/SearchResults.asp?Cat57.
65
Mobile Robot Kinematics

whence:
Dr 5 2R sin π=n


sinα
ð2:88Þ
Solving (2.87) for sinα and noting that cosα 5 sinα=tgα, (2.86d) gives:
d 5 2R sinðπ=nÞ
tgα
ð2:89Þ
For a roller angle α 5 450, Eqs. (2.88) and (2.89) give:
Dr 5 2
ﬃﬃﬃ
2
p
R sinðπ=nÞ
ð2:90aÞ
d 5 2R sinðπ=nÞ
ð2:90bÞ
For a roller angle α 5 900 (universal wheel) we get:
Dr 5 2R sinðπ=nÞ
ð2:91aÞ
d 5 0
ðideallyÞ
ð2:91bÞ
In this case, d can have any convenient value required by other design considerations.
References
[1] Angelo A. Robotics: a reference guide to new technology. Boston, MA: Greenwood
Press; 2007.
[2] Muir PF, Neuman CP. Kinematic modeling of wheeled mobile robots. J Rob Syst
1987;4(2):281329.
[3] Alexander JC, Maddocks JH. On the kinematics of wheeled mobile robots. Int J Rob
Res 1981;8(5):1527.
[4] Muir PF, Neuman C. Kinematic modeling for feedback control of an omnidirectional
wheeled mobile robot. In: Proceedings of IEEE international conference on robotics and
automation, Raleigh, NC; 1987, p. 17728.
[5] Kim DS, Hyun Kwon W, Park HS. Geometric kinematics and applications of a mobile
robot. Int J Control Autom Syst 2003;1(3):37684.
[6] Rajagopalan R. A generic kinematic formulation for wheeled mobile robots. J Rob Syst
1997;14:7791.
[7] Sreenivasan SV. Kinematic geometry of wheeled vehicle systems. In: Proceedings of
24th ASME mechanism conference, Irvine, CA, 96-DETC-MECH-1137; 1996.
[8] Balakrishna R, Ghosal A. Two dimensional wheeled vehicle kinematics. IEEE Trans
Rob Autom 1995;11(1):12630.
[9] Killough SM, Pin FG. Design of an omnidirectional and holonomic wheeled platform
design. In: Proceedings of IEEE conference on robotics and automation, Nice, France;
1992, p. 8490.
66
Introduction to Mobile Robot Control

[10] Sidek N, Sarkar N. Dynamic modeling and control of nonholonomic mobile robot with
lateral slip. In: Proceedings of seventh WSEAS international conference on signal pro-
cessing robotics and automation (ISPRA’08), Cambridge, UK; February 2022, 2008,
p. 6674.
[11] Giovanni I. Swedish wheeled omnidirectional mobile robots: kinematics analysis and
control. IEEE Trans Rob 2009;25(1):16471.
[12] West M, Asada H. Design of a holonomic omnidirectional vehicle. In: Proceedings of
IEEE conference on robotics and automation, Nice, France; May 1992, p. 97103.
[13] Chakraborty N, Ghosal A. Kinematics of wheeled mobile robots on uneven terrain.
Mech Mach Theory 2004;39:127387.
[14] Sordalen OJ, Egeland O. Exponential stabilization of nonholonomic chained systems.
IEEE Trans Autom Control 1995;40(1):3549.
[15] Khalil H. Nonlinear Systems. Upper Saddle River, NJ: Prentice Hall; 2001.
[16] Ashmore M, Barnes N. Omni-drive robot motion on curved paths: the fastest path
between two points is not a straight line. In: Proceedings of 15th Australian joint con-
ference on artificial intelligence: advances in artificial intelligence (AI’02). London:
Springer; 2002. p. 22536.
[17] Huang L, Lim YS, Li D, Teoh CEL. Design and analysis of a four-wheel omnidirec-
tional mobile robot. In: Proceedings of second international conference on autonomous
robots and agents, Palmerston North, New Zealand; December 2004. p. 4258.
[18] Doroftei I, Grosu V, Spinu V. Omnidirectional mobile robot: design and implementa-
tion. In: Habib MK, editor. Bioinspiration and robotics: walking and climbing robots.
Vienna, Austria: I-Tech; 2007. p. 51227.
[19] Phairoh T, Williamson K. Autonomous mobile robots using real time kinematic signal
correction and global positioning system control. In: Proceedings of 2008 IAJC-IJME
international conference on engineering and technology, Sheraton, Nashville, TN;
November 2008, Paper 087/IT304.
[20] De Luca A, Oriolo G, Samson C. Feedback control of a nonholonomic car-like robot.
In: Laumond J-P, editor. Robot motion planning and control. Berlin, New York:
Springer; 1998. p. 171253.
67
Mobile Robot Kinematics

3 Mobile Robot Dynamics
3.1
Introduction
The next problem in the study of all types of robots, after kinematics, is the
dynamic modeling [14]. Dynamic modeling is performed using the laws
of mechanics that are based on the three physical elements: inertia, elasticity, and
friction that are present in any real mechanical system such as the robot. Mobile
robot dynamics is a challenging field on its own and has attracted considerable
attention by researchers and engineers over the years. Most mobile robots,
employed in practice, use conventional wheels and are subject to nonholonomic
constraints that need particular treatment. Delicate stability and control problems,
that often have to be faced in the design of a mobile robot, are due to the existence
of longitudinal and lateral slip in the movement of the wheeled mobile robot
(WMR) wheels [57].
This chapter has the following objectives:
G
To present the general dynamic modeling concepts and techniques of robots.
G
To study the NewtonEuler and Lagrange dynamic models of differential-drive mobile
robots.
G
To study the dynamics of differential-drive mobile robots with longitudinal and lateral
slip.
G
To derive a dynamic model of car-like WMRs.
G
To derive a dynamic model of three-wheel omnidirectional robots.
G
To derive a dynamic model of four-wheel mecanum omnidirectional robots.
3.2
General Robot Dynamic Modeling
Robot dynamic modeling deals with the derivation of the dynamic equations of the
robot motion. This can be done using the following two methodologies:
G
NewtonEuler method
G
Lagrange method
The complexity of the NewtonEuler method is O(n), whereas the complexity
of the Lagrange method can only be reduced up to Oðn3Þ, where n is the number of
degrees of freedom.
Introduction to Mobile Robot Control. DOI: http://dx.doi.org/10.1016/B978-0-12-417049-0.00003-1
© 2014 Elsevier Inc. All rights reserved.

Like kinematics, dynamics is distinguished in:
G
Direct dynamics
G
Inverse dynamics
Direct dynamics provides the dynamic equations that describe the dynamic
responses of the robot to given forces/torques τ1; τ2; . . .; τm that are exerted by the
motors.
Inverse dynamics provides the forces/torques that are needed to get desired tra-
jectories of the robot links. Direct and inverse dynamic modeling is pictorially
illustrated in Figure 3.1.
In the inverse dynamic model the inputs are the desired trajectories of the link
variables, and outputs the motor torques.
3.2.1
NewtonEuler Dynamic Model
This model is derived by direct application of the NewtonEuler equations for
translational and rotational motion. Consider the object Bi (robotic link, WMR,
etc.) of Figure 3.2 to which a total force fi is applied at its center of gravity (COG).
Direct
dynamic
model 
τ1(t),τ2(t),…,τm(t)
q1(t),q2(t),…,qn(t)
τ1(t),τ2(t),…,τm(t)
q1(t),q2(t),…,qn(t)
Inverse
dynamic
model 
τ1
q1
q2
qn
τ2
τm
Figure 3.1 Direct and inverse dynamic modeling.
o
x
y
z
Ei
τi
Gi
fi
r
dv
COG
si
Bi
Figure 3.2 A solid body Bi and the inertial coordinate
frame Oxyz.
70
Introduction to Mobile Robot Control

Then, its translational motion is described by:
dEi
dt 5 fi
ð3:1Þ
Here, Ei is the linear momentum given by:
Ei 5 mi_si
ð3:2Þ
where mi is the mass of the body and si is the position of the COG with respect to
the world (inertial) coordinate frame Oxyz. Assuming that mi is constant, then
Eqs. (3.1) and (3.2) give:
mi€si 5 Fi
ð3:3Þ
which is the general translational dynamic model.
The rotational motion of Bi is described by:
dGi
dt 5 τi
ð3:4Þ
where Gi is the total angular momentum of Bi with respect to COG, and τi is the
total external torque that produces the rotational motion of the body. The total
momentum Gi is given by:
Gi 5 Iiωi
ð3:5Þ
Here, Ii is the inertia tensor given by the volume integral:
Ii 5
ð
Vi
½rΤrI3 2 rrΤρidV
ð3:6Þ
where ρi is the mass density of Bi, dV is the volume of an infinitesimal element of
Bi lying at the position r with respect to COG, ωi is the angular velocity vector
about the inertial axis passing from COG, I3 is the 3 3 3 unit matrix, and Vi is the
volume of Bi.
3.2.2
Lagrange Dynamic Model
The general Lagrange dynamic model of a solid body (multilink robot, WMR, etc.)
is described by1:
d
dt
@L
@_q

2 @L
@q 5 τ;
q 5 ½q1; q2; . . .; qnΤ

ð3:7Þ
1 Derivations of Eq. (3.7) from first principles are provided in textbooks on mechanics.
71
Mobile Robot Dynamics

where qi is the ith-degree of freedom variable, τ is the external generalized force
vector applied to the body (i.e., force for translational motion, and torque for rota-
tional motion), and L is the Lagrangian function defined by:
L 5 K 2 P
ð3:8Þ
Here, K is the total kinetic energy, and P the total potential energy of the body
given by:
K 5 K1 1 K2 1 ? 1 Kn
P 5 P1 1 P2 1 ? 1 Pn
ð3:9Þ
where Ki is the kinetic energy of link (degree of freedom) i, and Pi its potential
energy. The kinetic energy K of the body is equal to:
K 5 1
2 m_sΤ_s 1 1
2 ωΤIω
ð3:10Þ
where _s is the linear velocity of the COG, ω is the angular velocity of the rotation,
m is the mass, and I the inertia tensor of the body.
3.2.3
Lagrange Model of a Multilink Robot
Given a general multilink robot, the application of Eq. (3.7) leads (always) to a
dynamic model of the form:
DðqÞ€q 1 hðq;_qÞ 1 gðqÞ 5 τ
ð3:11aÞ
where, for any _q 6¼ 0; DðqÞ is an n 3 n positive define matrix, and:
q 5 ½q1; q2; . . .; qnΤ
ð3:11bÞ
is the vector of generalized variables (linear, angular) qi; DðqÞ€q represents the iner-
tia force, hðq;_qÞ represents the centrifugal and Coriolis force, and gðqÞ stands for
the gravitational force. Since τ is the net force/torque applied to the robot, if there
is also friction force/torque τf , then τ 5 τ0 2 τf where τ0 is the force/torque
exerted by the actuators at the joints.
It is useful to note that given the model (3.11a,b) one can express the kinetic
energy of the robot as:
K 5 1
2 _qTðtÞDðqÞ_qðtÞ
ð3:12Þ
The expressions of DðqÞ; hðq; _qÞ, and gðqÞ have to be derived in each particular
case. General derivations of Eq. (3.11a) from Eq. (3.7) are given in standard indus-
trial (fixed) robotics books.
72
Introduction to Mobile Robot Control

Typically, the function hðq;_qÞ can be written in the form:
hðq; _qÞ 5 Cðq; _qÞ_q
ð3:13Þ
A useful universal property of the Lagrange model given by Eqs. (3.11a)(3.13)
is that the n 3 n matrix A 5 _D 2 2C is antisymmetric, that is, AΤ 5 2 A.
3.2.4
Dynamic Modeling of Nonholonomic Robots
The Lagrange dynamic model of a nonholonomic robot (fixed or mobile) has the
form (3.7):
d
dt
@L
@_q

2 @L
@q 1 MΤðqÞλ 5 Eτ

ð3:14Þ
where MðqÞ is the m 3 n matrix of the m nonholonomic constraints:
MðqÞ_q 5 0
ð3:15Þ
and λ is the vector Lagrange multiplier. This model leads to:
DðqÞ€q 1 Cðq;_qÞ_q 1 gðqÞ 1 MΤðqÞλ 5 Eτ
ð3:16Þ
where E is a nonsingular transformation matrix. To eliminate the constraint term
MΤðqÞλ in Eq. (3.16) and get a constraint-free model we use an n 3 ðn 2 mÞ matrix
BðqÞ which is defined such that:
BΤðqÞMΤðqÞ 5 0
ð3:17Þ
From Eqs. (3.15) and (3.17), one can verify that there exists an ðn 2 mÞ-
dimensional vector vðtÞ such that:
_qðtÞ 5 BðqÞvðtÞ
ð3:18Þ
Now, premultiplying Eq. (3.16) by BΤðqÞ and using Eqs. (3.15), (3.17), and
(3.18) we get:
DðqÞ_v 1 Cðq;_qÞv 1 gðqÞ 5 Eτ
ð3:19aÞ
where:
D 5 BΤDB
C 5 BΤD _B 1 BΤCB
g 5 BΤg
E 5 BΤE
ð3:19bÞ
73
Mobile Robot Dynamics

The reduced (unconstrained) model (3.19a,b) describes the dynamic evolution of
the n-dimensional vector qðtÞ in terms of the dynamic evolution of the
ðn 2 mÞ-dimensional vector vðtÞ.
3.3
Differential-Drive WMR
The dynamic model of the differential-drive WMR will be derived here by both the
NewtonEuler and Lagrange methods.
3.3.1
NewtonEuler Dynamic Model
In the present case use will be made of the NewtonEuler equations:
m_v 5 F
ðTranslational motionÞ
ð3:20aÞ
I _ω 5 N
ðRotational motionÞ
ð3:20bÞ
where F is the total force applied at the COG G, N is the total torque with respect to
the COG m is the mass of the WMR, and I is the inertia of the WMR. Referring
to Figure 2.7, and assuming that the COG G coincides with the midpoint Q
(i.e., b 5 0), we find:
F 5 Fr 1 Fl;
τr 5 rFr;
τl 5 rFl
ð3:21aÞ
i.e:
F 5 1
r ðτr 1 τlÞ
ð3:21bÞ
where Fr and Fl are the forces that produce the torques τr and τl, respectively.
Also:
N 5 ðFr 2 FlÞ2a 5 2a
r ðτr 2 τlÞ
ð3:22Þ
Therefore, (3.20a,b) give the WMR’s dynamic model:
_v 5 1
mr ðτr 1 τlÞ
ð3:23aÞ
_ω 5 2a
Ir ðτr 2 τlÞ
ð3:23bÞ
74
Introduction to Mobile Robot Control

3.3.2
Lagrange Dynamic Model
We refer to Figure 2.7 and assume again that the point Q is at the position of point G.
Here, the nonholonomic constraint matrix is (Eq. (2.42)):
MðqÞ 5 2sin φ
cos φ
0 

ð3:24Þ
Since the WMR moves on a horizontal planar terrain, the terms Cðq;_qÞ and
gðqÞ in Eq. (3.16) are zero. Therefore, the model (3.16) becomes:
DðqÞ€q 1 MΤðqÞλ 5 Eτ
ð3:25Þ
where:
q 5
xQ
yQ
φ
2
4
3
5;
τ 5
τr
τl


ð3:26aÞ
DðqÞ 5
m
0
0
0
m
0
0
0
I
2
4
3
5;
E 5 1
r
cos φ
cos φ
sin φ
sin φ
2a
22a
2
4
3
5
ð3:26bÞ
To convert the model (3.25) to the corresponding unconstrained model (3.19a,b),
we need the matrix BðqÞ in (3.17). Here, this matrix is:
BðqÞ 5
cos φ
0
sin φ
0
0
1
2
4
3
5
ð3:27Þ
which satisfies Eq. (3.17). Therefore, Eq. (3.19b) gives:
D 5 BΤDB 5
m
0
0
I


;
E 5 1
r
1
1
2a
22a


ð3:28Þ
and the model (3.19a) becomes:
m
0
0
I


_v1
_v2


5 1
r
1
1
2a
22a


τr
τl


ð3:29Þ
Noting that v1 is the translation velocity v and v2 the angular velocity ω of the
robot, Eq. (3.29) gives the model:
_v 5 1
mr ðτr 1 τlÞ
ð3:30aÞ
75
Mobile Robot Dynamics

_ω 5 2a
Ir ðτr 2 τlÞ
ð3:30bÞ
which is identical to Eq. (3.23a,b), as expected. Finally, using Eq. (3.18) we get:
_xQ
_yQ
_φ
2
4
3
5 5
cos φ
0
sin φ
0
0
1
2
4
3
5 v
ω


5
v cos φ
v sin φ
ω
2
4
3
5
ð3:31Þ
which is the WMR’s kinematic model. The dynamic and kinematic Eqs. (3.30a,b)
and (3.31) describe fully the motion of the differential-drive WMR.
Example 3.1
The problem is to derive the Lagrange dynamic model using directly the Lagrangian func-
tion L for a differential-drive WMR in which:
1. There is linear friction in the wheels with the same friction coefficient.
2. The wheel midpoint Q does not coincide with the COG G.
3. The wheelmotor assemblies have nonzero inertia.
Solution
We will work with the WMR of Figure 2.7.The kinetic energy K of the robot is given by:
K 5 K1 1 K2 1 K3
ð3:32Þ
where:
K1 5 1
2 mv2
G 5 1
2 mð_x2
G 1 _y2
GÞ
K2 5 1
2 IQ _φ
2
K3 5 1
2 Io _θ
2
r 1 1
2 Io _θ
2
l
ð3:33Þ
and (Example 2.2):
_xG 5 _xQ 1 b_φ sin φ
_yG 5 _yQ 2 b_φ cos φ
with:
m 5 mass of the entire robot
vG 5 linear velocity of the COG G
IQ 5 moment of inertia of the robot with respect to Q
Io 5 moment of inertia of each wheel plus the corresponding motor’s rotor moment of
inertia.
76
Introduction to Mobile Robot Control

The velocities _xQ; _yQ, and _φ are given by Eq. (2.36ac):
_xQ 5 r
2 ð_θ r cos φ 1 _θ l cos φÞ 5 r
2 ð_θ r 1 _θ lÞcos φ
_yQ 5 r
2 ð_θ r sin φ 1 _θ l sin φÞ 5 r
2 ð_θ r 1 _θ lÞsin φ
_φ 5 r
2a ð_θ r 2 _θ lÞ
ð3:34Þ
Using Eqs. (3.33) and (3.34) in Eq. (3.32) the total kinetic energy K of the robot is
found to be:
Kð_θ r; _θ lÞ 5
mr2
8
1 ðIQ 1 mb2Þr2
8a2
1 Io
2
2
4
3
5_θ
2
r
1
mr2
8
1 ðIQ 1 mb2Þr2
8a2
1 Io
2
2
4
3
5_θ
2
l
1
mr2
4
2 ðIQ 1 mb2Þr2
4a2
2
4
3
5_θ r _θ l
ð3:35Þ
Here the kinetic energy is expressed directly in terms of the angular velocities _θ r and
_θ l of the driving wheels. The Lagrangian L is equal to K, since the robot is moving on a
horizontal plane and so the potential energy P is zero. Therefore, the Lagrange dynamic
equations of this robot are:
d
dt
@K
@_θ r
0
@
1
A 2 @K
@θr
5 τr 2 β _θ r
d
dt
@K
@_θ l
0
@
1
A 2 @K
@θl
5 τl 2 β _θ l
ð3:36Þ
where β is the wheels’ common friction coefficient, and τr; τl are the right and left actu-
ation torques. Using (3.35) in (3.36) we get:
D11 €θ r 1 D12 €θ l 1 β _θ r 5 τr
D21 €θ r 1 D22 €θ l 1 β _θ l 5 τl
ð3:37Þ
where:
D11 5 D22 5
mr2
4
1 ðIQ 1 mb2Þr2
8a2
1 Io
2
4
3
5
D12 5 D21 5
mr2
4
2 ðIQ 1 mb2Þr2
8a2
2
4
3
5
ð3:38Þ
77
Mobile Robot Dynamics

Using the known relations:
vr 5 r _θ r; vl 5 r _θ l; v 5 ðvr 1 vlÞ=2; ω 5 ðvr 2 vlÞ=2a, one can easily verify that in the case
where the above conditions 1, 2, and 3 are relaxed (i.e., β 5 0, b 5 0, Io 5 0), the above
dynamic model reduces to the model (3.30a,b). This model was implemented and validated
using MATLAB/SIMULINK in Ref. [8].
3.3.3
Dynamics of WMR with Slip
Here we will derive the NewtonEuler dynamic model of the slipping differential-
drive WMR considered in Example 2.2. The case where both longitudinal slip
(variables wr; wl) and lateral slip (variables zr; zl) are present will be considered [5].
For convenience we write again the kinematic equations of the robot (with steer-
ing angles ζr 5 ζl 5 0):
_γr 5 _xG cos φ 1 _yG sin φ 1 a_φ;
γr 5 rθr 2 wr
_γl 5 _xG cos φ 1 _yG sin φ 2 a_φ;
γl 5 rθl 2 wl
_zr 5 2 _xG sin φ 1 _yG cos φ 1 b_φ
_zl 5 2 _xG sin φ 1 _yG cos φ 1 b_φ
Defining the generalized variables’ vector q as:
q 5 ½xG; yG; φ; zr; zl; γr; γl; θr; θl
ð3:39Þ
the above relations can be written in the following Pfaffian matrix form:
MðqÞ_q 5 0
where:
MðqÞ 5
cos φ
sin φ
a
0
0
21
0
0
0
cos φ
sin φ
2a
0
0
0
21
0
0
2sin φ
cos φ
b
21
0
0
0
0
0
2sin φ
cos φ
b
0
21
0
0
0
0
2
6666664
3
7777775
ð3:40Þ
The matrix BðqÞ and the velocity vector vðtÞ that satisfy the relations
(Eqs. (3.17)(3.18)):
BΤðqÞMTðqÞ 5 0;
_q 5 BðqÞvðtÞ
ð3:41Þ
78
Introduction to Mobile Robot Control

are found to be:
BðqÞ 5
2sin φ
A
C
0
0
cos φ
B
D
0
0
0
1
2a
2 1
2a
0
0
1
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
1
2
666666666666664
3
777777777777775
ð3:42Þ
where:
v 5 ½_zl; _γr; _γl; _θ r; _θ lΤ
ðnote that zr 5 zl 5 yrÞ
ð3:43aÞ
A 5 a cos φ 2 b sin φ
2a
;
B 5 b cos φ 1 a sin φ
2a
C 5 a cos φ 1 b sin φ
2a
;
D 5 a sin φ 2 b cos φ
2a
ð3:43bÞ
To derive the NewtonEuler dynamic model we draw the free-body diagram of
the WMR body without the wheels (Figure 3.3), and the free-body diagram of the
two wheels (Figure 3.4).
In Figures 3.3 and 3.4, we have the following dynamic parameters and variables:
τb: The torque given to the wheels by the body of the WMR
τr; τl: The driving torques exerted to the right and left wheel by their motors
mb: The mass of the WMR body without the wheels
mw: The mass of each driving wheel assembly (wheel plus DC motor of the robot body)
Ibz: The moment of inertia about a vertical axis passing via G (without the wheels)
ф
Φ
F1
b
a
G
y
F2
F4
F3
–τb
–τb
x
o
r
Q
.
Figure 3.3 Forcetorque diagram of
the body of the WMR (without the
wheels).
79
Mobile Robot Dynamics

Iwy: The wheel moment of inertia about its axis
Iwz: The wheel moment of inertia about its diameter
Fi: The reaction forces between the WMR body and the wheels
Flat;r; Flat;l: The lateral traction force at each wheel.
Flong;r; Flong;l: The longitudinal traction force at each wheel.
Using the above notation, the NewtonEuler dynamic equations of the robot,
written down for each generalized variable in the vector (3.39), are:
mb €xG 5 ðF1 1 F2Þsin φ 2 ðF3 1 F4Þcos φ
mb €yG 5 2 ðF1 1 F2Þcos φ 2 ðF3 1 F4Þsin φ
ðIbz 1 2IwzÞ€φ 5 ðF1 1 F2Þb 2 ðF3 1 F4Þa
mw€zr 1 mw _φ_γr 5 Flat;r 2 F1
mw€zl 1 mw _φ_γl 5 Flat;l 2 F2
mw €γr 2 mw _φ_zr 5 Flong;r 2 F3
mw €γl 2 mw _φ_zl 5 Flong;l 2 F4
Iwy €θ r 5 τr 2 Flong;rr
Iwy €θ l 5 τl 2 Flong;lr
ð3:44Þ
where τb 5 Iwz €φ.
The detailed Eq. (3.44) can be written in the compact form of Eq. (3.16), namely:
DðqÞ€q 1 hðq;_qÞ 5 EðqÞτ 1 fð_qÞ 1 MΤðqÞF
ð3:45Þ
where MðqÞ is given by (3.40), and:
D 5 diag mb; mb; Ibz 1 2Iwz; mw; mw; mw; mw; Iwy; Iwy


h 5 0; 0; 0; mw _φ_γr; mw _φ_γl; 2 mw _φ_zr; 2 mw _φ_zl; 0; 0


z
F4
F3
Flong, l
Flong, r
Flat, l
Flat, r
y
ow
Left wheel
Right wheel
F2
F1
τb
τb
τl
τr
Figure 3.4 Forcetorque diagram
of the two wheels. The coordinate
frame Owγz is fixed at the driving
wheels’ midpoint.
80
Introduction to Mobile Robot Control

E 5
2O2 3 7
I2 3 2
2
4
3
5; τ 5
τr
τl


f 5 0; 0; 0; Flat;r; Flat;l; Flong;r; Flong;l; 2rFlong;r; 2rFlong;l

Τ
F 5 F1; F2; F3; F4
½
T
Applying to Eq. (3.45) the procedure of Section 3.2.4, we get the following
reduced model (3.19a,b):
DðqÞ_v 1 C1ðq;_qÞv 1 hðq;_qÞ 5 Eτ 1 fðqÞ
ð3:46Þ
where:
DðqÞ 5 BΤDB;
C1ðq;_qÞ 5 BΤD _B
hðq;_qÞ 5 BΤh;
E 5 BΤE;
fðqÞ 5 BΤf
The model (3.46) can be split as:
^Dð^qÞ_^v1 ^C1ð^q;_^qÞ^v 1 ^hð^q;_^qÞ 5 ^f
ð3:47aÞ
Iwy €θ r 5 τr 2 rFlong;r
ð3:47bÞ
Iwy €θ l 5 τl 2 rFlong;l
ð3:47cÞ
where:
^D5 ^BΤ ^D ^B;
^C1 5 ^BΤ ^D_^B
^h5 ^BΤ ^h;
^f 5 ^BΤ^f
^D 5 diag½mb; mb; Ibz12Iwz; mw; mw; mwmwΤ
^v 5 ½_zl; _γr; _γlΤ
^h 5 ½0; 0; 0; mw _φ_γr; mw _φγl; 2mw _φ_zr; 2mw _φ_zlΤ
^f 5 ½0; 0; 0; Flat;r; Flat;l; Flong;r; Flong;lΤ
^q 5 ½xG; yG; φ; zr; zl; γr; γlΤ
Bð^qÞ 5
2sin φ
A
C
cos φ
B
D
0
1
2a
2 1
2a
1
0
0
1
0
0
0
1
0
0
0
1
2
66666666664
3
77777777775
ð3:48Þ
81
Mobile Robot Dynamics

with A, B, C, and D as in Eq. (3.43b). Writing ^hð^q;_^qÞ in the form of Eq. (3.13),
that is, ^hð^q;_^qÞ 5 ^C2ð^q;_^qÞ_^q, the model (3.47a) takes the form:
^Dð^qÞ_^v1 ^Cð^q;_^qÞ^v 5 ^f
ð3:49Þ
where:
^Cð^q;_^qÞ 5 ^C1ð^q;_^qÞ 1 ^C2ð^q;_^qÞ;
^C2ð^q;_^qÞ 5 ^BΤ ^C2ð^q;_^qÞ
Finally, Eq. (3.47b,c) can be written in the matrix form:
I _θ 5 τ 2 rf
ð3:50Þ
where:
I 5
Iwy
0
0
Iwy


;
θ 5
_θ r_θ l


;
τ 5
τr
τl


;
f 5
Flong;r
Flong;l


ð3:51Þ
To summarize, the dynamic model of the differential-drive WMR with slip is:
^Dð^qÞ_^v1 ^Cð^q;_^qÞ^v 5 ^f
ð3:52aÞ
I _θ 5 τ 2 rf
ð3:52bÞ
with:
^q 5 ½xG; yG; φ; zr; zl; γr; γlΤ
^v 5 ½_zl; _γr; _γlΤ
ð3:52cÞ
3.4
Car-Like WMR Dynamic Model
The kinematic equations of the car-like WMR were derived in Section 2.6. Here,
we will derive the NewtonEuler dynamic model for a four-wheel rear-drive front-
steer WMR [9]. The equivalent bicycle model shown in Figure 3.5 will be used.
The WMR is subject to the driving force Fd and two lateral slip forces Fr and Ff
applied perpendicular to the corresponding wheels. Before presenting the dynamic
equations we derive the nonholonomic constraints that apply to the COG G which
lies at a distance b from Q, and a distance d from P. To this end, we start with the
nonholonomic constraints (2.50a,b) that refer to the points Q and P:
_xQ sin φ 2 _yQ cos φ 5 0
_xPsinðφ 1 ψÞ 2 _yP cosðφ 1 ψÞ 5 0
ð3:53Þ
82
Introduction to Mobile Robot Control

Denoting by xG and yG the coordinates of the point G in the world coordinate
frame we get:
xQ 5 xG 2 b cos φ;
xP 5 xG 1 d cos φ
yQ 5 yG 2 b sin φ;
yP 5 yG 1 d sin φ
_xQ 5 _xG 1 b_φ sin φ;
_xP 5 _xG 2 d _φ sin φ
_yQ 5 _yG 2 b_φ cos φ;
_yP 5 _yG 1 d _φ cos φ
ð3:54Þ
Introducing the relations (3.54) into (3.53) yields:
_xG sin φ 2 _yG cos φ 1 b_φ 5 0
_xG sin ðφ 1 ψÞ 2 _yG cos ðφ 1 ψÞ 2 d _φ cos ψ 5 0
ð3:55Þ
Now, denoting by ½_xg; _yg the velocity of the COG G in the local coordinate frame
Gxgyg and applying the rotational transformation (2.17) ( Example 2.1) we get:
_xG
_yG


5
cos φ
2sin φ
sin φ
cos φ


_xg
_yg


ð3:56Þ
Introducing Eq. (3.56) into Eq. (3.55) yields:
_yg 5 b_φ;
_φ 5 tgψ
D _xg
ð3:57Þ
which, upon differentiation, give:
€yg 5 b€φ
ð3:58aÞ
€φ 5 tgψ
D €xg 1
1
D cos2 ψ _xg _ψ
ð3:58bÞ
ψ
D
yg direction
xg direction
b
G d
P
Q
φ
y
x
o
Fd
Fr
Ff
Figure 3.5 Free-body diagram of the
equivalent bicycle.
83
Mobile Robot Dynamics

From Eqs. (3.57) and (3.58a,b) we get:
_yg 5 b
D ðtgψÞ_xg;
€yg 5 b
D ðtgψÞ€xg 1
b
D cos2 ψ _xg _ψ
ð3:59Þ
Referring to Figure 3.5 we obtain the following NewtonEuler dynamic
equations:
mð€xg 2 _yg _φÞ 5 Fd 2 Ff sin ψ
ð3:60aÞ
mð€yg 1 _xg _φÞ 5 Fr 1 Ff cos ψ
ð3:60bÞ
J €φ 5 dFf cos ψ 2 bFr
ð3:60cÞ
_ψ 5 2 1
Τ ψ 1 K
Τ us
ð3:60dÞ
where:
m 5 mass of the WMR
J 5 moment of inertia of the WMR about G
Fd 5 driving force
Ff; Fr 5 front and rear wheel lateral forces
T 5 time constant of the steering system
us 5 steering control input
K 5 a constant coefficient (gain)
Fd 5 ð1=rÞτd, with r the rear wheel radius, and τd the applied motor torque.2
We will now put the above dynamic equations into state space form, where the
state vector is:
x 5 ½xG; yG; φ; _xg; ψΤ
ð3:61Þ
To this end, we solve Eq. (3.60c) for Fr and introduce it, together with
Eq. (3.58a) into (3.60b) to get:
mðb€φ 1 _xg _φÞ 5 Ff cos ψ 1 d
b Ff cos ψ 2 J
b
€φ
5 DFf cos ψ 2 J €φ
b
ðD 5 b 1 dÞ
2 Here, the driving motor dynamics is not included. Derivation of DC motor dynamic models is provided
in Example 5.1.
84
Introduction to Mobile Robot Control

whence:
Ff 5
mb2 1 J
D cos ψ


€φ 1
mb_xg
D cos ψ


_φ
ð3:62Þ
Now introducing Eqs. (3.57), (3.58b), and (3.62) into Eq. (3.60a) we get:
€xg 5 _xgðmb2 1 JÞtgψ
a
_ψ 1 D2 cos2 ψ
a
Fd
ð3:63aÞ
where:
a 5 ðcos2 ψÞðmD2 1 ðmb2 1 JÞtg2ψÞ
ð3:63bÞ
Finally, introducing Eq. (3.57) into Eq. (3.56) gives:
_xG 5 fcos φ 2 ðb=DÞðtgψÞsin φg_xg
_yG 5 fsin φ 1 ðb=DÞðtgψÞcos φg_xg
_φ 5 ½ð1=DÞtgψ_xg
€xg 5 ð1=aÞ½ðmb2 1 JÞðtgψÞ _ψ_xg 1 ð1=aÞðD2 cos2 ψÞFd
_ψ 5 2ð1=ΤÞψ 1 ðK=ΤÞus
ð3:64Þ
with Fd 5 ð1=rÞτd. The model (3.64) represents an affine system with two inputs
ðτd; usÞ, a five-dimensional state vector:
x 5 ½xG; yG; φ; _xg; ψΤ
ð3:65aÞ
a drift term:
g0ðxÞ 5
½cos φ 2 ðb=DÞðtgψÞsin φ_xg
½sin φ 1 ðb=DÞðtgψÞcos φ_xg
ð1=DÞðtgψÞ_xg
ð1=aÞ½ðmb2 1 JÞðtgψÞ _ψ_xg
2ð1=TÞψ
2
66664
3
77775
ð3:65bÞ
and the two input fields:
g1ðxÞ 5 0; 0; 0; 1
ra D2cos2ψ; 0

Τ
;
g2ðxÞ 5 ½0; 0; 0; 0; K=ΤΤ
ð3:65cÞ
namely:
_x 5 g0ðxÞ 1 g1ðxÞτd 1 g2ðxÞus
ð3:66Þ
85
Mobile Robot Dynamics

3.5
Three-Wheel Omnidirectional Mobile Robot
Here, we will derive the dynamic model of a three-wheel omnidirectional robot
using the NewtonEuler method [10]. This derivation is the same for any number
of universal (orthogonal) omniwheels. Some further studies of omnidirectional
WMRs are presented in Refs. [1114].
Consider the WMR in the pose shown in Figure 3.6 in which the three wheels
are placed at angles 30; 150; and 270, respectively.
The rotation matrix of the robot’s local coordinate frame Qxryr with respect to
the world coordinate frame Oxy is:
RðφÞ 5
cos φ
2sin φ
sin φ
cos φ


ð3:67Þ
Let sQ 5 xQ
yQ Τ

be the position vector of the COG Q. Then:
M€sQ 5 FQ;
FQ 5 FQx
FQy Τ

ð3:68Þ
where M 5 diagð m; m Þ, m is the robot’s mass, and FQ is the force exerted at the
COG expressed in the world coordinate frame. Denoting by:
sr 5 xr
yr Τ;
Fr 5 ½Fxr; FyrΤ

ð3:69Þ
the position vector of the COG and the force vector expressed in the local (moving)
coordinate frame, then Eq. (3.67) implies that:
_sQ 5 RðφÞ_sr;
FQ 5 RðφÞFr
ð3:70Þ
Thus, using Eq. (3.70) in Eq. (3.68) we get:
Mð€sr 1 RΤðφÞ _RðφÞ_srÞ 5 Fr
ð3:71Þ
1
120
120
30
xr
yr
Q
D
2
3
x
y
0
Fd2
Fd3
Fd1
Fr
φ
ψ
θ
Figure 3.6 Geometry of the three-wheel
omnidirectional WMR.
86
Introduction to Mobile Robot Control

Now, the dynamic equation of the rotation about the COG Q is:
IQ €φ 5 τQ
ð3:72Þ
where IQ is the moment of inertia of the robot about Q, and τQ is the applied torque
at Q.
From the geometry of Figure 3.6 we obtain:
Fxr 5 2 1
2 Fd1 2 1
2 Fd2 1 Fd3
Fyr 5
ﬃﬃﬃ
3
p
2 Fd1 2
ﬃﬃﬃ
3
p
2 Fd2
τQ 5 ðFd1 1 Fd2 1 Fd3ÞD
ð3:73Þ
where D is the distance of the wheels from the rotation point Q, and Fdiði 5 1; 2; 3Þ
are the driving forces of the wheels.
The rotation of each wheel is described by the dynamic equation:
Io €θ i 1 β _θ i 5 Kτi 2 rFdi
ði 5 1; 2; 3Þ
ð3:74Þ
where Io is the common moment of inertia of the wheels, θi is the angular position
of wheel i, β is the linear friction coefficient, r is the common radius of the wheels,
τi is the driving input torque of wheel i, and K is the driving torque gain.
Now, from the geometry of Figure 3.6 we see that the angles of the wheel veloci-
ties
in
the
local
coordinate
frame
are:
301905120; 12011205240;
and 24011205360
Thus, we obtain the inverse kinematics equations from _pr 5 ½_xr; _yr; _φΤ to
_q 5 ½_θ 1; _θ 2; _θ 3Τ 5 ½ω1; ω2; ω3Τ as:
rω1 5 21
2 _xr 1
ﬃﬃﬃ
3
p
2 _yr 1 D_φ
ð3:75aÞ
rω2 52 1
2 _xr 2
ﬃﬃﬃ
3
p
2 _yr 1 D_φ
ð3:75bÞ
rω3 5 _xr 1 D_φ
ð3:75cÞ
Using Eqs. (3.69)(3.75c) we get after some algebraic manipulation:
€xr 5 a1 _xr 1 a
2 _yr _φ 2 b1ðτ1 1 τ2 2 2τ3Þ
ð3:76aÞ
€yr 5 a1 _yr 2 a
2 _xr _φ 1
ﬃﬃﬃ
3
p
b1ðτ1 2 τ2Þ
ð3:76bÞ
€φ 5 a3 _φ 1 b2ðτ1 1 τ2 1 τ3Þ
ð3:76cÞ
87
Mobile Robot Dynamics

where:
a1 5 23βð3Io 1 2mr2Þ;
a
2 5 2mr2=ð3Io 1 2mr2Þ
a3 5 23βD2=ð3IoD2 1 IQr2Þ
b1 5 Kr=ð3Io 1 2mr2Þ;
b2 5 Kr=ð3IoD2 1 IQr2Þ
Finally, combining Eqs. (3.67), (3.68), and (3.76ac) we get the following
statespace dynamic model of the WMR’s motion:
_x 5 AðxÞx 1 BðxÞu
ð3:77aÞ
y 5 Cx
ð3:77bÞ
where:
x 5 ½xQ; yQ; φ; _xQ; _yQ; _φΤ
y 5 ½_xQ; _yQ; φΤ
u 5 ½τ1; τ2; τ3Τ
AðxÞ 5
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
a1
2a2 _φ
0
0
0
0
a2 _φ
a1
0
0
0
0
0
0
a3
2
6666664
3
7777775
BðxÞ 5
0
0
0
0
0
0
0
0
0
b1β1
b1β2
2b1 cos φ
b1β3
b1β4
2b1 sin φ
b2
b2
b2
2
6666664
3
7777775
5 b1ðxÞ
b2ðxÞ
b3ðxÞ 

C 5
0
0
0
1
0
0
0
0
0
0
1
0
0
0
1
0
0
0
2
4
3
5
a2 5 1 2 a
2 5 3Io=ð3Io 1 2mr2Þ
β1 5 2
ﬃﬃﬃ
3
p
sin φ 2 cos φ; β2 5
ﬃﬃﬃ
3
p
sin φ 2 cos φ
β3 5
ﬃﬃﬃ
3
p
cos φ 2 sin φ; β4 5 2
ﬃﬃﬃ
3
p
cos φ 2 sin φ
88
Introduction to Mobile Robot Control

The azimuth of the robot in the world coordinate frame is denoted by ψ, where
ψ 5 φ 1 θ (θ denotes the angle between Qxr and Fr, that is, the azimuth of the robot
in the moving coordinate frame). Then:
_xQ 5 υ cos ψ; _yQ 5 υ sin ψ;
υ 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
_x2
Q 1 _y2
Q
q
Whence:
ψ 5 tg21ð_yQ=_xQÞ
ð3:78Þ
where the positive direction is the counterclockwise rotation direction. It is noted
that the motions along xQ and yQ are coupled because the dynamic equations are
derived in the world coordinate frame. But since the rotational angle is always
equal to φ 5 ψ 2 θ, despite the fact that θ may be changing arbitrarily, the WMR
can realize a translational motion without changing the pose (i.e., the WMR is
holonomic). The model (3.77a) can be written in a three-input affine form with
drift as follows:
_x 5 goðxÞ 1
X
3
i51
giðxÞui
ð3:79Þ
where:
goðxÞ 5 AðxÞx; giðxÞ 5 biðxÞ
ði 5 1; 2; 3Þ
The dynamic model (3.77a) of a three-wheel omnidirectional robot (with univer-
sal wheels) is one of the many different models available. Actually, many other
equivalent models were derived in the literature.
Example 3.2
Derive the dynamic equations of the three-wheel omnidirectional robot using the unit
directional vectors ε1; ε2; ε3 of the wheel velocities.
Solution
To simplify the derivation we select the pose of the WMR in which the wheel 1 orientation
is perpendicular to the local coordinate axis Qxr as shown in Figure 3.7 [15]. Thus the unit
directional vectors ε1; ε2, and ε3 are:
ε1 5
0
1


;
ε2 5 2
ﬃﬃﬃ
3
p
=2
1=2


;
ε3 5
ﬃﬃﬃ
3
p
=2
21=2


ð3:80Þ
The rotational matrix of Qxryr with respect to Oxy is given by (3.67).
89
Mobile Robot Dynamics

Therefore, the driving velocities υiði 5 1; 2; 3Þ of the wheels are:
υ1 5 r _θ 1 5 2 _xQ sin φ 1 _yQcos φ 1 D_φ
υ2 5 r _θ 2 5 2 _xQ sin ðπ=3 2 φÞ 2 _yQ cos ðπ=3 2 φÞ 1 D_φ
υ3 5 r _θ 3 5 _xQ sin ðπ=3 1 φÞ 2 _yQ cos ðπ=3 1 φÞ 1 D_φ
or
_q 5 J21ðφÞ_pQ
ð3:81Þ
where:
J21ðφÞ 5 1
r
2sin φ
cos φ
D
2sin ðπ=3 2 φÞ
2cos ðπ=3 2 φÞ
D
sin ðπ=3 1 φÞ
2cos ðπ=3 1 φÞ
D
2
64
3
75
_q 5 ½_θ 1; _θ 2; _θ 3Τ; _pQ 5 ½_xQ; _yQ; _φΤ
This is the inverse kinematic model of the robot. Now, applying the NewtonEuler
method to the robot we get:
m €xQ
€yQ


5 s1ðφÞFd1 1 s2ðφÞFd2 1 s3ðφÞFd3
ð3:82aÞ
IQ €φ 5 DðFd1 1 Fd2 1 Fd3Þ
ð3:82bÞ
where Fdiði 5 1; 2; 3Þ is the magnitude of the driving force of the ith wheel, m is the robot
mass, IQ is the robot moment of inertia about Q, and:
siðφÞ 5 RðφÞεi
ði 5 1; 2; 3Þ
ð3:83Þ
are two-dimensional vectors found using Eqs. (3.67) and (3.80). The driving forces
Fdiði 5 1; 2; 3Þ are given by the relation:
Fdi 5 aVi 2 βr _θ i
ði 5 1; 2; 3Þ
ð3:84Þ
where Vi is the voltage applied to the motor of the ith wheel, “a” is the voltageforce
constant, and β is the friction coefficient.
120°
x
y
o
1
2
3
ε1
ε2
ε3
D
yr
xr
Q
ф
Figure 3.7 The three-wheel omnidirectional
mobile robot (ε1 perpendicular to Qxr).
90
Introduction to Mobile Robot Control

Combining Eqs. (3.82a,b), (3.83), and (3.84) we obtain the model:
D€pQ 1 CðφÞ_pQ 5 Ev
ð3:85aÞ
where:
D 5
m
0
0
0
m
0
0
0
IQ
2
4
3
5; CðφÞ 5
βr
a


EðφÞJ21ðφÞ
ð3:85bÞ
EðφÞ 5 a s1ðφÞ
s2ðφÞ
s3ðφÞ
D
D
D


; v 5
V1
V2
V3
2
4
3
5
ð3:85cÞ
The model (3.85ac) has the standard form of the robot model described by
Eqs. (3.11) and (3.13).
Example 3.3
We are given a car-like robot where there are lateral slip forces and longitudinal friction
forces on all wheels. It is desired to write down the NewtonEuler dynamic equations in
the form (3.60ad).
Solution
The free-body diagram of the robot is as shown in Figure 3.8.
We use the following definitions:
Fd 5 Fl
d 1 Fr
d ðTotal driving forceÞ
Fxg;r 5 Fl
xg;r 1 Fr
xg;r ðRear total longitudinal frictionÞ
Fxg;f 5 Fl
xg;f 1 Fr
xg;f ðFront total longituditial frictionÞ
Fyg;r 5 Fl
yg;r 1 Fr
yg;r ðRear total lateral forceÞ
Fyg;f 5 Fl
yg;f 1 Fr
yg;f ðFront total lateral forceÞ
P
b
d
G
Q
ψ
D
yg direction
xg direction
φ
y
x
o
Fl
yg,r
Fl
yg,f
Fl
xg,r
Fl
xg,f
Fl
xg,f
Fr
yg,r
Fr
yg,f
Fr
xg,r
Fl
d
Fr
d
Figure 3.8 Free-body diagram of the
car-like WMR.
91
Mobile Robot Dynamics

where the upper index r refers to the right wheel and the upper index l to the left wheel.
Therefore, considering the bicycle model of the WMR we get the following NewtonEuler
dynamic equations in the local coordinate frame:
mð€xg 2 _yg _φÞ 5 Fd 2 Fxg;r 2 Fxg;f cos ψ 2 Fyg;f sin ψ
mð€yg 1 _xg _φÞ 5 Fyg;r 2 Fxg;f sin ψ 1 Fyg;f cos ψ
J€φ 5 dFyg;f cos ψ 2 bFyg;r
_ψ 5 2ð1=TÞψ 1 ðK=ΤÞus
where all variables have the meaning presented in Section 3.4. From this point the devel-
opment of the full model can be done as in Section 3.4.
3.6
Four Mecanum-Wheel Omnidirectional Robot
We consider the four-wheel omnidirectional robot of Figure 3.9A [3].
The total forces Fx and Fy acting on the robot in the x and y directions are:
Fx 5 ðFx1 1 Fx2 1 Fx3 1 Fx4Þ
ð3:86aÞ
Fy 5 ðFy1 1 Fy2 1 Fy3 1 Fy4Þ
ð3:86bÞ
where Fxi; Fyi
ði 5 1; 2; 3; 4Þ are the forces acting on the wheels along the x and y
axes. In the absence of separate rotation motion, the direction of motion is defined
by an angle δ where:
δ 5 tg21ðFy=FxÞ
ð3:87Þ
The torque τ that produces pure rotation is:
τ 5 ðFx1 2 Fx2 2 Fx3 1 Fx4Þd1 1 ðFy3 1 Fy4 2 Fy1 2 Fy2Þd2
ð3:88Þ
where positive rotation is in the counterclockwise direction.
1
2
3
4
(A)
(B)
y
d2
d1
x
Q
Fx2
Fx3
Fx4
Fy4
Fy3
Fx1
Fy1
Fy2
Figure 3.9 (A) The four-wheel mecanum WMR and the forces acting on it and (B) an
experimental four-wheel mecanum WMR prototype.
Source: www.robotics.ee.uwa.edu.au/eyebot/doc/robots/omni.html.
92
Introduction to Mobile Robot Control

The NewtonEuler motion equations are:
m€x 5 Fx 2 βx _x
ð3:89aÞ
m€y 5 Fy 2 βy _y
ð3:89bÞ
IQ €φ 5 τ 2 βz _φ
ð3:89cÞ
where βx; βy, and βz are the linear friction coefficients in the x, y, and φ motion,
and m; IQ are the mass and moment of inertia of the robot. Equations (3.89ac)
show that the robot can achieve steady-state velocities ð€x 5 0; €y 5 0; €φ 5 0Þ
_xss; _yss
and _φss equal to:
_xss 5 Fx
βx
;
_yss 5 Fy
βy
;
_φss 5 τ
βz
ð3:90Þ
Example 3.4
The problem is to calculate the wheel angular velocities which are required to achieve a
desired translational and rotational motion (WMR velocities v and _φ) of the mecanum
mobile robot of Figure 3.9A.
Solution
A solution to this problem was provided by the relations (2.84) and (2.85). Here we will
provide an alternative method [3]. We draw the displacement and velocity vectors of a sin-
gle wheel which are as shown in Figure 3.10A and B, where “a” is the roller angle
ða 5 6 45Þ.
The vector sp represents the displacement due to the wheel rotation (in the positive
direction), sr represents the displacement vector due to rolling which is orthogonal to the
roller axis, and s represents the total displacement vector. The dotted horizontal lines rep-
resent the discontinuities where the roller contact point transfers from one roller to the
next. In Figure 3.10, the point A was selected in the middle of this discontinuity line to
facilitate the calculation.
From Figure 3.10B we get ðωrÞcos a 5 v cosða 2 γÞ because the components of ωr and
v 5 ds=dt along the roller axis are equal. Therefore:
ωr 5 v cosða 2 γÞ=cos a
ð3:91Þ
(A)
γ
γ
α
α
Roller axis
Roller axis
νr
ν
νi=ωr
x
o
y
y
x
sr
sp
s
A
(B)
Figure 3.10 (A) Displacement vectors of a
wheel and roller and (B) velocity vectors
(a is the rollers angle).
93
Mobile Robot Dynamics

for a 6¼ π=2 1 kπ ðk 5 0; 1; 2; . . .Þ. If a 5 π=2 1 kπ, the rotation of the wheel does not
produce translation motion of the rollers. Solving Eq. (3.91) for v we get:
v 5 ωr
cos a
cos ðα 2 γÞ ;
a 2 γ 6¼ π
2 1 kπ ðk 5 0; 1; 2; . . .Þ
ð3:92Þ
When a 2 γ 5 π=2, the rotational speed ω of the wheel must be zero, but the wheel
can have any value of translation velocity because of the motion of the other wheels.
We now calculate the wheel angular velocity ωi for a desired translational motion
velocity v. From Eq. (3.90), we see that the wheel velocity is proportional to the forces
applied by each wheel, that is:
“v proportional to ðF1 1 F2 1 F3 1 F4Þ,” and because the WMR is a rigid body, all wheels
should have the same translational speed, that is:
vi 5 v
ði 5 1; 2; 3; 4Þ
Therefore, from Eq. (3.92) we have:
ωi 5 v cosðai 2 γÞ
ri cos ai
;
ai 6¼ π
2 1 kπ ðk 5 0; 1; 2; . . .Þ
ð3:93Þ
Equation (3.93) gives the velocities ωiði 5 1; 2; 3; 4Þ of the four wheels needed to get a
desired translational velocity v of the robot.
Finally, we will calculate the wheel angular velocities required to get a desired rota-
tional speed _φ. Consider a robot with velocity v. Then the instantaneous curvature radius
(ICR) of its path is:
R 5 v=_φ with ε 5 ctg21ðvx=vyÞ
ð3:94Þ
These relations give the following world frame coordinates xICR; yICR of the ICR
(Figure 3.11):
xICR 5 2 R sin ε
yICR 5 R cos ε
γi
xi
xi
xi
yi
yi
Q
yi
νi
i
Li
R
ICR(xICR,yICR)
ν
ηi
ε
ε
li
Φ
.
Figure 3.11 Geometry of wheel i with
reference to the coordinate frame Qxiyi.
Each wheel has its own total velocity vi
and angle γi.
94
Introduction to Mobile Robot Control

The geometry of each wheel is defined by its position ðxi; yiÞ and the orientation ai of
its rollers. Let Σi be the contact point of wheel i then:
ηi 5 tg21ðxi=yiÞ;
li 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
x2
i 1 y2
i
q
ð3:95Þ
where li is the same for all wheels due to the symmetry of the wheels with respect to the
point Q. The distance Li of the ICR and the contact point Σiðxi; yiÞ is given by the triangle
formula, as:
Li 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
l2
i 1 R2 2 2liR cos ðηi 1 εÞ
q
ð3:96Þ
The velocity vi of the wheel should be perpendicular to the line Li. Therefore, the angle
ζi of the line Li with the x-axis is determined by the relation:
tgζi 5 ðyICR 2 yiÞ=ðxICR 2 xiÞ
ð3:97Þ
whence:
γi 5 π=2 1 ζi
ð3:98Þ
because, in Figure 3.11, ζi is actually negative ðxICR 2 xi , 0Þ. Now, in view of
Eq. (3.94) we have:
jvij 5 Lij_φj
ð3:99Þ
Finally, using Eq. (3.99), Eq. (3.93) gives:
ωi 5 Li _φ cos ðai 2 γiÞ=r cos ai
ði 5 1; 2; 3; 4Þ
ð3:100Þ
for ai 6¼ π=2 1 kπ ðk 5 0; 1; 2; . . .Þ, where r is the common radius of the wheels. Equation
(3.100) gives the required rotational speeds of the wheels in terms of the position
ðxICR; yICRÞ of ICR and the desired rotational speed _φ of the robot. Actually, in view of
Eqs. (3.94) and (3.96), Eq. (3.100) gives ωi in terms of the known (desired) values of v,
_φ, xi, and yi.
Example 3.5
It is desired to describe a way for identifying the dynamic parameters of a differential-
drive WMR by converting its dynamic model to a linear form which uses the robot linear
displacement in place of xQ; yQ, and φ.
The nonlinear dynamic model of this WMR is given by Eqs. (3.30a,b) and (3.31) which
is written as:
_v 5 γ1u1;
u1 5 τr 1 τl
_ω 5 γ2u2;
u2 5 τr 2 τl
_xQ 5 v cos φ
_yQ 5 v sin φ
_φ 5 ω
95
Mobile Robot Dynamics

where γ1 5 1=mr and γ2 5 2a=Ir are the dynamic parameters to be identified. Using the
robot’s linear displacement:
l 5 xQ cos φ 1 yQ sin φ
in place of the components xQ and yQ (Figure 2.7) we get the linear model:
_v 5 γ1u1;
_ω 5 γ2u2;
_l 5 v;
_φ 5 ω
which reduces to:
€l 5 γ1u1;
€φ 5 γ2u2
ð3:101Þ
This is a linear model with two outputs l and φ. The identification will be made render-
ing the model (3.101) to the standard linear regression model:
y 5 MðuÞξ 1 e
ð3:102Þ
where:
y 5 ½y1; y2; . . .; ymΤ
ðthe vector of measurable signalsÞ
ξ 5 ξ1; ξ2; . . .; ξn

Τ
ðthe vector of unknown parametersÞ
e 5 e1; e2; . . .; em
½
Τ
ðthe vector of measurement errorsÞ
MðuÞ 5
μ11ðuÞ
. . .
μ1nðuÞ
μm1ðuÞ
. . .
μmnðuÞ . . .


ðan m 3 n known matrixÞ
The matrix MðuÞ is known as “regressor matrix.” Under the assumption that e is inde-
pendent of MðuÞ, and m . n the solution for ξ is given by3:
^ξ 5 ðMΤðuÞMðuÞÞ21MΤðuÞy
ð3:103Þ
To convert Eq. (3.101) into the regression form (3.102) we discretize it in time using
the first order approximation: dx=dtCðxk11 2 xkÞ=Τ, where xk 5 xðtÞt5kΤ
ðk 5 0; 1; 2; . . .Þ;
and Τ is the sampling period. Then, Eq. (3.101) becomes:
Δlk 5 Δlk21 1 ξ1u1;k
Δφk 5 Δφk21 1 ξ2u2;k
ð3:104Þ
3 Equation (3.103) can be found by minimizing the function JðξÞ 5 eΤe 5 ðy2MξÞΤðy 2 MξÞ with
respect to ξ. An easy way to get Eq. (3.103) is by multiplying Eq. (3.102) by MΤðuÞand solving for ξ,
under the assumption that MΤðuÞe 5 0 (e independent of MðuÞ,and rank MðuÞ 5 minðm; nÞ 5 n.
Actually, Eq. (3.103) is ^ξ 5 Myy where My is the generalized inverse of M (Eq. (2.8a)).
96
Introduction to Mobile Robot Control

where ξi 5 Τγi; ði 5 1; 2Þ. Clearly, the parameters ξ1 and ξ2 are identifiable, but the diffi-
culty is that l cannot be measured. To overcome this difficulty we use a second-order
parametric representation of xQðtÞ; yQðtÞ, and φðtÞ, namely [16]:
xQðμÞ 5 a2μ2 1 a1μ 1 a0;
yQðμÞ 5 b2μ2 1 b1μ 1 b0
tgφðμÞ 5 fðμÞ 5 dyQ=dμ
dxQ=dμ 5 2b2μ 1 b1
2a2μ 1 a1
with boundary conditions:
xQð0Þ 5 x0
Q 5 a0;
xQð1Þ 5 x1
Q 5 a2 1 a1 1 a0
yQð0Þ 5 y0
Q 5 b0;
yQð1Þ 5 y1
Q 5 b2 1 b1 1 b0
fð0Þ 5 tgφð0Þ 5 b1=a1
fð1Þ 5 tgφð1Þ 5 ð2b2 1 b1Þ=ð2a2 1 a1Þ
From the above conditions, the parameters ai and bi ði 5 0; 1; 2; . . .Þ are computed as:
a0 5 x0
Q;
a1 5 2ðtgφð1ÞÞðx1
Q 2 x0
QÞ 2 y1
Q 1 y0
Q
tgφð1Þ 2 tgφð0Þ
ð3:105aÞ
b0 5 y0
Q;
b1 5 a1tgφð0Þ;
b2 5 y1
Q 2 y0
Q 2 a1
ð3:105bÞ
The approximate length increment Δ^l of Δl is given by:
jΔ^lj 5
ð1
0
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
dxQ
dμ

2
1
dyQ
dμ

2
s
dμ 5
ð1
0
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
k2μ2 1 k1μ 1 k0
p
dμ
ð3:106aÞ
where:
k0 5 a2
1 1 b2
1; k1 5 4a1a2 1 4b1b2; k2 5 4a2
2 1 4b2
2
ð3:106bÞ
Equation (3.106a) is an integral equation, the close solution of which is available in
integral tables. The sign of Δl, which determines whether the robot moved forward or
backward, can be determined by the following relation:
Δl 5 l1 2 l0 5 ðx1
Q cos φð0Þ 1 y1
Q sin φð0ÞÞ 2 ðx0
Q cos φð0Þ 1 y0
Q sin φð0ÞÞ
where x1
Q; y1
Q is the current position and x0
Q; y0
Q is the past position of the robot. Several
numerical experiments performed using the above WMR identification method gave very
satisfactory results for both Δl and Δφ [16].
Example 3.6
Outline a method for applying the least squares identification model (3.102)(3.103) to
the general nonholonomic WMR model (3.19ab)
97
Mobile Robot Dynamics

Solution
The model (3.19a,b):
DðqÞ_v 1 Cðq; _qÞv 1 gðqÞ 5 Eτ
is put in the form:
_v 5 Mðv;τÞξ
where ξ is the vector of unknown parameters The above model can be written in the form
(3.102) by computing yk as:
yk 5 vk 2 vk21 5
ð
kΤ
k21
ð
ÞΤ
½Mðv; tÞ; τðtÞdt
0
B
@
1
C
Aξ
ð3:107Þ
where vk5vðtÞ t5kT and T is the sampling period of the measurements.
Now, defining yN and MN as:
yN 5 ½y1; y2; . . .; yNΤ
MN 5 Ð T
0 Mdt; Ð 2T
0 Mdt; . . .; Ð NT
ðN21ÞT Mdt
h
iT
The model (3.107), after N measurements, becomes:
yN 5 MNξ
ð3:108Þ
This method overcomes the noise problem posed by the calculation of the acceleration.
The optimal estimate ^ξ of ξ is given by Eq. (3.103):
^ξ 5 ðM
Τ
NMNÞ
21M
Τ
NyN
ð3:109Þ
The identification process can be simplified if it is split in two parts, namely: (i) identifi-
cation when the robot is moving on a line without rotation, and (ii) identification when the
robot has pure rotation movement. In the pure linear motion we keep the angle φ constant
(i.e., _φ 5 0), and in the pure rotation motion we have vQ 5 0 (i.e., _xQ 5 0 and _yQ 5 0) [17].
References
[1] McKerrow PK. Introduction to robotics. Reading, MA: Addison-Wesley; 1999.
[2] Dudek G, Jenkin M. Computational principles of mobile robotics. Cambridge:
Cambridge University Press; 2010.
[3] De Villiers M, Bright G. Development of a control model for a four-wheel mecanum
vehicle. In: Proceedings of twenty fifth international conference of CAD/CAM robotics
and factories of the future conference. Pretoria, South Africa; July 2010.
[4] Song JB, Byun KS. Design and control of a four-wheeled omnidirectional mobile robot
with steerable omnidirectional wheels. J Rob Syst 2004;21:193208.
98
Introduction to Mobile Robot Control

[5] Sidek SN. Dynamic modeling and control of nonholonomic wheeled mobile robot sub-
jected to wheel slip. PhD Thesis, Vanderbilt University, Nashville, TN, December
2008.
[6] Williams II RL, Carter BE, Gallina P, Rosati G. Dynamic model with slip for wheeled
omni-directional robots. IEEE Trans Rob Autom 2002;18(3):28593.
[7] Stonier D, Se-Hyoung C, SungLok C, Kuppuswamy NS, Jong-Hwan K. Nonlinear
slip dynamics for an omniwheel mobile robot platform. In: Proceedings of IEEE
international conference on robotics and automation. Rome, Italy; April 1014, 2007.
p. 236772.
[8] Ivanjko E, Petrinic T, Petrovic I. Modeling of mobile robot dynamics. In: Proceedings
of seventh EUROSIM congress on modeling and simulation. Prague, Czech Republic;
September 69, 2010. p. 47986.
[9] Moret EN. Dynamic modeling and control of a car-like robot. MSc Thesis, Virginia
Polytechnic Institute and State University, Blacksburg, VA, February 2003.
[10] Watanabe K, Shiraishi Y, Tzafestas SG, Tang J, Fukuda T. Feedback control of an
omnidirectional autonomous platform for mobile service robots. J Intell Rob Syst
1998;22:31530.
[11] Pin FG, Killough SM. A new family of omnidirectional and holonomic wheeled plat-
forms for mobile robots. IEEE Trans Rob Autom 1994;10(4):4809.
[12] Rojas R. Omnidirectional control. Freie University, Berlin; May 2005. ,http://
robocup.mi.fu-berlin.de/buch/omnidrive.pdf. .
[13] Connette CP, Pott A, Hagele M, Verl A. Control of a pseudo-omnidirectional, non-
holonomic, mobile robot based on an ICM representation in spherical coordinates. In:
Proceedings of 47th IEEE conference on decision and Control. Canum, Mexico;
December 911, 2008. p. 497683.
[14] Moore KL, Flann NS. A six-wheeled omnidirectional autonomous mobile robot. IEEE
Control Syst Mag 2000;20(6):5366.
[15] Kalmar-Nagy T, D’Andrea R, Ganguly P. Near-optimal dynamic trajectory generation
and control of an omnidirectional vehicle. Rob Auton Syst 2007;46:4764.
[16] Cuerra PN, Alsina PJ, Medeiros AAD, Araujo A. Linear modeling and identification
of a mobile robot with differential drive. In: Proceedings of ICINCO international
conference on informatics in control automation and robotics. Setubal, Portugal; 2004.
p. 2639.
[17] Handy A, Badreddin E. Dynamic modeling of a wheeled mobile robot for identifica-
tion, navigation and control. In: Proceedings of IMACS conference on modeling and
control of technological systems. Lille, France; 1992. p. 11928.
99
Mobile Robot Dynamics

4 Mobile Robot Sensors
4.1
Introduction
The sensors designed for robots resemble the sensors of the human sensory system
(e.g., vision, hearing, kinesthetic) that provide input signals to the brain for proces-
sing, utilization, and action. The use of sensors in robotics is of paramount impor-
tance for closing the feedback control loops that secure efficient and automated/
autonomous operation of robots in real-life applications. Sensing methods provide
the robots (fixed, mobile, and hybrid) with higher level and intelligence capabilities
that go far beyond the “preprogrammed” style of operation through repetitive exe-
cution of a set of programmed tasks.
The aim of this chapter is to provide a conceptual introduction to a number of
important sensors used in both fixed and mobile robots (wheeled mobile robots
(WMRs), mobile manipulators, humanoid robots). The particular objectives of the
chapter are the following:
G
To provide a popular classification of sensors, along with their operational features
G
To discuss sonar, laser, and infrared sensors
G
To present an outline of robotic vision and its principal functions (including omnidirec-
tional vision)
G
To list the operation principles of gyroscope, compass, and force/tactile sensors
G
To give a brief introduction to the global positioning system
In compatibility with the book’s scope, the material of the chapter is presented at
an introductory descriptive level. Physical, design, and operational details are pro-
vided in textbooks dedicated to sensors [19]. The use of sensors in some
representative mobile robot applications can be found in Refs. [1022].
4.2
Sensor Classification and Characteristics
4.2.1
Sensor Classification
In general, robotic sensors are distinguished into
G
analog sensors
G
digital sensors
Analog sensors provide analog output signals which need analog-to-digital
(A/D) conversion. Examples of analog sensors are analog infrared distance sensor,
microphone, and analog compass.
Introduction to Mobile Robot Control. DOI: http://dx.doi.org/10.1016/B978-0-12-417049-0.00004-3
© 2014 Elsevier Inc. All rights reserved.

Digital sensors are more accurate than analog sensors, and their outputs
may be of different form. For example, they may have a “synchronous serial”
form (i.e., bit by bit data reading), or a “parallel form” (e.g., 8 or 16 digital
output lines).
In all cases, the desired sensor “features” are high resolution, wide operation
range, fast response, easy calibration, high reliability, and low cost (for purchase,
support, maintenance). The sensors include also the visual cameras with all their
auxiliary equipments needed in computational or artificial vision.
The transfer of data from the sensor to the computer (CPU) can be initiated by
the computer (called pollin) or by the sensor itself (via interrupt). In the CPU-
initiated case, the CPU must properly check whether the sensor is ready, by reading
a status line in the loop. In the sensor initiating case, the availability of an interrupt
line is needed. The second way is much faster since once an interrupt (indicating
that data is ready) is made, the CPU reacts immediately to this request.
The sensory systems for robot positioning can be grouped in the following
categories:
G
Mechanical
G
Acoustic
G
Electromagnetic
G
Magnetic
G
Optical
Some of them are suitable for a position measurement of a stationary robot,
whereas others are suitable for a position measurement during a motion of the
robot. Mechanical systems require a physical contact between the robot and the
sensor. Frequently, they are integrated in the robot body. Acoustic and electromag-
netic sensors use the directionality and the time-of-flight measurement of sent and
received signals in order to compute the angular position and the linear position of
the object of interest.
Acoustic systems employ ultrasound frequencies, and electromagnetic sensor
systems include optical, laser, and radar equipment. In both cases, a free “line of
sight” between the transmitter and the receiver is required. Magnetic sensors
employ the spatial configuration of static magnetic fields of the Earth and solenoids
for the calculation of the position. Finally, optical sensors use appropriate vision
cameras (monocular, binocular, omnidirectional).
Some further classifications of robot sensors are the following:
From a robot viewpoint:
G
On-board (local) sensors, that is, sensors mounted on the robot
G
Global sensors, that is, sensors mounted outside the robot in the environment and
sending the sensor data back to the robot.
From a passive/active viewpoint:
G
Passive sensors, that is, sensors monitoring the environment without affecting it (e.g.,
gyroscope, vision camera)
G
Active sensors, that is, sensors that stimulate the environment for monitoring it (e.g.,
infrared sensor, laser scanner, sonar sensor).
102
Introduction to Mobile Robot Control

Mobile robots’ sensors:
G
Internal (proprioceptive) sensors, that is, sensors that monitor the robot internal state
G
External (exteroceptive) sensors, that is, sensors that monitor the robots environment.
Internal sensors include the sensors that measure motor speed, wheel load, robot
arm joint angles, and battery voltage. External sensors include sensors that measure
distance, sound amplitude, and light intensity. Examples of passive sensors include
temperature probes, microphones, and charge-coupled device (CCD) or CMOS
cameras. Examples of active sensors, which emit energy into the environment, are
wheel quadrature encoders and laser range finders.
4.2.2
Sensor Characteristics
The sensors used in stationary or mobile robotics have a variety of performance charac-
teristics. These characteristic features are different in controlled environments (indoor,
laboratory
environments)
and
noncontrolled
environments
(outdoor,
real-world
environments).
The basic features of sensors are as follows:
G
Dynamic range (i.e., the spread between the lower and upper limits of input values for which
the sensor is working normally). Typically, in order to cover both very small and very large
signal ranges use is made of the logarithm of the ratio of the maximum and minimum values,
that is,
Range 5 20 log Maximum input
Minimum input


ðdBÞ
For example, if the interval of voltage values measured by a voltmeter is between
Vmin 5 1 mV and Vmax 5 20 V, then,
Range 5 20 log Vmax
Vmin
5 20 log
20
0:001


5 86 dB
G
Resolution (i.e., the minimum difference of the measured variable that can be recognized
by the sensor). Typically, in analog sensors, the resolution coincides with the lower limit
of the sensor’s operational range. However, this is not true in digital sensors, where the
analog input is converted into binary form.
G
Linearity (i.e., the property that the output value fðx 1 yÞ of a sensor to a sum of inputs
x and y is equal to the sum fðxÞ 1 fðyÞ of the output values of the sensor obtained sepa-
rately
by
each
input).
In
a
more
general
formulation,
linearity
implies
that
fðk1x 1 k2yÞ 5 k1fðxÞ 1 k2fðyÞ, where k1 and k2 are constant parameters.
G
Bandwidth (i.e., the maximum rate or frequency of readings/data the sensor can provide).
The number of readings (measurements) per second provides the sensor’s frequency
measured in hertz.
Other features that are important for nonlaboratory environments include the
following:
G
Sensitivity (i.e., the degree to which the changes of the input signal affect the output signal).
103
Mobile Robot Sensors

G
Accuracy (i.e., in what degree the sensors reading coincides with the true value of the
input). If e 5 y 2 x 5 sensor reading 2 true value, then accuracy is given by
Accuracy 5 1 2 jerrorj=x
G
Precision (which applies in case the errors are random). Precision is defined as
Precision 5 Range=σ
where σ is the standard deviation of the error from a mean value m.
4.3
Position and Velocity Sensors
4.3.1
Position Sensors
The position sensors (or position transducers) are used to determine whether the
joints (linear/rotational axes) of robotic links or mobile platforms have moved in
the correct position, which drives the end effectors or mobile platform in the
desired position/orientation of the Cartesian space. Similarly, the velocity sensors
measure the speed of the motion (linear, angular) of the robot’s joints or platform.
The three basic position sensors are the following:
1. Potentiometers
2. Resolvers
3. Encoders
The potentiometer (linear or angular) is a device that gives an output V0ðtÞ pro-
portional to the position of the pointer, that is, V0 5 KPΘðtÞ, where Kp is the poten-
tiometers coefficient.
The resolver is an analog sensor that gives an output proportional to the rotation
angle of another object with respect to a fixed element. In the simplest case, a
resolver has a simple winding in the rotor and a couple of windings in the stator
that have a relative angle of 90. If the rotor receives a signal A sinðωtÞ, the vol-
tages at the two terminals of the stator are
Vs1ðtÞ 5 A sinðωtÞsin θ
Vs2ðtÞ 5 A sinðωtÞcos θ
where θ is the rotation angle of the rotor with respect to the stator.
The encoders are distinguished into differential (or incremental) encoders and
absolute encoders. Encoders constitute a basic feedback sensor for motor control.
The two typical techniques for building an encoder is the Hall effect sensor (mag-
netic sensor) and the optical encoder (a sector disk with black and white segments
together with a light-emitting diode (LED) and a photo-diode; Figure 4.1A). The
photodiode detects reflected light during a white segment, but not during a black
104
Introduction to Mobile Robot Control

segment. Therefore, if this disk has 16 white and 16 black segments, the sensor
will receive 16 pulses during a revolution.
The differential (or quadrature) encoder is the most common feedback device
of robots. An encoder is mounted on each joint (motor) axis.
A differential encoder has a second track added to generate a pulse that occurs
once per revolution (index signal) which is used to indicate an absolute position
(Figure 4.1B). To obtain information about the direction of rotation, the lines on
the disk are read out by two different photodiode elements that look at the disk pat-
tern with mechanical shift of one-fourth the pitch of line pair between them. As the
disk rotates, the two photodiodes generate signals that have a 90 phase difference.
These two signals are usually called the quadrature A and B signals (Figure 4.1B).
The clockwise direction is typically defined as positive when the channel A moves
before the channel B.
The disk of an absolute encoder is patterned with a number of discrete tracks
which correspond to the word length (Figure 4.1C). The pattern shown in
Figure 4.1C for this 4-bit absolute encoder is the so-called Gray (or reflected
binary) code.
4.3.2
Velocity Sensors
Typically, the velocity of robotic joints is measured directly with the aid of tach-
ometers. The indirect way of velocity measurement through numerical differentia-
tion of the position signal is not preferred because of the occurring differentiation
noise. Tachometers are distinguished in direct current (DC) and alternate current
(AC) tachometers. In robotics, the DC tachometer is mostly used (Figure 4.2).
0
G0
G1
G2
G3
15
Photo 
detector
(A)
(B)
(C)
Light 
source
Shaft
Rotating disk
Channel A
Channel B
Index
Code track
Figure 4.1 (A) An encoder disk with 16 white and 16 black segments. (B) A differential
encoder with an index track. (C) A 4-bit absolute encoder.
105
Mobile Robot Sensors

The DC tachometer (tachogenerator) develops a DC voltage, at its output, which
is proportional to the speed of the motor connected to it. The permanent magnetic
field (permanent magnet) used eliminates the need of external excitation and offers
very reliable and stable outputs. DC tachometers use a commutator and so a small
ripple appears in the output that cannot be filtered out entirely. The accuracy of the
tachogenerator determines the maximum resolution of speed measurement. The
ripple does not appear in AC tachometers.
4.4
Distance Sensors
Sensors which measure the distance of a robot with the obstacles around it include
the following:
G
Sonar sensors
G
Laser sensors
G
Infrared sensors
4.4.1
Sonar Sensors
Sonar sensors (or, simply, sonars from sound and navigation and ranging sensors)
have a relatively narrow cone (Figure 4.3) and so for a 360 coverage, a typical
mobile robot sensor configuration is to use 24 sensors, each one mapping a cone of
about 15 each. Actually, there are available for selected commercial sonars with a
wide range of cone angles, such as to fit all possible practical applications.
The operation principle of sonar sensors includes the emission of a short
acoustic signal (of duration about 1 ms) at an ultrasonic frequency of 50250 kHz
and the measurement of the time from signal emission until the echo returns to the
sensor. The measured time-of-flight is proportional to twice the distance of the
nearest obstacle in the sensor cone. If no signal is received within a maximum time
period,
then
no
obstacle
is
detected
within
the
corresponding
distance.
Measurements are repeated about 20 times per second (which correspond to its typ-
ical clicking sound).
As shown in Figure 4.3D, the sonar sensors beam is not fully confined to a
narrow cone because of the presence of side lobes which, if reflected first, might
Figure 4.2 A DC tachometer.
106
Introduction to Mobile Robot Control

confuse the interpretation of the time-of-flight information. Very often, the actual
scenes appear differently on the observer’s perspective. For example, in case a
sonar sensor is mounted on a mobile robot, due to the relatively wide beam width,
important characteristics of the environment (obstacles, doors, etc.) only show up
when the robot is close enough to observe them. The distance L of an object that
causes the reflection of the wave is given by
L 5 1
2 vst0
where vs is the speed of sound (about 330350 m/s in air) and t0 is the time-of-
flight.
From physics we know that the speed vs of sound in air is given by
vs 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
γRT
p
where R is the gas constant, γ is the ratio of specific heat, and T is the absolute
temperature ðKÞ. The effective range of most sonars used in mobile robotics varies
approximately between 12 cm and 5 m, with 9899.1% accuracy.
(D)
(E)
30°
0
R
330
300
270
30
60
90
(B)
(C)
(A)
Figure 4.3 (A) A commercial sonar sensor (MPC). (B) A cone of about 30 is mapped
by each sonar sensor. (C) If the obstacle in front of the sensor is far from perpendicular,
then no signal will be reflected toward the sensor and so the object will not be detected.
(D) Typical beam intensity pattern. (E) 360 scan in an environment with several obstacles.
Source: (D) Reprinted from Ref. [22], with permission of Elsevier Science Ltd.;
(E) Courtesy of N. Katevas.
107
Mobile Robot Sensors

The equivalent beam angle δ and the 3-dB angle θ3dB in degrees, that is, the
angle between the lines that correspond to the half-intensity direction on either side
of the main lobe axis, are related by the following equations:
a 5 1:6=μ sinðθ3dB=2Þ
δ 5 5:78=ðμaÞ2
ðsteradiansÞ
or
δ 5 10 log
5:78
ðμaÞ2


ðdBÞ
where
μ 5 2π=λ
is the wave number, λ is the wavelength in meters, and a is the active radius of a
circular transducer.
4.4.2
Laser Sensors
Laser proximity sensors constitute a special case of optical sensors that have a
range from centimeters to meters. They are commonly called laser radars (or
lidars 5 light direction and ranging sensors). Energy is emitted at impulses. The
distance is computed from the time-of-flight. They can also be employed as laser
altimeters for obstacle avoidance or for vehicle detection in highways. A typical
laser range finder is shown in Figure 4.4.
Figure 4.4 (A) A laser range finder (SICK LMS 210). (B) SICK LRF mounted on a
P2AT WMR.
Source: http://www.ai.sri/centibots/tech_design/robot.html.
108
Introduction to Mobile Robot Control

Laser sensors supply speed and height. Rotary lasers are based on rotation of
the wave 360 at more than 1 or 2 rpm, and a mirror at 45. This overcomes the
problem of hidden areas. Very often, they work in conjunction with reflecting
beacons. Rotary lasers measure angular position. Scanning laser range finders
combine the lidar with the rotary lasers by providing both range and angular posi-
tion of the detected object. They do not need reflecting beacons and so they are
very useful because they can work in unstructured environments. Unfortunately,
laser sensors are very large and heavy (and also too expensive) for small mobile
robots. For this reason, infrared distance sensors are very popular in mobile
robots. The time-of-flight measurement in a laser range finder is done using a
pulsed laser and measuring the elapsed time directly as in the sonar sensor. The
easiest way is to measure the phase shift of the reflected light—as explained in
Section 4.4.3.
4.4.3
Infrared Sensors
Near-infrared light can be produced by an LED or a laser. The typical wavelength
of the infrared light emitted is somewhere between 820 and 880 nm. Therefore,
most surfaces have a roughness greater than the wavelength of the incident light
and so diffuse reflection takes place, that is, the light is reflected almost isotropi-
cally. The component of the infrared light that falls within the aperture of the sen-
sor
returns
almost
parallel
to
the
transmitted
beam
for
distant
objects
(Figure 4.5A).
The sensor sends a 100% amplitude modulated light at a given frequency f and
measures the phase shift between the transmitted and the reflected signals. If φ is
the electronically measured phase shift and λ the wavelength, then the distance 2D
is equal to ðφ=2πÞλ, that is, (Figure 4.5B)
D 5
λ
4π


φ
A real infrared sensor in front of a wall obstacle is shown in Figure 4.6.
Transmitted beam
Reflected beam
D
Transmiter
(V)
(m)
Transmitted 
Reflected 
T
0
φ
Phase
measurement 
Figure 4.5 (A) Infrared distance measurement use the phase-shift method. (B) Phase shift
between transmitted and received signals.
109
Mobile Robot Sensors

4.5
Robot Vision
4.5.1
General Issues
Robot vision is the ability of a robot to see and recognize objects via the collection
into an image of the light reflected by these objects, and then interpreting and pro-
cessing this image. Robot vision employs optical or visual sensors (cameras) and
proper electronic equipment to process/analyze visual images and recognize objects
of concern in each robotic application.
An ideal pinhole camera model is used to represent an ideal lens, and assumes
that rays of light travel in straight lines from the object through the pinhole to the
image (sensor) plane. Pinhole camera is the simplest device that captures accurately
the geometry of perspective projection. The pinhole is an infinitesimally small
aperture. The image of the object is formed by the intersection of the light rays
with the image plane (Figure 4.7). This mapping from the three dimensions onto
two dimensions is called perspective projection. The best radius r of the pinhole is
approximately equal to rC
ﬃﬃﬃﬃﬃﬃ
λd
p
, where λ is the wavelength of the light and d the
distance of the sensor from the pinhole.
Lens distortions displace image points from their ideal “pinhole model” loca-
tions on the sensor plane. Lens distortions are distinguished into the following:
G
Radial (displacements toward the center image or away from it)
G
Tangential (these displacements occur at right angles to the radial direction and usually
are much smaller than radial displacements)
G
Asymmetric radial or tangential (here the error functions vary for different locations on
the image plane)
G
Random (these displacements cannot be mathematically modeled)
The process of correcting the image displacements that occur due to elements of
the camera’s interior orientation is called camera calibration. The ideal pinhole
camera model can be achieved by very accurate and expensive lenses. A stereo
Figure 4.6 Infrared sensor mounted on a mobile robotic platform (infrared sensors can be
used for obstacle avoidance, line following, and map building).
Source: http://www.trossenrobotics.com/c/robot-IR-sensors.aspx.
110
Introduction to Mobile Robot Control

camera has two or more lenses with a separate image sensor or film frame for
each lens and can simulate human binocular vision. Thus, it has the ability to cap-
ture 3-D images, a process called stereophotography. The distance between the
lenses is typically near-equal to the distance between the humans eyes. A pinhole
camera and a stereocamera are shown in Figure 4.8A and B.
Image (electronic image) is an array of pixels (picture elements) that has been
digitized into the memory of a computer. A binary number is stored in each pixel
to represent the intensity and the wavelength of the light falling on that part of the
image.
A pixel, which is a vector of binary numbers, represents a particular color. A
digital image which is a matrix of pixels (vectors) filled with corresponding colors
creates a picture of the scene from the point of view of the camera. Cameras used
in robotic vision include primarily television cameras that consist either of a tube
or a solid-state imaging sensor, and the associated electronics. A common represen-
tative of tube family of television cameras is the vidicon tube. The principal repre-
sentative of solid-state imaging sensors is the so-called charge-coupled device.
d
Figure 4.7 Schematic of pinhole camera geometry (for d 5 50 mm and λ 5 550 nm, we get
r 5 0:165 mm).
Figure 4.8 (A) A commercial pinhole camera mounted on a robot. (B) A typical stereo camera.
Source: http://www.caminax.com, http://www.benezin.com/3d/.
111
Mobile Robot Sensors

Solid-state imaging devices possess a number of advantages over tube cameras,
e.g., smaller weight, smaller size, longer life, and lower power consumption. But,
the resolution of some video tubes is still beyond the capabilities of CCDs.
In vidicon tubes, the electron beam scans the entire surface of the object
30 times per second. Each complete scan (called a frame) consists of 525 lines
of which 480 lines contain image information. Sometimes, the frame consists of
559 lines with 512 lines containing the image data.
CCD devices are distinguished in line-scan sensors and area sensors. The funda-
mental component of a line-scan CCD sensor is a row of silicon imaging elements
called photosites. Image photons pass through a transparent polycrystalline silicon
gate structure and are absorbed in the silicon crystal, creating electronhole pairs.
The resulting photoelectrons are collected in the photosites, with the amount of
charge collected at each photosite being proportional to the illumination intensity
at that location. Line-scan cameras give only one line of an input image and so
they are ideally suited for applications in which objects are moving past the sensor
(as in conveyor belts). The resolution of line-scan sensors ranges between 256 and
2048 elements. The resolution of medium-resolution area sensors range between
32 3 32 elements at the low end and 256 3 256 elements. Current CCD sensors are
capable to achieve a resolution of 1024 3 1024 elements or higher.
It is obvious that cameras actually convert 2-D or 3-D reality into 2-D represen-
tations, although via postprocessing they can reconstruct 3-D representations from
the initial 2-D images. Cameras are not error-free devices. Broadly speaking, the
camera distortions are distinguished into the following:
G
Geometric (objects’ representation shifting on the sensor plane)
G
Radiometric (errors in “brightness values” of the pixels due to variations of pixels
sensitivity)
G
Spectral (errors in pixel “brightness values” due to the varying response of the sensor to
different wavelengths of light)
Robot (computer) vision can be grouped in the following principal subareas:
G
Sensing
G
Preprocessing
G
Segmentation
G
Description
G
Recognition
G
Interpretation
4.5.2
Sensing
The sensing subarea of robotic vision includes the following (besides the design of
cameras in their own):
G
Camera calibration
G
Image acquisition
G
Illumination
G
Imaging geometry
112
Introduction to Mobile Robot Control

4.5.2.1
Camera Calibration
Camera calibration, the first requirement in sensing, is concerned with the correc-
tion of image displacements that occur due to the characteristics of the camera’s
interior orientation. Lens distortion is one of the reasons of the displacement of
image points from their ideal “pinhole model” positions on the sensor plane. The
methods of camera calibration are classified as follows:
G
Model-based methods (where specific dominant features of the error are modeled and
corrected)
G
Mapping-based methods (where a proper reality-to-image or image-to-reality mapping is
produced, without the need to understand the underlying causes)
A basic contributing factor which is modeled is the radial lens distortion. The
model typically used is a second-, third-, or fourth-order order polynomial. Then,
least squares or interpolating techniques are used to determine the values of the
coefficients that lead to a best model of the observed error.
In the mapping-based approach, no attempt is made to understand the particular
causes of the error. All that matters is to have an explicit image-to-reality mapping
preserved.
In practice, both approaches to camera calibration can remove most of the dis-
placement errors caused by elements of interior camera orientation, prior to image
preprocessing.
4.5.2.2
Image Acquisition
As described in Section 4.5.1, visual information is converted to electrical signals
by visual sensors and associated electronic equipment. When sampled spatially and
quantized in amplitude, these visual signals yield a digital image.
Three basic topics that are included in image acquisition are as follows:
1. Imaging techniques
2. Effects of sampling on spatial resolution
3. Effects of amplitude quantization on intensity resolution
Let fðx; yÞ be a 2-D image, which for computer processing is digitized both spa-
tially and in amplitude (intensity). Digitization of the spatial coordinates x and y is
known as image sampling, while the amplitude digitization is called intensity or
gray-level quantification (for monochrome images, i.e., blackwhite variations in
shades of gray).
The digitized form of fðx; yÞ (called digital image) is represented by a matrix of
the type
fðx; yÞC
fð0; 0Þ
fð0; 1Þ
. . .
fð0; M 2 1Þ
fð1; 0Þ
fð1; 1Þ
. . .
fð1; M 2 1Þ
^
^
&
^
fðN 2 1; 0Þ
fðN 2 1; 1Þ
. . .
fðN 2 1; M 2 1Þ
2
664
3
775
113
Mobile Robot Sensors

where
now
x
and
y
have
discrete
values
at
x 5 0; 1; 2; . . .; N 2 1
and
y 5 0; 1; 2; . . .; M 2 1. Each element in the array is what we call a pixel or
image element. Clearly, fð0; 0Þ represents the pixel at the origin of the image,
fð0; 1Þ the next pixel to the right, and so on. The value fðx; yÞ at any point
ðx; yÞ represents the intensity of the image fðx; yÞ at that point. The greater N
and M are the better (of higher resolution) image representation obtained
(Figure 4.9).
4.5.2.3
Illumination
Illumination of a scene is a critical factor that affects the complexity of the vision
algorithms. A good lighting system illuminates a scene so as to minimize the com-
plexity of the produced image and improve (enhance) the information needed for
object detection and extraction. The four typical illumination techniques used in
robotics are as follows:
1. Diffuse lighting (for objects with smooth and regular surfaces)
2. Back lighting (for applications where silhouettes of objects are sufficient for recognition
or other purposes)
3. Structured lighting (i.e., projection of points, stripes, or grids onto the work surface, e.g.,
when a block is illuminated by parallel light planes that become light stripes when inter-
secting a flat surface)
4. Directional
lighting
(where
a
highly
directed
light
(or
laser)
beam
is
used
to inspect object surfaces and detect defects on the surface such as pits and
scratches)
2× 2
5 ×5
10 ×10
20 ×20
50× 50
100×100
Figure 4.9 Sequence of increasing number of pixels and the corresponding image
resolution.
Source: http://patriottruckleasing.com/resolution.html.
114
Introduction to Mobile Robot Control

4.5.2.4
Imaging Geometry
The relation of the position of the point fðx0; y0Þ in the 2-D image plane and the
corresponding point Fðx0; y0; z0Þ in the real 3-D space is determined by the laws of
optics. Because the aperture of camera lenses via which the light falls onto the image
(sensor) plane is very much smaller than the size of the objects under study, one can
replace the lenses by the pinhole model. Thus, points from space are projected onto
the image plane by lines intersecting in a common point which is known as center of
projection. In the pinhole camera model, the center of projection is located at the
center of lenses1. The camera itself represents a rigid body to which a coordinate
frame is assigned. This coordinate frame describes the poses of the camera.
Figure 4.10A shows the basic model of the perspective (or imaging) transformation.
Camera
frame 
Object point 
(x,y,z)
O
Image plane
(inverted) 
y
x
z
Optical
axis 
z
r
lf
lf
r′
x′
y′
y′
(x′,y′)
(x′,y′)
Camera 
frame
Object point
(x,y,z)
O
Equivalent
Image plane  
y
x
z
z
r
r ′
x′
A′
B
C′
x
y
(A)
(B)
B
C
A
Figure 4.10 (A) Perspective imaging projection (called reverse perspective projection).
The camera coordinate frame ðx; y; zÞ is aligned with the world frame. In the camera frame
the focal length lf has negative value. (B) Equivalent forward image plane, symmetrical to
real image plane with respect to the origin of the camera frame.
1 It is recalled that the equation of a convex lens is 1=lo 1 1=lim 5 1=lf where lf is the focal length (i.e.,
the distance of the focus from the lens) and lo, lim are the distances of the object and the image from
the lens, respectively (see Section 4.8).
115
Mobile Robot Sensors

The optical axis (i.e., the line through the 3-D point O and perpendicular to the
image plane) is along the z-axis. The distance between the image plane and the
center of projection O is the focal length lf (e.g., the distance between the lens and
the CCD array). The intersection of the optical axis with the image plane is called
the principal point (or image center). Note that the principal point is not always the
“actual” center of the image. To derive the geometric relations between the 3-D
object and its 2-D image, it is convenient to work with the equivalent image plane
shown in Figure 4.10B, which is symmetrical to the real image plane with respect
to the origin of the camera frame and has a positive focal length. In Figure 4.10B,
we use the following notations:
ðx; y; zÞ ! ðX; Y; ZÞ; r ! R
ðx0; y0; z0Þ ! ðx; y; zÞ; r0 ! r
Using the similar triangles OA0B0 and OAB we get
lf
Z 5 r
R
Similarly, from the similar triangles A0B0C0 and ABC we obtain
x
X 5 y
Y 5 r
R
From the above relations, we get the following forward perspective projection
equations:2
x 5 lfX
Z ;
y 5 lfY
Z ;
z 5 lf
which are nonlinear (because they involve division by Z). However, using
homogeneous representations, we can obtain a forward linear perspective
transformation from λ½X; Y; Z; 1Τ to ½x; y; z; 1Τ, where λ is a scaling factor,
namely,
x
y
z
1
2
664
3
775 5
lf
0
0
0
0
lf
0
0
0
0
lf
0
0
0
1
0
2
664
3
775λ
X
Y
Z
1
2
664
3
775
2 The
reverse
perspective
equations
can
be
found
from
Figure
4.10A,
and
are:
x 5
2 lf X=ðZ 2 lf Þ 5 lf X=ðlf 2 ZÞ;
y 5 2 lf Y=ðZ 2 lf Þ 5 lf Y=ðlf 2 ZÞ; z 5 2 lf
116
Introduction to Mobile Robot Control

with λ 5 1=Z. Thus, our forward linear perspective transformation matrix is
P 5
lf
0
0
0
0
lf
0
0
0
0
lf
0
0
0
1
0
2
664
3
775
It is now easy to verify that by simply using P21 (the inverse) we cannot recover
a 3-D point from its image. This can be done only if at least one of the coordinates
of the 3-D point is known.
4.5.3
Preprocessing
Image preprocessing can be performed by two general methodologies:
1. Spatial domain methodology
2. Frequency domain methodology
Spatial domain preprocessing methods work directly with image pixel arrays. In
general, spatial domain preprocessing functions have the form
pðx; yÞ 5 Hðfðx; yÞÞ
where fðx; yÞ is the input image, HðÞ is an operator on fðx; yÞ, and pðx; yÞ is the out-
come of preprocessing (i.e., the preprocessed image). In the simplest case, H has
the form of an intensity mapping I, that is,
u 5 IðvÞ
where v and u denote the intensities of fðx; yÞ and pðx; yÞ, respectively. Another typ-
ical image preprocessing technique is the so-called window (or template or filter)
technique which uses convolution masks.
Frequency domain preprocessing methods use the Fourier transform of an image
which converts the image to an aggregate of complex-valued pixels. To reduce the
noise and other spurious effects resulting from the operations of sampling, quanti-
zation transmission, and any other disturbances of the environment, appropriate
smoothing operations are employed. For example,
G
Neighborhood averaging (where the smoothed image is obtained by averaging the inten-
sity values of the pixels contained in a predefined region around ðx; yÞ)
G
Median filtering (where the smoothed image is obtained by using the median instead of
the average of the pixels in the desired surrounding region)
Other preprocessing operations include the following:
G
Image enhancement (automatic adaptation to illumination variations)
G
Edge detection (this is the preliminary step for numerous detection algorithms)
G
Image thresholding (i.e., the selection of a threshold T that separates the intensity modes,
e.g., in images that have intensities that are grouped into two dominant models, viz., light
objects on a dark background)
117
Mobile Robot Sensors

4.5.4
Image Segmentation
Image segmentation is the process that splits a source image into its constituent
parts or objects. The regions of interest are selected on the basis of several criteria.
For example, it may be necessary to find a single part out of a bin. For navigation
purposes, it may be useful to extract only floor lines from an image. In general, by
segmentation, objects are extracted from a scene for subsequent recognition and
analysis.
The two basic principles used in segmentation algorithms are as follows:
1. Discontinuity (e.g., edge detection)
2. Similarity (e.g., thresholding, region growing)
These processes can be applied to both static and time-varying scenes. Edge
detection is based on intensity discontinuities and yield pixels that are on the
boundary between the objects and the background. However, the boundaries
detected in this way are not sharp in many cases due to noise and other spuri-
ous effects. This is avoided by using linking and other boundary detection meth-
ods by which edge pixels are assembled into a proper and meaningful set of
objects.
4.5.5
Image Description
Image description is the process of extracting features from an object for recogni-
tion purposes. Descriptors must be independent of the size, location, and orienta-
tion,
and
provide
sufficient
discriminatory
information.
Typical
boundary
descriptors are as follows:
G
Chain codes (these represent a boundary as a set of straight line segments of specified
length and direction)
G
Polygonal approximations (here a digital boundary can be approximated to any desired
degree by a polygon)
G
Fourier descriptors (here, a 2-D boundary can be represented by 1-D transform, that is, a
point ðx; yÞ is reduced to the complex number x 1 jy)
Typical regional descriptors include the following:
G
Texture (this provides quantitative measures of properties such as smoothness, regularity,
and coarseness)
G
Region skeleton (here a thinning algorithm (or skeletonizing algorithm) is used to obtain
the skeleton of the region)
G
Moment invariants (here normalized central moments of order p 1 q are used as descrip-
tors because they are invariant to translation, rotation, and scaling)
The segmentation and description methods mentioned in Section 4.5.4 and the
present section are applicable to 2-D scene data. The corresponding processes for
3-D scenes are more complex and include the following:
G
Constructing planar patches
118
Introduction to Mobile Robot Control

G
Gradient technique (to obtain patch representations)
G
Generalized cones or cylinders (moving along a cross section of the object, e.g., a ring,
along a straight line spine)
4.5.6
Image Recognition
Image recognition is called the labeling process applied to a segmented object of a
scene. That is, the image recognition presumes that objects in a scene have been
segmented as individual elements (e.g., a bolt, a seal, a wrench). The typical con-
straint here is that images are acquired in a known viewing geometry (often perpen-
dicular to the workspace).
Image recognition methodologies are distinguished into the following:
G
Decision theoretic methods (these methods use proper decision or discriminant functions
for matching the objects to one of several prototypes)
G
Structural methods (here, an object is decomposed in a set of primitive element pattern of
predefined length and direction). A simple example is shown in Figure 4.11. Three other
structural methods are (i) shape number matching, (ii) string matching, and (iii) syntactic
methods (string grammars, semantics, etc.).
4.5.7
Image Interpretation
Image interpretation is a higher level process which uses combinations of the
methods discussed earlier, namely, sensing, preprocessing, segmentation, descrip-
tion, and recognition. A machine vision system is ranked according to its general
ability to extract useful information from a scene under a wide repertory of viewing
conditions, needing minimal knowledge about the objects at hand. Factors that
make image interpretation a difficult task include variations in illumination condi-
tions, viewing geometry, and occluding bodies.
Occlusion problems occur when we have a multiplicity of objects in an uncon-
strained work terrain.
α
α
α
α
α
α
b
b
b
b
d
d
d
d
c
c
c
c
c
c
d
b
(A)
(B)
Figure 4.11 Representation of an object’s boundary (A), by primitives (B).
119
Mobile Robot Sensors

Other subsidiary processes that help in better image interpretation are as
follows:
G
Multiscaling (this is useful whenever multiple scales of an image can be obtained). Many
camera image systems can emit a thumbnail along the main image. This can be used as a
basis for searching the main image, that is, any regions in the low-resolution image lack-
ing in pixels probably represent empty or very sparse areas in the full image.
G
Sequential search (here, each and every pixel is examined once and only once). New pix-
els are compared against previously found groups of pixels and inserted into the matching
group. If a pixel is found to belong to two groups, the groups must be combined.
4.5.8
Omnidirectional Vision
Omnidirectional vision deals with the capture and interpretation of images that
depict full 360 view (i.e., horizontal panoramic view) of the environment.
Combining separate horizontal panoramic views one can get full spherical projec-
tion. The ability of omnidirectional vision to capture 360 view leads to improved
results for optical flow, feature selection, and feature matching.
The vision systems that can generate omnidirectional/panoramic images fall into
the following categories:
G
Camera-only systems: Here, a standard camera is rotated about its vertical axis, and the
perspective images are produced by sticking them so as to obtain 360 panoramic views
(Figure 4.12A). The overall resolution does not depend on the camera resolution but on
the resolution of the angular rotation.
G
Single camera-single mirror systems: These systems are very popular in vision-based
mobile robot navigation. In general, a CCD camera is pointed vertically up as shown in
Figure 4.12B.
Camera
Camera
Object
point
Object
point
Mirror 1
Mirror 2
CCD
Mirror 1
Mirror 2
CCD
CCD
CCD
(A)
(B)
(C)
(D)
Figure 4.12 Schematic
representation of omnidirectional
vision systems: (A) Rotating
camera system. (B) Single
camerasingle mirror system
with parabolic mirror.
(C,D) Two examples of single
cameramultimirror systems.
120
Introduction to Mobile Robot Control

G
Single camera-multimirror systems: These are also called catadioptric cameras, and have
the important feature of compactness. Two single camera-double mirror systems are
shown in Figure 4.12C and D.
G
Multicamera-multimirror systems: A group of cameras is arranged in some way together
with an equal number of mirrors, for example, placing a camera under each mirror, and
merging the images obtained from all cameras to produce a 360 panoramic view of the
environment.
An example of single camera-single mirror system is shown in Figure 4.13A,
and a multicamera-multimirror system in Figure 4.13B.
In the system of Figure 4.13B, four triangular planar mirrors are placed side by
side in a pyramidal shape, and a camera is placed under each mirror. The images
of all cameras obtained are merged to provide a 360 panoramic view of the envi-
ronment. This configuration provides high resolution, and the possibility of a single
view, but it is not isotropic. The camera-only systems present some problems with
the registration and the delay of the camera motion. In single camera-single mirror
systems, there is no need to wait for a moving camera or to synchronize multiple
cameras.
Actually, there are several types of mirrors:
G
Planar mirrors, which possess no radial distortion and no radial loss of resolution.
G
Radially curved mirrors, which include three popular quadric surface mirrors (elliptic,
hyperbolic, parabolic) possessing a single viewpoint at their focal points.
G
Cone mirrors, which combine the desired features of planar mirrors and rotational sym-
metricity of radially curved mirrors. These sensors are simple to set up and function, and
isotropic (i.e., the projection is the same for all directions). They capture the entire envi-
ronment in a single instant and so they are suited for dynamic environments analysis and
moving cameras. The image resolution and distortion of the cone mirror are better than
other catadioptric system.
In many applications, true fish-eye lenses are used which are hemispherical (i.e.,
they have a wide field of view of at least 180). True fish-eye systems are easy to
set up and use, but the lens need correction and calibration. They are isotropic and
Mirror
Camera
(A)
(B)
Figure 4.13 (A) A single
camerasingle mirror
omnidirectional vision
system. (B) A pyramidal
multicameramultimirror
system.
Source: (B) (Full View, Inc.).
http://www.lkl.ac.uk/niall/
nwdis/, http://www.english.
pan.pl/images/stories/pliki/
publikacje/academia/2005/06/
28-29_siemiatkowska.pdf.
121
Mobile Robot Sensors

dynamic, and have very low image resolution at the periphery, and medium to high
in the center.
The three primary problems that can be faced by omnidirectional vision are as
follows:
1. Determine the distance of objects in a purely passive visual way (i.e., by strereopsis).
This is very useful in mobile robots because it provides higher precision in range estima-
tion than that obtained with active ranging.
2. Find the position and motion of the camera, very useful in mobile robot navigation.
3. Determine the position and motion of the robot and the objects.
Tasks that have to be performed for solving the above problems include the
following:
G
Registration, that is, identification and estimation of the camera and mirror intrinsic and
extrinsic parameters.
G
Unwarping, which is done by transformation of the original circular image from polar
ðρ; θÞ coordinates to rectangular Cartesian ðx; yÞ coordinates that can be more easily inter-
preted by humans. The unwarped image (which is a distortion-free image) is known as
“panoramic image.”
G
Image mapping, that is, matching pairs of images by which the rotation of the robot in
between two positions can be found more easily than using a compass.
G
Estimation of the distance of objects, which can be done by matching individual image
features along the radial epipolar lines.
G
Estimation of the instantaneous velocities of object edges from pairs of images of a mov-
ing object.
Figure 4.14A shows an omnidirectional camera sensor mounted on the Pioneer 1
WMR. The sensor is made by a camera pointed upward at the vertex of a spherical
(A)
(B)
1
2
3
4
Figure 4.14 (A) The Pioneer 1 WMR equipped with an omnicamera. (B) A mobile robot
(4) with a catadioptric sensor (1), a camera (2), and a laser scanner (3).
Source: http://www.ippt.gov.pl/Bbsiem/ecmr_last.pdf, http://home.elka.pw.edu.pl/
Bmmajchro/romansy2006/romansy06.pdf.
122
Introduction to Mobile Robot Control

mirror, with the camera optical axis and the mirror optical axis aligned. Due to the
spherical shape of the mirror, the sensor resolution depends on the distance
between the camera and the observed region. The resolution of an image is maxi-
mum near the robot and so it is possible to localize very precisely objects that are
nearer to the robot.
Figure 4.15A shows an omnidirectional image obtained by a Canon camera
equipped with the 0360 panoramic optic. Finally, Figure 4.15B shows an image
obtained by a standard perspective camera observing a hyperbolic mirror, with its
reprojection on cylindrical surface shown in Figure 4.15C.
Example 4.1
We are given a single camera-single mirror omnidirectional vision system with hyperbolic
mirror.
(a) Investigate the hyperbolic mirror geometry.
(b) Using the property of single center of projection, show how a perspective image from
the image sensed by the omnidirectional system can be created.
(A)
(B)
(C)
Figure 4.15 (A) Omnidirectional image acquired by a Canon EOS35OD combined with a
0360 omnidirectional optic. (B) An omnidirectional image obtained by hypercatadioptric
system. (C) Reprojection of (B) on cylindrical surface.
Source: http://www.oru.se/PageFiles/15214/Valgren_Licentiate_Thesis_Highres.pdf, http://
cmp.felk.cvut.cz/demos/Omnivis/Hyp2Img/.
123
Mobile Robot Sensors

Solution
(a) We will work with the omnidirectional vision system of Figure 4.16[18].
The hyperbolic mirror is described by:3
y 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
p2ð1 1 x2=q2Þ
p
2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
p2 1 q2
p
ð4:1Þ
where the origin of the coordinate frame ðx; yÞ is at the mirror focal point Fm, and p, q are
the mirror parameters. The other symbols in Figure 4.16 are as follows:
Fc, focal point of the camera;
h, distance between the mirror top (rim) and the camera focal point;
rtop, the x coordinate of the mirror top;
c, half of the distance ðFm; FcÞ, the mirror eccentricity ðc 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
p2 1 q2
p
Þ
ytop, the y coordinate of the mirror top ðytop 5 h 2 2cÞ;
α, the vertical angle of view.
The maximum angle of view αmax of the mirror is determined by
tgðαmax 2 π=2Þ 5 ðh 2 2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
p2 1 q2
p
Þ=rtop
ð4:2Þ
Applying
the
mirror
equation
at
ðx; yÞ 5 ðrtop; ytopÞ,
we
obtain
h 2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
p2 1 q2
p
5 p2ð1 1 r2
top=q2Þ, which if squared gives h2 1 q2 2 2h
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
p2 1 q2
p
5 p2r2
top=q2.
Completing the squares gives ðh
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
11p2=q2
p
2qÞ2 5 ðp2=q2Þðh2 1 r2
topÞ. So, finally we have
Camera projection 
plane
h
2c
c
c
lf
Fc
P
A
B
y
z
x
rtop
lfp
lfp
Yim
(x,y)
Fm
Fm
x
α
φ
ψ
Camera
View from A
Perspective projection
plane
Perspective projection
plane
View from B
Perspective projection
plane
Mirror top(rim)
Xim
Xim
Yim
Figure 4.16 Geometrical parameters and features of the camera hyperbolic mirror system.
Source: Courtesy of J. Okamoto Jr.
3 This
equation
is
derived
from
the
standard
hyperbola
equation
opening
along
the
axis
y: ðy2y0Þ2=p2 2 ðx2x0Þ2=q2 5 1(see Eq. (9.107d)), where the origin is at the mirror focal point
Fmðx0 5 0;
y0 5 2 c;
c 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
p2 1 q2
p
Þ.
124
Introduction to Mobile Robot Control

q 1 ðp=qÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
h2 1 r2
top
q
5 h
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 1 p2=q2
p
ð4:3Þ
We can easily verify that the relation of h with the focal length lf of the camera, the
mirror top radius in pixels ðrpixelÞ, and the size of each pixel in the camera CCD in milli-
meters ðtpixelÞ is the following:
h 5 lfrtop=rpixel  tpixel
ð4:4Þ
The parameters lf, tpixel, and rpixel depend on the camera and mirror used and on the
image acquired by the system. The above relation is valid when the horizontal and vertical
scale factors are the same in which case the image of the mirror top is a circle and not an
ellipse.
Equations (4.1)(4.4) are used to design a proper omnidirectional cameramirror
pair that can be placed on a WMR. Specifically, to get a desired value of h we need
Eq. (4.4) to select rtop. Having available the value of h we choose a value for the ratio
p=q, and using Eq. (4.3), we find the value of q, and then the value of p. Finally, using
h, p, and q we can find the mirror equations, and the maximum angle of view using
Eqs. (4.1) and (4.2).
The perspective images are created by mapping the pixels of the omnidirectional
image into a plane perpendicular to a ray passing via the center of projection Fm.
The distortion-free image that is obtained is equivalent to an image acquired by a
perspective camera with its focus located at Fm. The transformation of omnidirec-
tional images into perspective or panoramic images is called unwrapping. Obviously,
if the creation of equivalent perspective images is possible, all available vision tech-
niques for perspective projection images (including those of visual servoing) can be
applied.
Figure 4.16 (right-hand side) shows also the perspective images obtained in the
directions A and B. A pixel in the perspective image is described by the coordinates
ðxim; yimÞ. A perspective projection plane is defined by ðlfp; φa; φeÞ, where lfp is the dis-
tance in pixels from the focal point of the hyperbolic mirror to the plane concerned, φa
is the azimuthal angle of the plane direction, and φe is the elevation angle of the
plane (Figure 4.16).
(b) As shown in Figure 4.16, the hyperbolic mirror has the property of single projection
center, that is, the light rays that contribute to the image formation, after their reflection
by the mirror surface, intersect at the (virtual) focal point Fm, which is the center of pro-
jection of the mirror. The two most popular ways to construct an omnidirectional vision
system with this property are the following (see Section 9.9.2):
1. Use a parabolic mirror with an orthographic projection lens camera.
2. Use a hyperbolic mirror with a perspective projection lens camera.
Figure 4.17 is referred to the second way and shows how perspective and panoramic
images can be created by projecting the acquired image into properly defined projection
planes [18].
By assuming that the above perspective plane parameters define the pose of a virtual
perspective camera placed on a pan/tilt mechanism, lfp would represent the focal distance
of the virtual camera, and φa, φe would be the pan and tilt angles (i.e., azimuth and
zenith angles), respectively.
125
Mobile Robot Sensors

It can be verified that the relation of the pixel ðx; yÞ on the mirror surface and the pixel
ðxim; yimÞ on the perspective projection plane is the following [18]:
xim 5 xð2c 1 ytopÞrpixel
ðxtgφ 1 2cÞ
cos ψ
ð4:5aÞ
yim 5 xð2c 1 ytopÞrpixels
ðxtgφ 1 2cÞ
sin ψ
ð4:5bÞ
The relations of ðφ; ψÞ and ðlfp; φa; φeÞ are
tgφ 5 lfpsin φe 1 yimcos φe
lfpcos φe
ð4:6aÞ
tgψ 5 ðlfpcos φe 2 yimsin φeÞsin φa 2 ximcos φa
ðlfpcos φe 2 yimsin φeÞcos φa 1 ximcos φa
ð4:6bÞ
4.6
Some Other Robotic Sensors
In this section, some further important robotic sensors are briefly described:
G
Gyroscope
G
Compass
G
Force and tactile sensors
Perspective image
Projection plane
of the camera
Panoramic image
Fc
Fm
lf
Figure 4.17 Creation of perspective (planar) image and panoramic (on cylindrical surface)
image.
Source: Courtesy of J. Okamoto Jr.
126
Introduction to Mobile Robot Control

4.6.1
Gyroscope
Gyroscopes are devices measuring angular position by preserving their orientation
with respect to a world coordinate frame. Thus, they are used in aeronautics,
navigation, and mobile robot navigation. The gyroscope is distinguished into free
gyroscope and two degrees of freedom (DOF) gyroscope (Figure 4.18).
The disc is rotating with very large angular speed (in the range 10,000 rad/min
up to 25,000 rad/min). The Cardano mounting system via the two of gimbals allows
the angular momentum axis of the rotating disk to have three DOF of motion about
the axes x; y, and z. The free gyroscope can permanently indicate either the
south-north direction (horizontal direction gyroscope) or the vertical direction if its
angular momentum axis is aligned with the gravity direction (vertical direction
gyroscope), that is, if the gyroscope is tipped, the gimbals will reorient to keep the
spin axis of the disk in the same direction. This is the implication of the angular
momentum conservation principle. The gyroscope with two DOF is obtained from
the free gyroscope by placing a torsional spring along the axis x between the gim-
bals I and II. In this way, the rotating disc can rotate only about the axis yand the
axis x. If we apply a torque T about the axis x, then the angular momentum axis of
the disc will rotate around the axis z with a velocity
Ω 5 Τ=ωJ
where ω is the speed of the disc and J its moment of inertia. The 2-DOF gyroscope
is used to measure angular speeds instead of absolute position (rate gyro). The rota-
tion of the spin axis around the axis z (velocity Ω) is known as twist rotation or
recession.
4.6.2
Compass
The magnetic compass was first used in China in 2634 BC, when a piece of natu-
rally magnetic iron ore (magnetite) was suspended by a silk thread to allow its
alignment with the lines of the Earth’s magnetic field, which are horizontal at the
equator and vertical at the magnetic poles.
G=Jω
ω
(A)
(B)
Spin
axis
Figure 4.18 (A) Schematic gyroscope. (B) An aircraft gyroscope.
Source: http://hyperphysics.phy-astr.gsu.edu/hbase/gyr.html.
127
Mobile Robot Sensors

Today, many electronic compass modules are available for on-board use of mobile
robots. Analog compass modules can be easily integrated with robot controllers. A
simple analog compass can distinguish only eight directions which are represented by
respective voltage levels. This is used in many four-wheel-drive systems. Digital com-
pass modules are much more complex than analog ones and provide a considerably
higher directional resolution (e.g., one resolution in indoor applications).
Some commercially available compasses for mobile robots include the Dinsmore
Starguide Magnetic Compass, fluxgate compasses (Zemco compass, Watson gyro
compass, KVA compasses, Philips magnetoresistive compass, etc.). A well-known
mobile robot with a gyroscope and a compass integrated is Pioneer II (Figure 4.19).
4.6.3
Force and Tactile Sensors
4.6.3.1
Force Sensors
Actually, there are many types of force sensors which measure force (load) or tor-
que. There are many reasons why we would need to directly measure forces in a
robot, e.g., weight measurement, force quantification, parameter optimization. A
notable example is a humanoid, where a force sensor helps to know how much
weight is on each leg. Another is to put a force sensor in a mobile manipulator’s
gripper to control gripper friction in order to not crush or drop anything picked up
and to know if the robot has reached its maximum carrying weight or how much
weight it is carrying.
A usual force sensor is the so-called strain gauge which is a tiny flat coil of con-
ductive wire that changes its resistance when it is bent. The strain is directly related
to the force applied to bend the beam. If the anticipated forces are small, a conduc-
tive elastomer of foam can be used as a strain gauge a conductive foam.
Compressing the foam lowers the electrical resistance. To face the problem of a
Figure 4.19 Pioneer II mobile robot integrated with gyroscope and compass.
Source: http://www.intechopen.com/source/pdfs/15967/InTech-Mobile_robot_integrated_
with_gyroscope_by_using_ikf.pdf.
128
Introduction to Mobile Robot Control

typical strain gauge, due to which it has a very low change in resistance when bent,
the strain gauges are connected inside the force sensor, usually a multiple of four, in
a Wheatstone bridge configuration. In this way, the very small change in resistance
is converted into a usable electrical signal. Passive components such as resistors and
temperature-dependent wires are employed to compensate and calibrate the bridge
output signal. The primary reason for transducer failure is the force overload which
should be properly selected. The range of values of mass for which the result is not
affected by other limit error is called measuring range. Two other parameters that
secure proper operation of the force sensor are the safe load/torque limit (the maxi-
mum load/torque that can be applied without producing a permanent shift) and the
safe side load (i.e., the maximum load that cannot act 90 to the axis along the sen-
sor and cannot produce a permanent shift in the performance beyond specified).
Two industrial applications of force sensor use are shown in Figure 4.20.
4.6.3.2
Tactile Sensors
Touch and tactile sensors are devices that measure the parameters of a contact
between the sensor and an object, which is confined to a small well-defined region.
This is in contrast with a force sensor which measures the total forces that are
applied to an object.
Touch sensing is the detection and measurement of a contact force at a defined
point (which can also be binary, i.e., touch, no touch).
Tactile sensing is the detection and measurement of the spatial distribution of
forces perpendicular to a defined sensory area, followed by an interpretation of this
force distribution. A tactile sensing array is obtained by the coordination of a group
of touch sensors.
Slip is the detection and measurement of the movement of an object relative to a
sensor. This can be done either by a specially designed slip sensor or via interpreta-
tion of the data obtained by a touch sensor or tactile array. Some desired properties
of tactile sensors are as follows:
G
A proper sensory area (ideally a single-point) contact
Figure 4.20 Force sensor uses
(FANUC): (A) gear assembly;
(B) clutch assembly.
Source: http://lrmate.com/
forcesensor.htm.
129
Mobile Robot Sensors

G
A proper sensor sensitivity depending on the application concerned (typical sensitivity
range being 0.410 N)
G
A minimum sensor bandwidth of 100 Hz
G
Stable and repeatable sensor features with low hysteresis
G
Robustness and protection against large environmental changes
The principal physical principles used for the design of touch/tactile sensors
capable of working with rigid objects are as follows:
G
Conventional mechanical switch (binary touch sensor).
G
Resistive-based sensors (based on the measurement of the resistance of a conductive elas-
tomer of foam between two points; Figure 4.21).
G
Force-sensing resistor (a piezoresistive conductive polymer that changes resistance in a
predictable way following the application of force to its surface). As with resistive-based
sensors, a relative simple interface is needed.
G
Capacitance-based sensors (i.e., sensors using the capacitance between two parallel plates,
C 5 εA=l, where A is the plate area, l the distance between plates, and ε the permittivity
of the dielectric medium).
G
Magnetic-based sensors (utilizing the movement of a small magnet by an applied force
that causes a change to the flux density, or using a magnetoelastic material whose mag-
netic properties change when external forces are applied to it).
Other ways of constructing touch/tactile sensors include optical fiber-based sen-
sors (using the internal state microbending of optical fibers), piezoelectric sensors
(using polymeric materials such as polyvinylidene fluoride (PVDF)), silicon-based
sensors (silicon possesses a tensile strength comparable to steel, and elastic to
breaking point with very small mechanical hysteresis), and strain gauge-based tac-
tile sensors.Figure 4.22 shows a tactile sensor pad mounted on RIKEN’S RI-MAN
Increase in
particle density
R2
R1
Applied force
Figure 4.21 The resistance R of the elastomer
changes according to the force applied.
Figure 4.22 Tactile sensor array mounted on the
torso and arm of humanoid.
Source: http://lh6.ggpht.com/touchuiresourcecenter/
SNHVwL98kPI/AAAAAAAAAR8/uPv6Qy_e0s0/
s1600-h/image%5B3%5D.png.
130
Introduction to Mobile Robot Control

humanoid robot, and Figure 4.23 shows Shadow’s tactile sensor mounted on the
fingers of a robotic hand (sensitivity of sensor tactile elements from 0.1 to 25 N)
suitable for both delicate handling and power grasping.
4.7
Global Positioning System
The global positioning system (GPS) is a space-based radio-positioning and time-
transfer system. GPS satellites transmit signals to proper equipment on the ground.
These signals provide accurate position, velocity, and time (PVT) information to an
unlimited number of users on ground, sea, air, and space. GPS receivers need an
unobstructed view of the sky, so they can only be used outdoors, and they usually do
not operate well in near tall buildings or forestry areas. Passive PVT fixes are offered
worldwide in all weathers in a worldwide common grid system. The three primary
parts of GPS are space, control, and user segments (Figure 4.24A). Each satellite
transmits data continuously indicating its location and current time, and all satellite
signals are sent at the same time (synchronous transition). The GPS receiver has a
quartz clock, but although three satellites can give the 3-D position, four satellites
are used (the fourth satellite for time correction).
By knowing its distance from a satellite, each receiver also “knows” it is located
somewhere on the surface of an imaginary sphere centered at the satellite. By
determining the sizes of several spheres, one for each satellite, the receiver finds its
location at the intersection point of these spheres (Figure 4.24B).
The space segment involves a minimum constellation of 24 satellites. The control
segment consists of a network of monitoring and control facilities (master stations,
monitoring stations, uploading stations) that are used to manage the satellite constel-
lation and to update the satellite navigation data messages. The users segment
involves all the radio navigation receivers that can receive, decode, and process the
GPS satellite ranging codes and navigation data messages. Because GPS satellites
Figure 4.23 A tactile sensor mounted on
robotic hand fingers and thumb.
Source: https://www.shadowrobot.com/
products/dexterous-hand.
131
Mobile Robot Sensors

are merely an information source, the localization resolution they provide depends
strongly on the strategies employed. The basic strategy (called pseudorange strategy)
gives a resolution of 15 m and can be improved to 1 m if a second receiver, which is
static and at a known exact position, is employed. This technique is called differen-
tial GPS or dual frequency GPS. In general, the basic performance parameters that
are used to compare different GPS receivers are as follows:
G
Position accuracy
G
Velocity accuracy
G
Time accuracy
G
Time to first failure
The real-time satellite tracking (GPS Operational Satellites) can be found in the
site www.n2yo.com/satellites/?c520.
(B)
Space segment
Control segment
Ground antenna
Master control
station
Antennas
User segment
Monitor stations
GPS satellites
(A)
Figure 4.24 (A) The three segments of GPS. (B) The receiver lies at the intersection of the
spheres centered at the four satellites.
Source: http://www.nasm.si.edu/gps/work.html.
132
Introduction to Mobile Robot Control

4.8
Appendix: Lens and Camera Optics
We start by deriving the formula of a convex lens (Figure 4.25).
It is assumed that the distance ðOAÞ 5 lo of the object AB, which is perpendicu-
lar to the principal axis Ox of the lens, is greater than the focal length
ðF1OÞ 5 ðF2OÞ 5 lf of the lens. From the similarity of the triangles OAB and
OA0B0, we get
ðA0B0Þ
ðABÞ 5 ðOA0Þ
ðOAÞ 5 lim
lo
where ðOA0Þ 5 lim is the distance of the image from the lens. Also, from the simi-
larity of the triangles OF2C and F2A0B0, we have
ðA0B0Þ
ðOCÞ 5 ðF2A0Þ
ðOF2Þ 5 ðOA0Þ 2 ðOF2Þ
lf
5 lim 2 lf
lf
From the above equations, taking into account that ðOCÞ 5 ðABÞ, we get
lim
lo
5 lim 2 lf
lf
or
limlf 5 limlo 2 lflo
Dividing this equation throughout by limlolf gives the desired lens equation:
1
lo
1 1
lim
5 1
lf
This formula is used in camera optics for determining depth from focus, based
on the fact that image properties change both as a function of the scene and as a
function of the intrinsic camera parameters (focal length lf, distance le from the
lens to the focal point, and the parameter ε) (Figure 4.26).
Focal plane
Image plane
Object
A
B′
A′
o
B
C
lo
lim
lf
lf
F1
x
2F2
F2
Image
Figure 4.25 Geometry of convex lens.
133
Mobile Robot Sensors

From Figure 4.26, we have 1=lf 5 1=lo 1 1=le. In order to obtain a sharp image
of the point ðx; y; zÞ, the image plane of the camera must coincide with the focal
plane. Otherwise, the image of the point ðx; y; zÞ will be blurred as can be seen
from this figure. This is because if the image plane is located at distance le from
the lens, then for the specific object voxel depicted, all light will be focused at a
single point on the image plane, and the object voxel will be focused. But, when
the image plane is not at distance le (as shown in Figure 4.26), the light from the
object voxel will be mapped on the image plane as a blur circle. It is easy to show
(using similar angles calculation) that the radius R of the blur circle is equal to
R 5 Dε=2le
where D is the diameter (aperture) of the lens. Clearly, R 5 0 if ε 5 0, or D 5 0 (as
in the pinhole camera, where the lens reduces to a point). This is in agreement with
the fact that decreasing the iris aperture opening causes the depth of field to
increase until all objects are in focus (of course with less light entering to form the
image on the image plane).
References
[1] Borenstein J, Everett HR, Feng L. Navigating mobile robots: sensors and techniques.
Wellesley, MA: Peters A K Ltd; 1999.
[2] Everett HR. Sensors for mobile robots: theory and applications. New York, NY: Peters
A K Ltd; 1995.
[3] DeSilva CW. Control sensors and actuators. Upper Saddle River, NJ: Prentice Hall;
1989.
[4] Adams MD. Sensors modeling, design and data processing for autonomous navigation.
Singapore: World Scientific Publishers; 1999.
[5] Bishop RH. Mechatronic systems sensors and actuators: fundamentals and modeling.
Boca Raton, FL: CRC Press; 2007.
[6] Leonard JL. Directed sonar sensing for mobile robot navigation. Berlin: Springer; 1992.
[7] Gonzalez RG. Computer vision. New York, NY: McGraw-Hill; 1985.
[8] Haralick RM, Shapiro LG. Computer and robot vision, vols. 1 and 2). Boston, MA:
Addison Wesley; 1993.
Image plane
Object 3D point
(x,y,z)
Focal plane
Blur area
Camera lens
le
lo
lf
F1
F2
ε
Figure 4.26 Camera optics.
134
Introduction to Mobile Robot Control

[9] Davies ER. Machine vision: theory algorithms, practicalities. Amsterdam, The
Netherlands: Morgan Kaufmann/Elsevier; 2005.
[10] Tzafestas SG, editor. Intelligent robotic systems. New York/Basel: Marcel Dekker;
1991.
[11] Tzafestas SG, editor. Advances in intelligent autonomous systems. Dordrecht/Boston:
Kluwer; 1999.
[12] Panich S, Afzulpurkar N. Mobile robot integrated with gyroscope by using IKF. Int J
Adv Robot Syst (INTECH Open Access) 2011;8(2):12236.
[13] Kleeman L, Kuc R. Mobile robot sensor for target localization and classification. Int J
Robot Res 1995;14(4):295318.
[14] Phairoh T, Williamson K. Autonomous mobile robots using real time kinematic signal
correction and global positioning system control. Proceedings of 2008 IAJC-IJME
international conference paper 087. IT304, Nashville, TN; November 1719, 2008.
[15] Tzafestas SG. Sensor integration and fusion techniques in robotic applications. J Int
Robot. Syst. 2005;43(1):1110 [special issue]
[16] Taha Z, Chew JY, Yap HJ. Omnidirectional vision for mobile robot navigation. J Adv
Comput Intell Intell Inform 2010;14(1):5562.
[17] Goh M, Lee S. Indoor robot localization using adaptive omnidirectional vision system.
Int J Comput Sci Netw Secur 2010;10(4):6670.
[18] Grassi Jr. V, Okamoto Jr J. Development of an omnidirectional vision system. J Braz
Soc Mech Sci Eng 2006;28(1):118.
[19] Menegatti E. Omnidirectional vision for mobile robots. PhD thesis. Italy: Department
of Information Engineering, University of Padova; December, 2002.
[20] Svoboda T, Pajda T, Hlavac V. Central panoramic cameras geometry and design.
Research report no. K355/97/147. December 5, 1997. ,ftp://cmp.felk.cvut.cz/pub/
cmp/articles/svoboda/TR-K355-97-147.ps.gz.
[21] Benosman R, Kang R. Panoramic vision. Berlin: Springer; 2000.
[22] Velagic J, Lacevic B, Perunicic BA. 3-level autonomous mobile robot navigation sys-
tem designed by using reasoning/search approaches. Robot Auton Syst 2006;5
(12):9991004.
135
Mobile Robot Sensors

5 Mobile Robot Control I: The
Lyapunov-Based Method
5.1
Introduction
Robot control deals with the problem of determining the forces and torques that
must be developed by the robotic actuators in order for the robot to go at a desired
position, track a desired trajectory, and, in general, to perform some task with
desired performance requirements. The solution to control problems in robotics
(fixed and mobile) is more complicated than usual due to the inertial forces,
coupling reaction forces, and gravity effects. The performance requirements con-
cern both the transient period and the steady-state period. In well-structured and
fixed environments, such as the factory, the environment can be arranged to match
the capabilities of the robot. In these cases, it can be assured that the robot knows
certainly the configuration of the environment, and people are protected from
the robot’s operation. In such controlled environments, it is sufficient to employ
some type of model-based control, but in uncertain and varying (uncontrolled)
environments, the control algorithms must be more sophisticated involving some
kind of intelligence. The techniques to be presented in this chapter assume that the
goal of the control and the robot kinematic and dynamic parameters are precisely
known, and if this goal (posture or path) is changing, the change is compatible with
the environment, and risk free.
Specifically, the objectives of the chapter are as follows:
G
To provide a minimal set of general control concepts and methods that are used in the
control of robots
G
To study the basic general robot controllers that are applicable to all types of robots
G
To present a number of feedback controllers, designed using the Lyapunov-based control
theory, for the differential drive, car-like, and omnidirectional mobile robots.
These controllers refer to the problems of position (posture) tracking, trajectory
tracking, parking, and leader following. In all cases, the control design involves
two stages, viz., kinematic control (where only the kinematic models are used),
and dynamic control (where the robot dynamics and actuators are also taken
into account).
Introduction to Mobile Robot Control. DOI: http://dx.doi.org/10.1016/B978-0-12-417049-0.00005-5
© 2014 Elsevier Inc. All rights reserved.

5.2
Background Concepts
In this section, the following fundamental control concepts and techniques are
briefly discussed:
G
State-space model
G
Lyapunov stability
G
State feedback control
G
Second-order systems
Knowledge of these concepts is a basic prerequisite for the understanding of
the material presented in the chapter. Full accounts are given in standard control
textbooks [1].
5.2.1
State-Space Model
The state-space model of a control system is based on the concept of state vector
xðtÞARn, which is the minimum dimensionality Euclidean vector, with components
called state variables, the knowledge of which at an initial time t 5 t0, together
with the input vector uðtÞ, for t $ t0, determines completely the behavior of the
system for any time t $ t0. The dimension n of the state vector specifies the system’s
dimensionality.
The above definition of the state means that the state of the system is deter-
mined by its initial value xðt0Þ at t 5 t0 and the input for t $ t0, and is independent
of the state and the inputs for times previous to t 5 t0.
It is noted that the state variables x1ðtÞ; x2ðtÞ; . . .; xnðtÞ of an n-dimensional
system may not necessarily be measurable physical quantities, although in practice,
an effort is made to use as more as possible measurable variables, because the state
feedback control laws need all of them.
The
expression
of
xðtÞ
as
a
function
of
t; t0; xðt0Þ 5 x0,
and
uðτÞ 5 ½u1ðτÞ; . . .; umðτÞT, τ $ t0, that is, xðtÞ 5 ϕðt; t0; x0; uðτÞÞ, is called the system’s
trajectory.
The output yðtÞ of the system is a similar function of xðtÞ, uðtÞ, and t, that is:
yðtÞ 5 ηðt; ϕðt; t0; x0; uðτÞÞ; uðtÞÞ
for all t $ t0
The trajectories satisfy the transition property:
ϕðt; t0; xðt0Þ; uðτÞÞ 5 ϕðt; t1; xðt1Þ; uðτÞÞ
for all t0 , t1 , t, where xðt1Þ 5 ϕðt1; t0; x0; uðτÞÞ.
In state-space model, the dynamic model of a nonlinear system (in continuous
time) has the form:
_xðtÞ 5 fðx; u; tÞ
ðt $ t0Þ
ð5:1aÞ
138
Introduction to Mobile Robot Control

yðtÞ 5 gðx; u; tÞ
ðt $ t0Þ
ð5:1bÞ
where fðÞ and gðÞ are nonlinear vector functions of their arguments with proper
dimensionality and the continuity and smoothness properties required in each case.
The state vector x belongs to the state space X, the input (control) u belongs to the
input space U, and the output y to the output space Y, where XCRn, UARm,
YCRp, and Rn is the n-dimensional Euclidean space.
If the vector functions f and g are linear, then the system is linear and is
described by the model:
_x 5 Ax 1 Bu
ðxðt0Þ knownÞ
ð5:2aÞ
y 5 Cx 1 Du
ðxðt0Þ knownÞ
ð5:2bÞ
where A, B, C, D may be time-invariant or time-varying matrices of proper
dimensionality (in many cases, D 5 0). A linear state-space model with the follow-
ing matrices, A, B, and uAR (scalar), is called the controllable canonical model of
the system that represents:
x 5
x1
x2
^
xn
2
66664
3
77775
;
A 5
0
1
?
0
0
0
?
0
^
^
&
^
0
0
?
1
2an
2an21
?
2a1
2
6666664
3
7777775
;
B 5
0
0
^
0
1
2
6666664
3
7777775
;
u 5 u
ð5:3Þ
For a scalar output yAR, a linear time-invariant system described by the nth-order
differential equation:
ðDn 1a1Dn21 1?1an21D1anÞyðtÞ5ðb0Dn 1b1Dn21 1?1bnÞuðtÞ; D5d=dt
or transfer function:
yðsÞ
uðsÞ 5 b0sn 1 b1sn21 1 ? 1 bn21s 1 bn
sn 1 a1sn21 1 ? 1 an21s 1 an
ð5:4Þ
where s 5 a 1 jω is the complex frequency variable, can be modeled as in
Eqs. (5.2a), (5.2b), and (5.3), if we define the state variables x1; x2; . . .; xn as:
Dx1 5 x2; Dx2 5 x3; . . .; Dxn21 5 xn
ð5:5Þ
139
Mobile Robot Control I: The Lyapunov-Based Method

Indeed, using Eq. (5.5) we get:
Dxn 5 2a1xn 2 a2xn21 2 ? 2 an21x2 2 anx1 1 u
y 5 bnx1 1 bn21x2 1 ? 1 b1xn 1 b0ðu 2 a1xn 2 ? 2 an21x2 2 anx1Þ
which gives the state-space model ((5.2a), (5.2b), and (5.3)) with:
D 5 ½b0;
C 5 bn 2 anb0; bn21 2 an21b0; . . .; b1 2 a1b0
½

ð5:6Þ
This model, also called phase variables canonical model, is very convenient for the
pole-placement (or assignment) state feedback controller design.
The block diagram representation of the general model ((5.2a) and (5.2b)) has
the form shown in Figure 5.1.
Other state-space canonical models of the system ((5.2a) and (5.2b)) are the
observable canonical form and the Jordan canonical form fully described in control
textbooks. To convert a given model ((5.2a) and (5.2b)) to some canonical form,
use is made of a proper nonsingular linear (similarity) transformation x 5 Tz,
where z is the new state vector.
Example 5.1
In this example, we derive the dynamic models (transfer function, state-space model) of
the direct current (DC) electrical motor which is used in mobile robots to provide the
torques that lead to the desired acceleration and velocity of them. DC motors are distin-
guished into motors controlled by the rotor (armature controlled), and motors controlled
by the stator (field controlled). Both the motors will be considered.
Armature Controlled DC Motor
The rotor involves the armature and the commutator. A schematic of this motor is shown
in Figure 5.2, where Ra and La are the resistance and inductance of the rotor, IL is the load
moment of inertia, and β is the linear friction coefficient.
The mechanical torque is given by:
TmðtÞ 5 KaiaðtÞ
ð5:7Þ
where Ka is the motor’s torque constant. The back electromotive force (emf) eb which is
subtracted from the input voltage υa is proportional to ωm, that is:
eb 5 KbωmðtÞ;
ωmðtÞ 5 dθmðtÞ=dt
ð5:8Þ
+
+
+
+
u(t)
x(t)
•
x(t)
y(t)
A
Σ
Σ
∫
B
D
C
Figure 5.1 Block diagram of a general linear state-space model.
140
Introduction to Mobile Robot Control

The characteristic curves of “Tm versus ωm” are obtained by plotting the following
function:
Tm 5 Ka
υa 2 Kbωm
Ra


and have the linear form shown in Figure 5.3.
At the point where eb 5 υa, the motor maintains a constant angular speed (assuming
of course that no external disturbances affect the motor). The differential equation of the
motor can be found using the following relations:
Tm 5 Kaia 5 IL D2θm 1 β Dθm
υa 5 Raia 1 La Dia 1 Kb Dθm
where D 5 d=dt, IL is the moment of inertia of the load (plus the moment of inertia of the
motor), and β is the linear friction coefficient, and eliminating ia. The result is:
ðτa D2 1 τb D 1 1ÞDθmðtÞ 5 KυaðtÞ
Stator
Load
Friction β
Rotor
if=const
IL
ia
Ra
eb
ωm
Θ
La
νa
m
Figure 5.2 Schematic of the armature controlled DC motor (θm 5 rotation angle,
ωm 5 motor angular speed, IL 5 load moment of inertia).
0.33eb
ωm (rad/min)
0
Tm
eb
0.67eb
Figure 5.3 Characteristic curves of armature controlled system.
141
Mobile Robot Control I: The Lyapunov-Based Method

where K 5 Ka=ðβRa 1 KaKbÞ; τa 5 ILLa=ðβRa 1 KaKbÞ; τb 5 ðβLa 1 ILRaÞðβRa 1 KaKbÞ;
τa 5
ILLa=ðβRa 1 KaKbÞ; τb 5 ðβLa 1 ILRaÞðβRa 1 KaKbÞ. If La is negligible, then τaC0, and
the above differential equation reduces to:
ðτb D2 1 DÞθmðtÞ 5 KυaðtÞ
ð5:9Þ
where K is as above, and τb 5 ILRa=ðβRa 1 KaKbÞ. In practice, the model in Eq. (5.9) repre-
sents an adequate approximation of the motor dynamics. The controllable state-space
canonical model of the motor is found from Eq. (5.9) using Eqs. (5.3) and (5.6), that is:
x 5
x1
x2


;
A 5
0
1
0
21=τb


;
B 5
0
1


;
C 5
K
τb
; 0


ð5:10Þ
with u 5 υa and x1 5 θm. A DC motor is shown in Figure 5.4A. Figure 5.4B shows DC motors
of several sizes.
Field Controlled DC motor
In this motor, the rotor’s current is kept constant (coming from a constant-current
source), whereas the error is fed to the magnetic field of the stator via a phase-sensitive
amplifier (Figure 5.5). The characteristic curves Tm 5 Tmðωm; ifÞ show that when ωm is not
0.33if
0.67if
Rotor
ia=const
Armature
Constant Tm
region
if
if
Tm
ωm
Stator
Input error
Amplifier
(A)
(B)
ω
0
Figure 5.5 (A) Field controlled DC motor. (B) Characteristic curves for different if
(for very large ωm, we have a fall of Tm).
Figure 5.4 (A) A heavy-duty DC robot motor. (B) Several DC motors with embedded
gear boxes.
Source: http://www.goldmine-elec-products.com/prodinfo.asp?number50; http://www.
robojrr.tripod.com/motortech.htm.
142
Introduction to Mobile Robot Control

excessively large, the mechanical torque Tm is independent of ωm, depending only on the
field current if (Figure 5.5B).
A schematic of this motor is shown in Figure 5.6.
Here, we have the following relations:
Tm 5 Kfif
υf 5 Rfif 1 Lf dif=dt
Tm 5 IL d2θm=dt2 1 β dθm=dt
ð5:11Þ
where Kf is the field current/torque constant and Lf is the inductance of the stator, and all
other symbols have the same meaning as in the armature controlled motor.
Combining the above equations we get:
ð1 1 DτfÞð1 1 DτmÞDθmðtÞ 5 KυfðtÞ
ð5:12Þ
where K 5 Kf=βRf (DC gain), and τf, τm are the time constants of the magnetic field and
the mechanical time constant of the motor, respectively, given by:
τf 5 Lf=Rf;
τm 5 IL=β
In practice, τf is much smaller than τm, and so Eq. (5.12) reduces to:
ðτm D2 1 DÞθmðtÞ 5 KυfðtÞ
ð5:13Þ
which is of the same form as Eq. (5.9). Therefore, the motor has the state-space model
(similar to Eq. (5.10)):
x 5
x1
x2


;
A 5
0
1
0
21=τm


;
B 5
0
1


;
C 5
K
τm
; 0


with x1 5 θm and u 5 υf. If the motor dynamics (Eq. (5.9) or (5.13)) is expressed in terms of
the angular velocity ωm 5 Dθm 5 _θ m, then (whenever the motor dynamics is not neglected)
we use the form:
_ωmðtÞ 5 2ð1=τÞωmðtÞ 1 ðK=τÞuðtÞ
where τ 5 τb or τ 5 τf. This is analogous to a first-order RC circuit with time constant
τ 5 RC.
Rotor
ia=const
Armature
IL
Lf
Rf
if
νf
β
θm
Figure 5.6 Schematic of the field controlled
DC motor.
143
Mobile Robot Control I: The Lyapunov-Based Method

5.2.2
Lyapunov Stability
Stability is a binary property of a system, that is, a system cannot be simultaneously
stable or not stable. However, a stable system is characterized by a degree or index
that shows how much near to instability the system is (relative stability). A system
is defined to be bounded-input bounded-output (BIBO) stable if any bounded input
leads always to a bounded output. A linear time-invariant system is BIBO stable if
and only if all the poles of its transfer function or the eigenvalues of the matrix A
of its state-space model ((5.2a) and (5.2b)) lie strictly on the left-hand complex
semiplane s 5 a 1 jω. The matrix A with the above property is called a Hurwitz
matrix. The Routh and Hurwitz algebraic criteria specify the conditions that the
coefficients of the system’s characteristic polynomial must satisfy in order for
the system to be stable. A first-order system _x 1 ax 5 bu (with a real pole 2a) is
stable if a . 0, and has the impulse response:
xðtÞ 5 b e2at
Since xðtÞ tends to zero as t !N, asymptotically, the system is said to be asymptoti-
cally stable. Furthermore, since the convergence is exponential, that is, according to
b e2at, this system is called exponentially stable. For a second-order system where the
matrix A has the eigenvalues 2a1 6 jω, a1 . 0, the impulse response has the form:
xðtÞ 5 ðK=ωÞe2a1t sinðωtÞ;
a1 . 0
Since xðtÞ ! 0 as t ! N, the system is asymptotically stable, and because
jxðtÞj # ðK=ωÞe2a1t, the system is exponentially stable. The above results hold for
any combination of first-order and second-order systems.
The study of stability of a system and its stabilization via state or output feedback
are two of the central problems in control theory. But the Routh and Hurwitz stability
criteria can only be used for time-invariant linear single-input single-output (SISO)
systems.
Lyapunov’s stability method can also be applied to time-varying systems and
to nonlinear systems. Lyapunov has introduced a generalized notion of energy
(called Lyapunov function) and studied dynamic systems without external input.
Combining Lyapunov’s theory with the concept of BIBO stability we can derive
stability conditions for input-to-state stability (ISS).
Lyapunov has introduced two stability methods. The first method requires the
availability of the system’s time response (i.e., the solution of the differential equa-
tions). The second method, also called direct Lyapunov method, does not require
the knowledge of the system’s time response.
Definition 5.1 The equilibrium state x 5 0 of the free system _x 5 AðtÞx is stable in
the Lyapunov sense (L-stable) if for every initial time t0 and every real number
ε . 0, there exists some number δ . 0 as small as desired, that depends on t0 and ε,
such that: if jjx0jj , δ, then jjxðtÞjj , ε for all t $ t0, where jj  jj denotes the norm
of the vector x, that is, jjxjj 5 ðx2
11x2
21?1x2
nÞ1=2.’
144
Introduction to Mobile Robot Control

Theorem 5.1 The transition matrix Φðt; t0Þ of a linear system is bounded by
jjΦ t; t0
ð
Þjj , kðt0Þ for all t $ t0 if and only if the equilibrium state x 5 0 of _x 5 AðtÞx
is L-stable.’
The bound of jjxðtÞjj of a linear system does not depend on x0. In general, if the
system stability (of any kind) does not depend on x0, we say that we have global
(total) stability or stability in the large. If the stability depends on x0, then it is
called local stability. Clearly, total stability of a linear system implies also local
stability.
Definition 5.2 The equilibrium state x 5 0 is asymptotically stable if:
(i) It is L-stable.
(ii) For every t0 and x0 sufficiently near to x 5 0, the condition xðtÞ ! 0, for t ! N holds.’
Definition 5.3 If the parameter δ in Definition 5.1 does not depend on t0, then we
have uniform L-stability.’
Definition 5.4 If the system _xðtÞ 5 AðtÞx is uniformly L-stable, and for all t0 and
for arbitrarily large ρ, the relation jjx0jj , ρ implies xðtÞ ! 0 for t ! N, then the
system is called uniformly asymptotically stable.’
Theorem 5.2 The linear system _x 5 AðtÞx is uniformly asymptotically stable if and
only if there exist two constant parameters k1 and k2 such that: jjΦ t; t0
ð
Þjj # k1 e2k2ðt2t0Þ
for all t0 and all t $ t0.’
Definition 5.5 The equilibrium state x 5 0 of _x 5 AðtÞx is said to be unstable if for
some real number ε . 0, some t1 . t0 and any real number δ arbitrarily small, there
always exists an initial state jjx0jj , δ such that jjxðtÞjj . ε for t $ t1.’
Figure 5.7 illustrates geometrically the concepts of L-stability, L-asymptotic
stability, and instability.
Direct Lyapunov method: Let dðxðtÞ; 0Þ be the distance of the state xðtÞ from the
origin x 5 0 (defined using any valid norm). If we find some distance dðxðtÞ; 0Þ
which tends to zero for t ! N, then we conclude that the system is asymptotically
stable. To show that a system is asymptotically stable using Lyapunov’s direct
Σ(ε)
Σ(ε)
Σ(ε)
Σ(δ)
Σ(δ)
xε
xε
xε
xo
xo
xo
Σ(δ)
Figure 5.7 Illustration of L-stability (A), L-asymptotic stability (B), and instability (C).
ΣðεÞ and ΣðδÞ symbolize n-dimensional balls (spheres) with radii ε and δ, respectively.
145
Mobile Robot Control I: The Lyapunov-Based Method

method, we do not need to find such a distance (norm), but a Lyapunov function
which is actually a generalized energy function.
Definition 5.6 Time-invariant Lyapunov function is called any scalar function VðxÞ
of x which for all t $ t0 and x in the vicinity of the origin satisfies the following
four conditions:1
(i) VðxÞ is continuous and has continuous derivatives
(ii) Vð0Þ 5 0
(iii) VðxÞ . 0 for all x 6¼ 0
(iv) dVðxÞ
dt
5 @VðxÞ
@x

Tdx
dt , 0 for x 6¼ 0
Theorem 5.3 If a Lyapunov function VðxÞ can be found for the state of a nonlinear
or linear system _xðtÞ 5 fðxðtÞ; tÞ, where fð0; tÞ 5 0 (f is a general function), then the
state x 5 0 is asymptotically stable.’
Remarks
(i) If Definition 5.6 holds for all t0, then we have “uniformly asymptotic stability.”
(ii) If the system is linear, or we replace in Definition 5.6 the condition “x in the vicinity of
the origin,” by the condition “x everywhere,” then we have “total asymptotic stability.”
(iii) If the condition (iv) of Definition 5.6 becomes dVðxÞ=dt # 0, then we have (simple)
L-stability.
Clearly, to establish L-stability of a system, we must find a Lyapunov function.
Unfortunately, there does not exist a general methodology for this.
Analogous results hold for the case of time-varying Lyapunov functions VðxðtÞ; tÞ,
namely:
Definition 5.7 Time-varying Lyapunov function Vðx; tÞ for the system’s state is any
scalar function of x and t, which, for all t $ t0 and x near the origin x 5 0, has the
following properties:
(i) Vðx; tÞ and its partial derivatives exist and are continuous
(ii) Vð0; tÞ 5 0
(iii) Vðx; tÞ $ aðjjxjjÞ for x 6¼ 0 and t $ t0, where að0Þ 5 0 and aðξÞ is a scalar continuous
non decreasing function of ξ
(iv) dVðx; tÞ
dt
5 @Vðx; tÞ
@x

Tdx
dt 1 @Vðx; tÞ
@t
, 0 for x 6¼ 0
Remark For all t $ t0, the function Vðx; tÞ should have values greater than or equal
to the values of some continuous nonreducing time-invariant function.
1 Note
that
here
@VðxÞ=@x
is
considered
to
be
a
column
vector,
that
is,
@VðxÞ=@x
5 ½@V=@x1; @V=@x2; . . .; @V=@xnT.
146
Introduction to Mobile Robot Control

Theorem 5.4 If a time-varying Lyapunov function Vðx; tÞ can be found for the state
xðtÞ of the system _xðtÞ 5 fðxðtÞ; tÞ, then the state x 5 0 is asymptotically stable.’
Definition 5.8
(i) If the conditions of Definition 5.7 hold for all t0 and Vðx; tÞ # βðjjxjjÞ, where βðξÞ is
a continuous nondecreasing scalar function of ξ with βð0Þ 5 0, then we have uniformly
asymptotic stability.
(ii) If the system is linear or the conditions of Definition 5.7 hold everywhere (not only in a
region of the origin x 5 0), and if we have aðjjxjjÞ ! N, when jjxjj ! N, then we say
that the system is uniformly totally asymptotically stable.’
In the case of a linear time-varying system _x 5 AðtÞx the Lyapunov (time varying)
function Vðx; tÞ is given by the quadratic (energy) function:
Vðx; tÞ 5 xTPðtÞx
ð5:14aÞ
where P tð Þ satisfies the following matrix differential equation:
dPðtÞ=dt 1 ATðtÞPðtÞ 1 PðtÞAðtÞ 5 2QðtÞ;
QðtÞ . 0
ð5:14bÞ
If the system is time-invariant AðtÞ 5 A 5 const:, then PðtÞ 5 P 5 const: and the above
differential equation for PðtÞ reduces to the following algebraic equation for P:
ATP 1 PA 5 2Q
ð5:15Þ
In this case, we can select a positive definite matrix Q . 0 and solve the
nðn 1 1Þ=2 equations (P is symmetric) for the elements of P. Then, if P . 0 (i.e., if
P is positive definite) the system is asymptotically stable.
Remark Using Eq. (5.15), we can show that the Lyapunov stability criterion is
equivalent to the Hurwitz stability criterion.
5.2.3
State Feedback Control
State feedback control is more powerful than classical control because the design
of a total controller for a multiple-input multiple-output (MIMO) system is per-
formed in a unified way for all control loops simultaneously, and not serially one
loop after the other which does not guarantee the overall system stability and
robustness. In this section we will briefly review the eigenvalue placement control-
ler for SISO systems.
Let a SISO system:
_xðtÞ 5 AxðtÞ 1 BuðtÞ;
yðtÞ 5 CxðtÞ 1 DuðtÞ;
uAR;
yAR;
xARn
147
Mobile Robot Control I: The Lyapunov-Based Method

where A is a n 3 n constant matrix, B is an n 3 1 constant matrix (column vector),
C is an 1 3 n matrix (row vector), u is a scalar input, and D is a scalar constant.
In this case, a state feedback controller has the form:
uðtÞ 5 FxðtÞ 1 υðtÞ
ð5:16Þ
where υðtÞ is a new control input and F is an n-dimensional constant row vector:
F 5 ½f1; f2; . . .; fn. Introducing this control law into the system, we get the state
equations of the closed-loop (feedback) system:
_xðtÞ 5 ðA 1 BFÞxðtÞ 1 BυðtÞ;
yðtÞ 5 ðC 1 DFÞxðtÞ 1 DυðtÞ
ð5:17Þ
The eigenvalue placement design problem is to select the controller gain matrix F
such that the eigenvalues of the closed-loop matrix A 1 BF are placed at desired
positions λ1; λ2; . . .; λn. It can be shown that this can be done (i.e., the system
eigenvalues are controllable by state feedback) if and only if the system ðA; BÞ is
totally controllable. The concept of controllability has been developed to study the
ability of a controller to alter the performance of the system in an arbitrary desired
way. As it is known, the positions of the eigenvalues specify the performance
characteristics of a system.
Definition 5.9 A state x0 of a system is called totally controllable if it can be driven
to a final state xf as quickly as desired independently of the initial time t0. A system
is said to be totally controllable if all of its states are totally controllable.’
Intuitively, we can see that if some state variables do not depend on the control
input uðtÞ, no way exists that can drive it to some other desired state. Thus, this
state is called a noncontrollable state. If a system has at least one noncontrollable
state, it is said to be nontotally controllable or, simply, noncontrollable. The above
controllability concept refers to the states of a system and so it is characterized as
state controllability. If the controllability is referred to the outputs of a system then
we have the so-called output controllability. In general, state controllability and
output controllability are not the same.
Theorem 5.5 The necessary and sufficient condition for a linear system ðA; BÞ to
be totally state controllable is that the controllability matrix:
Qc 5 B ^ AB ^ A2B ^ ? ^ An21B

ð5:18aÞ
has
rank Qc 5 n
ð5:18bÞ
where n is the dimensionality of the state vector x.’
148
Introduction to Mobile Robot Control

The most straightforward technique for selecting the feedback matrix F is
through the use of the controllable canonical form. This technique involves the
following steps:
Step 1: We write down the characteristic polynomial χAðsÞ of the matrix A:
χAðsÞ 5 jsI 2 Aj 5 sn 1 a1sn21 1 ? 1 an21s 1 an
Step 2: Then, we find a similarity transformation T that converts the given system to its
controllable canonical form ^A 5 T21AT.
Step 3: From the desired eigenvalues of the closed-loop system, we determine the desired
characteristic polynomial:
χdesiredðsÞ 5 sn 1 ~a1sn21 1 ? 1 ~an21s 1 ~an
The feedback gain matrix ^F of the controllable canonical model is given by:
^F 5 FT 5 ½ ^f n; ^f n21; . . .; ^f 1
Step 4: Equating the last rows of A 1 BF and ^A 1 ^B^F we find:
a1 2 ^f 1 5 ~a1; a2 2 ^f 2 5 ~a2; . . .; an 2 ^f n 5 ~an
and so solving for F 5 ^FT21 we find:
F 5 ½ ^f n; ^f n21; . . .; ^f 1T21 5 ½an 2 ~an; an21 2 ~an21; . . .; a1 2 ~a1T21
ð5:19Þ
5.2.4
Second-Order Systems
The state vector x of a second-order system contains the position and velocity of
the variable (physical quantity) of interest, namely:
xðtÞ 5
xðtÞ
_xðtÞ


Suppose that it is desired to design the state feedback controller such that the sys-
tem’s state follows a desired trajectory xdðtÞ. In this case, the feedback must use
the measured error:2
~xðtÞ 5 xdðtÞ 2 xðtÞ
ð5:20Þ
and the controller should reduce the sensitivity of the system to the inaccuracy and
the uncertainty in the parameter values used in the dynamic model.
2 It is remarked that the error ~xðtÞ can also be defined as ~xðtÞ 5 xðtÞ 2 xdðtÞ. In this case, the feedback
gains have opposite signs and actually lead to the same negative feedback controller (Section 5.3.1).
149
Mobile Robot Control I: The Lyapunov-Based Method

A fundamental characteristic of control systems is the bandwidth Ω which deter-
mines the operation speed of a system and capability of fast trajectory tracking. The
greater the bandwidth is the better, but Ω should not be very high because it may excite
possible high-frequency components that have not been included in the system model.
As an example, consider the following simple second-order system:
€xðtÞ 5 uðtÞ
ð5:21Þ
which by Eq. (5.20) gives the error equation:
€~xðtÞ 5 €xdðtÞ 2 €xðtÞ 5 €xd 2 uðtÞ
ð5:22Þ
Defining the state vector:
~x 5
~x
_~x
 
5
~x1
~x2


we get the following controllable canonical form:
_~x5 A~x 1 bð€xd 2 uÞ
ð5:23aÞ
where:
A 5
0
1
2a2
2a1


5
0
1
0
0


;
b 5
0
1


ð5:23bÞ
To get a desired bandwidth Ω, we select the desired closed-loop characteristic
polynomial as:
χdesiredðsÞ 5 s2 1 ~a1s 1 ~a2 5 s2 1 2ζωns 1 ω2
n
ð5:24Þ
where ωn is the undamped natural frequency (taken equal to the desired
bandwidth Ω) and ζ is the damping coefficient (usually selected ζ $ 0:7).
Then, the feedback controller is selected as in Eqs. (5.16) and (5.19) with T 5 I
(unit matrix), namely:
€xd 2 u 5 ½f2; f1
~x
_~x
 
1 υ
or
u 5 €xd 2 f2; f1
½

~x
_~x
" #
2 υ
5 €xd 1 ~a2; ~a1
½

~x
_~x
" #
2 υ
5 €xd 1 2ζωn_~x1 ω2
n ~x 2 υ
ð5:25Þ
150
Introduction to Mobile Robot Control

The closed loop error system is obtained using Eqs. (5.22) and (5.25), that is:
€~xðtÞ 1 2ζωn_~xðtÞ 1 ω2
n ~xðtÞ 5 υðtÞ
ð5:26Þ
which has the desired damping and bandwidth specifications.
The control law (5.25) contains the proportional term ω2
n ~x and the derivative
term 2ζωn_~x, that is, it is a PD (proportional plus derivative controller) which is one
of the most popular and efficient controllers. For second-order systems, the PD
controller gives exact results.
Example 5.2
Consider the case where the system (5.21) is corrupted by a disturbance ξðtÞ as:
€xðtÞ 5 uðtÞ 1 ξðtÞ
(a) It is desired to determine the steady-state position error ~xss 5 lim
t!N~xðtÞ of the closed-
loop PD controlled system when ξðtÞ is a step disturbance of amplitude ξ0:
ξðtÞ 5
ξ0;
t $ 0
0;
t , 0
ðξ0 6¼ 0Þ

(b) Show that this steady-state error vanishes if, instead of the PD controller, a PID
(proportional plus integral plus derivative) controller is used.
Solution
(a) With the PD controller, the closed-loop disturbed system (5.26) becomes:
€~xðtÞ 1 2ζωn_~x1 ω2
n ~x 5 υðtÞ 1 ξðtÞ
With υðtÞ 5 0 and ξðtÞ, the above step disturbance, the steady-state position error,
obtained by setting lim
t!N
_~xðtÞ 5 0 and lim
t!N
€~xðtÞ 5 0, satisfies the relation:
ω2
n lim
t!N~xðtÞ 5 ω2
nxss 5 ξ0
Thus:
~xss 5 ξ0=ω2
n
We see that ~xss has a nonzero finite value which is proportional to ξ0 and inversely propor-
tional to the square of the bandwidth Ω 5 ωn.
(b) If we use a PID controller:
u 5 €xd 1 2ζΩ_~x1 Ω2 ~x 1 Ω3
ðt
0
~xðτÞdτ
151
Mobile Robot Control I: The Lyapunov-Based Method

the closed-loop error system becomes:
€~xðtÞ 1 2ζΩ_~xðtÞ 1 Ω2 ~xðtÞ 1 Ω3
ðt
0
~xðtÞdτ 5 υðtÞ 1 ξðtÞ
Then, in the limit as t ! N, for lim
t!N
_~xðtÞ 5 0, lim
t!N
€~xðtÞ 5 0 and υðtÞ 5 0, t $ 0 we get:
Ω2 ~xss 1 Ω3 lim
t!N
ðt
0
~xðτÞdτ 5 ξ0
which implies ~xss 5 lim
t!N~xðtÞ 5 0. Otherwise, lim
t!N
Ð t
0 ~xðtÞdτ cannot have a finite value.
Remark The above results can also be obtained using the well-known Laplace transform
final-value property ~xss 5 lim
t!N~xðtÞ 5 lim
s!0s~xðsÞ and computing ~xðsÞ via the error system’s
transfer function.
Example 5.3
It is desired to check the stability of the following systems using the Lyapunov method:
(a)
_x 5 2x
(b) _x1 5 x2 2 x1ðx2
1 1 x2
2Þ
_x2 5 2x1 2 x2ðx2
1 1 x2
2Þ
Solution
System (a): This system has the equilibrium state x 5 0. We examine if the function
VðxÞ 5 x2 is a Lyapunov function. This is done by checking if all conditions for VðxÞ to be
a Lyapunov function are satisfied. We have:
G VðxÞ 5 x2 and dV=dx 5 2x are continuous
G Vð0Þ 5 0
G VðxÞ 5 x2 . 0 for x 6¼ 0
G _VðxÞ 5 ð2xÞ_x 5 ð2xÞð2xÞ 5 22x2 , 0 for all x 6¼ 0
We see that all properties of the candidate function required to be a Lyapunov function
hold, and so the system _x 5 2 x is uniformly asymptotically stable.
System (b): We try the following candidate Lyapunov function:
VðxÞ 5 x2
1 1 x2
2
This function has the following properties:
G VðxÞ and dV=dx are continuous
G Vð0Þ 5 0
G VðxÞ . 0 for x 6¼ 0
152
Introduction to Mobile Robot Control

G _VðxÞ 5 2x1 _x1 1 2x2 _x2 5 22ðx2
11x2
2Þ2 , 0 for x 6¼ 0
that is, it possesses all properties of a Lyapunov function. Therefore, the only equilibrium
state xðtÞ 5 ½x1ðtÞ; x2ðtÞT 5 0 (i.e., the origin) is totally asymptotically stable.
5.3
General Robot Controllers
The following general controllers, which are standard in robotics [24], will be
examined:
G
Proportional plus derivative control
G
Lyapunov function based control
G
Computed torque control
G
Resolved motion rate control
G
Resolved motion acceleration control
5.3.1
Proportional Plus Derivative Position Control
Here, it will be shown that PD control leads to satisfactory results in the control of
the position of a general robot described in Eqs. (3.11a) and (3.11b):
DðqÞ€q 1 hðq; _qÞ 1 gðqÞ 5 τ
q 5 ½q1; q2; . . .; qnT
where for any _q 6¼0, D is a known positive definite matrix. Assuming that the
friction is negligible and omitting the gravity term gðqÞ, which anyway is zero in
mobile robots moving on an horizontal terrain, we get:
DðqÞ€q 1 Cðq; _qÞ_q 5 τ
ð5:27Þ
where Cðq; _qÞ is defined as in Eq. (3.13), and the matrix _D 2 2C is antisymmetric.
Let ~q 5 q 2 qd be the error between q and qd. Then, the PD controller has the form
τ 5 Kpðqd 2 qÞ 1 Kdð_qd 2 _qÞ
5 2Kp ~q 2 Kd_~q5 2Kp ~q 2 Kd _q
ðsince qd 5 const:Þ
ð5:28Þ
where Kp and Kd are positive definite symmetric matrices. The resulting feedback
control scheme has the form of Figure 5.8.
Let us try the following candidate Lyapunov function:
Vð~qÞ 5 1
2 ð~qTKp ~q 1 _qTD_qÞ
ð5:29Þ
153
Mobile Robot Control I: The Lyapunov-Based Method

where the term ð1=2Þ_qTD_q is the robot’s kinetic energy, and the term ð1=2Þ~qTKp ~q
represents the proportional control term. Thus, the function V can be considered
as representing the total energy of the closed-loop system. Since Kp and D are
symmetric positive definite matrices, we have Vð0Þ 5 0 and Vð~qÞ . 0 for ~q 6¼0.
Therefore, we have to check the validity of property (iv) of Definition 5.6.
Here, Eq. (5.29) gives:
_V 5 ~qTKp _q 1 _qTD~q 1 ð1=2Þ_qT _D_q
5 ~qTKd _q 1 _qTðτ 2 C_qÞ 1 ð1=2Þ_qT _D_q
ð5:30Þ
Now, introducing the control law (5.28) into Eq. (5.30) we get:
_V 5 ~qTKp _q 1 _qTðC 1 KdÞ_q 2 _qTKp ~q 1 ð1=2Þ_qT _D_q
5 2 _qTðC 1 KdÞ_q 1 ð1=2Þ_qT _D_q
ð5:31Þ
Therefore, since the matrix _D 2 2C is antisymmetric, Eq. (5.31) finally gives:
_V 5 2 _qTKd _q # 0
ð5:32Þ
We observe that while the Lyapunov function V in Eq. (5.29) depends on Kp, its
derivative _V depends on Kd, which is analogous of the known property of classical
SISO PD control. The condition (5.32) ensures that the feedback error control
system (Figure 5.8) is L-stable. It is also useful to remark that the PD control (5.28)
is particularly robust with respect to mass variations because it does not require
knowledge of the parameters that depend on mass.
A special case of the controller (5.28) is:
τj 5 2Kjp ~qj 2 Kjd_~qj
ð j 5 1; 2; . . .; nÞ
which is applied to each joint separately. If the motion is subject to friction (assumed
linear) the robot model (5.27) must simply be replaced by:
DðqÞ€q 1 Cðq; _qÞ_q 1 Bf _q 5 τ
where Bf is the diagonal matrix of friction coefficients. The present PD controller
can be enhanced with an integral term as given in Example 5.2.
τ
+
+
+
+
–
–
Robot
qd
Kp
Kd
qd
q
q,q
•
•
q
•
Figure 5.8 PD robot control.
154
Introduction to Mobile Robot Control

5.3.2
Lyapunov Stability-Based Control Design
The control design method applied to the above problem is known as Lyapunov-based
controller design and constitutes a widely used method for both linear and nonlinear
systems. The steps of this method are the following:
Step 1: Select a trial (candidate) Lyapunov function, which is typically some kind of
energy-like function for the system, and possesses the first three properties of Lyapunov
functions (Definition 5.6, Section 5.2.2).
Step 2: Derive the equation for the derivative _VðxÞ along the system trajectory:
_x 5 fðx; u; tÞ
and select a feedback control law:
u 5 uðxÞ
which ensures that:
dVðxÞ
dt
, 0
for x 6¼ 0
Typically, uðxÞ is a nonlinear function of x that contains some parameters and gains
which can be selected to make dV=dt , 0, and thus ensures that the closed-loop system is
asymptotically stable.
Remark For a given system, one may find several Lyapunov functions and
corresponding stabilization controllers. If the system is linear time-invariant
_x 5 Ax 1 Bu, then it is not necessary to work using the Lyapunov method. In this
case, the controller is a static linear state feedback controller uðtÞ 5 FxðtÞ 1 υðtÞ
(Eq. (5.16)), which can be selected to make the closed-loop matrix a Hurwitz
matrix (with all its eigenvalues on the strict left-hand s-plane). This assures that
the closed-loop system is asymptotically and exponentially stable. The Lyapunov-
based controller design method will be applied as a rule in most cases of the
discussions that follow.
5.3.3
Computed Torque Control
The computed torque control technique reduces the effects of the uncertainty in all
the terms of the Lagrange model. The controller τ is selected to have the same
form as the dynamic model (Eq. (3.11a)), that is:
τ 5 DðqÞu 1 hðq; _qÞ 1 gðqÞ
ð5:33Þ
Thus, since the inertia matrix is positive definite (and so invertible), introducing
the control law (5.33) in the system (3.11a), we get:
€qðtÞ 5 uðtÞ
ð5:34Þ
155
Mobile Robot Control I: The Lyapunov-Based Method

This implies that uðtÞ can be a decoupled controller (PD, PID) that can control
each joint (motor axis) independently. The basic problem of the computed torque
method is that we do not have available the exact values of DðqÞ, hðq; _qÞ,
and gðqÞ, but only approximate values ^DðqÞ, ^hðq; _qÞ, and ^gðqÞ. Then, instead of
Eq. (5.33) we get:
τ 5 ^DðqÞu 1 ^hðq; _qÞ 1 ^gðqÞ
ð5:35Þ
and so Eq. (5.34) is replaced by:
€qðtÞ 5 ðD21 ^DÞu 1 D21ð^h 2 hÞ 1 D21ð^g 2 gÞ
ð5:36Þ
A problem with the model in Eq. (5.35) is the investigation of its robustness to
modeling uncertainties which include uncertainties in the parameter values and
nonmodeled high-frequency components (e.g., structural resonance, sampling rate, or
omitted time delays). The computed torque control method belongs to the general
class of linearization techniques via nonlinear state feedback (see Section 6.3).
Solving the model (Eq. (3.11a)) for €q we get:
€q 5 D21ðqÞ½τ 2 hðq; _qÞ 2 gðqÞ
ð5:37Þ
Introducing the controller (5.35) into (5.37) we obtain the block diagram of the
overall closed-loop system shown in Figure 5.9.
5.3.4
Robot Control in Cartesian Space
The controllers presented thus far work in the joints’ (motors’) space and are based
on the error ~q 5 q 2 qd between the actual and desired generalized joint variables
q1; q2; . . .qn (called internal variables). The motion of the robot in the Cartesian
(or task, or working) space is obtained indirectly from the motion of the joints.
However, in many cases, it is required to design the controller so as to work
directly with the Cartesian variables, called external variables.
τ
q
q
•
q
••
u
++
+
+
+
+
+
–
•
D(q)
D–1(q)
g(q)
∫
∫
g(q)
h(q,q)
•
h(q,q)
Figure 5.9 Closed-loop computed torque control system.
156
Introduction to Mobile Robot Control

In Cartesian space, we have three types of controllers which are known as resolved
motion controllers:
G
Resolved motion rate control
G
Resolved motion acceleration control
G
Resolved force control
Here, we will study the resolved motion rate control which is mostly used in
mobile robots. The resolved acceleration controller is actually an extension of the
resolved motion rate control that includes the acceleration, a fact that will also be
briefly considered.
5.3.4.1
Resolved Motion Rate Control
The resolved motion rate control is the control where the joints are moved simulta-
neously in different velocities, such that a desired motion in Cartesian (or task) space
is obtained.
In general, the relation of the linear and angular velocity (motion rate) vector:
_pðtÞ 5
vðtÞ
ωðtÞ


in Cartesian space and the velocities _qðtÞ of the robotic joints is given by the Jacobian
relation (Eq. (2.6)):
_p 5 JðqÞ_q
ð5:38Þ
where the Jacobian matrix J is given by Eq. (2.5), which has the inverse (see Eqs. (2.7b)
and (2.8)):
_q 5 J21ðqÞ_p
ðif J is invertibleÞ
ð5:39aÞ
or the generalized inverse:
_q 5 JyðqÞ_p
ðif J is not squareÞ
ð5:39bÞ
Differentiating Eq. (5.38) we get:
_vðtÞ
_ωðtÞ


5 _JðqÞ_q 1 JðqÞ€q
ð5:40Þ
Thus, introducing into Eq. (5.40) the expression of _qðtÞ given by Eq. (5.39a) yields:
€qðtÞ 5 J21ðqÞ
_vðtÞ
_ωðtÞ


2 J21ðqÞ_JðqÞJ21ðqÞ vðtÞ
ωðtÞ


ð5:41Þ
157
Mobile Robot Control I: The Lyapunov-Based Method

This relation gives the accelerations of the joints for given linear/angular velocity
and acceleration of the robot end-effector in Cartesian space. If JðqÞ is not square,
we use the generalized inverse Jy in place of J21.
The block diagram of the resolved velocity control is shown in Figure 5.10.
In the simplest case, the joints control can be a proportional control law with
gain Ka. In many cases, the task space control is required to be in a coordinate
frame attached to the robot (and not in the world coordinate frame). The velocity
_pðtÞ is then given by:
_pðtÞ 5 R0
m_rðtÞ
ð5:42Þ
where _rðtÞ is the desired velocity of the robot and R0
m is a matrix that relates _rðtÞ
to _pðtÞ. Then, from Eqs. (5.39a) and (5.39b) we get:
_qðtÞ 5 JyðqÞ_pðtÞ 5 JyðqÞR0
m_rðtÞ
ð5:43Þ
The relation (5.43) is typically used in vision-based robot control (visual servoing).
5.3.4.2
Resolved Motion Acceleration Control
The resolved motion acceleration control method is based on the equation:
€pðtÞ 5 JðqÞ€qðtÞ 1 _JðqÞ_qðtÞ
ð5:44Þ
which is found by differentiating Eq. (5.38). The desired position, velocity, and
acceleration of the robot in Cartesian space are assumed to be known from the trajec-
tory planner. Thus, to reduce the position error, we must apply appropriate forces/
torques at the robot joints, such that the acceleration in Cartesian space satisfies the
relation:
_vðtÞ 5 _vdðtÞ 1 Kdv½vdðtÞ 2 vðtÞ 1 Kpv½sdðtÞ 2 sðtÞ
ð5:45Þ
where sdðtÞ; vdðtÞ, and _vdðtÞ are the desired translation position, velocity, and accel-
eration, respectively.
Robot
+
–
q,q
•
P
•
Pd
•
P
•∼
J –1(q)
Robot
J  (q)
Joints’
Control Law
Sensors
Figure 5.10 Block diagram of the robot resolved rate control based on Eq. (5.39a).
158
Introduction to Mobile Robot Control

Here, the position error is:
epðtÞ 5 sdðtÞ 2 sðtÞ
Therefore, in terms of epðtÞ, Eq. (5.45) is written as:
€epðtÞ 1 Kds_epðtÞ 1 KpsepðtÞ 5 0
ð5:46Þ
and the gains Kps; Kds should be selected such that epðtÞ tends asymptotically to zero.
A similar error equation can be derived for the angular acceleration _ωðtÞ, using the
control law:
_ωðtÞ 5 _ωdðtÞ 1 Kdφ½ωdðtÞ 2 ωðtÞ 1 Kpφ½ϕdðtÞ 2 ϕðtÞ
ð5:47Þ
where ϕðtÞ is the orientation angle in the Cartesian space. Combining Eqs. (5.45)
and (5.47), and inserting into Eq. (5.44), we get:
€qðtÞ 5 J21ðqÞf€pdðtÞ 1 Kd½_pdðtÞ 2 _pðtÞ 1 KpeðtÞ 2 _JðqÞ_qg
ð5:48Þ
where:
_pdðtÞ5
vd
ωd


; eðtÞ5
sd 2s
ϕd 2ϕ


5 ep
eφ


; Kp 5 Kps
0
0
Kpφ


; Kd 5 Kds
0
0
Kdφ


Equation (5.48) constitutes the basis for the resolved motion acceleration control
of robots. The position qðtÞ and velocity _qðtÞ are measured by potentiometers or
optical encoders.
5.4
Control of Differential Drive Mobile Robot
The control procedure will involve two stages:
1. Kinematic stabilizing control
2. Dynamic stabilizing control
The resulting linear and angular velocities in the kinematic stage will be used as
reference inputs for the dynamic stage. Thus, this procedure belongs to the general
class of backstepping control [513].
5.4.1
Nonlinear Kinematic Tracking Control
The robot motion is governed by the dynamic model (Eqs. (3.23a) and (3.23b), the
kinematic model (Eq. (2.26)), and the nonholonomic constraint (Eq. (2.27)), namely:
_v 5 1
mr ðτr 1 τlÞ 5 1
m τa;
τa 5 1
r ðτr 1 τlÞ
ð5:49aÞ
159
Mobile Robot Control I: The Lyapunov-Based Method

_ω 5 2a
Ir ðτr 2 τlÞ 5 1
I τb;
τb 5 2a
r ðτr 2 τlÞ
ð5:49bÞ
_x 5 v cos φ
_y 5 v sin φ
ð5:49cÞ
where, for notational simplicity, the index Q was dropped from xQ, yQ and ω 5 _φ,
τa, τb are the control inputs, with:
p 5 ½x; y; φT
ð5:49dÞ
being the state vector.
The problem is to track a desired state trajectory:
pdðtÞ 5 ½xdðtÞ; ydðtÞ; φdðtÞT
ð5:50Þ
with error that goes asymptotically to zero.
To this end, the Lyapunov stabilizing method will be used. For realizability, the
desired trajectory must satisfy both the kinematic equations and the nonholonomic
constraint, that is:3
_xd
_yd
_φd
2
64
3
75 5
vd cos φd
vd sin φd
ωd
2
64
3
75;
_xdsin φd 5 _yd cosφd
ð5:51Þ
The errors ~x 5 ðxd 2 xÞ, ~y 5 ðyd 2 yÞ, and ~φ 5 φd 2 φ, expressed in the wheeled
mobile robots’ (WMR’s) local (moving) coordinate frame Qxryr, are given by (see
Eq. (2.17)):
~xr
~yr


5
cos φ
2sin φ
sin φ
cos φ

21
~x
~y
 
5
cos φ
sin φ
2sin φ
cos φ


~x
~y
 
ð5:52aÞ
and
~φr 5 ~φ
ð5:52bÞ
Differentiating Eqs. (5.52a) and (5.52b) and taking into account Eqs. (5.49c) and (5.51),
we get the following kinematic model for the error ½~xr; ~yr; ~φrT:
_~xr 5 vd cos ~φr 2 v 1 ~yrω
_~yr 5 vd sin ~φr 2 ~xrω
_~φr 5 ωd 2 ω
_~pr 5
_~xr
_~yr
_~φr
2
664
3
775
ð5:53Þ
3 This is equivalent to considering that our WMR has to track a similar (virtual) differential-drive WMR
which is moving with linear velocity vd and angular velocity ωd.
160
Introduction to Mobile Robot Control

where the linear and angular velocities v and ω are the kinematic control variables.
Clearly, Eq. (5.53) satisfies the kinematic and nonholonomic equations of the WMR.
Therefore, the kinematic feedback controller will be based on Eq. (5.53).
The Lyapunov stabilizing method of Section 5.3.2 will be applied. Since here the
controller should be nonlinear, we cannot select its structure beforehand. Its struc-
ture will be determined by the choice of the candidate Lyapunov function. Here,
the following candidate function is selected [14]:
Vð~prÞ 5 1
2 ð~x2
r 1 ~y2
r Þ 1 ð1 2 cos ~φrÞ
ð5:54Þ
This function satisfies the first three properties of Lyapunov functions, namely:
(i) Vð~prÞ is continuous and has continuous derivatives
(ii) Vð0Þ 5 0
(iii) Vð~prÞ . 0 for all ~pr 6¼0
We therefore have to check under what conditions the fourth property can be
satisfied.
Differentiating Eq. (5.54) with respect to time we get:
_Vð~prÞ 5 ð2v 1 vd cos ~φrÞ~xr 1 ð2ω 1 vd ~yr 1 ωdÞsin ~φr
ð5:55Þ
To make _Vð~prÞ # 0, the control inputs v and ω are selected such that:
_Vð~prÞ 5 2ðKx ~x2
r 1 Kjsin2 ~φrÞ
ð5:56Þ
which leads to:
v 5 vc 5 Kx ~xr 1 vd cos ~φr
ð5:57aÞ
ω 5 ωc 5 Kφ sin ~φr 1 vd ~yr 1 ωd
ð5:57bÞ
Clearly, for Kx . 0 and Kφ . 0, we have _Vð~prÞ # 0 with the equality obtained only
when ~xr  0 and ~φr  0. Thus, the controller ((5.57a) and (5.57b) guarantees total
asymptotic tracking to the desired trajectory.
Remark 1 We can also add a gain Ky in the second term of Eq. (5.56b), that is,
choose ω as
ω 5 ωc 5 ωd 1 Kyvd ~yr 1 Kφ sin ~φr
ð5:58aÞ
In this case, total asymptotic tracking to the desired trajectory ½xdðtÞ; ydðtÞ; φdðtÞT
can be proved using the following Lyapunov function:
V 5 1
2 ð~x2
r 1 ~y2
r Þ 1
1
Ky


ð1 2 cos ~φrÞ;
Ky . 0
ð5:58bÞ
161
Mobile Robot Control I: The Lyapunov-Based Method

Remark 2 A more general kinematic controller can be obtained by using the
Lyapunov function:
Vð~prÞ 5 Kpð~x2
r 1 ~y2
r Þμ 1 Kqð1 2 cos ~φrÞ
ð5:58cÞ
with Kp . 0, Kq . 0, and μ . 1. In the following, we will derive this controller.
To this end, we define new control variables u1 5 vd cos ~φr 2 v, u2 5 ω 2 ωd and
write the model (5.53) as:
_~xr 5 ω~yr 1 u1;
_~yr 5 vd sin ~φr 2 ~xrω;
_~φr 5 u2
ð5:58dÞ
Differentiating Eq. (5.58c) and using the model (5.58d) we get:
_V 5 2Kpμð~x2
r 1 ~y2
r Þμ21 ~xru1 1 Kqðsin ~φrÞu2
1 2Kpμð~x2
r 1 ~y2
r Þμ21 ~yrvd sin ~φr
To assure that _V # 0, we use two functions FðxÞ $ M . 0 and GðxÞ $ M . 0 for all
x 5 ~prAR3, and select u1 and u2 such that:
_V 5 22K2
pμ2 ~x2
r ð~x2
r 1 ~y2
r Þμ21FðxÞ 2 K2
qðsin2 ~φrÞGðxÞ , 0
ð5:58eÞ
Then, it follows that:
u1 5 2μKp ~xrFð~prÞ
u2 5 2ð2Kp=KqÞμð~x2
r 1 ~y2
r Þμ21vdyr 2 KqðsinφrÞGð~prÞ
The above controller assures that ~x2
r 1 ~y2
r ! 0 and ~φr ! kπ ðk 5 0; 1; 2; . . .Þ. Since
_Vð~prÞ is uniformly continuous, Barbalat’s Lemma (Sec. 6.2.3) implies that
_Vð~prÞ ! 0. This by Eq. (5.58e) implies that xr ! 0 and ~φr ! kπ ðk 5 0; 1; 2; . . .Þ.
Now, obviously, _~φr ! 0 (i.e., ω ! ωd). One way to overcome the fact that _~φr does
not converge only to 0 but also to kπ ðk 5 1; 2; . . .Þ is to take care that the WMR,
before trying to track immediately the desired trajectory (i.e., the virtual WMR), is
rotating about its own axis with an increasing angular velocity ω until it sees the
virtual robot. The reader can verify that this can be done by the controller [11]:
u
1 5 γðtÞu1ðtÞ;
u
2 5 γðtÞu2ðtÞ 1 ½1 2 γðtÞωðtÞ
where γðtÞ is given by the dynamic model:
a2 €γðtÞ 1 a1 _γðtÞ 1 γðtÞ 5 σðtÞ
with σðtÞ being a step input function defined by:
σðtÞ 5
1
for t1A½0; t with φðt1Þ 5 atan2ð~yrðt1Þ; ~xrðtlÞÞ
0
otherwise

162
Introduction to Mobile Robot Control

5.4.2
Dynamic Tracking Control
Having selected v and ω as in Eqs. (5.57a) and (5.57b) (or Eq. (5.58a)), we select
the control inputs (torques) τa and τb in Eq. (5.49a) as:
τa 5 m_vc 1 Ka ~vc
ð5:59aÞ
τb 5 I _ωc 1 Kb ~ωc
ð5:59bÞ
where:
~vc 5 vc 2 v
~ωc 5 ωc 2 ω
ð5:59cÞ
Introducing Eqs. (5.59a)(5.59c) into Eqs. (5.49a) and (5.49b) we get the velocities’
error equations:
_~vc 1 ðKa=mÞ~vc 5 0;
_~ωc 1 ðKb=IÞ ~ωc 5 0
which for Ka . 0 and Kb . 0 are stable and ~vc, ~ωc converge to zero asympto-
tically. Therefore, in selecting the feedback control inputs (torques) as in
Eqs. (5.59a) and (5.59b) with vc and ωc given by Eqs. (5.57a) and (5.57b), the
tracking of the desired trajectory ½xdðtÞ; ydðtÞ; φdðtÞT is achieved asymptotically,
as required. The block diagram of the feedback tracking controller is depicted
in Figure 5.11.
P
P=
Pd =
•
1/m
1/I
Controller
(5.59 a,b)
Controller
(5.57 a,b)
Transformation
(5.52 a,b)
Kinematics
(5.49 c)
+
–
–
+
τa
τb
υ
ω
φ
υ
ω
x
y
φ
x
y
φ
xd
yd
φd
∫
∫
υc
ωc
υc
ωc
∼
∼
∼
∼
∼
Figure 5.11 The complete trajectory tracking feedback control system of the WMR.
163
Mobile Robot Control I: The Lyapunov-Based Method

5.5
Computed Torque Control of Differential
Drive Mobile Robot
The control design procedure involves again two stages: kinematic control followed
by dynamic control. We will work on the WMR shown in Figure 5.12, where the
motor dynamics includes a gear box (of ratio N) [13].
Here, Q is the midpoint of wheel baseline, G the center of gravity, and C the
point traced by the controller (different than the point Q).
The meaning of the remaining symbols are self-evident (the same as in Figure 2.7).
5.5.1
Kinematic Tracking Control
The kinematic equations of the robot are:
_x 5 v cos φ 2 ½_yr 1 ðc 2 bÞωsin φ
5 v cos φ 2 cω sin φ
ð5:60aÞ
_y 5 v sin φ 1 ½_yr 1 ðc 2 bÞωsin φ
5 v sin φ 1 cω cos φ
ð5:60bÞ
_φ 5 ω
ð5:60cÞ
where _yr is the lateral velocity in the local coordinate frame Gxryr.
Equations (5.60a) and (5.60b) are written in the matrix form:
_x
_y
 
5 cos φ
2c sin φ
sin φ
c cos φ


v
ω


5Rðc;φÞ v
ω


Now, using new control variables uv and uφ defined by:
v
ω


5R21ðc;φÞ uv
uφ


5
cos φ
sin φ
2ð1=cÞsin φ
ð1=cÞcos φ

 uv
uφ


ðc 6¼ 0Þ
ð5:61Þ
x
D
y
o
Q
2α
ф
v
r
G
yr
xr
bC
c
Figure 5.12 Differential drive WMR where
the point C traced by the controller is
different than Q and G.
164
Introduction to Mobile Robot Control

we get:
_x 5 uv;
_y 5 uφ
ð5:62Þ
The dynamic system (5.62) is linear and decoupled, and so the state-feedback law:
uv 5 _xd 1 Kx ~x;
uφ 5 _yd 1 Ky ~y
ð5:63Þ
yields the error dynamics:
_~x1 Kx ~x 5 0;
_~y1 Ky ~y 5 0
ð5:64Þ
with ~x 5 xd 2 x and ~y 5 yd 2 x.
Therefore, from Eq. (5.64), it follows that for any positive gains:
Kx . 0;
Ky . 0
the tracking error tends exponentially to zero.
Combining Eqs. (5.61) and (5.63) we get the overall nonlinear kinematic control
law:
v
ω


5
cos φ
sin φ
2ð1=cÞsin φ
ð1=cÞcos φ


_xd
_yd


1
Kx
0
0
Ky


~x
~y
 


ð5:65Þ
5.5.2
Dynamic Tracking Control
The feedback kinematic controller (5.65) incorporates the WMR kinematic equations
and so one can now use the reduced (unconstrained) dynamic model ((3.19a) and
(3.19b)) of the robot for the selection of the control inputs (motor torques or motor
voltages), as described in Section 5.4.2, where actually the computed torque method
was applied.
For the robot of Figure 5.12, with the motor dynamics included, the reduced
model has the following form:
D_v 1 Cv 5 EV
ð5:66Þ
where:
v5
v
ω


; D5 D11
0
0
D22


; C5 C11 C12
C21 C22


; E5 E1
0
0
E2


; V5 Va
Vb


ð5:67Þ
165
Mobile Robot Control I: The Lyapunov-Based Method

with:
D11 5 ð1 1 2Im=mr2Þ
D22 5
Iz 1 2a2
r2 Im 1 b2m
0
@
1
A
C11 5 2
m
βm
r2 1 N2K1K2
Rr2
0
@
1
A;
C12 5 2bω
C22 5 2a2
βm
r2 1 N2K1K2
Rr2
0
@
1
A;
C21 5 bmω
E1 5 ðNK1=RrmÞVa;
E2 5 ðNK2 a=RrÞVb
Va 5 Vr 1 Vl;
Vb 5 Vr 2 Vl
ð5:68Þ
Here:
Im 5 combined wheel, motor rotor, and gearbox inertia
βm 5 combined wheel, motor, and gearbox friction coefficient
Vr; Vl 5 right and left wheel motor voltage
R 5 electrical resistance
K1; K2 5 motor voltage/torque constants
Now, applying the computed torque (linearization) technique to Eq. (5.66), we
choose the voltage control vector V as:
V 5 E
21ðDu 1 CvÞ
ð5:69Þ
where u is the new control vector. Introducing Eq. (5.69) into Eq. (5.66) we get:
_v 5 u
with:
v 5
v
ω


;
u 5
u1
u2


Therefore, selecting the linear state feedback control law:
u 5 K~v 1 _vd
ð5:70Þ
166
Introduction to Mobile Robot Control

yields the error system:
_~v1 K~v 5 0;
K 5 diag K1; K2
½

which for K . 0 is asymptotically stable with equilibrium state ~v 5 vd 2 v 5 0.
Combining Eq. (5.69) with Eq. (5.70), we get the full dynamic controller:
V 5 E21ðDK~v 1 _vd 1 CvÞ
ð5:71Þ
The overall tracking controller of the robot is given by Eqs. (5.65) and (5.71).
Example 5.4
It is desired to formulate and solve the kinematic problem of a unicycle-like WMR to go
asymptotically from an initial state (position and orientation) to a goal state (position
and orientation) using polar coordinates.
Solution
The unicycle-like robots are described by the kinematic Eq. (2.26):
_x 5 v cos φ;
_y 5 v sin φ;
_φ 5 ω
ð5:72Þ
where v is the velocity and φ is the angle of the WMR xr axis with the goal (world) coordinate
frame x-axis. All WMRs with the above kinematic equations, for example, the differential drive
WMR, are said to belong to the unicycle-like class of mobile robots. The geometry of the WMR
that will be used for formulating the polar coordinates is shown in Figure 5.13 [15].
The kinematic control variables of the robot are v and ω. The polar coordinates (position
and orientation) of the WMR are its distance l from the goal, and its orientation ψ with
respect to the goal’s coordinate frame Gxy. The steering angle is ζ 5 ψ 2 φ. Then, in polar
coordinates, the kinematic model in Eq. (5.72) is replaced by:
_l 5 2v cos ζ
ð5:73aÞ
_ζ 5 2ω 1 ðv=lÞsin ζ
ð5:73bÞ
_ψ 5 ðv=lÞsin ζ
ð5:73cÞ
ф
ψ
y
x
G
ζ
ψ
Q
1
Goal
v
yr
xr
Figure 5.13 Polar coordinates of the unicycle. Here,
the goal coordinate frame Gxy is considered to be the
world coordinate frame.
167
Mobile Robot Control I: The Lyapunov-Based Method

These relations hold for l . 0, a condition which will be always satisfied by an asymptotic
reduction of l to zero (since for any finite time there always be l . 0).
Our goal tracking control problem is to find a state-feedback law:
v
ω


5 uðl; ζ; ψÞ
ð5:74Þ
which guarantees that l ! 0, ζ ! 0, and ψ ! 0, asymptotically.
To this end, we will apply the Lyapunov-based control method. Let us choose the
following candidate Lyapunov function [15]:
VðxÞ 5 1
2 xTQx;
x 5 ½l; ζ; ψT
Q 5
q1
0
1
0
q2
2
4
3
5;
q1 . 0;
q2 . 0
ð5:75Þ
Clearly, the function VðxÞ possesses the first three properties of Lyapunov functions.
We will determine the controller (5.74) which will assure that the fourth property _V # 0 is
also possessed by V, along the system trajectory.
The time derivative of VðxÞ, along the trajectory determined by Eqs. (5.73a)(5.73c),
is found to be:
_VðxÞ 5 xTQ_x
5 q1l_l 1 ζ _ζ 1 q2ψ _ψ 5 _V1 1 _V2
ð5:76aÞ
where:
_V1 5 q1l_l 5 2q1lv cos ζ
ð5:76bÞ
_V2 5 ζ _ζ 1 q2ψ _ψ
5 ζ½ 2 ω 1 ðv=lÞsin ζ 1 ðq2=lÞðsin ζÞψv
ð5:76cÞ
Choosing v as:
v 5 K1ðcos ζÞl;
K1 . 0
ð5:77Þ
yields:
_V1 5 2 K1q1ðcos2 ζÞl2 # 0
ð5:78Þ
Now, introducing Eq. (5.77) into Eq. (5.76c) we get:
_V2 5 ζ½ 2 ω 1 K1ðcos ζÞðsin ζÞðζ 1 q2ψÞ=ζ
168
Introduction to Mobile Robot Control

Thus, selecting ω as:
ω 5 K2ζ 1 K1ðcos ζÞðsin ζÞðζ 1 q2ψÞ=ζ; K2 . 0
ð5:79Þ
yields:
_V2 5 2K2ζ2 # 0
ð5:80Þ
The inequalities (5.78) and (5.80) give:
_V 5 2K1q1ðcos2 ζÞl2 2 K2ζ2 # 0
ð5:81Þ
This implies, by the Lyapunov stability theorem, that ζ and l go asymptotically to zero for
any ψ. Thus, we have to see what happens to ψ. To this end, we get the closed-loop system
kinematics by introducing the control laws ((5.77) and (5.79)) into Eqs. (5.73a)(5.73c),
namely:
_l 5 2K1l cos2 ζ;
lð0Þ . 0
ð5:82aÞ
_ζ 5 2K2ζ 2 K1q2ðcos ζÞ sin ζ
ζ


ψ
ð5:82bÞ
_ψ 5 K1ðcos ζÞðsin ζÞ
ð5:82cÞ
These equations show that the asymptotic convergence of l and ζ to zero implies the
asymptotic convergence of ψ to its only equilibrium state ψs 5 0. In fact, from Eq. (5.82c)
it follows that ζ ! 0 implies _ψ ! 0, that is, ψ tends to some finite value ψs. Then,
we see from Eq. (5.82b) that the uniformly continuous function _ζ tends necessarily
to 2K1q2ψs.4 On the other hand, by Barbalat’s lemma, _ζ tends to zero, which in turn
implies that ψs 5 0 (see Section 6.2.3). Therefore, the smooth kinematic control laws
((5.77) and (5.79)) assure the asymptotic tracking of the goal position and orientation by
the WMR, as desired. The fact that the control laws ((5.77) and (5.79)) are continuously
differentiable does not contradict Brockett’s Theorem 6.6 and its corollary (c) because
this theorem is valid for the Cartesian state-space representation (Eq. (5.72)) of the
unicycle-like WMR [15].
5.6
Car-Like Mobile Robot Control
For the car-like WMR, we will study the following two representative problems:
1. Parking (or posture) control
2. Leaderfollower (formation) control
4 Recall that ðsin ζÞ=ζ ! 1 as ζ ! 0.
169
Mobile Robot Control I: The Lyapunov-Based Method

5.6.1
Parking Control
Consider a car-like WMR (Figure 2.9) which controls the steering angle ψ and the
rear-wheels’ velocity v1. The orientation of the car body (i.e., of v1) is φ. The kinematic
equations of the robot are given by Eq. (2.52):
_x 5 v1cos φ
_y 5 v1sin φ
_φ 5 ðv1=DÞtg ψ
_ψ 5 v2
ð5:83Þ
The problem is to control the WMR (using v1 and ψ) so as to move it to a desired
parking position and orientation, which here is assumed to be x 5 0, y 5 0, and
φ 5 0 (as it was actually done in Example 5.4 for the unicycle WMR). Here, this
problem will be solved by a two-step maneuver to overcome the turning radius
limitation of the car-like mobile robot, namely [16]:
Step 1: Controller stabilizing y and φ
Step 2: Controller stabilizing x and φ
Use of the Lyapunov-based control method will again be made as usual.
Step 1: ðy;ϕÞ control
We select the following candidate Lyapunov function:
VðxÞ 5 1
2 xTQx;
x 5
y
φ


;
Q 5
q1
0
0
1


which satisfies the first three conditions of Lyapunov functions. We will check if
the third condition can be satisfied, along the trajectory of the system in Eq. (5.83).
We have:
_V 5 xTQ_x 5 q1y_y 1 φ_φ
5 q1y sin φ 1 φ=D

	
tg ψ


v1
ð5:84Þ
Choosing:
v1 51jV1j 5 const:
or
v1 5 2jV1j 5 const:
tg ψ 5 2 D
v1

q1v1
sin φ
φ
y 1 K1φ

ð5:85Þ
170
Introduction to Mobile Robot Control

and introducing into Eq. (5.84) yields:
_V 5 2K1φ2 # 0
for K1 . 0
ð5:86Þ
which, by Lyapunov theorem, implies that φ tends asymptotically to zero.
The closed loop kinematics equation is:
_φ 5 2K1φ 1 q1v1
sin φ
φ


y
ð5:87Þ
Let φ 5 0. For φ to stay zero, _φ should be zero. Then, Eq. (5.87) implies that y should
also tend to zero, for v1 5 const: 6¼ 0.
The change of the sign of v1 is needed when the WMR cannot move with its
current velocity due to the presence of obstacles or when the measured states
exceed some predetermined bounds. Of course, the initial selection of jV1j affects
the efficiency of the path. Actually, the controller (5.85) is a nonlinear bang-bang
controller. Therefore, a switching rule is needed to determine when the change
from v1 51jV1j to v1 5 2jV1j must occur.
Defining ε 5 tgðy=xÞ, the switching rule is:
If cosðε 2 φÞ . 0
Then v , 0; otherwise v . 0
This rule means that if the front part of the WMR is closer to the origin, then it
will go forward or backward.
Step 2: ðx;ϕÞ control
We use the same form of the candidate Lyapunov function:
VðxÞ 5 1
2 xTQx
with x 5 ½x; φT and Q 5 diag ½q1; 1.
The time derivative _V along the trajectory of Eq. (5.83) is:
_V 5 q1x_x 1 φ_φ 5 q1xv1 cos φ 1 φðv1=DÞtg ψ
Choosing:
v1 5 2K2x; tg ψ 5 2v1φ
ð5:88Þ
gives:
_V 5 2K2q1x2 cos φ 2 ð1=DÞðv2
1φ2Þ
5 2K2x2½q1 cos φ 1 ðK2=DÞφ2 $ 2K2x2
ð5:89Þ
which is negative semidefinite for K2 . Dq1.
171
Mobile Robot Control I: The Lyapunov-Based Method

For φ 5 0, in which case, by Eq. (5.88), ψ 5 0, we have
_V 5 2K2q1x2 # 0
Therefore, the mobile robot is uniformly L-stable at x 5 0. However, φ cannot
converge without increasing jxj, a fact which is due to the low bound of the WMR
turning radius (Figure 5.14A) [16,17]. Actually, φ can be made arbitrarily small
at the first step. Therefore, when we achieve a very small φ (i.e., jφj , ε), we use
ψ 5 0, and v1 5 2K2x.
This situation is overcome if we use the transformation [16]:
θ 5 atan2ðy; xÞ
for
jatan2ðy; xÞ 2 φj , 90
θ 5 atan2ðy; xÞ 2 sgnðatan2ðy; xÞ 2 φÞ 3 180
for
jatan2ðy; xÞ 2 φj . 90
ð5:90Þ
In this way, the distance from the origin to the WMR is equal to the error of x, and
the difference angle of the WMR with the x-axis becomes the error of φ. As shown
in Figure 5.14D, the controller (5.88) with the above switching type transformation
assures that the WMR can go to ðx; yÞ 5 ð0; 0Þ starting from any initial posture.
5.6.2
LeaderFollower Control
Consider two car-like robots that follow a path with the first car acting as the leader
and the second being a follower (Figure 5.15). For more WMRs, one following the
other in front of it, this problem is known as formation control [7].
The leaderfollower control problem under consideration is to find a velocity
control input for the follower that assures convergence of the relative distance Llf
and relative bearing angle θlf of the WMRs to their desired values, under the
y
x
v1
y
x
θ
X
Y
x
X
y
Y
θ
v1
(A)
(B)
Initial posture
(C)
(D)
ψ
ϕ
0
0
0
0
Figure 5.14 (A) For φ to converge,
jxj must be increased. (B) Case
jatan2ðx; yÞ 2 φj , 90. (C) Case
jatan2ðx; yÞ 2 φj . 90.
(D) Convergence to the origin
ðx; yÞ 5 ð0; 0Þ using Eq. (5.90).
172
Introduction to Mobile Robot Control

assumption that the leader motion is known and is the result of an independent
control law [7]. To solve the problem, we will apply the Lyapunov-based control
design method using the kinematic and dynamic equations of the bicycle equivalent
presented in Section 3.4 (see Eqs. (3.56), (3.57), (3.60a)(3.60d)).
In Figure 5.15, vl, φl, and ψl are the linear velocity, orientation angle, and steering
angle of the leader, and vf, φf, ψf are the respective variables of the follower. The
coordinates of points Gl and Gf are denoted by ðxl; ylÞ and ðxf; yfÞ.
We first derive the dynamic equations for the errors:
ε1 5 xfd 2 xf;
ε2 5 yfd 2 yf;
ε3 5 φfd 2 φf
ð5:91Þ
where xfd, yfd, and φfd represent the desired trajectory of the follower in world
coordinates, which are transformed to εf1, εf2, and εf3 in the local coordinate frame
of the follower. From Figure 5.15 we get:
L2
lf 5 L2
lf;x 1 L2
lf;y
ð5:92aÞ
Llf;x 5 xl 2 xf 2 dðcos φl 1 cos φfÞ
ð5:92bÞ
Llf;y 5 yl 2 yf 2 dðsin φl 1 sin φfÞ
ð5:92cÞ
tgðθlf 1 φl 2 πÞ 5 Llf;y=Llf;x
ð5:92dÞ
ψf
A
φf
Gf
Gl
B
d
d
D
a
a
0
y
x
d
d
D
θlf
Vf
Llf
ψl
Vl
φl
(A)
(B)
Figure 5.15 (A) Two car-like WMRs (leaderfollower structure). (B) Four WMRs in a
typical formation (diamond structure).
Source: www.robot.uji.es/lab/plone/research/pnebot/index_html2.
173
Mobile Robot Control I: The Lyapunov-Based Method

Differentiating Eqs. (5.92b) and (5.92c) gives:
_Llf;x 5 _xl 2 _xf 1 dð_φl sin φl 1 _φf sin φfÞ
ð5:93aÞ
_Llf;y 5 _yl 2 _yf 2 dð_φl cos φl 1 _φf cos φfÞ
ð5:93bÞ
with (see Eqs. (3.56) and (3.57)):
_ygl 5 ðd=DÞ_xgltgψl;
_ygf 5 ðd=DÞ_xgftgψf
ð5:93cÞ
where D 5 2d.
Using Eqs. (3.56) and (5.93c) in Eqs. (5.93a) and (5.93b), we obtain:
_Llf;x 5 _xgl cos φl 2 _xgf cos φf 1 _xgfðtg ψfÞsin φf
ð5:94aÞ
_Llf;y 5 _xgl sin φl 2 _xgf sin φl 2 _xgfðtg ψfÞcos φf
ð5:94bÞ
while from Figure 5.13 we have:
Llf;x
Llf
5 cosðθlf 1 φl 2 πÞ;
Llf;y
Llf
5 sinðθlf 1 φl 2 πÞ
ð5:95Þ
Then, differentiating Eqs. (5.92a) and (5.92d), introducing Eqs. (5.94a) and (5.94b),
and using the auxiliary variable:
ζf 5 θlf 1 φl 2 φf
we get, after some algebraic manipulation:
_Llf 5 2 _xgl cosðθlfÞ 1 _xgftgðψfÞsin ζf 1 _xgf cos ζf
ð5:96aÞ
_θ lf 5ð1=LlfÞ½ð_xgl sin θlf 2 _xgf sin ζfÞ1 _xgftg ψf cos ζf2ð1=DÞ_xgltgψl
ð5:96bÞ
Using Figure 5.15, the actual and desired coordinates of the follower’s point A can
be expressed in terms of the coordinates of the leader’s point B. Therefore, we
use the variables fLlf; θlfg and fLlfd; θlfdg and get the error equations (in the world
coordinate frame):
εf1 5 Llfd cosðθlfd 1 εf3Þ 2 Llf cosðθlf 1 εf3Þ 2 d cosðεf3Þ 1 d
ð5:97aÞ
εf2 5 Llfd sinðθlfd 1 εf3Þ 2 Llf sinðθlf 1 εf3Þ 2 d sinðεf3Þ
ð5:97bÞ
εf3 5 φl 2 φf
ð5:97cÞ
174
Introduction to Mobile Robot Control

Finally, differentiating Eqs. (5.97a)(5.97c), we obtain the dynamic equations for
εf1, εf2, and εf3:
_εf1 5 _xgl cosðεf3Þ 2 _xgf 1 _ygl½2Llf sin ζf 2 εf2 1 _ygfεf2
ð5:98aÞ
_εf2 5 _xgl sinðεf3Þ 2 D_ygf 1 _yglðεf1 2 dÞ 2 _ygfðεf1 2 dÞ 1 _yglLlf cos ζf
ð5:98bÞ
_εf3 5 ð1=DÞ½_xgl tg ψl 2 _xgf tg ψf
ð5:98cÞ
We are now ready to apply the usual two-stage (kinematic, dynamic) backstep
controller design.
5.6.2.1
Kinematic Controller
We select the candidate Lyapunov function [7]:
V 5 1
2 ðq1ε2
f1 1 q2ε2
f2Þ 1 q3ð1 2 cosðεf3ÞÞ
ð5:99Þ
which is similar to Eq. (5.54), which possesses the first three properties of
Lyapunov functions. The feedback control inputs _xgf and _ygf will be selected such
that to make _V , 0. Differentiating Eq. (5.99) gives:
_V 5 q1εf1_εf1 1 q2εf2_εf2 1 q3ðsin εf3Þ_εf3
ð5:100Þ
Introducing Eqs. (5.98a)(5.98c) into Eq. (5.100), we find that the selection of _xgf
and _ygf as:
_xgf 5 Kxfεf1 1 _xgl cosðεf3Þ 2 _yglLlf sinðζfÞ
ð5:101aÞ
_ygf 5 2 _ygl 1
1
d
0
@
1
AðKx 1 _xglÞsinðεf3Þ 1
1
d
0
@
1
A_yglLlf cosðζfÞ 1 εf2
2
1
qdjεf2j 1 q3
f2q3 _ygl 1 ð1=dÞq3 _yglLlf 1 q3jεf2j 1 qKxjεf2jg
ð5:101bÞ
with q1 5 q2 5 q makes _V , 0. Indeed, introducing Eqs. (5.98a)(5.98c), (5.101a),
and (5.101b) into Eq. (5.100) yields:
_V , 2fqKxfε2
f1 1 qdε2
f2 1 ð1=dÞ½q3ðKx 1 _xglÞsin2 εf3g
Since _xgl . 0, choosing q . 0, q3 . 0, and Kx . 0 makes _V , 0.
175
Mobile Robot Control I: The Lyapunov-Based Method

5.6.2.2
Dynamic Controller
Use will be made of the dynamic model (Eqs. (3.60a)(3.60d)). The desired velocity
and steering angle vfd, ψfd of the follower are given by the results of the kinematic
controller. We define the error:
~zf 5 zfd 2 zf;
zf 5
vf
ψf


;
zfd 5
vfd
ψfd


ð5:102Þ
From Eqs. (3.60a)(3.60c), applied to the follower WMR, we get:
_vf 5 1
m 2Ff sin ψf 1 d
D2 v2
f tg2 ψf 1 τf
r


ð5:103Þ
where τf 5 rFd is the driving torque of the steering wheel of radius r, and the relation
_xgf 5 vf was used. Combining Eqs. (5.103) and (3.60d) gives:
_zf 5 2AðzfÞzf 2 G 1 Eτ
ð5:104Þ
where:
AðzfÞ 5
a11
a12
a21
a22


;
G 5
g1
g2


;
E 5
e11
0
0
e22


;
τ 5
τf
us


a11 5 2ðd=D2Þvf tg2 ψf;
a12 5 a21 5 0;
a22 5 1=T
g1 5 ð1=mÞFf sin ψf;
g2 5 0;
e11 5 1=rm;
e22 5 K=T
Subtracting both sides of Eq. (5.104) from _zfd we get:
_~zf 5 _zfd 1 AðzfÞzf 1 G 2 Eτ
Now, adding and subtracting AðzfÞzfd to the right-hand side of this equation yields:
_~zf 5 2AðzfÞ~zf 1 _zfd 1 AðzfÞzfd 1 G 2 Eτ
5 2AðzfÞ~zf 1 Fðx0Þ 2 Eτ
ð5:105aÞ
where:
Fðx0Þ 5 AðzfÞzfd 1 _zfd 1 G
ð5:105bÞ
and x0 5 ½εf1; εf2; εf3T:
176
Introduction to Mobile Robot Control

The torque τ in Eq. (5.105a) can be found using the computed torque technique as:
τ 5 E21½K~zf 1 Fðx0Þ
ð5:106Þ
which is introduced into Eq. (5.105a) to give the closed-loop error equation:
_~zf 5 2ðA 1 KÞ~zf
ð5:107Þ
It only remains to select K such that ~zf tends to zero asymptotically. We select the
candidate Lyapunov function:
V0 5 V 1 1
2 ~zT
f ~zf
ð5:108Þ
where V is given by Eq. (5.99).
Differentiating V0 and introducing the result into Eq. (5.107) gives:
_V0 5 _V 2 ~zT
f ðA 1 KÞ~zf
Since
_V , 0 by the kinematic controller design, we can assure that
_V0 , 0 by
selecting the gain matrix K such that the matrix A 1 K is positive definite, that is:
K 5
K1 1 ðd=D2Þvf tg2 ψf
0
0
K2 2 1=T


or
A 1 K 5
K1
0
0
K2


with K1 . 0 and K2 . 0. Then, we find:
_V0 5 _V 2 K1 ~v2
f 2 K2 ~ψ
2
f
This implies that the error ~zf in Eq. (5.105a) tends asymptotically to zero, that is,
vf ! vfd and ψf ! ψfd, as required. The function Fðx0Þ in the control law Eq. (5.106)
can be approximated by a neural network using a weight updating rule as described in
Section 8.5 [7].
5.7
Omnidirectional Mobile Robot Control
We will consider the three-wheel omnidirectional dynamic robot model (Eqs. (3.77a)
and (3.77b)) derived in Section 3.5:
_x 5 AðxÞx 1 BðxÞu;
y 5 Cx
ð5:109Þ
177
Mobile Robot Control I: The Lyapunov-Based Method

and will apply the resolved motion acceleration technique, combined with PI or
PD control [18]. Solving Eq. (5.109) for uiði 5 1; 2; 3Þ, we get the inverse dynamic
resolved acceleration equations:
u1 5 ðβ1=6b1Þ½€x0 2 a1 _x 1 a2 _φ_y
1 ðβ3=6b1Þ½€y0 2 a1 _y 2 a2 _φ_x 1 ð1=3b2Þð€φ0 2 a3 _φÞ
ð5:110aÞ
u2 5 ðβ2=6b1Þ½€x0 2 a1 _x 1 a2 _φ_y
1 ðβ4=6b1Þ½€y0 2 a1 _y 2 a2 _φ_x 1 ð1=3b2Þð€φ0 2 a3 _φÞ
ð5:110bÞ
u3 5 ðcos φ=3b1Þ½€x0 2 a1 _x 1 a2 _φ_y
1 ðsin φ=3b1Þ½€y0 2 a1 _y 2 a2 _φ_x 1 ð1=3b2Þ½€φ0 2 a3 _φ
ð5:110cÞ
where:
€x0 5 €xd 1 Kp_x_~x1 Ki_x
ðt
0
_~x dτ
ð5:111aÞ
€y0 5 €yd 1 Kp_y_~y1 Ki_y
ðt
0
_~y dt
ð5:111bÞ
€φ0 5 €φd 1 Kυφ _~φ1 Kpφ ~φ
ð5:111cÞ
with:
_~x5 _xd 2 _x;
_~y5 _yd 2 _y;
~φ 5 φd 2 φ
being the errors between the desired and actual trajectories, and Kp_x, Kp_y, Kpφ being
proportional gains, Ki_x, Ki_y integral gains, and Kυφ the derivative (velocity) gain of φ.
Note that the factors b1 and b2 in Eqs. (5.110a)(5.110c) are always nonzero
(see Eqs. (3.76a)(3.76c)) and so the above resolved acceleration controllers u1, u2,
and u3 exist for all t.5
The block diagram of the overall feedback control WMR system is shown in
Figure 5.16.
The above controller was applied to a real robot with physical parameters:
IQ 511:25 kgm2; I0 50:02108 kg m2; β55:98331026kgm2=s; M59:4 kg;
L50:178 m; r50:0245 m; and K 51:0
5 All the coordinates are those of the center of gravity (and symmetry) Q. The index Q was dropped for
notational simplicity.
178
Introduction to Mobile Robot Control

The initial state used is:
xð0Þ 5 ½xQð0Þ; yQð0Þ; φð0Þ; _xQð0Þ; _yQð0Þ; _φð0ÞT 5 0
A basic experiment was to check the WMRs holonomic property, that is, the ability
of the robot to independently achieve translational and rotational motion around the
center of gravity in the x-y plane.
To check this property, it was assumed that the robot must travel with a single
azimuth ψd 5 π=4 rad (see Figure 3.6) for 20 s, but with zero rotational angle for
010 s, and a uniformly varying rotational angle from φd 5 0 rad to φd 5 π=2 rad
for 1020 s. Although the moving velocity was to be vd 5 0:05 m=s in the steady
state, a sinusoidal reference velocity was set for each 2 s at the starting and ending
period. Using these vd and ψd values, the corresponding _xd and _yd were derived from:
_xd 5 v cos ψ;
_yd 5 v sin ψ
where the positive rotational direction of motion is the counterclockwise direction.
The desired accelerations €xd and €yd were derived by differentiation of _xd and _yd.
A selection of the gains used is given in Table 5.1.
The responses obtained for _x, _y, and φ, and the (x,y) trajectory are shown in
Figure 5.17.
We observe that despite the occurrence of some oscillations in the velocities _x
and _y, the ðx; y; φÞ trajectory matches perfectly the desired trajectory ðxd; yd; φdÞ.
Another experiment was carried out to follow a circular path with a radius less
than half the distance between the wheels (say 0.1 m with distance between the
wheels 0.356 m). Using the same gains as before, the results were very good, with
perfect tracking of the circular path [18].
+
+
+
+
+
+
+
+
+
+
−
−
−
xd
Kp,x
y0
y
x
u1
u2
u3
x0
Kp,y
xd
yd
yd
φd
φ0
d/dt
d/dt
φ
φ
·
·
·
Kp,φ
R −1
Robot
Kv,φ
··
·
··
··
··
··
y0
x0
φ0
·
·
·
··
Figure 5.16 Resolved acceleration control system for the three-wheel omnidirectional robot
(R21 represents the robot inverse dynamics Eqs. (5.110a)(5.110c)).
179
Mobile Robot Control I: The Lyapunov-Based Method

Example 5.5
Apply the computed torque technique to derive a PD path tracking controller for a three-wheel
omnidirectional robot.
Solution
We will work with the dynamic model derived for the omnidirectional robot of Figure 3.7
in Example 3.2, namely:
DðpQÞ€pQ 1 CðφÞ_pQ 5 Ev
ð5:112Þ
Table 5.1 Gains of the Resolved Acceleration PI/PD Controller
Kpφ
Kυφ
Kp_x
Ki_x
Kp_y
Ki_y
2.25
3.0
10.0
25.0
10.0
25.0
0.04
0.02
Velocity
Velocity
0
0
10
20
(A)
(B)
0.04
0.02
0
0
0
10
20
1
Time t(s)
0
10
20
(C)
Time t(s)
Time t(s)
Reference
Response
Rotational angle
xd(    )
m
s
φ (rad)
φ (rad)
Reference
Response
0
0.2
0.4
0.6
Y-distance (m)
0
0.2
0.4
0.6
(D)
X-distance (m)
Reference
Response
Reference
Response
.
yd(    )
m
s
.
Figure 5.17 (A,C) Actual velocity responses compared to the desired velocity responses _xd
and _yd. (B) Trajectory of φ, (D) (x,y) trajectory.
Source: Reprinted from Ref. [18], with permission from Springer Science1Business Media BV.
180
Introduction to Mobile Robot Control

The computed torque control law has the form (Eq. (5.33)):
Ev 5 DðpQÞu 1 CðφÞ_pQ
ð5:113Þ
and the PD control has the form:
u 5 2Kp ~pQ 2 Kd_~pQ 1 €pQ;d
ð5:114Þ
where:
~pQðtÞ 5 pQðtÞ 2 pQ;dðtÞ
ð5:115Þ
is the tracking error of the resulting path from the desired path pQ;dðtÞ. Combining Eqs. (5.113)
and (5.114) and introducing into Eq. (5.112), we get the closed-loop system:
D€pQ 5 2DðKp ~pQ 1 Kd_~pQÞ 1 D€pQ;d
or the tracking error dynamic equation:
€~pQ 1 Kd_~pQ 1 Kp ~pQ 5 0
ð5:116Þ
since the matrix D is nonsingular. We see that the origin ~pQ 5 0 is an equilibrium point
of this system. Therefore, selecting proper values of Kd and Kp, we can assure that ~pQðtÞ
tends to zero, that is, pQðtÞ ! pQ;dðtÞ, asymptotically. To this end, we use the following
candidate Lyapunov function:
Vð~pQÞ 5 1
2 ~pT
QðKp 1 γKdÞ~p 1 1
2
_~p
T
Q_~pQ 1 γ~pT
Q_~pQ
ð5:117Þ
which, for sufficiently small constant γ . 0, satisfies the first two properties of Lyapunov
functions.
We will now examine under what conditions _Vð~pQÞ is negative. The time derivative of
Vð~pQÞ along the trajectory of €pQðtÞ 5 2Kd_~pQðtÞ 2 Kp ~pQðtÞ is found to be:
_V 5 ~pT
QðKp 1 γKdÞ_~pQ 1 _~pQT €~pQ 1 γ~pT
Q€~pQ 1 γ_~pQT _~pQ
5 ~pT
QðKp 1 γKdÞ_~pQ 1 _~pTð2 Kd_~pQ 2 Kp ~pQÞ
1 γ~pT
Qð2 Kd_~pQ 2 Kp ~pQÞ 1 γ_~pQT _~pQ
5 2_~pQTðKd 2 γIÞ_~pQ 2 γ~pT
QKp ~pQ
Choosing Kd 2 γI . 0 and Kp . 0 (positive definite), we get
_V , 0 and so ~pQ !0
asymptotically.
181
Mobile Robot Control I: The Lyapunov-Based Method

The performance of the controller was tested on the WMR of Section 5.7 with the same
values of physical parameters and the same desired path using the PD gains given in Table 5.2.
The responses obtained for the ðx; y; φÞ and ð_x; _yÞ trajectories are similar to those shown
in Figure 5.17. As an exercise, the reader may test the performance of the controller for a
circular desired path with a given radius R (e.g., R 5 10 or 40 cm). A parameterized form
of the circle equation, namely, xdðtÞ 5 R cos adðtÞ, ydðtÞ 5 R sin adðtÞ can be used where
adðtÞ is the angle of the point ðxd; ydÞ with reference to the x-axis of the world coordinate
frame. In this case, a suitable polynomial or other series representation of the angle aðtÞ
can be used. A more complex desired path that might be examined is an 8 shaped path of
a given size.
References
[1] Ogata K. State space analysis of control systems. Upper Saddle River, NJ: Prentice
Hall; 1997.
[2] Asada H, Slotine JJ. Robot analysis and control. New York, NY: Wiley; 1986.
[3] Spong MW, Vidyasagar M. Robot dynamics and control. New York, NY: Wiley; 1989.
[4] Wolovich W. Robotics: Basic Analysis and Design. Birmingham, UK: Holt Rinehart
and Winston, Dreyden Press;1987.
[5] Kanayama Y, Kimura Y, Noguchi T. A stable tracking control method for a nonholo-
nomic mobile robot. IEEE Trans Robot Autom 1991;7:123641.
[6] Yiaoping Y, Yamamoto Y. Dynamic feedback control of vehicles with two steerable
wheels. Proceedings of IEEE international conference on robotics and automation.
Minneapolis, MN; 1996. 12(1), p. 10061010.
[7] Panimadai Ramaswamy SA, Balakrishnan SN. Formation control of car-like mobile
robots: a Lyapunov function based approach. Proceedings of 2008 American Control
Conference. Seattle, Washington; June 1113, 2008.
[8] Tian Y, Sidek N, Sarkar N. Modeling and Control of a nonholonomic wheeled mobile
robot with wheel slip dynamics. Proceedings of IEEE symposium on computational intel-
ligence in control and automation. Nashville, TN; March 30April 2, 2009. p. 714.
[9] Chang CF, Huang CI, Fu LC. Nonlinear control of a wheeled mobile robot with nonholo-
nomic constraints. Proceedings of 2004 IEEE International conference on systems, man,
and cybernetics. The Hague, The Netherlands; October 1013, 2004. p. 54049.
[10] Samson C, Ait Abderrahim K. Feedback control of nonholonomic wheeled cart
in Cartesian space. Proceedings of IEEE Conference on robotics and automation.
Sacramento, CA; 1990. p. 113641.
[11] Velagic J, Lacevic B, Osmic N. Nonlinear motion control of mobile robot dynamic
model [Chapter 27] In: Jing X-J, editor. Motion Planning. In Tech, Open Books; 2008.
p. 53456.
Table 5.2 Gains of the Computed Torque PD Controller
Kpx
Kpy
Kpφ
Kdx
Kdy
Kdφ
40
30
20
70
55
10
182
Introduction to Mobile Robot Control

[12] Zhang Y, Hong D, Chung JH, Velinsky SA. Dynamic model based robust tracking
control of a differentially steered wheeled mobile robot. Proceedings of american
control conference (ACC ’88). Philadelphia, PA; June 1988. p. 8505.
[13] Ashoorizad M, Barzamini R, Afshar A, Zouzdani J. Model reference adaptive path
following for wheeled mobile robots. Proceedings of international conference on infor-
mation and automation (IEEE/ICIA’06). Colombo, Sri Lanka; 2006. p. 28994.
[14] Gholipour A, Dehgham SM, Ahmadabadi MN. Lyapunov based tracking control of non-
holonomic mobile robot. Proceedings of 10th Iranian conference on electrical engineering.
Tabeiz, Iran; 2002. 3, p. 26269.
[15] Aicardi M, Casalino G, Bicchi A, Balestrino A. Closed-loop steering of unicycle-like
vehicles via Lyapunov techniques. IEEE Robot Autom Mag 1995; March:2735.
[16] Lee S, Kim M, Youm Y, Chung W. Control of a car-like mobile robot for parking
problem. Proceedings of 1999 IEEE international conference on robotics and automation.
Detroit, MI; May 1999. p. 15.
[17] Lee S, Youm Y, Chung. Control of car-like mobile robots for posture stabilization.
Proceedings IEEE/RSJ international conference on intelligent robots and systems (IROS’99).
Kyongju, Korea; October 1999. p. 174550.
[18] Watanabe K, Shiraishi Y, Tang J, Fukuda T, Tzafestas SG. Autonomous control for an
omnidirectional mobile robot with feedback control system [Chapter 13] In: Tzafestas
SG, editor. Advances in intelligent autonomous systems. Boston / Dordrecht: Kluwer;
1999. p. 289308.
183
Mobile Robot Control I: The Lyapunov-Based Method

6 Mobile Robot Control II: Affine
Systems and Invariant
Manifold Methods
6.1
Introduction
The control of wheeled mobile robots (WMRs) is a challenging subject for both its
theoretical and practical value. Mobile robots, omnidirectional and nonholonomic,
are highly nonlinear, and especially nonholonomic constraints have motivated the
development of highly nonlinear control techniques. Most of the control results
available in the literature were developed for the two major categories of nonholo-
nomic WMRs, namely:
G
Unicycle-type WMRs that involve two independently driven wheels on a common axis
and one or more passive/castor wheels.
G
Rear-drive car-like WMRs that involve a motorized real-axis at the rear of the chassis,
and one (or two) orientable front steering wheel(s).
As discussed in Chapter 5, the three control problems of mobile robots are as
follows:
1. Path tracking: Given a curved planar path, the WMR has to follow this path with a
prespecified longitudinal velocity.
2. Trajectory tracking: Here, the velocity is not prespecified. The WMR has, in addition to
path following, the goal to control the distance gone along the curve.
3. Posture stabilization: The goal is to stabilize at zero the posture (position and orientation)
of the WMR with respect to a fixed coordinate frame (e.g., in car parking).
In Chapter 5, we have presented the basic computed torque/motion and
Lyapunov-based control design techniques drawn from standard robotic manipulator
control theory. The goal of this present chapter is to provide further advanced WMR
control methods either utilizing the state feedback linearization of affine systems
(before a linear controller design), or following the invariant manifolds methodology
(which leads directly to overall nonlinear controllers).
In particular, the chapter:
G
provides an introduction to the fundamental concepts of affine control systems, invariant/
attractive manifolds, and related extended Lyapunov stability theory;
Introduction to Mobile Robot Control. DOI: http://dx.doi.org/10.1016/B978-0-12-417049-0.00006-7
© 2014 Elsevier Inc. All rights reserved.

G
presents a number of feedback linearization (and linear trajectory tracking) controllers for
differential-drive and car-like WMRs;
G
derives kinematic and dynamic controllers of Brockett integrator and (2,n)-chained
models of differential-drive and car-like WMRs.
6.2
Background Concepts
6.2.1
Affine Dynamic Systems
As we saw in Chapters 2 and 3, mobile robots belong to the affine systems class of
nonlinear systems described by the general state-space equation [14]:
_x 5 g0ðxÞ 1
X
m
i51
giðxÞui;
xARn;
giðxÞARn
5 g0ðxÞ 1 GðxÞu
ð6:1Þ
with:
x 5 ½x1; x2; :::; xnT
u 5 ½u1; u2; :::; umT;
m # n
G xð Þ 5 ½g1ðxÞ^g2ðxÞ^:::^gmðxÞ
gi x
ð Þ 5 gi1ðxÞ; gi2ðxÞ; :::; ginðxÞ
½
T
where the drift term g0ðxÞ represents the general kinematic constraints of the system.
Consider a nonlinear mapping from the n-dimensional Euclidean space Rn to
itself:
z 5 ϕðxÞ;
xARn;
zARn
ð6:2Þ
where z is a new vector, and ϕðxÞARn is a vector function (field):
ϕðxÞ 5
φ1ðxÞ
φ2ðxÞ
^
φnðxÞ
2
664
3
775;
x 5
x1
x2
^
xn
2
664
3
775;
z 5
z1
z2
^
zn
2
664
3
775
with the following properties:
1. The function ϕðxÞ is invertible, that is, there exists a function ϕ21ðzÞ such that:
ϕ21ðϕðxÞÞ 5 x;
ϕðϕ21ðzÞÞ 5 z
ð6:3Þ
for all xARn and zARn.
2. Both functions ϕðxÞ and ϕ21ðzÞ have continuous partial derivatives of any order
(i.e., they are smooth functions).
186
Introduction to Mobile Robot Control

Definition 6.1 (Diffeomorphism)
A function of the type (6.2) with the above two properties is called diffeomorphism.
Sometimes, it is not possible to find a diffeomorphism valid for all x in a domain of
interest. In these cases, we can define a diffeomorphism in the vicinity of a given point
x0 in this domain. A transformation of this type is called a local diffeomorphism.
The condition for the existence of a local diffeomorphism in the vicinity of
a point x 5 x0 is the nonsingularity of the Jacobian matrix @ϕðxÞ=@x at the
point x0.’
Definition 6.2 (Lie derivative)
Given a smooth real-valued scalar function:
sðxÞ 5 sðx1; x2; :::; xnÞAR
ð6:4Þ
and a vector-valued function (vector field):
fðxÞ 5
f1ðx1; :::; xnÞ
^
fnðx1; :::; xnÞ
2
4
3
5ARn
ð6:5Þ
of the vector variable x 5 ½x1; x2; . . .; xnT, the scalar function LfsðxÞ is defined as:
LfsðxÞ 5 Lfsðx1; x2; :::; xnÞ
5
@sðxÞ
@x
2
4
3
5fðxÞ 5 ðrsðxÞÞfðxÞ
5
X
n
i51
@s
@xi
fiðx1; x2; :::; xnÞ
ð6:6Þ
where:
@sðxÞ
@x
5 rsðxÞ 5 @sðxÞ
@x1
; @sðxÞ
@x2
; . . .; @sðxÞ
@xn


is called the Lie derivative of sðxÞ along the field fðxÞ.’
Clearly, the function LfsðxÞ represents the projection of the gradient vector
rsðxÞ along the vector fðxÞ, that is, the directional derivative of sðxÞ along the
direction of the vector fðxÞ.
Successive application of Eq. (6.6) gives, for example, the Lie derivative of sðxÞ
first along fðxÞ and then along another function gðxÞ, that is:
LgLfsðxÞ 5 @LfsðxÞ
@x


gðxÞ
187
Mobile Robot Control II: Affine Systems and Invariant Manifold Methods

In general:
Lk
fsðxÞ 5 @Lk21
f
sðxÞ
@xk21


fðxÞ
ð6:7Þ
For example, given _x 5 fðxÞ, y 5 hðxÞ we find:
_y 5 @h
@x


_x 5 Lfh;
€y 5 @ðLfhÞ
@x


_x 5 L2
f h
and so on.
Using the Lie derivative, we can introduce the concept of relative degree of a
system, as follows:
Definition 6.3 (Relative degree)
We say that the affine system:
_x 5 fðxÞ 1 gðxÞu;
y 5 hðxÞ;
uAR
ð6:8Þ
has relative degree r at the point x0 if:
(a) LgLk
fhðxÞ 5 0 for all x in the vicinity of x0 and all k , r 2 1
(b) LgLr21
f
hðx0Þ 6¼ 0’
It is easy to verify that the relative degree r of a system is equal to the number of
times the output y must be differentiated such that the input uðtÞ appears in the deriva-
tive equation. This shows the importance of the functions hðxÞ; LfhðxÞ; . . .; Lr21
f
hðxÞ,
because they can be used for finding a local transformation in the vicinity of x0, where
x0 is a point for which the relation:
LgLr21
f
hðx0Þ 6¼ 0
ð6:9Þ
holds.
As an example, consider the system:
_x 5 fðxÞ 1 gðxÞu;
y 5 hðxÞ 5 x3
where:
x 5
x1
x2
x3
2
64
3
75;
fðxÞ 5
0
x2
1 1 sin x2
2x2
2
64
3
75;
gðxÞ 5
ex2
1
0
2
64
3
75
188
Introduction to Mobile Robot Control

For this system, we have:
@h
@x 5 ½ 0
0
1 ;
LghðxÞ 5 0;
LfhðxÞ 5 2x2
@Lfh
@x 5 ½ 0
21
0 ;
LgLfhðxÞ 5 21
Thus, the system has relative degree r 5 2 at any point x0. If we choose
y 5 hðxÞ 5 x2, we get LghðxÞ 5 1, and so the relative degree of the system is r 5 1
at any point x0.
Definition 6.4 (Flow induced by a vector field)
Consider the system:
_x 5 fðxÞ;
xAXCRn
ð6:10Þ
and assume that there is a unique solution xðx0; tÞ for each initial state xðx0; 0Þ 5 x0
(which is not necessary to be found analytically). Then, the mapping:
ðx0; tÞ ! xðx0; tÞ
ð6:11Þ
is called the flow (or dynamic) system induced by the vector field f.’
Essential properties of the flow are the following:
1. xðx0; 0Þ 5 x0 for each x0AX
2. xðx0; t 1 sÞ 5 xðxðx0; tÞ; sÞ 5 xðxðx0; sÞ; tÞ for all x0AX and s; t on the real line R
3. @
@txðx0; tÞ 5 fðxðx0; tÞÞ
Denote by exp tf the transformation on X induced by f. Clearly, exp f maps
each initial point onto xðx0; tÞ. From properties 1 and 2 it follows that
exp 0f 5 identity, and expðt 1 sÞf 5 ðexp tfÞðexp sfÞ, which denotes the mapping
composition.
Since exp 0f 5 I, it follows that:
ðexp tfÞ21 5 exp 2 tf
and so, the mappingfexp tf:tARg constitutes a commutative group of (local) diffeo-
morphisms. This group is called the 1-parameter group of diffeomorphisms induced
by f.
If f is linear vector field, that is, f 5 Ax on X, then xð0; tÞ 5 etAx0 with
etA 5 PN
k50ðtk=k!ÞAk. Therefore, in this case, exp tf is a linear transformation on X
equal to the exponential of a linear mapping (matrix) A.
189
Mobile Robot Control II: Affine Systems and Invariant Manifold Methods

Definition 6.5 (Lie bracket)
Let fðxÞ and gðxÞ be two vector functions (fields) of the vector variable
x 5 ½x1; x2; :::; xnT. Then, we define a new vector function of x denoted by ½f; gðxÞ
and called the Lie bracket (or Lie product) as:
f; g
½
ðxÞ 5
@g
@x


fðxÞ 2
@f
@x


gðxÞ
ð6:12Þ
where @f=@x and @g=@x are the Jacobian matrices of fðxÞ and gðxÞ, with elements
ð@f=@xÞij 5 @fi=@xj and ð@g=@xÞij 5 @gi=@xj, respectively.’
For example, the Lie bracket of the system _x1 5 22x1 1 bx2 1 sin x1;
_x2 5
2x2 cos x1 1 u cos 2x1, which can be written as:
_x 5 fðxÞ 1 gðxÞu;
fðxÞ 5 22x1 1 bx2 1 sin x1
2x2 cos x1


;
gðxÞ 5
0
cos 2x1


is:
½f; gðxÞ 5
0
0
22 sin x1
0
"
# 22x1 1 bx2 1 sin x1
2x2 cos x1
"
#
2
22 1 cos x1
b
x2 sin x1
2cos x1
"
#
0
cos 2x1
"
#
5
b cos 2x1
cos x1 cos 2x1 2 2ðsin 2x1Þð22x1 1 bx2 1 sin x1Þ
"
#
The Lie bracket can be applied successively, using the following notation:
adfgðxÞ 5 ½f; gðxÞ
ad2
f gðxÞ 5 ½f; ½f; gðxÞ
^
adk
f gðxÞ 5 ½f; adk21
f
gðxÞðxÞ
ð6:13Þ
with initial condition ad0
f gðxÞ 5 gðxÞ. Clearly, the Lie bracket has the properties:
1. ½f; gðxÞ 5 2½g; fðxÞ
2. If ½f; gðxÞ 5 0, then ðexp tfÞðexp sgÞ 5 ðexp sgÞðexp tfÞ and conversely (communication of
flows)
190
Introduction to Mobile Robot Control

For example, if fðxÞ 5 a and gðxÞ 5 b are two arbitrary constant fields, and so
½f; gðxÞ 5 0, then ðexp tfÞðxÞ 5 x 1 ta and ðexp sgÞ 5 x 1 sb. Therefore:
ðexp tfÞðexp sgÞðxÞ 5 ðx 1 sbÞ 1 ta 5 ðx 1 taÞ 1 sb
5 ðexp sgÞðexp tfÞðxÞ
which confirms that the flows commute.
Definition 6.6 (Involutive set)
A set of n-dimensional column vector functions (fields) ΔðxÞ 5 fX1ðxÞ; X2ðxÞ;
. . .; XmðxÞg for which the matrix ½X1ðxÞ; X2ðxÞ; . . .; XmðxÞ has rank m at the point
x 5 x0
is called involutive in the vicinity of x0, if for every pair ði; jÞ;
ði; j 5 1; 2; :::; mÞ, the matrix:
ΔðxÞ 5 ½X1ðxÞ; X2ðxÞ; . . .; XmðxÞ; ½Xi; XjðxÞ
ð6:14Þ
has also rank m for all x in a vicinity of x0.’
The field distribution ΔðxÞ 5 fX1ðxÞ; X2ðxÞ; . . .; XmðxÞg is said to be nonsingular
if dim ΔðxÞ 5 r is constant for all x, in which case r is called the dimension of the
distribution. The distribution Δ in Eq. (6.14) that has the same rank as Δ for all x
in a vicinity of x0 is called the involutive closure of Δ under the Lie bracket
operation.
The concepts of Lie derivative, Lie bracket, and relative degree play a key role
in the analysis and design of affine control systems, namely, in the solution of state
feedback linearization, total stabilization via state feedback, adaptive control,
robust control, and controllability/observability problems.
A central result used in the above studies is the Frobenius theorem which gives
the necessary and sufficient condition for a set (distribution) of m vector fields to
be completely integrable. We will formulate this theorem.
Let a driftless system be of the form:
_x 5
X
m
i51
giðxÞui;
xAXDRn
ð6:15Þ
From Definition 6.6, the distribution:
ΔðxÞ 5 fg1ðxÞ; g2ðxÞ; . . .; gmðxÞg
ð6:16aÞ
is involutive if for each Lie bracket ½gi; gjðxÞ, there exist m real coefficients
βk ðk 5 1; 2; :::; mÞ such that:
½gi; gjðxÞ 5
X
m
k51
βkgkðxÞ
ð6:16bÞ
191
Mobile Robot Control II: Affine Systems and Invariant Manifold Methods

which means that every Lie bracket can be expressed as a linear combination
of the system vector fields, and so it already belongs to Δ. In other words, the
Lie brackets cannot escape Δ and produce new directions of motion. It is noted
that because of the property ½gi; gjðxÞ 5 2½gj; giðxÞ ði; j 5 1; 2; . . .; mÞ, it is not
necessary to consider all m2 possible brackets, but it is sufficient to consider
m
2


5 mðm 2 1Þ
2
Lie brackets. Using the above concepts, the Frobenius theorem
is stated as follows.
Theorem 6.1 (Frobenius theorem)
A smooth affine system with a nonsingular distribution is completely integrable
if and only if it is involutive.’
Example 6.1
Consider a two-input driftless affine system with:
g1ðxÞ 5
x2
0
1
2
4
3
5;
g2ðxÞ 5
x3
1
0
2
4
3
5
The dimension of Δ 5 fg1; g2g is 2 at any point xAR3. The Lie product is found to be:
½g1; g2ðxÞ 5 0
Therefore, Δ is involutive, and hence Δ and the system is integrable. Indeed, the
system gives:
_x1 5 x2u1 1 x3u2;
_x2 5 u2;
_x3 5 u1
that is, _x1 5 x2 _x3 1 x3 _x2 5 dðx2x3Þ=dt, whence x1 2 x2x3 5 k, where k is a real constant.
Example 6.2
The unicycle WMR (2.28a) has two fields:
g1 5
cos φ
sin φ
0
2
4
3
5;
g2 5
0
0
1
2
4
3
5
and so it yields a two-dimensional distribution:
Δ 5 fg1; g2g 5
cos φ
sin φ
0
2
4
3
5;
0
0
1
2
4
3
5
8
<
:
9
=
;
192
Introduction to Mobile Robot Control

This distribution is nonsingular because for any ð x
y
φ Þ in the coordinate neighbor-
hood, the resulting vector space Δðx; y; φÞ is two-dimensional. We form the distribution
Δ by adding to Δ a third column equal to ½g1; g2ðxÞ. Noting that x 5 ½x; y; φT, this is
found to be:
g1; g2


ðxÞ 5 @g2
@x g1 2 @g1
@x g2 5
sin φ
2cos φ
0
2
4
3
5
Hence, we get the matrix:
Δ 5
cos φ
0
sin φ
sin φ
0
2cos φ
0
1
0
2
4
3
5
which has det Δ 5 1 6¼ 0 and rank 5 3 . 2. Thus, [g1,g2](x) is linearly independent of g1
and g2, and the distribution Δ 5 {g1,g2} is not involutive. Therefore, by Frobenius theo-
rem, the unicycle system is nonintegrable (nonholonomic). The same can be verified to
hold for all unicycle-like WMRs, the car-like WMRs, and the Brockett integrator (2.59),
_x1 5 u1; _x2 5 u2; _x3 5 x1u2 2 x2u1, which has the two fields:
g1 5 1
0
2x2 T;
g2 5 0
1
x1 T


Example 6.3
Let two linear vector fields fðxÞ 5 Bx and gðxÞ 5 Ax on X. Then ½f; gðxÞ is a linear vector
field given by ½f; gðxÞ 5 ðAB 2 BAÞx. For example, if:
A 5
1
0
0
21


and
B 5
0
1
1
0


are the matrices corresponding to f and g relative to a linear system of coordinates, then
the matrix C that corresponds to ½f; gðxÞ is:
C 5 2
0
1
21
0


The flows of these fields have the form shown in Figure 6.1.
6.2.2
Manifolds
A manifold is a topological space which is locally Euclidean, that is, around every
point, there is a neighborhood which is topologically the same as the open unit ball
in Rn.
193
Mobile Robot Control II: Affine Systems and Invariant Manifold Methods

Practically, any geometric object that can be charted (drawn, parameterized,
etc.) is a manifold. One of the aims of manifold theory is to find ways of distin-
guishing manifolds. For example, a circle is topologically the same as any closed
loop despite the differences possibly observed. In the same way, the surface of a
coffee mug with a handle is topologically the same as the surface of a ring, called
a one-handled torus. A manifold may be compact or noncompact in the topological
sense, connected or disconnected, with or without boundary. The closed ball in
Rn is a manifold with boundary the unit hypersphere.
Definition 6.7 (Differentiable manifold)
A differentiable manifold is a continuously and smoothly parameterizable geo-
metric object for which it is possible to establish a system under which every point
within the object can be labeled with a unique identifier (e.g., coordinates), and for
which the labels vary continuously and smoothly as one moves across the object.’
The word smooth means that the parameterization of a differentiable manifold has
at least as many continuous derivatives as required. Since differentiable manifolds
are continuously parameterizable, then in any specified smooth coordinate frame
(e.g., a state/phase space), it is possible to express the manifold locally (i.e., in some
neighborhood of any given point) as the graph of a function.
Formally, a differentiable manifold M is defined as:
M 5 fxARn: sðxÞ 5 0g
ð6:17Þ
where s: Rn ! Rm is a smooth map.
Definition 6.8 (Invariant set)
An invariant set Σi is defined any set of points (states) in a dynamic system
which are mapped into other points in the same set by the dynamic evolution
operator.’
(f(x))
[f,g](x)
(g(x))
Figure 6.1 Flows of the fields
fðxÞ;gðxÞ, and ½f;gðxÞ.
194
Introduction to Mobile Robot Control

Similarly, a trajectory is an invariant set because each point in the trajectory
evolves into another point in the same trajectory under the action of the evolution
operator.
Definition 6.9 (Invariant manifold)
An invariant manifold is an invariant set that happens to be a differentiable
manifold. More specifically, let s: Rn ! Rm be a smooth map. A manifold
M 5 fxARn: sðxÞ 5 0g is invariant for the dynamic system _x 5 fðx; uÞ if all system
trajectories starting in M at t 5 t0 remain in this manifold for all t $ t0.’
This implies that the Lie derivative of s along the vector field f is zero, that is:
LfsðxÞ 5 0
for all xAM
ð6:18Þ
It is remarked that a single equilibrium point is an invariant manifold (actually a
trivial, zero-dimensional, manifold). But a set of equilibrium points is not an invari-
ant manifold because it lacks continuity.
Definition 6.10 (Attractive manifold)
An invariant manifold M 5 fxARn: sðxÞ 5 0g is said to be an attractive manifold
in an open domain X of Rn, where X =2 M, if for all t0 $ 0 such that xðt0ÞAX, then
lim
t!N xðtÞAM which implies that any xðt0Þ outside M is always attracted toward M.’
A sufficient condition for MAR to be attractive is:
sðxÞ_sðxÞ , 0
for all xAX
ð6:19Þ
that is:
_sðxÞ , 0 for sðxÞ . 0;
xAX
_sðxÞ . 0 for sðxÞ , 0;
xAX
The condition (6.19) is derived using the Lyapunov function V 5 ð1=2Þs2ðxÞ and
requiring _VðxÞ 5 sðxÞ_sðxÞ , 0 in accordance with the Lyapunov theorem.
6.2.3
Lyapunov Stability Using Invariant Sets
The concepts of invariant set (Definition 6.8) and invariant manifold (Definition 6.9)
extend the concept of equilibrium point which is an invariant monoset. Using this con-
cept, we can find ways of determining (constructing) Lyapunov functions for nonlinear
systems. The Lyapunov functions V defined for invariant sets have the physical prop-
erty that their reduction rate should be gradually reduced (i.e., _V must be zero), since
V is bounded from below. This is expressed by the Barbalat lemma which states:
Barbalat Lemma If the Lyapunov function VðxÞ of Definition 5.6 has the addi-
tional property of _VðxÞ to be uniformly continuous (i.e., to have finite second deriv-
ative €VðxÞ), then _VðxÞ ! 0.’
195
Mobile Robot Control II: Affine Systems and Invariant Manifold Methods

The invariant set-based Lyapunov stability is based on the following two
theorems [2].
Theorem 6.2 (LaSalle local invariant sets theorem)
Let an autonomous system _x 5 fðxÞ with f continuous, and VðxÞ a scalar function
with continuous first derivatives. We assume that:
1. For some γ . 0, the region Ωγ which is defined by VðxÞ , γ is bounded
2.
_VðxÞ # 0 for all xAΩγ
Let G be the set of points inside Ωγ, for which _VðxÞ 5 0, and S be the larger
invariant set in GðSCGÞ. Then, every solution xðtÞ of the system that starts in Ωγ
tends to S for t ! N.’
Here, the term “larger” is used in the set theory sense, that is, S is the union of
all the invariant sets (e.g., equilibrium points or limit cycles). If the entire set G is
invariant, then S 5 G. The geometric interpretation of this theorem is shown in
Figure 6.2, where a trajectory that departs in a bounded Ωγ converges to the larger
invariant set S.
Theorem 6.3 (LaSalle global invariant sets theorem)
Consider the nonlinear autonomous system _x 5 fðxÞ, where f is a continuous func-
tion, and a nonlinear function VðxÞ with continuous first-order partial derivatives.
We assume that:
1.
_VðxÞ # 0 over the entire state space
2. VðxÞ ! N for jjxjj ! N
Let G be the set of points for which _V 5 0, and S be the larger invariant set
within G. Then, all solutions of the system possess total (global) asymptotic
convergence.’
This theorem shows, for example, that the convergence to a limit cycle is global,
and all trajectories of the system converge to the limit cycle. The construction of
Lyapunov functions for linear systems is made as described in Section 5.2.2
(Eqs. (5.14a), (5.14b), and (5.15)). A general method for constructing Lyapunov
V=l
V
Ω
G
S
x1
x0
x2
Figure 6.2 Geometric representation of the local
invariant sets theorem (convergence to the larger
invariant set S).
196
Introduction to Mobile Robot Control

functions for nonlinear systems was developed by Krasovskii and is formulated by
the following two theorems.
Theorem 6.4 (Krasovskii theorem)
Let the autonomous system _x 5 fðxÞ, where the equilibrium point of concern is
at the origin x 5 0. Let AðxÞ 5 @f=@x be the Jacobian matrix of f. If the matrix
F 5 A 1 AT is negative definite in a region Ω, then the equilibrium point x 5 0 is
asymptotically stable. A Lyapunov function for this system is:
VðxÞ 5 fTðxÞfðxÞ
ð6:20Þ
If Ω is the entire state space, and VðxÞ ! N for jjxjj ! N, then the equilibrium
point is totally asymptotically stable.’
Theorem 6.5 (Generalized Krasovskii theorem)
Consider the system _x 5 fðxÞ, where the equilibrium point of concern is the ori-
gin x 5 0. If AðxÞ is the Jacobian matrix of the system, then a sufficient condition
for x 5 0 to be asymptotically stable is the existence of symmetric positive definite
matrices P and Q such that, for every x 6¼ 0, the matrix:
FðxÞ 5 ATP 1 PA 1 Q
ð6:21Þ
is negative semidefinite in a region Ω of the origin. Then, the function VðxÞ 5 fTPf
is a Lyapunov function of the system. If the region Ω is the entire state space and
VðxÞ ! N for :x: ! N, then the system is globally asymptotically stable.’
Clearly, VðxÞ possesses the first three properties of Lyapunov functions. The
derivative _V is computed as:
_V 5 @V
@x fðxÞ 5 fTPAðxÞf 1 fTATðxÞPf
5 fTFf 2 fTQf
ð6:22Þ
Since F is negative semidefinite, and Q is positive definite, then _V is negative,
and the system is asymptotically stable. If VðxÞ ! N for jjxjj ! N, the global
asymptotic stability of the system follows from the conventional Lyapunov stability
theory (Section 5.2.2).
Example 6.4 (Application of Krasovskii theorem)
We will examine the stability of the nonlinear system:
_x1 5 26x1 1 2x2;
_x2 5 2x1 2 6x2 2 2x3
2
using Theorem 6.4.
197
Mobile Robot Control II: Affine Systems and Invariant Manifold Methods

Here, we have:
A 5 @f
@x 5
26
2
2
26 2 6x2
2


;
F 5 A 1 AT 5
212
4
4
212 2 12x2
2


It is easy to show that F is negative definite, and so the origin x 5 0 is asymptotically
stable, and a Lyapunov function is V 5 fTf, that is:
VðxÞ 5 ð26x112x2Þ2 1 ð2x126x222x3
2Þ2
Since VðxÞ ! N for jjxjj ! N, the equilibrium point x 5 0 is totally asymptotically
stable.
In many cases, it is difficult to check if the matrix F 5 A 1 AT is negative definite for
all x, and also, the Jacobian matrices of many systems do not satisfy the condition of the
theorem. In these cases, we must work with the generalized Krasovskii theorem and FðxÞ
given by Eq. (6.21).
The theorem of Brockett provides a necessary condition for the existence of a
stabilizing control law of a nonlinear system. This theorem is stated as follows [5]:
Theorem 6.6 (Brockett theorem)
Consider the nonlinear system:
_x 5 fðx; uÞ;
fðx0; 0Þ 5 0
where f is a continuously differentiable function in a neighborhood of ðx0; 0Þ.
Then, a necessary condition for the existence of a continuously differentiable
control law that makes ðx0; 0Þ asymptotically stable is that:
1. The linearized system _x 5 Ax 1 Bu; A 5 ð@f=@xÞx5x0; B 5 ð@f=@uÞu50 has no uncontrol-
lable modes associated with eigenvalues whose real part is positive.
2. There exists a neighborhood Ω of ðx0; 0Þ such that for each ξAΩ, there exists a control
uðξ; tÞ defined for t $ 0 such that this control steers the state response (solution) of the
system from x 5 ξ at t 5 0 to x 5 x0 at t 5 N.
3. The mapping Γ:ðx; uÞ ! fðx; uÞ is onto an open set containing 0.’
Corollary
(a) For an affine system with drift: _x 5 g0ðxÞ 1 Pm
i51 giðxÞui;
xðtÞAΩCRn the condition
(1) implies that the stabilization problem cannot have a solution if there is a smooth dis-
tribution Δ which contains g0(  ) and g1ðÞ; . . .; gmðÞ with dimΔ , n.
(b) If the affine system is driftless (i.e., g0ðxÞ 5 0), with the vectors giðxÞ being linearly
independent at x0, then there exists a solution to the stabilization problem only if m 5 n.
(c) There is no continuously differentiable stabilizing control law for
_x 5 u1; _y 5 u2;
_z 5 xu2 2 yu1 because this system satisfies only the conditions (1) and (2) of the theorem,
failing to satisfy condition (3).’
198
Introduction to Mobile Robot Control

Remark If A 5 ð@f=@xÞx5x0 has an eigenvalue with zero real part, then the equilib-
rium state x0 is said to be an asymptotically stable critical point. If there exists
a control law that makes x 5 0 an asymptotically stable equilibrium point for the
system _x 5 Ax 1 Bu, then there exists a control law which makes x0 an asymptoti-
cally stable critical point, provided that the algebraic system Ax0 1 Bu0 5 0
can be solved for u0. In fact, if u 5 Kx makes x 5 0 an asymptotically
stable
equilibrium
point,
then
u 5 Kx 1 u0
makes
x0
an
asymptotically
stable equilibrium point. Therefore, if x 5 0 can be made asymptotically stable,
then there is an entire subspace U 5 fx: AxArange Bg of points that can be made
asymptotically stable.
6.3
Feedback Linearization of Mobile Robots
6.3.1
General Issues
The methodology of linearization via state feedback consists in the algebraic
transformation of the nonlinear system dynamics to an equivalent linear form so
that linear control laws to be applicable. We distinguish two cases of feedback
linearization:
1. Input-state linearization: In this case, we seek to find a state transformation z 5 zðxÞ and
then an input/transformation u 5 uðx; υÞ, where υ is the new manipulable input. The pur-
pose of the above transformation is to bring the system _x 5 fðx; uÞ to the linear form
_z 5 Az 1 Bυ.
2. Inputoutput linearization: In this case, we have the system _x 5 fðx; uÞ with output
y 5 hðxÞ. The basic feature of this system is that the output y is connected to u only indi-
rectly through x. Therefore, to achieve inputoutput linearization, we must find a direct
relation between the input and the output of the system. This may be done by successive
differentiation of the output y 5 hðxÞ until all inputs appear in the resulting derivative
equations.
The nonholonomic mobile robots can be modeled by the affine system:
_x 5 fðxÞ 1 gðxÞu;
xARn
where x is the n-dimensional state and u the m-dimensional control input. For driftless
systems (which typically represent first-order kinematic models), we have fðxÞ 5 0,
and the state involves the robot generalized coordinates. The input-state linearization
of nonholonomic mobile robots is not/possible by means of smooth state feedback
because the fields fðxÞ and gðxÞ are not involutive as discussed in Example 6.2.
But these robots may be inputoutput linearizable (and decoupled). Here, we will
study the case of a differential-drive WMR with two outputs: the coordinates xQ and
yQ of the center point Q. In this case, the inputoutput linearization is not possible by
static feedback, but it can be achieved using dynamic feedback.
199
Mobile Robot Control II: Affine Systems and Invariant Manifold Methods

For single-input systems, we have the following results which are directly appli-
cable to many practical cases [14].
Theorem 6.7 (Feedback linearization conditions)
Let a single-input affine system be:
_x 5 fðxÞ 1 gðxÞu;
xARn
ð6:23aÞ
Then, there exists an output function y 5 hðxÞ for which the system has relative
degree n at the point x 5 x0, if and only if the following conditions are satisfied:
1. The matrix:
½gðx0Þ^adfgðx0Þ^?^adn22
f
gðx0Þ^adn21
f
gðx0Þ
ð6:23bÞ
has rank n.
2. The distribution:
Δ 5 fgðxÞ; adfgðxÞ; . . .; adn22
f
gðxÞg
ð6:23cÞ
is involutive in the vicinity of x0.
Theorem 6.8 (Controllable canonical form)
Defining new state variables as:
zi 5 φiðxÞ 5 Li21
f
hðxÞ;
i 5 1; 2; . . .; n
ð6:24aÞ
we can transform the system (6.23a) to the controllable canonical form:
_z1 5 z2
_z2 5 z3
^
_zn21 5 zn
_zn 5 bðzÞ 1 aðzÞu
ð6:24bÞ
where z 5 ½z1; z2; . . .; znΤ, and the function aðzÞ is nonzero in a vicinity of z0
(since z0
i 5 φiðx0Þ.’
As described in Definition 5.9, a system state x0 is called controllable if it is
possible to find a control uðtÞ that drives x0 to a desired final state xf. In this case,
xf is said to be reachable from x0. The system is said to be totally controllable if
all of its states are controllable. To understand better the controllability concept,
consider a driftless two-input system:
_x 5 g1ðxÞu1 1 g2ðxÞu2;
xAR3
200
Introduction to Mobile Robot Control

The question is to find which states can be reached from a given initial state x0.
If the input fields g1ðxÞ and g2ðxÞ are linearly independent, then the system can
move in two independent directions, namely, along g1(x) when u2 5 0 and
u1 5 61, or along g2(x) when u1 5 0 and u2 5 61. Of course, the system can move
in any direction which is a linear combination of the directions g1 and g2. The
question is whether the system can move in any direction independent of g1 and g2.
This can be done if g1ðx0Þ; g2ðx0Þ, and adg1g2ðx0Þ are linearly independent, where
adg1g2ðx0Þ 5 ½g1; g2ðx0Þ. The motion in the ½g1; g2ðx0Þ direction can be done by
appropriate switching between g1 and g2, namely:
uðtÞ 5
u1ðtÞ
u2ðtÞ


5
1
0


if
0 # t # Δt
0
1


if
Δt # t # 2Δt
21
0


if
2Δt # t # 3Δt
0
21


if
3Δt # t # 4Δt
8
>
>
>
>
>
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
>
>
>
>
>
:
Then, we get:
xð4ΔtÞ 5 x0 1 ð1=2Þ Δt
ð
Þ2 ½g1; g2ðx0Þ 1 0ðΔt3Þ
which is illustrated in Figure 6.3.
The above results can be extended to the general case, where there are m . 2
inputs, and we have the following reachability and controllability theorem.
Theorem 6.9 (ReachabilityControllability)
The system
_x 5 fðxÞ 1
X
m
i51
giðxÞui;
xARn
[g1,g2](x0)
x0
–g2
–g1
g1
g1
g2
x(4Δt)
x(3Δt)
x(Δt)
g2 x(2Δt)
Figure 6.3 Illustration of approximating the Lie bracket
½g1; g2ðx0Þ by successive switching between g1 and g2.
201
Mobile Robot Control II: Affine Systems and Invariant Manifold Methods

is locally reachable from x0 if the following reachability field distribution Qc spans
the n-dimensional space, where:
Qc 5 ½g1; g2; . . .; gm; ½gi; gj; . . .; adk
gigj; . . .; ½f;gi; . . .; adk
f gi; . . .
if it has rank n. If fðxÞ 5 0 and Qc has rank n, then the system is controllable.’
This theorem extends to nonlinear systems, the linear controllability condition
((5.18a) and (5.18b)). The gi terms correspond to the B terms, the terms ½gi; gj are
new coming from the nonlinearity of gi, and the ½f; gi terms correspond to the AB
terms, and so on. Denoting B 5 ½b1; b2; . . .; bm and writing the linear system
_xðtÞ 5 Ax 1 BuðtÞ as _x 5 Ax 1 Bu 5 Ax 1 b1u1 1 ::: 1 bmum, we have fðxÞ 5 Ax,
giðxÞ 5 bi, @f=@x 5 A; @g=@x 5 0, and so on. Then, the controllability matrix
((5.18a) and (5.18b)) takes the form:
Qc 5 ½b1; b2; . . .; bm; adfb1; . . .; adfbm; . . .; adn21
f
b1; . . .; adn21
f
bm
and so the controllability condition remains the same.
Example 6.5
Let us check the reachability and controllability of the unicycle (2.26) which has the
affine representation (see Eq. (2.32)):
_x 5 fðxÞ 1 g1u1 1 g2u2;
x 5 x
y
φ ΤAR3

where u1 5 vQ; u2 5 _φ, and:
fðxÞ 5 0;
g1 5
cos φ
sin φ
0
2
4
3
5;
g2 5
0
0
1
2
4
3
5
Here, we have:
Qc 5 ½g1; g2; ½g1; g2
where:
½g1; g2 5 @g2
@x g1 2 @g1
@x g2
5 2
0
0
2sin φ
0
0
cos φ
0
0
0
2
64
3
75
0
0
1
2
64
3
75 5
sin φ
2cos φ
0
2
64
3
75
202
Introduction to Mobile Robot Control

Therefore:
Qc 5
cos φ
0
sin φ
sin φ
0
2cos φ
0
1
0
2
4
3
5
Since det Qc 5 21, that is, rank Qc 5 3 everywhere, the unicycle is locally reachable from
everywhere, and since fðxÞ 5 0, the system is controllable.
Theorem 6.10 (Generalized controllable canonical form)
Consider the system:
_x 5 fðxÞ 1 gðxÞu;
y 5 hðxÞ
ð6:25aÞ
with relative degree r # n at x 5 x0. We set,
φ1ðxÞ 5 hðxÞ; φ2ðxÞ 5 LfhðxÞ; . . .; φrðxÞ 5 Lr21
f
hðxÞ:
If r is strictly less than n, then we can find n 2 r additional functions
φr11ðxÞ; . . .; φnðxÞ such that the mapping:
ΦðxÞ 5 ½φ1ðxÞ; φ2ðxÞ; . . .; φnðxÞ
ð6:25bÞ
has invertible Jacobian matrix at x 5 x0. It can therefore be used as local transfor-
mation in a region around x0. It is always possible to select the functions
φr11ðxÞ; . . .; φnðxÞ such that:
LgφiðxÞ 5 0
ð6:25cÞ
for all i with r 1 1 # i # n and all x in the vicinity of x0. Then, using the new state
variables zi 5 φiðxÞ, i 5 1; 2; . . .; n, the system can be written in the canonical form:
_z1 5 z2
_z2 5 z3
^
_zr21 5 zr
_zr 5 bðzÞ 1 aðzÞu;
aðz0Þ 5 aðΦðx0ÞÞ 6¼ 0
_zr11 5 cr11ðzÞ
^
_zn 5 cnðzÞ
y 5 z1 5 φ1ðxÞ
ð6:25dÞ
203
Mobile Robot Control II: Affine Systems and Invariant Manifold Methods

Proof To derive the canonical mode ((2.25c) and (2.25d)) we differentiate
ziði 5 1; 2; . . .; rÞ and get:
_z1 5
@φ1
@x
0
@
1
A_x 5 LfhðxðtÞÞ 5 φ2ðxðtÞÞ 5 z2ðtÞ
^
_zr21 5
@φr21
@x
0
@
1
A_x 5
@Lr22
f
hðxÞ
@x
0
@
1
A_x 5 Lr21
f
hðxÞ 5 φrðxÞ 5 zrðtÞ
_zr 5 Lr
fhðxÞ 1 LgLr21
f
hðxÞu
where xðtÞ is replaced by xðtÞ 5 Φ21ðzðtÞÞ to give:
_zr 5 bðzðtÞÞ 1 aðzðtÞÞu
where aðzÞ 5 LgLr21
f
hðΦ21ðzÞÞ; bðzÞ 5 Lr
fhðΦ21ðzÞÞ 6¼ 0:
For the remaining variables ziði 5 r 1 1; . . .; nÞ we may not have a special form,
but choosing them such that LgΦiðxÞ 5 0; i 5 r 1 1; . . .; n, we get:
_zi 5 ð@φi=@xÞ½fðxÞ 1 gðxÞu 5 LfφiðxÞ 1 LgφiðxÞu 5 LfφiðxÞ
Therefore:
_zi 5 LfðφiðΦ21ðzÞÞÞ 5 ciðzÞ;
i 5 r 1 1; . . .; n
The model (6.24b) is a special case of (6.25d) obtained when r 5 n.
Theorem 6.11 (Linearizing feedback law)
Selecting the following state feedback law:
u 5
1
aðzÞ 2b zð Þ 1 υ
½

ð6:26Þ
in the canonical model (6.24b), the following linear and controllable system is
obtained:
_z1 5 z2; _z2 5 z3; . . .; _zn21 5 zn; _zn 5 υ
ð6:27Þ
which is valid in the vicinity of z0 where aðzÞ 6¼ 0.’
204
Introduction to Mobile Robot Control

Theorem 6.7 provides the necessary and sufficient conditions under which
Theorems 6.8 and 6.10 are valid. Theorems 6.7 and 6.11 suggest the following: three-
step procedure for linearizing a system of the form (6.23a) namely:
Step 1: Find the output y 5 hðxÞ for which the system (6.23a) satisfies the conditions of
Theorem 6.7.
Step 2: Compute the state transformation:
z 5 ΦðxÞ 5
φ1ðxÞ
φ2ðxÞ
^
φnðxÞ
2
664
3
775 5
hðxÞ
LfhðxÞ
^
Ln21
f
hðxÞ
2
664
3
775
ð6:28Þ
for x in the vicinity of x0.
Step 3: Compute the local state feedback law:
u 5
1
aðΦðxÞÞ 2bðΦðxÞÞ 1 υ
½

5
1
LgLn21
f
hðxÞ 2Ln
f hðxÞ 1 υ


ð6:29Þ
One can see that Eq. (6.29) provides the expression of the state feedback linear-
izing controller in terms of the functions fðxÞ; gðxÞ, and hðxÞ of the original system:
_x 5 fðxÞ 1 gðxÞu;
y 5 hðxÞ;
xARn
ð6:30Þ
Now, using a new linear feedback controller:
υ 5 Kz;
K 5 ½k1; k2; . . .; kn
ð6:31aÞ
we can choose the gains kiði 5 1; 2; . . .; nÞ such that to achieve desired specification
as usual.
Introducing Eq. (6.28) into Eq. (6.31a) we get the overall controller:
υ 5 k1hðxÞ 1 k2LfhðxÞ 1 ? 1 knLn21
f
hðxÞ
ð6:31bÞ
which is actually a nonlinear state feedback controller for the system (6.30).
Remarks:
1. The steps 2 and 3 of the algorithm can be interchanged, if it is more convenient.
2. Although the algorithm was presented, for simplicity, for single-input systems, it is appli-
cable to mobile robots that have two or more inputs (of course with the proper selection
of the output, the diffeomorphic state transformation, and the state feedback controller).
3. If n 5 2 (second-order nonlinear system), then the conditions of Theorem 6.7 hold always
since ½g; gðxÞ 5 0.
205
Mobile Robot Control II: Affine Systems and Invariant Manifold Methods

4. If the system (6.25a) has relative degree r , n, but satisfies the conditions (1) and (2)
of Theorem 6.7, then there exists another output function cðxÞ for which the relative
degree of the system is n, and the theorem is applicable. However, the real output,
expressed in terms of the state variables z, that is y 5 hðΦ21ðzÞÞ remains nonlinear.
Therefore, we naturally need to determine whether there exists a transformation and a
feedback controller that can make the overall system (state and output) linear and con-
trollable. This can be done if some more conditions are satisfied as it is described in
the following theorem.
Theorem 6.12 (Generalized linearizing feedback law)
Suppose that the system:
_x 5 fðxÞ 1 gðxÞu;
y 5 hðxÞ;
xARn
has at the point x 5 x0 relative degree r , n. Then, there exists a static state feed-
back controller and a state transformation, defined in a vicinity of x0, such that the
system is transformed into linear and controllable form:
_x 5 Ax 1 Bu;
y 5 Cx
if and only if the following conditions are satisfied:
1. The matrix ½gðx0Þ^adfgðx0Þ^?^adn22
f
gðx0Þ^adn21
f
gðx0Þ has rank n.
2. The n-dimensional vector fields ~fðxÞ and ~gðxÞ that are defined by:
~fðxÞ 5 fðxÞ 2
Lr
fhðxÞ
LgLr21
f
hðxÞ ;
~gðxÞ 5
gðxÞ
LgLr21
f
hðxÞ
are such that:
½adi
f ~gðxÞ^ad j
~f ~gðxÞ 5 0
for all pairs ði; jÞ with i; j 5 1; 2; . . .; n.
Example 6.6
It is desired to check: (a) if the system:
_x 5 fðxÞ 1 gðxÞu;
y 5 x2;
fðxÞ 5
x3 2 x2
0
x3 1 x2
1
2
4
3
5;
gðxÞ 5
0
expðx1Þ
expðx1Þ
2
4
3
5
can be put in controllable form according to Theorem 6.8, and (b) if the entire system
(state and output) can be transformed to a linear and controllable form according to
Theorem 6.12.
206
Introduction to Mobile Robot Control

Solution
(a) This system has relative degree r 5 1 at all points x because LghðxÞ 5 expðx1Þ. We can
easily verify that conditions (1) and (2) of Theorem 6.7 (and 6.8) are satisfied. Thus,
there exists an output function hðxÞ for which the system has relative degree r 5 n 5 3.
The output function hðxÞ must satisfy the condition:
@h
@x


gðxÞ
adfgðxÞ
½
 5 0
The function hðxÞ 5 x1 satisfies this condition. Therefore, the state feedback control-
ler and the state transformation that give a linear controllable closed-loop system are:
u 5 2L3
fhðxÞ 1 υ
LgL2
fhðxÞ
5 2x1x2 2 2x1x3 2 x3 2 x2
1 1 υ
expðx1Þ
z1 5 hðxÞ 5 x1;
z2 5 LfhðxÞ 5 x3 2 x2;
z3 5 L2
fhðxÞ 5 x3 1 x2
1
(b) The original output y 5 x2 expressed in terms of the new state variables z1; z2, and z3
is y 5 2z2 1 z3 2 z2
1. To check whether the entire (state and output) system can be
transformed to linear controllable form, we must check if condition (2) of theorem is
satisfied for the functions ~fðxÞ and ~gðxÞ. Because here LfhðxÞ 5 0, we have:
~fðxÞ 5 fðxÞ and ~gðxÞ 5
0
1
1
2
4
3
5
Now, we easily find that:
ad~f ~g 5
0
0
21
2
4
3
5;
ad2
~f ~g 5
1
0
1
2
4
3
5;
ad3
~f ~g 5
21
0
22x1 2 1
2
4
3
5
Thus,
ad2
~f ~g
ad3
~f ~g 6¼ 0
h
that is, the condition (2) of Theorem 6.11 is not satisfied. Therefore, the entire
system (with its output) cannot be put in linear controllable form.
6.3.2
Differential-Drive Robot InputOutput Feedback Linearization
and Trajectory Tracking
6.3.2.1
Kinematic Constraint Revisited
We consider the differential-drive WMR of Figure 6.4, where the ðxQ; yQÞ coordi-
nates of the wheel axis midpoint Q are denoted by x and y, and work following the
207
Mobile Robot Control II: Affine Systems and Invariant Manifold Methods

path of Yun and Yamamoto [6]. An analogous derivation for car-like robots is pro-
vided in Ref. [7].
The generalized coordinates of the WMR are:
x 5 ½x; y; φ; θr; θlΤ
The robot has the three constraints defined by Eqs. (2.39) and (2.41a) which can
be written as:
MðxÞ_x 5 0
ð6:32aÞ
with:
MðxÞ 5
2sin φ
cos φ
0
0
0
2cos φ
2sin φ
2a
r
0
2cos φ
2sin φ
a
0
r
2
4
3
5
ð6:32bÞ
We define, the 5 3 2 dimensional matrix BðxÞ:
BðxÞ 5 ½b1ðxÞ^b2ðxÞ 5
ρa cos φ
ρa cos φ
ρa sin φ
ρa sin φ
ρ
2ρ
1
0
0
1
2
66664
3
77775
ð6:33Þ
(where ρ 5 r=2a) which has two independent columns and the property:
MðxÞBðxÞ 5 0
ð6:34Þ
Consider the field distribution ΔðxÞ formed by the two columns of BðxÞ. From
Frobenius theorem we know that if ΔðxÞ is involutive, all the constraints are inte-
grable (holonomic). If the smallest involutive distribution that involves ΔðxÞ,
C(xc,yc)
(x,y)
L
a
r
Q
V
x
y
0
φ
Figure 6.4 The differential-drive robot.
208
Introduction to Mobile Robot Control

denoted by Δ (see Eq. (6.14)) spans1 the entire 5-dimensional space (where 5 is
the dimensionality of x), then all the constraints are holonomic. If dim Δ
 5 5 2 s,
then s constraints are holonomic and the remaining nonholonomic.
To determine the involutivity of ΔðxÞ, we find the Lie bracket of b1ðxÞ
and b2ðxÞ:
b3ðxÞ 5 b1
b2


ðxÞ 5 @b2
@x b1 2 @b1
@x b2 5
2rρ sin φ
rρ cos φ
0
0
0
2
66664
3
77775
which is linearly independent of b1ðxÞ and b2ðxÞ. Thus, one of the constraints is
surely nonholonomic. We continue by computing the Lie bracket of b1ðxÞ and
b3ðxÞ, that is:
b4ðxÞ 5 b1
b 3


ðxÞ 5 @b3
@x b1 2 @b1
@x b3 5
2rρ2 cos φ
2rρ2 cos φ
0
0
0
2
66664
3
77775
which is linearly independent of b1ðxÞ; b2ðxÞ, and b3ðxÞ. Here, the distribution
spanned by b1ðxÞ; b2ðxÞ; b3ðxÞ, and b4ðxÞ is involutive, that is:
Δ 5 span½b1ðxÞ; b2ðxÞ; b3ðxÞ; b4ðxÞ
Therefore, we conclude that two of the three constraints in Eqs. (6.32a) and
(6.32b) are nonholonomic. To find the holonomic constraint, we subtract the sec-
ond Eq. (2.41a) from the first Eq. (2.41a), that is, the third line of Eq. (6.32a) with
M given by Eq.(6.32b) from the second line, and get:
2a_φ 5 rð_θ r 2 _θ lÞ
which can be integrated to give:
φ 5 ρðθr 2 θlÞ;
ρ 5 r=2a
ð6:35Þ
1 If every vector vAV (where V is a vector space) can be written as a linear combination of the vectors
of some set, then we say that this set spans the vector space V (as in Eq. (6.16b)).
209
Mobile Robot Control II: Affine Systems and Invariant Manifold Methods

This is obviously an holonomic constraint. We can therefore eliminate φ from
the generalized coordinate vector, and obtain a four-dimensional vector:
x 5
x1
x2
x3
x4
2
664
3
775 5
x
y
θr
θl
2
664
3
775
ð6:36Þ
It follows that the two nonholonomic constraints are:
_x sin φ 2 _y cos φ 5 0
_x cos φ 1 _y sin φ 5 ρað_θ r 1 _θ lÞ
ð6:37aÞ
The above two constraints are written in matrix form as:
MðxÞ_x 5 0;
MðxÞ 5
2sin φ
cos φ
0
0
2cos φ 2sin φ
ρa
ρa


ð6:37bÞ
6.3.2.2
InputOutput Linearization
Since the WMR has two inputs, it is natural to select an output equation with its
two independent components, the coordinates of the point Q, that is:
y 5
y1
y2


5 hðxÞ 5
x
y
 
ð6:38Þ
The Lagrangian of this WMR which is equal to the kinetic energy (due to the
absence of gravitational effects) is given by:
L 5 K 5 1
2 mð_x2
1 1 _x2
2Þ 1 mpρbð_θ r 2 _θ lÞð_x2 cos φ 2 _x1 sin φÞ
1 1
2 Iwð_θ
2
r 1 _θ
2
l Þ 1 1
2 Iρ2ð_θ r2 _θ lÞ2
ð6:39Þ
where b is the displacement of Q from the center of gravity, and:
m 5 mp 1 2mw;
I 5 Ip 1 2mwa2 1 2Im
Here, m is the total mass of the robot, mp the mass of the platform, mw the mass of
each wheel, Ip the moment of inertia of the robot without the driving wheels and
the motors’ rotors, and Im the moment of inertia of each driving wheel and its rotor
210
Introduction to Mobile Robot Control

about a wheel diameter. Working as usual, using Eqs. (3.14) and (3.15), we get the
dynamic model (3.16) of the robot:
DðqÞ€q 1 Cðq;_qÞ_q 1 MΤðqÞλ 5 Eτ
ð6:40Þ
where:
DðqÞ 5
m
0
2mpρb sin φ
mpρb sin φ
0
m
mpρb cos φ
2mpρb cos φ
2mpρb sin φ
mpρb cos φ
Iρ2 1 Iw
2Iρ2
mpρb sin φ
2mpρb cos φ
2Iρ2
Iρ2 1 Iw
2
664
3
775
Cðq;_qÞ_q 5
2mpb_φ
2 cos φ
2mpb_φ
2 sin φ
0
0
2
6664
3
7775;
E 5
0
0
0
0
1
0
0
1
2
664
3
775;
τ 5
τr
τl


;
λ 5
λ1
λ2


The nonholonomic constraint matrix MðqÞ is given by Eq. (6.37b) with q in
place of x. Eliminating the constraint MΤðqÞλ from Eq. (6.40), as usual, we get the
unconstrained Lagrange model ((3.19a) and (3.19b)):
DðqÞ_v 1 Cðq; _qÞv 5 Eτ;
_q 5 BðqÞv
ð6:41aÞ
where D 5 BΤDB;
C 5 BΤD _B 1 BΤCB, and E 5 BΤE 5 I2 3 2; and:
BðqÞ 5
ρa cos φ
ρa cos φ
ρa sin φ
ρa sin φ
1
0
0
1
2
664
3
775
ð6:41bÞ
Choosing the following state vector:
x 5 ½x1; x2; x3; x4; x5; x6Τ
5 ½x; y; θr; θl^v1; v2Τ
5 ½qΤ^vΤΤ
ð6:42Þ
the model (6.41a) can be written in the following affine state-space form:
_x 5 fðxÞ 1 gðxÞuðtÞ;
u 5 τ
ð6:43aÞ
where:
fðxÞ 5
Bv
2D21Cv


;
gðxÞ 5
0
D21E


5
0
D21


ð6:43bÞ
211
Mobile Robot Control II: Affine Systems and Invariant Manifold Methods

We will now examine the inputoutput linearization of Eqs. (6.43a) and (6.43b)
with output (6.38) using the state feedback control law:
uðtÞ 5 FðxÞ 1 GðxÞυðtÞ
ð6:44aÞ
where υðtÞ is the new control variable, and:
FðxÞ 5 E21Cv 5 Cv;
GðxÞ 5 E21D 5 D
ð6:44bÞ
according to the static computed torque method.
To this end, we introduce Eqs. (6.44a) and (6.44b) into Eqs. (6.43a) and (6.43b)
and get the closed-loop system:
_xðtÞ 5 fcðxÞ 1 gcðxÞυðtÞ
ð6:45aÞ
where:
fcðxÞ 5 fðxÞ 1 gðxÞFðxÞ 5
Bv
^
0
2
4
3
5
ð6:45bÞ
gcðxÞ 5 gðxÞGðxÞ 5
0
^
I232
2
4
3
5
ð6:45cÞ
To see if the control law ((6.44a) and (6.44b)) inputoutput linearizes the sys-
tem ((6.45a)(6.45c) and (6.38)), we differentiate the output yðtÞ, and obtain:
_y 5
 @h
@x

_x 5
 @h
@x

fcðxÞ 1 gcðxÞυ


5 B1ðxÞv
where:
@h
@x 5 I232^0
½
;
B1ðxÞ 5
ρa cos φ
ρa cos φ
ρa sin φ
ρa sin φ


ð6:46Þ
We see that _y does not involve the input υ, and so we compute the second deriv-
ative €y, namely:
€y 5 B1ðxÞ_v 1 _B1ðxÞv
5 B1ðxÞυ 1 _B1ðxÞv
ð6:47aÞ
212
Introduction to Mobile Robot Control

where:
_B1ðxÞv 5 ρ2aðv2
1 2 v2
2Þ 2sin φ
cos φ


ð6:47bÞ
We see that now the input υ appears explicitly in €y, and so B1ðxÞ is the decou-
pling matrix. However, B1ðxÞ is not invertible, which means that the system cannot
be decoupled by a static state feedback of the form ((6.44a) and (6.44b)). Actually,
no static state feedback exists that inputoutput linearizes the differential-drive
WMR when the outputs are the coordinates of the point Q.
However, as it is given below, the system can be inputoutput linearized by
dynamic state feedback law of the form [6]:
_z 5 f1ðx; zÞ 1 g1ðx; zÞw
ð6:48aÞ
υ 5 Fðx; zÞ 1 Gðx; zÞw
ð6:48bÞ
To this end, we start by using the static decoupling law ((6.44a) and (6.44b)) to lin-
earize and decouple one of the outputs, which can be done since rank B1ðxÞ 5 1 for
all x. Choosing to linearize the output y1, and introducing the following feedback
law into Eq. (6.47a):
υ 5 F1ðxÞ 1 G1ðxÞw;
w 5 ½w1; w2Τ
ð6:49aÞ
where:
F1ðxÞ 5
ρðv2
1 2 v2
2Þtgφ
0


;
G1ðxÞ 5
1=ρa cos φ
1
0
21


ð6:49bÞ
we get:
€y 5 A2ðxÞ 1 B2ðxÞ w1
w2


where:
A2ðxÞ 5
0
ρ2aðv2
1 2 v2
2Þ=cos φ


;
B2ðxÞ 5
1
0
tg φ
0


We see that:
€y1 5 w1
that is, y1 is linearized and decoupled, controlled only by w1. But y2 is still nonlin-
ear and controlled by w1.
213
Mobile Robot Control II: Affine Systems and Invariant Manifold Methods

Then, we continue by introducing the static feedback law ((6.49a) and (6.49b))
into Eqs. (6.45a)(6.45c), which yields the new closed-loop state-space equation:
_x 5 fcðxÞ 1 gcðxÞυ
5 fcðxÞ 1 gcðxÞ½F1ðxÞ 1 G1ðxÞw
5 f1
cðxÞ 1 g1
cðxÞw
ð6:50aÞ
where:
f1
cðxÞ 5
Bv
ρðv2
1 2 v2
2Þtg φ
0
2
4
3
5;
g1
cðxÞ 5
0
0
1=ρa cos φ
1
0
21
2
4
3
5
ð6:50bÞ
Now, differentiating y2 with the system ((6.50a) and (6.50b)), and considering w1
as a time varying parameter, we get:
_y2 5 ρaðv1 1 v2Þsin φ
€y2 5 ρ2aðv2
1 2 v2
2Þ=cos φ 1 ðtg φÞw1
y⃛
2 5 ρ3aðv2
1 2 v2
2Þðv1 2 v2Þðsin φÞ=cos2 φ 1 ½ρðv1 2 v2Þ=cos2 φw1
1 ð2ρ2av1=cos φÞ½ρðv2
1 2 v2
2Þtg φ 1 ð1=ρa cos φÞw1
1 ðtg φÞ _w1 1 ½2ρ2aðv1 1 v2Þ=cos φw2
We see that now w2 appears explicitly in ⃛y2, which can be written as:
y⃛2 5 P1ðxÞ 1 P2ðxÞw1 1 P3ðxÞ _w1 1 P4ðxÞw2
ð6:51Þ
where PiðxÞ have obvious definition. Finally, in view of Eq. (6.51), y2 can be line-
arized by using the feedback law:
w2 5 P21
4 ðxÞðυ
2 2 P1ðxÞ 2 P2ðxÞw1 2 P3ðxÞ _w1Þ
ð6:52aÞ
The derivative _w1 appearing in Eq. (6.52a) can be eliminated by using an inte-
grator to the first input channel, that is:
_z 5 υ
1;
w1 5 z
ð6:52bÞ
From the above, it follows that the dynamic state feedback law has the form:
_z 5 f2ðx; zÞ 1 g2ðx; zÞυ
w 5
w1
w2


5 F2ðx; zÞ 1 G2ðx; zÞυ;
υ 5 ½υ
1; υ
2T
ð6:53Þ
214
Introduction to Mobile Robot Control

where:
f2ðx; zÞ 5 0;
g2ðx; zÞ 5 1; 0
½

F2ðx; zÞ 5
z
2P21
4 ðxÞ½P1ðxÞ 1 P2ðxÞz


;
G2ðx; zÞ 5
0
0
2P21
4 ðxÞP3ðxÞ
P21
4 ðxÞ


ð6:54Þ
Using this dynamic feedback law, we get the overall linearized and decoupled
system:
y⃛1 5 υ
1;
y⃛2 5 υ
2
ð6:55Þ
which is of third order.
6.3.2.3
Trajectory Tracking Control
Having available the model (6.55):
y⃛1 5 w1;
y⃛2 5 w2
and the desired trajectory½y1dðtÞ; y2dðtÞΤ, we select as usual the linear controllers:
w1 5 y⃛1d 1 K21ð€y1d 2 €y1Þ 1 K11ð_y1d 2 _y1Þ 1 K01ðy1d 2 y1Þ
ð6:56aÞ
w2 5 y⃛2d 1 K22ð€y2d 2 €y2Þ 1 K12ð_y2d 2 _y2Þ 1 K02ðy2d 2 y2Þ
ð6:56bÞ
The error dynamics for y1 and y2 are:
~y⃛1 1 K21€~y1 1 K11_~y1 1 K01 ~y1 5 0
~y⃛2 1 K22€~y2 1 K12_~y2 1 K02 ~y2 5 0
The corresponding characteristic polynomials are:
χiðλÞ 5 λ3
i 1 K2iλ2
i 1 K1iλi 1 K0i ði 5 1; 2Þ
ð6:57Þ
Therefore, selecting appropriate values for the position, velocity, and accelera-
tion gains K0i; K1i, and K2i ði 5 1; 2Þ we assure, as usual, convergence to the desired
trajectory, with a desired rate of convergence, from any initial state. Especially, if
the initial point belongs to the desired trajectory, that is:
~yið0Þ 5 yidð0Þ 2 yið0Þ 5 0;
_~yið0Þ 5 _yidð0Þ 2 _yið0Þ 5 0;
€~yið0Þ 5 €yidð0Þ 2 €yið0Þ 5 0
the state will always remain on this trajectory.
215
Mobile Robot Control II: Affine Systems and Invariant Manifold Methods

Example 6.7
It is desired to derive a kinematic feedback inputoutput linearizing controller for the
car-like WMR (2.52):
_x
_y
_φ
_ψ
2
6664
3
7775 5
cos φ
sin φ
ðtg ψÞ=D
0
2
6664
3
7775v1 1
0
0
0
1
2
6664
3
7775v2
ð6:58Þ
employing the (2,4) chained model (2.69):
_x1 5 u1
_x2 5 u2
_x3 5 x2u1
_x4 5 x3u1
ð6:59Þ
Solution
We select as output vector y the following [8]:
yðtÞ 5 hðxÞ 5
xðtÞ
yðtÞ


5
y1ðtÞ
y2ðtÞ


ð6:60Þ
Knowledge of this output determines fully the entire trajectory ½xðtÞ; yðtÞ; φðtÞ; ψðtÞT,
and the corresponding input ½v1ðtÞ; v2ðtÞT. Indeed, from the first two rows of Eq. (6.58)
we get the first input:
v1ðtÞ 5 6
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
_x2ðtÞ 1 _y2ðtÞ
q
ð6:61Þ
where the sign ‘‘1’’ corresponds to the forward motion, and the sign ‘‘2’’ to the back
motion. Division of y by x gives:
φðtÞ 5 tg21½yðtÞ=xðtÞ
ð6:62Þ
Differentiating the first two rows of Eq. (6.58) gives:
_φðtÞ 5 €yðtÞ_xðtÞ 2 €xðtÞ_yðtÞ
½
=v2
1ðtÞ
ð6:63Þ
Introducing Eq. (6.63) into the third row of Eq. (6.58) we obtain:
ψðtÞ 5 tg21D½€yðtÞ_xðtÞ 2 €xðtÞ_yðtÞ=v3
1ðtÞ
ð6:64Þ
which is valid in the interval ð2π=2; π=2Þ. Finally, the second input v2ðtÞ can be obtained
by introducing the derivative of Eq. (6.64) into _ψðtÞ 5 v2ðtÞ, namely:
v2ðtÞ 5 vnumðtÞ=½v6
1 1 D2ð€y_x 2 €x_yÞ2
ð6:65Þ
216
Introduction to Mobile Robot Control

where:
vnumðtÞ 5 Dv1fðy⃛_x 2 x⃛_yÞv2
1 2 3ð€y_x 2 €x_yÞð_x€x 1 _y€yÞg
Equations (6.61)(6.65) determine all the state and input variables in terms of the
output variables xðtÞ and yðtÞ. This means that if we are given desired outputs xdðtÞ and
ydðtÞ, they specify via Eqs. (6.61)(6.65) the desired trajectories for all the other state
variables of the car-like WMR.
On the basis of the above, we can use the following inputoutput model for the car-
like robot:
_y 5 H1ðφÞv;
y 5
x
y
 
;
H1ðφÞ 5
cos φ
0
sin φ
0


;
v 5
v1
v2


ð6:66Þ
We see that at least one of the inputs v1 and v2 appears in _y1 and _y2. Therefore, H1ðφÞ
is the decoupling matrix, which however is not invertible and cannot be diagonalized by
any static similarity transformation. To overcome this difficulty, we must use a dynamic
controller:
v 5 Fðy; zÞ 1 Gðy; zÞw
_z 5 f1ðy; zÞ 1 g1ðy; zÞw
ð6:67Þ
where zðtÞ is the dynamic controller state and wðtÞ is the new input.
The controller Eq. (6.67) will be applied to the chain representation Eq. (6.59) of the
WMR, where (see Eq. (2.67)):
x1 5 x;
x2 5 ðtg ψÞ=D cos3 φ;
x3 5 tg φ;
x4 5 y
and so (due to (2.68)):
y 5
x
y
 
5
x1
x4


;
_y 5
_x1
_x4


5
1
0
x3
0


u1
u2


ð6:68Þ
In Eq. (6.68), u2 does not appear, and the decoupling matrix:
H2 5
1
0
x3
0


is again noninvertible. Thus, a dynamic feedback controller of the form (6.67) will be
used. We start by adding an integrator with state z1 to the first input u1, that is:
_z1 5 u
1;
u1 5 z1
ð6:69Þ
where u
1 is an auxiliary control input. Now, the model (6.68) becomes:
_y 5
z1
x3z1


ð6:70Þ
217
Mobile Robot Control II: Affine Systems and Invariant Manifold Methods

Differentiating Eq. (6.70) we get, from the third line of Eq. (6.59):
y 5
_z1
_x3z1 1 x3_z1
"
#
5
0
_x3z1
"
#
1
1
0
x3
0
"
# u
1
u2
"
#
5
0
x2z2
1
"
#
1
1
0
x3
0
"
# u
1
u2
"
#
ð6:71Þ
Again, u2 does not appear in €y, and so we add one more integrator:
_z2 5 u
1 ;
u
1 5 z2
ð6:72Þ
to obtain:
y⃛5 H0 1 H3 u
1
u2


;
H0 5
0
3x2z1z2


;
H3 5
1
0
x3
z2
1


ð6:73Þ
Now, the decoupling matrix H3 is invertible for z1 6¼ 0, and so selecting the feedback
control law:
u
1
u2


5 H21
3
w1
w2


2 H0
	

5
w1
ðw2 2 x3w1 2 3x2z1z2Þ=z2
1


ð6:74Þ
we get the linear and fully decoupled system:
y⃛1 5 w1;
y⃛2 5 w2
ð6:75Þ
as required. This is of the same form as the linearized-decoupled model of the differential-
drive robot.
Example 6.8
Show that by using output as y the coordinates of a point Cðxc; ycÞ in front of the car-like
robot that lies on its steering line (at an angle φ 1 ψ in the Oxy coordinate frame), it is
possible to decouple the inputs and outputs using a static feedback linearizing controller.
Solution
Consider the bicycle model of the car-like robot shown in Figure 6.5, where L 6¼ 0.
From Figure 6.5 we see that [8]:
y 5
y1
y2


5
xc
yc


5
x 1 D cos φ 1 L cosðφ 1 ψÞ
y 1 D sin φ 1 L sinðφ 1 ψÞ


ðL 6¼ 0Þ
218
Introduction to Mobile Robot Control

Differentiating y we get:
_y 5 Hðφ; ψÞv;
v 5 ½v1; v2Τ
where:
Hðφ; ψÞ 5
cos φ 2 ½sin φ 1 L sinðφ 1 ψÞ=Dtg ψ
2L sinðφ 1 ψÞ
sin φ 1 cos φ 1 L cosðφ 1 ψÞ=D


tg ψ
L cosðφ 1 ψÞ


Here det Hðφ; ψÞ 5 L=cos ψ 6¼ 0 for 0 # ψ , 90, and so the system can be linearized
and decoupled by a state feedback law:
v 5 Fðφ; ψÞυ
where Fðφ; ψÞ 5 H21ðφ; ψÞ. The resulting closed-loop system is:
_y1 5 υ1
_y2 5 υ2
_φ 5 ð1=DÞ½cosðφ 1 ψÞυ1 1 sinðφ 1 ψÞυ2sin ψ
_ψ 5 2½ð1=DÞcosðφ 1 ψÞsin ψ 1 ð1=LÞsinðφ 1 ψÞυ1
2½ð1=DÞsinðφ 1 ψÞsin ψ 2 ð1=LÞcosðφ 1 ψÞυ2
Obviously, this system is a first-order inputoutput linearized and decoupled system,
but not input to state-decoupled. This means that the outputs y1ðtÞ and y2ðtÞ can track
any desired trajectories y1dðtÞ and y2dðtÞ by a standard linear state feedback law:
υi 5 _ydi 1 kpiðydi 2 yiÞ;
kpi . 0
ði 5 1; 2Þ
as usual, but the trajectories of φðtÞ and ψðtÞ cannot follow specified (desired) trajecto-
ries. This problem can be alleviated by applying the Lyapunov-based control technique to
the closed-loop system as a whole.
Remark 1 Another selection of the output vector y 5 ½y1; y2Τ which can linearize
and decouple the car-like robot is to use the first two variables x1 5 x
and x2 5 ðtg ψ=D cos3 φÞ of the (2,4) chained model (6.59) studied in Example 6.7
(see Eq. (2.67)).
C(xc,yc)
L
D
y
x
0
ψ
φ
Figure 6.5 Using the coordinates xc; yc of the point C as
output components y1 and y2.
219
Mobile Robot Control II: Affine Systems and Invariant Manifold Methods

Remark 2 Similar results can be obtained for the differential-drive WMR of
Figure 6.4 if we use as outputs the coordinates xc; yc of the point C in front of the
vehicle, that is, y 5 xc; yc
½
Τ.
6.4
Mobile Robot Feedback Stabilizing Control Using
Invariant Manifolds
The mobile robot stabilization approach which is based on invariant manifolds
leads to nonlinear controllers directly without prior feedback linearization.
Its mathematical power and beauty have been applied by many scientists, and
this approach is further enhanced in many ways. The two general model catego-
ries used for mobile robots and other nonholonomic systems are the nonholo-
nomic (Brockett) integrators (simple, double, extended) and the (2-n)-chain
models. This approach treats in an elegant way the nonholonomic constraints,
and an extended literature exists with a large repertory of different controllers
(that are valid under different conditions) [5,915]. On the theoretical side,
the key result is Brockett’s stabilization condition (theorem), which establishes
the fact that nonholonomic systems cannot be asymptotically stabilized to a
single equilibrium state using a smooth (or even continuous) static feedback
controller (see Theorem 6.6).
6.4.1
Stabilizing Control of Unicycle in Chained Model Form
We
have
seen
in
Section
2.3.5.1
that
the
unicycle
kinematic
model
_x 5 vQ cos φ; _y 5 vQ sin φ; _φ 5 vφ can be transformed to the (2,3) chain form
(2.63):
_z1 5 u1
_z2 5 u2
_z3 5 z2u1
ð6:76Þ
where u1 5 vφ and u2 5 vQ 2 z3u1. The problem to be considered here is to deter-
mine a static quasi-continuous state feedback control law u 5 uðzÞ which asymptot-
ically stabilizes the system (6.76) to the origin [10].
One can directly verify that the control law:
u 5 ½2k1z1; 2k1z2Τ;
u 5 ½u1; u2Τ;
k1 . 0
ð6:77Þ
makes the origin z 5 ½z1; z2; z3Τ 5 0
0
0 Τ

globally asymptotically stable. The
resulting closed-loop system is:
_z 5 fðzÞ;
fðzÞ 5 ½2k1z1; 2k1z2; 2k1z1z2Τ
ð6:78Þ
220
Introduction to Mobile Robot Control

We can easily verify that the manifold:
M 5 fzAR3:sðzÞ 5 z1z2 2 2z3 5 0g
ð6:79Þ
is an invariant manifold of the system (6.78). Indeed, we get (see Definition 6.9):
LfsðzÞ 5
X
3
i51
@s
@zi
fiðz1; z2; z3Þ
5 z2ð2k1z1Þ 1 z1ð2 k1z2Þ 1 ð22Þð2k1z1z2Þ
5 0
The time derivative of sðzÞ along the trajectories of (6.78) is found to be:
_sðzÞ 5 z1u2 2 z2u1 5 0
which means that the trajectories once on the surface (manifold) Mremain there.
Furthermore, since z1ðtÞ ! 0 and z2ðtÞ ! 0, as t ! N, for any trajectory on M we
have z3ðtÞ ! 0 as t ! N, and so:
½z1ðtÞ; z2ðtÞ; z3ðtÞΤ !
0
0
0 Τ as t ! N

One can observe that M does not depend on k1.
We will now construct a stabilizing control law, which makes M an attractive
manifold. To this end, the feedback controller (6.77) must be enhanced such that to
satisfy the attractivity condition(6.19), namely:
If sðzÞ , 0; then _sðzÞ . 0
zAR3
If sðzÞ . 0; then _sðzÞ , 0
zAR3
ð6:80Þ
A possible enhancement of the controller (6.77) is the following:
u 5
2k1z1 2 z2HðsÞ
z2
1 1 z2
2
2k1z2 1 z1HðsÞ
z2
1 1 z2
2
2
6664
3
7775
for z2
1 1 z2
2 6¼ 0
ð6:81aÞ
where the scalar mapping HðsÞ satisfies the condition:
sHðsÞ , 0
ð6:81bÞ
to assure the satisfaction of Eq. (6.80). A function HðsÞ with this property is:
HðsÞ 5 2k2s
ð6:82Þ
221
Mobile Robot Control II: Affine Systems and Invariant Manifold Methods

The closed-loop system obtained using the controller ((6.81a), (6.81b), and
(6.82)) is:
_z1
_z2
_s
2
64
3
75 5 f
z1
z2
s
2
64
3
75
0
B
@
1
C
A;
f 5
2k1z1 1 k2z2s
z2
1 1 z2
2
2k1z2 2 k2z1s
z2
1 1 z2
2
2k2sðzÞ
2
6666664
3
7777775
ð6:83Þ
where sðzÞ is given by Eq. (6.79). It is remarked that the transformation:
z ! ~z;
z 5
z1
z2
z3
2
4
3
5;
~z 5
z1
z2
s
2
4
3
5
ð6:84Þ
is a diffeomorphism.
To summarize, with the controller ((6.81a), (6.81b), and (6.82)) and k2 . 2k1,
the trajectory ~zðtÞ 5 ½z1ðtÞ; z2ðtÞ; sðtÞΤ is bounded for all t $ 0, and converges expo-
nentially to ½0; 0; 0Τ with a decay rate greater than or equal to k1. Likewise, the
control uðtÞ is bounded for all t $ 0 and tends exponentially to ½0; 0Τ with a decay
rate at least k1. In other words, for any initial condition with z1ð0Þ2 1 z2
2ð0Þ 6¼ 0, the
feedback control law ((6.81a), (6.81b), and (6.82)) is well defined for all t $ 0 and
drives the unicycle to the origin, while avoiding the manifold:
M 5 fzAR3:
z2
1 1 z2
2 5 0;
z1z2 2 2z3 6¼ 0g
6.4.2
Dynamic Control of Differential-Drive Robots Modeled by the
Double Brockett Integrator
The nonlinear controller designed in Section 6.4.1 is a stabilizing controller for the
kinematic performance of the unicycle. Here, we will derive a quasi-continuous
dynamic controller, using invariant manifolds, for the combined kinematic and
dynamic performance of a differential-drive WMR using the double Brockett inte-
grator model (2.60) [15]. The full dynamic model of the WMR is (see Eqs. (3.22),
(3.23a), (3.23b), and (3.31)):
_x 5 v cos φ
ð6:85aÞ
_y 5 v sin φ
ð6:85bÞ
_φ 5 ω
ð6:85cÞ
m_v 5 F
ð6:85dÞ
I _ω 5 N
ð6:85eÞ
222
Introduction to Mobile Robot Control

where the symbols have the standard meaning, and the index Q was deleted from
xQ; yQ for notational convenience.
The chained form of the kinematic equations (6.85a)(6.85c) is given by
Eq. (2.63) (see also Eq. (6.76)). Defining new variables:
x1 5 z1;
x2 5 z2;
x3 5 22z3 1 z1z2
ð6:86Þ
the chained model is transformed to the Brockett (nonholonomic) integrator model
(2.65):
_x1 5 u1;
_x2 5 u2;
_x3 5 x1u2 2 x2u1
ð6:87Þ
In the previous section, we have derived a stabilizing kinematic controller for
Eqs. (6.85a)(6.85c) using the chained form. A similar controller can also be
derived using the Brockett integrator model (6.87). This controller should make the
manifold:
M 5 fxAR3:x3 5 0g
an invariant manifold. Working as in Section 6.4.1 we can prove that the nonlinear
feedback controller:
u 5
u1
u2


5
2k1x1 1
k2x3x2
ðx2
1 1 x2
2Þ
2k1x2 2 k2x3x1
x2
1 1 x2
2
x2
1 1 x2
2 6¼ 0
2
6664
with k1 . 0 and k2 . 0, brings the state x 5 ½x1; x2; x3Τ to the origin asymptotically.
We now consider the full model (6.85a)(6.85e). By using the state transformation:
z1 5 φ;
z2 5 x cos φ 1 y sin φ;
z3 5 x sin φ 2 y cos φ
ð6:88aÞ
and input transformation:
u1 5 N=I;
u2 5 F=m 2 ðN=IÞz3 2 ω2z2
ð6:88bÞ
the full model can be transformed to the extended chained form:
€z1 5 u1;
€z2 5 u2;
_z3 5 z2_z1
which in terms of the new variables in Eq. (6.86) takes the form of the extended
(double) Brockett integrator (2.60):
€x1 5 u1
€x2 5 u2
_x3 5 x1 _x2 2 x2 _x1
ð6:89Þ
223
Mobile Robot Control II: Affine Systems and Invariant Manifold Methods

We will work with the model (6.89). The control problem is to derive a nonlinear
state feedback controller that asymptotically stabilizes the system at the origin [11].
We define the manifold:
M 5 fxAR5:x3 5 0g
ð6:90Þ
where:
x 5 ½x1; x2; x3; _x1; _x2Τ
ð6:91Þ
As usual, we will select a control law that makes M invariant and assures that
x ! 0 asymptotically. If x =2 M, the control law drives x3 to M asymptotically. The
controller that we will examine is [11]:
u 5
u1
u2


5
2k1x1 2 k2 _x1 1 k3x3x2
x2
1 1 x2
2
2k1x2 2 k2 _x2 2 k3x3x1
x2
1 1 x2
2
x2
1 1 x2
2 6¼ 0
2
6664
ð6:92aÞ
with:
k2 . 0;
k2
2=4 . k1 . 0;
k2
2=4 . k3 . 0
ð6:92bÞ
It will be shown that for any x =2 M where:
M 5 fxAR5:x2
1 1 x2
2 5 0;
x3 6¼ 0g
the controller ((6.92a) and (6.92b)) stabilizes the system at the origin. Indeed, intro-
ducing Eq. (6.92a) into Eq. (6.89) we get the closed-loop system:
€x1 1 k1x1 1 k2 _x1 2 k3x2x3=ðx2
1 1 x2
2Þ 5 0
ð6:93aÞ
€x2 1 k1x2 1 k2 _x2 1 k3x1x3=ðx2
1 1 x2
2Þ 5 0
ð6:93bÞ
€x3 1 k2 _x3 1 k3x3 5 0
ð6:93cÞ
Equation (6.93c) represents a second-order linear system with characteristic
polynomial:
χðλÞ 5 λ2 1 k2λ 1 k3
224
Introduction to Mobile Robot Control

which is exponentially stable when the eigenvalues λ1;2 5 2k2=2 7 ð
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
k2
2 2 4k3
p
Þ=2
are negative. The conditions for λi , 0 ði 5 1; 2Þ are k2 . 0 and k2
2=4 . k3 . 0
which give λ1 , λ2 , 0. Clearly, for any initial condition x3ð0Þ 6¼ 0, x3ðtÞ ! 0
asymptotically, that is, x3ðtÞ is attracted to go in the invariant manifold M given by
Eq. (6.90). Once x3 is in this manifold, Eqs. (6.93a) and (6.93b) become:
€x1 1 k2 _x1 1 k1x1 5 0;
€x2 1 k2 _x2 1 k1x2 5 0
which are exponentially stable when:
k2
2=4 . k1 . 0;
k2 . 0
The above results show that using the controller ((6.92a) and (6.92b)), the origin
x 5 0 becomes exponentially stable for all xAM. It is remarked that the controller
((6.92a) and (6.92b)) is valid when x2
1 1 x2
2 6¼ 0.
6.4.3
Stabilizing Control of Car-Like Robot in Chained Model Form
We consider the (2,4) chained form (2.69) of the rear-wheel driven car:
_x1 5 u1
_x2 5 u2
_x3 5 x2u1
_x4 5 x3u1
x 5
x1
x2
x3
x4
2
664
3
775AX1CRn;
u 5
u1
u2


AUCR2
ð6:94Þ
The problem is to derive a static discontinuous nonlinear controller u 5 uðxÞ
which stabilizes the system (6.94).
We will use the invariant manifold approach, starting by constructing an invari-
ant manifold of the system via the linear state feedback controller:
u1ðxÞ 5 2k1x1;
u2ðxÞ 5 2k2x1 2 k3x2
ð6:95Þ
where k1; k2; k3 are real constant gains with k1 . 0, k2AR, k3 . 0, and k1 6¼ k3.
Then, we will enhance this controller to make the constructed manifold an attrac-
tive manifold. Introducing Eq. (6.95) into Eq. (6.94), we get the closed-loop sys-
tem, which has the following solution [12]:
x1ðtÞ 5 X1
1 e2k1t
x2ðtÞ 5 X1
2 e2k1t 1 X2
2 e2k3t
x3ðtÞ 5 X1
3 e22k1t 1 X2
3 e2Kbt 1 s3ðx0Þ
x4ðtÞ 5 X1
4 e2k1t 1 X2
4 e23k1t 1 X3
4 e2ðk11KbÞt 1 s4ðx0Þ
ð6:96Þ
225
Mobile Robot Control II: Affine Systems and Invariant Manifold Methods

where:
X1
1 5 x10;
X1
2 5 ðk2=KaÞx10;
X2
2 5 ½x20 2 ðk2=KaÞx10
X1
3 5 ðk2=2KaÞx2
10;
X2
3 5 ðk1=KbÞ½x20 2 ðk2=KaÞx10x10
X1
4 5 x10s3ðx0Þ;
X2
4 5
K2
6Ka
0
@
1
Ax3
10;
X3
4 5
k2
1x2
10
Kbðk1 1 KbÞ
x20 2 k2
Ka
x10
2
4
3
5
Ka 5 k1 2 k3; Kb 5 k1 1 k3
ð6:97Þ
Here, s3ðx0Þ and s4ðx0Þ are integration constants determined by the initial conditions
xið0Þ 5 xi0 at t 5 0. Clearly:
x1ðtÞ ! 0;
x2ðtÞ ! 0;
x3ðtÞ ! s3ðx0Þ;
x4ðtÞ ! s4ðx0Þ
Thus, if we select the initial conditions such that s3ðx0Þ 5 0 and s4ðx0Þ 5 0, then
the entire state x 5 ½x1; x2; x3; x4Τ tends to the origin. Now, we set t 5 0 in
Eq. (6.96), solve for s3ðx0Þ and s4ðx0Þ, and then replace x0 by xðtÞ. In this way, we
construct the functions:
s3ðxÞ 5 x3 2
k1
Kb
0
@
1
Ax1x2 1
k2
2Kb
0
@
1
Ax2
1
s4ðxÞ 5 x4 2 x1x3 1
k1
k1 1 Kb
x2
1x2 2
k2
3ð2k1 1 KbÞ x3
1
ð6:98Þ
These functions define the manifold:
M 5 fxAR4:siðxÞ 5 0;
i 5 3; 4g
ð6:99Þ
Cleary, since x1ðtÞ and x2ðtÞ tend exponentially to zero, if the state xðtÞ belongs to
M, then it tends to zero. It is easy to verify that the manifold M is an invariant mani-
fold for the closed-loop system ((6.94) and (6.95)), that is, M is invariant for the sys-
tem (6.94) under the linear control law (6.95). Indeed, evaluating the Lie derivative
of sjðxÞ along the vector fields of (6.94) with the controller (6.95) we get:
_sjðxÞ 5 Lf sj 5 0;
j 5 3; 4 for all xAM
This is the condition (6.18) that assures the invariance of M for the closed-loop
system. This means that once the trajectories of the closed-loop system are in M,
they remain there for all future times. Now, since the mapping:
ðx1; x2; x3; x4Þ ! ðx1; x2; s3; s4Þ
226
Introduction to Mobile Robot Control

is a diffeomorphism, the stabilization of ðx1; x2; x3; x4Þ is equivalent to the stabiliza-
tion of ðx1; x2; s3; s4Þ. Therefore, to stabilize the system (6.94), it is sufficient to
lead ðx1; x2; x3; x4Þ into M by an additional state feedback, that is, to make M an
attractive manifold. The closed-loop system for ½x1; x2; s3; s4Τ with the controller
(6.95) is found to be:
_x1 5 u1
_x2 5 u2
_s3 5 ð1=KbÞ½ðk3x2 1 k2x1Þu1 2 k1x1u2
_s4 5 2½1=ðk1 1 KbÞ½ðk3x2 1 k2x1Þu1 2 k1x1u2x1
ð6:100Þ
The enhanced controller to be used is:
u1 5 2k1x1;
u2 5 2k2x1 2 k3x2 1 υ
ð6:101Þ
where υ is a control term added to the u2 channel. Introducing Eq. (6.101) into
Eq. (6.100) we get:
_x1 5 2k1x1
_x2 5 2k2x1 2 k3x2 1 υ
_s3 5 2ðk1=KbÞx1υ
_s4 5 ½k1=ðk1 1 KbÞx2
1υ
ð6:102Þ
The system of the last two equations (6.102) can be written as:
_s 5 Eðx1Þbυ;
s 5 ½s3; s4Τ
ð6:103aÞ
where:
Eðx1Þ 5
x1
0
0
x2
1


;
b 5
2k1=Kb
k1=ðk1 1 KbÞ


ð6:103bÞ
Now, introducing a transformed variable z:
z 5 E21ðx1Þs
ð6:104Þ
which is valid for x1 6¼ 0, we get:
_z 5 d
dt E21ðx1Þs 1 E21ðxÞ_s
5
2_x1=x2
1
0
0
22_x1=x3
1
"
#
Eðx1Þz 1 bυ
5
k1=x1
0
0
2k1=x2
1


Eðx1Þz 1 bυ
227
Mobile Robot Control II: Affine Systems and Invariant Manifold Methods

that is:
_z 5 Az 1 bυ
ð6:105aÞ
where:
A 5
k1
0
0
2k1


ð6:105bÞ
The system ((6.105a) and (6.105b)) will be stabilized by using a feedback con-
troller of the form:
υ 5 gΤz;
g 5 ½g1; g2T
ð6:106Þ
Using Eq. (6.106) the closed-loop system is:
_z 5 Acz
ð6:107Þ
where:
Ac 5 A 1 bgΤ
ð6:108Þ
is the closed-loop matrix. Therefore, to obtain exponential convergence to z 5 0,
the gain vector g must be selected such as all eigenvalues of Ac are negative real
numbers. When z ! 0 asymptotically, we have:
E21ðx1Þs 5
1=x1
0
0
1=x2
1


s ! 0
ð6:109Þ
which assures that s tends to zero faster than x1. Therefore, the manifold M is
reached before x1 becomes zero (which assures the boundedness of the control law
(6.106)).
For example, selecting k1 5 2; k2 5 0; k3 5 4; Kb 5 k1 1 k3 5 6, we get:
A 5
2
0
0
4


;
b 5
21=3
1=4


;
A 1 bgΤ 5
2 2 g1
3
2g2
3
g1
4
4 1 g2
4
2
664
3
775
If we wish the eigenvalues of A 1 bgΤ to be λ1 5 22 and λ2 5 23, the gain
vector g must be:
g 5
230
284


228
Introduction to Mobile Robot Control

From the above values of k1; λ2, λ3, and Eq. (6.109), it follows that x1; s3, and
s4 converge to zero according to:
x1 5 C1 e22t; s3 5 C3 e24t and s4 5 C4 e27t
Example 6.9
It is desired to apply the method of constructing invariant manifolds presented in
Section 6.43 for the feedback stabilization to x 5 0 of the following Brockett type integra-
tor models:
(a) Double integrator:
_x1 5 u1;
_x2 5 u2;
_x3 5 x1u2 2 x2u1
ð6:110Þ
(b) Extended double integrator:
_x1 5 y1;
_x2 5 y2;
_x3 5 x1y2 2 x2y1;
_y1 5 u1;
_y2 5 u2
ð6:111Þ
Justify the type of WMRs that can be modeled by the above integrators.
Solution
(a) Double integrator:
The double integrator model describes the kinematic performance of a differential-drive
WMR and is derived as described in Section 6.4.2 (Eq. (6.87)). To derive an invariant mani-
fold, we introduce the linear control law [13]:
u1 5 2k1x1;
u2 5 2k1x2;
k1 . 0
ð6:112Þ
into Eq. (6.110), and get the closed-loop system:
_x1 1 k1x1 5 0;
_x2 1 k1x2 5 0
ð6:113aÞ
_x3 1 k1x1x2 2 k1x1x2 5 0
ð6:113bÞ
The response of Eqs. (6.113a) and (6.113b) when the initial conditions are x1ð0Þ 5 x10;
x2ð0Þ 5 x20 and x3ð0Þ 5 x30, is:
x1ðtÞ 5 x10 e2k1t;
x2ðtÞ 5 x20 e2k1t;
x3ðtÞ 5 x30
ð6:114aÞ
The candidate manifold M 5 fxAR3; sðxÞ 5 0g is selected here as:
sðxÞ 5 x3ðtÞ
ð6:114bÞ
for which we obtain:
_sðxÞ 5 _x3ðtÞ 5 0
ð6:114cÞ
229
Mobile Robot Control II: Affine Systems and Invariant Manifold Methods

that is:
sðxÞ 5 const:
ð6:114dÞ
Clearly, once the system state reaches M (i.e., sðxÞ 5 0), then by Eq. (6.114b) the entire
state x 5 ½x1; x2; x3Τ tends to zero exponentially. Therefore, M is an invariant manifold for
the double integrator (6.110) under the feedback control law (6.112). To make M an
attractive manifold, we work with the candidate Lyapunov function:
VðxÞ 5 ð1=2Þs2ðxÞ
ð6:115Þ
for which, in view of Eq. (6.110), we obtain:
_VðxÞ 5 sðxÞ_sðxÞ 5 sðxÞðx1u2 2 x2u1Þ
To make _VðxÞ # 0, we select u1 and u2 as:
u1 5 k2sðxÞx2ðtÞ;
u2 5 2k2sðxÞx1ðtÞ;
k2 . 0
which give:
_VðxÞ 5 2k2PðtÞs2ðxÞ , 0
ð6:116aÞ
for
PðtÞ 5 x2
1ðtÞ 1 x2
2ðtÞ
ð6:116bÞ
Here, we have:
_PðtÞ 5 2ðx1 _x1 1 x2 _x2Þ
5 2ðx1u1 1 x2u2Þ 5 k2sðxÞ½x1x2 2 x2x1  0
ð6:116cÞ
Therefore, for:
PðtÞ 5 Pð0Þ 5 x2
1ð0Þ 1 x2
2ð0Þ 6¼ 0
the manifold sðxÞ tends to zero, and assures that x1ðtÞ ! 0; x2ðtÞ ! 0, and x3ðtÞ ! 0,
provided that x1ð0Þ 6¼ 0 and/or x2ð0Þ 6¼ 0.
The total quasi-continuous stabilizing controller is selected as:
u 5
u1
u2


5 2k1 x1
x2


1 k2
s
P
 
x2
2x1


ð6:117aÞ
which leads to (see Eq. (6.116c)):
_PðtÞ 5 2ðx1u1 1 x2u2Þ 5 22k1PðtÞ
230
Introduction to Mobile Robot Control

and
_sðtÞ 5 _x3ðtÞ 5 x1u2 2 x2u1 5 2 k2sðtÞ
It follows that:
PðtÞ 5 Pð0Þe22k1t;
sðtÞ 5 sð0Þe2k2t
ð6:117bÞ
To ensure that Eq. (6.117a) is bounded, sðtÞ must converge to zero faster than PðtÞ.
From Eq. (6.117b), we see that this holds if:
k2 . 2k1
(b)Extended double integrator:
The extended double integrator describes the full (kinematic and dynamic) performance
of the differential-drive WMR as explained in Section 6.4.2(see Eqs. (6.85a)(6.85e),
(6.88a), (6.88b), and (6.89)). The solution to be given here is different than the one pro-
vided in Section 6.4.2. Here, to construct an invariant manifold, we start with the control
law [13,15]:
u1 5 22k1y1 2 k2
1x1;
u2 5 22k1y2 2 k2
1x2
ð6:118Þ
The state vector of the system is:
x 5 ½x1; x2; y1; y2Τ
ð6:119Þ
Introducing the control (6.118) into Eq. (6.111), we get the closed-loop system:
_x1 5 y1
_x2 5 y2
_y1 5 22k1y1 2 k2
1x1
_y2 5 22k1y2 2 k2
1x2
ð6:120Þ
which gives the response:
x1ðtÞ 5 x10½e2k1t 1 k1t e2k1t 1 y10t e2k1t
x2ðtÞ 5 x20½e2k1t 1 k1t e2k1t 1 y20t e2k1t
y1ðtÞ 5 x10½ 2 k2
1t e2k1t 1 y10½e2k1t 2 k1t e2k1t
y2ðtÞ 5 x20½ 2 k2
1t e2k1t 1 y20½e2k1t 2 k1t e2k1t
ð6:121Þ
where xi0; yi0ði 5 1; 2Þ are the initial conditions. The expression for x3ðtÞ is derived by
integrating the third equation of (6.111): _x3 5 x1y2 2 x2y1. The result is:
x3ðtÞ 5 s3ðx0Þ 2 ðx10y20=2k1Þ½e22k1t 2 1 1 ðy10x20=2k1Þ½e22k1t 2 1
ð6:122Þ
231
Mobile Robot Control II: Affine Systems and Invariant Manifold Methods

where s3ðx0Þ is the integration constant determined using the initial conditions. Clearly,
x1ðtÞ ! 0; x2ðtÞ ! 0; y1ðtÞ ! 0; y2ðtÞ ! 0, and x3ðtÞ ! s3ðx0Þ. Therefore, selecting
the initial conditions such that s3ðx0Þ 5 0, we get x3ðtÞ ! 0. Now, we set t 5 0 in
Eq. (6.122), solve for s3ðx0Þ, and replace xð0Þ by xðtÞ to get the function:
s3ðxÞ 5 ð1=2k1Þx1ðtÞy2ðtÞ 2 ð1=2k1Þy1ðtÞx2ðtÞ
ð6:123Þ
which constitutes an invariant manifold, because:
_s3ðxÞ 5 Lfs3ðxÞ 5 0;
xAR4
ð6:124Þ
where Lf is the derivative of s3 along the system field f of the closed-loop system (6.120):
f 5 ½y1; y2; 22k1y12k2
1x1; 22k1y22k2
1x2Τ
ð6:125Þ
This means that once x1ðtÞ; x2ðtÞ; y1ðtÞ, and y2ðtÞ enter the manifold at some time t 5 Τ:
M 5 fxAR4:s3ðxÞ 5 0g
ð6:126Þ
they remain there for all subsequent times t $ Τ. Overall, the controller (6.118) assures
that x1ðtÞ ! 0; x2ðtÞ ! 0; y1ðtÞ ! 0; y2ðtÞ ! 0, and x3ðtÞ ! 0 because s3ðxÞ 5 0 has
already been satisfied. It remains to enhance the controller (6.118), as usual, in order to
assure that M is an attractive manifold. To this end, we use the Lyapunov function:
VðxÞ 5 1
2 s2
3ðxÞ
and check if the control law:
u1 5 22k1y1;
u2 5 2k2s3ðxÞ=x1ðtÞ 2 2k1y2
ðk2 . 0Þ
ð6:127Þ
makes _VðxÞ # 0. Indeed from Eq. (6.123) we have:
_VðxÞ 5 s3ðxÞ_s3ðxÞ 5 s3ðxÞ x1y2 2 x2y1 1 1
2k1
ðx1u2 2 x2u1Þ
2
4
3
5
5 s3ðxÞ½x1y2 2 x2y1 1 ð1=2k1Þ½ 2 k2s3ðxÞ=x1ðtÞ 2 x1y2 1 x2y1
5 2ðk2=2k1Þs2 # 0
for k2 . 0; k1 . 0
Now, using the controller (6.127), it is easy to verify that:
_s3ðtÞ 5 2ðk2=2k1Þs3ðtÞ
ð6:128Þ
which has the response:
s3ðtÞ 5 s30 e2ðk2=2k1Þt
ð6:129Þ
232
Introduction to Mobile Robot Control

To assure that the controller u2 in Eq. (6.127) is bounded, we must select k1 and k2
such that:
s3ðxÞ
x1ðtÞ

 , N
as t ! N
ð6:130aÞ
This can be done easily using Eqs. (6.121) and (6.129).
The overall controller is formed by joining Eqs. (6.118) and (6.127) as:
u1 5 22k1y1 2 k2
1x1
u2 5 22k1y2 2 k2
1x2 2 k2s3ðxÞ=x1ðtÞ
ð6:130bÞ
Now, it follows again that _s3ðtÞ 5 2 ðk2=2k1Þs3ðtÞ, and so for keeping u2 bounded, the
same condition (6.130a) should be assured with proper choice of k1 and k2.
Example 6.10 (Treatment of controller singularity)
It is desired to find ways of avoiding the controller singularity of:
(a) the controller ((6.81a) and (6.81b)) when z2
1 1 z2
2 5 0
(b) the controller (6.127) when x1ðtÞ 5 0
(a) Singularity at z2
1 1 z2
2 5 0
One way to avoid the controller singularity at z2
1 1 z2
2 5 0, that is, at z1 5 z2 5 0, is to
create a region around the z3-axis where this controller is not used, and replaced by a new
controller [16]. Defining the new variable ζ as:
ζ 5 s=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
z2
1 1 z2
2
q
5 s=l
ð6:131aÞ
It can be shown that such a region is:
Uζ 5 fðz1; z2; z3ÞAR3:jζj $ ζg
ð6:131bÞ
where ζ is a chosen large positive bound. In the region Uζ, we may use, instead of
Eqs. (6.81a) and (6.81b), the control law:
u2 5 b sgnðsÞ;
u1 5 0
ð6:132Þ
where b is a constant gain that specifies how near to the singularity the switching
between the two controllers takes place. It can be easily verified that using the control
law (6.132), the system leaves the region Uζ in finite time. It can also be verified that:
_ζ 5 2ðk2=2 2 k1Þζ;
k2 . 2k1
ð6:133Þ
233
Mobile Robot Control II: Affine Systems and Invariant Manifold Methods

and so the region ^Uζ, outside Uζ, in R3 is invariant. Therefore, once the system goes out-
side Uζ, it stays there for all future times, and the controller ((6.81a) and (6.81b)) can
be used without any singularity problem.
(b) Singularity at x1ðtÞ 5 0
In this case, k1 and k2 must be chosen such that js3ðxðtÞÞ=x1ðtÞj is bounded at all times.
Using Eqs. (6.111), (6.127), and (6.129), we get [13,15]:
lim
t!N
s3ðxðtÞÞ
x1ðtÞ
5 lim
t!N
s30 exp½2ðk2=2k1Þt
x10 2 ð1=2k1Þy10½expð2 2k1tÞ 2 1 5 0
for x10 1 ð1=2k1Þy10 6¼ 0, and x10 6¼ 0. But, if x10 6¼ 0 and x10 1 ð1=2k1Þy10 5 0, we get:
s3ðxÞ
x1ðtÞ


5
s30 exp 2ðk2=k1Þt


2 ð1=2k1Þy10 expð2 2k1tÞ


# jð2k1s30=y10Þexp½ 2 ðk2=2k1 2 2k1Þtj
Clearly, since 2k1s30=y10 is bounded, js3ðxÞ=x1ðtÞj decreases exponentially if k1 and k2
are such that k2=2k1 2 2k1 . 0, that is, if:
k2 . 4k2
1
Now, we have to assure that y2ðtÞ ! 0 as t ! N. From Eqs. (6.111) and (6.127), we
get:
_y2 5 2 2k1y2 1 σðx; tÞ
where σðx; tÞ 5 2 k2s3ðxÞ=x1ðtÞ. It can be easily verified that if σðx; tÞ decreases faster
than expð2 2k1Þ, then y2ðtÞ converges exponentially to zero with rate 2k1. Therefore k1
and k2 must be such that:
k2=2k1 2 2k1 . 2k1;
i:e:;
k2 $ 8k2
1
ð6:134Þ
From the above, it follows that the controller (6.127), with k1 and k2 satisfying
(6.134), leads to a closed-loop system free of singularities.
References
[1] Isidori A. Nonlinear control systems: an introduction. Berlin/New York: Springer; 1985.
[2] Slotine JJ, Li W. Applied nonlinear control. Englewood Cliffs: Prentice Hall; 1991.
[3] Nijmeijer H, Van der Schaft HR. Nonlinear dynamical control systems. Berlin/New York:
Springer; 1990.
[4] Sastry S. Nonlinear systems: analysis stability and control. Berlin/New York: Springer;
1999.
[5] Brockett RW. Asymptotic stability and feedback stabilization: differential geometric
control theory. Boston, MA: Birkhauser; 1983.
234
Introduction to Mobile Robot Control

[6] Yun X, Yamamoto Y. On feedback linearization of mobile robots technical report
(CIS). University of Pennsylvania: Department of Computer and Information Science;
1992.
[7] Yang E, Gu D, Mita T, Hu H. Nonlinear tracking control of a car-like mobile robot via
dynamic feedback linearization. Proceedings of control 2004. University of Bath: UK;
September 2004 [paper 1D-218].
[8] DeLuca A, Oriolo G, Samson C. Feedback control of a nonholonomic car-like robot.
In: Laumont JP, editor. Robot motion planning and control. Berlin/New York:
Springer; 1998. p. 171253.
[9] Astolfi A. Exponential stabilization of a wheeled mobile robot via discontinuous con-
trol. J Dyn Syst Meas Control 1999;121:1216.
[10] Reyhanoglu M. On the stabilization of a class of nonholonomic systems using invariant
manifold technique. Proceedings of the 34th IEEE conference on decision and control.
New Orlean, LA; December 1995. p. 212526.
[11] DeVon D, Bretl T. Kinematic and dynamic control of a wheeled mobile robot.
Proceedings of IEEE/RSJ international conference on intelligent robots and systems.
San Diego, CA; October 29November 2, 2007. p. 406570.
[12] Tayebi A, Tadijne M, Rachid A. Invariant manifold approach for the stabilization of
nonholonomic chained systems: application to a mobile robot. Nonlinear Dynamics
2001;24:16781.
[13] Watanabe K, Yamamoto K, Izumi K, Maeyama S. Underactuated control for nonholo-
nomic mobile robots by using double integrator model and invariant manifold theory.
Proceedings of IEEE/RSJ international conference on intelligent robots and systems.
Taipei, Taiwan; October 1822, 2010. p. 286267.
[14] Peng Y, Liu M, Tang Z, Xie S, Luo J. Geometry stabilizing control of the extended
nonholonomic double integrator. Proceedings of IEEE international conference on
robotics and biomimetics. Tianijn, China; December 1418, 2010. p. 92631.
[15] Izumi K, Watanabe K. Switching manifold control for an extended nonholonomic dou-
ble integrator. Proceedings of international conference on control and automation sys-
tems. Kintex, Gyeonggi-do, Korea; October 2730, 2010. p. 89699.
[16] Kim BM, Tsiotras P. Controllers for unicycle-type wheeled robots: theoretical results
and experimental validation. IEEE Trans Robot Autom 2002;18(3):294307.
235
Mobile Robot Control II: Affine Systems and Invariant Manifold Methods

7 Mobile Robot Control III: Adaptive
and Robust Methods
7.1
Introduction
Most advanced control systems, such as robots, aircrafts, and missiles, have slowly
varying unknown parameters and contain important uncertainties or disturbances
due to load variation, fuel consumption, and other effects. All controllers presented
in Chapters 5 and 6 were based on the assumption that the wheeled mobile robots
(WMRs) do not involve such unknown parameters or disturbances. One of the
basic methodologies for treating such uncertain systems is the adaptive control
methodology which always employs an algorithm for identifying (estimating) the
varying parameters in real time [19]. The alternative methodology is the robust
control methodology which requires a priori knowledge of the bounds of the
parameter variations. The more precise the knowledge of these bounds is, the better
is the robustness achieved by these controllers [1017].
The adaptive controllers (control laws or algorithms) improve their perfor-
mance as the adaptation evolves with time. On the other hand, robust controllers
are trying to keep an acceptable performance right from the beginning. The adap-
tive controllers require little or no a priori knowledge for the parameters under
estimation. But the robust controllers can face large disturbances, fast variations,
and nonmodeled characteristics. Almost always, the adaptive control techniques
require some linear parameterization of the dynamic of the nonlinear under
control.
The two widely used adaptive control methods are as follows:
1. The model reference adaptive control (MRAC) method
2. The self-tuning control (STC) method
In WMRs, the MRAC method is the typical adaptive control method used, and
will be studied in this chapter.
Specifically, the objectives of the chapter are as follows:
G
To provide the necessary background concepts for understanding the material of the chap-
ter with minimum prior control knowledge
G
To present a number of implementations of model reference adaptive control applied to
mobile robots
G
To study the application of sliding mode and Lyapunov-based robust control to mobile
robots.
Introduction to Mobile Robot Control. DOI: http://dx.doi.org/10.1016/B978-0-12-417049-0.00007-9
© 2014 Elsevier Inc. All rights reserved.

7.2
Background Concepts
7.2.1
Model Reference Adaptive Control
The general architecture of a model reference adaptive control system is shown in
Figure 7.1, and contains four basic components (units) [1,2]:
1. The system to be controlled that involves unknown parameters
2. A reference model for the overall and compact determination of the desired system output
3. A feedback controller with adaptive (adjusted) parameters
4. An adaptation mechanism for updating the controller parameters
It is assumed that the structure of the controlled system is known, and only its
parameters are unknown. The reference model provides the ideal response of the
system that must be achieved through the parameters adaptation. The control law is
parameterized with a number of adaptable parameters. The control law must have
the ability to follow perfectly or asymptotically the reference response (trajectory).
This means that when the system parameters are exactly known, the respective con-
troller parameters must make the system output identical with the output of the ref-
erence model. The adaptation law searches to find the parameter values which
assure that the system response under MRAC is ultimately the same with the refer-
ence model response, that is, assure convergence of the error between the two
responses to zero. Essentially, the basic difference between conventional and adap-
tive controllers is the use of such parameter adaptation law. The two most popular
methods of designing the controller’s adaptation law are as follows:
1. The steepest descent method
2. The Lyapunov stability method
7.2.1.1
Steepest Descent Parameter Adaptation Law
This scheme is known as MIT rule since it was developed at the Massachusetts
Institute of Technology. Let β be the parameter vector, and e the error between the
actual and reference outputs. We use the following criterion:
Reference
model 
System under
control
Adaptation
mechanism
Control
law
Input r
ym
u
y
βˆ
Error e
–
+
Figure 7.1 MRAC system architecture (^β represents the estimated parameter vector).
238
Introduction to Mobile Robot Control

IðβÞ 5 1
2 e2
To reduce IðβÞ, it is logical to vary the parameters in the opposite direction
shown by dI=dβ, that is:
dβ
dt 5 2 γ dI
dβ 5 2 γe ϑe
ϑβ
ð7:1Þ
For slowly varying parameters (much slower than the other variables of the sys-
tem), the derivative ϑe=ϑβ can be computed for β constant. This derivative is
called sensitivity derivative. If we use the error criterion:
IðβÞ 5 jej;
then the adaptation law is:
dβ
dt 5 2γ ϑe
ϑβ sgnðeÞ
ð7:2Þ
where sgnðeÞ is the known signum function. The adaptation laws are applicable to
both linear and nonlinear system. In all cases, the error dynamics must first be
determined.
7.2.1.2
Lyapunov-Based Adaptation Law
This adaptation law assumes right from the beginning that the error eðtÞ will really
converge to zero. For clarity, we will illustrate this method for the simple scalar
system:
_yðtÞ 5 2 ayðtÞ 1 buðtÞ
ð7:3Þ
where uðtÞ is the control variable and yðtÞ is the measured output. The
stable reference model is assumed to be:
_ymðtÞ 5 2amymðtÞ 1 bmυðtÞ;
am . 0
ð7:4Þ
The control law is selected as:
uðtÞ 5 2k1yðtÞ 1 k0υðtÞ
ð7:5Þ
Defining the output error e 5 y 2 ym, we get the dynamic equation for the
closed-loop error, obtained using the control law (7.5), as:
_eðtÞ 5 2ame 1 ðam 2 a 2 bk1Þy 1 ðbk0 2 bmÞυ
ð7:6Þ
239
Mobile Robot Control III: Adaptive and Robust Methods

Clearly, if a 1 bk1 5 am; that is, k1 5 ðam 2 aÞ=b, and bk0 2 bm 5 0 (i.e.,
k0 5 bm=b), the closed loop system is identical with the reference model and so
eðtÞ ! 0 for t ! N. To construct an adaptation law that leads the controller para-
meters k0 and k1 to the above ideal values k0 5 bm=b and k1 5 ðam 2 aÞ=b, we use
the following candidate Lyapunov function:
Vðe; k0; k1Þ 5 1
2 e2 1 1
bγ ðbk11a2amÞ2 1 1
bγ ðbk02bmÞ2


ð7:7Þ
This function satisfies the first three properties of Lyapunov functions, and has
zero value when k0 and k1 have the ideal values. Differentiating V, we get:
_V 5 e_e 1 1
γ ðbk1 1 a 2 amÞ _k1 1 1
γ ðbk0 1 bmÞ _k0
5 2 ame2 1 1
γ ðbk1 1 a 2 amÞð _k1 2 γyeÞ
1 1
γ ðbk0 1 bmÞð _k0 1 γυeÞ
ð7:8Þ
Thus, we choose the parameter adaptation (updating) laws as:
_k0 5 2γυe
_k1 5 γye
ð7:9Þ
Then, Eq. (7.6) gives:
_V 5 2ame2 , 0
ðam . 0Þ
ð7:10Þ
Therefore, by the Lyapunov stability criterion, eðtÞ tends asymptotically to zero
as t ! N. Of course, the convergence of k0 and k1 to the ideal values is not
assured unless some other appropriate conditions are posed. We observe that the
adaptation laws (7.9) are of the general form:
_β 5 γψe
ð7:11Þ
where β is the parameter vector, e is the error between the closed loop output and
the reference output, and ψ is a known function depending on υ and y. The above
Lyapunov adaptation method is applicable to MIMO and nonlinear systems, as it
will be shown in the case of WMRs.
7.2.2
Robust Nonlinear Sliding Mode Control
The sliding mode control methodology is the robust control methodology most
commonly used in robotics, including mobile robots. For convenience, we will
240
Introduction to Mobile Robot Control

describe this method for the single-input single-output (SISO) canonical nonlinear
model (see Eq. (6.24b)) [16]:
_x1 5 x2
_x2 5 x3
^
_xn21 5 xn
_xn 5 bðxÞ 1 aðxÞu 1 dðtÞ
y 5 x1
ð7:12Þ
where uðtÞ is the scalar input, yðtÞ the scalar output, and dðtÞ a scalar disturbance
input. The state vector is:
x 5 ½x1; x2; :::; xnΤ 5 ½y; dy=dt; d2y=dt2; . . .; dn21y=dtn21Τ
The nonlinear function bðxÞ is not exactly known but with some error (or impre-
cision) jΔbðxÞj which is bounded from above by a known continuous function of x.
Similarly, the control input gain aðxÞ is not exactly known. We know its sign and
an upper bounding function. The problem under consideration is the following: It is
desired to find that uðtÞ which drives the state on a desired trajectory
xd 5 ½yd; dyd=dt; . . .; dn21yd=dtn21 in spite of the presence of the disturbance dðtÞ
and the fact that bðxÞ and aðxÞ are known with uncertainty. This problem will be
first treated under the assumption that xdðt 5 0Þ 5 xð0Þ 5 x0. The tracking error ~xðtÞ
is ~xðtÞ 5 xðtÞ 2 xdðtÞ 5 ½~y; d~y=dt; . . .; dn21 ~y=dtn21. We define a time-varying sliding
surface SðtÞ within the state space Rn as:
sðx; tÞ 5 0;
sðx; tÞ 5 ðd=dt1ΛÞn21~xðtÞ
ð7:13Þ
where Λ is a positive constant that represents the control signal bandwidth. Under
the above condition xdð0Þ 5 x0, the trajectory tracking problem xðtÞ 5 xdðtÞ is equiv-
alent to the problem of remaining on the sliding surface SðtÞ for all t. This follows
from the fact that sðx; tÞ 5 0 is a differential equation which, with initial condition
~xð0Þ 5 0, has the unique solution ~xðtÞ 5 0 for all t. Thus, to assure the trajectory
tracking xðtÞ ! xdðtÞ, we must maintain sðx; tÞ 5 0 which can be done if uðtÞ is
selected such that outside the surface SðtÞ, the following sliding condition holds:
1
2
d
dt s2ðx; tÞ # 2 γjsj
ð7:14Þ
where γ is a positive constant. This condition forces all trajectories to slide toward
the surface SðtÞ, and for this reason, the technique was named sliding mode control
technique (Figure 7.2).
The basic idea behind Eqs. (7.13) and (7.14) is to find a suitable function s of
the error and then choose a control law such that the function s2 to be (and
remain) a Lyapunov function despite the presence of the disturbance and the
241
Mobile Robot Control III: Adaptive and Robust Methods

model
uncertainty.
Moreover,
the
condition
(7.14)
secures
that
if
xdðt 5 0Þ 5 xð0Þ 5 x0 is not valid, then again the trajectory will arrive at the sur-
face SðtÞ after the elapse of some time less than or equal to jsðt 5 0Þj=γ. Indeed,
integrating
Eq.
(7.14)
with
sðt 5 0Þ . 0,
from
t 5 0
up
to
t 5 ta
we
get
0 2 sðt 5 0Þ 5 sðt 2 taÞ 2
sðt 5 0Þ # 2γðta 2 0Þ
from
which
it
follows
that
ta # sðt 5 0Þ=γ.
The
same
result
is
obtained
when
sðt 5 0Þ , 0.
Moreover,
Eq. (7.13) implies that once the trajectory arrives at the surface SðtÞ, the tracking
error converges asymptotically to zero with time constant ðn 2 1Þ=Λ or decay rate
Λ=ðn 2 1Þ. To face robustly the disturbances and model uncertainties, the sliding
mode controller that satisfies the condition (7.14) must be discontinuous when
crossing SðtÞ. This is not desired in practice, since it may excite high frequency
unmodeled dynamics (chattering effect).
On the basis of the above, the design of the sliding mode robust controllers
involves three steps:
Step 1: Select a control law that satisfies the sliding condition. This controller follows to
be of the switching (discontinuous) type involving the signum function:
sgnðsÞ 5
11;
s . 0
21;
s , 0

ð7:15Þ
Step 2: To avoid the chattering effect, the switching type controller is smoothened to get
a compromise between trajectory tracking accuracy and control signal bandwidth. This
can be typically achieved by approximating the sharply varying “sgn” function by a satu-
ration function and using a sliding boundary layer BðtÞ, instead of the sliding surface, as
shown in Figure 7.3, where BðtÞ is defined as:
BðtÞ 5 fx:jsðx; tÞ # U; xARng;
U . 0
ð7:16Þ
The boundary layer BðtÞ is an invariant region of the state space (all trajectories that
depart from a state inside BðtÞ for t 5 0, remain always inside it for all t . 0). Within the
boundary layer, the function sgnðsÞ is replaced by the smooth linear function z 5 s=U,
where U is the width of the boundary layer. Specifically, the sliding mode controller that
uses the “saturation” function, is:
satðzÞ 5
z
if
jzj # 1
sgnðzÞ
if
jzj . 1

ð7:17Þ
S(t)
Figure 7.2 . The sliding condition (7.14) forces all
trajectories to be directed toward the sliding
surface SðtÞ.
242
Introduction to Mobile Robot Control

Of course, in this case the trajectory tracking is achieved with a certain maximum error
ε, that is, for all trajectories originating inside Bðt 5 0Þ, the following condition holds:
jdi ~yðtÞ=dtij # 2ðΛÞiε;
i 5 0; 1; 2; :::; n 2 1
Step 3: Outside the boundary layer BðtÞ, the control law is defined as before, that is, to
satisfy the standard sliding condition (7.14). To illustrate how a sliding mode controller is
designed, we consider the following simple, but representative, system:
€x 5 b 1 u
where x is the scalar output, u the scalar control input, and the function bðxÞ(possibly
nonlinear or time varying) is not precisely known but only approximately with uncer-
tainty bound ρmax, that is:
j ^b 2 bj # ρmax
ð7:18Þ
The sliding surface s 5 0 that assures xðtÞ 5 xdðtÞ is given by Eq. (7.13), that is:
sðtÞ 5 ðd=dt 1 ΛÞ~x 5 _~x1 Λ~x
ð7:19Þ
where ~xðtÞ 5 xðtÞ 2 xdðtÞ. Differentiating Eq. (7.19) we get:
_s 5 €x 2 €xd 1 Λ_~x5 b 1 u 2 €xd 1 Λ_~x
ð7:20Þ
Thus, the best approximation of the continuous control law that gives _s 5 0, is:
^u 5 2 ^b 1 €xd 2 Λ_~x
ð7:21Þ
Boundary
layer
Boundary
layer
–φ
φ
u–(x,t)
u(x,t)
u+(x,t)
x
x
•
(A)
(B)
ε
ε
φ
Figure 7.3 (A) Boundary layer for n 5 2. (B) Control signal smoothing inside the
boundary layer.
243
Mobile Robot Control III: Adaptive and Robust Methods

To satisfy the sliding condition (7.14), despite the uncertainty in the function b of the
system model, we add the term 2k sgnðsÞ, and so:
u 5 ^u 2 k sgnðsÞ
ð7:22Þ
where sgnðsÞ is given by Eq. (7.15). Choosing the amplitude function k 5 kðx; _xÞ suffi-
ciently large, we can assure that the condition (7.14) is satisfied. Indeed, from
Eqs. (7.20)(7.22), we obtain:
1
2
d
dt s2 5 _ss 5 ½b 2 ^b 2 k sgnðsÞs
5 ðb 2 ^bÞs 2 kjsj
ð7:23Þ
Therefore, if we select the function kðx; _xÞ as:
k 5 ρmax 1 γ
ð7:24aÞ
and take into account the condition (7.18), the relation (7.23) gives:
1
2
d
dt s2 # 2 γjsj
ð7:24bÞ
which is the desired condition (7.14). In a similar way, we can treat a system of the
type €x 5 b 1 au, where the gain function a (possibly nonlinear) is not exactly known but
by an estimated value ^a satisfying the inequality:
1=η # ^a=a # η
ð7:24cÞ
where η 5 ηðxÞ is a given bounding function known as gain margin function. In this
case, one can easily verify that the controller:
u 5 ^a21f^u 2 k sgnðsÞg
ð7:24dÞ
with k 5 ηðρmax 1 γÞ 1 ðη 2 1Þj^uj, satisfies the sliding condition (7.24b). If the uncer-
tainty in aðxÞ is specified by:
ηmin # ^a=a # ηmax
ð7:24eÞ
then we can put it in the form (7.24c) by setting η 5 ðηmax=ηminÞ1=2 and replacing the
estimate ^a by ðηminηmaxÞ21=2 ^a. Finally, if the uncertainty in aðxÞ is determined by
ηmin # a # ηmax, then we can express it in the form (7.24c) by setting ^a 5 ðηminηmaxÞ1=2
and η 5 ðηmax=ηminÞ1=2.
7.2.3
Robust Control Using the Lyapunov Stabilization Method
This is an alternative robust control method which is based on Lyapunov stabili-
zation. In this method, we construct a Lyapunov function V for the nominal
244
Introduction to Mobile Robot Control

closed loop system, and then, we use it for the design of the controller that
assures the robustness against the system uncertainties. Consider the nonlinear
system [16]:
_x 5 fðxÞ 1 gðxÞu 1 dðx; tÞ;
xARn
ð7:25Þ
where u is the scalar control input, fðxÞ; gðxÞ have the standard meaning, and
dðx; tÞ is an uncertain function which is bounded by a known function ρðxÞ as:
jjdðx; tÞjj # ρðxÞ
ð7:26Þ
We assume that the nominal system is stabilizable, that is, there exists a state
feedback controller ^uðxÞ which gives an asymptotically closed loop system:
_x 5 fðxÞ 1 gðxÞ^uðxÞ
ð7:27Þ
at the equilibrium point x 5 0. We assume that we know a Lyapunov function V
such that:
ϑVðxÞ
ϑx

Τ
fðxÞ 1 gðxÞ^uðxÞ
½
 , 0
ðfor all x 6¼ 0Þ
ð7:28Þ
The problem is to design an additional stabilizing controller urobustðxÞ such that
the total controller:
uðxÞ 5 ^uðxÞ 1 urobustðxÞ
ð7:29Þ
robustly stabilizes the uncertain system (7.25). The requirement of robust stability
is satisfied if _V is negative along the trajectories of the system for all allowable
uncertainties. Here:
_V 5 ϑVðxÞ
ϑx

T
fðxÞ 1 gðxÞ^uðxÞ
½
 1 ϑVðxÞ
ϑx

T
gðxÞurobustðxÞ 1 dðx; tÞ
½

ð7:30Þ
Thus, urobustðxÞ should be selected such that _V , 0. We observe that the first
term of Eq. (7.30) is negative due to the selection of ^uðxÞ (see the condition
(7.28)). A solution for such a urobustðxÞ can be found if the disturbance dðx; tÞ has
the form:
dðx; tÞ 5 gðxÞdðx; tÞ
ð7:31Þ
for some uncertain function dðx; tÞ. Then, obviously:
½ϑVðxÞ=ϑxΤdðx; tÞ 5 ½ϑVðxÞ=ϑxTgðxÞdðx; tÞ 5 0
245
Mobile Robot Control III: Adaptive and Robust Methods

for all x for which ½ϑVðxÞ=ϑxTgðxÞ 5 0. The structural condition (7.31) is known
as matching condition, because it allows to write the system (7.25) as:
_x 5 fðxÞ 1 gðxÞ½u 1 dðx; tÞ
ð7:32Þ
which means that the uncertainty dðx; tÞ enters the system from the same input chan-
nel. If the matching condition (7.31) holds, the robustifying controller urobustðxÞ can
be determined in several ways. For example, if the uncertainty dðx; tÞ is bounded as:
jjdðx; tÞjj # ρðxÞ
ð7:33Þ
for some known function ρðxÞ, then the controller:
urobustðxÞ 5
2ρðxÞ ½ ϑV=ϑx

TgðxÞT
jj ϑV=ϑx

TgðxÞjj
;
jj ϑV=ϑx

TgðxÞjj 6¼ 0
0;
jj ϑV=ϑx

TgðxÞjj 5 0
8
>
>
<
>
>
:
ð7:34Þ
gives:
_V # ϑVðxÞ
ϑx

Τ
fðxÞ 1 gðxÞ^uðxÞ
½
 1

ϑVðxÞ
ϑx

T
gðxÞ
 2ρðxÞ 1 jjd x; t
ð
Þjjg , 0

because the first term is negative by the condition (7.28), and the second term is
negative by Eq. (7.33). Thus, the controller:
uðxÞ 5 ^uðxÞ 1 urobustðxÞ
where ^u satisfies Eq. (7.28) and urobust given by Eq. (7.34) assures the robust
stabilization of the system for all uncertainties dðx; tÞ that are bounded as in
Eq. (7.33).
In the scalar control input case, the controller urobustðxÞ in Eq. (7.34) reduces to:
urobustðxÞ 5 2ρðxÞ sgn½ðϑV=ϑxÞTgðxÞ
We
see
that,
as
in
the
sliding
mode
case,
at
the
points
x
where
ðϑV=ϑxÞTgðxÞ 5 0, the controller urobustðxÞ is discontinuous. For this reason, some
smooth approximation must be used, which will assure convergence not at x 5 0,
but in an arbitrarily small region around x 5 0. A simple way to do this is to
replace the function zTðxÞ=jjzðxÞjj in Eq. (7.34), by the function:
σðxÞ 5
zΤðxÞ
jjzðxÞjj 1 δðxÞ ;
zðxÞ 5
ϑVðxÞ
ϑx
	

T
gðxÞ
246
Introduction to Mobile Robot Control

where δðxÞ is a smooth strictly positive function which is once differentiable and
reduces to zTðxÞ=jjzðxÞjj when δðxÞ  0. It is easy to verify that using σðxÞ, the
smooth control law ^uðxÞ gives _V # 0 for all x except for x 5 0, provided that δðxÞ is
sufficiently small.
7.3
Model Reference Adaptive Control of Mobile Robots
7.3.1
Differential Drive WMR
We consider the feedback tracking controller of the differential drive WMR shown
in Figure 5.11, and assume that the inertial parameters m and I are unknown [8,9].
Therefore, if ^m and ^I are the estimates of m and I, the control laws ((5.59a) and
(5.59b)) are replaced by:
τa 5 ^m_vd 1 Ka ~vc;
~vc 5 vd 2 vc
ð7:35aÞ
τb 5 ^I _ωd 1 Kb ~ωc;
~ωc 5 ωd 2 ωc
ð7:35bÞ
Now, introducing Eqs. (5.49a) and (5.49b) into Eqs. (7.35a) and (7.35b) we
find:
_vc 5 β1 _vd 1 β2ðvd 2 vcÞ
ð7:36aÞ
_ωc 5 β3 _ωd 1 β4ðωd 2 ωcÞ
ð7:36bÞ
where:
β1 5 ^m=m;
β2 5 Ka=m;
β3 5 ^I=I;
β4 5 Kb=I
ð7:36cÞ
are linearly appearing parameters that will be estimated (updated) by the adaptation
law. The reference models for vc and ωc are taken to be:
_vr 1 βrvvr 5 0;
βrv . 0
ð7:37aÞ
_ωr 1 βrωωr 5 0;
βrω . 0
ð7:37bÞ
where βrv and βrω are linear and angular damping coefficients, respectively. We
will now work with the linear velocity system (7.36a) and the corresponding refer-
ence model (7.37a) which is written in the form:
_vm 5 _vd 1 βrvvd 2 βrvvm
ð7:38Þ
247
Mobile Robot Control III: Adaptive and Robust Methods

where vm 5 vd 2 vr. Now, using Eqs. (7.36a) and (7.38) we find that the error
e 5 vc 2 vm between the system and reference model velocities is described by the
dynamic equation:
_eðtÞ 5 2 βrve 1 ðβ1 2 1Þ_vd 1 ðβ2 2 βrvÞðvd 2 vcÞ
ð7:39Þ
To construct an adaptation law for β1 and β2, we use the following candidate
function, in analogy to Eq. (7.7):
Vðe; β1; β2Þ 5 1
2 e2 1 1
γ1
ðβ121Þ2 1 1
γ2
ðβ22βrvÞ2


Differentiating V and using Eq. (7.39) we get:
_V 5 e_e 1 ð1=γ1Þðβ1 2 1Þ_β1 1 ð1=γ2Þðβ2 2 βrvÞ_β2
5 e½2βrve 1 ðβ1 2 1Þ_vd 1 ðβ2 2 βrvÞ vd 2 vc
ð
Þ 1 ð1=γ1Þðβ1 2 1Þ_β1
1 ð1=γ2Þðβ2 2 βrvÞ_β2
5 2βrve2 1 ðβ1 2 1Þ½e_vd 1 ð1=γ1Þ_β1 1 ðβ2 2 βrvÞ½eðvd 2 vcÞ 1 ð1=γ2Þ_β2
Thus, selecting the adaptation laws for β1 and β2 as:
_β1 5 2γ1 _vde 5 γ1ψ1e;
ψ1 5 2 _vd
ð7:40aÞ
_β2 5 2γ2ðvd 2 vcÞe 5 γ2ψ2e;
ψ2 5 vc 2 vd
ð7:40bÞ
we get:
_V 5 2βrve2 # 0
ðsince βrv . 0Þ
which, by the Lyapunov stability criterion, implies that eðtÞ converges asymptoti-
cally to zero. The corresponding adaptation laws for β3 and β4, which have the
form (7.40a) and (7.40b), are:
_β3 5 γ3ψ3e0;
_β4 5 γ4ψ4e0
ð7:41Þ
where e0, ψ3, and ψ4 have obvious corresponding definitions.
On the basis of the above, the tracking control system of Figure 5.11 can be
upgraded to an adaptive control system by embedding in the boxes for 1=m and
1=I the adaptation laws (7.40a), (7.40b), and (7.41).
248
Introduction to Mobile Robot Control

7.3.2
Adaptive Control Via InputOutput Linearization
7.3.2.1
Tracking Control for Known Parameters
In general, nonholonomic WMRs (unicycle-type and car-like type) have two inputs
and two outputs. It was described, in Example 6.8, that using properly the outputs,
the system can be inputoutput decoupled using a static feedback linearizing/
decoupling controller. For car-like robots, these outputs may be the coordinates
xc; yc of a point C in front of the robot that lies on the steering line. For the differ-
ential drive robots, the proper outputs are selected to be the coordinates xc; yc of a
point C in front of the robot that lies on the axis of the linear velocity v of the robot
(see Figure 6.4) which has an angle φ with respect to the x-axis of the world coor-
dinate frame. Therefore, the output vector is:
y 5
y1
y2


5
x 1 L cos φ
y 1 L sin φ


5
xc
yc


ð7:42Þ
The proof that the use of the output (7.42) allows static inputoutput decoupling
is analogous to the proof given in Example 6.8 for the car-like robot. Now, assum-
ing that the outputs were selected such that the system can be decoupled by a static
state feedback controller, we will develop the general feedback linearization pro-
cess for an m-input m-output affine system of the form [7]:
_x 5 fðxÞ 1 g1ðxÞu1 1 ? 1 gmðxÞum;
xARn
ð7:43aÞ
y 5
y1
^
ym
2
64
3
75 5
h1ðxÞ
^
hmðxÞ
2
64
3
75ARm;
u 5
u1
^
um
2
64
3
75ARm
ð7:43bÞ
Differentiating the output yi we get:
_yi 5 LfhiðxÞ 1
X
m
k51
LgkhiðxÞuk
ð7:44aÞ
where:
LfhiðxÞ 5
X
n
j51
ϑhiðxÞ
ϑxj
_xjf
ð7:44bÞ
LgkhiðxÞ 5
X
n
j51
ϑhiðxÞ
ϑxj
_xjg
ð7:44cÞ
where _xjf and _xjg are the parts of the jth state equation (7.43a) that are due to fðxÞ
and gðxÞ, respectively.
249
Mobile Robot Control III: Adaptive and Robust Methods

Clearly, if all LgkhiðxÞ in Eq. (7.44a) are zero, then no input appears in _yi. Suppose
that ri is the relative degree corresponding to yi, that is, the lowest integer for which at
least one of the inputs appears explicitly in driyi=dtri (see Definition 6.3). Then:
yðriÞ
i
5 driyi=dtri 5 Lri
f hiðxÞ 1
X
m
k51
LgkðLri21
f
hiðxÞÞuk
ð7:45aÞ
for i 5 1; 2; . . .; m, where:
Lri
f hiðxÞ 5
X
m
j51
ϑLri21
f
hiðxÞ
ϑxj
_xjf
ð7:45bÞ
LgkLri21
f
hiðxÞ 5
X
n
j51
ϑLgkLri22
f
hiðxÞ
ϑxj
_xjg
ð7:45cÞ
and
LgkLri21
f
hiðxÞ 6¼ 0
for at least one k and all x in the linearization region of interest.
Now defining ΘðxÞ as:
ΘðxÞ 5
Lg1Lri21
f
h1. . .LgmLri21
f
h1
^
Lg1Lrm21
f
hm. . .LgmLrm21
f
hm
2
64
3
75
ð7:46Þ
the relation (7.45a), i 5 1; 2; . . .; m, is written in the compact form:
yðr1Þ
1
^
yðrmÞ
m
2
64
3
75 5
Lr1
f h1
^
Lrm
f hm
2
64
3
75 1 Θ x
ð Þ
u1
^
um
2
64
3
75
ð7:47Þ
Therefore, if the inverse Θ21ðxÞ of the decoupling matrix ΘðxÞ exists for x in
the region of interest, we can use the state feedback law [7]:
uðxÞ 5 FðxÞ 1 GðxÞυ;
υ 5 ½υ1; υ2; . . .; υmT
ð7:48aÞ
where υ is the new input vector, and:
FðxÞ 5 2Θ21ðxÞ
Lr1
f h1
^
Lrm
f hm
2
64
3
75;
GðxÞ 5 Θ21ðxÞ
ð7:48bÞ
250
Introduction to Mobile Robot Control

Introducing the controller ((7.48a) and (7.48b)) in Eq. (7.47) we get the closed
loop system:
yðr1Þ
1
^
yðrmÞ
m
2
64
3
75 5
υ1
^
υm
2
64
3
75
ð7:48cÞ
which is linear and inputoutput decoupled for the selected outputs that allow
inputoutput decoupling via static state feedback.
Now, the above decoupling m-input m-output method will be applied to the
dynamic model ((6.43a) and (6.43b)) of the differential drive WMR:
_x 5 fðxÞ 1 gðxÞu;
xAR6
ð7:49aÞ
fðxÞ 5
Bv
^
2D
21Cv
2
64
3
75 5
Bv
^
f2
2
64
3
75AR6
ð7:49bÞ
gðxÞ 5
0
^
D
21
2
64
3
75AR6;
u 5
u1
u2


5
τr
τl


AR2
ð7:49cÞ
with the output (7.42), which expressed in the world coordinate frame (see
Eq. (2.17)), is:
y 5
y1
y2


5
x
y
 
1
cos φ
2sin φ
sin φ
cos φ

 xc
yc


5
h1ðxÞ
h2ðxÞ


ð7:50Þ
Differentiating y in Eq. (7.50) twice with respect to time yields:
€y 5 L2
f hðxÞ 1 ΘðxÞuðxÞ
ð7:51aÞ
where:
L2
f hðxÞ 5 p 1 qf2;
ΘðxÞ 5 qD
21
ð7:51bÞ
with p and q being computed from fðxÞ. The model ((7.51a) and (7.51b)) has the
form ((7.45a), (7.45b)), or (7.47)), and so using the state feedback control law:
uðxÞ 5 FðxÞ 1 GðxÞυ;
υ 5 ½υ1; υ2T
ð7:52aÞ
with:
FðxÞ 5 Θ21ðxÞL2
f hðxÞ;
GðxÞ 5 Θ21ðxÞ
ð7:52bÞ
251
Mobile Robot Control III: Adaptive and Robust Methods

we get:
€y1ðtÞ 5 υ1ðtÞ
€y2ðtÞ 5 υ2ðtÞ
ð7:53Þ
Now, having available the desired output trajectory yd 5 ½y1d; y2dT, the linear
tracking controllers are selected as:
υ1 5 €y1d 1 k11ð_y1d 2 _y1Þ 1 k01ðy1d 2 y1Þ
υ2 5 €y2d 1 k12ð_y2d 2 _y2Þ 1 k02ðy2d 2 y2Þ
which lead to the closed loop error dynamics:
€~y1 k11_~y1 k01 ~y1 5 0;
€~y2 1 k12_~y1 k02 ~y2 5 0
Selecting the parameters kijði; j 5 1; 2Þ such that to obtain a desired damping
ratio ζ and undamped natural frequency (bandwith) Ω 5 ω2
n, we get the desired
asymptotic tracking performance.
7.3.2.2
Adaptive Tracking Controller
If the system ((7.43a) and (7.43b)) has unknown parameters, then the linearization
obtained by Eqs. (7.48a) and (7.48b) is not perfect. In this case, we use estimates
of these parameters, which lead to estimates ^fðxÞ; ^gðxÞ; and ^hðxÞ of fðxÞ, gðxÞ, and
hðxÞ, and the controller ((7.52a) and (7.52b)) is replaced by:
^uðxÞ 5 ^FðxÞ 1 ^GðxÞυ
ð7:54aÞ
where:
^FðxÞ 5 ^Θ
21ðxÞ ^L
2
f ^hðxÞ;
^GðxÞ 5 ^Θ
21ðxÞ
ð7:54bÞ
involves the estimates ^β of the unknown parameters. Working as usual, we can get
the adaptation law:
_^β5 γψe
ð7:55Þ
where e is the error between the closed loop output and the reference model output,
ψ depends on the system structure, and γ is a constant specifying the rate of
convergence.
7.3.3
Omnidirectional Robot
We will work with the model (3.77a):
_x 5 AðxÞx 1 BðuÞu;
xAR6;
uAR3
ð7:56Þ
252
Introduction to Mobile Robot Control

which involves unknown parameters in AðxÞ and BðxÞ. Let the reference model be:
_xm 5 Amxm 1 Bmu;
xmAR6;
uAR3
ð7:57Þ
We define the generalized state vector error as:
e 5 xm 2 x
in which case Eq. (7.56) can be written as:
_x 5 Aðe; tÞx 1 Bðe; tÞu;
xAR6;
uAR3
ð7:58Þ
Therefore, the error equation is:
_e 5 Ame 1 ½Am 2 Aðe; tÞx 1 ½Bm 2 Bðe; tÞu
ð7:59Þ
To find the adaptation laws for Aðe; tÞ and Bðe; tÞ, we define the following can-
didate Lyapunov function V in the increased state space R6 3 R6 3 6 3 R6 3 3:1
V 5 1
2 eTPe 1 tracef½Am 2 Aðe; tÞTF21
A ½Am 2 Aðe; tÞg
1 tracef½Bm 2 Bðe; tÞΤF21
B ½Bm 2 Bðe; tÞg
ð7:60Þ
where P, F21
A , and F21
B
are positive define matrices. The matrix P will be deter-
mined below, but the matrices FA and FB can be arbitrary. Differentiating V along
the error trajectory (7.59) we find:
_V 5 eTðAT
mP 1 PAmÞe
1 tracef½Am 2 Aðe; tÞT½PexT 2 F21
A _Aðe; tÞg
1 tracef½Bm 2 Bðe; tÞT½PeuT 2 F21
B _Bðe; tÞg
ð7:61Þ
Therefore, if Am is a Hurwitz matrix, then:
AT
mP 1 PAm 5 2 Q
ð7:62Þ
where Q is a positive definite matrix that allows the computation of the proper
matrix P. Therefore, the first term in _V is negative for all e 6¼ 0 and the other two
terms become zero if we select the adaptation laws for Aðe; tÞ and Bðe; tÞ as:
_Aðe; tÞ 5 FAPexT
ð7:63aÞ
_Bðe; tÞ 5 FBPeuT
ð7:63bÞ
1 It is recalled that the trace of a n 3 n matrix A 5 ½aij is defined as the sum of its diagonal elements,
that is, trace A 5 Pn
i5 aii.
253
Mobile Robot Control III: Adaptive and Robust Methods

The result is that the laws ((7.63a) and (7.63b)) assure the asymptotic conver-
gence of the MRAC scheme for any FA . 0, FB . 0, and any input vector u. We
will now see under what conditions the zero error eðtÞ  0 implies that
Aðe; tÞ 5 Am and Bðe; tÞ 5 Bm. From Eqs. (7.63a) and (7.63b), it follows (after inte-
gration) that if lim
t!NeðtÞ 5 0, then:
lim
t!N½Am 2 Aðe; tÞ 5 ~A and lim
t!N½Bm 2 Bðe; tÞ 5 ~B
where ~A and ~B represent the asymptotic difference in the parameters. Now, from
Eq. (7.59) it follows that if eðtÞ  0, then:
~Ax 1 ~Bu  0
ð7:64Þ
The identity (7.64) can hold for all t if:
1. The vectors x and u are linearly dependent and ~A 6¼ 0; ~B 6¼ 0
2. The vectors x and u are identically equal to zero
3. The vectors x and u are linearly independent and ~A 5 0; ~B 5 0
From the above it follows that only in the third case the parameters converge
with certainty. Any controller PID or other (such as the one presented in
Section 5.7) can be used in conjunction with the above parameter adaptation laws
((7.63a) and (7.63b)). Typically, the parameters that vary and are unknown while
the robot is at work (e.g., while it carries and delivers objects) are the robot mass
m, and moment of inertia I about the rotation axis.
Example 7.1
Consider a differential drive WMR where the mass m and moment of inertia I are constant
(or very slowly variant) but unknown. Derive an adaptive tracking controller of your
choice, different than that presented in Section 7.3.1.
Solution
The solution will be derived using the dynamic equation of the WMR expressed in terms of
x; y, and φ. For tracking feasibility, we assume (as usual) that the desired trajectory to be
tracked obeys the same kinematic equations as the robot at hand, namely:
_xd 5 vdcos φd;
_yd 5 vdsin φd;
_φd 5 ωd
ð7:65Þ
The kinematic and dynamic equations of the robot are (see Eqs. (3.30a), (3.30b),
and (3.31)):
_x 5 v cos φ;
_y 5 v sin φ;
_φ 5 ω
ð7:66aÞ
_v 5 ð1=mrÞu1;
_ω 5 ð2a=IrÞu2
ð7:66bÞ
where u1 5 τr 1 τl; u2 5 τr 2 τl, and τr and τl are the torques exerted by the right and
left wheel, respectively. Multiplying _x by cos φ, and _y by sin φ and adding we get
v 5 _x cos φ 1 _y sin φ
ð7:67Þ
254
Introduction to Mobile Robot Control

Differentiating _x; _y, and _φ in Eq. (7.66a), and introducing Eqs. (7.66b) and (7.67) we
obtain the following dynamic model for x; y, and φ:
€x 5 2ð_x cos φ 1 _y sin φÞ_φ sin φ 1 β1ðcos φÞu1
€y 5 ð_x cos φ 1 _y sin φÞ_φ cos φ 1 β1ðsin φÞu1
€φ 5 β2u2
ð7:68Þ
where β1 5 1=mr and β2 5 2a=Ir are the parameters to be adaptively estimated. Note that
the kinematic parameters r and 2a are precisely known (or assumed to be known). We define
x 5 x
y
φ T

and xd 5 ½xd; yd; φdT; the problem is to design a feedback controller that
drives the error ~xðtÞ 5 xdðtÞ 2 xðtÞ asymptotically to zero, while estimating adaptively the
unknown parameters β1 and β2. The first step is to find the dynamics of the tracking error
~x. For convenience, we use the equivalent error ε in the local coordinate frame, that is [8]:
ε 5 E~x;
E 5
cos φ
sin φ
0
2sin φ
cos φ
0
0
0
1
2
64
3
75
ð7:69Þ
where E is invertible and so ε ! 0 if and only if ~x ! 0. Taking into account the nonholo-
nomic constraint 2_x sin φ 1 _y cos φ 5 0, we get the following error dynamics:
_ε1 5 ωε2 2 v 1 vd cos ε3
_ε2 5 2 ωε1 1 vd sin ε3
_ε3 5 ωd 2 ω
ð7:70Þ
where ω 5 _φ and v is given by Eq. (7.67). We observe that the control inputs do not
appear explicitly in Eq. (7.70), but instead, we have the variables v and ω given by
Eq. (7.66b). Therefore, we will first select v and ω such that ε1 ! 0; ε2 ! 0, and ε3 ! 0
asymptotically, and then use them as inputs for the next step of choosing u1 and u2. If vm
and ωm are the desired values of the conceptual intermediate controls v and ω, then we
have the errors ~v 5 v 2 vm and ~ω 5 ω 2 ωm. Therefore:
v 5 vm 1 ~v;
ω 5 ωm 1 ~ω
ð7:71Þ
Now, in analogy to Eq. (5.58b) we use the following Lyapunov function:
V 5 1
2 ðε2
1 1 ε2
2Þ 1 ð1=K2Þð1 2 cos ε3Þ
ð7:72Þ
_V 5 ε1_ε1 1 ε2_ε2 1 ð1=K2Þðsin ε3Þ_ε3
5 ε1ðωε2 2 vm 2 ~v 1 vdcos ε3Þ
1 ε2ð2 ωε1 1 vd sin ε3Þ 1 ð1=K2Þðsin ε3Þðωd 2 ωm 2 ~ωÞ
5 ε1ð2 vm 1 vd cos ε3Þ 1
ε2vd 1 1
K2
ωd 2 1
K2
ωm
0
@
1
Asin ε3 2 ε1 ~v 2 ð1=K2Þ ~ω sin ε3
255
Mobile Robot Control III: Adaptive and Robust Methods

Selecting vm and ωm such that:
2vm 1 vdcos ε3 5 2 K1ε1;
K1 . 0
2 1
K2
ωm 1 1
K2
ωd 1 ε2vd 5 2 K3
K2
sin ε3;
K2 . 0
that is:
vm 5 vd cos ε3 1 K1ε1
ð7:73aÞ
ωm 5 ωd 1 K2vdε2 1 K3 sin ε3
ð7:73bÞ
the derivative of V becomes:
_V 5 2 K1ε2
1 2 ðK3=K2Þsin2 ε3 2 ε1 ~v 2 ð1=K2Þðsin ε3Þ ~ω
ð7:74Þ
Now, from Eqs. (7.66b) and (7.71) we obtain:
_~v5 β1u1 2 _vm;
_~ω5 β2u2 2 _ωm
ð7:75Þ
The first two terms of _V are negative, and so for asymptotic stability ðε1 ! 0; ε3 ! 0Þ
and parameter convergence, we have to make ~v ! 0 and ~ω ! 0. To this end, we add to
the Lyapunov function V a second term V0 defined as [8]:
V0 5 1
2 ð~v2 1 ~ω2Þ 1 1
2
jβ1j
γ1
~θ
2
1 1 jβ2j
γ2
~θ
2
2


ð7:76Þ
where ~θ 1 5 θ1 2 ^θ 1;
^θ 2 5 θ2 2 ^θ 2; θ1 5 1=β1, and θ2 5 1=β2. The total Lyapunov function
is V0 5 V 1 V0, which if differentiated in time along the trajectories of the error systems
described by Eqs. (7.70) and (7.75), and choosing:
u1 5 ^θ 1ð2K4 ~v 1 ε1 1 _vdÞ;
K4 . 0
u2 5 ^θ 2ð2K5 ~ω 1 ð1=K2Þsin ε3 1 _ωdÞ;
K5 . 0
_^θ2 5 γ1ψ1 ~v;
ψ1 5 2ð2K4 ~v 1 ε1 1 _vdÞsgnðβ1Þ
_^θ2 5 γ2ψ2 ~ω;
ψ2 5 2½2K5 ~ω 1 ð1=K2Þsin ε3 1 _ωdsgnðβ2Þ
ð7:77Þ
gives:
_V0 5 2 ðK1ε2
1 1 ðK3=K2Þsin2 ε3 1 K4 ~v2 1 K5 ~ω2Þ # 0
Now, computing the second derivative €V0 (taking into account that vd; ωd are smooth)
we find that it is finite, and so, by Barbalat’s lemma (see Section 6.2.3), _V0 is uniformly
continuous and _V0 ! 0 for t ! N. Then, it follows that ε1 ! 0; ε3 ! 0; ~v ! 0, and
~ω ! 0. It can also easily be shown that ε2 ! 0 by assuming that vd and ωd do not both
go to zero simultaneously. It is again remarked that actually the above solution is based
on the well known backstepping control procedure.
256
Introduction to Mobile Robot Control

7.4
Sliding Mode Control of Mobile Robots
Here, the sliding mode control of Section 7.2.2 will be applied to the differential
drive mobile robot ((7.66a) and (7.66b)), with kinematically compatible desired tra-
jectory ½xd; yd; φdT [12,17]. The tracking error ε, expressed in the local coordinate
frame, is given by (see Eq. (7.69)):
ε1
ε2
ε3
2
64
3
75 5
cos φd
sin φd
0
2sin φd
cos φd
0
0
0
1
2
64
3
75
x 2 xd
y 2 yd
φ 2 φd
2
64
3
75
ð7:78Þ
and satisfies the differential equations:
_ε1 5 _x cos φd 1 _y sin φd 1 ωdε2 2 vd
_ε2 5 2 _x sin φd 1 _y cos φd 2 ωdε1
_ε3 5 ω 2 ωd
ð7:79Þ
Without loss of generality, we will assume that jε3j , π=2. The system control
inputs are u1 5 τr 1 τl and u2 5 τr 2 τl.
Here, we have a two-dimensional sliding surface:
s 5
s1
s2


ð7:80aÞ
and a two-component control of the type (7.22). The components of s are defined as:
s1 5 _ε1 1 Λ1ε1
ð7:80bÞ
s2 5 _ε3 1 Λ2ε3 1 Λ0jε2jsgn ðε3Þ
ð7:80cÞ
Clearly, if s1 ! 0, then ε1 ! 0. If s2 ! 0, then we have:
_ε3 5 2Λ2ε3 2 Λ0jε2jsgnðε3Þ
ð7:81Þ
Therefore, since jε2j is bounded, we get the following conditions:
If ε3 , 0; then
_ε3 . 0
If ε3 . 0; then
_ε3 , 0
ð7:82Þ
Differentiating s1 and s2 in Eqs. (7.80b) and (7.80c), we get:
_s1 5 €ε1 1 Λ1_ε1
_s2 5 €ε3 1 Λ2_ε3 1 Λ0jε2j0 sgnðε3Þ
ð7:83Þ
257
Mobile Robot Control III: Adaptive and Robust Methods

Equations (7.66a), (7.66b), (7.78), and (7.80a)(7.80c) can be written as:
_s 5 2Hs 2 Λ sgnðsÞ
ð7:84Þ
where:
H 5
H1
0
0
H2


;
Λ 5
Λ1
0
0
Λ2


;
sgnðsÞ 5
sgnðs1Þ
sgnðs2Þ


Now, as in the SISO case, we define the following candidate Lyapunov function:
V 5 1
2 sTs 5 1
2 s2
1 1 1
2 s2
2
Differentiating V, we get:
_V 5 sT_s 5 sTð2Hs 2 Λ sgnðsÞÞ
5 2 sTHs 2 s1Λ1 sgnðs1Þ 2 s2Λ2 sgnðs2Þ
5 2 sTHs 2 Λ1js1j 2 Λ2js2j
Therefore, _V # 0 if H and Λ are selected as:
H1 . 0;
H2 . 0;
Λ1 . 0;
and
Λ2 . 0
The controller that satisfies the sliding condition (7.14) has the form of Eqs. (7.21)
and (7.22). To avoid the undesired chattering effect, the function ‘‘sgn’’ of
Eq. (7.15) is replaced by the ‘‘satðÞ’’ function given by Eq. (7.17). Therefore:
ui 5 ^ui 2 ki satðsi=UÞ
ði 5 1; 2Þ
ð7:85Þ
where U is the thickness of the boundary layer.
The best continuous control law approximation ^uiði 5 1; 2Þ that satisfies _si 5 0
ði 5 1; 2Þ is found using the dynamic model of the robot (see Eqs. (7.67)(7.68)):
€x 5 2ðv sin φÞ_φ 1 ðβ1 cos φÞu1
€y 5 ðv cos φÞ_φ 1 ðβ1 sin φÞu1
€φ 5 β2u2
together with Eqs. (7.79) and (7.80a)(7.80c). The result is (see Eq. (7.21)):
^u1 5 ð1=^β1 cos ε3Þ½_x_ε3 sin φd 2 _y_ε3 cos φd 1 _ωdε2 1 ωd_ε2 1 Λ1_ε1
ð7:86aÞ
^u2 5 ð1=^β2Þ½ 2 _ωd 1 Λ2_ε3 1 Λ0ðsgn ε3Þ_ε2 sgnðε2Þ
ð7:86bÞ
258
Introduction to Mobile Robot Control

The gains ki ði 5 1; 2Þ must be selected sufficiently large to assure that the slid-
ing
condition
(7.24b)
is
satisfied,
given
the
maximum
bounds
j^βi 2 βij 5 jΔβij # Bi;max ði 5 1; 2Þ. After some calculation, we can verify that two
gains which do the job are [17]:
k1 5 a1½H1s1 1 Λ1 satðs1=BÞ 1 p1ðB1;max=^β1Þ
k2 5 a2½H2s2 1 Λ2 satðs2=BÞ 1 p2ðB2;max=^β2Þ
where:
a1 5 1=ð^β1 1 B1;maxÞcos ε3;
a2 5 1=ð^β2 1 B2;maxÞ
p1 5 _x_ε3 sin φd 2 _y_ε3 cos φd 1 _ωdε2 1 ωd_ε2 1 Λ1_ε1
p2 5 2 _ωd 1 Λ2_ε3 1 Λ0ðsgn ε3Þ_ε2sgnðε2Þ
7.5
Sliding Mode Control in Polar Coordinates
7.5.1
Modeling
The technique develops as in the previous section, with the basic difference that
the kinematic model should be expressed in polar coordinates. Referring to the
geometry of Figure 7.4, the robot’s actual and desired kinematic equations:
_x 5 v cos φ;
_y 5 v sin φ;
_φ 5 ω;
_xd 5 vd cos φd;
_yd 5 vd sin φd;
_φd 5 ωd
have the following polar coordinates form [12,13]:
_x 5
_l
_ψ
_φ
2
64
3
75 5
v cosðψ 2 φÞ
2ðv=lÞsinðψ 2 φÞ
ω
2
64
3
75
ð7:87aÞ
l
V
Q
y
y
yr
xr
x
x
0
ψ
φ
Figure 7.4 Geometry of polar coordinates.
259
Mobile Robot Control III: Adaptive and Robust Methods

_xd 5
_ld
_ψd
_φd
2
64
3
75 5
vd cosðψd 2 φdÞ
2ðvd=ldÞsinðψd 2 φdÞ
ωd
2
64
3
75
ð7:87bÞ
where:
l 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
x2 1 y2
p
; φ 5 tg21ðy=xÞ; ld 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
x2
d 1 y2
d
q
; φd 5 tg21ðyd=xdÞ
ð7:87cÞ
We will work with the dynamic model (7.66b) enhanced with linear and rota-
tional frictional terms and disturbance inputs, namely:
mr_v 1 c1v 1 d1 5 u1
ð7:88aÞ
ðIr=2aÞ _ω 1 c2ω 1 d2 5 u2
ð7:88bÞ
where c1 and c2 are linear friction coefficients and d1, d2 are unknown disturbances
representing all uncertain inputs (e.g., due to slip). We assume that d1 and d2 are
expressed in the form:
d1 5 mrd1;
d2 5 ðIr=2aÞd2
ð7:89Þ
that is, they satisfy a matching condition similar to Eq. (7.32). To assure that vd=ld
takes finite values, we assume that ld $ ld;min . 0, where ld;min is a selected mini-
mum allowed value for ld. Also, for convenience, we assume l $ ld;min. As usual,
ld; φd, and ψd are assumed to have smooth first-order and second-order time deriva-
tives. Finally, without loss of generality, we assume that:
lðtÞ . 0;
ldðtÞ . 0 ðfor t . 0Þ;
ldð0Þ 5 0
ð7:90Þ
2π , ψd , π;
2π , ψ , π;
2π , φd , π;
2 π , φ , π
jjψd 2 φdj 2 ð2k 1 1Þπ=2j $ η
ðk 5 0; 1Þ
for all t, where η is a positive constant. (i.e., it is required the robot should not
have a posture with heading angle tangential to any circle drawn around the world
frame origin).
7.5.2
Sliding Mode Control
Here, we have a two-dimensional sliding surface s 5 ½s1; s2T defined by:
s1 5_~l1 Λ1~l
ð7:91aÞ
s2 5 _~φ1 Λ2 ~φ 1 j ~ψj sgnð~φÞ
ð7:91bÞ
where ~l 5 l 2 ld; ~φ 5 φ 2 φd and ~ψ 5 ψ 2 ψd.
260
Introduction to Mobile Robot Control

If s1 ! 0, then ~l ! 0 asymptotically. Now, since by Eq. (7.90), j ~ψj is bounded,
s2 ! 0 implies that:
If ~φ , 0; then _~φ. 0
If ~φ . 0; then _~φ, 0
Finally, if ~φ ! 0 and _~φ ! 0, then j ~ψj ! 0. To compute s1 and s2, we use the
expressions for _~l and _~φ:
_~l5 _l 2 _ld 5 v cosðψ 2 φÞ 2 vd cosðψd 2 φdÞ
_~φ5 _φ 2 _φd 5 ω 2 ωd
Therefore, by Eqs. (7.91a) and (7.91b):
s1 5 v cosðψ 2 φÞ 2 vd cosðψd 2 φdÞ 1 Λ1ðl 2 ldÞ
ð7:92aÞ
s2 5 ω 2 ωd 1 Λ2ðφ 2 φdÞ 1 ðψ 2 ψdÞsgnðφ 2 φdÞ
ð7:92bÞ
To
assure
that
l ! ld; φ ! φd,
and
ψ ! ψd
the
sliding
condition
sT_s , 0; s 5 ½s1; s2T must hold. We therefore need the expressions for _s1 and _s2,
which are found by differentiating Eqs. (7.92a) and (7.92b):
_s1 5 _v cosðψ2φÞ2 _vd cosðψd 2φdÞ1vd
d
dtcosðψ2φÞ
2vd
d
dtcosðψd 2φdÞ1Λ1 ~l
5 _v cosðψ2φÞ2v sinðψ2φÞð _ψ2 _φÞ
2 _vd cosðψd 2φdÞ1vd sinðψd 2φdÞð _ψd 2 _φÞ1Λ1 ~l
5 _v cosðψ2φÞ2v sinðψ2φÞ
h
2v
l sinðψ2φÞ
i
1vω cosðψ2φÞ
2 _vd cosðψd 1φdÞ1vd sinðψd 2φdÞ
"
2vd
ld
sinðψd 2φdÞ
#
2vdωd cosðψd 2φdÞ1Λ1 ~l
5 _v cosðψ2φÞ1Fðx;vÞ2 _vd cosðψd 2φdÞ2Fdðxd;vdÞ1Λ1 ~l
ð7:93aÞ
_s2 5 _ω2 _ωd 1Λ2ðω2ωdÞ1 d
dtj ~ψj sgnð~φÞ
ð7:93bÞ
with:
Fðx; vÞ 5 v sinðψ 2 φÞ½ðv=lÞsinðψ 2 φÞ 1 vω cosðψ 2 φÞ
261
Mobile Robot Control III: Adaptive and Robust Methods

Fðxd 2 vdÞ 5 vd sinðψd 2 φdÞ½ðvd=ldÞsinðψd 2 φdÞ 1 vdωd cosðψd 2 φdÞ
Now, _v and _ω are given by Eqs. (7.88a) and (7.88b), and so choosing u1 and u2 as:
u1 5 mr_vd 1 c1v 1 mrυ1
ð7:94aÞ
u2 5 ðIr=2aÞ _ωd 1 c2ω 1 ðIr=2aÞυ2
ð7:94bÞ
we get the closed-loop equations:
_v 5 _vd 1 υ1 2 d1;
d1 5 mrd1
ð7:95aÞ
_ω 5 _ωd 1 υ2 2 d2;
d2 5 ðIr=2aÞd2
ð7:95bÞ
Introducing Eqs. (7.95a) and (7.95b) into Eqs. (7.93a) and (7.93b), and comput-
ing sT_s we get:
sT_s 5 s1_s1 1 s2_s2
5 s1½_vd cosðψ 2 φÞ 1 υ1 cosðψ 2 φÞ
2d1 cosðψ 2 φÞ 1 Fðx; vÞ 2 _vd cosðψd 2 φdÞ 2 Fdðxd; ydÞ 1 Λ1 ~l
1 s2
"
_ωd 1 υ2 2 d2 2 _ωd 1 Λ2 ~φ 1 d
dt j ~ψj sgnð~φÞ
#
Therefore, selecting υ1 and υ2 such that:
_vd cosðψ 2 φÞ 1 υ1 cosðψ 2 φÞ 1 Fðx; vÞ 2 Fdðxd; vdÞ
2 _vd cosðψd 2 φdÞ 1 Λ1 ~l 5 2H1s1 2 K1 sgnðs1Þ
and
υ2 1 Λ2 ~φ 1 d
dt j ~ψj sgnð~φÞ 5 2H2s2 2 K2 sgnðs2Þ
that is:
υ15
1
cosðψ 2 φÞ ½2 _vd cosðψ 2 φÞ 2 Fðx; vÞ 1 Fdðxd; vdÞ 1 _vd cosðψd 2 φdÞ
2Λ1 ~l 2 H1s1 2 K1 sgnðs1Þ

ð7:96aÞ
υ2 5 2Λ2 ~φ 2 H2s2 2 K2 sgnðs2Þ 2 d
dt j ~ψj sgnð~φÞ
ð7:96bÞ
262
Introduction to Mobile Robot Control

yields:
sT_s 5 2s1½H1s1 1 K1 sgnðs1Þ 2 d1 cosðψ 2 φÞ
5 2s2½H2s2 1 K2 sgnðs2Þ 2 d2
5 2sΤHs 2 ½K1js1j 2 d1s1 cosðψ 2 φÞ
2½K2js2j 2 d2s2
which is negative definite for:
H 5
H1
0
0
H2


. 0;
K1 . d1;
K2 . d2
ð7:97Þ
7.6
Robust Control of Differential Drive Robot Using the
Lyapunov Method
We consider the differential drive WMR with disturbances in both the torque/force
input channels and the x; y velocities, that is:
_x 5 ðv 1 dvÞcos φ;
_y 5 ðv 1 dvÞsin φ;
_φ 5 ω
_v 5 2ðc1=mrÞv 1 ð1=mrÞðu1 2 d1Þ
_ω 5 2ð2ac2=IrÞω 1 ð2a=IrÞðu2 2 d2Þ
ð7:98Þ
where the dynamic model ((7.88a) and (7.88b)) is considered. The model (7.98)
can be written as:
_x 5 fðxÞ 1 GðxÞu 1 d;
u 5 u1
u2 T

ð7:99aÞ
with d 5 ½dx; dy; 0; d1; d2T and:
x 5
x
y
φ
v
ω
2
6666664
3
7777775
;
fðxÞ 5
v cos φ
v sin φ
ω
2ðc1=mrÞv
2ð2ac2=IrÞω
2
666664
3
777775
;
GðxÞ 5
0
0
0
0
0
0
1=mr
0
0
2a=Ir
2
6666664
3
7777775
ð7:99bÞ
where dx 5 dv cos φ and dy 5 dv sin φ. Defining state vectors z1 and z2 as:
z1 5
x
y
 
;
z2 5
_x
_y
 
5 _z1
ð7:100Þ
263
Mobile Robot Control III: Adaptive and Robust Methods

the system ((7.99a) and (7.99b)) can be written (after some standard algebraic
manipulation) in the form [11]:
_z1 5 z2
_z2 5 RðφÞ½u 1 δðz2; φÞ
_φ 5 ωðz2; φÞ
ð7:101Þ
where δðz2; φÞ incorporates all the uncertainties of the system. The corresponding
nominal model is obtained from Eq. (7.101) by setting to zero all the uncertain
terms involved in δðz2; φÞ, that is:
_z1 5 z2
_z2 5 RðφÞ½u 1 ^δðz2; φÞ
ð7:102Þ
_φ 5 ωðz2; φÞ
where ^δðz2; φÞ is the nominal part of δðz2; φÞ. The problem is to design a robust
tracking controller for the system (7.101) using appropriately the Lyapunov method
presented in Section 7.2.3. Defining the tracking errors:
~z1ðtÞ 5 z1ðtÞ 2 z1dðtÞ;
~z2 5 _~z1ðtÞ 5 _z1ðtÞ 2 _z1dðtÞ
ð7:103Þ
and using the computed torque control law:
u 5 R21ðφÞ½_z2dðtÞ 1 υðtÞ 2 ^δðz2; φÞ
ð7:104Þ
the system (7.102) gives the following closed-loop error equation:
_εðtÞ 5 AεðtÞ 1 BυðtÞ
ð7:105Þ
where εðtÞ 5 ½~zT
1ðtÞ; ~zT
2ðtÞT; υðtÞ is the new control input, and:
A 5
0
I2 3 2
0
0


AR4 3 4;
B 5
0
I2 3 2


AR4 3 2
ð7:106Þ
Thus, the control law (7.104), applied to the disturbed system (7.101), gives the
linearized closed-loop error system:
_εðtÞ 5 AεðtÞ 1 B½υðtÞ 1 ζðz2; φÞ
ð7:107Þ
where ζðz2; φÞ is the overall disturbance which is assumed to be bounded as:
jjζðz2; φÞjj # ρðεÞ , N
ð7:108Þ
264
Introduction to Mobile Robot Control

From now on, the robust tracking controller design is made in two steps [11]:
Step 1: Design a nominal stabilizing controller ^υðεÞ for the error system (7.105).
Step 2: Design the robustifying control term υrobustðεÞ using Eq. (7.34).
7.6.1
Nominal Controller
Selecting a linear feedback controller:
^υðεðtÞÞ 5 2 KεðtÞ;
K 5 K1
K2 

ð7:109Þ
we obtain the closed-loop error system:
_εðtÞ 5 AcεðtÞ;
Ac 5 A 2 BK
ð7:110Þ
To find the gain matrix K we use the Lyapunov function:
VðεÞ 5 1
2 εTPε
where P is a symmetric positive definite matrix given by (see Eq. (5.15)):
AT
c P 1 PAc 5 2 Q
ð7:111Þ
and Q is a selected positive definite matrix. This assures that Ac is a Hurwitz
matrix (having eigenvalues with strictly negative real parts), and so εðtÞ ! 0 expo-
nentially as t ! N.
7.6.2
Robustifying Controller
We calculate υrobustðεÞ using (7.34). Here:
½ϑVðεÞ=ϑεTB 5 BΤPεðtÞ
Therefore (see (7.108)):
υrobustðεÞ 5
2 ρðεÞBTPεðtÞ=jjBTPε tð Þjj;
jjBTPεðtÞjj 6¼ 0
0;
jjBTPεðtÞjj 5 0

ð7:112Þ
To summarize, the overall robust controller is given by the combination of the
computed torque linearizing controller (7.104), the nominal stabilizing controller
(7.109), and the robustifying controller (7.112).
265
Mobile Robot Control III: Adaptive and Robust Methods

Example 7.2
It is desired to design a robust kinematic tracking controller for the differential drive WMR
using the Lyapunov-based method of Section 7.3.
Solution
The nonrobust kinematic controller was designed in Section 5.4.1 using the Lyapunov
function (5.54):
Vð~prÞ 5 1
2 ð~x2
r 1 ~y2
r Þ 1 ð1 2 cos ~φrÞ
for the kinematic error system (5.53)
_~pr 5 fð~prÞ 1 Gð~prÞu;
u 5 ½v; ωΤ 5 ½u1; u2Τ
where:
~pr 5
~xr
~yr
~φr
2
64
3
75;
fð~prÞ 5
vd cos ~φr
vd sin ~φr
ωd
2
64
3
75;
Gð~prÞ 5
~g1
~g2  5
21
~yr
0
2~xr
0
21
2
64
3
75
2
64
where ~xr; ~yr, and ~φr are the errors between the actual and desired trajectories of xðtÞ; yðtÞ,
and φðtÞ, respectively.
The nominal controller that satisfies _Vð~prÞ # 0 is given by Eqs. (5.56a) and (5.56b):
^u 5
^v
^ω


5
vd cos ~φr 1 Kx ~xr
ωd 1 vd ~yr 1 Kφ sin ~φr
"
#
Here, the partial derivative of Vð~prÞ with respect to ~pr is:
ϑV=ϑ~pr 5
~xr
~yr
sin φr Τ

Therefore:
ϑV=ϑ~pr

ΤGð~prÞ 5
2 ~xr
2sin ~φr


Now, assume that the disturbances dv and dω in v and ω satisfy the matching conditions:
du 5
dv
dω


5 Gð~prÞ
dv
dω
"
#
5 Gð~prÞdu
with:
jjdujj # ρu
266
Introduction to Mobile Robot Control

Then, the robustifying control urobustð~prÞ is given by (see (7.34)):
urobustð~prÞ 5
2 ρu
GΤð~prÞ½ϑV=ϑ~pr
jjGΤð~prÞϑV=ϑ~prjj ;
:GΤð~prÞ ϑV
ϑ~pr
: 6¼ 0
0;
jjGΤð~prÞϑV=ϑ~prjj 5 0
8
>
>
<
>
>
:
5
2
ρu
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
~x2
r 1 sin2 ~φr
q
2 ~xr
2sin ~φr


;
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
~x2
r 1 sin2 ~φr
q
6¼ 0
0;
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
~x2
r 1 sin2 ~φr
q
5 0
8
>
>
>
<
>
>
>
:
Therefore, the full robust controller is:
uð~prÞ 5 ^uð~prÞ 1 urobustð~prÞ
The discontinuous controller can be smoothened as explained in Section 7.3. Further
results and examples on WMR robust control can be found in Refs. [10,14,15].
References
[1] Marino R, Tomei P. Nonlinear control design: geometric, adaptive and robust. Upper
Saddle River, NJ: Prentice Hall; 1995.
[2] Marino R. Adaptive control of nonlinear systems: basic results and applications. IFAC-
Review Control 1997;21:5566.
[3] Huang HC, Tsai CC. Adaptive trajectory tracking and stabilization for omnidirectional
mobile robot with dynamic effect and uncertainties. Proceedings of 17th IFAC world
congress. Seoul, Korea; July 611, 2008. p. 538388.
[4] Ashoorizad M, Barzarnimi R, Afshar A, Jouzdani J. Model reference adaptive path fol-
lowing for wheeled mobile robots. Proceedings of international conference on informa-
tion and automation. Colombo, Sri Lanka (IEEE-ICIA2006); 2006. p. 28994.
[5] Alicja M. A new universal adaptive tracking control law for nonholonomic wheeled
mobile robots moving in R3 space. Proceedings of IEEE international conference robot-
ics and automation. Dedroit, MI; May 1015,1999.
[6] Kuc TY, Baek SM, Park K. Adaptive learning controller for autonomous mobile robots.
IEE Proc—Control Theory Appl 2001;148(1):4954.
[7] Fetter Lages W, Hemerly EM. Adaptive linearizing control of mobile robots. In:
Kopacek P, Pereira CE, editors. Intelligent manufacturing systems—a volume from
IFAC workshop. Gramado-RS, Brazil: Pergamon Press; November 911, 1998. 2000.
p. 239.
[8] Pourboghrat F, Karlsson MP. Adaptive control of dynamic mobile robots with nonholo-
nomic constraints. Comput Electr Eng 2002;28:24153.
[9] Gholipour A, Dehghan SM, Ahmadabadi MN. Lyapunov-based tracking control of non-
holonomic mobile robot. Proceedings of 10th Iranian conference on electrical engineer-
ing. vol. 3. Tabriz, Iran; 2002. p. 26269.
267
Mobile Robot Control III: Adaptive and Robust Methods

[10] Dong W, Kuhnert KD. Robust adaptive control of nonholonomic mobile robot with
parameter and nonparameter uncertainties. IEEE Trans Robot 2005;21(2):2616.
[11] Zhang Y, Hong D, Chung JH, Velinsky S. Dynamic model based robust tracking con-
trol of a differentially steered wheeled mobile robot. Proceedings of the American con-
trol conference, Philadelphia, PA; June 1988, p. 85055.
[12] Yang JM, Kim JK. Sliding mode control for trajectory tracking of nonholonomic
wheeled mobile robots. IEEE Trans Robot Autom 1999;15(3):57887.
[13] Chwa D. Sliding-mode tracking control of nonholonomic wheeled mobile robots in
polar coordinates. IEEE Trans Control Syst Technol 2004;12(4):63744.
[14] Zhu X, Dong G, Hu D, Cai Z. Robust tracking control of wheeled mobile robots not
satisfying nonholonomic constraints. Proceedings of the 6th international conference on
intelligent systems design and applications (ISDA’06), 2006. p. 6438.
[15] Dixon WE, Dawson DM, Zergeroglu E, Zhang F. Robust tracking and regulation con-
trol for mobile robots. Int J Robust Nonlinear Control 2000;10:199216.
[16] Slotine JJ, Li W. Applied nonlinear control. Englewood Cliffs: Prentice Hall; 1991.
[17] Solea R, Filipescu A, Nunes U. Sliding mode control for trajectory tracking of a
wheeled mobile robot in presence of uncertainties. Proceedings of 17th Asian control
conference. Hong Kong, China; August 2729, 2009. p. 17016.
268
Introduction to Mobile Robot Control

8 Mobile Robot Control IV: Fuzzy and
Neural Methods
8.1
Introduction
Fuzzy logic systems (or, simply, fuzzy systems, FSs) and neural networks (NNs)
have found wide application in the identification, planning, and control of robotic
and other complex nonlinear technological systems. This is because FSs and NNs
are universal approximators, that is, they can approximate any nonlinear function
(mapping) with any desired accuracy. They are two of the three field components
of computational intelligence, with the third component being the field of genetic
or evolutionary algorithms (GA). The NN concept was coined by Mc Culloch and
Pitts in 1943 as a result of their study of the human brain cell, which they name
“neuron.” The big next step in NNs was done in 1949 by Hebb who coined the
concept of synaptic weights. Fuzzy logic or fuzzy set theory in its present form was
coined by the control scientist Lofti Zadeh in 1965, thus breaking down the classi-
cal two-valued (yes/no) Aristotelian logic. Genetic and evolutionary algorithms
were developed in the 1950s and 1960s, and in 1973, Rechenberg introduced the
so-called evolution strategies as a method of parameter optimization, using a popu-
lation with two atoms: a parent and an off spring. Holland has extended this con-
cept in 1975 to a multiple-member population introducing the operators of
crossover, inversion, and mutation.
Fuzzy logic offers a unified approximate (linguistic) way of drawing conclusions
from uncertain data using uncertain rules. NNs offer the possibility of learning and
training either autonomously (unsupervised learning) or nonautonomously (super-
vised learning) or via evaluation of their performance (reinforcement learning)
[17]. Pictorially, supervised learning (from a teacher), unsupervised learning
(without a teacher), and reinforcement learning (with a critic) are shown in
Figure 8.1.
In many practical cases (including mobile robots), we use combined neurofuzzy
systems (NFSs) that provide better performance. NFSs are distinguished as follows:
G
Cooperative NFS (the NN determines some sub-blocks of the FS, e.g., fuzzy rules, which
are then used without the presence of the NN) (Figure 8.2A).
G
Concurrent NFS (the NN and the FS work continuously together, where the NN prepro-
cesses the inputs, or postprocesses the outputs, of the FS) (Figure 8.2B).
G
Hybrid NFS (an FS that uses a heuristic learning algorithm, inspired by NN theory, to
determine its parameters, i.e., fuzzy sets and fuzzy rules, via the inputoutput patterns).
Introduction to Mobile Robot Control. DOI: http://dx.doi.org/10.1016/B978-0-12-417049-0.00008-0
© 2014 Elsevier Inc. All rights reserved.

The objectives of this chapter are the following:
G
To provide a brief introduction to neural networks and FSs
G
To derive and discuss the general structure of fuzzy and neural robot controllers
G
To provide the details of mobile (nonholonomic) fuzzy tracker controller design
G
To fuzzify the model-based sliding mode controller and apply it to mobile robots
G
To solve the mobile adaptive tracking controller design problem using MLP and RBF
neural networks.
System
Reinforcement
learning
NN
Environment
Output
NN
Input
NN
(C)
NN
(A)
(B)
Learning
Error
NN
+
−
Desired
Output
Learning
Teacher
Error
Input
Output
Input
Output
Figure 8.1 (A) Supervised learning NN, (B) unsupervised learning NN, and
(C) reinforcement learning NN.
Neural
network
Neural
network
Fuzzy
system
Fuzzy rules
Fuzzy sets
Fuzzy
system
Fuzzy
system
(A)
(B)
Neural
network
Figure 8.2 (A) Cooperative NFS, (B) concurrent neurofuzzy system.
270
Introduction to Mobile Robot Control

8.2
Background Concepts
8.2.1
Fuzzy Systems
8.2.1.1
Fuzzy Sets
Fuzzy sets constitute an extension of the classical concept of set which is one of
the foundations of the mathematics discipline. In a classical (or crisp) set X, only
one of the following is true:
An element x belongs to X or does not belong to X, symbolically xAX or x =2 X.
This dichotomy was broken by the fuzzy sets coined by Zadeh [1]. Let
X 5 fx1; x2; x3; x4; x5g be a classical set. The set X is called the reference superset
(universe of discourse). Now, let A 5 fx1; x3; x5g be a classical subset of X. An
equivalent representation of A is:
A 5 fðx1; 1Þ; ðx2; 0Þ; ðx3; 1Þ; ðx4; 0Þ; ðx5; 1Þg
which is an ordered set of pairs ðx; μAðxÞÞ, where x is the element of xAX of con-
cern and μAðxÞ is the membership of x in the subset A, where:
μAðxÞ 5
1
if
xAA
0
if
x =2 A


That is, here we have μA:A ! f0; 1g, where the set f0; 1g has two elements,
namely, 0 and 1. If we allow the membership function μAðxÞ to be:
μA:A ! ½0; 1
where ½0; 1 is the full closed interval between 0 and 1 (i.e., 0 # μAðxÞ # 1), then we
have the fuzzy subset A of X, defined as:
A 5 fðx; μAðxÞÞjxAX; μAðxÞ:X ! ½0; 1g
Another notation for the fuzzy set A is:
A 5 μAðx1Þ=x1 1 μAðx2Þ=x2 1 ? 1 μAðxnÞ=xn
where the symbol “ 1 ” represents union of points, and the symbol “/” does not rep-
resent division.
Example 8.1
A fuzzy set with discrete points is:
A1 5 fð7; 0:1Þ; ð8; 0:5Þ; ð9; 0:8Þ; ð10; 1Þ; ð11; 0:8Þ; ð12; 0:5Þ; ð13; 0:1Þg
5 0:1=7 1 0:5=8 1 0:8=9 1 1=10 1 0:8=11 1 0:5=12 1 0:1=13
271
Mobile Robot Control IV: Fuzzy and Neural Methods

A fuzzy set with continuous domain of elements x is:
A2 5 fðx; μAðxÞÞjxAX; μAðxÞ 5 1=½1 1 ðx210Þ2g
Fuzzy Set Operations
The three fundamental operations of fuzzy sets are defined as extensions of the
respective operations of classical sets, that is:
Intersection
C 5 A - B 5 fðx; μCðxÞÞjxAX; μCðxÞ 5 minðμAðxÞ; μBðxÞÞg
Union
D 5 A , B 5 fðx; μDðxÞÞjxAX; μDðxÞ 5 maxfμAðxÞ; μBðxÞgg
Complement
Ac 5 fðx; μAcðxÞÞjxAX; μAcðxÞ 5 1 2 μAðxÞg
It is easy to verify that the standard properties of sets hold also here (i.e., De
Morgan, absorption, associativity, distributivity, idempotency).
Fuzzy Set Image
The image fðAÞ of a fuzzy set A through the mapping (function) fðUÞ is the fuzzy set:
fðAÞ 5
X
Y
μAðxÞ=fðxÞ
For example, if y 5 fðx1Þ 5 fðx2Þ for x1 6¼ x2, we have:
μAðx1Þ=fðx1Þ 1 μAðx2Þ=fðx2Þ 5 maxfμAðx1Þ; μAðx2Þg=y
Fuzzy Inference
The fuzzy inference (or fuzzy reasoning) is an extension of the classical inference
based on the modus ponens and modus tollens rules. Thus, we have:
Fuzzy modus ponens
Rule: IF x 5 A, THEN y 5 B
Fact: x 5 A0
Inference: y 5 B0
where A, A0, B, and B0 are fuzzy sets.
Fuzzy modus tollens
Rule: IF x 5 A, THEN y 5 B
Fact: y 5 B0
Inference: x 5 A0
272
Introduction to Mobile Robot Control

Fuzzy Relations
Let X and Y be two reference supersets. Then, with the term fuzzy relation R, we
mean a fuzzy set in the Cartesian product:
X 3 Y 5 fðx; yÞ; xAX; yAYg
which has membership function μRðx; yÞ:
μR: X 3 Y ! ½0; 1
For each pair ðx; yÞ the membership function μRðx; yÞ represents the connection
degree between x and y.
Zadeh’s Max-Min Composition
On the basis of the above, we can formulate the rule of “max-min fuzzy composi-
tion” developed by Zadeh, which is as follows. Let the fuzzy sets A and B be:
A 5 fðx; μAðxÞÞjxAXg;
B 5 fðy; μBðyÞÞjyAYg
and a fuzzy relation upon X 3 Y, namely:
R 5 fððx; yÞ; μRðx; yÞÞjðx; yÞAX 3 Yg
Then, if A is the input to R, the membership function of the output set B is given
by the relation:
μBðyÞ 5 max
x fmin½μAðxÞ; μRðx; yÞg
ð8:1aÞ
or, symbolically:
B 5 A3R
ð8:1bÞ
where }3} denotes the max-min operation.
If we are given a fuzzy rule:
IF x is A, THEN y is B we can find the corresponding fuzzy relation Rðx; yÞ
using one of the following rules:
Mamdani’s rule (minimum)
μRðxi; yjÞ 5 minfμAðxiÞ; μBðyjÞg
ð8:2aÞ
Larsen’s rule (product)
μRðxi; yjÞ 5 μAðxiÞμBðyjÞ
ð8:2bÞ
273
Mobile Robot Control IV: Fuzzy and Neural Methods

Zadeh’s arithmetic rule
μRðxi; yjÞ 5 minf1; 1 2 μAðxiÞ 1 μBðyjÞg
ð8:2cÞ
Zadeh’s maximum rule
μRðxi; yjÞ 5 maxfmin½μAðxiÞ; μBðyjÞ; 1 2 μAðxiÞg
ð8:2dÞ
Example 8.2
Let X 5 Y 5 f1; 2; 3; 4g
A 5 ‘‘x small’’ 5 fð1; 1Þ; ð2; 0:6Þ; ð3; 0:2Þ; ð4; 0Þg
and
R 5 ‘‘x nearly equal to y’’ with fuzzy relation:
x=y
1
2
3
4
1
1
0.5
0
0
2
0.5
1
0.5
0
3
0
0.5
1
0.5
4
0
0
0.5
1
Then, the max-min rule B 5 A3R gives:
μBðyÞ 5 max
x fminfμAðxÞ; μRðx; yÞgg
5 fð1; 1Þ; ð2; 0:6Þ; ð3; 0:5Þ; ð4; 0:2Þg
Obviously, the result can be interpreted as the fuzzy set “x5nearly small.” Thus, in this
case, the “fuzzy modus ponens” rule gives:
IF “x is small” AND “x is nearly equal to y,” THEN “y is nearly small.”
8.2.1.2
FSs Structure
The general structure of an FS (or fuzzy decision algorithm) involves the following
four units (Figure 8.3) [27]:
1. A fuzzy rule base (FRB), that is, a base of IF-THEN rules
2. A fuzzy inference mechanism (FIM)
3. An input fuzzification unit (IFU)
4. An output defuzzification unit (ODU)
The FRB usually contains, besides the fuzzy or linguistic rules, a standard arith-
metic database section. The fuzzy rules are provided by human experts or are
derived through simulation. The IFU (fuzzifier) receives the nonfuzzy input values
274
Introduction to Mobile Robot Control

and converts them into fuzzy or linguistic form. The FIM is the core of the system
and involves the fuzzy inference logic (e.g., the max-min rule of Zadeh). Finally,
the ODU (defuzzifier) converts the fuzzy results provided by FIM to nonfuzzy form
using a defuzzification method.
The
fuzzifier
performs
a
mapping
from
the
set
of
real
input
values
x 5 ½x1; x2; . . .; xnAX to the fuzzy subset A of the superset X.
Two possible choices of this mapping are:
μAðx0Þ 5
1
if
x0 5 x
0
if
x0 6¼ x
Singleton fuzzifier
ð
Þ

μAðx0Þ 5 exp ðx02xÞTðx0 2 xÞ
σ2


Bell-type fuzzifier
ð
Þ
The two most popular defuzzification methods are the following:
1. Center of gravity (COG) method: The defuzzified value w0 is given by:
w0 5
X
i
wiμBðwiÞ
"
#, X
i
μBðwiÞ
"
#
ð8:3Þ
2. Mean of maxima (MOM) method: Here, the defuzzified output w0 is equal to:
w0 5
X
m
j51
wj
"
#,
m
ð8:4Þ
where wj is the value that corresponds to the j maximum of the membership function
μBðwÞ.
8.2.2
Neural Networks
Neural networks are large-scale systems that involve a large number of special type
nonlinear processors called neurons. Biological neurons are nerve cells which have
a number of internal parameters called synaptic weights. The human brain consists
of over 10 million neurons. The weights are adjusted adaptively according to the
IFU
FIM
ODU
Fuzzy Rule Base
Nonfuzzy
inputs
Nonfuzzy
outputs
Figure 8.3 General structure of an FS.
275
Mobile Robot Control IV: Fuzzy and Neural Methods

task under execution such that to improve the overall system performance. Here,
we will discuss artificial NNs, the neurons of which are characterized by a state, a
list of weighted inputs from other neurons, and a state equation governing their
dynamic operation. The NN weights can take new values through a learning pro-
cess which is accomplished by the minimization of a certain objective function
through the gradient or the NewtonRaphson algorithm. The optimal values of the
weights are stored as the strengths of the neurons’ interconnections. The NN
approach is suitable for systems or processes that cannot be modeled with concise
and accurate mathematical models; typical examples being machine vision, pattern
recognition, control systems, and human-based operations. The three primary fea-
tures of NNs are (i) utilization of large amounts of sensory information, (ii) collec-
tive processing capability, and (iii) learning and adaptation capability [4]. Learning
and control in neurocontrollers are achieved simultaneously, and learning continues
as long as perturbations are present in the plant under control and/or its environ-
ment. The practical implementation of NNs was made possible by the recent devel-
opments in fast parallel methods (VLS, electro-optical, and others). The two NNs
that are mostly suitable for decision and control purposes are the multilayer percep-
tron (MLP) and the radial basis functions (RBF) networks.
8.2.2.1
The Basic Artificial Neuron Model
This model is based on the McCullochPitts model which has the form shown in
Figure 8.4A. The neuron has a basic processing unit which consists of three
elements:
1. A set of connection branches (synapses)
2. A linear summation node
3. An activation (nonlinear) function
Each connection branch has a weight (strength) which is positive if it has an
excitatory role and negative if it has an inhibitory role. The summation node sums
the input signals multiplied by the respective synaptic weights. Finally, the activa-
tion function (or as otherwise called) squashing function limits the allowable ampli-
tude of the output signal to some finite value, typically in the normalized interval
[0, 1] or, alternatively, in the interval [21, 1]. The neuron model has also a thresh-
old θ which is applied externally, and practically lowers the net input to the activa-
tion function.
From Figure 8.4A, it follows that the neuron is described by the following
equations:
u 5
X
n
i51
wixi
y 5 σðzÞ
z 5 u 2 θ;
θ . 0
276
Introduction to Mobile Robot Control

If the threshold 2θ is regarded as a normal input x0 5 2 1 with corresponding
weight w0 5 θ, then the neuron model takes the form:
y 5 σðzÞ;
z 5
X
n
i50
wixi
ð8:5Þ
The nonlinear activation function σðxÞ can be of the on-off or the saturation
function type, or of the sigmoid function type with values either in the interval [0, 1]
or in the interval [21,1], as shown in Figure 8.4B.
The first sigmoid function is the logistic function:
y 5 σðzÞ 5
1
1 1 e2z ;
yA 0; 1
½

w2
Inputs
Output
y
u
z
y
y
y
Logistic
Hyperbolic tangent
z
z
x1
x0
x1
x2
xn
x2
xn
w1
wn
∑
σ (·)
+
+
+
Activation
function
Threshold
−
1
0
0
+1
–1
∑
σ (·)
(A)
(B)
Synaptic weights
+
+
+
+
w0
w1
w2
wn
Figure 8.4 (A) The basic artificial neuron model and (B) two forms of sigmoid functions.
277
Mobile Robot Control IV: Fuzzy and Neural Methods

and the second is the hyperbolic tangential function:
y 5 σðzÞ 5 tan z
2
 
5 1 2 e2z
1 1 e2z ;
yA 21; 1
½

8.2.2.2
The Multilayer Perceptron
The MLP NN has been developed by Rosemblat (1958) and has the structure
shown in Figure 8.5. It involves the input layer of nodes, the output layer of nodes,
and a number of intermediate (hidden) layers of nodes. It is noted that even one
only hidden node layer is sufficient for the MLP NN to perform operations that can
be achieved by many hidden layers. As we will see later, this comes from the uni-
versal approximation theorem of Kolmogorov. In this case, the output of the NN
(with m output neurons and L neurons in the hidden layer) is given by the relation:
yi 5
X
L
j51
vijσ
X
n
k50
wjkxk
 
!
"
#
;
i 5 1; 2; . . .; m
ð8:6Þ
where xkðk 5 0; 1; 2; . . .; nÞ are the NN inputs (including the thresholds), wjk are the
input-to-hidden-layer interconnection weights, and vij are the hidden-to-output-
layer interconnection weights.
In compact form, Eq. (8.6) can be written as:
y 5 VTσðWTxÞ
ð8:7Þ
where:
x 5 ½x0; x1; . . .; xnT;
y 5 ½y1; y2; . . .; ymT
Input
layer
Hidden
layer
Output
layer
Figure 8.5 A single-hidden layer MLP with
eight input nodes, four hidden nodes, and two
output nodes.
278
Introduction to Mobile Robot Control

VT 5
vT
1
vT
2
^
vT
m
2
666664
3
777775
;
σðWTxÞ 5
σðwT
1xÞ
σðwT
2xÞ
^
σðwT
LxÞ
2
666664
3
777775
vT
i 5 ½vi1; vi2; . . .; viL;
wT
i 5 ½wi0; wi1; . . .; win
8.2.2.3
The Backpropagation Algorithm
The backpropagation (BP) algorithm is a supervisory learning algorithm which
updates (adapts) the synaptic weights such that to minimize the mean square error
between the desired and actual outputs after the presentation of each input vector
(pattern) at the NN input layer. The errors are propagated via the hidden layers for
the computation of the weight corrections in the same way as is done in the output
layer.
Thus, the weights are updated so as to minimize the criterion:
EpðtÞ 5 1
2
X
k
e2
kðtÞ;
ekðtÞ 5 dkðtÞ 2 ykðtÞ
where:
dkðtÞ is the desired output (response) of the kth output neuron at time t.
ykðtÞ is the output neuron actually obtained after the presentation of each vector pattern x
at the input layer at time t.
This means that the minimization of EpðtÞ must be done consecutively pattern
by pattern. In the above criterion function, EpðtÞ, ykðtÞ, and ekðtÞ are given by:
ykðtÞ 5 σðzkðtÞÞ;
ekðtÞ 5 dkðtÞ 2 σkðzkðtÞÞ;
zk 5
X
n
i50
wkiðtÞyiðtÞ
The minimization of EpðtÞ can be done by the well-known gradient (steepest
descent) rule:
ΔwkiðtÞ 5 2 γ @EpðtÞ
@wkiðtÞ
for the p input pattern, where γ is the learning parameter.
Here:
@EpðtÞ
@wkiðtÞ 5 @EpðtÞ
@ekðtÞU @ekðtÞ
@ykðtÞU @ykðtÞ
@zkðtÞU @zkðtÞ
@wkiðtÞ
5 ekðtÞð2 1Þ dσðzkðtÞÞ
dzkðtÞ yiðtÞ
279
Mobile Robot Control IV: Fuzzy and Neural Methods

where, for the logistic function σðÞ:
dσðzkðtÞÞ
dzkðtÞ
5 d
dzk
1
1 1 e2zk


5 2 ð2e2zkÞ
ð11e2zkÞ2 5 ykðtÞ 1 2 ykðtÞ
½

Thus:
ΔwkiðtÞ 5 γ½dkðtÞ 2 ykðtÞyk½1 2 ykðtÞyiðtÞ 5 γδkðtÞyiðtÞ
ð8:8aÞ
where:
δkðtÞ 5 ½dkðtÞ 2 ykðtÞ½1 2 ykðtÞyk
ð8:8bÞ
is the so-called delta of the BP algorithm. The index k extends over all the output
neurons, and the index i over all the neurons of the last hidden layer.
The learning (weight updating) rule (8.8a) for the hidden layers takes the form:
ΔwjiðtÞ 5 γyiðtÞyjðtÞ½1 2 yjðtÞ
X
m
δmðtÞwmjðtÞ
ΔwjiðtÞ 5 wjiðt 1 1Þ 2 wjiðtÞ
ð8:9Þ
where the index i refers to the neurons of the layer that lies behind the
considered layer. To accelerate the convergence we add a “momentum” term
a½wjiðtÞ 2 wjiðt 2 1Þ, where a is a parameter in the interval 0 # a # 1.
On the basis of the above, the BP learning algorithm involves the following steps:
Step 1: Select the initial weights and thresholds using small positive random values.
Step 2: Present the training input pattern vector xðtÞ and the desired output vector dðtÞ:
xðtÞ 5 ½x0ðtÞ; x1ðtÞ; . . .; xnðtÞT
dðtÞ 5 ½d1ðtÞ; d2ðtÞ; . . .; dmðtÞT
Step 3: Compute the actual outputs of all the neurons of the NN neuron-to-neuron in the
forward direction using the current values of the synaptic weights, that is:
yiðtÞ 5 σjðzjðtÞÞ;
zjðtÞ 5
X
i
wjiðtÞyiðtÞ
where yiðtÞ (the output of the ith neuron) is the ith input to the jth neuron and wji is
the synaptic weight that connects the ith neuron with the jth neuron. For the neurons j of
the first hidden layer, we have:
yiðtÞ 5 xiðtÞ;
i 5 1; 2; 3; . . .; n
where xiðtÞ is the ith component of the input pattern vector xðtÞ.
280
Introduction to Mobile Robot Control

Step 4: Update the synaptic weights starting from the output neurons and going backward
toward the input layer, using the rule:
wjiðt 1 1Þ 5 wjiðtÞ 1 γδjðtÞyiðtÞ 1 a½wjiðtÞ 2 wjiðt 2 1Þ
Step 5: Repeat the procedure from step 2 (until a desired accuracy is obtained or a maxi-
mum number of iterations is reached).
In the literature, there are available many variations or improvements of the
above basic BP algorithm with faster convergence and acceptable computational
effort.
8.2.2.4
The RBF Network
An RBF network approximates an inputoutput mapping by employing a linear
combination of radially symmetric functions (Figure 8.6). The kth output yk is
given by:
ykðxÞ 5
X
m
i51
wkiφiðxÞ
ðk 5 1; 2; . . .; pÞ
ð8:10Þ
where:
φiðxÞ 5 φðjjx 2 cijjÞ 5 φðriÞ 5 exp 2 r2
i
2σ2
i

	
;
ri $ 0; σi $ 0
ð8:11Þ
The RBF networks have always one hidden layer of computational nodes with
nonmonotonic transfer functions φðÞ. Theoretical studies have shown that the
choice of φðÞ is not very crucial for the effectiveness of the network. In most
cases,
the
Gaussian
RBF
given
by
Eq.
(8.11)
is
used,
where
ci
and
σiði 5 1; 2; . . .; mÞ are selected centers and widths, respectively. The training proce-
dure of the RBF network involves the following steps:
Step 1: Group the training patterns in M subsets using some clustering algorithm (e.g.,
the k-means clustering algorithm) and select their centers ci.
Input
layer
Hidden (RBF)
layer
Output
layer
x1
x2
xm
xmp
y1
w11
φ1
φ2
φm
y2
yp
Figure 8.6 The RBF network.
281
Mobile Robot Control IV: Fuzzy and Neural Methods

Step 2: Select the widths, σiði 5 1; 2; . . .; mÞ, using some heuristic method (e.g., the p
nearest-neighbor algorithm).
Step 3: Compute the RBF activation functions, φiðxÞ, for the training inputs using
Eq. (8.11).
Step 4: Compute the weights by least squares. To this end, write Eq. (8.10) as
bk 5 Awk ðk 5 1; 2; . . .; pÞ and solve for wk, that is:
wk 5 Aybk;
wk 5 ½wk1; . . .; wkmT
where Ay is the generalized inverse of A given by:
Ay 5 ðATAÞ21AT
and bk is the vector of the training values for the output k.
It is remarked that MLP NNs perform global matching to the inputoutput data,
whereas in RBF NNs, this is done only locally, of course with better accuracy.
8.2.2.5
The Universal Approximation Property
Neural Network Approximator
The MLP neural network with (at least) one hidden layer has the universal approxi-
mation property.1 This follows from the Kolmogorov theorem which states [4]:
Given any continuous function:
F: ½0; 1n ! FðxÞ 5 y; yARm
where I 5 ½0; 1n is the n-dimensional unit cube, F can be approximated (realized)
exactly with a perceptron NN of three layers, n nodes in the input (source) layer
fx 5 ½x1; x2; . . .; xnTg, L 5 2n 1 1 nodes in the middle (hidden) layer, and m nodes
in the output layer fy 5 ½y1; y2; . . .; ymTg.
The nonlinear function fðzÞ used in the nodes can be selected so as to satisfy the
Lipschitz condition jfðz1Þ 2 fðz2Þj # cjz1 2 z2jμ for 0 , μ , 1, where c is a constant.
Any sigmoid type function fðzÞ 5 σðzÞ or RBF fðzÞ 5 φðzÞ satisfies the Lipschitz
condition.
In practice, the NN representation of y 5 FðxÞ, after a finite number of steps of
weight updating (using the BP learning algorithm), is approximate, involving an
error ε, that is:
VTσðWTxÞ 5 FðxÞ 2 ε
ð8:12Þ
1 By definition, an MLP has a nonconstant, bounded, and monotonically increasing continuous activation
function.
282
Introduction to Mobile Robot Control

for some number L of hidden neurons. Actually, for any positive number ε0, the
weights can be trained (e.g., by BP), and the number L of hidden neurons can be
selected such that:
jjεjj , ε0
for all x 5 I. The selection of L $ 2n 1 1 can be done in several ways, but it is still
an open problem. Depending on the rate of change of FðxÞ, one can determine the
smallest L that assures a described accuracy ε0.
Fuzzy Logic Universal Approximator
Consider a multi-input single-output (MISO) fuzzy logic system with singleton fuz-
zifier, COG defuzzifier, and Gaussian-type membership functions:
μAj
iðxiÞ 5 ρj
iexp 21
2
xi2xj
i
σj
 
!2
2
4
3
5
ð8:13Þ
where xi is the ith component of the fuzzy input vector x 5 ½x1; x2; . . .; xnT, and ρj
i,
xj
i, and σj
i are real-valued parameters with 0 # ρj
i # 1. The system consists of m fuzzy
rules Rjðj 5 1; 2; . . .; mÞ of the form Rj: IF x1 is Aj
1 and x2 is Aj
2 and . . . and xn is Aj
n,
THEN y is Bj, where xiði 5 1; 2; . . .; nÞ and y are fuzzy (linguistic) variables repre-
sented by fuzzy membership functions μAj
iðxiÞ and μBjðyÞ, respectively, and yj is the
point in the output space y at which μBjðyÞ takes its maximum value. If the member-
ship functions of xi are given by Eq. (8.13), then the output FðxÞ of the system can
be written as:
FðxÞ 5
Pm
j51 yj
iðLm
i51μAj
iðxiÞÞ
Pm
j51ðLm
i51μAj
iðxiÞÞ
ð8:14Þ
Indeed, consider fuzzy basis functions (FBFs) of the form:
ψjðxÞ 5
Ln
i51μAj
iðxiÞ
Pm
j51 Ln
i51μAj
iðxiÞ
ð8:15Þ
where μAj
iðxiÞ are the Gaussian functions (8.13). Then, the FS (8.14) can be written
as an FBF expansion with yj
i as a free coefficient of the form:
FðxÞ 5
X
m
j51
yj
iψjðxÞ 5 ψTðxÞβ
ð8:16Þ
283
Mobile Robot Control IV: Fuzzy and Neural Methods

where:
ψðxÞ 5 ½ψ1ðxÞ; . . .; ψmðxÞT
is the fuzzy basis vector, and
β 5 ½y1; y2; . . .; ymT
is the parameter vector to be estimated. The output of a vector-valued MIMO func-
tion (FS) FðxÞ is expressed as:
FðxÞ 5 ΨðxÞβ
ð8:17Þ
where:
FðxÞ 5
F1ðxÞ
^
FpðxÞ
2
664
3
775;
ΨðxÞ 5
ψ1ðxÞ
^
ψpðnÞ
2
664
3
775 5
ψ11ðxÞ
?
ψ1mðxÞ
^
&
^
ψp1ðxÞ
?
ψpmðxÞ
2
664
3
775
It is shown in the literature [8] that Eq. (8.14) (which is actually an adaptive
fuzzy system) can uniformly approximate any real continuous function over a
compact input set to any desired accuracy. Thus, Eq. (8.14) is a fuzzy universal
approximator.
8.3
Fuzzy and Neural Robot Control: General Issues
8.3.1
Fuzzy Robot Control
The basic structure of a fuzzy robot control system is built using the FS of
Figure 8.3, and has the form shown in Figure 8.7 [6].
Here, the FRB stores all the relevant knowledge (i.e., how to control the system)
which eliminates the need to have available an analytical mathematical model of
the robot. Two practical implementations of the basic control loop are shown in
Figure 8.8A and B.
In the simple structure of Figure 8.8A, the input to the robot consists of delayed
measured values of the control input and the output of the modeled robot. Thus, if
fðÞ is the unknown nonlinear mapping that describes the unknown dynamics of the
robot, and:
zðkÞ 5 ½yðk21Þ; yðk22Þ; . . .; yðk2nyÞ; uðk21Þ; . . .; uðk2nuÞT
284
Introduction to Mobile Robot Control

is the information vector, then the training data are generated (modeled) by using
the relation:
yðkÞ 5 fðzðkÞÞ
Then, a fuzzy model of the robot has the form:
^yðkÞ 5 ^fðzðkÞÞ
where ^fðÞ is the fuzzy approximation (description) of fðÞ. Under the assumption
that the measurements are correct (accurate), the delay degrees ny and nu have been
estimated correctly, and that the control signal acts persistently, the fuzzy model
FIM
ODU
Fuzzy Rule Base
Control
signal
Robot
x(t)
u(t)
y(t)
Fuzzy controller
Input
Output
IFU
Figure 8.7 Basic fuzzy robot control loop.
Fuzzy
system
Fuzzy
system
Modeled
system (robot)
−
+
u(k)
e(k)
e(k)
y(k)
yˆ (k)
y(k)
yˆ (k)
u(k)
−
+
(A)
(B)
Modeled
system (robot)
Figure 8.8 Two implementations of fuzzy robot control: (A) use of delayed inputoutput
values and (B) use of output feedback.
285
Mobile Robot Control IV: Fuzzy and Neural Methods

can approximate the given system satisfactorily. But even if the above conditions
are not satisfied, the FS can track the output asymptotically, that is:
lim
k!N eðkÞ 5 0
where eðkÞ 5 yðkÞ 2 ^yðkÞ.
When the measured output is contaminated by noise, the result of the system of
Figure 8.8A may by erroneous, because the inputs are disturbed, and one cannot
determine whether some output error is due to parametric uncertainty or input
error. To overcome this difficulty, we feedback the approximate output ^yðkÞ to the
input (instead of the measured output yðkÞ) as is shown in Figure 8.8B. Thus, we
have now the fuzzy model:
^yðkÞ 5 ^fðz0ðkÞÞ
where:
z0ðkÞ 5 ½^yðk21Þ; ^yðk22Þ; . . .; ^yðk2nyÞ; uðk21Þ; . . .; uðk2nuÞT
Of course, here it is tacitly assumed that ^yðkÞ is very close to yðkÞ. In both cases,
the success of control depends on the actual structure of the FS (i.e., the type of
fuzzy rules, the form of membership functions, and the defuzzification method).
Example 8.3
In this example, we demonstrate how to design a robot fuzzy PD controller which receives
the inputs eðkÞ and ΔeðkÞ. The real control signal is:
uðkÞ 5 uðk 2 1Þ 1 ΔuðkÞ
The fuzzy rules produce the incremental control signal ΔuðkÞ and are described in
Table 8.1.
Table 8.1 Fuzzy Rule base of the Fuzzy Controller PD
ΔeðkÞ
e(k)
NB
NM
NS
AZ
PS
PM
PB
NB
AZ
PS
PM
PB
PB
PB
PB
NM
NS
AZ
PS
PM
PB
PB
PB
NS
NM
NS
AZ
PS
PM
PB
PB
AZ
NB
NM
NS
AZ
PS
PM
PB
PS
NB
NB
NM
NS
AZ
PS
PM
PM
NB
NB
NB
NM
NS
AZ
PS
PB
NB
NB
NB
NB
NM
NS
AZ
286
Introduction to Mobile Robot Control

Here, the symbols of the table for eðkÞ, ΔeðkÞ, and uðkÞ have the following meaning:
NB, negative big; NS, negative small; NM, negative medium; AZ, almost zero; PB, positive
big; PS, positive small; PM, positive medium.
These rules are pictorially illustrated in Figure 8.9.
Table 8.1 involves 7 3 7 5 49 rules, that is:
R1: IF eðkÞ 5 NB AND ΔeðkÞ 5 NB, THEN ΔuðkÞ 5 AZ.
R2: IF eðkÞ 5 NB AND ΔeðkÞ 5 NM, THEN ΔuðkÞ 5 NS
. . .
R49: IF eðkÞ 5 PB AND ΔeðkÞ 5 PB, THEN ΔuðkÞ 5 AZ.
As we know, the tuning of conventional PD controllers is made by selecting the gains
Kp and Kυ. But in fuzzy PD controllers, the tuning is done by selecting the position of the
membership functions. The resulting form of the control action is an interpolation
between several types of control for each subdivision of the input space. The performance
of a fuzzy PD controller is illustrated pictorially in the phase plane, with axes eðkÞ and
ΔeðkÞ. In Fig 8.10, the symbol “” denotes normalized values in the interval [21,1].
The various regions of the phase plane correspond to positions in Table 8.1. Typically,
the desired performance of a fuzzy controller is the one shown in Figure 8.10, that is,
(eðkÞ, ΔeðkÞ) converges asymptotically to the equilibrium point (0,0).
NB
NM
NS
AZ
PS
PM
PB
NB
NM
NS
AZ
PS
PM
PB
μe(k)
e(k)
Δe(k)
μΔe(k)
Figure 8.9 Fuzzy values (membership
functions) of eðkÞ and ΔeðkÞ of the fuzzy PI
controller.
–1
–1
1
1
Δe(k)
e(k)
Figure 8.10 Trajectory of the error dynamics
state ½eðkÞ; ΔeðkÞ of the fuzzy PI controller.
287
Mobile Robot Control IV: Fuzzy and Neural Methods

8.3.2
Neural Robot Control
Neural control uses “well-defined” neural networks for the generation of desired
control signals. Neural Networks have the ability to learn and generalize, from
examples, nonlinear mappings (i.e., they are universal approximators), and so they
are suitable for solving complex nonlinear control systems, like robotic systems,
with high speed [5,9].
Neural control can be classified in the same way as NNs, that is:
G
Neural control with supervised learning
G
Neural control with unsupervised learning
G
Neural control with reinforcement learning
In each case, the proper NN should be used. Here, the case of neural control
with supervised learning, which is very popular for its simplicity, will be consid-
ered. The structure of supervised learning neurocontrol is as shown in Figure 8.11.
The teacher trains the neurocontroller via the presentation of control signal
examples that can control the robot successfully. The teacher can be either a human
controller or any classical, adaptive, or intelligent technological controller. The out-
puts or states are measured and sent to the teacher as well as to the neurocontroller.
During the control period by the teacher, the control signal and outputs/states of
the robotic system are sampled and stored for the training of the neural network.
After the training period, the neurocontroller takes the control actions, and the
teacher is deconnected from the system.
The most popular type of supervised neurocontrol is the direct inverse neurocon-
trol in which the NN learns successfully the robot inverse dynamics and is used
directly as controller as shown in Figure 8.12A [9].
Another variation of the direct inverse neurocontroller is shown in Figure 8.12B
which is called specialized direct inverse neurocontrol. Here, the NN is trained
on-line and the error e of the closed-loop system is transmitted backward at each
sampling instant. On the contrary, in the direct inverse neurocontrol scheme of
Figure 8.12A, the training of the NN is performed “off-line.”
Another neurocontrol scheme is the so-called indirect neurocontrol which uses
two NNs as shown in Figure 8.13 [9].
Robot
Output
or state
Neuro
Controller
Input
+
−
Teaching
Controller
Figure 8.11 Structure of neurocontrolled robot with supervised learning.
288
Introduction to Mobile Robot Control

The first NN is used as simulator of the robot, and the second NN as controller.
The simulation NN can be trained either off-line (batch learning) or on-line using
random inputs for the learning of the robot dynamics. All the above types of neuro-
control use NNs of the MLP type with BP learning.
The most general type of neurocontrol involves two NNs: the first is used as
feedforward controller (FFC) and the second as feedback controller (FBC). The
structure of this control scheme which is known as feedback error learning neuro-
controller is shown in Figure 8.14.
Output
Input
Robot
u
e
x
u
e
y
+
−
(A)
Delayed
signals
Delayed
signals
Updating of weights
+
−
(B)
Robot
Figure 8.12 (A) Direct inverse neurocontroller and (B) specialized direct inverse
neurocontroller.
+
−
Robot
+
−
Neuro
controller
Reference
signal
e
u
y
y
e
r
Delayed
signals
Delayed
signals
Output
Figure 8.13 Indirect robot neurocontrol (use of an adaptive NN simulator and a NN
controller).
289
Mobile Robot Control IV: Fuzzy and Neural Methods

8.4
Fuzzy Control of Mobile Robots
8.4.1
Adaptive Fuzzy Tracking Controller
Here, a direct fuzzy tracking control scheme will be discussed based on Figure 8.7
[10,11]. A decentralized fuzzy logic control (FLC) scheme for multiple wheeled
mobile robots (WMRs) is presented in [12]. The robot kinematic and dynamic
model that has been derived in Section 5.4.1 and Example 7.1 will be considered.
For this system, we have derived the two-step (backstepping) controller. The kine-
matic controller is (see Eqs. (7.73a) and (7.73b)):
vm 5 vd cos ε3 1 K1ε1
ωm 5 ωd 1 K2vdε2 1 K3 sin ε3
ð8:18Þ
The problem to be discussed here is to find a control input u 5 ½u1; u2T 5 ½τa; τbT
(see Eqs. (5.49a) and (5.49b)) which stabilizes to zero the errors: ~v 5 v 2 vm,
~ω 5 ω 2 ωm (see Eq. (7.71)) where the trajectory error:
~x 5 xd 2 x;
x 5 ½x; y; φT;
xd 5 ½xd; yd; φdT
is described by Eqs. (7.69) and (7.70):
ε 5
ε1
ε2
ε3
2
64
3
75 5
cos φ
sin φ
0
2sin φ
cos φ
0
0
0
1
2
64
3
75
~x
~y
~φ
2
64
3
75 5 Eð~xÞ~x
ð8:19aÞ
where:
_ε1 5 ωε2 2 v 1 vd cos ε3;
_ε2 5 2 ωε1 1 vd sin ε3;
_ε3 5 ωd 2 ω
ð8:19bÞ
A simple design method is to use the Mamdani rule given by Eq. (8.2a). Here, we
have two control inputs u1; u2 and two outputs v and ω. The fuzzy rules represent the
mapping from ~v and ~ω to the motor torques u1 and u2. For convenience, the reference
superset (universe of discourse) is taken to be the interval [21,1] for all fuzzy
e
u
r
Robot
FFC
FBC
+
+
+
Reference
signal
Error
Control
input
−
y
Output
Figure 8.14 General structure of
FFC-FBC (feedback error
learning) neurocontrol.
290
Introduction to Mobile Robot Control

variables. The membership functions of these variables are selected to have a triangu-
lar/trapezoidal form with three functions each, as shown in Figure 8.15 [11].
Clearly, we have the FRB given in Table 8.2, where the following notation is
used for the linguistic values of ~v; ~ω; u1, and u2: N, negative; Z, zero; P, positive.
The entries Table 8.2 represent the fuzzy values of the pairs ðu1; u2Þ with corre-
sponding fuzzy values (N,N), (N,Z), (N,P), and so on.
In linguistic form, the nine ð3 3 3Þ fuzzy rules of the rule base of the FLC are
the following:
Rule 1: IF ~v is Z and ~ω is Z, THEN u1 is Z and u2 is Z.
Rule 2: IF ~v is Z and ~ω is P, THEN u1 is Z and u2 is P.
Rule 3: IF ~v is Z and ~ω is N, THEN u1 is Z and u2 is N.
Rule 4: IF ~v is P and ~ω is P, THEN u1 is P and u2 is P.
Rule 5: IF ~v is P and ~ω is N, THEN u1 is P and u2 is N.
Rule 6: IF ~v is P and ~ω is Z, THEN u1 is P and u2 is Z.
Rule 7: IF ~v is N and ~ω is N, THEN u1 is N and u2 is N.
Rule 8: IF ~v is N and ~ω is P, THEN u1 is N and u2 is P.
Rule 9: IF ~v is N and ~ω is Z, THEN u1 is N and u2 is Z.
The convenient deffuzification method is the COG/centroid of area (COA)
given by Eq. (8.3). Clearly, the above controller is a fuzzy proportional controller.
u1
u2
−0.5
0.5
0
1
0
1
(A)
(B)
μν
−0.5
0.5
0
1
0
1
μω
−0.5
0.5
0
1
0
1
μ1
−0.5
0.5
0
1
0
1
μ2
ν
ω
∼
∼
Figure 8.15 Membership functions μðxÞ. (A) Variables ~v and ~ω and (B) variables u1; u2.
Table 8.2 Fuzzy Rule Base in Table form
~v
~ω
N
Z
P
N
(N,N)
(N,Z)
(N,P)
Z
(Z,N)
(Z,Z)
(Z,P)
P
(P,N)
(P,Z)
(P,P)
291
Mobile Robot Control IV: Fuzzy and Neural Methods

The overall feedback tracking control system has the structure of Figure 5.11 with
the dynamic controller ((5.59a) and (5.59b)) being replaced by the FLC designed
on the basis of the rule base given in Table 8.2. The FLC accepts crisp values
which are fuzzified in the IFU unit and provides crisp values for the robot inputs
(torques) after the deffuzification process by the ODU unit (Figure 8.7). Therefore,
the block diagram of the two-step controller, namely, the crisp kinematic controller
Eq. (8.18) and the nine-rule FLC has the form of Figure 8.16 [11].
Some remarks about this control scheme are the following:
G
The FLC performs, implicitly through the fuzzy rules, the robot dynamics identification,
thus replacing both the deterministic dynamic controller of Section 5.4.2 (Eqs. (5.59a)
and (5.59b)) and the adaptive controller of Section 7.3.1 which uses the parameter adapta-
tion laws (7.40a), (7.40b), and (7.41).
G
The kinematic controller remains a crisp controller. This does not reduce the applicability
of the control scheme, since the kinematic controller (8.18) does not involve any
unknown (or potentially unknown) parameter. All variables are known or measured.
G
A better performance can be obtained if, instead of the present proportional fuzzy control-
ler, we use a PD fuzzy controller such as the controller designed in Example 8.3.
Example 8.4
It is desired to describe fully the computation of the fuzzy output of a Mamdani fuzzy con-
troller with inputs the error eðtÞ and the change ΔeðtÞ of the error, with three linguistic
values N, Z, P that are represented by triangular and trapezoidal membership functions.
Solution
A Mamdani-type fuzzy PD controller is described by the rule base (matrix) of Table 8.3,
where the elements represent the changes Δu of the control signal u.
The linguistic values N (negative), Z (zero), and P (positive) for e, Δe, and Δu are as
shown in Figure 8.17 with universes of discourse:
X1 5 feg 5 ½23; 3;
X2 5 fΔeg 5 ½21; 1;
Y 5 fΔug 5 ½26; 6
We will compute the control action Δu when:
fe; Δeg 5 f22:5; 0:5g
u1
u2
Robot 
Dynamics
Robot 
Kinematics
Fuzzy 
Controller
Kinematics 
Controller
−
+
−−+
+
ν
νm
ε
ωm
ν
ω
∫
∫
ω
.
ω
∼
E(x)
x
.
x
x
xd
•
ν∼
Figure 8.16 Structure of overall adaptive fuzzy tracking WMR controller.
292
Introduction to Mobile Robot Control

Since we have a 3 3 3 FRB table, the controller consists of nine rules ði 5 1; 2; . . .; 9Þ of
the form:
Ri: IF e 5 Ai1 AND Δe 5 Ai2, THEN Δu 5 Bi.
where:
R1: A11 5 negative e, A12 5 negative Δe, B1 5 negative Δu;
R2: A21 5 negative e, A22 5 zero Δe, B2 5 negative Δu;
R3: A31 5 negative e, A32 5 positive Δe, B3 5 zero Δu;
R4: A41 5 zero e, A42 5 negative Δe, B4 5 negative Δu;
Table 8.3 Fuzzy PD Controller Rule Base
Δe
e
N
Z
P
N
N
N
Z
Z
N
Z
P
P
Z
P
P
0
−1
−2
−3
3
x1
x2
x3
2
1
Negative e
Positive e
Zero e
0.5
0.5
1
(A)
0
−0.5
−1
1
0.5
1
(B)
0
−3
−6
6
3
Negative Δu
Positive Δu
Zero Δu
1
(C)
−4.5
−1.5
1.5
4.5
e
Negative Δe
Positive Δe
Zero Δe
Δu
Δe
0.5
Figure 8.17 Fuzzy sets (membership functions) of
e(A), Δe (B), and Δu(C).
293
Mobile Robot Control IV: Fuzzy and Neural Methods

R5: A51 5 zero e, A52 5 zero Δe, B5 5 zero Δu;
R6: A61 5 zero e, A62 5 positive Δe, B6 5 positive Δu;
R7: A71 5 positive e, A72 5 negative Δe, B7 5 zero Δu;
R8: A81 5 positive e, A82 5 zero Δe, B8 5 positive Δu;
R9: A91 5 positive e, A92 5 positive Δe, B9 5 positive Δu.
Using uniform partitioning of the universe of discourse Y 5 ½26; 6 as shown in
Figure 8.17, we have:
Y 5 ½26; 24:5; 23; 21:5; 0; 1:5; 3; 4:5; 6
Therefore, the membership functions of the output fuzzy sets are:
B1ðYÞ 5 ½ 1
1
1
0:5
0
0
0
0
0 
B2ðYÞ 5 ½ 1
1
1
0:5
0
0
0
0
0 
B3ðYÞ 5 ½ 0
0
0
0:5
1
0:5
0
0
0 
B4ðYÞ 5 ½ 1
1
1
0:5
0
0
0
0
0 
B5ðYÞ 5 ½ 0
0
0
0:5
1
0:5
0
0
0 
B6ðYÞ 5 ½ 0
0
0
0
0
0:5
1
1
1 
B7ðYÞ 5 ½ 0
0
0
0:5
1
0:5
0
0
0 
B8ðYÞ 5 ½ 0
0
0
0
0
0:5
1
1
1 
B9ðYÞ 5 ½ 0
0
0
0
0
0:5
1
1
1 
Since the rules are of the Mamdani type, we use the operator “min” for the AND operations
on the left-hand side of the rules, and so the membership function μi 5 μRiðx1; x2Þ of the
left-hand side of rule Ri is equal to:
μi 5 minfμAi1ðx1Þ; μAi2ðx2Þg
Then, the output (conclusion) of this rule is computed by:
μΔu;iðyÞ 5 fminðμi; Bi1ðyÞÞ; . . .; minðμi; B19ðyÞÞg
5 fmin½μAi1ðx1Þ; μAi2ðx2Þ; Bi1ðyÞ; . . .; min½μAi1ðx1Þ; μAi2ðx2Þ; Bi9ðyÞg
From the data, we find:
fx1; x2g 5 feðkÞ; ΔeðkÞg 5 f22:5; 0:5g
and so:
1. μA11ð22:5Þ 5 1;
μA12ð0:5Þ 5 0
μΔu;1ðyÞ 5 ½ 0
0
0
0
0
0
0
0
0 
294
Introduction to Mobile Robot Control

2. μA21ð22:5Þ 5 1;
μA22ð0:5Þ 5 0
μΔu;2ðyÞ 5 ½ 0
0
0
0
0
0
0
0
0 
3. μA31ð22:5Þ 5 1;
μA32ð0:5Þ 5 1
μΔu;3ðyÞ 5 ½ 0
0
0
0:5
1
0:5
0
0
0 
4. μA41ð22:5Þ 5 1;
μA42ð0:5Þ 5 0
μΔu;4ðyÞ 5 ½ 0
0
0
0
0
0
0
0
0 
5. μA51ð22:5Þ 5 1;
μA52ð0:5Þ 5 0
μΔu;5ðyÞ 5 ½ 0
0
0
0
0
0
0
0
0 
6. μA61ð22:5Þ 5 1;
μA62ð0:5Þ 5 1
μΔu;6ðyÞ 5 ½ 0
0
0
0
0
0
0
0
0 
7. μA71ð22:5Þ 5 0;
μA72ð0:5Þ 5 0
μΔu;7ðyÞ 5 ½ 0
0
0
0
0
0
0
0
0 
8. μA81ð22:5Þ 5 1;
μA82ð0:5Þ 5 0
μΔu;8ðyÞ 5 ½ 0
0
0
0
0
0
0
0
0 
9. μA91ð22:5Þ 5 0;
μA92ð0:5Þ 5 0
μΔu;9ðyÞ 5 ½ 0
0
0
0
0
0
0
0
0 
Consequently, the total membership function which is equal with the “max” of μΔu;iðyÞ
(union of fuzzy sets, i 5 1; 2; . . .; 9) is:
μΔuðyÞ 5 ½ 0
0
0
0:5
1
0:5
0
0
0 
Applying COG/COA defuzzification we find:
Δu 5 0:5 3 ð21:5Þ 1 1 3 ð0:0Þ 1 0:5 3 ð11:5Þ
0:5 1 1 1 0:5
5 0
Exercise:
Repeat
the
above
computation
for
fe; Δeg 5 f0:9; 0:2g
and
fe; Δeg 5 f0:75; 0:75g.
295
Mobile Robot Control IV: Fuzzy and Neural Methods

8.4.2
Fuzzy Local Path Tracker for Dubins Car
8.4.2.1
The Problem
The kinematic model of Dubins car is found from the standard car-like model
(2.52) by omitting the equation for the steering angle velocity _ψ, that is (see
Figure 2.9):
_x
_y
_φ
2
64
3
75 5
cos φ
sin φ
ðtg ψÞ=D
2
64
3
75v1
ð8:20Þ
where, for notational simplicity, the index Q was dropped from _xQ and _yQ (since
here it is not needed). Clearly, the Dubins car is a four-wheeled WMR under
bounded curvature constraints, with drift, and forward motion. These constraints
imply that the turning radius of the mobile robot is bounded (just like actual cars)
and that without any input, the robot remains still [13]. Here, the following discre-
tized form of the model (8.20), will be used:2
Δφ 5 κΔs
ð8:21aÞ
Δx 5 ð2=κÞsinðκðΔsÞ=2Þcosðφ0 1 κðΔsÞ=2Þ
ð8:21bÞ
Δy 5 ð2=κÞsinðκðΔsÞ=2Þsinðφ0 1 κðΔsÞ=2Þ
ð8:21cÞ
where κ is the robot’s curvature and Δs the covered distance in a control loop. The
curvature is related to steering angle ψ by the following equation:
κ 5 ðtg ψÞ=D
ð8:22Þ
which is obtained from the relation y1 5 R_φ 5 _φ=κ and the third line of Eq. (8.20)
for _φ 6¼ 0.
The motion of the robot described by this model is as follows:
G
At t0, the robot has a curvature κ0.
G
The robot maintains the curvature for a path arc length Δs.
G
At t1, the robot has covered the distance Δs and instantly changes curvature to κ1.
G
The loop starts all over.
The control outputs are the curvature κ and the distance Δs. The control can be
further simplified if the length Δs, covered at each control loop, is assumed to
have a fixed constant value. Therefore, the only control output is the curvature.
Geometrically, this model describes a motion consisting of connected arcs.
Although this simplification eases the efforts to control the robot, it increases the
2 Based on the approximations, Δφ=2 5 sinðΔφ=2Þ 5 sinðκΔs=2Þ and φ 5 φ0 1 Δφ=2 5 φ0 1 κΔs=2 [14].
296
Introduction to Mobile Robot Control

complexity of a physical implementation of the controller. Fixed Δs with a variable
robot speed implies that the control period ΔT is also variable. The relation
between Δs and ΔT is:
ΔT 5 Δs=v1
So, the controller must be a multirate controller. Now, suppose that we have a
mobile robot that has to follow a reference path. In case the robot is misplaced, the
path tracker must steer it back on course. Mathematically, this is equivalent to min-
imizing the orientation error ψ and the position error d as shown in Figure 8.18.
The majority of the path tracking controllers use one or both of these two vari-
ables as inputs and try to minimize them, thereby positioning the robot right on
path. Here, use is made of a different set of variables as control inputs, which are
more appropriate for the control philosophy of the present fuzzy control scheme
[14,15]. This set consists of the orientation error ψ2, the same as above, and the
angular error ψ1 (Figure 8.19).
In this case, in order for the robot to track the path, ψ1 and ψ2 must be zero.
This set of variables allows us to define control actions for the whole universe of
discourse, as ψ1, ψ2 are angles and ψ1; ψ2Að2π; π.
Ψ
d
Reference path
Tangent of path at
closest point
Orientation of robot
Closest
path point 
Figure 8.18 Standard control input variables for path tracking.
ψ1
d
ψ2
Figure 8.19 The control input variables for the fuzzy controller are the angular errors ψ1
and ψ2.
297
Mobile Robot Control IV: Fuzzy and Neural Methods

8.4.2.2
Tracking Methodology
In order for the robot to follow the path, the latter is sampled under a fixed sam-
pling spacing Δspath. Each point is assigned a triplet ðx; y; φÞ, where x; y are the
point’s coordinates and φ is the angle of the line connecting the current point with
the next. Thus, the path can be represented by a matrix where the ith column
describes the ith point:
column i:
xi
yi
φi
2
64
3
75;
column i 1 1:
xi11
yi11
φi11
2
64
3
75;
φi 5 tan21 yi11 2 yi
xi11 2 xi

	
ð8:23Þ
As the robot moves, the closest point of the path is picked up, and the orientation
and angular errors for this point are considered. These variables are presented to a
fuzzy controller, and the appropriate steering command is issued. The orientation
error ψ2 and the angular error ψ1 are partitioned in nine fuzzy sets with Gaussian
(bell-type) membership functions, that is:
ψ1 5 fn180; n135; n90; n45; z; p45; p90; p135; p180g
ψ2 5 fnvb; nbig; nmid; ns; zero; ps; pmid; pbig; pvbg
where n180 denotes a bell-shaped function centered at 180 and so on. The sym-
bols nvb, nb, and so on denote bell-type fuzzy numbers: nvb, negative very big; nb,
negative big; nmid, negative medium; ns, negative small; z, zero; ps, positive
small; pmed, positive medium; pb, positive big; pvb, positive very big (as shown in
Figures 8.20 and 8.21).
The output variable, curvature κ, is partitioned in five sets with Gaussian mem-
bership functions as well (Figure 8.22).
The Mamdani inference scheme along with the “min” aggregation operator are
suitable for the present problem. The rule base of the fuzzy logic controller consists
of 9 3 9 5 81 IF-THEN rules. The philosophy of these rules is that of driving a car
(robot) that sees road signs (closest path points) that show the right direction (angle
ψ2). So, according to where the sign is (angle ψ1) and where it points to, the car is
steered appropriately (curvature κ). Thus, a typical rule would have the form:
R1: IF ψ1 is p45 AND ψ2 is nb, THEN κ is pb,
which is shown pictorially in Figure 8.23.
If the closest point is at ψ1 5 45 and points to ψ2 5 2135, the wheel is turned
hard left. A special case that needs attention is when the closest point lies at
ψ1 5 180, which is equivalent to ψ1 5 2180.
Although ψ1Að2π; πÞ and thus ψ1 never gets the value 2180, we speak in
terms of fuzzy sets. The rules that apply to the fuzzy sets n180, p180 must be con-
sistent with each other in terms of continuity. The sets n180, p180 essentially cover
two cases that are “continuous” in the real world even if their mathematical values
298
Introduction to Mobile Robot Control

lie on the two opposing ends of the universe of discourse of ψ1. This is a natural
consequence of the cyclical periodicity of angles (see Figure 8.24). To better under-
stand this, we must look at the FLC rule surface shown in Figure 8.25.
We observe that at ψ1; ψ2  2180, there is a spike. If the surface was flat,
then when ψ1 passes from 2180 to 180, the curvature would suddenly change
1
n180
n135
n90
n45
z
p45
p90
p135
p180
ψ1
–150
–100
–50
0
50
100
150
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Figure 8.20 Partition of angular error ψ1.
Source: Reprinted from Ref. [14], with permission from World Scientific.
1
nvb
nbig
nmid
ns
zero
ps
pmid
pbig
pvb
–150
–100
–50
0
50
100
150
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
ψ2
Figure 8.21 Partition of orientation error ψ2.
Source: Reprinted from Ref. [14], with permission from World Scientific.
299
Mobile Robot Control IV: Fuzzy and Neural Methods

from a minimum negative to a positive value. This kind of behavior would cause
an oscillatory motion of the robot, something which was experimentally confirmed.
Therefore, in order to avoid this, we bias the rules there, hence introducing the
spike on the FLC surface. The present path tracking controller was implemented on
a SoC (System on Chip) consisting of a parameterized FLC IP core and a Xilinx
Microblaze soft processor as the top level flow controller [1618].
1
nb
nm
ze
pm
pb
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
–0.015
–0.01
–0.005
0
Curvature
0.005
0.01
0.015
Figure 8.22 Partition of the curvature κ.
Source: Reprinted from Ref. [14], with permission from World Scientific.
Figure 8.23 Qualitative pictorial illustration of rule R1.
Two cases for ψ1 
Nine cases for ψ2
+180°
–180°
Figure 8.24 The cyclical periodicity of angles presents equivalent case rules.
Source: Reprinted from Ref. [14], with permission from World Scientific.
300
Introduction to Mobile Robot Control

The Field Programmable Gate Array (FPGA) board (Spartan 3-1500) hosting
the SoC was mounted to an ActivMedia Pioneer P3-DX8 robot which was used in
the experiments. The experimental results were obtained using the zero-order
Takagi-Sugeno inference scheme combined with triangular membership functions
with first degree adjacent overlap (see also Section 13.10). A full account of mobile
control using a scalable FPGA architecture is given in Ref. [19].
8.4.3
Fuzzy Sliding Mode Control
8.4.3.1
The Mobile Robot Model
This controller will be based on the controller presented in Section 7.2.2, and will
be applied to a WMR that moves on a surface gðx; y; zÞ 5 0 along a continuously dif-
ferentiable path pðrÞ 5 ½xðrÞ; yðrÞ; zðrÞ [20,21]. The path pðrÞ describes the motion
of the center of gravity of the robot with respect to a world coordinate frame. Under
the assumption of conservative forces, the dynamic model of the mobile robot is:3
½m 1 IRðrÞ_rðtÞ 1 I0
RðrÞ_r2ðtÞ 1 mgz0ðrÞ 5 uðtÞ
ð8:24Þ
FLC surface
1
0.5
0
−0.5
−1
200
100
0
–100
–200 –200 –150 –100 –50
0
ψ2
ψ1
50
100
150
200
Figure 8.25 FLC rule surface.
Source: Reprinted from Ref. [14], with permission from World Scientific.
3 Followed
from
the
Lagrange
equation
dð@L=@_rÞ=dt 2 @L=@r 5 u,
where
Lðr; _rÞ 5 K 2 P 5 ð1=2Þ
IRðrÞ_r2 1 ð1=2Þm_r2 2 mgzðrÞ (see Eqs. (3.7)(3.10)).
301
Mobile Robot Control IV: Fuzzy and Neural Methods

where ðÞ0 denotes the derivative dðÞ=dr, and IRðrÞ 5 hΩðrÞ; IΩðrÞi is the robot
“reflected inertia” (with ω 5 ΩðrÞ_r). An equivalent formulation uses the parameter
r instead of the time t. To this end, we use the variable:
υðrÞ 5 ðdr=dtÞðrÞ
ð8:25Þ
and obtain:
½m 1 IRðrÞυðrÞυ0ðrÞ 1 I0
RðrÞυ2ðrÞ 1 mgz0ðrÞ 5 uðrÞ
that is:
½m 1 IRðrÞ_υðrÞ 1 I0
RðrÞυ2ðrÞ 1 mgz0ðrÞ 5 uðrÞ
ð8:26Þ
Due to the limited range of the steering angles of the wheels, the floor inclina-
tion and the actuators’ constraints, the steering force is bounded as:
2U2 # u # U1
ð8:27Þ
8.4.3.2
Similarity of Fuzzy Logic Controller and Sliding Mode Controller
The sliding mode controller (SMC) for a system of the type:
€x 5 bðxÞ 1 aðxÞu
ð8:28Þ
where the gain function satisfies the inequality (7.24c):
1=ηðxÞ # ^a=a # ηðxÞ
ð8:29Þ
with η 5 ηðxÞ being the “gain margin” of the system, is given by Eq. (7.24d):
u 5 ^a21f^u 2 k sgnðsÞg
ð8:30aÞ
or
u 5 ^a21
^u 2 k sat s
B
 
n
o
ð8:30bÞ
where:
satðzÞ 5
z
if
zj j # 1
sgnðzÞ
if
zj j . 1
(
ð8:31aÞ
^u 5 2 b 1 €xd 2 Λ_~x
ð8:31bÞ
k $ ηðρmax 1 γÞ 1 ðη 2 1Þj^uj
ð8:31cÞ
302
Introduction to Mobile Robot Control

Now, consider a nonautonomous second-order single-input single-output nonlin-
ear system. In the case of a diagonal-type FLC (similar to the one of Example 8.3,
Table 8.1), regions where the controller output becomes zero lie on the diagonal
which separates the fuzzy plane into two semi-planes. For all the fuzzy regions
below (above) the diagonal, the controller output takes a positive (negative) fuzzy
value with a magnitude that depends on the distance between this fuzzy region and
a particular zero region on the diagonal, below (above) which the given fuzzy
region is located. Figure 8.26A and B illustrate the set of all fuzzy regions below
(above) the diagonal [20].
As distance between a “fuzzy region below (above) the diagonal and the diago-
nal” is defined the distance between the “center of this region and the center of the
zero region below (above) which the given fuzzy region is located.”
Looking at SMC (8.30)(8.31), we see that the controller involves the following
terms:
G
A filtering term:
ufilt 5 2 ^a21Λ_~x
ð8:32aÞ
which rejects unmodeled frequencies of the system.
G
A feedforward term:
uff 5 ^a21 €xd
ð8:32bÞ
G
A compensation term:
ucomp 5 2 ^a21b
ð8:32cÞ
G
A feedback control term:
uc 5 2 ^a21ðkÞsat s
B
 
ð8:32dÞ
which prevents the error state vector ~x 5 xðtÞ 2 xdðtÞ 5 ½~x1; _~x1T from moving away from
the sliding surface s 5 0. The negative sign indicates that the control action takes place
NLE1NLE2
(A)
(B)
PL
.
En
PLEn
PL
.
En−1
PLEn−1
NLE1NLE2
PLEn
PLEn−1
NL
.
E2
NL
.
E1
PL
.
En
PL
.
En−1
NL
.
E2
NL
.
E1
Figure 8.26 (A) Fuzzy regions below the diagonal and (B) fuzzy regions above the
diagonal.
303
Mobile Robot Control IV: Fuzzy and Neural Methods

always in the direction of decreasing error. The part 2k satðs=BÞ is of diagonal form, with
s 5 0 being the diagonal line. Therefore, the fuzzy-logic-based modification of the feed-
back control term forms the basis of the diagonal-type FLC.
8.4.3.3
Analytical Representation of a Diagonal-Type FLC
The diagonal for a second-order system is described by the equation (see Eq. (7.13)):
s 5 _~x1 Λ~x
ð8:33Þ
The rules of the diagonal-type FLC can be selected as follows:
1. The states ~x; _~x are bounded as:
2ρ0
x;max # ~x # ρ0
x;max
2ρ1
x;max # _~x# ρ1
x;max
2. The control signal u is bounded as:4
2ρu;max # u # ρu;max
3. The state ~x and _~x that are located on the diagonal produce zero control signals.
4. The states ~x and _~x that are located below the diagonal produce positive control signals.
5. The states ~x and _~x that are located above the diagonal produce negative control signals.
6. The magnitude of the control signal juj increases when the distance from the diagonal
increases, and vice versa. On the basis of the above, we can express the diagonal-type
FLC by the formula:
ufuzzy 5 2 Kfuzzyð~x; _~x; ΛÞ sgnðsÞ
ð8:34Þ
with the condition:
Kfuzzyð~x1; _~x1; ΛÞ # Kfuzzyð~x2; _~x2; ΛÞ
for jΛ~x1 1 _~x1j # jΛ~x2 1 _~x2j, which means that the greater the distance of ð~x; _~xÞ from the
sliding surface, the greater the control signal.
8.4.3.4
Reduced Complexity Sliding Mode Fuzzy Logic Controller
To design a reduced-complexity sliding mode fuzzy logic controller (RC-SMFLC),
we start from the conditions that assure the error convergence to zero, namely:
IF eðtÞ_eðtÞ , 0, THEN xðtÞ ! xdðtÞ.
that is, eðtÞ 5 ~xEðtÞ 5 xðtÞ 2 xdðtÞ ! 0.
IF eðtÞ_eðtÞ . 0, THEN xðtÞ deviates from xdðtÞ.
4 Note that here ~x is defined as ~xðtÞ 5 eðtÞ 5 xðtÞ 2 xdðtÞ. Therefore, if ~xðtÞ # 0, then the control action u
should be positive, and if ~xðtÞ . 0, the control action u should be negative.
304
Introduction to Mobile Robot Control

As we can observe, these conditions could be derived by using the Lyapunov func-
tion V 5 ð1=2Þe2ðtÞ for which _V 5 eðtÞ_eðtÞ. The sliding condition is then defined as:
_Vðx; tÞ 5 eðtÞ_eðtÞ , 0
ð8:35Þ
Here, we have two possible control actions: increase or decrease. Therefore, we
have the following control rules:
1. IF sgnðeðtÞ_eðtÞÞ , 0, THEN the control action leads to convergence and should be maintained.
2. IF sgnðeðtÞ_eðtÞÞ . 0, THEN the control action leads to divergence and should be changed.
The increase (decrease) of the control signal is assured by the following rules:
1. Increase
IF uk is U1, THEN uk11 is U2; . . .; IF uk is Un21, THEN uk11 is Un.
2. Decrease
IF uk is U2, THEN uk11 is U1; . . .; IF uk is Un, THEN uk11 is Un21.
where uk 5 uðtÞt5kΔt with Δt being a given time increment (sampling period) and
U1; U2; . . .; Un are the fuzzy subsets in which the fuzzy phase plane U of the con-
trol input is divided. In the RC-SMFLC case, e 5 0 plays the role of the diagonal.
The properties of RC-SMFLC are analogous to the ones of a diagonal type, and
can be expressed mathematically as:
ufuzzy 5 2KfuzzyðjsjsgnðsÞÞ
ð8:36Þ
where s is the distance from the diagonal. If the membership functions have the same
shape (e.g., triangular with the same width and slopes), then the control law produced
is ufuzzy 5 2Kfuzzy sgnðsÞ. To overcome this problem, the width of the membership
functions should be modified at every crossing of the diagonal e 5 0. The last two
control signals uk21; uk are taken into account, namely, uk21 is the last signal below
(above) the diagonal; uk is the last control signal above (below) the diagonal. As the
diagonal is approached, the width of the membership functions is reduced, and so
the gain Kfuzzy is reduced too. In this way, the control law takes the form:
ufuzzy 5 2Kfuzzyðsgnðekek21ÞÞsgnðsÞ
ð8:37Þ
with s 5 e_e, where ek21 is the error at the ðk 2 1Þth step of the algorithm and ek the
error at the kth step. On the basis of the above, the similarity between RC-SMFLC
and the diagonal-type FLC or the conventional SMFLC becomes clear.
8.4.3.5
Application to the Mobile Robot
We write the model (8.26) as:
_υðrÞ 1
I0
RðrÞ
m 1 IRðrÞ υ2ðrÞ 1 mgz0ðrÞ
m 1 IRðrÞ 5
1
m 1 IRðrÞ uðrÞ 5 uðrÞ
ð8:38Þ
305
Mobile Robot Control IV: Fuzzy and Neural Methods

and assume that m, the moment of inertia IðrÞ, and the reflected inertia IRðrÞ are
exactly known, the only parametric uncertainty being in the slope:
z0ðrÞ 5 @z=@r
of the path.
The control signal that compensates the uncertainties and leads to closed-loop
velocity stability has the form:
uðtÞ 5 _υdðtÞ 1
I0
RðrÞ
m 1 IRðrÞ υ2ðtÞ 1 K
peðtÞ 1 uRC-SMFLCðtÞ
ð8:39Þ
In the case of position control, the closed-loop stability can be established as follows:
We use the robot model (8.24):
€rðtÞ 1
I0
RðrÞ
m 1 IRðrÞ _r2ðtÞ 1 mgz0ðrÞ
m 1 IRðrÞ 5 uðtÞ
ð8:40aÞ
and the PD controller:
uðtÞ 5 €rdðtÞ 1
I0
RðrÞ
m 1 IRðrÞ _r2ðtÞ 1 KpeðtÞ 1 Kd _eðtÞ 1 uRC-SMFLCðtÞ
ð8:40bÞ
to get:
€rdðtÞ 2 €rðtÞ 1 KpeðtÞ 1 Kd _eðtÞ 1 uRC-SMFLCðtÞ 2 mgz0ðrÞ
m 1 IRðrÞ 5 0
ð8:41Þ
where eðtÞ 5 rdðtÞ 2 rðtÞ. Thus:
€eðtÞ 1 Kd _eðtÞ 1 KpeðtÞ 5 mgz0ðrÞ
m 1 IRðrÞ 2 uRC-SMFLCðtÞ
ð8:42Þ
Therefore, if:
lim
t!N
mgz0ðtÞ
m 1 IRðrÞ 2 uRC-SMLCðtÞ


5 0
ð8:43Þ
the gains Kp and Kd can be selected such as eðtÞ ! 0 as t ! N. The design of the
RC-SMFLC controller guarantees that the condition (8.43) will be satisfied, and
therefore, the closed-loop system will be asymptotically stable. The proof of stabil-
ity in the case of velocity control is similar. The block diagram of the complete
hybrid closed-loop system with the PD controller (gains Kp; Kd), and the RC-
SMFLC controller is shown in Figure 8.27 [20].
306
Introduction to Mobile Robot Control

Example 8.5
It is desired to provide a solution to the parallel car-parking problem using the RC-SMFLC
controller.
Solution
Consider a car-like WMR which is to park in a restricted rectangular space of dimensions a
and β (Figure 8.28A) [22].
The steps followed by a human driver for parallel parking are the following [22]:
Step 1: The vehicle is set parallel and ahead of the parking area.
Step 2: The vehicle backtracks to a certain point and then the front wheels are turned
so that the vehicle moves toward the parking area until it reaches a certain angle of
approach.
Step 3: As long as the vehicle stays inside the boundaries of the parking place, the
vehicle continuous to backtrack.
PD
RC-SMFLC
Mobile
robot
Nonlinear
control term
∑
∑
∑
+
−
+
+
+
+
ν, ν.
ν , νd
d
Nonlinear feedback
controller
Sliding mode fuzzy
logic controller 
Figure 8.27 Structure of hybrid PD and RC-SMFLC control system.
ψ
K0
K1
K2
xref
ref
K6
K4
K3
K5
y
α
α
A
B
β
β
y
x
x
(A)
(B)
D
Figure 8.28 (A) Parking space and vehicle and (B) parameters of parking maneuver strategy.
307
Mobile Robot Control IV: Fuzzy and Neural Methods

Step 4: When the vehicle reaches the boundaries, the front wheels are turned the other
way round, and the direction of movement is also changed.
Step 5: Step 3 is repeated until the vehicle is found in the direction parallel to the
desired one, but inside the limitations of the parking space.
One can easily conclude that the driver’s behavior can be expressed by linguistic rules,
and so fuzzy logic can be used to design the required controller. This controller will be
derived using RC-SMFLC rules.
The parking space is parameterized by φref and xref. Let A and B be the dimen-
sions of the vehicle, and assume that initially it is on the right of the parking place
with y0 5 h and x0 5 0. The parameters φref and xref are given by:
sin φref 5 a=B
xref 5
1
tg φref
h 1 a
2 2
D
tg ψ 2 A cos φref


1
1
sin φref
A
2 1
D
tg ψ


where h is the initial position of the center of the rear axis of the car (i.e., y0 5 h)
and D is the length of the wheel-base of the car. The maneuver strategy is demon-
strated in Figure 8.28B [22,23]. The equations defining xref and φref can be easily
interpreted as follows: the aim is for the car to enter the parking place with the
highest angle possible, such that the right front part of the car remains within the
parking place. xref is selected such that, after maneuvers ðK0Þ, ðK1Þ, and ðK2Þ are
completed, the vehicle reaches the left border of the parking area, with its left rear
corner, in order to have as much room as possible to reorient.
The parameters xref and φref are not always known in advance in real-world park-
ing situations; however, empirical rules can be used to approximate them. In general,
a genetic algorithm can be used to find the appropriate values of xref and φref.
The parking maneuvers consist of the following steps: the vehicle backs up ðK0Þ
until the back of the car reaches xref, it turns right ðK1Þ until the orientation over-
shoots φref, and then it backs up ðK2Þ until the rear of the car touches the left or the
lower border of the parking area. Finally, the vehicle reorients by repeating the fol-
lowing sequence: (i) if the rear part touches the border of the parking place ðK3Þ, it
drives forward and turns right ðK4Þ; (ii) if the front part touches the border ðK5Þ, or
if the left rear corner of the car reaches the upper limit of the parking place, the
vehicle backs up ðK6Þ. As soon as the vehicle becomes parallel to the desirable axis
ðφd 5 0Þ, the parking maneuvers stop.
Every rule Ki can be implemented by a rule-based incremental controller.
Standard parking algorithms need only to consider an increment Δv 5 2v on the
speed component of the input vector, and an increment Δψ 5 2ψ on the steering
angle component of the input vector. Although this control approach does not
308
Introduction to Mobile Robot Control

explicitly minimize the number of maneuvers, it is rather effective. Actually, no
increment on the speed component is considered, while the increment Δψ on the
steering angle is calculated via fuzzy inference. Additionally, we want to provide a
controller that will not be strictly based on the rules given by a human expert. The
controller will be able to determine smooth changes of the steering angle.
Referring to Figure 8.28B, the control strategy can be expressed as follows: the
vehicle is set parallel and ahead of the parking place with its rear far ahead from
point xref. The vehicle backtracks to the point xref, and then it turns the front wheels
so as to move toward the parking place until it reaches the angle of approach φref.
As long as the vehicle stays inside the parking area, it continuous to backtrack
using the modified RC-SMFLC algorithm. When it reaches the boundaries, some
additional rules are used to accelerate the convergence to the desirable final posi-
tion. The vehicle changes direction and moves forward turning its wheels driven by
the controller output ψ. The goal is to set the vehicle in parallel to φd 5 0 and
inside the parking place. The error is e 5 φ 2 φd and its derivative _e is also
calculated.
The present RC-SMFLC control algorithm involves the following rules:
R1: IF sgnðeðtÞ_eðtÞÞ , 0 AND the previous control action was to increase the control sig-
nal, THEN keep on increasing the control signal.
R2: IF sgnðeðtÞ_eðtÞÞ , 0 AND the previous control action was to decrease the control sig-
nal, THEN keep on decreasing the control signal.
R3: IF sgnðeðtÞ_eðtÞÞ . 0 AND the previous control action was to increase the control sig-
nal, THEN set ψ 5 0 and decrease ψ.
R4: IF sgnðeðtÞ_eðtÞÞ . 0 AND the previous control action was to decrease the control sig-
nal, THEN set ψ 5 0 and increase ψ.
To increase the flexibility in the vehicle’s maneuvering, and to accelerate the conver-
gence to the desirable angle φd 5 0, the width of the fuzzy subsets describing the steer-
ing angle has to be modified each time the vehicle reaches the boundaries of the parking
area. Thus, two more rules are employed:
R5: IF the vehicle reaches the FRONT or the INSIDE boundary, THEN increase Δψ and
change sgnðuÞ.
R6: IF the vehicle reaches the REAR or the OUTSIDE boundary, THEN decrease Δψ
and change sgnðuÞ.
An increase of Δψ means that the error vector ½e; _eT lies far from the desirable
value ½0; 0T, and so bigger changes of the control signal ψ are required to speed up
convergence. A decrease of Δψ means that the error vector ½e; _eT lies near the
desirable value ½0; 0T, thus more subtle changes of the control signal ψ are required
to speed up convergence (Figure 8.29) [22].
It is noted that unlike the conventional RC-SMFLC, described in Section 8.4.3.3,
here the error e never changes sign. Also, between two consecutive control actions,
the immediate change of sign of the control signal ψ is prohibited. Therefore, to
switch from an increase to a decrease control action and vice versa, the steering
angle ψ must first be set to ψ 5 0.
309
Mobile Robot Control IV: Fuzzy and Neural Methods

8.5
Neural Control of Mobile Robots
In this section, the control of WMRs will be considered using the two types of neu-
ral networks presented in Section 8.2.2, namely:
G
MLP network
G
RBF network
In the literature, there are also used other types of NNs (wavelet networks, recur-
rent networks, self-organizing maps, local neural networks, etc.) or NFSs [24,25]. For
all of them, the methodology is practically the same as that developed in this section.
8.5.1
Adaptive Tracking Controller Using MLP Network
The two-step (backstepping) procedure used in Section 8.4.1 for designing adaptive
fuzzy controllers will be followed, namely:
G
Kinematic controller design, which produces the auxiliary velocity control inputs vmðtÞ and
ωmðtÞ that assure asymptotic convergence of the robot trajectory qðtÞ 5 ½xðtÞ; yðtÞ; φðtÞT to
the desired one: qdðtÞ 5 ½xdðtÞ; ydðtÞ; φdðtÞT.
G
Dynamic controller design, which produces the torque control inputs that assure asymp-
totic tracking of vmðtÞ and ωmðtÞ which are used as reference inputs to the dynamic con-
trol subsystem.
The kinematic controller is (see Eq. (8.18)):
vm 5 vd cos ε3 1 K1ε1
ωm 5 ωd 1 K2vdε2 1 K3 sin ε3
ð8:44Þ
The dynamic controller was derived in Section 5.4.2 for the dynamic model
((5.49a) and (5.49b)) with exactly the known dynamic parameters (m, I, etc.), in
Example 7.1 for the case of unknown constant parameters using the parameter
adaptation law (7.77), and in Section 8.4.1 using the simple proportional fuzzy con-
trol law that has nine fuzzy control rules, as given in Table 8.2.
Here, the dynamic control job will be accomplished using a neural network con-
troller in place of the fuzzy controller [26,27]. For more generality, we will con-
sider the unconstrained dynamic model of the nonholonomic WMR described by
Eqs. ((3.19a) and (3.19b)):
DðqÞ_v 1 Cðq; _qÞv 1 gðqÞ 5 Eτ
outside: increase Δψ
Rear
decrease Δψ
Front
increase Δψ
inside: decrease Δψ 
Figure 8.29 Adaptation of the fuzzy
subsets describing steering angle.
310
Introduction to Mobile Robot Control

omitting the gravitational term gðqÞ which for the WMR is zero, and adding a lin-
ear friction term BðvÞ, that is:
DðqÞ_v 1 Cðq; _qÞv 1 BðvÞ 5 τ
ð8:45Þ
where τ 5 Eτ and v 5 ½v; ωT.
For convenience, the kinematic controller (8.44) is expressed as:
vm 5 fmðεm; vd; KÞ;
K 5 ½K1; K2; K3T
ð8:46Þ
Differentiating vm we get:
_vm
_ωm
"
#
5
_vd cos ε3
_ωd 1 K2 _vdε2
"
#
1
K1
0
2vd sin ε3
0
K2vd
K3 cos ε3
"
#
_ε1
_ε2
_ε3
2
64
3
75
Now, if vdðtÞ 5 vd 5 const: and ωdðtÞ 5 ωd 5 const:, then:
_vm
_ωm
"
#
5
K1
0
2vd sin ε3
0
K2vd
K3 cos ε3
"
#
_ε
ð8:47Þ
and the feedback control input vector u 5 ½u1; u2T is selected as:
u 5 _vm 1 K4ðvm 2 vÞ
ð8:48Þ
where K4 is a diagonal positive definite matrix:
K4 5 k4I2 3 2
ð8:49Þ
If vdðtÞ and ωdðtÞ are varying, there is no change in the form of the control law
(8.47). The two-step controller defined by Eqs. (8.46) and (8.48) assures asymptotic
tracking of the desired state trajectory qdðtÞ 5 ½xdðtÞ; ydðtÞ; φdðtÞT. This can be
shown as usual by using the extended Lyapunov function:
V 5 K1ðε2
1 1 ε2
2Þ 1 ð2K1=K2Þð1 2 cos ε3Þ 1 ð1=2K4Þ½ε2
4 1 ðK1=K2K3Þε2
5
where ε4 and ε5 are the components of ~v 5 vm 2 v.
We now proceed to show how to use the neural net for the design of the adap-
tive neurocontroller. Differentiating the velocity tracking error ~v 5 vm 2 v and
using Eq. (8.45), we get the robot dynamics in terms of ~v, namely:
DðqÞ_~v5 2 Cðq; _qÞ~v 1 FðxÞ 2 τ
ð8:50Þ
311
Mobile Robot Control IV: Fuzzy and Neural Methods

where:
FðxÞ 5 DðqÞ_vm 1 Cðq; _qÞvm 1 BðvÞ
ð8:51Þ
The vector needed for the computation of FðxÞ is:
x 5
v
vm
_vm
2
64
3
75
which is available (measured). The function FðxÞ involves all the dynamic para-
meters of the robot (masses, moments of inertia, friction coefficient, etc.) that in
practice are unknown or involve uncertainties.
From Eq. (8.50), we see that a proper feedback control law is:
τ 5 ^FðxÞ 1 K4~v 2 μ
ð8:52Þ
where ^FðxÞ is an estimate of FðxÞ; K4 5 diag½K41; K42 with K41 . 0, K42 . 0, and μ
is a robustifying term required to compensate any unmodeled structural disturbances.
Introducing Eq. (8.52) into Eq. (8.50), we get the closed-loop system:
DðqÞ_~v5 2½Cðq; _qÞ 1 K4~v 1 FðxÞ 2 ^FðxÞ 1 μ
ð8:53Þ
Clearly, to assure that Eq. (8.53) is asymptotically stable at ~v 5 0, we have to
select properly K4, ^FðxÞ, and μ. Here, the estimate ^FðxÞ of FðxÞ is provided by a
neural network approximator (8.12), namely:
^FðxÞ 5 ^VTσð ^WTxÞ 1 ε
ð8:54Þ
in which case Eq. (8.52) becomes:
τ 5 ^VTσð ^WTxÞ 1 K4~v 2 μ 1 ε
and the closed-loop system becomes:
DðqÞ_~v5 2½Cðq; _qÞ 1 K4~v 1 VTσðWTxÞ 2 ^VTσð ^WTxÞ 2 ε 1 μ
ð8:55Þ
Now, we only have to select the neural network parameter updating (tuning) and
the robustifying term μ. Choosing μðtÞ as:
μðtÞ 5 2 Kμ~v
ð8:56aÞ
312
Introduction to Mobile Robot Control

we get:
DðqÞ_~v5 2 ½Cðq; _qÞ 1 K4 1 Kμ~v 1 VTσðWTxÞ 2 ^VTσð ^WTxÞ 2 ε
ð8:56bÞ
Then, we can show that updating the NN weights as (Section 8.5.3):
_^V5 A ^σ~vT 2 A ^σ0 ^WTx~vT 2 λAjj~vjj ^V
ð8:57Þ
_^W5 Bxð ^σ0T ^V~vÞT 2 λBjj~vjj ^W
ð8:58Þ
where σ0 is the hidden-layer gradient corresponding to σ, A and B are positive
definite design matrices, and λ . 0, stabilizes ~v to an invariant region around
~v 5 0. For the sigmoid function σ, σ0 is given by:
σ0ð^zÞ 5 ½@σðzÞ=@zz5^z 5 diagfσð ^WTxÞg½I 2 diagfσð ^WTxÞg
The block diagram of the overall adaptive neurocontroller is shown in
Figure 8.30 [26,27].
Example 8.6
It is desired to design a fuzzy logic adaptive controller using the above neural network
control scheme, along with the fuzzy universal approximator (8.14).
Solution
The analysis of the controller is exactly the same as that described in Section 8.5.1, up to
the design of the controller (8.52), where ^FðxÞ is an estimate of the dynamics function
FðxÞ. This estimate can be computed using Eq. (8.17) as:
^FðxÞ 5 ΨðxÞ^β
ð8:59Þ
where x 5 ½v; vm; _vmT; ΨðxÞ is the FBF matrix with elements given by Eq. (8.13), and ^β is
the approximator’s parameter vector estimate that must be updated as the time passes.
The adaptation law has the standard form:
_^β5 ΓΨTðxÞ~v 2 λΓjj~vjj^β;
~v 5 vm 2 v
ð8:60Þ
Here, Γ is a selected positive constant matrix (e.g., Γ 5 diag½γ1; γ2) and λ is a small
positive constant. Using the standard Lyapunov stability theory, of adaptive control and
adaptation laws, it follows that the estimator (8.60) combined with the controller (8.52)
assures overall stability to ~v 5 0 (actually to a small vicinity of ~v 5 0). The structure of
this adaptive controller is the same as that of NN controller with the neural estimator
block being replaced by the fuzzy estimator (Figure 8.30).
313
Mobile Robot Control IV: Fuzzy and Neural Methods

8.5.2
Adaptive Tracking Controller Using RBF Network
The RBF NNs can be used in a way similar to MLP networks and the fuzzy univer-
sal approximator [28]. The approximation of the function FðxÞ in Eq. (8.51) is per-
formed by the relation (8.10) for the jth component yjðxÞ 5 FjðxÞ; j 5 1; 2; . . .; p:
FjðxÞ 5
X
m
i51
wjiφiðxÞ
ð8:61Þ
of FðxÞ, where:
φiðxÞ 5 φðjjx 2 cijjÞ 5 φðriÞ 5 expð2r2
i =2σ2
i Þ;
ri $ 0;
σi $ 0
ð8:62Þ
The algorithm for updating the weights wki, the RBF centers ci, and the widths
σi involves the four steps described in Section 8.2.2.4. In general, the weight
wiðk 1 1Þ and center ciðk 1 1Þ updating algorithm can be either the gradient descent
law, or a second-order Newton-like updating law, which minimizes a squared error
function:
J 5 1
2
X
n
k51
ε2ðkÞ
ð8:63aÞ
where:
εðkÞ 5 yðkÞ 2
X
m
i51
wiφðjjxðkÞ2cijjÞ2
ð8:63bÞ
with inputoutput data fxðkÞ; yðkÞg.
Therefore, in the case of the gradient descent updating law, we have:
wiðk 1 1Þ 5 wiðkÞ 2 ηw
@εðkÞ
@wiðkÞ
ð8:64aÞ
ciðk 1 1Þ 5 ciðkÞ 2 ηc
@εðkÞ
@ciðkÞ
ð8:64bÞ
where ηw and ηc are design parameters that control the rate of convergence. Many
alternative ways exist in the literature for updating (adapting) the RBF parameters
and weights.
Overall, the RBF representation (8.61) is similar to the fuzzy universal approxi-
mator and can be used in the same manner for estimating FðxÞ required by the con-
trol law (8.52). This estimator is placed in the NN/FLC box of the overall control
scheme shown in Figure 8.30.
314
Introduction to Mobile Robot Control

8.5.3
Appendix: Proof of Neurocontroller Stability
Here, a brief sketch of the proof of asymptotic convergence to an invariant set
around the origin of the closed-loop error, obtained using the controller (Eqs. (8.52),
(8.56a), (8.57), (8.58)), will be presented (omitting the lengthy calculations). We
assume that the universal approximator Eq. (8.12) is valid with a given accuracy ε
for all x in a compact set Ux, and consider the candidate Lyapunov function:
V05 K2
2 ðε2
11ε2
2Þ1 1
2ε2
31V0
0; V0
05 1
2 ~vT
mD~vm1tr
~V
TA21 ~V
n
o
1tr
~W
TB21 ~W
n
o
h
i
~V5V2 ^V; ~W5W2 ^W
The derivative of V0 is given by:
_V0 5 K2ðε1_ε1 1 ε2_ε2Þ 1 ε3_ε3 1 _V0
0
Introducing _ε1; _ε2, and _ε3 and computing _V0
0, we find (after the calculations) that
the condition:
jj~vmjj . ρm
where ρm, a properly selected bound, assures that _V0 , 0 outside the compact set
Ux. According to the LaSalle invariant set stability condition (see Section 6.2.3),
this means that ~v and ~ε (i.e., ~x) converge to their corresponding invariant sets asso-
ciated with Ux [26,27].
x
y
Reference 
WMR
Mobile
Robot 
sin Φ
Neural Network
(Approximator)
Eq. 8.54
0
0
0
1
fm (εm, vd,k) 
Eq.8.46
Robot
kinematics
Kinematic
controller
(Eq.7.69)
=
+
τ
τ
μ
+
+
+
−
−
−
·
·
(Eq.8.45)
E
Kμ
K4
(Eq.8.56)
q
q
νm
ε
νm
νm
ν
ν
ν
∼
E(q)
q
∼
∼
F(x)
cos Φ
Φ
xd
yd
=
qd
Φd
Figure 8.30 Structure of overall adaptive neurocontroller.
315
Mobile Robot Control IV: Fuzzy and Neural Methods

References
[1] Zadeh LA. Fuzzy sets. Inf Control 1965;8:33853.
[2] Kosko B. Neural networks and fuzzy systems: a dynamical system approach to
machine intelligence. Englewood Cliffs, NJ: Prentice Hall; 1992.
[3] Chen CH. Fuzzy logic and neural network handbook. New York, NY: McGraw-Hill;
1996.
[4] Haykin S. Neural networks: a comprehensive foundation. Upper Saddle River, New
Jersey: Macmillan College Publishing; 1994.
[5] Tzafestas SG, editor. Soft computing and control technology. Singapore/London:
World Scientific Publishers; 1997.
[6] Tsoukalas LH, Uhrig RE. Fuzzy and neural approaches in engineering. New York,
NY: John Wiley & Sons; 1997.
[7] Tzafestas SG. Fuzzy systems and fuzzy expert control: An overview. Knowl Eng Rev
1994;9(3):22968.
[8] Wang LX, Mendel JM. Fuzzy basis functions, universal approximation, and orthogonal
least-squares learning. IEEE Trans Neural Networks 1992;3(5):80714.
[9] Omatu S, Khalid M, Yusof R. Neuro-control and its applications. London/Berlin:
Springer; 1996.
[10] Das T, Narayan Kar I. Design and implementation of a adaptive fuzzy logic-based con-
troller for wheeled mobile robots. IEEE Trans Control Syst Technol 2006;14
(3):50110.
[11] Castillo O, Aguilar LT, Ca´rdenas S. Fuzzy logic tracking control for unicycle mobile
robots. Eng Lett 2006;13(2): [EL. 13-2-4:73-7].
[12] Driesen BJ, Feddema JT, Kwok KS. Decentralized fuzzy control of multiple nonholo-
nomic vehicles. J Intell Robot Syst 1999;26:6578.
[13] Balluchi A, Bicchi A, Balestrino A, Casalino G. Path tracking control for Dubins cars.
Proceedings of 1996 IEEE international conference on robotics and automation,
Minneapolis, MI; April 1996. p. 312328.
[14] Moustris G, Tzafestas SG. A robust fuzzy logic path tracker for non-holonomic mobile
robots. J Artif Intell Tools 2005;14(6):93565.
[15] Moustris G, Tzafestas SG. Switching fuzzy tracking control for the Dubins car.
Control Eng Practice 2011;19(1):4553.
[16] Deliparaschos KM, Moustris GP, Tzafestas SG. Autonomous SoC for fuzzy robot path
tracking. Proceedings of the European control conference. Kos, Greece; July 25,
2007. p. 5471-78.
[17] Moustris GP, Deliparaschos KM, Tzafestas SG. Tracking control using the strip-wise
affine transformation: an experimental SoC design. Proceedings of the European con-
trol conference. Budapest, Hungary; August 2326, 2009. [paper MoC3.5.]
[18] Tzafestas SG, Deliparaschos KM, Moustris GP. Fuzzy logic path tracking control for
autonomous non-holonomic robots: design of system on a chip. Rob Auton Syst
2010;58:101727.
[19] Moustris GP, Deliparaschos KM, Tzafestas SG. Feedback equivalence and control of
mobile robots through a scalable FPGA architecture. In: Velenivov Topalov A, editor.
Recent advances in mobile robotics. In Tech; 2011,www.interchopen.com/books..
[20] Rigatos GG, Tzafestas CS, Tzafestas SG. Mobile robot motion control in partially
unknown environments using a sliding-mode fuzzy-logic controller. Robot Autom Syst
2000;33:111.
316
Introduction to Mobile Robot Control

[21] Kyriakopoulos KJ, Saridis GN. Optimal and Suboptimal motion planning for collision
avoidance of mobile robots in non-stationary environments. J Intell Robot Syst
1995;11(3):22367.
[22] Rigatos GG, Tzafestas SG, Evangelidis GJ. Reactive parking control of nonholonomic
vehicles via a fuzzy learning automaton. IEE Proc Control Theory Appl 2001;148
(2):16979.
[23] Luzeaux D. Parking maneuvers and trajectory tracking. Proceedings of the third inter-
national workshop on advanced motion control. Berkeley, CA: University of
California; 1994.
[24] De Oliveira VM, De Pieri ER, Lages WF. Wheeled mobile robot using sliding modes
and neural networks: learning and nonlinear models. Review Soc. Brasileria de Redes
Neurais 2003;1(2):10321.
[25] Oubbati M, Schanz M, Levi P. Kinematic and dynamic adaptive control of a nonholo-
nomic mobile robot using a RNN. Proceedings of the IEEE symposium on computa-
tional intelligence in robotics and automation (CIRA’05). Espoo, Helsinki, Finland;
June 2730, 2005. p. 2733.
[26] Lewis FL, Campos J, Selmic R. Neuro-fuzzy control of industrial systems with actua-
tor nonlinearities. Philadelphia, PA: SIAM; 2002.
[27] Fierro R, Lewis FL. Control of a nonholonomic mobile robot: Backstepping kinematics
into dynamics. J Robot Syst 1997;14(3):14963.
[28] Bayar G, Konukseven EI, Koku AB. Control of a differentially driven mobile robot
using radial basis function based neural networks. WSEAS Trans Syst Control 2008;3
(12):100213.
317
Mobile Robot Control IV: Fuzzy and Neural Methods

9 Mobile Robot Control V:
Vision-Based Methods
9.1
Introduction
Vision is an important robotic sensor since it can be used for environment measure-
ment without physical contact. Visual robot control or visual servoing is a feedback
control methodology that uses one or more vision sensors (cameras) to control the
motion of the robot. Specifically, the control inputs for the robot motors are pro-
duced by processing image data (typically, extraction of contours, features, corners,
and other visual primitives). In robotic manipulators, the purpose of visual control
is to control the pose of the robot’s end-effector relative to a target object or a set
of target features. In mobile robots, the vision controller’s task is to control the
vehicle’s pose with respect to some landmarks. Tracking stability can be assured
only if the vision sensing delays are sufficiently small and/or the dynamic model of
the robot has a sufficient accuracy. Over the years many techniques were developed
for compensating this delay of the visual system in robot control. A rich literature
has been oriented to the control of nonholonomic systems in order to handle vari-
ous challenging problems associated with vision-based control.
The objectives of this chapter are as follows:
G
To present the basic aspects of visual servoing
G
To discuss the position-based and image-based visual-control problems
G
To apply visual servoing to a number of selected mobile robot control problems
G
To study the mobile robot visual servoing using omnidirectional vision
9.2
Background Concepts
9.2.1
Classification of Visual Robot Control
Visual robot controllers (VRCs) depend on whether the vision system provides set-
points as input to the robot joint controllers, or computes directly the joint level
inputs, and whether the error signal is determined in task space coordinates or
directly in terms of image features.
Therefore, VRCs are classified in the following four categories [18]:
G
Dynamic look-and-move system—Here the control structure is hierarchical in which the
vision system provides set-point inputs to the joint controllers and so the robot is inter-
nally controlled using joint feedback.
Introduction to Mobile Robot Control. DOI: http://dx.doi.org/10.1016/B978-0-12-417049-0.00009-2
© 2014 Elsevier Inc. All rights reserved.

G
Direct visual servo control—Here the robot joint controller is eliminated and replaced by
a visual servo controller which directly computes the inputs of the joints, and stabilizes
the robot using only vision signals. Actually, most implemented VRCs are of the look-
and-move type because internal feedback with a high sampling rate provides the visual
controller with an accurate axis dynamic model. Also, look-and-move control separates
the kinematics singularities of the system from the visual controller, and bypasses the low
sampling rates at which the direct visual control can work.
G
Position-based visual robot control (PBVRC)—Here, use is made of features extracted
from the image and used together with a geometric model of the target and the available
camera model to determine the pose of the target with respect to the camera. Thus, the
feedback loop is closed using the error in the estimated pose space.
G
Image-based visual robot control (IBVRC)—Here, direct computation of the control sig-
nals is performed using the image features. IBVRC reduces the computational time, does
not need image interpretation, and eliminates the errors of sensors’ modeling and camera
calibration. But its implementation is more difficult due to the complex nonlinear dynam-
ics of robots.
The PBVRC and IBVRC control schemes have the structure shown in
Figure 9.1.
9.2.2
Kinematic Transformations
Kinematic (homogeneous) transformations were discussed in Section 2.2.2. The
task space of a robot (fixed or mobile) is denoted by Cs and describes the set of
positions and orientations attainable by a mobile robot or by the end-effector of a
fixed robot. Robotic tasks are typically specified with respect to specific coordinate
frames (e.g., the camera frame, the target/object frame). As seen in Section 2.2.2,
Desired
pose
Cartesian 
space 
controller
τ
Pose 
estimation
Image feature 
extraction
Video
Actual 
pose
+
−
Cartesian 
pose error 
(A)
Feature 
space 
controller
τ
Image feature
extraction
Video
e(f)
+
−
Feature pose 
error 
(B)
Desired image 
features  
C
C
Figure 9.1 (A) Position-based visual robot control loop and (B) image-based visual robot
control loop.
320
Introduction to Mobile Robot Control

the notation Aj
i represents both a coordinate transformation or a pose, involving a
rotation matrix Rj
i and a translation pj
i (or dj
i) (see Eqs. (2.9) and (2.10)). A multiple
coordinate transformation is used to obtain a desired multistep change of coordi-
nates as shown by Eq. (2.18).
In vision-based robot control, the following coordinate frames are typically
needed:
G
the target coordinate frame At attached to the object (target),
G
the world coordinate frame A0 (attached to a given fixed point of the workspace),
G
the camera coordinate frame Aci attached to the ith camera,
G
the robot frame Ar (attached to a fixed point of the robot body).
Consider a fixed-robot manipulator working in a 3D space. The motion of its
end-effector is described in world coordinates by a translational velocity vðtÞ and
an angular velocity ωðtÞ, where:
vðtÞ 5 ½vx,vy,vzΤ
ωðtÞ 5 ½ωx,ωy,ωzΤ
ð9:1Þ
Let p 5 ½x,y,zΤ be a point rigidly attached to the end-effector, where x,y,z are
the world coordinates of p. Then, _p is given by:
_p 5 ω 3 p 1 v
ð9:2aÞ
where ω 3 p is the cross product of ω and p, that is:
ω 3 p 5
ωyz 2 yωz
ωzx 2 zωx
ωxy 2 xωy
2
4
3
5
ð9:2bÞ
The combined velocity vector _r, where:
_rðtÞ 5
v
ω


ð9:3Þ
is called the velocity screw (or velocity twist) of the robotic end-effector. In com-
pact form, Eqs. (9.2a) and (9.2b) can be written as:
_pðtÞ 5 J0ðpÞ_r,
J0ðpÞ 5 ½I3 3 3^SðpÞ
ð9:4aÞ
where S(p) is the skew symmetric matrix:
SðpÞ 5
0
z
2y
2z
0
x
y
2x
0
2
4
3
5
ð9:4bÞ
321
Mobile Robot Control V: Vision-Based Methods

The platform of mobile robots is moving with a linear velocity vðtÞ 5 ½vx,vy,0Τ
and angular velocity ωðtÞ 5 ½0,0,ωΤ
ðω 5 ωzÞ. Therefore Eq. (9.2b) becomes:
ω 3 p 5
2yω
xω
0
2
4
3
5
and (9.2a) gives:
_x
_y
_z
2
4
3
5 5
2yω
xω
0
2
4
3
5 1
vx
vy
0
2
4
3
5
or
_x
_y
 
5
vx 2 yω
vy 1 xω


with z 5 constant 5 0. Embedding the relation _φ 5 ω in the above equation, we get:
_x
_y
_φ
2
4
3
5 5
1
0
2y
0
1
x
0
0
1
2
4
3
5_r,
_r 5
vx
vy
ω
2
4
3
5
ð9:4cÞ
If the wheeled mobile robot (WMR) involves a steering angle ψ, then the corre-
sponding equation _ψ 5 ωψ must be added to Eq. (9.4c) as a fourth line.
9.2.3
Camera Visual Transformations
As discussed in Section 4.5.1, each camera involves a lens that forms a 2D projec-
tion of the scene on the image plane where the sensor is located. Due to this projec-
tion, depth information is lost, and so each point on the image plane corresponds to
a ray in 3D space. Depth information may be obtained from multiple cameras, mul-
tiple views with a single camera, or knowledge of the geometric relation between
several feature points on the target.
Here, the perspective projection model (presented in Section 4.5.2.4) will be
used. Two other projection models are the scaled orthographic projection and the
affine projection. The geometry of the perspective projection model is shown in
Figure 9.2 (see Figure 4.10).
Optical
axis
Object
Image
Image plane
(xim,yim)
lf
(x,y,z)
x
y
Camera
frame 
View
point
o
Figure 9.2 Geometry of camera lens
system ðlf 5 focal lengthÞ.
322
Introduction to Mobile Robot Control

In perspective projection, a point p 5 ½x,y,zΤ whose coordinates are expressed
with respect to the camera coordinate frame Ac, projects onto the image plane point
f 5 ½xim,yimΤ given by:
fðx,y,zÞ 5
xim
yim


5 lf
z
x
y
 
ð9:5Þ
If the point p is expressed with coordinates in an arbitrary frame Aa, then these
coordinates must first be transformed to the camera coordinate frame.
Image feature is any structural feature that can be extracted from an image (e.g., a
contour edge, corner). Usually, an image feature corresponds to the projection of a phys-
ical feature of an object (e.g., the end-effector, a goal object), onto the camera image
plane. Image feature parameter is defined to be any real-valued numerical quantity
which can be determined from one or more image features. An image parameter vector
f is a vector with components fiði 5 1,2,. ..,kÞ the image feature parameters, that is:
f 5 ½ f1,f2,. . .,fkΤAF
ð9:6Þ
where F is the image parameter vector space. The mapping M from the position
and orientation of the WMR or the fixed-robot end-effector to the perspective
image feature parameters, that is:
M 5 Cs ! F
ð9:7Þ
can be found using the projective geometry of the camera. Here, the perspective
projection geometry is used:
f 5 ½xim,yimΤ
where xim and yim are given by Eq. (9.5). The actual form of Eq. (9.7) partly
depends on the relative configuration of the camera and the mobile robot or end-
effector. The two typical camera configurations are shown in Figure 9.3:
G
Onboard camera configuration
G
Fixed camera in the workspace (e.g., on the ceiling)
Clearly, in Figure 9.3 we have:
At
0 5 Ar
0Ac
rAt
c
ð9:8aÞ
At
0 5 Ac
0At
c
ð9:8bÞ
Another camera configuration is that in which the camera is not fixed in the
workspace but mounted on another robot or pan-tilt head so as to observe the visu-
ally controlled robot from the best position. In all cases, camera calibration is
needed before the execution of the visual-control task.
323
Mobile Robot Control V: Vision-Based Methods

9.2.4
Image Jacobian Matrix
The discussion that follows is referred to the end-effector of a fixed robot, but it
also covers the mobile robots’ local coordinate frame. Given a feature parameter
vector: f 5 ½ f1,f2,. . .,fkΤAF, _fði 5 1,2,. . .,kÞ is the corresponding vector of rates of
change _f iði 5 1,2,. . .,kÞ of the feature parameters. Let r be the end-effector coordi-
nate vector in some parameterization of the task (configuration) space Cs and _r the
corresponding velocity, that is, the screw of the end-effector:
_r 5
v
ω


ð9:9Þ
The image Jacobian Jim is defined to be the transformation of _r to _f, that is:
_f 5 JimðrÞ_r
ð9:10Þ
where:
JimðrÞ 5 @f
@r


5
@f1ðrÞ=@r1. . .@f1ðrÞ=@rm
?
@fkðrÞ=@r1. . .@fkðrÞ=@rm
2
4
3
5
ð9:11Þ
yr
yr
zr
zr
c
C
C
ω
ω
(A)
(B)
y
y
y
z
x
y
x
x
x
xr
xr
o
o
o
z
z
Ac
0
Ac
0
Ac
r
c
Ac
r
Ac
r
Ac
r
Ac
0
Ac
0
At
0
At
0
Target
Target
Target
Target
At
c
At
c
At
c
At
c
At
0
At
0
Ar
0
Ar
0
Ar
0
Ar
0
z
Q
Q
o
o
Q
Figure 9.3 Coordinate frames used in vision-based robot control: (A) onboard/eye-in-hand
configuration and (B) fixed camera configuration.
324
Introduction to Mobile Robot Control

where m is the dimension of the space Cs. The image Jacobian matrix is also
known as interaction matrix or feature sensitivity matrix. Equation (9.10) provides
the feature parameters changes corresponding to a change of the robot pose. In
VRC, we need to determine the end-effector velocity _r required to obtain a certain
desired value _fd of _f.
In the following we consider point features, but other cases (such as lines, con-
tours, areas) can be handled by properly extending the present analysis.
To compute JimðrÞ we work as follows. Let p 5 ½x,y,zΤ a point expressed with
respect to the camera frame. This point is assumed to be fixed in the world coordi-
nate frame. Then, from Eq. (9.5) we know that the image parameter vector f is:
f 5
xim
yim


5 lf
z
x
y
 
ð9:12Þ
Differentiating Eq. (9.12) we get:
_f 5
_xim
_yim


5
lf=z
0
2xim=z
0
lf=z
2yim=z


_p
5 Jcðxim,yim,lfÞ_p
ð9:13aÞ
where:
Jc 5
lf=z
0
2xim=z
0
lf=z
2yim=z


ð9:13bÞ
Now, _pðtÞ is given by Eqs. (9.4a) and (9.4b). Therefore, combining Eqs. (9.4a)
and (9.4b) and Eqs. (9.13a) and (9.13b) we get:
_f 5 Jcðxim,yim,z,lfÞJ0ðpÞ_r
5 Jimðxim,yim,z,lfÞ_r
ð9:14Þ
where:
Jimðxim,yim,z,lfÞ 5 Jcðxim,yim,z,lfÞJ0ðpÞ
5
lf
z
0
2xim
z
2ximyim
lf
l2
f 1 x2
im
lf
2yim
0
lf
z
2yim
z
2lz
f 1 y2
im
lf
ximyim
lf
xim
2
66664
3
77775
ð9:15Þ
This expresses the image plane velocity of a point in terms of the velocity of the
end-effector with respect to the camera. The image Jacobian Eq. (9.15) depends on
the distance z of the end-effector (or the target point being imaged, in general). If
325
Mobile Robot Control V: Vision-Based Methods

the target is the end-effector, this distance can be calculated using information
from the camera calibration, and the forward kinematics of the robot. It is remarked
that if the image feature parameters are point coordinates, then the rates _xim and _yim
are image plane velocities.
Example 9.1
It is desired to derive the image Jacobian matrix of a unicycle-type WMR with a pinhole
onboard camera and a target with three feature points in the camera field of view.
Solution
We will apply the procedure described in Section 9.2.4. Consider a camera-equipped WMR
as shown in Figure 9.4, where the target S is identifiable via the three point features D,E,F
on the ðx,yÞ plane [9,10].
Here, we have the following coordinate frames:
Owðxw,ywÞ is world coordinate frame with origin at Ow,
Qðxr,yrÞ is local coordinate frame with origin at Q,
Cðxc,ycÞ is camera coordinate frame with origin at C,
Sðxs,ysÞ is target coordinate frame with origin at S.
The relative position and orientation of these coordinate frames is as shown in
Figure 9.4.
The coordinates xc
m,yc
m of the three features points m 5 fD,E,Fg in the camera frame C are:
xc
m 5 xc
s 1 xs
m cos φc
s 2 ys
m sin φc
s
ð9:16Þ
yc
m 5 yc
s 1 xs
m sin φc
s 1 ys
m cos φc
s
where xc
s,yc
s are the position coordinates, and φc
s the orientation angle, of S with respect
to the camera frame C. The feature points D,E, and F, represented by ðxC
D,yC
DÞ,ðxC
E,xC
EÞ and
yw
xw
xc
xr
xs
ys
ψ
γ
β
β
φ
yc
L1
L2
yr
B
C
F
D
S
E
Q
ow
d
Figure 9.4 WMR with rotating onboard
camera (The target frame Sðxs; ysÞ is
translated along the axis Owxw by a
distance d).
326
Introduction to Mobile Robot Control

ðxC
F,yC
FÞ in the camera frame, are mapped on the image plane (which here is 1D, i.e., a line)
through the forward perspective transformation:
fm 5 lf
yc
m
xc
m
ðm 5 D,E,FÞ
ð9:17Þ
where fm is the image of ðxc
m,yc
mÞ, x is the “depth” variable (replacing the depth variable z
of Eq. (9.12)). Therefore, in our case the sensory data vector is:1
f 5 ½fD,fE,fF,ψΤ
ð9:18aÞ
The full position and orientation vector of the target frame S with respect to the cam-
era frame C is:
xc
s 5 ½xc
s,yc
s,φc
s,ψc
sΤ
ð9:18bÞ
Differentiating Eq. (9.17) we get (for m 5 D,E,F):
_f m 5 lf
_yc
m
xc
m
2 lf
yc
m
ðxc
mÞ2 _xc
m
5
2fm
xc
m
, lf
xc
m
,0
0
2
4
3
5_xc
m
ð9:19aÞ
where:
_xc
m 5 ½_xc
m,_yc
m,_φ
c
m, _ψ
c
mΤ
Now, by Eq. (9.4c) we have:
_xc
m 5
1
0
2yc
m
0
0
1
xc
m
0
0
0
1
0
0
0
0
1
2
664
3
775_xc
s
ð9:19bÞ
where the relations _φ
c
m 5 _φ
c
s and _ψ
c
m 5 _ψ
c
s 5 _ψ were used. Therefore by Eqs. (9.18a) and
(9.18b) and Eqs. (9.19a) and (9.19b) we get:
_f 5 Jc
im_xc
s
ð9:20Þ
where:
Jc
im 5
2fD=xc
D
lf=xc
D
ðl2
f 1 f 2
D Þ=lf
0
2fE=xc
E
lf=xc
E
ðl2
f 1 f 2
E Þ=lf
0
2 fF=xc
F
0
lf=xc
F
0
ðl2
f 1 f 2
F Þ=lf
0
0
1
2
664
3
775
ð9:21Þ
1 The camera angular deviation ψ is assumed to be measured by a proper sensor.
327
Mobile Robot Control V: Vision-Based Methods

We will now find the relation between the state vector velocity:
_x 5 ½_xQ,_yQ,_φ, _ψΤ
ð9:22aÞ
and the velocity vector:
_xc
s 5 ½_xc
s,_yc
s,_φ
c
s, _ψΤ
ð9:22bÞ
of the target frame S in the camera frame C. To this end, we first write the equations for
xQ,yQ and φ from Figure 9.4:
xQ 5 2 xc
s cos φc
s 2 yc
s sin φc
s
2 L1 cosðφc
s 1 ψÞ 2 L2 cos φc
s 1 d
yQ 5 xc
s sin φc
s 2 yc
s cos φc
s 1 L1 sinðφc
s 1 ψÞ 1 L2 sin φc
s
φ 5 2 ðφc
s 1 ψÞ
ð9:23Þ
Differentiating Eq. (9.23) and using the notation _ψ 5 ωψ, we get the relation:
_xc
s 5 J0ðφ,ψ,L1,L2Þ_x
ð9:24aÞ
where:
J0 5
2cosðφ 1 ψÞ
2sinðφ 1 ψÞ
L1 sin ψ
0
sinðφ 1 ψÞ
2cosðφ 1 ψÞ
2ðL1 cos ψ 1 L2Þ
2L2
0
0
21
21
0
0
0
1
2
664
3
775
ð9:24bÞ
Combining (9.20) and (9.24a) we obtain the overall image Jacobian relation from the
state vector velocity _x to the feature rate of change vector _f:
_f 5 Jim_x
ð9:25aÞ
where:
Jim 5 Jc
imJ0
ð9:25bÞ
9.3
Position-Based Visual Control: General Issues
As it is evident from Figure 9.3, the origin Q of the local coordinate frame Q xryr
of a WMR (see, e.g., Figure 2.7A) plays the role of the origin of the end-effector
(flange) coordinate frame of a fixed robot. Therefore, for more generality, our anal-
ysis will be presented for a fixed or mobile manipulator’s end-effector.
Point-to-point positioning—Some point pe on the robot with end-effector coor-
dinates has to reach a fixed stationary point s visible in the scene.
328
Introduction to Mobile Robot Control

Pose-based motion control—Here, the end-effector positioning tasks are directly
determined in terms of a known object pose, which can be defined in terms of sta-
tioning points with respect to the object pose.
A positioning task is represented by an error eðtÞ from the configuration (task)
space Cs to the screw space R6. A positioning task is completed when the
end-effector pose xe satisfies eðxeÞ 5 0. Actually, the error function restricts some
number d , 6 degrees of freedom of the robot, where d is called the degree of con-
straint. The error function can be regarded as a virtual kinematic constraint
between the end-effector and the target.
9.3.1
Point-to-Point Positioning
Consider a fixed camera. Then, the error ep in world coordinates is given by:
ep 5 xeðpeÞ 2 s
ð9:26Þ
where xeðpeÞ expresses the position of the point pe in world coordinates, and is
actually the variable to be controlled. We assume that an estimate ^sc of s is pro-
vided by a calibrated camera with respect to the camera coordinate frame. Then,
using the camera pose ^xc in world coordinates, we have:
^s 5 ^xcð^scÞ
ð9:27Þ
From Eqs. (9.26) and (9.27) it follows that, if no disturbance exists, the propor-
tional negative feedback law:
u 5 2 Kep
5 2 K½xeðpeÞ 2 ^xcð^scÞ
ð9:28Þ
with K a positive definite gain matrix, drives the equilibrium state to a value where
ep 5 0. In practice, xe may also be subject to errors. In these cases, xe in Eq. (9.28)
must be replaced by an estimate ^xe of xe, which may lead to positioning errors.
The case where the cameras are mounted on board and calibrated to the end-
effector can be treated in the same way.
9.3.2
Pose-Based Motion Control
Here, we work as above defining the positioning tasks in terms of the object pose.
We use a desired stationing pose xe;d of the end-effector with respect to the target
coordinate frame. If epose is the positioning error of the actual end-effector pose xe
from the desired one, then again a proportional feedback controller:
u 5 2 Kepose
5 2 Kðxe 2 xe,dÞ,
K . 0
ð9:29Þ
329
Mobile Robot Control V: Vision-Based Methods

stabilizes to zero the error epose. Clearly, the problem in the above control laws is
the estimation of the variables that are used to parameterize the feedback (i.e.,
pe,s). This issue will be discussed later.
9.4
Image-Based Visual Control: General Issues
9.4.1
Use of the Inverse Jacobian
The Jacobian Eq. (9.14) provides the rate of change _f of the image feature para-
meters, perceived in the image plane, using the screw vector _r of translational and
angular velocities of the end-effector. But visual robot control applications require
the inverse, that is, to determine _r from _f. This can be done by solving Eq. (9.14)
for _r, but the solution is not always unique. If JimðrÞ is invertible ðk 5 mÞ the solu-
tion is exact and unique given by:
_r 5 J21
im ðrÞ_f
ð9:30Þ
If k 6¼ m, the inverse Jacobian does not exist. Therefore, we use the (least
squares) generalized Jacobian matrix given by Eqs. (2.8a) and (2.8b), that is:
Jy
im 5 ðJΤ
imJimÞ21JΤ
im when k . m
ð9:31aÞ
Jy
im 5 JΤ
imðJimJΤ
imÞ21 when k , m
ð9:31bÞ
If there are more feature parameters than the task degrees of freedom, that is,
k . m, the algebraic system (9.14) is overdetermined. If k , m the algebraic sys-
tem (9.14) is underdetermined (i.e., there are some components of the object that
cannot be observed, because there are not enough features to determine uniquely
the object velocity _r). Thus, the proper generalized inverse Jacobian is given by
Eq. (9.31b).
If as control vector u we use the vector _r, then in the nonsingular case (9.30) we
have:
u 5 J21
im ðrÞ_f
ð9:32Þ
Therefore, defining the error function eðfÞ as:
eðfÞ 5 fd 2 f
ð9:33Þ
where fd is a desired feature parameter vector to be reached, a convenient propor-
tional (resolved-rate) control law is:
u 5 J21
im ðrÞKeðfÞ
ð9:34Þ
330
Introduction to Mobile Robot Control

where K is a constant positive definite gain matrix of appropriate dimensionality
(usually diagonal). This control law guarantees asymptotic convergence of eðfÞ to
zero. To verify this, we select the Lyapunov function:
VðeðfÞÞ 5 1
2 eΤðfÞeðfÞ . 0
ð9:35Þ
The derivative of V is given by:
_VðeðfÞÞ 5 eΤðfÞ_eðfÞ
ð9:36Þ
where, using Eqs. (9.32)(9.34):
_e 5 2 _f 5 2 JimðrÞu
5 2 JimðrÞ½J21
im ðrÞKeðfÞ
5 2 KeðfÞ
ð9:37Þ
Thus:
_VðeðfÞÞ 5 2 eΤðfÞKeðfÞ , 0
ð9:38Þ
for K positive definite. This proves asymptotic stability of eðfÞ. If k 6¼ m we use the
control law (9.34) with J21
im being replaced by Eq. (9.31a) or Eq. (9.31b) as the case
may be. The asymptotic stability of the error dynamics of ep (see Eq. (9.28)) or epose
(see Eq. (9.29)) can be proved in the same way using the proper Lyapunov functions.
9.4.2
Use of the Transpose-Extended Jacobian
An alternative method to image visual-control design is to use the transpose-
extended Jacobian JΤ
0 instead of the inverse image Jacobian J21
im (or Jy
im). This
method bypasses the problem of inverting the Jacobian, but actually, if a physical
singularity occurs the control may be erroneous, although it will not fail computa-
tionally. The accuracy of this method is better if larger gains can be used.
Consider the Lagrange dynamic model of a robotic manipulator:
DðqÞ€q 1 Cðq; _qÞ_q 1 gðqÞ 5 τ
ð9:39Þ
and the extended Jacobian J0ðqÞ relating the rate of change _f of the image feature
parameter vector f with the joint variable velocity vector _q, that is:
_f 5 J0ðqÞ_q
ð9:40Þ
The control law is assumed to have the form:
τ 5 JΤ
0ðqÞKpeðfÞ 2 Kυ _q 1 gðqÞ
ð9:41Þ
331
Mobile Robot Control V: Vision-Based Methods

where eðfÞ 5 fd 2 f and Kp,Kυ are symmetric positive definite gain matrices.
Using the control law (9.41) in the above robot model, we get the autonomous
closed-loop system (for which _q 5 0 is an equilibrium point):
DðqÞ€q 1 Cðq; _qÞ_q 5 JΤ
0ðqÞKpeðfÞ 2 Kυ _q
which assures that the image-based visual robot control objective:
lim
t!N eðfÞ 5 0
is achieved.
This can be established by using the Lyapunov function:
Vðq; _qÞ 5 1
2 _qΤDðqÞ_q 1 1
2 eΤðfÞKpeðfÞ
The time derivative of V along the trajectories of the closed-loop system is
found to be:
_Vðq; _qÞ 5 2 _qΤKυ _q,
Kυ . 0
ð9:42Þ
where use was made of the property _f 5 J0ðqÞ_q, and the skew symmetricity
property:
_qΤ 1
2
_DðqÞ 2 Cðq; _qÞ


_q 5 0
We see in Eq. (9.42) that _Vðq,_qÞ is negative which, as usual, proves that the
error eðfÞ tends to zero asymptotically.
9.4.3
Estimation of the Image Jacobian Matrix
The main advantage of position-based control is that the desired tasks can be
described in terms of the Cartesian pose, as it is the common practice in robotics.
The principal disadvantage is that the feedback is closed using estimated values of
quantities that are functions of system calibration parameters. Therefore, in many
cases the sensitivity of the controller to calibration errors is very large [11,12].
Pose-based methods provide a generic approach to visual-based control, but often
they are computationally very expensive due to the computation time needed for
solving the orientation problem involved. However, Kalman-filter-based estimation
methods provide a good and fast solution via the use of microprocessor and field-
programmable logic gate array technologies [1316].
In image-based visual control, the key issue is to get accurate estimates of the
image Jacobian matrix, despite the possibly large uncertainties in focal length lf
332
Introduction to Mobile Robot Control

(intrinsic camera parameter), handeye/vehicleeye calibration (extrinsic camera
parameters), depth z of point features, etc.
The estimation of Jim can be split in two parts: estimation of the robot Jacobian
part J0ðpÞ (see Eq. (9.4a)) and estimation of the visual interaction matrix part
Jcðxim,yim,lf,zÞ (see Eq. (9.13b)). In the closed loop, the error dynamics of features
becomes (see Eq. (9.37)):
_eðfÞ 5 2 JimðrÞ^J
y
imðrÞKeðfÞ
where ^JimðrÞ is the estimate of JimðrÞ. In the ideal case we have JimJy
im 5 I, and in
reality (due to the estimation error) we have Jim^J
y
im 6¼ I. Clearly, a sufficient condi-
tion for local convergence is:
Jim^J
y
im . 0
Now, let us consider the problem of estimating the depth z for each considered
point feature. This can be solved using a dynamic state estimator of the type:
_^x5 Fð^x,fÞu 1 gð^x,f; uÞ
where:
x 5 ½xim,yim,1=zΤ
is the actual state, and
^x 5 ½^xim,^yim,1=^zΤ
is the estimated state, with:
f 5 ½xim,yimΤ
being the measured output. This estimator assures that jjxðtÞ 2 ^xjj ! 0 as t ! N, if:
G
the camera has linear velocity different than zero,
G
the linear velocity is not aligned with the projection ray of the point feature under
consideration,
G
there are persistent excitation conditions (i.e., the system is state observable).
Figure 9.5A shows the structure of the nonlinear dynamic state estimator (e.g.,
extended Kalman filter),2 and Figure 9.5B shows how this estimator can be inte-
grated into an image-based visual robot control system [15,16].
2 The Kalman filter is discussed in Section 12.2.3 and the extended Kalman filter in Section 12.8.2
333
Mobile Robot Control V: Vision-Based Methods

Here, it is useful to discuss an important property of the Jacobian matrix
Jim and how it can be exploited. Looking at Eq. (9.14) we see that it can be
written as:
_f 5 Jim,vðxim,yim,zÞv 1 Jim,ωðxim,yim,lfÞω
where Jim,v contains the first three columns of Jim and depends on both the
image coordinates ðxim,yimÞ and the depth z, while Jim,ω contains the last three
columns which are only functions of ðxim,yimÞ and do not depend on the depth z.
This means that errors in z merely cause a scaling of the matrix Jim,v which can
be easily compensated for through fairly simple control procedures. The above
property constitutes the core of the so-called partitioned estimation and control
methods.
The camera velocity ½vΤ,ωΤΤ has six degrees of freedom, but only two values
(xim and yim) are observed in the image. This means that the matrix JimAR2 3 6 has
a null space of dimension 4, that is, the equation:
Jimðxim,yim,lf,zÞα 5 0
Camera
system 
Input
Estimator
+
−
(A)
IBVRC 
controller
z
z
z
f
f
e = f – f
Depth 
observer
Image 
feature 
extraction
Desired
image 
feature 
+
−
Feature 
error
(B)
C
τ
ν
ω
Figure 9.5 Estimation of the depth z: (A) block diagram of the depth estimator and (B)
integration of the estimator into an image-based VRC.
334
Introduction to Mobile Robot Control

has a solution vector 0α0 that lies on a 4D subspace R4. Actually, it can be verified
that the null space of Jim in Eq. (9.15) is spanned by the following four vectors:
ξ1 5 ½xim,yim,lf,0,0,0Τ
ξ2 5 ½0,0,0,xim,yim,lfΤ
ξ3 5 ½ximyimz,2ðx2
im1l2
f Þz,lfyimz,2l2
f ,0,lfximΤ
ξ4 5 ½lfðx2
im1y2
im1l2
f Þz,0,2ximðx2
im1y2
im1l2
f Þz,lfximyim,2ðx2
im1l2
f Þz,l2
f ximΤ
The first vector corresponds to motion of the camera frame along the projection
ray that contains the point p 5 ½x,y,zΤ, and the second vector corresponds to rota-
tion of the camera frame about a projection ray which contains p.
9.5
Mobile Robot Visual Control
Vision-based methods have been applied to solve several robot control problems,
pose-based and image-based. The basic tool in all these methods is the robot and
image Jacobian matrices. As we already know the treatment of nonholonomic
WMRs is more challenging due to the fact that continuous controllers cannot expo-
nentially stabilize them to a desired pose [1721]. In Chapters 5 through 8 we
have presented several ways for treating the pose control problem.
In the present chapter, we will treat several particular problems including pose
stabilization, path following, wall following, target vehicle following, and keeping
a landmark in the camera’s field of view.
9.5.1
Pose Stabilizing Control
The general pose-based visual-control loop of a mobile robot is shown in
Figure 9.1. The controller involves two parts:
1. a standard (nonvision-based) controller,
2. a pose estimator based on image feature measurements provided by one or more cameras.
Here, we will illustrate this scheme as applied to the unicycle-type WMR of
Figure 9.4, where it is desired to asymptotically stabilize to zero the pose:
x 5 ½xQ,yQ,φ,ψΤ
ð9:43Þ
using measurements from the onboard camera and a sensor (e.g., encoder) that
measures ψ.
The kinematic performance of the unicycle-type WMR:
_xQ 5 v cos φ,
_yQ 5 v sin φ,
_φ 5 ω
ð9:44Þ
is described by several equivalent affine models of the form of Eq. (6.1), for exam-
ple, chain model, Brockett integrator model.
335
Mobile Robot Control V: Vision-Based Methods

As an example, we will use the chain model:
_z1 5 u1
_z2 5 u2
_z3 5 z2u1
u1 5 ω
u2 5 v 2 z3u1
ð9:45Þ
which was derived via the variable transformation (2.62):
z0 5 Fðx0Þ,
z0 5 ½z1,z2,z3,
x0 5 ½xQ,yQ,φΤ
z1 5 φ,
z2 5 xQ cos φ 1 yQ sin φ,
z3 5 xQ sin φ 2 yQ cos φ
ð9:46aÞ
with inverse:
x0 5 F21ðz0Þ,
v 5 u2 1 z3u1,
ω 5 u1
ð9:46bÞ
xQ 5 z2 cos z1 1 z3 sin z1,
yQ 5 z2 sin z1 2 z3 cos z1,
φ 5 z1
For this model, we have derived the stabilizing controller (6.81a) using the
invariant manifolds method:
u 5
u1
u2


5 uðz0,tÞ
ð9:47Þ
To ensure that the three point landmarks (see Figure 9.4) remain in the camera’s
field of view, during the mobile robot’s movement, the error z4:
z4 x
ð Þ 5 ψ 2 θ xQ,yQ,φ


θ 5 tg21
yQ 1 L1 sin φ
xQ 1 L1 cos φ 2 d
0
@
1
A 2 φ
ð9:48aÞ
must be stabilized to zero [9]. The time evolution of z4 is described by the relation:
_z4 5 ωψ 2
@θ
@x0
0
@
1
A
@F21
@z0
0
@
1
A_z0
5 ωψ 2
@θ
@x0
0
@
1
A
@F21
@z0
0
@
1
A
1
0
0
1
z2
0
2
64
3
75
u1
u2


ð9:48bÞ
where the function x0 5 F21ðz0Þ is defined by Eq. (9.46b).
336
Introduction to Mobile Robot Control

Having available a feedback stabilizing controller uðz0Þ 5 ½u1,u2Τ for the
chained model (9.45), the unicycle controller ½v,ωΤ is obtained as:
vðx0Þ 5 u2 1 z3u1 5 vðFðx0Þ,tÞ
ð9:49aÞ
ωðx0Þ 5 ωðFðx0Þ,tÞ
ð9:49bÞ
Now, to ensure that z4ðtÞ tends to zero, asymptotically, we must select the con-
trol signal ωψ 5 _ψ in Eq. (9.48b) such that:
_z4 5 2 k4z4 with k4 . 0
Therefore, we select ωψ as:
ωψðxÞ 5 2 k4z4ðxÞ 1 Gðx0ÞuðFðx0Þ,tÞ
ð9:50aÞ
where:
Gðx0Þ 5
@θ
@x0

 @F21
@z0


1
0
0
1
z2
0
2
4
3
5,
x 5
x0
ψ


ð9:50bÞ
The overall state-feedback stabilizing controller UðxÞ of the WMR is:
UðxÞ 5
vðx0Þ
ωðx0Þ
ωψðxÞ
2
4
3
5
ð9:51Þ
where vðx0Þ,ωðx0Þ and ωψðxÞ are given by Eqs. (9.49a), (9.49b), (9.50a), and
(9.50b). This completes the design of the nonvision-based part of the controller
which assumes that the state vector (pose) x 5 ½xQ,yQ,φ,ψΤ is available.
An estimate of x can be constructed using the camera measurements of the fea-
ture points D, E, and F (see Figure 9.4) through the image Jacobian relation (9.25a)
and (9.25b). Assuming that the initial pose is sufficiently close to the desired pose
xd 5 0, the image Jacobian relation (9.25a) and (9.25b) gives:
Δf 5 JimðxdÞΔx,
Jim 5 Jc
imJ0
ð9:52aÞ
where:
Δf 5 f 2 fd,
Δx 5 x 2 xd 5 x
ð9:52bÞ
and fd are the camera data at the desired pose. From Eqs. (9.52a) and (9.52b) we
get the vision-based estimate ^x of x as:
^x 5 J21
im ðxdÞðf 2 fdÞ
ð9:53Þ
337
Mobile Robot Control V: Vision-Based Methods

which, if introduced into Eq. (9.51), gives the overall pose-based visual controller:
Uð^xÞ 5 ½vð^xÞ,ωð^xÞ,ωψð^xÞΤ
ð9:54Þ
The block diagram of the closed-loop vision-based pose control system is shown
in Figure 9.6.
Exactly the same procedure can be applied using other types of stabilizing con-
trollers, and also to the car-like WMR described by Eqs. (2.66)(2.69) with the
controller (6.101).
9.5.2
Wall Following Control
Wall following is actually a particular case of path following. Consider a car-like
WMR which uses an omnidirectional camera as a sensor without use of odometry.
As described in Section 4.5.8, an omnidirectional camera provides 360 field of
view, and there exists a proper geometric mapping from the image plane or any
other desired plane [17,2225].
The kinematic model of the robot in terms of path variables is (Figure 9.7):
_s 5 v1 cos φp
_d 5 v1 sin φp
_φp 5 v1ðtgψÞ=D
_ψ 5 v2
ð9:55Þ
Stabilizing 
controller
Mobile 
robot
Camera
+
−
+
−
xd = 0
x(t)
f(t)
x(t)
U(x)
fd
Jim
–1(xd)
f  – fd
xd – x
Figure 9.6 General structure of the pose-based visual-control system.
ψ
φw
φ
v1
s
d
D
Figure 9.7 WMR wall following ðφw 5 π=2; φp 5 π=2 2 φÞ.
338
Introduction to Mobile Robot Control

where φp 5 φw 2 φ,
φw 5 π=2. The robot is to follow the wall with a piecewise
constant velocity v1ðtÞ.
The output zðtÞ of the system and its derivative are zðtÞ 5 dðtÞ and _zðtÞ 5 _dðtÞ.
Then:
€d 5 v1ðcos φpÞ_φp 5 ðv2
1=DÞðcos φpÞtgψ 5 u
ð9:56Þ
If the desired distance to the wall is d0 (a constant), then the feedback control
law for u is:
u 5 €d0 1 Kυð_d0 2 _dÞ 1 Kpðd0 2 dÞ,
Kp . 0,
Kυ . 0
ð9:57Þ
from which we obtain the feedback steering angle control law:
ψ 5 tg21
D
v2
1 cos φp
Kpðd0 2 dÞ 2 Kυv1 sin φpg

	
"
ð9:58Þ
This control law requires the measurements of the angle φp 5 π=2 2 φ, the distance
d, and the linear velocity v1 5 _d=sin φp. The first two variables can be measured by
“wall detection” and “obstacle detection” sensing, which is performed by processing
the image data. For example, one can apply a Sobel gradient3 to the original image of
the omnidirectional image, considering the resulting edges in the image to be the fea-
tures of interest. Then, assuming that the ground is planar, the distance to the nearest
feature in the sector of interest can be determined from its relative elevation angle of
the mirror. This gives a range map of all the obstacles at frame rate [14,17].
The velocity v1 can be estimated by numerically differentiating the respective dis-
tance provided by the range measurement (see next section). A better velocity estima-
tor can be constructed using an extended Kalman filter, discussed in Section 12.8.2.
9.5.3
LeaderFollower Control
Consider a WMR that follows a leader mobile robot moving in an arbitrary trajec-
tory with unknown velocity (Figure 9.8A). The problem is the WMR (called the
follower) to keep a desired distance ld to the leader vehicle while pointing to it
(i.e., φd 5 0) [18]. Actually, the leaderfollower problem is a special case of the
formation control where a convoy of robots exists [26].
The robot has a fixed mounted camera on the robot center, which looks ahead
and captures the image of a pattern mounted on the leader WMR, with four marks
lying on the corners of a square with known side L (Figure 9.8B). The positions of
the pattern marks on the image are ðxi,yiÞ,i 5 A,B,C,D (in pixels), and are
3 The Sobel gradient method detects the edges by looking for the maximum and minimum in the first
derivative of the image. An edge has the one-dimensional shape of a ramp and calculating the deriva-
tive of the image can highlight its location (see References [79] of Ch.4).
339
Mobile Robot Control V: Vision-Based Methods

considered as the image features. From these features, which are measured by the
camera, we can compute the pose:
x0 5 ½x0,y0,φ0
of the leading (target) vehicle in the camera coordinate frame OcðC,xc,zcÞ.
Referring to Figure 9.9 [18], which shows the horizontal projection of the vision
system, the components of x0 are found to be:
x0 5 xR 1 xL
2
,
z0 5 zR 1 zL
2
,
cos ϕ0 5 xR 2 xL
L
ð9:59Þ
We use reverse perspective imaging projection and obtain the following (see
Figure 4.10A):
xL 5
zL 2 lf
lf
0
@
1
AxA,
xR 5
zR 2 lf
lf
0
@
1
AxB
zL 5 lf
1 1 L
hL
0
@
1
A,
zR 5 lf
1 1 L
hR
0
@
1
A
ð9:60Þ
xw
xB, xD
xA, xc
y
x
yw
yB
yA
yC
yD
A
B
D
C
hL
hR
l
ν
ν0
ϕ
θ
0
Leader
Follower
0
(A)
(B)
Figure 9.8 (A) Leader and follower positions and (B) image of the pattern’s marks.
zc
zR
zL
lf
l
xB xA
xL
xR
xc
x0 (t)
z0 (t)
L
ϕ
θ
φ
Marks B and D
Marks A and C
Image plane
φ0 (t)
Figure 9.9 Horizontal
projection (image) plane of
the vision system (location
of the leader vehicle).
340
Introduction to Mobile Robot Control

where xA 5 xC,
xB 5 xD,
hL and hR are as shown in Figure 9.8B. Using
Eqs. (9.59) and (9.60) we find the relative pose of the follower WMR with respect
to the leader WMR:
tgφ 5 zR 2 zL
xR 2 xL
,
tgϕ 5 x0
z0
θ 5 φ 1 ϕ,
l 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
x2
0 1 z2
0
p
ð9:61Þ
The control objective is to make: l ! ld and ϕ ! ϕd as t ! N using feedback
control laws for v and ω.
To this end, we derive the dynamic equations for the errors:
el 5 ld 2 l and eϕ 5 ϕd 2 ϕ
ð9:62Þ
Projecting the velocities of the leader and follower (v0 and v respectively) on the
line that connects them, we get the dynamic equation for _el:
_el 5 2 v0 cos θ 1 v cos eϕ
ð9:63aÞ
Similarly, taking into account that the angle error velocity _eϕ has three compo-
nents, that is, the angular velocity ω of the follower WMR, and the rotational
effects (interactions) of both WMRs, we get:
_eϕ 5 ω 1 ðv0=lÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 2 cos2 θ
p
1 ðv=lÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 2 cos2 eϕ
q
ð9:63bÞ
To stabilize el and eϕ asymptotically to zero, we must select v and ω in
Eqs. (9.63a) and (9.63b) such that the closed-loop equations are:
_el 5 2 Klel,
_eϕ 5 2 Kϕeϕ,
Kl . 0,
Kϕ . 0
ð9:64Þ
From Eqs. (9.63a), (9.63b), and (9.64) we get the control laws:
vðtÞ 5 ð1=cos eϕÞð2 Klel 1 v0 cos θÞ
ωðtÞ 5 2 Kϕeϕ 2 ð1=lÞðv0 sen θ 1 v sen eϕÞ
ð9:65Þ
where by definition sen a 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 2 cos2 a
p
.
As usual, selecting the Lyapunov function V 5 ð1=2Þðe2
l 1 e2
ϕÞ, we obtain
_V 5 el _el 1 eϕ _eϕ 5 2 ðKle2
l 1 Kϕe2
ϕÞ , 0 which assures that el and eϕ are asymptoti-
cally stabilized to zero.
From Eq. (9.65) we see that the controllers require the knowledge of v0, the lin-
ear velocity of the leader robot. This can be estimated from the camera visual data.
A simple estimation way is to approximate
_e as
_edCðlk 2 lk21Þ=T
where
341
Mobile Robot Control V: Vision-Based Methods

lk 5 lðkΤÞ,
k 5 1,2,3,. . ., and T is a suitable sampling period. Then, Eq. (9.63a), if
solved for v0, gives:
^v0 5 ð1=cos θÞ½v cos eϕ 2 ðlk 2 lk21Þ=T
ð9:66Þ
which uses visual range data for the distance l.
It is noted that for successful control, the follower robot should have weaker
curvature constraint, that is:
κfollower $ κleader
where κ 5 1=R 5 ω=v (R 5 the instantaneous curvature radius of the robots).
9.6
Keeping a Landmark in the Field of View
The problem is to develop a control law that enables a unicycle-type WMR to
move on a piecewise smooth trajectory, while keeping a landmark in the camera’s
field of view [27]. The point landmark is located at the origin, the camera optical
center is above the origin of the robot’s local frame origin, and the camera optical
axis is parallel to the robot axis xr.
The robot’s kinematic model is:
_x 5 v cos φ,
_y 5 v sin φ,
_φ 5 ω
ð9:67Þ
The robot must be pointed toward the landmark, in order to keep the landmark lying
at the origin within the field of view of the camera. This is so if (Figure 9.10) [27]:
φ 1 ψ 5 π 1 tg21ðy=xÞ 5 π 1 θ
ð9:68Þ
The field of view of the camera ranges from 2 ~ψ to 1 ~ψ, as measured from the
robot platform xr-axis. If 2 ~ψ , ψ , ~ψ, then the landmark is visible in the image.
The two steps of the controller are [27]:
G
rotation of the robot until the image of the point landmark at the origin is on the edge of
the image (i.e., ψ 5 6 ~ψ),
G
forward or backward motion of the robot while keeping the image point on the edge of
the image.
This results in a curved path, called T-curve, which is described by:
ρ 5 ρ0 expfðθ0 2 θÞ=tgψg
ð9:69Þ
where ðρ0,θ0Þ is a point via which the T-curve passes. We have two T-curves one
at ψ 5 2 ~ψ and the other at ψ 5 ~ψ. Respecting the constraint ρmin , ρ , ρmax, by
342
Introduction to Mobile Robot Control

alternating between these two T-curves, the robot can move within an annulus A
centered on the landmark. This set of T-curves is called an S-curve. The S-curve
allows the robot to move between any two points in the annulus while keeping the
landmark in the field of view. This can be shown using the reachability and con-
trollability concept of affine systems. As shown in Example 6.5, using Lie brackets,
the unicycle system is locally reachable from everywhere and controllable. This
was done using the two standard unicycle fields:
g0
1 5 ½cos φ,sin φ,0Τ,
g2 5 ½0,0,1Τ
The question is whether this is also true when the robot motion is constrained to
the two available T-curves, and find a proper relevant stabilizing controller.
Referring to Figure 9.10A, the polar kinematic model of the robot is:
_ρ
_θ
_φ
2
4
3
5 5
cosðθ 2 φÞ
ð1=ρÞsinðθ 2 φÞ
0
2
4
3
5v 1
0
0
1
2
4
3
5ω 5 g1
1v 1 g2ω
Now, using the constraint φ 1 ψ 5 π 1 θ (which assures that the robot points
toward the landmark), and ψ 5 6 ~ψ (at the edge of the image), we get:
cosðθ 2 φÞ 5 2 cosðψÞ,
sinðθ 2 φÞ 5 sinðψÞ,
_φ 5 _θ and so the above polar model
splits in two models, one valid at ψ 5 2 ~ψ and the other at ψ 5 ~ψ:
_ρ
_θ
_θ
2
4
3
5 5
2cosð ~ψÞ
ð1=ρÞsinð ~ψÞ
ð1=ρÞsinð ~ψÞ
2
4
3
5v 1
0
0
1
2
4
3
5ω 5 g1
1,1v 1 g2ω
ð9:70aÞ
ρmin
ρmax
y
y
x
x
H1
H2
H3
φ
ψ
+ψ −ψ
Θ
0
1
2
−1
−2
−2
−1
0
1
2
(B)
(A)
(C)
Figure 9.10 (A) Geometry of
robot with the camera, (B) an
S-curve for ψ 5 π=6, and (C)
switching surfaces H1, H2,
and H3.
343
Mobile Robot Control V: Vision-Based Methods

_ρ
_θ
_θ
2
4
3
5 5
2cosð2 ~ψÞ
ð1=ρÞsinð2 ~ψÞ
ð1=ρÞsinð2 ~ψÞ
2
4
3
5v 1
0
0
1
2
4
3
5ω 5 g1
1,2v 1 g2ω
ð9:70bÞ
The vector fields g1
1,1 and g1
1,2 describe the velocity directions along the
T-curves. The field g2 rotates the robot to get a desired value of φ. It is now easy
to show that the field distribution:
Δ 5 ff1,f2,f3g 5 fg1
1;1; g1
1;2; g2g
satisfies the reachabilitycontrollability condition of Theorem 6.8, and so the sys-
tem is locally reachable from any point in the annulus A, and controllable. The ini-
tial posture of the robot is Xð0Þ 5 ½ρð0Þ,θð0Þ,φð0ÞΤ, and it is assumed (without loss
of generality) that the desired value of θ is θd 5 0. Define the following three
switching surfaces:
H1 5 fX 5 ½ρ,θ,φΤ
with
ρ 5 ρmaxg
H2 5 fX 5 ½ρ,θ,φΤ
with
ρ 5 ρming
H3 5 fX 5 ½ρ,θ,φΤ
with
θ 5 θdg
ð9:70cÞ
If θð0Þ . θd (i.e., if the robot must move clockwise), then the robot starts follow-
ing the flow of the vector field:
f0 5 ½~v cos ~ψ,2ð1=ρÞ~v sin ~ψ,2ð1=ρÞ~v sin ~ψΤ,
~v . 0
ða constant velocityÞ
The state variable ρ increases and the state variable θðtÞ decreases. When the
robot contacts the surface H1, then the system switches to the vector field:
fH1 5 ½2 ~v cosð2 ~ψÞ, ð1=ρÞ~v sinð2 ~ψÞ, ð1=ρÞ~v sinð2 ~ψÞΤ
which decreases both ρðtÞ and θðtÞ. If the robot state contacts the surface H2 the
controller switches to follow the vector field:
fH2 5 f0
If the state contacts H3, then the controller is switched to follow the vector field:
fH3 5 ½2k cosð0Þ½ρðtÞ2ρd,2ð1=ρÞ~v sinð0Þ,2ð1=ρÞ~v sinð0ÞΤ
5 ½2kðρðtÞ2ρdÞ,0,0Τ
where the gain k is positive. A similar switching control procedure is applied when
θð0Þ , θd.
344
Introduction to Mobile Robot Control

Defining a Lyapunov function Vθ 5 ð1=2Þe2
θ, where eθðtÞ 5 θðtÞ 2 θd, it is easy to
show that along the field fH1, _Vθ 5 eθð~v=ρÞsinð2 ~ψÞ , 0 with ρmin , ρ , ρmax. The
same is true when following the field fH2. Thus, during the motion along the fields
fH1 and fH2, θ converges to θd, in finite time. When, for θðtÞ 5 θd, the state is on H3,
the system switches to follow the field fH3. Defining a new Lyapunov function
V 5 Vθ 1 ð1=2Þe2
ρ, where eρ 5 ρðtÞ 2 ρd, we find that along the flow fH3, that is,
along _ρ 5 2 keρ and _θ 5 0, we have _V 5 2 ke2
ρ , 0. Therefore, when θðtÞ 5 θd, we
have ρðtÞ ! ρd along the field fH3. Finally, at this point the system can be controlled
to go asymptotically to the desired orientation φd, following the vector field
g2 5 ½0,0,ωΤ.
Overall, the above switching control scheme assures that any initial pose
Xð0Þ 5 ½ρð0Þ,θð0Þ,φð0ÞΤ
is
driven
asymptotically
to
any
desired
goal
pose
Xd 5 ½ρd,θd,φdΤ, as t ! N.
The controller needs a single point landmark, the availability of the distance to
the origin (measured by the camera), and the ability to switch instantly between the
T-curves. It assures that while the robot is moving within the task space (i.e.,
within the annulus defined by ρmin and ρmax), the landmark is kept in the camera’s
field of view.
The vision-based control algorithm is as follows [27]:
Step 1: The orientation angle φ is increased until the landmark lies on the left edge of the
image.
Step 2: When ψ 5 ~ψ, the robot moves backward and steered so as the image of the land-
mark is kept on the left edge.
Step 3: When ρ 5 ρmax, φ is decreased to move the landmark on the right edge of the
image.
Step 4: When ψ 5 2 ~ψ, the robot moves forward and steered so as the landmark remains
on the right edge of the image.
Step 5: When the robot is on the radial line with θ 5 θd the robot is turned so as ψ 5 0,
and driven toward ρd.
An analogous sequence is applied when 2π , θð0Þ # θd, with the robot moving
counterclockwise around the workspace. The switching surfaces are reached when
ρ 5 ρmin, ρ 5 ρmax and θ 5 θd (see Figure 9.10C).
The vision-based implementation of this method together with experimental
results for the case where the landmark is a square of coplanar points is provided in
Ref. [27]. In the experiments, an image of the landmark taken at the goal position
xd and knowledge of the feature point at the goal were assumed to be available.
Therefore, the homography between images of planar points could be used to esti-
mate
the
robot
state
xðtÞ.
The
robot
started
from
the
position
xð0Þ 5 ½ρ, θ, φT 5 ½2:75, π=3, 5π=6T
and
the
goal
position
was
xd 5 ½21:2, 0, 0T. The application of the above switching controller has led to a
bounded and periodic distance error eρ 5 ρðtÞ 2 ρd and an angle error eθ 5 θðtÞ 2 θd
decreasing asymptotically to zero with time. Then, the remaining error in ρðtÞ was
regulated to zero. These results showed the ability of the switching controller to
keep the landmark in the camera field of view. The details of the pose
345
Mobile Robot Control V: Vision-Based Methods

reconstruction using the homography between images are given in Ref. [27].
A more general homography-based control methodology for WMRs with nonholo-
nomic and field-of-view constraints is presented in Ref. [28]. In this methodology,
the control laws are directly expressed in terms of the individual entries in the
homography matrix (i.e., the pose parameters are not estimated using the homogra-
phy between images as in other approaches). The methodology is applied for the
development of specific control laws for the three standard types of paths, namely,
circular, straight line segments, and logarithmic spirals. The control law appropriate
to each case is selected via tomography decomposition prior to initiating the
navigation.
Example 9.2
Throughout the book we have seen that selecting the controller such that the closed-loop
dynamics of the error e, between the desired and actual variable under control, has the
form:
_eðtÞ 5 2 KeðtÞ,
K . 0
assures that eðtÞ goes to zero asymptotically as t ! N. To avoid the saturation of the
controller, in many practical cases, we multiply the gain K by the hyperbolic tangential
function tghðμeðtÞÞ instead of eðtÞ [18].
(a) Show that using the modified control law, convergence of the error to zero is still
assured.
(b) Investigate the robustness of the modified controller against measurement errors, in
the mobile robot leaderfollower controller of Section 9.5.3.
Solution
(a) In the present case, the closed-loop error dynamic equation becomes:
_eðtÞ 5 2 K tghðμeðtÞÞ
ð9:71Þ
To show that eðtÞ ! 0, we select as usual the Lyapunov function:
V 5 ð1=2Þe2
Then, we find:
_V 5 e_e 5 2 K etghðμeÞ
5 2 K0xtghðxÞ
ð9:72Þ
where x 5 μe and K0 5 K=μ. The function tghðxÞ is defined as:
y 5 tghðxÞ 5 ðex 2 e2xÞ=ðex 1 e2xÞ
and has the plot of Figure 9.11.
346
Introduction to Mobile Robot Control

We observe that y 5 tghðxÞ has the following properties:
tghð0Þ 5 0
tghðxÞ . 0
for
x . 0
tghðxÞ , 0
for
x , 0
Therefore, xtghðxÞ . 0 for x 6¼ 0, which implies that _V in Eq. (9.72) has the proper-
ties as:
_Vð0Þ 5 0
_VðeÞ , 0
for
e 6¼ 0
This assures that the feedback controller (9.71) stabilizes asymptotically to zero
the error eðtÞ.
(b) As seen in Section 9.5.3 [18], the controllers in Eq. (9.65) assume the availability of
the velocity v0 of the leader robot which can be estimated by the vision system. Let
^v0 be the estimate, and ~v0 5 v0 2 ^v0 the estimation error. Then:
v0 5 ^v0 1 ~v0
ð9:73Þ
Using Eq. (9.73) in Eqs. (9.63a) and (9.63b) we get:
_el 5 2 ð^v0 1 ~v0Þcos θ 1 v cos eϕ
5 ð2 ^v0 cos θ 1 v cos eφÞ 2 ~v0 cos θ
ð9:74aÞ
_eϕ 5 ω 1 ½ð^v0 1 ~v0Þ=l
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 2 cos2 θ
p
1 ðv=lÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 2 cos2 eϕ
q
5 ω 1 ð^v0=lÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 2 cos2 θ
p
1 ðv=lÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 2 cos2 eϕ
q
1 ð~v0=lÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 2 cos2 θ
p
ð9:74bÞ
Therefore, selecting v and ω such as:
2^v0 cos θ 1 v cos eϕ 5 2 FlðelÞ
ð9:75aÞ
ω 1 ð^v0=lÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 2 cos2 θ
p
1 ðv=lÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 2 cos2 eϕ
q
5 2 FϕðeϕÞ
ð9:75bÞ
where:
FlðelÞ 5 KltghðμlelÞ,
FϕðeϕÞ 5 KϕtghðμϕeϕÞ
ð9:75cÞ
x
y
0
1
−1
Figure 9.11 Graphical representation of the hyperbolic tangential
function.
347
Mobile Robot Control V: Vision-Based Methods

we get, from Eqs. (9.74a) and (9.74b):
_el 5 2 FðelÞ 2 ~v0cos θ
ð9:75dÞ
_eϕ 5 2 FðeϕÞ 1 ð~v0=lÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 2 cos2 θ
p
ð9:75eÞ
Now, if we select the Lyapunov function:
V 5 Vl 1 Vϕ,
Vl 5 ð1=2Þe2
l ,
Vϕ 5 ð1=2Þe2
ϕ
we get:
_Vl 5 el _el 5 2 ½elFðelÞ 1 el ~v0 cos θ
ð9:76aÞ
_Vϕ 5 eϕ _eϕ 5 2 ½eϕFðeϕÞ 2 eϕð~v0=lÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 2 cos2 θ
p

ð9:76bÞ
From Eq. (9.76a) we see that _Vl , 0 if:
elFlðelÞ . jeljj~v0 cos θj
ð9:77Þ
When t ! N, we have jFlðelÞj ! Kl and so Eq. (9.77) gives Kl . j~v0j. For small el
(in the linear region) we obtain:
FlðelÞ 5 Flð0Þ 1 Fl
0ð0Þel
5 Klμl sec2 ð0Þel 5 Klμlel
Therefore, Eq. (9.77) gives:
Klμl . j~v0j
which implies that eðtÞ converges to a neighborhood of the origin with radius:
σl 5 j~v0j=Klμl
ð9:78aÞ
In the same way, from Eq. (9.76b) it follows that _Vϕ , 0 if eϕFðeϕÞ . jeϕjj~v0=lj,
which in the saturation state ðt ! NÞ becomes:
Kϕ . j~v0j=l
For eϕ in the linear region, we find (after some calculation) that eϕ tends to a
neighborhood of zero with radius [18]:
σϕ 5
j~vjKlμl
KϕðKlμll 2 j~v0jÞ
ð9:78bÞ
for Klμll . j~v0j.
348
Introduction to Mobile Robot Control

9.7
Adaptive Linear Path Following Visual Control
9.7.1
Image Jacobian Matrix
Here, an adaptive vision-based control scheme for straight path following will be
presented using a camera mounted on the center of mass of the WMR [21,29]. The
image plane ½xim,yim is parallel to the ground and the camera points downwards.
The coordinate frames needed are Owxwywzw (world coordinate frame), Orxryrzr
(local WMR frame), Ocxcyczc (camera frame), Opxpypzp (path frame), and
Oimximyimzim (image frame). Figure 9.12 shows the coordinate frames Oc and Oim,
and the image feature parameterization [21,29].
Consider a straight line path LG on the ground parameterized with the para-
meters k0 and λ0 as:
yL
w 5 k0xL
w 1 λ0
ð9:79Þ
The camera image of LG is also a straight line described by a similar equation:
yL
im 5 kimxL
im 1 λim
ð9:80aÞ
using the parameters kim and λim, or by the polar form equations:
xL
im 5 ρ cos θ,
yL
im 5 ρ sin θ
ð9:80bÞ
xc
yim
ximj
yc
yr = yc
xr = xc
 = [xp
im,yp
im]
zc
oc
oim
xim
Pim
Pc = [xp
c,yp
c,–h]T
Lim
yim
oim
lf
c
h
l
θ
ρ
Viewpoint
Image plane
Image
Camera view
(A)
(B)
xw
ow
yw
LG
φ
ν
α
Figure 9.12 (A) Coordinate frames and (B) image feature parameterization.
349
Mobile Robot Control V: Vision-Based Methods

with parameters ρ and θ. The polar relations in Eq. (9.80b) can also be written in
the form of Eq. (9.80a) as:
xL
im cos θ 1 yL
im sin θ 5 ρ
Working with the polar coordinates ρ and θ, which are found from ½xw,yw,zwΤ
and Eq. (9.80a), using the related perspective transformation xL
im 5 μxL
w,yL
im 5 μyL
w
from the camera to the ground, we find that ρ and θ are given by (see
Figure 9.12B):
ρ 5 μ k0xL
w 1 λ0 2 yL
w
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 1 k2
0
p
sgnðk0Þ
ð9:81aÞ
θ 5 2 a 2 tg21ð1=k0Þ
ð9:81bÞ
a 5 φ 2 90
ð9:81cÞ
where the parameter μ is defined by:
μ 5 lf=h
ðaspect ratioÞ
with lf being the camera’s focal length, and h the distance of the camera from the
ground. Differentiating Eqs. (9.81a) and (9.81b) with respect to time we get:
_ρ
_θ


5
μk0sgnðk0Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 1 k2
0
p
2μ sgnðk0Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 1 k2
0
p
0
0
0
21
2
64
3
75
_xw
_yw
_α
2
4
3
5
ð9:82Þ
Now, the WMR of Figure 9.12B is described by the kinematic model (see
Eq. (9.81c)):
_xw
_yw_φ
2
4
3
5 5
cos φ
0
sin φ
0
0
1
2
4
3
5 v
ω


or
_xw
_yw
_α
2
4
3
5 5
2sin α
0
cos α
0
0
1
2
4
3
5 v
ω


ð9:83Þ
Combining Eqs. (9.82) and (9.83) and using the relation [21,29]:
k0 sin α 1 cos α 5 2 sgnðκ0Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 1 k2
0
q
sin θ
which can be verified using (9.81b), we get the image Jacobian model:
_f 5 Jim_r
ð9:84aÞ
350
Introduction to Mobile Robot Control

f 5
ρ
θ


,
_r 5
v
ω


,
Jim 5
μ sin θ
0
0
21


ð9:84bÞ
If the camera is placed elsewhere, Jim takes the form:
Jim 5
μ sin θ
hðθÞ
0
21


ð9:85aÞ
hðθÞ 5 μl sinðζ 1 θ 2 γÞ
ð9:85bÞ
where l is the distance between the vehicle’s center of mass and the center of cam-
era, ζ is the angle between xr and xim, and γ is the angle between xr and
~
QOc(see
Figure 9.12).
9.7.2
The Visual Controller
The image Jacobian model in Eqs. (9.84a) and (9.84b) describes the kinematics of
the entire system including the robot and the camera, and will be used in the design
of the controller. The controller design develops in three steps (backstepping proce-
dure), as described in Sections 5.4 and 7.3, namely:
G
Design of the kinematic controller
G
Design of the dynamic controller
G
Design of the adaptive controller
embedding a parameter adaptation scheme to the dynamic controller.
9.7.2.1
Kinematic Controller
It is assumed that the feature vector f 5 ½ρ,θΤ is measured by the camera. For prop-
erly approaching the path, we use an approach angle ψðρÞ defined as [29,30]:
ψðρÞ 5 2 signðvÞ e2kψρ 2 1
e2kψρ 1 1 θa,
ψð0Þ 5 0
ð9:86Þ
where v 6¼ 0 is a given linear velocity, kψ . 0 is a constant gain, and θa # π=2.
Clearly, when ρ is away from zero, ψðρÞ is approximately equal to θα. Then using
the Lyapunov function:
V1 5 ð1=2Þðθ2ψÞ2
ð9:87aÞ
we find that the feedback kinematic law:
ωθ 5 2 Kθðθ 2 ψÞ 1 _ψ,
Kθ . 0
ð9:87bÞ
351
Mobile Robot Control V: Vision-Based Methods

gives:
_V1 5 ðθ 2 ψÞð_θ 2 _ψÞ 5 ðθ 2 ψÞðωθ 2 _ψÞ
ðωθ 5 _θ Þ
5 2 Kθðθ2ψÞ2 , 0
ð9:87cÞ
for Kθ . 0, which implies that in the closed-loop ρ and θ converge asymptotically
to zero. Now, since by Eq. (9.81b) _θ 5 ωθ 5 2 _a 5 2 ω, in terms of ω the control-
ler (9.87b) becomes:
ω 5 Kθðθ 2 ψÞ 2 _ψ
ð9:88Þ
9.7.2.2
Dynamic Controller
The dynamic model of the WMR is (see, e.g., Eq. (7.66b)):
_v 5 ð1=mrÞu1,
_ω 5 ð2a=IrÞu2
ð9:89Þ
where u1 5 τr 1 τl,
u2 5 τr 2 τl and τr,τl are the right wheel and left wheel motor
torques. The problem is to design feedback control laws for u1 and u2 so as to drive
vðtÞ to zero, and ωðtÞ to ωðtÞ, where:
ωðtÞ 5 Kθðθ 2 ψÞ 2 _ψ
ð9:90Þ
as specified by Eq. (9.88). Working in the usual way, we use the candidate
Lyapunov functions:
Vv 5 1
2 ðv2vdÞ2,
Vω 5 1
2 ðω2ωÞ2
Differentiating Vv with respect to time gives:
_Vv 5 ðv 2 vdÞð_v 2 _vdÞ 5 ðv 2 vdÞ
1
mr u1 2 _vd


Therefore, selecting u1 as:
u1 5 mr½_vd 2 Kvðv 2 vdÞ
ð9:91Þ
gives
_Vv 5 2 Kvðv2vdÞ2 which is negative for Kv . 0. Consequently, vðtÞ con-
verges to vd asymptotically.
Similarly:
_Vω 5 ðω 2 ωÞð _ω 2 _ωÞ
5 ðω 2 ωÞ½ð2a=IrÞu2 2 Kθð_θ 2 _ψÞ 1 €ψ
ð9:92Þ
352
Introduction to Mobile Robot Control

which, selecting u2 as:
u2 5 ðIr=2aÞ½Kθð_θ 2 _ψÞ 2 Kωðω 2 ωÞ 2 €ψ
ð9:93Þ
becomes:
_Vω 5 2 Kωðω2ωÞ2 , 0 for Kω . 0
ð9:94Þ
Thus, the feedback control law in Eq. (9.93) assures that ω ! ω, asymptoti-
cally as t ! N. The controllers in Eqs. (9.91) and (9.93) need the availability of
θ, ψ (i.e., ρ), and v which are measured by the vision system and the robot position
and velocity sensors. The block diagram of the closed-loop vision-based system is
shown in Figure 9.13.
9.7.2.3
Adaptive Controller
The design of the adaptive vision-based controller is performed as described in
Section 7.3.1. Assuming that only m and I are unknown, the controllers in
Eqs. (9.91) and (9.93) are written as:
u1 5 ^mr½_vd 2 Kvðv 2 vdÞ
ð9:95aÞ
u2 5 ð^Ir=2aÞfKθð_θ 2 _ψÞ 2 Kω½ω 2 Kθðθ 2 ψÞ 1 _ψ 2 €ψg
ð9:95bÞ
where (see Eqs. (9.84a), (9.84b), and (9.86)):
_ψ 5 s1ðtÞμ,
s1ðtÞ 5 ðϑψ=ϑρÞðsin θÞv
ð9:96aÞ
€ψ 5 s1ðtÞ_μ 1 s2ðtÞμ2,
s2ðtÞ 5 ðϑs1=ϑρÞðsin θÞv
ð9:96bÞ
Vision 
system 
Jim
∫
ψ(ρ)
ψ
..
ψ
.
θ
∫
ψ
∑
∑
∑
Kv
u1
u2
∑
1
−
−
+
+
+
+ −
∫
∑
Kθ
Kθ
d dt
mr
mr
∑
∑
Kω
−
−
+
−
−
+
−
+
ρ
d dt
.ρ
.ν
.νd
.ω
.
θ
ω∗
ω
ν
νd
2α Ir
Ir
2α
Figure 9.13 Block diagram of vision-based path following system.
353
Mobile Robot Control V: Vision-Based Methods

Introducing the controllers (9.95a) and (9.95b) in the system dynamic equations
(9.89) we get:
_v 5 ð ^m=mÞ_vd 2 ð ^m=KvmÞðv 2 vdÞ
5 β1 _vd 1 β2ðv 2 vdÞ
ð9:97aÞ
_ω 5 ð^I=IÞ 2 ðKθ 1 KωÞðω 1 _ψÞg 1 KθKωðθ 2 ψÞ 2 €ψ


5 β3ðω 1 _ψÞ 1 β4ðθ 2 ψÞ 1 β5 €ψ
ð9:97bÞ
where:
β1 5 ^m=m,
β2 5 2 Kvβ1
ð9:98aÞ
β3 5 2 ðKθ 1 KωÞð^I=IÞ,
β4 5 KθKωð^I=IÞ,
β5 5 2 ð^I=IÞ
ð9:98bÞ
The
dynamic
relations
in
Eqs.
(9.97a)
and
(9.97b)
with
parameters
βiði 5 1,2,3,4,5Þ
are
similar
to
Eqs.
(7.36a)
and
(7.36b)
with
parameters
βiði 5 1,2,3,4Þ. Therefore, the adaptation laws can be derived in the same way and
have the general form of Eqs. (7.40a), (7.40b), and (7.41). If the aspect ratio μ is
also unknown it can be included in the parameters to be estimated, in which case
the functions _ψ and €ψ in Eqs. (9.96a) and (9.96b) are replaced by their estimates:
_^ψ5 s1ðtÞ^μ
ð9:99Þ
€^ψ5 s1ðtÞ_^μ1 s2ðtÞ^μ2
ð9:100Þ
The general procedure remains the same. The reader may also derive, as an
exercise, an adaptive vision-based neurocontroller following the results of
Section 8.5.
Example 9.3
(a) Derive the image Jacobian of a differential-drive WMR assuming that the rigidly
mounted camera optical axis z is identical with the direction of the linear velocity v.
(b) Extend this matrix to include as an extra feature the actual measured distance ρ
between the camera and the target (landmark) where ρ replaces z.
Solution
(a) According to the requirement about the camera optical axis z the configuration of
the WMR and camera is as shown in Figure 9.14A [31].
We have the following coordinate frames:
Owxwywzw (world coordinate frame),
Q xryrzr (local coordinate frame).
C xcyczr (camera coordinate frame with zc-axis identical to zr).
354
Introduction to Mobile Robot Control

In the present case, the velocity vectors vðtÞ and ωðtÞ in Eq. (9.1) are:
vðtÞ 5 ½vx,vzΤ,
ωðtÞ 5 ωy
Therefore, only the columns of Jim in Eq. (9.15), which correspond to vx,vz, and ωy
should be retained, that is:
Jimðxim,yim,z,lfÞ 5
lf
z
2xim
z
l2
f 1 x2
im
lf
0
2yim
z
ximyim
lf
2
6664
3
7775
ð9:101aÞ
that is:
_f 5
_xim
_yim


5 Jimðxim,yim,z,lfÞ
_xQ
_zQ_φ
2
4
3
5
ð9:101bÞ
where:
_xQ 5 v cos φ,
_zQ 5 v sin φ,
_φ 5 ωy
ð9:101cÞ
(b) We assume that the actual distance ρ of the landmark from the camera is measured
by a range-finder camera [31,32]. Referring to Figure 9.15 we find:
ρim=ρ 5 lf=z
that is:
z 5 ρlf=ρim,
ρim 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
x2
im 1 y2
im 1 l2
f
q
ð9:102Þ
Image plane
Object
Image (xim, yim)
Camera
frame
View point
xw
lf
zw
yc
zc
xc
(xc,yc,zc)
zr
yr
Q
C
yw
xQ
zQ
φ
ν
0
(B)
(A)
Figure 9.14 (A) Coordinate frames and (B) perspective projection geometry.
Source: Reprinted and adapted from [31], with permission from European Union Control
Association.
355
Mobile Robot Control V: Vision-Based Methods

Therefore, Jim in Eq. (9.101a) becomes:
Jim 5
ρim
ρ
2ximρim
ρlf
l2
f 1 x2
im
lf
0
2yimρim
ρlf
ximyim
lf
2
6664
3
7775
ð9:103Þ
Now, if we use ρim as an extra feature, the rate of change _ρim should be expressed in
terms of ½_xQ,_yQ,_φΤ, and added as a third component to the feature vector velocity _f.
From Eq. (9.102) we obtain:
_ρim 5 1
ρim
ðxim _xim 1 yim _yimÞ
5 1
ρim
xim,yim
½
 _xim
_yim


5 1
ρim
xim,yim
½
Jim
_xQ
_yQ
_φ
2
64
3
75
5 ½xim=ρ, 2 ðx2
im 1 y2
imÞ=ρlf,ximρim=lf_x
ð9:104Þ
Therefore, the increased image Jacobian relation is:
_f 5 Jim_x
ð9:105Þ
where _f 5 ½_xim,_yim,_ρimΤ,
_x 5 ½_xQ,_yQ,_φΤ
Jim 5
ρim
ρ
2ximρim
ρlf
l2
f 1 x2
im
lf
0
2yimρim
ρlf
ximyim
lf
xim
ρ
2x2
im 1 y2
im
ρlf
ximρim
lf
2
6666666664
3
7777777775
ð9:106Þ
This is the desired extended image Jacobian matrix of the WMR.
Target point
ρim
(xim,yim)
Image plane
Camera
lf
P(xc,yc,zc)
z
xc
yc
zc
ρ
Figure 9.15 Geometry of camera target and
image configuration.
Source: Reprinted and adapted from [31],
with permission of European Union Control
Association.
356
Introduction to Mobile Robot Control

9.8
Image-Based Mobile Robot Visual Servoing
In Sections 9.59.7, the problems of pose stabilization, wall following, vehicle
leaderfollower,
etc.
were
treated
using
position-based
visual
control
(Figure 9.1A), where the vision system provides estimates of the parameters that
are needed to implement conventional controllers. Here, we will examine the prob-
lem of pose control (parking, docking) using image-based visual control where the
controller is designed and implemented employing directly the visual data as shown
in Figure 9.1B [33,34].
The main problem for the visual control of differential-drive WMR (and any
other nonholonomic vehicle) is to keep the target always visible while the robot is
moving. Two methods for keeping the target (landmarks) in the camera field of
view were presented in Sections 9.5.1 and 9.6. The method of Section 9.5.1 belongs
to the general method which uses a range-finder camera mounted on a pan-tilt
head, and controls the pan-angle so as to keep the features in the camera field of
view. Since the camera rotates independently of the platform in order to track the
target features, there occurs a difference angle ψ between the orientations of the
camera and the platform as shown in Figure 9.16 (see also Figure 9.4). Therefore, a
special care is required to control ψðtÞ. This needs the computation of the coordi-
nates of the landmarks on the image plane.
To implement the closed-loop system, the images are monitored continuously
while the robot is moving, using the camera vision system. Assuming that the
image Jacobian matrix for each feature point f is given by Eq. (9.106), and invert-
ing Eq. (9.105), we get:
_x 5 J
y
im_f
Typically, for reducing the camera measurement error (or noise) effects, we use
more feature points than the number n of degrees of motion of the WMR (i.e.,
k . n). Here, a minimum of four feature points will be used (since n 5 3). Since
Jim is full rank (rank Jim 5 n), Jy
im is given by Eq. (9.31a):
J
y
im 5 ðJ
Τ
imJimÞ21J
Τ
im
φ
ψ
ν
xw
zc
yw
xc
xQ
yQ
0
Figure 9.16 Differential-drive WMR with a camera
mounted on a pan-tilt head.
357
Mobile Robot Control V: Vision-Based Methods

The feedback control law is:
_x 5 J
y
imKeðfÞ,
eðfÞ 5 fd 2 f
where fd is the desired feature vector (corresponding to the desired pose of the
robot). The desired vector fd is determined before the application of the control by
generating and storing a trajectory that leads from the starting pose to the desired
pose employing the typical robotics technique of “teaching-by-doing.” The block
diagram of the complete image-based WMR visual servoing system is shown in
Figure 9.17, where a dynamic controller is also in an inner loop.
The orientation angle difference ψ between the platform and the rotating camera
is included in the image-based controller by using the concept of an infinite “virtual
image plane,” which is obtained by rotating the physical image plane by the varying
angle ψ (which is continuously measured by the camera) [33,34]. In this way, the
platform and the camera point in the same direction, and the controller gains Jy
imK
can be used in the standard way by introducing to Jy
im the transformed feature values
corresponding to the virtual image plane. This is conceptually shown in Figure 9.17
with the dotted line ψ from the camera to the image-based controller.
9.9
Mobile Robot Visual Servoing Using Omnidirectional
Vision
In this section, the visual servoing design problem of mobile robots will be consid-
ered using omnidirectional vision. The equations of the conics (hyperbola, parab-
ola, ellipse), that are needed in catadioptric cameras, are first derived, followed by
the catadioptric projection geometry, and the derivation of the relevant image
Jacobian used for the visual servoing.
Visual command
Field of view
(xQ, yQ, φ)Actual
ψ
Teaching 
by doing
e(t)
x(t)
f(t)
fd
Dynamic 
controller
Feature 
processing
Feature 
extraction
Target
landmark
C
Image-based
controller
+
+
−
−
ν
ω
J†
imK
Figure 9.17 Complete kinematic and dynamic visual servoing of the mobile robot
(including the feedback loop for the camera orientation deviation ψ(t)).
358
Introduction to Mobile Robot Control

9.9.1
General Issues: Hyperbola, Parabola, and Ellipse equations
Catadioptric vision systems are composed by combinations of hyperbolic, para-
bolic, and elliptic mirrors and cameras (lenses). These systems are distinguished
according to whether they have a unique effective viewpoint or not. Most fish-eye
lenses do not possess this unique viewpoint property, but a hyperbolic mirror in
front of a perspective camera, a parabolic mirror in front of an orthographic cam-
era, or an elliptic mirror in front of a perspective camera have a single effective
viewpoint.
Actually, a catadioptric camera is equivalent (up to distortion) to a perspective
camera, and the lines in space, along which the image is constant, intersect at a sin-
gle effective viewpoint (Figure 9.18A).
It was shown that “a necessary and sufficient condition for a catadioptric camera
to have a single effective viewpoint is that the mirror’s cross section is a conic sec-
tion” (i.e., a hyperbola, parabola, ellipse, or circle shown in Figure 9.19AD) [25].
Hyperbola—The hyperbola (Figure 9.20A) is defined to be a conic section that
represents the locus of all points Σ in the plane for which the distances d1 5 F1Σ
ð
Þ
and d2 5 ðF2ΣÞ from two fixed points (the foci) F1 and F2, separated by a distance
2c, have a given constant difference d2 2 d1 5 λ. When the point Σ is on the left
y = h(x)
y
x
(A)
(B)
ϕ
σ
ϕ
Figure 9.18 The single effective viewpoint property of catadioptric systems.
Figure 9.19 Conic sections (conics). Conics are generated by the intersections of a plane
with one or two nappes of a double-napped cone: (A) hyperbola, (B) parabola, (C) ellipse,
and (D) circle.
Source: http://math2.org/math/algebra/conics.htm.
359
Mobile Robot Control V: Vision-Based Methods

vertex we find that λ 5 ðc 1 pÞ 2 ðc 2 pÞ 5 2p. Let x,y be the coordinates of Σ in a
Cartesian frame centered at ðx0,y0Þ 5 ð0,0Þ. Then, by the hyperbola definition we
get (see Figure 9.20A):
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðx1cÞ2 1 y2
q
2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðx2cÞ2 1 y2
q
5 2p or
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðx1cÞ2 1 y2
q
5 2p 1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðx2cÞ2 1 y2
q
Squaring both sides of this equation and solving for
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðx2cÞ2 1 y2
q
we obtain
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðx2cÞ2 1 y2
q
5 cx=p 2 p which, after completing the squares, gives the hyperbola
equation:
x2
p2 2 y2
q2 5 1,
q2 5 c2 2 p2
ð9:107aÞ
which, when ðx0,y0Þ 6¼ ð0,0Þ, becomes:
ðx2x0Þ2
p2
2 ðy2y0Þ2
q2
5 1
ð9:107bÞ
The line through a focus parallel to the y axis in Figure 9.20A is called the lac-
tus rectum lr (from lactus 5 side and rectum 5 straight) of the hyperbola. The
parameter:
e 5 c=p 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
p2 1 q2
p
=p 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 1 ðq=pÞ2
q
. 1
ð9:107cÞ
is called the hyperbola’s eccentricity.
Parabola—A parabola is the locus of all points in the plane equidistant from a
given line L (called directrix since it shows the direction of the conic section) and
a given point F(the focus) not on the line (Figure 9.20B). The distance between
∑
∑
α
α
(x,y)
y
p
p
c
c
F1
F2
F
F1
F2
2b
2c
2α
C
d1
d2
x
Vertex(0,0)
Focus(α,0)
x
lr
L
y
(x0,y0)
(A)
(B)
(C)
Figure 9.20 Geometry of conics on the plane: (A) hyperbola, (B) parabola, and (C) ellipse.
360
Introduction to Mobile Robot Control

this line and the focus is equal to p 5 2a, where a is the distance of the vertex from
the line L. For a parabola opening to the right with vertex at ð0,0Þ as in
Figure 9.20B, the parabola’s Cartesian equation is:
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðx2aÞ2 1 y2
q
5 x 1 a
which reduces to:
y2 5 4ax
ð9:108aÞ
For x 5 a we get y2 5 4a2, that is, y 5 6 2a and so the lactus rectum lr is equal to:
lr 5 2jyj 5 4a
ð9:108bÞ
If the vertex is at ðx0,y0Þ 6¼ ð0,0Þ, then the parabola equation is:
ðy2y0Þ2 5 4aðx 2 x0Þ
ð9:108cÞ
For a parabola opening upwards the equation is:
x2 5 4ay
ð9:108dÞ
Ellipse—An ellipse is the locus of all points Σ in the plane for which the sum
of their distances d1 and d2 from two fixed points F1 and F2 (the foci) separated by
a distance 2c is a given positive constant 2a, that is, d1 1 d2 5 2a, where a is called
the semimajor axis (Figure 9.20C).
Assuming that C is at ðx0,y0Þ 5 ð0,0Þ, the foci points are F1ð2 c,0Þ and F2ðc,0Þ.
Therefore, from the ellipse definition we get:
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðx1cÞ2 1 y2
q
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðx2cÞ2 1 y2
q
5 2a
which
leads
to
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðx1cÞ2 1 y2
q
5 2a 2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðx2cÞ2 1 y2
q
or
ðx1cÞ2 1 y2 5 4a2 1
ðx2cÞ2 1 y2 2 4a
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðx2cÞ2 1 y2
q
. Solving the last equation for
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðx2cÞ2 1 y2
q
, and
squaring gives ðx2cÞ2 1 y2 5 ½a2ðc=aÞx2, from which we get the ellipse equation:
x2
a2 1 y2
b2 5 1,
b2 5 a2 2 c2
ðb , aÞ
ð9:109aÞ
When ðx0,y0Þ 6¼ ð0,0Þ, the ellipse equation is:
ðx2x0Þ2=a2 1 ðy2y0Þ2=b2 5 1
ð9:109bÞ
361
Mobile Robot Control V: Vision-Based Methods

The circle is a special case of ellipse, when F1 and F2 are at C, and so d1 5 d2
and a 5 b 5 r (circle radius).
9.9.2
Catadioptric Projection Geometry
A basic result in central catadioptric projection is the following theorem [35]:
Catadioptric projection with a single effective viewpoint (called central catadiop-
tric projection) is equivalent to projection to a sphere followed by projection to a
plane from a point.’
The three central catadioptric vision cases are illustrated in Figure 9.21.
To establish the above theorem, we will derive the projection equations for each
of the above projection models, starting from the spherical mirror [35].
Consider the projection of a point Σðx,y,zÞ of the world space to a unit sphere
centered at Oð0,0,0Þ, and then to an image plane z 5 2 ξ (Figure 9.22).
From Figure 9.22 we see that the world point Σðx,y,zÞ is projected via the sphere
to two antipodal points Σ1ðx=ρ,y=ρ,z=ρÞ and Σ2ð2 x=ρ, 2 y=ρ, 2 z=ρÞ on the sphere
where ρ 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
x2 1 y2 1 z2
p
. These points are then projected to the points Σ0
1 and Σ0
2
on the image plane z 5 2 ξ from the point Nð0,0,dÞ. This is expressed by:
σ1,d,ξðx,y,zÞ 5 xðd1ξÞ
dρ2z , yðd1ξÞ
dρ2z ,2ξ

Τ
ð9:110aÞ
F1
F2
∑
World
point
∑
World
point
Image
plane
Image
point
Image
point
F
ϕ
ϕ
(A)
(B)
Image
point
World
point
F1
F2
∑
(C)
Figure 9.21 Central catadioptric solutions and projection models: (A) parabolic mirror and
orthographic camera, (B) hyperbolic mirror and perspective camera, and (C) elliptic mirror
and perspective camera.
362
Introduction to Mobile Robot Control

for point Σ1, and:
σ2,d,ξðx,y,zÞ 5 2xðd1ξÞ
dρ1z ,2 yðd1ξÞ
dρ1z ,2ξ

Τ
ð9:110bÞ
for point Σ2. If the projection is on the plane z 5 2 β, then the relation between
the two projections is:
σi,d,ξðx,y,zÞ 5
d 1 ξ
d 1 β


σi,d,βðx,y,zÞ
ð9:110cÞ
that is, they differ only by a scaling factor ðd 1 ξÞ=ðd 1 βÞ. Therefore, if ξ is not
given it can be taken as ξ 5 1. When d 5 1 and ξ 5 0 the point of projection N is
the North pole, and if d 5 0 and ξ 5 1 we get perspective projection:
σi,0,1ðx,y,zÞ 5 x
z , y
z

Τ
ð9:110dÞ
Parabolic mirror—Now, consider a parabolic mirror (Figure 9.23). The points
Σ1 and Σ2 are orthographically projected to the points Σ0
1 and Σ0
2 on the image
plane, that is, any line (e.g., a ray of light) incident with the focus F is reflected
such that it is perpendicular to the image plane. Therefore, the projection of Σ is
equivalent to its central projection followed by standard orthographic projection.
A paraboloid surface opening upwards (in the z direction) is described by:
z 5 1
4a ðx2 1 y2Þ 2 a
ð9:111Þ
where a is its focal length, its axis is assumed to be the z-axis, and its focus is
located at the origin.
Σ (x,y,z)
z
N
d
O
Image plane
x
Σ1
Σ2
ξ
Σ′1
Σ′2
Figure 9.22 A unit spherical
mirror centered at (0,0,0). A
point Σ(x,y,z) is projected to
the points Σ1 and Σ2 which are
then projected to the image
plane points Σ
0
1 and Σ
0
2.
Σ
Σ1
Σ′2
Σ′1
Σ2
z1
z
x
x2
z2
F
x1
4a
(x,y,z)
Figure 9.23 Parabolic mirror. The image plane
is via the focal point F.
363
Mobile Robot Control V: Vision-Based Methods

The projection of Σ to the paraboloid surface consists of the two antipodal
points Σ1 and Σ2.
Σ1
2ax
ρ 2 z , 2ay
ρ 2 z , 2az
ρ 2 z


,
Σ2
2 2ax
ρ 1 z , 2 2ay
ρ 1 z , 2 2az
ρ 1 z


ð9:112Þ
where ρ 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
x2 1 y2 1 z2
p
. These points are then orthographically projected to the
plane z 5 0 giving the points Σ0
1 and Σ0
2:
Σ0
1
2ax
ρ 2 z , 2ay
ρ 2 z


,
Σ0
2
2 2ax
ρ 1 z , 2 2ay
ρ 1 z


ð9:113Þ
Hyperbolic Mirror—Consider the hyperbolic mirror of Figure 9.24, where the
image plane is again through the focal point F. The 3D point Σ of the world is pro-
jected to the antipodal points Σ1 and Σ2 which are then perspectively projected to
Σ0
1 and Σ0
2 from the second focal point F0, that is, here, rays incident with one of the
focal points of the mirror are reflected into rays incident with the second focal point.
The focus F
is at the origin ðx,y,zÞ 5 ð0,0,0Þ and the focus F0
is at
ðx,y,zÞ 5 ð0,0, 2dÞ. The surface of the hyperboloid is described by:
ðz1d=2Þ2
p2
2 x2 1 y2
q2
5 1
ð9:114aÞ
where:
p 5 1
2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
d2 1 4a2
p
2 2a


,
q 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
a
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
d2 1 4a2
p
2 2a2
q
ð9:114bÞ
The perspective projection (image) points Σ0
1 and Σ0
2 of Σ1 and Σ2 via the point
(focus) F0ð0,0, 2 dÞ are:
Σ0
1: 2xad=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
d2 1 4a2
p
d
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
d2 1 4a2
p
ρ 2 z
, 2yad=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
d2 1 4a2
p
d
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
d2 1 4a2
p
ρ 2 z
 
!
ð9:115aÞ
x
F
F′
z
4a
d
Σ1
Σ
Σ′1
Σ′2
Σ (x,y,z)
Figure 9.24 Cross section of hyperboloid
mirror. The image plane is through the focal
point F.
364
Introduction to Mobile Robot Control

Σ0
2:
22xad=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
d2 1 4a2
p
d
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
d2 1 4a2
p
ρ 1 z
, 2 2yad=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
d2 1 4a2
p
d
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
d2 1 4a2
p
ρ 1 z
 
!
ð9:115bÞ
Ellipsoid Mirror—The surface equation of an ellipsoid with foci at ð0,0,0Þ and
ð0,0, 2 dÞ (see Figure 9.21C) and lactus rectum, 4a, is:
ðz1d=2Þ2
p2
1 ðx2 1 y2Þ
q2
5 1
ð9:116aÞ
where:
p 5 1
2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
d2 1 4a2
p
1 2a


,
q 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
a
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
d2 1 4a2
p
1 2a2
q
ð9:116bÞ
Here, the projection (image) points Σ0
1 and Σ0
2 are:
Σ0
1:
2xad
ρd 1 z
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
d2 1 4a2
p
,
2yad
ρd 1 z
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
d2 1 4a2
p
 
!
ð9:117Þ
Σ0
2:
2
2xad
ρd 2 z
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
d2 1 4a2
p
, 2
2yad
ρd 2 z
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
d2 1 4a2
p
 
!
ð9:118Þ
Equations (9.115a), (9.115b), (9.117), and (9.118) show that the ellipsoid mirror
gives the same reflection about z 5 0 as the hyperboloid mirror. Comparing
Eqs. (9.110a), (9.110b), (9.113), (9.115a), (9.115b), (9.117), and (9.118) the valid-
ity of the catadioptric projection theorem, stated at the beginning of this section, is
directly established. This theorem provides a unified geometric model of catadiop-
tric projection, using the projective (image) plane induced by the projection point
σ1,d,ξðx,y,zÞ or σ2,d,ξðx,y,zÞ (see Eqs. (9.110a) and (9.110b)).
Actually, the central catadioptric imaging can be modeled by projecting the
scene on the surface of the sphere and then reprojecting these points on the image
plane from a new projection point N (see Figure 9.22). This is illustrated in
Figure 9.25 which shows that the projections (images) Σ0
1 and Σ0
2 of the point Σ to
the catadioptric image plane Π via the parabolic mirror and orthographic projec-
tion, and via the spherical mirror through the point N and perspective projection
are coincident. As an exercise, the reader can verify geometrically that this is true
using the parabola and circle properties.
To describe formally this general catadioptric projection we draw the model
shown in Figure 9.26, where Fc corresponds to the projection point N of
Figure 9.22 [36,37].
Specifically, the mapping of 3D world points to points in the catadioptric image
plane involves three stages as shown in Figure 9.27.
365
Mobile Robot Control V: Vision-Based Methods

The world point x 5 ½x,y,z,1Τ is projected to the catadioptric image point
xim 5 ½xim,yim,zimΤ. Each visible point Σ can be associated with a projective ray ^x
joining the point with the effective viewpoint of the system. The vectors ^x and x
are related by:
^x 5 Ax
ð9:119aÞ
(A)
(B)
F
F
N
Σ1
Σ1
Σ
Σ2
Σ
Σ′1
Σ′2≡Σ′1
∏
∏
Ξ
Ξ
Figure 9.25 Parabolic orthographic image
point Σ
0
1 (A) is coincident with spherical
perspective image point Σ
0
2(B).
Object in
3D space
Image plane
ϕ
ϕ
zσ
Oσ
xσ
xim
(xΣ
im,yΣ
im,zΣ
im)
Oim
d
yim
xc
yc
Fc
Fm
z
x
y
zc
Σ (x,y,z)
Σ1 (x,y,z)
yσ
4a
Figure 9.26 Central catadioptric projection of a point Σ of the 3D world.
x
xim
x
x
A
Gc
F (.) 
ˆ
–
Figure 9.27 Catadioptric image formation.
366
Introduction to Mobile Robot Control

where A is a typical 3 3 4 projection matrix:
A 5 R½I3 3 3^ 2 s0
ð9:119bÞ
with R being the rotation matrix between world and mirror coordinate systems, and
s0 being the world’s frame origin. Assuming, without loss of generality, that the
world and sensor coordinate systems are the same, then:
A 5
1
0
1^
0
0
1
0^
0
0
0
1^
0
2
64
3
75
ð9:119cÞ
We regard the projective ray ^x as a point in an oriented projective plane P2
which is transformed to a point xAP2 by a nonlinear transformation Fð^xÞ. Then,
the image point xim is obtained via the transformation:
xim 5 Gcx,
Gc 5 QcRcΤc
ð9:120Þ
where Qc involves the camera intrinsic parameters, Rc is the rotation matrix
between the camera and the mirror, and Τc depends on the shape of the mirror. The
parameters d0 and ξ0 of this general model for parabolic, hyperbolic, elliptic, and
perspective (spherical) systems are shown in Table 9.1, where 4a is the lactus rec-
tum of the mirror and d the distance between the foci of the camera and mirror.
These parameters are found by comparing the spherical projection equation
(9.110a) and (9.110b) and the parabolic, hyperbolic, and elliptic mirror projection
equations (9.113), (9.115a), and (9.117). The function FðxÞ and the matrix Τc are
given by:
FðxÞ 5
x
z 1 d0
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
x2 1 y2 1 z2
p
y
z 1 d0
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
x2 1 y2 1 z2
p
1
2
666664
3
777775
ð9:121aÞ
Table 9.1 Unified Catadioptric Model Parameters d0 and ξ0
Mirror
d0
ξ0
Point
Parabolic
1
2a 2 1
ðx; y; z; 1Þ
Hyperbolic
d=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
d2 1 2a2
p
ð2a 2 1Þd=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
d2 1 2a2
p
ðx; y; z; 1Þ
Elliptic
d=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
d2 1 2a2
p
ð2a 2 1Þd=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
d2 1 2a2
p
ðx; y; 2 z; 1Þ
Perspective
0
1
ðx=z; y=z; 1Þ
367
Mobile Robot Control V: Vision-Based Methods

or, equivalently:
FðxÞ 5
x
y
z 1 d0
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
x2 1 y2 1 z2
p
2
4
3
5
ð9:121bÞ
and
Τc 5
ξ0 1 d0
0
0
0
ξ0 1 d0
0
0
0
1
2
4
3
5
ð9:121cÞ
Detailed derivations and extensions of the above results to more general models
can be found in Refs. [3843].
An example of hyperboloidal omnidirectional image and the corresponding pan-
oramic (cylindrical) and perspective images, falling in the general catadioptric cam-
era model of Figures 9.26 and 9.27, is shown in Figure 9.28.
Example 9.4
It is desired to derive the catadioptric image Jacobian matrix for a set of n points (fea-
tures) Σi:xσ,1,xσ,2,. . .,xσ,n of an object shown in Figure 9.26.
Perspective view
Omnidirectional Image
Panoramic view
Image plane
F
F′
Mirrorpoint
Objectpoint
Imagepoint
Figure 9.28 A hyperboloidal catadioptric vision system and examples of generated
transformed images.
368
Introduction to Mobile Robot Control

Solution
Denote by xσ an arbitrary point of the object in the local coordinate frame Oσðxσ,yσ,zσÞ. If
R is the rotation matrix between the frame Oσ and the mirror’s reference frame Fðx,y,zÞ,
and s is the position vector of Oσ in mirror’s coordinates then the homogeneous transfor-
mation from F(x,y,z) to Oσ(xσ,yσ,zσ) can be expressed as:
x 5 Rxσ 1 s
ð9:122Þ
The point x is projected to the point ximi 5 FiðxÞ of the catadioptric image plane as shown
by Eq. (9.121a). The motion of the object is described by the screw vector:
_r 5
v
ω


and so the 3D velocity of the point Σi due to the object motion is:
_x 5 J0_r
ð9:123Þ
Now, if Ji is the Jacobian matrix of the first two lines of the function fiðxÞ in
Eq. (9.121a), then the image Jacobian Jim from _xσ,i to _r is given by:
_xσ,1
_xσ,2
^
_xσ,n
2
664
3
775 5 Jim_r 5
J1
im
J2
im
^
Jn
im
2
664
3
775_r
ð9:124aÞ
where:
Ji
im 5 JiJ0
ð9:124bÞ
From Eq. (9.121a) we find that [37]:
Ji 5
1
ρðz1ζρÞ2
ρz 1 ζðy2 1 z2Þ
2ζxy
2xðρ 1 ζzÞ
ζxy
2½ρz 1 ζðx2 1 z2Þ
yðρ 1 ζzÞ


ð9:125aÞ
where ζ 5 d0 and ρ 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
x2 1 y2 1 z2
p
. The matrix J0 is equal to J0 5 ½I3 3 3^SðxÞ,
where S xð Þ is the skew symmetric matrix associated with x, that is (see (9.4a) and
(9.4b)):
SðxÞ 5
0
z
2y
2z
0
x
y
2x
0
2
4
3
5
ð9:125bÞ
Using the above expressions for Ji and J0, Eq. (9.124b) gives:
Ji
im 5 ½ðJi
imÞ1^ðJi
imÞ2
369
Mobile Robot Control V: Vision-Based Methods

ðJi
imÞ1 5
xσ,i yσi
ð1 1 x2
σ,iÞμ 2 y2
σ,iζ
μ 1 ζ
yσ,i
ð1 1 y2
σ,iÞμ 2 x2
σ,iζ
μ 1 ζ
xσ,iyσ,i
2xσ,i
2
666664
3
777775
ðJi
imÞ2 5
1 1 x2
σ,i½1 2 ζðμ 1 ζÞy2
σ,i
ρðμ 1 ζÞ
xσ,iyσ,iζ
ρ
2 xσiμ
ρ
2 xσ,iyσ,iζ
ρ
1 1 x2
σ,i 1 y2
σ,i½1 2 ζðμ 1 ζÞ
ρðμ 1 ζÞ
2 yσ,iμ
ρ
2
666664
3
777775
ð9:126Þ
where μ 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 1 ðx2
σ,i 1 y2
σ,iÞð1 2 ζ2Þ
q
. We can verify that when ζ 5 0 (perspective projec-
tion), we get the standard Jacobian matrix of perspective cameras. In fact, in this case
Eq. (9.121a) becomes the standard perspective transformation fi 5 ½x=z, 2 y=z, and Ji
takes the form:
Ji 5 Ja,iJc,i
ð9:127aÞ
Ja,i 5
zðρz 1 ζðy2 1 z2ÞÞ
ρðz1ζρÞ2
ζxyz
ρðz1ζρÞ2
ζxyz
ρðz1ζρÞ2
zðρz 1 ζðx2 1 z2ÞÞ
ρðz1ζρÞ2
2
66664
3
77775
ð9:127bÞ
Jc,i 5
1=z
0
2x=z2
0
21=z
y=z2


ð9:127cÞ
For ζ 5 0, the matrix Ja,i reduces to the identity matrix, and so:
Ji 5
1=z
0
2x=z2
0
21=z
y=z2


ð9:127dÞ
which is the standard Jacobian matrix (see Eqs. (9.13a) and (9.31b)). The above Jacobian
matrix can be used directly for visual servoing based on catadioptric omnidirectional
vision.
9.9.3
Omnidirectional Vision-Based Mobile Robot Visual Servoing
Having available the catadioptric image Jacobian matrix, the visual servoing of a
fixed or mobile robot can be performed in the usual way (see, e.g., Section 9.8),
that is, using the feedback control law (see Eq. (9.34)):
u 5 _r 5 Jy
imKeσ,
eσ 5 xσ,d 2 xσ
370
Introduction to Mobile Robot Control

where eσ 5 ½eΤ
σ,1,eΤ
σ,2,. . .,eΤ
σ,nΤ, and:
Jim 5
Ja,1
0
?
0
0
Ja,2
?
0
:::
:::
:::
:::
:::
:::
:::
:::
0
0
?
Ja,n
2
66664
3
77775
Jc,1
Jc,2
^
Jc,n
2
664
3
775
with Ja,i given by Eq. (9.127b) and Jc,i by Eq. (9.127c), i 5 1,2,. . .,n. The general-
ized inverse Jy
im is given by Eqs. (9.31a) and (9.31b). To get a realizable solution,
the number n of object points (features) must be larger than the number of the task
degrees of freedom in which case we use Eq. (9.31a).
Now, consider the case of mobile robot image-based visual servoing using a
perspective camera-hyperbolic mirror catadioptric sensor. Since the extraction of
target feature parameters, needed to implement catadioptric visual feedback, is
computationally very demanding, a feasible approach is to consider a small pixel
region that corresponds to the target. However, for a moving target we need to
continuously update the coordinates of the target, in order to make possible a
real-time computation of the target parameters. Using a convenient landmark on
the target (e.g., a ’ drawn on it) the target visual tracking can be done in two
steps:
Step 1: Localization of the target in the entire image (this is done off line).
Step 2: Real-time visual tracking with initial position coordinates of the target the coordi-
nates found in step 1.
To apply the real-time tracking we can use a discrete time affine model
describing the target movement between two successive times t and t 1 Δt which
involves a translation and rotation displacement specified by a vector d and a
rotation matrix R. Since the affine transformation cannot be applied directly to
the omnidirectional image, this image is first transformed to a perspective image
ðxim,yimÞ described by the parameters lfp, φa, and φe
(see Example 4.1,
Eqs. (4.5a), (4.5b), (4.6a), and (4.6b)). The updated pose ðφnew,ψnewÞ is computed
using the relations:
φnew 5 φ 1 tg21 dyim
lf


,
ψnew 5 ψ 1 tg21 dxim
lf


where dxim and dyim are the horizontal and vertical translation, respectively, of the
target center. In this way, we assure that the target remains always at the center of
the perspective image. Clearly, this process would be the same if a virtual perspec-
tive camera mounted on a pan-tilt mechanism is used, with its focus always placed
at the hyperbola focus (see Section 9.8). Now, the standard image-based visual ser-
voing procedure (described in Section 9.8) can be applied. Choosing the target
region, being tracked, to be a square, and the target features to be the coordinates
371
Mobile Robot Control V: Vision-Based Methods

in the perspective image of the four corners ði 5 1,2,3,4Þ of this square, then the
feature vector f is (see Figure 4.16):
f 5 ½xim,1,xim,2,xim,3,xim,4,yim,1,yim,2,yim,3,yim,4Τ
The motion of the perspective camera frame is described by the vector (see
Eq. (9.9)):
_rc 5
vc
ωc


5 ½υc
x,υc
y,υc
z,ωc
x,ωc
y,ωc
zΤ
The relation of _f and _rc is given by Eq. (9.9):
_f 5 JimðrÞ_rc
where _f has eight rows and six columns. Assuming that the square target region is
centered at the center of the image and has dimension 2η, the feature vector fΤ
becomes:
fΤ 5 ½2η,η,η,2η,2η,η,η,2ηΤ
and the Jacobian matrix Jim ffΤ has four 2 3 6 blocks of the form of Eq. (9.15),
where we can put z 5 1 without loosing convergence. The image-based feedback
control law is as usual:
_r 5 Jy
imKeΤðfΤÞ,
eΤðfΤÞ 5 fΤ,d 2 fΤ
where fΤ,d is the desired feature vector. The dimensionality of J can be reduced by
taking into account the fact that here we have υc
y 5 0, since the robot and the cam-
era move only on a horizontal plane (see Figure 4.16). Considering a synchrodrive
WMR, the control vector is u 5 ½υp
z,ωp
ψ, where υp
z is the translational speed, and ωp
ψ
the steering speed of the robot platform. Here, we can also assume that
ωc
x 5 ωc
z 5 0. Therefore, _rc becomes:
_rc 5 ½υc
x,υc
z,ωc
y
Example 9.5
Derive the kinematic transformation of a 3D point, that moves on a panoramic (cylindrical)
image plane, to the perspective camera image plane.
Solution
Consider a hyperboloidal mirror-perspective camera vision system of the form shown in
Figure 9.26. The corresponding virtual cylindrical image plane has the form of Figure 9.29.
372
Introduction to Mobile Robot Control

The world point PðX,Y,ZÞ is mapped to the point ðx,yÞ of the image plane after reflec-
tion of the light ray originated from P at the mirror. The focal points of the mirror
and camera are denoted by Fm and Fc, respectively. The hyperboloidal mirror surface equa-
tion is:
ðZ1cÞ2=p2 2 ðX2 1 Y2Þ=q2 5 1,
c 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
p2 1 q2
p
ð9:128Þ
In terms of the parameters p,q,c and lf (the camera focal length), the coordinates of
the image point of PðX,Y,ZÞ are given by (see Eqs. (9.114a), (9.114b), and (9.121a):
x
y
 
5 μ X
Y


,
μ 5
lfðp2 2 c2Þ
ðp2 1 c2ÞZ 2 2pc
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
X2 1 Y2 1 Z2
p
ð9:129Þ
The projection of a point P of the cylindrical surface to the image plane is given by:
λx 5 Ap
ð9:130Þ
where x 5 ½x,y,0Τ,A 5 diag½1,1,0, p 5 ½X,Y,ZΤ, and λ 5 1=μ. As we know, the motion of
a 3D space point p 5 ½X,Y,ZΤ which is related to the camera motion is described by the
linear velocity vðtÞ and angular velocity ωðtÞ as shown in Eq. (9.2a) and (9.2b), that is,
_p 5 2 ðω 3 p 1 vÞ where v 5 ½vx,vy,vzΤ and ω 5 ½ωx,ωy,ωzΤ. Therefore, differentiating
Eq. (9.130) with respect to time we get λ_x 1 _λx 5 A_p which gives:
_x
_y
 
5 J0,vv 1 J0,ωω
ð9:131Þ
Omnidirectional camera
Virtual cylindrical image plane
P(X,Y,Z)
X
y
x
Y
Z
Figure 9.29 Virtual cylindrical image plane.
Source: Reprinted from [23], with permission from International Journal of Control,
Automation and Systems.
373
Mobile Robot Control V: Vision-Based Methods

where:
J0,v 5 1
λ
21 1 γλx2
γλxy
ða=βÞx 1 γxZ
γλxy
21 1 γλy2
ðα=βÞy 1 γyZ


ð9:132aÞ
J0,ω 5
ðα=βÞxy
2Z=λ 2 ðα=βÞx2
y
Z=λ 1 ðα=βÞy2
2ðα=βÞxy
2x


ð9:132bÞ
with α 5 p2 1 c2,
β 5 lfðp2 2 c2Þ,
γ 5 2 2pc=ðβjjpjjÞ and jjpjj 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
X2 1 Y2 1 Z2
p
. Since
the WMR is moving on a plane (i.e., v 5 ½vx,vy,0Τ,
ω 5 ½0,0,ωzΤ), Eqs. (9.131),
(9.132a), and (9.132b) reduce to:
_x
_y
 
5 J0,vv0 1 J0,ωω
ω 5 ωz,v0 5
vx
vy




ð9:133Þ
where:
J0,v 5 1
λ
21 1 γλx2
γλxy
γλxy
21 1 γλy2


,
J0,ω 5
y
2x


ð9:134Þ
Example 9.6
It is desired to find the image Jacobian matrix of a camera WMR system, where the camera
is fixed to the ceiling above the WMR such as the xcyc camera plane is parallel to the
image plane ximyim.
Solution
We consider the system configuration shown in Figure 9.30 [44].
We have the following coordinate frames:
Owxwywzw is world coordinate frame.
Ocxcyczc is camera coordinate frame.
Oimximyimzim is image plane coordinate frame.
Image plane
C′w
Oim
yim
xim
Cy
yw
zw
Ow
Oc
xc
zc
yc
Cx
xw
Q
φ
φ0
Figure 9.30 Geometry of the vision
and robot system.
374
Introduction to Mobile Robot Control

Let C0
wðcx,cyÞ be the point where the camera optical axis crosses the Owxwyw (robot)
plane (assumed parallel to the image plane), ðcx,im,cy,imÞ the coordinates of the image of
Oc in the image plane, QðxQ,yQÞ the position of the WMR in the world coordinate frame,
and ðxQ,im,yQ,imÞ the image of ðxQ,yQÞ.
Then, it is easy to find that the camera perspective model is:
xQ,im 5 ΛRðφ0Þ½xQ 2 cw 1 cim
ð9:135aÞ
where:
xQ,im 5
xQ,im
yQ,im


,xQ 5
xQ
yQ


,cw 5
cx
cy


,cim 5
cx,im
cy,im


ð9:135bÞ
and Λ 5 diag½λ1,λ2 with λ1,λ2 being constants that are specified by the vision system
parameters (depth, focal length, scaling factors in the xim and yim axis respectively). The
rotation matrix Rðφ0Þ is given by:
Rðφ0Þ 5
cos φ0
sin φ0
2sin φ0
cos φ0


ð9:136Þ
where φ0 is the angle between the xim axis and the xw axis, positive in the anticlockwise
direction.
Carrying out the algebraic calculations in Eq. (9.135a), using Eq. (9.136), we get [44]:
f 5 Gx 1 F
ð9:137aÞ
where:
f 5 ½xΤ
Q, im,φΤ,x 5 ½xΤ
Q,φΤ
ð9:137bÞ
G 5
λ1 cos φ0
λ1 sin φ0
0
2λ2 sin φ0
λ2 cos φ0
0
0
0
1
2
4
3
5,
F 5
2cxλ1 cos φ0
2cyλ1 sin φ0 1 cx,im
cxλ2 sin φ0
2cyλ2 cos φ0 1 cy,im


ð9:137cÞ
Differentiating Eq. (9.137a) with respect to time we obtain:
_f 5 Jim_r,
_r 5 ½v,ωΤ
ð9:138aÞ
where Jim is the image Jacobian matrix:
Jim 5
λ1 cosðφ 2 φ0Þ
0
λ2 sinðφ 2 φ0Þ
0
0
1
2
4
3
5
ð9:138bÞ
which represents the kinematic model of the camera robot system, from the WMR screw
vector _r to the vision feature rate vector _f.
375
Mobile Robot Control V: Vision-Based Methods

Example 9.7
The problem is to find a vision-based control law τðtÞ which drives the system trajectory
xðtÞ 5 ½xQ,yQ,φΤ of a differential-drive robot to a desired trajectory xdðtÞ 5 ½xQ,d,yQ,d,φdΤ
and is robust against a bounded unknown disturbance dðtÞ with:
jjdðtÞjj # dmax
ð9:139Þ
Solution
We will apply a two-step backstepping procedure, assuming as usual that the desired tra-
jectory satisfies the same kinematic equations as the WMR [44,45]. Working with the cam-
era robot configuration of Figure 9.30, the desired trajectory satisfies the kinematic model
(9.138a) of the robot trajectory on the image plane:
_fd 5 Jim_r,
_r 5 ½v,ωΤ
ð9:140aÞ
where fd 5 ½xQ, d,yQ, d,φdΤ and:
Jim 5
λ1 cosðφd 2 φ0Þ
0
λ2 sinðφd 2 φ0Þ
0
0
1
2
4
3
5
ð9:140bÞ
Kinematic Controller
Without loss of generality we assume that λ1 5 λ2 5 λð0 , λmin # λ # λmaxÞ, which is
the unknown vision system parameter. The feature vector error:
~f 5 fd 2 f
ð9:141Þ
expressed in the WMR (local) coordinate frame is (see Eqs. (5.51) and (7.69)):
ε 5 E~f,
ε 5 ½ε1,ε2,ε3Τ
ð9:142aÞ
where:
E 5
cosðφ 2 φ0Þ
sinðφ 2 φ0Þ
0
2sinðφ 2 φ0Þ
cosðφ 2 φ0Þ
0
0
0
1
2
4
3
5
ð9:142bÞ
Therefore, since E is invertible, ε ! 0 if and only if ~f ! 0. Now, in analogy to Eq. (7.70),
using Eqs. (9.138a), (9.138b), (9.140a), (9.140b), (9.142a), and (9.142b) we get the fol-
lowing error dynamics:
_ε1 5 ωε2 2 λv 1 λvd cos ε3
_ε2 5 2 ωε1 1 λvd sin ε3
_ε3 5 ωd 2 ω
ð9:143Þ
376
Introduction to Mobile Robot Control

Therefore, working as in Example 7.1 (Eqs. (7.71) and (7.72)), we choose the following
candidate Lyapunov function [44,45]:
V 5 1
2 ðε2
1 1 ε2
2 1 λε2
3Þ
ð9:144Þ
Differentiating V with respect to time and introducing Eq. (9.143) into the result we
find:
_V 5 ε1_ε1 1 ε2_ε2 1 λε3_ε3
5 ε1ðωε2 2 λvm 2 λ~v 1 λvdcos ε3Þ
1 ε2ð2 ωε1 1 λvd sin ε3Þ 1 λε3ðωd 2 ωm 2 ~ωÞ
5 ε1λð2 vm 1 vd cos ε3Þ 1 ε3λ ε2vd
sin ε3
ε3
0
@
1
A 1 ωd 2 ωm
2
4
3
5 2 λε1 ~v 2 λε3 ~ω
ð9:145Þ
Therefore, selecting:
2 vm 1 vd cos ε3 5 2 K1ε1
ε2vd
sin ε3
ε3
0
@
1
A 1 ωd 2 ωm 5 2 K3ε3
that is:
vm 5 K1ε1 1 vd cos ε3
ð9:146aÞ
ωm 5 ωd 1 K3ε3 1 ε2vd
sin ε3
ε3


ð9:146bÞ
we get:4
_V 5 2 λK1ε2
1 2 λK3ε2
3 2 λε1 ~v 2 λε3 ~ω
ð9:146cÞ
The first two terms of _V are negative, and so, for asymptotic trajectory tracking and
parameter convergence, we have to ensure that ~v and ~ω tend asymptotically to zero. This
will be done by the dynamic adaptive controller that follows.
Adaptive Controller
We will use the dynamic model of Eq. (3.29) enhanced with the external disturbance
dðtÞ:
Dv 5 Eτ 1 dðtÞ
ð9:147aÞ
4 This is a valid controller since for ε3 ! 0,ðsin ε3Þ=ε3 ! 1:
377
Mobile Robot Control V: Vision-Based Methods

where:
v 5
v
ω


,d 5
d1
d2


,D 5
m
0
0
I


,E 5 1
r
1
1
2a
22a


ð9:147bÞ
Using the velocity error vector:
~v 5
~v
~ω


5
v 2 vm
ω 2 ωm


5 v 2 vm
ð9:148Þ
the model of Eq. (9.147a) can be written as:
D~v 5 2 Mβ 1 Eτ 1 dðtÞ
ð9:149aÞ
where the relation:
Dvm 5
m
0
0
I
2
4
3
5
vm
ωm
2
4
3
5 5
vm
0
0
ωm
2
4
3
5
β1
β2
2
4
3
5 5 Mβ
M 5
vm
0
0
ωm
2
4
3
5,β 5
β1
β2
2
4
3
5 5
m
I
2
4
3
5
ð9:149bÞ
was used. The matrix M is a known as linear regressor matrix that multiplies the unknown
parameter vector β (see Eq. (3.102)).
To design the dynamic adaptive controller, we add to V in Eq. (9.144) an extra
Lyapunov function term V0 defined as:
V0 5 1
2 ~vΤD~v 1 1
2
~β
ΤΓ21 ~β 1
1
2γ3
~λ
2
ð9:150Þ
where:
~β 5 β 2 ^β, β 5 ½β1,β2Τ 5 m
I

	Τ, ~λ 5 λ 2 ^λ
Γ 5 diag½γ1,γ2, γ1 . 0, γ2 . 0, γ3 . 0
ð9:151Þ
We have seen that the time derivative of V along the trajectory of the system described
by Eq. (9.143), with the feedback controls of Eqs. (9.146a) and (9.146b) leads to the
expression (9.146c). Therefore, the time derivative of the total Lyapunov function
V 5 V 1 V0 along the trajectories of the system in Eqs. (9.143), (9.149a), and (9.149b) is
given by:
378
Introduction to Mobile Robot Control

_V0 5 2 λK1ε2
1 2 λK3ε2
3 2 λε1 ~v 2 λε3 ~ω
1 ~vΤð2 Mβ 1 Eτ 1 dÞ 1 ~βΓ21 _~β 1 1
γ3
~λ_~λ
5 2 λK1ε2
1 2 λK3ε2
3 2 ð^λ 1 ~λÞε1 ~v 2 ð^λ 1 ~λÞε3 ~ω
1 ~vΤð2 M^β 2 M~β 1 Eτ 1 dÞ 2 ~βΓ21_^β 2 1
γ3
~λ_^λ
5 2 λK1ε2
1 2 λK3ε2
3 1 ~vΤ
2M^β 2 ^λ ε1
ε2


1 Eτ 1 d


2 ~β
ΤðΓ21_^β 1 MΤ~vÞ 2 ~λ
1
γ3
_^λ 1 ~vΤ ε1
ε2


0
@
1
A
ð9:152Þ
where the relations β 5 ^β 1 ~β, λ 5 ^λ 1 ~λ, and v 5 ^v 1 ~v were used. Now, selecting the
control vector τ and the updating laws of the parameter estimates as:
Eτ 5 M^β 1 ^λ ε1
ε2


2 Ka~v 2 urobust
ð9:153Þ
_^β5 2 ΓMΤ~v
ð9:154Þ
_^λ5 2 γ3½ε1,ε3~v
ð9:155Þ
Ka 5
Ka1
0
0
Ka2


,Ka1 . 0,Ka2 . 0
we get:
_V0 5 2 λ1K1ε2
1 2 λK3ε2
3 2 ~vΤKa~v 1 ~vΤðd 2 urobustÞ
ð9:156Þ
The first three terms of _V0 are nonpositive. Therefore, to guarantee that _V0 # 0 the
robustifying control term urobust must be selected such as:
~vΤðd 2 urobustÞ # 0
ð9:157Þ
Given the bound dmax of the disturbance dðtÞ (see Eq. (9.139)), the condition (9.157)
is satisfied by selecting:
urobust 5 dmax sgn ~v,
sgn ~v 5
sgn ~v
sgn ~ω


379
Mobile Robot Control V: Vision-Based Methods

which, by Eq. (9.139) implies:
~vΤðd 2 dmax sgn ~vÞ # jj~vjjðjjdjj 2 dmaxÞ # 0
To summarize, the adaptive robust trajectory controller for the differential-drive WMR
is given by:
vm 5 K1ε1 1 vd cos ε3
ð9:158aÞ
ωm 5 ωd 1 K3ε3 1 ε2vd
sin ε3
ε3


ð9:158bÞ
τ 5 E
21 M^β 1 ^λ ε1
ε2


2 Ka~v 2 dmax sgn ~v


ð9:158cÞ
_^β5 2 ΓMΤ~v
ð9:158dÞ
_^λ5 2 γ3½ε1,ε2~v
ð9:158eÞ
This controller assures that V0ðtÞ is a nonincreasing function that converges to a limit-
ing value V0 $ 0. Thus, εðtÞ,~vðtÞ, ~β, and ~λ are all bounded. Now, assuming that
vd,_vd,ωd, _ωd are bounded, it follows that _ε1,_ε2,_ε3,~v, ~ω are all bounded. Then, ε1,ε3,~v, and ~ω
are uniformly continuous, and so by Barbalat lemma ε1 ! 0, ε3 ! 0, ~v ! 0, and
~ω ! 0. It only remains to show that ε2 ! 0. To this end, we consider the closed-loop
equation for ε3, namely:
_ε3 5 2 K3ε3 2 ðε2=ε3Þvd sin ε3 2 ~ω
Then, from ε3 ! 0, ~ω ! 0, and ðsin ε3Þ=ε3 ! 1, and assuming that minjvdj ! η . 0,
t ! N,
it
follows
that
ε2 ! 0.
Therefore,
overall,
we
have
established
that
ε1 ! 0, ε2 ! 0, ε3 ! 0, ~v ! 0, ~ω ! 0, and ^β,^λ bounded (not necessarily equal to
the true values β and λ).
To overcome the chattering that may occur due to the signum robustifying term
urobust 5 dmax sgn ~v, we replace the signum function by the saturation function:
sat ~v 5
~v=U,
if
jj~vjj # UðtÞ
sgn ~v,
if
jj~vjj . UðtÞ

where UðtÞ is the width of the boundary layer used (see Eq. (7.17)). By selecting UðtÞ such
as
Ð N
0
UðτÞdτ # δ, where δ is a nonnegative constant we can show that the saturation
robustifying term leads again to ε1 ! 0, ε2 ! 0, ε3 ! 0, ~v ! 0, ~ω ! 0, and bounded
^β and ^λ (the proof is left as an exercise).
380
Introduction to Mobile Robot Control

Remark A similar controller can be derived by selecting, in analogy to Eq. (7.72),
the Lyapunov function:
V 5 1
2 ðε2
1 1 ε2
2Þ 1 ðλ=K2Þð1 2 cos ε3Þ,
λ . 0, K2 . 0
Then, differentiating V along the trajectory of the system of Eq. (9.143), we find
that selecting:
vm 5 K1ε1 1 vd cos ε3
ð9:159aÞ
ωm 5 ωd 1 K2vdε2 1 K3 sin ε3
ð9:159bÞ
gives
_V 5 2 K1ε2
1 2 λðK3=K2Þðsin ε3Þ2 2 ε1λ~v 2 ðλ=K2Þðsin ε3Þ ~ω
which is analogous to Eq. (9.146c). Therefore, adding a second Lyapunov function
term V0, given by Eq. (9.150), and working as above we find the kinematic control-
ler (9.159a) and (9.159b), and the dynamic adaptive-robust trajectory tracking
controller:
τ 5 E
21 M^β 1 ^λ
ε1
sin ε3


2 Ka~v 2 dmax sgn ~v


_^β 5 2 ΓMΤ ~v
_^λ 5 2 γ3½ε1,sin ε3~v
ð9:160Þ
Like the controller of Eqs. (9.158a)(9.158e), we again achieve asymptotic con-
vergence of ε1,ε2,ε3 to zero, and of ^β,^λ to bounded values (possibly different than
the true values of β and λ).
References
[1] Corke P. Visual control of robot manipulators: a review. In: Hashimoto K, editor.
Visual servoing. Singapore: Word Scientific; 1993. p. 131.
[2] Espiau B, Chaumette F, Rives P. A new approach to visual servoing in robotics. IEEE
Trans Robot Autom 1992;8:31326.
[3] Hutchinson S, Hager G, Corke P. A tutorial on visual servo control. IEEE Trans Robot
Autom 1996;12:65170.
[4] Haralik RM, Shapiro LG. Computer and robot vision. Reading, MA: Addison Wesley;
1993.
[5] Cherubini A, Chaumette F, Oriolo G. An image-based visual servoing scheme for fol-
lowing paths with nonholonomic mobile robots. In: Proceedings of international confer-
ence on control automation. Robotics and Vision. Hanoi, Vietnam; December 1720,
2008. p. 108113.
381
Mobile Robot Control V: Vision-Based Methods

[6] Cherubini A, Chaumette F, Oriolo G. A position-based visual servoing scheme for fol-
lowing paths with nonholonomic robots. In: Proceedings of IEEE/RSJ international
conference on intelligent robots and systems (IROS 2008). Nice, France; September
2008. p. 164854.
[7] Chaumette F, Hutchinson S. Visual servo control tutorial, parts I and II. IEEE Robot
Autom Mag 2007;13(14):8290, 14(1):109118
[8] Burshka D, Hager G. Vision-based control of mobile robots. In: Proceedings of 2001
IEEE international conference on robotics and automation. Seoul, Korea; May 2126,
2001. p. 170713.
[9] Tsakiris
D,
Samson
C,
Rives
P.
Vision-based
time-varying
stabilization
of
a mobile manipulator. In: Proceedings of fourth international conference on con-
trol, automation, robotics and vision (ICARV’96). Westin Stamford, Singapore;
December 36, 1996.
[10] Tsakiris
D,
Samson
C,
Rives
P.
Vision-based
time-varying
robot
control.
In: Proceedings of ERNET workshop. Darmstadt, Germany; September 910, 1996.
p. 16372.
[11] Huang TS, Netravali NA. Motion and structure from feature correspondences: a
review. Proc IEEE 1994;82(2):25268.
[12] Kumar R. Robust methods for estimating pose and a sensitivity analysis. CVGIP:
Image Underst 1994;3:31342.
[13] Wilson W. Visual servo control of robots using Kalman filter estimates of robot pose
relative to workpieces. In: Hashimoto K, editor. Visual servoing. Singapore: World
Scientific; 1994. p. 71104.
[14] Horswill I. Polly: a vision-based artificial agent. In: Proceedings of eleventh national
conference on artificial intelligence (AAAI’93). Washington, DC: MIT Press; July
1115, 1993. p. 82429.
[15] Rives P, Espiau B. Closed-loop recursive estimation of 3D features for a mobile vision
system. In: Proceedings of IEEE international conference on robotics and automation
(ICRA 1987). Rayleigh, NC; April 1987. p. 143644.
[16] Qian J, Su J. Online estimation of image Jacobian matrix by KalmanBucy filter for
uncalibrated stereo vision feedback. In: Proceedings of international conference on
robotics and automation (ICRA 2002). 2002. p. 5627.
[17] Das AK, Fierro R, Kumar V, Southall B, Spletzer J, Taylor CJ. Real-time vision-
based control of a nonholonomic mobile robot. In: Proceedings of 2001 IEEE
international conference on robotics and automation. Seoul, Korea; May 2126,
2001. p. 171419.
[18] Carelli R., Soria CM, Morales B. Vision-based tracking control for mobile robots. In:
Proceedings of twelveth international conference on advanced robotics (ICAR’05).
Seatle, WA; July, 1820, 2005. p. 14852.
[19] Maya-Mendez M, Morin P, Samson C. Control of a nonholonomic mobile robot via
sensor-based target tracking and pose estimation. In: Proceedings of 2006 IEEE/RSJ
international conference on intelligent robots and systems. Beijing, China; October
915, 2009. p. 561218.
[20] Dixon WE, Dawson DM, Zergeroglu E. Adaptive tracking control of a wheeled mobile
robot via an uncalibrated camera system. IEEE Trans Syst Man Cybern Cybern
2001;31(3):34152.
[21] Soetanto D, Lapierre L, Pascoal A. Adaptive, non-singular path-following control of
dynamic wheeled robots. In: Proceedings of fourtysecond IEEE conference on decision
and control. Maui, HI; December 2003. p. 176570.
382
Introduction to Mobile Robot Control

[22] Abdelkader HH, Mezouar Y, Andreff N, Martinet P. Image-based control of mobile
robot with central catadioptric cameras. In: Proceedings of 2005 IEEE international
conference on robotics and automation. Barcelona, Spain; April 2005. p. 35338.
[23] Kim J, Suga Y. An omnidirectional vision-based moving obstacle detection in mobile
robot. Int J Control Autom Syst 2007;5(6):66373.
[24] Shakernia O, Vidal R, Sastry S. Infinitesimal motion estimation from multiple central
panoramic views. In: Proceedings of IEEE international workshop on motion and video
computing. December, 2002. p. 22934.
[25] Baker S, Nayar S. A theory of single-viewpoint catadioptric image formation. Int J
Comput Vis 1999;35(2):122.
[26] Das AK, Fierro R, Kumar V, Ostrowski JP, Spletzer J, Taylor CJA. Vision-based for-
mation control framework. IEEE Trans Robot Autom 2002;18(5):81325.
[27] Gans NR, Hutchinson SA. A stable vision-based control scheme for nonholonomic
vehicles to keep a landmark in the field of view. In: Proceedings of 2007 international
conference on robotics and automation. Rome, Italy; April 1014, 2007. p. 2196200.
[28] Lopez-Nicolas G, Gans NR, Bhattacharya S, Sagues C, Guerrero JJ, Hutchinson S.
Homography-based control scheme for mobile robots with nonholonomic and field-of-
view constraints. IEEE Trans Syst Man Cybern Cybern 2010;40(4):111527.
[29] Lapierre LP, Soetanto DJ, Pascoal A. Adaptive vision-based path following control of
a wheeled robot. In: Proceedings of IEEE Mediterranean control conference (MED
2002). Lisbon, July 912, 2002. p. 16.
[30] Micaelli A, Samson C. Trajectory tracking for unicycle-type and two-steering—wheels
mobile robots. INRIA Technical Report No. 2097, France; 1993.
[31] Lietmann T, Bornstedt B, Lohmann B. Visual servoing for a non-holonomic mobile
robot using a range-image camera. In: Proceedings of European control conference
(ECC’01). Porto, Portugal; September 47, 2001.
[32] Augustin B, Lietmann T, Lohmann B. Image-based visual servoing of a non-
holonomic mobile platform using a pan-tilt head. In: Proceedings of eight IEEE inter-
national conference on methods and models in automation and robotics (MMAR
2002). Szeczecin, Poland; 2002, p. 9416.
[33] Grigorescu S, Macesanu G, Cocias T, Puiu D, Moldoveanu F. Robust camera pose and
scene structure analysis for service robotics. Robot Auton Syst 2011;58:113.
[34] Kragic D, Christensen HI. Advances in robot vision. Robot Auton Syst 2005;52:13.
[35] Geyer C, Daniilidis K. A unifying theory for central panoramic systems and practical
implementations. In: Vernon D, editor. Proceedings of European Conference on
Computer Vision (ECCV’2000). Dublin: Springer; 2000. p. 44561.
[36] Okamoto Jr J, Grassi Jr V. Visual servo control of a mobile robot using omnidirec-
tional vision. In: Proceedings of mechatronics 2002, University of Twente; June
2426, 2002. p. 41322.
[37] Barreto J, Martin F, Horaud R. Visual servoing/tracking using central catadioptric
images. Expl Rob Springer Tracks Adv Robot 2003;5:24554.
[38] Baker S, Nayar S. A theory of catadioptric image formation. In: Proceedings of 1997
international conference on computer vision (ICCV’97). 1998. p. 3542.
[39] Barreto JP, Araujo H. Geometric properties of central catadioptric line images and their
application to calibration. IEEE Trans Pattern Anal Mach Intell 2005;27(8):132733.
[40] Barreto J.P. General central projection systems: modeling, calibration and visual servo-
ing [Ph.D. thesis]. University of Coimbra; 2003.
[41] Geyer C.M. Catadioptric projective geometry: theory and applications [Ph.D. disserta-
tion]. University of Pennsylvania; 2003.
383
Mobile Robot Control V: Vision-Based Methods

[42] Gong X. Omnidirectional vision for an autonomous surface vehicle [Ph.D. disserta-
tion]. Virginia Polytechnic Institute and State University; 2008.
[43] Barreto JP, Araujo H. Issues on the geometry of central catadioptric image formation.
Proc Comput Vis Pattern Recog 2001;:4227.
[44] Yang F, Wang C. Adaptive tracking control for uncertain dynamic nonholonomic
mobile robots based on visual servoing. J Control Theory Appl 2012;10(1):5663.
[45] Liang Z, Wang C. Robust exponential stabilization of nonholonomic wheeled mobile
robots with unknown visual parameters. J Control Theory Appl 2011;9(2):295301.
384
Introduction to Mobile Robot Control

10 Mobile Manipulator Modeling
and Control
10.1
Introduction
Mobile manipulators (MMs) are robotic systems consisting of articulated arms (manipu-
lators) mounted on holonomic or nonholonomic mobile platforms. They provide the
dexterity of the former and the workspace extension of the latter. They are able to reach
and work over objects that are initially outside the working space of the articulated arm.
Therefore, MMs are appealing for many applications, and today they constitute the
main body of service robots [127]. One of the principal and most challenging pro-
blems in MMs research is to design accurate controllers for the entire system. Due to
the strong interaction and coupling of the mobile platform subsystem and the manipula-
tor arm(s) mounted on the platform, a proper coordination between the respective con-
trollers is needed [18,23]. However, in the literature there are also available unified
control design methods that treat the whole system using the full-state MM model
[3,10,22,24]. In either case the control methods studied in this book (computed-torque
control, feedback linearizing control, robust sliding-mode or Lyapunov-based control,
adaptive control, and visual-based control) can be employed and combined.
The objectives of this chapter are as follows:
G
To present the DenavitHartenberg method of articulated robots’ kinematic modeling,
and provide a practical method for inverse kinematics
G
To study the general kinematic and dynamic models of MMs
G
To derive the kinematic and dynamic model of a differential drive, and a three-wheel
omnidirectional MM, with two-link and three-link mounted articulated arms, respectively
G
To derive a computed-torque controller for the above differential-drive MM, and a
sliding-mode controller for the three-wheel omnidirectional MM
G
To discuss some general issues of MM visual-based control, including a particular indica-
tive example of hybrid coordinated (feedback/open-loop) visual control and a panoramic
visual servoing example.
10.2
Background Concepts
Here, the following aspects which will be used in the chapter are considered:
G
The DenavitHartenberg direct manipulator kinematics method
G
A general method for inverse manipulator kinematics
Introduction to Mobile Robot Control. DOI: http://dx.doi.org/10.1016/B978-0-12-417049-0.00010-9
© 2014 Elsevier Inc. All rights reserved.

G
The manipulability measure concept
G
The two-link manipulator’s direct and inverse kinematic model and manipulability
measure.
10.2.1
The DenavitHartenberg Method
The DenavitHartenberg method provides a systematic procedure for determining
the position and orientation of the end-effector of an n-joint robotic manipulator,
that is, for computing X0 in (2.18).
Consider a link i that lies between the joints i and i 1 1 of a robot as shown in
Figure 10.1.
Each link is described by the distance ai between the (possibly nonparallel) axes
i and i 1 1 of the two joints, and the rotation angle αi from the axis i to the axis
i 1 1 with respect to the common normal of the two axes. Each joint (prismatic or
rotary) is driven by a motor (translational or rotational) that produces the motion of
link i. In overall, the robotic arm has n joints and n 1 1 links. The functional rela-
tion of the end-effector and the displacements of the joints can be found using the
convention of parameters shown in Figure 10.2, called the DenavitHartenberg
parameters.
These parameters refer to the relative position of the coordinate frames
Oi21xi21yi21zi21 and Oixiyizi, and are the following:
G
The length ai of the common normal ΣiOi
G
The distance di between the origin of Oi21 and the point Σi
G
The angle αi between the joint i (i.e., the axis zi21) and the axis zi in the positive (clock-
wise) direction
G
The angle θi between the axis xt21 and the common normal (i.e., rotation about the axis
zi21) in the positive direction.
On the basis of the above, the transfer from the frame Oi21xi21yi21zi21 to the
frame Oixiyizi can be done in four steps:
Step 1: Rotation of frame i 2 1 about the axis zi21 by an angle θi.
Step 2: Translation of frame i 2 1 along the axis zi21 by di.
Step 3: Translation of the rotated axis xi21 (which now coincides with xi) along the com-
mon normal by ai.
Step 4: Rotation about xi by an angle αi.
Link i
Joint i+1
Joint i
αi
ai
Figure 10.1 A robotic link between joints i and i11.
386
Introduction to Mobile Robot Control

Denoting by A
i the result of steps 3 and 4 and by Ai21

the result of steps 1 and 2,
the overall result of steps 1 through 4 is given by:
Ai21
i
5 A
i Ai21

5
cos θi
2sin θi cos αi
sin θi sinαi
ai cos θi
sin θi
cosθi cos αi
2cos θi sinαi
ai sin θi
0
sin αi
cos αi
di
0
0
0
1
2
664
3
775
ð10:1Þ
where Ai21
i
gives the position and orientation of the frame i with respect to
the frame i 2 1. The first three columns of Ai21
i
contain the direction cosines of the
axes of frame i, whereas the fourth column represents the position of frame Oi.
In general, the displacement of joint i is denoted as qi, where:
qi 5 θi for revolute joint,
qi 5 di for prismatic joint.
The position and orientation of link i with respect to link i 2 1 is a function of
qi, that is, Ai21
i
ðqiÞ.
The kinematic equation of a robotic arm gives the position and orientation of
the last link with respect to the coordinate frame of the base, and obviously con-
tains all generalized variables q1; q2; . . .; qn of the joints. Figure 10.3 shows pictori-
ally the consecutive coordinate frames from the base up to the end-effector of the
serial robotic kinematic chain.
According to Eq. (2.18), the matrix Τ given by:
Τ 5 A0
1ðq1ÞA1
2ðq2Þ. . .An21
n
ðqnÞ
ð10:2Þ
Joint i–1
Joint i
Joint i+1
Link i–2
Link i–1
Link i
Link i+1
θi–1
θi
θi+1
θi
Σi
αi
di
x′i
y′i
z′i
xi
xi–1
yi
zi
zi–1
yi–1
Oi–1
Oi
ai
Figure 10.2 DenavitHartenberg robotic parameters.
387
Mobile Manipulator Modeling and Control

represents the position and orientation of the end-effector (which is the final link
with respect to the base). It is now easy to determine Τ for all types of robots
(Figure 10.4) using Eq. (10.2).
10.2.2
Robot Inverse Kinematics
In the direct kinematics problem we are finding Τ knowing the values of
q1; q2; . . .; qn. In the inverse kinematics problem we do the converse, that is, given
Τ we determine q1; q2; . . .; qn by solving (10.2) with respect to qiði 5 1; 2; . . .; nÞ.
The direct kinematics Eq. (10.2) can be written in the vectorial form:
X 5 fðqÞ
ð10:3Þ
zi–1
Oi–1
O1
T
O0
y0
y1
Oi
xn
zn
yn
On
yi
zi
xi–1
yi–1
x1
z1
z0
x0
xi
Ai
i–1(qi)
Figure 10.3 Pictorial representation of the end-effector position and orientation by the 4 3 4
matrix Τ.
3
2
1
LLL
(A)
3
2
1
(B)
RLL
3
2
1
(C)
RRL
3
2
1
(D)
RRR
1
2
3
(E)
RRL
o
a
n
(F)
Figure 10.4 The five types of industrial robotic arms: (A) Cartesian, (B) cylindrical,
(C) spherical (polar), (D) articulated (anthropomorphic), (E) SCARA robot, and (F)
end-effector coordinate frame. L denotes linear (translational) motion, and R denotes
rotational motion.
388
Introduction to Mobile Robot Control

where X is the six-dimensional vector:
X 5
p
?
ψ
2
4
3
5;
p 5
x
y
z
2
4
3
5;
ψ 5
ψx
ψy
ψz
2
4
3
5
ð10:4Þ
of the end-effector’s position p and orientation ψ, f is a six-dimensional nonlinear
column vectorial function, and
q 5 ½q1; q2; . . .; qnΤ
ð10:5Þ
Therefore, the inverse kinematics equation is:
q 5 f21ðXÞ
ð10:6Þ
where f21ðÞ denotes the usual inverse function of fðÞ.
A straightforward practical method for inverting the kinematic Eq. (10.2) is the
following. We start from:
Τ 5 A0
1ðq1ÞA1
2ðq2Þ. . .A5
6ðq6Þ
and obtain the following sequence of equations:
ðA0
1Þ21Τ
5 Τ1
6
ðA1
2Þ21ðA0
1Þ21Τ
5 Τ2
6
ðA2
3Þ21ðA1
2Þ21ðA0
1Þ21Τ
5 Τ3
6
ðA3
4Þ21ðA2
3Þ21ðA1
2Þ21ðA0
1Þ21Τ
5 Τ4
6
ðA4
5Þ21ðA3
4Þ21ðA2
3Þ21ðA1
2Þ21ðA0
1Þ21Τ
5 Τ5
6
ð10:7Þ
The elements of the left-hand sides of these equations are functions of the ele-
ments of Τ and the first i 2 1 variables of the robot. The elements of the right-hand
sides are constants or functions of the variables qi; qi11; . . .; q6. From each matrix
equation we get 12 equations, that is, one equation for each of the elements of the
four vectors n; o; a; and p 5 x0. From these equations we can determine the values
of qiði 5 1; 2; . . .; 6Þ of the robot.
Although the solution of the direct kinematics problem is unique, it is not so for
the inverse kinematics problem, because of the presence of trigonometric functions.
In some cases, the solution can be found analytically, but, in general, the solution
can only be found approximately using some approximate numerical method and the
computer. Also, if the robot has more than six degrees of freedom (i.e., if we have a
redundant robot), there are infinitely many solutions to qiði 5 1; 2; . . .; n; n . 6Þ that
lead to the same position and orientation of the end-effector.
389
Mobile Manipulator Modeling and Control

10.2.3
Manipulability Measure
An important factor that determines the ease of arbitrarily changing the position
and orientation of the end-effector by a manipulator is the so-called manipulability
measure. Other factors are the size and geometric shape of the workspace envelope,
the accuracy and repeatability, the reliability and safety, etc. Here, we will examine
the manipulability measure, which was fully studied by Yoshikawa.
Given a manipulator with n degrees of freedom, the direct kinematics equation
is given by Eq. (10.3):
X 5 fðqÞ
where X is the m-dimensional vector ðm 5 6Þ of the end-effector’s position
p 5 ½x; y; zΤ and orientation vector ψ 5 ½ψx; ψy; ψzΤ. Differentiating this relation
we get the well-known differential kinematics model:
_X 5 JðqÞ_q
where JðqÞ is the manipulator’s Jacobian. This relation relates the velocity
_q 5 ½_q1; _q2; . . .; _qnΤ of the manipulator joints with the velocity of the end-effector,
that is, the screw of the robot.
The set of all end-effector velocities that are realizable by joint velocities such as:
jj_qjj 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
_q2
1 1 _q2
2 1 ? 1 _q2
n
q
# 1
is an ellipsoid in the m-dimensional Euclidean space where m is the dimensionality
of _X. The maximum speed of motion of the end-effector is along the major axis, and
the minimum speed along the minor axis of this ellipsoid is called manipulability
ellipsoid. One can show that the manipulability ellipsoid is the set of all v 5 _X that
satisfy:
vΤðJyÞΤJyv # 1
for all v in the range of J. Indeed, by using the relation _q 5 Jyv 1 ðI 2 JyJÞk
(k 5 arbitrary constant vector) and the equality ðI2JyJÞΤJy 5 0, we get
jj_qjj2 5 _qΤ _q 5 vΤðJyÞΤJyv 1 2kΤðI2JyJÞΤJyv
1 kΤðI2JyJÞΤðI 2 JyJÞk
$ vΤðJyÞΤJyv
Thus, if jj_qjj # 1, then vΤðJyÞΤJyv # 1. Conversely, if we choose an arbitrary ^v
such that ^vΤðJyÞΤJy^v # 1, then there exists a vector ^z such that ^v 5 J^z, that is,
_^q5 Jy^v. Then, we find that:
J_^q5 JJy^v 5 JJyJ^z 5 J^z 5 ^v
390
Introduction to Mobile Robot Control

and
jj_qjj 5 ^vðJyÞΤJy^v # 1
In nonsingular configurations, the manipulability ellipsoid is given by:
vΤðJ21ÞΤJ21v # 1
The manipulability measure w of the manipulator is defined as:
w 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
det JðqÞJΤðqÞ
q
which for m 5 n reduces to:
w 5 jdet JðqÞj
In this case, the set of all velocities that are implemented by a velocity _q of the
joints such that:
j_qij # 1;
i 5 1; 2; . . .; m
is a parallelepiped in m-dimensional space with a volume of 2mw. This means that
the measure w is proportional to the volume of the parallelepiped, a fact that pro-
vides a physical representation of the manipulability measure.
10.2.4
The Two-Link Planar Robot
10.2.4.1
Kinematics
Consider the planar robot of Figure 10.5.
The kinematic model of this robot can be found by simple trigonometric calcula-
tion. Referring to Figure 10.5 we readily obtain the direct kinematic model:
xðθ1; θ2Þ 5 l1 cos θ1 1 l2 cosðθ1 1 θ2Þ
yðθ1; θ2Þ 5 l1 sin θ1 1 l2 sinðθ1 1 θ2Þ
ð10:8aÞ
that is:
p 5 fðθÞ;
p 5
x
y
 
;
θ 5
θ1
θ2


ð10:8bÞ
To derive the inverse kinematic model θ 5 f21ðpÞ we work on Figure 10.5B.
Thus, using the cosine rule we find:
x2 1 y2 5 l2
1 1 l2
2 2 2l1l2 cosð180 2 θ2Þ
391
Mobile Manipulator Modeling and Control

from which the angle θ2 is found to be:
θ2 5 arccos½ðx2 1 y2 2 l2
1 2 l2
2Þ=2l1l2
ð10:9aÞ
The angle θ1 is equal to:
θ1 5 φ1 2 φ2
where:
tan φ1 5 y
x ;
tan φ2 5
l2 sin θ2
l1 1 l2 cos θ2
Therefore:
θ1 5 arctan y
x
 
2 arctan
l2 sin θ2
l1 1 l2 cos θ2


ð10:9bÞ
Actually, we have two configurations that lead to the same position p of the
end-effector, viz., elbow-down and elbow-up as shown in Figure 10.5. When
ðx; yÞ 5 ð0; 0Þ, that can be obtained if l1 5 l2, the ratio y=x is not defined.
(B)
θ2
θ2
θ1
θ1
φ1
φ2
l2
l2
l1
l1
y
y
y
x  +  y
(x,y)
(x,y)
x
x
O
O
(A)
x
l2
l1
θ1
θ1
x0
y0
O0
y
Elbow-down
Elbow-up
Figure 10.5 (A) Planar two degrees of freedom robot and (B) geometry for finding the
inverse kinematic model (elbow-down, elbow-up).
392
Introduction to Mobile Robot Control

If θ2 5 180, the base ðx; yÞ 5 ð0; 0Þ can be reached for all θ1. Finally, when a point
is out of the workspace, the inverse kinematic problem has no solution.
The differential kinematics equation is:
dp 5 J dθ;
J 5
@x=@θ1
@x=@θ2
@y=@θ1
@y=@θ2


Here, the Jacobian matrix J is obtained by differentiating Eq. (10.8a), that is:
J 5
J11
J12
J21
J22


5 2l1 sin θ1 2 l2 sinðθ1 1 θ2Þ
2l2 sinðθ1 1 θ2Þ
l1 cos θ1 1 l2 cosðθ1 1 θ2Þ
l2 cosðθ1 1 θ2Þ


ð10:10Þ
The inverse of J is found to be:
J21 5
1
J11J22 2 J21J12
J22
2J12
2J21
J11


5
1
l1l2 sin θ2
l2 cosðθ1 1 θ2Þ
l2 sinðθ1 1 θ2Þ
2l1 cos θ1 2 l2cosðθ1 1 θ2Þ
2l1 sin θ1 2 l2 sinðθ1 1 θ2Þ


ð10:11Þ
Thus, the inverse differential kinematics equation of the robot is:
dθ
dt 5 J21 dp
dt
ð10:12Þ
The
singular
(degenerate)
configurations
occur
when
det J 5 J11J22 2 J21
J22 5 l1l2 sin θ2 5 0, that is, when θ2 5 180 or θ2 5 0. These two configurations
correspond, respectively to the origin ð0; 0Þ and to the full extension (i.e., when the
robot end-effector is at the boundary of the workspace) (see Figure 10.5B).
10.2.4.2
Dynamics
To derive the dynamic model of the robot we apply directly the Lagrange method.
Consider the notation of Figure 10.6.
The symbols θ1 and θ2 have the usual meaning, m1 and m2 are the masses of the
two links (concentrated at their centers of gravity), and l1 and l2 are the lengths of
the links. The symbol lc i denotes the distance of the ith-link’s center of gravity
(COG) from the axis of the joint i, and ~Ii denotes the moment of inertia of link i
with respect to the axis passing via the COG of this link and is perpendicular to the
plane xy (parallel to the axis z). Here, q1 5 θ1 and q2 5 θ2, and the kinematic and
potential energies of the links 1 and 2 are given by:
K1 5 1
2 m1l2
c1 _θ
2
1 1 1
2
~I1 _θ
2
1;
P1 5 mglc1S1
ð10:13aÞ
393
Mobile Manipulator Modeling and Control

K2 5 1
2 m2_sΤ
2 _s2 1 1
2
~I2ð_θ
2
1 1 _θ 1 _θ 2Þ;
P2 5 m2gðl1S1 1 lc2S12Þ
ð10:13bÞ
where S1 5 sin θiði 5 1; 2Þ;
Ci 5 cos θi
ði 5 1; 2Þ;
s2 5 ½s2x; s2yΤ is the position
vector of the COG of link 2 with:
s2x 5 l1C1 1 lc2C12;
s2y 5 l1S1 1 lc2S12
and Cij 5 cosðθi 1 θjÞ;
Sij 5 sinðθi 1 θjÞ.
Using the Lagrangian function of the robot, L 5 K1 1 K2 2 P1 2 P2, we find the
equations:1
D11 €θ 11D12 €θ 21h122 _θ
2
212h112 _θ 1 _θ 21g15τ1; D21 €θ 11D22 €θ 21h211 _θ
2
11g25τ2
ð10:14Þ
with:
D11 5 m1l2
c1 1 ~I1 1 m2ðl2
1 1 l2
c2 1 2l1lc2C2Þ 1 ~I2
D12 5 D21 5 m2ðl2
c2 1 l1lc2C2Þ 1 ~I2;
D22 5 m2l2
c2 1 ~I2
h122 5 h112 5 2 h211 5 2 m2l1lc2S2
g1 5 m1glc1C1 1 m2gðl1C1 1 lc2C12Þ;
g2 5 m2glc2C12
ð10:15Þ
where τ1 and τ2 are the external torques applied to joints 1 and 2. The coefficient
Dii is the effective inertia of the joint i, Dij is the coupling inertia of joints i and j, hijj
is the coefficient of centrifugal force, hijk
ðj 6¼ kÞ is the coefficient of Coriolis
acceleration of the joint i due to the velocities of the joints j and k, and gi ði 5 1; 2Þ
represent the torques due to gravity. The dynamic relations in Eqs. (10.14) and
(10.15) can be written in the standard compact form:
DðθÞ €θ 1 hðθ; _θ Þ 1 gðθÞ 5 τ
ð10:16aÞ
x
y
l1
l2
l2
lc1
lc2
l1
m1
m2
θ1
θ2
Link 1
Link 2
Joint 1
Joint 2
Figure 10.6 Two-link planar robot.
1 If there is friction τf in the motors of the joints (e.g., τf 5 2 β _θ i;
β 5 friction coefficient), then τi
should be replaced by τ0
i 5 τi 2 β _θ i.
394
Introduction to Mobile Robot Control

DðθÞ 5
D11
D12
D21
D22


;
gðθÞ 5
g1
g2


;
τ 5
τ1
τ2


ð10:16bÞ
hðθ; _θ Þ 5 col
X
2
j51
X
2
k51
@Dij
@θk
2 1
2
@Djk
@θi


_θ j _θ k
"
#
ð10:16cÞ
where col½hi denotes a column vector with elements hiði 5 1; 2Þ. In the special
case, where the link masses m1 and m2 are assumed to be concentrated at the end
of each link, we have lc1 5 l1 and lc2 5 l2. It is easy to verify the properties
described by Eqs. (3.12) and (3.13) and the antisymmetricity of _D 2 2C, where K
is the total kinetic energy K 5 K1 1 K2 of the robot.
10.2.4.3
Manipulability Measure
If we use the position ½x; yΤ of the end-effector as the vector v 5 _X, then (see
(10.10)):
J 5 2l1 sin θ1 2 l2 sinðθ1 1 θ2Þ
2l2 sinðθ1 1 θ2Þ
l1 cos θ1 1 l2 cosðθ1 1 θ2Þ
l2 cosðθ1 1 θ2Þ


and so the manipulability measure w is:
w 5 jdet Jj 5 l1l2jsin θ2j
Thus, the optimal configurations of the manipulator, at which w is maximum,
are: θ2 5 6 90 for any l1; l2 and θ1. If the lengths l1 and l2 are specified under the
condition of constant total length (i.e., l1 1 l2 5 const:), then the manipulability
measure takes its maximum when l1 5 l2 for any θ1 and θ2.
10.3
MM Modeling
The total kinematic and dynamic models of an MM are complex and strongly cou-
pled, combining the models of the mobile platform and the fixed robotic manipula-
tor [1,3,7]. Today, MMs are in use with differential-drive, tricycle, car-like, and
omnidirectional platforms that offer maximum maneuverability.
10.3.1
General Kinematic Model
Consider the MM of Figure 10.7 which has a differential-drive platform and a mul-
tilink robotic manipulator.
395
Mobile Manipulator Modeling and Control

Here, we have the following four coordinate frames:
Owxwywzw is the world coordinate frame,
Opxpypzp is the platform coordinate frame,
Obxbybzb is the manipulator base coordinate frame,
Oexeyeze is the end-effector coordinate frame.
Then, the manipulator’s end-effector
position/orientation
with respect
to
Owxwywzw is given by (see Eq. (10.2)):
Τ 5 Aw
p Ap
bAb
e
ð10:17Þ
where:
Aw
p is the transformation matrix from Ow to Op,
Ap
b is the transformation matrix from Op to Ob,
Ab
e is the transformation matrix from Ob to Oe.
In vectorial form, the end-effector position/orientation vector xw
e in world coor-
dinates has the form:
xw
e 5 FðqÞ
ð10:18Þ
where:
q 5 ½pΤ; θΤΤ
p 5 ½x; y; φΤ ðPlatform configurationÞ
θ 5 ½θ1; θ2; . . .; θnmΤ ðManipulator configurationÞ
Therefore, differentiating Eq. (10.18) with respect to time we get:
_xw
e 5
@F
@p


_p 1
@F
@θ


_θ
ð10:19Þ
Link 1
Link 2
Link 3
Link n m
Active
wheels
xw
zw
zp
zb
xb
xe
yb
ye
φ
0p
0b
0e
0w
ze
yp
xp
yw
Passive
wheel
Figure 10.7 Geometric features of an MM with differential-drive platform.
396
Introduction to Mobile Robot Control

where _p is given by the kinematic model of the platform (see (6.1)):
_p 5 GðpÞup;
upAR2
ð10:20aÞ
and _θ by the manipulator’s kinematic model. Assuming that the manipulator is
constraint-free, we can write:
_θ 5 um
ð10:20bÞ
where um is the vector of manipulator’s joint commands. Combining Eqs. (10.19),
(10.20a), and (10.20b) we get the overall kinematic model of the MM:
_xw
e ðtÞ 5
@F
@p
0
@
1
AGðpÞup 1
@F
@θ
0
@
1
Aum
5 JðqÞuðtÞ
ð10:21aÞ
where:
uðtÞ 5 ½uΤ
pðtÞ; uΤ
mðtÞΤAR21nm
JðqÞ 5 ½JpðqÞGðqÞ^JmðθÞ
JpðqÞ 5 @F=@p
JmðqÞ 5 @F=@θ
ð10:21bÞ
Equation (10.21a) represents the total kinematic model of the MM from the
inputs to the end-effector (task) variables.
Actually, in the present case the system is subject to the nonholonomic
constraint:
MðpÞ_p 5 0;
MðpÞ 5 ½2 sin φ; cos φ; 0; . . .; 0
ð10:22Þ
where _p cannot be eliminated by integration. Therefore, JðqÞ should involve a row
representing this constraint. Since the dimensionality 2 1 nm of the control input
vector uðtÞ is less than the total number 3 1 nm of variables (degrees of freedom) to
be controlled, the system is always underactuted.
10.3.2
General Dynamic Model
The Lagrange dynamic model of the MM is given by Eq. (3.16):
DðqÞ€q 1 Cðq;_qÞ_q 1 gðqÞ 1 MΤðqÞλ 5 Eτ
ð10:23Þ
397
Mobile Manipulator Modeling and Control

where MðqÞ is given by Eq. (10.22). This model contains two parts, namely:
Platform Part
Dpðqp; qmÞ€qp 1 Cpðqp; qm; _qp; _qmÞ 5 Epτp 2 MΤðqpÞλ 2 Dpðqp; qmÞ€qm
Manipulator Part
DmðqmÞ€qm 1 Cmðqp; qm; _qpÞ 5 τm 2 Dmðqp; qmÞ€qp
where the index p refers to the platform and m to the manipulator, and the symbols
have the standard meaning. Appling the technique of Section 3.2.4 we eliminate
the nonholonomic constraint MðqpÞ_qp 5 0, and get the reduced (unconstrained)
model of the form of Eqs. (3.19a) and (3.19b):
DðqÞ_v 1 Cðq;_qÞv 1 gðqÞ 5 Eτ
ð10:24Þ
where:
q 5 ½qΤ
p; qΤ
mΤ
and DðqÞ; Cðq;_qÞ, and gðqÞ are given by the combination of the respective terms in
the platform and manipulator parts (the details of derivation are straightforward
and are left as exercise).
10.3.3
Modeling a Five Degrees of Freedom Nonholonomic MM
Here, the above general methodology will be applied to the MM of Figure 10.8
which consists of a differential-drive mobile platform and a two-link planar manip-
ulator [3,22].
10.3.3.1
Kinematics
Without loss of generality, the COG G is assumed to coincide with the rotation
point Q (midpoint of the two wheels), that is, b 5 0. The nonholonomic constraint
of the platform is:
2 _xQ sin φ 1 _yQ cos φ 5 0
This constraint, when expressed at the base Obðxb; ybÞ of the manipulator,
becomes:
2 _xb sin φ 1 _yb cos φ 1 lb _φ 5 0
398
Introduction to Mobile Robot Control

where lb is the distance between the points GðQÞ and Ob. The kinematic equations
of the wheeled mobile robot (WMR) platform (at the point Ob) are:
_xb 5
r
2 cos φ 1 rlb
2a sin φ
0
@
1
A_θ l 1
r
2 cos φ 2 rlb
2a sin φ
0
@
1
A_θ r
_yb 5
r
2 sin φ 2 rlb
2a cos φ
0
@
1
A_θ l 1
r
2 sin φ 1 rlb
2a cos φ
0
@
1
A_θ r
_φ 5 r
2a ð_θ r 2 _θ lÞ
ð10:25Þ
which are written in the Jacobian form:
_p 5 J _θ ;
_p 5 ½_xb; _yb; _φΤ;
_θ
Τ 5 ½_θ l; _θ rΤ
where:
J 5
Jb
^
2 r=2a
r=2a
2
4
3
5
Jb 5
r
2 cos φ 1 rlb
2a sin φ
r
2 cos φ 2 rlb
2a sin φ
r
2 sin φ 2 rlb
2a cos φ
r
2 sin φ 1 rlb
2a cos φ
2
66664
3
77775
5 RðφÞWb
ð10:26Þ
y
ye
y1
x1
l1
l1
l2
lc2
l2
m2
E
(m0, I0)
a
a
Q
r
r
Ob
lb
xe
x
m1
lc1
θl
θr
θ1
θ2
ψe
φ
0
.
.
Figure 10.8 The five degrees
of freedom MM. Platform’s
mass and moment of inertia
m0 and I0.
399
Mobile Manipulator Modeling and Control

with:
RðφÞ 5
cos φ
2sin φ
sin φ
cos φ


;
Wb 5
r=2
r=2
2rlb=2a
rlb=2a


ð10:27Þ
The kinematic parameters of the two-link manipulator are those of Figure 10.6,
where now its first joint has linear velocity _φ 1 _θ 1. The linear velocity ½_xe; _yeΤ of
the end-effector is given by:
_xe
_ye


5
_xb
_yb


1 RðφÞJmðθmÞ
_φ 1 _θ 1
_θ 2


ð10:28aÞ
where ðθm 5 ½θ1; θ2ΤÞ:
JmðθmÞ 5
2l1 sin θ1 2 l2 sinðθ1 1 θ2Þ
2l2 sinðθ1 1 θ2Þ
l1 cos θ1 1 l2 cosðθ1 1 θ2Þ
l2 cosðθ1 1 θ2


5
Jm;11
Jm;12
Jm;21
Jm;22


ð10:28bÞ
is the Jacobian matrix of the manipulator with respect to its base coordinate frame
(see Eq. (10.10)). Here, _φ is given by the third part of Eq. (10.25), that is,
_φ 5 ðr=2aÞð2 _θ l 1 _θ rÞ. Thus:
_φ 1 _θ 1
_θ 2


5 S
_θ l_θ r


;
S 5
1 2 r=2a
r=2a
0
1


ð10:29aÞ
and so:
_xe
_ye


5 ½RðφÞWb 1 RðφÞJmðθmÞS
_θ l_θ r


ð10:29bÞ
The overall kinematic equation of the MM is:
_p0 5 J0 _θ 0
ð10:30aÞ
where:
_p0 5
_xe
_ye
^
_xb
_yb
2
6666664
3
7777775
;
_θ 0 5
_θ l
_θ r
^
_θ 1
_θ 2
2
6666664
3
7777775
;
J0 5
RðφÞ½Wb 1 JmðθmÞS
????????
RðφÞWb
2
64
3
75
ð10:30bÞ
400
Introduction to Mobile Robot Control

If the mobile platform of the MM is of the tricycle or car-like type shown in
Figure 2.8 or Figure 2.9, its position/orientation is described by four variables _xQ; _yQ; _φ;
and _ψ, and the kinematic Eq. (2.45) or Eq. (2.53) is enhanced as in Eq. (10.25) with the
lb terms. The overall kinematic model of the MM is found by the same procedure.
10.3.3.2
Dynamics
Assuming that the moment of inertia of the passive wheel is negligible, the
Lagrangian of the MM is found to be:
L 5 1
2 m0ð_x2
Q 1 _y2
QÞ 1 1
2 I0 _φ
2 1 1
2 m1ð_x2
A 1 _y2
AÞ
1 1
2 I1ð_φ1 _θ 1Þ2 1 1
2 m2ð_x2
B 1 _y2
BÞ 1 1
2 I2ð_φ 1 _θ 1 1 _θ 2Þ
ð10:31Þ
where:
m0; I0 are mass and moment of inertia of the platform,
m1; m2; I1; I2 are masses and moments of inertia of links 1 and 2, respectively,
_xQ; _yQ are x; y components of the velocity of point Q.
_xA; _yA; _xB; _yB are x; y velocities of links 1 and 2, respectively.
Using the Lagrangian (10.31) we derive the dynamic model of Eq. (10.23), with
the nonholonomic constraint:
MðqÞ 5 ½ 2 sin φ; cos φ; lb; 0; 0
ð10:32Þ
Now, using the transformation _qðtÞ 5 BðqÞvðtÞ with BΤðqÞMΤðqÞ 5 0, where:
q 5 ½xb; yb; φ; θ1; θ2Τ
v 5 ½v1; v2; v3; v4Τ 5 ½_θ l; _θ r; _θ 1; _θ 2Τ
and B is selected as:
B 5
B0
O
O
I


;
B0 5
b0
11
b0
12
b0
21
b0
22
b0
31
b0
32
2
4
3
5
ð10:33Þ
where I is the 2 3 2 unit matrix, and
b0
11 5 rðcos φÞ=2 1 lbrðsin φÞ=2a
b0
12 5 rðcos φÞ=2 2 lbrðsin φÞ=2a
b0
21 5 rðsin φÞ=2 2 lbrðcos φÞ=2a
b0
22 5 rðsin φÞ=2 1 lbrðcos φ=2a
b0
31 5 2 r=2a;
b0
32 5 r=2a
ð10:34Þ
the constrained Lagrangian model is reduced, as usual, to the unconstrained
form (10.24).
401
Mobile Manipulator Modeling and Control

Example 10.1
It is desired to investigate the manipulability measure of an MM and compute it for a five
degrees of freedom MM.
Solution
The manipulability measure is given by:
w 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
det JðqÞJΤðqÞ
q
or w 5 jdet Jj, if J is square. Let us define by σ1; σ2; . . .; σm, the m larger numbers
ﬃﬃﬃﬃﬃ
λi
p
; i 5 1; 2; . . .; n, where λi is the ith eigenvalue of the matrix JΤJ. Then, the above defi-
nition of w reduces to:
w1 5 σ1σ2. . .σm;
σ1 $ σ2 $ ? . σm $ 0
The numbers σiði 5 1; 2; . . .; mÞ are known as singular values of J, and form the
matrix Σ:
Σ 5
σ1
0
^
&
0
0
σm
^
2
4
3
5
which defines the singular value decomposition J 5 UΣVΤ of J with U and V being orthog-
onal matrices of dimensionality m and n respectively. The principal axes of the manipula-
bility ellipsoid are σ1u1; σ2u2; . . .; σmum where ui are the columns of U. Some other
definitions of the manipulability measure are:
(i) w2 5 σm=σ1: The ratio of the minimum and maximum singular values (radii) of the
manipulator ellipsoid. This provides only qualitative information about the manipu-
lability of the robot. If σm 5 σ1, then the ellipsoid is a sphere and the robot has the
same manipulability in all directions.
(ii) w3 5 σm: The minimum radius which gives the upper bound of the velocity of the
end-effector motion in any direction.
(iii) w4 5 ðσ1σ2. . .σmÞ1=m: The radius of the sphere that has the same volume of the
manipulability ellipsoid.
(iv) w5 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 2 σ2
m=σ2
1
p
: The generalized concept of eccentricity of an ellipse.
Consider the manipulability definition via JðqÞ. In the present case, the total Jacobian
matrix of the MM is given by Eqs. (10.21a) and (10.21b), and represents the transforma-
tion from:
uðtÞ 5
upðtÞ
umðtÞ


;
upðtÞ 5
vðtÞ
ωðtÞ


;
umðtÞ 5
θ1
θ2
^
θm
2
664
3
775
to
xe 5 ½_xe; _ye; _ψΤ
402
Introduction to Mobile Robot Control

where xe; ye are the position coordinates, and ψe the orientation angle of the end-effector.
Then:
w 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
det JðqÞJΤðqÞ
q
and the MM manipulability ellipsoid is defined by:
jjujj 5
					
upðtÞ
umðtÞ

					 # 1
Unfortunately, in most cases it is not possible to decouple upðtÞ and umðtÞ in comput-
ing the ellipsoid jjujj # 1. Now, consider the five-link MM of Figure 10.8. It is straightfor-
ward to find that:
J 5
J11
J12
J13
J14
J21
J22
J23
J24
0
0
0
1
2
4
3
5
J11 5 cos φ
J12 5 2 lb sin φ 2 l1 sinðφ 1 θ1Þ 2 l2 sinðφ 1 θ1 1 θ2Þ
J13 5 2 l1 sinðφ 1 θ1Þ 2 l2sinðφ 1 θ1 1 θ2Þ
J14 5 2 l2 sinðφ 1 θ1 1 θ2Þ
J21 5 sin φ
J22 5 lb cos φ 1 l1 cosðφ 1 θ1Þ 1 l2 cosðφ 1 θ1 1 θ2Þ
J23 5 l1 cosðφ 1 θ1Þ 1 l2 cosðφ 1 θ1 1 θ2Þ
J24 5 l2 cosðφ 1 θ1 1 θ2Þ
Consider the manipulability measure w5 which gives information on the shape of the ellip-
ses. As w5 tends to zero, that is, σm=σ1 ! 1 the ellipsoid tends to a sphere and the
attainable end-effector speeds tend to be the same in all directions. Actually, they can be
equal (isotropic) when w5 5 0, for a given bounded velocity control signal.
10.3.4
Modeling an Omnidirectional MM
Here, the kinematic and dynamic model of the MM shown in Figure 10.9 will be
derived. This MM involves a three-wheel omnidirectional platform (with orthogo-
nal wheels) and a 3D manipulator [8,11].
10.3.4.1
Kinematics
As usual, the kinematic model of the MM is found by combining the kinematic
models of the platform and the manipulator.
The workspace of the robot is described by the world coordinate frame
Owxwywzw. The platform’s coordinate frame Opxpypzp has its origin at the COG of
403
Mobile Manipulator Modeling and Control

the platform, and the wheels have a radius r, an angle 120 between them, and a
distance D from the platform’s COG. The kinematic model of the robot was
derived in Section 2.4. For the platform shown in Figure 10.9, the model given by
Eqs. (2.74a) and (2.74b) becomes:
_θ p1
_θ p2
_θ p3
2
4
3
5 5 1
r
21=2
ﬃﬃﬃ
3
p
=2
D
21=2
2
ﬃﬃﬃ
3
p
=2
D
1
0
D
2
4
3
5
_xp
_yp
_φ
2
4
3
5
that is,
_θ r 5 J21
0 _pp
ð10:35Þ
where _θ piði 5 1; 2; 3Þ are the angular velocities of the wheels, φ the rotational angle
between xw and xp axes, and _xp; _yp are the components of the platform’s velocity
expressed in the platform’s (moving) coordinate system. The coordinate transfor-
mation matrix between the world and the platform coordinate frames is:
Rw
p 5
cos φ
2sin φ
0
sin φ
cos φ
0
0
0
1
2
4
3
5
ð10:36Þ
Thus, the relation between vw
p 5 ½_xw
p ; _yw
p ; _φΤ and _θ r 5 ½_θ p1; _θ p2; _θ p3Τ is
_θ r 5 J21
0 Rw
p vw
p
ð10:37Þ
Inverting Eq. (10.37) we get the direct differential kinematic model of the robot
from _θ r to vw
p as:
vw
p 5 JpðφÞ _θ r
ð10:38aÞ
120°
30°
D
yw
xp
φ
0w
xw
0p
yp
θp2
.
θp1
.
θp3
.
Figure 10.9 Geometric features of the
omnidirectional platform.
404
Introduction to Mobile Robot Control

where:
JpðφÞ 5 ðJ21
0 Rw
p Þ21
5 r
3
2cos φ 2
ﬃﬃﬃ
3
p
sin φ
2cos φ 1
ﬃﬃﬃ
3
p
sin φ
2 cos φ
2sin φ 1
ﬃﬃﬃ
3
p
cos φ
2sin φ 2
ﬃﬃﬃ
3
p
cos φ
2 sin φ
1=D
1=D
1=D
2
64
3
75
ð10:38bÞ
Now, the platform’s kinematic model of Eq. (10.38a) will be integrated with the
manipulator’s kinematic model (see Figure 10.10).
From Figure 10.9 we get:
xw
e 5 xw
p 1 ½l2 cos θ2 1 l3 cosðθ2 1 θ3Þcosðφ 1 θ1Þ
yw
e 5 yw
p 1 l2 cos θ2 1 l3 cosðθ2 1 θ3Þ
½
sinðφ 1 θ1Þ
zw
e 5 r 1 H 1 l1 1 l2 sin θ2 1 l3 sinðθ2 1 θ3Þ
ð10:39Þ
where H is the height of the platform. Now, differentiating Eq. (10.39) and using
Eqs. (10.38a) and (10.38b) we get:
_pe 5 Jeðφ; θ1; θ2; θ3Þ_q
ð10:40Þ
where: _q 5 ½_θ p1; _θ p2; _θ p3; _θ 1; _θ 2; _θ 3Τ and Je is a 3 3 6 matrix with columns:
Je1 5
2r
3ðCΦ1
ﬃﬃﬃ
3
p
SΦÞ2 r
3Dðl2C2SΦ1 1l3C23SΦ1Þ
r
3ð2SΦ1
ﬃﬃﬃ
3
p
CΦÞ1 r
3Dðl2C2CΦ1 1l3C23CΦ1Þ
0
2
666664
3
777775
;Je4 5
2l2C2SΦ1 2l3C23SΦ1
l2C2CΦ1 1l3C23CΦ1
0
2
64
3
75
Je2 5
r
3ð2CΦ1
ﬃﬃﬃ
3
p
SΦÞ2 r
3Dðl2C2SΦ1 1l3C23SΦ1Þ
2r
3ðSΦ1
ﬃﬃﬃ
3
p
CΦÞ1 r
3Dðl2C2CΦ11l3C23CΦ1Þ
0
2
666664
3
777775
;Je5 5
2l2S2CΦ1 2l3S23CΦ1
2l2S2SΦ1 2l3S23SΦ1
l2C2 1l3C23
2
64
3
75
Je3 5
2
3rCΦ 2 r
3Dðl2C2SΦ1 1l3C23SΦ1Þ
2
3rSΦ 1 r
3Dðl2C2CΦ1 1l3C23CΦ1Þ
0
2
6666664
3
7777775
;Je6 5
2l3S23CΦ1
2l3S23SΦ1
l3C23
2
64
3
75
ð10:41Þ
405
Mobile Manipulator Modeling and Control

The notations used in the above expressions are:
SΦ 5 sin φ
CΦ 5 cos φ
Si 5 sin θ
Ci 5 cos θ
ði 5 1; . . .; 3Þ
SΦ1 5 sinðφ 1 θ1Þ
CΦ1 5 cosðφ 1 θ1Þ
S23 5 sinðθ2 1 θ3Þ
C23 5 cosðθ2 1 θ3Þ
10.3.4.2
Dynamics
To find the dynamic model of the MM, the local coordinate frames are employed
according to the DenavitHartenberg convention. Figure 10.10 shows all the coor-
dinate frames involved. Using the above notations, we obtain the following rota-
tional matrices:
Rp
1 5
C1
0
S1
S1
0
2C1
0
1
0
2
4
3
5;
R1
2 5
C2
2S2
0
S2
C2
0
0
0
1
2
4
3
5;
R2
3 5
C3
2S3
0
S3
C3
0
0
0
1
2
4
3
5
Rw
1 5 Rw
p Rp
1 5
CΦ1
0
SΦ1
SΦ1
0
2CΦ1
0
1
0
2
64
3
75
Rw
2 5 Rw
1 R1
2 5
C2CΦ1
2S2CΦ1
SΦ1
C2SΦ1
2S2SΦ1
2CΦ1
S2
C2
0
2
64
3
75
Rw
3 5 Rw
2 R2
3 5
C23CΦ1
2S23CΦ1
SΦ1
C23SΦ1
2S23SΦ1
2CΦ1
S23
C23
0
2
64
3
75
zw
yw
xp
yp
l1
l2
l3
x1
x2
x3
z1
o1
o2
o3
y1
y2
y3
z2
z3
θ1
θ2
θ3
zp
0w
0p
2r + H
xw
Figure 10.10 Coordinate frames of the
MM.
406
Introduction to Mobile Robot Control

Now, for each rigid body composing the MM, the kinetic and potential energies are
found. Then the Lagrangian function ðLÞ is built. Finally, the equations of the gen-
eralized forces are calculated using the Lagrangian function:
d
dt
@L
@_θ pi
 
!
2 @L
@θpi
5 τpi
ði 5 1; . . .3Þ
d
dt
@L
@_θ i


2 @L
@θi
5 τi
ði 5 1; . . .3Þ
where τpi
ði 5 1; . . .; 3Þ are the generalized torques for the wheel actuators that
drive the platform, and τi
ði 5 1; . . .; 3Þ are the generalized torques for the joint
actuators that drive the links of the manipulator. The parameters of the MM are
defined in Table 10.1:
Then, the differential equations of the dynamic model are expressed in compact
form as:
DðqÞ€q 1 hðq;_qÞ 1 gðqÞ 5 τ
ð10:42Þ
where:
hðq;_qÞ 5 BðqÞ_q  _q 1 CðqÞ_q2
DðqÞ5
1a1
?
1a6
^
&
^
6a1
?
6a6
2
4
3
5;
BðqÞ5
1b1
?
1b15
^
&
^
6b1
?
6b15
2
4
3
5;
CðqÞ5
1c1
?
1c6
^
&
^
6c1
?
6c6
2
4
3
5
Table 10.1 Parameters of the Omnidirectional MM
mr: Mass of each lateral orthogonal wheel
l2: Length of link 2
r: Radius of the wheels
lc2: Distance of the COG of link 2 from the
second joint
mp: Mass of the platform
I2
xx: Moment of inertia of link 2 with respect
to x-axis
D: Distance of the platform’s COG from each
assembly
I2
zz: Moment of inertia of link 2 with respect
to z-axis
g: Gravitational constant (9.8062 m/s2)
ml3: Mass of link 3
ml1: Mass of link 1
l3: Length of link 3
I1
xx: Moment of inertia of link 1 with respect to
x-axis
lc3: Distance of the COG of link 3 from the
third joint
Ip
zz: Moment of inertia of the platform with
respect to z-axis
I3
xx: Moment of inertia of link 3 with respect
to x-axis
ml2: Mass of link 2
I3
zz: Moment of inertia of link 3 with respect
to z-axis
407
Mobile Manipulator Modeling and Control

g q
ð Þ5½1g;2g;3g;4g;5g;6gΤ
€q5½€θ p1; €θ p2; €θ p3; €θ 1; €θ 2; €θ 3Τ
_qU _q5½_θ p1 _θ p2;...; _θ p1 _θ 3; _θ p2 _θ p3;...; _θ p2 _θ 3; _θ p3 _θ 1;...; _θ p3θ3; _θ 1 _θ 2; _θ 1 _θ 3; _θ 2 _θ 3Τ
_q25½_θ
2
p1; _θ
2
p2; _θ
2
p3; _θ
2
1; _θ
2
2; _θ
2
3Τ
τ5½τp1;τp2;τp3;τ1;τ2;τ3Τ
ð10:43Þ
with jai; jbk; cj
i and ig ði; j 5 1; 2; . . .; 6; k 5 1; 2; . . .; 15Þ being defined via intermedi-
ate coefficients [8].
10.4
Control of MMs
10.4.1
Computed-Torque Control of Differential-Drive MM
Here, the control of the five degrees of freedom MM studied in Section 10.3.3 will
be considered [22]. We use the reduced dynamic model of Eq. (10.24) with para-
meters given by Eqs. (10.32)(10.34). The vector q of generalized variables is:
q 5 ½xb; yb; φ; θ1; θ2Τ
ð10:44aÞ
and the vector v is:
v 5 ½v1; v2; v3; v4Τ 5 ½_θ l; _θ r; _θ 1; _θ 2Τ 5 θΤ
0
ð10:44bÞ
To convert v 5 θΤ
0 to the Cartesian coordinates velocity vector:
_p0 5 ½_xe; _ye; _xb; _ybΤ
ð10:44cÞ
we use the Jacobian relation (10.30a) and (10.30b).
_p0 5 J0 _θ 0 5 J0v
ð10:44dÞ
where the 4 3 4 Jacobian matrix J0 is invertible. Differentiating Eq. (10.44d)
we get:
€p0 5 _J0v 1 J0_v
which, if solved for _v, gives:
_v 5 J21
0 ð€p0 2 _J0vÞ
ð10:45Þ
408
Introduction to Mobile Robot Control

Now, introducing Eq. (10.45) into Eq. (10.24), and premultiplying by ðJ21
0 ÞΤ
gives the model:
D €p0 1 F _p0 1 G 5 Eτ
ð10:46Þ
where:
D 5 J21Τ
0
DJ2
0 1
F 5 J21Τ
0
ðC 2 DJ2
0 1_J0ÞJ21
0
G 5 J21Τ
0
g
E 5 J21Τ
0
E
ð10:47Þ
If the desired path for p0ðtÞ is p0;dðtÞ, and the error is defined as ~p0 5 p0;d 2 p0,
we select as usual the computed-torque law:
Eτ 5 Du 1 F _p0 1 G
ð10:48Þ
Introducing Eq. (10.48) into Eq. (10.46), under the assumption that the robot para-
meters are precisely known, gives:
€p0 5 u
ð10:49aÞ
Now, we can use the linear feedback control law:
u 5 €p0;d 1 Kυ_~p0 1 Kp ~p0;
~p0 5 p0;d 2 p0
ð10:49bÞ
In this case, the dynamics of the closed-loop error system is described by:
€~p0 1 Kυ_~p0 1 Kp ~p0 5 0
Therefore, selecting properly Kp and Kυ we can get the desired performance
specifications.
10.4.2
Sliding-Mode Control of Omnidirectional MM
We work with the dynamic model of the MM, given by Eq. (10.42) which due to
the omnidirectionality of the platform does not involve any nonholonomic con-
straint [8,11]. Applying computed-torque control:
τ 5 DðqÞu 1 hðq;_qÞ 1 gðqÞ
ð10:50aÞ
we arrive, as usual, to the linear dynamic system:
€q 5 u
ð10:50bÞ
409
Mobile Manipulator Modeling and Control

and the feedback controller:
u 5 €qd 1 Kυ_~q1 Kp ~q
ð10:50cÞ
that leads to an asymptotically stable error dynamics.
If D; h; and g are subject to uncertainties, and we know only their approxima-
tions ^D;^h;^g, then the computed-torque controller is:
^τ 5 ^DðqÞu 1 ^hðq;_qÞ 1 ^gðqÞ
ð10:51aÞ
and the system dynamics is given by:
€q 5 ðD21 ^DÞu 1 D21ð^h 2 hÞ 1 D21ð^g 2 gÞ
ð10:51bÞ
In this case, some appropriate robust control technique must be used. A conve-
nient controller is the sliding-mode controller described in Section 7.2.2. This con-
troller involves a nominal term (based on the computed-torque control law), and an
additional term that faces the imprecision of the dynamic model.
The sliding condition for a multiple-input multiple-output system is a generali-
zation of Eq. (7.14):
1
2
d
dt sΤðx; tÞsðx; tÞ #2 ηðsΤsÞ1=2; η . 0
ð10:52aÞ
with:
s 5 _~q1 Λ~q 5 _q 2 _qr
ð10:52bÞ
where _qr 5 _qd 2 Λ~q and Λ is a Hurwitz (stable) matrix. The condition (10.52a)
guarantees that the trajectories point towards the surface s 5 0 for all t . 0. This
can be shown by choosing a Lyapunov function of the form:
VðtÞ 5 1
2 sΤDs
where D is the inertia matrix of the MM. Differentiating VðtÞ we get:
_VðtÞ 5 sΤðτ 2 D€qr 2 h 2 gÞ
ð10:53aÞ
The control law is now defined as:
τ 5 ^τ 2 k sgnðsÞ
ð10:53bÞ
410
Introduction to Mobile Robot Control

where k sgnðsÞ is a vector with components ki sgnðsiÞ. Furthermore, the ^τ term is
the control law part which could make _VðtÞ 5 0, if there is no dynamic imprecision
inside the estimated dynamic model, that is, according to Eq. (10.53a):
^τ 5 ^D€q 1 ^h 1 ^g
where ^D;^h; and ^g are the available estimates of D; h; and g. Now, calling D; h;
and g the real matrices of the robot, we define the matrices:
~D 5 ^D 2 D;
~h 5 ^h 2 h;
~g 5 ^g 2 g
as the bounds on the modeling errors. Then, it is possible to choose the components
ki of the vector k such that:
ki $ jj½ ~DðqÞ€qr 1 ~hðq;_qÞ 1 ~gðqÞijj 1 ηi; ηi . 0
ð10:54aÞ
If this control mode is used, the condition:
_VðtÞ # 2
X
n
i51
ηijsij # 0
ð10:54bÞ
is verified to hold. This means that the sliding surface s 5 0 is reached in a finite
time, and that once on the surface, the trajectories remain on the surface and, there-
fore, tend to qdðtÞ exponentially.
Example 10.2
The end-effector of a five degrees of freedom planar MM is dragged by a human operator
to follow a desired trajectory. The problem is:
(a) To derive a nonlinear controller that drives the mobile platform such that the end-
effector follows this trajectory with a desired (preferred) manipulator configuration.
(b) To extend the method of (a) and find a state controller that achieves the trajectory
tracking by completely compensating the platformmanipulator dynamic interaction.
Solution
(a) Clearly, the MM must track the desired trajectory with the manipulator fixed at the
desired configuration which here is selected as the one that maximizes the manipulator’s
manipulability measure (see Section 10.2.4.3):
w 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
det JðqÞJΤðqÞ
q
5 jdet Jj 5 l1l2jsin θ2j
The optimal configurations of the manipulator for which w is maximum are θ2 5 6 90
for any l1; l2; and θ1. Here we choose the desired angles θ2d 51 90 and θ1d 5 2 45 as
411
Mobile Manipulator Modeling and Control

shown in Figure 10.11A. If l1 5 l2 the choice of θ1d 5 2 45 implies that the end-effector
is located on a line parallel to the WMR axis which passes through the point Ob.
The coordinates xrG; yrG (in the platform coordinate frame) of the manipulator end-
effector at the desired (optimal) configuration are constant, being equal to:
xrG 5 xbG 1 l1 cos θ1d 1 l2 cosðθ1d 1 θ2dÞ
5 xbG 1 ð
ﬃﬃﬃ
2
p
=2Þl1 1 ð
ﬃﬃﬃ
2
p
=2Þl2
yrG 5 ybG 1 l1 sin θ1d 1 l2 sinðθ1d 1 θ2dÞ
5 ybG 2 ð
ﬃﬃﬃ
2
p
=2Þl1 1 ð
ﬃﬃﬃ
2
p
=2Þl2
where xbG and ybG are the coordinates of the manipulator base Ob in the platform coordi-
nate frame GxGyG. It is remarked that any trajectory tracking error will result in getting the
manipulator out of the desired configuration, thus reducing the value of the manipulabil-
ity measure. We will apply the affine-systems methodology of Section 6.3.2 [23,24].
Since the manipulator is kept at the desired configuration, its dynamics is ignored.
Therefore, we need only the dynamic model of the platform which in reduced form (with
the nonholonomic constraint eliminated) has the affine state-space form of Eqs. (6.43a)
and (6.43b):
_xðtÞ 5 fðxÞ 1 gðxÞuðtÞ;
u 5 τ 5 ½τr; τlΤ
fðxÞ 5
Bv
^
2D
21Cv
2
64
3
75;
gðxÞ 5
0
^
D
21
2
64
3
75
0
x  
y  
yG
xG
x
End-effector
r 
Desired
trajectory
0
y  
yG
xG
End-effector
E
(A)
(B)
Figure 10.11 (A) The MM at the desired manipulator configuration θ1d 5 2 45 and
θ2d 51 90. The point G is the COG of the platform with world coordinates xG and yG. The
world coordinates of the manipulator base point Ob are xb and yb. (B) While the MM is
moving to achieve trajectory tracking, the platform moves to bring the manipulator into the
preferred configuration.
412
Introduction to Mobile Robot Control

where B; C; and D are given by Eq. (6.41b) and:
x 5
q
^
v
2
4
3
5;
q 5 ½x; y; θr; θlΤ;
v 5 ½v1; v2Τ
Applying the computed-torque-like controller given by Eq. (6.44a):
uðtÞ 5 FðxÞ 1 GðxÞυðtÞ;
FðxÞ 5 Cv;
GðxÞ 5 D
where υðtÞ is a new control vector, the above model takes the form (6.45a)(6.45c):
_xðtÞ 5 fcðxÞ 1 gcðxÞυðtÞ
where:
fcðxÞ 5
Bv
^
0
2
4
3
5;
gcðxÞ 5
0
^
I2 3 2
2
4
3
5
We have seen in Section 6.3.2 that if the output components y1 and y2 are taken to be
the coordinates of the wheels’ axis midpoint Q, the above model can be inputoutput lin-
earizable (and decoupled) by using a dynamic nonlinear state-feedback controller, but it
cannot be linearized by any static state-feedback controller. However, as explained in
Example 6.8 it is possible to decouple the inputs and outputs using a static feedback con-
troller if y1 and y2 are the coordinates xr; yr of a reference point C in front of the vehicle.
To find the general conditions under which a differential-drive WMR can be inputoutput
decoupled by static state-feedback control, we assume that the output vector components
y1 and y2 are the world coordinates xr and yr of an arbitrary reference point, that is:
y 5 hðxÞ 5
xr
yr


We now express xr and yr in the platform’s coordinate frame with origin at the platform
COG G. The result is:
xr 5 xG 1 xrG cos φ 2 yrG sin φ
yr 5 yG 1 xrG sin φ 1 yrG cos φ
where xrG and yrG are the coordinates of the reference point in the platform coordinate
frame.
Working as in Example 6.8 we find, via successive differentiation of y, the dynamic
model of y, namely:
€y 5 _HðxÞv 1 HðxÞυ
413
Mobile Manipulator Modeling and Control

where HðxÞ is the matrix:
HðxÞ 5
H11
H12
H21
H22


with ðρ 5 r=2aÞ:
H11 5 ρ½ða 2 yrGÞcos φ 2 ðb 1 xrGÞsin φ
H12 5 ρ½ða 1 yrGÞcos φ 1 ðb 1 xrGÞsin φ
H21 5 ρ½ða 2 yrGÞsin φ 1 ðb 1 xrGÞcos φ
H22 5 ρ½ða 1 yrGÞsin φ 2 ðb 1 xrGÞcos φ
The determinant of HðxÞ is found to be:
det HðxÞ 5 2 r2ðb 1 xrGÞ=2a
which is different than zero if xrG 6¼ 2 b, that is, if the reference point is not a point of the
WMR wheel axis. Therefore, if xrG 6¼ 2 b, the matrix HðxÞ is nonsingular (i.e., it is a decou-
pling matrix) and can be inverted.2 As a result, choosing the state-feedback control law:
υðtÞ 5 H21ðxÞ½w 2 _HðxÞv
where v 5 ½v1; v2Τ 5 ½_θ r; _θ lΤ, and wðtÞ is the new control input vector:
wðtÞ 5
w1
w2


we get the decoupled system €y1 5 w1; €y2 5 w2.
The fact that for decouplability the reference point must not lie on the WMR wheel axis
is due to the nonholonomicity property which implies that a point on the wheel axis has
instantaneously one degree of freedom, whereas all other points have instantaneously two
degrees of freedom.
On the basis of the above, it follows that the overall static feedback controller that leads
to a linear inputoutput decoupled system consists of the computed control-like part:
uðtÞ 5 Cv 1 DυðtÞ
ð10:55aÞ
and the decoupling part:
υðtÞ 5 H21ðw 2 _HvÞ
ð10:55bÞ
2 Note that the computed-torque control technique applied in Section 5.5 for the reference point C that
does not belong to the common wheel axis (i.e., b 6¼ 0) is a special case of this general inputoutput
linearizing and decoupling static feedback controller.
414
Introduction to Mobile Robot Control

with the result being €y 5 w, that is:
€y1 5 w1; €y2 5 w2
ð10:55cÞ
Now, the asymptotic tracking of the desired trajectory ydðtÞ 5 ½yidðtÞ; y2dðtÞΤ can be
achieved by a linear PD controller as usual.
(b) In this case, we have to drive the MM at the desired trajectory and at the same
time achieve the desired manipulator configuration. Therefore, we need the dynamic mod-
els of both the platform and the manipulator, which in combined reduced (unconstrained)
form are given by Eq. (10.24):
DðqÞ_v 1 Cðq;_qÞv 5 Eτ
where DðqÞ; Cðq;_qÞ are given by the combination of the respective terms of the platform
and manipulator, and:
q 5 ½qΤ
p; qΤ
mΤ; τ 5 ½τr; τl; τ1; τ2Τ
This model can be written in the state-space affine form:
_xðtÞ 5 fcðxÞ 1 gcðxÞυðtÞ
where:
x 5 ½qΤ
p; qΤ
m; vΤ; _qΤ
mΤ
fcðxÞ 5
Bv
_qm


; gcðxÞ 5
0
0
I
2
64
3
75
Here, we have four inputs, τAR4 and so we can have inputoutput decoupling of up
to four outputs. Therefore, we will select four outputs: yAR4. The manipulator end-
effector cannot track the desired trajectory alone without the help of the mobile platform,
because it may be overstretched reaching the boundary of its workspace. Therefore, while
the manipulator is moving to track as much as possible the desired trajectory, the mobile
platform should be controlled such that to bring the manipulator into the preferred
configuration.
The first two components of the output vector y are selected to be the coordinates
xE; yE of the point E in the platform coordinate frame (see Figure 10.11B), that is:
y1 5 xbG 1 l1cos θ1 1 l2 cosðθ1 1 θ2Þ
y2 5 ybG 1 l1sin θ1 1 l2 sinðθ1 1 θ2Þ
To assure that the platform controller moves the platform so as to bring always the
manipulator to the desired configuration, we choose the other two output components as:
y3 5 xb 1 ½l1 cos θ1d 1 l2 cosðθ1d 1 θ2dÞcos φ
y4 5 yb 1 ½l1 sin θ1d 1 l2 sinðθ1d 1 θ2dÞsin φ
415
Mobile Manipulator Modeling and Control

which are the coordinates of the reference point R of Figure 10.11 in the world coordinate
frame. Clearly, the desired values y3d and y4d of y3 and y4 must be set to the actual loca-
tion of the end-effector position E such that to bring R to E, that is, the manipulator con-
figuration into the preferred one.
Thus, the overall output vector y is:
y 5 hðxÞ 5
y1
y2
y3
y4
2
664
3
775 5
xE
yE
xR
xR
2
664
3
775
From this point, the controller design proceeds as before. Differentiating twice the out-
put y we get:
€y 5 _HðxÞvM 1 HðxÞυ
where vM 5 ½vΤ; _qΤ
pΤ, and:
HðxÞ 5 H1
^
H2


H1 5
ρða cos φ 2 2l sin φÞ
ρða cos φ 1 2l sin φÞ
ρða sin φ 1 2l cos φÞ
ρða sin φÞ 2 2l cos φ
0
0
0
0
2
6664
3
7775
H2 5
0
0
0
0
2l1 sin θ1 2 l2 sinðθ1 1 θ2Þ
2l2 sinðθ1 1 θ2Þ
l1 cos θ1 1 l2 cosðθ1 1 θ2Þ
l2 cosðθ1 1 θ2Þ
2
6664
3
7775
Therefore, selecting υðtÞ as:
υðtÞ 5 H21ðw 2 _HvMÞ
we get the linear decoupled system:
€y1 5 w1; €y2 5 w2; €y3 5 w3; €y4 5 w4
which can be controlled by a diagonal PD state-feedback controller as usual with given
desired natural frequency ωn and damping ratio ζ. As an exercise, the reader is advised to
solve the problem by taking into consideration the WMR maneuverability. The above con-
trollers for the problems (a) and (b) were tested by simulation in Refs. [23,24] showing
very satisfactory performance (see Section 13.9.2). It is noted that the tracking control
problem studied in Section 10.4.1 is a special case of the present problem (b) in which
the maximum (or other desired) manipulability measure condition is relaxed.
416
Introduction to Mobile Robot Control

10.5
Vision-Based Control of MMs
10.5.1
General Issues
As described in Section 9.4, the image-based visual control uses the image
Jacobian (or interaction) matrix. Actually, the inverse or generalized inverse of the
Jacobian matrix or its generalized transpose is used, in order to determine the screw
(or velocity twist) r of the end-effector which assures the achievement of a desired
task. In MMs where the platform is omnidirectional (and does not involve any non-
holonomic constraint), the method discussed in Section 9.4 is directly applicable.
However, if the MM platform is nonholonomic, special care is required for both
determining and using the corresponding image Jacobian matrix. This can be done
by combining the results of Sections 9.2, 9.4, and 10.3.
The Jacobian matrix is defined by Eq. (9.11) and relates the end-effector screw:
_r 5
v
ω


ð10:56aÞ
with the feature vector rate of change:
_f 5 ½f1; f2; . . .; fkΤ
ð10:56bÞ
that is:
_f 5 JimðrÞ_r
ð10:56cÞ
The relation of _r to _p, where p is the position vector ½x; y; zΤ rigidly attached to
the end-effector, is given by Eq. (9.4a):
_pðtÞ 5 J0ðpÞ_rðtÞ;
J0ðpÞ 5 ½I3 3 3^SðpÞ
ð10:57Þ
where SðpÞ is the skew symmetric matrix of Eq. (9.4b). The matrix J0ðpÞ is the
robot Jacobian part. The relation of _p to _f is given by Eq. (9.13a):
_f 5 Jcðxim; yim; lfÞ_p
ð10:58aÞ
where:
Jc 5
lf=z
0
2xim=z
0
lf=z
2yim=z


ð10:58bÞ
The
matrix
Jcðxim; yim; lfÞ
is
the
camera
interaction
part.
Combining
Eqs. (10.57) and (10.58a) we get the expression of JimðrÞ, namely,
JimðrÞ 5 Jcðxim; yim; lfÞJ0ðpÞ
ð10:59Þ
as given in Eq. (9.15) for the case of two image plane features xim and yim.
417
Mobile Manipulator Modeling and Control

Now, we will examine the case where the task features are the components of
the vector:
f 5 ½xim;1; yim;1; xim;2; yim;2; . . .; xim;k=2; yim;k=2ΤARk
In this case, the camera interaction part Jc, in Eq. (10.58b), is given by:
Jc 5
Jc;1
Jc;2
^
Jc;k=2
2
6664
3
7775;
Jc;i 5
lf=zi
0
2xim;i=zi
0
lf=zi
2yim;i=zi


ð10:60aÞ
for i 5 1; 2; . . .; k=2. Therefore, the overall Jacobian matrix Jim is given by:
Jim 5 JcJ0 5
Jc;1J0
Jc;2J0
^
Jc;k=2J0
2
6664
3
7775
5
lf
z1
0
2 xim;1
z1
2xim;1yim;1
lf
l2
f 1 x2
im;1
lf
2yim;1
0
lf
z1
2yim;1
z1
2l2
f 1 y2
im;1
lf
xim;1yim;1
lf
xim;1
?
?
?
?
?
?
?
?
lf
zk=2
0
2xim;k=2
zk=2
2xim;k=2yim;k=2
lf
l2
f 1 x2
im;k=2
lf
2yim;k=2
0
lf
zk=2
2yim;k=2
zk=2
2
l2
f 1 y2
im;k=2
lf
xim;k=2yim;k=2
lf
xim;k=2
2
6666666666666666664
3
7777777777777777775
ð10:60bÞ
The control of the MM needs the control of the platform and the manipulator
mounted on it. Two ways to do this are the following:
1. Control the platform and the arm separately, and then treat the existing coupling between
them.
2. Control the platform and the arm jointly with the coupling kinematics and dynamics
included in the overall model used (full-state model).
418
Introduction to Mobile Robot Control

Example 10.3
Outline a method for MM visual-based point stabilization, taking into consideration the
coupling that exists between the platform and the manipulator.
Solution
We will consider the problem of stabilizing an MM in a desired configuration using mea-
surements of the manipulator (arm) joint positions and the measures (feature measure-
ments) provided by an onboard camera mounted on the end-effector. This problem
involves the following requirements:
1. To reduce asymptotically to zero the feature errors provided by the camera
measurements.
2. To move the platform such that, during the arm control for reducing the feature errors,
to maintain the manipulator in the nonsingular configuration space.
3. To steer the platform such that to have the manipulator to the desired configuration,
while the arm is controlled to keep constant the camera measures.
These requirements can be satisfied using a hybrid control scheme that merges a feed-
back and an open-loop control strategy as follows [18].
Feedback Control
The feedback control Uf drives the camera measurement errors asymptotically to zero,
and, at the same time, maintains the manipulator far from singularities. If ef 5 f 2 fd is
the error between the actual and desired image features, then _ef 5 _f, and the manipulator
position qmðtÞ must be such that efðtÞ ! 0 as t ! N, asymptotically, that is, such that:
_efðtÞ 5 2 KfefðtÞ
ð10:61Þ
where
Kf 5 diag½kf; kf; . . .; kf;
kf . 0
is
a
matrix
gain
determining
the
rate
of
convergence.
The kinematic model of the MM from the input velocities to the features’ rate of varia-
tion _f has the form of Eq. (10.21a), that is:
_fðtÞ 5 Jim;pðf;qÞup 1 Jim;mðf;qmÞ_qmðtÞ
ð10:62Þ
where qm 5 ½qm;1; qm;2; . . .; qm;nmΤ is the vector of the manipulator joint positions,
q 5 ½qΤ
p; qΤ
mΤ
ðqp 5 ½x; y; φΤ is the platform position/orientation vector, Jim;p and Jim;m
are the platform and manipulator image Jacobian matrices, respectively, and up 5 ½v; ωΤ
is the platform control vector (linear velocity, v, and angular velocity, ω). The image
Jacobian matrices Jim;p and Jim;m consist of the robot part and camera part as shown in
Eq. (10.59). Note that if we use three landmark points the camera part is a 6 3 6 matrix,
and if these landmarks are not aligned the matrix is nonsingular (see Eqs. (10.60a),
(10.60b), and (9.15)).
To get Eq. (10.61), _qmðtÞ in Eq. (10.62) must be selected as:
_qmðtÞ 5 2 Jy
im;mðf;qmÞ½Jim;pðf;qÞup 1 Kfef
ð10:63Þ
419
Mobile Manipulator Modeling and Control

Now if the desired position of the manipulator is qd
m, we get em 5 qm 2 qd
m and
_em 5 _qm. Therefore, selecting the platform control vector up as:
upðtÞ 5 ðJy
im;mJim;pÞy½Kpem 2 Jy
im;mKfef
ð10:64Þ
we obtain:
_em 5 2 Kpem
ð10:65Þ
where the gain Kp 5 diag½kp; kp; . . .; kp; kp . 0 must be suitably selected such that to have
the desired rate of convergence to zero.
To keep the arm far from singular configurations, the above two controllers must be
suitably coordinated. To this end, Kf 5 diag½kf; kf; . . .; kf in Eq. (10.64) is modified as:
kfðqmÞ 5
σjjemjj
if
jemj , η
0
if
jemj $ η

ð10:66Þ
where η is a threshold ensuring the avoidance of singular configurations, and σðUÞ is a
suitable strictly decreasing function such as:
σðjjemjjÞ 5 σ0 e2μjjemjj2
ð10:67Þ
with proper coefficients σ0 and μ. Indeed, for manipulator configurations far from the
desired configuration (where also singular configurations may occur), we have kf ! 0 and
_ef ! 0. Therefore, the manipulator is forced to compensate only the motion of the plat-
form which is controlled to reduce jjemjj according to Eq. (10.64).
Open-Loop Control
The open-loop control U0 is to steer the platform so as to have the manipulator to the
desired configuration qd
m, while the arm is controlled to maintain constant the camera measures.
The hybrid control scheme is as follows:
UðtÞ 5
Ufðf; fd; qm; qd
m; tÞ
for
kT # t # ðk 1 1ÞT
U0ðqm; qd; tÞ
for
ðk 1 1ÞT , t # ðk 1 2ÞT

ð10:68Þ
where T is a selected time period, and k 5 0; 2; 4; . . .: Given an initial configuration
qmðt0Þ 5 qm;0, the open-loop control must assure that qmðt1Þ 5 qd
m, and qmðtÞ belongs to
the
nonsingular
configuration
space
for
all
tA½t0; t1,
where
t0 5 ðk 1 1ÞΤ
and
t1 5 ðk 1 2ÞΤ. Setting Kf 5 0 in Eq. (10.63) (i.e., no feature feedback control) we get:
_qmðtÞ 5 2 Jy
im;mðqmÞJim;pðf;qÞup
ð10:69Þ
Now, for a given upðτÞ; t0 # τ # t, we have:
qmðtÞ 5 q0 2
ðt
t0
J21
im;mðqðτÞÞJim;pðf;qÞupðτÞdτ
ð10:70Þ
and so the joint trajectory is fully determined by q0 and upðτÞ; t0 # τ # t. Therefore, as
long as the arm is controlled using Eq. (10.69), the open-loop control problem is to
420
Introduction to Mobile Robot Control

compute an open-loop platform control u0
pðqm;0; tÞ such that qmðt1Þ 5 qd
m, and qmðtÞ is a
nonsingular configuration for all t0 # t # t1. Actually, many such control sequences can be
found. For the planar case, it can be verified that the sequence:
upðtÞ 5
0; 1
T1 arctg
ypf
xpf
 !
"
#Τ
;
0 , t # T1
1
T2 2 T1
arctg
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
x2
pf 1 y2
pf
q


; 0
2
4
3
5;
T1 , t # T2
0;
1
T2T2 arcsinðξÞ
"
#Τ
;
T2 , t # T
8
>
>
>
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
>
>
>
:
ð10:71Þ
does
the
job.
Here,
½xpf; ypf; φpfΤ
is
the
platform
final
configuration,
t 5 t 2 ðk 1 1ÞT;
0 , T1 , T2 , T, and:
ξ 5
cosðφbfÞ
sinðφbfÞ


3
cos½arctgðypf=xpfÞ
sin½arctgðypf=xpfÞ


with the assumption that the platform coordinate frame coincides with the world coordi-
nate frame [18].
10.5.2
Full-State MM Visual Control
The vision-based control of mobile platforms was studied in Section 9.5, where sev-
eral representative problems (pose stabilizing control, wall following control, etc.)
were considered. Here we will consider the pose stabilizing control of MMs combin-
ing the platform pose and manipulator/camera pose control [3,14]. Actually, the
results of Section 9.5.1 concern the case of stabilizing the platform’s pose ðxQ; yQ; φÞ
and the camera’s pose ψ. The camera pose control can be regarded as the stabilizing
pose control of a one-link manipulator (using the notation θ1 5 ψ). Therefore, the
vision-based control of a general MM can be derived by direct extension of the con-
troller derived in Section 9.5.1. This controller is given by Eqs. (9.49a), (9.49b),
(9.50a), and (9.50b), and stabilizes to zero the total pose vector:
x 5 ½xQ; yQ; φ; ψΤ
on the basis of feature measurements:
f 5 ½fD; fE; fFΤ
of three nonaligned target feature points D; E; and F, provided by an onboard cam-
era, and a sensor measuring ψ (see Figure 9.4). This controller employs the image
Jacobian relation (9.25a) and (9.25b):
_f 5 Jim_x;
Jim 5 Jc
imJ0
where Jc
im and J0 are given by Eqs. (9.21) and (9.24b), respectively.
421
Mobile Manipulator Modeling and Control

Using three feature points as in Figure 9.4, the camera part Jc
im of the Jacobian
Jim has the same form as in Eq. (9.21). But the robot part J0 of the image Jacobian
is different depending on the number and type of the manipulator links.
For example, if we consider the five-link MM of Figure 10.8, the Jacobian matrix of
the overall system (including the coupling between the platform and the manipulator)
is given by Eqs. (10.30a) and (10.30b), where the wheel motor velocities _θ l and _θ r are
equivalently used as controls instead of v 5 ðr=2Þð_θ r 1 _θ lÞ and ω 5 ðr=2aÞð_θ r 2 _θ lÞ.
Now, the controller design proceeds in the usual way as described in Sections 9.3 and
9.4 for both cases of position-based and image-based control.
Example 10.4
We consider a five degrees of freedom MM consisting of a four degrees of freedom articu-
lated robotic manipulator, a one degree of freedom linear slide, a fixed camera, and a sta-
tionary spherical mirror as shown in Figure 10.12. Assuming that two fictitious landmarks
are mounted on the end-effector, the problem is to derive an appropriate image Jacobian
that can be used for 3D visual servoing of the MM.
Solution
The system has the structure of Figure 9.3B with the following coordinate frames:
G Orðxr; yr; zrÞ is the robot coordinate frame,
G Ocðxc; yc; zcÞ is the camera coordinate frame,
G Omðxm; ym; zmÞ is the mirror coordinate frame.
The mirror frame is mapped to the camera frame by the homogenous transformation:
0
0
Ac
m =
=
I3×3 d
d
1 ,
0
−d
ð10:72Þ
zr
yr
xr
z
y
x
Robot
Slide
zc
yc
xc
d
zm
xm
ym
R
Spherical
mirror
Om
Camera
Oc
Target points
1
2
Landmarks
Landmarks mirror
projections
Figure 10.12 Structure of
MM/spherical catadioptric
vision system. The optical
axis of the camera passes
through the center Om.
422
Introduction to Mobile Robot Control

where d is the distance of the mirror and camera. Similarly, the camera frame can be trans-
formed to the robot frame by Tr
c. The relationship between the landmarks Lm
r1; Lm
r2 and their
mirror projections (reflections) Lm
m1 and Lm
m2 represented in the spherical mirror frame is
found using the spherical convex mirror reflection rule as [25,26]:
Lm
mi 5 μiLri;
μi 5 R=ð2jjLm
ri jj 2 RÞ
ði 5 1; 2Þ
ð10:73Þ
where R is the spherical mirror radius. Using Eqs. (10.72) and (10.73) one finds that the
transformation from the landmarks and their mirror reflections in Omðxm; ym; zmÞ to those
in the camera frame Ocðxc; yc; zcÞ are:
Lc
ri 5 Ac
mLm
ri ;
Lc
mi 5 Ac
mLm
mi
ði 5 1; 2Þ
ð10:74Þ
from which we obtain that the landmarks’ coordinates to its mirror reflection, represented
in the camera frame, are given by (compare with the geometry of Figure 9.22):
xc
mi 5
R
μi


xc
ri;
yc
mi 5
R
μi


yc
ri;
zc
mi 5
R
μi


ðzc
ri 1 dÞ 2 d
ð10:75Þ
Differentiating Eq. (10.75), we find the transformation Τi 5 ½Tjk of the velocity screw
of the landmarks to their mirror reflections, represented in the camera frame, namely:
_xc
mi 5 Τi_xc
ri
ði 5 1; 2Þ
ð10:76Þ
where:
xc
mi 5 ½xc
mi; yc
mi; zc
miΤ;
xc
ri 5 ½xc
ri; yc
ri; zc
riΤ
T11 5 R
2
1
λi
2 x2
ri
λ3
i
0
@
1
A;
T22 5 R
2
1
λi
2 y2
ri
λ3
i
0
@
1
A;
T33 5 R
2
1
λi
2 ðzc
ri1dÞ2
λ3
i
0
@
1
A
T12 5 T21 5 2 R
2
xc
riyc
ri
λ3
i
0
@
1
A;
T13 5 T31 5 2 R
2
xc
riðzc
ri 1 dÞ
λ3
i
;
T23 5 T32 5 2 R
2
yc
riðzc
ri 1 dÞ
λ3
i
with λi 5 1=jjLm
ri jj3
ði 5 1; 2Þ.
As we know (see Figures 9.21 and 9.22) in a spherical catadioptric vision system, the
camera is a typical perspective camera which, in general, is described by:
ui
vi
1
2
4
3
5 5
λu
0
uoffset
0
λv
voffset
0
0
1
2
4
3
5λi
xi
yi
zi
2
4
3
5
ð10:77Þ
with
ui; vi
being
the
image
coordinates
in
the
2D
image
plane,
λi 5 1=zi;
λu 5 lfku;
λv 5 lfkv where lf is the camera focal length, ku and kv are scaling
factors that correspond to the effective pixels’ size in the horizontal and vertical direc-
tions, and uoffset; voffset are offset parameters that represent the principal point of the
image in the pixel frame (typically at or near the image center).
423
Mobile Manipulator Modeling and Control

Now, let ður1; vr1Þ and ður2; vr2Þ be the 2D image plane coordinates of the landmarks 1
and #2 mounted on the end-effector. Similarly, let ðum1; vm1Þ and ðum2; vm2Þ be the mirror
reflections of the landmarks on the image plane (Figure 10.13) [25]. Thus, actually we
have eight features:
f 5 ½ur1; vr1; ur2; vr2; um1; vm1; um2; vm2
By bringing these features to their desired values in the image plane where:
fd 5 ½ur1;d; vr1;d; ur2;d; vr2;d; um1;d; vm1;d; um2;d; vm2;d
(simultaneously) we can have 3D visual control of the end-effector.
Since the landmarks on the end-effector are considered as geometric points, they do not
possess any roll motion. Therefore, in the present case, the above eight features can be
reduced to five features, still allowing 3D visual servoing. These five features can be found
using any possible morphology. Referring to Figure 10.13 one can choose the following five
features: l1; l2; d1; d2; and d3 where l1 and l2 are the distances between the landmarks and
their images, d1 and d2 are the distances between the middle points of the line segments.
ðður1; vr1Þ 2 ðum1; vm1ÞÞ and ðður1;d; vr1;dÞ 2 ðum1;d; vm1;dÞÞ, and d3 is the distance between
the one-thirds of the above line segments [25,26]. From the geometry of Figure 10.13 we
find that:
l1ðum1; ur1; vm1; vr1Þ 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðum12ur1Þ2 1 ðvm12vr1Þ2
q
l2ðum2; ur2; vm2; vr2Þ 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðum22ur2Þ2 1 ðvm22vm2Þ2
q
d1ðum1; ur1; vm1; vm2Þ 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
um11ur1
2
2α1
 
!2
1
vm11vr1
2
2β1
 
!2
"
#
v
u
u
t
d2ðum2; ur2; vm2; vr2Þ 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
um21ur2
2
2α2
 
!2
1
vm21vr2
2
2β2
 
!2
v
u
u
t
d3ðum1; ur1; vm1; vm2Þ 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
um112ur1
3
2α
 
!2
1
vm112vr1
3
 
!2
2β
 
!2
v
u
u
t
Target
(ur2,d,vr2,d)
(ur2,vr2)
(ur1,d,vr1,d)
(ur1,vr1)
(um2,d,vm2d)
(um2,vm2)
(um1,vm1)
(α,β)
(α2,β2)
(α1,β1)
Landmark
l1d
l2d
l2
l1
Mirror
projection
d3
d1
d2
Figure 10.13 Landmarks, target points, and their images on the image plane.
424
Introduction to Mobile Robot Control

where ðα1; β1Þ and ðα2; β2Þ are the 2D image coordinates of the midpoint of the line seg-
ment connecting the desired positions of the landmarks and their images (reflections),
respectively, and ðα; βÞ represent the 2D image coordinates of the one-third of the line
segment that connects the desired position of landmark 1 and its reflection, projected
onto the 2D image plane. Clearly:
l1ðum1; ur1; vm1; vr1Þ 5 l1ður1; um1; vr1; vm1Þ
l2ðum2; ur2; vm2; vr2Þ 5 l2ður2; um2; vr2; vm2Þ
d1ðum1; ur1; vm1; vr1Þ 5 d1ður1; um1; vr1; vm1Þ
d2ðum2; ur2; vm2; vr2Þ 5 d2ður2; um2; vr2; vm2Þ
d3ðum1; ur1; vm1; vr1Þ 6¼ d3ður1; um1; vr1; vm1Þ
ð10:78Þ
This means that the values of the features l1; l2; d1; and d2 are invariant in terms of the
image information obtained on the real landmarks or their reflections. This is not so for
the feature d3, and so one can use d3 to distinguish the landmarks from their projections
on the image plane. Indeed it was shown in Ref. [27] that with the above 5D feature
vector:
f 5 ½l1; d1; d3; l2; d2Τ
the mirror reflection of a landmark projected onto the image plane will always remain
closer to the center of the image plane than from the landmark. Thus, a real landmark can
be distinguished from its mirror reflection without the need of any image tracking. It is
noted that special care has to be taken if a real landmark and its mirror reflection are seen
to be the same on the image plane. In this case, the singularity can be avoided by moving
the robot around the singular configuration.
We now have to study the motion of the landmarks in the 3D space. To this end, we
consider as end-effector (tip) point the midpoint between the landmarks in the robot’s
base coordinate frame. The relevant geometry is shown in Figure 10.14 [25].
In Figure 10.14, let us assume that O1ðxr
r; yr
r; zr
rÞ is the midpoint between the landmarks
ðxr
r1; yr
r1; zr
r1Þ and ðxr
r2; yr
r2; zr
r2Þ where the upper index r denotes position coordinate in the
robot coordinate frame. Here, φ is the angle between the x-axis of the robot coordinate
frame and the projection on the xy-plane of the line segment between the points
ðxr
r1; yr
r1; zr
r1Þ and ðxr
r2; yr
r2; zr
r2Þ.
The angle θ is the angle between the line segment defined by the above two points
and the xy-plane of the robot coordinate frame.
Denoting by D the distance between the landmarks in the world coordinate frame we
find that:
xr
r1 5 xr
r 2 ðD=2Þcos θ cos φ
xr
r2 5 xr
r 1 ðD=2Þcos θ cos φ
yr
r1 5 yr
r 2 ðD=2Þcos θ sin φ
yr
r2 5 yr
r 1 ðD=2Þcos θ sin φ
zr
r1 5 zr
r 2 ðD=2Þsin θ
zr
r2 5 zr
r 1 ðD=2Þsin θ
ð10:79Þ
425
Mobile Manipulator Modeling and Control

The image Jacobian matrix which relates the velocity screw:
_r 5 ½vΤ; ωΤΤ;
v 5 ½_xr; _yr; _zrΤ;
ω 5 ½ _ϕ; _θ 
ð10:80Þ
of the landmark midpoint coordinate frame (with origin at O1ðxr
r; yr
r; zr
rÞ) and the image fea-
ture rate vector:
_f 5 ½_l1; _d1; _d3; _l2; _d2
ð10:81Þ
can be found, as usual, by differentiating Eqs. (10.78) and (10.79), namely:
_f 5 Jim_r
ð10:82Þ
Now, defining the feature error eðfÞ 5 fd 2 f, the control law that assures asymptotic
convergence of eðfÞ to zero is the resolved-rate control law given by Eq. (9.34):
u 5 J21
im KeðfÞ
ðu 5 _r 5 control vectorÞ
ð10:83Þ
where K is a constant positive definite gain matrix (usually diagonal). Of course, if Jim is
not exactly known we can use in Eq. (10.83) an estimate ^Jim which can be found as
explained in Section 9.4.3 using the estimates of the camera-intrinsic and -extrinsic para-
meters. Here, the intrinsic parameters include the two scaling factors λu; λv and the two
offset terms uoffset and voffset. The extrinsic parameter is the distance d of the camera frame
from the mirror’s center. Therefore, the vector ξ that has to be estimated is:
ξ 5 ½uoffset; voffset; λu; λv; dΤ
while the measurement vector z is:
z 5 ½um1; vm1; ur1; vr1; um2; vm2; ur2; vr2Τ
z
x
y
Landmark #1
Landmark #2 
Mid-point frame
(xr2,yr2,zr2)
O1 (xr, yr, zr)
r
r
r
r
(xr1,yr1,zr1)
r
r
r
r
r
Robot frame
rxr
ryr
rzr
O
Figure 10.14 Geometry for defining the
positions of the two landmarks in 3D space.
426
Introduction to Mobile Robot Control

References
[1] Padois V, Fourquet JY, Chiron P. Kinematic and dynamic model-based control of
wheeled mobile manipulators: a unified framework for reactive approaches. Robotica
2007;25(2):15773.
[2] Seelinger M, Yoder JD, Baumgartner ET, Skaar BR. High-precision visual control of
mobile manipulators. IEEE Trans Robot Autom 2002;18(6):95765.
[3] Tzafestas CS, Tzafestas SG. Full-state modeling, motion planning and control of
mobile manipulators. Stud Inform Control 2001;10(2):10927.
[4] Kumara P, Abeygunawardhana W, Murakami T. Control of two-wheel mobile manipu-
lator on a rough terrain using reaction torque observer feedback. J Autom Mobile
Robot Intell Syst 2010;4(1):5667.
[5] Chung JH, Velinsky SA. Robust interaction control of a mobile manipulator: dynamic
model based coordination. J Intell Robot Syst 1999;26(1):4763.
[6] Li Z, Chen W, Liu H. Robust control of wheeled mobile manipulators using hybrid
joints. Int J Adv Rob Syst 2008;5(1):8390.
[7] Yamamoto Y, Yun X. Effect of the dynamic interaction on coordinated control of
mobile manipulators. IEEE Trans Rob Autom 1996;12(5):81624.
[8] Tzafestas SG, Melfi A, Krikochoritis T. Omnidirectional mobile manipulator modeling
and control: analysis and simulation. Syst Anal Model Control 2001;40:32964.
[9] Mazur A, Szakiel D. On path following control of non-holonomic mobile manipulators.
Int J Appl Math Comput Sci 2009;19(4):56174.
[10] Meghadari A, Durali M, Naderi D. Investigating dynamic interaction between one d.o.
f.
manipulator
and
vehicle
of
a
mobile
manipulator.
J
Intell
Robot
Syst
2000;28:27790.
[11] Watanabe K, Sato K, Izumi K, Kunitake Y. Analysis and control for an omnidirec-
tional mobile manipulator. J Intell Robot Syst 2000;27(12):320.
[12] De Luca A, Oriolo G, Giordano PR. Image-based visual servoing schemes for nonholo-
nomic mobile manipulators. Robotica 2007;25:13145.
[13] Burshka D, Hager G. Vision-based control of mobile robots. In: Proceedings of 2001
IEEE international conference robotics and automation. Seoul, Korea; May 2126,
2001.
[14] Tsakiris D, Samson C, Rives P. Vision-based time-varying stabilization of a mobile
manipulator. In: Proceedings of fourth international conference on control, automation,
robotics and vision (ICARCV’96). Westin Stanford, Singapore; December 36, 1996.
p. 15.
[15] Yu Q, Chen Ming I. A general approach to the dynamics of nonholonomic mobile
manipulator systems. Trans ASME 2002;124:51221.
[16] DeLuca A, Oriolo G, Giordano PR. Kinematic control of nonholonomic mobile manip-
ulators in the presence of steering wheels. In: Proceedings of 2010 IEEE international
conference on robotics and automation. Anchorage, AK; May 38, 2010. p. 179298.
[17] Phuoc LM, Martinet P, Kim H, Lee S. Motion planning for nonholonomic mobile
manipulator based visual servo under large platform movement errors at low velocity.
In: Proceedings of 17th IFAC world congress. Seoul, Korea; July 611, 2008.
p. 431217.
[18] Gilioli M, Melchiori C. Coordinated mobile manipulator point-stabilization using
visual-servoing techniques. In: Proceedings of IEEE/RSJ international conference on
intelligent robots and systems. Lausanne, CH; 2002. p. 30510.
427
Mobile Manipulator Modeling and Control

[19] Ma Y, Kosecha J, Sastry S. Vision guided navigation for nonholonomic mobile robot.
IEEE Trans Robot Autom 1999;15(3):52136.
[20] Zhang Y. Visual servoing of a 5-DOF mobile manipulator using an omnidirectional
vision system. Technical Report RML-3-2, University of Regina, 2006.
[21] Bayle B, Fourquet JY, Renaud M. Manipulability analysis for mobile manipulators. In:
Proceedings of 2001 IEEE international conference on robotics and automation. Seoul,
Korea; May 2126, 2001. p. 125156.
[22] Papadopoulos E, Poulakakis J. Trajectory planning and control for mobile manipulator
systems. In: Proceedings of eighth IEEE Mediterranean conference on control and
automation. Patras, Greece; July 1719, 2000.
[23] Yamamoto Y, Yun X. Coordinating locomotion and manipulation of a mobile manipu-
lator. In: Proceedings of 31st IEEE conference on decision and control. Tucson, AZ;
December, 1992. p. 264348.
[24] Yamamoto Y, Yun X. Modeling and compensation of the dynamic interaction of a
mobile manipulator. In: Proceedings of IEEE conference on robotics and automation.
San Diego, CA; May 813, 1994. p. 218792.
[25] Zhang Y, Mehrandezh M. Visual servoing of a 5-DOF mobile manipulator using a pan-
oramic vision system. In: Proceedings of 2007 Canadian conference on electrical and
computer engineering. Vancouver, BC, Canada; April 2226, 2007. p. 45356.
[26] Zhang Y, Mehrandezh M. Visual servoing of a 5-DOF mobile manipulator using a
catadioptric vision system. In: Proceedings of SPIE, the international society for optical
engineering; 2007. p. 617.
[27] Zhang Y. Visual servoing of a 5-DOF mobile manipulator using panoramic vision sys-
tem [M.Sc. thesis]. Regina, Canada, Faculty of Engineering, University of Regina,
2007.
428
Introduction to Mobile Robot Control

11 Mobile Robot Path, Motion, and
Task Planning
11.1
Introduction
Robot planning is concerned with the general problem of figuring out how to
move to get from one place to another and how to perform a desired task. As a
whole, it is a wide research field in itself. Actually, the term planning means
different things to different scientific communities. The three categories of plan-
ning in robotics are [132]:
1. Path planning
2. Motion planning
3. Task planning
Planning represents a class of problem solving which is an interdisciplinary area
of system theory and artificial intelligence (AI) [12,19]. A general problem solver
is basically a search program in which the problem to be solved is defined in terms
of a given initial state and a desired goal state. This program guides the search by
evaluating the current state and operator (rule) ordering, that is, by applying only
the operators that promise the best movement in the search space. The principal
problem-solving method is the means-ends analysis, which consists in repeated
reduction of the difference between the goal state and the current state. This needs
a special feedback control strategy. An example of the use of means-ends analysis
in AI problem solving is the well-known Towers of Hanoi puzzle problem. The
general problem-solving methodology has limited value for specific problems that
can be solved only by skilled experts such as fault diagnosis, decision support, and
robot planning.
The objectives of this chapter are as follows:
G
To present the general conceptual definition of robot path planning, motion planning, and
task planning
G
To investigate the path planning problem of mobile robots, including the basic operations
and classification of methods
G
To study in some detail the model-based mobile robot path planning, including configura-
tion space and road map planning methods
G
To discuss mobile robot motion planning presenting the vector fields method and the ana-
lytical parameterized method
Introduction to Mobile Robot Control. DOI: http://dx.doi.org/10.1016/B978-0-12-417049-0.00011-0
© 2014 Elsevier Inc. All rights reserved.

G
To show how global and local path planning can be integrated such that to achieve the
desired features of path smoothness and trapping avoidance
G
To outline the basic issues of fixed and mobile robot task planning, including plan repre-
sentation and generation, and the three phases of task planning, namely, world modeling,
task specification, and robot program synthesis.
11.2
General Concepts
Path planning is a general capability embedded in all kinds of robots (robotic
manipulators, mobile robots/manipulators, humanoid robots, etc.). In a broad sense,
robot path planning is concerned with the determination of how a robot will move
and maneuver in a workspace or environment in order to achieve its goals. The
path planning problem involves computing a collision-free path between a start
position and a goal position. Very often, besides the obstacle avoidance, the robot
must also satisfy some other requirements or optimize certain performance criteria.
Path planning is distinguished according to the knowledge available about the
environment (i.e., fully known/structured environment, partially known environ-
ment, and fully unknown/unstructured environment). In most practical cases, the
environment is only partially known, where the robot, prior to path planning and
navigation, has already knowledge of some areas within the workspace (i.e., areas
likely to pose local minima problems). The nature of an obstacle is described via
its configuration which may be convex shaped or concave shaped or both.
The status of an obstacle may be static (when its position and orientation relative
to a known fixed coordinate frame is invariant in time), or dynamic (when either
its position), or orientation or both change relative to the fixed coordinate frame
change.
Path planning may be either local or global. Local path planning is performed
while the robot is moving, taking data from local sensors. In this case, the robot
has the ability to generate a new path in response to the changes of the environ-
ment. Global path planning can be performed only if the environment (obstacles,
etc.) is static and perfectly known to the robot. In this case, the path planning
algorithm produces a complete path from the start point to the goal point before the
robot starts its motion.
Motion planning is the process of selecting a motion and the corresponding
inputs such that to assure that all constraints (obstacle avoidance, risk avoidance,
etc.) are satisfied. Motion planning can be considered as a set of computations
which provide subgoals or set points for the control of the robot. These computa-
tions and the resulting plans are based on a suitable model of the robot and the
environment in which it is moved. The process by which the robot executes
(follows) the planned motion is the control process studied in Chapters 510.
We recall that the motion of a robot can be described in three different spaces:
1. Task or Cartesian space
2. Joints’ (motors’) space
3. Actuators’ space
430
Introduction to Mobile Robot Control

The relation of these spaces is pictorially shown in Figure 11.1.
The description of a motion in the task space, that is, the specification of a refer-
ence point (e.g., the tip of the end effector) is typically made in the Cartesian world
coordinate frame.
The specification of the end effector or mobile robot position is not always suffi-
cient to determine the positions of all the links. For this reason, we use the joints’
space which is the Cartesian product of the allowable ranges of all the degrees of
freedom (usually a subset of n). Finally, for each robot motion that is compatible
with the kinematic and dynamic constraints, there must exist at least one set of
forces/torques that produce that motion. The forces/torques of the actuators (motors)
that generate all the allowable motions define the motors’ space. A basic issue in
robot motion planning is the presence of obstacles and the resulting requirement to
find an obstacle-free path. This is the path planning or path finding problem. The
need for motion planning comes from the fact that there is a very large number (the-
oretically, an infinite number) of motions via which the robot can go to a goal pose
and execute a desired task. Also, for a given motion, there may be more than one
inputs (forces/torques) of the motors that produce the desired motion.
Task planning involves three phases [8]:
1. World modeling
2. Task specification
3. Robot program synthesis
World modeling: A world model must involve a geometric description of robots
and objects in the environment (which is usually embodied in a CAD system), a
physical description of objects (e.g., mass and inertia of parts), a kinematic descrip-
tion of robot body and linkages, and a description of robot features (e.g., joint
limits, maximum possible or allowed acceleration, and sensor features).
Task specification: The task planner usually receives the tasks as a sequence of
models of the world state at several steps of the task’s execution. Actually, a task
specification is a model of the world accompanied by a sequence of changes in the
positions of the model components.
Robot program synthesis: This is the most important phase for the task planner
to be successful. The program synthesized must include grasp commands, motion
properties, sensor commands, and error tests. This implies that the program must
be in the robot-level programming language of the robot.
Joints’ space
Motors’
space
Task space
Inverse
kinematics
Direct
kinematics
Dynamics 
Inverse 
dynamics
Figure 11.1 Robot motion spaces and their relationships.
431
Mobile Robot Path, Motion, and Task Planning

11.3
Path Planning of Mobile Robots
11.3.1
Basic Operations of Robot Navigation
Path planning of a mobile robot is one of the basic operations needed to implement
the navigation of the robot. These operations are as follows:
G
Self-localization
G
Path planning
G
Map building and map interpretation
Robot localization provides the answer to the question “where am I?” The
path planning operation provides the answer to the question “how should I get to
where I’ am going?” Finally, the map building/interpretation operation provides
the geometric representation of the robot’s environment in notations suitable for
describing locations in the robot’s reference frame. Vision-based navigation
employs optical sensors including laser-based range finders and CCD cameras by
which the visual features needed for the localization in the robot’s environment
are extracted.
Actually, to date, there is no generic method for mobile robot positioning (local-
ization). The specific techniques that exist are divided into two categories:
1. Relative localization methods
2. Absolute localization methods
Because
no
single,
globally
good
localization
method
is
available,
designers of autonomous guided vehicles (AGVs) and autonomous mobile
robots (AMRs) usually employ some combination of methods, one from each
category.
Relative localization is performed by odometry or inertial navigation. The first
uses encoders to measure wheel rotation and/or steering angle. Inertial navigation
employs gyroscopes (or accelerometers in some cases) to measure the rate of rota-
tion and the angular acceleration.
Absolute localization uses the following:
G
Active beacons, where the absolute position of the mobile robot is computed by
measuring the direction of incidence of three or more transmitted beacons. The trans-
mitters use light or radio frequencies and are placed at known positions in the
environment.
G
Recognition of artificial landmarks, which are placed at known locations in the environ-
ment and are designed so as to provide maximal detectability even under bad environ-
mental conditions.
G
Recognition of natural landmarks, that is, distinctive features of the environment, which
must be known in advance. This method has lower reliability than the artificial landmarks
method.
G
Model matching, that is, comparison of the information received from on-board sensors
and a map of the environment. The absolute location of the robot can be estimated if the
sensor-based features match the world model map. The robot navigation maps are
432
Introduction to Mobile Robot Control

distinguished in geometric maps and topological maps. The first category represents the
world in a global coordinate frame, whereas the second category represents the world as
a network of arcs and nodes.
11.3.2
Classification of Path Planning Methods
Path planning is a robotics field on its own. Its solution gives a feasible collision-
free path for going from one place to another. Humans do path planning without
thinking how it is done. If there is an obstacle ahead that has not been there before,
humans just pass it. Very often, the human needs to change his/her pose in order to
go through a narrow passage. This is a simple type of the so-called piano-mover’s
problem. If the obstacle blocks the way completely, humans just use another way.
To perform all the above operations, a robot must be equipped with suitable high-
level intelligence capabilities.
A very broad classification of free (obstacle-avoiding) path planning involves
three categories, which include six distinct strategies. These are the following:
1. Reactive control (“Wander” routine, circumnavigation, potential fields, motor schemas)
2. Representational world modeling (certainty grids)
3. Combinations of both (vector field histogram)
In many cases, the above techniques do not assure that a path is found that
passed obstacles although it exits, and so they need a higher level algorithm to
assure that the mobile robot does not end up in the same position over and over
again. In practice, it may be sufficient that the robot detects that it is “stuck”
despite the fact that a feasible path way exists, and calls for help. In indoor applica-
tions, a maneuver for avoiding an obstacle is a good action. Outdoor situations are
more complex, and more advanced perception techniques are needed (e.g., for dis-
tinguishing a small tree from an iron pole).
A research topic receiving much attention over the years is the piano-mover’s
problem, which is well known to most people that tried a couch or big
table through a narrow door. The object has to be tilted and moved around through
the narrow door. One of the first research works on this problem is described in
Latombe [1].
On the basis of the way the information about the robot’s environment is
obtained, most of the path planning methods can be classified into two categories:
1. Model-based approach
2. Model-free approach
In the first category, all the information about the robot’s workspace are
prelearned, and the user specifies the geometric models of objects and a description
of them in terms of these models. In the model-free approach, some of the informa-
tion about the robot’s environment is obtained via sensors (e.g., vision, range, touch
sensors). The user has to specify all the robotic motions needed to accomplish a task.
433
Mobile Robot Path, Motion, and Task Planning

11.4
Model-Based Robot Path Planning
The obstacles that may exist in a robotic work environment are distinguished into
static obstacles and moving obstacles. Therefore, two types of path finding pro-
blems have to be solved, namely:
G
Path planning among stationary obstacles
G
Path planning among moving obstacles
The path planning methodology for stationary obstacles is based on the configu-
ration space concept, and is implemented by the so-called road map planning
methods discussed below.
The path planning problem for the case of moving obstacles is decomposed into
two subproblems:
1. Plan a path to avoid collision with static obstacles.
2. Plan the velocity along the path to avoid collision with moving obstacles.
This combination constitutes the robot motion planning [25].
11.4.1
Configuration Space
Configuration space is a representation in which path planning for both manipula-
tor robots and (most of) mobile robots is performed. For example, if the mobile
system is a free-flying rigid body (i.e., a body that can move freely in space in any
direction without any kinematics constraint), then six configuration parameters are
required to determine fully its position, namely, x; y; z and three directional (Euler)
angles. Path planning finds a path in this six-dimensional space. But, actually, a
robot is not a free-flying robot. Its possible motion depends on its kinematic struc-
ture, for example, a unicycle-like robot has three configuration parameters: x; y,
and φ. As we have seen, very often, these parameters are not independent, for
example, the robot may or may not be able to turn on the spot (change φ while
keeping x and y fixed), or be able to move sideway. A robotic arm which has n
rotational joints needs n configuration parameters to specify its configuration in
space, in addition to constraints such as the minimum or maximum values of each
angular join. For example, a typical car-like robotic manipulator has 10 configura-
tion parameters (4 for the mobile platform with the trailer, and 6 for the arm),
whereas a certain humanoid robot such ASIMO or HRP may have 52 configuration
parameters (2 for the head, 7 for each arm, 6 for each leg, and 12 for each hand
that involves 4 fingers with 3 articulations each).
Now, given a robot with n configuration parameters moving in a certain envi-
ronment, we define the following:
G
The configuration q of the robot, that is, an n-tuple of real numbers that specifies the n
parameters needed to determine the position of the robot in physical space.
G
The configuration space CS of the robot, that is, the set of values that its configuration q
may take.
434
Introduction to Mobile Robot Control

G
The free configuration space CSfree, that is, the subset of CS of configurations that are not
in collision with the obstacles existing in the robot’s environment.
It is noted that the degrees of freedom of a mobile robot are its control variables
(a robot arm or a humanoid robot has as many degrees of freedom as configuration
parameters, but a differential drive robot has three configuration parameters and
only two degrees of freedom).
From the above definitions, it follows that path planning is the problem of finding
a path in the free configuration space CSfree, between an initial and a final configura-
tion. Thus, if CSfree could be determined explicitly, then path planning is reduced to
a search for a path in this n-dimensional continuous space. Actually, the explicit defi-
nition of CSfree is a computationally difficult problem (its computational complexity
increases exponentially with the dimension of CS), but there are available efficient
probabilistic methods that solve this path planning problem in reasonable time [11].
As we have seen before, these techniques must involve two operations:
1. Collision checking (i.e., check whether a configuration q or a path between two config-
urations lies entirely in CSfree)
2. Kinematic steering (i.e., find a path between two configurations q0 and qf in CS that
satisfies the kinematic constraints, without taking into account obstacles)
Example 11.1 Two-link planar manipulator
Consider the two-link planar manipulator of Figure 11.2A which has two configuration
parameters θ1 and θ2 (i.e., its CS is two-dimensional).
An obstacle in an n-dimensional configuration space CS is represented by a slice projec-
tion which is defined by a range of values for one of the defining parameters of CS and an
(n 2 1)-dimensional volume. The approximation of the full obstacle is built as the union
of a number of (n 2 1)-dimensional slice projections, each for a different range of values
of the same joint parameter [8]. In the present case (Figure 11.2A), the obstacles are
approximated by a set of θ2 ranges (which are shown by dark lines) for a set of values of θ1.
A sample path of a two-link robot in another environment with obstacles is shown in
Figure 11.2B.
11.4.2
Road Map Path Planning Methods
The robot navigation maps which are used to represent the environment can be a
continuous geometric description or a decomposition-based geometric map or a
topological map. These maps must be converted to discrete maps appropriate for
the path algorithm under implementation. This conversion (or decomposition) can
be done by four general methodologies, namely [1,21]:
1. Road maps
2. Cell decomposition
3. Potential fields
4. Vector field histograms
In the following sections, a short description of these methodologies is given.
435
Mobile Robot Path, Motion, and Task Planning

11.4.2.1
Road Maps
The basic idea is to capture the connectivity of CSfree with a road map (graph or
network) of one-dimensional curves. After its construction, a road map is used as a
network of path (road) segment for the planning of the robot motion. The main
goal here is to construct a set of roads that as a whole enable the robot to reach any
position in its CSfree. This is actually a hard problem.
The path planning problem is stated as follows:
Given input configurations qstart and qgoal and the set B of obstacles.
Find a path in CSfree connecting qstart and qgoal.
The basic steps of the path planning algorithm are as follows:
Step 1: Build a road map in CSfree (the road map nodes are free or semifree configura-
tions; two nodes are connected by an edge if the robot can easily move between them).
(A)
(B)
(2)
360°
Θ1
Θ2
Θ2
Θ1
360°
360°
180°
0
0
180°
360°
1
2
(1)
Figure 11.2 Configuration space of a two-link planar robot. (A) The manipulator with the
obstacles, and the CS with obstacles approximated by a set of one-dimensional slice
projections (shown in dark). (B) Another two-link manipulator in an environment with
obstacles, and a possible path in CSfree.
Source: http://robotics.stanford.edu/Blatombe/cs326/2009/class3/class3.htm; http://www.cs.
cmu.edu/Bmotionplanning/lecture/Chap3-Config-Space_howie.pdf.
436
Introduction to Mobile Robot Control

Step 2: Connect qstart and qgoal to road map nodes υstart and υgoal.
Step 3: Find a path in the road map between υstart and υgoal, which gives directly a path
in CSfree.
A road map example is shown in Figure 11.3.
The methods for building road maps are distinguished into:
G
Traditional deterministic methods (they are suitable only for low-dimensional CSs, they
build CSfree, and they are complete).
G
Modern probabilistic methods (they do not build CSfree, they apply to both low- and high-
dimensional CSs, but they are not complete).
Visibility graph of CS: A visibility graph for a polygonal CS is an undirected
graph G where the nodes in G correspond to vertices of the polygonal obstacles,
under the condition that the nodes can be connected by a straight line that lies fully
in CSfree or by the edges of the obstacles (i.e., under the condition that the nodes
can see each other including the initial and goal positions as vertices as well). An
example of visibility graph is shown in Figure 11.4.
Mobile path planning based on visibility graphs is popular because of its sim-
plicity. There are efficient algorithms with complexity Oðn2Þ, where n is the
number of vertices of the objects or OðE 1 n log nÞ, where E is the number of edges
in G [17,22]. Visibility graphs are really suitable only for two-dimensional CS. It is
noted that they are also methods for constructing “reduced” visibility graphs where
not all edges are needed. A visibility graph and a reduced visibility graph (corre-
sponding to it) are shown in Figure 11.5.
O1
O2
O3
Start
Goal
Road map
Figure 11.3 A road map example in an
environment with obstacles O1; O2; O3.
O1
O2
O3
Start
Goal
Road map Figure 11.4 A visibility graph example.
437
Mobile Robot Path, Motion, and Task Planning

Voronoi diagram: A Voronoi diagram, after the name of the German mathe-
matician who coined it in 1908, ensures a maximum distance between the robot
and the obstacles in the map. The construction steps of a Voronoi diagram are
as follows:
Step 1: For each point in the free space CSfree, compute its distance to the nearest
obstacle.
Step 2: Plot that distance as a vertical height. The height increases as the point is moving
away from the obstacle.
Step 3: At equidistant points from two or more obstacles, this distance plot has sharp
ridges.
Step 4: Construct the Voronoi diagram by the union of the edges formed by these sharp
ridges.
Mathematically, the Voronoi diagram V is a polygonal region defined as:
V 5 fqACSfree:jnearðqÞj . 1; CS 5 R2g
where:
nearðqÞ 5 fpAδ:jjq 2 pjj 5 clearanceðqÞg
clearanceðqÞ 5 minfjjq 2 pjj:pAδg for qACSfreeg
δ 5 @ðCSfreeÞ, the boundary of CSfree
We see that, actually, nearðqÞ is the set of boundary points of CSfree that mini-
mize the distance to q. The Voronoi diagram V consists of all points in CSfree with
at least two nearest neighbors in the CSfree boundary δ. The Voronoi diagram is a
finite collection of straight line segments and parabolic segments (called arcs),
where:
G
Straight arcs are defined by two vertices or two edges of the set of obstacles (i.e., the set
of points equally close to two points or two line segments is a line).
G
Parabolic arcs are defined by one vertex and one edge of the obstacle set (i.e., the set of
points equally close to a point (focus) and a line (directrics) is a parabolic segment).
O1
O2
O3
Start
Goal
O1
O2
O3
Start
Goal
(a)
(b)
Figure 11.5 (A) A visibility graph. (B) An associated reduced visibility graph.
438
Introduction to Mobile Robot Control

A construction procedure of V based on the above definition involves the fol-
lowing basic steps:
Step 1: Compute all arcs (for all vertex-vertex, edge-edge, vertex-edge pairs).
Step 2: Compute all intersection points (dividing arcs into segments).
Step 3: Keep the segments which are closest only to the vertices/edges that defined them.
A simple Voronoi diagram example for a terrain with two parallel walls and a
triangular object between them is shown in Figure 11.6A. The Voronoi diagram of
an office floor that has the map of Figure 11.6B is shown in Figure 11.6C, and a
safe path from S to G found by the fast marching path planning method is depicted
in Figure 11.6D [3336].
11.4.2.2
Cell Decomposition
Cell decomposition is concerned with the discrimination between cells (i.e., con-
nected geometric areas) in CS that are free (i.e., belong to CSfree) or are occupied
by objects. The cells have a predefined resolution. After the step of determining the
cells, the path planning procedure proceeds as follows:
G
Determine the open cells that are adjacent and construct a connectivity graph G.
G
Determine the cells that contain the starting and goal configurations and search for a path
in G that joins the start and goal cell.
(D)
S
G
(C)
S
G
O1
O2
O3
(B)
F
Parabola
(A)
Figure 11.6 (A) An example of Voronoi diagram. (B) Environment (map) of an office floor.
(C) Voronoi diagram of the floor. (D) A safe path found by the fast marching method.
Source: Reprinted from Ref. [33] with the courtesy of S. Garrido and permission from CSC Press.
439
Mobile Robot Path, Motion, and Task Planning

G
In the sequence of cells that joins the starting and goal cell, find a path within each cell
(e.g., moving via the midpoints of the cell boundaries or moving along the walls).
Cell decomposition is distinguished into:
G
Exact cell decomposition
G
Approximate cell decomposition
In the first, the boundaries are placed as a function of the environment’s struc-
ture, and so the decomposition is lossless. In approximate cell decomposition, we
obtain some approximation of the actual map. An example of exact cell decomposi-
tion is shown in Figure 11.7.
In Figure 11.7A, the boundaries of the cells are found by taking into account the
geometric criticality. The path planning is complete because each cell is either
completely free or completely occupied. It is clear that the robot can move from
each free cell to adjacent free cells.
In approximate cell decomposition, the resulting cells may be free, completely
occupied or mixed, or have reached an arbitrary resolution threshold. One way to
apply approximate decomposition is to use the occupancy grid method. A possible
occupancy grid can be obtained by assigning to each of the cells a value that relates
to the probability of this cell’s occupation. The decomposition threshold in this
case defines the minimum required probability for a cell in the occupancy grid to
be deemed occupied. Actually, the idea of “approximate” is to fuse neighboring
1
2
3
4
5
6
7
8
9
10
11 12
13
14
15
16
17
18
19
Start
Goal
(A)
1
2
3
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
Start
Goal
(B)
4
Figure 11.7 (A) Cell decomposition. (B) Network representation and a possible path from
the start node to the goal node.
440
Introduction to Mobile Robot Control

free cells into larger cells to allow a fast path determination. In two-dimensional
workspaces, the approximate cell decomposition operation consists in recursively
subdividing each cell which contains a mix of free and obstructed space into cells.
Because of this property, this method is known as quadtree method. The recursion
is ended when each cell is found to contain entirely free or obstructed space, or
when the maximum desired resolution is reached. The height of the decomposition
is the maximum allowable level of recursion, and specifies the resolution of the
decomposition.
Figure 11.8 shows an application example of the approximate cell decomposi-
tion method. The workspace contains three obstacles and the robot has to move
from S to G.
The result of approximate decomposition is a drastic reduction of the number of
cells to be considered. In an example, a high resolution map that contains 250,000
cells, with a crude decomposition of height 4 was reduced to just 109 cells [37]. A
problem that has to be faced here is to determine cell adjacency (i.e., to find which
cells share a common border or edge with another one). Actually, many techniques
are available to solve the adjacency problem.
One of them is to use tesseral (or quad tesseral) addressing which has the abil-
ity to map every part of a 2D (or nD) spatial domain into 1D sequence, and when
stored with attributes in a database, each address can perform as a single key to
data [37,38]. To generate tesseral addresses, the positive quadrant of 2D-Cartesian
space is quartered to give parent tiles with labeling as shown in Figure 11.9A. This
process can be continued with new tesseral addresses generated by always append-
ing to the right of the parent addresses, until a desired depth is reached.
Another solution to global path planning, via decomposition, is to use local
node refinement, path nodes refinement, and curve parametric interpolation [39].
A quad tesseral address can be stored in a quadtree structure. For example, the
address of Figure 11.9A (right) can be stored by the quadtree structure of
(A)
(B)
Goal
Start
Start
Goal
Figure 11.8 (A) A simple three-obstacle path planning problem. (B) An obstacle-free path
based on approximate cell decomposition.
Source: http://www-cs-faculty.stanford.edu/Beroberts/courses/soco/1998-99/robotics/
basicmotion.html.
441
Mobile Robot Path, Motion, and Task Planning

Figure 11.9B, which if traversed from left to right produces a linearization. This
important property of tesseral addressing facilitates the storage, comparison, and
translation of groups of addresses.
In global path planning, the environment is assumed to be a priori known, in
which case, a distance optimal path from the robot’s current position to the goal
can be found. This path consists of a sequence of waypoints whose proximity to
one another is specified by the decomposition resolution and the configuration of
the obstacles in the environment. In a changing environment, one must combine
the above global path planning technique with a waypoint driven local path plan-
ning method. A distance-optimal global path planning method which uses tesseral
addressing is presented in Ref. [37]. In this method, use is made of the connectivity
graph to produce nodes that represent physical locations in the environment and
arcs that represent the ability to avoid the obstacles during the motion along the
path. The distance-optimal path is found by minimizing the following heuristic cost
function using a suitable graph search method such as the A algorithm [9,11,19]:
L0ðNÞ 5 LsðNÞ 1 LgðNÞ
LsðNÞ 5
X
N
i5s11
ai
LgðNÞ 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðxg2xnÞ2 1 ðyg2ynÞ2
q
where ai is the length of the arc ðni21; niÞ between two adjacent nodes i 2 1 and i in
a sequence ðns; ns11; . . .; nN21; nNÞ of nodes that connect the node s to node N, and
LgðNÞ is the distance from node N to the goal node G. Clearly, LgðNÞ is an underes-
timate of the optimal cost from N to G.
00
03
010
011 012 013
020
021 022 023
(B)
(A)
0
00
01
02
03
00
010
03
011
012 013
020 021
022 023
Depth 0
Depth 1
Depth 2
Figure 11.9 (A) 2D tesseral
decomposition (addressing).
(B) Quadtree structure of the
above depth-2 address.
442
Introduction to Mobile Robot Control

To carry out the minimization, the so-called tesseral1 arithmetic is used. For
example, in this arithmetic, addition or subtraction is tile translation to the right
(addition of tesseral 1, binary 01) or to the left (subtraction of tesseral 1).
Correspondingly, translation down and up is equivalent to the addition or subtrac-
tion of tesseral 2 (binary 10), respectively. It is remarked that actually the resulting
optimal path depends on the connectivity graph employed between the initial
robot’s position and the goal, that is, it is not globally optimal. The computation
time of the tesseral arithmetic algorithms is of order Oð4nÞ. A critical review of
spatial reasoning using quad tesseral representation is provided in Ref. [38]. This
representation allows to carry out spatial reasoning (i.e., manipulation of 2D and
3D objects) as if in only one dimension.
11.4.2.3
Potential Fields
The potential field method for path planning is very attractive because of its sim-
plicity and elegance. The idea of potential field is borrowed from nature, for exam-
ple, a charge particle navigating a magnetic field, or a small ball rolling on a hill.
We know that depending on the strength of the field, or the slope of the hill, the
particle, or the ball, will go to the source of the field (the magnet), or the valley of
this example. In robotics, we can emulate the same phenomenon by developing an
artificial potential field that will attract the robot to the goal. In conventional path
planning, we calculate the relative position of the robot to the goal, and then apply
the necessary forces that will drive the robot to the goal.
If the robot’s environment is free of obstacles, we simply create an attractive
field that goes to the goal. The potential field is defined over the entire free space,
and at each time increment, we compute the potential field at the robot position,
and then calculate the induced force by this field. The result is that the robot will
move according to this force. If the robot’s space has an obstacle, we make it gen-
erate a repulsive field in the surrounding space, which when the robot approaches
the obstacle, pushes the robot away from it. If we want to make the robot going to
the goal and avoiding the obstacle, then we superimpose an attractive field around
the goal and a repulsive field around each obstacle in the robot’s space. The above
concepts are illustrated in Figure 11.10AD, where the path of the robot from a
certain starting point to the goal is also shown (Figure 11.10E).
In the simplest situation, a mobile robot can be considered to be a point robot,
in which case the robot’s orientation φ is not included in the calculations, the
potential field is two-dimensional, and the robot’s position is simply q 5 ½x; yT.
In general, a mobile robot has q 5 ½x; y; φT, and a robotic manipulator has
q 5 ½q1; q2; . . .; qnT. Mathematically, the potential field method develops as follows
[1,5] (see also [24,25] for more formulations). Define:
G
UðqÞ the total potential field, which is equal to the sum of the attractive potential field
UattðqÞ and the repulsive potential field UrepðqÞ.
1 The term tesseral comes from the Greek word τεσσερα (tessera5four).
443
Mobile Robot Path, Motion, and Task Planning

G
FattðqÞ the attractive force, FrepðqÞ the repulsive force, and FðqÞ the total force applied to
the robot.
Then, we have:
UðqÞ 5 UattðqÞ 1 UrepðqÞ
FðqÞ 5 FattðqÞ 1 FrepðqÞ
(B)
(C)
(D)
(E)
Robot
Obstacle
Obstacle repulsion
(A)
Direction 
to go
Goal attraction
Figure 11.10 Illustration of the potential field path planning method. (A) Goal attraction,
obstacle repulsion, and direction to go. (B) Attractive field to the goal. (C) Repulsive field
around an obstacle. (D) Combination of the above two fields. (E) A possible robot path.
Source: http://www.cs.mcgill.ca/Bhsafad/robotics/index.html.
444
Introduction to Mobile Robot Control

where:
FðqÞ 5 2 rUðqÞ; FattðqÞ 5 2 rUattðqÞ; FrepðqÞ 5 2 rUrepðqÞ
with r½ 5 ½@=@q1; @=@q2; . . .; @=@qnT being the gradient operator.
Now, suppose that UattðqÞ is quadratic:
UattðqÞ 5 1
2 jjq 2 qgoaljj2 5 1
2
X
n
i51
ðqi2qi;goalÞ2
In this case, we have:
rUattðqÞ 5 1
2 ð2jjq 2 qgoaljjrjjq 2 qgoaljjÞ
5 jjq 2 qgoaljj ðq 2 qgoalÞ
jjq 2 qgoaljj
5 q 2 qgoal
Therefore:
FattðqÞ 5 2 ðq 2 qgoalÞ
which shows that FattðqÞ tends linearly to 0 with distance to goal. This is good for
stability but tends to infinity when the distance goes far from goal.
If we define UattðqÞ as the norm of q 2 qgoal, that is:
UattðqÞ 5 jjq 2 qgoaljj
then:
rattUðqÞ 5 r
X
n
i51
ðqi2qi;goalÞ2
(
)1=2
5 1
2
X
n
i51
ðqi2qi;goalÞ2
(
)21=2
r
X
n
i51
ðqi2qi;goalÞ2
"
#
5 ðq 2 qgoalÞ=
X
n
i51
ðqi2qi;goalÞ2
"
#1=2
5 ðq 2 qgoalÞ
jjq 2 qgoaljj
445
Mobile Robot Path, Motion, and Task Planning

In this case:
FattðqÞ 5 2 ðq 2 qgoalÞ=jjq 2 qgoaljj
which is singular (unstable) at the goal, and tends to 1 far from the goal.
Very often, in practice, we use a composite attractive potential field of the form:
UattðqÞ 5
1
2 λkq 2 qgoalk2
if
kq 2 qgoalk , ε
μkq 2 qgoalk
if
kq 2 qgoalk $ ε
8
<
:
where λ and μ are scaling factors and ε is a selected distance from the goal. The
repulsive potential field is typically selected as:
UrepðqÞ 5 1=jjq 2 qjj
where q is the closest point to the obstacle.
Therefore:
FrepðqÞ 5 2 rUrepðqÞ 5 ðq 2 qÞ=jjq 2 qjj2
For each additional obstacle, we add a corresponding repulsive potential field. If
an obstacle is non-convex, we triangulate it into multiple convex obstacle and
weigh the separate fields of the object in case the summed repulsive fields are
greater than that of the original obstacle. The main problem with potential-field-
based path planning is that the robot may be trapped to a local minimum of the
field. Actually, several methods exist for avoiding this trapping. Specifically, when
the robot goes into a local minimum position, we can correct this situation in one
of the following ways [27]:
G
Backtrack from the local minimum and use an alternative strategy to avoid the local
minimum.
G
Perform certain random movements, hoping that they will help escaping the local
minimum.
G
Use more complex potential fields that are local minimum free (e.g., harmonic potential
fields).
G
Apply an extra force Fvfs which is called virtual free space force.
Of course, all the above methods assume that the robot can detect that it is
trapped, which is also a difficult problem on its own. The virtual free space force is
proportional to the amount of free space around the robot [28], and helps to pull
the robot away from the local minimum region. In other words, the virtual force
drags the robot outside the local minimum, and so the robot can begin again using
the potential field planner. Fortunately, it is unlikely the robot to be trapped again
446
Introduction to Mobile Robot Control

to the same local minimum. The above virtual force concept and its effect is illus-
trated pictorially in Figure 11.11AC, where the robot is moved by the total force:
F 5 Fatt 1 Frep 1 Fvfs
A hybrid virtual force field method that integrates the virtual force field concept
with the virtual obstacle and virtual goal concept is presented in Ref. [40].
One way to detect that the robot is trapped is to employ an open-loop position
estimator which estimates the current position of the robot. If the current position
does not change for a predefined period of time (time threshold), then the virtual
force is generated and applied to the robot for pulling it out from the local mini-
mum. Obviously, due to the repulsive force, the robot velocity is decreased as it
approaches an obstacle. A Java source code of a path planning algorithm that uses
Fvfs is provided in Ref. [27].
Three other problems that may be encountered in potential-field-based path
planning are the following [29]:
1. The robot cannot pass between closely spaced obstacles.
2. Oscillations occur in the presence of obstacle disturbances.
3. Oscillations occur in narrow passages.
Goal
U_trap
Robot
Goal
Wall
Robot
(B)
(C)
Goal
U_trap
Robot
Goal
Wall
Robot
Robot
Frep
Fatt
Fvfs
Obstacle
Goal
(A)
Figure 11.11 (A) The virtual free
space force Fvfs drags the robot
outside the local minimum area in
CSfree. (B) Fvfs untraps the robot from
a U-trap. (C) Untrapping from a wall
trap.
447
Mobile Robot Path, Motion, and Task Planning

The first situation is illustrated in Figure 11.12A. The two obstacles exert the
repulsive forces Frep;1 and Frep;2 giving a total (resultant) repulsive force Frep. Thus,
the robot moves under the influence of the goal attractive force Fatt and the total
obstacles’ repulsive force Frep. Their sum:
Ftotal 5 Fatt 1 Frep
in this case gets the robot away from the passage that leads toward the robot. Of
course, depending on the relative magnitude of Fatt and Frep, the robot may be
moved toward the goal passing through the opening between the obstacles.
In Figure 11.12B, a robot is forced to move alongside a wall which obstructed
its path. At a certain point, the wall has a discontinuity which causes the robot to
move in an oscillatory mode. Figure 11.12C (right) shows the oscillatory behavior
caused when the robot moves to a narrow corridor. This is due to that the robot is
subject to two repulsive forces from opposite sides, simultaneously. If the corridor
Fatt
Ftotal
Frep,2
Frep,1
Frep
FR
Ol
O2
L
θ
θ
Goal
(A)
Goal
Start
Wall
Start
Goal
Wall
Wall
Start
Goal
Wall
Wall
(B)
(C)
Figure 11.12 (A) A situation in which the robot does not pass through the opening between
obstacles. (B) The robot motion enters an oscillatory mode when it encounters an obstacle
disturbance. (C) In a wide corridor, the robot manages to move without oscillations (left),
but if the corridor is very narrow, the robot exhibits oscillatory motion (right).
448
Introduction to Mobile Robot Control

is sufficiently wide (Figure 11.12C, left), the robot may manage to get a
stable (nonoscillatory) motion. All the above phenomena have been studied analyti-
cally by stability theory, and have been observed in practical or simulated experi-
ments [29]. To overcome these limitations of the potential field approach, Koren
and Borenstein developed the so-called vector field histogram (VFH) method [30],
which was further improved in Ref. [31].
11.4.2.4
Vector Field Histograms
In this method, the obstacle-free path planning is performed with the aid of an
intermediate data structure about the local obstacle distribution, called polar histo-
gram which is an array of, say, 72 (5 wide) angular sectors (Figure 11.13). To
take into account the robot changing position and the new sensor readings, the
polar histogram is totally updated and rebuilt every, say, 30 ms (sampling period).
The method involves two steps [30]:
1. The histogram grid is reduced to one-dimensional polar histogram which is built around
the robot’s instantaneous location (Figure 11.15A). Each sector in the polar histogram
involves a value that represents the polar obstacle density (POD) in this direction.
2. The most suitable sector from among all polar histogram sectors with a low POD is
selected, and the robot moves in that direction.
To implement these steps, a window (called active window) moves with the
robot, overlying a square region of cells (e.g., 33 3 33 cells) in the histograms. All
(B)
Histogram grid
Polar histogram
Active
cells
Certainty
values 
Sonar
Previous
reading
Current
reading
Motion 
direction
Measured
distance d
Cone
width
(A)
Obstacle
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
04
04
02
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
Figure 11.13 (A) Histogram grid. (B) The active cells are mapped onto the polar histogram.
449
Mobile Robot Path, Motion, and Task Planning

cells that lie on the moving window, each time, are called active cells. Actually,
the cell that lies on the sonar axis and corresponds to the measured distance d
found by each range reading is incremented and increases the certainty value (CV)
of the cell (Figure 11.13A). The contents of each active cell in the histogram are
mapped onto the corresponding polar histogram sector (say the kth sector), which
gives a value Hk to this sector (Figure 11.13A). The value is higher if there are
many cells with high CV in one sector. Obviously, this value can be regarded as
the POD in the direction of sector k.
A typical terrain with three obstacles A, B, and C is shown Figure 11.14.
The polar histogram of the obstacle configuration of Figure 11.14 has the form
of Figure 11.15B, and its counterclockwise pseudoprobability polar histogram
(from A to C) on the H-k plane has the form of Figure 11.15A.
The peaks A, B, and C in Figure 11.15A result from the obstacle clusters A, B,
and C in the histogram grid. To determine the safe directions of motion, a threshold
T in POD is used. If POD . T, then we have unsafe (prohibited) direction of
motion. If POD , T, we can select the most suitable direction in this sector.
Figure 11.16AC depicts three possible cases of robot motion alongside an obsta-
cle. When the robot is too close to the obstacle, the steering angle ψsteer points
away from the obstacle. If the robot is away from the obstacle, ψsteer points toward
the obstacle. Finally, when the robot is at the proper distance from the obstacle, the
robot moves alongside it.
11.4.3
Integration of Global and Local Path Planning
As discussed in Section 11.2, mobile robot path planning is distinguished in local
and global path planning. In the first, which is performed while the robot is mov-
ing, the robot can find a new path according to the changes of the environment
(obstacles, stairs, etc.). Global path planning can be performed only in a static envi-
ronment which is known to the robot. In this case, the path planning algorithm pro-
duces a complete path from the start point to the goal before the robot starts its
1
2
3
y(m)
1
0
2
3
x(m)
Object
cluster C
Object
cluster B
Robot
Start
Pole (object) A
Goal Figure 11.14 A mobile robot
moving in a terrain with three
obstacles (or clusters of obstacles)
A, B, and C.
450
Introduction to Mobile Robot Control

motion. A property that characterizes a path is its smoothness which is defined as
the maximum curvature of the path measured over both its segments and the entire
path. A motion planning method that takes into account the path curvature is pre-
sented in Section 11.5.3. The relative advantages and disadvantages of a typical
global and local path planning can be seen in Figure 11.17.
The global path might be produced, for example, by the approximate cell
decomposition method, but this could be done by including segments with high
curvature values (e.g., the sharp turn shown in Figure 11.17A at the original motion
steps of the navigation). This is a result of the nature of global path planning and is
caused by the digitization of the space cells. But a global path method (e.g., cell
decomposition) has an excellent capability in driving the robot out of an H-shaped
obstacle which is known to be one of the most difficult cases (Figure 11.17A).
(B)
d<dp
d>dp
d=dp
ψsteer
ψsteer
ψsteer
Goal
(A)
Goal
Goal
(C)
Figure 11.16 (A) Direction of motion when the robot obstacle distance d is smaller than the
proper (desired) distance dp. (B) Motion alongside the obstacle ðd 5 dpÞ. (C) Direction of
motion when d . dp.
(B)
A
C
B
Start
Threshold
Active window
(A)
H(k)
Threshold
Polar
histogram
Goal
A
B
C
k
270°
180°
90°
0°
Figure 11.15 (A) The one-dimensional polar histogram (pseudoprobability distribution)
corresponding to obstacles A, B, and C of Figure 11.14 in the counterclockwise direction
from the x-axis. (B) The polar form of (A).
Source: Reprinted from Ref. [30], with permission from Institute of Electrical and Electronic
Engineers.
451
Mobile Robot Path, Motion, and Task Planning

As it is seen in Figure 11.17B, the local navigation strategies (e.g., active kine-
matic histograms [41]) present a series of simpler problems. The local planner man-
ages to reach the goal in the first two cases, but in the last case, when the obstacle
fully obstructs the way to the goal, the method fails to find a path although one
exists, and the robot stays trapped in local minima in front of the obstacle. In this
case, all the paths produced are sufficiently smooth, and the robot is traveling even
adapting its speed according to obstacle configuration.
A global path planner is able to find a path from the start to the goal, if there
exists such a path. But, in order to achieve smooth motion, the local planner gets
the subgoals produced by the global planner and drives the robot toward them
avoiding obstacles in the vicinity of the robot. Each time the local planner reaches
a subgoal, the control returns back to the global path planner with the next subgoal.
The above illustrates the necessity to properly integrate a global and a local path
planner in order to warrant smooth and obstacle-free paths (without trapping at
local minima). Actually, if a local minimum is detected at some point of the path,
the global path planner is called to escape from the situation.
The above integration (collaboration) of local and global path planning is illus-
trated in pseudocode form as follows [42]:
Go to goal (start position, final goal)
While not at final goal
next subgoal 5 Global Path Planner (final goal)
Local path planner (next subgoal)
While not at next subgoal and not blocked in local minima
Drive the robot toward next subgoal
If blocked at local minima
Call recursively the go to goal algorithm with start operant the current position and
goal operant the final goal.
A fuzzy algorithm which implements global and local navigation integration
and uses potential fields is presented in Section 13.10. Another integrated (hybrid)
path planner for indoor environments is presented in Ref. [32]. This hybrid planner
Start
Goal
(A)
S
G
S
S
G
G
(B)
Figure 11.17 Typical cases of path
planning: (A) global path planning and
(B) local path planning.
452
Introduction to Mobile Robot Control

combines the so-called distance transform path planner (DTPP) and the potential
field planner. Unlike most planners, the distance transform (DT) planner treats the
task of path planning by finding paths from the goal location back to the start
location. This path planner propagates a distance wave front through all free space
grid cells in the environment from the goal cell as shown in Figure 11.18A.
Figure 11.18B shows a minimal length DT path [43].
Actually, the robot has to build the local map on the move, and at the same time
constantly recompute the distance map and the path using locally available sensory
data. DTPP employs an occupancy grid-based map of the workspace to compute its
distance map. At the initialization stage, the goal cell and obstacle cells are assigned
values that represent their distance from the goal. These distance values are propa-
gated flowing around the obstacles. An algorithm similar to raster scanning iterates
until the values of all cells are stabilized, in which case distance values are assigned
to the rest of the cells. Obviously, the obstacle cells must be given very high values,
and are passed over in the raster scan, and the raster scans are repeated until no fur-
ther changes occur. The free configuration space cells should be treated in the same
way. It was shown that the resulting DT is independent of any start point and repre-
sents a potential field without local minima [32]. Thus, a globally minimum distance
path from any start point in free space can be determined via the standard gradient/
steepest descend technique (see section 7.2.1). For a nonpoint robot, the obstacles are
grown as usual by the maximum effective radius of the robot to convert the path plan-
ning to that of a point (dimensionless) robot.
Omitting the details, the steps of DTPP are as follows [32]:
Step 1: Using DT, create an optimal global path from the start point to the final goal.
Step 2: If not at the final goal, create a circle centered at the robot current position with
reasonable radius depending on the configuration of the environment.
(A)
G
8
7
7
7
6
6
6
6
5
5
5
5
5
4
4
4
4
4
3
3
4
3
3
3
3
2
2
2
2
2
2
2
2
2
2
2
1
1
1
1
1
1
1
1
3
3
3
3
3
3
8
8
8
8
8
9
4
4
4
4
4
5
6
7
7
7
8
9
(B)
Figure 11.18 (A) The DT path planning scheme. (B) A typical minimal length path found
by the steepest descend and DT.
Source: Reprinted from Ref. [43], with permission from International Journal of Computer
Science and Applications.
453
Mobile Robot Path, Motion, and Task Planning

Step 3: Select the next subgoal as the intersection of the circumference of the circle and
the planned global path.
Step 4: Use the potential field planner to reach the subgoal. Since the subgoals are always
on the obstacle-free global path, the potential field planner cannot be trapped at local
minima.
If there are two intersections along the planned global path, the one with lower
distance value is selected such that the subgoal will always move toward the final
goal. If no intersections are found, the radius of the circle is increased until an
intersection with the planned global path is found. Implementation examples of
DTPP are provided in Ref. [32]. A useful modification of DTPP method is pro-
vided in Ref. [44]. This modification extends the method to multiple robots by set-
ting the initial exploration directions.
11.4.4
Complete Coverage Path Planning
Complete coverage path planning produces a path in which the robot sweeps all
areas of free space in an environment. It is needed very often in practice (e.g., in
autonomous room vacuum, security robots, lawn mowers). In the complete cover-
age DTPP, the robot moves away from the goal keeping track of the cell it has vis-
ited. The robot moves to a cell that has a smaller distance from the goal, only if it
has visited all the neighboring cells that lie further away from the goal. The typical
DTPP algorithm for complete coverage is as follows) [45,46]:
Set the start cell to current cell
Set all cells to not visited
Loop
Find unvisited neighboring cell with highest DT
If no neighbor cell is found then
Mark as visited and stop at Goal
If Neighbor cell DT ,5 Current cell DT then
Mark as visited and stop at Goal
Set current cell to neighboring cell
End Loop
A one-obstacle environment with corresponding DT values is shown in
Figure 11.19A, and a complete coverage path for this environment is depicted in
Figure 11.19B.
A modified complete coverage DT path planning, called path transform path
planning (PTPP) [45] propagates a weighted sum of the distance from the goal and
a measure of the discomfort of moving too close to obstacles. Thus actually, a
PTPP is a form of DTPP without the trapping to local minima possibility. The steps
of the PTPP are the following:
Step 1: Invert the DT transform into an obstacle transform (OT), where the obstacle cells
become the goals. This implies that for each free cell, the minimal distance from the cen-
ter of the free space to the boundary of an obstacle cell is obtained.
454
Introduction to Mobile Robot Control

Step 2: Define a cost function transform, called the path transform ðVPTÞ, of the form:
VPTðcÞ 5 min
pAP
LðpÞ 1
X
ciAp
λ OðciÞ
(
)
where P is the set of all possible paths to the goal, ci is the ith cell in P, and p is a single
path in P. The function LðpÞ is the length of the path p to the goal, and the function OðciÞ is
a cost function produced by using the values of OT. The constant λ $ 0 is a weight that
specifies how strongly the PT will avoid obstacles. The minimization of LðpÞ 1 P λ OðciÞ
is done by the standard steepest descent method, without the possibility to be locked at
some minimum. This is because all costs of paths to the goal from each cell are computed.
Figure 11.20 depicts an environment with four obstacles and distances shown
(A), a path found with the obstacle transform (B), and two paths found by path
transform with weight constants λ1 , λ2 (C and D) [45,46].
Figure 11.21A shows the actual path generated by PTPP implemented on the
AMROS simulator [47] in a 7 m 3 6 m room with a 1:5 m 3 1:0 m obstacle at
the room’s center.
G
S
(B)
(A)
13 12 11 10
9
8
7
7
7
7
7
7
7
7
9
8
7
6
5
4
3
2
2
2
2
2
3
4
9
8
7
6
5
4
3
2
1
1
1
2
3
4
9
8
7
6
5
4
3
2
1
1
2
3
4
1
9
8
7
6
5
4
3
2
1
1
2
3
4
2
9
8
7
6
5
4
3
2
2
2
2
3
4
3
9
8
7
6
5
4
3
3
3
3
3
3
4
4
4
4
4
4
13 12 11 10
9
8
7
6
6
6
6
6
6
6
12 11 10
9
8
7
6
5
5
5
5
5
5
S
G
Figure 11.19 (A) An environment with
a single obstacle. (B) A complete
coverage DT path from S to G.
Source: Reprinted from Ref. [45], with
permission from Japan Robot
Association.
455
Mobile Robot Path, Motion, and Task Planning

Another class of complete coverage path planning methods is based on the spiral
algorithm as shown in Figure 11.21B [48].
If no obstacle on the right turn right
ELSE
If no obstacle in the front
Move forward
ELSE
If no obstacle on the left
Turn left
OTHERWISE
Terminate the algorithm
In this algorithm, previously covered cells and presently occupied cells are
regarded as obstacles. The above basic spiral complete coverage path planning
algorithm has been improved by many authors. For example, in Ref. [48], the com-
plete coverage algorithm represents the environment as a union of robot-sized cells
(a)
(b)
Weight λ1
(C)
Weight λ2
(C)
2
1
1
1
1
1
1
2
2
2
2
2
3
4
2
2
2
2
2
2
2
1
1
1
1
1
1
2
3
4
1
1
1
1
1
1
2
1
1
2
3
4
1
2
1
1
2
3
4
1
2
1
1
2
3
4
2
2
1
2
2
2
2
3
4
1
1
2
3
4
5
1
1
2
3
4
4
1
1
1
1
1
2
3
3
4
2
1
1
2
3
3
3
3
3
4
9 10 11 12 15 18 21 24
33 36 39 42
6
7
8 11 14 17 20 23 26 29 32 35 38 41
3
4
7 10 13 16 19 22 25 28 31 34 37 40
G
3
20 23 26 29 32 35 38 41
38
34
39 42 45 S
35
31
38 41 44 47
34
31
30
29
28
37 40 45 46
33
30
27
26
25
36 39 42 45
32
29
26
23
22
35 38 41 44
25
22
19
36 37 40 43
19 20 21 25 29 33 30 41
62 55 55 57
8
9 13 17 21 25 29 40 46 50 54 51 53 55
4 12 21 25 29 33 37 34 38 42 46 49 51 54
G 11
45 38 38 41 44 47 50 53
72
76
64 63 64 S
67
65
60 60 63 66
59
55
54
53
54
56 59 62 65
51
47
43
42
50
55 59 62 65
59
54
46
38
46
63 60 61 62
42
34
41
67 59 58 59
133 134 135 156 177 198 149 240
291 200 179 180
42
43
64
85 106 127 148 239 262 283 286 199 178 179
21
92 135 156 177 198 219 170 191 194 195 196 177 178
G
91
262 191 172 173 174 175 176 177
298
486
207 186 185 S
297
395
206 185 184 185
296
305
304
303
304
205 184 183 184
227
234
213
212
283
206 203 182 183
298
305
262
191
262
293 202 181 182
241
170
241
292 201 180 181
Figure 11.20 (A) DT for an environment with four obstacles. (B) Obstacle transform with
the corresponding path. (C, D) Paths found with the PTPP for the same environment, where
each path corresponds to the minimum propagated path cost to the goal. The path (C)
corresponds to a lighter cost function than (D), that is, λ1 , λ2.
Source: Reprinted from Ref. [45], with permission from Japan Robot Association.
456
Introduction to Mobile Robot Control

and then uses a spiral scheme. The overall path planner links the basic spiral paths
using the constrained inverse DT.
11.5
Mobile Robot Motion Planning
11.5.1
General Online Method
The motion planning algorithms are distinguished into:
G
Explicit
G
Nonexplicit
according to the underlying path planning scheme.
(A)
(B)
Figure 11.21 (A) A simulated complete coverage path for the Yamabico robot using PTPP
with estimated position correction. (B) Spiral complete coverage path planning for initial
robot orientation parallel (left) and nonparallel (right) to the wall.
Source: (A) Reprinted from Ref. [45], with permission from Japan Robot Association.
457
Mobile Robot Path, Motion, and Task Planning

Discrete explicit path planning schemes are the road map method and the cell
decomposition method. The continuous explicit schemes are open-loop control
algorithms. When a path compatible with the problem constraints is selected, this
path and its derivatives constitute the feedforward component (reference trajectory)
for the closed-loop system. Depending on the level at which the motion planning is
made, we have kinematic planning and dynamic planning.
The nonexplicit motion planning methods (or as otherwise called, online
methods) can be regarded as feedback control algorithms which are based on the
displacement of the robot from the desired trajectory which is (possibly) generated
by an explicit motion planning algorithm. The motion plan is an algorithm that
determines how the robot should move, given its present state and present knowl-
edge. A representative nonexplicit method uses an artificial potential field function
in the configuration space which takes a minimum at the goal configuration.
Suppose that the configuration space is a subset of Rn. Let q be the n-dimensional
vector of joint positions, and qd the vector of desired joint positions (that specify the
goal configuration of the robot). Then, we define a potential function as:
V0ðqÞ 5 ðq2qdÞTKðq 2 qdÞ
ð11:1aÞ
where K is an n 3 n positive definite matrix ðK . 0Þ. The simplest way to go to the
goal is to use a velocity controller:
_q 5 2 @V0ðqÞ
@q
ð11:1bÞ
which leads the robot asymptotically to the goal configuration qd. If there are
obstacles, then the velocity _q should move the robot away from them. Let dðq; EiÞ
be a function that gives the distance of the robot’s point that lies in a smaller dis-
tance from the obstacle Ei. We construct a new potential function VEiðqÞ as:
VEiðqÞ 5 2 ki=d q; Ei
ð
Þr
ð11:2Þ
where r is a suitable integer and ki a positive constant, and instead of Eq. (11.1a)
we use a total potential function:
VðqÞ 5 V0ðqÞ 1 VEiðqÞ
Then, the controller:
_q 5 2 @VðqÞ
@q
5 2 @V0ðqÞ
@q
2 @VEiðqÞ
@q
ð11:3Þ
leads the robot to the goal qd while simultaneously moves it away from the obsta-
cle Ei. Clearly, for each obstacle Eiði 5 1; 2; . . .Þ, we must add a new VEiðqÞ.
458
Introduction to Mobile Robot Control

Example 11.2
The problem is to derive the local motion planning equations for a WMR using potential
fields and polar representation.
Solution
For the purposes of local motion planning, we only need the local (relative) polar coordi-
nates depicted in Figure 11.22 [26].
These coordinates are as follows:
G Distance dGR and angle φGR of the goal relative to the robot
G Distance dOR and angle φOR of the obstacle relative to the robot
G Distance dOG and angle φOG of the obstacle relative to the goal
G Angle φγ between the robot axis direction and the direction in which the robot should
go to the goal
G Angle φδ between the robot-goal and the obstacle-goal direction lines
From the geometry of Figure 11.22, we find the relations:
dOR 5 ðd2
GR1d2
OG22dGRdOG cos φδÞ1=2
φδ 5 ðφGR 2 φγÞ 2 ðπ 2 φOGÞ
φOR 5 φ sgnðφγÞ 1 φGR
where:
cos φ 5
d2
GR 1 d2
OR 2 d2
OG
2dGRdOR


The above relations give the distance dOR and angle φOR of the obstacle relative to the robot
in terms of dOG and φOG which are independent of the robot position and direction of motion.
Now,
applying
the
general
potential-field-based
motion
planning
method
of
Section 11.5.1, we define a potential function:
VðqÞ 5 VattðqÞ 1 VrepðqÞ
O
dOR
R
Robot
dOG
Obstacle
φδ
φGR
φOR
φOG
G
Goal
φγ
dGR
Figure 11.22 Geometry of local WMR
motion planning in the presence of an
obstacle.
459
Mobile Robot Path, Motion, and Task Planning

which involves the goal-attraction term:
VattðqÞ 5
1
2 λjjq 2 qgoaljj2 5 1
2 λd2
GR
when
dGR , ε
μjjq 2 qgoaljj 5 μdGR
when
dGR . ε
8
>
<
>
:
and an obstacle-repulsion term:
VrepðqÞ 5 1=jjq 2 q
obstaclejj 5 1=dOR
with λ, μ being scaling factors, ε . 0 being a distance from the goal, and q
obstacle the
closest point to the obstacle. For each additional obstacle Oi, a corresponding repulsive
potential field Vrep;iðqÞði 5 1; 2. . .; MÞ should be added, where M is the number of obstacles.
In this case, the total repulsive potential is equal to:
VrepðqÞ 5
X
M
i51
Vrep;iðqÞ
The attractive and repulsive forces applied to the robot are:
FattðqÞ 5 2 @VattðqÞ=@q;
FrepðqÞ 5 2 @VrepðqÞ=@q
And the total force is:
FðqÞ 5 FattðqÞ 1 FrepðqÞ
Clearly, since here we consider only local motion planning, we can assume that dGR , ε
for some appropriate value of the distance ε that specifies the local area around the robot.
Therefore:
VattðdGRÞ 5 ð1=2Þλd2
GR
VrepðdORÞ 5 ðd2
GR1d2
OG22dGRdOG cos φδÞ21=2
φδ 5 φGR 2 ðφγ 1 π 2 φOGÞ
and
@VattðdGRÞ=@dGR 5 λdGR
@Vrep=@dGR 5 2 ðVrepÞ3ðdGR 2 dOG cos φδÞ
@Vrep=@φGR 5 2 ðVrepÞ3dGRdOG sin φδ
since @φδ=@φGR 5 1.
460
Introduction to Mobile Robot Control

These gradients are now used in the robot motion (velocity) planning controller
(11.3). The result is:
vGR 5 _dGR 5 2 @Vatt
@dGR
2 @Vrep
@dGR
5 2 λdGR 1 ðVrepÞ3ðdGR 2 dOG cos φδÞ
ωGR 5 _φGR 5 ðVrepÞ3dGRdOG sin φδ
Two examples of the potential field path followed by a robot in a terrain with one or
three obstacles are shown in Figure 11.23.
The potential field motion planning method enhanced with neural network learning
was studied in Ref. [26].
11.5.2
Motion Planning Using Vector Fields
This method is particularly useful for the motion planning of nonholonomic
WMRs. Consider the differential drive WMR of Figure 2.7 which is described by
the affine kinematic model (2.37a):
_xQ
_yQ
_φ
2
4
3
5 5
ðr=2Þcos φ
ðr=2Þsin φ
r=2a
2
4
3
5u1 1
ðr=2Þcos φ
ðr=2Þsin φ
2r=2a
2
4
3
5u2
ð11:4Þ
where u1 5 _θ r and u2 5 _θ l. As we already know, although using the two inputs u1
and u2 in Eq. (11.4), the robot can go to any desired point of the terrain, and due to
the nonholonomicity, the vehicle cannot follow all the possible trajectories on the
plane.
Any motion plan (program) should include the constraint Eq. (11.4). A solution
to this is as follows [23].
Let a robot be described by:
_x 5 f1ðxÞu1 1 f2ðxÞu2 1 ? 1 fmðxÞum
ð11:5Þ
Obstacle
Start
Goal
Start
Goal
Figure 11.23 Robot paths with
the above potential-based local
motion planning.
461
Mobile Robot Path, Motion, and Task Planning

where x 5 ½x1; x2; . . .; xnT is the n-dimensional state vector and u 5 ½u1; u2; . . .; umT
the m-dimensional input vector. The initial and final configurations are, respectively:
xðt0Þ 5 x0;
xðtfÞ 5 xf
ð11:6Þ
where t0 is the initial time and tf is the final time.
The motion planning problem is to find a piecewise continuous and bounded
input vector uðtÞ such that to satisfy the second relation in Eq. (11.6). This is actu-
ally an open-loop final state (pose) control problem. Here, we will examine it using
the vector-field and Lie bracket theory. The Lie bracket of the vector fields fi and
fjði; j 5 1; 2; . . .; mÞ in the configuration space CS is defined as in Eq. (6.12), that is:
fi; fj


ðxÞ 5 @fj
@x fi 2 @fi
@x fj
ð11:7Þ
It is an antisymmetric operator that returns a vector field and provides a measure of
how the flows @f=@x that correspond to the vector fields fi and fj are interchanged.
Actually, the Lie bracket represents the infinitesimal motions that result when we
have a forward flow by fi and then by fj and after that a backward flow by 2fi and
then by 2fj. Clearly, if this input sequence is applied to a linear system, it will lead to
a zero net motion. But in nonlinear systems, we can generate new motion directions
by simply transposing vector fields through the Lie bracket (see Figure 6.3).
This can be done by applying the Lie bracket (operator) to each of the new
motion directions and each of the directions that has been used in their generation.
Let Δ0 be the span distribution of the initial input vector fields f1; f2; . . .; fm.
We denote the sequential application of the Lie bracket to the input vector
fields as:
Δi 5 Δi21 1 spanf½α; β; αAΔ0; βAΔi21g
The criterion for checking controllability is the following (see also Theorem 6.9):
“The system is controllable if and only if we have Δk 5 Rn for some k.”
Applying this criterion to our differential drive WMR Eq. (11.4), we have:
f1 5
ðr=2Þcos φ
ðr=2Þsin φ
r=2a
2
4
3
5;
f2 5
ðr=2Þcos φ
ðr=2Þsin φ
2r=2a
2
4
3
5
ð11:8Þ
The Lie bracket of f1 and f2 is:
f3 5 ½f1; f2 5
2ðr2=2aÞsin φ
ðr2=2aÞcos φ
0
2
4
3
5
ð11:9Þ
462
Introduction to Mobile Robot Control

Because these vector fields cover all the allowable motion directions on the
plane, the system is controllable. This criterion can be used for the generation of
desired motions (i.e., of successfully controlling robots with nonholonomic con-
straints along a motion plan). Indeed, suppose that we want to find a motion for a
Lie bracket (system) ½fi; fj of first order (Eq. (11.9)). This can be done by using
high-frequency sinusoid inputs of the form [23,49].
ui 5 ξiðtÞ 1
ﬃﬃﬃﬃω
p ξijðtÞsin ωt
ð11:10aÞ
uj 5 ξjðtÞ 1
ﬃﬃﬃﬃω
p ξjiðtÞcos ωt
ð11:10bÞ
In the limit ω !N, we get the motion:
_x 5 fiðxÞξiðtÞ 1 fjðxÞξjðtÞ 1 1
2 ξijðtÞξjiðtÞ fi; fj


ðxÞ
ð11:11Þ
This means that using the above ui and uj allows the robot to follow the direc-
tion of the Lie bracket ½fi; fj as if it was one of the initial controlled directions.
11.5.3
Analytic Motion Planning
The analytic obstacle avoidance motion planning approach consists in describing
the WMR trajectory parametrically and determining the optimal values of the para-
meters used [13]. Here, we will use sinusoid parameterization. Let x0 be an initial
configuration of the mobile robot M that moves on the configuration space
CS 5 R2, which involves a number of stationary obstacles Biði 5 1; 2; . . .; nÞ. The
problem is to find a trajectory that determines a sequence of configurations of M
from x0 to a desired configuration (pose) xd such that the robot avoids collisions
with the obstacles Bi.
To solve this problem, we express the plane trajectory by the Cartesian paramet-
ric equations x 5 xðtÞ and y 5 yðtÞ which represent the position of the robot M as a
function of the parameters from t 5 0 to t 5 tf 5 1 at the end of the trajectory. Such
a curve must be sufficiently smooth to assure the continuity of the curvature κðtÞ,
which is defined as:
κðtÞ 5 dφ
ds 5 dφ=dt
ds=dt 5
_φðtÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
_x2 1 _y2
p
ð11:12aÞ
where φ is the tangential angle and s is the curve length. The derivative _φðtÞ can be
found using the relation:
tg φðtÞ 5 dy
dx 5 dy=dt
dx=dt 5 _yðtÞ
_xðtÞ
ð11:12bÞ
463
Mobile Robot Path, Motion, and Task Planning

From Eq. (11.12b), we find:
d
dt tg φðtÞ 5
1
cos2 φðtÞ
_φðtÞ
5 ð1 1 tg2 φðtÞÞ_φðtÞ
5 ð1 1 _y2=_x2Þ_φðtÞ
that is:
_φðtÞ 5
_x2
_x2 1 _y2
0
@
1
A d
dt tg φðtÞ
5
_x2
_x2 1 _y2
0
@
1
A d
dt
_yðtÞ
_xðtÞ
0
@
1
A
5 _x€y 2 _y€x
_x2 1 _y2
ð11:13Þ
Therefore, Eq. (11.12a) gives:
κðtÞ 5 _xðtÞ€yðtÞ 2 _yðtÞ€xðtÞ
½_x2ðtÞ1 _y2ðtÞ3=2
ð11:14Þ
It follows that for κðtÞ to be continuous, the functions xðtÞ and yðtÞ must be
differentiable at least up to order 3.
Some typical forms of parametric curves fxðtÞ; yðtÞg used for path/motion plan-
ning are as follows:
Cartesian polynomials
xðtÞ 5
X
n
i50
aiti;
yðtÞ 5
X
n
i50
biti
ðai; biARÞ
Cubic spirals
_φðtÞ 5 1
2 At2 1 Bt 1 C 5 κðtÞ
φðtÞ 5 1
6 At3 1 1
2 Bt2 1 Ct 1 D
This class minimizes the integral of the square of the curvature’s derivative.
464
Introduction to Mobile Robot Control

Finite sums of sinusoids
xðtÞ 5
X
n
i51
½Ai cosðiωtÞ 1 Bi sinðiωtÞ
ð11:15aÞ
yðtÞ 5
X
n
i51
½Ci cosðiωtÞ 1 Di sinðiωtÞ
ð11:15bÞ
For n 5 N, these functions can approximate any continuous function.
The optimization criterion is typically the length of the curve cðtÞ 5 cðxðtÞ; yðtÞÞ
which is given by:
J 5
ð1
0
½_x2ðtÞ 1 _y2ðtÞdt
ð11:16Þ
The optimal trajectory minimizes J. Any trajectory that satisfies the inequality
J , Jmax, where Jmax is a predetermined (allowed) upper bound, is defined as sub-
optimal trajectory.
In the following, we will assume a given number n of sinusoids and a given fre-
quency ω. Thus, the parameter vector θ to be selected has dimensionality 4n:
θ 5 ½A1; B1; C1; D1; A2; B2; C2; D2; A3; B3; C3; D3; . . .; An; Bn; Cn; DnT
ð11:17Þ
Differentiating Eqs. (11.15a) and (11.15b), we get:
_xðtÞ 5
X
n
i51
½2 ðiωÞAi sin ðiωtÞ 1 ðiωÞ Bi cosðiωtÞ
ð11:18aÞ
_yðtÞ 5
X
n
i51
½2 ðiωÞCi sinðiωtÞ 1 ðiωÞDi cosðiωtÞ
ð11:18bÞ
Therefore, the above quadratic function J is a square function of the components
of θ, independent of the parameter t.
Now, suppose that the robot is to go from the initial position xð0Þ 5 0, yð0Þ 5 0
with orientation φ0, to the goal (final) position xð1Þ 5 xf, yð1Þ 5 yf with orientation φf.
The corresponding conditions for Ai, Bi, Ci, and Di are found by replacing the above
initial and final conditions in Eqs. (11.15a), (11.15b), (11.18a), and (11.18b), namely:
xð0Þ 5 0 gives
X
n
i51
Ai 5 0
465
Mobile Robot Path, Motion, and Task Planning

yð0Þ 5 0 gives
X
n
i51
Ci 5 0
xð1Þ 5 xf gives
X
n
i51
½Ai cosðiωÞ 1 Bi sinðiωÞ 5 xf
yð1Þ 5 yf gives
X
n
i51
½Ci cosðiωÞ 1 Di sinðiωÞ 5 yf
_yð0Þ
_xð0Þ 5 tg φ0 gives
Pn
i51½iωDi
Pn
i51½iωBi 5 tg φ0;
_yð1Þ
_xð1Þ 5 tg φf gives
Pn
i51½2 iωCi sinðiωÞ 1 iωDi cosðiωÞ
Pn
i51 2iωAi sinðiωÞ 1 iωBi cosðiωÞ
½
 5 tg φf
The obstacles that exist in the task space can also be represented by parametric
curves. For example, an object with the shape of an ellipse is described by:
ðxðtÞ2xÞ2=a 1 ðyðtÞ2yÞ2=b 5 1
ð11:19Þ
An edge of a polygonic object is described by a straight line segment:
yðtÞ 5 axðtÞ 1 b
Therefore, one way to guarantee obstacle avoidance is to assure that the path
and objects parametric curves do not intersect. This can be done if we equate the
expression of cðtÞ with any boundary of the obstacle (which is another parametric
function with parameter s) and solve the resulting algebraic system with respect to
t and s. The two curves intersect if:
0 # t # 1
and
0 # s # 1
Another method is to consider the constrained minimization of the function J
with the constraint:
gðθÞ # 0
ð11:20Þ
that expresses the obstacle-free task space. For example, for the elliptic obstacle,
this obstacle avoidance constraint is:
½xðtÞ2x2=a 1 ½yðtÞ2y2=b 2 1 # 0
ð11:21Þ
466
Introduction to Mobile Robot Control

The difficulty of this technique is that the constraints (11.20) and (11.21)
depend on the parameter t, whereas the optimization problem requires their
expression in terms of θ. One way to face this difficulty is to apply
Eq. (11.21) for a certain number of values (e.g., equidistant values) of t. This
is done in the simulation results that follow, where the optimization was per-
formed with the aid of Matlab’s optimization toolbox (function “constr”). The
input data used are:
ðx0; y0Þ 5 ð0; 0Þ;
φ0 5 0;
ðxf; yfÞ;
φf;
ω 5 π
Case 1 Obstacle-free space with n 5 2, ω 5 π
Initial parameter value θ0 5 ½0 0 0 0 0 0 0 0T
Final position/orientation xf 5 9, yf 5 1, φf 5 3π=4
The optimal value of J obtained is J0 5 14:28. The resulting trajectory
fxðtÞ; yðtÞg and curvature κðtÞ are shown in Figure 11.24 [13].
We see that the curvature does not take large values. Thus, this trajectory may
be appropriate for a WMR with nonholonomic or hard constraints (e.g., constraints
on the steering angle).
Case 2 Obstacle nonfree space with n 5 2; ω 5 π
Here:
ðx0; y0Þ 5 ð0; 0Þ;
ðxf; yfÞ 5 ð8; 4Þ;
φ0 5 φf 5 0
The initial parameter value is again θ0 5 ½0 0 0 0 0 0 0 0T.
The resulting trajectories for an elliptic and hexagonal obstacle are shown in
Figure 11.25.
0
1
2
3
4
5
6
–1
–2
–3
–4
0
2
4
6
8
10
y
x
(A)
Trajectory
0
0.5
1
1.5
2
2.5
3
–0.5
–1
–1.5
–2
0
0.2
0.4
0.6
0.8
1
κ (t)
t
(B)
Curvature
Figure 11.24 Obstacle-free case ðn 5 2; ω 5 πÞ. (A) Trajectory and (B) curvature.
467
Mobile Robot Path, Motion, and Task Planning

The trajectory length for the elliptic obstacle is L 5 10:37 and for the hexagonal
L 5 8:75. In both cases, the obstacle-avoiding path is shown by the continuous line,
while the obstacle-free path is shown by the dotted line.
Example 11.3
Examine the motion planning problem in the joint space for a single joint of a fixed or
mobile robot to go from an initial angular position θ0 at time t 5 0, to a final position θf
at t 5 tf. No obstacle is present in the robot’s configuration space.
Solution
Each joint of the robot is driven (actuated) by a motor (see Example 5.1). The simplest
way to plan the motion is to consider that the acceleration and deceleration periods of
the motor are equal. We assume that at t 5 0, the motor is stationing (zero velocity and
zero acceleration). At time t 5 01, we apply a constant acceleration 1a0, and we assume
that the maximum velocity allowed is vmax. When reaching the maximum velocity at time
t1, the motor is moving with constant speed up to time tf 2 t1, and then decelerates with
deceleration 2a0. During the acceleration period ½0; t1 5 2T, we have:
θðtÞ 5 1
2 a0t2;
θ1 5 θðt1Þ 5 1
2 a0t2
1;
vmax 5 at1
During the constant velocity period ½t1; t2 5 tf 2 t1, we have:
θðtÞ 5 θ1 1 vmaxðt 2 t1Þ;
θ2 5 θðt2Þ 5 θ1 1 vmaxðt2 2 t1Þ
Finally, during the deceleration period ½t2; tf we have:
θðtÞ 5 θ2 1 vmaxðt 2 t2Þ 2 1
2 a0ðt2t2Þ2
x x
x
x
x
x
x
x
x x
0
2
4
6
8
–6
–4
–2
0
–1
–2
2
1
3
4
Trajectory
1
0
2
3
4
4
6
8
10
–2
0
2
x
x
x
x
x
x
Trajectory
(A)
(B)
y
y
x
x
Figure 11.25 Path generation for obstacle avoidance. (A) Elliptic obstacle and
(B) hexagonal obstacle.
468
Introduction to Mobile Robot Control

The final position θf 5 θðtfÞ is equal to:
θf 5 θ2 1 vmaxðtf 2 t2Þ 2 1
2 a0ðtf2t2Þ2
5 θ2 1 vmaxt1 2 1
2 a0t2
1 5 θ2 2 θ1 1 vmaxt1
5 vmaxðt2 2 t1Þ 1 vmaxt1 5 vmaxt2
The time t1 (switching from constant acceleration 1a0 to zero acceleration) and the
time t2 (switching from zero acceleration to deceleration 2a0) are given by:
t1 5 vmax=a0
and
t2 5 θf=vmax
ð11:22Þ
The total time of motion is equal to:
tf 5 t1 1 t2 5 vmax
a0
1 θf
vmax
ð11:23Þ
Given a0, vmax, and θf, the switching times t1 and t2 can be automatically computed
using the above formulas (11.22). Figure 11.26 shows the acceleration, velocity, and posi-
tion plots for the constant acceleration/deceleration case.
θf
θ(t)
θ2
θ1
0
t
t
t
v(t)
vmax
α
α0
−α 0
t1
t2
tf
0
0
Figure 11.26 Typical plots for the point-to-point
motion planning case.
469
Mobile Robot Path, Motion, and Task Planning

11.6
Mobile Robot Task Planning
11.6.1
General Issues
Robot task planning belongs to the general planning problem of artificial intelli-
gence, called AI planning [612]. AI planning is a process of generating possible
multistage solutions to a specific problem. It is based on partitioning a problem in
smaller, simpler, quasi-independent steps starting with an initial situation (state)
and reaching a specified goal state by performing only the allowed movement steps
along the solution path. This is equivalent to the movement through the solution
space using the permitted state transformation operators. Thus, we can say that AI
planning is a specific search process in the solution space. Basic concepts of AI
planning are the following:
G
State space: A space that involves all possible situations (states) that may occur. For
example, a state could represent the position and orientation of a robot, the position and
velocity of a car, and so on. The state space may be discrete (finite or countably infinite)
or continuous (uncountably infinite).
G
Actions or operators: A plan generates actions (operators) that manipulate the state. The
terms actions and operators are used indistinguishably in AI, control, and robotics. The
planning formulation must include a specification of how the state changes when actions
(or controls) are applied. In discrete time, this may be defined as a state-valued function,
and in continuous time as an ordinary differential equation. In some cases, actions could
be selected by nature and are not under the control of the decision maker.
G
Initial and goal states: A planning problem must involve some initial state, and specify
the goal state in a set of goal states. The actions must be selected so as to drive the sys-
tem from the initial state to the desired goal state.
G
Criterion: This encodes the desired outcome of a plan in terms of the state and actions
that are performed. The two basic criteria used in planning are feasibility (i.e., find a plan
that causes the arrival at a goal state, regardless of its efficiency) and optimality (i.e., find
a feasible plan that optimizes the system performance in some desired way, in addition to
arriving at a goal state). Achieving optimality is much more challenging than simply
assuring feasibility.
G
Plan: A plan imposes a specific strategy or behavior on a decision maker. It may simply
specify a sequence of actions to be taken, independently of how complicated this
sequence is.
G
Algorithm: This is difficult to be precisely and uniquely defined. In theoretical computer
science, algorithm is a Turing machine (i.e., a finite-state machine with a special head
that can read and write along an infinite piece of tape). Using the Turing machine as a
model for algorithms implies that the physical world must be first properly modeled and
written on tape before the algorithm can make decisions. But if changes take place in the
world during the execution of the algorithm, it is not clear what may happen (e.g., in case
a mobile robot is moving in a cluttered environment where humans are walking around).
Thus, an online algorithm model is more appropriate in these cases (which relies on spe-
cial sensory information).
G
Planner: A planner simply produces a plan and may be a machine or a human. If the
planner is a machine, it is generally considered to be a planning algorithm (a Turing
machine or an online program). In many cases, the planners are humans who develop
470
Introduction to Mobile Robot Control

plan(s) that can work in all situations. The three ways of employing a generated plan are:
execution by a machine (e.g., a robot) or a simulator, (ii) refinement (i.e., improvement to
a better plan), and (iii) hierarchical inclusion (i.e., packaging it as an action in a higher
level plan).
Representative examples of robot task planning problems are the following:
G
Automotive and other assembly planning
G
Sealing cracks in automotive assembly
G
Parking cars and trailers
G
Mobile robot navigation
G
Mobile manipulator service planning
Questions that have to be addressed in task planning are: What is a plan? How
is a plan represented? How is it computed? What is expected to achieve? How is
its quality evaluated? and Who or what is going to use it? Answers to these ques-
tions can be found in AI textbooks.
11.6.2
Plan Representation and Generation
11.6.2.1
Plan Representation
Plans can be represented and generated as:
G
State-space problems
G
Action-ordering problems
State-Space Representation
It is very familiar in systems theory and automata theory. When applied to plan
representation, the state vectors represent the states of the application domain of
the plan and the actions or operators. The state-space representation is the best way
to recognize a solution of the problem at hand. In addition, state-space representa-
tions can be easily converted to equivalent computer programs, and are directly
usable when searching algorithms are applied to problem solving. In this case, the
solution to a problem in state space provides a path that connects the initial and
goal state in a feasible way.
Clearly, the planning process in the state space is actually a search process,
driven by the requirement of achieving a goal (i.e., it is essentially a goal-driven
search process). Nodes of search trees can be viewed as possible states of the world
presented as some possible partial plans, and the goal pursuing process, as a
partial-plan search process.
Action-Ordering Representation
This representation involves a number of actions in the order in which they should
be executed, along with the associated restrictions to be compiled. For example, it
might be a simple list of actions under execution in the order they appear on the
list. The action-ordering representation focuses on actions to be performed and not
on conditions that could be affected by the individual actions. It is therefore a plan
471
Mobile Robot Path, Motion, and Task Planning

representation very different than the state-space representation. Actually, no states
outside the list are used for problem representation. Essentially, action-ordering
representation is a behavioral representation, because it contains no explicit notion
of state, although one can associate with each action-ordering plan a certain num-
ber of possible states to be generated by the actions. Two major advantages of
action-ordering plan representation are as follows:
1. Capability to represent parallel activities, which is very important when the action-
ordering list is only partially ordered leaving room for reordering actions.
2. Easy implementation in a computer (e.g., as a directed graph structure), where the nodes
represent the actions and the arcs the corresponding ordering relations.
Partially ordered plans allow parallel actions, and are called parallel plans.
Totally, ordered plans require sequential actions, and are called serial plans. Serial
plans are equivalent to state-space transition diagrams with single-operator labeled
arcs. Alternative methods for formulating robotic planning problems are predicate
calculus, tick lists, and triangle tables.
11.6.2.2
Plan Generation
As we saw before, planning is a problem of performing a search through a state
space. At each planning stage, several computational steps are required before its
execution. The computations provide a selection guide for the next move in the
search space, that is, for the path construction that leads from the initial state to the
goal state. Each plan node in the state space involves partial-plan information, and
so the full set of nodes between the initial and the goal state represents the total
plan. By searching through possible plan states, the planner generates the plan. A
basic issue in planning is to select a suitable modeling method or knowledge repre-
sentation describing the task domain. Some methods commonly used are as
follows:
G
Production rules (forward/backward)
G
Predicate logic
G
Procedural nets
G
Object or schema-based methods
G
AND/OR graphs
G
Petri nets
Usually, if the problem is too complicated and the state space to be searched is
very large, the total planning problem is decomposed into a number of simple sub-
processes that can be performed separately. When solving a planning problem, the
planning system must take into account the existing constraints which must be
properly formulated, propagated, and satisfied.
Constraint formulation helps in specifying the interactions among individual
solutions to different goals. Constraint propagation creates new constraints out
of old ones and helps to refine the solution path in screening its nonpromising
links. Finally, constraint satisfaction must be assured by the generated plan.
472
Introduction to Mobile Robot Control

The least-commitment method helps to refine only those parts of the plan that will
not be abandoned later.
The plan generation methods so far discussed produce plans that are linear
sequences of complete subplans and are known as linear planning. On the contrary,
nonlinear planners can generate plans whose individual subplans, needed to
achieve the given conjunctive goals, are generated in parallel.
11.6.3
World Modeling, Task Specification, and Robot Program
Synthesis
We now discuss a little more the three phases of robot task planning, namely,
world modeling, task specification, and program synthesis.
11.6.3.1
World Modeling
World modeling is concerned with the geometric and physical description of the
objects in the workspace (including the robot itself) and the representation of the
assembly state of the objects. A geometric model provides the spatial information
(dimensions, volume, shape, etc.) of the object in the workspace. The typical
method used for modeling 3D objects is the constructive solid geometry (CSG)
where the objects are defined as combinations or constructions via regularized set
operations (union, intersection, etc.) of primitive objects (cube, parallelpiped, cylin-
der). The primitives can be represented in many ways, such as:
G
A set of points and edges
G
A set of surfaces
G
Generalized cylinders
G
Cell decomposition
G
A procedure name that calls to other procedures representing other objects.
In the AUTOPASS assembly planner, the world state is represented by a graph,
the nodes of which represent objects and edges represent relationships, such as [6]:
G
Attachment (rigid, nonrigid, or conditional attachment of objects)
G
Constraints (relationships that represent physical constraints, translational or rotational,
between objects)
G
Assembly component (which indicates that the subgraph linked by this edge is an assem-
bly part that can be referenced as an object).
11.6.3.2
Task Specification
This can be done using a high-level specification language. For example, an assem-
bly task can be described as a sequence of states of the world model. The states
must be provided by the configurations of all the objects in the workspace (as it
will be done later in Example 11.4). Another way of task description is to use a
sequence of symbolic operations on the objects, including proper spatial constraints
to eliminate possible ambiguities. Most robot-oriented languages use this type of
473
Mobile Robot Path, Motion, and Task Planning

task specification. AUTOPASS uses this kind of specification, but it uses a more
detailed syntax, which divides its assembly related statements into three groups:
1. State change statement (i.e., an assembly operation description)
2. Tools statement (i.e., description of the kind of tools that must be used)
3. Fastener statement (i.e., description of fastening operation)
11.6.3.3
Robot Program Synthesis
This is the most difficult and important phase of task planning. The major steps in
this phase are as follows:
G
Grasp planning
G
Motion planning
G
Plan checking
Grasp planning specifies how the way the object is grasped affects all subsequent
operations. The way the robot can grasp an object depends on the geometry of the
object being grasped and the other objects present in the workspace. To be useful, a
grasping configuration must be feasible and stable. The robot must be able to reach
the object without any collision with other objects in the workspace. Also, the object
once grasped must be stable during subsequent operations of the robot.
Motion planning specifies how the robot must move the object to its destination
and complete the operation. The steps for successful motion planning are as follows:
G
Guarded departure from the current configuration
G
Collision-free motion to the desired configuration
Plan checking is made to ensure that the plan accomplishes the desired sequence
of tasks, and if not to attempt an alternative plan.
Example 11.4 (Blocks world problem)
We are given three blocks A, B, and C in the initial configuration shown in Figure 11.27A.
The problem is for the robotic manipulator to move the blocks to the final configuration
shown in Figure 11.27B through the application of the following forward production rules
(F-rules):
1. pick up (X)
2. put down (X)
B
C
A
C
B
A
(A)
(B)
Figure 11.27 A “blocks world” problem.
(A) Initial state and (B) final state.
474
Introduction to Mobile Robot Control

3. stack (X,Y)
4. unstack (X,Y)
that represent possible robot actions.
Solution
The initial block configuration (arrangement) can be represented by the conjunction of
the following statements:
clear (B)
Block B has a clear top
clear (A)
Block A has a clear top
on (A,C)
Block A is on block C
on table (B)
Block B is on the table
on table (C)
Block C is on the table
handempty
The robot hand is empty
Robot actions change one state, or arrangement, of the world to another. Given that
the
final
state
is
to
be
achieved
using
the
above
four
F-rules,
the
triangle
table representation of the planning process is as shown in Figure 11.28.
Each column of the table describes the new situation after an operator (action) (e.g.,
unstuck, put down) has been carried out, while handempty and handholding mean that the
clear (A)
on (A,C)
ontable (B) 
clear (B)
ontable (C)
clear (C)
holding (A)
clear (A)
holding (B)
handempty
clear (B)
handempty
on (B,A)
holding (C)
on (C,B)
0  Hand 
empty
1  
Unstack (A,C)
2  
put down (A)
3  
pick up (B)
4  
stack (B,A)
5  
pick up (C)
6  
stack  (C,B)
Figure 11.28 The triangle table representation of the planning process that solves the
problem of Figure 11.15 (the numbers above the columns indicate the sequence of robot
actions).
475
Mobile Robot Path, Motion, and Task Planning

robot hand is empty or it is holding a block, respectively. Each column represents the pre-
conditions of the next F-rule to be applied.
In our case, the plan sequence of F-rules is the following:
G handempty
G unstack (A,C)
G put down (A)
G pick up (B)
G stack (B,A)
G pick up (C)
G stack (C,B)
with the preconditions shown in the previous (left) column of each F-rule being applied.
References
[1] Latombe JC. Robot motion planning. Boston, MA: Kluwer; 1991.
[2] Lozano-Perez T. Spatial planning: a configuration space approach. IEEE Trans
Comput 1983;32(2):10820.
[3] Erdmann M, Lozano-Perez T. On multiple moving obstacles. Algorithmica 1987;2
(4):477521.
[4] Fugimura K. Motion planning in dynamic environments. Berlin/Tokyo: Springer; 1991.
[5] Khatib O. Real-time obstacle avoidance for manipulators and mobile robots. Int J
Robot Res 1986;5(1):908.
[6] Lieberman LL, Wesley M. AUTOPASS: an automatic programming system for com-
puter controlled mechanical assembly. IBM J Res Dev 1977;32133.
[7] Homemde Mello LS, Sanderson AC. A correct and complete algorithm for the gener-
ation of mechanical assembly sequences. IEEE Trans Robot Autom 1990;7
(2):22840.
[8] Sheu PCY, Xue Q. Intelligent robotic systems. Singapore/London: World Scientific
Publishers;1993.
[9] Pearl J. Heuristics: intelligent search strategies for computer problem solving. Reading,
MA: Addison-Wesley;1984.
[10] Bonert M. Motion planning for multi-robot. Ottawa: National Library of Canada;1999.
[11] LaValle SM. Planning algorithms. Cambridge: Cambridge University Press;2006.
[12] Popovic D, Bhatkar VP. Methods and tools for applied artificial intelligence. New
York, NY: Marcel Dekker;1994.
[13] Gallina P, Gasparetto A. A technique to analytically formulate and solve the
2-dimensional constrained trajectory planning for a mobile robot. J Intell Robot Syst
2000;27(3):23762.
[14] Lozano-Pe´rez T, Jones JL, Mazers E, O’Donnel P. Task-level planning of pick-and-
place robot motions. IEEE Comput 1989;March:219.
[15] Hatzivasiliou FV, Tzafestas SG. A path planning method for mobile robots in a struc-
tured environment. In: Tzafestas SG, editor. Robotic systems: advanced techniques and
applications. Dordrecht/Boston: Kluwer;1992.
[16] Kant K, Zuckler S. Toward efficient trajectory planning: the path velocity decomposi-
tion. Int J Robot Res 1986;5:7289.
476
Introduction to Mobile Robot Control

[17] Canny J. The complexity of robot motion planning. Cambridge, MA: MIT Press;1988.
[18] Garcia E, De Santos PG. Mobile-robot navigation with complete coverage of unstruc-
tured environment. Robot Auton Syst 2004;46:195204.
[19] Russel S, Norwig P. Artificial intelligence: a modern approach. Upper Saddle River,
NJ: Prentice Hall;2003.
[20] Stentz A. Optimal and efficient path planning for partially known environments.
Proceedings of IEEE conference on robotics and automation. San Diego, CA; May
1994. p. 331017.
[21] Amato N. Randomized motion planning, Part 1, roadmap methods, course notes.
University of Padova;2004.
[22] Welzl E. Constructing the visibility graph for n line segments in O(n2) time. Inf
Process Lett 1985;20:16171.
[23] Kumar V, Zefran M, Ostrowski J. Motion planning and control of robots.In:Nof S. edi-
tor.Handbook of Industrial Robotics. New York: Wiley and Sons:1999, p.295315.
[24] Wang Y, Chirikjian GS. A new potential field method for robot path planning.
Proceedings of 2000 IEEE conference robotics and automation. San Francisco, CA;
April 2000. p. 97782.
[25] Pimenta LCA. Robot navigation based on electrostatic field computation. IEEE Trans
Magn 2006;42(4):145962.
[26] Engedy I, Horvath G. Artificial neural network based local motion planning of a
wheeled mobile robot. Proceedings of 11th international symposium on computational
intelligence and informatics. Budapest, Hungary; November 2010. p. 21318.
[27] Safadi H. Local path planning using virtual potential field. Report COMP 765: spatial
representation and mobile robotics—project. School of Computer Science, McGill
University, Canada; April 2007.
[28] Ding FG, Jiang P, Bian XQ, Wang HJ. AUV local path planning based on virtual
potential field. Proceedings of IEEE international conference on mechatronics and
automation. Niagara Falls, Canada; 2005. 4, p. 171116.
[29] Koren Y, Borenstein J. Potential field methods and their inherent limitations for mobile
robot navigation. Proceedings of IEEE conference on robotics and automation.
Sacramento, CA; April 2005. p. 13981404.
[30] Borenstein J, Koren Y. The vector field histogram: fast obstacle avoidance for mobile
robots. IEEE J Robot Autom 1991;7(3):27888.
[31] Ulrich I. Borenstein journal of VFH: local obstacle avoidance with look-ahead verifi-
cation. Proceedings of IEEE international conference on robotics and automation. San
Francisco, CA; May 2000.
[32] Wang LC, Yong LS, Ang Jr MR. Hybrid of global path planning and local navigation
implemented on a mobile robot in indoor environment. Proceedings of IEEE interna-
tional symposium on intelligent control. October 2002. p. 82126.
[33] Garrido S, Moreno L, Blanco D, Jurewicz P. Path planning for mobile robot navigation
using Voronoi diagram and fast marching. Int J Robot Autom 2011;2(1):4264.
[34] Sethian JA. Theory, algorithms, and applications of level set methods for propagating
interfaces. Acta Numerica, Cambridge: Cambridge University Press; 1996. p. 30995.
[35] Sethian JA. Level set methods. Cambridge: Cambridge University Press;1996.
[36] Garrido S, Moreno L, Blanco D. Exploration of a cluttered environment using Voronoi
transform and fast marching. Robot Auton Syst 2008;56(12):106981.
[37] Arney T. An efficient solution to autonomous path planning by approximate cell
decomposition. Proceedings of international conference on information and automation
for sustainability (ICIAFS 07). Colombo, Sri Lanka; December 46, 2007. p. 8893.
477
Mobile Robot Path, Motion, and Task Planning

[38] Coenen FP, Beattle B, Shave MJR, Bench-Capon TGM, Diaz GM. Spatial reasoning
using the quad tesseral representation. J Artif Intell Rev 1998;12(4):32143.
[39] Katevas NI, Tzafestas SG, Pnevmatikatos CG. The approximate cell decomposition
with local node refinement global path planning method: path nodes refinement and
curve parametric interpolation. J. Intell Robot Syst 1998;22:289314.
[40] Olunloyo VOS, Ayomoh MKO. Autonomous mobile robot navigation using hybrid vir-
tual force field concept. Eur J Sci Res 2009;31(2):20428.
[41] Katevas NI, Tzafestas SG. The active kinematic histogram method for path planning of
non-point
non-holonomically
constrained
mobile
robots.
Adv
Robot
1998;12
(4):37595.
[42] Katevas NI, Tzafestas SG, Matia F. Global and local strategies for mobile robot navi-
gation. In: Katevas N, editor. Mobile robotics in healthcare. Amsterdam, the
Netherlands: IOS Press;2001.
[43] Jarvis R. Intelligent robotics: past, present and future. Int J Comput Sci Appl 2008;5
(3):2335.
[44] Taylor T, Geva S, Boles WW. Directed exploration using modified distance transform.
Proceedings of international conference on digital imaging computing: techniques and
applications. Cairus, Australia; December 2012. p. 20816.
[45] Zelinsky A, Jarvis RA, Byrne JC, Yuta S. Planning paths of complete coverage of an
unstructured environment by a mobile robot. Proceedings of international symposium
on advanced robotics. Tokyo, Japan; November 1993.
[46] Zelinsky A, Yuta S. A unified approach to planning, sensing and navigation for mobile
robots. Proceedings of international symposium on experimental robotics. Kyoto,
Japan; October 1993.
[47] Kimoto K, Yuta S.A Simulator for programming the behavior of an autonomous
sensor-based mobile robot. Proceedings of international conference on intelligent
robots and systems (IROS ’92). Raleigh, NC; July 1992.
[48] Choi Y-H, Lee T-K, Baek S-H, Oh S-Y. Online complete coverage path planning for
mobile robots based on linked spiral paths using constrained inverse distance trans-
form. Proceedings of IEEE/RSJ international conference on intelligent robots and sys-
tems. St. Louis, MO; 2009. p. 56885712.
[49] Murray RM, Sastry SS. Nonholonomic motion planning: steering using sinusoids.
IEEE Transactions on Automatic Control 1993;38(5):700715.
478
Introduction to Mobile Robot Control

12 Mobile Robot Localization and
Mapping
12.1
Introduction
As described in Section 11.3.1, localization and mapping are two of the three basic
operations needed for the navigation of a robot. Very broadly, other fundamental
capabilities and functions of an integrated robotic system from the task/mission
specification to the motion control/task execution (besides path planning) are the
following:
G
Cognition of the task specification
G
Perception of the environment
G
Control of the robot motion
A pictorial illustration of the interrelations and organization of the above func-
tions in a working wheel-driven mobile robot (WMR) is shown in Figure 12.1.
The robot must have the ability to perceive the environment via its sensors in
order to create the proper data for finding its location (localization) and determin-
ing how it should go to its destination in the produced map (path planning). The
desired destination is found by the robot through processing of the desired task/
mission command with the help of the cognition process. The path is then provided
as input to the robot’s motion controller which drives the actuators such that the
robot follows the commanded path. The path planning operation was discussed in
Chapter 11. Here we will deal with the localization and map building operations.
Specifically, the objectives of this chapter are as follows:
G
To provide the background concepts on stochastic processes, Kalman estimation, and
Bayesian estimation employed in WMR localization
G
To discuss the sensor imperfections that have to be taken care in their use
G
To introduce the relative localization (dead reckoning) concept, and present the kinematic
analysis of dead reckoning in WMRs
G
To study the absolute localization methods (trilateration, triangulation, map matching)
G
To present the application of Kalman filters for mobile robot localization, sensor calibra-
tion, and sensor fusion
G
To treat the simultaneous localization and mapping (SLAM) problem via the extended
Kalman filter (EKF), Bayesian estimation, and particle filter (PF)
Introduction to Mobile Robot Control. DOI: http://dx.doi.org/10.1016/B978-0-12-417049-0.00012-2
© 2014 Elsevier Inc. All rights reserved.

12.2
Background Concepts
Stochastic modeling, Kalman filtering, and Bayesian estimation techniques are
important tools for robot localization. Here, an introduction to the above concepts
and techniques, which will be used in the chapter, is provided [13].
12.2.1
Stochastic Processes
Stochastic process is defined to be a collection (or ensemble) of time functions
Xðh; tÞ of random variables corresponding to an infinitely countable set of
experiments h, with an associated probability description, for example, pðx; tÞ
(see Figure 12.2). At time t1, Xðt1Þ is a random variable with probability pðx; t1Þ.
X
0
Sample
functions
t1
X(t1)
X(t2)
t2
h1
h2
h3
hm
t
Figure 12.2 Ensemble representation of a stochastic process.
Cognition and
path planning
Motion
control
Sensing and
perception 
Localization and
map building 
Environment
Desired task/mission
Path
Actuators
Sensors
Perceived
data
Position
map 
Where am
I ? 
Figure 12.1 General organization of
the functions of an autonomous mobile
robot.
480
Introduction to Mobile Robot Control

Similarly, Xðt2Þ is a random variable with probability pðx; t2Þ. Here, t1 is a random
variable over the ensemble every time, and so we can define first and higher-order
statistics for the variables t1; t2; . . ..
First-order statistics is concerned only with a single random variable XðtÞ and is
expressed by a probability distribution Pðx; tÞ and its density pðx; tÞ 5 dPðx; tÞ=dt for
the continuous-time case, or its distribution function Pðxi; tÞ for the discrete-time
case.
Second-order statistics is concerned with two random variables Xðt1Þ and Xðt2Þ
at two distinct time instances t1 and t2. In the continuous-time case, we have the
following probability functions for xðt1Þ 5 x1 and xðt2Þ 5 x2.
Pðxðt1Þ; xðt2Þ:t1; t2Þ
ðJoint distributionÞ
pðxðt1Þ; xðt2Þ:t1; t2Þ 5 dP=dx1dx2
ðJoint densityÞ
pðx2; t2Þ 5
ðN
2N
pðx1; x2:t1; t2Þdx1
ðMarginal densityÞ
pðx1; t1jx2; t2Þ 5 pðx1; x2:t1; t2Þ=pðx2; t2Þ
ðConditional densityÞ
Using the above probability functions we get the first- and second-order
averages (moments) as follows:
xðtÞ 5 E½XðtÞ 5
ðN
2N
xpðx; tÞdx
ðMean valueÞ
Rxxðt1; t2Þ 5
ðN
2N
ðN
2N
x1x2pðx1; x2:t1; t2Þdx1dx2
ðAutocorrelationÞ
Cxxðt1; t2Þ 5 Eð½xðt1Þ 2 xðt1Þ½xðt2Þ 2 xðt2ÞÞ
5 Rxxðt1; t2Þ 2 xðt1Þxðt2Þ
ðAutocovarianceÞ
Cxxðt; tÞ 5 σ2
x
ðAutocovarianceÞ
The sample statistics time averages of order n over the sample functions of the
continuous-time process XðtÞ are defined as:
hXni 5 lim
T!N
1
2T
ð1T
2T
XðtÞndt
and the time averages of the discrete-time process Xi as:
E½Xn
i  5 lim
N!N
X
N
i51
Xn
i PxðxiÞ
481
Mobile Robot Localization and Mapping

Stationarity: A stochastic process is said to be stationary if all its marginal and
joint density functions do not depend on the choice of the time origin. If this is not
so, then the process is called nonstationary.
Ergodicity: A stochastic process is said to be ergodic if its ensemble moments
(averages) are equal to its corresponding sample moments, that is, if:
E Xn
½
 5
ðN
2N
xnpðxÞdx 5 lim
T!N
1
2T
ðT
2T
X tð Þndt 5 hXni
Ergodic stochastic processes are always stationary. The converse does not
always hold.
Stationarity and Ergodicity in the Wide Sense: Motivated by the normal
(Gaussian) density, which is completely described by its mean and variance, in
practice we usually employ only first- and second-order moments. If a process is
stationary or ergodic up to second-order moments, then it is called a stationary or
ergodic process in the wide sense.
Markov Process: A Markov process is the process in which for t1 . t2?. tn the
following property (called Markovian property) is true:
ProbfXðt1Þ # x1jXðt2Þ 5 x2; . . .; XðtnÞ 5 xng
5 ProbfXðt1Þ # x1jXðt2Þ 5 x2g
ð12:1Þ
This means that the p.d.f. of the state variable Xðt1Þ at some time instant t1
depends only on the state Xðt2Þ at the immediately previous time t2 (not on more
previous times).
12.2.2
Stochastic Dynamic Models
We consider an n-dimensional linear discrete-time dynamic process described by:
xk11 5 Akxk 1 Bkwk;
xkARn;
wkARr
ð12:2aÞ
zk 5 Ckxk 1 vk;
vkARm
ð12:2bÞ
where Ak, Bk, Ck are matrices of proper dimensionality depending on the discrete-
time index k, and wk, vk are stochastic processes (the input disturbance and mea-
surement noise, respectively) with properties:
E½wk 5 0;
E½vk 5 0
E½wkwT
j  5 Qkδkj;
E½vkvT
j  5 Rkδkj
E½wkvT
j  5 0
ð12:3Þ
where δkj is the Kronecker delta defined as δkk 5 1, δkj 5 0 ðk 6¼ jÞ.
482
Introduction to Mobile Robot Control

The initial state x0 is a random (stochastic) variable, such that:
E½vkxT
0 5 E½wkxT
0 5 E½wkvT
j  5 0
ð12:4Þ
The above properties imply that the processes wk and vk and the random variable
x0 are statistically independent. If they are also Gaussian distributed, then the model
is said to be a discrete-time GaussMarkov model (or GaussMarkov chain), since
as can be easily seen, the process fxkg is Markovian. The continuous-time counterpart
of the GaussMarkov model has an analogous differential equation representation.
12.2.3
Discrete-Time Kalman Filter and Predictor
We consider a discrete-time GaussMarkov system of the type described by
Eqs. (12.2)(12.4), with available measurements fzð1Þ; zð2Þ; . . .; zðkÞg. Let ^xðk 1 1jkÞ
and ^xðk 1 1jk 1 1Þ be the estimate of xðk 1 1Þ with measurements up to zðkÞ and
zðk 1 1Þ, respectively. Then, the discrete-time Kalman filter for the stochastic sys-
tem (12.2a) with measurement process (12.2b) is described by the following recur-
sive equations:
^xðk 1 1jk 1 1Þ 5 AðkÞ^xðkjkÞ 1 Kðk 1 1Þ zðk 1 1Þ 2 Cðk 1 1ÞAðkÞ^xðkjkÞ
½

ð12:5Þ
Kðk 1 1Þ 5 Σðk 1 1jkÞCTðk 1 1Þ½Cðk 1 1ÞΣðk 1 1jkÞCTðk 1 1Þ 1 Rðk 1 1Þ21
ð12:6Þ
Σðk 1 1jk 1 1Þ 5 Σðk 1 1jkÞ 2 Kðk 1 1ÞCðk 1 1ÞΣðk 1 1jkÞ
ð12:7Þ
Σðk 1 1jkÞ 5 AðkÞΣðkjkÞATðkÞ 1 BðkÞQðkÞBTðkÞ; Σð0j0Þ 5 Σ0
ð12:8Þ
where ΣðjjiÞ is the covariance matrix of the estimate ^xðjÞ on the basis of data up to
time i:
ΣðkjkÞ 5 E½~xðkjkÞ~xTðkjkÞ
ð12:9aÞ
Σðk 1 1jkÞ 5 E½~xðk 1 1jkÞ~xTðk 1 1jkÞ
~xðk 1 1jkÞ 5 xðk 1 1Þ 2 ^xðk 1 1jkÞ
ð12:9bÞ
and the notation zðkÞ 5 zk, QðkÞ 5 Qk, etc. is used. These equations can be repre-
sented in the block diagram (Figure 12.3).
An alternative expression of Kðk 1 1Þ is obtained by applying the matrix inver-
sion Lemma, and is the following:
Kðk 1 1Þ 5 Σðk 1 1jk 1 1ÞCTðk 1 1ÞR21ðk 1 1Þ
ð12:10Þ
483
Mobile Robot Localization and Mapping

The initial conditions for the state estimate Eq. (12.5) and the covariance equa-
tion (12.7) are:
^xð0j0Þ 5 0; Σð0j0Þ 5 Σ0
ðpositive definiteÞ
ð12:11Þ
State Prediction: Using the filtered estimate ^xðkjkÞ of xðkÞ at the discrete-time
instant k, we can compute a predicted estimate ^xðjjkÞ, j . k, k 5 0; 1; 2; . . . of the
state xðjÞ on the basis of measurements up to the instant k. This is done by using
the equations:
^xðk 1 1jkÞ 5 AðkÞ^xðkjkÞ
ð12:12aÞ
^xðk 1 2jkÞ 5 Aðk 1 1Þ^xðk 1 1jkÞ 5 Aðk 1 1ÞAðkÞ^xðkjkÞ
ð12:12bÞ
^xðk 1 3jkÞ 5 Aðk 1 2Þ^xðk 1 2jkÞ 5 Aðk 1 2ÞAðk 1 1ÞAðkÞ^xðkjkÞ
ð12:12cÞ
and so on. Therefore:
^xðjjkÞ 5 Aðj 2 1ÞAðj 2 2Þ. . .Aðk 1 1ÞAðkÞ^xðkjkÞ;
j . k
ð12:13Þ
12.2.4
Bayesian Learning
Formally, the probability of event A occurring, given that the event B occurs, is
given by:
ProbðAjBÞ 5 ProbðA and BÞ
ProbðBÞ
and similarly:
ProbðBjAÞ 5 ProbðA and BÞ
ProbðAÞ
z(k+ 1)
+
–
K(k+1)
C(k + 1)
A(k+1)
A( j–1)
x(j⏐k)
Iq–1
A(k)
–
∑
∑
Predictor
x(k+1⏐k+1)
+
Figure 12.3 Block diagram representation of the Kalman filter and predictor (I 5 unit
matrix, q21 is the delay operator q21xðk 1 1Þ 5 xðkjkÞ).
484
Introduction to Mobile Robot Control

For notational simplicity, the above formulas are written as:
PðAjBÞ 5 PðA; BÞ
PðBÞ ;
PðBjAÞ 5 PðA; BÞ
PðAÞ
ð12:14Þ
where PðA; BÞ is the probability of A and B occurring jointly.
Combining the two formulas (12.14), we get the so-called Bayes formula:
PðAjBÞ 5 PðBjAÞPðAÞ
PðBÞ
ð12:15aÞ
Now, if A 5 H where H is a hypothesis (about the truth or cause), and B 5 E
where E represents observed symptoms or measured data or evidence, Eq. (12.15a)
gives the so-called Bayes updating (or learning) rule:
PðHjEÞ 5 PðEjHÞPðHÞ
PðEÞ
ð12:15bÞ
where
PðEÞ 5 PðEjHÞPðHÞ 1 PðEjnotHÞPðnotHÞ and PðnotHÞ 5 1 2 PðHÞ.
Here,
PðHÞ is the “prior” probability of the hypothesis H (before any evidence is
obtained), PðEÞ is the probability of the evidence, and PðEjHÞ is the probability
that the evidence is true when H holds. The term PðHjEÞ is the “posterior” proba-
bility (i.e., the “updated” probability) of H after the evidence is obtained. Consider
now the case that two evidences E1 and E2 about H are observed one after the
other. Then, Eq. (12.15b) gives:
PðHjE1Þ 5 PðE1jHÞPðHÞ
PðE1Þ
PðHjE1; E2Þ 5 PðE2jHÞPðHjE1Þ
PðE2Þ
5 PðE2jHÞPðE1jHÞPðHÞ
PðE1ÞPðE2Þ
Clearly, the above equation says that PðHjE1; E2Þ 5 PðHjE2; E1Þ, that is, the
order in which the evidence (data) are observed does not influence the resulting
posterior probability of H given E1 and E2.
Example 12.1
It is desired to derive the Kalman filter using the least-squares estimation approach.
Solution
The Kalman filter described by Eqs.(12.5)(12.11) can be derived using several
approaches, viz., (i) orthogonality principle, (ii) Gaussian Bayesian technique, and (iii) the
least-squares approach (various formulations). Here, the least-squares estimator (3.103)
will be used which was derived for the measurement system (3.102). This estimator uses all
485
Mobile Robot Localization and Mapping

available data y 5 ½y1; y2; . . .; ymT at once, and provides an off-line estimate of ξ. The deri-
vation of the Kalman filter (12.5)(12.11) will be done by converting Eq. (3.103) to recur-
sive form that updates (improves) the estimate ξk (found on the basis of k measurements
y1; . . .; yk) using the next, ðk 1 1Þth, measurement yk11. To this end, we define:
Mk11 5
"
Mk
μT
k11
#
;
yk11 5
"
yk
yk11
#
Σk 5 MT
kMk

21;
Σk11 5 MT
k11Mk11

21
ð12:16aÞ
The matrices Σk11 and Σk are related as follows:
Σk11 5
 Mk
μT
k11
T Mk
μT
k11
21
5

MT
kjμk11
 Mk
μT
k11
21
5 ðMT
kMk1μk11μT
k11Þ21 5 ðΣ21
k 1μk11μT
k11Þ21
Therefore:
Σ21
k
5 Σ21
k11 2 μk11μT
k11
ð12:16bÞ
Now, using Eqs. (3.103) and (12.16a) we get:
^ξk 5 ΣkMT
kyk
^ξk11 5 Σk11ðMT
kyk 1 μk11yk11Þ
To express ^ξk11 in terms of ^ξk we must eliminate MT
kyk from the above equations. The
first of these equations gives MT
kyk 5 Σ21
k ^ξk. Thus, the second equation can be written as:
^ξk11 5 Σk11ðΣ21
k ^ξk 1 μk11yk11Þ
5 Σk11½ðΣ21
k11 2 μk11μT
k11Þ^ξk 1 μk11yk11
5 ^ξk 1 Σk11μk11ðyk11 2 μT
k11 ^ξkÞ
ð12:17aÞ
Equation (12.17a) provides the desired recursive least-squares estimator. The new esti-
mate ^ξk11 is a function of the old estimate ^ξk, the new data pair ½μT
k11; yk11 and the
matrix Σk11. Actually, the new estimate ^ξk11 is equal to the old estimate ^ξk plus a correc-
tion term equal to an adaptation (or learning) gain vector Σk11μk11 multiplied by the pre-
diction (learning) error or innovation process ~yk11 5 yk11 2 μT
k11 ^ξk. For this reason
Eq. (12.17a) is also called “least-squares learning equation or rule.” We now have to find
how Σk11 can be found from Σk.
486
Introduction to Mobile Robot Control

From Eq. (12.16b) we obtain:1
Σk11 5 ðΣ21
k 1μk11μT
k11Þ21
5 Σk 2 Σkμk11ðI1μT
k11Σkμk11Þ21μT
k11Σk
5 Σk 2 Σkμk11μT
k11Σk
1 1 μT
k11Σkμk11
ð12:17bÞ
for k 5 0; 1; 2; . . .; m 2 1 (since μT
k11Σkμk11 is a scalar). The full recursive least-squares
estimation algorithm given by Eqs. (12.17a) and (12.17b) is initiated using selected ini-
tial values Σ0 and ^ξ0. One may also start from k 5 n, by using the first n measurements to
get ^ξn and Σn directly:
Σn 5 ½MT
nMn21; ^ξn 5 ΣnMT
nyn
Now, using in Eqs. (12.17a) and (12.17b) the following variable substitutions:
^ξk11 5 ^xðk 1 1jk 1 1Þ;
^ξk 5 ^xðk 1 1jkÞ 5 AðkÞ^xðkjkÞ;
Σk 5 Σðk 1 1jkÞ;
Σk11 5 Σðk 1 1jk 1 1Þ;
μT
k11 5 Cðk 1 1Þ and yk 5 zðkÞ
we get the Kalman filter Eqs. (12.5)(12.11).
The initial conditions for the state estimate Eq. (12.5) and the covariance equation
(12.7) are:
^xð0j0Þ 5 0; Σð0j0Þ 5 Σ0
ðpositive definiteÞ
To see how the Kalman filter works, we consider a scalar time-invariant discrete-time
GaussMarkov system:
xðk 1 1Þ 5 AxðkÞ 1 wðkÞ
zðkÞ 5 xðkÞ 1 vðkÞ
ðk 5 0; 1; 2; . . .Þ
with A 5 1, Q 5 25, R 5 15, and Σ0 5 100. The Kalman filter Eqs. (12.5)(12.11) give:
^xðk 1 1jk 1 1Þ 5 A^xðkjkÞ 1 Kðk 1 1Þ½zðk 1 1Þ 2 A^xðkjkÞ; ^xð0j0Þ 5 0
Σðk 1 1jkÞ 5 A2ΣðkjkÞ 1 Q
Kðk 1 1Þ 5 ½A2ΣðkjkÞ 1 Q=½A2ΣðkjkÞ 1 Q 1 R
Σðk 1 1jk 1 1Þ 5 R½A2ΣðkjkÞ 1 Q=½A2ΣðkjkÞ 1 Q 1 R; Σð0j0Þ 5 Σ0
1 Using the following matrix inversion lemma: ðA1BCÞ21 5 A21 2 A21BðI1CA21BÞ21CA21 with
A 5 Σ21
k , B 5 μk11, and C 5 μT
k11.
487
Mobile Robot Localization and Mapping

Since ΣðkjkÞ $ 0, the second equation tells us that Σðk 1 1jkÞ $ Q, that is, the one-
step prediction accuracy is at minimum equal to the variance of the input disturbance
wðkÞ. From the third equation, we see that 0 # Kðk 1 1Þ # 1ðk 5 0; 1; 2; . . .Þ. The fourth
equation implies that Σðk 1 1jk 1 1Þ 5 RKðk 1 1Þ. Thus, 0 # Σðk 1 1jk 1 1Þ # R. This
means that if Σð0j0ÞcR, the use of the first measurement zð1Þ gives Σð1j1Þ # R{Σ0,
that is, it provides a drastic reduction in the estimation error. Using the given values
A 5 1, Q 5 25, R 5 15, and Σ0 5 Σð0j0Þ 5 100, the variance equations give the results as
shown in Table 12.1.
The steady-state value of ΣðkjkÞ is found by setting Σðk 1 1jk 1 1Þ 5 ΣðkjkÞ 5 Σ, in
which case we obtain Σ
2 1 25Σ 2 375 5 0. Since Σ $ 0, the acceptable solution is
Σ 5 10:55. Thus, K 5 Kðk 1 1Þ 5 0:703, and
^xðk 1 1jk 1 1Þ 5 ^xðkjkÞ 1 0:703½zðk 1 1Þ 2 ^xðkjkÞ
5 0:297^xðkjkÞ 1 0:703zðk 1 1Þ
ðk 5 4; 5; 6; . . .Þ
12.3
Sensor Imperfections
The primary prerequisite for an accurate localization is the availability of reliable
high-resolution sensors. Unfortunately, the practically available real-life sensors
have several imperfections that are due to different causes. As we saw in Chapter 4,
the usual sensors employed in robot navigation are range finders via sonar, laser and
infrared technology, radar, tactile sensors, compasses, and GPS. Sonars have very
low spatial bandwidth capabilities and are subject to noise due to wave scattering,
and so the use of laser range sensing is preferred. But despite laser sensors possess a
much wider bandwidth, they are still affected by noise. Furthermore, lasers have
a restricted field of view, unless special measures are taken (e.g., incorporation of
rotating mirrors in the design).
Since both sonar-based and laser-based navigation have the above drawbacks,
robotic scientists have strengthened their attention to vision sensors and camera-
based systems. Vision sensors can offer a wide field of view, millisecond sampling
rates, and are easily applicable for control purposes. Some of the drawbacks of
vision are the lack of depth information, image occlusion, low resolution, and the
need for recognition and interpretation. However, despite the relative advantages
Table 12.1 Evolution of the Optimal Filter
k
Σðkjk 2 1Þ
KðkÞ
ΣðkjkÞ
0


100
1
125
0.893
13.40
2
38.4
0.720
10.80
3
35.8
0.704
10.57
4
35.6
0.703
10.55
488
Introduction to Mobile Robot Control

and disadvantages, the use of monocular cameras as a sensor for navigation leads
to a competitive selection.
In general, the sensor imperfections can be grouped in: sensor noise and sensor
aliasing categories [48]. The sensor noise is primarily caused by the environmen-
tal variations that cannot be captured by the robot. Examples of this in vision sys-
tems are the illumination conditions, the blooming, and the blurring. In sonar
systems, if the surface accepting the emitted sound is relatively smooth and angled,
much of the signal will be reflected away, failing to produce a return echo. Another
source of noise in sonar systems is the use of multiple sonar emitters (1640 emit-
ters) that are subject to echo interference effects. The second imperfection of robotic
sensors is the aliasing, that is, the fact that sensor readings are not unique. In other
words, the mapping from the environmental states to the robot’s perceptual inputs is
many-to-one (not one-to-one). The sensor aliasing implies that (even if no noise
exists) the available amount of information is in most cases not sufficient to identify
the robot’s position from a single sensor reading. The above issues suggest that in
practice special sensor signal processing/fusion techniques should be employed to
minimize the effect of noise and aliasing, and thus get an accurate estimate of the
robot position over time. These techniques include probabilistic and information
theory methods (Bayesian estimators, Kalman filters, EKFs, PFs, Monte Carlo
estimators, information filters, fuzzy/neural approximators, etc).
12.4
Relative Localization
Relative localization is performed by dead reckoning, that is, by the measurement of
the movement of a wheeled mobile robot (WMR) between two locations. This is done
repeatedly as the robot moves and the movement measurements are added together to
form an estimate of the distance traveled from the starting position. Since the individ-
ual estimates of the local positions are not exact, the errors are accumulated and the
absolute error in the total movement estimate increases with traveled distance. The
term dead reckoning comes from the sailing days term “deduced reckoning” [4].
For a WMR, the dead reckoning method is called “odometry,” and is based on
data obtained from incremental wheel encoders [9].
The basic assumption of odometry is that wheel revolutions can be transformed
into linear displacements relative to the floor. This assumption is rarely ideally
valid because of wheel slippage and other causes. The errors in odometric measure-
ment are distinguished in:
G
Systematic errors (e.g., due to unequal wheel diameters, misalignment of wheels, actual
wheel base is different than nominal wheel base, finite encoder resolution, encoder sam-
pling rate).
G
Non-systematic errors (e.g., uneven floors, slippery floors, overacceleration, nonpoint
contact with the floor, skidding/fast turning, internal and external forces).
Systematic errors are cumulative and occur principally in indoor environment.
Nonsystematic errors are dominating in outdoor environments.
489
Mobile Robot Localization and Mapping

Two simple models for representing the systematic odometric errors are:
Error due to unequal wheel diameter:
ed 5 dR=dL
Error due uncertainty about the effective wheel base:
eb 5 bactual=bnominal
where dR, dL are the actual diameters of the right and left wheels, respectively, and
b is the wheel base. It has been verified that either ed or eb can cause the same final
error in both position and orientation measurement. To estimate the nonsystematic
errors, we use the worst possible case, that is, we estimate the biggest possible dis-
turbance. A good measure of nonsystematic errors is provided by the average abso-
lute orientation errors eθ clockwise and counterclockwise directions, namely eavrg
θ;cw
and eavrg
θ;ccw, respectively. Some ways for reducing systematic odometry errors are:
G
Addition of auxiliary wheels (a pair of “knife-edge,” nonload-bearing encoder wheels;
Figure 12.4).
G
Use of a trailer with encoder wheels.
G
Careful systematic calibration of the WMR.
Methods for reducing nonsystematic odometry errors include:
G
Mutual referencing (use of two robots that measure their positions mutually).
G
Correction internal position error (two WMRs mutually correct their odometry errors).
G
Use of mechanical or solid-state gyroscopes.
Clearly, more accurate odometry reduces the requirements on absolute position
updates, but does not eliminate completely the necessity for periodic updating of
the absolute position.
12.5
Kinematic Analysis of Dead Reckoning
The robot state (position and orientation) at instant k is xðkÞ 5 ½xðkÞ; yðkÞ; φðkÞT.
The dead-reckoning (odometry)-based localization of WMRs is to estimate
xðk 1 1Þ at instant k 1 1, and linear and angular position increments ΔlðkÞ, and
Drive wheel
Encoder wheel
Drive motor
Castor
Figure 12.4 Pictorial illustration of the use of encoder
wheels for a differential drive WMR.
490
Introduction to Mobile Robot Control

ΔφðkÞ 5 φðk 1 1Þ 2 φðkÞ, respectively, between the time instants k and k 1 1, where
ΔlðkÞ 5 ðΔx2ðkÞ1Δy2ðkÞÞ1=2, ΔxðkÞ 5 xðk 1 1Þ 2 xðkÞ, and ΔyðkÞ 5 yðk 1 1Þ 2 yðkÞ.
Assuming that ΔφðkÞ is very small, we have the following approximations:
xðk 1 1Þ 5 xðkÞ 1 ΔlðkÞcosðφðkÞ 1 ΔφðkÞ=2Þ
yðk 1 1Þ 5 yðkÞ 1 ΔlðkÞsinðφðkÞ 1 ΔφðkÞ=2Þ
φðk 1 1Þ 5 φðkÞ 1 ΔφðkÞ
To determine the position and angle increments from the wheel movements (i.e.,
the encoder values), we must use the kinematic equations of the WMR under con-
sideration. We therefore investigate the localization problem for differential drive,
tricycle drive, synchrodrive, Ackerman steering, and omnidirectional drive.
12.5.1
Differential Drive WMR
In this case, two optical encoders are sufficient. They supply the increments Δl1ðkÞ
and Δl2ðkÞ of the left and right wheels. If 2a is the distance of the two wheels, and
RðkÞ the instantaneous curvature radius, we have:
Δl1ðkÞ 5 ½RðkÞ 2 aΔφðkÞ
Δl2ðkÞ 5 ½RðkÞ 1 aΔφðkÞ
and therefore:
Δl2ðkÞ 2 Δl1ðkÞ 5 2aΔφðkÞ
Now
ΔlðkÞ 5 RðkÞΔφðkÞ 5 Δl1ðkÞ 1 aΔφðkÞ
5 Δl1ðkÞ 1 1
2 Δl2ðkÞ 2 Δl1ðkÞ
½

5 1
2 Δl1ðkÞ 1 Δl2ðkÞ
½

ð12:18aÞ
and
ΔφðkÞ 5 1
2a Δl2ðkÞ 2 Δl1ðkÞ
½

ð12:18bÞ
These formulas give ΔlðkÞ and ΔφðkÞ in terms of the encoders’ values Δl1ðkÞ
and Δl2ðkÞ.
491
Mobile Robot Localization and Mapping

12.5.2
Ackerman Steering
We work using the robot’s diagram of Figure 1.28 denoted by D the length of the vehi-
cle (from the rotation axis of the back wheels to the axis of the front axis). We have:
ctgφi 5 L=D;
ctgφo 5 ðL 1 2aÞ=D
ctgφo 2 ctgφi 5 2a=D;
ctgφo 2 ctgφs 5 a=D
where φs is the actual steering angle of the vehicle (i.e., the angle of the virtual
wheel located in the middle of the two front wheels), and φo, φi are the steering
angles of the outer and inner wheel, respectively.
Working as in the differential drive case, we find:
ΔlðkÞ 5 1
2 Δl1ðkÞ 1 Δl2ðkÞ
½

ð12:19aÞ
ΔφðkÞ 5 1
D Δl2ðkÞ 2 Δl1ðkÞ
½

ð12:19bÞ
Here, we also find:
ΔφðkÞ 5 1
2D tgφsðkÞ Δl1ðkÞ 1 Δl2ðkÞ
½

ð12:20Þ
Two encoders are sufficient to provide ΔlðkÞ and ΔφðkÞ using the two relations
(12.19a) and (12.19b). To use Eq. (12.20) we need a third encoder. But the result
can be used to decrease the errors of the other two encoders.
12.5.3
Tricycle Drive
Here, the solution is the same as that for the Ackerman steering:
ΔlðkÞ 5 1
2 Δl1ðkÞ 1 Δl2ðkÞ
½
;
ΔφðkÞ 5 1
D Δl2ðkÞ 2 Δl1ðkÞ
½

ð12:21Þ
ΔφðkÞ 5 1
2D tgφsðkÞ Δl1ðkÞ 1 Δl2ðkÞ
½

ð12:22Þ
12.5.4
Omnidirection Drive
For the three-wheel case, we have the following relations:
v1 5 vx 1 ωR
ð12:23aÞ
v2 5 2 vx sin 30 2 vy cos 30 1 ωR
ð12:23bÞ
v3 5 2 vx sin 30 1 vy cos 30 1 ωR
ð12:23cÞ
492
Introduction to Mobile Robot Control

12.6
Absolute Localization
12.6.1
General Issues
Landmarks and beacons form the basis for absolute localization of a WMR [10].
Landmarks may or may not have identities. If the landmarks have identities, no a
priori knowledge about the navigating robot’s position is needed. Otherwise, if
landmarks have no identities some rough knowledge of the position of the robot
must be available. If natural landmarks are employed, then the presence or not of
identities depends on the environment. For example, in an office building, one of
the rooms may have a large three-glass window, which can be used as a landmark
with identity.
The landmarks (beacons) are distinguished in:
G
active artificial landmarks,
G
passive artificial landmarks,
G
natural landmarks.
An active landmark emits some kind of signal to enable easy detection. In
general, the detection of a passive landmark needs more of the sensor. To sim-
plify the detection of passive landmarks, we may use an active sensor together
with a passive landmark that responds to this activeness. For example, use of bar
codes on walls and visual light to detect them for building topological maps. It
is preferable that natural landmarks can be employed with sustainable robustness
of the navigational system. They can provide increased flexibility to the adapta-
tion to new environments with minimum requirement of engineering design. For
example, natural landmarks in indoor environments can be chairs, desks, and
tables.
According to Borenstein [4], localization using active landmarks is distinguished in:
G
trilateration,
G
triangulation.
12.6.2
Localization by Trilateration
In trilateration, the location of a WMR is determined using distance measure-
ments to known active beacon, Bi. Typically, three or more transmitters are
used and one receiver on the robot. The locations of transmitters (beacons) must
be known. Conversely, the robot may have one transmitter onboard and
the receivers can be placed at known positions in the environment (e.g., on
the walls of a room). Two examples of localization by trilateration are the bea-
con systems that use ultrasonic sensors (Figure 12.5) and the GPS system
(Figure 4.24b).
Ultrasonic-based trilateration localization systems are appropriate for use in rela-
tively small area environments (because of the short range of ultrasound), where no
obstructions that interfere with the wave transmission exist. If the environment is
large, the installation of multiple networked beacons throughout the operating area
493
Mobile Robot Localization and Mapping

is of increased complexity, a fact that reduces the applicability a sonar-based trila-
teration. The two alternative designs are:
G
Use of a single transducer transmitting from the robot with many receivers at fixed
positions.
G
Use of a single listening receiver on the robot, with multiple fixed transmitters serving as
beacons (this is analogous to the GPS concept).
The position Pðx; yÞ of the robot in the world coordinate system can be found by
least-squares estimation as follows. In Figure 12.5, let ðxi; yiÞ; i 5 1; 2; 3; . . .; N
ðN $ 3Þ be the known world coordinates of the beacons Bi; i 5 1; 2; 3; . . .; N and
Ri; i 5 1; 2; 3; . . .; N the corresponding robotbeacon distances (radii of the inter-
secting circles). Then, we have:
ðxi2xÞ2 1 ðyi2yÞ2 5 R2
i
ði 5 1; 2; 3; . . .; NÞ
Expanding and rearranging gives:
x2 1 y2 1 ð22xiÞx 1 ð22yiÞy 1 x2
i 1 y2
i 5 R2
i
To eliminate the squares of the unknown variables x and y, we pairwise subtract
the above equations, for example, we subtract the kth equation from the ith equa-
tion to get:
2ðxk 2 xiÞx 1 2ðyk 2 yiÞy 5 bi
where:
bi 5 R2
i 2 R2
k 1 ðx2
k 1 y2
kÞ 2 ðx2
i 2 y2
i Þ
Overall, we obtain the following overdetermined system of linear equations with
two unknowns:
Ax 5 b;
x 5 ½x; yΤ;
b 5 ½b1; b2; . . .; bNT
C1
B1
B2
R2
C2
B3
R3
C3
R1
P
Figure 12.5 Graphical illustration of localization by
trilateration. The robot P lies at the intersection of the
three circles Ci centered at the beacons Bi with radii
Riði 5 1; 2; 3Þ.
494
Introduction to Mobile Robot Control

where A is the matrix:
A 5 2
ðx2 2 x1Þ
ðy2 2 y1Þ
ðx3 2 x1Þ
ðy3 2 y1Þ
^
^
ðxN 2 xN21Þ
ðyN 2 yN21Þ
2
6664
3
7775
The solution of this linear algebraic system is given by (see Eqs. (2.7) and (2.8a)):
x 5 AyB;
Ay 5 ðATAÞ21AT
under the assumption that the matrix ATA is invertible.
An alternative way of computing x 5 ½x; yT is to apply the general iterative
least-squares technique for nonlinear estimation. To this end, we expand the non-
linear function:
Fi 5 ðxi2xÞ2 1 ðyi2yÞ2 2 R2
i 5 0
ði 5 1; 2; . . .; NÞ
in Taylor series about an a priori position estimate ^xq 5 ½^xq; ^yqTðq 5 0Þ, and keep
only first-order terms, namely:
FðxÞ 5 Fð^xqÞ 1 Að^xqÞΔxq 5 0;
q 5 0; 1; 2; . . .:
where:
Δxq 5 x 2 ^xq; FðxÞ 5 ½F1ðxÞ; F2ðxÞ; . . .; FNðxÞT
and Að^xqÞ is the Jacobian matrix of FðxÞ evaluated at x 5 ^xq:
Að^xqÞ 5
@FðxÞ
@x
2
4
3
5
x5^xq
5 2
ð^xq 2 x1Þ
ð^yq 2 y1Þ
ð^xq 2 x2Þ
ð^yq 2 y2Þ
^
^
ð^xq 2 xNÞ
ð^yq 2 yNÞ
2
6664
3
7775
Therefore, we get the following iterative equation for updating the estimate ^xq:
^xq11 5 ^xq 2 Ayð^xqÞFð^xqÞ; q 5 0; 1; 2; . . .:
where ^x0 is known.
The iteration is repeated until jj^xq11 2 ^xqjj becomes smaller than a predeter-
mined small number ε or the number of iterations arrives at a maximum number
qmax 5 Q.
495
Mobile Robot Localization and Mapping

In practice, due to measurement errors in the positions of the beacons and the
radii of the circles, the circles do not intersect at a single point but overlap in a small
region. The position of the robot is somewhere in this region. Each pair of circles
gives two intersection points, and so with three circles (beacons) we have six inter-
section points. Three of these points are clustered together more closely than the
other three points. A simple procedure for determining the smallest intersection
(clustering) region is as follows:
G
Compute the distance between each pair of circle intersection points.
G
Select the two closest intersection points as the initial cluster.
G
Compute the centroid of this cluster (The x,y coordinates of the centroid are obtained by
the averages of the x,y coordinates of the points in the cluster).
G
Find the circle intersection point which is closest to the cluster centroid.
G
Add this intersection point to the cluster and compute again the cluster centroid.
G
Continue in the same manner until N intersection points have been added to the cluster,
where N is the number of beacons (circles).
G
The location of the robot is given by the centroid of the final cluster.
As an exercise, the reader may consider the case of two beacons (circles) and
compute geometrically the positions of their intersection points. Also, the conditions
under which the solution is nonsingular must be determined.
12.6.3
Localization by Triangulation
In this method, there are three or more active beacons (transmitters) mounted at
known positions in the workspace (Figure 12.6). A rotating sensor mounted on the
robot R registers the angles θ1, θ2, and θ3 at which the robot “sees” the active bea-
cons relative to the WMR’s longitudinal axis xr. Using these three readings, we can
compute the position ðx; y; φÞ of the robot, where x stands for X0 and y stands for Y0.
The active beacons must be sufficiently powerful in order to be able to transmit
the appropriate signal power in all directions. The two design alternatives of trian-
gulation are:
1. Rotating transmitter and stationary receivers
2. Rotating transmitterreceiver and stationary reflectors
B1
B2
B3
Y0
Y
0
X0
X
R
θ1
φ
θ2
θ3
Figure 12.6 Triangulation method. A rotating
sensor head measures the angles θ1; θ2; and θ3
between the three active beacons B1, B2, and B3
and the robot’s longitudinal axis.
496
Introduction to Mobile Robot Control

It must be noted that when the observed angles are small, or the observation
point is very near to a circle containing the beacons, the triangulation results are
very sensitive to small angular errors. The three-point triangulation method works
efficiently only when the robot lies inside the boundary of the triangle formed by
the three beacons. The geometric circle intersection shows large errors when the
three beacons and the robot all lie on or are very close to the same circle. In case
the NewtonRaphson method is used to determine the robot’s location, no solution
can be provided if the initial guess of the robot’s position/orientation is in the exte-
rior of a certain bound. A variation of the triangular method creates a virtual bea-
con as follows: a range or angle obtained from a beacon at time t can be used at
time t 1 Δt, provided that the cumulative movement vector, recorded since the
reading was obtained, is added to the position vector of the beacon. This generates
a new virtual beacon.
Referring to Figure 12.6, the basic trigonometric equation, which relates x,y, and
φ with the measured angles θi i 5 1; 2; 3
ð
Þ of the beacons, is:
tg φ 1 θi
ð
Þ 5 yi 2 y
ð
Þ= xi 2 x
ð
Þ
where xi; yi are the coordinates of the beacon i. Three beacons are sufficient for com-
puting x; y; φ
ð
Þ under the condition that the measurements are noise free. Then, we
will have a nonlinear algebraic system of three equations with three unknowns which
can be solved by approximate numerical techniques such as the NewtonRaphson
technique and its variations. But if the measurements are noisy (as it typically
happens in practice), we need more beacons. In this case, we get an overdetermined
noisy nonlinear algebraic system which can be solved by the iterative least-squares
technique, in analogy to the above-mentioned trilateration method [11].
The results obtained are more accurate if the artificial beacons are optimally
placed (e.g., if they are about 120 apart in the three-beacon case). Otherwise, the
robot position and orientation may have large variations with respect to an optimal
value. Moreover, if all beacons are identical, it is very difficult to identify which
beacon has been detected. Finally, mismatch is likely to occur if the presence of
obstacles in the environment obscures one or more beacons. A natural way to over-
come the problem of distinguishing one beacon from another is to manually initial-
ize and recalibrate the robot position when the robot is lost. However, such a manual
initialization/recalibration is not convenient in actual applications. Therefore, many
automated initialization/recalibration techniques have been developed for use in
robot localization by triangulation. One of them uses a self-organizing neural net-
work (Kohonen network) which is able to recognize and distinguish the beacons
[12] (see also Section 12.7).
12.6.4
Localization by Map Matching
Map-matching-based localization is a method in which a robot uses its sensors to
produce a map of its local environment [13]. When we talk about a map in real
life, we usually imagine a geometrical map like the map of a country or a town
497
Mobile Robot Localization and Mapping

with names of places and streets. To find our way in a map of a city using street
names, we walk up to a street corner, read the name (i.e., the landmark) and com-
pare the name with what is written in the map. Now, if we know where we wish to
go, we plan a path to this goal. In our way, we use natural landmarks (e.g., street
corners, buildings, streets, blocks) to keep track of where we are along the planned
path. If we lose track we have to revert the reading signs and matching the street
names to the map. If a road is closed, dead-end, or one-way street, then we plan an
alternative path. This methodology is in principle used by WMRs in map genera-
tion and matching, of course with landmarks suitable for the robot’s environment
in each case.
The robot compares its local map to a global map already stored in the compu-
ter’s memory. If a match is found, then the robot can determine its actual position
and orientation in the environment. The prestored map may be a CAD model of the
environment, or it can be created from prior sensor data.
The basic structure of a map-based localization system is shown in Figure 12.7.
The benefits of the map-based positioning method are the following:
G
No modification of the environment is required (only structured natural landmarks are
needed).
G
Generation of an updated map of the environment can be made.
G
The robot can learn a new environment and enhance positioning accuracy via
exploration.
Some drawbacks are as follows:
G
A sufficient number of stationary, easily distinguishable, features are needed.
G
The accuracy of the map must be sufficient for the tasks at hand.
G
A substantial amount of sensing and processing power is required.
Maps are distinguished as follows:
Topological maps: They are suitable for sensors that provide information about the iden-
tity of landmarks. A real-life topological map example is the subway map which provides
information only on how you get from one station (place) to another (no information
Sensor
signals 
Map
building 
Local and
global map
matching
Computed 
location
Data processing
•
Filtering
•
Sensor
fusion
•
Error
modelling
Figure 12.7 Structure of map-based localization (search can be reduced if the initial
position guess is close to the true position).
498
Introduction to Mobile Robot Control

about distances is given). To enable easy orientation when changing trains, color codings
are provided. Topological maps model the world by a network of arcs and nodes.
Geometrical maps: These maps can be constructed from topological maps if the proper
equipment for measuring distance and angle is available. Geometrical maps suit sensors
that provide geometrical information (e.g., range), like sonar and laser range finders. In
general, it is easier to use geometrical maps in conjunction with sensors of this type, than
using topological maps, since less advanced perception techniques are needed. In extreme
situations, the sensor measurements can be directly matched to the stored map.
Currently, used maps are often CAD drawings or they have been measured by
hand. The human is deciding on which parts of the environment should be included
in the map. It is also up to the human to decide which sensor is the most appropriate
to use (taking of course into account its price), and how it will respond to the envi-
ronment. Ideally, it is desired that the WMR is able to construct maps autonomously
by itself. In this direction, a solid methodology called simultaneous localization and
mapping has been developed which will be examined in the following text.
12.7
Kalman Filter-Based Localization and Sensor
Calibration and Fusion
The Kalman filter described by Eqs. (12.5)(12.11) can be used for three purposes,
namely:
G
Robot localization
G
Sensor calibration
G
Sensor fusion
12.7.1
Robot Localization
The steps of the localization algorithm that is based on Kalman filter are the
following:
Step 1—One-Step Prediction
The predicted robot position ^xðk 1 1jkÞ at time k 1 1, given the known filtered posi-
tion ^xðkjkÞ at time k, is given by Eq. (12.12a), that is:
^xðk 1 1jkÞ 5 AðkÞ^xðkjkÞ
ð12:24Þ
where AðkÞ represents the kinematic equation of the robot. The corresponding position
uncertainty is given by the covariance matrix Σðk 1 1jkÞ described by Eq. (12.8), that is:
Σðk 1 1jkÞ 5 AðkÞΣðkjkÞATðkÞ 1 BðkÞQðkÞBTðkÞ
ð12:25Þ
with Σð0j0Þ 5 Σ0 (a known value). The respective prediction ^zðk 1 1jkÞ of the sensors’
measurements is given by:
^zðk 1 1jkÞ 5 Cðk 1 1Þ^xðk 1 1jkÞ
499
Mobile Robot Localization and Mapping

Step 2—Sensor Observation
At this step, a new set of sensor measurements zðk 1 1Þ at time k 1 1 is taken.
Step 3—Matching
The measurement innovation process:
~zðk 1 1jkÞ 5 zðk 1 1Þ 2 ^zðk 1 1jkÞ
5 zðk 1 1Þ 2 Cðk 1 1Þ^xðk 1 1jkÞ
5 zðk 1 1Þ 2 Cðk 1 1ÞAðkÞ^xðkjkÞ
ð12:26Þ
Sðk 1 1jkÞ 5 Ef~zðk 1 1jkÞ~zTðk 1 1jkÞg
5 Cðk 1 1ÞΣðk 1 1jkÞCTðk 1 1Þ 1 RðkÞ
ð12:27Þ
is constructed.
Step 4—Position Estimation
Calculate the Kalman filter gain matrix Kðk 1 1Þ using Eq. (12.10), that is:
Kðk 1 1Þ 5 Σðk 1 1jk 1 1ÞCTðk 1 1ÞR21ðk 1 1Þ
ð12:28Þ
and the updated position estimate ^xðk 1 1jk 1 1Þ is given by Eqs. (12.5)(12.8), that is:
^xðk 1 1jk 1 1Þ 5 ^xðk 1 1jkÞ 1 Kðk 1 1Þ~zðk 1 1jkÞ
ð12:29Þ
Σðk 1 1jk 1 1Þ 5 Σðk 1 1jkÞ 2 Kðk 1 1ÞCðk 1 1ÞΣðk 1 1jkÞ
ð12:30Þ
where ~zðk 1 1jkÞ and Σðk 1 1jkÞ are given by Eqs. (12.26) and (12.25), respectively.
The initial value Σð0j0Þ 5 Σ0 of the estimate’s covariance, as well as the values of QðkÞ
and RðkÞ needs to be determined experimentally (before the application of the method)
using the available information about the sensors and their models. Typically,
QðkÞ 5 Q 5 const: and RðkÞ 5 R 5 const: Taking into account that the measurement (sen-
sor observation) vector zðkÞ may involve data from several sensors (usually of different
type), we can say that the Kalman filter provides also a sensor fusion and integration
method (see Example 12.5).
In all cases, it is advised to check if the new measurements are statistically inde-
pendent from the previous measurements and disregard them if they are statistically
dependent, since they do not actually provide new information. This can be done at
the matching step on the basis of the innovation process (or residual) ~zðk 1 1jkÞ
and its covariance Sðk 1 1jkÞ given by Eqs. (12.26) and (12.27). The process
~zðk 1 1jkÞ expresses the difference between the new measurement zðk 1 1Þ and its
expected value Cðk 1 1ÞAðkÞ^xðkjkÞ according to the system model.
To this end, we use the so-called normalized innovation squared (NIS) defined as:
~zNISðk 1 1 kÞ 5 ~zTðk 1 1jkÞS21ðk 1 1jkÞ~zðk 1 1jkÞ

The process ~zNISðk 1 1jkÞ follows a chi-square ðχ2Þ distribution with m degrees
of freedom (more precisely when the innovation process ~zðk 1 1jkÞ is Gaussian).
Therefore, we can read from the chi-square, the bounding values (confidence
500
Introduction to Mobile Robot Control

interval) for a χ2 random variable with m degrees of freedom, and discard a mea-
surement, if its ~zNISðk 1 1jkÞ is outside the desired confidence interval. More spe-
cifically, if the criterion:
~zNISðk 1 1jkÞ # χ2
is satisfied, then the measurement zðk 1 1Þ is considered to be independent from
zðkÞ and so it is included in the filtering process. Measurements that do not satisfy
the above criterion are ignored. The random variable χ2 is defined as follows.
Consider two random variables (sensor signals) x and y which are independent if
the probability distribution of x is not affected by the presence of y. Then, the chi-
square variable is given by:
χ2 5
X
ij
ðfij2eijÞ2
eij
where fij is the observed frequency of events belonging to both the ith category of x
and jth category of y, and eij is the corresponding expected frequency of events if x and
y are independent. The use of chi-square (or contingency table) is described in books
on statistics (see, e.g., in http://onlinestatbook.com/stat_sim/chisq_theory/index.html).
12.7.2
Sensor Calibration
The Kalman filter provides also a method for sensor calibration, which is based on
the minimization of the error between the actual measurements and the simulated
measurement of the sensor with the parameters at hand. The parameters that have
to be calibrated may be intrinsic or extrinsic. Intrinsic parameters cannot be
directly observed, but extrinsic parameters can. For instance, the focal distance, the
location of the optical center, and the distortion of a camera are intrinsic para-
meters, but the position of the camera is an extrinsic parameter.
To perform sensor calibration with the aid of the Kalman filter, we use the para-
meters under calibration, as the state vector of the model, in place of the robot’s
position/orientation. The prediction step gives simulated values of the parameters
at the present state (i.e., with their present values). The state observation step pro-
vides a set of sensor captions taken at different robot positions, and not during the
motion of the robot (as we did for robot localization, i.e., for position estimation
and mapping). The use of Kalman filters for sensor calibration is general and inde-
pendent of the nature of the sensor (range, vision, etc.). The only things that change
are the state vector xðkÞ and the measurement vector with the associated matrices
AðkÞ, BðkÞ, and CðkÞ, and the noises wðkÞ and vðkÞ.
12.7.3
Sensor Fusion
Sensor fusion is the process of merging data from multiple sensors such that to
reduce the amount of uncertainty that may be involved in a robot navigation motion
501
Mobile Robot Localization and Mapping

or task performing. Sensor fusion helps in building a more accurate world model in
order for the robot to navigate and behave more successfully. The three fundamen-
tal ways of combining sensor data are the following:
G
Redundant sensors: All sensors give the same information for the world.
G
Complementary sensors: The sensors provide independent (disjoint) types of information
about the world.
G
Coordinated sensors: The sensors collect information about the world sequentially.
The three basic sensor communication schemes are [14]:
G
Decentralized: No communication exists between the sensor nodes.
G
Centralized: All sensors provide measurements to a central node.
G
Distributed: The nodes interchange information at a given communication rate (e.g.,
every five scans, i.e., one-fifth communication rate).
The centralized scheme can be regarded as a special case of the distributed
scheme where the sensors communicate to each other every scan. A pictorial repre-
sentation of the fusion process is given in Figure 12.8.
Consider, for simplicity, the case of two redundant local sensors (N 5 2). Each
sensor processor i provides its own prior and updated estimates and covariances
^xiðk 1 1jkÞ, Σiðk 1 1jkÞ and ^xiðk 1 1jk 1 1Þ, Σiðk 1 1jk 1 1Þ, i 5 1; 2. Assume that
the fusion processor has its own total prior estimate ^xðk 1 1jkÞ and Σðk 1 1jkÞ. The
fusion problem is to compute the total estimate ^xðk 1 1jk 1 1Þ and covariance matrix
Σðk 1 1jk 1 1Þ using only those local estimates and the total prior estimate. The total
updated estimate can be obtained using linear operations on the local estimates, as:
^xðk 1 1jk 1 1Þ 5 Σðk 1 1jk 1 1Þ½Σ1ðk11jk11Þ21 ^x1ðk 1 1jk 1 1Þ
1Σ2ðk11jk11Þ21 ^x2ðk 1 1jk 1 1Þ
2Σ1ðk11jkÞ21 ^x1ðk 1 1jkÞ 2 Σ2ðk11jkÞ21 ^x2ðk 1 1jkÞ
1Σðk11j1Þ21 ^xðk 1 1jkÞ
Sensor
processor 1
Sensor
processor 2
Sensor
processor N
Fusion
processor
System
Sensor
data 2 
Sensor
data N
Sensor
data 1 
Action
x,∑
x1,∑1
x2,∑2
xN,∑N
Figure 12.8 Illustration of the sensor fusion process.
502
Introduction to Mobile Robot Control

where:
Σðk 1 1jk 1 1Þ 5 ½Σ1ðk11jk11Þ21 1 Σ2ðk11jk11Þ21 2 Σ1ðk11jkÞ21
2Σ2ðk11jkÞ21 1 Σðk11jkÞ2121
When the local processors and the fusion processor have the same prior esti-
mates, the above fusion equations can be simplified as:
^xðk 1 1jk 1 1Þ 5 Σðk 1 1jk 1 1Þ½Σ1ðk11jk11Þ21 ^x1ðk 1 1jk 1 1Þ
1Σ2ðk11jk11Þ21 ^x2ðk 1 1jk 1 1Þ
2Σðk11jkÞ21 ^xðk 1 1jkÞ
ð12:31aÞ
Σðk 1 1jk 1 1Þ 5 ½Σ1ðk11jk11Þ211Σ2ðk11jk11Þ212Σðk11jkÞ2121
ð12:31bÞ
This means that the common prior estimates (i.e., the redundant information) are
subtracted in the linear fusion operation. The above equations constitute the fusion
processor shown in Figure 12.8. Typically, the sensor processors 1; 2; . . .; N are
Kalman filters.
Example 12.2
We will derive the formula for fusing the measurements xk provided for a quantity x (e.g.,
the position of a WMR) by m independent sensors. It is assumed that the measurement xk
of the kth sensor is normally distributed (Gaussian) with variance σ2
k.
We will apply the maximum likelihood estimation method in which the joint probability
distribution pðx1; x2; . . . xmjx; σÞ of x1; x2; . . .; xm is maximized with respect to the fused
value x and fused variance σ2. The joint Gaussian distribution pðx1; x2; . . .; xmjx; σÞ is:
pðx1; x2; . . .; xmjx; σÞ 5 L
m
k51
1
σk
ﬃﬃﬃﬃﬃﬃ
2π
p
e2ðxk2xÞ2=2σ2
k
The likelihood function L is defined as the logarithm of pðx1; x2; . . .; xmjx; σÞ, that is:
Lðx1; x2; . . .; xmjx; σÞ 5 21
2 m lnð2πÞ 2 m
X
m
k51
ln σk 2
X
m
k51
ðxk2xÞ2=ð2σ2
kÞ
To maximize L, we equate to zero the derivative of L with respect to x, that is:
@L
@x 5
X
m
k51
ðxk 2 xÞ
σ2
k
5
X
m
k51
xk
σ2
k
2 x
X
m
k51
1
σ2
k
0
@
1
A 5 0
503
Mobile Robot Localization and Mapping

Thus, the fused estimate ^x of the m sensors is equal to:
^x 5
X
m
k51
xk
σ2
k
 
!
 X
m
k51
1
σ2
k
 
!
The variance σ^x of ^x is given by:
σ2
^x 5
X
m
k51
σ2
k
@^x
@xk

2
The partial derivative @^x=@xk is:
@^x
@xk
5 @
@xk
Pm
k51ðxk=σ2
kÞ
Pm
k51ð1=σ2
kÞ 5
1=σ2
k
Pm
k51ð1=σ2
kÞ
Therefore:
σ2
^x 5
X
m
k51
σ2
k

1=σ2
k
Pm
k51ð1=σ2
kÞ
2
5
X
m
k51
1=σ2
k
½Pm
k51ð1=σ2
kÞ2 5
1
Pm
k51ð1=σ2
kÞ
or
1
σ2
^x
5
X
m
k51
1
σ2
k
To illustrate the meaning of the above formulas for ^x and σ2
^x, we consider the case of
two sensors ðm 5 2Þ, for example, a laser range finder sensor and an ultrasonic range sen-
sor, namely:
^x 5
x1
σ2
1
1 x2
σ2
2


1
σ2
1
1 1
σ2
2


5
σ2
2
σ2
1 1 σ2
2


x1 1
σ2
1
σ2
1 1 σ2
2


x2
1
σ2
^x
5 1
σ2
1
1 1
σ2
2
5 σ2
1 1 σ2
2
σ2
1σ2
2
or
σ2
^x 5 σ2
1σ2
2=ðσ2
1 1 σ2
2Þ
We see that the variance of the fused estimate is smaller than all the variances of the
individual sensor measurements (similarly to the connection of pure resistances in paral-
lel) (see Figure 12.9). The formula for ^x can be written as:
^x 5 x1 1 ½σ2
1=ðσ2
1 1 σ2
2Þðx2 2 x1Þ
504
Introduction to Mobile Robot Control

Thus, we get the following sensor estimate updating formula:
^xk11 5 ^xk 1 Σk11ðyk11 2 ^xkÞ;
yk11 5 x2;
^xk 5 x1
Σk11 5 σ2
k=ðσ2
k 1 σ2
yÞ
with σ2
k 5 σ2
1 and σ2
y 5 σ2
2. The updated variance σk11 of ^xk11 is found to be:
σ2
k11 5 σ2
k 2 Σk11σ2
k
The above sequential equations for ^xk11 and σ2
k11 represent a discrete Kalman filter
(see Section 12.2.3 and Example 12.1) that can be used when the sensors collect and pro-
vide their data sequentially.
In the above analysis, the variable ^x was assumed to have a fixed value (e.g., when the
WMR is not moving). If the variable x changes, and the change can be expressed by a
dynamic stochastic system, then the dynamic Kalman filter should be used and σk11 is
decreasing with time (see Table 12.1).
12.8
Simultaneous Localization and Mapping
12.8.1
General Issues
The SLAM problem deals with the question if it is possible for a WMR to be
placed at an unknown location in an unknown environment, and for the robot to
incrementally build a consistent map of this environment while simultaneously
determining its location within this map.
The foundations for the study of SLAM were made by Durrant-Whyte [15] by
establishing a statistical basis for describing relationships between landmarks and
manipulating geometric uncertainty. The key element was the observation that there
must be a high degree of correlation between estimates of the location of several land-
marks in a map and that these correlations are strengthened with successive observa-
tions. A complete solution to the SLAM problem needs a joint state, involving the
WMR’s pose and every landmark position, to be updated following each landmark
observation. Of course, in practice this would need the estimator to employ a very
high dimensional state vector (of the order of the number of landmarks maintained in
p(x)
Pdf of sensor 1
Pdf of sensor 2
Fused  pdf
σ1
σ2
σx
x2
x
x1
x
Figure 12.9 The variance
σ^x of the fused estimate ^x is
smaller than both σ1 and σ2.
505
Mobile Robot Localization and Mapping

the map) with computational complexity reduction proportional to the number of
landmarks. A wide range of sensors, such as sonars, cameras, and laser range finders,
has been used to sense the environment and achieve SLAM ([58]).
A generic self-explained diagram showing the structure and interconnections of
the various functions (operations) for performing SLAM is provided in Figure 12.10.
The three typical methods for implementing SLAM are:
G
EKF
G
Bayesian estimator
G
PF
EKF is an extension of Kalman filter to cover nonlinear stochastic models,
which allows to study observability, controllability, and stability of the filtered sys-
tem. Bayesian estimators describe the WMR motion and feature observations
directly using the underlying probability density functions and the Bayes theorem
for probability updating. The Bayesian approach has shown a good success in
many challenging environments. The PF (or as otherwise called, sequential Monte
Carlo method) is based on simulation [15]. In the following sections, all these
SLAM methods will be outlined.
Onboard sensor
measurement
Position
prediction 
(odometry) 
Estimation 
(fusion) 
using map
Refine
features 
Add new
features 
Remove bad
features 
Map
Encoder
Matching
? 
?
Map building and maintenance
Perception
Unexpected
measurement 
Raw sensor data or
extracted features 
Matched predictions
and measurements 
No
No
Yes
Yes
Localization
cycle 
Unobserved
predictions 
Unexpected
measurements 
Figure 12.10 Interconnection diagram and flow chart of the SLAM functions.
506
Introduction to Mobile Robot Control

12.8.2
EKF SLAM
The motion of the WMR and the measurement of the map features are described
by the following stochastic nonlinear model [16]:
xk11 5 fðxk; uk; wkÞ
ð12:32aÞ
zk 5 hðxkÞ 1 vk
ð12:32bÞ
where the state vector xk contains the m-dimensional position xr;k of the vehicle,
and a vector xf of n stationary d-dimensional map features, that is, xk has
dimensionality m 1 d  n:
xk 5
xr;k
xf


ð12:33Þ
The l-dimensional input vector uk is the robot’s control command, and the
l-dimensional process wk is a Gaussian stochastic zero-mean process with constant
covariance matrix Q. The function hðxkÞ represents the sensor model, and vk repre-
sents the inaccuracies and noise. It is also assumed that vk is a zero-mean Gaussian
process.
Given a set of measurements Zk 5 fz1; z2; . . .; zkg for the current map estimate
xk kj the expression:
xk11 5 fðxkjk; uk; 0Þ
ð12:34aÞ
gives an a priori noise-free estimate of the new locations of the robot and map fea-
tures after the application of the control input uk. In the same way:
zk11jk 5 hðxk11jkÞ 1 0
ð12:34bÞ
provides a noise-free a priori estimate of the sensor measurements.
If fð; ; Þ and hðÞ are linear, then the Kalman filter, Eqs. (12.5)(12.10), can be
directly applied. But, in general fð; ; Þ and hðÞ are nonlinear in which case we use
the following linearizations (first-order Taylor approximations):
xk11  xk11jk 1 Fðxk 2 xkjkÞ 1 Gwk
ð12:35aÞ
zk11  zk11jk 1 Hðxk11 2 xk11jkÞ 1 vk
ð12:35bÞ
where F, G, and H are the Jacobian matrices:
F 5 @f
@x

ðxkjk;uk;0Þ
;
G 5 @f
@w

ðxkjk;uk;0Þ
;
H 5 @h
@x

ðxk11jk;0Þ
507
Mobile Robot Localization and Mapping

Given that the landmarks are assumed stationary, their a priori estimate is:
xf;k11jk 5 xf;kjk
Thus, the overall state model of the vehicle and map dynamics is:
xk11 
xr;k11jk
xf;kjk


1
Fr
0
0
I

 ~xr;kjk
~xf;kjk


1
Gr
0


wk
0


zk11  zk11jk 1 Hr
Hf

 ~xr;k11jk
~xf;k11jk


1 vk11
or
xk11 5 xk11jk 1 A~xkjk 1 Bw
k
ð12:36Þ
zk11 5 zk11jk 1 C~xkjk 1 vk11
ð12:37Þ
where:
A 5
Fr
0
0
I


;
B 5
Gr
0


;
C 5
Hr
Hf ;
w
k 5
wk
0



ð12:38aÞ
xk11jk 5 ½ xT
r;k11jk
xT
f;kjk T;
~xkjk 5 ½ ~xT
r;kjk
~xT
f;kjk T
ð12:38bÞ
Using the augmented linearized model for the dynamics of the position and
environmental landmarks, we can apply directly the same steps as those given in
Section 12.7 for the linear localization problem.
For convenience, we repeat here the sequence of the covariances:
Prediction covariance: Σk11jk 5 AΣkjkAT 1 BQBT
Innovation covariance: Sk11jk 5 CΣk11jkCT 1 R
Filter gain: Kk11 5 Σk11jk11CTR21
Filter covariance: Σk11jk11 5 Σk11jk 2 Kk11CΣk11jk
ð12:39Þ
Under the condition that the pair
A; C
ð
Þ is completely observable, the filter
covariance tends to a constant matrix Σ which is given by the following steady-
state (algebraic) Riccati equation:
Σ 5 A½Σ 2 ΣCTðCΣCT1RÞ21CΣAT 1 Q
ð12:40Þ
508
Introduction to Mobile Robot Control

It is noted that in SLAM the complete observability condition is not satisfied in
all cases. Clearly, the steady-state covariance matrix Σ depends on the values of
Σr;0j0, Q and R, as well as on the total number n of landmarks. The solution of
(12.40) can be found by standard computer packages (e.g., Matlab).
The EKF-SLAM solution is well developed and possesses many of the benefits
of the application of the EKF technique to navigation or tracking control problems.
However, since EKF-SLAM uses linearized models of nonlinear dynamics and
observation models, it leads sometimes to inevitable and critical inconsistencies.
Convergence and consistence can only by assured in the linear case as illustrated in
the following example.
Example 12.3
Consider a one-dimensional WMR (monorobot) with state xr;k, and a one-dimensional land-
mark xf as shown in Figure 12.11 [16].
The WMR’s position error dynamics is:
xr;k11 5 xr;k 1 uk 1 wk
ð12:41Þ
and the landmarks dynamics is:
xf;k11 5 xf;k
ð12:42Þ
The map generated by this system is a single static landmark xf. The observation (mea-
surement) model for this landmark is:
zk11 5 xf;k11 2 xr;k11jk 1 vk
ð12:43Þ
where vk is the landmark’s measurement error. Equations (12.41)(12.43) can be written
in the standard form of Eqs. (12.2a) and (12.2b), that is:
xk11 5 Axk 1 B0uk 1 Bwk
ð12:44Þ
zk11 5 Cxk 1 vk
ð12:45Þ
where:
xk 5
xr;k
xf;k


;
A 5
1
0
0
1


;
B0 5
1
0


;
B 5
1
0


;
C 5 21
1


;
wk 5
wk
0


For the filtering problem, the term B0uk is just an additive term, and has no influence
on the filtered estimate. The filtered estimate is given by:
^xk11jk11 5 A^xkjk 1 B0uk 1 Kðk 1 1Þ½zk11 2 CA^xkjk 5 ðA 2 KCAÞ^xkjk 1 B0uk
xr,k
xf
xr,k+1|k
xr,k+1
uk
wk
Figure 12.11 A single-dimensional robot with a
one-dimensional landmark.
509
Mobile Robot Localization and Mapping

Here, the matrix A0 5 A 2 KCA is equal to:
A0 5
k1 1 1
2k1
k2
2k2 1 1


where;
K 5
k1
k2


ð12:46Þ
which has the eigenvalues λ1 5 1 and λ2 5 k1 2 k2 1 1. We see that λ1 is always equal to 1
regardless of the filter gain matrix K. Therefore, the filter in this case is only marginally
stable (i.e., it performs constant amplitude oscillations). This can easily be verified via simu-
lation. To overcome this problem which is caused by a partially observable system, we can
use several techniques to assure complete observability of the pair ðA; CÞ, that is, the matrix:
P 5
C
CA
^
CAN21
2
66664
3
77775
ðN 5 dimensionality of AÞ
ð12:47Þ
is of full rank N, in which case it is invertible. These methods include [17]:
G use of anchors or markers,
G use of fixed global references,
G use of an external sensor,
G use of the relative position of the landmark with respect to the position of the robot
instead of global positioning.
Therefore, let us assume that we add an anchor in the measurement process of
Eq. (12.43) with state zð0Þ
k
and noise vð0Þ
k . Then, the measurement model of Eq. (12.45) has:
C 5
21
0
21
1


; zk 5
zð0Þ
k
zk


;
vk 5
vð0Þ
k
vk


ð12:48Þ
Now, the gain matrix K becomes:
K 5
k11
k12
k21
k22


ð12:49Þ
and the matrix A0 5 A 2 KCA is:
A0 5
1 1 k11 1 k12
2 k12
k21 1 k22
1 2 k22


ð12:50Þ
The matrix gain K is given by Eqs. (12.10), (12.7), and (12.8). If σw and σv are the
standard deviations of w and v, respectively, then using the present form of the matrices
A,B, and C (see Eq. (12.48)), we can verify that:
k11 5 2ðσ4
v 1 σ2
wσ4
v 1 4σwσ2
v 1 2σ2
v 1 3σ2
wÞ=μ
k12 5 2ðσ4
v 1 σ2
w 1 3σ2
wσ2
v 1 σ2
wσ4
vÞ=μ
k21 5 2ðσ2
v 1 σ2
wσ2
v 1 2σ2
wÞ=μ
k22 5 ðσ4
v 1 2σ2
v 1 σ2
wσ2
v 1 2σ2
wÞ=μ
ð12:51Þ
510
Introduction to Mobile Robot Control

where:
μ 5 σ6
v 1 2σ2
wσ4
v 1 6σ4
v 1 7σ2
wσ2
v 1 4σ2
v 1 4σ2
w
ð12:52Þ
Now, we can see that the eigenvalues of A0 5 A 2 KCA given by Eq. (12.50) always lie
inside the unit circle of the complex z plane, and so adding the “anchor” the filter becomes
stable.
12.8.3
Bayesian Estimator SLAM
In probabilistic (Bayesian) form, the SLAM problem is to compute the conditional
probability [1821]:
Pðxr;k; xfjZ0;k; U0;k; xr;0Þ
ð12:53Þ
where:
xf 5 fxf1; xf2; . . .; xfng
Z0;k 5 fz1; z2; . . .; zkg 5 fZ0;k21; zkg
U0;k 5 fu1; u2; . . .; ukg 5 fU0;k21; ukg
represent the history of landmark observations zk and the history of control inputs
uk, respectively. This probability distribution represents the joint posterior density
of the landmark locations and WMR state (at time k) given the recorded observa-
tions and control inputs up to and including the time k together with the initial state
xr;0 of the robot. The Bayesian SLAM is based on Bayesian learning, discussed in
Section 12.2.4. Starting with an initial estimate for the distribution:
Pðxr;k21; xfjZ0;k21; U0;k21; xr;0Þ
ð12:54Þ
at time k 2 1 the joint posterior probability, following a control uk and observation
zk is computed by Bayes learning (updating) formula (12.15b). The steps of the
algorithm are as follows:
Step 1—Observation Model
Determine the observation model that describes the probability of making an observa-
tion zk when the WMR location and landmark locations are known. In general, this model
has the form:
Pðzkjxr;k; xfÞ
ð12:55Þ
Of course, we tacitly assume that once the robot’s location and the map are defined,
the observations are conditionally independent given the map and the current WMR state.
511
Mobile Robot Localization and Mapping

Step 2—WMR Motion Model
Determine the motion model of the vehicle which is described by the Markovian con-
ditional probability:
Pðxr;kjxr;k21; ukÞ
ð12:56Þ
This probability indicates that the state xr;k at time k depends only on the state xr;k21 at
time k 2 1 and the exerted control uk, and does not depend on the observations and the map.
Step 3—Time Update
We have:
Pðxr;k; xfjZ0;k21; U0;k; xr;0Þ
5
ð
Pðxr;kjxr;k21; ukÞPðxr;k21; xfjZ0;k21; U0;k21; xr;0Þdxr;k21
ð12:57Þ
Step 4—Measurement Update
According to Bayes update law:
Pðxr;k; xfjZ0;k; U0;k; xr;0Þ
5 Pðzkjxr;k; xfÞPðxr;k; xfjZ0;k21; U0;k; xr;0Þ
PðzkjZ0;k21; U0;kÞ
ð12:58Þ
Equations (12.57) and (12.58) provide a recursive algorithm for calculating the joint
posterior probability Pðxr;k; xfjZ0;k; U0;k; xr;0Þ for the WMR state xr;k and map xf at time k
using all observations Z0;k and all control inputs U0;k up to and including time k. This
recursion uses the WMR model Pðxr;kjxr;k21; ukÞ and the observation model Pðzkjxr;k; xfÞ.
We remark that here the map building can be formulated as computing the conditional
density PðxfjX0;k; Z0;k; U0;kÞ. This needs the location xr;k of the WMR to be known at all
times, under the condition that the initial location is known. The map is then produced
through the fusion of observations from different positions. On the other hand, the locali-
zation problem can be formulated as the problem of computing the probability distribu-
tion Pðxr;kjZ0;k; U0;k; xfÞ. This requires the landmark locations to be known with certainty.
The goal is to compute an estimate of the WMR location with respect to these landmarks.
The above formulation can be simplified by dropping the conditioning on historical
variables in Eq. (12.54) and write the joint posterior probability as Pðxr;k21; xfjzkÞ.
Similarly, the observation model Pðzkjxr;k; xfÞ makes explicit the dependence of obser-
vations on both the vehicle and landmark locations. But, here the joint posterior proba-
bility cannot be partitioned in the standard way, that is, we have:
Pðxr;k; xfjzkÞ 6¼ Pðxr;kjzkÞPðxfjzkÞ
ð12:59Þ
Thus, care should be taken not to use the partition shown in Eq. (12.59) because
this could lead to inconsistencies.
However, the SLAM problem has more intrinsic structure not visible from the
above discussion. The most important issue is that the errors in landmark location
estimates are highly correlated, for example, the joint probability density of a pair of
landmarks, Pðxfi; xfjÞ, is highly peaked even when the independent densities PðxfiÞ
may be very dispersed. Practically, this means that the relative location xfi 2 xfj of
any two landmarks xfi; xfj may be estimated much more accurately than their
512
Introduction to Mobile Robot Control

individual positions where xfi; xfj may be quite uncertain. In other words, the relative
location of landmarks always improves and never diverges, regardless of the WMR’s
motion. Probabilistically, this implies that the joint probability density on all land-
marks PðxfÞ becomes monotonically more peaked as more observations are made.
Example 12.4
We consider a differential drive WMR mapping two-dimensional landmarks xf 5 ½xi
f; yi
fT,
i 5 1; 2; . . .; m (Figure 12.12) [16]. The state space of the robot is three dimensional,
where the state vector is:
xk 5 xk
yk
φk T

ð12:60Þ
The robot is controlled by a linear velocity υ and an angular velocity ω.
Let l be the distance from the center of the wheel axis to the location of the center of pro-
jection for any given sensor, and Δt the discrete-time step. The dynamic model of the trajec-
tory of the center of projection of the sensor, which includes the noises wυ;k and wω;k, is:
xk11 5 frðxk; uk; wkÞ;
wk 5 ½wυ;kwω;kT
ð12:61Þ
or in detailed form:
xk11
yk11
φk11
2
64
3
75 5
xk 1 ½ðυk 1 wυ;kÞcos φk 2 lðωk 1 wω;kÞsin φkΔt
yk 1 ½ðυk 1 wυ;kÞsin φk 1 lðωk 1 wω;kÞcos φkΔt
φk 1 ðωk 1 wω;kÞΔt
2
664
3
775
ð12:62Þ
Differentiating Eq. (12.62) with respect to xk and wk 5 ½wυ;k; wω;kT, we find the
Jacobian matrices:
Ar 5 @fr
@xk
5
1
0
2ðυk sin φk 2 lωk cos φkÞΔt
0
1
ðυk cos φk 2 lωk sin φkÞΔt
0
0
1
2
4
3
5
Gr 5 @fr
@w
5
ðcos φkÞΔt
2ðl sin φkÞΔt
ðsin φkÞΔt
ðl cos φkÞΔt
0
Δt
2
4
3
5
ð12:63Þ
Sr
X
Y
yr
xr
Sm
Figure 12.12 Differential drive WMR on the plane
X 2 Y.
513
Mobile Robot Localization and Mapping

The measurement model of the sensor (here a laser range scanner) is:
zk 5
zr;k
zβ;k


5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðxi
f2xkÞ2 1 ðyi
f2ykÞ2
q
1 vr;k
tg21
yi
f 2 yk
xi
f 2 xk
0
@
1
A 2 φk 1 π
2 1 vβ;k
2
66664
3
77775
ði 5 1; 2; . . .; mÞ
ð12:64Þ
where zr;k and zβ;k are the range and bearing of an observed point landmark with respect to
the laser center of projection. The position of the ith landmark is ðxi
f; yi
fÞ and the measure-
ment noises are vr;k and vβ;k. The Jacobian matrix of this nonlinear model is:
Hi 5
2x1
f 2 xk
d1
2y1
f 2 yk
d1
0
?
xm
f 2 xk
dm
ym
f 2 yk
dm
0
y1
f 2 yk
d2
1
2x1
f 2 xk
d2
1
21
?
2ym
f 2 yk
d2
m
xm
f 2 xk
d2
m
21
2
66664
3
77775
ð12:65aÞ
where:
di 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðxi
f2xkÞ2 1 ðyi
f2ykÞ2
q
;
i 5 1; 2; . . .; m
ð12:65bÞ
The measurement model of a global reference, fixed at the origin, for the nonlinear
vehicle is:
hð0Þ 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
x2
k 1 y2
k
q
1 vr;k
tg21ðyk=xkÞ 2 φk 1 π
2 1 vβ;k
2
64
3
75
which has the Jacobian matrix:
H0 5
xk=qk
yk=qk
0
0
?
2yk=q2
k
xk=q2
k
21
0
?


ð12:66Þ
where qk 5 ðx2
k 1y2
kÞ1=2. Thus, the overall measurement matrix C is:
C 5
H0
Hi


ð12:67Þ
It can be verified that with the addition of the global reference at the origin, the
observability matrix P given by Eq. (12.47) is of full rank and so the EKF is stable and pro-
vides a steady-state covariance matrix Σ (see Eq. (12.40)). This has also been verified
experimentally [16].
514
Introduction to Mobile Robot Control

12.8.4
PF SLAM
The purpose of the PF is to estimate the robot’s position and map parameters xk (see
Eq. (12.53)) for k 5 0; 1; 2; . . . on the basis of the data zk, k 5 0; 1; 2; 3; . . .. The
Bayes method computes the xk using the posterior probability pðxkjz0; z1; . . .; zk;
u0; u1; . . .; ukÞ. The Markov sequential Monte Carlo (MSMC) method (PF) is based
on the total probability distribution pðx0; x1; . . .; xkjz0; z1; . . .; zk; u0; u1; . . .; ukÞ
[18,22,23].
Here, the Markov stochastic model given by Eqs. (12.32a) and (12.32b) of the
system is described in a probabilistic way as follows [18]:
1. x0; x1; . . .; xk is a first-order Markov process such that:
xkjxk21 corresponds to Pxjxk21ðxjxk21Þ with initial distribution Pðx0Þ.
2. Assuming that x0; x1; . . .; xk are known, the observations z0; z1; z2; . . . are conditionally
independent, that is, zkjxk is described by PzjxðzjxkÞ.
Particle methods belong to the sampling statistical methods which generate a set
of
samples
that
approximate
the
filtering
probability
distribution
pðxkjz0; z1; . . .; zkÞ.2 Therefore, with M samples, the expectation with respect to the
filtering distribution is approximately given by:
ð
fðxkÞpðxk
z0; z1; . . .; zkÞdxk  1
M
X
M
m51
fðxm
k Þ
where fðxkÞ can give, by the Monte Carlo technique, all moments of the distribution
up to a desired degree. The particle method which is mostly used is the so-called
sequential importance resampling (SIR) method proposed by Gordon and collea-
gues [18]. In this method, the filtering distribution pðxkjz0; z1; . . .; zkÞ is approxi-
mated by a set of M particles (multiple copies) of the variable of interest:
fðxm
k ; wm
k Þ; m 5 1; 2; . . .; Mg
where wm
k ðm 5 1; 2; . . .; M; k 5 0; 1; 2; . . .Þ are weights that signify the relative qual-
ity of the particles, that is, they are approximations to the relative posterior proba-
bilities (densities) of the particles such that:
w1
k 1 w2
k 1 ? 1 wM
k 5 1
SIR is a recursive (sequential, iterative) form of importance sampling where the
expectation of a function fðÞ is approximated by a weighted average:
ð
fðxkÞpðxkjz0; . . .; zkÞdxk 
X
M
m51
wm
k fðxm
k Þ
One of the problems encountered in PFs is the depletion of the particle popula-
tion in some regions of space after some iterations. As most of the particles have
2 For convenience, we drop the dependence on the inputs u0; u1; . . .; uk.
515
Mobile Robot Localization and Mapping

drifted far enough, their weights become very small (close to zero) and they no lon-
ger contribute to estimates of xk (i.e., they can be omitted).
The samples of xm
k ; m 5 1; 2; . . .; M are drawn using a proposed probability dis-
tribution ppðxkjx0; x1; . . .; xk21; z1; . . .; zkÞ. Usually, as proposed probability distribu-
tion we select the transition prior distribution pðxkjxk21Þ, which facilitates the
computations.
The PF algorithm consists of several steps. At each step, for m 5 1; 2; . . .; M, we
do the following:
1. Draw from the proposed distribution ppðxkj?Þ samples xm
k :
xm
k 2ppðxkjx0; x1; . . .; xk21; z0; z1; . . .; zkÞ
2. Update the importance (or quality) weights up to a normalizing constant:
^wm
k 5 wm
k21
pðzkjxm
k Þpðxm
k jxm
k21Þ
ppðxm
k jxm
0 ; xm
1 ; . . .; xm
k21; z0; z1; . . .; zkÞ
ð12:68aÞ
If as proposed distribution, the prior distribution pðxm
k jxm
k21Þ is used, that is,
ppðxm
k jxm
0 ; xm
1 ; . . .; xm
k21; z0; . . .; zkÞ 5 pðxm
k jxm
k21Þ, the above expression for ^wm
k reduces to:
^wm
k 5 wm
k21pðzkjxm
k Þ
ð12:68bÞ
3. Compute the normalized weights as:
wm
k 5 ^wm
k

X
M
q51
^wq
k
ð12:69Þ
4. Compute an estimate of the effective sample size (i.e., number of particles) ESS as:
ESS 5 1

X
M
m51
ðwm
k Þ2
ð12:70Þ
5. If ESS , Nmax, where Nmax is a maximum (threshold) number of particles, then perform
population resampling as described below.
The PF method can be applied to the mobile robot localization problem.
Actually, we have three phases:
1. Prediction—use a model for the simulation of the effect that a control action has on the
set of particles with the noise added (see Eq. (12.32a)).
2. Update—use information obtained from sensors to update the weights such that to
improve the probability distribution of the WMR’s motion (see Eq. (12.32b)).
3. Resampling—draw M particles from the current set of particles with probabilities propor-
tional to their weights and replace the current set of particles with this set selecting the
weights as wm
k 5 1=Mðm 5 1; 2; . . .; MÞ.
516
Introduction to Mobile Robot Control

Note 1: Resampling is performed if ESS in Eq. (12.70) is ESS , Nmax (see
Section 13.13, Phase 3).
Note 2: A Matlab code for EKF- and PF-based SLAM can be found in:
http://www.frc.ri.cmu.edu/projects/emergencyresponse/radioPos/index.html.
A schematic representation of the PF loop is as shown in Figure 12.13:
We recall that after a certain number of iterations, k, most weights become
nearly zero and therefore the corresponding particles have negligible importance.
This fact is indeed faced by the resampling process which replaces these low-
importance particles with particles of higher importance.
12.8.5
Omnidirectional Vision-Based SLAM
The main issue in SLAM is that the robot can build a map of the environment and
localize its position/posture in this map on the basis of noisy measurements of char-
acteristic points (landmarks) from the images. The measurements of a catadioptric
camera are particularly suited for use in conjunction with EKF in which every state
variable or output variable is represented by its mean and covariance. The motion
of the WMR and the measurements are described by Eqs. (12.32a) and (12.32b),
where f and h are nonlinear functions of their arguments. The state vector xk
at time t 5 kT ðk 5 0; 1; 2; . . .Þ contains the position vector xr;k of the vehicle, and
the vector xf of the map features. The disturbances/noises wk and vk are
assumed Gaussian with zero means and known covariances. The EKF equations
involve the Jacobian matrices of f and h. For the catadioptric camera system, these
A priori quantities
A posteriori quantities
Measurements
Zk–1 = {z1, z2, ..., zk–1}
Zk = {z1, z2, ..., zk} = {Zk–1, zk}
Measurements (New
measurement zk)
M particles xi
k–1
w
m
k–1 (m = 1, 2, ..., M)
with weights
Prediction of new particle
(state vector) xk
with updated weights
Normalized weights
Do resampling if
where
New value of state vector
Set (w
m
k  = 1/M, m = 1, 2, ..., M)
p (xk|Zk–1) = Σ w
m
k–1f(x
m
k–1)
m=1
1
ˆ
p (xk|Zk) = Σ w
m
k  f(x
m
k )
m=1
M
q=1
M
ˆ
ˆ
ˆ
ˆ
wm
k  = wm
k–1P(zk|xm
k )
29
wm
k  = wm
k  / Σ wq
k 
m=1
M
ESS = 1 / Σ (w
m
k  )2 < Nmax
f(x
m
k–1) ~ p (xk|xk–1)
Figure 12.13 PF loop structure.
517
Mobile Robot Localization and Mapping

matrices have been derived in Example 9.4, and can be used to formulate the EKF
equations in a straightforward way. Two examples of this application are provided
in Refs. [24,25].
Example 12.5
In this example, we will develop the full EKF equations for the localization of a unicycle-
type WMR using (fusing) two kinds of sensors [26]:
G Encoders on the powered wheels that provide a measure of the incremental rotation
angles over a possibly varying sampling period Δt.
G A set of sonar sensors mounted on the platform of the WMR.
The WMR kinematic equations _xQ 5 v cos φ, _yQ 5 v sin φ, and _φ 5 ω are discretized
using the first-order approximation _xðtÞC½xðk 1 1Þ 2 xðkÞ=Δt, giving the discrete-time
model:
xQðk 1 1Þ 5 xQðkÞ 1 TvðkÞcos φðkÞ 1 w1ðkÞ
yQðk 1 1Þ 5 yQðkÞ 1 TvðkÞsin φðkÞ 1 w2ðkÞ
φðk 1 1Þ 5 φðkÞ 1 TωðkÞ 1 w3ðkÞ
ð12:71Þ
where t 5 kT (or t 5 k for simplicity) and T 5 Δt (constant). This model is nonlinear in
the state vector:
xðkÞ 5 ½xQðkÞ; yQðkÞ; φðkÞT
ð12:72aÞ
The control vector is:
uk 5 ½vðkÞ; ωðkÞT
ð12:72bÞ
The disturbance inputs wiðkÞ
ði 5 1; 2; 3Þ are zero-mean Gaussian white noises with
known identical variances σ2
wi 5 σ2
w. The model in Eq. (12.71) can be written in the com-
pact form of Eq. (12.32a):
xðk 1 1Þ 5 fðxðkÞ; uðkÞÞ 1 wðkÞ
ð12:73Þ
where:
xðkÞ 5
xQðkÞ
yQðkÞ
φðkÞ
2
4
3
5;
fðx; uÞ 5
xQ 1 Tv cos φ
yQ 1 Tv sin φ
φ 1 Tω
2
4
3
5
ð12:74Þ
with additive disturbance wðkÞ. The linear approximation of Eq. (12.73) is (see
Section 12.8.2):
xðk 1 1Þ 5 fð^xðkjkÞ; 0Þ 1 AðkÞ½xðkÞ 2 ^xðkjkÞ 1 BðkÞuðkÞ 1 wðkÞ
ð12:75Þ
518
Introduction to Mobile Robot Control

where ^xðkjkÞ is the current estimate of xðkÞ (based on measurements up to time k), and:
fð^xðkjkÞ; 0Þ 5
^xQðkjkÞ
^yQðkjkÞ
^φðkjkÞ
2
64
3
75
ð12:76aÞ
AðkÞ 5 @f
@x


^xðkjkÞ;0
5
1
0
0
0
1
0
0
0
1
2
4
3
5
ð12:76bÞ
BðkÞ 5 @f
@u


^xðkjkÞ;0
5
T cos ^φðkjkÞ
0
T sin ^φðkjkÞ
0
0
T
2
4
3
5
ð12:76cÞ
Let xr;i; yr;i be the coordinates of the ith sonar sensor in the vehicles coordinate frame
Qxryr, and φr;i the orientation angle of the ith sonar in Qxryr as shown in Figure 12.14.
The discrete-time kinematic equations in Oxy of the ith sonar are:
xiðkÞ 5 xQðkÞ 1 xr;i sin φðkÞ 1 yr;i cos φðkÞ
yiðkÞ 5 yQðkÞ 2 xr;i cos φðkÞ 1 yr;i sin φðkÞ
φiðkÞ 5 φðkÞ 1 φr;iðkÞ
ð12:77Þ
Now, consider a plane (surface) Πj as shown in Figure 12.15, and a sonar i with
beam width δ (all sonars are assumed to have the same beam width δ). Each plane
Πj can be represented in Oxy by pj
n and θj
n, where:
G
pj
n is the (normal) distance of Πj from the origin O of the world coordinate frame.
G
θj
n is the angle between the normal line to the plane Πj and the Ox direction.
The distance dj
i of sonar i from the plane Πj is (see Figure 12.15):
dj
i 5 ðpj
n 2 xi cos θj
n 2 yi sin θj
nÞ
ð12:78aÞ
φ
φr,i
xr
xQ
x
y
O
Sonar i
yr
yQ
Q
Figure 12.14 WMR with incremental
encoders on the wheels and sonar sensors
on its platform.
519
Mobile Robot Localization and Mapping

for
φi 2 δ=2 # θj
n # φi 1 δ=2
ð12:78bÞ
The measurement vector zðkÞ contains the encoder and sonar measurements and
has the form:
zðkÞ 5
z1ðkÞ
z2ðkÞ


5 hðxðkÞÞ 1 nðkÞ
ð12:79Þ
where nðkÞ is a Gaussian zero-mean white measurement noise with covariance
matrix RðkÞ 5 diag½σ2
nðkÞ; σ2
nðkÞ; . . ., and:
z1ðkÞ 5 ½xQðkÞ1n1ðkÞ; yQðkÞ1n2ðkÞ; φðkÞ1n3ðkÞT
ð12:80aÞ
z2ðkÞ 5 ½dj
1ðkÞ 1 n4ðkÞ; dj
2ðkÞ 1 n5ðkÞ; . . .; dj
msðkÞ 1 n31msðkÞ
ð12:80bÞ
with i 5 1; 2; . . .; ms (ms is the number of sonars), dj
iðkÞ the distance measurement
with respect to the plane Πj provided by the ith sonar, j 5 1; 2; . . .; mp (mp is the
number of planes), and hðxðkÞÞ 5 ½xQðkÞ; yQðkÞ; φðkÞ; d1
1ðkÞ; d1
2ðkÞ; . . .; dmp
ms T
For simplicity, we assume (without loss of generality) that only one sensor and
one plane are used (i.e., ms 5 1 and mp 5 1, in which case:
hðxðkÞÞ 5 ½xQðkÞ; yQðkÞ; φðkÞ; d1
1ðkÞT
ð12:81Þ
Linearizing the measurement Eq. (12.79) about ^xðkjk 2 1Þ, where ^xðkjk 2 1Þ is
the estimate of xðkÞ using measurement data up to time k 2 1, and noting that hðÞ
does not depend on u, we get:
zðkÞ 5 hð^xðkjk 2 1ÞÞ 1 CðkÞ½xðkÞ 2 ^xðkjk 2 1Þ 1 nðkÞ
ð12:82Þ
xi
x
y
O
Sonar i
Plane Πj
φi
θ
j
n
p j
n
d j
i
yi
δ
Figure 12.15 Geometry of sonar i.
520
Introduction to Mobile Robot Control

where:
hð^xðkjk 2 1ÞÞ 5 zðkjk 2 1Þ 5 ½^xQðkjk21Þ; ^yQðkjk21Þ; ^φðkjk21Þ; d1
1ðkjk21ÞT
ð12:83aÞ
CðkÞ 5 @h
@x


^xðkjk21Þ
5
1
0
0
0
1
0
0
0
1
2cos θ1
n
2sin θ1
n
xr;1cosð^φðkjk 2 1Þ 2 θ1
nÞ
2yr;1sinð^φðkjk 2 1Þ 2 θ1
nÞ
2
66664
3
77775
ð12:83bÞ
Now, having available the linearized model of Eqs. (12.75) and (12.82), we can
use directly the linear Kalman filter Eqs. (12.5)(12.12). For the robot localization
we use the steps described in Section 12.7.1.
Therefore:
Step 1: One-Step Prediction
^xðk 1 1jkÞ 5 fð^xðkjkÞ; 0Þ 1 BðkÞuðkÞ
5 ^xðkjkÞ 1 BðkÞuðkÞ
Σðk 1 1jkÞ 5 AðkÞΣðkjkÞATðkÞ 1 QðkÞ
QðkÞ 5 diag½σ2
w; σ2
w; σ2
w
ð12:84Þ
Step 2: Sensor Observation
A new set of sensors’ measurements zðk 1 1Þ is taken at time k 1 1
Step 3: Matching
The measurement innovation process:
~zðk 1 1jkÞ 5 zðk 1 1Þ 2 ^zðk 1 1jkÞ
5 zðk 1 1Þ 2 hð^xðk 1 1jkÞÞ
ð12:85Þ
is constructed.
Step 4: Position Estimation
Kðk 1 1Þ 5 Σðk 1 1jk 1 1ÞCTðk 1 1ÞR21ðk 1 1Þ
ð12:86aÞ
^xðk 1 1jk 1 1Þ 5 ^xðk 1 1jkÞ 1 Kðk 1 1Þ~zðk 1 1jkÞ; xð0j0Þ 5 x0
ð12:86bÞ
Σðk 1 1jk 1 1Þ 5 Σðk 1 1jkÞ 2 Kðk 1 1ÞCðk 1 1ÞΣðk 1 1jkÞ;
Σð0j0Þ 5 Σ0
ð12:86cÞ
with Σ0 a given symmetric positive definite matrix.
521
Mobile Robot Localization and Mapping

Any valid controller that guarantees the stability of the closed-loop system can be
used. Here, a dynamic state feedback linearizing and decoupling controller derived by the
method of Example 6.7 will be used [26,27]:
We select the output vector:
y 5
y1
y2


5
xQ
yQ


ð12:87aÞ
and differentiate it to obtain:
_y 5
_y1
_y2


5 H1ðφÞu;
H1ðφÞ 5
cos φ
0
sin φ
0


;
u 5
v
ω


ð12:87bÞ
Clearly, ω does not influence _y, and so we introduce a dynamic compensator
_z 5 μ; v 5 z
ð12:88Þ
where μ is the linear WMR acceleration. Therefore, Eq. (12.87a) takes the form:
_y 5 z cos φ
sin φ


ð12:89Þ
Differentiating Eq. (12.89) we get:
€y 5 _z cos φ
sin φ


1 z_φ
2sin φ
cos φ


5 H2ðφÞ μ
ω


ð12:90Þ
where H2ðφÞ is the nonsingular decoupling matrix:
H2ðφÞ 5
cos φ
2z sin φ
sin φ
z cos φ


;
H21
2 ðφÞ 5
cos φ
sin φ
2ðsin φÞ=z
ðcos φÞ=z


ð12:91Þ
with z 5 v 6¼ 0.
Thus, defining new inputs w1 and w2 such that:
€y1 5 w1;
€y2 5 w2
ðinput 2 output decoupled systemÞ
ð12:92Þ
and solving Eq. (12.90) for ½μ; ωT 5 ½_z; ωT we get:
_z
ω


5
cos φ
sin φ
2ðsin φÞ=z
ðcos φÞ=z


w1
w2


which is the desired dynamic state feedback linearizing and decoupling controller:
_z 5 w1 cos φ 1 w2 sin φ
ð12:93aÞ
v 5 z
ð12:93bÞ
ω 5 ðw2 cos φ 2 w1 sin φÞ=z
ð12:93cÞ
522
Introduction to Mobile Robot Control

with:
€y1 5 w1;
€y2 5 w2
ð12:93dÞ
Numerical Results—We consider the following values of the system parameters and ini-
tial conditions:
G
Initial WMR position: xQð0Þ 5 1:4 m; yQð0Þ 5 1:3 m; φð0Þ 5 45
G
Sonar position in Qxryr: xr;1 5 0:5 m;
yr;1 5 0:5 m;
φr;1 5 0
G
Plane position: p1
n 5 7:0 m;
θ1
n 5 45
G
Disturbance/noise:
Q 5 diag½0:1; 0:1; 0:1;
R 5 diag½1023; 1023; 1023; 1023
The desired trajectory starts at y1;dð0Þ 5 xQ;dð0Þ 5 1:5 m, y2;dð0Þ 5 xQ;dð0Þ 5 1:5 m
and is a straight line that forms an angle of 45 with the world coordinate Ox axis
as shown in Figure 12.16.
The new feedback controller ½w1ðtÞ; w2ðtÞT is designed, as usual, using the linear
PD algorithm:
w1 5 €y1;d 1 Kp1ðy1;d 2 y1Þ 1 Kd1ð_y1;d 2 _y1Þ
w2 5 €y2;d 1 Kp2ðy2;d 2 y2Þ 1 Kd2ð_y2;d 2 _y2Þ
Figure 12.17A shows the trajectory obtained using odometric and sonar mea-
surements and the desired trajectory. Figure 12.17B shows the desired orientation
φd, and the real orientation φ obtained using the EKF fusion. As we see, the perfor-
mance of the EKF fusion is very satisfactory.
yQ,d(o)
yr
x
y
O
yQ(o)
xQ(o)
xQ,d(o)
d1
1
Q
xr
Π
j
Desired
trajectory
θn=45°
1
φ =45°
Figure 12.16 Desired trajectory of the WMR.
523
Mobile Robot Localization and Mapping

However, the experiment showed that the trajectories obtained by pure odometric
measurements, and by combined odometric and sonar data are similar. This is perhaps
due to modeling reasons. Here, the case of using both sensors was treated jointly by a
single EKF. A better picture of the improvement can be obtained by using the sensor
fusion process of Figure 12.8 by first computing separately the (local) estimates pro-
vided by each individual kind of sensor, and then finding the combined estimate accord-
ing to (12.31a) and (12.31b) which eliminates the effect of any redundant information.
Further improvement in the accuracy of the state vector estimation can be
obtained using the PF approach. This is because the EKF assumes Gaussian distur-
bances and noise processes which in general is not so, whereas in the PF no
assumptions are made about the probability distributions of the stochastic distur-
bances and noises. In PF, a set of weighted particles (state vector estimates) is
employed, which evolve in parallel. Each iteration of the PF involves particle
updating and weight updating, and convergence is assured through resampling by
which particles with low weights are replaced by particles of high weights. Indeed,
simulation experiments using PF for the above sensor fusion system example, with
number of particles N $ 1000, have shown that the PF is superior than the EKF. As
the number of particles was increased, better estimates of the WMR state vector
were obtained, of course with higher computational effort.
Example 12.6
It is desired to give a solution to the problem of estimating the leader’s velocity vl in a
leaderfollower vision-based control system (Figure 12.18).
Solution
The image processing algorithms give the range and bearing data [28], that is:
Range: L2
lf 5 ðxl2xfÞ2 1 ðyl2yfÞ2
ð12:94aÞ
Bearing: γlf 5 π=2 2 φf 1 θlf;
tgθlf 5 ðyl 2 yfÞ=ðxl 2 xfÞ
ð12:94bÞ
y
(A)
(B)
Time (s)
4.5
4
3.5
3
2.5
1.5
1
2
x
1
1.5
2
2.5
3
3.5
4
4.5
0
0.2
0.4
0.6
0.8
1
1
2
3
4
5
6
7
8
9
Φ (rad)
Figure 12.17 (A) Actual trajectory obtained using both kinds of sensors versus the desired
trajectory (?) and (B) corresponding curves for the orientation φd (?. . .) and φ (—).
524
Introduction to Mobile Robot Control

Differentiating Llf and γlf we get:
_Llf 5 ð~xlf_~xlf 1 ~ylf_~ylfÞ=Llf
ð12:95aÞ
_γlf 5 ð~xlf_~ylf 2 ~ylf_~xlfÞ=L2
lf 2 φf
ð12:95bÞ
where:
~xlf 5 xl 2 xf;
~ylf 5 yl 2 yf
ð12:95cÞ
Defining the angle ζlf as:
ζlf 5 γlf 1 φf 2 φl
ð12:96Þ
and assuming that vl 5 const: and ωl 5 const:, we find that the state vector under estima-
tion is described by the nonlinear model:
_XðtÞ 5 FðX; uÞ 1 wðtÞ;
u 5 ½vf; ωfT
ð12:97aÞ
where wðtÞAR6 is a Gaussian stochastic disturbance input process with zero-mean and
known covariance matrix and:
X 5
φl
vl
ωl
Llf
γlf
φf
2
6666664
3
7777775
;
FðX; uÞ 5
ωl
0
0
vl sin ζlf 2 vf sin γlf
ðvl cos ζlf 2 vf cos γlfÞ=Llf 2 ωf
ωf
2
6666664
3
7777775
ð12:97bÞ
The measured output zðtÞ is:
zðtÞ 5
Llf
γlf


5 HðXÞ 1 nðtÞ
ð12:97cÞ
where nðtÞ is the zero-mean sensor measurement Gaussian noise with known covariance
matrix.
C
V
v1
vf
Llf
C
V γlf
(xf, yf)
(xl, yl)
φf
φl
Figure 12.18 Leaderfollower system with
leader pose ðxl; yl; φlÞ and follower pose
ðxf; yf; φfÞ.
525
Mobile Robot Localization and Mapping

Discretizing Eqs. (12.97a)(12.97c) with first-order approximation and sampling
period T gives the nonlinear discrete-time model:
Xðk 1 1Þ 5 FdðXðkÞ; uðkÞÞ 1 wðkÞ
ð12:98aÞ
zðkÞ 5 HðXðkÞÞ 1 nðkÞ
ð12:98bÞ
where:
FdðXðkÞ; uðkÞÞ 5
φl1Tωl
vl
ωl
Llf1Tðvl sin ζlf2vf sin γlfÞ
γlf1Tfðvl cos ζlf2vf cos γlfÞ=Llfg2ωf
φf1Tωf
2
66666666664
3
77777777775
t5kT
ð12:98cÞ
We will apply the EKF technique to the nonlinear model given by Eqs. (12.98a)
(12.98c) which is linearized, as usual, about the current estimate ^XðkjkÞ to give (see
Eqs. (12.32a)(12.35b)):
Xðk 1 1Þ 5 Fdð^XðkjkÞ; 0Þ 1 AðkÞ½XðkÞ 2 ^XðkjkÞ 1 BðkÞuðkÞ 1 wðkÞ
ð12:99aÞ
where the estimate ^XðkjkÞ is based on measurements ZðkÞ 5 fzð0Þ; zð1Þ; . . .; zðkÞg, and:
Fdð^XðkjkÞ; 0Þ 5
^φlðkjkÞ 1 T ^ωlðkjkÞ
^vlðkjkÞ
^ωlðkjkÞ
^LlfðkjkÞ 1 T ^vlðkjkÞsin ^ζlfðkjkÞ
^γlfðkjkÞ 1 T½^vlðkjkÞcos ^ζlfðkjkÞ=Llf
^φfðkjkÞ
2
6666666666664
3
7777777777775
ð12:99bÞ
AðkÞ 5 @Fd
@X


^XðkjkÞ;0
5
1
0
T
j
0
0
0
0
1
0
j
0
0
0
0
0
1
j
0
0
0
0
T sin ^ζlfðkjkÞ
0
j
1
0
0
0
ðT=^LlfðkjkÞÞcos ^ζlfðkjkÞ
0
j
0
1
0
0
0
0
j
0
0
1
2
66666664
3
77777775
ð12:99cÞ
526
Introduction to Mobile Robot Control

BðkÞ 5 @Fd
@u


^XðkjkÞ;0
5
0
0
0
0
0
0
2T sin ^γlfðkjkÞ
0
2ðT=^LlfðkjkÞÞcos ^γlfðkjkÞ
21
0
T
2
666666664
3
777777775
ð12:99dÞ
The measurement Eq. (12.97c) is linear by its own, and can be written as:
zðkÞ 5
Llf
γlf


5 HðXÞ 1 nðtÞ 5 CXðkÞ 1 nðtÞ
ð12:100aÞ
where:
C 5
0
0
0
1
0
0
0
0
0
0
1
0


ð12:100bÞ
The linear state-space and measurement model given by Eqs. (12.99a)(12.99d),
(12.100a), and (12.100b) has the standard form of Eqs. (12.2a) and (12.2b) or
Eqs. (12.75) and (12.82), and so the formulation of the EKF equations is straightforward.
As an exercise the reader can write down these equations and write a computer program
with proper values of the parameters.
Example 12.7
Given that a sensor is available to measure the range and the bearing of observed land-
marks, it is desired to outline an algorithm that updates the pose (position and orienta-
tion) of a car-like WMR.
Solution
The algorithm is similar to the velocity estimation algorithm of the previous example.
Here, the kinematic model of the car-like WMR is:
_x 5 v1 cos φ
_y 5 v1 sin φ
_φ 5 ð1=Dv1tgψÞ
_ψ 5 2 aψ 1 bv2;
a . 0
ð12:101Þ
where “a” is the decay parameter of the steering angle ψ, which is assumed to be con-
strained as jψj , ψmax , 90, and b is an input gain.
The control vector is:
u 5 ½u1; u2T 5 ½v1; v2T
527
Mobile Robot Localization and Mapping

To use the EKF for updating the position and orientation of the WMR, we first discretize
(as usual) this model and get (see Eqs. (12.98a) and (12.98b)):
Xðk 1 1Þ 5 FdðXðkÞ; uðkÞÞ 1 wðkÞ
ð12:102aÞ
zðkÞ 5
z1ðkÞ
z2ðkÞ
^
zmðkÞ
2
6664
3
7775 5
h1ðp1; XðkÞÞ
h2ðp2; XðkÞÞ
^
hmðpm; XðkÞÞ
2
6664
3
7775 1 nðkÞ
ð12:102bÞ
where wðkÞ and nðkÞ 5 ½n1ðkÞ; n2ðkÞ; . . .; nmðkÞT are zero-mean Gaussian white processes
with known covariance matrices QðkÞ and RðkÞ, respectively, and ziðkÞ 5 hiðpi; XðkÞÞ is the
position of the ith landmark:
piðkÞ 5 ½pixðkÞ; piyðkÞT
ði 5 1; 2; . . .; mÞ
ð12:102cÞ
The output function hiðpi; XðkÞÞ is:
hiðpi; XðkÞÞ 5
½ðpix2xðkÞÞ21ðpiy2yðkÞÞ21=2
tg21½ðpiy 2 yðkÞÞ=ðpix 2 xðkÞÞ 2 φðkÞ
"
#
ð12:102dÞ
The function FdðXðkÞ; uðkÞÞ is given by:
FdðXðkÞ; uðkÞÞ 5
xðkÞ 1 Tu1 cos φðkÞ
yðkÞ 1 Tu1 sin φðkÞ
φðkÞ 1 ðT=DÞu1tgψðkÞ
ψðkÞ 2 TaψðkÞ 1 Tbu2ðkÞ
2
6664
3
7775
ð12:103aÞ
where:
XðkÞ 5 ½xðkÞ; yðkÞ; φðkÞ; ψðkÞT;
uðkÞ 5 ½u1ðkÞ; u2ðkÞT
ð12:103bÞ
The linearized state and measurement model is found to be:
Xðk 1 1Þ 5 Fdð^XðkjkÞ; 0Þ 1 AðkÞ½XðkÞ 2 ^XðkjkÞ 1 BðkÞuðkÞ 1 wðkÞ
ð12:104aÞ
zðkÞ 5 Hdð^XðkjkÞ; 0Þ 1 CðkÞ½XðkÞ 2 ^XðkjkÞ 1 vðkÞ
ð12:104bÞ
where:
Fdð^XðkjkÞ; 0Þ 5
^xðkjkÞ
^yðkjkÞ
^φðkjkÞ
ð1 2 TaÞ ^ψðkjkÞ
2
6664
3
7775
528
Introduction to Mobile Robot Control

AðkÞ 5 @Fd
@X


^XðkjkÞ;0
5
1
0
2Tu1 sin ^φðkjkÞ
0
0
1
Tu1 cos ^φðkjkÞ
0
0
0
1
ðT=DÞu1 sec2 ^φðkjkÞ
0
0
0
1 2 Ta
2
66664
3
77775
BðkÞ 5 @Fd
@u


^XðkjkÞ;0
5
T cos ^φðkjkÞ
0
T sin ^φðkjkÞ
0
ðT=DÞtg ^ψðkjkÞ
0
0
Tb
2
66664
3
77775
Hdð^XðkjkÞ; 0Þ 5 ½hT
1ðp1; ^XðkjkÞÞ; . . .; hT
mðpm; ^XðkjkÞÞT
hiðpi; ^XðkjkÞÞ 5
ðpix2 ^xðkjkÞÞ21ðpiy2 ^yðkjkÞÞ2

1=2
tg21½ðpiy 2 ^yðkjkÞÞ=ðpix 2 ^xðkjkÞÞ 2 φðkÞ
"
#
CðkÞ 5 @Hd
@X


^XðkjkÞ;0
5
@hT
1
@X


;
@h2
@X

T
; . . .;
@hm
@X

T
"
#
^XðkjkÞ;0
@hi
@X


^XðkjkÞ
5
Δ^xðkjkÞ=^λp
Δ^yðkjkÞ=^λp
0
0
2Δ^yðkjkÞ=^λ
2
p
Δ^xðkjkÞ=^λ
2
p
21
0
"
#
where Δ^xkðkjkÞ 5 ^xðkjkÞ 2 pix, Δ^ykðkjkÞ 5 ^yðkjkÞ 2 piy, and ^λp 5 ½Δ^x2ðkjkÞ1Δ^y2ðkjkÞ1=2.
Again, the model given by Eqs. (12.104a) and (12.104b) is a standard linear time-
varying stochastic model to which the EKF is applied directly. The four steps:
G One-step prediction
G Sensor observation
G Matching
G Pose estimation
provide the solution (see Section 12.7.1). In the matching step, we can use the χ2 cri-
terion to validate the matching (independence) of each landmark (sensor) measurement.
Measurements that do not satisfy this criterion are ignored. It is remarked that the control
input uðkÞ in the term BðkÞuðkÞ of Eq. (12.104a) is assumed to be known (since our aim
here is purely the estimation of XðkÞ). This input can be selected by a proper method
(Chapters 59) for achieving a desired control objective as it was done in Example 12.5.
529
Mobile Robot Localization and Mapping

References
[1] Papoulis A. Probability, random variables and stochastic processes. New York, NY:
Mc Graw-Hill; 1965.
[2] Meditch JS. Stochastic optimal linear estimation and control. New York, NY:
Mc Graw-Hill; 1969.
[3] Anderson BDO, Moore JB. Optimal filtering. Prentice Hall, NJ: Englewood Cliffs;
1979.
[4] Borenstein J, Everett HR, Feng L. Navigating mobile robots: sensors and techniques.
Wellesley, MA: A.K. Peters Ltd; 1999.
[5] Adams MD. Sensor modeling design and data processing for automation navigation.
Singapore: World Scientific; 1999.
[6] Davies ER. Machine vision: theory, algorithms, practicalities. San Francisco, CA:
Morgan Kaufmann; 2005.
[7] Bishop RH. Mechatronic systems, sensors and actuators: fundamentals and modeling.
Boca Raton, FL: CRC Press; 2007.
[8] Leonard JL. Directed sonar sensing for mobile robot navigation. Berlin: Springer;
1992.
[9] Kleeman, L. Advanced sonar and odometry error modeling for simultaneous localiza-
tion and map building. In: Proceedings of the 2004 IEEE/RSJ international conference
on intelligent robots and systems, Sendai, Japan, 2004, p. 186671.
[10] Betke M, Gurvis L. Mobile robot localization using landmarks. IEEE Trans Rob
Autom 1997;13(2):25163.
[11] Andersen CS, Concalves JGM. Determining the pose of a mobile robot using triangula-
tion: a vision based approach. Technical Report No I. 195-159, European Union Joint
Research Center, December 1995.
[12] Hu H, Gu D. Landmark-based navigation of industrial mobile robots. Int J Ind Rob
2000;27(6):45867.
[13] Castellanos JA, Tardos JD. Mobile robot localization and map building: a multisensor
fusion approach. Berlin: Springer; 1999.
[14] Chang KC, Chong CY, Bar-Shalom Y. Joint probabilistic data association in distrib-
uted sensor networks. IEEE Trans Autom Control 1986;31:889.
[15] Durrant-Whyte HF. Uncertainty geometry in robotics. IEEE Trans Rob Autom 1988;4
(1):2331.
[16] Vidal Calleja TA. Visual navigation in unknown environments. Ph.D. Thesis, IRI,
Univ. Polit. de Catalunya, Barcelona, 2007.
[17] Guivant JE, Nebot EM. Optimization of the simultaneous localization and map-
building algorithm for real-time implementation. IEEE Trans Rob Autom 2001;17
(3):24257.
[18] Gordon NJ, Salmond DJ, Smith AFM. Novel approach to nonlinear/nonGaussian
Bayesian estimation. Proc IEE Radar Signal Process 1993;140(2):10713.
[19] Rekleitis I, Dudek G, Milios E. Probabilistic cooperative localization and mapping in
practice. Proc IEEE Rob Autom Conf 2003;2:190712.
[20] Rekleitis I, Dudek G, Milios E. Multirobot collaboration for robust exploration. Ann
Math Artif Intell 2001;31(14):740.
[21] Bailey T, DurrantWhyte H. Simultaneous localization and mapping (SLAM), Part I.
IEEE Rob Autom Mag 2006;13(2):99110 Part II, ibid, (3):10817.
530
Introduction to Mobile Robot Control

[22] Doucet A, De Freitas N, Gordon NJ. Sequential Monte Carlo methods in practice.
Berlin: Springer; 2001.
[23] Crisan D, Doucet A. A survey of convergence results on particle filtering methods for
practitioners. IEEE Trans Signal Process 2002;50(3):73646.
[24] Rituerto A, Puig L, Guerrero JJ. Visual SLAM with an omnidirectional camera. In:
Proceedings of twentieth international conference on pattern recognition (ICPR),
Istanbul, Turkey, 2326 August, 2010, p. 34851.
[25] Kim JM, Chung MJ. SLAM with omnidirectional stereo vision sensor. In: Proceedings
of 2003 IEEE/RSJ international conference on intelligent robots and systems, Las
Vegas, NV, October, 2003, p. 44247.
[26] Rigatos GG, Tzafestas SG. Extended Kalman filtering for fuzzy modeling and multi-
sensor fusion. Math Comput Model Dyn Sys 2007;13(3):25166.
[27] Oriolo G, DeLuca A, Venditteli M. WMR control via dynamic feedback linearization:
design implementation and experimental validation. IEEE Trans Control Sys Technol
2002;10(6):83552.
[28] Das AK, Fierro R, Kumar V, Southall B, Spletzer J, Taylor CJ. Real-time mobile
robot. In: Proceedings of 2001 international conference on robotics and automation,
Seoul, Korea, 2001, p. 171419.
531
Mobile Robot Localization and Mapping

13 Experimental Studies
13.1
Introduction
In this book, we have presented fundamental analytic methodologies for the derivation
of wheeled mobile robot (WMR) kinematic and dynamic models, and the design of
several controllers. Unavoidably, these methodologies represent a small subset of the
variations and extensions available in the literature, but the material included in the
book is over sufficient for its introductory purposes, taking into account the required
limited size of the book. All methods reported in the open literature are supported by
simulation experimental results, and in many cases, the methods were applied and
tested in real research mobile robots and manipulators.
The objective of this chapter is to present a collection of experimental simulation
and physical results drawn from the open literature for most of the methodologies
considered in the book. Specifically, the book provides sample results obtained under
various artificial and realistic conditions. In most cases, the desired paths to be fol-
lowed and the trajectories to be tracked are straight lines, curved lines, or circles or
appropriate combinations of them. The experiments presented in the chapter cover
the following problems treated in the book:
G
Lyapunov-based model-based adaptive and robust control
G
Pose stabilization and parking control using polar, chained, and Brockett-type integrator
models
G
Deterministic and fuzzy sliding mode control
G
Vision-based control of mobile robots and mobile manipulators
G
Fuzzy path planning in unknown environments (local, global, and integrated globallocal
path planning)
G
Fuzzy tracking control of differential drive WMR
G
Neural network-based tracking control and obstacle avoiding navigation
G
Simultaneous localization and mapping (SLAM) using extended Kalman filters (EKFs)
and particle filters (PFs)
By necessity, hardware, software, or numerical details are not included in the
chapter. But, most of the simulation results were obtained using Matlab/Simulink
functions. The reader is advised to reproduce some of the results with personal sim-
ulation or physical experiments.
Introduction to Mobile Robot Control. DOI: http://dx.doi.org/10.1016/B978-0-12-417049-0.00013-4
© 2014 Elsevier Inc. All rights reserved.

13.2
Model Reference Adaptive Control
Model reference adaptive control was studied in Chapter 7. Two equivalent control-
lers were derived. The first of them is given by Eqs. (7.40a), (7.40b), and (7.41),
and the second is given by Eq. (7.77). In both cases, the mass m and the moment
of inertia I of the WMR were considered to be unknown constants or slowly vary-
ing parameters. Starting the operation of the controllers using available initial
guessed values of these parameters, the adaptive controllers perform simultaneously
two tasks, namely, updating (adapting) the parameter values and stabilizing the
tracking of the desired trajectories. As the time passes, the parameters approach
their true values, and the tracking performance is improved. This fact has been ver-
ified by simulation of the controllers [1,2]. Figure 13.1AC shows the performance
of the first controller achieved with gain values Kx 5 Kφ 5 Ky 5 5 and adaptation
parameters γ1 5 γ2 5 10. The true parameter values are m 5 1 and I 5 0:5. The
mobile robot initial pose is xð0Þ 5 0, yð0Þ 5 0, and φð0Þ 5 0. Figure 13.1A depicts
the convergence of the errors ~x 5 xd 2 x, ~y 5 yd 2 y, and ~φd 5 φd 2 y using the true
1.5
(A)
(B)
(C)
1
0.5
5
–0.5
1.5
1
0.5
5
–0.5
1.5
1
0.5
5
–0.5
–1
0
1
2
3
4
5
Time (sec)
6
7
8
9
10
0
1
2
3
4
5
Time (sec)
6
7
8
9
10
0
1
2
3
4
5
Time (sec)
6
7
8
9
10
Figure 13.1 (A) Convergence with the true m and I values. (B) Convergence of the
nonadaptive controller when the initial parameters are m 5 4 and I 5 2. (C) Convergence of
the adaptive controller with the same initial parameter values.
Source: Reprinted from Ref. [1], with permission from European Union Control Association.
534
Introduction to Mobile Robot Control

values. Figure 13.1B and C shows the error convergence performance when the ini-
tial parameter values of the nonadaptive and adaptive controller are m 5 4 and
I 5 2, respectively.
The convergence when the true parameter values are used in the controller is
achieved at 3 s (Figure 13.1A). The convergence of the nonadaptive controller when
the parameter values are different from the true ones is achieved after 10 s
(Figure 13.1B), and the convergence of the adaptive controller is achieved at about
4 s. The above results illustrate the robustness of the adaptive controller against
parameter uncertainties. Analogous performance was obtained with the second
adaptive controller [2]. The WMR parameters used in Ref. [2] are β1 5 β2 5 0:5
(true values of unknown dynamic parameters with known signs), γ1 5 γ2 5 10, and
K4 5 K5 5 100. Two cases were studied, namely, (i) a 45 straight-line desired tra-
jectory xdðtÞ 5 0:5t; ydðtÞ 5 0:5t; φdðtÞ 5 π=4 rad and (ii) a unity radius circular tra-
jectory, centered at the origin, produced by a moving point with constant linear
velocity 0.5 m/s and initial pose ½xdð0Þ; ydð0Þ; φdð0ÞΤ 5 ½1; 0; π=2Τ. In the first case,
the initial pose of the robot was ½xð0Þ; yð0Þ; φð0ÞΤ 5 ½1; 0; 0Τ. Clearly, φð0Þ 5 0
means that the robot was initially directed toward the positive x-axis. In the second
case, the initial pose of the robot was ½xð0Þ; yð0Þ; φð0ÞΤ 5 ½0; 0; 0Τ. The results
obtained in these two cases are shown in Figure 13.2 [2]. We see that in the first
case, the robot is initially moving backward, and maneuvers to track the desired lin-
ear trajectory, whereas in the second case, the robot moves immediately toward the
circular trajectory in order to track it. From Figure 13.2B and D, we see that the
tracking (zero error reaching) times in the two cases are 2 and 1.5 s, respectively.
13.3
Lyapunov-Based Robust Control
Several simulation experiments were carried out for the differential drive WMR using
the Lyapunov-based robust controller of Section 7.6, which involves the nonrobust lin-
ear controller part (7.109) and the robust controller part (7.112) [3]. In one of them, a
circular reference trajectory was considered. The parameters of the nonrobust propor-
tional feedback control part used are Knrob 5 diag½0:16; 0:16. The corresponding para-
meters for the robust controller are Krob 5 diag½0:96; 0:96. The simulation started
with zero initial errors. The resulting ðx; yÞ trajectories and the φðtÞ trajectory are
depicted in Figure 13.3. We see that the trajectories obtained with the nonrobust con-
troller possess a deviation from the desired (reference) ones, which is eliminated by
the robust controller. (Note that the dashed curves represent the desired trajectory and
the solid lines the actual trajectory.)
A second simulation experiment was performed including an external distur-
bance pushing force ðF 5 2200 NÞ. The resulting ðx; yÞ and φðtÞ trajectories are
shown in Figure 13.4, where again dotted lines indicate the reference trajectories
and solid lines the actual ones.
One can see that the nonrobust controller cannot face the disturbance and leads
to an unstable system. However, the robust controller leads to excellent tracking
performance despite the existence of the large disturbance.
535
Experimental Studies

13.4
Pose Stabilizing/Parking Control by a Polar-Based
Controller
The simulation was carried out using the WMR polar model (5.73a)(5.73c) and
the v; ω controllers (5.77) and (5.79):
v 5 lK1ðcos ζÞl;
K1 . 0
ω 5 K2ζ 1 K1ðcos ζÞðsin ζÞðζ 1 q2ψÞ=ζ;
K2 . 0
where l is the distance (position error) of the robot from the goal and ζ is the steer-
ing angle ζ 5 ψ 2 φ (see Figure 5.11). Several cases were studied in Ref. [4].
Figure 13.5A shows the evolution of the robot’s parking maneuver from a start
pose until it goes to the desired goal pose.
6
(A)
(B)
(D)
(C)
1
0.5
0
–0.5
–1
–1.5
–2
0
2
4
6
8
10
5
4
3
Reference
trajectory
Reference
trajectory
Robot
trajectory
Robot
trajectory
ex
ei(f)
ey
y
y
y
2
1
0
–1
1
1.6
1.4
1.2
ei(f)
ex
ey
1
0.8
0.6
0.4
0.2
0
–0.2
0.5
0
–0.5
–1
–1
–0.5
0
X
0.5
1
0
2
Time (s)
4
6
8
10
12
14
0
1
2
3
4
X
X
5
Figure 13.2 WMR trajectories and tracking errors. (A) Desired and actual robot trajectory
for the straight-line case. (B) Corresponding trajectory tracking errors. (C) Desired and
actual robot trajectory in the circular case. (D) Tracking errors corresponding to (C).
Source: Reprinted from Ref. [2], with permission from Elsevier Science Ltd.
536
Introduction to Mobile Robot Control

We see that the starting pose is ðx; y; φÞ 5 ð21; 1; 3π=4Þ and the target pose is
ðx; y; φÞ 5 ð0; 0; 0Þ. The maneuver trajectory shown in Figure 13.5A was obtained with
gain values K1 5 3, K2 5 6, and q2 5 1. The initial error in polar form corresponding
to the initial pose is ðl; ζ; ψÞ 5 ð
ﬃﬃﬃ
2
p
; 2π; 2π=4Þ where ζð0Þ 5 ψð0Þ 2 φð0Þ 5
2π=4 2 3π=4 5 2π.
Figure 13.5B shows the resulting robot maneuvering for a different set of start-
ing poses where the initial robot’s orientation is always φð0Þ 5 π=2.
Note that the robot always approaches the parking pose with positive velocity,
which is needed because ζ ! 0 as the controller operates.
13.5
Stabilization Using Invariant Manifold-Based
Controllers
The results to be presented here were obtained using the extended (double)
Brockett integrator model (6.89) of the full differential drive WMR described
by the kinematic and dynamic Eqs. (6.85a)(6.85e). A typical “parallel park-
ing” problem was considered with initial pose ðx; y; φÞ0 5 ð0; 2; 0Þ and final
4
(A)
(B)
0
–2
–4
–6
–8
2
0
–2
–4
–6
6
4
2
0
–2
–4
–5
0
y (m)
x (m)
x (m)
Orientation (rad)
0
–2
–4
–6
–8
Orientation (rad)
5
–5
0
y (m)
5
0
10
20
Time (s)
30
0
10
20
Time (s)
30
Figure 13.3 (A) Performance of the nonrobust controller. (B) Performance of the robust
controller.
Source: Reprinted from Ref. [3], with permission from American Automatic Control
Council.
537
Experimental Studies

4
(A)
(B) 6
4
2
0
–2
–4
0
–2
–4
–6
–8
0
10
Time (s)
20
30
0
10
Time (s)
20
30
2
0
–2
–4
–5
0
y (m)
x (m)
x (m)
Orientation (rad)
0
–2
–4
–6
–8
Orientation (rad)
5
–5
0
y (m)
5
Figure 13.4 Performance of the controllers for the disturbed case. (A) Nonrobust controller
and (B) robust controller.
Source: Reprinted from Ref. [3], with permission from American Automatic Control
Council.
1
Start
Goal
Y
(A)
(B)
1
0.5
0
–0.5
–1
–1
–0.5
0
0.5
1
0.8
0.6
0.4
0.2
0
–1
–0.5
0 X
Figure 13.5 Parking movement using a polar coordinate stabilizing controller. (A) Robot
parking maneuvering and (B) parking maneuver of the robot starting from a different initial
pose ðφð0Þ 5 π=2Þ.
Source: Reprinted from Ref. [4], with permission from Institute of Electrical and Electronic
Engineers.
538
Introduction to Mobile Robot Control

pose ðx; y; φÞf 5 ð0; 0; 0Þ [5]. The controller (6.92a) and (6.92b), derived using
the invariant-attractive manifold method, has been applied, namely:
u1 5 2k1x1 2 k2 _x1 1 k3x3x2=ðx2
1 1 x2
2Þ;
x2
1 1 x2
2 6¼ 0
u2 5 2k1x2 2 k2 _x1 2 k3x3x1=ðx2
1 1 x2
2Þ;
x2
1 1 x2
2 6¼ 0
with gains k1 5 0:25; k2 5 0:75; and k3 5 0:25. The parameters of the robot are
m 5 10 kg and I 5 15 kg m2. The control inputs are the pushing force F and the
steering torque N. The resulting time evolution of these inputs is shown in
Figure 13.6. When the initial conditions x1ð0Þ and x2ð0Þ do not violate the con-
troller singularity condition, that is, when x2
1ð0Þ 1 x2
2ð0Þ 6¼ 0, the state converges
to an invariant manifold, and once on the invariant manifold, no switching takes
place. When x2
1ð0Þ 1 x2
2ð0Þ 5 0, we use initially the controller (6.132):
u2 5 b sgnðsÞ;
u1 5 0
50
40
30
20
10
0
Torque/force
–10
–20
–30
–40
–50
0
2
4
6
Time (seconds)
N (Nm)
F (N)
8
10
12
Figure 13.6 Control inputs (force and torque) for the dynamic and constrained kinematic
stabilizing control.
Source: Reprinted from Ref. [5], with permission from Institute of Electrical and Electronic
Engineers.
539
Experimental Studies

derived in Example 6.10, which drives the system out of the singularity region, and
then the above controller which stabilizes to zero the system [5].
When the force and torque are constrained, the system cannot track the reference
velocities provided by the kinematic controller.
The invariant manifold controllers in Eqs. (6.117a) and (6.117b) for the double
integrator (kinematic) WMR model and Eq. (6.130b) for the extended Brockett inte-
grator (full kinematic and dynamic) model were also simulated [6]. Figure 13.7A
and B shows the time performance of the kinematic controller in Eqs. (6.117a) and
(6.117b) with xð0Þ 5 21:5 m; yð0Þ 5 4 m, and φð0Þ 5 22:3 rad, sampling period
Δt 5 0:01 s, and control gains k1 5 4; k2 5 10. The very quick convergence of
xðtÞ; yðtÞ, and φðtÞ to zero is easily seen.
The performance of the full dynamic controller (6.130b) was studied for the case
where xð0Þ 5 21:5 m; yð0Þ 5 4 m; φð0Þ 5 22:3 rad; vð0Þ 5 21 m=s; ωð0Þ 5 1 rad=s;
Δt 5 0:01 s; k1 5 1:5, and k2 5 9. The physical parameters of the WMR used are
m 5 10 kg, I 5 2 kg m2; r 5 0:03 m, and 2a 5 0:06 m. A slightly better performance
was observed.
13.6
Sliding Mode Fuzzy Logic Control
Here, some simulation results will be provided for the reduced complexity sliding
mode fuzzy logic controller (RC-SMFLC) (8.39) [7,8]:
uðtÞ 5 _υdðtÞ 1
I0
RðrÞ
m 1 IRðrÞ υ2ðtÞ 1 K
peðtÞ 1 Kd _eðtÞ 1 uRC-SMFLCðtÞ
The uncertainty in the robot system lies in the variation of slopes @z=@r. The
robot may mount an uphill slope @z=@r . 0 or go down a downhill slope
@z=@r , 0, where both the magnitude and the sign of the slope are unknown and
time varying.
7
20
x
v
ω
y
10
0
–10
–20
–30
–40
–50
–60
6
5
4
x(m)
y(m)
φ(rad)
v(m/s)
ω(rad/s)
3
2
1
0
–1
–2
–3
–40
5
Time (s)
Time (s)
10
15
0
5
10
15
θ
(A)
(B)
Figure 13.7 (A,B) Performance of the kinematic controller (6.117). (A) Trajectories of the
states x; y, and φ. (B) Time evolution of linear velocity vðtÞ and angular velocity ωðtÞ.
Source: Reprinted from Ref. [6], with permission from Institute of Electrical and Electronic
Engineers.
540
Introduction to Mobile Robot Control

The case of moving on an uphill was first considered. The fuzzy controller imi-
tates the action of a human driver. If the slope increases, then the driver has to
press more the acceleration pedal to maintain the velocity at the desired value, by
compensating the increased effect of the gravitational term ½mg=ðm 1 IRðtÞÞz0ðrÞ. If
the robot acceleration exceeds the desired value, the pressure on the acceleration
must be reduced. These “increasedecrease” acceleration actions, with gradually
reducing amplitude, continue until the desired action is reached. The operation of
the RC-SMFLC controller is similar when the robot moves on a downhill slope.
Figure 13.8 shows the velocity fluctuation and the corresponding robot trajectory
for a robot moving on an uphill with slope varying between 5% and 10%. The
desired velocity was 4.2 m/s. Figure 13.9 shows the results of the robot when the
robot descends a downhill with slope in the interval 210%; 25%
½
.
The RC-SMFLC controller was also tested in the car-parking problem at a
certain position of the uphill slope (here, at xd 5 9 m), which is in the middle of
the uphill slope. Clearly, this is a simplified form of the backing up control of a
truck on an uphill slope, which is a difficult task for all but the most skilled
drivers. To achieve backing up the desired position, the driver has to attempt
backing, move forward, back again, go forward again, and so on. Figure 13.10
shows the performance of the position controller when the robot mounts an
uphill (Figure 13.10A and B) and when the robot goes down a downhill slope
(Figure 13.10C and D).
13.7
Vision-Based Control
Here, a number of simulation studies carried out using vision-based controllers will
be presented [911].
10
(A)
(B)
2
1.8
1.6
1.4
1.2
1
0.8
0.6
0.4
0.2
00
5
10
15
20
9
8
7
6
Velocity (m/s)
Z (m)
5
4
3
2
1
0
0
50
100
k Iterations
x (m)
150
200
Figure 13.8 Robot performance ascending an uphill of uncertain slope. (A) Velocity
fluctuation and (B) robot trajectory.
Source: Reprinted from Ref. [7], with permission from Elsevier Science Ltd.
541
Experimental Studies

10
(A)
(B)
2
1.8
1.6
1.4
1.2
1
0.8
0.6
0.4
0.2
0
0
5
10
15
20
X (m)
9
8
7
Velocity (m/sec)
Z (m)
6
5
4
3
2
1
0
0
50
100
150
k iterations
200
Figure 13.9 Robot performance descending a downhill of uncertain slope. (A) Velocity
fluctuation and (B) robot trajectory.
Source: Reprinted from Ref. [7], with permission from Elsevier Science Ltd.
(A)
(B)
(C)
(D)
k Iterations
k Iterations
Position x (m)
Z (m)
Z (m)
X (m)
X (m)
Position x (m)
10
9
8
7
6
5
4
3
2
1
0
0
20
40
60
80
100
2
1.8
1.6
1.4
1.2
1
0.8
0.6
0.4
0.2
0
2
1.8
1.6
1.4
1.2
1
0.8
0.6
0.4
0.2
0
0
2
4
6
8
12
10
0
2
4
6
8
12
10
10
9
8
7
6
5
4
3
2
1
0
0
20
40
60
80
100
Figure 13.10 Control of the robot position on an uphill or downhill 65% slope. (A) Uphill
position evolution. (B) Robot trajectory corresponding to (A). (C) Downhill position
evolution. (D) Robot trajectory corresponding to (C).
Source: Reprinted from Ref. [7], with permission from Elsevier Science Ltd.
542
Introduction to Mobile Robot Control

13.7.1
LeaderFollower Control
First, some experimental results, obtained by the vision-based leaderfollower
tracking controller in Eqs. (9.75a) and (9.75b), with the linear velocity being esti-
mated by Eq. (9.66), will be given [9]. These results were obtained for two Pioneer
2DX mobile robots, one following the other (Figure 13.11). The system hardware
involved a frame grabber PXC200 in the vision part, which allowed image captur-
ing by a SONY-EV-D30 camera mounted on the follower robot, and an image pro-
cessor in charge of the calculation of the control actions (Pentium II-400 MHzPC).
Figure 13.12 shows the evolution of the distance to the leader. The task of the
follower was to follow the leader robot maintaining a desired distance ld 5 0:50 m
Figure 13.11 Pioneer 2DX robots used in the experiment.
Source: Courtesy of R. Carelli.
1000
l/(mm)
900
800
700
600
500
400
300
200
100
0
0
2
4
6
Time (s)
8
10
Figure 13.12 Time evolution of the leaderfollower distance.
Source: Courtesy of R. Carelli.
543
Experimental Studies

and an angle ϕd 5 0. The desired control gains used are Kl 5 200 and Kϕ 5 10,
and the values of μl and μϕ were μl 5 0:005 and μϕ 5 0:1.
Figure 13.13 shows the time profiles of the angles ϕ and θ, the control signals v
and ω of the follower robot, the estimated velocity of the leader, and the trajectory
of the follower robot.
From Figures 13.12 and 13.13 we see that the visual control scheme with vari-
able gains according to Eq. (9.75c) assures the achievement of the desired overall
objectives, while avoiding possible control saturation.
13.7.2
Coordinated Open/Closed-Loop Control
Second, some results obtained using the coordinated hybrid open/closed-loop control-
ler of Example 10.2 will be given for a mobile manipulator [10]. The MM consists of
a differential drive mobile platform and a 3-DOF manipulator. The manipulator
Jacobian is found analogously to the 2-DOF manipulator, by adding to Eq. (10.10) an
extra term to each element of J corresponding to the third link (e.g., J11 is equal to
2l1 sin θ1 2 l2 sinðθ1 1 θ2Þ 2 l3 sinðθ1 1 θ2 1 θ3Þ). The link lengths used in the
(A)
v(mm/s)
Θ(rad)
Φ(rad)
ω(rad/s)
mm/s
y(mm)
Time (s)
(B)
Time (s)
(C)
(D)
Time (s)
x (mm)
0.1
0.05
–0.05
–0.1
0
2
4
6
8
10
200
150
100
50
0
10
5
0
–5
0
2
4
6
8
10
0
2
4
6
8
10
0
0.4
0.2
–0.2
–0.4
0
200
500
450
400
350
300
250
200
150
100
50
0
0
500
1000
1500
2000
2500
3000
3500
150
100
50
0
–50
–100
–150
–200
0
2
4
6
8
10
2
4
6
8
10
0
Figure 13.13 Performance of the leaderfollower visual controller. (A) Angles φ and θ. (B)
Control signals v and ω. (C) Estimated velocity of the leader robot. (D) Trajectory of the
follower robot.
Source: Courtesy of R. Carelli.
544
Introduction to Mobile Robot Control

simulation are l1 5 0:8 m; l2 5 0:5 m; and l3 5 0:3 m. To maximize the manipulability
index w 5 jdet Jj 5 ljsin θ2j of the manipulator, its desired configuration was selected
as qd 5 ½0; π; 0Τ. The kinematic model of the MM from the input velocities to the var-
iation rates of the features is given in Eq. (10.62). The combination of controllers
(10.63) and (10.64) for the feedback controller part was used with kfðqmÞ, being given
by Eq. (10.66). The hybrid (open-loop/closed-loop) controller scheme (10.68) with
parameters σ0 5 0:15; μ 5 1, and Kp 5 0:5 was used (see Eqs. (10.65) and (10.67)).
The times T1 and T2 for the open-loop controller (10.71) were selected as T1 5 5 s and
T2 5 10 s. Three landmarks ½x; yΤ, that is, ½1:7; 7Τ, ½2:4; 9Τ, and ½1; 7Τ, were used.
The initial pose of the mobile platform was at the origin, and the manipulator’s initial
configuration was selected close to the desired configuration. Figure 13.14 shows
the features’ camera measurement errors eiðfÞ; i 5 1; 2; 3, the mobile base pose errors
exðtÞ; eyðtÞ, the mobile platform ðx; yÞ trajectory, and the platform’s control inputs vðtÞ
and ωðtÞ. The manipulability index value during the closed-loop control phase was
w 5 0:4, but it was sharply decreasing during the open-loop control phase. The camera
0.7
Error
Error
v,ω
Error
t[sec]
(a)
t[sec]
(b)
t[sec]
(c)
t[sec]
(d)
θ1
ex
v[t]
ω[t]
ey
θ2
θ3
2
1
0
–1
–2
–3
–4
–5
0
20
40
60
80
100 120 140 160 180
0.6
0.5
0.4
0.3
0.2
0.1
0
–0.1
–0.2
–0.3
5
4
3
2
1
0
–0.5
0
0.5
1
1.5
2
2.5
3
3.5
0
20
40
50
80
100 120 140 160 180
0.5
1
0
–0.5
0
20
40
60
80
100 120 140 160 180
Figure 13.14 Performance of the coordinated hybrid visual controller of the MM.
(A) Feature camera measurement errors. (B) Mobile platform pose errors. (C) Mobile
Cartesian space trajectory. (D) Mobile platform’s control inputs v and ω, which, in order to
avoid the occurrence of limit cycles (due to the open-loop action), were applied only when
the platform’s pose errors were larger than a selected threshold.
Source: Reprinted from Ref. [10], with permission from Institute of Electrical and Electronic
Engineers.
545
Experimental Studies

measurement errors eiðfÞ were strictly decreasing during the closed-loop control phase
but remained constant during the open-loop control phase.
Finally, some results obtained using a full-state image-based controller for the
simultaneous mobile platform and end effector (camera) pose stabilization are
depicted in Figure 13.15. This controller was designed via a merging of the results of
Sections 9.2, 9.4, 9.5, and 10.3, as described in Section 10.5. The distance d of the
target S along the xw-axis of the world coordinate frame was selected as d 5 2:95 m
(see Figure 9.4), the camera focal length is lf 5 1 m, and the manipulator’s link
lengths are l1 5 0:51 m and l2 5 0:11 m [11].
13.7.3
Omnidirectional Vision-Based Control
Experimental results through the two-step visual tracking control procedure of
Section 9.9.3 were derived in Ref. [12] using a synchro-drive WMR and a PD
controller:
_r 5 Jy
im½KpeΤðfΤÞ 1 Kd_eΤðfΤÞ
where eΤðfΤÞ 5 fΤ;d 2 fΤ is the target’s feature error vector. The visual control loops
worked at a rate of 33 ms without any frame loss, with the execution controller run-
ning at a rate of 7.5 ms. Originally, the target was at a distance 1 m from the robot,
and then moved away along a straight path by 60 cm. Figure 13.16A shows a snapshot
of the image acquired with the omnivision system, Figure 13.16B shows the robot and
the target to be followed, and Figure 13.16C shows the evolution of the error signal in
pixels. When the target stopped moving, the error was about 6 2 pixels.
0.0
0.40
0.25
0.00
–0.25
–0.2
–0.4
y-axis
x-axis
Time
(x,y) Trajectory
Control inputs (v,ω)
–0.6
–0.8
–0.35
0.60
0.35
0.75
0
200
400
600
800
1000
(A)
(B)
Figure 13.15 (A) Platform ðx; yÞ trajectory from an initial to a final pose. (B) Control inputs
ðv; ωÞ profiles.
Source: Reprinted from Ref. [11], with permission from Springer Science1Business BV.
546
Introduction to Mobile Robot Control

We will now present a method and some experimental results derived for a car-
like WMR:
_x 5 v cos φ;
_y 5 v sin φ;
_φ 5 ðv=DÞtgψ
using a calibrated catadioptric vision system, modeled as shown in Figure 9.26, in
which case, A 5 I;
Gc 5 I, and:
xim 5 FðxÞ
where FðxÞ is given in Eq. (9.121b). The coordinate frame Fr attached to the robot is
assumed to coincide with the mirror frame Fm which implies that the camera frame
and the WMR are subjected to identical kinematic constraints (Figure 13.17A).
The problem is to drive the x-axis of the WMR (control) frame parallel to a
given 3D straight line L, while keeping a desired constant distance yc;d to the line
(Figure 13.17B and C) [13]. The line L is specified by the position vector of a point
Σ on the line and its cross-product with the direction vector uL 5 ½uLx; uLy; uLzΤ
(A)
(B)
(C)
8
6
4
2
0
Error (pixels)
–2
–4
–6
–8
0
50
100
150 200
250
Iteration
300 350
400 450
500
Figure 13.16 (A) Omnidirectional image showing the robot and target. (B) The robot and
target. (C) Error signal. The robot started its movement at iteration 125.
Source: Courtesy of J. Okamoto Jr [12].
547
Experimental Studies

of the line, that is, ðuL; OmΣ
!
3 uLÞ in the mirror frame. Defining the vector
n 5 OmΣ
!
3 uL=jj OΣ
!
3 uLjj 5 ½nx; ny; nzΤ, the line L is expressed in the so-called
Plu¨cker coordinates ½uΤ
L; nΤΤ with uΤ
Ln 5 0. Let S be the intersection between the
interpretation plane Π (defined by the line L and the mirror focal point Om) and the
mirror surface. Clearly, S is the line’s projection in the mirror surface. The projec-
tion S of the line L in the catadioptric image plane is obtained using the relation
xim 5 fðxÞ, where x is an arbitrary point x 5 ½x; y; zΤ on L, and:
fðxÞ 5
x
y
z 1 ζ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
x2 1 y2 1 z2
p
2
664
3
775
ðζ 5 d0Þ
(A)
(B)
(C)
x
y
O
yr
D
ψ
φ
xr
yc,d
Qc
Fc(camera frame)
Image
plane 
S
uL
Om
Fm(mirror frame)
L
Σ
n
x
y
z
h
Σ
Lateral 
deviation 
yc
Path followed
3D line
Figure 13.17 (A) A 3D line L is projected into a conic in the image plane. (B) Variables
and parameters of the robot and the problem. (C) Pictorial representation of the task to be
performed.
Source: Adapted from Refs. [13,14].
548
Introduction to Mobile Robot Control

under the condition:
nΤx 5 nxx 1 nyy 1 nzz 5 0
which expresses the fact that n is orthogonal to the interpretation plane. Inverting
the relation xim 5 fðxÞ, we get:
x 5 f21ðximÞ;
xim 5 ½xim; yim; zimΤ
where:
f21ðximÞ 5
mðximÞxim
mðximÞyim
mðximÞzim 2 ζ
2
664
3
775
with:
mðximÞ 5
zimζ 1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
z2
im 1 ð1 2 ζ2Þðx2
im 1 y2
imÞ
q
x2
im 1 y2
im 1 z2
im
Introducing the above expression for x into the condition nΤx 5 0, we get:
nΤf21ðximÞ 5 ½nx; ny; nz
mðximÞxim
mðximÞyim
mðximÞzim 2 ζ
2
664
3
775 5 0
which after some algebraic manipulation can be written in the standard quadratic
form [13,14]:
xΤ
imHxim 5 0
H 5
n2
xð1 2 ζ2Þ 2 nm
x ζ2
nxnyð1 2 ζ2Þ
ð2m 2 3Þnxnm21
z
nxnyð1 2 ζ2Þ
n2
yð1 2 ζ2Þ 2 nm
z ζ2
ð2m 2 3Þnynm21
z
ð2m 2 3Þnxnm21
z
2m 2 3
ð
Þnynm21
z
nm
z
2
66664
3
77775
where m 5 2 in the general case, and m 5 1 in the parabolic mirrororthographic
camera case. In expanded form, the above quadratic equation can be expressed in
the following normalized form:
h0x2
im 1 h1y2
im 1 2h2ximyim 1 2h3xim 1 2h4yim 1 1 5 0
549
Experimental Studies

where hi 5 ai=a5
ði 5 1; 2; 3; 4; 5Þ, and:
a0 5 λ½n2
xð1 2 ζ2Þ 2 nm
z ζ2;
a1 5 λ½n2
yð1 2 ζ2Þ 2 nm
z ζ2
a2 5 λnxnyð1 2 ζ2Þ;
a3 5 λð2m 2 3Þnxnm21
z
a4 5 λð2m 2 3Þnynm21
z
;
a5 5 λnz
where λ is a scaling factor. The nondegenerate case nz 6¼ 0 is considered. The state
of the WMR (and camera) is:
xc 5 ½xc; yc; φΤ
where xc; yc are the world coordinates of the camera and φ the angular deviation
with respect to the straight line. The task is achieved when the lateral deviation yc
is equal to the desired deviation yc;d, and the angular deviation is zero. These devia-
tions are expressed in terms of image features as [14]:
yc 5 h=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
h2
3 1 h2
4
q
;
φ 5 tg21 h3=h4
assuming h2
3 1 h2
4 6¼ 0 and h3=h4 6¼ 0 (i.e., that the line L does not lie on the xy
plane of the camera frame Fc, and the x-axis of the mirror is not perpendicular to
the line L). The parameter h is shown in Figure 13.17C. Choosing z1 5 xc in the
WMR chained model we get:
u1 5 v cos φ
Further, choosing z2 5 yc we have _z2 5 v sin φ, and so z3 5 tgφ (for φ 6¼ π=2).
Now:
u2 5 _z3 5 _φ=cos2 φ
5 ðv=DÞtgψ=cos2 φ
Dividing the equations of the chained model by u1 we get:
z0
1 5 1;
z0
2 5 z3;
z0
3 5 u3
where u3 5 u2=u1. Selecting the state feedback controller u3 for the above linear
system as:
u3 5 2Kdz3 2 Kpz2
we get the closed-loop error (deviation) equation:
_z02 1 Kdz0
2 1 Kpz2 5 0
550
Introduction to Mobile Robot Control

Therefore, selecting proper values for the gains Kd and Kp assures that z2 ! 0
and z3 ! 0, as t ! N (with desired convergence rates) independently of the lon-
gitudinal velocity as long as v 6¼ 0. From the relations z2 5 yc and z3 5 tgφ, it fol-
lows that also yc ! 0 and φ ! 0. The expression for the feedback control steering
angle ψ is found from:
u3 5 u2=u1 5 ðv=DÞtgψ=cos2 φ
v cos φ
5 ð1=DÞtgψ=cos3 φ
that is:
tgψ 5 Dðcos3 φÞu3
5 2Dðcos3 φÞ½Kdz3 1 Kpz2
5 2Dðcos3 φÞ½Kdtgφ 1 Kpyc
In terms of the measured features h3 and h4, this controller is expressed as:
tgψ 5 2D cos3 tg21 h3
h4




Kd
h3
h4
1 Kp
h
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
h2
3 1 h2
4
p
"
#
The parameter h is obtained from proper measurement.
The above controller was tested by simulation for both a paracatadioptric sen-
sor (parabolic mirror plus orthographic camera) and a hypercatadioptric sensor
(hyperbolic mirror plus perspective camera) [13,14]. In the first case, the image of
a line is a circle and in the second case is a part of a conic. Figure 13.18A and B
shows the line configuration and actual robot trajectory in the world coordinate
frame, and the profiles of the lateral and angular deviations provided by the
simulation.
Similar results were reported for the case of hypercatadioptric system [13].
13.8
Sliding Mode Control of Omnidirectional Mobile Robot
Here, some simulation results for the omnidirectional mobile manipulator of
Section 10.3.4, with three links ðnm 5 3Þ will be given. The sliding mode controller
(10.53b) has been applied, replacing the switching term sgnðsÞ by the saturation
function approximation satðsi=φiÞ as described in Section 7.2.2. The parameter
values of the MM are given in Table 13.1. The masses and moments of inertia
were assumed to contain uncertainties in intervals between 6 5% and 6 30% of
551
Experimental Studies

their nominal values (Table 13.1). The simulation results were obtained using the
extreme values as the real ones, and the mean values of the true and extreme values
as the estimated values for the computation of the control signals.
Two cases were studied:
1. Linear desired trajectory of the platform’s center of gravity
2. Circular desired trajectory of the platform’s center of gravity
In both cases, an angular platform velocity of 1=s with respect to the world coordi-
nate frame was applied. For the joint variables, a ramp reference was used. The results
obtained via simulation are shown in Figures 13.19 and 13.20 [15]. The results
obtained using the exact values in the computed torque method showed that the slid-
ing mode controller (with the ‘‘sat’’ function approximation) can indeed face large
parametric uncertainties and follow the desired trajectories successfully.
(A)
(B)
3D line
0
–0.5
–1
–1.5
–2
0.5
1
1.5
2
0
0.5
1
1.5
2
0
2
4
6
8
10
Lateral deviation (m)
Angular deviation (rad)
0
50 100 150 200 250 300 350 400 450 500
–0.7
–0.6
–0.5
–0.4
–0.3
–0.2
–0.1
0
0.1
0
50 100 150 200 250 300 350 400 450 500
–0.6
–0.4
–0.2
0.4
0
0.2
Figure 13.18 Omnidirectional visual servoing results with ðKp; KdÞ 5 ð1; 2Þ. (A) Desired line
and actual robot trajectory, (B) Evolution of lateral and angular deviations.
Reprinted from [13], with permission from Institute of Electrical and Electronic Engineers.
552
Introduction to Mobile Robot Control

13.9
Control of Differential Drive Mobile Manipulator
13.9.1
Computed Torque Control
In the following discussion, some simulation results obtained by the application of the
computed torque controller ((10.49a) and (10.49b)) to the 5-DOF MM of Figure 10.8
will be provided. The parameters of the MM are given in Table 13.2 [16].
The desired performance specifications for the error dynamics were selected to
be ζ 5 1 and ωn 5 4 (settling time Ts 5 1 s). Then, the gain matrices Kp and Kυ in
Eq. (10.49b) are:
Kp 5 diag½36; 36; 36; 36;
Kυ 5 diag½12; 12; 12; 12
The control torques of the left and right wheel and the forearms and upper arms
that result with the above gains are shown in Figure 13.21A and B. The desired
trajectories of the manipulator front point Ob and the end effector tip E are shown
in Figure 13.21C. The corresponding actual animated paths are shown in
Figure 13.21D.
Table 13.1 Parameter Values of the Robotic Manipulator
Physical Variable
True Value
Extreme Uncertain
Values
Mass of each wheel
0.5 kg
0.525 kg (15%)
Radius of each wheel
0.0245 m
0.0245 m
Mass of the platform
30 kg
33 kg (110%)
Wheel distance from the platform COG
0.178 m
0.178 m
Platform moment of inertia with respect
to axis z
0.93750 kg m2 0.98435 kg m2 (15%)
Mass of link 1
1.25 kg
1.375 kg (110%)
Length of link 1
0.11 m
0.11 m
Link 1 moment of inertia with respect to axis x 0.01004 kg m2 0.010542 kg m2 (15%)
Mass of link 2
4.17 kg
5.421 kg (130%)
Length of link 2
0.5 m
0.5 m
Position of link’s COG along its axis
0.25 m
0.25 m
Link 2 moment of inertia with respect to axis x 0.34972 kg m2 0.367206 kg m2 (15%)
Link-2 moment of inertia with respect to axis z 0.00445 kg m2 0.0046725 kg m2 (15%)
Mass of link 3
0.83 kg
1.0790 kg (130%)
Length of link 3
0.10 m
0.10 m
Position of link’s 3 COG along its axis
0.05 m
0.05 m
Link 3 moment of inertia with respect to axis x 0.00321 kg m2 0.0033705 kg m2 (15%)
Link 3 moment of inertia with respect to axis z
0.00089 kg m2 0.0009345 kg m2 (15%)
Linear friction coefficient for all rotating joints 0.1 N m s
0.13 (130%)
553
Experimental Studies

13.9.2
Control with Maximum Manipulability
Here, we will present a few representative simulation results obtained in Refs.
[17,18] for the MM of Figure 10.11. The platform parameters used are those of the
LABMATE platform (Transition Research Corporation) and the manipulator para-
meters are m1 5 m2 5 4 kg; l1 5 l2 5 l 5 0:4 m; and I1 5 I2 5 0:0533 kg m2.
Angle between the moving and the
world coordinate frame
Time (s)
Time (s)
Time (s)
10
20
30
40
50
60
–6
–5
–4
–3
–2
–1
0
1
0
1
2
3
–1
–2
–3
0
–0.1
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0
–0.2
0.2
0.4
0.6
0.8
1
1.2
(rad)
Θ1(t)(rad)
Θ2(t)(rad)
Θ3(t)(rad)
y
10
20
30
40
50
60
0
Trajectory of the platform COG
0
10
20
30
40
50
60
0
10
20
30
40
50
60
0
Figure 13.19 Control performance for the tracking of the circular trajectory of the
platform’s center of gravity using computed torque control.
554
Introduction to Mobile Robot Control

The center of gravity of each link was assumed to be at the midpoint of the link,
and the sampling period was set to T 5 0:01 s.
13.9.2.1
Problem (a)
The controller ((10.55a) and (10.55b)) was applied leading to the decoupled system
(10.55c). This system was controlled by a diagonal (decoupled) PD tracking
(rad)
y
Θ1(t)(rad)
Θ2(t)(rad)
Θ3(t)(rad)
Trajectory of the platform e.g.
Time (sec)
–6
–5
–4
–3
–2
–1
0
1
-0.2
0
0.2
0.4
0.6
0.8
1
1.2
–2
–1
0
1
2
3
–3
10
20
30
40
50
60
0
Angle between the moving and the
world coordinate frame
10
20
30
40
50
60
0
Time (sec) 
10
20
30
40
50
60
0
Time (sec) 
10
20
30
40
50
60
0
–0.1
0
0.1
0.3
0.6
0.7
0.8
0.9
0.2
0.4
0.5
–0.1
0
0.1
0.3
0.6
0.7
0.8
0.9
0.2
0.4
0.5
–0.1
0
0.1
0.3
0.6
0.7
0.8
0.9
0.2
0.4
0.5
Figure 13.20 Control performance for the tracking of the circular trajectory of the
platform’s center of gravity using sliding mode control.
555
Experimental Studies

controller for the desired specifications ωn 5 2:0 and ζ 5 1:2. The overdamping
ζ 5 1:2 was selected to match the slow response of the platform. The velocity along
the path was assumed constant.
Figure 13.22A and B shows the desired and actual trajectories of the wheel mid-
point Q and the reference (end effector) point for the case where the desired path
Table 13.2 Parameter Values of the MM
Platform
m 5 50 kg;
J0 5 1:417 kgm2;
a 5 0:15 m;
r 5 0:10 m;
lb 5 0:5 m
Link 1
m1 5 4 kg;
J1 5 0:030 kg m2;
l1 5 0:30 m;
lc1 5 0:15 m
Link 2
m2 5 3:5 kg;
J2 5 0:035 kg m2;
l2 5 0:35 m;
lc2 5 0:12 m
6
4
2
0
Torque (Nm)
–2
–4
–6
0.5
y(m)
y(m)
0
–0.5
–1
–1.5
–2
0.5
0
–0.5
–1
–1.5
–2
0
0.5
Front point
desired
path
End-effector
desired path
1
1.5
x (m)
2
2.5
0
0.5
1
1.5
x (m)
2
2.5
Final
position
Initial position
3
2
Upper
arm
Fore arm
1
0
Torque (Nm)
–1
–2
–30
1
2
3
4
5
6
7
8
0
1
2
3
4
t (s)
(A)
(B)
(C)
(B)
t (s)
Right wheel
Left wheel
5
6
7
8
Figure 13.21 (A) Torques of driving wheels. (B) Forearm and upper arm torques. (C)
Desired paths of the platform’s front point Ob and end effector. (D) Actual animated paths
of the platform and the end effector.
Source: Courtesy of E. Papadopoulos [16].
556
Introduction to Mobile Robot Control

was a straight line 45 from the x-axis. The notch on one side of each square plat-
form shows the forward direction of motion.
The manipulability was kept at its maximum except for the initial maneuvering
period of the platform (about 5 s).
13.9.2.2
Problem (b)
Four cases were simulated with or without compensation as follows:
Case 1: Compensation of both the arm and the platform
Case 2: Compensation of the arm only
Case 3: Compensation of the platform only
Case 4: No compensation of the dynamic interaction
Figure 13.23A and B shows representative results for the case of a circular
desired path.
13.10
Integrated Global and Local Fuzzy Logic-Based Path
Planner
Here, simulation results of an integrated navigation (global planning and local/reactive
planning) scheme for omnidirectional robots using fuzzy logic rules will be presented
[19,20]. A description of this two-level model (Figure 13.24) will be first provided.
(A)
(B)
x
y
Desired
Actual
x
y
12.5
10
7.5
5
2.5
–2.5
2.5
5
7.5
10
12.5
15
15.00
14.00
15.00
12.00
11.00
10.00
15.00
8.00
7.00
6.00
5.00
4.00
3.00
2.00
1.00
0.00
–1.00
–2.00
0.00
5.00
10.00
15.00
Figure 13.22 (A) Trajectory of the point Q for the 45 straight path. (B) Desired and actual
trajectories of the end effector for the path (A).
Source: Reprinted from Ref. [17], with permission from Institute of Electrical and Electronic
Engineers.
557
Experimental Studies

The global path planner is based on the potential field method and provides a
nominal path between the current configuration of the robot and its goal. The local
reactive planner generates the appropriate commands for the robot actuators to fol-
low the global path as close as possible, while reacting in real time to unexpected
events by locally adapting the robot’s movements, so as to avoid collision with
unpredicted or moving obstacles. This local planner consists of two separate fuzzy
controllers for path following and obstacle avoidance (Figure 13.25).
The inputs to the local planner are the path provided by the global path planner
and the locally observed obstacles. The output is the steering command for the
actuators of the robot. This steering command is a weighed sum of the outputs of
the two fuzzy controllers as shown in Figure 13.26.
The purpose of the fuzzy path following module is to generate the appropriate com-
mands for the actuators of the robot, so as to follow the global path as close as possi-
ble, while minimizing the error between the current heading of the robot and the
desired one, obtained from the path calculated by the global planner (Figure 13.26).
It has one input, the difference Δα between the desired and the actual heading of the
robot, and one output, a steering command Δθ.
Global path planner
Reactive local planner
Global model
Global position
Locally observed
obstacles
Steering command
Figure 13.24 Two-level
navigation control
architecture.
(A)
(B)
End point
Task trajectory
C M of the platform
Control scheme
Case 1
Case 2
Case 3
Case 4
x (m)
0
–0.5
–0.5
0.5
0
Mobile manipulator
0.5
1
Time (s)
0
2
4
6
8
10
0.005
0.01
0.015
0.02
0.025
0.03
Tracking 
Error (m)
y (m)
Figure 13.23 (A) Trajectories of COG and end point, and task trajectory. (B) Corresponding
tracking errors.
Source: Reprinted from Ref. [18], with permission from Electrical and Electronic Engineers.
558
Introduction to Mobile Robot Control

The fuzzy module/controller is based on the TakagiSugeno method.1 The input
space is partitioned by fuzzy sets as shown in Figure 13.27. Here, asymmetrical triangu-
lar and trapezoidal functions which allow a fast computation, essential under real-time
conditions, are utilized to describe each fuzzy set (see Eqs. (13.113.3)) below.
Local reactive
planner
Fuzzy path
following
Fuzzy obstacle
avoidance
∑
Global path
Locally observed 
obstacles
Heading 
error
Current Goal 
position
Position of obstacles
Steering command 1
Steering command 2
Steering
command
Figure 13.25 Structure of the fuzzy reactive local planner.
αd
αd
αa
Δα
Y
X
Robot desired
heading
Desired
position
Mobiled robot
Actual
position
Mobile robot
Robot actual
heading
Figure 13.26 Actual and
desired configurations of the
mobile robot. αd denotes the
desired heading, αa denotes
the actual heading, and Δα
denotes the difference
between the actual and the
desired heading of the mobile
robot.
1 In the TakagiSugeno method, the rules have the form IF x1 is Ai
1 AND . . . AND xn 5 Ai
n, THEN
yi 5 ci
0 1 ci
1x1 1 ::: 1 ci
nxn where ci
0; ci
1; :::; ci
n are crisp-valued coefficients. The total conclusion of this
fuzzy system is y 5 Σm
i51wiyi=Σm
i51wi, where wi 5 Πn
k51μAi
k xk
ð
Þ are the weights of the averaging process.
559
Experimental Studies

To calculate the fuzzy intersection, the product operator is employed. The final
output of the unit is given by a weighted average over all rules (see Eq. (13.5) and
Figure 13.27).
Intuitively, the rules for the path following module can be written as sentences with
one antecedent and one conclusion (see Eq. (13.4)). As described in Example 8.3, this
structure lends itself to a tabular representation. This representation is called fuzzy
associative matrix (FAM) and represents the prior knowledge of the problem domain
(Table 13.3).
The symbols in Table 13.3 have the following meaning: LVB, left very big; LB,
left big; LS, left small; LVS, left very small; N, neutral (zero); RVB, right very
big; RB, right big; RS, right small; RVS, right very small.
The rows represent the fuzzy (linguistic) values of the distance to an obstacle,
the columns are the fuzzy values of angles to the goal, and the elements of the
matrix are the torque commands of the motor. The tools of fuzzy logic allow us to
translate
this
intuitive
knowledge
into
a
control
system.
The
fuzzy
set
μðjÞ
~pj ; ~pj 5 1; . . .; pj is described by asymmetrical triangular and trapezoidal functions.
Input: Angular difference between 
desired and actual robot heading
Output: Steering command
Far right
(A)
(B)
Right
Quite right forward
Quite left 
Left 
Horizon
Far left
LVB
LVS
RVS RS RB RVB
N
LB LS
μ(j)(Δαj)
μ(j)(Δθj)
–Horizon
Cls right
Cls left
Δαj
Δθj
Figure 13.27 Fuzzy sets for the path following unit. (A) Input ðΔαÞ: angular difference
between desired and actual robot heading. (B) Output ðΔθÞ: steering command (note that the
output is not partitioned into fuzzy sets, but consists of crisp values).
Table 13.3 A Fuzzy Rule Base for the Mobile Robot Navigation
vi=μj
Very Close
Close
Far
Very Far
Far right
RB
RS
LVB
LVB
Right
RB
RS
LB
LB
Quite right
RB
RB
LS
LS
Close right
RB
RB
LVS
LVS
Forward
N
N
N
N
Close left
RVS
RVS
RVS
RVS
Quite left
RS
RS
RS
RS
Left
RB
RB
RB
RB
Far left
RVB
RVB
RVB
RVB
560
Introduction to Mobile Robot Control

Defining the parameters mlðjÞ
~pj and mrðjÞ
~pj as the x-coordinates of the left and right
zero crossing, respectively, and mclðjÞ
~pj and mcrðjÞ
~pj as the x-coordinates of the left and
right side of the trapezoid’s plateau, the trapezoidal functions can be written as:
μðjÞ
~pj ðΔαjÞ5
maxððΔαj 2mlðjÞ
~pj Þ=ðmclðjÞ
~pj Þ2mlðjÞ
~pj ;0Þ
if
Δαj ,mclðjÞ
~pj
1
if
mclðjÞ
~pj #Δαj ,mcrðjÞ
~pj
maxððΔαj 2mrðjÞ
~pj Þ=ðmcrðjÞ
~pj 2mrðjÞ
~pj Þ;0Þ if
Δαj .mcr jð Þ
~pj
8
>
>
>
>
<
>
>
>
>
:
ð13:1Þ
with ~pj 5 1; 2; . . .; pj. Triangular functions can be achieved by setting mclðjÞ
~pj 5
mcrðjÞ
~pj . On the left and right side of the interval, the functions are continued as con-
stant values of magnitude 1, that is:
μðjÞ
1 ðΔαjÞ 5
1
if
Δαj # mcrðjÞ
1
maxððΔαj 2 mrðjÞ
1 Þ=ðmcrðjÞ
1 2 mrðjÞ
1 Þ; 0Þ
if
Δαj . mcrðjÞ
1
(
ð13:2Þ
and
μðjÞ
pj ðΔαjÞ 5
maxððΔαj 2 mlðjÞ
pj Þ=ðmclðjÞ
pj 2 mlðjÞ
pj Þ; 0Þ
if
Δαj # mclðjÞ
pj
1
if
Δαj . mclðjÞ
pj
8
<
:
ð13:3Þ
The fuzzy set μðjÞ
~qj is associated with linguistic terms AðjÞ
~qj . Thus, for the mobile
robot, the linguistic control rules RðjÞ
1 ; . . .; RðjÞ
rj , which constitute the rule base, can be
defined as:
RðjÞ
~rj :IF
Δαj
is
AðjÞ
~pj ;
THEN
fðΔθ~rjÞ
ð~rj 5 1; 2; . . .; rjÞ
ð13:4Þ
Finally, the output of the unit is given by the weighted average over all rules:
Δθj 5
X
rj
~rj51
σ~rj  Δθ~rj=
X
rj
~rj51
σ~rj
ð13:5Þ
Equation (13.4) together with Eq. (13.5) defines how to translate the intuitive
knowledge reflected in the FAM into a fuzzy rule base. The details of this transla-
tion can be modified by changing the number of fuzzy sets, the shape of the sets
(by choosing the parameters mlðjÞ
~pj , mrðjÞ
~pj ; mclðjÞ
~pj ; mcrðjÞ
~pj ) as well as the value Δθ~rj of
each of the rules in Eq. (13.5). As an example, in this application, the number of
561
Experimental Studies

fuzzy sets that fuzzifies the angular difference in heading Δαj is chosen to be nine.
All the other parameters were refined by trial and error. The fuzzy-based obstacle
avoidance unit, which controls the mobile robot, has three principal inputs:
1. The distance dj between the robot and the nearest obstacle.
2. The angle γj between the robot and the nearest obstacle.
3. The angle θj 5 αj 2 βj between the robot’s direction and the straight line connecting
the current position of the robot and the goal configuration, where βj is the direction of the
straight line connecting the robot’s current position and the goal position, and αj is the cur-
rent direction of the robot (Figure 13.28).
The output variable of the unit is the motor torque command τj. All these vari-
ables can be positive or negative, that is, they do not only inform about the magni-
tude but also about the sign of displacement relative to the robot left or right. The
motor command which can be interpreted as an actuation for the robot’s direction
motors is fed to the mobile platform at each iteration. It is assumed that the robot
is moving with constant velocity, and no attempt is being made to control it.
For the calculation of the distance, the only obstacles considered are those which
fall into a bounded area surrounding the robot and moving along with it. In this
implementation, this area is chosen to be a cylindrical volume around the mobile
platform and reaches up to a predefined horizon. This area can be seen as a simpli-
fied model for the space scanned by ranging sensors (e.g., ultrasonic sensors)
attached to the sides of the robot. Besides an input from ultrasonic sensors, a cam-
era can also be used to acquire the environment. Mobile robots are usually
equipped with a pan/tilt platform where a camera is mounted. This camera can also
be utilized. If no obstacle is detected inside the scan area, the fuzzy unit is
informed of an obstacle in the far distance.
Fuzzy unit
τj
αj
θj
γj
βj
Mobile robot
dj, γj, θj
* Obstacle
Goal 
position
Robot
direction
*
X axis
Y axis
Figure 13.28 The omnidirectional mobile robot connected to the corresponding fuzzy-based
obstacle avoidance unit. This unit receives via an input the angle θj 5 αj 2 βj between the
robot’s direction and the straight line connecting the current position of the robot and the
goal configuration, and the distance and the angle of the nearest obstacle ðdj; γjÞ. The output
variable of the unit is the motor command τj.
562
Introduction to Mobile Robot Control

13.10.1
Experimental Results
The functioning of the proposed system, applied to an omnidirectional mobile robot,
was evaluated through simulations using Matlab. In all cases, the proposed planner
provided the mobile robot with a collision-free path to the goal position. Simulation
results, obtained in three different working scenarios, are shown inFigure 13.29 [20].
Working scenario 1
(A)
(B)
(A)
(B)
(A)
(B)
Working scenario 2
Working scenario 3
00
2
4
6
8
10
1
2
3
4
5
6
7
8
9
10
00
2
4
6
8
10
1
2
3
4
5
6
7
8
9
10
00
2
4
6
8
10
1
2
3
4
5
6
7
8
9
10
00
2
4
6
8
10
1
2
3
4
5
6
7
8
9
10
00
2
4
6
8
10
1
2
3
4
5
6
7
8
9
10
00
2
4
6
8
10
1
2
3
4
5
6
7
8
9
10
Actual path
Actual path
Actual path
Figure 13.29 Three scenarios of simulation results. (A) Actual path of the robot and the
global path coincide since the world model was accurate. (B) Actual path of the robot is
different from the path provided by the global path planner since the position/size of the
obstacle was not very accurate or the obstacle moved to a new position.
563
Experimental Studies

The present two-level fuzzy global and local path planning control method can also
be directly applied to fixed industrial robots [19], as well as to mobile manipulators.
13.11
Hybrid Fuzzy Neural Path Planning in Uncertain
Environments
Here, a hybrid sensor-based mobile robot path planning algorithm in uncertain and
unknown environments, with corresponding simulation results, will be presented [21].
The constraints and the solution style adopted are similar to those followed by a
human decision maker when he (she) tries to solve a path planning problem of this
type. The human does not know precisely the position of the goal, and so he (she)
makes use of its direction, and the information he (she) gets about the distance, the
shape of the obstacles, at their visible side, which is strongly fuzzy. The present path
planning problem does not allow the use of the standard scheme “data inputproces-
singdata output” since here there are some rules that do not depend on the input data
but affect the output. Examples of such rules are as follows:
G
The robot motion must be close as much as possible to the direction of the goal.
G
If a movement direction has been selected, it is not advisable to change it without prior
consideration, unless some other much better direction is brought to the robot’s attention.
G
If the robot returns to a point that has been passed before, it should select an alternative
route (direction) in order not to make endless loops and consequently reach a dead end.
For this reason, the robot path planning system should involve two subsystems,
namely, a subsystem connected to the input, and a subsystem that is not connected
to the input, but is modified by the output values. It seems to be convenient to use
a fuzzy logic algorithm in the first subsystem (since the sensor measurements are
fuzzy) and a neural model in the second subsystem for the rules that have to be
adaptive. The general structure of such a system is shown in Figure 13.30.
Here, the subsystem II is implemented algorithmically, that is, without the use
of neural network, but through a merging operation based on simple multiplication.
Sensor
Subsystem I
(fuzzy system)
Subsystem I
(neural net) 
Merging
Output
Figure 13.30 General structure of a WMR path planner in unknown terrain.
564
Introduction to Mobile Robot Control

13.11.1
The Path Planning Algorithm
It will be assumed that the autonomous robot (represented by a point in the space)
has to move from an initial position S0 (start) to a final position G (goal). The avail-
able data are measurements from the sensors about all required quantities. These
measurements are obtained from the current position Si of the robot. The direction of
the goal G, that is, the direction of the straight line SiG is also assumed to be mea-
sured, but the length (distance) ðSiGÞ is not measured. Then, the algorithm works as
follows: we first define a coordinate frame attached to the robot system with Ox-axis
the straight line SiG, and divide the space into K directions with reference the SiG
(or Ox) direction. Obviously, two consecutive directions have an angular distance of
360=K degrees. It is convenient to select the number K of directions such that the
axes Ox; Ox0; Oy, and Oy0 are involved in them. Now, from the sensors’ measure-
ments and the fuzzy inference on the database, we determine and associate to each
direction a priority. Then, we select the direction with the highest priority, and the
new point Si11 is placed in this direction with ðSiSi11Þ 5 b, where b is the step length
that we select.
On the basis of the above, the path planning algorithm is implemented by the
following steps:
Step 1: Obtain the measurements from the position Si.
Step 2: Fuzzify the results of these measurements.
Step 3: Introduce the fuzzy results into the rules adopted.
Step 4: Determine a priority value for each direction.
Step 5: Multiply the above priority value by a given a priori weight for each direction.
Step 6: Determine the direction with the highest priority, move by a step length in this
direction to find Si11.
Step 7: Repeat the algorithm for the position Si11.
The fuzzy database contains suitable rules, the inputs of which are obtained
from the sensors. The a priori knowledge used for the robot motion is expressed
in the form of a weight for each direction, and there is no need to be determined
each time from the knowledge base. For example, the direction from the current
position to the goal must have the highest priority. Similarly, the directions that
approach the goal must have higher priorities than the directions that depart from
the goal. The representation of this a priori knowledge in the form of weights
suggests the use of a neural network which accepts as inputs the priorities of the
K directions, and has as weights the weights of the a priori knowledge
(Figure 13.30). These weights can be predetermined by trial and error work and
can be updated periodically after a certain number of steps. In this way, we ensure
the learning of the relationship between the a priori knowledge and the output of
the fuzzy inference (not the learning of the a priori knowledge itself). An algo-
rithm is also required to overcome the dead ends that may appear when a cycle
occurs in which the weights of the a priori knowledge do not change. This algo-
rithm varies from case to case. In the simulation example that follows, we used a
fast algorithm for this purpose which runs over each cycle of the path planning
algorithm.
565
Experimental Studies

The present algorithm has a very general structure and can be used to any path
planning problem in an unknown environment where optimality is not required.
The algorithm needs only to have available the rules of the fuzzy database, the
length b of the motion step, and the number K of the directions that are examined.
13.11.2
Simulation Results
The simulation robot path planning example was designed to have the following
particular features:
G
The distance D from the point Si to some obstacle point (e.g., an obstacle or the room
wall) is assumed to be measured by a sensor.
G
The measurement results are quantized in 11 intervals and fuzzified as given in
Table 13.4
The entries of this table are the values of membership functions μjðDÞ of the distance
in the various ranges j 5 0; 1; . . .; 10 as distributed among the linguistic values L, VL,
MORLL, NL, ME, MM, NM, HI, VH, MH, NH, and UN. The respective code of these
values is described as follows: L, low; VL, very low; MORLL, more or less low; ME,
medium; MM, more or less medium; NM, not medium; H, high; VH, very high; MH,
more or less high; NH, not high; UN, unknown.
G
The rules employed are the following:
R1 5 IF Di is low, THEN Pi is high
R2 5 IF Di is medium, THEN Pi is medium
R3 5 IF Di is high, THEN Pi is low
where Pi is the priority of the ith direction quantized in 11 intervals and fuzzified accord-
ing to Table 13.4.
G
The knowledge is represented by fuzzy matrices where Zadeh’s max-min inference rule
is applied.
G
The fuzzification is performed using the singleton method, and the defuzzification using
the center of gravity method.
G
The number of directions is selected to be K 5 16 and the step length to be b 5 5 pixels.
Table 13.4 Distance Quantization
Range
L
VL
MORLL
NL
ME
MM
NM
HI
VH
MH
NH
0
D,15
1.00
1.00
1.00
0.00
0.00
0.00
1.00
0.00
0.00
0.00
1.00
1
15,D,60
0.67
0.45
0.82
0.33
0.00
0.00
1.00
0.00
0.00
0.00
1.00
2
60,D,105
0.33
0.11
0.57
0.67
0.25
0.50
0.75
0.00
0.00
0.00
1.00
3
105,D,150
0.00
0.00
0.00
1.00
0.50
0.71
0.50
0.00
0.00
0.00
1.00
4
150DD,195
0.00
0.00
0.00
1.00
0.75
0.87
0.25
0.00
0.00
0.00
1.00
5
195,D,240
0.00
0.00
0.00
1.00
1.00
1.00
0.00
0.00
0.00
0.00
1.00
6
240,D,285
0.00
0.00
0.00
1.00
0.75
0.87
0.25
0.20
0.04
0.45
0.80
7
285,D,330
0.00
0.00
0.00
1.00
0.50
0.71
0.50
0.40
0.16
0.63
0.60
8
330,D,375
0.00
0.00
0.00
1.00
0.25
0.50
0.75
0.60
0.36
0.77
0.40
9
375,D,420
0.00
0.00
0.00
1.00
0.00
0.00
1.00
0.80
0.64
0.89
0.20
10
D.420
0.00
0.00
0.00
1.00
0.00
0.00
1.00
1.00
1.00
1.00
0.00
566
Introduction to Mobile Robot Control

G
The weights in the network selecting the maximum direction priority (i.e., the weights of
the a priori knowledge) are predetermined as:
ð1010; 9000; 800; 70; 6; 5; 4; 3; 2; 3; 4; 5; 6; 70; 800; 9000Þ
The robot environment used in the present example was obtained on a
VGA640x480 screen. During the motion, the “mouse” was regarded as a moving
obstacle. The results of four experiments with different starting and goal points are
shown in Figure 13.31AD [21].
13.12
Extended Kalman Filter-Based Mobile Robot SLAM
Here, some results obtained for the SLAM problem using EKFs will be presented
(Section 12.8.2). The differential drive WMR SLAM problem of Example 12.3
(Figure 12.11) was solved using a global reference at the origin, which assures that
the EKF is stable [22]. Actually, the robot and landmark covariance estimates were
proved to be significantly reduced, and the actual robot path and landmark esti-
mates showed a significant error reduction. This is shown in Figure 13.32 where
the results are compared to the GPS measurements for two anchors located at
(2.8953, 24.0353) and (9.9489, 6.9239).
So
G
So
G
So
G
So
G
(A)
(B)
(C)
(D)
Figure 13.31 Path planning results for four (AD) different starting point and goal
configurations.
567
Experimental Studies

East meters
Estimated 
Path
GPS path
Path
Deviatiovn (m)
Deviatiovn (m)
Deviatiovn (m)
Deviatiovn (m)
Time (s)
Time (s)
Time (s)
Time (s)
Vehicle error (lon)
Vehicle error (lat)
X var
Y var
Steer
Beac 1 (east)
Beac 1 (north)
Beac 3 (east)
Beac 3 (north)
Beac 5 (east)
Beac 5 (north)
Beac 7 (east)
(A)
(B)
(C)
(D)
(E)
1
0.8
0.6
0.4
0.2
0
–0.2
–0.4
–0.6
–0.8
–1
1.5
1
0.5
0
0
20
40
60
15
10
5
0
–5
–10
–15
–20
–20
–15 –10
–5
0
5
10
15
20
25
30
80
100
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
20
40
60
80
100
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
20
40
60
80
100
0
20
40
60
80
100
North meters
Figure 13.32 (A,B) EKF errors and covariance of the robot. (C,D) Landmark localization
errors and covariance. (E) Robot path and landmark location estimates compared to GPS
ground truth.
Source: Courtesy of T.A. Vidal Calleja [22].
568
Introduction to Mobile Robot Control

13.13
Particle Filter-Based SLAM for the Cooperation of
Two Robots
Here,
some
results
obtained
by
applying
the
particle
filter
approach
of
Section 12.8.4 to WMR SLAM for a pair of robots performing collaborative
exploration will be presented [23,24]. The parameters xk used allow the modeling
of the poses and uncertainties of the robots. The strategy for cooperative explora-
tion of the two robots is to allow them to take turns moving such that at each
time one robot is stationary and can be considered as a fixed reference point
(Figure 13.33).
The positions of the two robots are estimated via the particle filter combining an
open-loop estimate of odometry error with data from a range finder on one robot
and a three-plane target mounted on top of the other robot.
13.13.1
Phase 1 Prediction
Here, the variable of interest is the pose ½x; y; φΤ of the moving robot. The robot
motion is performed as a rotation followed by a translation. The rotation angle is
equal to Δφ 5 φk 2 φ, where φk 5 arctgðΔy=ΔxÞ, and the forward translation dis-
tance is l 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðΔx2Þ 1 ðΔy2Þ
p
. If the starting pose is ½xk; yk; φkΤ, then the resulting
pose is ½xk11; yk11; φk11Τ given by:
xk11 5
xk11
yk11
φk11
2
64
3
75 5
xk 1 l cos φk
yk 1 l sin φk
φk
2
64
3
75
The rotation error (noise) Δφ due to odometric error is assumed to be a
Gaussian process with mean value μrot and standard deviation proportional to Δφ,
that is, σrot 5 σΔφ. The translation noise has two components: the first is related to
Figure 13.33 The two cooperating WMRs exploring one side of a room.
Source: Reprinted from Ref. [24], with permission from Institute of Electrical and Electronic
Engineers.
569
Experimental Studies

the actual distance travelled (pure translation noise), and the second is due to
changes in orientation during the forward translation. This component is called
drift. The mean values μtrans; μdrift and the standard deviations σtrans; σdrift of these
two components can be determined experimentally by discretizing the simulated
motion in L steps. The standard deviations per step are equal to:
σtrs 5 σtrans
ﬃﬃﬃ
L
p
and
σdrft 5 σdrift
ﬃﬃﬃﬃﬃﬃﬃﬃ
L=2
p
Therefore, the WMR stochastic prediction model is:
xk11 5
xk11
yk11
φk11
2
64
3
75 5
xk
yk
φk
2
64
3
75 1
ðΔl 1 εΔlÞcosðφk 1 εφ1Þ
ðΔl 1 εΔlÞsinðφk 1 εφ2Þ
εφ1 1 εφ2
2
64
3
75
where εΔl; εφ1, and εφ2 are Gaussian noises with mean values μtransl; μφ1 5 μφ2 5
μdrift=2 and standard deviations σΔl 5 σtrans
ﬃﬃﬃ
L
p
Δl; σφ1 5 σφ2 5 ð
ﬃﬃﬃﬃ
N
p
=
ﬃﬃﬃ
2
p
ÞσdriftΔl.
An experiment where the robot moved forward three times (horizontally to the
right), rotated by 90, then moved forward three more times, after that rotated again
by 90, and translated forward five times showed that the uncertainty grows with-
out bound.
13.13.2
Phase 2 Update
After each motion, the robot range finder (RF) sensor sends a measurement vector
z 5 ½l; φ; θΤ, where l; φ, and θ are as shown in Figure 13.34.
Sr
With sensor
Moving robot
With target
W
M
W
S
(Xm, ym)
(Xs, ys)
Mr
L
Rf
Sr
Figure 13.34 The range finder (RF) sensor mounted on the stationary robot (SR) observes
the moving robot (MR) that carries the target. The tracker gives l; ^φ, and ^θ ðφw 5 ^φ 1 ^φs; θw 5
^θ 1 ^φmÞ.
Source: Adapted from Ref. [24].
570
Introduction to Mobile Robot Control

The laser range finder can always detect at least two planes of the three-plane
target from any position around the moving robot. The sensor output ½l; φ; θΤ is
given by:
z 5
l
φ
θ
2
64
3
75 5
ððdxÞ21ðdyÞ2Þ1=2
atanðdy; dxÞ 2 θs
atanð2dy; 2dxÞ 2 θm
2
64
3
75
where xm 5 ½xm; ym; φmΤ is the pose of the moving robot and xs 5 ½xs; ys; φsΤ is the
pose of the stationary robot, dx 5 xm 2 xs and dy 5 ym 2 ys.
Using the sensor measurement z, the weights are updated as shown in
Eqs. (12.68b) and (12.69):
wm
k11 5 ^wm
k11=
X
M
q51
^wq
k11
 
!
;
^wm
k11 5 wm
k pðz=xkÞ
where pðz=xkÞ is the following Gaussian distribution:
pðz=xkÞ  e
2
ðl2lkÞ2
σ2
l
ﬃﬃﬃﬃﬃﬃ
2π
p
σl
 e
2ðφ2φkÞ2
σ2
φ
ﬃﬃﬃﬃﬃﬃ
2π
p
σφ
 e
2
ðθ2θkÞ2
σ2
θ
ﬃﬃﬃﬃﬃﬃ
2π
p
σθ
assuming that the processes l; φ, and θ are statistically independent.
13.13.3
Phase 3 Resampling
When the effective sampling size ESS (see Eq. (12.70)) is less than the threshold
Nmax, the particle population is resampled to eliminate (probabilistically) the ones
with small weights.
13.13.4
Experimental Work
The mapping algorithm of the room is based on triangulation of free space by the two
robots (see Section 12.6.3). The “sweep” of the space is performed using the line of
visual contact of the two robots, that is, if the two robots can see each other, there is
no obstacle in the space between them. If a robot is located at a corner (stationary) and
the other moves along a wall (always without losing visual contact), then a triangle of
free space is mapped. The environment is completely mapped by the robots using an
online triangulation of the free space [23,24]. Figure 13.35 shows experimental results
with two robots exploring the convex area of the corridors of a building. Both
subfigures show a spatial integration of the particles during the full trajectory.
571
Experimental Studies

13.14
Neural Network Mobile Robot Control and Navigation
Here, simulation results will be presented concerning two problems:
1. Neural network-based mobile robot trajectory tracking
2. Neural network-based mobile robot obstacle avoiding navigation.
13.14.1
Trajectory Tracking
The general method of Section 8.5 was applied to a differential drive WMR using a
PD velocity controller to train a multilayer perceptron (MLP), as shown in Figure 8.11.
The PD teacher controller has the form:
v 5 Kðxd 2 xÞ 1 _xd
where x 5 ½x; yΤ, and has been designed to guarantee exponential convergence of
xðtÞ to the desired trajectory xdðtÞ. The desired trajectory was generated by a refer-
ence (virtual) WMR so as to assure that it is compatible with the kinematic nonholo-
nomic constraint of the robot under control. The gain matrix K was chosen such that:
de=dt 5 2Ke;
e 5 xd 2 x;
K . 0
A specific value of K that assured good exponential trajectory convergence is
K 5 diag½k1; k2 5 ½102:9822; 1:3536, and was used in the experiment. A two-layer
Update phase robot 0
Update phase robot 1
Y-axis
X-axis
Y-axis
–200
–100
0
100
200
0
200
400
600
800
1000
0.3
0.2
0.1
0
0.15
0.05
0.1
0
1000
800
600
400
0
–200
200
0
100
200
300
–100
–200
–300
–400
–500
X-axis
Figure 13.35 Convex area exploration by the two robots: (A) trajectory of robot 0;
(B) trajectory of robot 1. The peak heights represent accuracy (the higher a peak the more
accurate the estimation).
Source: Reprinted from Ref. [24], with permission from Institute of Electrical and Electronic
Engineers.
572
Introduction to Mobile Robot Control

MLP NN was used with 10 nodes in the hidden layer and 1 node in the output
layer. This NN was trained by the BP algorithm with inputoutput data the veloc-
ity vðtÞ and the position xðtÞ, respectively. The structure of the neurocontroller is
shown in Figure 13.36 [25].
In this scheme the NN, instead of learning explicit trajectories, is trained to learn
the relationship between the linear velocity vðtÞ and the position error eðtÞ. The
learning rate is adjusted by the rule:
γðk 1 1Þ 5 γðkÞ½1 2 μe2ηðkÞsgnðη=kÞ;
μA½0; 1
where ηðtÞ is the normalized ratio:
ηðkÞ 5 ΔVðw; kÞ
Vðw; kÞ 5 Vðw; kÞ 2 Vðw; k 2 1Þ
Vðw; kÞ
of the total squared error:
Vðw; tÞ 5
X
N
k50
jjvðkÞ 2 vNNðkÞjj2
which was actually minimized by updating the NN weights w 5 ½w1; w2; .. .Τ. The
experiment was performed for several values of μA½0; 1. The value of μ that leads to
the best learning convergence was used in the NN training. In the experiments, the ini-
tial value γð0Þ 5 0:02 was used for both layers. The best results were obtained with
μ 5 0:71.
After training, the neurocontroller was used to control the robot. The results
obtained with the neurocontroller are shown in Figures 13.3713.39. Figure 13.37
Delay
Delay
PD teaching
controller
WMR
Neural
controller
Weight
updating
xd(t) +
–
e(t)
VNN(t)
–
+
V(t)
x(t)
Figure 13.36 WMR supervised tracking neurocontroller.
Source: Adapted from Ref. [25].
573
Experimental Studies

shows the desired versus the actual WMR position trajectory and orientation angle
φðtÞ. Figure 13.38 shows the time evolution of the x and y errors, and Figure 13.39
shows the time evolution of the left and right wheel velocities [25]. These
figures show the very satisfactory performance of the neural trajectory tracking
controller, not only for x and y but also for the robot’s orientation φ.
Φ (degrees)
4
(A)
(B)
3
2
1
0
–1
–2
–3
–4
200
Desired
Actual
150
100
50
0
–50
–100
–150
–200
0
20
40
60
80
Time (s)
100
120
140
–6
–4
–2
0
2
Actual
Desired
4
6
x (m)
y (m)
Figure 13.37 (A) Comparison of
desired and actual ðx; yÞ trajectory
achieved by the neurocontroller.
(B) Comparison of desired and
actual orientation φðtÞ.
Source: Courtesy of J. Velagic [25].
Time [s]
x-error [m]
Time [s]
0.5
(A)
(B) 0.5
0
–0.5
–1
–1.5
Y coordinate error (m) 
–2
–2.5
–3
0.4
0.3
0.2
0.1
0
–0.1
–0.2
0
20
40
60
80
100
120
140
0
20
40
60
80
100
120
140
Figure 13.38 Coordinated errors: (A) x error; (B) y error.
Source: Courtesy of J. Velagic [25].
574
Introduction to Mobile Robot Control

13.14.2
Navigation for Obstacle Avoidance
The neural nets can be used and have been used for WMR navigation purposes,
besides the control purposes. The results to be provided here were obtained by
combining MLPs for local modeling of the WMR, and RBFs for the activation of
the local models. The division of the input space to subregions was performed
using the fuzzy C-means method. For full comprehension of the present mobile
robot navigation method, we first describe briefly the concepts of local model NNs
(LMNs) and the fuzzy C-means segmentation (FCM) algorithm.
13.14.2.1
Local Model Networks
In this approach, the operation regime is divided in subareas, and each one of them
is associated to a local neural model that approximates the system behavior within
this subarea. The operating regime includes all the operating points where the sys-
tem is capable of functioning. Usually, linear neural networks are used. To use the
LMNs for system modeling, the parameters that must be included in the operating
point (OP) must be determined. The structure of LMN modeling is shown in
Figure 13.40 [2628], where k is the serial number of the local model.
0.8
(A)
(B)
0.6
0.4
Linear velocity of right wheel (m/s)
Linear velocity of left wheel (m/s)
0.2
0
0.8
0.6
0.4
0.2
–0.20
20
40
60
80
Time (s)
100
120
140
0
0
20
40
60
80
100
120
140
Time (s)
Figure 13.39 Linear velocities of the two wheels: (A) left wheel; (B) right wheel.
Source: Courtesy of J. Velagic [25].
F
F
Output
F
Constant LM
weights
Participation
weights (depending on the
operating point)
0
1 2 3 4
k
Winner
Participation weight
Σ
Σ
Σ
Σ
Figure 13.40 LMN structure (A), with participation weights (B).
575
Experimental Studies

Each linear model is actually a linear predictor, giving a linear estimate of the
next value of the system output depending on the previous ones as well as on the
past values, namely:
^yðkÞ 5 α1yðk 2 1Þ 1 ? 1 αnyðk 2 nÞ 1 b1uðk 2 1Þ 1 ? 1 bmuðk 2 mÞ
where n and m are the model’s orders with respect to the output yðkÞARp and the
input uðkÞARq, respectively. In the special case where m 5 n 5 1, the model
reduces to:
^yðkÞ 5 αyðk 2 1Þ 1 buðk 2 1Þ
The estimator (predictor) ^y is trained within each specific part of the operating
regime,
giving
the
corresponding
coefficients
αiði 5 1; 2; . . .; nÞ
and
biði 5 1; 2; . . .; mÞ. Once there is an LMN trained for every part of the operating
regime, the system modeling is complete. The values of its coefficients are given
by the estimated values ^αiði 5 1; 2; . . .; nÞ and ^biði 5 1; 2; . . .; mÞ. In Figure 13.40,
the winner-take-all strategy is illustrated, but it is also possible to have a multi-
LMN participation for the total system output.
13.14.2.2
The Fuzzy C-Means Algorithm
This is an extension of the classical C-means algorithm using fuzzy reasoning, and
involves four steps as described below [26,29]. Consider a set of patterns. These
are classified (clustered) into c fuzzy clusters by minimizing a cost function of the
form:
JðM; υÞ 5
X
c
i51
X
N
k51
ðμikÞmdik
where:
G
μik stands for the membership value of pattern k into cluster i;
G
N stands for the number of input patterns;
G
M stands for the c 3 N fuzzy partition matrix ½μik;
G
υi represents the center of ith cluster;
G
dik stands for the distance between the patterns k and the center of cluster i.
The steps of the fuzzy C-means algorithm are the following:
Step 1: Define the number c of clusters ð2 # c # NÞ, the membership power factor
m ð1 # m , NÞ, and the metric used for measuring clusters distance.
Step 2: Initialize the fuzzy partition matrix Mð0Þ
Step 3: At each step b, b 5 1; 2; . . .
(i) Calculate the vectors of the clusters’ centers as υi 5 P
N
k51
ðμikÞmxk= P
N
k51
ðμikÞm
576
Introduction to Mobile Robot Control

(ii) Update the fuzzy partition matrix MðbÞ and find the next one Mðb11Þ as follows:
Calculate for each pattern xk the number of clusters that pattern is associated with:
Ik 5 fij1 # i # c; dik 5 jjxk 2 υijj 5 0g, and the set of clusters Tk; k 5 1; 2; . . .;
c 2 Ik to which the pattern is not associated, with the rules:
IF Ik 6¼ 0, THEN μik 5 1= P
jAIk
dik
djk

2=ðm21Þ
IF Ik 5 0, THEN μik 5 0 for all iATk and P
jAIk
μjk 5 1
Step
4:
Compare
MðbÞ
and
Mðb11Þ
using
the
selected
measure
and
if
jjMðbÞ 2 Mðb11Þjj # εL (convergence threshold), terminate the algorithm; otherwise return
to step 3.
13.14.2.3
Experimental Results
The training process termination criterion of each LMN used is the root mean
square (RMS):
yRMS 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ð1=NÞ
X
N
i51
ðydðkÞ2yNNðkÞÞ2
v
u
u
t
where ydðkÞ is the desired output and yNN is the LMN output:
yNN 5
X
M
i51
^yiφiðjjx 2 cijjÞ
with ^yi being the output estimate of the ith local submodel, and φiðjj  jjÞ the ith
RBF function defined by Eq. (8.62). The results were obtained using 500 learning
datasets, each dataset involving five parameters:
1. LD, left distance
2. RD, right distance
3. FD, front distance
4. TB, target bearing
5. SA, change in steering angle
The experiment used one mobile robot, one target, and four obstacles (if more
than one robot exists, each mobile robot is considered as an obstacle to the other
robots). The MLP NNs used for local modeling involved three layers (input, hid-
den, output layer). The input layer has four nodes, that is, three nodes for the input
values of the front, left and right distances from obstacles, and one node for the tar-
get bearing (if no target is detected, the input to the fourth node is zero). The output
layer has a single node which computes the robot steering angle. Figure 13.41AC
shows the path of a single robot followed under the control of an MLP, an RBF,
and an LMN (based on MLP and RBF), respectively. Details and statistics of the
method are provided in Ref. [26].
577
Experimental Studies

13.15
Fuzzy Tracking Control of Differential Drive Robot
Here, performance results of the fuzzy logic controller designed by the
Mamdani model, as discussed in Section 8.4.1, will be presented. The kinematic
model is:
_x 5 v cos φ;
_y 5 v sin φ;
_φ 5 ω;
_x sin φ 5 _y cos φ
and the dynamic model in the unconstrained form (3.19a):
DðqÞ_v 1 Cðq;_qÞv 1 Fv 5 τ 1 dðtÞ
where v 5 ½v; ωΤ; q 5 ½x; y; φΤ 5 x, F is the linear friction matrix, and dðtÞ is an
external disturbance, has the parameters [30]:
DðqÞ 5
0:3749
20:0202
20:0202
0:3739
2
4
3
5;
F 5
10
0
0
10
2
4
3
5
Cðq;_qÞ 5
0
0:1350_φ
20:1350_φ
0
2
4
3
5;
u 5 τ 5
u1
u2
2
4
3
5
Figure 13.41 Performance of neural network-based navigation: (A) MLP, (B) RBF, and (C)
full LMN navigator.
Source: Courtesy of H.A. Awad [26].
578
Introduction to Mobile Robot Control

The disturbance vector dðtÞ is:
dðtÞ 5
δðt 2 tkÞ
δðt 2 tkÞ


;
tk 5 2; 4; . . .
where the delta function δðÞ is applied every 2 s. The desired velocity vector
vd 5 ½vd; ωdΤ is assumed to be:
vd 5
vdðtÞ
ωdðtÞ


5
0:25 2 0:25 cosð2πt=5Þ
0


and the initial conditions are:
qð0Þ 5 ½0:1; 0:1; 0Τ; vð0Þ 5 ½vð0Þ; ωð0ÞΤ 5 ½0; 0Τ
The crisp kinematic controller used is (see Eq. (8.18)):
vm 5 vd cos ε3 1 K1ε1;
ωm 5 ωd 1 K2vdε2 1 K3 sin ε3
with gains K1 5 K2 5 K3 5 5. The membership functions of the linguistic (fuzzy)
variables ~v 5 ½~v; ~ωΤ; u1; u2, where ~v 5 v 2 ~vd, have the triangular and trapezoidal
forms shown in Figure 8.15. The dynamic fuzzy controller has the rule base
of Table 8.2 which represents 3 3 3 5 9 fuzzy rules as described in Section 8.4.1.
The fuzzy values of u1 and u2 are converted to crisp values with the COG defuz-
zification method and are represented by the inputoutput surfaces shown in
Figure 13.42.
The performance of the full controller was tested by simulation on the Matlab
(Simulink) in Ref. [30]. It was verified that the error
~x goes to zero at
t 5 0:5 s; ~y goes to zero at t 5 1:0 s, and ~φ goes to zero at t 5 1:2 s. The velocity
errors were brought to zero at t 5 0:25 s. The switching behavior of the controller
was very fast. The simulation plots of the unperturbed closed-loop control system
are shown in Figure 13.43.
The simulation results for the perturbed system showed again a very good
robustness of the controller. Both position and orientation errors converged quickly
to zero despite the external disturbances.
13.16
Vision-Based Adaptive Robust Tracking Control of
Differential Drive Robot
Here, some results obtained by simulation in Refs. [31,32], using the adaptive
robust trajectory tracking controller (9.158a)(9.158e), will be presented. The sys-
tem and controller parameters used are given in Table 13.5.
579
Experimental Studies

The robot initial pose was ½0; 0; 0 and the desired trajectory started from
½3:9; 4:1; 0:2. The components d1 and d2 of d were random in the interval ½ 2 1; 1
and dmax 5 2. The values of vd and ωd were selected as vd 5 1 m=s and
ωd 5 0:5 rad=s. Figure 13.44 shows the convergence of the errors ε1; ε2; ε3; ~v, and ~ω
to zero, despite the existence of the external disturbance.
Figure 13.45A shows the torques τ1; τ2 which, due to the chattering effect, con-
verge to a small area around zero. Figure 13.45B shows the robot desired and
actual trajectory in the world coordinate frame.
The experiments were repeated with the saturation-type controller using a
boundary layer width UðtÞ 5 1= 11t
ð
Þ3. Now, the convergence of the errors and
the actual trajectory are about the same as in Figures 13.44A and B and 13.45B,
but the chattering in the control signals τ1 and τ2 disappeared as shown in
Figure 13.46.
The estimates of the parameters β1 5 m; β2 5 I; and λ converged to constant
values
^β1 ! 102; β2 ! 61, and λ ! 0:97. For comparison, the reader is
advised to perform a simulation experiment using the alternative controller
(9.160) with the same WMR vision system values, desired trajectory, and vd; ωd
values.
0.5
0
–0.5
–1
1
0.6
0.4
0.2
0
–0.2
–0.4
–0.6
–1
–0.5
0
0.5
1
0.5
0
–0.5
–1
1
0.6
0.4
0.2
0
–0.2
–0.4
–0.6
–1
–0.5
0
0.5
1
ω
v
ω
v
u1
u2
Figure 13.42 Overall
inputoutput surfaces of the
fuzzy controller f~v; ~ωÞ !
u1;COG; ð~v; ~ωÞ ! u2;COGg.
Source: Courtesy of
O. Castillo [30].
580
Introduction to Mobile Robot Control

0.5
0
–0.5
x (m)
y (m)
y (m)
u1 (Nm)
u2 (Nm)
ν (m/s)
ω (rad/s)
φ (rad)
0
2
4
6
0
2
4
6
0
2
Time (s)
x (m)
Time (s)
Time (s)
4
6
0
2
4
6
0
2
4
6
0.2
2
–0.2
–0.5
0.6
0.4
0.2
0
–0.2
0.15
0.1
0.05
0
–0.05
0
–0.5
–1
0.14
0.12
0.1
0.08
Vehicle path
X-Y path
Desired path
0.06
0.04
0.02
0
–0.02
0.5
0
–0.5
–1
0.5
0
–0.5
–1
–1.50
2
4
6
8
0
2
4
6
8
–5
0
5
10
15
20
Figure 13.43 Closed-loop performance of the fuzzy controller when dðtÞ 5 0. (A) Errors
~x; ~y; ~φ; ~v, and ~ω. (B) xy path and control input (torques) u1 and u2.
Source: Courtesy of O. Castillo [30].
Table 13.5 System and Controller Parameters
WMR Vision System Parameters
λ
λmax
λmin
r
a
m
I
φ0
1
2
0.5
0.1 m
0.5 m
10 kg
5 kg m2
2π=2 rad
Controller Parameters
k1
k3
ka1
ka2
γ1
γ2
γ3
2
2
20
20
1
1
5
581
Experimental Studies

13.17
Mobile Manipulator Spherical Catadioptric Visual
Control
The problem discussed in Example 10.4 was thoroughly studied in Ref. [33]. The
resolved-rate visual servo controller (10.83) was applied using an estimate ^Jim of
the image Jacobian computed using the EKF technique. Since the parameters
2.0
(A)
(B) 0.30
0.25
0.20
0.15
0.10
0.05
0.00
–0.05
1.5
1.0
0.5
0.0
–0.5
–1.0
0
1
2
3
4
5
6
t (s)
t(s)
7
8
9
10
0
1
2
3
4
5
6
7
8
9
10
ε1
ε2
ε3
ε (m)
ν (m·s–1)
∼
ν∼
ω∼
Figure 13.44 (A) Convergence of the errors ε1; ε2, and ε3. (B) Convergence of the velocity
errors.
Source: Reprinted from Ref. [31], with permission from Springer Science1Business Media BV.
0.5
(A)
(B) 6
5
4
3
2
1
0
–0.5 0.0 0.5 1.0
Desired trajectory
Actual trajectory
1.5 2.0 2.5 3.0 3.5 4.0
0.0
–0.5
–1.0
–1.5
τ (Nm)
y(m)
t (s)
x(m)
–2.0
–2.5
–3.0
–3.5
–4.0
0
1
2
3
4
5
6
7
8
9
τ1
τ2
10
Figure 13.45 (A) The controls τ1 and τ2 suffer from chattering. (B) Actual and desired
trajectories.
Source: Reprinted from Ref. [31], with permission from Springer Science1Business Media BV.
582
Introduction to Mobile Robot Control

involved in the vector ξ are constant, the state transition model has the linear
form:
ξk11 5 ξk 1 wk
ð13:6Þ
where wk is a Gaussian zero-mean disturbance process with covariance matrix Qk.
The measurement vector is:
zk 5 ½um1; vm1; ur1; vr1; um2; vm2; ur2; vr2Τ
and the measurement equation is
zk 5 hðξk; nkÞ
where:
h5 h1^h2
½
Τ
h15 uoffset1 xm
m1λu
zm
m12d;voffset1 ym
m1λv
zm
m12d;uoffset1 xm
r1
zm
r1
λu;voffset1 ym
r1
zm
r1
λv
2
4
3
5
h25 uoffset1
xm
m2
zm
m22dλu;voffset1
ym
m2
zm
m22dλv;uoffset1
xm
r2
zm
r22dλu;voffset1
ym
r2
zm
r22dλv
2
4
3
5
0.5
6
5
4
Desired trajectory
Actual trajectory
3
2
1
0
–0.5
0.0 0.5
1.0 1.5
2.0
2.5 3.0
3.5 4.0
0.0
–0.5
–1.0
τ1
τ2
τ(Nm)
y(m)
–1.5
–2.0
–2.5
–3.0
–3.5
0
1
2
3
4
5
6
t (s)
x (m)
7
8
9
10
(A)
(B)
Figure 13.46 (A) Time evolution of τ1 and τ2 in the case of saturation-type robustifying
control term. (B) Resulting actual trajectory versus desired trajectory.
Source: Reprinted from Ref. [31], with permission from Springer Science1Business Media BV.
583
Experimental Studies

with ηk a zero-mean Gaussian measurement noise with covariance matrix Rk.
Defining the vector:
εðtÞ 5 ½ur1; vr1; ur2; vr2; um1; vm1; um2; vm2Τ
and the 3 3 8 dimensional state vector:
e 5 ½εΤ; _εΤ; €εΤΤ
the measurement equation can take the linear form:
zk11 5 Hek 1 ηk
ð13:7Þ
with:
zk 5 ½ur1; vr1; ur2; vr2; um1; vm1; um2; vm2Τ
where, under the assumption that all the components of the state vector e are
measurable:
H 5 ½I8 3 8^O8 3 8^O8 3 8Τ
In the following, a small representative set of simulation results are provided
with the following data [33,34]:
G
Robot base location in the camera frame: ð400; 200; 2 1900Þ mm
G
Mirror
frame
location
(with
origin
at
sphere’s
center)
in
the
camera
frame:
ð0; 0; 2 4000Þ mm
G
Spherical mirror radius: 500 mm
G
Camera intrinsic parameters: λu 5 2998:97; λv 5 2916:23; uoffset 5 342:70; voffset 5
236:88
G
Landmarks’ midpoint in robot frame: ð400; 500; 250Þ mm
G
Landmarks’ connecting line initial orientations: φ 5 π=6 rad; θ 5 π=2 rad
G
Desired landmarks’ midpoint in robot frame: ð200; 250; 200Þ mm
G
Landmarks’ connecting line desired orientations: φ 5 π=3 rad; θ 5 π=6 rad
G
Deviation of camera intrinsic and extrinsic parameters from their real values: average
130%
G
Variance of zero-mean Gaussian noise: 2 pixels
Figure 13.47AD shows a set of results.
These results verify the decay over time of the features’ errors, the accuracy of
the EKF estimates of the camera parameters and image features, and show the actual
trajectories of the landmarks in the 3D workspace.
584
Introduction to Mobile Robot Control

References
[1] Gholipour A, Yazdanpanah MJ. Dynamic tracking control of nonholonomic mobile robot
with model reference adaptation for uncertain parameters. In: Proceedings of 2003
European control conference (ECC’2003), Cambridge, UK; September 14, 2003.
[2] Pourboghrat F, Karlsson MP. Adaptive control of dynamic mobile robots with nonholo-
nomic constraints. Comput Electr Eng 2002;28:24153.
[3] Zhang Y, Hong D, Chung JA, Velinky SA. Dynamic model based robust tracking con-
trol of a differentially steered wheeled mobile robot. In: Proceedings of American con-
trol conference. Philadelphia, PA; June 1988. p. 85055.
[4] Aicardi M, Casalino G, Bicchi A, Balestrino A. Closed-loop steering of unicycle vehi-
cles via Lyapunov techniques. In: IEEE robotics and automation magazine. March 1995.
p. 2733.
[5] Devon D, Bretl T. Kinematic and dynamic control of a wheeled mobile robot. In:
Proceedings of 2007 IEEE/RSJ international conference on intelligent robots and sys-
tems. San Diego, CA; October 29November 2, 2007. p. 406570.
Time (s)
l1
(A)
(B)
(D)
(C)
0
–20
20
0
–20
20
0
–20
20
0
–20
20
0
–20
20
0
–20
20
0
–20
20
0
–20
20
0
–20
20
0
–20
20
0
–20
20
0
–20
20
0
–20
20
0
–20
20
0
–20
20
5
10
15
20
25
30
5
10
15
20
25
30
5
10
15
20
25
30
5
10
15
20
25
30
5
10
15
20
25
30
0
5
10
15
20
25
30
0
5
10
15
20
25
30
0
5
10
15
20
25
30
0
5
10
15
20
25
30
0
5
10
15
20
25
30
0
d1
0
d3
0
l2
0
d2
0
5
10
15
20
25
30
0
5
10
15
20
25
30
0
5
10
15
20
25
30
0
5
10
15
20
25
30
0
5
10
15
20
25
30
0
Time (s) 
x
y
z
φ
Θ
0
–300
200
400
–100
0
100
200
300
340
380
420
460
500
Start
Start
Goal
Goal
x
y
Uoffse
tVof
λfset
u
λv
d
Time (s)
z
Figure 13.47 (A) Time evolution of the image features’ errors. (B) Camera parameters’
estimates. (C) Midpoint frame pose. (D) 3D trajectories of the landmarks.
Source: Reprinted from Ref. [34], with permission from International Society for Optical
Engineering (SPIE).
585
Experimental Studies

[6] Watanabe K, Yamamoto T, Izumi K, Maeyama S. Underactuated control for nonholo-
nomic mobile robots by using double integrator model and invariant manifold theory.
In: Proceedings of 2010 IEEE/RSJ international conference on intelligent robots and
systems. Taipei, Taiwan; October 1822, 2010. p. 286267.
[7] Rigatos GG, Tzafestas CS, Tzafestas SG. Mobile robot motion control in partially
unknown environments using a sliding-mode fuzzy-logic controller. Rob Auton Syst
2000;33:111.
[8] Rigatos GG, Tzafestas SG, Evangelidis GJ. Reactive parking control of nonholonomic
vehicles via a fuzzy learning automaton. IEE Proc Control Theory Appl 2001;148
(2):16979.
[9] Carelli R, Soria CM, Morales B. Vision-based tracking control for mobile robots. In:
Proceedings of twelfth international conference on advanced robotics (ICAR’05).
Seatle, WA; July 1820, 2005. p. 14852.
[10] Gilioli M, Melchiori C. Coordinated mobile manipulator point-stabilization using
visual-servoing techniques. In: Proceedings of IEEE/RSJ international conference on
intelligent robots and systems (IROS’2002). vol. 1. Lausanne, CH; 2002. p. 30510.
[11] Tsakiris D, Rives P, Samson C. Extending visual servoing techniques to nonholonomic
mobile robots. In: Hager G, Kriegman D, Morse S, editors. Vision and control
(LNCIS). Berlin: Springer; 1998.
[12] Okamoto Jr J, Grassi Jr V. Visual servo control of a mobile robot using omnidirec-
tional vision. In: Van Amerongen J, Jonker B, Regtien P, Stramigiolis S, editors.
Proceedings of mechatronics conference 2002. University of Twente; June 2426,
2002. p. 41322.
[13] Abdelkader HH, Mezouar Y, Andreff N, Martinet P. Image-based control of mobile
robot with central catadioptric cameras. In: Proceedings of 2005 IEEE international
conference on robotics and automation (ICRA 2005). Barcelona, Spain; April 2005.
p. 353338.
[14] Mezouar Y, Abdelkader HH, Martinet P, Chaumette F. Central catadioptric visual ser-
voing from 3D straight lines. In: Proceedings of IEEE/RS international conference on
intelligent robots and systems (IROS’04). Sendai, Japan; 2004. p. 34349.
[15] Tzafestas SG, Melfi A, Krikochoritis T. Kinematic/dynamic modeling and control of
an omnidirectional mobile manipulator. In: Proceedings of fourth IEEE/IFIP interna-
tional conference on information technology for balanced automation systems in pro-
duction and transportation (BASYS2000). Berlin, Germany; 2000.
[16] Papadopoulos E, Poulakakis J. Trajectory planning and control for mobile manipulator
systems. In: Proceedings of eighth IEEE Mediterranean conference on control and
automation (MED’00). Patras, Greece; July 1719, 2000.
[17] Yamamoto Y, Yun X. Coordinating locomotion and manipulation of a mobile manipu-
lator. In: Proceedings of thirty-first IEEE conference on decision and control. Tucson,
AZ; 1992. p. 264348.
[18] Yamamoto Y, Yun X. Modeling and compensation of the dynamic interaction of a
mobile manipulator. In: Proceedings of IEEE conference on robotics and automation.
San Diego, CA; 1994. p. 218792.
[19] Zavlangas PG, Tzafestas SG, Althoefer K. Navigation for robotic manipulators
employing fuzzy logic. In: Proceedings of third world conference on integrated design
and process technology (IDPT’98). vol. 6. Berlin, Germany; 1998. p. 27883.
[20] Zavlagas PG, Tzafestas SG. Integrated fuzzy global path planning and obstacle avoid-
ance for mobile robots. In: Proceedings of European workshop on service and human-
oid robots (SERVICEROB’2001). Santorini, Greece; 2001.
586
Introduction to Mobile Robot Control

[21] Tzafestas SG, Stamou G. A fuzzy path planning algorithm for autonomous robots mov-
ing in an unknown and uncertain environment. In: Proceedings of European robotics
and intelligent systems conference (EURISCON’94). Malaga, Spain; August 1994.
p. 14049.
[22] Vidal Calleja TA. Visual navigation in unknown environments. PhD thesis. Barcelona:
IRI, The Universitat Polite`cnica de Catalunya; 2007.
[23] Rekleitis I, Dudek G, Milios E. Multirobot collaboration for robust exploration.
Ann Math Artif Intell 2001;31(14):740.
[24] Rekleitis I, Dudek G, Milios E. Probabilistic cooperative localization and mapping in
practice. In: Proceedings of IEEE international conference on robotics and automation.
vol. 2. Taipei, Taiwan; 2003. p. 190712.
[25] Velagic J, Osmic N, Lacevic B. Neural network controller for mobile robot motion
control. World Acad Sci Eng 2008;23:1938.
[26] Awad HA, Al-zorkany M. Mobile robot navigation using local model network. Trans
Eng Comput Technol 2004;VI:32631.
[27] Skoundrianos EN, Tzafestas SG. Fault diagnosis via local neural networks. Math
Comput Simul 2002;60:16980.
[28] Skoundrianos EN, Tzafestas SG. Finding fault: diagnosis on the wheels of a mobile
robot using local model neural networks. IEEE Rob Autom Mag 2004;11(3):8390.
[29] Tzafestas SG, Raptis S. Fuzzy image processing: a review and comparison of methods.
Image Process Commun 1999;5(1):324.
[30] Castillo O, Aguilar LT, Cardenas S. Fuzzy logic tracking control for unicycle mobile
robots. Eng Lett 2006;13(2):737 [EL.13-2-4].
[31] Yang F, Wang C. Adaptive tracking control for uncertain dynamic nonholonomic
mobile robots based on visual servoing. J Control Theory Appl 2012;10(1):5663.
[32] Wang C, Liang Z, Du J, Liang S. Robust stabilization of nonholonomic moving robots
with uncalibrated visual parameters. In: Proceedings of 2009 American control confer-
ence. Hyatt Regency River Front, St. Louis, MO; 2009. p.134751.
[33] Zhang Y. Visual servoing of a 5-DOF mobile manipulator using panoramic vision system.
M.A.Sc. thesis. Regina, Canada: Faculty of Engineering, University of Regina; 2007.
[34] Zhang Y, Mehrandezh M. Visual servoing of a 5-DOF mobile manipulator using a cata-
dioptric vision system. Proc SPIE Conf Optomechatronic Syst Control III 2007;
6719:617.
587
Experimental Studies

14 Generic Systemic and Software
Architectures for Mobile Robot
Intelligent Control
14.1
Introduction
In this book we have studied several controllers of mobile robots, and provided a
set of fundamental issues regarding the high-level functions of path/motion plan-
ning, task planning, localization, and mapping. In Chapter 13, we have presented a
host of experimental results (most of which are simulation results) of the above
methods and controllers. In this chapter, we will be concerned with systemic and
software architectures which can be used for integrating controllers and high-level
functional units to achieve overall intelligent performance of a mobile robot.
Software architectures help to deal with the high degree of heterogeneity among
the subsystems involved, to face strict operational requirements posed by real-time
interactions with the robot’s environment, and to treat the system’s complexity that
goes beyond the capabilities of a single designer. In general, robot software is the
coded commands that tell the robot what functions to perform in order to carry out
and control its actions. Actually, the development of robot software is a nontrivial
task. Many software frameworks and systems have been developed to make robot
programming easier, to achieve a proper run-time for fault tolerant execution, and
to obtain systems that are portable to other robotic applications.
Unfortunately, there is no widely accepted software standard for developing
mobile robot applications. Robot companies provide their own development frame-
works such as ERSP from Evolution Robotics, Open-R from Sony, ARIA from
ActivMedia, and Saphira from SRI International. On the other hand, university
research groups have developed particular software platforms such as:
G
Miro [1]
G
CLARAty [2]
G
Marie [3]
G
Player/Stage [4]
G
CARMEN [5]
The purpose of this chapter is to provide an overview of fundamental concepts and
architectures that integrate low-, medium-, and high-level control and planning func-
tions of mobile robotic systems. Specifically, the following issues are considered:
G
Hierarchical, multiresolutional, reference model, and behavior-based intelligent control
system architectures
Introduction to Mobile Robot Control. DOI: http://dx.doi.org/10.1016/B978-0-12-417049-0.00014-6
© 2014 Elsevier Inc. All rights reserved.

G
Basic characteristics of mobile robot control software architectures
G
Two examples of mobile robot control software architectures
G
Comparative evaluation of two mobile robot control software architectures
G
Intelligent humanrobot interfaces
G
Two integrated intelligent mobile robot research prototypes
G
Design for heterogeneity and modularity
14.2
Generic Intelligent Control Architectures
14.2.1
General Issues
Intelligent control (IC) has an age of over 40 years and constitutes a generalization
of traditional control of the 1940s and 1950s and modern control of the 1960s and
1970s to incorporate autonomous human-like interactive behavior of the controller
with the environment. The term “intelligent control” was coined by Fu [6] to
embrace the area beyond adaptive and learning control, and according to Saridis
[7,8] represents the field which merges control, artificial intelligence (AI), and
operational research (OR).
Intelligent control provides the means to achieve autonomous behavior such as
planning at different levels of detail, imitation of human behavior, learning from
past experience, integration and fusion of sensor information, identification of
abrupt changes in system operation, and proper interaction with a changing envi-
ronment [9].
The field of intelligent control started with the development of generic IC archi-
tectures (ICAs), which are mainly the following:
G
Hierarchical ICA (Saridis)
G
Multiresolutional/nested ICA (Meystel)
G
Reference model ICA (Albus)
G
Behavior-based ICAs, namely, subsumption ICA (Brooks) and motor schemas ICA
(Arkin)
These architectures were expanded, enriched, or combined over the years in sev-
eral ways [10,11]. In the following, we give a brief overview of these architectures
in their original abstract form. Most of the software systems and integrated
hardwaresoftware systems developed for intelligent mobile robot control follow
in one or the other way one of these generic architectures or suitable combinations
of them. This will be clear from the discussions of Sections 14.4, 14.5, and 14.7.
14.2.2
Hierarchical Intelligent Control Architecture
This architecture has three main levels, namely (Figure 14.1):
1. Organization level
2. Coordination level
3. Execution level
590
Introduction to Mobile Robot Control

These levels may involve several layers within them that follow the human con-
trol mode of interaction between a director (supervisor) with his subordinate
employees [7,8].
The organization level implements the higher level functions (e.g., learning,
decision making) which imitate functions of human performance and can normally
be represented and treaded by artificial intelligence techniques. This level receives
and interprets feedback information from the lower levels, defines the planning/
sequencing and decision making strategies to be executed in real time, and pro-
cesses large amounts of knowledge/information with little or no precision. Here,
long-term memory exchange is taking place.
The coordination level consists of several coordinators (each implemented by a
piece of S/W or a dedicated microprocessor) which receive the task(s) from the
organization level. All necessary details must be provided so that the chosen task’s
plan is successfully executed.
The execution level involves the actuators, the hardware controllers, and the
sensing devices (visual, sonar, etc.) and executes the action programs issued by the
coordination level.
Saridis has developed a complete analytic theory for this architecture, formulat-
ing and exploiting the Principle of Increasing Precision with Decreasing
Intelligence using the information entropy concept. Neural networks, fuzzy systems,
Petri nets, and optimal control have been used in these hierarchical levels [12].
14.2.3
Multiresolutional Intelligent Control Architecture
This architecture was developed by Meystel [1315] and first applied to intelligent
mobile robots. It follows the commonsense model Planner-Navigator-Pilot-
Execution Controller. The Planner delivers a rough plan. The Navigator computes
a more precise trajectory of the motion to be executed. The Pilot develops online
tracking open-loop control. Finally, the Execution Controller executes plans and
compensations computed by the planner, the navigator, and the pilot. This scheme
Coordination
level
Execution
level
Increasing
intelligence
Increasing
precision
Organization
level
Figure 14.1 Hierarchical control
architecture.
591
Generic Systemic and Software Architectures for Mobile Robot Intelligent Control

is implemented in the form of the so-called multiresolution six-box (Figure 14.2A).
Each level contains perception (P), knowledge representation, interpretation and
processing (K), and planning and control (P/C) operations which are shown in
more detail in Figure 14.2B.
In Figure 14.2B, A is a source and a storage of the world model, B is a computer
controller which processes the sensor data and computes the control commands
required to achieve the system’s goal, and C is the machine that performs the pro-
cess of interest with actuators that transform control commands into actions, and
with sensors that inform the computer controller about the process.
The fundamental properties of the multiresolutional ICA are the following:
P1: Computational independence of the resolutional levels.
P2: Each resolution level represents a different domain of the overall system.
P3: Different resolution levels deal with different frequency bands within the overall system.
P4: Loops at different levels are six-box diagrams nested in each other.
P5: The upper and lower parts of the loop correspond to each other.
P
P
P
S
K
K
K
W
P/C
A
First level
Second level
Third level
P/C
P/C
Expert
Knowledge
base
Existing
models
Process
identification
Situation
recognition
Output
estimation
Information
organization
Planning/
control
A
B
Sensors
Actuation
Process
C
(A)
(B)
M
N
Figure 14.2 (A) Three-level
multiresolution architecture (6-
box-representation). (B) Each level
has its own feedback loop.
592
Introduction to Mobile Robot Control

P6: The system behavior is the result of superposition of the behaviors generated by the
actions at each resolution level.
P7: The algorithms of behavior generation are similar at all levels.
P8: The hierarchy of representation evolves from linguistic at the top to analytical at the
bottom.
P9: The subsystems of the representation are relatively independent.
14.2.4
Reference Model Intelligent Control Architecture
This architecture (RMA) was developed and expanded at the National Institute of
Standards (NIST) by Albus and colleagues [1618]. It is suitable for modular
expansion (Figure 14.3).
The control problem in the reference model ICA is decomposed in the following
subproblems:
G
Task decomposition
G
World modeling
G
Sensory processing
G
Value judgment
The various control elements are clustered into computational nodes arranged in
hierarchical layers, each one of which has a particular function and a specific
Global 
database
Objects
State 
variables
Models
Detect
integrate
Model
evaluate
Plan
execute
G2
G3
Evaluation
functions
G4
G5
M5
H5
M4
H4
M3
H3
M2
H2
Sensory
processing
World
modeling
Task
decomposition
Mission
Group
Vehicle task
E-move
Primitive
Coordinate
transform
servo 
Action
Sense
G1
M1
H1
Figure 14.3 The NIST RMA hierarchical control architecture.
593
Generic Systemic and Software Architectures for Mobile Robot Intelligent Control

timing behavior. The NIST paradigm has been refined many times. Starting with
the cerebellar model in the 1970s, it was evolved to the RCS-4 (Real-time Control
System-4) in the 1990s and was applied to automated manufacturing systems, army
field material handling systems, multiple underwater mobile robots, and telerobotic
service systems. The main design issues addressed by RMA are the following:
G
Real-time task and software execution
G
Smart interface/communication methods
G
Information/knowledge base management
G
Optimized allocation of resources
14.2.5
Behavior-Based Intelligent Control Architectures
These architectures are based on the concept of agent and can be implemented
using knowledge-based systems, neural, fuzzy or neurofuzzy structures [1419].
The two most common behavior-based architectures are the subsumption architec-
ture developed by Brooks [20,21] and the motor schema architecture developed by
Arkin [2224]. The subsumption architecture follows the decomposition of the
behavior paradigm (Figure 14.4B) and was first employed in the autonomous
mobile robot Shakey.
The tasks, achieving behavior, are represented as separate layers. Individual
layers work on individual goals concurrently and asynchronously. At the lowest
level, the system behavior is represented by an augmented finite state machine
(AFSM) shown in Figure 14.5.
The term “subsumption” originates from the verb “to subsume” which means to
think about an object as taking part of a group. In the context of behavioral robot-
ics, the term subsumption comes from the coordination process used, between the
layered behaviors within the architecture. Complex actions subsume simple beha-
viors. Each AFSM performs an action and is responsible for its own perception of
the world [15,16].The reactions are organized in a hierarchy of levels where each
level corresponds to a set of possible behaviors. Under the influence of an internal
Sensors
(A)
(B)
Pe
rce
pti
on
M
od
eli
ng
Pla
nn
in
g
Tas
k
ex
ec
uti
on
m
ot
or
co
ntr
ol
Actuators
Sensors
Manipulate the world
Build maps
Explore
Avoid hitting things
Locomote
Actuators
Figure 14.4 Distinction between the
classical sense-plan-act model (A) and
the subsumption model (B).
594
Introduction to Mobile Robot Control

or external stimulation, a particular behavior is required. Then, it emits an influx
toward the inferior level. At this level, another behavior arises as a result of simul-
taneous action of the influx and other stimuli. The process continues until terminal
behaviors are activated. A priority hierarchy fixes the topology. The lower levels in
the architecture have no awareness of higher levels. This allows the use of incre-
mental design. That is, higher level competencies are added on top of an already
working control system without any modification of those lower levels.
The motor schemas architecture was more strongly motivated by biological
sciences and uses the theory of schemas, the origin of which goes back to the eigh-
teenth century (Immanuel Kant). Schemas represent a means by which understand-
ing is able to categorize sensory perception in the process of realizing knowledge
of experience. The first applications of schema theory include an effort to explain
postural control mechanisms in humans, a mechanism for expressing models of
memory and learning, a cognitive model of interaction between motor behaviors in
the form of schemas interlocking with perception in the context of the perceptual
cycle, and a means for cooperation and competition between behaviors.
From among the various definitions of the schema concept available in the liter-
ature, we give here the following representative ones [17,18]:
G
A pattern of action or a pattern for action
G
An adaptive controller which is based on an identification procedure for updating the
representation of the object under control
G
A perceptual entity corresponding to a mental entity
G
A functional unit that receives special information, anticipates a possible perceptual con-
tent, and matches itself to the perceived information
A convenient working definition is the following [19]: “A schema is the funda-
mental entity of behavior from which complex actions can be constructed, and
which consists of the knowledge how to act or perceive, as well as the computa-
tional process by which it is enacted.”
Using schemas, robot behavior can be encoded at a coarser granularity than neu-
ral networks while maintaining the features of concurrent cooperativecompetitive
control involved in neuroscientific models. More specifically, schema theory-based
analysis and design of behavior-based systems possesses the following capabilities:
G
It can explain motor behavior in terms of the concurrent control of several different activities.
G
It can store both how to react and how to realize this reaction.
G
It can be used as a distributed model of computation.
Behavioral
unit
I
R
S
Input
wires 
Inhibitor
Reset
Suppressor
Output
wires
Figure 14.5 AFSM employed in the
subsumption architecture.
595
Generic Systemic and Software Architectures for Mobile Robot Intelligent Control

G
It provides a language for connecting action and perception.
G
It provides a learning approach via schema elicitation and schema tuning.
G
It can explain the intelligence functions of robotic systems.
Motor schema behaviors are relatively large grain abstractions, which can be
used in a wide class of cases. Typically, these behaviors have internal parameters
which offer extra flexibility in their use. Associated with each motor schema there
is an embedded perceptual schema which gives the world specific for that particu-
lar behavior and is capable of providing suitable stimuli.
Three ways in which planning (deliberative) and reactive behavior can be
merged are [24] as follows:
G
Hierarchical integration of planning and reaction (Figure 14.6A)
G
Planning to guide reaction, that is, permitting planning to select and set parameters for
the reactive control (Figure 14.6B)
G
Coupled planningreacting, where these two concurrent activities, each guides the other
(Figure 14.6C)
One of the first robotic control schemes that were designed using the hybrid
deliberative (hierarchical) and reactive (schema-based) is the autonomous robot
architecture (AuRA) [24]. AuRA incorporated a traditional planner that could rea-
son over a modular and flexible behavior-based control system (Figure 14.7).
14.3
Design Characteristics of Mobile Robot Control
Software Architectures
To design or evaluate a robot control software architecture, the following desirable
key characteristics should be considered [25]:
G
Robot hardware abstraction
G
ExtendibilityScalability
Highest
level
Middle
levels
Lowest
level
More
reactive
More
deliberative
Deliberation
Projection
Behavioral advice 
Selection and 
configuration of 
parameters
Reaction
unit
Planner
Reactor
(A)
(B)
(C)
Figure 14.6 (A) Hierarchical hybrid deliberativereactive structure. (B) Planning to guide
reaction scheme. (C) Coupled planning and reacting scheme.
596
Introduction to Mobile Robot Control

G
Reusability
G
Repeatability
G
Run-time overhead
G
Software features
G
Tools and techniques
G
Documentation
A brief description of each of them is as follows.
Robot hardware abstraction: A primary design goal of hardware design is porta-
bility because robot hardware is generally changing. Abstraction of hardware such
as actuators and sensors must be accommodated in a portable architecture.
Typically, the hardware provided by manufacturers involve hardware-specific com-
mands (such as to move in absolute or velocity mode) which are encapsulated into
a generalized set of commands. It is highly desirable to keep the hardware charac-
teristics in a single file of the software source. This file must be the only place
where changes have to be performed when moving the system to a new hardware.
Extendibilityscalability: Extendibility is the capability to add new software
components and new hardware modules to the system. This is a very important
characteristic since robotic systems in research and development environments
evolved in terms of both hardware and software. For example, the addition of new
sensors is a typical process in these environments. Scalability can be achieved
using dynamic objects and process invocation on a live-when-needed basis.
Modern software tools have factory patterns that help in this.
Reusability: Reusing existing knowledge from previous designs can speed up
the mobile robot software development. A popular approach to this is through soft-
ware reuse of components, structure, framework, and software patterns [26,27].
Software patterns are classified according to three software development levels,
namely, analysis or conceptual patterns for the analysis level, design patterns for
the design level, and programming patterns for the implementation level.
Plan recognition 
User profile
Spatial learning
Opportunism
Online
adaptation
User intentions
Spatial goals
Mission 
alterations
Teleautonomy
Mission planner
Spatial reasoner
Plan sequencer
R
e
p
r
e
s
e
n
t
Schema Controller
Motor
Perceptual
Actuation
Sensing
Hierarchical 
component
Reactive 
component
Learning
User input
Figure 14.7 General AuRA structure.
597
Generic Systemic and Software Architectures for Mobile Robot Intelligent Control

Repeatability: Repeatability means that running the same program on the
same input gives the same result. For typical single-threaded programs, repeat-
ability is a must for functional correctness. For real-time distributed systems,
repeatability is not necessary for correctness. A simple form of repeatability
allows the developer to debug an individual task by rerunning it on logged data,
presenting logged messages to a task in the same order in which they were
received.
Run-time overhead: When a program is running (executing), we say that it is in
run-time. The term run-time is used by a computer language to manage a program
written in the language. A run-time error is an error that occurs while the program
is executing. Run-time overhead is specified by several issues, such as memory
requirements, CPU requirements, frequency, and end-to-end latency.
Software features: A mobile robotic control system must be reliable and robust
to unexpected events. The framework for robust integration should integrate all
skills. For research and development purposes, software architecture, besides reus-
ability and repeatability, should provide the means for the following:
G
Simple integration of new devices and units
G
Clear distinction between levels of competence
G
Prototyping
G
Simple debugging
For a general software system, the following features are important:
G
Design simplicity (in both the implementation and interface)
G
Design correctness (in all aspects)
G
Design consistency (i.e., absence of inconsistencies)
G
Design completeness (i.e., coverage of as many important aspects as it is practical)
A good architecture must be based on a formal theory which is adhered by the
software developers.
Tools and methods: Today, several tools for constructing software architec-
tures, standardized by international bodies (ISO, ANSI, OMG, etc.), are available.
The hardware providers offer the basic interface for evaluating the hardware.
This may be a C language API. Early software systems for mobile robots were
almost exclusively programmed in C. Artificial intelligence workers were using
LISP. Now, we use popular OO-based languages such as C1 1 and Java or the
component technology CORBA (common object request broker architecture).
Very popular are also the LabVIEW (laboratory virtual instrumentation engineer-
ing workbench) of national instruments (NI), and the UML (unified modeling lan-
guage) which is supported by tools that allow the automatic synthesis, analysis,
and code generation.
A key component in a robotic system is a reliable and efficient communication
mechanism for exchanging and transmitting data and events. Data transfer can be
initiated in either a pull or a push way. The interaction of tools such as CORBA,
Microsoft Active X, and enterprise Java beans (EJB) can be achieved through inter-
face description language (IDL).
598
Introduction to Mobile Robot Control

Documentation: Software architecture should be accompanied with proper and
rigorous documentation, which can be used nonlocally, and includes the following:
G
The architecture’s philosophy
G
A programmer’s guide
G
A user’s guide
G
A reference manual
G
Code documentation
Obviously, the philosophy of an architecture remains the same throughout, but the
other elements of documentation should be refreshed to reflect any changes or improve-
ments performed over the time. Documentation can be made in the following ways:
G
Printed manuals
G
Web-based documentation
G
UML/class diagrams
G
Comments in the source
A combination of them is very useful and desirable. Today, there are available
JavaDoc/Doxygen utilities that are embedded in the code.
14.4
Brief Description of Two Mobile Robot Control
Software Architectures
14.4.1
The Jde Component-Oriented Architecture
The Jde architecture uses schemas which are combined in dynamic hierarchies to
unfold the global behavior [28,29]. Each schema is built separately into a plug-in
and linked to the framework dynamically when required. The Jde software archi-
tecture follows the hierarchical scheme shown in Figure 14.6A, that is, it combines
deliberation and reactiveness in a proper and successful way. Each schema is a
task-oriented piece of software which is executed independently. At any time, there
may be in execution several schemas, each one designed to achieve a goal or com-
plete a particular task. In Jde, a schema:
G
is tunable (i.e., it can modulate its behavior accepting continuously some parameters);
G
is an iterative process (i.e., it performs its work via periodical iterations giving an output
when each iteration is finished);
G
can be stopped or resumed at the end of any iteration.
In Jde, hierarchy is considered as a co-activation that only means predisposition.
A parent can coactivate several children at the same time, but this does not mean
that all children gain control of the robot. The children’s real activation is deter-
mined by an actionselection mechanism that continuously selects which one gets
the control at each iteration, given the current goal and environment condition.
Three
advantages of this hierarchical scheme are reduced complexity for
actionselection, actionperception coupling, and distributed monitoring. A motor
599
Generic Systemic and Software Architectures for Mobile Robot Intelligent Control

schema may control the actuators directly or may awake a set of new child sche-
mas. The sequence of activations creates a specific hierarchy of schemas for pro-
ducing a particular global behavior (Figure 14.8) [28]. All awake schemas
(checking, ready, winner) run concurrently. Hierarchies are specific to each global
behavior. Once the parent has awaked its children schemas, it continues executing
and checking its own preconditions, monitoring the actions of its children, and
modulating them appropriately. All the active schemas, except the winner one, are
deactivated, and a new tree is generated under the new winner.
In Figure 14.8, circles represent motor schemas, and squares represent percep-
tual schemas. The Jde architecture was implemented in the Jdec software platform
in the language C. The Jdec platform supports the hardware of Pioneer robot
equipped with additional vision sensors. Several schema-based behaviors were con-
structed, namely, person following, laser-based and vision-based localization, vir-
tual force field-based local navigation, and gradient-based deliberative global
navigation. In the hierarchy, each schema provides a set of shared variables for
communication with other schemas which is performed by shared memory. When
in winner state, a schema specifies and updates continuously its output. Perceptive
schemas do not take part in the actionselection process and always gain easily the
winner state.
The pseudocode of a Jdec schema is [29]:
Initialization code
Loop
If (slept) stop-the-schema
Action_selection
Check preconditions
Check brother’s state
If (collision OR absence)
father_arbitrates
If (winner) then schema_iteration
msleep
End loop
Due to the iterative execution style, CPU consumption is moderate and facili-
tates the design of an application in a reactive fashion. Each schema is written in
two separate C files, namely:
1. myschema.h (with the declaration of shared variables)
2. myschema.c
1
2
5
6
7
8
12
13
14
15
18
19
21
22
24
25
27
28
3
4
9
11
10
16
17
20
23
26
Figure 14.8 Jde hierarchical
architecture where there is one winner
(denoted in shaded) at each level.
600
Introduction to Mobile Robot Control

They are both compiled jointly in a single C module. All schemas of an applica-
tion are statically linked together in the executable.
In the enhanced Jde architecture (called Jde-neoc), several new tools were
developed and added. These are as follows:
G
A visualization tool for the visualization of sensors, actuators, and other elements
G
A management tool which allows the manual activation and deactivation of schemas, as
well as their graphical user interface (GUI). This helps very much the debugging of any
set of schemas.
In Jde-neoc, the perception and control are distributed among a set of schemas
which are software elements, with a clear API each built as a plug-in on a separate
file. Details on the design and implementation of Jde and Jde-neoc can be found in
Refs. [28,29].
14.4.2
Layered Mobile Robot Control Software Architecture
This is simple software architecture of the type shown in Figure 14.1, which
involves three or four hierarchical layers, each of which depends only on the spe-
cific hardware platform used, and is not informed on the contents of layer above or
below it [30]. This layered architecture is depicted in Figure 14.9 for a mobile
robot or manipulator, which includes all necessary high-level and low-level func-
tions from task definition, path planning, and sensor fusion to sensor/actuator inter-
facing and motor control.
For fully autonomous operation, the layers 2 through 4 are needed; the layer 1 is
not always required.
User interface
layer
Algorithm layer
Platform layer
Driver layer
• Video display
• Teleoperation
• Robot  health
• Status feedback
Layer 1
Layer 2
Layer 3
Layer 4
• Path planning
• Obstacle avoidance
• Mapping
• Task definition
• Steering
• Sensor fusion
• Image processing
• Robot model 
• Actuator interface
• Sensor interface
• Robot model
Figure 14.9 Layered architecture of
a mobile robot/manipulator.
601
Generic Systemic and Software Architectures for Mobile Robot Intelligent Control

This software architecture was designed and developed for implementation on
the NI CompactRIO platform for robotics combined with the LabVIEW graphical
development environment.
LabVIEW is typically used for data acquisition, instrument control, and indus-
trial automation on a variety of platforms including Microsoft Windows, and sev-
eral versions of UNIX, Linux, and MacOS [31]. The programming language used
in LabVIEW is called G, and is a data flow programming language. The execution
sequence of the LabVIEW graphical syntax is as well defined as with any textually
coded language (C, Visual BASIC, etc.). LabVIEW embeds into the development
cycle, the construction of user interfaces (called “front panels”). LabVIEW pro-
grams and subroutines are called virtual instruments (VIs), each one of which has
three elements (block diagram, front panel, connector panel). The front panel exhi-
bits controls and indicators that allow a user to input data into or extract data from
a running VI. In addition, the front panel can also act as a programming interface.
A benefit of LabVIEW over other development environments is the extensive sup-
port for accessing instrumentation hardware (http://ni.com//labview).
A short description of the functions performed in each layer of the architecture
is as follows.
User interface layer: The user interface (UI) allows a human operator to interact
physically with the robot via relevant information provided on the host PC. To dis-
play live data from an on-board or fixed camera or the xy coordinates of nearby
obstacles on a map, a GUI should be employed. This layer can also be used for
reading input data from a mouse or joystick or to drive a simple display. An emer-
gency (high priority) stop must also be included in this layer.
Algorithm layer: This layer involves the high-level control algorithms of the
robot. Its units obtain information (position, speed, video images) and make feed-
back control decisions for the tasks which the robot has to perform. Here, the com-
ponents for mapping the robot’s environment and performing obstacle avoiding
path/motion planning, as well as for high-level task planning are included.
Platform layer: Here, the code that corresponds to the physical hardware config-
uration is contained. Actually, it can be used as a translator between the driver
layer and algorithm layer. This layer converts low-level information (from the sen-
sors’ interface and actuators’ interface) to a more complete form to be sent at the
algorithm layer, and vice versa.
Driver layer: Here, the low-level driver functions needed to move the robot are
generated, depending on the sensors and actuators used and the hardware in which
the driver software runs. The actuator set points (for position, velocity, torque, etc.)
are received by the driver layer in engineering units and converted to low-level sig-
nals, potentially including code to close the appropriate loops over those set points.
Similarly, the raw sensor data are turned into meaningful units and passed to the
other layers of the architecture. The driver level can be implemented in field pro-
grammable gate array (FPGA). In the NI architecture, the driver code is implemen-
ted in LabVIEW FPGA and executes on an embedded FPGA on an NI
CompactRIO platform. The driver can be connected to physical sensors or actua-
tors, or it can interface to simulated inputoutput data within a simulator of the
602
Introduction to Mobile Robot Control

environment. For research and development purposes, a switch between simulation
and actual hardware must be provided, which operates without affecting the other
layers. An overall pictorial representation of the above mobile robot reference con-
trol software architecture overlayed on an NI CompactRIO or NI Single-Board RIO
embedded system is shown in Figure 14.10 [30].
The above architecture is similar to that used in the NASA mobile manipulators
designed by “Superdroid Robots” (see Figure 1.30) (http://superdroid.com/#customized-
robots-and-robot-parts).
14.5
Comparative Evaluation of Two Mobile Robot Control
Software Architectures
14.5.1
Preliminary Issues
Here, a comparative evaluation of two mobile robot control software systems
drawn from Ref. [25] will be summarized. The two systems evaluated are the
Saphira architecture, which was developed at SRI International Artificial Center
[32], and behavior-based robot research architecture (BERRA) [33].
Saphira architecture was developed to exert intelligent control to the Flakey mobile
robot [34] following the perceptionaction cycle scheme. The software runs a fuzzy
logic-based reactive planning and a behavior sequencer. The system includes inte-
grated modules for sonar sensor interpretation, mapping, and navigation. The core of
the system consists of a server that manages the hardware, and Saphira software as a
client to this server. The Saphira architecture is depicted in Figure 14.11A and B [32].
Embedded real-time processor
Algorithm layer
Platform layer
I/O
Embedded FPGA
Driver layer
• Sensor interface
• Actuator interface
• Control
Sensors
Actuators
Sensors
Host PC
User 
interface 
layer
Figure 14.10 Mobile robot reference control software architecture overlayed onto an
embedded real-time processor and FPGA.
603
Generic Systemic and Software Architectures for Mobile Robot Intelligent Control

Saphira and user routines in Figure 14.11A are all microtasks that are invoked
during every synchronous cycle (100 ms) by built-in microtasking operating system.
These routines realize packet communication with the robot, create the robot’s state
picture, and carry out more complex tasks such as sensor interpretation and naviga-
tion. The internal State Reflector helps to avoid the tedious task of control programs
to deal with packet communication issues, reflecting well the robot’s state on the
host computer. The robot server can handle up to 10 routines or more per 100 ms
cycle. Additional user routines on the left subsystem of Figure 14.11A can be exe-
cuted asynchronously as separate threads sharing the same address space.
Saphira client processes
User micro-tasks
and activities
Control and
application routines
State reflector
Packet
communications
Synchronous
microtasking
operating system
User 
asynchro-
nous 
routines
Super scout
TTY or TCP/IP
connection
(A)
Display
routines
Multirobot
interface 
Gradient
real-path
planner
TCP/IP link to
other agents
Markov
localization
routines 
Sensor 
interpretation 
routines
Global map
space
Local
perceptual
space
Colbert
executive
Behavioral
control
Direct
motion
control
State reflector
(B)
Figure 14.11 (A) Saphira system
architecture. (B) Saphira control
architecture.
604
Introduction to Mobile Robot Control

As shown in Figure 14.11B, the control architecture is built on the top of the
state reflector, and consists of a set of microtasks/asynchronous tasks that imple-
ment all navigation functions, interpreting the sensor readings about a geometric
world model, and mapping the robot states to control functions.
Localization is performed using Markov-type routines that connect the robot’s
local sensor readings to its map of the world. The multirobot interface links the
robot to other robots via TCP/IP connections [35]. The path planning is performed
combining the two available geometric representations for local path planning
(local perceptual space, LPS) and global path planning (global map space, GMS).
BERRA was developed with primary design goals the flexibility and scalability.
It was implemented using the adaptive communication environment (ACE) package
[36]. The use of ACE allows the portability of the system across a large class of
operating systems and provides powerful means for server/client interaction and
service functioning.
The evaluation presented in Ref. [25] was carried out using a test-case service
agent, where the operator could command the robot to navigate in an office envi-
ronment for which a map was known a priori. The two architectures were ported to
the Nomadic Super Scout robot shown in Figure 14.12.
This is a small robot having 16 ultrasonic sensors (Polaroid) and 6 tactile sen-
sors. A serial port is used for the communication of the motherboard and the con-
troller board. The Scout robot is equipped with the Red Hat Linux operating system
and a C language API.
14.5.2
The Comparative Evaluation
14.5.2.1
Operating System and Language Support
Saphira: Most operating systems are supported (UNIX, MS Windows, etc.). The GUI
is based on Motif. The core of the system is programmed in C. It has a C-like syntax
with finite state machine-based semantics, and part of Saphira is written in LISP.
Figure 14.12 The Nomadics super scout
mobile robot.
Source: http://ubirobot.ucd.ie/content/
nomad-scout-2-and-nomad-super-scout.
605
Generic Systemic and Software Architectures for Mobile Robot Intelligent Control

BERRA: Linux and Solaris Operating Systems and all ACE platforms are sup-
ported. The Esmeralda speech recognition system is used [37], and vision functions
employ Blitz11 [38].
14.5.2.2
Communication Facilities
Only BERRA is of the multiprocess communication. BERRA uses sockets based
on ACE, and support UNIX and INET socket protocols.
14.5.2.3
Hardware Abstraction
In Saphira, hardware abstraction is performed in the robot server (i.e., there is only
one abstraction level). Client processes cannot address the lower level hardware.
BERRA also has one high-level abstraction, but control of lower level hardware can
be achieved by parameterizing the high-level commands using a difficult syntax.
14.5.2.4
Porting and Application Building
Saphira porting of the hardware level code needs major effort. In the study
described in Ref. [20], only the source code of the Pioneer platform server was
available (actually a number of C files with no clear interdependencies). All rele-
vant behaviors were provided with the system, and the construction of the map and
its incorporation into the LPS was easy. So, localization performed well out of the
box. Originally, BERRA ran on Nomad 2000 and Nomad 4000. Although the hard-
ware of them can accept calls from multiple clients, the Scout can only be accessed
by one client and created a problem for BERRA. A newer version of BERRA
allowed only one of its processes to access and control the robot hardware.
14.5.2.5
Run-Time Consideration
Saphira can be easily started by first starting the robot server and then Saphira
which then is connected to the server. Then, behaviors and tasks can be directly
started and stopped by the operator in the GUI with the Colbert interpreter.
Libraries can be dynamically loaded in run-time. Using GUI the robot’s position in
the map of the environment can be updated. In Saphira, 10 MB of memory is
sufficient, but the response time (B0.6 s) is high. The major drawback of Saphira
is the inaccuracy of the localization system (position track is lost after approxi-
mately 10 m).
BERRA can be started by a shell-script as long as everything goes as planned. If
for any reason a process goes wrong, the system needs to be totally restarted. The
time needed from a sensor reading to a corresponding actuator control signal is
very low (about 0.17 s). BERRA needs 36 MB of run-time memory, but does not
have a GUI. In the tests, BERRA performed very well (the scout could traverse a
department for hours according to navigation requests).
606
Introduction to Mobile Robot Control

14.5.2.6
Documentation
Saphira is very well documented and supported by many publications and a com-
plete manual that includes a user’s guide. But the code is not so well documented.
BERRA is supported by many publications and a web-based documentation. Users
and programmers have at their disposal short guides.
Details of the evaluation together with problems that must be avoided, and
guidelines on how to choose among available commercial or research mobile robot
platforms in order to meet specific goals and requirements can be found in Ref.
[25]. A survey of nine open sources, freely available, robotic development environ-
ments (RDEs) is provided in Ref. [39]. This survey compares and evaluates these
RDEs by establishing and using a comprehensive list of evaluation criteria, which
includes the criteria presented in Section 14.3. First, a conceptual framework of
four broad categories is presented based on the characteristics and capabilities of
RDEs. The evaluation and comparison of these nine RDEs conclude with guide-
lines on how to use profitably its results. A comprehensive book dedicated to soft-
ware architecture with deep and illuminated design issues is Ref. [40].
14.6
Intelligent HumanRobot Interfaces
14.6.1
Structure of an Intelligent HumanRobot Interface
Interfaces play a key role for the successful and efficient operation of an intelligent
robot such that to fulfill its goals with the aid of multisensors and shared autonomy
[41]. Here, the basic design principles of intelligent humanrobot interfaces
(HRIs) will be outlined. An intelligent HRI has the general self-explained structure
shown in Figure 14.13.
The robotic system involves a supervisor, a planner, and a controller, and some-
times, if required, a decision support component which contributes to the realiza-
tion of cooperative humanrobot decision making and control.
The three main types of users are operators, engineers, and maintenance specia-
lists. These users interact with the robotic system via the HRI. Users have in gen-
eral different but overlapping needs with respect to depth and quantity.
14.6.2
Principal Functions of Robotic HRIs
The principal functions of HRIs are the following [42]:
G
Input handling
G
Perception and action
G
Dialogue handling
G
Tracking interaction
G
Explanation
G
Output generation
607
Generic Systemic and Software Architectures for Mobile Robot Intelligent Control

The input handling function provides the means to utilize the type of inputs
received by the system which may be analog, digital, probabilistic, linguistic, and
fuzzy.
The perception and action function is fundamental for the overall HRI perfor-
mance and is supported by the presentation level of the HRI which determines how
to present the information to the user and how to transform his/her control inputs.
Dialogue handling (control) undertakes the task of determining what informa-
tion to treat and when. A dialogue is any logically coherent sequence of actions
and reactions exchanged between the user and the HRI. Humanmachine dialogues
are necessary for many robotic operations (e.g., scheduling, supervision, planning,
control).
Tracking interaction is concerned with tracking the entire interaction between
the HRI and the human user, as well as between the HRI and the robotic system at
hand.
The explanation function needs a model of the robotic system to be available.
Its role is to explain to the user the meaning of the various aspects and components
of the robotic system, and sometimes of the HRI itself. It should be also capable of
explaining how the various parts of the system operate.
Output generation is realized using graphical editors and typically offers appro-
priate graphical and textual pictures which are dynamically changing. In more
User
• Operator
• Engineer
• Maintenance 
specialist
Robot
• Supervisor
• Planner
• Controller
System toolkit
•
Sensors
•
Effectors
•
Communications
Command 
interpreter
Modeler
User
goals 
Task
model
User
model
Robotic
system 
model
Output 
selection and 
formatting
Model
comparator 
Data
Commands
Information
Figure 14.13 General structure of an intelligent HRI.
608
Introduction to Mobile Robot Control

recent applications, multimedia presentations are also provided. If the HRI is
required to be able to adapt to different users or user classes, a user model is also
needed. To design a user model, it is necessary to use our knowledge on human
processing behavior and represent the cognitive strategies, via rules, algorithms,
and reasoning mechanisms. A more complete user model must also include a model
of the robotic system in order to incorporate the user’s view with respect to the
robotic system.
14.6.3
Natural Language HumanRobot Interfaces
A special very popular class of HRIs is the class of natural language interfaces
(NLIs). NLIs possess humanized properties since the user can communicate with
the robot through a kind of verbal language (e.g., a small subset of English).
Actually, NLIs are not the best interfaces in all cases. Thus, to decide whether to
use a NLI or not, one has to consider several factors, of which some examples are
the following:
G
Ease of learning: If a full natural language (NL) is used, no human effort is necessary to
learn it. This is not so if a restricted language with legal statements is used.
G
Conciseness: The desire for conciseness is usually in conflict with the user friendliness.
G
Precision: Many English sentences are ambiguous. It is so natural that English does not
use parentheses as artificial logical languages do.
G
Need for pictures: Words are not the best way to describe shapes, positions, and curves.
A picture is worth many words. However, programs that handle graphical objects (e.g.,
CAD systems) are still good candidates for NLIs and other linguistic interfaces.
G
Semantic complexity: NLs are concise and efficient when the universe of possible mes-
sages is large. Actually, no trivial language can perform the interfacing job, since the
number of different messages that have to be handled is extremely large.
G
Cost: The cost of NLIs used is higher than that of standard HRIs.
The components of an NL understanding system, that is, a system that trans-
forms statements from the language in which they were made in a program-specific
form that initiates appropriate actions, are as follows:
G
Words and lexicons
G
Grammar and sentence structure
G
Semantics and sentence interpretation
The above primary components can be merged into an integrated understanding
system in the following three ways:
1. Interactive selection: The system displays the options to the user who chooses among
them to gradually construct a complete statement, which corresponds to actions that the
target program can perform.
2. Semantic grammars: The window-based approach does not allow the user to control inter-
actions or compose free-form statements that the system has to understand. The alterna-
tive is for the user to compose entire statements. A semantic grammar provides one
implementation of this alternative approach but is appropriate when a relatively small
subset of an NL has to be recognized.
609
Generic Systemic and Software Architectures for Mobile Robot Intelligent Control

3. Syntactic grammars: If a large part of NL is used as HRI, the capture of as much of the
language regularity as possible is required. To this end, it is necessary to capture the syn-
tactic regularity of the NL at hand. Thus, one needs to use a syntactically motivated
grammar.
In the literature, several tools are described that assist in the building process of
the lexicon, the grammar, the semantic rules, and the code that uses all of them.
Also, some programs exist that do most of the understanding in all three
approaches discussed above.
NLIs in robotics have been considered and used by many researchers, for
example:
G
Nilsson [43], where the mobile robot Shakey, capable of understanding simple NL com-
mands, is presented.
G
Sato and Hirai [44], where NL instructions are employed for teleoperation control.
G
Torrance [45], where an NL interface is used to navigate an indoor mobile robot.
14.6.4
Graphical HumanRobot Interfaces
Graphical HRIs (GHRIs) represent a very large area of information technology.
GHRIs are used for task analysis, online monitoring, and direct control. For exam-
ple, to teleoperate a mobile robot in a critical workspace, a considerable effort
must be devoted to preparing the task, training the operator, and finding the optimal
cooperation modes in various situations. Before actually executing a task, a GHRI
can help the user to specify his intention, display the commands, and the expected
consequences on the monitor. In this way, the user can interactively generate and
modify a plan.
On a GHRI, an operator can define a series of movements and actions by click-
ing or dragging a mouse on the screen. The available task and geometric planners
can then find a sequence of motions and actions that implement the task. A simula-
tion system is usually designed and used to animate the robot’s motion on a 2D or
3D workspace, where several viewpoints can be set to monitor and observe the
robot’s behavior and its relation to the world. Possible collisions with obstacles,
robots, and other objects are avoided. Here, the optimal utilization of various sen-
sors is a fundamental prerequisite. As an additional aid, a task editor is used to sup-
port the task specification by interactively modifying a plan. It is useful if with this
task editor, the operator can also define a sequence of actions as a macro. The
macros can be retrieved and used to represent and implement an entire task plan.
A useful concept that can be used in task analysis is the concept of telesensor pro-
gramming (Hirzinger, [10]). Due to the unavoidable errors in the dead-reckoning
and world models, the sensor patterns have to be employed by the robot to ensure
an accurate relation with the world.
Graphical interfaces are very often combined with animation and virtual reality
(VR) tools. Examples of this type are the works of Heinzmann [46], Rossmann
[47], and Wang et.al [48].
610
Introduction to Mobile Robot Control

In Ref. [46], the HRI of the robot consists of a visual face tracking system. The
system employs a monocular camera and a hardware vision system to track several
facial features (eyes, eye brows, ears, mouth, etc.). The 3D pose and orientation of
the head are computed using this information. The solution to the design of human-
friendly robots, provided, satisfies two safety goals.
Safety goal 1: A human-friendly robot should be able to operate without posing
a threat when humans are inside the robot’s workspace.
Safety goal 2: In an unstructured environment which may involve humans, any
action autonomously taken by the robot must be safe even when the robot’s sensor
information about the environment is uncertain or false.
In Ref. [48] the humanmachine system includes a virtual tools system, an
automatic path planner, and a collision detection simulator. Tests on the perfor-
mance of the path planner are also discussed. A virtual tools HRI for point specifi-
cations of tasks, which interweaves virtual robot end/effector representations with
physical reality to immerse the human in the scene using simple hand gestures,
needs to be developed for flexibly designating where the robot should grasp as an
incoming part. The virtual tools system is displayed in four quadrants on a Silicon
Graphics workstation with Galileo video. The virtual gripper is displayed on the
two left quadrants display, superimposed on two camera views and blended with
live video, to create the illusion of a real gripper in two views in the physical scene.
The top right quadrant is occupied by the toolbox of graphic icons representing var-
ious tools available for use by the robot. The bottom right quadrant displays homo-
geneous transformation matrix information such as graphic object models and
views from the robot camera. This Pennsylvania University system which is based
on the virtual tool concept allows the operator to direct robot tasks in a natural way
in almost real-time.
In Ref. [47] a multirobot system (called CIROS) is designed that implements the
capability to derive robot operations from tasks performed in the virtual reality
environment. To this end, two appropriate components are used: the change-
detection component and the change-interpretation component. The VR system
employed is based on a special simulation system (COSIMIR, cell-oriented simula-
tion of industrial robots) developed at the Institute of Robotics Research in
Dortmund, Germany. In CIROS, a new VR concept is used. This is called projec-
tive virtual reality (PVR), because the actions carried out by humans in the VR are
projected on to robots to carry out the task in the physical environment. The intelli-
gent controller implements PVR-based control by adding the levels for online colli-
sion avoidance, multirobot coordination, and automatic action planning.
As an example, we describe briefly the NL HRI of the KAMRO intelligent
mobile robot, which was designed and built at the University of Karlsrue [49,50],
and uses a multiagent architecture.
A fundamental problem in such multiagent system (MAS) is the negotiation
among the agents that compete for a given task. This negotiation process can be
performed by a centralized mediator or a selected candidate or by many (or all)
candidates. All agents should be able to negotiate with the competing agents. One
611
Generic Systemic and Software Architectures for Mobile Robot Intelligent Control

way to manage the communication among agents is via a blackboard system.
Possible in a MAS are the following:
G
Deadlocks caused by agent bodies/external resources
G
Deadlocks caused by special agents
G
Deadlocks caused by agent teams
These
deadlock
situations
are
successfully
translated
into
corresponding
mechanisms in the KAMRO robot.
The NL HRI of KAMRO performs the following functions:
G
Task specification/representation, that is, analysis of instructions related to the implicit
robot operations (e.g., “pick-and-place,” bin picking)
G
Execution representation
G
Explanation of error recovery
G
Updating and describing the environment representation
The KAMRO NL HRI architecture is as shown in Figure 14.14.
The robot and the NL HRI have permanent access to the correct environment
representation via an overhead camera. This information is stored in a common
database. Since the world representation changes over time, a timestamp of the
snapshot is used which allows merging older and newer knowledge about the envi-
ronment. The processing of NL instructions is illustrated in Figure 14.15.
An edited book with outstanding contributions covering a wide repertory of con-
cepts, techniques, and applications of HRIs is Ref. [51].
14.7
Two Intelligent Mobile Robot Research Prototypes
Here, two working research prototypes of integrated mobile robots will be briefly
presented. These are the robotic wheelchair SENARIO, developed within the frame
Analysis
Evaluation
Generation
• Conceptual 
and syntactic 
knowledge
• User model
• Task representation
• Execution representation
• Error recovery
• Environment representation
Robot knowledge
(encapsulated)
Commands 
Queries
Descriptions 
Explanations 
Queries
Robot
Figure 14.14 NL HRI architecture of the KAMRO (Karlsruhe autonomous intelligent
mobile robot).
612
Introduction to Mobile Robot Control

of the European Union TIDE project SENARIO: sensor-aided intelligent naviga-
tion system for powered wheelchairs1 [52,53], and the ROMAN service MM,
designed and built at the Technical University of Munich [5456].
14.7.1
The SENARIO Intelligent Wheelchair
This intelligent mobile robot system was implemented and tested on a Meyra pow-
ered wheelchair as shown in Figure 14.16.
The architecture of the SENARIO mobile robot, which is a virtually centralized
hierarchical control architecture, is shown in Figure 14.17. Two functional alterna-
tives are possible in SENARIO:
1. Semi-autonomous
2. Fully autonomous
The following four processes determine the autonomous behavior actions of
SENARIO (Figure 14.17):
1. Task planning, which schedules the execution of all the other processes and is responsible
for overall system control. The planning, path following, and goal monitoring procedures
Analysis
Evaluation
•
•
•
Interpretation of 
structure
Analysis of spatial 
relation to identify 
objects to be located
Select most 
plausible 
interpretation
Take the spacer
between the shaft
and the lever
Pick
spacer 2
Robot
Figure 14.15 Structure of NL
instruction processing in the
KAMRO.
Control
computer
Navigation
sensors
Protection
sensor
Figure 14.16 The SENARIO
wheelchair intelligent mobile robot
system was integrated on a
commercial MEYRA platform. The
components of the system include a
computer, an orientation (encoder)
sensor, and an ultrasonic sensor
array (eight sensors for navigation
and three sensors for protection; two
in the front and one in the back).
1 SENARIO Consortium: Zenon S.A. (GR), National Technical University of Athens (GR), Microsonic
GmbH (DE), Reading University (UK), and Montpellier University (FR).
613
Generic Systemic and Software Architectures for Mobile Robot Intelligent Control

reside in the task planner. Task planning is on the top of the control hierarchy of the
robot.
2. Environment perception, where a set of sensors is used to truck the interactions of the
robot with its environment. Sensor data processing, environment features extraction, and
positioning, as well as user input, are grouped together as environment perception
processes.
3. Risk management, which is responsible for both risk detection and avoidance. This pro-
cess uses the output of environment perception to detect potential risks and determine
their importance. Then, it computes the necessary sequence of actions for risk avoidance.
4. Actuation, which handles the interface with the robot actuators.
This organization is an instance of a centralized intelligent control scheme
(Figure 14.1) because task planning assumes the responsibility for the coordination
of all the other processes in the system (organization level). However, some of the
processes can override task planning and can communicate with each other
directly, in cases of emergency. This option offers a distributed alternative to the
centralization organization. This hybrid solution is called virtually centralized con-
trol [24]. Virtually centralized control combines the reactivity of the distributed
approach, along with the high-level control features of the centralized one.
Semi-autonomous mode: The system receives commands to move on a direction,
or to take an action (e.g., go ahead, turn left, stop). The system realizes the
Task planner
Path following—Goal monitoring
Current task
Risk management
Risk detection
Risk classification
Emergency risk 
Avoidance
Obstacle avoidance
Risk list
Actuation
Motion command interpreter
Robot motion
Environment perception
Positioning—sensing—User interface
Robot position data
environment representation
Figure 14.17 The SENARIO control
architecture.
614
Introduction to Mobile Robot Control

instructed action, while preventing risk conditions and avoiding obstacles during
execution. Each time a risk is detected, SENARIO informs the user and takes
appropriate corrective measures to preserve safety and continues with the execution
of the instruction. In semi-autonomous mode, the user can override system actions,
for example, approach closer to a wall than the system’s alarm distance. In these
cases, the system applies minimum speed limit in all instructed commands. In any
case, if SENARIO detects an emergency situation, it stops moving and asks for
user instruction to recover. So, the responsibility of actions is shared between the
system and the user. This mode requires risk detection and risk avoidance
functionality.
Fully autonomous mode: This is a superset of semi-autonomous mode. The sys-
tem receives all the commands of semi-autonomous mode, along with “go to goal”
commands. For example, the user can issue commands such as “go to living
room.” In this case, the system locates itself and then the target position in the
environment map. It then plans and executes a path to the specified destination,
avoiding all obstacles and risks on the way. During goal execution, the user can
interfere with the system, as in semi-autonomous mode, or he/she can specify a
new destination at any point of the goals set. In this mode, the system takes full
responsibility for execution. Full autonomy requires path planning, along with risk
detection and risk avoidance functionality.
Each process of the system consists of a series of executive tasks. These are spe-
cialized procedures computing the parameters that characterize each process. In
particular, the task planner monitors the overall system functionality through the
path following and goal monitoring tasks. The task planner computes the current
task of the robot based on the risk management and environment perception out-
puts. Environment perception consists of the positioning, sensing, and user inter-
facing executive tasks. Similarly, risk management is split into risk detection, risk
classification, emergency risk avoidance, and obstacle avoidance tasks.
The positioning task is responsible for reporting the robot’s position, whenever
it is asked to report, while running individually in its loop. As a task belonging to
environment perception, it employs sensors and processes their information. The
output of this task is referred to as the position estimation of the robot or, equiva-
lently, as the robot position data.
Supplementary to the action of positioning is the sensing task. Both tasks
employ sensors, and occasionally, share the same environmental information. We
refer to the output of the sensing task as the environment representation.
SENARIO supports multiple environment representations ranging from simple
combinations of sensor data to occupancy grid representations for the environment.
Risk detection is responsible for the detection of both external risks threatening
the robot and originating from the environment and risks internal to the control
system, such as malfunctions. The detection and reaction methods to these risks are
different. The former, allows the subsequent use of risk classification, while the lat-
ter is implemented by low-level, robust reliable, and fast components, that do not
require additional processing and that directly react on the actuators through emer-
gency lines.
615
Generic Systemic and Software Architectures for Mobile Robot Intelligent Control

Risk classification employs a default set of criteria for classifying risks accord-
ing to their emergency. All the risks identified during the detection process are
classified according to the criteria used in the risk classification task. The outcome
of these tasks is a risk list sorted in decreasing order of emergency. This risk list is
further processed, either by task planning, or by risk avoidance (emergency risk
avoidance task), or directly by the actuation processes, thus supporting the virtually
centralized control scheme specified above.
Obstacle avoidance receives input from three processes: environment perception
(positioningposition estimation, and sensingenvironment representation), risk
detection (risk classificationrisk list) and task planning (current target position or
direction).
The task needs only one of the three input sources in order to maintain reliable
operation, namely, the environment representation. Any additional information is
affecting the robot route according to the needs of either task planning or risk
detection with varying priority. This scheme is another instance of virtually central-
ized control, due to the fact that either risk avoidance or task planning has absolute
control of the system, but there is a supervised distribution of control [24] among
the subsystems based on dynamic priority ordering.
Clearly, the interactions between risk detection and risk avoidance, and the dif-
ference between emergency risk avoidance and obstacle avoidance tasks should be
discriminated. Emergency risk avoidance is triggered by an emergency risk situa-
tion (i.e., a risk with a higher emergency classification in the risk list), while obsta-
cle avoidance covers the rest of the cases. Obstacle avoidance uses a local path
planning module based on the vector field histogram (VHF) method extended such
as to hold for nonpoint WMRs. This extension is known as active kinematic histo-
gram (AKH) [57].
Actuation realizes the commands of the rest of the supervised distributed control
scheme described above. It consists of a motion command interpreter task, which
receives commands by the risk management and task planner tasks in a common
format, and translates these instructions into motion commands for the actuators.
The output of the actuation task is identical to the output of the overall system and
is referred to as the robot motion.
Implementation: The configuration used and some of the results obtained are as
follows.
Sensing task: The sensing task is multimodal, that is, it uses a combination of
sensing principles (ultrasound, infrared light, etc.). The wheelchair is equipped
with proximity (ultrasonic) and positioning (encoders-infrared scanners) sensors.
A minimum number of sensors were used to achieve the required functionality in
order to keep the trade-off between functionality and cost in balance. Specifically,
there are 11 ultrasonic sensors in total, supplied by Microsonic GmbH-Dortmund.
The ultrasonic sensors are divided into two clusters: navigation and protection.
The difference in the two sensor’s clusters is that protection sensors supply
human protection functionality, working in fail-safe mode. Both protection and
navigation sensors cover a range of 250 cm, while the robot dimensions are 132 cm
in length and 82 cm in width. The ultrasonics sensors are mounted on the robot as
616
Introduction to Mobile Robot Control

shown in Figure 14.18. The letter “n” denotes the navigation sensors, while “p”
denotes the protection ones.
Manmachine interface: The user interface supports speech recognition using
an automatic speech recognition module. The module can record and interpret user
commands providing the appropriate control signals to the task planner. The
emphasis is on user-dependent speech recognition, to avoid accidental command
initiation due to other people talking nearby. During this phase, the speech recogni-
tion unit maps the acoustic signal (voiceprints) in its input to a set of commands
for the actuation into the range position in less than 1 min.
The system was tested in several environments with complexity below and
above average, involving points in adjacent rooms acting in real time, and showed
a success performance. All tests were performed with an average speed of 0.2 m/s.
Typical tasks performed successfully by the system are avoidance of furniture,
avoidance of persons seating or standing on its way, location of a door besides a
glass wall, passing through the door, and arriving at the desired goal destination.
14.7.2
The ROMAN Intelligent Service Mobile Manipulator
ROMAN involves the following subsystems (Figure 14.19):
G
Mobile platform: A three-wheel (diameter 0.2 m) omnidirectional platform (0.63 m
width 3 0.64 m depth 3 1.85 m height) equipped with a multisensoric system (a laser-
based angle measurement system and an eye-safe laser beam for scanning the workspace
in a horizontal plane and measuring the azimuth angle).
G
Robotic arm: An anthropomorphic manipulator (maximum range 0.8 m, maximum pay-
load 1.5 kg) for carrying out service tasks.
n1
n2
n3
p1
p2
n4
n5
n6
n7
n8
p3
0m
0.5m
1.0m
1.5m
2.0m
2.5m
3.0m
542 mm
Figure 14.18 Field of view of navigation and protection ultrasonic sensors (p1, p2 are the
front protection sensors).
617
Generic Systemic and Software Architectures for Mobile Robot Intelligent Control

G
Task planner and coordinator: The planner enables ROMAN to autonomously perform
typical tasks such as finding its way to the goal position, opening the doors, or handling
the desired objects.
G
Vision system: The camera is mounted on a tilt-unit to allow object recognition over the
entire workspace. Two object recognition techniques are used to handle a variety namely: a
feature-based method for large objects, and an appearance-based method for small objects.
G
Multimodal humanrobot interface (MRI): This is used for natural voice/speech based
dialogue between the human user and the robot.
The navigation of the platform is helped via a localization system that provides
platform position and orientation data in real time. An ultrasonic sensor array helps
to detect any obstacle(s) and works with the motion platform controller to avoid
collisions with the obstacles.
The 6-DOF robotic arm of ROMAN is suitable for manipulating/handling light-
weight geometrically simple objects (e.g., dishes, glasses, bottles, journals). The
control strategy for motion coordination of the robotic arm and the mobile platform
of ROMAN is shown in Figure 14.20.
The integrated processing and control architecture of ROMAN is depicted in
Figure 14.21. It is implemented on a VME bus-based multiprocessor system, com-
municating with the environment via an Ethernet link (10 Mbit/s).
In ROMAN, the information exchange between the operator and the robot is
performed in two stages: task specification and task execution (semi- or fully
Service robot
Workstation/MRI
Localization
system
Camera
Sonar
array
Voice
signal
Figure 14.19 The ROMAN
service mobile service robot (MRI
includes an NLI with commands
such as take the “cup away,” and
“bring the box”).
EE path
planer
End effector
controller
Local motion
generator
Locomotion
controller
Object
location
Obstacles
Criteria
i
d
q
i
q
Joint
angles
x,y,
z,φ
6 DOF
4 DOF
Figure 14.20 Coordinated control structure of ROMAN’s platform and arm.
618
Introduction to Mobile Robot Control

autonomous). The task specification requirements include task description. The
task execution requirements involve approaching the goal area, object specification,
object handling, and object handover. In addition, there are the monitoring and sen-
sor support requirements. The humanrobot dialogue involves the following:
G
Dialogue-oriented natural speech (voice) command input
G
Visual screen-based monitoring
G
Tactile supervisory control during mobile handling
G
Voice output during task operation
The NL HRI architecture of ROMAN is as shown in Figure 14.22.
The task commands consist of service task-specific actions, the support com-
mands are the operators’ responses to requests received from the motion planning
level during task execution, and the supervision commands are initiated by the
operator during task execution and immediately interrupt the current operation. The
command language is able to represent both the user-defined service tasks and ser-
vice robot-specific commands. The sensor information passed to the NL HRI from
the planning level involves off-line environmental data, continuous sensor data,
and abstract sensor data. Any problem arising during task execution initiates a
request for support, which needs to be interpreted by the human operator.
Human–robot
interface 
Task planner and
coordinator
Black
board
Long 
distance 
locomotion
Manipulation
Mobile 
manipulation
Object 
recognition
Localization 
and obstacle 
detection
Local user
Microphone
Loudspeaker
Radio 
Transmission
Intranet/internet
Monitor
Keyboard
Data
base
Figure 14.21 ROMAN’s hierarchical processing and control architecture.
619
Generic Systemic and Software Architectures for Mobile Robot Intelligent Control

The command generator translates semantic structures into robot commands.
The command generator of ROMAN receives the operator’s instructions and per-
forms the following functions:
G
Translation
G
Consistency check
G
Completeness check
G
Data expansion
G
Macro separation
Its output is the corresponding robot command. When receiving an NL com-
mand from the user, ROMAN’s interface converts the voice signal into an
executable robot command, and splits it up via the task planner into a sequence of
typical subtasks such as open door, pass door, or travel along corridor. These typi-
cal tasks are then executed by either the expert for long distance (e.g., room to
room) or by the mobile manipulation expert. The task planner also coordinates the
experts during task execution (Figure 14.21). For example, during the “open door”
task execution, the object recognition expert is connected with the expert for
mobile manipulation to locate the door handle.
Typical ROMAN’s tasks are the following:
G
Cleaning a table among several tables, by receiving the desired table specification by the
user (via NL or mouse clicking), decomposing the command into a sequence of subtasks,
planning a suitable path to the table, and starting its motion.
Input
Output
Command
generator
Knowledge
base
Motion planning
Motion execution
Multi
sensor
system
HRI
Service robot
Operator
Robot commands
Task commands
Support commands
Supervision commands
Teleoperation data
Support request 1
Sensor information 2 
Figure 14.22 Multimodal NL HRI architecture of the ROMAN.
620
Introduction to Mobile Robot Control

G
Door opening, via a standard opening maneuver of its manipulator, and then passing it. If
an obstacle is encountered on its way, ROMAN slows down and performs an obstacle
avoidance maneuver, if possible. Upon arriving at the desired table, ROMAN looks for
specified objects (e.g., a cup, a bottle), grasp it, and puts it (e.g., on the refrigerator).
G
Drawer opening by determining the drawer’s position (via the object recognition unit),
taking out a box, and carrying it at the hands of the user.
14.8
Discussion of Some Further Issues
Two further important issues for the development and application of software
architectures for intelligent mobile robot control (and other control systems) are the
following:
1. Design for heterogeneity
2. Modular design
14.8.1
Design for Heterogeneity
The hardware heterogeneity that exists between the various components in com-
mercial and research mobile robots is one of the most difficult problems to face.
The heterogeneity is due to the variety of multiple processing units, communication
components, central control computers and workstations, sensor and actuator
hardware, and so on. In addition, on every processing node, off-line and online
(real-time) operations coexist. To face this extended heterogeneity, the concept of
middleware was developed [1]. Very broadly, middleware is a software layer that
defines unified (standardized) interfaces and communication services according to
the capabilities of each individual robot. In the mobile robotics field, the middle-
ware must provide interfaces to the various types of actuators and sensors and
encapsulate them such that advanced software can be easily ported from one robot
(hardware) to another.
Commercially, several middleware platforms are available such as Miro [1],
Marie [3], Player [4], ORCA 1/2 [58], and MCA [59].
A general approach for the development of middleware that faces high-degree
heterogeneity involves the following three architectural abstractions [6062]:
1. Architecture design abstractions, which enable the development of adaptive, reusable,
and hierarchical subsystems and components
2. Architecture modeling and analysis, which permits early, integrated, and continuous eval-
uation of system behaviors
3. Middleware architecture, which permits self-adaptation in highly dynamic, changing, and
heterogeneous environments
A general diagram that illustrates the middlewear layers between hardware and
user is shown in Figure 14.23.
Design abstractions concern the representation and reasoning about complex
systems at a high level. To this end, several canonical architectural constructs were
621
Generic Systemic and Software Architectures for Mobile Robot Intelligent Control

developed, namely, components, connectors, interfaces, services, communication
ports, and configuration. The use of these constructs which are described by heuris-
tics or constraints can be made in two styles: client-to-server and peer-to-peer.
Traditional software architectures are layered, where components at a given layer
need the services of components at the layer below. In an adaptive-layered style,
components at a given layer monitor, manage, and adapt components of the lower
layer. For a discussion of an adaptive-layered architecture, called PLASMA (plan-
based layered architecture for software systems) the reader is referred to Ref. [60].
PLASMA possesses three bottom-up adaptive layers:
1. Application layer (components at this layer reside at the bottom layer)
2. Middle layer (adaptation layer), which monitors, manages, and adapts components of the
bottom layer
3. Top layer (planning layer), which manages the adaptation layer and the generation of
plans or user-supplied goals and component specifications
This architecture is an instance of the generic hierarchical architecture shown in
Figure 14.1. Clearly, if a nonadaptive system is designed, only the application layer
is needed.
Modeling and analysis of software for robotics is concerned with architectural
modes and analyses to guide and direct design decisions about dynamic planning
and adaptation. A software language that can be used for modeling and analysis is
SADEL (Software Architecture Description and Evaluation Language) [60,63]. In
SADEL, a model specifies the functional interfaces and application components,
and another model deals with the management interfaces of components (deploy,
connect, suspend, etc.). With SADEL, the implementation of tools that allow one
to carry experiments with various system design decisions regarding nonfunctional
features, policies for initiating replanning, and alternatives for reusing software
components is feasible.
Middleware architectures available for robotics are not always effective, especially
in cases where the systems are distributed across multiple, heterogeneous platforms.
User
Application
Middleware
Operating
system
Hardware
Figure 14.23 Structure of middlewear layers.
622
Introduction to Mobile Robot Control

A modified middleware solution that alleviates these shortcomings and can be
used effectively in many mobile robotic platforms is RoboPrism [64]. This is
achieved by providing the required low-level abstractions for interfacing with the
operating system at hand, implementing software systems through the use of con-
structs (component, connector, etc.), offering a wide collection of metalevel ser-
vices, and enabling the management and adaptation of the metalevel services to
obtain in overall an adaptive-layered system. All the above can be achieved with
low total cost (memory, CPU, network).
Another
middleware
solution
with
many
important
features
is
Miro
Architecture[1]. Miro is a CORBA-based framework for programming robots,
developed at the University of Ulm (Germany). Common object request broker
architecture (CORBA) is an open vendor-independent architecture and infrastruc-
ture produced and offered by OMG [65]. Through the standard HOP protocol, a
CORBA-based program from any vendor (on almost any computer, operating sys-
tem, programming language, and network) can interoperate with any other
CORBA-based program. CORBA is a middleware suitable for servers that have to
handle reliably large number of users at high bit rates. CORBA takes successful
care of issues such as load balancing, resource control, and fault tolerance on the
server side. CORBA 2 and CORBA 3 represent complete releases of the entire
CORBA specification [66].
The Miro Architecture (two representations) is shown in Figure 14.24. Miro
manual is available in Ref. [68].
Miro provides an object-oriented middleware for robots which besides CORBA
employs standard and widely used packages such as ACE, TAO, and Qt. It facili-
tates and improves the development process and the integration of system informa-
tion processing frameworks.
Miro satisfies the following goals of robotics middleware:
G
Object-oriented design
G
Open architecture style
G
Hardware and operating system abstraction
G
Proper communication support and interoperability
G
Client-server style
G
Software design patterns that offer a high-quality framework for common well-
understood functionalities
In general, the integration of new hardware devices on a given middleware falls
in the following categories:
1. The hardware is already fully supported by the middleware.
2. The middleware already provides support for hardware services with equal functionality.
3. The middleware supports similar devices.
4. The middleware does not support the hardware at hand.
In case 1, the service offered can be used with no or moderate additional effort.
In case 2, one may reuse the existing interfaces and so he/she has to implement
only the hardware-specific parts. In case 3, the system designer must derive his/her
623
Generic Systemic and Software Architectures for Mobile Robot Intelligent Control

(B)
Behavior
mechanism  
Own robot
control program
Visualization
client
Motion
Odometry
Range
sensors
Robot-specific
base service
Laser scanner
service
CORBA (ACE/TAO) 
Naming service
Use via 
CORBA
CORBA
Uses
service
Provides
service
Application
Application
Application
Miro 
Frameworks
Miro
Sensor/actuator services
Miro 
Communication/configuration
Miro
Device layer
Device layer
Linux OS
Linux OS
Windows/solaris OS
(A)
Pioneer
PC/workstation
Sparrow
Figure 14.24 Two representations of the Miro architecture: (A) general abstraction layers
and (B) the role of CORBA in the architecture.
Source: Adapted from Refs. [1,67].
624
Introduction to Mobile Robot Control

own interfaces and add the missing functionality. Finally, in case 4, the implemen-
tation of new services has to be made almost from the beginning. A successful
example of porting new robots to the Miro middleware is reported in Ref. [69]. A
discussion on reusing software for robotic applications employing analysis patterns
is provided in Ref. [27].
14.8.2
Modular Design
The modular design of software can be based on the following:
G
Software design
G
Software architecture
The software design is concerned with the decomposition of functionality into
layers with increasing degree of abstraction. For complex environments pure reac-
tive control is likely not to be successful. Therefore, high-level AI functions and
reactive behavior must be suitably merged.
The software architecture deals with the implementation details. It is based on
available middleware such as the middleware for cooperative robotics (MIRC) [1].
The hardware may be split into the following modules (layers) [70]:
G
Driving module, which is responsible for handling the motion of the robot
G
Actuator module, which performs all active interactions with the environment
G
Sensor module, which is responsible for the entire sensing of the environment
G
Control module, which performs the more complex information processing for the robot
control
A modular software architecture that achieves the goals of:
G
Flexibility
G
Maintainability
G
Testability
G
Modifiability
is
reported
in
Ref.
[71].
This
architecture
is
based
on
an
asynchronous
publishsubscribe mechanism and a blackboard object that handles synchronized
access to shared data. The publishsubscribe mechanism with a blackboard decouples
the sender and receiver and reduces modules’ dependencies to a very large extent.
Publisher
1
Publisher
2
Message
channel 1
Message
channel 2
Message
broker
Message
channel 1
Message
channel 2
Message
channel 3
Subscriber
1
Subscriber
2
Subscriber
3
MC1
MC2
MC3
MC1
MC2
Figure 14.25 Structure of the publishsubscribe messaging pattern.
625
Generic Systemic and Software Architectures for Mobile Robot Intelligent Control

The
publishsubscribe
messaging
pattern
has
the
structure
shown
in
Figure 14.25, and involves three main components: the publisher, the broker, and
the subscriber [71].
The publisher generates and publishes signals (messages) which are used by the
subscriber. The broker is a signal router that monitors every module’s output chan-
nel signal and according to the signal type passes it to the input channel of each
subscriber. Publishers and subscribers are actually decoupled via the message chan-
nels that are configured during the system’s initialization.
To achieve the desired (high) flexibility, implementability, and testability, the
perception, planning, localization, and control tasks are decomposed into a set of
simple modules. For example, localization using a GPS sensor can be split into the
following two modules:
1. GPS reader module, which receives and processes messages from the GPS sensor
2. Localization module, which filters raw sensor data and updates the robot’s state
In this way, there is no need for the localization module to know how to connect
and get data from the GPS sensor (i.e., changing the sensor or communication pro-
tocol does not influence localization). A typical module has the structure shown in
Figure 14.26 [71].
An overall generic software architecture that uses the above concepts is shown
in Figure 14.27 [71].
The high-level modules A, B, C perform task-specific perception, planning, and
control. The low-level modules perform the execution of the commands issued by
the high-level modules, accept and process sensor data sending the processed data
to high-level modules, and send the proper commands to the robot’s actuators. The
data synchronization with each other is achieved through the publishsubscribe
pattern and the shared blackboard.
In Ref. [71], the flexibility, extensibility, and testability of the above architecture
was tested by constructing and applying a controller for a real automobile followed
<thread>
Controller
<thread>
Routine
Control 
input  
channel
Device
BB
Input 
channel
Output 
channel
Blackboard
Module
Send/receive
Update/query
Initiate/terminate
Read
Figure 14.26 The standard module
structure (which involves a
controller thread and a routine
thread).
626
Introduction to Mobile Robot Control

by a virtual reality model of the car. To this end, the design was based on five prin-
cipal high-level modules, namely:
1. Localization module (based on a GPS sensor and an electronic compass). An extended
Kalman filter was used for sensor fusion and state estimation.
2. Obstacle detection module (using a front-facing monocular camera).
3. Traffic recognition module (using a second monocular camera for the detection of traffic
signs via color and shape information).
4. Planning module (which updates the status of wave-points that have been reached and
decides what movement to do next).
5. Control module (with steering input set-point found using a fuzzy logic control algorithm).
The simulation experiments were based on the bicycle model of the car and a cam-
era simulator based on the Open GL library for rendering 3D scenes. The car module
is an active blackboard storing the current position, orientation, and speed of the car-
like robot, while the simulator is running with 1 ms updating period. The direct com-
munication and the publishsubscribe schemes were applied and compared. The
dependency diagrams of the above two schemes are shown in Figure 14.28 [71].
The overhead of the publishsubscribe scheme for message payload from 50 up
to 1000 bytes was found to be 100 µs. Although it is much higher than the 10-µs
overhead of the direct communication scheme, it was proved to be quite acceptable.
This is so because the sensor, actuator, and control loop period is much slower.
CVCV
CVCV
BB
Module
A
Module
B
Module
C
Message
broker
Sensor modules
Actuator
modules
Sensors
Actuators
High-level modules
Input
channel 
Input
channel
Output
channel
Input
channels
Output
channels
Output
channels
Input
channels
Low-level modules
Figure 14.27 A generic
architecture for a mobile robotic
system based on
publishsubscribe paradigm with
a message broker.
627
Generic Systemic and Software Architectures for Mobile Robot Intelligent Control

Two other works devoted to software/middleware architectures using the black-
board concept are described in Refs. [72,73]. A comprehensive review of the mid-
dleware work on networked robots, up to 2009, is provided in Ref. [74], and an
annotated literature survey with rich comments up to 2012 is given in Ref. [67].
A global picture of the features of 15 robotics middleware frameworks, as sum-
marized in Ref. [67], can be obtained by the following list:
G
OpenRTMaist, a modular software structure platform that simplifies the process of build-
ing robots by simply combining selected modules
G
ASEBA, which allows distributed control and efficient resources utilization of robots with
multiprocessors
G
MARIE, which creates flexible distributed components that allow sharing, reusing, and
integrating new or existing software programs for rapid robotic application development
G
RSCA, a real-time support for robotic applications with abstractions that make them both
portable and reusable on different hardware platforms
G
MRDS, a robotic software platform that supports a wide variety of hardware devices and
a set of useful tools facilitating programming and debugging
Map
Localization
Planning
Traffic sign
recognition
Obstacle
detection
Control
GUI
Compass
reader
Image
grabber
GPS
reader 
Message
broker
Steering
actuator
Speed
actuator
(A)
Localization
Planning
Traffic sign
recognition
Obstacle
detection
Control
GUI
Compass
reader
Image
grabber
GPS
reader
Steering
actuator
Speed
actuator
(B)
Figure 14.28 (A) Dependency diagram of the publishsubscribe scheme. (B) Dependency
diagram in case of direct communication.
628
Introduction to Mobile Robot Control

G
OPROS, a component-based generic software platform that enables complicated functions
to be developed easily by using the standardized components in the heterogeneous com-
munication network
G
CLARAty, a reusable robotic framework that enables integration, maturation, and demonstra-
tion of advanced robotic technologies, from multiple institutions on NASA’s rover platforms
G
ROS, which provides OS services such as hardware abstraction, low-level device control,
message-passing between processes, and package management
G
OROCOS, a general-purpose modular framework for robot and machine control
G
PYRO, a programming environment for easily exploring advanced topics in artificial
intelligence and robotics without having to worry about the low-level details of the under-
lying hardware
G
PLAYER, a development framework that supports different hardware devices and com-
mon services needed by different robotic applications
G
ORCA, a framework that enables software reuse in robotics using component-based
development
G
ERSP, which provides cutting edge technologies for vision, navigation, and system
development
G
WEBOTS, a rapid prototyping environment for modeling, programming, and simulating
mobile robots
G
ROBOFRAME, a system framework that covers the special needs of autonomous light-
weight robots such as dynamical locomotion and stability
Obviously, the selection of a commercial software/middleware platform for a
given robotic system or application is a complex problem and needs deep and care-
ful considerations.
References
[1] Utz H, Sablantno¨g S, Euderde E, Kraetzschmar G. Miro-middleware for mobile robot
applications. IEEE Trans Robot Autom 2002;18(4):4937.
[2] Nesnas A, Wright A, Bajiracharya M, Simmons R, Estlin T. CLARAty and challenges
of developing interoperable robotic software. Proceedings IEEE/RSJ international con-
ference on intelligent robots and systems (IROS’2003), vol. 3. Las Vegas, NV; 2003. p.
242835.
[3] Cote C, Brosseau Y, Letourneau D, Raivesty C, Michand F. Robotic software integra-
tion using MARIE. Int J Adv Robot Syst 2006;3(1):5560.
[4] Gerkey B, Vaugham R, Howard A. The player/stage project: tools for multi-robot and
distributed sensor systems. Proceeding of eleventh international conference on advanced
robotics (ICAR’2003). Coimbra, Portugal; 2003. p. 317323.
[5] Montemerlo M, Roy N, Thrun S. Perspectives on standardization in mobile robot pro-
gramming: The Carnegie Mellon Navigation (CARMEN) toolkit. Proceedings of IEEE/
RSJ international conference on intelligent robotics and systems. Las Vegas, NV; 2003.
p. 24362441.
[6] Fu K-S. Learning control systems and intelligent control systems: an intersection of artifi-
cial intelligence and automatic control. IEEE Trans Autom Control 1971;AC-16(1):702.
[7] Saridis GN. Toward the realization of intelligent controls. Proc IEEE 1979;67
(8):111533.
629
Generic Systemic and Software Architectures for Mobile Robot Intelligent Control

[8] Saridis GN. Foundations of intelligent controls. Proceedings of IEEE workshop on
intelligent control. Troy, NY; 1985. p. 238.
[9] Antsaklis P, Passino KM, editors. An introduction to intelligent and autonomous sys-
tems. Berlin: Kluwer/Springer; 1993.
[10] Coste-Manie´re E, Simmons R. Architecture, the backbone of robotic systems. Proceedings
of IEEE international conference on robotics and automation (ICRA’2000). San Francisco,
CA; 2000. p. 6772.
[11] Xie W, Ma J, Yang M, Zhang Q. Research on classification of intelligent robotic archi-
tecture. J Comput 2012;7(2):4507.
[12] Saridis GN. Analytical formulation of the principle of increasing precision and decreas-
ing intelligence for intelligent machines. Automatica 1989;25(3):4617.
[13] Meystel AM. Architectures of intelligent control. The science of autonomous intelli-
gence. Proceedings of IEEE international symposium on intelligent control. Chicago,
IL; 1993. p. 428.
[14] Meystel AM. Autonomous mobile robots: vehicles with cognitive control. Singapore:
World Scientific Singapore; 1991.
[15] Meystel AM. Multiresolutional hierarchical decision support systems. IEEE Trans Syst
Man Cybern-Part C: Appl Rev 2003;SMR-AR 33:86101.
[16] Albus JS. System description and design architecture for multiple autonomous under-
sea vehicles. NIST Tech. Note 1251, Washington, D.C.; September 1988.
[17] Albus JS, Quintero R. Towards a reference model architecture for real-time intelligent
control systems. Robotics and manufacturing, vol. 3. New York, NY: ASME; 1990.
[18] Albus JM. Outline for a theory of intelligence. IEEE Trans Syst Man Cybern 1991;
SMC-21(3):473509.
[19] Ayari I, Chatti A. Reactive control using behavior modeling of a mobile robot. Int J
Comput, Commun Control 2007;2(3):21728.
[20] Brooks RA. A robust layered control system for a mobile robot. IEEE J Robot Automn
1986;RA-2:1423.
[21] Brooks RA. Intelligence without reason. AI Memo. No. 1293, AI Laboratory, MIT;
1991.
[22] Arkin RC. Motor schema-based mobile robot navigation. Int J Robot Res 1989;8
(4):92112.
[23] Arkin RC. Cooperation without communication: multi-agent schema based robot navi-
gation. J Robot Syst 1992;9(2):35164.
[24] Arkin RC. Behavior-based robotics. Cambridge, MA: The MIT Press; 1998.
[25] Oreback A, Christensen HL. Evaluation of architectures for mobile robots. Auton
Robots 2003;14:3349.
[26] Riehle D, Zullighoven H. Understanding and using patterns in software development.
Theory Pract Object Syst 1996;2(1):313.
[27] Jawawi D, Deris S, Mamat R. Software reuse for mobile robot applications through
analysis patterns. Int Arab J Inf Technol 2007;4(3):2208.
[28] Canas JM, Matellan V. Integrating behaviors for mobile robots: an ethological
approach. Cutting edge robotics. Pro Literature Verlag/ARS; 2005. p. 31150.
[29] Canas JM, Ruiz-Ayucar J, Aguero C, Martin F. Jde-neoc: component oriented software
architecture for robotics. J Phys Agents 2007;1(1):16.
[30] Kerry M. Simplifying robot software design layer by layer. National Instruments RTC
Magazine. , http://rtcmagazine.com/articles/view/102283.; 2013 [20 AUGUST].
[31] Travis J, Kring J. LabVIEW for everyone: graphical programming made easy and fun.
Upper Saddle River; NJ: Prentice-Hall; 2006.
630
Introduction to Mobile Robot Control

[32] Konolige K., Myers K. The Saphira architecture for autonomous mobile robots. SRI
International. ,http://www.wv.inf.tu-dresden.de/Teaching/MobileRoboticsLab/Download/
Saphira-5.3-Manual.pdf,
http://www.cs.jhu.edu/Bhager/Public/ICRAtutorial/Konolige-
Saphira/saphira.pdf.; 2013 [20 AUGUST].
[33] Lindstrom M, Oreback A, Christensen H. BERRA: a research architecture for service
robots. Proceedings of IEEE international conference robotics and automation
(ICRA’2000). San Francisco, CA; April 2428, 2000. p. 327883.
[34] Saffioti A, Ruspini E, Konolige K. Blending reactivity and goal-directness in a fuzzy
controller. Proceedings of 2nd IEEE international conference on fuzzy systems. San
Francisco, CA; 1993. p. 1349.
[35] Guzzoni D, Cheyer A, Julia A, Konolige K. Many robots make short work. AI Mag
1997;18(1):5564.
[36] Schmidt DC. The ADAPTIVE communication environment: object-oriented network
programming components for developing client/server applications. Proceedings of
eleventh and twelveth Sun Users Group conference. San Jose, CA; June 1417,
December 79, 1993.
[37] Fink GA. Developing HMM-based recognizers with ESMERALDA. Lecture notes in
artificial intelligence, vol. 1692. Berlin: Springer; 1999. p. 22934
[38] Veldhuizen TL. Arrays in Blitz11. Proceedings of second international scientific
computing in object-oriented parallel environments: ISCOPE’ 98. Santa Fe. NM,
Berlin: Springer; 1998.
[39] Kramer J, Scheutz M. Development environments for autonomous mobile robots: a
survey. Auton Robots 2007;22(2):10132.
[40] Qian K, Fu X, Tao L, Xu C-W. Software architecture and design illuminated.
Burlington, MA: Jones and Bartlett Publishers; 2009.
[41] Hirzinger G. Multisensory shared autonomy and telesensor programming: key issues in
space robotics. Robot Auton Syst 1993;11:14162.
[42] Tzafestas SG, Tzafestas ES. Humanmachine interaction in intelligent robotic sys-
tems: a unifying consideration with implementation examples. J Intell Robot Syst
2001;32(2):11941.
[43] Nilsson NJ. Shakey the robot. Technical Note No. 323, Al Center. Menlo Park, CA:
SRI International; 1984.
[44] Sato T, Hirai S. Language-aided robotic teleoperation system (LARTS) for advanced
teleoperation. IEEE J Robot Autom 1987;3(5):47680.
[45] Torrance MC. Natural communication with robots. MScthesis, DEEC. MA: MIT Press;
1994.
[46] Heinzmann J. A safe control paradigm for humanrobot interaction. J Intell Robot
Syst 1999;25:295310.
[47] Rossmann J. Virtual reality as a control and supervision tool for autonomous systems.
In: Remboldt U, editor. Intell Auton Syst. Amsterdam: IOS Press; 1995. p. 34451.
[48] Wang C, Ma H, Cannon DJ. Humanmachine collaboration in robotics: integrating
virtual tools with a collision avoidance concept using conglomerates of spheres. J
Intell Robot Syst 1997;18:36797.
[49] Laengle T, Remboldt U. Distributed control architecture for intelligent systems.
Proceedings
of
international
symposium
on
intelligent
systems
and
advanced
manufacturing. Boston, MA; November 1822, 1996. p. 5261.
[50] Laengle T, Lueth TC, Remboldt U, Woern H. A distributed control architecture for
autonomous mobile robots—implementation of the Karlsruhe multi-agent robot archi-
tecture. Adv Robot 1998;12(4):41131.
631
Generic Systemic and Software Architectures for Mobile Robot Intelligent Control

[51] Sarkar M. Human robot interaction. In-Tech, e-Books; 2008.
[52] Katevas NI, Sgouros NM, Tzafestas SG, Papakonstantinou G, Beatie G, Bishop G, et
al. The autonomous mobile robot SENARIO: A sensor-aided intelligent navigation sys-
tem for powered wheelchairs. IEEE Robot Autom Mag 1997;4(4):6070.
[53] Katevas NI, Tzafestas SG, Koutsouris DG, Pnevmatikatos CG. The SENARIO
autonomous navigation system. In: Tzafestas SG, editors. Mobile robotics technology
for health care services. Proceedings of first MobiNet symposium. Athens; 1997.
p. 8799.
[54] Ettelt E, Furtwangler R, Hanbeck UD, Schmidt G. Design issues of a semi-autonomous
robotic assistant for the health care environment. J Intell Robot Syst 1998;22
(34):191209.
[55] Fisher C, Buss M, Schmidt G. Hierarchical supervisory control of service robot using
human-robot interface. Proceedings of international conference on intelligent robots
and systems (IROS’96). Osaka, Japan; 1996. p. 140816.
[56] Fischer C, Schmidt G. Multi-modal human-robot interface for interaction with a
remotely operating mobile service robot. Adv Robot 1998;12(4):397409.
[57] Katevas NI, Tzafestas SG. The active kinematic histogram method for path planning of
non-point non-holonomically constrained robots. Adv Robot 1998;12(4):37595.
[58] Brooks A. Toward component-based robotics. Proceedings of IEEE/RSJ international con-
ference on intelligent robots and systems (IROS 2005). Edmonton, Alberta, Canada; August
26, 2005. p. 1638. ,http://orca-robotics.sourceforge.net.; 2013 [20 AUGUST].
[59] Scholl KU. MCA2-modular controller architecture. , http://mac2.sourceforge.net . ;
2013 [20 AUGUST].
[60] Brun Y, Edwards G. Engineering heterogeneous robotic systems. Computer 2011;
May:6170.
[61] Taylor RN, Medvidovic N, Dashofy EM. Software architecture: foundations, theory
and practice. New York, NY: John Wiley & Sons; 2009.
[62] Edwards G, Garcia J, Tajalli H, Popescu D, Medvidovic N, Sukhatme G, et al.
Architecture-driven
self-adaption
and
self-management
in
robotics
systems.
Proceedings of international workshop on software engineering for adaptive and self-
managing systems (SEAMS’09). Los Angeles, CA: IEEE Computer Society Press;
March 2009. p. 14251.
[63] Medvidovic N. A language and environment for architecture based software develop-
ment and evolution. Proceedings of twentyfirst international conference on software
engineering (ICSE’99). IEEE Computer Science Press; 1999. p. 4453.
[64] Available from: http://sunset.usc.edu/Bsoftarch/Prism; 2013 [20 AUGUST].
[65] Available from: http://omg.org; 2013 [20 AUGUST].
[66] Available from: http://omg.org/getingstarted/corba.faq.htm.
[67] Elkady A, Sobh T. Robotics middleware: a comprehensive literature survey and
attributed-based bibliography. J Robot 2012 [Open Access].
[68] Miro-middleware
for
robots.
, http://orcarobotics.sourceforge.net.;
2013
[20
AUGUST].
[69] Kruger D, Van Lil I, Sunderhauf N, Baumgartl, Protzel P. Using and extending the
Miro middleware for autonomous mobile robots. Proceedings of international confer-
ence on towards autonomous robotic Systems (TAROS 06). Survey, UK; September
46, 2006. p. 905.
[70] Steinbauer G, Fraser G, Muhlenfeld A, Wotawa A. A modular architecture for a multi-
purpose mobile robot. Proceedings of seventeenth conference on industrial and
632
Introduction to Mobile Robot Control

engineering applications of AI and ES (IEA/AIE): innovations of artificial intelligence.
Ottawa, Canada; 2004. p. 100715.
[71] Limsoonthrakul S, Dailey ML, Sirsupundit M. A modular system architecture for
autonomous
robots
based
on
blackboard
and
publish-subscribe
mechanisms.
Proceedings of IEEE international conference on robotics and biomimetics (ROBIO
2009). Bangkok, February, 2225, 2009. p. 63338.
[72] Schneider S, Ullman M, Chen V. Controlshell: a real-time software framework.
Proceedings of IEEE international conference on systems engineering. Fairborn, OH;
1991. p. 12934.
[73] Shafer S, Stentz A, Thorpe C. An architecture for sensor fusion in a mobile robot.
Proceedings of IEEE international conference on robotics and automation. San
Francisco, CA; April 1986. p. 200211.
[74] Mohamed N, Al-Jaroodi J, Jawhar I. A review of middleware for networked robots. Int
J Comput Sci Netw Secur 2009;9(5):13943.
633
Generic Systemic and Software Architectures for Mobile Robot Intelligent Control

15 Mobile Robots at Work
15.1
Introduction
Robots of all types constitute one of the keys to human social and economic devel-
opment. The first robots were devices with one or more arms making human-like
motions. In our days, the shape of robots covers a very large repertory of forms
including wheeled platforms, mobile manipulators, legged bodies, animal-like
robots, and so on. All these robots contribute in one or the other way to human,
industrial, agricultural, technical, and social life improvements. The benefits of
using autonomous and intelligent mobile robots (wheeled or legged) in medical,
assistive, and service applications are numerous, and their positive impact in mod-
ern society is continuously increasing.
The purpose of the present chapter is purely encyclopedic and aims to provide
the reader a small set of mobile robots of various types that are actually used in
modern industry and society [125]. Specifically, the following robot categories
are discussed with relevant photos included:
G
mobile robots and manipulators in the factory and industry,
G
mobile robots in the society (rescue, guidance, hospital),
G
mobile robots for home services (cleaning, other services),
G
assistive mobile robots (autonomous wheelchairs, service mobile manipulators for the
impaired),
G
mobile telerobots and web robots,
G
other mobile robot applications.
The above applications illustrate the importance and value of mobile robots for
achieving a better quality of human life in a wide variety of directions.
15.2
Mobile Robots in the Factory and Industry
Mobile robots used on the factory shop floor are typically called automated or
autonomous guided vehicles (AGVs) and move following markers or wires on the
floor or employ sensory systems (lasers, vision). AGVs are used in factory for
material handling, product transfer from one place to another, for inspection and
quality control, etc. They work around the clock contributing to better continuous
Introduction to Mobile Robot Control. DOI: http://dx.doi.org/10.1016/B978-0-12-417049-0.00015-8
© 2014 Elsevier Inc. All rights reserved.

floor and just-in-time delivery. An AGV can tow objects behind them in trailers to
which they can autonomously attach. The first AGV was commercially available in
the 1950s by Barrett Electronics (Northbrook, IL), actually being a tow truck fol-
lowing a wire in the floor instead of a rail. Today, AGVs are primarily laser navi-
gated, and can be programmed to communicate with other robots to assure smooth
product movement and storage in the industrial premises. All modern flexible
manufacturing systems use AGVs for achieving their goals and assure fast and
high-quality production 24 h a day. The navigation of AGVs can be performed in
the following ways (Wikipedia article: Automated-Guided -Vehicles#Wired).
G
Wired navigation—A wired sensor placed on the bottom of the platform (facing the
ground) detects the radio frequency which is transmitted from a wire placed in a slot
about 1 in. below the ground. The AGV is then moved following the wire.
G
Guide tape-based navigation—This navigation type is suitable for automated guided carts
(i.e., light-duty AGVs). A tape is used to guide the vehicle, which is equipped with a
suitable guide sensor enabling it to follow the path of the tape.
G
Laser-based navigation—This is a wireless navigation process which is performed by
mounting retroreflective tape on walls, poles, and machines. The AGV has a rotating tur-
ret carrying a laser transmitter and receiver. The laser is sent off and then received again
allowing the computation of the vehicle’s orientation (and in some cases the distance)
automatically. The AGV is equipped with a reflector map stored in memory, and can cor-
rect its position by a feedback control technique (using the error between the desired and
received measurement).
G
Gyroscope-based navigation—This is an inertial guidance system. Transporters are
embedded in the floor of the work environment. The AGV employs these transporters to
detect the correctness of its route, and a gyroscope measures the change in the direction
of the vehicle. This change (error) is used to correct the vehicle’s motion and return it on
the path.
G
Natural features-based navigation—This allows the AGV navigation with no workspace
retrofitting. Typically, one or more range finder sensors (laser, gyroscope, etc) are used,
and the extended Kalman filter or particle/Monte Carlo filters are used for localization
and mapping. The most common steering type in factory AGVs is the differential drive.
When there are several AGVs working on the shop floor, some kind of traffic control is
required (forward sensing control, zone control, etc.).
The applications of factory and industrial AGVs include the following:
G
Trailer loading—Pick up of pallets from conveyors, or staging lanes and deliver into a
trailer according to a loading pattern.
G
Raw material handling—Transportation of raw materials (metal, plastic, rubber, paper,
etc.) received in the warehouse, and delivery to the proper production line.
G
Finished product handling—Transportation of finished goods from the production line to
storage or shipping to customers.
The above applications of AGVs are needed in most modern industries—auto-
motive industry, food and beverage industry, paper-and-print industry, chemical/
pharmaceutical industry, manufacturing industry, etc.
Figures 15.115.5 show some AGVs used in factories and industries, including
autonomous mobile manipulators.
636
Introduction to Mobile Robot Control

Figure 15.1 Two Hitachi autonomously guided vehicles for the factory.
Source: http://www.hitachi-pt.com/agv/intelligentcarry/index.html.
Figure 15.2: Two more industrial AGVs. (A) Adept handling AGV, (B) DTA Vehicles
AGV pallet truck.
Source: http://www.directindustry.com/industrial-manufacturer/agv-80196.html/.
Figure 15.3 (A) Eagle series E200 cart transporter and (B) Falcon series F150 heavy-duty
conveyor.
Source: http://www.coreconagvs.com/products.
637
Mobile Robots at Work

Figure 15.5 An industrial autonomous mobile manipulator.
Source: http://blog.robotiq.com/bid/32556/Hybrid-Robots-Autonomous-Industrial-Mobile-
Manipulators; http://www.machinevision.dk/joomla/index.php?lang5en.
Figure 15.4 Seegrid autonomous industrial mobile robot at work (moving a highly loaded
passive vehicle).
Source: http://www.engadget.com/2008/06/04/seegrid-shows-off-autonomous-industrial-
mobile-robot-system/.
638
Introduction to Mobile Robot Control

15.3
Mobile Robots in the Society
The applications of mobile robots in the society are numerous with new ones added
continuously. Some of these applications are as follows:
G
mobile manipulators for rescue,
G
robotic canes and guiding assistants,
G
mobile robots for home services,
G
assistive mobile robots for the elderly and “persons with special needs” (PwSN),
G
mobile telerobots and web robots.
15.3.1
Mobile Manipulators for Rescue
Natural and manmade disasters offer unique challenges for effective cooperation of
robots and humans. The location of disasters are usually too dangerous for human
intervention or cannot be reached. In many cases, there are additional difficulties
such as extreme temperatures, radioactive levels, strong wind forces that do not
allow a fast action of human rescuers. Lessons learned from past disaster experi-
ence have motivated extended research and development in many countries for the
construction of suitable robotic rescuers. Due to the strong earthquake activity,
Japan is one of the countries where strong and effective autonomous or semiauton-
omous robotic systems for rescue were developed. Modern robot rescuers are light,
flexible, and durable. Many of them have cameras with 360 rotation that provide
high resolution images, and other sensors that can detect body temperature and col-
ored clothing. Figure 15.6 shows the search and rescue robot “Telemax” promoted
at RoboCup2009, and Figure 15.7 shows the urban search and rescue robot (devel-
oped by NIST/DHS) at an actual exercise.
Figure 15.8 shows a rescue robot operated when needed by the Tokyo Fire
Department.
Rescue robots are also of the snake-type as shown in Figure 15.9A and B.
Figure 15.6 The Telemax robot for rescue.
Source: http://www.gizmag.com/search-and-rescue-robots-at-robocup-2009-12144-12144/.
639
Mobile Robots at Work

15.3.2
Robotic Canes, Guiding Assistants, and Hospital Mobile Robots
Mobile robots and humanoid robots have been exploited for developing and con-
structing systems that help blind people to find their way around large buildings
(such as supermarkets, museums, hospitals, airports). Two of them are:
G
the eye-Robot,
G
the robotic shopping assistant.
The eye-Robot (Figure 15.10) is a system that can be regarded as being between
a white cane and a seeing eye dog.
Figure 15.7 The search and rescue robot moving across a rubble pile in a NIST/DHS exercise.
Source: http://www.science20.com/news/rescue_robots_are_on_the_way.
Figure 15.8 Rescue robot used by Tokyo Fire Department.
Source: http://web-japan.org/trends/09_sci-tech/sci100909.html.
640
Introduction to Mobile Robot Control

The eye-Robot has been designed by using the Roomba robot as a base and can
guide a blind or visually impaired user through cluttered and populated environ-
ments. The user indicates his/her desired motion by intuitively pushing on and
twisting the handle. The robot uses this information and finds an obstacle-free route
down a hallway or across a room, using sonar to steer the user in an appropriate
direction. In actuality, the user can naturally follow behind the robot with little con-
scious thought. The eye-Robot has four ultrasonic range finders and two IR sensors
facing out 90 to the right and left to enable the robot in wall following.
Figure 15.9 Two examples of snake-type rescue robots.
Source: (A) http://dart2.arc.nasa.gov/Exercises/TMR2004/TMR-d2/images/23DSC00207.jpg
and (B) http://www.elistmania.com/images/articles/21/Thumbnail/Snake_Robots.jpg.
Figure 15.10 The eye-Robot cane for the blind.
Source: http://forums.trossenrobotics.com/showthread.php?1409-The-eyeRobot-Robot-
Blind-Aid; http://www.instructables.com/id/eyeRobot---The-Robotic-White-Cane/.
641
Mobile Robots at Work

The robotic shopping assistant is shown in Figure 15.11. It was created at Utah
State University. The user, when arriving at the grocery store, grabs the shopping
assistant which leads him/her to the different products. When the user leaves the
store he/she leaves the robot behind.
Figure 15.12 shows a mobile hospital robot developed by InTouch Health and
put in operation at the Healdsburg District Hospital (Somona County, CA). This
robot can implement online remote interviews of the patient with the specialist
doctor.
Another hospital mobile robot is shown in Figure 15.13 which was developed at
Keio University (Japan). This robot can automatically generate a map of its envi-
ronment and carry medical supply in hospitals.
15.3.3
Mobile Robots for Home Services
Home or domestic robots are mobile robots and mobile manipulators designed for
household tasks such as floor cleaning, pool cleaning, coffee making, serving.
Also, robots suitable for helping elderly people and PwSN may be included in the
class of home robots, although they can be regarded to belong to the more general
class of assistive robots which will be discussed separately. Today, robots include
also humanoid robots suitably created for helping in the house.
Figure 15.11 The robotic shopping assistant can be used in large stores and airports.
Source: http://news.bbc.co.uk/2/hi/technology/4509403.stm.
642
Introduction to Mobile Robot Control

Figure 15.13 The Keio University MKP003 hospital mobile robot.
Source: http://www.robotliving.com/robot-news/hospital-robot.
Figure 15.12 The InTouch Health RP-7 hospital robot.
Source: http://www.cnet.com/2300-11394_3-6184443-2.html.
643
Mobile Robots at Work

Examples of home robots are the following:
Dustbot—This is a series of multifunctional robots that can keep homes and cities clean.
This series includes the Dust-Cart, a humanoid robot for door-to-door garbage collection.
It has a height of 1.45 m, a weight of 70 kg, and two wheels for feet. It is appropriate for
door-to-door garbage collection in very narrow city streets that traditional garbage trucks
cannot access (Figure 15.14A).
Dust Clean can be used for automatic cleaning of narrow town streets (Figure 15.14).
Care-O-bot 3—This robot (Figure 15.15) has a highly flexible arm with a three-finger
hand that is capable of picking up home items (a bottle, a cup, etc.). It can carefully pick
up a bottle of orange juice and put it next to the glasses on the tray in front of it. To do
this, it is equipped with many sensors (stereo vision, color cameras, laser scanners, and a
3D range camera).
It knows what a glass looks like, where to find it in the kitchen, and can be
taught to recognize new objects.
ARI-100 robot—This robot is specialized to duct cleaning and inspection. It
involves an articulated arm which can move in all directions allowing it to clean
efficiently every corner of the duct, without the need to manually adjust the length
of the brush (Figure 15.16).
Roomba discovery vacuum—This is a robotic floor vacuum capable of moving
around home and sweeping up dirt while moving. It performs three types of clean-
ing via two rotating brushes that sweep the floor, a vacuum sucking dust and parti-
cles off the floor, and side sweeping brushes to clean baseboards and walls
(Figure 15.17).
Roomba recognizes its position in the room and avoids possible risks and stairs.
It returns for a self-charging home after the floor is clean or when it needs to
recharge.
Figure 15.14 The humanoid garbage collector Dust-Cart (left) and the mobile robot
automatic street-cleaner (right).
Source: http://www.gizmag.com/dustbot-multifunctional-robots-keep-town-tidy/12923/.
644
Introduction to Mobile Robot Control

15.4
Assistive Mobile Robots
Assistive mobile (and fixed) robots belong to assistive technology which in our
times is a major field of research, given the aging of population and diminishing
number of available caregivers. Assistive robotics (AR) includes all robotic systems
that are developed for PwSN and attempt to enable disabled people to reach and
Figure 15.16 The duct cleaning and inspection robot of Robotics Design Inc (Anatroller).
Source: http://roboticsdesign.en.ec21.com/Duct_Cleaning_and_Inspection_Robot3113255_
3113256.html.
Figure 15.15 The Care-O-bot 3 home service robot (height 1.45 m).
Source: http://phys.org/news134145359.html.
645
Mobile Robots at Work

maintain their best physical and/or social functional level, improving their quality
of life and work productivity [6,7].
The main categories of PwSN are as follows:
G
PwSN with loss of lower limb control (paraplegic patients, spinal cord injury, tumor,
degenerative disease)
G
PwSN with loss of upper limb control (and associated locomotor disorders)
G
PwSN with loss of spatiotemporal orientation (mental, neuropsychological impairments,
brain injuries, stroke, ageing, etc.).
The field of AR was initiated in North America and Europe in the 1960s. A
landmark assistive robot is the so-called Golden Armo developed in 1969, a seven
degrees-of-freedom orthosis moving the arm in space (Rancho Los Amigos
Hospital, CA). In 1970, the first robotic arm mounted on a wheelchair was
designed. Today many smart AR systems are available, including the following:
i. Smart-intelligent wheelchairs that can eliminate the user’s task to drive the wheelchair
and can detect and avoid obstacles and other risks.
ii. Wheelchair-mounted robots (WMRs) which offer the best solution for people with motor
disabilities increasing the user’s mobility and the ability to handle objects. Today,
WMRs can be operated in all alternative ways (manual, semiautomatic, automatic)
through the use of proper interfaces.
iii. Mobile autonomous manipulators, that is, robotic arms mounted on mobile platforms,
that can follow the user’s (PwSNs) wheelchair in the environment, can perform tasks in
open environments, and can be shared between several users.
Figure 15.17 The Roomba Discovery vacuum.
Source: www.robotshop.com/robotics-floor-cleaners.html.
646
Introduction to Mobile Robot Control

Three well-known European assistive robots are the French MASTER robot,
the Dutch MANUS robot, and the UK RTX robot. The European Union has
launched in 1991 the “Technology Initiative for Disabled and Elderly People”
(TIDE). During the pilot phase of TIDE the following robotic systems were
developed: MARCUS, M3S, RAID, and MECCS. During the next phase (Bridge
Phase) the following systems were created: SENARIO, FOCUS, EPI-RAID,
OMNI, and MOVAID in the framework of respective R&D projects [815].
Figures 15.18 and 15.19 show the Bremen (IAT) wheelchair with the “functional
robot arm with user friendly interface for disabled people (FRIEND)” and the ser-
vice mobile manipulator MOVAID [11].
Typical tasks of MOVAID are to warm up some food in a microwave oven and
serve it to the user’s bed, clean the kitchen surface, remove dirty sheets from a bed,
etc. [11].
15.5
Mobile Telerobots and Web Robots
Telerobots combine the capabilities of standard robots (rigid or mobile) and teleo-
perators. Teleoperators are operated by direct manual control and need an operator
to work in real time for hours. Of course, due to the human supervision they can
perform nonrepetitive tasks (as, e.g. it is required in nuclear environments) [4].
Telerobots have more capabilities than either a standard robot or a teleoperator,
because they can carry out many more tasks that can be accomplished by each one
of them alone. Therefore, the advantages of both are fruitfully exploited, and their
limitations minimized. Telerobots can work with incomplete knowledge and mod-
els of the task space, being able to perform nonrepetitive tasks. The basic drawback
Figure 15.18 The wheelchair FRIEND armed with the MANUS robot arm.
Source: www.AMaRob.de.
647
Mobile Robots at Work

of telerobots is the occurrence of variable delays between the operator and the
manipulator, especially in space applications.
Web robots, that is, robots taking their inputs and sending their outputs using as
infrastructure the World Wide Web, are used for teleoperation, education, and
entertainment purposes [26,27]. These systems can be remotely controlled via the
Internet from any site having a typical web browser that incorporates the human
operator control interface [25]. Telerobots find applications in space, terrestrial and
deep sea exploration, as well as in remote intervention/surgery [4].
One of the major problems of telerobotic systems is that if there exist significant
delays in the communication links, the system may exhibit instability. One way to
face this problem is to use a shared supervisory scheme, where the control of the
robots and other devices can be shared between a local control system and the human
operator [4]. Two other techniques for solving this problem are the following:
G
the scattering/wave variable teleoperation technique [28],
G
the autoregressive integrated moving average (ARIMA) delay modeling/identification
technique [29].
Human factors do not, in general, play a major role in robotic applications, but
when a supervisory type of control is used with a human being at the center of the
control loop they should be necessarily taken into account. That is, important infor-
mation on how to improve the system can be acquired from the analysis of the
human operator.
Figure 15.20 shows a pictorial representation of the evolution of teleoperation
towards intelligent telerobotics [30].
Figure 15.21 shows a prototype mobile telerobot built at Sethu Institute of
Technology which is capable of picking up objects with maximum degree of free-
dom with distant commands.
Figure 15.19 MOVAID—Mobility and activity assistance system for the disabled.
Source: http://www.robocasa.net/workshop/2007/pdf.laschi.pdf.
648
Introduction to Mobile Robot Control

NASA has put a considerable research effort and investment in three fundamen-
tal areas [5]:
G
Remote operations on planetary and lunar surfaces
G
Satellite and space system servicing
G
Robotics tending of scientific payloads
These areas required advance automation technology (to reduce crew interac-
tion), hazardous material handling, robotic vision systems, collision avoidance
algorithms, etc.
Mechanical
teleoperators
Servo-controlled
master-slave
Computer-aided
teleoperation
Shared-autonomy
teleoperation
Supervisory
control
Intelligent
autonomous
robots
Robotics
Evolution of
teleoperation systems
Robot
autonomy
Human operator
control/teleoperator
Intelligent telerobotics
Figure 15.20 Evolution of teleoperation systems.
Figure 15.21: The Sethu mobile telerobotic manipulator.
Source: http://robots.net/robomenu/1195489702.html.
649
Mobile Robots at Work

The most famous robots used in the outer space applications are the NASA
Mars Rovers. The Pathfinder Mission landed on Mars in 1997, with its robotic
rover, Sojourner (Figure 1.2, http://mars.jpl.nasa.gov/MPF/mpf/rover.html).
The current mission to Mars consists of two robotic rovers known as the Mars
Exploration Rovers. These twin rovers have a panoramic camera suitable for tex-
ture, color, mineralogy, and structure examination of the local terrain. Also the
rovers were equipped with a miniature thermal emission spectrometer for the iden-
tification of rocks. Jet Propulsion Laboratory (JPL) in Pasadena (CA) has devel-
oped the HAZMAT telerobot (Figure 15.22) for the safe exploration of dangerous
sites and handling of hazardous materials in conjunction with the Hazardous
Materials Team (HAZMAT). Two video, cameras, one located on the platform and
one on the gripper, provide feedback to the operator.
The key issue that will lead to the development of more advanced telerobotic
systems, suitable for precise intervention and services, is the interaction and
merging of machine intelligence properties with human capabilities and skills.
Web-based remote laboratories can be used for both actual operation tests and edu-
cational purposes.
In general, a web-based remote laboratory contains an access management sys-
tem (AMS), a collaboration server (CS), and the experimental server (ES). These
components can be implemented using any proper combination of available techno-
logical tools (Matlab, LabView, VRML, Java, etc). This architecture (Figure 15.23)
uses the client/server scheme implemented in several ways. The AMS coordinates
the accessibility of users (operators, students, etc) to the experiment, who can use
any station equipped with a web browser and Java environment if the heterogeneity
problem must be eliminated.
A few representative examples of the development and use of web robots,
including their educational applications, are the following:
G
the Mercury web robot [16],
G
the Telegarden web robot [19],
G
the Australian telerobot [17],
Figure 15.22 The Hazmat JPL
telerobot.
Source: http://www.engadget.com/
2008/06/04/seegrid-shows-off-
autonomous-industrialmobile-robot-
system/.
650
Introduction to Mobile Robot Control

G
the teleoperated multirobot platform (Teleworkbench) [18].
G
the Swish open access web robot (Khep on the Web) [20].
The Mercury web robot (one of the early web-based robots, built in 1994) con-
sists of an industrial robot arm fitted with a CCD camera and a pneumatic system
(http://usc.edu/dept/raiders). All robots are accessible via standard point-and-click
mouse commands, using a 2D workspace.
The Telegarden web robot was developed as a continuation of the Mercury
robot at the University of Southern California, launched in June 1995 (http://www.
usc.edu/dept/garden).
The Australian telerobot was invented by Ken Taylor and developed at the
University of Western Australia in 1994. A 6-axis ABB robot is teleoperated over
the web. Java and Java script is used which enables standard common gateway
interface (CGI) communication with the web server while enabling moves to be
programmed using a Java wireframe of the robot. The overall structure of the sys-
tem is shown in Figure 15.24.
The teleworkbench was developed at the University of Paderborn (Germany) to
ease the tasks of carrying out experiments with single or multiple mini (mobile)
robots. The general architecture of Teleworkbench is shown in Figure 14.25A with
many communicating minirobots, and the teleworkbench server structure is shown
in Figure 15.25B.
Teleworkbench provides a standard environment in which algorithms and pro-
grams can be tested and validated on real robots. Two of the robots used in
Teleworkbench are Khepera II and Bebot minirobot (Heinz Nixdorf Institute).
Khep on the Web—This system was built to remotely control Khepera mobile
robots at the LAMI (Microprocessor and Interface Lab) of the Swiss Federal
Institute of Technology of Lausanne (EPFL). The objective of Khep on the Web
was to provide an open access web robotic platform for scientific research in
mobile robotics. Khepera is a small cylindrical robot (diameter 55 mm, variable
height) with a suspended cable for power and other signals supply without any dis-
turbance on its motion. The camera used may not have wide-angle lenses. The sys-
tem consists of a sensory-motor board (with eight infrared proximity sensors), a
WWW
…
Access management
system
Collaboration server
Experimental server
HTML
Student station 1
Student station 2
Student station n
Experiment
robot
SQL DB
Figure 15.23 General architecture of a web-based telelab (experiment).
651
Mobile Robots at Work

CPU board (Motorola 68331 µC), and a video board having a color CCD camera
(500 3 582 pixels). The web server launches a number of CGI scripts to perform
the tasks that communicate via shared memory. A Java applet running on the client
side sends several information requests, and a CGI script on the server replies to
these requests by obtaining the information from the robot and the shared memory.
The structure of the Khep on the Web system is shown in Figure 15.26.
Figure 15.27 shows the Khepera robot equipped with a video camera and look-
ing to a mirror. The robot worked in a maze environment with walls higher than
the robot such that the visitor has to move around to visit the maze (Figure 15.28).
One of the walls is a mirror [20].
15.6
Other Mobile Robot Applications
Other applications of mobile robots include war robots and entertainment robots.
15.6.1
War Robots
The design, development, and construction of robots for the war has raised strong
ethical questions. In general, the weaponized of robots (missile, unmanned combat
air vehicles, unmanned terrestrial vehicles, etc.) has got a substantial portion of the
investment in robotics research and development [24]. In general, military robots
operate in geopolitically sensitive environments, and so a greater caution is cer-
tainly needed. For example, what happens if a unmanned aerial vehicle (UAV) mis-
takenly decides that a “friendly” is a target and then fires it? A small subset of
Web
Image
server
Robot
server
CGI
script
HTTP 
server
Web server PC
Remote PC
Remote PC
x
y
Camera
Controller
Figure 15.24 Architecture of the Australian UWA telerobot (http://telerobot.mech.uwa.edu.au).
652
Introduction to Mobile Robot Control

land, aerial, and underwater robotized autonomous combat or exploration vehicles
is the following:
Squat four-wheeled robot—This is driving itself through densely wooded terrain and it is
too small to be threatening (Figure 15.29). It is called XU12 (for “experimental
unmanned vehicle”) and can navigate autonomously from point A to point B without
WWW
Cam 1
Cam 5
VS-1
VS-2
VS-3
VS-4
VS-5
WWW server
User PC
User PC
User PC
User PC
Robot
Wireless
link
(A)
Cam 2
Cam 3
Cam 4
Robot
comm
Socket
comm
Message
processor
Other
modules
WWW
Bluetooth
MiniWMRs
Client
Client
Client
Client
(B)
Teleworkbench server
Figure 15.25 Teleworkbench system: (A) General architecture and (B) Teleworkbench
server.
Web
Serial port 1
Frame grabber 
multiplexer
Serial port 2
PC
Camera
Khepera
Users
Figure 15.26 The architecture of Khep on the Web system.
653
Mobile Robots at Work

clobbering a boulder, hammering a tree, and so on. The robot is useful for inspection and
surveillance purposes.
Mapping swarmbots—These have been developed jointly by Georgia Tech. University
and JPL. They are collaborative mobile robots capable of autonomously mapping whole
building for first responder and military purposes. They are equipped with a video camera
and a laser scanner each (Figure 15.30).
Figure 15.27 Khepera looking a mirror.
Source: www.biorobotics.ri.cmu.edu/papers/spb_papers/integrated1/khepera_vsmm97.pdf.
Figure 15.28 A 65 3 90 cm maze environment where Khepera has to move.
Source: www.biorobotics.ri.cmu.edu/papers/spb_papers/integrated1/khepera_vsmm97.pdf.
654
Introduction to Mobile Robot Control

Voyeur-autonomous UAV—This is an autonomous UAV that can be used for military
applications such as surveillance and target acquisition, as well as locating and detonating
improvised explosive devices (IEDs) (Figure 15.31). It can be launched from an airplane
or carried in a backpack and launched manually.
HAUV-N—This is an IED detection and neutralization version of the autonomous under-
water vehicle (AUV) able to identify and destroy ship hull mines and underwater IEDs
sparing human divers from this dangerous task (Figure 15.32).
X-47B—This is a stealth UAV that closely resembles a strike fighter (Figure 15.33). It
can take off from and land on an aircraft carrier and support midair refueling. X-47B has
a range of 3380 km, it can fly up to 40,000 ft at high subsonic speeds, and can carry up
to 2000 kg of ordnance in two weapons bays.
Figure 15.29 The XU12 vehicle.
Source: http://www.popsci.com/scitech/article/2005-12/robots-go-war.
Figure 15.30 GaTech/JPL mapping swarmbots.
655
Mobile Robots at Work

15.6.2
Entertainment Robots
These robots belong to the more general class of social robots. A social robot is a
high-level intelligent autonomous robot that knows how to interact in a humanistic
way with humans. Social robots are equipped with sensory, learning, social rules,
and performance, etc. A basic requirement for a robot to be socialized is to be
completely autonomous, capable of communication and cooperative interaction
with everyday persons (nonprofessionals). Most of the available socialized and
entertainment robots are mobile robots that have a legged-humanoid or wheeled-
Figure 15.31 The Voyeur-Autonomous UAV (Northrop Grumman Corp.).
Source: http://thefutureofthings.com/pod/1261/voyeur-autonomous-uav.html.
Figure 15.32 HAUV-N underwater IED detector and neutralizer (Bluefin Robotics Corp).
Source: www.militaryaerospace.com/index/display/mae-defense-executive-article-display/
3856814793/articles/military-aerospace-electronics/executive-watch-2/2011/.
656
Introduction to Mobile Robot Control

semihumanoid (the upper body part) appearance, that have the ability to write, play
musical instruments, interact emotionally, dance, soccer, etc.
According to Dautenhahn [22], a list of basic social skills that an entertainment/
socialized robot must have is the following:
G
Ability to contact with humans in a repeated and long-life setting. The robot should have
the ability to become personalized, recognizing and adapting to its owner’s preferences.
G
Ability to negotiate tasks and preferences and provide “companionship.”
G
Ability to adapt, to learn, and to expand its skills, e.g., by being taught new performances
by its owner.
G
Ability to play a role of companion in a more human-like way (probably similarly to
pets).
G
Social skills—These are essential for a robot to be acceptable as a companion. For exam-
ple, is good to have a robot that says “would you like me to bring a cup of coffee?”. But,
it may not be desirable to ask this question while we are watching our favorite TV.
G
When moving in the same area as a human, the robot always changes its route to avoid
getting too close to the human, especially if the human’s back is turned.
G
The robot turns its camera properly to indicate by its gaze that it was looking in order to
participate or anticipate what is going on in the surrounding area.
Some
functional
entertainment
and
socialized
robots
are
shown
in
Figures 15.3415.37.
G
Kaspar—This is a child-sized humanoid robot able to interact with children with autism,
using gestures, expressions, synchronization, and imitation [23]. It has eight degrees of
freedom in the head and neck and six degrees of freedom in the arms and hands. The
face is a silicon-rubber mask, which is supported by an aluminum frame (Figure 15.34).
G
Wow Wee Roboscooper—This entertainment robot has both functional capabilities and
funny and cute looking (Figure 15.35).
Figure 15.33 The X-47B US Navy Stealth UAV (Northrop Grumman Corp.).
Source: http://thefutureofthings.com/pod/6239/x-47b-first-navy-stealth-uav-ready.html.
657
Mobile Robots at Work

Figure 15.34 The socialized robot Kaspar with open arms entertains a girl.
Source: http://kaspar.herts.ac.uk.
Figure 15.35 The entertainment robot Wow Wee Roboscooper.
Source: http://www.learnaboutrobots.com/entertainment.htm.
Figure 15.36 The bar service entertainment robot.
658
Introduction to Mobile Robot Control

G
Robot-Barman—This robot is capable of opening beer bottles and other drink bottles and
serve the bar’s customers (Figure 15.36).
G
Humanoid robot soccer—This is an adult-sized humanoid robot, fully autonomous, that
can participate efficiently in a robot football championship (Figure 15.37). The ultimate
goal of the designers is to develop by 2050 robot football player that can win against the
human world champion team in soccer. Every robot on the field can handle different
situations in the field (e.g., finding the ball, moving towards it, and carefully manipulat-
ing it towards the goal).
Eventually the skills developed for a soccer team will be transferred to other
areas like domestic robotics, education, rescue, etc.
15.6.3
Research Robots
All the robots considered in the present chapter have passed through several stages
of research and development, and most of them are still used in universities and
institutions for further research. Just as a final example, Figure 15.38 shows two
fully equipped P3-DX robots that can be used for research and actual cooperative
operations in large areas such as workshop and office floors, logistic areas, hospi-
tals. Each robot is equipped with an ultrasonic array, a SICK laser range finder, a
pan-tilt camera, an onboard netbook, and a different bar code landmark for robot
identification and distinction. Tasks that can be performed by groups of such robots
include a large repertory of exploration and surveillance tasks, such as building and
updating maps, locating particular objects on the map, tracing the product flow in a
logistic space, etc. Actually, Section 13.13 discusses a case of cooperative robotic
exploration of this kind.
Figure 15.37 Bots testing their soccer skills in RoboCup 2010.
Source: www.youtube.com/watch?v=4wMSiKHPKX4.
659
Mobile Robots at Work

15.7
Mobile Robot Safety
The application of mobile robots in industry and society removes the requirement
for humans to perform several useful operations that may be difficult or dangerous
for the human operator. Although fixed robot safety has long been a central issue
of consideration, actually there has been less effort for the investigation and assur-
ance of autonomous robots’ safety. Safety is especially critical for mobile robots in
home and health services where the humans (typically not accustomed to robots)
might get in contact with them. Another point that creates the demand for safety,
when using mobile robots, is the fact that these robots work autonomously with
nonrepetitive or nonpredictable motions. These motions are due to that the robots
operate in uncertain and unstructured environments, which need their ability to
autonomously (and intelligently) change tasks, operating online on the basis of sen-
sory information that may be not accurate enough, etc. Therefore, to secure safety
for the people and the environment, the following safety measures are needed:
G
Accurate, reliable, and redundant sensory systems
G
Reliable software
G
Low speed of operation
The maximum speed of operation that can be achieved depends on the following:
G
Mechanical and dynamic limitations
G
Computational and control limitations
G
Unexpected dynamic changes of the environment
The first issue concerns the case where there is the possibility of longitudinal or
lateral slippage when a certain velocity is exceeded. In practice, however this is not
Figure 15.38 Two P3-DX robots that can be used in cooperative exploration and
surveillance tasks.
Source: http://areeweb.polito.it/ricerca/MacP4Log/index.php?option5com_content&view5
article&id528:cooproboteamsupervandmgmtlosspaces&catid59.
660
Introduction to Mobile Robot Control

a major problem since maximum speed of motion is subject to other more strict
limitations. The second issue has to do mainly with how quickly the robot can
avoid obstacles, which depends on the computational speed, the control algorithm,
and the sensing rate and accuracy. The third issue refers to unexpected collision
and visibility problems. All these issues were studied following different
approaches under different conditions and assumptions (see, e.g., Refs. [3133]).
The implementation of safety measures, to avoid any risk both for the user and per-
sons in the environment of the mobile robot, can be made in the following three levels:
G
Passive safety—This depends on the electromechanical design (low inertia of moving
parts, restricted motor speeds at maximum voltage, limited static motor torques at stall,
possibility of passive elements, etc.).
G
Supervised safety—This includes watch dog, safety monitor, velocity and force/torque
limits, forbidden zones, etc.
G
Interactive safety—This implies that the user is always able to switch off the system, that
there exist proper (well distinguished) alarm signals, and that “control-blocking” condi-
tions (i.e., conditions under which the user looses access to the control transducers where
the transducers get out of reach) are not possible to occur.
The dominant means for mobile robots safety systems is braking. There must
always be possible to stop the robot’s motion in one of the following ways:
G
Emergency stop—user initiated or automatic shutdown due to an unrecoverable hazard
condition.
G
Intrusion pause—initiated by a sensor when an external hazard occurs, in which case the
user must be able to restart the system when the hazard is removed.
G
User pause—initiated by the user whenever he/she detects a potential hazard or problem,
in which case the user can restart or cancel the operation after the removal of the problem.
All these aspects are under extensive study by research institutes and mobile
robot manufacturers, and today many alternative safety systems are available inter-
nationally. These systems are compatible with existing safety standards, such as the
US ASME B56.5-2004 Safety Standard for driverless industrial vehicles [34], and
the British EN 1525-1998 Safety Standard for driverless industrial vehicles allow-
ing the use of noncontact safety sensors [35]. The National Institute Standards and
Technology (NIST) is working continuously to enhance the safety standards for
AGVs with the use of noncontact sensors, but also for providing evaluation stan-
dards of novel 3D real-time range sensor technology. A study where the above
NIST enhancements are considered is provided in Ref. [36]. A discussion of the
hazards associated with the use of industrial robots and the principles of guarding
to assure human safety is provided in Ref. [37]. In this work, the hazards associated
to robots are distinguished as follows:
G
Control errors
G
Mechanical hazards
G
Environmental hazards
G
Human errors
G
Ancillary equipment
661
Mobile Robots at Work

The guarding measures, while the robots are functioning, are classified as:
G
Mechanical guards with safety interlock
G
Presence sensing systems
G
Trip devices
G
Emergency stop equipment
G
Screens between station
G
Work envelope limit stop
References
[1] Nof S, editor. Handbook of industrial robotics. New York, NY: John Wiley & Sons;
1999.
[2] Schraft RD, Schmierer G. Service robots. London: Peter AK/CRC Press; 2000.
[3] Takahashi Y, editor. Service robot applications. In Tech/Read Online; 2008.
[4] Sheridan TB. Telerobotics, automation and human supervisory control. Cambridge,
MA: MIT Press; 1992.
[5] Votaw B. Telerobotic applications. ,http://www1.pacific.edu/eng/research/cvrg/mem-
bers/bvotaw.; 2013 [accessed 20 August].
[6] Cook AM, Hussey SM. Assistive technologies: principles and practice. St. Louis, MO:
Mosby; 2002.
[7] Reddy R. Robotics and intelligent systems in support of society. IEEE Intell Syst
2006;MayJune:2431.
[8] Dallaway JL, Jackson RD, Timmers PHA. Rehabilitation robotics in Europe. IEEE
Trans Rehabil Eng 1995;23:3545.
[9] Tzafestas SG, editor. Autonomous robotic wheelchair projects in Europe improve
mobility and safety (Special Issue). IEEE Robot Autom Mag 2001;17(1):173.
[10] Tzafestas SG, editor. Autonomous mobile robots in health care services (Special
Issue). J Intell Robot Syst 1998;22(34):177350.
[11] Dario P, Guglielmelli E, Laschi C, Teti G. MOVAID: a personal robot in everyday life
of disabled and elderly people. J Technol Disabl 1999;10:7793.
[12] Pires G, Honorio N, Lopes C, Nunes U, Almeida AT. Autonomous wheelchair for dis-
abled people. In: Proceedings of IEEE international symposium on industrial electron-
ics (ISIE ’97). Guimaraes; 1997. p. 797801.
[13] Duffy BR. Social embodiment in autonomous mobile robotics. Int J Adv Robot Syst
2004;1(3):15570.
[14] Tiwari P, Warren J, Day KJ, McDonald B. Some non-technology implications for
wider application of robots assisting older people. In: Proceedings of HIMMS Asia
Pacific 11. Melbourne, Australia; 2023 September 2011. p. 115.
[15] Martens C, Prenzel O, Gra¨ser A. The rehabilitation robots FRIEND I&II: daily life
independence through semi-autonomous task execution. Vienna: I-Tech Education and
Publishing; 2007.
[16] Goldberg K, Gentler S, Sutter C, Wiegley J. The Mercury project: a feasibility study
for internet robots. IEEE Robot Autom Mag 2000;7(1):3540.
[17] Trevelyan J. Lessons learned from 10 years experience with remote laboratories. in:
Proceedings of international conference on engineering education and research. VSB-
TUO, Ostrava; 2004. p. 110.
662
Introduction to Mobile Robot Control

[18] Tanoto A, Ru¨ckert U, Witkowski U. Teleworkbench: a teleoperated platform for
experiments in multirobotics. In: Tzafestas SG, editor. Web-based control and robotics
education. Berlin/Dordrecht: Springer; 2009. p. 26796.
[19] Goldberg K, editor. The robot in the garden: telerobotics and telepistemology in the
age of internet. Cambridge, MA: MIT Press; 2000.
[20] Saucy P, Mondada F. Khep on the Web: open access to a mobile robot on the internet.
IEEE Robot Autom Mag 2000;7(1):417.
[21] Prassler E, Scholz J, Fiorini P. Navigating a robotic wheelchair in railway station dur-
ing rush hour. Int J Robot Res 1999;18:76072.
[22] Dautenhahn K. Socially intelligent robots: dimensions of humanrobot interaction.
Philos Trans R Soc Lond B Biol Sci 2007;362:679704.
[23] Dautenhahn K. Kaspar: kinesis and synchronization in personal assistant robotics.
University of Hertfordshire, UK: Adaptive Research Group; ,http://kaspar.feis.herts.
ac.uk..
[24] Zaloga S. Unmanned aerial vehicles: robotic air warfare 19172007. Oxford: Osprey
Publishing; 2008.
[25] Taylor K, Dalton B. Internet robots: a robotics niche. IEEE Robot Autom Mag 2000;7
(1):2734.
[26] Tzafestas SG, editor. Web-based control and robotics education. Berlin: Springer; 2009.
[27] Tzafestas SG, Mantelos A-I. Time delay and uncertainty compensation in bilateral tele-
robotic systems: state-of-art with case studies. In: Habib M, Davim P, editors.
Engineering creative design in robotics and mechatronics. Mershey, PA: IG Global;
2013. p. 20838.
[28] Munir S, Book WJ. Internet-based teleoperation using wave variable with prediction.
Proc IEEE/ASME Trans Mechatron 2002;7:12433.
[29] Yang M, Li XR. Predicting end-to-end delay of the internet using time series analysis.
Technical Report, University of New Orleans, Lake front; November 2003.
[30] Tzafestas CS. Web-based laboratory on robotics: remote vs virtual training in program-
ming manipulators. In: Tzafestas SG, editor. Web-based control and robotics educa-
tion. Berlin: Springer; 2009. p. 195225.
[31] Hong T, Bostelman R, Madhavan R. Obstacle detection using a TOF range camera for
indoor AGV navigation. Gaithersburg, MD: PerMIS; 2004.
[32] Pare C, Seward DW. A model for autonomous safety management in a mobile robot.
Proceedings of CICMCA’05 international conference on computational intelligence for
modeling control and automation. Washington, DC: IEEE Computer Society; 2005.
[33] Chung W, Kim S, Choi M, Choi J, Kim H, Moon CB, et al. Safe navigation of a
mobile robot considering visibility of environment. IEEE Trans Ind Electron 2009;56
(10):39419.
[34] American Society of Mechanical Engineers. Safety standard for guided industrial vehi-
cle and automated functions manned industrial vehicle. Technical Report ASME
B56.5, 1993.
[35] British standard safety of industrial trucks—driverless trucks and their systems.
Technical Report BSEN-1525, 1998.
[36] Bostelman RV, Hong TH, Madhavan R, Chang TY. Safety standard advancement
toward a mobile robot use near humans. Proceedings of RIA -SIAS’05 Conference,
Chicago, IL, U.S.A.; 2005. www.et.byu.edu/Bered/ME486/Professional_Journal.pdf.
[37] Department of Labor, Robot safety. ,http://www.osh.dol.govt.nz/order/catalogue/
robotsafety.shtml.; 2013 [accessed 20 August].
663
Mobile Robots at Work

Problems
A. Kinematics
1. Derive the formulas for sinðθ 1 φÞ and cosðθ 1 φÞ expanding symbolically two rota-
tions of θ and φ via the concept of the rotation matrix R.
2. Extending the kinematic analysis of the three-omni-wheel system of Section 2.4,
derive the kinematic equations of a four-wheel and five-wheel omnidirectional
WMR shown in Figure P.1, and study the velocity augmentation factor in each one
of
them.
To
this
end,
write
a
Matlab
program
for
angular
displacements
φA½0; 180.
3. Derive the kinematic model of a bicycle.
4. Derive the Jacobian matrix of a WMR with longitudinal and lateral wheel slip: (a) for
differential drive, (b) for car-like, (c) for three- and four-wheel omnidirectional
robots.
5. Study analytically the inaccessible area for a differential drive, a bicycle, and a car-
like WMR.
B. Dynamics
6. Derive the Lagrange dynamic model of the nonholonomic two-wheel inverted pen-
dulum robot shown in Figure P.2.
The body of the pendulum is assumed a particle with its mass concentrated at its
end. The wheels are conventional wheels.
7. Derive the Lagrange dynamic model of a conventional wheel with slip. Use this
result to find the corresponding dynamic model for differential-drive WMR with
slip.
8. Derive the kinematic and Lagrange dynamic model of the WMR shown in
Figure P.3 with pure rolling nonslipping wheels. The two wheels are placed in the
front of the robot and the third wheel is connected with a fixed structure and can
rotate freely around its vertical axis. The generalized coordinates vector is
qðtÞ 5 ðx; y; θ; ψ; φ1; φ2; φ3Þ as shown in Figure P.3. Formulate a least-squares param-
eter procedure in the following two cases:
(a) Without the wheel motor’s dynamics
(b) Including the wheel motor’s dynamics
9. (a) Derive the dynamic equation of the double pendulum shown in Figure P.4
where coordinates, parameters, and forces/torques are shown. Formulate the
dynamic model using the Newton and the Lagrange method. Describe how to
identify its parameters converting the dynamic model to a regressor form.
(b) Derive the Lagrange dynamic model of a SCARA (selective compliance assem-
bly robot arm) (see Figure 10.4E).
10. Propose a suitable optimization method for finding optimal persistently exciting tra-
jectories by using either a Fourier series or a polynomial representation of the trajec-
tory. Explain why these are good approximate persistently exciting trajectories.

yR
y
x
o
2α
ψ
d
φ1
Θ
φ3
φ2
Q
Center of mass (xm,ym)
Figure P.3 Mobile robot for identification.
Figure P.1 Four- and five-wheel omnidirectional robots.
A
B
C
q3
n2
n3
n1
q4
Figure P.2 Schematic of two-wheel inverted pendulum WMR.
666
Problems

C. Sensors
11. We are given an ultrasonic proximity sensor for detecting the presence of objects
within 0.5 m of the device. At time t 5 0, the sensor is pulsed for 0.1 ms. Suppose it
takes 0.4 ms for resonances to decay out within the transducer, and 20 ms for echoes
in the environment to die out.
(a) Find the range of time needed as a window.
(b) What is the minimum detectable distance?
(The speed of sound is 344 m/s.)
12. The output of a laser finder is corrupted by a Gaussian zero-mean noise having a
standard deviation σ 5 100 cm.
(a) How many measurements have to be averaged to obtain an accuracy of
6 0:5 cm with probability p 5 0:95?
(b) Show how to compensate the range measurements in case the noise has mean
value 5 cm.
13. What is the upper limit on the frequency of a modulating sine wave to obtain a
working distance of up to (but not including) 5 m using a continuous-beam laser
range finder?
14. (a) How many bits are needed to store a 512 3 512 image in which each pixel can
have 256 possible intensity values?
(b) Propose a procedure for computing the median in an n 3 n neighborhood.
(c) Propose a method which uses a single light sheet for determining the diameter
of cylindrical objects (assuming that the distance between the camera and
the centers of the cylinders is fixed, and that the array camera has a resolution
of N pixels).
15. What is the minimum number of moment descriptors required to differentiate
between the boundary shapes of Figure P.5?
16. What is the number of different shapes (called shape number) and the order of the
shape number in each of the following 2D objects? Consider the clockwise direction
(Figure P.6).
x2
y2
l2
I2
y1
x1 c2
l1
m1
m2
I1
c1
Fh1
Fh2
Fz2
Fh2
Fυ1
Fυ2
Fυ2
Fh1
Fz1
τ1
τ2
τ2
τ1
Figure P.4 The double pendulum model ðFh 5 Fhorizontal; Fv 5 FverticalÞ.
667
Problems

17. Figure P.7 shows a linear accelerometer in which y is the displacement of the mass
m with respect to the frame, and x is the displacement of the frame.
(a) Write down the differential equation of the accelerometer and find the transfer
function yðsÞ=aðsÞ, where aðtÞ 5 d2x=dt2 is the frame acceleration. Explain why
the output of the accelerometer is proportional to the acceleration and the condi-
tion under which this is true.
(b) Repeat the same for a rotational accelerometer.
18. We are given the 1-DOF gyroscope of Figure P.8.
A rotation of the frame about z generates a rotation of the gyroscope’s disc about
the axis y. This motion is affected by the spring parameter k and the friction β
according to the relation Ty 5 2 kθy 1 β dθy
dt


Figure P.6 Four planar objects specified by boundary primitives ð! o!; m1; ’2’; k3Þ.
A
θ
r
π
4
π
2
3π
2
π 5π
2
3π
2
7π
2
2π
r (Θ)
r (Θ)
Θ
π
4
π
2
3π
2
π 5π
2
3π
2
7π
2
2π
Θ
2A
A
A
A
θ
Figure P.5 Two boundary shapes and their corresponding distance vs angle signatures
ðaÞ rðθÞ 5 const: ðbÞ rðθÞ 5 A sec θ.
k
β
m
Y
y
x
Figure P.7 Accelerometer.
668
Problems

(a) Using the differential equation:
Τy 5 Jy
d2θy
dt2 1 βy
dθy
dt 1 J0ω0
dθz
dt
determine the equation that relates θy and θz (omit Jy and βy).
(b) When only the spring exists, find the transfer function of the resulting rate
gyroscope.
(c) When only the friction exists, find the transfer function of the resulting inte-
grated rate gyroscope.
19. Doppler sensors are based on the Doppler shift in frequency for measuring velocity
in maritime and aeronautical applications. Maritime systems use acoustical energy,
reflected from the ocean floor, and airborne systems sense microwave radio fre-
quency energy bounced off the surface of the earth. The microwave radar sensor is
aimed downward at an angle θ (usually 45) to sense ground velocity (Figure P.9).
Derive the formula that gives the true (actual) ground velocity υa in terms of the mea-
sured Doppler velocity component υd, the inclination angle θ, the speed of light c, the
transmitted frequency f0, and the observed Doppler shift frequency.
D. Control
20. Consider the control loop of a single robot axis. (a) Derive the transfer functions of
the open-loop and the closed-loop system. (b) Study the position and velocity
steady-state errors.
z
Frame
y
x
Figure P.8 One degree of freedom gyroscope.
Θ
υd
uα
Figure P.9 A Doppler ground-speed sensor inclined at an angle θ.
669
Problems

21. The dynamic model of a two-link robot is:
d11ðθ2Þ
d12ðθ2Þ
d12ðθ2Þ
d22


€θ 1
€θ 2
"
#
1
β12ðθ2Þ_θ
2
2 1 2β12ðθ2Þ_θ 1 _θ 2
2β12ðθ2Þ_θ
2
1
"
#
1
c1ðθ1; θ2Þg
c2ðθ1; θ2Þg


5
τ1ðtÞ
τ2ðtÞ


where g is the gravitational acceleration. (a) Choose a suitable state vector xðtÞ and
a control vector uðtÞ. (b) Under the assumption that D21ðθÞ exists, express the corre-
sponding state-space model equations of the robot in terms of dij; βij, and ci.
(c) Find a nonlinear inputoutput decoupling law when yðtÞ 5 xðtÞ. Determine the
Jacobian matrix of the robot with respect to the base coordinate frame.
22. Describe two drawbacks for each of the following two robot control methods:
(a) resolved rate control and (b) resolved acceleration control.
23. Find the unit step response of the following third-order system:
GðsÞ 5
a0
s3 1 a2s2 1 a1s 1 a0
5
1
~s3 1 a~s2 1 β~s 1 1
where ~s 5 s=ða0Þ1=3; a 5 a2=ða0Þ1=3, and β 5 a1=ða0Þ2=3. Express the transfer
function in terms of the parameters p; ζ, and ωc that are defined by:
~s3 1 a~s2 1 β~s 1 1 5 ð~s 1 pÞð~s2 1 2ζωc~s 1 ω2
cÞ.
Explain
why
for
a 5 1:3
and
β 5 2:0 the step response involves a reverse motion before it reaches the steady-
state value 1. Does this phenomenon occur in the second-order system? Where can
this be used in the third-order systems?
24. Prove that the control law τj 5 2 kjp ~qj 2 kjD_~qj 1 kjI
Ð t
0 ~qj dτ; j 5 1; 2; . . .; n assures
asymptotic stability despite the presence of friction and gravity terms in the robot
equation (3.11a).
25. Consider the two-link robot of Figure 10.6 and assume that there is an uncertainty in
the load placed at its end point (tip). The load mass is less than 1. The smallest
structural resonance frequency is 10 Hz. Compare via simulation (using Matlab) the
performance of the controllers:
G
Local PID controller
G
Computed torque controller
G
Sliding mode controller
in the following two cases:
(a)
θ1d 5 2 π=3 1 ðπ=3Þ½1 2 cosðπt=ΤÞ
θ2d 5 2π=3 2 ðπ=3Þ 1 2 cosðπt=ΤÞ


using T 5 1; T 5 0:5, and T 5 2. What is the minimum sampling period T
required for the implementation of the controller?
(b) The desired trajectory is a straight line from the initial position ðx; yÞ 5 ð1; 0Þ to
the final position ðx; yÞ 5 ð0; 1:5Þ which must be traversed with constant velocity
in 2 s time. Initially, the robot is stationary at the position ðx; yÞ 5 ð1; 0Þ and the
“elbow-down” configuration.
670
Problems

Hint: When the derivatives of the desired trajectories are not directly available, we
generate qd by smoothing the reference trajectory using a low-pass second-order fil-
ter with bandwidth Ω. This filter gives directly _qd and €qd.
26. We are given a two-link robotic arm of Figure P.10 which is attached to the ceiling
and is under the influence of gravity. The reference coordinate frame is ðx0; y0; z0Þ.
The link masses are assumed to be lumped at their ends.
(a) Find the transformation matrices Ai21
i
; i 5 1; 2.
(b) Find the pseudo inverse matrix Ji of each link.
(c) Find the elements of the DðθÞ; hðθ; _θÞ, and cðθÞ matrices and write down the
Lagrange dynamic model of the robot.
(d) Formulate the computed torque control problem.
(e) Choose the values for the robot and the controller parameters and simulate the
system.
27. Consider a differential-drive WMR in the inertial coordinate frame (Figure P.11):
(a) Write the WMR’s dynamic model in polar coordinates ðρ; α; βÞ, where the error
is given by the relations ρ 5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðΔxÞ2 1 ðΔyÞ2
q
; α 5 2 φ 1 atan2ðΔy; ΔxÞ; and
β 5 2 φ 2 α.
(b) Show that the control law v 5 Kρρ; ω 5 Kαα 1 Kββ gives an exponentially
stable system when Kρ . 0; Kβ , 0; Kα 2 Kρ . 0.
Hint: For small x 5 α; β use the approximation cos x 5 1 and sin x 5 x. Determine
the values of Kρ; Kα, and Kβ which assure that the poles of the closed-loop charac-
teristic polynomial have negative real parts.
Δy
Δx
ρ
α
Φ
β
y
x
0
Figure P.11 Differential-drive robot in the inertial coordinate frame.
y0
y1
y2
x2
m2
m1
l2
l1
x0
x1
Θ1
Θ2
Figure P.10 A 2-linkrobot attached to the ceiling.
671
Problems

28. Formulate the control problem of a mobile manipulator, where the platform is an
Ackerman’s vehicle and the manipulator has two links. Write down the correspond-
ing equations, and simulate them with parameter values of your choice.
29. (a) Apply the sliding mode control to a differential-drive WMR with uncertainty in
the platform’s mass.
(b) Derive a discontinuous feedback control law with exponential convergence for
the WMR with slip studied in Section 3.3 using the dynamic model ((3.47a)
(3.47c)). Choose an appropriate wheel surface traction model (see Ref. [5] of
Chapter 3; Ward C, Iagnemma K. ICRA2007, p. 27249; Pota H, Katupitiya J,
Eaton R. CDC2007, p. 596601; Albagul A, Martono W, Muhida R. Cutting
edge robotics. In: Kordic V et al., editors. ARS/pIV, Germany, www.intercho-
pen.com/download/pdf/1. p. 784.
30. Formulate and solve the control problem of a four-wheel Mecanum vehicle in the
following cases:
(a) Lyapunov-based control
(b) Adaptive control
(c) Sliding mode control
E. Visual Servoing
31. (a) Describe the visual servo control following the typical classification of the
problem.
(b) Describe the camera calibration and the hand-eye (on-board) calibration
problems.
32. Describe the general visual servoing problem and a solution when the vision system
used is a central catadioptric mirror camera system.
33. (a) Describe and analyze the general pinhole camera model.
(b) Consider a pinhole camera fixed to the ceiling and assume that the camera plane
and WMR plane are parallel. Here, we have the world coordinate frame
Owxwywzw, the camera frame Ocxcyczc, and the image frame Oimximyim (which
is assumed identical with the camera plane xc 2 yc). Let C be the crossing point
of the optical axis of the camera and the xw 2 yw plane with coordinates ðxp; ypÞ
with respect to the xc 2 yc plane. Let ðx; yÞ be the coordinates of the center of
mass of the WMR (coinciding with the geometric center) and ðxm; ymÞ the coor-
dinates of ðx; yÞ relative to the image frame. Prove that the pinhole camera
model is:
xm
ym


5
k1
0
0
k2


R
x
y
 
2
xp
yp


	

1
Oc1
Oc2


where ðOc1; Oc2Þ are the coordinates of the original point of the camera with
respect to the image plane, k1; k2 are constants depending on the depth informa-
tion, focal length, and scaling factors along xim; yim axes, and
R 5
cos φ0
sin φ0
2sin φ0
cos φ0


with φ0 being the angle between the xm-axis and the xw-axis with a positive
anticlockwise orientation, assuming that the xw-axis, the x-axis, and the xm-axis
have the same orientation.
672
Problems

(c) Develop fully the general catadioptric camera model and express it in the form
xim 5 KfðxÞ
where xim 5 ½xim; yim; 1Τ is the projection of the 3D world space point
x 5 ½x; y; zΤ, fðxÞ 5 ð1=wÞ x; y; 1
½
Τ; w 5 z 1 ζ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
x2 1 y2 1 z2
p
; K is the triangular
calibration matrix of the catadioptric camera (containing the mirror-lens intrin-
sic parameters) and ζ is the mirror intrinsic parameter.
34. Define the camera robot system as a dynamic system:
xk11 5 Fðxk; ukÞ;
yk 5 HðxkÞ
where ykAYCRm is the set of possible output values, FðÞ describes the dynam-
ics, HðÞ is the output mapping, and ukAUCR6 contains the desired pose change of
the camera coordinate system.
35. In the above model, define the mapping of robot movements to image changes (for-
ward visual model) ϕkðuÞ 5 ϕðxk; uÞ 5 HðFðxk; uÞÞ, ϕ:X 3 U ! Y with X being
system state space.
36. Derive the inverse model of the above forward model. Since the mapping ϕ is too
complicated, describe the most popular method of linear approximation of
yk11 5 HðFðxk; ukÞÞ.
37. A more accurate approximation of the camera robot system model is to use a qua-
dratic approximation. Describe and develop this “quadratic model.”
38. Figure P.12 shows the structure of a point-to-point positioning system using a binoc-
ular camera.
Specifically, this positioning task may be one of the following: (i) to grasp an
object, (ii) to execute an insertion task, (iii) to place an object held by the gripper to
a pose defined in the image. In Figure P.12 the error function is defined as
el 5 fc
l 2 fd
l for the left image, and er 5 fc
r 2 fd
r for the right image. This error can be
driven to zero by estimating the image Jacobian via stacking the two monocular
image Jacobians defined for each of the two cameras. To control three translational
degrees of freedom of the robotic manipulator, it is sufficient to estimate the dis-
tance between two points in the image, that is, it is not needed an accurate estima-
tion of the transformation between the robot and the camera coordinate systems (XC
R
must only roughly known).Develop an image-based visual servoing control algo-
rithm for this task, assuming that an image-based tracking algorithm is available
(that estimates 2D image positions of feature points), and the desired positions fd
l
and fd
r are selected manually at the beginning of the servoing procedure.
xG
xC
fl
c
fl
d
fl
c
fl
d
Figure P.12 Robotic positioning via a binocular camera.
673
Problems

F. Fuzzy and Neural Methods
39. Explain in your own words what we mean by the term intelligent control, describing
its ingredients and primary architectures.
40. Describe briefly the three main components of computational intelligence.
41. Investigate using Matlab the membership function: μbigðxÞ 5 1=½1 1 ðx=F2Þ2F1; xAR
The parameters F1 and F2 are known as “exponential fuzzifier” and “denomina-
tor fuzzifier” respectively.
Hint: Plot μbigðxÞ in two cases:
(a) Constant F2 and varying F1 (e.g., F2 5 50; F1 5 1; 2; 4; 100; 100)
(b) Constant F1 and varying F2 (e.g., F1 5 4; F2 5 30; 40; 50; 60; 70)
42. We are given the fuzzy sets A and B, where A 5 {x greater than 15} and B 5 {x nearly
17} with membership functions:
μAðxÞ 5
1
1 1 ðx215Þ22 ; x . 15
0; x # 15
μBðxÞ 5
1
1 1 ðx217Þ4
8
>
<
>
:
(a) Find and draw the membership function of the fuzzy set C 5 (x greater than 15)
AND (x nearly 17)
(b) Similarly find and draw the membership function of D 5 (x greater than 15) OR
(x nearly 17)
(c) Finally draw the membership function of E 5 (x not greater than 15) AND
(x nearly 17)
43. We are given the fuzzy rule: “IF x is A THEN y is B” where:
A 5 0:33=6 1 0:67=7 1 1:00=8 1 0:67=9 1 0:33=10
B 5 0:33=1 1 0:67=2 1 1:00=3 1 0:67=4 1 0:33=5
If we know that “x is A0” with:
A0 5 0:5=5 1 1:00=6 1 0:5=7
Find the result B0 with the Mamdani rule.
44. Consider the bell-type membership function μAðxÞ 5 bellðx; 1:5; 2; 0:5Þ and the function:
fðxÞ 5
ðx21Þ2 2 1;
x $ 0
x;
x # 0
	
Find the membership function of the fuzzy set B 5 fðAÞ.
Hint: Use the relation fðAÞ 5 ΣμAðxÞ=fðxÞ, where Y is the superset Y of the mapping
y 5 fðxÞ; yAY.
45. Consider the rule base:
R1: IF x is low, THEN y is low
R2: IF x is medium, THEN y is medium
R3: IF x is high, THEN y is high
where the linguistic variables (fuzzy sets), low, medium, and high are defined as follows:
Variablex: X 5 f1; 2; 3; 4; 5g
Low 5 1=1 1 0:75=2 1 0:5=3 1 0:25=4 1 0=5
Medium 5 0:5=1 1 0:75=2 1 1=3 1 0:75=4 1 0:5=5
High 5 0=1 1 0:25=2 1 0:5=3 1 0:75=4 1 1=5
674
Problems

Variabley: Y 5 f6; 7; 8g
Low 5 1=6 1 0:6=7 1 0:3=8
Medium 5 0:6=6 1 1=7 1 0:6=8
High 5 0:3=6 1 0:6=7 1 1=8
Find the equivalent relational matrix R of this rule database using (i) Mamdani’s
rule and (ii) Zadeh’s rule.
46. In the above case, find the output:
B0 5 A03R
using the maxmin composition rule, where:
A0 5 “nearly high”
5 0:25=1 1 0:50=2 1 0:75=3 1 1=4 1 0:75=5
and express the result linguistically.
47. Modify the BP propagation of Section 14.4.3.3 such that to hold for the hyperbolic
tangent sigmoid function.
48. Prove the convergence of the BP algorithm.
49. Write the equations that describe the MLP shown in Figure P.13 in the matrix form
(14.9). Assume that all neurons (except the neurons of the first-input layer) have the
same activation function σðÞ and zero threshold value.
50. Describe the principal optimization algorithms that are based on the steepest descent
learning method. How can we improve their rate of convergence?
51. Apply the BP training algorithm to the NN shown in Figure P.14. Give in analytic
form the weight updating relations. Use the activation function:
σðzÞ 5 ½1 2 expðzÞ=½1 1 expðzÞ
y
x1
x2
Figure P.14 A simple NN.
x1
y1
y2
x2
x3
P1
P2
P3
P4
Figure P.13 An MLP with the same activation functions in the hidden and output layers.
675
Problems

52. Describe the basic algorithms for selecting the centers ci of the RBF basis functions.
When the centers ci have been selected, how can we select the parameters σi of the
RBFs?
53. Consider an RBF NN with p inputs, one output, and m RBF basis functions. The
basic functions are:
φðjjx 2 cijjÞ 5 1=½1 1 σiðx2ciÞ2
Give in detailed form the training equations for the parameters:
G
The weights wi of the output layer
G
The widths σi of the RBFs
G
The centers ci of the RBFs
using the steepest algorithm.
54. (a) Explain the universal approximation property of an NN with three layers.
(b) Examine the use of BP learning method, with a sigmoid function, of the follow-
ing forms:
(i) σðxÞ 5 1=x;
1 # x # 100
(ii) σðxÞ 5 log10x;
1 # x # 10
(iii) σðxÞ 5 expð2xÞ;
1 # x # 10
(iv) σðxÞ 5 sin x;
0 # x # π=2
In each case use two data sets: training data and verification data. Using the
training data and one hidden layer compute the synaptic weights. Evaluate the accu-
racy using the verification data.
55. (a) Construct an ANFIS two-rule two-input system Mamdani fuzzy system using
the maxmin composition and the COG deffuzification method. Describe fully
the operation of the layers of this neuro-fuzzy system.
(b) Derive the data using the Matlab file “tm-2in.m” and the Matlab program “tan-
mip.m” to learn this mapping. The NN must have about the same number of
parameters as in (a). Plot the results.
56. Design a fuzzy controller for the stabilization of the inverted pendulum of
Figure P.15.
The system parameters are: pendulum length L, pendulum bar mass m, vehicle
mass M.
57. Consider the system
_x1ðtÞj 5 x2ðtÞj; _x2ðtÞj 5 x3ðtÞj; . . .; _xnðtÞj 5 2 fðxðtÞjÞ 1 bðxðtÞjÞuðtÞj
Θ
L
υ
Mobile vehicle
Pendulum bar
Figure P.15 Inverted pendulum-cart system.
676
Problems

where xðtÞj 5 ½x1ðtÞj; x2ðtÞj; . . .; xnðtÞjΤ is the state vector, uðtÞj is the control input,
and fðxjÞ; bðxjÞ are unknown functions. The index j denotes the cycle (repetition)
number and tA½0; Τ. The problem is to control xjðtÞ such that to follow a desired
state xdðtÞ 5 ½xd; _xd; . . .; xðn21Þ
d
Τ in tA 0; Τ
½
. Assume that fðxÞ and bðxÞ are bounded,
xdðtÞ is measurable and bounded, ½ejðtÞ 5 ½xjðtÞ2xj
dðtÞt50 5 0 for each cycle for all
xjARn, and that the bound bL: 0 , bL , bðxjÞ is a positive constant.
Hint: Show that the controller uj has the form:
uj 5 Uj
m 2 sgnðsjÞ½1 1 1=bLjUj
mj
signðsjÞ 5
1;
sj . 0
0;
sj 5 0
21;
sj , 0
8
>
<
>
:
58. Apply the above controller through simulation to: (a) a two-link robot (with
l1 5 l2 5 1 m, m1 5 m2 5 1 kg), (b) a car-like WMR, and (c) to the MM consisting
of the above.
59. Derive an adaptive neuro-fuzzy controller with Gaussian functions for controlling
the differential-drive WMR shown in Figure P.16.
Hint: The world coordinate frame is Oxy. Using the symbols of Figure P.16, the
WMRs dynamic equations are:
Iυ €φ 5 Dl
r 2 Dl
l
M_υ 5 Dr 1 Dl
Iω €θ i 1 c_θ i 5 kui 2 rDi
ði 5 l; rÞ
r _θ r 5 υ 1 l_φ
r _θ l 5 υ 2 l_φ
which, defining the state vector x 5 ½υ; φ; _φΤ, the control vector u 5 ½ur; ulΤ, and
the output vector y 5 ½υ; φΤ, can be written in the standard state-space form:
_x 5 Ax 1 Bu; y 5 Cx.
y
x
φ
l
l
lv
Dl
Dr
Mυ
0
•
Figure P.16 Differential drive WMR for neuro-fuzzy control.
677
Problems

60. Derive a stable 2D visual servoing controller for MMs with planar robotic manipula-
tors using RBF NNs under the following conditions: (i) the gravity and friction
terms are unknown and (ii) there are modeling errors in the vision system.
Hint: Use RBFs with Gaussian base functions φðxÞ 5 e2ðx2cÞ2=bðb . 0Þ, and the stan-
dard PD control law:
u 5 2 Kpðq 2 qdÞ 2 Kd _q
assuming that the joint velocities _q are measurable. Express the NN in standard
way: y 5 WΤΦðxÞ. Add the neural term in the above control law which is not accu-
rate due to that gravity and friction are unknown, that is:
u 5 Kpðq 2 qdÞ 2 Kdð_q 2 _qdÞ 1 ^WΦð ^VsÞ
where ^W is the weight matrix produced after the training of the NN. Since the robot
is to be controlled by visual servoing the above PD control must be replaced by:
τ 5 JΤKpRΤ~xs 2 Kd _q 1 ^W
ΤΦðsÞ
where s 5 ½s0; qΤ; _qΤ
s0 is the input vector to the NN, s0 is the NN threshold, W is the
weight matrix with N hidden-layer neurons, and ~xsARn is the position error in the camera:
~xs 5 ahRðθÞ½fd 2 f; h 5 lf=ðlf 2 zÞ
Here, RðθÞ is the standard 2 3 2 rotation matrix, lf the focal length, 0a0 the scale
factor, and fðqÞ; fðqdÞ the image features (actual and desired). The term
^W
ΤΦðsÞ
approximates the gravity and the friction term BðsÞ 5 gðqÞ 1 Fυð_qÞ. Treat first the
case where the visual space can match the world frame exactly, and then the case
where the vision system cannot provide exactly the world frame. Prove that the con-
troller is asymptotically stable, and simulate the resulting close-loop system in the
Matlab using parameter values of your choice.
G. Planning
61. How many 8-bit bytes are necessary for storing a continuous path of 1 min duration
for a six degrees of freedom robot. Assume that for the storage of the position of
each joint we need 16-bit words and that the sampling period is 16 ms.
62. Assume that the polynomial
xðtÞ 5 a4t4 1 a3t3 1 a2t2 1 a1t 1 a0
describes the position of a mobile robot with respect to t in the time interval
½ 2 Τ; 1 Τ. If the necessary boundary conditions are:
xð2 ΤÞ 5 0; _xð2 ΤÞ 5 0; €xð2 ΤÞ 5 0
xðΤÞ 5 C; _xðΤÞ 5 C=Τ; €xðΤÞ 5 0
Show that the coefficients ai i 5 0; 1; 2; 3; 4
ð
Þ are given by:
a0 5 3C=16; a1 5 C=2Τ; a2 5 3C=8Τ2; a3 5 0; a4 5 2 C=16Τ4
678
Problems

63. Write a computer program for implementing the paths:
xðtÞ 5 a4t4 1 a3t3 1 a2t2 1 a1t 1 a0
yðtÞ 5 b4t4 1 b3t3 1 b2t2 1 b1t 1 b0
with the following boundary conditions:
For t 5 2 3: x 5 0; _x 5 0; €x 5 0; y 5 0; _y 5 0; €y 5 0
For t 51 3: x 5 10; _x 5 5; €x 5 0; y 5 16; _y 5 8; €y 5 0
Plot the results with point spacing Δt 5 0:2 s.
64. The motion planning technique described in Example 11.2 is called 2-1-2 because
the path is described by a second-order polynomial of time (constant acceleration),
followed by a first-order (linear) polynomial (constant velocity), and then again by a
second-order polynomial (constant deceleration). Develop the analogous methods:
(a) 4-1-4 where the first and last path sections are described by fourth-order
polynomials
(b) 4-3-4 where the second path section is described by a third-order polynomial.
65. A single link robot is to move from an initial angle θð0Þ 5 30 to a final angle
θ 2
ð Þ 5 100 in 2 s. The velocity and acceleration of the joint at the initial and final
positions are zero. Find the coefficients of a third-, fourth-, and fifth-order polyno-
mial that realize this motion.
66. (a) Define and illustrate graphically the concept of shortest-path roadmaps.
(b) Define and illustrate graphically the concept of maximum clearance roadmaps.
67. The exact cell decomposition illustrated in Figure 11.7 is known as vertical cell
decomposition, where the free configuration space is decomposed into a finite col-
lection of 2-cells (trapezoids that have vertical sides or triangles which are degener-
ate trapezoids) and 1-cells (vertical segments that are the borders between two
2-cells).
(a) Extend the above basic vertical cell decomposition to treat the case where
CSfree has two or more points that lie on the same vertical segments, without
using random perturbations.
(b) Apply the vertical decomposition method for obstacle boundaries which are
described as chains of circular arcs and line segments.
68. (a) For the obstacle environment and the initial-goal states shown in Figure P.17A
draw the vertical decomposition.
(b) For the obstacle-start-goal state situation of Figure P.17B illustrate the visibility
graph.
(c) Draw the shortest path for the cases shown in Figure P.18A and B. Explain your
answer.
(A)
(B)
qstart
qstart
qgoal
qgoal
Figure P.17 Three-obstacle environments with different start-goal positions.
679
Problems

69. Produce the Voronoi diagram of the office floor environment shown in Figure P.19,
and find two noncolliding paths from the start point to the goal point.
70. Write a computer program for drawing a Voronoi diagram through the so-called per-
pendicular bisectors. In this method, the Voronoi edge for two obstacles A and B is
a
perpendicular
bisector
of
the
segment
joining
the
two
obstacles
(see
Figure P.20A). If a third obstacle is added, then the perpendicular bisectors 1,2,
and 3 between all the pair points (A,B), (A,C), and (B,C) should be computed
(Figure P.20B).
(A)
Start
Start
Goal
Goal
(B)
Figure P.18 Two environments for drawing the shortest start-goal paths.
Start
Goal
Figure P.19 The map of an office floor.
Voronoi edge
1
1
2
3
1
2
3
Voronoi 
vertex
A
B
A
B
C
Figure P.20 (A) Bisection Voronoi edge (1) for two objects A and B, (B) bisections 1, 2,
and 3 of the objects A, B, and C, and (C) bisection-based Voronoi vertex (path) for three
objects.
680
Problems

71. We are given a U-shaped environment of the form shown in Figure P.21.
Derive the potential field equations and write a program that finds the respective
path for going from the initial point to the target point showing by arrows the attrac-
tive repulsive field around the U-shaped wall.
72. Four significant drawbacks that are inherent to the potential field method (indepen-
dently of the particular implementation) are the following:
(a) Trap situations due to local minima
(b) No passage between closely spaced obstacles
(c) Oscillations in the presence of obstacles
(d) Oscillations in narrow passages
Describe in your own way each of the above situations and justify their
possibility.
73. Define the concepts of path planning, motion planning, and task planning and
describe their similarities and differences.
74. Give a short description of the robot task planning in terms of the AI problem-
solving methodology, that is, problem representation, problem reduction, and prob-
lem solution (solution space search methods).
75. Solve the well-known “cannibals” problem which is the following: Three missionar-
ies and three cannibals want to cross a river from the right bank to the left bank by
boat. The maximum capacity of the boat is two persons. If the missionaries are out-
numbered at any time by the cannibals, the cannibals will eat the missionaries. Find
a solution for the safe crossing of all six persons.
Hint: The state of the system is ðNmiss; NcannibÞ where Nmiss and Ncannib is the number
of missionaries and cannibals to the left bank respectively. The possible intermediate
states are (0,1), (0,2), (0,3), (1,1), (2,2), (3,0), (3,1), and (3,2).
76. Describe the three steps of robot task planning, namely: world modeling, task speci-
fication, and program synthesis. The three principal types of 3D object representa-
tion schemes are: (i) boundary representation, (ii) sweep representation, and
(iii) volumetric representation. Provide a short description of the volumetric repre-
sentation, including the constructive solid geometry (using operations on primitive
shapes and blocks).
77. We are given the objects as shown in Figure P.22, which is described as:
Put object face 1 (S1 against S3) and (S2 against S4). It is desired to find a set of
relations that constrain the configuration of object 1 in relation to the known config-
uration of object 2.
78. Find a task plan for placing four protectors at the four nonadjacent edges of an
object (product) which is to be packed into the box. The protectors are required for
keeping the object (e.g., a fragile object) into the box. The initial state of the config-
uration is an empty box (Figure P.23A), and the goal state is shown in Figure P.23C
with intermediate state as shown in Figure P.23B.
Initial position
Target position
Figure P.21 A U-shaped obstacle.
681
Problems

79. Apply the method of triangle table to generate a task plan for transferring a box B1
using a WMR into a nearby room as shown in Figure P.24 nearby (the robot is ini-
tially in room R1, the box is in room R2, and the box should be transferred in
room R3). The robot has available two motion operators:
GOTHRU—Movement from the room R1 to the room R2 through the respective door, etc.
PUSHTHRU—The robot pushes the box B1 via the door D2 from room R2 to room R3
The initial database is:
INROOM (ROBOT, R1)
CONNECTS (D1, R1, R2)
S1
S2
S3
S4
Object 1
Object 2
Figure P.22 A 2-Object world.
A4
A1
A2
A3
A5
A6
A7
A8
Protector 1
Protector 2
Protector 3
Protector 4
Face 1
Face 2
Face 1
Face 3
Face 2
Face 3
Product
Face 1
Face 3
Face 2
Empty 
box
Product
Protector 2
Protector 1
Protector 3
Protector 4
Protector 1
Protector 2
Protector 3
Protector 4
Face 1
Face 2
Face 1
A2
(A)
(B)
(c)
Figure P.23 (A) Initial configuration (empty box), (B) protectors placed on the product, and
(C) protectors placed in the box.
682
Problems

CONNECTS (D2, R2, R3)
BOX (B1)
INROOM (B1, R2)
The rule is:
ð ’ x; ’ y; ’ zÞ½CONNECTSðXÞðYÞðZÞ ! CONNECTSðXÞðYÞðZÞ
The goal state G0 is:
ð'XÞ½BOXðXÞXINROOMðX; R2Þ
80. Solve problem 79 using AND/OR graphs.
H. Localization and Mapping
81. Describe the following localization methods: (i) odometry, (ii) inertial navigation,
(iii) active beacons, (iv) landmark recognition (artificial, natural), and (v) model
matching.
82. Describe some ways of measuring and reducing nonsystematic odometry errors.
83. Describe how the global positioning system operates and give an example.
84. Describe a method for probabilistic localization.
85. Describe in your own words the boundary-following algorithm.
86. Describe a method for sensor array calibration via tracking with the EKF.
87. Formulate two ways of SLAM based on probabilities.
88. Formulate a method for localizing a WMR in an environment with landmarks.
89. Formulate the particle filter (sequential Monte Carlo) method for SLAM.
90. When the environment of a WMR involves moving objects we need to solve, in
addition to the SLAM problem, the detection and tracking problem (DTM) of these
dynamic objects. Derive the Bayesian formula for the combined SLAMDTM
problem. Propose a practical algorithm for performing DTM from a moving plat-
form equipped with range sensors.
91. Show how omnidirectional images, provided by a single catadioptric camera, can
be used to generate the representations required for topological navigation and
visual path following. It is recalled that topological navigation is based on the
robot’s global position, estimated by a set of omnidirectional images obtained dur-
ing the training stage.
92. Develop a method for integrating a catadioptric camera model with: (a) the
extended Kalman filter SLAM and (b) the particle filter SLAM.
Door D1
Door D2
Room R1
Room R2
Room R3
Robot
Figure P.24 A 3-room environment with two doors.
683
Problems

I. Affine Systems and Invariant Manifolds
93. The transformation y 5 Ax 1 b between two finite dimensional spaces (affine
transformation or affine map or an affinity) consists of a linear transformation A
followed by a translation b. Geometrically, an affine transformation in Euclidean
space is a transformation that possesses the following properties:
G
Colinearity preservation: Three points that lie on a line continue to be colinear
after the transformation.
G
Distance ratio preservation: The distance ratio ðP2P1Þ=ðP3P2Þ of three colinear
points P1; P2, and P3 is preserved.
In the 1D case A and b are the well-known slope and intercept of the plot
y 5 Ax 1 b. The matrix A represents a rotation (or shear) and b a “shift” (transla-
tion). So, homogeneous transformations are affine transformations:
y
1
=
A
b
0
1
x
...
...
1
,
T =
A
b
0
1
(a) Prove that the following propositions are equivalent:
(i) A 2 I is invertible.
(ii) A does not have the number 1 as one of its eigenvalues.
(iii) For all b the transformation has exactly one fixed point.
(iv) There exists a b for which the transformation has exactly one fixed point.
(v) Affine transformations with matrix A can be written as a linear transfor-
mation with some point as origin.
(b) Show that affine transformations in 2D, where A has an eigenvalue 1 (i.e., a
2D affine transformation without fixed point) is a pure translation. Determine
when an affine transformation is invertible.
94. For the following nonlinear control system:
_x1 5 3x1 1 x2
2 2 2x2 1 u;
_x2 5 3 sin x1 2 x2 2 u
do the following: (a) For u 5 0 plot the state-space trajectories in the neighborhood
of the origin which is defined by jx1j # 1 and jx2j # 2. (b) Linearize the system
around the point x1 5 0; x2 5 0, and u 5 0, and show that the linearized system is
unstable. (c) Design a linear state feedback controller for this system such that the
closed-loop system has the specifications ζ 5 0:5 and ωn 5 3. (d) Apply this con-
troller to the original system and draw the trajectories of the resulting closed-loop
system in state space.
95. A controller that leads the system _x1 5 x1 1 u1;
_x2 5 x2 1 u2 to the origin
ðx1; x2Þ 5 ð0; 0Þ is ui 5
2 yi
for
jyij # 5
2 5sgn yi
ð Þ
otherwise
	
with yi 5 5xi.
(a) Determine analytically the region of attraction of the system by studying the
derivative _V of the Lyapunov function: V 5 ð1=2Þðx2
1 1 x2
2Þ
(b) Determine the real region of attraction.
96. For the system: _x1 5 sin x2; _x2 5 x4
1 cos x2 1 u: (a) Design a controller for which the
system can track the arbitrary trajectory xd1ðtÞ under the assumption that the state
½x1ðtÞ; x2ðtÞΤ is measured exactly, the signals xd1ðtÞ; _xd1ðtÞ and €xd1ðtÞ are all known,
684
Problems

and the system does not involve any uncertainty, and (b) Verify using Matlab the
ability of the closed-loop system to track the desired trajectory.
97. Find a controller υðxÞ that stabilizes robustly the system:
_x1 5 x2 1 d1ðx; tÞ; _x2 5 z 1 d2ðx; tÞ; _z 5 υ 1 d3ðx; tÞ when the disturbances are
bounded: jd1j # ρ1ðx1Þ; jd2j # ρ2ðx1; x2Þ; jd3j # ρ3ðx1; x2; x3Þ. Use the Lyapunov-
based robust control method.
98. (a) Determine whether the system
_x 5 fðxÞ 1 bu;
x 5 ½x1; x2; x3Τ
b 5 ½1; 0; 1Τ;
fðxÞ 5 ½x21x2
21x2
3; x31sinðx12x3Þ; x2
3Τ
is input-state linearizable, and (b) can the variables
z1 5 x1 2 x3; z2 5 x2 1 x2
2
z3 5 x3 1 sinðx1 2 x3Þ 1 2x2½x3 1 sinðx1 2 x3Þ
be used as linearization state variables?
99. Examine the controllability of the differential drive, tricycle, car-like, and three- or
four-wheel omnidirectional mobile robots using their affine models.
100. (a) Develop a steering method for a car-like WMR that provides smooth paths under
curvature constraints and integrate it with a global motion planning technique for
obstacle avoidance. (b) Develop a robust controller for a car-like robot using the
“virtual” vehicle approach. Consider the case of existing errors and disturbances.
101. Develop a tracking controller with collision avoidance for a group of unicycle-type
WMRs. Use a supervisory system that assigns to each robot its reference path,
together with the desired velocity profiles of a function of the position along the path.
102. Develop an asymptotically stable control scheme for simultaneous position and tor-
que tracking using the backstepping technique including the actuator dynamics.
103. Consider
the
m-input
affine
system
_x 5 fðxÞ 1 Σm
i51giðxÞui
where
xARn;
u 5 ½u1; u2; . . .; umΤARm, and the nonlinear static state feedback control law:
u 5 aðxÞ 1 βðxÞυðtÞ;
υARm0;
m0 , mu
We know that the above system is nonregular (static state) feedback linearizable
if there exists a discontinuous state transformation z 5 ΦðxÞ; zARn and a state feed-
back of the above form, such that the transformed system with state z and input υ
is a controllable system. Show via the Frobenius theorem that the two-input drift-
less system: _x 5 g1ðxÞu1 1 g2ðxÞu2 is nonregular feedback linearizable if the nested
distributions defined by:
Δi 5 spanfg2g;
Δi 5 Δi21 1 adg1Δi21;
i 5 1; 2; . . .; n 2 2
and Δn21 5 Δn22 1 spanfg1g have the following properties:
G
Δi is involutive and has constant rank for i 5 0; 1; :::; n 2 1
G
rank Δn21 5 n
Apply this result to the nonholonomic chained system:
_x1 5 u1; _x2 5 u2; _x3 5 x2u1; . . .; _xn 5 xn21u1
685
Problems

104. Consider the chained WMR kinematic model:
_x1 5 u1; _x2 5 u2; _x3 5 x2u1
with output z 5 hðxÞ 5 x1, and apply the nonsmooth transformation φðxÞ 5 x1=3
3 . Use
the result of problem 103 to derive the discontinuous state and input transforma-
tions that converts the system into the canonical form starting with the transforma-
tion u1 5 x1=3
3 . Prove that the resulting linear single-input canonical system is:
_z1 5 z2; _z2 5 z3; _z3 5 υ
and find the feedback control law u1 5 u1ðxÞ; u2 5 u2ðxÞ which leads to a closed-
loop system with eigenvalues 2λ1; 2 λ2; 2 λ3
ð0 , λ1 , λ2 , λ3Þ, also giving
the stability conditions.
105. Consider the chained WMR model:
_x1 5 u1; _x2 5 u2; _x3 5 x1u2; υ 5 u1 1 x3u2; ω 5 u2
Determine the conditions, via the invariant and attractive manifold technique,
under which the control law:
u1 5 2 k1x1 1 k2
sðxÞ
x2
1 1 x2
2
x2; u2 5 2 k1x2 2 k2
sðxÞ
x2
1 1 x2
2
with sðxÞ 5 x3 2 x1x2=2; stabilizes the system to the origin for any initial conditions
x1 6¼ 0; x2 6¼ 0 if k1 . 0; k2 . 0. Furthermore, show that the control inputs u1 and u2
are bounded along the trajectories of the closed-loop system under the condition:
k2 . 2k1 . 0.
106. Consider the ðn; 2Þ chained system:
_x1 5 u1; _x2 5 u2; _x3 5 x2u1; . . .; _xn 5 xn21u1
which is completely controllable but not asymptotically stabilizable by smooth (or
even continuous) static or dynamic feedback controls (Brockett’s theorem). Find
under what conditions the control law:
u1 5 2 x1
u2 5 k2x2 1 k3
x3
x1
0
@
1
A 1 k4
x4
x2
1
0
@
1
A 1 ? 1 kn
xn
xn22
1
0
@
1
A
with ½xi=xi22
1
ð0;0Þ 5 0, leads to a closed-loop system that possesses a unique forward
solution for any initial condition xð0Þ such that x1ð0Þ 6¼ 0.
Hint: Use matrix A where:
A 5
k2
k3
k4
k5
. . .
kn21
kn
21
1
0
0
. . .
0
0
0
21
2
0
. . .
0
0
0
0
21
3
. . .
0
0
0
0
0
0
. . .
21
n 2 2
2
66664
3
77775
686
Problems

107. (a) For the above ðn; 2Þ-chained system find the conditions under which we have
_s 5 2 bs; b . 0, where x1ð0Þ 6¼ 0, and s 5 x2 1 a1
x3
x1 1 a2
x4
x2
1 1 . . . 1 an22
xn
xn22
1 .
This means that the manifold s 5 0 is invariant and attractive (exponentially).
(b) For the same system find the conditions under which the feedback controller:
u1 5
u1AðxÞ 5 2 x1;
jsj # μ
u1BðxÞ 5 sgnðx1Þ;
jsj . μ
	
u2 5
u2AðxÞ 5 k2x2 1 k3
x3
x1
1 ? 1 kn
xn
xn22
1
;
jsj # μ
u2BðxÞ 5 2 λx2;
jsj . μ
8
<
:
with λ . 0; μ . 0; and sgnðx1Þ 5
1; x1 $ 0
21; x1 , 0
	
assures a unique forward solution, for any initial condition x 0
ð Þ, which con-
verges exponentially to zero.
Note: A solution approach to problems 106 and 107 is provided by Astolfi A. and
Valtolina. Discontinuous control of nonholonomic system. Systems Control Lett
1996;27:3745; Local robust regulation of chained systems. Systems Control Lett
2003;49:2318; and Global regulation and local robust stabilization of chained
systems. In: Proceedings of the IEEE CDC, Sydney, Australia; December 1215,
2000. p. 163742.
687
Problems

Robotics Web Sites
1. General Robotics
1. www.robocommunity.com/article/10050/List-of-Other-Robot-Websites
2. www.robotstxt.org
3. www.dprg.org/robolinks.html
4. www.zerorobotics.org/web/zero-robotics/home-public
5. www.nasa.gov/audience/foreducators/robotics/home/index.html
6. www.msdn.microsoft.com/en-us/robotics/aa731517
7. http://researchguides.library.tufts.edu/content.php?pid5127295&sid51118912
8. www.jafsoft.com/searchengines/webbots.html
9. http://www.seattlerobotics.org/
10. www.topsite.com/best/robotics
11. www.roboteers.com
12. www.cimwareukandusa.com/aRobotAdam.html
13. www.ryerson.ca/aferwon/courses/CPS607/CLASSES/CPS607CL.HTML
14. http://www.eventscope.org/es/index.shtml
15. www.docstoc.com/docs/35855596/Agrobots—Robots-in-Agriculture
16. www.universal-robots.com
17. www.robotics.org
18. www.rethinkrobotics.com
19. www.densorobotics.com/world
20. www.therobotreport.com/index.php/industrial_robots
2. Mobile Robotics
1. www.mrpt.org
2. www.mobosoft.com
3. www.ccsrobotics.com
4. http://mobots.epfl.ch/self-assembling-robots.html
5. www.davidbuckley.net/DB/HistoryMakers.htm
6. www.surveyor.com/SRV_info.html
7. www.wn.com/khepera_mobile_robot
8. www.k-team.com
9. www.arrickrobotics.com/arobot
10. www.hobbyengineering.com/H1937.html
11. https://researchspace.auckland.ac.nz/handle/2292/2725
12. www.robots.net/rcfaq.html
 All web sites provided in the book were valid on 20 AUGUST 2013. Perhaps, some of them may be
changed or removed by their creators at a later time.

13. www.automation.com/content/rmt-robotics-announces-audio-feature-for-its-adam-
mobile-robot
14. www.mobilerobot.ru
15. www.automation.hut.fi
16. www.roboticsclub.org/links.html
17. www.roboticsbusinessreview.com/rbr50/category
18. www.cresis.ku.edu/sites/default/files/TechRpt101.pdf
19. www.computerworld.com/s/article/9027523/Mobile_robots_aren_t_science_fiction_
anymore
20. www.automation.com/product-showcase/rmt-robotics-makes-adam-mobile-robots-vocal
21. www.ri.cmu.edu/pub_files/pub1/simmons_reid_1999_1/simmons_reid_1999_1.pdf
22. www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA433772
23. www.faculty.cooper.edu/mar/mar.htm
24. www.robots.net/rcfaq.html
25. www.arrickrobotics.com/arobot
3. Thirty Five Mobile Robot Companies
1. www.autopenhosting.org/robots/companies.html
2. http://stason.org/TULARC/science-engineering/robotics/35-Mobile-Robot-
Companies.html#.Ufn2KNLTw2c
3. www.hotstockchat.com/mobile-robot-companies-geckosystems-us-and-zmp-japan-
sign-mou/
4. www.k-team.com/kteam/index.php?rub53&site51&version5EN&page53
5. www.mesa-robotics.com
6. http://robotics.sandia.gov/Roboticvehicles.html
7. http://www.barrett.com/robot/products/hand/handfrom.htm
8. www.aai.ca/robots
9. www.wifibot.com
10. www.esit.com/mobile-robots/
11. www.themachinelab.com/
12. www.ise.bc.ca/robotics.html
13. www.alibaba.com
14. www.motoman.com
15. www.seegrid.com
16. www.robotics.nasa.gov/links/industry.html
17. www.bastiansolutions.com
18. www.directindustry.com
19. www.wanyrobotics.com
20. www.botsinc.com/list-of-robot-companies
21. www.cyperbotics.com
22. www.personalrobots.com
23. www.robots.com
24. www.reisrobotics.com
25. www.rotundus.se
26. www.robotshop.com
27. www.pedsco.com
28. www.floorbotics.com
29. www.recce-robotics.com
690
Robotics Web Sites

30. www.robosoft.com
31. http://gizmodo.com/5966895/mitsubishis-remote-control-tankbot-is-yet-another-mem-
ber-of-the-robot-clean1up-crew-army
32. www.irobot.com
33. www.destaco.com
34. www.mobilerobots.com
35. www.geckosystems.com
A Sample of Commercial Robots
No.
ROBOT
NAME
COMPANY
DIMENSIONS
WEB SITE
(See Mobile
Robot
Companies)
1.
Matilda III
MESA
ROBOTICS. . .
30 in 3 21 in 3 40 in
[5]
2.
Ratler
Marvin
SANDIA
[6]
3.
M2GAIA
Applied AI Systems
14.2 cm 3 73 cm 3 48 cm
[8]
4.
MR-2,MR-7
Engineering Services
[10]
5.
Rotundus
ROTUNDUS
60 cm 3 90 cm 3 80 cm
[25]
6.
Easy-Roller
ROBOTSHOP
[26]
7.
RMI-9XD
PEDSCO
[27]
8.
Micro
VGTV
RECCE ROBOTICS
22 cm 3 43 cm 3 10 cm
[29]
9.
Robu-
CAR-TT
ROBOSOFT
[30]
10.
TankBot
MITSUBISHI
[31]
11.
ATRVJr
i-RobotCorporation
75.5 cm 3 49 cm 3 55 cm
[32]
12.
PackBot
DESTACO
[33]
13.
SeekurJr
Adept Mobile
Robots
119.8 cm 3 83 cm 3 50 cm
[34]
14.
PioneerLX
Adept Mobile
Robotics
69.7 cm 3 43.7 cm 3 44.8 cm
[34]
15.
CareBot
Gecko Systems
[35]
16.
AmigoBot
Adept Mobile
Robotics
33 cm 3 29 cm 3 15 cm
[34]
17.
Pioneer P3-
DX
Adept Mobile
Robotics
45.5 cm 3 38.1 cm 3 23.7 cm
[34]
18.
GP8 Pallet
Track
SEEGRID
91in 3 36in 3 90.75in
[15]
19.
PeopleBot
Adept Mobile
Robotics
41 cm 3 49 cm 3 112 cm
[34]
20.
AQUA2
Adept Mobile
Robotics
63.8 cm 3 46.7 cm 3 12.7 cm
[34]
691
Robotics Web Sites


Essays on Coding Theory
Critical coding techniques have developed over the past few decades for data storage, 
retrieval and transmission systems, significantly mitigating costs for governments and 
corporations that maintain server systems containing large amounts of data. This book 
surveys the basic ideas of these coding techniques, which tend not to be covered in the 
graduate curricula, including pointers to further reading. Written in an informal style, 
it avoids detailed coverage of proofs, making it an ideal refresher or brief introduction 
for students and researchers in academia and industry who may not have the time to 
commit to understanding them deeply. Topics covered include fountain codes designed 
for large file downloads; LDPC and polar codes for error correction; network, 
rank-metric and subspace codes for the transmission of data through networks; 
post-quantum computing; and quantum error correction. Readers are assumed to have 
taken basic courses on algebraic coding and information theory.
Ian F. Blake is Honorary Professor in the Department of Electrical and 
Computer Engineering at the University of British Columbia, Vancouver. He is a 
fellow of the Royal Society of Canada, the Institute for Combinatorics and its 
Applications, the Canadian Academy of Engineers and a Life Fellow of the IEEE. In 
2000, he was awarded an IEEE Millennium Medal. He received his undergraduate 
degree at Queen’s University, Canada and doctorate degree at Princeton University in 
1967. He also worked in industry, spending sabbatical leaves with IBM and M/A-Com 
Linkabit, and working with the Hewlett-Packard Labs from 1996 to 1999. His research 
interests include cryptograph and algebraic coding theory, and he has written several 
books in these areas.
Published online by Cambridge University Press

“This book is an essential resource for graduate students, researchers and professionals 
delving into contemporary topics in coding theory not always covered in textbooks. 
Each expertly-crafted essay offers a clear explanation of the fundamental concepts, 
summarizing key results with a consistent notation and providing valuable references 
for further exploration.”
Frank R. Kschischang, University of Toronto
“This volume lives up to its title: it explores many modern research directions in coding 
theory without always insisting on complete proofs. Professor Blake nevertheless 
manages to explain not only the results themselves, but also why things work the way 
they do. This volume will be a wonderful supplement to in-depth presentations of the 
topics that it covers.”
Alexander Barg, University of Maryland
“This book provides an excellent and comprehensive presentation of 16 major topics 
in coding theory. The author brings the highly mathematical subjects down to a level 
that can be understood with basic knowledge in combinatorial mathematics, modern 
algebra, and coding and information theory. It can be used as a textbook for a graduate 
course in electrical engineering, computer sciences and applied mathematics. It is also 
an invaluable reference for researchers and practitioners in the areas of communications 
and computer sciences.”
Shu Lin, retired from University of California, Davis
“This very unique contribution by Professor Blake consists of a collection of essays on 
coding theory that can be read independently and yet are coherently written. It covers a 
comprehensive list of topics of interest and is an excellent reference for anyone who is 
not an expert on all of these topics.”
Raymond W. Yeung, The Chinese University of Hong Kong
Published online by Cambridge University Press

Essays on Coding Theory
Ian F. Blake
University of British Columbia
gH Cambridge
UNIVERSITY PRESS
Published online by Cambridge University Press

MS CAMBRIDGE
WW UNIVERSITY PRESS
Shaftesbury Road, Cambridge CB2 8EA, United Kingdom
One Liberty Plaza, 20th Floor, New York, NY 10006, USA
477 Williamstown Road, Port Melbourne, VIC 3207, Australia
314-321, 3rd Floor, Plot 3, Splendor Forum, Jasola District Centre, New Delhi - 110025, India
103 Penang Road, #05-06/07, Visioncrest Commercial, Singapore 238467
Cambridge University Press is part of Cambridge University Press & Assessment, 
a department of the University of Cambridge.
We share the University’s mission to contribute to society through the pursuit of 
education, learning and research at the highest international levels of excellence.
www.cambridge.org
Information on this title: www.cambridge.org/9781009283373
DOI: 10.1017/9781009283403
© Ian F. Blake 2024
This publication is in copyright. Subject to statutory exception 
and to the provisions of relevant collective licensing agreements, 
no reproduction of any part may take place without the written 
permission of Cambridge University Press & Assessment.
First published 2024
A catalogue record for this publication is available from the British Library
A Cataloging-in-Publication data record for this book is available from the Library of Congress
ISBN 978-1-009-28337-3 Hardback
Cambridge University Press & Assessment has no responsibility for the persistence or 
accuracy of URLs for external or third-party internet websites referred to in this publication 
and does not guarantee that any content on such websites is, or will remain, 
accurate or appropriate.
Published online by Cambridge University Press

To Betty, always
Published online by Cambridge University Press

Published online by Cambridge University Press

Contents
Preface 
page xi
1
1.1
1.2
1.3
Introduction 
1
Notes on Finite Fields and Coding Theory 
1
Notes on Information Theory 
16
An Overview of the Chapters 
21
2
2.1
2.2
2.3
Coding for Erasures and Fountain Codes 
26
Preliminaries 
27
Tornado Codes and Capacity-Achieving Sequences 
30
LT and Raptor Codes 
44
3
3.1
3.2
3.3
Low-Density Parity-Check Codes 
66
Gallager Decoding Algorithms A and B for the BSC 
71
Performance of LDPC Codes on the BIAWGN Channel 
77
Thresholds, Concentration, Gaussian Approximation, EXIT
Charts 
87
4
4.1
4.2
4.3
Polar Codes 
97
Preliminaries and Notation 
98
Polar Code Construction 
102
Subchannel Polarization and Successive Cancellation Decoding 111
5
5.1
5.2
5.3
Network Codes 
128
Network Flows 
129
Network Coding 
135
Construction and Performance of Network Codes 
142
6
6.1
Coding for Distributed Storage 
157
Performance Limits on Coding for Distributed Storage 
158
vii
Published online by Cambridge University Press

viii
Contents
6.2
6.3
Regenerating Codes for Distributed Storage and Subpacketization 
Array-Type Constructions of Regenerating Codes
167
174
7
Locally Repairable Codes
182
7.1
Locally Repairable Codes
182
7.2
Maximally Recoverable Codes
193
7.3
Other Types of Locally Repairable Codes
195
8
Locally Decodable Codes
206
8.1
Locally Decodable Codes
207
8.2
Coding for Local Decodability
211
8.3
Yekhanin’s 3-Query LDCs
223
9
Private Information Retrieval
230
9.1
The Single-Server Case
233
9.2
The Multiple-Server Case
237
9.3
Coding for PIR Storage
241
10
Batch Codes
255
10.1 Batch Codes
255
10.2 Combinatorial Batch Codes
265
10.3 The Relationship of Batch Codes to LDCs and PIR Codes
271
11
Expander Codes
275
11.1
Graphs, Eigenvalues and Expansion
275
11.2 Tanner Codes
283
11.3 Expander Graphs and Their Codes
290
12
Rank-Metric and Subspace Codes
298
12.1 Basic Properties of Rank-Metric Codes
299
12.2 Constructions of MRD Rank-Metric Codes
309
12.3 Subspace Codes
313
13
List Decoding
327
13.1 Combinatorics of List Decoding
331
13.2 The Sudan and Guruswami-Sudan Algorithms for RS Codes
334
13.3 On the Construction of Capacity-Achieving Codes
346
14
Sequence Sets with Low Correlation
361
14.1 Maximum-Length Feedback Shift Register Sequences
361
14.2 Correlation of Sequences and the Welch Bound
368
14.3 Gold and Kasami Sequences
375
15
Postquantum Cryptography
380
15.1 Classical Public-Key Cryptography
382
Published online by Cambridge University Press

Contents
ix
15. 2 Quantum Computation 
386
15. 3 Postquantum Cryptography 
394
16 
Quantum Error-Correcting Codes 
404
16.1 General Properties of Quantum Error-Correcting Codes 
405
16.2 The Standard Three-, Five- and Nine-Qubit Codes 
408
16.3 CSS, Stabilizer and F4 Codes 
413
17 
Other Types of Coding 
424
17.1 Snake-in-the-Box, Balanced and WOM Codes 
424
17.2 Codes for the Gaussian Channel and Permutation Codes 
429
17.3 IPP, Frameproof and Constrained Codes 
434
Appendix A Finite Geometries, Linearized Polynomials and
Gaussian Coefficients 
443
Appendix B 
Hasse Derivatives and Zeros of Multivariate
Polynomials 
450
Index 
454
Published online by Cambridge University Press

Published online by Cambridge University Press

Preface
The subject of algebraic coding theory arose in response to Shannon’s remark­
able work on information theory and the notion of capacity of communication 
channels, which showed how the structured introduction of redundancy into 
a message can be used to improve the error performance on the channel. The 
first example of an error-correcting code was a Hamming code in Shannon’s 
1948 paper. This was followed by the Peterson book on error-correcting codes 
in 1961. The books of Berlekamp in 1968 and the joint volume of Peterson 
and Weldon in 1972 significantly expanded access to the developing subject. 
An impressive feature of these books was a beautiful treatment of the theory of 
finite fields - at a time when most engineers had virtually no training in such 
algebraic concepts.
Coding theory developed through the 1960s, although there were concerns 
that the encoding and decoding algorithms were of such complexity that 
their use in practice might be limited, given the state of electronic circuits 
at that time. This was dispelled during the 1970s with the increasing capa­
bilities of microelectronics. They are now included in many communications 
and storage applications and standards and form a critical part of such 
systems.
Coding theory has expanded beyond the original algebraic coding theory 
with very significant achievements in systems, such as LDPC coding, polar 
coding and fountain codes, and these systems are capable of achieving capacity 
on their respective channels that have been incorporated into numerous 
standards and applications. It has also embraced new avenues of interest 
such as locally decodable codes, network codes, list decoding and codes for 
distributed storage, among many others.
While topics such as LDPC coding and polar coding are of great impor­
tance, only a few graduate departments will be able to devote entire courses 
to them or even partially cover them in more general courses. Given the 
xi
https://doi.org/10.1017/9781009283403.001 Published online by Cambridge University Press

xii
Preface
pressure departments face, many of the other topics considered here may not 
fare so well.
The idea of this book was to create a series of presentations of modest 
length and depth in these topics to facilitate access to them by graduate 
students and researchers who may have an interest in them but defer from 
making the commitment of time and effort for a deeper understanding. Each 
chapter is designed to acquaint the reader with an introduction to the main 
results and possibilities without many of the proofs and details. They can 
be read independently and a prerequisite is a basic course on algebraic 
coding and information theory, although some of the topics present technical 
challenges.
There are as many reasons not to write such a book as to write it. A few of 
the areas have either excellent monographs or tutorials available on the web. 
Also, it might be argued that an edited book on these topics with chapters 
written by acknowledged experts would be of more value. Indeed, such a 
volume is A Concise Encyclopedia of Coding Theory, W.C. Huffman, J.-L. 
Kim and P. Sole, eds., 2021, CRC Press. However, the entries in such a volume, 
as excellent as they usually are, are often of an advanced nature, designed for 
researchers in the area to bring them abreast of current research directions. It 
was felt that a series of chapters, written from a fairly consistent point of view 
and designed to introduce readers to the areas covered, rather than provide 
a deep coverage, might be of interest. I hope some readers of the volume will 
agree. For many of the areas covered, the influence of the seminal papers on the 
subjects is impressive. The attempt here is to explain and put into context these 
important works, but for a serious researcher in an area, it does not replace the 
need to read the original papers.
Choosing the level of the presentation was an interesting challenge. On 
the one hand it was desired to achieve as good an appreciation of the results 
and implications of an area as possible. The emphasis is on describing and 
explaining contributions rather than proving and deriving, as well as providing 
a few examples drawn mainly from the literature. On the other hand the 
inclusion of too much detail and depth might discourage reading altogether. 
It is hoped the compromise reached is satisfactory. While efforts were made to 
render readable accounts for the topics, many readers might still find some of 
the topics difficult.
Another problem was to choose a consistent notation when describing 
results from different authors. Since one of the goals of the work was to provide 
an entree to the main papers of an area, an effort was made to use the notation 
of the seminal works. Across the chapters, compromises in notation had to be 
made and the hope is that these were reasonable. Generally there was a bias 
https://doi.org/10.1017/9781009283403.001 Published online by Cambridge University Press

Preface
xiii
toward describing code construction techniques for the areas which tended to 
make some sections technically challenging.
I would like to thank the many colleagues around the world who provided 
helpful and useful comments on many parts of the manuscript. First among 
these are Shu Lin and Frank Kschischang, who read virtually all of the work 
and consistently supported the effort. I cannot thank them enough for their 
comments and suggestions. I would also like to thank Raymond Yeung, Vijay 
Kumar, Eitan Yaakobi, Rob Calderbank, Amir Tasbihi and Lele Wang, who 
read several of the chapters and provided expert guidance on many issues.
I would also like to thank the Department of Electrical and Computer 
Engineering at the University of British Columbia and my colleagues there, 
Vijay Bhargava, Lutz Lampe and Lele Wang, for providing such a hospitable 
environment in which to pursue this work.
Finally, I would like to thank my wife Betty without whose love, patience 
and understanding this book would not have been written.
https://doi.org/10.1017/9781009283403.001 Published online by Cambridge University Press

https://doi.org/10.1017/9781009283403.001 Published online by Cambridge University Press

1
Introduction
Since the early 2000s we have seen the basic notions of coding theory expand 
beyond the role of error correction and algebraic coding theory. The purpose 
of this volume is to provide a brief introduction to a few of the directions that 
have been taken as a platform for further reading. Although the approach is to 
be descriptive with few proofs, there are parts which are unavoidably technical 
and more challenging.
It was mentioned in the Preface that the prerequisite for this work is a basic 
course on algebraic coding theory and information theory. In fact only a few 
aspects of finite fields, particularly certain properties of polynomials over finite 
fields, Reed-Solomon codes and Reed-Muller codes and their generalizations 
are considered to provide a common basis and establish the notation to be used. 
The trace function on finite fields makes a few appearances in the chapters 
and its basic properties are noted. Most of the information will be familiar 
and stated informally without proof. A few of the chapters use notions of 
information theory and discrete memoryless channels and the background 
required for these topics is also briefly reviewed in Section 1.2. The final 
Section 1.3 gives a brief description of the chapters that follow.
1.1 Notes on Finite Fields and Coding Theory
Elements of Finite Fields
A few basic notions from integers and polynomials will be useful in several 
of the chapters as well as considering properties of finite fields. The greatest 
common divisor (gcd) of two integers or polynomials over a field will be a 
staple of many computations needed in several of the chapters. Abstractly, an 
integral domain is a commutative ring in which the product of two nonzero 
elements is nonzero, sometimes stated as a commutative ring with identity 
1
https://doi.org/10.1017/9781009283403.002 Published online by Cambridge University Press

2
1 Introduction
which has no zero divisors (i.e., two nonzero elements a, b such that ab = 0). 
A Euclidean domain is an integral domain which is furnished with a norm 
function, in which the division of an element by another with a remainder of 
lower degree can be formulated. Equivalently the Euclidean algorithm (EA) 
can be formulated in a Euclidean domain.
Recall that the gcd of two integers a,b e Z is the largest integer d 
that divides both a and b.LetF be a field and denote by F[x] the ring of 
polynomials over F with coefficients from F. The gcd of two polynomials 
a(x), b(x) e F[x] is the monic polynomial (coefficient of the highest power 
of x is unity) of the greatest degree, d(x), that divides both polynomials. The 
EA for polynomials is an algorithm that produces the gcd of polynomials a(x) 
and b(x) (the one for integers is similar) by finding polynomials u(x) and v(x) 
such that
d(x) = u(x)a(x) + v(x)b(x). 
(1.1)
It is briefly described as follows. Suppose without loss of generality that 
deg b(x) < deg a(x) and consider the sequence of polynomial division steps 
producing quotient and remainder polynomials:
a(x) = q1 (x )b(x) + r1 (x), 
deg r1 < deg b
b(x) = q2(x)r1(x) + r2(x), 
deg r2 < deg r1
r1(x) = q3(x)r2(x) + r3(x), 
deg r3 < deg r2
.. 
.. 
..
rk (x) = qk+2(x)rk+1 (x) + rk+2(x), 
deg rk+2 < deg rk+1
rk+1 (x) = qk+3 (x)rk+2 (x), 
d(x) = rk+2(x).
That d(x), the last nonzero remainder, is the required gcd is established by 
tracing back divisibility conditions. Furthermore, tracing back shows how two 
polynomials u(x) and v(x) are found so that Equation 1.1 holds.
A similar argument holds for integers. The gcd is denoted (a, b) or 
(a(x), b(x)) for integers and polynomials, respectively. If the gcd of two 
integers or polynomials is unity, they are referred to as being relatively prime 
and denoted (a,b) = 1 or (a(x),b(x)) = 1.
If the prime factorization of n is
n = P11 P e2 ••• Pkk, P1 ,P 2, ...,Pk distinct primes,
then the number of integers less than n that are relatively prime to n is given 
by the Euler Totient function $(n) where
k
<Kn) = flPe 1 (Pi - 1). 
(1.2)
https://doi.org/10.1017/9781009283403.002 Published online by Cambridge University Press

1.1 Notes on Finite Fields and Coding Theory
3
A field is a commutative ring with identity in which elements have additive 
inverses (0 denotes the additive identity) and nonzero elements have multi­
plicative inverses (1 denotes the multiplicative identity). It may also be viewed 
as an integral domain in which the nonzero elements form a multiplicative 
group.
A finite field is a field with a finite number of elements. For a finite field, 
there is a smallest integer c such that each nonzero element of the field added 
to itself a total of c times yields 0. Such an integer is called the characteristic 
of the field. If c is not finite, the field is said to have characteristic 0. Notice 
that in a finite field of characteristic 2, addition and subtraction are identical in 
that 1 + 1 = 0. Denote the set of nonzero elements of the field F by F*.
Denote by Zn the set of integers modulo n, Zn ={0, 1,2,...,n - 1}. It is 
a finite field iff n is a prime p, since if n = ab, a,b e Z is composite, then it 
has zero divisors and hence is not a field. Thus the characteristic of any finite 
field is a prime and the symbol p is reserved for some arbitrary prime integer. 
In a finite field Zp, arithmetic is modulo p. Ifa e Zp,a = 0, the inverse of a 
can be found by applying the EA to a<pand p which yields two integers 
u, v e Z such that
ua + vp = 1inZ
and so ua + vp (mod p) = ua = 1 (mod p) and a-1 = u (mod p). The 
field will be denoted Fp . In any finite field there is a smallest subfield, a set 
of elements containing and generated by the unit element 1, referred to as the 
prime subfield, which will be Fp for some prime p .
Central to the notion of finite fields and their applications is the role of 
polynomials over the field. Denote the ring of polynomials in the indeterminate 
x over a field F by F[x] and note that it is a Euclidean domain (although the 
ring of polynomials with two variables F[x,y] is not). A polynomial f(x) = 
fnxn + fn-1 xn-1 + • • • + f1 x + f0 e F[x],fi e F is monic if the leading 
coefficient fn is unity.
A polynomial f(x) e F[x] is called reducible if it can be expressed as the 
product of two nonconstant polynomials and irreducible if it is not the product 
of two nonconstant polynomials, i.e., there do not exist two nonconstant 
polynomials a(x),b(x) e F[x] such that f(x) = a(x)b(x). Let f(x) be a 
monic irreducible polynomial over the finite field Fp and consider the set of 
pn polynomials taken modulo f(x) which will be denoted
Fp[x]/{f(x)) = [a--1 xn-1 + an-2xn-2 +-------+ a 1 x + a0,ai e Fp}
where (f (x)} is the ideal in Fp generated by f (x). Addition of two polyno­
mials is obvious and multiplication of two polynomials is taken modulo the 
https://doi.org/10.1017/9781009283403.002 Published online by Cambridge University Press

4
1 Introduction
irreducible polynomial f (x), i.e., the remainder after division by f(x). The 
inverse of a nonzero polynomial a(x) e Fp[x]/{f(x)} is found via the EA as 
before. That is since by definition (a(x), f (x)) = 1 there exist polynomials 
u(x),v(x) such that
u(x)a(x) + v(x)f (x) = 1
and the inverse of a(x) e Fp[x]/{f(x)} is u(x). Algebraically this structure 
might be described as the factor field of the ring Fp [x] modulo the maximal 
ideal {f(x)).
It follows the set Fp[x]/{f(x)} forms a finite field with pn elements. It is 
conventional to denote q = pn and the field of pn elements as either Fpn or 
Fq . Every finite field can be shown to have a number of elements of the form 
q = pn for some prime p and positive integer n and that any two finite fields of 
the same order are isomorphic. It will be noted that an irreducible polynomial 
of degree n will always exist (see Equation 1.4) and so all finite fields can be 
constructed in this manner.
In general, suppose q = pm and let f(x)be a monic irreducible polynomial 
over Fq of degree m (which will be shown to always exist). The set of qm 
polynomials over Fq of degree less than m with multiplication modulo f(x) 
will then be a finite field with qm elements and designated Fqm . For future 
reference denote the set of polynomials of degree less than m by Fq <m [x] 
and those less than or equal by Fq -m [x]. Since it involves no more effort, 
this general finite field Fqm will be examined for basic properties. The subset 
Fq Q Fqm is a field, i.e., a subset that has all the properties of a field, a subfield 
of Fqm .
The remainder of the subsection contains a brief discussion of the structure 
of finite fields and polynomials usually found in a first course of coding theory.
It is straightforward to show that over any field F (xm - 1) divides (xn - 1) 
iff m divides n, written as
(xm - 1) | (xn - 1) iff m | n.
Further, for any prime p,
(pm - 1 )\(pn - 1) iff m | n.
The multiplicative group of a finite field, F*m, can be shown to be cyclic 
(generated by a single element). The order of a nonzero element a in a field F 
is the smallest positive integer £ such that ae = 1, denoted as ord (a) = £ and 
referred to as the order of a. Similarly if £ is the smallest integer such that the 
polynomial f (x) | (x£ - 1), the polynomial is said to have order £ over the 
https://doi.org/10.1017/9781009283403.002 Published online by Cambridge University Press

1.1 Notes on Finite Fields and Coding Theory
5
understood field. The order of an irreducible polynomial is also the order of 
its zeros.
If p has order I, then p has order l/(i, I). Similarly if p has order I and y 
has order k and (I, k) = 1, then the order of Py is £k.
An element a e Fqm of maximum order qm — 1 is called a primitive element. 
If a is primitive, then a1 is also primitive iff (i,qm — 1) = 1 and there are 
$(qm — 1) primitive elements in Fqm.
A note on the representation of finite fields is in order. The order of 
an irreducible polynomial f(x) over Fq of degree k can be determined by 
successively dividing the polynomial (xn — 1) by f (x) over Fq as n increases. 
If the smallest such n is qk — 1, the polynomial is primitive. To effect the 
division, arithmetic in the field Fq is needed. If f(x) is primitive of degree k 
over Fq , one could then take the field as the elements
Fqk = 0, 1,x,x2,...,xqk—2 .
By definition the elements are distinct. Each of these elements could be taken 
modulo f(x) (which is zero in the field) which would result in the field 
elements being all polynomials over Fq of degree less than k. Multiplication 
in this field would be polynomials taken modulo f (x). The field element x 
is a primitive element. While this is a valid presentation, it is also common to 
identify the element x by an element a with the statement “let a be a zero of the 
primitive polynomial f (x) of degree k over Fq.” The two views are equivalent.
There are $(qk — 1) primitive elements in Fq and since the degree of 
an irreducible polynomial with one of these primitive elements as a zero is 
necessarily k, there are exactly $(qk — 1 )/k primitive polynomials of degree 
k over Fq .
Suppose f(x) is an irreducible nonprimitive polynomial of degree k over 
Fq. Suppose it is of order n<qk — 1, i.e., f(x) | (xn — 1). One can define the 
field Fqk as the set of polynomials of degree less than k
Fqk = at— —1 xk 1 + ak2Xxk 2 + • • • + a 1 x + a0, aiFq|
with multiplication modulo f (x). The element x is not primitive if n < (qk-1) 
but is an element of order n,n | qk — 1 (although there are still $(qk — 1) 
primitive elements in the field).
Let a e Fqm be an element of maximum order (qm -1) (i.e., primitive) and 
denote the multiplicative group of nonzero elements as
F*m = {a) = |1,a,a2,... ,aq — 2|.
https://doi.org/10.1017/9781009283403.002 Published online by Cambridge University Press

6
1 Introduction
Let p F Fmm be an element of order £ which generates a cyclic multiplica­
tive subgroup of Fmm of order £ and for such a subgroup £ | (qm - 1). The 
order of any nonzero element in Fqm divides (qm - 1). Thus
xqm - x = n (x - p), xq-1 -1 = n (x - p) (1.3)
is a convenient factorization (over Fqm ).
Suppose Fqm has a subfield Fqk - a subset of elements which is itself a field. 
The number of nonzero elements in Fqk is (qk - 1) and this set must form a 
multiplicative subgroup of Fqmm and hence (qk - 1) | (qm - 1) and this implies 
that k | m and that Fqk is a subfield of Fqm iff k | m. Suppose Fqk is a subfield 
of Fqm . Then
p F Fqm is in Fqk iff pq = p
and p = aj F Fqm is a zero of the monic irreducible polynomial f(x) of 
degree . over Fq. Thus
f (x) = xk + fk-1 xk 1 + • • • + f1 x + f0, fi F Fq, i = 0,1,2,... ,k — 1
and f(aj) = 0. Notice that
f(x)q = (xk + fk-1 xk-1 + • • • + f1 x + fo) q
= xkq + fkq-1 xq(k-1} + • • • + tfxq + fq
= xkq + fk-1 xq(k-1) + ••• + f1 xq + fo, as fq = fi for fi F Fq
= f(xq)
and since p = aj is a zero of f (x) so is pq. Suppose £ is the smallest integer 
such that pq = p (since the field is finite there must be such an £) and let
C j = { aj = p,pq,pq 2,.../-1}
referred to as the conjugacy class of p . Consider the polynomial
£-1 
g(x) = x - pqj
i=o
and note that
£-1 
£-1 
£-1
g(x)q = 
x-pqj q= xq - pqj+1 = 
xq-pqj = g(xq)
i=o 
i=o 
i=o
and, as above, g(x) has coefficients in Fq , i.e., g(x) F Fq [x]. It follows that 
g(x) must divide f(x) and since f(x) was assumed monic and irreducible it 
must be that g(x) = f (x). Thus if one zero of the irreducible f(x) is in Fqm, 
https://doi.org/10.1017/9781009283403.002 Published online by Cambridge University Press

1.1 Notes on Finite Fields and Coding Theory
7
all are. Each conjugacy class of the finite field corresponds to an irreducible 
polynomial over Fq .
By similar reasoning it can be shown that if f(x)is irreducible of degree k 
over Fq, then f(x) | (xqm -x) iffk | m. It follows that the polynomial xqm -x 
is the product of all monic irreducible polynomials whose degrees divide m. 
Thus
xqm - x = 
f (x).
f (x) irreducible
over Fq
degreef (x)=k|m
This allows a convenient enumeration of the polynomials. If Nq (m) is the 
number of monic irreducible polynomials of degree m over Fq , then by the 
above equation
qm = 
kNq(k)
k|m
which can be inverted using standard combinatorial techniques as
Nq(m) = — 52 ^m\qk 
mk
k|m
(1.4)
where ^(n) is the Mobius function equal to 1 if n = 1, (— 1 )s if n is the 
product of s distinct primes and zero otherwise. It can be shown that Nq (k) is 
at least one for all prime powers q and all positive integers k. Thus irreducible 
polynomials of degree k over a field of order q exist for all allowable 
parameters and hence finite fields exist for all allowable parameter sets.
Consider the following example.
Example 1.1 Consider the field extension F26 over the base field F2. The 
polynomial x26 — x factors into all irreducible polynomials of degree dividing 
6, i.e., those of degrees 1,2, 3 and 6. From the previous formula
N2(1) = 2, N2(2) = 1, N2(3) = 2, N2(6) =9.
For a primitive element a the conjugacy classes of F26 over F2 are (obtained by 
raising elements by successive powers of2 mod 63, with tentative polynomials 
associated with the classes designated):
a1 ,a2,a4,a8,a16,a32 ^ f1 (x)
a3,a6,a12,a24,a48,a33 « f2(x)
a5,a10,a20,a40,a17,a34 ^ f3 (x)
a7,a14,a28,a56,a49,a35 ^ f4(x)
a9,a18,a36 ^ f5 (x)
a11 ,a22,a44,a25,a50,a37 ^ f6(x)
https://doi.org/10.1017/9781009283403.002 Published online by Cambridge University Press

8
1 Introduction
a13,a26,a52,a41 ,a19,a38 ^ f7(x)
a15,a30,a60,a57,a51 ,a39 ^ f8(x)
a21 ,a42 ^ f9 (x)
a23,a46,a29,a58,a53,a43 « f10(x)
a27,a54,a45 ^ f11 (x)
a31 ,a62,a61 ,a59,a55,a47 « f12(x).
By the above discussion a set with £ integers corresponds to an irreducible 
polynomial over F2 of degree £. Further, the order of the polynomial is the 
order of the conjugates in the corresponding conjugacy class.
Notice there are $(63) = $(9 • 7) = 3 • 2 • 6 = 36 primitive elements in F26 
and hence there are 36/6 = 6 primitive polynomials of degree 6 over F2.Ifa 
is chosen as a zero of the primitive polynomial f1 (x) = x6 + x + 1, then the 
correspondence of the above conjugacy classes with irreducible polynomials is
Poly. No. Polynomial
Order
f1 (x)
x6 + x + 1
63
f2(x)
x6 + x4 + x3 + x2 + x + 1
21
f3(x)
x6 + x5 + x2 + x + 1
63
f4(x)
x6 + x3 + 1
9
f5(x)
x3 + x2 + 1
7
f6(x)
x6 + x5 + x3 + x2 + 1
63
f7(x)
x6 + x4 + x3 + x + 1
63
f8(x)
x6 + x5 + x4 + x2 + 1
21
f9(x)
x2 + x + 1
3
f10(x)
x6 + x5 + x4 + x + 1
63
f11(x)
x3 + x + 1
7
f12(x)
x6 + x5 + 1
63
The other three irreducible polynomials of degree 6 are of orders 21 (two of 
them, f2(x) and f8(x)) and nine (f4(x)). The primitive element a in the above 
conjugacy classes could have been chosen as a zero of any of the primitive 
polynomials. The choice determines arithmetic in F26 but all choices will 
lead to isomorphic representations. Different choices would have resulted in 
different associations between conjugacy classes and polynomials.
Not included in the above table is the conjugacy class {a63} which 
corresponds to the polynomial x + 1 and the class {0} which corresponds to 
the polynomial x . The product of all these polynomials is x26 - x .
The notion of a minimal polynomial of a field element is of importance for 
coding. The minimal polynomial mp(x) of an element p e F qn over F q is that 
https://doi.org/10.1017/9781009283403.002 Published online by Cambridge University Press

1.1 Notes on Finite Fields and Coding Theory
9
the monic irreducible polynomial of least degree that has P as a zero. From 
the above discussion, every element in a conjugacy class has the same minimal 
polynomial.
Further notions of finite fields that will be required include that of a 
polynomial basis of Fqn over Fq which is one of a form {1,a,a2,... ,an-1} 
for some a e Fqn for which the elements are linearly independent over Fq. 
A basis of Fqn over Fq of the form {a,aq,aq2,... ,Oqn 1} is called a normal 
basis and such bases always exist. In the case that a e Fqn is primitive (of 
order qn -1) it is called a primitive normal basis.
The Trace Function of Finite Fields
Further properties of the trace function that are usually discussed in a first 
course on coding will prove useful at several points in the chapters. Let Fqn 
be an extension field of order n over Fq. For an element a e Fqn the trace 
function of Fqn over Fq is defined as
n — 1
Trqn|q(a) 
aq•
i=0
The function enjoys many properties, most notably that [8]
(i) 
Trqn।q(a + P) = Trqn।q(a) + Trqn।q(P), a,p e Fqn
(ii) Trqn।q(aa) = aTrqn।q(a), a e Fq,a e Fqn
(iii) Trqn |q (a) = na, a e Fq
(iv) Trqn |q is an onto map.
To show property (iv), which the trace map is onto (i.e., codomain is Fq), it 
is sufficient to show that there exists an element a of Fqn for which Trqn ।q(a) = 
0 since if Trqn।q(a) = b = 0,b e Fq, then (property ii) Trqn।q(b-1 a) = 1 and 
hence all elements of Fq are mapped onto. Consider the polynomial equation
xq" -1 + xq"-2 + ■■■ + x = 0
that can have at most qn—1 solutions in Fqn . Hence there must exist elements 
of P e Fqn for which Trqn |q (P) = 0. An easy argument shows that in fact 
exactly qn-1 elements of Fq n have a trace of a e Fq for each element of Fq .
Notice that it also follows from these observations that
xq — x = J~[ xxn 
+ xq 
+ • • • + x — a^
aeFq
since each element of Fqn is a zero of the LHS and exactly one term of 
the RHS.
https://doi.org/10.1017/9781009283403.002 Published online by Cambridge University Press

10
1 Introduction
Also, suppose [8] L(•) is a linear function from Fq to Fq in the sense that 
for all a 1 ,a2 e Fq and all a 1 ,a2 e Fqn
L(a 1 a 1 + a 2 a 2) = a 1 L(a 1) + a 2 L(a 2).
Then L(•) must be of the form
L(a) = Tr qn । q(fia) = Lp(a)
for some p. Thus the set of such linear functions is precisely the set 
LP(•}, P e Fqn
and these are distinct functions for distinct p .
A useful property of the trace function ([8], lemma 3.51, [11], lemma 9.3)
is that ifu1,u2,...,un is a basis of Fqn over Fq and if
Trqn|q (aui) = 0fori = 1,2,...,n, ae Fqn,
then a = 0. Equivalently if for a e Fqn
Trqn|q(au) = 0 Vu e Fqn,
(1.5)
then a = 0. This follows from the trace map being onto. It will prove a useful 
property in the sequel. It also follows from the fact that
q
u1 u1
q
u2 u2
qn-1 
u1
qn -1 
u 2
un
q 
un
qn-1
• • Un
is nonsingular iff u1,u2,...,un e Fqn are linearly independent over Fq. 
A formula for the determinant of this matrix is given in [8].
If ^ = {^ 1 ,^2, ...,^n} is a basis of Fqn over Fq, then a basis v = 
{v 1 ,v2,..., vn} is called a trace dual basis if 
Tr qn | q(^ivj') = &i,j
1ifi = j
0ifi = j
and for a given basis a unique dual basis exists. It is noted that if ^ = 
{^ 1,..., ^n} is a dual basis for the basis {v 1,..., vn}, then given
n 
n
y 
ai* 
then y 
Tr qn । q(yvi)^i, ai e F q. 
(1.6)
Thus an element y e Fqn can be represented in the basis ^, by the traces 
Trqn|q(yvj), j = 1,2,...,n.
https://doi.org/10.1017/9781009283403.002 Published online by Cambridge University Press

1.1 Notes on Finite Fields and Coding Theory
11
It can be shown that for a given normal basis, the dual basis is also normal. 
A convenient reference for such material is [8, 12].
Elements of Coding Theory
A few comments on BCH, Reed-Solomon (RS), Generalized Reed-Solomon 
(GRS), Reed-Muller (RM) and Generalized Reed-Muller (GRM) codes are 
noted. Recall that a cyclic code of length n and dimension k and minimum 
distance d over Fq , designated as an (n,k,d)q code, is defined by a polynomial 
g(x) e Fq [x] of degree (n — k), g(x) | (xn - 1) or alternatively as a principal 
ideal (g(x)} in the factor ring R = Fq[x]/{xn — 1).
Consider a BCH code of length n | (qm — 1) over Fq. Let p be a primitive 
n-th root of unity (an element of order exactly n). Let
g(x) = lcm mp (x),mp2 (x), . . . ,mp2t (x)
be the minimum degree monic polynomial with the sequence p, p2, ...,p2t of 
2t elements as zeros (among other elements as zeros). Define the BCH code 
with length n designed distance 21 + 1 over Fq as the cyclic code C = {g(x')} 
or equivalently as the code with null space over Fq of the parity-check matrix
1pp
1 P 2 
P'
H=
2 ... p((n—1) _
■4 ... p2(n — 1)
1 p21 p2(2t) ••• p2t(n — 1)
That the minimum distance bound of this code, d = 2t + 1, follows since any 
21 x 21 submatrix of H is a Vandermonde matrix and is nonsingular since the 
elements of the first row are distinct.
A cyclic Reed-Solomon (n,k,d = n — k + 1)q code can be generated by 
choosing a generator polynomial of the form
n— k
g(x) = Il(x — ai), a e Fq, a primitive of order n.
i=1
That the code has a minimum distance d = n — k + 1 follows easily from the 
above discussion.
A standard simple construction of Reed-Solomon codes over a finite field 
Fq of length n that will be of use in this volume is as follows. Let u = 
{u1,u2,...,un} be a set, referred to as the evaluation set (and viewed as a 
set rather than a vector - we use boldface lowercase letters for both sets and 
https://doi.org/10.1017/9781009283403.002 Published online by Cambridge University Press

12
1 Introduction
vectors) of n < q distinct evaluation elements of Fq. As noted, F<k[x] is the 
set of polynomials over Fq of degree less than k. Then another incarnation of 
a Reed-Solomon code can be taken as
RSn,k(u,q) = { cf = (f(u 1), f(u 2),..., f(un)), f e F <k [ x ] }
where cf is the codeword associated with the polynomial f. That this is an 
(n,k,d =n - k + 1)q code follows readily from the fact that a polynomial of 
degree less than k over Fq can have at most k - 1 zeros. As the code satisfies 
the Singleton bound d < n - k + 1 with equality it is referred to as maximum 
distance separable (MDS) code and the dual of such a code is also MDS. 
Of course the construction is valid for any finite field, e.g., Fq<.
The dual of an RS code is generally not an RS code.
A slight but useful generalization of this code is the Generalized Reed- 
Solomon (GRS) code denoted as GRSn,k(u,v,q), where u is the evaluation set 
of distinct field nonzero elements as above and v = {v 1 ,v2,..., vn}, vi e F * 
q 
(referred to as the multiplier set) is a set of not necessarily distinct nonzero 
elements ofFq. Then GRSn,k(u,v,q) is the (linear) set of codewords:
GRSn,k(u,v,q) = cf = (v1f (u1),v2f (u2), . . . ,vnf (un)), f e Fq<k [x] .
Since the minimum distance of this linear set of codewords is n - k + 1 
the code is MDS, for the same reason noted above. Clearly an RS code is a 
GRSn,k(u,v,q) code with v = (1, 1,...,1).
The dual of any MDS code is MDS. It is also true [7, 9, 10] that the dual 
of a GRS code is also a GRS code. In particular, given GRSn,k(u,v,q) there 
exists a set w e (F*)n such that 
q
GRSn^ju, v,q) = GRSn,n-k(u, w,q)
= w1g(u1),w2g(u2),...,wng(un), g e Fq<n-k [x] .
(1.7)
In other words, for any f(x) e Fq<k[x] and g(x) e Fq<n-k [x] for a given 
evaluation set u = {u1,...,un} and multiplier set v = {v1,...,vn} there is 
a multiplier set w ={w1,..., wn} such that the associated codewords cf e 
GRSn,k(u,v,q) and cg e GRSn,n-k(u,w,q) are such that
(Cf, Cg) = V 1 f (u 1 )w 1 g(u 1) +------+ Vnf (Un)Wng(Un) = 0.
Indeed the multiplier set vector w can be computed as
wi = vi 
(ui - uj) 
. 
(1.8)
j=i
https://doi.org/10.1017/9781009283403.002 Published online by Cambridge University Press

1.1 Notes on Finite Fields and Coding Theory
13
To see this, for a given evaluation set u (distinct elements), denote
n
e(x) = (x - ui) and ei (x) = e(x)/(x - ui) = (x - uk),
a monic polynomial of degree (n - 1). It is clear that
ei(uj) 
ei(ui)
1if
0if j=i 
j = i.
It follows that for any polynomial h(x) e Fq [x] of degree less than n that takes 
on values h(ui) on the evaluation set u ={u1,u2,...,un} can be expressed as
n
h(x) = 
h(ui) ei(x) 
ei(ui)
To verify Equation 1.7 consider applying this interpolation formula 
to f (x)g(x) where f(x) is a codeword polynomial f(x) e Fq<k[x] 
(in GRSn,k(u,v,q)) and g(x) e F<n-k[x] (in GRSn,k(u,v,q)± = 
GRSn,n-k(u,w,q)) where it is claimed that the two multiplier sets v = 
{v1,v2,...,vn} and w ={w1,w2,...,wn} are related as in Equation 1.8.
Using the above interpolation formula on the product f (x)g(x) (of degree 
at most (n - 2)) gives
ek(x) 
f(x)g(x) = 52 f(uk)g(uk)——7.
ek(uk)
k=1
The coefficient of xn-1 on the left side is 0 while on the right side is 1 (as ek(x) 
is monic of degree (n - 1)) and hence
” 
1 
” 
/ v-1 
\
0 = 
—7 f(uk)g(uk) = 52 (vkf(uk))\ k ggM]
k=1 ek(uk) 
k=1 
ek(uk)
n
= 
(vkf (uk))(wkg(uk)) 
(by Equation 1.8)
k=1
= (cf, cg) = 0.
It is noted in particular that
RS^,k(u,q) = GRSn,n-k(u, w,q)
for the multiplier set wi = j=i (ui - uj).
https://doi.org/10.1017/9781009283403.002 Published online by Cambridge University Press

14
1 Introduction
Reed-Muller Codes
Reed-Muller (RM) codes are discussed in some depth in most books on coding 
(e.g., [3, 4]) with perhaps the most comprehensive being [2] which considers 
their relationship to Euclidean geometries and combinatorial designs. The 
properties of RM codes are most easily developed for the binary field but the 
general case will be considered here - the Generalized Reed-Muller (GRM) 
codes (generalized in a different sense than the GRS codes). The codes are 
of most interest in this work for the construction of locally decodable codes 
(Chapter 8) and their relationship to multiplicity codes introduced there.
Consider m variables x1,x2,...,xm and the ring Fq[x1,x2,...,xm] = Fq [x] 
of multivariate polynomials over Fq (see also Appendix B). The set of all 
monomials of the m variables and their degree is of the form
| xl = xi1 x22 • xfi, i ~ (i 1 ,i2,.. .,im), degree 
ij }. 
(1.9)
A multivariate polynomial f(x) e Fq [x] is the sum of monomials over Fq and 
the degree of f is largest of the degrees of any of its monomials. Notice that 
over the finite field Fq,xiq = xi and so only degrees of any variable less than q 
are of interest.
In the discussion of these codes we will have the need for two simple 
enumerations: (i) the number of monomials on m variables of degree exactly 
d and (ii) the number of monomials of degree at most d . These problems are 
equivalent to the problems of the number of partitions of the integer d into at 
most m parts and the number of partitions of all integers at most d into at most 
m parts. These problems are easily addressed as “balls in cells” problems as 
follows.
For the first problem, place d balls in arow and add a further m balls. There 
are d + m - 1 spaces between the d + m balls. Choose m - 1 of these spaces 
in which to place markers (in d+mm--1 1 ways). Add markers to the left of the 
row and to the right of the row. Place the balls between two markers into a 
“bin” - there are m such bins. Subtract a ball from each bin. If the number of 
balls in bin j is ij , then the process determines a partition of d in the sense 
that i 1 + i2 + • • • + im = d and all such partitions arise in this manner. Thus 
the number of monomials on m variables of degree equal to d is given by
(
d + m — 1 
। 
।
= I {i 1 + i2 + • • • + im = d, ij e Z>o}l. 
(1.10)
m-1
To determine the number of monomials on (at most) m variables of total 
degree at most d, consider the setup as above except now add another ball 
to the row to have d + m + 1 balls and choose m of the spaces between
https://doi.org/10.1017/9781009283403.002 Published online by Cambridge University Press

1.1 Notes on Finite Fields and Coding Theory
15
the balls in d+mm ways in which to place markers corresponding to m + 1 
bins. As before subtract a ball from each bin. The contents of the last cell are 
regarded as superfluous and discarded to take into account the “at most” part 
of the enumeration. The contents of the first m cells correspond to a partition 
and the number of monomials on m variables of total degree at most d is
d+m
= |{ i 1 + i 2 + • 
m
• • + im < d, lj € Z>0} |.
(1.11)
Note that it follows that
dE
j=1
j+m- 
m-1
d+m
m
1
(i.e., the number of monomials of degree at most d is the number of monomials 
of degree exactly j for j = 1, 2,...,d) as is easily shown by induction.
Consider the code of length qm denoted GRMq (d, m) generated by mono­
mials of degree at most d onm variables ford<q-1, i.e., let f(x) € Fq[x] be 
an m-variate polynomial of degree at most d < q — 1 (the degree of polynomial 
is the largest degree of its monomials and no variable is of degree greater than 
q — 1). The corresponding codeword is denoted
cf = f (a), a € Fqm, f € Fq [x], f of degree at most d ,
i.e., a codeword of length qm with coordinate positions labeled with all 
elements of Fqm and coordinate labeled a € Fqm with a value of f(a). It is 
straightforward to show that the codewords corresponding to the monomials 
are linearly independent over Fq and hence the code has length and dimension
code length n = qm
and
code dimension k =
m+d 
d
To determine a bound on the minimum distance of the code the theorem 
([8], theorem 6.13) is used that states the maximum number of zeros of a 
multivariate polynomial of m variables of degree d over Fq is at most dqm—1. 
Thus the maximum fraction of a codeword that can have zero coordinates is 
d/q and hence the normalized minimum distance of the code (code distance 
divided by length) is bounded by
1 — d/q, d < q — 1.
(Recall d here is the maximum degree of the monomials used, not code 
distance.) The normalized (sometimes referred to as fractional or relative) 
distance of a code will be designated as A = 1 — d/q. (Many works use S to 
denote this, used for the erasure probability on the BEC here.) Thus, e.g., for
https://doi.org/10.1017/9781009283403.002 Published online by Cambridge University Press

16
1 Introduction
d22
m = 2 (bivariate polynomials) this subclass of GRM codes has the parameters 
q2, d+d2 ,q2 - dq . Note that the rate of the code is
q2 « d2/2q2 = (1 - A)2/2.
Thus the code can have rate at most 1/2.
For a more complete analysis of the GRM codes the reader should consult 
([2], section 5.4). Properties of GRS and GRM codes will be of interest in 
several of the chapters.
1.2 Notes on Information Theory
The probability distribution of the discrete random variable X, Pr(X = x), 
will be denoted PX(x) or as P(x) when the random variable is understood. 
Similarly a joint discrete random variable X x Y (or XY) is denoted Pr(X = 
x,Y = y) = PXY (x,y). The conditional probability distribution is denoted 
Pr(Y = y | X = x) = P(y | x). A probability density function (pdf) for a 
continuous random variable will be designated similarly as pX(x) or a similar 
lowercase function.
Certain notions from information theory are required. The entropy of a 
discrete ensemble {P (xi), i = 1,2,...} is given by
H(X) = - £ P(xi) log P(xi)
and unless otherwise specified all logs will be to the base 2. It represents the 
amount of uncertainty in the outcome of a realization of the random variable.
A special case will be important for later use, that of a binary ensemble 
{p,(1 - p)} which has an entropy of
H2(p)=-plog2p- (1 - p) log2(1 -p) 
(1.12)
referred to as the binary entropy function. It is convenient to introduce the 
q-ary entropy function here, defined as
x x . _ f X logq(q - 1) - x logq x - (1 - x) logq( 1 - x), 0 <x < 0 = (q - 1 )/q 
Hq(x)= 0, 
x=0
(1.13) 
an extension of the binary entropy function. Notice that Hq (p) is the entropy 
associated with the q-ary discrete symmetric channel and also the entropy of 
the probability ensemble {1 - p, p/(q - 1),...,p/(q- 1)} (total ofq values). 
The binary entropy function of Equation 1.12 is obtained with q = 2.
https://doi.org/10.1017/9781009283403.002 Published online by Cambridge University Press

1.2 Notes on Information Theory
17
Similarly the entropy of a joint ensemble P (xi,yi),i = 1, 2,...} is 
given by
H(X,Y) = - 
P(xi,yi) log P(xi,yi)
and the conditional entropy of X given Y is given by
H(X | Y) = - £ P(xi,yi) log P(xi | yi) = H(X,Y) - H(Y)
which has the interpretation of being the expected amount of uncertainty 
remaining about X after observing Y, sometimes referred to as equivocation.
The mutual information between ensembles X and Y is given by
I(X; Y) = V P(xi,yj) log P(xi,yj)
X-yi 
P(xi)P(yj)
(1.14)
and measures the amount of information knowledge that one of the variables 
gives on the other. The notation {X; Y} is viewed as a joint ensemble. It will 
often be the case that X will represent the input to a discrete memoryless 
channel (to be discussed) and Y the output of the channel and this notion 
of mutual information has played a pivotal role in the development of 
communication systems over the past several decades.
Similarly for three ensembles it follows that
I(X;Y,Z) = 
P(xi,yj,zk) log
P(xi,yj,Zk)
.
P(xi)P(yj,Zk)
The conditional information of the ensemble {X; Y} given Z is
P(xi,yj | zk)
I(X; Y | Z) = 
p(xi,yj,zk) log——•——----•—-
P(xi | zk)P (yj | zk) 
i,j,k
or alternatively
I(X;Y | Z)= 
P(xi,yj,zk) log P(xi,yj,zk')P(zk')
.
P (xi,zk)P (yj,zk)
The process of conditioning observations of X, Y on a third random variable Z 
may increase or decrease the mutual information between X and Z but it can 
be shown the conditional information is always positive. There are numerous 
relationships between these information-theoretic quantities. Thus
I(X;Y) = H(X)-H(X| Y)= H(X) + H(Y) - H(X, Y).
Our interest in these notions is to define the notion of capacity of certain 
channels.
https://doi.org/10.1017/9781009283403.002 Published online by Cambridge University Press

18
1 Introduction
A discrete memoryless channel (DMC) is a set of finite inputs X and a 
discrete set of outputs Y such that at each instance of time, the channel accepts 
an input x e X and with probability W(y | x) outputs y e Y and each use of 
the channel is independent of other uses and
W(y | x) = 1 for each x e X.
yeY
Thus if a vector x = (x1,x2,...,xn), xi e X is transmitted in n uses of the 
channel, the probability of receiving the vector y = (y1,y2,...,yn) is given by
n
P(y | x) = 
W(yi | xi).
At times the DMC might be designated simply by the set of transition 
probabilities W = {W(yi | xi )}.
For the remainder of this chapter it will be assumed the channel input is 
binary and referred to as a binary-input DMC or BDMC where X ={0, 1} 
and that Y is finite. Important examples of such channels include the binary 
symmetric channel (BSC), the binary erasure channel (BEC) and a general 
BDMC, as shown in Figure 1.1 (a) and (b) while (c) represents the more 
general case.
Often, rather than a general BDMC, the additional constraint of symmetry is 
imposed, i.e., a binary-input discrete memoryless symmetric channel by which 
is meant a binary-input X ={0, 1} channel with a channel transition probability 
{W(y | x),x e X,y e Y} which satisfies a symmetry condition, noted later.
The notion of mutual information introduced above is applied to a DMC 
with the X ensemble representing the channel input and the output ensemble Y 
to the output. The function I(X; Y) then represents the amount of information 
the output gives about the input. In the communication context it would be 
desirable to maximize this function. Since the channel, represented by the 
channel transition matrix W(y | x), is fixed, the only variable that can be 
adjusted is the set of input probabilities P(xi ), xi e X. Thus the maximum
0
1 - p 
p\/ 
p
0
0
1 - 8
8
8
1
1
1
0
E
1
X = {0,1}..
.. Y 
.
1 -p
1-8
BSC, C = 1 - H(p) BEC, C = 1 - 8 
BDMC W(y | x)
Figure 1.1 Binary-input DMCs
https://doi.org/10.1017/9781009283403.002 Published online by Cambridge University Press

1.2 Notes on Information Theory
19
amount of information that on average can be transmitted through the channel 
in each channel use is found by determining the set of input probabilities that 
maximizes the mutual information between the channel input and output.
It is intuitive to define the channel capacity of a DMC as the maximum rate 
at which it is possible to transmit information through the channel, per channel 
use, with an arbitrarily low error probability:
channel capacity = C = max I(X,Y) = I(W)
P(x),x eX
W(y | x)p(x)
= p(max 
xex,yeY W(y |x)P(x) log P(X)(P^)
P (x),xeX 
(x )( (y)
where P(y) = xeX W (y|x)P (x). For general channels, determining channel 
capacity can be a challenging optimization problem. When the channels exhibit 
a certain symmetry, however, the optimization is achieved with equally likely 
inputs:
Definition 1.2 ([6]) A DMC is symmetric if the set of outputs can be 
partitioned into subsets in such a way that for each subset, the matrix of 
transition probabilities (with rows as inputs and columns as outputs) has the 
property each row is a permutation of each other row and each column of 
a partition (if more than one) is a permutation of each other column in the 
partition.
A consequence of this definition is that for a symmetric DMC, it can be 
shown that the channel capacity is achieved with equally probable inputs ([6], 
theorem 4.5.2). It is simple to show that both the BSC and BEC channels 
are symmetric by this definition. The capacities of the BSC (with crossover 
probability p) and BEC (with erasure probability S) are
Cbsc = 1 + P log2 P + (1 - P) log2(1 — P) and Cbec = 1 - S. (1.15)
This first relation is often written
CBSC = 1 - H2 (P)
and H2(P) is the binary entropy function of Equation 1.12.
For channels with continuous inputs and/or outputs the mutual information 
between channel input and output is given by
I(X,Y) = f P p(x,y) log( p(\,y\^ dxdy 
x y P(x)P(y)
for probability density functions p(•, •) and p(•).
Versions of the Gaussian channel where Gaussian-distributed noise is added 
to the signal in transmission are among the few such channels that offer
https://doi.org/10.1017/9781009283403.002 Published online by Cambridge University Press

20
1 Introduction
N ■ N(0,a 2)
X e {±1}
Y =X+N
Figure 1.2 The binary-input additive white Gaussian noise channel
tractable solutions and are designated additive white Gaussian noise (AWGN) 
channels. The term “white” here refers to a flat power spectral density function 
of the noise with frequency. The binary-input AWGN (BIAWGN) channel, 
where one of two continuous-time signals is chosen for transmission during 
a given time interval (0,T) and experiences AWGN in transmission, can be 
represented as in Figure 1.2:
Yi = Xi + Ni, and Xi e {±1}, 
BIAWGN,
where Ni is a Gaussian random variable with zero mean and variance a2, 
denoted Ni ~ N(0,a2). The joint distribution of (X, Y) is a mixture of discrete 
and continuous and with P(X = +1) = P(X = -1) = 1/2 (which achieves 
capacity on this channel) and with p(x') ~ N(0,a2). The pdf p(y) of the 
channel output is
1
P(y) = 2
V2 na 2
2 a2 + 1 • 
.
2 V2 na 2
(y+1)2
(y-1)2
(1.16)
na2
exp
(y+1)2
+ exp
(y-1)2
1
e
1
e
2 a 2
1
2 a 2
2 a 2
and maximizing the expression for mutual information of the channel (equally 
likely inputs) reduces to
Cbiawgn = - J p(y) log2 p(y)dy - 
log2(2nea2). 
(1.17)
The general shape of these capacity functions is shown in Figure 1.3 where 
SNR denotes signal-to-noise ratio.
Another Gaussian channel of fundamental importance in practice is that of 
the band-limited AWGN channel (BLAWGN). In this model a band-limited 
signal x(t),t e (0,T) with signal power < S is transmitted on a channel 
band-limited to W Hz, i.e., (-W, W) and white Gaussian noise with two-sided 
power spectral density level No/2 is added to the signal in transmission. This 
channel can be discretized via orthogonal basis signals and the celebrated and 
much-used formula for the capacity of it is
https://doi.org/10.1017/9781009283403.002 Published online by Cambridge University Press

1.3 An Overview of the Chapters
21
Figure 1.3 Shape of capacity curves for (a) BSC and (b) BIAWGN
CBLAW GN = W log2(1 + S/NoW) bits per second. 
(1.18)
The importance of the notion of capacity and perhaps the crowning 
achievement of information theory is the following coding theorem, informally 
stated, that says k information bits can be encoded into n coded bits, code rate 
R = k/n, such that the bit error probability Pe of the decoded coded bits at the 
output of a DMC of capacity C can be upper bounded by
Pe < e - nE(R), R < C 
(1.19)
where E(R), the error rate function, is > 0 for all R<C. The result implies 
that for any code rate R<Cthere will exist a code of some length n capable 
of transmitting information with negligible error probability. Thus reliable 
communication is possible even though the channel is noisy as long as one 
does not transmit at too high a rate.
The information-theoretic results discussed here have driven research into 
finding efficient codes, encoding and decoding algorithms for the channels 
noted over many decades. The references [5, 15] present a more comprehensive 
discussion of these and related issues.
1.3 An Overview of the Chapters
A brief description of the following chapters is given.
The chapter on coding for erasures is focused on the search for erasure­
correcting algorithms that achieve linear decoding complexity. It starts with a 
discussion of Tornado codes. Although these codes did not figure prominently 
in subsequent work, they led to the notion of codes from random graphs with 
irregular edge distributions that led to very efficient decoding algorithms. In 
turn these can be viewed as leading to the notion of fountain codes which 
https://doi.org/10.1017/9781009283403.002 Published online by Cambridge University Press

22
1 Introduction
are not erasure-correcting codes. Rather they are codes that can efficiently 
recreate a file from several random combinations of subfiles. Such codes led 
to the important concept of Raptor codes which have been incorporated into 
numerous standards for the download of large files from servers in a multicast 
network while not requiring requests for retransmissions of portions of a file 
that a receiver may be missing, a crucial feature in practice.
Certain aspects of low-density parity-check (LDPC) codes are then dis­
cussed. These codes, which derive from the work of Gallager from the early 
1960s, have more recently assumed great importance for applications as 
diverse as coding for flash memories as well as a wide variety of communi­
cation systems. Numerous books have been written on various aspects of the 
construction and analysis of these codes. This chapter focuses largely on the 
paper of [13] which proved crucial for more recent progress for the analytical 
techniques it introduced.
The chapter on polar codes arose out of the seminal paper [1]. In a deep 
study of information-theoretic and analytical technique it produced the first 
codes that provably achieved rates approaching capacity. From a binary-input 
channel with capacity C < 1, through iterative transformations, it derived 
a channel with N = 2n inputs and outputs and produced a series of NC 
sub-channels that are capable of transmitting data with arbitrarily small error 
probability, thus achieving capacity. The chapter discusses the central notions 
to assist with a deeper reading of the paper.
The chapter on network coding is devoted to the somewhat surprising idea 
that allowing nodes (servers) in a packet network to process and combine 
packets as they traverse the network can substantially improve throughput of 
the network. This raises the question of the capacity of such a network and 
how to code the packets in order to achieve the capacity. This chapter looks at 
a few of the techniques that have been developed for multicast channels.
With the wide availability of the high-speed internet, access to information 
stored on widely distributed databases became more commonplace. Huge 
amounts of information stored in distributed databases made up of standard 
computing and storage elements became ubiquitous. Such elements fail with 
some regularity and methods to efficiently restore the contents of a failed 
server are required. Many of these early distributed storage systems simply 
replicated data on several servers and this turned out to be an inefficient 
method of achieving restoration of a failed server, both in terms of storage 
and transmission costs. Coding the stored information greatly improved 
the efficiency with which a failed server could be restored and Chapter 6 
reviews the coding techniques involved. The concepts involved are closely 
related to locally repairable codes considered in Chapter 7 where an erased 
https://doi.org/10.1017/9781009283403.002 Published online by Cambridge University Press

1.3 An Overview of the Chapters
23
coordinate in a codeword can be restored by contacting a few other coordinate 
positions.
Chapter 8 considers coding techniques which allow a small amount of 
information to be recovered from errors in a codeword without decoding 
the entire codeword, termed locally decodable codes. Such codes might find 
application where very long codewords are used and users make frequent 
requests for modest amounts of information. The research led to numerous 
other variations such as locally testable codes where one examines a small 
portion of data and is asked to determine if it is a portion of a codeword of 
some code, with some probability.
Private information retrieval considers techniques for users to access infor­
mation on servers in such a manner that the servers are unaware of which infor­
mation is being sought. The most common scenario is one where the servers 
contain the same information and users query information from individual 
servers and perform computations on the responses to arrive at the desired 
information. More recent contributions have shown how coded information 
stored on the servers can achieve the same ends with greatly improved storage 
efficiency. Observations on this problem are given in Chapter 9.
The notion of a batch code addresses the problem of storing information 
on servers in such a way that no matter which information is being sought no 
single server has to download more than a specified amount of information. 
It is a technique to ensure load balancing between servers. Some techniques to 
achieve this are discussed in Chapter 10.
Properties of expander graphs find wide application in several areas of 
computer science and mathematics and the notion has been applied to the 
construction of error-correcting codes with efficient decoding algorithms. 
Chapter 11 introduces this topic of considerable current interest.
Algebraic coding theory is based on the notion of packing spheres in a 
space of n-tuples over a finite field Fq according to the Hamming metric. 
Rank-metric codes consider the vector space of matrices of a given shape 
over a finite field with a different metric, namely the distance between two 
such matrices is given by the rank of the difference of the matrices which 
can be shown to be a metric on such a space. A somewhat related (although 
quite distinct) notion is to consider a set of subspaces of a vector space over 
a finite field with a metric defined between such subspaces based on their size 
and intersection. Such codes of subspaces have been shown to be of value in 
the network coding problem. The rank-metric codes and subspace codes are 
introduced in Chapter 12.
A problem that was introduced in the early days of information theory was 
the notion of list decoding where, rather than the decoding algorithm producing 
https://doi.org/10.1017/9781009283403.002 Published online by Cambridge University Press

24
1 Introduction
a unique closest codeword to the received word, it was acceptable to produce a 
list of closest words. Decoding was then viewed as successful if the transmitted 
codeword was on the list. The work of Sudan [14] introduced innovative 
techniques for this problem which influenced many aspects of coding theory. 
This new approach led to numerous other applications and results to achieve 
capacity on such a channel. These are overviewed in Chapter 13.
Shift register sequences have found important applications in numerous 
synchronization and ranging systems as well as code-division multiple access 
(CDMA) communication systems. Chapter 14 discusses their basic properties.
The advent of quantum computing is likely to have a dramatic effect 
on many storage, computing and transmission technologies. While still in 
its infancy it has already altered the practice of cryptography in that the 
US government has mandated that future deployment of crypto algorithms 
should be quantum-resistant, giving rise to the subject of “postquantum 
cryptography.” A brief discussion of this area is given in Chapter 15. While 
experts in quantum computing may differ in their estimates of the time frame 
in which it will become significant, there seems little doubt that it will have a 
major impact.
An aspect of current quantum computing systems is their inherent instabil­
ity as the quantum states interact with their environment causing errors in the 
computation. The systems currently implemented or planned will likely rely 
on some form of quantum error-correcting codes to achieve sufficient system 
stability for their efficient operation. The subject is introduced in Chapter 16.
The final chapter considers a variety of other coding scenarios in an effort 
to display the width of the areas embraced by the term “coding” and to further 
illustrate the scope of coding research that has been ongoing for the past few 
decades beyond the few topics covered in the chapters.
The two appendices cover some useful background material on finite 
geometries and multivariable polynomials over finite fields.
References
[1] Arikan, E. 2009. Channel polarization: a method for constructing capacity­
achieving codes for symmetric binary-input memoryless channels. IEEE Trans. 
Inform. Theory, 55(7), 3051-3073.
[2] Assmus, Jr., E.F., and Key, J.D. 1992. Designs and their codes. Cambridge Tracts 
in Mathematics, vol. 103. Cambridge University Press, Cambridge.
[3] Blahut, R.E. 1983. Theory and practice of error control codes. Advanced Book 
Program. Addison-Wesley, Reading, MA.
[4] Blake, I.F., and Mullin, R.C. 1975. The mathematical theory of coding. Academic 
Press, New York/London.
https://doi.org/10.1017/9781009283403.002 Published online by Cambridge University Press

References
25
[5] Forney, G.D., and Ungerboeck, G. 1998. Modulation and coding for linear 
Gaussian channels. IEEE Trans. Inform. Theory, 44(6), 2384-2415.
[6] Gallager, R.G. 1968. Information theory and reliable communication. John Wiley 
& Sons, New York.
[7] Huffman, W.C., and Pless, V. 2003. Fundamentals of error-correcting codes. 
Cambridge University Press, Cambridge.
[8] Lidl, R., and Niederreiter, H. 1997. Finite fields, 2nd ed. Encyclopedia of Mathe­
matics and Its Applications, vol. 20. Cambridge University Press, Cambridge.
[9] Ling, S., and Xing, C. 2004. Coding theory. Cambridge University Press, 
Cambridge.
[10] MacWilliams, F.J., and Sloane, N.J.A. 1977. The theory of error-correcting 
codes: I and II. North-Holland Mathematical Library, vol. 16. North-Holland, 
Amsterdam/New York/Oxford.
[11] McEliece, R.J. 1987. Finite fields for computer scientists and engineers. The 
Kluwer International Series in Engineering and Computer Science, vol. 23. 
Kluwer Academic, Boston, MA.
[12] Menezes, A.J., Blake, I.F., Gao, X.H., Mullin, R.C., Vanstone, S.A., and 
Yaghoobian, T. 1993. Applications of finite fields. The Kluwer International Series 
in Engineering and Computer Science, vol. 199. Kluwer Academic, Boston, MA.
[13] Richardson, T.J., and Urbanke, R.L. 2001. The capacity of low-density parity­
check codes under message-passing decoding. IEEE Trans. Inform. Theory, 47(2), 
599-618.
[14] Sudan, M. 1997. Decoding of Reed Solomon codes beyond the error-correction 
bound. J. Complexity, 13(1), 180-193.
[15] Ungerboeck, G. 1982. Channel coding with multilevel/phase signals. IEEE Trans.
Inform. Theory, 28(1), 55-67.
https://doi.org/10.1017/9781009283403.002 Published online by Cambridge University Press

2
Coding for Erasures and Fountain Codes
A coordinate position in a received word is said to be an erasure if the 
receiver is using a detection algorithm that is unable to decide which symbol 
was transmitted in that position and outputs an erasure symbol such as E 
rather than risk making an error, i.e., outputting an incorrect symbol. One 
might describe an erasure as an error whose position is known. For binary 
information symbols the two most common discrete memoryless channels 
are shown in Figure 1.1, the binary erasure channel (BEC) and the binary 
symmetric channel (BSC), introduced in Chapter 1. The symbols p and S will 
generally refer to the channel crossover probability for the BSC and erasure 
probability for the BEC, respectively. Both symbols sometimes occur with 
other meanings as will be noted. Each such channel has a capacity associated 
with it which is the maximum rate at which information (per channel use) 
can be sent through the channel error-free, as discussed in Chapter 1. The 
subject of error-correcting codes arose to meet the challenge of realizing such 
performance.
It is to be emphasized that two quite different channel error models are used 
in this chapter. The BEC will be the channel of interest in the first part of 
this chapter. Thus a codeword (typically of a linear code) is received which 
contains a mix of correct received symbols and erased symbols. The job of the 
code design and decoder algorithm is then to “fill in” or interpolate the erased 
positions with original transmitted symbols, noting that in such a model the 
unerased positions are assumed correct.
Codes derived for the BEC led to the notion of irregular distribution codes 
where the degrees of variable and check nodes of the code Tanner graph, to 
be introduced shortly, are governed by a probability distribution. These in turn 
led to fountain codes, which is the interest of the last section of the chapter. 
In such channels each transmitted packet is typically a linear combination 
of information packets over some fixed finite field. The receiver gathers a 
26
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

2.1 Preliminaries
27
sufficient number of transmitted packets (assumed without errors) until it is 
able to retrieve the information packets by, e.g., some type of matrix inversion 
algorithm on the set of packets received. The retriever then does not care which 
particular packets are received, just that they receive a sufficient number of 
them to allow the decoding algorithm to decode successfully. This is often 
described as a “packet loss” channel, in that coded packets transmitted may be 
lost in transmission due to a variety of network imperfections such as buffer 
overflow or failed server nodes, etc. Such a packet loss situation is not modeled 
by the DMC models considered.
While the two channel models examined in this chapter are quite different, 
it is their common heritage that suggested their discussion in the same chapter.
2.1 Preliminaries
It is convenient to note a few basic results on coding and DMCs for future 
reference. Suppose C = (n,k,d)q is a linear block code over the finite field 
of q elements Fq , designating a linear code that has k information symbols 
(dimension k) and (n - k) parity-check symbols and minimum distance d . 
Much of this volume is concerned with binary-input channels and q = 2.
Suppose a codeword c = (c1,c2,...,cn) is transmitted on a BEC and the 
received word is r = (r 1 ,r2,...,rn) which has e erasures in positions E c 
{1,2, ...,n}, |E|= e. Then ri = E for i e E for E the erasure symbol. The 
unerased symbols received are assumed correct.
A parity-check matrix of the code is an (n — k) x n matrix H over Fq 
such that 
H • ct = 0t
n—k
for any codeword c where 0n—k is the all-zero (n — k)-tuple, a row vector over 
F q. If the columns of H are denoted by h(i), i = 1,2, ...,n, then H • ct is 
the sum of columns of H multiplied by the corresponding coordinates of the 
codeword, adding to the all-zero column (n — k)-vector 0t. Similarly let He be 
the (n—k)x e submatrix of columns of H corresponding to the erased positions 
and ce be the e-tuple of the transmitted codeword on the erased positions. Then
He • ce = -yte
where yte is the (n — k)-tuple corresponding to the weighted sum of columns 
of H in the nonerased positions, i.e., columns of H multiplied by the known 
(unerased) positions of the received codeword.
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

28
2 Coding for Erasures and Fountain Codes
As long as e < d - 1 the above matrix equation can be solved uniquely 
for the erased word positions, i.e., ce . However, this is generally a task of 
cubic complexity in codeword length, i.e., O(n3). The work of this chapter 
will show how a linear complexity with codeword length can be achieved with 
high probability.
This chapter will deal exclusively with binary codes and the only arithmetic 
operation used will be that of XOR (exclusive or), either of elements of F2 or 
of packets of n bits in F2n . Thus virtually all of the chapter will refer to packets 
or binary symbols (bits) equally, the context being clear from the problem of 
interest.
It is emphasized that there are two different types of coding considered in 
this chapter. The first is the use of linear block codes for erasure correction 
while the second involves the use of fountain codes on a packet loss channel.
Virtually all of this chapter will use the notion of a bipartite graph to 
represent the various linear codes considered, a concept used by Tanner in his 
prescient works [37, 38, 39]. A bipartite graph is one with two sets of vertices, 
say U and V and an edge set E, with no edges between vertices in the same 
set. The graph will be called (c, d)-regular bipartite if the vertices in U have 
degree c and those of V have degree d. Since | U |= n, then | V |= (c/d)n. 
The U set of vertices will be referred to as the left vertices and V the right 
vertices. Bipartite graphs with irregular degrees will also be of interest later in 
the chapter.
There is a natural connection between a binary linear code and an (n — k) x n 
parity-check matrix and a bipartite graph. Often the left vertices of the bipartite 
code graph are associated with the entire n codeword coordinate positions and 
referred to as the variable or information nodes or vertices. Equivalently they 
represent the columns of the parity-check matrix. Similarly the (n - k) right 
nodes or vertices are the constraint or check nodes which represent the rows 
of the parity-check matrix. The edges of the graph correspond to the ones in 
the check matrix in the corresponding rows and columns. Such a graphical 
representation of the code is referred to as the Tanner graph of the code, a 
notion that will feature prominently in many of the chapters.
The binary parity-check matrix of the code is an alternate view of the 
incidence matrix of the bipartite graph. The following illustrates the Tanner 
graph associated with the parity-check matrix for a Hamming (8, 4, 4)2 code 
which is used in Example 2.3 shown also in Figures 2.1 and 2.3.
As a second graph representation of a binary linear code, it is equally 
possible to have the left nodes of the graph as the k information nodes and 
the (n - k) right nodes as the check nodes and this is the view for most of the 
next section. As a matter of convenience this representation is referred to as the
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

2.1 Preliminaries
29
H
00011110
01 1001 10
10101010
11111111
(b)
(a)
Figure 2.1 (a) The Hamming (8, 4, 4)2 code and (b) its Tanner graph
normal graph representation of a code in this work, although some literature 
on coding has a different meaning for the term “normal.” The Tanner graph 
representation seems more common in current research literature.
The next section describes a class of linear binary codes, the Tornado codes, 
which use a cascade of (normal) bipartite graphs and a very simple decoding 
algorithm for correcting erasures. To ensure the effectiveness of decoding it 
is shown how the graphs in the cascade can be chosen probabilistically and 
this development introduced the notion of irregular distributions of vertex/edge 
degrees of the left and right vertices of each graph in the cascade. This notion 
has proved important in other coding contexts, e.g., in the construction of 
LDPC codes to be considered in Chapter 3.
Section 2.3 introduces the notion of LT codes, standing for Luby trans­
form, the first incarnation of the important notion of a fountain code where 
coded packets are produced at random by linearly XORing a number of 
information packets, according to a probability law designed to ensure efficient 
decodability. That section also considers Raptor codes, a small but important 
modification of LT codes that has been standardized as the most effective way 
to achieve large downloads over the Internet.
The notion of Tornado codes introduced the idea of choosing random bipar­
tite graphs to effect erasure decoding. Such a notion led to decoding algorithms 
of fountain codes where a file is comprised of randomly linearly encoded 
pieces of the file. These decoding algorithms achieve linear complexity rather 
than the normally cubic complexity associated with Gaussian elimination with 
a certain probability of failure. As noted, there is no notion of “erasure” 
with fountain codes as there is with Tornado codes. A significant feature of 
these fountain codes is that they do not require requests for retransmission 
of missing packets. This can be a crucial feature in some systems since such 
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

30
2 Coding for Erasures and Fountain Codes
requests could overwhelm the source trying to satisfy requests from a large 
number of receivers, a condition referred to as feedback implosion. This is the 
multicast situation where a transmitter has a fixed number of packets (binary 
n-tuples) which it wishes to transmit to a set of receivers through a network. 
It is assumed receivers are not able to contact the transmitter to make requests 
for retransmissions of missing packets. The receiver is able to construct the 
complete set of transmitted information packets from the reception of any 
sufficiently large set of coded packets, not a specific set. Typically the size 
of the set of received packets is just slightly larger than the set of information 
packets itself, to ensure successful decoding with high probability, leading to 
a very efficient transmission and decoding process.
Most of the algorithms in the chapter will have linear complexity (in 
codeword length or the number of information symbols), making them very 
attractive for implementation.
2.2 Tornado Codes and Capacity-Achieving Sequences
The notion of Tornado codes was first noted in [7] and further commented onin 
[2] with a more complete account in [10] (an updated version of [7]). While not 
much cited in recent works they introduced novel and important ideas that have 
become of value in LDPC coding and in the formulation of fountain codes. At 
the very least they are an interesting chapter in coding theory and worthy of 
some note.
For this section it is assumed transmission is on the BEC. Since only the 
binary case is of interest the only arithmetic operation will be the XOR between 
code symbols and thus the code symbols (coordinate positions) can be assumed 
to be either bits or sequences of bits (packets). Any received packet is assumed 
correct - no errors in it. Packets that are erased will be designated with a special 
symbol, e.g., E (either a bit or packet) when needed.
Tornado codes can be described in three components: a cascade of a 
sequence of bipartite graphs; a (very simple) decoding algorithm for each 
stage, as decoding proceeds from the right to the left and a probabilistic design 
algorithm for each of the bipartite graphs involved. As mentioned, the design 
algorithm has proven influential in other coding contexts.
Consider the first bipartite graph B0 of Figure 2.2 with k left vertices, 
associated with the k information packets and pk, p < 1, right nodes, the check 
nodes (so each code in the cascade is a normal graph - one of the few places 
in these chapters using normal graphs). It is assumed the codes are binary and, 
as noted, whether bits or packets are used for code symbols is immaterial.
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

2.2 Tornado Codes and Capacity-Achieving Sequences
31
C(B0) C(B1)
C(Bm)
C0
pmk
pm +1 k kpm+2
(1
p)
2 (Numbers of nodes)
k
Figure 2.2 The cascade construction of the Tornado code C (B 0 ,B 1, ... ,Bm,C 0) - 
code rate =1 — p, p e (0, 1), code length k/( 1 — ft). Graph edges are not shown 
for clarity
For the sake of concreteness packets will be assumed. The parity-check matrix 
could have been used here but the equivalent graph is more convenient. The 
terms vertices and nodes are used interchangeably. The term packets will often 
be used to emphasize the application of the ideas to general networks. For a 
given set of information packets the check packets are a simple XOR of the 
connected information packets. Note that the edges have not been included in 
the figure of this cascade of graphs as it tended to obscure other aspects of 
importance.
To show how decoding could take place on such a graph, consider the first 
graph on the right of the cascade, C0, and assume for the moment that all 
check packets (the furthest right nodes) have been received without erasures 
and consider the following simple decoding algorithm [7, 10].
Algorithm 2.1 Given the value of a check packet and all but one of its 
neighbor information packets, set the missing information packet to the XOR 
of the check packet and its neighbors.
Thus to decode, one searches for a check node at each stage of the decoding 
satisfying the criterion - namely that only one of its neighbor variable nodes is 
erased, and decodes that packet, finding the value of the missing information 
packet. That information packet is then XORed with its neighbor check 
packets. Each stage proceeds similarly. For the decoding to be successful, there 
must exist suitable check nodes of degree one at each stage of the decoding 
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

32
2 Coding for Erasures and Fountain Codes
until all variable (information) packets for that stage are found - which are 
then the check nodes for the next (left) stage of the graph.
This first code in the cascade (on the left) is designated as C(B0) and 
consists of the k left information packets and pk right check packets. The 
next bipartite graph in the cascade will consist of the pk left nodes (the check 
packets of C(B0) and the “information packets” of C(B1)) and p2k check 
packets on these for some fixed p . This stage of the “code” will be designated 
as C(B1). The process of constructing the cascade continues, each graph 
consisting of the previous check nodes and constructing a new set of check 
nodes of size p times that of the set of left nodes. The cascade components are 
designated C(Bi),i = 0, 1,...,m. Each stage of the cascade has kpi left nodes 
and kpi+1,i= 0, 1,2,...,m check nodes. The cascade is concluded with 
a conventional block code C0 which is assumed to be a simple conventional 
erasure-correcting code with kpm+1 left nodes and kpm+2/(1 - p) check 
nodes, capable of correcting any fraction of p erasures with high probability. 
This code is not specified further - any of a number of standard codes will 
suffice. The whole cascade is designated as C(B0,B1,...,Bm,C0) (see Figure 
2.2). As mentioned, the edges of the graph have been omitted as they would 
tend to obscure the simple process involved. The code has k information 
packets and
m+1
kpi kp + kpm+2/(1 - p) = kp/( 1 - p), p e (0,1) 
i=1
check packets and hence the overall code has rate 1 - p .
To decode this code one starts at the extreme right graph/code C0. It is 
assumed this ordinary erasure-correcting code is strong enough to correct 
all erasures in its parity checks. At each stage of decoding, as it progresses 
from right to left, it will be assumed the check nodes are all determined. 
Applying the above decoding algorithm, the variable node values of C0 can 
be determined as long as there is a check node satisfying the criterion noted 
for the decoding algorithm, i.e., that all but one of its neighbor variables are 
determined. If all such variable nodes of C0 are determined, one proceeds 
to decoding C(Bm), all of whose check nodes (variable nodes of C0) are 
now known. Applying the decoding algorithm to these determines its variable 
nodes - assuming the supply of suitable check nodes is not exhausted prior to 
completion. The process continues to the left. The decoding is successful only 
if each stage is successful, i.e., at each stage a suitable check node is always 
available with high probability until all information nodes for that stage are 
determined.
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

2.2 Tornado Codes and Capacity-Achieving Sequences
33
The problem remains as to how to choose the component graphs in this 
cascade (i.e., the variable/check node connections) so that the algorithm will 
complete with high probability.
The above assumed a graph representation for each stage of the code in 
what has been designated as the normal form with information nodes on the 
left and check nodes on the right. It is clear that there are equivalent versions 
of the decoding algorithm for a Tanner graph representation of a code, with the 
n codeword symbols (information and check symbols) on the left and (n - k) 
check symbols associated with the nodes on the right (a single stage only), 
with erasures randomly occurring possibly in both variable and check nodes. 
Much of the recent work on coding uses the Tanner graph representation and a 
modification of the above discussion for such graphs might be as follows:
Algorithm 2.2 Identify the received codeword symbols (or packets) (with 
erasures) with the n variable (left) nodes and associate a register with each 
of the (n - k) check nodes on the right, each initially set to the all-zero 
packet. XOR the value of each nonerased variable node to its check neighbors 
and delete those variable nodes and edges emanating from them - thus only 
variable nodes that have been erased remain at this stage. If there is a check 
node of degree one, transfer its contents to its variable node neighbor, say v, 
and then XOR this value to the check neighbors of v and remove all associated 
edges. Continue in this manner until there are no check nodes of degree one. 
If decoding completes, the values of the variable nodes are the decoded word. 
If at any stage before completion, there are no check nodes of degree one the 
algorithm fails (there are still edges left in the graph).
This algorithm can be adapted for use with the cascade structure of a 
Tornado code. In both of the algorithms, decoding will be successful only if 
there is a sufficient number of suitable right nodes of degree one at each stage 
until completion. Thus the problem of designing each stage of the decoding 
algorithm to ensure this condition with high probability is of importance. 
Henceforth only one stage of the graph is of interest and the Tanner graph 
description of the binary linear code is assumed. The connection to the normal 
graph representation of interest above is immediate.
Example 2.3 Consider a parity-check matrix of a (8, 4, 4)2 extended Ham­
ming code:
H=
000 11110
0 0 110
0 11
1 01 0 10 10
1 11 11111
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

34
2 Coding for Erasures and Fountain Codes
(a)
Received 
word
Check 
registers
Decoded 
word
E \
E \
E \
0 •
0
0 V0 
°’\
0 ’\
0 •
0
1 v5^/0 c 1 1 • \1^ M
1 ’ \\ • 1 
1 •
• 1 
1
E \\®>0 c 2 E
\\’ 0
1 •
3’0 1 ■
• 0 
1
0 
>0c330 0 c 3 0’
35 0
0 •
V 0^ 0 •
• 0 
0
0 
//0C40 0 c 4 0 •
/ 0
0 •
1 1 0 •
z M 0
1 r/ 
1 ' /
1 ■ // 
1 • /
1
E S
E '
E /
E
1
(b) 
(c) 
(d)
Figure 2.3 Decoding the triple erasure-correcting extended Hamming (8, 4, 4)2 
code
As a code of distance 4 it is capable of correcting three erasures. Suppose the 
codeword c = (0,0, 1, 1,0,0, 1, 1) is transmitted on a BEC and the received 
word is y = (E, 0, 1,E,0, 0, 1,E) is received. The decoding process is shown 
in Figure 2.3.
Algorithms where messages are passed on graph edges are termed message­
passing algorithms. When the messages reflect a belief on the bit values 
involved they are termed belief propagation (BP) algorithms. A Gaussian 
elimination (GE) decoding algorithm is where decoding is achieved by solving 
a matrix equation via matrix reduction, involving received symbols and 
information symbols (packets). Such decoding algorithms are often optimum 
achieving a minimum probability of error - hence a maximum-likelihood (ML) 
algorithm - but suffer from a much higher complexity, often O(n3), than BP. In 
BP algorithms it is quite possible that the system of equations to be solved is of 
full rank - hence possesses a unique solution, yet the BP decoding algorithm 
fails because there is no appropriate check node. Hence BP algorithms are 
generally not ML.
Decoding algorithms that achieve linear decoding complexity are sought. 
For successful decoding of the Tornado codes it is necessary for there to 
be at least one suitable check node at each iteration and the analysis of the 
algorithm to compute the probability of complete decoding involves computing 
the probability of there being such a check node at each stage of decoding 
until completion. The construction of graphs that allow for such an analysis 
is needed - an overview of the following innovative approach of [11] that 
achieves this goal is discussed.
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

2.2 Tornado Codes and Capacity-Achieving Sequences
35
For a graph edge, define its left (resp. right) degree to be i if it is connected 
to a variable (resp. check) node of degree i. Thus all edges incident with 
a degree i vertex (either variable or check) are of edge degree i . Define a 
pair of degree sequences, (X 1,... ,Xm) for the left (variable) nodes and right 
(p 1,..., pm) for the right check nodes, with Xi the fraction of edges with left 
degree i and pi the fraction of edges with right degree i. It is convenient for 
the analysis to follow to define two polynomials
X(x) = 
Xi xi-1 and p(x) = 
pi xi-1.
The exponents of (i - 1) rather than i in these polynomials are an artifact 
of the analysis. Graphs with these edge degree fractions will be denoted (X, p) 
distribution graphs or equivalently Cn(X,p). Thus by a Cn(X, p) graph is meant 
some incarnation of a bipartite graph with some n variable nodes with these 
edge distributions. One can think of choosing a graph uniformly at random 
from this ensemble of graphs for analysis.
The case of (dv,dc) biregular bipartite discussed previously corresponds to 
X(x) = xdv-1, p(x) = xdc-1.
If E is the number of edges in the bipartite graph (previously used to 
indicate a coordinate position containing an erasure) then the number of left or 
variable nodes of degree i is given by EXi/i and the total number of variable 
nodes is then E i Xi/i, with similar comments for check nodes. The average 
left degree of nodes in the graph is
aL = 1 
Xi/i = 1 
X(x)dx
and the average right degree is
aR = 1 
pi/i = 1 
p(x)dx
The code rate is given by
R = k = 1 - (n - k) > 1 - E i Pi/i = 1 - aL = 1 - 0 P'x'dx 
n 
n 
ET^i/i aR f0 X(x)dx
with equality only if the check equations are linearly independent.
Our goal will be to determine conditions on these two-degree distributions 
that ensure, with high probability, the availability of degree one check nodes 
at each stage of decoding to allow the decoding algorithm to complete. Before 
considering this problem, it is noted that given such degree distributions it is 
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

36
2 Coding for Erasures and Fountain Codes
not difficult to give a random graph construction with such distributions, as 
follows.
To construct a Tanner graph corresponding to a code of length n = k + 
m, m = n - k and dimension k the number of left variable nodes is n and 
the number of check right nodes is m. Decide on an appropriate number of 
graph edges E which is the total number of 1’s in the parity-check matrix and 
is proportional to the complexity of the proposed decoding algorithm. For a 
given (k(x),p(x)) distribution the number of left (resp. right) nodes of degree 
i is Eki/i (resp. Epi/i). For the sake of argument assume all such quantities 
are integers. To construct a bipartite graph with the given edge distributions, 
imagine an array consisting of four columns of nodes, the first column is the 
set of n variable (codeword nodes) and the fourth column corresponding to 
the m check nodes. The second and third columns each have E nodes. From 
each left variable node of degree i, designate, for each i, Eki/i variable (first 
column) nodes to be of degree i and generate i edges emanating from each 
of them terminating in a total of Eki (disjoint) nodes in the second column. 
Similarly for each right check node of degree i, designate, for each i, Epi/i 
nodes to be of degree i and generate i edges from each of them terminating in a 
total of Epi (disjoint) nodes in the third column. The second and third columns 
each contain E nodes and so far are of degree one. A random permutation of 
order E is generated to join nodes in the second and third columns. The nodes 
in the first and fourth columns are now joined (in the sense of edges between 
them) and the nodes in the second and third columns can be deleted. There is 
a small probability that this procedure might generate a bipartite graph with 
multiple edges between nodes in the first and fourth columns. These may be 
removed and the effect on the probabilistic analysis is minimal. Similarly there 
is a probability the associated parity-check matrix will not be of full rank.
It is noted again that for such graphs with edge distributions, the central 
question is how to choose the distributions k(x) and p(x) in order to have the 
previous algorithm complete the decoding with high probability? Equivalently 
how to choose the distributions so that, with high probability, there is at least 
one degree one check node at each stage of the decoding. A formal analysis 
involving the use of martingales and differential equations is given in [7, 10] 
and will be commented on later. The following informal argument is instructive 
and gives an idea as to why the condition on the degree distributions developed 
arises [9].
Consider an edge e joining variable node v and check node ci-1 with 
left edge degree i and right edge degree j . Consider the graph generated by 
variable node v and all paths emanating from v with the first edge e for £ 
iterations, with one iteration being one traverse from a variable node to a check 
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

2.2 Tornado Codes and Capacity-Achieving Sequences
37
node and back again to another variable node. If the girth of the graph is greater 
than 4£, this graph will be a tree and all variable nodes encountered when 
traversing from v to the variable nodes (the leaves of the graph) at level 0 will 
have been erased on the channel independently, each with probability S.
Let pt be the probability that a variable node is not resolved (its value is not 
known) by level I. A recursion is developed for pt+1 and a condition found 
on the two edge distributions that ensures these probabilities are decreasing 
as the algorithm continues. The implication is that if the conditions stated 
remain satisfied as the number of iterations increases, the probability will 
vanish implying successful decoding. Suppose all variable node neighbors of 
check node ci -1 are resolved. Since each such node has the same probability 
of being resolved at level I, the probability they are resolved is (1 - pt)j-1. 
The probability the edge e has right degree j is pj and the probability that at 
least one such neighbor of ci-1 is unresolved is
1 - ^2 pj( 1 - p£)(j-1) = 1 - p( 1 — p£)
(hence the reason for the (i - 1) exponent in the polynomial definition). Now 
the variable node v at iteration £ + 1 will be unresolved only if all i - 1 check 
nodes of v at lower level in the tree have at least one lower-level variable node 
unresolved and since each edge joining v to these lower-level check nodes has 
left degree i with probability ki, the probability the variable node v remains 
unresolved at level £ + 1 is
pt+1 = S £ki( 1 - p( 1 - pt))(-1) = 8k( 1 - p( 1 - pt)) 
(2.1)
where p0 = S, the erasure probability on the channel, the initial condition 
that variable node v was unresolved (received as an erasure). Thus [28] 
successful decoding gives the condition that the probability a variable node 
being unresolved decreases with the level:
pt+1 = Sk( 1 - p( 1 - pt))<pt. 
(2.2)
Thus as the number of iterations £ increases the probabilities will tend to 0 
and decoding will be successful if the condition holds. In other words, finding 
edge distributions k(x) and p(x) that satisfy this condition will, with some 
probability, assure the completion of the decoding algorithm.
Alternatively, replacing pt by a variable x, if neighborhoods of a variable 
node of depth £ are trees (which assures the independent erasure condition 
holds), and if
Sk( 1 - p( 1 - x)) < (1 - e)x, 0 < x < S, 
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

38
2 Coding for Erasures and Fountain Codes
then after £ iterations the probability a variable node is unresolved is at most 
(1 - e)£S. Thus edge distributions p(x') and X(x) are sought that, for a given 
erasure channel S, satisfy the condition
SX( 1 - p( 1 - x))<x, 0 <x<S 
(2.3)
so that with high probability, decoding on a BEC channel with erasure 
probability at most S will be successful, i.e., the probabilities the nodes are 
unresolved decrease with iterations.
Notice that as a polynomial with positive coefficients X(x) is an increasing 
function on (0,1) and hence is invertible and letting x = SX( 1 - y) the 
condition can be written as
p( 1 - SX(1 - y)) > 1 - X-1 (X(1 - y)) = y on (0,1) 
(2.4)
and the equivalent (dual) condition to Equation 2.3 is [29]
p( 1 - SX( 1 - y)) > y, 0 < y < 1.
Equivalently by letting y = p-1 (1 - x) this equation can be written as
SX( 1 - p(y))< 1 - y, y e [0,1) 
(2.5)
(which is also obtained by substituting y = 1 - x in Equation 2.3). Degree 
distributions that satisfy these conditions for as large a value of S as possible 
are sought, where S is the probability of erasure on the channel. Thus with 
high probability, for (U(x), p (x)) distributions that satisfy these conditions, the 
decoding will complete for as high a channel erasure probability as possible. 
Indeed, distributions that result in a S satisfying a code rate R = 1 - S achieve 
capacity on a BEC.
The informal development for the condition in Equation 2.3 can be made 
formal. The approach in [7, 8, 10, 23, 24] is outlined. Let ¥() and r^ be the 
fraction of left (resp. right) edges at stage (time) t of the algorithm of degree 
i , where by fraction is meant the actual number of left (resp. right) divided 
by the original total number of edges E. A discrete time differential equation 
is developed in terms of the degree distributions and solved. In particular it is 
shown ([10], proposition 1) that the fraction of degree one right edges is
r1 (x) = SU(x)[x - 1 + p(1 - SU(x))]
where x is defined via dx/dT = -x/e(r) where e(T) is the fraction of edges 
remaining at time t . Thus the decoding continues as long as r 1 (t) > 0 which 
leads to the proposition:
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

2.2 Tornado Codes and Capacity-Achieving Sequences
39
Proposition 2.4 ([10], proposition 2) Let B be a bipartite graph that is chosen 
at random with edge degree distributions k(x) and p(x). Let S be fixed so that
p( 1 - Sk(x)~) > 1 - x, x e (0,1]. 
(2.6)
For all n > 0 there is some ko such that for all k > k0, if the k (left) message 
bits of C(B) are erased independently with probability S, then with probability 
at least 1 - k2/3 exp(-\/k/2) the recovery algorithm terminates with at most 
nk message bits erased.
Notice the proposition only gives an upper bound on the number of erasures 
remaining after the algorithm terminates.
The result can be used to determine suitable distributions to ensure there 
is at least one check node available of degree one for the completion of the 
algorithm. Returning to Tornado codes, the following conclusion can be shown 
as a condition on the degree distributions:
Theorem 2.5 ([10], theorem 2) Let k be an integer and suppose that
C=C(B0,...,Bm,C0)
is a cascade of bipartite graphs where B0 has k variable nodes. Suppose each 
Bi is chosen at random with edge degrees specified by k(x') and p(x) such that 
k i = k2 = 0 and suppose that S is such that
p( 1 — SA(x)) > 1 — x, 0 < x < 1.
Then, if at most a S-fraction of the coordinates of an encoded word in C are 
erased independently at random, the erasure-decoding algorithm terminates 
successfully with probability 1 - O(k-3/4) and does so in O(k) steps.
The O(k) complexity follows from the fact the average node degree is 
a constant. Recall that if the probability of an erasure on the BEC is S, 
the capacity of the channel, the maximum rate at which information can be 
transmitted through the channel reliably, is R = 1 - S, or conversely, the 
maximum-erasure rate that a code of rate R may be used reliably on a BEC is 
S = 1 - R. The following result shows this:
Theorem 2.6 ([10], theorem 3) For any rate R with 0 < R < 1, any r with 
0 < r < 1 and sufficiently large block length n, there is a linear code and a 
decoding algorithm that, with probability 1 - O(n-3/4), is able to correct a 
random (1 — R)( 1 — r)-fraction of erasures in time proportional to n In (1 /e).
Note that this result achieves the goal of a linear-time (in code length) 
decoding algorithm. Thus the challenge is to devise a distribution pair 
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

40
2 Coding for Erasures and Fountain Codes
(X(x),p(x)) such that the rate R of the corresponding code (the related 
bipartite graph) is such that the code is able to correct a fraction of 1 - R 
erasures, asymptotically on average. Distribution pairs that achieve this 
relationship are referred to as capacity-achieving sequences.
The average node degrees on the left and right, aL and aR, were shown to be
aL = | 
Xi/ij 
and aR = | 
Pi/i j
ii
The following theorem is of interest:
Theorem 2.7 ([27], theorem 1) Let G be a bipartite graph with distributions 
(X(x),p(x)) and let S be a positive number such that
SX( 1 — p( 1 — x)) < x, 0 < x < S.
Then
S < — ( 1 — (1 — S)aR). 
aR
It is also of interest to note ([27], lemma 2) that if X and p are polynomials 
satisfying the above with the above notation, then S < p' (1 )/X' (1). Thus the 
distribution pair determines the rate of the code and this result determines a 
bound on how close it will be to achieving capacity in the sense it gives a 
bound on the erasure-correcting capability of the code. In addition it is of 
interest to find good degree distributions so as to yield as large a value of 
erasure probability S as possible.
We return to the notion of a Tornado code which is a cascade of graphs 
each of whose distributions satisfy the above conditions. They are referred to 
as Tornado [2] as in practice it often occurs that as the substitution process 
progresses (recovery of variable nodes by check nodes of degree one), the 
decoding process typically proceeds slowly until the resolution of one more 
variable node results in the whole set of variable nodes being resolved quickly.
To summarize, the arguments that led to the conditions Equations 2.3 and 
2.4 say that under certain conditions on the edge distributions, the probability 
of a node not being resolved by a certain iteration decreases with the number 
of iterations and hence tends to zero and to complete decoding. The results 
depend only on edge distributions and hence can apply also to the Tanner 
graph representation of a code. The decoding algorithm is equivalent to that 
of Tornado codes and, as noted, the crucial property is to have check nodes of 
degree one with high probability at each iteration, which the stated edge degree 
distributions tend to fulfill.
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

2.2 Tornado Codes and Capacity-Achieving Sequences
41
The fact that the differential equation/martingale approach to the decoding 
problem and this rather different approach leads to the same conditions is an 
interesting confirmation of the results.
Examples of Capacity-Achieving Distributions
Numerous works give examples of pairs of distributions (k(x),p(x)) that 
satisfy the above conditions. Recall that the average left and right degrees of a 
graph with edge distributions (k(x),p(x)) are 1 / (Xi ki/i^ and 1 / (Xi pi/ii] 
(or 1 //01 k(x)dx and 1 / /01 p(x)dx^. The rate R of the code is at least 
1 - ( fo P(x)dx/ f o k(x')dx^ (depending on the corresponding matrix having 
linearly independent rows). It can be shown [28] that for given (k(x),p(x)) 
distributions satisfying condition of Equation 2.3, S is always less than or 
equal to 1 - R for R the rate of the resulting code derived from the edge 
distributions. For the formal definition of a capacity-achieving sequence we 
use the following:
1
kD(x) = H(D)Ylx,/i and pD(x) = e^(x 1) (D terms).
Here p. is the solution to the equation
1 (1 - e->) = 1-R (1 -^)
p 
H(D) D + 1
Definition 2.8 ([28], section 3.4) An edge distribution sequence (k(x),p(x)) 
is called capacity achieving of rate R if
(i) the corresponding graphs give rise to codes of rate at least R;
(ii) for all e > 0 there exists an n0 such that for all n > n0 we have
k( 1 - p( 1 - x)) <x for x e (0,( 1 - R)( 1 - e))
where n is the length of the probability distributions k and p.
Example 2.9 The first example of such distributions, cited in numerous 
works (e.g., [2, 10, 27, 28, 29]) is referred to as heavy-tailed/Poisson sequences 
for reasons that will become clear.
Let D be a positive integer (that will be an indicator of how close S can be 
made to 1 - R for the sequences obtained). Let H(D) = iD=1 1/i denote the 
harmonic sum truncated at D and note that for large D, H(D) ^ ln(D). The 
two distributions parameterized by the positive integer D are 
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

42
2 Coding for Erasures and Fountain Codes
and such edge distributions give a code of rate; at least R and note the average 
left degree is aL = H(D)(D + 1 )/D and /01 kD(x)dx = (1 /H(D))( 1 - 
1/(D + 1)). The right degree edge distribution is the truncated Poisson 
distribution. It can also be established that
1 — C)(—1 )k+1
8^d( 1 — Pd( 1 — x)) = 8Xd( 1 — e - ^x) 
- 8
< 
ln (e—)
8[ix
= H(D)'
For the right-hand side of the last equation to be at most x it is required that
8 < H(D')/^
(8 is the largest erasure probability possible on the channel possible for the 
conditions) which can be shown to be equal to
(1 — R)( 1 — 1 /(D + 1 ))/( 1 — e—»).
That this pair of distributions satisfies condition
(1 — R)( 1 — 1 /D)^d( 1 — Pd( 1 — x)) <x, 0 < x < (1 — R)( 1 — 1 /D),
is shown in [28]. (Note: Such codes are referred to as Tornado codes there in 
contrast to the definition of such codes used here.)
Example 2.10 Another example from [27, 28] is the following: Recall the 
general binomial theorem
(x + y)a 
nV''''.''' —
j
—=0 —
for a any real or complex number and the fractional binomial coefficients are 
given by
(
a \ = a(a — 1) • • • (a - (n — 1 ))/n! . 
n
Consider the right regular graphs with the check nodes all of degree a,fora 
positive integer a > 2 and the distributions
^a,n(x) —
vn-1 a n k +1 k 
k—k—1 U (—1) x
and pa (x) — xa 1
where a — 1/(a - 1) is real, positive and noninteger. Notice that the fractional 
binomial coefficients alternate in sign for a< 1 and hence the coefficients 
of the distribution ^a,n are positive. It is convenient to introduce a parameter 
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

2.2 Tornado Codes and Capacity-Achieving Sequences
43
v such that 0 < v < 1 by n = v-1 /a (ignoring integer constraints). It is 
shown in ([27], proposition 2 and theorem 2), that for the code rate defined as 
R = 1 — aL/aR and aR = a = (a + 1 )/a
—— > 1 — va+1 /a = 1 — vaR.
1—R
As v < 1 this suggests S « 1 — R for large aR, approaching capacity.
The properties of these distributions and the sense in which they are 
asymptotically optimal and satisfy the conditions such as Equation 2.3 or 2.4 
is discussed in [27]. Notice in this case the graphs are right regular - all right 
check nodes of the same degree.
Example 2.11 A variety of techniques have been developed to determine 
capacity-achieving sequences, including a linear programming approach and 
density evolution, a method used effectively in the analysis of LDPC codes 
(e.g., see [10, 34] and Chapter 3).
As an example, the following distribution pair given in [34] was found using 
density evolution:
X(x) = 0.29730x + 0.17495x2 + 0.24419x5 + 0.28353x19
p(x) = 0.33181 x6 + 0.66818x7
The Example 2.10 is interesting in that one is able to construct good 
sequence pairs with one of the pair being a monomial, i.e., all right nodes 
have the same degree (right regular). However, it is shown in [7] that sequence 
pairs which are both monomials (hence biregular graphs) perform poorly.
The relationship of these results on the BEC to those obtained for other 
channels, most notably the BIAWGN using belief propagation as discussed 
in Chapter 3, is of interest. To briefly note the approach taken there, the pdf 
pt of the log likelihood ratios (LLRs) (not the same pt used in the previous 
analysis) passed during the decoding process with the code graph described 
by the distribution pair ('k(x),p(x')) satisfied the recursion (Equation 3.28 of 
Chapter 3) is shown there to satisfy
pt = p0 ® X(r—1 (pV(pt—1))), t > 1
where ® indicates convolution and r is an operator introduced in Chapter 3 
that gives the pdf of it argument. It should be noted the arguments in that 
chapter will be likelihood ratios as messages passed on the decoding graph 
and these are random since they depend on received random messages.
Applying this equation to the erasure channel, the pdf [28] is a two-point 
mass function with a mass of pt at 0 and mass 1 — pt at rc>. Performing the 
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

44
2 Coding for Erasures and Fountain Codes
convolutions with such mass functions in the above equation can be shown to 
yield the result
P£ = SX( 1 - p( 1 - P£-1))
where, as before, S is the channel erasure probability. This is essentially the 
Equation 2.2 obtained by very different means, an interesting confirmation.
2.3 LT and Raptor Codes
The formulation of the Tornado codes of the previous section introduced 
(at least) two novel ideas: the notion of deriving edge probability distributions 
to generate bipartite code graphs where certain simple decoding algorithms 
could prove effective. In a sense to be discussed these ideas led to the notion 
of fountain codes.
To initiate the discussion, consider the following simple situation. A server 
has k information packets of data of some fixed number of bits to be 
downloaded over a network to a large number of users. The users are interested 
in receiving a complete set of the k packets. Packets can be XORed. One 
possibility to achieve this download is to simply forward the packets on the 
Internet with each user obtaining the packets in some order. However, due to 
imperfections in the Internet such as buffer overflow or node servers failing, 
it is likely some users will miss one or several of the packets leading to the 
requirement of some feedback mechanism to the source where each user is able 
to request retransmission of their missing packets. This can lead to inefficient 
operation and congestion (implosion) on the network.
A more interesting possibility is for the server to first code, each coded 
packet consisting of the XOR of a random selection of information packets. 
This notion will be the basis of fountain codes.
Consider first a (not very efficient) case of random fountain coding where 
each of the k information packets is included in the formation of a coded packet 
with a probability of 1/2 independently and transmitted on the Internet, i.e., the 
coder chooses uniformly at random a selection of information packets, XORs 
them together to form a coded packet which is transmitted on the network.
A user now must gather a sufficient number of any of the coded packets to 
allow the solution of the set for the original information packets. In effect the 
user must gather a set of (k + m) coded packets for m a small positive integer 
to allow for a full-rank random k x (k+m) matrix equation to be formed which 
is necessary to guarantee solution. The fact that any of the coded packets can 
be used is a positive feature of the scheme. However, solving for the original 
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

2.3 LT and Raptor Codes
45
k information packets involves the solution of a k x (k + m) binary random 
matrix, say by Gaussian elimination, an operation that is O(k3 ) in complexity 
which, since k might easily on the order of several thousand be far higher than 
is typically of interest. By Equation A.8 of Appendix A the probability such a 
matrix is of full rank, Qk, k+m (hence solvable by Gaussian elimination), is
0.999 < Qk,k +10 < 0.999 + e, e < 10-6.
Such codes are generally referred to as fountain codes for the obvious reason. 
They are also referred to as rateless codes as their design involves no particular 
rate in the sense of the design of block codes.
A possible remedy to the large complexity of the Gaussian elimination 
decoding argument might be as with the Tornado codes and form a coding 
graph in the obvious manner. The headers of the coded packets contain the 
information as to which information packets were XORed to form the coded 
packet allowing the code graph to be formed consisting of information nodes 
on the left corresponding to information packets (nodes) and code packets 
(nodes) on the right. There is an edge between an information packet and a 
code packet if the information packet is involved with the formation of the 
code packet. The following decoding algorithm is as for Tornado codes.
Algorithm 2.12 If there is a coded node of degree 1, XOR the contents of the 
coded node to the neighbor information node thus resolving that information 
node. XOR this information node to its other neighbor coded nodes and delete 
all edges involved (decreasing the number of unresolved information nodes by 
one and the number of coded nodes not yet used by at least one).
This algorithm is simple with linear decoding complexity. Notice that if the 
k x (k + m) matrix formed, corresponding to the graph, is of full rank (k), 
Gaussian elimination is guaranteed to provide a solution for the k information 
packets. On the other hand the decoding algorithm above, while very simple 
with linear complexity, is unlikely to run to completion and will almost 
certainly fail.
The remedy for this situation is not to choose the packets for XORing to 
form a coded packet uniformly at random but rather formulate a probability 
distribution for the number of information packets to be XORed, much as for 
a single section of the Tornado code algorithm, so that with high probability, 
the previous simple decoding algorithm will be successful, i.e., at each stage 
of decoding find a check node of degree one all the way to completion. With 
such a distribution the simple (linear complexity) decoding algorithm can be 
used rather than the computationally expensive Gaussian elimination. This is 
the genius behind the LT (Luby transform) [6] fountain codes discussed below.
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

46
2 Coding for Erasures and Fountain Codes
LT Codes
The approach of Tornado codes and capacity-achieving sequences of the 
previous section suggests the following possibility [6, 31]. As above, assume 
there are k information packets to be coded (to give coded packets). A discrete 
probability distribution (the literature typically uses Q(x) for this distribution, 
a convention which is followed here)
k 
k
&i, i = 1,2, ...,k, ^2 Qi- = 1, 
Q(x) = ^2 '^ixi
is to be chosen and used in the following manner to form coded packets: for 
each coded packet, the distribution is first sampled - if d is obtained (with 
probability lid, d e [k]) - then d distinct information packets are chosen 
uniformly at random from among the k and XORed together to form a coded 
packet to be transmitted on the network. The choice of packets is stored in 
the coded packet header. The process is repeated as required. As noted, such a 
coding process is referred to as a fountain code or as a rateless code since there 
is no specific rate associated with the process of forming coded words. It is the 
form of the distribution that restores the complexity of decoding to linear and 
makes the above decoding algorithm effective.
The receiver gathers slightly more than k, say k + e(k) (to be discussed), 
coded packets from the network and forms the bipartite graph with k informa­
tion packet nodes (considered as initially empty registers), on the left, denoted 
xi,x2,...,xk (to be determined by the decoding algorithm) and k + e(k) 
received coded packets on the right, denoted y 1, j2,... ,yk+^k).
The problem is to choose the distribution Q(x) to have the probability the 
decoding algorithm is successful - runs to completion. Clearly at least one 
check node of degree one is required at each stage of decoding until all infor­
mation symbols are found. If this is not the case the decoding algorithm fails. 
Clearly a balance is needed in the sense a sufficient supply of degree one coded 
nodes is required at each stage to make the decoding algorithm robust and suc­
cessful while too many degree one nodes will make it inefficient and give poor 
performance. The analysis is somewhat intricate and the reader is referred to [6, 
31, 33] for details. The essence of the results is described here without proofs.
To discuss the situation further, the behavior of the algorithm as it iterates - 
at each stage finding a coded node of degree one, transferring the coded packet 
associated with such a node to the connecting information node and then 
XORing that packet to all neighbor check nodes and removing the information 
node and all its edges. The terms “node,” “symbol” and “packet” are used 
interchangeably.
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

2.3 LT and Raptor Codes
47
Some definitions are needed. The number of coded symbols gathered for 
decoding, beyond the number of information symbols, is termed the overhead 
of the decoding algorithm. Thus if k( 1 + ek) coded packets are used the 
overhead is ke. A simple argument ([33], proposition 2.1) shows that if an 
ML decoder is to succeed with high probability for an overhead fraction e that 
tends to zero with k, then 1 has to converge to zero. At stage i of the algorithm 
define the output ripple (or simply ripple) as the set of coded nodes of degree 
one. At this stage, since one information symbol (packet) is released at each 
stage of the algorithm, there are k - i undecoded (or unresolved) information 
nodes remaining at stage i . A coded node is said to be released at stage i + 1 
if its degree is greater than one at stage i and equal to one at step i + 1. To 
calculate the probability [30, 31, 33] a coded node of initial degree d is released 
at stage i + 1 it is assumed the original neighbor nodes of a coded node are 
chosen with replacement - convenient for the analysis and having little effect 
on the code performance. The probability a coded node, originally of degree d, 
has only one neighbor at stage i + 1 among the k - (i + 1) information nodes 
not yet recovered and that all of its other (d - 1) information node neighbors 
are in the set of recovered information nodes is
/ k - (i + 1) \ / i + 1 \ d-1 
d ------------- ------
kk
(this is where the choosing with replacement assumption appears).
Another way of viewing this formula is to reverse the situation and suppose 
the set of variable (information) nodes resolved at the i-th iteration is Vi, 
| Vi |= i and at the (i + 1)-st iteration is Vi+1 D Vi, | Vi+1 |= i + 1. We 
ask what is the probability the edges of a constraint (coded) node c of original 
degree d are chosen so it is released at the (i + 1)-st iteration with these sets 
of resolved variable nodes. Thus at iteration i there are at least two variable 
(information) neighbors of c unresolved and at iteration i + 1 there is only 
one unresolved - c is in the output ripple at that stage. There are d ways of 
choosing the edge from c that will be unresolved at the (i + 1)-st iteration and 
the probability it is unresolved then is ^1 - ir1)- For the constraint node c to 
be released at the i-th iteration it has (d - 1) edges attached to resolved nodes 
at the (i + 1)-st iteration but not all of these were resolved at the i-th iteration. 
The probability of this is
(
/ i + 1 \ d-1 
/ i \ d-1\
I ------- I — 
.
kk
Thus, as in the above formula, the probability a coded node, originally of 
degree d, is released at stage i + 1 of the decoding is approximately
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

48
2 Coding for Erasures and Fountain Codes
Multiplying this expression by Qd and summing yields
Pr(output symbol is released at stage i + 1)
Recalling the approximation for a smooth continuous function (like polynomi­
als) f"(x) « (f'(x + A) - f'(x))/A for small A it is clear that
Q' — - Q'(-) 
kk
«
and the expected number of coded nodes released at step i + 1isn times this 
amount (n = k + e(k), the number of coded packets collected)
n 
k
i + 1 
k
1
(2.7)
If one sets x = i/k, if the probability the last coded symbol is released at 
stage k + e(k) = n is one, this is equivalent to the equation
(1 — x)Q"(x) = 1, 0 < x < 1
which, with the required condition that Q( 1) = 1, has the solution
Q(x) = V—x---- .
fe i(i + 1)
This is referred to as the limited degree distribution [33]. Clearly this distribu­
tion is useless for our purpose since it produces no coded nodes of degree 
one and decoding could not start. In addition, the distribution is infinite - 
the needed distribution should produce no coded nodes of degree greater 
than k, the number of information nodes assumed. The analysis, however, is 
instructive. The following modified distribution is suggested [6]:
Definition 2.13 The (ideal) soliton distribution is defined as
aso
1 k,
i(i -1),
i=1
i= 2, 3 , .. . ,k.
(2.8)
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

2.3 LT and Raptor Codes
49
The term soliton arises in a refraction problem in physics with similar 
requirements. The distribution is associated with the polynomial
Qsol(x) = x + V—x---- .
( ) k + “ (i - 1 )i
That this is a distribution (probabilities sum to unity) follows readily by an 
induction argument or by observing that 1/(i(i - 1)) = 1/(i - 1) - 1/i.
Notice that this distribution has a coded node of expected degree
V k>'s° = - + V i---- 1----= V - « ln(k)
1 - a = 1 - k( 1 - or f 1 - 
= a
kk
and in the limit
lim (1 - -) 
—> exp (-n/k) ^ a/k or n ^ k log(k/a).
k —>to 
k
Thus for the encoding process to cover each information symbol at least once 
with probability at least 1 -a, the average degree of the approximately k coded 
nodes must be at least log(k/a).
As discussed below, the ideal soliton distribution will prove to be unsatis­
factory for several reasons. One problem is that the variance of the number 
of coded nodes in the ripple at each stage of the decoding is so large that 
the probability there is no node of degree one at each stage is too high. 
Nonetheless it does have some interesting properties. If the probability that 
a coded node of initial degree i is released when there are L information nodes 
remaining unrecovered is denoted r(i, L) and r(L) is the overall probability of 
release [6],
i k i(i - 1) i
which is the first k terms of the harmonic series which is well approximated by 
ln(k) for large k. This is the minimum possible to give a reasonable possibility 
of each information node being involved with at least one coded node in the 
following sense. The problem is equivalent to throwing n balls (the coded 
nodes) into k cells (the information nodes) and asking for the probability 
there is no empty cell (although with replacement). This is just one less the 
probability at most (k - 1) cells are occupied which is
If it is wished to have a probability of 1 - a to have no cell empty (all 
information nodes covered) is
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

50
2 Coding for Erasures and Fountain Codes
r(L) = r(i,L).
Then for the ideal soliton distribution it is shown ([6], proposition 10) that 
r(L) = 1/k and the probability a coded node is released at each stage of 
decoding is r(L) = 1/k, i.e., the probability of release is the same at each 
stage of decoding.
For the following let n (S in [6], used for erasure probability here) be the 
target probability the decoding algorithm fails to complete decoding. In an 
effort to improve the performance of the ideal soliton distribution the robust 
soliton distribution was proposed, defined as follows ([6], definition 11):
Definition 2.14 The robust soliton distribution denoted £2rs is defined 
using the ideal Soliton distribution and the function Ti as follows. Let R = 
cs/k ln(k/n) for a constant c > 0 and define
R/ik for i = 1,2,...,k/R- 1
Ti
R ln(R/n)/k for i =k/R
0 
fori = k/R + 1,...,k.
Then the robust soliton distribution is
Qrs = (QS°1 + Ti)/p, 
i = 1,2,... ,k 
(2.9)
for the normalizing constant fi = ^\k=i(^io + Ti).
The rationale for this distribution is that it tends to maintain the size of the 
ripple, the number of coded nodes of degree one at each stage of decoding, 
preventing the size to fall to zero at any stage before the end - which would 
lead to decoding failure.
The distribution chosen for the formation of the coded packets is critical 
for performance of the decoding algorithm. The relationship between the 
distribution and the number of extra coded packets (beyond the number of 
information packets k) required to achieve a given error probability (the 
probability the decoding fails to complete) is complex. For the Robust Soliton 
distribution it is shown ([6], theorems 12 and 17) that for a decoder failure 
probability of n an overhead of K = k + O(jk • ln2 (k/n)) is required and 
that the average degree of a coded node is O(ln(k/n)) and that the release 
probability when L information nodes have not been covered is of the form 
r(L) > L/((L - 0R)K),L > R for some constant 0 and R as in the above 
definition.
It has been observed above the average degree of a coded node under the 
Robust Soliton distribution is O(log(k)) (for constant probability of decoding 
error n) and hence each coded symbol requires O(log(k)) operations to
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

2.3 LT and Raptor Codes
51
produce it and the decoding operation is of complexity O(klog(k)). For very 
large k a decoder with linear complexity would be more desirable and this will 
be possible with the Raptor codes discussed in the next section.
The literature on the analysis of aspects of decoding LT codes is extensive. 
The references [4, 13] are insightful. A detailed analysis of the decoding error 
probability is given in ([31], section 3).
Shokrollahi [30] defines a reliable decoding algorithm as one that can 
recover the k information symbols from any set of n coded symbols and errs 
with a probability that is at most inversely polynomial in k, i.e., a probability 
of the form 1/kc . Recall the overhead of a decoding algorithm is the number 
of coded symbols beyond k needed to achieve the target probability of error. 
A random LT code is one with a uniform distribution on the choice of 
information symbols to combine for a coded symbol. This corresponds to the 
choice of the distribution
S2(x) = (1 + x)k • (1 /2k)
and corresponds to the random analysis given earlier in this section. It can be 
shown ([30], propositions 1 and 2) that for any LT code with k information 
symbols there is a constant c such that the graph associated with the decoder 
has at least ck log(k) edges and that any random LT code with k information 
symbols has an encoding complexity of k/2 and ML decoding is a reliable 
decoder with an overhead of O(log(k)/k). ML is Gaussian elimination for the 
k x (k + O(log(k)/k)) matrix formed at the decoder with complexity O(k3). 
As noted previously in general BP (the algorithm described above) is not ML - 
indeed it is possible that if K coded symbols are gathered that the k x K matrix 
can be of full rank k (for which ML decoding would be successful) and yet the 
BP algorithm fail to completely decode (run out of coded nodes of degree one 
before completing). However, the low computational requirements of the BP 
decoding algorithm and its error performance are impressive.
The implication of these comments is that it will not be possible to have 
a reliable LT decoder and achieve linear time encoding and decoding. The 
development of the Raptor codes of the next section shows how this goal can 
be achieved by a slight modification of LT codes.
Raptor Codes
The idea behind Raptor codes (the term is derived from RAPid TORnado) 
is simple. It was observed that decoding failures for LT codes tend to occur 
toward the end of the decoding process when there are few information 
symbols left unresolved. This suggests introducing a simple linear block 
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

52
2 Coding for Erasures and Fountain Codes
erasure-correcting code to correct the few remaining erasures after the LT 
decoding fails.
For Raptor codes the k information symbols/packets are first precoded with 
an efficient simple (easy to decode) linear block erasure-correcting Cn = 
(n,k,d)2 code and then these n packets are LT coded. Note this introduces 
(n — k) parity packets - i.e., the parities of the code Cn are formed across 
the k information packets. For decoding, some n( 1 + e) of the LT coded 
packets are gathered for LT decoding, as before. Using the LT decoding on the 
precoded bits, the decoding might (typically) stall with a few uncoded packets 
left undecoded (no more coded nodes of degree one in the ripple). These can 
then be decoded with the linear erasure-correcting code Cn . It is also desired 
to have linear coding and decoding complexity and because of the reduced 
requirements on the LT decoding, this will be possible. Recall from the above 
discussion that a reliable decoding algorithm for LT codes must have at least 
O(log k) information node degrees and an overhead of O(log(k)/k).
Much of the development and commercial deployment of Raptor codes 
is due largely to the work of Michael Luby and Amin Shokrollahi and their 
colleagues although the key idea behind them was independently found in the 
work of Maymounkov [15, 16]. Both of the papers [30] and [15] are important 
reading.
A Raptor code then has parameters (k, Cn, £2 (x)) where Cn is a binary linear 
(n,k,d)2 erasure-correcting code and Q(x) is a distribution on n letters. For 
simplicity is used in the remainder of this chapter or &.D when the parameter 
D is needed. The coordinates of the code Cn will be an (n,k,d)2 code and will 
produce n intermediate packets from the k information packets and (n — k) 
parity packets. This code is not specified further - as noted its requirements 
are minimal in that a variety of simple codes, capable of correcting a few 
erasures with linear complexity, can be used. It is used essentially to correct 
a few erasures if the LT decoding leaves a few nodes unresolved (viewed as 
erasures) toward the end of its decoding process. That is, if the LT decoding 
does not quite complete to the end, leaving a few unresolved packets, the code 
Cn completes the decoding, treating the unresolved packets as erasures. The 
output of the LT code will be referred to as the coded packets and (n, ^D(x)) 
LT coding, as before.
It will be important to recognize that while LT codes cannot have linear 
decoding complexity, using the LT decoding process with an appropriate dis­
tribution to decode up to a few remaining erasures can have linear complexity.
As before, a reliable decoding algorithm for the Raptor code will have a 
probability of error of inverse polynomial form, 1/kc for some positive con­
stant c. Such codes will be analyzed with respect to their space requirements, 
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

2.3 LT and Raptor Codes
53
overhead and cost or complexity. The two extremes of such codes are the LT 
codes where C is the trivial (k,k, 1)2 code and the precode only (PCO) code 
where there is no LT code. The decoding cost of such a code is the expected 
number of arithmetic operations per information symbol. The overhead of 
the code is the number of coded symbols beyond k needed to recover all 
information symbols. The Raptor code (k,Cn,QD(x)) is thought of in terms 
of three columns of nodes or symbols (the left part of the previous figure):
• a left column of k information packets (associated with information 
packets);
• a middle column of intermediate packets consisting of the k information 
packets and (n - k) check packets computed using the linear 
erasure-correcting code Cn = (n,k,d)2;
• a right column of N = n( 1 + e) coded packets consisting of LT coding of 
the intermediate packets using the distribution &D(x).
A representation of the Raptor encoder/decoder is shown in Figure 2.4.
In analyzing Raptor codes it will be assumed ([30], proposition 3) the 
precode Cn can be encoded with complexity pk for some constant p > 0 and 
that there is an e > 0 such that Cn can be decoded over a BEC with erasure 
probability 1 - R( 1 + e) with high probability with cost/complexity Yk. With 
such assumptions Raptor codes will be designed with constant encoding and 
decoding cost per symbol (hence both of overall complexity O(k)), and their 
space consumption is close to one and their overhead close to zero.
kn
information intermediate 
packets packets
n( 1 + c) n LT decoded k decoded
LT coded intermediate information 
received packets packets packets
_ with erasures
Cn encode
= (n,k,d)2
^
LT encode 
(n, Qd(x)) 
^
"T “*
Raptor encoder
Raptor decoder
Figure 2.4 Basic structure of a Raptor code (k, Cn,So(x)) system - encoder/ 
decoder - graph edges not shown for clarity
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

54
2 Coding for Erasures and Fountain Codes
We discuss the section VI of [30] on Raptor codes with good asymptotic 
performance. The arguments needed to achieve linear encoding and decoding 
require a delicate balancing of parameters.
The distribution used for the LT coding ([30], section VI) is a slightly 
modified ideal soliton:
1 
D X XD+1 D
&D(x) = 
/XX + ^ —-----— + 
V ^xl (2.10)
X + 1 
“2 i(i - 1) D 
=
where x = X/2) + X/2)2 and D = [4(1 + e)/e"|. That this is in fact a 
distribution is seen by observing
D 1 D / 1 
1 \ 
1
y------------- V (---------------- ) = 1 —
= i(i-1) 
= i-1 i 
D
and hence it is a distribution for any positive number X and positive integer 
D>2. It will be shown later that for constant D this distribution will yield 
coded nodes with constant average degree (as opposed to being a function of 
k ) which allows the overall algorithm to have a linear complexity. With this 
distribution we have:
Lemma 2.15 ([30], lemma 4) There exists a positive real number c (depending 
on e) such that with error probability at most e-cn any set of (1 + e/2)n + 1 
LT coded symbols with parameters (n, ^D(x)') are sufficient to recover at least 
(1 - S)n intermediate symbols via BP decoding where S = (e/4)(1 + e).
The proof of this important lemma is straightforward although it requires 
some interesting manipulations. Some discussions are given to assist - the 
lemma itself is not proved.
The right-degree node (packet) distribution is by assumption &D(x) and 
hence the right edge degree distribution is
w(x) = ^D (x)
.
V d( 1)
Recall there are n input (intermediate - or left nodes for this argument) 
nodes to the LT coding process and N = (1 + e/2)n + 1 coded (right) nodes 
for the decoding. To compute the left edge degree distribution i(x), let a be the 
average degree of an output node (LT coded) node which is a = D( 1). Let u 
be a left node and w an output (right) node. The probability that u is a neighbor 
of w, since on average w has a input neighbors, is a/n. Assuming each output 
node has this same probability of having u as a neighbor independently, on 
average the degree of u will have a binomial distribution, i.e., the probability 
u has degree £ is given by
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

2.3 LT and Raptor Codes
55
N\ / a \ £ / 
a \ N -1
In 
n
N = n( 1 + e/2) + 1
and the generating function of the degree distribution of the left input nodes 
is thus
N - £
x£
a(1 - x)
n
(2.11)
The edge distribution is i(x) where i(x) = L'(x)/L'(1) and
i(x) =
a( 1 - x)\(1+e/2)n
n
For these edge distributions Equation 2.5 is equivalent to the condition that
i( 1 - v( 1 - x)) < 1 - x for x e [0,1 - 5].
Sincelimn^-v(1-b/n)n —> exp(-b) andthatforO < b < m, (1-b/m)m < 
exp(-b) then
I o' h A (1+Q/2)n 
a U (1 - x) 
l( 1 - “(1 -x)) = v - n-fcrr)
(
a &D( 1 - x)
- n-sDnr(1+,J2 n
< exp(-(1 + e/2)Q'D( 1 - x)), since a = 'D( 1).
The above condition then reduces to showing that
exp ( - (1 + e/2)QD(x)^ < 1 - x, x e [0,1 - 5]. 
(2.12)
To establish this inequality using the form of D(x') of Equation 2.10 is a 
technical development [10], which completes the proof of Lemma 2.15.
The analysis implies ([31], p. 79) that, asymptotically, the input ripple (the 
expected fraction of input symbols connected to LT coded symbols of degree 
one when a fraction of input symbols that have been recovered is x), is given by
1 - x - exp( - (1 + e)QD(x^. 
(2.13)
The analysis of [9] further gives the fact that if x0 is the smallest root 
of Equation 2.13 in [0, 1), then the expected fraction of input symbols not 
recovered at the termination of the decoding process is 1 - x0. Thus if 
distributions are designed so that x0 is maximized then [30] the associated 
Raptor codes will have an average coded node degree of O(log(1 /e)) a linear 
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

56
2 Coding for Erasures and Fountain Codes
decoding complexity of O(k log(1 /e)) and a decoding error probability which 
decreases inversely polynomial in k .
The conditions on the choice of erasure-correcting code Cn are simple:
(i) The rate of Cn is at least (1 + e/2)(1 + e).
(ii) The BP decoder for Cn used on a BEC can decode to an erasure 
probability of 5 = (e/4)/(1 + e) (which is half the capacity) and has 
linear decoding complexity.
The details are omitted. The key theorem for Raptor codes is the following:
Theorem 2.16 ([30], theorem 5) Let e be a positive real number, k an integer, 
D = [4(1 + e)/e], R = (1 + e/2)/(1 + e), n = k/R and let Cn beablock 
erasure code satisfying the conditions above. The Raptor code (k,Cn,QD(x)) 
has space consumption 1 /R, overhead e and cost O(log(1 /c)) with respect to 
BP decoding of both the precode Cn and LT code.
Lemma 2.15 shows the LT code with these parameters is able to recover at 
least a fraction of (1 - 5) of the intermediate symbols and the erasure code is, 
by assumption and design, to correct this fraction of erasures to recover the k 
information symbols. It remains to show the cost is linear in k . The average 
degree of coded nodes is (from Equation 2.10)
(
DI n \
v- + 12 - + —yr ) 
i=1 
/
= 1 + H(D)/( 1 + /x) = 1 + ln 11 • 4 (1 + e)) + 1 = ln 11) + O(e)
where the standard upper bound on the truncated Harmonic series H(D) < 
ln(D) + 1 has been used. Thus the cost of encoding the LT code is O(ln(1 /c)) 
per coded symbol which is also the cost of LT decoding and also the cost of 
decoding the code Cn by assumption. Notice the overhead of the LT code is 
approximately e/2.
It is noted again that the linear encoding/decoding here is achieved by 
relaxing the condition that the LT have a high probability of complete 
decoding - it is sufficient to achieve almost complete decoding and complete 
the task with the linear code. This allows the average graph degree to be 
constant rather than linear in log(k).
It has been observed [11] that if the graph that remains of the LT decoding 
algorithm above when it stalls (all constraint node degrees greater than 1) is 
a good enough expander graph, then a hard decision decoding algorithm can 
proceed and with high probability complete the decoding process - thus all 
errors can be corrected. A brief informal explanation of this “expander-based 
completion” argument is given as it will also apply to certain LDPC arguments.
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

2.3 LT and Raptor Codes
57
A bipartite graph has expansion (a,fi) if for all variable node subsets S of 
size at most a, the set of neighbor constraint nodes is at least p | S |. For such 
codes/graphs the following two simple decoding algorithms are proposed [36]:
Algorithm 2.17 (Sequential decoding) If there is a variable node that has 
more unsatisfied constraint neighbors than satisfied, flip the value of the 
variable node. Continue until no such variable exists.
Algorithm 2.18 (Parallel decoding) In parallel, flip each variable node that 
is in more unsatisfied than satisfied constraints and repeat until no such nodes 
exist.
It can be shown ([11], lemma 2, [36], theorems 10 and 22) that if the 
graph remaining after LT decoding as discussed above is an (a,p) and is a 
good enough expander (p > 3/4 + e), then the above parallel and sequential 
decoding algorithms will correct up to an errors in linear time, n the number 
of variable nodes. Comments on the probability the remaining graph will be 
such an expander are given in [11] - and hence this switching of decoding 
algorithms after the LT decoding will lead to complete decoding.
A brief interesting heuristic discussion of the analysis of the decoding 
performance of Raptor codes from ([30], section VII) is given. The interme­
diate and coded edge degree distributions have been noted as i(x) and v(x), 
respectively. Let pi+1 be the probability an edge in the decoding graph is of 
right degree one at iteration i . The analysis of Section 2.2 shows that
Pi+1 = v( 1 - i( 1 - pi)) 
(2.14)
and it is desired to design the distribution Q(x) such that this quantity 
decreases. Equation 2.11 giving the binomial moment generating function 
(mgf), for large n, can be approximated with the standard Poisson mgf
i(x) = exp (a(x — 1))
where a is the average degree of an input node. This function is also the input 
node degree distribution since it is easily verified that
i(x) =
i' (x)
.
i ' (1)
This analysis required the tree involved to have no cycles and the variables 
involved being independent. Let ui denote the probability that an input symbol 
is recovered at step i of the LT decoding algorithm. Then given an unresolved 
input node of degree d (note input node degrees only change when the node is 
resolved), then
ui = 1 - (1 - pi)d
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

58
2 Coding for Erasures and Fountain Codes
and averaging over the input degree distribution gives
Ui = 1 - i( 1 - pi) = 1 - exp(-api).
Hence pi = - In(1 - ui)/a.
From this relation and Equation 2.14 write
Pi+1 = ^( 1 - exp(1 - (1 - pi))) = rn( 1 - exp(-api))
= rn( 1 - exp (- a {- In (1 - ui)} /a)) = rn( 1 - (1 - Ui))
= M(ui)
and so, since arn(x) = (1 + e)QD(x) it is argued [30, 31, 33] that, for k( 1 + 
e) coded (output) nodes the expected fraction of symbols in the input ripple 
is given, when the expected fraction of input nodes that have already been 
recovered is x ,by
1 — x — exp ( — (1 + ()&d(x)}.
It is further argued this fraction should be large enough to ensure continued 
operation of the decoding algorithm to completion and it is suggested as a 
heuristic that this fraction satisfy
1
x - exp - (1 + €)Qd(x) I > cJ 1—
x e [0,1 - 5], 5 > c/Vk.
Probability distributions that satisfy this criterion are found to be similar to the 
Soliton distribution for small values of d and give good performance.
This has been a heuristic overview of aspects of the performance of Raptor 
codes. The reader is referred to the literature for more substantial treatments 
on both the performance analysis and code design using such techniques as 
density evolution and linear programming, in [4, 5, 7, 8, 10, 11, 13, 14, 14, 18, 
19, 21, 22, 24, 27, 29, 30, 31, 33, 34, 40].
The remainder of the section considers aspects of Raptor codes that prove 
important in their application, including the notion of inactivation decoding, 
the construction of systematic Raptor codes and their standardization.
Inactivation Decoding of LT Codes
It has been noted that BP decoding of LT codes is very efficient but is clearly 
not ML since it is possible to construct a situation where the decoding process 
will not complete due to the lack of degree one nodes, yet the decoding 
matrix formed from the information symbols and received coded symbols is 
of full rank - and hence ML, which is Gaussian elimination, would yield the 
unique solution. For a k x (k + m) matrix, ML would have complexity O(k3) 
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

2.3 LT and Raptor Codes
59
which for the file and symbol sizes considered here, would be far too large. 
Inactivation decoding seeks to combine the two techniques in a manner that 
can significantly decrease the overhead and probability of failing to decode for 
a slight increase of decoding complexity. It takes advantage of the efficiency 
of the BP decoding and the optimality of ML decoding to yield an efficient 
version of an ML algorithm.
The following description of the inactivation decoding procedure is slightly 
unconventional but intuitive. Consider a first phase of BP decoding on the 
bipartite graph representing n coded symbols (received, known) and k informa­
tion symbols whose values are sought. To start with, the bipartite graph of the 
set of k unknown information nodes (registers) is on the left and n = k( 1 + e) 
received coded nodes (registers) is on the right. A coded node of degree one 
is sought and if found, its value is transferred to the attached information node 
and its value is then added (XORed) to all its right neighbors. The associated 
edges involved are all deleted. The process continues until no coded node of 
degree one is available. In this case suppose a coded symbol of degree two is 
available (in a well-designed LT code this will often be the case). Referring to 
Figure 2.5, suppose the register associated with the node contains the symbol 
ri3 . If the information nodes that are neighbors are u and v, then create a 
variable x 1 (an inactivation variable) and assign a variable ri3 ®x 1 to u and x 1 to 
v. The variable value ri3 ®x 1 is then added symbolically to all coded neighbors 
of u and x1 to all neighbors of v and the BP process is continued as though 
the value of the variables are known. The process is shown in Figure 2.5. 
The process continues and each time no node of degree one is available, a 
unknown 
received
information 
coded
packets 
packets
ri1
ri2
ri3
ri4
ri5
x1
x 1 ® ri 3
unknown 
received
information 
coded
symbols 
symbols
x 1 ® ri2 ® ri 1
x 1 ® ri2
ri3
ri2 ® ri4
x1 ® ri5
Figure 2.5 One stage of inactivation decoding
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

60
2 Coding for Erasures and Fountain Codes
sufficient number of variables are introduced to allow the process to continue. 
At the end of the process each information node is ’’resolved” - although many 
may contain unknown variables. Typically the number of unknown variables 
is small. Suppose z inactivation variables have been introduced. Using the 
original received values of the right nodes and the values assigned to the 
information nodes, it is possible then to set up a linear relationship between 
the nodes on the left containing inactivation variables and the known received 
values on the right. These equations can then be solved, a process equivalent 
to solving a z x z matrix equation of complexity O(z3). If the original 
received nodes are equivalent to a full-rank system, the process is guaranteed 
to be successful and is ML. However, since the number of inactivation 
variables is typically quite small, this use of a combination of BP and ML 
is typically far more efficient than performing ML on the original received 
variables.
The above discussion gives an intuitive view of inactivation decoding. More 
efficient versions are found in the literature, including [1, 12, 20, 33] as well 
as the original patent description in [26].
Systematic Encoding
There are many situations where a systematic Raptor code is beneficial, i.e. one 
that with the usual Raptor coding produces the original information symbols 
among the coded output symbols. A natural technique might be to simply 
transmit all the information symbols first and then the usual Raptor coded 
symbols. This turns out not to be a good idea as it gives very poor error 
performance and requires large overheads to make it work at all. A convincing 
mathematical proof that this is so is given in [31].
A brief description of the more successful approach of [30] is noted. 
The terminology of that work is preserved except we retain our notation 
of information, intermediate and coded symbols/packets for consistency. As 
before a packet is viewed as a binary word of some fixed length and the only 
arithmetic operations employed will be the XOR of words or packets of the 
same length. The approach is to determine a preprocessing of the information 
packets in such a way as to ensure the original information packets are among 
the Raptor encoded packets. Thus the Raptor encoding method, and the impact 
of the distribution derived, is not disturbed ensuring that its promised efficiency 
is retained. The process is somewhat involved.
First consider the usual Raptor encoding. Let x = (x1, x2,...,xk) denote 
the k information packets and consider the Raptor code (k,Cn,&D(x)). Let G 
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

2.3 LT and Raptor Codes
61
be a binary (fixed) n x k generator matrix of the precode Cn assumed to be of 
full rank.
To initiate the process k( 1 + e) LT encoded vectors are generated by sam­
pling ^D(x) this number of times to combine packets (n-tuples chosen ran­
domly from F2n) to generate the (k, QD(x)) coded vectors v 1, v2,..., vk( 1+6). 
It will be assumed the n-tuples vi1, vi2,...,vik are linearly independent over 
F2. Let A be the k x n matrix with these independent n-tuples as rows and let 
R =AG, an invertible k x k matrix.
To encode the k information packets x = (x1,x2,...,xk), a vector whose 
i-th component ([30], algorithm 11) is the information packet xi. Compute the 
vector yt = (y1,y2,...,yk)t = R-1xt and encode these packets via the Cn 
code as u = (u 1, u2,..., un) where u* = G • yt. From the packets vj (vectors) 
introduced above compute the inner products
u1
Zi = Vi • u = (Vi, 1 ,Vi,2, . . . ,Vi,k) •
i = 1,2,...,k( 1 + e)
uk
which are the output packets. Notice that from the generating process for the 
packets Vj this last operation is a (k, ^D(x)) LT coding of the coded packets 
uj. Further packets zk( 1+6) +1,zk( 1+6)+2... are formed by (k,QD(x)) coding 
of the uj packets. Thus these output packets are LT codings of the yj packets.
The point of the process is to notice that the output packet zij = xj ,j = 
1,2,... ,k. To see this ([30], proposition 12) note that since R = A • G and 
Ryt = xt. Then A • Gyt = A • ut = xt .By construction of the matrix A, the 
j-th row is Vij and hence this last equation gives the inner product
Vij • xt = Xj, j = 1,2, ...,k.
Thus the Raptor code is systematic.
To decode the Zj,j = 1,2, ...,k( 1 + e) one uses the usual decoding 
procedure for the (k,Cn,QD(x)) code to yield the packets y 1,y2,... ,yk and 
then compute xt = R • yt for the original information packets.
Itis important to note that the point of this process is to retain the optimality 
of the distribution of the LT process, yet have certain coded outputs as the 
input information packets. Of course the process requires the preprocessing of 
information packets regarding the matrix R and systematic row indices to be 
known to receivers. The process can be made efficient and is incorporated into 
many of the standards of Raptor codes. More details on the process are given 
in [30] which also contains a complexity analysis. Additional information is in 
the patents regarding these techniques [26, 32, 35].
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

62
2 Coding for Erasures and Fountain Codes
Standardized Raptor Codes
Raptor codes have been adopted in a large number of mobile and broadcast and 
multicast standards. These are discussed extensively in the monograph [33]. 
Two versions of Raptor codes are available commercially, the Raptor version 
10, or R10 code and the RaptorQ code. Both are systematic and use versions 
of inactivation decoding. The R10 code supports blocks of up to 8,192 source 
symbols per block and up to 65,536 coded blocks. It achieves a probability of 
failure of 10-6 with only a few blocks of overhead across the entire range of 
block sizes. It is described in detail in [25].
The RaptorQ code is designed for a much wider range of applications and 
achieves faster encoding and decoding with good failure probability curves. It 
supports blocks of up to 56,403 source symbols and up to 16,777,216 coded 
blocks. It uses the observation that the rank properties of random matrices 
are superior in the sense of achieving full rank for much lower overheads. 
The field of 256 elements, F256 is used for this code which is described in 
[17], although the reference [33] gives an excellent description of both of these 
codes.
Other Aspects of Fountain Codes
The term “online codes” has also been used [15] for the term “fountain 
codes.” Additionally [3] it has been used to describe decoders for fountain 
codes that adapt to changing conditions of the channel and decoder and are 
thus able to achieve better performance. It requires some method to measure 
states and transmission back to the encoder, an important consideration in 
implementation.
The performance of similar techniques on other channels such as the BSC 
for error correction remains an interesting possibility.
Comments
The chapter has traced the progress in the remarkable development of erasure­
correcting and lost packet codes/fountain codes with linear encoding and 
decoding complexity. The notion of choosing the parity checks accord­
ing to a probability distribution in order to ensure, with high probability, 
a particular decoding algorithm completes, is novel and surprising. The 
success of both the theoretical and practical developments techniques is 
impressive.
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

References
63
References
[1] Blasco, F.L. 2017. Fountain codes under maximum likelihood decoding. CoRR, 
abs/1706.08739v1. arXiv:1706.08739v1,2017.
[2] Byers, J.W., Luby, M., Mitzenmacher, M., and Rege, A. 1998. A digital fountain 
approach to reliable distribution of bulk data. Pages 56-67 of: Proceedings of the 
ACM SIGCOMM ’98. ACM, New York.
[3] Cassuto, Y., and Shokrollahi, A. 2015. Online fountain codes with low overhead. 
IEEE Trans. Inform. Theory, 61(6), 3137-3149.
[4] Karp, R., Luby, M., and Shokrollahi, A. 2004 (June). Finite length analysis of LT 
codes. Page 39: Proceedings of the IEEE International Symposium on Information 
Theory, June 2004. ISIT.
[5] Kim, S., Lee, S., and Chung, S. 2008. An efficient algorithm for ML decoding 
of raptor codes over the binary erasure channel. IEEE Commun. Letters, 12(8), 
578-580.
[6] Luby, M.G. 2002. LT codes. Pages 271-280 of: Proceedings of the 43rd Annual 
IEEE Symposium on Foundations of Computer Science.
[7] Luby, M.G., Mitzenmacher, M., Shokrollahi, M.A., Speileman, D., and Stenman, 
V. 1997. Practical loss-resilient codes. Pages 150-159 of: Proceedings of the 
Twenty-Ninth Annual ACM Symposium on the Theory of Computing. ACM, New 
York.
[8] Luby, M.G., Mitzenmacher, M., Shokrollahi, M.A., and Spielman, D. 1998. 
Analysis of low density codes and improved designs using irregular graphs. Pages 
249-258 of: Proceedings of the Thirtieth Annual ACM Symposium on the Theory 
of Computing. ACM, New York.
[9] Luby, M.G., Mitzenmacher, M., and Shokrollahi, M.A. 1998. Analysis of random 
processes via And-Or tree evaluation. Pages 364-373 of: Proceedings of the 
Ninth Annual ACM-SIAM Symposium on Discrete Algorithms (San Francisco, 
CA, 1998). ACM, New York.
[10] Luby, M.G., Mitzenmacher, M., Shokrollahi, M.A., and Spielman, D.A. 2001. 
Efficient erasure correcting codes. IEEE Trans. Inform. Theory, 47(2), 569-584.
[11] Luby, M.G., Mitzenmacher, M., Shokrollahi, M.A., and Spielman, D.A. 2001. 
Improved low-density parity-check codes using irregular graphs. IEEE Trans. 
Inform. Theory, 47(2), 585-598.
[12] Lzaro, F., Liva, G., and Bauch, G. 2017. Inactivation decoding of LT and Raptor 
codes: analysis and code design. IEEE Trans. Commun., 65(10), 4114-4127.
[13] Maatouk, G., and Shokrollahi, A. 2009. Analysis of the second moment of the LT 
decoder. CoRR, abs/0902.3114.
[14] Maneva, E., and Shokrollahi, A. 2006. New model for rigorous analysis of LT- 
codes. Pages 2677-2679 of: 2006 IEEE International Symposium on Information 
Theory.
[15] Maymounkov, P. 2002. Online codes. Technical report. New York University.
[16] Maymounkov, P., and Mazieres, D. 2003. Rateless codes and big downloads. 
Pages 247-255 of: Kaashoek, M.F., and Stoica, I. (eds.), Peer-to-peer systems 
II. Springer, Berlin, Heidelberg.
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

64
2 Coding for Erasures and Fountain Codes
[17] Minder, L., Shokrollahi, M.A., Watson, M., Luby, M., and Stockhammer, T. 2011 
(August). RaptorQ forward error correction scheme for object delivery. RFC 
6330.
[18] Oswald, P., and Shokrollahi, A. 2002. Capacity-achieving sequences for the 
erasure channel. IEEE Trans. Inform. Theory, 48(12), 3017-3028.
[19] Pakzad, P., and Shokrollahi, A. 2006 (March). Design principles for Raptor codes. 
Pages 165-169 of: 2006 IEEE Information Theory Workshop - ITW ’06 Punta del 
Este.
[20] Paolini, E., Liva, G., Matuz, B., and Chiani, M. 2012. Maximum likelihood 
erasure decoding of LDPC codes: Pivoting algorithms and code design. IEEE 
Trans. Commun., 60(11), 3209-3220.
[21] Pfister, H.D., Sason, I., and Urbanke, R.L. 2005. Capacity-achieving ensembles 
for the binary erasure channel with bounded complexity. IEEE Trans. Inform. 
Theory, 51(7), 2352-2379.
[22] Rensen, J.H.S., Popovski, P., and Ostergaard, J. 2012. Design and analysis of LT 
codes with decreasing ripple size. IEEE Trans. Commun., 60(11), 3191-3197.
[23] Richardson, T.J., and Urbanke, R.L. 2001. The capacity of low-density parity­
check codes under message-passing decoding. IEEE Trans. Inform. Theory, 47(2), 
599-618.
[24] Richardson, T.J., Shokrollahi, M.A., and Urbanke, R.L. 2001. Design of capacity­
approaching irregular low-density parity-check codes. IEEE Trans. Inform. The­
ory, 47(2), 619-637.
[25] Shokrollahi, A., Stockhammer, T., Luby, M.G., and Watson, M. 2007 (October). 
Raptor forward error correction scheme for object delivery. RFC 5053.
[26] Shokrollahi, A.M., Lassen, S., and Karp, R. 2005 (February). Systems and 
processes for decoding chain reaction codes through inactivation. US Patent 
6856263.
.
 www.freepatentsonline.com/6856263.html
[27] Shokrollahi, M.A. 1999. New sequences of linear time erasure codes approaching 
the channel capacity. Pages 65-76 of: Applied algebra, algebraic algorithms and 
error-correcting codes (Honolulu, HI, 1999). Lecture Notes in Computer Science, 
vol. 1719. Springer, Berlin.
[28] Shokrollahi, M.A. 2000. Codes and graphs. Pages 1-12 of: In STACS 2000 
(invited talk), LNCS No. 1770.
[29] Shokrollahi, M.A. 2001. Capacity-achieving sequences. Pages 153-166 of: 
Codes, systems, and graphical models (Minneapolis, MN, 1999). The IMA 
Volumes in Mathematics and Its Applications, vol. 123. Springer, New York.
[30] Shokrollahi, M.A. 2006. Raptor codes. IEEE Trans. Inform. Theory, 52(6), 
2551-2567.
[31] Shokrollahi, M.A. 2009. Theory and applications of Raptor codes. Pages 59-89 
of: MATHKNOW - Mathematics, applied sciences and real life. Modeling, 
Simulation & Analysis (MS&A), vol. 3. Springer, Milan.
[32] Shokrollahi, M.A., and Luby, M. 2004 (April). Systematic encoding and 
decoding of chain reaction codes. US Patent WO 2004/034589 A2. 
www.freepatentsonline.com/y2005/0206537.html
[33] Shokrollahi, M.A., and Luby, M. 2009. Raptor codes. Pages 213 - 322 of: 
Foundations and trends in communications and information theory, vol. 6. NOW 
Publishers.
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

References
65
[34] Shokrollahi, M.A., and Storn, R. 2005. Design of efficient erasure codes with 
differential evolution. Springer, Berlin Heidelberg.
[35] Shokrollahi, M.A., Lassen, S., and Karp, R. 2005 (February). Systems and 
processes for decoding chain reaction codes through inactivation. US Patent 
20050206537.
.
 www.freepatentsonline.com/y2005/0206537.html
[36] Sipser, M., and Spielman, D.A. 1996. Expander codes. IEEE Trans. Inform. 
Theory, 42(6, part 1), 1710-1722.
[37] Tanner, R.M. 1981. A recursive approach to low complexity codes. IEEE Trans.
Inform. Theory, 27(5), 533-547.
[38] Tanner, R.M. 1984. Explicit concentrators from generalized N -gons. SIAM 
J. Algebraic Discrete Methods, 5(3), 287-293.
[39] Tanner, R.M. 2001. Minimum-distance bounds by graph analysis. IEEE Trans. 
Inform. Theory, 47(2), 808-821.
[40] Xu, L., and Chen, H. 2018. New constant-dimension subspace codes from 
maximum rank distance codes. IEEE Trans. Inform. Theory, 64(9), 6315-6319.
https://doi.org/10.1017/9781009283403.003 Published online by Cambridge University Press

3
Low-Density Parity-Check Codes
The inspiration for a large amount of work of the past two decades on low- 
density parity-check (LDPC) codes and for their enormous importance in 
application is the thesis of Gallager [8]. Although largely ignored for three 
decades after its appearance, it remains a major focus of coding activity and 
the genesis of a large number of new directions in coding theory. It has been 
shown that such codes can achieve very low error probabilities and low error 
floors making them suitable for a wide variety of applications. Many of the 
ideas explored in this chapter have overlap with Chapter 2.
The term low density stems from the work of Gallager whose interest was 
in finding algorithms that approximated maximum-likelihood (ML) decoding 
for a code defined by a parity-check matrix. Such algorithms often have 
complexity that is exponential in codeword length and often too computation­
ally expensive to implement in practice. His approach used message-passing 
techniques and while the work does not mention graphs, considering the 
equivalence (essentially) of a binary parity-check matrix and a bipartite Tanner 
graph (see Chapter 1), it has become common to interpret his work as message 
passing on edges of graphs. Note that message-passing algorithms include the 
subset of belief propagation algorithms where the messages being passed on 
edges represent a belief as to the value of the associated variable nodes. The 
computational complexity of such algorithms is often a function of the number 
of edges in the graph (also the number of 1s in the parity-check matrix). It can 
be shown that in some circumstances the performance of these algorithms can 
approach ML. One interpretation of the terms sparse or low density is a code 
with parity-check matrix with a number of ones that is linear in code length - 
hence a fairly sparse matrix. As noted it might also be taken to mean coding 
graphs with a number of edges that render the decoding algorithm feasible.
Note also that the two methods noted for constructing LDPC codes, by 
constructing a parity-check matrix at random and constructing a graph by the 
66
https://doi.org/10.1017/9781009283403.004 Published online by Cambridge University Press

3 Low-Density Parity-Check Codes
67
method noted in the previous chapter using random permutations are not quite 
identical in that the latter method could result in multiple edges between two 
nodes while with the former method this could not happen. In either case one 
imposes a uniform probability distribution on the ensembles. For large code 
lengths the two techniques are “essentially” equivalent. While Gallager [8] 
used the parity-check matrix method, Richardson and Urbanke [20] and Luby 
et al. [17] make a strong case that the graph ensembles are more convenient to 
use for analysis and this method is used here.
This chapter is at odds with the stated purpose of this volume in that 
important work on these codes is available in numerous published books (see 
[11, 12, 13, 19, 22]) as well as at least two excellent survey articles [10, 25]. 
One purpose of including such a chapter is for the relationship the material 
has with the graph algorithms of Chapter 2 and other chapters. This chapter 
largely ignores many of the important design and performance issues for LDPC 
codes, covered in the books mentioned. This includes omitting a discussion 
of important structural features such as trapping sets, absorbing sets and small 
cycle distribution that to a large extent determine the performance of them. The 
February 2001 issue of the IEEE Transactions on Information Theory (volume 
47, no. 2), a special issue on Codes on Graphs and Iterative Algorithms, 
contains numerous seminal papers that have shaped the subject. In particular 
the three papers [17, 20, 21] in that volume are of fundamental importance 
and this chapter attempts to convey the sense of some of the material in these 
three papers with less detail. The papers are, however, essential reading for any 
researcher in the area.
The graph-theoretical background needed for this work is modest, con­
sisting mainly of the essential (especially for large code lengths) equivalence 
between a binary matrix and a bipartite graph (Tanner graph) noted in Chapters 
1 and 2 and the construction and a few of the properties of the irregular graphs 
with (^(x),p(x)) edge distributions, which as noted previously will also be 
referred to as in the class Cn(k,p), the ensemble of graphs with n variable 
nodes and edge distributions A(x) and p(x).
The codes are described either by a binary parity-check matrix H where 
n columns represent codeword positions and (n - k) rows parity checks or, 
equivalently, by a bipartite (Tanner) graph with n variable or left nodes and 
(n - k) check, constraint or right nodes.
This chapter will restrict its attention to binary-input symmetric channels 
and deal with the performance of belief propagation (BP) algorithms and the 
analysis of LDPC codes on such channels. While the analysis and techniques 
developed can be applied to general binary-input symmetric channels, the 
channels of particular interest are the binary symmetric channel (BSC) and the
https://doi.org/10.1017/9781009283403.004 Published online by Cambridge University Press

68
3 Low-Density Parity-Check Codes
N ■ N(0,a 2)
Figure 3.1 Binary-input channels: (a) BSC and (b) BIAWGN
X e {±1}
(b)
Y =X+N
binary-input additive white Gaussian noise (BIAWGN) channel whose figures 
are repeated here for convenience in Figure 3.1. The binary erasure channel 
(BEC), of interest in Chapter 2, will be mentioned occasionally. The BEC tends 
to be a simple case where many formulae can be stated exactly rather than 
approximated. The same situation occurs with polar codes whose properties 
on the BEC can be analyzed more successfully and explicitly than on other 
channels.
The capacity formulae for these channels were noted in Chapter 1. For the 
BSC with crossover probability p it is
CBSC = 1 - H2(p), H2(x) =-xlog2(x)-(1-x)log2(1-x), (3.1)
H2 (p) the binary entropy function. For the BIAWGN channel the joint 
distribution of (X, Y), input and output, is a mixture of discrete and continuous 
and with P(X = +1) = P(X = -1) = 1/2 (which achieves capacity on this 
channel) and pdf p(x) ~ N(0,a2). The pdf p(y) of the output is
p(y) = 1 
' e - (& + 1 
' e—(y^
2 V2 na2 
2 V2 na2
1
V8 na 2
(y + 1)2
2 a 2
+ exp
(y — 1)2
I------------ I I
\ 
2 a 2
and the expression for capacity reduces to
Cbiawgn = - j p(y) log2 p(y)dy — 2 log2(2nea2). 
(3.2)
The general shape of these capacity functions was shown in Figure 1.3.
Much of the analysis in the literature is for the case of (c, d)-biregular 
bipartite codes where the Tanner graph has variable nodes of degree c and 
the check nodes of degree d . It will be convenient to refer to these degree 
parameters as (dv,dc). The notion of irregular graphs considered in Chapter 2 
https://doi.org/10.1017/9781009283403.004 Published online by Cambridge University Press

3 Low-Density Parity-Check Codes
69
is used here as well for both the BSC and BIAWGN case. Recall that such 
irregular graphs are assumed to have an edge distribution pair (U(x),p(x)) 
where
U(x) = ^2 ^iXl-1, 
P(x) = 52 PiX-1.
The construction of such graphs has been previously described. Note that 
the (dv,dc)-biregular case corresponds to U(x) = xdv-1 and P(x) = xdc-1. 
The equivalent distributions for node degrees are easily derived from these 
edge distributions (see Chapter 2). As shown there such irregular distributions 
play a crucial role in determining effective erasure-correcting codes. Indeed 
Chapter 2 shows that such irregular edge distributions are able to achieve 
capacity on the BEC. The results for the binary-input channels of interest in 
this chapter, the BSC and BIAWGN, are far less complete and more complex 
and challenging. Nonetheless the progress for these channels in the past few 
decades has been impressive.
It will be of value for this chapter to briefly review a result of Section 2.2 
for the BEC. Suppose in a Tanner graph of a code with edge distributions 
(U(x),p(x)), channel symbols are erased with probability S and let pt be the 
probability a variable node is unresolved at the I-th iteration. It was shown in 
Chapter 2 how a recursion can be developed for the (I + 1)-st iteration of the 
decoding Algorithm 2.2. A simple argument yields that
pr +1 = SU( 1 - p( 1 - pt))
and a condition for successful decoding is that pt+1 < pt, i.e., with each 
iteration the probability a variable node remains unresolved decreases. This 
translates into the condition that for a given BEC with erasure probability S, 
the edge distributions should be chosen so that (Equation 2.3 of Chapter 2)
SU(1 - P(x)) < x, 0 <x <S. 
(3.3)
If the code graph is chosen with such edge distributions, then with high 
probability the BP decoding algorithm will decode successfully with up to 
a fraction of symbols S being erased. Furthermore the rate of the code, 
determined by the distributions U(x) and P(x), should approach the capacity 
of the channel 1 - S . That such distributions can be found, and hence capacity 
achieved on the BEC is a major result of Chapter 2.
The notion of graphs with irregular edge distributions (U(x),P (x)) seems 
to have first appeared in [14] for the BEC. These notions will also be of 
importance to codes on more general channels as considered in this chapter.
https://doi.org/10.1017/9781009283403.004 Published online by Cambridge University Press

70
3 Low-Density Parity-Check Codes
It is assumed all graphs will have no self-loops or multiple edges. All 
channels considered here will have binary inputs, be memoryless and will be 
parameterized by a single parameter, say y which often allows the notion of a 
threshold to be formulated as discussed below.
The notion of thresholds already present in the work of Gallager [8] is of 
importance. Consider the ensemble of codes Cn(k,p) for fixed distributions 
and channels that can be characterized with a single parameter, say y.Forthe 
BEC, BSC and BIAWGN these would be S (erasure probability on the BEC), 
p (crossover probability on the BSC) and a (noise standard deviation on the 
BIAWGN). The parameter y is generic for this purpose. Then Richardson [21] 
shows there exists a threshold y * which is a maximum channel parameter such 
that for any e > 0 and y < y* there exists a code length n(e,y) and a number 
l(e, y) such that almost every code in the ensemble Cn(k, p) with n > n(e, y) 
has a bit error smaller than e assuming that the transmission is over a channel 
with parameter y and the decoding algorithm performs £(e,y) iterations. 
Conversely, if the channel parameter y>y*, then for almost all codes in 
the Cn (k, p) ensemble for a fixed number of decoding algorithm iterations, the 
probability of bit error will exceed some constant n which depends on channel 
parameter y but not on the number of iterations.
Finally it is noted that although the case of the BEC was not treated in the 
work of Gallager [8], the other notions of importance here, namely message 
passing on a graph and the analysis involving iterations and convergence were 
clearly present in that work, although augmented since by the work of many 
others. As in Chapter 2 the input and output alphabets are binary {0, 1} for 
the hard decision algorithms of the next section on the BSC. For Section 3.2 
on the BIAWGN channel, the input alphabet will be {±1} and the output 
and message-passing alphabets will be the real numbers. Most algorithms 
considered here will be BP algorithms,
As noted, the results for channels such as the BSC and BIAWGN channels 
treated here are not as complete as those for the BEC and the algorithms 
involved are more complex than for the BEC. The BSC is considered in the 
next section where Gallager algorithms A and B are given and then recast 
into message-passing algorithms in preparation for the application of similar 
notions to the BIAWGN channel. The case of the BIAWGN is then treated in 
Section 3.2 where the work of [20] is considered. The final section considers 
the notions related to the performance of the algorithms of Section 3.2 such as 
an extrinsic information transfer (EXIT) chart as an aid in tracing the progress 
of the decoding algorithm as the number of iterations increases and the notions 
of concentration of densities and thresholds.
https://doi.org/10.1017/9781009283403.004 Published online by Cambridge University Press

3.1 Gallager Decoding Algorithms A andB for the BSC
71
3.1 Gallager Decoding Algorithms A and B for the BSC
Gallager [8], in his influential thesis, considered the performance of LDPC 
codes on several classes of binary-input channels. He gave two algorithms for 
decoding on the BSC that have come to be known as Gallager algorithms A 
and B. The analysis of these two algorithms for decoding on the BSC follows. 
Although, as noted, Gallager did not use graph terminology, they are the first 
true message-passing algorithms. The algorithms differ in the manner in which 
they formulate the messages to be passed. These algorithms were originally 
determined for the case of (dv,dc)-biregular Tanner graphs (in Gallager’s work, 
matrices with constant row and column sums) and were generalized in [17] to 
the case of ensembles of irregular (k(x),p(x)) graphs, Cn(k, p), which are also 
considered here. The section concludes by recasting the original description 
of these algorithms as message-passing algorithms, as described in [20], in 
preparation for the discussion of these ideas on the BIAWGN in Section 3.2.
Consider the Tanner graph ofan (n,k)2 code with n variable nodes of degree 
dv and m = (n - k) check nodes of degree dc . Thus the edge distributions for 
such biregular graphs are A(x) = xdv -1 for variable or information nodes and 
p(x) = xdc-1 for check or constraint nodes.
In this section, all messages on the graph edges are binary from the alphabet 
{0, 1} and the channel a BSC.
Gallager Decoding Algorithm A
The algorithm is described as follows.
Algorithm 3.1 In the first round (round 0) the binary received word r = 
(r 1 ,r2,... ,rn),ri e {0,1}, is associated with the n variable nodes and these 
values are retained by the variable nodes for the entire algorithm. It will 
be convenient to label the received bit for the variable node v as rv,v = 
1,2,...,n. The algorithm proceeds in rounds or iterations. One iteration will 
consist of messages from variable nodes to check nodes (first phase) and from 
the check nodes back to variable nodes (second phase).
Assuming an edge from variable node v to check node c, at round 0, node 
v transmits its received value rv to its neighboring check nodes. In this and 
each succeeding iteration, each check node c sends to an attached variable 
node v the parity (mod 2 sum) of the messages it received from all its variable 
neighbors except node v immediately previously. Thus c sends to v the value 
it believes node v should have. In each succeeding iteration, variable node v 
sends to neighboring check node c the following: if all the messages in the 
https://doi.org/10.1017/9781009283403.004 Published online by Cambridge University Press

72
3 Low-Density Parity-Check Codes
previous iteration to node v, except that from c, were b, then v sends b to c; 
otherwise, it sends its original received value rv .
Note again that all messages are either 0 or 1. Assuming there is an edge 
between nodes v and c, denote the messages sent in the £-th iteration as 
(e) 
(£')
mvc and mcv , respectively. The maximum number of iterations that can be 
sustained and still have the variables involved be statistically independent is 
g/4 where g is the girth of the bipartite graph. In essence this means that 
for a fixed node v as one traces edges out from v out to a depth of less than 
g/2 edges the evolved graph would be a tree and hence all the variable nodes 
visited are distinct and their associated values are statistically independent, 
a requirement for the analysis, at times referred to as the cycle-free case. 
Denote the set of neighbors of a variable node v (respectively check node 
c)byNv (respectively Nc). Further define Nv,c (respectively Nc,v) as the set 
of neighbors of v (respectively c) except c (respectively v). This implies that 
messages transmitted use only extrinsic information from previous rounds, 
i.e., not messages received from that vertex in the previous round, in order to 
preserve independence of the information. Independence is of course crucial 
for the validity of the analysis.
Note that the variable N denotes white Gaussian noise leading to overuse 
of this variable. The distinction of this usage from neighbor sets will be by 
subscripts and should cause no confusion.
a)
For each edge (v, c) of the graph the message passed is denoted mvc ,the 
message passed from variable node v to check node c at iteration £, as
mW = 
vc =
rv if £ = 0
b if mc- 1) = b V c' e Nv,c, I > 1
rv otherwise 
(3.4)
and similarly for m(cv, the message passed from check node c to variable node 
v at iteration £:
m(e> = m m(e) £ > 0 
mcv — kt/ mv/ c C > 0.
(3.5)
The algorithm is both a message-passing algorithm and a belief propagation 
(BP) algorithm.
The last equation is the parity of the bits seen by the check node, exclusive 
of the bit received from the variable node being sent to. Thus the check node 
would be satisfied if the variable node receiving the message had the same 
value. If the input alphabet {±1} is used, as for the BIAWGN, the message 
sent from the check node could be expressed as
https://doi.org/10.1017/9781009283403.004 Published online by Cambridge University Press

3.1 Gallager Decoding Algorithms A andB for the BSC
73
m(e) = 
m(e) £ > 0
mcv 
mv!,c > 0.
A further comment on this will be given later. Until then assume all messages 
transmitted are binary {0, 1} as for the BSC.
The analysis of the algorithm simply assesses the various probabilities of 
the transmitted messages being correct. For the analysis of the algorithm it 
is useful to verify that if a random variable has a binomial distribution with 
parameters k, p, then
E( ‘) P( 1 - p)) - ==1+1-2PL,
j even
a result achieved by considering the sums
k 71\ 
k 71 \
(a+b)k = 
aj bk-j and (-a + b)k = 
(-a)j bk-j
jj
j=0 
j=0
and taking a = -p and b = 1 - p. Summing these terms with the values
indicated, the odd terms cancel and hence
e( ‘) pj( 1 - p) - j=1±<1-pk
j even
and similarly
E(k) pj( 1 - p) - j=1-1-M..
j odd j
Assuming an edge (v, c) in the graph, let pt+1 be the probability that v 
sends an incorrect value to c in the (£ + 1)-st iteration. The probability that a 
variable node was originally received correctly is 1-p0, p0 = p, the crossover 
probability on the BSC. Conditioned on this event, the probability the node 
passes on an error is the probability that all neighbor vertices of v, except c, 
will indicate an error which is the probability that each neighbor c' in Nv,c, 
passes an error to v (i.e., each c' e Nv,c receives and odd number of errors), 
which is
1 - (1 - 2pt)d-1 
2
(3.6)
and hence the probability rv was received correctly but sent incorrectly at the 
(I + 1)-st iteration is
(1
(1 - 2pt)d-1
dv-1
2
In this event the node v will pass an error to its Nv,c neighbors.
https://doi.org/10.1017/9781009283403.004 Published online by Cambridge University Press

74
3 Low-Density Parity-Check Codes
Similarly suppose node v was originally received in error with probability 
p0. The probability that node v receives no error indication from each of its 
dv - 1 neighbors (i.e., its neighbors except c)is
1 + (1 - 2 Pt)d-1 \ dv 1
2 
7
The probability that at least one neighbor of v (other than c) passes an error 
indication to v is 1 minus this quantity. Putting the two statements together 
gives
1 + (1
P£+1 = P 0 1 - ----
- 2Pl)d -1
2
dv -1
+(1-P0)
1 - (1 - 2pt)d-1'
2
dv-1
(3.7)
which is the sum of the probabilities the received symbol rv was received in 
error and passed on as correct at the (I + 1)-st iteration plus the probability rv 
was received correctly and passed on as incorrect at that iteration.
The algorithm continues until values at all variable nodes satisfy the check 
equations or until some preset number of iterations at which point failure may 
be declared or the probabilities may be consulted and decisions made.
The idea of Gallager [8] is then to find a threshold for the BSC crossover 
probability p*, such that for crossover probabilities less than this, the pt 
decreases monotonically. Recall the analysis is only accurate if the indepen­
dence assumption holds, i.e., if the graph has a sufficiently large girth or if the 
probabilities converge sufficiently fast. Typically, randomly chosen graphs do 
not have large girth so constructions of good graphs are required.
Some exact thresholds for Gallager’s algorithm A are discussed in ([3], 
table 1). For example, for the standard dv = 3 ,dc = 6 rate 1/2 code, the 
threshold using the Gallager A decoding algorithm is shown to be t(3,6) = 
(1 - *Ja)/2 ~ 0.039) where a is the (single) positive root of the equation 
r(x) =-1 - x - x2 +x3 +x4. Thus for a code used on a BSC with crossover 
probability below this threshold, decoded with Gallager A, asymptotically 
with length, will decode successfully with high probability and the error 
probability will be above a constant if the crossover probability is above this 
threshold. Further developments on this problem are given for code ensembles 
(and Gallager A decoding) for code ensembles Cn(k,p) discussed later in the 
section.
Gallager Decoding Algorithm B
Gallager [8] suggests a modification to the above decoding algorithm to 
improve performance. For the I-th iteration a threshold bt is proposed,
https://doi.org/10.1017/9781009283403.004 Published online by Cambridge University Press

3.1 Gallager Decoding Algorithms A andB for the BSC
75
depending on the iteration number, such that if at least bt neighbors of v 
(except c) sent the same bit in the previous round £, then v sends this bit to 
c in the (I + 1)-st round. Otherwise it will send the original received bit rv. 
The thought here is that rather than insisting all extrinsic information is the 
same, as in Algorithm A above, if a large fraction of them are the same it is 
likely to be the correct value and that performance of the algorithm is likely to 
improve if this is taken into account. Note the threshold bt is a function of the 
iteration number.
The above formulae are modified in a straightforward manner to:
2
2Pt)dc 1 \1
+ (1
dv
1-
1 - (1 - 2 pt)d-1
2
dv -1-1
(1 - 2pfd-1 \1
2 I
1 + (1 - 2 pfd-1-1 \dv 1 1
2 I
(3.8)
It is desired to choose the threshold bt to minimize pt+1. This can be achieved 
as follows. Parameterize the probabilities with the threshold bt, the threshold 
at the I-th iteration, as p^+1 ,bt and consider the smallest value of bt such that
P£ +1,bi - pe+1 ,bi-1 < 0,
i.e., at the (£ + 1)-st iteration choose the smallest threshold that ensures the 
probabilities get smaller. This is equivalent to ([8], equation 4.16) choosing bt 
as the smallest integer that satisfies the equation
- p0
r 1 + (1 - 2 pf>d-11(2 bt - dv+1)
p0
1 - (1 - 2pe)d-1
1
<
Note that bt decreases as pt decreases. Typically, as before, the algorithm 
runs until the variable values satisfy all the check equations or until a 
fixed number of iterations is reached. The following quantifies the decoding 
performance somewhat:
Theorem 3.2 ([17], Theorem 1) Let £ > 0 be an integer constant and let Zt 
be the random variable describing the fraction of edges containing incorrect 
messages after round £ of the above algorithm. Further, let pt be as given in 
Equation 3.8. Then there is a constant n (which depends on the maximum of 
(dv,dc)) such that for any c > 0 and sufficiently large n we have
Pr(| Zr - pt | > e) < exp(-cc2n).
Recall that n is the number of bits in a codeword (the number of variable 
nodes in the graph). Thus the probabilities of the above formulae accurately 
https://doi.org/10.1017/9781009283403.004 Published online by Cambridge University Press

76
3 Low-Density Parity-Check Codes
track the number of errors at each stage of the algorithm. The following 
corollary further supports this view:
Corollary 3.3 ([17], corollary 1) Given a random (dv,dc)-biregular code with 
pt defined as above, if the sequence pt converges to 0, then for any n > 0 
there is a sufficiently large message size such that Gallager’s hard decision 
decoding algorithm decodes all but at most nn bits in some constant number 
fn rounds with high probability.
Thus the Gallager algorithm is capable of correcting almost all errors, but 
typically not all. The situation is reminiscent of that ofLT codes that in moving 
to Raptor codes employed a simple erasure-correcting code to correct the 
residual errors. Here, in order to correct all the errors it is suggested that an 
expander-based graph argument be used to complete the decoding, as noted in 
an earlier chapter. See [17] for further details.
The above discussion is for the biregular graph/code case. The extension to 
irregular graphs is contained in [15, 17]. The results are simply stated below. 
Suppose the code graph is a member of the ensemble of graphs Cn(k,p). 
As before let pt+1 denote the probability a variable node sends an incorrect 
message at the (I +1)-st iteration. The probability a check node in the irregular 
code receives an even number of error messages, using an argument identical to 
that leading to Equation 3.6, conditioned on the check degree being dc, then is
1 + (1 - 2pt)d-1
2 .
If the degree, however, is chosen according to the distribution p(x) = 
^2i pix -1, then with probability pi the right edge degree will be i-1. Averaged 
over all possibilities, the probability the check node receives an even number 
of errors is
1 + p( 1 - 2pt)
2 .
It is then straightforward to generalize Equation 3.8 to the recursion going from 
round i to i + 1, here dt is the number of terms in p(x):
pi+1 = f(pi) where
d«
f(x) = p0 - E 'j p0
j-1
+(1 - p0)
t=bi,7
1 + p( 1 - 2x) 1 t |" 1 - p( 1 - 2x) 1j 1 2 t
1 - p( 1 - 2x) 1 t I" 1 + p( 1 - 2x) 1j-1--'
2
2
2
(3.9)
2
https://doi.org/10.1017/9781009283403.004 Published online by Cambridge University Press

3.2 Performance of LDPC Codes on the BIAWGN Channel
77
The threshold [17] bi,j is chosen as the smallest integer that satisfies the 
equation
1 - p0 < r 1 + p( 1 - 2 pi) i2 bi,j - j+1
p0
1 - p( 1 - 2pi)
Notice that the exponent in the above equation can be expressed as
2
bi,j =
2bi, j - j + 1 = bi,j - (j - 1 - bi,j).
There remains the problem of choosing the distributions A(x) and p(x) in 
order to yield the largest possible value ofp0 (BSC crossover probability) such 
that the sequence {pt} decreases to 0. The following procedure does not find 
optimum sequences but does give sequences that perform better than biregular 
ones. For a fixed p0 and a given sequence p 1 ,p2,..., pdc. Let
log(1 - p0)/p0 
j 1 +
J log(1 + p( 1 - 2x))/( 1 - p( 1 - 2x))
The criterion then is, for a given p0 and right sequence p(x), to find the left 
degree sequence X(x) such that f(pt) = pt+1 and f (x) < x on the open 
interval (0,p0) for f(x) given by Equation 3.9. For details see [17].
As a final comment, it is noted that the above analysis does not guarantee 
the decoding process always completes successfully. It can be shown, using 
graph expansion arguments and modified decoding, that the type of random 
graph constructed here will typically be a good expander and that with these 
notions the decoding can be completed with high probability. In the context 
of bipartite graphs the notion of graph expansion will hold if (informally), for 
a given subset of variable nodes, the number of its check node neighbors is 
large. Such notions can be used to show that if the variable subset is identified 
with undecoded bits, the existence of a sufficiently large set of neighbors 
can be used to complete the decoding (by switching to a different decoding 
algorithm).
3.2 Performance of LDPC Codes on the 
BIAWGN Channel
The performance of belief propagation decoding for a BIAWGN is considered. 
Most of the analysis here applies to other binary input symmetric channels but 
our interest will be in the BIAWGN.
The capacity of the BIAWGN channel is given by Equation 3.2 and the 
general shape of its capacity curve is shown in Figure 1.3(b). A binary code 
https://doi.org/10.1017/9781009283403.004 Published online by Cambridge University Press

78
3 Low-Density Parity-Check Codes
will be assumed and the inputs will be from the alphabet {±1} for transmission 
on the AWGN channel. As noted previously, these are often derived from 
a binary {0,1} alphabet via the transformation 0 ^ 1, 1 ^ -1, i.e., via 
(-1 )c,c e {0,1}. The messages on the Tanner graph of the code will be log 
likelihood ratios, hence real numbers representing beliefs (probabilities) as to 
the values of the various variables.
The decoding algorithm will consist of passing messages back and forth 
on the Tanner graph of the code with the thought that as the number of 
iterations progresses the log likelihoods polarize to either minus infinity or 
infinity indicating a bit decision. This section will give an overview of the 
construction of the messages, and the performance of the algorithm. The details 
are contained in [20] and [21], and are required reading for researchers in the 
area. The aim here is to give background and discussion to complement reading 
of the original papers.
For notation the cumulative distribution function (cdf) of a random variable 
X is denoted by capital FX (x) and its corresponding probability density 
function (pdf) by fX (x) although the subscripts will be dropped when the 
random variable is understood. In the case of smooth continuous functions they 
are related by differentiation. From the previous section recall the definitions 
of Nv, Nv,c, Nc and Nc,v as sets of neighbor vertices.
The decoding algorithm will proceed in iterations of passing messages on 
the graph edges, one iteration will consist of forming messages at each of the 
variable nodes, passing the messages from variable nodes to check nodes (first 
phase of an iteration), forming new messages at the check nodes and then 
passing the messages back from check nodes to the variable nodes (second 
phase). The method of forming the messages for each type of node will be 
described. Since they will be log likelihoods they will represent a belief as to 
the values associated with the variable nodes, given past messages, and the 
algorithm is thus a belief propagation (BP) algorithm as well as a message­
passing algorithm.
To determine the performance of the algorithm the probability densities 
of the messages passed on the edges have to be determined and this is the 
technical part of the development. Finally some properties of the pdfs that aid 
in determining decoding error probabilities are commented on.
The messages passed on the graph at the £-th iteration from a variable node 
v to a check node c, assuming (v, c) is an edge in the graph will be designated 
(^) 
(£)
mvc an tose rom c to vmcv .
Various notions of symmetry are important to the discussion, although 
perhaps not entirely intuitive, assuming inputs {±1}:
https://doi.org/10.1017/9781009283403.004 Published online by Cambridge University Press

3.2 Performance of LDPC Codes on the BIAWGN Channel
79
Definition 3.4 ([20, 21])
(i) Channel symmetry: The channel is output symmetric if
p(y = q | x =+1) = p(y =-q | x =-1)
where p(• | •) is the pdf of the (continuous) channel output, given the 
input of the channel.
(ii) Check node symmetry: The signs factor out of the check node message 
maps:
'(''' 
( in 
'('''
mcv (b1 m1,...,bdc -1 mdc -1 ) = mcv (m1,...,mdc -1 ) 
bi
for any ±1 input sequence {bi} and where the arguments of the functions 
(£)
mcv are the inputs to the check nodes except that from node v.
(iii) Variable node symmetry: Inverting the signs of all {±1} incoming 
messages to a variable node v will invert the sign of the messages sent 
from v.
(iv) Message pdf and cdf symmetry: A pdf f is symmetric if 
f(x) = exf( — x), x e R and equivalently for the cdf.
The notion of message symmetry is slightly unusual but arises out of the 
analysis. For an example that will be of use later, consider a Gaussian (normal) 
pdf with mean p. and variance a2 denoted by N(p,a2). It is readily verified 
that for such a pdf to satisfy the symmetry condition it is necessary and 
sufficient that a2 = 2p, i.e., if
1
V2^a
exp ( — (x — p.)2/2a 2))
1
2naa
exp ( — (— x — p)2/2a2)) • exp(x).
(3.10)
The messages passed on the code graph edges represent belief as to the 
transmitted value in the following manner. Consider a variable node v and 
connected check node c. The message passed from v to c in a given iteration 
is the belief of the value of the variable node v given all the messages (beliefs) 
passed to v from all neighbor check nodes except c in the previous phase. 
Similarly the message passed from c to v is the belief as to v’s value, given all 
the messages passed to c in the previous phase except that from v. It is assumed 
that all messages are independent meaning that at the £-th iteration there are 
not cycles of length 2£ or less in the graph. The term belief in this will be the 
likelihood ratio or log likelihood ratio in the following sense.
Suppose a binary {±1} value X is transmitted and a continuous value Y is 
received and assuming the BIAWGN, Y = X + N where N is a Gaussian 
https://doi.org/10.1017/9781009283403.004 Published online by Cambridge University Press

80
3 Low-Density Parity-Check Codes
random variable with mean zero and variance a2, Y ~ N(±1,a2). The 
conditional likelihood or likelihood ratio (LR) value is defined as
L(x|y)=P(X=1 |Y)/P(X=-1 |Y) 
(3.11)
which is the likelihood (probability) the transmitted value was +1 given the 
received value Y, compared to the probability it was -1, given that Y was 
received. Note that the LR is a random variable and the values of L(x | y) 
range from 0 to +^ - a value near 0 indicates a belief the transmitted value 
was -1 while a large value represents a belief it was 1. For the BIAWGN the 
likelihood ratio (LR) is
L(x | y) = P(X =+1 | Y)
P(X = -1 | Y)
exp (- (y - 1) 2 / 2 a 2) = Z2 \
exp (— (y + 1) 2 / 2 a 2) 
P a 2
(3.12)
More often the log likelihood ratio (LLR) ln L(x |y) is used. Notice that 
ln L(x |y) = a2y for the case above is a normal random variable (recall the 
received signal is of the form Y = X + N) and for Y>0 its mean is 2/a2 and 
variance is 4/a2, i.e., ~ N(2/a2, 4/a2), and, as can be checked, the density 
for this LLR is symmetric by the above definition.
As above, a high positive value for an LLR message represents a belief the 
variable value was +1 and a strong negative value that it was -1.
In Equation 3.12 let p1 = P(X =+1 | Y) and p-1 = P(X=-1 | Y) and 
let L(x | y) = p 1 /p-i and HJL(x | y) = ln L(x | y) and
m = ln -p-^ = ln L(x | y) = U(x | y). 
p-1
The hyperbolic tan function is defined as
tanh(x) = ex - e- x 
ex + e- x
e 2 x - 1
e 2 x + 1
For any probability pair (p1,p-1), p1 + p-1 = 1,m= ln L = ln
tanh
em - 1
em + 1
— - 1 
p-1
L-1
— + 1 
p-1
L+1 = p1 - p-1,
(3.13)
which will be a useful relationship. Equivalently
L(x | y) = 1 + (P1 - P-1)
1 - (P1 - P-1)
1 + tanh (m/2)
1 - tanh (m/2)'
(3.14)
https://doi.org/10.1017/9781009283403.004 Published online by Cambridge University Press

3.2 Performance of LDPC Codes on the BIAWGN Channel
81
It is assumed the channel input variables X e ±1 are equally likely. It 
follows that L(x | y) = L(y | x) since by Bayes theorem, informally,
L(x | y) = P(X = +1,Y)/p(Y)
P(X = -1,Y)/p(Y) 
P(X = +1,Y)
P(X = -1,Y)
and
L(y | x) = P(X = +1,Y)/P(X = +1)
P(X = -1,Y)/P(X = -1)
P(X —+1,Y)
P(X — -1,Y)
where P(X= i, Y) is a joint discrete and continuous probability density which 
requires greater care.
Consider a check node c with a set of variable node neighbors Nc . It will be 
convenient, when the code parity properties are under consideration, to view 
the codeword chosen c = (c1,...,cn) as binary ci e{0, 1} and view the word 
transmitted on the channel as x = (x1,...,xn) as binary xi e {±1}. As noted, 
the two are related as xi = (-1 )ci or 0 —> 1 and 1 —> -1. It will be 
clear from the context which is under consideration and the equivalence will 
be assumed without comment. The related probabilities are denoted {p0,p1} 
and {p1,p-1}, with superscripts indicating iteration number if needed. The 
received channel output at variable node v will be denoted Yv = Xv + N . 
Note the double duty of the notation N - without any subscript it will denote a 
Gaussian noise variable.
In the absence of channel noise the value of a variable node v e Nc could 
be obtained by noting that the check values are all zero for a valid codeword 
input. Thus the bit associated with any check node would be 0 and the belief 
of check node c as to the value associated with variable node v, xv would be
0 = ® Xvi or Xv = ® Xvi, Nc,v = Nc\{v} = {v 1 ,v2,...,vf}.
vi eNc 
vi eNc,v
The message to be passed from check node c to variable node v at the £-th 
iteration would be
mc.v — &(xv | yv 1, . . . , yvf), vi e Nc.v
= lnL(x | yv 1,.. .,yVf)
p Xv =+1 | Yv1 ,...,Yvf
— ln —■,-------------------------- ®
PXv —-1 | Yv 1 ,...,Yvf)
(3.15)
with Nc,v as above with corresponding variable node values Yvi , i.e., the 
messages transmitted to c from the variable nodes in the previous phase. To 
compute these expressions note the following. Since the yi initial received 
variables are assumed independent the joint density factors: 
https://doi.org/10.1017/9781009283403.004 Published online by Cambridge University Press

82
3 Low-Density Parity-Check Codes
p yv1,yv2,...,yvf | xv = p yvi | xv .
i=1
The assumption of independence requires the path generated from an original 
node to be cycle-free. Then
L yv1 ,yv2 ,...,yvf | xv = p yvi | xvi =+1 /p yvi | xvi =-1 = L yvi | xvi 
i=1 
i=1
and by the above observation that L yvi | xvi = L xvi | yvi for equally likely 
data variables and
L xv | yv1 ,...,yvf = L xv | yvi . 
(3.16)
i=1
Suppose Xv = Xv 1 ® Xv2 ® • • • ® Xvf (where Yvi = Xvi + N) and Yi 
depends only on Xi and assuming binary {0, 1} code symbols. Define
r( , 
\ P (Xvi = 1 I Yvi) P (i) 
f
LXv I yv = ----/----------------T = —777, 
i = 1,2, . .., f .
VvH7v^ 
P(XV. = 0 I Y^) 
p0i)
Let
p0 = P(Xv 1 ® • • • ® Xvf = 0 | Yv 1,... Yvf) 
and p 1 = P(Xv 1 ® • • • ® Xvf = 1 | Yv 1 , . . ,YV^.
Under the above conditions it is claimed that
f , 
. 
. x
p0 - p1 = 
p0(i) - p1(i) .
i=1
(3.17)
This can be shown by induction. Suppose the relation is true for f - 1 variables 
and let Zf-1 = Xv 1 ® • • • ® Xvf -1. Then
P0 = P(Xv 1 ® • • • ® Xvf = 0 | Yv 1, . . . Yvf)
= P (Zf-1 = 0 I Yv 1 ,... Yvf-1 ) P (Xvf = 0 I Yvf )
+ P (Zf-1 = 1 I Yv 1 ,...Yvf -1 ) P (Xvf = 1 I Yvf )
and similarly
P 1 = P (Xv 1 ©•••© Xvf = 1 I Yv 1 ,...Yvf )
= P (Zf-1 = 1 I Yv 1 ,... Yvf-1 ) P (Xvf = 0 I Yvf )
+ P (Zf-1 = 0 I Yv 1 ,...Yvf -1 ) P (Xvf = 1 I Yvf )
https://doi.org/10.1017/9781009283403.004 Published online by Cambridge University Press

3.2 Performance of LDPC Codes on the BIAWGN Channel
83
Subtracting the expressions gives
P (Zf-1 = 0 | Yv 1,... Yvf-1) f P (Xvf = 0 | Yvf ) - P (Xvf = 1 | Yvf))
- P (Zf—1 = 1 | Yv 1,... Yvf—1) (P (Xvf = 0 | Yvf) - P (Xvf = 1 | Yvf))
which leads to
z 
z f-1 z
(p f) - p(f)) n (p (i) - Pi))=p o - p 1
i =1
(3.18)
as claimed.
(£)
Interpreting the above developments for the message mcv from check node 
c to variable node v e Nc at the (second phase of the) £-th iteration it follows 
from Equation 3.15 and Equations 3.13, 3.14 and 3.17 that
m&
cv
14 n L (p (i) - p-0) 
= ln----- 7-------- 7-------------
1 -(n L (p i - p -i))
(1 + n f=1 tanh Wi/ 2)) 
p®
= ln 
t > 0, <■ = ln^,i = 1,2,..., f
(1 - nf=1 tanh(^^i 12)) 
" 
P-)1
(1 + 0 v >e Nc,v tanh (mv "id 2)) £>Q
(1 - 0v-eNcv tanh mvC}/2)) 
> .
(3.19)
For the messages passed from the variable nodes to the check nodes, 
consider the following. For the 0-th iteration at variable node v, m(v0c) ,the 
message passed to check node c e Nv will be m(v0c) = ln P(Xv =+1 | 
Yv)/P (Xv =-1 | Yv) given by Equation 3.16. For later iterations it is 
given by 
m (P) = 
vc =
(0)
mvc,
(0) 
(^ -1)
mvc +E c 'S Nv,cmc' v
(3.20)
t = 0
£ > 1
which is the LLR ln(P (Xv =+1 | Y1,...,yf)/P(Xv =-1 | Y1,...,Yf)) 
where as before the Y variables correspond to nodes in Nv,c .
The message sent from v to c in the £-th iteration is the variable node v’s 
estimate of its (i.e., v’s) value via LLRs which is the simple sum of the LLRs 
it received in the second phase of the previous iteration. The additive form of 
this equation is convenient. Since the tanh and ln tanh functions are monotonic 
on [-rc>, + rc>] the messages can be represented in a (sign, absolute value) 
representation as follows:
https://doi.org/10.1017/9781009283403.004 Published online by Cambridge University Press

84
3 Low-Density Parity-Check Codes
Y : —> F2 x [0, + ro]
x 1—> y(x) = (Y1 (x),Y2(x)) = (sgn(x), Intanh 121 
(3.21)
where the sgn function is defined as
sgn(x) =
0, 
x>0
0, 
with probability1/2 if x = 0
1 , 
with probability1/2 if x = 0
1, 
ifx<0.
The function y (•) has a well-defined inverse y 1. The message representa­
tion used then is
mcv = y 11 
AmvC)
v ZSV cv
(3.22)
The advantages of this additive version of the message passing are clear, 
compared to Equation 3.19 .
To recap, the message formats that have been developed are:
m& 
mvc
mv, 
£ = 0
mV +Ec'SCv,cmC--1), 
£ ^ 1
(3.23)
and
m
= 
cv =
-1
Y
(3.24)
and recall that the pdf of the initial LLR message from variable nodes is the 
pdf of the signal Yv = Xv + N, N ~ N(0,a 2) received from the channel at the 
variable node v is (from Equation 3.11) N(2/a2, 4/a4), i.e., the LLR received 
from the channel in the 0-th iteration is mVc) = 2y/a2 and
pv(0c)(z) =
a
V2n2
exp ( - Z— - 02^ ((8/a2)
While the pdfs cease to be Gaussian as the iterations progress, they remain 
symmetric for the original Gaussian channel noise, i.e., this pdf is symmetric 
by Definition 3.4 (part iv).
Having determined the form of the messages passed on the edges of the code 
Tanner graph, it remains to examine the performance of this message-passing 
algorithm, i.e., to determine the probability of error one might expect. This 
will be discussed in the remainder of this section. Section 3.3 will comment on 
various aspects and properties of the decoding algorithm.
https://doi.org/10.1017/9781009283403.004 Published online by Cambridge University Press

3.2 Performance of LDPC Codes on the BIAWGN Channel
85
I rpn nil1 hv n(^)rv^thpndfpfthp tnpQQfi op m (^) ',i m I p)^ l lip ppri'PQnnt'irli 11 o 
enote y pvc (x ) te p o te message mvc an vc te corresponng 
(£)
cdf. Let Pe denote the error probability at the £-th iteration. As noted, the 
message pdfs (pcv and pvc ) are symmetric functions as the message passage 
algorithm decoder iterates. A consequence of this is that ([20], lemma 1) the 
error probability is independent of the codeword chosen for transmission. Thus 
without loss of generality it can be assumed the all +1 codeword (in the {±1} 
code) was transmitted to compute the error probability.
The message mVC at the £-th iteration is the log likelihood of node v’s 
belief it represents a transmitted +1 over that of -1. Since itis assumed the all 
+1 codeword was transmitted, if the process was stopped at this iteration an 
error would be made if this message was negative. In the following analysis it 
is assumed (without further comment) the messages are independent random 
variables, i.e., the cycle-free case, together with the fact that each node uses 
only extrinsic information in forming messages. Thus the error probability, 
given the all-ones codeword was transmitted, is given by
Pe(e) = 0 pVC(x)dx 
(3.25)
—TO
or in the case the densities have discrete probability components, in terms of 
the cdfs,
pCO = 1 p()( 0) + + p))0)— 
(3 26)
e 
2 v ) ) + v ) ) 
(.)
where half of the probability at the origin is added.
Note again that while the decoder message pdfs are symmetric they are 
not Gaussian - except for the initial pdf from the channel pV01 as shown in 
Equation 3.12 although for a large number of variable nodes the pdf of mV0 
might be modeled as such since, as a sum of a large number of independent 
random variables, the Central Limit Theorem might be applied. However, 
the form of the messages m0V) as in Equation 3.20 with log tanh functions 
precludes normality of the associated pdfs. The issue will be discussed further 
in the next section.
An iterative process will be derived to determine the pdfs of the messages 
passed as the iterations progress which will allow expressions for the proba­
bility of error of the algorithm. Before addressing this problem two comments 
are in order.
It was noted earlier that typically, the decoding process will continue until 
either some fixed number of iterations has been reached or the LLRs passed 
to the variable nodes exceed some predetermined threshold to indicate some 
confidence the decision has been reached. Note that BP often represents a 
feasible (achievable) approximation to ML decoding. Again, the cycle-free 
https://doi.org/10.1017/9781009283403.004 Published online by Cambridge University Press

86
3 Low-Density Parity-Check Codes
case is assumed although in some cases in practice, the number of iterations 
exceeds this limit with satisfactory results.
The pdfs of the messages transmitted are required as the iterations proceed. 
The tracking of the pdfs to be described will be an iterative process and is 
termed density evolution. Itis required to determine the pdfs of the messages of 
the form in Equations 3.23 and 3.24. The situation is complicated somewhat by 
(£)
the fact that the messages mcV are represented on a product space F2 x [0, +rc>] 
with one part of the probability involving discrete components (probability 
mass functions or weighted impulse functions) and the other part continuous. 
Nonetheless the basic tool is convolution and an overview of the technique and 
results will be described. The pdf of a sum of d independent random variables 
will be the d-fold convolution of their respective pdfs.
The pdfs of the message Equations 3.23 and 3.24 for the messages mVc) and 
m (^) ■i| ||> 4 ill i li rul /I /iljvl tt''') 'iil/l tt''') 
r;>l y 7 
l/i l/niil/l
mLcv at the £-th iteration, denoted pvc and pcv , respectively, are to be found. 
The sum of independent random variables can be computed as convolutions 
of their pdfs or products of their Fourier transforms. The following analysis is 
technical and only a sketch of the ideas of the developments in [21] (with minor 
changes in notation) is given. The messages mcV involve y-1 of a sum of 
random variables, each involving the function y and defined over F2x [0, +rc>], 
(£)
The equation for the pdfs of the messages mvc is straightforward. Assume 
for the moment the code graph is (dv,dc)-biregular: then
n(£ +1) = „(0) (2) 
pvc 
pvc ^
® (dv -1)
/ (£-1)\®(dv 1) 
(£)
where pcv 
is the convolution of pcv with itself applied dc - 1
times, as the pdfs are the same for each check node. For the irregular 
(X(x),p(x)) graph, this argument is conditioned on the edge having a certain 
degree and averaged over the distribution of edge degrees and hence for the 
irregular graph with these edge distributions it follows that
<++1 ) = 
(0) _
pvc — pvc ^
where X
= pV0) ® X
(£) \® (i-1)
(3.27)
X i Xi •
(€}
Finding the pdfs for the messages mcv is more difficult due to their form 
as 2-tuples over the product of fields F2 x (0, rc>) and that this form combines 
continuous and discrete functions. Hence the convolutions of such probability 
expressions are more subtle.
(£) 
(£)
Let V(pv,c) be the pdf of the random variable y(mv0) (a 2-tuple) and the 
pdf of the addition of the random variables is found by convolution of these
https://doi.org/10.1017/9781009283403.004 Published online by Cambridge University Press

3.3 Thresholds, Concentration, Gaussian Approximation, EXIT Charts 87
expressions. It is noted this map has a well-defined inverse r 1. Using the 
(£)
argument above, the pdf of the message mcV on an irregular graph with (k, p) 
edge distribution, as given in Equation 3.24, is given by ([21], equation 7)
„++1) = r-1 (0(r(DMAYA where z>(r(d(e)= Vzr (r (d(e) "Y (i — 1 1 
pcv 
1 p 1 pvc 
where p 1 pvc p J pi A pvc 
.
i “2 
(3.28)
(e)
Substituting the expression for pcv into Equation 3.27 yields ([21], 
theorem 2)
<++1) = Y0) <* k(r-1(o(V(n(e) YY 
(3 29)
pvc 
pvc ^ k 1 p 1 pvc 
, 
(3.29)
which, as noted, is a symmetric function.
It was noted in Chapter 2 that this development is entirely consistent with 
ce) 
the result of erasure coding on (the BEC) irregular graphs in that from pvc 
above, one identifies the variable pt of that chapter and the above relation can 
be shown to be equivalent to the relationship for BECs
Pt+1 = P0k( 1 - p( 1-t)), p0 = 3
(where now pt has the interpretation of the probability a variable node is 
unresolved at iteration I) which is the important result of the equation in 
Theorem 2.7 in Chapter 2 achieved by different means.
Several important properties of this result are developed in [20]. As noted 
in Equation 3.26 the expression for the probability of error is given by
P(£) = 1 p()( o ) — + p()0)+ 
(3 30)
e 2 v 
v 
.
ce) 
and ([20], corollary 1) this function converges to 0 as £ —> rc> iff pV 
converges to 3TO, the delta function at infinity (probability mass of 1).
Further interesting and important results on the probability of error are 
noted, namely that under symmetry and stability conditions, the probability of 
error is a nonincreasing function of the number of iterations £ and will decrease 
to zero (see [20], theorems 7 and 8 and corollary 2).
3.3 Thresholds, Concentration, Gaussian 
Approximation, EXIT Charts
There are numerous issues around the design and performance of LDPC codes 
with BP decoding that are not addressed in this brief treatment. A major one is 
the design of edge distributions k(x) and p(x) to minimize the probability of 
https://doi.org/10.1017/9781009283403.004 Published online by Cambridge University Press

88
3 Low-Density Parity-Check Codes
error. This issue was briefly touched on previously and in the previous chapter 
for erasure codes but is not considered here.
The topics of this section include indications of the notions of concentration 
of performance of code, the establishment of thresholds, the use of Gaussian 
approximation to simplify the analysis of density evolution and the decoding 
algorithm and the notion of an EXIT chart as a visual aid in viewing the 
performance of the decoding algorithm.
Thresholds and Concentration
The class of binary memoryless symmetric (BMS) channels is considered 
and in particular the subclass that is parameterized by a real parameter 
Y not to be confused with the mapping of Equation 3.21. Thus as noted 
previously for the BSC, BEC and BIAWGN the parameter would be the 
channel crossover probability p, the channel erasure probability S and the noise 
variance a2, respectively. The capacity of such a channel typically decreases 
with increasing Y (for the range of interest).
The notion of a threshold for a channel and decoding algorithm involves 
being able to conclude that if the probability of error performance converges to 
0 for a given channel parameter Y, then it will converge to zero under the same 
conditions for all parameters y ' < Y .An interesting approach to this problem 
is as follows. Cover and Thomas discuss the notion of a physically degraded 
channel [5]. Consider a channel W with transition probabilities p(y | x). 
A channel W' is said to be physically degraded with respect to W if there 
is a channel Q such that the transition probabilities behave as
pw'(y' | x) = PQ(y' | y)pw(y | x).
In essence a degraded channel may be viewed as the catenation of the original 
channel with another channel that degrades the performance of the original 
one. The following theorem establishes the monotonicity in the performance 
of such channels ([20], theorem 1):
Theorem 3.5 Let W and W' be two given symmetric memoryless channels and 
assume that W' is physically degraded with respect to W. For a given code 
and BP decoder let p be the expected fraction of incorrect messages passed 
at the I-th iteration assuming tree-like neighborhoods with transmission over 
channel W and let p' be the equivalent quantity over W'. Then p < p'.
The theorem ensures monotonicity with respect to a particular decoder if 
convergence for a parameter y' implies convergence for all y < Y'• It certifies 
the notion of a threshold.
https://doi.org/10.1017/9781009283403.004 Published online by Cambridge University Press

3.3 Thresholds, Concentration, Gaussian Approximation, EXIT Charts 89
An instance of such a threshold is contained in ([21], theorem 5) which 
asserts the error performance of a code with an edge degree distributions (X, p) 
pair and symmetric channel noise pdf denoted pv(0c) , for a parameter r
r = — ln f /" 
pV0)(x)e-x/2dx
—<x
then under certain conditions
(i) if X (0)p (1) > er there exists a constant f such that for all
£ e N, P^ > f;
(ii) if X (0)p (1) < er there exists a constant f such that if, for some
£ e N, Pe^ < f, then Pe^ converges to 0 as £ tends to rc>.
Another important contribution of [20] is the notion of concentration of 
code performance. Applied to the ensemble of randomly generated ensemble 
of graphs Cn(X,p), it asserts that the behavior ofa code chosen at random from 
this ensemble over the noisy channel concentrates around the average behavior 
of this ensemble as the code length increases. Thus if the channel parameter 
is below the threshold, then any small, desired error probability for such a 
randomly chosen graph can be achieved, with probability approaching one and 
exponential code length n, with a fixed number of iterations £ (which depends 
on the error probability). A converse to this statement can also be formulated. 
Thus the average performance of a graph in this ensemble is a good predictor 
of performance for any randomly chosen graph in the ensemble.
A particular instance of concentration is the following for a (dv,dc)- 
biregular Tanner graph. Let Z be the number of incorrect messages being sent 
out on all dvn edges from variable nodes (of a (dv,dc)-biregular code/graph 
with n variable nodes) at the £-th iteration, and its expected value E [Z] (over 
all possible such graphs and decoder inputs). Similarly let p be the expected 
number of incorrect messages passed along an edge from a given variable 
node which forms the root of a directed tree of depth 2£ and note that p is 
calculated from the derived densities given in Equation 3.27. Then it is shown 
([20], theorem 2) that
Pr (| Z — E[Z] |> ndve/2) < 2exp(—fie2n) 
(3.31)
where fi is a function of graph parameters and iteration number £. Thus finding 
the mean performance for the ensemble of graphs will with high probability be 
a good indicator of actual performance of a given graph.
https://doi.org/10.1017/9781009283403.004 Published online by Cambridge University Press

90
3 Low-Density Parity-Check Codes
Gaussian Approximation
The expressions for the pdfs of the message densities sent from variable and 
check nodes respectively are given in Equations 3.27, 3.29 and 3.28. These 
can be quite difficult to compute in practice. Fourier transforms are usually 
employed since the Fourier transform ofa convolution is the product of Fourier 
transforms and the approach often leads to simpler update equations. Another 
possibility is to approximate the densities as normal (Gaussian) densities. 
Since the messages from check nodes to variable nodes involve logs of tanh 
functions, this is generally far from accurate for these messages. It is often 
approximately true for variable-to-check node messages as these involve the 
sums of independent random variables and if there are a large number of them, 
the Central Limit Theorem might make the proposition approximately true. 
The accuracy of the Gaussian approximation is discussed in greater detail in 
[1] where a technique is developed that considers outputs of variable nodes to 
check nodes in two steps, i.e., considering the outputs of variable nodes over a 
complete iteration. The approach leads to considerable improvements over the 
assumption of normality from both sets of nodes.
Regardless of the limitation of the Gaussian assumption noted, the approach 
is of interest. It has been shown that a normal pdf is symmetric iff its variance 
is twice its mean. Since the initial channel pdf is normal and symmetric and 
density evolution preserves symmetry, the assumption of normality requires 
only that the mean of the messages is tracked. This is an easier approach which 
is considered in several works and in particular [4]. Only the (dv,dc)-biregular 
case will be considered and to distinguish means from messages, means will 
be designated by p and in particular the means of messages from variable and 
check nodes in the £-th iteration are denoted pV and pCV, respectively.
From Equation 3.23 it follows that
(£)   ,00) d 1 
(^-I) 
3 32^
pvc = pvc + (dv- 1)pcv 
(3.32)
and using Equations 3.13 and 3.17 it follows that
tanh
c 'e. Nv,c
(3.33)
and hence
=E
tanh
(3.34)
where the expectations are with respect to the (assumed) normal densities. 
Since all densities are symmetric and assumed Gaussian, each must have its
https://doi.org/10.1017/9781009283403.004 Published online by Cambridge University Press

3.3 Thresholds, Concentration, Gaussian Approximation, EXIT Charts 91
variance twice the mean and the expressions therefore depend only on the 
mean. For this last equation
W\2
E
tanh
4 ^cv
du.
If [4] the function
0(x) =
1 
, 
/ 
\2 .
1 - 
R tanh(u) exP (- u-x^ du, x> 0
1, 
x=0 
is defined, then the update equations for the check means are
/ 
\ dc 1
= $-U1 - 1 - ^M0 + (dv - 1 )/<. 1))] 
) 
(3.35)
where ^0 is the mean from the channel and ^CV = 0. This equation yields the 
updates for the means of messages from the check nodes (under the Gaussian 
assumption) and, along with Equation 3.32, is used to update the variable 
messages.
Under the Gaussian assumption, from these means, the variances can 
be found and the pdf of the messages at each iteration, and from this the 
probability of error. The thresholds below which the error probability tends to 
zero can thus be determined. As an example (from table II of [20]) for a (3, 6)- 
biregular code (rate 1/2) the variance achieved under Gaussian approximation 
is 0.8747 while the exact value from the previous analysis is 0.8809. The 
variance corresponding to the capacity of 0.5 (the rate of the code) is 0.979 
(from Equation 3.2). From the same table, the general level of accuracy of the 
Gaussian approximation technique is impressive. However, in spite of these 
comments, it is clear in practice [1] the accuracy of the Gaussian assumption 
is limited, especially for messages sent from the check nodes.
EXIT Charts
The notion of an EXIT is a graphical means of tracking the process of the BP 
decoder as it iterates on the Tanner graph of the code. As such, it is a valuable 
tool for designing the code and estimating decoding thresholds. It arose in the 
seminal works of Stephan ten Brink and colleagues [2, 26, 27]. A variety of 
metrics can be used for this process, such as the mean, SNR, probability of 
error and mutual information between a graph message node information bit 
and output message node bit decision. Mutual information is often regarded as 
the most useful and will be discussed here.
https://doi.org/10.1017/9781009283403.004 Published online by Cambridge University Press

92
3 Low-Density Parity-Check Codes
As the decoding process progresses the improvement in the decoding 
performance from the input message of a node (either variable or check) and 
the output message, in terms of the quality of the information bit estimate is 
tracked. The message at the input to the node is regarded as a priori information 
and is designated with a subscript A. The output information is regarded 
as extrinsic and is designated with a subscript E. If the decoder is working 
well, one can expect the mutual information to improve with iterations and to 
migrate from some initial point on a plot to the point (1, 1) where the bit is 
successfully decided.
The analysis assumes a (dv,dc)-biregular bipartite graph, the cycle-free 
case and a large codeword length (number of variable nodes) and a large 
number of decoding iterations. It will further be assumed that the messages 
have a symmetric normal pdf and from the previous discussion on Gaussian 
approximation it is sufficient to track means since the variance of a symmetric 
pdf is twice the mean (see line before Equation 3.10). For a variable node with 
(£) 
(£)
input message random variable mcv and output message random variable mvc 
at some iteration, the mutual information between these two random variables 
and the information bit associated with the variable node v are denoted IA,v 
and IE,v, respectively. The similar quantities for a check node c are denoted 
IA,c and IE,c, respectively.
Using Equation 3.32 it follows that
aV2 = an + (dv - 1 )aA
(0)
where a2 is the variance of the message mVC leaving variable node v, a,2 is 
the variance of the channel noise and aA is the variance of the message mCv) 
entering the variable node. The mutual information between the information bit 
(£)
associated with variable node v (say Xv) and message mvc, which is denoted 
by m,is
Ie,v = H(X) - I(X | M)
= 1 - E 2 C P(m | x) g2
lo
= 1 - /-+» p(m I x = +1) log2(1 + e-m)dm
where p ~ N(a 2/2,a 2). It is convenient [9,22] to designate this expression as
p(m | x =+1) + p(m | x =-1) 
----------------------------------------- I dm
p(m | x)
IE,v = J(an) = J 
an2 + (dv - 1)aA2
In a similar development
IA, v = J (aA)
https://doi.org/10.1017/9781009283403.004 Published online by Cambridge University Press

3.3 Thresholds, Concentration, Gaussian Approximation, EXIT Charts 93
(f.) „
which is the mutual information between variable node inputs mcv (hence a 
priori) and the channel bit Xv .
This function can be shown to be monotonic and has an inverse aA = 
J -1(IA,v) and hence
Ie,v = J^y/rf + (dv - 1) I J-1 (Ia,v) I2
It is possible to plot the extrinsic information IE,v against the intrinsic informa­
tion IA,v to monitor progress of the algorithm, i.e., monitor the improvement 
of the bit estimates as it goes through the iterations through variable nodes. 
For binary inputs, the range of both quantities is (0, 1) and, as the algorithm 
progresses, the mutual information increases horizontally or vertically if the 
algorithm is working correctly. In a similar fashion one can compute IA,c and 
IE,c through check nodes and these can also be shown on the same graph with 
the axes reversed. However, this relationship is a little more complicated due 
to the form of the messages transmitted, namely (see Equation 3.33):
mcv = 2tanh-1 f n tanh ^'),c/2^ 
Vv 'e Nc,v
Nonetheless, accurate approximations have been developed and it is shown 
[9] that
Ie,c = 1 - ^](dc - 1 )(J-1 (1 - Ia,^))2
Figure 3.2 shows a representative example (exaggerated somewhat for 
clarity) as two curves in the unit square. In order for the decoding to be 
successful, the chart of the variable nodes must lie above the inverse of that 
of the check nodes. As the channel parameter is adjusted, at the point where 
the curves just touch determines the threshold of the algorithm.
As the algorithm progresses (assuming successful decoding) at each step 
the mutual information functions increase with either a horizontal move to 
the right or vertical move upward, depending on the node being considered. 
The moves tend toward the upper right corner (1,1) where certainty on the 
transmitted bit is achieved.
Numerous important properties of EXIT charts have been shown (see 
[2, 9, 26, 27]). As the noise variance on the channel increases for the BIAWGN 
the curves approach each other and eventually intersect. As noted, the point 
at which this occurs is the channel threshold as decoding then becomes 
impossible as the decoding trajectory cannot work its way through to the upper 
right-hand corner and the decoding algorithm fails. Thus the EXIT chart is
https://doi.org/10.1017/9781009283403.004 Published online by Cambridge University Press

94
3 Low-Density Parity-Check Codes
IA, v or IE, c
Figure 3.2 Typical shape of an EXIT chart for a biregular code
useful to determine thresholds for codes. In addition it has been used to design 
good codes.
While mutual information is generally regarded as the most accurate 
parameter to use for these charts, other parameters such as means, SNR and 
error probability have also been used [1, 6, 7].
Another interesting and useful aspect of EXIT charts is their area property, 
considered in [2] and further in [18]. Define [22] the following areas:
Ac = IE, c(IA,c)dIA,c and Av = IE, v (IA, v)dIA, v
(on respective axes). It can be shown that, for dc and dv , the average number 
of check and variable nodes,
. — ... —
Ac = 1 /dc and Av = 1 - (1 - C)/dv
for C the channel capacity. Numerous other relationships are noted in the 
works cited.
Comments
This chapter has been an introduction to the important class of LDPC 
codes. Many aspects of the message-passing variables and expressions for the 
probability of error have been quite technical, although the notion of estimating 
https://doi.org/10.1017/9781009283403.004 Published online by Cambridge University Press

References
95
information bits, the estimates improving with each iteration, seems intuitive. 
Reading the original works is indispensable for a deeper understanding of the 
subject. These include the seminal works of Gallager [8] and Richardson et 
al. [20, 21]. The survey articles of Shokrollahi [25] and Guruswami [10] and 
the volumes [12, 13, 19, 22] and [9] cover a variety of other construction and 
performance aspects of the codes and also make for excellent reading. Other 
fundamental references for LDPC codes include [14, 15, 16, 17, 23, 24].
References
[1] Ardakani, M., and Kschischang, F.R. 2004. A more accurate one-dimensional 
analysis and design of irregular LDPC codes. IEEE Trans. Commun., 52(12), 
2106-2114.
[2] Ashikhmin, A., Kramer, G., and ten Brink, S. 2004. Extrinsic information transfer 
functions: model and erasure channel properties. IEEE Trans. Inform. Theory, 
50(11), 2657-2673.
[3] Bazzi, L., Richardson, T.J., and Urbanke, R.L. 2004. Exact thresholds and optimal 
codes for the binary-symmetric channel and Gallager’s decoding algorithm A. 
IEEE Trans. Inform. Theory, 50(9), 2010-2021.
[4] Chung, S.-Y., Richardson, T.J., and Urbanke, R.L. 2001. Analysis of sum-product 
decoding of low-density parity-check codes using a Gaussian approximation. 
IEEE Trans. Inform. Theory, 47(2), 657-670.
[5] Cover, T.M., and Thomas, J.A. 2006. Elements of information theory, 2nd ed. 
Wiley, Hoboken, NJ.
[6] Divsalar, D., Dolinar, S., and Pollara, F. 2001. Iterative turbo decoder analysis 
based on density evolution. IEEE J. Select. Areas Commun., 19(5), 891-907.
[7] El Gamal, H., and Hammons, A.R. 2001. Analyzing the turbo decoder using the 
Gaussian approximation. IEEE Trans. Inform. Theory, 47(2), 671-686.
[8] Gallager, R.G. 1963. Low-density parity-check codes. MIT Press, Cambridge, 
MA.
[9] Gianluigi, L., Song, S., Lan, L, Zhang, Y., Lin, S., and Ryan, W.E. 2006. Design 
of LDPC codes: a survey and new results. J. Commun. Softw. Syst., 2(3), 191-211.
[10] Guruswami, V. 2006. Iterative decoding of low-density parity check codes. Bull. 
Eur. Assoc. Theor. Comput. Sci. EATCS, 53-88.
[11] Johnson, S.J. 2010. Iterative error correction. Cambridge University Press, 
Cambridge.
[12] Li, J., and Li, B. 2017. Beehive: erasure codes for fixing multiple failures 
in distributed storage systems. IEEE Trans. Parallel Distrib. Syst., 28(5), 
1257-1270.
[13] Lin, S., and Costello, D.J. 2004. Error control coding, 2nd ed. Prentice Hall, 
Upper Saddle River, NJ.
[14] Luby, M.G., Mitzenmacher, M., Shokrollahi, M.A., Speileman, D., and Stenman, 
V. 1997. Practical loss-resilient codes. Pages 150-159 of: Proceedings of the 
https://doi.org/10.1017/9781009283403.004 Published online by Cambridge University Press

96
3 Low-Density Parity-Check Codes
Twenty Ninth Annual ACM Symposium on the Theory of Computing 1997. ACM, 
New York.
[15] Luby, M.G., Mitzenmacher, M., Shokrollahi, M.A., and Spielman, D. 1998. 
Analysis of low density codes and improved designs using irregular graphs. Pages 
249-258 of: Proceedings of the Thirtieth Annual ACM Symposium on the Theory 
of Computing 1998. ACM, New York.
[16] Luby, M.G., Mitzenmacher, M., and Shokrollahi, M.A. 1998. Analysis of random 
processes via And-Or tree evaluation. Pages 364-373 of: Proceedings of the 
Ninth Annual ACM-SIAM Symposium on Discrete Algorithms (San Francisco, 
CA, 1998). ACM, New York.
[17] Luby, M.G., Mitzenmacher, M., Shokrollahi, M.A., and Spielman, D.A. 2001. 
Improved low-density parity-check codes using irregular graphs. IEEE Trans. 
Inform. Theory, 47(2), 585-598.
[18] Measson, C., Montanari, A., Richardson, T.J., and Urbanke, R.L. 2009. The 
generalized area theorem and some of its consequences. IEEE Trans. Inform. 
Theory, 55(11), 4793-4821.
[19] Richardson, T.J., and Urbanke, R.L. 2008. Modern coding theory. Cambridge 
University Press, Cambridge.
[20] Richardson, T.J., and Urbanke, R.L. 2001. The capacity of low-density parity­
check codes under message-passing decoding. IEEE Trans. Inform. Theory, 47(2), 
599-618.
[21] Richardson, T.J., Shokrollahi, M.A., and Urbanke, R.L. 2001. Design of capacity­
approaching irregular low-density parity-check codes. IEEE Trans. Inform. The­
ory, 47(2), 619-637.
[22] Ryan, W.E., and Lin, S. 2009. Channel codes. Cambridge University Press, 
Cambridge.
[23] Shokrollahi, M.A. 2000. Codes and graphs. Pages 1-12 of: In STACS 2000 
(invited talk), LNCS No. 1770.
[24] Shokrollahi, M.A. 2001. Capacity-achieving sequences. Pages 153-166 of: 
Codes, systems, and graphical models (Minneapolis, MN, 1999). The IMA 
Volumes in Mathematics and Its Applications, vol. 123. Springer, New York.
[25] Shokrollahi, M.A. 2004. LDPC codes: an introduction. Pages 85-110 of: Feng, 
K., Niederreiter, H., and Xing, C. (eds.), Coding, cryptography and combina­
torics. Birkhauser, Basel.
[26] ten Brink, S. 1999. Convergence of iterative decoding. Electron. Lett., 35(10), 
806-808.
[27] ten Brink, S., Kramer, G., and Ashikhmin, A. 2004. Design of low-density 
parity-check codes for modulation and detection. IEEE Trans. Commun., 52(4), 
670-678.
https://doi.org/10.1017/9781009283403.004 Published online by Cambridge University Press

4
Polar Codes
The advent of polar coding in the remarkable paper by Erdak Arikan [3] 
represents a unique and important milestone in the history of coding and 
information theory. With almost no prior literature on such codes, it presented 
a theory of code construction, encoding and decoding algorithms and detailed 
performance analysis. Indeed, polar coding is the first known explicit con­
struction with rigorous proofs of achieving rates within e of capacity with 
block length, encoding and decoding complexity bounded by polynomials 
in 1 /e [18]. The aim of this chapter is to introduce the essential notions 
involved with polar codes drawn largely from this seminal work. Besides its 
extraordinary technical contributions it is a beautiful exposition of information 
theory.
The next section outlines some of the needed information-theoretic notions 
required and their properties. Section 4.2 introduces the basic ideas of polar 
coding through properties and construction of their generating matrices. Using 
a series of structured simple matrix operations, a base binary-input discrete 
memoryless channel (BDMC), W, is transformed into a channel with N = 2n 
inputs and N outputs, designated WN , referred to as the compound channel. 
The final section of the compound channel is a parallel bank of N base 
channels W whose outputs are the compound channel outputs. It will be noted 
that since the generator matrix involves 2n information bits and a like number 
of “coded” bits, polar codes involve different concepts than ordinary parity­
check codes.
The final Section 4.3 shows how, from the defined compound channel 
WN, N binary-input subchannels, WN(i),i = 1,2,...,2n can be defined and 
it is these channels that will be shown to have been polarized, i.e., for large 
n a subset of the N = 2n subchannels will approach noiseless or perfect 
channels. The other channels will approach useless channels, i.e., channels 
that are not capable of transmitting any data reliably. Further, the number 
97
https://doi.org/10.1017/9781009283403.005 Published online by Cambridge University Press

98
4 Polar Codes
of perfect channels is such that the capacity of the base channel W can be 
achieved per input information bit. Thus if the binary-input base channel W has 
capacity C < 1 the number of perfect subchannels which each have a capacity 
approaching unity will be approximately NC and these channels can be used to 
transmit the information (without error) to achieve per input bit capacity of C . 
The polar code is the determination of the good channels. A decoding process, 
successive cancellation decoding, will be defined as a natural consequence of 
the series of transformations used in the construction of the channels, and its 
performance will be derived.
The coding problem will be to identify which are the good subchannels. 
Once identified, only the good subchannels will be used for near-perfect 
communication and the bad subchannels will be referred to as frozen with 
predetermined bits conveying no useful information. That there is a sufficient 
number of good channels WN(i) that allows achieving overall capacity of the 
system per input bit is a crucial result of the work.
The basic ideas of polar codes are revolutionary and simple although the 
analysis requires effort. The iterative processes defined are highly structured 
and relatively simple and the important contribution of the work is to show how 
the iterative constructions and the resulting decoding strategies achieve the 
properties desired. Many of the proofs involve intricate information-theoretic 
computations. As with other chapters the emphasis will be to place the results 
in context rather than to trace the developments. The work is an impressive 
exercise in the power of information-theoretic notions.
4.1 Preliminaries and Notation
As in previous chapters, denote a row vector a e Fn over the field F by a = 
(a 1 ,a 2,... ,an),ai e F. A matrix A over F is denoted A = (aij),aij e F and 
the Kronecker or tensor product of two matrices A and B is A 0 B = (aijB). 
Simple properties of the product include
(A 0B)(C0D) = (AC 0 BD)
(A 0B) 0C =A 0 (B0C) 
(4.1)
(A 0 B)-1 =A-1 0B-1
assuming dimension-compatible matrices. In particular the n-fold tensor prod­
uct of a matrix A is denoted A0n = A 0 A0(n-1) = A 0 A 0- • • 0 A (n times) 
and A01 = A.
As previously, the probability distribution of the discrete random variable 
X, Pr(X = x) will be denoted PX(x) or as P(x) when the random variable 
https://doi.org/10.1017/9781009283403.005 Published online by Cambridge University Press

4.1 Preliminaries and Notation
99
is understood. Similarly a joint discrete random variable X x Y (or XY) 
is denoted Pr(X = x, Y = y) = PXY (x,y). The conditional probability 
distribution is Pr(Y = y | X = x) = P(y | x). A probability density function 
(pdf) for a continuous random variable X will be designated similarly as 
pX(x).
Certain notions from information theory as introduced in Chapter 1 will 
play a central role for polar codes and are repeated here for convenience. The 
entropy of a discrete ensemble X = {xi,P(xi), i = 1,2,...} is given by
H(X) = - £ P(xi) log P(xi)
and similarly the joint entropy of two ensembles is given by
H(X, Y) = - £ P(x, y) log P(x, y).
The mutual information between discrete ensembles X and Y is given by 
I(X; Y) = V V P(xi,yj) log P(xi,yj) 
( ; ) 
) g P(xi)P(yj)
and measures the amount of information that one of the variables gives on the 
other. It can also be shown that (Chapter 1)
I(X;Y)=H(X)-H(X|Y).
A discrete memoryless channel (DMC), also introduced in Chapter 1,is 
a discrete input set X, an output set Y such that for an input vector x = 
(x 1 ,x2,... ,xn), xi e X the probability of receiving the DMC output vector 
j e Yn is
n
P(y | x) = 
W(yi | xi)
where W(yi | xi) is a channel transition probability, the probability of output 
yi given input xi .
As has been noted, the channels BEC, BSC and general BDMC are shown 
in Figure 1.1 and the capacities of the BSC and BEC were noted there as (see 
Equation 1.15)
CBSC = I(WBSC) = 1+ P log2 P +(I-P) log2(I-P), CBEC = I(WBEC) = I-S.
This first relation is often written
I (WBSC) = 1 - H2(P) where H2(P) = -P log2 P - (1 - P) log2(1 - P) 
https://doi.org/10.1017/9781009283403.005 Published online by Cambridge University Press

100
4 Polar Codes
is the binary entropy function of the two-element ensemble {p, 1 - p}.
The notion of the BIAWGN channel, shown in Figure 1.2, has capacity
Cbiawgn = - j p(y) log2 p(y)dy — 1 log2(2nea2)
where the pdf p(y) is given in Equation 1.16. The general shape of these 
capacity functions is shown in Figure 1.3.
The notion of computational cutoff rate plays a role in the formulation of 
polar codes. This concept arose in the use of convolutional codes with sequen­
tial decoding, a particular backtracking decoding algorithm for such codes. 
The computational cutoff rate then is the rate at which the expected decoding 
complexity becomes unbounded. Polar codes arose out of considerations of 
the properties of this function in the research of sequential decoding [16, 35]. 
For a general DMC it will be defined for the channel W with input distribution 
{P(xi)}, as [27]:
Ro(X; Y) = - log £ £ P(xi\/W(yj | xi)
2
For a BSC with crossover probability p the computation gives
Ro(X, Y) = 1 - log2 (1 + 2^p( 1 - p)) .
The interest in these quantities for this chapter is the bounds they pro­
vide [27]:
I(X; Y) > Ro(X; Y).
Another parameter of interest for (binary input) BDMCs, apart from the 
channel capacity I(W), is the Bhattacharyya parameter expressed as
Z(W) = £ VW(y | 0)W(y | 1)
y eY
and this is often taken as an upper bound on the probability of deciding in error 
on a BDMC under maximum-likelihood detection. A simple argument is given 
to show this. For channel inputs X ={o, 1} let Yo be defined as
Yo = {y e Y | W(y | 0) < W(y | 1)}
Let Pe, o be the probability of error under maximum-likelihood detection, given 
o was sent. Then
Pe,0 = 
W(y | 0).
yeYo
https://doi.org/10.1017/9781009283403.005 Published online by Cambridge University Press

4.1 Preliminaries and Notation
101
For y e Y0, \/W(yTT)7W(yT0) > 1 hence this expression can be multiplied 
by this amount to get the bound
Pe,0 < E VW(y 10)W(y 11) 
yeY0
and summing over the entire Y rather than just Y0 gives
Pe,0 < 
W(y | 0)W(y 11).
yeY
Finally,
Pe = 2 Pe,0 + 1 Pe, 1 < £ VW(y | 0)W(y | 1) = Z(W).
yeY
A more refined argument [37] shows Pe < 2Z(W).
Thus Z(W) can be viewed as an upper bound on the probability of error 
under ML decoding when the channel is used to transmit a single bit. It serves 
as a reliability measure for the channel since for large Z(W) the channel is 
unreliable. Conversely, I(W)is the channel symmetric capacity or rate and the 
closer I(W) is to unity, the more reliable the channel. Intuitively one would 
expect I(W) ^ 1 when the channel is very good and hence Z(W) ^ 0 since 
the probability of error would be low. Conversely I(W) ^ 0 when Z(W) ^ 1.
It is clear that
Z(Wbec) = S and Z(Wbsc) = 
p( 1 - p)
(where S is the erasure probability for the BEC and p the crossover probability 
for the BSC) and hence when S ~ 1, (Z(WBEC) ~ 1) the BEC outputs mainly 
erasures and is useless while for p ~ 172 the BSC has Z(WBSC) ~ 1 and 
produces almost random {0, 1} outputs, hence is also useless. As shown in [3], 
many of the expressions of interest can be evaluated exactly for the BEC while 
only bounds are available for the BSC.
The following relationships between these two functions are of interest:
Proposition 4.1 ([3, 5]) For any BDMC W
(i) I(W) > log1+Z2(W)
(ii) I(W) 1 — Z(W)2
(iii) I(W)+ Z(W) > 1 (equality for BEC) 
(iv) I(W)2 + Z(W)2 < 1.
(4.2)
https://doi.org/10.1017/9781009283403.005 Published online by Cambridge University Press

102
4 Polar Codes
Furthermore for a BDMC one can define a variational distance for the 
channel W as
d(W) = 1 Elw(y 10) - w(y ।1 )l
and it can be shown that I(W) < d(W) and d(W) < y/1 - Z(W)2.
The remainder of the chapter will describe the generator matrix that defines 
the polar codes, a decoding algorithm for them and a probability of error 
expression for their performance. It presents a limited overview of the original 
reference [3] which is essential reading for a deeper understanding of the key 
concepts.
4.2 Polar Code Construction
As noted, three types of channels will be defined, the (given) base channel W 
and two other channels defined by recursion. The base channel is a BDMC with 
binary input from X ={0, 1} and assumed finite number of channel outputs, 
Y. The compound channel WN , where N = 2n for some positive integer n, 
will have N binary inputs and N outputs, each from Y. The construction of 
this compound channel is of a recursive and highly structured nature. The final 
stage of this compound channel will involve N copies of the base channel 
W in parallel whose N outputs are the compound channel outputs. From the 
compound channel WN, the same number of subchannels WN(i),i = 1,2,...,N 
will be defined. These will be the central players of the subject. It is these 
subchannels that will be polarized and form the heart of polar coding. The 
subchannel WN(i) has a single binary input ui and N+ (i - 1) outputs consisting 
of the N compound channel outputs and the (i - 1) previous input bits 
u1,...,ui-1, assumed to have been provided. A decision on the i-th bit ui will 
be made sequentially, based on these outputs noting that the channel outputs 
are functions of all input bits.
The notion of the subchannels is intimately connected to the successive 
cancellation decoding algorithm described in Section 4.3. While their defini­
tion using the constructed compound channel WN is perhaps not obvious, it is 
the fact that their polarization can be proved in a rigorous manner that makes 
them of such great interest. The final segment of the compound channel WN 
will involve N copies of the base channel W in parallel.
Several sets of input and output variables are defined during the devel­
opment. The inputs to the compound channel will be designated ui,i = 
1,2,...,N, ui e {0,1},N = 2n , or uN = (u 1 ,u2,...,uN), and are the 
https://doi.org/10.1017/9781009283403.005 Published online by Cambridge University Press

4.2 Polar Code Construction
103
information bits. Compound channel outputs will be yi,i = 1,2,... ,N, yi e 
Y or y1N . The inputs to the base channels at the final stage of the compound 
channel will be xi, = 1, 2,...,N, xi e X, the codewords generated from the 
compound channel input and the generator matrix GN . Thus x1N = u1N GN 
for the generator matrix GN to be described. As noted this is not the usual 
form of a generator matrix as in algebraic coding theory since it is a square 
N x N matrix that is of full rank over F2. A recursive figure of the situation, as 
shown in Figure 4.2, will be developed. Numerous sets of other variables will 
be introduced as the development proceeds as needed.
The remainder of this section will describe the recursive construction of the 
binary N x N = 2n x 2n generator matrix GN and its relevant properties via 
sets of other matrices performing specific actions. The relationships developed 
will be applied to the polar coding problem in the next section.
Denote the matrix
F = 1101 
(4.3)
which will be referred to as the kernel matrix. Other kernels have been 
considered and these will be commented on later. Tensor products of this 
matrix with itself will figure prominently in the construction. Notice that F 
over F2 satisfies the relation
F2 = I2 or F-1 = F over F2.
Define recursively for N = 2n
Fn = F®n = F ® Fn/2 = F ® F® (n-1), F01 = F
and F®n is a N x N binary matrix. It follows that
(F ® n )-1 = F ® n, 
over F2.
Define the permutation transformation RN,anN x N permutation matrix 
as follows. Operating on any input row vector a1N = (a1,a2,a3,...,aN) this 
permutation produces the output b1N as follows:
aN • Rn = bN = (a 1 ,a3, ..., aN-1 ,a2,a4, ... ,aN),
i.e., it rearranges the components of the input vector to have all the odd 
subscript coordinates sequentially first followed by the even subscript coor­
dinates. The notation a1N is used rather than simply a when discussion is on the 
coordinates.
It will be convenient to define the N x N matrix SN using these two 
matrices as:
SN = (IN/2 ® F)RN . 
(4.4)
https://doi.org/10.1017/9781009283403.005 Published online by Cambridge University Press

104
4 Polar Codes
By direct computation it is verified that
Vv = (In/2 0 F)Rn = Rn(F 0 In/2)
(4.5)
or in terms of matrices
F 0 ••• 0’
0 f • 0
SN =
.. . 
.
.. . 
.
RN
.. . .
00 • • • f_
RN
IN/2 
0
IN/2 IN/2
Equivalently, for any input row vector a1N , by direct computation
a1N SN = a1N (IN/2 0 F)RN = a1N RN (F 0 IN/2)
= (a 1 ® a 2 ,a 3 ® a 4, ... ,aN-1 ® aN,a 2 ,a 4, ... ,aN).
This structure of S8 is shown in Figure 4.1(b) and SN in (c).
The matrices F, FN , RN and SN are used recursively to derive binary- 
coded symbols via the generator matrix GN . The data binary information 
input sequence uN e {0,1}N is encoded to yield a binary-coded sequence 
xN e {0,1}N via an N x N generator matrix GN, i.e.,
x1N = u1NGN, x1N e{0, 1}N.
u1
u2
u 1 ® u 2
u2
(a)
(b)
Figure 4.1 (a) F ; (b) construction of S8; (c) SN
u1 
u2
uN/2-1 
uN/2 
uN/2+1
uN-1 
uN
SN = (IN/2 0 F)RN 
(c)
F
https://doi.org/10.1017/9781009283403.005 Published online by Cambridge University Press

4.2 Polar Code Construction
105
Each code bit is then transmitted through the base channel BDMC W to yield 
the compound channel outputs jN e YN. Thus the channel output symbols 
over the alphabet Y are given probabilistically by
. 
... 
... 
.A
WN j1N | u1N = WN j1N | x1N = u1NGN = 
W(yi | xi), 
(4.6)
i=1
the relation that governs the overall input, coding and channel output of the 
system. The designation of W N is simply WN with the variables x rather than 
u as the relationship is one-to-one. Based on the received word j1N at the output 
of the compound channel WN , decisions as to the system information inputs 
will be made.
The above transformations and permutations are used to recursively con­
struct the generator matrix, using the commutativity noted above in Equation 
4.5, given by ([3], section VIIA):
GN = (IN/2 ® F)RN(12 ® GN/2), N > 2, G1 = 11
= SN(12 ® GN/2 )
= RN(F ® IN/2)(12 ® GN/2)
= RN(F ® GN/2).
(4.7)
The matrix version of this recursion would be:
N/2 
0
N/2 GN/2
SN
GN/2 
0
0 GN/2
GN = SN(12 ® GN/2) 
P JnN | xN) = nN=1 W(yi | xi)
Figure 4.2 Recursive generator matrix representation of WN : x1N = u1N GN
https://doi.org/10.1017/9781009283403.005 Published online by Cambridge University Press

106
4 Polar Codes
Numerous recursion relationships can be developed between many of the 
matrices introduced. A few selective ones are considered. Thus GN/2 = 
RN/2(F 0 GN/4) and using Equation 4.1:
GN = RN(F 0 RN/2(F 0 GN/4))
= RN(12F 0 RN/2(F 0 GN/4))
and using Equation 4.1 (with A = 12, B = F, C = RN/2 and D = (F0 GN/4)), 
this equation can be written as
GN = RN(12 0 RN/2 )(F02 0 GN/4).
This is a convenient form to repeat the iterations to give:
GN = RN(I2 0RN/2)(I4 0RN/4) • • • (IN/2 0R2)F0n = BNF0n = BNFN,
(4.8)
where FN = F0n and
BN = RN(12 0RN/2)(14 0RN/4) ••• (IN/2 0R2), 
(IN/2 0R2) = IN.
(4.9)
In matrix form this equation can be expressed as 
BN = RN
RN/2 0 
0 RN/2
I2 0 RN/4 
0
0 
I2 0 RN/4
IN/4 0 R2 
0
0 
IN/4 0 R2
and from the block diagonal form of this expression it follows from the “top 
half” of the equation that
BN = RN (I2 0 BN/2) 
(4.10)
where the last equation is seen by writing the full matrix for BN and 
observing that
BN/2 = RN/2 (12 0 RN/4) • • • (IN/4 0 R2).
It is clear by definition that each term in this expression for BN of the form 
(I2i 0RN/2i) is a permutation matrix and since BN is a product of permutation 
matrices, it is a permutation matrix.
To gather the recursions Equations 4.7 and 4.10 and the trivial one for FN 
we have
GN = RN(F0GN/2), BN = RN (I2 0BN/2) and FN = F0FN/2, N= 2n.
(4.11)
Example 4.2 The construction of G8 illustrates the form of the matrices. 
Thus
G8 = S8(I2 0 G4) = B8F8 and B8 = R8(I2 0 B4).
https://doi.org/10.1017/9781009283403.005 Published online by Cambridge University Press

4.2 Polar Code Construction
107
Thus
-1 0 0 0-
’1 0 0 0-
-1 0 0 0-
0 0 10
110 0
10 10
0 10 0
10 10 =
110 0
0 0 0 1
1111
1111
and
G 8 = 5 8
(12 ® G 4)
1 0 0 0 0 0 0 0
1 0 0 0 0 0 0 0
1 0 0 0 1 0 0 0
1 0 1 0 0 0 0 0
0 1 0 0 0 0 0 0
1
1 0 0 0 0 0 0
0 1 0 0 0 1 0 0
1
1
1
1 0 0 0 0
=
0 0 10 0 0 0 0
0 0 0 0 1 0 0 0
0 0 10 0 0
1 0
0 0 0 0 1 0
1 0
0 0 0 10 0 0 0
0 0 0 0 1
1 0 0
0 0 0 10 0 0
1
0 0 0 0 1
1
1
1
1 0 0 0 0 0 0 0
1 0 0 0 1 0 0 0
1 0 10 0 0 0 0
1 0 10 10
1 0
=
1
1 0 0 0 0 0 0
1 10 0 11 0 0
1 1110 0 0 0
1 11111
1
1
Alternatively
-1 0 0 0 0 0 0 0' 
0 0 0 0 1 0 0 0 
0 1 0 0 0 0 0 0 
0 0 0 0 0 1 0 0 
0 0 1 0 0 0 0 0 
0 0 0 0 0 0 1 0 
0 0 0 1 0 0 0 0
.0 0 0 0 0 0 0 1.
1 
0 0 0 0 0 0 0'
1 
0 1 0 0 0 0 0
1 
1 0 0 0 0 0 0
1 
1 1 1 0 0 0 0
1 
0 0 0 1 0 0 0
10 10 10 10 
110 0 110 0 
.11111111.
G 4 = B 4 • F 4
G8 = R 8 (F ® G4)
-1 0 0 0 0 0 0 0­
1 0 0 0 1 0 0 0
1 0 1 0 0 0 0 0
10 10 10 10
1 1 0 0 0 0 0 0
110 0 110 0 
1 1 1 1 0 0 0 0 
.11111111.
https://doi.org/10.1017/9781009283403.005 Published online by Cambridge University Press

108
4 Polar Codes
Additionally
B 8 = R 8 (12 ® B 4)
’1 0 0 0 0 0 0 0’
’10000000’
00001000
00100000
01000000
01000000
00000100
00010000
00100000
00001000
00000010
00000010
00010000
00000100
00000000
00000001
’10000000’
00001000
00100000
00000010
01000000
00000100
00010000
00000001
More can be said of the structure of BN . Consider a vector u of dimension 
N and N x N matrix M with coordinate positions and rows/columns labeled 
with binary n-tuples. Thus, rather than index coordinate positions with integers 
from 1 to 2n they can be indexed by binary n-tuples, the binary expansions 
of 0, 1, 2,...,2n - 1. The two methods are equivalent. The bit reversal of 
the vector u is v if u = (u00-00,u00-01,...,u 11...10,u 11---11) and v = 
(u00-00,u 10-00,... ,u01-11 ,u 11-11).
The matrix M is said to be a bit reversal matrix if v = uM and v is the 
bit reversal vector of u. Thus v(i1,i2,...,in) = u(in,in-1,...,u1), i.e., the binary 
expansion of the vector subscripts are reversed.
It is claimed that BN is a bit reversal matrix. A recursive proof of this 
follows from the recursion Equation 4.9, i.e., if BN/2 is assumed a bit reversal 
matrix, then using Equation 4.9 it is shown that BN is. The following small 
example will be a useful reference for the comments to follow.
Example 4.3 Consider the following array of N = 24 subscripts in the first 
column. Using Equation 4.9, the matrix R16 operates on this column to produce 
the second column, placing the odd row 4-tuples sequentially first (all those 
ending with a 0) followed sequentially by the even row 4-tuples (those ending 
in 1). The second column is operated on by 12 ® R8 - equivalently R8 operates 
https://doi.org/10.1017/9781009283403.005 Published online by Cambridge University Press

4.2 Polar Code Construction
109
on the first eight rows and the bottom eight rows independently to produce the 
third column. Similarly 14 ® R4 operates on the third column to produce the 
last column which is the bit reversal of the first column. The overall effect is to 
operate on the first column by B16 to produce the last column.
Original 
subscript 
order
R16 12 ® R 8
Final 
(reverse) 
order 
14 ® R 4
0000
0000
0000
0000
0001
0010
0100
1000
0010
0100
1000
0100
0011
0110
1100
1100
0100
1000
0010
0010
0101
1010
0110
1010 
412
0110
1100
1010
0110 
(4.12)
0111
1110
1110
1110
1000
0001
0001
0001
1001
0011
0101
1001
1010
0101
1001
0101
1011
0111
1101
1101
1100
1001
0011
0011
1101
1011
0111
1011
1110
1101
1011
0111
1111
1111
1111
1111
Denote a subscript as a binary n-tuple i = (i1,i1,...,in) and its bit reversal 
as ir = (in,in-1,...,i1). That BN is a bit reversal matrix can be shown by 
recursion. Assume BN/2 is a bit reversal matrix and consider Equation 4.10
Bn = Rn(I2 0 Bn/2)
and consider a column array of binary n-tuples representing the binary 
expansions of the integers 0, 1,2,...,2n - 1. The first bit of the elements of 
the top half of the array is 0 and the first bit of elements of the bottom half of 
the array is a 1. The impact of RN on the top half of the array is to place this 
first 0 bit last in each of the top 2(n-1) n-tuples and to place the first 1 bit of the 
bottom 2(n-1) array elements last. By assumption BN/2 is a bit reversal matrix 
- consider its operation on the top half of the array. Since it achieves the bit 
reversal by reordering the rows of the array, the impact on the top half of the 
array is to reorder the rows so that the first (n - 1) bits are reversed. Thus the 
impact of RN and BN/2 on each half of the array is bit reversal.
https://doi.org/10.1017/9781009283403.005 Published online by Cambridge University Press

110
4 Polar Codes
Since Bn is a permutation matrix and a bit reversal matrix it is a symmetric 
matrix (with ones on the main diagonal corresponding to subscript positions 
where i = ir), and ifBN = (bi, j), then bi, j = 1iffj = ir.
Thus
BTN = BN and B-N1 = BN and B2N = IN (over both F2 and R).
It was previously observed that FN = F®n is also a matrix of order 2, i.e., 
FN 2 = IN over F2 although not symmetric.
Further properties of the matrices FN , GN and BN are noted.
Denote the matrix F = (fi 1 ,j 1 ),i 1 ,j 1 e {0,1} shown in Figure 4.1. The 
Boolean expression for the elements of the matrix is easily seen to be
fi 1 ,j 1 = 1 ® j 1 ® i 1j 1
which specifies a zero element for the top right matrix entry and one for the 
other three elements. For the matrix F®2 = (fi 1 i2,j 1 j2) the corresponding 
Boolean expression is
fi 1 i2,j 1 j2 = (1 ® j 1 ® i 1j 1)(1 ® j2 ® i2j2).
In this case the second subscript (i2 and j2) specifies the 2 x 2 subblock, the 
upper right subblock being all zero and the other three F. The first subscript 
(i1 and j1) specifies the element within the 2 x 2 subblocks.
The extrapolation to the general case for F®n = (fi 1 i2...in,j 1 j2...jn) is 
immediate ([3], equations 72 and 73):
n_
fi1...in,j1...jn = 
fik,jk
k=1
= 
(1 ® jk ® ikjk),
i =1
i.e., reversing the subscripts leaves the expression unchanged. Notice that from 
the above expression
fi,j=fir,jr. 
(4.13)
A similar expression for the generator matrix GN is desired. Recall from 
Equation 4.8 that GN = BNF®n = (gi, j) and that BN = (bi, j) is a symmetric 
permutation matrix with a 1 in row i in column ir and bi, ir = bir , i . It follows 
from these properties that
gi, j = fir , j .
It is interesting to note that the matrices BN and GN commute - consider
F®nBN = (g'i, j) and note that gi, j = fi, jr 
https://doi.org/10.1017/9781009283403.005 Published online by Cambridge University Press

4.3 Subchannel Polarization and Successive Cancellation Decoding 111
since the j-th column ofBN has a 1 inrowjr. But from the property of Equation 
4.13 it follows that
gi, j = fi,jr = fir,j = gi,j
and BN and F®n = FN commute as do GN and FN. It follows from this and a 
previous equation that
n
gi, j = fir, j = J"J (1 ® jk ® in - kjk).
Thus
GN = F®nBN = BNF®n and GN = BNFNBNFN = FNBN = IN over F2.
4.3 Subchannel Polarization and Successive 
Cancellation Decoding
The inputs to the final parallel base W channels in the above figures are 
codewords in a linear code with generator matrix GN = BNFN = BN F®n , 
i.e., x1N = u1N GN . It follows that the transition probabilities for the overall 
compound channel WN as given in Equation 4.6 are given by:
/ 
X 
/ 
X N
wjyN | «^ = WN(yN | XN = UNG^ = 
W(yi | Xi), yN e YN, uN e XN
11 
11 
1 
1 
1
i=1
and the probabilistic behavior of the channel is entirely determined by the 
transformation process resulting in the code generator matrix GN and the base 
channel W .
It remains to devise a decoding algorithm process for these codes and 
to determine the performance of the code and its decoding in terms of the 
probability of error, topics addressed in this section. The iterative and recursive 
structure of the generator matrix GN leads naturally to a notion of successive 
cancellation decoding which will be outlined here. This will be applied, not to 
the compound channel WN but to certain subchannels WN(i),i = 1,2,...,N, 
which will be defined using WN . It is these subchannels to which successive 
cancellation decoding will be applied and which will be shown to have 
remarkable properties. The error performance will be evaluated for these 
subchannels.
From the compound channel WN the BMS subchannels (also referred to 
as split channels) WN(i),i = 1,2,...,N are defined in the following manner.
https://doi.org/10.1017/9781009283403.005 Published online by Cambridge University Press

112
4 Polar Codes
Consider a channel with single binary input ui and outputs y1N and all previous 
inputs ui1-1 which are assumed available. Thus the subchannels are defined by:
WN (yN ,ui—1 | ui): X -+ YN x Xi-1
Ui 1--- > (yl,u1-^).
It is somewhat unconventional to define a channel with an input of ui and 
output consisting of all previous channel inputs as well as all channel outputs. 
In practice these outputs uii-1 are not available and estimates of the previous 
subchannel inputs are used. It is convenient for the analysis to assume a genie 
has provided the correct output of the previous (i -1) channels at the output of 
the BMS channel WN(i) and the subchannel will be used to derive an estimate 
ui of the input ui given knowledge of all previous inputs and all channel 
outputs as i goes from 1 to N . Such a process is termed successive cancellation 
decoding.
To obtain the transition probabilities for the subchannel WN(i) consider the 
following. Let P (y1N, u1N) denote the probability that u1N is chosen at the input 
for transmission (independently and equally likely), and y1N is received at the 
compound channel output. Then P(u1N) = 1/2N and
p (y N-u N) = 2NWN (y Ni« N)
and
p (yN-«1) = 
H 
2NWN (yN I uN)
and the transition probabilities for the subchannel, WN(i) are
WN(i) y1N,ui1-1 | ui = p(y1N,ui1)/p(ui), p(ui) = 1/2 
y -J—
,, N-i N--1
«N+1eXN-i 2
y1N | u1N
(4.14)
where
WN y1l | u1N = WN y1l | x1N = u1NGN = 
W(yi | xi). 
(4.15)
i=1
Thus the probabilistic behavior of the subchannels WN(i) y1N, ui1-1 | ui is 
entirely determined by the probabilistic behavior of the base channel W and the 
code specified by the matrix GN. All properties of these N subchannels such as 
the mutual information (symmetric capacity) I WN(i) between the input and 
https://doi.org/10.1017/9781009283403.005 Published online by Cambridge University Press

4.3 Subchannel Polarization and Successive Cancellation Decoding 113
outputs and their Bhattacharyya parameters can be computed. It turns out that 
the Bhattacharyya parameter is often easier to work with:
Z (Wi)\ = p p Li) LN ui-1 | u = 1\W(i) LN ui — 1 | u = 0\ 
Z WN = 
WN y1 , u1 
| ui = 
WN y1 , u1 
| ui = 0 ,
V 7 
yN eYN ui-1 eXi—lV 
V 
' 
' 
7
i = 1 , 2, ...,N.
(4.16) 
for the transition probabilities of Equation 4.15.
The information-theoretic behavior of the subchannels will be discussed 
later in the section where it will be noted that as N ^ rc> these subchannels 
tend to polarize in the sense that asymptotically a subchannel WN(i) will tend 
to be either a perfect channel for which I(wNi)) ~ 1 (or Z(WN)) ~ 0) or 
a useless channel for which I(WNi~) ~ 0 (or ZCWNi') ~ 1). Furthermore 
the fraction of the subchannels that asymptotically approach perfect tends to 
I(W) which will imply the performance of the overall code approaches the 
compound channel capacity.
The computations are necessarily intricate and here we focus on their 
implications while preserving the thread of the argument, but omitting the 
details of the computations.
The central problem of polar coding will be to identify the good channels 
and this is a nontrivial problem. These channels will be used and the remainder 
of the channels, bad channels, will convey fixed or frozen bits and hence 
convey no useful information. More comments on this key issue will be 
considered after discussion of decoding and the resulting probability of error. 
For the moment it will be assumed that a subset A of the N subchannels 
have been chosen as the polar code, the set of good channels, the remaining 
useless channels Ac conveying only fixed information known to the receiver. 
Furthermore it will be convenient to identify the good channels A with the 
corresponding rows of the generator matrix GN . say GA, and that the resulting 
code will be a binary (N, K)2 code, | A |= K, the actual number of useful 
inputs.
The likelihood ratio for the subchannels will be used to successively decode 
them and proceeds as follows: let
L(Ni) y1N,ui1-1
W(i)
WN
1
W(i)
WN
1
| ui = 0
----------.
| ui = 1
(4.17)
Using this LR the successive cancellation decoding rule is
ui
ui known, 
if i e Ac
h^yN, u'-1), i e A,
(4.18)
A
https://doi.org/10.1017/9781009283403.005 Published online by Cambridge University Press

114
4 Polar Codes
where
hi(y N,u I-1)
0,
1,
if L(Ni) y1N,ui1-1
otherwise.
W, (y n, U1-1 10)
<p(y n, u1-1 | 1)
A
Figure 4.3 shows the decoder for the successive cancellation decoder for 
the subchannel WN(i) . The overall successive cancellation decoder is shown in 
Figure 4.4. Previous input estimates are shown for this decoding - for analysis, 
as noted, the actual inputs ui1-1 will be assumed known.
Before returning to a consideration of the important properties of the 
Bhattacharyya parameter, an expression for the probability of error of the 
successive cancellation decoder is obtained. This will further illuminate the 
relevance of this parameter.
Such a receiver (successive cancellation decoder) will make a block error 
if uA = uA since the bits of ucA are assumed known. This is a one-pass 
algorithm.
Consider a code of length N = 2n and dimension K with the set of good 
channels A. The probability of block error for such a code will be given by
Pe(N,K,A, uAc) = £ 2K £ Wn (yN | uN). 
u AeX K 
y N SY N
ii N = u N
The expression indicates the probability of error being dependent on the frozen 
(known) bits of the code coordinate positions of Ac . In fact it can be shown 
[3] that for a symmetric channel the error performance is independent of the
Figure 4.3 Channel splitting and decoding
https://doi.org/10.1017/9781009283403.005 Published online by Cambridge University Press

4.3 Subchannel Polarization and Successive Cancellation Decoding 115
Figure 4.4 Successive cancellation decoder
chosen frozen bits and our interest is in such symmetric channels. So the frozen 
bits chosen will be assumed fixed, say all ones.
Interest is to derive an expression for this probability of error. Consider the 
block error analysis for a polar code defined by the set of positions A. The 
joint probability of the chosen input u1N and the resulting channel output y1N 
on the space XN x YN is given by
P (. N.y N) = WN (, N|. N) 2N
and consider the decision on the i-th output bit as
ui
ui. 
i e A Ac
hi(yN. Ui-1). i e A.
since the frozen bits are known at the receiver. Likewise the decoder will make 
a block error if any of the bits in the set A are not correct - thus define the 
error event
E = |(uN.yN) e XN x YN| iia(uN.yN) = ua}.
Define the event Ei as the event an error occurs at the i-th decoded bit (not 
necessarily the first decoding error in the block) - then ui = hi (y1N.ui1-1) or 
equivalently, in terms of the LR,
https://doi.org/10.1017/9781009283403.005 Published online by Cambridge University Press

116
4 Polar Codes
o A NN N nN 
(i 1) N i — 1 
(i 1) N i — 1
Ei — u 1 ,y 1 e X xY | wn y 1 ,u 1 
| ui — WN y 1 ,u 1 
| ui ®1
since the LR implies the decision will be made in favor of ui ® 1 and hence in 
error. The event there is at least one error in the received block is, by the union 
bound,
E C U E i 
i eA
and P(E) — 
P(Ei).
Let n be a set indicator function in the sense that for a set Ei C X x Y
n( E i) —
1, if u1N,y1N e Ei 
0, else.
Then
P(Ei)
E
u1N eXN,y1N eYN
21NWN (y N
u N)n( e i)
—
u1N eXN,y1N eYN
2NWn (y N
uN
—
ui1—1eXi—1,y1NeYN uiN+1eXN—i
WN\yN,u 1 
w(Nr\y Nur11 ui
i-1 | ui ® 1
i—1
wNi^yNU-1 
w^yN,u1-1 | Ui
| Ui ® 1 
i—1
since the argument of the square root is by definition greater than unity 
on Ei and since the expression under the square root does not involve the 
variables uiN+1. To show this last expression is Z wN(i) proceed as follows. 
By Equation 4.14
P(Ei) — 
E 
wN^yN,ui1— 1 | u^ •
ui1—1eXi—1,y1N eYN
w(i) 
wN
y1N,u i—
1
wN(i) y1N,ui1—1 | ui
1
| Ui ® 1
— 
E 
Jn^NN,ui1—1 । ui ® 1)W^(yN,ui1— 1 | u/)
ui1—1eXi—1,y1N eYN
— 
Z wN(i) .
Thus
p( E) — £ Zw(Nl)).
The equation implies that to minimize the probability of error one should 
choose the | A | most reliable channels for the set A, an obvious choice which 
is reflected in the definition below.
https://doi.org/10.1017/9781009283403.005 Published online by Cambridge University Press

4.3 Subchannel Polarization and Successive Cancellation Decoding 117
Proposition 2 of [3] follows from these observations:
Proposition 4.4 For any BDMC W and any choice of parameters (N = 
2n,K =|A|) the probability of block error is bounded by
Pe(N,K, A) < 
ZwN}). 
(4.19)
Assuming the set of useful inputs A is known, the code with generator 
matrix GN can be broken down into a “useful” part and a “useless” part as
x1N = uAGN (A), uAc GN (Ac)
where the arguments of GN indicate corresponding submatrices of GN and 
the comma indicates catenation. Thus the encoding function is from uA to x1N 
with | Ac | frozen (predetermined) variables among the N coordinate positions 
of x1N . These restricted codes are termed coset codes for obvious reasons. In 
the case where the frozen bits are all set to 0 the polar code is a linear code. 
For | A |= K the overall code is referred to as an (N, K, A) code.
The proposition suggests the following definition of polar codes:
Definition 4.5 Given a BDMC W,aGN coset code with parameters 
(N, K, A, uAc ) will be called a polar code forW if the information set A is 
chosen as a K element subset of {1,2,...,N} such that
z (wN) < Z (Wj)
for all i e A and j e Ac.
Thus the K =| A | information positions (subchannels) are chosen 
corresponding to the K subchannels with the smallest values of Z(WN(i)).
The above material suggests several problems. It is interesting to note a 
polar code is chosen via the mutual information or Bhattacharyya parameter of 
the subchannels. It is a determination of which of the N = 2n channels are the 
K = NR most reliable subchannels (those with lowest Bhattacharyya param­
eter or highest mutual information) R = K/N the rate of the code, which, for 
N large enough, will be close to perfect, a reflection of the polarization of the 
subchannels, while the other channels are virtually useless. The computation 
of the Bhattacharyya parameter for each of the subchannels is computationally 
expensive (of order O(N log N)). Other techniques to determine the good 
channels have been devised and a brief comment on these is noted below.
The above discussion has shown the crucial role of the subchannels in the 
formulation of polar codes. To complete the picture it is necessary to establish 
that the number of good subchannels is sufficient to achieve capacity on the 
https://doi.org/10.1017/9781009283403.005 Published online by Cambridge University Press

118
4 Polar Codes
channel. The theorem below, which is a composite of three key theorems of 
([3], theorems 1, 2 and 3), stated without proof, establishes this. This theorem 
encapsulates the key ideas of the entire chapter: Let W be a BDMC with 
symmetric capacity I(W) > 0:
Theorem 4.6 (i) The channels ^) polarize in that for any fixed e e [0,1] 
as N goes to infinity through powers of 2 the fraction of indices 
i e [1,N ] forwhich I^wN^i) e (1 - e, 1] goesto I(W) and the fraction 
for which IWNi)) e [0,e) goesto 1 - I(W).
(ii) Forafixed R < I(W) there exists a sequence of sets A N c [1,N ] such 
that |A N |> NR and Z(wNi) < O(N—5 /4) for all i e A.
(iii) The block error probability for polar coding under successive 
cancellation decoding satisfies
Pe(N,R) = O(N-1/4).
It is in this sense that polar codes achieve capacity on the base channel W 
which has capacity C, |C |< 1. To summarize, the base channel is expanded to 
the compound channel WN, N = 2n which will have capacity NC and which 
is comprised of approximately NI(W) good subchannels, each of which is 
approximately perfect.
All three of these statements (theorems) are remarkable. The third statement 
follows from Proposition 4.4 ([3], proposition 2) and from item (ii) of the 
theorem in that the number of terms in the expression for the probability of 
error is O(N), each term being O(N-5/4). The implication of the theorem 
is that as N increases I (WN(i)) tends either to 0 or 1, and the fraction of 
subchannels that tend to unity is I(W). Thus capacity of the compound channel 
WN tends to NI(W) where I(W) is the (symmetric) capacity of the base 
channel W. The overall coding/decoding system achieves capacity. It will be 
noted later the error performance can be improved to O(2—2nf’}.
The highly iterative construction of the polar coder is useful in deriving 
recursive expressions for the transition probabilities of the subchannels WN(i) 
and the likelihood ratios as the channel order increases through powers of two. 
It is also useful to give an indication of how information theory can be used 
to show polarization of these channels. The subchannel WN(i) has an input ui 
and outputs jn, ui-1 which the subchannel uses to form the estimate ui. The 
transition probabilities for this channel were derived in Equation 4.14 as
wNi) (JN, u1-1 I ui) = £ 
2N—1 Wn (JN I uN) 
(4.20)
uiN+1eXN-i
and thus expressions for I (WN(i)) and Z(WN(i)) can be computed.
https://doi.org/10.1017/9781009283403.005 Published online by Cambridge University Press

4.3 Subchannel Polarization and Successive Cancellation Decoding 119
A few comments on the computation of the quantities Z(WN(i)) and I (WN(i)) 
and of the transition probabilities WN(i)(y1N, ui1-1 | ui) are in order. For these 
quantities the highly recursive/iterative structure of the code generator matrix 
can be exploited effectively. The briefest of overviews of this computational 
issue is given.
The doubling construction, as noted in Figure 4.3, leads to the following 
relationships which are most useful in reducing the complexity of the required 
computations ([3], proposition 3):
W(2n-1)(y2N,u2i-21 u2i-i) = 
1 w(?(yN,u2i-2 ® u2i--21 u2i-1 ® u2i)
2N 
1 
1 
- 
u2i 2 N 1 1,o 
1,e 
-
• WN) (y NN+i ,u 1 i-21 u 2 i)
(4.21) 
and
W2N) (y 2N,u 2 i-11u 2 i) = 2 WN) (y Nu 2 i-2 ®u 1i-21u 2 i-1 ®u 2 i)
• WN) (y N+i, u 2i-21 u 2 i),
(4.22) 
thus leveraging the i-th computation to give the 2i-th one.The subscripts “e” 
and “o” indicate even and odd subscripts are to be taken. Furthermore it can be 
shown that
1 
A. 2 N 
2 N \ 
1 
1 
A. N 
2 N 
2 N \ 
1 
(2 2 N 
2 N \
22N -1 W2N y 1 
1 u 1 
= ^ • 2N-1 • WN y 1 1 u 1 ,o ® u 1 ,e ' 2N-T WN yN+1 1 u 1 ,e .
Equations 4.21 and 4.22 do not directly lead to the previous statements on 
polarization of the subchannels but give an indication of how polarization 
occurs as N increases. Numerous very interesting information-theoretic rela­
tions are established in [3] that are useful in computing this doubling transition 
probability and other relationships but are not considered here. Again, the BEC 
is a special case of these where exact relationships rather than just bounds can 
be established.
As noted, the central problem of polar coding is the identification of the 
good subchannels - those with /(WNi)) ~ 1 (or equivalently those with 
Z(Wl(i') ~ 0). Since the original paper [3] considerable progress has been 
made on this and related aspects of polar coding. Limited comments on 
these problems are noted below. Of course conceptually one could consider 
computing the Bhattacharyya parameters for all N subchannels and choosing 
those with values close to 0 but this is prohibitively computationally expensive.
There is a connection of polar codes to Reed-Muller (RM) codes as follows. 
For an RM code of length N = 2n, a parity-check matrix for an (N, K) RM 
https://doi.org/10.1017/9781009283403.005 Published online by Cambridge University Press

120
4 Polar Codes
code by choosing an (N — K) x N submatrix of F®n of all rows of Hamming 
weight above some minimum weight. It is not hard to determine the minimum 
distance of such a code and its dimension following from the fact that the 
matrix F®n is of full rank over a field of characteristic 2. However, it is shown 
([3], section X) that this leads to the inclusion of some bad subchannels and is 
not a good scheme.
The next subsections comment on certain aspects of polar coding.
The Special Case of the BEC
It is clear that the BEC is a special case of the above relationships, where 
equality is often achieved while inequality exists for more general channels. 
Before returning to general BMSs the situation is commented on further. In 
particular from the above, we have for a BEC
(2i—1) 
(i) 
(i) 
(2i) 
(i)
Z W2N 
= 2Z WN 
— Z WN 
and Z W2N 
= 2Z WN 
.
A probability of erasure of S will be assumed. These recurrences define a 
sequence of polynomials that, given the polarized nature of the channel noted 
in the above theorem, should have interesting properties. Such properties are 
pursued in the interesting study [31, 32]. Indeed it is shown there how one 
might use the properties of the polynomials to determine the good channels. 
The approach is not pursued here.
Construction of Polar Codes
The problem of constructing polar codes is considered further. The original 
work of Arikan [3] suggested a technique based on the computation of 
Bhattacharrya parameters for the subchannels leading to the probability of 
error. The work of Mori and Tanaka [27, 28] suggested interpreting the polar 
code as an LDPC code with Tanner graph and determine the probability of 
error via pdfs of LLRs and using density evolution as described in Chapter 
3. Those channels with small probability of error are identified as the good 
channels. The probability of error calculations use the recursive forms of 
likelihood ratios noted above for the polar code case and drawing on results of 
Chapter 3 for the other computations, including density evolution and Gaussian 
approximation.
Polar codes defined with this density evolution technique are referred to 
as density evolution polar codes. It is anticipated that since they are defined 
https://doi.org/10.1017/9781009283403.005 Published online by Cambridge University Press

4.3 Subchannel Polarization and Successive Cancellation Decoding 121
using the probability of error rather than the Bhattacharyya parameters, one 
can expect better performance and experimental evidence bears this out [29].
As with LDPC codes one can use the method of Gaussian approximation 
[11]. This and other aspects of designing polar codes using density evolution 
and/or Gaussian approximations are considered in the works [13, 24, 27, 28, 
29, 44, 45]. The situation parallels the techniques for the LDPC codes in 
Chapter 3.
Tal and Vardy [41] also address the problem of the efficient construction of 
polar codes. They use an approximation technique that brackets the original 
bit channel, a degrading and upgrading quantization method, a concept hinted 
at previously in connection with LDPC codes. The degrading quantization 
transforms the original subchannel into one with a smaller output alphabet 
(hence simpler calculations) while the original subchannel is a degraded 
version of the upgraded one. The result is an interesting technique that yields 
polar codes in linear time ([41], theorem 14), with the upgraded and degraded 
channel versions being very close in terms of performance. The notions of such 
channels were also discussed in [12] and [37].
The result of the algorithms ([41], theorem 1) is to show that for a BMS 
W of capacity I(W) and for fixed e > 0 and P < 1 /2 there is an even 
integer m0 such that for all even integers m > Mo and all sufficiently long 
code lengths N = 2n there is a code construction algorithm with running time 
O(N • m2 log M) that produces a polar code of rate R > I( W) - e such that
Pblock < 2—N
under successive cancellation decoding, a result also achieved in [7] and 
noted below. The complexity of the algorithm is linear in code length. Note 
the improvement in this probability of error from the original O(N-1/4) = 
O(2-n/4) to O(2-2nP). Further comments on the probability of error for polar 
codes are given below.
Kernels and the Rate of Polarization
An important problem for the construction of polar codes is the speed at which 
the channels polarize or the speed with which Z(WN(i)) approaches 0 or 1 as 
n —> x>,N = 2n, since this is directly connected to the probability of error 
via Proposition 4.4 and the feasibility of implementation of such codes. From 
Theorem 4.6, the rate of polarization achieved in the original work led to a 
probability of error that was O(N -1/4) = O(2-n/4). As noted above, this was 
improved with the following result ([7], theorem 1):
https://doi.org/10.1017/9781009283403.005 Published online by Cambridge University Press

122
4 Polar Codes
Theorem 4.7 Let W be a BMS with I( W) > 0 .Let R< I (W) and fi < 1 / 2 
be fixed. Then for N = 2n the best achievable block error probability for polar 
coding under successive cancellation decoding at block length N and rate R 
satisfies
Pe(N,R) = o 2-Nfi .
A question arises as to whether successive cancellation decoding can be 
improved upon. The following implies not.
Theorem 4.8 ([22], theorem 5) Let R> 0,fi > 1 /2,N = 2n,n > n(fi, R, W) 
and W be any BMS. The probability of block error under maximum a 
posteriori (MAP) decoding satisfies
Pe (N, R) > 2-Nfi.
The implication of this is that successive cancellation decoding performance 
is comparable to that of maximum a posteriori (MAP) decoding.
The above bounds on the error probability are valid for any code rate. One 
might naturally expect to achieve a better error probability at lower code rates. 
A step in this direction is the result of Tanaka and Mori [43] who showed for 
the same parameters as above that
Pe(N,R) = o (2-2(n+t 
2 ),
where t < Q-1 (R/I( W)) and Q(x} = f™ e - u 2 /2 du/42n. As with the code 
independent result one might wonder if a MAP decoder, rather than an SC 
decoder, might have a better performance. This is answered negatively in [20] 
which gives a lower bound of the same form for MAP decoding.
The original work of Arikan [3] noted it would be natural to consider kernels 
for polar codes larger than the 2 x 2 considered (i.e., the F matrix). It is 
somewhat remarkable that such good results were obtained in the original 
paper for such a small kernel. The problem of £ x £ (binary) kernels was 
considered in [23]. The work gives necessary and sufficient conditions for 
a kernel to be polarizing, and in particular that any £ x £ matrix, none of 
whose column permutations gives an upper triangular matrix, will polarize a 
symmetric channel (the only kind under consideration).
It is also shown there is no polarizing kernel of size less than 16 with an 
exponent which exceeds 1/2. Using BCH codes and shortened versions of 
them, a polar code with £ = 16 is defined with exponent greater than 1 /2 
is achieved (with exponent 0.51828). It is also shown the exponent converges 
to unity as £ ^ rc> for larger values of £.
It is natural to also formulate polar codes for nonbinary channels and 
numerous authors have considered these including [27, 30, 33, 39, 40]. The 
https://doi.org/10.1017/9781009283403.005 Published online by Cambridge University Press

4.3 Subchannel Polarization and Successive Cancellation Decoding 123
thesis of Mori [27] addresses the extension to nonbinary codes and their rate 
of polarization. It is shown that an asymptotic polarization rate approaching 
unity as block size increases is achievable using Reed-Solomon matrices in 
the construction processes which, it is argued, are a natural generalization 
of the kernel used for the binary case. It is also shown that the polarization 
constant achieved with an RS kernel of size 4 is larger than the largest exponent 
achievable with a binary kernel of size 16. Interestingly it is also shown 
that using Hermitian codes (from algebraic geometry) leads to even better 
exponents.
Another interesting aspect of polar codes is to determine how fast their rate 
approaches capacity as a function of their block length, for a given probability 
of block error, an aspect often referred to as scaling [15, 17, 18, 19, 21, 26, 
36, 46]. Only the work of [18] is briefly mentioned. It shows that for a fixed 
probability of error on a BMS channel W, polar codes of rates with a gap to 
capacity (I(W)) of e with construction complexity and decoding complexity 
that are polynomial in 1 /e are possible. Specifically the following theorem is 
shown:
Theorem 4.9 ([18], theorem 1) There is an absolute constant p (the scaling 
constant) such that the following holds. Let W be a binary-input memoryless 
output-symmetric channel with capacity I(W). Then there exists aW < rc> 
such that for all c > 0 and all powers of two N > aW( 1 /c)lx, there is a 
deterministic poly(N/e) time construction of a binary linear code of block 
length N and rate at least I(W) - e and a deterministic Npoly(log N) time 
.49 
decoding algorithm for the code with block error probability at most 2-N 
for communication over W.
Other work ([17], proposition 3) shows that for a q-ary channel W for 
transmission at rate R with error probability at most Pe then if q = 3 a code 
length of
(I(W) - R)5.571
is sufficient where f is a constant that depends on Pe .A similar result for q = 5 
and scaling exponent 6.177 is given.
Comments
The original article on polar codes by their inventor, [3], is essential reading. It 
is a beautiful exposition on information theory and coding and should be read 
for a deeper understanding of the subject. This chapter is a limited introduction 
to the central ideas of that work.
https://doi.org/10.1017/9781009283403.005 Published online by Cambridge University Press

124
4 Polar Codes
The performance of polar codes has generated a very large literature and it 
is difficult to convey the depth and elegance of the many aspects investigated 
in a short essay. Initially it was felt that while the theoretical results on polar 
codes were unquestioned, their performance at practical block lengths might 
not compete with other coding systems such as LDPC. This concern appears to 
have diminished considerably and they are under consideration for standards 
for 5G networks [8]. They will continue to attract intense interest.
An aspect of polar codes is that in many if not most instances, code design is 
not universal in that a code designed for a channel with one parameter may not 
be suitable for a channel with a different parameter. A technique to overcome 
this is considered in [38].
While successive cancellation decoding has a natural relationship to the 
construction of polar codes, other decoding techniques have been consid­
ered with a view to achieving superior performance or a better complex- 
ity/performance trade-off. As an example, Ahmed et al. [1] consider the use 
of belief propagation decoding for such a purpose.
The 2 x 2 matrix F used in the construction of binary polar codes has a 
natural connection to the (u, u + v) construction used in the construction of 
RM codes and a few limited comments on this connection were given. Also, 
some of the inequalities established on the channel reliability and performance 
or symmetric capacity functions, Z(W) and I(W), as in Equation 4.2, as 
noted, achieve equality for the BEC. The large literature on RM codes and 
the equalities in these equations have led to numerous interesting contributions 
which include [2, 4, 25, 31].
The use of list decoding with polar codes has also been considered. A 
stronger notion of polarization than used here has been of interest (e.g., 
[9, 10]). Other interesting contributions to the problem include [6, 14, 22, 34, 
35, 40, 41, 42, 44].
References
[1] Ahmed, E.A., Ebada, M., Cammerer, S., and ten Brink, S. 2018. Belief propaga­
tion decoding of polar codes on permuted factor graphs. CoRR, abs/1801.04299.
[2] Arikan, E. 2008. A performance comparison of polar codes and Reed-Muller 
codes. IEEE Commun. Lett., 12(6), 447-449.
[3] Arikan, E. 2009. Channel polarization: a method for constructing capacity­
achieving codes for symmetric binary-input memoryless channels. IEEE Trans. 
Inform. Theory, 55(7), 3051-3073.
[4] Arikan, E. 2010. A survey of Reed-Muller codes from polar coding perspective. 
Pages 1-5 of: 2010 IEEE Information Theory Workshop on Information Theory 
(ITW 2010, Cairo), Cairo, Egypt.
https://doi.org/10.1017/9781009283403.005 Published online by Cambridge University Press

References
125
[5] Arikan, E. 2016. Polar codes. In: 2016 JTG / IEEE Information Theory Society 
Summer School, Department of Electrical Communication Engineering, Indian 
Institute of Science, Bangalore, India, 27 June-1 July 2016.
[6] Arikan, E. 2016. On the origin of polar coding. IEEE J. Select. Areas Commun., 
34(2), 209-223.
[7] Arikan, E., and Telatar, E. 2008. On the rate of channel polarization. CoRR, 
abs/0807.3806.
[8] Bioglio, V., Condo, C., and Land, I. 2018. Design of polar codes in 5G new radio. 
CoRR, abs/1804.04389.
[9] Blasiok, J., Guruswami, V., Nakkiran, P., Rudra, A., and Sudan, M. 2018. General 
strong polarization. Pages 485-492 of: Proceedings of the 50th Annual ACM 
SIGACT Symposium on Theory of Computing. STOC ’18. ACM, New York.
[10] Blasiok, J., Guruswami, V., and Sudan, M. 2018. Polar Codes with exponentially 
small error at finite block length. APPROX-RANDOM, arXiv:1810.04298.
[11] Chung, S.-Y., Richardson, T.J., and Urbanke, R.L. 2001. Analysis of sum-product 
decoding of low-density parity-check codes using a Gaussian approximation. 
IEEE Trans. Inform. Theory, 47(2), 657-670.
[12] Cover, T.M., and Thomas, J.A. 2006. Elements of information theory, 2nd ed. 
Wiley, Hoboken, NJ.
[13] Dai, J., Niu, K., Si, Z., Dong, C., and Lin, J. 2017. Does Gaussian approxi­
mation work well for the long-length polar code construction? IEEE Access, 5, 
7950-7963.
[14] Eslami, A., and Pishro-Nik, H. 2011. A practical approach to polar codes. CoRR, 
abs/1107.5355.
[15] Fazeli, A., and Vardy, A. 2014 (September). On the scaling exponent of binary 
polarization kernels. Pages 797-804 of: 2014 52nd Annual Allerton Conference 
on Communication, Control, and Computing (Allerton), Monticello, IL.
[16] Gallager, R.G. 1968. Information theory and reliable communication. John Wiley 
& Sons, New York.
[17] Goldin, D., and Burshtein, D. 2018. On the finite length scaling of q-ary polar 
codes. IEEE Trans. Inform. Theory, 64(11), 7153-7170.
[18] Guruswami, V., and Velingker, A. 2015. An entropy sumset inequality and 
polynomially fast convergence to Shannon capacity over all alphabets. Pages 
42-57 of: Proceedings of the 30th Conference on Computational Complexity. 
CCC ’15. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, Germany.
[19] Guruswami, V., and Wang, C. 2013. Linear-algebraic list decoding for variants of 
Reed-Solomon codes. IEEE Trans. Inform. Theory, 59(6), 3257-3268.
[20] Hassani, S.H., and Urbanke, R.L. 2010 (June). On the scaling of polar codes: 
I. The behavior of polarized channels. Pages 874-878 of: 2010 IEEE International 
Symposium on Information Theory, Austin, TX.
[21] Hassani, S.H., Alishahi, K., and Urbanke, R.L. 2014. Finite-length scaling for 
polar codes. IEEE Trans. Inform. Theory, 60(10), 5875-5898.
[22] Hussami, N., Korada, S.B., and Urbanke, R.L. 2009. Polar codes for channel and 
source coding. CoRR, abs/0901.2370.
[23] Korada, S.B., Sasoglu, E., and Urbanke, R.L. 2009. Polar codes: characterization 
of exponent, bounds, and constructions. CoRR, abs/0901.0536.
https://doi.org/10.1017/9781009283403.005 Published online by Cambridge University Press

126
4 Polar Codes
[24] Li, H., and Yuan, Y. 2013 (April). A practical construction method for polar codes 
in AWGN channels. Pages 223-226 of: IEEE 2013 Tencon - Spring.
[25] Mondelli, M., Hassani, S.H., and Urbanke, R.L. 2014. From polar to Reed­
Muller codes: a technique to improve the finite-length performance. IEEE Trans. 
Commun., 62(9), 3084-3091.
[26] Mondelli, M., Hassani, S.H., and Urbanke, R.L. 2016. Unified scaling of polar 
codes: error exponent, scaling exponent, moderate deviations, and error floors. 
IEEE Trans. Inform. Theory, 62(12), 6698-6712.
[27] Mori, R. 2010. Properties and construction of polar codes. CoRR, abs/1002.3521, 
1-24.
[28] Mori, R., and Tanaka, T. 2009. Performance and construction of polar codes on 
symmetric binary-input memoryless channels. CoRR, abs/0901.2207.
[29] Mori, R., and Tanaka, T. 2009. Performance of polar codes with the construction 
using density evolution. IEEE Commun. Lett., 13(7), 519-521.
[30] Mori, R., and Tanaka, T. 2014. Source and channel polarization over finite fields 
and Reed-Solomon matrices. IEEE Trans. Inform. Theory, 60(5), 2720-2736.
[31] Ordentlich, E., and Roth, R.M. 2017 (June). On the pointwise threshold behavior 
of the binary erasure polarization subchannels. Pages 859-863 of: 2017 IEEE 
International Symposium on Information Theory (ISIT).
[32] Ordentlich, E., and Roth, R.M. 2019. On the pointwise threshold behavior of 
the binary erasure polarization subchannels. IEEE Trans. Inform. Theory, 65(10), 
6044-6055.
[33] Park, W., and Barg, A. 2013. Polar codes for q-ary channels, q = 2r . IEEE Trans. 
Inform. Theory, 59(2), 955-969.
[34] Pedarsani, R., Hassani, S.H., Tal, I., and Telatar, E. 2012. On the construction of 
polar codes. CoRR, abs/1209.4444.
[35] Pfister, H. 2014. A brief introduction to polar codes. Tutorial paper, Duke 
University. 
.
http://pfister.ee.duke.edu/courses/ecen655/polar.pdf
[36] Pfister, H.D., and Urbanke, R.L. 2016 (July). Near-optimal finite-length scaling 
for polar codes over large alphabets. Pages 215-219 of: 2016 IEEE International 
Symposium on Information Theory (ISIT).
[37] Richardson, T., and Urbanke, R. 2008. Modern coding theory. Cambridge Uni­
versity Press, Cambridge.
[38] Sasoglii. E., and Wang, L. 2016. Universal polarization. IEEE Trans. Inform. 
Theory, 62(6), 2937-2946.
[39] Sasoglu. E.. Telatar, E.. and Arikan, E. 2009. Polarization for arbitrary discrete 
memoryless channels. CoRR, abs/0908.0302.
[40] Tal, I. 2017. On the constriction of polar codes for channels with moderate inpit 
alphabet sizes. IEEE Trans. Inform. Theory, 63(3), 1501-1509.
[41] Tal, I., and Vardy, A. 2013. How to constrict polar codes. IEEE Trans. Inform. 
Theory, 59(10), 6562-6582.
[42] Tal, I., and Vardy, A. 2015. List decoding of polar codes. IEEE Trans. Inform. 
Theory, 61(5), 2213-2226.
[43] Tanaka, T., and Mori, R. 2010. Refined rate of channel polarization. CoRR, 
abs/1001.2067.
[44] Trifonov, P. 2012. Efficient design and decoding of polar codes. IEEE Trans. 
Commun., 60(11), 3221-3227.
https://doi.org/10.1017/9781009283403.005 Published online by Cambridge University Press

References
127
[45] Wu, D., Li, Y., and Sun, Y. 2014. Construction and block error rate analysis 
of polar codes over AWGN channel based on Gaussian approximation. IEEE 
Commun. Lett., 18(7), 1099-1102.
[46] Yao, H., Fazeli, A., and Vardy, A. 2019. Explicit polar codes with small scaling 
exponent. CoRR, abs/1901.08186.
https://doi.org/10.1017/9781009283403.005 Published online by Cambridge University Press

5
Network Codes
The first notion of nodes in a network performing computation on packets 
before forwarding in the network to increase network throughput seems to 
have risen explicitly in the work [9] in the context of satellite repeaters.1 The 
notion of a network code was formalized in the influential paper [1]. The idea 
is simply to show how, by allowing nodes in a communications network to 
process packets as they arrive at servers, rather than just store and forward 
them, the throughput of the network can be improved significantly and, indeed, 
be shown to achieve an upper bound. Such codes have clear possibilities for 
existing networks and the theoretical and practical literature on them is now 
large. This work will largely focus on the fundamental papers [1, 34, 36] which 
set the stage for many of the advances in the area.
1 The author is grateful to Raymond Yeung for this reference.
The next section introduces the terminology of the problem to be used 
and discusses the celebrated max-flow min-cut theorem for networks. This 
theorem has been proved by several authors and in many books on graph 
theory (e.g., [13, 17, 18, 49]). The simple proof of [17] is discussed here. The 
section also considers the important example of the so-called butterfly network 
contained in the seminal paper [1], a simple but convincing example of the 
value of network coding.
A more detailed consideration of the work [1] is given in Section 5.2. 
The work is information-theoretic in nature and introduces the fundamental 
aspects of network coding. It establishes the existence of network codes that 
achieve the max-flow min-cut bound for certain types of networks. This was 
a foundational paper for virtually all subsequent work. The work [34] shows 
how the network coding problem can be formulated in an algebraic manner 
which is mathematically convenient for both the discussion and the proofs. 
It is an important contribution to the area as it allowed a greater insight into the 
128
https://doi.org/10.1017/9781009283403.006 Published online by Cambridge University Press

5.1 Network Flows
129
network coding problem. The section also includes a discussion of the paper 
[36] on linear network codes which establishes the important result that it is 
sufficient to consider linear coding (which essentially involves a linear sum 
of packets of some fixed length entering a node, over some finite field) to 
obtain a maximum flow for the multicast case (although it is noted in [30] that 
network coding does not increase the achievable rate when the one source node 
multicasts to all other nodes in the network). The max-flow min-cut theorem 
does not hold for the case of multiple sources and multiple sinks in the network.
Section 5.3 considers a few examples of actual construction techniques 
for network codes and their performance on networks. Given the enormous 
theoretical advantages proved for network codes, it is not surprising there is 
a considerable literature on the problems associated with the implementation 
of network coding and their performance on peer-to-peer networks, such as 
overlay networks on the Internet.
This chapter considers only network codes on networks that are delay- 
free and acyclic (i.e., contain no cycles). Thus as packets are launched into 
the network, they appear sequentially at each node and the final sink without 
delay. Under certain conditions, virtually all of the results, however, can also 
be shown to hold for networks with delays and cycles, with further analytical 
techniques.
5.1 Network Flows
A network is viewed as a directed graph (digraph) G without loops on a set of 
nodes V and edges E such that each edge is assigned a nonnegative capacity 
c(e) > 0,e e E. For the sake of simplicity, only integral capacities will be 
considered here. It will be assumed the flows are in bits per unit time. Often it 
is assumed each edge of the network has a unit capacity and allows for multiple 
edges between nodes to account for various integer capacities. More recent 
literature (e.g., [59]) refers to edges as channels and the terms are regarded as 
interchangeable.
In a unicast network there are two distinguished nodes, a source node s e V 
and a sink or terminal node t e V , while the multicast network has a single 
source and multiple sinks. The case of multiple sources and sinks is also of 
interest although not discussed further except for noting that [17] for some 
purposes the case of multiple sources and multiple sinks can be handled by 
adding anew source which has an outgoing edge to each of the original sources 
and a new sink which has an incoming edge from each of the original sinks, 
https://doi.org/10.1017/9781009283403.006 Published online by Cambridge University Press

130
5 Network Codes
thus converting the system back to a single source, single sink case. However, 
for many questions the multiple sink case is more complicated.
In the unicast network, single source/single sink, it is of interest to 
determine the maximum flow achievable through the network from source to 
sink, so that the capacity constraint on any edge on the flow is not violated, 
referred to as a feasible flow.
If the directed edge e e E is (u,v) e E (directed from u to v), the node u 
is referred to as the tail of the edge e and denoted e- and v is the head of the 
edge and denoted e+ . The source vertex is s and sink or terminal vertex is t.A 
flow in a network (integer flows on edges are considered here) is defined as a 
function f from the set of edges E to the positive integers Z>0 satisfying the 
properties:
(i) 0 < f(e) < c(e) for all e e E,
(ii) Ee-= v f(e) = Ee+=v f(e) for v e V\{s,t}.
The second item above asserts the conservation of flow for all vertices 
except the source and sink, i.e., the flow out of node v is equal to the flow 
into node v. (Some authors allow flow into a source and out of a sink.) The 
term c(e) is the capacity of the edge e, the maximum flow allowed on that 
edge at any instant of time.
The value of a flow is defined as
val(f) = £ f(e) = £ f(e) 
(5.1)
which is simply the flow out of the source node s which is also the flow into 
the sink node t .
The problem of interest is to determine the maximum value that a flow in 
a given directed network may achieve in such a manner that the flow on any 
edge does not exceed the capacity c(e) of that edge.
_ _ _ - _ - - ~ _ _  ~ —
Define an (S, T) cut of the network, for S c V, T = V\ S = Si as the set 
of edges with one end in S and the other in T, s e S, t e T, with the property 
that any path from a source s to sink t traverses an edge in (S, T).Fortheflow 
assigned to edge e e E with capacity c(e) and flow f (e), define the value and 
capacity of a cut as
val(S,T) = f(e) and cap(S,T) = 
c(e).
The celebrated max-flow min-cut theorem can be found in many texts on 
graph theory, algorithms and combinatorics (e.g., [8, 11, 13, 31, 48, 52]). The 
proof of [17] is instructive.
https://doi.org/10.1017/9781009283403.006 Published online by Cambridge University Press

5.1 Network Flows
131
Theorem 5.1 (Max-Flow Min-Cut) ([17]) The maximum value of a flow in a 
network for any source s and sink t is equal to the minimum capacity of any 
cut of the network and this maximum flow is achievable.
Proof: To show that the flow of a minimum valued cut-set cannot be exceeded, 
consider the cut C = (S,T ) to be such a minimum cut and denote a given flow 
in the network as f. Then val (f) < cap(C) since capacity of the cut would 
only be achieved if all directed branches from S to T were carrying capacity 
and those from T to S were carrying 0. It is clear that the flow across the cut 
from left to right is the flow into the sink node t - since all nodes in T except 
the terminal t have a net flow of zero. Thus val(f) < cap(C).
To show that if C is a minimum cut-set (min-cut), then a flow exists that 
achieves cap(C). The notion of a reduced network is introduced. Consider the 
following process to construct a reduced network from a given network. If 
there is an edge in the network which is not in some minimum cut-set, reduce 
its capacity until it is in some minimum cut-set or its flow value is 0 (equivalent 
to eliminating it from the network). Continue to look for such edges until there 
are no edges not in some min-cut. The reduced network then has the properties:
(i) the graph of the reduced network is that of the original except possibly 
some edges are missing;
(ii) each edge of the reduced network has capacity < that edge of the 
original network;
(iii) each edge of the reduced network is in at least one min-cut of the 
reduced network.
It is clear that a valid flow in the reduced network is a valid flow in the original. 
Thus proving the theorem for reduced networks is sufficient to prove it for 
the original network. The proof is by induction on the number of nodes in 
the network. It is clear that any network (of any number of nodes) with path 
lengths from s to t of length at most 2 will satisfy the theorem.
Suppose the theorem is true for any reduced network with n or fewer nodes. 
We show it true for reduced networks with n + 1 nodes. Suppose the reduced 
network with n+ 1 nodes has a path of length at least 3. There exists an internal 
edge (end vertices neither s nor t) on this path and this edge is in some min- 
cut C (since the network is reduced). On each edge of the cut-set introduce a 
middle node such that each resulting edge has the same capacity as the original 
edge. Merge these new nodes into one node (say v*) - the effect is to create 
two networks, say N 1 and N2 in series with the node v* serving as the sink for 
the first network and source for the second network. Each of these networks 
has fewer than n + 1 nodes and every path from s to t goes through v* . Each of 
https://doi.org/10.1017/9781009283403.006 Published online by Cambridge University Press

132
5 Network Codes
these networks contains a min-cut corresponding to C and neither can contain 
a lower valued min-cut as this process can only eliminate cut-sets - not create 
new ones. Since by induction the theorem is true for each of the networks it is 
true for the original network and hence for all networks with n + 1 nodes. By 
separating the node v* into the original nodes and eliminating the introduced 
nodes on the cut-set edges, any flow in the two networks corresponds to a flow 
in the original network and the theorem is proven. ■
The more conventional approach to proving this theorem, which mirrors 
somewhat the notion of a reduced network in the above proof, uses the notion 
of augmented flows. To give a flavor of this approach only integer flows and 
capacities (the extension to noninteger flows is not difficult) are considered and 
for an arbitrary set of vertices S of the network G, S c V,s e S,t e S = V \S, 
define [8]:
S* = {e e E : e - e S,e + e S},
S* ={e e E : e- e SS,e+ e S}.
From these definitions and the above discussion it is easy to see that
E f(e) - E f(e) = val(f) ^ capC)
where C is the capacity of the cut (S,SS). Let f be an (integer) flow. Then either 
there is a cut C with cap(C) = val (f) or there is a flow f with val (f ) = 
val(f) + 1.
For the integer flow f on the digraph G with source and sink nodes s and t , 
construct the set S as follows:
Set S ={s} and while there is an edge e = (v, w) with either
(i) f(e) < c(e), v e S, w e/ S or
(ii) f(e) >0,v e/ S,w e S add to S the vertex either v or w 
not already in S.
The set S so created can be used to either increase the flow or show that the 
flow is already at capacity. If t e S, then there is a path from s to t ,say 
v0 = s,v1,...,vk = t such that
(i) (vi,vi+1 ) is a (forward) edge e such that f(e) < c(e) or
(ii) (vi+1,vi) is a (backward) edge e such that f(e) >0.
Thus the path can contain both “forward” and “backward” edges (going with 
or against the edge direction). On the forward edges increase the flow by 1 
(since the flows and capacities are integers and by assumption f(e) < c(e) 
this can be done). On backward edges decrease the flow by 1 (other edges are 
https://doi.org/10.1017/9781009283403.006 Published online by Cambridge University Press

5.1 Network Flows
133
not affected). It is clear that all nodes not in the path are on edges that are not 
affected. It is easily shown that these operations preserve conservation of flow 
at nodes that are neither 5 nor t and the flow is increased by 1, i.e., val (f ) = 
val(f) + 1.
Similarly it is shown that if t / S, then C = (S* ,S*) is a cut and the original 
flow achieves capacity, i.e., val(f) = cap(C), which finishes the proof. Thus 
a maximum flow is achieved if and only if there is no path which can be 
augmented.
The above theorem shows that a maximal flow of a single source, single­
sink network (a unicast network) always exists and the flow is given by the 
flow across a min-cut. While integer flows have been assumed, it is easily 
generalized to real flows.
Consider now a multicast situation where there is a single source node 5 
and multiple sink nodes, say TL = {t1,t2,...,tL} (referred to as a multicast 
network). It is clear that if in such a network the maximum flow possible, say 
rn, so that every sink node receives all information from the source at a rate of 
at least rn, then it must be true that
rn < min max-flow(s,t^).
tl T Tl
Until now it has been assumed that in the network the nodes are only capable 
of either storing or forwarding messages they receive (replicating them if 
necessary for each forwarding edge from the node). In this case the above 
upper bound on received rate is not generally possible to achieve. In [1] it is 
shown that if one allows the nodes to perform network coding, then the bound 
is achievable. Furthermore in [36] it is shown that linear coding is sufficient 
to achieve capacity (max-flow) in multicast networks. By this it is meant that 
packets arriving at a node are linearly combined, treating the packets as a set 
of symbols in a finite field and allowing multiplication of packets by elements 
of the field and summing packets over that finite field. Generally the point is to 
allow each node to perform operations on the information it receives on each 
of its incoming edges and transmit functions of the received information on its 
outgoing edges. The situation is discussed further in the next section. It will 
be shown how allowing such operations can increase information flow. The 
messages transmitted on the edges are often assumed to be of the same length 
and referred to as packets or symbols, depending on the discussion.
The example below, the famous butterfly network,2 contained in the original 
network coding paper [1], illustrates the point convincingly. (It is included in 
numerous papers on the subject.)
2 It is remarked in [60] that this term was first suggested by Michelle Effros of CalTech.
https://doi.org/10.1017/9781009283403.006 Published online by Cambridge University Press

134
5 Network Codes
(a)
Figure 5.1 Butterfly network: (a) no 
processing
(b)
node processing and (b) with node
Example 5.2 In the butterfly network of Figure 5.1 a single source, two- 
sink network, the object is to transmit two bits to each of the sinks in a 
delay-less manner. In Figure 5.1(a) it is assumed that each node is capable 
of only replicating and forwarding the received data on an outgoing edge 
and each edge has unit capacity. The cut-set bound for each sink is 2. 
Node v3 has to make a decision as to which bit, either x1 or x2 to forward 
to v4. The result is that one sink node receives both bits while the other 
receives only one. In Figure 5.1(b) nodes are allowed to process the received 
information before forwarding. If node v3 computes x 1 ® x2, it is clear 
both sink nodes t1 and t2 can recover both bits by computing, respectively, 
x 1 ® (x 1 ® x2) = x2 and x2 ® (x 1 ® x2) = x 1, in the same number of 
transmissions.
https://doi.org/10.1017/9781009283403.006 Published online by Cambridge University Press

5.2 Network Coding
135
The example is both simple and elegant and a convincing demonstration 
that allowing processing in the node can significantly improve the transmission 
performance of networks.
As noted, only the case of multicasting is considered in this work where 
there is a single source node and multiple sink nodes and the objective is to 
send the same information to all sinks. In this case it is known [30, 58] that a 
rate that is the minimum of all the max-flow min-cut rates from the source to 
the individual sinks is achievable with linear coding multicast to all sinks in 
this multicast situation. Although Jaggi et al. [30] note that “a classical result 
of Edmonds [14] shows that network coding does not increase the achievable 
rate in the special case where one node is the source and all other nodes in 
the network are sinks” (V = {5} U T where T is the set of sinks). In other 
words, if a certain flow is achievable between a single source and several sink 
nodes individually, then with linear coding this (minimum) rate is achievable 
simultaneously.
The next section gives an overview of the information theoretic basis of 
network coding of the paper [1]. The algebraic approach to representing this 
network coding paradigm contained in [34] is also discussed as well as the 
approach of [46, 61]. Section 5.3 will consider representative explicit network 
coding schemes and some of the problems faced in implementing network 
codes on real networks such as the Internet.
5.2 Network Coding
The description of a network code for the multicast situation with a single 
source node 5 and L sink nodes T = {t1,t2,...,tL} is necessarily involved 
since it must encompass a description of the coding process at each interme­
diate node from 5 to for each such path. The aim is to describe a process 
that gives a prescription of how to code at each node that allows a proof that 
the capacity (max-flow) can be achieved and that will allow each sink node 
to retrieve all information transmitted by the source at the maximum rate. The 
situation in real networks will involve adjustments to the ideas used here in that 
the networks are seldom delayless and synchronous. In practice servers/nodes 
might well combine packets that are not synchronous and operate with time 
tolerances that are not considered in this work.
First a formal model will be introduced as originally conceived ([1] and 
[58]) and based on this model a more familiar linear system-theoretic model 
will be discussed. While this model involves a certain amount of notation 
https://doi.org/10.1017/9781009283403.006 Published online by Cambridge University Press

136
5 Network Codes
and terminology, it captures the essence of the network coding problem. 
Section 5.3 considers a few actual constructions of network codes found in the 
literature as examples of the requirements. It is to be kept in mind that the goal 
of any network coding system is to ensure that each terminal node is able to 
decode the received packets into the set of transmitted packets at the maximum 
rate. This generally means that a sufficient supply of combined packets is 
received by each terminal node that allows the node to solve for the transmitted 
packets, meaning that as the network nodes combine packets (usually linearly), 
a sufficient supply of received combined packets forms a matrix equation of 
full rank that will allow for solution for the unknown information packets. 
Naturally, identification of all the combining processes of the nodes traversed 
is stored in the headers of transmitted packets.
It is assumed all edges (communication links between nodes) are error- 
free and the directed network is acyclic and delayless. Thus the packets are 
transmitted sequentially along a path in that a node can transmit a packet only 
after it has received it, but the process takes no delay. The introduction of 
directed cycles in such a network might lead to instabilities in the processes.
Consider first a network G [58] with a single source node {s} that generates 
a certain amount of information that must be transmitted to each of the terminal 
nodes TL .LetR = (Ri,j) denote the capacity of the directed graph edge 
(i,j) e E. Clearly the max-flow bound from the source to sink t? is governed 
by the max-flow min-cut bound of the previous section. The bound
rn < min max-flow(s,t^) 
(5.2)
tt e Tl
is referred to as the max-flow bound of the multicast network. It is the largest 
rate possible to each terminal node, governed by the smallest (maximum) rate 
to any of the terminal nodes. That such a rate is achievable, in the sense there 
will exist a network code that can achieve such a network rate, is one of the 
important results of network coding.
For a formal and somewhat abstract model of network coding [1, 24, 58], 
consider a source of rate t bits/use and view a message source X which is 
sampled to givex, viewed as a sampling of the set x = 11,2,... ,r = \2nT 1} 
messages, sent as codewords of length n over a symbol alphabet, usually a 
finite field of some order . The coding strategy will be such that each node will 
choose coefficients to linearly combine arriving packages for forwarding to 
the next node so that all r messages can be decoded from the received packets 
at all L sinks at a rate approaching capacity (max-flow) of the network. An 
(n,(ntj : (i,j) e E),t)-code is defined, where E is the set of directed edges 
in the network, n is the length of the block code and for (i,j) e E and t is the 
https://doi.org/10.1017/9781009283403.006 Published online by Cambridge University Press

5.2 Network Coding
137
information rate of the source. A network code is a set of server/node encoding 
functions. It will be referred to as feasible if it is capable of transmitting the 
messages from the source to the sinks within the edge capacities. Functions 
are defined to account for the construction of messages formed at each node, 
based on the messages received at that node. Define a transaction to be the 
sending of a message from one node to an adjacent node and Tij the set of 
transactions sent from node i to node j, (i,j) e E an edge and let V denote 
the set of network nodes. It is assumed there are a total of K transactions in 
the entire coding session from the source to all sinks and the edge on which 
the k-th transaction is sent is specified by two functions u and v:
u: {1,2,... K} —> V
v: {1,2,... K} —> V
such that the k-th transaction takes place on the edge (u(k), v(k)) e E and
Tij ={1 < k < K : (u(k),v(k)) = (i,j)},
the set of transactions from node i to node j . Define sets of indices
Ak={1,2,..., |Ak |}, 
1 <k< K,
where | Ak | indicates the number of indices possible. Similarly nij is the 
number of possible index tuples that can be sent from node i to node j (hence 
on edge (i, j) e E) and the average bit rate of the code on this edge is given 
by n-1 log nij. The total number of index tuples that can be sent on edge (i,j) 
during the coding session is nij. In the k-th transaction, the node u(k) encodes 
the information it receives on its incoming edges according to the function fk 
and sends an index in Ak to v(k), on an edge which connects the two nodes. 
If u(k) = s, then
fk: x —> Ak,
where, as noted, x is the set of possible messages, | x |= r .If u(k) = s define 
the set of nodes that transmit messages to u(k) as Qk where
Qk = {1 < k' <k : v(k') = u(k)},
i.e., the set of nodes that send messages to u(k) e V. If this set is nonempty, 
then
fk: 0 Ak’-+ Ak,
k 'e Qk
which determines the messages sent to node v(k) from u(k). As noted, Tij is 
the set of transactions sent from node i to node j (which determines the edge 
https://doi.org/10.1017/9781009283403.006 Published online by Cambridge University Press

138
5 Network Codes
(u(k), v(k))) for some k via the maps u and v, nij is the number of all possible 
index tuples sent from i to j .
Finally, for each I e {1,2,... ,L} the set We is the set of vertices sending 
data to a sink node t?, i.e.,
We = {1 < k < K : v(k)} = te, 
£ = 1,2,...,L
and the function ge maps the information received on all incoming branches to 
t£ and retrieves the information transmitted at the source in x, i.e.,
g£: fl Ak' —> X.
k 'e Wt
This definition ofa network code, although somewhat abstract and complex, 
captures all the necessary ingredients of specifying at each node the action 
necessary for the transmission of information to a sink. Importantly, it allows 
an information-theoretic proof of the fundamental theorem of network coding.
The following definition establishes the notion of an achievable edge rate 
[1, 58]:
Definition 5.3 For a network G with rate constraints R = {Rij,(i,j) e E}, 
an information rate rn is asymptotically achievable if, for any c > 0 there exists 
for sufficiently large n an (n, (nij : (i, j) e E), t)-code on G such that
n-1log2 nij < Rij + e
for all (i,j) e E where n-1 log2 nij is the average bit rate of the code on 
channel (i, j) and
t > rn — c.
An asymptotically achievable information rate will simply be referred to as an 
achievable information rate.
The fundamental theorem of network coding, stated without proof, is ([1], 
theorem 1, [58], theorems 11.3 and 11.4):
Theorem 5.4 For a multicast network G with rate constraints R, w is an 
achievable information rate if and only if
rn < min max-flow (s,t^).
where the minimum is over all sink nodes t? e TL.
The proofs of such a statement usually treat the delayless acyclic (no cycles 
in the digraph) case and cyclic case separately as further care must be taken in 
treating sequences of nodes, in particular cycles, among other considerations.
https://doi.org/10.1017/9781009283403.006 Published online by Cambridge University Press

5.2 Network Coding
139
While only the single source multicast case has been considered it is generally 
possible to extend the analysis to cover the multiple source case as well.
An algebraic formulation of the network coding problem is given in [34] 
and discussed further in [20, 24, 25, 28], that makes more concrete the above 
approach. The following discussion describes a linear network coding scheme 
following the approach of [24].
It is assumed the source node is s , with source messages designated X = 
(X1,X2,...,Xr), and each of these independent messages must be communi­
cated from the single source to the set of sink nodes TL ={t1,t2,...,tL}.To 
discuss the notion of a linear operation of the nodes it is convenient to assume 
the bitstreams of the sources and edge processes are divided into symbols or 
packets, say of m bits each, allowing their representation as elements of the 
finite field Fq where q = 2m , perhaps even a single field element per message. 
It is also assumed the network is acyclic and delayless so all transmissions 
occur instantaneously and simultaneously although sequentially in that a node 
transmits information only after it has received information on its input edges.
For a nonterminal node v e V\{s,TL} denote by Ni(v) the set of input 
edges to node v and by No(v) the set of output edges. It is assumed there are no 
self-loops on nodes and that Ni (s) = No(tt) = ft, i.e., there are no input edges 
to the source node and no output edges from any sink node. It is desired to 
determine conditions to network code the messages of x = {X 1 ,X2, •• •,Xr} 
for transmission so that each of the L terminal nodes receives all the messages. 
Denote the signal on edge £ of the network to be Y^ where £ e No(v) and
Y = {Er=1 ai,^Xi, if £ e No(s)
£ 
I EkeNi(v) fk,eYk, I e No(v), v e V\{s, Tl}.
As discussed, the coefficients are in a suitable finite field F2m for some symbol 
size m. The coefficients used by each node as it transmits messages to the 
next node are stored in some manner in the message header and modified by 
each node as it forms the messages it transmits on its out edges. A successful 
coding system will ensure the coding is such that it enables the terminal node 
to recover all of the messages sent, i.e., the coefficients chosen by the nodes as 
the messages traverse the network are such that the messages received at each 
terminal e TL node allow solution for all r of the original messages. Thus 
at the sink node e TL, the encoding process, with node coefficients used 
in each linear combining stored in packet headers, has been such that each 
source message input Xi,i= 1, 2,...,r,X = (X1,...,Xr), can be recovered 
from the signals on the edges entering each terminal node tt, Ni (t?), by finding 
appropriate coefficients btt such that
https://doi.org/10.1017/9781009283403.006 Published online by Cambridge University Press

140
5 Network Codes
Ztt,i = E btaij)Yj, i = 1,2,...,r,te e Tl (5.3)
and that Ztt,i = Xi, i = 1,2,. ..,r, i.e., it is the i-th message reproduced at 
the terminal node tg. Equivalently the matrix of such coefficients Btt = btt,(i,j) 
an r x E matrix is of full rank for each terminal node where E is the set of 
edges and | E |= E. The matrix Btt is nonzero only for edges (columns of 
Btt) into the terminal node tg. It is the matrix of coefficients that restores the 
messages at terminal node tg from the input messages.
Let the r x E matrix involved in describing the messages on edges 
emanating from the source node be denoted A = (ai,j) and the E x E matrix 
describing messages on edges out of internal nodes be F = (fi,j), i.e., fi,j is 
the coefficient used to multiply the message on edge i into a node to form part 
(a component) of the message output on edge j.LetX = (X1,X2,...,Xr) be 
the vector of messages.
The transfer function from 5 to tt is required which will describe the transfer 
from the source messages to the sink outputs, i.e., it tracks the paths of the 
packets and the specific linear operations performed on the packets at each 
node as the packets traverse the network. The matrix product X • A, A an r x E 
matrix then gives the signals on the edges of N0 (5), the output edges of the 
source.
The E x E matrix F = (fi,j) gives the transfer of signals on edges from 
one instant to the next. Label the rows and columns of the matrix F by edges 
e1,e2,...,e|E|. The matrix will have the coefficient fij at row ei and column 
ej if and only if ei,+ = ej,- and the message into node ei,+ on edge ei is 
multiplied by fij before combining with other messages and transmitted out 
on edge ej . It will have zeros in matrix positions not corresponding to edges. 
If the signals on the edges are given by Y = (Ye1,Ye2,...,YeE) at one stage, at 
the next stage they are given by Y' = Y • F. Notice that
(I - F)-1 = I + F + F2 + ••• .
For an acyclic network the matrix links can be indexed and ordered in a 
sequential manner such that if e1 and e2 are two edges such that e1,+ = e2, -, 
then e1 has a lower index than e2 and it follows that the matrix F is upper 
triangular (and in particular, zeroes on the main diagonal - no loops). Hence F 
is upper triangular and hence nilpotent (i.e., Fm = O for some positive integer 
m) and the matrix (I - F)-1 is nonsingular. The matrices {A,F,Bt1,...,BtL} 
form a linear network code as it is clear that the sum of any two codewords 
is a codeword [20, 25, 29, 34]. The transfer function from the inputs X = 
https://doi.org/10.1017/9781009283403.006 Published online by Cambridge University Press

5.2 Network Coding
141
(X 1 ,X2,..., Xr) to the output processes Ztt = (Ztt, 1,..., Ztl,r) is given by 
the transfer matrix Mtl = A (I - F) -1 BT in that Ztt = X • A (I - F)-1 BT.
Thus [29] the set (A,F,Bt1,...,BtL) is a network code and a feasible 
solution to the multicast coding problem. The £-th column of the matrix 
A (I - F)-1 specifies the map from input signals to the signal on edge £. If 
the set of edges into sink tg are denoted t? and the corresponding columns of 
the matrix by ATt, the network has a feasible solution iff this matrix has full 
rank for all sinks in TL .
To examine this nonsingularity define the extended transfer matrix [20] 
(Edmonds matrix in [25]) as
Ntt =
AO
I - FBT
(5.4)
a form that shows certain properties of the transfer matrix more clearly. A 
simple proof of the following lemma is given:
Lemma 5.5 ([20], lemma 2.2, [25]) det Mtt = ± det Ntt
Proof: From the matrix equation
A O1 ((I - F)-1BT (I - F)-1
I - FB?] 1 
-1 
O 
'Mtl A(I - F)-1
OI
and taking determinants of both sides gives
± det Ntt • det (-I) • det (I - F)-1 = det Mtt • det I.
■
By examining the structure of the determinant of the above matrices and the 
polynomials in the matrix indeterminates, one can show it is always possible 
to choose matrix elements so this is true, given a large enough finite field for 
the coefficients:
Lemma 5.6 ([29], theorem 1) For a feasible multicast network problem with 
independent or linearly correlated sources and L sinks, in both the acyclic 
delay-free case and general case with delays, there exists a solution
(A,F,Bt1,...,BtL)
over the finite field ifq>L.
The proof of this lemma is rather involved and beyond our purpose (see 
[26], theorem 4, [25], theorem 1, [29], lemma 2), involving the products of 
gains along the edge-disjoint paths and the determinant of subsets of columns 
of the matrix A as well as the degree of the polynomial of certain variables 
https://doi.org/10.1017/9781009283403.006 Published online by Cambridge University Press

142
5 Network Codes
in the determinants. Thus if the entries of the matrices are treated as variables 
and the determinant is expanded into the variables, a multivariable polynomial 
results. If the original entries are substituted for the variables, the polynomial 
evaluation should be nonzero or a nonsingular matrix.
The above linear network coding is interesting in that it illuminates the 
requirements of network codes from a linear system point of view, yet falls 
short of actually constructing network codes. The following section considers 
a few examples of such constructions.
Further aspects of properties of network codes, such as generic linear 
network codes, linear dispersion codes, linear broadcast and multicast codes, 
are considered in [36, 59, 61].
While only the delay-free and acyclic cases have been considered here, the 
extension of the results to networks with cycles and delays is generally possible 
by introducing a delay variable and rational functions.
5.3 Construction and Performance of Network Codes
To recap the previous two sections, assume an acyclic multicast delayless 
directed network is assumed with one source node s and L sink nodes TL and 
the same source information is required at each sink. More general situations 
with multiple sources and sinks with different sources transmitting different 
information and sinks requiring different information from the different 
sources are possible but the analysis of such general situations becomes even 
more complex and the special case of multicast considered here gives an 
indication of the techniques used and insight of network performance.
The capacity of a multicast network was given by Equation 5.2
rn = min max-flow(s,tt),
tt S TL
the minimum of the capacities of the L unicast paths. For a given source­
sink pair, if all edges are assumed to have unit capacity and parallel edges 
are allowed, then the maximum number of pairwise edge-disjoint paths from 
source to sink is equal to the capacity of the min-cut, a result referred to as 
Menger’s theorem. The Ford-Fulkerson algorithm can be used to find such 
paths [18]. The assumption of unit capacity edges is often easier to handle but 
the generalization to integer capacities is straightforward.
The previous section noted that a rate approaching w is achievable asymp­
totically by using coding at the nodes [1]. In [36] it was shown that this rate can 
in fact be achieved with linear coding, assuming coding over some finite field. 
Such a result is not valid in general networks with multiple sources and sinks.
https://doi.org/10.1017/9781009283403.006 Published online by Cambridge University Press

5.3 Construction and Performance of Network Codes
143
The question as to how such practical and specific network codes can be 
constructed is commented on below with three particular constructions from 
the literature.
Deterministic Constructions of Network Codes
The construction of linear network codes is necessarily algorithmic since an 
arbitrary but given network is assumed and the coding scheme is a function 
of the network topology. Specification of the operations required at each node 
of the network is required. The works of [20, 30, 42] assume unit capacity 
edges and find edge-disjoint paths from 5 to t^ for each single source-sink pair, 
t£ e TL. Techniques to overcome the restriction to unit capacity edges are 
given there but not considered here. The informal discussion of the proof for 
the basic construction of such codes is outlined.
The basic algorithm of [30], referred to as the linear information flow (LIF) 
algorithm or the Jaggi algorithm, is a centralized algorithm in that the topology 
of the network is known and the code is designed beforehand. It assumes a 
network of unit capacity edges with multiple edges allowed, which is able to 
model networks with integer flows.
Assume the network has a max-flow of r symbols per channel session 
(rather than the previous rn to emphasize integer value). Note this implies at 
least r edges in No(s) and in Ni(tt) for each tt e TL. The objective of the 
algorithm is to have a nonsingular matrix transmitted with the information 
packets so the terminal node tt is able to retrieve the r information packets 
from the r received packets. Recall for this discussion it is assumed each edge 
of the graph has unit capacity and parallel edges are allowed.
The case of only one terminal node is considered - the extension to 
multiple terminal nodes is obvious. The algorithm proceeds in two stages. In 
the first stage a set of r edge-disjoint paths from source 5 to tt is found. The 
Ford-Fulkerson algorithm can be used for such a purpose. That such a set 
of paths exist follows from Menger’s theorem, previously mentioned, which 
proves for unit capacity edges the maximum number of edge-disjoint paths 
between any two nodes is the capacity of the min-cut between those nodes, in 
our case assumed to be r .
The following is a slightly modified version of the LIF in [30].
Consider the r edge-disjoint paths P1,...,Pr from 5 to tt. Messages 
(or symbols or packets) X 1 ,X2,... ,Xr are to be transmitted to . Denote the 
message sent on edge e by Ye where
r
Ye = 
me,i Xi.
https://doi.org/10.1017/9781009283403.006 Published online by Cambridge University Press

144
5 Network Codes
(If not all packets are represented by the incoming packets to the node, zero 
coefficients occur.) The edges on the r paths are examined sequentially both 
across the paths and along the paths in such a manner that if edges ej and ek 
are on a particular path Pi and ej precedes ek in travel from 5 to tt on Pi, then 
ej is considered before ek, i.e., as each path is considered in the algorithm, the 
“first” nonvisited edge along the path is considered.
Two properties of the transmission algorithm are tracked as each path is 
visited and updated as each edge is considered: an r x r invertible matrix Mt 
(over the symbol field, often F2» - for some suitable 5) and a set of edges Ct.
Consider visiting each path sequentially in some fixed order. As a path Pi 
is visited the first edge along the path from source 5 to terminal node tt not yet 
visited is considered, say edge e = (u, v) e Pi. The set Ct keeps track of the 
edges visited, one from each path. As each new edge e = (u, v) e Pi is con­
sidered, the previous edge on Pi is dropped from Ct and the new edge added.
The r x r matrix Mt keeps track of the coefficients used to form the message 
on each edge in the set Ct, the i-th column containing the coefficients used for 
forming the message on the last edge considered on Pi . As the edge e e Pi 
is visited the corresponding column of Mt (representing the previous edge 
on Pi visited) is dropped and replaced with a new column representing the 
coefficients used for the message on the new edge e in such a way that the 
matrix Mt is nonsingular. The next path (in some sequence) is then visited, 
considering the first edge not yet visited on that path. As the algorithm evolves 
eventually a stage is reached where it reaches the terminal node tt and the fact 
the matrix Mt, which represents the coefficients used to form the messages 
on the r input edges to tt, is nonsingular means the r original messages 
X1,...,Xr can be retrieved. In the case the r paths are not of equal length, 
the algorithm is easily modified.
The properties of this LIF algorithm are described in ([30], theorem 3). The 
algorithm has complexity O(| E | • | TL | • (h + | TL |)) and any field of size 
q > 2 | TL | can be used for the encoding. A node needs time 
O min(| TL | , |Ni (e- |) to compute the symbol to be sent on edge e.
While this simple argument pertains to unicast acyclic networks with unit 
capacity edges, itis shown [30] that it can be extended to integer capacity edges 
and multicast networks and even to algorithms that deal with edge failures. 
Network codes that can deal with edge failures are often referred to as robust 
network codes.
Numerous other interesting results are shown in [30] including the simple 
demonstration (theorem 2) that there are acyclic networks with unit capacity 
edges where multicasting with coding allows a factor Q(log | V |) larger rate 
than multicasting without coding, V the set of nodes of the network.
https://doi.org/10.1017/9781009283403.006 Published online by Cambridge University Press

5.3 Construction and Performance of Network Codes
145
Random Linear Codes
The previous approach to constructing an effective network code required a 
degree of centralization and knowledge of the network topology since this 
must be known in the construction. This can be a drawback in situations where 
the entire network structure may be difficult to determine or is evolving. An 
approach that removes this constraint is to choose the message coefficients 
from Fq at random. At each sink then, for the process to be successful, the 
random matrix of coefficients must be invertible, i.e., have full rank. The 
approach was first considered in [25, 26, 27] and further developed in [29]. 
Only the single-source unicast delay-free case is discussed here although the 
work [29] considers general networks with cycles and delays.
Suppose, as above, there are r information sources, which for convenience 
are considered packets, at the source node s . At each node along the path 
to the sink, a number of information packets are received. The node forms 
a linear (over Fq) combination of the packets to transmit on each out edge, 
each element of Fq being chosen uniformly at random. The sink node t must 
receive a sufficient number of linearly independent coded packets in order to 
determine the r information packets, i.e., each node and in particular the sink 
node receives at least r (usually more) linear sums of the r information packets 
from which it is to solve for the information packets.
Expressions for the probability of success in retrieving the information 
packets are readily obtained. Suppose the sink node obtains r + m, m > 0 
packets, each packet containing a random linear sum of the information 
packets, X1,X2,...,Xr, each coefficient chosen uniformly at random from 
F2. The probability the r x (r + m) coefficient matrix over F2 is of full rank 
(r) has been derived (e.g., Appendix A, Equation A.7) to be
r+m / 
1 \
n (1 - 20 ■
= m +1
The m = 0 case corresponds to exactly r packets being received. For large r 
and m> 1 the probabilities can be shown to converge to unity rapidly with m. 
This approach is pursued in [47]. Thus, typically very few extra packets m are 
required to ensure nonsingularity of the matrix.
Another approach to determining the performance of random linear coding 
is to consider again the (ordinary) transfer matrices of Lemma 5.5 and whether 
or not they are of full rank and hence allow solution, where some or all of the 
matrix entries are chosen at random. A more detailed look at the expression for 
the determinants of these matrices (as, e.g., given in lemma 2 of [29]) leads to 
the following:
https://doi.org/10.1017/9781009283403.006 Published online by Cambridge University Press

146
5 Network Codes
Theorem 5.7 ([24], theorem 2.6, [29], theorem 2) Consider a multicast 
connection problem on an arbitrary network with independent or linearly 
correlated sources and a network code in which some or all of the network 
code coefficients {a,f} are chosen uniformly at random from the finite field Fq 
where q>Land the remaining code coefficients, if any, are fixed. If there 
exists a solution to the network connection problem with the same values for 
the fixed code coefficients, then the probability the random network code is 
valid for the problem is at least (1 - L/q) where n is the number of edges 
with the associated random coefficients.
As before, this theorem follows from considering the expansion of the trans­
fer matrix determinants viewing the entries as variables and considering the 
number of variable values for which the determinant is nonzero. The expansion 
of the determinant is viewed as a multivariate polynomial in the indeterminates 
and the question of nonsingularities becomes one of polynomial zeros. The 
actual assigned values can be considered as random values and hence the 
probability mentioned in the previous theorem.
It is argued in [35] that random network coding (RNC) (usually random 
linear network coding - RLNC) is likely to be most effective in peer-to-peer 
(P2P) networks where overlay networks are formed among participating users 
and where centralized coordination for transmission and coding strategies is 
not needed. Further, such peers are more likely to be able to afford the comput­
ing power necessary to implement finite field arithmetic and matrix inversion. 
That work considers progress on the use of RLNC in both large file download 
and streaming applications. It also reports on the use of gossiping techniques, 
where each user transmits to selected neighbors to reach all required partici­
pants after a number of rounds. An advantage of RLNC, similar to rateless cod­
ing techniques, is that a receiver needs only to collect the required number of 
any coded packets before applying Gaussian elimination to decode, rather than 
wait for specific packets it may be missing. A disadvantage is the complexity of 
inverting a matrix over the finite field in order to obtain a solution. Interesting 
references on RLNC and packet networks more generally include [12, 37, 53].
Batched Sparse (BATS) Codes
BATS codes are a special class of interesting network codes designed for the 
efficient transmission of packets through a network. They combine aspects 
of fountain coding and linear network coding to achieve certain performance 
characteristics that make them attractive for implementation. A brief overview 
of their construction is described. The notation of [54, 55, 56] is followed.
https://doi.org/10.1017/9781009283403.006 Published online by Cambridge University Press

5.3 Construction and Performance of Network Codes
147
Suppose a file is divided into K packets each represented by T symbols over 
the alphabet Fq and define the T x K matrix whose columns are packets 
(column vectors):
B=[b1,b2,...,bK].
A subset of the columns (packets) of B, Bi is formed as follows. Much as for 
fountain codes (see Chapter 2) a discrete distribution = J K=0 is assumed. 
The distribution will be optimized later. The distribution is sampled to give 
integer di with probability ^di. From B, di packets (columns) are chosen 
uniformly at random from the K packets and used as columns to form the 
T x di matrix Bi. For a fixed integer M a batch Xi of M packets is formed by 
the matrix operation
Xi = BiGi
where Gi is a di x M matrix over Fq . This matrix Gi can be either predesigned 
or randomly generated as needed. Only the random case is considered here and 
the elements of Gi are chosen uniformly at random from Fq . Thus the batch 
Xi consists of the M packets formed by random linear combinations of the 
(original uncoded) di packets of Bi . The notion of batches seems related to 
the notion of “generations” used by [10] and “chunks” used by [39] in other 
contexts. Suppose n (not fixed) batches {X1,X2,...,Xn} are formed in this 
manner. The situation is represented in Figure 5.2(a) as a Tanner graph with the 
K variable nodes of the original packets and the check nodes as the n (coded) 
batches, each batch containing M random linear combinations of a randomly 
selected subset of certain packets of a certain number di (for Bi ) determined 
by sampling the distribution .
The packet header contains information as to the batch number and original 
packets. The source transmits the packets in a batch randomly into the network. 
An intermediate node receives the packets of a batch over incoming links 
and applies (random) linear network coding of packets into new packets for 
outgoing links. It is important to note that in this operation only packets from 
the same batch are linearly combined.
The specific types of operations allowed at the intermediate nodes might 
depend on the type of network and the application. However, the transfor­
mations on the packets as they traverse the network do not depend on the 
specifics of the network operations. Let Yi be the set of packets received 
by the destination node, all from the same batch. As in a previous section, 
the combined effect of the linear combining operations used at the various 
intermediate nodes of the network can be realized by a transfer matrix Hi . 
Thus Yi can be expressed by
https://doi.org/10.1017/9781009283403.006 Published online by Cambridge University Press

148
5 Network Codes
Yi = BiGiHi
where the matrix Hi is the transfer matrix that reflects the operations performed 
by the intermediate network nodes. Such channels are referred to as linear 
operator channels (LOC) [57]. It is a matrix with M rows and a number of 
columns that depends on the number of distinct packets received - say L. Thus 
Yi is a T x L matrix over Fq. The matrix GiHi is known to the receiver by 
contents of the packet headers. The equation is to be solved for the di data 
packets that were used in the formation ofBi. This will be possible if and only 
if the matrix Gi Hi is of full rank di .
The decoding operation is depicted in Figure 5.2 (only the edges for check 
node GiHi are shown for clarity) and is as follows. If rank of the di x n 
matrix GiHi is full (i.e., di and assuming all batches are represented), then 
the above equation will have a unique solution for the involved di original 
packets obtained by Gaussian elimination. The solution for the di packets 
bij,j = 1,2,...,di can be found and substituted for the unknown variable 
nodes on the left-hand side of the graph. These now-known packet values 
can then be used by each neighbor check node to reduce the size of the 
matrix equations (by removing the corresponding columns) - thus reducing 
the complexity of further matrix inversions. The process is repeated until all 
variable nodes are resolved.
original batches
packets
original received
packets 
information
bi 
bi+1
bK-2 
bK-1 
bK
b1 
b2
b3
B1
B2
B3
Bi
Bi+1
Bi+2
Bn-2
Bn-1 
Bn
(a)
• G1H1
• G 2 H2
bK-1 • 
• Gn-2 Hn-2
bK • 
• Gn-1 Hn-1
• GnHn
(b)
(only edges for 
Gi Hi shown)
Figure 5.2 BATS code representations: (a) encoding and (b) decoding
https://doi.org/10.1017/9781009283403.006 Published online by Cambridge University Press

5.3 Construction and Performance of Network Codes
149
Further details on the construction, analysis and performance analysis of 
these BATS codes may be found in the papers [54, 55] and especially the 
detailed monograph [56]. Conditions are developed on how to choose the 
distribution to ensure decoding success with high probability.
Index Codes
The notion of index coding arose first in the context of source coding with side 
information (e.g., Birk and Kot [5, 6]). The form of the problem discussed in 
[3, 4] is briefly outlined here to give an indication of the results available. In 
general the situation is that a server has n clients, R1,R2,...,Rn, and a set 
of information blocks x = {X 1 ,X2,... ,XL}. Each client already contains a 
subset of the blocks and is able via a low-rate back channel to request other 
blocks from the server. The contents of each client’s cache are known to the 
server and the problem is for the server to transmit (broadcast) a sufficient 
minimum amount of information so that each client receives the blocks 
requested. There is no client-to-client communication and all communication 
is error-free.
It is clear ([6], lemma 1) that one can limit interest to the case where only 
one block is requested by each client in that a client requesting r blocks could 
be “split” into r subclients. The following model is of interest [4, 5, 6]. The 
data held by the server is regarded as an n -bit string x e {0,1}n ~ F2n and 
there are n receivers. Receiver Ri,i = 1, 2,...,n is interested in receiving 
xi and possesses a subset of x (although not xi). The side information of the 
clients is conveniently represented by a directed graph G (referred to as the 
side information graph) on n nodes V with no self-loops (node i does not have 
xi as side information) and no parallel edges. The graph has a directed edge 
(i,j) if receiver Ri has bit xj. The case of more information bits than clients 
is easily taken care of.
For a subset S c [n] let x [S] be the subset of x corresponding to the 
coordinates of S. The side information contained by client Ri is denoted
Ni = {j e V | (i,j)) e E}, i / Ni,
where E is the set of directed edges of the graph G. Note that one could also 
have used a directed bipartite graph (as in [32]) with the set of clients as left 
vertices and information bits as right vertices with directed edges indicating 
side information and information requests of clients.
Definition 5.8 ([4]) A deterministic index code C for a string x ~ F2 
with side information graph G is a set of codewords over F2 of total length 
£ together with
https://doi.org/10.1017/9781009283403.006 Published online by Cambridge University Press

150
5 Network Codes
R2 
R3
R1
N1 = {2}N2 ={3}
N3 = {4}N4 ={5}
N5 = {6}N6 ={1}
R6 
R5
(a)
R2 
R3
R1
N1 = {3,5N},2 = {4,6}
N3 = {1,5N},4 = {2,6}
N5 = {1,5N},6 = {2,4}
R6 
R5
(b)
Figure 5.3 Example side information graphs
N1 = {2,3},N2 = {3,4}
N3 = {4,5},N4 = {5,6}
N5 = {6,1},N6 = {1,2}
(i) an encoding function E that maps {0, 1}n to a set of codewords;
(ii) a set of decoding functions D1,D2,...,Dn such that
Di (E(x),x[Ni]) = xi,i = 1, 2,...,n.
Example 5.9 Figure 5.3 shows some small examples of nodes containing 
files and their associated side information graphs to illustrate the concept. The 
corresponding side information matrices are discussed in Example 5.13.
The following definition will prove important to the subject.
Definition 5.10 Let G be a directed graph on n vertices without self-loops or 
parallel (directed) edges. The {0, 1} matrix A = (aij) is said to fit G if for all 
i and j (1) aii = 1 and (2) aij = 0 whenever (i,j) is not an edge of G. The 
matrix A is the side information matrix of the side information graph G.
Notice that for such a matrix A — In for In the n x n identity matrix is an 
adjacency matrix for the edge subgraph ofG. Denote by rk2 the rank ofa {0, 1} 
matrix A over F2.
Definition 5.11 minrk2(G) = min |rk2(A) | A fits G .
This quantity minrk2 (G) is shown to be related to the clique number of 
G (the largest complete subgraph in G), the Shannon capacity of G and 
the chromatic number of G (smallest number of colors needed so that no 
two neighboring vertices have the same color). An index code then is a set 
of codewords (bits in our case) the server can transmit so that each client 
Ri,i = 1,2,...,nis able to retrieve xi from the transmitted codewords given 
their side information. The code is called linear if the encoding functions of 
the server are linear over F2.
https://doi.org/10.1017/9781009283403.006 Published online by Cambridge University Press

5.3 Construction and Performance of Network Codes
151
The important property of minrk2 (G) for our interest is the following:
Theorem 5.12 ([3, 4]) For any side information graph G there exists a linear 
index code for G whose length is minrk2(G). The bound is optimal for all linear 
index codes for G.
Example 5.13 The side information matrices of the examples in Example 
5.9 are as follows:
1 10000
101010
011000
0 10 10 1
001100
10 10 10
A(a) =
000110
, A(b) =
0 10 10 1
000011
10 10 10
100001
010101
111000
0 1110 0
A(c) =
001110 
000111 
100011
1 10001
A code for each of the cases is simple to find. The matrix A(a) is of 
rank 5 and a coding scheme that achieves the bound of Theorem 5.12 is 
x 1 ® x2,x2 ® x3 ,x3 ® x4,x4 ® x5 and x5 ® x6 (each of the five codewords is 
one bit). R 1 receives x 1 by XORing its side information with x 1 ® x2 retrieves 
x2 and similarly for R2, ..., R5. R6 receives x6 by XORing all the codewords 
together to give x 1 ® x6 and XORing this with its side information x 1.
Another “extreme” contrived case (not shown in the figure) might be where 
each receiver has as side information all bits except its own, i.e., Ri,i = 
1, 2, ..., n has side information Ni = V \{i }. In this case the side information 
matrix would be the all-ones matrix of rank 1 and a code would be the single 
bit x 1 ® • • • ® xn.
The matrix A(b) is of rank 2 - the graph is the union of two disjoint graphs, 
each with three vertices. The two codewords are x1 ®x3 ®x5 and x2 ®x4 ®x6.
The matrix A(c) can be shown to have rank 4 and four codewords are y1 = 
x1 ® x2 ® x3, y2 = x2 ® x3 ® x4, y3 = x3 ® x4 ® x5,y4 = x1 ® x5 ® x6.
The notion of random codes for index codes is also considered [2]. It is 
clear there are close connections between index coding and the more general 
network coding and a few of these are explored in [15, 16]. The papers [7, 32] 
consider index coding with error correction.
https://doi.org/10.1017/9781009283403.006 Published online by Cambridge University Press

152
5 Network Codes
The monograph [2] is an excellent and comprehensive view of this sub­
ject that considers determining capacity regions for such codes and code 
constructions.
Network Codes on Real Networks
Given the theoretical results on network coding that imply very significant 
performance gains on network performance, it is not a surprise there has 
been considerable effort to realize these gains on real networks. This includes 
the pervasive Internet where such gains would have a major impact on 
performance. This would be at both the IP layer and the application layer where 
overlay networks are of particular interest. Other packet networks such as ATM 
networks, wireless ad hoc networks and many others are potential customers 
for the benefits of network coding.
However, there are major obstacles standing in the way of these perfor­
mance gains. Typically in real networks, packets experience random delays 
on edges whose capacities may be unknown and may be bidirectional. Hence 
it may not be realistic to compute the capacity of the network with any 
degree of confidence. Typically there will be many cycles, contradicting the 
assumption of an acyclic network, although the precise network topology may 
be unknown. Edges and nodes may fail requiring continuous adjustment of 
the transmission strategy. Transmitting a large file on a network coded system 
would generally require breaking the file into manageable segments which 
introduces the problem of latency at the nodes as the nodes wait for a sufficient 
number of packets to combine for their next linear combining for transmission.
The work of Chou et al. [10] introduces two ideas to address some of these 
problems. First they consider appending a global encoding vector which, in 
essence, gives the coefficients of the information packets transmitted from 
a given node on a given outgoing edge. Thus each packet that arrives at 
a node contains a global encoding vector. As each node forms a linear 
encoding of the packets arriving, it is also able to translate those coefficients 
into the coefficients that would be needed to form the outgoing packet in 
terms of the information packets themselves. Thus if a sink node receives 
a sufficient number of linearly independent packets, it is able to compute 
from the coefficients in the arriving packets, via Gaussian elimination, the 
information packets themselves. This solution has the desirable properties that 
it does not depend on network topology and is unaffected by link and node 
failures. In addition the work [10] suggests dividing an information source into 
generations (and packets within a generation), a generation number appearing 
in a packet header. Only packets within a generation are then combined which 
limits the size of the Gaussian elimination needed at the receiver.
https://doi.org/10.1017/9781009283403.006 Published online by Cambridge University Press

References
153
Numerous other issues concerning the advantages and practicality of net­
work coding are considered in the extensive literature, including the references 
[10, 19, 21, 22, 23, 35, 38, 39, 40, 41, 44, 45, 50, 51].
Comments
The subject of network coding embraces a wide variety of techniques and 
disciplines and this chapter is limited in its scope. The seminal papers of the 
subject include [1] the paper that first outlined the possible gains to be had 
through network coding [36], the paper that showed that linear coding was 
sufficient to achieve these gains (under certain circumstances) and [28, 29] the 
papers that described the important notion of RLNC. The monographs [24] and 
[59] are very informative, as is the article [60]. The paper [34] described the 
algebraic approach to network coding that is widely followed. The monographs 
[2] on index coding and [56] on BATS codes are detailed and authoritative 
expositions on these topics.
The important technique of random subspace coding for networks, devel­
oped in [33, 43], is discussed in Chapter 12 due to its connections with rank­
metric codes.
References
[1] Ahlswede, R., Cai, N., Li, S. R., and Yeung, R.W. 2000. Network information 
flow. IEEE Trans. Inform. Theory, 46(4), 1204-1216.
[2] Arbabjolfaei, F., and Kim, Y.-H. 2018. Foundations and trends in communications 
and information theory. NOW Publisher, Boston, MA.
[3] Bar-Yossef, Z., Birk, Y., Jayram, T.S., and Kol, T. 2006. Index coding with 
side information. Pages 197-206 of: 2006 47th Annual IEEE Symposium on 
Foundations of Computer Science (FOCS’06).
[4] Bar-Yossef, Z., Birk, Y., Jayram, T.S., and Kol, T. 2011. Index coding with side 
information. IEEE Trans. Inform. Theory, 57(3), 1479-1494.
[5] Birk, Y., and Kol, T. 1998. Informed-source coding-on-demand (ISCOD) over 
broadcast channels. Pages 1257-1264 of: Proceedings. IEEE INFOCOM ’98, the 
Conference on Computer Communications. Seventeenth Annual Joint Conference 
of the IEEE Computer and Communications Societies. Gateway to the 21st 
Century (Cat. No. 98, San Francisco, CA), vol. 3.
[6] Birk, Y., and Kol, T. 2006. Coding on demand by an informed source (ISCOD) for 
efficient broadcast of different supplemental data to caching clients. IEEE Trans. 
Inform. Theory, 52(6), 2825-2830.
[7] Byrne, E., and Calderini, M. 2017. Error correction for index coding with coded 
side information. IEEE Trans. Inform. Theory, 63(6), 3712-3728.
https://doi.org/10.1017/9781009283403.006 Published online by Cambridge University Press

154
5 Network Codes
[8] Cameron, P.J. 1994. Combinatorics: topics, techniques, algorithms. Cambridge 
University Press, Cambridge.
[9] Celebiler, M., and Stette, G. 1978. On increasing the down-link capacity of 
a regenerative satellite repeater in point-to-point communications. Proc. IEEE, 
66(1), 98-100.
[10] Chou, P.A., Wu, Y., and Jain, K. 2003 (October). Practical network coding. In: 
Allerton Conference on Communication, Control, and Computing. Invited paper.
[11] Cormen, T.H., Leiserson, C.E., Rivest, R.L., and Stein, C. 2009. Introduction to 
algorithms, 3rd ed. MIT Press, Cambridge, MA.
[12] Dana, A.F., Gowaikar, R., Palanki, R., Hassibi, B., and Effros, M. 2006. Capacity 
of wireless erasure networks. IEEE Trans. Inform. Theory, 52(3), 789-804.
[13] Diestel, R. 2018. Graph theory, 5th ed. Graduate Texts in Mathematics, vol. 173. 
Springer, Berlin.
[14] Edmonds, J. 1965. Minimum partition of a matroid into independent subsets. 
J. Res. Nat. Bur. Standards Sect. B, 69B, 67-72.
[15] Effros, M., El Rouayheb, S., and Langberg, M. 2015. An equivalence between 
network coding and index coding. IEEE Trans. Inform. Theory, 61(5), 2478-2487.
[16] El Rouayheb, S., Sprintson, A., and Georghiades, C. 2010. On the index coding 
problem and its relation to network coding and matroid theory. IEEE Trans. 
Inform. Theory, 56(7), 3187-3195.
[17] Elias, P., Feinstein, A., and Shannon, C. 1956. A note on the maximum flow 
through a network. IRE Trans. Inform. Theory, 2(4), 117-119.
[18] Ford, Jr., L.R., and Fulkerson, D.R. 1956. Maximal flow through a network. 
Canad. J. Math., 8, 399-404.
[19] Gkantsidis, C., Miller, J., and Rodriguez, P. 2006 (February). Anatomy of a P2P 
content distribution system with network coding. In: IPTPS’06.
[20] Harvey, N.J.A., Karger, D.R., and Murota, K. 2005. Deterministic network coding 
by matrix completion. Pages 489-498 of: Proceedings of the Sixteenth Annual 
ACM-SIAM Symposium on Discrete Algorithms. SODA ’05, Philadelphia, PA.
[21] Heide, J., Pedersen, M.V., Fitzek, F.H.P., and Larsen, T. 2008. Cautious view on 
network coding From theory to practice. J. Commun. Net., 10(4), 403-411.
[22] Heide, J., Pedersen, M.V., Fitzek, F.H.P., and Medard, M. 2014 (May). A perpetual 
code for network coding. Pages 1-6 of: 2014 IEEE 79th Vehicular Technology 
Conference (VTC Spring).
[23] Heide, J., Pedersen, M.V., F.H.P., and Medard, M. 2015. Perpetual codes for 
network coding. CoRR, abs/1509.04492.
[24] Ho, T., and Lun, D.S. 2008. Network coding. Cambridge University Press, 
Cambridge.
[25] Ho, T., Karger, D.R., Medard, M., and Koetter, R. 2003 (June). Network coding 
from a network flow perspective. Page 441 of: IEEE International Symposium on 
Information Theory, 2003. Proceedings.
[26] Ho, T., Koetter, R., Medard, M., Karger, D.R., and Effros, M. 2003 (June). The 
benefits of coding over routing in a randomized setting. Page 442 of: IEEE 
International Symposium on Information Theory, 2003. Proceedings.
[27] Ho, T., Medard, M., Shi, J., Effros, M., and Karger, D.R. 2003. On randomized 
network coding. In: 2010 48th Annual Allerton Conference on Communication, 
Control, and Computing (Allerton).
https://doi.org/10.1017/9781009283403.006 Published online by Cambridge University Press

References
155
[28] Ho, T., Medard, M., Koetter, R.L., Karger, D.R., Effros, M., Shi, J., and Leong, 
B. 2004. Toward a random operation of networks. IEEE Trans. Inform. Theory, 
2004, 1-8.
[29] Ho, T., Medard, M., Koetter, R., Karger, D.R., Effros, M., Shi, J., and Leong, B. 
2006. A random linear network coding approach to multicast. IEEE Trans. Inform. 
Theory, 52(10), 4413-4430.
[30] Jaggi, S., Sanders, P., Chou, P.A., Effros, M., Egner, S., Jain, K., and Tolhuizen, 
L.M.G.M. 2005. Polynomial time algorithms for multicast network code con­
struction. IEEE Trans. Inform. Theory, 51(6), 1973-1982.
[31] Jungnickel, D. 2013. Graphs, networks and algorithms, 4th ed. Algorithms and 
Computation in Mathematics, vol. 5. Springer, Heidelberg.
[32] Kim, J.W., and No, J.-S. 2017. Index coding with erroneous side information. 
IEEE Trans. Inform. Theory, 63(12), 7687-7697.
[33] Koetter, R., and Kschischang, F.R. 2008. Coding for errors and erasures in random 
network coding. IEEE Trans. Inform. Theory, 54(8), 3579-3591.
[34] Koetter, R., and Medard, M. 2003. An algebraic approach to network coding. 
IEEE/ACM Trans. Netw., 11(5), 782-795.
[35] Li, B., and Niu, D. 2011. Random network coding in peer-to-peer networks: from 
theory to practice. Proc. IEEE, 99(3), 513-523.
[36] Li, S.-R., Yeung, R.W., and Cai, N. 2003. Linear network coding. IEEE Trans. 
Inform. Theory, 49(2), 371-381.
[37] Lun, D.S., Medard, M., Koetter, R., and Effros, M. 2008. On coding for reliable 
communication over packet networks. Phys. Commun., 1(1), 3 - 20.
[38] Maymounkov, P., and Mazieres, D. 2003. Rateless codes and big downloads. 
Pages 247-255 of: Kaashoek, M.F., and Stoica, I. (eds.), Peer-to-peer systems 
II. Springer, Berlin, Heidelberg.
[39] Maymounkov, P., and Harvey, N.J.A. 2006. Methods for efficient network coding. 
In: In Allerton Conference in Communication, Control and Computing.
[40] Medard, M., and Sprintson, A. 2011. Network coding: fundamentals and applica­
tions. Academic Press, New York/London.
[41] Medard, M., Fitzek, F., Montpetit, M.-J., and Rosenberg, C. 2014. Network 
coding mythbusting: why it is not about butterflies anymore. Commun. Mag., 
IEEE, 52(07), 177-183.
[42] Sanders, P., Egner, S., and Tolhuizen, L. 2003. Polynomial time algorithms for 
network information flow. Pages 286-294 of: Proceedings of the Fifteenth Annual 
ACM Symposium on Parallel Algorithms and Architectures. SPAA ’03. ACM, 
New York.
[43] Silva, D., Kschischang, F.R., and Koetter, R. 2008. A rank-metric approach to 
error control in random network coding. IEEE Trans. Inform. Theory, 54(9), 
3951-3967.
[44] Silva, D., Zeng, W., and Kschischang, F.R. 2009 (June). Sparse network coding 
with overlapping classes. Pages 74-79 of: 2009 Workshop on Network Coding, 
Theory, and Applications.
[45] Sundararajan, J.K., Shah, D., Medard, M., Jakubczak, S., Mitzenmacher, M., and 
Barros, J. 2011. Network coding meets TCP: theory and implementation. Proc. 
IEEE, 99(3), 490-512.
https://doi.org/10.1017/9781009283403.006 Published online by Cambridge University Press

156
5 Network Codes
[46] Tan, M., Yeung, R.W., Ho, S., and Cai, N. 2011. A unified framework for linear 
network coding. IEEE Trans. Inform. Theory, 57(1), 416-423.
[47] Trullols-Cruces, O., Barcelo-Ordinas, J.M., and Fiore, M. 2011. Exact decoding 
probability under random linear network coding. IEEE Commun. Lett., 15(1), 
67-69.
[48] van Leeuwen, J. (ed.). 1990. Handbook of theoretical computer science. Vol. A. 
Elsevier Science Publishers, B.V., Amsterdam; MIT Press, Cambridge, MA.
[49] van Lint, J.H., and Wilson, R.M. 1992. A course in combinatorics. Cambridge 
University Press, Cambridge.
[50] Wang, M., and Li, B. 2006 (June). How practical is network coding? Pages 
274-278 of: 2006 14th IEEE International Workshop on Quality of Service.
[51] Wang, M., and Li, B. 2007. R2: random push with random network coding in live 
peer-to-peer streaming. IEEE J. Sel. Areas Commun., 25(9), 1655-1666.
[52] West, D.B. 1996. Introduction to graph theory. Prentice Hall, Upper Saddle 
River, NJ.
[53] Wu, Y. 2006. A trellis connectivity analysis of random linear network coding with 
buffering. Pages 768-772 of: 2006 IEEE International Symposium on Information 
Theory.
[54] Yang, S., and Yeung, R.W. 2011. Coding for a network coded fountain. Pages 
2647-2651 of: 2011 IEEE International Symposium on Information Theory 
Proceedings.
[55] Yang, S., and Yeung, R.W. 2014. Batched sparse codes. IEEE Trans. Inform. 
Theory, 60(9), 5322-5346.
[56] Yang, S., and Yeung, R.W. 2017. BATS codes: theory and practice. Synth. Lect. 
Commun. Netw., 10(09), 1-226.
[57] Yang, S., Ho, S.-W., Meng, J., and Yang, E.-H. 2010. Linear operator channels 
over finite fields. CoRR, abs/1002.2293.
[58] Yeung, R.W. 2002. A first course in information theory. Information Technology: 
Transmission, Processing and Storage. Kluwer Academic/Plenum Publishers, 
New York.
[59] Yeung, R.W. 2008. Information theory and network coding, 1st ed. Springer, 
Boston, MA.
[60] Yeung, R.W. 2011. Network coding: a historical perspective. Proc. IEEE, 99(3), 
366-371.
[61] Yeung, R.W., Li, S.-Y. Robert, Cai, N., and Zhang, Z. 2006. Network coding 
theory. Found. Trends Commun. Inform. Theory, 2(4), 241-329.
https://doi.org/10.1017/9781009283403.006 Published online by Cambridge University Press

6
Coding for Distributed Storage
Over the past two decades a considerable effort has been made in developing 
coding techniques in connection with storing and retrieving information in a 
network environment, clearly an area of practical importance where significant 
impact of research may be realized. This chapter and the next four chapters are 
closely linked. This chapter considers the coding of information for storing 
on networked servers in such a manner that if a server fails a new one can be 
regenerated by downloading from other servers to restore the contents of the 
failed server. The aim will be to code the information stored on the servers 
(also referred to as nodes) in such a way that the amount of information stored 
on the servers and the total amount that needs to be downloaded in order to 
restore the failed server are, in a sense to be specified, minimized. Such coded 
systems can be shown to have substantial gains over simple replication of the 
server contents. The next section discusses the theoretical gains that might 
be realized from such a system and the subsequent section considers explicit 
coding systems to realize these gains.
The next chapter introduces codes that are locally repairable in the sense 
that an erasure in a codeword coordinate position can be restored (or interpo­
lated) by contacting a limited number of other coordinate positions. Here the 
term repairable refers to a coordinate position being erased as opposed to being 
in error, which is considered in the subsequent chapter on locally decodable 
codes (LDCs). Chapter 9 considers private information retrieval (PIR) codes. 
For these systems the information is coded and stored on servers in such a 
way that users can query the database and not have the servers obtain any 
information (in an information-theoretic sense) on which information is being 
sought. It will be shown there is an intimate connection with LDCs and PIR 
codes and this is explored in that chapter.
Finally, in Chapter 10, ways of encoding information on the servers in 
such a way that users retrieving information from them, no matter which 
157
https://doi.org/10.1017/9781009283403.007 Published online by Cambridge University Press

158
6 Coding for Distributed Storage
information is being sought, no server is required to download more than some 
fixed number of symbols. Such a code is referred to as a batch code. It is a load­
sharing type of coding ensuring that no server is overloaded with download 
requests, regardless of the information being requested.
As noted, these four topics are interrelated and all have been vigorously 
researched over the past two decades. The literature on them is now large and, 
in keeping with the purpose of this volume, these four chapters are merely 
intended to introduce the topics by providing some of the basic concepts and a 
few examples with pointers to the more general results available. Each chapter 
discusses some of their interrelationships.
For the present chapter the performance limits and techniques laid out in 
the seminal paper [5] have generated great interest in the research community 
addressing such problems and generated an unprecedented amount of research. 
An overview of this important work is given in the next section.
The number of coding techniques developed to achieve the performance 
limits given in Section 6.1 is very large. The use of Reed-Solomon codes for 
this problem is natural and recent work has shown that modifications of them 
can be very effective.
6.1 Performance Limits on Coding for Distributed Storage
It is difficult to overestimate the importance of the problem of storing and 
retrieving information from a network of servers to the global society. Most 
governments and large corporations maintain significant data centers and the 
cost of creating, powering, updating and maintaining them to manage this data 
is incalculable, not to mention the telecommunications infrastructure needed to 
use them. Further, the servers and memory devices used in these systems fail 
with some predictability and the failed units must be restored or replaced. The 
simple solution of replicating data on backup servers is not efficient and the 
role of coding has become central to such systems. In a sense it is remarkable 
that the application of coding-theoretic techniques to this problem did not arise 
sooner.
As noted, the paper [5] gives fundamental limits for the basic storage and 
repair models assumed here and is the starting point of most of the work in 
this area during the past decade. The purpose of the chapter is to discuss the 
approach of this paper and some of the subsequent work on the construction 
of codes for the models of interest. There will be two criteria of interest for 
such systems. The first is that of minimum storage of the coded system, the 
minimum of the total amount of storage for the file among all servers for the 
https://doi.org/10.1017/9781009283403.007 Published online by Cambridge University Press

6.1 Performance Limits on Coding for Distributed Storage
159
reliable operation of repairing a failed server, and the second one is minimum 
download bandwidth or repair bandwidth, the total amount of information that 
must be downloaded from adjacent servers to restore a failed server. Much 
of the notation in that influential paper is adapted with a view to making a 
full reading of that work (recommended) more amenable. There are numerous 
variations of this basic model considered in the literature.
The following basic model will be convenient for our purpose. Consider 
afileM of | M |= m symbols. The term symbol will be defined in various 
contexts and is left vague for the moment as is the term segment, meant to 
be some fixed number of symbols. It will at times be taken to be a single or 
multiple elements of a fixed finite field. These m file symbols are divided into k 
segments, each ofm/k symbols. One possibility then is that these segments are 
encoded with a code to give n coded segments and each such coded segment 
is stored on one of n storage nodes or servers (terms used interchangeably), 
one segment per node. For this chapter it will often be the case that a segment 
size will be the symbol (element) size of a finite field (hence field size) and 
thus may be of a very large order. A server then may hold either a single or 
multiple finite field elements. A single finite field element may also be regarded 
as several subfield elements. The work [5] uses bits rather than symbols. When 
a node fails, it will be assumed the coding is such that the entire file M can 
be regenerated by downloading the contents of any k of the surviving servers. 
It is established that while this model is good in terms of the total amount of 
storage required, it may not be good in terms of the number of symbols that 
must be downloaded in order to repair or regenerate a failed server, the repair 
bandwidth, in the sense that k segments must be downloaded to repair just one 
segment (in the failed server). Indeed it is shown that if each of n servers is 
able to store slightly more than m/k symbols, this repair bandwidth can be 
significantly decreased.
As above, assume the original file has |M |= m symbols, and a symbol is 
considered an element of a finite field Fq for some prime power order q .The 
file is divided into k segments of m/k = a finite field symbols each. These 
segments are encoded into n segments by an (n, k) code (possibly over an 
extension field of order the size of segments) and the n segments are stored on 
n servers, i.e., each of the n servers is able to store a finite field symbols, one 
segment. Much of the research assumes an (n,k,dmin)q MDS code for which 
dmin = n - k + 1. For this chapter the parameter d will indicate the number 
of helper nodes approached to regenerate the contents of a failed node, rather 
than code minimum distance which will be specified as dmin . Thus, if a node 
fails, a replacement node can contact any d surviving nodes and download 
P < a symbols from each of them. The repair bandwidth is the total number 
https://doi.org/10.1017/9781009283403.007 Published online by Cambridge University Press

160
6 Coding for Distributed Storage
of symbols downloaded to this replacement node, y = dfi. Such a model will 
be described as an (n,k,d,a,y)-tuple. A code will be referred to as feasible if 
there exists a coding system with storage a symbols in each node and repair 
bandwidth y symbols, each of the d helper nodes downloading p symbols for 
the regeneration process. The literature contains numerous other models with 
differing meaning of the parameters. The following captures the intent of the 
model of interest here.
For future reference it will be convenient to record these parameters:
M : original file of m Fq elements (= symbols)
m 
: |M |, size of original file in Fq symbols
k : information in any k servers is sufficient to restore original file
n 
: number of active servers in the system at any time
d 
: number of surviving servers contacted by new node to regenerate
a 
: the number of symbols stored in each server = m/k
p 
: the number of symbols downloaded from each of the d
helper nodes contacted for the regeneration process
y 
: the bandwidth of the regeneration process = total number of
symbols downloaded = dp
In the literature of coding for distributed storage, different repair strategies 
[5, 6] are considered. In exact repair the contents of the failed server are exactly 
reproduced. In functional repair the contents of the restored server may differ 
from the original as long as the overall MDS property of the information is 
maintained, i.e., the ability of using the contents of any k of the n servers 
to reconstruct the original file. In functional repair, e.g., the reconstructed 
contents of the new server may simply be a scalar multiple of the original 
contents. In some works systematic exact repair of the systematic information 
is exact while the parity information may be functionally repaired. Only exact 
repair is considered in this work.
For each set of parameters (n,k,d,a,y) it is of interest to determine con­
ditions and parameter sets corresponding to feasible solutions to the problem, 
i.e., conditions which guarantee the existence of feasible codes. Furthermore 
it is of interest to determine coding schemes that correspond to minimum 
storage regeneration (MSR) codes and minimum bandwidth regeneration 
(MBR) codes. The following technique is developed [5] to determine such 
conditions.
The notion of an information flow graph is introduced to tackle the 
problem. The idea, borrowing from techniques in network coding, is that the 
replacement node will be able to repair the information lost by a server failure 
with the parameter set posed, only if the associated information flow graph 
https://doi.org/10.1017/9781009283403.007 Published online by Cambridge University Press

6.1 Performance Limits on Coding for Distributed Storage
161
has a sufficiently large min-cut with an accompanying max-flow. Thus the 
server content reconstruction problem can be viewed as the flow problem of 
a multicast network considered in Chapter 5. A description of the problem 
formulation and solution is given with reference to [5] for details.
The process is described. Initially assume a source node S containing the 
entire file of |M|= m Fq symbols. Assume the k segments, each of size m/k 
symbols, are encoded into n segments according to some (n, k)q code (which 
is not yet specified - the construction of such codes will be of interest for 
suitable parameters). Each of these n segments will be stored in one of the n 
servers - there aren servers active at any given time. Designate the n servers as 
x(i),i = 1, 2,...,n- except that each server will be viewed as a pair of nodes 
x(i) 
(i) 
(i) 
(i)
in and xout . For each node the edge between xin and xout ,i = 1 , 2,...,n is 
assumed to have a transmission capacity of a symbols, the capacity assumed 
of each server. By definition of the assumed code, the contents of any k of the 
servers is sufficient to reconstruct the original file. However, only the contents 
of the failed server are to be reconstructed and it will be assumed d < k, i.e., it 
is sufficient to contact d helper nodes to reconstruct the contents of the failed 
node. The edges between the source S and the n active server input nodes 
xin are assumed to have infinite capacity. The edges between each node xout 
and the sink node, called data collector node DC, or terminal node T which 
becomes the regenerated node at the end of the process, are also assumed to 
have infinite capacity at this stage. None of the edges of infinite capacity will 
be involved in the graph cuts noted.
After initially loading the n servers, the source node becomes inactive. 
Assume there have been multiple node failures which have been repaired 
(exactly) so there remain at any time n active nodes. Assume for simplicity 
by renaming reconstructed nodes, at some point in the process, the currently 
active nodes are x(1),x(2),...,x(n) (and each such node is actually an (input, 
output) pair with a single edge of capacity a between the input node and output 
node). When a node fails, a new data collector node DC is formed which 
downloads p symbols from each node of some set of d active nodes (a total 
of y = dp symbols downloaded) and from these dp symbols the contents of 
the failed node can be reconstructed. Assume the d helper nodes involved are 
x(i1),...,x(id).
It is desired to determine the capacity of any min-cut of the resulting flow 
graph, noting that the only edges in the cut C are those of the form either 
xin to xout (of capacity a) or xout to xin ,i = j (of capacity p ), either from a 
helper node to helper node or non-helper node to helper node. Each active node 
contains a symbols. As mentioned, the capacity of the edges from the original 
source to an active node and from an active helper node to the terminal node 
https://doi.org/10.1017/9781009283403.007 Published online by Cambridge University Press

162
6 Coding for Distributed Storage
(data collector DC) have infinite capacity and are not included in the min-cut 
computation.
The argument to determine the capacity of a min-cut is as follows [5]. 
Consider a min-cut of the signal flow graph representing the current server 
storage configuration from the original source S and the current data collector 
DC (i.e., reconstruction node) with the restriction that the min-cut does not 
include paths to/from source or terminal DC. The source S initially loads 
information into n servers and after that plays no role in the argument. When a 
server fails a new data collector node DC is formed and connects to d servers 
downloading p symbols from each in order to regenerate the failed server. The 
links between the DC and servers are assumed to have infinite capacity and 
are not part of any cut considered below. After completion of the process the 
DC becomes the new active server, replacing the failed one. The similarity of 
the situation for network coding in Chapter 5 is immediate although it may 
not be clear how this formulation solves the regeneration problem. Recall the 
max-flow min-cut theorem of Chapter 5:
Theorem 6.1 (Max-Flow Min-Cut Theorem) In any flow network with source 
S and terminal node T = DC, the value of the maximum (S, T) flow is equal 
to the capacity of the (S, T ) min-cut.
The point of the argument is to realize the value of the max-flow through 
the min-cut must be at least m, the size of the file, in order for a file or server 
content to be regenerated. The repair bandwidth (the total number of symbols 
downloaded to regenerate the contents of a failed node) is y = dP•
For a given system parameter set it can be shown the minimum amount of 
storage needed for each server, a, must satisfy
min{ d,k }-1 
,, 
,
m < 
^2 min 1---- I y,a 
(6.1)
i=0
or correspondingly
min{d,k}—1
min-cut > 
min {(d — i)P,a}.
i=0
Recall that while the contents of any k servers can be used to reconstruct the 
entire file, only p symbols are downloaded from each of the d helper nodes 
from which the contents of the failed server are reconstructed. Also note that 
if d<kthe above equation implies that any d nodes will have a flow of m in 
which case, by the assumptions made, it would imply d = k. Thus later on it 
https://doi.org/10.1017/9781009283403.007 Published online by Cambridge University Press

6.1 Performance Limits on Coding for Distributed Storage
163
will be assumed d > k. The minimum storage problem [5] is then, for a given 
d,Y = dp, to determine the smallest server storage a, i.e.,
min{d,k }-1 
,, 
. , 
,
a * (d,Y) = min a subject to 
min 1 — y,a > m. (6.2)
d 
i=0
For this reason the quantity a is often referred to as the capacity for the 
codes. The argument below will apply to any signal flow graph. It is not 
difficult to construct a signal flow graph which reflects the above equation 
although it seems difficult to display such a graph in a useful manner. Such 
a graph appears in [5] and ([7], chapter by Liu and Oggier) who also give an 
argument as to how Equation 6.1 or 6.2 arises.
Consider a min-cut between the source S and data collector DC, using no 
edges emanating from either S or DC. All edges between nodes have capacity 
p and those from the input to output within a node capacity a . Define the cut C 
by the disjoint vertex sets of the n active nodes, U,V such that U U V contains 
(i) 
(i) 
( ) 
( )
a te vertces xin , xout , = , , . ..,n. or te rst eper noe xin , xout , 
if xi(n1) is in U, then the edge (of capacity a ) between xi(n1) and xo(1u)t is in the 
cut C since the edge between xo(1u)t and DC is not in the cut, by definition. If 
x(n) e V, then the d input edges of x(n) are in C and it is clear the first helper 
node contributes min{d,a)P to the sum.
() 
() 
() 
()
or te secon eper noe xin , xout 
xin e , ten xout must e n
(2)
and the edge between the input and output parts is in the cut. If xin e V, then 
xo(2u)t is also and at least d - 1 edges, each of capacity p must be in the cut since 
there could be an edge between xo(2u)t and x(1) . Hence the contribution of the 
out in
helper node to the sum is at least min{(d - 1)p, a}. The argument continues to 
give the sum in Equation 6.1 [5, 7].
A failed server cannot be regenerated if the min-cut is smaller than the 
original file size m and that if the min-cut of the signal flow graph does satisfy 
the above equation, then there will exist ([5], proposition 1), for a sufficiently 
large field, a linear code that will allow regeneration. Also ([5], lemma 2) there 
will exist a signal flow graph for coding parameters (n,k,d,a,y) if the above 
bound is met with equality.
It is of interest to determine all points on the (a,y) plane that satisfy the 
above Equations 6.1 and 6.2. It is noted ([7], chapter by Liu and Oggier) that 
for given values of a, y = dp and d, the inequality of Equation 6.1 can be 
uniquely determined in a closed-form equation. Before giving this solution 
note first that if a < (1 — (k — 1 )/d]y, then a is less than each term in the sum 
and hence Equation 6.1 yields
m = ka
https://doi.org/10.1017/9781009283403.007 Published online by Cambridge University Press

164
6 Coding for Distributed Storage
and hence
ad m d
Y - d - (k - 1) = k ' d - (k - 1) ’
It is clear that this corresponds to a minimum storage code since we have the 
restriction that the contents of any k nodes can be used to regenerate the entire 
file, each node must store at least m/k symbols. Thus a code achieving this 
storage and download point (a, y) is referred to as the minimum storage repair 
(MSR) bound and designated
(aMSR,YMSR) = ( -T’7T7 
k k(d
md
- k + 1)
MSR bound. (6.3)
For the minimum storage solution the minimum download is achieved with 
d = n - 1, i.e., when a node fails, the minimum download is achieved by 
downloading from all remaining active nodes.
At the other end of the storage range suppose a - y and hence the larger 
term for each term of the summation, and consider the solution to Equation 
6.1, i.e.,
min{d,k}-1
m = E 
min{(1 - d)y,a}
i=0
min{ d,k}-1 
, 
x 
,
— V 1 i 
k 
k(k-1) ) — k 1 k-1
= 
u - d Y = Y k-----— ) = kY 1 - —
i=0
and in this case
2md
Y = ------------------- .
k(2d - (k- 1))
MBR bound.
(aMBR,yMBR) =
Since this derivation assumed a - y the minimum storage possible for this 
minimum download y is a = y and the corresponding bound is referred to the 
minimum bandwidth repair (MBR) bound and the code with these parameters 
is referred to as an MBR code and designated:
2md 
2md
k(2d - (k - 1)), k(^2d - (k - 1))
(6.4)
Notice that since the two terms of this relation are the same, i.e., aMBR = 
yMBR for the MBR regime, the amount of download is minimized since the 
contents of each server are a symbols. The expression for a (and hence y) 
is minimized with d = n - 1, i.e., as noted [5] even though the number 
of nodes from which information is downloaded from increases, the amount 
downloaded from each node is less and the product y = dfl decreases.
For the case between the two extremes of a < (1 - (k - 1 )/d)Y 
and a - y , discussed above, the situation is slightly more complicated 
https://doi.org/10.1017/9781009283403.007 Published online by Cambridge University Press

6.1 Performance Limits on Coding for Distributed Storage
165
but straightforward - the solution is given in the following theorem (see 
appendix of [5]).
Theorem 6.2 ([5], theorem 1) For any a > a * (n,k,d,Y) the points 
(n,k,d,a,Y) are feasible and linear network codes suffice to achieve them. It is 
information theoretically impossible to achieve points with a < a * (n,k,d,Y). 
The threshold function a * (n,k,d,Y) is the following:
a * (n,k,d,Y ) =
m
T’
m - g(i)Y 
k-i
Y e [f(0), + rc)
Y e [f(i),f(i - 1))
(6.5)
where
f(i) =
2md
(2k - i - 1 )i + 2k(d - k + 1)
g(i) = (2d - 2k + i + 1 )i 
2d
where d < n - 1. For d,n,k given, the minimum repair bandwidth y is
YMBR = f(k- 1) =
2md
2kd - k2 + k
(6.6)
Good discussions of the signal flow graph and the related issues are 
contained in the references [5, 6, 7, 14].
Over the past decade the search for coding and storage schemes that 
approach the bounds of Equations 6.3 and 6.4 has been intense with contri­
butions by a very large number of researchers.
The quantity a*(n,k,d,Y) will be referred to as a* (d, Y) when n and k 
are understood. It is clear that a* (d, Y) is a decreasing function of d since 
accessing more nodes could result in lower storage per node.
In the case where d = n - 1, the case where the replacement node contacts 
all other active nodes, the previous MSR bound yields the point
amin min 
aMSR,YMSR
m m n-1 
k,k n-k
(6.7)
This regime requires a factor of n-1j more download than each node stores 
which is a fundamental requirement of this regime, achieved by MDS codes.
Similarly for the MBR regime, Equation 6.4 for the extreme case where 
d = n - 1 the minimum is given by
2 (n - 1) m
2 n - k + 1 ,~k
2 (n - 1)
2 n - k - 1
(6.8)
https://doi.org/10.1017/9781009283403.007 Published online by Cambridge University Press

166
6 Coding for Distributed Storage
Thus, as expected, the MBR case requires slightly less download bandwidth 
compared to the MSR case but requires more storage for each node.
The parameter a is often referred to as the subpacketization level taken 
in the following sense. If a = 1, then one finite field symbol is stored in 
each node. For many cases it is possible to construct more efficient codes 
by using a smaller field than the symbol field size and storing more than one 
subfield element in each node, giving more flexibility for coding options. More 
comments on this issue are discussed in the next section. The parameter p, the 
amount transferred by a helper node to a regenerating node, is often much 
smaller than a, the amount stored in each node ([12], chapter by Ramkumar 
et al., section 31.2.5 and also [7], chapter by Liu and Oggier, section 2.2). 
These references also contain interesting observations on the subtleties of the 
achievability of the bounds discussed above for functional repair as opposed to 
exact repair. In particular, constructions of exact repair MSR and MBR pairs 
(a,Y) can always be constructed. Other pairs, referred to as interior points, 
which do not correspond to MSR or MBR points, may not be possible to 
achieve.
The next two sections demonstrate aspects of a few of the many coding 
schemes that have been reported in the literature for these cases. Although 
the sections are necessarily limited in scope, they are indicative of the large 
number of results available.
Section 6.2 considers the problem of constructing codes for distributed 
storage that either meet various criteria or achieve the MBR or MSR cut-set 
bounds. The final section of the chapter considers array-type constructions 
of such codes. The distinction between the two types is artificial. In addition 
the particular constructions chosen for these sections from the vast array of 
possibilities are arbitrary, being mainly the ones the author was familiar with 
or found interesting. They tend to be from the more recent literature and 
by authors who have contributed significantly to the field. The selection of 
construction techniques chosen by another author might well be disjoint from 
those given here.
The next chapter considers the construction of locally repairable codes 
(LRCs). These codes are erasure-correcting codes which are able to cor­
rect an erased coordinate in a codeword by accessing a limited number 
of other nonerased coordinate positions. While they represent a separate 
research effort from the distributed coding techniques of this chapter it 
appears to be appropriate to study them in this context. The constructions 
of such codes tend to involve intricate uses of finite field and combinatorial 
structures.
https://doi.org/10.1017/9781009283403.007 Published online by Cambridge University Press

6.2 Regenerating Codes for Distributed Storage and Subpacketization 167
6.2 Regenerating Codes for Distributed Storage 
and Subpacketization
This section describes a few of the numerous approaches to constructing 
codes for distributed storage. The MSR or MBR cut-set bounds which for 
convenience are repeated as:
(aMSR,YMSR)
(aMBR,YMBR)
m md
k’k(d - k + 1) ,
2md 
2md
k(2d - (k - 1)) , k(2d - (k - 1))
(6.9)
The aim of the work is to construct codes that either achieve one of the bounds 
or whose parameters approach the bound as well as describe the repair process. 
Of course, codes whose parameters lie in between these bounds can also be of 
interest.
Recall the model from the previous section: An information file of m finite 
field Fq symbols is encoded in some manner and a Fq symbols are stored on 
each of n servers in such a way that the information on any of the k servers is 
sufficient to regenerate the entire file.
In many of the coding schemes in the literature this model is specialized by 
using a finite field Fq and an extension field of order I, Fq<. The original data 
file consists of m = k, Fq< symbols which are encoded using a linear (over 
Fq<) (n,k,dmin)qt code and one Fq< symbol is stored on each of n servers. 
The one Fq< symbol can be viewed as I Fq symbols and some subset of p 
of these can be downloaded from each of the d servers contacted during the 
regenerating process. For obvious reasons the parameter I is referred to as the 
subpacketization level. Such a code is termed an (n,k,d,q,a,y) code. Recall 
the parameter d in this work denotes the number of helper nodes contacted 
for repair or the download quantity in terms of Fq symbols. When the code 
minimum distance is required it will be referred to as dmin explicitly.
Only the case of a single node failure and exact repair is considered 
here. Other works consider multiple node failures and cooperative repair. 
It is natural, from the reconstruction requirement, to consider MDS codes, 
(n,k,dmin)qt codes for which the minimum distance is dmin = n - k + 1. This 
might often refer to Reed-Solomon codes but variations are often considered.
As a matter of notation a few standard observations on codes are noted. Let 
Fq be a subfield of the finite field Fq<. Consider an C' = (n,k,dmin)qt MDS 
code over Fq<. Such a code is referred to as a scalar code (linear over Fq). 
For c e C',c = (c1,c2,... ,cn), ci e Fqt, one can replace each coordinate 
https://doi.org/10.1017/9781009283403.007 Published online by Cambridge University Press

168
6 Coding for Distributed Storage
element ci by a column £-tuple over Fq to form the code C which might be 
viewed as a C = (nt,kt\ code. Such a code can be referred to as a vector 
code over Fq or array code since a codeword can be represented as an £ x n 
array ofFq symbols. The notions of MSR and MBR bounds also apply to such 
a model.
It is convenient to reproduce some coding results from Chapter 1.Asin 
Chapter 1, a standard construction of Reed-Solomon codes over any Fq is 
as follows. Let u = {u1,u2,. ..,un} be an evaluation set of n < q distinct 
evaluation elements of F q and F <k c F q [ x 1,.. .,xn ] be the set of polynomials 
over Fq of degree less than k. Then one incarnation of a Reed-Solomon code 
can be taken as
RSn,k(u,q) = {<y = (f(u 1 ),f(u 2),..., f(un)),f e F <^.
The code is MDS and the dual of such a code is also MDS. While the RS code 
always contains the all-ones vector (since f(x) = 1 is a valid polynomial) the 
dual code in general does not and the dual code is not generally an RS code, 
although it is an MDS code. Of course the construction is valid for any finite 
field, e.g., Fq.
From Chapter 1, a generalization of this code is the Generalized Reed- 
Solomon (GRS) code denoted GRSn,k(u,v,q), where u is the evaluation set 
of distinct field elements as above and v = {v 1 ,v2,..., vn}, vi e Fq is the 
multiplier set of nonzero elements. The GRSn,k(u,v,q) is the (linear) set of 
codewords:
GRSn,k(u,v,q) = f = (v1f (u1),v2f (u2), . .. ,vnf (un)), f e Fq<k .
An RS code is a GRSn,k(u,v,q) code with v = (1, 1,...,1).
The dual of a GRS code is also GRS. In particular, given GRSn,k(u,v,q) 
with evaluation set u and multiplier set v there exists a multiplier set w e (F* )n 
such that
GRSntk(u, v,q) = GRSn,n-k(u, W,q)
= (w1f (u1),w2f (u2)...,wnf (un), f e Fq<(n-k) .
Note in particular that
RSn,k(u,q) = GRSn,k(u, w,q)
for some multiplier set w.
https://doi.org/10.1017/9781009283403.007 Published online by Cambridge University Press

6.2 Regenerating Codes for Distributed Storage and Subpacketization 169
It was noted in Chapter 1 that if ^ = {^ 1,..., ^n} is a basis of Fqn over Fq 
and {v 1,... ,vn} is a trace dual basis for the basis, then
n 
n
y ^^2’’i'i 
Z2TrqnIq(yvi)^i. 
(6.10)
Thus an element y e Fqn can be represented by the traces Trqn ।q(yvj) e 
Fq,j= 1,2,...,n. This will be of importance for the code constructions to 
follow.
For the remainder of the section a few constructions of codes for distributed 
storage are considered. As noted, the number of these that can be found in the 
literature is very large and many important constructions are not mentioned 
here.
To initiate the discussion, two explicit constructions of codes that achieve 
either the MSR or MBR bounds are mentioned to show that there are simple 
schemes for both regimes ([7], chapter by Liu and Oggier).
Consider afileM of IMI= m = k Fq symbols. Thek Fq symbols are MDS 
encoded into n Fq symbols and each symbol is stored on one of the n servers. 
To restore a failed server, the replacement node could contact k other servers 
and download their contents to restore the entire file (by definition).This is 
clearly wasteful since it involves the download of k symbols to restore a single 
symbol. However, note that this is an MSR solution since d = k, ft = 1, y = 
dft = k, a = 1 and hence by Equation 6.9
(aMSR,YMSR) = ( 
—r~77
k k(d - k + 1)
To show a simple scheme that achieves the MBR parameters consider the 
following [15]. Consider only a single node failure. For this case the data file 
M consists ofm = k(k + 1) Fq symbols divided into (k + 1) data segments, 
f (i),i = 0, 1,...,k, each segment containing k Fq symbols. There are n = k+ 
1 nodes and each node will contain 2k Fq symbols in the following manner. Let 
g(i),i = 1, 2,...,kbe k linearly independent vectors over Fq (k-tuples in Fkq). 
The node i stores the data segment of k Fq symbols f (i) and the k inner prod­
uct Fq symbols {(f (i+1),g(1)), (f (i+2),g(2)),...,(f (i+k),g(k))} (superscripts 
taken mod (k + 1)). Suppose node n* has failed. The regenerating node con­
tacts the k other nodes and downloads sufficient information to generate f(* * 
and the inner products (f(n*+j),g(j)), j = 1, 2,...,k, superscripts taken 
mod (k + 1). From each node it downloads 2 Fq symbols. From node i = 
n* it downloads the two symbols (f (n*),g(n*-i)) and (f (i),g(i+(n-n*))) = 
(f(n*+j),g(j)), j = 1, 2,...,k. The segment f (n*) is reconstructed by a simple 
= (1,k).
https://doi.org/10.1017/9781009283403.007 Published online by Cambridge University Press

170
6 Coding for Distributed Storage
matrix equation since the vectors g(i) are linearly independent and the other k 
symbols are as required.
The system parameters are n = (k + 1), d = k, and m = k(k + 1 ),a = 2k 
and hence a = y = 2k. From Equation 6.9
2md 
2k2(k + 1)
aMBR =-----------------=-------------- = 2 k and
MBR 
k(2d-k+ 1) 
k(k+ 1)
2md 
2k2(k + 1)
Ymbr =--------------------=--------------= 2 k
MBR 
k(2d - (k- 1)) 
k(k+ 1)
and it follows the system described achieves the MBR parameters.
The construction contained in the work of Guruswami and Wootters [8, 9, 
10] uses the trace representation of field elements as in Equation 6.10 and has 
been influential in generating a considerable number of further constructions. 
The basic construction of this work is described using also the approach in [4].
Let M consist of m = k Fq symbols which are encoded by an RSn,k(u,qe-') 
code into n Fq< symbols, with the n distinct nonzero evaluation set elements 
u = {u 1 ,u2,... ,un}. Assume f(x) e Fq[x] is of degree less than k is used 
for encoding the database, i.e., the coded word is (f(u1), f(u2),...,f(un)) 
where the original data is the set of k coefficients of f(x) . Each such symbol 
f (ui) e Fq is stored in one of n servers which for convenience are labeled 
with the corresponding evaluation set elements ui. These Fq symbols are 
viewed as £ Fq symbols. Let r = n - k denote the redundancy of the code. 
The dual of the code is a GRSn,n-k(u^w,q^') code, for some multiplier set 
w, whose codewords can be viewed as being generated by (and identified by) 
polynomials over Fq of degree less than n — k. Thus for an evaluation set 
u e Fnt and for polynomials f (x) and g(x) of degrees less than k and n — k, 
q
respectively, it follows that
f (ui)wig(ui) = 0.
ui eu
It can be shown that when n =| Fq |= qe then the multiplier set is wi = 1Vi. 
It will be convenient to make this assumption regardless of the size of the 
evaluation set since evaluation of a code component wi g(ui) is equivalent to 
finding the value g(ui).
Suppose f (x) e Fq< [x] of degree at most k — 1 corresponds to a codeword 
in the MDS (n,k,dmin)q code C and g(x) e Fq[x] of degree at most n — 
k — 1 = r — 1, in C± and that (assuming a multiplier set of unity elements for 
convenience)
n
f(ui)g(ui) = 0. 
(6.11)
i=1
https://doi.org/10.1017/9781009283403.007 Published online by Cambridge University Press

6.2 Regenerating Codes for Distributed Storage and Subpacketization 171
Assume r = n - k > q€-1 and consider the polynomial g(x) e Fq< [x] as 
a codeword in the dual code and let u * correspond to the failed server (also 
codeword position or evaluation point).
As noted in Equation 6.10, for a basis a = {a 1 ,a2,... ,a^} of F q over F q 
and trace dual basis p = {p 1 ,p2,.. . p} of Fq< over Fq, any element a e Fq 
can be expressed as
a = ETrqlIq^*• 
(6.12)
For the basis a = {a 1 ,...,&£} of Fq over Fq and evaluation set 
u = {u1,u2,...,un}, u* e u has failed and the contents f (u*) are to be regen­
erated by contacting the (n - 1) other servers. Here f e Fq [x], deg(f) < 
k, as noted above, is some fixed polynomial used in the storage system, 
corresponding to the codeword in use. The strategy will be to define a suitable 
set of polynomials g e Fq [x], deg g < n — k, associated with codewords in 
the dual code, so the orthogonality condition in Equation 6.11 is satisfied.
Define the polynomials
gi(x) = Trq<।q(ai(x — u*)) /(x — u*), i = 1,2,...,t, 
u e u
= E (ctixx — u*^q /x — u*) 
j=0
= £ aq (x — u*)qj—1
j=0
(6.13)
and
(x — u * )gi(x) 
j — u *) qj = Tr q । q(ai(u — u *)).
j=0
(Note that (x — u *) divides each term of the trace function — hence the result 
gi(x') is a polynomial.) It is clear that each of these I polynomials has the 
properties that
deg(gi(x) = qe —1 — 1 and gi(u*) = ai, i = 1,2,...,£.
Assuming r — 1 > qe—1 — 1 these £ polynomials can be viewed as generators of 
codewords in the dual code to the RSn,k(u,qe-') code (GRSn,n—k(u, w, Fqe)) and 
hence for each such polynomial for f e F<k [x] (assuming unit multipliers)
n __
fuf(u/)gi(=j) = 0 or gi(u*)f(u*) 
^E gi(u)f(u), i = 1,2, ...,£.
https://doi.org/10.1017/9781009283403.007 Published online by Cambridge University Press

172
6 Coding for Distributed Storage
Since the data file of k symbols over Fqt was RS encoded with the polynomial 
f(x') e F<k[x], these I polynomials gi(x) give conditions which allow 
q
regeneration of the contents of the failed server in the following manner.
Taking traces of the above equation gives
Trq£।q(gi(u*)f(u*)) = - £ Trqt|q(gi(u)f(u)), i = 1,2,...,£.
u e u\{ u *}
From the above properties of the polynomials gi (x) and the linearity property 
of the trace function, and substituting Equation 6.13 for gi (u), this can be 
expressed as
Trq।q(fu*)) = - E Trq।qa(u-u*)Krq^।qGuf—u)*)), 
u-u 
ueu\{u*}
i 1, ^2>, ..., .
Each server is able to compute the first trace in the summation Trqt ।q(ai (u - 
u*) . Thus if the server corresponding to evaluation element u e u\{u*} 
transmits the quantity
Trq | q
f(u) 
u — u *
e Fq,
(for a total of (n-1) Fq elements), the regenerating server is able to reconstruct 
the contents of the failed server via Equation 6.12, i.e.,
_L
f(u *) = E Tr q I q(lxif(u * ))Pi.
This method of reconstruction thus requires each of n - 1 servers to download 
a single Fq element in order to reconstruct a single Fqt. element (or £ Fq 
elements), as opposed to each server downloading a single element of Fqt 
previously. This construction (from [4]) is a convenient description of the 
theorem 4 and algorithm 1 of [10], an example of the deeper results of 
that work.
This approach of linear repair with the use of trace functions appears to have 
inspired numerous other works. The optimality of the approach is discussed in 
[4, 8, 10]. In particular, since n is typically quite large in practice and the 
restriction of the argument that r > q£ — 1, the level of subpacketization £ 
can be large and numerous efforts have been reported to reduce this level. In 
particular a large number of (n,k)qt coding schemes with subpacketization £ 
are available for a wide variety of parameter sets in the literature.
A novel and interesting variation of the above construction is given in [18] 
and the elements of it are discussed, preserving as far as possible the notation. 
The technique involves the distribution of primes and has the feature that the 
https://doi.org/10.1017/9781009283403.007 Published online by Cambridge University Press

6.2 Regenerating Codes for Distributed Storage and Subpacketization 173
number of helper nodes required to repair a node depends on the node. An 
overview of it is given.
Let n(r) be the number of primes less than or equal to r, a well- 
studied function in number theory, and if n(r) = m let the primes be 
p1,p2,...,pm. Let
m
n = n Pi and ni = n/pi.
Several fields are defined. Let q > n - m, F = Fq and Fi = Fqni and E = Fqn 
so that [E . Fi ] = pi. Let Fqpi = F (ai), ai e E and
E = Fqn = Fq(a 1, .. .,am).
Adjoin to the set of above elements {a1,a2,...,am} a further set of n - m 
distinct elements of E and let a = {a 1 ,a2,... ,an} be an evaluation set for 
an RS code C over E of dimension k and C± its dual (GRS) code, say with 
multiplier set v. The work shows that assuming the i-th node has failed, for 
i = 1,2,...,m (i.e., one of the first m nodes) its contents can be optimally 
repaired from any di = Pi + k - 1 helper nodes.
To see this choose a subset Rii of di helper nodes, Rii c [n]\{i} and note 
that |di |< n — 1. Define h(x) to be the annihilator polynomial
h(x) = 
(x - aj)
and note the degree of h(x) is n - (k + Pi) and so deg(xsh(x)) < r for 
5 = 0,1,... ,pi — 1. Thus the codeword in C± (multiplier set v) corresponding 
to such a polynomial is
1 afh(a 1),... ,vnasnh(an)^ e C±, s = 0,1, ...,pi — 1.
Suppose the polynomial f (x), deg(f (x)) < k corresponds to the codeword 
(f(a1),...,f(an)) e C used for the distributed data storage system. Then, by 
definition
n
vjajsh(aj)f(aj) = 0,s= 0, 1,...,Pi — 1. 
j=1
The argument is as in [10]. Taking traces of this expression (E over Fi ) and 
rearranging terms gives
TrE/Fi viaish(ai)f(ai) =— 
ajs TrE/Fi (vj h(aj)f (aj)).
https://doi.org/10.1017/9781009283403.007 Published online by Cambridge University Press

174
6 Coding for Distributed Storage
(Note that for j = i, aj e Fi.) Thus the j-th helper node downloads the single 
Fi element to the i-th node:
TrE/Fi (vj h(aj)f (aj)).
The elements {v1h(a1),viaih((ai),...,viaipi-1h(ai)} form a basis of E over 
F i and let 0 1,0 2,... ,0Pi-1 be the trace dual basis. As before the i -th node then 
is able to compute
Ps -1
f(ai) = 
TrE/Fi (viash(ai)f (ai))0s
s=0
to restore the node. It is argued [18] that this scheme attains the cut-set bound 
of Equation 6.9 (or Equation 6.14). Further developments of this technique 
with linear combining and trace functions primes are considered in [19, 23].
Another interesting construction considered here is that ofYe and Barg [22]. 
Itis relatively simple and representative. The work of Guruswami and Wootters 
[8] with a similar approach is also of interest.
As in the previous example, it will be assumed the file consists of k Fq 
symbols MDS encoded into n Fq symbols, each stored on a separate server. 
When a node fails, a replacement node appears and must restore its contents by 
downloading information from the remaining n - 1 (or fewer) nodes. Interest 
will only be in the case of exact regeneration and using all n - 1 surviving 
nodes. From Equation 6.9 if download for repair is obtained from all (n - 1) 
nodes, the repair bandwidth of a replacement node must be at least
m(n - 1) t(n - 1)
. 
k(n - k) (n - k)
(6.14)
when each node is viewed as containing a single symbol of a field Fq or I Fq 
symbols.
6.3 Array-Type Constructions of Regenerating Codes
As noted earlier, an (n,k)q, r = (n - k), array code C is constructed as 
follows. A codeword C e C will be of the form C = (C1,C2,...,Cn) where 
each component Ci is an £ column vector over Fq. Many works in the literature 
refer to this as an (n,k,k)q code. Since this might be confused with the more 
common (n,k,dmin)q the preference here will be to state explicitly it is an 
(n,k)q code with subpacketization level I. The usual RS code over q£ is an 
example of such a code. Another is the code construction below. While quite 
https://doi.org/10.1017/9781009283403.007 Published online by Cambridge University Press

6.3 Array-Type Constructions of Regenerating Codes
175
intricate in its requirements the manner in which it achieves reconstruction is 
of interest.
The code is defined as:{
n 
a
(C1,C2,...,Cn) : = As,iCi =0,s= 1,2,...,r (6.15)
where {As, i} is a set of rn £ x I matrices, thought of as an r x n array of I x I 
matrices, over Fq and
Ci = (ci,0,ci, 1, ...,ci,£-1 )t, ci,j G Fq.
The several code constructions in [22] differ in their choice of these matrices.
For the construction of interest here ([22], section III) choose
As,i = Ais-1,s G [r],iG [n]
for diagonal matrices Ais-1.
The redundancy of the storage scheme is r = n - k and assume q>rn 
and let I = rn and note the exponential dependence of I on r. Each coordinate 
of the array code is a column vector of length I over Fq. Each integer a in the 
range {0,1,2,... ,£ -1} can be expressed (represented) as an r-ary n-tuple: a = 
(an,an-1,...,ai,...,a1) where each ai is an integer in the set {0, 1,2,...,r- 
1}, i.e.,
n
a = air ~ (an,an-1, .. . ,a 1), a = 0,1, . .. ,£—1 = rn — 1,0 < ai < r — 1, i G [n].
Thus as a runs through the integers 0 to rn - 1, the representation runs 
through all possible r-ary n-tuples. We will often not distinguish between the 
integer a and its r-ary vector representation in the sense that if a and its r- 
ary representation (a1,...,an) occur in an expression, it will be assumed they 
correspond.
Let {Xi,j,i = 1,2,... ,n,j = 0,1,... ,r — 1} be an array of rn distinct 
nonzero elements of F * (q > rn).
q
Let e(a),a = 0,1,... ,£ — 1 be the standard basis of column vectors of Fqt 
over Fq, i.e., e(a) contains a single 1 in the a-th position and all other elements 
are 0. Define the matrices above as:
r—1
Ai = ^2ki,aie(a')e(a)), 
i = 1,2,... ,n.
a=0
Thus
Ai = diag{ ki,a 0 ,ki,a 1 ,...,ki,at—1}, 
i = 1,2, ... ,n.
https://doi.org/10.1017/9781009283403.007 Published online by Cambridge University Press

176
6 Coding for Distributed Storage
These matrices are diagonal and the matrix e(a)e(a)t is an all-zero matrix except 
for a single 1 on the diagonal in the a-th row and column. The code defined by 
Equation 6.15 can then be defined as: C = (C1, C2,... ,Cn) e C, each Ci a 
column vector of dimension £ over Fq, if
(6.16)
The zeros in the right-hand column vector are vectors of all zeros of 
dimension £, and Ci is the column vector over Fq of dimension £, 
Ci = (ci, 0,ci, 1, . . . , ci, £ — 1) .
This is equivalent to the system of equations: 
n
^^c.sia.=,a = 0, s = 0,1,... ,r - 1, a = 0,1,...,£ — 1 
(6.17)
i=1
where the ai subscripts in the X’s in the summation correspond to the a in the 
ci, a .All £ x £ matrices are viewed as having rows and columns indexed by
where zeros indicate zero matrices/column vectors of appropriate dimension 
and the matrices Ba are r x n matrices and the a-th row of this equation is: 
1
1
1
c1,a
Ba • Ca
X1,a1
X2
X1,a1
X2,a2 • • • Xn,an
X2,a2 • • • Xn,an
c2,a
c3,a
Xr—1
X1,a1
r—1 
r—1
X2,a2 • • • Xn,an_
cn, a
(6.19)
0
0
0
a = 0, 1,...,£ — 1
and
Ca = (c 1 ,a,c 2,a,...,cn,a), 
a = 0, 1, . . . ,1 — 1 
https://doi.org/10.1017/9781009283403.007 Published online by Cambridge University Press

6.3 Array-Type Constructions of Regenerating Codes
177
and the equivalence between a and its r -ary expansion is noted. It follows 
directly from this equation that if r = n-k components of the original 
codeword C e C,C = (C 1 ,C2,...,Cn),Ci e Fq are erased, they can be 
reconstructed since the matrix in Equation 6.19 has the property that any r 
columns have full rank r . Thus the codes are MDS. For an equivalent view, 
consider the original codeword as an £ x n array of Fq elements. Equation 
6.19 then implies that each row of the array is a codeword in an RS code with 
evaluation set {X 1 ,a 1 ,c2,a2,... ,cn,an} and in this sense the MDS property of 
the overall code C is inherited from the RS/MDS property of its rows.
It is important to note that as the r -ary n-tuples a vary there will be 
repetition of elements and hence in any set such as {X1,a1,X2,a2,...,Xn,an} 
there will also be repetition. This is used below to effect a repair process. Also 
it is tacitly assumed for the matrices or vectors indexed by £ there is a fixed 
order - say lexicographic - to the n-tuples that need not be specifically noted.
To consider the repair process, suppose the i-th server fails and a replace­
ment server is to replace the data lost by downloading certain information 
from the remaining n - 1 nodes. It will be useful for the following discussion 
to be reminded of the structure of a codeword C = (C1, C2,...,Cn) where 
Ci = (ci, 0 ,ci, 1 ,...,ci,£-1 )t, Ci,j e F q, i = 1,2,... ,n and Ci e F qt ~ F q . 
The set of integer subscripts L = {0,1, ...,£ - 1} is associated in some 
one-to-one manner with the set of £ = rn n-tuples in the set Rn = {a = 
(an,an-1,...,a1), ai e R} where R ={0, 1,...,r- 1}. The mapping between 
the two sets is not specified.
It is clear the set Rn can be decomposed into rn-1 subsets Ri,i = 
1,2,...,rn-1, | Ri |= r so that within a set Ri the n-tuples differ only in 
the i-th component for i fixed. For an n-tuple a e Rn define
a(i,u) = (an,an-1,...,ai+1,u,ai-1,...,a1),
i.e., an n-tuple whose i-th component is replaced by a variable u. The variable 
u can assume the values u = 0, 1,...,r - 1. Clearly there are rn-1 such sets, 
corresponding to the possibilities of (n - 1)-tuples {an,...,ai+1,ai-1,... a1}.
Equation (6.17) is
n
YaclaSi^ = 0, s = 0,1,... ,r - 1,a = 0,1, ...,£ - 1. 
(6.20)
i=1
Assume node i fails and it is required to reconstruct the code column Ci 
by downloading information for the n - 1 other nodes, i.e., it is required 
to determine the codeword (ci,0,ci, 1,... ,ci,e-1) where in ci, j the equivalent 
https://doi.org/10.1017/9781009283403.007 Published online by Cambridge University Press

178
6 Coding for Distributed Storage
r-ary n-tuple is used. In the above equation choose a to be a(i, u) (leaving u 
as variable for the moment and noting that a(i,u) is a valid r-ary n-tuple):
n
ki,uci,a(i,u) + 
k j,ajcj,a(i,u) = 0.
j=i
Suppose the j-th node j = i computes the quantity
r — 1
j = E Cj,a(i,u), j e [n]\{i} 
(6.21)
u=0
which is possible since it contains the j-th coordinate of the coded database 
Cj. Rewriting Equation 6.20 over u = 0, 1,...,r- 1 gives
r —1 
r —1 n
H, ^s,uci,a(i,u) + X X ^Sj,aicj,a(i,u)
u=0 
u=0 j=1
j=i
r —1 
n 
r —1
= X ^s,uci,a(i,u) + S ksj,aj X cj,a(i,u)
j=i
r-1 
n 
i \
= 
tf,uci,a(i,u) + X ^ajV'ji = 0, s = 0, 1, . . . , r - 1
u=0 
j=1 
j
j=i
For fixed values of i and u there are rn—1 possibilities for a(i,u) corresponding
to coordinates j e [n]\{i}. In matrix form these equations may be written:
■ 1
^i, 0 
.
1.
ki, 1 .
.
. 
1 -
.. ki,r—1 
..
ci,a(i,0) 
ci,a(i, 1)
.
= —
r 
n 
,,aa'> 
1
j=1 ,j=i j
n= n=1 ,j=i Kj,aj^(j',\
.
.
..
^r-1
LAi, 0,
. .
kr—1
ki,1 .
..
..
.. kir,—r —1 1
. .
ci,a(i,r—1)
. .
n 
rr—1 „ (a)
L2_j = 1,= ikj,a]^j,i J
(6.22)
Since the matrix has a Van der monde form and the elements are chosen to 
be distinct, it may be solved for the r elements of Ci . Thus as each of the 
(n— 1) surviving nodes transmits a single Fq symbol in the coordinated manner 
shown, the replacement node is able to compute r coordinates of its codeword 
Ci. Each coordinate corresponds to £ = rn Fq symbols and each surviving 
node must transmit a total of rn—1 Fq symbols for the regenerating node to 
compute Ci .
Thus in this scenario each node downloads a total of rn—1 Fq symbols and 
the total repair bandwidth is
Y = (n — 1 )rn -1 = (n — 1 )£/r F q symbols,
which by Equation 6.14 gives an optimal bandwidth scheme.
https://doi.org/10.1017/9781009283403.007 Published online by Cambridge University Press

6.3 Array-Type Constructions of Regenerating Codes
179
A variant of the above approach is to consider the notion of repair spaces 
rather than matrices. First considered by [17], it was also considered in [20] and 
more recently by [2, 3]. Chowdhury and Vardy [2] summarize this approach 
which is not described here.
In the literature of code constructions a problem of particular interest has 
been to devise constructions that yield optimal storage requirements while 
keeping the packetization level low - many of the optimal or near-optimal 
performances in terms of download bandwidth and/or constructions have large 
sub-packetization levels. To address this problem, [13] recently introduced the 
notion of an e-MSR code defined as follows:
Definition 6.3 Let e > 0 and C be an (n,U, dmin = n — k + 1 )q MDS code. 
The code C is an (n,k,d,£)q e-MSR code if, for every i e [n] there is a repair 
scheme to repair the i-th code block c(i) with
£
Pj,i < (1 + e) • -—+ i symbols (over Fq)
for all j e R c [n]\{i} such that | R |= d. Here flj,i denotes the number 
of symbols that the code block c(j) contributes during the repair of the code 
block c(i).
The notion of relaxing the requirement of optimal repair bandwidth to 
within e of the optimal in order to achieve lower packetization levels for MSR 
codes has been pursued to advantage also in [11].
The idea of interference alignment arose in the context of wireless com­
munications where techniques were developed to design multiuser signal 
systems such that signals from unwanted users occupy a subspace leaving 
the remainder of the space for the intended users. This idea has been 
adopted in the distributed coding domain to develop effective coding strate­
gies. Here the interference refers to the download of symbols not used by 
the repair node (interference) and the techniques are developed to mitigate 
their effect. Numerous works develop these techniques including [16, 21] 
and [1].
Comments
The past two decades has seen considerable progress in the areas of code con­
structions and protocols for the distributed storage and retrieval of information. 
The economic importance of the problem is clear and only a few of the many 
techniques developed are noted here. The gains that can be realized with these 
ideas are very significant.
https://doi.org/10.1017/9781009283403.007 Published online by Cambridge University Press

180
6 Coding for Distributed Storage
References
[1] Cadambe, V.R., Jafar, S.A., Maleki, H., Ramchandran, K., and Suh, C. 2013. 
Asymptotic interference alignment for optimal repair of MDS codes in distributed 
storage. IEEE Trans. Inform. Theory, 59(5), 2974-2987.
[2] Chowdhury, A., and Vardy, A. 2018. New constructions of MDS codes with 
asymptotically optimal repair. Pages 1944-1948 of: 2018 IEEE International 
Symposium on Information Theory (ISIT).
[3] Chowdhury, A., and Vardy, A. 2021. Improved schemes for asymptotically 
optimal repair of MDS codes. IEEE Trans. Inform. Theory, 67(8), 5051-5068.
[4] Dau, H., and Milenkovic, O. 2017. Optimal repair schemes for some families of 
full-length Reed-Solomon codes. CoRR, abs/1701.04120.
[5] Dimakis, A.G., Godfrey, P.B., Wu, Y., Wainwright, M.J., and Ramchandran, 
K. 2010. Network coding for distributed storage systems. IEEE Trans. Inform. 
Theory, 56(9), 4539-4551.
[6] Dimakis, A.G., Ramchandran, K., Wu, Y., and Suh, C. 2011. A survey on network 
codes for distributed storage. Proc. IEEE, 99(3), 476-489.
[7] Greferath, M., Pavcevic, M.O., Silberstein, N., and Vazquez-Castro, M.A. (eds.). 
2018. Network coding and subspace designs. Signals and Communication Tech­
nology. Springer, Cham.
[8] Guruswami, V., and Wootters, M. 2016. Repairing Reed-Solomon codes. Page 
216-226 of: Proceedings of the Forty-Eighth Annual ACM Symposium on Theory 
of Computing. STOC ’16. Association for Computing Machinery, New York.
[9] Guruswami, V., and Wootters, M. 2016. Repairing Reed-Solomon codes. CoRR, 
abs/1509.04764v2.
[10] Guruswami, V., and Wootters, M. 2017. Repairing Reed-Solomon codes. IEEE 
Trans. Inform. Theory, 63(9), 5684-5698.
[11] Guruswami, V., Lokam, S.V., and Jayaraman, S.V.M. 2020. MSR codes: con­
tacting fewer code blocks for exact repair. IEEE Trans. Inform. Theory, 66(11), 
6749-6761.
[12] Huffman, W.C., Kim, J.L., and Sole, P. Eds. 2021. A concise encyclopedia of 
coding theory. CRC Press, Boca Raton, FL.
[13] Rawat, A.S., Tamo, I., Guruswami, V., and Efremenko, K. 2018. MDS code 
constructions with small sub-packetization and near-optimal repair bandwidth. 
IEEE Trans. Inform. Theory, 64(10), 6506-6525.
[14] Sasidharan, B., Senthoor, K., and Kumar, P.V. 2014. An improved outer bound on 
the storage-repair-bandwidth tradeoff of exact-repair regenerating codes. Pages 
2430-2434 of: 2014 IEEE International Symposium on Information Theory.
[15] Shum, K.W., and Hu, Y. 2011. Exact minimum-repair-bandwidth cooperative 
regenerating codes for distributed storage systems. Pages 1442-1446 of: 2011 
IEEE International Symposium on Information Theory Proceedings.
[16] Suh, C., and Ramchandran, K. 2011. Exact-repair MDS code construction using 
interference alignment. IEEE Trans. Inform. Theory, 57(3), 1425-1442.
[17] Tamo, I., Wang, Z., and Bruck, J. 2014. Access versus bandwidth in codes for 
storage. IEEE Trans. Inform. Theory, 60(4), 2028-2037.
https://doi.org/10.1017/9781009283403.007 Published online by Cambridge University Press

References
181
[18] Tamo, I., Ye, M., and Barg, A. 2017. Optimal repair of Reed-Solomon codes: 
achieving the cut-set bound. Pages 216-227 of: 58th Annual IEEE Symposium 
on Foundations of Computer Science -FOCS ’17. IEEE Computer Society, Los 
Alamitos, CA.
[19] Tamo, I., Ye, M., and Barg, A. 2019. The repair problem for Reed-Solomon codes: 
optimal repair of single and multiple erasures with almost optimal node size. IEEE 
Trans. Inform. Theory, 65(5), 2673-2695.
[20] Wang, Z., Tamo, I., and Bruck, J. 2016. Explicit minimum storage regenerating 
codes. IEEE Trans. Inform. Theory, 62(8), 4466-4480.
[21] Wu, Y., and Dimakis, A.G. 2009. Reducing repair traffic for erasure coding-based 
storage via interference alignment. Pages 2276-2280 of: 2009 IEEE International 
Symposium on Information Theory.
[22] Ye, M., and Barg, A. 2017. Explicit constructions of high-rate MDS array codes 
with optimal repair bandwidth. IEEE Trans. Inform. Theory, 63(4), 2001-2014.
[23] Ye, M., and Barg, A. 2017. Repairing Reed-Solomon codes: universally achieving 
the cut-set bound for any number of erasures. CoRR, abs/1710.07216.
https://doi.org/10.1017/9781009283403.007 Published online by Cambridge University Press

7
Locally Repairable Codes
The term repairable here refers to erasure correction in that the coordinate 
position where the erasure occurs is known and contains a special symbol 
indicating erasure. This is in contrast to locally decodable codes considered 
in the next chapter where a symbol (coordinate position in a codeword) of 
interest may be in error which can be corrected by computation with a few 
other coordinate symbols.
This topic is related to the material of the previous and following two 
chapters. It is perhaps closest to the material of the previous chapter where 
the repair of a failed server is of interest, a notion which is almost identical 
to the repair of an erased codeword position. The focus of this chapter is 
on minimizing the number of codeword positions (or servers) that must be 
contacted to retrieve information from which the repair is effected. Otherwise 
the two themes have many similarities. We retain the distinction in this chapter, 
based largely on the literature being quoted, although the techniques developed 
here are applicable to the previous chapter as well.
The codes of these four topics are all related to the storage and retrieval 
of information with certain properties and have been vigorously researched 
over the past two decades. The literature on them is now large and, in keeping 
with the purpose of this volume, these four chapters are merely intended 
to introduce the topics by providing some of the basic concepts and a few 
examples with pointers to the more general results available. Interrelationships 
between the four topics are commented upon.
7.1 Locally Repairable Codes
This section is concerned with the reconstruction of codewords that have 
erasures in them with an emphasis on code constructions that require access to 
182
https://doi.org/10.1017/9781009283403.008 Published online by Cambridge University Press

7.1 Locally Repairable Codes
183
as few other codeword coordinate positions as possible. The restrictions such 
conditions place on the code construction are of interest.
The notion of a locally repairable code (LRC) arose in the works [13, 25]. 
To be consistent with the notation of the previous chapter the minimum 
distance of a code is denoted explicitly as dmin (rather than d , which was the 
number of helper nodes in the previous chapter). Also the parameter r in this 
section refers to the number of symbols needed to reconstruct an erased symbol 
as opposed to n -k in the previous section. The changes in notation reflect that 
of the literature in the respective areas.
The definition of a locally repairable code from [25].
Definition 7.1 An (n,r,dmin,m,a) locally repairable code (LRC) is a code 
that takes a file M of size m bits and creates k symbols, each of size a =| 
M| /k = m/k bits and encodes it into n coded symbols, such that each of the 
coded symbols can be reconstructed by accessing at most r other symbols. It 
is often assumed the size of a symbol is that of a finite field q . Moreover the 
minimum distance of the code is dmin implying a file of size m symbols can be 
reconstructed by accessing any of the (n - dmin + 1) coded symbols.
For the last sentence of this definition, note that for linear (n,k,dmin)q codes, 
a property of a k x n generator matrix of the code with minimum distance dmin 
is that every k x (n — dmin + 1) submatrix is of full rank since if this were 
not the case there would exist a nonzero linear combination of rows with zeros 
in (n — dmin + 1) coordinates giving a nonzero codeword of weight at most 
dmin — 1 and a contradiction. Thus from any (n — dmin + 1) coordinates, the 
remaining (dmin — 1) coordinates can be reconstructed.
Codes that satisfy the above definition are referred to as codes with locality 
r. The work [13] refers to such codes as local (r,dmin) codes with the view 
that such codes have the property that a given coordinate erasure can be 
recovered by access to at most r other coordinate positions. This notion will 
be generalized to the notion of an (r, &) code, which allows recovery even in 
the presence of other codeword position failures in a manner to be discussed 
shortly.
It is natural that the additional constraint for an LRC that each codeword 
position can be reconstructed by contacting at most r other positions imposes 
limits on the code parameters. The first general study of codes with locality 
is [13] although the work [18] contained similar ideas in their definition of 
Pyramid codes discussed later in the section. For example [13], suppose C is an 
(n,k,dmin)q maximum distance separable (MDS) code and hence the number 
of parity relationships in the code is n — k = dmin — 1. Take one of these parity
https://doi.org/10.1017/9781009283403.008 Published online by Cambridge University Press

184
7 Locally Repairable Codes
equations (leaving (d - 2) others and replace it with k parity checks, each of 
size at most r, i.e., each parity sums r other information coordinate positions). 
This would yield a code with
k
r
n-k=
+ dmin - 2,
a modification of the usual MDS condition, indicating that repair locality 
incurs a cost in code parameters. It turns out this relationship for linear codes 
extends to more general codes than MDS as the following theorem shows:
Theorem 7.2 ([13], theorem 5) For any (n,k,dmin)q linear code with informa­
tion locality r
k
r
n — k >
+ dmin - 2.
(7.1)
Note that for k = r the bound reduces to the Singleton bound. Codes that 
achieve this bound with equality will be referred to as distance optimal LRCs, 
noting that the introduction of (r,8) LRCs and maximally recoverable (MR) 
LRCs later in the chapter will result in modified bounds leading to ambiguity 
in the term optimal.
The following is of interest:
Theorem 7.3 ([13], theorem 17) Let n,k,r and d > 2 be positive integers. Let 
q>knk be a prime power. Suppose (r + 1) | n and
k
r
n-k=
+ dmin - 2.
Then there exists an (n,k,dmin)q LRC code where all code symbols have 
locality r.
Other works considering bounds for locally repairable codes include [5, 6]. 
More generally for a file of size m bits that maps it into k = m/a symbols of 
size a bits with a code of minimum distance dmin and locality r satisfies ([25], 
theorem 1) 
m
a
dmin — n
m
ra
+2
(7.2)
which reduces to the previous bound for m/a = k.
As variations of LRC codes are considered in the remainder of the section, 
the above bound will be changed to accommodate the variations considered.
https://doi.org/10.1017/9781009283403.008 Published online by Cambridge University Press

7.1 Locally Repairable Codes
185
Two constructions for locally repairable codes are considered here. While 
not representative of the wide variety of constructions available they give a 
brief indication of some of the approaches taken to the problem.
The first construction, due to [25], is simpler than many yet effective and 
intuitive. The construction uses MDS codes, achieves an MDS distance and 
suffers only an additive factor of 1/r reduction in the code rate. It will be 
convenient to use the code described to regenerate all the elements of a failed 
server while the definition of an LRC code only requires the regeneration 
of a single coordinate position. Thus we return briefly to the problem of the 
previous chapter.
Denote by yj e Fq the j-th component of the n-tuple j(i) over Fq. For 
simplicity the approach of [25] is slightly adapted for the first construction.
Let a file consist of M = rk symbols of the finite field Fq , x = 
[x(1),...,x(r)] and x(i) e Fqk row vectors. These k-tuples over Fq are 
(n,k,dmin)q MDS encoded (q>n)as
y(i) = x(i)G, 
x(i) e Fkq, y(i) e Fqn,i= 1,2,...,r,
where G is a k x n generator matrix of the code. Generate the overall parity 
check
s = ©r=#(> e Fq.
Among the codewords and parity-check word, there is a total of (r + 1)n Fq 
symbols. Each of the n servers will store a = (r + 1) symbols chosen from 
distinct codewords and from the parity-check word. Assume (r + 1) | n and 
n = n/(r + 1). The n servers will be divided into n groups of servers and 
a failure of a server within a group will be restored entirely by information 
from other servers within the group. To facilitate node restoration the symbols 
are arranged in a cyclic fashion, within a group of nodes. A simple example (a 
particular case of an example in ([25], section 5) will easily expand to illustrate 
the general procedure.
Example 7.4 (Refer to Figure 7.1) Consider the case of k = 4 x 12 
data symbols over a field of characteristic 2 (the more general case is easily 
considered). The data are arranged into r = 4 words of 12 Fq symbols each. 
The 4 words x(i),i = 1,2, 3,4 are MDS encoded using a (15, 12,4)q,q > 15 
into the 4 words y(i),i = 1,2, 3,4. The coordinates of the 4 codewords and 
the one parity-check word are stored on 15 servers, each server containing 
(r + 1) = 5 Fq symbols. The servers are divided into n = 15/5 = 3 groups 
each group containing five servers. The situation is depicted in Figure 7.1. 
As noted
https://doi.org/10.1017/9781009283403.008 Published online by Cambridge University Press

186
7 Locally Repairable Codes
Server group 1
#1
#2
#3
#4
#5
y171) y271)
71) 
y3
71) 
y4
y575)
y272) y372) y472) y572) y172)
73) 
y3
73) 
y4
y573) y173)y273)
74) 
y4
74) 
y5
74) 
y1
74) 
y2
74) 
y3
55
51
52
53
54
Server group 2
server 
failure
#6
#7
#8
#9
#1o
y671)y1
71) 
y8
71) 
y9 y171o)
y772)
8^
72) 
y9 y172o) y672)
73) 
y8
v(3) 
y 9
73) 
y1o y673) y773)
74) 
y9 -(4) 
y Jo
74) 
y6
74) 
y7
74) 
y8
510 5 6
57
58
59
Server group 3
#11 #12 #13 #14 #15
71) 
y11
71) 
y12
71)
y13
71) 
y14
71) 
y15
72) 
y12
72)
y13
72) 
y14
72) 
y15
72) 
y11
73)
y13
73) 
y14
73) 
y15
73) 
y11
73) 
y12
74) 
y14
74) 
y15
74) 
y11
74) 
y12
74)
y13
515 511 512 513 514
Figure 7.1 Example 7.4 of LRC code, k = 12,n = 15,r = 4,n = 3, failure of 
server 7
y(i) = x(i) • G e F|5, 
i = 1,2,3,4, 
s = ®4=1 y(i) e F|5.
Suppose server 7 fails. The original contents of the server are to be recon­
structed using only symbols from the other servers within group 2, recon­
structing each symbol by using no more than r = 4 other symbols. The 
reconstruction of the required symbols is as follows:
y71) = 57 ® y72) ® y73) ® y74) 
y82) = 58 ® y81) ® y83) ® y84) 
y93) = 59 ® y 91) ® y92) ® y 94) 
y1730) = s10 ® y1710) ® y1720) ® y1730)
and these equations satisfy the requirements.
As noted above, the coding scheme has been described as a scheme 
to regenerate the entire contents of a failed server, containing many Fq 
symbols. However, it is clear the code satisfies the definition of an LRC code, 
regenerating a single coordinate position.
The scheme in the above example is easily generalized to more parameters. 
In that case each node would contain r Fq code symbols from each of the r 
codewords (same set of subscripts) plus the same set of subscripts of the overall 
parity-check word. Also the circular pattern of indices for each row/column in 
the same group is easily derived from the first group and the above example. 
The fact the code has locality r follows directly from the construction.
https://doi.org/10.1017/9781009283403.008 Published online by Cambridge University Press

7.1 Locally Repairable Codes
187
Since the codewords y(i) are from an MDS generator matrix, these code­
words have minimum distance dmin = n - k + 1. The code rate is
= rk 
k k
(r + 1)n 
n 
(r + 1)n
showing a slight reduction of the overall code rate in order to accommodate 
local repairability.
Also, since the total number of overall data is M = rk Fq symbols and 
(r+ 1) Fq symbols are stored on each server, it is straightforward to verify that 
for the case (r + 1) \ k the bound of Equation 7.1 is satisfied in that [25]
n-
M
ra
+2=n-
rk 
r+1
+2=n -k+ 1
M
a
k 
r+1
(noting that frk/(r + 1) 1 + Tk/(r + 1) 1 = k + C-k/(r + 1) 1 + Tk/(r + 1) 1 = 
k+1).
For the second LRC code construction, recall the bound 7.1 and that codes 
that meet this bound are referred to as distance-optimal LRC codes. The codes 
of [31] achieve this bound through an interesting and novel construction. 
Recall first that a polynomial f (x) that interpolates through the set ofn points 
(xi,yi), i = 1,2,...,n, i.e., yi = f(xi) is of degree at most n - 1 can be 
constructed of the form
n n
f(x)= 
yi
i=1 j=1
(x - xj) 
(xi — xj)'
j=i
The polynomial can be of degree less than n - 1 depending on the x,y values.
A code of length n over Fq, q > n is constructed. It will be an evaluation 
code on a set U c Fq, | U |= n of distinct elements of Fq. It is assumed 
that r | k and that (r + 1) | n (restrictions that further constructions 
in [31] remove). The set U is divided into equal-sized subsets Ui,i = 
1,2,... ,Ui, U = Uf=1 Ui such that the subsets are each of size (r + 1). The 
reason for the (r + 1) subsets is that, in constructing codes of locality r,ifa 
server (codeword position) within a subset fails, the code construction will be 
such that contacting the r other nonfailed servers (codeword positions) within 
the subset will be sufficient to allow server regeneration. The construction 
assumes the existence of a polynomial g(x) e Fq [x] of degree r + 1 which is 
constant on the subsets. Such a polynomial, referred to as a good polynomial, 
will be considered shortly. The construction will lead to optimal (n,k,r) LRC 
codes where r is the locality of the code, i.e., any missing symbol can be 
reconstructed with knowledge of r other code symbols.
https://doi.org/10.1017/9781009283403.008 Published online by Cambridge University Press

188
7 Locally Repairable Codes
The codeword corresponding to an information sequence a e Fqk (k Fq 
symbols) is constructed as follows. Assume that r | k and write the information 
vector a as the r x (k/r) rectangular array
aij, i = 0,1, ...,r — 1, j = 0,1, ...,---- 1.
r
For g(x) a good polynomial, constant on each of the sets Ui, construct the two 
polynomials:
1 
k — 1
r-1 
r 1
fa (x) = 
fi(x)xi, fi(x) = 
aijg(x)j, i = 0, 1,...,r-1. 
(7.3)
i=0 
j=0
Notice immediately that since g(x) is constant on each set Uj,j = 1,2,... ,1, 
so is the polynomial fi (x).
The code C then is defined by evaluating polynomial fa on the evaluation 
set U, one codeword for each of the possible qk polynomials fa:
C = cfa = (fa(u1),fa(u2),...,fa(un)), a = (a1,...,ak) e Fqk . (7.4)
The degrees of these polynomials fa are at most (k/r — 1)(r + 1) + r — 
1 = k + k/r — 2, and hence the minimum distance of the code is n minus 
this quantity, i.e., two such polynomials corresponding to different information 
k-tuples cannot interpolate to the same codeword and there are qk codewords. 
Since the code is linear the minimum distance is at least as large as n less the 
maximum degree of the polynomials (the maximum number of zeros), i.e.,
de > n - k k +------ 2 |
r
which achieves the minimum distance bound for codes of locality r of 
Equation 7.2.
To see that the codes have locality r suppose the symbol (server) u * e 
Uj is erased. We show how such a symbol can be regenerated by use of the 
remaining r symbols in Uj, i.e., it is desired to find fa(u*) where u* is known 
and the remaining values of fa (u), u e Uj \{u*} are given. Consider the two 
polynomials
E
n x - V 
z- z x 
-
Cu 
, Cu = fa(u). 
(7.5)
u — v 
u e Uj \{ u *} 
v e Uj\{ u * ,u}
and
r — 1
n2(x) = 
fi(u*)xi,u* e Uj 
(7.6)
i=0 
https://doi.org/10.1017/9781009283403.008 Published online by Cambridge University Press

7.1 Locally Repairable Codes
189
where fi(x) is defined in Equation 7.3 (and u* is the erased position in 
Uj). Notice that the polynomial n 1 can be computed by the regeneration 
process since all quantities involved are known, while polynomial n2 requires 
knowledge of all polynomials fi and hence the information sequence a e Fqk 
which is unknown to the regeneration process. Both of these polynomials have 
degree at most r - 1. It is claimed that the polynomials are equal since by 
definition
n 1 (Y) = cY = fa(Y), Y e Uj\{u*},
and similarly by Equation 7.3
r-1
n2(Y) = 
fi (u*)Y i = cY,Ye Uj
1 + a = a4
1 + a2 = a8
1 + a3 = a14
1 + a4 = a
1 + a5 = a10
1 +a6 = a13
1 + a7 = a9
i=0
and the two polynomials of degree at most r - 1 assume the same values on the 
set Uj \{u*} of r values. Hence they are equal. Thus the value cu* = n2(u*), 
the erased value sought, can be evaluated by polynomial n1(u*) (= n2(u*)) 
which can be formed from known quantities.
A key aspect of the above construction is the need to form a polynomial 
g(x) that is constant on the subsets of the evaluation set Ui where U = Ui=1 Ui. 
A simple suggestion in [31] is to consider cosets of a subgroup of Fq, either 
additive or multiplicative. For example, if n is a primitive element of Fq and 
5 | q — 1,t = (q -1 )/s, then the subgroup G = {ns) is cyclic of order t and the 
(multiplicative) cosets of Fq* are niG, i = 0, 1,...,s- 1 and the polynomial
g(x) n (x - ^)=x -1
is easily shown to be constant on the cosets. The following extended example 
illustrates these constructions:
Example 7.5 Let q = 24 and base the arithmetic on the primitive polynomial 
f(x) = x4 + x + 1 over F2 with the table shown allowing easy arithmetic in 
the field where a is a primitive element (zero of f (x)). Define the subsets of 
interest to be the cosets Ui of the cyclic subgroup of Fq* of order 5 generated 
by a3, i.e., G = {a3) = {1 ,a3,a6,a9,a 12}, i.e.
g(x) = x5 + 1
U1 = {1,a3,a6,a9,a12}, g(U1) = 0
U2 = {a,a4,a7,a10,a13}, g(U2) = a10
U3 = {a2,a5,a8,a11,a14}, g(3) = a5
1 + a8 = a2
1 + a9 = a7
1 + a10 = a5
1 + a11 = a12
1+a12 =a11
1 +a13 = a6
1 + a14 = a3
https://doi.org/10.1017/9781009283403.008 Published online by Cambridge University Press

190
7 Locally Repairable Codes
Consider the (15, 8)q code with information symbol array (chosen arbitrarily), 
with the associated polynomials:
A = (aij) =
4
a
a
13
7
a
a
2
11
a
a
0
1
associated polynomials:
f0 (x) = a4 + ag(x) = ax5 + 1
fi (x) = a13 + a7 g(x) = a7 x5 + a 5 
f2(x) = a2 + a11g(x) = a11x5 + a9 
f3(x) = 0 +x5 + 1.
These computations lead to the codeword evaluation polynomial and code­
word:
fa (x) = x8 + a11x7 + a7x6 + ax5 + x3 + a9x2 + a5x + 1
cfa = (fa(1),fa(a),fa(a2),fa(a3),fa(a4),fa(a5),fa(a6),fa(a7), 
fa(a8),fa(a9),fa(a10),fa(a11),fa(a12),fa(a13),fa(a14)) 
= (a9,a10,a4,a2, 0,a2,a14,a10, 0,a11,a8,a9,a9,a3,a).
Suppose the codeword position of u * = a10 in the subset U2 is erased and it is 
desired to regenerate the codeword value in this position (a8) using Equation 
7.5 which refers to the codeword values at the other coordinate positions of 
subset U2, i.e., the coordinate positions of {a,a4,a7,a13} - the codeword 
values at these positions are respectively {a10, 0,a10,a3}. Equation 7.5 yields:
z 
y- 
n 
(x - P')
n 1(x) y 
cP 
P ,)
P e U2\{a *} 
P ,e U2\{ a *, P }'"
■m- (x + P')
(P - P')
P/3'^{ a 4 ,a 7 ,a13}
( n 
(x + P')
• 
(P - P')
XP 'e{ a1 ,a 4 ,a13}
a2 • a6 • a9
n 1 (a10) = a10 • 
14------+ 0 + a10 •
1 • a14 • a12
= a10
+a10
•a •a
7,a13}
(x + P')
• 
(P - P')
VP 'e{ a1 ,a4 ,a 7}
a8 • a2 • a9 
3 
a8 • a2 • a6
—— + + + a 3 • —— —---- =■
a14 • a3 • a5 a12 • a11 • a5
•a •a
•a •a
• a3 • a
• a11 • a
(x + P') 
(P - P’)
• a
= a + 0 + a7 + a6 = a8, 
as required. By construction the code is a (15, 8)24 LRC code with r = 4.
The work [31] develops many other interesting aspects of this construction. 
We limit ourselves to the following discussion for which it is not necessary 
for the m partition sets Ui to be of equal size. Denote the partition set U = 
{U1,U2,...,Um} and let h(x) = ueU (x - u), the annihilator polynomial of 
U. Denote by FU [x] the set of polynomials that are constant on the sets of U 
taken modulo h(x), i.e.,
FU [x] ={f eFq[x]: fis constant on the sets Ui,i = 1,2,...,m, deg f<| U|}.
https://doi.org/10.1017/9781009283403.008 Published online by Cambridge University Press

7.1 Locally Repairable Codes
191
The set is closed under addition and multiplication modulo h(x) and can 
be shown to be a commutative algebra with identity and has the following 
properties ([31], proposition 3.5):
(i) A nonconstant f e FU[x] satisfies maxi |Ui | < deg(f) < |U|.
(ii) The m polynomials
x - v , i = 1,2,...,m 
u-v
ueUi veU \u
form a basis of FU [x], dim(FU [x]) = m, and indeed
fi(Uj) = &ij
where 8ij is the Kronecker delta function. It follows immediately that: 
(iii) if g(x) takes on the value ui on set Ui,i= 1,2,...,m, deg(g) <|U |, 
then g(x) can be written
m 
g(x) = 
ui fi (x)
and the polynomials 1,g(x),g2(x),...,gm-1 (x) form a basis ofFU[x]. 
(iv) There exist m integers 0 = d0 < d 1 < • • • < dm -i < | U | such that the
degree of each polynomial in FU[x] is di for some i and di = deg fi.
In the case the subsets Ui are of equal size (r + 1), then di = i(r + 1), i = 
0, 1 , . . . ,m - 1.
More properties of this interesting algebra FU [x] are established in [31] and 
used for further constructions of LRCs.
Given the nature of the coding problem it is not surprising that combina­
torial configurations have been used to construct interesting classes of locally 
repairable codes with their use. These include resolvable designs [29], partial 
geometries [24], Hadamard designs [26] and matroid theory [26, 33] among 
many other configurations.
The construction techniques and the properties of the resulting codes 
available in the literature vary a great deal and the examples discussed here 
give a brief look into a few of the techniques used.
For the remainder of the section some classes of LRC codes are considered 
as well as classes of codes specifically designed for the recovery of certain 
types of storage systems. It is clear that the notion of optimality can only be 
discussed in reference to the type of constraints of these systems and it differs 
for each class.
https://doi.org/10.1017/9781009283403.008 Published online by Cambridge University Press

192
7 Locally Repairable Codes
(r, S) locally repairable codes
The notion of an (r, 8') code is introduced:
Definition 7.6 ([28]) The i-th code symbol ci, 1 < i < n of an (n,k,d)q 
linear code C, is said to have locality (r, 8) if there exists a punctured subcode 
of C with support containing position i whose length is at most r + 8 - 1 
and whose minimum distance is at least 8. Equivalently there exists a subset 
S i Q [ n ] = {1,2,... ,n} such that 
(i) i e Si and |Si |< r + 8 - 1;
(ii) the minimum distance of the code CSi obtained by deleting code symbols 
cj,j e [n]\Si is at least 8.
In terms of parity-check matrices, if the linear code with parity-check matrix 
H has all coordinate positions in [n]\Si removed, the remaining ni * | Si | 
matrix Hi, for some ni < n — k, has the property that any 8 — 1 columns are 
linearly independent.
If all information positions i of the code C have locality (r, 8), it is referred 
to as having information locality and designated an (r, 8)i code. If all positions 
(information and parity) have (r, 8) locality, it is referred to as an (r, 8)a code. 
The point of the definition is that if a code has information locality, then 
a systematic coordinate position can be repaired by using r other positions 
even in the presence of 8 - 2 coordinate erasures. The previous (r, d) codes 
correspond to (r, 2)i codes (i.e., 8 = 2).
With the extra constraint imposed by such codes it is shown [28] that the 
minimum distance of an (r,8)i code must be upper bounded by
dmin < n - k + 1
- 1 (8 - 1).
(7.7)
Under certain conditions the bound also applies to (r, 8)a codes and codes 
meeting this bound can be constructed. In particular it is shown:
Theorem 7.7 ([28], theorem 9) Let q>knk and (r + 8 - 1) | n. Then there 
exists an (r, 8)a (n,k,dmin)q linear code with 8 - 1 < d satisfying the bound 
Equation 7.7.
Some nonexistence results are also available ([30], theorems 1 and 11):
Theorem 7.8 If (r + 8 — 1) \ n and r | k, then there exists no (r,8')a linear 
code which achieves the bound Equation 7.7.
Further aspects of these codes are discussed in [20]. Cyclic versions of the 
codes are considered in [10] and codes with differing localities, an aspect of 
interest in practice, in [9].
https://doi.org/10.1017/9781009283403.008 Published online by Cambridge University Press

7.2 Maximally Recoverable Codes
193
7.2 Maximally Recoverable Codes
The idea of maximally recoverable (MR) LRC codes arose in the work [12] as 
a stronger notion than LRC. The notion in that work is quite general. It defines 
the notion of an erasure configuration and the set of all recoverable erasures 
E(&) and a code as being MR if it is able to recover any erasure e e E(&), i.e., 
it is able to recover all erasure patterns it is information theoretically capable 
of correcting.
It will be helpful to consider the discussion of a somewhat narrower class 
of MR LRCs definition in ([14], definition 5):
Definition 7.9 Let C be a linear systematic (n, k) code. C is a (k,r,h) local 
code if the following conditions are satisfied:
(i) There are k data symbols and h heavy parity symbols (which may 
depend on all data symbols).
(ii) Assume r | (k + h) and n = k + h + (k + h)/r.
(iii) The (k + h) symbols are grouped into I = (k + h)/r groups of size r 
and an overall parity check is added to each group (hence the length of 
the code is n).
(iv) The local code is said to be MR if for any set E c [n] where E is 
obtained by choosing one coordinate from each of the groups, 
puncturing C in the positions of E yields an MDS (k + h, k) code.
Notice the definition implies such a code has locality r for all code symbols 
and that after correction of a single erasure in each of the groups, from the 
MDS property of the punctured code, it is able to correct any h remaining 
erasures. It is clear that such a code is by definition capable of correcting any 
pattern E c [n] of erasures where E is formed by picking one coordinate from 
each of the (k + h)/r groups and any h additional positions. These codes are 
also called partial MDS or PMDS codes in [3]. These are discussed further 
later in the section as PMDS codes as there is a considerable literature under 
that name. The distinction is preserved for reference to the literature as a matter 
of convenience. They are, however, MR codes.
An example of the construction of a local MR LRC is as follows. The work 
[14] refers to multisets, where set elements may be repeated although attention 
here is restricted to sets (no repetition of elements). Let Fq be a finite field of 
characteristic 2 and let S = {a 1 ,a2,... ,an}, ai e Fq and denote the h x n 
parity-check matrix A(S,h) = (agj) by
agj = aj 
, g = 1 , 2, . . . , h, j = 1 , 2, . . . , n.
https://doi.org/10.1017/9781009283403.008 Published online by Cambridge University Press

194
7 Locally Repairable Codes
Definition 7.10 The set S c Fq is said to be t-wise independent over a field 
F' C Fq if every subset of S of size at most t is linearly independent over F'.
Denote the code C(S,h) containing x = (x 1 ,x2,... ,xn) e Fqn if
n
aj xj = A(S,h) • xt = 0, g = 1,2, ... ,h.
j=1
The following lemma ([14], lemma 10) is immediate:
Lemma 7.11 The code C(S, h) has distance h + 1 if and only if the set S is 
h-wise independent over F2 c Fq.
This follows since every h x h submatrix of A(S,h) is nonsingular if and 
only if any h elements of S are linearly independent over Fq and by lemma 
3.51 of [22] it is sufficient they are linearly independent over F2.
Let
n = k + h + (k + h)/r = (r + 1 )(k + h)/r and let t = 
= k+h.
r+1 r
Let the i-th group contain the r + 1 symbols {xi,1,xi,2,...,xi,r+1}, i = 
1,2,... ,t, a relabeling of the components of x. In each of the t groups, the 
variables satisfy a parity check sr=+11 xi,s = 0, i.e., one symbol is the overall 
parity check of the other r symbols in the group. It follows that all code 
coordinates have locality r . To facilitate discussion of these groups write the 
set S as
S = {ai,s} for i e [£], s e [r + 1].
With this notation the parity checks may be written as:
£ r +1
ai2,gs- xi,s = 0forg = 1,2,...,h 
i=1 s=1
r +1
x^ = s = 0 for i = 1,2,...,I, 
s=1
and these equations define the code C(S, r, h).
Let e = (e 1 ,e2, ...,et) e [r + 1]£ be an indicator vector indicating a 
single position within each of the I code groups and let C-e(S,r,h) be the 
code obtained by puncturing C(S,r,h) in positions {i,ei},i = 1,2, ...,£, 
one position in each group. By the Definition 7.9(iv) the code C(S,r,h) is 
maximally recoverable if and only if the code C-e (S,r,h) is MDS for every 
e e [ r + 1]£ .A criterion to achieve this is the following ([14], proposition 11):
https://doi.org/10.1017/9781009283403.008 Published online by Cambridge University Press

7.3 Other Types of Locally Repairable Codes
195
Proposition 7.12 The code C (S,r,h) is maximally recoverable if and only if 
for every e e [r + 1]£ the set
T(S,e) = {ai,s + ai,ei,i e [],s e [r + 1]\ei}
is h-wise independent.
Thus the construction of maximally recoverable C (S,r,h) codes is reduced 
to the construction of h-wise independent sets. Several constructions of such 
sets are given in [14]. The following theorem results from using binary BCH 
codes to derive such sets:
Theorem 7.13 ([14], theorem 13) Let k, r, h be positive integers such that r | 
(k + h). Let m be the smallest integer such that
n = k + h + <*-+» < 2 m - 1.
Then there exists an MR local (k,r,h) code over the characteristic 2 field F2hm.
As with previous types of codes, combinatorial configurations have been 
used to construct interesting classes of LRCs and MR LRCs (e.g., [26, 33]). 
The topic of MR codes has a large and interesting literature and the above is 
but a brief look at the variety of such constructions. As noted, the PMDS codes 
to be discussed are also MR codes.
7.3 Other Types of Locally Repairable Codes
There have been interesting classes of codes proposed for certain types 
of storage devices to address specific requirements. For example, the code 
construction might depend on the fact that the storage device has a redundant 
array of independent disks (RAID) structure. A view of this architecture [27] 
is as an array of n disks, referred to as a stripe, each disk containing a certain 
number of sectors or symbols, each symbol comprised of a number of bits. The 
various RAID architectures vary in the number of disks and sectors devoted to 
parities.
A few of the types of coding related to these structures are discussed in this 
section since they do not fall easily into the previous sections. In particular the 
notions of partial MDS (PMDS) and sector-disk (SD) codes arose in response 
to these architectures [2, 3, 4, 27]. A few comments on such codes are given 
below.
https://doi.org/10.1017/9781009283403.008 Published online by Cambridge University Press

196
7 Locally Repairable Codes
Partial MDS and Sector-Disk Codes
It has been noted that PMDS codes are in the class of MR LRC codes discussed 
previously. The reason the previous material and the material here was not 
combined is to access the literature under the PMDS name. These PMDS 
codes have a particular structure and, since they are capable of correcting 
all erasures which they are information theoretically capable of, they are MR 
codes. Certain aspects of these codes are more relevant to the RAID storage 
architecture noted and to the SD codes discussed below. The definition of the 
codes used here (equivalent to the definition of MR codes considered earlier in 
the chapter) is as follows [11]:
Definition 7.14 An m x n array code C over Fq is called an (r,s) PMDS 
code if for positive integers r,s:
(i) C is a linear (mn, m(n - r) - s)q code.
(ii) The set of rows of the code C (a codeword as an m x n array) is an
(n,n - r,r+ 1)q MDS code.
(iii) For any m nonnegative integers s 1 ,s2, . ..,sm, s 1 + s2 +----- + sm = s the
array code can correct up to si + r erasures in row i,i = 1, 2,...,m. 
(More precisely, an (r, s) PMDS code is capable of correcting r erasures 
in each of the m rows and any other s erasures in the array.)
Sector-disk (SD) codes arise from considering types of failure of storage 
systems. Array codes often consider a sector failure of a disk as an entire disk 
failure. The SD codes arise from differentiating these types of failures. They 
are a special case of PMDS codes.
Definition 7.15 An m x n array code C over Fq is called an (r, s) SD code 
if for positive integers r, s:
(i) C is a linear (mn, m(n - r) - s)q code.
(ii) Each row of the code C is an (n,n - r,r + 1)q MDS code.
(iii) C can correct up to mr + s erasures provided that among the erasures 
mr of them are in m columns (i.e., r of the columns are completely 
erased, the other s erasures are unrestricted).
Thus an SD code is a restricted type of PMDS code where certain of the 
erasures are constrained to all be in a column.
An example of an (r, 1) PMDS code is given [11]:
Example 7.16 Consider information symbols {a 1 ,a2, ...,amk-1} over Fq 
and form the m x k array
https://doi.org/10.1017/9781009283403.008 Published online by Cambridge University Press

7.3 Other Types of Locally Repairable Codes
197
S=
O 1
Ok +1
O2 
••• Ok-1 Ok
Ok+2 
■■• O2 k -1 O2 k
_O(m. — 1 )k +1 O(m — 1 )k+2 • • • Omk —1 Omk_
where the Omk = S element is the only noninformation symbol in the array 
given by
S = Omk = - (Ok + O2k + • • • + O(m — 1 )k) or
Oik = - (Ok + • • • + O(i — 1 )k + O(i+1 )k + • • • O(m — 1 )k + S),
(7.8)
i.e., S is chosen to make the sum of the last column 0. Let a 1, ...,an be distinct 
nonzero elements of Fq (hence q>n) and encode the above information 
symbol array into the code array:
X = S •
a
1
a1
1
a2
1
an
k—1 
1
k—1
2
k—1 
n
f1(a1) f1(a2) ••• f1(an) 
f2(a1) f2(a2) ••• f2(an)
. 
...
. 
...
. 
...
fm(a1) fm(a2) ••• fm(an)
where fi (x) = 
jk—=10Oik+j+1xj, i = 1,2,...,m.
Suppose the matrix Y = (yij),i = 1,2,...,m, j = 1,2,...,n is the array 
X with at most r erasures introduced into m — 1 of the rows and at most r + 
1 erasures introduced into the other row. Suppose the rows with at most r 
erasures are the first m — 1 and the last row contains r + 1 erasures. It can 
be verified that these assumptions can be made without loss of generality, i.e., 
the argument is easily changed to accommodate alternate assumptions. Since 
the rows are defined by evaluating a polynomial of degree at most k — 1atthe 
points ai, they may be viewed as a codeword in an RS code with parameters 
(n,k,n — k + 1)q and r = n — k. Thus the first m — 1 rows of the array can 
be decoded to give the corresponding information values. The value ymk can 
be decoded as minus the sum of the (correctly) decoded elements in the last 
column in the top (m — 1) rows. This value is Omk. The value Omkank—1 from 
each of the nonerased entries of the last row ofYnow correspond to evaluations 
of polynomials of degree at most k—2 and hence to codewords ofan RS coded 
(n,k — 1,n — k + 2)q and thus capable of correcting (r + 1) erasures. Thus the 
code is an (r, 1) PMDS code.
The above is a simple example of a PMDS code. The following construction 
is general and somewhat intricate but of significant interest, yielding (r, s) 
PMDS codes for arbitrary positive integers r,s. The construction is based on 
https://doi.org/10.1017/9781009283403.008 Published online by Cambridge University Press

198
7 Locally Repairable Codes
rank-metric codes (as in Chapter 12). The notation of [8] is preserved except 
the roles of r and m are reversed here from that work.
Let g1,g2,...,gN be N linearly independent elements of FqM over Fq and 
encode information symbols u0,u 1,... ,uK-1 e FqM into
■V = (x 1 ,x2, ■ ■■,xN) = (f(g1 ),f(g2), ...,f(gN)') e FNM, 
q
where f(x) = 
jK=-01 uixqi e FqM [x].
Note that V is a codeword in a rank-metric Gabidulin code introduced in 
Chapter 12. The code is clearly linear and the parameters chosen will give 
an (N = K + s,K, D = s + 1)qM rank-metric MDS (MRD) code. Thus the 
rank of the difference of any two distinct codewords over FqM is at least s + 1. 
These codes will be used to produce an (r, s) PMDS code.
Assume the code length N can be written as N = K + s = m(n - r) = 
mk, r = n - k, and the codeword x is written as an m x k array/matrix X (over 
FqM). For this it will be convenient to identify
xik+j ^ xi,j and gik+j ^ gi,j, i = 0, 1, ■ ■ ■ ,m - 1, j = 1,2, ■ ■ ■ ,k.
Define the matrices
X=
x0,1
x 1, 1
..
x 2 
•
x 1,2 
•
. .
• x 0,k
• x 1,k
..
..
and
G =
g0,1 
g1,1
..
g0, 2 • 
g1, 2 
•
. .
• 
g0,k
• g1,k
..
..
.
xm-1,1
.
xm -1, 2 • ..
• xm -1 ,k_
.
gm-1, 1
.
g m-1, 2 • ..
• g m-1 ,k-
Denote by Vj,j = 0,1, ■ ■ ■ ,m - 1 the vector subspaces of the vector 
space of FqM over Fq, each of dimension k, generated by the elements of Vj, 
i.e., the subspace of linear sum of elements over Fq of xjk+1, ■ ■ ■ ,x(j+1 )k = 
xj, 1, ■ ■ ■ ,xj,k which are linearly independent over Fq. By assumption any two 
distinct subspaces Vi and Vj ,j = i intersect only in the zero element.
These m x k matrices will both be extended to m x n matrices. Denote the 
j-throw of X as Xj,j = 0,1,2, ■■■ ,m - 1, Xj = (xj, 1, ■■■,xj,k) e F ^M,j = 
0,1, ■■■ ,m - 1. Consider an (n,k,n - k + 1 )q MDS code over F q with k x n 
generator matrix G assumed to be in systematic form (the rows of X are over 
FqM). Any such generator matrix G has the property that any k columns are 
linearly independent over Fq since otherwise it would be possible to generate 
a nonzero codeword of weight n - k, a contradiction.
Let
Y=XG, G e Fqkxn.
https://doi.org/10.1017/9781009283403.008 Published online by Cambridge University Press

7.3 Other Types of Locally Repairable Codes
199
Since the generator matrix is assumed systematic, the result is
Y=XG=
x0,1 
x 1, 1
...
x 0, 2 • 
x 1, 2 •
. . .
• x 0,k
• x 1,k
..
..
..
y 0,k+1 • 
y 1,k+1 •
. . .
• 
y 0,n
• 
y 1,n
..
..
..
x(m—1), 1 x(m —1), 2 •
• xm( —1 ),k ym—1,k +1 •
• ym — 1 ,n_
where
yi,j = Tk='■=1 wu = Er:=1 f( g i,^,k+j = f ( E m-1 g i,^,k+j 
i = 0, 1 , . . . ,m - 1 ,j = k + 1 , . . . ,n.
From this equation define new evaluation points as
m — 1
gi,j = 
gi,^g£,k + j ,i = 0, 1, ... ,m — 1,j = k + 1, ... ,n.
£=1
The matrix G above can be extended to
Ge =
g0, 1 
g1,1
. . .
g0, 2 
g1,2
. . .
g0,k
g1,k
..
..
..
g0,k+1 
g1,k+1
...
• 
g 0,n
• g1,n
..
..
..
gm—1,1 gm—1,2
gm—1,k gm—1,k+1
gm—1,n
Thus, any set of elements of the matrix Ge with at most k elements from each 
row will result in a set of linearly independent elements of FqM over Fq .
For the polynomial f(x) = K— K— uixq e FqM [x] formed from the 
information symbols ui e FqM,i = 0,1,... ,K — 1, form the corresponding 
m x n codeword as
cf =
f (g0,1)
f (g1,1)
f( g0,2 ) ••• 
f( g0,k) 
f( g 0 ,k +1) 
••• f( g 0,n)
f( g1,2) ••• 
f( g1,k) 
f( g 1 ,k +1) 
••• g 1,n
Lf(gm — 1,1) f(gm — 1,2) ••• f(gm — 1,k) f(gm — 1,k +1) ••• f(gm — 1,n)_l
Suppose there are r = n — k erasures in each of the m rows and a further s 
erasures anywhere in the array. Suppose specifically there are si erasures in 
the i-th row and Em=1 si = s• Under these conditions let £i be the number 
of unerased positions in the i-th row and note that by construction £i < k. It 
follows that the number of linearly independent evaluations (since at most k 
evaluations from each row are used) of the polynomial f that can be formed is
m m m
^^min{ li,k} = 
li = mk — 
si = mk — s = K.
https://doi.org/10.1017/9781009283403.008 Published online by Cambridge University Press

200
7 Locally Repairable Codes
The linear independence of the evaluations ensures that in setting up a 
matrix equation between the K linearized polynomial coefficients ui and the 
evaluations, the matrix will have full rank and hence is solvable for a unique 
solution. The conclusion then is the code is an (r, s) PMDS code which can be 
formed for any positive integers r, s.
More recent work [23] uses rank-metric codes in an interesting and deeper 
technique to construct MR LRCs. Further aspects of both SD and PMDS codes 
can be found in [1, 2, 3,4,7, 11, 15, 16, 17, 21].
Pyramid Codes
The class of pyramid codes is introduced in [19] as particularly effective 
erasure-correcting codes for commercial application. The classes of basic 
pyramid codes (BPC) and generalized pyramid codes (GPC) are introduced 
there but only BPCs are considered here. The BPCs are modified MDS codes 
and while not MDS they have several important properties making them 
attractive for disk storage arrays.
Denote a codeword c = (x 1 ,x2,... ,xk,p 1,... ,pn-k) e Fq in an (n,k)q 
MDS code, C , r = n - k, for some finite field Fq for a set of k information 
symbols x e Fqk and parity symbols p e Fqr and c = (x, p) (catenation). 
Separate the k data symbols into a disjoint union of subgroups Si,i = 
1,2,...,L of not necessarily the same size, where | Si |= ki, iL=1 ki = k 
and denote by xi the set of information symbols corresponding to Si,i = 
1,2,...,L. Without loss of generality let x = (x1,x2,...,xL).
From the codewords of this code a new set of codewords is constructed as 
follows. For any parity subvectorp e Fr,r = n — k, letp' e Fr°, denote the 
r0-tuple of the first r0 elements of p,r0 <r. For the construction any fixed 
r0 parity coordinates could be chosen but wlog the set of first r0 coordinates 
is convenient. Further, let p" denote the last r 1 = r — r0 elements of p (thus 
p = (p',p")). For a subgroup S' of information symbols xi,i = 1,2,... ,L, 
compute a parity vector pi e Fqr by using the parity-check matrix of the full 
code C by assuming all other subgroup information symbols xj ,j = i are 
all-zero kj -tuples (over Fq). Notice that the set of all ki + r -tuples (xi,pi) 
corresponding to the qki codewords for all possible q k1 -tuples over Fq for xi 
form a (ki + r,ki)q MDS code since it is a subcode of an MDS code. If this 
code is punctured in its last r 1 parity positions of pi to give p i, the result is an 
MDS (ki + r0,ki)q code.
The BPC code is then the set of qk codewords of the form
C ' = c ' = lx 1, p 11, x 2, p 2,..., xL, p L p " 
.
https://doi.org/10.1017/9781009283403.008 Published online by Cambridge University Press

7.3 Other Types of Locally Repairable Codes
201
The code is clearly a (k + r0L + r1,k)q BPC code although not an MDS code.
The property of interest for this code construction is
Theorem 7.17 ([19], theorem 1) The (k+r0L +r1,k)q BPC code constructed 
above can recover from r = n - k = r0 + r1 erasures.
Proof: Assume r erasures have occurred and let e be the number of erasures 
among the r0L positions of the subgroup parities p'1,... ,p'L, e < r. Consider 
two cases: e > r0 and e < r0.
If e > r0 puncture the code in all r0L group parity positions. The code 
remaining is by definition a (k + r1,k)q MDS code which can correct r1 
erasures. The actual number of erasures among the k + r1 places of the 
punctured code is r - e < r - r0 = r1 which the punctured code is able 
to correct.
In the case e < r0 first note that the subgroup parities pi satisfy the 
relationship
® P i = P', 
i=1
i.e., the sum of the first r0 positions of each of the subgroup parities add to the 
global parity of the original codeword. Then e of the erasures in the subgroup 
parity symbols can affect at most e<r0 of the first r0 positions of the global 
parity p. At least r0 > r0 - e of the first r0 positions of p can be computed 
correctly by the above equation. These parity positions together with the k 
information symbols and the r1 last global parity symbols of p form a (k + 
r00 + ri,k) MDS code which can correct r0 + r 1 erasures. Since r0 + r 1 > 
r0 - e + r1 = r - e which is the actual number of erasures remaining which 
the code can correct. ■
For the construction and decoding of GPCs the reader is referred to [19]. 
It is noted that both BPCs and GPCs require slightly more storage space than 
the equivalent MDS codes, but have significantly better access efficiency, the 
reconstruction read cost or the average number of symbols that must be read to 
access a missing symbol. The update complexity, the number of symbols that 
need to be accessed to update an information symbol, is the same as for MDS 
codes.
Zigzag Codes
Zigzag codes are introduced in [32] as codes that achieve a lower bound on the 
fraction of information that must be accessed in the use of MDS codes to repair 
a given number of erasures, referred to as the rebuilding ratio. The optimal 
https://doi.org/10.1017/9781009283403.008 Published online by Cambridge University Press

202
7 Locally Repairable Codes
update property is also of interest there, the minimum number of rewrites when 
an information symbol is changed. Here the term zigzag refers to the formation 
of parity symbols from an information array by using a single element from 
each row and column to form the parities - rather than simple row or column 
parities as is common.
Recall that the cut-set MBR bound for contacting n - 1 other nodes to repair 
a single failed node using (n, k) MDS codes is
m(n - 1) 
k(n - k)
Thus to repair a single node requires contacting a fraction of 1/(n - k) = 1/r 
of the total data stored in all nodes other than the failed node.
Additionally it is of interest to determine the number of stored symbols that 
are required to change when an information symbol is changed or updated. The 
optimal value for this is r + 1 achieved by the use of MDS codes.
Much of the work of [32] is concerned with constructing (k + 2,k)q MDS 
codes over some alphabet F to be specified. A brief description of the technique 
is noted.
For integers p,k consider a p x k array of information symbols over F, 
say {ai,j,i = 0, 1,...,p - 1,j = 0, 1,...,k - 1}. Two parity columns are 
added to the array and the overall (k + 2,k)q code is MDS in the sense that 
the failure of any two columns can be restored. The two added parity columns 
are constructed in the following manner. The first parity column, designated 
the row parity column R = {r0,r 1,... ,rp-1}t is simply a linear sum of the 
information elements in the same row, i.e., ri = 52k=0 ai, jai, j for some set of 
coefficients ai,j e F.
For the second parity column, denoted the zigzag parity column
Z = {z0,z1,...,zp-1 } ,
a set of permutations is first developed. These permutations are then used 
to determine the data symbols to be used in forming the zigzag parities 
- specifically for zigzag parity element zj the data symbols used will be 
{aj0,0,aj1, 1, ...,ajk-1,p-1} where the subscripts ji are chosen using the 
permutations to ensure that each ensemble has one element from each row 
of the data array. The element zj is then given by zj = k-o Pji,iaji,i for 
some set of constants ftji,i.
The coefficients in these parity sums are chosen to ensure the resulting 
code is MDS so that any two column erasures can be repaired. None of the 
parity coefficients can be 0 and it seems sufficient to choose the row parity 
coefficients as unity, 1. It is shown in [32] that a field size of 3 (e.g., F3) is 
https://doi.org/10.1017/9781009283403.008 Published online by Cambridge University Press

References
203
sufficient to achieve the properties required for the (k + 2,k) MDS code and 
renders the arithmetic involved simple. Generalizations of the construction for 
more parity checks are also considered in that work. It is further noted that a 
field size of4 is sufficient for the construction of a (k +3,3) MDS code. Notice 
that such codes are array codes and hence the alphabet when calling it an MDS 
code is as a column over a small finite field and the usual restriction of lengths 
of MDS codes less than the alphabet size has to be adjusted.
The general construction gives an (n,k) MDS code (over a certain size field) 
and all the constructions have an optimal rebuild ratio of 1/r under certain 
conditions.
Comments
The past two decades have seen considerable progress in the areas of code 
constructions and repairs and protocols for the distributed storage and retrieval 
of information. The economic importance of the problem and the gains that 
can be realized with these techniques are significant. Only a few of the many 
techniques developed are noted here.
References
[1] Blaum, M. 2013. Construction of PMDS and SD codes extending RAID 5. CoRR, 
abs/1305.0032.
[2] Blaum, M., and Plank, J.S. 2013. Construction of two SD codes. CoRR, 
abs/1305.1221.
[3] Blaum, M., Hafner, J.L., and Hetzler, S. 2013. Partial-MDS codes and their 
application to RAID type of architectures. IEEE Trans. Inform. Theory, 59(7), 
4510-4519.
[4] Blaum, M., Plank, J.S., Schwartz, M., and Yaakobi, E. 2016. Construction of 
partial MDS and sector-disk codes with two global parity symbols. IEEE Trans. 
Inform. Theory, 62(5), 2673-2681.
[5] Cadambe, V., and Mazumdar, A. 2013. An upper bound on the size of locally 
recoverable codes. Pages 1-5 of: 2013 International Symposium on Network 
Coding (NetCod).
[6] Cadambe, V.R., and Mazumdar, A. 2015. Bounds on the size of locally recover­
able codes. IEEE Trans. Inform. Theory, 61(11), 5787-5794.
[7] Cai, H., and Schwartz, M. 2021. On optimal locally repairable codes and 
generalized sector-disk codes. IEEE Trans. Inform. Theory, 67(2), 686-704.
[8] Calis, G., and Koyluoglu, O.O. 2017. A general construction for PMDS codes. 
IEEE Commun. Letters, 21(3), 452-455.
https://doi.org/10.1017/9781009283403.008 Published online by Cambridge University Press

204
7 Locally Repairable Codes
[9] Chen, B., Xia, S.-T., and Hao, J. 2017. Locally repairable codes with multiple 
(rj,8j)-localities. Pages 2038-2042 of: 2017 IEEE International Symposium on 
Information Theory (ISIT).
[10] Chen, B., Xia, S.-T., Hao, J., and Fu, F.-W. 2018. Constructions of optimal cyclic 
(r, 8) locally repairable codes. IEEE Trans. Inform. Theory, 64(4), 2499-2511.
[11] Chen, J., Shum, K.W., Yu, Q., and Sung, Chi W. 2015. Sector-disk codes and 
partial MDS codes with up to three global parities. Pages 1876-1880 of: 2015 
IEEE International Symposium on Information Theory (ISIT).
[12] Chen, M., Huang, C., and Li, J. 2007. On the maximally recoverable property 
for multi-protection group codes. Pages 486-490 of: 2007 IEEE International 
Symposium on Information Theory.
[13] Gopalan, P., Huang, C., Simitci, H., and Yekhanin, S. 2012. On the locality of 
codeword symbols. IEEE Trans. Inform. Theory, 58(11), 6925-6934.
[14] Gopalan, P., Huang, C., Jenkins, R., and Yekhanin, S. 2014. Explicit maximally 
recoverable codes with locality. IEEE Trans. Inform. Theory, 60(9), 5245-5256.
[15] Holzbauer, L., Puchinger, S., Yaakobi, E., and Wachter-Zeh, A. 2020. Partial MDS 
codes with local regeneration. CoRR, abs/2001.04711.
[16] Horlemann-Trautmann, A.-L., and Alessandro, N. 2017. A complete classification 
of partial-MDS (maximally recoverable) codes with one global parity. CoRR, 
abs/1707.00847.
[17] Hu, G., and Yekhanin, S. 2016. New constructions of SD and MR codes over 
small finite fields. Pages 1591-1595 of: 2016 IEEE International Symposium on 
Information Theory (ISIT).
[18] Huang, C., Chen, M., and Li, J. 2007. Pyramid codes: flexible schemes to trade 
space for access efficiency in reliable data storage systems. Pages 79-86 of: Sixth 
IEEE International Symposium on Network Computing and Applications (NCA 
2007).
[19] Huang, C., Chen, M., and Li, J. 2013. Pyramid codes: flexible schemes to trade 
space for access efficiency in reliable data storage systems. ACM Trans. Storage, 
9(1).
[20] Kamath, G.M., Prakash, N., Lalitha, V., and Kumar, P.V. 2014. Codes with 
local regeneration and erasure correction. IEEE Trans. Inform. Theory, 60(8), 
4637-4660.
[21] Li, J., and Li, B. 2017. Beehive: erasure codes for fixing multiple failures 
in distributed storage systems. IEEE Trans. Parallel Distribut. Syst., 28(5), 
1257-1270.
[22] Lidl, R., and Niederreiter, H. 1997. Finite fields, 2nd ed. Encyclopedia of Mathe­
matics and Its Applications, vol. 20. Cambridge University Press, Cambridge.
[23] Martinez-Penas, U., and Kschischang, F.R. 2019. Universal and dynamic locally 
repairable codes with maximal recoverability via sum-rank codes. IEEE Trans. 
Inform. Theory, 65(12), 7790-7805.
[24] Pamies-Juarez, L., Hollmann, H.D.L., and Oggier, F. 2013. Locally repairable 
codes with multiple repair alternatives. CoRR, abs/1302.5518.
[25] Papailiopoulos, D.S., and Dimakis, A.G. 2014. Locally repairable codes. IEEE 
Trans. Inform. Theory, 60(10), 5843-5855.
[26] Papailiopoulos, D.S., Dimakis, A.G., and Cadambe, V.R. 2013. Repair optimal 
erasure codes through Hadamard designs. IEEE Trans. Inform. Theory, 59(5), 
3021-3037.
https://doi.org/10.1017/9781009283403.008 Published online by Cambridge University Press

References
205
[27] Plank, J.S., Blaum, M., and Hafner, J.L. 2013. SD codes: erasure codes designed 
for how storage systems really fail. Pages 95-104 of: 11th USENIX Conference on 
File and Storage Technologies (FAST 13). USENIX Association, San Jose, CA.
[28] Prakash, N., Kamath, G.M., Lalitha, V., and Kumar, P.V. 2012. Optimal linear 
codes with a local-error-correction property. Pages 2776-2780 of: 2012 IEEE 
International Symposium on Information Theory Proceedings.
[29] Ravagnani, A. 2016. Rank-metric codes and their duality theory. Des. Codes 
Cryptogr., 80(1), 197-216.
[30] Song, W., Dau, S.H., Yuen, C., and Li, T.J. 2014. Optimal locally repairable linear 
codes. IEEE J. Select. Areas Commun., 32(5), 1019-1036.
[31] Tamo, I., and Barg, A. 2014. A family of optimal locally recoverable codes. IEEE 
Trans. Inform. Theory, 60(8), 4661-4676.
[32] Tamo, I., Wang, Z., and Bruck, J. 2013. Zigzag codes: MDS array codes with 
optimal rebuilding. IEEE Trans. Inform. Theory, 59(3), 1597-1616.
[33] Tamo, I., Papailiopoulos, D.S., and Dimakis, A.G. 2016. Optimal locally 
repairable codes and connections to matroid theory. IEEE Trans. Inform. Theory, 
62(12), 6661-6671.
https://doi.org/10.1017/9781009283403.008 Published online by Cambridge University Press

8
Locally Decodable Codes
Chapter 6 considered codes designed to be efficient in repairing a failed node 
or server in a distributed storage system by either minimizing the amount 
of information that must be downloaded from adjacent nodes to allow exact 
reconstruction of the missing data, the minimum repair bandwidth situation, 
or minimizing the amount of information that must be stored, the minimum 
storage situation. The notion of a locally repairable code was considered in the 
previous chapter as well as the notion of maximum recoverability. These are 
related to erasures of information. As seen in the previous chapter, repairing 
erasures is often related to solving matrix equations.
This chapter introduces the notion of locally decodable codes (LDCs) 
which allow the decoding of a few bits of a codeword that may be corrupted 
(with errors) by accessing a few coordinate positions of the codeword without 
decoding the entire codeword. The corrupted positions are thus unknown and 
any given position may contain an incorrect symbol. Thus with an LDC, if a 
database is encoded using very long codewords for efficiency, the retrieval of 
a single information symbol would be possible without the more extensive 
procedure of decoding the entire codeword. If the symbol of interest is in 
error the decoding technique of LDCs may allow its correction under certain 
conditions. The price to be paid for such a property is that the codes constructed 
are longer and less efficient. Such systems have a natural association with 
private information retrieval (PIR) schemes (considered in Chapter 9) which 
allow the retrieval of information from an array of servers without revealing 
to the servers which bits are of interest. The relationships between LDCs and 
PIRs will be discussed later in the chapter.
Efficient and practical schemes for local decoding are considered in this 
chapter. The next section discusses the basic properties of LDCs as well 
as their relation to a variety of allied concepts such as locally correctable 
codes (LCCs) and locally testable codes (LTCs). Section 8.2 describes several 
206
https://doi.org/10.1017/9781009283403.009 Published online by Cambridge University Press

8.1 Locally Decodable Codes
207
coding/decoding schemes for the local decoding setting - a rather extensive 
section. The final Section 8.3 considers the interesting and seminal work of 
Yekhanin on the construction of 3-query LDCs and related work, results which 
have had a significant impact on both LDCs and PIR schemes.
8.1 Locally Decodable Codes
Efficient codes in terms of error-correcting capability can have long block 
lengths. Thus it might be advantageous if a small number of informa­
tion symbols of interest could be computed from a few codeword symbols 
extracted from the possibly corrupted codeword without having to decode the 
entire word.
In this work, it will be assumed that n information symbols over an alphabet 
E are encoded into codewords of length m over an alphabet r:
C: E" —> rm
x I—> c(x).
Although at odds with usual coding notation, this notation has become standard 
in the literature and will be used here.
Informally, a (q,S,e) LDC is one that makes at most q queries to positions 
of the codeword and computes the value of the i-th information symbol from 
a corrupted version of the codeword with the probability e of the retrieved 
symbol being correct, provided that the fraction of errors in the word does 
not exceed S. Thus the overworked symbol S, used for various purposes in 
other chapters, including fraction of erasures allowed, represents the largest 
fractional number of errors to be allowed in a codeword for this chapter. 
Similarly, it is common in the LDC literature to use q for the number of queries 
and thus q is serving double duty as the finite field size when necessary. It will 
be clear from the context which it refers to.
Often the alphabets will be finite fields. As before, denote for any positive 
integer n, the set [n] = {1,2,...,n}, and the Hamming distance on a vector 
space over Fq by d(x,y) or, for normalized distance A(x,y) = d(x,y)/m for a 
code of length m. It is emphasized the symbol S in this chapter is reserved for 
the fraction of errors in a codeword.
Definition 8.1 ([42]) A code C: E" -^ Tm is said to be a (q,S,e) LDC if 
there exists a randomized decoding algorithm A which produces an estimate 
of an information symbol of the codeword such that
https://doi.org/10.1017/9781009283403.009 Published online by Cambridge University Press

208
8 Locally Decodable Codes
(i) For all information sequences x = (x 1 ,x2,...,xn) e Fq, with 
corresponding codeword c(x) e Fqm, and all vectors y e Fq1 such that 
d(c(x),y) < 8m
Pr[Ay(i) = xi] > 1 - e,
where the probability is taken over the random coin tosses of the 
algorithm A.
(ii) A makes at most q queries to the word y.
Definition 8.2 The (q,8,e) LDC code C is called smooth if the queries to 
locations of the database (codeword) x are probabilistically uniform.
It is noted there is also the notion of a smooth (q,c,e) LDC/decoding 
algorithm [17] where at most q queries are made and the probability any 
given coordinate position of the codeword is queried is at most c/q and the 
probability a position can be recovered is > 1 / | E | + e.
A closely related concept is the following.
Definition 8.3 ([21, 42]) A code C: En —> Tm is said to be a (q,8,e) LCC 
if there exists a randomized decoding algorithm A which produces an estimate 
of a coded symbol such that
(i) For all codewords c = (c 1 ,c2,... ,cm) e C c Sm and all vectorsy e rm 
such that d(c(x),y) < 8q
Pr[Ay(i) = ci] > 1 - e,
where the probability is taken over the random coin tosses of the 
algorithm A.
(ii) A makes at most q queries to the word y.
Note the difference in the two definitions is the estimation ofan information 
symbol (LDC) versus a codeword symbol (LCC). There are slight differences 
in the definitions of these concepts by various authors. The following is noted 
([21], claim 2.5) that a (q,8,c) LCC is a (q,8,c) LDC and that a code being an 
LDC does not imply it is an LCC.
The two results below follow easily ([17], theorem 2.7, [19], theorem 1):
Theorem 8.4 Let C be a (q,8,c) LCC. Then C is also a (q, 8q, e) LCC code.
Theorem 8.5 ([21]) If C c Fm is a (q,8,e) LCC, then C is a (q,8,e) LDC.
The notion of an LTC is similar in that in such a code it is of interest to 
query q coordinates of a word to determine probabilistically if it is in a given 
code. More formally [25]:
https://doi.org/10.1017/9781009283403.009 Published online by Cambridge University Press

8.1 Locally Decodable Codes
209
Definition 8.6 A code C is called an LTC with q queries if there is a 
randomized algorithm T which, given oracle access to a word j e Tm, makes 
at most q queries to y and has the following behavior:
(i) If y e C, then P r (Taccepts) = 1.
(ii) Pr(Trejects) > 4A(j,C) 
(normalized distance).
The notion of local testability is intimately connected with probabilistically 
checkable proof (PCP) systems, which is a proof that allows the correctness 
of a claim with some probability by looking at a constant number of symbols 
of the proof. This is an important connection for certain aspects of computer 
science and there is a large literature on the connection.
In general the notion of an LTC is quite different from that of an LDC 
[21] although certain codes (such as the Hadamard code described in the 
next section) are in both categories. That said, it appears that sparse codes 
(polynomial number of codewords in code length) and random linear codes 
[21] are both LDC and LTC. The class of codes investigated there are codes 
of large length m and minimum distance m/2 - ©(^m). This includes a class 
referred to as dual-BCH (m, t) where the codes for t = 1 correspond to the 
Hadamard codes described in the next section. An interesting result ([20], 
theorem 1) is that a linear code C with distance at least m/2 — Jtm is LTC 
using O(t/e) queries. In [20] such codes are referred to as almost orthogonal 
binary codes.
A notion related to LDCs is that of locally updatable codes (which are 
also LDC) is introduced in [4]. The notion asks to construct codes for which 
changing a few information bits in a codeword results in changing only a 
few bits of the corresponding codeword. In a sense the two notions of local 
updatability and local decodability are in opposition since one often requires 
a large distance for codes - hence changing a single bit will change many 
codeword bits - yet such codes are feasible.
An interesting related problem is the polynomial identity testing (PIT) 
problem where one is given a circuit computing a multivariable polynomial 
over a field and one is to determine if the polynomial is identically zero. Its 
relation to the LDC problem is considered in [6].
The first notion of an LDC appears in [1] and [34] and was first formally 
defined in [19]. As noted, much of the work on LDCs arises from work on PIR 
schemes.
An important aspect of LDCs is the trade-off between the local correctabil­
ity properties of the code, the query length and the code length. For a given 
number of queries and fraction of errors allowed, it is important to minimize 
the length of a code that achieves the given parameters. For the remainder of 
https://doi.org/10.1017/9781009283403.009 Published online by Cambridge University Press

210
8 Locally Decodable Codes
the section bounds on the lengths achievable for LDCs with a given set of 
parameters are discussed.
From [19] it is shown there is an upper bound on the length of the data 
sequence n for which a (1,S,e) LDC code can exist that is independent of the 
codeword length. An argument is given showing that from this observation, no 
such (1,S,e) LDC codes can exist.
In general the length of the codes needed for a (q,S,t) LDC is of interest.
Lower bounds on the length m of codewords for given data length n have been 
derived with particular interest in 2 and 3 query codes including such works as 
[19, 24, 36] and [29] where it is shown that for C a binary linear (2,S,e) LDC 
we must have1
1 Recall that f(n) = Q(g(n)) if, for k > 0,3n0 st Vn > n0, | f (n) |> k(g(n).
m > 2
Sn 
1-2 g
Also [12] for linear binary codes (i.e., C: {0,1}n —> {0,1}m ) if C is a linear 
binary (2,S,e) LDC and n > 8/eS, then
m > 2W4).
This is improved in ([6], theorem 1.2) to
m > 24-1.
a bound which is also expressed as m = 2O(n).
The 3-query case has attracted considerable attention. The best lower bound 
for linear codes for this case, for many years, was
m = &(n2/ log log n).
However, somewhat dramatically it was shown in [41] that 3-query LDCs 
exist, assuming there are an infinite number of Mersenne primes, and that for 
a Mersenne prime of the form p = 2t - 1, the code length satisfies
m = exp n1/t
and since the largest known such prime has t = 32,582, 657 > 107 there exists 
a 3-query LDC for every n of length
-7 \
m = exp n10
and more generally of length
m = exp (n1 /loglogn),
https://doi.org/10.1017/9781009283403.009 Published online by Cambridge University Press

8.2 Coding for Local Decodability
211
a very significant improvement over the above bound. This 3-query case is 
discussed in Section 8.3.
It is interesting to recall that a polynomial f (x) e Fq [x] of degree d < q/2 
can be recovered from its values at 2d points even if a small fraction of its 
values are corrupted (in error). It is shown [33] that a k-sparse polynomial of 
degree d < q/2 can be recovered from its values at O(k) randomly chosen 
points even if a small fraction of them are corrupted. The result can be shown 
from the discussion in Chapter 13.
Section 8.2 considers a few of the techniques to encode and decode LDCs. 
In keeping with the aim of this volume, this is a rather extensive section, 
hopefully of interest to readers.
The final Section 8.3 is on the class of 3-query codes of Yekhanin [41] 
which represents a breakthrough in the construction of such codes in terms of 
the short lengths they achieve compared to prior constructions. The work of 
Efremenko [9] and Gopalan [13] and in particular Raghavendra [32] on the 
problem is also noted.
Many of the results of the chapter assume linear codes but this will be stated 
explicitly for the results.
8.2 Coding for Local Decodability
The interest in constructing LDCs is to have the code rate high, the number 
of queries low and the probability of correct decoding high. Of course the 
parameters are conflicting and the trade-off is of interest.
Several classes of interesting LDCs have been investigated in the literature, 
including Hadamard codes, generalized Reed-Muller (GRM) codes, multiplic­
ity codes and matching vector codes. The construction and local decoding of 
them is briefly discussed. The reference [44] is a concise reference for such 
topics.
Hadamard Code
Hadamard matrices arise out of the following ([3], Hadamard’s theorem 
16.6.1): If A = (aij) over R is such that | aij |< 1 for all i,j, then | det(A) |< 
nn/2 and equality is achieved iff aij =±1 for all i,j and AAt = nIn . Such a 
±1 matrix is called a Hadamard matrix.
Suppose A is an n x n Hadamard matrix. Any column can be multiplied 
by ±1 without changing it being a Hadamard matrix and hence without loss 
of generality it can be assumed the first row of the matrix contains all +1’s.
https://doi.org/10.1017/9781009283403.009 Published online by Cambridge University Press

212
8 Locally Decodable Codes
The signs in any given column in rows 2 and 3 are either {+, +}, {+, -}, {-, +} 
or {-, -}. If a,b,c,d denote the number of columns containing these combi­
nations, by the orthogonality assumption between pairs of the first three rows, 
it can be shown that a = b = c = d = n/4 and hence 4 | n for the order of 
any Hadamard matrix.
It is long conjectured that Hadamard matrices exist for all orders divisible 
by 4. Denote by Hn a 2n x 2n Hadamard matrix. The following construction 
can be used to give Hadamard matrices of order a power of 2:
1
1
H1 =
-1
1
Hn = H2 ® Hn-1,
and construct recursively the 2n x 2n Hadamard matrices, where ® denotes 
tensor product. This construction yields a sparse sequence of such matrices. 
Numerous other constructions, however, give other matrices whose orders are 
divisible by 4. The smallest order for which a Hadamard matrix of order n has 
not been shown to exist currently appears to be n = 668.
Another construction of a Hadamard matrix Hn of order 2n that will be 
convenient for the discussion on Hadamard codes is as follows. Label the rows 
and columns of H'n with all 2n binary (0,1) n-tuples and let
Hn = (hUv), u, v e Fn, h= (-1 )(u,v
and let h'u be the row corresponding to the label u. Clearly each row and 
column contains an equal number of +1s and -1s and any two distinct 
columns are orthogonal as are any two distinct columns and H'n is a Hadamard 
matrix of order 2n . A related matrix is
Hn = (hu,v), hu,v = (u,v) e {0,1}, u,v e F2n (inner product).
Clearly each row and column of this matrix has the same number of zeros as 
ones and any two distinct rows or columns have a Hamming distance of 2n-1.
The two matrices are related by
Hn -^ Hn 
(-1 )(uv ^ (u,v)
-1 ^ 1
+ 1 ^ 0.
Encode a binary {0, 1} information n-tuple u e F2n to be the 2n-tuple hu, 
the row of Hn corresponding to u.
https://doi.org/10.1017/9781009283403.009 Published online by Cambridge University Press

8.2 Coding for Local Decodability
213
Let C be the set of codewords which are the rows of Hn . Suppose the binary 
2n-tuple codeword hm of the information n-tuple m e F2 is transmitted and the 
word r e F2 is received with at most S2n errors. To show the code is an LDC 
suppose the bit in the coordinate position w e F2n of hm, i.e., hm,w is sought. 
Choose at random v e F2 and query positions w and v ® w of the received 
word r, i.e., rv and rv®w. These two received word positions are uncorrupted 
with probability (1 - S)2 « 2S. In this case
rv ® rv®w = hm,v ® hm,v®w = (m,v) ® (m,v ® w) = (m,w) = hm,w.
By choosing m appropriately one can retrieve information bits and it is seen 
the Hadamard code is a (2,S, 1 - 2S) LDC.
The Hadamard code is also a 3-query LTC [21]. To test whether the received 
word r e F22n is a codeword, query the word in coordinate positions u, v e F2n 
and u ® v and accept it as a codeword if
ru ® rv = ru ® v.
The analysis of this test is more complicated as various errors can result in 
acceptance and is not considered here. While the simplicity of the decoding 
procedure is attractive, the low rate of such a code is a severe limitation.
Generalized Reed-Muller Codes
It is claimed the class of GRM codes GRMq (d, m) (d the degree of polynomial 
used, not minimum distance) can be used as a locally correctable (d+ 1,S,(d + 
1 )S) code, i.e., as a code for which a coordinate position in position a e Fm 
can be decoded by making at most (d+ 1) queries to other codeword positions 
with a probability of being correct ofat least 1 - (d+ 1)S. Recall from Chapter 
1, such codes have length qm , the coordinate positions labeled with a e Fqm 
and using m-variate monomials of degree at most d to generate rows of the 
generator matrix yields an (n,k,dmin)q codes where
n = qm,k= m and dmin = qm-1(1 - d/q), d < q - 1. 
d
To see how local correctability is achieved with such codes, consider a 
codeword cf = (f (u1), f (u2),...,f (uqm)) (for some ordering of the points 
u 1 ,u 2,... ,uqm of Fqm) for some m-variable polynomial f of total degree < d. 
It is desired to retrieve the coordinate position represented by the element u, 
i.e., the value f (u),u e Fqm of the transmitted codeword by querying a number 
qm
of other positions of the received word y e Fq .
https://doi.org/10.1017/9781009283403.009 Published online by Cambridge University Press

214
8 Locally Decodable Codes
Consider the bivariate case (m = 2). Since the code is coordinated by a 
two-dimensional vector space choose another point v e Fq^ and consider those 
q - 1 coordinate positions that fall on the line {u + kv,k e F*}. Choose a 
subset S of these (q - 1) points, | S |= d + 1, i.e., (d + 1) distinct points 
u + kjv,i = 1,2,... ,d + 1 on the line and let zi = f(u + ki v). Determine the 
univariate polynomial h(x) of degree d over Fq that interpolates these (d + 1) 
points in that h(ki) = zi,i= 1, 2,...,d+ 1. Thus h is a univariate polynomial 
of degree at most d whose values are known at d + 1 elements and hence can 
be evaluated. It follows that h(0) = f (u). This value h(0) will be the correct 
value if the values zi,i = 1, 2,...,d+ 1 are uncorrupted, i.e., are the correct 
values, and this happens with probability (1 - S')d +1 « 1 - (d + 1 )S. It easily 
follows that the GRM code GRMq(d,m) is a ((d +1 ),S, (d +1 )S) LCC. (Recall 
that S is the maximum fraction of received code symbols that are corrupted.) 
The decoding of the multiplicity codes of the next subsection will involve a 
similar process.
That GRM codes are LTC codes is established in a similar manner (see, 
e.g., [25]).
The construction of the multiplicity codes to be considered next, as with cer­
tain of the codes treated in Chapter 13, depends on the theory of multiplicities 
of zeros of multivariable functions over a finite field. For the properties needed 
the notion of the Hasse derivatives is useful and a brief account of these is 
given in Appendix B.
Multiplicity Codes
Multiplicity codes, proposed by Kopparty et al. [26, 27], can be viewed as 
an interesting generalization of the GRMq (d,m) codes. While somewhat 
technical to describe, they have excellent properties and local decoding algo­
rithms reminiscent of those for GRM codes. The original papers of Kopparty 
et al. [26, 27] develop their basic properties and local decoding algorithms. 
Chapter 3 of [42] contains proofs of some of the background material.
The bivariate case will give an easy introduction to these codes [26]. Results 
for the more general m-variate case will then be stated. The code will be 
indexed by elements of F- hence this bivariate code is of length q2 - and 
let f (x1,x2) be a bivariate polynomial of total degree at most d = 2q(1 - S) 
for some fixed S (error fraction). The codeword corresponding to f e Fq [x] 
will be
cf(a) = ffWfa^fa^X a e F2, and cf = (c/(a 1),. 
dx dy 
q 
.,cf (aqm-1))
https://doi.org/10.1017/9781009283403.009 Published online by Cambridge University Press

8.2 Coding for Local Decodability
215
and each coordinate position of a codeword is a 3-tuple of elements of Fq . 
(As noted in Appendix B the first partial ordinary and Hasse derivatives 
are identical.) Denote the codeword coordinate corresponding to a e FJ; as 
cf (a) for f of degree at most d. The code symbols are now triples over Fq 
corresponding to the function f and its two partial derivatives, and the code 
length is q2, corresponding to the elements of F2q. The symbol alphabet for 
this code designated E = FJ3.
To compute the Hamming distance of the code, consider a codeword cf, 
f e Fq-d[x 1 ,x2] of degree at most d. What is the minimum weight such 
a codeword can have? For the codeword cf to have a zero at position a e 
F2q the polynomial f must have a zero of multiplicity 2 (in order to have the 
polynomial and its two partial derivatives to be zero) and hence the codeword 
symbol over E to be 0. The maximum number of zeros of multiplicity 2 that 
f can have is governed by Equation B.2 of Appendix B, namely
d
.
2q
Thus the minimum fractional weight of a codeword in this code (hence the 
minimum fractional distance of the code) is
1- d
2q,
recalling again that the d here is the total degree of the bivariate polynomials 
used in the code construction. In the literature the fractional distance quantity 
is often designated S. For this chapter S is reserved for the assumed maximum 
fraction of corrupted codeword positions of a received word in error under 
local decoding (and also not to be confused with the erasure probability 
used in previous chapters for the BEC). This gives the relationship between 
the minimum fractional distance of the code and the degree of polynomials 
used. The total number of monomials of degree at most d is d+22 (Equation 
1.11) and the space of such monomials is clearly a vector space of this 
dimension over Fq, i.e., the number of codewords corresponds to the number 
of Fq coefficients of such monomials which is q(d+2 2) . Thus this code has the 
parameters
q2,( d + 2), dmin - 1 - d/2q ) , over S = F3- 
2 
s
Since each coordinate position of the code has a symbol over E, it can also be 
viewed as code of dimension d+22 /3 over Fq. Note that these code parameters 
https://doi.org/10.1017/9781009283403.009 Published online by Cambridge University Press

216
8 Locally Decodable Codes
* = £D * V d 
n 3q2 
6 q
can be compared to those of the GRM code for m = 2, namely (q2, d+2), 
1 - d/q .
The local decoding of the multiplicity codes will be considered below. 
The code can be viewed as a 3 x q2 array over Fq and has rate
2
= 2(1 - A)2/3 (as d/q = 2(1 - A)), 
(8.1)
a rate that approaches 2/3 (as opposed to the maximum rate of 1/2 for the 
bivariate GRM case considered earlier) where A is the fractional minimum 
distance. Thus the process of requiring codeword polynomials to have zeros 
of higher multiplicities has led to codes with improved minimum distance and 
rate at the expense of larger field sizes.
Before considering the decoding techniques for these codes and the fact 
they are locally decodable, the general construction of multiplicity codes is 
considered.
As before, let i = (i 1 ,i2,... ,im) be a set of nonnegative integers and its 
weight be wt(i) = 
jm=1 ij . As before, denote by x the set x1,x2,...,xm
and Fq[x1,x2,...,xm] by Fq[x]. For i = {i1,i2,...,im} let xi denote the 
tt m 
i j
monomial jm=1 xjj and its total degree by wt(i). For the general case, rather 
than each codeword symbol being just a polynomial evaluation along with 
first derivatives as in the bivariate case considered above, higher-order Hasse 
derivatives are used (see Appendix B). Specifically we have:
Definition 8.7 Let f(x) e Fq [x] be an m-variable polynomial over Fq of 
degree at most d. Then the 5-evaluation of f at a point a e Fm is the column 
vector of all Hasse derivatives of order less than s. There are m+ms-1 such 
derivatives (i.e., positive integer m-tuples i with wt(i) < 5) (from Equation 
1.11 with d = (5 - 1)) and such a vector will be denoted f[<5]. Denote the 
m+5-1
alphabet for the set of such vectors by E = Fq m v viewed as a column vector 
of length m+m5-1 over Fq, the vector of all Hasse derivatives of f of order less 
than 5.
Note that f [<5] denotes a vector of length m+m5-1 over Fq while Fq<k [x] 
denotes polynomials over Fq of degree less than k.
The parameters of the multiplicity codes are determined by the positive 
integers m, 5 and d and the base field Fq (with d the maximum total degree 
of polynomials - not code distance). The codes will be of length qm, 
coordinate positions indexed by the elements of Fqm . The information symbols 
corresponding to a codeword or m-variable polynomial f e Fq [x] will serve 
https://doi.org/10.1017/9781009283403.009 Published online by Cambridge University Press

8.2 Coding for Local Decodability
217
as the coefficients of the monomials of the polynomial, each such polynomial 
generating a codeword by evaluation, and for a polynomial of degree at most 
d there are mm+d such coefficients/monomials (Equation 1.11). Clearly the 
set of all such polynomials is a vector space of this dimension over Fq . 
Furthermore the codewords generated by the set of all such monomials are 
linearly independent and could be used as rows for the generating matrix for 
the code.
For a polynomial f e Fq [x] of degree at most d the corresponding 
codeword will be denoted
cf = (f[<s](a), a e Fqm
(f [<s](a1),f [<s](a2),...,f [<s](aqm)), a1,a2,.
aqm e Fqm ,
(for some ordering of the elements of Fqm ). The information symbols are taken 
as the coefficients of the monomials in forming a codeword polynomial. In 
determining the code rate it is convenient to use the Fq symbols. There are 
mm+d Fq coefficients available for the monomials and the number of Fq 
symbols in a codeword, viewing the codeword as a (m +--1) x qm array of 
such symbols, gives the code rate as
m+d
. V m ’ 
.
(m +m 
qm
The codewords are viewed as having coordinate symbols that are over 
E = q( m 1), or equivalently as (m +--1)-tuples over Fq, corresponding 
to the Hasse derivatives of all orders less than s . In computing the code 
minimum distance as a code over E, meaning coordinate positions of two 
distinct codewords are the same only if all Hasse derivatives have the same 
value. The normalized minimum distance of the code is obtained as for 
the previous case, namely using the result of Equation B.1 of Appendix B, 
the maximum fraction of zeros in a codeword is at most d/sq and hence the 
normalized minimum weight of a nonzero codeword (codeword polynomial), 
the normalized distance of the code is at least 1 - d/sq.
The properties of the code are summarized in the following:
Lemma 8.8 ([26], lemma 9, [42], lemma 3.7) Let Cm, d,s, q be the multiplicity 
code of m-variable polynomials of degree at most d and order s -evaluations 
over Fq. Then the code over has:
length = qm, rate = —m, —, normalized distance = 1 —— 
(m +m-1) qm 
sq
https://doi.org/10.1017/9781009283403.009 Published online by Cambridge University Press

218
8 Locally Decodable Codes
The code rate can be lower bounded as follows:
m+d
. v m 1
(m +m-1) qm
nm=01 (d + m - j) 
nm=i (s+m - j)q
n m-1 d( 1 + (m - j)/d) 
nm=i(qs( 1 + (m - j)/s)
The approximation used in the last line of the development includes
(
1 \ m
------  | > (1 - x)m > (1 - mx), 0 < x < 1.
1+x
Consider now the local decoding of the bivariate multiplicity codes dis­
cussed earlier. The code has length q2, dimension d+d 2 and minimum distance 
(1 - d/2q), consisting of all bivariate polynomials of degree at most d and 
derivatives less than s = 2. Denote partial derivatives (Hasse derivative is the 
usual derivative for s = 2) of f(x 1 ,x2) wrt to xi,i = 1,2 by fxi = df/dxi. As 
noted earlier the coordinate position corresponding to a e F2 in the codeword 
corresponding to the bivariate polynomial f (x 1 ,x2) e Fq [x 1 ,x2] is
/ 
\t / 
df(a) df(a)\t 
/ [<2] 
\t
cf(a) = [faaffx 1 (a),fx2(a)) = {f(a), 
,—-------) = If[ 2](aH .
dx 1 
dx 2
Consider the line L = {a + kb,k e Fq, a,b e F2q} and define the univariate 
polynomial
h(k) = f(a + kb), k e Fq.
An estimate of the codeword coordinate position a e Fq2 is desired: cf (a) = 
(f (a),fx1(a),fx2(a))t is desired. Notice that ([45])
h(k) = f(a + kb),a,b e F2q
dh(k)
—)— (k) = b 1 fx 1 (a + kb) + b 2 fx 2 (a + kb) 
dk
where h(k) is of degree at most d = 2q( 1 - A). If we knew the function 
h, then
h( 0) = f(a) and d~^)(. 0) = b 1 fx 1 (a) + b 2 fx 2 (a). 
dk
https://doi.org/10.1017/9781009283403.009 Published online by Cambridge University Press

8.2 Coding for Local Decodability
219
However, the partial derivatives of the function appear as a sum and their 
individual values are required. To overcome this problem a second “direction 
vector” b' is chosen and the above process is repeated to give
h( 0) = f (a) and ^h-^ 0) = b 1 fx 1 (a) + b 2 fx 2 (a) 
ok
and if the two vectors b, b' e F2 are chosen not to be colinear with a, the 
q
equations can be solved to give
cf (a) = (f (a),fx1(a),fx2(a))t.
Thus if at most 2q queries are made and these 2q queries are to uncorrupted 
places of the codeword, which occurs with probability at least (1 - 5)2q, the 
above procedure yields a (2q, 5,1 - (1 -5)2q) LDC code. It is noted [26,27,45] 
that these bivariate multiplicity codes for polynomial degree 2q(1 - 5) (see 
Equation 8.1) have rates approximately
3 (1 -A)2,
or a maximum rate of 2/3 compared to the GRM codes which had rates only 
up to 1/2.
The decoding of the general multiplicity code with the parameters of 
Lemma 8.8 is a generalization of the bivariate case and ordinary partial 
derivatives (s = 2) case to higher-order Hasse derivatives. The procedure 
is technical but straightforward and requires obtaining values of a sufficient 
number of direction vectors that solutions for the individual Hasse derivatives 
(rather than linear sums of them) can be obtained via Gaussian elimination 
of a matrix equation. To get rates approaching 1 requires increasing both 
the number of variables and the number of Hasse derivatives considered. 
The query complexity compared to the higher-dimension space is improved. 
Local decoding of such codes, while a straightforward generalization of the 
bivariate case, is quite technical. Good discussions of the technique appear in 
several places, including [27, 42, 45]. The following results are representative 
(notation adapted to that used here):
Lemma 8.9 ([27], theorem 3.6) Let Cm,d,s,q be a multiplicity code of order s 
evaluations of degree d polynomials in m variables over F q. Let A = 1 - d/sq 
be the lower bound for the code’s normalized minimum distance. Suppose 
q>max 10m, d + 6s
--------, 12 (s + 1)
s
then C is locally correctable from a fraction &/10 errors with (O(s)m • q) 
queries.
https://doi.org/10.1017/9781009283403.009 Published online by Cambridge University Press

220
8 Locally Decodable Codes
The multiplicity codes will also be used to construct codes for PIR retrieval 
(see Section 9.2) as well as batch codes (see Section 10.1).
Matching Vector Codes
Matching vector codes [44, 45] share common elements with the GRM code 
decoding procedure discussed above. For positive integer m, m | (q - 1) let 
Cm = {g) C Fq be a cyclic multiplicative subgroup of order m of the finite 
field Fq* generated by g = a(q-1)/m for a e Fq a primitive element. The 
exponents of g are in Zm , the integers modulo m, and the following definition 
describes certain subsets of such exponent vectors:
Definition 8.10 Let S C Zm \{0}, | S |= s and consider two families of 
subsets of vectors of Znm, U = {u(1),...,u(k)}, V ={v(1),...,v(k)}.The 
sets U, V are said to form an S-matching family if the following conditions 
are satisfied:
(i) (u(i), v(i)) = 0,i= 1, 2,...,k, the inner product in Zm
(ii) For i = j, i,j e [k], (u(\ v(j)) e S c Zm.
The set S is to be specified. The definitive works for the construction of 
such matching sets are [14, 15].
The coordinates of the vectors will only appear in exponents of g e Fq, an 
element of order m, gm = 1, for m | (q - 1).
For a vector w e Znm, w = (w1,w2,...,wn) define the exponential
gw = (ww 1 ,gw2,... ,gw^ (exponents modulo m)
and the corresponding set on n-tuples over Zm :
Mw,v = {gw+^v | ^ e Zm}.
Notice that gw and the set Mw,v are in Cm = {g}n C Fq (i.e., they are n- 
tuples of Fq elements). Similarly define the monomial in the variables z = 
(z1,...,zn) 
n
mu(z) = 
ziui e Fq [z] = Fq[z1,...,zn]
and notice that
n
mu(gw+Zv) = H gli<-(^w + ^Vi) = g(u,w)
(gk)(u,v)
https://doi.org/10.1017/9781009283403.009 Published online by Cambridge University Press

8.2 Coding for Local Decodability
221
Consider the following univariate polynomial
mu,v,w(y) = gMyM e Cm [y] C Fq [y], y = gk.
To encode the information vector x e Fq evaluate the polynomial
F(z) = F(z 1,... ,zn) = £Xi • muj (z) = 
n Z^j 
(8.2)
j = 1 
j = 1 
€=1
at all mn points of Cmn and for the sequences u(j) in the S matching family. 
Notice the role of only one set U of the set of matching vectors here. Notice 
also that while the cyclic group is multiplicatively closed it is not necessarily 
additively closed and so the evaluations of F(z) are in Fq .
To show the decoding process, assume the information symbol xi e Fq 
is required from the codeword corresponding to F (z). A random w e Znm is 
chosen and the valuations of F(z) are retrieved at the s + 1 locations
jgw+W‘)}, k e{0,1,2,...,s} c Zm
(the inner products of the matching sets are in S, |S|= s) and suppose
F (gw+W-)) = ck e Fq.
Form the unique polynomial h(y) of degree s such that h(gk) = ck .It 
follows that
k 
r 
,
h(v) = VX i'u ' 'w \-u ' 
u(i) = L(i) u& 
u((A u(i'> e Z
(y ) = xj g 
y , u = u1 , u2 , . . . , un , uj e m .
In the summation when j = i the exponent of y is (u(i), v(i)) = 0by 
construction of the matching sets and hence corresponds to the constant term 
in the polynomial, i.e.,
h(0) = x . !/ W or Xi = h(0)Ig(lu('i^.
If the (s+ 1) places of the code are error-free, the correct symbol is retrieved 
by this process. If the error rate of the code symbols is uniformly S, the 
probability all places of the code queried are error-free is (1 - S)s +1 which 
is at least 1 - (s + 1)S and the probability the retrieved symbol is in error is at 
most (s + 1)S. Hence the matching vector codes are (s + 1,S,(s + 1)S) LDCs.
These codes are studied further in [5, 7, 8, 10, 23, 32, 33, 40].
https://doi.org/10.1017/9781009283403.009 Published online by Cambridge University Press

222
8 Locally Decodable Codes
Lifted Codes
The interesting notion of a lifted code was introduced in [2] and studied further 
in [16] and is briefly noted here. From the definition of a Reed-Solomon code 
(Chapter 1)as
C' = RS(d, q) = {C/ = (f(a 1), f (a 2), ..., f(an)), f e F q <d [x ], deg (f) < d} 
the evaluations of all polynomials over Fq of degree at most d at n distinct 
points of ai e Fq . Consider also the GRM code
C = GRM(d,q,m) = { (f(a 1 ),f(a2),..., f (aqm)),
f e Fq<d[x1,x2,...,xqm], degf < d,aj e Fqm , 
where the ai are the qm distinct elements of the vector space of m-tuples over 
Fq . The affine transformation
y = Ax + b, A an m x m nonsingular matrix over Fq, b e Fm
is viewed as a transformation (a permutation) on the coordinate positions of 
the code C.Ifcf (a) is a codeword in C, then cf (Aa + b) (the coordinate 
positions are permuted) is also a codeword of C, the code is referred to as 
being affine-invariant.
In the vector space of m-tuples over Fq consider the set
L = at + b , a, b e F qm,t a variable with values in F q .
If one restricts the code C to these q points (to obtain a code of length q by 
deleting all other coordinates) one obtains a Reed-Solomon code for every 
such affine transformation. The minimum distance of such a “projected” code 
is determined by the maximum degree of the polynomial interpolating the 
points on the line.
It is noted [28] that a limitation of GRM codes is that they have low 
rate. If one could determine a code C by restricting the code to use only 
m-variate evaluation polynomials (rather than m-variate polynomials of degree 
at most d) which are low-degree univariate polynomials when restricted to 
every line of the affine geometry, one might obtain a more efficient code. 
This is in fact the case: [16, 28]: if C is defined as the code resulting by 
using all polynomials whose restrictions to lines of the geometry are low- 
degree univariate polynomials, one obtains a code that is much larger than 
the corresponding GRM code. Such a code C is referred to as a lifted Reed- 
Solomon code.
Such lifted codes find application to codes with disjoint repair groups [28, 
30] and in batch codes [18, 31].
https://doi.org/10.1017/9781009283403.009 Published online by Cambridge University Press

8.3 Yekhanin’s 3-Query LDCs
223
8.3 Yekhanin’s 3-Query LDCs
In a groundbreaking contribution [40, 41] Yekhanin constructed a class of 
3-query LDCs. Previous 3-query LDCs were known with exponential length 
(in data length) and a lower bound that was approximately the square of 
the length. The work of Yekhanin lowered these code lengths considerably. 
The original work uses notions of algebraically nice and combinatorically 
nice sets which require some effort to describe. It also required the existence 
of an infinite number of Mersenne primes (long conjectured to be true) to 
yield 3-query codes of length exp(nO(1/ log log n)) for data length n. Efremenko 
[9, 10] modified the construction to give an unconditional proof of 3-query 
codes of length exp (exp (o(Vlog n log log n)))• Gopalan [13] showed 
Efremenko’s work can be seen from the viewpoint of Reed-Muller codes. 
The work of Kedlaya and Yekhanin [22, 23] further expands on [41].
Raghavendra [32] captures the essence of the Yekhanin work in a simple 
and elegant manner (although still requiring Mersenne primes) and this section 
follows that approach. Interestingly, these works all use the notions of match­
ing vector sets discussed earlier for the matching vector codes with a specific 
definition/restriction of the set S. To recall the definition, two families of 
vectors (m-tuples over Fp for some prime p) U ={u(1),u(2),...,u(n)},V= 
{r(1), r(2),..., v(n)}, each vector of length m, u(i),v(j) e Fpm, are said to be 
matching if:
(i) For all i e [n], (u(i),v(i)) = 0.
(ii) For all i,j e [n] such that i = j, (u(i),v(j)) = 2rij mod p for some 
integer rij, i.e., the set S contains powers of two.
The construction uses two finite related fields Fp and F2t and it will be 
assumed 2t - 1 = p is a prime. Primes of the form 2t - 1 are referred to 
as Mersenne primes and as noted, a long-standing conjecture is that there are 
an infinite number of such primes. The largest currently known such prime 
appears to be 282,589,933 - 1 (some 24,862,048 digits - although the search 
for such primes is an active area and a distributive computational effort known 
as the Great Internet Mersenne Prime Search (GIMPS) is ongoing). It is noted 
that if 2t - 1 is a (Mersenne) prime, then t is a prime and t | (2t - 2) (e.g., 
25 - 1 = 31 is a Mersenne prime and 5 | (25 - 2)).
The structure of the two finite fields and the matching vector sets are 
described. Assume that p = (2t - 1) is a Mersenne prime and consider the 
two finite fields Fp and F2t. The nonzero elements of F22i form a cyclic group 
(g} of order p = (2t — 1) assumed to be generated by the primitive element 
g e F2t, gp = 1.
https://doi.org/10.1017/9781009283403.009 Published online by Cambridge University Press

224
8 Locally Decodable Codes
In Fp the integer 2 generates the group G = (2) = {1,2,22,..., 2t-1} 
(multiplicative) of order t since by assumption 2t = 1inFp . Notice that since 
it will be assumed that p = (2t — 1) is prime and Fp is a cyclic group, if 
£ = p-1, then the £-th power of any element in Fp is of order t and hence in 
G, i.e., a power of2.
It is desired to define matching sets of vectors U and V such that 
(u(i), v(i)) = 0 and for i = j, ((u(i), v(j)) = 2r for some integer r (which 
may depend on the particular vectors). The reason for this is that for the proof 
to follow it will be useful to invoke the property of fields F2t of characteristic 
2 that
a2r + b2r = (a + b)2r.
To define the matching vector sets, let M be an integer larger than p. The 
sets will each contain n = pM—1 vectors, the vectors will each be of length 
m = M(p—1 )/t = M^l = (p — 1 )/t and will be defined over Fp.
It is noted [32] that n = (pM1) > (y) and since m = M(p—1 )/t then 
m = O(n1/t) and ifwe choose M = 2p the code length is m = nO(1/ log log n), 
a significant shortening over previous constructions.
Let 1M e FM be the M-tuple of all ones. Consider the set of n = p—pp) 
binary (0, 1) incidence vectors of length M, of all subsets of the set [M] of size 
p — 1, denoted by {m (l),i = 1,2,... ,n} and define a second set by v (l) = 
1M - M (i) (over Fp) - the binary complements. Consider the ordinary tensor 
product of vectors (of length M2):
p'(i) ® p'(i) =
p 1 .p 2ii, ...,p Mi),^ 2ii[p p ,p 2ii, ...,p Mi},..
. . -,P Mj P ^’P 2li, . . -’P M
= ( m ' (i) r
The extension to an £-fold tensor - (m (l))®£ is clear.
Define the sets
U ={u() = (m'(af\i = 1,2, ... ,n] and
V ={ v(i) = (v' (i) f *,i = 1,2,...,n ],
the £-fold tensor product of the vectors m (l) and v (l), respectively. That the 
sets U and V are matching follows from the fact that
(^i'), v(j)} = ( (M'(i)f", (V'j))®£ ) = (M'(i), v'j)}' 
https://doi.org/10.1017/9781009283403.009 Published online by Cambridge University Press

8.3 Yekhanin’s 3-Query LDCs
225
and that (^(i),v(i)) = 0 and that the offset values (^((\v(i)), i = j are I-th 
powers of an element of F* and hence, as an I-th power, is in G, i.e., a power 
of 2 - which is the reason for the particular parameters chosen. The sets U,V 
are matching sets of vectors of length m = Me and size n =| U |=| V |= QM1) 
over Fp.
To consider the encoding and decoding algorithms for the LDC codes to be 
defined and their properties, recall p = (2t - 1) is a prime and g a primitive 
element of the finite field of characteristic 2, F2*t, and note that g2t -1 = gp = 1. 
Define the integer y such that
1 + g + gY = 0, Y< 2t - 1. 
(8.3)
Let U,V be matching sets (over Fp) derived above and define the functions
f: F Pm -+ F* t, 
i = 1,2,...,n
x | ^ (U^- X)
x 1 r g
and note the function is a homomorphism in that fi(x + y) = fi(x')fi(y'),Vx, 
y g F m.
p
The following relationship ([32], observation 3.1) follows directly from the 
definition of y and the fact that squaring in fields of characteristic 2 is a linear 
operation and that by definition of matching sets (u'i, vij)) = (1 - j2r for 
some positive integer r (depending on i and j) . Thus
fi (x) + fi(x + v(j)) + fi(x + y v(j))
= g(u(i), x) + g(u(i), (x+v(j))) + g(u(i), (x+yv(j))
= 
g(u(i), x) 1 + g(u(i),v(j)) + gy (u(i),v(j))
= 
' giil<ii, x)( 1 + g + gY)2r = 0, 
if i = j
g(u(i), x)(1 + 1 + 1) = g(u(i),x), ifi = j. 
.
With these basic properties the codes, and local decoding algorithms for them, 
can be formed. The codes will be of length pm with coordinates indexed by 
elements of Fpm and coordinate values from F2t - it will encode information 
a = (a 1 ,a2,... ,an) g Fnt by forming the codewords
C = cf = (f (a1),f (a2),...,f (apm)), f(x) g F2t [x] ,
n
f(a)= = aifi(a), a g Fpm, ai gF2t
for some ordering of the elements of Fpm . This is the LDC encoding of the 
information a g Fmt .
2t
https://doi.org/10.1017/9781009283403.009 Published online by Cambridge University Press

226
8 Locally Decodable Codes
The fact the code is a 3-query code follows directly from its construction. 
Suppose it is desired to determine the information symbol ai e F2t, recalling 
the matching sets U = {u(j)}, V = {v(k)}. Let b e Fpm be an arbitrary 
coordinate position of the code. The code is queried in the three positions 
b,b + v(i), and b + yv(i') to obtain the values f(b), f(b + v(i)) and f (b + yv(i)) 
and compute the quantity
g-(u<i’b) (Cf(b) + Cf(b + v(i)) + Cf(b + Yv(i))
= g-(u(i),b) 
jn=1 aj fj (b) + fj(b + v(i)) + fj(b + yv(i))
is computed and by Equation 8.3 and the last lines of Equation 8.4 it follows 
that
g-(u(i),b) cf (b) + cf(b + v(i)) + cf(b + yv(i) = ai g-(u(i),b)g(u(i),b) = ai.
If the probability a coordinate queried is incorrect is 8, then the probability 
all three coordinates queried are correct is (1 - 8)3 « 1 - 38. Hence this system 
is a (3,8, 38) LDC system as claimed, with a length complexity noted earlier 
of m = exp(n10-7 ). Other LDC parameter sets are obtainable with similar 
considerations (e.g., [32, 41]).
Comments
The monographs [42, 43] and survey papers [44, 45] make excellent reading 
for the material of this and the next chapter, as do the research papers, e.g., 
[37, 38, 39, 40, 41], all the work of Yekhanin.
There is a natural connection with work on LDCs and PIR considered in 
Chapter 9 to the extent that a construction in one area yields one in the other. 
A brief outline of this connection is noted. To discuss the connection it is 
necessary to extend the LDC discussion to the q -ary case. Further comments 
on the connection can be found in the Comments section of Chapter 9.
It will be noted that an r-server q -ary PIR protocol P is a triplet of 
algorithms Q, A, C that, respectively, query, answer and compute information 
from servers. Itis assumed in aPIR (uncoded) scheme that each server contains 
the same n q-ary symbols x = {x1,x2,...,xn} and that the user U wishes 
to retrieve a particular symbol xi by querying the r servers (using r here for 
the number of queries as q is the field size) and computing the information 
received back, without revealing to any of the servers the particular symbol 
of interest. The query algorithm Q generates a random string rand from 
which r queries q1,q2,...,qr are generated and transmitted to the respective 
servers. The servers respond with answers aj and the computation C , on inputs 
a1,a2,...,ar, computes xi . For such a PIR protocol the answer should be 
https://doi.org/10.1017/9781009283403.009 Published online by Cambridge University Press

References
227
correct with probability one and that, for the servers to glean no information 
as to the symbol of interest, the queries should be uniformly distributed for all 
the servers and for any symbol of interest.
To obtain an r -server PIR protocol from a smooth q -ary r query LDC 
suppose ([19], [42], lemma 7.2) C is a q-ary r query LDC that is perfectly 
smooth (uniformly distributed queries over all codeword symbols) that encodes 
k symbol-long messages to codewords of length N . Each server contains 
the codeword corresponding to the original information symbols. The user 
constructs the queries q1,q2,...,qr (the set of codeword positions the LDC 
queries) and the j-th server responds with a symbol C(qj) (a computation 
of the symbols that query contained). From the responses the user is able to 
compute the required symbol xi via the LDC construction process. The privacy 
of this PIR protocol follows from the assumed uniform distribution of the 
LDC queries. The correctness of the PIR protocol follows from the assumed 
properties of the LDC code protocol. Further comments on the PIR/LDC 
connection are noted in the next chapter.
Only the basic approaches to LDCs and their connection to PIR have been 
discussed. The literature on these and numerous related topics is extensive and 
deep. The numerous interesting works on this connection include [11, 12, 17, 
29, 35].
References
[1] Babai, L., Fortnow, L., Levin, L.A., and Szegedy, M. 1991. Checking computa­
tions in polylogarithmic time. Pages 21-32 of: Proceedings of the Twenty-Third 
Annual ACM Symposium on Theory of Computing. STOC ’91. ACM, New York.
[2] Ben-Sasson, E., Maatouk, G., and Shpilka, A. 2011. Symmetric LDPC codes are 
not necessarily locally testable. Proceedings of the Annual IEEE Conference on 
Computational Complexity, 06.
[3] Cameron, P.J. 1994. Combinatorics: topics, techniques, algorithms. Cambridge 
University Press, Cambridge.
[4] Chandran, N., Kanukurthi, B., and Ostrovsky, R. 2014 (February). Locally 
updatable and locally decodable codes. Pages 489-514: Theory of Cryptography 
Conference.
[5] Dvir, Z., and Hu, G. 2013. Matching-vector families and LDCs over large modulo. 
Pages 513-526 of: Approximation, randomization, and combinatorial optimiza­
tion. Lecture Notes in Computer Science, vol. 8096. Springer, Heidelberg.
[6] Dvir, Z., and Shpilka, A. 2006. Locally decodable codes with two queries 
and polynomial identity testing for depth 3 circuits. SIAM J. Comput., 36(5), 
1404-1434.
[7] Dvir, Z., Gopalan, P., and Yekhanin, S. 2010 (October). Matching vector codes. 
Pages 705-714 of: 2010 IEEE 51st Annual Symposium on Foundations of 
Computer Science.
https://doi.org/10.1017/9781009283403.009 Published online by Cambridge University Press

228
8 Locally Decodable Codes
[8] Dvir, Z., Gopalan, P., and Yekhanin, S. 2011. Matching vector codes. SIAM J. 
Comput., 40(4), 1154-1178.
[9] Efremenko, K. 2009. 3-Query locally decodable codes of subexponential length. 
Pages 39-44 of: Proceedings of the Forty-First Annual ACM Symposium on 
Theory of Computing. STOC ’09. ACM, New York.
[10] Efremenko, K. 2012. 3-Query locally decodable codes of subexponential length. 
SIAM J. Comput., 41(6), 1694-1703.
[11] Gasarch, W. 2004. A survey on private information retrieval. Bull. EATCS, 
82(72-107), 113.
[12] Goldreich, O., Karloff, H., Schulman, L.J., and Trevisan, L. 2006. Lower bounds 
for linear locally decodable codes and private information retrieval. Comput. 
Complex., 15(3), 263-296.
[13] Gopalan, P. 2009. A note on Efremenko’s Locally Decodable Codes. Electron. 
Colloquium Comput. Complex., 16, 69.
[14] Grolmusz, V. 2000. Superpolynomial size set-systems with restricted intersections 
mod 6 and explicit Ramsey graphs. Combinatorica, 20(1), 71-85.
[15] Grolmusz, V. 2002. Constructing set systems with prescribed intersection sizes. 
J. Algorithms, 44(2), 321 - 337.
[16] Guo, A., and Sudan, M. 2012. New affine-invariant codes from lifting. CoRR, 
abs/1208.5413.
[17] Hielscher, E. 2007. A survey of locally decodable codes and private information 
retrieval schemes.
[18] Holzbaur, L., Polyanskaya, R., Polyanskii, N., and Vorobyev, I. 2020. Lifted 
Reed-Solomon codes with application to batch codes. Pages 634-639 of: 2020 
IEEE International Symposium on Information Theory (ISIT).
[19] Katz, J., and Trevisan, L. 2000. On the efficiency of local decoding procedures for 
error-correcting codes. Pages 80-86 of: Proceedings of the Thirty-Second Annual 
ACM Symposium on Theory of Computing. STOC ’00. ACM, New York.
[20] Kaufman, T., and Litsyn, S. 2005 (October). Almost orthogonal linear codes are 
locally testable. Pages 317-326 of: 46th Annual IEEE Symposium on Foundations 
of Computer Science (FOCS’05).
[21] Kaufman, T., and Viderman, M. 2010. Locally testable vs. locally decodable 
codes. Springer, Berlin, Heidelberg.
[22] Kedlaya, K.S., and Yekhanin, S. 2008. Locally decodable codes from nice subsets 
of finite fields and prime factors of Mersenne numbers. Pages 175-186 of: Twenty- 
Third Annual IEEE Conference on Computational Complexity. IEEE Computer 
Society, Los Alamitos, CA.
[23] Kedlaya, K.S., and Yekhanin, S. 2008/09. Locally decodable codes from nice 
subsets of finite fields and prime factors of Mersenne numbers. SIAM J. Comput., 
38(5), 1952-1969.
[24] Kerenidis, I., and de Wolf, R. 2004. Exponential lower bound for 2-query locally 
decodable codes via a quantum argument. J. Comput. System Sci., 69(3), 395-420.
[25] Kopparty, S., and Saraf, S. 2017. Local testing and decoding of high-rate error­
correcting codes. Electron. Colloquium Comput. Complex., 24, 126.
[26] Kopparty, S., Saraf, S., and Yekhanin, S. 2011. High-rate codes with sublinear­
time decoding. Pages 167-176 of: Proceedings of the Forty-Third Annual ACM 
Symposium on Theory of Computing. STOC ’11. ACM, New York.
https://doi.org/10.1017/9781009283403.009 Published online by Cambridge University Press

References
229
[27] Kopparty, S., Saraf, S., and Yekhanin, S. 2014. High-rate codes with sublinear­
time decoding. J. ACM, 61(5), Art. 28, 20.
[28] Li, R., and Wootters, M. 2020. Lifted multiplicity codes and the disjoint repair 
group property. IEEE Trans. Inform. Theory, 1-1.
[29] Obata, K. 2002. Optimal lower bounds for 2-query locally decodable linear 
codes. Pages 39-50 of: Randomization and approximation techniques in computer 
science. Lecture Notes in Computer Science, vol. 2483. Springer, Berlin.
[30] Polyanskii, N., and Vorobyev, I. 2019. Constructions of batch codes via finite 
geometry. Pages 360-364 of: 2019 IEEE International Symposium on Information 
Theory (ISIT).
[31] Polyanskii, N., and Vorobyev, I. 2019 (October). Trivariate lifted codes with 
disjoint repair groups. Pages 64-68 of: 2019 XVI International Symposium “Prob­
lems of Redundancy in Information and Control Systems” (REDUNDANCY).
[32] Raghavendra, P. 2007. A note on Yekhanin’s locally decodable codes. Electron. 
Colloquium Comput. Complex., 14(01).
[33] Saraf, S., and Yekhanin, S. 2011. Noisy interpolation of sparse polynomials, and 
applications. Pages 86-92 of: 26th Annual IEEE Conference on Computational 
Complexity. IEEE Computer Society, Los Alamitos, CA.
[34] Sudan, M. 1995. Efficient checking of polynomials and proofs and the hard­
ness of approximation problems. Lecture Notes in Computer Science, vol. 
1001. Springer-Verlag, Berlin; Association for Computing Machinery (ACM), 
New York.
[35] Wehner, S., and de Wolf, R. 2005. Improved lower bounds for locally decodable 
codes and private information retrieval. Springer, Berlin, Heidelberg.
[36] Woodruff, David. 2007. New lower bounds for general locally decodable codes. 
Electron. Colloquium Comput. Complex., 14(01).
[37] Woodruff, D., and Yekhanin, S. 2007. A geometric approach to information- 
theoretic private information retrieval. SIAM J. Comput., 37(4), 1046-1056.
[38] Yekhanin, S. 2006. New locally decodable codes and private information retrieval 
schemes. Electron. Colloquium Comput. Complex., 13.
[39] Yekhanin, S. 2007. Locally decodable codes and private information retrieval 
schemes. PhD thesis, Massachusetts Institute of Technology, Cambridge, MA.
[40] Yekhanin, S. 2007. Towards 3-query locally decodable codes of subexponential 
length. Pages 266-274 of: STOC ’07: Proceedings of the 39th Annual ACM 
Symposium on Theory of Computing. ACM, New York.
[41] Yekhanin, S. 2008. Towards 3-query locally decodable codes of subexponential 
length. J. ACM, 55(1), Art. 1, 16.
[42] Yekhanin, S. 2010. Locally decodable codes. Found. Trends Theor. Comput. Sci., 
6(3), front matter, 139-255.
[43] Yekhanin, S. 2010. Locally decodable codes and private information retrieval 
schemes, 1st ed. Springer, Berlin, Heidelberg.
[44] Yekhanin, S. 2011. Locally decodable codes: a brief survey. Pages 273-282 of: 
Coding and cryptology. Lecture Notes in Computer Science, vol. 6639. Springer, 
Heidelberg.
[45] Yekhanin, S. 2014. Codes with local decoding procedures. Pages 683-697 of: 
Proceedings of the International Congress of Mathematicians - Seoul 2014. Vol. 
IV. Kyung Moon Sa, Seoul.
https://doi.org/10.1017/9781009283403.009 Published online by Cambridge University Press

9
Private Information Retrieval
Chapters 6, 7, 8, 10 and this chapter all deal with some aspect of information 
storage and retrieval. This chapter introduces the notion of a private infor­
mation retrieval (PIR) protocol which allows a user to download information 
from a server, without the server being aware of the particular information 
of interest. It was noted in Chapter 8 that this notion is intimately connected 
with that of locally decodable codes (LDCs) in that any smooth LDC yields a 
PIR scheme. Some additional aspects of this connection are noted later in the 
chapter.
The elements of PIR protocols are introduced. As with previous chapters, 
efficient and practical schemes that could achieve these goals are of interest. 
Current approaches and results on these concepts are given in the next two 
sections, divided into single-server and multiple-server cases. The multiple­
server case often involves replicating the database leading to inefficiency of 
data storage. The final section studies the recent ideas of [19] for PIR protocols 
that use coding of the databases to reduce storage requirements.
The basic notions of PIR were first introduced in [14, 15]. In the basic 
model, a file of n bits x1,x2,...,xn is replicated on k servers (databases)
DB1, DB2,...,DBk.
AuserU wants to retrieve a bit, say xi without the servers being aware which 
bit the user is interested in. Further, only one-round protocols are of interest 
in this discussion, where the user sends queries to each of the databases and 
computes the information of interest from the responses. It is assumed the 
servers are unable to collude. With only a single server (database), to ensure 
information-theoretic privacy a trivial solution is for the server to download 
the entire database and it can be shown [15] that this trivial solution is in fact 
the only possible one. However, in this case, one can achieve computational 
security, meaning there is no polynomial computation on the user-server 
230
https://doi.org/10.1017/9781009283403.010 Published online by Cambridge University Press

9 Private Information Retrieval
231
information to discover the bit of interest. While seemingly counterintuitive, 
this can be achieved using public key cryptographic techniques. Such systems 
are considered in the next section. It is not clear how much current interest 
there is in this single-server case. Yet the fact it is possible seems worthy of 
some attention. It requires certain concepts from computational number theory, 
of which an outline is provided in the next section.
In the multiserver case, it is assumed there are k servers, each holding 
identical copies of the database. The total amount of communication between 
the user and the servers is of interest and the servers do not collude. The 
seminal work of this area is [14] (and the full journal paper [15]). For privacy 
the protocol must ensure that communication between servers and user is 
uniformly distributed to divulge no information as to the index i of the bit 
of interest. Furthermore, for a set S c [ n ] = {1,2,... ,n} and an index i e [ n ] 
define the notation
S ® 
u{ i} if i /S
® i | S\{i} if i e S.
(9.1)
This use of the ® operator between a set and set element is for the remainder 
of the chapter while as a binary operator between bits it retains the usual 
definition. Both meanings are used and it will be clear from the context which 
is applicable.
For the two-server case, k = 2, the following simple example [14] illustrates 
a technique to achieve download privacy. The servers DB1 and DB2 each 
contain the bitstring x e F2n of length n. The user U wishes to retrieve the 
bit xi without either server being aware of this fact or chooses a subset S of 
[1,n] at random (choosing each element of [1,n] for inclusion in the set with 
probability 1 /2). The set is transmitted to DB1. The user computes S ® i and 
transmits it to DB2. The server DB1 computes y 1 = ®jeSxj (mod 2 sum) 
which is transmitted to U and DB2 computes y2 = ®jeSeixj which is also 
transmitted to U. The user computes y 1 ® y2 = xt. The complexity of this 
protocol is O(n) since each server receives a list of O(n) bits and each server 
returns one bit to the user. Neither server gleans any information on the bit of 
interest to the user. Although simple, the protocol is instructive and suggestive 
for further development.
A more formal description of a PIR protocol is given, similar to that in 
[5, 14, 15, 39]
Definition 9.1 A k-server PIR protocol includes the following:
(i) k servers DB1,DB2,...,DBk, each containing an identical copy of the 
n bits x e{0, 1}n .
(ii) A user U requires xi e x in a manner such that no information on i is 
leaked.
https://doi.org/10.1017/9781009283403.010 Published online by Cambridge University Press

232
9 Private Information Retrieval
The protocol is a triple of algorithms (Q, A, C), a query algorithm Q that 
generates queries to send to a subset of the servers, an answer algorithm A 
used by the servers to generate answers to respond to user queries and a 
reconstruction algorithm C used by the user to construct the required bit from 
the responses received. Thus
(iii) U generates random bits with a uniform distribution from which queries 
q1,q2,...,qk are generated, sending qj to server DBj .
(iv) Server DBj computes a response aj = A(j),j = 1,2,...,kwhich is 
transmitted to the user U.
(v) From the responses, the user computes C(a1,a2,...,ak) and with 
probability 1 over the random string r, this is equal to the required bit xi .
The protocol has the properties:
(i) (Correctness) for any x e {0,1}n and i e [n] the algorithm produces the 
correct value of the bit xi with probability 1.
(ii) (Privacy) each server learns no information on the index i - the 
distributions of the queries are the same for any requested bit.
Interest is restricted to one-round protocols where the user determines the 
bit of interest from a computation involving the responses to their queries. 
While in practice interest is in small values k (fewer servers and less replication 
of the data), the general case is also of interest. The communication complexity 
of the protocol is the total number of bits sent in the queries and answers. 
For a given number of servers, ways of minimizing the total amount of 
communication are sought.
The next section considers protocols for the single-server case. As has been 
noted, the single-server protocol involves computational security rather than 
the stronger information-theoretic security. It is somewhat counterintuitive to 
imagine retrieving a bit from a server without the server being aware of some 
information on the bit in question, in the case ofa single server. The techniques 
involve notions drawn from public-key cryptography which are typically 
computationally expensive. For a reader with no background in computational 
number theory, the section might seem onerous and since the single server is of 
limited interest, that section can be skipped. However, the techniques of such 
protocols are interesting and appear to be of potential value to pursue.
Section 9.2 considers the case of multiple servers and the aim is to minimize 
the complexity (amount) of communications while preserving information- 
theoretic security. It is assumed here the servers (databases) do not collude, 
although some works define t-privacy where up to t servers are allowed to 
collude. It is also assumed the multiple servers contain identical copies of the 
https://doi.org/10.1017/9781009283403.010 Published online by Cambridge University Press

9.1 The Single-Server Case
233
information of n bits, x e Fn, and a user wishes to receive a single bit xi with 
no server obtaining any information on which bit is of interest.
This multiple-server model typically has the significant disadvantage of 
having the information replicated across all servers, leading to storage inef­
ficiency. The recent work of Fazeli et al. [19] addresses this problem and 
considers coding techniques that allow the total storage across all servers to 
approach n, the original size of the database, while preserving information- 
theoretic privacy. Section 9.3 discusses this approach.
As noted there is a strong connection between PIR protocols and LDCs 
and indeed an instance of the latter with smooth queries yields a k-server PIR 
protocol. This connection was briefly discussed in the previous chapter and 
will be noted upon further in the Comments section of this chapter.
9.1 The Single-Server Case
It was shown in [14] that at least &(n) bits of communication are required to 
securely retrieve a single bit from a single (k = 1) database in the information- 
theoretic model (information-theoretic security). This is on the order of the 
trivial solution of the database downloading the entire file to the user for 
receiving a request for a single bit and is clearly not of interest. The situation 
is similar to the nonexistence of a (1,5,e) LDC noted earlier, shown by [24]. 
However, it has been shown [25] that an interesting (and unexpected) solution 
for this single-server case can be achieved in the computationally secure 
case [5]. While computationally expensive, this is an interesting aspect of the 
subject. It requires some elements from computational number theory which 
is the basis of many public key cryptosystem algorithms. An overview of the 
requirements is given.
Much of the prequantum world of public-key cryptography depended on 
either the difficulty of factoring a large integer which is the product of two 
primes or the difficulty of finding discrete logarithms in either a finite field 
or in the group of an elliptic curve. Less well used for this purpose is the 
quadratic residuosity problem which will be described and used for the single­
server PIR case. For this section let p, q denote distinct odd large primes and 
n = pq their product. The problem to be described will be trivial to solve for a 
user who knows this factorization and computationally infeasible for one who 
does not. The term “computationally infeasible” will mean beyond the means 
of a nonquantum computer in feasible time.
Some preliminary notions and facts from elementary number theory are 
needed. An excellent source for these is the handbook [27]. No proofs are 
https://doi.org/10.1017/9781009283403.010 Published online by Cambridge University Press

234
9 Private Information Retrieval
given here. Let Zp be the set of integers modulo the prime p and Zp be the 
set of nonzero integers modulo p . It forms a multiplicative subgroup of Zp of 
order (p - 1).
An integer a e Zp is called a quadratic residue (QR) mod p if there exists 
an integer x such that the equation
x2 = a (mod p)
is satisfied and if no such x exists a is called a quadratic nonresidue (QNR) 
mod p. The zero element is not in Zp or Zp and is neither a QR nor a QNR. 
The set of QRs modulo p is denoted Qp and theNQRs modulo p denoted Qp. 
It is easy to establish that
| Qp |= (p - 1)/2 =| Qp | .
For a prime p the Legendre symbol is defined as
0, if p | a
1, if a e Qp
-1, if a e Qp.
The Legendre symbol enjoys many properties that allow its efficient computa­
tion. A generalization of this symbol is the Jacobi symbol to be defined shortly.
Consider the situation for the equation for a general positive integer n
x2 = a (mod n)
(9.2)
and define Qn so that a e Qn if the equation has solutions modulo n and 
a e Qn if it does not. It can be shown that for n the product of two primes 
n = pq,
| Qn |= (p — 1 )(q — 1)/4 and 
| Qn |= 3(p - 1 )(q - 1)/4
and if a e Qn the equation has four solutions.
In general if n = ik=1 piei , then
|Zn*|= <Kn = 0 (pi — 1 p
where Zn is the set of integers mod n and Zn the set of elements relatively 
prime to n.Ifa e Qn, Equation 9.2 has 2k solutions, where k is the number of 
distinct prime divisors of n and where $(n) is the Euler Totient function, | Zn*|.
Similar to the Legendre symbol for prime moduli one can define the Jacobi 
symbol for the composite case using the Legendre symbol. In particular for 
(a,n) = 1 and n = ik=1 piei define the Jacobi symbol as
https://doi.org/10.1017/9781009283403.010 Published online by Cambridge University Press

9.1 The Single-Server Case
235
i=1
a
Pi
a product of powers of Legendre symbols. If n is a prime the Jacobi symbol 
is the Legendre symbol. The Jacobi symbol enjoys many properties (section 
2.4.5 of [27], true for arbitrary positive n, not just for products of two primes):
(i) (n) = 0,1, or - 1 (and is 0 iff gcd(a,n) = 1),
(ii) (ab) = (a)(b) 
(hence (£) = 1),
' 
\ n / Xn) nn) 
v 
\^/ 
,
(iii) m=) = m&)®,
(iv) if a = b (mod n), then (n) = (b-) and (n) = (a ' n l) ,
v (n) = (n) (-1 )(a-1 )(n-1)/4.
With these properties one can determine the Jacobi symbols easily (without 
knowing the factorization of n, an important property for the sequel).
Define the sets (for arbitrary n)
{
. / a Al
a e Z * | 
= 1 and Qn = Jn\ Qn.
Note that Qn c Jn. Thus for an element a e Jn \ Qn the equation x2 = a 
mod n has no solutions (mod n) while for a e Qn (i.e., a is a QR mod n)it 
has 2k solutions for n a product of powers of k distinct primes (and hence four 
solutions ifn = pq). Elements in Jn\Qn are referred to [27] as pseudoprimes 
since they are not squares yet have a Jacobi symbol of 1.
For the remainder of the section assume n = pq, a product of two large 
primes. It follows that a e Qn iff a e Qp and a e Qq . For a given element 
a e Zn and not knowing the factorization of n it is easy to determine if a e Jn 
(via the formulae for the Jacobi symbols noted earlier) but computationally 
difficult (termed computationally infeasible in the cryptographic context) to 
determine if a e Qn . On the other hand, knowing the factorization of n it is 
simple to determine if a e Qn since one can easily determine the Legendre 
symbols mod p and mod q . It is this fact that is used in the following single­
server PIR protocol.
From the above discussion two problems are defined ([27], p. 99):
Definition 9.2
(i) The quadratic residuosity problem (QRP) is: given a e Jn , decide if 
a e Qn.
(ii) The square root modulo n problem (SQROOT) is: given a composite 
integer n and a e Qn find a square root of a (mod n).
https://doi.org/10.1017/9781009283403.010 Published online by Cambridge University Press

236
9 Private Information Retrieval
Conjecture 9.3 For n = pq the QRP problem is as difficult as the problem of 
factoring n.
It is clear the SQROOT problem for n = pq is computationally equivalent 
to factoring.
Example 9.4 Let n = 5 • 7 = 35. The set of QRs mod 35 is computed as 
Q35 = {1,4,9,11,16,29} and, for a e Q35 the equation x2 = a (mod 35) has 
four solutions. Similarly the set Q35 = J35\Q35 = {6,9, 10,12,15,18} is such 
that for a e Q35 the equation x2 = a (mod 35) has no solutions.
These facts form the basis of certain public key cryptographic protocols. 
For the single-server PIR example to be described it is assumed the primes 
p and q are chosen so that the integer n = pq is computationally infeasible 
to factor (a standard assumption, e.g., in implementing an RSA public key 
cryptosystem). Thus the QRP problem mod n is assumed computationally 
infeasible.
For the basic PIR single-server scheme of [25] assume the filex on st bits is 
written on the server as an 5 x t array of bits, M = (mij), an array of 5 rows and 
t columns. A user wants to retrieve the bit mab from the server with privacy. 
The user generates the primes p, q,n = pq and t integers y1,y2,...,yt such 
that yb is a QNR and the remaining (t- 1)yj are QRs. These t integers are sent 
to the server along with n. The server computes the 5 integers zi as follows:
wr,j =
yj2 if mrj = 0 
yj if mrj = 1
and then
JL 
zr = 
wrj,r= 1,2,...,5.
j=1
It is clear that for j = b, wrj is a QR for all r while for j = b, wrb is a QR 
iff mrb = 0 and a QNR otherwise. Thus zr is a QR iff mrb = 0 and a QNR 
otherwise.
The server sends the integers z1,z2,...,z5 to the user. The only variable 
of interest to the user is za - if it is a QR, then mab is 0 and otherwise it 
is a 1. Since the factorization ofn is known to the user it is easy for the user to 
determine if it is a QR or not.
If n, the product of two primes, is k bits and if one chooses 5 = t = ^n., 
the communication complexity of this scheme is
(2Vn + 1 )k
https://doi.org/10.1017/9781009283403.010 Published online by Cambridge University Press

9.2 The Multiple-Server Case
237
bits or O(n1/2) for a constant number of servers. For the argument that the 
scheme is private see [25]. The argument is essentially to show that if for two 
indices i and i' one is able to distinguish the queries and responses, one can 
create an argument that some information is leaked concerning the quadratic 
residuosity of a generated parameter, in contradiction to the assumptions stated 
above.
A further protocol, based on a recursion technique, is described in [25].
Thus the single database case under computational security, rather than 
the information-theoretic case used in the following sections, can be made 
relatively efficient in term of communications.
Other aspects of single-server PIR systems (under computational security) 
are surveyed in [28] and [5] including their relationships to several crypto­
graphic primitives such as collision-resistant hash functions, oblivious transfer, 
public key systems and homomorphic encryption. A single-server PIR system 
with constant communication complexity is given in [10, 21].
9.2 The Multiple-Server Case
The case of multiple servers (databases) and information-theoretic security 
is considered. Several interesting techniques are discussed as much for their 
novelty and inventiveness as for their effectiveness. The current state of affairs 
is broader than these techniques might indicate yet these systems remain of 
interest. More recent approaches will be noted later in the section.
Recall the model of interest in the case of multiple databases (as defined in 
Definition 9.1) is that the k databases, DB1,DB2,...,DBk contain the same 
string of n bits, x = {x1,x2,...,xn} and they do not communicate. A user 
U sends queries to each of the databases, and, depending on the query, each 
database computes a response which is returned to the user, from which the 
user computes the (single) bit of interest. No database gains any information 
as to which bit is of interest to the user (i.e., the protocol is information- 
theoretically private). Only one-round protocols are considered.
The first scheme is an extension of the two-database scheme described in 
the introduction to this chapter - that scheme will correspond to the case of 
d = 1 in the more general discrete d -dimensional cube encountered in other 
chapters [14].
Consider the case of k = 2d databases, for some positive integer d, each 
database containing n = £d bits for some positive integer £. These are viewed 
as d-dimensional cubes, with the i-th bit, xi, of the database x e F2' contained 
in cell (i 1 ,i2,..., id) (say the £-ary expansion of i e [n]) of the cube. Thus the 
https://doi.org/10.1017/9781009283403.010 Published online by Cambridge University Press

238
9 Private Information Retrieval
databases are labeled with binary d-tuples and the cells of each database by 
£-ary d-tuples. Such a scenario is referred to as a subcube code in the literature 
and these appear as examples for numerous structures (e.g., see batch codes).
Suppose the user U wishes to retrieve the bit corresponding to position 
(i 1 ,i 2,... ,id) in the cube, ij e [ £ ]. U chooses uniformly at random d subsets 
S0,S0,..., Sd, S0 c [£] (hence generally of differing sizes). As with the d = 1 
case considered in the introduction, a second set of subsets is chosen by the 
10 
10 10
user as 1 — 1 <p 1, 2 — 2 <p 2, . . . , d — d \p d were te enuou oi 
“®” of Equation 9.1 for these sets is recalled.
Consider the database DBa indexed by — — (a 1 ,a2,... ,ad), jj e {0,1}. 
The user Usends to DBa — DBa 1 ,a2,...,ad the sets S01 ,S02,... ,Sadd. This is 
done for each of the k — 2d databases, each database receiving different sets 
depending on its binary d-tuple label.
The database DBr responds with the XOR of all the bits in the rectangular 
solid defined by the sets sent to it, i.e., by the bit
p 
xj1,...,jd. 
(9.3)
• 11 
dd
j1 eS1 ,..., jd eSd
There are | Sr11 x-.-x | Sr | bits in the XOR sum.
The user XORs the k bits received from all k — 2d servers to construct 
the bit xi . The correctness of the scheme is clear from the construction and 
the privacy is assured by the uniform and random construction of the sets sent 
to each database. The complexity (number of bits transmitted) of the scheme 
is as follows: each set transmitted to the k databases consists of d bits (as an 
indicator vector for a subset of size d) and each database returns a single bit for 
a total of k(ld + 1) ^ 2dn1 /d bits (recall k — 2d and n — £d). The following 
trivial example illustrates some of the issues involved with the computations.
Example 9.5 As above assume k — 2d servers, labeled by binary d-tuples, 
each containing the information x e {0,1}n, n — £d, x stored as a 
d-dimensional £-ary array, £ > 4. Suppose the sets Si0 are chosen as Si0 — 
{1},i — 1,2,...,d and that it is wished to retrieve the data element x4,4,...,4 
from the servers, i.e., the element ofx stored in cell (4,4,...,4) of each array. 
By the above construction the sets Si1 — Si0 p{4} — {1,4},i — 1, 2,...,d.The 
set S d1 x Sr2 x-- - x Sdd is sent to database labeled a — (a 1,... ,ad) e {0,1}d. 
Clearly the only database whose response includes x4,4,...,4 is the one labeled 
with (1, 1,...,1) which received the set {1, 4}d.
Since all server responses are XORed together it remains to show that every 
other server location (than (4, 4,...,4)) is included an even number of times 
(including 0) counting all server responses. A simple general proof of this 
https://doi.org/10.1017/9781009283403.010 Published online by Cambridge University Press

9.2 The Multiple-Server Case
239
seems not immediately available but, e.g., consider the number of servers that 
include the data element x1, 1,1,4,4,...,4 XORed in its response. Clearly only 
the eight servers with addresses of the form (a,b,c, 1,1,..., 1 ),a,b,c e {0,1} 
have this cell in its response. Any other server cell location can be treated in a 
similar manner.
A slight variation of this scheme is able to accommodate a smaller number 
of servers at the expense of greater communication complexity using the 
notion of a covering code. In the general case consider the vector space of 
n-tuples over Fq , Vn(q). An e-covering code of Vn (q) is a set of codewords 
C = {c1,c2,...,cM,ci e Fqn} such that every q-ary n-tuple of Vn(q) is within 
Hamming distance e of at least one codeword. Said another way, the spheres 
of radius e around each codeword cover Vn(q) or every element of Vn(q) is 
in at least one sphere around some codeword. Perfect codes are an extreme or 
optimal case of covering codes, when they exist, in that each word in Vn(q) is 
in exactly one sphere around the codewords.
Example 9.6 The code C = {000, 111} is a 1-covering code for V3(2) since 
the spheres of radius 1 around each codeword exhaust V3 (2), i.e., the two 
spheres are
S1 = {000,001,010,100} S2 = {111,110,101,011}.
Only 1-covering binary codes are considered here. As above let the size of 
the database be n = £d. A database DBa for a = (a 1 ,a2,... ,ad) e {0,1}d 
receives the sets Sa1 ,Sa2,■■■ ,Sad and responds as above in Equation 9.3. Let 
C be a binary 1-covering code
C = c1, c2, . ..,ck, ci e F2d
The strategy will be as follows. Each server is assigned a codeword (hence the 
need for k codewords). Without loss of generality assume the all-zero d-tuple 
is the codeword c1 assigned to server labeled 00...0. The user sends DB0,0,0 
the queries S10,S20, ..., Sd0 and the server responds with the bit computed as the 
XOR of all the bits in the database subcube corresponding to all the indices in 
the set
S0 x S2 x ... x Sd.
The user requires such information from all 2d sets S1a1,S2a2,...,Sdad although 
now only k of these sets correspond to server assignments due to the use of 
the 1-covering code. However, the database DB0,0,...,0 knows that if database
https://doi.org/10.1017/9781009283403.010 Published online by Cambridge University Press

240
9 Private Information Retrieval
DB0,0,..., 1 did exist, the information it would have received from the user 
would have been of the form
S 0 X S2 X ... x Sd
for Sd of the form Sd ® j for some j e [£]. Thus in place of DB0,0,..., 1, the 
DB0,0,...,0 computes all of the bits corresponding to the £ sets
S 0 x So x...x {Sd ® j|, j = 1,2, ...,£
and transmits all £ bits to the user who knows which bit is the correct one 
they would have received had the corresponding server existed. The server 
DB0,0,...,0 repeats the process for each of the d missing servers corresponding 
to binary d-tuples distance 1 from its own codeword. Each server does the 
same and hence each of the k servers sends to the user a total of
(1 + M)k
bits. However, if the code is not perfect some of the d-tuples are within distance 
1 of more than one codeword and these should be eliminated. The total number 
of bits transmitted then is
k + (2d - k)£
bits from the databases to the user and
Mk
bits from the user to the databases, for a total of (theorem 1 of [14])
k + (2d + (d - 1 )k)E, 
£ = n1 /d.
Thus one can reduce the number of servers at the expense of increasing the 
number of bits transmitted from the servers via covering codes.
Another scheme of the influential paper ([14], section 4) is based on 
polynomial interpolation, a technique used for several other schemes. The 
basic idea is for the user to distribute partial information to the servers who 
compute partial information on the interpolation which is relayed back to the 
user who is able to complete the polynomial interpolation from the information 
provided.
An unusual construction from [41] is of interest. The protocol will yield a 
k-server PIR protocol with communication complexity of O k3n1/(2k-1) for 
a database of size n symbols (assumed from a finite field of size q, i.e., Fq). 
The construction is sufficiently involved that it is not described here.
There are numerous other ingenious protocols to achieve the PIR property, 
including the work of Beimel et al. [6] which achieves a communication 
complexity of nO log log k/k log k k for a k-server scheme, more efficient
https://doi.org/10.1017/9781009283403.010 Published online by Cambridge University Press

9.3 Coding for PIR Storage
241
than previous schemes for k > 3. It is noted the techniques used also result 
O ^g log k
in the construction of LDCs of length exp (n k logk ') which is shorter than 
previous constructions for all k > 4. The work is interesting for its use 
of multivariable polynomials. This approach is further developed in ([6], 
corollary 3.9) to yield a k-server PIR protocol for k > 3 with bit complexity 
O(n2loglogk/klogk), a significant improvement for this number of servers.
The cases of k = 2 and k = 3 servers have received considerable attention. 
A 2-server protocol with communication complexity nO(^loglogn/ logn is 
described in [16, 17], a significant improvement in known 2-server protocols 
which previously had complexity O(n1/3). The construction requires families 
of matching vectors, which gives further evidence of the relationship between 
PIR protocols and LDCs. The work of [35] develops tight bounds on the 
complexity of 2-server schemes that imply many of the known 2-server 
protocols are in fact close to optimal.
The equivalence of the notions of a smooth LDC and a PIR has been 
considered (indeed, this was noted in the original work of [24]). Thus given 
a smooth LDC one can construct an information-theoretic PIR and, given a 
PIR one can describe a smooth LDC, making the appropriate identification of 
the various parameters. Further aspects of this equivalence are noted in the 
Comments section. The relationships are further discussed in numerous works 
including [13, 22, 24, 35].
This equivalence has proved very useful in constructing new efficient 
PIRs. In particular it means finding lower bounds for information-theoretic 
PIRs is essentially equivalent to finding lower bounds on code lengths for 
smooth LDCs.
In particular, the breakthrough construction of 3-query LDC codes 
of [37] (see Chapter 8) of subexponential length, had length N = 
exp(O(1/ log log n)), a considerable improvement from the previously best- 
known O(n1/2), and these smooth LDC codes can be used to construct PIR 
schemes with three servers.
Finally it is mentioned that many of the works on PIR protocols address the 
notion of t -private k-server PIR protocols in which the user’s privacy is main­
tained under collusion of up to t servers. These are not considered in this work.
9.3 Coding for PIR Storage
Two obvious shortcomings of the PIR protocols described in the previous 
section include the large amount of storage required to implement them and 
the somewhat unrealistic assumption that a user wishes to retrieve only one 
https://doi.org/10.1017/9781009283403.010 Published online by Cambridge University Press

242
9 Private Information Retrieval
bit. Replicating a large database across multiple servers is expensive in both 
storage costs and transmission costs and coded approaches to this problem are 
considered. The first work that considers the possibility of coding rather that 
replicating the database appears to be [32]. It is shown there (theorem 1) that 
using k servers containing R bits and any erasure code, then any PIR algorithm 
achieving information-theoretic privacy must download at least R + 1 bits and 
must connect to at least R + 1 servers which in a typical application would 
be large. The papers [11, 12] consider the optimal trade-off between storage 
cost and complexity and show that the optimal trade-off can be achieved with 
MDS codes and that this optimal trade-off depends on the number of records 
in the system. Augot et al. [3] use multiplicity codes considered earlier and a 
PIR scheme to achieve more efficient storage schemes than replication.
The significant breakthrough on this issue that is considered here is the work 
of Fazeli et al. [19] (the author is unaware if this important contribution was 
published in a journal) which shows that PIR protocols can be adapted to work 
with information on the servers coded in such a way that the total amount of 
storage can be made to approach the size of one copy of the database rather 
than k times this amount as required in the k-server protocols described in the 
previous section. This while maintaining privacy of the downloads. The work 
has inspired a number of important papers. A brief overview of the original 
work in [18, 19] is described here.
It will be helpful to elaborate on the informative example in ([19], exam­
ple 2). Our treatment will be informal to focus on the ideas behind the coded 
system. As before a database X = (x1,...,xn) of n bits and a k-server 
PIR defined previously as the triple P = (Q, A, C) which will allow the 
downloading of a single bit xi with accuracy and privacy. As before, to invoke 
the protocol the user calls on Q to (randomly) generate k queries with query 
qj sent to server Sj,j = 1, 2,...,k. Each server uses routine A to generate 
answers to the queries, the answer from the j-th server designated as aj .The 
user then invokes the reconstruction algorithm C(a1,a2,...,ak) to generate the 
answer xi . The PIR protocol to be described can be proved correct and private 
for which the reader is referred to the original paper [19]. Interest will be in 
showing how the storage capacity can be significantly reduced, in fact to not 
much more than the size of a single server, with the use of a coding strategy 
while maintaining privacy.
PIR Codes from Binary Generating Matrices
All the coded PIR constructions of this section can be cast as being from binary 
code generating matrices. The technique is best illustrated via an extended
https://doi.org/10.1017/9781009283403.010 Published online by Cambridge University Press

9.3 Coding for PIR Storage
243
example. For the baseline uncoded system assume a 3-server PIR protocol 
is available, each server storing the entire database of n bits, a total storage 
of 3n. Consider another base system with the same storage (3n) where it is 
assumed that each server is only able to store n/4 bits and hence 12 servers 
are needed. From the assumed k = 3-server PIR protocol available generate 
3 queries (on n/4 bits). These queries can be sent to any of the 12 servers 
(any 3 queries in each round). The user receives answers back from the servers 
it has queried and determines the bit of interest from these answers. To use 
the protocol the user determines which quarter of the database the bit it is 
wished to recover lies in. Suppose it is in the first quarter - three of the servers 
contain this quarter and the user implements the 3-server protocol using these 
three servers. It will be assumed the PIR protocols are additive in that the 
response to a query corresponds to the sum of two database bits. Virtually all 
PIR protocols developed so far have this additive property. This is the uncoded 
baseline system. Its total storage, as before, is 3n.
The aim of the following example (an expansion of example 2 in [19]) is to 
show how a PIR protocol with eight servers and total storage of 2n (compared 
to 3n for the original 3-server strategy or the 12-server strategy) can be used, 
thus demonstrating the advantage of coding.
Example 9.7 Divide the entire database x into four parts X1,X2,X3 
and X4, each of size n/4 (as in the above 12-server model). Thus X = 
(X1,X2,X3,X4) = (x1,x2,...,xn) and
Xi = x(i-1)n/4+1,x(i-1)n/4+2,...,x(i-1)n/4+n/4 ,i = 1, 2, 3, 4.
Rather than store these quarters on the servers they are first coded into 
pieces C1,C2,...,C8, each of size n/4 according to the (8,4)2 binary code:
C = X • M
1
(C1,C2,...,C8) = (X1,X2,...,X4)
0001001
1001100
0100110
001001 1
0
0
0
The arithmetic is in F2 - thus C5 = X1 ® X2 which is of dimension n/4 and is 
stored on server 5.
This matrix has an interesting property, the disjoint repair group property 
which is described as follows (it was also used in LDCs from multiplicity 
codes discussed in Chapter 8). Let ei,i = 1,2, 3,4 be column vectors, binary 
4-tuples, the indicator vectors for 4-tuples. The matrix M is said to have the 
disjoint repair group property for a k = 3-PIR if, for each i = 1,2, 3,4, there 
https://doi.org/10.1017/9781009283403.010 Published online by Cambridge University Press

244
9 Private Information Retrieval
are three (in general k) disjoint sets of columns of the matrix M, R3(i) (different 
sets for each i) such that the columns in each set add to ei,i = 1,2, 3,4. Thus 
if we label the column vectors of the matrix M by m1,m2,...,m8 we have the 
following sets of disjoint repair groups:
R31) = {{1}, {2,5}, {4,8}} : e1 = m1 = m2 © m5 = m4 © m8 = (1,0,0,0)t
R32) = j{2}, {1,5}, {3,6}} : e2 = m2 = m1 © m5 = m3 © m6 = (0,1,0,0)t
R33) = j{3}, {2,6}, {4,7}} : e3 = m3 = m2 © m6 = m4 © m7 = (0,0,1,0)t
R3(4) = {4},{3,7},{1,8} : e4 =m4 =m3 ©m7 =m1 ©m8 = (0,0,0,1)t
For general k, Rk(j) will consist of a set of k disjoint subsets of columns of 
the coding matrix M which each add up to the same indicator vector, ej of 
dimension equal to the number of rows of M. Denote the j-th subset of Rk(i) 
as Rk(i,)j. The construction of such disjoint (repair) groups is of great interest in 
this and other chapters.
The crucial property of this matrix now is that the following inner products 
form independent assessments of the various information bits required:
X1 = X, 
mj = X, 
mj = X, 
mj
(1) 
(1) 
(1)
j 6 R 3, 1 
j 6 R 3,2 
j 6 R 3,3
X2 = Xx, E mA = Xx, E mA = Xx, E mj
6 R 2) 
6 R 2) 
6 R 2)
j 6 R 3, 1 
j 6 R 3,2 
j 6 R 3, 3
X3 = Xx, E mj} = Xx, E mA = Xx, E mj
(3) 
(3) 
(3)
j 6 R 3, 1 
j 6 R 3, 2 
j 6 R 3, 3
X4 = Xx, E mj} = Xx, E mj} = Xx, E mj
(4) 
(4) 
(4)
j 6 R 3, 1 
j 6 R 3,2 
j 6 R 3, 3
Consider the three sets of R3(1) and the queries generated by the 3-query PIR, 
say (q1,q2,q3). These queries are designed to retrieve a bit xi from a database 
(of size n/4) and denote the responses from the three servers as (a1,a2,a3). 
Suppose these queries are sent to the appropriate coded servers. Specifically 
suppose q1 is sent to the first set of R3(1, 1), namely server 1 with response a1. 
Send query q2 to the coded servers S2 and S5 and denote the responses a2 and 
a5, respectively, and send q3 to coded servers S4 and S8 and their responses 
a4 and a8, respectively. The primes indicate the responses are from a coded 
database. Queries may be sent to other databases but their responses, indicated 
with a *, are not used in the reconstruction. It is clear then because of the 
disjoint repair property discussed the three responses are
a1, a2 = a2 © a5 and a3 = a4 © a8 
https://doi.org/10.1017/9781009283403.010 Published online by Cambridge University Press

9.3 Coding for PIR Storage
245
because of the assumed additive property of the responses. Said another way, 
the sum of the parts of the databases stored in servers 2 and 5 is X1 which 
is also the sum of the databases of servers 4 and 8. Thus by the coding of the 
disjoint repair property three identical databases X1 have been created to which 
the original 3-PIR protocol can be applied. This explains the use of the disjoint 
repair groups constructed. Similar comments lead to the other equations.
To recap, to retrieve a bit from the sub-database X1 one uses the 3-PIR to 
generate queries (q1,q2,q3) and sends these to the coded servers as
(q 1 ,q2, * ,q3,q2, * , * ,q3) and receives responses (a1 ,a2, * ,a4,a5, * , * ,a8) 
from which are derived the responses a 1 = a1 ,a2 = a2 0 a5, a3 = a4 0 a8 
from which the original reconstruction algorithm of the 3-PIR protocol can be 
used to determine the bit xi . The paper [19] sends random queries from those 
of the servers not used to assist with satisfying the privacy condition in that 
responses from these other servers are not used in the reconstruction process. 
We use the * to explain the “null” responses.
The summary of this process for the bit of interest in each of the four 
sub-databases is as follows for the 3-server queries (q1,q2,q3 ) and coded 
server responses (a1 ,a2,...,a8) used to create input to the 3-server PIR 
reconstruction:
xi in
coded sever queries
inputs to the 3-PIR reconstruction
X1 (q 1 ,q2, *,q3,q2, *, *,q3) a 1 = a 1, a 2 = a 2 0 a 5, a 3 = a 4 0 a 8
X2
(q2,q 1 ,q3, *,q2,q3, *,*)
a 1 = a 2, a 2 = a1 0 a 5, a 3 = a 3 0 a 6
X3
(*,q2,q 1 ,q3, * ,q2,q3, *)
a 1 = a 3, a 2 = a 2 0 a 6, a 3 = a 4 0 a 7
X4 (q3, *,q2,q 1, *, *,q2,q3) a 1 = a 4, a 2 = a 3 0 a 7, a 3 = a1 0 a 8
Note in particular the use of eight servers while using only a 3-server PIR 
protocol.
The example thus achieves the same storage/performance (3n bits) of a 
12-server uncoded PIR protocol while using only storage of 2n bits.
To generalize this instructive example to k servers it will be assumed a 
k-server PIR protocol is available and that the database X of n bits is divided 
into s sub-databases X1, X2,...,Xs , each containing n/s bits (in the above 
example k = 3,s = 4). There will be m > k servers in the coded protocol.
A k-server PIR code is defined as follows ([19], definition 4):
Definition 9.8 An 5 x m binary matrix G has property Ak if, for each i e [5] 
there exists k disjoint subsets of columns of G that add to ei (for each i) the 
indicator 5-tuple of the i-th position. A binary linear [m, 5] code C will be 
called a k-server PIR code if there exists a generator matrix G for C with 
https://doi.org/10.1017/9781009283403.010 Published online by Cambridge University Press

246
9 Private Information Retrieval
property Ak, i.e., for each i e [5] there exist k disjoint sets of columns of G, 
Rk(i),i = 1,2,...,s, | Rk(i) |= k, such that
e = £ mi = ...= £ mi, j = 1,2,...,s.
ieRk(j, 1) 
ieRk(j,k)
It is shown ([19], theorem 5) that the upload and download complexity of 
an [m, 5] k-server PIR code with each server containing n/5 bits is m times that 
of the uncoded k-server PIR protocol with servers containing n/5 bits. A lower 
bound is given in [31]. Aspects of the problem of constructing matrices with 
the disjoint repair groups property are explored further in [26, 29].
An interesting question is to determine for a given value of 5 , the number 
of pieces the original database is divided into (in essence the number of 
information symbols of the code), values for m and k.
A natural figure of merit for such coded PIR systems is the ratio of total 
storage among all servers to the size of the database. Define [19] the quantity 
A(5,k) as the optimum value ofm such that an [m,5] k-server PIR code exists, 
which is the total number of coded bits of the system. Thus
A(s,k) = total number of stored bits and n = A(s,k)/n = storage efficiency
where n is the ratio of total stored to original information bits, the storage 
overhead, is a measure of the efficiency of the coded scheme. It should be 
noted it is irrelevant whether a single bit or portion of the database is stored in 
each cell for these measures. For the examples it will be assumed a single bit 
is stored in each cell.
There remains the problem of constructing k-server PIR codes. The problem 
is addressed in the original manuscript [19] where the multidimensional cubic 
construction (a similar construction was used for uncoded PIR codes earlier 
in this chapter and batch codes in Chapter 10), Steiner systems and majority 
logic decodable codes are used to give simple constructions. Only the cubic 
construction is briefly described here.
The Cubic k-PIR Code Construction
For the derivation of a cubic [m, 5] k-server PIR code (recall: in general, m 
servers of which k will be contacted in the protocol, each containing n/5 
bits) the general cubic construction considers a discrete set of £k-1 cells in 
(k — 1) dimensions, an £ x £ x • •• x £, ((k — 1)-fold) array. For simplicity 
it will be assumed each cell contains a single bit of the database, i.e., n = 5 . 
The extension to n/s will be clear. The cube side notation of £ is used to be 
consistent with the cubic construction in other sections (a is used in [19]).
https://doi.org/10.1017/9781009283403.010 Published online by Cambridge University Press

9.3 Coding for PIR Storage
247
Thus n = s = £(k -1 ) .On each of the (k - 1) “faces” of the cube, each 
containing £(k-2) cells, a parity symbol is placed in each cell, giving a total of 
(k — 1 )t(k-2) parity-check symbols. The parity bit is the XOR of the contents 
of all cells corresponding to an addressing coordinate being constant. Thus 
the length of the code (information plus parity bits, also the total number of 
servers) is
m = ((-1) + (k — 1 )E(k-2).
The situations for k = 3 and £ = 3 are shown in Figure 9.1.
It is a simple matter to translate this cube arrangement into an ordinary 
binary code via a generating matrix — as will be done for the following exam­
ple. The geometric picture of the (k- 1)-dimensional cube, however, is useful.
To use these cubic structures to generate the disjoint repair groups of 
columns of the matrix, consider an information bit in a particular cell and 
consider the information symbols and parity symbols obtained by running lines 
through this symbol (cell) along directions of the (k - 1) perpendicular axes 
of the cell, plus the singleton of the symbol in the cell itself. This generates k 
disjoint repair subsets of the columns of the s x m generating matrix G and 
these form the set of sets Rk(xi) where the content of the cell is the symbol xi .
For the k-server cubic codes we have
A(s,k) = £k-1 + (k - 1 )£k-2 and n = 1 + (k - 1 /.
This quantity goes to unity with £ for a fixed dimension k. Compare this with 
the replication schemes with a value equal to the number of servers. Numerous 
works consider the design of k-server coded schemes ([7, 8] and references 
cited there).
Example 9.9 Consider the case £ = 2,k = 3 which is similar to the 
case of the previous example. The situation is shown in Figure 9.1(a) with 
the equivalent matrix G (b). The disjoint repair groups for each possible 
information bit readily follow. For example, for the information symbol x4 
in Figure 9.1 the three disjoint repair groups are
{x4,x4 = x 1 ® x7 ® p 1 ,x4 = x5 ® x6 ® p5}.
Note that in the general construction k is the number of disjoint repair groups 
for each information bit and hence the scheme requires a k-PIR protocol and a 
total of m servers.
The work of Shah et al. [32], noted previously, apart from introducing the 
notion of erasure coding for PIR download schemes, also introduced schemes 
that considered a database of k records X, each record R bits in length and a
https://doi.org/10.1017/9781009283403.010 Published online by Cambridge University Press

248
9 Private Information Retrieval
|P1 p2 p3|
x1 x2 x3
x4 x5 x6
x7 x8 x9
(a)
x1x2x3x4x5x6x7x8x9p1p2p3p4p5p6
1 0 0 0 0 0 0 0 0
1 0 0
1 0 0
0
1 0 0 0 0 0 0 0 0
1 0
1 0 0
0 0
1 0 0 0 0 0 0 0 0
1
1 0 0
G=
0 
0
0 
0
0 
0
1 
0
0 
1
0 
0
0 
0
0 
0
0 
0
1 
0
0 
1
0 
0
0 
0
1
1
0
0
0 0 0 0 0
1 0 0 0 0 0
1 0
1 0
0 0 0 0 0 0
1 0 0
1 0 0 0 0
1
0 0 0 0 0 0 0
1 0 0
1 0 0 0
1
0 0 0 0 0 0 0 0
1 0 0
1 0 0
1
(b)
Figure 9.1 Subcube codes: (a) k = 3 (b) € = 3 associated generator matrix
user wishes to retrieve a record, with privacy and accuracy, rather than a single 
bit, as has been considered to this point. A very large number of works since 
that seminal work has furthered development of this model, e.g., [4, 9, 11, 12, 
33, 34] but such interesting avenues are not pursued here.
The interesting relationships between LDCs and PIR codes have been noted 
earlier (and in the Comments section to follow) and the relationship to batch 
codes will be discussed in Chapter 10. The notion of disjoint recovery or 
repair sets appears in them all to some extent. What might be viewed as a 
precursor to these ideas is that of majority logic decodable codes of coding 
theory, a notion studied in the coding literature since the 1960s. For the binary 
majority logic decodable codes case, this involves determining parity-check 
vectors (codewords in the dual code) which each involve the bit xi of interest 
but each is otherwise disjoint on remaining coordinate positions. If2e+ 1 such 
check vectors can be found and if e or fewer errors are made in transmission, 
then a majority of the check equations will yield the same correct answer. In 
essence the parity-check equations can be viewed as disjoint repair equations 
and so their application to the construction of coded PIR systems is clear.
PIR Codes from Multiplicity Codes
The role of disjoint recovery sets is clear in the previous generating matrix 
approach to PIR codes, where a recovery set is defined more formally in the 
definition below. Each column indicator vector can be described as a disjoint 
sum of generator matrix columns, each such column vector associated with a 
part of the database. The number of disjoint sums for the same indicator vector 
is k to allow a k-PIR scheme to be used on the same part of the database. 
This motivates the use of disjoint recovery sets below.
The work [1] shows how the multiplicity codes Cm,d,s,q can be used to 
generate such recovery sets and hence yield PIR codes. Recall from Section 
8.2 the following properties of multiplicity codes. For a database of n symbols 
https://doi.org/10.1017/9781009283403.010 Published online by Cambridge University Press

9.3 Coding for PIR Storage
249
from Fq , the multiplicity code gives N = qm coded symbols over the alphabet 
£ = q( +m ) .As each coordinate position contains all Hasse derivates of 
order <5 of the m-variate polynomial of degree at most d over Fq, the code 
is also thought of as an array of (m +m--1) x qm Fq symbols. The code Cm,d,s,q 
has parameters:
length = qm, rate = —m , normalized distance A = 1 ——.
(m +m-1) qm 
5q
The definition of a PIR code is modified slightly:
Definition 9.10 ([1]) The code C will be called a k-PIR code, denoted by 
[N,n,k]q if for every information symbol xi,i e [n] there exists k mutually 
disjoint sets Ri, 1 ,Ri,2,... ,Ri,k such that for all j e [k] xi is a function of 
the symbols in Ri,j Such sets are referred to as recovery sets for the code 
position i.
It is desired to minimize N, the number of coded symbols, for a given n 
and k, and such a value will be designated Npir (n, k) and schemes that give 
limn^OT Npir(n,k)/n = 1 are sought. A brief outline of the use of multiplicity 
codes [1] in the derivation of k-PIR codes is given.
An interpolation set for f(x) e Fq [x 1,... ,xm] = Fq [x] is a set R c Fq1 
such that if for every polynomial for which g(x) e Fq[x], f (x) = g(x) Vx e 
R, then f(x) = g(x) Vx e F qm .Consider sets of the form Z = A1 x A 2 x- • -x 
Am with | Ai |= d + 1, Ai c Fq, a set of distinct elements and note that this is 
an interpolation set for f e Fq[x1,x2,...,xm] of degree d ([1], lemma 10).
Consider a codeword cf e Cm,d,5,q
cf = (f[ <5 ] (a 1 )f[ <5 ] (a 2),..., f[ <5 ] (aqm))
for some ordering of the elements aj of Fqm .IfA c Fqm is an interpolation set 
for homogeneous polynomials of degree at most 5 - 1, then for any a e Fqm
Ra,v = {a + kv, k e Fq}, v e Fqm
is a recovery set for the code coordinate position corresponding to a e Fqm . 
Thus it is to be shown how the codeword coordinate position a of cf , i.e., 
f [<5] (a), can be computed from the coordinate positions of the recovery set 
Ra,v, i.e., to compute
f[<5] (a) from f [<5] (u), u e Ra,v.
The argument is similar to the one for the discussion of multiplicity codes 
in Chapter 8. For a codeword polynomial f(x) e Fq [x] of total degree at most 
d, define the univariate polynomial
https://doi.org/10.1017/9781009283403.010 Published online by Cambridge University Press

250
9 Private Information Retrieval
h(X) = f(a + Xp) e Fq[X]
of degree at most d. Assume this polynomial is known for X e F*. Furthermore 
it can be shown [1] to be unique and can be used to determine f [<s] (a) since
h(0) = f(a).
It follows that from the values of f [<s](b),b e Ra,p the value of the codeword 
coordinate position a, i.e., f [<s] (a) can be determined. Thus any interpolation 
set for a homogeneous polynomial of degree less than s leads to a recovery set 
for a codeword position. For details see ([1], theorem 14).
Furthermore ([1], theorem 16, corollary 17) it can be shown there are 
|_|J mutually disjoint such recovery sets and hence the multiplicity code 
Cm,d,s,q yields a k-PIR code with
N = qm,n = ( m ) , k = q , and symbol field size E = q( +s- 1) 
m+s-1ms
where N is the number of coded symbols and each database contains n 
symbols. (Note: The roles of the parameters m and s here are reversed from 
those of [1].)
Comments
The intimate connection between the construction of LDCs and PIR schemes 
was noted in Chapter 8. As a more concrete relationship consider the following 
([38], lemma 7.2). Suppose there is available a q-ary smooth LDC with r 
queries that encodes a database x e Fqk to a codeword y e Fqn . The codeword 
is stored on each of the r servers, S1,S2,...,Sr. The PIR scheme then uses 
the r queries of the LDC to query each of the servers to receive answers 
A(y,j),j = 1, 2,...,r from which the desired database element xi e Fq is 
recovered via the LDC decoding algorithm. The privacy of the resulting PIR 
scheme derives from the assumed smoothness of the LDC code.
As further evidence of this connection the following two results are quoted, 
the first a result of the 3-query construction of LDCs of Yekhanin introduced 
in Chapter 8:
Theorem 9.11 ([30], theorem 5,3) Let p = 2t - 1 be a fixed Mersenne prime. 
For every positive integer n there exists a 3-server PIR protocol of length 
O(n1/t) and answers of length t. Specifically, for every positive integer n there 
exists a 3-server PIR protocol with communication complexity O(n1/32582658).
https://doi.org/10.1017/9781009283403.010 Published online by Cambridge University Press

References
251
In the reverse direction the following lemma is a generalization of a result 
of [22]:
Lemma 9.12 ([23], lemma 4.1) Assume the existence of a PIR scheme 
with k servers, database size n, query size t, answer size a, and recovery 
probability at least 1 /2 + e. Then there exists a (k,k + 1,e)-smooth LDC code 
C: {0,1}n -^ ({0,1}a)m, with m < (k2 + k) • 2t. Further:
(i) If the PIR scheme is linear, then the LDC C is also linear.
(ii) If in the PIR scheme the user only uses k predetermined bits from each 
answer aj, then this is also true for the decoding algorithm ofC.
This lemma uses the slightly different definition of an LDC noted earlier 
in that the code is (q,c,T)-smooth if, in the decoding, the probability the 
algorithm queries a particular coordinate is upper-bounded by c/m, the 
meaning of the other parameters the same.
Research into techniques for the distributed storage and private retrieval 
of information into servers with various types of restrictions has developed 
rapidly over the past decade. The chapters of this volume on coding for 
distributed storage, PIR, LDCs and batch codes are strongly related and simply 
different aspects of the storage and retrieval problem. These four short chapters 
do not capture the full scope of work in progress on these important problems 
but hope to have provided a glimpse into the models and direction the work is 
taking.
The monographs [38] and [39] make excellent reading and cover both LDCs 
and PIR schemes and their relationships. In addition there are excellent survey 
papers such as [2], [20], [23] and [40]. Yekhanin’s doctoral thesis at MIT [36] 
is also interesting reading.
References
[1] Asi, H., and Yaakobi, E. 2019. Nearly optimal constructions of PIR and batch 
codes. IEEE Trans. Inform. Theory, 65(2), 947-964.
[2] Asonov, D. 2001. Private information retrieval - an overview and current trends. 
.
https://api.semanticscholar.org/CorpusID:14698164
[3] Augot, D., Levy-dit Vehel, F., and Shikfa, A. 2014. A storage-efficient and robust 
private information retrieval scheme allowing few servers. Pages 222-239 of: 
Cryptology and network security. Lecture Notes in Computer Science, vol. 8813. 
Springer, Cham.
[4] Banawan, K., and Ulukus, S. 2018. The capacity of private information retrieval 
from coded databases. IEEE Trans. Inform. Theory, 64(3), 1945-1956.
https://doi.org/10.1017/9781009283403.010 Published online by Cambridge University Press

252
9 Private Information Retrieval
[5] Beimel, A. 2008. Private information retrieval: a primer. Ben-Gurion University.
[6] Beimel, A., Ishai, Y., Kushilevitz, E., and Raymond, J.F. 2002. Breaking the 
O(n1(2k-1)/) barrier for information-theoretic Private Information Retrieval. 
Pages 261-270 of: The 43rd Annual IEEE Symposium on Foundations of Com­
puter Science, 2002. Proceedings.
[7] Blackburn, S.R., and Etzion, T. 2016. PIR array codes with optimal PIR rates. 
CoRR, abs/1609.07070.
[8] Blackburn, S.R., Etzion, T., and Paterson, M.B. 2017 (June). PIR schemes with 
small download complexity and low storage requirements. Pages 146-150 of: 
2017 IEEE International Symposium on Information Theory (ISIT).
[9] Blackburn, S.R., Etzion, T., and Paterson, M.B. 2020. PIR schemes with small 
download complexity and low storage requirements. IEEE Trans. Inform. Theory, 
66(1), 557-571.
[10] Cachin, C., Micali, S., and Stadler, M. 1999. Computationally private information 
retrieval with polylogarithmic communication. Springer, Berlin, Heidelberg.
[11] Chan, T.H., Ho, S.-W., and Yamamoto, H. 2014. Private information retrieval for 
coded storage. CoRR, abs/1410.5489.
[12] Chan, T.H., Ho, S., and Yamamoto, H. 2015 (June). Private information retrieval 
for coded storage. Pages 2842-2846 of: 2015 IEEE International Symposium on 
Information Theory (ISIT).
[13] Chee, Y.M., Feng, T., Ling, S., Wang, H., and Zhang, Liang, F. 2013. Query­
efficient locally decodable codes of subexponential length. Comput. Complex., 
22(1), 159-189.
[14] Chor, B., Goldreich, O., Kushilevitz, E., and Sudan, M. 1995. Private information 
retrieval. Pages 41- of: Proceedings of the 36th Annual Symposium on Founda­
tions of Computer Science. FOCS ’95. IEEE Computer Society, Washington, DC.
[15] Chor, B., Kushilevitz, E., Goldreich, O., and Sudan, M. 1998. Private information 
retrieval. J. ACM, 45(6), 965-981.
[16] Dvir, Z., and Gopi, S. 2015. 2-Server PIR with sub-polynomial communication. 
Pages 577-584 of: STOC ’15: Proceedings of the 2015 ACM Symposium on 
Theory of Computing. ACM, New York.
[17] Dvir, Z., and Gopi, S. 2016. 2-Server PIR with subpolynomial communication. 
J. ACM, 63(4), Art. 39, 15.
[18] Fazeli, A., Vardy, A., and Yaakobi, E. 2015 (June). Codes for distributed PIR with 
low storage overhead. Pages 2852-2856 of: 2015 IEEE International Symposium 
on Information Theory (ISIT).
[19] Fazeli, A., Vardy, A., and Yaakobi, E. 2015. PIR with low storage overhead: 
coding instead of replication. CoRR, abs/1505.06241.
[20] Gasarch, W. 2004. A survey on private information retrieval. Bull. EATCS, 
82(72-107), 113.
[21] Gentry, C., and Ramzan, Z. 2005. Single-database private information retrieval 
with constant communication rate. Springer, Berlin, Heidelberg.
[22] Goldreich, O., Karloff, H., Schulman, L.J., and Trevisan, L. 2006. Lower bounds 
for linear locally decodable codes and private information retrieval. Comput. 
Complex., 15(3), 263-296.
https://doi.org/10.1017/9781009283403.010 Published online by Cambridge University Press

References
253
[23] Hielscher, E. 2007. A survey of locally decodable codes and private information 
retrieval schemes.
 
 https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&
doi=f0e9a3406eeb7facef0c0ce55e0a3c67cb28877d.
[24] Katz, J., and Trevisan, L. 2000. On the efficiency of local decoding procedures for 
error-correcting codes. Pages 80-86 of: Proceedings of the Thirty-Second Annual 
ACM Symposium on Theory of Computing. STOC ’00. ACM, New York.
[25] Kushilevitz, E., and Ostrovsky, R. 1997. Replication is not needed: single 
database, computationally-private information retrieval. Pages 364 of: Proceed­
ings of the 38th Annual Symposium on Foundations of Computer Science. FOCS 
’97. IEEE Computer Society, Washington, DC.
[26] Li, R., and Wootters, M. 2019. Lifted multiplicity codes and the disjoint repair 
group property. Art. No. 38, 18 of: Approximation, randomization, and combina­
torial optimization: Algorithms and techniques. LIPIcs - Leibniz International 
Proceedings in Informatics, vol. 145. Schloss Dagstuhl. Leibniz-Zentrum fur 
Informatik, Wadern.
[27] Menezes, A.J., van Oorschot, P.C., and Vanstone, S.A. 1997. Handbook of applied 
cryptography. CRC Press Series on Discrete Mathematics and Its Applications. 
CRC Press, Boca Raton, FL.
[28] Ostrovsky, R., and Skeith, W.E. 2007. A survey of single-database private 
information retrieval: techniques and applications. Springer, Berlin, Heidelberg.
[29] Polyanskii, N., and Vorobyev, I. 2019. Constructions of batch codes via finite 
geometry. Pages 360-364 of: 2019 IEEE International Symposium on Information 
Theory (ISIT).
[30] Raghavendra, P. 2007. A note on Yekhanin’s locally decodable codes. Electron. 
Colloquium Comput. Complex., 14(01).
[31] Rao, S., and Vardy, A. 2016. Lower bound on the redundancy of PIR codes. CoRR, 
abs/1605.01869.
[32] Shah, N.B., Rashmi, K.V., and Ramchandran, K. 2014 (June). One extra bit of 
download ensures perfectly private information retrieval. Pages 856-860 of: 2014 
IEEE International Symposium on Information Theory.
[33] Sun, H., and Jafar, S.A. 2017. The capacity of private information retrieval. IEEE 
Trans. Inform. Theory, 63(7), 4075-4088.
[34] Sun, H., and Jafar, S.A. 2019. The capacity of symmetric private information 
retrieval. IEEE Trans. Inform. Theory, 65(1), 322-329.
[35] Wehner, S., and de Wolf, R. 2005. Improved lower bounds for locally decodable 
codes and private information retrieval. Springer, Berlin, Heidelberg.
[36] Yekhanin, S. 2007. Locally decodable codes and private information retrieval 
schemes. Ph.D. thesis, Massachusetts Institute of Technology, Cambridge, MA. 
AAI0819886.
[37] Yekhanin, S. 2007. Towards 3-query locally decodable codes of subexponential 
length. Pages 266-274 of: STOC ’07: Proceedings of the 39th Annual ACM 
Symposium on Theory of Computing. ACM, New York.
[38] Yekhanin, S. 2010. Locally decodable codes. Found. Trends Theor. Comput. Sci., 
6(3), front matter, 139-255 (2012).
[39] Yekhanin, S. 2010. Locally decodable codes and private information retrieval 
schemes, 1st ed. Springer-Verlag, Berlin, Heidelberg.
https://doi.org/10.1017/9781009283403.010 Published online by Cambridge University Press

254
9 Private Information Retrieval
[40] Yekhanin, S. 2011. Locally decodable codes: a brief survey. Pages 273-282 of: 
Coding and cryptology. Lecture Notes in Computer Science, vol. 6639. Springer, 
Heidelberg.
[41] Yuval, I., and Kushilevitz, E. 1999. Improved upper bounds on information- 
theoretic private information retrieval (extended abstract). Pages 79-88 of: Annual 
ACM Symposium on Theory of Computing (Atlanta, GA, 1999). ACM, New York.
https://doi.org/10.1017/9781009283403.010 Published online by Cambridge University Press

10
Batch Codes
Previous chapters have addressed various topics related to the distributed 
storage and transmission of data on networks of computers. These have 
included: network coding where problems related to the efficient transmis­
sion of information through a network of servers were considered; locally 
decodable and locally repairable codes capable of reconstructing a codeword 
with errors or erasures by querying a few coordinate positions; coding for 
distributed storage so the contents of a server can be restored from adjacent 
servers and private information retrieval (PIR) that considers how information 
can be coded so a user is able to download information in privacy. This chapter 
considers coding information on servers in such a manner that the amount 
of information downloaded from any particular server is limited regardless 
of the information requested - referred to as batch codes. The literature 
on this type of coding and its relationships with the other types of coding 
considered is perhaps less developed than the other aspects of information 
storage considered. The potential value of such a system is nonetheless of 
interest. A brief glimpse of the topic is given.
The next section introduces the notion of batch codes and considers some 
of their basic properties. Section 10.2 considers an important class of batch 
codes, the combinatorial batch codes (CBCs) which involve no coding of the 
original data, only the storage of carefully designed subsets in a manner that 
achieves the goal of limited download from any server. Just as there are natural 
connections between PIR protocols and locally decodable codes (LDCs), the 
final Section 10.3 considers relations between the three types of coding.
10.1 Batch Codes
The data/server model will be much as before. A database x e E" is assumed 
to be n symbols over an alphabet E, often (but not necessarily) assumed to 
255
https://doi.org/10.1017/9781009283403.011 Published online by Cambridge University Press

256
10 Batch Codes
be binary, E = {0,1}. There are m servers (often referred to as buckets in 
the literature) whose contents are possibly coded versions of subsets of x, i.e., 
there is a coding function C such that
m 
E *)
where E * indicates a string of arbitrary length over E (not the set of nonzero 
symbols as used before) - thus the map C yields a set of m strings, each of 
some length and each stored on one of the m servers.
The initial problem of interest will be to design an encoding function such 
that k users decide independently one symbol (each) they wish to recover from 
the servers in such a way that no server is required to download more than t 
symbols to satisfy the users, i.e., the users choose subscripts i1,i2,...,ik so that 
the decoding algorithm provides xi1,xi2,...,xik to thek users, downloading no 
more than t bits from any server. There are two commonly considered cases - 
where the k indices are distinct and where repetition is allowed. The first case 
might arise when a single user is requesting the symbols and the second where 
k users are requesting them and there is no coordination between the users and 
each user determines a distinct way to download the bit of interest. The second 
case will be referred to as a multiset batch code. (The standard definition of 
the term multiset allows for repeated elements.) When distinct elements are 
required, it will be referred to simply as a batch code and when repetition of 
the requests is allowed it will be referred to explicitly as a multiset batch code.
An interesting subcase is that of replication-based batch codes [14] where, 
rather than coding, subsets of the original database are stored directly on the 
servers to satisfy the download requirement. This reduces the situation to a 
purely combinatorial problem and such codes are referred to as CBCs which 
will be discussed in the next section.
The seminal paper [14] lays out many of the basic properties of batch codes.
Definition 10.1 An (n,N,k,m,t) batch code over an alphabet E is defined 
by an encoding function C: En —> (E*)m (mapping a database En of n 
symbols over an alphabet E to m nonempty strings, each of some arbitrary 
length stored on the m servers) and a decoding algorithm A such that
(i) the total number of bits (length) stored in all servers is N ;
(ii) for any databasex e En and set (distinct elements) {i1,i2,...,ik} 
c [n] the algorithm
A(C(x),i1,i2,...,ik) = (xi1,xi2,...,xik)
retrieves the k requested elements and A receives at most t symbols from each 
server (positions determined by indices).
https://doi.org/10.1017/9781009283403.011 Published online by Cambridge University Press

10.1 Batch Codes
257
It will be assumed that batch codes are systematic meaning each of the 
elements of the database is included uncoded on some server. Further, the 
term (n,N,k,m) standard batch code refers to an (n,N,k,m,1) batch code over 
E = {0,1}, i.e., t = 1 with distinct queries. The notion is easily extended to 
the case t> 1.
With minor modification:
Definition 10.2 An (n,N,k,m) multiset batch code is an (n,N,k,m) standard 
batch code (i.e., t = 1) such that for any multiset {i 1 ,i2,. ..,ik} e [n] (not 
necessarily distinct) there is a partition of the servers such that S1 ,S2,... ,Sk c 
[m] such that each item xij ,j e [k] can be recovered by reading at most one 
symbol from each server in Sj,j = 1, 2,...,k.
It is important to note that in the multiset case, when users request the same 
element from the database, distinct download estimates of that element are 
required. Equivalently the algorithm is unaware there is repetition among the 
elements requested.
Definition 10.3 A primitive batch code is an (n,N,k,m) batch code where 
each server contains a single symbol (hence N = m).
The following lemma ([14], lemma 2.4) gives some obvious relations 
between batch codes with various parameter sets. Only two of the statements 
are demonstrated:
Lemma 10.4 The following statements hold for both batch and multiset batch 
codes:
(i) 
An (n,N,k,m,t) batch code implies an (n, tN, k, tm) batch code 
(thus downloading at most a single element from a server).
(ii) An (n,N,k,m) batch code implies an (n,N,tk,t) code and 
an (n,N,kfm/t~],t) code.
(iii) An (n,N,k,m) (binary) batch code implies an (n,N,k,m) code 
over E = {0,1}w for arbitrary w.
(iv) An (n,N,k,m) batch code over E = {0,1}w implies 
a (wn,wN,k,wm) code over E = {0,1}.
To see (i), if the m servers and their contents are replicated t times, then 
rather than downloading t items from the original server, a single item can be 
downloaded from each of the t replicated servers, proving the assertion. To see 
the first part of (ii) consider an (n,N,k,m) batch code and consider expanding 
the query set from k indices to tk,sayint batches ofk. Each batch ofk queries 
uses the original t = 1 batch code, downloading at most one element from 
each server. Repeating the process for each of the t batches of queries means 
https://doi.org/10.1017/9781009283403.011 Published online by Cambridge University Press

258
10 Batch Codes
that the number downloaded from a server is at most t and hence the overall 
effect is that of an (n,N,tk,m,t) batch code. The remaining arguments are 
similar.
In a similar vein, consider the following lemma for multiset batch codes 
(referred to as the “Gadget Lemma” in [14], lemma 3.3, [18], lemma 1):
Lemma 10.5 Let C = (n,N,k,m) be a multiset batch code (t = 1). Then for 
any positive integer r there is an (rn,rN,k,m) multiset batch code Cr.
Proof: Letx ={x1,x2,...,xn} denote the database for the multiset batch code 
C. Define the r x n data array x' = (xij, i = 1,2,... ,r, j = 1,2,. ..,n), xij = 
xj,j = 1,2,...,n. In the batch code C replace the element xj in all servers 
combining it, possibly in coded form, with the r -tuple xij,i = 1, 2,...,r(the 
j-th column of the data array x'). It follows from the fact that any k-multiset 
drawn from the data array x' (of size rn) can be obtained by querying the same 
servers as in the code C and the multiset batch code property follows. ■
A simple example of a batch code from [14] is instructive. Suppose x is a 
database of n symbols and it is desired to download 2 symbols from servers 
with t = 1. This requires at least two servers each containing the entire 
database with a total storage of 2n symbols. An alternative is to consider a 
three-server case by dividing the database into two halves, say L and R and 
computing L ® R, each of the three servers containing n/2 symbols (and each 
stored on a separate server). Thus m = 3,k = 2 and we want t = 1. Consider 
xi1 ,xi2 a pair of items requested by a user, downloading at most one item 
from a server. If the items are on different servers, they can be downloaded 
successfully. If they reside on the same server (say L), then one item (say i1) 
can be downloaded from L. The other item can be obtained from downloading 
some item i3 from R and i2 ® i3 from L ® R to obtain i2, thus achieving 
the download with one item from each server. Thus an (n, 3n/2, 2, 3) multiset 
batch code. This is an example of the subcube batch codes to be considered 
shortly.
Another simple scheme is also interesting ([14], lemma 3.4). Suppose the 
database size is n and the batch code is to be primitive (N = m). The number 
of servers is m = n+ 1 with a single distinct symbol stored on each of the first 
n servers and x 1 ® x2 ®- • -® xn stored on the (n +1)-st. Two symbols are to be 
read with at most one downloaded per server. If {i1,i2} are such that i1 = i2, 
the symbols can be read directly from the associated servers (among servers 
1ton). If i1 = i2, then xi1 can be read from the i1 -th server and the other i1 
the XOR of symbols from all other servers - leading to an (n,n + 1,2,n + 1) 
primitive multiset batch code. Refer to this code as C*.
https://doi.org/10.1017/9781009283403.011 Published online by Cambridge University Press

10.1 Batch Codes
259
It is of interest to consider using existing batch codes to construct others 
and in this the following composition lemma ([14], lemma 3.5), stated without 
proof, is useful:
Lemma 10.6 Let C1 be an (n1,N1,k1,m1) batch code and C2 an 
(n2,N2,k2,m2) such that N1 = m1n2. Then there is an (n,N,k,m) batch 
code with n = n1,N = m1N2,k = k1k2, and m = m1m2. Moreover, if C1 and 
C2 are multiset batch codes, then so is C and if the servers of C2 are of equal 
size, then so also are those of C. Denote C = C 1 ® C2.
The literature on batch codes contains a number of construction techniques 
based on a variety of combinatorial structures. For the remainder of this section 
only a few types are considered: the subcube batch codes, batch codes obtained 
from linear error-correcting codes, batch codes derivable from bipartite graphs 
and batch codes from multiplicity codes. While the constructions discussed 
are typical, they are by no means exhaustive of the techniques available. The 
original paper on batch codes [14] considered several such constructions.
Subcube Codes
Subcube codes [14] have previously been described for LDC and PIR codes 
and their construction for (multiset) batch codes is on very similar lines. 
Consider the 3 x 3 square array with parity checks on rows and columns and 
on checks as illustrated in Figure 10.1(a). The 3 x 3 array contains the nine 
elements of the database. There are three row checks and three column checks. 
There is an overall parity check (which is the XOR of all nine elements in 
the database). It is easy to check that any four elements of the database can 
be recovered by downloading a single element from the appropriate cells. 
Recall that in the multiset case four distinct downloads must be obtained, 
corresponding to four users requesting the same database element.
Example 10.7 To obtain the multiset x2,x2,x2,x2 from the (9,N = m = 
16,k = 4) primitive multiset batch code of Figure 10.1(a) one can download 
the sets as follows:
{x2}
{x 1 ® x2 ® x3,x 1 ,x3}
{x2 ® x5 ® x8,x5,x8}
| ® xi,x4 ® x5 ® x6,x7 ® x8 ® x9,x 1 ® x4 ® x7,x4,x7,x3 ® x6 ® x9,x6,x9}
Adding the elements within a set yields an independent estimate of x2. 
Notice that no cell is downloaded from more than once.
https://doi.org/10.1017/9781009283403.011 Published online by Cambridge University Press

260
10 Batch Codes
single
parity checks parity check 
on columns on all checks
x 2 ® X 5 ® X 8
x 1 ® x 4 ® x7 x 3 ® x 6 ® x 9
X1 X2 X3
x1 ® x2 ® x3 
x4 ® x5 ® x6 
x7 ® x8 ® x9
(a) 
(b)
parity checks 
on rows
Figure 10.1 Subcube codes: (a) d = 2(b) £ = 2 arbitrary £
The example is generalized to £ x £ database cells as shown in Figure 10.1(b) 
and it is straightforward to verify that this gives an (£2,N = m = (£ + 1)2, 
k = 4) primitive multiset batch code. The check on parity-checks cell contains 
an overall parity check.
This two-dimensional case codes can be generalized to higher dimensions. 
For example in three dimensions (d = 3) one can form an £ x £ x £ cube of cells 
containing the database. One could form a 1 x £ x £ array on the right-hand 
side of parity check on rows of the database. Similarly one can form arrays 
of cells of parity checks on front to back rows and also of columns to form a 
total of 3£2 parity-check cells. Parities on the 3 x £ x £ “side” parities can be 
formed and finally one could form a single cell containing the XOR of all £3 
database elements. One can verify that such is a primitive multiset batch code 
with parameters (n = £3,N = m = (£ + 1)3,k = 23).
The extension to higher dimensions is immediate to yield, in dimension d, 
an (n = £d,N = m = (£ + 1 )d,k = 2d) primitive multiset batch code ([14], 
lemma 3.6).
Batch Codes from Linear Codes
The notions of using LDCs, in particular smooth codes, and their relation to 
batch codes, were noted in [14] and are discussed further in Section 10.3. The 
purpose of this subsection is to consider more direct uses of the properties 
of linear block codes. The principles of such an approach were laid down 
in the work of [16]. There is a conflict in notation between the areas of 
https://doi.org/10.1017/9781009283403.011 Published online by Cambridge University Press

10.1 Batch Codes
261
error-correcting codes and their application to batch codes. We adjust the 
notation of other works to that introduced so far in this chapter.
Definition 10.8 The alphabet S = Fq is considered (rather than the binary 
considered so far). A primitive multiset batch code will be referred to as a 
linear batch code if the contents of all servers are linear combinations over Fq 
of database elements.
Only primitive multiset batch codes will be considered (one Fq element per 
server or bucket). For the finite field Fq let x = {x 1 ,x2,.. .,xn} e Fq be the 
database and let the contents of the m servers be j = {y 1 ,y2,..., ym} e Fq7 
consisting of linear combinations over Fq of the database elements, stored in 
the N = m servers, one Fq element per server. In particular let
yi = 
gj,ixj, i = 1,2,...,m, j= 1,2,...,n xi,yj,gi,j e Fq
j=1
and form the n x m matrix G = (gj,i), i.e., j = xG. Let g(i) e Fm denote the 
i-th row of G,i = 1,2,...,nandg(j) the j-th column, j = 1, 2,...,m.
Denote the i-th indicator column vector et(i) e Fqn with a one in the 
i -th position and zeros elsewhere. The matrix G will define an (n,N = 
m, k,m)q multiset batch code (over Fq )ifany multiset of database items 
{xi1,xi2,...,xik} can be retrieved from the contents of the servers in k disjoint 
ways. Suppose from G it is possible to form k disjoint sets of columns 
of G, Sj,j = 1,2,...,k such that for any such database subset (multiset) 
{i1,i2,...,ik} it is possible to form k distinct sums of columns such that
ej = Z2 az g(e'l,j = 1,2, ...,k,
where et(i ) is an indicator vector (weight one) and all such indicator vectors 
can be formed. It follows the query set can be obtained as
xij = x, et(i ) 
(inner product).
Note the matrix G must have rows of weight at least k since the columns of G 
can be partitioned to form e(tj) from k distinct subsets of columns and similarly, 
G must be of full rank since any of the indicator column vectors et(j),j = 
1,2,...,n can be formed. Numerous other properties of the code are given 
[16]. The success of this method depends on the ability to form the column 
subsets required.
https://doi.org/10.1017/9781009283403.011 Published online by Cambridge University Press

262
10 Batch Codes
Example 10.9 An example of such codes from binary Hamming codes [2] 
is adapted. Consider a binary Hamming code (2£ - 1,2£ - 1 - £, 3)2 with 
(2£ — 1 — £) x (2£ — 1) generator matrix G and £ x (2£ — 1) parity-check matrix 
H. Assume G is in systematic form. A binary nonprimitive batch code with 
the following parameters is constructed with: n = 2£ — 1 — £ binary database 
elements total storage of N = 2£ — 1 bits, k = 2 queries and m = 2£—1 
servers. Let x e F2 be the n binary database elements, and let y = xG e FN 
be the corresponding codeword, and yHt = 0N , the all-zero word of length 
N.Allm servers will contain 2 bits of the codeword y except one which 
will contain a single element. For convenience label the columns of H by 
hi,i = 1,2,..., 2£ — 1 and associate the i -position of y,yi, with column hi .The 
columns of H consist of all 2£ — 1 nonzero binary £-tuples. If hi ® hj = 1 £, 
the all-ones £-tuple, then bits yi and yj are placed on the same server. The 
columns of H divide into 2£— 1 — 1 pairs that add to 1 £ plus the all-ones £ - 
tuple. The single element corresponding to the all-ones column ofH is placed 
into the last server. Notice wlog it can be assumed the code is systematic and 
database elements ofx appear explicitly. The database elements also appear as 
coded on other servers.
To show that this arrangement allows a batch code with k = 2 and t = 1, 
argue as follows. It is desired to retrieve yi and yj ,i = j . If they reside on 
separate servers, they can be retrieved. If they are on the same server, it is 
argued as follows. Suppose yi is downloaded. Let hj be the binary £-tuple 
column of H associated with yj . It will always be possible (usually in many 
different ways) to represent hj as the sum of two other columns say hr and 
hs, hj = hr ® hs, corresponding to bits ys and yr with neither hr nor hs equal 
to hj. These clearly reside on distinct servers and yr and ys can be downloaded 
and added to give yj = yr ® ys, resulting in obtaining yi and yj with t = 1 
when they are on the same server.
Other techniques for obtaining batch codes from linear codes, GRM codes 
in particular are discussed in [2] as well in [14].
In general, it can be shown that the generator matrix for an (n,N,k,m = N) 
primitive batch code yields a binary error-correcting code of length N, dimen­
sion n and minimum distance > k. It is noted the converse is not true in that 
the generator matrix of an error-correcting code may not yield a batch code.
Batch Codes from Bipartite Graphs
The use of unbalanced bipartite expander graphs in constructing good batch 
codes was noted in [14] (a balanced bipartite graph has an equal number of left 
and right vertices). This approach is able to take advantage of the vast literature 
https://doi.org/10.1017/9781009283403.011 Published online by Cambridge University Press

10.1 Batch Codes
263
on such graphs but tends to produce batch codes that are not optimal. The more 
combinatorial/constructive approach in [18] is considered here.
Consider a bipartite graph G(V1,V2,E) with a left set of vertices V1, 
| V1 |= n and a right set, V2, | V2 |= N - n with edge set E containing 
edges only between the two sets. It is shown how, under certain circumstances, 
such a graph can yield an (n,N,k,m) primitive multiset batch code (theorem 
1 and lemma 2 of [18]). The n left vertices are associated with the database 
symbols and assumed binary for this discussion. The N - n right vertices are 
associated with parity checks on the left vertices and the total N symbols will 
be associated with the contents with the N = m servers, one per server (i.e., a 
primitive multiset code). The resulting code is systematic. (This is not a Tanner 
graph of a code as defined in Chapter 3 as the left vertices are only the database 
bits and do not include parity bits.)
The notion of repair group in the context of the graph is considered. The 
left vertices, associated with the database bits, are labeled x1,x2,...,xn and 
the right vertices with the parity checks cn+1,cn+2,...,cN. There is an edge 
between xi and cj if xi participates in the check cj and the edge is denoted 
(xi,cj) e E. As in Chapter 3 denote the set of neighbors of the left node xi 
as Nxi, and of the check node cj by Ncj. Further, assuming (xi,cj) e E is an 
edge of the graph, denote by Nxi,cj (resp. Ncj,xi) as the set of neighbors ofxi 
except cj (resp. as the set of neighbors of cj except xi). Let nxi =|Nxi | denote 
the degree of left node xi and similarly ncj, |Ncj | for right nodes.
Recall that, identifying data bits and check bits with their nodes,
cj = ® xk, 
xkeNcj
i.e., the value of the check symbol cj is the mod 2 sum of the connected left 
data symbols.
It follows directly from this observation that a repair group for the database 
symbol xi can be formed for each cj e Nxj as the mod 2 sum of the data 
symbols in the set
Ri,j = xk e Ncj ,xi ,
i.e., the set of all left nodes participating in the formation of the check symbol 
cj except the symbol xi itself. Clearly Ri,j is a repair group for the data symbol 
xi since it is the mod 2 sum of its variables that gives xi , i.e., for (xi,cj ) e E
since cj = ® xk then xi = cj ® xi.
xk eNcj 
xk eNcj,xi
This is true for each set Ri,j, cj e Nxi .
https://doi.org/10.1017/9781009283403.011 Published online by Cambridge University Press

264
10 Batch Codes
The problem might arise that these repair groups are not disjoint since only 
one download is permitted per server. Before discussing this issue, recall that 
for a subset of vertices of a general graph G with vertex set V , for any subset 
of vertices U c V the induced subgraph G (U) = G is the graph that contains 
all the edges of (vi,vj) e E with vi,vj e U.
The conditions and parameters of theorem 1 and lemma 2 of [18] guarantee 
that the above repair groups are in fact disjoint resulting in the primitive 
multiset batch code being (n,N,k,m) and these conditions are as follows. 
Returning to bipartite graphs, suppose there exists a subset of check nodes, 
V2 C V2 such that the induced subgraph G(V1, V2) (consisting of all edges 
between V1 and V2) with the property that
(i) all vertices in V1 have degree at least k ;
(ii) the bipartite induced graph has girth at least 8.
Then the code C = (n,N,k,m = N) is a primitive multiset batch code.
The proof of this statement shows some surprising elements. Suppose there 
is a subset of check nodes V2 satisfying the above conditions. Then it is shown 
the k repair groups are indeed disjoint and, further, that for i = j,i, j = 
1,2,...,n any one of the disjoint repair groups for xi has common elements 
with at most one of the disjoint repair groups for xj .
The proof of this interesting result is intricate. While not proven here the 
following observations are noted. The requirement (i) above ensures that each 
data symbol vertex has at least k repair groups. It is first established that, 
under the conditions stated, the repair groups associated with a data symbol 
vertex are disjoint. Assuming the contrary, it can be shown to imply a graph 
cycle of length 4 which is a contradiction. To show that repair groups for xi 
have common symbols with at most one of the repair groups of xj,i= j, 
again assume the opposite. It is shown this implies either a cycle of length 4 
or 6 which is again a contradiction to the assumption of girth 8. To complete 
the proof of the theorem, an algorithm is given ([18], figure 4) that assigns k 
queries to k disjoint sets of data symbols.
The remainder of the paper [18] is concerned largely with construction 
techniques for suitable large girth bipartite graphs, a problem that has received 
attention in the mathematical literature in the sense there are many papers on 
the construction of bipartite graphs with large girth. Numerous constructions of 
suitable graphs are given that result in a wide variety of batch code parameters.
Batch Codes from Multiplicity Codes
Itis shown [1] that the multiplicity codes defined in Chapter 8 can also be used 
to define a class of (primitive) batch codes (as they were used in Chapter 9 
https://doi.org/10.1017/9781009283403.011 Published online by Cambridge University Press

10.2 Combinatorial Batch Codes
265
to define a class of k-PIR codes). A slightly modified definition of batch code 
is used:
Definition 10.10 A code C will be called a k-batch code, denoted (N,n,k)q 
if for every multiset request ofk symbols, {i1,i2,...,ik}, there exist k mutually 
disjoint subsets Ri 1 ,Ri2, ...,Rik of [N] such that for all j e [k], xij is a 
function of the symbols in Rij .
Here it is assumed a database of n symbols from Fq is encoded into N 
symbols over Fq such that the (N, n)q code is linear and a single coded symbol 
is stored on each server.
Recall from Chapter 8 the multiplicity code Cm,d,s,q using m-variable poly­
nomials of degree at most d and order s-evaluations over Fq has parameters
length = qm, rate = —m —, normalized distance A = 1------.
(m +m “1) qm 
sq
It is shown in [1] that the multiplicity code Cm,d,s,q for d < s(q — ksm“1 - 2) 
and k < [q/sJm“1 is a k batch code with qm servers, a database of length 
n = d+mm and symbol alphabet of size q (m+ms“1) in that k disjoint recovery 
sets can be generated in the same manner as for the PIR codes, basically 
by considering lines in the vector space Fqm that intersect in a single point, 
representing codeword coordinate positions. The code is of length N = qm 
and a single Fq element is stored on each server. By the definition of disjoint 
recovery sets any multiset of k < [q/sJm“1 requests can be satisfied with no 
server downloading more than a single element.
The constructions of batch codes described here are but a few on the many 
available. Describing the range of combinatorial techniques brought to bear on 
the problem would require a separate volume.
10.2 Combinatorial Batch Codes
A special type of batch code, the CBC has attracted attention in the literature. 
In the original work [14] such a code was referred to as replication coding, 
where the information stored on the servers is a subset of the database x, i.e., 
the stored information is not coded at all. Furthermore, only the case of t = 1, 
where only one symbol can be read from a server is generally considered and 
the set of k queries is distinct (although multiset combinatorial codes have 
been considered [24]). These conditions tend to cast the situation as a purely 
combinatorial problem allowing standard techniques to be used.
https://doi.org/10.1017/9781009283403.011 Published online by Cambridge University Press

266
10 Batch Codes
The following definition (from [17]) specializes the definition of a CBC in 
a convenient form:
Definition 10.11 An (n,N,k,t) CBC is a set system (x, B), where x = 
{x1,x2,...,xn} (the database) is a set of n symbols (also called items, often 
bits), B = {B1,B2,...,Bm } is a collection of m subsets of x stored on m 
servers, S, where Bi is the set of elements of the database x stored on server 
i, with a total storage of N = Bi\Bisb | Bi |, such that for each k-subset of 
(distinct) subscripts {xi 1 ,xi2,... ,xik} c x there exists a subset Ci c Bi, where 
|Ci |< t,i = 1,2,... ,m such that
|xi 1 ,xi2, ...,xi-k| c UCi.
Such a set system could as well be described by the sets B* = 
{B*,B*, ...,B*} where B* is the set of servers storing element xi of the 
database x and S the set of servers. Such a pair of sets (S, B*) is referred to as 
the dual set system. In the set system (x, B) the set x is referred to as the set 
of points and B the set of blocks while in the dual set system S is the set of 
servers and B* the set of blocks.
Since consideration is restricted to the case t = 1 for CBCs, they will be 
designated as (n,N,k,U) CBCs. Much of the work on such codes is centered 
on the problem of determining codes, for given parameters n, k and U, that 
have the lowest storage requirements N . The least such value of N for given 
parameters is denoted N(n,k,U) and the corresponding batch code with this 
storage is referred to as optimal. A problem of interest is to determine this 
value for various ranges of parameters.
Interest in this section is restricted to a few of the many examples of 
parameter sets where exact descriptions of optimal CBC batch codes exist 
as well as to introduce a few of the combinatorial structures used to obtain 
optimal codes. Only a few of the many bounds on N(n,k,U) will be noted. 
The standard batch code model is assumed - distinct queries - although the 
multiset version will be noted later.
Consider first some simple examples [17] of optimal CBCs, batch codes 
that achieve the minimum amount of storage for given n,k, U. If the number of 
servers is equal to the size of the database n, then clearly one symbol can be 
placed in each server and
N(n,k,n) = n.
If the number of servers U is equal to the query size, k, then we must have 
([17], theorem 2.2)
N(n,k,k) = kn — k(k — 1), n > k.
https://doi.org/10.1017/9781009283403.011 Published online by Cambridge University Press

10.2 Combinatorial Batch Codes
267
For the database x = {x1,x2,...,xn} a code that achieves this bound places 
x1,x2, ...,xk into the k servers, one symbol per server. (The argument is valid 
for any distinct k elements.) It places all the remaining symbols xk+1,...,xn 
into each of the servers for a total of k(n - k + 1) storage. To see that this 
is optimal, consider the query for the symbols x1,x2,...,xk. These symbols 
must appear in different servers (by the batch code assumption). Since t = 1 
the server containing x1 say, does not contain another copy of x1. However, it 
must contain each of the symbols xk+1,...,xn since the query xj ,x2,...,xk 
must be satisfied for each xj,j = k + 1,...,nand can only be satisfied if the 
server contains all these elements. Similarly for the other servers.
A slight variation of this argument ([17], theorem 2.5) shows that
N(m + 1,k,m) = m + k.
In this case the optimal code [4, 6] places items x1,x2,...,xm in separate 
servers, one symbol per server (any m distinct items). Note the database is 
{x1,x2,...,xm,xm+1}. It then places k copies of xm+1 in any k of the servers 
(one copy per server chosen). A query of the form x1,...,xk-1,xm+1 can be 
satisfied since xm+1 is contained in at least one of the servers not containing 
the first k - 1 symbols.
The determination of N(m + 2,k,m) is surprisingly more difficult. It was 
established in [6] (and in [8]) that for all m > k > 1:
(i) if m + 1 - k > rvm1, then N(m + 2,k,m) = m + k - 2 + ^Vk+lj; 
(ii) if m + 1 - k< rVkTT], then N(m + 2,k,m) = 2m - 2 + 1 + —k+1 .
m+1-k
In addition it is shown that
N(m + 2,k,m) = N(m + 1 ,k,m - 1) + 1
= m + k - 2 + W k + 1
for all m > k + Vk.
More generally [8],
N(n,k,m) < N(n - 1,k,m - 1) + 1 for all n > m > k + 1 > 2
N(n,k,m) < 2m + (n - m - 1) m-k 
- b for all n > m + 2
n - m-1 +1
b = remainder (m - k) (mod m - k + 1).
An interesting general result of [17] is outlined. Suppose n > (k - 1) km-1 . 
Consider creating an m x n incidence matrix A of a dual set system with rows 
indexed with the m servers and columns indexed with the n database elements. 
Let the first (k - 1) k-m1 columns of A each contain (k - 1) copies of each 
possible server subset of size (k - 1) - meaning that each particular column
https://doi.org/10.1017/9781009283403.011 Published online by Cambridge University Press

268
10 Batch Codes
with (k — 1) ones appears (k — 1) times for each of the (k--J possible server 
subsets of size (k - 1). The remaining n - (k - 1) km-1 columns ofA are filled 
with any k ones arbitrarily. The total number of elements stored is
N = (k — 1)2 km—1 + k n — (k —
= kn — (k — 1) km—1
1) (km— 1))
(10.1)
which is also obtained by noting that each column contains either k or k — 1 
ones and the number containing (k — 1) is (k — 1) km—1 . This construction 
clearly represents an (n,N,k,m) CBC since, by construction, for any k 
elements of the database it will be possible to findk distinct servers from which 
to download an element. The argument will be made more convincing later in 
the section through an argument involving systems of distinct representatives 
and Hall’s conditions on set systems and transversals.
The following results of [17] are shown without proof:
Theorem 10.12
(i) 
For n > (k — 1 ^-) , N(n,k,m) < kn — (k — 1 ^k—J.
(ii) 
For n > (k — 1 )(km1) , N(n,k,m) = kn — (k — 1 ^km—J.
The following result has also been shown by several authors [4, 7, 19]:
m
For all
< n < (k —
m
1
N (n,k, m) = (k — 1)n —
(k — 1) km
—1 — n
m—k+1
2
Also [19]
(i) If n > (k — 1) k—m1 — m + k, then N (n,k,m) = n(k — 1).
(ii) If n > (k — 1) km
—1 — m + k — 1, then N(n,k,m) = n(k — 1) — 1.
Numerous other works are available for certain parameter sets [9, 15, 20]. 
Also a complete enumeration [7] of N(n, 3,m) and N(n,4,m) is given in [7]. 
It remains an open problem to determine N(n,k,m) for n < km
—2 .
Recall from Definition 10.11 that the dual set system (S, B*) where S = 
{S1 ,S2,..., Sm} is the set of m servers and B* = {B*,B*,..., Bn} is the set of 
blocks, where Bi* represents the set of servers that database element xi resides 
on. Such a system is entirely equivalent to the m x n incidence matrix A = 
(ai,j):
f 1 if xj e Si 
ai j = 
, 
0 otherwise
where the matrix rows represent servers and columns the database elements.
https://doi.org/10.1017/9781009283403.011 Published online by Cambridge University Press

10.2 Combinatorial Batch Codes
269
Suppose the incidence matrix A has the property that for any k columns 
(corresponding to k database elements) there exist k rows (corresponding to 
servers) such that the k x k submatrix has the property that there are 1’s in 
each of the k rows that appear in distinct columns. If for any k columns there 
are k rows with this property, it is described as having a k-transversal. Clearly 
the dual set system having a k-transversal for any set ofk columns is equivalent 
to a CBC (n,N,k,m).
These comments are closely related to the notion of a system of distinct 
representatives (SDRs) defined as follows:
Definition 10.13 A set system on x is a set of subsets of the set x, A = 
{A 1 ,A 2,... ,Am },Ai- c x,i = 1,2,... ,m. The set system has an SDR if there 
exists an m-tuple of elements {x 1 ,x2,... ,xm} such that xi e Ai,i e [m] and 
for i = j, xi = xj, i.e., the representatives of the sets xi are distinct.
The following Hall’s theorem (sometimes referred to as Hall’s marriage 
theorem [5]) gives conditions for the existence of an SDR:
Theorem 10.14 A set system A = {Ai,A2,...,Am } has an SDR iff for any 
set S c [m]
I U Ai I > I51.
ieS
The reference to “marriage theorem” follows if one interprets the m sets as 
the sets of boys m girls know, then having an SDR is equivalent to each girl 
knowing at least one distinct boy.
For the study of CBCs a slightly weaker form of the theorem is sufficient 
[4, 17]: Given the dual set system (S,B*) a CBC (n,N,k,m) will exist iff for 
any k subset of B*, {B*,..., B*} it is true that
1<U< B j >r
for all 0 < r < k.
HC1
An equivalent form of this condition that is sometimes more convenient 
to apply is that any r -element subset of S contains at most r sets of B* for 
0 < r < k - 1 as otherwise Hall’s condition would be violated, i.e., B* has an 
SDR iff
Any 5 c S, 151 = r, contains at most r sets of B* for all r < m,
0 < r <k- 1. 
HC2
These conditions on sets will be used in the construction of CBCs [4, 17].
The relevance of SDRs to the batch coding problem is as follows. If the dual 
set system (S, B*) is an (n,N,k,m) CBC, then any k sets of B* must have an 
https://doi.org/10.1017/9781009283403.011 Published online by Cambridge University Press

270
10 Batch Codes
SDR, i.e., by definition, to retrieve the query set xi1,xi2,...,xik the code finds 
servers Si 1 ,Si2,..., Sik,xij in Sij, i.e., Sij e B*. as required.
Other aspects of CBCs are briefly noted. The notion of a c-uniform CBC or 
fixed-rate CBC was introduced in [17] as follows:
Definition 10.15 A c-uniform CBC is one in which each database element is 
contained in exactly c servers. Alternatively it is one whose m x n incidence 
matrix has each column containing c 1's.
Clearly the total number of stored elements is cn and results in an 
(n, cn, k, m) CBC. The issue of interest for such systems is the maximum 
value of n for which such a system exists which is denoted by n(m, c,k). 
The uniformity imposed on the problem allows a variety of combinatorial 
techniques to be employed in their construction.
A great many results on such systems are available. For example the 
following bound on n(m, c,k) [17] can be shown: Consider an km-1 x n matrix 
M = (Mij) with the rows labeled by all possible (k - 1) sets of the m servers 
and the columns labeled with the blocks of the dual system (which are blocks 
of servers containing a given database element). Let Mij = 1 if the block of 
column j is contained in the row corresponding to a set of (k - 1) severs. 
By the modified Hall’s condition HC2 any k blocks of the dual system must 
contain at least k elements and hence each row of the matrix M can contain at 
most (k — 1) 1 s. Since by the c-uniformity assumption each block of the dual 
system contains c elements (the c servers the corresponding database element 
is stored on) and each such set is contained in km—1——cc sets and hence each 
column of M contains this many 1’s. Thus
n
m
k
—c
1—
< (k — 1)
m
c
k
1
which is manipulated to
n(m, c,k) < (k - 1 )C) 
(k—')
A simple construction [17] that led to Equation 10.1 can also be used for c- 
uniform CBCs. Consider the following m x n incidence matrix of a c-uniform 
CBC with m servers and a database of n elements. Each of the possible mc 
columns with c 1's is replicated c times. It is seen this represents an (n = 
c mc ,N = c2 mc ,k = c + 1,m) CBC. There are a great many constructions 
of such uniform CBCs in the literature that take advantage of the many 
combinatorial regular structures and readers are referred to [3, 4, 17, 19, 22] 
for other results.
https://doi.org/10.1017/9781009283403.011 Published online by Cambridge University Press

10.3 The Relationship of Batch Codes to LDCs and PIR Codes
271
This section has focused on the condition that t = 1. For the extension 
to t>1 readers are referred to [9, 10, 19]. In addition the CBC codes studied 
here are of the standard type (distinct download elements). The case of multiset 
CBCs has been considered in [24, 25].
Hall’s theorem is used in many works on constructing CBCs. In [9, 10]a 
modified form of Hall’s theorem (a (k, t) Hall condition) is used to consider 
construction of CBCs with t>1.
The following is also shown ([9], proposition 12): For every set of positive 
integers n,k,m and t if m = \k/t~\ and n > tm, then N(n,k,m,t) = mn — 
tm(m — 1) which gives an interesting generalization of a theorem of [17].
Theorem 13: If m >|" k/t "| and n>t 
k — 1 
™ J, then
t 
Ik/t 11
N(n,k,m,t) = n
m
1
r k/t i — 1
The notions of transversals and SDRs are used in [6] to find optimal CBCs 
for n = m + 1 and m + 2 as already noted. Similarly, these notions are used in 
[22] to construct transversal designs and from such designs, c-uniform CBCs 
for c ~ k- - as noted above they were previously only known for c = 2,k — 1 
or k - 2.
10.3 The Relationship of Batch Codes 
to LDCs and PIR Codes
As mentioned, there are strong relationships between the notions of LDCs, 
PIR codes and batch codes. All notions deal with aspects of retrieving data 
from storage according to different scenarios. A few comments on these 
relationships are in order.
The notion of recovery sets for information bits, central to PIR codes and 
batch codes, was already present in the definition of one-step majority logic 
decodable binary codes from the 1960s. As noted previously, for such codes it 
is possible to formulate 2e + 1 parity checks with the property that each check 
contains the information bit of interest and whose other check bits are disjoint 
with other check equations. This is another version of disjoint repair groups. 
Thus if e or fewer errors are made in transmission, a majority of the check 
equations applied to the received word are correct.
That a smooth LDC can also be viewed as a batch code [14] is evident in 
the following sense. For a smooth LDC code one can recover an information 
symbol by querying up to some fixed number of codeword positions. If the 
https://doi.org/10.1017/9781009283403.011 Published online by Cambridge University Press

272
10 Batch Codes
symbols of the LDC codeword of length N are placed in N servers, and the k 
queries of the batch code are processed serially as an LDC code, one obtains 
from the responses a batch code. Of course there is no notion of errors in the 
batch code scenario.
Other relationships between batch, PIR and LDC codes are noted in [23] 
and [13]. They are also illustrated in the work [1] where, as has been described, 
several classes of batch and PIR codes are constructed from multiplicity and 
array codes.
An (r,S,e) LDC has been defined (8.3) as one that produces an estimate of 
an information symbol by making at most r queries to codeword symbols and 
yields a correct answer with probability at least 1 - e if the fraction of codeword 
errors is at most S. Other versions such as locally repairable codes produce 
estimates of the codeword coordinate position (rather than just information 
coordinates) which is similar to the case of locally correctable codes (LCCs). 
The notion of locality and availability has also been considered [23]:
Definition 10.16 The code C has locality r > 1 and availability a > 1 if for 
any j e C any symbol in the codeword can be reconstructed by using any of a 
disjoint sets of symbols, each of size at most r .
The notion of a k-server PIR code introduced in [12], discussed in Chapter 
9, involves querying coded databases and involved the construction of an 5 x m 
binary matrix G which has property Ak if, for each i e [s], there exists k 
disjoint subsets of columns of G that add to e(i) the indicator 5-tuple of the i -th 
position. A binary linear [m, 5] code C will be called a k-server PIR code if 
there exists a generator matrix G for C with property Ak, i.e., for each i e [5] 
there exist k disjoint sets of columns of G,Rj(i),i = 1, 2,...,5, j = 1, 2,...,k, 
such that for each indicator vector
e(i) = £ mj = ...= £ mj, i = 1,2,...,s.
The notion of disjoint repair groups is common to locally decodable, PIR and 
batch codes.
The idea ofa distributed storage code is somewhat different in that they seek 
to be able to reconstruct the contents of a failed server in an efficient manner 
(in terms of downloads) from adjacent servers.
Various relationships exist between these types of codes. For example ([23], 
corollary 1) a linear systematic code is an LRC code with locality r and 
availability a = t - 1 of information symbols iff it is a PIR code with queries 
of size t and size of reconstruction sets at most r . Additional references on 
this problem include [11] and [21].
https://doi.org/10.1017/9781009283403.011 Published online by Cambridge University Press

References
273
Comments
Of the coding systems considered in this volume, the notions of PIR and 
batch codes are perhaps the least developed. However, given the importance 
of privacy and server efficiency problems in today’s information storage and 
transmission environment, it is not too difficult to imagine that a major 
development in either area could have a far-reaching impact on commercial 
systems.
References
[1] Asi, H., and Yaakobi, E. 2019. Nearly optimal constructions of PIR and batch 
codes. IEEE Trans. Inform. Theory, 65(2), 947-964.
[2] Baumbaugh, T., Diaz, Y., Friesenhahn, S., Manganiello, F., and Vetter, A. 2018. 
Batch codes from Hamming and Reed-Muller codes. J. Algebra Comb. Discrete 
Struct. Appl., 5(3), 153-165.
[3] Bhattacharya, S. 2015. Derandomized construction of combinatorial batch codes. 
CoRR, abs/1502.02472.
[4] Bhattacharya, S., Ruj, S., and Roy, B. 2012. Combinatorial batch codes: a lower 
bound and optimal constructions. Adv. Math. Commun., 6(2), 165-174.
[5] Bollobas, B. 1986. Combinatorics. Cambridge University Press, Cambridge.
[6] Brualdi, R.A., Kiernan, K.P., Meyer, S.A., and Schroeder, M.W. 2010. Combina­
torial batch codes and transversal matroids. Adv. Math. Commun., 4(3), 419-431.
[7] Bujtas, C., and Tuza, Z. 2011. Optimal batch codes: many items or low retrieval 
requirement. Adv. Math. Commun., 5(3), 529-541.
[8] Bujtas, C., and Tuza, Z. 2011. Optimal combinatorial batch codes derived from 
dual systems. Miskolc Math. Notes, 12(1), 11-23.
[9] Bujtas, C., and Tuza, Z. 2012. Relaxations of Hall’s condition: optimal batch 
codes with multiple queries. Appl. Anal. Discrete Math., 6(1), 72-81.
[10] Bujts, C., and Tuza, Z. 2011. Combinatorial batch codes: extremal problems 
under Hall-type conditions. Electron. Notes Discrete Math., 38, 201-206. The 
Sixth European Conference on Combinatorics, Graph Theory and Applications, 
EuroComb 2011.
[11] Diffie, W., and Hellman, M. 1976. New directions in cryptography. IEEE Trans. 
Inform. Theory, 22(6), 644-654.
[12] Fazeli, A., Vardy, A., and Yaakobi, E. 2015. PIR with low storage overhead: 
coding instead of replication. CoRR, abs/1505.06241.
[13] Henry, R. 2016. Polynomial batch codes for efficient IT-PIR. Proc. Priv. Enhanc­
ing Technol., 2016(02).
[14] Ishai, Y., Kushilevitz, E., Ostrovsky, R., and Sahai, A. 2004. Batch codes and 
their applications. Page 262-271 of: Proceedings of the Thirty-Sixth Annual ACM 
Symposium on Theory of Computing. STOC ’04. Association for Computing 
Machinery, New York.
https://doi.org/10.1017/9781009283403.011 Published online by Cambridge University Press

274
10 Batch Codes
[15] Jia, D., and Gengsheng, Z. 2019. Some optimal combinatorial batch codes with 
k = 5. Discrete Appl. Math., 262, 127-137.
[16] Lipmaa, H., and Skachek, V. 2015. Linear batch codes. Pages 245-253 of: 
Coding theory and applications. The CIM Series in Mathematical Sciences, vol. 
3. Springer, Cham.
[17] Paterson, M.B., Stinson, D.R., and Wei, R. 2009. Combinatorial batch codes. Adv. 
Math. Commun., 3(1), 13-27.
[18] Rawat, A.S., Song, Z., Dimakis, A.G., and Gal, A. 2016. Batch codes 
through dense graphs without short cycles. IEEE Trans. Inform. Theory, 62(4), 
1592-1604.
[19] Ruj, S., and Roy, B.K. 2008. More on combinatorial batch codes. CoRR, 
abs/0809.3357.
[20] Shen, Y., Jia, D., and Gengsheng, Z. 2018. The results on optimal values of some 
combinatorial batch codes. Adv. Math. Commun., 12(11), 681-690.
[21] Shor, P.W. 1994. Algorithms for quantum computation: discrete logarithms 
and factoring. Pages 124-134 of: 35th Annual Symposium on Foundations of 
Computer Science (Santa Fe, NM, 1994). IEEE Computer Society Press, Los 
Alamitos, CA.
[22] Silberstein, N., and Gal, A. 2016. Optimal combinatorial batch codes based on 
block designs. Des. Codes Cryptogr., 78(2), 409-424.
[23] Skachek, V. 2016. Batch and PIR codes and their connections to locally repairable 
codes. CoRR, abs/1611.09914.
[24] Zhang, H., Yaakobi, E., and Silberstein, N. 2017. Multiset combinatorial batch 
codes. Pages 2183-2187 of: 2017 IEEE International Symposium on Information 
Theory (ISIT).
[25] Zhang, H., Yaakobi, E., and Silberstein, N. 2018. Multiset combinatorial batch 
codes. Des. Codes Cryptogr., 86(11), 2645-2660.
https://doi.org/10.1017/9781009283403.011 Published online by Cambridge University Press

11
Expander Codes
Expander graphs, which informally are graphs for which subsets of vertices 
have a large number of neighbors, have been of interest to numerous areas of 
mathematics and computer science. The paper/monograph [20] is an excellent 
reference for such graphs. In the context of coding, the notion of graph 
expansion arose in the search for codes that could be encoded and decoded 
with algorithms that have linear complexity in code length.
The aim of this chapter is to understand how the notion of graph expansion 
leads to the interesting class of expander codes which have low decoder 
complexity and excellent performance. The literature on the subject of codes 
and expander graphs is large and only the fundamentals are discussed.
The next section introduces basic graph notions and defines graph expan­
sion along with the use of eigenvalues of the graph adjacency matrix as an 
aid in determining graph expansion properties. The important notion of a 
Ramanujan graph is noted but, except for an example, is not pursued. Many of 
the expander code constructions use a particular technique (and variants) due 
to Tanner [30], and all such variants will be referred to here as Tanner codes. 
This construction and its properties are discussed in Section 11.2. The final 
Section 11.3 gives a discussion of the seminal work of Sipser and Spielman 
[29] on expander codes as well as some related work.
11.1 Graphs, Eigenvalues and Expansion
All graphs are assumed to be finite, connected, undirected and with no multiple 
edges or self-loops. A graph G with vertex set V and edge set E will be denoted 
G = (V, E). The work will be concerned largely with two classes of graphs, 
the d-regular graphs and (dv,dc)-regular bipartite graphs. A few of their basic 
properties are noted.
275
https://doi.org/10.1017/9781009283403.012 Published online by Cambridge University Press

276
11 Expander Codes
The degree of a vertex v e V is the number of edges incident with v. 
The graph G will be denoted d-regular if all vertices have degree d . This 
use of the parameter d is unfortunate in that it is also used conventionally for 
code minimum distance. The convention followed in this chapter (as in other 
chapters) is that d will denote vertex degree while any code minimum distance 
will be denoted explicitly as dmin.
The diameter of a graph is the maximum distance (edge distance - the 
number of edges traversed) between any two distinct vertices and the girth 
of a graph is the length of the shortest cycle in the graph.
As with previous chapters the (dv,dc)-regular bipartite graphs will be of 
interest here with variable nodes of degree dv and check nodes of degree dc , 
referred to as biregular. If there are n variable nodes, then the number of edges 
is |E |= ndv = mdc where there are m check nodes.
A graph G = (V, E), | V |= v, has a v x v adjacency matrix A = (aij) 
where aij = 1 if (vi,vj) e Eand zero otherwise. The set of eigenvalues ofA, 
{k0 > k 1 > • • • > kn-1}, is referred to as the spectrum of the graph. Since the 
adjacency matrix is real and symmetric, the eigenvalues are real.
There is a wealth of information on graph properties and a few are 
noted.
Theorem 11.1 ([12], theorem 1.3.14, p. 15) For a connected graph G the 
following are equivalent:
(i) G is bipartite;
(ii) if k is an eigenvalue of G, then so is -k with the same multiplicity;
(iii) if p(A) is the maximum eigenvalue of A, then -p(A) is also an 
eigenvalue.
Furthermore ([12], p. 56) if E(k) is an eigenspace of the adjacency matrix 
of a bipartite graph corresponding to the eigenvalue k, then the dimension 
of E(k) is the same as that of E(-k). Additionally a bipartite graph can be 
characterized ([14], p. 15) as a graph that contains no odd cycles (i.e., a graph 
is bipartite iff it contains no odd cycles) and any graph of diameter d1 that 
contains a cycle has girth g < 2d 1 + 1. A graph with diameter d 1 must have at 
least d1 + 1 distinct eigenvalues [17].
Let G be a d-regular graph with n vertices and spectrum and {k0 > k 1 > 
• • • > kn-1}. The following theorem summarizes some useful properties of 
such graphs:
Theorem 11.2 ([20], p. 16) Let G be a finite d -regular graph with n vertices 
with eigenvalues k0 > k1 > ••• > kn-1 . Then
https://doi.org/10.1017/9781009283403.012 Published online by Cambridge University Press

11.1 Graphs, Eigenvalues and Expansion
277
(i) k0 = d;
(ii) G is connected iff k0 > k 1;
(iii) G is bipartite iff k0 = -kn-1 .
For a (dv,dc)-regular bipartite graph [21] the maximum and minimum 
eigenvalues are ±Vdvdc• In fact [5] a nontrivial (at least one edge) graph is 
bipartite iff its spectrum is symmetric about the origin (as noted above) and a 
connected d -regular graph is bipartite iff -d is an eigenvalue.
An interesting characterization of regular graphs [12] is that a graph is 
regular iff in=-01 ki2 = nk0 (and if the graph is not connected, the multiplicity 
of k0 is the number of its components).
The girth of d-regular graphs is of interest. For a fixed integer d > 3 let 
Gi = (Vi,Ei)>1 be a family of d-regular graphs of increasing order | Vi |= vi 
with girth gi . Such a family is said to have large girth if
gi > Y logd-1 (vi)
for some constant y . It is known that y < 2 although the largest such constant 
for any family of graphs is 4/3 and the family of Ramanujan graphs, to be 
introduced shortly, asymptotically achieves this bound [18].
For coding applications, the parity-check matrix of a linear code of length 
n with m independent parity checks, H is the m x n matrix indicating the 
connection between m constraint (check) and n codeword (variable) positions. 
The related (n+m) x (n+m) adjacency matrix A of the bipartite graph, which 
includes both the left vertices of the Tanner graph (variables of the code) and 
the right vertices (constraints of the code), is of the form
A= OHT
HO
(11.1)
This is a matrix whose associated graph, in particular whose graph expan­
sion properties, will be of interest. In the case of a (dv,dc )-regular bipartite 
graph, the rows of the m x n matrix H have weight dc and columns have 
weight dv. The matrix H can be viewed as an incidence matrix of a bipartite 
graph in the sense that row i and column j contains a 1 if there is an edge 
between constraint node i and variable node j.
Note that 
A2
HT HO
OHHT
(11.2)
and if x is an eigenvector of HTH with eigenvalue k = 0, then x' = Hx is 
an eigenvector of HT with the same eigenvalue. Thus HTH and HHT have 
https://doi.org/10.1017/9781009283403.012 Published online by Cambridge University Press

278
11 Expander Codes
the same eigenvalues (possibly with different multiplicities). It is not difficult 
to show that if the eigenvalues of the matrix H in Equation 11.1 are {^i, i = 
0,1, ... ,n — 1}, then the eigenvalues of HT H are {^.?, i = 0,1,.. .,n - 1} and 
similarly the eigenvalues of A are the square roots of those of HTH.
Graph Expansion and the Second Eigenvalue
The notion of graph expansion has played an important role in many aspects of 
computer science and mathematics. Their appearance in coding theory arises 
from the construction of asymptotically good codes ([4] and, most notably, 
[29]). A standard reference for expander graphs is [20].
Let G = (V , E) be a graph with vertex set V and edge set E . As noted, 
the two classes of graphs of most interest for present purposes are the set 
of d-regular graphs and the set of (dv,dc)-regular bipartite graphs. There is 
a connection between these two classes, the edge-vertex graph, that will arise 
sufficiently often that the following definition is useful:
Definition 11.3 Let G = (V, E) be a d-regular graph with n vertices. The 
bipartite graph is formed with left set of vertices the nd/2 edges ofG and right 
set of n vertices the vertices of G, with an edge between vertices on the left 
and right if there is an edge in G joining the vertices on the right, i.e., if the 
vertex on the right is an endpoint of the edge on the left. It is a (2,d)-regular 
bipartite graph referred to as the edge-vertex graph of d-regular graph G and 
denoted Gev.
From a coding theory point of view one might expect expander bipartite 
graphs to be interesting in that having numerous variable/check edges might 
lead to numerous estimates to decode a codeword symbol/variable node of 
interest.
In the literature the formal definition of expansion tends to vary slightly. 
With minor variations the notation of [20] will be followed. For a graph G 
(connected, undirected, no multiple edges, or loops, not necessarily bipartite) 
let S, T be subsets of vertices V and E(S,T) the set of all edges between S and 
T. The edge boundary of a set S c V is defined as 
- 
d * S = E(S,S),
the set of all edges between S and S. Less restrictively, one can define
dS = U dv, dv = {u e V | (u, v) e E}.
v e S
Note that dv = Nv, the set of neighbors of v in previous chapters.
https://doi.org/10.1017/9781009283403.012 Published online by Cambridge University Press

11.1 Graphs, Eigenvalues and Expansion
279
Definition 11.4 The graph G = (V, E) is called an (a, e) expander if for all 
sets of vertices S c V of size | S |< a | V | , a < 1 /2, the set dS is of size 
at least e | S |. The parameter e is called the expansion parameter of the graph 
where
e =
min
dS
S, |S|< a|V| |S| 
a <1/2
In some works, dS is replaced with d*S in this definition. Often interest 
in expansion is restricted to d-regular graphs. For (dv,dc)-regular bipartite 
graphs the notion is the same except that only expansion of variable nodes 
is of interest and all neighbor vertices are distinct from the variable nodes - 
and the distinction disappears:
Definition 11.5 The (dv,dc)-regular bipartite graph G = (NUM,E) is called 
an (a, e) expander if for all sets of vertices S c N of size | S |< a | N | , a < 
1 /2, the set dS is of size at least e | S|. The parameter e is called the expansion 
parameter of the graph where
e = min
S, |S|<a|N| 
e <1 / 2
dS
|S|,
i.e., only expansion of variable node sets is of interest. Such a code can be 
referred to an (n,dv,dc,a,e) expander graph or code. For some applications 
the expansion parameter will be denoted as c(a).
For a given graph G, it appears to be a complex task to determine its 
expansion property in the sense that the expansion of all vertex subsets of size 
at most half the vertices should be found. In a surprising result the expansion 
of graphs is found to be related to the size of the second largest eigenvalue of 
the graph adjacency matrix.
To discuss this, let G be a d-regular graph with n vertices and spectrum 
{X0 = d > k 1 > • • • > Xn-1}. Define the parameter X* as
X *= X* (G) = max | ki | .
|Xi |=d
For obvious reasons this parameter X* = X1 is referred to as the second 
eigenvalue of G. The quantity d - X* for d-regular graphs, referred to as the 
spectral gap of the graph, can be related to the expansion of the graph, as 
discovered for finite graphs independently by Tanner [31] for (dv,dc)-regular 
bipartite graphs and Alon and Milman [3]. The result of Tanner, translated 
to our terminology is: let G be a (dv,dc)-regular bipartite graph with second 
eigenvalue X*. Then G is an (a, e) expander where ([31], theorem 2.1):
https://doi.org/10.1017/9781009283403.012 Published online by Cambridge University Press

280
11 Expander Codes
> ______ dv 2___________
[ a(dvdc — X *2) + X *2]
(11.3)
Note that in the Tanner work, X2 is the second eigenvalue of MMt , where M 
is the incidence matrix of variable to constraint vertices (corresponding to the 
matrix H in Equation 11.3 and hence X2 is the square of an eigenvalue of the 
corresponding matrix H).
For a d-regular graph:
Theorem 11.6 ([20], theorem 2.4) For a d-regular graph G the expansion 
parameter e is bounded by
d — X * 
,--------------------
—<— < e <J2d(d — X*).
This relates the spectral gap d — X* to expansion. Indeed it can be shown that 
([20], theorem 6.1) for any d-regular graph of size n and diameter A there 
exists a constant c such that
X * > V (d — 1)
Thus d-regular graphs for which the spectral gap is large (X* small) will have 
good expansion.
The edge-vertex graph of a d-regular graph plays a prominent role in 
the work of Sipser and Spielman [29], as will be discussed in the next 
section. Its expansion factor in terms of the expansion factor of the d-regular 
graph G from which (Definition 11.3) it is derived is given in the next 
section (see Proposition 11.12). As pointed out in the work of [1], this is a 
special case of the more general construction of using all paths of length k 
for the left-hand side of the bipartite graph and vertices for the right-hand 
side of G.
Let there be a (dv,dc )-regular bipartite graph with n variable vertices and 
m = ndv /dc constraint vertices with rdv,dc the rank of its adjacency matrix and 
Xmax = X0 and X* be the maximum and second eigenvalue, respectively. Then 
([19], corollary 1):
(i) Xmax = ^dvdc and
ndv — dvdc \1 /2 
rdv,dc — 1
with equality iff the eigenvalues are ±Xmax with multiplicity 1, ±X with 
multiplicity (rdv,dc — 1) and 0 with multiplicity m — rdv,dc.
X* >
https://doi.org/10.1017/9781009283403.012 Published online by Cambridge University Press

11.1 Graphs, Eigenvalues and Expansion
281
(ii)
X * >
- dvdc
-1
1/2
with equality if the eigenvalues are ±Xmax with multiplicity 1, ±X with 
multiplicity (n - 1) and 0 with multiplicity n - m.
Ramanujan Graphs
The following result will motivate the definition of Ramanujan graphs:
Theorem 11.7 ([13], theorem 1.3.1) Let Gi be a family of d-regular graphs 
such that for | Vi |= vi ^ +^ as i ^ +^, then
liminfX*{Gi} > 2Vd - 1.
i ^+to
Notice the bound asymptotically limits the size of the spectral gap. In 
addition ([13], theorem 1.3.1), if the girth of such a family tends to +^ with 
i, then the smallest eigenvalue ^ is upper bounded by limsupi^+ot ^(Gi) < 
-2Vd—1.
Ramanujan graphs are extremal in terms of this bound. While our interest 
in them for this work is limited, they appear sufficiently often in the coding 
literature, as do Cayley graphs, that they are defined here.
Definition 11.8 
(i) A Ramanujan graph is a d-regular graph satisfying, for
every nontrivial eigenvalue X,
| X |< 2Vd - 1 
(11.4)
and in particular this is true of X* .
(ii) A (dv,dc)-regular bipartite graph has maximum/minimum eigenvalues 
[19, 25] of ±Vdvdc and is defined to be Ramanujan if
X* (G) < Jdv - 1 + Vdc - 1, 
(11.5)
a natural extension of the regular graph case by setting dv = dc .
It was recently ([25], theorem 5.6) established that there exists an infinite 
sequence of (dv,dc)-regular bipartite graphs for all dv,dc > 3. It has also been 
established [25] that there exists (dv,dc)-regular Ramanujan bipartite graphs 
of every degree and number of vertices.
Thus the spectral gap of d-regular graphs is maximized when the second 
eigenvalue achieves the bound of Equation 11.4 and hence the expansion 
parameter (via spectral gap) e of the graph is maximized for Ramanujan 
https://doi.org/10.1017/9781009283403.012 Published online by Cambridge University Press

282
11 Expander Codes
graphs. From the result of Tanner in Equation 11.3 the largest spectral gap for 
(dv,dc)-regular bipartite graphs is also achieved when the second eigenvalue 
achieves the bound in Equation 11.5.
The second eigenvalue X* has become a useful criteria for determining if a 
given graph is a good expander. The second eigenvalue problem for graphs has 
typically meant the determination of graphs with “small” second eigenvalue or 
large spectral gap.
The construction of graphs that meet the above bounds has been of great 
interest in graph theory, computer science and mathematics. Until recently the 
construction of the most famous examples of such graphs involved a certain 
amount of number theory, group theory, quaternion algebra and the transfor­
mation group PSL2 (q) to construct a celebrated class of graphs labeled Xp,q . 
They are Cayley graphs (see below) and the situation is summarized as:
Theorem 11.9 ([13], theorem 4.2.2) Let p,q be distinct odd primes with 
q > 2^p. The graphs Xp,q are (p + 1 )-regular graphs which are connected 
and Ramanujan. Moreover
(i) q) = 1 (Legendre symbol), then Xp,q is a nonbipartite graph with 
q(q2 - 1)/2 vertices satisfying the girth estimate
g(Xpq) > 2 logpq.
(ii) If( P ) = -1, then Xp,q is a bipartite graph with q(q2 - 1) vertices, 
q
satisfying
g(Xp,q) > 4logpq - logp 4.
That the class of graphs constructed has second eigenvalue satisfying the 
Ramanujan condition is inherent in the construction. The graphs were first 
constructed in the work of Margulis [26] and later by Gabber and Galil [16] 
and also considered in the works [24] and [13].
Morgenstern [27] extended the above by giving a class of Ramanujan graphs 
that are (q + 1)-regular for all prime powers q, some bipartite and some not. 
Importantly and impressively the recent work of Marcus et al. [25] has shown 
the existence of bipartite Ramanujan graphs of every degree and every number 
of vertices using the notion of interlacing families of polynomials (sets of 
polynomials whose roots interlace).
The notion of a Cayley graph is sufficiently common in coding literature 
that a definition [17] may be of value:
Definition 11.10 Let C be a subset of a group G such that if g e C, then 
g-1 e G (closed under inverses) and does not contain the identity. The vertices 
https://doi.org/10.1017/9781009283403.012 Published online by Cambridge University Press

11.2 Tanner Codes
283
of the Cayley graph of G, G, are the elements of G and (g, h) is an edge in G 
iff gh-1 e C. The graph is undirected with no loops.
11.2 Tanner Codes
The notion of a Tanner code introduced in [30] will be helpful for the expander 
codes to be introduced in the next section. A brief discussion of a Tanner code 
and some of their variants, all of which will be referred to as Tanner codes, 
is given here. Although expressions for the minimum distances of these codes 
will involve the second eigenvalue, a discussion of the role of expansion in the 
definition and properties of expander codes is postponed to the next section.
A related concept to a Tanner code is that of a product code whose definition 
is recalled. A product code of two binary linear codes C1 = (n1,k1,d1)2 and 
C2 = (n2,k2,d2)2 is a collection of n 1 x n2 arrays such that each column 
is a codeword in C1 and each row is a codeword in C2. One might view the 
construction as starting with a k1 x k2 array of information bits and completing 
each row and column using the appropriate code and then computing parity 
checks on parity checks, noting that because of linearity the two possible ways 
of computing these parity-on-parity checks yield the same result. The resulting 
product code is an (n1n2,k1k2,d1d2)2 linear code.
Such codes can be decoded by first decoding rows and then decoding 
columns. It can be shown (e.g., [23]) that this technique is able to correct any 
pattern of d1d2/4 errors. With a more sophisticated algorithm it is possible to 
decode up to (d1d2 - 1)/2 errors.
From this product code interpretation, a few variations ofa Tanner code will 
be described, all of which will be termed Tanner codes, although the original 
work of Tanner [30] used only the first variation. It will be convenient to use 
notation that overlaps and use of any particular variation will be resolved by 
sufficient description.
In their most general terms each of them will involve a (dv,dc)-regular 
bipartite graph G with disjoint vertex sets V = N U M, | N |= n, | M |= m, 
with the vertices of N of degree dv and those of M of degree dc - and hence 
with edge set E, |E|= ndv = mdc. Code minimum distance will be explicitly 
stated as dmin .
Let C be a (dc,rdc,Sdc)2 linear subcode of length dc, rate r and relative 
distance S. Denote by x = (x 1 ,x2,...,xn) e Fn and associate the bit xi with 
variable node vi,i = 1,2,... ,n. For a subset S = {i 1,.. .,is} c N, |S| = s let
x | s = (x (vi 1 ),...,x (vis , 
https://doi.org/10.1017/9781009283403.012 Published online by Cambridge University Press

284
11 Expander Codes
i.e., the projection of the binary n-tuple x onto the set S. Let Nc c N set of 
degree dc variable neighbors of the constraint node c e M in the graph G. 
Consider the code
C1 = |x e Fn | x।Nc e C for all c e M^,
i.e., the dc codeword bits associated with the neighbors of each constraint node 
c e M form a codeword in the code C, for some fixed ordering of the variable 
nodes. The binary code C1 is of length n and its rate and distance are of interest. 
It is noted [30] that the properties of the code C1 will depend on the labeling 
of the variable node neighbors of the constraint node with the codeword bits, 
i.e., how the codeword bits are assigned to the dc neighbors. Each variable 
node is included in dv parity-check equations of code C and each constraint 
node contributes dc - dcr check equations and hence a total of m(dc - dcr) 
equations. To see this one might imagine constructing a parity- check matrix 
for the code C1 of length n. In the worst case, for each codeword assigned to 
the dc variable neighbors of a constraint code, one might have the dc - dcr 
check equations of a codeword of the code C of length dc . Hence a bound for 
the overall rate of the Tanner code is given by
n - mdc (1 - r)
R >
n
= 1 - (1 - r) — = 1 - (1 - r)c, 
n
as nv = mdc .
(11.6)
It will be shown [29] in the next section that if the graph G is an (a, e) expander, 
then the minimum distance of the code (for a sufficiently large e) will be at 
least an (since a simple decoding algorithm will be given that can correct an/2 
errors). This code will be denoted C1 (G,C) and is a linear (n, R > (1 - (1 - 
r)dv,dmin > an)2 code.
Variations on the above code construction technique might be considered. 
Suppose G is a d-regular graph or a (dv,dc)-regular bipartite graph and 
C = (dc,rdc,&dc)2 a binary linear code of length dc - as above. One might 
populate the edges of either of these graphs with codeword bits so that in the 
d-regular graph the bits on the edges incident with a given vertex (assuming 
some fixed edge ordering for the codeword bits) contain a codeword of C 
(for dc = d) or, in the case of the (dv,dc)-regular bipartite graph, the edges 
incident with a constraint node, contain a codeword of C of length dc . Notice 
the difference of this model with that of the former where variable nodes 
were identified with codeword bits rather than edges. Thus in these models 
the length of the code is either nd/2 for the d-regular graph or mdc for the 
(dv,dc)-regular bipartite graph. This model will be designated C2(G,C) where 
G is either the d-regular or (dv,dc)-regular bipartite. Any confusion will be 
resolved by designating the model and graph before use.
https://doi.org/10.1017/9781009283403.012 Published online by Cambridge University Press

11.2 Tanner Codes
285
More generally, one might consider a (dv,dc)-regular bipartite graph and 
two codes, C1 = (dv,r 1 dv,d 1 = 51 dv)2 and C2 = (dc,r2dc,d2 = 82dc)2 and 
assign codeword bits to all edges so that edges emanating from variable nodes 
form a codeword of C1 and those from constraint nodes a codeword of C2. This 
Tanner code is denoted C3 (G,C1,C2). Note that the encoding is not trivial in 
that it requires codeword bits on edges serve the two purposes of being a bit in 
a codeword of C1 as well as one of C2.
All of these versions have been considered in the literature.
The remainder of the section gives a sketch of work of Janwa and Lal [21, 
22] which:
(i) gives a lower bound on the expansion of the graph (n, 2,d,a,c(a)) where 
Gev is the edge-vertex graph of a d-regular graph G with n vertices and 
second eigenvalue X *;
(ii) establishes a lower bound on the minimum distance of the code 
C2(Gev,C) where G is a (dv,dc)-regular bipartite graph [21];
(iii) establishes a lower bound on the minimum distance of the code 
C3(G,C1,C2) where G is a (dv,dc)-regular bipartite graph [21].
The techniques used in both cases are of interest although the proofs are not 
considered here.
Expansion Properties of Gev from the Properties of G
Let G be a d-regular graph with n vertices and second eigenvalue X^, and let 
M be its edge-vertex incidence matrix (of dimension e x n). The edge-vertex 
graph Gev of G then has adjacency matrix
AGev
OM
Mt O
The n x n adjacency matrix of G is seen to satisfy
AG + dI = MtM
and note that
AGevAtGev
MMt 
O
O MtM
The eigenvalues of MMt are the same (possibly different multiplicities) as 
those of MtM and the eigenvalues of AGev are the square roots of those of 
AG + dI. It follows easily then that
https://doi.org/10.1017/9781009283403.012 Published online by Cambridge University Press
X 0, G ev = ^2d and X G ev = dd + X g.

286
11 Expander Codes
These can be applied to the expansion result of Tanner (see Equation 11.3) 
which is recalled, for a (dv,dc)-regular bipartite graph as:
e = c(a) >
dv2
a[dvdc - X*2) + XG2
which for the case in hand (for the (2,d)-regular bipartite graph Gev) gives:
Proposition 11.11 ([22], proposition 2.1) Let G be a d-regular graph with 
n vertices and second eigenvalue X&. Then its edge-vertex graph Gev is an 
(nd/2, 2,d,a,c(a)) expander graph with maximum eigenvalue and second 
eigenvalue as in the above equations. The graph expansion factor is given by
c(a) >—-------- -±-----------.
a(d - X *) + (d + X *)
GG
(11.7)
Better Expansion with Alon and Chung Edge Estimate
The expansion coefficient c(a) above can be improved upon using the 
celebrated result of Alon and Chung [2] on the number of edges in an induced 
subgraph. Since the Alon and Chung result makes frequent appearances in 
research on expander codes, a sketch of the expansion coefficient improvement 
is given.
Let G be a d-regular graph with n vertices, second eigenvalue X*, S a subset 
of | S|= Yn vertices and e(S) the number of edges in the subgraph induced by 
S. Then
e(S) - 1 dY2n < 1X*y( 1 - Y)n.
(11.8)
It is noted that 1 dY2n is approximately the expected number of edges one 
would expect from a subgraph of an vertices of a d-regular graph. This 
interesting result [22] can be used to improve the expansion coefficient c(a) to:
Proposition 11.12 ([22], theorem 2.2) Let G be a d -regular graph with n 
vertices and second eigenvalue X*G. Then the edge-vertex graph Gev is an 
(nd/2, 2,d,a,c(a)) expander graph with graph expansion factor of
4
c(a) > X* + V(X*)2 + 4ad(d - X*)' 
(1L9)
It is straightforward to show that this result is indeed an improvement over 
that of the previous proposition, i.e.,
c(a) >
4 
4
X* +V(X*)2 + 4ad(d - X*) > a(d - X*) + d + X*
https://doi.org/10.1017/9781009283403.012 Published online by Cambridge University Press

11.2 Tanner Codes
287
It is clear that in some cases the size of the vertex expansion set size an and 
expansion coefficient c(a) can be traded off. In the above theorem concerning 
the expander graph Gev edge-vertex graph of G,ad -regular graph G on 
n vertices with second eigenvalue X*, Gev an (nd/2,2,d,a,c(a)) expander 
graph, if one chooses for the code C = (Gev,C), a code C = (d,rd,8d)2 
and
, 
8(d8 - X*)
a = -------------
(d-X*)
then
4 
c(a) > -------- , 
.
X * + (X (X *)2 + 4 da'(d — X *)
4
> --------
.
* 
* 2 
8(da-X*) ( J 
1*\
X + V (X ) + 4d (d -X*) (d X )
> 
4
> 
X* +-j(X* — 2d8)2
2
> 
d88'
This computation results in an expander graph with parameters
nd/2, 2,d
8 d8 — X*) 2
(d — x*) , d8
(11.10)
This result will be of interest in the next section where, as for a previous code, 
it will be shown that in fact the corresponding code to this expander graph has 
8 d8 — X*)
a minimum distance of —----------- and rate (Equation 17.2 with c = 2) of
(d — X*)
2r — 1. This is in fact the code used in [29] which contained the minimum 
distance bound 
' (d8 — X *) I2
_ (d — X *) _
referred to here as C2 (Gev,C) for C a (d, rd, 8d)2 code, and this bound can be 
shown to be inferior to the previous bound above.
For the item (iii) mentioned above, it is of interest to determine the 
minimum distance of the code C3 = C(G,C1,C2) following the interesting 
argument of [21], where G is a (dv,dc )-regular bipartite graph with vertices 
N U M, | N |= n, | M |= m and C1 = (dv,r 1 dv,d 1 = 81 dv)2 and 
C2 = (dc,r2dc,d2 = 82dc)2 two binary linear codes. Again assuming a fixed 
edge assignment to codeword bit positions, label the graph edges incident with 
variable nodes with codewords of C1 and those incident with constraint nodes 
https://doi.org/10.1017/9781009283403.012 Published online by Cambridge University Press

288
11 Expander Codes
with codewords of C2 (a single codeword bit per edge). Clearly C3 is a binary 
linear code of length N = ndv (number of edges in the bipartite graph). The 
rate of the code can be bounded using the previous argument, i.e., each of 
the n variable nodes would add at most dv - dvr1 equations to the parity­
check matrix of C and each of the constraint nodes dc - dcr2 for a total of 
n(dv - dv r1) + m(dc - dcr2). The code rate R is then bounded as
R > N - (n(dv - dvr 1) + m(dc - dcr2))"|/N = r 1 + r2 - 1.
The minimum distance, as above, is derived via estimates of edge counts ([21], 
theorem 3.1) in the following manner.
As an extension of this edge count notion of Alon and Chung used for 
the previous case, for the codes of interest here, an estimate of the quantity 
e(S, T), the number of edges from a set S c N to T c M in a (dv,dc)-regular 
bipartite graph with disjoint vertex sets N and M is required. Such edge density 
estimates were also considered in [33] for the case of balanced bipartite graphs 
dv = dc (i.e., regular bipartite). The improvement of those estimates of [22] is 
used. Assume the (dv,dc)-regular bipartite graph G (i.e., its adjacency matrix) 
has second eigenvalue X*. Then it is shown ([22], equation 1) that
\e(S,T) — dc | Sn1 1 T | | < ^(| S | + | T | 
< X2*( |S| + |T| ).
| S|2
| T |2 
m
(11.11)
n
As noted, this is a generalization of the edge count given in [33] for the d- 
regular bipartite graph (all nodes, variable and constraint, of degree d). This 
edge count estimate is used in the proof of the following:
Theorem 11.13 ([21], theorem 3.1) LetG be a (dv,dc)-regular bipartite graph 
with n variable nodes and second eigenvalue X* and let C = C(G,C 1 ,C2) be 
the code described above, assuming d 1 > d2 > X* /2. Then C is an (N = 
ndv,R > r1 + r2 — 1,dmin)2 where
n 
dmin > -;d
X*
d 1d2 2 (d 1 + d2 )
To decode these codes [21] one can first decode the codewords in C1 
associated with the variable nodes - since they are independent this can be 
done in parallel. Then the codewords ofC2 associated with the constraint nodes 
can be decoded in parallel, and so on, iterating until there is no change. It can be 
shown ([21], theorem 5.2) this algorithm will converge to the correct codeword 
https://doi.org/10.1017/9781009283403.012 Published online by Cambridge University Press

11.2 Tanner Codes
289
if the weight of the error word is less than
5 1 5 2 
X *
a 2 (.2 - TJd-v
In some works regular bipartite graphs are of interest, i.e., both sets of 
vertices have the same degree (hence the same number of vertices on each 
side of the graph). Consider [7, 33] a dv-regular bipartite graph G with vertex 
sets N,M such that | N |=| M |= n, all vertices of degree dv. The graph is dv- 
regular. The graph has nc edges and the code will consist of binary codewords 
of length N = nc derived from bits assigned to the graph edges. Consider a 
subcode C a linear (dv,rdv,d1 = 51dv)2. The edges from each node of the 
graph, both left and right sets of nodes, will be assigned a codeword from C, 
for some fixed ordering of the edges for each node to allow the codeword bits 
of C to be assigned to the edges, as in the code C3 (G,C,C). Thus the set of 
edges incident with each node, variable and check, contains a codeword of C 
(again according to some fixed assignments of codeword bits to edges).
As previously, a simple decoding algorithm for this code [33] is to decode 
each of the subcode words on the edges emanating from the N variable nodes. 
Since the corresponding codewords are disjoint this can be done in parallel. 
Then decode the codewords on edges from all constraint node codewords, 
again in parallel. Continuing in this fashion it is shown in ([33], theorem 6) 
that, for X* the second eigenvalue of the adjacency matrix of G and d 1 > 3X*, 
as long as the error vector (of length ndv) e is such that 
X *
c
51 51 
w(e) < a — I —
ndv
for some a< 1, the above algorithm will converge to the transmitted 
codeword, implying the minimum distance of this Tanner code is at least twice 
this amount. Furthermore the decoding algorithm takes a number of steps that 
is logarithmic in the number of edges of the graph ndv .
It is noted [33] that the above decoding algorithm reduces to that of the 
product codes mentioned if dv = m and G is the complete bipartite graph.
It is interesting to observe [29] the following decoding was proposed for the 
code (G,C) where the codeword associated with the n variable nodes is such 
that the set of neighbors of each constraint node supports a codeword of the 
subcode C. Consider a constraint node c and the set of its dc variable neighbors 
forming a word - a dc-tuple. If this word differs from a codeword of C in at 
most 5dc /4 places, then send a “flip” message to each of those variables that 
differ. Then in parallel flip all variables that received at least one flip message. 
It is shown [29] that for this algorithm to decode successfully it is necessary 
https://doi.org/10.1017/9781009283403.012 Published online by Cambridge University Press

290
11 Expander Codes
that the fraction of variables in error a must be such that
a < 52(1 /3 - 4X*/5dc)/16 < 52/48.
Techniques introduced in [33] showed how this fraction can be reduced to 
52/4, a factor of 12 improvement.
11.3 Expander Graphs and Their Codes
The notion of graph expansion was introduced in the first section and aspects of 
it in terms of the second eigenvalue of a graph were considered in the previous 
section. The explicit use of graph expansion to determine codes that have very 
efficient decoding algorithms is considered here.
The seminal work of [29] is first considered. As before, let G be a (dv,dc )- 
regular bipartite graph with n variable and m = ndv /dc check nodes and ndv 
edges. Let C = (dc,rdc,5dc)2, r>(dv - 1)/dv, be a binary linear code. Let 
the variable neighbors of check node ci be Nci ={vi1,...,vidc } labeled in an 
appropriate manner. The expander code C = C(G,C) (previously C1) then is a 
binary code of length n with codeword x = (x1,x2,...,xn) such that for each 
check node ci,i = 1,2,...m, x|Nc , the projection of x onto the neighbors of 
ci , is a codeword in C .
The following ([29], theorem 7) gives the crucial property of graph 
expansion to code properties:
Theorem 11.14 Let G be an (n, dv,dc,a,dv/dc5) expander (dv,dc)-regular 
bipartite graph and C = (dc,r > (dv - 1)/dv,dmin > 5dc)2 binary linear 
code. Then the binary linear code C = (G,C) has rate >dvr - (dv - 1) and 
minimum relative distance at least a.
An informal argument [29] is used to illustrate the proof. To see the 
minimum distance of the code is at least an, suppose the variables support 
a codeword (of C) of weight at most an and let X, | X |= x be the 
nonzero variable positions, i.e., supporting 1s. There are dvx edges leaving 
these variable positions and, by the expansion properties of the graph, these 
edges lead to at least (dv/dc5')x constraint nodes. The average number of 
edges on these constraint nodes is therefore at most dvx (dv/dc5)x = dc5 . 
There must be at least one constraint node c* with fewer than dc 5 edges 
leaving it (in the original graph the other edges leaving c* are attached 
to variable positions with values of 0). By definition the edges leaving c* 
support a codeword of C of weight at least dc5 and the variable nodes of it 
supporting 1s are in X. But there are fewer than dc 5 1s in it so it cannot be 
https://doi.org/10.1017/9781009283403.012 Published online by Cambridge University Press

11.3 Expander Graphs and Their Codes
291
a codeword of C and hence the supposed codeword cannot be of weight less 
than an.
Consider the following simple decoding algorithm for the code C (G,C) 
above. Assume the graph has a received codeword with at most e = an/2 
errors in it. A constraint node is said to satisfied if it has a value 0, i.e., the mod 
2 sum of attached variable values is 0. By “flipping” a value (either variable or 
constraint) is meant changing 1 to 0 and 0 to 1.
Algorithm 11.15
(i) If there is a variable node with more unsatisfied constraints than satisfied, 
flip that variable value and all attached constraint values.
(ii) Continue until no such variable node exists.
It seems remarkable that it is possible to show this simple algorithm will 
correct up to an/2 errors in a received word ([28], [29], theorem 10) and 
hence the code has minimum distance at least an, as has been referenced 
previously. For the argument consider the code C (G,C) for a subcode C of 
all even-weight codewords and (c, d)-regular bipartite expander graph G = 
(n,dv,dc,a, 3dv/4). However, such a code is equivalent to a code where the 
constraint vertices are simple parity checks since in that case if the variable 
neighbors supported a codeword of even weight its mod 2 sum would be zero. 
Equivalently the argument applies to any code where the expander (dv,dc)- 
regular bipartite graph corresponds to an m x n parity-check matrix where the 
m rows correspond to the right-hand vertices (constraint or parity checks) and 
the n columns the variable vertices.
Refer to the variable positions supporting errors as “corrupt positions.” 
Assume initially there are e < an/2 corrupt variable positions and that by 
definition of the algorithm e is strictly decreasing as the algorithm progresses. 
It is required to show that there will exist variable positions attached to 
more unsatisfied constraints than satisfied ones until there are no unsatisfied 
constraint positions, i.e., all constraint positions have values of 0 and the 
corresponding variable positions form a codeword. At any iteration of the 
algorithm let u be the number of unsatisfied constraint positions and s the 
number of satisfied constraint positions attached to at least one corrupt variable 
position (i.e., the constraint position is attached to an even number of corrupt 
variable positions). Since the expansion factor of the graph is 3dv /4, by the 
expansion properties of the graph
u + s > (3dv/4)e.
https://doi.org/10.1017/9781009283403.012 Published online by Cambridge University Press

292
11 Expander Codes
Since each satisfied constraint position attached to a corrupt position is 
attached to at least two of them it follows that
u + 25 < dve
and these two equations imply that
u > dve/2.
This means that more than half of the check neighbors of a corrupt variable 
position connect to an unsatisfied check position. Thus there must be at least 
one corrupt variable attached to more unsatisfied constraint variables than 
satisfied. However, it could be there is an uncorrupted variable position also 
attached to more unsatisfied than satisfied check positions. If this position 
is chosen to be flipped, it will increase the number of corrupted positions 
e - making matters worse. Regardless, it has been shown that under the 
conditions noted there will be a variable position with more unsatisfied 
constraints than satisfied (whether it is corrupted or not). It could be that as 
e increases it reaches a point where e = an and the corrupt positions form a 
codeword. We show this cannot occur. Suppose it does and that e = an. Recall 
that u is strictly decreasing. If e = an, then by the formula above
u > dvan/2
but this cannot occur as the algorithm started with u = an/2 and is always less 
than this quantity. Thus the flipping algorithm is capable of correcting at least 
an/2 errors and hence the code distance is at least an.
What has been shown is that graph expansion can be used in the formulation 
of good codes and efficient and simple decoding algorithms. In particular if 
the coding graph is an (n,dv,dc,a,e) expander, then as long as the expansion 
coefficient e is sufficient, the relative distance of the resulting code C is a. 
This is the fundamental relationship established in [29] on which much work 
on expander codes is based.
Consider the code C (Gev,C) where G is a d-regular graph with n ver­
tices, with second eigenvalue X*, Gev its edge-vertex graph and C a binary 
linear code of length d, rate r and minimum relative distance S and G an 
(n,dv,dc,a,e = dv(a)) expander graph. As shown in the previous section if a 
is chosen as
S(dcS - X*)
(dc — X *) ,
https://doi.org/10.1017/9781009283403.012 Published online by Cambridge University Press

11.3 Expander Graphs and Their Codes
293
then this is a lower bound on the minimum distance of the code and the 
expansion factor is
2
c(a) = T7 
dCS
Using the Ramanujan graphs of Theorem 11.9 and good codes which are 
known to exist (by Varshamov-Gilbert bound) the following theorem can be 
shown ([29], theorem 19, improvement by [33], theorem 1A):
Theorem 11.16 For all S0 such that 1 - 2H(S0) > 0,H(•) the binary entropy 
function, there exists a polynomial-time constructible family of expander codes 
of length n and rate 1-2H (S0) and minimum relative distance arbitrarily close 
to S2 in which any a < S2 /4 fraction of errors can be corrected with a circuit 
of size O(n log n) and depth O(log n).
It is mentioned that [4] also uses the notion of expander graphs to construct 
asymptotically good codes (including nonbinary) but relies on the pseudo­
random properties such graphs tend to have for the results.
The work of Sipser and Spielman [29] established the importance of 
the notion of graph expansion in the construction of codes and decoding 
algorithms of linear complexity. It has generated a very large literature of 
significant results. Due to space only a few of these research directions and 
results will be noted.
Note that for a (dv,dc)-regular bipartite graph the largest expansion possible 
would be c and this would be realized only if the corresponding constraint 
vertices for a given set of variable vertices would be distinct. Thus many works 
refer to (a, (1 - e)dv) expanders with e corresponding to the backoff from 
the maximum possible expansion. It is crucial to observe many of the results 
achieve excellent distance properties and linear decoding complexity even for 
codes over small fields.
While the following result from [15] refers to a particular type of decoding, 
linear programming or LP decoding, not discussed here, it is the expan- 
sion/distance properties of the codes that are of interest.
Theorem 11.17 ([15], theorem 1) Let C be an LDPC code with length n and 
rate at least 1 - m/n described by a Tanner graph G with n variable nodes, 
m check nodes and regular left degree dv. Suppose G is an (a, Sdv) expander, 
where S>2/3 + 1/(3dv) and Sdv is an integer. Then the LP decoder succeeds, 
as long as at most 2S-2 (an - 1) bits are flipped by the channel.
Similarly from [32]:
https://doi.org/10.1017/9781009283403.012 Published online by Cambridge University Press

294
11 Expander Codes
Theorem 11.18 ([32], theorem 3.1) Let dv,dc,e,b be constants and let C C F21 
be a (dv,dc,e,&) code. Assume that e > 1 /2 and edv + h - dv > 0 where h = 
I- (2 e - 1 )c "I. Then C is decodable in linear time from ((edv + h - dv)/h) |_ 8n J 
errors. Moreover C is decodable in logarithmic time on a linear number of 
processors from ((edv + h - dv)/h) |_ 8n J errors.
Theorem 1.1 of [11] shows that the best minimum distance one might expect 
of such codes derived from a (c,d)-regular expander code is about an/2e in 
the sense that:
Theorem 11.19 ([11], theorem 1.1) Given any (dv,dc)-regular bipartite graph 
with expansion parameters (a, (1 - e) dv) and corresponding code C, then the 
minimum distance of the code is at least 2^n - Oe( 1) and for any constant 
n > 0 there exists an (a,( 1 - e)dv) expander graph which has distance at 
most (27 + n) and whose corresponding code has minimum distance at most 
(27 + ^n.
Actually the work requires only that the left vertices are dv -regular and 
that the right vertices are parity checks on their respective variable nodes. 
Furthermore under certain conditions these codes will have a linear-time 
decoding algorithm:
Theorem 11.20 ([11], theorem 1.2) Given any constants a, n > 0 and 0 < 
e < 1 /4 there exists a linear-time decoding algorithm that will decode up 
to (167 - ^n adversarial errors for any expander code associated with an 
(a, (1 - e)dv) expander graph.
Since the minimum distance of the code is an/16e, the linear complexity 
algorithm cannot correct all errors the code is capable of.
The following decoding algorithm [11, 32] is of interest. Let C be the code 
associated with the expander regular bipartite graph. For c e C the transmitted 
codeword let j e F^ be the received word with errors at locations F C N and 
number of errors |F |. Recall the constraint neighbors of variable node vi e N 
is denoted Nvi. Let A be a real number, a threshold parameter. L will be a 
subset of N and R a subset of M (the left and right vertices).
Algorithm 11.21 Input: j,A
1. L ^ f
2. R ^ unsatisfied parity checks of j
3. L ^ f
4. h ^ (1 - 2A)dv (a threshold)
5. while 3 vi e N\L st |Nvi A R | > h do
6. L ^ L U Vi
https://doi.org/10.1017/9781009283403.012 Published online by Cambridge University Press

11.3 Expander Graphs and Their Codes
295
7. R ^ R U Nvi
8. end while
9. return L
10. end
Note that in Step 5 if the algorithm finds a variable node attached to more 
than h unsatisfied parity checks, that node is added to the set of suspicious 
(possibly corrupted) variable nodes.
Under certain conditions it can be shown the set F of all error positions in 
the received codeword is contained in the set L after the algorithm completes. 
One can then treat the set F as erasures which can be corrected up to 
the minimum distance of the code (twice the number of correctable errors). 
Numerous variations/improvements to this basic algorithm are discussed in 
[11]. It is also noted there that from an (a,( 1 - e) dv) expander one can trade­
off parameters to give a (ka, (1 - ke) dv) expander as long as 1 > ke, a useful 
tool for some constructions.
Numerous other aspects of the properties of expander codes have been 
considered in the literature, including their error probability and error exponent 
curves [6, 7, 8, 9] where it is shown, among numerous other properties, 
they achieve capacity on the BSC with a linear decoding algorithm and error 
probability that decreases exponentially with code length n. Burshtein [10] 
shows that expander-type arguments can be used to show that the message­
passing algorithms (as considered, e.g., in Chapter 3) can actually be improved 
with the use of expander-type arguments to achieve the correction of a linear 
number of errors in a codeword.
Comments
The simple notion of graph expansion, well established as a topic of interest 
in the combinatorial, graph theoretic and computer science communities, 
has found a very interesting application in coding theory. The achievement 
of codes with excellent distance properties and simple linear-time decoding 
algorithms, using this notion, is surprising. The notion of expansion, briefly 
introduced in this chapter, will continue to be a topic of great interest in coding 
theory.
https://doi.org/10.1017/9781009283403.012 Published online by Cambridge University Press

296
11 Expander Codes
References
[1] Ajtai, M., Komlos, J., and Szemeredi, E. 1987. Deterministic simulation in 
LOGSPACE. Pages 132-140 of: Proceedings of the Nineteenth Annual ACM 
Symposium on Theory of Computing. STOC ’87. ACM, New York.
[2] Alon, N., and Chung, F.R.K. 2006. Explicit construction of linear sized tolerant 
networks. Discrete Math., 306, 1068-1071.
[3] Alon, N., and Milman, V.D. 1984 (October). Eigenvalues, expanders and super­
concentrators. Pages 320-322 of: 25th Annual Symposium on Foundations of 
Computer Science, 1984.
[4] Alon, N., Bruck, J., Naor, J., Naor, M., and Roth, R.M. 1992. Construction 
of asymptotically good low-rate error-correcting codes through pseudo-random 
graphs. IEEE Trans. Inform. Theory, 38(2), 509-516.
[5] Asratian, A.S., Denley, T.M.J., and Haggkvist, R. 1998. Bipartite graphs and their 
applications. Cambridge Tracts in Mathematics, vol. 131. Cambridge University 
Press, Cambridge.
[6] Barg, A., and Mazumdar, A. 2011. On the number of errors correctable with codes 
on graphs. IEEE Trans. Inform. Theory, 57(2), 910-919.
[7] Barg, A., and Zemor, G. 2002. Error exponents of expander codes. IEEE Trans. 
Inform. Theory, 48(6), 1725-1729.
[8] Barg, A., and Zemor, G. 2004. Error exponents of expander codes under linear- 
complexity decoding. SIAM J. Discrete Math., 17(3), 426-445.
[9] Barg, A., and Zemor, G. 2006. Distance properties of expander codes. IEEE Trans. 
Inform. Theory, 52(1), 78-90.
[10] Burshtein, D., and Miller, G. 2001. Expander graph arguments for message­
passing algorithms. IEEE Trans. Inform. Theory, 47(2), 782-790.
[11] Xue, C., Cheng, K., Li, X., and Ouyang, M. 2021. Improved decoding of expander 
codes. CoRR, abs/2111.07629.
[12] Cvetkovic, D., Rowlinson, P., and Simic, S. 1997. Eigenspaces of graphs. 
Encyclopedia of Mathematics and Its Applications, vol. 66. Cambridge University 
Press, Cambridge.
[13] Davidoff, G., Sarnak, P., and Valette, A. 2003. Elementary number theory, group 
theory, and Ramanujan graphs. London Mathematical Society Student Texts, vol. 
55. Cambridge University Press, Cambridge.
[14] Diestel, R. 2018. Graph theory, 5th ed. Graduate Texts in Mathematics, vol. 173. 
Springer, Berlin.
[15] Feldman, J., Malkin, T., Servedio, R.A., Stein, C., and Wainwright, M.J. 2007. 
LP decoding corrects a constant fraction of errors. IEEE Trans. Inform. Theory, 
53(1), 82-89.
[16] Gabber, O., and Galil, Z. 1981. Explicit constructions of linear-sized supercon­
centrators. J. Comput. System Sci., 22(3), 407-420.
[17] Godsil, C., and Royle, G. 2001. Algebraic graph theory. Graduate Texts in 
Mathematics, vol. 207. Springer-Verlag, New York.
[18] Halford, T.R., and Chugg, K.M. 2006. An algorithm for counting short cycles in 
bipartite graphs. IEEE Trans. Inform. Theory, 52(1), 287-292.
[19] H0holdt, T., and Janwa, H. 2012. Eigenvalues and expansion of bipartite graphs. 
Des. Codes Cryptogr., 65(3), 259-273.
https://doi.org/10.1017/9781009283403.012 Published online by Cambridge University Press

References
297
[20] Hoory, S., Linial, N., and Wigderson, A. 2006. Expander graphs and their 
applications. Bull. Amer. Math. Soc. (N.S.), 43(4), 439-561.
[21] Janwa, H., and Lal, A.K. 2003. On Tanner codes: minimum distance and 
decoding. Appl. Algebra Engrg. Comm. Comput., 13(5), 335-347.
[22] Janwa, H., and Lal, A.K. 2004. On expanders graphs: parameters and applications. 
CoRR, 
.
cs.IT/0406048
[23] Justesen, J., and H0holdt, T. 2004. A course in error-correcting codes. EMS 
Textbooks in Mathematics. European Mathematical Society (EMS), Zurich.
[24] Lubotzky, A., Phillips, R., and Sarnak, P. 1988. Ramanujan graphs. Combinator- 
ica, 8(3), 261-277.
[25] Marcus, A.W., Spielman, D.A., and Srivastava, N. 2015. Interlacing families 
IV: bipartite Ramanujan graphs of all sizes. Pages 1358-1377 of: 2015 IEEE 
56th Annual Symposium on Foundations of Computer Science—FOCS ’15. IEEE 
Computer Society, Los Alamitos, CA.
[26] Margulis, G.A. 1988. Explicit group-theoretic constructions of combinatorial 
schemes and their applications in the construction of expanders and concentrators. 
Problemy Peredachi Informatsii, 24(1), 51-60.
[27] Morgenstern, M. 1994. Existence and explicit constructions of q + 1 regular 
Ramanujan graphs for every prime power q . J. Combin. Theory Ser. B, 62(1), 
44-62.
[28] Richardson, T., and Urbanke, R. 2008. Modern coding theory. Cambridge Uni­
versity Press, Cambridge.
[29] Sipser, M., and Spielman, D.A. 1996. Expander codes. IEEE Trans. Inform. 
Theory, 42(6, part 1), 1710-1722.
[30] Tanner, R.M. 1981. A recursive approach to low complexity codes. IEEE Trans. 
Inform. Theory, 27(5), 533-547.
[31] Tanner, R.M. 1984. Explicit concentrators from generalized N -gons. SIAM 
J. Algebra. Discrete Meth., 5(3), 287-293.
[32] Viderman, M. 2013. Linear-time decoding of regular expander codes. ACM Trans. 
Comput. Theory, 5(3), 10:1-10:25.
[33] Zemor, G. 2001. On expander codes. IEEE Trans. Inform. Theory, 47(2), 
835-837.
https://doi.org/10.1017/9781009283403.012 Published online by Cambridge University Press

12
Rank-Metric and Subspace Codes
Algebraic coding theory usually takes place in the vector space of n-tuples 
over a finite field with the Hamming metric where the distance between two 
n-tuples is the number of coordinate positions where they differ. In essence 
one tries to find subsets of the space (codes) with the property that any two 
elements of the subset are at least some minimum distance apart. This can be 
viewed as finding a packing of spheres of a given radius in the Hamming space.
Similar notions can be found in many other structures with different 
metrics - a familiar one is Euclidean n-space with the usual Euclidean dis­
tance - a structure where the sphere packing problem has been of considerable 
mathematical interest as well as practical interest for the construction of codes 
for the Gaussian channel. Another is the real line with absolute difference 
between numbers as a metric.
This chapter considers two other spaces with metrics that are perhaps less 
familiar. The first is the case of rank-metric codes as subsets of matrices of 
a given shape (not necessarily square) over a finite field with norm/distance 
defined by matrix rank rather than Hamming distance. They have been of 
independent interest in coding theory and of interest for their relationship to 
space-time codes. The second is the case of subspace codes as subsets of the 
set of all subspaces of a vector space over a finite field with a certain metric. 
They arise naturally in the subject of network codes for file transmission over a 
network. This subject might well have been included in the chapter on network 
codes (Chapter 5) but is included here because of its relation to rank-metric 
codes. The structure and properties of both rank-metric codes and subspace 
codes has generated an enormous amount of research using sophisticated 
mathematical techniques. Our treatment of these interesting topics is limited 
to an introduction of the basic ideas.
The first instance of rank-metric codes appeared in the work of Delsarte [5] 
in the language of bilinear forms and association schemes. The explicit notion 
298
https://doi.org/10.1017/9781009283403.013 Published online by Cambridge University Press

12.1 Basic Properties of Rank-Metric Codes
299
of a rank-metric code as such, first appeared in the work of Gabidulin [16], 
work which has had a profound impact on subsequent research in the area.
Rank-metric codes are defined and some of their basic properties estab­
lished in the next section. Section 12.2 describes several important construc­
tions of rank-metric codes. The final Section 12.3 introduces the notion of 
subspace codes and considers their relation to rank-metric codes.
The amount of work on the rank-metric codes in the past decade is extensive 
with connections to network coding and to several areas in combinatorics and 
other areas of discrete mathematics. The seminal contributions of [5, 16, 27] 
have led to enormous interest in the coding community and the subject 
continues to be an active area of research.
Two metrics are used in this chapter, matrix rank and subspace dimension. 
The notation d for the distance metric will be for both since it will be clear 
from the context whether matrices or subspaces are of interest.
12.1 Basic Properties of Rank-Metric Codes
Let Fq and Fqn be the finite fields with q and qn elements, respectively, and 
Vn(q) the vector space of n-tuples over Fq which is also denoted as Fqn and 
there is a natural association of Vn (q) with Fqn. Similarly denote by Mkxm (q) 
the space of k x m matrices over Fq viewed as a vector space of shape k x m 
over Fq , as the term dimension will have another meaning. Denote the all-zero 
vector in Vn(q) as 0n and the all-one vector as 1n . The usual inner product on 
Vn(q) is denoted
n
(x,y) 22xiyi e fq, x,y e vn(q).
Recall that a norm N on Vn(q) satisfies the properties
(i) N(x) > 0 Vx e Vn(q);
(ii) N(x) = 0 <> x = 0n e Vn(q);
(iii) N(x + y) < N(x) + N(y') Vx,y e Vn(q) .
Similarly a distance function d(x,y) on Vn(q) satisfies the axioms
(i) d(x,y) > 0 Vx,y e Vn(q);
(ii) d(x,y) = 0 ^^ x = y;
(iii) d(x,y) < d(x,z) + d(z,y) Vx,y,z e Vn(q) (triangle inequality).
The norm function can be used to define a distance function on the space.
https://doi.org/10.1017/9781009283403.013 Published online by Cambridge University Press

300
12 Rank-Metric and Subspace Codes
The space Mkxm (q) is endowed with an inner product, the trace product as 
follows.Let A, B e Mk x m(q) and denote their i-th columns as Ai,Bi e F q,i = 
1,2,...,m, respectively. An inner product on Mkxm(q) is defined as
{A,B) = Tr(AB), A,B e Mkxm(q), 
(12.1)
where Tr indicates the matrix trace, the sum of the diagonal elements. It follows 
from the above that
m 
m k
<A, B ) = £ (Ai,Bi) = EE aijbij 
(12.2)
where (x, y) is the usual inner product of vectors. It is easy to verify that 
this matrix inner product is an inner product, i.e., it is bilinear, symmetric 
and nondegenerate on the space Mkxm(q). For one-dimensional matrices the 
definition is the usual one for vectors.
Denote by rank(A) the rank of a matrix A in Mkxm(q), assumed to be 
the rank over Fq unless otherwise specified. Recall that the rank is also the 
dimension of the row space generated by the rows of the matrix A which is the 
same as the dimension of the column space generated by the columns of A. 
Since
| rank (A) - rank (B) | < rank (A + B) < rank (A) + rank (B)
the rank satisfies the axioms for a norm function and in particular gives a valid 
distance function on Mkxm(q), i.e., that
d(A,B) = rank(A -B), A,B e Mkxm(q) 
(12.3)
satisfies the axioms of a distance function follows readily from the arguments 
used to prove the previous inequality. The following informal argument is 
given. Let C(X) and R(X) denote the column and row space, respectively of 
the matrix X e Mkxm (q). Note that for two matrices A and B of the same 
shape C(A + B) c C(A) u C(B) and let c = dim(C(A) n C(B)) and hence
rank (A + B) < dim (C(A)) + dim (C(B)) — dim (C(A) n C(B)) = rank (A) + rank (B) — c
and similarly for R(A) and R(B) and r = dim(R(A) n R(B). It follows that
rank(A +B) < rank(A) + rank(B) —max(c,r) or rank(A —B) < rank(A) + rank(B) 
and these relations can be used to show
d(A,C) = rank(A — C) < d(A,B) + d(B,C) = rank(A —B) + rank(B — C)
(noting that rank(—B) = rank(B)).
https://doi.org/10.1017/9781009283403.013 Published online by Cambridge University Press

12.1 Basic Properties of Rank-Metric Codes
301
Definition 12.1 A rank-metric code C of shape k x m will be a subset C 
of the set Mkxm(q) of k x m matrices over Fq. The minimum distance of the 
code is
min 
min
dmin = A,B £ C,A = B d(A B) = A,B £ C,A = b rank (A B) ■
The number of matrices in the code will be called its size. In the case of a linear 
rank-metric code, the set C is a linear space over Fq and the minimum distance 
is the rank of the minimum rank nonzero matrix in the code subspace. If the 
size of such a linear code is qK , it will be designated as a (k x m,K, ddim)q 
rank-metric code, 1 < K < km, | C |= qK and K will be the dimension of 
the code. For nonlinear codes the actual size (number of codewords M) will be 
used and referred to as a (k x m, M, dmin)q code.
The definition attempts to reflect that in use for ordinary algebraic codes. 
The notation used here is not standard in the literature. It is emphasized the 
term “dimension K” is restricted to give the dimension ofa linear rank-metric 
code as qK, i.e., the set of matrices forms a vector space over Fq of size qK 
and dimension K. In general nonlinear sets of matrices are also of interest.
Definition 12.2 In the case a rank-metric code contains only matrices of the 
same rank, it is referred to as a constant-rank code (CRC).
Definition 12.3 The rank distribution of a rank-metric code C is the set of 
nonnegative integers |Ai (C)| where
Ai(C) = |{ A e C | rank (A) = i) |, i = 0,1,..., min (k,m)
and if C is linear, the minimum distance d of the code is the smallest i>0 
such that Ai (C) = 0.
In analogy with ordinary algebraic codes, one can define a distance 
distribution [21] of rank-metric codes. In the case of linear codes the two 
concepts of rank and distance distribution are the same.
The reference [21] is an excellent collection of chapters on network codes 
and subspace designs by researchers. Various notions of the equivalence of 
rank-metric codes have arisen ([21], chapter by Otal and Ozbudak) and [3]:
Definition 12.4 Rank-metric codes C, C' c Mkxm (q) are equivalent if there 
exist X e GLk (q), Y e GLm(q) (GLk (q) the general linear group of k x k 
nonsingular matrices over Fq) and Z e Mkxm(q) such that
C’ = XCa Y + Z, when k = m
C' = XCa Y + Z, or C' = X(Ct)a + Z when k = m 
https://doi.org/10.1017/9781009283403.013 Published online by Cambridge University Press

302
12 Rank-Metric and Subspace Codes
where a is an isometry of Mkxm (q) (a bijective map that preserves rank) acting 
on the entries of C e C and the superscript t indicates matrix transpose. If 
both C and C' are closed under addition (additive codes), then Z must be 
the zero matrix, and if they are both linear codes, then a can be taken as the 
identity map.
Certain types of rank-metric codes have been investigated, initiated in the 
works of Delsarte [5] and Gabidulin [16], respectively. Although the next 
section gives constructions of these rank-metric codes, it is convenient to 
discuss the definitions and some of their properties here before giving their 
constructions. Reference [34] contains an excellent view of this material.
Definition 12.5 A Delsarte rank-metric code of shape k x m over Fq is an 
Fq linear subspace C c Mkxm (q). The dual code of C is
C± = {A e Mkxm(q) | (A,B) = 0VB e C}.
Gabidulin [16] introduced the notion of the rank of a vector over a subfield 
as follows:
Definition 12.6 Let Fqm be an extension field ofFq of degree m. A Gabidulin 
rank-metric C code of length k over Fqm is an Fqm linear subspace C c Fqkm . 
The rank of a vector a = (a 1 ,a2,. ..,ak) e Fqm is the Fq dimension of the 
subspace spanned by a 1,..., ak, denoted (a 1,... ,ak}. The dual of a Gabidulin 
code C is
C1 = {^ e Fkqm | (a,0) = 0Va e c}.
The above definitions of Delsarte and Gabidulin codes are slightly different 
from the codes defined in the original works ([5] and [16], respectively). At 
times it is convenient to refer to m x k matrices rather than k x m and the 
equivalence is clear.
Note that both Gabidulin and Delsarte rank codes are linear over their 
respective fields. It is clear that a Gabidulin code can be viewed as a Delsarte 
code by using a basis of Fqm over Fq and noting that linearity over Fqm implies 
linearity over Fq. To this end let G = { y 1,... ,Ym} be a basis of Fqm over 
Fq . For a codeword (vector) in the Gabidulin code, a e C c Fqkm, a = 
(a1,...,ak), aj e Fqm, associate an m x k matrix over Fq MG(a) by defining 
its j-th column as the expansion of aj with respect to the basis vectors of 
G, i.e.,
rn
aj = 
MG(a)ij yi a e C,j= 1,2,...,k
https://doi.org/10.1017/9781009283403.013 Published online by Cambridge University Press

12.1 Basic Properties of Rank-Metric Codes
303
and the Delsarte code associated with the Gabidulin code C c Fk is 
. q .
Cg(C) = {MG(a) | a e C} c Mmxk(q), M§(a) = (mg(aj.
It is clear that for every Fq basis of Fqm, G, the mapping
§ : Fkm  > Mmxk(q)
q
v ^ Mg(v)
is an Fq -linear bijective isometry. Further if C is a Gabidulin code (at times 
also referred to as a vector rank-metric code), then the corresponding rank­
metric code CG (C) has the same cardinality, rank distribution and minimum 
distance ([21], chapter by Gorla and Ravagnani). By this process it is clear 
that any Gabidulin code can be viewed as a Delsarte code. However, since 
Gabidulin codes are Fqm subspaces while Delsarte codes are Fq subspaces, the 
converse is clearly not true. It is noted [34] that for a Gabidulin code C c Fqkm 
and associated Delsarte code CG(C),
dimFq CG (C) = m dimFqm (C)
and that the rank distribution ofCG(C) is the same as that ofC. Thus a Delsarte 
code C with m \ dimFq (C) cannot arise from a Gabidulin code.
It is noted that in general
Cg (c ±) = Cg (c )±.
However, if G = {Yi} and G' = {Pj} are dual (trace dual, orthogonal 
or complementary - see Chapter 1) bases (i.e., Tr(Yi,Pj) = ^ij), then ([21], 
chapter by Gorla and Ravagnani, theorem 1, [34], theorem 21)
Cg- (C±) = Cg (C)±
For clarity and since the definition of a rank-metric code in the literature is 
not consistent, the following definitions are restated:
(i) A rank-metric code is a subset of Mkxm(q).
(ii) A vector rank-metric code is a subset of Fkm .
q
(iii) A Delsarte rank-metric code is an Fq linear subspace of Mkxm(q).
(iv) A Gabidulin rank-metric code is an Fqm subspace of Fqkm .
Recall from Definition 12.1 a (nonlinear) rank-metric code with Mkx m 
matrices over Fq and minimum distance dmin is denoted C = (k x m,M, dmin)q 
where k x m is the code shape and M the size of the code (number of 
codewords) and if the code is linear (over Fq ) and M = qK denoted a 
(k x m,K, dmin)q code.
https://doi.org/10.1017/9781009283403.013 Published online by Cambridge University Press

304
12 Rank-Metric and Subspace Codes
The notion of a maximum-rank distance (MRD) code is of importance 
and follows from the following bound, reminiscent of the Singleton bound 
in algebraic coding theory. It was first proved in [5]:
Theorem 12.7 Let C c Mkxm(q) be a nonzero (not necessarily linear) rank­
metric code with minimum rank distance dmin. Then for (wlog) k < m
|C|< min qqm<--dmin+1 ),qk(m-dmin+1)}. 
Singleton bound
Rank metric codes that meet this bound are called MRD codes.
Proof: From C obtain the set of matrices C' in M(k-dmin+1) xm by deleting 
the bottom (dmin - 1) rows of the matrices of C. The matrices of C' must be 
distinct as otherwise a subtraction would yield an all-zero matrix implying a 
nonzero matrix in C had rank less than dmin contrary to assumption. Since the 
number of such matrices is at most qm((k-dmin+1) the result follows. ■
As noted, both the Delsarte and Gabidulin codes in the original works [5, 
16] were specific constructions to be given in the next section. In a sense the 
MRD rank-metric codes can be viewed as analogs of the Reed-Solomon codes 
(or more generally maximum distance separable (MDS) codes) of algebraic 
coding theory since they achieve similar upper bounds on their size. To obtain 
further bounds, much as for block codes, define the notion of spheres and balls 
of radius i in Mkxm (q) with center Y as
Si (Y) = {X e Mkxm (q) | rank(X - Y = i} and Bt = U Si.
i=0
It is clear the quantities are independent of the center chosen. Define the 
quantities
st =|St | 
and bt =|Bt |
where in these expressions the dependence on q, k and m is understood. Let 
Nq (k x m,r) denote the set of k x m matrices over Fq of rank r and denote 
by Nq(k x m,r) =|Nq(k x m,r) | . As shown in Appendix A this quantity is 
evaluated as
r-1 
((q-i - 1)
Nq(k x m,r) = fl qqm - ql) jqii + 1 - 1j . 
(12.4)
This can be expressed in terms of Gaussian coefficients. Recall that the 
number of subspaces of a vector space of dimension k in Vn(q) is given by the 
Gaussian coefficient (see Appendix A) which is:
https://doi.org/10.1017/9781009283403.013 Published online by Cambridge University Press

12.1 Basic Properties of Rank-Metric Codes
305
n
= (qn — 1 )(qn — q)(qn — q2) ••• (qn — qk—1 — 1)
k q = (qn —
(qk — 1 )(qk — q) ••• (qk — 
1 )(qn—1 — 1) ••• (qn—k +1 —
qk—1)
1)
(qk — 1 )(qk—1 — 1) ... (q — 1)
.
(12.5)
(It is understood that for n < 0,k < 0ork>nthe value is 0 and for n = 0 and 
k > 0 the value is 1.) These coefficients have interesting and useful properties, 
a few of which are noted in Appendix A. Thus
Nq(k x m,r) = q r2 m
r
i- r
k n(q‘
q q i=0
- 1).
(12.6)
The quantities of interest in the above computations are st and bt . Bounds 
on st and bt can be obtained ([30], proposition 1):
q(k+m —2)t—t2 < st < q(k+m +1 )t—t2
q(k+m —2)t—t2 < bt < q(k+m+1 )t — t2 + 1
The sphere packing bound on (k,M,dmin)qm codes in the rank metric then 
is, for t = L(dmin — 1)/2J
M• |Bt1= Mbt < qkm 
Sphere packing bound
and it is observed ([30], proposition 2) that no perfect codes exist in the rank 
metric (i.e., equality cannot be obtained for any set of parameters in the above 
inequality). The proof of this is relatively simple following from the form of the 
equations for the sizes of st and bt given above. The corresponding statement 
for algebraic coding theory, that all perfect codes are known (essentially two 
Golay codes, Hamming codes, binary repetition codes and some nonlinear 
codes the same parameters as Hamming codes) was a difficult result achieved 
after many years of research.
The following proposition is viewed as an analog of the Varshamov-Gilbert 
(VG) bound of algebraic coding theory:
Proposition 12.8 ([30], proposition 3) Let k,m,M, dmin be positive integers, 
t = L (dmin — 1)/ 2 J ■ If
Mx |Bd—1 |< qmk 
Varshamov-Gilbert bound
then a (k, M + 1,dmin)qm vector rank code exists.
In the next section MRD codes for all allowable parameters will be 
constructed and so the question of obtaining further bounds for non-MRD 
codes seem less compelling than is the case in algebraic coding theory.
https://doi.org/10.1017/9781009283403.013 Published online by Cambridge University Press

306
12 Rank-Metric and Subspace Codes
As with algebraic block codes that meet the Singleton bound, MRD 
codes have interesting mathematical structure and have been studied widely. 
Many aspects of their construction and properties are considered in ([21], 
chapter by Otal and Ozbudak). These include known constructions of linear, 
nonlinear additive, nonadditive (but closed under multiplication by a scalar) 
and nonadditive (and not closed under multiplication by a scalar) MRD codes. 
A brief overview of a small part of this work is considered in the next 
section.
The following properties of such codes are straightforward consequences 
of the definitions. Let C be a Delsarte rank-metric code, a linear subspace of 
Mkxm(q) over Fq. Recall that
C1 = {A e Mkxm(q) | {A,B) = 0VB e C}
noting the inner product on Mkxm(q). Clearly
dimFq (C) + dimFq (C1) = km
(where dim here refers to the dimension ofC as a subspace of Mkxm(q)) and
(C1)1 = C.
Further properties of the dual space are contained in lemma 5 of [34] (the 
journal version of [33]) including that for two subspaces C and D
(C n D)1 = C1 + D1 and (C + D)' = C ' D'.
The addition of spaces here refers to the smallest subspace containing both 
spaces.
In analogy with the linear MDS codes of algebraic coding theory it was 
shown for both Delsarte codes ([5], theorem. 5.6) and Gabidulin codes ([16], 
theorem 3) that:
Theorem 12.9 A linear rank-metric code C is MRD ^^ C1 is MRD.
Furthermore in both cases it is established that the rank distribution of a 
linear MRD is entirely determined by its parameters [5, 16] much like the 
weight distribution for MDS codes of algebraic coding theory. The expression 
for the rank distribution of a linear MRD code is omitted. It is shown in [2] this 
is true of any (linear or nonlinear) MRD code.
The following proposition ([21], chapter by Gorla and Ravagnani, proposi­
tion 3) summarizes a further property:
Proposition 12.10 Let C c Mkxm(q) be a linear rank-metric code with 1 < 
dim(C) < km — 1 and k < m. The following are equivalent:
https://doi.org/10.1017/9781009283403.013 Published online by Cambridge University Press

12.1 Basic Properties of Rank-Metric Codes
307
(i) C is MRD.
(ii) C' is MRD.
(til) dmin(C) + dmin(CA) = k + 2.
It is also easy to show that ([2], proposition 39) for C c Fqkm, a Gabidulin 
code of minimum distance dmin (i.e., rank) and dual minimum distance dAn 
one of the following two statements holds:
(i) CisMRDanddmin+dmAin=k+2.
(ii) dmin + dmin < k.
Given that codes in the rank metric share many properties with codes in 
the Hamming metric it is perhaps not surprising that MacWilliams identities 
can be formulated for such codes. The following theorem is analogous to the 
familiar MacWilliams identities for algebraic coding theory:
Theorem 12.11 ([34], theorem 31) Let ^C c ^Mkxm(q) be a linear rank­
metric code. Let {Ai}i eN and {Bj} jeN be the rank distributions of C and Cm, 
respectively. For any integer 0 < v < k we have
k—v
E Ai
i=0
k - i _ | C | 
,, k - j
— 
7^1 
-
v 
qmv 
j v — j
q 
j—0 
q
v— 0, 1 , 2, . . . ,k.
It follows directly from this theorem that the rank distribution of a linear 
rank-metric MRD code C c Mkxm (q) depends only on k, m and the minimum 
rank of the code. Indeed by Theorem 12.11 and Proposition 12.10 if dmin — 
min rank(C), then Cm has minimum rank distance k — dmin + 2 and hence the 
above identities yield ([34], corollary 44), for 0 < v < k — dmin
k
v q
k—v
+ 
Ai
i—d
k—
|C| k
mv
q
v—0,1,2,...,k—dmin
i
q
v q
This system of k — dmin + 1 equations with k — dmin + 1 unknowns is seen 
to be upper triangular with ones on the diagonal and hence easily solvable and 
the above statement is verified.
Finally it is noted that both Delsarte [5] and Gabidulin [16] gave expressions 
for the rank distributions of their MRD codes. An impressive recent result 
of ([21], chapter by Gorla and Ravagnani, theorem 8) gives the following 
expression for the rank distribution of a not necessarily linear MRD code C 
with | C |> 2, 0 e C, k < m, with min rank(C) — dmin and A0 — 1,Ai — 0 
for 1 < i < dmin — 1 and
d1
At — E ( —1 )i — uq(—u)
u—0
i 
quq
+ 
(—1 )i — Uq(i2 u) +m(u—dmin+1) ki
iu
qq
k 
i
for dmin < i < k .
https://doi.org/10.1017/9781009283403.013 Published online by Cambridge University Press

308
12 Rank-Metric and Subspace Codes
The next section discusses constructions of certain classes of rank-metric 
codes, including the Delsarte and Gabidulin MRD codes, and shows, in 
particular, that such codes exist for all allowable parameter sets.
The section is concluded with a brief discussion of the notion of an anticode, 
first appearing in the work of Delsarte [4] and discussed in ([21], chapter by 
Gorla and Ravagnani) and in ([26], chapter by Kschischang).
Definition 12.12 A rank-metric A anticode is a nonempty subset A c 
Mkxm(q) such that d(A,B) < A V A,B e A and it is a linear anticode if 
it is an Fq linear subspace of Mkxm (q).
The equivalent notion for algebraic codes over Fq with the Hamming metric 
would lead to an optimal (maximal size) anticode being a sphere of diameter 
A in the Hamming space. This is not the case for codes in the rank metric - 
specifically a sphere of diameter d with the rank metric is an anticode but not 
an optimal anticode (see below for a definition of optimal for this case). Most 
importantly we have the following ([21], chapter by Gorla and Ravagnani, 
theorem 9):
Theorem 12.13 Let A be a A anticode in Mkxm(q) with 0 < A < k — 1. 
Then | A|< qmA and the following conditions are equivalent:
1. |A|= qmA
2. A + C = Mkxm(q) for some MRD code C with dmin(C) = A + 1.
3. A + C = Mkxm(q) for all MRD codes C with dmin(C) = A + 1 
where
A + C = {X + Y | X e A, Y e C}.
A A anticode is said to be optimal if it attains the above bound, |A|= qmA. 
The same work ([21], chapter by Gorla and Ravagnani, proposition 4) also 
contains the result:
Proposition 12.14 Let A c Mkxm (q) be a linear rank-metric code with 
dim (C) = mA and 0 < A < k — 1. The following are equivalent:
1. A is an optimal A anticode.
2. A n C = {0} for all nonzero MRD linear codes C c Mkxm(q) with
dim (C) = A + 1.
Finally it is noted that the dual of an optimal linear A anticode in Mkxm(q) 
is an optimal linear k — A anticode.
https://doi.org/10.1017/9781009283403.013 Published online by Cambridge University Press

12.2 Constructions of MRD Rank-Metric Codes
309
12.2 Constructions of MRD Rank-Metric Codes
The previous section has considered general properties of rank-metric codes 
and in particular linear rank-metric codes. The original constructions of linear 
MRD codes of Delsarte [5] and Gabidulin [16] are described here along with 
proof they are MRD. Other more recent constructions of MRD codes are also 
considered. The notation of these works is adapted to ours as required.
It has been commented on that since MRD codes meet the rank-metric 
Singleton bound and, as is about to be shown, they exist for all allowable sets 
of parameters, there seems less motivation to search for other classes of codes. 
A question that has been of interest is whether there are MRD codes that are 
not Gabidulin or Delsarte codes. This has been answered in the affirmative, an 
example of which is the class of twisted Gabidulin (TG) codes discussed later 
in this section.
The Delsarte MRD codes are first constructed. The original work of 
Delsarte [5] is in the setting of bilinear forms and association schemes. 
The matrix approach used here is based on that approach. Without loss of 
generality, assuming k < m, the construction will develop a linear space of 
qm(k-d+1) m x k matrices each over fq, each nonzero matrix of rank at least 
d (hence MRD). Let Trqm|q denote the trace function of Fqm over Fq. Let 
{rn 1 ,v2,... ,rnm} be a basis of Fqm over Fq. Let {^ 1,..., ^k} be a set of k ele­
ments from Fqm linearly independent over Fq. Let u = {u0,u1,...,uk-d} be 
an arbitrary set of (k-d+ 1) elements from Fqm for some integer d (which will 
be the code minimum distance), 1 <d <k. Define the set of m x k matrices
k— d 
£ \
E u^i^q I, 
.=0 
( 
(12.7)
i = 1 , 2, . . . , m, j = 1 , 2, . . . ,k .
Since the set of (k — d + 1) elements of u, ui e Fqm, can be chosen in 
q m(k—d +1) ways (including the all-zero one), this is the number of matrices 
(codewords) generated. To see that each nonzero matrix in the code has rank 
at least d consider the number of solutions to the matrix equation
M(u) ct = 0, c = (c1,...,ck) e Fqk. 
(12.8)
Equivalently
k 
k 
k—d 
£\
M( M(u)ij 
V Trqm।J E ut^irf ) Cj
j 1 
j 1 z k-—d° 
qt
= Trqm|q(Vi X; uJ X; Cj^j ) I = 0, i = 1,2, ...,m,
e=0 
j=1
C = M(u) = (M(u)ij) | M(u)ij = Trqm|q
https://doi.org/10.1017/9781009283403.013 Published online by Cambridge University Press

310
12 Rank-Metric and Subspace Codes
since cj e Fq. The polynomial
k—d
p(x) = y? u^xq , ue e Fqm
£=0
is a linearized or q-polynomial (see Appendix A) over Fqm and its set of 
roots forms a vector space of some dimension over Fq. Since the result of 
the Trace operation is zero for all i if and only if p(Yq=0 cj^j) = 0 (see 
Chapter 1) the equation has at most qk—d solutions and the solution space 
of Equation 12.8 is of dimension at most k — d and the matrices M(u) have 
rank at least d for each of the qm(k—d+1) possible values ofu e F km—d+1. The 
q
transposes of the matrices are of shape k x m and have the same properties. 
It is clear that such a construction yields MRD rank-metric codes for all 
allowable parameter sets. Also, as noted, the rank distribution of such a code is 
known.
To describe Gabidulin MRD codes consider the following construction of 
m x k matrices over Fqm . Let g1,g2, . ..,gk e Fqm be linearly independent 
over Fq,k < m. First, define the K x k matrix over Fqm:
g1
g1
g 2 
•
• gk
MK xk =
q 
g 1
..
q 
g 2 
•
..
q
• gk
..
..
qK—1 
g2
qK—1
qK—1 
gk
and the matrix is of rank K < k over Fqm ([29], lemma 3.51 and appendix A) - 
since any square submatrix has nonzero determinant. Thus its row space over 
Fqm contains qmK vectors. The Gabidulin code C c Fis the row space over 
Fqm of the matrix MKxk containing qmK codewords/vectors.
As previously, the rank ofa vector a = (a1,...,ak) in Fqkm is the dimension 
over Fq of the space spanned by a1,...,ak. Equivalently expand each element 
of a e Fqm into a column vector of dimension m over Fq to form an m x k 
matrix over Fq . Thus each codeword vector over Fqm of the Gabidulin code is 
associated to such a matrix and the rank ofa vector is the rank of the associated 
matrix over Fq .
To determine the rank distance of the Gabidulin code, the rank of the 
associated matrix of a codeword is determined by proceeding as follows. 
Consider a codeword f of the Gabidulin code C which can be expressed 
f= (f (g1),f (g2),...,f (gk)) e Fqkm where
k—1
f(x) = 
fixq e Fqm[x], i.e., fi e Fqm
i=0
https://doi.org/10.1017/9781009283403.013 Published online by Cambridge University Press

12.2 Constructions of MRD Rank-Metric Codes
311
which is a q-polynomial. Let {n 1,... ,nm} be a basis of Fqm over Fq and 
express f(gi) = m= ”m=1 f (gi)j nj where f(gi)j is the coefficient of nj in the 
expansion f(gi) in the n basis. The codeword f can be expressed as the matrix 
equation
f(g1)1 
f(g1)2
f = (n 1, ...,nm) •
f(g 2 ) 1 ••• f(gk) 1
f(g2)2 ••• f(gk)2
= n • fn
Jgg 1 )m f(g2)m ••• f(gk)m_
where n is the row vector of basis elements and fn is the m x k matrix 
corresponding to the Gabidulin codeword f. That fn has rank at least k — K +1 
follows from a similar argument as used in the Delsarte codes.
Consider the following. Let f = (f(g 1 ),f(g2), ...,f(gk)) e F qm be a 
codeword in the Gabidulin code and it is wished to determine the rank of the 
word f over Fq . Equivalently, determine the null-space of the codeword over 
Fq, i.e., determine the number of vectors c = (c1,c2,...,ck) e Fqk orthogonal 
tof. This is equivalent to determining the number of zeros of the equation:
k /k—1 
\
YC(\Y fjge) = 0, c£ e Fq, fj e Fqm 
t=1 
' j=0 
'
K—1 
, k 
x
= E fj(J>gf)
K — 1 
, k x qj
= ^f^ Yc C£ge) .
Since jK=—01 fjxqj is a linearized polynomial its roots form a linear space with 
at most qK—1 elements. Thus the null-space of the m x k matrix over Fq is at 
most of dimension K — 1 and hence the rank offn is at least k — K + 1. Thus 
the Gabidulin code is equivalent to qK m x k matrices over Fq , each of rank 
at least k — K + 1. It follows MRD code.
Inherent in the previous arguments is the fact that a linear space of certain q- 
polynomials over Fqm corresponds to a linear MRD code. Also it is noted that 
a vector rank-metric code is equivalent to a matrix rank-metric code (although 
the vector spaces involved may differ) in the sense that a vector rank-metric 
code over Fqm of length k leads to an m x k rank-metric matrix code by 
representing elements in Fqm as vectors over Fq as above.
Many of the more recent MRD codes found are also based on subsets of 
q-polynomials. A recent classification of the codes is given in ([21], chapter by 
https://doi.org/10.1017/9781009283403.013 Published online by Cambridge University Press

312
12 Rank-Metric and Subspace Codes
Otal and Ozbudak). References are given there to recent constructions of linear 
MRD codes, nonlinear additive MRD codes, nonadditive (but closed under 
scalar multiplication) MRD codes, nonadditive and not closed under scalar 
multiplication codes. Morrison [32] examines notions of equivalence of such 
matrix codes. A few of the constructions of these codes are briefly noted. The 
class of generalized Gabidulin codes was formulated in [28] (see also [36] and 
[24]) by considering the vector space of linearized q-polynomials, for a fixed 
s and K,
G k,s = |f0 + f1 xq + • • • + fK-1 xq * ' | fi e F qm].
That these are linear MRD codes with shape m x k (where k is as in 
the construction of the usual Gabidulin codes) and minimum rank distance 
(k — K + 1) is easily established. Defining [i] = ql for the following matrix 
(in conflict with its usual meaning) the generator matrix for these generalized 
Gabidulin codes is 
MKxk
g1s[0] 
g1s[1]
g2s[0] 
g2s[1]
gks[0] 
gks[1]
s[K-1] 
g1
g2s[K-1] ••• gks[K-1]
for elements g1,g2,...,gk in Fqm, linearly independent over Fq and K<k. 
The Gabidulin codes correspond to the space Gk, 1 (s=1) and it is known ([36]) 
that there exist codes in Gk,s that are not equivalent to Gabidulin codes. It is 
shown in [28] also ([24], proposition 2.9) that if C is a generalized Gabidulin 
code of dimension K over Fqm, then C± c Fqqkm is a generalized Gabidulin 
code of dimension k - K and these codes are MRD.
A class of TG codes is formulated in [36] by denoting the set of q - 
polynomials
HK*n,K) = {f0x + f1 xq +----- + fK—1 xqK 1 + nf0q xqK | fi e Fqmj
where n e Fqm is such that N(n) = (—1 )mK where N*x) = x*qm—1 )/*q—1) 
is the norm function of Fqm over Fq and gcd*m, s) = 1. That these are linear 
MRD codes of dimension mK (qmK matrices) and minimum distance m - 
K + 1 is established in [36]. It is also shown that some of these codes are not 
equivalent to Gabidulin codes.
Generalized TG codes are also discussed in ([21], chapter by Otal and 
(Ozbudak, theorem 4) and in [36]. Numerous other aspects of rank-metric codes 
have been considered in the extensive literature.
https://doi.org/10.1017/9781009283403.013 Published online by Cambridge University Press

12.3 Subspace Codes
313
A class of rank-metric codes that will be of interest in the next section 
on subspace codes are those of constant rank, i.e., each codeword (vector or 
matrix) will have the same rank. These can be obtained from more general 
codes as subsets of constant rank and are generally nonlinear.
12.3 Subspace Codes
This section considers another related type of coding where the codewords are 
subspaces of a vector space according to a distance metric defined on such 
spaces. The application of these codes to network coding is noted. Clearly part 
of the section could have been placed in Chapter 5. It is included here due to 
its association with the use of rank-metric codes in their construction. The first 
part of this section introduces the notion of subspace codes and the remainder 
gives some constructions using the previously discussed rank-metric codes. 
The application of subspace codes to network coding is briefly noted.
The basic notions of subspaces of a vector space over the finite field Fq 
are recalled. Let Pq (n) denote the set of all subspaces of the vector space 
of dimension n over Fq, Fqn ~ Vn(q), and Gq(n,k) denote the set of all 
k-dimensional subspaces of the space Fqn , referred to as the Grassmannian 
of order k of Fqn . As noted previously the cardinality of this Grassmannian is
|Gq(n,k)|=
n
(12.9)
The following simple bound can be useful ([17, 27] and Appendix A): 
n
k q
qk(n-k) <
< K— qk(n-k)
(12.10)
where Kq = ]”[°=1 (1 - q-i). The constant K-1 is monotonically decreasing 
with q with a maximum of 3.4594 at q = 2 and a value 1.0711 at q = 16. This 
follows directly from the definition of the Gaussian coefficient.
Consider now the problem of defining codes on Pq (n) for which a metric 
is required. For rank-metric codes the matrix rank was used for distance. For 
subspaces the equivalent notion is that of dimension of certain subspaces.
For any two subspaces of Vn(q), U and V denote U + V as the smallest 
subspace containing both U and V. Then
dim(U + V) = dim(U) + dim(V) - dim(U n V).
It is easy to verify that the subspace distance defined by
dS(U,V) = dim(U) + dim(V) - 2dim(U n V) 
(12.11) 
https://doi.org/10.1017/9781009283403.013 Published online by Cambridge University Press

314
12 Rank-Metric and Subspace Codes
is a metric on subspaces of Vn(q), the “subspace distance” between subspaces 
U and V . The similarity with the definition of the rank metric for matrices is 
immediate. It also follows that
dS(U,V) = 2dim(U + V)- dim(U) - dim(V). 
(12.12)
For a subspace U c Vn(q) the dual space is
U± = {v e Vn(q) | (u,v) = 0, Vu e U}
where (u,v) is the usual inner product on Vn (q).
The following identities were noted earlier (e.g., [27]):
(U + V)± = U± n V± and (U n V)± = U± + V±.
and hence
ds(U ± ,V ±) = ds(U,V).
Definition 12.15 A subspace code in Vn (q), C c Pq (n), is a collection of 
subspaces of Vn(q). The minimum distance of C is
dS,min(C) = 
min 
dS(U,V).
, mn 
U,V eC,U =V
Such a code is designated a C = (n, | C |, dS,min(C))q code. Further define 
dS.max(C) = maxUeC dim(U), a maximum dimension of codewords. If all 
the words of the code have the same dimension (C c Gq (n, k) for some 
k), it is referred to as a constant-dimension code (CDC), and designated as 
a C = (n, | C | , k,dS,min(C))q CDC code. Denote by AqSC (n, dS, min) and 
AqCDC (n,k,dS, min) the maximum number of codewords (subspaces) in these 
two types of codes, respectively.
From the previous comments it is immediate that if C = (n,M,k,dS, min)q 
is a constant-dimension subspace code of subspaces of dimension k, then C± 
is a constant-dimension (n,M,n - k, dS, min)q subspace code since the dual of 
a subspace of dimension k is one of dimension n - k.
Another type of metric on subspaces is introduced in [39] suitable for 
an adversarial type of network channel. For network coding a collection of 
packets is identified with the subspace they generate, the reason for the interest 
in subspace coding. It is shown that in this adversarial case a more appropriate 
metric is the injection metric defined as follows:
https://doi.org/10.1017/9781009283403.013 Published online by Cambridge University Press

12.3 Subspace Codes
315
Definition 12.16 The injection distance between two subspaces U,V e 
Pq (n) is defined as
dI(U, V) = max{dim(U), dim(V)} - dim(U n V) 
= dim(U + V) - min{dim(U), dim(V)} 
= 1 ds(U, V) + 2 |dim(U) - dim(V) | .
That this is in fact a metric is shown in ([39], theorem 22). If an injection 
is defined as the deletion of a packet at a node and the insertion of another 
(orthogonal) packet (a packet outside the space of information packets received 
at that node), then the injection metric between spaces U and V may be viewed 
as the number of injections required to modify the space U to become the 
space V. Example 12.17 attempts to demonstrate the relationships in the above 
definition.
Example 12.17 Suppose U,V e Gq (n, k). In this case the subspaces have 
constant dimension and
di(UV) = 2 ds(U,V)
(the subspace distance for subspaces of the same dimension is always even) 
and for this case the two metrics are equivalent. As in Figure 12.1
dS(U,V) = dimU +dimV -2dimU n V = k+k -2(k - i) =2i
and
dI (U,V) = i.
Since dim(U\UnV) = i it clearly takes i insertions and deletions to transform 
U to V.
Figure 12.1 Subspace enumeration, dS (U,V) = 2i , equal dimension spaces
https://doi.org/10.1017/9781009283403.013 Published online by Cambridge University Press

316
12 Rank-Metric and Subspace Codes
The relationship between CRC (constant rank) matrix codes and CDC 
(constant dimension) subspace codes is interesting and is pursued in [18] using 
the injection distance. A few key results from that work are described adapted 
to our terminology.
For the remainder of the section interest will be on the bounding and 
construction of CDC codes in Pq (n) of dimension k (i.e., in Gq (n, k)). Recall 
from Definition 12.15 the maximum size of such a code with minimum dis­
tance dS, min is denoted AqCDC (n,k,dS, min) (minimum distance d and subspace 
dimension k).
To discuss the analogs of the usual algebraic coding bounds for subspaces, 
and the rank distance coding bounds, it is of interest to determine the number 
of subspaces at distance 2i from a fixed subspace U in Gq (n,k). Define [27] a 
ball of radius 2i and a sphere of radius 2t around the subspace U as (recalling 
such distances are even):
Bq(n,k,i) = {V e G q(n,k) | ds(U,V) = 2 i}, bq(n,k,i) =| Bq(n,k,i) | 
Sq(n,k,t) ={ V e G q(n,k) | ds(U,V) < 21}, sq(n,k,t) =| Sq(n,k,t) |
= U i < tBq(n,k,i).
(12.13)
It is clear the enumeration is independent of the “center,” U, although allowing 
subspaces of other dimensions would negate this statement [27]. To enumerate 
this quantity, consider a fixed subspace U e Gq (n, k) of dimension k and a 
fixed - for the moment - subspace of U, W, of dimension k — i. There are
k
k
k —i q
ways of choosing W in U. Consider the number of ways of 
adding points (linearly independent) to W in the exterior of U in such a way 
to expand U n V into a k space V e Gq(n,k). This would result in
dS(U, V) = dim(U) + dim(V) — 2dim(U n (V) = 2k — 2(k — i) = 2i.
The number of distinct ways this can be done using standard arguments is:
(qn — qk)(qn — qk +1) ••• (qn — qk+(i — 1))
q(k—i)+(k—i+1) +----+ (k — 1)
(qk — qk i)(qk — qk i+ 1) ••• (qk — qk 1)
k + (k +1) +----+ (k+(i — 1))
q
n-k
(12.14)
i
=qi2
k
q
where the denominator of the first expression represents the number of ways 
of choosing independent vectors to complete a (k - i) space to a k space.
https://doi.org/10.1017/9781009283403.013 Published online by Cambridge University Press

12.3 Subspace Codes
317
Since the number of ways in choosing the intersection subspace U n V in U is
k
k
k-i 
q
we have
tE
i2
qi
i=0
kn -k
(12.15)
i
q
i
q
The simplest bound [11, 27] for the size ofa CDC code of distance 2t is the 
sphere packing bound, the analog of that in algebraic coding theory,
AqCDC (n,k,dS =2t)
n
k
■ n q 
n
k
q
n-k
i 
qq
Sphere packing bound
qi
0
i2 k 
i
(12.16)
<
<
With similar reasoning for a fixed V e Gq(n,k) the number of W e 
Gq(n,j) which intersect V in a subspace of dimension I is given by ([26], 
chapter by Kschischang):
Nq(n,k,j,t) = q(k -e)(j -£)
k
n-k
£ j 
q
£
q
and the result of Equation 12.14 follows immediately from this.
Other bounds on the sizes of subspace codes are noted. The analog of 
the Varshamov-Gilbert (VG) bound of algebraic coding theory for constant­
dimension subspace code is argued in a very similar manner. Consider 
constructing a subspace code of minimum distance dS,min in the following way. 
Choose an element of Gq (n,k) as an initial codeword and surround it with a 
sphere of radius dS,min - 1. Continue selecting codewords (subspaces) that 
are external to any of the spheres. If at some point the number of codewords 
chosen in this manner is M - 1 and
(M - 1) • sq(n,k,ds,min - 1) <
n
k
q
then another codeword can be added. Thus if C is an optimal code (with 
maximum number of codewords AqCDC (n,dS,min,k)) with distance dS,min, then 
we must have
n
ACc°C(n,k,ds, min) >
sq (n,k, dS, min
1), Varshamov-Gilbert bound
since otherwise another codeword could be added.
https://doi.org/10.1017/9781009283403.013 Published online by Cambridge University Press

318
12 Rank-Metric and Subspace Codes
For another type of bound an interesting puncturing technique is described 
[27] that leads to an analog of the Singleton bound. Consider the CDC 
subspace code C = (n,M,k,dS,min)q c Gq(n,k). Let S be a subspace of 
Vn(q) of dimension n - 1. For a positive integer k define a stochastic erasure 
operator Hk(U) that operates on the subspaces Pq (n) such that if:
Hk(U) =
random subspace of U of dimension k 
if dim(U) > kU else.
(12.17)
This operator is motivated by network coding considerations as in [27, 38].
This interesting work is not pursued here.
Define the code
C' = {U' = Hk-1 (U) | U e C}.
It is shown that C' = (n - 1,M',k - 1,d' > ds,min -2}q is a CDC code. To see 
this let U' = Hk —1 (U) and V' = Hk-1 (V) for two distinct codewords U,V 
of C. Since
2 dim(U' n V') < 2 dim(U n V) = 2k - ds, min, 
by the definition of minimum distance (dS) in C it follows that
ds(U',V') = 2(k - 1) - 2dim(U' n V') > ds,min - 2.
Thus, while the codewords of the code C' may not be unique, due to the 
random erasure operator, the parameters of the code are M' = M and 
d' > dS, min - 2, giving the parameters stated.
Suppose in the CDC code C = (n,M,k,dS, min)q the minimum subspace 
distance is ds, min = 2y + 2. Applying this puncturing operation to the code C 
S times yields a code C"(n - Y,M,k - Y,d" > 2)q. Since the codewords are 
unique, this implies that
M< n- 
k-
n-S
y
S
q
n
k q
We could also apply the puncturing to the dual code C± = (n,M,n - k,d)q 
(same minimum distance) and hence 
AqCDC(n,k,dS,min = 2y +2) <
n-y 
max {k, n - k}
Singleton bound
(12.18)
which is the desired bound ([11], equation 3, [27], theorem 9), regarded as an 
analog of the Singleton bound for CDC codes.
https://doi.org/10.1017/9781009283403.013 Published online by Cambridge University Press

12.3 Subspace Codes
319
Using the inequality of Equation 12.10 gives another version of the 
inequality (assume k > n/2):
AcCDC(n,k,d = 2Y + 2) < K- qk(n-k-y). 
(12.19)
Another approach gives a further interesting bound. As with rank-metric 
codes the following notion of a subspace anticode is of interest, first raised 
in the work of Delsarte on association schemes [4]. Delsarte’s work was 
concerned with bounding the size of cliques in association schemes.
To introduce the notion of an anticode bound in this subspace environment 
consider first the notion applied to binary codes and define an anticode AH (d- 
1) to be a binary set of n-tuples such that the Hamming distance between any 
two x, j e AH(d - 1) is at most d - 1. A sphere of radius (d - 1)/2 in the 
Hamming metric is an optimal anticode. Suppose C is an (n,M,d)2 binary 
code and consider the array of n-tuples whose M rows consist of the words of 
the anticode added to a word of the code. It is clear that the words of this array 
are distinct and hence
M• |AH(d - 1)|< 2n
which is viewed as an anticode bound for binary codes.
A similar situation occurs for CDC codes in Gq (n, k) although it is not 
justified here. As noted in the previous section, while spheres are optimal 
anticodes in Hamming spaces they are not in Vn (q) and indeed [11] are far 
from optimal and the maximal size of such sets is, as discussed below, found 
by Frankl and Wilson [15].
Definition 12.18 A CDC anticode A in Gq(n, k) of diameter 2Y is any subset 
of G(n, k) such that dS(U,V) < 2Y for all U,V e A.
Thus any two subspaces in A intersect in a subspace of dimension at least 
k - Y. Fankl and Wilson [15] (see also [23]) determined the maximum size of 
anticodes in Gq(n,k) as follows. The set of subspaces of Vn(q) which contain 
a fixed subspace of dimension k - Y + 1 is an anticode of diameter 2Y - 2. 
This set is of size
n-k+Y - 1 
Y - 1 q.
Similarly the set of subspaces of Gq (n, k) contained in a fixed subspace of 
dimension k + Y - 1 is also an anticode of dimension 2Y - 2 with at least 
k + a -1
https://doi.org/10.1017/9781009283403.013 Published online by Cambridge University Press

320
12 Rank-Metric and Subspace Codes
subspaces (see [15], theorem 1). These anticodes are the largest possible in 
Gq (n, k) whose elements intersect in at least a subspace of dimension k — y 
and is of size
max
n—k+y — 1 k+y — 1 
y—1 q, k q
and hence are of diameter 2(y — 1).
It is convenient to define y so that the code minimum distance is dS, min = 
2y + 2. For this the Delsarte anticode bound ([4], theorem 3.9, p. 32) then 
([11], theorem 1, [26], theorem 1.6.2) is the size of Gq (n, k) divided by the 
size of the largest anticode or
AqDC(n,k,ds,min) = 2y + 2) <|Gq(n,k)\/\A(y) |=
n
k q
n—k+y
.
yq
Anticode bound
This bound was first obtained in [40] in the context of authentication codes. It 
was observed in [41] that this bound is always better than the Singleton bound 
(Equation 12.18) for nontrivial CDCs.
Another such bound is given in ([11], theorem 2) as:
Theorem 12.19
AqCDC(n,k,d = 2y +2) < k—
n
k
k
y q
Proof: Suppose C is an optimal CDC CCDC(n,k,dS,min = 2y +2) code with 
M =\C \. The proof is by enumeration ofk — y subspaces. Note first that any 
two codewords U and V cannot have a k — y subspace of Vn(q) in common 
as otherwise, if they had such a subspace in common, then
dS(U,V) < 2k — 2(k — y)= 2y<d,
a contradiction. The total number of k — y subspaces in Vn (q) is
n
k
yq
On the other hand each codeword contains
k
such subspaces and by
k
yq
the previous comment these are distinct and hence
M •
k
n
k— yq
k—y
as was required to show. ■
https://doi.org/10.1017/9781009283403.013 Published online by Cambridge University Press

12.3 Subspace Codes
321
There are numerous other bounds on CDC codes, including an analog of the 
Johnson bound of algebraic coding theory [41]. The chapter on Network Codes 
in the volume [26] gives an excellent discussion on bounds (and indeed on the 
whole subject of network coding). The work of [11] also discusses bounds and 
gives improvements of many of them, as does [23].
It is interesting to note that, as was the case for rank-metric codes, there are 
no perfect subspace codes (not just no perfect CDC codes) ([11], theorem 16).
With the above bound on the size of CDCs for a given distance and 
dimension, the construction of subspace codes is considered. An interesting 
construction of such codes uses the rank-metric codes of the previous two 
sections as follows.
Denote by CRM a rank-metric code of shape k x m with minimum distance 
dR and {A) the row space of the matrix A e CR. Let AB denote the matrix 
which is the catenation (not multiplication) of compatible matrices A and B 
(i.e., the columns ofAB are those ofA followed by those of B). Consider
Cs = {(IA) | A e CRM|
a collection of row spaces generated by the rank metric matrices of CRM 
with I a dimension-compatible identity matrix. Denote the minimum subspace 
distance ofCS by dS and the minimum rank-metric distance of CRM by dR,min. 
Further let
(AB} and B
be the row spaces of the matrix A catenated with B horizontally and vertically 
(rows of B added to those of A), respectively, assuming dimension compati­
bility. Recall that the rank of a matrix is the dimension of its row space (or 
column space). In the proof of the following proposition it will be convenient 
to denote the corresponding matrices as
AB
and
A
B
For any matrix A, dim (A} = rank [A]. Then ([38], proposition 4):
Proposition 12.20 With the notation above for subspaces
dS = 2dR .
Proof: For A, B e CR let U = (IA) and V = {IB) and note that dim U = 
dim V = k. From Equation 12.12
https://doi.org/10.1017/9781009283403.013 Published online by Cambridge University Press

322
12 Rank-Metric and Subspace Codes
dS(U,V) = 2dim(U + V) - 2k
= 2rank
= 2rank
= 2rank
IA
IB
IA
0B-A
B -A = 2dR(A,B)
and the result is immediate. ■
This construction of a subspace code from a rank-matrix code is referred to 
as lifting. Thus an MRD rank-metric code yields a lifted subspace code with 
constant dimension and whose distance distribution is easily derivable from 
that of the MRD code. Such codes exist for all allowable parameter sets.
Suppose CMRD is an MRD rank-metric code in M^xm(q) with minimum 
rank distance dR and number of codewords (matrices) equal to the Singleton 
bound:
M = qm(k-dR+1).
The above lifting process then yields a CDC code with M subspaces in Vn(q) 
of dimension k and minimum distance 2dR - hence a CCDC = C(n = k + 
m, M = qm(k-dR+1),k,2dR)q code. It is argued in [25] that these MRD-lifted 
subspace codes are asymptotically optimal in the following sense. Using the 
Singleton bound for CDC codes with minimum distance 2y + 2 of Equation 
12.18 and the q-coefficient approximation of Equation 12.10 yields the upper 
and lower bounds 
q(n-k)(k-y+1) < ACDC(n,k,dS) <
n
k-y +1
1q(n-k)(k-y +1)
^ Kq
but the codes lifted from an MRD rank-metric code have
(n-k)(k-y +1) 
q
subspaces which is within a factor of Kq-1 of the above upper bound and hence 
it is argued the lifted MRD codes are asymptotically optimal.
The work of Gadouleau and Yan [18] improved on the lifted construction as 
follows. Recall from Definition 12.15 ACDC(n,k,dS) is the maximum number 
q
of subspaces of Vn(q) of dimension k in Vn(q) with distance dS. For an 
arbitrary matrix A e Mkxm(q) let RSP(A) and CSP(A) denote its row and 
column spaces, respectively. For a rank-metric code C c Mkxm(q) define 
subspace codes in the natural manner:
CSP(C) = {U e Pq(k) | 3A e C,CSP(A) = U}
RSP(C) = {V e Pq(m) | 3B e C,RSP(B) = V}.
https://doi.org/10.1017/9781009283403.013 Published online by Cambridge University Press

12.3 Subspace Codes
323
To derive CDC subspace codes from CRC rank-matrix codes the following 
results are noted:
Proposition 12.21 ([18], propositions 1 and 2) Let C be a CRC of rank r and 
minimum rank distance dR in Mkxm(q)
(i) Then the subspace codes
RSP(C) C Gq(m,r) and CSP(C) c Gq(k,r)
have minimum distance at least dR - r;
(ii) Suppose C has minimum rank distance d + r, (1 < d < r) in Mkxm(q). 
Then RSP(C) c Gq(m,r) and CSP (C) c Gq(k,r) have cardinality |C| 
and their minimum injection distances satisfy
dI (CSP (C)) +dI(RSP(C)) < d+r.
Interestingly, a reverse statement is also obtained in ([18], proposition 3) in 
that if M is a CDC in Gq(k,r) and N a CDC in Gq(m,r) such that |M|=|N|. 
Then there will exist a constant rank-metric code C c Mkxm(q) with constant 
rank r and cardinality | M| such that
CSP(C) = M and RSP(C) = N
and the minimum injection distances satisfy
dI (N) +dI(M) < dR < min{dI (N),dI (M)} + r.
Theorem 2 of [18] gives conditions on the code parameters for the row 
spaces of an optimal constant rank, rank-metric code to give an optimal CDC. 
As a consequence it shows that if C is a constant rank-metric code in Mkxm(q) 
with constant-rank codewords of rank r and minimum distance d + r for 2r < 
k < m and 1 < d < r, then for m > m 0 = (k — r )(r — d + 1) or d = r the row 
space code ofC, RSP (C), is an optimal CDC in Gq (n,r). Thus finding optimal 
CRCs with sufficiently many rows, under the conditions stated gives optimal 
CDC codes. It seems that in general the rank-metric code problem yields more 
tractable approaches for the construction of subspace codes.
There is an impressive literature on the construction and analysis of 
subspace codes, in particular those of constant dimension, and this section 
is only a brief indication of that work. A variety of combinatorial structures 
have been applied to the problems and the area has attracted significant 
interest. The notions of Ferrer’s diagrams, graph matchings and Grassman 
graphs have been of interest in these constructions including the works 
[6, 7, 8,9, 10, 11, 13, 13, 14, 14, 19, 19, 20, 22, 31, 35, 37, 42]. The notion of 
a q-analog of a Steiner system is particularly interesting, consisting of a set of 
https://doi.org/10.1017/9781009283403.013 Published online by Cambridge University Press

324
12 Rank-Metric and Subspace Codes
k-dimensional subspaces S such that each t-dimensional subspace of Vn(q) is 
contained in exactly one subspace of S [1, 6, 7, 12].
There is a natural association of subspace codes and network coding 
explored in [27, 38] among many other works. Consider a system where 
packets are transmitted on the network and at each node a random linear 
combination of the arriving packets is forwarded. Additive erroneous packets 
may be included at any of the intermediate nodes as the packets progress 
through the network and information packets may be lost at any node due to 
network impairments. Thus the spaces associated with a set of packets and the 
network operations of errors, including packet insertions and deletions, can be 
viewed via the subspace codes introduced here. The reader is referred to the 
references noted for an account of this interesting area.
Comments
The chapter has given a brief overview of rank-metric and subspace codes and 
their relationships. While such concepts of coding may be in unfamiliar set­
tings, such as sets of matrices and sets of subspaces, the results obtained are of 
great interest, drawing for their development on geometry and combinatorics. 
The application of subspace codes to network coding is of particular interest 
and the survey article [25] is an excellent source for this area.
References
[1] Arias, F., de la Cruz, J., Rosenthal, J., and Willems, W. 2018. On q -Steiner 
systems from rank metric codes. Discrete Math., 341(10), 2729-2734.
[2] Cruz, J., Gorla, E., Lopez, H., and Ravagnani, A. 2015 (October). Rank distribu­
tion of Delsarte codes. CoRR. arXiv:1510.00108v1.
[3] Cruz, J., Gorla, E., Lopez, H.H., and Ravagnani, A. 2018. Weight distribution of 
rank-metric codes. Des. Codes Cryptogr., 86(1), 1-16.
[4] Delsarte, P. 1973. An algebraic approach to the association schemes of coding the­
ory. Philips Research Reports Supplements. N.V. Philips’ Gloeilampenfabrieken.
[5] Delsarte, P. 1978. Bilinear forms over a finite field, with applications to coding 
theory. J. Combin. Theory Ser. A, 25(3), 226-241.
[6] Etzion, T. 2014. Covering of subspaces by subspaces. Des. Codes Cryptogr., 
72(2), 405-421.
[7] Etzion, T., and Raviv, N. 2015. Equidistant codes in the Grassmannian. Discrete 
Appl. Math., 186, 87-97.
https://doi.org/10.1017/9781009283403.013 Published online by Cambridge University Press

References
325
[8] Etzion, T., and Silberstein, N. 2009. Error-correcting codes in projective spaces 
via rank-metric codes and Ferrers diagrams. IEEE Trans. Inform. Theory, 55(7), 
2909-2919.
[9] Etzion, T., and Silberstein, N. 2013. Codes and designs related to lifted MRD 
codes. IEEE Trans. Inform. Theory, 59(2), 1004-1017.
[10] Etzion, T., and Vardy, A. 2008 (July). Error-correcting codes in projective space. 
Pages 871-875 of: 2008 IEEE International Symposium on Information Theory.
[11] Etzion, T., and Vardy, A. 2011. Error-correcting codes in projective space. IEEE 
Trans. Inform. Theory, 57(2), 1165-1173.
[12] Etzion, T., and Vardy, A. 2011. On q -analogs of Steiner systems and covering 
designs. Adv. Math. Commun., 5, 161.
[13] Etzion, T., and Zhang, H. 2019. Grassmannian codes with new distance measures 
for network coding. IEEE Trans. Inform. Theory, 65(7), 4131-4142.
[14] Etzion, T., Gorla, E., Ravagnani, A., and Wachter-Zeh, A. 2016. Optimal Ferrers 
diagram rank-metric codes. IEEE Trans. Inform. Theory, 62(4), 1616-1630.
[15] Frankl, P., and Wilson, R.M. 1986. The Erdos-Ko-Rado theorem for vector spaces. 
J. Combin. Theory Ser. A, 43(2), 228-236.
[16] Gabidulin, E. M. 1985. Theory of rank codes with minimum rank distance. 
Problemy Peredachi Informatsii, 21(1), 1-12.
[17] Gadouleau, M., and Yan, Z. 2008. On the decoder error probability of bounded 
rank-distance decoders for maximum rank distance codes. IEEE Trans. Inform. 
Theory, 54(7), 3202-3206.
[18] Gadouleau, M., and Yan, Z. 2010. Constant-rank codes and their connection to 
constant-dimension codes. IEEE Trans. Inform. Theory, 56(7), 3207-3216.
[19] Gadouleau, M., and Yan, Z. 2010. Packing and covering properties of subspace 
codes for error control in random linear network coding. IEEE Trans. Inform. 
Theory, 56(5), 2097-2108.
[20] Gorla, E., and Ravagnani, A. 2014. Partial spreads in random network coding. 
Finite Fields Appl., 26, 104-115.
[21] Greferath, M., Pavcevic, M.O., Silberstein, N., and Vazquez-Castro, M.A. (eds.). 
2018. Network coding and subspace designs. Signals and Communication Tech­
nology. Springer, Cham.
[22] Heinlein, D. 2019. New LMRD code bounds for constant dimension codes and 
improved constructions. IEEE Trans. Inform. Theory, 65(8), 4822-4830.
[23] Heinlein, D., and Kurz, S. 2017. Asymptotic bounds for the sizes of constant 
dimension codes and an improved lower bound. Pages 163-191 of: Coding theory 
and applications. Lecture Notes in Computer Science, vol. 10495. Springer, 
Cham.
[24] Horlemann-Trautmann, A.-L., and Marshall, K. 2017. New criteria for MRD and 
Gabidulin codes and some rank-metric code constructions. Adv. Math. Commun., 
11(3), 533-548.
[25] Huffman, W.C., Kim, J.-L., and Sole, P. (eds.). 2020. Network coding in a concise 
encyclopedia of coding theory. Discrete Mathematics and Its Applications (Boca 
Raton). CRC Press, Boca Raton, FL.
[26] Huffman, W.C., Kim, J.L., and Sole, P. (eds.). 2021. A concise encyclopedia of 
coding theory. CRC Press, Boca Raton, FL.
https://doi.org/10.1017/9781009283403.013 Published online by Cambridge University Press

326
12 Rank-Metric and Subspace Codes
[27] Koetter, R., and Kschischang, F.R. 2008. Coding for errors and erasures in random 
network coding. IEEE Trans. Inform. Theory, 54(8), 3579-3591.
[28] Kshevetskiy, A., and Gabidulin, E. 2005 (September). The new construction 
of rank codes. Pages 2105-2108 of: Proceedings International Symposium on 
Information Theory, 2005. ISIT 2005.
[29] Lidl, R., and Niederreiter, H. 1997. Finite fields, 2nd ed. Encyclopedia of Mathe­
matics and Its Applications, vol. 20. Cambridge University Press, Cambridge.
[30] Loidreau, Pierre. 2006. Properties of codes in rank metric. CoRR, abs/cs/0610057.
[31] Martin, W.J., and Zhu, X.J. 1995. Anticodes for the Grassmann and bilinear forms 
graphs. Des. Codes Cryptogr., 6(1), 73-79.
[32] Morrison, K. 2014. Equivalence for rank-metric and matrix codes and auto­
morphism groups of Gabidulin codes. IEEE Trans. Inform. Theory, 60(11), 
7035-7046.
[33] Ravagnani, A. 2014. Rank-metric codes and their MacWilliams identities. 
arXiv:1410.11338v1.
[34] Ravagnani, A. 2016. Rank-metric codes and their duality theory. Des. Codes 
Cryptogr., 80(1), 197-216.
[35] Schwartz, M., and Etzion, T. 2002. Codes and anticodes in the Grassman graph.
J. Combin. Theory Ser. A, 97(1), 27-42.
[36] Sheekey, J. 2016. A new family of linear maximum rank distance codes. Adv. 
Math. Commun., 10(3), 475-488.
[37] Silberstein, N., and Trautmann, A. 2015. Subspace codes based on graph match­
ings, Ferrers diagrams, and pending blocks. IEEE Trans. Inform. Theory, 61(7), 
3937-3953.
[38] Silva, D., Kschischang, F.R., and Koetter, R. 2008. A rank-metric approach to 
error control in random network coding. IEEE Trans. Inform. Theory, 54(9), 
3951-3967.
[39] Silva, D., Zeng, W., and Kschischang, F.R. 2009 (June). Sparse network coding 
with overlapping classes. Pages 74-79 of: 2009 Workshop on Network Coding, 
Theory, and Applications.
[40] Wang, H., Xing, C., and Safavi-Naini, R. 2003. Linear authentication codes: 
bounds and constructions. IEEE Trans. Inform. Theory, 49(4), 866-872.
[41] Xia, S.-T., and Fu, F.-W. 2009. Johnson type bounds on constant dimension codes. 
Des. Codes Cryptogr., 50(2), 163-172.
[42] Xu, L., and Chen, H. 2018. New constant-dimension subspace codes from 
maximum rank distance codes. IEEE Trans. Inform. Theory, 64(9), 6315-6319.
https://doi.org/10.1017/9781009283403.013 Published online by Cambridge University Press

13
List Decoding
The two commonly assumed error models that are encountered in the literature 
are reviewed. The first model is the random error model which can be viewed 
in terms of the binary symmetric channel (BSC) as discussed in several earlier 
chapters. Each bit transmitted on the channel has a probability p of being 
received in error, where p is the channel crossover probability. The number 
of errors in n bit transmissions then is binomially distributed with parameters 
n and p with an average number of errors in a block of n bits of pn. As 
usual, denote by an (n,M,d)2 binary code with M codewords of length n 
and minimum distance d. The performance of an (n,M,d)2 binary code with 
bounded distance decoding is of interest. In this model a received word is 
decoded to a codeword only if it falls within a sphere of radius e = [d-1 J 
about a codeword - otherwise an error is declared. The probability of correct 
reception then is the probability that no more than e errors are made in 
transmission where
Pcorrect = 
^Pjt 1 - p)-j,
j 
j < e V/
and Perror = 1 - Pcorrect. This is referred to as the random or Shannon error 
model [9].
Another channel model, referred to as the Hamming or adversarial model, 
specifies the maximum number of errors that are made in transmission. The 
adversary is free to choose the positions of the errors to insure maximum 
disruption to communications and the choice may depend on the particular 
codeword chosen for transmission. The error probability then is dependent on 
the code properties and the number of errors assumed.
The implications of the two models are quite different in the following 
sense. Consider the Hamming error model and let A2 (n, d) denote the max­
imum number of binary codewords of length n with minimum Hamming
327
https://doi.org/10.1017/9781009283403.014 Published online by Cambridge University Press

328
13 List Decoding
distance d between any two codewords. It is argued that the maximum fraction 
of errors that can be tolerated with such a Hamming model is p< 1/4. For 
p>1/4 the number of errors will be of the order pn requiring a code with 
minimum distance d > 2pn > n/2. If 2d > n, by the Plotkin bound 
(e.g., [20]) the maximum number of codewords with minimum distance d is 
bounded by 
d
A2 (n,d) < 2
2d - n
1
1 - n/2d
and hence asymptotically for fixed d/n, the rate of the code
1- lim log2 A2(n,d) —> 0.
n n ^to
Thus if p> 1/4 error-free communications at a positive rate are not possible 
in the Hamming model.
On the other hand, in the Shannon model, the capacity of the BSC is (see
Chapter 1)
CBSC = 1 - H2(p) = 1 + p log2 p+ (1 - p) log2(1 - p)
where H2 (•) is the binary entropy function, and this function is > 0 for all 
0 <p< 1/2 implying that asymptotically there are codes that will perform 
well for this fraction. The difference of the two models is of interest.
To introduce list decoding, consider decoding a code C = (n,M,d)q , 
d = 2e + 1. A codeword c e C is transmitted on a DMC and j = c + n e Fq 
is received for some noise word n e Fqn. For unique (bounded distance) 
decoding, consider surrounding the received word with a sphere of radius e. 
If the sphere with center the received word j and radius e, contains a single 
codeword, it is assumed that at most e errors were made in transmission and 
the received word is decoded to that codeword. This is maximum-likelihood 
decoding since the probabilities are monotonic with Hamming distance.
Suppose now the sphere around the received word j is expanded to one of 
radius e > e. As the radius of the sphere e increases around j, it is likely 
that more codewords will lie in this sphere. Suppose, for a given e there 
are L codewords in the sphere. Rather than unique decoding the list decoder 
produces a list of L possible codewords within distance e of the received word. 
The relationship of the size of the list L and the decoding radius e is of interest 
as well as algorithms to produce the list, central issues for list decoding.
The subject of list decoding is explored in detail in the works of Guruswami, 
including monographs [7, 8] and excellent survey papers on various aspects of 
the subject. The treatment of this chapter is more limited.
https://doi.org/10.1017/9781009283403.014 Published online by Cambridge University Press

13 List Decoding
329
Definition 13.1 For a code C c Fq, |C|= M, and anyy e Fq define A(y,e) 
as the set of codewords within distance e of the received word y or:
A(y,e) = {c e C | d(y,c) < e}.
The code C = (n,M,d)q is said to be an (q,e,L) code if for any y e Fq
| A(m)|< l.
Decoding is deemed successful if the transmitted codeword is in the list.
Thus a code is (q,e,L) iff no sphere of radius e around any y e Fq contains 
more than L codewords. Equivalently a code is (q,e,L) if the spheres of radius 
e around codewords in C contains each point of Fq at most L times. Unique 
bounded distance decoding can be viewed as (n,e, 1) list decoding for a code 
with minimum distance d = 2e + 1.
As before, define the sphere of radius e about y e Fq as:
Definition 13.2 The sphere of radius e about y e Fq is the set of points
Sn,q(y,t) = {r e Fqq | d(x,y) < e}
Sq,q(y,^) = | Sq,q(y,^) |= E (q)(q - 1 )i
i=0
where the last equality follows since sq,q(e) is independent of the sphere 
center.
Some useful approximations for these functions can be formulated as 
follows. The size of the sphere, as q ^ rc> takes on an interesting form. Recall 
from Equation 1.13 the q -ary entropy function is defined as
H ( 
_ f x logq(q - 1) - x logqX - (1 - x) logq( 1 - x), 0 <x < & = (q - 1 )/q,
Hq (x) = 0,x= 0,
an extension of the binary entropy function.
It can be shown [8, 23, 26] that
q 11 mv (1 /q) log qSq,q( L pq J ) = Hq(p)
where p denotes the probability of error in each symbol of the received word 
(and pq the expected number of errors in a word). Thus a good approximation 
to the size of a sphere of radius e = pq in Fq is given by qHq(e/n) for e < &. 
Indeed [8] for any y e Fqq it can be shown1
qH1(P)q - o(q) < sq,q(pq) < qHq(p)n or sq,q(pq) ^ qHq(p)q.
Using this estimate and a random coding argument, the following is shown:
1 Recall that a function f(q) is o(g(q) if for all k > 0 3q0 st Vq > q0 | f (q) | < kg(q).
https://doi.org/10.1017/9781009283403.014 Published online by Cambridge University Press

330
13 List Decoding
Theorem 13.3 ([5], proposition 15), [8], theorem 3.5) For integer L > 2 
and every p e (0,0), 0 = (q — 1 )/q, there exists a family of (n,pn,L) list- 
decodable codes over Fq of rate R satisfying
R> 1—Hq(p)—1/L.
Thus the largest rate a code can have and be (n, pn, L) list decodable, where 
p is the fraction of errors that can be tolerated, for polynomially sized lists, is 
approximately 1 — Hq (p). It is shown ([8], section 3) that codes with rate
R > 1 — Hq(p) > 1 — p — Y
are possible for list size O( 1 /y) and the challenge is to devise the construction 
of codes and decoding algorithms that can achieve these bounds. This is the 
challenge met in Sections 13.2 and 13.3 of this chapter.
Notice that this also implies that a code of rate R can be list decoded 
with a fraction of errors up to 1 — R with list decoding. For this reason this 
quantity can be viewed as the list-decoding capacity. Since 1 — R = (n — k)/n 
this would suggest that on average the number of codeword positions in 
error should be less than (n — k) which would seem to be a requirement 
for correct decoding since any more errors would prohibit the production of 
all information symbols. Thus a fraction of errors in a received codeword 
< (1 — R) is a requirement for successful list decoding. The size of the list is 
an important consideration in this since an exponential size list gives problems 
for practical considerations. Expressions for the size of the list for the various 
codes considered here are generally available but, beyond the fact they will be 
polynomially related to code length, will not be a priority to discuss here.
The next section considers mainly the work of Elias [5] which deals largely 
with the capability of a given binary code to be list decodable for a number 
of errors beyond its unique decoding bound. Said another way, given a binary 
code, as one increases the size of decoding spheres, what can one say about 
the size of decoding lists produced? This work has very much a combinatorial 
flavor although some information-theoretic considerations are included. This 
line of inquiry was not pursued much in subsequent literature and while not 
contributing to the goal of constructing capacity-achieving codes, it remains of 
interest for the principles it introduces.
The remainder of the chapter traces the remarkable work of a few 
researchers, namely Sudan, Guruswami and others, to construct new classes 
of codes that achieve capacity on the list-decoding channel, i.e., to construct 
codes and decoding algorithms capable of correcting a fraction 1 — R errors 
with a code of rate R producing a list of L possible codewords containing the 
transmitted codeword with high probability. A sketch of the incremental steps 
https://doi.org/10.1017/9781009283403.014 Published online by Cambridge University Press

13.1 Combinatorics of List Decoding
331
taken to achieve this is given here. The monograph [7] (based on the author’s 
prize-winning thesis) is a more comprehensive discussion of this subject.
13.1 Combinatorics of List Decoding
The work of Elias [5] treated only binary codes and derived analogs to 
the generalized Hamming and Varshamov-Gilbert bounds for list decoding. 
Interest here is restricted to determining conditions under which a binary C = 
(n,M,d = 2e + 1)2 code can also be an (n,e,L) list decodable for parameters 
e and L. From the previous section, a code C = (n,M,d)2 is an (n,e,L) code 
iff for any y e F2 the decoder produces a list of L possible codewords in any 
sphere of radius e around any received word. Decoding is successful if the 
transmitted codeword is in the list. Equivalently, a code is an (n,e,L) code iff 
no sphere around any y e F^’ of radius e contains more than L codewords.
The Johnson bound of coding gives an upper bound on Aq(n,d), the largest 
number codewords in a (linear or nonlinear) code of length n and distance 
d over Fq . Also Aq (n,d,w) is the same for a constant weight w code (see 
[16], section 2.3). Define A2(n,d,d) to be the maximum number of points 
that may be placed in an e-sphere (sphere of radius e) around any point in 
F2n with all points a distance at least d apart. It follows immediately that any 
(n,M,d)2 code (or an (n,e, 1) list-decodable code) is an (n,e,L = A2(n,d,d)) 
list-decodable code. The following bounds for A2 (n,d,d), obtained by com­
binatorial arguments, are shown in ([5], proposition 10). Their derivations are 
straightforward but intricate and are omitted.
Proposition 13.4
(a) If n > d > 2e + 1, then A2(n,d,d) = 1.
(b) If e > 0 and n > 2e > d > 2e(n — e)/n, then n > 2 and
2 < A2(n,d,d) <
nd
nd — 2 e(n — e)
< max{2,n(n — 1)/2}.
(c) If d is odd, then A2 (n,d,d) = A2 (n + 1,d + 1,e). Therefore for odd d if 
e > 0 and n + 1 > 2e > d + 1 > 2e(n + 1 — e/(n + 1)), then n > 1 and
(n+1)(d+1)
A2(n,d,e) <
(n + 1 )(d + 1) — 2 e(n + 1 — e) 
< max{2,n(n + 1)/2}.
(d) If d is odd and the conditions in the first line of (b) hold, then 
A2(n,d,e) < n.
https://doi.org/10.1017/9781009283403.014 Published online by Cambridge University Press

332
13 List Decoding
Two interesting propositions follow from this one ([5], propositions 11,12). 
To discuss them consider L, pL,p as fixed and let eL = npL,e = np determine 
the sphere radii and let n —> ro. Then:
Proposition 13.5 Let L > 2 and pL and p satisfy
PL = p( 1 - p)L/( 1 - L), 0 < P < 1 /2
P = 1(1 - V - 4PL( 1 - 1 /L)), 0 < PL < L/4(L - 1).
If npL and np are integers (e.g., p = j/k (i.e., rational) andn = ik2 (L - 1)), 
then each (n,nPL, 1) code is also an (n,nP,L) code. More generally for any 
n let
eL(n) =L (n + 1 )PL J, 
e(n) =L (n + 1 )P J.
Then ifa code is (n,eL(n), 1) it is also (n, e(n),L).
The asymptotic version of the proposition ([5], proposition 12), keeping P 
fixed and L increasing reads as follows:
Proposition 13.6 Let p and pro satisfy
Pro = p( 1 - p), 0 < p < 1 //2 
p = 2(1 - V1 - 4pw^, 0 < pro < 1 /4.
If np and npro are integers (e.g., p = i/k,n = ik2), then if a code is an 
(n,npro, 1) code itis also an (n,np, n) code and more generally if for any n
e ro (n) = [ np rol, e(n) = L np J
then ifa code isan (n, ero(n), 1) it is also an (n, e(n), n) code.
An interesting aspect of this approach is to determine the trade-off between 
list size and error-correction capability, keeping in mind that with list decoding 
successful decoding means the transmitted codeword is in the decoded list. 
The culmination of this approach is the following proposition ([5], proposition 
17) which follows directly from the above two propositions. Assume a binary 
code with minimum distance d = 2e1 + 1 which is then an (n,e1, 1) (list 
decodable) code.
Proposition 13.7
(a) LetC be an (n,e1, 1) code and e > e1 + 1. Then C is also an (n,e,L) code 
where
L = (n + 1 )(e 1 + 1)
_e2 - (n + 1 )(e - e 1 - 1)_
ifthe denominator is positive.
https://doi.org/10.1017/9781009283403.014 Published online by Cambridge University Press

13.1 Combinatorics of List Decoding
333
(b) In particular if C is (n,e, 1) it is an (n, (e + 1), [ (n + 1 )/(e + 1) J) 
list-decodable code.
Notice that item (b) of the proposition is obtained by replacing e with e1 + 1 
in part (a).
Numerous examples of this proposition are given in ([5], appendix A), 
including the binary perfect Hamming codes and the (23, 12, 7)2 Golay code. 
In particular the above results show that a (2k - 1, 2k -k, 3)2 Hamming code is 
a (2k - 1, 2,2k-1) list-decodable code. Similarly the (23, 12, 7)2 Golay code is 
a (23, 4, 6) list-decodable code. These two cases can also be obtained by simple 
combinatorial arguments which are outlined in the following example.
Example 13.8 Recall the notion of a t-design, denoted a X - (v,k,t) design. 
It is a collection of k-element subsets (or blocks B) of a v-element set, say 
X, such that every t -element subset of X is contained in exactly X subsets 
of B. Let Xs be the number of subsets containing a given s-element subset, 
s = 1, 2,...,t, Xt = X. Then by elementary counting
(
V - s XH k - s \
. 
(13.1)
t-s t-s
There are numerous conditions known on the parameters for the existence 
of such designs (e.g., see [3]). Of special interest are the Steiner systems 
corresponding to X = 1, often denoted S(t,k, v).
It is known that the codewords of weight 3 in the binary Hamming (2k - 
1,2k -k, 3)2 perfect code form a S(2, 3, 2k - 1) Steiner triple system. A point in 
2k 1
F22 -1 is either a codeword or at distance 1 from a codeword. Spheres of radius 
1 around codewords exhaust the space (i.e., it is a perfect code). Spheres of 
radius 2 around any codeword contain no other codeword. Consider a sphere 
2k 1
of radius 2 around a word of F2 - of weight 1, say u. By Equation 13.1 
exactly
nonzero codewords lie within distance 2 of u. Including the all-zero codeword 
shows the code is a (2k - 1,2, 2k-1) list-decodable code, as shown by the above 
proposition.
Similarly, it is known the codewords of weight 8 in the extended (24, 12, 8)2 
Golay code form a S(5, 8, 12) system, a remarkable combinatorial structure. 
The codewords of weight 8 in the extended (parity check added) code 
correspond to codewords of weight either 7 or 8 in the perfect (23, 12, 7)2 
code. Consider a word v of weight 4 in F223. By Equation 13.1 the number 
https://doi.org/10.1017/9781009283403.014 Published online by Cambridge University Press

334
13 List Decoding
of codewords in the extended code of weight 8 in the sphere of radius 4 
around v is
X 4 = (24-4)/(8-4) = 5.
In the (23, 12, 7)2 code this means, taking into account the all-zero codeword, 
the code is a (23, 4, 6) list-decodable code.
Returning from the combinatorial interlude, it is clear the above proposition 
can be applied to many other classes of codes. For example, the (31,6, 15)2 
BCH code is a (31,7, 1) unique decodable code and also a (31, 8,4) list- 
decodable code.
This combinatorial approach to the list-decoding problem does not seem to 
have been pursued in the literature. The following two sections describe the 
remarkable work on obtaining codes that achieve list-decoding capacity.
13.2 The Sudan and Guruswami-Sudan
Algorithms for RS Codes
The previous section has considered certain aspects of the list decodability of 
known codes. This section and the next are concerned with the problem of 
constructing codes and decoding algorithms that will allow us to obtain codes 
of rate R that are list decodable up to capacity, i.e., a fraction of codeword 
errors of 1 - R. As noted, this can be effective when the fraction of errors 
exceeds the unique decoding bound which for Reed-Solomon (RS) codes 
would be (n — k + 1)/2n ~ (1 - R)/2, half of the capacity.
The story will be how researchers were able to reach the list-decoding 
capacity of 1 - R in a series of incremental steps, often introducing novel and 
ingenious ideas. The progression will yield codes of rates 1 — V2R (Sudan), 
1 —sfR (Guruswami/Sudan and slight improvements by Parvaresh and Vardy) 
and finally 1 - R (Guruswami and Rudra).
Reed-Solomon codes and certain relatives will be considered in the remain­
der of this chapter although much of the material can be extended to more 
general classes of codes such as generalized RS codes, alternant and algebraic 
geometry codes. Letx1,x2,...,xn be n distinct evaluation points in Fq - hence 
n < q - and consider the RS code
C = {f = (f(x 1 ),f(x 2 ),...,f(xn)) | f(x) e F q [ x ], deg f(x) < k — 1}
which defines a linear (n,k,d = n — k + 1)q MDS code. The notions of 
codeword as an n-tuple f = (f1,f2,...,fn), fi = f(xi) over Fq and 
https://doi.org/10.1017/9781009283403.014 Published online by Cambridge University Press

13.2 The Sudan and Guruswami-Sudan Algorithms for RS Codes 
335
codeword polynomial f(x) are often identified. To encode the Fq information 
or message symbols m0,m 1,... ,mk-1 one might use the polynomial f(x) = 
ik=-01 mixi to form the nonsystematic codeword by evaluation of f at n 
distinct points of Fq .
The approaches of this and the next section may appear somewhat chal­
lenging in that it involves the construction of multivariable interpolation 
polynomials with properties such as multiple zeros on given interpolation 
sets. However, the approach is a straightforward generalization of the single­
variable case and the results achieved are of great interest.
As a precursor to introducing the innovative work of Sudan [25] the 
Berlekamp-Welch decoding algorithm for RS codes (e.g., see [6, 21]) is 
recalled. Let C = (n,k,d = n — k + 1 )q be an RS code and denote e = [n-2+1J 
- for convenience assume d is odd and d = 2e + 1. Suppose the codeword f 
is transmitted and j = f + n, yi = fi + ni,i = 1,2, •• -,n,m e Fq received 
where n F Fqn is a noise word. (n is used as n will have another use.) Consider 
the problem of finding two polynomials, D(x) and N(x) over Fq with the 
following properties:
(i) deg D(x) < e,
(ii) deg N(x) < e + k - 1, 
satisfying the conditions:
(iii) N(xi) = yi D(xi), i = 1,2,...,n.
Denote by E the set of (unknown) error positions of the received word, E = 
{i | yi = fi }, and let D(x) be the error locator (monic) polynomial whose 
zeros are at the (as yet unknown) locations of the errors
D(x) = (x - xj).
The actual number of errors, | E|, is assumed to be < e. Let N(x) = f (x)D(x) 
which is of degree at most e + k - 1. Clearly
N(xi) = f(xi)D(xi) = fiD(xi), 
i = 1,2,...,n,
since if xi e E both sides are zero and if xi / E, yi = fi and they are equal 
by definition.
Let
e+k—1 
e
N(xi) = 
nj xij = fi D(xi) = yi 
djxij, i = 1,2,...,n,
j=0 
j=0
since yi = fi, i / E. The number of unknowns ni,dj in this equation is at 
most 2e + k<n(D(x) is monic) and so the solutions for them, given the 
https://doi.org/10.1017/9781009283403.014 Published online by Cambridge University Press

336
13 List Decoding
n values yi , may be found by Gaussian elimination. Note that if t<eerrors 
occur, the appropriate coefficients will be determined as zero. The complexity 
of the decoding is O(n3). Other algorithms have been formulated which have 
lower complexity.
With N(x) and D(x) determined, by definition the codeword polynomial 
is f(x) = N(x)/D(x) assuming they are both nonzero. Suppose the rank 
conditions on the Gaussian elimination are such that more than one pair of 
solutions is obtained. Suppose {N1(x),D1(x)} and {N2(x), D2(x)} are both 
solutions - and hence result in the same codeword. We would like to verify 
this: i.e., that we must have N1(x)/D1(x) = N2(x)/D2(x) or N1 (x)D2(x) = 
N2(x)D1(x). Suppose for some coordinate position yi = 0. Then by property 
(iii) above N1(xi) = N2(xi) = 0 and the condition is satisfied trivially. 
Suppose yi = 0 then
N1(xi)N2(xi) = yi D1(xi)N2(xi) = yi N1(xi)D2(xi)
and hence D1(xi)N2(xi) = D2(xi)N1(xi), i = 1,2,...,n. Since they are 
equal on n>2e + k distinct points they are equal as polynomials and
N1(x)/D1(x) = N2(x)D2(x)
and so multiple solutions to the Gaussian elimination yield the same codeword 
solution.
The approach of [25] was to notice that the above Berlekamp-Welch 
decoding algorithm can be formulated as a bivariate polynomial problem [9] 
as follows. Consider the bivariate polynomial
Q(x,y) = yD(x) - N(x) = (y - f(x))D(x) = (y - f(x) n (x - xi),
deg D < e, deg N < e + k - 1
where E is the set of error positions and D(x),N (x) are as before. Notice that
Q(xi,yi) = 0,i= 1,2,...,n.
Conversely, suppose we could formulate a bivariate polynomial Q(x,y) such 
that Q(xi,yi) = 0,i= 1, 2,...,n. If this polynomial could be factored it 
would have (y - f(x)) as a factor and the code would be decoded. Thus the 
RS decoding problem is cast as a bivariate interpolation problem. This is the 
innovative approach of Sudan [25].
Note that rings of multivariate polynomials such as Fq [x1,x2,...,xn] 
have unique factorization (up to order and multiplication by scalars) [4] and 
factorization algorithms for such polynomials are available in the computer 
science literature.
https://doi.org/10.1017/9781009283403.014 Published online by Cambridge University Press

13.2 The Sudan and Guruswami-Sudan Algorithms for RS Codes 
337
The bounded distance decoder produces the unique codeword closest to the 
received word provided no more than e = [(n-k) j errors have been made. 
The maximum fraction of errors that can be tolerated for unique decoding is 
(1 - R)/2. For list decoding, we would like to be able to list decode up to a 
fraction of 1-R of errors in which case a list of possible codewords is produced 
and that this fraction of (n-k)/n errors is clearly the maximum possible. Thus 
the problem of interest is to construct codes and list decoding algorithms for 
them that are able to “correct” (in the list-decoding context) up to a fraction 
of 1 - R of received symbols in error. Equivalently interest is in finding all 
codewords within a radius e as e expands beyond the unique decoding bound e.
In the sequel it may be helpful to note the equivalence of the following two 
algorithms [13] stated without details:
Algorithm 13.9 (RS list decoding)
Input: A finite field Fq, code C = (n,k,d = n - k + 1)q, 
an n-tuple y e Fq and error parameter e < n
Output: All codewords c e C such that d(y, c) < e
Algorithm 13.10 (Polynomial reconstruction)
Input: Integers k, t and n points {(xi,yC)}n=1 e Fq x Fq
Output: All polynomials f(x) e Fq[x], deg f < k - 1 
and |{ f (xi) = yi,i = 1,2,... ,n] | > t
The polynomial reconstruction algorithm asks for all polynomials f of 
degree less than k for which yi = f(xi) for at least t points and the reduction 
of the first algorithm to the second for t = n — e is clear.
In the remainder of this chapter four algorithms are discussed which, in 
sequence, achieve error fractions for a code of rate R of 1 — V2R, 1 — RR, 
a low-rate improvement of this last bound and finally, 1 — R, the capacity of 
the list-decoding problem. The last two achievements are discussed in the next 
section.
The contribution of Sudan [25] is discussed as it introduced a radically new 
and unexpected way of viewing the decoding problem that altered the mindset 
of many coding theorists. The work utilizes bivariate polynomials Q(x, y) e 
Fq [x, y] and weighted degrees such as
Q(x,y) = 
qijxiyj, qij e Fq
which is a linear sum of the monomials xi yj . The (r, s)-weighted degree of 
this monomial is
ir + js
https://doi.org/10.1017/9781009283403.014 Published online by Cambridge University Press

338
13 List Decoding
and the weighted degree of the polynomial Q(x, y) is the maximum of the 
weighted degrees of its nonzero monomials. For the RS code of interest we 
will only be interested in the (1,k - 1)-weighted degree of Q(x,y) and the 
reason for this is in the degree of the univariate polynomial Q(x,f (x)) when 
a polynomial of degree at most k - 1 is substituted for the y variable. This 
corresponds to the RS code (n,k,d = n - k + 1)q code where, as before, 
codewords are identified with polynomials over Fq of degree at mostk- 1. For 
the remainder of the chapter denote k1 = k - 1. Recall the RS code coordinate 
positions are labeled with the distinct xi e Fq, i = 1,2,... ,n and the received 
word isy = (y1,y2,...,yn).
Two simple arguments will be of importance in the discussion of this and 
the next section:
(i) the fact that a bivariate (or multivariate) polynomial Q(x,y) will exist if 
the number of its coefficients is greater than the number of homogeneous 
conditions placed on these coefficients; and
(ii) the fact that if the number of zeros of the univariate polynomial
Q(x, f (x)), deg f<kexceeds its degree, it must be the zero polynomial.
Of course the degree of Q(x, f (x)) is the (1,k1)-weighted degree of Q(x, y) 
for deg f<k. In spite of the inefficiency, the argument will be repeated for 
each case.
The essence of the Sudan list-decoding algorithm for RS codes is estab­
lished in the following:
Algorithm 13.11 (Sudan decoding of RS codes)
Input: 
n,k,t e Z, {(xi,yi)}^ e Fq x Fq
Integer parameters i,m - to be determined for optimization
Output: 1. A bivariate function Q(x, y) e Fq [x,y] with the properties: 
Q(xi,yi) has (1,k 1)-weighted degree at most m + tk 1, 
0 < m < I, and is such that Q(xi,yi) = 0, i = 1,2,.. .,n 
and is not identically zero
2. Find the irreducible factors of Q(x,y)
3. Output all those irreducible factors of the form (y - f (x)) 
for which it is true that yi = f(xi) for at least t 
of the points (xi,yi).
The algorithm is straightforward although there are a number of variables 
that must be kept track of. The approach of [25] is outlined. Consider the 
bivariate polynomial
£ m+(£ - j)k 1
Q(x, y) = 
qhjxh yj
https://doi.org/10.1017/9781009283403.014 Published online by Cambridge University Press

13.2 The Sudan and Guruswami-Sudan Algorithms for RS Codes 
339
t + 1
2
with a maximum y degree of £ and maximum weighted degree of m + Ik 1. 
Notice the maximum x-degree is m + £k 1. The total number of coefficients in 
such a polynomial is
Y+m + (£ - j)k 1 + 1) = (m + 1 )(£. + 1) + k 1 
j=0
To ensure we can solve for the coefficients of such a nonzero bivariate 
polynomial we need this quantity to be greater than n since we are requiring 
the n homogeneous conditions that
Q(xi,yi) = 0,i= 1,2,...,n, 
Condition (*) 
(13.2)
and for a solution it is necessary to have more variables than conditions, i.e., 
one can set up an n x (m + Ik 1) homogeneous matrix equation which will have 
nonzero solutions iff m + £k 1 > n. In determining the polynomial Q(x, y) the 
Condition (*) can be satisfied using Gaussian elimination of complexity at 
most O(n3).
Notice that n, k are fixed parameters and we want to determine the param­
eters t, m and I to determine the bivariate polynomial Q(x, y) with the above 
properties to find all polynomials f(x) that correspond to codewords in the 
RS code of distance at most n - t from the received word y for as small a 
value of t as possible, given the other parameters. The set of such polynomials 
correspond to the decoded list.
It seems remarkable that we will be able to show that if f (x) corresponds to 
any codeword (deg f<k) whose evaluations agree with the n received values 
in at least t places, then if
m + Ik 1 < t
then (y - f(x)) must appear as an irreducible factor of Q(x, y). Consider the 
polynomial
p(x) = Q(x,f (x))
which is a univariate polynomial of degree in x at most m + Ik 1 since 
the degree of f(x) is at most k1. In the received codeword places that are 
correct yi = f(xi ) = fi and for at least t values of xi by construction, 
Q(xi,f(xi')') = 0 for these t places. Recall yi = fi + ni, i = 1,2,... ,n 
and it is assumed ni = 0 for at least t values. If the degree of this univariate 
polynomial is at most m+Ik 1 <t, as assumed, the polynomial has more zeroes 
than its degree and hence Q(x,f (x)) must be the zero polynomial (although 
Q(x,y) is a nonzero polynomial). Viewing Q(x, y) as a polynomial in y with 
coefficients from the rational field K of Fq [x] (field of fractions), Q(x, y) = 
https://doi.org/10.1017/9781009283403.014 Published online by Cambridge University Press

340
13 List Decoding
Qx(y) = 12i hi(x)yl,hi(x) e K, the polynomial can be factored over K. If it 
has a zero, say n e K, then (y - n) | Qx(y) .Hence (y - f(x)) | Q(x,y), since 
the form of Q has unit denominator. The ring Fq [x, y] has unique factorization 
since if R is a ring with unique factorization (unique factorization domain 
(UFD)), then so is the polynomial ring R[x]. This argument holds for any 
polynomial satisfying the stated conditions and thus produces all possible 
codewords within the stated distance from the received word. This number 
cannot exceed £, the y-degree of Q(x,y). It is possible that a factor obtained 
from the process outlined produces a polynomial for which f(xi ) = ri for 
fewer than t values in which case it is discarded.
The algorithm requires the factorization of the bivariate polynomial 
Q(x,y). Efficient means to achieve this for univariate polynomials have 
been well explored and algorithms to extend these to bivariate polynomials are 
available [18] and are not considered here.
For the above argument leading to a codeword polynomial to be valid we 
need the conditions:
m + £k 1 < t and (m + 1 )(£ + 1) + k 1 ( 
j > n.
Condition (**)
(13.3)
The first relation will be used to conclude that a polynomial in a single 
variable with degree m + £k 1 < t and t zeros, must be the zero polynomial. 
The second ensures that a nonzero bivariate polynomial with more than n 
coefficients (monomials) satisfying n homogeneous conditions, exists - as 
noted previously.
We seek to “optimize” the parameters by choosing favorable values of £, t 
and m. For a given value of £ the second condition above gives
m>
n + 1 - k 1(£+1) 
£ + 1
-1
(13.4)
and so by the first condition
t > m + £k 1 >
n + 1 - k 1 (£+1) 
£ + 1
- 1 + £k 1 + 1 = n + 1 
k 1 £
£ +1 + T.
We would like the algorithm to work for as few agreements (t ) between the 
received word and codewords as possible (to be able to “correct” as many 
errors (n - t ) as possible). By differentiating the right-hand side of this last 
equation the optimum value of £ is found as
£ = / 2^+D - 1.
https://doi.org/10.1017/9781009283403.014 Published online by Cambridge University Press

13.2 The Sudan and Guruswami-Sudan Algorithms for RS Codes 
341
and from Equation 13.4
m> n +
7+T
k 1£ 
k i n + 1 
Ik i n + 1 
k i 
k
— ~ — y 2 y 2 
+ T - = T - .
To summarize, for the parameter values
m=
k1
and £ =
2 (n + 1) 
k 1
2
i
one can check that the conditions of Equation 13.3 are satisfied and hence the 
algorithm will find all codewords within distance n - t to y . From the above 
equations, the corresponding value of t to these values is
t > ^2(n + 1 )k 1 - k1 — 1. 
(13.5)
The result of the above discussion ([25], theorem 5) is that the Algorithm 
13.18 will find all such polynomials that agree with the received word 
(polynomial) in at least t positions as long as the conditions (*) and (**) are 
met for the parameters chosen.
Rather than the optimum values of the parameters, if one chooses instead 
the approximations, then
t _ [n — 1 and m = k1
k1 
2 
2
it is easy to verify (ignoring integrality concerns which can be taken into 
account) the conditions (**) are satisfied as long as t is chosen so that
t > m + k 1 £ = — +
= 5/ 2 nk 1.
(13.6)
Thus, asymptotically, the normalized number of errors that can be tolerated by 
this Sudan algorithm is ftSD = (n — t)/n satisfies
t
Psd < 1 — n
1—
n
(13.7)
«
It follows that Algorithm 13.18 can correct an RS code received codeword with 
a fraction of errors at most 1 — V2R with a list of codewords within a sphere 
of radius at most n — t around the received word of size as noted above. The 
list size will be at most £ ^ ^2n/k ^ V2/R since this is the y-degree of the 
bivariate polynomial and there can be no more factors of the required form.
https://doi.org/10.1017/9781009283403.014 Published online by Cambridge University Press

342
13 List Decoding
It is of interest to note the improvement of the error fraction obtained 
compared to unique decoding which is able to correct a fraction
n-k+1
Pud =-------------- > (1 - R)/2. 
(13.8)
n
It is verified that the smallest value of Rfor which this expression (Equation 
13.7) equals (1 - R)/2 is about R = 0.17. Thus for code rates below 0.17 
the list-decoding algorithm, for some size list, will correct a greater fraction 
of errors than unique decoding at the cost of producing a list of possible 
transmitted codewords rather than a unique word.
The next innovation to improve the situation is that of Guruswami and 
Sudan [13] who modified the algorithm to allow for higher-rate codes. The 
key to this improvement is to allow higher-total degree bivariate polynomials 
and for each zero to have a higher order multiplicity than one. In essence the 
curve represented by the bivariate polynomial will intersect itself at a zero of 
multiplicity s , s times. The improvement will develop an algorithm that will 
produce a list of polynomials satisfying the distance criterion that will be able 
to tolerate a fraction of errors up to 1 - Rr rather than 1 - V2R as above. 
While it is not immediately intuitive why having higher-order singularities will 
lead to improvements, the development is of interest. The issue is discussed in 
[9] and [17] (chapter of Kopparty). While somewhat technical, the ideas are 
straightforward extensions of previous material.
Suppose the bivariate polynomial
P(x,y) = 
pi, jxi yj
has a zero of order 5 at the point (a,b) e Fq x Fq. Consider the translated 
polynomial
t 
i 
j 
t ij
P (x, y) = P(x+a, y +b) = pi,j (x +a) (y +b) = pi,jx y
and it is easy to show [13] that the coefficients of the translated polynomial are 
p.j='ll e( ")(j') p-^i-j j.
For P (x,y) to have a zero of order 5 at the point (a, b) it can be shown that it is 
necessary and sufficient that the translated polynomial P t (x,y) = P(x+a,y+ 
b) have the coefficients of all monomials x'yj to be zero for all i > 0,j > 0 
https://doi.org/10.1017/9781009283403.014 Published online by Cambridge University Press

13.2 The Sudan and Guruswami-Sudan Algorithms for RS Codes 
343
and i + j < s. The number of possible monomials of the form xi yv, u + v < s 
is s+21 (as shown in Chapter 1). Hence the total number of conditions such a 
requirement imposes on the coefficients of P(x,y) to have zeros of order s at 
each of the points (xi,yi), i = 1, 2,...,nis
s+1
n
2
(13.9)
This is the number of monomials of degree <sand hence is the number of 
equations relating the linear sums of coefficients of P (x,y) to zero, set up as a 
matrix equation.
With these definitions the procedure for list decoding of RS codes is 
discussed, repeating the previous discussion with zeros of multiplicity s rather 
than s = 1. As before suppose f = (f1,f2,...,fn) is a codeword in the 
(n,k,n-k1)q RS code corresponding to the polynomial f (x), fi = f(xi), i = 
1,2,... ,n, and j = (y 1 ,y 2,. ..,yn) the received word in F q, j = f + n• Denote 
the set of pairs
I = | (x 1 ,y 1 ),(x 2 ,y2),..., (x«,y«)}, (xi,yi) e F q2 V i 
as the interpolation set and let
t = I {i | yi = f(xi), i = 1,2, ... ,q I
be the number of agreements between the codeword f and received word y. 
Define the bivariate polynomial Q(x, y) where
I d - j)k 1
Q(x,y) = 
qi,hxhyj
j=0 h=0
where £ is the y-degree of Q(x,y) and all monomials have (1,k 1 = k - 1)- 
weighted degree at most tk 1. Notice the slight difference from the previous 
case of Sudan decoding (no m - the argument is easily changed to allow a total 
weighted degree of m+tk 1 as before). The number of coefficients of the above 
bivariate polynomial is computed as
(I + 1) + k J £ ++J. 
(13.10)
From the above discussion such a nonzero bivariate polynomial will exist and 
have zeros of order s on all points of the interpolation set if 
(£ + 1) + k 1
■£ + 1
2
>n s+1
2
https://doi.org/10.1017/9781009283403.014 Published online by Cambridge University Press

344
13 List Decoding
i.e., if the number of coefficients exceeds the number of homogeneous 
conditions. Furthermore if f(x') e Fq [x] is of degree at most k 1, then the 
univariate polynomial Q(x,f(x}) is of degree at most tk 1. If the number 
of agreements yi = fi is at least t (hence n - t or fewer errors in the 
received word), then Q(x, f (x)) has st zeros since each zero is of order 
5. If st > tk 1 (i.e., the number of zeros exceeds the weighted degree of 
Q(x, y)) it must be the zero polynomial. The crucial lemma that will verify 
the Guruswami-Sudan list-decoding algorithm then is (adapted from [12] 
and [13]):
Lemma 13.12 With the notations above, let Q(x, y) e Fq [x,y] be a bivariate 
polynomial such that for each i = 1, 2,...,n the pair (xi,yi) isazeroof 
multiplicity s and Q(x,y) has (1,k 1 )-weighted degree at most D = tk 1. Let 
f(x) e Fq[x] be of degree at most k1 with the property that yi = f(xi) for at 
least t values of i e [n]. If the following conditions (Equations 13.9 and 13.10) 
are met:
(i) (£ + 1) + k 1 
> w^ "2 and (ii) ts > Ik 1,
then (y - f(x)) divides Q(x, y) for any codeword polynomial within distance 
n - t of the received word y.
Proof: Consider the polynomial g(x) = Q(x, f (x)) which is of degree at 
most D = tk 1 as the (1,k 1)-weighted degree of Q(x,y) is at most D. By 
definition, since (xi,yi) is a zero of multiplicity s of Q(x,y) it follows that 
Q(x + xi,y + yi) has no monomial terms of degree less than s. Consider a 
value of i for which yi = f(xi) = fi and note that x | f(x + xi) - yi 
and for such a value of i, xs | Q(x + xi,y + yi) and hence (x - xi)s | 
Q(x,y) and this is true for t values of xi which are distinct. Hence the 
number of zeros is at least st and since by assumption st > tk 1 the 
implication is that Q(x,y) is the zero polynomial. As in a previous argument, 
writing
Q(x,y) = Qx (y) = 
qi (x)yi, qi(x) e Fq(x)
where the coefficients are in the rational field Fq (x), the polynomial has a zero 
in Fq(x), say p and (y - p) | Q(x,y) or if p = f(x), (y - f(x)) | Q(x,y). ■
Notice that the algorithm finds all the polynomials (codewords) within the 
required distance n - t . This result leads to the algorithm :
https://doi.org/10.1017/9781009283403.014 Published online by Cambridge University Press

13.2 The Sudan and Guruswami-Sudan Algorithms for RS Codes 
345
Algorithm 13.13 (Guruswami-Sudan decoding of RS codes)
Input: 
n,k,t e Z, {(xi,yi)}^ e Fq x Fq
Output: 
All polynomials f(x\ deg f < k 1, within distance n — t of j
Output: 1. Compute parameters 5,1 and k 1 satisfying the inequalities
st > tk 1 and ns(s + 1)/2 < (£ + 1) + k 1(€+1)
2. 
A nonzero bivariate function Q(x,y) over Fq of weighted 
degree at most tk 1 with the property that (xi,yi) is a zero 
of multiplicity s,i = 1,2,...,n, i.e., for each
i e [n] Q(x + xi,y + yi) has no monomials 
of degree less than s
3. 
Find the irreducible factors of Q(x,y)
4. 
Output all those irreducible factors of the form (y — f (x)) 
for which it is true that yi = f(xi) for at least t of 
the pairs of points.
Any set of parameters n,k, s, t and t which satisfy the conditions of Lemma 
13.12 will lead to the RS code list-decoding algorithm to find all codewords of 
distance less than (n —t)to the received word. The following set are suggested 
(adapted from Kopparty [17]). For the given RS (n,k,n — k1)q, k1 = k — 1 
code let
t =
s= k, t = nk
and assume the fractions in the arguments of the floor/ceiling functions are not 
integral. Then the first condition of Lemma 13.12 gives
(t+1) + k 1
nk 3 
k + 1
---- > n •
2 k 1 
2
k(k + 1) 
2
'€ + 1
2
k 1t2 
k 1 k2 • nk
~ > 7 ^ k2
= n •
and the last inequality follows from observing that k3/k1 >k(k+ 1) or k3 > 
k(k2 — 1).
The second condition for the parameters gives
ts = Vnk"| k > tk 1 =
k 1 ~ k\/nk
where the inequality relies on the floor function and noninteger values of its 
argument. Thus the maximum fraction of errors in an RS code of rate R that 
can be corrected by the Guruswami-Sudan algorithm is
Pgsd = 1------- > 1
n
R with list size t ^ WR.
(13.11)
https://doi.org/10.1017/9781009283403.014 Published online by Cambridge University Press

346
13 List Decoding
The parameter set used for these calculations leads to the following (note the 
y-degree of Q(x,y) is £ which is also the maximum number of solutions 
obtainable, the maximum size of the list obtained):
Theorem 13.14 For every RS (n,k,d)q code there is a list decoding algorithm 
that corrects up to n — Rkn errors (fractional rate of 1 - Rr) that runs in 
time polynomial in n and q and outputs a list of at most Rnk polynomials.
Another set of parameters is used in [13] with the same result. This is 
a considerable improvement over 1 — R2R result [25] and the decoding 
algorithm applies also to certain classes of algebraic-geometric codes as well 
as generalized RS codes. It still falls short of the capacity bound 1 — R of the 
previous section, a problem that is addressed in the next section by considering 
the construction of certain classes of codes (rather than only RS codes).
13.3 On the Construction of Capacity-Achieving Codes
The elegant results of Guruswami and Sudan of the previous section gave the 
fraction of errors tolerable in list decoders of 1 —RR which was the best known 
until the work of Parvaresh and Vardy [22]. The key idea of that work was 
to introduce the notion of multivariate interpolation (again with multiplicities 
of zeros) and to use codes other than RS codes (although they were later 
shown to be related to RS codes (see [12])). The work is interesting from a 
technical view and an outline of this work is discussed here. The work led 
to the more complete results of Guruswami and Rudra [15] and the rather 
simple yet effective notion of folded Reed-Solomon (FRS) codes. Our aim is a 
description of these important contributions rather than a detailed analysis for 
which the reader is referred to the original papers. Numerous improvements 
in the efficiencies of the various original algorithms have been proposed. The 
work is strongly related to the decoding of interleaved Reed-Solomon codes 
as represented by such works as [1, 2, 24, 27].
While the general theory to be considered is valid for M variable polyno­
mials over a field, the essence of the results can be appreciated by considering 
only the trivariate (three variable, M = 3) case which simplifies the situation 
considerably without compromising the power of the theory. Only the trivariate 
case will be considered in this section with the occasional reference to the M 
variable case.
Before embarking on the coding application some simple properties of 
trivariate polynomials are considered, generalizing those of the bivariate case 
of the previous section.
https://doi.org/10.1017/9781009283403.014 Published online by Cambridge University Press

13.3 On the Construction of Capacity-Achieving Codes
347
As before, consider the interpolation set
I = {(x 1 ,y 1 ,z 1 ),(x2,y2,z2),... ,(xn,yn,zn), Xi,yi,zi e F3} 
(13.12)
where the xi are distinct. A polynomial will be a linear sum over Fq of 
monomials of the form xi yj1 zj2 and the degree of such a monomial will be 
i +j1 +j2. As before, we will be interested in the (1,k1,k1)-weighted degree of 
such a monomial i+j1k1+j2k1. (Recall that we will be interested in the degree 
of the univariate polynomial after substituting polynomials of degree at most k1 
for the variables y and z, relating to an RS code (n,k,d = n-k+1 = n-k1)q.) 
The weighted degree of a trivariate polynomial is the largest weighted degree 
of any monomial it contains.
For the treatment of the M-variable case it is useful to define a lexicographic 
ordering or lex ordering [4], on monomials and this notion is described. Denote 
M variables x1,x2,...,xM simply by x and the exponents a1,a2,...,aM by 
a where ai e Z>0 and a e (Z>0)M. The monomial x^1 x22 • • • xa
aM is then 
denoted Xa. The lex ordering on the integer M-tuples a, b e (Z>0)M is then 
that a > b if in the vector difference a — b e ZM the leftmost nonzero 
entry is positive. This ordering induces a lex ordering on monomials via their 
exponents, i.e., Xa > xb ■& a > b. Notice that the definition implies that 
xi > xj ■&■ i < j, i, j e Z.
From the definition of zeros of multivariate polynomials discussed in 
Appendix B, a polynomial P (x, y, z) e Fq [x, y,z] has a zero of multiplicity 
5 at the point (a,ft,y) if the translated polynomial P(x + a,y + fi,z + y) has 
no monomials of degree less than s and at least one such monomial of degree s.
The extension to zeros of general multivariable polynomials is clear. The 
number of monomials of degree less than 5 and weighted (1,k1,k1) degree at 
most D will be of interest.
The number of monomials of degree less than 5 is
| {a + b + c < s,a,b,c e Z>o} |.
As noted in Chapter 1 in the discussion of RM codes, this number is
(s + 2^
3.
and the total number of conditions on the coefficients of the monomials of 
degree less than s in the n coordinates is n times this number.
The total number of monomials of (1,k1,k1)-weighted degree at most D is 
slightly more involved. Let D = m + tk 1 with 0 < m < t. The number of 
weighted terms of the polynomial
P (x,y,z) = 
pi,j,hxiyj zh
https://doi.org/10.1017/9781009283403.014 Published online by Cambridge University Press

348
13 List Decoding
or equivalently the number of positive integer triples (i,j,h) : i + jk 1 + hk 1 < 
D is needed. Clearly j + h < £ or the bound D would be violated. Let j + h = 
u, then the exponent of the variable x can range over [0,m + 1 + (I — u)k 1] 
and hence the enumeration is
1+1 + 1 + (I — u)k 1 )(u + 1) = (m + 1 )K ++ J + k J 3 J (13.13) 
u=0
where use has been made of the relation Xf=1 i2 = ^(^ + 1)(2+ 1)/6. The 
second term is the dominant one in this expression and note that using the 
approximation D ~ tk 1
k 
+ 2) ~ k 1 ~ ~
36
D 3
6 k2
(13.14)
For an approximation to this expression, consider the three-dimensional 
figure formed by the three axes planes, x > 0,y > 0,z > 0 and the condition 
x + yk1 + zk1 < D, a form of skewed pyramid P in three dimensions as 
shown in Figure 13.1. Let N3(D) be the number of such weighted monomials. 
To each valid weighted triple (a,b,c) associate a cube of volume unity, rooted 
at (a,b,c). Then it is argued the volume of the set of all such cubes exceeds 
the volume of P. But the volume of P is easily evaluated by noting that a 
horizontal plane at height x through the figure intersects the figure in a triangle 
of area (D — x)2/2k12 and integrating this from x = 0tox = D gives
—( ( (D — x)2dx = D3/6k2
2k12 0 
1
which is also the approximation of Equation 13.14. This serves as a good and 
tractable lower bound to the number of monomials of interest. For analytical 
Figure 13.1 Pyramidal figure to approximate N3 (D)
https://doi.org/10.1017/9781009283403.014 Published online by Cambridge University Press

13.3 On the Construction of Capacity-Achieving Codes
349
purposes the approximation is easier to use than the exact expression above - 
although the exact expression is needed for some purposes.
As with the Guruswami-Sudan case, a nonzero trivariate polynomial 
Q(x, y, z) is sought with zeros of order s at each of the n points of the 
interpolation set I of Equation 13.12. As before this requires that at each point 
of the set I the coefficients of each monomial of weighted (1,k 1 ,k 1) degree 
<sbe zero giving a total number of conditions of n s+32 .
In order for the required polynomial to exist it is necessary that the 
number of unknown coefficients in Qs (x,y,z) (the number of weighted 
(1,k1,k1) monomials) exceed the number of conditions (the total number of 
monomials of (ordinary) degree less than s which evaluate to 0 in the translated 
polynomials). Hence we require (using the approximation to N3 (D)) that
D3
N3 (D) = 
2 > n
6k12
s(s + 1 )(s + 2) 
6
(13.15)
Thus [22] the weighted degree of a polynomial that has zeros of degree s at 
each of the n points of the interpolation set I is at least
D >
^InkSS+vyS+i)
(13.16)
As long as the number of unknowns (coefficients of the monomials - as given 
by the left-hand side of Equation 13.15) is greater than this number, there will 
exist a nonzero polynomial Q(x,y,z) with zeros of degree s at each of the n 
interpolation points of I of Equation 13.12. The importance of the (1,k1,k1)- 
weighted degree will be that when polynomials f(x) and g(x) of degree at 
most k1 are substituted for the variables y and z, the univariate polynomial 
Q(x, f (x),g(x)) will be of degree at most D. Recall the similarity with the 
previous Guruswami-Sudan case.
The Parvaresh-Vardy Codes
With this background, an informal description of the Parvaresh-Vardy list­
decoding algorithm is given. For the details and proof the reader is referred to 
[22]. The encoding process is first described. Let f(x'),g(x) e Fq[x] be two 
polynomials over Fq of degree at most k1 . The polynomial f(x) is thought of 
as the “information polynomial,” i.e., its coefficients carry the k information 
symbols over Fq . The polynomial g(x) will be derivable from f(x) as
g(x) = f (x)a mod e(x)
where e(x) is an arbitrary irreducible polynomial over Fq of degree k 
for a sufficiently large positive integer a. The value of a suggested ([22], 
https://doi.org/10.1017/9781009283403.014 Published online by Cambridge University Press

350
13 List Decoding
equation 13) is f(sD + 1 )/k 1]. Thus g(x) represents redundant information 
which decreases the rate of the code but assists in the decoding process.
Let a e Fq2 be such that {1,a} is a basis for Fq2 over Fq. For the n 
distinct points of xi of Fq of the interpolation set I let the codeword over 
Fq2 corresponding to the polynomial f(x)be c = (c1,c2,...,cn) where
ci = f(xi) + ag(xi), i = 1,2,...,n
where g(x) is as in the previous equation. This is a subset of a Reed-Solomon 
code C = (n,k,n - k + 1)q2 over Fq2 of length n dimension k over Fq2 and 
minimum distance n -k+ 1. To see this note that while both f (x) and g(x) are 
polynomials of degree at most k1 over Fq, the polynomial f (x)+ag(x) is also 
polynomial of degree at most k1 over Fq2 and hence in the RS code. However, 
not all such polynomials are obtained with this construction and hence the set 
of such codewords is a subset of the code. It is generally not a linear code. 
Recall that the xi are distinct elements of Fq - hence n < q — 1 although 
the code is over Fq2 . However, the rate of the subset C (nonlinear) code is 
R = logq2 | C| /n = k/2n as each coordinate position of the codeword
c = (c1,c2,...,cn) = (f (x1) + ag(x1),f (x2) + ag(x2),...,f (xn) + ag(xn))
is an element of Fq2 .
It is convenient to designate the received word as a vector v = 
(v1,v2,...,vn), an n-tuple over Fq2 . It is easily decomposed to two n-tuples 
over Fq via
vi = yi + azi,i= 1,2,...,n.
Given two polynomials f (x), g(x) e Fq[x] of degree at most k1, used for 
encoding a word, for the interpolation set I define an agreement parameter: 
t = |{i : f (xi) = yi and g(xi) = zj |.
This will later be the number of correct code positions corresponding to a 
codeword generated by f(x) compared to a received word vi = yi + azi . 
(Notice: This definition of t is in contradiction to that of [22], which uses t for 
the number of error positions (disagreements), but conforms to our earlier use 
and later definitions.)
As before let Q(x,y,z) be the interpolation polynomial corresponding to the 
interpolation set I, of weighted (1,k1,k1) degree D given by Equation 13.16. 
It has zeros of degree s at each of the n points of I. Define the univariate 
polynomial
p(x) = Q(x,f(x),g(x)).
By definition, if t is the agreement parameter for these polynomials f,g, the 
number of zeros of p(x) evaluated on a received word, is at least st, counting 
https://doi.org/10.1017/9781009283403.014 Published online by Cambridge University Press

13.3 On the Construction of Capacity-Achieving Codes
351
multiplicities of the t zeros. From the definition of the weighted degree of 
Q(x,y, z), the degree of the univariate polynomial p(x) is at most D bounded 
by the quantity in Equation 13.16 and
t > 1 i T3n(k1)2S(s+l)Cs+2) + 11 , k 1 = k - 1
> j x n(k 1 )2( 1 ■ 1)( 1 ■ 2 ) + i
(13.17)
In this case the weighted degree of p(x) = Q(x, f (x),g(x)) will be strictly 
less than st and the number of zeros of the polynomial is strictly greater than 
its degree and hence p(x) is the zero polynomial.
In other words, if a given received n-tuple v = (v 1 ,v 2,..., vn) e F n2 
q 
differs from a codeword generated by the polynomials f (x),g(x) in at most
e positions where
e = n — t <
n — 3 n(k — 1)2 f 1 +— ) (1 +— )----
s ss
(13.18)
then p(x) = Q(x, f (x),g(x)) will have at least t zeros indicating positions 
where the given n-tuple and codeword agree.
A problem remains to actually determine the polynomials f(x) and 
g(x) from the interpolation polynomial Q(x, y, z). This is slightly technical 
although not difficult. An overview of the technique is given from [22].
Recall the problem is to determine all codewords in a given code over Fq2 
at distance at most n — t from a received word v e Fqn2 whose i-th position is 
vi = yi + azi. From this information the interpolation polynomial Qs(x,y,z) 
is found which has zeros of degree s on the interpolation set I and from this 
trivariate polynomial we are to determine a set of at most L codewords.
It is first noted ([22], lemma 5) that a polynomial Q(x, y,z) e Fq [x, y,z] 
satisfies
Q(x,f(x),g(x)) = 0
if and only if it belongs to the ideal in the ring Fq [x, y, z] generated by the 
elements y — f(x) and z — g(x), denoted by (y — f(x)} and (z — g(x)}, 
respectively, i.e., iff there exists polynomials a(x, y, z), b(x, y, z) such that
Q(x,y,z) = a(x,y,z)(y — f(x))+ b(x,y,z)(z — g(x)).
In this setting an ideal is a subset of the ring Fq [x, y, z] which is closed under 
addition and multiplication by elements of Fq [x, y,z].
It will be convenient to consider the field
K = F q [ x ]/(e(x))
https://doi.org/10.1017/9781009283403.014 Published online by Cambridge University Press

352
13 List Decoding
for the irreducible polynomial e(x) of degree k over Fq, introduced earlier. The 
elements of K can be viewed as polynomials in x of degree less than k with 
multiplication in K taken modulo e(x) and |K|= qk .
Clearly any trivariate polynomial such as Q(x,y,z) can be written as
Q(x,y,z) = EE qij^y'zj, qij(x) e Fq[x]
and if the polynomials are taken modulo e(x) so that qj(x) = qij(x) 
mod e(x), then define
P(y,z) = EE q'ij(x)ylzj e K[y,z].
It is not hard to establish that P(y,z) is not the zero polynomial and that if p 
and y = Pa correspond to the polynomials f(x) and g(x) = f(x')a in K and 
([22], lemmas 7 and 8) if Q(x, f(x),g(x)) = 0, then
P(P,Pa) = 0.
This implies that p = f(x) e K is a zero of the univariate polynomial
H(y) = P(y,ya).
Thus finding the roots of H(y) will yield a polynomial f(x) corresponding to 
a codeword. Finding roots of univariate polynomials is a standard problem of 
finite fields for which there are many efficient algorithms. The algorithm then 
is as follows:
Algorithm 13.15 (Parvaresh-Vardy decoding of RS codes - trivariate 
case)
Input: 
n,k, t e Z,
received word v = (vj = yj + azj)n 
irreducible polynomial e(x) e Fq [x], degree k, 
positive integer a
Output: 
A list of codewords that agree with word v in at least t places
1. 
Compute parameters 5 and D satisfying the inequalities
st > D and n s+22 < N3(D) (Equation 13.15 )
2. 
Compute the interpolation polynomial Q(x,y,z) over Fq
of weighted (1,k1,k1) degree at most D 
with the property that (xi,yi,zi) e I is a zero of 
multiplicity 5, i = 1 , 2, . . .,n
3. 
Compute the polynomial
P(y,z) = Q(x,y,z) mod e(x) e K[y,z]
4. 
Compute the univariate polynomial H(y) = P (y,ya)
5. 
Compute the roots of H(y) in K as a polynomial 
f(x) (and ya = g(x)) for which it is true that 
vi = f(xi) + ag(xi ) for at least t of the points of I.
https://doi.org/10.1017/9781009283403.014 Published online by Cambridge University Press

13.3 On the Construction of Capacity-Achieving Codes
353
The procedure outlined leads to the result ([22], theorem 10) that for a given
received word v e F n2 and code rate k/2n and zero parameter 5, the algorithm 
q
outputs all codewords that differ from v in at most
e=
n - n74R2( 1 + 1 )( 1 + 2 ) - 1 
5 
55
coordinate positions and the size of the list of outputs is at most L2 where
L=
2R
The factor of 4 in the equation for e arises from the rate of the code being 
R = k/2n. Thus asymptotically for this trivariate case, the code of rate R 
constructed is able to correct a fraction of errors of
1 - 34R2 ~ 1 - (2R)2/3.
The trivariate case discussed above contains all the ingredients for the 
general case which is briefly described. For the M-variate case (M = 2for 
the trivariate case), the encoding process involves choosing a sequence of 
suitable integers ai,i = 1,2,...,M- 1 and from the information polynomial 
f(x) e Fq [x] generate polynomials
gi(x) = f(x)a mod e(x), i = 1,2,...,M - 1
where, as before, e(x) is an irreducible polynomial over Fq and the correspond­
ing codeword to f(x) is then
M-1
c = (c 1 ,c2,.. .,Cn), where cj = f(xj) + 
aigi(xj), j = 1,2,... ,n
i=1
where {1,a 1 ,a2,... ,aM-1} is a basis of FqM over Fq. The rate of the code is 
then R = k/Mn. The addition of these related polynomials gi (x) decreases 
the code rate but improves the decoding procedure.
The remainder of the argument for the general multivariate case mirrors that 
of the trivariate case. The interpolation polynomial used is of the form
Q5(x,f (x),g1(x),...,gM-1(x)) = 0
with each zero of multiplicity 5. Proceeding as in the trivariate case, the general 
case yields the result:
Theorem 13.16 ([22], Theorem 1) Let q be a power of a prime. Then for all 
positive integers m,M,n < q and k < n there is a code C of length n and rate 
R = k/nM over FqM equipped with an encoder E and a decoder D that have 
https://doi.org/10.1017/9781009283403.014 Published online by Cambridge University Press

354
13 List Decoding
the following properties. Given any vector v e FqM the decoder outputs the 
list of all codewords that differ from v in at most
e = - - M+1+1MMR^ 1 + 1)...(1 + M^ - 1
positions. The size of the list is at most
L=
M +1
m
MR
M
+ o(1).
Moreover both the encoder and decoder run in time (Fq operations) bounded 
by a polynomial in a and s for every given M > 1.
Approximating the expression in the theorem for the fraction of errors 
tolerated by the algorithm to produce the decoding list yields:
Ppvd ^ 1 - M+/MMRM = 1 - (MR)M/M+1.
This fraction of errors achievable for decoding improves on the Guruswami- 
Sudan bound of 1 - RR only for rates less than 1 /16 which, while it 
represented very significant progress, the general problem of achieving the full 
range of 1 - R remained open.
The Guruswami-Rudra Codes
The seminal work [11] introduced the notion of FRS codes and was successful 
in showing that, using a multivariable interpolation approach, with multiplicity 
of zeros, such codes could achieve list decoding in polynomial time up to a 
fraction of 1 - R -e of adversarial (worst-case) errors. That work also considers 
the relationship between the Parvaresh-Vardy codes and the FRS codes. Rather 
than give an outline of that interesting and important work, the simpler and 
elegant approach of [10, 14] is considered in some detail.
The treatment here will follow these references with occasional adaptations 
of notation and results to suit our purpose. While the codes are not the best 
available in terms of list size and achievable rate, they are simpler to describe 
and, in a sense, representative of the approach.
Let y be a primitive element of Fq and let xi = Y, i = 0,1,2, • --,q - 2 = 
a - 1. Consider the (a,k,d)q RS code
C= (f (x0),f (x1),...,f (xa-1)),f e Fq[x], degf<k.
https://doi.org/10.1017/9781009283403.014 Published online by Cambridge University Press

13.3 On the Construction of Capacity-Achieving Codes
355
Let m |n, N = n/m. Codewords of this code will be designated y e C, y = 
(y0,y 1,.. .,yn-1) e Fq.
The folded RS code then is a rearrangement of this RS code to the form:
C =
f(1)
f(x 1)
...
f (xm-1 )
f(xm) 
f (xm +1)
.. .
f (x2m-1)
f(xn-m) 
f(Xq - m+1)
f (xn-1)
, deg f<k
(13.19)
Denote this code as FRSqm[n,k], an (mn, k, m)qm code, also designated as 
C. The code coordinates are viewed as column vectors of length m over
Fq (elements of Fqm ). The parameter m will be referred to as the folding 
parameter of the code. The rate of the code remains the same as the original
RS code.
The reason such a construction might be of value lies in the fact that several 
errors might affect only a single or few coordinate positions in the folded RS 
code and thereby lead to improved performance.
Codewords of the code C will be designated as y = (y0,y1,...,yn-1) e 
F qn,n = q — 1 and codewords of the code C will be of the form of N-tuples of 
column vectors of length m over Fq, i.e., as z = (z0,z1,...,zN-1) e (Fqm)N 
where each coordinate position zj is a column vector over Fq, zj e Fqm . Thus
/
z 0,0 
z0, 1
...
z1,0 
z1,1
...
zN-1,0 
zN-1,1
...
z = (z0, z1,.
. , zN-1) =
,
,...,
e C
I
z0,m-1
z1,m-1
zN-1,m-1
J
(13.20) 
and it follows that yjm+k = zj,k. Refer to y e C as the unfolded version of 
z e C and z as the folded version of y.
It is the folded code C that is of interest. As a slight variation of previous 
notation denote by v = (v0,v1,...,vN-1),vj e Fqm a received word for some 
transmitted word z e C. The received word v is said to have t agreements 
with a codeword z e C if t of the columns of v agree with the corresponding t 
columns of the transmitted codeword z, i.e., the column m-tuples over Fq agree 
in all m positions.
Before proceeding, it is first noted that under certain conditions, the 
polynomial e(x) = xq—1 — y e Fq[x] (different from that of previous 
use) is irreducible over Fq (see [19], theorem 3.75 or [12], lemma 3.7). 
This polynomial is useful in the present context since for any polynomial 
f(x) = i fixi e Fq [x] it is true that
https://doi.org/10.1017/9781009283403.014 Published online by Cambridge University Press

356
13 List Decoding
f(x)q = 
fix'} = ZL fiqx'q = ZL fix“q = f(xq) = f(Yx), mod e(x)
i 
ii
since in the field K = Fq[x]/{e(x)} it is true that xq = yx. Let f(x) e Fq[x] 
be of degree at most k 1 which generates the codeword j = (y0,y 1,..., yn-1) e 
C, an element of Fqn, n = q - 1. Further let g(x) = f (yx) = f(x)q. The 
definition of g(x) may seem strange but it will be shown shortly to assist in the 
decoding of the code C. Take as our interpolation set
I = {(yi,f(yi),g(yi) = f(Y+1 )),i = 0,1,...,n - 1}.
A polynomial Q(x, y,z) is sought which has n zeros of order s at each 
point of the interpolation set I. From previous considerations, the number 
of monomials in such a polynomial of weighted (1,k1,k1) degree D is 
approximated by D3/6k12. The number of homogeneous conditions imposed 
by requiring zeros of order s at each point of the interpolation set is n s+32 . 
Thus the nonzero polynomial Q(x,y,z) satisfying these conditions will exist 
if its weighted degree satisfies
or D > |_^/nk1s(s+1)Cs+~2)j + 1.
Before discussing decoding the folded codes, consider the above interpola­
tion polynomial written as Q(x, y 1 ,y2) which may be written as Q(x, y 1 ,y2) =
i,j qij(x)y1iy2j, qij(x) e Fq[x] and denote
Q'(x,y 1 ,y2) = £2q'ij(x')yiyj, mod e(x)
where
qij(x) = qij(x), mod e(x).
The elements qij(x) can be thought of as elements of the field K = 
Fq[x]/{e(x)}. It is clear that if f (x) is a solution to Q(x, f (x),f (yx)) = 0, 
then it is also a solution to Q'(x, f(x), f(yx)) = 0. The problem is to find 
solutions to the equation
Q'(x,f(x),f(x)q) = P(f(x),f(x)q) e K[x].
As with the previous codes, define
H(y) = P(y,yq)
and it can be shown that this is a nonzero polynomial. The solution of this 
equation in K will be a polynomial of degree at most k1 which will correspond 
D3 s + 2
—> > n
6 k 2 
3
https://doi.org/10.1017/9781009283403.014 Published online by Cambridge University Press

13.3 On the Construction of Capacity-Achieving Codes
357
to a codeword polynomial in the folded code C within the specified distance to 
the received word v' e (Fm )N and this polynomial will be a factor (y — v(x)) 
of Q(x,y,yq).
Suppose the received folded word is v' e (Fqm >N and let v e Fq be the 
unfolded version of this received word. Assume that t columns of the word v' 
are correct and assume column j of v', vj e Fqm is correct. Then
V j = ( Vj, 0 ,Vj, 1 ,...,vj,m — 1) = ( YPn,YPn+1 ,...,Yjm+m—1).
It follows (and this is the point of the folding process) that for a correct column 
the (n — 1) values
Qyjjm+e,f(yjm+^,f(yjm+£+1) = 0, £ = 0,1,...,m — 2 
and thus as Q(x, y, z) is evaluated over all adjacent pairs of the received word, 
each correct column contributes s(m — 1) zeros.
(Note: A slightly better argument, credited to J0rn Justesen in ([12], section 
3.D), follows from the observation that an error in the j-th position of the FRS 
code can lead to at most m + 1 errors in the interpolating triples and this leads 
to slightly better parameter values for t.)
Thus if the total number of such zeros exceeds the weighted degree of 
Q(x, y, z), then it is the zero polynomial and by the process noted above 
a codeword polynomial within distance at most q — t is attained under the 
circumstances noted, i.e., if
t(m— 1)s > D.
Hence t , the maximum number of error-free positions in the received word 
tolerated by the list-decoding algorithm, can be chosen as
k2n 
+ i^A 2^ i
(m — 1)3 
s s (m — 1)s
+1.
Since n = mN this expression can be written as
t=
3 
k2 " (1 + 1V1 + 2 \ + 
1
( (m — 1)3 
+ s + s + (m — 1 )s
+1
3 k1 
n 3
n2 (m—1)3
1
(m — 1 )s
+ 1.
Asymptotically, using (n/(m — 1))3 « (1 + e)N3, this gives
t « N( 1 + e)R2/3.
https://doi.org/10.1017/9781009283403.014 Published online by Cambridge University Press

358
13 List Decoding
Thus the fractional error rate for the folded RS codes is
N (N - t) & 1 - (1 + e)R2/3.
The conclusion of these arguments is the following [11, 12]:
Theorem 13.17 For every e > 0 and rate R, 0 < R < 1 there is a family of 
m-FRS codes for m = O( 1 /e) that have rate at least R and which can be list 
decoded up to a fraction 1 - (1 + e)R2/3 of errors in time polynomial in the 
block length and 1 /r
This development has been for the trivariate case. The M-variate case is 
commented on below. The details discussed above for the trivariate case are 
gathered in the algorithm:
Algorithm 13.18 
case)
Input:
(Guruswami-Rudra decoding of FRS codes - trivariate
n,k, d, Fq,Y e Fq primitive, folding parameter
m, N = n/m, irreducible polynomial e(x) e Fq[x], 
degree q - 1, 5 = zeros order, codeword polynomial 
f(x) e Fq[x] degree < k1, received word, v e (Fqm')N, 
K = F q [ x ] / {e(x))
Output:
A list of codewords that agree with word v in at 
least t places
Algorithm: 1.
Compute parameters t and D satisfying the inequalities 
n 5+32 < D3/6k12 and t(m - 1)5 > D
2.
Compute the interpolation polynomial Q(x,y,z) over Fq 
of weighted (1,k1,k1) degree at most D with zeros of 
order 5 at the n points (Y ,f(Y'), f(Y+1 )),i = 1,... ,n
3.
Compute the polynomial P (y,z) = Q(x,y,z) mod e(x), 
P(y,z) e K[y, z]
4.
5.
Compute the univariate polynomial H(y) = P (y, ya) 
Compute the roots of H(y)in K as a polynomial f(x) in x 
(and ya = g(x))
6.
Output those polynomials f(x) that agree with received 
word in at least t positions
As with the Parvaresh-Vardy codes, the M-variate case can be used for 
which the fraction of errors tolerated in the list-decoding algorithm can be 
shown to be
1-
1 + M 
m\rmmm+1)
5 m — M + 1
https://doi.org/10.1017/9781009283403.014 Published online by Cambridge University Press

References
359
and asymptotically this tends to the 1 - R result promised. Thus for the 
Guruswami-Rudra decoding folded RS decoding, the fraction of errors that 
can be tolerated in an RS code of block length n is, asymptotically for large 
block lengths,
Pgr ^ 1 - R.
(13.21)
For details and comments on the resulting list size, the references cited should 
be consulted.
Comments
The chapter has attempted to give a broad outline of the interesting develop­
ment of the field of list decoding that have led to capacity-achieving codes. 
The work of Sudan and Guruswami and others has been exceptional in terms 
of the new techniques introduced and the results obtained. While many of the 
derivations considered are quite specialized and intricate, the impressive results 
obtained and the insight they give are surprising.
References
[1] Bartz, H., and Puchinger, S. 2021. Decoding of interleaved linearized Reed- 
Solomon codes with applications to network coding. ArXiv, abs/2101.05604.
[2] Bleichenbacher, D., Kiayias, A., and Yung, M. 2003. Decoding of interleaved 
Reed Solomon codes over noisy data. Pages 97-108 of: Proceedings of the 30th 
International Colloquium on Automata, Languages and Programming (ICALP).
[3] Cameron, P.J., and van Lint, J.H. 1991. Designs, graphs, codes and their links. 
London Mathematical Society Student Texts, vol. 22. Cambridge University 
Press, Cambridge.
[4] Cox, D.A., Little, J., and O’Shea, D. 2015. Ideals, varieties, and algorithms. 
Fourth edn. Undergraduate Texts in Mathematics. Springer, Cham.
[5] Elias, P. 1991. Error-correcting codes for list decoding. IEEE Trans. Inform. 
Theory, 37(1), 5-12.
[6] Gemmell, P., and Sudan, M. 1992. Highly resilient correctors for polynomials. 
Inform. Process. Lett., 43(4), 169-174.
[7] Guruswami, V. 2005. List decoding of error-correcting codes. Vol. 3282. Springer.
[8] Guruswami, V. 2006. Algorithmic results in list decoding. Found. Trends Theor. 
Comput. Sci., 2(2), 107-195.
[9] Guruswami, V. 2010. Bridging Shannon and Hamming: list error-correction with 
optimal rate. Pages 2648-2675 of: Proceedings of the International Congress of 
Mathematicians. Volume IV. Hindustan Book Agency, New Delhi.
https://doi.org/10.1017/9781009283403.014 Published online by Cambridge University Press

360
13 List Decoding
[10] Guruswami, V. 2011. Linear-algebraic list decoding of folded Reed-Solomon 
codes. Pages 77-85 of: 26th Annual IEEE Conference on Computational Com­
plexity. IEEE Computer Society, Los Alamitos, CA.
[11] Guruswami, V., and Rudra, A. 2006. Explicit capacity-achieving list-decodable 
codes or decoding up to the Singleton bound using folded Reed-Solomon codes. 
Pages 1-10 of: STOC ’06: Proceedings of the 38th Annual ACM Symposium on 
Theory of Computing. ACM, New York.
[12] Guruswami, V., and Rudra, A. 2008. Explicit codes achieving list decoding 
capacity: error-correction with optimal redundancy. IEEE Trans. Inform. Theory, 
54(1), 135-150.
[13] Guruswami, V., and Sudan, M. 1999. Improved decoding of Reed-Solomon and 
algebraic-geometry codes. IEEE Trans. Inform. Theory, 45(6), 1757-1767.
[14] Guruswami, V., and Wang, C. 2013. Linear-algebraic list decoding for variants of 
Reed-Solomon codes. IEEE Trans. Inform. Theory, 59(6), 3257-3268.
[15] Guruswami, V., Umans, C., and Vadhan, S. 2009. Unbalanced expanders and 
randomness extractors from Parvaresh-Vardy codes. J. ACM, 56(4), 20:1-20:34.
[16] Huffman, W.C., and Pless, V. 2003. Fundamentals of error-correcting codes. 
Cambridge University Press, Cambridge.
[17] Huffman, W.C., Kim, J.L., and Sole, P. (eds.). 2021. A concise encyclopedia of 
coding theory. CRC Press, Boca Raton, FL.
[18] Kaltofen, E. 1992. Polynomial factorization 1987-1991. Pages 294-313 of: 
Simon, Imre (ed.), LATIN ’92. Springer, Berlin, Heidelberg.
[19] Lidl, R., and Niederreiter, H. 1997. Finite fields, 2nd ed. Encyclopedia of Mathe­
matics and Its Applications, vol. 20. Cambridge University Press, Cambridge.
[20] MacWilliams, F.J., and Sloane, N.J.A. 1977. The theory of error-correcting 
codes. I and II. North-Holland Mathematical Library, vol. 16. North-Holland, 
Amsterdam/New York/Oxford.
[21] Mullen, G.L. (ed.). 2013. Handbook of finite fields. Discrete Mathematics and Its 
Applications (Boca Raton). CRC Press, Boca Raton, FL.
[22] Parvaresh, F., and Vardy, A. 2005. Correcting errors beyond the Guruswami- 
Sudan radius in polynomial time. Pages 41 of: Proceedings of the 46th Annual 
Symposium on Foundations of Computer Science. FOCS ’05. IEEE Computer 
Society, Washington, DC.
[23] Peterson, W.W. 1961. Error-correcting codes. The MIT Press, Cambridge, MA.
[24] Puchinger, S., and Rosenkilde ne Nielsen, J. 2017. Decoding of interleaved Reed- 
Solomon codes using improved power decoding. Pages 356-360 of: 2017 IEEE 
International Symposium on Information Theory (ISIT).
[25] Sudan, M. 1997. Decoding of Reed-Solomon codes beyond the error-correction 
bound. J. Complexity, 13(1), 180-193.
[26] van Lint, J.H. 1992. Introduction to coding theory, 2nd ed. Graduate Texts in 
Mathematics, vol. 86. Springer-Verlag, Berlin.
[27] Wachter-Zeh, A., Zeh, A., and Bossert, M. 2014. Decoding interleaved Reed- 
Solomon codes beyond their joint error-correcting capability. Des. Codes Cryp- 
togr., 71(2), 261-281.
https://doi.org/10.1017/9781009283403.014 Published online by Cambridge University Press

14
Sequence Sets with Low Correlation
The design of sequences for communication purposes such as ranging, 
synchronization, frequency-hopping spread spectrum, code-division multiple 
access (CDMA), certain cell phone systems and space communications has 
a long and interesting history. As with so many other of the areas covered 
in this volume, the mathematical techniques used to construct sequences 
with the desired properties tend to be challenging. While many of the results 
and techniques date back to the 1950s, the area continues to encompass a 
vigorous research community as more and more applications and variations 
are discovered. This chapter provides the basic information of the area.
14.1 Maximum-Length Feedback Shift Register Sequences
Consider an n-stage recursion over Fq (although generally interest will be in 
F2) as shown in Figure 14.1:
Xt = a 1 Xt-1 + a2Xt-2 +------+ anxt-n, t = n,n + 1, ..., ai e Fq (14.1)
with initial values in Fq given for the n cells, xn-1,xn-2,...,x0, referred 
to as the initial state of the shift register. This equation is referred to as the 
characteristic equation of the sequence it generates.
The operation of the shift register is that at each clock cycle t of the register 
a new feedback value Xt is formed, the contents of each of the n registers are 
shifted one cell to the right, the new value is fed into the leftmost cell and a 
value Xt-n is produced for output. The set of values (Xt-1,Xt-2,...,Xt-n) is 
referred to as the state of the shift register at time t. If the state of the register is 
all-zeroes, then the register remains in that state always. We will be interested 
in sequences over Fq, Xt e Fq forallt. In order to analyze the system, however,
361
https://doi.org/10.1017/9781009283403.015 Published online by Cambridge University Press

362
14 Sequence Sets with Low Correlation
xt -(n+1)
xt
Figure 14.1 Linear-feedback shift register
we will also consider values of xt in some extension field, say Fqn of Fq as a 
means to an end.
Such sequences will be referred to as linear-feedback shift register (LFSR) 
sequences. The properties of such sequences are of interest in this chapter, 
especially the correlation and cross-correlation properties which play a crucial 
role in numerous synchronization, ranging, CDMA and frequency-hopping 
spread spectrum systems. One could also replace the linear sum of the cell 
contents with an arbitrary Boolean function to form the feedback variable, to 
give a nonlinear-feedback shift register but the properties of such sequences 
are considerably more difficult to analyze and are not considered here.
For sequences over Fq there are qn - 1 possible nonzero states of the 
register. If the register is in the all-zero state, it remains there as the clock 
cycles and produces only zeros as output. If the sequence produced by the shift 
register is such that
xt = xt+N for all t
for the smallest N, the sequence has period N. Clearly the maximum period 
of such a shift register is qn - 1 and this is achieved if and only if the state of 
the register cycles through all qn - 1 nonzero states in each cycle, assuming 
a sequence over Fq . Such sequences are termed maximum-length shift register 
sequences or ML sequences (not to be confused with maximum-likelihood 
(ML) receivers considered in other chapters) or, more simply, m-sequences, 
the preferred term for this work. Thus the period of an m-sequence is typically 
q£ - 1 for some positive integer £, generated by a shift register of length £. 
Such sequences will have interesting properties that will be discussed.
Define the characteristic polynomial of the recursion of the sequence over 
Fq of Equation 14.1 as
f (x) = xn — a 1 xn 1 — a2xn - — • • • — an, ai c Fq.
https://doi.org/10.1017/9781009283403.015 Published online by Cambridge University Press

14.1 Maximum-Length Feedback Shift Register Sequences
363
Consider the question of whether xt = a‘,t = 0,1,2,... for some element 
a in an extension field of Fq is a solution of the recursion by substituting into 
Equation 14.1, i.e., could it be true that
a = a 1 a-1 + a2a—2 + • • • + ana-n,t > n
or a1 -n{an - a 1 an-1 — a2an—2 — • • • — an) = 0, t = n,n + 1, ...
or equivalently f(a) = 0, i.e., a must be a root of the recursion characteristic 
polynomial in order for xt = at,t = 0, 1,2,... to be a solution. In such a 
case it follows that xt = ka*, for X a constant also in the extension field, 
is also a solution of the recursion since the effect is to multiply all terms of 
the characteristic equation by the same constant. Since a is an element in 
an extension field of Fq , the sequence would then also be over the extension 
field. Our interest will be in sequences over Fq . Suppose further that a is an 
element of order N in the extension field. It follows immediately the sequence 
it generates is of period N.
By the same reasoning ifa1,a2,...,an are distinct roots of the characteris­
tic polynomial f(x)in some extension field of Fq, say Fqn, then the sequence
xt = X1 a1 + X2a2 + • • • + Xna^, t > n, Xi e Fqn 
(14.2)
satisfies the linear recurrence Equation 14.1 and produces a sequence over Fqn .
Our (almost exclusive) interest will be in the case where f(x)is a primitive 
polynomial of degree n over Fq.Ifa is a zero of f (x), then the other zeros 
are aq,aq2,...,aqn—1 . From Equation 14.2 a solution to the characteristic 
Equation 14.1 would be
xt = X1a + X2a q + ••• + Xna q
for constants Xi e Fqn,i = 1, 2,...,n. The sequence xt is over Fqn but notice 
that if the Xi are chosen as Xqi,i = 1,2,...,n, then
n — 1
xt 
Xaiq = Trqn|q(Xa‘), X e F*n
i=0
is a sequence over Fq and is a solution of the shift register equation. From 
the above comments the period of this sequence is the order of the zeros aqi , 
which is qn — 1 and these are m-sequences over Fq .
Notice that Equation 14.1 will produce qn solutions according to the qn 
initial conditions of the shift register {xn—1,xn—2,...,x0}, including the trivial 
all-zero solution. The above equation also produces qn solutions according to 
the values of X e Fqn. If these sequences are distinct, then any such sequence 
that satisfies Equation 14.1 can be so represented. Note the trivial all-zero 
https://doi.org/10.1017/9781009283403.015 Published online by Cambridge University Press

364
14 Sequence Sets with Low Correlation
solution is included in the count. Of course, for a given feedback connection, 
since each m-sequence cycles through each possible nonzero the (qn - 1) 
sequences are simply translates of each other.
It is easy to verify that
Xt = TrqnIq^kat'), k e Fqn, t > 0
(14.3)
is a solution of the recursion Equation 14.1 as follows:
Xt = a 1 Xt-1 + a2Xt-2 +------+ amXt-n, ai e Fq
Tr qn । q(kat) = a iTrqn\q(kat-1) +------ + an Tr qn । q(ka - n)
or 0 = Trqn\q(k(at — aia1 1 — ■■■ — ana‘ ”)
or 0 = Trqn\qlka1 —n(an - a 1 an-1 — • • • — an}
which follows since a is assumed a zero of the characteristic polynomial. 
Suppose k1 = k2 and consider if the associated solutions to the recursion 
are equal, i.e., suppose
x* = Trqi।q(k 1 d) = xt = Trqi\q(k2d), t > 0, 
k 1 ,k2 e Fqi
and so
xt — xt = 0 = Trqn।q(k 1 at) — Trqn\q(k2at) = Trqn\q((k 1 — k2)d} = 0 for all t > 0.
From the onto property of the trace function (Equation 1.5 of Chapter 1) this 
would imply that k1 — k2 = 0, a contradiction. Thus every solution over Fqto 
the recurrence, corresponding to different initial states of the register, under the 
above condition, can be represented in the form of Equation 14.3, each field 
element k corresponding to an initial condition ([8], theorem 9.2).
The sequence xt is called cyclically equivalent to the sequence xt = 
Trqn\^k1 d}, k e Fqn, if for some fixed i
xt = xt+i for all t > 0.
Otherwise the two sequences are cyclically inequivalent or cyclically distinct. 
Suppose a is an element of order N in Fqn (hence the sequence is of period 
N). The two sequences will be cyclically equivalent iff for some i
xt— xt+i = 0 = Trqn\q(k2d)—Trqn\q(k 1 d+9, or Trqn\q((k2— k 1 d)d) for all t > 0.
Thus the two sequences are cyclically equivalent iff k2 = k 1 a1, i.e., iff their 
associated constants must be related by a multiplicative constant of the group 
Gn = {1,a,a2,... ,aN—1}, i.e., iff the two constants are in the same coset of 
this multiplicative subgroup of Fqn .
https://doi.org/10.1017/9781009283403.015 Published online by Cambridge University Press

14.1 Maximum-Length Feedback Shift Register Sequences
365
Interest in these sequences for the remainder of the chapter will be m- 
sequences, i.e., maximum-length sequences over Fq where the period is N = 
qn — 1. From the above observations then a is a primitive element of Fqn and 
the sequence characteristic polynomial will be a primitive polynomial over Fq 
of degree n. For such a generator, the previous discussion implies any two 
sequences with generator element a will be cyclically equivalent, i.e., for the 
primitive element a e Fqn for any k1 ,k2 e F*n the sequences Trqn।^k1 at} 
and Trqn ।q(k2a1} are cyclically equivalent. As noted, for an m-sequence over 
Fq , the shift register producing it cycles through each nonzero state in any 
given cycle.
The sequences corresponding to primitive characteristic polynomials will 
have interesting properties. Recall that ifa is a primitive element ofFq, then ai 
will also be primitive iffi and qn — 1 are relatively prime (i.e., (i,qn — 1) = 1). 
Since the minimal polynomial of any primitive element of Fqn is of degree 
n there are exactly $(qn — 1 )/n primitive polynomials of degree n over Fq, 
where it is recalled $(£) is the Euler <p function, the number of integers in 
[f ] = {1,2,... ,1} relatively prime to I. Any of these primitive polynomials 
could be used to produce an m-sequence. The sequences produced by distinct 
primitive polynomials are not cyclically equivalent. To see this suppose a is a 
primitive element of Fqn. Let d be an integer (d,qn — 1) = 1 and ft = ad. 
Then ft is a distinct primitive element in Fqn. Assume the sequences produced 
by these elements are cyclically equivalent and so
Trqn|q(^k 1 a^ = Trqn।q(k2ad(+i)^, t > 0 ^^ k 1 a* = k2ad(+i), d > 1, t > 0
which is clearly not possible and such sequences cannot be cyclically equiv­
alent. It follows there are $(qn — 1 )/n cyclically inequivalent sequences of 
period qn — 1 over F q.
For a given m-sequence xt = Trqn।q^ka*}, a e F*n primitive and k e F*n, 
call the sequence by taking every d-th sequence element, so that
yt = xdt for allt > 0
the d-decimated sequence. It is possible to show that any two cyclically 
inequivalent m-sequences of the same length over the same field are related 
by decimation. Suppose xt and yt are two m-sequences over Fq . Then there 
exists a positive integer d, relatively prime to qn — 1, such that
yt = xdt for all t > 0.
To see this assume xt = Trqn|q (kat) and the d-decimated sequence be 
yt = xt[d] = Trqn |q (kadt) = Trqn|q (kftt) where ft = ad which, since 
https://doi.org/10.1017/9781009283403.015 Published online by Cambridge University Press

366
14 Sequence Sets with Low Correlation
(d, qn - 1) = 1, is a primitive element. Thus the number of cyclically 
inequivalent m-sequences of this period is $(qn — 1 )/n and they are generated 
by one primitive element drawn from each conjugacy class of each primitive 
polynomial of degree n. It easily follows that any m-sequence can be obtained 
from any other m-sequence by suitable decimation and translation.
In general if an m-sequence of length N = qn — 1 is d-decimated the period 
of the resulting sequence will be N/(N,d).
A curious property of m-sequences is that for any translation r e Z of an 
m-sequence xt there exists another unique translation a such that
xt + xt+T = xt+a for all t > 0,
often referred to as shift-and-add property [8]. To see this consider a fixed 
integer r and note that for some X
xt + xt+r = Tr qn । q (Xat) + Tr q^q.a+T) 
= Trqn ।^\a (1 + a))
but since a is primitive in Fqn there exists a unique a such that (1 + aT) = aa 
and the result follows.
It might be asked if there is regularity as to the number of each finite field 
element in the period ofan m-sequence and the following result is as expected:
Tr qn । q(ka) =
V,
0,
qn 1 times, V e Fq * 
qn-1 - 1 times.
More generally [6] it is true that for k | n:
Tr qn | qk()a =
V, q n-k times, V e F*k q 
0, qn-k - 1 times.
Example 14.1 Consider the two m-sequences over F2 generated by the 
circuits in the figure below (the characteristic polynomials are reciprocal 
polynomials but this property is not important for the development).
characteristic polynomial 
f(x) = x4 + x + 1
characteristic polynomial 
f(x) = x4 + x3 + 1
https://doi.org/10.1017/9781009283403.015 Published online by Cambridge University Press

14.1 Maximum-Length Feedback Shift Register Sequences
367
In the tables below, for each circuit, the sequence of states the shift registers 
go through, the output m-sequences obtained (always the last bit of the state 
sequence) and the m-sequence produced by the Tr24|2 function, with the 
purpose of showing (for each register) the two m-sequences (from the shift 
register and from the trace function for that register) are cyclically equivalent 
but the two shift registers produce cyclically inequivalent m-sequences.
register 
state
output 
sequence
a‘
Tr24|2(at)
0110
0
1
0
1011
0
a
0
0101
1
a 2
0
1010
1
a3
1
1101
0
a4
0
1110
1
a5
0
characteristic
a6
7
polynomial
1111
0
1
x4 + x + 1
0111
1
a
a8
1
0011
1
0
0001
1
a9
1
1000
1
a10
0
0100
0
a11
1
0010
0
a12
1
1001
0
a13
1
1100
1
a14
1
--­
0110
-­
0
-­
1
-­
0
register
output
at
Tr24|2(at)
state
sequence
0001
0
1
0
1000
1
P
1
1100
0
P 2
1
1110
0
P3
1
1111
0
P4
1
0111
1
P5
0
characteristic
P6
P7
P8
polynomial 
x4 + x3 + 1
1011
0101
1
1
1 
0
1010
1
1
1101
0
P9
1
0110
1
P10
0
0011
0
P11
0
1001
1
P12
1
0100
1
P13
0
0010
0
P14
0
--­
0001
-­
0
-­
1
-­
0
https://doi.org/10.1017/9781009283403.015 Published online by Cambridge University Press

368
14 Sequence Sets with Low Correlation
There are numerous other properties of m-sequences but the remainder of 
the chapter will focus on their correlation properties, of crucial importance to 
their application.
14.2 Correlation of Sequences and the Welch Bound
Interest in m-sequences lies mainly in their cycle structure, pseudorandomness 
properties and application to such areas as radar, ranging and spread spectrum 
where their auto- and cross-correlation properties are of importance. The 
representation of Equation 14.3 is useful in these studies. A brief overview 
of their properties and applications is noted here.
While much of the remainder of the chapter will restrict sequences to 
be binary (either {0, 1} for developing the theory or {±1} for applications) 
many of their interesting auto- and cross-correlation functions properties do 
not depend on this and a few of their general properties are discussed before 
further restricting the alphabet. Let x, y be complex sequences with period N, 
i.e., xi = xi+N,yi = yi+N for all i . Define their periodic auto- and cross­
correlation functions as
jV-1 
N-1
9x(t) ^22 xiXi+t and 0x,y (T) 
22 xi?i+*, t e Z, xi,yj e C (14.4)
i=0 
i=0
where the bar indicates complex conjugation. Numerous properties of these 
functions follow from their definition, e.g., [10]
N-1 
/ N-1 
X1 /2/ N-1 
X1 /2
E I Gx,y(^) I2 < E I &x(t) I2 
E I &y(t) I2
£=0 
\ £=0 
' 
\ £=0
N-1 
/ 
/ N-1 
V/2/ N-1 
V/2
Yx I Gxyd) I2 e 9x(0)Qy(0) ± E I 9x(t) I2 E I ^) I2
£=0 
\ 
\ £=1 
£=1
The aperiodic case can also be of interest for some applications. For such a 
case sequences can be formulated from one period of each sequence with the 
sequences containing zeros outside of the one period and correlations formed 
between one sequence and a translated version of the other sequence. Such will 
not be considered here. Interest will be in sets of sequences with low maximum 
periodic cross-correlation, with questions such as how large a set can be found 
with a given maximum cross-correlation? The following definitions are of 
interest. Assume the set C contains M complex sequences of length N .
Define the peak out-of-phase autocorrelation for a sequence x e CN as
= max{I GX(V) I , 1 < £ < N - 1}
https://doi.org/10.1017/9781009283403.015 Published online by Cambridge University Press

14.2 Correlation of Sequences and the Welch Bound
369
and the peak cross-correlation as
0c = max{| 0x,y(t) |, 0 < £ < N - 1} 
x,y eC
and
0max = max(0a, 0c ).
Then [9] for the set of sequences C
N-1 
02
-------- a > 1
N(M - 1) n
and it follows that 
0max >N
M-1
1/2
NM - 1
which is a special case of the Welch bound (with the parameter k = 1) shown 
below where the sequences have norm unity.
For the remainder of the discussion attention will be restricted to the binary 
primitive case, i.e., m-sequences although there is a considerable literature on 
nonbinary sequences and their properties. The length of the m-sequence is N = 
2n - 1 and as one considers sliding a window of length n along the sequence 
each possible (2n - 1) nonzero n-tuple will appear as a state of the register - 
only the all-zeros state is missing. As noted above, each period of length N of 
the sequence contains 2n-1 1’s and (2(n-1) - 1) 0’s.
For applications, interest is in binary {±1} sequences rather than binary 
{0, 1} and the usual transition is made via yt = (-1)xt. The (periodic) 
autocorrelation function of a sequence of length N = 2n - 1 for the sequence 
represented by yt = (-1 )xt where xt = Trqn।q(ka‘), a a primitive element of 
F2n, is
N-1
0y(T) = E ytyt+t
i=0
N-1 
N-1
= 
(-1)xt (-1)xt+t = 
(-1)xt +xt+t
i=0 
t=0
N-1
= 
(-1)Trqn|q(^a‘) +Trqn।q(Xa‘+T)
t=0
N-1
= 
(-1)Trqn|q(^«t( 1+a1)).
t=0
As t ranges over 0 to N- 1 = 2n -2, the trace function gives 1, 2n-1 times and 
0, (2n-1 -1) times yielding a value of -1 for the sum. Thus the autocorrelation 
of an m-sequence is two-valued, namely
https://doi.org/10.1017/9781009283403.015 Published online by Cambridge University Press

370
14 Sequence Sets with Low Correlation
Figure 14.2 Ideal correlation function of m-sequence of period 2m - 1
Oy(T) =
N = 2n - 1,
-1,
if t = 0
if t = 0
mod (2n - 1) 
mod (2n - 1).
It is this peaked correlation function that is the prime characteristic of 
interest for the application of such sequences. If the ±1 values modulated 
pulses are of height +1 and width one time unit, the “classic” picture of such 
an autocorrelation function is shown in Figure 14.2. It is what makes them of 
importance in applications.
As noted, m-sequences are also referred to as pseudonoise or pn sequences. 
To explain this terminology, Golomb’s sequence randomness postulates are 
noted: A sequence is referred to as a pn-sequence if 
(i) the number of 1’s in a period differs by at most 1 from the number of 0’s; 
(ii) in a complete period, the number of runs (of either 0’s or 1’s), half have
length 1, one-fourth have length 2, one-eighth of length 3, etc. (although 
there will be only one run of 1’s of length n and none of 0’s of length n 
and one run of 0’s of length n - 1 and none of 1’s of this length);
(iii) the autocorrelation function of the sequence is two-valued (say k for 
off-peak and N for peak value where for the m-sequences we have seen 
k=-1).
It is argued that these are properties one might expect of a purely random 
binary sequence and since the m-sequences, generated deterministically, have 
such properties, they are often referred to as pseudorandom or pn-sequences. 
The notation m-sequences will be retained here.
For the multiuser application introduced below there will be interest 
not only in autocorrelations of m-sequences noted above but also cross­
correlations of distinct m-sequences (of the same length). The above result 
indicates that for such sequences it is sufficient to consider decimations of 
sequences. Suppose xt and yt are {0, 1} m-sequences of length N = 2n - 1. 
The periodic cross-correlation of the corresponding {±1} sequences will be 
defined as
https://doi.org/10.1017/9781009283403.015 Published online by Cambridge University Press

14.2 Correlation of Sequences and the Welch Bound
371
tV-1
ex,y(T) = E(—1) (— 1 )yt++
t=0
and each sequence has the same norm.
Specifically it will be of interest to generate large sets of sequences with 
the property that for any two sequences in the set, x, y, x = y the largest cross­
correlation (since for an m-sequence 0a = -1)
c max = max 10x,y(T) | , T = 0, 1, . . . , N - 1 
x=y
is as small as possible. A natural set to consider is the set of all m-sequences 
of a given length. This turns out not to be useful as the results are poor. A table 
of such cross-correlations is given in [10] and, e.g., for binary {±1} sequences 
of length 27 - 1, the number of such sequences is $(27 - 2)/7 = 18 and for 
this set cmax = 41.
To examine the trade-off possible between cmax, the number of sequences 
and their length, an influential bound due to Welch [11] is useful. It is 
developed here, using his original argument with slightly changed notation.
Let C be a set of M sequences of length N over the complex numbers of 
norm unity. Notice that this includes the alphabet { ± -N}.
C = {xv, xX,...}, |C| = M, xv = (*[, ...,xvN) e CN,
v = 1,2,...,M, £ N=1 x?X V = 
N=1 । Xi |2= 1,
where the bar indicates complex conjugation. The sequences are complex with 
the norm of each sequence unity. Although this approach seems restrictive for 
the kinds of binary sequences of interest, it will in fact yield useful bounds. 
Developing bounds on inner products of these sequences will yield bounds on 
auto- and cross-correlations of interest.
For the setC ofM complex sequences of length N and norm unity, the inner 
product between two sequences xv and xX is
N
v X 
v X v X
Cv,x = (x ,x) 
Xixci, x ,x e C
and the maximum of all distinct inner products as
cmax = max | cv, X | 
(14.5)
The following theorem, the Welch bound, that relates cmax to the sequence 
parameters is of interest:
https://doi.org/10.1017/9781009283403.015 Published online by Cambridge University Press

372
14 Sequence Sets with Low Correlation
Theorem 14.2 ([11]) Let k be a positive integer. Then for any setofM complex 
sequences of length N and norm unity,
(c 
)2k
(cmax )
1>------
M-
M
1L (N+kk-1) -1
Proof: Consider an M x N matrix M whose rows are the M complex 
sequences and the M x M matrix MAM = (mv,k) where mv,k = cv,k. Denote 
by Bk the sum of all 2k powers of the magnitude of elements of this matrix. 
Thus
Bk = E ICv,^ I2k^ M(M - 1 )cmkax + M.
An estimate for Bk will be derived that yields a relationship (bound) between 
cmax and M and N . Clearly
Bk = E 
(cv,k)k(C v,x)k
v,= =1, 2 ,...,M
_  
( Vv k 
v k 
v k 
v k
= 
x 1 x 1 +----- + xNxN ••• x 1 x 1 +-----+ xNxN •••
v,k =1,2,...,M V 
' V 
'
vk 
vk 
vk 
vk
lx 1 x 1 +-----+ xNxN ••• x 1 x 1 +•••+ xNxN ,
a summation over the product of 2k brackets, each bracket a sum of N terms. 
In expanding the brackets one term is chosen from each bracket to yield:
Bk =
v,k=1,2,.
N k
E
xvxvxkxk 
xsi xti xsi xti
, M s1,..., sk=1 i =1
t1,..., tk=1
and by interchanging the order of the summations it is verified that
N M k 
2
Bk = E EnxSixv
s 1 ,...,sk = 1 v =1 i = 1
t1,..., tk =1
As with the usual binomial expansion, many terms in the sum will be the same. 
The extension to this multinomial case is straightforward. Let
n
b = (b 1 ,b2, ■ ■ .,bn), bi G Z>0, Y, bi = k 
in
d = (d 1 ,d2, .. .,dn), bi g Z>0, Y, di = k 
k \ 
k 
[ k \ 
i =ic
b 
ni(bi!) 
a"J 
\d 
ni(di!) 
https://doi.org/10.1017/9781009283403.015 Published online by Cambridge University Press

14.2 Correlation of Sequences and the Welch Bound
373
and gathering like terms in the previous expression for Bk yields
/i\ M N
Bk=k) 0 rn (xv)b(x v)d 
b,d ' ' v v v =1 i=1
2
An inequality is created by dropping (all terms positive) terms for which b = d 
to yield
MN 
q2
sin -'i'l'-bi 
v=1 i = 1 
-1
Recall the Cauchy-Schwarz inequality over real numbers as
and identifying vi with 1 and wi as a term in the sum over theb summation 
yields
Bk> IE (k)e n ।xvi2bi 12 /e i2=E(kn ।*yi2bi f7(N+k -1)
k 
-b 
hi=\ i 
-b 
-b ^V=1 i=\' i 
k J
where N+kk-1 is the number of possible vectors b, i.e., the number of 
partitions of the integer N into k parts, each part between 0 and k (Equation 
1.10 of Chapter 1), and interchanging the order of the summations and 
recalling all sequences have a norm of unity gives
_M_ 
(N+k-1)
and from the first equation of the proof
M(M- 1)cm2kax + M > Bk > _Mi_ 
(N+k-1)
from which the theorem statement follows. ■
The above bound refers to correlations of a set of M unit-length complex 
sequences of length N . To examine the off-peak correlations and cross­
correlations we can add all cyclic shifts of all sequences to the list and use 
the same bound by replacing M by MN. Thus for this case the bound reads 
(without changing the cmax)
c 
)2k
cmax )
1 MN
>---------- .... .
- MN - 1 [ (N +k--1) -1
(14.6)
https://doi.org/10.1017/9781009283403.015 Published online by Cambridge University Press

374
14 Sequence Sets with Low Correlation
cmax —
For a set of M distinct m-sequences of length N = 2n - 1, cmax is the max- 
min cross-correlation. Then the above expression with k = 1 gives a lower 
bound on this value as
M - 1
MN - 1
which for large M and N will be approximately 1 /JN. Thus the smallest 
maximum cross-correlation any set of M m-sequences of length N = 2n - 1 
can have is approximately VN ^ 2-n/2.
Numerous properties of m-sequences and their correlations are developed 
in the seminal paper [10] and a few of these are noted. For x, y two distinct 
±1 m-sequences of length 2n — 1, the cross-correlation 0x,y(£) is always an 
odd integer and indeed 0x,y(£) + 1 is divisible by 8 unless the sequences are 
generated by reciprocal polynomials. Note that these sequences have length N 
and the Welch bound Equation 14.6 must be adjusted for this case. Also it is 
true that
2n—2
0x,y(£) = +1
t=0
which implies that the average value of the cross-correlation over a period is 
nearly 0 and that the cross-correlations take on negative as well as positive 
values. In addition
2n—2
E | Gx,y d) |2 = (2n 
£=0
= 22n
— 1)2 + (2n
— 2n + 1.
—1)+1
As noted previously, the peaked autocorrelation function of an m-sequence 
as in Figure 14.2 is often described as an “ideal” correlation function. The 
view of this figure is that of one period of an m-sequence correlating with 
infinite translations ofa version of the same m-sequence. For cross-correlation 
of distinct m-sequences, the picture is more complex. In particular, if x and 
y are m-sequences generated by distinct primitive polynomials of degree m, 
then the cross-correlation function 0x,y(•) must take on at least three values 
(shown by Golomb). Furthermore, as shown in the previous section, any two 
m-sequences are related through a d-decimation, i.e., one is the d-decimation 
of the other. Then the cross-correlation function of this pair of m-sequences is 
the same as for any other pair of m-sequences related by d-decimation.
Certain sets of m-sequences with good correlation properties have been 
proposed in the literature and the following is of interest.
Theorem 14.3 ([10], theorem 1) Letxandy denote two m-sequences of length 
2n — 1 with y the d-decimated sequence of x where d = 2k + 1 or d = 
https://doi.org/10.1017/9781009283403.015 Published online by Cambridge University Press

14.3 Gold and Kasami Sequences
375
22k - 2k + 1.Ife = (n, k) is such that n/e is odd, then the cross-correlation 
0 x, y is three-valued with the values with frequencies of values:
-1 + 2(n+e)/2 occurs 2n-e-1 + 2(n-e-2)/2 times
-1 occurs 2n - 2n-e - 1 times
-1 - 2(n+e)/2 occurs 2n-e-1 - 2(n-e-2)/2 times.
As a consequence of this result, it is useful to define the function
t(n) = 1 + 2L(n+2)/2J. 
(14.7)
Then it can be shown that for n ^ 0 (mod 4) there exists pairs of m-sequences 
with three (nontrivial) cross-correlation values -1, -t(n) and t(n) - 2. Any 
such pair with such a three-valued cross-correlation is called a preferred pair of 
m-sequences and the cross-correlation function a preferred three-valued cross­
correlation function.
It was noted earlier in many applications of m-sequences it is of interest 
to have large sets of them with as low a cross-correlation between any two 
sequences as possible. It was also noted that subsets of the set of all f(2n -1 )/n 
distinct m-sequences of length 2n - 1 have relatively poor cross-correlation 
properties. The literature on construction techniques for such sequence sets is 
large. The constructions due to Gold [1, 2] and Kasami [7], now viewed as the 
“classical” constructions, are discussed in the next section using the treatment 
in [10] and the original works. The description of such sequences tends to be 
complex and only an overview is given.
14.3 Gold and Kasami Sequences
To construct sets of sequences with small peak cross-correlations it is natural to 
consider cyclic codes since all cyclic shifts of the sequences are in the code and 
the correlation properties are easily derived from the distance structure of the 
code. Only an outline of the code constructions is given. Thus if a,b e {0,1}N 
and x,y are the equivalent vectors in {±1}n via xi = (-1)ai, then
(x, j) = N - 2w(a ® b)
where w(•) is Hamming weight. Thus from the code distance structure the set 
of cyclically distinct sequences can be determined as well as the maximum 
cross-correlation of them.
The following construction appears to have been independently discovered 
by Gold [1, 2] and Kasami [7]. They constructed a three nonzero weight binary 
cyclic code of length N = 2n - 1 and dimension 2n with generator polynomial 
https://doi.org/10.1017/9781009283403.015 Published online by Cambridge University Press

376
14 Sequence Sets with Low Correlation
f1 (x)fe(x) where f1 (x) is a primitive polynomial of degree n over F2 with 
primitive root a e F2n and fy(x) is an irreducible polynomial with root a2 +1, 
which is primitive provided m is odd and (l,n) = 1. The resulting binary cyclic 
code has [2] the following weight distribution:
codeword weight w
2n-1
2n-1 + 2(n-1)/2
2n-1 - 2(n-1)/2
number of codewords Aw
(2n - 1)(2n-1 + 1)
(2n - 1)(2n-2 - 2(n-3)/2)
(2n - 1)(2n-2 + 2(n-3)/2)
The factor (2n - 1) in the number of codewords refers to the 2n - 1 possible 
cyclic shifts of each distinct codeword. Equivalently [2] the three possible 
values of the cross-correlations from these codeword weights are
2n
1 - 2 • 2n-1 = -1
Ox,y(t) =
2n - 1 - 2 • 2nn-1 ± 2(n-1)/2J
- 2(n+1)/2 + 1 or + 2(n+1)/2 - 1
Taking one sequence from each cyclic equivalency class then gives an alterna­
tive way of describing the set of these 2n + 1 Gold sequences [2, 6, 10] as:
xt = Trqn।q\aa{ + adtJ U {Tqn।q(at'^, d = 2£ + 1, a e F2n. 
(14.8)
A useful alternative way of describing these Gold sequences [10] is by using 
the shift operator on vectors (sequences). Thus for x = (x0,x1,...,xn-1) the 
operator T is such that Tx = (x1,x2,...,xn-1,x0), one cyclic shift to the left. 
Using this notation, a slightly different definition of Gold sequences is given 
in [10]. Given the influence of that work, that approach is outlined.
In general let f(x) = h1(x)h2(x) be binary polynomials (primitive) with 
no factors in common generating sequences of periods 2n - 1. The set of 
all sequences with characteristic polynomial f(x) is the set of all binary 
sequences length 2n - 1 of the form
x ® y
where x is generated by h1 (x) and y by h2 (x). Denote by G(x, y) the set of 
2n + 1 sequences of length 2n - 1
G(x,y) = {x,y,x®y,u ® Ty,x® t2y,...,x® t2n-2y}.
The above discussion leads to the below definition:
Definition 14.4 Let x,y be a preferred pair of m-sequences of period 2n - 1 
generated by the primitive polynomials h1 (x) and h2 (x). Then the set G(x,y)
https://doi.org/10.1017/9781009283403.015 Published online by Cambridge University Press

14.3 Gold and Kasami Sequences
377
characteristic polynomial
h(x) = xm + hm-i xm-1 + • • • + h 1 x + 1
characteristic polynomial
h' (x) = xm + h m _1 xm-1 + ••• + h 1 x + 1
Figure 14.3 Gold sequence set generator, (h(x),h (x)) a preferred pair of m- 
sequences
of 2n + 1 sequences is a set of Gold sequences. For any two distinct sequences 
u,v e G(x,y) the cross-correlation value is in the set {-1,_t(n),t(n) _ 2} 
where t(n) is as given in Equation 14.7. It is noted the nonmaximal-length 
sequences in the set can be obtained by adding together the outputs of shift 
registers with characteristic polynomials h1 (x) and h2(x), respectively. Note 
also that since each of the two registers could be in the all-zero state, the 
individual m-sequences generated by h1 (x) and h2 (x) are also in the set.
A generator for Gold sequences is shown in Figure 14.3. To generate the 
full set, (exactly) one of the generators may be in the all-zero state.
The work of Kasami [7] presented two classes of sequences with low auto- 
and cross-correlation, the small and large classes of Kasami sequences. Brief 
descriptions of these classes are given. Let n be an even integer and note that 
2n _ 1 = (2n/2 _ 1)(2n/2 + 1). Let a be a primitive element of F2n, a root of 
the primitive polynomial h(x) of degree n over F2.
Let u be the m-sequence associated with h(x) and v the decimated sequence 
v = u[d] where d = 2n/2 + 1. Thus the sequence v is of length 2n _ 1 and 
period 2n/2 _ 1.
Let s(n) = 2n/2 + 1 = d and h (x) the polynomial of degree n/2 whose 
roots are the s(n) = d powers of the roots of h(x). Thus the roots of h (x) are
ad,a2d,...,aa(2 
)d since a2n = a
https://doi.org/10.1017/9781009283403.015 Published online by Cambridge University Press

378
14 Sequence Sets with Low Correlation
and note that
a 2 n/2 d = a 2n/2 (2n/2 + 1) = a 2n+2n/2 = a 1+2n/2 = a<i
and the set of elements in the previous equation is in fact a conjugacy class - 
the roots of the minimal polynomial over F2 of ad.
As noted, the sequence v is a sequence of period 2n/2 - 1 and length 2n - 1. 
Consider sequences generated by the polynomial h(x)h (x) of degree 3n/2. 
Such sequences are of the form [10]
Ks = |u,u ® v, u ® T v,...,u ® T2 n/ 2—2}, | Ks |= 2n/2 
(14.9)
which is the small set of Kasami sequences [7], a set of 2n/2 sequences of 
length and period 2n - 1. The correlation functions of these sequences can be 
shown to belong to the set
{—1, — s(n),s(n) — 2} hence 0 max = s(n) = 1 + 2 n/2.
Recall the Welch bound was derived for sets of complex sequences of norm 
unity. Noting that 0max is odd, the Welch bound can be improved for binary 
sequences [10] into
0max > 1 + 2n/2
and hence the small set of Kasami sequences is an optimal set with respect to 
this bound.
As before the alternative description of the small set of Kasami sequences 
[6] can be, for a a primitive element of F2n , taken as
{t-2n|2(at) + Tr2n/2|2(bad) | d = 2n/2 + 1, b e F2n/2).
The definition of the large set of Kasami sequences is more involved to 
describe although uses concepts already introduced. Briefly, let h(x) be a 
primitive polynomial of degree n that generates the m-sequence u of length 
and period 2n — 1. Let v be the decimated sequence v = u[s(n)] where, as 
before s(n) = 1 + 2n/2, a sequence of period 2n/2 — 1 (an m-sequence) by the 
polynomial h (x), a polynomial of degree n/2, and w the decimated sequence 
w = u[tnn]] where t(n) = 1 + 2^(n +1)/21, generated by the polynomial h (x) 
of degree n, a sequence of period 2n — 1. The large set of Kasami sequences 
then is generated by the polynomial h(x)h (x)h (x) and is a set of sequences 
of size
23n/2 + 2n/2 if n = 2 (mod 4)
23n/2 + 2n/2 — 1if n = 0 (mod 4)
and the sequences take correlations in the set {—1, —t(n),t(n) — 2, — 
s(n),s(n) — 2} with © max = t(n).
https://doi.org/10.1017/9781009283403.015 Published online by Cambridge University Press

References
379
Numerous other sets of sequences appear in the literature including the 
basic references [6, 10]. The volume [4] is an excellent volume on this topic as 
well as the references therein.
Comments
The founder of this area of sequence design was Solomon Golomb of USC, 
author of the influential volume [3] which generations of communications 
researchers were familiar with. The follow-on volume [4] has also proved 
a valued contribution as is the more recent [5]. The beautiful little volume 
[8] has an elegant and useful approach to the description and properties of 
m-sequences (as well as finite field theory) and was a valuable reference 
for this chapter. There are numerous survey articles on the general topic of 
sequence design for low correlation. The one by Tor Helleseth [6] is a readable 
and comprehensive treatment. The research article [10] contains a wealth of 
information on such sequences not found elsewhere.
References
[1] Gold, R. 1967. Optimal binary sequences for spread spectrum multiplexing 
(Corresp.). IEEE Trans. Inform. Theory, 13(4), 619-621.
[2] Gold, R. 1968. Maximal recursive sequences with 3-valued recursive cross­
correlation functions (Corresp.). IEEE Trans. Inform. Theory, 14(1), 154-156.
[3] Golomb, S.W. 1967. Shift register sequences. Holden-Day, San Francisco, CA.
[4] Golomb, S.W., and Gong, G. 2005. Signal design for good correlation. Cambridge 
University Press, Cambridge.
[5] Goresky, M., and Klapper, A. 2012. Algebraic shift register sequences. Cambridge 
University Press, Cambridge.
[6] Helleseth, T., and Kumar, P.V. 1998. Sequences with low correlation. Pages 1765­
1853 of: Handbook of coding theory, Vol. I, II. North-Holland, Amsterdam.
[7] Kasami, T. 1966. Weight distribution formula for some class of cyclic codes. 
University of Illinois, CSL Technical Report R 285.
[8] McEliece, R.J. 1987. Finite fields for computer scientists and engineers. The 
Kluwer International Series in Engineering and Computer Science, vol. 23. 
Kluwer Academic Publishers, Boston, MA.
[9] Sarwate, D. 1979. Bounds on cross-correlation and autocorrelation of sequences 
(Corresp.). IEEE Trans. Inform. Theory, 25(6), 720-724.
[10] Sarwate, D.V., and Pursley, M.B. 1980. Cross-correlation properties of pseudo­
random and related sequences. Proc. IEEE, 68(5), 593-619.
[11] Welch, L. 1974. Lower bounds on the maximum cross-correlation of signals 
(Corresp.). IEEE Trans. Inform. Theory, 20(3), 397-399.
https://doi.org/10.1017/9781009283403.015 Published online by Cambridge University Press

15
Postquantum Cryptography
Interest in this chapter is in giving some background on the history of 
classical public-key cryptography and the influence the advent of quantum 
computing has had on it, leading to the present situation of postquantum 
public-key cryptography [16]. In particular an attempt is made to understand 
how the quantum computation model is able to reduce certain problems with 
exponential complexity on a classical computer to polynomial complexity on 
a quantum computer. Although it does not involve coding, it does set the stage 
for the next chapter which overviews the notions of quantum error-correcting 
codes.
Public-key cryptography arose in the 1970s with seminal papers of Rivest, 
Shamir and Adelman [26] and Diffie and Hellman [6] based on the assumed 
difficulty of factoring integers and of finding logarithms in certain finite fields, 
respectively. Importantly, it led to the notion of asymmetric-key cryptography, 
cryptosystems with different enciphering and deciphering keys. Such systems 
led to the notions of secure exchanges of keys over a public network and of 
digital signatures. With a digital signature, a document can be “signed” by 
attaching several bytes which give assurance that only the assumed sender 
could have sent the message.
It will be useful to note the role of the National Security Agency (NSA) 
and the National Institute of Standards and Technology (NIST) in this work. 
The NSA, whose budget and workforce size are officially secrets (although 
estimates of both are found on the Internet), has a wide mandate in intelligence 
and counterintelligence and security and in the protection of US communi­
cation networks and information systems. It was noted on a Wikipedia page 
the NSA is likely the world’s largest employer of mathematicians. NIST has 
developed a suite of protocols which have proved invaluable to corporate 
users and individuals involved with communications security around the world. 
These are referred to as Federal Information Processing Standards (FIPS) 
380
https://doi.org/10.1017/9781009283403.016 Published online by Cambridge University Press

15 Postquantum Cryptography
381
and cover several areas (not only security). The standards for a particular 
cryptographic function are often initiated by an open call to academic and 
corporate cryptographic groups around the world to propose algorithms which 
are gathered on the NIST website. A period of evaluation and discussion 
follows by which finalists are decided upon. The NSA lends their expertise, 
which is considerable, to this process and is involved in the final selection and 
suggested parameter selection.
Lurking in the background of public-key cryptography are two associ­
ated algorithms, those of symmetric-key encryption algorithms and of hash 
functions which are used by virtually all such algorithms. A symmetric-key 
algorithm uses a key, usually several bytes long, and operates on blocks of 
data ofa standard size to produce encrypted blocks of data, usually of the same 
size. These are very efficient algorithms capable of high data speeds. They are 
decrypted at a receiver with the same key. The symmetric-key system in almost 
universal usage is the Advanced Encryption Standard (AES) which uses key 
sizes and data block sizes of between 128 and 256 bits. The FIPS Pub 197, 
2001 concerning AES gives a detailed description of the algorithm as well as 
a set of configurations or modes in which the algorithm can be used. Typically 
public-key systems are computationally expensive so are used only to establish 
a common key or signature while a symmetric-key encryption algorithm such 
as AES is used for the actual encryption once the common key is established.
Another workhorse of the cryptographic world is the hash function, the 
Secure Hash Algorithm-2 (SHA-2), which supports a variety of block sizes. 
The notion of a hash function is to produce a digest of a long message. The 
digest might be a word of 256 bits, such that if any bit or bits of the message 
it operates on - which may be several gigabytes - are changed, many bits of 
the digest (hash) will be changed. When attached to a message, a receiver may 
produce a hash value from the received message and compare it to the received 
hash to justify the message was not altered in transmission. They also play a 
central role in digital signatures. A variety of word and block sizes for the hash 
function standard are specified in FIPS Pub 180-2. There is also SHA-3.
The reader is reminded that confidence in the security of most cryptographic 
systems is only established after a large number of capable cryptanalysts with 
enough resources have tried breaking the systems for a sufficient length of 
time. Even then there is always the possibility some researcher with a clever 
new direction of attack may render the system useless.
The advent of quantum computers has had a profound impact on crypto­
graphic practice. Eminent physicists in the 1980s considered that quantum 
computers might offer computation models that render them capable of provid­
ing results not achievable by a classical computer. However, it was the work 
https://doi.org/10.1017/9781009283403.016 Published online by Cambridge University Press

382
15 Postquantum Cryptography
of Shor [27] who devised algorithms that could run on a quantum computer 
that reduced classical cryptographic algorithms useless. The capabilities of 
such a computer were assumed, based on physical models, since no such 
computer actually existed at that time. This led the NSA to almost immediately 
recommend all future development effort be on algorithms that resist such 
a possibility which has led to postquantum cryptography. The term refers 
to cryptographic algorithms that appear not to run significantly faster on a 
quantum computer than on a classical one. A suite of such postquantum 
algorithms has evolved over the past few years and is discussed in Section 
15.3. As with classical algorithms there seems no guarantee that a quantum 
algorithm could not be found for such postquantum or quantum-resistant 
algorithms to render them susceptible, given enough effort and time.
The next section gives a brief overview of the classical public-key algo­
rithms that were rendered suspect by Shor’s algorithms, serving as a reminder 
of the cryptographic protocols of interest. Section 15.2 attempts to describe the 
quantum computation model that allowed Shor to devise quantum algorithms 
for the problems of integer factoring and discrete logarithms in a finite field. 
These quantum algorithms reduced the complexity of these problems from 
subexponential on a classical computer to polynomial on a quantum computer. 
The final Section 15.3 describes a few of the algorithms that are currently 
viewed as quantum-resistant, i.e., no quantum algorithms are known for them 
that would allow them to run faster on a quantum computer than on a classical 
one. They are regarded as postquantum algorithms.
15.1 Classical Public-Key Cryptography
The section recalls basic elements of the role of discrete logarithms and 
integer factorization in certain public-key cryptographic protocols. This serves 
as a precursor to the pivotal work of Shor in showing how their complexity 
under quantum computation is reduced from subexponential to polynomial - 
and hence useless from a cryptographic point of view. In general, public­
key cryptography provides a means for key exchange, digital signatures and 
encryption, although the number of ingenious other cryptographic protocols 
that have arisen out of these ideas is almost without limit.
Consider first the basic Diffie-Hellman key exchange algorithm [6] con­
taining perhaps the first idea of what was possible for communicating in secret 
over a public network. Let Fp be a prime field, integers modulo a large prime 
p, and let a be a generator of a large cyclic subgroup G = {a} c Fp of order 
q for some prime q. Thus q | (p — 1). Let ft = aa, for some positive integer 
a, a quantity that is efficient to compute using, say, a square and multiply 
https://doi.org/10.1017/9781009283403.016 Published online by Cambridge University Press

15.1 Classical Public-Key Cryptography
383
technique. The discrete logarithm of p to the base a then is loga(P) = a. 
Given p, a and p then, the problem is to determine the integer a, the so-called 
discrete logarithm problem (DLP). The thesis is that finding such discrete 
logarithms is much harder than exponentiation, as will be commented on 
later.
Assuming the DLP is hard, what can this be used for? Diffie and Hellman 
[6] suggested the following groundbreaking idea. Suppose Alice and Bob wish 
to communicate in secret over a public network with no prior communications. 
They publicly share a modulus (prime) p and large subgroup generator a and 
its order q . Alice chooses a secret exponent a and sends aa to Bob. Bob 
chooses a secret exponent b and sends ab to Alice. They each compute aab 
from which the common key is derived. The common key might then be used 
for a symmetric-key algorithm such as AES. The security of the scheme rests 
on the presumed difficulty of the noted DLP which will be commented on 
shortly. Thus an eavesdropper, seeing aa and ab in the transmissions, is unable 
to compute either a, b or aab (modulo p).
For n > 2 an arbitrary positive integer, Z* is the set of integers in [n] 
relatively prime to n and ^(n) =| Z* |, ^(n) the Euler’s totient function. The 
set Z*n is a multiplicative group mod (n). The RSA cryptographic system relies 
on Euler’s theorem which states that for a e Zn
a^nn = 1 (mod n).
To introduce the RSA public-key cryptosystem (standing for its inventors 
Rivest, Shamir and Adleman) and the notion of a digital signature, let n = pq 
be the product of two large primes (chosen carefully with certain properties), 
and $(n) = (p - 1 )(q - 1). Notice that knowing n is a product of two primes 
and the value of $(n) allows one to factor n. An encryption exponent e is 
chosen so that (e,$(n)) = 1, i.e., relatively prime to $(n), which allows a 
unique decryption exponent d to be chosen so that ed = 1 mod $(n).
RSA encryption is as follows. For user A, the integers nA = pAqA, the 
product of two primes, and eA are chosen by user A and made public (for 
each system user). Here eA is the user A public key, while the corresponding 
decryption exponent, dA, computed by user A, knowing the factorization of 
nA , is kept secret. For Bob to send an encrypted message to Alice he retrieves 
her public exponent, say nA and eA and for the message m, assumed to be an 
integer in the range (0,nA - 1) sends Alice r = meA mod nA. On receiving 
this encryption, Alice computes m = rdA = meAdA = m mod nA thus 
retrieving the message.
Note that if the integer nA can be factored, then it is possible to determine 
$(nA) and hence determine dA from eA and the system is insecure. Thus the 
security of the system rests on the difficulty of factoring large integers.
https://doi.org/10.1017/9781009283403.016 Published online by Cambridge University Press

384
15 Postquantum Cryptography
To recall the notion of a digital signature for a message m, assume that 
m e [1,n - 1]. In practice one would either block the message into suitable 
subblocks or employ some publicly known redundancy function with the 
appropriate properties. For Bob to sign the message m to Alice he uses his 
secret key dB and computes the signature
5 = mdB mod nB
and sends the pair (m, s) to Alice. On reception Alice retrieves Bob’s public 
key eB, nB and computes
5eB = mdB eB = m mod nB
which is compared with the received message m and accepted if they match. 
In practice of course numerous details are to be accounted for but the essence 
of the scheme is as above. Details of the system are to be found in the 
encyclopedic reference [19]. The essential notion of a digital signature is that 
only Bob could have produced the signature 5 for the message m, since only 
he knows his secret key dB. However, any user, given the message m,5,nB and 
the public key eB can verify it.
It is clear that the DLP can be implemented in any finite group, and of 
course, in an additive or multiplicative group. A particularly successful group 
in terms of its complexity characteristics has been the additive group of an 
elliptic curve. Although somewhat complex to discuss, a brief introduction to 
this aspect of cryptography is given. An elliptic curve (EC) over a field of 
characteristic 2, q = 2n , has a representative form [4]
E(Fq): y2 + xy = x3 + a2x2 + a6, a2 e{0,Y}, a6 e F*, q = 2n,
where y is a fixed element of trace unity over F2, i.e., Trq|2(y) = 1. A point 
P = (x, y) is on the curve if the coordinates satisfy the equation. The set of all 
points on the curve E(Fq) form an additive (and Abelian) group of size
| E(Fq ) |= q + 1 - t
where t is the Frobenius trace of the curve which satisfies 11|< 2^q. If P1 = 
(x1,y1) and P2 = (x2,y2) are on the curve, then from the form of the equation, 
by “drawing a straight line” through the points will intersect the curve in a 
unique third point P3 = (x3,y3 ). In a field of characteristic 2 the negative of a 
point P1(x1,y1) is -P1 = (x1,x1 + y1) and the sum of a point and its negative 
gives “point at infinity,” the identity of the group. Intuitive addition formulae 
can be given for the addition of two points on the curve [4] but are omitted.
Each point on the curve has an additive finite order. The DLP on elliptic 
curves (ECDLP) is then: given P e E(Fq ) and Q = aP = P + P + 
https://doi.org/10.1017/9781009283403.016 Published online by Cambridge University Press

15.1 Classical Public-Key Cryptography
385
• • • P {a times}, find a. The surprising fact is that the ECDLP problem is a 
much harder problem on an elliptic curve than the DLP in a finite field of 
comparable size and this led to elliptic curve being widely adopted during the 
1990s, especially for small devices. Elliptic curve algorithms for encryption, 
signatures and key exchange are available. The digital signature algorithm 
(DSA) in both prime fields and in fields of characteristic 2, as well as the 
elliptic curve DSA (ECDSA), is described in the digital signature standard 
(DSS) FIPS Pub 186-4 (2013).
To discuss the complexity of the above problems define the function [4, 19]
Ln(a,c) = exp (c(logn)a(loglogn)( 1-a))
and note that when a = 1 the function is exponential in log n and when it is 
zero it is polynomial. For in-between values it is described as subexponential 
for intuitive reasons.
The fastest algorithms for both integer factorization and finding discrete 
logarithms are versions of the number field sieve. In a prime field they have 
a complexity of the form Ln (1/3,c) for a small number c, typically less than 
2, for n the size (number of bits) of the integer or prime. Recent work on the 
finite field DLP, however, found weaknesses (lower complexity) for certain 
cases. For the discrete logarithm a technique referred to as index calculus was 
found and explored extensively.
The ECDLP, however, had a complexity that is square root of the field 
of definition of the curve. The best-known algorithm for achieving this 
complexity is the somewhat naive so-called baby-step-giant-step algorithm 
(BSGS). Briefly, in this algorithm, for a base point P of order n on an elliptic 
curve, a table of size p = ^n is created containing iP,i = 1,2,...,p. For 
a given point Q whose log to the base P is required, the point R = pP is 
successively added until it is a member of the table at which point its log can 
be calculated. Such an algorithm has square root complexity, i.e., if the order 
of the finite field of the elliptic curve is q = 2n, the complexity of the BSGS 
algorithm is O 2n/2 .
Comparing the complexities for an RSA modulus (product of two primes) 
with 4096 bits, an ECDLP over a field with 313 bits gave approximately the 
same level of security, a consideration which led to the popularity of the elliptic 
curve cryptography for certain applications, although arithmetic on the elliptic 
curve is more complex than for integers.
As will be discussed in the next section, the discovery of a quantum 
algorithm for integer factorization and for discrete logarithms (which were 
shown to include elliptic curve versions) by Shor [27] led the NSA/NIST 
https://doi.org/10.1017/9781009283403.016 Published online by Cambridge University Press

386
15 Postquantum Cryptography
in 2015 to promote the use of postquantum algorithms (also referred to 
as quantum-resistant). It resulted in the relegation of the above “classical” 
algorithms to limited usage. In essence, the possibility of a quantum computer, 
led to the demise of the above classical public-key systems.
15.2 Quantum Computation
In order to appreciate the need for postquantum cryptography, some knowledge 
of the techniques of quantum computation is needed, that is, an appreciation 
of how quantum computers acquire their tremendous speedup for certain 
algorithms. It is difficult to delve into the intricacies of quantum mechanics 
to give a detailed description but perhaps an intuitive understanding of the 
situation might be possible. The next few pages attempt such an overview.
Before beginning it should be mentioned that the construction of quantum 
computers that achieve the theoretical gains predicted by the algorithms 
mentioned has been a daunting task. Many authors expressed skepticism that 
such computers with sufficient memory and sufficient stability to yield useful 
results on the problems posed will be possible. However, the expected gains 
are so tempting that numerous large corporations have invested huge resources 
to make such computers a reality and sufficient progress has been made that it 
now appears likely that such computers may well appear in the not too distant 
future.
A word on quantum algorithms is in order. It is not clear which computa­
tional problems can be formulated to derive an effective quantum algorithm. 
Indeed it is questioned in [29] why there are so few good quantum algorithms. 
In general it seems a difficult problem to pose such problems and generate 
suitable algorithms for quantum computation.
However, there have been some notable successes and perhaps first and 
foremost among these are the quantum integer factorization and quantum 
DLPs of Shor [27, 28]. These reduced their complexity from subexponential on 
a classical computer to polynomial (on input lengths) on a quantum computer 
and prompted their demise as trusted public-key algorithms. The quantum 
algorithms changed the practice of cryptography and data security techniques 
forever. It was also shown that even the elliptic curve DLP was included in this 
speedup.
The integer factorization problem used two subalgorithms: (i) that of finding 
the order of an element modulo a large integer N (i.e., for x e [N] find 
the least integer r such that xr = 1 (mod N)), and (ii) a quantum Fourier 
transform (QFT). While the entire integer factorization method of Shor has 
https://doi.org/10.1017/9781009283403.016 Published online by Cambridge University Press

15.2 Quantum Computation
387
detail that requires lengthy development, the QFT is sufficiently simple with 
an impressive performance that an intuitive overview of it will be used for 
illustrative purposes here.
Among the other notable quantum algorithms that achieve speedup are the 
Simon algorithm, Grover’s algorithm and the hidden subgroup problem. These 
problems are briefly described.
For Simon’s problem we are given a function f(x): {0,1}n ^ {0,1}n as a 
black box where given x e {0,1}n the box produces f(x). We are also given the 
assurance that for all x, j e {0,1}n there exists an i* e {0,1}n such that either 
f (x) = f(y) or f (x) = f(y ® i*). The goal is to determine the string i* by 
making queries to the black box. This problem is exponential in n on a classical 
computer and polynomial (O(n3) or less [11]) on a quantum computer.
Grover’s algorithm is a search algorithm that is often cast as a database 
search algorithm. This is equivalent [30] to the formulation of being given a 
function f(x): {0,1}n ^ {0, 1} and the assurance that for exactly one x0
1,
0,
if x = x0
if x = x0.
The problem is to determine x0. This problem is O(2n) on a classical computer 
and O(^2n) on a quantum computer, a quadratic speedup.
The hidden subgroup problem [11] is: given a group G,asetX and 
subgroup H < G and a function f: G ^ X that is constant on cosets of 
the subgroup H - but has different values on different cosets, determine the 
subgroup H or a set of generators for it. The quantum algorithm for the case 
the group G is Abelian is polynomial in log | G |. A quantum algorithm for 
non-Abelian groups does not appear to be known. It is interesting [11, 23] that 
this problem has strong ties to numerous other standard problems of interest, 
including Shor’s algorithms for discrete logarithms and integer factorization.
The remainder of the section contains a discussion of quantum computing 
and quantum gates with a view to obtaining an intuitive discussion of how they 
lead to the impressive speedups they do for certain algorithms. The QFT is 
given as an example of a speedup from exponential complexity on a classical 
computer to polynomial on a quantum computer in the log of the size of the 
transform.
In classical computing the fundamental unit is the bit that takes on value of 
either 0 or 1. In quantum computing the corresponding unit is the qubit which 
is viewed as a superposition of a bit, a vector in two-dimensional complex 
space of the form
| 0) = a| 0)+ b| 1), 
|a |2 + |b|2= 1, a,b e C.
https://doi.org/10.1017/9781009283403.016 Published online by Cambridge University Press

388
15 Postquantum Cryptography
The notation | •) is referred to as “ket.” Notice that the notion of qubit, 
as a projective quantity, is invariant under scalar (over C) multiplication. 
The quantities 10) and 11) are viewed as orthogonal two-dimensional (with 
dimensions labeled with 10) and 11)) column vectors in the space, which form 
a basis of the space:
10>= 0 
11>= 0 
। $ >=
If one measures the qubit the outcome is not deterministic but rather random 
with a probability of | a |2 of measuring the result 10) and | b |2 of measuring 
the result 11).
For a two-qubit system, the nature of quantum mechanics dictates that the 
joint quantum state system is the tensor product of the individual quantum state 
systems. Thus the orthonormal basis vectors (over C) of the two-qubit system 
can be expressed as
100) = 10) ®| 0) = |0>| 0), 101) = 10) ®| 1) = 10)| 1),
110) = | 1)0| 0) = | 1)| 0), 111) = | 1)0| 1) = | 1)| 1)
with these four basis states corresponding to unit vectors in C4 as
| 00) = | 0) 0 | 0) =
1
0
0
0
0
1
0
0
, | 01) =
, 110) =
, 111) =
00
00
10
01
1
0
0
1
0
A general element in the quantum state space of the two-qubit system (four­
dimensional over C) can be expressed as
a 1| 00) + a2| 01) + a3| 10) + a4111), ai e C, ^2 ।ai |2= 1
and, as with the single-qubit system, measuring this state will produce, 
e.g., state 110) with probability | a3 |2. Such states can be viewed as four­
dimensional column vectors over C as above. Operations in a two-qubit system 
correspond to 4 x 4 unitary matrices with rows and columns labeled with the 
states 100), 101), 110) and 111), respectively. Thus 110) = (0,0,1,0)t.
In general an n-qubit system will have a quantum state space with 2n basis 
states. One of these can be expressed, e.g., as
| । 0 11)®|0)• • • 0 | 1) = 1110... 1)
and to describe a general element in the quantum state space, identify the 
integer x, 0 < x < 2n - 1 with its binary expansion in Sn = {0,1}n, and
2n-1 
2n-1
' ax | x} 
' ax | x}, 
| ax | = 1, 
ax e C 
https://doi.org/10.1017/9781009283403.016 Published online by Cambridge University Press

15.2 Quantum Computation
389
and measuring the state of the system will yield the state | x) with probability 
| ax |2. Such states can be viewed as column vectors over C of dimension 2n . 
We use position in the binary n-tuple to distinguish the n qubits. The elements 
| x),x e Sn are viewed as orthonormal column vectors of dimension 2n over 
C. The notation (x |,x e Sn (called “bra”) will be viewed as the dual vector to 
| x), a row vector of the complex conjugates of the column vector x. Further 
the notation (a | b),a,b e C2n for two superimposed states will indicate an 
inner product, i.e.,
a = £ ax | x ),b = £ by | y) 
^ 
(a | b ) = £ ax bx e C
where the states | x} and | y},x = y are orthogonal and their inner product 
{x | y} = 0. There is also the notion of an outer product operator expressed as 
| w}{v |, with v, w in the inner product spaces V, W, respectively. Define the 
outer product operator as:
|w)(v |; V —> W 
| v') ^ (v|v') | w).
A state | $} that cannot be written as a tensor product | a )| b} of two single­
qubit states is referred to as an entangled state. It can be shown [23] that
1^2 (| 00)+| 11))
is an entangled state.
As noted it requires 2n - 1 complex numbers to specify a state in the pro­
jective quantum state space and the state space is C2n (actually the unit ball in 
this space). The exponential dimension of the quantum state space with number 
of qubits contributes to the computational power of quantum computing. The 
quantum state space is taken to be a Hilbert space of dimension 2n , i.e., a 
complex vector space of dimension 2n with an inner product that is complete - 
that every Cauchy sequence of vectors converges. For this discussion it is 
enough to assume a complex vector space with the usual inner product.
The laws of quantum mechanics allow only computational operations or 
transformations of the quantum state space that can be modeled as unitary 
matrices over the complex numbers. A unitary matrix (also referred to as 
Hermitian adjoint) U is unitary iff its inverse is equal to its conjugate 
transpose, i.e.,
U-1 = U * t = Ut
where for compatibility with much of the quantum literature * indicates 
complex conjugate and t indicates transpose and t conjugate transpose. Such 
https://doi.org/10.1017/9781009283403.016 Published online by Cambridge University Press

390
15 Postquantum Cryptography
matrices map points on the unit ball in C2n to points on the unit ball. In addition 
their rows and columns form orthonormal basis sets as do the set of their 
eigenvectors. For reference recall a Hermitian matrix H is one where H = H^.
Just as for classical logic gates, a quantum logic gate can be expressed by 
its truth table. However, [28] not all quantum logic truth tables correspond to 
realizable quantum gates as many do not correspond to unitary matrices. A 
quantum logic gate on a single qubit corresponds to a 2 x 2 unitary matrix 
(with rows and columns labeled with the basis states 10) and 11)) and one 
on two qubits to a 4 x 4 unitary matrix (with rows and columns labeled with 
the basis states 100), 101), 110) and 111)). A few examples will illustrate. The 
unitary matrices of some basic single-input quantum logic gates including the 
Pauli gates and the Hadamard gate are shown in Equation 15.1.
0
X= 01
10 ,Y= i0 ,Z= 10
,H=
11
(15.1)
0
1
1
1
Pauli-X gate Pauli-Y gate Pauli-Z gate Hadamard gate
These matrices will play a prominent role in the next chapter on quantum 
error-correcting codes, where they will denote error operators. Specifically in 
an n-qubit state | a 1 a2 • • • an) the operators are n-fold tensor products of these 
matrices. For example, a bit flip in the i-th qubit is designated as Xi and a 
phase flip by Zi . Thus
X1 & 12 & Z3| 111) = X1Z3| 111) = -| 011).
Further properties of the set of operators (or 2 x 2 matrices) are developed in 
the next chapter. It is the nature of these operators that requires a different 
approach to the construction and decoding algorithms for quantum error­
correcting codes from classical coding.
The notion of a controlled gate where inputs include control bits and target 
bits, where certain actions are taken on the target bits iff certain conditions hold 
on the control bits. An important example of this is the simple 2-input 2-output 
CNOT (controlled NOT) gate whose unitary matrix is shown in Equation 15.2 
- the target bit is flipped iff the control bit is 1 - rows and columns labeled with 
100), 101), 110) and 111). The quantum logic operation for the gate would be 
Ucnot : | c)| t) —> | c)| t ® c):
UCNOT
1000
0 10 0
0 0 0 1
0010
(15.2)
Thus if the control bit c is 1, the target bit t is flipped to t ® 1.
https://doi.org/10.1017/9781009283403.016 Published online by Cambridge University Press

15.2 Quantum Computation
391
An important example of a three-qubit quantum logic gate is the Toffoli 
gate whose corresponding unitary matrix is given in Equation 15.3 (rows and 
columns labeled with 1000), 1001), 1010), 1011),..., 1110), 1111)). There are 
two control bits c1 and c2 and one target bit t and the quantum logic operation 
is Utoffoli: | c 1)| c2)| t) —> | c 1)| c2)| t ® (c 1 A c2)) - thus the target bit is 
flipped iff both control bits are set 1: thus the 8 x 8 matrix
10000000
01000000
00100000
00010000
UTOFFOLI =
00001000
. 
(15.3)
00000100
00000001
00000010
It is well known in classical logic that a logic function f: Sn ^ {0, 1}M 
can be realized using only AND, OR and NOT gates. Such a set is referred to 
as a universal set of gates. Equivalently [11] the set NAND and FANOUT is a 
universal set for such functions.
In a similar fashion there exists universal quantum gate sets in the sense 
[23] the set will be universal if any unitary operation can be approximated 
to an arbitrary level of accuracy using only gates in the universal set. Thus 
the set of all two-level unitary quantum logic gates is universal for quantum 
computation. Equivalently any unitary matrix can be decomposed into a 
product of 2 x 2 unitary matrices. Similarly the set of all single-qubit logic 
gates and CNOT gates is universal. Also, the set of Toffoli logic gates by itself 
is universal as is the set of Fredkin gates, not considered here.
From the above comments a quantum algorithm can be thought of conceptu­
ally as a set of horizontal 2n lines representing the states of an n-qubit system. 
As time proceeds one inserts unitary operations on one-qubit (a 2 x 2 unitary 
matrix operating on two of the lines) or a two-qubit operation (a 4 x 4 unitary 
matrix operating on four of the lines). For example, suppose Ai,j is a 4 x 4 
unitary matrix representing a quantum logic operation operating on two qubits 
i and j of the n-qubit state. In a two-qubit system the rows and columns would 
be indexed with the states 100), 101), 110) and 111). In the n-qubit system the 
rows and columns of the 4 x 4 matrix Ai,j would be indexed with the states
| b 1 b2 ... bi-10bi+1.. .bj-10bj+1 ...bn) 
| b 1 b 2 ... bi-10 bi+1.. .bj-11 bj+1 ...bn) 
| b 1 b 2 ... bi-11 bi+1.. .bj-10 bj+1 ...bn) 
| b 1 b2 ... bi-11 bi+1... bj-11 bj+1 ...bn) 
https://doi.org/10.1017/9781009283403.016 Published online by Cambridge University Press

392
15 Postquantum Cryptography
and the remaining 2n - 4 of the lines remain unchanged. The logic operation 
on these two qubits would then be realized by a 4 x 4 unitary matrix operating 
on these four lines. Finding such sequences of operations to effect a particular 
computation/algorithm is the challenge of quantum computing.
It is somewhat surprising that it is not possible to clone an unknown 
quantum state. The following argument [23] is instructive. Suppose the initial 
state is | $} 0 15} where 5 is one of the base orthonormal states and $ is an 
unknown quantum state. It is desired to find an operation (unitary) U such that
U: | $ > 0 | 5) -+ | $ > 0 | $)
and assume this can be done for two such states | $} and | ^} so
U(] 0 > 0 | 5» = | 0 >0 | 0 )
U (| ^} 0 | 5)) = | ^} 0 | ^}.
Taking inner products of the LH sides and RH sides of these equations yields
<0 | ty} = ({<fr | ty})2 ^ 
$ | ty) = 0 or 1
which implies either | $} = | ^} or the states are orthogonal. Thus only states 
which are orthogonal can be cloned and a general cloning operation is not 
possible. Thus [30] it is not possible to make a copy of an unknown quantum 
state without destroying the original, i.e., one can cut and paste but not copy 
and paste.
The QFT is discussed as an example where dramatic gains with quantum 
computing over classical computing can be realized. Recall the classical 
discrete Fourier transform (DFT). Only the case of N = 2n is considered - 
the extension to arbitrary N is straightforward. The DFT of the vector x = 
(x0,x 1,... ,xN-1) e CN isy = (y0,.. .,yN-1) where
1 N-1
yk 
— xj\ Xjj k = 0,1,...,N - 1, WN = e2 ni/N
NN 
Nj =0
and WN an N-th root of unity in C. The classical complexity of taking DFTs is 
O(NlogN)= O(n2n).
Let |x},x = 0, 1,2,... ,N - 1 be the standard orthonormal basis of the 
quantum state space of an n-qubit system, indexed by the elements of {0, 1}n . 
The QFT then is the map, considered as the transformed basis | ^k}, k = 
0, 1 , 2, . . . ,N - 1 = 2n - 1 map, where
1 N-1
QFT: | j) -+ | ^j) = — j wN| k), j e {0,1}n.
N k=0
https://doi.org/10.1017/9781009283403.016 Published online by Cambridge University Press

15.2 Quantum Computation
393
Equivalently [23] the QFT is the map on arbitrary superpositioned states
E xj i j )-^ E yk । k>, yk ' E 
N
j=0 
k=0 
NN £=0
where {xj } and {yk} are DFT pairs. Note that the QFT matrix, denoted FN = 
(fij), fij = Jn^n is unitary.
In order to implement a QFT on n qubits (quantum state space of dimension 
N = 2n) it can be shown that only two types of operations are needed [23] 
which are:
(i) Gate Rj which operates on the j-th qubit of the quantum computer:
1
(rows and columns indexed by 10) and 11)) which operates on the j-th qubit; 
and
(ii) Gate Sjk which operates on qubits i and j<k 
Sjk =
100 
0 1 0 
001 
000
0
0
0
ei^ - j
where 0k- j = n/2k-j and rows and columns indexed with 100), 101), 110), 
111), respectively.
The QFT is then implemented as the sequence of these two operations:
Rn-1 Sn-2,n-1 Rn-2 Sn-3,n-1 Sn-3,n -2 Rn-3 • • • R1S0,n-1S0.n-2 • • • S0, 1R0
the R gates in decreasing order and the Sjk gates for k>jonly. There are 
n(n - 1)/2 S gates and n R gates and since each such quantum logic gate has a 
constant complexity the overall complexity of the QFT is O(n2). This is com­
pared to the DFT complexity mentioned of O(n2n), a dramatic exponential 
order difference. It is hoped the manner by which this computational speedup 
for the quantum case was achieved with the unitary matrices acting on the 
quantum states gives some indication of the difference between the classical 
and quantum computational models. The interested reader should consult [23] 
and [28] for a much more detailed and persuasive discussion than this sketch.
As with the classical DFT, the QFT is a fundamental workhorse for quantum 
algorithms, including the quantum integer factorization algorithm of Shor [27, 
28]. It is suggested that the above discussion of the QFT is representative of the 
huge complexity advantage of certain quantum algorithms over their classical 
counterparts. To date only certain problems, such as integer factorization and 
the DLP, have been found to take advantage of this speedup.
https://doi.org/10.1017/9781009283403.016 Published online by Cambridge University Press

394
15 Postquantum Cryptography
15.3 Postquantum Cryptography
Having discussed the impact of quantum computing on cryptographic practice, 
it remains to describe the current state of cryptography and in particular 
those algorithms that have been recommended for use to implement public­
key cryptography in this postquantum era. These are commonly referred 
to as postquantum algorithms or quantum-resistant algorithms. It should be 
mentioned that these algorithms are typically determined by extensive research 
efforts to find quantum algorithms for their solution rather than by observing 
any particular feature of them. It seems possible that such a postquantum 
algorithm could migrate into the class of quantum-weak algorithms should 
some clever researcher discover the right technique.
Below, four commonly accepted postquantum algorithms are described. 
There are others - such as isogenies of super-singular elliptic curves but the 
background required for a reasonable discussion of them is excessive.
Many symmetric-key encryption algorithms - such as AES with a suffi­
ciently large key/block size, and hash functions - such as SHA-2 (SHA-256) 
are regarded as quantum-resistant since no quantum algorithms to “break” 
these have yet been formulated.
Code-Based Public-Key Cryptosystems
McEliece [17] devised a simple and effective public-key encryption system 
that has withstood the test of intensive scrutiny and remains an interesting 
option for quantum-resistant cryptography. It appears that Goppa codes are 
particularly suitable for this application. The construction and basic properties 
of Goppa codes are recalled, e.g., ([15,17]). Let g(x) e Fqm [x] of degree t and 
let L = {a 1,... ,an} c Fqm disjoint from any zeros of g(x) in Fqm. Define a 
t x n parity-check matrix
g(a 1)— 1 
... 
g(an)—1 -
a 1g(a1)—1 ... ang(an)— 1
H=
...
...
...
_a 1—1 g(a 1)—1 ... a‘n—1 g(an)-
This is a parity-check matrix for an (n,k > n — mt,d > 2t + 1 )q t error­
correcting Goppa code, often denoted G = r(L,g). For consistency with 
previous systems chose q = 2.
With the above parameters n, k, t and associated generator matrix G fixed, 
user Alice A chooses a k x k binary nonsingular matrix SA and an n x n 
permutation matrix PA. The k x n generator matrix G*A = SAGPA is computed.
https://doi.org/10.1017/9781009283403.016 Published online by Cambridge University Press

15.3 Postquantum Cryptography
395
Alice’s public key then is (G*A,t) and secret key (SA,G,PA). For Bob to send 
a message m to Alice, assumed to be binary of length k, an error vector e of 
length n and weight t is chosen and the encrypted message c = mG* + e is 
sent. On receiving c Alice computes c* = cP-1 and decodes this binary n- 
tuple with the decoding algorithm for the Goppa code with generator matrix G 
to give m*. The original message is then computed to be m = m*SA-1.
L = a 1 v 1 + a2v2 + • • • + anVn,a 1 ,a2, ... ,an e Z C Rn.
Various attacks on this system have been proposed and none have proved 
successful. An obvious drawback to the system is the rather large number of 
bits in the public key. The parameters suggested in [18] are n = 1024,t = 50 
- hence m = 10 and k > 1024 — 500 = 524. Thus the public key size is 
approximately the number of bits in the generator matrix or 1024 x 524, a 
large number. Thus while the encryption operations are efficient, the size of 
typical public keys is a drawback.
Niederreiter [22] proposed an interesting variant of this scheme. Let C = 
(n,k,d > 21 + 1 )q be a t-error correcting over Fq, e.g., the Goppa code noted 
above. Again we could choose q = 2 for consistency but the argument is the 
same for any q .LetH be a full-rank (n - k) x n parity-check matrix for the 
code and note that, by the error-correcting properties of the code for any two 
distinct messages m 1 = m2 e F2, each of weight < t, Hm 1 = Hm2. This 
is the basis of the Niederreiter public-key encryption system, namely: choose 
an (n - k) x (n - k) nonsingular matrix R and an n x n permutation matrix 
P and let H* be the (n - k) x n matrix H* = RHP. The public key for the 
system is (H*,t) and the secret key is (R, P). All matrices will be subscripted 
by a user ID.
It is assumed all original messages m are of weight < t which can be 
arranged by a variety of techniques. For Bob to send Alice the message 
m,wH (m) < t he retrieves Alice’s public key H*A and sends the encryption 
H*Amt = yt. To decrypt this encryption Alice computes RA-1H*Amt = 
HAPAmt = yt . Since PAmt is of weight < t, it can be decoded to z and 
the original message is recovered by z(PtA)-1.
Other aspects of quantum-resistant code-based public-key systems can be 
found in [14], [8] and the chapter by Overbeck et al. in [3].
Lattice-Based Public-Key Cryptosystems
Let v1,v2,...,vn be linearly independent vectors in the n-dimensional real 
vector space Rn. A lattice will then be a Z-linear combination of these vectors, 
namely
https://doi.org/10.1017/9781009283403.016 Published online by Cambridge University Press

396
15 Postquantum Cryptography
Any set of n linearly independent vectors in the lattice generates L and all such 
sets have n vectors in them, the dimension of the lattice. A lattice is a discrete 
subgroup of Rn . Any two bases of the lattice [10] are related by an integer 
matrix with determinant ±1.
The fundamental domain of a lattice with basis {v1,...,vn} is the set of 
points
F = fl1V1 + ••• + fnVn, 0 < fi< 1}
and it is easy to see that any point in z e Rn can be expressed as z = w + f for 
w e L and f e F, i.e.,
Rn = U (w + F)
weL
and the translations of the fundamental domain by lattice points cover (tile) the 
space Rn .
Let V be the n x n matrix with vi as the i -th column. Then the volume of 
the fundamental domain F,vol(F) is given by
vol(F) =|det(V)|
and it can be shown that
vol (F) <| V11...| Vn | , 
| Vi |= 
V2j} 
.
j=1
Indeed ([10], corollary 6.22), every fundamental domain of a lattice has the 
same volume and hence det(V) is an invariant of the lattice, independent of the 
choice of basis of the lattice.
There are numerous interesting and important problems associated with 
lattices and an excellent introductory treatment of them is [10]. The use 
of lattices in cryptography centers largely around two fundamental lattice 
problems which, for the right parameters will be presumed hard. Given a lattice 
L, they are ([10], section 6.5.1):
Definition 15.1 The shortest vector problem (SVP) in a lattice L is to find a 
shortest nonzero vector in the lattice.
Definition 15.2 The closest vector problem (CVP) is, given a vector w e 
Rn, w e/ L, to find a vector V e L closest to w.
There are other interesting problems associated with lattices - e.g., one 
might ask only for approximate solutions to the above problems, solutions that 
fall within some prescribed bounds.
https://doi.org/10.1017/9781009283403.016 Published online by Cambridge University Press

15.3 Postquantum Cryptography
397
Notice in each case the solutions may not be unique. The difficulty of 
solving these problems for a given lattice and basis, is largely a function of 
properties of the basis used to represent lattice points. Generally the more 
nearly orthogonal the basis vectors are, the more amenable the problems are to 
solution.
While our interest will be only in the public-key encryption system of 
[9], referred to as GGH (for its authors Goldreich, Goldwasser and Halevi) 
it should be noted there are many deep and fundamental theoretical results 
related to lattices. For example, Ajtai and Dwork [1], devised a scheme whose 
security could be shown to be provably secure unless an equivalent worst case 
problem could be solved in polynomial time, showing essentially a worst- 
case/average case connection and a celebrated result of complexity theory.
To discuss the GGH system let {v1,...,vn } be a basis for the lattice L and 
the V, the n x n matrix whose i -th column is vi. The following definition will 
be useful for the discussion:
Definition 15.3 For an n x n nonsingular matrix V define the orthogonality 
defect as
orth-defect = ^i=1 | vi |
|det (V) |
for |vi | the norm of the i-th column of V.
It is immediate that orth-defect(V) = 1 iff the columns of V are orthogonal 
and > 1 otherwise.
There are two important results on lattice basis reduction that should be 
noted. The first is, given a basis for a lattice, the LLL (for its authors Lenstra, 
Lenstra and Lovasz) [13] basis reduction algorithm operates on the basis to 
produce another basis that has relatively short and quasi-orthogonal basis 
vectors. This celebrated result has found critical applications to a variety 
of important lattice complexity problems and is relevant to the problems of 
interest here. The reader is referred to the original work for a detailed look at 
the algorithm and to obtain a better appreciation for the terms “relatively short” 
and “quasi-orthogonal.”
Another fundamental and useful work is that of Babai [2] who gave 
two simple algorithms for solving the CVP problem - up to a multiplica­
tive constant - for lattices with relatively short and quasi-orthogonal bases, 
rounding off and nearest plane algorithms. In particular the algorithms tend 
to be efficient for lattices with an LLL reduced basis (in particular quasi- 
orthogonal bases). Only the rounding off algorithm will be noted here. Suppose 
{v 1,..., vn} is a basis for the lattice L and let x = n= n=1 ai vi e Rn be an 
https://doi.org/10.1017/9781009283403.016 Published online by Cambridge University Press

398
15 Postquantum Cryptography
arbitrary element in Rn. Let ["ai J be the coefficient rounded off to the nearest 
integer. Then the point w = ^2n=1Fai J vi is a nearest lattice point up to a 
multiplicative constant that depends on the dimension n.
The impact of these results is that lattices that have a relatively short and 
nearly orthogonal basis tend to have efficient solutions to the CVP problem in 
dimensions up to several hundred. There are other intuitive reasons (apart from 
Babai’s result) and experimental results that indicate the CVP problem and 
approximate versions [9, 10] of them have a difficulty that increases with the 
orth-defect. Assuming this, a description of the public-key encryption system 
of [9] is of interest.
Suppose a lattice L is generated by the basis v1,...,vn that has a low orth- 
defect - the basis vectors are quasi-orthogonal and let V be the matrix whose 
i-th column is vi. Choose an integer matrix U with determinant ±1 and let wi 
be the i-th column ofW = VU. The basis {w1,w2,...,wn} generates the same 
lattice. It is assumed this basis has a large orth-defect for which solving CVP 
is computationally infeasible.
Suppose user Alice generates the matrices VA and WA with “good” basis 
VA kept secret and makes “bad” basis WA public. Suppose Bob wishes to send 
the (column) message m with small components. He generates the lattice point 
WAm and sends the perturbed (column) vector z
z = WAm + r = 
mi wAi + r
for some vector r with small components. The vector z is, by definition, close 
to a lattice point. Alice then uses the Babai rounding off algorithm described 
above and her private key good basis VA to find the closest lattice vector (in 
the bad basis) z' = WAm and recovers the message by
W-1 z' = m.
This is a simple and efficient public-key encryption algorithm. However, the 
previous comments apply to it, namely that for any high orth-defect basis one 
could apply the LLL algorithm to determine a “good” basis and hence break the 
GGH algorithm. However, applying the LLL algorithm becomes more difficult 
as the dimension of the lattice increases. Thus for a sufficiently high dimension 
the GGH algorithm may be secure. On the other hand very high dimensions are 
not attractive due to inefficiency concerns.
In spite of these considerations, various lattice problems remain of interest 
to postquantum cryptography. No quantum algorithms are known that are 
better than classical ones for solving lattice problems and the area remains 
of research interest.
https://doi.org/10.1017/9781009283403.016 Published online by Cambridge University Press

15.3 Postquantum Cryptography
399
Hash-Based Public-Key Cryptosystems
Most of the hash-based systems in the literature correspond to signature 
schemes. Some have the disadvantage they can only be used for a small number 
of signatures. However, they tend to be efficient and there is some confidence 
as to their quantum-resistance.
A hash function H(x) maps {0,1}* a binary input of arbitrary length, to 
{0, 1}n to an output of some fixed length n. Normally it is the case the input to 
the hash function is (much) longer than the output and the function provides 
compression. The result y = H(x) is often referred to as digest of the input x. 
It should have some important properties [19]:
(i) It should be easy/efficient to compute;
(ii) Preimage resistance: given y = H(x) it should be computationally 
infeasible to compute x;
(iii) Second preimage resistance: given x1 and y = H (x1) it should be 
computationally infeasible to compute x2 = x1 such that
H(x1) = H(x2);
(iv) Collision resistance: it should be computationally infeasible to compute 
any two x1 = x2 such that H (x1) = H (x2).
Naturally there are relationships between these concepts. The hash function 
SHA-2 = SHA-256 is a common - and widely used - example of such a 
function. A related notion is that of a one-way function (OWF): An OWF f( •) 
is such that for any x in the domain of f, f(x) is easy to compute while for 
essentially all y in the range of f it is computationally infeasible to determine 
x such that y = f (x).
Compared to hash functions, an OWF typically has a fixed-size domain and 
range and need not provide compression. See [19] for an interesting discussion.
There are numerous examples of hash-based one-time digital signature 
schemes in the literature. Three examples are discussed here.
The Lamport one-time signature scheme [3, 12] requires an OWF 
f: {0,1}n ^ {0,1}n and a hash function H that produces digests of n bits. 
Denote by {0, 1}(n,m) a set ofm binary n-tuples. The signature and verification 
keys of the Lamport scheme will be
signature keys: X = xo,o,xo, 1 ,x 1,0,x 1,1,...,xn-1,0,Xn—i,1} e {0.1}(n,2n) 
verification keys: Y = y0,00, 1 1,01, 1, ... Jn-1,0Jn-1,1} e {0.1}(n,2n) 
where the signature keys are chosen uniformly at random, and the verification 
keys are generated from the signature keys by
yi,j = f (xi,j), i = 0,1,...,n- 1,j= 0,1.
https://doi.org/10.1017/9781009283403.016 Published online by Cambridge University Press

400
15 Postquantum Cryptography
The signature generation for a message m e {0,1}* is as follows: Let h = 
(h0,h1,...,hn-1) = H(m) be the hash of the message. The signature for m 
then is
Sm = (x0,h0, x1,h1 ,...,xn1,hn -1 ) e{0, 1} , = (s0, s1,...,sn-1).
It is clear why it is a one-time signature scheme since reusing signa- 
ture/verification key pairs can be used for forgery. The pair (m, Sm ) is 
transmitted.
The signature verification process is straightforward. On receiving (m, am), 
am = (ao,ai,...,an-1), the receiver determines the hash value of the 
message: h = (h0,h1,...,hn-1) = H(m) and uses the public OWF f to 
verify that
(y0,h0,y1,h1,...,yn-1,hn-1) = (f (a0),f (a1),...,f(an-1))
and verifies that f(ai) is yi,mi .
While the above one-time signature scheme is quite efficient in terms of 
the computations required, the signature size can be large. Winternitz [20] 
proposed an improvement by grouping bits together for signature (rather than 
single bits) - see [7] and [3] (chapter by Buchmann et al.) for details of this 
system.
One-time signature schemes are inefficient in that a new set of keys is 
required to be generated for each signature. Merkle [20] showed how an 
authentication tree can be used to authenticate the verification keys for any 
one-time signature scheme thereby allowing the one-time signature scheme to 
be used to sign a predetermined number of messages.
Multivariable Public-Key Cryptosystems
Multivariable public-key cryptosystems typically employ a number of poly­
nomials of small degree over small fields in a large number of variables. The 
technique includes the hidden field equations (HFE) systems. Unfortunately a 
complete description of such a system requires a level of detail and complexity 
that it is not pursued here. Readers are referred to [5, 24] and [3] (chapter by 
Bernstein) [25] and [21] (section 16.3).
The above has been a very brief tour through the main contenders for 
postquantum public-key cryptosystems. No doubt others will arise and 
some will fail as further research to improve their security and efficiency 
continues.
https://doi.org/10.1017/9781009283403.016 Published online by Cambridge University Press

References
401
Comments
The section attempted to give an overview of the state of public-key algorithms 
that are quantum-resistant and the path that led to the current situation. The 
existence of a quantum computer with a sufficient size, memory and stability 
seems not yet available to actually render the classical algorithms useless. 
However, given the enormous power and advantages such machines offer and 
the enormous efforts being expended by well-funded and determined interests, 
it seems only a matter of time before their promise is realized. This chapter 
attempted to give an appreciation of the developments.
References
[1] Ajtai, M., and Dwork, C. 1999. A public-key cryptosystem with worst- 
case/average-case equivalence. Pages 284-293 of: STOC ’97 El Paso, TX. ACM, 
New York.
[2] Babai, L. 1986. On Lovas lattice reduction and the nearest lattice point problem. 
Combinatorica, 6(1), 1-13.
[3] Bernstein, D.J., Buchmann, J., and Dahmen, E. (eds.). 2009. Post-quantum 
cryptography. Springer-Verlag, Berlin.
[4] Blake, I.F., Seroussi, G., and Smart, N. 1999. Elliptic curves in cryptography. 
Cambridge University Press, Cambridge.
[5] Courtois, N.T. 2001. The security of hidden field equations (HFE). Pages 
266-281 of: Topics in cryptology - CT-RSA 2001 (San Francisco, CA). Lecture 
Notes in Computer Science, vol. 2020. Springer, Berlin.
[6] Diffie, W., and Hellman, M. 1976. New directions in cryptography. IEEE Trans. 
Inform. Theory, 22(6), 644-654.
[7] Dods, C., Smart, N.P., and Stam, M. 2005. Hash based digital signature schemes. 
Pages 96-115 of: Smart, Nigel P. (ed.), Cryptography and Coding, 10th IMA 
International Conference, Cirencester, UK, December 19-21, 2005, Proceedings. 
Lecture Notes in Computer Science, vol. 3796. Springer.
[8] Esser, A., and Bellini, E. 2022. Syndrome decoding estimator. Pages 112-141 of: 
Hanaoka, G., Shikata, J., and Watanabe, Y. (eds.), Public-key cryptography - PKC 
2022. Springer, Cham.
[9] Goldreich, O., Goldwasser, S., and Halevi, S. 1997. Public-key cryptosystems 
from lattice reduction problems. Pages 112-131 of: Advances in cryptology - 
CRYPTO ’97 (Santa Barbara, CA, 1997). Lecture Notes in Computer Science, 
vol. 1294. Springer, Berlin.
[10] Hoffstein, J., Pipher, J., and Silverman, J.H. 2014. An introduction to mathemat­
ical cryptography, 2nd ed. Undergraduate Texts in Mathematics. Springer, New 
York.
[11] Kaye, P., Laflamme, R., and Mosca, M. 2007. An introduction to quantum 
computing. Oxford University Press, Oxford.
https://doi.org/10.1017/9781009283403.016 Published online by Cambridge University Press

402
15 Postquantum Cryptography
[12] Lamport, L. 1979. Constructing digital signatures from a one way function. In: 
Report. SRI INternational.
[13] Lenstra, A.K., Lenstra, Jr., H.W., and Lovasz, L. 1982. Factoring polynomials 
with rational coefficients. Math. Ann., 261(4), 515-534.
[14] Li, Y.X., Deng, R.H., and Wang, X.M. 1994. On the equivalence of McEliece’s 
and Niederreiter’s public-key cryptosystems. IEEE Trans. Inform. Theory, 40(1), 
271-273.
[15] Ling, S., and Xing, C. 2004. Coding theory. Cambridge University Press, 
Cambridge.
[16] Luby, M.G. 2002. LT codes. Pages 271 - 280 of: Proceedings of the 43rd Annual 
IEEE Symposium on Foundations of Computer Science.
[17] McEliece, R.J. 1977. The theory of information and coding. Addison-Wesley, 
Reading, MA.
[18] McEliece, R.J. 1978. A public key cryptosystem based on algebraic coding theory. 
In: NASA DSN Progress Report.
[19] Menezes, A.J., van Oorschot, P.C., and Vanstone, S.A. 1997. Handbook of applied 
cryptography. CRC Press Series on Discrete Mathematics and Its Applications. 
CRC Press, Boca Raton, FL.
[20] Merkle, R.C. 1989. A certified digital signature. Pages 218-238 of: Brassard, 
G. (ed.), Advances in cryptology - CRYPTO ’89, 9th Annual International 
Cryptology Conference, Santa Barbara, California, USA, August 20-24, 1989, 
Proceedings. Lecture Notes in Computer Science, vol. 435. Springer.
[21] Mullen, G.L. (ed.). 2013. Handbook of finite fields. Discrete Mathematics and Its 
Applications (Boca Raton). CRC Press, Boca Raton, FL.
[22] Niederreiter, H. 1986. Knapsack-type cryptosystems and algebraic coding the­
ory. Problems Control Inform. Theory/Problemy Upravlen. Teor. Inform., 15(2), 
159-166.
[23] Nielsen, M.A., and Chuang, I.L. 2000. Quantum computation and quantum 
information. Cambridge University Press, Cambridge.
[24] Patarin, J. 1996. Hidden fields equations (HFE) and isomorphisms of polynomials 
(IP): two new families of asymmetric algorithms. Pages 33-48 of: Maurer, U.M. 
(ed.), Advances in Cryptology - EUROCRYPT ’96, International Conference 
on the Theory and Application of Cryptographic Techniques, Saragossa, Spain, 
May 12-16, 1996, Proceeding. Lecture Notes in Computer Science, vol. 1070. 
Springer.
[25] Patarin, J., Goubin, L., and Courtois, N.T. 1998. Improved algorithms for 
isomorphisms of polynomials. Pages 184-200 of: Nyberg, Kaisa (ed.), Advances 
in Cryptology - EUROCRYPT ’98, International Conference on the Theory and 
Application of Cryptographic Techniques, Espoo, Finland, May 31 - June 4, 1998, 
Proceeding. Lecture Notes in Computer Science, vol. 1403. Springer.
[26] Rivest, R.L., Shamir, A., and Adleman, L. 1978. A method for obtaining digital 
signatures and public-key cryptosystems. Commun. ACM, 21(2), 120-126.
[27] Shor, P.W. 1994. Algorithms for quantum computation: discrete logarithms 
and factoring. Pages 124-134 of: 35th Annual Symposium on Foundations of 
Computer Science (Santa Fe, NM, 1994). IEEE Computer Society Press, Los 
Alamitos, CA.
https://doi.org/10.1017/9781009283403.016 Published online by Cambridge University Press

References
403
[28] Shor, P.W. 1997. Polynomial-time algorithms for prime factorization and discrete 
logarithms on a quantum computer. SIAM J. Comput., 26(5), 1484-1509.
[29] Shor, P.W. 2003. Why haven’t more quantum algorithms been found? J. ACM, 
50(1), 87-90.
[30] Yanofsky, N.S., and Mannucci, M.A. 2008. Quantum computing for computer 
scientists. Cambridge University Press, Cambridge.
https://doi.org/10.1017/9781009283403.016 Published online by Cambridge University Press

16
Quantum Error-Correcting Codes
Error-correcting codes play a central and even a critical role in quantum 
computing in contrast to classical computing where they are not a factor. 
There are several reasons for this. Quantum computing takes place in a Hilbert 
space over the (continuous) complex numbers and the codewords are quantum 
states which are subject to environmental interactions which can alter the 
state leading to errors or decoherence. Thus the physical qubits correspond to 
some atomic-scale phenomena such as atoms, photons or trapped ions and can 
become entangled with other states with the potential to destroy the desired 
computation. The role of error correction in this scenario is thus crucial to 
give sufficient stability to the system to allow successful completion of the 
computation.
Quantum codes differ from classical coding in several ways. In classical 
error correction the error process is often modeled by a discrete memoryless 
channel as noted in previous chapters and these can generally be thought of 
as bit flips - 0 ^ 1. In quantum computing errors can involve not only bit 
flips but phase changes which can be extreme and result in states that evolve 
continuously from the original state. The state of a quantum computation 
can thus become entangled with the states of other qubits which make error 
correction critical.
There are two approaches to quantum error correction, one viewing code­
words as elements/vectors of a quantum state space and the other a stabilizer 
approach where codewords are sets of states fixed by a subgroup of trans­
formation operations. In the first approach one finds a subspace containing 
the received state and error correction involves determining the transmitted 
vector in that subspace. In the second approach one finds a coset containing the 
received element and error correction involves finding the group element in the 
coset. The situation is reminiscent of syndrome decoding in classical coding 
where one finds the coset containing the received codeword by computing 
404
https://doi.org/10.1017/9781009283403.017 Published online by Cambridge University Press

16.1 General Properties of Quantum Error-Correcting Codes
405
the syndrome and decodes to find the transmitted word. Both approaches are 
discussed here, beginning with the state space approach which has notions 
more closely allied with classical coding.
In addition to the above problems, measurement in quantum systems poses 
a significant challenge since measuring a qubit will alter its state as well as 
its correlations with other qubits. Thus care must be taken to arrange such 
measurements be taken only at the end of computations or in a manner that 
will not destroy the required end result. This is often done by the introduction 
of ancilla states, extra states where operations and output may be effected 
without disturbing the main computation.
The next section introduces general properties of quantum error-correcting 
codes by considering the error operators which are taken as the Pauli 
operators introduced in the previous chapter. While somewhat abstract, the 
properties noted are fundamental to the quantum error-correction process. 
The remainder of the chapter takes a concrete approach to error correction 
to establish some of the approaches while limiting discussion of such 
important issues as input/output and effect of measurements of the quantum 
system.
Section 16.2 introduces the notion of standard simple quantum error­
correcting codes, the three-, five- and nine-bit codes that bear resemblance to 
repetition codes in classical error correction. A seven-bit code will be discussed 
in the final section. These codes are able to correct bit flips (| 0) to 11) and 
vice versa) and phase flips (| 1) to -| 1) and vice versa). The differences of 
this type of error correction from classical error correction are noted. Many 
of the quantum error-correction codes rely on notions and constructions from 
classical coding although with significant differences. Section 16.3 gives an 
indication of other important types of codes, notably the CSS (for the authors 
Calderbank, Shor and Steane) codes, the stabilizer codes and codes constructed 
over GF (4) = F4.
16.1 General Properties of Quantum 
Error-Correcting Codes
Given the importance of quantum error-correcting codes for the success­
ful implementation of quantum computers and the huge possibilities for 
such computers, there has been considerable effort to construct effective 
and practical error-correcting codes in the quantum domain. This section 
briefly considers properties that any quantum error-correcting code should 
have.
https://doi.org/10.1017/9781009283403.017 Published online by Cambridge University Press

406
16 Quantum Error-Correcting Codes
Recall from the previous chapter the Pauli matrices:
01
10
0 -i Z 10
i 0 ,Z= 0 -1
and recall their action on n-qubit states - e.g., X3 Z5| 11101) = -| 11001) 
where the operator subscript indicates the qubit being operated on.
As a trivial example of error correction consider a code with two codewords, 
| $1) = 1000) and | $2) = 1111) and suppose | p) = 1101) is received. 
Note that
Z1Z21 P) = -| P) and Z1Z31 P) = | P}■
The first relation means that qubit one and two of the received word (state) are 
different while the second means that qubits one and three are the same. The 
received word can only be 1010) or 1101) and it can be established that the 
most likely transmitted codeword (fewest errors) is | $2) = 1111).
All operators, both error formation and correction, will be assumed to 
be Pauli operators. Such operators have order 2 and either commute or 
anticommute with each other. For two operators A and B, define the bracket 
operations:
[A,B] = AB - BA and {A,B} = AB + BA.
Then A and B are said to commute if [A,B] = 0 and anticommute if {A, B}= 
0 - equivalently if AB = BA and AB = -BA, respectively.
Notice in particular that all the matrices have order 2 and anticommute:
XY =iZ=-YX, {X,Y} = 0
XZ=iY = -ZX, {X,Z} = 0
YZ=iX=-ZY, {Y,Z} = 0
and the identity matrix commutes with all matrices.
The group of tensor products of the Pauli matrices on n qubits is of the form 
G n = {± iku 1 ® U 2 ® — ® Un], X e{0,1}, ui e{ I,X,Y,Z}.
It is clear that Gn is a group on n generators and that it has order 
|Gn|= 22n+2.
Notice that
G1 = { ± I, ± il, ± X, ± iX, ± Y, ± iY, ± Z, ± iZ }.
This group Gn describes the set of possible errors on a system with n qubits. 
It is also the set of operators that may be used on received codewords for error 
https://doi.org/10.1017/9781009283403.017 Published online by Cambridge University Press

16.1 General Properties of Quantum Error-Correcting Codes
407
correction. Typically one decides on a subset E c Gn as the set of errors a 
code will correct. The weight [5] of an operator in Gn is the minimum number 
of qubits it acts on which it differs from the identity. A t -error-correcting code 
will then be able to correct all operators in G n of weight < t .A code to encode 
k qubits into n qubits will have 2k basis codewords. If the code can correct 
errors introduced by operators Ea,Eb e Gn, it can correct a linear (over C) 
sum of the errors and it is sufficient to ensure error correction on the basis 
codewords. The code will be a subspace of dimension 2k in the Hilbert space 
of dimension 2n . A quantum error-correcting code C coding k qubits to n 
qubits will be designated as an
[[n,K,d]] or [[n,k,d)]], k = log2(K)
code of length n, dimension K = 2k and minimum distance d where d is the 
lowest weight operator in Gn that can transform one codeword into another 
codeword. Such a code ([8], chapter of Ezerman) can correct all errors of 
weight < Ld-1 J.
For a code to be capable of distinguishing an error Ea e Gn acting on 
codeword (state) & from error Eb acting on codeword &, the two results must 
be orthogonal, i.e.,
<& | El Eb | & )= 0
as otherwise there will be a nonzero probability of confusion. It is argued in [2, 
9] that for all distinct basis codewords | &), | &) e C and two error operators 
Ea and Eb in the error set of interest E c Gn we must have
(& | E\ Eb | & ) = { & | El Eb | &)
or else information on the error could be gleaned from computing such 
quantities. The above relations can be combined into the necessary and 
sufficient conditions for the code C to be able to correct all errors in E
{& | El Eb | & ) = CabSij 
(16.1)
for all possible distinct codewords | &}, | &} e C and error operators 
El, Eb e E. The matrix Clb is Hermitian (i.e., view l, b as fixed as the 
codewords vary to obtain a matrix for each such error pair El, Eb ). Thus [9] 
the code C will correct all errors in a set E c Gn iff the above condition 
Equation 16.1 is satisfied for all El,Eb e E and all codewords in C.
Gottesman [5] defines a code for which there is a pair El, Eb such that the 
matrix Clb is singular as a degenerate code and a code for which no such pair 
exists as a nondegenerate code.
https://doi.org/10.1017/9781009283403.017 Published online by Cambridge University Press

408
16 Quantum Error-Correcting Codes
In Equation 16.1 for E = E^a Eb, the weight of the smallest weight E e 
Gn for which the relationship does not hold for | $i}, | fy} in the code under 
consideration is the distance of the code. As with classical coding, in order for 
a quantum error-correcting code to correct t errors it must have a distance of at 
least 2t +1. Notice that ifE is an operator of weight equal to the code minimum 
distance, then it is possible E | $i} is a codeword and the above relationship 
would be violated, much as for classical coding.
For the remainder of the chapter several classes of quantum error-correcting 
codes are discussed, the next section giving three specific codes that have 
played an important part in the development and Section 16.3 more general 
classes of codes.
16.2 The Standard Three-, Five- and Nine-Qubit Codes
Three specific codes are discussed in this section. While simple in structure 
they illustrate the encoding and decoding procedures. The next section consid­
ers important and more general construction techniques.
The Three-Qubit Code
A three-qubit code is first considered. The two codewords are given by:
10 l )^| 000) and 11 l )^| 111),
denoting logical 0 and 1, similar to the repetition code of classical coding. 
Notice [1] the mapping represents an embedding of the two-dimensional space 
of the single qubit into a two-dimensional subspace of the eight dimensions of 
the three-qubit space rather than a cloning.
The code maps the state a 10) + b 11) into
a10) + b11) ^ $ = a1000) + b1111).
To decode a single bit flip consider the following projection operators [12]:
P0 = 1000)(000| + 1111X1111
P 1 = 1100)<100| + 1011X0111
p2 = । oioxoio| + 1101x1011
p3 = 1001x0011 + । iioxiio|.
These are recognized as projection operators as the following example will 
illustrate.
https://doi.org/10.1017/9781009283403.017 Published online by Cambridge University Press

16.2 The Standard Three-, Five- and Nine-Qubit Codes
409
Suppose a single bit flip has occurred in the state | 0) in position 2 of the 
codeword above then it is easily computed that
< 0 IP0| 0 ) = 0
< 0 IP1I 0 ) = 0
< 0 IP2| 0) = 1
< 0 I P3| 0 ) = 0
indicating the bit flip in the second position. To illustrate a computation 
consider (mapping the quantum states to eight-dimensional vectors):
(01P2I 0) = (a<010I + b<101I^^I 010X010I + I 101><1010 
010) + bI 101/)
= a^t<010I + b(101) (a1 010) + b.0 + a.0 + bI 101))
= a2 + b2 = 1
using the inner product formula of the previous chapter. Thus if any bit flip 
occurs it can be corrected in this manner.
An interesting alternative to this method of correcting bit flips is the 
following [12]. Consider the projection operator on the three qubits:
(100X00I + 111><111) 0 12 - (101X011 + 110X10I) 0 i2
and note the results of the operator acting on the states I 00), I 01), I 10) and 
I 11) are, respectively, 1, -1, -1, and 1. This can be expressed as Z1Z2 which 
can be interpreted as giving +1 if the first two qubits are the same and -1 if 
they are different. This is a convenient way of expressing the relation
Z1 0 Z 2 0 12
where Zi is the Pauli operator previously defined on the i-th qubit and this 
matrix acting on the set of states gives
"1 0 0 0 0 0 0 0’
000
000
0 1 0 0 -1 0 0 0
001
001
00 -1 0 0 000
010
-010
000 -1 0 000
011
-011
000 0 -1 000
100
-100
000 0 0 -1 00
101
-101
000 0 0 010
110
110
_0 0 0 0 0 0 0 1_
111
111
from which the view that Z1 Z2, as an operator, i.e. as above, is +1 for states 
for which the first two state qubits are the same and -1 if they are different. 
Similarly we have
https://doi.org/10.1017/9781009283403.017 Published online by Cambridge University Press

410
16 Quantum Error-Correcting Codes
11 ® Z 2 ® Z 3 =
10000000
000
000
0 -10 0 -10 00
001
-001
00 -10 0 0 00
010
-010
00 0 -1 0 0 00
011
011
0 0 0 0 
1 
0 0 0
100
100
00 0 0 0 -100
101
-101
00 0 0 0 0 -1 0
110
-110
00000001
111
111
from which the shorthand observation that Z2 Z3 operating on the states is +1 
if the second and third qubits are the same and -1 otherwise.
These quantities Z1, Z2 and Z3 can be viewed as syndromes in classical 
coding in the sense that, e.g., if Z1Z2 =-1 and Z2Z3 = +1 then the first 
two qubits are different and qubits 2 and 3 are the same, and the first qubit 
is most likely to have been in error while if Z1Z2 =-1 and Z2Z3 =-1 
the second qubit is most likely an error, etc. Quantum gates representing the 
unitary operators Zi are simple to implement.
Suppose now that a phase flip has occurred in a state 11) ^ -| 1) in any 
of the three positions of the three-bit code. To see how this can be corrected 
consider rotating the axis of the basis to states
I P} = (10) + | 1))/V2 and | m) = (10)-| 1))/V2
where the p and m are “plus” and “minus” rotations. This is equivalent to 
operating on the data with a Hadamard operator. The above three-bit code in 
the original basis now maps to the codewords
I PL) = I PPP) and I mL) = I mmm}
where the subscript L denotes logical as before.
In this rotated basis a phase shift (the Pauli Z operator) 1 ^ -1 acts as a 
bit flip operator on the | p} ^ | m} states. Thus a phase flip in the | 0), | 1) 
basis corresponds to a bit flip in the | p}, | m} basis. The rotated basis is easily 
derived from the original basis using unitary transformations corresponding to 
CNOT gates and Hadamard transforms.
Thus to correct phase flips in the original basis one can use the previous bit 
flip procedure on the rotated basis. It can be shown that the previous projection 
operators, in a rotated basis resolve to
Pj -^ H®3PjH®3
and, similarly, the syndrome measurements resolve to
H ®3 Z1Z 2 H ®3 = X1X 2 and H ®3 Z 2 Z 3 H ®3 = X 2 X 3.
https://doi.org/10.1017/9781009283403.017 Published online by Cambridge University Press

16.2 The Standard Three-, Five- and Nine-Qubit Codes
411
It can be shown that X1 X2 will be +1 if the first two qubits in the rotated basis 
are the same - either | p)| p}® (•) or | m)| m}® (•) otherwise. Then, from these 
syndromes, if a bit flip is detected in the first qubit of the rotated basis (| p} ^ 
| m)) this corresponds to a phase flip in the original basis (| 1) ^ | -1)) and is 
corrected by applying the operator HX1H = Z1.
Thus the three-bit code can correct a single bit flip in the original basis or 
a single phase flip in the rotated basis. The nine-qubit Shor code will correct 
both a bit flip and a phase flip (not necessarily on the same qubit) using many 
of the same ideas. For a more detailed discussion of this interesting approach 
see [5, 12].
The Five- Qubit Code
A five-qubit quantum single error-correcting code was found [2, 10]. The two 
logical codewords (codewords that correspond to the transmission of a data 0 
and a 1) are sums of 16 states as follows:
10 l ) = 100000) + 110010) + 101001) + 110100)
+ | 01010) - 111011) - 100110) - 111000)
-| 11101) - 100011) - 111110) - 101111)
-| 10001) - 101100) - 110111) + 100101) 
and
11L) = 111111) + 101101) + 110110) + 101011)
+ | 10101) - 100100) - 111001) - 100111)
-| 00010) - 111100) - 100001) - 110000)
-|01110) - 110011) - 101000) + 111010)
The codeword 11L) is the complement of 10L) (16 terms, each the complement 
of the above statements with the same sign). The codeword 10L) contains all 
terms with an odd number of 0’s and 11L) with an even number. That these 
two codewords can correct an arbitrary (bit flip, phase flip) error the reader is 
referred to the arguments in [7, 12]. This code will be discussed further in the 
next section from a stabilizer construction point of view. In particular, as it is a 
[[5, 1, 3]] code its parameters will be shown to meet the quantum Hamming 
bound for single error-correcting codes (see Equation 16.3 for the general 
Hamming bound)
(1 + 3n)2k < 2n
and hence is a perfect code (in analogy with classical coding), one of many 
single error-correcting perfect quantum codes. A more detailed discussion of 
the structure of this code is given in [11].
https://doi.org/10.1017/9781009283403.017 Published online by Cambridge University Press

412
16 Quantum Error-Correcting Codes
There is an interesting seven-qubit error-correcting code which is a CSS 
code, also referred to as the Steane code, which is discussed in the next section.
The Shor Nine-Qubit Code
Using the notation above the two codewords of the Shor nine-qubit code are
10L)^ 2^(1000)+ | 111»(|000)+ | 111))(|000)+ 1 111))
11L} ^ 2;t2(1000) - 11110(1000) - 11110(1000) - I •
Consider the operator X1 X2X3 X4X5X6 acting on the state
(| 000) + I 111))(| 000) ± I 111)).
The result will be
X1X 2 X 3 X4 X 5 X 6(| 111) +1000))(| 111) + 1000)) 
= (| 000) + | 1110(| 000) + | 111))
X1X2X3X4X5X6(1111) + |000))(| 000) - 1111))
= (| 000)+ | 111))(| 111) - | 000)) 
= -(| 000)+ | 1110(| 000)-| 111))
Thus the operator X 1 X2X3 X4X5X6 takes on the value +1 if the signs in 
the two brackets are the same and -1 if they are different. Similarly for the 
syndrome X4X5X6X7X8X9 for the signs in the last two brackets. A complete 
set of syndromes then for the Shor nine-qubit code is
S1 = Z1Z 2, S2 = Z 2 Z 3, S3 = Z 4 Z 5, S4 = Z 5 Z 6,
S5 = Z7Z8, S6 = Z8Z9, S7 = X1X2X3X4X5X6, S8 = X4X5X6X7X8X9.
The first six syndromes determine operations on the bits within a bracket and 
the last two the signs between brackets.
As an example of the code correcting one bit flip and one phase flip in the 
same qubit, suppose the second codeword above is transmitted and suffers both 
a bit flip and phase flip in the fifth qubit. Thus the received word is
r = (| 000) - 1111))(| 010) + | 101))(| 000) - | 111)).
The relevant syndromes are S 1 = +1,S2 = +1,S3 = -1,S4 = -1,S5 = 
+ 1,S6 = +1,S7 = -1,S8 = -1. To correct bit flips first, the syndromes 
S3 = -1, S4 = -1 indicate a bit flip in the fifth bit. It could also have 
indicated two bit flips in the fourth and sixth qubits but the former is more 
likely. It is corrected to
c = (| 000) - | 111))(| 000) + 1111))(| 000) - 1111)).
https://doi.org/10.1017/9781009283403.017 Published online by Cambridge University Press

16.3 CSS, Stabilizer and F4 Codes
413
The syndromes S7 =-1,S8 =-1 indicate a phase flip among the one qubits 
of the middle bracket - it could have been in any of them. These syndromes 
could also have resulted from phase flips in the first and third bracket but the 
former is more likely. It is corrected to
c = (|000) - 1111>)(|000) - 1111>)(|000) - 1111)).
16.3 CSS, Stabilizer and F4 Codes
The CSS codes form an interesting class of quantum error-correcting codes, 
derived from classical error-correcting codes that are able to use the classical 
error-correcting properties and algorithms to correct quantum errors in the 
qubits. They are based on the work of Calderbank and Shor in [3] and Steane 
in [13] and discussed extensively in [5, 12].
Let C 1 = (n,k 1)2 and C2 = (n,k2)2, C2 c C 1 be two (classical) linear 
codes over F2 such that k 1 > k2 and C 1 and C® are able to correct t errors, 
i.e., the minimum distance of the respective codes is at least 2t + 1inthe 
classical code/channel case. It is shown [12] how to derive a quantum n-qubit 
error-correcting code, designated CSS(C1,C2) capable of correcting t -qubit 
errors (bit flips and phase flips - a total of t ) using the classical code error­
correction algorithms. The code will contain 2k1 -k2 codewords.
The quantum states will be on n qubits, designated | x},x e C 1 giving a 
space of dimension 2n and the states are orthogonal. Denote the superimposed 
quantum state
| x + C2) = y== x I x ® y} = 211/212 1 x ®y>, x e C1.
| C2 | yeC2 
2 /2 yeC2
Denote the set of cosets of C2 in C1 as CC1 |C2 . The set can be viewed as a 
2k 1— k2 x 2k2 array of codewords of C 1. The rows of the array (cosets) are 
disjoint. Label the codewords in the first column of this array as xj ,j = 
0, 1,...,2k1 -k2 - 1 called coset leaders (a standard term in classical coding 
where they are usually chosen as minimal weight within the coset). Any 
codeword in a coset can act as a coset leader as the cosets are closed under 
addition of codewords of C2. One can define addition on the cosets in an 
obvious manner and the set of cosets is closed under this addition.
For x,x' in the same coset, |x + C2) is equal to |x' + C2). For x,x' not in 
the same coset, the states | x + C2) and | x' + C2) are orthogonal, being the 
disjoint sums of orthonormal states. The CSS code is then the set of states
CSS(C 1 ,C2) = {|xj + C2), j = 0,1,2,...,2k1 — k2}.
https://doi.org/10.1017/9781009283403.017 Published online by Cambridge University Press

414
16 Quantum Error-Correcting Codes
Consider first the correction of bit flips in a state. Consider a corrupted 
codeword | x + C2),x e C 1 with at most t bit flips among the n qubits of the 
code. This can be modeled as the state
1
■2—2 > , I x ® J ® e 1), x + J e C1
/ yeC2
where e 1 e F2 contains ones where the bit flips occur - of weight at most t. Let 
an (n — k 1) x n parity-check matrix for C 1 be H 1. By definition H 1 (x +j + e 1) = 
H1 e1. Add a sufficient number of ancilla (extra, initially empty) states to the 
system state to allow the syndrome H1e1 to be derived, i.e., by applying the 
unitary operators to the states | x ® j ® e 1) to implement the effect of H 1 on 
the system state which in effect is computing the syndrome of the “received 
word” x ® j ® e 1. The result | H 1 e 1) is stored in the ancilla states and the entire 
system plus ancilla states give the state:
Ix + J + e 1)| H 1 e 1).
The quantity H1 e1 is the syndrome of the error pattern and this can be 
read from the ancilla states which are then discarded. From this syndrome the 
classical error-correcting algorithm can be used to determine the error (bit flip) 
positions which are then corrected in the system quantum state.
Consider [12] the case of t phase flips errors and the corrupted codeword 
represented as
__C_ 
(—1 )(x®J,e2) | x © j} 
(16.2)
where the phase flip errors occur where the vector e2 is one. The result of the 
phase flips on the codeword component | x ® j) is to yield —| x ® j) if the 
number of positions where the word x ® j and where the phase flips occur is 
odd, i.e., if the inner product (x ® j, e2) is odd.
By applying Hadamard gates to each of the n qubits it can be shown [12] 
the problem reduces to one similar to the bit flip case, an argument that was 
used previously. This operation wields 2n terms for each of the n-qubit states 
| x ® j) in the above equation. For example, applying Hadamard gates to each 
of the three qubits in the state | 001) yields the set of states:
2^2 ((| 0> + | 1»(| 0) — | 1»(| 0) — | 1)))
and expanding these out yields eight terms:
1 I(| 000) — 1001) — | 010) + 1011) + | 100) — | 101) — | 110) + | 111) 
W2
https://doi.org/10.1017/9781009283403.017 Published online by Cambridge University Press

16.3 CSS, Stabilizer and F4 Codes
415
In general, all 2n possible terms are present and the sign of the terms is to be 
determined. The following shows the computation.
Applying this to Equation 16.2 can be shown [12] to yield
1
V\C2\2n £ £(-1 )(x ® J,e2 ®z)|z>. 
z e{0,1} n y e C2
Changing the variable to w = z ® e2 this can be expressed as
1
V\C2\2n £ £(-1 )(x ®y,w)| w ®e2>.
we{0,1}n yeC2
Now for any binary linear code C, if x e C^ then £yeC(-1 )(x,y) =1C I 
while ifx e/ C then yeC (-1)(x,y) = 0. This follows since the number of 
codewords y e C such that (x, y) = 0, x e/ C is the same as the number for 
which the inner product is 1 and hence the result. (The set for which the inner 
product is zero is a subspace.) This argument reduces the above equation to
, 
(-1 )(X,w) । W ® e2^
y2^4\C2\ weC2^
The form of this equation is precisely that used to correct bit flips (the 
(-1)(x, w) term does not affect the argument). Thus, returning to the original 
equation up to t phase flips can be corrected using the classical decoding 
algorithm to determine the positions of the phase flips.
The above development treated the cases of bit flips separate to phase flips. 
Clearly the analysis [12] could have considered the combined expression
1 £ (-1 )(x®ye2) I x ®y + e 1)
\C2 \ yeC2
for a total of t errors although the analysis of the two cases remains the same.
Steane [13] independently arrived at an equivalent construction and the code 
contained in the following example is usually referred to as the Steane seven­
qubit quantum code ([3, 7, 12]):
The Steane Seven-Qubit Code
Let C1 denote the Hamming (7,4,3)2 classical Hamming code and C2 = C^, a 
(7,3,4)2 code consisting of those codewords ofC1 of even weight. The Steane 
code is the CSS(C1,C2) code and a codeword of this seven-qubit code is
https://doi.org/10.1017/9781009283403.017 Published online by Cambridge University Press

416
16 Quantum Error-Correcting Codes
10 L} = 
10000000) + 11010101) + 10110011) + 11100110)
+|0001111)+|1011010)+|0111100)+|1101001)
and the codeword 11L) takes the complements of these states. Again, this is a 
[[7, 1, 3]] quantum error-correcting code capable of correcting an arbitrary error 
in a single qubit.
To elaborate on its construction of this code C1, let a generator and a parity­
check matrix for the (7, 4, 3)2 Hamming code be denoted
G=
100
0 1 0
0011
and
H=
’0
1
111100
011010
01 0 1
001 01
1 0
_ _ _
1 101001
000 11
1
1
respectively. The codewords of C2 = C± are
0000000 0111100 1011010 1101001
1100110 1010101 0110011 0001111
which gives the codeword 10L) above. The coset of C2 in C1 is given by 
adding a binary 7-tuple not in C2 to the words of C2, say 1111111 which gives 
the complements of the components of 10L) for 11L). The code is considered 
further later from a stabilizer/generator point of view.
Many of the quantum codes developed are related to classical codes and, in 
some manner, decode using a variant of the classical decoding algorithm. As 
noted earlier, many works denote a binary linear classical code as (n,k,d)2 and 
a quantum code on n qubits as [[n, k, d]] with double brackets, the convention 
which is followed here. Such a code is of dimension 2k .
Given the relationship to classical coding notions, it is natural to consider 
quantum code bounds analogous to the classical code bounds. Corresponding 
to the three Pauli matrices, there are three types of quantum errors a given qubit 
can suffer and hence a quantum code that can encode k qubits into n qubits and 
correct t quantum errors must satisfy the relation
2 k < 2 n
quantum Hamming bound (16.3)
which is the analog of the classical coding Hamming bound - referred to as 
the quantum Hamming bound. As with classical codes, if equality is achieved 
the code is perfect. In particular for t = 1 the bound reduces to
(1 + 3n)2k < 2n
https://doi.org/10.1017/9781009283403.017 Published online by Cambridge University Press

16.3 CSS, Stabilizer and F4 Codes
417
and a code meeting this bound will be an [[n, k, 3]] code and a perfect single 
error-correcting code, an example of which is the five-qubit code discussed 
above.
In a similar manner, using arguments as in the classical coding situation, it 
can be shown that if the following condition is satisfied
d — 1 
d W
3 H J I 2 k < 2 n, 
quantum Varshamov-Gilbert bound
i=0
then an [[n, k,d]] quantum error-correcting code will exist. Using standard 
approximations to the binomial coefficients these two relationships can be 
expressed as ([3], [6], part of theorem 4):
Theorem 16.1 For large n, R = k/n and p = d/2n fixed, the best 
nondegenerate quantum codes satisfy
1-2plog23-H2(2p) < R < 1 -plog23-H2(p)
where H2 (•) is the binary entropy function.
Similarly the quantum bound analogous to the classical Singleton bound for 
an [[n,k, d]] quantum code can be shown as [9]
n - k > 2d — 2. 
quantum Singleton bound
To discuss the stabilizer approach to quantum codes it will be helpful to 
recall a few notions from group theory. For an arbitrary finite group G the 
center of G, usually denoted Z(G), is defined as the set of elements that 
commute with all elements of G, i.e.,
Z(G) = {a e G | ag = ga Vg e G}.
Also a subgroup N c G is a normal subgroup, denoted N < G if for all n e N 
and g e G, gng-1 e N.
More generally, for S a subset (not necessarily a subgroup) of G the 
centralizer of S in G is
ZG(S) ={g e G | gs = sg Vs e S},
i.e., the set of group elements that commute with all elements of S. Similarly 
the normalizer of S in G is
NG(S) ={g e G | gS = Sg}.
Clearly the centralizer is a more stringent requirement and ZG(S) c N^(S) 
and both are subgroups.
https://doi.org/10.1017/9781009283403.017 Published online by Cambridge University Press

418
16 Quantum Error-Correcting Codes
As a trivial introduction to stabilizer codes consider the two three-qubit 
codewords of C given by
1 / 
\ 
1 / 
\
| $ 0) = —(| 000) + | 111)) 
| <P 1) = 
000) -| 1119.
v2 
2
Consider the error operator Z1Z2 e G3 acting on these codewords giving
Z1Z2| $0) = \ $0) and Z1Z2| $2) = \ $2)
which can be described as the operator Z1 Z2 stabilizing the codewords of C. 
In a similar argument the codewords are stabilized by Z1Z3 and Z2Z3 and the 
set of operators
{I,Z1Z2,Z1Z3,Z2Z3}
is a subgroup of G3. Note that while X1X2X3 stabilizes | $0) it does not 
stabilize | $ 1).
The plan, which appears in Gottesman [5] in his Caltech thesis and 
independently in [4], is that by considering the stabilizer of a code one can 
determine a set of error operators that it is able to correct. An outline of this 
innovative work is described.
Specifically suppose C is a quantum error-correcting code that encodes k 
qubits into n qubits, and S a subset of Gn that fixes (stabilizes) the codewords. 
Typically each codeword will contain a sum of states and the operators of S 
might permute the terms of a codeword while fixing the codeword itself. If 
g 1, g2 e S, then for | $) e C, g 1 g2| $) = g 1| $) = | $) and g 1 g2 e S and 
S is a subgroup of Gn. Furthermore g 1 g2| $) = g2g 1| $) and S is Abelian. 
Alternatively one could define a code by specifying a subgroup S c Gn and 
setting the code as
C = {| 4) | g| 4) = | 4)Vg e S}.
To encode k qubits into n the code C must have dimension 2k and thus [5, 12] 
S is a subgroup with 2n-k elements. Such a group can be generated by n - k 
independent elements, i.e.,
S = {g 1 ,g2,.. .,gn-k}, gi e sGn
where the angle brackets indicate the set of operators generated by the enclosed 
elements and where by independent is meant deleting any one element from 
the set will result in the generation of a strict subset of S.
Notice that - In / S as otherwise we would have - In | ^) = | ^) implying 
| ^) = 0 and the code would be trivial. It is interesting to observe [12] that if 
S = (g 1 ,g 2,.. .,gn - k), then - In / S iff gj = In for all j and gj = - In for 
all j and g2 = In for all g e S and that g^ = g.
https://doi.org/10.1017/9781009283403.017 Published online by Cambridge University Press

16.3 CSS, Stabilizer and F4 Codes
419
Similarly if g 1 ,g2 e S, as products of Pauli matrices they either commute 
or anticommute. Suppose they anticommute and for each state | $} in C, | $) = 
g 1 g2| $} = -g2g 1| $} = -| $}, again a contradiction.
Recall that the elements of Gn are all unitary (hence g^ = g-1Vg e Gn) 
and either Hermitian (g^ = g) or anti-Hermitian (g^ = -g) (the operators of 
g e Gn can be viewed as matrices). In addition recall the elements ofGn either 
commute or anticommute.
It has been noted ZGn c N§n and indeed [5]:
ZGn(S) = NGn(S)
and to see this let g e Gn, 5 e S and g e Ngn(S) and consider g^sg. Since 
s, g e Gn they either commute or anticommute so
gf sg = ± gf gs = ± s.
However, -s / S and so g^sg = s, i.e., g e ZGn(S'). Similarly it is shown [5] 
that S < Ngn (S) and that Ngn (S) has 4 • 2n+k elements.
The conclusion of these observations ([5], section 3.2, [12], theorem 10.8) 
is that the code C generated by the stabilizer group S c Gn will correct a set 
of errors E c G n if EaEb e S U G n \ Ng n(S)) for all Ea, Eb e E.
Thus the stabilizer approach is able to both provide a means for the efficient 
description ofa code and an estimate of the set of errors it is able to correct, an 
impressive achievement of the approach.
Several examples of the stabilizer approach are given in the following, using 
the codes already described.
The Five-Qubit Code
The codewords of this code were previously given as:
10 L ) = 100000) + 110010) + 101001) + 110100) 
+ | 01010) - 111011) - 100110) - 111000) 
-| 11101) - 100011) - 111110) - 101111) 
-| 10001) - 101100) - 110111) + 100101)
and the complements for 11L). It can be shown [7, 12] that the stabilizer of this 
code is
S = (g 1 ,g 2 ,g 3 ,g 4)
where g1 = X1Z2Z3X4, g2 = X2Z3Z4X5, g3 = X1X3Z4Z5, g4 = Z1,X2,X4 Z5 
and
S = {A g 1 ,g2, g3, g4, g 1 g2, g 1 g3, g 1 g4, g2g3, g2g4, g3g4, g 1 g2g3, g 1 g2g4, 
g1g3g4, g2g3g4, g1g2g3g4 .
https://doi.org/10.1017/9781009283403.017 Published online by Cambridge University Press

420
16 Quantum Error-Correcting Codes
As an example of the computations in terms of operators from G5, con­
sider e.g.,
g2g4 = X2Z3Z4X5Z1X2X4Z5 = Z1X22Z3Z4X4X5Z5.
Operators with different subscripts commute and from the form of the Pauli 
matrices
ZX = iY and XZ =-iY where Y = 01.
-10
and so
g2g4 = Z1Z3(iY4)(-iY5) = Z1Z3Y4Y5.
For the record we have:
1 = I5
g1 = X1Z2Z3X4
g2 = X2Z3Z4X5
g3 = X1X3Z4Z5 
g4 = Z1X2X4Z5
g1g2 = X1Y2Y4X5 
g1g3 = Z2Y3Y4Z5 
g1g4 = Y1 Y2Z3Z5 
g2g3 = X1X2Y3Y5 
g2g4 = Z1 Z3Y4Y5
g3g4 = Y1X2Y3Y4 
g1g2g3 = Y2X3X4Y5 
g1g2g4 = Y1 Z2Y3X4 
g1g3g4 = Z1Y2Y3X4 
g2g3g4 = Y1Y3X4X5
g1g2g3g4 = Z1Z2X3X5
and hence the codewords are generated by the action of this subgroup on 
100000) and 111111):
10 l ) = s100000)
= 100000) + 110010) + 101001) + 110100)
+| 01010) - 111011) - 100110) - 111000)
-| 11101) - 100011) - 111110) - 101111)
-| 10001) - 101100) - 110111) + 100101)
with similar computations for 11L}. Notice the cyclic nature of the generating 
set. It was previously noted this five-qubit code is perfect in that it meets the 
Hamming bound (Equation 16.3) for t = 1,n= 5,k= 1:
(1 + 3n)2k < 2n or (1 + 15) • 2 = 25 = 32.
It is noted [5] that for n of the form (22j - 1)/3 for positive integers j>1 
then (1 + 3n) will be a power of2 (e.g., n = 5,21,85...) and hence there is a 
possibility of perfect codes for such lengths: i.e., [[(22j - 1)/3,(22j - 1)/3 - 
2j, 3]] and such perfect single error-correcting quantum codes do exist for such 
parameters.
The Steane Seven-Qubit Code
Recall the codewords of the Steane code (an example of a CSS code) discussed 
earlier are
https://doi.org/10.1017/9781009283403.017 Published online by Cambridge University Press

16.3 CSS, Stabilizer and F4 Codes
421
10 L} = -1=| 10000000) + 11010101) + 10110011) + 11100110) 
+| 0001111) + 11011010) + 10111100) + 11101001)}
and its complements giving 11L). From earlier comments [12] the generators 
can be derived from the parity-check matrix of the dual of the (7, 4, 3)2 
Hamming code as
S = (g 1 = X4X5X6X7, g2 = X2X3X6X7, g3 = X1X3X5X7) 
= {/7, X 4 X 5 X6 X7, X2 X 3 X6 X7, X1X 3 X5 X7, X2 X 3 X4 X 5,
X1X3X4X6, X1X2X5X6, X1X2X4X7 .
The terms in the sum of states for each codeword are then obtained, as above, 
by the operators of the stabilizer group S acting on an initial state, e.g.,
10 L ) = S10000000) and 11 l ) = S11111111).
The Shor Nine-Qubit Code
The codewords of the Shor nine-qubit code were reported earlier as:
10 ' • 2-20 000) + | 111 »(|000) + | 111 »(|000) + | 111)) 
11> ^ 2-2(| 000) - 11110(| 000) - 11110(| 000) - 11110.
The stabilizer description of this code has a stabilizer group
S = |g 1 = Z1Z2, g2 = Z2Z3, g3 = Z4Z5, g4 = Z5Z6, g5 = Z7Z'8, 
g6 = Z8Z9, g7 = X1X2X3X4X5X6, g8 = X4X5X6X7X8X9 .
The code generation from the stabilizer group is not pursued here.
The following gives an overview of the interesting work of [3] that uses 
classical error-correcting codes over F4 to construct large classes of quantum 
codes. Denote by E the quotient group
E = Gn/{±I, ± il}
which is an elementary Abelian group of order 22n and hence is isomorphic to 
a binary vector space of dimension 2n. Elements of E are written as (a | b) 
(catenation), a, b binary n-tuples and the space is furnished with the inner 
product
({a 1 | b 1), (a2 | b2)) = (a 1 ,b2) + (a2,b 1) € F2 
https://doi.org/10.1017/9781009283403.017 Published online by Cambridge University Press

422
16 Quantum Error-Correcting Codes
where (•, •) is the usual inner product. This is a symplectic inner product since 
(a | b), (a | b)} = 0. The weight of element (a | b) e E is the number of 
the n coordinates i such that at least one of ai,bi is 1. The following theorem 
(theorem 1 of [3]) sets the stage for a further theorem that derives quantum 
codes from classical error-correcting codes.
Theorem 16.2 Let S be an (n - k)-dimensional linear subspace of E which 
is contained in S^ (by the above symplectic inner product) such that there are 
no vectors of weight < d in S^\S. Then there is a quantum error-correcting 
code mapping k qubits to n qubits which can correct L(d — 1)/2J errors.
Codes obtained from this theorem are termed additive codes.
Let c be a zero of the polynomial x2 + x + 1, irreducible over F2 and note 
that F4 = {0,1, c, ci = rn2 = rn + 1}. Associate to v = (a | b) e E,a,b binary 
n-tuples, the F4 n-tuple f(v) = ca + <yb e F4 and note that
E-weight of v(a | b) e E = usual Hamming weight of $(v) e F4n 
E-distance of v1 = (a1 | b1), v2 = (a2 | b2) e E
= Hamming distance dH($(v 1 ),$(v2)).
A key theorem of the work then is:
Theorem 16.3 Suppose C is an additive self-orthogonal subcode of F4n 
containing 2n—k vectors, such that there are no vectors of weight < d in C ±\C. 
Then any eigenspace of f—1 (C) is an additive quantum error-correcting code 
with parameters [[n, k,d]].
The theorem applied to classical codes leads to a wide array to quantum 
error-correcting codes. In essence the elements of the field F4 are associated 
with the four Pauli error matrices.
Comments
The chapter has introduced a few of the ideas related to the construction 
and performance of quantum error-correcting codes although many important 
aspects of the subject, such as the implementation of the codes to obtain 
reliable quantum computation and the measurement of computation outputs, 
were not addressed. The literature on these codes is substantial and goes far 
beyond the ideas mentioned here. The importance of quantum error correction 
and its potential impact on the implementation of quantum computers will 
ensure the importance of research on such codes.
https://doi.org/10.1017/9781009283403.017 Published online by Cambridge University Press

References
423
References
[1] Bennett, C.H., and Shor, P.W. 1998. Quantum information theory. vol. 44. IEEE.
[2] Bennett, C.H., DiVincenzo, D.P., Smolin, J.A., and Wootters, W.K. 1996. Mixed- 
state entanglement and quantum error correction. Phys. Rev. A, 54, 3824-3851.
[3] Calderbank, A.R., and Shor, P.W. 1996. Good quantum error-correcting codes 
exist. Phys. Rev. A, 54(Aug), 1098-1105.
[4] Calderbank, A.R., Rains, E.M., Shor, P.W., and Sloane, N.J.A. 1997. Quantum 
error correction and orthogonal geometry. Phys. Rev. Lett., 78, 405-408.
[5] Gottesman, D. 1997. Stabilizer codes and quantum error correction. arXiv:quant- 
ph/9705052.
[6] Gottesman, D. 2002. An introduction to quantum error correction. Pages 221-235 
of: Quantum computation: a grand mathematical challenge for the twenty-first 
century and the millennium (Washington, DC, 2000). Proceedings of Symposia in 
Applied Mathematics, vol. 58. American Mathematical Society, Providence, RI.
[7] Huang, L., and Wu, X. 2021. New construction of nine-qubit error-correcting 
code. arXiv::quant-ph2110.05130v4.
[8] Huffman, W.C., Kim, J.L., and Sole, P. (eds.). 2021. A concise encyclopedia of 
coding theory. CRC Press, Boca Raton, FL.
[9] Knill, E., and Laflamme, R. 1997. A theory of quantum error correcting codes. 
Phys. Rev. A, 55, 900-911.
[10] Laflamme, R., Miquel, C., Paz, J.P, and Zurek, W.H. 1996. Perfect quantum error 
correcting code. Phys. Rev. Lett., 77, 198-201.
[11] Mermin, N.D. 2007. Quantum computer science: an introduction. Cambridge 
University Press, Cambridge.
[12] Nielsen, M.A., and Chuang, I.L. 2000. Quantum computation and quantum 
information. Cambridge University Press, Cambridge.
[13] Steane, A.M. 1998. Introduction to quantum error correction. R. Soc. Lond. 
Philos. Trans. Ser. A Math. Phys. Eng. Sci., 356(1743), 1739-1758.
https://doi.org/10.1017/9781009283403.017 Published online by Cambridge University Press

17
Other Types of Coding
From the contents of the previous chapters it is clear the term “coding” covers 
a wide variety of applications and techniques. The literature contains a much 
more impressive array of topics. It seems that as each new technological 
development arrives, a form of coding arises to improve its performance. The 
purpose of this chapter is to give brief and shallow descriptions of a few of 
them, ranging from a paragraph to a few pages for each topic to illustrate, 
hopefully extending slightly the range of coding considered in this volume. 
The list remains far from exhaustive. The range of mathematics beyond that 
already encountered and the ingenuity of techniques used to effect solutions to 
intriguing problems is interesting.
17.1 Snake-in-the-Box, Balanced and WOM Codes
Snake-in-the-Box Codes and Gray Codes
Let Cn denote the binary cube in n dimensions, i.e., a graph with 2n vertices 
where each vertex has n neighbors. It is convenient to label the vertices with 
binary n-tuples in such a way that two vertices are adjacent iff their labels have 
Hamming distance of one. A Gray code then will be an ordered list of binary 
n-tuples (vertex labels) such that two consecutive codewords have distance 
one. Clearly such a code corresponds to a Hamiltonian path where every vertex 
is visited once. The added constraint that the list of codewords represents a 
cycle in the graph where the last codeword is distance one from the first, is 
referred to as a Hamiltonian cycle. That a cube always has a Hamiltonian cycle 
is well known ([22], theorem 10.1.1). One application for such a code might 
be in labeling the levels of a quantizer with the codewords. If a single bit error 
is made, the level will be received as an adjacent level with limited impact 
(except for the first and last words).
424
https://doi.org/10.1017/9781009283403.018 Published online by Cambridge University Press

17.1 Snake-in-the-Box, Balanced and WOM Codes
425
Perhaps of more interest is the notion of a snake-in-the-box code introduced 
in [29]. This also is represented by a cycle in Cn (hence an ordered list of 
binary n-tuples) except that a codeword is adjacent (distance one) to the two 
codewords preceding it and succeeding it in the list and to no other codeword in 
the list. In graph terms it represents a cycle in Cn without chords where a chord 
is an edge of the graph Cn which joins two vertices of the cycle (which are then 
adjacent - distance one) but is not an edge of the cycle. Thus a snake-in-the- 
box code is a simple cycle of Cn without chords. Said another way, such a code 
has each word adjacent to two other words of the cycle (the ones preceding it 
and succeeding it) but to no other word of the code. One could define such a 
code as a path in the cube rather than a cycle (first and last words distance one 
apart) but the literature tends to consider cycles which is of interest here.
Notice that if a single error is made in a codeword, it will result in either 
an adjacent word or a noncodeword. In this sense the code will detect a single 
error to a nonadjacent word. The original work on such codes is [29].
Example 17.1 Consider a code for n = 5 ([29]):
00000 ^ 00001 ^ 00011 ^ 00111 ^ 01111 ^ 11111 ^ 11101
t 
;
00100 - 10100 - 10110 - 10010 - 11010 - 11000 - 11001
Of interest in such codes is the length of the longest snake for each 
dimension, a problem addressed in numerous works. Let S(n) be the length 
of the longest snake in the n-dimensional binary cube Cn . S(n) is given in [29] 
for small values of n. Since S(5) = 14 the above example is maximal. It is 
known [1, 51] that
2 n—1(1 - 8911/2 + o(D) * 1<">.
Balanced Codes
For some applications in memory and transmission systems it assists system 
performance to transmit balanced codewords, words that have an equal (or 
approximately equal) number of ones and zeroes (or +1, - 1’s), e.g., if the 
system suffers from DC wander. The encoding of arbitrary bit strings into such 
balanced words has received attention in the literature [3, 5, 30, 37, 45]. A brief 
introduction to such coding using the results of Knuth [30] is discussed.
It is desired to encode binary information words of length n, x e F2’, into 
balanced codewords by operating on x and adding a prefix u e Fm of m 
bits to allow for decoding of the concatenated codeword word to restore x.
https://doi.org/10.1017/9781009283403.018 Published online by Cambridge University Press

426
17 Other Types of Coding
The transmitted codewords will have weight (n + m)/2. To construct such 
codewords, let x(k) denote the information n-tuple x with the first (say from the 
left) k, 0 < k < n, bits complemented. Suppose x has weight I .As k increases 
from 0 to n the weight of x(k) goes from £ to n — £ .As k varies the weight 
of x(k) can vary outside this range but certainly all weights in the interval 
[£, n — £] will be obtained for any initial word x. Notice that n/2 in particular 
is in this interval. Thus as k increases from 0, the weight w(x(k)) increases or 
decreases by one and there will be a minimum value for k such that the weight 
w(x((k')') achieves any given value in the range [£,n — £]. In particular there will 
be a minimum value for k such that for n even, w(x(k)) = n/2, a balanced 
word. One might consider encoding this minimum value of k that achieves 
this balance into the prefix m-tuple u and transmit the concatenated codeword 
ux(k) . In general this word will not be balanced as u will not in general be 
balanced. There will be a simple fix for this. Before considering this it is asked 
how large m must be to accommodate this type of encoding?
The number of balanced binary words of length (n + m) (assume balance 
means words of weight [(m + n)/2J) will have to be at least as large as 
2n to allow for the encoding of all binary information n-tuples - thus it is 
required that
n + m 
> 2 n.
L (n + m)/2J/
(17.1)
By using Stirling’s approximation good approximations can be obtained to 
show that one requires the number of prefix bits to be at least
m
n
2
« 2log2 (n) + 2 log2
12log2 (n) + 0.326.
(17.2)
k
The work of Knuth [30] contains several encoding methods, including the 
following technique. Suppose w(x) = I and assume n = 2m, x e F2’ (so the 
above inequality is satisfied). As noted, it will always be possible to find an 
integer k that w(x(k')) e [£,n — £]. Assume for convenience n is even. For each 
£ e [0,n], the weight of w(x), it is desired an m-tuple ut such that
w(u) + w(x(k^ = (n + m)/ /2
hence giving balanced codewords ux(k) where we are free to choose k (as long 
as w(x((k'>') is in the interval [£, n — £]) and u^ so that this is true. Assuming this 
can always be done, the decoding is as follows. On receiving a codeword, the 
prefix u^ is used by table look-up to give the weight of the original information 
word x and the bits of the rest of the received codeword x(k) are complemented 
successively from the left until that weight is reached.
https://doi.org/10.1017/9781009283403.018 Published online by Cambridge University Press

17.1 Snake-in-the-Box, Balanced and WOM Codes
427
The following example illustrates the technique.
Example 17.2 Let x e F16 be a binary information word of weight £ and 
length 16 and let ak(x') = st be the weight of x(k) where st e [i,n - £] and 
k the smallest integer that achieves this weight. For each possible weight £ a 
value of st is chosen in the interval and associated with a binary 4-tuple ut so 
that the codeword utx(k') of length 20 has weight 10.
£ [t,n - £] st
ut
£
[t,n - £] st
ut
0
[0,16]
6 1111
8
[8,8]
8 0110
1
[1,15]
7 1110
9
[7,9]
8 0101
2
[2, 14]
7 1101
10
[[6,10]
8 0011
3
[3,13]
7 1011
11
[5,11]
9
1000
4
[4, 12]
7 0111
12
[4, 12]
9 0100
5
[5,11]
8 1100
13
[3,13]
9 0010
6
[6, 10]
8 1010
14
[2, 14]
9 0001
7
[7,9]
8 1001
15
[1,15]
10 0000
On receiving the codeword utx(k), the weight of the information word x 
is determined by table lookup using the prefix ut. The bits of the received 
n-tuple x(k) are then complemented (from the left) until this weight is achieved 
to retrieve x (the smallest number of complementations to achieve this weight).
It can be established [30] that this procedure will work for all positive 
integers m for data words of length n = 2m and m parity bits satisfying the 
above bound. The important point is to verify it is always possible to arrange 
the association of prefix m-tuples with the modified information words x(k) 
such that their total weight is (n + m)/2, to yield the correct weight. The 
problem was further examined in [37] who managed to reduce the redundancy 
to about log2(n/2+1). As noted earlier numerous researchers have contributed 
further to the problem including [3, 5, 45].
Write-Once Memory Codes
Certain storage media such as paper tape, punch cards and more recently 
optical discs and flash memories have the feature that writing to them is 
irreversible. Thus initially the medium, considered as n cells (often called wits 
in the literature), may be viewed as being in the “all-zero” state. Writing a 1 
in a cell is irreversible in the sense the cell cannot be returned to the 0-state. 
In a surprising development it was shown [36] how such write-once memories 
(WOMs) can be reused by using coding techniques to write information to its 
cells several times, each time of course avoiding cells that are already in the 
https://doi.org/10.1017/9781009283403.018 Published online by Cambridge University Press

428
17 Other Types of Coding
one state. An indication of how this can be accomplished is discussed. The 
elegant demonstration of this in the seminal work [36] writes two bits to three 
cells twice - thus writing four bits to three cells.
The storage ability, capacity, of WOMs was considered from an 
information-theoretic point of view in [47]. Consider a WOM with n cells. 
Information is written on the device in t stages or generations. The maximum 
amount of information (number of bits) that can be written and retrieved from 
the device in the t stages, the capacity, is denoted by Ct,n.
As a matter of terminology denote by supp (x) c [n], x e Fn the set of 
coordinate positions containing 1’s and denote by j > x if j covers x, i.e., 
supp(j) 2 supp (x).
Consider the following informal definition (following the notation of [50]).
An {n,t; M1,...,Mt} t-write WOM code C is a coding scheme with n cells 
and t pairs of encoding and decoding maps Ei and Di,i = 1,2,...,t such 
that the first write operation maps one of M1 possible information symbols m1 
(hence log2(M1) bits) into a binary n-tuple say c1. At the second write one of 
M2 possible information symbols, m2 , is encoded into a binary n-tuple c2 such 
that the support of c2 is disjoint to that of c 1 (i.e., supp(c2) Asupp(c 1) = $) and 
so on for the t -th write, at each stage of writing the support of the codeword 
used is disjoint to that of the previous stages.
For each stage of decoding there is a decoding function:
Di: {0,1}n -+ {1,2,...,Mi} 
ci ^ mi.
The decoding operation of the i-th write may depend on knowing the state of 
the system (the contents of the cells) at the previous stage.
The sum-rate of this t -write WOM code is defined as
R = Ei=1 log2 Mi 
n
and, as above, the maximum achievable sum-rate is the capacity of the n- 
cell t -write WOM Ct,n. To consider the capacity, there are four cases to be 
considered, depending on whether the encoder (resp. decoder) needs to know 
the state of the decoder at the previous state in order to perform its operations. 
In the case where neither the encoder nor the decoder knows the previous state 
the capacity is shown to be
Ct,n < n2n/5ln2 « 2.37n. (previous encoder/decoder state unknown)
In the other three cases the capacity is
Ct,n=nlog(t+1).
https://doi.org/10.1017/9781009283403.018 Published online by Cambridge University Press

17.2 Codes for the Gaussian Channel and Permutation Codes
429
It seems interesting that the case of multiple writes increases the capacity of 
the memory over the 1-write case by a factor of log(t + 1).
Some constructions of WOM codes use linear codes and covering codes, 
codes for which the placing of spheres of a certain radius around each 
codeword results in the entire space being covered (see [20]). Other approaches 
to the problem include [26, 28, 39, 40, 48, 50]. As noted earlier, the original 
seminal paper of the subject [36] contains a simple, elegant and compelling 
example of WOMs, giving a 2-write WOM writing two bits in three cells twice 
- thus a total of four bits. It is viewed in [20, 49] as a coset coding example.
17.2 Codes for the Gaussian Channel and 
Permutation Codes
Codes for the Gaussian Channel
An undergraduate course on digital communications often begins by defining a 
digital communication system which transmits one of M possible continuous 
signals of length T seconds, every T seconds and to which white Gaussian 
noise is added to it in transmission. This is the additive white Gaussian noise 
(AWGN) channel as discussed in Chapter 3. Even for channels where it is 
not a good model its tractability may give useful intuition. Such a system 
can be shown to be reducible to a discrete-time model in Euclidean n-space 
(for some n) where the code is a set of points in the n-space (either within a 
sphere of some radius - the power-constrained case - or on the surface of a 
sphere - the equal power case). In either case the problem can be described as 
a packing problem - either packing spheres of a given radius within a larger 
sphere or packing spherical caps on the surface of a sphere. When a given 
point (signal) is transmitted channel noise (Gaussian with mean 0 and variance 
N0/2 as discussed in Chapter 3) perturbs the point off the transmitted point 
and the decoding problem is to determine the closest code point to the received 
point. The related coding problem then is reduced to finding sets of points in 
Euclidean n-space Rn, as far apart as possible under a given constraint. Such a 
strategy can be shown to optimize system performance.
The approaches are summarized as:
(i) equal power constraint which corresponds to the mathematical problem 
of finding M points on a sphere in Euclidean n-space of some radius; or 
(ii) maximum power constraint, which corresponds to the mathematical
problem of finding M points as far apart as possible within a sphere of a 
given radius.
https://doi.org/10.1017/9781009283403.018 Published online by Cambridge University Press

430
17 Other Types of Coding
Both of these signal constraints have been extensively studied in the mathe­
matical and engineering literature and many volumes on them are available. 
The discussion here gives a brief indication of two of the approaches.
An interesting approach to the equal power problem appeared in [42], using 
group representations in terms of orthogonal matrices acting on an initial 
vector resulting in points on the unit sphere in En . An approach to the signal 
design problem with a maximum power constraint uses the theory of lattices 
very successfully.
The studies of group representations and lattices are well-established 
mathematical disciplines and their use for coding often translates many of their 
results to a different domain, while introducing new aspects to the problem.
A group representation of a finite group G degree n is a homomorphism 
from G into the set of n x n orthogonal matrices over R:
p : G --- > On x n
g ।--- > P(g).
For the current interest, the representations are simply a method to generate 
a set of orthogonal matrices. Representation theory is one more mathematical 
tool to investigate the structure of the group. A group code is then formed by 
choosing an initial vector x on the unit sphere in Rn and using the orthogonal 
matrices of the above representation to generate n points (hopefully distinct) 
on the unit sphere. The code then is
C ={P(g)x | g e G, |X|= 1}.
Assuming the generator vectors are distinct, define the minimum distance of 
the code with initial vector x e Rn as
dn(x) = min |p(g)x - x | .
geG,g=1
The initial vector problem is to find the initial vector x e Rn, |x |= 1, i.e., 
find x such that
dn(x) = 
sup 
dn(y)
yeRn,|y|=|x|=1
is maximized. This approach was initiated by the work [42] with further 
contributions in [12, 23] and [25, 35]. The approach is mathematically 
challenging and has produced results of limited interest.
A particularly interesting class of equal energy codes - and of the above 
group representation approach - for the Gaussian channel is that of permuta­
tion codes [41]. One version is to choose the initial vector as balanced (around 
0) and components equally spaced - thus
https://doi.org/10.1017/9781009283403.018 Published online by Cambridge University Press

17.2 Codes for the Gaussian Channel and Permutation Codes
431
x — n—1 a n—3 a 
— a 0 a — n—3 a — n—1 a 
n odd
x — 
2 ^a, 
2 O', ... , 
^a, , O', ... , 
2 ^a, 
2 ^a, , n o
x — n— 1 a 
— — a 
n—1 a 
n even
2 , . . . , 
2, 2 , . . . , 
2 
, ,
where a is chosen to make |x |— 1, i.e. in either case
a = I" 12 
11 / 2
(n — 1 )n(n + 1)
The code is the set of all vectors obtained by permuting the initial vector in all 
possible ways. The maximum distance of this natural representation operating 
on the above initial vector then is
dmin — V2a. 
(17.3)
The use of lattices to generate good codes in Euclidean n-space, and more 
generally to obtain good sphere packing in Euclidean space, has achieved 
exceptional results. The notion of lattices was considered in the previous 
chapter for their use in postquantum cryptosystems. For convenience some of 
the notions introduced there are repeated.
The fundamental reference on lattices is the encyclopedic and authoritative 
volume [21]. Only real lattices are considered, i.e., additive subgroups of points 
in Rn . There is a slight repetition of the material here with that introduced in 
the discussion of quantum-resistant cryptography of Chapter 15.
Let X — {u1, u2,...,un } be vectors in Rm and define a lattice as the set of 
points
{
n 
1
J — 
aUi, ai e Z . 
(17.4)
The vectors X will be referred to as a basis of the lattice which will often 
(although not necessarily) be assumed to be linearly independent over R.If 
ui — (ui1,ui2,...,uim), then a generator matrix of the lattice is
M—
u11 u12 •• • u 1 m
U 21 U 22 • • • U 2m
un1 un2 • • • unm
A mathematical objective for lattices is to choose the basis vectors in such 
a way that the lattice points are as far apart as possible which often yields 
lattices with interesting mathematical properties. This is the same objective for 
constructing lattices for the AWGN channel, since the further apart the points, 
the greater the noise immunity of the system to the additive noise. For the 
https://doi.org/10.1017/9781009283403.018 Published online by Cambridge University Press

432
17 Other Types of Coding
power-constrained channel all lattice points within a sphere of a given radius 
might be used. For equal energy signals all lattice points on a sphere of a given 
radius would be used.
The fundamental domain of the lattice is the set of points
a 1 u 1 + a2u2 +------+ anun, 
0 < ai < 1
and it is readily seen that each lattice point has such a region associated with 
it and the set of all such regions exhausts the space. The determinant of A for 
the case of linearly independent basis vectors is given by
det A = (detM)2 (or det (MMT) for M nonsquare)
and the volume of a fundamental domain in this case is given (det A)1 /2.
A question of fundamental interest - mathematical and practical - is 
the packing of spheres of equal radius in Rn and the study of lattices has 
been used extensively in this study. Thus surrounding each lattice point with 
nonintersecting spheres of radius p, a question of interest is what fraction of 
the space in Rn is taken up by the spheres?
Define a Voronoi region of a lattice point of x e A as the set of all points of 
Rm closest to x than to any other lattice point. This notion is of more relevance 
to the coding problem than that of the fundamental domain. Clearly the set 
of all Voronoi regions around the set of lattice points exhausts the space as 
does the set of translated fundamental domains as noted previously. Denote by 
p the radius of the maximum radius possible for nonintersecting spheres of a 
given lattice. Thus such a sphere of radius p around each lattice point fits, by 
definition, within the Voronoi region for that lattice point. Denote by A the 
ratio of the volumes of all the spheres to the total volume of the space and, 
recalling the volume of an n-dimensional sphere of radius p in Rm is
nm/2
Vm(p) = —7------ 7pm, 
r the Gamma function, F(x + 1) = xT(x),
r(m + 1
note that
volume of a sphere of radius p 
volume of fundamental region 
nm/2n m I
= n p 
/ (det A)1 /2.
r(m + 1/
Perusal of the volumes [46] and [21] is a rewarding exercise.
https://doi.org/10.1017/9781009283403.018 Published online by Cambridge University Press

17.2 Codes for the Gaussian Channel and Permutation Codes
433
Permutation Codes for DMCs
The first notion of the use of permutation groups for coding appears to be 
the work of Slepian [41] who constructed codes for the Gaussian channel in 
Euclidean n-space Rn using, naturally, the Euclidean metric. He termed these 
code “permutation modulation,” as discussed above. The notion of permutation 
codes for discrete channels with the Hamming metric appears to have been 
initiated in the work [14]. A few of the results of that work and [41] are 
outlined here. The coding notions are a direct consequence of properties of 
the Hamming metric and the properties of permutation groups.
Consider the set N = [n] ={1, 2,...,n}.LetG be a permutation group of 
degree n and order |G| whose elements act on the set N. Thus a codeword is 
formed from a e G by a(N) = (a( 1 ),a(2),... ,a(n)). A permutation code 
C on N is a set
C={a(N),ae G}.
An (n,M,d) permutation code C of size M is such that the Hamming distance 
between any two distinct codewords is at least d. For an arbitrary permutation 
group it may be quite difficult to determine the minimum distance of the code. 
For the transitive and sharply transitive groups, however, the minimum distance 
is immediate.
Definition 17.3 A permutation group of degree n is k-transitive if for any 
two ordered k sets of distinct elements of K, {i1,i2,...,ik} and {j1,j2,...,jk} 
if there exists an element a e G such that a(i£) = jz,t = 1,2,... ,k .If there 
is exactly one such element the group is called sharply k-transitive.
In a sharply k-transitive, every nonidentity element can move at least n - 
k + 1 elements. It is immediate that the order of such a group is
k-1
|G|= (n-i)
i=0
and such groups give rise to (n, ik=-01 (n - i),n - k + 1) code with an alphabet 
size of n.
In a sense the codes corresponding to sharply transitive groups are optimum 
for their parameters. The structure of all sharply k-transitive groups for k > 2 
is known [34] and the corresponding codes are summarized in [12]. Bounds on 
permutation codes (arrays) are given [14].
There is a sizeable literature on these and related arrays of which [14] is an 
introduction and survey up to its date. The problem gives interesting slants on 
standard combinatorial and group-theoretic problems.
https://doi.org/10.1017/9781009283403.018 Published online by Cambridge University Press

434
17 Other Types of Coding
17.3 IPP, Frameproof and Constrained Codes
Codes with the Identifiable Parent Property
Consider an alphabet Q with q letters, | Q |= q , and words a = 
(a 1 ,a2,... ,an), b = (b 1 ,b2,... ,bn) e Qn and define the set of words in 
Qn , referred to as descendants of a and b as
desc(a,b) = x = (x1,x2,...,xn) e Qn | xi e {ai,bi} . 
(17.5)
In this case the words a, b are called parents of the descendants desc(a, b). Thus 
a descendant of a pair of words is a word that can be formed by successively 
choosing a coordinate from each of the words, referred to as parents in all 
possible ways. Thus for binary words of length 4
desc(0100,1110) = {0100,1110,0110,1100}.
More generally let A = a1,a2,...,at , aj e Qn,aj = (aj1,aj2,...,ajn),
aji e Qbe a set oft words in Qn.Asforthet = 2 case above define the set of 
descendants as
desc(A) = y eQn | yi e{a1i,a2i,...,ati},i= 1,2,...,n . 
(17.6)
Definition 17.4 Let C c Qn be a code. The code C is said to have the 
identifiable parent property of order t (t-IPP) [4] if for any word d of Qn , 
either d is not a descendant of any set of t codewords of C or, for any set of t 
codewords P c C of size t such that d e desc(P) at least one of the parents 
(elements of P) can be identified.
These codes were first discussed in [27] for the case t = 2 although the idea 
was present in the work of Chor et al. [18] on traitor tracing to be discussed 
next with which they have a natural association.
A small example may be illustrative (essentially theorem 3 of [27]).
Example 17.5 Let C be an equidistant code over Fq with odd distance d. 
Then C is 2-IPP. To see this consider two distinct codewords x, y e C c Fqn 
which without loss of generality can be viewed as
x = x1,x2, ... xd,z1,z2, ... zn-d
y = y1,y2,... yd,z1,z2,...zn-d, xi = yi,i= 1, 2,...,d.
To form a descendant u of these two words, one can choose i < d coordinates 
from among the first d of x and d - i from y. The last n - d coordinates of u 
will be z1, ...,zn-d . Thus the descendant u will be distance <d/2 to one of 
the codewords x or y and a greater distance to all other codewords and hence a
https://doi.org/10.1017/9781009283403.018 Published online by Cambridge University Press

17.3 IPP, Frameproof and Constrained Codes
435
parent is identified. It is noted [27] that if d is even and n<3d/2 the code is 
also 2-IPP.
With similar logic one can show ([9], proposition 1.2) that if C is an 
(n,k,d)q code over Fq , then it is t -IPP if
d>(1 - 1/t 2)n.
For example, if C is the RS code over F2k (2k, 2k-2,2k - 2k-2 + 1)2k,k > 2, 
then it is 2-IPP.
Numerous properties of such t -IPP codes have been considered including 
the following:
Lemma 17.6 ([27], lemma 1) A code C c Qn is 2-IPP iff:
(i) a, b, c distinct in C ^ ai,bi ,ci distinct for some i ;
(ii) a, b, c, d e C with {a, b} n {c, d} = f ^ {ai,bi} n {ci,di} = f for some i.
The first of these properties of trifference was studied in [31] as of 
independent interest.
Problems of interest for these codes include construction of such codes and 
the maximum size of such codes. Define
Ft(n,q) = ma^ |C| | 3C c Qn such that C is t-IPP}.
Numerous works on this function as well as t -IPP constructions are found 
in the literature including [15] (t = 1), [6] (t = 2,n = 4), [9] (shows for 
arbitrary t < q — 1 there exist sequences of codes with nonvanishing rate), [7] 
(relationship of t-IPP to certain notions of hashing), [4] (establishes general 
bounds on Ft (n, q) with particular attention to constructions and bounds for 
t = 3 and n = 5, 6), [11] (introduces the notion of a prolific t -IPP code). These 
works are suggested as representative of the large literature on the subject.
Frameproof Codes and Traitor Tracing Schemes
The notion of a traitor tracing scheme arose in the context of distributing 
content (say music or pay-per-view video) to a set of authorized users in such 
a way that if a subset of authorized users (traitors) collude to sell the access 
mechanism to an unauthorized user, the distributor, on discovering this user’s 
access, will be able to determine at least one of the traitors. Similarly the notion 
of a frameproof code consists of assigning authorizations to content in such a 
way that a subset of authorized users of less than a certain size is unable to 
pool their access information in such a way to derive access information for an 
unauthorized user (i.e., the subset cannot frame another user). Typically these 
concepts are realized through the use of key distribution techniques.
https://doi.org/10.1017/9781009283403.018 Published online by Cambridge University Press

436
17 Other Types of Coding
The concepts arose in the work of Chor et al. [18, 19] (for traitor tracing) and 
Fiat et al. [24] for broadcast encryption for a version of frameproof codes. The 
notion of a frameproof code is also inherent in the work of Boneh et al. [16] 
for watermarking content in such a manner that collusions to provide access to 
an illegal copy can be traced. Our interest here is merely to define the ideas in 
an informal and limited manner and give simple examples of them. There are 
slight variations in the literature in the concept definitions. The approaches of 
[44] and [10] are of particular interest.
The definition of a c-frameproof code [10] is straightforward.
Definition 17.7 Consider a set of m users U = U1,U2,...,Um and F an 
alphabet of q symbols and C a set of codewords over F of length n, C c Fn. 
Assume each codeword is assigned to a user. The code C is a c-frameproof 
code if, for all subsets P c C, | P |< c, desc{P} n C = P (see Equation 17.5 
for the definition of desc).
The definition implies that with the code C, if any subset of c users collude 
in trying to construct another codeword from using only the coordinates of its 
own sets, they will be unable to do so - hence they will be unable to frame 
another user.
The same work [10] contains an interesting nontrivial example of such 
codes.
Example 17.8 Let the alphabet F = Fq and ai,a2, ...,an be distinct 
elements of Fq and let C be
C = {(f(ai),f(a2),...,f(an)), deg(f) < f(n/c 1}.
To show that C is c-frameproof, suppose P c C, 
| C |< c and let x e
descP n C. Each coordinate of x agrees with at least one of the codewords 
of P and hence there must be a codeword that agrees with x in at least fn/c"| 
positions. But by construction this means x is equal to that codeword since it 
uniquely specifies the codeword polynomial. Hence the code is c-frameproof.
Other interesting examples of binary c-frameproof codes are found in [44].
Consider now the notion of a traceability scheme [i8, i9, 44] for which 
the following model is introduced. A large file is to be transmitted to a set 
of m legitimate users U ={Ui,U2,...,Um},m = 2r. The content provider 
generates a set of v keys, T , and gives each user a subset of r keys, referred 
to as the user’s personal keys and denoted P(Ui) for user Ui e U,i= 
i,2,...,m. The large file is broken up into N segments and each segment 
is transmitted as an encrypted content block, CBi,i = i, 2,...,N, block 
CBi encrypted with symmetric algorithm key i* (say AES). Along with each 
https://doi.org/10.1017/9781009283403.018 Published online by Cambridge University Press

17.3 IPP, Frameproof and Constrained Codes
437
encrypted block is an enabling block, EBi,i = 1, 2,...,N. For each user, 
the information in the enabling block, along with the users set of personal 
keys, P(Ui), allows the user to derive the symmetric key i* for each block (the 
key could be different for each block). Several interesting traceability schemes 
based on this model are given in [19]. The simplest one is described in the 
following example:
Example 17.9 Following the system description above suppose there are 
m = 2r authorized users and users are identified with binary r -tuples, user 
uj assigned an ID of (uj 1 ,uj2,... ,ujr) e F2. The encrypted block EBi is 
encrypted with the key s e F2 which is implemented as the XOR of r keys, 
i.e., s = ® j=1 sj, sj e F2. The content provider generates r pairs of keys 
(0 _ ( 1°)0) „(■ 1 A („)0) „(■ 1 A (0)0) (1 1 A oil In 1?^ I TTenr ti w oeemnnrl
A = (a1 , a1 ), (a2 , a2 ), . .., (ar , ar ), all in F2 . User uj is assigned 
the set of keys P(Uj- = a(1uj1-,a(2uj2-,...,ar(ujr- ,j = 1,2,...,m. The 
content provider includes in the enabling block for each segment of the content 
the encryption of each key sj by both of the keys a(0- and a(1- , j = 1,2,...,r, 
a set of 2r encryptions.
Clearly each legitimate user is able, from the information in the enabling 
block to retrieve each of the subkeys sj and hence s. Furthermore if a pirate 
content decoder box is discovered the content provider will be able to look 
(up1- (up2- 
(upr-
at the keys inside a1 
, a2 
, ...,ar 
which will identify the user who
provided them. Hence the scheme is 1-traceable.
More generally interest is in protecting content against largest sets of 
colluders. Such schemes are of great interest but tend to be more intricate 
than space here allows. Suppose a set of users n c U (traitors) conspire 
to form a set of r keys given to a pirate decoder $, A c T, | T |= v such 
that A c n Ue nP(U). To detect at least one of the traitors one could form 
the set | A n P(U) | for all users U e U. If | A n P(U) |>| $ n V | for all 
users V = U, then user U is identified as an exposed user in that they have 
contributed more keys to the pirate decoder than other users. In this case [44]:
Definition 17.10 Suppose an exposed user U is a member of the coalition n 
for a pirate decoder $ produced by n, | n |< c. Then the traitor tracing scheme 
is referred to as a c-traceability scheme.
Interesting c-traceability schemes are constructed in [44] from t-designs 
and other combinatorial structures. The relationships between frameproof 
codes and traceability schemes are discussed in numerous works, including 
[43, 44].
https://doi.org/10.1017/9781009283403.018 Published online by Cambridge University Press

438
17 Other Types of Coding
Constrained Coding and Run-Length Limited Codes
For many types of storage and communication systems it is necessary for the 
transmitted system to possess certain properties for their correct operation. 
For example, certain disk storage systems had timing and synchronization 
circuits derived from the data sequences themselves. If the data had too many 
consecutive zeros in a sequence, the dc value tended to wander, making bit 
detection inaccurate, while if it had too few the synchronization circuits tended 
to lose bit synchronization. These resulted in the (d, k)-sequences where the 
sequences must have the property of having at least d and at most k zeros 
between any two ones [8, 33, 38]. Such sequences were used in certain IBM 
storage systems.
Such a constraint can be modeled by the finite-state machine (FSM) as 
shown in Figure 17.1 for (d, k) = (2, 4). Similarly, one could consider a 
constraint of sequences that have no runs of either zeros or ones of length 
greater than k, referred to as (n,k)-sequences [13], or simply as k-sequences.
Any path around the FSM for a given constraint will satisfy the constraint 
and the issue will be to construct such codes as well as encoding and decoding 
algorithms. In practice such codes might have a finite length n and one might 
add the constraint that the concatenation of any two codewords also satisfies 
the constraint. Such issues have been considered in the literature and many 
have been implemented in practice.
The only issue of interest in this work is the notion of the capacity of the 
sequences, i.e., what is the maximum amount of information that can be carried 
by a coded bit. This question was answered by Shannon in his foundational 
paper [38]. Let D be the directed transition matrix of the FSM representing 
the constraint, i.e., dij is 1 if there is a transition from state i to state j and 
0 otherwise (it is assumed there is not more than one edge between any two 
states - although it is only necessary for the symbols on multiple outgoing 
branches to be distinct). D is a K x K matrix for a FSM with K states. Shannon 
showed that the capacity of the sequence is
C = log2 X bits per code symbol 
(17.7)
0000
CO CO CO CO C5j
Capacity = 0.4057
Figure 17.1 Constrained code (2, 4)
https://doi.org/10.1017/9781009283403.018 Published online by Cambridge University Press

17.3 IPP, Frameproof and Constrained Codes
439
where X is the largest eigenvalue of the transition matrix D representing the 
constraint. Thus a long codeword of length n satisfying the constraint would 
represent Xn information bits. Equivalently the capacity of the constrained 
sequences is the largest real root of the polynomial expressed by
det(Dz-1 - I) = 0. 
(17.8)
Given the transition matrix D representing the constraint, this equation leads 
to a polynomial equation. For example, the (d, k) constraint the above matrix 
equation can be reduced [8, 33, 38] to the polynomial equation
zk+2 - zk+1 - zk-d+1 + 1 = 0.
Thus the capacity of the constraint is log2 X where X is the largest real root 
of the polynomial equation. Similarly the polynomial equation for the k- 
sequences can be derived as
z-2k{z2k - z2((1 - zk)/( 1 - z))2} = 0
and again the capacity of the constraint is the logarithm to the base 2 of the 
largest root of this equation. The capacity of the example displayed in Figure 
17.1 is shown there. An extensive table of the capacity of the (d, k)-sequences 
is given in [33].
Of the literature on this subject the paper [2] is seminal and the book [32] 
is an excellent source.
Comments
The purpose of this chapter has been to note further types of coding beyond 
the types of the preceding chapters by providing (relatively) brief descriptions, 
simply to expand the number of coding systems of interest. Readers might 
well be aware of numerous other types of coding not mentioned. These might 
have included arithmetic coding or source coding topics such as Lempel-Ziv 
codes and enumerative source coding among numerous other types. Another 
type of coding is designed to reduce adjacent channel intersymbol interference 
or crosstalk (see [17] and the references therein). Similar ideas have already 
found commercial application to improve chip-to-chip transmission speeds 
significantly. Indeed many of the types of coding considered in this volume 
have been of value in system implementations.
https://doi.org/10.1017/9781009283403.018 Published online by Cambridge University Press

440
17 Other Types of Coding
References
[1] Abbott, H.L., and Katchalski, M. 1988. On the snake in the box problem. 
J. Combin. Theory Ser. B, 45(1), 13-24.
[2] Adler, R., Coppersmith, D., and Hassner, M. 1983. Algorithms for sliding block 
codes - an application of symbolic dynamics to information theory. IEEE Trans. 
Inform. Theory, 29(1), 5-22.
[3] Al-Bassam, S., and Bose, B. 1990. On balanced codes. IEEE Trans. Inform. 
Theory, 36(2), 406-408.
[4] Alon, N., and Stav, U. 2004. New bounds on parent-identifying codes: the case of 
multiple parents. Combin. Probab. Comput., 13(6), 795-807.
[5] Alon, N., Bergmann, E.E., Coppersmith, D., and Odlyzko, A.E. 1988. Balancing 
sets of vectors. IEEE Trans. Inform. Theory, 34(1), 128-130.
[6] Alon, N., Fischer, E., and Szegedy, M. 2001. Parent-identifying codes. J. Combin. 
Theory Ser. A, 95(2), 349-359.
[7] Alon, N., Cohen, G., Krivelevich, M., and Litsyn, S. 2003. Generalized hashing 
and parent-identifying codes. J. Combin. Theory Ser. A, 104(1), 207-215.
[8] Ashley, J., and Siegel, P. 1987. A note on the Shannon capacity of run-length­
limited codes. IEEE Trans. Inform. Theory, 33(4), 601-605.
[9] Barg, A., Cohen, G., Encheva, S., Kabatiansky, G., and Zemor, G. 2001. A hyper­
graph approach to the identifying parent property: the case of multiple parents. 
SIAM J. Discrete Math., 14(3), 423-431.
[10] Blackburn, S.R. 2003. Frameproof codes. SIAM J. Discrete Math., 16(3), 499-510 
(electronic).
[11] Blackburn, S.R., Etzion, T., and Ng, S.-L. 2008. Prolific codes with the identifiable 
parent property. SIAM J. Discrete Math., 22(4), 1393-1410.
[12] Blake, I.F. 1974. Configuration matrices of group codes. IEEE Trans. Inform. 
Theory, IT-20, 95-100.
[13] Blake, I.F. 1982. The enumeration of certain run length sequences. Inform. and 
Control, 55(1-3), 222-237.
[14] Blake, I.F., Cohen, G., and Deza, M. 1979. Coding with permutations. Inform. 
and Control, 43(1), 1-19.
[15] Blass, U., Honkala, I., and Litsyn, S. 1999. On the size of identifying codes. 
In: In AAECC-13, LNCS No. 1719.
[16] Boneh, D., and Shaw, J. 1998. Collusion-secure fingerprinting for digital data. 
IEEE Trans. Inform. Theory, 44(5), 1897-1905.
[17] Chee, Y.M., Colbourn, C.J., Ling, A., Zhang, H., and Zhang, X. 2015. Optimal 
low-power coding for error correction and crosstalk avoidance in on-chip data 
buses. Des. Codes Cryptogr., 77(2-3), 479-491.
[18] Chor, B., Fiat, A., and Naor, M. 1994. Tracing traitors. Pages 257-270 of: 
Advances in cryptology - CRYPTO ’94. Lecture Notes in Computer Science, vol. 
839. Springer, Heidelberg.
[19] Chor, B., Naor, M., and Pinkas, B. 2000. Tracing traitors. IEEE Trans. Inform. 
Theory, 46(3), 893-910.
[20] Cohen, G., Godlewski, P., and Merkx, F. 1986. Linear binary code for write-once 
memories (Corresp.). IEEE Trans. Inform. Theory, 32(5), 697-700.
https://doi.org/10.1017/9781009283403.018 Published online by Cambridge University Press

References
441
[21] Conway, J.H., and Sloane, N.J.A. 1999. Sphere packings, lattices and groups, 3rd 
ed. Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of 
Mathematical Sciences], vol. 290. Springer-Verlag, New York. With additional 
contributions by E. Bannai, R. E. Borcherds, J. Leech, S. P. Norton, A. M. 
Odlyzko, R. A. Parker, L. Queen and B. B. Venkov.
[22] Diestel, R. 2018. Graph theory, 5th ed. Graduate Texts in Mathematics, vol. 173. 
Springer, Berlin.
[23] Djokovic, D.Z., and Blake, I.F. 1972. An optimization problem for unitary 
and orthogonal representations of finite groups. Trans. Amer. Math. Soc., 164, 
267-274.
[24] Fiat, A., and Naor, M. 1994. Broadcast encryption. Adv. Cryptol., 773, 480-491.
[25] Fossorier, M.P.C., Nation, J.B., and Peterson, W.W. 2007. A note on the optimality 
of variant-I permutation modulation codes. IEEE Trans. Inform. Theory, 53(8), 
2878-2879.
[26] Fu, F.-W., and Han Vinck, A.J. 1999. On the capacity of generalized write-once 
memory with state transitions described by an arbitrary directed acyclic graph. 
IEEE Trans. Inform. Theory, 45(1), 308-313.
[27] Hollmann, H.D.L., van Lint, J.H., Linnartz, J.-P., and Tolhuizen, L. M. 1998. 
On codes with the identifiable parent property. J. Combin. Theory Ser. A, 82(2), 
121-133.
[28] Horovitz, M., and Yaakobi, E. 2017. On the capacity of write-once memories. 
IEEE Trans. Inform. Theory, 63(8), 5124-5137.
[29] Kautz, W.H. 1958. Unit-distance error-checking codes. IRE Trans. Electron. 
Comput., EC-7(2), 179-180.
[30] Knuth, D.E. 1986. Efficient balanced codes. IEEE Trans. Inform. Theory, 32(1), 
51-53.
[31] Korner, J., and Simonyi, G. 1995. Trifference. StudiaSci. Math. Hungar., 30(1-2), 
95-103.
[32] Lind, D., and Marcus, B. 1995. An introduction to symbolic dynamics and coding. 
Cambridge University Press, Cambridge.
[33] Norris, K., and Bloomberg, D. 1981. Channel capacity of charge-constrained run­
length limited codes. IEEE Trans. Magnet., 17(6), 3452-3455.
[34] Passman, D. 1968. Permutation groups. W. A. Benjamin, New York/Amsterdam.
[35] Peterson, W.W., Nation, J.B., and Fossorier, M.P. 2010. Reflection group codes 
and their decoding. IEEE Trans. Inform. Theory, 56(12), 6273-6293.
[36] Rivest, R.L., and Shamir, A. 1982. How to reuse a “write-once” memory. Inform. 
and Control, 55(1-3), 1-19.
[37] Schouhamer Immink, K.A., and Weber, J.H. 2010. Knuth’s balanced codes 
revisited. IEEE Trans. Inform. Theory, 56(4), 1673-1679.
[38] Shannon, C.E. 1948. A mathematical theory of communication. Bell Syst. Tech. 
J., 27, 379-423.
[39] Shpilka, A. 2013. New constructions of WOM codes using the Wozencraft 
ensemble. IEEE Trans. Inform. Theory, 59(7), 4520-4529.
[40] Shpilka, A. 2014. Capacity-achieving multiwrite WOM codes. IEEE Trans. 
Inform. Theory, 60(3), 1481-1487.
[41] Slepian, D. 1965. Permutation modulation. Proc. IEEE, 53, 228-236.
https://doi.org/10.1017/9781009283403.018 Published online by Cambridge University Press

442
17 Other Types of Coding
[42] Slepian, D. 1968. Group codes for the Gaussian channel. Bell System Tech. J., 47, 
575-602.
[43] Staddon, J.N., Stinson, D.R., and Wei, R. 2001. Combinatorial properties 
of frameproof and traceability codes. IEEE Trans. Inform. Theory, 47(3), 
1042-1049.
[44] Stinson, D.R., and Wei, R. 1998. Combinatorial properties and constructions of 
traceability schemes and frameproof codes. SIAM J. Discrete Math., 11(1), 41-53 
(electronic).
[45] Tallini, L.G., Capocelli, R.M., and Bose, B. 1996. Design of some new efficient 
balanced codes. IEEE Trans. Inform. Theory, 42(3), 790-802.
[46] Thompson, T.M. 1983. From error-correcting codes through sphere packings to 
simple groups. Carus Mathematical Monographs, vol. 21. Mathematical Associa­
tion of America, Washington, DC.
[47] Wolf, J.K., Wyner, A.D., Ziv, J., and Krner, J. 1984. Coding for a write-once 
memory. AT&T Bell Lab. Tech. J., 63(6), 1089-1112.
[48] Wu, Yunnan. 2010. Low complexity codes for writing a write-once memory 
twice. Pages 1928-1932 of: 2010 IEEE International Symposium on Information 
Theory.
[49] Yaakobi, E., Kayser, S., Siegel, P.H., Vardy, A., and Wolf, J.K. 2010. Efficient 
two-write WOM-codes. Pages 1-5 of: 2010 IEEE Information Theory Workshop.
[50] Yaakobi, E., Kayser, S., Siegel, P.H., Vardy, A., and Wolf, J.K. 2012. Codes for 
write-once memories. IEEE Trans. Inform. Theory, 58(9), 5985-5999.
[51] Zemor, G. 1997. An upper bound on the size of the snake-in-the-box. Combina- 
torica, 17(2), 287-298.
https://doi.org/10.1017/9781009283403.018 Published online by Cambridge University Press

Appendix A Finite Geometries, Linearized 
Polynomials and Gaussian Coefficients
This appendix presents a few standard facts on geometries, certain enumeration 
problems and related polynomials drawn mainly from the excellent references [1, 6, 8]. 
The results are relevant largely to the chapters on rank-metric codes and network codes.
Finite Geometries
In general terms, a finite geometry is a set of finite points along with a set of axioms 
as to how the lines, (a subset of points) and points, interact. The Euclidean geometry 
EG(n,q) is often introduced through the Projective geometry P G(n,q) of dimension 
n over the finite field Fq , which is briefly introduced as follows. Denote by Vn+1 (q) 
the vector space of (n + 1)-tuples over Fq . For the points of PG(n,q) identify scalar 
multiples (over Fq) in Vn+1 (q) to yield a total of
qn+1 - 1
--------- =— = 1 + q + q + • • • + qn =| PG(n, q) | 
q-1
points. For two points p1 = (a1,a2,...,an+1) and p2 = (b1,b2,... bn+1) (not scalar 
multiples) the line through these points is the equivalence class of
aP1 + fiP2 = (aa 1 + fib 1 ,aa2 + fib2, . .. ,aan + fibn)
and hence there are (q2 - 1)/(q - 1) = q + 1 points on a line in P G(n,q). Any two 
lines in PG(n,q) intersect. Subgeometries of P G(n,q) of dimension m are referred to 
as m-flats, PG(m,q), which can be defined in the obvious manner. The number of such 
m-flats in PG(n,q), m < n, is given by
m n 
i=0
(qn+1-i - 1)
(qm +1-i — 1) •
The formal manner of defining a Euclidean geometry is often to start with P G(n,q) 
and delete a hyperplane or (n - 1)-flat resulting in
|EG(n,q)|= qn.
443
https://doi.org/10.1017/9781009283403.019 Published online by Cambridge University Press

444 A Finite Geometries, Linearized Polynomials and Gaussian Coefficients
The point/line behavior of this derived geometry is inherited from the projective 
geometry. For example, the number of I-flats in EG(n, q) is calculated as
n - € 
q
k=1
(qn+1-k - 1)
(q1* + 1 —k — 1) '
Notions of parallel lines and flats are restored in such a geometry. These can be viewed 
as cosets of a vector subspace Vn(q). The remainder of this appendix focus will be on 
a vector space over Fq , Vn (q), and its (vector) subspaces.
Subspace Enumeration and Gaussian Coefficients
are defined as the number of k-dimensional subspaces of Vn (q), i.e.,
Definition A.1 The Gaussian coefficients (also referred to as q-binomial coefficients) 
n 
k q
’n! = (qn — 1 )(qn — q) ... (qn — qn—(k—1))
.k\q ~ (qk — 1 )(qk — q) ••• (qk — qk— 1)
= (qn — 1 )(qn—1 — 1) ■ ■■ (qn—(k—1) — 1)
~ (qk — 1 )(qk—1 — 1) ••• (q — 1)
The numerator of the first equation in the definition is the number of ordered ways of 
choosing a k basis vectors in Vn(q) and its denominator is the number of ordered ways 
to choose a basis of a k-dimensional subspace, the ratio being the number of distinct 
subspaces of dimension k .
These coefficients share a number of interesting properties similar to those of the 
ordinary binomial coefficients. These properties are of independent interest and a 
number of them will be stated without proof. All can be found in the references [1, 8].
It is interesting to note that if one views the Gaussian coefficient as a rational 
function of the variable q, it is in fact a polynomial of degree k(n — k) as a consequence 
of it taking on an integral value for an infinite number of integral values of q .The 
coefficients of this polynomial have combinatorial significance ([8], theorem 24.2), i.e.,
n
k q
k(n—k)
aiqi.
i=0
Example A.2
5
2 q
(q5—1)(q4—1)
(q2—1)(q—1) = (q + q + q + q + 1)(q + 1).
Let Pq (n) denote the set of all subspaces of the vector space Fqn and Gq (n, k) denote 
the set of all k-dimensional subspaces of the Fqn , referred to as the Grassmannian of 
order k of Fqn . By definition the cardinality of this Grassmannian is
|Gq (n,k)|=
n
k q
(A.1)
https://doi.org/10.1017/9781009283403.019 Published online by Cambridge University Press

A Finite Geometries, Linearized Polynomials and Gaussian Coefficients 445
The Gaussian coefficients play a significant role in the subject of subspace coding 
and the following simple bound can be useful [3] and ([4], Kschischang chapter):
qk(n-k) <
< Kq-1qk(n-k)
(A.2)
n
k q
where Kq = f[i=1 (1 — q—i)• The constant K—1 is monotonically decreasing with q 
with a maximum of 3.4594 at q = 2 and a value 1.0711 at q = 16.
The relations
n
0 q
n
n q
n
m q
n
= 1 , and
-m
0 < m < n,
are verified by direct computation. Similar to the binomial relation
n+1 
k
n
k
n
+
k
1
there is the Gaussian coefficient relation:
n+1
k
q
+ qn+1-k k-
(A.3)
1 q
and, additionally [2]:
nm
mqpq
nn
pn
q
-m
,0< p < m < n.
p
These relations can be verified by direct computation.
It is also noted [1] that as a consequence of applying l’Hopital’s rule
lim qx
q — 1 qy
xq 
lim
q — 1 yq
x-1
y-1
1
1
x
y
and hence
lim 
q —— 1
nl 
n(n — 1) • • • (n — k + 1)
= lim = -----------------------------------
kjq 
q —1 
k(k — 1) ••• 1
A version [1] of the usual binomial theorem is
(1+t)r=t(ry 
k 
i=0
valid for any real number r where
'r r(r — 1) • • • (r — k + 1) 
k = 
k!
An analog of this for q -binomial coefficients is, for n > 1
n —1 
n
n (1 + qt) = 12 qk(k—1)/2 n 
k
tk
q
https://doi.org/10.1017/9781009283403.019 Published online by Cambridge University Press

446 A Finite Geometries, Linearized Polynomials and Gaussian Coefficients
or, similarly
1
n
n-1
i=0 (1
qit)
?c
i=o
i+1
ti .
q
Another binomial relation is
which is easily argued by the number of ways of choosing k items from a set of n + m 
items and considering the number of such sets obtained by choosing i from a subset 
of m of the items and k - i from the complement set. For the q -binomial analog the 
argument might be as follows. Let U be a vector space over Fq of dimension n + m 
and V a subspace of dimension m. Let W be a subspace of V of dimension i - there
m
i q
are
such subspaces. We ask how many ways can W be completed with vectors
entirely outside of V to form a subspace of U of dimension k. A sum of such quantities
over i will yield 
sought is:
n+m 
k
, the number of subspaces of U of dimension k. The quantity
q
(qn +m - qm)(qn+m - qm+1) • • • (qn+m - qm+(k-i)-1)/
(qk - qi)(qk - ql+1) ••• (qk - qk -1)
= qm(lk-i)+1+2+-+(k-)--1 (qin - 1 )(qn-1 - 1) , . . (qn-(k-i)+1 - 1)/
q(k-i)i+1+2+-+(k-i)-1 (qk-i - 1 )(qk-i-1 _ 1) „ • (q - 1)
n
= q(k-i)(m-i) k- i q
This leads to the relation [8]
n+m
k ...
= 
q(k-i)(m-i)
i=0
iqk
In addition there is an expansion relation:
m
n
k
q
n
E (-1 )j
j=0
zjqj(j-1)/2 = (-1 )n(z - 1 )(zq - 1) ••• (zqn-1 - 1)
n
i q
and a type of inversion relation:
m
If am =
j=0
d+m
d+j
q
bj,
m
then bm = 
(-1)m+j d+m
d+j
q
q(m-j)(m-j-1)/2aj
This last relation was used in [2] for the derivation of the rank distribution of a linear 
MRD rank-matrix code.
Numerous other analogs between binomial and Gaussian coefficients exist.
https://doi.org/10.1017/9781009283403.019 Published online by Cambridge University Press

A Finite Geometries, Linearized Polynomials and Gaussian Coefficients 447
Linearized Polynomials
Assume that q is a fixed prime power.
Definition A.3 A polynomial of the form
n
L(x) = Z2 aixq , ai e Fqm
is called a linearized polynomial over Fqm . It is also referred to as a q-polynomial, the 
term which will be used here. For L(x) a q -polynomial over Fqm , a polynomial of the 
form
A(x) = L(x) — a, a e Fnm
is called an affine q -polynomial.
The standard reference for the properties of such polynomials is the indispensable 
volume [6]. Some of their properties are discussed in abbreviated form here. Let Fq m 
be an extension of the field Fq . One property that makes such polynomials of interest 
is the fact the set of roots of such a polynomial form a vector space over Fq , a fact that 
follows from the properties that for L(x) a q -polynomial over Fqm we have
L(a + /) = L(a) + L(/), Ya,/ e Fq 
L(c/) = cL(/), Yc e Fq, and Y/ e Fq.
It is shown ([6], theorem 3.50) that for Fqs , an extension of Fqm , that contains all 
the roots of L(x ), then each root of L(x) has the same multiplicity which is either 1 or a 
power of q and the set of such roots forms a vector subspace of Fqs viewed as a vector 
space over Fq . A converse of this statement ([6], theorem 3.52) is that if U is a vector 
subspace of Fqm over Fq , then for any nonnegative integer k the polynomial
L(x) = y (x - P)q
is a q -polynomial over Fqm .
Similar statements hold for affine q -polynomials with subspace in the above 
replaced by affine space.
For the material on rank-metric codes it has already been noted that the following 
matrix ([6], lemma 3.51) for elements p1,p2,...,pn e Fqm :
p 1 pq pq2 ••• pqn-1
^ = p2 pq pq2 ••• p2q-1 
(A4)
= 
(.)
.. 
. 
. 
.
.. 
. 
. 
.
.. 
. 
. 
.
2 
2% 2^ qn- 1
pn pn pn 
pn
is nonsingular iff p1,p2,...,pn are linearly independent over Fq . Indeed the formula 
for the determinant [6] is given as
--I 
„ / 
,2, 
'
det(B) = p1 
pj +1 — 
ckpk
j=1 c1,...,cjeFq 
k=1
https://doi.org/10.1017/9781009283403.019 Published online by Cambridge University Press

448 A Finite Geometries, Linearized Polynomials and Gaussian Coefficients
The matrix is sometimes referred to as a Moore matrix.
Clearly the q -polynomials have interesting properties and it is natural to ask, for a 
given q polynomial L(x) = in=0 ai xqi how its properties relate to the properties of 
the associated polynomial £(x) = "=o aix1. The two polynomials will be referred 
to simply as q -associates of each other, the direction being clear from the context. 
Interesting properties of these polynomials are found in [6].
Random Matrices over Finite Fields
In connection with the random packet coding scheme noted in Section 2.3 it is of 
interest to determine the rank properties of random matrices over finite fields. This 
is a well-investigated problem in the literature and only elementary properties are 
considered here, specifically the probability that a random binary k x (k + m) matrix 
over Fq is of full rank. The number of s x t matrices of rank r over a finite field Fq, 
denoted by ^q(s,t,r), is first enumerated (although interest here is almost exclusively 
in the binary q = 2 case). This quantity satisfies the recursion [5]:
^q(s,t,r) = qr^q(S — 1 ,t,r) + (qt — qr 1 )^q(s — 1 ,t,r — 1) 
(A-5)
with initial conditions
^q(s,t, 0) = 1, <&q(s,t,r) = 0 for r > min(s,t)
which is the number of ways of augmenting an (s - 1) x t matrix of rank r to an s x t 
matrix of rank r plus the number of ways of augmenting an (s - 1) x t matrix of rank 
r - 1toans x t matrix of rank r. With the stated initial conditions the solution can be 
determined as
r—1 t i (qs - q)
®q(s,t,r) = (q - q V-t-1----—, r < s < t. 
(A.6)
i=0 
(qi+1 - 1)
It must also be the case that the total number of matrices is
s
^2( S^t[(s,t=') = qst 
for s < t.
r=0
It follows that for the equally distributed case the probability a random s x t matrix over 
Fq has rank r is
pq(s,t,r) = ^q(s,t,r)/qst.
The probability a kx(k+m) binary matrix (i.e., over F2) is of full rank (which would 
allow Gaussian elimination to be successful) is given by the expression in Equation A.7:
k+m , 
1 s 
co 
, 
1 s
$2(k,k + m,k)/2k(k+m) = Qk,m = [] (1 - 
« f[ (1 - 
. (A.7)
This quantity is approximated by
k+m 
k+m 
k+m
1 
j<Qt.m< 1 — £ j + £ 
2[j
j =m+1 
j =m+1 
i,j =m+1
https://doi.org/10.1017/9781009283403.019 Published online by Cambridge University Press

References
449
and it is straightforward to establish that
1
2 m
1
1
2k
1
< Qk,m < 1 - —
^ 
+ k + 22m '
The equation implies that for a sufficiently large k, the probability the matrix is of 
full rank tends to unity with m fast, independently of k. For example, for k large the 
probability a k x (k + 10) matrix is of full rank is in the range
0.999 < Qk,k +10 < 0.999 + e, e < 10-6. 
(A.8)
An interesting conjecture in [7] is that the average number of extra columns m for 
a k x (k + m) matrix over Fq to achieve full rank is only m = 1.606695, a conjecture 
supported by a significant number of simulations. It would be of interest to have a 
theoretical result to back this assertion up. These results are of interest to the random 
fountain coding scheme of Section 2.3.
References
[1] Cameron, P.J. 1994. Combinatorics: topics, techniques, algorithms. Cambridge 
University Press, Cambridge.
[2] Gabidulin, E.M. 1985. Theory of rank codes with minimum rank distance. Prob. 
Peredachi Inform., 21(1), 1-12.
[3] Gadouleau, M., and Yan, Z. 2008. On the decoder error probability of bounded 
rank-distance decoders for maximum rank distance codes. IEEE Trans. Inform. 
Theory, 54(7), 3202-3206.
[4] Greferath, M., Pavcevic, M.O., Silberstein, N., and Vazquez-Castro, M.A. (eds.). 
2018. Network coding and subspace designs. Signals and Communication Tech­
nology. Springer, Cham.
[5] Landsberg, G. 1893. Ueber eine Anzahlbestimmung und eine damit zusammenhn- 
gende Reihe. J. fr die Reine und Angew. Math., 111, 87-88.
[6] Lidl, R., and Niederreiter, H. 1997. Finite fields, 2nd ed. Encyclopedia of Mathe­
matics and Its Applications, vol. 20. Cambridge University Press, Cambridge.
[7] Studholme, C., and Blake, I.F. 2010. Random matrices and codes for the erasure 
channel. Algorithmica, 56(4), 605-620.
[8] van Lint, J.H., and Wilson, R.M. 1992. A course in combinatorics. Cambridge 
University Press, Cambridge.
https://doi.org/10.1017/9781009283403.019 Published online by Cambridge University Press

Appendix B Hasse Derivatives and Zeros of 
Multivariate Polynomials
The basic reference for this subject is [4] with interesting discussions in [5, 6] and 
[8] and further information in [1, 3]. Such derivatives are considered to have more 
interesting and useful properties for polynomials over a finite field than the usual 
derivative for fields of characteristic 0. The results of this section are mostly related 
to the discussion of multiplicity codes in Chapter 8.
Consider first the case of univariate functions. The n-th Hasse derivative of a 
univariate function f (x) will be denoted f [n] (x) and for a polynomial xm it is defined, 
for characteristic of the field either 0 or greater than n,as
(xm) [n] = ( (mm)xm -n, m > n
0, 
otherwise,
and so for a univariate polynomial f(x) = im=0 fixi
f[n] (x) = mCf - n and f [ n ] (x - a) = iLC^^iX - a) - n. 
nn
i=n 
i=n
It follows from the definition that if the ordinary (over fields of characteristic 0) 
derivative of f is denoted f (n)(x) = dnf (x)/dxn then
f[ n] (x) = 1 f(n(x). 
n!
For a bivariate polynomial f (x1,x2) = i ,i fi1,i2 x1i1 x2i2 the ordinary partial 
derivative is given by
d(j 1+j2) f(x 1 ,x 2)
dxj1 dx 2j2
i1,i2
ji11 j2! 
ji22 fi1,i2x1i1-j1x2i2-j2
i 1 > j 1, i 2 > j j2
while the equivalent Hasse derivative is
dx[ j 1] dx 2j2]
d[j 1+j2] f(x 1 ,x2)
fi1,i2x1i1-j1x2i2-j2
i1 > j1, i2 > j2.
Notice ([4], p. 148) that for indeterminates u,v
f(x1 + u,x2 + v) =
V d [ j 1+ j2] f(x 1 ,x 2 ) u 1 v2
d^ 
[j 1] [j2] u
j 1 ,j2>0 
dx 1 dx 2
450
https://doi.org/10.1017/9781009283403.020 Published online by Cambridge University Press

B Hasse Derivatives and Zeros of Multivariate Polynomials
451
The m-variate case is now straightforward to consider ([2, 3, 4, 5, 8]). Recall 
the notation of x = {x1,x2,...,xm} for the set of m variables and f(x) = 
f(x 1 ,x2, ... ,xm) e Fq[x] = Fq[x 1, ... ,xm] and [m] = {1,2, ... ,m}. For nonnegative 
integer m-tuples i = {i 1 ,i2, ... ,im} and j = {j 1 ,j2, ...,jm} denote i > j if ik > 
jk, k = 1,2, ... ,m and i > j if i > j and ik > jk for some k e [m]. Denote the total 
weight wt (i) = jm=1 ij the weight of the m-tuple i e Z>m0. As above, a term of the 
form
xi = x'i1 x 22 ••• xmm for i = { i 1 ,i 2... ,im }e Zm q
is referred to as a monomial and is of degree wt (i) = jm=1 ij . A multivariate 
polynomial in m variables is a sum of such monomials and its degree is the maximum 
of the degrees of its monomials:
f(x 1,x2, . . . ,xm) 
fi 1 ,i2,...,imxi1 x22 ••• xm = f(x) .
i1,..., im
Further, the m-variate polynomial f e Fq[x] is homogeneous of weight d if it is the 
sum of monomials each of weight d. For m-tuples i and j < i denote
/•x 
™ x
(’) = n j.
=Aje-J
The following definition is equivalent to the previous one:
Definition B.1 For m-variates x,z and an m-variate polynomial f(x) =
keZ>m fkxk e Fq [x] and a nonnegative integer m-tuple i e Z>m0 the Hasse derivative 
f [i](x) is defined as the coefficient ofzi in the expansion
f(x + z) = 
f [k] (x) Zk where f[k] = 
fj j x-k.
Several useful properties follow from such a definition. It is noted [3] that in fields 
of finite characteristic this definition of derivative is more interesting and in particular 
allows a form of Taylor series expansion to be given. The following properties [3, 4, 5, 
8] follow readily from the definition and are stated without proof. For two m-variable 
functions f,g e Fq [x] and nonnegative integer vectors i,j
(i) (f)[i] (x) = Xf[i] (x),
(ii) f[i] (x) + g[i] (x) = (f + g)[i] (x),
(iii) (f • g)[i](x) = Ej f [j'](x)g[i-j] (x),
(iv) 
for a homogeneous polynomial of weight d, 
f[i] (x) is homogeneous of degree d - wt (i),
(iv 
) (f[i] (x))[j] = (+j)f[i+j] (x).
The notion of multiplicities of zeros of multivariable polynomials is of interest in 
the definition of multiplicity codes of Chapter 8. The notion for univariate polynomials 
may be expressed that a univariate polynomial f(x) e Fq [x] has a zero of order s>0 
at a e Fq if (x - a)s | f (x). For multivariate polynomials the concept is slightly more 
complicated.
https://doi.org/10.1017/9781009283403.020 Published online by Cambridge University Press

452
B Hasse Derivatives and Zeros of Multivariate Polynomials
Definition B.2 The multiplicity of a zero of f(x) e Fq [x] at a point a e Fm, denoted 
mult(f, a), is the largest integer s such that for every i e Zm0, wt(i) < s it is true that 
f[i] (a) = 0. Alternatively f(x) has a zero of multiplicity s at a e Fqn if f(x + a) has 
no monomials of degree less than s .
Note that the number of monomials of degree less than s, by Equation 1.11 (with 
d = s - 1) is m+ms-1 , which will be used in the discussion of multiplicity codes. The 
number of zeros of multivariable polynomials is of interest for the definition of such 
codes. It is first noted ([7], theorem 6.13) that the number of zeros of the m-variate 
polynomial f(x) e Fq[x] of degree d is
< d • qm-1. 
(B.1)
The proof is straightforward using a double induction on the number of variables m 
and d and is omitted. A useful consequence of these observations is that two distinct 
m-variate polynomials over Fq of total degree d cannot agree on more than dqm-1 
of points of Fqm since otherwise their difference, a polynomial of total degree at most 
d, would have more than this number of zeros and the two polynomials are equal. A 
refinement of the above result [3, 5, 6] is that:
Lemma B.3 For f e Fq [x] an m-variable polynomial of total degree at most d, for 
any finite subset S C Fq
mult(f, a) < d •I S |m -1 .
aeSm
In particular, for any integer s>0
d
Prae Sni(mult(f, a) > s) < —Sy.
It follows that for any m-variate polynomial over Fq of degree at most d
mult (f, a) < dqm-1. 
(B.2)
aeFm 
q
An interesting property of the multiplicity of m-variate polynomials is:
Lemma B.4 ([3], lemma 5) Iff(x) e Fq [x] and a e Fqm are such that mult(f, a) = m, 
then mult(f [i], a) > m - wt(i).
Notice that for the univariate case m = 1 the notion of multiplicity coincides with 
the usual notion, i.e., a root a of the univariate polynomial f(x1 ) of multiplicity m 
implies that (x1 - a)m | f(x1 ) or x1m | f(x1 + a).
The following result ([3], proposition 10) is also of interest:
Proposition B.5 Given a set K C Fqm and nonnegative integers m,d such that 
m+n-1 •| | d+n 
nn
there exists a nonzero polynomial p e Fq [x] of total degree at most d such that 
mult(p, a) > m Ya e K.
https://doi.org/10.1017/9781009283403.020 Published online by Cambridge University Press

References
453
References
[1] Asi, H., and Yaakobi, E. 2019. Nearly optimal constructions of PIR and batch 
codes. IEEE Trans. Inform. Theory, 65(2), 947-964.
[2] Augot, D., Levy-dit Vehel, F., and Shikfa, A. 2014. A storage-efficient and robust 
private information retrieval scheme allowing few servers. Pages 222-239 of: 
Cryptology and network security. Lecture Notes in Computer Science, vol. 8813. 
Springer, Cham.
[3] Dvir, Z., Kopparty, S., Saraf, S., and Sudan, M. 2009. Extensions to the method 
of multiplicities, with applications to Kakeya sets and mergers. Pages 181-190 of: 
2009 50th Annual IEEE Symposium on Foundations of Computer Science - FOCS 
’09. IEEE Computer Society, Los Alamitos, CA.
[4] Hirschfeld, J.W.P., Korchmaros, G., and Torres, F. 2008. Algebraic curves over a 
finite field. Princeton Series in Applied Mathematics. Princeton University Press, 
Princeton, NJ.
[5] Kopparty, S., Saraf, S., and Yekhanin, S. 2011. High-rate codes with sublinear­
time decoding. Pages 167-176 of: Proceedings of the Forty-Third Annual ACM 
Symposium on Theory of Computing. STOC ’11. ACM, New York.
[6] Kopparty, S., Saraf, S., and Yekhanin, S. 2014. High-rate codes with sublinear-time 
decoding. J. ACM, 61(5), Art. 28, 20.
[7] Lidl, R., and Niederreiter, H. 1997. Finite fields, 2nd ed. Encyclopedia of Mathe­
matics and Its Applications, vol. 20. Cambridge University Press, Cambridge.
[8] Yekhanin, S. 2010. Locally decodable codes. Found. Trends Theor. Comput. Sci., 
6(3), front matter, 139-255 (2012).
https://doi.org/10.1017/9781009283403.020 Published online by Cambridge University Press

Index
m-sequence
autocorrelation, 369, 374
m-sequences, 362
cross correlation, 374
properties, 374
q -binomial coefficient, 304, 444
t design, 333
acyclic graph, 138
additive code, 422
adjacency matrix, 276
Advanced Encryption Standard, 381
algorithm
Berlekamp-Welch, 335
Diffie-Hellman, 382
Euclidean, 2
Ford-Fulkerson, 142
Guruswami-Rudra decoding, 358
Guruswami-Sudan list decoding, 344
linear information flow, 144
maximum likelihood, 34
message passing, 34
Parvaresh-Vardy list decoding
Reed-Solomon codes, 352
polynomial reconstruction, 337
Reed-Solomon list decoding, 337
Sudan list decoding, 338
ancilla states, 414
anticode, 308
anticode bound, 320
augmented flows, 132
autocorrelation of sequences, 368
balanced code, 425
basis
normal, 9
trace dual, 10
batch code
bipartite graphs, 262
from multiplicity code, 264
LDC and PIR codes, 271
primitive multiset, 258
subcube, 259
batch codes, 255-272
from multiplicity codes, 264
belief propagation algorithm, 34
Bhattacharyya parameter, 100
binary input DMC (BDMC), 18
binary input symmetric channel (BMS), 18
binary input white Gaussian
channel(BIAWGN)
capacity, 68
binary symmetric channel (BSC)
capacity, 68
binomial theorem, 445
bound
anticode, 320
minimum bandwidth repair (MBR), 164
minimum storage (MSR), 164
Singleton, 304
sphere packing, 305
Varshamov-Gilbert, 305
Welch, 368, 371
bra notation, 405
butterfly network, 133
capacity achieving sequences, 40
capacity of DMC, 19
Cauchy-Schwarz inequality, 373
Cayley graph, 282
454
https://doi.org/10.1017/9781009283403.021 Published online by Cambridge University Press

Index
455
channel capacity, 68
channel polarization, 102
channel symmetry, 79
characteristic equation, 361
characteristic polynomial, 362
check node symmetry, 79
closest vector problem, 396
code
k-PIR, 249
balanced, 425
batch, 255-272
combinatorial batch, 265, 266
constrained, 438
covering, 239
CSS, 413
cyclic, 11
distributed storage, 157-179
expander, 275, 290
folded Reed-Solomon, 354
for private information retrieval,
241
frameproof, 435
Gabidulin, 302
Gaussian channel, 429
Generalized Reed-Muller (GRM), 15, 213
Generalized Reed-Solomon (GRS), 12, 168
Golay, 333
Gray, 424
Hadamard, 211
identifiable parent property (IPP), 434
lifted, 222
linear block, 27
linear network, 140
locally correctable, 208
locally decodable, 206
locally repairable, 182-203
locally testable, 208
LT, 44
matching vector, 220
maximally recoverable, 195
maximum distance separable (MDS), 12
maximum rank distance code (MRD),
304
multiplicity, 214
multiset batch, 257
network, 128
normal graph representation, 29
Parvaresh-Vardy, 358
permutation, 433
polar, 102
primitive batch multiset, 261
primitive multiset batch, 258
private information retrieval (PIR), 246
private information retrieval and
multiplicity code, 248
quantum, 404-422
rank metric, 298-324
Raptor, 44-62
Reed-Muller, 14-16, 124
Reed-Solomon, 13, 168, 334
run-length limited, 438
Shor nine bit, 421
snake-in-the-box, 424
stabilizer, 413
Tanner, 283
twisted Gabidulin, 312
WOM, 427
code base cryptosystems, 394
column space, 300
combinatorial batch code, 265, 266
uniform, 270
computational cutoff rate, 100
conservation of flow, 130
constant dimension code, 314
anticode bound, 320
Singleton bound, 318
sphere packing bound, 317
Varshamov-Gilbert bound, 317
constrained code, 438
controlled gate, 390
covering code, 239
cross correlation of sequences, 368
CSS Code, 413
cumulative distribution function (cdf), 78
cyclic code, 11
cyclic graph, 138
decimated sequence, 365
density evolution, 86
Diffie-Hellman algorithm, 382
digital signature, 384
digraph, 129
discrete logarithm problem, 383
elliptic curve, 385
discrete memoryless channel (DMC), 18, 99
distance function, 299
distributed storage code, 157-179
dual basis, 303
dual set system, 266
edge distribution, 69
edge head, 130
https://doi.org/10.1017/9781009283403.021 Published online by Cambridge University Press

456
Index
edge tail, 130
elliptic curve cryptography, 384
entropy, 16, 99
q -ary, 16, 329
conditional, 17
Euclidean algorithm, 2
Euclidean geometry, 443
Euler Totient function, 2, 234
expander code, 275, 290
minimum distance, 290
expander graph, 279
extended transfer matrix, 141
extrinsic information transfer (EXIT), 91
feasible flow, 130
finite geometries, 443
flipping algorithm, 291
flow, 130
flow value, 130
folded Reed-Solomon code, 354
frameproof code, 435
Frobenius trace, 384
fundamental domain, 396
fundamental domain of lattice, 432
Gabidulin code, 302
Gaussian approximation, 90
Gaussian coefficients, 444
Gaussian elimination, 34
generalized Gabidulin code, 312
Generalized Reed-Muller (GRM) code, 15, 
213
Generalized Reed-Solomon (GRS) code, 12, 
168
global encoding vector, 152
Golomb postulates, 370
graph
adjacency matrix, 276
Cayley, 282
diameter, 276
edge-vertex, 278
expander, 279
expansion parameter, 280
Ramanujan, 281
regular, 276
spectrum, 276
Tanner, 277
graphs, 275
Grassmanian, 313, 444
Gray code, 424
Grover’s problem, 387
Guruswami-Rudra list decoding algorithm, 
358
Guruswami-Sudan list decoding algorithm, 
344
Hadamard code, 211
Hall’s theorem, 269
hash function, 381, 399
hidden subgroup problem, 387
identifiable parent property (IPP), 434
identifiable parent property (IPP) code, 434
information flow graph, 160
injection distance, 315
Jacobi symbol, 234
kernel, 103
ket notation, 405
lattice, 431
ldpc code, 66, 94
performance on the BIAWGN, 77
Legendre symbol, 234
lexicographic ordering monomials,
347
LFSR sequences, 362
lifted code, 222
lifting, 322
likelihood ratio (LR), 80
linear coding, 133
linear network code, 140
linearized polynomial, 310, 447
list decoding, 327-359
capacity, 330
definition, 329
locally correctable code, 208
locally decodable code, 206
smooth, 208
Yekhanin 3-query, 223
locally repairable code, 182-203
locally testable code, 208
log likelihood ratio (LLR), 80
LT codes, 44-51
matching vector code, 220
matrices rank r over Fq
enumeration, 304
max-flow min-cut theorem, 131
maximally recoverable code, 195
maximum distance separable (MDS) code, 12 
https://doi.org/10.1017/9781009283403.021 Published online by Cambridge University Press

Index
457
maximum length shift register sequences, 361
maximum likelihood algorithm (ML), 34
maximum rank distance (MRD) code, 304
construction, 309-313
Delsarte, 309
Gabidulin, 310
generalized Gabidulin, 312
Merkle authentication tree, 400
message passing algorithm, 34
message symmetry, 79
min-cut, 130
capacity, 130
value, 130
minimum bandwidth repair bound (MBR), 164
minimum cut-set storage repair bound, 164
monomial, 14
monomial enumeration, 14
multicast network, 133, 135
multiplicity code, 214
and batch code, 264
and private information retrieval, 248
multiset batch code, 257
multisets, 193
multivariable polynomials
multiplicity of zeros, 452
multivariate polynomial, 14
multivariate polynomials
zeros, 347
mutual information, 17, 99
network
acyclic, 142
network code, 135
algebraic formulation, 139
fundamental theorem, 138
random linear (RLNC), 146
network codes, 128-153
robust, 144
network coding, 133
no cloning theorem, 392
norm, 299
normal basis, 9
normal pdf, 79
one-way function, 399
orthogonality defect, 397
Parvaresh-Vardy code, 358
Parvaresh-Vardy list decoding, 353
Pauli matrices, 406
peer-to-peer networks, 146
permutation code, 433
physically degraded channel, 88
pn sequences, 370
polar code, 124
definition, 117
polar code construction, 102
polar codes, 97
polynomial
multivariate, 14
polynomial interpolation, 13
post-quantum cryptography, 380, 394
code based, 394
post-quantum cryptosystems
hash based, 399
hidden field equations, 400
Lamport signature, 399
lattice based, 395
McEliece system, 394
multivariable systems, 400
primitive multiset batch code, 261
private information retrieval (PIR), 230-251
and smooth LDC, 241
and smooth LDC’s, 250
multiple server, 237
multiplicity code, 248
single server, 233
subcube code, 246
probability density function (pdf), 78
projective geometry, 443
public key crytpography, 233
q-polynomial, 311
quadratic nonresidue, 234
quadratic residue, 234
quadratic residuosity problem, 233
quantum code, 404-422
five qubit, 411
nine qubit, 412
projection operators, 408
Steane seven qubit, 415
three qubit, 408
quantum Fourier transform, 392
quantum integer factorization, 386
quantum logic gate, 390
quantum operator weight, 407
quantum Singleton bound, 417
quantum Varshamov-Gilbert bound,
417
Ramanujan graph, 281
random matrices, 448
https://doi.org/10.1017/9781009283403.021 Published online by Cambridge University Press

458
Index
rank distribution
MRD code, 307
rank metric code, 298-324
anticode, 308
basic properties, 299-308
constant rank code (CRC), 301
Delsarte, 302
dual, 302
equivalence, 301
Gabidulin, 302
linear, 301
maximum rank distance (MRD), 304
rank distribution, 301
Singleton bound, 304
sphere packing bound, 305
Varshamov-Gilbert bound, 305
rank of vector, 302
rank vector code, 303
Raptor code, 44, 62
rate
asymptotically achievable, 138
rate of polarization, 121
reduced network, 131
Reed-Muller code, 14-16
Reed-Solomon code, 13, 168, 334
regenerating codes for distributed storage, 167
regular graph, 276
repair bandwidth, 162
repair group, 263
repair groups
disjoint, 245
robust network codes, 144
row space, 300
RSA cryptosystem, 383
run-length limited code, 438
second eigenvalue, 279, 282
sequences
correlation properties, 368
cyclically equivalent, 364
Gold, 375
Kasami, 375
set system, 266
shift register, 361
Shor nine qubit code, 412, 421
shortest vector problem, 396
Simon’s problem, 387
Singleton bound, 304
constant dimension code, 318
rank metric code, 304
sink node, 129, 135
Snake-in-the-box codes, 424
source node, 129, 135
spectral gap, 279
sphere packing bound, 305
constant dimension code, 317
square root rpoblem, 235
stabilizer codes, 413
Steane code, 420
Steane seven qubit code, 415
Steiner system, 333
stochastic erasure operator, 318
sub-channel polarization, 111
sub-packetization level, 166, 179
subcube batch code, 259
subcube PIR code, 238
subexponential complexity, 385
subspace code, 313-324
constant dimension, 314
injection distance, 315
successive cancellation decoding, 111
Sudan list decoding algorithm, 338
syndromes, 410
system of distinct representatives, 269
Tanner code, 283
Tanner graph, 71
tensor product, 98
theorem
fundamental network coding, 138
max-flow min-cut, 131, 162
trace dual basis, 10
trace function, 9
trace product, 300
traitor tracing, 435
transaction, 137
twisted Gabidulin code, 312
unicast network, 133
universal set of gates, 391
variable node symmetry, 79
variational distance, 102
Varshamov-Gilbert bound, 305
constant dimension code, 317
Voronoi region of lattice point, 432
Welch bound, 368, 371
WOM code, 427
Yekhanin 3-query code, 223
https://doi.org/10.1017/9781009283403.021 Published online by Cambridge University Press


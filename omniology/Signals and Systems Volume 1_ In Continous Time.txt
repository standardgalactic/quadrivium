Weigang Zhang
Signals and Systems
De Gruyter Graduate
Unauthenticated
38 PM

Also of Interest
Signals and systems
G. Li, L. Chang, S. Li, 2015
ISBN 978-3-11-037811-5, e-ISBN (PDF) 978-3-11-037954-9, e-ISBN
(EPUB) 978-3-11-041684-8
Energy harvesting
O. Kanoun (Ed.), 2017
ISBN 978-3-11-044368-4, e-ISBN (PDF) 978-3-11-044505-3, e-ISBN
(EPUB) 978-3-11-043611-2, Set-ISBN 978-3-11-044506-0
Pulse width modulation
S. Peddapelli, 2016
ISBN 978-3-11-046817-5, e-ISBN (PDF) 978-3-11-047042-0, e-ISBN
(EPUB) 978-3-11-046857-1, Set-ISBN 978-3-11-047043-7
Wind energy & system engineering
D. He, G. Yu, 2017
ISBN 978-3-11-040143-1, e-ISBN 978-3-11-040153-0, e-ISBN (EPUB)
978-3-11-040164-6, Set-ISBN 978-3-11-040154-7
Unauthenticated
38 PM

Weigang Zhang
Signals and
Systems
|
Volume 1: In continuous time
Unauthenticated
38 PM

Author
Prof. Weigang Zhang
Chang’ an University
Mid South 2nd Ring Road
Shaanxi Province
710064 XI ’AN China
wgzhang@chd.edu.cn; 648383177@qq.com
ISBN 978-3-11-041754-8
e-ISBN (PDF) 978-3-11-041953-5
e-ISBN (EPUB) 978-3-11-042650-2
Library of Congress Cataloging-in-Publication Data
A CIP catalog record for this book has been applied for at the Library of Congress.
Bibliographic information published by the Deutsche Nationalbibliothek
The Deutsche Nationalbibliothek lists this publication in the Deutsche Nationalbibliograﬁe;
detailed bibliographic data are available on the Internet at http://dnb.dnb.de.
© 2018 Walter de Gruyter GmbH, Berlin/Boston
Cover image: Creatas/Creatas/thinkstock
Typesetting: le-tex publishing services GmbH, Leipzig
Printing and binding: CPI books GmbH, Leck
♾Printed on acid-free paper
Printed in Germany
www.degruyter.com
Unauthenticated
38 PM

Preface
The course of Signals and systems is an important professional, fundamental course
for undergraduates majoring in electronics, information and communication, and
control, etc. The course has a profound inﬂuence on the cultivation of students’ over-
all capabilities, such as independent learning, scientiﬁc thinking in problem solving,
practical skills, etc. The course is not only compulsory for undergraduates, but it is
also necessary for postgraduate entrance examinations in related majors. The course
plays a critical role in undergraduate education and it is the theoretical foundation
to information theory and information technology. It is even regarded as the key to
opening the door to information science in the twenty-ﬁrst century.
Main contents in the course
From the content aspect, the course of Signals and Systems is more of a mathemat-
ics course integrated into professional characteristics than a specialized course. The
so called signal is actually the function in mathematics, given “voltage”, “current” or
other physical background. System is considered as a module that can transfer (pro-
cess) a signal.
In essence, the contents of the course can be summarized as the study of the rela-
tion between before and after a signal is transformed by a given system or a function
is processed by a given operation module. Here, the signal before being transformed
is called the input or the excitation, and the signal after being transformed is called
the output or the response. The excitation is the cause (or the independent variable
in mathematical description), and the response is the consequence (or the dependent
variable in mathematical description). Mathematically, it can be described as after
an independent variable (excitation) is calculated by a module (system) a dependent
variable (response) is the result (plotted in 󳶳Figure 1). A real physical system (a trans-
former and the relation between voltages on its two ports) is shown in 󳶳Figure 1a and
the mathematical description or the equivalent model of this physical system is given
)
(
)
(
1
2
t
u
n
n
t
u
i
o
=
System
Excitation
Response
]
[
T
)
(t
y
)
(t
f
)
(t
ui
)
(t
uo
1n
2
n
K
)
(t
f
)
(t
y
)
(
)
(
t
Kf
t
y
=
Physical description
Mathematical description
(a)
(b)
Fig. 1: Excitation, response and system.
https://doi.org/10.1515/9783110419535-201
Authenticated
39 PM

VI
|
Preface
by 󳶳Figure 1b. From the ﬁgures it can be seen that, in fact, Signals and Systems is just
a course that abstracts a physical system as a mathematical model, and then studies
the performance of the system through analyzing the model, namely solving the rela-
tionship between the excitation and the response. Please note that the symbol T[∙] in
󳶳Figure 1 shows a processing method or a transformation to the element in brackets [
]. It is obvious that the signal is an object processed by the system, and the system is
the main carrier to process the signal; both complement each other.
Herein, the relationship between signal and system is described by a mathemat-
ical model (mathematical analysis formula), such as y(t) = T[f(t)], therefore, solving
themathematicalmodelor solvingtheequationinlayman’stermswillrunthroughthe
course. The various solutions for the model are main knowledge points of the book.
Since the signal and the system are the two key points of this course, all research
is concentrated around them.
The analysis of signals covers the following points:
(1) Modeling. Various physical signals in the real world can be mathematically ab-
stracted into mathematical models, and the process is known as “mathematical
modeling.” The aim of the modeling is to change a physical signal into a function
that can be analyzed theoretically on paper.
(2) Decomposition and composition of signals. One signal can be decomposed into a
linear combination of other signals; or, a set of signals can be used to represent a
signal with their linear combination.
Solutions in time 
domain
Solutions
in frequency
domain
Fourier series
solutions
Fourier transform
solutions
Solutions
in s-domain
Classical solution
Convolution 
solution
Solutions
with impulse response
Solutions with 
models
in s-domain
Solutions in 
time domain
solutions
in z-domain
Solutions
of difference equation
Discrete 
convolution 
solutions
solving
differential 
model
Solutions in time 
domain
Solutions
in s-domain
Solutions
in z-domain
solving
difference
model
solving
state 
model
Fig. 2: Tree of content in Signals and Systems.
Authenticated
39 PM

Preface
|
VII
The analysis of systems focuses on the study of the response of a given system to an
arbitrary excitation (󳶳Figure 2), or, analyzes the transform characteristics of a system
to signals known under the system constitution.
Brieﬂy, this book consists of two parts, signal analysis and system analysis, and
discusses how to solve differential or difference equations in the time and transform
domains (real frequency domain, complex frequency domain and z domain).
Features of the course
This course has three main features:
(1) A sound theoretical basis. Various mathematical methods to solve the differential
or difference equations in the time and transform domains are introduced.
(2) A strong specialty. Building up mathematical models of various systems from the
real world must rely on the fundamental laws and theorems of related ﬁelds.
(3) Wide applications. The research results can be generalized to real applications in
nature and society, even to nonlinear system analysis.
The aims of learning
After thinking carefully, we ﬁnd that the real world is constituted by various systems.
For example, the human body includes the nervous, blood and digestion systems, etc.;
and then there is transport, lighting, water supply, ﬁnance, communication, control
systems and so on in daily life. The functions of systems can be summarized as pro-
cessing or transforming an input. So, the relationships between the inputs and the
outputs of these systems are exactly the main topic studied in this book.
To facilitate research, real physical systems can be abstracted into mathematical
models. Further, these models can be classiﬁed into two types of linear and nonlinear
systems according to their characteristics. Thus, the aims of learning here are to mas-
ter methods to analyze the relationship between the excitation and the response of a
linear system and to apply these analytical results to the analysis of nonlinear systems
and then to solve various practical problems in real systems.
Learning the course can help us to establish a correct, scientiﬁc and reasonable
approach to analyzing and solving problems, and to improve the treatment ability of
various problems and difficulties encountered in study, work and life, while at the
same time, to master how to solve practical problems using basic knowledge, espe-
cially mathematical knowledge.
Authenticated
39 PM

VIII
|
Preface
Research route of this book
The contents of this book can be divided into two layers: the lower layer which is signal
analysis and the upper layer which is system analysis. The lower layer is the basis of
the upper layer, while the upper one is the achievement of the lower one. Based on
󳶳Figure 2, the research route of this book is shown in 󳶳Figure 3.
modeling
Continuous / discrete signals
Periodic/ Non periodic functions
Decomposition and combination of function (signal)
modeling
Continuous / Discrete LTI systems
Differential / Difference /State equations
Transform domain 
methods
Time domain methods
analyzing
solving
Zero input response + zero state response
Signal analysis layer
System analysis layer
Convolution +Discrete convolution +Fourier series+ + Fourier 
transform + Laplace transform +z transform
result
result
Fig. 3: The roadmap of the book.
Authenticated
39 PM

Preface
|
IX
Relationship between the course and other basic courses
There is no doubt that mathematics is an important basic course for signals and sys-
tems. The mathematics knowledge herein includes expansion and summation of se-
ries, solutions of differential/difference and algebraic equations, partial fraction ex-
pansion, basic calculus operations and linear algebra. In addition, this book, which
takes electric systems as the objects to be studied, also involves professional knowl-
edge of circuit analysis, analog circuits and digital circuits. Among of these ﬁelds, cir-
cuit analysis relates closely to the work herein; it is the precursor of the course, and
the course is an expansion of content and the improvement in methodologies of the
former. The similarities and differences between them are listed below:
(1) Both of the objects to be studied are circuits or networks consisting of electronic
components.
(2) Both of the main aims are to ﬁnd circuit variables such as voltage and current.
(3) The main analysis method in circuit analysis is to obtain the responses (node
voltages and branch currents) of a circuit to excitations by constructing algebraic
equations where excitations are direct or alternating current signals.
(4) The main analysis method for signals and systems is to obtain the responses (out-
put voltages or output currents) of a system to excitations by building differential/
difference equations where excitations are periodic or nonperiodic signals.
For instance, in 󳶳Figure 4, if uS and i are, respectively, the excitation and the response
of a system or a circuit, how can we obtain the response i of the system under differ-
ent excitations? From circuit analysis, the currents in 󳶳Figure 4a and b are i = uS
R and
̇I =
̇US
R+jωL, respectively. Because excitations in 󳶳Figure 4c and d are non-sinusoidal
periodic and nonperiodic signals, respectively, the currents cannot be obtained. For-
tunately, these problems can be solved by Fourier series and Fourier transform in sig-
nals and systems.
(5) Circuits Analysis includes solution methods for algebraic equations obtained by
laws and theorems of circuits and the phasor analysis method for alternating cur-
rent circuits.
+
−
S( )
u t
( )
i t
R
L
t
o
U
S
u
+
−
S( )
u t
( )
i t
R
L
t
o
U
S
u
+
−
S( )
u t
( )
i t
R
L
t
o
U
S
u
+
−
S( )
u t
( )
i t
R
L
t
o
U
S
u
(a)
(b)
(c)
(d)
Fig. 4: Circuits Analysis examples.
Authenticated
39 PM

X
|
Preface
LTI electric system
(circuit)
Excitation
Response
1. DC Signals
2. AC Signals
Circuits analysis
Signals and Systems
( )
u t
( )
i t
( )
u t
( )
i t
Response
( )
u t
( )
i t
( )
u t
( )
i t
1. DC Signals
2. AC Signals
1. Aeriodic signals
2. Aperiodic signals
3. Discrete signals
1. Aeriodic signals
2. Aperiodic signals
3. Discrete signals
Algebraic equation
Differential or difference
 equation
Excitation
Fig. 5: Main differences between Signals and Systems and Circuits Analysis.
(6) Signals and Systems include solution methods for differential equations in the
time, frequency and complex frequency domains and difference equations in the
time and z domains.
The main differences and similarities between the two courses are shown in 󳶳Figure 5.
Note that the points in the ﬁgure only focus on electricity technology, in fact, the con-
tents of Signals and Systems can be also used in mechanical systems and other anal-
ogous systems.
Status of the course
In conclusion, Signals and Systems is a professional basic course, which takes the
signal and the system as the core, has system performance analysis as the purpose and
employs mathematics as the tool to establish the mathematical model as the premise
and solve the model as the means.
Features of this textbook
The Signals and Systems course is not only a specialized course with professional con-
cepts, but also a mathematical one with massive computations. To help readers master
the contents in a better way, we deliberately increased the number of examples (about
140 sets) to expand insights and to improve understanding by analogy. At the same
time, we also arranged more than 160 problems with answers to help readers grab
and consolidate knowledge learned. In addition, to deepen readers’ understanding of
the contents and to improve problem solving skills, a section called Solved Questions
was added at the end of each chapter, which includes about 50 exam questions and
solutions selected from other universities.
Authenticated
39 PM

Preface
|
XI
Due to space limitations, this book has been split into two volumes. Volume 1
mainly discusses problems concerning continuous-time signals and systems analysis.
Volume 2 focuses on issues about discrete time signals and systems analysis.
I would like to express my sincere thanks to Associate Professor Wei-Feng Zhang,
the coauthor of the books, and Associate Professors Tao Zhang and Shuai Ren, and
lecturers Xiao-Xian Qian and Jian-Fang Xiong for preparing the Solved Questions and
translating the Chinese manuscript into English. I also wish to thank Dan-Yun Zheng,
Jie-Xu Zhao, Xiang-Yun Li, Pei-Cheng Wang, Juan-Juan Wu, Jing Wu and Run-Qing
Li for proofreading all examples, exercises and answers and helping to translate the
manuscript. Finally, thanks are due to authors and translators of reference books in
the books.
The books are the summary of the authors’ teaching experience of many years;
any suggestions for improvements to the book would be greatly appreciated. Please
feel free to contact us about any problems encountered during reading; we would be
grateful for your comments.
June 2016
Weigang Zhang
Authenticated
39 PM

Authenticated
39 PM

Contents
Preface | V
1
Signals | 1
1.1
The concept of signals | 1
1.2
Classiﬁcation of signals | 3
1.2.1
Continuous time and discrete time signals | 3
1.2.2
Periodic and aperiodic signals | 7
1.2.3
Energy and power signals | 8
1.2.4
Deterministic and random signals | 9
1.2.5
Causal and anticausal signals | 10
1.3
Basic continuous-time signals | 10
1.3.1
Direct current signals | 11
1.3.2
Sinusoidal signals | 11
1.3.3
Exponential signals | 12
1.3.4
Complex exponential signals | 13
1.3.5
Signum signal | 14
1.3.6
Unit step signal | 14
1.3.7
Unit ramp signal | 16
1.3.8
Unit impulse signal | 16
1.3.9
Unit doublet signal | 20
1.3.10
Unit gate signal | 22
1.3.11
Bell shaped pulse signal | 23
1.4
Operations of continuous signals | 24
1.4.1
Arithmetic operations | 24
1.4.2
Operations of even and odd signals | 25
1.4.3
Time shifting | 25
1.4.4
Time reversal | 26
1.4.5
Time scaling | 26
1.4.6
Differentiation and integration | 28
1.4.7
Decomposition and synthesis | 28
1.4.8
Convolution integral | 29
1.4.9
Plotting | 36
1.5
Solved questions | 37
1.6
Learning tips | 41
1.7
Problems | 41
Unauthenticated
38 PM

XIV
|
Contents
2
Systems | 45
2.1
The concept of a system | 45
2.2
Excitation, response and system state | 46
2.3
Classiﬁcation of systems | 49
2.3.1
Simple and complex systems | 49
2.3.2
Continuous-time and discrete-time systems | 50
2.3.3
Linear and nonlinear systems | 50
2.3.4
Time-variant and time-invariant systems | 55
2.3.5
Causal and noncausal systems | 57
2.3.6
Dynamic and static systems | 58
2.3.7
Open-loop and closed-loop systems | 58
2.3.8
Stable and unstable systems | 59
2.3.9
Lumped and distributed parameter systems | 59
2.3.10
Invertible and nonreversible systems | 60
2.4
Models of LTI systems | 61
2.4.1
Mathematical models | 61
2.4.2
Mathematical modeling | 62
2.4.3
Block models | 65
2.5
Analysis methods for LTI systems | 67
2.6
Solved questions | 68
2.7
Learning tips | 70
2.8
Problems | 71
3
Analysis of continuous-time systems in the time domain | 73
3.1
Analysis methods with differential equations | 74
3.1.1
The classical analysis method | 74
3.1.2
Response decomposition analysis method | 78
3.2
Impulse and step responses | 88
3.2.1
Impulse response | 88
3.2.2
Step response | 90
3.3
The operator analysis method | 94
3.3.1
Differential and transfer operators | 94
3.3.2
Determining impulse response by the transfer operator | 99
3.4
The convolution analysis method | 100
3.5
Judgment of dynamics, reversibility and causality | 103
3.5.1
Judgment of dynamics | 104
3.5.2
Judgment of reversibility | 105
3.5.3
Judgment of causality | 105
3.6
Solved questions | 105
3.7
Learning tips | 107
3.8
Problems | 107
Unauthenticated
38 PM

Contents
|
XV
4
Analysis of continuous-time systems excited by periodic signals in the real
frequency domain | 111
4.1
Orthogonal functions | 112
4.1.1
The orthogonal function set | 112
4.1.2
Trigonometric function set | 114
4.1.3
Imaginary exponential function set | 114
4.2
Fourier series | 115
4.2.1
Trigonometric form of Fourier series | 115
4.2.2
Relations between function symmetries and Fourier coefficients | 118
4.2.3
Exponential form of the Fourier series | 122
4.2.4
Properties of the Fourier series | 125
4.3
Frequency spectrum | 129
4.3.1
Concept of frequency spectrum | 129
4.3.2
Properties of the frequency spectrum | 132
4.4
Fourier series analysis | 138
4.4.1
System function | 138
4.4.2
Analysis method | 139
4.5
Solved questions | 142
4.6
Learning tips | 144
4.7
Problems | 144
5
Analysis of continuous-time systems excited by nonperiodic signals in the
real frequency domain | 147
5.1
The concept of Fourier transform | 147
5.2
Fourier transforms of typical aperiodic signals | 152
5.2.1
Gate signals | 152
5.2.2
Unilateral exponential signals | 153
5.2.3
Bilateral exponential signals | 153
5.2.4
Unit DC signals | 154
5.2.5
Unit impulse signals | 155
5.2.6
Signum signals | 156
5.2.7
Unit step signals | 157
5.3
Properties of the Fourier transform | 158
5.3.1
Linearity | 158
5.3.2
Time shifting | 159
5.3.3
Frequency shifting | 160
5.3.4
Time scaling | 161
5.3.5
Symmetry | 163
5.3.6
Properties of convolution | 165
5.3.7
Differentiation in the time domain | 167
5.3.8
Integration in the time domain | 169
Unauthenticated
38 PM

XVI
|
Contents
5.3.9
Modulation | 170
5.3.10
Conservation of energy | 171
5.4
Fourier transforms of periodic signals | 172
5.5
Solutions for the inverse Fourier transform | 174
5.6
System analysis methods for aperiodic signals | 175
5.6.1
Analysis method from system models | 175
5.6.2
Analysis with the system function | 176
5.6.3
Analysis with signal decomposition | 177
5.7
System analysis methods for periodic signals | 181
5.8
The Hilbert transform | 182
5.9
Advantages and disadvantages of Fourier transform analysis | 185
5.10
Solved questions | 185
5.11
Learning tips | 188
5.12
Problems | 189
6
Analysis of continuous-time systems in the complex frequency
domain | 193
6.1
Concept of the Laplace transform | 193
6.2
Laplace transforms of common signals | 198
6.3
Laplace transforms of periodic signals | 198
6.4
Properties of the Laplace transform | 199
6.4.1
Linearity | 199
6.4.2
Time shifting | 200
6.4.3
Complex frequency shifting | 202
6.4.4
Time scaling | 202
6.4.5
Differentiation in the time domain | 203
6.4.6
Integration in the time domain | 204
6.4.7
Convolution theorem | 205
6.4.8
Initial value theorem | 207
6.4.9
Final value theorem | 207
6.4.10
Differentiation in the s domain | 209
6.4.11
Integration in the s domain | 210
6.5
Solutions for the inverse Laplace transform | 211
6.6
Analysis method of the system function in the s domain | 216
6.6.1
System function | 216
6.6.2
Analysis method with the system function | 219
6.7
Analysis methods with system models in the s domain | 221
6.7.1
Analysis with mathematic models | 221
6.7.2
Analysis with a circuit model | 224
6.8
Analysis method from signal decomposition in the s domain | 230
6.9
Relationship among the time, frequency and complex frequency domain
methods | 231
Unauthenticated
38 PM

Contents
|
XVII
6.10
Solved questions | 233
6.11
Learning tips | 236
6.12
Problems | 236
7
Simulation and stability analysis of continuous-time systems | 241
7.1
System simulation | 241
7.1.1
Basic arithmetic units | 241
7.1.2
Simulating system with block diagrams | 242
7.1.3
Simulating systems with ﬂow graphs | 245
7.2
System stability analysis | 255
7.2.1
System stability | 255
7.2.2
Pole-zero analysis of the system function H(s) | 257
7.2.3
Relationships between stability and ROC, and poles | 264
7.2.4
Stability judgment based on the R–H criterion | 264
7.3
Controllability and observability of a system | 270
7.4
Solved questions | 272
7.5
Learning tips | 276
7.6
Problems | 276
A
Reference answers | 281
Bibliography | 303
Index | 305
Unauthenticated
38 PM

Unauthenticated
38 PM

Contents of Volume 2
Preface | V
Preface of Volume I | VII
8
Analysis of discrete signals and systems in the time domain | 1
8.1
Basic discrete signals | 1
8.1.1
Periodic sequences | 1
8.1.2
Sinusoidal sequences | 2
8.1.3
Complex exponential sequences | 3
8.1.4
Exponential sequences | 4
8.1.5
Unit step sequence | 5
8.1.6
Unit impulse sequence | 5
8.1.7
z sequence | 6
8.2
Fundamental operations of sequences | 6
8.2.1
Arithmetic operations | 6
8.2.2
Time shifting | 7
8.2.3
Time reversal | 9
8.2.4
Accumulation | 9
8.2.5
Difference | 9
8.2.6
Time scaling | 10
8.2.7
Convolution sum | 11
8.2.8
Sequence energy | 17
8.2.9
Sequence representation with unit impulse sequences | 17
8.3
Discrete systems | 17
8.3.1
The concept of discrete systems | 17
8.3.2
Properties of discrete systems | 17
8.4
Description methods for discrete systems in the time domain | 19
8.4.1
Representation with the difference equation | 19
8.4.2
Representation with the transfer operator | 21
8.4.3
Representation with unit impulse response | 23
8.5
Analysis of discrete systems in the time domain | 24
8.5.1
Classical analysis method | 24
8.5.2
Unit impulse response | 28
8.5.3
Unit step response | 31
8.5.4
Analysis with response decomposition | 31
8.6
Testing for memorability, invertibility and causality | 38
8.7
Solved questions | 39
8.8
Learning tips | 41
8.9
Problems | 42
Unauthenticated
38 PM

XX
|
Contents of Volume 2
9
Analysis of discrete signals and systems in the z domain | 47
9.1
Deﬁnition of the z transform | 47
9.2
z transforms of typical sequences | 49
9.3
Properties of the z transform | 51
9.4
Solutions of the inverse z transform | 55
9.5
Relations between the z domain and the s domain | 58
9.6
Analysis of discrete systems in the z domain | 61
9.6.1
Analysis method from the difference equation | 61
9.6.2
System function analysis method | 66
9.6.3
Sequence decomposition analysis method | 69
9.7
Emulation of discrete systems | 69
9.8
Stability analysis of discrete systems | 71
9.9
Analysis methods of discrete systems in the frequency domain | 74
9.9.1
Discrete-time fourier series | 74
9.9.2
Discrete time fourier transform | 75
9.9.3
Analysis method of discrete systems with fourier transform | 76
9.9.4
Frequency characteristic of the discrete system | 76
9.10
Concept comparisons between discrete systems and continuous
systems | 79
9.11
Solved questions | 80
9.12
Learning tips | 82
9.13
Problems | 82
10
State space analysis of systems | 87
10.1
State space description of a system | 87
10.2
State equations of a system | 90
10.3
Establishment of the state model | 92
10.3.1
Establishment from circuit diagrams | 93
10.3.2
Establishment method from emulation diagrams | 95
10.3.3
Establishment method from mathematical models | 97
10.4
Solutions of the state model | 102
10.4.1
Solutions in the s domain | 102
10.4.2
Solutions in the time domain | 108
10.4.3
Calculation of eAt | 111
10.5
Judging stability | 112
10.6
Judging controllability and observability | 113
10.7
Solved questions | 114
10.8
Learning tips | 117
10.9
Problems | 118
Unauthenticated
38 PM

Contents of Volume 2
|
XXI
11
Applications of signals and systems analysis theories and methods | 123
11.1
Nondistortion transmission systems | 123
11.2
Equilibrium systems | 125
11.3
Filtering systems | 127
11.3.1
Ideal ﬁlters | 128
11.3.2
Real ﬁlters | 131
11.4
Modulation/demodulation systems | 131
11.4.1
DSB modulation system | 132
11.4.2
DSB demodulation system | 134
11.4.3
AM modulation system | 135
11.4.4
AM demodulation system | 136
11.5
Solved questions | 141
11.6
Learning tips | 143
11.7
Problems | 144
A
Reference answers | 145
Bibliography | 157
Resume | 159
Index | 161
Unauthenticated
38 PM

Unauthenticated
38 PM

1 Signals
Questions: How do we analyze the various signals that we often come across in the
process of productive practice? Can we ﬁnd some general methods?
How to proceed: Seek the similarities of different signals and classify them →select
and analyze the basic signals →ﬁnd out the analytical methods adaptive to most
signals.
Results: Deﬁnitions and properties of 11 basic signals such as sinusoidal, complex
exponential, step, impulse signals and so on. Study of the plotting method and 8
basic operations such as arithmetic, operations among even and odd signals, time
shifting, reversal, time scaling, differential/integral, decomposition/synthetic
and convolution integral.
1.1 The concept of signals
1. Deﬁnition of signals
We know that the aim of communication is to transfer information, which is based on
the transmission of signals. In other words, a communication task can be completed
only after a signal has been transferred. The signals used in communication are usu-
ally generated or controlled by people and are called artiﬁcial signals, such as the
beacon ﬁre in ancient China, the semaphores on ships, the voice of people, etc. In ad-
dition, there is another category of signals, which is usually seen in measurement and
control ﬁelds. Because these signals are created by nature, they are called natural sig-
nals, such as the vibration of the earth’s crust, temperature and humidity, the human
pulse, etc. All signals in real life are either artiﬁcial or natural. Signals are the physical
carriers of information and play an important role in communication, measurement
and control systems.
According to live examples, signals can be deﬁned as
all physical phenomena or quantities that can carry information and be perceived by
humans or instruments are called signals.
Signals and systems is a foundational course of communication principles, automatic
control principles, etc., so the information characteristic of signals is not the focus of
discussion here. Whereas the signals in this book are only considered as mathematical
functions which can be realized physically and are useful in productive practice, and
furthermore the functions can be taken as general, abstract forms which characterize
physical signals with both representativeness and universality. As a result, we deﬁne
it as
a kind of physical quantity varying with time, such as the voltage and current in circuits.
https://doi.org/10.1515/9783110419535-001
Authenticated
29 PM

2
|
1 Signals
To facilitate the research of physical signals in theory, we must abstract a mathemat-
ical model from them, and this process is called the modeling to signal; this math-
ematical model is just the function that is familiar to us. Therefore, in theory signal
and function are equivalent herein, so the terms signal and function can be used in-
terchangeably if we do not indicate otherwise.
2. Ways of describing signals
Usually, there are three methods to describe a signal in the familiar time domain:
1.
A mathematical expression or model. This is a function of time.
2.
A graph. This is a changing waveform related to the function (dependent variable)
and time (independent variable), which can be plotted from the expression or the
measurement.
3.
A table. This is frequently used to describe those signals that cannot be expressed
by a mathematical expression.
We also need to know and obtain the other three forms of signals, i.e. the expression,
the graph and the table in transform domains, respectively. In this case, the indepen-
dent variable of a signal is not time but real frequency, complex frequency or z. The
forms to express signals in transform domains are the supplements for the time do-
main and the auxiliary tools for the study of signals, and are usually only used in the
equation solving process or in speciﬁc circumstances. Of course, the different forms
of signals have also different models.
3. Basic properties of signals
Generally, a signal has the four basic properties of time, frequency, energy and infor-
mation.
The time feature includes changing relations of size (amplitude) and speed (fre-
quency) and delay (phrase) with time variables. The frequency feature consists of
changing relations of amplitude and phase with frequency variables. The energy fea-
ture involves changing the rules of the energy or the power of a signal with time and
frequency variables. The information feature entails that a signal can carry the infor-
mation that exists in a type of changing waveform of a signal in all cases.
4. Contents of signal research
Herein, the research on signals consists of two aspects: signal analysis and signal pro-
cessing.
Signal analysis refers to the concepts, theories and methods to decompose a sig-
nal into several components, such as the convolution integral and convolution sum,
Fourier series, Fourier transform, Laplace transform and z transform, etc.
Authenticated
29 PM

1.2 Classiﬁcation of signals
|
3
Signal analysis can be applied in time and transform domains. Moreover, in the
transform domain it can be regarded as the supplement and extension to concepts
and methods for time domain analysis. For example, the spectrum concept obtained
from frequency domain analysis is important in the application and the processing of
signals.
Signal processing refers to the process, operation, modiﬁcation and transforma-
tion to a signal based on special requirements or aims. Filter, ampliﬁcation, modu-
lation and demodulation, coding and decoding, encryption and decryption, equilib-
rium, smoothness and sharpening are all examples of signal processing.
The signal is the object being processed, whereas the main body executing the
process to the signal is just the system.
5. Means of signal analysis
Analysis methods mainly includearithmetic, timeshifting, timereversal, timescaling,
differentiation/integration, decomposition/synthetic, convolution integration (sum)
and plotting.
6. The purpose of signal analysis
Because a signal is employed by excitation and response of a system, the purpose
of signal analysis is to facilitate analysis of transferring or processing features of the
system to various signals, namely characteristics of the system. In a nutshell, signal
analysis is the foundation of system analysis.
1.2 Classiﬁcation of signals
Signals can be classiﬁed into the following categories according to their characteris-
tics.
1.2.1 Continuous time and discrete time signals
The continuous-time signal and the discrete-time signal play leading roles in this
book; they correspond to the analog signal and the digital signal.
1. Continuous-time signals
Usually, a signal takes time as the independent variable, so we can give a deﬁnition
as:
A signal whose independent variable time has all values in the domain of deﬁnition is
called a continuous-time signal or, simply, a continuous signal.
Authenticated
29 PM

4
|
1 Signals
)
(t
f
t
0
Fig. 1.1: A continuous-time signal (analog signal).
Two examples this type of signal are shown in 󳶳Figure 1.1 and 󳶳Figure 1.6. Note that
values of the dependent variable of a continuous signal can be continuous or discon-
tinuous.
The analog signal is always discussed together with the continuous signal, which
can be deﬁned as
A signal whose dependent variable changes continuously with continuous variations of
independent variable time is called an analog signal.
The graph of an analog signal is a continuous curve (󳶳Figure 1.1). Note that an ana-
log signal must be a continuous signal, but the reverse is not true; for example, the
continuous digital signal plotted in 󳶳Figure 1.5b is not an analog signal.
Usually, a continuous signal can be denoted as f(t), x(t), y(t), s(t) and so on.
2. Discrete-time signals
A signal whose independent variable time only takes discrete values in the domain of
deﬁnition is called a discrete-time signal or, simply, a discrete signal.
A typical signal where the independent variable is not time is the frequency spectrum
of a periodic signal, which takes frequency as the independent variable; this will be
introduced in Chapter 4. In this book, discrete signals usually refer to discrete-time
signals if no other explanation is given.
The f[tn] in 󳶳Figure 1.2 is a discrete signal that only has values at instants tn.
Assuming an interval between tn and tn+1 is Tn, which is a constant T or a function
of n, if Tn = T, the discrete signal f[tn] only has values at points at equal intervals
0
1.5
0.5
1
2
3
4
n
-1
-2
-1
1
2
1.5
[ ]
n
f t
L
L
Fig. 1.2: A discrete-time signal.
Authenticated
29 PM

1.2 Classiﬁcation of signals
|
5
0
-0.5
2.1
5.9
15.5
19.5
18.1
13.3
6.4
0.1
1
2
3
4
5
6
7
8
9
10 11 12
/ 0C
(year 2010)
(month)
n
24.3
27.1
26.3
[ ]
f n
Fig. 1.3: Average monthly temperatures in a city in 2010.
)
(t
f
t
0
0
t
t
0
Ts
S( )
y t
( )
p t
S
Fig. 1.4: The sampling process.
tn = ⋅⋅⋅−2T, −T, 0, T, 2T . . . , and it is expressed as f[nT] or as f[n] if T = 1. Usually,
this discrete signal whose value distributes uniformly on the time axis is also called a
sequence.
To distinguish between continuous and discrete signals, we use square brackets
to express a discrete signal, such as f[n], x[n], y[n] and s[n]. At the same time, we
stipulate that the independent variable of sequence n is only an integer, and sequence
values at noninteger points of the independent variable are either undeﬁned or zero.
Discrete signals can be divided into two types from their sources; one is a natural
discrete signal of which the independent variable itself is discrete, such as the average
temperatures of months in 2010 of a city f[n] (󳶳Figure 1.3). The other is an artiﬁcial
discrete signal, which can be obtained by sampling to a continuous signal; for exam-
ple, if a continuous signal f(t) applies to a switch S controlled by a sampling pulse
sequence p(t), the output yS(t) of S is a discrete signal as shown in 󳶳Figure 1.4.
Sampling is a kind of operation or process to acquire orderly instantaneous values of a
continuous signal with a certain time interval.
Looking at the waveform, a discrete signal obtained by sampling has lost much con-
tent (information) carried by the original continuous signal; so readers are likely to
ask: what is the purpose to make such a discrete signal?
In fact, it is an important part in a communication principles course to change a
continuous or an analog signal into a discrete signal by sampling; the aim of such A/D
Authenticated
29 PM

6
|
1 Signals
conversion is to achieve digital communication. The sampling theorem is intended to
ensure that all information carried by the original continuous signal can be recovered
from the discrete.
Sampling theorem: A bandlimited or lowpass continuous signal f(t) with fre-
quency band [fL, fH], if it is sampled with sampling frequency fS ≥2fH (the sampling
interval is TS ≤
1
fS ), will be entirely determined by a sampled signal, which is rep-
resented as yS(t) = {f(nTS)}. Alternatively, the original continuous signal f(t) can be
restored from the sampled signal yS(t) without any distortion.
The sampling interval and the sampling frequency are called the Nyquist interval
and the Nyquist frequency, respectively.
Note that lowpass signals are a kind of signal whose bandwidths must meet rela-
tion fH −fL > fL, whereas signals that met relation fH −fL < fL are called bandpass
signals.
The sampling theorem is a bridge between the continuous (analog) and discrete
(digital) signals and is the theoretical basis of time division multiplexing technology.
Therefore, it is widely used in the computer and the communication ﬁelds.
The main characteristics of discrete signals are as follows:
(1) Although the independent variable is discrete, the dependent variable (amplitude
of the discrete signal) can be continuous (has inﬁnite possible values) or discrete.
(2) The graph of a discrete signal is a series of vertical line segments located at dis-
crete independent variable points.
Other details regarding discrete signals can be found in Chapter 8. A digital signal is
often discussed accompanied with a discrete one and is deﬁned as:
A discrete or continuous signal whose dependent variable is discrete and takes limited
values is called a digital signal.
In general, a digital signal of which the independent variable is discrete is also called a
digital sequence; this is shown as f[n] in 󳶳Figure 1.5a. The dependent variable values
of the sequence are only 0 and 1, and the independent variable n is an integer, so f[n]
can be expressed as
f[n] =
{
{
{
0
(n < 0)
1
(n ≥0)
.
(1.2-1)
To change a discrete signal into a digital one, the discrete signal needs to be quantiﬁed
as usual.
A process or a method changing inﬁnite possible values of the dependent variable into
ﬁnite ones is called quantization.
As a result, a digital signal is also said to be a signal with quantiﬁed amplitude.
In computer and communication ﬁelds, the common digital signal is a kind of
continuous time signal as shown in 󳶳Figure 1.5b, and is called a baseband digital
Authenticated
29 PM

1.2 Classiﬁcation of signals
|
7
n
0
1
2
3
4
1
1
−
L
L
5
6
( )
f t
t
0
A discrete-time digital signal
A continuous-time digital signal
[ ]
f n
5
1
2
3
4
(a)
(b)
Fig. 1.5: Digital signals.
signal. From the communication standpoint, the digital signal is also considered as:
a signal using limited values or states of dependent variable to carry messages.
In conclusion, the terms continuous and discrete describe the change of the inde-
pendent variable of a signal. The terms analog and digital, however, depict the change
of the dependent variable of a signal.
1.2.2 Periodic and aperiodic signals
The periodic signal is a type of signal which waveform arises periodically with a cer-
tain time interval and without beginning and end, as shown in 󳶳Figure 1.6a, and sat-
isﬁes the equation
f(t) = f(t + nT)
n = 0, ±1, ±2, . . .
(arbitrary integer) .
(1.2-2)
The minimum value of T satisfying the equation is known as the period of a periodic
signal. If the expression or a complete period waveform of a periodic signal is given, all
values of the signal for all time can be determined. The trigonometric function f(t) =
A sin(ωt + θ) is the best known example of a periodic signal to our knowledge.
A signal that is not periodic is called aperiodic such as the exponential signal
f(t) = Aeat shown in 󳶳Figure 1.6b.
A periodic signal will become aperiodic if its period T tends to inﬁnity so that its wave-
form no longer arises repeatedly.
This concept is very important because it reveals the inner relationship between a pe-
riodic signal and an aperiodic one; this will be used to introduce the Fourier transform
in Chapter 5.
)
(t
f
t
2
T
T
−
0
T
)
(t
f
0
t
A
An aperiodic Signal
 A periodic signal
2
T
−
L
L
1
(a)
(b)
Fig. 1.6: A periodic signal and an aperiodic signal.
Authenticated
29 PM

8
|
1 Signals
1.2.3 Energy and power signals
To understand the energy feature of a current or a voltage signal, we need to study the
energy or power consumption of a signal through a unit resistor. If the instantaneous
power of signal f(t) consumed by a unit resistor is |f(t)|2, then following equations are
true:
(1) The consumed energy of f(t) over interval [−T, T] is deﬁned as
T
∫
−T
|f(t)|2dt .
(1.2-3)
The average consumed power of f(t) is deﬁned as
1
2T
T
∫
−T
|f(t)|2dt .
(1.2-4)
(2) The consumed energy E of f(t) over range (−∞, ∞) is deﬁned as
E
def= lim
T→∞
T
∫
−T
|f(t)|2 dt
(in J) .
(1.2-5)
(3) The consumed power P of f(t) over range (−∞, ∞) is deﬁned as the average power
P
def= lim
T→∞
1
2T
T
∫
−T
|f(t)|2dt
(in W) .
(1.2-6)
Some conclusions based on the above deﬁnitions are as follows:
(1) If 0 < E < ∞(at this time, P = 0), the signal is called a limited energy signal or,
simply, an energy signal. Its features are limited amplitude, limited duration time
and aperiodicity, for example, a single rectangular pulse.
(2) If 0 < P < ∞(at this time, E = ∞), the signal is called a limited power signal or,
simply, a power signal. Its features are limited amplitude and limitless duration
time, for example, a direct current signal, a periodic signal and a random signal.
A periodic signal must be a power signal, but a power signal is not necessarily a
periodic signal.
(3) A signal cannot be both an energy signal and a power signal, but it can be
neither an energy signal nor a power signal. For f(t) = e−2|t|, its energy E =
limT→∞∫
T
−T
󵄨󵄨󵄨󵄨󵄨e−2|t|󵄨󵄨󵄨󵄨󵄨
2 dt = 2 ∫
∞
0 e−4tdt = 1
2 and power P = 0, therefore it is an en-
ergy signal. For the example f(t) = e−2t, its energy
E = lim
T→∞
T
∫
−T
(e−2t)2dt = lim
T→∞[1
4 (e4T −e−4T)] = ∞,
Authenticated
29 PM

1.2 Classiﬁcation of signals
|
9
and power
P = lim
T→∞
E
2T = lim
T→∞
e4T −e−4T
8T
= lim
T→∞
e4T
8T = lim
T→∞
4e4T
8
= ∞.
Obviously, f(t) is neither a power nor an energy signal. The classiﬁcation for sig-
nals based on the energy and the power of signal cannot include all signals theo-
retically.
It is very meaningful for research on energy or power spectrums of a signal in commu-
nication principles work to classify signals into energy signals and power signals.
Note the following:
(1) Above T is not the period of signal but a certain moment; f(t) can represent a
voltage or a current signal.
(2) If f(t) is a complex signal, the symbol |f(t)|2 represents that f(t) is ﬁrst taken by
magnitude computation and then squared.
1.2.4 Deterministic and random signals
Common signals in engineering are also divided into two types of deterministic and
random signals from their change rules. Signals whose future values can be described
accurately by a certain mathematical function are deterministic signals, for example,
whole values of a sine signal can be determined by a sine function. If the value of a
signal is random at any moment, namely, future values of the signal cannot be deter-
mined by an accurate function of time, the signal is called an uncertain or a random
signal, such as speech signals from the human body. Because future values of this type
of signal change randomly with time, it is necessary to describe them by a probability
distribution or statistical average values, these signals are also called statistical time
signals. Moreover, not only the amplitude of a random signal can change randomly,
but also frequency and phase can also change randomly.
Strictly speaking, signals in the objective world are mainly random, for example,
audio signals, image signals, biological electric signals, earthquake signals, etc. Only
the basic signals for analyses and tests, for example, sines, square waves, triangle
waves, exponential signals, etc., are exactly deterministic. The diagrams of determin-
istic and random signals are shown in 󳶳Figure 1.7.
From the perspective of communication, it is meaningful to transmit random sig-
nals, because random signals always carry contents or information that receivers do
not know but want to know. However, mainly deterministic signals will be considered
herein. The reasoning here is that although deterministic signals cannot be used in
communication tasks, they can be used to analyze the characteristics of a system as
basic signals, and whose analysis methods and results can be directly generalized or
applied to the analysis of random signals. This is the motivation for the analysis of
Authenticated
29 PM

10
|
1 Signals
t
( )
f t
0
1
2
3
4
t
( )
f t
0
1
2
3
4
5
5
A random signal
A deterministic signal
(a)
(b)
Fig. 1.7: A deterministic signal and a random signal.
deterministic signals. On the other hand, the analysis of random signals often needs
the additional help of stochastic processes to be completed, falling under the topic of
communication principles and so will not be discussed herein.
1.2.5 Causal and anticausal signals
According to change features of a signal before and after an observed time point, sig-
nals can be divided into causal and anticausal types.
If time t = 0 or n = 0 is set as the time we start observing a signal, a causal signal
satisﬁes
f(t) = 0 , t < 0
or
f[n] = 0 , n < 0 ,
(1.2-7)
and an anticausal signal satisﬁes
f(t) = 0 , t > 0
or
f[n] = 0 , n > 0 .
(1.2-8)
1.3 Basic continuous-time signals
It is impossible to study various signals one by one in real life, so some basic signals
have been selected to be researched for speciﬁc reasons as follows:
(1) Basic signals can be used to represent other signals accurately or approximately
by mathematical means. For example, the basic form of Fourier series only in-
cludes sine and cosine functions, but it can represent most different periodic sig-
nals, which will be seen in Chapter 4.
(2) Responses of a system to basic signals play leading roles in system analysis and
are of general signiﬁcance. As examples, impulse or step responses are produced
by systems in response to impulse or step signals.
Research on the characteristics and performance of basic signals will be the basis for
the analysis of other signals and systems.
Authenticated
29 PM

1.3 Basic continuous-time signals
|
11
)
(t
f
0
C
t
Fig. 1.8: A direct current signal.
1.3.1 Direct current signals
In general, a current or voltage signal with only one changing direction is called a DC
signal (direct current signal). However, a DC signal which is often mentioned refers to
one whose magnitude and direction are both changeless with time, namely, constant
voltage or a constant current signal, which is expressed as
f(t) = C
(a constant) ,
(1.3-1)
and is plotted in 󳶳Figure 1.8. Obviously, the constant term which is familiar to us in a
mathematical expression has a physical meaning in the electric engineering ﬁeld, it
represents a DC signal.
1.3.2 Sinusoidal signals
Sinusoidal signals generally refer to sine and cosine signals that are familiar to us. A
cosine signal is actually a sine signal with a shifted phase π
2, namely, the mathemat-
ical relation between of them is cos(ωt + φ) = sin(ωt + φ + π
2 ). Therefore, sine and
cosine signals are collectively called the sinusoidal signal to facilitate research, which
is written as
f(t) = A sin(ωt + φ)
(1.3-2a)
or
f(t) = A cos(ωt + φ) .
(1.3-2b)
Where A is the amplitude, ω the angular frequency and φ the initial phase. The wave-
form of equation (1.3-2a) is shown in 󳶳Figure 1.9.
0
ϕ
ω
A
)
(t
f
2
T
π
ω
=
t
Fig. 1.9: A sinusoidal signal.
Authenticated
29 PM

12
|
1 Signals





t
j2
Im(
)
t
e
π
j2
Re(
)
t
e
π





t
-j2
Im(
)
t
e
π
-j2
Re(
)
t
e
π





t
Im
Re





t
Im
Re
Fig. 1.10: Graphical interpretation of Euler’s relation.
In the real world, the natural response (resonant wave) from a LC circuit, simple
harmonic vibration in a mechanical system and the sound pressure wave of a single
tone from an instrument are all examples of sinusoidal signals.
In circuits analysis, signals and systems, and communication principles courses,
sinusoidal signals are often expressed by imaginary exponential signals with Euler’s
relation, e.g.
ejωt = cos ωt + j sin ωt ,
(1.3-3)
e−jωt = cos ωt −j sin ωt ,
(1.3-4)
or
sin ωt = ejωt −e−jωt
2j
,
(1.3-5)
cos ωt = ejωt + e−jωt
2
.
(1.3-6)
The signiﬁcance of Euler’s relation is that it is not only a bridge between trigonometric
functions and exponential functions but also a link between real functions and imagi-
nary functions. It has provided an effective way for sinusoidal AC circuit or sinusoidal
signal analysis. The graphical explanation for Euler’s relation when ω = 2π is shown
in 󳶳Figure 1.10.
1.3.3 Exponential signals
The exponential signal is also known as the real exponential signal and is of the form
f(t) = Keat ,
(1.3-7)
where K is a constant and a is a real number. It is sketched in 󳶳Figure 1.11a.
Authenticated
29 PM

1.3 Basic continuous-time signals
|
13
t
( )
f t
0
at
Ke
(
0)
a >
(
0)
a <
K (
0)
a =
K
t
( )
f t
0
K
−
K
An exponential signal
An exponential decay sine signal
(a)
(b)
Fig. 1.11: An exponential signal and an exponential decay sine signal.
If a > 0, the signal size increases with time t; but if a < 0, the size decreases
with time t; and if a = 0, the signal becomes a DC signal. Sometimes we also discuss
those sinusoidal signals multiplied by a growing exponential or a decaying one. The
sinusoidal signal multiplied by a decaying exponential is usually called a damped
sinusoid and is expressed as
f(t) =
{
{
{
0
(t < 0)
Ke−at sin ωt
(t ≥0)
.
(1.3-8)
It is plotted in 󳶳Figure 1.11b.
1.3.4 Complex exponential signals
A complex exponential signal is expressed as
f(t) = Kest ,
(1.3-9)
where s = σ + jω is called a complex frequency, K is a real constant in general and
both σ and ω are real numbers.
Based on Euler’s relation, a complex exponential signal can also be expressed as
f(t) = Kest = Ke(σ+jω)t = Keσt cos ωt + jKeσt sin ωt .
This expression shows that a complex exponential signal can be decomposed into a
real signal and an imaginary signal. Moreover, the real component contains a cosine
signal; the imaginary part includes a sine signal. The ω in complex frequency s is
the angular frequency of the sine and cosine signals, and σ represents the amplitude
change of the sine and cosine functions with time. Therefore, ﬁve points are now given
from the above:
(1) When σ > 0, the sine and cosine components are grown sinusoids.
(2) When σ < 0, the sine and cosine components are damped sinusoids.
(3) When σ = 0 and K = 1, the complex exponential signal has only the imaginary
part and becomes an imaginary exponential signal f(t) = ejωt.
Authenticated
29 PM

14
|
1 Signals
(4) When ω = 0, the complex exponential signal becomes a real exponential signal.
(5) When s = 0, the complex exponential signal becomes a DC signal.
Although the complex exponential signal cannot be generated in real practice, it can
include changing situations of various signals from the above discussions. Therefore,
it is frequently used to describe some basic signals, such as DC signals, exponential
signals, sine and cosine signals, grown and damped sinusoids, so it isa very important
basic signal. Another important feature is that the response of a linear time invariant
system to it is still a complex exponential signal, which only has a different amplitude
[as shown in equation (6.8-1)].
1.3.5 Signum signal
The signum signal is deﬁned as
sgn(t)
def=
{
{
{
−1,
(t < 0)
1,
(t > 0)
(1.3-10)
and is plotted in 󳶳Figure 1.12. Its meaning is obvious from the waveform, that is, when
t > 0, the signal values are positive; otherwise, they are negative. Note that the signum
signal can be also written as the sign signal.
1.3.6 Unit step signal
The unit step signal is deﬁned by
ε(t)
def=
{
{
{
0,
(t < 0)
1,
(t > 0)
,
(1.3-11)
and is illustrated in 󳶳Figure 1.13. It is to be noted that it has no deﬁnition at time t = 0.
sgn( )t
0
1
−
t
1
Fig. 1.12: A sign signal.
( )t
ε
0
t
1
Fig. 1.13: A unit step signal.
Authenticated
29 PM

1.3 Basic continuous-time signals
|
15
6
R
R ( )
( )
u
t
t
ε
=
+
−
1
E
 V
=
Fig. 1.14: An example of the unit step signal.
The unit step signal is abstracted from practical applications. For example, in
󳶳Figure 1.14 a switch S is closed at t = 0, and voltage on a resistor R is uR(t) = Eε(t) =
ε(t) in ideal condition. This shows that the unit step signal canbe used to control other
signals as a switch.
Note that the unit step signal is represented by U(t) or u(t) in some books, but U(t)
and u(t) are more often used to express a voltage signal in electronics, and therefore,
to distinguish it from the voltage symbol, ε(t) is employed to represent the unit step
signal in this book.
The unit step signal can be also combined with a sign signal and a DC signal
ε(t) = 1
2 + 1
2 sgn(t) .
(1.3-12)
Similarly, the sign signal is also described with a step signal and a DC signal
sgn(t) = 2ε(t) −1 .
(1.3-13)
The unit step signal has four main purposes:
(1) To indicate the starting time of an input or an output signal of a system or to con-
trol another signal as a switch. For example, a signal f(t) acting in a system at time
t = 0 can be denoted by f(t)ε(t), and the zero-state response of a system can be
expressed as yf(t) = EuC(t)ε(t).
(2) To describe the causality of a system or a signal, namely, the righted side property.
(3) To describe other signals by a linear combination of it and its shifted signals,
which can simplify the expressions and analysis courses of these signals as well
as the analysis course of system.
(4) As the integral signal of the unit impulse signal, it is used to generate the step
response, which is closely related to the impulse response.
Example 1.3-1. Please write math expression of the signal in 󳶳Figure 1.15.
1
2
0
1
2
3
)
(t
f
t
Fig. 1.15: E1.3-1.
Authenticated
29 PM

16
|
1 Signals
Solution. This signal must be divided into ﬁve segments to show in the general
method, but now from the unit step signal it can be concisely expressed as
f(t) = ε(t) + ε(t −1) −ε(t −2) −ε(t −3) .
1.3.7 Unit ramp signal
The unit ramp signal is deﬁned as
r(t)
def=
{
{
{
t
t > 0
0
t ≤0
(1.3-14)
and is sketched in 󳶳Figure 1.16. It is also written as r(t) = tε(t).
A unit ramp signal and a unit step signal can be related by
r(t) =
t
∫
−∞
ε(τ)dτ
(1.3-15)
or
dr(t)
dt
= ε(t)
(1.3-16)
Obviously, the derivative of a unit ramp signal is a unit step signal, and this is the main
purpose for introducing the ramp signal.
1.3.8 Unit impulse signal
The unit impulse signal (or Delta signal) can be deﬁned in various ways.
1. Evolution from a rectangular pulse signal
As shown in 󳶳Figure 1.17a, a rectangular pulse pτ(t) with width τ height 1
τ is given.
Keeping its area τ ⋅1
τ = 1 the same, when the width τ approaches zero, the height
1
τ will tend to inﬁnity; in this case, pτ(t) becomes a unit impulse signal and can be
expressed as
δ(t)
def= lim
τ→0 pτ(t) .
(1.3-17)
( )
r t
0
1
t
1
Fig. 1.16: A unit ramp signal.
Authenticated
29 PM

1.3 Basic continuous-time signals
|
17
0
t
0
t
τ
1τ
)
(t
δ
)1(
( )
p t
τ
( )
f
t
Δ
τ
−
τ
(a)
(b)
Fig. 1.17: A unit impulse signal.
It can be represented by a vertical arrow (as shown in 󳶳Figure 1.17), and the pulse
weight 1 (the area under the pulse) is labeled in parentheses next to the arrow. If the
area of pτ(t) is a constant A, then the impulse signal with weight A can be expressed
as Aδ(t), and in the graph A is placed in parenthesis next to the arrow.
2. Dirac’s deﬁnition
Another deﬁnition was given by the British theoretical physicist Dirac (1902–1984),
which is now also a common deﬁnition, as
{
{
{
δ(t) = 0,
t
̸= 0
∫
∞
−∞δ(t)dt = 1,
−∞< t < +∞
.
(1.3-18)
It can be shown that values of a unit impulse signal are zero at all time points except
the origin, but the area surrounded by the signal waveform is 1.
Of course, besides above two deﬁnitions, the unit impulse signal can be also de-
ﬁned by other methods, such as the evolution process of a triangle pulse in Exam-
ple 1.3-4 and the deﬁnition from the screening characteristics.
The unit impulse signal δ(t) can be thought of as an idealized model of physi-
cal phenomena which can generate a lot of energy in a very short time. For example,
lightning and earthquakes in the natural world, high voltage sparks in industrial pro-
duction and the sound of a nail being hit by a hammer in real life, etc.
3. The main properties of unit impulse signal
(1) Sifting
Suppose signal f(t) is continuous at t = 0 and bounded everywhere, then we have
f(t)δ(t) = f(0)δ(t) .
(1.3-19)
Equation (1.3-19) indicates that the product of f(t) and δ(t) is a unit impulse signal with
weight f(0). Moreover, it can be also generalized as f(t)δ(t −t0) = f(t0)δ(t −t0).
Integrating on both sides of equation (1.3-19), we have
∞
∫
−∞
f(t)δ(t)dt =
∞
∫
−∞
f(0)δ(t)dt = f(0)
∞
∫
−∞
δ(t)dt = f(0) .
(1.3-20)
Authenticated
29 PM

18
|
1 Signals
Equation (1.3-20) can be extended as
∞
∫
−∞
f(t)δ(t −t0)dt =
∞
∫
−∞
f(t0)δ(t −t0)dt = f(t0) .
(1.3-21)
Thesiftingproperty shows thatafter wetaketheintegralto theproductof a continuous
bounded function f(t) and a unit impulse function δ(t) over range (−∞, ∞), the value
of f(t) at t = 0 can be obtained, that is, the value f(0) is sifted. If δ(t) is delayed to t0,
f(t0) will be sifted. The term “sifting” can be also replaced by “sampling”.
Note that equation (1.3-20) can also be treated as a deﬁnition of the unit impulse
function, which means that if x(t) holds the expression ∫
∞
−∞f(t)x(t)dt = f(0), x(t) is a
unit impulse function, or x(t) = δ(t). This deﬁnition is stricter than equation (1.3-17)
in the mathematical sense.
(2) Even property
δ(t) is an even function because it meets
δ(−t) = δ(t) .
(1.3-22)
Proof. Considering ∫
∞
−∞f(t)δ(−t)dt and letting t = −τ, we have
∞
∫
−∞
f(t)δ(−t)dt = −
−∞
∫
∞
f(−τ)δ(τ)dτ =
∞
∫
−∞
f(−τ)δ(τ)dτ = f(0) .
Combining expression ∫
∞
−∞f(t)δ(t)dt =f(0), we obtain δ(−t) = δ(t).
Example 1.3-2. Find the value of ∫
3
1 cos[ω(t −3)]δ(2 −t)dt.
Solution. From the even property of δ(t) we have
3
∫
1
cos[ω(t −3)]δ(2 −t)dt =
3
∫
1
cos[ω(t −3)]δ(t −2)dt .
From the sifting property, we have
3
∫
1
cos[ω(t −3)]δ(t −2)dt =
3
∫
1
cos[ω(2 −3)]δ(t −2)dt
=
3
∫
1
cos(−ω)δ(t −2)dt
= cos ω
3
∫
1
δ(t −2)dt
Authenticated
29 PM

1.3 Basic continuous-time signals
|
19
Because values of δ(t −2) equal zero everywhere, except time t = 2, and the integral
interval is [1, 3], we have
3
∫
1
cos[ω(t −3)]δ(2 −t)dt = cos ω .
(3) Time scaling
If a is a real number, we have
δ(at) = 1
|a| δ(t) .
(1.3-23)
(4) Integral property
From Dirac’s deﬁnition, we know that
t
∫
−∞
δ(τ)dτ =
{
{
{
0
(t < 0)
1
(t > 0)
.
Referring to the deﬁnition [equation (1.3-10)] of the unit step signal, it can be obtained
by
t
∫
−∞
δ(τ)dτ =ε(t) .
(1.3-24)
This shows that the unit step signal is a running integral of the unit impulse signal.
Accordingly, the derivative of a unit step signal is a unit impulse signal
δ(t) = dε(t)
dt
.
(1.3-25)
The expression explains that a unit impulse signal can describe the changing rate of
which a unit step signal occurs a step at time t = 0.
There are four purposes of the impulse signal δ(t):
(1) To make the unit step signal derivable for all time.
(2) As a typical signal, it is used to produce the impulse response in the system anal-
ysis.
(3) To express a common signal with a linear combination of the signal and its time-
shifted signals. This can be seen from the expression f(t) = ∫
+∞
−∞f(τ)δ(t −τ)dτ in
Section 1.4.8.
(4) For the modeling of a sampling system, i.e. it can be considered as an ideal sam-
pling pulse train, like p(t) in Section 1.2.1.
Because the impulse and the step signals are different from some familiar common
signals, they are also known as singularity signals or generalized signals.
Example 1.3-3. The waveform of signal f(t) is shown in 󳶳Figure 1.18a, please write
expressions of f(t) and y(t) = d
dt f(t), and draw waveform of y(t).
Authenticated
29 PM

20
|
1 Signals
)
(t
f
t
2
1
0
1
1
−
1
−
)
(t
y
t
2
(2)
( 1)
−
( 2)
−
1
1
−
1
−
2
−
2
1
0
(a)
(b)
Fig. 1.18: E1.3-2.
Solution. From the unit step signal, f(t) can be expressed as
f(t) = (t + 1)[ε(t + 1) −ε(t)] −ε(t) + 2ε(t −1) −ε(t −2) .
Taking the derivative to f(t), we have
y(t) = d
dt f(t) = ε(t + 1) −ε(t) + (t + 1) [δ(t + 1) −δ(t)] −δ(t) + 2δ(t −1) −δ(t −2)
= ε(t + 1) −ε(t) −2δ(t) + 2δ(t −1) −δ(t −2) .
󳶳Figure 1.18b shows the plot of y(t).
1.3.9 Unit doublet signal
The unit doublet signal is deﬁned as the ﬁrst order derivative of the unit impulse sig-
nal; it is written as
δ󸀠(t) = dδ(t)
dt
or
δ(t) =
t
∫
−∞
δ󸀠(τ)dτ .
(1.3-26)
Its waveform is composed of two impulse signals whose weights are −∞and +∞, re-
spectively, as shown in 󳶳Figure 1.19. Needless to say, it is also a singularity signal.
Example 1.3-4. Proof weights of a unitdoubletsignalare±∞, asshownin󳶳Figure1.18.
Proof. Because an impulse signal can be expressed as
δ(t) = lim
τ→0
1
τ [ε (t + τ
2) −ε (t −τ
2)] ,
from δ(t) = dε(t)
dt
the unit doublet signal can be deduced by
δ󸀠(t) = {lim
τ→0
1
τ [ε (t + τ
2) −ε (t −τ
2)]}
󸀠
= lim
τ→0
1
τ [δ (t + τ
2) −δ (t −τ
2)] .
( )t
δ′
t
0
(
)
−∞
(
)
+∞
Fig. 1.19: A unit doublet signal.
Authenticated
29 PM

1.3 Basic continuous-time signals
|
21
Obviously, ± 1
τ are weights of the two impulse signals; when τ →0, they tend to ±∞.
Another way, δ(t) can be considered as a limit of a triangle pulse f∆(t) with width
2τ and height 1
τ. When τ →0, we have δ(t) = limτ→0 f∆(t), which is plotted with
dotted lines in 󳶳Figure 1.16a). So, δ󸀠(t) = [limτ→0 f∆(t)]󸀠= limτ→0 f 󸀠
∆(t) is a limit of two
rectangular pulses with width τ and different heights 1
τ2 and −1
τ2 , which are located at
left and right sides of the point of origin when τ →0. Their weights (areas) are limits
of ± 1
τ when τ →0, that is, ±∞.
Note: In the above proofs, because two impulse signals 1
τ δ(t+ τ
2) and −1
τ δ(t−τ
2) or
two rectangular pulses 1
τ2 and −1
τ2 are located separately on both sides of the origin,
when τ →0 they will be close to each other inﬁnitely but cannot meet at the same
time point after all, and, therefore, the effects of the two pulses cannot cancel each
other out.
The unit doublet signal has the following main features:
(1) Odd property
δ󸀠(−t) = −δ󸀠(t) .
(1.3-27)
(2) Integral property
∞
∫
−∞
δ󸀠(t)dt = 0 .
(1.3-28)
This feature can be seen from 󳶳Figure 1.19; that is, since the areas under the pos-
itive and the negative impulse signals can be offset by each other, the integral
should by zero. In addition, we have
∞
∫
−∞
δ󸀠(t)f(t)dt = −f 󸀠(0) ,
(1.3-29)
where f(t) is continuous at t = 0, and f 󸀠(0) is ﬁrst-order derivative value of f(t) at
t = 0.
Proof.
∞
∫
−∞
δ󸀠(t)f(t)dt =
∞
∫
−∞
f(t)dδ(t) = f(t)δ(t)|∞
−∞−
∞
∫
−∞
δ(t)f 󸀠(t)dt
= −f 󸀠(0)
∞
∫
−∞
δ(t)dt = −f 󸀠(0) .
Similarly,
∞
∫
−∞
f(t)δ(n)(t)dt = (−1)nf (n)(0) ,
(1.3-30)
∞
∫
−∞
f(t)δ(n)(t −t0)dt = (−1)nf (n)(t0) .
(1.3-31)
Authenticated
29 PM

22
|
1 Signals
Note: Equation (1.3-30) can also be considered as a deﬁnition of the unit doublet
signal just like one of the unit impulse signal.
(3) Multiplication property
f(t)δ󸀠(t −t0) = f(t0)δ󸀠(t −t0) −f 󸀠(t0)δ(t −t0) .
(1.3-32)
(4) Timing scaling
δ󸀠(at) = 1
|a|
1
a δ󸀠(t) .
(1.3-33)
It can be generalized as
δ(n)(at) = 1
|a|
1
an δ(n)(t) .
(1.3-34)
(5) Convolution property
f(t) ∗δ󸀠(t) = d
dt f(t) .
(1.3-35)
Example 1.3-5. Obtain the value of integral ∫
∞
−∞A sin tδ󸀠(t)dt.
Solution. According to equation (1.3-29), we obtain
∞
∫
−∞
A sin tδ󸀠(t)dt = −dA sin t
dt
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨t=0
= −A cos t|t=0 = −A .
1.3.10 Unit gate signal
The unit gate signal gτ(t) is a rectangular pulse signal located at the origin with width
τ and height 1, and is deﬁned as
gτ(t)
def=
{
{
{
1
|t| < τ
2
0
|t| > τ
2
.
(1.3-36)
( )
g t
τ
t
0
1
2
τ
−
2
τ
Fig. 1.20: A unit gate signal.
Authenticated
29 PM

1.3 Basic continuous-time signals
|
23
It is illustrated in 󳶳Figure 1.20. Note that it is undeﬁned at t = −τ
2 and τ
2. From
󳶳Figure 1.20, it can be also described by two shifted step signals
gτ(t) = ε(t + τ
2) −ε(t −τ
2) .
(1.3-37)
The gate signal is important in communication principles and has the following main
functions:
(1) Representation of the frequency characteristic of an ideal lowpass ﬁlter.
(2) From the time shifting property, a series of shifted gate signals can be employed
to represent a binary digital signal.
(3) It and the sampling signal, which is also an important signal, form the Fourier
transform pair.
1.3.11 Bell shaped pulse signal
The bell shaped pulse signal is deﬁned as
f(t)
def= Ee−( t
τ )2 .
(1.3-38)
It is illustrated in 󳶳Figure 1.21 and is a monotone attenuation even function. The pa-
rameter τ represents the time width occupied by f(t) when it declines from the max-
imum value E to 0.78E. The bell shaped pulse signal is very useful in random signal
analysis.
After careful observation, it is found that most of the above signals relate to com-
plex exponential signals or impulse signals, so complex exponential and impulse sig-
nals are considered as two core signals herein.
To sum up, although there are various signals, those that are analyzed here are
analyzed are continuous or discrete, periodic or aperiodic deterministic signals.
( )
f t
t
0
E
0.78E
2
τ
−
2
τ
Fig. 1.21: The bell shaped pulse signal.
Authenticated
29 PM

24
|
1 Signals
1.4 Operations of continuous signals
1.4.1 Arithmetic operations
A new continuous signal can be obtained by the addition (subtraction, multiplication
or division) of two continuous signals, its value at any time being consistent with the
sum (difference, product or quotient) of these two continuous signals at that time.
Example 1.4-1. The waveforms of f1(t) and f2(t) are shown in 󳶳Figure 1.22a nd b, plot
f1(t) + f2(t) and f1(t) −f2(t) and write their expressions.
Solution. According to the waveforms, the expressions of f1(t) and f2(t) are written
as
f1(t) =
{
{
{
{
{
{
{
0
(t < 0)
t
(0 ≤t ≤1)
1
(t > 1)
and
f2(t) =
{
{
{
{
{
{
{
0
(t < 0)
1
(0 ≤t ≤1)
0
(t > 1)
.
Moreover, the expressions of f1(t) + f2(t) and f1(t) −f2(t) are separately
f1(t) + f2(t) =
{
{
{
{
{
{
{
0
(t < 0)
t + 1
(0 ≤t ≤1)
1
(t > 1)
and
f1(t) −f2(t) =
{
{
{
{
{
{
{
0
(t < 0)
t −1
(0 ≤t ≤1)
1
(t > 1)
.
So, they are plotted in 󳶳Figure 1.22c and d.
It is especially important to bear in mind that when two signals with different varying
frequency aremultiplied, thelower frequency signalwillconstitutetheenvelopeof the
product signal, and the envelope can reﬂect the change trend of the product signal.
It is also the mathematical foundation of the amplitude modulation technology in a
“communication principles” course.
)
(
1 t
f
0
1
)
(
)
(
2
1
t
f
t
f
+
)
(
)
(
2
1
t
f
t
f
−
2
1
−
t
2( )
f t
t
t
0
0
0
1
t
1
1
1
1
1
1
(a)
(b)
(c)
(d)
Fig. 1.22: E1.4-1.
Authenticated
29 PM

1.4 Operations of continuous signals
|
25
1.4.2 Operations of even and odd signals
Even and odd signals
If a signal f(t) holds the equation f(t) = −f(−t), f(t) is an odd signal.
If a signal f(t) holds the equation f(t) = f(−t), f(t) is an even signal.
The deﬁnitions are also suitable in discrete time.
Operations of even signals
(1) The sum or difference of two even signals is even. It is also suitable for discrete
signals.
(2) The product of two even signals is even. It is also suitable for discrete signals.
(3) The derivative of an even signal is odd.
Proof. If f(t) is even, then the derivative of f(−t) isf 󸀠(−t) = −f 󸀠(t), and obviously, f 󸀠(t) =
−f 󸀠(−t), and so the derivative of an even signal is odd.
Operations of odd signals
(1) The sum or difference of two odd signals is odd. It is also suitable for discrete
signals.
(2) The product of two odd signals is even. It is also suitable for discrete signals.
(3) The derivative of an odd signal is even.
Operations of an odd signal and an even signal
(1) The product of an odd signal and an even signal is odd.
(2) The sum of an odd signal and an even signal is neither odd nor even.
The two points are also suitable for discrete signals.
1.4.3 Time shifting
The waveform of shifted signal f(t + t0) or f(t −t0) leads or lags behind one of f(t) with
a time t0, that is, the waveform of f(t + t0) or f(t −t0) results by moving one of f(t) to
the left or to the right with t0. Both are shown in 󳶳Figure 1.23.
Authenticated
29 PM

26
|
1 Signals
0
t
)
(t
f
1
0
t
t +
0t
0
(
0)
t >
0
t
0
1
t
t
−
−
0t
−
0
(
0)
t <
0
t
)
(
0t
t
f
−
0
(
3 )
f t
t
+
1t
Original signal
Shifting right
Shifting left
(a)
(b)
(c)
Fig. 1.23: Time Shifting Operation.
0
t
)
(t
f
1t
−
0
t
(
)
f
t
−
1t
Original signal
Reversal signal
(a)
(b)
Fig. 1.24: The reversal operation.
1.4.4 Time reversal
The waveform of f(−t) is the mirror image of f(t) about the vertical axis, as is shown in
󳶳Figure 1.24.
The waveform of f [−(t + t0)] can be obtained by moving f(−t) (󳶳Figure 1.25a) to
the left with t0 as shown in 󳶳Figure 1.25b, and the waveform of f [−(t −t0)] results by
moving f(−t) to the right with t0, as shown in 󳶳Figure 1.25c.
1.4.5 Time scaling
The time scaling operation needs to replace the independent variable t with at in f(t),
and f(at) can be obtained, where a is a constant and is called the scale coefficient.
(1) If a > 1, the waveform of f(at) can be obtained by that the waveform of f(t)
(󳶳Figure 1.26a) is compressed to 1
a times the original scale in time (󳶳Figure 1.26b).
(2) If 0 < a < 1, the waveform of f(at) can be obtained by the waveform of f(t) being
linearly expanded to 1
a times of the original scale in time (󳶳Figure 1.26c).
(3) If a < 0, the waveform of f(at) can be obtained by the waveform of f(t) being
inversed ﬁrst and then compressed or expanded to
1
|a| times the original scale in
time.
Authenticated
29 PM

1.4 Operations of continuous signals
|
27
0
t
(
)
f
t
−
0
1
t
t
−
−
0t
−
0
(
0)
t >
0
t
0
1
t
t
+
0t
0
(
0)
t >
0
t
0
(
)
f
t
t
−−
0
(
3 )
f
t
t
−+
1t
−
Reversal signal
Reversal and  
shifting left
Reversal and 
shifting right
(a)
(b)
(c)
Fig. 1.25: The reversal operation with time shifting.
0
t
( )
f t
0
t
0
t
(2 )
f
t
1
(
)
2
f
t
1
2
12 1
2
4
Original signal
Signal compressed 
Signal extended
(a)
(b)
(c)
Fig. 1.26: Time scaling.
Example 1.4-2. Please give the waveform of y(t) = f(−2t + 3) the gate signal f(t) is
given in 󳶳Figure 1.27a.
Solution. The waveform of y(t) can be obtained after f(t) by scaling and shifting and
reversal. The order of the ﬁrst two operations can be changed, that is, there are two
methods to solve the problem. Here, we use the way of shifting ﬁrst and scaling to
solve it.
First, f(t + 3) is obtained by moving f(t) to the left by 3 units; it is plotted in
󳶳Figure 1.27b. Then, f(2t + 3) is obtained by compressing the waveform of f(t + 3)
to half of the original one; it is shown as 󳶳Figure 1.27c. Last, overturning the wave-
0
1
t
t
0
1
t
1
1
)
(t
f
)
(
3
+
t
f
)
(
3
2 +
t
f
1
−
2
−
3
−
4
−
0
2
−
3
−
1
−
1
−
(a)
(c)
(d)
t
1
( 2
3)
f
t
−
+
2
0
1
−
2
−
1
3
(b)
Fig. 1.27: E1.4-2.
Authenticated
29 PM

28
|
1 Signals
form of f(2t + 3) to obtain y(t) (the mirror image with the vertical axis), it is sketched
in 󳶳Figure 1.27d.
Note: If there are time scaling and time shifting and reversal operations in an exercise
at the same time, a suggested approach would be ﬁrst time shifting, then scaling and,
ﬁnally, reversal.
1.4.6 Differentiation and integration
If f(t) is known, its differentiation and integration operations are shown, respectively,
by
f 󸀠(t) = df(t)
dt
,
(1.4-1)
f (−1)(t) =
t
∫
−∞
f(t)dt .
(1.4-2)
1.4.7 Decomposition and synthesis
Generally speaking, the process or the method where one signal is broken into ﬁnite
or inﬁnite other signals is called decomposition. The process or the method where one
signal is formed by several or inﬁnite other signals is called synthesis.
Just as in Example 1.3-1, f(t) in 󳶳Figure 1.15 can be expressed as the algebraic sum
of several step signals, and this expression is the decomposition to f(t). For instance,
again, any signal f(t) can be broken into an algebraic sum of even and odd signals.
Proof. If fe(t) and fo(t) are, respectively, even and odd signals. we have
fe(t) = 1
2f(t) + 1
2 f(−t) ,
(1.4-3)
fo(t) = 1
2f(t) −1
2f(−t) .
(1.4-4)
It can be seen that
f(t) = fe(t) + fo(t) .
Equations (1.4-3) and (1.4-4) are usually used to ﬁnd the odd and the even components
in any signal.
There are several signal decomposition methods. Fourier series, Fourier transform,
Laplace transform and z transform, which will be introduced later, can reﬂect the con-
cept of signal decomposition. It can be said that decomposition is the essence of signal
analysis. Since synthesis is the inverse operation of decomposition, we have nothing
more to say about it.
Authenticated
29 PM

1.4 Operations of continuous signals
|
29
1.4.8 Convolution integral
The convolution integral, or simply convolution, is a kind of special operation, it is
deﬁned as follows:
The convolution of two given functions f1(t) and f2(t) is written as
f1(t) ∗f2(t)
def=
+∞
∫
−∞
f1(τ)f2(t −τ)dτ ,
(1.4-5)
where, the operator symbol “∗” represents the convolution operation.
Example 1.4-3. the signals f1(t) = (3e−2t −1)ε(t) and f2(t) = etε(t) are known. Find
their convolution.
Solution.
f1(t) ∗f2(t) =
+∞
∫
−∞
f(τ)f(t −τ)dτ =
+∞
∫
−∞
(3e−2τ −1) ε(τ)et−τε(t −τ)dτ .
Since ε(τ) = 0 for τ < 0, ε(τ) = 1 for τ > 0, ε(t −τ) = 0 for τ > t, and ε(t −τ) = 1 for
τ < t, we have
f1(t)∗f2(t) =
t
∫
0
(3e−2τ −1)et−τdτ = et
t
∫
0
3e−3τdτ −et
t
∫
0
e−τdτ
= −et ⋅e−3τ󵄨󵄨󵄨󵄨󵄨
t
0 + et ⋅e−τ󵄨󵄨󵄨󵄨󵄨
t
0
= 1 −e−2t
t ≥0 .
Equation (1.4-5) is a high level abstract mathematical equation, so we will interpret the
concept of convolution by means of plotting to understand it better. According to the
deﬁnition, the convolution operation can be divided into ﬁve steps, namely change of
variables, reversal, shifting, multiplication, and integration, which will be explained
by graphic transformation as follows.
Example 1.4-4. The waveforms of f1(t) and f2(t) are shown in 󳶳Figure 1.28a and b,
respectively. Find y(t) = f1(t) ∗f2(t).
Solution. (1) Change of variables. Changing variable t of f1(t) and f2(t) into τ, we
obtain f1(τ) and f2(τ) as shown in 󳶳Figure 1.28a and b respectively.
(2) Reversal. Taking the vertical axis as the symmetry axis, we obtain the mirror sym-
metry signal f2(−τ) of f2(τ), as shown in 󳶳Figure 1.28c.
Authenticated
29 PM

30
|
1 Signals
(a)
(d)
(g)
(h)
(i)
(e)
(f)
(b)
(c)
1( )
f t
–τ
– τ
– τ
– τ
– τ
– τ
–
–
–
–
–
–
τ
τ
τ
τ
τ
τ
τ
τ
τ
τ
τ
t
1
1
0
2( )
f
t
2
1
0
)
(
2f
0
2
1
2
1
2
1
t
1
1
0
0
0
))
(
)(
(
2
1
t
f
f
1
1
1
2
1
t
t
))
(
)(
(
2
1
t
f
f
t
))
(
)(
(
2
1
t
f
f
2
1
t
t
2
1
t
0
1
2
1
t
t
0
1
2
1
t
t
))
(
)(
(
2
1
t
f
f
))
(
)(
(
2
1
t
f
f
1
1
1
0
1
( )
y t
4
16
t
1.5
0.5
Fig. 1.28: The illustration of convolution.
(3) Shifting. The f2(−τ) is shifted by t units to obtain f2(t −τ). From 󳶳Figure 1.28d
to h, we can see that the waveform of f2(t −τ) shifts along the τ axis with different
values of t.
(4) Multiplication. The product f1(τ) ⋅f2(t −τ) depends on the values of t.
(5) Integral. The integral to f1(τ) ⋅f2(t −τ) is calculated and the graphic area corre-
sponding to the product f1(τ) ⋅f2(t −τ) is computed.
The calculation steps of convolution over different ranges of t are as follows.
(1) For t ≤0 shown in 󳶳Figure 1.28d, the graphs of f1(τ) and f2(t −τ) do not overlap
and their product is zero; we have
y(t) = f1(t) ∗f2(t) = 0 .
(2) For 0 < t ≤1
2 shown in 󳶳Figure 1.28e, f2(t −τ) and f1(τ) have a nonzero overlap-
ping portion, which is the area under their product curve. The lower limit is zero
and the upper limit is t of the integral, so
y(t) = f1(t) ∗f2(t) =
t
∫
0
τ ⋅1
2dτ = 1
4 t2 .
Authenticated
29 PM

1.4 Operations of continuous signals
|
31
(3) For 1
2 < t ≤1 shown in 󳶳Figure 1.28f, f2(t −τ) and f1(τ) overlap completely. The
lower and upper limits of the integral are, respectively, t −1
2 and t, and we have
y(t) = f1(t) ∗f2(t) =
t
∫
t−1
2
τ ⋅1
2dτ = 1
4 t −1
16 .
(4) For 1 < t ≤3
2 shown in 󳶳Figure 1.28g, a portion of f2(t −τ) has moved out of the
nonzero region of f1(τ). The lower and upper limits are, respectively, t −1
2 and 1,
and we have
y(t) = f1(t) ∗f2(t) =
1
∫
t−1
2
τ ⋅1
2dτ = −1
4t2 + 1
4 t + 3
16 .
(5) When t > 3
2, as shown in 󳶳Figure 1.28h, f2(t −τ) has completely moved out of the
nonzero region of f1(τ), and their product is zero, that is,
y(t) = f1(t) ∗f2(t) = 0 .
From the above 5 equations, the sectionalized expression of y(t) is
y(t) =
{
{
{
{
{
{
{
{
{
{
{
{
{
{
{
1
4t2
(0 ≤t ≤1
2)
1
4t −1
16
( 1
2 < t ≤1)
−1
4 t2 + 1
4t + 3
16
(1 < t ≤3
2)
0
(other)
,
which is pictured in 󳶳Figure 1.28i.
As we have seen, convolution can actually be considered as an overlap integral.
The above graphic representation also shows that convolution to some functions can
be computed directly by using the graphical method to avoid complicated mathemat-
ical calculations.
Convolution has the following properties.
(1) A commutative property
f1(t) ∗f2(t) = f2(t) ∗f1(t) .
(1.4-6)
Proof.
f1(t) ∗f2(t) =
+∞
∫
−∞
f1(τ)f2(t −τ)dτ
Let t−τ=x
=
+∞
∫
−∞
f1(t −x)f2(x)dx = f2(t) ∗f1(t) .
Authenticated
29 PM

32
|
1 Signals
(2) A distributive property
f1(t) ∗[f2(t) + f3(t)] = f1(t) ∗f2(t) + f1(t)f3(t) .
(1.4-7)
Proof.
f1(t) ∗[f2(t) + f3(t)] =
+∞
∫
−∞
f1(τ)[f2(t −τ) + f3(t −τ)]dτ
=
+∞
∫
−∞
f1(τ)f2(t −τ)dτ +
+∞
∫
−∞
f1(τ)f3(t −τ)dτ
= f1(t) ∗f2(t) + f1(t) ∗f3(t) .
(3) An associative property
[f1(t) ∗f2(t)] ∗f3(t) = f1(t) ∗[f2(t) ∗f3(t)] .
(1.4-8)
Proof.
[f1(t) ∗f2(t)] ∗f3(t)
=
+∞
∫
−∞
[
[
+∞
∫
−∞
f1(τ)f2(η −τ)dτ]
]
f3(t −η)dη
Exchange
sequence
=
+∞
∫
−∞
f1(τ) [
[
+∞
∫
−∞
f2(η −τ)f3(t −η)dη]
]
dτ
Let η−τ=x
=
+∞
∫
−∞
f1(τ) [
[
+∞
∫
−∞
f2(x)f3(t −τ −x)dx]
]
dτ
Then
f1(t) ∗[f2(t) ∗f3(t)] = f1(t) ∗[
[
+∞
∫
−∞
f2(τ)f3(t −τ)dτ]
]
=
+∞
∫
−∞
f1(ξ) [
[
+∞
∫
−∞
f2(τ)f3(t −ξ −τ)dτ]
]
dξ .
By comparing the two equations, the property is proved to be true.
(4) A differential property
Suppose y(t) = f1(t) ∗f2(t), then
y󸀠(t) = f1(t) ∗f 󸀠
2(t) = f 󸀠
1(t) ∗f2(t) .
(1.4-9)
Authenticated
29 PM

1.4 Operations of continuous signals
|
33
Proof.
y󸀠(t)
=
d
dt [f1(t) ∗f2(t)] = d
dt
[
[
+∞
∫
−∞
f1(τ)f2(t −τ)dτ]
]
Exchange
sequence
=
+∞
∫
−∞
f1(τ) { d
dt [f2(t −τ)]}dτ =
+∞
∫
−∞
f1(τ) {
d
d(t −τ) [f2(t −τ)]} dτ
=
f1(t) ∗df2(t)
dt
.
(5) An integral property
Suppose y(t) = f1(t) ∗f2(t), then
t
∫
−∞
y(x)dx = f1(t) ∗[
[
t
∫
−∞
f2(x)dx]
]
= [
[
t
∫
−∞
f1(x)dx]
]
∗f2(t) .
(1.4-10)
Proof.
t
∫
−∞
y(x)dx
=
t
∫
−∞
[
[
+∞
∫
−∞
f1(τ)f2(x −τ)dτ]
]
dx
Exchange
sequence
=
+∞
∫
−∞
f1(τ) [
[
t
∫
−∞
f2(x −τ)dx]
]
dτ
x−τ=ξ
=
+∞
∫
−∞
f1(τ) [
[
t−τ
∫
−∞
f2(ξ)dξ]
]
dτ
ξ=x
=
+∞
∫
−∞
f1(τ) [
[
t−τ
∫
−∞
f2(x)dx]
]
dτ = f1(t) ∗[
[
t
∫
−∞
f2(x)dx]
]
.
(6) A calculus property
Suppose y(t) = f1(t) ∗f2(t), and if f1(t)|t=−∞= f2(t)|t=−∞= 0, then
y(t) = f 󸀠
1(t) ∗
t
∫
−∞
f2(t)dt =
t
∫
−∞
f1(t)dt ∗f 󸀠
2(t) .
(1.4-11)
The proof of this property is simple, and the only thing we need to do is to apply
the differential property into equation (1.4-10).
(7) Convolution with the impulse signal property
f(t) ∗δ(t) =
+∞
∫
−∞
f(τ)δ(t −τ)dτ =
+∞
∫
−∞
f(τ)δ(τ −t)dτ = f(t) .
(1.4-12)
Authenticated
29 PM

34
|
1 Signals
Similarly,
f(t)∗δ(t−t0) =
+∞
∫
−∞
f(τ)δ(t −τ −t0)dτ =
+∞
∫
−∞
f(τ)δ(τ −t + t0)dτ = f(t−t0). (1.4-13)
Obviously, the convolution of an arbitrary function f(t) and a unit impulse signal
that is delayed by t0 units is just that f(t) is also delayed with t0 units, and its
waveform is unchanged. This phenomenon is called the reappearance character-
istic.
Using the properties of convolution differentials and integrals, we also obtain
f(t) ∗δ󸀠(t) = f 󸀠(t) ,
(1.4-14)
f(t) ∗ε(t) =
t
∫
−∞
f(x)dx .
(1.4-15)
We also have the generalized expressions
f(t) ∗δ(k)(t) = f (k)(t) ,
(1.4-16)
f(t) ∗δ(k)(t −t0) = f (k)(t −t0) .
(1.4-17)
Example 1.4-5. f1(t) and f2(t) are, respectively, depicted in 󳶳Figure 1.29a and b;
please ﬁnd the result of f1(t) ∗f2(t).
Solution. According to the calculus feature of convolution, we obtain
f1(t) ∗f2(t) = f 󸀠
1(t) ∗
t
∫
−∞
f2(τ)dτ =
t
∫
−∞
f1(τ)dτ ∗f 󸀠
2(t) ,
then
f1(t) ∗f2(t) = f 󸀠
2(t) ∗
t
∫
−∞
f1(τ)dτ = [δ(t) + δ(t −1)] ∗
t
∫
−∞
2e−τε(τ)dτ
= 2 [δ(t) + δ(t −1)] ∗
t
∫
0
e−τdτ = 2 [δ(t) + δ(t −1)] ∗[(1 −e−t) ε(t)]
= 2 (1 −e−t) ε(t) + 2 [1 −e−(t−1)] ε(t −1) .
)
(
2 t
f
t
(b)
)
(
1 t
f
t
)
(
2
t
e tε
−
2
0
0
2
2
1
1
(a)
Fig. 1.29: E1.4-5.
Authenticated
29 PM

1.4 Operations of continuous signals
|
35
(8) The time shifting property
If y(t) = f1(t) ∗f2(t), then
f1(t −t1) ∗f2(t −t2) = y(t −t1 −t2) .
(1.4-18)
The proof is omitted here.
Example 1.4-6. Find the convolution y(t) = f1(t) ∗f2(t), where f1(t) = sgn(t −1)
and f2(t) = e−(t+1)ε(t + 1).
Solution. With time shifting, we have
y(t) = f1(t) ∗f2(t) = sgn(t −1) ∗e−(t+1)ε(t + 1) = sgn(t) ∗e−tε(t)
= [2ε(t) −1] ∗e−tε(t) = 2ε(t) ∗e−tε(t) −1 ∗e−tε(t)
and
ε(t) ∗e−tε(t) =
∞
∫
−∞
ε(t −τ)e−τε(τ)dτ =
t
∫
0
e−τdτ = −e−τ󵄨󵄨󵄨󵄨
t
0 = (1 −e−t) ε(t)
1 ∗e−tε(t) =
∞
∫
0
e−τdτ = −e−τ󵄨󵄨󵄨󵄨
∞
0 = 1 ,
and then
y(t) = f1(t) ∗f2(t) = 2 (1 −e−t) ε(t) −1 .
For convenience, the main properties of the convolution operation are listed in
Table 1.1.
Here, readers may ask: Why do we construct the convolution as such a strange opera-
tion? The reason is that it can be used in signal decomposition and plays an important
role in system analysis in the time domain.
Tab. 1.1: Main properties of convolution.
No.
Property
Expression
1
Commutative property
f1(t) ∗f2(t) = f2(t) ∗f1(t)
2
Distribution property
f1(t) ∗[f2(t) + f3(t)] = f1(t) ∗f2(t) + f1(t)f3(t)
3
Associative property
[f1(t) ∗f2(t)] ∗f3(t) = f1(t) ∗[f2(t) ∗f3(t)]
4
Differential property
y󸀠(t) = f1(t) ∗f 󸀠
2(t) = f 󸀠
1(t) ∗f2(t)
5
Integral property
∫
t
−∞y(x)dx = f1(t) ∗[∫
t
−∞f2(x)dx] = [∫
t
−∞f1(x)dx] ∗f2(t)
6
Calculus property
y(t) = f 󸀠
1(t) ∗∫
t
−∞f2(t)dt = ∫
t
−∞f1(t)dt ∗f 󸀠
2(t)
7
Convolution with an
impulse signal property
f(t) ∗δ(t) = f(t),
f(t) ∗δ(k)(t) = f (k)(t)
f(t) ∗δ(t −t0) = f(t −t0),
f(t) ∗δ(k)(t −t0) = f (k)(t −t0)
8
Time shifting property
f1(t −t1) ∗f2(t −t2) = y(t −t1 −t2)
Authenticated
29 PM

36
|
1 Signals
1.4.9 Plotting
Plotting is the visualization of the mathematical expression of a signal. Plotting meth-
ods have been learned by means of the calculus properties of functions in advanced
mathematics courses, but in this section, the emphasis will be on ways of mapping
with the operation features of signals and the characteristics of signal itself. Herein,
most plotting ways relate to sign, step and impulse signals, so readers should un-
derstand the deﬁnitions and properties of these signals very well. Additionally, read-
ers need to note that the operating ways for a composite function formed by the im-
pulse function are totally different from common composite functions, for example,
the derivative operation to this type of composite function. The reason is
δ[f(t)] =
n
∑
i=1
1
󵄨󵄨󵄨󵄨f 󸀠(ti)󵄨󵄨󵄨󵄨
δ(t −ti) ,
(1.4-19)
where ti are n different real roots of f(t) = 0. For example, if f(t) = 4t2 −1, we have
δ (4t2 −1) = 1
4δ (t + 1
2) + 1
4δ (t −1
2) .
Thefollowingexamples should bestudied carefully to master plottingskills and meth-
ods better.
Example 1.4-7. Plot the following signals.
(1) f1(t) = ε(−2t + 3)
(2) f2(t) = e−t sin 4πt[ε(t −1) −ε(t −2)]
(3) f3(t) = ε [cos(πt)]
(4) f4(t) = sgn [sin(πt)]
(5) f5(t) = δ (t2 −4)
(6) f6(t) = sin [πt sgn(t)]
Solution. Waveforms of these signals are shown in 󳶳Figure 1.30.
)
(
1 t
f
t
)
(
5 t
f
t
1
−
1
)
(
3 t
f
t
1
t
)
(
2 t
f
)
(
4 t
f
t
)
(
6 t
f
(1)
t
3
2
0
1
0
1
2
1
0 1
2
1
2
−
3
2
3
2
−
1
1
0
2
1
−
2
−
1
0
2
1
−
2
−
(1)
0
1
2
1
1
−
2
−
1
−
1
−
(1)
(2)
(3)
(4)
(5)
(6)
L
L
L
L
Fig. 1.30: E1.4-7.
Authenticated
29 PM

1.5 Solved questions
|
37
t
(1)
1
0
2
1
−
2
−
(1)
(1)
( 1)
−
( 1)
−
3
( )
f t
t
1
0
2
3
( )
f t
L
L
L
1
2
3
4
4
(1)
(2)
Fig. 1.31: E1.4-8.
Example 1.4-8. Plot the following signals.
(1) f(t) = d
dt {ε[sin(πt)]}
(2) f(t) = ∫
t
0−δ(sin(πτ))dτ
Solution. The solution for this example can be obtained by (1.4-19), but it is more trou-
ble, so we instead ﬁnd it by using the deﬁnitions and properties of singularity func-
tions.
(1) The independent variable of the composite step signal in this example is a sine
function; from the deﬁnition of the unit step signal the values of the composite
signal are 1 in all the positive sine semicycles, while values are 0 in all the nega-
tive semicycles. So, the waveform of the composite signal is a periodic rectangular
pulse train, and the derivative waveform of the signal is a series of unit impulse
signals located at different time points which alternate between positive and neg-
ative values, as shown in 󳶳Figure 1.31a.
(2) Referring to the solution of (1), we have
f(t) =
t
∫
0−
δ(sin(πτ))dτ =
t
∫
0−
[δ(τ) + δ(τ −1) + δ(τ −2) + . . . ] dτ
= ε(t) + ε(t −1) + ε(t −2) + . . .
So, the waveform of f(t) is the sum of a series of unit step signals located in differ-
ent places, as shown in 󳶳Figure 1.31b.
1.5 Solved questions
Question 1-1. Calculate the following equations.
(1) ∫
∞
−∞δ(−at)f(t)dt
(a > 0);
(2) ∫
∞
−∞cos 2πtδ(2t −1)dt
Solution. (1) From the even, the sampling, and the scaling properties, we have
∞
∫
−∞
δ(−at)f(t)dt = 1
a
∞
∫
−∞
δ(t)f(t)dt = 1
a f(0) .
Authenticated
29 PM

38
|
1 Signals
(2) According to the sampling and scaling properties of the impulse signal, we obtain
∞
∫
−∞
cos 2πtδ(2t −1)dt = 1
2
∞
∫
−∞
cos 2πtδ (t −1
2) dt = 1
2 cos 2πt
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨t= 1
2
= −1
2 .
Question 1-2. Calculate the value of δ(t) cos(2t).
Solution. According to the sampling of the impulse signal, we obtain
δ(t) cos(2t) = δ(t) cos(0) = δ(t) .
Question 1-3. Calculate ∫
t
0 (τ2 + 2)δ(2 −τ)dτ
Solution.
t
∫
0
(τ2 + 2) δ(2 −τ)dτ =
t
∫
0
(22 + 2) δ(2 −τ)dτ = 6
t
∫
0
δ(2 −τ)dτ
= 6
t
∫
0
δ(τ −2)dτ = 6
t−2
∫
−2
δ(η)dη
t−2>0
=
6ε(t) .
Question 1-4. Given f(6 −2t) = 2δ(t −3), ﬁnd the value of ∫
∞
−1 f(t)dt.
Solution. Let x = 6 −2t, then f(x) = 2δ ( x
2).
So
∞
∫
−1
f(t)dt =
∞
∫
−1
f(x)dx =
∞
∫
−1
2δ ( x
2) dx
y= x
2
=
∞
∫
−1/2
4δ(y)dy = 4ε(y)
t=y
= 4ε(t) .
Question 1-5. Calculate the value of ∫
∞
−∞(1 −2t2 + sin πt
3 ) δ(1 −2t)dt.
Solution. since the impulse signal is even, from the sampling and the scaling of it,
there is
∞
∫
−∞
(1 −2t2 + sin πt
3 ) δ(1 −2t)dt =
∞
∫
−∞
(1 −2t2 + sin πt
3 )
󵄨󵄨󵄨󵄨󵄨󵄨󵄨t= 1
2
δ(1 −2t)dt
=
∞
∫
−∞
δ(1 −2t)dt = 1
2
∞
∫
−∞
δ (t −1
2) dt = 1
2 .
Question 1-6. Calculate the value of ∫
∞
−∞(t2 + 2) [δ󸀠(t −1) + δ(t −1)] dt.
Solution.
∞
∫
−∞
(t2 + 2) [δ󸀠(t −1) + δ(t −1)] dt =
∞
∫
−∞
(t2 + 2) δ󸀠(t −1)dt +
∞
∫
−∞
(t2 + 2) δ(t −1)dt
= −2 + 3 = 1 .
Authenticated
29 PM

1.5 Solved questions
|
39
Question 1-7. Calculate the value of [ε(t) −ε(t −2)] δ(2t −2).
Solution. From sampling and scaling of the impulse signal, we have
[ε(t) −ε(t −2)] δ(2t −2) = [ε(t) −ε(t −2)] δ[2(t −1)] = [ε(t) −ε(t −2)] 1
2 δ[(t −1)]
= [ε(1) −ε(1 −2)] 1
2δ(t −1) = 1
2 δ(t −1)
Question 1-8. f(t)is shownin 󳶳FigureQ1-8, plots0.5 [f(t) + f(−t)] and 0.5 [f(t) −f(−t)].
( )
f t
0
1
2
t
-1
1
Fig. Q1-8
Solution. Reversing f(t), f(−t) is shown in 󳶳Figure Q1-8-1a;
Adding f(t) and f(−t), then 0.5[f(t) + f(−t)] is shown in 󳶳Figure Q1-8-1b;
Subtracting f(t) and f(−t), then 0.5 [f(t) −f(−t)] is shown in 󳶳Figure Q1-8-1c.
(
)
f
t
−
0
-1
1
t
-2
1
[
]
0.5
( )
(
)
f t
f
t
+
−
0
-1
1
t
-2
1
2
0.5
[
]
0.5
( )
(
)
f t
f
t
−
−
0
-1
1
t
-2
1
2
-1
(a)
(b)
(c)
Fig. Q1-8-1
Question 1-9. Signal f(t) is shown in 󳶳Figure Q1-9. Plot f(t + 1)[ε(t)−ε(t −1)], f (−1
2t)
and f(2t −1).
(a)
(b)
(c)
(d)
Fig. Q1-9
Authenticated
29 PM

40
|
1 Signals
Solution. From the algorithm and the properties of ε(t), the above signals can be plot-
ted in 󳶳Figure Q1-9a, b and c.
Question 1-10. Calculate [(1 −t)δ󸀠(t)] ∗ε(t).
Solution.
[(1 −t)δ󸀠(t)] ∗ε(t) = [δ󸀠(t) −tδ󸀠(t)] ∗ε(t) = [δ󸀠(t) −0δ󸀠(t) + δ(t)] ∗ε(t)
= δ󸀠(t) ∗ε(t) + δ(t) ∗ε(t) = δ(t) + ε(t) .
Question 1-11. The waveforms of f(t) and h(t) are shown in 󳶳Figure Q1-11 (1)a and b.
Plot f(t) ∗h(t).
( )
f t
0
1
t
-1
1
-1
( )
h t
0
4
t
2
1
-1
(a)
(b)
Fig. Q1-11 (1)
Solution. Integrating f(t) and differentiating h(t), we obtain the waveforms in
󳶳Figure Q1-11 (2)a and b.
Then
f∆(t) =
t
∫
∞
f(τ)dτ,
h󸀠(t) = δ(t) −2δ(t −2) + δ(t + 4) .
According to the characteristics of convolution, we obtain
f(t) ∗h(t) = f∆(t) ∗h󸀠(t) = f∆(t) −2f∆(t −2) + f∆(t −4) .
Thus, f(t) ∗h(t) can be plotted as shown in 󳶳Figure Q1-11 (3).
( )
f
t
Δ
0
1
t
-1
1
'( )
h t
0
4
t
2
-1
-2
(1)
(2)
(1)
(a)
(b)
Fig. Q1-11 (2)
( )
( )
f t
h t
∗
3
1
2
t
-1
-1
0
4
5
-2
Fig. Q1-11 (3)
Authenticated
29 PM

1.7 Problems
|
41
1.6 Learning tips
A signal is an object and a product of system processing, so readers should pay more
attention to the following points:
(1) A signal is a function of time.
(2) If the period T tends to inﬁnity, a periodic signal will become a nonperiodic signal.
(3) The complex exponential and impulse signals are the core signals herein.
(4) Usually, a signalcanbedecomposed into thealgebraic sum of limited or unlimited
other signals.
(5) Convolution integral operation.
(6) The characteristics of the signal itself can be used to draw its waveform.
1.7 Problems
Problem 1-1. Plot the following composite signals.
(1) f1(t) = ε (t2 −4)
(2) f2(t) = sgn (t2 −1)
(3) f3(t) = δ (t2 −9)
(4) f4(t) = sgn [cos (πt)]
(5) f5(t) = δ [cos (πt)]
(6) f6(t) = ε [cos (2πt)]
(7) f7(t) = sin [πt sgn(t)]
(8) f8(t) = d
dt {ε [sin(πt)]}
(9) f9(t) = d
dt [e−3tε(t)]
(10) f10(t) = ∫
t
0−δ (sin(πτ))dτ
Problem 1-2. Plot the following functions.
(1) f1(t) = ε (−2t + 3),
(2) f2(t) = tε(t −1)
(3) f3(t) = (t −1)ε(t −1),
(4) f4(t) = [ε(t −1) −ε(t −2)] e−t cos(10πt)
(5) f5(t) = [ε(t) −2ε(t −T) + ε(t −2T)] sin ( 4π
T t)
Problem 1-3. The waveform of f(t) is shown in 󳶳Figure P1-3, try to plot the following
functions.
(1) f(2t)
(2) f(t + 3)
(3) f(t −2)
(4) f(−4 −t)
(5) f(2t −4)
(6) f(4 −3t)
)
(t
f
1
1
t
0
2
)1(
1
−
Fig. P1-3
Authenticated
29 PM

42
|
1 Signals
Problem 1-4. What is the numerical value of each of the following integrals?
(1) ∫
∞
−∞f (t −t0)δ(t)dt
(2) ∫
∞
−∞f (t0 −t)δ(t)dt
(3) ∫
∞
0 δ(t −4)ε(t −2)dt
(4) ∫
∞
−∞(e−2t + te−t)δ(2 −t)dt
(5) ∫
∞
−∞e−3t sin(πt) [δ(t) −δ (t −1
3)]dt
(6) ∫
∞
−∞
sin(2t)
t
δ(t)dt
(7) ∫
1
−1 (t2 −3t + 1)δ(t −2)dt
(8) ∫
1
−1 δ (t2 −4) dt
(9) ∫
∞
−∞e−τδ(2τ)dτ
(10)∫
t
−∞e−τδ󸀠(2τ)dτ
(11) f(t) = 3δ(t −3) is known, ﬁnd the value of ∫
∞
0−f(6 −3t)dt.
(12) f(5 −2t) = 2δ(t −3) is known, ﬁnd the value of ∫
∞
0−f(t)dt.
Problem 1-5. What is the numerical value of each of the following equations?
(1)
d
dt [e−tδ(t)]
(2)
d
dt [e−2tε(t)]
(3)
d
dt [ε(t) −2tε(t −1)]
Problem 1-6. Write the expressions of the signals shown in 󳶳Figure P1-6.
)
(
1 t
f
)
(
2 t
f
)
(
3 t
f
π
4( )
f t
2
π
(a)
(b)
(c)
(d)
0
0
0
0
1
1
-1
1
2
2
t
t
t
t
2
1
-1 1
2
-1
-2
L
L
Fig. P1-6
Problem 1-7. The waveform of f(3 −2t) is shown in 󳶳Figure P1-7, plot f(t).
)
2
3
(
t
f
−
t
3 2
1
-1
(1)
(-1)
1
2
0
Fig. P1-7
Authenticated
29 PM

1.7 Problems
|
43
Problem 1-8. Calculate the energy and power of the following signals and classify the
signals as energy signals or power signals.
(1) f1(t) =
{
{
{
{
{
{
{
t
0 ≤t ≤1
2 −t
1 ≤t ≤2
0
other
(2) f2(t) = cos(t)ε(t)
(3) f3(t) = 5 cos(πt)+sin(5πt), −∞<t<∞
(4) f4(t) =
{
{
{
5 cos(πt)
−1 ≤t ≤1
0
other
(5) f5(t) = ε(t) −ε(t −1)
(6) f6(t) = tε(t)
Problem 1-9. Plot the odd and even components of eachsignal shown in 󳶳Figure P1-9.
)
(
1 t
f
t
1
0
1
1
−
)
(
2 t
f
t
1
1
−
0
1
2
)
(
3 t
f
t
1
0
1
)
(
4 t
f
t
1
0
1
2
1
−
3
−
2
−
1
−
L
L
3
(a)
(b)
(c)
(d)
Fig. P1-9
Problem 1-10. Signals are plotted in 󳶳Figure P1-10, ﬁnd each expression of ∫
t
−∞fi(τ)dτ
and sketch it.
)
(
1 t
f
1
t
0
2
1
3
( 1)
−
4
)
(
2 t
f
1
t
0
2
1
2
)
(
3 t
f
1
t
0
2
1
2
3
2
−
(a)
(b)
(c)
Fig. P1-10
Problem 1-11. Known is f1(t) = e−2t
(−∞< t < ∞), f2(t) = e−tε(t), ﬁnd f1(t) ∗f2(t).
Problem 1-12. Known are
(1) f1(t) ∗tε(t) = (t + e−t −1)ε(t)
(2) f2(t) ∗[e−tε(t)] = (1 −e−t)ε(t) −[1 −e−(t−1)]ε(t −1)
Find f1(t) and f2(t).
Authenticated
29 PM

44
|
1 Signals
Problem 1-13. Find the values of f1(t) ∗f2(t) for each group of signals shown in
󳶳Figure P1-13 and sketch them.
)
(
2 t
f
t
)
(
1 t
f
t
(a)
(b)
0
0
1
1
1
1
2
3
)
(
2 t
f
t
)
(
1 t
f
t
0
0
1
1
1
1
-1
2
Fig. P1-13
Problem 1-14. Find these convolutions.
(1) f1(t) = e−tε(t) ∗ε(t)
(2) f2(t) = sin(2πt)[ε(t) −ε(t −1)] ∗ε(t)
Authenticated
29 PM

2 Systems
Questions: Many tasks in productive practice can be considered as being performed
by a system. Then what is a system? How can we analyze a system?
Solution: Develop a system concept from practice →seek similarities among systems
→categorize systems →ﬁnd out the basic analysis methods for a system.
Results: 20 types of systems, such as linear and nonlinear, time variant and time
invariant, continuous and discrete systems, and so on. There are six basic oper-
ations, such as addition, subtraction, multiplication, time delay, differentiation
and integration. There are four basic analysis methods, for example, the external
ways in the time and transform domains, the state space ways in the time and
transform domains.
2.1 The concept of a system
1. The deﬁnition of system
As stated in the Preface, there are many kinds of systems in our lives; we can ﬁnd some
common features of these systems through analysis on them, such as: having a certain
scale; consisting of various things, equipment, facilities, and related staff; performing
special tasks, etc. As a result, a system can be said to be
a set of some elements that are mutually independent and relate to each other, and can
achieve some speciﬁc functions together.
This deﬁnition is applicable in all physical and nonphysical systems.
The systems discussed herein are mainly physical systems, for example, circuits
or electric networks and related models composed of resistors R, inductors L and ca-
pacitors C. Therefore, the term system in this book can be deﬁned as
the general designation for circuits, devices or algorithms which can transmit, transform
or process signals.
Obviously, a system can be abstracted into a functional module or an entirety with
cause-and-effect relationships, which can produce one or more outputs (effects or re-
sults) from one or more inputs (causes or reasons), as is sketched in 󳶳Figure 2.1, where
thetransformingrelationship betweentheoutput y(t) and the input f(t) or thepurpose
of a system can be denoted by the equation
y(t) = T[f(t)] ,
(2.1-1)
where the notation T[∙] represents a transformation to an element in brackets [ ]. If
the variable t is replaced by n, equation (2.1-1) also ﬁts the description of discrete sys-
https://doi.org/10.1515/9783110419535-002
Authenticated
29 PM

46
|
2 Systems
)
(t
f
A simple system
System
Input signals
Output signals
)
(
1 t
f
A complex system
)
(
2 t
f
( )
mf
t
)
(
1 t
y
)
(
2 t
y
( )
ky t
System
Input signal
Output signal
)
(t
y
(a)
(b)
Fig. 2.1: Schematic diagram of the system.
tems. If T[∙] can be further expressed by a speciﬁc mathematical equation, the explicit
equationshould becalled themathematicalmodelof thesystem or, simply, the model.
Note: The systems mentioned subsequently will be the models of systems in the-
ory, not the physical systems themselves, if there is no special explanation.
2. Contents and purposes of system analysis
The main contents of system analysis can be described equivalently by words as fol-
lows:
(1) After a signal has passed through a system, what will it look like?
(2) After an excitation has acted in a system, what response will be produced by the
system?
To sum up, system analysis aims to discuss the characteristics of transmission, trans-
formation or treatment of a signal by a known system, or is a process to analyze the
relationship between the excitation and the response of a given system. The main pur-
poses of system analysis are the following:
(1) To provide theoretical support and a method of guidance for the design, building
up or using a system that is described by the differential or difference equation
such as a real communication system or a control system.
(2) To lay the foundation for the study of the principles of communication, automatic
control and other professional courses.
It should be explained that this course only focuses on electronic systems, but the
analysis methods can be generalized to other analogous systems, because many phys-
ical systems are similar to electronic systems. So called analogous systems are systems
that have the same mathematical models, for example, an LC circuit and a mechanism
simple pendulum are analogous systems.
2.2 Excitation, response and system state
Before analyzing a system, we need to know the basic concepts such as excitation,
response and state.
The excitation is a kind of external energy or impact acting on a system, it is also
known as the input signal.
Authenticated
29 PM

2.2 Excitation, response and system state
|
47
R
C
E
6
Excitation
Response
System
E
t
0
system structure
 response without the starting state
0
=
t
)
(t
u&
)
(t
u&
(a)
(b)
Fig. 2.2: An RC charging circuit and its response with zero starting state.
The response is a kind of reaction or result from a system to an excitation, it is also
known as the output signal.
The RC circuit shown as 󳶳Figure 2.2 is a typical electric system. If the power E is
set as an excitation, then the voltage uC(t) on the capacitor C is the response produced
by E after the switch S is closed. If at t = 0 the switch S is closed, according to the three-
factor method in circuits analysis, there is
y(t) = y (∞) + [y (0+) −y (∞)] e−1
τ t ,
(2.2-1)
then the voltage uC(t), that is, the response of the system is
uC(t) = uC (∞) + [uC (0+) −uC (∞)] e−1
τ t = E + (0 −E)e−1
τ t = E (1 −e−1
τ t) ,
(2.2-2)
where, uC(∞) is the ﬁnal value of uC(t) and uC (0+) is the initial value, which is the in-
stantaneous value after switch S is closed at time t = 0; τ = RC is called the time con-
stant whose size can reﬂect the transient process length of uC(t); the greater the time
constant, the longer the transient process, and the waveform changes more slowly.
The waveform of uC(t) is shown in 󳶳Figure 2.2b. It can be seen that the system re-
sponse is generated entirely by the excitation existing after time t = 0.
Note: The above conclusion is reached under the premise that uC(t) is zero before
switch S is closed, i.e. uC (0−) = 0. If uC(t) is not zero before switch S is carried out, for
example, uC (0−) = E1, then what will be the performance of the system response?
In 󳶳Figure 2.3a, before switch S is closed the circuit has been in a stable state,
i.e. uC (0−) = E1. At time t = 0, S is pulled to position 2, according to the Law of
Switching in circuits theory, the voltage across a capacitor cannot jump (the current
through an inductor cannot jump either). Then uC (0+) = uC (0−) = E1, again using
the three-factor method, and we have
uC(t) = uC (∞) + [uC (0+) −uC(∞)] e−1
τ t = E + (E1 −E) e−1
τ t
= E1e−1
τ t + E (1 −e−1
τ t) .
(2.2-3)
Obviously, the response uC(t) consists of two parts, the ﬁrst is E1e−1
τ t generated by the
stored voltage (or energy) E1 of the capacitor before time t = 0; the second is E(1−e−1
τ t)
caused by the external excitation E after time t = 0, as shown in 󳶳Figure 2.3b.
It is necessary to point out that the stored voltage on the capacitor before t = 0 can
be called the starting state of the circuit or the system. The ﬁrst part in the response is
Authenticated
29 PM

48
|
2 Systems
1
E
1
2
R
C
E
S
C( )
u
t
Excitation
Response
System
0
=
t
E
t
0
E
t
0
1
E
Response decomposition 
Total response
1
E
First part response
Second part response
1
E
E
<
C( )
u
t
A RC charging circuit
 response
C( )
u
t
(a)
(b)
Fig. 2.3: An RC charging circuit and its response with start-
ing state.
generated by the starting state and is unrelated to the excitation existing after t = 0.
So, the state that can cause the response is also a kind of excitation in concept. To
differentiate from the input signal, the state is also called the inner excitation, whereas
the input signals we often mention all refer to the external excitations of a system.
Note: Although E1 is an external excitation in form, it does not work after t = 0,
and its role has been converted to the storage energy in the capacitor, so the external
excitation is only E in the example.
This way, the state of the system will be introduced as a new concept. The state is
considered as a kind of characteristic of things. The changing of the state means the
development or the changing of things, thus, the state can be considered as a basis
for dividing up the development stages of things. So, the state of a system is a set of
the fewest data that needs to be known and can be used to determine the response
at any time after t = t0 with the excitation appearing for t ≥t0. In general, this set
of data represents the energy storage conditions of every energy storage component
before the excitations are applied to the system. In other words,
under the condition of which all external excitations are given, a set of necessary and
sufficient data that must be known in order to determine the future response of system
is called the state of system.
Due to the inﬂuence of switching to a circuit, the state values of system may change
instantaneously at moment t = t0. In order to distinguish values before and after the
state instantaneously changes, we use the notations t0−and t0+ to denote, respec-
tively, the instants at which before and after the excitation acts to the system at t = t0.
Authenticated
29 PM

2.3 Classiﬁcation of systems
|
49
)
(t
y
T
)}
(
{
–
0t
x
)]
(
)},
(
[{
)
(
t
f
t
x
T
t
y
−
=
0
)
(t
f
Fig. 2.4: Relation between response and excitations.
Then the state at the instant before the excitation is accessed is named the starting
state, its values are denoted as x1(t0−), x2(t0−), . . . , xn(t0−). Obviously, this set of data
records all the related historical information of the system. The state at the instant af-
ter the excitation is accessed is called the initial state, and its values are denoted as
x1(t0+), x2(t0+), . . . , xn(t0+).
From the above concept of the state of the system, we can see that system response
y(t) for t ≥t0 is a function of the starting state and the input f(t), which is connected
to the system at time t = t0, and is expressed as
y(t) = T [x1 (t0−) , x2 (t0−) , . . . , xn (t0−) , f(t)]
t ≥t0 .
(2.2-4)
For convenience, the starting state values x1(t0−), x2(t0−), . . . , xn(t0−) at t = t0 can be
represented by the symbol{x(t0−)}; then (2.2-4) can be expressed as
y(t) = T [{x (t0−)} , f(t)]
t ≥t0 ,
(2.2-5)
as plotted in 󳶳Figure 2.4.
To sum up, the response y(t) of system for t ≥t0 is codetermined by the starting
state {x(t0−)} and the excitation f(t) existing in a range [t0, t]. This conclusion is also
applicable in multi-input multi-output systems. The state of a system at a given mo-
ment can give us all the information of the system at that moment. Hence, the analysis
of a system must focus on all behaviors of the system in the past, at present and in the
future.
2.3 Classiﬁcation of systems
According to the different characteristics of a system, systems herein are divided
mainly into 20 types, including linear and nonlinear systems.
2.3.1 Simple and complex systems
A system with single input and single output signals is known as a simple system
(SISO system), while a system with multi-input and multi-output signals is known as
a complexsystem (MIMOsystem), thesearesketched in 󳶳Figure2.1. Thesimplesystem
is the main topic that will be discussed in this book.
Authenticated
29 PM

50
|
2 Systems
Continuous system
)
(t
f
)
(t
y
Discrete system
[ ]
f n
[ ]
y n
( )
[ ( )]
y t
T f t
=
[ ]
[ [ ]]
y n
T f n
=
Fig. 2.5: Continuous system and discrete system.
2.3.2 Continuous-time and discrete-time systems
Like the classiﬁcation of signals, systems are also classiﬁed into continuous-time and
discrete-time systems.
A system with continuous-time input and output is a continuous-time system or,
simply, a CT system, while a system with discrete-time input and output is a discrete-
time system or, simply, a DT system or a digital system.
The excitation and response in a continuous system are represented by f(t) and
y(t), while the input and the output for a discrete system are represented by f[n] and
y[n], as is shown in 󳶳Figure 2.5. Practically, there may be a hybrid system with both
continuous and discrete signals.
2.3.3 Linear and nonlinear systems
A system that consists of some linear devices such as linear resistors, inductors and
capacitor can be called the linear system. A system containing some nonlinear devices
such as diodes is a nonlinear system. Both are plotted in 󳶳Figure 2.6.
Then, the question is how to judge the linearity of a system whose internal struc-
ture and components are unknown. Firstly, we must know a lot about the linearity
concept.
The linearity contains homogeneity and additivity.
(1) Homogeneity. If an input (excitation) increases k times, the output (response)
will also increase k times (k is any constant). That is,
if
f(t) →y(t) ,
then
kf(t) →ky(t) ,
(2.3-1)
)
(t
f
D
)
(t
y
1
R
R
)
(t
f
)
(t
y
R
A nonlinear system
A Linear system
(a)
(b)
Fig. 2.6: Examples of linear and nonlinear system.
Authenticated
29 PM

2.3 Classiﬁcation of systems
|
51
where the symbol “→” represents a kind of transform or process to a signal; it will
appear frequently later.
Note: If the coefficient k = 0,
a conclusion of which zero input must result in zero output for a linear system will
be obtained, i.e. the “zero-in/zero-out” property.
(2) Additivity. This means that if several excitations act on a system at the same time,
the total response of the system can be represented as an algebraic sum of re-
sponses generated by each excitation acting alone separately (others are zero).
This characteristic can be expressed as:
if
f1(t) →y1(t)
and
f2(t) →y2(t) ,
then
f1(t) + f2(t) →y1(t) + y2(t) .
(2.3-2)
The above two criteria can be combined to yield the superposition property or the
linearity,
if
f1(t) →y1(t)
and
f2(t) →y2(t) ,
then
k1f1(t) + k2f2(t) →k1y1(t) + k2y2(t) .
(2.3-3)
The linearity is shown in 󳶳Figure 2.7.
So far, the basic deﬁnition of the linear system can be given by
a system whose relationship between the response and the excitation meets linearity or
superposition property is linear.
This concept of the linear system has been widely accepted and has appeared in many
textbooks and monographs. Moreover, linearity is also regarded as a basis for estimat-
)
(t
f
)
(t
y
)
(t
kf
)
(t
ky
System
System
)
(
1 t
f
)
(
1 t
y
)
(
)
(
2
1
t
f
t
f
+
)
(
)
(
2
1
t
y
t
y
+
)
(
2 t
f
)
(
2 t
y
System
System
)
(
)
(
t
f
k
t
f
k
2
2
1
1
+
)
(
)
(
t
y
k
t
y
k
2
2
1
1
+
)
(
1 t
f
)
(
1 t
y
)
(
2 t
f
)
(
2 t
y
System
System
Additivity
homogeneity
linearity
(a)
(c)
(c)
Fig. 2.7: Schematic diagram of linear characteristic.
Authenticated
29 PM

52
|
2 Systems
Linear 
system
)
(t
f
)
(t
y
Continuous incrementally linear system
2( )
y t
[ ]
f n
[ ]
y n
Discrete incrementally linear system
2[ ]
y n
1( )
y t
1[ ]
y n
Linear 
system
(a)
(b)
Fig. 2.8: Incrementally linear systems.
ing if a system is linear and is called the linearity judgment condition. The conclusion
of “zero-in/zero-out” can be directly used to test whether a system is nonlinear. Ob-
viously, the condition can work well with the external measurements of a system but
does not need to understand the internal structure of a system.
A deﬁciency of the condition can be found by careful analysis; the basic deﬁnition
of linear systems just applies to systems whose response is only produced by external
excitation. However, this is not suitable for linear systems whose complete response
may also contain a part produced by the starting state of the system. So, under this
condition, systems that have a response component from the starting state should be
judged as nonlinear systems and are further deﬁned as the incrementally linear sys-
tems displayed in 󳶳Figure 2.8. The complete response can be considered as a super-
position of a response that satisﬁes the linearity condition and an increment response
in the systems. For example, y(t) = af(t) + b can be written as y(t) = y1(t) + y2(t),
where y1(t) = af(t) is the response that holds the linear condition, and y2(t) = b is an
increment response. It can be known that the linear systems meeting the basic deﬁni-
tion or the linearity condition only belong to a subset of incrementally linear systems
or are a special case in incrementally linear systems when the increment is zero.
American scholar B.P. Lathi put forward another deﬁnition for linear systems in
his work Signals, Systems, and Controls published in 1974:
A system is linear if and only if it can satisfy the response decomposability, the zero-state
linearity and the zero-input linearity.
For convenience, this deﬁnition is called the Lathi deﬁnition in this book.
The so called response decomposability, zero-state linearity and zero-input lin-
earity are illustrated separately as follows:
(1) Response decomposability. From Section 2.2, it is known that the response y(t)
of a system depends not only on the excitation f(t) but also on the starting state
{x(t0−)}. This fact also considers that the response of a system is generated by two
different excitations of f(t) and {x(t0−)}. Thus, the complete response y(t) should
be the sum of two parts; one is the response yx(t) which is only caused by the
starting state {x(t0−)} when the input f(t) is zero; the other is the response yf(t)
which only results from the input signal f(t) when the starting state {x(t0−)} is
zero. Usually, yx(t)is called thezero-inputcomponentor zero-inputresponse, and
yf(t) is called the zero-state component or zero-state response. Thus, the complete
Authenticated
29 PM

2.3 Classiﬁcation of systems
|
53
response y(t) can be expressed as
y(t) = yx(t) + yf(t) .
(2.3-4)
A living example can deepen the understanding of equation (2.3-4) for us: You
hurt your left foot yesterday when you carelessly kicked a stone, so your left foot
is still very sore today. This existing soreness can be compared to the zero-input
response of your nervous system now. Unfortunately, your left foot was also hit by
a fallingcup today, and thisnewpaincanbeconsidered tothezero-stateresponse.
Your pain is the result of one disaster after another; the old pain caused by the
stone and new pain from the cup together form the complete response of your
nervous system.
Equation (2.3-4) is just response decomposability, that is
the response decomposability is that the complete response of a system can be de-
composed into two components, a zero-input response and a zero-state response.
The response decomposability allows us to obtain the complete response with a
relative simple method, that is, we can calculate, respectively, two components
caused by two different excitations and then superimpose them to form the com-
plete response. In equation (2.2-3), the ﬁrst term E1e−1
τ t is the zero-input compo-
nent, and the second E(1 −e−1
τ t) is the zero-state component.
(2) Zero-state linearity. When the starting state {x(t0−)} is zero, the zero-state re-
sponse yf(t) must satisfy the linear condition to various inputs.
If
f1(t) →yf1(t)
and
f2(t) →yf2(t) ,
then
k1f1(t) + k2f2(t) →k1yf1(t) + k2yf2(t) .
(2.3-5)
(3) Zero-input linearity. When the input f(t) is zero, the zero-input response yx(t)
must satisfy the linear condition to various starting-state values.
If
x1(t0−) →yx1(t)
and
x2(t0−) →yx2(t) ,
then
k1x1(t0−) + k2x2(t0−) →k1yx1(t) + k2yx2(t) .
(2.3-6)
Obviously, if the zero-state response and zero-input response are all nonzero, not
only the complete response y(t) and the excitation f(t) do not hold the linear con-
dition, but the complete response y(t) and the starting state {x(t0−)} do not either.
Under the Lathi deﬁnition, y1(t) and y2(t) in 󳶳Figure 2.7 can be regarded as yf(t)
and yx(t) in equation (2.3-4), so the incrementally linear system is linear. Clearly,
a linear system that only satisﬁes the basic deﬁnition is actually a Lathi system
whose zero-input response is zero. The Lathi deﬁnition is generalized by the basic
Authenticated
29 PM

54
|
2 Systems
deﬁnition and is more comprehensive than the basic one. Thus, linear systems in
the book are all considered as Lathi systems or incrementally linear systems in
the basic deﬁnition, if there is no special explanation.
There is an important property in application of linear systems, that is, the re-
sponse and the excitation must be of the same frequency. In other words, the re-
sponse will not contain different components from the excitation in frequency.
This property is the theoretical basis for which alternating stable circuits can be
analyzed by the phasor method from circuit analysis courses. So far, a nonlinear
system can be easily judged from the linearity concept. A system must be nonlin-
ear, if it is not linear.
Based on the above characteristics, the analysis method for a linear system can be
greatly simpliﬁed. The two components in the complete response are easily cal-
culated by using the decomposability (the zero-input response can be obtained
if the input signal is set to be zero, the zero-state response can be obtained if the
starting-state values are set to be zero). Then, if the excitation f(t) can be decom-
posed into the algebraic sum of many simple functions (signals)
f(t) = a1f1(t) + a2f2(t) + a3f3(t) + . . . ,
(2.3-7)
so, according to zero-state linearity, the zero-state response can be expressed as
yf(t) = a1yf1(t) + a2yf2(t) + a3yf3(t) + . . . yfi(t) + . . . ,
(2.3-8)
where, yfi(t) is the zero-state response of system to the input fi(t).
The following examples show the usages and the characteristics of linear systems.
Example 2.3-1. If the starting-state values of a linear system are x1(0−) = 1 and
x2(0−) = 2 or, simply, {1, 2}, the zero-input response is 2 + 3e−2t. Please give the
zero-input response when the starting-state values increase ﬁve times compared to
the original ones.
Solution. Because the starting state is {5, 10}, according to zero-input linearity, the
zero-input response can expressed as 5(2 + 3e−2t).
Example 2.3-2. Suppose that the zero-input response is 2 −3e−2t when the starting
state is {1, 2} for a linear system, and the zero-input response is 5 + 2e−2t when the
starting state is {4, 1}. Find the zero-input response for the starting state is {5, 3}.
Solution. Obviously, the starting state {5, 3} is the superposition of {1, 2} and {4, 1},
so the corresponding zero-input response is also the superposition of the zero-input
response of 2 −3e−2t and 5 + 2e−2t, that is, 7 −e−2t.
Authenticated
29 PM

2.3 Classiﬁcation of systems
|
55
Example 2.3-3. Please judge whether or not the following systems are linear.
(1) y(t) = t ⋅f 2(t)
(2) y(t) = t ⋅f(t)
(3) y(t) = x(0−) + f 2(t)
(4) y(t) = x2(0−) + ∫
t
0 f(τ)dτ
(5) y(t) = 5x(0−)f(t)
(6) y(t) = 3f(t) + 6
Solution. Suppose f1 →y1; f2 →y2, then
(1) because af1 + bf2 →t(af1 + bf2)2 = t(af1)2 + t(bf2)2 + t2abf1f2
̸= ay1 + by2, the
system does not satisfy the linear condition and is nonlinear.
(2) Because af1 + bf2 →t(af1 + bf2) = atf1 + btf2 = ay1 + by2, the system satisﬁes
the linear condition and is a linear system.
(3) The system can satisfy the decomposability and zero-input linearity, but not zero-
state linearity, so it is a nonlinear system.
(4) The system can satisfy decomposability and zero-state linearity, but not zero-
input linearity, so it is a nonlinear system.
(5) The system cannot satisfy decomposability, so it is a nonlinear system.
(6) The system can satisfy decomposability, zero-input linearity and zero-state lin-
earity, so it is a linear system under the Lathi deﬁnition.
2.3.4 Time-variant and time-invariant systems
If the zero-state response is yf(t) to the excitation f(t) for a system, and the response
can become yf(t−t0) when the excitation is f(t−t0), or, if a time shift in the input only
results in the same time shift in the output, this system is said to be a time-invariant
system or, simply, a TI system. This can be expressed as
if
f(t) →yf(t) ,
then
f(t −t0) →yf(t −t0) .
(2.3-9)
The time invariance of the system is illustrated in 󳶳Figure 2.9.
)
(t
f
t
0
)
(t
y
t
0
)
(
0t
t
f
−
t
0
)
(
0t
t
y
−
t
0
0t
0t
A time invariant
system
Fig. 2.9: Time invariant property.
Authenticated
29 PM

56
|
2 Systems
Time invariance means that all parameters in the mathematical model of a system
do not change with time or are constant. A TI system is also called a ﬁxed system.
Most systems in real life are TI systems or can be reasonably approximated to this
kind of system. Otherwise, a system without this property is a time-variant system.
If a system not only meets the linear condition, but also meets the time invariant
condition, the system is called a linear time-invariant system or, simply, an LTI system.
The differential and integral properties of an LTI system can be shown as follows:
if
f(t) →y(t) ,
then
df(t)
dt
→dy(t)
dt
(2.3-10)
and
t
∫
−∞
f(τ)dτ →
t
∫
−∞
y(τ)dτ .
(2.3-11)
RLC networks, which are often studied, and circuits that include some active devices
(such as electronic tubes and transistors) are all TI systems. Note that here y(t) refers
to the zero-state response yf(t).
Example 2.3-4. Judge whether or not a system described by yf(t) = tf(t) is time-
invariant.
Solution. The expression of response with time shifting t0 is
yf(t −t0) = (t −t0)f (t −t0) .
The response corresponding to the excitation with time shifting t0 is
T [f (t −t0)] = tf (t −t0) .
Obviously, T[f(t −t0)]
̸= yf(t −t0), so the system is time variant.
The decomposability, the superposition and time invariance of the response can pro-
vide many methods for linear system analysis; for example, the convolution integral,
the convolution sum and the analysis methods in transform domains are all based on
these properties. So,
the linearity and time invariance are the essence of system analysis.
Because there are none of the above properties, the analysis of nonlinear systems is
very difficult, not only are the analysis methods not simple, but also the general so-
lution of the system cannot be obtained. Luckily, many nonlinear systems can be ap-
proximated to a linear system working in a limited range, which is the shortcut for
the analysis of nonlinear systems. So, the analysis methods for linear systems are also
important for the analysis of nonlinear systems.
Authenticated
29 PM

2.3 Classiﬁcation of systems
|
57
2.3.5 Causal and noncausal systems
A system is causal if its output at a certain moment is only determined by the inputs at
that moment and before, and is unrelated to the input in the future. Or we can say the
output of a causal system cannot precede its input, or a response will not be produced
by the system without an excitation; put simply, the result follows the cause.
The causal system is also the achievable system, and its property is
f(t) = 0,
t < t0
→
y(t) = 0,
t < t0 .
(2.3-12)
Note: We generally suppose t0 = 0. Thus, equation (2.3-12) can be regarded as the judg-
ment condition for a causal system. A system that cannot satisfy the causal condition
is a noncausal system. A noncausal system cannot be achieved physically.
Example 2.3-5. Test the causality of the following systems.
(1) y(t) = f(t) −f(t −3)
(2) y(t −1) = f(t) + f(t −1)
(3) y(t) = f(t −1) + f(t + 2)
(4) y(t) = f(3t)
Solution. From the deﬁnition of causal system, if t0 = 0, and put it into all expres-
sions, there are
(1) y(0) = f(0) −f(−3)
The excitation values f(0) and f(−3) are both before the response y(0), so the sys-
tem is causal.
(2) y(−1) = f(0) + f(−1)
The response value y(−1) arises before t0 = 0 and f(0), so the system is noncausal.
(3) y(0) = f(−1) + f(2)
The f(2) arises after the response y(0), which means the response y(0) at t0 = 0
relates to not only f(−1) before it but also f(2) after it, so the system is noncausal.
(4) If t = 1, there is y(1) = f(3), the response leads the excitation, so the system is
noncausal.
Example 2.3-6. Judge the linearity, time invarianceand causality of the following sys-
tems.
(1) y󸀠(t) + a0y(t) = b0f(t) + b1f 󸀠(t)
(2) y(t) = 2f(t)ε(t)
(3) y(t) = ∫
5t
−∞f(t)dt
Solution. (1) The system is a linear, time-invariant and causal system.
(2) The system satisﬁes linearity and causality. However,
y (t −t0) = 2f (t −t0) ε (t −t0)
̸= 2f (t −t0) ε(t) ,
so, the system is linear, causal and time variant.
(3) The system satisﬁes the linear condition.
If f(t) = et, then y(t) = ∫
5t
−∞f(t)dt = ∫
5t
−∞etdt = e5t.
If f(t −1) = et−1, then y(t) = ∫
5t
−∞et−1dt = e5t−1 = e5(t−1
5 ).
Authenticated
29 PM

58
|
2 Systems
Thus, the excitation delay is 1, but the response delay is only 0.5; the response
appears before the excitation, so the system cannot satisfy the causality. Addi-
tionally, because y(t −1) = e5(t−1), there is T[f(t −1)] = e5(t−1
5 )
̸= y(t −1) = e5(t−1),
the system does not satisfy time invariance, so it is linear, noncausal and time
variant.
2.3.6 Dynamic and static systems
If the response y(t0) of a system at time t0 relates to not only the excitation value f(t0)
at t0, but also values of excitation before time t0, this system is called a dynamic sys-
tem or a memory system. If the response y(t0) at time t0 only depends on the excitation
f(t0) at t0, the system is called a static, memoryless, real time or instantaneous system.
For example, a system that only consists of resistors is a real-time system, because a
resistor cannot store energy. A system that includes storage elements (such as capaci-
tors, inductors and magnetic cores) or memory circuits (such as registers) is a dynamic
system. The instances of the two systems are shown in 󳶳Figure 2.10.
Note: Although a sinusoidal steady state circuit in a circuits analysis course con-
tains dynamic devices in the form of a circuit, it is not a dynamic system because the
excitation is a sinusoidal signal and the steady-state response is only determined by
the excitation at present. Therefore, if we want to test whether a system is dynamic,
dynamic elements must be included in it, the types of excitation and the focus on re-
sponse must also considered.
Usually, the mathematical model of a static system in the time domain is an alge-
braic equation, but the mathematical model of a dynamic system is a differential or
difference equation.
2.3.7 Open-loop and closed-loop systems
A system that only allows the signal to be transmitted forward (from the input end to
the output end) is called an open-loop system or a no feedback system. A system that
also includes a back branch from the output to the input as well as a forward branch
is called a closed-loop system or a feedback system. These are plotted in 󳶳Figure 2.11.
)
(t
f
C
)
(t
y
1
R
3
R
)
(t
f
)
(t
y
R
A dynamic system
A non dynamic system
2
R
(a)
(b)
Fig. 2.10: Dynamic system and nondynamic systems.
Authenticated
29 PM

2.3 Classiﬁcation of systems
|
59
T1[  ]
)
(t
f
)
(t
y
An open-loop system
A closed-loop system
T1[  ]
)
(t
f
)
(t
y
T2[  ]
(a)
(b)
Fig. 2.11: Open-loop and closed-loop systems.
From the topology or physical structure, the diagram of an open-loop system is
like a straight line, and that of a closed-loop system is actually like a loop because the
feedback branch exists.
According to the different effects of a feedback signal in a feedback system, closed-
loop systems can be divided into positive and negative feedback systems. Systems
whose feedback signal enhances the effect of the input signal are positive feedback
systems (the operation symbol “+” is used in the graph), while systems whose feed-
back signal weakens the effect of the input signal are negative feedback systems (the
symbol “−” is used in the graph).
2.3.8 Stable and unstable systems
For any system whichhas no energy storageatthestarting moment, if a bounded input
produces a bounded output (BIBO: bound input/bound output), then the system is
called a stable system; it can be expressed as:
|f(t)| < ∞→|y(t)| < ∞.
(2.3-13)
If the input is bounded but the output is unbounded (inﬁnite), then the system is
known as an unstable system. Note that the y(t) here still refers to the zero-state re-
sponse yf(t). A positive feedback system is usually unstable, but a negative feedback
system is stable.
2.3.9 Lumped and distributed parameter systems
Systems composed of lumped parameter elements are called lumped parameter sys-
tems.
The lumped parameter element refers to one whose characteristics can be rep-
resented intensely by one parameter. Generally speaking, electronic elements whose
physical sizes are much smaller than their working wavelengths are lumped parame-
ter elements, such as common resistors, inductors, capacitors, transistors and trans-
formers, etc.
The main characteristics of a lumped parameter system are as follows:
(1) The effects from an excitation applied to any point of the system instantaneously
spreads to the whole system. In other words, once an excitation is applied on the
Authenticated
29 PM

60
|
2 Systems
input port of the system, the response of the system will immediately be produced
on the output port.
(2) There is only one independent variable in a lumped parameter system, and it can
be time, frequency or other physical parameters, so a lumped parameter system
can be described by an ordinary differential equation.
The systems composed of distributed parameter elements are called distributed pa-
rameter systems. The distributed parameter element refers to one whose character-
istics cannot be intensively represented by a parameter. That is, electronic elements
whose physical sizes are approximately same as their working wavelengths are dis-
tributed parameter elements, for example, transmission lines, antennas, ducts of
waves and mechanical shafts, etc. The characteristics of a distributed parameter
element cannot be reﬂected by only one parameter, but by multiparameters at the
same time. For example, in a section of a transmission line because the resistance,
inductance and capacitance exist on every point along the line, their three effects will
appear in the section at same time together, we must use three parameters such as R,
L and C to represent the features of the transmission line.
The main characteristics of a distributed parameter system are as follows:
(1) The effects from an excitation applied on any point of the system will spread to
the entire system after a period of time. In other words, although an excitation is
applied on the input port, the response cannot be produced immediately on the
output port.
(2) The variables involved in a distributed parameter system include not only time
but also space. This means the system variables are more than one, so the system
needs to be described by the partial differential equation.
In fact, all actual systems are distributed parameter systems, but some of them can be
approximated as lumped parameter systems. This approximation is reasonable only
if the element sizes of the system are much smaller than the input signal wavelength.
Usually, all the low frequency circuits are the lumped parameter systems. Only mi-
crowave circuits where the working frequency is greater than 300 MHz are likely to be
treated as distributed parameter systems. All systems discussed herein are the lumped
parameter systems.
2.3.10 Invertible and nonreversible systems
If different excitations can result in different responses, the system is an invertible
system, otherwiseitis a nonreversiblesystem. For example, y(t) = 3f(t)is an invertible
system, but y(t) = 3f 2(t) is not invertible because +f(t) and −f(t) will result in the same
y(t).
The invertible system has an important property: If a system is invertible, there
must be an inverse system corresponding to it. The response of the inverse system is
Authenticated
29 PM

2.4 Models of LTI systems
|
61
System
)
(t
f
Inverse 
system
)
(t
f
System
)
(t
f
Inverse 
system
)
(t
f
( )
3 ( )
y t
f t
=
1
( )
( )
3
y t
f t
=
Fig. 2.12: The feature of invertible system and an example.
the excitation of the system when the system and its inverse system are connected in
cascade form, namely, the input of the whole system is equal to output. For example,
a system y(t) = 1
3 f(t) is the inverse system of a system y(t) = 3f(t); they are plotted
in 󳶳Figure 2.12. Usually, if the response is equal to the excitation, the system can be
called an identity system.
The invertibility of a system is an important concept; for example, the coder must
be an invertible system in a communication system.
From the above content, we know that a real system can have several properties,
such as linearity, time invariance and causality, etc. Among various systems, the LTI
causal system is the most basic and important, and it is also the basis for the analysis
of other systems, so we will mainly discuss LTI causal systems or, simply LTI systems,
in this book.
Note: The above concepts or deﬁnitions of various continuous systems hold in
discrete time in the book if there are no further instructions.
2.4 Models of LTI systems
2.4.1 Mathematical models
Because the structures and functions of different LTI systems are quite different from
each other, it is necessary to seek a general analysis method in order to analyze the
various LTI systems effectively. Based on their structures and functions, the various
physical systems can be abstracted as a uniﬁed model – the mathematical expression,
which is the basis of various analytical methods. Therefore, we have the following
deﬁnition:
The mathematical expression that can fully reﬂect the characteristics of a system is
called the mathematical model of the system or, simply, the model. The process of look-
ing for this mathematical expression is called modeling.
It can be proved that the mathematical model for nth-order continuous LTI systems is
an nth-order linear constant coefficient differential equation like
an
dny(t)
dtn
+ ⋅⋅⋅+ ai
diy(t)
dti
+ ⋅⋅⋅+ a1
dy(t)
dt
+ a0y(t)
= bm
dmf(t)
dtm
+ ⋅⋅⋅+ bi
dif(t)
dti
+ ⋅⋅⋅+ b1
df(t)
dt
+ b0f(t)
(2.4-1a)
Authenticated
29 PM

62
|
2 Systems
or
n
∑
i=0
ai
diy(t)
dti
=
m
∑
j=0
bj
djf(t)
dtj
.
(2.4-1b)
The mathematical model of Nth-order discrete LTI systems is an Nth-order linear con-
stant coefficient difference equation like
aNy[n] + aN−1y[n −1] + ⋅⋅⋅+ aiy[n −i] + ⋅⋅⋅+ a0y[n −N]
= bMf[n] + bM−1f[n −1] + ⋅⋅⋅+ bif[n −i] + ⋅⋅⋅+ b0f[n −M]
(2.4-2a)
or
N
∑
k=0
aN−ky[n −k] =
M
∑
r=0
bM−rf[n −r] .
(2.4-2b)
Note: If equation (2.4-1) satisﬁes the slacking (at rest) condition
f(t) = 0 , t < t0
→
y(t) = 0 , t < t0 ;
(2.4-3)
if equation (2.4-2) satisﬁes the slacking condition
f[n] = 0 , n < n0
→
y[n] = 0 , n < n0 ,
(2.4-4)
then the system described by equation (2.4-1) or equation (2.4-2) is a continuous linear
or discrete linear system, which can only satisfy the basic deﬁnition of a linear system.
Because a system with a nonslacking starting condition can produce the zero-input
response caused by the starting state, namely, the increment response, so the system
will become an incrementally linear system. Additionally, it can be proved that the
system with a slacking condition and modeled by equation (2.4-1) or equation (2.4-2)
is also time invariant.
By this observation, the slacking condition equation (2.4-3) is the same as the
causal condition equation (2.3-12), which means that a system described by a linear
differential equation under the slacking condition is both an LTI system and a causal
system. This conclusion ﬁts well for discrete systems.
For the convenience of analysis, the t0 in equation (2.4-3) and the n0 in equa-
tion (2.4-4) are set to be t0 = 0 and n0 = 0. The ai, bj in equation (2.4-1) and the aN−k,
bM−r in equation (2.4-2) are all real constants. Values of m, n, M and N depend on the
related parameters of the system, usually m ≤n and M ≤N. If the coefficients ai, bj,
aN−k and bM−r are functions of time t or n, the system will be a linear time variant one.
Note: The starting slacking condition of an equation is equal to that the starting
condition of a system is zero. Because the starting condition is determined by the start-
ing state, zero starting means zero-state, and zero-state means zero stored energy.
2.4.2 Mathematical modeling
It is depends largely on the comprehensiveness and accuracy of the mathematical de-
scription of the system whether we can completely and accurately analyze a system, so
Authenticated
29 PM

2.4 Models of LTI systems
|
63
Tab. 2.1: Component constraint.
Component
Symbol
Constraint relationship
Resistor
R
u
Ri
R
+
−
iR(t) = uR(t)
R
Capacitor
C
u
Ci
C
+
−
iC(t) = C duC(t)
dt
Inductor
L
u
Li
+
−
L
uL(t) = L diL(t)
dt
modeling is a premise and a key to system analysis. For an electronic system, model-
ing is used to seek the equation that can reﬂect the motion behavior of a system based
on some related circuit principles (constraint conditions) and the system structure.
The essential basis to establish the mathematical model of an electric system is
two constraint conditions which exist in all circuits:
(1) The constraint condition for a component is a relation expression to describe the
characteristics of a component and is usually the relationship between voltage
and current on the component. It is expressed as the V-A characteristic. Table 2.1
shows the constraint relations for common components.
(2) The constraint conditions for a network are constraint relations among voltages
and currents in a circuitand are determined by the network structure. They are ex-
pressed by Kirchhoff’s voltage law (KVL) and Kirchhoff’s current law (KCL). Thus,
we can say that
the V-A characteristics of components and Kirchhoff’s laws of circuits are the basic
theories of modeling of electric systems.
Kirchhoff’s current law (the ﬁrst law) states that for any node in a circuit, the al-
gebraic sum of all branch currents on this node is zero at any time. That is,
∑i(t) = 0
or
∑iin(t) = ∑iout(t) .
(2.4-5)
Kirchhoff’s voltage law (the second law) states that for any loop in a circuit, the
algebraic sum of all branch voltages along the loop is zero at any time. That is,
∑u(t) = 0
or
∑uup(t) = ∑udown(t) .
(2.4-6)
Details of modeling of an electronic system will be shown by the following examples.
Example 2.4-1. 󳶳Figure 2.13 shows a third-order electronic system, where uS(t) is the
excitation, u(t) is the response, L1 = L2 = 1 H and R1 = R2 = 1 Ω, C = 2 F. Try to give
the differential equation of this system.
Authenticated
29 PM

64
|
2 Systems
II
I
Ci
1i
2i
2
L
1
L
1
R
S( )
u t
( )
u t
C
a
C
U
2
R
Fig. 2.13: E2.4-1.
Solution. Meshes I, II and their currentreferencedirections areshownin󳶳Figure2.13.
Using KCL to write the current equation on node a, we have
iC = i1 −i2 .
Using KVL to write equations of meshes I, II separately
R1i1 + L1
di1
dt + uC = uS
and
R2i2 + L2
di2
dt = uC .
Putting the component parameters into them, we have
i1 + di1
dt + uC = uS
and
i2 + di2
dt = uC .
From the above two expressions, we obtain
i1 + di1
dt + i2 + di2
dt = uS .
(2.4-7)
Then we can write the relationships between the voltage and current of elements, and
putting the component parameters into them, we have
i2 = u
R2
= u ,
(2.4-8)
uC = L2
di2
dt + u = du
dt + u ,
iC = C duC
dt = 2d2u
dt2 + 2du
dt = i1 −i2 .
Therefore,
i1 = 2d2u
dt2 + 2du
dt + i2 .
(2.4-9)
Putting equations (2.4-8) and (2.4-9) into equation (2.4-7), the differential equation of
the system is
d3u
dt3 + 2d2u
dt2 + 2du
dt + u = 1
2 uS .
(2.4-10)
Note: The order number of a differential or a difference equation is also the order of
the system and also the number of independent energy storage devices in the system.
Authenticated
29 PM

2.4 Models of LTI systems
|
65
To sum up, the modeling steps for an electronic system are the following:
(1) From the V-A characteristic of each device, write the V-A relations of all devices in
a system.
(2) Put these relations into the KCL and KVL equations in the system to obtain several
algebra and differential equations.
(3) All these equations can be rearranged and simpliﬁed by means of elimination into
a differential equation that only contains excitation and response, which is the
mathematical model of the system.
Herein, the mathematical model of an electronic system is exactly a constant coeffi-
cient linear differential or difference equation.
2.4.3 Block models
From the picture books of our childhood, we know that the knowledge represented by
graphics is much easier to understand and to remember. In other words, the graphical
representation of knowledge is the highest level of teaching. Thus the question is, can
the formula in equation (2.4-1) or equation (2.4-2) be represented by graphics? Can a
differential or a difference equation be diagramed?
By observing equation (2.4-1) carefully, we ﬁnd that the mathematical model of
a system is an expression whose excitation and response are connected by addition,
multiplication and differential operations. Naturally, we think that if these operations
can be shown by graphs, then f(t) and y(t) can be connected by them, and thereby
the graphical model of a system can be obtained. For this case, operations such as
addition, multiplication, differentiation, delay and integration all are abstracted as
a unit or a module and further represented by block diagrams, which are shown in
󳶳Figure 2.14. Note that a differentiator is an inverse system of an integrator.
∫
nf
f
f
y
⋅
⋅
⋅
=
2
1
)
(
)
(
t
f
a
t
y
⋅
=
)
(t
f
)
(t
f
dt
d
dt
t
df
t
y
)
(
)
(
=
)
(t
f
)
(t
f
a
1
2
n
y
f
f
f
=
+
+
+
1f
2f
nf
∫∞
−
=
t
d
f
t
y
τ
τ )
(
)
(
T
y(t) = f (t – T)
Summer
Multiplier
Number  multiplier 
Delayer
Differentiator
Integrator
1f
2f
nf
(a)
(b)
(c)
(d)
(e)
(f)
Fig. 2.14: Basic block diagrams of system operations.
Authenticated
29 PM

66
|
2 Systems
)
(t
f
( )
y t
′′
)
(t
y
∫
∫
1a
0a
( )
y t
′
Fig. 2.15: E2.4-2.
According to the linear condition, the above operators can be proved as linear
systems. Thus, using these graphical components for basic operators, we can trans-
late a mathematical model that is an abstract and obscure differential equation into a
visual and comprehensible graphical model. It is known as the block diagram simu-
lation method and uses block diagrams to express a system; the similar ﬂow diagram
simulation method will be introduced in Chapter 7.
Example 2.4-2.
d2y(t)
dt2 + a1
dy(t)
dt + a0y(t) = f(t) is a two-order system model; try to sim-
ulate this system with basic block diagrams.
Solution. The basic principle of solving a differential equation is to extract the origi-
nal function using the integral operation. For the above equation, obviously, twice as
many integral operations are necessary. Thus, we transform the original expression
into
d2y(t)
dt2
= −a1
dy(t)
dt
−a0y(t) + f(t) .
It can be seen that the right side of this equation can be considered as the sum of three
terms, that is, the output of an adder is a second-order function term d2y(t)
dt2 , which can
become the original function after it has been integrated twice. Then the function is
multiplied separately by corresponding coefficients and is fed back to the adder, and
the problem is solved. The system simulation is plotted in 󳶳Figure 2.15.
Example 2.4-3.
d2y(t)
dt2 +a1
dy(t)
dt +a0y(t) = b1
df(t)
dt +b0f(t) is a second-order linear model;
simulate the system by basic operators.
Solution. Because b1
df(t)
dt appears on the right side, the solution in Example 2.4-2can-
not be borrowed directly. Thus, an assistant signal m(t) is introduced to connect f(t)
and y(t) to form two submodels, and the solution in Example 2.4-2 can be used to solve
this problem.
The ways to design the submodels are:
(1) change y(t) into m(t) on the left side; only f(t) is kept on the right side, so the ﬁrst
submodel is obtained.
)
(t
f
( )
m t
′′
( )
m t
′
( )
m t
∫
∫
1a
0
a
0b
1b
)
(t
y
Fig. 2.16: E2.4-3.
Authenticated
29 PM

2.5 Analysis methods for LTI systems
|
67
(2) Only keep y(t) on left side and change f(t) into m(t) on the right side to obtain the
second submodel. We have
d2m(t)
dt2
+ a1
dm(t)
dt
+ a0m(t) = f(t) ,
(1)
y(t) = b1
dm(t)
dt
+ b0m(t) .
(2)
Simulating the above two equations, we obtain 󳶳Figure 2.16.
The solution here also ﬁts well for situations where there are higher orders of deriva-
tives of the excitation on the right side. In addition, this method is only used to explain
the simulation principle, but Mason’s formula in Chapter 7 is the common simulation
method in real applications.
The above examples illustrate that a system can be represented by both the mathe-
matical model and the block diagram model. Note that the block diagram model is
not a new system model that is different from the mathematical model, but just the
graphical expression of the mathematical model, and thus, the mathematical model
is the basis. A system can be modeled with different of its mathematical model.
2.5 Analysis methods for LTI systems
As stated in above, LTI system models can reﬂect directly the relation between exci-
tation and response, namely, the analysis results of the models can only reﬂect the
external properties or the I/O properties of a system and are unrelated to the internal
parameters. This analysis method is called the external analysis method or the I/O or
ports analysis method. The external method is the main method used for analysis of
a system herein.
In real applications, people often want to know what inﬂuence an internal change
of a system has on the response, so state space analysis method can be used.
The state space analysis method can connect excitations, responses and internal
state variables by a set of state equations and a set of output equations, so that the
change laws of the responses are revealed with changes of the internal parameters in
a system.
The state equations and the output equations are all mathematical models of a
system, so, whether the external analysis method or the state space analysis method
is used, the basic principle of both is to solve mathematical models or to solve equa-
tions. Moreover, the solutions to an equation can be classiﬁed into time domain and
transform domain methods. So, the ways to analyze an LTI system mainly consist of
four analysis methods: the external time domain, transform domains, the state space
time domain, and transform domains.
Authenticated
29 PM

68
|
2 Systems
In conclusion, there are four analysis methods for a LTI system, and their prin-
ciples are all based on building the model of the system and ﬁnding its solution. In
other words, the essence of system analysis is just to solve equations.
2.6 Solved questions
Question 2-1. For an LTI system with the same initial condition, the complete re-
sponse is y1(t) = [2e−3t + sin(2t)] ε(t) when the excitation is e(t), while the complete
response is y2(t) = [e−3t + 2 sin(2t)] ε(t) when the excitation is2e(t). Then
(1) When the initial condition is invariant, ﬁnd the complete response y3(t) when the
excitation is e(t −t0), and t0 is a real constant and greater than 0.
(2) If the initial condition increase by one, ﬁnd the complete response y4(t) when the
excitation is 0.5e(t).
Solution. (1) Letting the zero-input and the zero-state responses be yx(t) and yf(t),
we have
y1(t) = yx(t) + yf(t) = [2e−3t + sin(2t)] ε(t) ,
y2(t) = yx(t) + 2yf(t) = [e−3t + 2 sin(2t)] ε(t) .
Then
yx(t) = 3e−3tε(t)
and
yf(t) = [−e−3t + sin(2t)] ε(t) .
When the excitation is e(t −t0), the complete response is
y3(t) = 3e−3tε(t) + [−e−3(t−t0) + sin(2t −2t0)] ε(t −t0) .
(2) From the results of (1), we know that yx(t) and yf(t), so, when the initial condition
increases by one and the excitation is 0.5e(t), the complete response is
y4(t) = 2yx(t) + 0.5yf(t) = 2 × 3e−3tε(t) + 0.5 × [−e−3t + sin(2t)] ε(t)
= [5.5e−3t + 0.5 sin(2t)] ε(t) .
Question 2-2. The initial conditions of an LTI system are x1(0) and x2(0).
(1) When x1(0) = 1, x2(0) = 0, the zero-input response is yx1(t) = (e−t + e−2t)ε(t).
(2) When x1(0) = 0, x2(0) = 1, the zero-input response is yx2(t) = −(e−t −e−2t)ε(t).
If the complete response is (2 + e−t)ε(t) when the excitation is f(t), x1(0) = 1 and
x2(0) = −1, ﬁnd the complete response when the excitation is 2f(t), x1(0) = −1 and
x2(0) = −2.
Solution. From the known conditions (1) and (2), we have
yx1(t) = T {f(t) = 0, x1(0) = 1, x2(0) = 0} = (e−t + e−2t) ε(t) ,
yx2(t) = T {f(t) = 0, x1(0) = 0, x2(0) = 1} = −(e−t −e−2t) ε(t) .
Authenticated
29 PM

2.6 Solved questions
|
69
Because
yf(t) = T {f(t), x1(0) = 0, x2(0) = 0} ,
from the known condition, we obtain
y1(t) = T {f(t), x1(0) = 1, x2(0) = −1} = yf(t) + yx1(t) −yx2(t)
= yf(t) + 2e−tε(t) = (2 + e−t) ε(t) .
Thus, the zero-state response is
yf(t) = (2 −e−t)ε(t) .
Hence, for the excitation 2f(t), x1(0) = −1, x2(0) = −2, the complete response y(t) is
y(t) = T {2f(t), x1(0) = −1, x2(0) = −2} = 2yf(t) −yx1(t) −2yx2(t)
= (4 −e−t −3e−2t) ε(t) .
Question 2-3. Known the zero-state response of a CT system is yf(t) = f(4t) when
the excitation is f(t). Judge whether the system is time variant and linear. Prove your
conclusions.
Solution. According to the given conditions, we have
f1(t) →yf1(t) = f1(4t)
and
f2(t) →yf2(t) = f2(4t) ,
then
af1(t) + bf2(t) →ayf1(t) + byf2(t) = af1(4t) + bf2(4t) .
Thus, the system satisﬁes linearity. According to the given conditions, we also know
that
f(t −t0) →f(4t −t0) .
From the time invariance property, this can be written as
f(t −t0) →yf(t −t0) = f(4t −4t0) .
Obviously, the system cannot satisfy the time invariant condition, so it is linear, time
variant.
Question 2-4. Judge whether or not the system described by y(t) = (t + 5) cos ( 1
x(t)) is
causal and time variant. Prove your conclusions.
Solution. The output y(t) is only related to the current output, so the system is causal.
Letting
x(t) →y(t) = (t + 5) cos [ 1
x(t)] ,
we have
x(t −τ) →(t + 5) cos [
1
x(t −τ)]
̸= y(t −τ) .
So, the system is time variant.
Authenticated
29 PM

70
|
2 Systems
Question 2-5. Judge whether or not the system described by y(t) = ∫
2t−1
−∞f(τ)dτ is
linear, time variant. Prove your conclusions.
Solution. The known conditions can be expressed as
f(t) →y(t) =
2t−1
∫
−∞
f(τ)dτ .
Let
f1(t) →y1(t) =
2t−1
∫
−∞
f1(τ)dτ, f2(t) →y2(t) =
2t−1
∫
−∞
f2(τ)dτ .
Because
αf1(t) + βf2(t) →
2t−1
∫
−∞
[αf1(τ) + βf2(τ)] dτ = αy1(t) + βy2(t) ,
the system is linear. Because
f(t −t0) →
2t−1
∫
−∞
f(τ −t0)dτ =
2t−1−t0
∫
−∞
f(η)dη
̸= y(t −t0) ,
the system is time variant.
2.7 Learning tips
The system is the main object of research herein, and readers should pay attention to
the following points:
(1) Theoretically, a system is a kind of signal converter and is also a type of algorithm,
the characteristics of which can be described by a mathematical model. In fact,
systems mainly refers to circuits herein. Therefore, the basic knowledge from cir-
cuit analysis.
(2) The state and the response of a system are separate reactions of the system to the
storage energy situation and the excitations.
(3) The LTI system is the basis for analysis of other systems.
(4) The system can be described by models, so analyzing a system is actually analyz-
ing its models.
(5) Each system model is a bridge to connect excitation and response, which is estab-
lished by means of the characteristics and the structure of system itself.
(6) Thesystem mathematicalmodelcanbesimulated by operatingblock diagrams, or
the mathematical model and the block diagram model of a system are equivalent.
Authenticated
29 PM

2.8 Problems
|
71
2.8 Problems
Problem 2-1. If the starting state of a system is x(0−), the excitation is f(t) and the
response is y(t), try to judge whether or not the following systems are linear.
(1) y(t) = x2(0−) + f 2(t)
(2) y(t) = x(0−) log f(t)
(3) y(t) = x(0−) sin(t) + tf(t)
(4) y(t) = x2(0−) + ∫
t
0 f(τ)dτ
(5) y󸀠(t) = log[x(0−)] + f 2(t) + ty(t)
Problem 2-2. If the starting state of a system is x(0−), the excitation is f(t) and the
response is y(t), try to judge whether or not the following systems are time invariant.
(1) y(t) = f(t) + f(t −t0)
(2) y(t) = x(0−) + 3tf 2(t)
(3) y(t) = f(t) + tx(0−)
(4) y󸀠󸀠(t) = y󸀠(t)y(t) + x1(0−) + x2(0−) + lg[f(t)]
Problem 2-3. Test whether or not the following systems are causal.
(1) y(t) = cos(t) ⋅f(t)
(2) y(t) = f(−t)
(3) y(t) = f(t −1) −f(1 −t)
(4) y(t) = f(t) ⋅f(t −b)
(5) y(t) = 2f(t) ⋅ε(t)
Problem 2-4. Judge whether or not the following systems are linear, time invariant
and causal.
(1) y(t) = x(0−) sin(t) + at2f(t)
(2) y(t) = f(t + 10) + f 2(t)
(3) y(t) = (t + 1)f(t)
(4) y󸀠(t) + 10y(t) = f(t)
(5) y󸀠(t) + y(t) = f(t + 10)
(6) y󸀠(t) + t2y(t) = f(t)
Problem 2-5. When the input of a system is δ(t −τ), the output is h(t) = ε(t −τ)−ε(t −
3τ), judge whether or not the system is time invariant and causal.
Problem 2-6. The starting state of an LTI causal system is zero, when the excitation
is f1(t) = ε(t), the response is y1(t) = (3e−t + 4e−2t)ε(t), ﬁnd the response when the
excitation is instead the signal as shown in the 󳶳Figure P2-6.
)
(
2 t
f
t
1
0
1
2
1
−
)
(
3 t
f
t
1
0
1
2
)
(
4 t
f
t
1
0
2
)1(
( 2)
−
)1(
(a)
(b)
(c)
Fig. P2-6
Problem 2-7. An LTI system has a starting state, and the complete response is y1(t) =
3e−2t + sin(4t)
t > 0 when the excitation is f(t). If the starting state is constant and
Authenticated
29 PM

72
|
2 Systems
the excitation is 2f(t), the complete response is y2(t) = 4e−2t + 2 sin(4t)
t > 0. Find
the complete response when the excitation is 3f(t) and the starting state is the same.
Problem 2-8. There is an LTI system with starting conditions x1(0−) and x2(0−), the
excitation is f(t),and the response is y(t). It is known that
(1) when f(t) = 0, x1(0−) = 5, x2(0−) = 2, we have y(t) = e−t(7t + 5), t > 0;
(2) when f(t) = 0, x1(0−) = 1, x2(0−) = 4, we have y(t) = e−t(5t + 1), t > 0;
(3) when f(t) =
{
{
{
0, t < 0
1, t > 0
, x1(0−) = 1, x2(0−) = 1, we have y(t) = e−t(t + 1), t > 0.
Find the zero-state response of the system when f(t) =
{
{
{
0, t < 0
3, t > 0
.
Problem 2-9. Try to write the differential equation of uC(t) in 󳶳Figure P2-9.
+
−
S( )
u t
2Ω
2Ω
1F
C( )
u
t
+
−
Fig. P2-9
Problem 2-10. An excitation is f(t) = sin(2t)ε(t), voltages across capacitors are zero
at initial moment, ﬁnd the expression of the output signal uC(t) in 󳶳Figure P2-10.
+
−
)
(t
f
1F
3
1
R
1F
2
1
C
2
C
2
R
Ω
1
Ω
1
+
−
C( )
u
t
Fig. P2-10
Problem 2-11. The circuit is shown in 󳶳Figure P2-11, try to write out the differential
equations between u1(t), u2(t) and i(t).
)
(t
i
2Ω
1F
2
2H
1( )
u t
2( )
u t
2Ω
+
−
+
−
Fig. P2-11
Problem 2-12. Plot block diagram models of the following systems.
(1) y󸀠󸀠(t) + 7y󸀠(t) + 12y(t) = f(t)
(2) y󸀠󸀠󸀠(t) + 4y󸀠󸀠(t) + 10y󸀠(t) + 3y(t) = f 󸀠󸀠(t) + 10f(t)
Authenticated
29 PM

3 Analysis of continuous-time systems in the time
domain
Questions: In order to analyze the relationship between the excitation and response
of a continuous LTI system, we need to solve the system model, that is, the differ-
ential equation in the time domain.
Solution: (1) Use classical methods of advanced mathematics.
(2) Introduce the operator to simplify the solution process and to provide support
for solving from impulse response.
(3) Consider a basic signal (impulse signal or step signal) as the input →Obtain
the solution (impulse response or step response) →Find the relationship be-
tween the basic signal and other signals →Use linearity to obtain system re-
sponse when other signals serve as the excitation.
Results: Response decomposition, impulse response, step response and transport
operator.
The system analysis process is generally divided into three stages, as shown in
󳶳Figure 3.1.
(1) Establish the system model. Write the mathematical expression that can show the
relationship between the input and the output signals of a system. Usually, the
model of a continuous system is a differential equation, whereas the model of a
discrete system is a difference equation.
(2) Solve the system model. Analyze and solve the model using appropriate mathe-
matical methods; in short, solve the linear differential or difference equation.
(3) Analyze the results. Give the physical interpretations for the responses (equation
solutions) obtained in the time or frequency domain, enhance the understanding
level for the transfer or processing of a system to signals, and obtain the desired
conclusions from them.
Herein, only LTI systems will be discussed. The mathematical model for these systems
is an nth-order constant coefficient linear differential or difference equation. There-
fore,
Establish system model
Analyze obtained results
Solve system model
The first stage
The second stage
The third stage
Fig. 3.1: The process of system analysis.
https://doi.org/10.1515/9783110419535-003
Authenticated
29 PM

74
|
3 Analysis of continuous-time systems in the time domain
the system analysis is to establish and solve an nth-order constant coefficient linear dif-
ferential or difference equation, which is the model of a system, then to analyze the re-
sults.
For a continuous system, with respect to how to solve the differential equation, we
will successively introduce three basic methods in the time domain, frequency domain
and complex frequency domain. For a discrete system, with respect to how to solve the
difference equation, we will also introduce the corresponding two methods in the time
domain and the z domain.
After having studied this material, readers should know that these analysis meth-
ods are all based on two cornerstones: the decomposability of a signal and the linear-
ity and time-invariance of a system. In this chapter, we will present external analysis
methods of a continuous system in the time domain, that is, we will discuss the vari-
ation of the response of a system with time, or the time characteristics of a system, or
solving methods of system equations in the time domain.
3.1 Analysis methods with differential equations
3.1.1 The classical analysis method
Usually, “differential equation” refers to an equality containing derivative functions
of unknown functions. The maximum order in derivative terms is also the order of the
differential equation.
We studied the classical analysis method to solve differential equations in an ad-
vanced mathematics course and will present a brief review here. As we know, the
mathematical model of an LTI system is a constant coefficient linear differential equa-
tion, and its complete solution y(t) is composed of the homogeneous solution yc(t)
and the particular solution yp(t) in two parts, that is,
y(t) = yc(t) + yp(t) .
(3.1-1)
First, ﬁnd the homogeneous solution. For a constant coefficient differential equation
n
∑
i=0
aiy(i)(t) =
m
∑
j=0
bjf (j)(t) ,
(3.1-2)
it can be also written as
any(n)(t) + an−1y(n−1)(t) + ⋅⋅⋅+ a1y󸀠(t) + a0y(t)
= bmf (m)(t) + bm−1f (m−1)(t) + ⋅⋅⋅+ b1f 󸀠(t) + b0f(t)
When its right side equals zero, the solution is homogeneous. This means that the
solution satisﬁes a homogeneous equation
any(n)(t) + an−1y(n−1)(t) + ⋅⋅⋅+ a1y󸀠(t) + a0y(t) = 0 .
(3.1-3)
Authenticated
29 PM

3.1 Analysis methods with differential equations
|
75
Usually, the homogeneous solution is combined with several functions in the form
Ceλt.
Substituting the term Ceλt into equation (3.1-3), we have
anCλneλt + an−1Cλn−1eλt + ⋅⋅⋅+ a1Cλeλt + a0Ceλt = 0 ,
if C
̸= 0, so,
anλn + an−1λn−1 + ⋅⋅⋅+ a1λ + a0 = 0 .
(3.1-4)
Equation (3.1-4) is called the characteristic equation of the differential equation (3.1-
2); n roots λ1, λ2, . . . , λn of the characteristic equation are said to be the characteristic
roots, the natural frequencies or inherent frequencies of the differential equation.
If there are no repeated roots in (3.1-4), the homogeneous solution should be
yc(t) =
n
∑
i=1
cieλit = cneλnt + cn−1eλn−1t + ⋅⋅⋅+ c1eλ1t .
(3.1-5)
If there are repeated roots, the form of the homogeneous solution should be different.
Suppose λ1 is an r-repeated root, that is, λ1 = λ2 = ⋅⋅⋅= λr, then n −r rest roots are
simple, so the homogeneous solution becomes
yc(t) =
r
∑
i=1
citr−ieλit +
n
∑
j=r+1
cjeλjt
= c1tr−1eλ1t + c2tr−2eλ2t + ⋅⋅⋅+ creλrt + cr+1eλr+1t + ⋅⋅⋅+ cneλnt .
(3.1-6)
Second, ﬁnd the particular solution. The form of the particular solution is related to
the form of excitation, several typical excitation signals f(t) and their corresponding
particular solutions yp(t) are listed in Table 3.1.
After each particular solution yp(t) in Table 3.1 is substituted into the original dif-
ferential equation, according to the criterion that the corresponding coefficients on
both sides of an equation are the same, the unknown coefficients p and B in yp(t) can
be determined.
Tab. 3.1: Typical excitation signals and their particular solutions.
No.
Excitation f(t)
Particular solution yp(t)
1
tm
pmtm + pm−1tm−1 + ⋅⋅⋅+ p0
2
eαt
peαt (α is not the characteristic root)
∑r
i=0 pitieαt (α is the r-repeated roots)
3
cos βt
p1 cos βt + p2 sin βt
4
sin βt
p1 cos βt + p2 sin βt
5
A (constant)
B (constant)
Authenticated
29 PM

76
|
3 Analysis of continuous-time systems in the time domain
Finally, n unknown coefficients ci (i = 1, 2, . . . n) in the homogeneous solution
need to be determined. Now we only need to substitute n values of initial condition of
the system into the total solution to obtain the coefficients. What is the initial condi-
tion?
Similarly to the concepts of initial state and starting state in Chapter 2, because of
the effect and the inﬂuence of the switching of circuit, the response and its all-order
derivatives may change instantaneously at time t0 = 0. To distinguish between their
values before and after the step occurs, we use “0−” to represent the moment before
the excitation is accessed or the switch is activated (the starting moment), and we use
“0+” to represent the moment after the excitation is accessed or the switch activated
(the initial moment). Hence, we can say that the starting condition of a system refers
to the values of system response and its all-order derivatives at time 0−, which can be
expressed as {y(i)(0−), i = 0, 1, . . . n −1}. While the initial condition of the system is
the values of system response and its all-order derivatives at time 0+, which can be
expressed as {y(i)(0+), i = 0, 1, . . . n −1}.
Usually, the response of system is the output of the system after the system is
driven by an excitation, that is, the response exists over a range 0+ ≤t < +∞. There-
fore, we can work out the unknown coefficients in the homogeneous solution using
the initial condition of the system.
Note: The starting condition is different from the starting state, and the initial con-
dition is also different from the initial state. The state of a system refers to the situa-
tions or data of the energy storage of the system. Because energy storage elements
in the electric system are generally inductors and capacitors, the energy storage sit-
uations can be reﬂected by the currents through inductors and the voltages across
capacitors. Therefore, in an electronic system, the starting state refers to the current
values through inductors and the voltage values across capacitors at the starting mo-
ment. For example, with iL(0−) and uC(0−). The initial state refers to the current values
through inductors and the voltage values across capacitors at the initial moment. For
example, iL(0+) and uC(0+).
The responses of a system are not always capacitor voltages and inductor currents
but may be the terminal voltage across or the current through a resistor or other pa-
rameters, hence, the values of response and the all-order derivatives at the starting
or initial moment {y(i)(0−), i = 0, 1, . . . n −1} or {y(i)(0+), i = 0, 1, . . . n −1} cannot
be directly expressed by the starting or the initial state. However, according to the
concepts of initial condition and initial state, we know that the initial state should be
included by the initial condition, and the initial state is deﬁnitely the initial condition,
whereas the initial condition is not always the initial state but can be calculated from
the initial state. Similarly, the starting state is included by the starting condition and
is certainly the starting condition, but not vice versa; the starting condition can be
calculated based on the starting state. Therefore, in an electronic system, the initial
condition or starting condition usually refers to including inductor currents and ca-
pacitor voltages and values of other circuit parameters or variables (may include their
nth-order derivatives) caused by them at the initial or starting moment.
Authenticated
29 PM

3.1 Analysis methods with differential equations
|
77
0
0−
0+
Starting moment
Initial moment
Starting state
Initial state
( )(0 )
i
x
−
( )(0 )
i
x
+
t
( )(0 )
i
y
−
( )(0 )
i
y
+
Starting condition
Initial condition
Fig. 3.2: Starting/initial moment and state/condition.
Because the two conditions can deﬁnitely be calculated from the two states, we
often equate the condition and the state. Moreover, since the value of the state gen-
erally cannot jump, this means the inductor current and the capacitor voltage cannot
change suddenly, so the starting state is equal to the initial state, i.e. iL(0−) = iL(0+)
and uC(0−) = uC(0+). However, we must note that the starting condition is not always
equal to the initial condition. 󳶳Figure 3.2 shows the starting and initial moments and
the corresponding states and conditions.
Example 3.1-1. An LTI system model is
d2
dt2 y(t) + 3 d
dt y(t) + 2y(t) = d
dt f(t) + 2f(t) .
If an excitation f(t) = t2 and the initial condition y(0+) = 1, y󸀠(0+) = 1, work out the
complete response of the system.
Solution. The homogeneous equation of the system is
d2
dt2 y(t) + 3 d
dt y(t) + 2y(t) = 0 .
Its characteristic equation is
λ2 + 3λ + 2 = 0 .
The characteristic roots are
λ1 = −1,
λ2 = −2 ,
so, the homogeneous solution is
yc(t) = c1e−t + c2e−2t .
Consider f(t) = t2; the particular solution can be set as
yp(t) = p2t2 + p1t + p0 .
Substituting the above expression and f(t) = t2 into the system model, we have
2p2t2 + (2p1 + 6p2) t + (2p0 + 3p1 + 2p2) = 2t2 + 2t ,
According to the balance equation rule, then
{
{
{
{
{
{
{
2p2 = 2
2p1 + 6p2 = 2
2p0 + 3p1 + 2p2 = 0
,
Authenticated
29 PM

78
|
3 Analysis of continuous-time systems in the time domain
and we obtain
p2 = 1,
p1 = −2,
p0 = 2 .
Thus, the particular solution
yp(t) = t2 −2t + 2 .
The complete solution
y(t) = c1e−t + c2e−2t + t2 −2t + 2 .
Substituting the initial condition y(0+) = 1; and y󸀠(0+) = 1 into the above expression,
{
{
{
c1 + c2 + 2 = 1
−c1 −2c2 −2 = 1
,
thus,
c1 = 1,
c2 = −2 .
The complete response is
y(t) = e−t −2e−2t + t2 −2t + 2
t ≥0 .
3.1.2 Response decomposition analysis method
We know that the complete solution of a linear constant coefficient differential equa-
tion can be decomposed into homogeneous and particular solutions. According to
different standards, the complete response of a system can be also decomposed into
other kinds of responses, such as zero-input and zero-state responses, transient and
steady state responses, or natural and forced responses. Thus, besides the classical
methods, other ways can be provided to solve the system model, for example, the
zero-input/zero-state response method in the following, which has also been called
the response decomposition method or the modern analysis method.
From Chapter 2, the complete response of an LTI system can be decomposed into
zero-input response and zero-state response, that is,
y(t) = yx(t) + yf(t) ,
(3.1-7)
where
yx(t) = T[x1(0−), x2(0−), . . . xn(0−), 0] = T[{x(0−)}, 0] ,
(3.1-8)
yf(t) = T[0, f1(t), f2(t), . . . , fn(t)] = T[0, {f(t)}] .
(3.1-9)
Because the starting state {x(0−)} can be equivalent to n excitation sources, the com-
plete response can be considered as the combined effects of the external excita-
tion source f(t) and the internal equivalent excitation sources {x(0−)}. Accordingly, a
Authenticated
29 PM

3.1 Analysis methods with differential equations
|
79
C
C( )
u
t
C( )
i
t
C
C( )
u
t
C( )
i
t
C(0 )
u
−
Capacitor circuit
Equivalent circuit
L
L( )
u
t
L( )
i t
L(0 )
i
−
Inductor circuit
Equivalent circuit
L
L( )
u
t
L( )
i t
(a)
(b)
(c)
(d)
Fig. 3.3: Capacitor and inductor circuits and their equivalent circuits.
method to analyze a system, that is, the response decomposition method has been
put forward.
There are two steps in the response decomposition analysis method. First, ﬁnd
the zero-input response and the zero-state response of a system; second, add them
together to obtain the complete response.
To use this method, we should ﬁrst research the voltage response uC(t) on a ca-
pacitor and the current response iL(t) through an inductor over the range [−∞, t] for
t ≥0.
For the capacitor C in 󳶳Figure 3.3a, suppose the starting voltage is uC(0−). Then
the response uC(t) can be written as
uC(t) = 1
C
t
∫
−∞
iC(τ)dτ = 1
C
0−
∫
−∞
iC(τ)dτ + 1
C
0+
∫
0−
iC(τ)dτ + 1
C
t
∫
0+
iC(τ)dτ
= uC (0−) + 0 + 1
C
t
∫
0+
iC(τ)dτ = uC (0−) + 1
C
t
∫
0+
iC(τ)dτ(t ≥0) .
Over the range [−∞, t] a capacitor with the nonzero starting voltage uC(0−) can be con-
sidered as a series form of a capacitor with zero starting voltage and a voltage source
uC(0−), which is shown in 󳶳Figure 3.3b.
Similarly, the analysis results of an inductor current response can be obtained.
For the inductor L with the starting current iL(0−) shown in 󳶳Figure 3.3c, iL(t) can be
written as
iL(t) = 1
L
t
∫
−∞
uL(τ)dτ = 1
L
0−
∫
−∞
uL(τ)dτ + 1
L
0+
∫
0−
uL(τ)dτ + 1
L
t
∫
0+
uL(τ)dτ
= iL (0−) + 0 + 1
L
t
∫
0+
uL(τ)dτ = iL (0−) + 1
L
t
∫
0+
uL(τ)dτ(t ≥0) .
It can be seen that over the range [−∞, t], an inductor L with the nonzero starting
current iL(0−) can be considered as the parallel form of an inductor with zero starting
current and a current source iL(0−), which is shown in 󳶳Figure 3.3d.
Obviously, both uC(0−) and iL(0−) belong to the zero-input response, whereas
1
C ∫
t
0+ iC(τ)dτ and 1
L ∫
t
0+ uL(τ)dτ are both the zero-state response.
Authenticated
29 PM

80
|
3 Analysis of continuous-time systems in the time domain
Based on the above response decomposition concepts of two energy storage el-
ements, a method has been developed to ﬁnd the zero-input and the zero-state re-
sponses of an LTI system.
1. Find the zero-input response yx(t)
Assume that the right side of the equation (3.1-2) is zero
n
∑
i=0
aiy(i)(t) = 0 ,
(3.1-10)
then the zero-input response is only caused by the starting state of the system for t ≥0
under f(t) and its all-order derivative values are zero, and it meets equation (3.1-10) and
has the same form as the homogeneous solution. In other words, its form is just like
equation (3.1-5) or equation (3.1-6).
Assuming that the characteristic roots are dissimilar simple roots, the zero-input
response is
yx(t) =
n
∑
i=1
cxi eλit
t ≥0 ,
(3.1-11)
wherecoefficients cxi(i = 1, 2, . . . , n)aredetermined by theinitialcondition{y(k)
x (0+)}
of the zero-input response.
According to the linearity concept of the LTI system, the initial condition y(k)(0+)
of the complete response (actually the initial condition of the system) is the sum of the
initial condition y(k)
x (0+) of the zero-input response and the initial condition y(k)
f (0+)
of the zero-state response, that is, y(k)(0+) = y(k)
x (0+) + y(k)
f (0+). Under the zero-input
condition, the equation is considered to exist over the range −∞< t < ∞; thus the
starting condition y(k)(0−) of the complete response is actually equal to the initial con-
dition y(k)
x (0+) of the zero-input response, and at this moment, y(k)(0−) = y(k)
x (0+) =
y(k)
x (0−). This shows that the coefficient cxi can be determined by the starting condi-
tion {y(k)(0−)} of the complete response. Note that y(k)(0+) cannot usually be used to
determine cxi, because it may include the effects of excitations.
Example 3.1-2. The homogeneous equation of an LTI system is y󸀠󸀠(t)+2y󸀠(t)+2y(t) =
0, and the system starting state values are y(0−) = 0 ,
y󸀠(0−) = 2. Try to ﬁnd the
zero-input response of the system.
Solution. The characteristic equation of the system is
λ2 + 2λ + 2 = 0 ,
its characteristic roots are
λ1 = −1 + j
λ2 = −1 −j ,
Authenticated
29 PM

3.1 Analysis methods with differential equations
|
81
and the zero-input response is
yx(t) = c1e(−1+j)t + c2e(−1−j)t .
Using Euler’s relations, the homogeneous solution can be transformed into trigono-
metric form
yx(t) = c1e(−1+j)t + c2e(−1−j)t
= e−t(c1 cos t + jc1 sin t + c2 cos t −jc2 sin t)
= e−t [(c1 + c2) cos t + j(c1 −c2) sin t]
= e−t(A1 cos t + A2 sin t) .
From the starting state values y(0−) = 0 and y󸀠(0−) = 2, we have
{
{
{
A1 = 0
A2 = 2 ,
so, the zero-input response is
yx(t) = 2e−t sin t
t ≥0 .
2. Find the zero-state response yf(t)
The zero state refers to the system not having any energy storage before the excitation
is applied. Therefore, the system response for t ≥0 can only be caused by the exci-
tation applied when t ≥0. At this moment, the system model is a nonhomogeneous
differential equation. Obviously, the zero-state response should satisfy this equation,
that is,
n
∑
i=0
aiy(i)
f (t) =
m
∑
j=0
bjf (j)(t)
(3.1-12)
In order to solve equation (3.1-12), we must also know a set of data {y(k)
f (0+)}, which is
the initial condition of the zero-state response. Note that {y(k)
f (0+)} relate to the excita-
tion f(t) applied to the system at t = 0, so the zero state may not mean {y(k)
f (0+)} = 0. In
fact, under the zero-state condition, we have y(k)
f (0+) = y(k)(0+) −y(k)(0−) = y(k)(0+),
and therefore, y(k)
f (0+) is also called the step change value of the system response.
Supposing that the characteristic roots λ1, λ2, . . . , λn of the system expressed by
equation (3.1-12) are simple and dissimilar, the homogeneous solution of the zero-state
response is
yfc(t) =
n
∑
i=1
cfieλit .
(3.1-13)
The method for the solution of the zero-state response is the same the classical
method, so
yf(t) = yfc(t) + yp(t) =
n
∑
i=1
cfi eλit + yp(t) .
(3.1-14)
Authenticated
29 PM

82
|
3 Analysis of continuous-time systems in the time domain
The coefficients cfi in Equations (3.1-13) and (3.1-14) can be determined by the data
{y(k)
f (0+)} via equation (3.1-14). In real system analysis, the starting state values at the
moment 0−are always known, so {y(k)
f (0+)} should be determined by the values. Two
methods are usually employed to solve this problem; one is the impulse function bal-
ance method, and the other one is the model analysis method based on some physical
concepts like the Law of Switching.
The fundamental principles of the impulse function balance method are the fol-
lowing:
(1) The differential equation to describe a system must be tenable over a range
(−∞, ∞). However, because the derivative of a function at a step point is usu-
ally inexistent, the differential equation formed by this function does not hold
over the range (−∞, ∞). The impulse function introduced in Chapter 1 can solve
the problem of the derivative existing at a step point, that is, δ(t) =
dε(t)
dt , so
that a differential equation with the step change can be tenable over the interval
(−∞, ∞).
(2) If an impulse component in the excitation leads to the emergence of the impulse
function and its all-order derivatives on the right side of the differential equation,
the left side of the equation should also have the corresponding impulse signal
and its all-order derivatives to make the equations tenable. Obviously, the balanc-
ing refers to generating the corresponding function terms on the left side of the
equation to terms on the right side. However, the production of these function
terms means that for some terms in y(k)(t) a step change must occur at t = 0.
In short, two key points of the method are:
(1) The balance refers to that δ(t) and its all-order derivative terms which appear on
both sides of a differential equation are the same order and have the same number
of terms.
(2) If there is no impulse function on the right side of the differential equation, a step
change will not occur in y(t) at t = 0.
Herein, we will mainly discuss the circuit model analysis method based on the Law of
Switching. In order to determine the initial condition, we need to use the internal en-
ergy storage continuity features of the system, which are the electric charge continuity
across a capacitor and the ﬂux linkage continuity through an inductor. Suppose that
a capacitor current iC(t) and an inductor voltage uL(t) are bounded over an interval
0−≤t ≤0+, then
uC(0+) −uC(0−) = 1
C
0+
∫
0−
iC(t)dt = 0
iL(0−) −iL(0+) = 1
L
0+
∫
0−
uL(t)dt = 0 ,
Authenticated
29 PM

3.1 Analysis methods with differential equations
|
83
Namely,
uC(0+) = uC(0−) ,
(3.1-15)
iL(0+) = iL(0−) .
(3.1-16)
Equations (3.1-15) and (3.1-16) show that if the capacitor current values are limited, the
voltage uC(t) is continuous at t = 0, and if the inductor voltage values are limited, the
current iL(t) is continuous at t = 0. In short, neither the capacitor voltage nor the
inductor current can change instantaneously. This conclusion is called the Law of
Switching, which we have learned in the circuits analysis course.
The following example will illustrate how can determine the zero-input and zero-
state responses using the Law of Switching.
Example 3.1-3. If the circuit pictured in 󳶳Figure 3.4a is in steady state, and the switch
S is closed quickly at t = 0, ﬁnd the zero-input response uCx(t) and zero-state response
uCf(t) of uC(t) for t ≥0+.
Solution. When the switch S is closed, the equivalent circuit of the system is as shown
in 󳶳Figure 3.4b.Accordingly, the differential equations of the equivalent circuit can be
written by the V-A relations of components and KCL, and
iC(t) = C duC(t)
dt
= 0.2duC(t)
dt
,
iR(t) = uC(t)
R
= uC(t) ,
iL(t) = iC(t) + iR(t) = 0.2duC(t)
dt
+ uC(t) ,
uL(t) = L diL(t)
dt
= 1.25 d
dt [0.2duC(t)
dt
+ uC(t)] = 0.25d2uC(t)
dt2
+ 1.25duC(t)
dt
.
According to KVL, we obtain
uL(t) + uC(t) = uS(t) = 2 ,
namely,
0.25d2uC(t)
dt2
+ 1.25duC(t)
dt
+ uC(t) = 2 .
After rearranging the expression above, we obtain
d2uC(t)
dt2
+ 5duC(t)
dt
+ 4uC(t) = 8 .
The characteristic equation can be written as
λ2 + 5λ + 4 = 0 .
Thus, the characteristic roots are
λ1 = −1, λ2 = −4 .
Authenticated
29 PM

84
|
3 Analysis of continuous-time systems in the time domain
C
L
S
)
(t
iL
H
.25
1
S( )
U t
V
2
1
R
Ω
2
2
R
Ω
2
F
.2
0
)
(t
uC
C
L
H
.25
1
V
2
R
Ω
1
F
.2
0
)
(t
uC
)
(t
iC
)
(t
iR
−
+
)
(t
uL
V
2
R
Ω
1
A
1
)
0
(
+
Li
−
+
+ )
( 0
L
u
)
0
(
+
Ri
i
)
(
+0
C
)
0
(
+
C
u
R
Ω
1
A
1
i
)
(
+0
x
R
)
(
+0
x
Li
−
+
+ )
( 0
x
L
u
)
(
+0
x
Ci
)
(
+0
x
C
u
R
Ω
1
)
(
+0
f
Li
V
2
−
+
+ )
( 0
f
L
u
)
(
+0
f
Ri
)
(
+0
f
Ci
)
(
+0
f
C
u
)
(t
iL
S( )
U t
S( )
U t
S( )
U t
(a)
(b)
(c)
(d)
(e)
Fig. 3.4: E3.1-4.
The zero-input response is assumed as
uCx(t) = cx1e−t + cx2e−4t .
(3.1-17)
The zero-state response of homogeneous solution is assumed as
uCfc(t) = cf1e−t + cf2e−4t .
It is easy to obtain the solution of zero-state response
uCfp(t) = 2 .
Then the zero-state response is
uCf(t) = cf1e−t + cf2e−4t + 2 .
(3.1-18)
Authenticated
29 PM

3.1 Analysis methods with differential equations
|
85
To obtain coefficients cx1, cx2, cf1, cf2, we need to ﬁnd uCx(0+), u󸀠
Cx(0+) and uCf(0+),
u󸀠
Cf(0+) using the Law of Switching. 󳶳Figure 3.4a shows that the circuit is powered by
a constant excitation and is stable when t = 0−; the inductor is a short circuit, and the
capacitor as an open circuit, so,
iL(0−) = uS
R1
= 1 A,
uC(0−) = 0 V .
According to the Law of Switching, at t = 0+, we have
iL(0+) = iL(0−) = 1 A,
uC(0+) = uC(0−) = 0 V .
The equivalent circuit is shown in 󳶳Figure 3.4c when t = 0+. Therefore, we can draw
the equivalent circuits of zero input and zero state as in 󳶳Figure 3.4d and e.
From 󳶳Figure 3.4d, we have uCx(0+) = 0 V and iCx(0+) = 1 A. Because iCx(0+) =
Cu󸀠
Cx(0+), u󸀠
Cx(0+) = 1
C iCx(0+) = 5 V/s. Thus, the initial conditions of the zero-input
response are
{
{
{
uCx(0+) = 0 V
u󸀠
Cx(0+) = 5 V/s
,
Putting the initial conditions into (3.1-17), we obtain cx1 = 5/3, cx2 = −5/3. Thus, the
zero-input response is
uCx(t) = 5
3 e−t −5
3e−4t
t ≥0 .
We ﬁnd uCf(0+) = 0 V, iCf(0+) = 0 A from 󳶳Figure 3.4e. Because iCf(0+) = Cu󸀠
Cf(0+),
u󸀠
Cf(0+) = 1
C iCf(0+) = 0 V/s. Thus, the initial conditions of the zero-state response are
uCf(0+) = 0 V
u󸀠
Cf(0+) = 0 V/s
Putting the initial conditions into (3.1-18), we have cf1 = −8/3, cf2 = 2/3. Thus, the
zero-state response is
uCf(t) = −8
3 e−t + 2
3 e−4t + 2 V
t ≥0 .
3. Find the complete response y(t)
From the zero-input response and zero-state response, the complete response can be
y(t) = yx(t) + yf(t) =
⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟
natural
response
zero input
response
⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞
n
∑
i=1
cxieλit +
zero state
response
⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞
n
∑
i=1
cfieλit + yp(t)
⏟⏟⏟⏟⏟⏟⏟⏟⏟
forced
response
(3.1-19)
=
n
∑
i=1
cieλit
⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟⏟
natural
response
+ yp(t)
⏟⏟⏟⏟⏟⏟⏟⏟⏟
forced
response
(3.1-20)
Authenticated
29 PM

86
|
3 Analysis of continuous-time systems in the time domain
Accordingto thetwo expressions above and features of theresponse, thecomplete
response can be expressed as follows:
(1) Complete response = zero-input response + zero-state response.
(2) Complete response = natural response + forced response.
(3) Complete response = transient response + steady state response.
Usually, the exponential functions in the response that ﬁt the form aeλt can be called
natural type terms, so the component composed of all natural type terms in the re-
sponse can be called the natural response or the free response. The form of natural
response is determined by the characteristic roots of the system. Aside from the nat-
ural response, those terms whose form is determined by an excitation are called the
forced response. Therefore, the homogeneous solution is the natural response and the
particular solution is the forced response in the classical method.
It can be seen from Equations (3.1-19) and (3.1-20) that:
(1) The natural response can be decomposed into two parts; one being caused by the
starting state, and another being generated by the excitation.
(2) Both the natural response and zero-input response are solutions that can satisfy
the homogeneous equation, but their coefficients are different. The cxi is only de-
termined by the starting state of the system, but ci is dependent on the starting
state and the excitation when t = 0.
(3) The natural response includes the whole zero-input response and a part of the
zero-state response, which is the homogeneous solution in the zero-state re-
sponse. For a stable system, the zero-input response must be a part of the natural
response.
According to the variation characteristic of a response with time, the complete re-
sponse can be divided into transient response and steady state response. The part last
decreases to zero with time increase is called the transient response, and the part that
becomes a constant or an oscillation term with time increase is called the steady state
response (this often includes step signals and periodic signals). Usually, transient re-
sponse may include the transient components in the forced response.
For a steady system, the characteristic roots that are also known as the free fre-
quencies are all negative, so all the natural terms will tend to zero with time increase,
and the natural response must be the transient response. For an unstable system, if a
root is a plural or a positive value, the natural term composed of this root will not tend
to zero with time increase, so the natural response will not be transient.
To enable a better understanding of the above, 󳶳Figure 3.5 shows the relation-
ships between the complete response and each sub response.
Note that the zero-state response is generated by a zero-state system, which is
accessed with an excitation at t = 0, so we always multiply the zero-state response
yf(t) by ε(t) or mark a time start t ≥0 behind yf(t), in order to show that yf(t) exists
from t = 0. The mark t ≥0 can be considered as t ≥0+ here.
Authenticated
29 PM

3.1 Analysis methods with differential equations
|
87
 Complete response
Zero input response
Zero state response
Free response
Forced response
Transient response 
Steady state response
Fig. 3.5: Complete response and its composi-
tion diagram.
Example 3.1-4. The input-output equation of a system is y󸀠(t) + 3y(t) = 3ε(t), and
the starting state is y(0−) =
3
2. Find the natural response, the forced response, the
zero-input response and the zero-state response.
Solution. Firstly, ﬁnd the natural response and the forced response. The characteris-
tic equation is
λ + 3 = 0 ,
and the characteristic root is
λ = −3 ,
so, the homogeneous solution is
yc(t) = c1e−3t .
Suppose the particular solution is yp(t) = A and substitute it into the system equation;
then A = 1 can be found. Thus,
yp(t) = 1 ,
and the complete solution is
y(t) = c1e−3t + 1 .
(3.1-21)
Because y(t) is continuous at t = 0 from the impulse signal balance, we obtain
y (0−) = y (0+) = 3
2 .
Substituting the initial condition into (3.1-21), we have
c1 = 1
2 ,
and, therefore, the natural response and the forced response are, respectively,
yc(t) = 1
2 e−3t,
yp(t) = 1 .
Authenticated
29 PM

88
|
3 Analysis of continuous-time systems in the time domain
Supposing the zero-input response and the zero-state response are, respectively,
yx(t) = c2e−3t,
yf(t) = c3e−3t + 1 ,
and because yx (0+) = y (0−) = 3
2, yf (0+) = yf (0−) = 0 are known, we obtain
c2 = 3
2,
c3 = −1 .
Thus, the zero-input response and the zero-state response are,
yx(t) = 3
2e−3t
t ≥0 ,
yf(t) = 1 −e−3t
t ≥0 .
3.2 Impulse and step responses
The ultimate objective of system analysis is to ﬁnd the response of a system to any
excitation, and to achieve it we expect to seek a simple and general approach. Fortu-
nately, theimpulseresponseand thestep response, whicharethe zero-stateresponses
produced by a system to impulse and step signals, can help us to make this possible.
3.2.1 Impulse response
The zero-state response generated by a system to a unit impulse signal δ(t) is called the
impulse response and is denoted as h(t).
If y(t) and f(t) in the system model represented by Equations (2.4-7) or (3.1-2) are,
respectively, replaced by h(t) and δ(t), then
n
∑
i=0
aih(i)(t) =
m
∑
j=0
bjδ(j)(t) .
(3.2-1)
Because values of δ(t) are zero for t > 0, h(t) and the homogeneous solution of the
differential equation must have the same form. If the equation has n different charac-
teristic roots such as λ1, λ2, . . . , λn, the form of the impulse response should be
h(t) =
n
∑
i=1
cieλitε(t) .
(3.2-2)
Substituting equation (3.2-2) into equation (3.2-1), it can be seen that the impulse
function and its all-order derivatives are on the right side of the equation, and the
highest-order derivative is δ(m)(t). Obviously, to balance all the corresponding singu-
lar functions on both sides of equation (3.2-1), the left side should also include the
δ(m)(t), δ(m−1)(t), . . . , δ(t) terms. Because the highest-order term on the left side of
Authenticated
29 PM

3.2 Impulse and step responses
|
89
the equation is h(n)(t), it should include the δ(m)(t) term, at least. We can see that the
form of h(t) relates to n and m. By comparing the highest-order m and n for both sides
(the corresponding excitation and response, respectively) of the equation, h(t) can be
expressed in the following three forms:
(1) n > m
If h(t) includes δ(t) term, the h(n)(t) on the left side of equation (3.2-1) will contain
δ(n)(t). However, the highest-order derivative term is δ(m)(t) on the right side of
the equation here, and the singular functions on both sides of the equation do
not match, so it not possible to obtain the coefficients of all the terms. Therefore,
h(t) cannot contain the δ(t) term and can be expressed as
h(t) =
n
∑
i=1
cieλitε(t) .
(3.2-3)
(2) n = m
If h(t) includes the derivative terms of δ(t), the derivative order of δ(t) on the left
side will be higher than that on the right side. Thus, here h(t) must include the
impulse function δ(t) itself rather than its all-order derivatives at this moment,
that is
h(t) =
n
∑
i=1
cieλitε(t) + Bδ(t) .
(3.2-4)
(3) n < m
Here, h(t) contains not only the impulse function term but also its derivative
terms. Supposing n + 1 = m, then
h(t) =
n
∑
i=1
cieλitε(t) + Aδ󸀠(t) + Bδ(t) .
(3.2-5)
We also structure this form of h(t) in order to substitute it into equation (3.2-1) and
make the corresponding singular functions match each other on both sides of the
equation.
The undetermined coefficients, such as A, B and ci in Equations (3.2-3), (3.2-4) and
(3.2-5), should be substituted into equation (3.2-1) and be determined by the principle
that the coefficients of each corresponding singular function must be equal to each
other on both sides.
Example 3.2-1. The differential equation of a second-order continuous system is
d2
dt2 y(t) + 4 d
dt y(t) + 3y(t) = d
dt f(t) + 2f(t)
Find the impulse response h(t) of this system.
Solution. Considering f(t) = δ(t) and the zero-state condition, we know that y(t) =
h(t), so,
h󸀠󸀠(t) + 4h󸀠(t) + 3h(t) = δ󸀠(t) + 2δ(t) ,
(3.2-6)
Authenticated
29 PM

90
|
3 Analysis of continuous-time systems in the time domain
the characteristic equation is
λ2 + 4λ + 3 = 0 ,
and the characteristic roots are
λ1 = −1,
λ2 = −3 .
Because n > m, the impulse response is
h(t) = (c1e−t + c2e−3t) ε(t) .
(3.2-7)
Taking the derivative of equation (3.2-7), we get
h󸀠(t) = (c1 + c2) δ(t) −(c1e−t + 3c2e−3t) ε(t) ,
(3.2-8)
h󸀠󸀠(t) = (c1 + c2) δ󸀠(t) −(c1 + 3c2) δ(t) + (c1e−t + 9c2e−3t) ε(t) .
(3.2-9)
Substituting Equations (3.2-7), (3.2-8) and (3.2-9) into equation (3.2-6), we obtain
(c1 + c2) δ󸀠(t) + (3c1 + c2) δ(t) = δ󸀠(t) + 2δ(t) .
Comparing the coefficients on both sides of the above equation, we obtain
{
{
{
c1 + c2 = 1
3c1 + c2 = 2
,
and then we ﬁnd
c1 = 1
2, c2 = 1
2 .
The impulse response is
h(t) = 1
2 (e−t + e−3t) ε(t) .
3.2.2 Step response
The zero-state response generated by a system to a unit step signal ε(t) is called the step
response and is denoted as g(t).
If y(t) and f(t) in the system model are, respectively, replaced by g(t) and ε(t), we
obtain
n
∑
i=0
aig(i)(t) =
m
∑
j=0
bjε(j)(t) .
(3.2-10)
Since δ(t) is the derivative of ε(t), the right side of equation (3.2-10) is a constant when
t ≥0+, and then
n
∑
i=0
aig(i)(t) = b0
t ≥0+ .
(3.2-11)
Authenticated
29 PM

3.2 Impulse and step responses
|
91
Obviously, the step response g(t) should have the same form as the nonhomogeneous
solution, which is composed of the homogeneous solution gc(t) and the particular
solution gp(t). However, the form of the solution should be a constant, which can be
assumed as Bε(t), and after it is substituted into equation (3.2-11) we obtain B = b0
a0 , so
the solution is
gp(t) = b0
a0
ε(t) .
(3.2-12)
If n characteristic roots λ1, λ2, . . . , λn are different, then the homogeneous solution is
gc(t) =
n
∑
i=1
cieλitε(t) .
(3.2-13)
Adding equation (3.2-12) to equation (3.2-13), we ﬁnd the step response
g(t) =
n
∑
i=1
cieλitε(t) + b0
a0
ε(t) .
(3.2-14)
Similarly to the impulse response, at the moment t = 0, the step response may also
contain the impulse signal and its all-order derivatives. When n ≥m, g(t) will not
contain the impulse function term because the highest-order of derivatives for δ(t) is
m −1 on the right side of equation (3.2-10). If g(t) contains the impulse function term,
then the highest-order of derivatives for δ(t) is n on the left side of the equation, and
the coefficients on both sides of the singular functions obviously do not match. So, the
step response g(t) must be
g(t) =
n
∑
i=1
cieλitε(t) + b0
a0
ε(t) .
(3.2-15)
When n < m, g(t) will contain the impulse function δ(t) and its derivatives, but the
derivative order will depend on the result of m−n. If n+1 = m, then it will only contain
δ(t); if n + 2 = m, it will contain δ(t) and δ󸀠(t); and so on. The step response g(t) will
be given in the following, when n + 1 = m,
g(t) = Aδ(t) +
n
∑
i=1
cieλitε(t) + b0
a0
ε(t) .
(3.2-16)
All the undetermined coefficients A and ci in equation (3.2-16) can be substituted into
the original equation (3.2-10) and be obtained by the method that the coefficients of
each singular function on both sides should be equal.
Because we only discuss LTI systems, and the step and the impulse signals satisfy
a calculus relationship, the step and the impulse responses can also satisfy this rela-
tionship, that is,
if
ε(t)
System transformation
󳨀󳨀󳨀󳨀󳨀󳨀󳨀󳨀󳨀󳨀󳨀󳨀󳨀󳨀󳨀󳨀󳨀→g(t) ,
δ(t)
System transformation
󳨀󳨀󳨀󳨀󳨀󳨀󳨀󳨀󳨀󳨀󳨀󳨀󳨀󳨀󳨀󳨀󳨀→h(t) ,
Authenticated
29 PM

92
|
3 Analysis of continuous-time systems in the time domain
because
δ(t) = dε(t)
dt
or
ε(t) =
t
∫
−∞
δ(τ)dτ ,
there are
h(t) = dg(t)
dt
(3.2-17)
and
g(t) −g(−∞) =
t
∫
−∞
h(τ)dτ ,
(3.2-18)
For a causal system, because g(−∞) = 0, equation (3.2-18) can be written as
g(t) =
t
∫
−∞
h(τ)dτ .
(3.2-19)
Equation(3.2-17)or equation(3.2-19)shows animportantcharacteristic of anLTI causal
system, which can reveal the relationship between the above two important responses
of a system and plays an important role in the analysis of an LTI causal system.
Example 3.2-2. The input-output differential equation of an LTI system is
y󸀠󸀠(t) + 5y󸀠(t) + 6y(t) = 3f 󸀠(t) + f(t)
Find the step response and the impulse response of this system.
Solution. Considering f(t) = ε(t) and the zero-state condition, the equation can be
rewritten as
g󸀠󸀠(t) + 5g󸀠(t) + 6g(t) = 3ε󸀠(t) + ε(t) .
(3.2-20)
We can write the characteristic equation as
λ2 + 5λ + 6 = 0 ,
and obtain the characteristic roots
λ1 = −2,
λ2 = −3 .
Then, the homogeneous solution part in the step response is
gc(t) = (c1e−2t + c2e−3t) ε(t) ,
and the solution part in the step response is
gp(t) = b0
a0
ε(t) = 1
6 ε(t) .
Authenticated
29 PM

3.2 Impulse and step responses
|
93
Because n > m, the step response is
g(t) = (c1e−2t + c2e−3t) ε(t) + 1
6ε(t) .
The ﬁrst-order derivative is
g󸀠(t) = (c1 + c2 + 1
6) δ(t) −2c1e−2t −3c2e−3t ,
and the second-order derivative is
g󸀠󸀠(t) = (c1 + c2 + 1
6) δ󸀠(t) −(2c1 + 3c2) δ(t) + 4c1e−2tε(t) + 9c2e−3tε(t) .
Substituting above three equations into equation (3.2-20), we obtain
(c1 + c2 + 1
6) δ󸀠(t) + (3c1 + 2c2 + 5
6) δ(t) = 3δ(t) .
Comparing the coefficients on both sides of the equation, we have
{
{
{
c1 + c2 + 1
6 = 0
3c1 + 2c2 + 5
6 = 3
,
and we obtain
c1 = 5
2,
c2 = −8
3 .
The step response is
g(t) = (5
2 e−2t −8
3 e−3t) ε(t) + 1
6 ε(t) .
Taking the derivative of g(t), we obtain h(t) as
h(t) = (8e−3t −5e−2t) ε(t) .
Example 3.2-3. The i(t) is a response of the circuit shown in 󳶳Figure 3.6; ﬁnd the step
and impulse responses.
Solution. According to KCL and KVL laws,
i1(t) = i(t) + iC(t) ,
2i(t) = 2iC(t) + uC(t) ,
uC(t) = 1
C
t
∫
−∞
iC(τ)dτ ,
2i1(t) + 2i(t) = uS(t) .
S( )
u t
F
3
1
+
−
C( )
u
t
C( )
i
t
)
(t
i
)
(
1 t
i
Ω
2
Ω
2
Ω
2
Fig. 3.6: E3.2-3.
Authenticated
29 PM

94
|
3 Analysis of continuous-time systems in the time domain
The above four equations are considered as a set of equations, so i1(t), uC(t), iC(t)
can be eliminated. All element values are substituted into the equation set, and the
differential equation can be obtained,
i󸀠(t) + i(t) = 1
6u󸀠
S(t) + 1
4uS(t) .
Letting uS(t) = ε(t) and the system state be zero,
g󸀠(t) + g(t) = 1
6 δ(t) + 1
4 ε(t) .
According to the impulse function balance method, we obtain g(0+) = 1
6. To solve the
differential equation, the step response can be expressed as
g(t) = (ce−t + 1
4) ε(t) .
After substituting the initial condition into the equation, c = −1
12 can be found. The
step response is
g(t) = (−1
12e−t + 1
4) ε(t) .
Taking the derivative of the step response, we get
h(t) = 1
12 e−tε(t) + 1
6 δ(t) .
3.3 The operator analysis method
3.3.1 Differential and transfer operators
To facilitate solving the differential equations in the time domain, we will introduce
the differential operator as a new concept. The differential operator is a simpliﬁed
symbol for the differential operation, which can be represented by lower case p, that
is,
p = d
dt
(3.3-1)
or
1
p =
t
∫
−∞
( )dt .
(3.3-2)
Then
px = dx
dt ,
pnx = dnx
dtn ,
1
p x =
t
∫
−∞
xdt .
At the same time,
p 1
p x = d
dt
t
∫
−∞
xdt = x
(3.3-3)
Authenticated
29 PM

3.3 The operator analysis method
|
95
Equation (3.3-3) indicates that the function will remain the same if it is ﬁrst integrated
and then differentiated. In equation (3.3-3), two characters of p can be reduced as vari-
ables. Note: If a function is ﬁrst operated by the differential and then by the integral,
we have
1
p px =
t
∫
−∞
dx
dτ dτ = x(t) −x(−∞) ,
so,
1
p px
̸= x ,
which means that two p cannot be reduced as two variables here, unless x(−∞) = 0.
Using the differential operator, a linear constant coefficient differential equation
as an LTI system model,
an
dny(t)
dtn
+ an−1
dn−1y(t)
dtn−1
+ ⋅⋅⋅+ a1
dy(t)
dt
+ a0y(t)
= bm
dmf(t)
dtm
+ ⋅⋅⋅+ b1
df(t)
dt
+ b0f(t) ,
can be simpliﬁed as
(anpn + an−1pn−1 + ⋅⋅⋅+ a1p + a0) y(t)
= (bmpm + bm−1pm−1 + ⋅⋅⋅+ b1p + b0) f(t) .
(3.3-4)
Obviously, because of the introduction of the differential operator, solving to a
differential equation can be changed to solving to an operator equation or a pseudo-
algebraic equation. It has the same form and some properties of the algebraic equa-
tion but cannot be equivalent to the algebraic equation, so it is called the pseudo-
algebraic equation. As a result, we should learn some operational properties related
to the operator.
Property 1: A positive power polynomial of p can be operated by expansion and fac-
torization like an algebraic polynomial.
Property 2: The order/sequence of two polynomials before a signal can be ex-
changed, for example,
(p + 1) (p2 + 2p + 3) f(t) = (p2 + 2p + 3) (p + 1)f(t) .
Property 3: The common factors that include p on both sides of an operator equation
cannot be eliminated casually. For example, the equation py(t) = pf(t) is usually
shown as y(t) = f(t) + c instead of y(t) = f(t), where c is a constant.
Property 4: The order/sequence of multiplication and division operations for a sig-
nal cannot be randomly changed.
Conclusion: The variables in a differential equation set consisting of operators can
be reduced with Cramer’s rule like an algebraic equation set; this is the main pur-
pose of introducing the differential operator. In other words, the introduction of
Authenticated
29 PM

96
|
3 Analysis of continuous-time systems in the time domain
the differential operator can simplify a differential equation (or set) whose solu-
tion process is relatively complex into an algebraic equation (set) whose solution
process is relatively easy.
Equation (3.3-4) can be rearranged as
y(t) = bmpm + bm−1pm−1 + ⋅⋅⋅+ b1p + b0
anpn + an−1pn−1 + ⋅⋅⋅+ a1p + a0
f(t) = N(p)
D(p) f(t) ,
(3.3-5)
where
N(p) = bmpm + bm−1pm−1 + ⋅⋅⋅+ b1p + b0 ,
(3.3-6)
D(p) = anpn + an−1pn−1 + ⋅⋅⋅+ a1p + a0 ,
(3.3-7)
and D(p) is the characteristic polynomial of a differential equation (or system); D(p) =
0 is the characteristic equation.
The transfer operator can be extracted from the above content as another impor-
tant concept. The ratio of the response y(t) to the excitation f(t) in an operator equation
(model) is deﬁned as a transfer operator of the system, which is represented as H(p),
that is
H(p) = y(t)
f(t) = N(p)
D(p) .
(3.3-8)
The importance/purpose of constructing H(p) is to extract the content that is unre-
lated to the input and the output of system model but can show the system charac-
teristics, and facilitates the analysis of the system. This result is exactly the concrete
reﬂection of the linearity of the system.
From equation (3.3-8), we have
y(t) = H(p)f(t) .
(3.3-9)
The input f(t) seems to be transferred from the input end to the output end of the
system using H(p) as the medium, which is the reason for H(p) being named as the
transfer operator. So, once we know the transfer operator H(p) of a system, the system
modelcanbedetermined, and thesystem response y(t)to any input f(t)canalso beob-
tained by H(p). In addition, the structure of H(p) is the same as that of the system func-
tion H(jω) or H(s) to be introduced later, where H(s) = H(p)|p=s. or H(jω) = H(p)|p=jω.
So, the introduction of H(p) provides a new method to ﬁnd the system function (trans-
fer function) also.
Example 3.3-1. Find the transfer operator of the system in Example 2.4-2.
Solution. The mathematical model of the system in Example 2.4-2 is
d3u
dt3 + 2d2u
dt2 + 2du
dt + u = 1
2 us .
Authenticated
29 PM

3.3 The operator analysis method
|
97
The operator is substituted into the above equation, and then the operator equation
is found as
(p3 + 2p2 + 2p + 1)u = 1
2 us .
Then, the transfer operator is
H(p) =
1
2(p3 + 2p2 + 2p + 1) .
Example 3.3-2. For the system shown in 󳶳Figure 3.7, i2(t) and f(t) are, respectively,
the response and the excitation. Find the system model and the transfer operator.
Solution. According to KVL, we can write a set of loop equations
{
{
{
i1 + 3 di1
dt −di2
dt = f
3i2 + di2
dt −di1
dt = 0
,
and transfer it to a set of operator equations
{
{
{
(3p + 1)i1 −pi2 = f
−pi1 + (p + 3)i2 = 0
.
Then, using Cramer’s rule to eliminate the variables in the operator equations
i2 =
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨
3p + 1
f
−p
0
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨
3p + 1
−p
−p
p + 3
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨
=
p
2p2 + 10p + 3 f
and
(2p2 + 10p + 3) i2 = pf .
We can write the system model as
2d2i2
dt2 + 10di2
dt + 3i2 = df
dt ,
and then the transfer operator is
H(p) =
p
2p2 + 10p + 3 .
Ω
2
Ω
1
)
(
1 t
i
)
(
2 t
i
2H
1H
)
(t
f
Ω
1
Fig. 3.7: E3.3-2.
Authenticated
29 PM

98
|
3 Analysis of continuous-time systems in the time domain
Example 3.3-3. Knowing the transfer operator and the initial conditions of a system,
ﬁnd the corresponding zero-input response,
H(p) =
1
p2 + 2p + 5 ,
yx (0+) = 1,
y󸀠
x (0+) = 1 .
Solution. The characteristic equation is
λ2 + 2λ + 5 = 0 ,
and the characteristic roots are
λ1 = −1 + 2j ,
λ2 = −1 −2j ,
so, the zero-input response is
yx(t) = e−t [A1 cos(2t) + A2 sin(2t)] ,
and
y󸀠
x(t) = −e−t [A1 cos(2t) + A2 sin(2t)] + e−t [−2A1 sin(2t) + 2A2 cos(2t)] .
Substituting the initial values yx(0+) = 1 and y󸀠
x(0+) = 1 into the above equations, we
have
A1 = A2 = 1 .
So, the zero-input response is
yx(t) = e−t [cos(2t) + sin(2t)] = √2e−t sin (2t + π
4 ) ,
t ≥0 .
From the above examples, we can summarize two common methods to ﬁnd the trans-
fer operator:
(1) Using the system model namely the differential equation (or simulation block di-
agram).
(2) Using the circuit constraint conditions.
Another purpose of introducing the transfer operator is to solve the zero-state re-
sponse. The different transfer operators correspond to different impulse responses, so
we can obtain a lot of relevant formulas through mathematical deduction. The form
of a transfer operator is the same as that of a transfer function (system function), so
the methods and formulas for solving the zero-state response are also similar to those
of for the transfer function, and the details will not be given here.
Authenticated
29 PM

3.3 The operator analysis method
|
99
3.3.2 Determining impulse response by the transfer operator
The transfer operator deﬁned by equation (3.3-8) can be processed in partial fractions
H(p) =
q
∑
i=0
Kipi +
l
∑
j=1
Kj
(p −λj)rj
(3.3-10)
According to the different situations of equation (3.3-10), the corresponding different
impulse responses h(t) can be obtained by
(1)
H(p) =
K
p −λ
→
h(t) = Keλtε(t) .
(3.3-11)
(2)
H(p) =
K
(p −λ)2
→
h(t) = Kteλtε(t) .
(3.3-12)
(3)
H(p) =
K
(p −λ)r
→
h(t) =
K
(r −1)! tr−1eλtε(t) .
(3.3-13)
(4)
H(p) = Kpn
→
h(t) = Kδ(n)(t) .
(3.3-14)
This way, we can obtain general steps to ﬁnd the impulse response using the transfer
operator.
Step 1: Find the transfer operator H(p).
Step 2: Decompose H(p) into partial fraction form, such as equation (3.3-10).
Step 3: Determine each hi(t) corresponding to each fraction, according to the formu-
las in Equations (3.3-11) to (3.3-14).
Step 4: Adding all hi(t) together, the impulse response h(t) of the system can be ob-
tained.
Example 3.3-4. Find h(t) of the system described by the following equation:
y(3) + 5y(2) + 8y(1) + 4y = f (3) + 6f (2) + 10f (1) + 6f
Solution. The operator equation of the system is
(p3 + 5p2 + 8p + 4)y = (p3 + 6p2 + 10p + 6)f .
Based on the long division method, we have
H(p) = p3 + 6p2 + 10p + 6
p3 + 5p2 + 8p + 4 = 1 +
1
p + 1 −
2
(p + 2)2 .
According to Equations (3.3-11) to (3.3-14), we have
h(t) = δ(t) + e−tε(t) −2te−2tε(t) = δ(t) + (e−t −2te−2t)ε(t) .
Authenticated
29 PM

100
|
3 Analysis of continuous-time systems in the time domain
3.4 The convolution analysis method
What is the purpose of introducing the impulse response? It is only the zero-state re-
sponse of a system to a speciﬁc signal, namely, the impulse signal, and its solution
process is also more complex, so what is the meaning of solving the zero-state re-
sponses of a system to a nonperiodic signal? In fact, the purpose is exactly to ﬁnd
the zero-state responses of a system to a nonperiodic signal, and there may be eas-
ier methods to solve the impulse response itself. Next, we will give the convolution
analysis method for solving zero-state response.
The f1(t) and p(t) shown in 󳶳Figure 3.8 can be related by
f1(t) = A ⋅∆⋅p(t).
(3.4-1)
Now we will research the relation between any signal f(t) and p(t); ̂f(t) can be
considered as an approximate signal of f(t) in 󳶳Figure 3.9 and is composed of the sum
of some rectangular pulses as shown in 󳶳Figure 3.9; each pulse can be expressed by
using p(t).
The pulse 0 has height f(0) and duration time −∆
2 ∼∆
2 and can be expressed as
f(0)∆p(t).
The pulse 1 has height f(∆) and duration time ∆
2 ∼3∆
2 and can be expressed as
f(∆)∆p(t −∆). The pulse k has height f(k∆) and duration time (2k−1)∆
2
∼(2k+1)∆
2
and
A
)
(
1 t
f
t
2
Δ
2
Δ
−
0
t
2
Δ
2
Δ
−
0
Δ
1
)
(t
p
(a)
(b)
Fig. 3.8: f1(t) and p(t).
t
‘ k’
‘ 1’
‘ 0’
‘ -1’
0
)
(t
f
)
( Δ
−
f
)0
(
f
)
(Δ
f
p(t)
)
( Δ
k
f
2
Δ
−
2
Δ
2
3Δ
2
)1
2(
Δ
−
k
2
)1
2(
Δ
+
k
( )
f t
∧
Fig. 3.9: The relationship between arbitrary f(t) and p(t).
Authenticated
29 PM

3.4 The convolution analysis method
|
101
can be expressed as f(k∆)∆p(t −k∆). Hence,
f(t) ≅̂f(t) =
+∞
∑
k=−∞
f(k∆)∆p(t −k∆)
(k is an integer).
If the zero-state response is written as x(t) for the excitation p(t), then from the linear-
ity and time invariant properties, the zero-state response yf(t) of an LTI system to any
excitation f(t) can be approximated as
yf(t) ≅
+∞
∑
k=−∞
f(k∆)∆x(t −k∆)
(k is an integer) .
When ∆→0, ∆can be written as dτ, k∆as τ, p(t−k∆) as δ(t−τ). Here, the summation
notation can be written as the integral symbol
f(t) = lim
∆→0
+∞
∑
k=−∞
f(k∆)∆p(t −k∆) =
+∞
∫
−∞
f(τ)δ(t −τ)dτ ,
(3.4-2)
yf(t) = lim
∆→0
+∞
∑
k=−∞
f(k∆)∆x(t −k∆) =
+∞
∫
−∞
f(τ)h(t −τ)dτ .
(3.4-3)
Equation (3.4-2) shows that any signal f(t) can be decomposed into a continuous sum
of countless impulse signals located at different times and with different weights.
Moreover, equation (3.4-3) shows that the zero-state response yf(t) of a system can
also be also decomposed into a continuous sum of countless impulse responses. Ob-
viously, according to the convolution deﬁnition, the zero-state response yf(t) is equal
to the convolution integral of the excitation f(t) and the impulse response h(t); as a
result, equation (3.4-3) can be written as
yf(t) = h(t) ∗f(t) .
(3.4-4)
The above deduction process is described in 󳶳Figure 3.10.
Moreover, f(t) canbe decomposed into the continuous sum of impulse signals and
it can also be decomposed into the continuous sum of step signals, that is,
f(t) = lim
∆→0
∞
∑
k=−∞
f(k∆) −f((k −1)∆)
∆
⋅ε(t −k∆) ⋅∆.
(3.4-5)
When ∆→0, equation (3.4-5) can be reduced to
f(t) =
∞
∫
−∞
df(τ)
dτ ε(t −τ)dτ .
(3.4-6)
Equation (3.4-6) shows that any signal f(t) can be expressed as a continuous sum or
an integral of inﬁnite step signals. Equation (3.4-6) is also called Duhamel’s integral.
Authenticated
29 PM

102
|
3 Analysis of continuous-time systems in the time domain
A LTI system
(zero-state)
( )
(
)
( ) (
)
( )
( ) (
)d
t
t
f
t
f t
f
t
δ
δ
τ
τ δ
τ
τ δ
τ
τ
∞
−∞
−
−
=
−
∫
(Superposition)
(Homogeneous)
(Time invariant)
)
(
)
(
)
(
)
(
)
(
)
(
∫
∞
∞
−
−
=
−
−
τ
τ
τ
τ
τ
τ
d
t
h
f
t
y
t
h
f
t
h
(Definition)
f
Excitation 
Response
f ( )
( )
y t
h t
=
Fig. 3.10: The derivation process of the relationship between zero-state response and impulse re-
sponse.
From the relation between the impulse signal and the step signal, the zero-state
response yf(t) of a system can be written as
yf(t) = f(t) ∗h(t) = f 󸀠(t) ∗
t
∫
−∞
h(τ)dτ = f 󸀠(t) ∗g(t) .
When the unit step response of a system is known, the zero-state response can be
directly calculated by the following equation:
yf(t) = g(t) ∗f 󸀠(t) .
(3.4-7)
Equation(3.4-7)canalso bededuced by a process similar to thatshownin 󳶳Figure3.10.
In conclusion, equation (3.4-4)is the greatest contribution to system analysis from
signal decomposability, system linearity and time-invariance, and it can provide a
new way to ﬁnd the zero-state response of a system. Moreover, this also is the main
purpose of the introduction of the convolution operation.
Example 3.4-1. The impulse response of an LTI system is h(t) = eαtε(t), and the exci-
tation is f(t) = ε(t −1). Find the zero-state response of this system.
Solution.
yf(t) = h(t) ∗f(t) =
+∞
∫
−∞
f(τ)h(t −τ)dτ =
+∞
∫
−∞
ε(τ −1)eα(t−τ)ε(t −τ)dτ =
t
∫
1
eα(t−τ)dτ
= eαt
t
∫
1
e−ατdτ = eαt (−1
α ) e−ατ 󵄨󵄨󵄨󵄨󵄨
t
1 = 1
α eαt (e−α −e−αt)
= 1
α [eα(t−1) −1] ,
t ≥0 .
Example 3.4-2. The waveforms of the excitation f(t) and the impulse response h(t) of
a system are, respectively, shown in 󳶳Figure 3.11a and b. Find the zero-state response
yf(t) and plot it.
Authenticated
29 PM

3.5 Judgment of dynamics, reversibility and causality
|
103
t
t
t
3 
)
(t
y f
)
(t
f
)
(t
h
2
1
0
7
2
1
0
2
5
12
3
6
9
2
1
0
0.5
(a)
(b)
(c)
Fig. 3.11: E3.4-2.
Solution. The excitation and the impulse response can, respectively, be written as
f(t) = 2ε(t −1) −2ε(t −7) ,
h(t) = 1
2ε(t −2) −1
2ε(t −5) .
Thus,
f 󸀠(t) = 2δ(t −1) −2δ(t −7) ,
t
∫
−∞
h(τ)dτ =
t
∫
−∞
1
2 [ε(τ −2) −ε(τ −5)] dτ
= 1
2(t −2)ε(t −2) −1
2(t −5)ε(t −5) .
Then
yf(t) = f(t) ∗h(t) = f 󸀠(t) ∗
t
∫
−∞
h(τ)dτ
= [2δ(t −1) −2δ(t −7)] ∗[1
2(t −2)ε(t −2) −1
2(t −5)ε(t −5)]
= (t −3)ε(t −3) −(t −6)ε(t −6) −(t −9)ε(t −9) + (t −12)ε(t −12)
This can be expressed by a piecewise function
yf(t) =
{
{
{
{
{
{
{
{
{
{
{
{
{
{
{
{
{
{
{
0
(t ≤3)
t −3
(3 < t ≤6)
3
(6 < t ≤9)
12 −t
(9 < t ≤12)
0
(t > 12)
,
and is pictured in 󳶳Figure 3.11c.
3.5 Judgment of dynamics, reversibility and causality
As mentioned before, the impulse response can describe or represent an LTI system,
so the properties of an LTI system can be also described or judged by the impulse
response.
Authenticated
29 PM

104
|
3 Analysis of continuous-time systems in the time domain
3.5.1 Judgment of dynamics
If the impulse response h(t) of a continuous system satisﬁes
h(t) = Kδ(t) ,
(3.5-1)
then the response y(t) and the excitation f(t) satisfy
y(t) = Kf(t) .
(3.5-2)
Obviously, the system is a static/memoryless system. Thus, equation (3.5-1) can be
considered as the judgment condition for whether a system is dynamic or static. If
the coefficient K > 1, the system is an ideal ampliﬁer and it is an ideal attenuator if
0 < K < 1.
Example 3.5-1. Prove the condition h(t) = Kδ(t) to make an LTI system a memoryless
system.
Proof. If the excitation of an LTI system is f(t), the response produced by f(t) is y(t),
so when t = t0, we have
y(t0) = f(t) ∗h(t) 󵄨󵄨󵄨󵄨t=t0 =
+∞
∫
−∞
f(τ)h(t0 −τ)dτ .
(3.5-3)
According to the sampling property of the impulse signal, we have
∞
∫
−∞
f(t)δ(t −t0)dt = f(t0) .
(3.5-4)
As an even signal, δ(t) is helpful for changing the above formula into
∞
∫
−∞
f(t)δ(t0 −t)dt = f(t0) .
(3.5-5)
Comparing equation (3.5-3) with equation (3.5-5), if
h(t) = Kδ(t) ,
(3.5-6)
where K is positive or negative constant, then we have
y(t0) =
+∞
∫
−∞
f(τ)Kδ(t0 −τ)dτ = Kf(t0) .
Thus, the present response is only determined by the present excitation for the system,
namely the system is memoryless.
Authenticated
29 PM

3.6 Solved questions
|
105
3.5.2 Judgment of reversibility
If the impulse responses of two systems are h(t) and hi(t), separately, then when
h(t) ∗hi(t) = δ(t) ,
(3.5-7)
the system with h(t) as the impulse response is a reversible system or an original sys-
tem, and the system with hi(t) as the impulse response is the inverse system of the
original system.
3.5.3 Judgment of causality
If the impulse response of a continuous system h(t) satisﬁes
h(t) = 0
t < 0 ,
(3.5-8)
the system is causal. In other words, a system is causal if its impulse response is a
causal signal.
3.6 Solved questions
Question 3-1. The unit step response of an LTI system is g(t) = e−3tε(t). Then the
impulse response of the system is h(t) =
.
Solution.
h(t) = dg(t)
dt
= δ(t) −3e−3tε(t) .
Question 3-2. The unit step response of an LTI system is g(t) = (3e−2t −1)ε(t). Using
time domain analysis ways ﬁnd:
(1) The impulse response h(t) of the system.
(2) The zero-state response yf1(t) of the system to an input f1(t) = tε(t).
(3) The zero-state response yf2(t) of the system to an input f2(t) = t[ε(t) −ε(t −1)].
Solution. (1) According to the relation between h(t) and g(t), we have
h(t) = dg(t)
dt
= 2δ(t) −3e−2tε(t) .
(2) From the relation between r(t) = tε(t) and ε(t), and the linearity, we obtain
yf1(t) =
t
∫
0
g(t)dt = (−3
2e−2t −t) 󵄨󵄨󵄨󵄨󵄨
t
0 = (3
2 −3
2 e−2t −t) ε(t) .
(3) Because f2(t) = t[ε(t) −ε(t −1)] = tε(t) −(t −1)ε(t −1) −ε(t −1), and based on
system linearity and time invariance, we obtain
yf2(t) = (3
2 −3
2e−2t −t) ε(t) −(3
2 + 3
2 e−2(t−1) −t) ε(t −1) .
Authenticated
29 PM

106
|
3 Analysis of continuous-time systems in the time domain
Question 3-3. The step response of an LTI system is g(t) = e−tε(t). Find the zero-state
response yf(t) when the input is f(t) = 3e2t.
Solution. Because
h(t) = dg(t)
dt
= δ(t) −e−tε(t) ,
yf(t) = f(t) ∗h(t) = 3e2t ∗[δ(t) −e−tε(t)] = 3e2t −3e2t ∗e−tε(t) ,
3e2t ∗e−tε(t) =
∞
∫
0
3e2(t−τ)e−τdτ = 3e2t
∞
∫
0
e−3τdτ = 3e2t e−3τ
−3
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨
∞
0
= e2t ,
we have
yf(t) = 3e2t −e2t = 2e2tε(t) .
Question 3-4. The impulse response of a system is h(t) = e−tε(t), and the excitation
is f(t) = ε(t).
(1) Find the zero-state response yf(t) of the system.
(2) The systems consisting of h1(t) and h2(t) are shown in 󳶳Figure Q3-4a and b for
h1(t) = 0.5 [h(t) + h(−t)], h2(t) = 0.5 [h(t) −h(−t)]. Find yf1(t) and yf2(t).
Solution. (1) From the calculus property of convolution, the zero-state response yf(t)
is
yf(t) = f 󸀠(t) ∗
t
∫
−∞
h(t)dt = [
[
t
∫
0
e−τdτ]
]
ε(t) = (1 −e−t) ε(t) .
(2) Letting the impulse response of the system in 󳶳Figure Q3-4a be ha(t),
ha(t) = h1(t) −h2(t) = 0.5 [h(t) + h(−t)] −0.5 [h(t) −h(−t)] = h(−t) = etε(−t) .
Thus,
yf1(t) = f(t) ∗ha(t) = ε(t) ∗etε(−t) =
t
∫
−∞
eτε(−τ)dτ .
When t < 0, we have
yf1(t) =
t
∫
−∞
eτdτ = et .
When t ≥0, we have
yf1(t) =
t
∫
−∞
eτdτ = 1 ,
1( )
h t
2( )
h t
+
_
( )
f t
f1( )
y
t
1( )
h t
2( )
h t
+
( )
f t
f 2( )
y
t
+
(a)
(b)
Fig. Q3-4
Authenticated
29 PM

3.8 Problems
|
107
and thus,
yf1(t) = etε(−t) + ε(t) .
Letting the impulse response of the system in 󳶳Figure Q3-4b be hb(t),
hb(t) = h1(t) + h2(t) = 0.5 [h(t) + h(−t)] + 0.5 [h(t) −h(−t)] = h(t) ,
and thus,
yf2(t) = yf(t) = (1 −e−t) ε(t) .
3.7 Learning tips
The time domain response is usually the ﬁnal result of system analysis, and the reader
should pay attention to the following points of knowledge:
(1) The introduction of operator can transform a differential equation into a pseudo-
algebraic equation, and thus the process of solving differential equation can be
simpliﬁed.
(2) The transfer operator can directly reﬂect the structure and characteristics of a sys-
tem itself.
(3) Theimpulseresponseis theﬁrstelementor soulinthetimedomainanalysis meth-
ods.
(4) Neither the impulse response nor the transfer operator have anything to do with
the response and the excitation of a system; they are relevant to the structure and
parameters of the system itself.
(5) The zero-state response can be obtained through excitation and impulse response
convolution. The essence is that an excitation f(t) can be changed into a contin-
uous sum of impulse signal δ(t), and a zero-state response yf(t) can be changed
into a continuous sum of impulse response h(t).
3.8 Problems
Problem 3-1. The differential equation of an LTI system is y󸀠(t)+3y(t) = f(t). Knowing
y(0+) =
3
2 and f(t) = 3ε(t), ﬁnd the natural response and forced response of this
system.
Problem 3-2. The differential equation of an LTI system is y󸀠󸀠(t) + 4y󸀠(t) + 4y(t) =
2f 󸀠(t) + 8f(t). Find the complete response and point out the natural response and
forced response in it when f(t) = e−t, y(0+) = 3 and y󸀠(0+) = 4.
Problem 3-3. The differential equation of a system is y󸀠󸀠(t) + 3y󸀠(t) + 2y(t) = f(t). Find
the natural response and forced response of the system for each situation of which
excitations and starting conditions are the following:
(1) f(t) = ε(t), y(0−) = 1, y󸀠(0−) = 2; and
(2) f(t) = e−2tε(t), y(0−) = 1, y󸀠(0−) = 2.
Authenticated
29 PM

108
|
3 Analysis of continuous-time systems in the time domain
Problem 3-4. The differential equation of an LTI system is y󸀠󸀠(t) + 3y󸀠(t) + 2y(t) =
f 󸀠(t) + 3f(t). If f(t) = ε(t), y(0−) = 1 and y󸀠(0−) = 2, ﬁnd the complete response and
point out the natural response and forced response, and the zero-input and zero-state
responses in it.
Problem 3-5. The differential equation of a system is y󸀠󸀠(t) + 2y󸀠(t) + 5y(t) = 2f 󸀠(t) +
4f(t), and y(0−) = 2 and y󸀠(0−) = −2. Find the zero-input response yx(t) of this system.
Problem 3-6. The differential equation of a system is y󸀠󸀠(t)+4y󸀠(t)+3y(t) = f(t). If the
starting conditions are y(0−) = 1 and y󸀠(0−) = 2, ﬁnd the zero-input response yx(t) of
this system.
Problem 3-7. The differential equation of a system is y󸀠󸀠(t)+3y󸀠(t)+2y(t) = f 󸀠(t)+5f(t).
Knowing f(t) = e−3tε(t), yf(0+) = 1 and y󸀠
f(0+) = 2, ﬁnd the zero-state response yf(t).
Problem 3-8. Inthecircuitis shownin 󳶳FigureP3-8, theswitchis at“1” and thecircuit
is in a steady state for t < 0. At moment t = 0 the switch is from position “1” to “2”.
(1) Find values of uC(0+) and i(0+).
(2) Find the complete response of uC(t) and point out the free and forced responses,
and the zero-input and zero-state responses in it.
+
−
1
S ( )
u
t
Ω
2
F
4
1
+
−
C( )
u
t
+
−
1
2
2
S ( )
u
t
V
20
V
10
Fig. P3-8
Problem 3-9. For the circuit shown in 󳶳Figure P3-9, it is known that L = 2 H, C = 1
4 F,
R1 = 1 Ωand R2 = 5 Ω; the starting voltage of the capacitor uC(0−) = 3 V, the starting
current of the inductor iL(0−) = 1 A; the excitation current source is iS(t) = ε(t). Find
the zero-input response iLx(t) and the zero-state response iLf(t) of the inductor current
iL(t).
S( )
i t
1
R
C
2
R
L
L( )
i t
Fig. P3-9
Problem 3-10. For the circuit shown in 󳶳Figure P3-10, ﬁnd the transfer operators of
i(t) and u(t) to the excitation f(t).
Authenticated
29 PM

3.8 Problems
|
109
)
(t
f
F
2
1
Ω
2
Ω
1
+
−
)
(t
u
Ω
2
F
2
1
)
(t
f
+
−
)
(t
i
Ω
1
H
1.0
H
1.0
(a)
(b)
Fig. P3-10
Problem 3-11. The transfer operator H(p) and the state or condition at moment 0+ of
a system are known. Find the zero-input response of the system.
(1) H(p) =
p+3
p2+3p+2, yx(0+) = 1, y󸀠
x(0+) = 2
(2) H(p) =
p+3
p2+2p+2, yx(0+) = 1, y󸀠
x(0+) = 2
(3) H(p) =
3p+1
p(p+1)2 , yx(0+) = y󸀠
x(0+) = 0, y󸀠󸀠
x (0+) = 1
Problem 3-12. The zero-state response of an LTI system to an input f(t) = 2e−3tε(t)
is yf(t), and to f 󸀠(t) it is yfd(t) = −3yf(t) + e−2tε(t). Find the impulse response of the
system h(t).
Problem 3-13. The zero-state response of a system to an input f(t) is yf(t) = ∫
∞
t−2 et−τf ⋅
(τ −1)dτ Find the impulse response of the system h(t).
Problem 3-14. The zero-state response and the input of a system are plotted in
󳶳Figure P3-14. Find the zero-state response yf1(t) of the system to another input
f1(t) = sin πt[ε(t) −ε(t −1)].
0
0
2
3
1
1
1
1
( )
f t
t
f ( )
y t
t
2
3
Fig. P3-14
Problem 3-15. The impulse response and excitation of a matched ﬁlter in a commu-
nication system are related by h(t) = f(T −t), where T is the duration time of f(t). If
f(t) = ε(t) −ε(t −T). Find the zero-state response of the matched ﬁlter yf(t).
Problem 3-16. The step response of an LTI system g(t) = (2e−2t −1)ε(t) is known. Find
the zero-state response to each excitation shown in 󳶳Figure P3-16 by the properties of
convolution.
0
0
2
3
1
1
1
1
)
(
1 t
f
t
)
(
2 t
f
t
-1
(a)
(b)
Fig. P3-16
Authenticated
29 PM

110
|
3 Analysis of continuous-time systems in the time domain
Problem 3-17. The system shown in 󳶳Figure P3-17 consists of several subsystems; the
impulse response of each subsystem is h1(t) = ε(t) (integrator), h2(t) = δ(t −1) (unit
delayer), h3(t) = −δ(t) (inverter), h4(t) = 3δ(t) (multiplier). Find the step response
g(t) and impulse response h(t) of total system and draw their waveforms.
)
(t
f
2( )
h t
1( )
h t
3( )
h t
)
(t
y
4( )
h t
+
−
1( )
h t
Fig. P3-17
Problem 3-18. The system shown in 󳶳Figure P3-18 consists of several subsystems; the
impulse responses of each subsystem are, respectively, ha(t) = δ(t −1) and hb(t) =
ε(t) −ε(t −3). Find the impulse response h(t) of the total system.
)
(t
f
)
(t
y
( )
bh t
+
+
( )
ah t
+
( )
ah t
( )
ah t
Fig. P3-18
Problem 3-19. In the circuit shown in 󳶳Figure P3-19, iS(t) is the input and uL(t) is the
output. Find the step response g(t) and the impulse response h(t).
S( )
i t
+
−
L( )
u
t
Ω
4
H
3
Ω
2
Fig. P3-19
Problem 3-20. In the circuit shown in 󳶳Figure P3-20, f(t) is the input and uC(t) is the
output. Find the impulse response h(t).
+
−
)
(t
f
H
1
F
1
Ω
1
+
−
C( )
u
t
C( )
i
t
)
(t
i
)
(
L t
i
Ω
1
Fig. P3-20
Authenticated
29 PM

4 Analysis of continuous-time systems excited by
periodic signals in the real frequency domain
Questions: Besides characteristics of the time domain, a signal also has character-
istics of the frequency domain. Then how can we analyze a system response to a
periodic signal in the frequency domain?
Solution: Seek basic signals that can represent a variety of periodic signals. →Find
the response of a system to the basic signals in the frequency domain. →Obtain
the frequency responses to other periodic signals using the same approaches as
for to the basic signals.
Results: Fourier series, system functions and harmonic response summation.
The time domain analysis method of an LTI system has some advantages, such as the
clear physical conception, the intuitive results, etc., so it is the basic method of system
analysis. However, it is also ﬂawed. For example, it has too many concepts, needs to
determine the boundary conditions (the state values of a system at moments 0−and
0+), involves more complicated calculations, etc. As a result, it is not commonly used
in practice.
In the time domain analysis, because the signal f(t) is a function of a time vari-
able, we are interested in the change relations of the signal magnitude and speed and
delay over time. Therefore, the analysis methods of signals and systems naturally are
developed from the time variable. We must note the fact that the size (amplitude) and
delay (phase) of a signal are also directly related to another variable frequency. In
other words, the amplitude and phase of a signal are the functions of the frequency
variable. So, we want to know whether can analyze signals and systems based on the
frequency variable, and the answer is affirmative. The real frequency domain or, sim-
ply, the frequency domain analysis method will be introduced as a method that is a
totally different analysis method from the time domain analysis method. Furthermore,
it can provide another method for the analysis of signals and systems. A comparison
of these two methods is shown in 󳶳Figure 4.1.
Very difficult
Solve a system model
Time-domain methods
Frequency-domain methods
Great
Fig. 4.1: Comparison between time and frequency domain methods.
https://doi.org/10.1515/9783110419535-004
Authenticated
32 PM

112
|
Analysis of continuous-time systems excited by periodic signals
As we know, all signals can be divided into two kinds, i.e. periodic and nonperi-
odic ones, so, this chapter will focus on the analysis of the responses of a system to
periodic signals in the frequency domain. The analysis method for aperiodic signals
will be discussed in Chapter 5.
4.1 Orthogonal functions
4.1.1 The orthogonal function set
As deﬁned in the calculus course, if two functions f1(t) and f2(t) deﬁned in an interval
[t1, t2] hold the relation
t2
∫
t1
f1(t)f2(t)dt = 0 ,
(4.1-1)
we can say that f1(t) and f2(t) are orthogonal to each other on the interval [t1, t2].
If two complex functions f1(t) and f2(t) deﬁned in an interval [t1, t2] are related
by
t2
∫
t1
f1(t)f ∗
2 (t)dt =
t2
∫
t1
f ∗
1 (t)f2(t)dt = 0 ,
(4.1-2)
we can also say that f1(t) and f2(t) are orthogonal to each other in the interval [t1, t2];
f ∗(t) is the conjugate function of f(t).
Suppose a function set consisting of n functions f1(t), f2(t), . . . , fn(t) satisﬁes the
following equation in an interval [t1, t2]:
t2
∫
t1
fi(t)fj(t)dt =
{
{
{
0
(i
̸= j)
ki
(i = j)
.
(4.1-3)
We say that this function set is an orthogonal function set; ki are constants.
If a complex function set {fn(t)} (n = 1, 2, . . . ) in an interval [t1, t2] holds the
equation
t2
∫
t1
fi(t)f ∗
j (t)dt =
{
{
{
0
(i
̸= j)
ki
(i = j)
,
(4.1-4)
we say that it is an orthogonal complex function set.
Authenticated
32 PM

4.1 Orthogonal functions
|
113
In 3D 
In 2D 
x
V
y
V
z
V
V
( , , )
D x y z
x
y
z
o
i
j
k
y
x
o
i
j
V
x
V
y
V
( ,
)
D x y
(a)
(b)
Fig. 4.2: Coordinate representation of a vector.
Except for f1(t), f2(t), . . . , fn(t) in an orthogonal function set, if no function y(t)
(0 < ∫
t2
t1 y2(t)dt < ∞) can satisfy the following equation:
t2
∫
t1
fi(t)y(t)dt = 0
(i = 1, 2, . . . , n) ,
this function set is called a complete orthogonal set. This means that no other function
can be orthogonal to all functions in this set.
The concept of orthogonal functions is similar to that of orthogonal (perpendic-
ular) vectors. For example, three-dimensional coordinate axes are three vectors that
are orthogonal to each other, this meaning that their projection lengths are all zero,
and each vector cannot be represented by others (linear independent). However, in
the three-dimensional space constructed with the three orthogonal vectors, any vec-
tor can be represented by their linear combination. For example, a vector V starting
from the origin can be written as
V = Vx + Vy + Vz ,
where V x, Vy and Vz are, respectively, projections of V in three-dimensional coor-
dinate axes, as shown in 󳶳Figure 4.2a. Of course, V can also be represented by unit
vectors i, j and k,
V = xi + yj + zk ,
where x, y and z are the vertex coordinates of V, and the vertex is represented by D.
A vector in a two-dimensional coordinate system also has a similar concept, as
shown in 󳶳Figure 4.2b.
A function can also have an inﬁnite dimensional existence space as a vector in
conception. Therefore, the purpose of introducing the concept of orthogonal functions
is to express a function linearly using an orthogonal function set.
Authenticated
32 PM

114
|
Analysis of continuous-time systems excited by periodic signals
Note that when a linear combination of all components in an orthogonal vector
set is used to represent a vector, the vector set must be complete. Similarly, if a linear
combination of all functions in an orthogonal function set is used to represent any
function (or signal), the function set must also be complete.
4.1.2 Trigonometric function set
A function set {1, cos ω0t, cos 2ω0t, . . . , cos nω0t, . . . , sin ω0t, sin 2ω0t, . . . ,
sin nω0t, . . . } deﬁned over an interval [t0, t0 + T](T =
2π
ω0 ) is orthogonal because
it satisﬁes equations
t0+T
∫
t0
cos nω0t cos mω0tdt =
{
{
{
0
(m
̸= n)
T
2
(m = n)
,
(4.1-5)
t0+T
∫
t0
sin nω0t sin mω0tdt =
{
{
{
0
(m
̸= n)
T
2
(m = n
̸= 0)
,
(4.1-6)
t0+T
∫
t0
sin nω0t cos mω0tdt = 0
(for all m and n) .
(4.1-7)
The trigonometric function set is important and is used widely because of the fol-
lowing advantages:
–
Trigonometric functions are a kind of basic function.
–
Trigonometric functions relate simultaneously to the two physical quantities time
and frequency.
–
Trigonometric functions are easily produced, transmitted and processed.
–
A trigonometric function through an LTI system is still one with the same fre-
quency, and only the amplitude and phase may change.
4.1.3 Imaginary exponential function set
The imaginary exponential function set {ejnω0t} (n = 0, ±1, ±2, . . . ) is also a complete
orthogonal function set over the interval [t0, t0 + T] (T = 2π
ω0 ), because it satisﬁes the
equation
t0+T
∫
t0
ejmωt.(ejnωt)∗dt =
t0+T
∫
t0
ej(m−n)ωtdt =
{
{
{
0
(m
̸= n)
T
(m = n)
.
(4.1-8)
The result from equation (4.1-8) is not surprising, because an imaginary exponential
function can be represented by trigonometric functions from Euler’s relation. The ob-
jective of discussing the concept of the orthogonal function is to introduce the Fourier
series in the next section.
Authenticated
32 PM

4.2 Fourier series
|
115
4.2 Fourier series
Fourier series (FS) is a periodic function analysis tool that was proposed by the French
mathematician Fourier in 1807.
4.2.1 Trigonometric form of Fourier series
If an arbitrary periodic function f(t) with a period T and an angular frequency ω0 = 2π
T
can satisfy the Dirichlet conditions:
–
it is continuous, or has a ﬁnite number of ﬁrst-class discontinuities in one period;
–
it has a ﬁnite number of maxima or minima in one period;
–
it is bounded, namely, it is absolutely integrable ∫
t0+T
t0
|f(t)|dt < ∞,
so, f(t) can be expanded in the trigonometric form of Fourier series:
f(t) = a0 + a1 cos ω0t + a2 cos 2ω0t + ⋅⋅⋅+ b1 sin ω0t + b2 sin 2ω0t + ⋅⋅⋅
or
f(t) = a0 +
∞
∑
n=1
(an cos nω0t + bn sin nω0t) ,
n = 1, 2, . . .
(4.2-1)
where a0, an and bn are called Fourier coefficients.
Equation (4.2-1) can be considered as the deﬁnition of the Fourier series. It tells us
that an arbitrary periodic signal can be represented by a linear combination of inﬁ-
nite orthogonal trigonometric functions. Thus, this is just the application example of
signal decomposability.
Note that although the Dirichlet conditions only are the sufficient conditions for a
periodic function to be expanded in Fourier series, the periodic signals in practice can
all satisfy them. Hence, all subsequent periodic signals can be expanded in Fourier
series without a special statement.
Wecanﬁnd theFourier coefficients by usingtheorthogonality of thetrigonometric
function set:
Find a0. Integrate to both sides of equation (4.2-1) term by term over an interval
[−T
2 , T
2 ],
T
2
∫
−T
2
f(t)dt =
T
2
∫
−T
2
a0dt +
∞
∑
n=1
[[
[
T
2
∫
−T
2
an cos nω0tdt +
T
2
∫
−T
2
bn sin nω0tdt]]
]
.
For arbitrary n, we obtain
T
2
∫
−T
2
sin nω0tdt = 0 .
Authenticated
32 PM

116
|
Analysis of continuous-time systems excited by periodic signals
If n
̸= 0, we should have
T
2
∫
−T
2
cos nω0tdt = 0 .
Thus,
a0 = 1
T
T
2
∫
−T
2
f(t)dt .
(4.2-2)
Find an. Multiply both sides of equation (4.2-1) by cos nω0t, and integrate them term
by term over the interval [−T
2 , T
2 ], that is,
T
2
∫
−T
2
f(t) cos nω0tdt =
T
2
∫
−T
2
a0 cos nω0tdt
+
∞
∑
n=1
[[
[
T
2
∫
−T
2
an cos nω0t cos nω0tdt +
T
2
∫
−T
2
bn sin nω0t cos nω0tdt]]
]
.
According to the trigonometric function characteristics and orthogonality, we have
an = 2
T
T
2
∫
−T
2
f(t) cos nω0tdt .
(4.2-3)
Find bn. Multiply to both sides of equation (4.2-1) by sin nω0t, and integrate them term
by term over the interval [−T
2 , T
2 ], that is,
bn = 2
T
T
2
∫
−T
2
f(t) sin nω0tdt .
(4.2-4)
Because equation (4.2-1) contains both sin and cosine functions, its physical concept
is not clear, and we need to combine the sin and cosine terms into a cosine form.
Supposing there is a right triangle in the fourth quadrant of a two-dimensional
coordinate system, the opposite side of φn is −bn on the vertical axis, the adjacent
side is an on the horizontal axis and the hypotenuse is cn, thus
cn = √a2n + b2n
(n = 1, 2, . . . ) ,
(4.2-5)
cos φn =
an
√a2n + b2n
= an
cn
(n = 1, 2, . . . ) ,
(4.2-6)
Authenticated
32 PM

4.2 Fourier series
|
117
sin φn =
−bn
√a2n + b2n
= −bn
cn
(n = 1, 2, . . . ) ,
(4.2-7)
tgφn = sin φn
cos φn
= −bn
an
→
φn = −arctan bn
an
.
(4.2-8)
Using the sum angle formula of trigonometric functions, equation (4.2-1) can be trans-
formed further as
f(t) = a0 +
∞
∑
n=1
(an cos nω0t + bn sin nω0t)
= a0 +
∞
∑
n=1
√a2n + b2n (
an
√a2n + b2n
cos nω0t −
−bn
√a2n + b2n
sin nω0t)
= a0 +
∞
∑
n=1
cn (cos φn cos nω0t −sin φn sin nω0t)
= c0 +
∞
∑
n=1
cn cos (nω0t + φn) ,
that is,
f(t) = c0 + c1 cos (ω0t + φ1) + c2 cos (2ω0t + φ2) + ⋅⋅⋅
= c0 +
∞
∑
n=1
cn cos (nω0t + φn) ,
(4.2-9)
where
c0 = a0
(4.2-10)
is the DC component. If c0 > 0, then φ0 = 0; while if c0 < 0, then φ0 = π. cn and φn
are the magnitude and the phase of the nth cosine component.
Equation (4.2-9) shows that any periodic signal satisfying the Dirichlet conditions
can be decomposed into the sum of a constant and inﬁnite cosine components with
different frequencies and phases. The ﬁrst constant term c0 is the average value of
f(t) in a period and represents the DC component of the periodic signal. The second
term c1 cos(ω0t + φ1) is called the fundamental wave or the ﬁrst harmonic, whose
angular frequency is the same as ω0 of the original signal f(t); the amplitude and
the initial phase are, respectively, c1 and φ1. The third term c2 cos(2ω0t + φ2) is
called the second harmonic; its frequency is the double of ω0, the amplitude and
the initial phase are, respectively, c2 and φ2. The remaining terms can be written
in the same manner. The term cn cos(nω0t + φn) is the nth harmonic, where cn
and φn are the amplitude and the initial phase angle, respectively, of the nth har-
monic.
Equation (4.2-9) can be considered as the standard trigonometric form of the
Fourier series and has the following signiﬁcance
Authenticated
32 PM

118
|
Analysis of continuous-time systems excited by periodic signals
(1) A periodic signal can be decomposed into the algebraic sum of a constant and inﬁ-
nite cosine signals with different frequencies and phases, or a periodic electronic
signal can be decomposed into the algebraic sum of a DC component and inﬁnite
harmonics.
(2) The initial phase of each cosine signal or each harmonic is just φn.
These two points will be used in plotting the frequency spectrum of a periodic signal.
4.2.2 Relations between function symmetries and Fourier coefficients
When ﬁnding the Fourier series of a periodic signal, it is necessary to obtain a0, an and
bn by integration three times, so the work is more complicated. From related research,
we discover that the symmetry of signal itself can simplify the computing work for
coefficients.
(1) If f(t) is even as shown in 󳶳Figure 4.2, f(t) cos nω0t is even and f(t) sin nω0t is
odd, we have
a0 = 1
T
T
2
∫
−T
2
f(t)dt = 2
T
T
2
∫
0
f(t)dt ,
(4.2-11)
an = 2
T
T
2
∫
−T
2
f(t) cos nω0tdt = 4
T
T
2
∫
0
f(t) cos nω0tdt ,
(4.2-12)
bn = 0 .
(4.2-13)
We can see that the Fourier series of an even signal includes only the cosine com-
ponents
f(t) = a0 +
∞
∑
n=1
(an cos nω0t) .
(4.2-14)
Whether or not the DC component a0 is existent depends on whether or not f(t) is
symmetric around the horizontal axis. For example, the DC component of f(t) in
󳶳Figure 4.3a exists, but it does not in 󳶳Figure 4.3b.
T
0
t
1
)
(t
f
T
−
0
t
T
T
−
)
(t
f
2
T
2
T
−
2
T
2
T
−
1
-1
(a)
(b)
Fig. 4.3: Periodic even signals.
Authenticated
32 PM

4.2 Fourier series
|
119
T
0
t
1
)
(t
f
T
−
0
t
T
1
T
−
)
(t
f
2
T
2
T
−
2
T
2
T
−
-1
-1
(a)
(b)
Fig. 4.4: Periodic odd signals.
(2) If f(t)isanodd signalasshownin 󳶳Figure4.4, f(t) cos nω0t is odd and f(t) sin nω0t
is even, we have
an = 0
(n = 0, 1, 2, . . . ) ,
(4.2-15)
bn = 4
T
T
2
∫
0
f(t) sin nω0tdt
(n = 1, 2, . . . ) .
(4.2-16)
We can see that there are only the sine signals rather than any cosine signals and
the DC components in the Fourier series of an odd signal, that is,
f(t) =
∞
∑
n=1
(bn sin nω0t) .
(4.2-17)
(3) If f(t) is an odd harmonic signal, it can be proved that its Fourier series expansion
contains a fundamental wave and odd harmonics but not even harmonics and DC
components. This is also the reason that it is named the odd harmonic signal.
If the waveform that is shifted by a half period along the time axis and is reversed
around the axis is the same as the original one of a signal, we can call this sig-
nal the odd harmonic signal, as shown in 󳶳Figure 4.4b. The odd harmonic signal
satisﬁes the expression
f(t) = −f (t ± T
2 ) .
(4.2-18)
This property is also called half period mirror symmetry.
(4) If f(t) is an even harmonic signal, it canbe proved that its Fourier series expansion
contains the DC component and even harmonics but not the fundamental wave
and odd harmonics.
If the waveform that is shifted by a half period along time axis is the same as the
original one of a signal, we can call this signal the even harmonic signal. The even
harmonic signal satisﬁes the expression
f(t) = f (t ± T
2 ) .
(4.2-19)
This property is also called the half period overlap.
In summary, the even and the odd characteristics of a signal can determine what
components (DC, sine and cosine components) appear in its Fourier series. The
Authenticated
32 PM

120
|
Analysis of continuous-time systems excited by periodic signals
even and odd harmonic characteristics of a signal can affect whether or not the
harmonic terms exist in its Fourier series, such as odd harmonics (including the
fundamental wave) and even harmonics.
Note: A periodic signal may satisfy both even/odd symmetry and even/odd har-
monic symmetry conditions at the same time. As a result, when we want to know
whether the Fourier series of a signal includes the sine or the cosine components,
wecanﬁrstuseeven/odd symmetry todetermineit, and thenwecanuseeven/odd
harmonic characteristics to determine whether the Fourier series includes the odd
or the even harmonics.
Example 4.2-1. Find the Fourier series of f(t), which is a square wave in 󳶳Figure 4.3b.
Solution. Because f(t) satisﬁes the odd and the odd harmonic symmetries at the same
time, a0 = 0, an = 0 and there are no even harmonics, then
bn = 4
T
T
2
∫
0
f(t) sin nω0tdt = 4
T
T
2
∫
0
sin nω0tdt = −4
T
1
nω0
cos nω0t|
T
2
0
= −4
T
1
nω0
(cos nω0
T
2 −1) = 2
nπ (1 −cos nπ) =
{
{
{
0
(n = 2m)
4
nπ
(n = 2m + 1)
,
so,
f(t) = 4
π [sin ω0t + 1
3 sin 3ω0t + 1
5 sin 5ω0t + ⋅⋅⋅+ 1
n sin nω0t + ⋅⋅⋅]
n = 1, 3, 5, . . .
Example 4.2-2. Find the Fourier series of f(t), which is a symmetric square wave in
󳶳Figure 4.5a.
Solution. Because f(t) satisﬁes even symmetry and odd harmonic symmetry at the
same time, bn = 0 and there are no even harmonics, then
a0 = 2
T
T
2
∫
0
f(t)dt = 0 ,
an = 4
T
T
2
∫
0
f(t) cos nω0tdt = 4
T
T
4
∫
0
cos nω0tdt−4
T
T
2
∫
T
4
cos nω0tdt
= 4
T
1
nω0
(2 sin nω0
T
4 −sin nω0
T
2 )
= 4
nπ sin nπ
2 = 4
nπ (−1)
n−1
2 ,
n = 1, 3, 5, . . .
Authenticated
32 PM

4.2 Fourier series
|
121
0
t
1
)
(t
f
0
t
2
T
2
T
−
2
T
2
T
−
-1
-1
4
T
−
4
T
)
(
1 t
f
t
0
cos
4
ω
π
4
T
4
T
−
0
t
0
t
2
T
2
T
−
2
T
2
T
−
-1
-1
4
T
−
4
T
4
T
4
T
−
t
0
3
cos
3
4
ω
π
−
t
0
cos
4
ω
π
)
(
2 t
f
5
4
π
t
0
5
cos ω
t
0
3
cos
3
4
ω
π
−
t
0
cos
4
ω
π
)
(
3 t
f
(a)
(b)
(c)
(d)
Fig. 4.5: Structures of a symmetric square wave.
We then have
f(t) =
∞
∑
n=1
an cos nω0t
= 4
π [cos ω0t −1
3 cos 3ω0t + 1
5 cos 5ω0t −1
7 cos 7ω0t + ⋅⋅⋅
⋅⋅⋅+ 1
n (−1)
n−1
2 cos nω0t + . . . ]
n = 1, 3, 5, . . .
The Fourier series with ﬁnite terms of a symmetric square wave f(t) is plotted in
󳶳Figure 4.5. 󳶳Figure 4.5b is the waveform of which the Fourier series only con-
tains the fundamental wave (f1(t) = 4
π cos ω0t). The waveform of which the Fourier
series contains two terms, such as the fundamental wave and the third harmonic
(f2(t) = 4
π cos ω0t −
4
3π cos 3ω0t), is shown in 󳶳Figure 4.5c. The waveform of which
the Fourier series contains the fundamental wave and its third and ﬁfth harmonics,
(f3(t) =
4
π cos ω0t −
4
3π cos 3ω0t +
4
5π cos 5ω0t) is shown in 󳶳Figure 4.5d. It can be
seen that
(1) The more terms (number of harmonics) a Fourier series contains, the greater sim-
ilarity between the series and the original f(t). However, the variation of the peak
values of the series waveform remains the same, which means that the variation
will not change with the number of harmonic waves. This fact is called the Gibbs
phenomenon.
(2) The amplitudes of high frequency components (harmonics) are smaller, and they
mainly impact on the step edges of a pulse. The amplitudes of low frequency com-
ponents are larger, and they mainly impacton the top form of a pulse, which is the
main part of a square wave. In other words, the details of a waveform are mainly
depicted by high frequency components, and the shape of a waveform is mainly
determined by low frequency components in the waveform.
Authenticated
32 PM

122
|
Analysis of continuous-time systems excited by periodic signals
Fourier series
Periodic signal 1
Periodic signal 2
Periodic signal 3
Periodic signal 4
Periodic signal n
Fig. 4.6: Fourier series function diagram.
(3) When any frequency component in a Fourier series changes, the waveform of the
signal will be more or less distorted.
(4) If f3(t) is given by 󳶳Figure 4.5, it will be transformed into f2(t) if cos 5ω0t is ﬁltered
by a lowpass ﬁlter. Moreover, f2(t) will be transformed into f1(t) if term cos 3ω0t
is also ﬁltered. Obviously, a lowpass ﬁlter can ﬁlter ripples produced by high fre-
quency components in the curve and smoothen the curve. This ﬁltering concept is
very important in communication principles, and readers should carefully learn
this.
From the Fourier series, we know that arbitrary periodic signals in real engineering
can be expressed as a uniﬁed form that is an algebraic sum of trigonometric functions
(sinusoidal signals). Its signiﬁcance is that we do not need to ﬁnd the correspond-
ing methods for thousands of periodic signals but only need to research one kind –
the sinusoidal signal. As a universal method, the Fourier series can be ﬁguratively
compared to a master key that can open various periodic signal locks, as shown in
󳶳Figure 4.6.
4.2.3 Exponential form of the Fourier series
If the imaginary exponential function set {ejnω0t} is chosen as the orthogonal function
set, a periodic signal f(t) can be written as the exponential form of the Fourier series.
Euler’s relation is substituted into equation (4.2-1), which yields
f(t) = a0 +
∞
∑
n=1
(an
ejnω0t + e−jnω0t
2
+ bn
ejnω0t −e−jnω0t
2j
)
= a0 +
∞
∑
n=1
( an −jbn
2
ejnω0t + an + jbn
2
e−jnω0t) .
(4.2-20)
Authenticated
32 PM

4.2 Fourier series
|
123
Let
Fn = an −jbn
2
= |Fn| ejφn ,
(4.2-21)
where
|Fn| = 1
2
√a2n + b2n,
φn = −arctan bn
an
.
Obviously, the conjugate F∗
n of Fn is F∗
n = an+jbn
2
, and therefore, equation (4.2-20) be-
comes
f(t) = a0 +
∞
∑
n=1
(Fnejnω0t + F∗
ne−jnω0t) ,
(4.2-22)
and
Fn = an −jbn
2
= 1
T
T
2
∫
−T
2
f(t) cos nω0tdt −j 1
T
T
2
∫
−T
2
f(t) sin nω0tdt
= 1
T
T
2
∫
−T
2
f(t)(cos nω0t −j sin nω0t)dt .
Changing the algebraic form of the complex number into exponential form, we have
Fn = 1
T
T
2
∫
−T
2
f(t)e−jnω0tdt .
(4.2-23)
Similarly,
F∗
n = an + jbn
2
= 1
T
T
2
∫
−T
2
f(t)ejnω0tdt .
(4.2-24)
Comparing equations (4.2-23) and (4.2-24), we ﬁnd that a difference between them is
the minus sign in front of the integer n, and we have
F∗
n = F−n .
(4.2-25)
Substituting equation (4.2-25) into equation (4.2-22), and changing F−n into Fn by
changing the summing direction, we obtain
f(t) = a0 +
∞
∑
n=1
(Fnejnω0t + F−ne−jnω0t) = a0 +
∞
∑
n=1
Fnejnω0t +
∞
∑
n=1
F−ne−jnω0t
= a0 +
∞
∑
n=1
Fnejnω0t +
−∞
∑
n=−1
Fnejnω0t
=
−1
∑
n=−∞
Fnejnω0t + a0 +
∞
∑
n=1
Fnejnω0t .
(4.2-26)
Authenticated
32 PM

124
|
Analysis of continuous-time systems excited by periodic signals
If a0 is written as F0ejφ0ej0ω0t, and φ0 = 0, F0 = a0, we have
f(t) =
∞
∑
n=−∞
Fnejnω0t
n = 0, ±1, ±2, . . . .
(4.2-27)
Equation (4.2-27) is the complex exponential form of the Fourier series.
Comparing equations (4.2-21), (4.2-6), (4.2-7) and (4.2-8), we obtain the coefficient
relations between the trigonometric and the exponential forms of the Fourier series as
follows:
F0 = c0 = a0 ,
(4.2-28)
|Fn| = 1
2cn
n
̸= 0 ,
(4.2-29)
φn = −arctgbn
an
.
(4.2-30)
Accordingly, the standard trigonometric and the complex exponential forms are re-
lated by
f(t) =
+∞
∑
n=−∞
Fnejnω0t =
+∞
∑
n=−∞
|Fn| ej(nω0t+φn) = F0 +
+∞
∑
n=1
2 |Fn| cos (nω0t + φn) . (4.2-31)
It can be seen that the DC components in both forms of Fourier series are the same,
whereas the magnitude of each exponential harmonic coefficient is equal to a half of
the value in the trigonometric form. Moreover, the initial phases of harmonic com-
ponents of both forms equal each other. However, the ranges of n in two forms are
different.
From equation (4.2-31) we know that although in the complex exponential form
of Fourier series f(t) can be constituted by a series of complex exponential signals
like Fnejnω0t which distribute in a range of frequency axis from −∞to +∞, only two
terms located at frequency points −nω0 and +nω0 among of these signals can form a
harmonic component after they are added, and the separate term located at the point
−nω0 or +nω0 is not a harmonic component, but is merely a kind of mathematical
representation. This concept can be described by
(Fnejnω0t + F−ne−jnω0t) = (|Fn| ejφnejnω0t + |Fn| e−jφne−jnω0t)
= cn cos (nω0t + φn)
n
̸= 0 .
Because Fn originates from an cos (nω0t + φn) and bn sin(nω0t + φn), it means that
Fn is related to two sinusoidal functions, so according to equation (4.2-31) the corre-
sponding sinusoidal term of Fn can be written as
fn(t) = 2 |Fn| cos (nω0t + φn)
n = 1, 2, 3, . . . .
(4.2-32)
The above two different forms of Fourier series show that a periodic signal with ar-
bitrary waveform can be regarded as a combination of numerous basic continuous
Authenticated
32 PM

4.2 Fourier series
|
125
signals (sinusoidal or complex exponential signals), namely, an algebraic sum con-
sists of numerous harmonics with ω0 as the basic frequency. Therefore, we come to
the following conclusion:
Periodic signals with different shapes are different from each other only because
they have different fundamental frequencies and different harmonic amplitudes and
phases.
Example 4.2-3. Find the exponential form of Fourier series for f(t), which is a square
wave as shown in 󳶳Figure 4.4b.
Solution.
Fn = 1
T
T
2
∫
−T
2
f(t)e−jnω0tdt
= −1
T
0
∫
−T
2
e−jnω0tdt + 1
T
T
2
∫
0
e−jnω0tdt = 1
T
1
jnω0
e−jnω0t󵄨󵄨󵄨󵄨󵄨
0
−T
2 −1
T
1
jnω0
e−jnω0t󵄨󵄨󵄨󵄨󵄨
T
2
0
= 1
T
1
jnω0
(1 −ejnω0 T
2 ) −1
T
1
jnω0
(e−jnω0 T
2 −1) = 1
T
1
jnω0
(2 −ejnω0 T
2 −e−jnω0 T
2 )
=
2
jnTω0
(1 −cos nω0
T
2 ) =
1
jnπ (1 −cos nπ) =
{
{
{
0,
n is even
2
jnπ ,
n is odd
then
f(t) =
∞
∑
n=−∞
Fnejnω0t = 2
jπ (⋅⋅⋅−1
3e−j3ω0t −e−jω0t + ejω0t + 1
3 ej3ω0t + ⋅⋅⋅) .
The expansion of f(t) only contains the fundamental wave and odd harmonics, and it
also veriﬁes the characteristic of the odd harmonic function.
4.2.4 Properties of the Fourier series
To facilitate writing, we usually use the symbol
F S
←󳨀→to describe the relation between
a function and its Fourier series.
1. Linearity
If
f1(t)
F S
←󳨀→F1n,
f2(t)
F S
←󳨀→F2n ,
then
a1f1(t) + a2f2(t)
F S
←󳨀→a1F1n + a2F2n .
(4.2-33)
Authenticated
32 PM

126
|
Analysis of continuous-time systems excited by periodic signals
2. Time shifting
If
f(t)
F S
←󳨀→Fn ,
then
f(t −t0)
F S
←󳨀→Fne−jnω0t0 .
(4.2-34)
Proof. Because
f(t) =
∞
∑
n=−∞
Fnejnω0t ,
then
f(t −t0) =
∞
∑
n=−∞
Fnejnω0(t−t0) =
∞
∑
n=−∞
Fne−jnω0t0ejnω0t ,
hence,
f(t −t0)
F S
←󳨀→Fne−jnω0t0 .
Example 4.2-4. The Fourier coefficient of the periodic signal f1(t) shown in 󳶳Figure
4.7a is Fn. Represent the Fourier coefficients of the signals shown in 󳶳Figure 4.7b–d
with Fn.
Solution. Because
f2(t) = f1 (t −T
2 ) ,
from time shifting we have
f2(t)
F S
←󳨀→e−jn T
2 ω0Fn = (−1)nFn .
)
(
1 t
f
t
4
T
1
T
4
T
−
T
−
)
(
2 t
f
2
T
)
(
3 t
f
2
T
−
3
4
T
−
)
(
4 t
f
t
4
T
1
T
4
T
−
T
−
0
0
t
4
T
1
T
4
T
−
T
−
0
3
4
T
t
4
T
1
T
4
T
−
T
−
0
1
−
(a)
(b)
(c)
(d)
Fig. 4.7: E4.2-4.
Authenticated
32 PM

4.2 Fourier series
|
127
Based on the given conditions, we have
f3(t) = f1(t) + f2(t) ,
so
f3(t)
F S
←󳨀→Fn + (−1)nFn ,
and
f4(t) = f1(t) −f2(t) ,
and then
f4(t)
F S
←󳨀→Fn −(−1)nFn .
3. Time reversal
If
f(t)
F S
←󳨀→Fn ,
then
f(−t)
F S
←󳨀→F−n .
(4.2-35)
Proof. Because
f(t) =
∞
∑
n=−∞
Fnejnω0t ,
then
f(−t) =
∞
∑
n=−∞
Fne−jnω0t m=−n
=
∞
∑
m=−∞
F−mejmω0t ,
so
f(−t)
F S
←󳨀→F−n .
4. Differential
If
f(t)
F S
←󳨀→Fn ,
then
f 󸀠(t)
F S
←󳨀→(jnω0)Fn .
(4.2-36)
Proof. Because
f(t) =
∞
∑
n=−∞
Fnejnω0t ,
then
f 󸀠(t) =
∞
∑
n=−∞
(jnω0)Fnejnω0t ,
so
f 󸀠(t)
F S
←󳨀→(jnω0)Fn .
Authenticated
32 PM

128
|
Analysis of continuous-time systems excited by periodic signals
( )
f t
′
( )
f
t
′′
( )
f t
t
t
0
t
2
4
6
8
-2
-4
-6
-8
2
0
2
4
6
8
-2
-4
-6
-8
1
0
2
4
6
-2
-4
-6
-8
8
1
(1)
(-2)
(a)
(b)
(c)
Fig. 4.8: E4.2-5.
Example 4.2-5. Find the Fourier series of the trigonometric wave shown in 󳶳Figure
4.8a.
Solution. Taking the derivative of f(t) two times, we obtain f 󸀠(t) and f 󸀠󸀠(t). Their wave-
forms of them are shown in 󳶳Figure 4.8b and c. Assume that the Fourier coefficients
of f(t) and f 󸀠󸀠(t) are, respectively, Fn and F2n, we have
F2n = 1
T
T
2
∫
−T
2
f 󸀠󸀠(t)e−jnω0tdt = 1
6
3
∫
−3
[δ(t + 2) −2δ(t) + δ(t −2)] e−jn π
3 tdt
= 1
6 (ejn 2π
3 −2 + e−jn 2π
3 ) = 1
3 (cos 2nπ
3
−1) = −2
3 sin2 nπ
3
According to the differential property, we have
F2n = (jnω0)2 Fn ,
hence,
Fn =
F2n
(jnω0)2 =
1
(jnω0)2 (−2
3 sin2 nπ
3 ) =
6
(nπ)2 sin2 nπ
3 .
The Fourier series of the periodic trigonometric wave is
f(t) =
∞
∑
n=−∞
Fnejn π
3 t = 6
π2
∞
∑
n=−∞
sin2 nπ
3
n2
ejn π
3 t .
5. Energy conservation
The energy conservation properties of a periodic signal f(t) in the time and frequency
domains can be described by Parseval’s relation, that is,
P = f 2(t) = 1
T
t0+T
∫
t0
f 2(t)dt = a2
0 + 1
2
∞
∑
n=1
(a2
n + b2
n) = c2
0 + 1
2
+∞
∑
n=1
c2
n =
+∞
∑
n=−∞
|Fn|2 . (4.2-37)
Authenticated
32 PM

4.3 Frequency spectrum
|
129
Equation (4.2-37) states that the whole average power in a periodic signal is equal to
the sum of the average powers in all its harmonic components (including the DC com-
ponent). Parseval’s relation is often applied in the communication principles to ﬁnd
the SNR.
For a real signal, Fn = F∗
−n, therefore,
P =
+∞
∑
n=−∞
|Fn|2 = F2
0 + 2
+∞
∑
n=1
|Fn|2 .
(4.2-38)
The change relation of P with nω0 is usually called the power spectrum of a periodic
signal.
4.3 Frequency spectrum
4.3.1 Concept of frequency spectrum
From equation (4.2-9) we can see that the relation between a periodic signal f(t) and its
variable t can be also expressed by an algebraic sum of a series of harmonics (cosine
signals here), so a value of f(t) at any moment t0 is equal to the algebraic sum of all
values of the harmonics at this time. By careful observation we also ﬁnd that both the
amplitudes and initial phases of all harmonics to represent f(t) are different, further-
more, their frequencies are also different and change according to the rule by which
they increase progressively with integer multiples of ω0. Obviously, if the amplitude
and the initial phase of each harmonic are deﬁned as dependent variables, then they
can be regarded as functions of the independent variable nω0, which is the angular
frequency of each harmonic. Note: The word “frequency” will subsequently refer to
both frequency and angular frequency if no special instructions.
Thus, the Fourier series gives an important hint: The harmonic amplitudes and
initial phases of any a periodic signal can be expressed as functions of frequency.
Thus, we know that we can analyze a periodic signal based on the relations between
the harmonic amplitudes or initial phases and frequency instead of using the tradi-
tional methods in the time domain.
Because components Fnejnω0t in the complex exponential form of Fourier series
appear at all harmonic frequency points on the frequency axis, and their complex am-
plitudes Fn = |Fn| ejφn (such as equation (4.2-21)) only relate to positions of all har-
monics at the frequency axis (i.e. points −nω0 and +nω0) rather than time t, hence
Fn is called the frequency spectrum function of a periodic signal or, simply, the spec-
trum. The relation of amplitudes |Fn| of Fn with variable nω is called the amplitude
frequency characteristic or amplitude spectrum. The relation of the initial phases φn
of Fn with nω is called the phase frequency characteristic or phase spectrum. In this
Authenticated
32 PM

130
|
Analysis of continuous-time systems excited by periodic signals
way, the spectrum function F(nω0) of any a periodic signal f(t) can be deﬁned as
F(nω0)
def= Fn = 1
T
T/2
∫
−T/2
f(t)e−jnω0tdt
n = 0, ±1, ±2, ±3, . . . .
(4.3-1)
When n = 0, F(nω0) = F(0) = 1
T ∫
T/2
−T/2 f(t)dt represents the average value of a periodic
signal in one cycle, which is the DC component a0 or c0. When n
̸= 0, F(nω0) repre-
sents the complex amplitude of each harmonic. The graph of F(nω0) is very similar to
the light spectrum, this is the reason for calling it the frequency spectrum.
Since Fn = |Fn| ejφn and F−n = |Fn| e−jφn, the amplitude spectrum |F(nω0)| = Fn
is even and the phase spectrum φn is odd. Hence, they can be easily drawn.
Due to the introduction of F(nω0), equation (4.2-27) can be written as
f(t) =
∞
∑
n=−∞
F(nω0)ejnω0t .
(4.3-2)
Thus, a periodic signal f(t) and its spectrum function F(nω0) or Fn can be connected
by the Fourier series. We can describe this relationship as
f(t)
F S
←󳨀→F(nω0)
(4.3-3)
or
f(t)
F S
←󳨀→Fn .
(4.3-4)
Although the frequency spectrum is deﬁned based on the complex exponential form
of the Fourier series, the concept is also suitable for the trigonometric form. According
to equation (4.2-9) the frequency spectrum can be also plotted. The speciﬁc plotting
steps of two forms of frequency spectrum are as follows:
1. Trigonometric form (see Example 4.3-1)
Step 1: Transform the trigonometric form of the Fourier series into the standard form
of equation (4.2-5), namely, f(t) = c0 + ∑∞
n=1 cn cos(nω0t + φn).
Step 2: Draw a coordinate system of the frequency domain. The amplitude cn and the
frequency ω are, respectively, expressed as the vertical and horizontal axes. Then
draw the scales with an interval ω0 on the horizontal.
Step 3: Draw a vertical line segment with length c0 at the origin and vertical line seg-
ments with length cn at interval nω0 in sequence, so that the amplitude spectrum
has been completed.
Step 4: Draw a coordinate system of the frequency domain with the phase φn as the
vertical axis again.
Step 5: At points nω0 on the axis ω, draw vertical line segments with corresponding
length φn, then the phase spectrum has been plotted. If c0 < 0, φ0 = π, otherwise,
if c0 > 0, φ0 = 0.
Authenticated
32 PM

4.3 Frequency spectrum
|
131
Note the following:
(1) Because the cosine series is considered as the standard form of the Fourier series,
that is, equation (4.2-9), if there are several sine components in this series, they
should be transformed into cosine form, for example, sin(nω0t±φn) = cos(nω0t±
φn −π
2), its initial phase is φn = ±φn −π
2 .
(2) If the harmonic component is −cos(nω0t ± αn), it should be transformed into the
form cos(nω0t ± αn ∓π) in which the initial phase is φn = ±αn ∓π, where the
selection of a positive or negative sign in front of π should satisfy −π < φn < π.
(3) If αn = 0 in item (2), φn can be valued as ±π, and φn = +π in this book.
(4) If bn = 0 in the expression (4.2-1), the relations of a0 ,αn over nω0 can be plotted
directly, namely, the spectrum.
(5) If αn = 0 in the expression (4.2-1), the relations of a0, bn over nω0 can be plotted
after the sine functions are converted to the cosine functions, namely, the spec-
trum.
2. Complex exponential form
Step 1: From F(nω0) = Fn = 1
T ∫
T/2
−T/2 f(t)e−jnω0tdt we ﬁnd Fn and transform it into the
form Fn = |Fn| ejφn.
Step 2: Draw line segments |Fn| at −nω0 and +nω0, respectively, on the frequency
axis in the coordinate plane of the amplitude spectrum.
Step 3: Draw line segment φn at point +nω0 on the frequency axis in the coordinate
plane of the phase spectrum, and draw line segment −φn at −nω0.
To sum up, the conclusions are as follows:
(1) The frequency spectrum of a periodic signal can be shown in complex exponential
and trigonometric two forms.
(2) The complex exponential spectrum is bilateral, and the trigonometric form is uni-
lateral.
(3) Every amplitude value of harmonic wave in the bilateral amplitude spectrum is
one half of corresponding value in the unilateral amplitude spectrum, but the DC
component values in the bilateral and the unilateral spectrums are the same. The
bilateral amplitude spectrum waveform is even symmetric.
(4) The waveform of unilateral phase spectrum and the waveform appearing on the
right side of the vertical axis of a bilateral phase spectrum are the same; the wave-
form of a bilateral phase spectrum is odd symmetric.
(5) Usually the amplitude and phase spectrums cannot be plotted in one picture at
the same time unless the phase spectrum only has the two values of π and 0.
(6) The physical meaning of a unilateral spectrum is that it can truly illustrate the
changing relations of the harmonic amplitudes and phases with frequency, but
the bilateral spectrum is just a mathematical expression without any practical
meaning because it includes the negative frequency components.
Authenticated
32 PM

132
|
Analysis of continuous-time systems excited by periodic signals
ω
t
0
0
0
ω
0
3ω
0
5ω
0
7ω
( )
f t
0
(
)
F nω
1
a
3
a
5
a
7
a
1
0
cos
a
t
ω
3
0
cos3
a
t
ω
5
0
cos5
a
t
ω
7
0
cos7
a
t
ω
Fig. 4.9: Time domain and frequency domain waveforms of a symmetrical square wave.
(7) The complex exponential form is more commonly used than the trigonometric
form, because its analysis and calculation are more convenient.
The comparison between two spectrums with different forms can be seen in Exam-
ple 4.3-3. Obviously, if one kind has been pictured, the other can be obtained according
to the above conclusions (3) and (4).
To facilitate understanding, the frequency spectrum can be qualitatively ex-
plained as follows:
The frequency spectrum of a periodic signal refers to the changing relations of
the DC value, the amplitudes and phases of the fundamental wave, and all harmonic
components of the signal with independent variable frequency.
In order to interpret more vividly the concept of the frequency spectrum, we re-
draw the waveforms in Example 4.2-2 in 󳶳Figure 4.9.
4.3.2 Properties of the frequency spectrum
The plotting procedure and properties of the frequency spectrum of a periodic signal
will be given by the following examples of three typical periodic signals.
Example 4.3-1. Plot the frequency spectrum of the periodic sawtooth wave f(t) shown
in 󳶳Figure 4.10.
0
t
T
)
(t
f
T
−
2
E
2
E
−
2
T
2
T
−
Fig. 4.10: Periodic sawtooth pulse signal.
Authenticated
32 PM

4.3 Frequency spectrum
|
133
ω
0
0
2ω
0
ω
0
3ω
E π
nc
0
4ω
2
E
π
3
E
π
Fig. 4.11: Amplitude spectrum of a periodic sawtooth pulse
signal.
ω
0
0
2ω
0
ω
0
3ω
2
π
n
ϕ
2
π
−
0
4ω
Fig. 4.12: Phase spectrum of a periodic sawtooth pulse signal.
Solution. f(t) is odd, so a0 = 0, an = 0, and we have
bn = 2
T
T
2
∫
−T
2
f(t) sin nω0tdt = 4E
T2
T
2
∫
0
t sin nω0tdt
= −
4E
nω0T2 ( t cos nω0t|
T
2
0 −
T
2
∫
0
cos nω0tdt)
= −
4E
nω0T2
T
2 cos nω0
T
2 = E
nπ (−1)n+1 .
The Fourier series of the periodic sawtooth wave f(t) is
f(t) = E
π (sin ω0t −1
2 sin 2ω0t + 1
3 sin 3ω0t −1
4 sin 4ω0t + ⋅⋅⋅)
= E
π
∞
∑
n=1
1
n(−1)n+1 sin nω0t = E
π
∞
∑
n=1
1
n (−1)n+1 cos (nω0t −π
2 )
The spectrum of the periodic sawtooth wave only contains sine components, and the
amplitude of each harmonic is attenuated with speed 1
n; its unilateral amplitude fre-
quency and phase frequency spectrums are, respectively, pictured in 󳶳Figure 4.11 and
󳶳Figure 4.12.
Example 4.3-2. Find the front four nonzero terms in the amplitude spectrums of the
half wave and full wave rectiﬁed signals with 50 Hz shown in 󳶳Figure 4.13 and draw
their amplitude spectrums.
Solution. Because the two waveforms are even, neither of them contain sine compo-
nents.
For the halfwave rectiﬁed signal, we have
ω0 = 100π rad/s,
T = 0.02 s ,
Authenticated
32 PM

134
|
Analysis of continuous-time systems excited by periodic signals
( )
f t
t
0
E
half-wave rectified signal
 full-wave rectified signal
( )
f t
t
0
T
T
2
T
0.02s
T =
0.01s
T =
0
50Hz
f
=
0
100Hz
f
=
E
2
T
(a)
(b)
Fig. 4.13: E4.3-2 (1).
and
a0 = 1
T
T
2
∫
−T
2
f(t)dt = 2 × 50E
0.02
4
∫
0
cos(100πt)dt = E
π ,
an = 2
T
T
2
∫
−T
2
f(t) cos nω0tdt = 4 × 50E
1
200
∫
0
cos(100πt) cos(100nπt)dt .
Using Euler’s relation, we have
an = 2E cos nπ
2
(1 −n2)π ,
when n = 1, with L’Hôpital’s rule we obtain a1 = E
2.
So,
f(t) = E
π + E
2 cos 100πt + 2E
3π cos 200πt −2E
15π cos 400πt + ⋅⋅⋅
For the full wave rectiﬁed signal, we have
ω0 = 200π rad/s,
T = 0.01 s ,
and
a0 = 2E
π ,
an = 4E cos nπ
(1 −4n2)π ,
and ﬁnally,
f(t) = 2E
π + 4E
3π cos 200πt −4E
15π cos 400πt + 4E
35π cos 600πt + ⋅⋅⋅
nc
ω
0
half-wave rectified signal spectrum
 full-wave rectified signal spectrum
200π
100π
0.02s
T =
0.01s
T =
0
100
ω
π
=
300π
0
200
ω
π
=
400π
E
π
2
E
2
3
E
π
2
15
E
π
nc
ω
0
200π
600π
400π
2E
π
4
3
E
π
4
15
E
π
4
35
E
π
600π
2
35
E
π
(a)
(b)
Fig. 4.14: E4.3-2 (2).
Authenticated
32 PM

4.3 Frequency spectrum
|
135
The amplitude spectrums of the half wave and full wave rectiﬁed waveforms are plot-
ted in 󳶳Figure 4.14.
From 󳶳Figure 4.14, we have the following two points:
(1) Theaveragevalueof thefullwaverectiﬁcation, namely, theDC componentis twice
that of the half wave. This is the reason why many electric devices can provide two
switch positions of high (strong) and low (weak), such as an electric blower and
an electric blanket.
(2) The frequency components of the full wave waveform are less than those of the
half wave, which is good for the power supply ﬁlter.
Note: The coefficient an cannot be calculated by equation (4.1-5), because at this time
the integrand function is not a complete cosine function, but only the cosine function
with a half cycle.
Tips: This example can be also solved by the results of Example 5.3-8, that is, cn =
2
T F(jω)|ω=nω0, ω0 = 2π
T .
Example 4.3-3. Suppose that there is a periodic rectangular pulse signal f(t) with
pulse width τ, amplitude E and period T, as shown in 󳶳Figure 4.15. Find the expo-
nential and trigonometric forms of the Fourier series for this signal.
Solution. According to equation (4.2-25), we ﬁnd
Fn = 1
T
T
2
∫
−T
2
f(t)e−jnω0tdt = 1
T E
τ
2
∫
−τ
2
e−jnω0tdt = E
T
1
−jnω0
e−jnω0t󵄨󵄨󵄨󵄨󵄨
τ
2
−τ
2
= 2E
T
sin nω0τ
2
nω0
= Eτ
T
sin nω0τ
2
nω0 τ
2
= Eτ
T sa ( nω0τ
2
) .
The Fn does not have an imaginary part but changes within positive and negative val-
ues. So, for Fn > 0, we have φn = 0, while for Fn < 0, φn = π. Because the phase
spectrum is odd, φn = −π on the left side of horizontal axis. In addition, sa( nω0τ
2 ) is
zero when ω = 2nπ
τ ; the phase spectrum is pictured in 󳶳Figure 4.16d. The complex
exponential form of the Fourier series of f(t) is
f(t) =
∞
∑
n=−∞
Eτ
T Sa ( nω0τ
2
)ejnω0t = Eτ
T
∞
∑
n=−∞
Sa ( nω0τ
2
) ejnω0t .
0
t
T
)
(t
f
T
−
E
2
τ
2
τ
−
Fig. 4.15: Periodic rectangular pulse signal in E4.3-3.
Authenticated
32 PM

136
|
Analysis of continuous-time systems excited by periodic signals
If f(t) is to be written in trigonometric form, based on odd and even properties, we
have
a0 = 2
T
τ
2
∫
0
f(t)dt = 2
T
τ
2
∫
0
Edt = Eτ
T ,
bn = 0 .
an = 4
T
T
2
∫
0
f(t) cos nω0tdt = 4E
T
τ
2
∫
0
cos nω0tdt = 4E
T
1
nω0
sin nω0t|
τ
2
0
= 4E
T
1
nω0
sin ( nω0τ
2
) = 2Eτ
T Sa ( nω0τ
2
) .
As a result,
f(t) = a0 +
∞
∑
n=1
an cos nω0t = Eτ
T + 2Eτ
T
∞
∑
n=1
Sa ( nω0τ
2
) cos nω0t .
The DC component is c0 = a0 =
Eτ
T and the nth harmonic amplitude is cn = an =
2Eτ
T Sa( nω0τ
2 ).
If the cycle value T = 5τ, the trigonometric and complex exponential frequency
spectrums of f(t) respectively are shown in 󳶳Figure 4.16a–d.
From 󳶳Figure 4.16, in addition to the relative relations between the amplitude and
phase of each frequency component of a periodic rectangular pulse signal, the fol-
lowing properties can also be found:
(1) The spectrum of a periodic rectangular wave is discrete. Spectral lines will only
exist in some discrete frequency points, such as 0, ω0, 2ω0, etc. The interval in
any two adjacent lines is always ω0 (ω0 = 2π
T ), and therefore the greater period T,
the smaller frequency interval ω0 will be.
(2) The height or amplitude of each line is proportional to the pulse height E and
width τ, and inversely proportional to period T.
(3) The height of each line will regularly change with the envelope Sa( nω0τ
2 ). For ex-
ample, when T = 5τ and E = 1, we have cn = 2
5
󵄨󵄨󵄨󵄨󵄨Sa( nπ
5 )󵄨󵄨󵄨󵄨󵄨, so the fundamental
wave amplitude is c1 = 0.37and the second harmonic amplitude is c2 = 0.30;
but when n = 5m(m = 1, 2, 3, . . . ), the corresponding amplitudes of spectral
lines all are zero.
(4) The spectrum of a periodic rectangular wave contains an inﬁnite number of lines,
which means that this signal can be decomposed into an inﬁnite number of fre-
quency components. The amplitude change trend of each component is to de-
crease with an increase of frequency.
In summary, the spectrum of a periodic signal has the following properties:
(1) Discreteness. Spectral lines exist only at points of integer times of the fundamen-
tal frequency, and height changes are nonperiodic.
(2) Harmonics. Spectral lines are uniformly spaced on the frequency axis with an in-
terval of fundamental frequency, and there are no other frequency components
Authenticated
32 PM

4.3 Frequency spectrum
|
137
ω
0
0
2ω
0
ω
0
3ω
π
2
τ
τ
π
4
T
Eτ
Unilateral amplitude spectrum
Unilateral phase spectrum
Bilateral amplitude spectrum
nc
2 E
T
τ
Bilateral phase spectrum
π
ω
0
0
2ω
0
ω
0
3ω
π
2
τ
τ
π
4
n
ϕ
−π
π
ω
0
0
2ω
0
ω
0
3ω
π
2
τ
τ
π
4
n
ϕ
−π
4π
τ
−
2π
τ
−
ω
0
0
2ω
0
ω
0
3ω
π
2
τ
τ
π
4
T
Eτ
τ
π
2
−
n
F
(a)
(b)
(c)
(d)
Fig. 4.16: The spectrums of a periodic rectangular pulse signal.
(spectral lines) except ones that are integral multiples of the fundamental fre-
quency.
(3) Convergence. The general changing trend of each harmonic amplitude or line
height is gradually damped with an increase in the harmonic number. The slower
a signal waveform changes in the time domain, the faster those higher frequency
components in its spectrum attenuate, and the fewer higher frequency compo-
nents areinthespectrum. Ontheother hand, thefaster a signalwaveform changes
in the time domain, the greater the number of high frequency components in the
spectrum.
To facilitate understanding, the decomposition of a periodic signal with Fourier series
can be compared with the decomposition phenomenon of a white light by a prism in
a physics course. A beam of natural light can be decomposed into seven color lights
with different wavelengths by a prism, such as red, orange, yellow, green, cyan, blue
and violet (as shown in 󳶳Figure 4.17), and a periodic signal can be decomposed into
numerous harmonics with different frequencies by a “Fourier prism”.
Authenticated
32 PM

138
|
Analysis of continuous-time systems excited by periodic signals
Three prism
White light
Screen
Red
Orange
Yellow
Green
Cyan
Blue
Purple
(Periodic signal)
(Fourier series)
(Fundamental wave)
(2 times harmonic)
(4 times harmonic)
(3 times harmonic)
(5 times harmonic)
(6 times harmonic)
(7 times harmonic)
Fig. 4.17: Principle of decomposition with a triangular prism.
As mentioned above, the signal spectrum can show physical properties that are
difficult to illustrate in the time domain. As a result, the Fourier series or the Fourier
transform is a bridge between the time domain and frequency domain analyses. This
is a signiﬁcant analysis method and has an extremely important position in signal
analysis.
4.4 Fourier series analysis
Although we have sought a master key that can be used to express all kinds of periodic
signals, e.g. the Fourier series, this is the ﬁrst step of system analysis. An ultimate goal
is to ﬁnd the response using Fourier series after a periodic signal passes through a
linear system. Therefore, the second step of the system analysis is to ﬁnd the response
of a system to a basic signal – sinusoidal signal, followed by the response of the system
to any periodic excitation.
4.4.1 System function
The steady state response of an LTI system to a sinusoidal signal such as excitation is
still a sinusoidal signal with the same frequency. Therefore, the frequency variables
in them can be ignored, and they can be represented by a phasor only related to the
amplitude and phase of a signal. For example, a signal f(t) = A cos(ω0t + φ) can be
expressed as ̇F = Aejφ or ̇F = A∠φ.
Assuming that the excitation and the response of a system are, respectively, ex-
pressed as ̇F and ̇Y, we can deﬁne as follows:
The ratio of the response and excitation phasors of a system is called the system
transfer function or system function; it can be represented by a symbol H(jω),
H(jω) =
̇Y
̇F
.
(4.4-1)
Usually, H(jω) is a complex function, so it can be written as
H(jω) = |H(jω)| ejφ(ω) ,
(4.4-2)
Authenticated
32 PM

4.4 Fourier series analysis
|
139
where |H(jω)| is the magnitude of H(jω) an even function about variable ω, φ(ω) is
the phase of H(jω) and an odd function about ω.
Because H(jω) can reﬂect variations of amplitudes and phases of the system func-
tion for different frequencies, H(jω) is also called the frequency response charac-
teristic of a system; its magnitude and phase are called the amplitude frequency
characteristic and the phase frequency characteristic, respectively. This concept is
very useful in the study of communication systems.
Readers might ask: There is no term ω in the phasors
̇F and
̇Y, so, why does ω
exist in H(jω)? The reason is that
̇F and
̇Y obviously do not contain ω but this does
not mean that they have nothing to do with it. Rather it is because both frequency
values in excitation and response are the same, the excitation and the response can
be simpliﬁed in the expressed form. In addition, H(jω) is the concentrated reﬂection
of the structure and performance of a system and should ﬁt for sinusoidal excitations
with different frequencies, that is, it is a function of ω, so, ω cannot be omitted in
H(jω), unlike ̇F and ̇Y. Example 4.4-1 can help us to understand this problem.
From the above, we can see that the system function and the spectrum are similar
in concept. In fact, we will ﬁnd that the system function is just the spectrum of the
impulse response h(t) from the following chapters.
Although the system function H(jω) is only a ratio of the output and input of a
system, it can reﬂect the internal structure of a system, which is independent of the
output and the input signals. Once a system is given, the system function H(jω) is
also determined and will not change with different excitations, and this concept is
consistent with the H(p) in Chapter 3. In fact, H(jω) can be obtained by H(p),
H(jω) = H(p)|p=jω .
(4.4-3)
Equation (4.4-1) can be also written as
̇Y = H(jω) ̇F .
(4.4-4)
Equation (4.4-4) tells us that the response of a system to a sinusoidal signal can be
found by the product of the system function and the excitation phasor. This is the
result that we wanted to reach in the second step mentioned above.
From the Fourier series, any a cosine or sine signal can be also known as a har-
monic in form, and, therefore, we can also say that the response of a system to a har-
monic can be obtained by means of the product of the system function and the har-
monic phasor.
4.4.2 Analysis method
Periodic signals are deﬁned over range (−∞, +∞). Therefore, if a periodic signal is
applied to a system, we can consider that the signal is accessed to the system when
t = −∞, and only the steady state response exists while the system is analyzed.
Authenticated
32 PM

140
|
Analysis of continuous-time systems excited by periodic signals
Based on step 1 of a periodic excitation being decomposed by Fourier series and
step 2 of the response of system to a single frequency excitation (harmonic) being de-
termined by equation (4.4-4), which was stated above, we can implement the ﬁnal step
of the analysis method, namely, to obtain the response of the system to any periodic
signal. This can be realized by using the linearity property of the system to ﬁnd the
sum of all harmonic responses. The speciﬁc steps are as follows:
Step 1: Expand a given periodic signal as the standard form of the Fourier series.
From the convergence of the spectrum, only limited terms in the Fourier series
are usually necessary, such as c0, c1 cos(ω0t + φ1)„ cn cos(nω0t + φn). To facil-
itate the calculation, the best choice is to write each harmonic as a phasor form
like ̇cn = cnejφn or ̇cn = cn∠φn.
Step 2: According to the knowledge about circuits or the concept of the transfer op-
erator, ﬁnd the system function H(jω) and its limited terms of harmonics,
H(jω) = |H(jω)| ejφ(ω) = {|H(j0)| ejφ(0), |H(jω0)| ejφ(ω0), |H(j2ω0)| ejφ(2ω0), . . .
. . . , |H(jnω0)| ejφ(nω0), . . . } .
Let ̇F = ̇cn and substitute each harmonic component ̇cn into equation (4.4-4), then
ﬁnd responses y0(t), y1(t), . . . , yn(t) of the system to each ̇cn, that is,
yn(t) = cn |H(jnω0)| cos(nω0t + φHn) ,
(4.4-5)
where φHn = φ(nω0) + φn, namely, the phase of a harmonic response is equal
to the sum of the phase of the system function and the initial phase of an excited
harmonic.
Step 3: Add all harmonic responses to form the total response y(t) = y0(t) + y1(t) +
⋅⋅⋅+ yn(t), namely,
y(t) =
∞
∑
n=0
cn |H(jnω0)| cos(nω0t + φHn) .
(4.4-6)
Example 4.4-1. A circuit and waveform of a voltage source uS(t) are shown in
󳶳Figure 4.18a and b, and T = 10 µs is given. Find voltage uo(t) on resistor R.
Solution. (1) The Fourier series of the excitation source is
uS(t) = 80
π2 (cos ω1t + 1
9 cos 3ω1t + 1
25 cos 5ω1t + . . . ) ,
+
−
S( )
u t
C
0.5 F
μ
4Ω
R
+
−
o( )
u t
0
t
2
T
10
S( )
u t
T
10
−
2
T
−
T
(a)
(b)
Fig. 4.18: E4.4-1.
Authenticated
32 PM

4.4 Fourier series analysis
|
141
where ω1 = 2π
T = 2π × 105rad/s, so the harmonic phasors of the excitation are,
respectively,
̇US1 = 80
π2 cos ω1t = 8.11∠0° V ,
̇US2 = 80
π2 × 1
9 cos 3ω1t = 0.90∠0° V ,
̇US3 = 80
π2 × 1
25 cos 5ω1t = 0.324∠0° V ,
...
(2) According to the split voltage formula, the system function can be written as
H (jω) =
̇UO
̇US
=
R
R −j 1
ωC
.
(3) The phasor of the harmonic response on R generated by each excitation phasor
can be obtained by equation (4.4-5)
̇UO1 = H (jω1) ̇US1 =
R
R −j
1
ω1C
̇US1 =
4
4 −j3.18 × 8.11∠0° = 6.35∠38.5° V ,
̇UO3 = H (j3ω1) ̇US3 =
R
R −j
1
3ω1C
̇US3 =
4
4 −j1.06 × 0.9∠0° = 0.87∠14.84° V ,
̇UO5 = H (j5ω1) ̇US5 =
R
R −j
1
5ω1C
̇US5 =
4
4 −j0.636 × 0.324∠0° = 0.32∠9.03° V ,
...
(4) Finally, from the superposition principle, the voltage response on R can be ob-
tained by equation (4.4-6) as
uO(t) = 6.35 cos(ω1t+38.5°)+0.87 cos(3ω1t+14.84°)+0.32 cos(5ω1t+9.03°)+⋅⋅⋅
Example 4.4-1gives another more accurateand more vivid name: harmonic response
summation for the analysis method of a system to a periodic signal in the frequency
domain.
Comparing the content of this chapter with the sinusoidal steady state analysis
way in the circuits analysis course, we can see that the phasor analysis method in si-
nusoidal circuits is the foundation of the Fourier series analysis method in this course.
Just because the phasor analysis method has solved the response of system to a sine
excitation, the Fourier series method can have scope for its power.
In this chapter, ﬁrst, according to the signal decomposition, a periodic signal was
represented by a basic signal – the sinusoidal signal. Then the response of system
to the basic signal was found by means of the system function. Finally, the response
of system to any periodic signal was obtained from the linearity of the system. This
idea is shown in 󳶳Figure 4.19 and is signiﬁcant for the system analysis in subsequent
chapters.
Authenticated
32 PM

142
|
Analysis of continuous-time systems excited by periodic signals
( )
f t
1
0
cos
c
t
ω
0c
2
0
cos2
c
t
ω
0
cos
nc
n
t
ω
0( )
y t
1( )
y t
2( )
y t
( )
ny t
(j )
Y
H
F
ω
=
( )
y t
Excitation 
Response
A LTI system 
Decomposition 
Synthesis
•
•
Fig. 4.19: Analysis idea of a system to a periodic signal.
4.5 Solved questions
Question 4-1. A period signal is f(t) = 3 cos t + sin (5t + π
6) −2 cos (8t −2π
3 ). Plot its
unilateral amplitude and phase spectrums.
Solution. From the known conditions, values of f(t) exist at frequency points 1, 5, 8.
The sine term should be changed into the cosine like sin (5t + π
6) = cos (5t + π
6 −π
2 ) =
cos (5t −π
3). Since the third term in f(t) is minus, its phase should be changed as π −
2π
3 = π
3. The unilateral amplitude and phase spectrums are, respectively, pictured in
󳶳Figure Q4-1a and b.
1
2
3
4
5
6
7
8
0
1
2
3
nc
1
2
3
4
5
6
7
8
0
n
ϕ
1
3π
1
3π
−
ω
ω
(a)
(b)
Fig. Q4-1
Question 4-2. A periodic signal is f(t) = 2 sin ( π
2 t + π
4) −cos ( 4π
3 t −3π
4 ).
(1) Find its period T and angular frequency of the fundamental wave ω0.
(2) Find all nonzero harmonics.
(3) Plot the unilateral amplitude and phase spectrums of this signal.
Solution. (1) Because T1 = 2π/(π/2) = 4 s, T2 = 2π/(4π/3) = 1.5 s, the period must
be the least common multiple of 4 and 1.5, namely T = m1T1 = m2T2, T1/T2 =
m1/m2 = 8/3, then T = 3T1 = 8T2 = 12 s and ω0 = 2π/T = π/6 (rad/s).
(2) From the expression of f(t); the third and eighth harmonic components are
nonzero.
(3) Transform the ﬁrst term sine component into a cosine and change the second term
from negative to positive. Then f(t) will be transformed into the standard form
f(t) = 2 cos (π
2 t −π
4 ) + cos (4π
3 t + π
4 ) .
Authenticated
32 PM

4.5 Solved questions
|
143
1
2
3
4
5
6
7
8
9
0
nc
1
2
3 4
5
6
7
8
9
0
n
ϕ
4
π
−
ω
ω
4
π
1
2
(a)
(b)
Fig. Q4-2
From the equation, the unilateral amplitude and the phase spectrums are plotted
in 󳶳Figure Q4-2.
Question 4-3. The period signal f(t) is shown in 󳶳Figure Q4-3. Find the DC compo-
nent.
-6 -5-4
-1
1 2 3 4 5 6
0
10
...
...
( )
f t
t
Fig. Q4-3
Solution. The DC component is equivalent to the coefficient a0 in Fourier series, so
we have
a0 = 1
T0
T0/2
∫
−T0/2
f(t)dt = 1
5
1
∫
−1
10dt = 4 .
The DC component of the period signal f(t) is 4.
Question 4-4. Judge which option is the spectrum component of the period signal
shown in 󳶳Figure Q4-4.
A.
Cosine components in harmonics
B.
Sine components in harmonics
C.
Cosine components in odd harmonics
D.
Sine components in odd harmonics
0
( )
f t
2
T
T
2
T
−
T
−
A
−
A
...
...
t
Fig. Q4-4
Solution. Because this signal meets expressions f(t) = −f(−t) and f(t) = −f(t ± T/2),
it is an odd harmonic function; the spectrum should only include sine components in
odd harmonics, so the answer is D.
Authenticated
32 PM

144
|
Analysis of continuous-time systems excited by periodic signals
4.6 Learning tips
Theperiodic signalisa commonly used and importantsignal. Frequency domainanal-
ysis is not only the supplement for analysis in time domain, but also the means of
display of the physical characteristics of a signal. Please pay attention to following
points:
(1) Fourier series is a master key to analyze various periodic signals.
(2) The frequency spectrum is another description that is as important as the expres-
sion in time domain; it can reﬂect some invisible characteristics of signals in the
time domain.
(3) The system function is an important function that can reﬂect the system struc-
ture and characteristics independently of excitation and response. It is also a link
between excitation and response in the frequency domain.
(4) The response of a system to any periodic signal can be expressed as an algebraic
sum of sinusoidal signals with different frequencies, namely, an algebraic sum of
harmonics.
4.7 Problems
Problem 4-1. Point out the frequency components in the Fourier series of periodic sig-
nals shown in 󳶳Figure P4-1 using the parity of the signal.
)
(
1 t
f
t
2
T
T
2
T
−
T
−
)
(
2 t
f
t
1
T
T
−
1
−
2
T
2
T
−
0
0
1
1
−
(a)
(b)
Fig. P4-1
Problem 4-2. Find the trigonometric and exponential forms of the Fourier series for
signals shown in 󳶳Figure P4-2 using the direct calculation method.
)
(
1 t
f
t
(1)
)
(
2 t
f
t
E
)
(
3 t
f
t
1
)
sin( t
π
0
2
T
−
T
−
T
2
T
2
T
T
2
T
−
T
−
0
0
1
1
−
2
2
−
(a)
(b)
(c)
Fig. P4-2
Authenticated
32 PM

4.7 Problems
|
145
Problem 4-3. Four signals with the same period are shown in 󳶳Figure P4-3.
(1) Find the Fourier series of signal f1(t) using the direct calculation method.
(2) Find the Fourier series of f2(t), f3(t) and f4(t) using the properties of Fourier series.
1( )
f t
t
2
T
1
T
2
T
−
T
−
0
2 ( )
f
t
t
2
T
1
T
2
T
−
T
−
0
3( )
f
t
t
2
T
1
T
2
T
−
T
−
0
4 ( )
f
t
t
2
T
1
T
2
T
−
T
−
0
Fig. P4-3
Problem 4-4. Find the trigonometric forms of the Fourier series for the signals shown
in 󳶳Figure P4-4 using the differential property of the Fourier series.
1( )
f t
t
1
T
2T
−
0
2T
2 ( )
f
t
t
1
1
0
2
1
−
2
−
T
−
Fig. P4-4
Problem 4-5. Plot the amplitude and phase spectrums of the following periodic sig-
nals.
(1) f(t) = 4
π (cos ω0t −1
3 cos 3ω0t + 1
5 cos 5ω0t −1
7 cos 7ω0t + . . . )
(2) f(t) = 1
2 −2
π (sin 2πt + 1
2 sin 4πt + 1
3 sin 6πt + 1
4 sin 8πt + . . . )
(3) f(t) = 1 −sin πt + cos πt +
1
√2 cos (2πt + π
6)
Problem 4-6. Let the Fourier series coefficients of f(t), f1(t), f2(t) and f3(t) be, respec-
tively, Fn, F1n, F2n, F3n, and f1(t) = f ∗(t), f2(t) = f(t) cos ω0t, f3(t) = f(t) sin ω0t.
Prove
(1) F1n = F∗
−n,
(2) F2n = 1
2(Fn+1 + Fn−1),
(3) F3n = 1
2j(Fn−1 −Fn+1).
Problem 4-7. If the Fourier coefficients of a complex functionf(t) and its conjugate
function f ∗(t) are, respectively,An and Bn, prove Bn = A−n.
Problem 4-8. Find the Fourier series of the half wave cosine signal in 󳶳Figure P4-8
and plot its amplitude and phase spectrums.
Authenticated
32 PM

146
|
Analysis of continuous-time systems excited by periodic signals
t
t
0
cosω
)
(t
f
E
T
2
T
T
−
2
T
−
0
Fig. P4-8
Problem 4-9. Find the Fourier series of the full wave cosine signal in 󳶳Figure P4-9 and
plot its amplitude and phase spectrums.
t
t
0
cosω
)
(t
f
E
T
2
T
T
−
2
T
−
0
Fig. P4-9
Problem 4-10. A circuit is shown in 󳶳Figure P4-10 and uS(t) = 6 + 10 cos(103t) +
6 cos(2 × 103t) V is known. Find the capacitor voltage uC(t).
S( )
u t
R
C
C( )
u
t
+
−
Ω
60
F
5.
12 μ
+
−
Fig. P4-10
Problem 4-11. In the circuit in 󳶳Figure P4-11a, voltage source uS(t) is a periodic signal
and its waveform is shown in 󳶳Figure P4-11b. Find the voltage u2(t) on the resistor.
(Do not consider harmonics above three orders.)
C
Ω
10
F
400
1
H
10
L
R
S( )
u t
R ( )
u
t
+
−
+
−
t
1
1
0
2
1
−
2
−
S( )
u t
(a)
(b)
Fig. P4-11
Problem 4-12. If the periodic square wave voltage shown in 󳶳Figure P4-12a is applied
to an RL circuit, ﬁnd the ﬁrst four harmonics of i(t).
L
R
+
−
H
1
Ω
1
)
(t
i
t
1
1
0
2
1
−
2
−
S( )
u t
S( )
u t
(a)
(b)
Fig. P4-12
Authenticated
32 PM

5 Analysis of continuous-time systems excited by
nonperiodic signals in the real frequency domain
Questions: We have found a master key or a general tool to analyze periodic signals
in frequency domain, which is the Fourier series. Similarly, can we also ﬁnd a
master key to analyze nonperiodic signals?
Solution: Seek the relation between a periodic signal and a nonperiodic signal →
Find the analysis methods to a nonperiodic signal using the ways and results for
a periodic signal.
Results: Fourier transform and system function. The Fourier transform of the zero-
state response is equal to the product of Fourier transform of the excitation and
the system function.
5.1 The concept of Fourier transform
The methods for solving for the response of a system to a periodic signal in the fre-
quency domain were presented in Chapter 4. Their core is that an arbitrary periodic
signal that satisﬁes the Dirichlet conditions can be developed as an algebraic sum of
inﬁnite sinusoidal signals (harmonics) with the Fourier series (usually, limited terms
are considered), then subresponses corresponding to each harmonic are found, and
ﬁnally they are superposed to form a complete response.
So, the questions are how to get the zero state responses to various nonperiodic
signals? Can a general analysis method like the Fourier series for all nonperiodic sig-
nals be found? The answer is “yes”.
Through the observations to periodic signal waveforms we can ﬁnd that if a re-
peated cycle T of a periodic signal becomes inﬁnite, its waveform is never repeated
and will evolve into a nonperiodic signal. This ﬁnding provides a new approach for
analysis to nonperiodic signals with the aid of the analysis methods for periodic sig-
nals.
After analysis of the spectrum of a periodic signal, we can see that the interval
between two adjacent lines ω0 will tend to be inﬁnitesimal when the period T tends to
inﬁnity, the original discrete spectrum will become continuous and, at the same time,
the amplitudes of frequency components (harmonics)will also approach inﬁnitesimal
but still keep a certain ratio. To describe this kind of frequency characteristic of an
aperiodic signal, we will introduce the concept of spectrum density in this section.
From equation (4.3-1) if T tends to inﬁnity, then F(nω0) or Fn will tend to zero;
obviously, it is not the spectrum function that we know. However, if the T on the de-
nominator on the right side of equation (4.3-1) is moved to the left, then F(nω0)
1
T
is an
https://doi.org/10.1515/9783110419535-005
Authenticated
32 PM

148
|
Analysis of continuous-time systems excited by aperiodic signals
indeﬁnite form of 0
0, and its limit may exist when T tends to inﬁnity,
F(nω0)
1
T
=
T/2
∫
−T/2
f(t)e−jnω0tdt ,
(5.1-1)
where 1
T = f0 = ω0
2π . Therefore, equation (5.1-1) becomes
2πF(nω0)
ω0
=
T/2
∫
−T/2
f(t)e−jnω0tdt .
(5.1-2)
Equation (5.1-2) is considered as the size of the complex amplitude on the unit fre-
quency, which has the obvious meaning of density. If T tends to inﬁnite, then ω0 tends
to be inﬁnitely small, the discrete variable nω0 becomes a continuous variable ω, and
F(nω0) will change from a discrete function to a continuous function. Therefore, we
deﬁne that the limit of equation (5.1-2) is a spectral density function, which is denoted
by F(jω), that is,
F(jω)
def=
lim
T→∞
2πF(nω0)
ω0
= lim
T→∞
T/2
∫
−T/2
f(t)e−jnω0tdt =
∞
∫
−∞
f(t)e−jωtdt .
(5.1-3)
Then equation (4.3-2) can be rewritten as
f(t) = lim
T0→∞
1
2π
∞
∑
n=−∞
2πF(nω0)
ω0
ejnω0t ⋅ω0 .
(5.1-4)
Under the conditions nω0 →ω, ∑∞
n=−∞→∫
∞
−∞and ω0 →dω, equation (5.1-4) will
become a typical integral expression
f(t) = 1
2π
∞
∫
−∞
F(jω)ejωtdω .
(5.1-5)
Equation (5.1-5) shows that f(t) can be treated as a continuous sum of complex expo-
nential signals ejωt, whose frequencies are inﬁnite density and amplitudes dω
2π F(jω)
are inﬁnitesimal and terms are inﬁnite. Thus, equations (5.1-3) and (5.1-5) are known
as the Fourier transform pair,
F(jω) = F[f(t)] =
∞
∫
−∞
f(t)e−jωtdt ,
(5.1-6)
f(t) = F−1[F(jω)] = 1
2π
∞
∫
−∞
F(jω)ejωtdω .
(5.1-7)
Equation (5.1-6)is called the Fourier transform and equation (5.1-7) is called the inverse
Fourier transform. The F(jω) is known as the Fourier transform of a signal f(t), and f(t)
Authenticated
32 PM

5.1 The concept of Fourier transform
|
149
is called the original function of F(jω). The symbol “F” represents the Fourier trans-
form operation, and “F−1” represents the operation of the inverse Fourier transform.
The f(t) and F(jω) can be related by symbol “
F
←→”, that is,
f(t)
F
←→F(jω) .
(5.1-8)
The Fourier transform is a type of linear transform and has the characteristic of one
to one correspondence. equation (5.1-6) is also called the decomposition formula, and
equation (5.1-7) is the synthetic formula. Obviously, the Fourier transform reﬂects the
decomposition and synthesis features of signals once again.
Generally speaking, F(jω) is a complex function and can be written as
F(jω) = R(ω) + jI(ω) = |F(jω)| ejφ(ω) ,
(5.1-9)
where |F(jω)| and φ(ω) are, respectively, the magnitude and phase of F(jω), and R(ω)
and I(ω) are, respectively, its real and imaginary parts.
Because |F(jω)| describes the relative size of each frequency component density of
an aperiodic signal f(t), φ(ω) describes the phase relationships between various fre-
quency component densities of f(t). Thus, to facilitate research, F(jω) is also known
as the frequency spectrum of f(t), curves |F(jω)| ∼ω and φ(ω) ∼ω are called, re-
spectively, the amplitude spectrum and the phase spectrum of a nonperiodic signal.
In this way, F(jω) and F(nω0) are uniﬁed in name. However, readers must remember
that their meanings are also different.
It should be pointed that the above process of derivation for the Fourier transform
focused on its physical concept. Strict mathematical derivation shows that the suffi-
cient condition for the existence of the Fourier transform for a signal f(t) is that f(t)
must be absolutely integrable,
∞
∫
−∞
|f(t)| dt < ∞.
(5.1-10)
For some signals that are not absolutely integrable, such as DC signals, symbol sig-
nals, etc., the Fourier transforms can be found with the limit method.
The Fourier series states that a periodic signal f(t) can be expressed as a discrete
sum of imaginary exponential signals, which is
f(t) =
∞
∑
n=−∞
F(nω0)ejnω0t .
The Fourier transform states that an aperiodic signal f(t) can be expressed as a con-
tinuous sum of imaginary exponential signals, which is
f(t) = 1
2π
∞
∫
−∞
F(jω)ejωtdω .
Authenticated
32 PM

150
|
Analysis of continuous-time systems excited by aperiodic signals
The evolution process from a periodic signal to an aperiodic signal is a change process
from a discrete sum to a continuous sum in form. Moreover, the Fourier transform is
an extension of the Fourier series.
Like a period signal, a nonperiodic signal can be also decomposed into a sum of
cosine components with different frequencies, which contains all frequency compo-
nents from zero to inﬁnity, that is, the frequency is a continuous variable. This con-
clusion can be deduced by
f(t) = 1
2π
∞
∫
−∞
F(jω)ejωtdω = 1
2π
∞
∫
−∞
|F(jω)| ej[ωt+φ(ω)]dω
= 1
2π
∞
∫
−∞
|F(jω)| cos[ωt + φ(ω)]dω +
j
2π
∞
∫
−∞
|F(jω)| sin[ωt + φ(ω)]dω .
(5.1-11)
We know
F(jω) =
∞
∫
−∞
f(t)e−jωtdt =
∞
∫
−∞
f(t) cos ωtdt −j
∞
∫
−∞
f(t) sin ωtdt ,
and then, we have
|F(jω)| = [
[
(
∞
∫
−∞
f(t) cos ωtdt)
2
+ (
∞
∫
−∞
f(t) sin ωtdt)
2
]
]
1/2
,
φ(ω) = −arctan
∫
∞
−∞f(t) sin ωtdt
∫
∞
−∞f(t) cos ωtdt
.
It can be seen that |F(jω)| is an even function of ω, and φ(ω) is odd, so the ﬁrst term
and the second one on right side of equation (5.1-11), respectively, are
1
2π
∞
∫
−∞
|F(jω)| cos[ωt + φ(ω)]dω = 1
π
∞
∫
0
|F(jω)| cos[ωt + φ(ω)]dω ,
j
2π
∞
∫
−∞
|F(jω)| sin[ωt + φ(ω)]dω = 0 ,
Therefore, equation (5.1-11) can be written as
f(t) = 1
π
∞
∫
0
|F(jω)| cos[ωt + φ(ω)]dω .
(5.1-12)
Authenticated
32 PM

5.1 The concept of Fourier transform
|
151
A comparison of equations (5.1-12) and (4.2-5) yields
f(t) = c0 +
∞
∑
n=1
cn cos(nω0t + φn) ;
it is not difficult to ﬁnd similarities and differences between them.
It needs to be explained that the Fourier transform F(jω) of a signal f(t) deﬁned
in this book emphasizes the imaginary number as its independent variable, but it is
also deﬁned as F(ω) in some books (such as «Communication Principles»), where the
idea is to deﬁne the independent variable on the real frequency axis, in order to un-
derstand easily the physical concepts of signals and noises in communication tech-
nology. From the aspect of application, there are no essential differences between the
two deﬁnitions and they are usually interchangeable. One reason that F(jω) is applied
is conventional, another one is it can more clearly reﬂect the relationship between the
Fourier transform and the Laplace transform. In the Laplace transform, when the real
part σ = 0 in the independent variable s = σ + jω, the Laplace transform F(s) of f(t)
will become its Fourier transform F(jω), that is, the Fourier transform is a special
case of the Laplace transform for a signal. In other words, the Fourier transform is
just the Laplace transform when σ = 0.
From equation (5.1-3), the Fourier transform F(jω) was derived from the Fourier
series coefficient Fn by using the limit method. So, what is the speciﬁc relationship
between them?
Assuming that fT(t) is a periodic signal, if f(t) is the model of an arbitrary periodic
waveform which is cut from the waveform of fT(t), then its Fourier transform should
be
F(jω) = F[f(t)] =
T/2
∫
−T/2
f(t)e−jωtdt .
Comparing above the equation with the Fourier series coefficient formula of a periodic
signal,
Fn = 1
T
T
2
∫
−T
2
f(t)e−jnω0tdt ,
we obtain
Fn = 1
T F(jω)|ω=nω0 .
(5.1-13)
Equation (5.1-13) shows that the Fourier coefficient Fn, namely, the spectrum F(nω0)
of a periodic signal, can be found by the Fourier transform F(jω) of any a periodic
waveform in the periodic signal.
Authenticated
32 PM

152
|
Analysis of continuous-time systems excited by aperiodic signals
5.2 Fourier transforms of typical aperiodic signals
The Fourier transforms (spectrums) of some typical aperiodic signals are given below.
5.2.1 Gate signals
The rectangular pulse f(t) with width τ and amplitude E shown in 󳶳Figure 5.1 is called
the gate signal when E = 1; it is denoted as gτ(t). The Fourier transform pair is
f(t) =
{
{
{
E
(|t| ≤τ
2)
0
(|t| > τ
2)
F
←→F(jω) = Eτ ⋅Sa ( ωτ
2 ) .
(5.2-1)
Proof. From equation (5.1-6), we have
F(jω) =
∞
∫
−∞
f(t)e−jωtdt =
τ
2
∫
−τ
2
Ee−jωtdt = −E
jω e−jωt󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨
τ
2
−τ
2
= 2E
ω sin ωτ
2 = Eτ ⋅Sa ( ωτ
2 ) .
That is,
|F(jω)| = Eτ ⋅
󵄨󵄨󵄨󵄨󵄨󵄨󵄨Sa ( ωτ
2 )
󵄨󵄨󵄨󵄨󵄨󵄨󵄨,
(5.2-2a)
φ(ω) =
{
{
{
0
(F(jω) ≥0)
π
(F(jω) < 0)
.
(5.2-2b)
Therefore, we obtain
gτ(t)
F
←→τ ⋅Sa ( ωτ
2 ) ,
(5.2-3)
where Sa(x) = sin x
x , or, Sa(t) = sin t
t
is called the sampling function, which is an impor-
tant function in the communication principles course. Obviously, the sampling func-
tion is even and has a feature of ∫
∞
0 Sa(t)dt = π
2 .
Because F(jω) is a real function, it can be expressed by a curve F(jω) ∼ω, as shown in
󳶳Figure 5.2. When F(jω) is positive, the phase is 0, and when it is negative, the phase
is π.
0
t
)
(t
f
E
2
τ
- 2
τ
Fig. 5.1: Rectangular pulse signal.
τ
E
22
.
0
−
τ
E
13
.
0
)
( ω
j
F
0
ω
2π
τ
2
- π
τ
τ
E
Fig. 5.2: Rectangular pulse signal spectrum.
Authenticated
32 PM

5.2 Fourier transforms of typical aperiodic signals
|
153
0
t
)
(t
f
1
at
e−
Fig. 5.3: Unilateral exponential signal.
0
ω
)
( ω
j
F
0
ω
)
(ω
ϕ
1
a
Fig. 5.4: Amplitude and phase spectrums of an unilateral exponential signal.
5.2.2 Unilateral exponential signals
The unilateral exponential signal is sketched in 󳶳Figure 5.3 and the Fourier transform
pair is
f(t) =
{
{
{
e−at
(t ≥0), (a > 0)
0
(t < 0)
F
←→F(jω) =
1
a + jω .
(5.2-4)
Proof. The spectrum of a unilateral exponential signal is
F(jω) =
∞
∫
−∞
f(t)e−jωtdt =
∞
∫
0
e−ate−jωtdt = −
1
a + jω e−(a+jω)t󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨
∞
0
=
1
a + jω .
That is,
|F(jω)| =
1
√a2 + ω2 ,
(5.2-5a)
φ(ω) = −arctan(ω
a ) .
(5.2-5b)
They are illustrated in 󳶳Figure 5.4.
5.2.3 Bilateral exponential signals
The Fourier transform pair of a bilateral exponential signal as shown in 󳶳Figure 5.5 is
f(t) =
{
{
{
e−at
(t ≥0), (a > 0)
eat
(t < 0)
F
←→F(jω) =
2a
a2 + ω2
(5.2-6)
0
t
1
)
(t
f
at
e−
at
e
Fig. 5.5: Bilateral exponential signal.
Authenticated
32 PM

154
|
Analysis of continuous-time systems excited by aperiodic signals
0
ω
)
( ω
j
F
2
a
Fig. 5.6: Amplitude spectrum of a bilateral exponential signal.
Proof. The spectrum of the bilateral exponential signal is
F(jω) =
∞
∫
−∞
f(t)e−jωtdt =
0
∫
−∞
eate−jωtdt +
∞
∫
0
e−ate−jωtdt
=
1
a −jω e(a−jω)t󵄨󵄨󵄨󵄨󵄨
0
−∞−
1
a + jω e(a+jω)t󵄨󵄨󵄨󵄨󵄨
∞
0 =
2a
a2 + ω2 .
That is,
|F(jω)| =
2a
a2 + ω2 ,
(5.2-7a)
φ(ω) = 0 .
(5.2-7b)
Equation (5.2-7a) is sketched in 󳶳Figure 5.6.
5.2.4 Unit DC signals
A DC signal with size 1 is called the unit DC signal and is shown in 󳶳Figure 5.7; its
expression is
f(t) = 1
(−∞< t < ∞) .
Because it is not absolutely integrable, its frequency spectrum cannot be directly
found by equation (5.1-6). However, it can be considered as a limit form of a bilateral
exponential signal when a →0, so, the frequency spectrum can be also considered
as a limit form of the spectrum of the bilateral exponential signal as a →0. We have
F(jω) = lim
a→0
2a
a2 + ω2 =
{
{
{
∞
(ω = 0)
0
(ω
̸= 0)
.
Obviously, the spectrum function of a unit DC signal is an impulse function. Now, the
weight of the impulse function needs to be determined. As we know, it should be an
0
t
1
)
(t
f
at
e
at
e−
Fig. 5.7: A unit DC signal.
Authenticated
32 PM

5.2 Fourier transforms of typical aperiodic signals
|
155
0
ω
)
( ω
j
F
)
( π
2
Fig. 5.8: Spectrum of a unit DC signal.
integral
∞
∫
−∞
2a
a2 + ω2 dω =
∞
∫
−∞
2
1 + ( ω
a )2 d (ω
a ) = 2 arctgω
a
󵄨󵄨󵄨󵄨󵄨󵄨󵄨
∞
−∞= 2π ,
and, therefore, the spectrum of a unit DC signal is
F(jω) = 2πδ(ω) .
That is,
f(t) = 1
F
←→F(jω) = 2πδ(ω) .
(5.2-8)
The spectrum is plotted in 󳶳Figure 5.8.
5.2.5 Unit impulse signals
The spectrum function of a unit impulse signal, as shown in 󳶳Figure 5.9, is
F(jω) =
∞
∫
−∞
δ(t)e−jωtdt =
∞
∫
−∞
δ(t)dt = 1 ,
So, we have
δ(t)
F
←→F(jω) = 1
(5.2-9)
The spectrum is displayed in 󳶳Figure 5.10. Obviously, the spectrum occupies the fre-
quency range from −∞to +∞, and amplitude values of all frequency components of
the spectrum are the same. It explains why the impulse signal contains rich frequency
components or contributions of these components to the signal are the same. There-
fore the spectrum is also called the uniform spectrum or the white spectrum.
0
t
(1)
)
(t
δ
Fig. 5.9: A unit impulse signal.
0
ω
)
( ω
j
F
1
Fig. 5.10: Spectrum of a unit impulse signal.
Authenticated
32 PM

156
|
Analysis of continuous-time systems excited by aperiodic signals
5.2.6 Signum signals
Because the signum or sign signal as shown in 󳶳Figure 5.11 is not also absolutely inte-
grable, its frequency spectrum cannot be directly found by equation (5.1-6) either, but
it can be obtained by the limit form of the spectrum F1(jω) of f1(t), which is shown in
󳶳Figure 5.12 as a →0.
Since
f1(t) =
{
{
{
e−at
(t > 0)
−eat
(t < 0)
,
its frequency spectrum function is
F1(jω) = F[f1(t)] =
∞
∫
−∞
f1(t)e−jωtdt =
0
∫
−∞
(−eat)e−jωtdt +
∞
∫
0
e−ate−jωtdt
= −
1
a −jω +
1
a + jω = −
2ω
a2 + ω2 j .
Therefore, the spectrum function of the signum signal is
F(jω) = F[sgn(t)] = lim
a→0 F1(jω) = −lim
a→0
2ωj
a2 + ω2 = 2
jω .
So, we have
f(t) = sgn(t)
F
←→F(jω) = 2
jω .
(5.2-10)
Because
F(jω) = |F(jω)| ejφ(ω) ,
we have
|F(jω)| = 2
|ω| ,
(5.2-11a)
φ(ω) =
{
{
{
−π
2
(ω > 0)
π
2
(ω < 0)
.
(5.2-11b)
-1
t
1
)
sgn(t
0
Fig. 5.11: A sign signal.
0
1
)
(
1 t
f
-1
t
at
e
−
at
e−
Fig. 5.12: f1(t).
Authenticated
32 PM

5.2 Fourier transforms of typical aperiodic signals
|
157
0
0
ω
)
( ω
j
F
)
(ω
ϕ
ω
π
2
π
2
Fig. 5.13: Amplitude and phase spectrums of a sign signal.
The amplitude spectrum and the phase spectrum are illustrated in 󳶳Figure 5.13.
Note: Strictly speaking, the Fourier transform of the signum should be F(jω) =
2
jω +kδ(ω), where the impulse term represents a DC component in the signum. Because
the average value of the signal is zero in the time domain, that is, k = 0, F(jω) = 2
jω.
5.2.7 Unit step signals
The spectrum function of a unit step signal is
F(jω) = 1
jω + πδ(ω) .
Thus, we have
ε(t)
F
←→1
jω + πδ(ω) .
(5.2-12)
The proof of equation (5.2-12) and the spectrum waveform can be seen in Example 5.3-1.
Note: The DC, the signum, the step signals and the periodic signals are all power
signals. Their Fourier transforms all contain impulse signal terms, and this is a general
characteristic of power signals and can be used to test if a signal is a power signal in
the frequency domain.
Tab. 5.1: The Fourier transforms of typical signals.
No.
Name
Expression
Spectrum
1
Gate signal
gτ(t)
τSa( ωτ
2 )
2
Unilateral exponential
e−atε(t)
(a > 0)
1
a+jω
3
Bilateral exponential
e−a|t|ε(t)
(a > 0)
2a
a2+ω2
4
Unit DC
1
2πδ(ω)
5
Unit impulse
δ(t)
1
6
Sign signal
sgn(t)
2
jω
7
Unit step signal
ε(t)
1
jω + πδ(ω)
8
Triangle pulse signal
∆2τ(t) = (1 −|t|
τ ) [ε (t + τ
2 ) −ε (t −τ
2)]
τSa2 ( ωτ
2 )
9
Sine signal
sin(ω0t)
jπ[δ(ω + ω0) −δ(ω −ω0)]
10
Cosine signal
cos(ω0t)
π[δ(ω + ω0) + δ(ω −ω0)]
Authenticated
32 PM

158
|
Analysis of continuous-time systems excited by aperiodic signals
Finally, the Fourier transforms of typical signals are summarized in Table 5.1 for
reference.
5.3 Properties of the Fourier transform
It is helpful for calculation of the Fourier transform of a signal to learn properties of
it.
5.3.1 Linearity
If
F[f1(t)] = F1(jω),
F[f2(t)] = F2(jω) ,
so,
F[a1f1(t) + a2f2(t)] = a1F1(jω) + a2F2(jω) .
(5.3-1)
Example 5.3-1. Find the frequency spectrum of the unit step signal shown in
󳶳Figure 5.14a.
Solution. Because
ε(t) = 1
2 sgn(t) + f1(t) ,
according to the linearity property, we have
F(jω) = F [1
2 sgn(t) + f1(t)] = 1
2 ⋅2
jω + 1
2 ⋅2πδ(ω) = 1
jω + πδ(ω) .
That is,
|F(jω)| = 1
|ω| + πδ(ω) ,
φ(ω) =
{
{
{
−π
2
(ω > 0)
π
2
(ω < 0)
.
The amplitude spectrum and the phase spectrum are plotted in 󳶳Figure 5.15a and b.
0
0
0
1
)
(t
ε
)
(
1 t
f
t
t
t
2
1
−
2
1
)
sgn( t
2
1
2
1
(a)
(b)
(c)
Fig. 5.14: E5.3-1.
Authenticated
32 PM

5.3 Properties of the Fourier transform
|
159
0
ω
)
( ω
j
F
0
ω
)
(ω
ϕ
)
(π
2
π
2
π
−
(a)
(b)
Fig. 5.15: Amplitude and phase spectrum of a unit step signal.
5.3.2 Time shifting
If
F[f(t)] = F(jω) ,
then
F [f (t −t0)] = e−jωt0F (jω) ,
(5.3-2)
F [f (t + t0)] = ejωt0F (jω) .
(5.3-3)
Proof.
F [f (t −t0)] =
∞
∫
−∞
f (t −t0) e−jωtdt
t−t0=x
=
∞
∫
−∞
f(x)e−jω(x+t0)d(x + t0)
= e−jωt0
∞
∫
−∞
f(x)e−jωxdx = e−jωt0F(jω) .
Similarly, this is proved by
F[f(t + t0)] = ejωt0F(jω) .
Time shifting shows that if a signal in the time domain is delayed time t1, then its am-
plitude spectrum stays the same, but the phase spectrum is shifted, −ωt1. The details
are given below.
If
F[f(t)] = F1(jω),
F[f(t −t1)] = F2(jω) ,
then
F2(jω) = e−jωt1F1(jω) = e−jωt1 |F1(jω)| ejφ1(ω) = |F1(jω)| ej[φ1(ω)−ωt1] ,
(5.3-4)
and
F2(jω) = |F2(jω)| ejφ2(ω) .
(5.3-5)
Comparing equation (5.3-4) and equation (5.3-5), we have
|F2(jω)| = |F1(jω)| ,
(5.3-6)
φ2(ω) = φ1(ω) −ωt1 .
(5.3-7)
Authenticated
32 PM

160
|
Analysis of continuous-time systems excited by aperiodic signals
0
0
1
t
t
T
1
)
(t
f
T
−
)
(t
g τ
2
τ
−
2
τ
−
2
τ
2
τ
(a)
(b)
Fig. 5.16: E5.3-3.
Example 5.3-2. Find the Fourier transform of a signal δ(t −t0).
Solution. We know that
F[δ(t)] = 1 ,
so, we have
F[δ(t −t0)] = e−jωt0 ⋅1 = e−jωt0 .
Example 5.3-3. Find the spectrum of a signal f(t) with three rectangular pulses shown
in 󳶳Figure 5.16a.
Solution. Itisknownthatthespectrum of a gatefunction gτ(t)shownin 󳶳Figure5.16b
G(jω) = τSa ( ωτ
2 ) ,
and, then
f(t) = gτ(t) + gτ(t + T) + gτ(t −T) .
According to the time shifting property, we have
F(jω) = G(jω) (1 + ejωT + e−jωT) = (1 + 2 cos ωT)τSa ( ωτ
2 ) .
5.3.3 Frequency shifting
If
F[f(t)] = F(jω) ,
then
F[f(t)ejω0t] = F[j(ω −ω0)] ,
(5.3-8)
F[f(t)e−jω0t] = F[j(ω + ω0)] .
(5.3-9)
Proof.
F[f(t)ejω0t] =
∞
∫
−∞
f(t)ejω0te−jωtdt =
∞
∫
−∞
f(t)e−j(ω−ω0)tdt = F[j(ω −ω0)] .
Similarly, it can be proved by
F[f(t)e−jω0t] = F[j(ω + ω0)] .
Authenticated
32 PM

5.3 Properties of the Fourier transform
|
161
t
1
0
)
(t
f
t
1
0
)
(t
gτ
t
1
0
t
0
ω
cos
0
cos
t
ω
0
ω
τ
(j )
G ω
0
ω
(j )
F
ω
2
τ
0
ω
ω
−
-1
-1
2
τ
−
2
τ
2
τ
−
2
τ
(a)
(b)
(c)
(d)
(e)
Fig. 5.17: E5.3-4.
Example 5.3-4. Find the spectrum of the high frequency pulse f(t) shown in
󳶳Figure 5.17a.
Solution. The high frequency pulse signal can be regarded as the product of the gate
signal shown in 󳶳Figure 5.17b and the cosine signal shown in 󳶳Figure 5.17c, that is,
f(t) = gτ(t) ⋅cos ω0t = 1
2[gτ(t)ejω0t + gτ(t)e−jω0t] .
Because F[gτ(t)] = G(jω) = τSa ( ωτ
2 ) as shown in 󳶳Figure 5.17d is known, from lin-
earity and frequency shifting, the spectrum of a high frequency pulse signal is
F(jω) = τ
2 {Sa [ τ(ω −ω0)
2
] + Sa [ τ(ω + ω0)
2
]} .
Its waveform is shown in 󳶳Figure 5.17e.
5.3.4 Time scaling
If
F[f(t)] = F(jω) ,
then
F[f(at)] = 1
|a| F (jω
a ) .
(5.3-10)
Proof. When a > 0, we have
F[f(at)] =
∞
∫
−∞
f(at)e−jωtdt
at=x
=
1
a
∞
∫
−∞
f(x)e−j ω
a xdx = 1
a F (jω
a )
When a < 0, we have
F[f(at)] =
∞
∫
−∞
f(at)e−jωtdt
at=x
=
1
a
−∞
∫
∞
f(x)e−j ω
a xdx = −1
a F (jω
a ) .
Authenticated
32 PM

162
|
Analysis of continuous-time systems excited by aperiodic signals
Integrating to the above two cases, the expression of the scaling transform is obtained
by
F[f(at)] = 1
|a| F (jω
a ) .
Equation (5.3-10) states that the width of a signal compressed to 1
a times of this original
signal (a > 1) in the time domain is equivalent to its spectrum width being extended a
times in the frequency domain, while its amplitude is reduced to the original 1
a times.
Conversely, the width of a signal is extended to the original one a times (0 < a < 1) in
the time domain, which is equivalent to its spectrum width being compressed 1
a times,
while its amplitude is increased to the original one a times in the frequency domain.
In a nutshell, the compression effect of a signal in the time domain is equiva-
lent to its expansion effect in the frequency domain; the expansion effect in the
time domain is equivalent to that of compression in the frequency domain. The
property is explained in 󳶳Figure 5.18 with a rectangular pulse signal.
In particular, when a = −1, we have
F[f(−t)] = F(−jω) .
(5.3-11)
Equation (5.3-11) is also called the time inversion theorem.
A living example of scaling property is that when a sound recorder set works with
a speed of twice the normal speed, the signal waveform in the time domain is com-
pressed, thehighfrequency components areenhanced, so, squealnoiseresults. Other-
wise, the sound becomes lower and deeper with a working speed of half of the normal
t
1
0
)
2
( t
f
4
τ
4
τ
−
ω
0
τ
π
4
τ
π
8
)
2
(
2
1
ω
j
F
2
a =
0
t
1
0
ω
τ
τ
−
)
2
( t
f
τ
2
)
2
(
2
ω
j
F
τ
π
2
τ
π
τ
π
2
ω
τ
0
t
1
0
)
( ω
j
F
)
(t
f
τ
π
2
τ
π
4
1
a =
1
2
a =
2
τ
−
2
τ
2
τ
(a)
(b)
(c)
Fig. 5.18: Scaling transform for a rectangle pulse signal.
Authenticated
32 PM

5.3 Properties of the Fourier transform
|
163
speed. This feature tells us that if a signal is compressed in the time domain to im-
prove the speed of information transmission, a communication system must provide
a wider passband for it in the frequency domain.
5.3.5 Symmetry
If
F[f(t)] = F(jω) ,
then
F[F(jt)] = 2πf(−ω) .
(5.3-12)
Proof. Because
f(t) = 1
2π
∞
∫
−∞
F(jω)ejωtdω ,
f(−t) = 1
2π
∞
∫
−∞
F(jω)e−jωtdω ,
and exchanging the variables ω and t, we have
∞
∫
−∞
F(jt)e−jωtdt = 2πf(−ω) ,
so,
F[F(jt)] = 2πf(−ω) .
We can see that F[F(jt)] = 2πf(ω) from equation (5.3-12) when f(t) is even.
Example 5.3-5. Find the spectrum of a sampling signal Sa(t) = sin t
t .
Solution. The spectrum of a gate signal gτ(t) with amplitude 1 and width τ is
gτ(t)
F
←→τSa ( ωτ
2 ) .
Letting τ = 2, we have
g2(t)
F
←→2Sa(ω) .
That is,
1
2g2(t)
F
←→Sa(ω) .
According to the symmetry property and paying attention to the fact that g2(t) is even,
we have
Sa(t)
F
←→2π 1
2 g2(ω) = πg2(ω) .
Therefore, the spectrum of a sampling signal is
F(jω) = πg2(ω) .
Authenticated
32 PM

164
|
Analysis of continuous-time systems excited by aperiodic signals
0
t
1
0
ω
2
π
)
(
2
ω
Sa
π
1
-1
0
t
1
0
ω
π
π
1
-1
)
(t
Sa
π
)
(
2 ω
πg
The  symmetry of a sampling signal and a gate signal
0
t
(1)
0
ω
1
1
-1
0
t
1
0
ω
(2 )
)
(t
F
The  symmetry of a DC signal and an impulse signal
)
(
2
ω
πδ
π
)
(t
δ
(j )
F
ω
2( )
g t
(a)
(b)
–
–
← →
← →
← →
← →
Fig. 5.19: Fourier transform symmetry.
We can see that the spectrum of a gate signal is a sampling signal, and the spectrum
of a sampling signal is a gate signal [as shown in 󳶳Figure 5.19a]. This is an important
conclusion and states that for a time limited signal (like a gate signal in the time do-
main), the frequency range of its spectrum is inﬁnite in the frequency domain (like a
sampling signal in the frequency domain); for a frequency limited signal (like a gate
signal in frequency domain), the time range of its original signal is inﬁnite in the time
domain (like a sampling signal in the time domain). It is very helpful to understand
the ISI concept in the communication principles course.
Another analogical case is that the spectrum of a unit impulse signal is a DC sig-
nal, and the spectrum of a DC signal is an impulse signal, as shown in 󳶳Figure 5.19b.
That is,
δ(t)
F
←→1 ,
1
F
←→2πδ(ω) .
Features of signals in two domains based on the symmetry property are listed in Ta-
ble 5.2.
Authenticated
32 PM

5.3 Properties of the Fourier transform
|
165
Tab. 5.2: Features of signals in two domains based on the symmetry property.
NO.
Time domain
Frequency domain
1
Periodic
F
←→
Discrete
Discrete
F
←→
Periodic
2
Continuous
F
←→
Nonperiodic
Nonperiodic
F
←→
Continuous
3
Inﬁnite lasting time
F
←→
Limited frequency band
Limited lasting time
F
←→
Inﬁnite frequency band
5.3.6 Properties of convolution
1. Convolution in the time domain
If
F[f1(t)] = F1(jω) ,
F[f2(t)] = F2(jω) ,
then
F [f1(t) ∗f2(t)] = F1 (jω) F2 (jω) .
(5.3-13)
Proof.
F [f1(t) ∗f2(t)] =
∞
∫
−∞
[f1(t) ∗f2(t)]e−jωtdt =
∞
∫
−∞
[
[
∞
∫
−∞
f1(τ)f2(t −τ)dτ]
]
e−jωtdt
=
∞
∫
−∞
f1(τ) [
[
∞
∫
−∞
f2(t −τ)e−jωtdt]
]
dτ =
∞
∫
−∞
f1(τ)e−jωτF2(jω)dτ
= F2(jω)
∞
∫
−∞
f1(τ)e−jωτdτ = F1(jω)F2(jω)
Example 5.3-6. Find the Fourier transform of a signal f(t) = sin(t−t0)
2t−2t0 .
Solution. Because
f(t) = 1
2 Sa(t) ∗δ (t −t0) ,
and
Sa(t)
F
←→πg2(ω) ,
δ (t −t0)
F
←→e−jωt0 .
Authenticated
32 PM

166
|
Analysis of continuous-time systems excited by aperiodic signals
0
1
t
)
(t
f
)
(t
gτ
t
)
(
1 t
f
(1)
1t
−
1
2
t
τ
+
−
1
2
t
τ
−
−
1t
1
2
t
τ
−
1
2
t
τ
+
0
1
t
2
τ
−
2
τ
1t
−
1t
(1)
0
(a)
(b)
(c)
Fig. 5.20: E5.3-7.
Thus, according to the time domain convolution property the Fourier transform is
F(jω) = 1
2F[Sa(t)] ⋅F [δ (t −t0)] = 1
2 πg2(ω) ⋅e−jωt0 .
Example 5.3-7. Find the spectrum of the signal shown in 󳶳Figure 5.20a.
Solution. The signal shown in 󳶳Figure 5.20a can be regarded as the convolution of
a gate signal shown in 󳶳Figure 5.20b and the signal f1(t) shown in 󳶳Figure 5.20c,
namely,
f(t) = gτ(t) ∗f1(t) = gτ(t) ∗[δ (t + t1) + δ (t −t1)] .
Therefore, according to the time domain convolution property, we have
F (jω) = F [gτ(t)] ⋅F [f1(t)]
= τSa ( ωτ
2 ) ⋅(ejωt1 + e−jωt1) = 2τSa ( ωτ
2 ) cos ωt1 .
2. Convolution in the frequency domain
If
F [f1(t)] = F1(jω) ,
F [f2(t)] = F2(jω) ,
then
F [f1(t) ⋅f2(t)] = 1
2π F1(jω) ∗F2(jω) .
(5.3-14)
Proof.
F−1 [F1(jω) ∗F2(jω)] = 1
2π
∞
∫
−∞
[
[
∞
∫
−∞
F1(jη)F2[j(ω −η)]dη]
]
ejωtdω
= 1
2π
∞
∫
−∞
F1(jη)
{
{
{
∞
∫
−∞
F2[j(ω −η)ejωt]dω
}
}
}
dη
=
∞
∫
−∞
F1(jη)f2(t)ejηtdη = f2(t)
∞
∫
−∞
F1(jη)ejηtdη
= 2πf1(t)f2(t)
Authenticated
32 PM

5.3 Properties of the Fourier transform
|
167
0
t
E
2
τ
−
2
τ
)
(t
f
Fig. 5.21: E5.3-8.
Example 5.3-8. Find the spectrum of the cosine pulse signal shown in 󳶳Figure 5.21.
f(t) =
{
{
{
E cos ( πt
τ )
(|t| ≤τ
2)
0
(|t| > τ
2)
.
Solution. The signal f(t) can be regarded as the product of a rectangular pulse signal
Egτ(t) and a cosine signal cos( πt
τ ), namely,
f(t) = Egτ(t) cos (πt
τ ) .
Moreover,
F [cos (πt
τ )] = 1
2F [e−j π
τ t + ej π
τ t] = 1
2 [2πδ (ω + π
τ ) + 2πδ (ω −π
τ )]
= πδ (ω + π
τ ) + πδ (ω −π
τ ) .
Thus, according to the convolution property in the frequency domain, we have
F[f(t)] = F [gτ(t) ⋅cos (πt
τ )] = 1
2π F [gτ(t)] ∗F [cos ( πt
τ )]
= 1
2π [EτSa ( ωτ
2 )] ∗[πδ (ω + π
τ ) + πδ (ω −π
τ )]
= 1
2 Eτ {Sa [ τ
2 (ω + π
τ )] + Sa [ τ
2 (ω −π
τ )]} = 2Eτ
π
cos ( ωτ
2 )
1 −( ωτ
π )2
5.3.7 Differentiation in the time domain
If
F[f(t)] = F(jω) ,
then
F [df(t)
dt ] = jωF(jω) .
(5.3-15)
Proof. Because
f(t) = 1
2π
∞
∫
−∞
F(jω)ejωtdω
Authenticated
32 PM

168
|
Analysis of continuous-time systems excited by aperiodic signals
and
df(t)
dt
= 1
2π
d
dt
∞
∫
−∞
F(jω)ejωtdω = 1
2π
∞
∫
−∞
[jωF(jω)]ejωtdω ,
so,
F [df(t)
dt ] = jωF(jω) .
This result can be also generalized to the following situation with an nth-order deriva-
tive:
F [dnf(t)
dtn ] = (jω)nF(jω) .
(5.3-16)
The differential property shows that the effect of differentiating a signal in the time
domain is equivalent to strengthening the sizes of high frequency components of the
signal in the frequency domain. In fact, the effect is to sharpen the signal waveform in
the time domain.
Example 5.3-9. Find the spectrum of the triangle pulse f(t) shown in 󳶳Figure 5.22.
f(t) =
{
{
{
E (1 −2|t|
τ )
(|t| < τ
2)
0
(|t| > τ
2)
.
Solution. The ﬁrst-order derivative function of the triangle pulse f(t) is
f 󸀠(t) = 2E
τ [ε (t + τ
2) −ε(t)] −2E
τ [ε(t) −ε (t −τ
2)] ,
and the second-order derivative function is
f 󸀠󸀠(t) = 2E
τ [δ (t + τ
2) + δ (t −τ
2) −2δ(t)] .
Thus, we have
F [f 󸀠󸀠(t)] = 2E
τ (ejω τ
2 + e−jω τ
2 −2) = 4E
τ [cos ( ωτ
2 ) −1] = −8E
τ sin2 ωτ
4 .
According to the differential property in the time domain, we have
F [f 󸀠󸀠(t)] = (jω)2F(jω),
F(jω) = F[f(t)] ,
so,
F(jω) =
1
(jω)2 F [f 󸀠󸀠(t)] = −
1
(jω)2 ⋅8E
τ sin2 ωτ
4 = Eτ
2 Sa2 ( ωτ
4 ) .
E
τ
E
2
τ
E
2
−
)
(
'' t
f
)
(
' t
f
)
(t
f
2
(
)
E
τ
0
t
t
2
τ
0
t
2
τ
−
2
τ
0
2
τ
−
2
(
)
E
τ
4
(
)
E
τ
2
τ
−
2
τ
(a)
(b)
(c)
Fig. 5.22: E5.3-9.
Authenticated
32 PM

5.3 Properties of the Fourier transform
|
169
5.3.8 Integration in the time domain
If
F[f(t)] = F(jω) ,
then
F [
[
t
∫
−∞
f(τ)dτ]
]
= 1
jω F(jω) + F(0)πδ(ω) .
(5.3-17)
Where, F(0) = F(jω)|ω=0 = ∫
∞
−∞f(t)dt.
Proof. We know that
f(t) ∗ε(t) =
t
∫
−∞
f(τ)dτ ,
so,
F [
[
t
∫
−∞
f(τ)dτ]
]
= F [f(t) ∗ε(t)] = F(jω) ⋅F[ε(t)] = F(jω) [ 1
jω + πδ(ω)] .
Moreover,
F(jω)δ(ω) = F(0)δ(ω) ,
so,
F [
[
t
∫
−∞
f(τ)dτ]
]
= 1
jω F(jω) + F(0)πδ(ω) .
The integral feature shows that a signal integrated in the time domain is equivalent
to enhancing its low frequency components in the frequency domain and reducing
its high frequency components. In fact, effect of the integral feature is to smooth the
waveform of a signal in the time domain. This is the theoretical basis of ∆-Σ modula-
tion technology in the communication principles course.
Example 5.3-10. Find the spectrum of the signal f(t) shown in 󳶳Figure 5.23a.
f(t) =
{
{
{
{
{
{
{
0
(t < 0)
t
(0 ≤t ≤1)
1
(t > 1)
0
1
t
t
0
)
(t
f
)
(t
y
1
1
1
(a)
(b)
Fig. 5.23: E5.2-10.
Authenticated
32 PM

170
|
Analysis of continuous-time systems excited by aperiodic signals
Solution. f(t) can be regarded as an integration of a rectangular pulse y(t), as shown
in 󳶳Figure 5.23b, namely
f(t) =
t
∫
−∞
y(τ)dτ ,
and
Y(jω) = F[y(t)] = Sa ( ω
2 ) e−j ω
2 .
It is known that
Y(0) = 1 ,
so, from the integration property, we have
F(jω) = 1
jω Y(jω) + πY(0)δ(ω) = 1
jω Sa ( ω
2 ) e−j ω
2 + πδ(ω) .
The properties of differentiation and integration in the time domain can be used to
change differentiation or integration equation in the time domain into an algebraic
equation in the frequency domain, which is very useful in the system analysis.
5.3.9 Modulation
If a signal f(t) is multiplied by the cosine cos ω0t, what will be the result? Let s(t) =
f(t) cos ω0t and its spectrum be S(jω). Then, based on Euler’s relation and the fre-
quency shifting property we have
S(jω) = F [f(t) cos ω0t] = F [1
2 f(t)ejω0t + 1
2 f(t)e−jω0t]
= 1
2 F [j (ω −ω0)] + 1
2F [j (ω + ω0)]
(5.3-18)
This shows that the result of f(t) being multiplied by cos ω0t in the time domain is
equivalent to the spectrum F(jω) of f(t) being shifted to points ±ω0 on the frequency
axis in the frequency domain, and its shape remains the same and the amplitude value
is only half of the original one. Thus, an oscillation signal whose amplitude changes
with the signal f(t) in the time domain results. This is called the modulation property
(theorem) and is further illustrated by Example 5.3-4.
In communication technology, this feature is often used to accomplish a process
called modulation, in which a low frequency signal f(t) (called a modulating signal) is
multiplied by a high frequency cosine signal (called a carrier). Thus, the waveform of
f(t) is modulated (placed) to the amplitude of the carrier cos ωct, and a high frequency
signal s(t) (called a modulated signal) forms, whose amplitude values contain wave-
form information of f(t); in other words, the spectrum of the low frequency signal f(t)
is shifted to two higher frequency places ±ωc on the frequency axis. The modulation
model is depicted in 󳶳Figure 5.24.
Authenticated
32 PM

5.3 Properties of the Fourier transform
|
171
( )
f t
c
cos
t
ω
c
( )
( )cos
s t
f t
t
ω
=
Fig. 5.24: Modulation model.
The main purpose of modulation is to transform a low frequency signal into a
high frequency signal, which can facilitate radioing with a size limited antenna or
frequency division multiplexing. Therefore, the modulation theorem is an important
theoretical foundation of radio communication and frequency division multiplexing
technologies.
5.3.10 Conservation of energy
Energy calculation ways of a nonperiodic signal in the time and frequency domains
are also related by Parseval’s relation
E =
+∞
∫
−∞
|f(t)|2 dt = 1
2π
+∞
∫
−∞
|F(ω)|2 dω =
+∞
∫
−∞
|F(f)|2 df .
(5.3-19)
The term |F(f)|2 represents the energy on the unit band, which can reﬂect the relative
size of the signal energy on different frequencies, that is, the energy distribution. Thus,
it is regarded as the energy spectral density or, simply, ESD, and it is denoted as E(f)
or E(ω),
E(f) = |F(f)|2
or
E(ω) = |F(ω)|2
(5.3-20)
Obviously, the ESD is associated only with the amplitude spectrum and has nothing
to do with the phase spectrum, and it is a nonnegative real even function. Thus, equa-
tion (5.3-19) can be written as
E = 1
π
+∞
∫
0
E(ω)dω = 2
+∞
∫
0
E(f)df .
(5.3-21)
We can prove that for a gate signal with time width τ, the energy within the range
from the ﬁrst zero point (ω = 2π
τ ) in its spectrum to the origin occupies around 90.3%
of the total energy of this signal. This means that the main energy is concentrated on
the range from the zero frequency point to the ﬁrst spectral zero point. This point also
ﬁts for the periodic rectangle pulse and the triangle pulse, etc. As a result, in commu-
nication systems, only the low frequency components within ω = 0 ∼2π
τ of a signal
are transferred in general, which is usually called the effective frequency band width
of the signal, denoted as Bω and Bω = 2π
τ ; its unit is rad/s, or Bf = 1
τ, with unit Hz.
Obviously, the effective frequency bandwidth Bω is inversely proportional to the pulse
duration τ for a rectangle pulse, which means the shorter the signal duration in the
Authenticated
32 PM

172
|
Analysis of continuous-time systems excited by aperiodic signals
Tab. 5.3: Properties of Fourier transforms.
No. Name
f(t) (time domain)
F(jω) (frequency domain)
1
Deﬁnition
f(t) =
1
2π ∫
∞
−∞F(jω)ejωtdω
F(jω) = ∫
∞
−∞f(t)e−jωtdt
2
Linearity
a1f1(t) + a2f2(t)
a1F1(jω) + a2F2(jω)
3
Time shifting
f(t ± t0)
e±jωt0 F(jω)
4
Frequency shifting
f(t)e±jω0t
F[j(ω ∓ω0]
5
Scaling transform
f(at)(a
̸= 0)
1
|a| F(j ω
a )
6
Symmetry
F(jt)
2πf(−ω)
7
Convolution
theorem
Time domain
f1(t) ∗f2(t)
F1(jω)F2(jω)
Frequency domain
f1(t) ⋅f2(t)
1
2π F1(jω) ∗F2(jω)
8
Differential in the time domain
f (n)(t)
(jω)nF(jω)
9
Integral in the time domain
∫
t
−∞f(τ)dτ
1
jω F(jω) + πF(0)δ(ω)
10
Differential in the frequency domain
(−jt)nf(t)
F (n)(jω)
11
Integral in the frequency domain
j f(t)
t
+ πf(0)δ(t)
∫
∞
−∞F(jΩ)dΩ
12
Parseval’s relation
E = ∫
+∞
−∞
󵄨󵄨󵄨󵄨f(t)󵄨󵄨󵄨󵄨
2 dt =
1
2π ∫
+∞
−∞|F(ω)|2 dω = ∫
+∞
−∞
󵄨󵄨󵄨󵄨F(f)󵄨󵄨󵄨󵄨
2 df
time domain, the wider the frequency band becomes in the frequency domain. This
conclusion ﬁts well for power signals, so it is of great importance in the communica-
tion principles course.
Finally, we summarize the properties of the Fourier transform in Table 5.3.
5.4 Fourier transforms of periodic signals
We know that the Fourier transform was put forward to analyze nonperiodic signals.
So, does the Fourier transform of a periodic signal exist?
As mentioned above, the sufficient condition for the existence of the Fourier trans-
form of a signal is that it is absolutely integrable. However, general periodic signals
cannot meet this condition, so their Fourier transforms cannot be found directly from
the deﬁnition formula but can be obtained indirectly by using a singularity signal—the
impulse signal.
If the spectrum of a periodic signal f(t) with period T is Fn = 1
T ∫
T
2
−T
2 f(t)e−jnω0tdt,
so its Fourier series is
f(t) =
∞
∑
n=−∞
Fnejnω0t .
Authenticated
32 PM

5.4 Fourier transforms of periodic signals
|
173
Fourier transformation on both sides of the equation leads to
F [f(t)] = F [
∞
∑
n=−∞
Fnejnω0t] =
∞
∑
n=−∞
FnF [ejnω0t] .
According to frequency shifting, we have
F [ejnω0t] = 2πδ(ω −nω0) .
So, the Fourier transform of f(t) is
F [f(t)] = 2π
∞
∑
n=−∞
Fnδ(ω −nω0) .
(5.4-1)
Equation (5.4-1) shows that the Fourier transform or spectrum density of a periodic
signal is composed by inﬁnite shifted impulse signals that are located at harmonic
frequency points nω0 and have the weights 2πFn.
Example 5.4-1. Find the Fourier transform of a periodic impulse signal (also called
the train of impulses or the unit comb) δT(t) = ∑+∞
n=−∞δ(t −nT) (see 󳶳Figure 5.25a).
Solution. Fn of δT(t) is
Fn = 1
T
T
2
∫
−T
2
δT(t)e−jnω0tdt = 1
T
T
2
∫
−T
2
δ(t)e−jnω0tdt = 1
T .
Equation (5.3-1) yields
F [δT(t)] = 2π
∞
∑
n=−∞
Fnδ(ω −nω0) = 2π
T
∞
∑
n=−∞
δ(ω −nω0) = ω0
∞
∑
n=−∞
δ(ω −nω0) .
If δω0(jω) = ∑∞
n=−∞δ(ω −nω0), then we have
δT(t)
F
←→ω0δω0(jω) .
(5.4-2)
This states that the Fourier transform of a train of impulses with cycle T, intensity 1, in
the time domain is still a train of impulses with cycle ω0 = 2π
T intensity ω0 = 2π
T in the
frequency domain (see 󳶳Figure 5.25b).This result can be used to prove the “sampling
theorem”.
0
(1)
t
0
T
( )
T t
δ
2T
3T
-T
-2T
-3T
0
(
)
ω
0
ω
0
2ω
0
ω
−
0
2ω
−
ω
0
0
( )
ω
ω δ
ω
(a)
(b)
Fig. 5.25: E5.4-1.
Authenticated
32 PM

174
|
Analysis of continuous-time systems excited by aperiodic signals
It can be proved that if a signal is absolutely integrable, there is no any impulse com-
ponent in its frequency spectrum; if there is impulse component in the frequency spec-
trum, this signal must be periodicity or have the DC component in the time domain.
5.5 Solutions for the inverse Fourier transform
If the Fourier transform F(jω) of a signal f(t) is known, in many cases the original
signal f(t) can be obtained by F(jω), which means that the inverse Fourier transform
operation needs to be discussed.
It was ﬁrst thought that the inverse Fourier transform is calculated by using equa-
tion (5.1-3), f(t) =
1
2π ∫
∞
−∞F(jω)ejωtdω, but this integral is actually too complex to
calculate in general, therefore, this method is not very commonly used. Usually, the
Fourier transform properties and the Fourier transforms of some typical signals can
be used to ﬁnd the inverse Fourier transforms.
Example 5.5-1. Find the original functions f(t) of the following spectrums.
(1) F(jω) = ω2
(2) F(jω) = δ(ω −2)
(3) F(jω) = 2 cos ω
Solution. (1) According to the Fourier transform of δ(t) and the differential property,
we have
ω2 = −(jω)2 × 1
F
←→−δ󸀠(t) ,
so,
f(t) = −δ󸀠(t) .
(2) According to the Fourier transform of the DC signal and the frequency shifting
property, we have
1
F
←→2πδ(ω) ,
1
2π ej2t
F
←→δ(ω −2) ,
so,
f(t) = 1
2π ej2t .
(3) Because cos ω0t = 1
2(ejω0t + e−jω0t), according to the Fourier transform of the DC
signal and the time shifting property, we have
cos 1t = 1
2(ejt + e−jt)
F
←→π[δ(ω −1) + δ(ω + 1)] ,
and according to the symmetry property, we have
2π(1
π cos ω)
F
←→δ(t −1) + δ(t + 1) ,
so,
f(t) = δ(t −1) + δ(t + 1) .
Authenticated
32 PM

5.6 System analysis methods for aperiodic signals
|
175
5.6 System analysis methods for aperiodic signals
As mentioned above, a nonperiodic signal can be converted into a continuous sum of
imaginary exponential signals or sinusoidal signals by using the Fourier transform,
and the spectrum density function of a signal is also introduced from it. So, what is
the signiﬁcance of this knowledge to system analysis? Or what are the advantages of
solving a linear system model with the Fourier transform?
Usually, a nonperiodic signal exists only in a certain time interval. In order to illus-
trate expediently the method of solving the system response to a nonperiodic signal
such as excitation, we assume that the starting state of the system is zero. Thus, we
only discuss the problem of ﬁnding the zero-state response in this chapter.
In Chapter 4, the phasors’ ratio of a response and an excitation was deﬁned as
the system function based on the characteristic of which a system’s response to a si-
nusoidal signal is still a sinusoidal signal with the same frequency. Thus, a bridge be-
tween the excitation and the response is built by this function. Then, under the action
of a nonperiodic signal, can the system’s excitation and the response also be linked
with the system function or not? Three aspects will be discussed.
5.6.1 Analysis method from system models
Suppose that the excitation and the zero-state response of a nth-order LTI system are,
respectively, f(t) and yf(t), the mathematical model of this system is
an
dnyf(t)
dtn
+ an−1
dn−1yf(t)
dtn−1
+ ⋅⋅⋅+ a1
dyf(t)
dt
+ a0yf(t)
= bm
dmf(t)
dtm
+ ⋅⋅⋅+ b1
df(t)
dt
+ b0f(t) .
(5.6-1)
Taking the Fourier transform on both sides of equation (5.6-1) and making Yf(jω) =
F [yf(t)], F(jω) = F[f(t)], from the linearity and differential properties of the Fourier
transform, we obtain
[an (jω)n + an−1(jω)n−1 + ⋅⋅⋅+ a1(jω) + a0] Yf(jω)
= [bm(jω)m + bm−1(jω)m−1 + ⋅⋅⋅+ b1(jω) + b0] F(jω) .
(5.6-2)
Thus, the Fourier transform of the zero-state response is
Yf(jω) = bm(jω)m + bm−1(jω)m−1 + ⋅⋅⋅+ b1(jω) + b0
an(jω)n + an−1(jω)n−1 + ⋅⋅⋅+ a1(jω) + a0
F(jω) .
(5.6-3)
Authenticated
32 PM

176
|
Analysis of continuous-time systems excited by aperiodic signals
Like in Chapter 4, we deﬁne the ratio of the Fourier transforms of the zero-state re-
sponse and the excitation as the system function, which is still expressed as
H(jω)
def= Yf(jω)
F(jω) .
(5.6-4)
Thus, the Fourier transform of the zero-state response of a system can be written as
Yf(jω) = H(jω)F(jω) ,
(5.6-5)
where
H(jω) = bm(jω)m + bm−1(jω)m−1 + ⋅⋅⋅+ b1(jω) + b0
an(jω)n + an−1(jω)n−1 + ⋅⋅⋅+ a1(jω) + a0
.
(5.6-6)
Equation (5.6-6) states that the system function only depends on the structure and the
element parameters of a system but does not relate to the excitation or the response
signals. Obviously, we obtained a similar result with the case of applying a periodic
signal to a system, which is not surprise to us, because a nonperiodic signal can be ex-
pressed as a continuous sum of sinusoidal signals by means of the Fourier transform,
the excitation and response of a system are still sinusoidal signals.
Equation (4.4-1) is different from equation (5.6-4) on the surface, because the sys-
tem function is deﬁned by means of the Fourier transform (spectrum) when a system
is acted upon by a nonperiodic signal, but it is deﬁned by using the phasor under the
action of a periodic signal. In fact, their essences are the same, because a phasor is ac-
tually another kind of simpliﬁed manifestation of the spectrum of a sinusoidal signal
with a single frequency.
Equation(5.6-5)is justthecontributionof theFourier transform to thesystem anal-
ysis. That is, the Fourier transform of the zero-state response of an LTI system to an
aperiodic excitation equals the product of the Fourier transform of the input signal
and the system function.
5.6.2 Analysis with the system function
We know that if the excitation is δ(t), the corresponding zero-state response is the
impulse response h(t). We have
F(jω) = F[δ(t)] = 1 ,
Yf(jω) = F[h(t)] .
According to equation (5.6-5), we have
Yf(jω) = F(jω)H(jω) = H(jω) ,
and we obtain
F[h(t)] = H(jω) .
(5.6-7)
Authenticated
32 PM

5.6 System analysis methods for aperiodic signals
|
177
Equation (5.6-7) reﬂects an important relationship between the system impulse re-
sponse and the system function, that is, they are a Fourier transform pair. It can be
expressed as
h(t)
F
←→H(jω) ,
H(jω) = F[h(t)] =
∞
∫
−∞
h(t)e−jωtdt ,
(5.6-8)
h(t) = F−1[H(jω)] = 1
2π
∞
∫
−∞
H(jω)ejωtdω .
(5.6-9)
The signiﬁcance of the conclusion is as follows:
If the structure of a system is not intuitive or invisible, the system function cannot
be obtained directly from constraint conditions of components and the system (cir-
cuit), but it can be found indirectly by the impulse response of the system.
From time domain analysis we know that the zero-state response yf(t) of an LTI
system to an excitation f(t) is the convolution of the impulse response h(t) and f(t),
yf(t) = f(t) ∗h(t) .
(5.6-10)
Letting Yf(jω) = F[yf(t)], F(jω) = F[f(t)], taking the Fourier transform on both sides
of equation (5.6-10) at same time and using the convolution theorem, we have
F[yf(t)] = F[f(t) ∗h(t)] = F[f(t)] ⋅F[h(t)]
or
Yf(jω) = F(jω)H(jω) .
(5.6-11)
This conclusion proves the correctness of equation (5.6-5) from another point of view.
5.6.3 Analysis with signal decomposition
The zero-state response of a system to a nonperiodic signal is deduced based on the
decomposition and synthesis of a signal as follows.
From equation (5.1-7), an aperiodic signal f(t) can be expressed as a linear com-
bination of inﬁnite imaginary exponential signals as ejωt. Therefore, ejωt is a kind of
basic signal, and ﬁrst of all we must ﬁnd the zero-state response yf1(t) of the system
to signal ejωt.
Because an excitation f1(t) = ejωt, from equation (5.6-10) we have
yf1(t) = h(t) ∗ejωt =
∞
∫
−∞
h(τ)ejω(t−τ)dτ =ejωt
∞
∫
−∞
h(τ)e−jωτdτ .
The term ∫
∞
−∞h(τ)e−jωτdτ = ∫
∞
−∞h(t)e−jωtdt is the Fourier transform H(jω) of h(t), so
yf1(t) = H(jω)ejωt .
(5.6-12)
Authenticated
32 PM

178
|
Analysis of continuous-time systems excited by aperiodic signals
ω
ω
ω
ω
A LTI system
(zero-state)
Excitation 
j
1
j
j
( )
1
(j )
d
2
1
(j )
d
2
( )
t
t
t
f t
e
F
e
F
e
f t
ω
ω
ω
ω
ω
π
ω
ω
π
∞
−∞
=
∫
j
f1
j
j
( )
(j )
1
(j )
(j )
d
2
1
(j )
(j )
d
2
t
t
t
y
t
H
e
F
H
e
F
H
e
yf(t) =   –1[F(jω)H(j   )]
ω
ω
ω
ω
ω
ω
ω
π
π
∞
−∞
=
∫
(Homogeneity)
(Superposition)
( )
f t
Response
f ( )
y t
( )
h t
Fig. 5.26: Derivation process of the zero-state response of a system to an aperiodic signal.
Equation (5.6-12) shows that the zero-state response of a system to a basic signal –
imaginary exponential ejωt is the product of the signal itself and a constant coefficient
that has nothing to do with the time t, and this coefficient is just the Fourier transform
of the impulse response h(t) of the system – system function H(jω). Accordingly, we
have that the Fourier transform of the zero-state response of a system to a nonperiodic
signal is equal to the product of the Fourier transform of this signal and the system
function, i.e., Yf(jω) = H(jω)F(jω); the proof process is plotted in 󳶳Figure 5.26. In
fact, with a slight change, this process can also be used for system analysis in the s
and z domains.
To sum up, the same conclusion is obtained from the three methods:
The zero-state response of an LTI system to an arbitrary aperiodic signal can be
obtained by the inverse Fourier transform of the product of the system function and
the Fourier transform of the excitation signal. Note that the system function is the
Fourier transform of the impulse response.
Steps to ﬁnd the zero-state response of a system to an aperiodic signal by using
the Fourier transform are as follows:
Step 1: Find the Fourier transform F(jω) of the excitation f(t).
Step 2: According to the deﬁnition (or differential equation), circuit knowledge,
transfer operator or impulse response, ﬁnd the system function H(jω).
Step 3: Find the product of F(jω) and H(jω), and obtain the Fourier transform Yf(jω)
of the zero-state response.
Step 4: Find the inverse Fourier transform of Yf(jω) and obtain the zero-state re-
sponse yf(t) of the system.
The system analysis methods for aperiodic signals are detailed by the following exam-
ples.
Authenticated
32 PM

5.6 System analysis methods for aperiodic signals
|
179
+
−
S( )
u t
Ω
4
F
.5
0
Ω
4
1
R
2
R
C
+
−
C( )
u
t
S( )
u t
t
10
0
1
(V)
(a)
(b)
Fig. 5.27: E5.6-1.
Example 5.6-1. A circuit is shown in 󳶳Figure 5.27a and a voltage source uS(t) that is
a rectangle pulse is shown in 󳶳Figure 5.27b. Find the zero-state response uC(t) of this
circuit.
Solution. The excitation uS(t) can be regarded as the sum of two step signals as fol-
lows:
uS(t) = 10ε(t) −10ε(t −1) .
If
uS1(t) = 10ε(t) ,
uS2(t) = −10ε(t −1) ,
then
uS(t) = uS1(t) + uS2(t) .
From the superposition principle, the responses uC1(t) and uC2(t) can be calculated
corresponding to uS1(t) and uS2(t), respectively. The total response of the system is
uC(t) = uC1(t) + uC2(t).
If the Fourier transform of uS1(t) is US1(jω), then
US1(jω) = F [uS1(t)] = 10 [πδ(ω) + 1
jω] .
From the circuit diagram, using the constraint conditions of components and circuit,
the system function is
H(jω) = F [uC(t)]
F [uS(t)] = UC(jω)
US(jω) =
R2
1+jωR2C
R1 +
R2
1+jωR2C
=
R2
R1 + R2
⋅
1
1 + jω R1R2
R1+R2 C
.
Substituting the element parameters into the above expression, we have
H(jω) = 1
2 ⋅
1
1 + jω .
According to equation (5.6-5) we have
UC1(jω) = US1(jω) ⋅H(jω) = 1
2 ⋅
1
1 + jω × 10 [πδ(ω) + 1
jω ]
=
5π
1 + jω δ(ω) +
5
jω(1 + jω) = 5πδ(ω) + 5
jω −
5
1 + jω
Authenticated
32 PM

180
|
Analysis of continuous-time systems excited by aperiodic signals
Because
F−1[5πδ(ω)] = 5
2 ,
F−1 [ 5
jω ] = 5
2 sgn(t) ,
F−1 [
5
1 + jω ] = 5e−tε(t) ,
we have
uC1(t) = 5
2 + 5
2 sgn(t) −5e−tε(t) = 5ε(t) −5e−tε(t) = 5 (1 −e−t) ε(t) .
Because
uS2(t) = −uS1(t −1) ,
and, according to the time shifting, we have
uC2(t) = −uC1(t −1) = −5 [1 −e−(t−1)] ε(t −1) .
So, the complete zero-state response is
uC(t) = uC1(t) + uC2(t) = 5 (1 −e−t) ε(t) −5 [1 −e−(t−1)] ε(t −1) .
Example 5.6-2. In the system shown in 󳶳Figure 5.28, the excitation f(t) is known, and
the impulse response is h(t) = 1
πt. Find the zero-state response yf(t) of the system.
Solution. Since the system structure is invisible, the system function must be ob-
tained by theimpulseresponsebutcannotbeobtained from the constraintconditions.
Considering F(jω) as the spectrum of f(t), the system function is
H(jω) = F[h(t)] = F [ 1
π ⋅1
t ] = 1
π [−jπ sgn(ω)] = −j sgn(ω) ,
and then
Yf(jω) = F(jω)H(jω)H(jω) = F(jω) ⋅[−j sgn(ω)] ⋅[−j sgn(ω)]
= F(jω) [−sgn(ω) sgn(ω)]
= −F(jω) .
So, the zero-state response is
yf(t) = −f(t) .
This system is an inverter.
)
(t
f
)
(t
h
)
(t
h
)
(t
y
Fig. 5.28: E5.6-2.
Authenticated
32 PM

5.7 System analysis methods for periodic signals
|
181
From the above examples, it can be seen that in the system analysis in the frequency
domain, changing the convolution in the time domain into the algebraic multiplica-
tion in the frequency domain can not only greatly simplify the process to ﬁnd the zero-
state response, but also give the lively physical concept of a signal – the spectrum. Es-
pecially the frequency response characteristics of a system, which can be illustrated
by the waveform of the system function, play an important role in communication
technology. In contrast to the time domain method, the main disadvantage of the fre-
quency domain method is the need to apply two transforms, namely, the Fourier trans-
form and the inverse Fourier transform.
5.7 System analysis methods for periodic signals
From Section 5.4 we know that the Fourier transform of a periodic signal also exists
after the impulse function has been introduced. Then, can the zero-state response of a
system to a periodic signal be obtained by the Fourier transform method? The answer
is “yes”.
Assume that the excitation is
f(t) = sin ω0t ,
then
F[f(t)] = F(jω) = jπ [δ (ω + ω0) −δ (ω −ω0)] .
If the system function is of the form
H(jω) = |H(jω)| ejφ(ω) ,
in places ±ω0, we have
H (jω0) = |H (jω0)| ejφ(ω0)
and
H (−jω0) = |H (jω0)| e−jφ(ω0) .
Therefore, the spectrum of the zero-state response is
Yf(jω) = F(jω)H(jω)
= jπH(jω) ⋅[δ (ω + ω0) −δ (ω −ω0)]
= jπ [H (−jω0) δ (ω + ω0) −H (jω0) δ (ω −ω0)]
= jπ |H (jω0)| [e−jφ(ω0)δ (ω + ω0) −ejφ(ω0)δ (ω −ω0)] .
Then the zero-state response of the system is
yf(t) = F−1 [Yf(jω)] = |H (jω0)| sin [ω0t + φ (ω0)] .
(5.7-1)
If the excitation is f(t) = A sin (ω0t + φ), the zero-state response yf(t) can be directly
written as
yf(t) = A |H (jω0)| sin [ω0t + φ (ω0) + φ] .
(5.7-2)
Authenticated
32 PM

182
|
Analysis of continuous-time systems excited by aperiodic signals
It can be explained by equation (5.7-2) that the zero-state response of a system to a sine
signal is still the sine wave with the same frequency of the excitation, but its amplitude
and phase are determined by the excitation and the system function together.
Similarly, the zero-state response of a system to f(t) = A cos (ω0t + φ) is
yf(t) = A |H (jω0)| cos [ω0t + φ (ω0) + φ] .
(5.7-3)
Itisnotunexpected thatequations (5.7-2)and (5.7-3)areof thesameform, this is just the
concrete embodiment of the time invariant and linearity properties of system, because
f(t) = A sin (ω0t + φ) results from the delay of f(t) = A cos (ω0t + φ) by 1/4 period.
Conclusion: For a commonperiodic signal, itmustbeexpanded as thetrigonomet-
ric form of the Fourier series ﬁrst, and then the subzero-state responses correspond-
ing to all its harmonics are calculated by equations (5.7-2) or (5.7-3), respectively, after
which they are superimposed into the total zero-state response.
Note: If there is no subsequent explanation, the response of a system to a periodic
signal refers to the zero-state response of the system.
Example 5.7-1. Find the response y(t) of a system to an excitation f(t) = 2 + cos t +
5 cos (3t + 20.6°). The system function H(jω) =
1
1+jω is known.
Solution. Since H(j0) = 1, H(j1) =
1
1+j =
1
√2e−j45°, H(j3) =
1
1+j3 =
1
√10 e−j71.6°, equa-
tion (5.7-3) is utilized three times, and it can be obtained by the superposition
y(t) = 2 + 1
√2
cos (t −45°) +
5
√10
cos (3t −51°) .
After comparing equations (5.7-3) and (4.4-5), yn(t) = cn |H (jnω0)| cos (nω0t + φHn),
it can be found that when we analyze a system response to a periodic signal, whether
with the Fourier series or the Fourier transform, the solution steps are basically the
same, wherebothneed thedecompositionoperationfor anexcitationand theaddition
operation for sub responses.
5.8 The Hilbert transform
Is there a constraint relationship between the real and imaginary parts of the system
function for a causal system? The Hilbert transform can answer this question.
We know that the impulse response h(t) of a causal system satisﬁes the equation
h(t) = 0, t < 0 .
That is,
h(t) = h(t)ε(t) .
(5.8-1)
Assume that the system function H(jω), which is also the Fourier transform of h(t),
can be decomposed into the real part R(ω) and the imaginary part I(ω), that is,
H(jω) = F[h(t)] = R(ω) + jI(ω) .
Authenticated
32 PM

5.8 The Hilbert transform
|
183
Using the convolution theorem of the Fourier transform in the frequency domain in
equation (5.8-1), yields
F[h(t)] = 1
2π {F[h(t)] ∗F[ε(t)]} .
So, we have
R(ω) + jI(ω) = 1
2π {[R(ω) + jI(ω)] ∗[πδ(ω) + 1
jω ]}
= 1
2π {R(ω) ∗πδ(ω) + I(ω) ∗1
ω } + j
2π {I(ω) ∗πδ(ω) −R(ω) ∗1
ω }
=
{
{
{
R(ω)
2
+ 1
2π
∞
∫
−∞
I(λ)
ω −λ dλ
}
}
}
+ j
{
{
{
I(ω)
2
−1
2π
∞
∫
−∞
R(λ)
ω −λ dλ
}
}
}
Comparing the two sides of the above equation, we have
R(ω) = 1
π
∞
∫
−∞
I(λ)
ω −λ dλ ,
(5.8-2)
I(ω) = −1
π
∞
∫
−∞
R(λ)
ω −λ dλ .
(5.8-3)
Equations (5.8-2) and (5.8-3) are known as the Hilbert transform pair. They give an
interdependence relationship between the real part R(ω) and the imaginary part I(ω)
of a system function H(jω) with the causality property; that is, the real part R(ω) is
only determined by the imaginary part I(ω), and the imaginary part I(ω) is also only
determined by the real part R(ω).
To generalize the general situation, we deﬁne the convolution of a real signal f(t)
and a signal 1
πt as the Hilbert transform of f(t), which is denoted as
̃f(t)
def= f(t) ∗1
πt = 1
π
∞
∫
−∞
f(τ)
t −τdτ .
(5.8-4)
Its inverse transform is
f(t)
def=
̃f(t) ∗−1
πt = −1
π
∞
∫
−∞
̃f(τ)
t −τdτ .
(5.8-5)
Considering H[⋅] as the Hilbert transform character, we have
̃f(t) = H[f(t)] ,
f(t) = H−1[ ̃f(t)] .
We can see an important feature of the Hilbert transform, which is that the transform
pair exists in the same variable domain. Comparing it with the Fourier transform, the
Authenticated
32 PM

184
|
Analysis of continuous-time systems excited by aperiodic signals
Hilbert transform reﬂects the relationship between one function and another one in
the same variable domain, whereas the Fourier transform reveals the relationship be-
tween two expressing forms of a function in the time domain and the frequency do-
main.
The Hilbert transform also has the following characteristics:
(1)
H[cos(ω0t + φ)] = sin(ω0t + φ) .
(5.8-6)
(2)
H[sin(ω0t + φ)] = −cos(ω0t + φ) .
(5.8-7)
(3) If the frequency band of f(t) is limited by |ω| ≤ω0, then we have
H[f(t) cos ω0t] = f(t) sin ω0t .
(5.8-8)
H[f(t) sin ω0t] = −f(t) cos ω0t .
(5.8-9)
(4) If F(jω) is the Fourier transform of f(t), then the Fourier transform
̃F(jω) of ̃f(t)
should be
̃F(jω) = F[ ̃f(t)] = −jF(jω) sgn ω .
(5.8-10)
Equation (5.8-10) has an obvious physical meaning, that is, ̃f(t) can be obtained by
a signal f(t) passing a ﬁlter with the transfer function −j sgn ω. This ﬁlter is called
the Hilbert ﬁlter or the 90° phase shifter and is an all pass system for all frequency
components without magnitude damping.
(5) The original signal f(t) can be restored because the Hilbert transform ̃f(t) of a real
signal f(t) is inverse transformed again. This is equivalent to the effect of f(t) pass-
ing through a lag system with 90° ﬁrst and then passing an advanced system by
90° again. The effect of twice Hilbert transforming to f(t) is equivalent to the pro-
cess of f(t) passing through two 90° lag systems, and f(t) is changed into the in-
verse phase signal −f(t).
(6) The Hilbert transform can only change the phase spectrum rather than the am-
plitude spectrum of f(t), which means that f(t) and ̃f(t) have the same amplitude
spectrum, energy spectrum or power spectrum and the same energy or power.
(7) f(t) and ̃f(t) are orthogonal to each other, that is, ∫
∞
−∞f(t) ̃f (t)dt = 0.
(8) If f(t) is even (odd), then ̃f(t) is odd (even).
Theoretically, the Hilbert transform is a kind of mathematical tool to obtain the cor-
responding orthogonal signal of a signal. It is also used to express a single side band
signal or to transfer a bandpass signal into a low pass signal in the communication
principles course.
Authenticated
32 PM

5.10 Solved questions
|
185
5.9 Advantages and disadvantages of Fourier transform analysis
Usually, the system analysis methods based on the Fourier series and the Fourier
transform are collectively referred to as the Fourier analysis method, which has obvi-
ous advantages as outlined below.
(1) The explicit physical meaning. The method is based on the frequency domain, so,
it can directly give the frequency properties of signals and systems.
(2) The wide range of applications. Technologies such as the resonance circuit, ﬁl-
ter, modulator and demodulator, samplingtheorem, frequency conversioncircuit,
frequency division multiplexing way, equalization circuit,spectrum analysis way,
etc., are all derived from it.
However, just as a coin has two sides, from the previous chapters we can see that it
has two obvious shortcomings in the system analysis process for either nonperiodic
or periodic signals:
(1) It cannot apply to all signals. Some signals are not absolutely integrable accord-
ing to Dirichlet conditions, such as an exponential signal eαtε(t) (α > 0), etc., and
since their Fourier transforms do not exist, the Fourier transform method cannot
be used for them. There are also some signals, such as the step signal, the DC
signal, the sgn signal and so on, whose Fourier transforms cannot be directly ob-
tained by the deﬁnition because they are also not absolutely integrable.
(2) Itisonlysuitablefor ﬁndingthezero-stateresponseofsystem. BecausetheFourier
transform does not involve the boundary conditions of a signal or a system, the
zero-input response of system cannot be given by this method.
So, it is natural to pose a question: Can we ﬁnd a better method to overcome the short-
comings of the Fourier analysis method for system analysis? This leads us into the next
chapter.
5.10 Solved questions
Question 5-1. Knowing the Fourier transform F1(jω) of f1(t), ﬁnd the Fourier trans-
form F2(jω) of ∫
t
∞f1 [2(τ −1)] dτ.
Solution. According to the characteristics of Fourier transform, we have
f1(t) →F1(jω),
f1(2t) →1
2 F1 (jω
2 ) ,
f1 [2(t −1)] →1
2 F1 (jω
2 ) e−jωt ,
t
∫
∞
f1 [2(τ −1)] dτ →π
2 F1(0)δ(jω) + F1(ω/2)e−jωt
2jω
.
Authenticated
32 PM

186
|
Analysis of continuous-time systems excited by aperiodic signals
Question 5-2. Knowing x(t) =
{
{
{
e−t,
0 ≤t ≤1
0,
other
, ﬁnd its Fourier transform using the
characteristics of the Fourier transform.
Solution. According to the differential property, we have
x(t) = e−t [ε(t) −ε(t −1)] ,
x󸀠(t) = −e−tε(t) + e−tε(t −1) + δ(t) −e−1δ(t −1) ,
jωX(jω) =
−1
1 + jω + e−1ejω
1 + jω + 1 −e−1ejω .
So, the Fourier transform of x(t) is
X(jω) = −1 + e−(1−jω)
jω(1 + jω)
+ 1 −e−(1+jω)
jω
= 1 −e−(1+jω)
1 + jω
.
Question 5-3. Proof ∫
∞
−∞Sa2(t)dt = π with the properties of the Fourier transform.
Solution. As we know, the Fourier transform pair is
Sa(t)
F
←→πG2(ω) .
Because
∞
∫
−∞
f 2(t)dt = 1
2π
∞
∫
−∞
|F(ω)|2 dω ,
and so,
∞
∫
−∞
Sa2(t)dt = 1
2π
∞
∫
−∞
|πG2(ω)|2 dω = 1
2π
1
∫
−1
π2dω = π .
Question 5-4. A signal f(t) = ε(t + 1) −ε(t −3) is given, its Fourier transform is F(jω).
Find the integral ∫
∞
−∞2F(jω)Sa(ω)ej2ωdω.
Solution. Letting Sa(ω) = F [g2(t)], according to convolution characteristics, yields
F(jω)Sa(ω) = F [f(t) ∗g2(t)] .
A gate signal can be obtained by
g2(t) = F−1 [Sa(ω)] = 1
2 [ε(t + 1) −ε(t −1)] ,
and so,
1
2π
∞
∫
−∞
F(jω)Sa(ω)ejωtdω = f(t) ∗g2(t) .
Obviously, the shape of f(t) ∗g2(t) is trapezoidal, and [f(t) ∗g2(t)]|t=2 = 1 by using
convolution, which is illustrated in 󳶳Figure Q5-4. So,
∞
∫
−∞
2F(jω)Sa(ω)ej2ωdω = 4π [f(t) ∗g2(t)]|t=2 = 4π .
Authenticated
32 PM

5.10 Solved questions
|
187
2
( )
( )
f t
g t
∗
1
t
0
1
-2
2
3
4
-1
Fig. Q5-4
Question 5-5. Calculate the convolution sin(2πt)
2πt
∗sin(8πt)
8πt
.
Solution. We have the Fourier transform pair
gτ(t)
F
←→τSa ( τω
2 ) .
From the symmetry of the Fourier transform we have
τSa ( τt
2 )
F
←→2πgτ(ω) .
Thus, we obtain
sin(2πt)
2πt
F
←→1
2 g4π(ω)
and
sin(8πt)
8πt
F
←→1
8g16π(ω) .
According to the property of which convolution in the time domain means multiplica-
tion in the frequency domain, the expression of convolution in the frequency domain
can be calculated by
sin(2πt)
2πt
∗sin(8πt)
8πt
F
←→1
16g4π(ω)g16π(ω) = 1
16 g4π(ω) .
Taking the inverse Fourier transform, the convolution can be found by
sin(2πt)
2πt
∗sin(8πt)
8πt
= sin(2πt)
16
.
Question 5-6. The frequency response of a stable LTI system is H(ω) = 1−e−(jω+1)
jω+1
. Cal-
culate its unit step response g(t).
Solution. First, we obtain the impulse response h(t) of the system by calculating the
inverse Fourier transform of H(ω). Because
H(ω) = 1 −e−(jω+1)
jω + 1
=
1
jω + 1 −−e−1
jω + 1 e−jω ,
calculating the inverse Fourier transform of H(ω) yields
h(t) = e−tε(t) −e−te−(t−1)ε(t −1) .
The unit step response g(t) can be obtained by integrating to h(t), s
g(t) =
t
∫
∞
h(τ)dτ = h(t) ∗ε(t) = e−tε(t) ∗ε(t) −e−1e−(t−1)ε(t −1) ∗ε(t)
= (1 −e−t) ε(t) −e−1 [1 −e−(t−1)] ε(t −1) .
Authenticated
32 PM

188
|
Analysis of continuous-time systems excited by aperiodic signals
Question 5-7. The excitation is f(t) = e−αtε(t) and the unit impulse response of the
system is h(t) = e−βtε(t). Calculate the zero-state response yf(t) of the system.
Solution. According to the deﬁnition of the zero-state response,
yf(t) = f(t) ∗h(t) = e−αtε(t) ∗e−βtε(t) .
Calculating and analyzing the expression of yf(t) above, we have
(1) When α = β, the zero-state response is yf(t) = te−αtε(t) = te−βtε(t).
(2) When α
̸= β, the zero-state response is yf(t) =
1
β−α (e−αt −e−βt) ε(t).
Question 5-8. In the system shown in 󳶳Figure Q5-8, H1(ω) = e−j2ω and h2(t) = 1 +
cos πt
2 . Find the zero-state response yf(t), when the excitation is f(t) = ε(t).
1( )
y t
f ( )
y t
( )
f t
_
+
2( )
h t
h1 (t)
Fig. Q5-8
Solution. If y1(t)
F
←→Y1(ω), the spectrum of y1(t) in the form of ε(t) is
Y1(ω) = F(ω) −F(ω)H1(ω) = (πδ(ω) + 1
jω ) (1 −e−j2ω) ,
so,
y1(t) = ε(t) −ε(t −2) .
The zero-state response yf(t) is
yf(t) = y1(t) ∗h2(t) = y󸀠
1(t) ∗
t
∫
0
h2(t)dt = [δ(t) −δ(t −2)] ∗[t + 2
π sin π
2 t]
= t + 2
π sin π
2 t −(t −2) −2
π sin π
2(t −2) = 2 + 2
π sin π
2 t −2
π sin ( π
2 t −π)
= 2 + 2
π sin π
2 t −[−2
π sin π
2 t] = 2 + 4
π sin π
2 t .
5.11 Learning tips
The aperiodic signal is as important as the periodic signal, and readers should pay
attention to the following points.
(1) A periodic signal can become aperiodic when its cycle T tends to inﬁnity, and this
is the foundation to introduce the Fourier transform.
Authenticated
32 PM

5.12 Problems
|
189
(2) The gate and the sampling signals are an important Fourier transform pair. The
important concept reﬂected by it is that the frequency range of a time limited sig-
nal (e.g., a gate signal in the time domain) is unlimited in the frequency domain
(e.g., a sampling signal in the frequency domain), and the duration of a frequency
limited signal (e.g., a gate signal in the frequency domain) is unlimited in the time
domain (e.g., a sampling signal in the time domain). Simply, time is limited →fre-
quency is unlimited, and frequency is limited →time is unlimited.
(3) The impulse response and the system function compose a Fourier transform pair.
(4) The Fourier transform of the zero-state response to a nonperiodic signal is equal
to the product of the Fourier transform of the excitation and the system function.
5.12 Problems
Problem 5-1. Find the frequency spectrum of each signal in 󳶳Figure P5-1.
5( )
f
t
1
2 ( )
f
t
τ
t
0
τ
0
τ
−
2τ
2τ
−
1
2
A
A
−
0
1 2
3
t
t
3( )
f
t
4 ( )
f
t
t
1
2
3
0
1
1
−
1( )
f t
τ
t
0
A
τ
−
Fig. P5-1
Problem 5-2. Find the frequency spectrums and plot them for the following signals
by the symmetry properties of the Fourier transform.
(1) f1(t) = [ sin(2πt)
2πt
]
2
(2) f2(t) = sin 50(t−3)
100(t−3)
(3) f3(t) =
2
4+t2
Problem 5-3. Using the calculus properties of the Fourier transform, ﬁnd the fre-
quency spectrum of the following graphic signals.
1
2
0
1
t
t
t
1( )
f t
2
T
−
2
T
0
1
1
−
3
4
2 ( )
f
t
E
2
1τ
2
2
τ
2
1
τ
−
2
2
τ
−
3( )
f
t
0
Fig. P5-3
Authenticated
32 PM

190
|
Analysis of continuous-time systems excited by aperiodic signals
Problem 5-4. The Fourier transform of f(t) is F(jω). Find the frequency spectrum of
each signal.
(1) f(2t −5)
(2) f(3 −5t)
(3) tf(2t)
(4) (t −4)f(−2t)
(5) t df(t)
dt
(6) f ( t
2 + 3) cos(4t)
(7) [1 −cos(4t)] f(t −3)
Problem 5-5. Assume that the Fourier transform of f(t) is F(jω). Prove
(1) F(0) = ∫
∞
−∞f(t)dt
(2) f(0) =
1
2π ∫
∞
−∞F(jω)dω
(3) ∫
∞
−∞|f(t)|2 dt =
1
2π ∫
∞
−∞|F(jω)|2dω
Calculate the following equations:
(1) ∫
∞
−∞
1
α2+ω2 dω
(2) ∫
∞
0
sin4 αω
ω4
dω
(3) ∫
∞
−∞Sa (ω0t) dt
Problem 5-6. Find the original functions f(t) of following frequency spectrums.
(1)
1
ω2 ;
(2) δ(ω + 100) −δ(ω −100);
(3) eαωε(−ω);
(4)
5e−jω
(jω−2)(jω+3)
Problem 5-7. Knowing the amplitude spectrum |F(jω)| and the phase spectrum φ(ω)
as shown in 󳶳Figure P5-7, ﬁnd the corresponding original function f(t).
1(j )
F
ω
ω
0
ω
0
ω
−
0
A
1( )
ϕ ω
ω
0
1
1
−
2
π
2
π
−
2(j )
F
ω
ω
0
ω
0
ω
−
0
A
2 ( )
ϕ
ω
ω
0
0
ω
0
ω
−
3 (j )
F
ω
3
1
1
−
2
−
3
−
2
0
1
1
−
ω
0
ω
−
0
ω
(a)
(b)
(c)
Fig. P5-7
Problem 5-8. Knowing the system function H(jω) =
−ω2+j4ω+5
−ω2+j3ω+2 and the excitation
f(t) = e−3tε(t), ﬁnd the zero-state response yf(t).
Problem 5-9. Knowing the system function H(jω) =
jω
−ω2+j5ω+6, the starting states of
the system y(0−) = 2, y󸀠(0−) = 1, excitation f(t) = e−tε(t), ﬁnd the full response y(t).
Problem 5-10. Excitation f(t) of an LTI circuit is shown in 󳶳Figure P5-10; the impulse
response h(t) = e−2tε(t) is known. Find the zero-state response yf(t) of the circuit by
the method in the frequency domain.
Problem 5-11. A circuit and a voltage source uS(t) are shown in 󳶳Figure P5-11. Find
the corresponding zero-state response uC(t).
Authenticated
32 PM

5.12 Problems
|
191
( )
f t
t
1
2
3
0
2
Fig. P5-10
+
−
R
C
Ω
1
F
1
t
1
2
3
0
1
-1
2
S( )
u t
S( )
u t
+
−
C( )
u
t
Fig. P5-11
Problem 5-12. A circuit is shown in 󳶳Figure P5-12. If uS(t) are the following signals,
ﬁnd corresponding zero-state responses iO(t).
(1) uS(t) = δ(t)
(2) uS(t) = ε(t)
(3) uS(t) = e−tε(t)
+
−
1Ω
2F
4Ω
O( )
i
t
S( )
u t
Fig. P5-12
Problem 5-13. A circuit is shown in 󳶳Figure P5-13 and f(t) = 10e−tε(t) + 2ε(t). Find
the unit impulse response h(t) and the zero-state response if(t) to i(t).
+
−
)
(t
f
Ω
4
2H
( )
i t
R
L
Fig. P5-13
Problem 5-14. A system is shown in 󳶳Figure P5-14. Find the system function H(jω)
and h(t). If f(t) = te−2tε(t), ﬁnd the zero-state response of the system.
)
(t
f
−
−
)
(t
y
6
∫
∫
8
Fig. P5-14
Authenticated
32 PM

Authenticated
32 PM

6 Analysis of continuous-time systems in the
complex frequency domain
Question: Can we ﬁnd a method to make up the deﬁciencies of Fourier analysis?
Solution: Seek a method to make an unbounded signal becomes absolutely inte-
grable →Analyze systems with the aid of the Fourier analysis approach.
Results: Laplace transform, system function, system models in the s domain.
6.1 Concept of the Laplace transform
The Fourier transform is an effective mathematical tool, which can be used to analyze
the responses resulting from some nonperiodic and periodic signals of a system in the
frequency domain. However, it has the following drawbacks:
(1) It is not applicable to signals whose Fourier transforms do not exist.
(2) It is incapable of giving the complete response of a system.
To compensate for the shortages, another mathematical method called the Laplace
transform was introduced.
The Laplace transform is also a kind of integral transform method, which was
proposed by the French mathematician Pierre Simon de Laplace (1749–1825) in 1780.
Itis notonly aneffectiveanalysis means for LTI systems, butalso a widely used method
in other technologies.
The Laplace transform can be drawn from the Fourier transform according to the
following procedures. For a signal f(t) which is not absolutely integrable, we hope that
the product of f(t) and an attenuation factor e−σt can be absolutely integrable. That is,
it is possible to ﬁnd an appropriate value of σ to make f(t)e−σt absolutely integrable,
and then obtain the Fourier transform of f(t)e−σt.
Assuming F[f(t)e−σt] = Fσ(jω), from deﬁnition of the Fourier transform, we ob-
tain
Fσ(jω) =
+∞
∫
−∞
[f(t)e−σt]e−jωtdt =
+∞
∫
−∞
f(t)e−(σ+jω)tdt .
(6.1-1)
The corresponding inverse Fourier transform is
f(t)e−σt = 1
2π
+∞
∫
−∞
Fσ(jω)ejωtdω .
(6.1-2)
Letting s = σ + jω, F(s) = Fσ(jω), equation (6.1-1) can be changed into
F(s) =
+∞
∫
−∞
f(t)e−stdt .
(6.1-3)
https://doi.org/10.1515/9783110419535-006
Authenticated
32 PM

194
|
6 Analysis of continuous-time systems in the complex frequency domain
Similarly to the Fourier transform, F(s) is deﬁned as the Laplace transform of f(t),
which is also called the image function of f(t). Like the case when variable ω is con-
sidered as the real frequency in the Fourier transform, the complex variable s in equa-
tion (6.1-3) is regarded as the complex frequency.
If both sides of equation (6.1-2) are multiplied by eσt, and Fσ(jω) is replaced by
F(s),
f(t) = 1
2π
+∞
∫
−∞
Fσ(jω)e(σ+jω)tdω
s=σ+jω
=
1
2πj
σ+j∞
∫
σ−j∞
F(s)estds .
(6.1-4)
Equation (6.1-4) is called the inverse Laplace transform of F(s), where f(t) is called the
original function of F(s).
Equations (6.1-3) and (6.1-4) are called the bilateral Laplace transform pair. Sym-
bols “L” and “L−1” represent the Laplace transform and the inverse transform oper-
ations, respectively, and they can be related by
F(s) = L[f(t)] ,
f(t) = L−1[F(s)] ,
f(t)
L
←→F(s) .
Obviously, the Laplace transform can convert a time function f(t) into a complex func-
tion F(s). Because the variable s is called the complex frequency, the Laplace trans-
form is considered to provide a new path for the analysis of the response of the system
to a nonperiodic signal in the complex frequency domain, which refers to the concep-
tion of the Fourier transform.
Comparing equations (6.1-3) with (5.1-3), we ﬁnd that F(s) is similar to F(jω) in
form, and equation (5.1-3) can be changed into equation (6.1-3) only if jω is replaced by
s, but it does not mean s=jω. Because s = σ+jω, the above result virtually illustrates
that if σ = 0, which is the real part of the variable s, the Laplace transform should
become the Fourier transform. This means that the Fourier transform is a special case
of the Laplace transform, whereas the Laplace transform is a generalization of the
Fourier transform.
Comparing the functions of these two transforms, we reach the following conclu-
sions:
The relationship between a signal in the time and frequency domains is estab-
lished by the Fourier transform, whereas the relationship between a signal in the time
domain and the complex frequency domain is established by the Laplace transform.
The Fourier transform decomposes a signal f(t) into a continuous sum of imaginary
exponential signals like ejωt, whereas the Laplace transform can express a signal f(t)
as a continuous sum of complex exponential signals like est. Clearly, they are different
approaches but give equally satisfactory results.
Compared with the Fourier transform, the Laplace transform is only a mathemat-
ical tool in theory without deﬁnite physical meaning. However, it has the operating
Authenticated
32 PM

6.1 Concept of the Laplace transform
|
195
Tab. 6.1: Relationship between bilateral and unilateral Laplace transform and Fourier transform.
Transform
Bilateral Laplace
transform
Unilateral Laplace transform
(f(t) = 0, t < 0)
Fourier transform
(σ = 0)
Variable properties
s = σ + jω,
−∞< t < +∞
s = σ + jω,
0 < t < +∞
s = jω,
−∞< t < +∞
capability that the Fourier transform has not and it can accomplish some tasks that
cannot be ﬁnished by the Fourier transform.
In practice, signals are all causal (unilateral) signals with a starting moment, so
if the starting time is set as the origin of time, we have
f(t) = 0
(t < 0) .
As a result, equation (6.1-3) will be changed into
F(s) =
+∞
∫
0−
f(t)e−stdt
(6.1-5)
Equation (6.1-5) is called the unilateral Laplace transform or the image function of f(t).
Because a signal may have a step at moment t = 0, the lower limit of the integral of
the unilateral Laplace transform is stipulated as 0−, equation (6.1-4) is changed into
f(t) =
{
{
{
0
(t < 0)
1
2πj ∫
σ+j∞
σ−j∞F(s)estds
(t > 0)
(6.1-6)
Equations (6.1-5) and (6.1-6) are known as the unilateral Laplace transform pair.
Table 6.1 shows the relationships between the bilateral Laplace transform, the
unilateral Laplace transform and Fourier transform.
Here, we only discuss the unilateral Laplace transform pair, because it is much
closer to the real situation, and in subsequent chapters the term Laplace transform is
just the unilateral Laplace transform without explanation. Thus, for t < 0, the values
of f(t) are irrelevant to the result of the Laplace transform. For example, the three sig-
nals shown in 󳶳Figure 6.1 are different, but their unilateral Laplace transforms are all
1
s+a.
t
1
0
)
(
1 t
f
)
(
3 t
f
)
(
2 t
f
t
1
0
t
1
0
( )
at
e
t
ε
−
a t
e
−
at
e−
0
a >
0
a >
0
a >
Fig. 6.1: Three signals with same unilateral Laplace transform.
Authenticated
32 PM

196
|
6 Analysis of continuous-time systems in the complex frequency domain
As stated with the introduction of the Laplace transform, it is possible to ﬁnd an
appropriate value of σ to make f(t)e−σt absolutely integrable. Thus, the Fourier trans-
form of f(t)e−σt should exist, and the Laplace transform of f(t) can be obtained. How-
ever, how can we determine an appropriate value of σ? If the range of σ is considered
as the ROC (region of convergence), how can we determine the ROC? The ROC con-
cept of the Laplace transform is given by the following example. Note that σ is also
expressed as Re(s).
Example 6.1-1. Find the image function F(s) of an exponential f(t) = eatε(t), (a > 0).
Solution. According to the deﬁnition,
F(s) =
∞
∫
0−
eate−stdt =
∞
∫
0−
e−(s−a)tdt = e−(s−a)t
−(s −a)
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨
∞
0−
=
1
(s −a) [1 −lim
t→∞e−(s−a)t] . (6.1-7)
Because s = σ + jω, the second term in equation (6.1-7) can be written as
lim
t→∞e−(s−a)t = lim
t→∞e−(σ−a)te−jωt .
(6.1-8)
If we choose σ > a, the e−(σ−a)t will be attenuated with the increase of time t, so,
lim
t→∞e−(s−a)t = 0 .
Thereby, equation (6.1-7) will be convergent. The image function of f(t) is
F(s) =
1
s −a
If σ < a, the e−(σ−a)t will increase with time t. When t →∞, equation (6.1-8) will tend
to inﬁnity, thus equation (6.1-7) is not convergent (divergent), and the image function
of f(t) does not exist.
From the above discussions it can be seen that whether f(t)e−σt holds the absolutely
integrable condition depends on the properties of f(t) and the relative relationship
between f(t) and σ. In general, if the value of limt→∞f(t)e−σt is zero for σ > σ0, the
function f(t)e−σt is convergent for the range of σ > σ0, and its integral exists, so the
Laplace transform of it also exists.
In the complex plane (s plane) with σ as the horizontal axis and jω as the vertical
axis, and σ0 is called the convergence coordinate, the vertical line crossing point σ0 is
the boundary of the ROC and is known as the convergence axis. This convergence axis
divides the s plane into two regions, that is, σ > σ0 is the ROC of F(s), but σ < σ0 is
the divergent region, which are shown in 󳶳Figure 6.2a. The Laplace transform of f(t)
only exists in its ROC, thus equation (6.1-5) should be written as
F(s) =
+∞
∫
0−
f(t)e−stdt
(σ > σ0) .
(6.1-9)
Authenticated
32 PM

6.1 Concept of the Laplace transform
|
197
jω
σ
0
0
σ
0
ω>
0
σ >
0
ω>
0
σ <
0
ω<
0
σ <
0
ω<
0
σ >
Region of 
divergent 
0
jω
σ
0
a
jω
σ
0
a
jω
σ
b
Convergence
axis
s-plane and ROC
ROC of left signals
ROC of right signals
ROC of bilateral
signals
Convergence 
axis
Convergence axis
Convergence axis
ROC
Convergence 
axis
Convergence 
coordinates
ROC
ROC
ROC
a
(a)
(b)
(c)
(d)
Fig. 6.2: s plane and ROC.
In Example 6.1-1, because σ0 = a, the complete answer should be written as
F(s) =
1
s −a
ROC: σ > a
or
Re(s) > a .
This means that f(t)e−σt is convergent for the range σ > a, and its Laplace transform
exists.
By further analysis, we reach the following points with respect to the ROC:
(1) For a left-sided signal f(t), the ROC lies to the left of the convergent axis σ = a
shown in 󳶳Figure 6.2b.
(2) For a right-sided signal f(t), the ROC lies to the right of the convergent axis σ = a
shown in 󳶳Figure 6.2c.
(3) For a bilateral signal f(t), the ROC is a strip region between two convergent axes
σ = a and σ = b shown in 󳶳Figure 6.2d.
(4) The ROC of a right-sided signal must be the right half-plane to the right of the
rightmost pole of H(s); the ROC of a left-sided signal must be the left half-plane to
the left of the leftmost pole of H(s).
(5) If f(t) is time limited and absolutely integrable, its ROC is the whole s plane.
(6) The ROC does not contain the convergence axis or its boundary, namely, the ROC
is an open set.
(7) The ROC does not contain any pole of F(s).
(8) Because the original function and the image function in the unilateral Laplace
transform are one to one correspondence, for convenience, the ROC of a unilateral
Laplace transform of a signal usually does not need to be labeled or emphasized.
(9) The original function and the image function in the bilateral Laplace transform
arenotona one-to-onecorrespondingrelationship, thatis, differentoriginalfunc-
tions can have the same image function. However, there is a one-to-one corre-
sponding relationship between the original function and the image function with
its ROC, thus it is necessary to label the ROC. For example, both bilateral Laplace
transforms of the causal signal f1(t) = e−atε(t) and the noncausal signal f2(t) =
−e−atε(−t) are
1
s+a, but the ROCs of F1(s) and F2(s) are, respectively, Re(s) > −a
and Re(s) < −a.
Authenticated
32 PM

198
|
6 Analysis of continuous-time systems in the complex frequency domain
6.2 Laplace transforms of common signals
Laplace transforms of two core signals will be discussed ﬁrst, and then others are
listed at end of the section.
1. Unit step signal ε(t)
Because the unit step signal has a step at moment t = 0, which means f(0−) = 0 and
f(0+) = 1, its image function is
F(s) = L [ε(t)] =
∞
∫
0−
e−stdt = −e−st
s
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨
∞
0−
= 1
s ,
namely,
ε(t)
L
←→1
s .
(6.2-1)
2. Unit impulse signal δ(t)
According to the deﬁnition, the Laplace transform of δ(t) is
F(s) = L[δ(t)] =
∞
∫
0−
δ(t)e−stdt = 1 ,
namely,
δ(t)
L
←→1 .
(6.2-2)
The Laplace transforms of some common signals are listed in Table 6.2.
6.3 Laplace transforms of periodic signals
If there is a periodic signal fT(t) with the period T, and its ﬁrst cycle waveform begin-
ning at t ≥0 is f1(t), its unilateral Laplace transform FT(s) equals the product of the
Laplace transform F1(s) of f1(t) and a factor
1
1−e−Ts , that is,
L[fT(t)] = FT(s) =
1
1 −e−Ts F1(s) .
(6.3-1)
Readers can decompose fT(t) into an algebraic sum of inﬁnite periodic waveforms
(fT(t) = ∑+∞
n=0 f1(t −nT)) and then use the time shifting property of the Laplace trans-
form and the summation of a series (Referring to Example 6.4-10) to prove equa-
tion (6.3-1). Note that the periodic signal here is a unilateral signal.
Authenticated
32 PM

6.4 Properties of the Laplace transform
|
199
Tab. 6.2: Laplace transforms of common signals.
No.
f(t), t > 0, α > 0
F(s)
ROC
1
δ(t)
1
Re(s) > −∞
2
δ(n)(t)
sn
Re(s) > −∞
3
ε(t)
1
s
Re(s) > 0
4
e−αt
1
s+α
Re(s) > −α
5
tn(n Positive integer)
n!
sn+1
Re(s) > 0
6
te−αt
1
(s+α)2
Re(s) > −α
7
tne−αt
n!
(s+α)n+1
Re(s) > −α
8
sin ω0t
ω0
s2+ω2
0
Re(s) > 0
9
cos ω0t
s
s2+ω2
0
Re(s) > 0
10
e−αt sin ω0t
ω0
(s+α)2+ω2
0
Re(s) > −α
11
e−αt cos ω0t
s+α
(s+α)2+ω2
0
Re(s) > −α
12
t sin ω0t
2ω0s
(s2+ω2
0)2
Re(s) > 0
13
t cos ω0t
s2−ω2
0
(s2+ω2
0)2
Re(s) > 0
14
shαt
α
s2−α2
Re(s) > α
15
chαt
s
s2−α2
Re(s) > α
6.4 Properties of the Laplace transform
6.4.1 Linearity
If
L[f1(t)] = F1(s)
and
L[f2(t)] = F2(s) ,
and then
L[af1(t) + bf2(t)] = aF1(s) + bF2(s) .
(6.4-1)
Proof.
L[af1(t) + bf2(t)] =
∞
∫
0−
[af1(t) + bf2(t)]e−stdt
= a
∞
∫
0−
f1(t)e−stdt + b
∞
∫
0−
f2(t)e−stdt
= aF1(s) + bF2(s)
Authenticated
32 PM

200
|
6 Analysis of continuous-time systems in the complex frequency domain
Example 6.4-1. Find the Laplace transform F(s) of f(t) = cos ω0t.
Solution.
F(s) = L [cos ω0t]
= L [e−jω0t
2
+ ejω0t
2
] = L [e−jω0t
2
] + L [ ejω0t
2
]
= 1
2 (
1
s + jω0
+
1
s −jω0
) =
s
s2 + ω2
0
With the same method, the image function of sin ω0t is
F(s) = L [sin ω0t] =
ω0
s2 + ω2
0
.
Example 6.4-2. Find the Laplace transform F(s) of the hyperbolic sine shαt and the
hyperbolic cosine chαt.
Solution. Because
shαt = eαt −e−αt
2
and
chαt = eαt + e−αt
2
,
according to linearity properties, we can obtain the image function of shαt
F(s) = L[shαt] = L [eαt
2 −e−αt
2 ]
= 1
2L [eαt] −1
2L [e−αt] = 1
2 (
1
s −α −
1
s + α ) =
α
s2 −α2 .
Similarly, we also obtain the image function of chαt
F(s) = L[chαt] = L [eαt
2 + e−αt
2 ] = 1
2 (
1
s −α +
1
s + α ) =
s
s2 −α2 .
6.4.2 Time shifting
If
L[f(t)] = F(s) ,
then
L [f (t −t0) ε (t −t0)] = e−st0F(s) .
(6.4-2)
Proof.
L [f (t −t0) ε (t −t0)]
=
∞
∫
0−
f (t −t0) ε (t −t0) e−stdt =
∞
∫
t0
f (t −t0) e−stdt
t−t0=x
=
∞
∫
0
f(x)e−s(x+t0)dx = e−st0
∞
∫
0
f(x)e−sxdx = e−st0F(s)
Authenticated
32 PM

6.4 Properties of the Laplace transform
|
201
t
0
0
t
)
(t
f
)
(
0t
t
f
−
0t
(a)
(b)
Fig. 6.3: Time shifting properties diagram.
Note that the original function in equation (6.4-2) is f(t −t0)ε(t −t0) rather than f(t −
t0). Waveforms of f(t) and f(t −t0) are, respectively, shown in 󳶳Figure 6.3a and b.
Obviously, the effective (shaded) parts of the unilateral Laplace transforms of f(t −t0)
and f(t −t0)ε(t −t0) are different; the time shifting property cannot be used by the
former.
Example 6.4-3. The signals f1(t) and f2(t) are plotted in 󳶳Figure 6.4; the image func-
tion of f1(t) is F1(s). Please ﬁnd the image function F2(s) of f2(t).
Solution. f1(t) and f2(t) can be related by
f2(t) = f1(t) −f1(t −1) .
According to the linearity and the time shifting of the Laplace transform, we have
F2(s) = F1(s) −e−sF1(s) = (1 −e−s) F1(s) .
Example 6.4-4. Find the Laplace transform of signal f(t) = t2ε(t −1).
Solution. The expression of the signal can be transformed as
f(t) = (t −1)2ε(t −1) + 2(t −1)ε(t −1) + ε(t −1) .
According to the time shifting, we have
L[ε(t −1)] = 1
s e−s ,
L[2(t −1)ε(t −1)] = 2
s2 e−s ,
L[(t −1)2ε(t −1)] = 2
s3 e−s .
According to the linearity property, we have
L[f(t)] = ( 2
s3 + 2
s2 + 1
s ) e−s .
0
t
-1
1
0
t
1
1
2
1
2
3
)
(
2 t
f
)
(
1 t
f
(a)
(b)
Fig. 6.4: E6.4-3.
Authenticated
32 PM

202
|
6 Analysis of continuous-time systems in the complex frequency domain
6.4.3 Complex frequency shifting
If
L[f(t)] = F(s) ,
then
L [f(t)es0t] = F (s −s0) .
(6.4-3)
Proof.
L [f(t)es0t] =
∞
∫
0−
[f(t)es0t]e−stdt =
∞
∫
0−
f(t)e−(s−s0)tdt = F(s −s0)
Example 6.4-5. Find the Laplace transforms of the attenuation sine e−at sin βt and the
attenuation cosine e−at cos βt, where a > 0.
Solution. We know that
L[sin βt] =
β
s2 + β2 .
According to the complex frequency shifting property, we obtain
L [e−at sin βt] =
β
(s + a)2 + β2 .
Similarly,
L[cos βt] =
s
s2 + β2 ,
so,
L [e−at cos βt] =
s + a
(s + a)2 + β2 .
6.4.4 Time scaling
If
L[f(t)] = F(s) ,
then
L[f(at)] = 1
a F ( s
a)
(a > 0) .
(6.4-4)
Proof.
L[f(at)] =
∞
∫
0−
f(at)e−stdt
τ=at
=
∞
∫
0−
f(τ)e−( τ
a)sd ( τ
a) = 1
a
∞
∫
0−
f(τ)e−( s
a)τdτ = 1
a F ( s
a ) .
Authenticated
32 PM

6.4 Properties of the Laplace transform
|
203
Example 6.4-6. If L[f(t)] = F(s) is known, ﬁnd L[f(at −b)ε(at −b)](a, b > 0).
Solution. From the time shifting, we have
L[f(t −b)ε(t −b)] = e−bsF(s) .
According to the time scaling property, we obtain
L[f(at −b)ε(at −b)] = 1
a e−b
a sF ( s
a ) .
6.4.5 Differentiation in the time domain
If
L[f(t)] = F(s) ,
then
L [df(t)
dt ] = sF(s) −f(0−) ,
(6.4-5)
where f(0−) is the starting value of f(t) at moment t = 0.
Proof.
L [df(t)
dt ] =
∞
∫
0−
f 󸀠(t)e−stdt = f(t)e−st󵄨󵄨󵄨󵄨󵄨
∞
0−+ s
∞
∫
0−
f(t)e−stdt = sF(s) −f(0−)
From the ﬁrst-order derivative, the second-order and nth-order derivatives can be de-
duced as
L [d2f(t)
dt2 ] = s2F(s) −sf(0−) −f 󸀠(0−) ,
(6.4-6)
L [dnf(t)
dtn ] = snF(s) −sn−1f(0−) −sn−2f 󸀠(0−) −⋅⋅⋅−f (n−1)(0−) .
(6.4-7)
Because the causal signal can satisfy the following relationship:
f(0−) = f 󸀠(0−) = ⋅⋅⋅= f (n−1)(0−) = 0 ,
Equations (6.4-5)–(6.4-7) can be simpliﬁed as
L [df(t)
dt ] = sF(s) ,
(6.4-8)
L [d2f(t)
dt2 ] = s2F(s) ,
(6.4-9)
L [dnf(t)
dtn ] = snF(s) .
(6.4-10)
Note: This property is unsuitable for all-order derivatives of the unit step signal.
Authenticated
32 PM

204
|
6 Analysis of continuous-time systems in the complex frequency domain
( )
f t
′
0
t
1
1
2
)
(
1 t
f
0
t
-1
1
1
2
(a)
(b)
Fig. 6.5: E6.4-8.
Example 6.4-7. Find the image function of δ󸀠(t) which is called the impulse couple.
Solution. Knowing L[δ(t)] = 1, and according to the differential property in the time
domain, we have
L [δ󸀠(t)] = s .
Example 6.4-8. Find the Laplace transform of f(t) shown in 󳶳Figure 6.5.
Solution. The ﬁrst-order derivative of f(t) is
f 󸀠(t) = ε(t) −2ε(t −1) + ε(t −2) ,
and it can be illustrated in 󳶳Figure 6.5b, so,
L [f 󸀠(t)] = 1
s −2
s e−s + 1
s e−2s = 1
s (1 −e−s)2 ,
from the differential property in the time domain we have
L [f 󸀠(t)] = sL[f(t)] ,
so,
L[f(t)] = 1
s2 (1 −e−s)2 .
6.4.6 Integration in the time domain
If
L[f(t)] = F(s) ,
then
L [
[
t
∫
−∞
f(τ)dτ]
]
= 1
s F(s) + 1
s
0−
∫
−∞
f(τ)dτ .
(6.4-11)
Proof. Since
L [
[
t
∫
−∞
f(τ)dτ]
]
= L [
[
0−
∫
−∞
f(τ)dτ]
]
+ L [[
[
t
∫
0−
f(τ)dτ]]
]
,
so, the ﬁrst term ∫
0−
−∞f(τ)dτ in the above equation is a constant, that is,
L [
[
0−
∫
−∞
f(τ)dτ]
]
= 1
s
0−
∫
−∞
f(τ)dτ .
Authenticated
32 PM

6.4 Properties of the Laplace transform
|
205
Moreover,
L [[
[
t
∫
0−
f(τ)dτ]]
]
=
∞
∫
0−
[[
[
t
∫
0−
f(τ)dτ]]
]
e−stdt = −e−st
s
t
∫
0−
f(τ)dτ
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨
∞
0−
+ 1
s
∞
∫
0−
e−stf(t)dt ,
the ﬁrst term on the right is equal to 0 for t = 0−and t →∞, so,
L [[
[
t
∫
0−
f(τ)dτ]]
]
= 1
s F(s) ,
and then
L [
[
t
∫
−∞
f(τ)dτ]
]
= 1
s F(s) + 1
s
0−
∫
−∞
f(τ)dτ .
Example 6.4-9. Find the image function of tnε(t).
Solution. Since
t
∫
0
ε(τ)dτ = tε(t) ,
and according to the integral property in the time domain, we have
L[tε(t)] = L [
[
t
∫
0
ε(τ)dτ]
]
= 1
s L [ε(t)] = 1
s2 .
Because
t
∫
0
τε(τ)dτ = 1
2t2ε(t) ,
we have
L [t2ε(t)] = 2L [
[
t
∫
0
τε(τ)dτ]
]
= 2
s L [tε(t)] = 2
s3 ,
and then
L [tn] =
n!
sn+1 .
6.4.7 Convolution theorem
If
L [f1(t)] = F1(s)
and
L [f2(t)] = F2(s) ,
then
L [f1(t) ∗f2(t)] = F1(s)F2(s) .
(6.4-12)
Authenticated
32 PM

206
|
6 Analysis of continuous-time systems in the complex frequency domain
This states that the product of the Laplace transforms of two signals is equal to the
Laplace transform of convolution of the two signals.
Proof. Since
L [f1(t) ∗f2(t)] = L [
[
∞
∫
0
f1(τ)f2(t −τ)dτ]
]
=
∞
∫
0
[
[
∞
∫
0
f1(τ)f2(t −τ)dτ]
]
e−stdt ,
exchanging the integral order of the equation, we obtain
L [f1(t) ∗f2(t)] =
∞
∫
0
f1(τ) [
[
∞
∫
0
f2(t −τ)e−stdt]
]
dτ =
∞
∫
0
f1(τ)e−sτF2(s)dτ
= F2(s)
∞
∫
0
f1(τ)e−sτdτ = F1(s)F2(s)
Example 6.4-10. Find the image function of the periodic signal f(t) shown in
󳶳Figure 6.6a.
Solution. Since the original signal can be expressed as a convolution of its ﬁrst cycle
waveform and a unilateral unit comb or a unilateral impulse train, that is,
f(t) = f1(t) ∗f2(t) ,
based on the results of Example 6.4-8, we have
L [f1(t)] = 1
s2 (1 −e−s)2 .
Because a unilateral impulse train is of the from
f2(t) = δ(t) + δ(t −2) + δ(t −4) + ⋅⋅⋅=
∞
∑
n=0
δ(t −2n) ,
and
L [f2(t)] = 1 + e−2s + e−4s + ⋅⋅⋅=
∞
∑
n=0
e−2ns =
1
1 −e−2s ,
0
t
(1)
0
t
1
1
2
1
2
3
)
(t
f
)
(
2 t
f
3
4
4
0
t
1
1
2
)
(
1 t
f
(1)
(1)
(a)
(b)
(c)
Fig. 6.6: E6.4-10.
Authenticated
32 PM

6.4 Properties of the Laplace transform
|
207
from the convolution theorem, we obtain
L[f(t)] = L [f1(t) ∗f2(t)] = 1
s2 (1 −e−s)2
1
1 −e−2s =
1 −e−s
s2 (1 + e−s) .
The answer to this question can also be directly written by using equation (6.3-1) and
the results of Example 6.4-8.
6.4.8 Initial value theorem
If the Laplace transforms of signal f(t) and its derivative df(t)
dt
exist, and the Laplace
transform of f(t) is F(s), we obtain
f(0+) = lim
t→0+ f(t) = lim
s→∞sF(s) .
(6.4-13)
Proof. According to the differential property in the time domain,
sF(s) −f(0−) = L [df(t)
dt ] =
∞
∫
0−
df(t)
dt e−stdt =
0+
∫
0−
df(t)
dt e−stdt +
∞
∫
0+
df(t)
dt e−stdt
= f(0+) −f(0−) +
∞
∫
0+
df(t)
dt e−stdt
so,
sF(s) = f(0+) +
∞
∫
0+
df(t)
dt e−stdt .
(6.4-14)
If s →∞, the limit of the second term on the right side of equation (6.4-14) should be
lim
s→∞
{
{
{
{
{
∞
∫
0+
df(t)
dt e−stdt
}
}
}
}
}
=
∞
∫
0+
df(t)
dt [ lim
s→∞e−st]dt = 0 ,
therefore, calculating the limit of equation (6.4-14) when s →∞, we have
f(0+) = lim
s→∞sF(s) .
6.4.9 Final value theorem
If the Laplace transform of f(t) and its derivative df(t)
dt exist, and the Laplace transform
f(t) is F(s), there should be
f(∞) = lim
t→∞f(t) = lim
s→0 sF(s) .
(6.4-15)
Authenticated
32 PM

208
|
6 Analysis of continuous-time systems in the complex frequency domain
Proof. Calculate the limit of equation (6.4-14) when s →0,
lim
s→0 sF(s) = f(0+) +
∞
∫
0+
df(t)
dt e−stdt = f(0+) + lim
s→0
∞
∫
0+
df(t)
dt e−stdt
= f(0+) + lim
t→∞f(t) −f(0+)
thus,
lim
t→∞f(t) = lim
s→0 sF(s) .
The initial and ﬁnal value theorems are usually used in the cases that f(0+) and f(∞)
can be calculated by F(s), and f(t) does not have to be found. However, please note
the application conditions for the initial and ﬁnal value theorems in the following.
(1) When we want to ﬁnd the initial value of a signal by equation (6.4-13), if F(s) is a
rational algebraic expression, it should be a real fraction. This means the order of
the numerator polynomial should be lower than that of the denominator in F(s).
However, if F(s) is not a real fraction, it should be operated by long division to ex-
tract the real fraction terms from F(s) denoted by F0(s). It can be proved that f(0+)
equals f0(0+) which is the initial value of f0(t) from the inverse Laplace transform
of F0(s):
f(0+) = f0(0+) = lim
s→∞sF0(s) .
(6.4-16)
(2) The ﬁnal value of f(t) can be calculated by equation (6.4-15) only if the ﬁnal value
of f(t) exists, otherwise, a false conclusion is obtained. The judgment conditions
about it is that when the poles of F(s) distribute on the left half of the s plane or
F(s) only has a ﬁrst-order pole at the origin, the ﬁnal value theorem can be used.
For example, when a > 0, the value of limt→∞eat does not exist, but the wrong
conclusion of limt→∞eat = lims→0 sF(s) = 0 will arise if the ﬁnal value theorem is
used directly. The reason is that the root s = a of the denominator in L [eat] =
1
s−a
is on the real axis of right half-plane, so the ﬁnal value theorem cannot be used in
this case.
Example 6.4-11. Solve the initial and ﬁnal values of the inverse transform for the fol-
lowing expressions.
(1) F(s) = s3+s2+2s+1
s2+2s+1
(2) F(s) =
s2+2s+3
(s2+ω2
0)(s+1)
Solution. (1) Because it is an improper fraction, F(s) should be dealt with by long
division in order to obtain real fraction terms,
F(s) = s −1 +
3s + 2
s2 + 2s + 1 ,
Authenticated
32 PM

6.4 Properties of the Laplace transform
|
209
so,
f(0+) = lim
s→∞s
3s + 2
s2 + 2s + 1 = 3 .
The poles of F(s) are on the left half-plane of the s plane, so the ﬁnal value is
f(∞) = lim
s→0 sF(s) = 0 .
(2)
f(0+) = lim
s→∞s
s2 + 2s + 3
(s2 + ω2
0) (s + 1)
= 1 .
Considering the pair of conjugate poles s = ±jω0 of F(s) on the imaginary axis,
the ﬁnal value of f(t) does not exist.
6.4.10 Differentiation in the s domain
If
L[f(t)] = F(s) ,
then
L [(−t)nf(t)] = dnF(s)
dsn
.
(6.4-17)
Proof. Because
F(s) =
∞
∫
0−
f(t)e−stdt ,
we have
dF(s)
ds
= d
ds
∞
∫
0−
f(t)e−stdt .
Exchanging order of the integral and differential, we have
dF(s)
ds
=
∞
∫
0−
f(t) d
ds e−stdt =
∞
∫
0−
(−t)f(t)e−stdt = L[(−t)f(t)] ,
so,
L[(−t)f(t)] = dF(s)
ds
.
Repeating this, we have
L [(−t)nf(t)] = dnF(s)
dsn
.
Authenticated
32 PM

210
|
6 Analysis of continuous-time systems in the complex frequency domain
6.4.11 Integration in the s domain
If
L[f(t)] = F(s) ,
then
L [ f(t)
t ] =
∞
∫
s
F(η)dη .
(6.4-18)
Proof. Substituting F(s) = ∫
∞
0−f(t)e−stdt into the integral ∫
∞
s
F(η)dη, and exchanging
the integral order, we have
∞
∫
s
F(η)dη =
∞
∫
s
∞
∫
0−
f(t)e−ηtdtdη =
∞
∫
0−
f(t) [
[
∞
∫
s
e−ηtdη]
]
dt
=
∞
∫
0−
f(t)
t e−stdt = L [f(t)
t ] .
Example 6.4-12. Find the Laplace image function of t2e−atε(t).
Solution. Because
L [e−atε(t)] =
1
s + a ,
according to the differential property in the s domain, we obtain
L [(−t)2e−atε(t)] = d2
ds2 (
1
s + a ) =
2
(s + a)3
or
L [t2e−atε(t)] =
2
(s + a)3 .
Example 6.4-13. Find the Laplace image function of sin t
t ε(t).
Solution. Because
L[sin t ⋅ε(t)] =
1
s2 + 1 ,
according to the differential property in the s domain, we obtain
L [sin t
t
ε(t)] =
∞
∫
s
1
η2 + 1dη = π
2 −arctan(s) = arctan 1
s .
The main properties of the Laplace transform are listed in Table 6.3 for reference.
Authenticated
32 PM

6.5 Solutions for the inverse Laplace transform
|
211
Tab. 6.3: The main properties of Laplace transform.
No. Name
Time domain
f(t)ε(t)
Complex frequency domain
F(s)
1
Linearity
af1(t) + bf2(t)
aF1(s) + bF2(s)
2
Time shifting
f (t −t0) ε (t −t0)
e−st0 F(s)
3
Complex frequency shifting
f(t)es0t
F (s −s0)
4
Scaling
f(at)(a > 0)
1
a F ( s
a )
5
Time differential
df(t)
dt
sF(s) −f(0−)
dnf(t)
dtn
snF(s)−sn−1f(0−)−sn−2f 󸀠(0−)−⋅⋅⋅−f (n−1)(0−)
6
Time integral
∫
t
−∞f(τ)dτ
1
s F(s) + 1
s ∫
0−
−∞f(τ)dτ
7
Time convolution
f1(t) ∗f2(t)
F1(s)F2(s)
8
Convolution in s domain
f1(t) ⋅f2(t)
1
2πj ∫
σ+j∞
σ−j∞F1(z)F2(s −z)dz
9
Initial value theorem
f(0+) = limt→0+ f(t) = lims→∞sF(s)
10
Final value theorem
f(∞) = limt→∞f(t) = lims→0 sF(s)
11
Differential in s domain
(−t)nf(t)
dnF(s)
dsn
12
Integral in s domain
f(t)
t
∫
∞
s
F(η)dη
13
Time transformation
f(at−b)ε(at−b)
a > 0, b ≥0
e−bs
a
a
F ( s
a )
6.5 Solutions for the inverse Laplace transform
For the unilateral Laplace transform, the inverse transform of F(s) can be written as
L−1[F(s)] =
{
{
{
0
t < 0
1
2πj ∫
σ+j∞
σ−j∞F(s)estds
t > 0
Generally, the above integral should be solved by the residue theorem in the theory of
complex functions. However, it is commonly seen that the image function is a rational
fraction with s in the form, that is, it can be written as a ratio of two polynomials in
the s domain,
F(s) = B(s)
A(s) = bmsm + bm−1sm−1 + b1s + b0
sn + an−1sn−1 + a1s + a0
,
(6.5-1)
where ai and bj are real constants. Because the orders of the numerator and denomi-
nator are arbitrary, this image function does not appear in standard tables of Laplace
transforms, such as Table 6.2. However, in common cases, it can be expressed as a
sum of functions that do appear in standard tables, using a technique called partial
fraction expansion, so its inverse Laplace transform can be obtained.
Authenticated
32 PM

212
|
6 Analysis of continuous-time systems in the complex frequency domain
If m ≥n, F(s) can be divided as a sum of a rational polynomial and rationalproper
fractions by long division, for example,
F(s) = 2s3 + s2 −1
s2 + 3s + 2 = 2s −5 +
11s + 9
s2 + 3s + 2 = polynomial + proper fraction ,
where the inverse Laplace transforms of the rational polynomial can be easily ob-
tained according to the Laplace transform of a typical signal δ(t) and the properties
of the Laplace transform, such as L−1[2s] = 2δ󸀠(t) and L−1[5] = 5δ(t). Therefore, we
should only discuss the inverse transform problem about the rational real fraction,
which is equation (6.5-1) for m < n.
First, let us introduce the concept about zero and the pole. In equation (6.5-1),
the roots p1, p2, . . . , pn, which satisfy A(s) = 0, are called poles, and the roots
z1, z2, . . . , zm, which satisfy B(s) = 0, are called zeros. Then, equation (6.5-1) can be
rewritten as
F(s) = B(s)
A(s) = bm (s −z1) (s −z2) . . . (s −zm)
(s −p1) (s −p2) . . . (s −pn)
.
(6.5-2)
According to different types of poles, the partial fraction expansion method can be
divided into the following situations for discussion.
1. There are only non-repeated real poles in F(s)
The so-called non-repeated real poles mean that p1, p2, . . . , pn are real numbers
and are different from each other. In this simplest case, n coefficients, such as
k1, k2, . . . , kn, can be found and F(s) can be written in partial fraction form
F(s) = B(s)
A(s) =
k1
s −p1
+
k2
s −p2
+ ⋅⋅⋅+
kn
s −pn
=
n
∑
i=1
ki
s −pi
.
(6.5-3)
Therefore, according to the Laplace transform of the exponential signal, the original
function f(t) of the image function F(s) could be written as
f(t) = (k1ep1t + k2ep2t + ⋅⋅⋅+ knepnt) ε(t) =
n
∑
i=1
kiepitε(t) .
(6.5-4)
Now, how can we determine k1, k2, . . . , kn in equation (6.5-4)? Let us multiply both
sides of equation (6.5-3) by (s −p1),
B(s)
(s −p2) (s −p3) . . . (s −pn) = k1 + (s −p1) k2
s −p2
+ ⋅⋅⋅+ (s −p1) kn
s −pn
.
Letting s = p1,
k1 =
B(s)
(s −p2) (s −p3) . . . (s −pn)
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨s=p1
.
We can use the same technique to ﬁnd the other ki,
ki = (s −pi) B(s)
A(s)
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨s=pi
.
(6.5-5)
Authenticated
32 PM

6.5 Solutions for the inverse Laplace transform
|
213
Now, the task of ﬁnding the inverse Laplace transform of a real fraction with non-
repeated real poles has been fulﬁlled, that is, the original function f(t) will be ex-
pressed as an algebraic sum of exponential signals like equation (6.5-4) in form.
Example 6.5-1. Find the original function f(t) of F(s) =
2s+1
s2+8s+15.
Solution.
F(s) =
2s + 1
s2 + 8s + 15 =
2s + 1
(s + 3)(s + 5) =
k1
s + 3 +
k2
s + 5 ,
and according to equation (6.5-5), the coefficients are
k1 = 2s + 1
s + 5
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨s=−3
= −5
2;
k2 = 2s + 1
s + 3
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨s=−5
= 9
2
Thus, the original function f(t) is
f(t) = (9
2 e−5t −5
2 e−3t) ε(t) .
2. There are conjugate complex poles in F(s)
If A(s) = 0 has a pair of conjugate roots, such as s = α ± jβ, F(s) can be written as
F(s) =
B(s)
(s −α2) + β2 =
B(s)
(s −α −jβ) (s −α + jβ) =
k1
s −α −jβ +
k2
s −α + jβ .
According to equation (6.5-5), we can obtain k1 and k2,
k1 = (s −α −jβ) B(s)
A(s)
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨s=α+jβ
= B(α + jβ)
2jβ
,
(6.5-6)
k2 = (s −α + jβ) B(s)
A(s)
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨s=α−jβ
= B(α −jβ)
−2jβ
.
(6.5-7)
Since B(α + jβ) and B(α −jβ) are conjugate, k1 and k2 are related in conjugation. If
k1 = A + jB and k2 = A −jB or k2 = k∗
1, the original function f(t) will be
f(t) = L−1 [
k1
s −α −jβ +
k2
s −α + jβ ]
= [k1e(α+jβ)t + k2e(α−jβ)t] ε(t)
= eαt (k1ejβt + k∗
1e−jβt) ε(t)
= 2eαt(A cos βt −B sin βt)ε(t) .
(6.5-8)
Example 6.5-2. Find the original function f(t) of F(s) =
s+3
s2+2s+5.
Solution.
F(s) =
s + 3
(s + 1 −2j)(s + 1 + 2j) =
k1
s + 1 −2j +
k2
s + 1 + 2j .
Authenticated
32 PM

214
|
6 Analysis of continuous-time systems in the complex frequency domain
According to equations (6.5-6) and (6.5-7), we can obtain k1, k2,
k1 =
s + 3
s + 1 + 2j
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨s=−1+2j
= 1 −j
2
k2 =
s + 3
s + 1 −2j
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨s=−1−2j
= 1 + j
2
According to equation (6.5-8), this yields
f(t) = 2e−t (1
2 cos 2t + 1
2 sin 2t) ε(t) = e−t(cos 2t + sin 2t)ε(t) .
3. There are repeated poles in F(s)
This means there are two or more poles are equal in p1, p2, . . . , pn.
Suppose that F(s) only has one m repeated root p1, then F(s) can be written as
F(s) = B(s)
A(s) =
B(s)
(s −p1)m =
k1m
(s −p1)m +
k1,m−1
(s −p1)m−1 + ⋅⋅⋅+
k11
s −p1
.
(6.5-9)
Using L [tnε(t)] =
n!
sn+1 and the frequency shifting property, the original function of
k1m
(s−p1)m can be obtained by
L−1 [
k1m
(s −p1)m ] =
k1m
(m −1)! tm−1ep1tε(t) .
Now, k1m, k1,m−1, . . . , k11 can be determined. Multiplying both sides of equation 6.5-9
by (s −p1)m
B(s) = k1m + (s −p1) k1,m−1 + ⋅⋅⋅+ (s −p1)m−1 k11 ,
(6.5-10)
and setting s = p1, we obtain
k1m = (s −p1)m B(s)
A(s)
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨s=p1
.
(6.5-11)
However, when we use the same method to ﬁnd other k1,m−1, k1,m−2, . . . , k11, we will
see that the denominator in equation 6.5-11 will be zero, so we can take the derivative
with respect to s on both sides of equation (6.5-10),
d
ds [(s −p1)m F(s)] = k1,m−1 + 2 (s −p1) k1,m−2 + ⋅⋅⋅+ (m −1) (s −p1)m−2 k11 ,
and setting s = p1, we obtain
k1,m−1 = d
ds [(s −p1)m F(s)]
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨s=p1
.
Similarly, we obtain
k1,m−2 = 1
2
d2
ds2 [(s −p1)m F(s)]
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨s=p1
.
Authenticated
32 PM

6.5 Solutions for the inverse Laplace transform
|
215
Thus, we obtain the general form of k1i,
k1i =
1
(m −i)! ⋅dm−i
dsm−i [(s −p1)m F(s)]
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨s=p1
.
(6.5-12)
Example 6.5-3. Find the original function f(t) of F(s) =
s+2
s2+6s+9.
Solution. Since
F(s) =
s + 2
(s + 3)2 =
k1
(s + 3)2 +
k2
s + 3
according to equation (6.5-11), we can obtain k1, k2,
k1 = (s + 2)|s=−3 = −1
and
k2 =
d
ds (s + 2)
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨s=−3
= 1 .
So,
F(s) = −
1
(s + 3)2 +
1
s + 3
and
L−1 [
1
(s + 3)2 ] = te−3tε(t) ,
L−1 [
1
s + 3] = e−3tε(t) .
Thus, the original function f(t) is
f(t) = −te−3tε(t) + e−3tε(t) = (1 −t)e−3tε(t) .
Example 6.5-4. Find the original function f(t) of F(s) =
s4+2
s3+4s2+4s.
Solution. F(s) can be rewritten in the form
F(s) =
s4 + 2
s3 + 4s2 + 4s = s −4 + 12s2 + 16s + 2
s(s + 2)2
= s −4 + k1
s +
k22
(s + 2)2 + k21
s + 2 .
Since F(s) has one simple and one pair of twice-repeated poles, the corresponding
coefficients k1, k21, k22 can be obtained by the solution methods for simple poles and
two-repeated poles, respectively.
k1 = 12s2 + 16s + 2
(s + 2)2
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨s=0
= 1
2
k22 = 12s2 + 16s + 2
s
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨s=−2
= −9
Authenticated
32 PM

216
|
6 Analysis of continuous-time systems in the complex frequency domain
Moreover,
k21 = d
ds (12s2 + 16s + 2
s
)
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨s=−2
= 23
2
L−1[s −4] = δ󸀠(t) −4δ(t)
L−1 [ 1
2s ] = 1
2 ε(t)
L−1 [
9
(s + 2)2 ] = 9te−2tε(t)
L−1 [
23
2(s + 2)] = 23
2 e−2tε(t)
So, the original function is
f(t) = δ󸀠(t) −4δ(t) + 1
2ε(t) −9te−2tε(t) + 23
2 e−2tε(t) .
The above examples state that the premise to ﬁnd the inverse Laplace transform of
an image function is that the function is expanded in the partial fractions, and the
key is that we must master the properties of the Laplace transform and the Laplace
transforms of basic signals.
6.6 Analysis method of the system function in the s domain
6.6.1 System function
If the Laplace transforms of the zero-state response and the excitation are, respec-
tively, set as Yf(s) = L [yf(t)] and F(s) = L[f(t)], the system function H(s) is deﬁned
by
H(s)
def= Yf(s)
F(s) .
(6.6-1)
In system analysis, the excitation and response may be a voltage or a current, there-
fore, the system function may be the impedance (the ratio of voltage and current), the
admittance (the ratio of current and voltage), or the constant (the ratio of two cur-
rents or the ratio of two voltages). In addition, if the excitation and the response are
in the same port of a system, the system function is called the driving point function,
but if they are not in the same port, it is called the transfer function. For example,
in 󳶳Figure 6.7, the ratio of U1(s) and I1(s), and the ratio of I2(s) and U2(s) are both
)
(
1 s
I
)
(
2 s
I
)
(
1 s
U
+
−
)
(
2 s
U
System
+
−
Fig. 6.7: Graph for deﬁning the driving point functions and trans-
fer functions.
Authenticated
32 PM

6.6 Analysis method of the system function in the s domain
|
217
Tab. 6.4: Names of system functions.
Position of excitation
and response
Excitation
Response
System function
At the same port
Current I1(s)
Voltage U1(s)
Driving point impedance H(s) = U1(s)
I1(s)
Voltage U1(s)
Current I1(s)
Driving point admittance H(s) = I1(s)
U1(s)
At different ports
Current I1(s)
Voltage U2(s)
Transform impedance H(s) = U2(s)
I1(s)
Voltage U1(s)
Current I2(s)
Transform admittance H(s) = I2(s)
U1(s)
Voltage U1(s)
Voltage U2(s)
Transform voltage ratio H(s) = U2(s)
U1(s)
Current I1(s)
Current I2(s)
Transform current ratio H(s) = I2(s)
I1(s)
driving point functions, but the ratio of U1(s) and U2(s), and the ratio of U1(s) and
I2(s) are both transfer functions. (Related content can refer to the two-port networks
in Circuits Analysis, Chinese edition, written by Weigang Zhang, Tsinghua University
Press, 2015.1). Obviously, the driving point function can only be the impedance or the
admittance function, but the transfer function can be the impedance function, the
admittance function or the numerical ratio. These system functions are listed in Ta-
ble 6.4. In system analysis, they are collectively referred to as system function, network
function or transfer function, rather than their distinguishing attributes in general,
and they are denoted by H(s).
After the deﬁnition of system function is known, we will study separately the re-
lationship between the system function and the impulse response, and that between
the system function and the system differential equation.
According to equation (6.6-1), we have
Yf(s) = F(s)H(s) .
(6.6-2)
At the same time,
yf(t) = f(t) ∗h(t) .
With the Laplace transform on both sides of the expression and using convolution
theorem, we obtain
L [yf(t)] = L[f(t)] ⋅L[h(t)]
or
Yf(s) = F(s) ⋅L[h(t)] .
Comparing this equation with equation (6.6-2), we will obtain the relationship be-
tween the system function and the impulse response, that is, they are a Laplace trans-
form pair.
H(s) = L[h(t)] ,
(6.6-3)
h(t) = L−1[H(s)] .
(6.6-4)
Authenticated
32 PM

218
|
6 Analysis of continuous-time systems in the complex frequency domain
We know that the relationship between the input and the output of an nth-order LTI
system is described by a differential equation,
n
∑
i=0
aiy(i)(t) =
m
∑
j=0
bjf (j)(t) .
(6.6-5)
With the Laplace transform on both sides of equation (6.6-5), the image function of
zero-state response yf(t) can be deduced as
Yf(s) =
∑m
j=0 bjsj
∑n
i=0 aisi F(s) = B(s)
A(s) F(s) = H(s)F(s) .
(6.6-6)
Since
H(s) =
∑m
j=0 bjsj
∑n
i=0 aisi = bmsm + bm−1sm−1 + ⋅⋅⋅+ b1s + b0
ansn + an−1sn−1 + ⋅⋅⋅+ a1s + a0
,
(6.6-7)
the relationship between the function system and the coefficients of a differential
equation are:
(1) The coefficient of term sj in the numerator polynomial of the system function cor-
responds to the coefficient bj belonging to the j-th order derivative of the input
f(t).
(2) The coefficient of term si in the denominator polynomial corresponds to the coef-
ﬁcient ai of the i-th order derivative of the output y(t).
Therefore, the system function of an LTI system can be obtained from the system differ-
ential equation. In turn, the differential equation of the system can be also determined
by the system function.
Comparing equations (6.6-2) and (6.6-7) with the related equations of H(p) and
H(jω) in Chapters 3–5, the deﬁnitions and the forms of H(jω) and H(s) are similar,
hence, they can be considered as two expressions of the system function, which are
used in the frequency domain and the complex frequency domain respectively. Addi-
tionally, the operator P represents a kind of operation instead of a variable, however,
H(p) is also quite similar to the system function in form, so the expression H(s) =
H(p)|p=s can be also used as another way of ﬁnding the system function.
The conclusions for this section are as follows:
(1) The H(p), H(jω) and H(s) can all express the relationship between the excitation
and the response, or, they are all mathematical models used to describe character-
istics of a system, and are determined by the system structure and the component
parameters.
(2) The H(p) reveals the differential relationship between the excitation and the re-
sponse in the time domain, but H(jω) and H(s) show virtually their algebraic re-
lationships in the frequency domain and in the complex frequency domain.
(3) Although H(p), H(jω) and H(s) have different meanings, they have the same struc-
ture, and all can be transformed into each other by variable substitutions.
Authenticated
32 PM

6.6 Analysis method of the system function in the s domain
|
219
Example 6.6-1. The step response is g(t) = (1 −e−2t) ε(t). Find the corresponding ex-
citation f(t) to produce a zero-state response as yf(t) = (1 −e−2t −te−2t) ε(t).
Solution. This kind of problem can be solved by means of H(s). Considering that g(t)
is the zero-state response with the action of ε(t),
H(s) = Yf(s)
F(s) =
1
s −
1
s+2
1
s
=
2
s + 2 .
Since
F(s) = Yf(s)
H(s) =
1
s −
1
s+2 −
1
(s+2)2
2
s+2
= 1
s −
1
2(s + 2) ,
the corresponding excitation f(t) is
f(t) = L−1[F(s)] = L−1 [1
s −
1
2(s + 2)] = (1 −1
2e−2t) ε(t) .
Example 6.6-2. Find the system function of the LTI system described by the following
differential equation:
d2
dt2 y(t) + 3 d
dt y(t) + 2y(t) = 2 d
dt f(t) −3f(t)
Solution. According to equation (6.6-7), we obtain
H(s) =
2s −3
s2 + 3s + 2 .
6.6.2 Analysis method with the system function
According to the deﬁnition of the system function, we have
Yf(s) = F(s)H(s) .
(6.6-8)
The zero-state response of a system should be
yf(t) = L−1 [Yf(s)] = L−1[F(s)H(s)] .
(6.6-9)
Equation (6.6-9) provides a new method to solve the zero-state response, which is
called the analysis method based on the system function in the complex frequency
domain.
Complex 
frequency 
domain
Time 
domain
LTI
continuous 
system
=
×
)
(t
h
)
(s
F
)
(s
H
)
(s
Yf
)
(t
f
)
(t
y f
∗
)
(t
h
=
Fig. 6.8: Complex frequency domain analysis method.
Authenticated
32 PM

220
|
6 Analysis of continuous-time systems in the complex frequency domain
The above analysis process for an LTI system by using Laplace transform is plotted
in 󳶳Figure 6.8.
Example 6.6-3. The excitation of an LTI circuit is a rectangular pulse with amplitude
1 and width 1; the impulse response of the circuit is h(t) = e−atε(t). Find the zero-state
response yf(t).
Solution. From the given conditions, we have
L[f(t)] = 1 −e−s
s
,
so, the system function of the circuit is
H(s) = L[h(t)] =
1
s + a .
According to equation (6.6-8), the image function of the zero-state response is
Yf(s) = H(s) ⋅F(s) = 1 −e−s
s(s + a) =
1
s(s + a) −
e−s
s(s + a) .
(6.6-10)
The ﬁrst term on the right side of the upper equation can be decomposed into
1
s(s + a) = 1
a (1
s −
1
s + a) ,
so,
L−1 [1
a (1
s −
1
s + a )] = 1
a ε(t) −1
a e−atε(t) = 1
a (1 −e−at) ε(t) .
The original function of the second term on the right side of equation (6.6-10) is
L−1 [
e−s
s(s + a)] = 1
α [1 −e−a(t−1)] ε(t −1) ,
thus, the zero-state response yf(t) of this circuit is
yf(t) = 1
a (1 −e−at) ε(t) −1
a [1 −e−a(t−1)] ε(t −1) .
From the above, it can be seen that the zero-state response can be found by means of
the formula yf(t) = F−1 [Yf(jω)] = F−1[F(jω)H(jω)] in the Fourier transform method
and by yf(t) = L−1 [Yf(s)] = L−1[F(s)H(s)] in the Laplace transform method, and there
seems to be no essential difference between them. In the formulas, the Laplace trans-
form method is just the Fourier transform method when the variable jω is replaced by
s. Therefore, once an excitation signal satisﬁes the Dirichlet conditions, the zero-state
response of a system can both be obtained by the Fourier transform method and the
Laplace transform method. The advantage of the Laplace transform method is only
that it can help to ﬁnd the zero-state responses to some signals that cannot meet the
Dirichlet conditions, so, it is just a supplement to the Fourier transform method rather
than the core value of the Laplace transform approach. Then, what is the most impor-
tant characteristic of the Laplace transform method?
Authenticated
32 PM

6.7 Analysis methods with system models in the s domain
|
221
6.7 Analysis methods with system models in the s domain
6.7.1 Analysis with mathematic models
As we know, the mathematical model of an LTI system is a differential equation, and
the differential operation can be transformed into multiplication by means of the dif-
ferential property of the Laplace transform in the frequency domain. Therefore, dif-
ferential equations that are difficult to solve can be converted into a simple algebraic
equation. Simultaneously, the boundary conditions of the system are also contained
in the algebraic equation, so the full solution of the equation can be obtained in one
stroke. This is the ace in the hole of the Laplace transform analysis method.
The steps to solve the full response of system using the Laplace transform are as
follows:
Step 1: According to circuit laws (KVL, KCL, etc.), build the system’s mathematical
model, namely get a linear differential equation with constant coefficients (if
there are integral terms in it, we should perform the differential operation on the
equation to remove the integral operations, and then a differential equation with
higher one order than the original one can be obtained).
Step 2: Change this differential equation of the system in the time domain into the al-
gebraic equation in the complex frequency domain using the differential property
of the Laplace transform, and substitute the boundary conditions into it.
Step 3: Find the solution of the algebraic equation in the complex frequency domain.
Step 4: Restore the solution in the time domain from the one in the complex fre-
quency domain using the inverse Laplace transform.
This system analysis method is also called the transform method, in which the Laplace
transform is a kind of transferring machine; when we put a differential equation in the
time domain into this machine, it can produce the algebraic equation in the complex
frequency domain as a product that is easy to handle, just like peanuts are squeezed
into peanut oil by a machine. The schematic diagram is shown in 󳶳Figure 6.9.
Example 6.7-1. In the circuit shown in 󳶳Figure 6.10, we know uS(t) = 3
5e−2tε(t) and
the starting condition uC(0−) = −2 V, RC = 0.2. Solve the voltage across the capacitor.
An algebraic equation
in s-domain
A differential equation
in time-domain
Laplasse
transform
machine
Peanut
transform
machine
Peanuts
Oils
ay' (t) + by (t) = cf (t)
asY (s) + bY (s) = cF (s)
Fig. 6.9: The machines of Laplace transform and peanuts transform.
Authenticated
32 PM

222
|
6 Analysis of continuous-time systems in the complex frequency domain
R
C
S( )
u t
C( )
u
t
Fig. 6.10: E6.7-1.
Solution. According to Kirchhoff’s law, the differential equation of the system is
duC(t)
dt
+ 1
RC uC(t) = 1
RC uS(t) .
Appling Laplace transform and the differential property to both sides of the upper
equation, we obtain
sUC(s) −uC(0−) + 5UC(s) = 5US(s) ,
namely,
UC(s) =
1
s + 5 [5US(s) + uC(0−)] .
Because
US(s) = L−1 [uS(t)] = 3
5 ⋅
1
s + 2,
uC(0−) = −2 ,
we have
UC(s) =
3
(s + 2)(s + 5) −
2
s + 5 =
1
s + 2 −
1
s + 5 −
2
s + 5 =
1
s + 2 −
3
s + 5
The capacitor voltage uC(t) is
uC(t) = e−2tε(t) −3e−5tε(t) .
From this example, we can see that the zero-state response and the zero-input re-
sponse of system can be found by the Laplace transform at the same time, which is
impossible to do with the Fourier transform method. In other words, the most obvious
characteristic of the Laplace transform is that the complete response of system can be
directly obtained by solving the system mathematical model at one time. The reason
for this advantage is that the differential property of Laplace transform can contain
the starting values of a signal.
Now, calculation for the complete response is given. If the differential equation
model of an LTI system is
y(n)(t) + an−1y(n−1)(t) + ⋅⋅⋅+ a1y󸀠(t) + a0y(t)
= bmf (m)(t) + bm−1f (m−1)(t) + ⋅⋅⋅+ b1f 󸀠(t) + b0f(t) ,
(6.7-1)
letting L[y(t)] = Y(s), L[f(t)] = F(s), using the differential property of Laplace trans-
form in the time domain, the Laplace transform of y(i)(t) which is i-th order derivative
of y(t) is
L [y(i)(t)] = siY(s) −Pi(s) ,
Authenticated
32 PM

6.7 Analysis methods with system models in the s domain
|
223
where
Pi(s) = si−1y(0−) + si−2y󸀠(0−) + ⋅⋅⋅+ sy(i−2)(0−) + y(i−1)(0−) .
If f(t) is a causal signal, its value is zero at time t = 0−,
f(0−) = f 󸀠(0−) = ⋅⋅⋅= f (m−1)(0−) = 0 ,
and then L [f (j)(t)] = sjF(s). With Laplace transform on both sides of equation (6.7-1),
we obtain
A(s)Y(s) −C(s) = B(s)F(s) ,
(6.7-2)
and
A(s) = sn + an−1sn−1 + ⋅⋅⋅+ a1s + a0 ,
C(s) = Pn(s) + an−1Pn−1(s) + ⋅⋅⋅+ a1P1(s) ,
B(s) = bmsm + bm−1sm−1 + ⋅⋅⋅+ b1s + b0 .
From equation (6.7-2), we have
Y(s) = C(s)
A(s) + B(s)
A(s) F(s) ,
(6.7-3)
where the ﬁrst term C(s)
A(s) is only related to the response y(t) and its all-order derivatives
at t = 0−; and the second term B(s)
A(s)F(s) is only related to the excitation f(t). Therefore,
C(s)
A(s) is the image function Yx(s) of the zero-input response, but B(s)
A(s)F(s) is the image
function Yf(s) of the zero-state response. That is,
Y(s) = L [yx(t)] + L [yf(t)] = Yx(s) + Yf(s) .
(6.7-4)
Example 6.7-2. The differential equation of a system is
y󸀠󸀠(t) + 5y󸀠(t) + 6y(t) = f 󸀠(t) + 6f(t) ,
the starting conditions are y(0−) = 1, y󸀠(0−) = 2, and the excitation is ε(t). Find the
zero-input response and the zero-state response.
Solution. With the Laplace transform on both sides of the equation at the same time,
we obtain
s2Y(s) −sy(0−) −y󸀠(0−) + 5sY(s) −5y(0−) + 6Y(s) = sF(s) −f(0−) + 6F(s) .
Because
f(0−) = 0 ,
we have
(s2 + 5s + 6) Y(s) −sy(0−) −y󸀠(0−) −5y(0−) = (s + 6)F(s) ,
Authenticated
32 PM

224
|
6 Analysis of continuous-time systems in the complex frequency domain
and the solution is
Y(s) =
s + 6
s2 + 5s + 6 F(s) + sy(0−) + y󸀠(0−) + 5y(0−)
s2 + 5s + 6
.
The ﬁrst term is only related to the excitation, which is the image function of the zero-
state response. The second term is only related to the starting conditions, which is the
image function Yx(s) of the zero-input response. Substituting F(s) = 1
s into the ﬁrst
term, we obtain
Yf(s) =
s + 6
s(s + 2)(s + 3) = 1
s −
2
s + 2 +
1
s + 3 .
Thus, the zero-state response is
yf(t) = ε(t) −2e−2tε(t) + e−3tε(t) .
According to the starting conditions y(0−) = 1, y󸀠(0−) = 2, we obtain
Yx(s) =
s + 7
(s + 2)(s + 3) =
5
s + 2 −
4
s + 3 ,
and the zero-input response is
yx(t) = 5e−2tε(t) −4e−3tε(t) .
The complete response is
y(t) = yf(t) + yx(t) = ε(t) + 3e−2tε(t) −3e−3tε(t) .
So far, two purposes of the introduction of the Laplace transform have been achieved.
One was to remedy the shortcoming or deﬁciency of the Fourier transform, which can
solve the problem that Fourier transforms of some signals are nonexistent. The other
was to obtain the zero-state and the zero-input responses, which form the complete
response, at one time. The key to solve the ﬁrst problem is the introduction of the
attenuation factor e−σt, whereas the fundamental reason for solving the second one
lies in the differential property of the Laplace transform.
The free response is constituted by the superposition of all components related to
the poles of H(s) in the complete response, and the remaining part of the full response
is the forced response.
6.7.2 Analysis with a circuit model
As stated in the above, if we want to analyze a system by means of the Laplace trans-
form, the ﬁrst thing is to establish the system’s mathematical model, that is, the dif-
ferential equation in the time domain. This is not difficult for general simple circuits,
but for some complex circuits, their differential equations may be too complicated to
build. So, we must try to ﬁnd a new way for this problem.
Authenticated
32 PM

6.7 Analysis methods with system models in the s domain
|
225
We know that the zero-input response roots in the energy storage elements (dy-
namic elements) in a system, such as inductors and capacitors. That is, the memory
(storage) property of the dynamic elements results in the zero-input response. There-
fore, understanding the circuit models of dynamic elements in the s domain is the
basis or premise for ﬁnding the full response from the circuit models.
As we know, a circuit system is generally composed of resistors, inductors and
capacitors. If circuit models of the components can be drawn in the s domain, then
the mathematic model of the system in the s domain can also be written directly from
KCL and KVL laws, etc., so that this method omits the complicated process where the
differential equation of the system in the time domain must be established ﬁrst and
then be transformed to the algebraic equation in Section 6.7.1; as a result, the purpose
of simplifying the solution steps of the equation has been achieved. So, ﬁrst we will
introduce the s domain mathematic models of Kirchhoff’s laws.
In the time domain, the expression of the KCL is ∑i(t) = 0. The Laplace transform
on both sides of the equation, yields
∑I(s) = 0 .
(6.7-5)
Equation (6.7-5) is the representation of the KCL in the s domain, where I(s) is the
image function of the corresponding branch current i(t). Equation (6.7-5) states that
the algebraic sum of the outﬂow and the inﬂow image currents at any a node in a
given circuit is zero.
Similarly, the representation of the KVL in the s domain is
∑U(s) = 0 .
(6.7-6)
That means that for any a closed loop in a circuit, the algebra sum of image voltages
on all components equals zero.
Next, we will discuss the circuit models for the elements R, L and C in the s do-
main.
1. Resistor model
At any moment, voltage uR(t) and current iR(t) for an LTI resistor R are restrained by
uR(t) = RiR(t) .
With the Laplace transform on both sides of the equation and setting L [uR(t)] = UR(s)
and L [iR(t)] = IR(s), we obtain
UR(s) = RIR(s) .
(6.7-7)
Equation (6.7-7) shows the relationship between the current and the voltage of R in the
s domain, which is called the s domain mathematical model of R. The circuit models
of a resistor R in the time domain and the s domain are illustrated in 󳶳Figure 6.11.
Authenticated
32 PM

226
|
6 Analysis of continuous-time systems in the complex frequency domain
s-domain model
R
+
)
(t
iR
)
(t
uR
R
+
)
(s
U R
R
)
(s
I
Time-domain model
(a)
(b)
Fig. 6.11: Time domain and s domain circuit models of a resistor.
2. Inductor model
At any moment, voltage uL(t) and current iL(t) of an LTI inductor L are restrained by
uL(t) = L diL(t)
dt
.
With the Laplace transform on both sides of the equation and letting L [uL(t)] = UL(s),
L [iL(t)] = IL(s), we obtain the s domain mathematical model of an inductor
UL(s) = sLIL(s) −LiL(0−) .
(6.7-8)
In equation (6.7-8), sL is called the inductive reactance in the s domain. It can be seen
that the image function UL(s) of the voltage on an inductor consists of two parts; one
is the product of the inductive reactance sL and the image current IL(s), the other is
the equivalent voltage source LiL(0−) in the s domain. As a result, the inductor L in the
s domain can be seen as a series circuit with an inductive reactance sL and a voltage
source LiL(0−). So, the circuit models of an inductor in the time and s domains are
shown in 󳶳Figure 6.12.
3. Capacitor model
At any moment, voltage uC(t) and current iC(t) of an LTI capacitor C are restrained by
uC(t) = 1
C
t
∫
−∞
iC(τ)dτ .
WiththeLaplacetransform onbothsides of theequationand setting L [uC(t)] = UC(s),
L [iC(t)] = IC(s), we can get the s domain mathematical model of a capacitor
UC(s) = 1
sC IC(s) + 1
sC
0−
∫
−∞
iC(τ)dτ = 1
sC IC(s) + 1
s uC(0−) ,
(6.7-9)
Model of time-domain 
Series model of s-domain
Parallel model of s-domain 
L
sL
)
(t
iL
)
(t
u L
)
(s
I L
)
0
(
−
L
Li
)
(s
U L
)
(s
I L
)
(s
U L
)
0
(
1
−
Li
s
sL
(a)
(b)
(c)
Fig. 6.12: Time domain and s domain circuit models of a inductor.
Authenticated
32 PM

6.7 Analysis methods with system models in the s domain
|
227
)
(t
uC
)
(t
iC
)
(s
I C
)
0
(
1
−
C
u
s
C
sC
1
)
(s
I C
)
(s
U C
sC
1
)
(s
U C
)
0
(
−
C
Cu
Model of time-domain 
Series model of s-domain
Parallel model of s-domain 
(a)
(b)
(c)
Fig. 6.13: Time domain and s domain circuit models of a capacitor.
where UC(s)and IC(s)aretheimagevoltageand image current, and 1
sC isthecapacitive
reactance in the s domain. It can be seen that the image voltage across a capacitor is
composed of two parts; the ﬁrst term is the product of the capacitive reactance and
the image current, the second is equivalent to a voltage source in the s domain, which
can be called the internal image voltage source. Based on the s domain KVL model, the
relation between UC(s) and IC(s) can be considered as a series circuit with a capacitive
reactance 1
sC and an image voltage source 1
s uC(0−). The circuit models of a capacitor
in the time and s domains are shown in 󳶳Figure 6.13.
The component circuit models in equations (6.7-8) and (6.7-9) are called the series
circuit models. If the two formulas are rewritten as
IL(s) = 1
sL UL(s) + 1
s iC(0−)
(6.7-10)
and
IC(s) = sCUC(s) −CuC(0−) ,
(6.7-11)
the s domain parallel circuit models of two elements can be obtained, which are illus-
trated in 󳶳Figure 6.12c and 󳶳Figure 6.13c, respectively.
In practice, which model should be adopted by us depends on the speciﬁc con-
ditions. If we need to write out voltage equations, the series model is the best choice,
and the parallel model is more suitable for current equations.
It can be seen from equations (6.7-8) and (6.7-9) that on their right-hand sides,
besides responses sLIL(s) and
1
sC IC(s) which can reﬂect the effect of the excitation
signal, there are also terms such as LiL(0−) and 1
s uC(0−), which can reﬂect the effect
from the state of the system, and the existence of the two terms just is the root cause to
obtain the complete response in one stroke by the Laplace transform. So, the essential
difference between the Laplace and Fourier transform analysis methods lies in their
different differential properties.
Example 6.7-3. In the circuit shown in 󳶳Figure 6.14a, f(t) = 3e−10tε(t), uC(0−) = 5 V
are known. Find the voltage y(t) on a resistor.
Authenticated
32 PM

228
|
6 Analysis of continuous-time systems in the complex frequency domain
+
−
)
(t
f
Ω
k
1
)
(t
y
F
100 μ
Ω
k
1
+
−
Ω
k
1
Ω
k
1
)
( s
F
s
5
F
s
104
C( )
u
t
1( )
I s
2( )
I
s
( )
Y s
(a)
(b)
Fig. 6.14: E6.7-3.
Solution. The s domain model of the circuit is shown in 󳶳Figure 6.14b. From Kirch-
hoff’s laws, we have
Y(s) = 1, 000 [I1(s) + I2(s)]
F(s) = Y(s) + 104
s I1(s) + 5
s
F(s) = Y(s) + 1, 000I2(s)
Solving the three simultaneous equations yields
Y(s) = s + 10
s + 20F(s) −
5
s + 20 .
Substituting F(s) =
3
s+10 into the upper equation, we obtain
Y(s) = −
2
s + 20 ,
so,
y(t) = −2e−20tε(t) V .
Example 6.7-4. For the circuit shown in 󳶳Figure 6.15a, the excitation is u(t) and the
response is i(t). Solve the impulse response and step response of the circuit.
Solution. According to the s domain model of zero-state response [as shown in
󳶳Figure 6.15b], we obtain the system function
H(s) = I(s)
U(s) =
1
sL1 + R1(sL2+R2)
sL2+R1+R2
=
1
s + 2(s+3)
s+5
=
s + 5
s2 + 7s + 6 .
Then,
h(t) = L−1[H(s)] = L−1 [
s + 5
s2 + 7s + 6] = L−1 [4
5 ×
1
s + 1 + 1
5 ×
1
s + 6]
= (4
5e−t + 1
5 e−6t) ε(t) .
The image function G(s) of the step response is
G(s) = H(s) ⋅L [ε(t)] =
s + 5
s (s2 + 7s + 6) = 5
6 × 1
s −4
5 ×
1
s + 1 −1
30 ×
1
s + 6 ,
Authenticated
32 PM

6.7 Analysis methods with system models in the s domain
|
229
1L
+
−
)
(t
u
H
1
2
L
H
1
1
R
2
R
1
sL
+
−
)
(s
U
2
sL
1
R
2
R
)
(t
i
)
(s
I
Ω
2
Ω
2
Ω
3
Ω
3
(a)
(b)
Fig. 6.15: E6.7-4.
so, the step response is
g(t) = L−1[G(s)] = (5
6 −4
5e−t −1
30e−6t) ε(t) .
Example 6.7-5. A circuit is shown in 󳶳Figure 6.16a. When t < 0, the switch S turns
off, and then the circuit is in steady state; when t = 0, the switch S turns on. Find the
output voltage u(t) when t ≥0.
Solution. First, ﬁnd the starting values of the capacitor voltage and inductor current,
uC(0−) = 12 V,
iL(0−) = uS
R2
= 3 A .
According to the s domain model of the circuit shown in 󳶳Figure 6.16b, we can obtain
the algebraic equation of the s domain model of the circuit, that is,
[ 1
R3
+
1
R1 + 1
sC
+
1
R2 + sL ] U(s) = US(s)
R3
+
uC(0−)
s
R1 + 1
sC
−LiL(0−)
R2 + sL
Substituting the element parameters and initial conditions into the upper equation,
we obtain
[1
2 +
1
2 + 2
s
+
1
s + 4] U(s) = 6
s +
12
s
2 + 2
s
−
3
s + 4 ,
so,
U(s) = 9s2 + 51s + 24
s (s2 + 5.5s + 3) =
9s2 + 51s + 24
s(s + 4.89)(s + 0.615) .
+
−
Ω
2
1
R
S
2
R
Ω
4
C
L
sL
+
−
+
−
F
2
1
3
R
Ω
2
V
12
H
1
+
−
S
3
R
Ω
2
sC
1
s
uC
)
0
( −
)
0
( −
L
Li
1
R
Ω
2
2
R
Ω
4
)
( s
US
)
(t
uS
Fig. 6.16: E6.7-5.
Authenticated
32 PM

230
|
6 Analysis of continuous-time systems in the complex frequency domain
We have the partial fraction expansion of the above equation,
U(s) = 7.98
s
−
0.493
s + 4.89 +
1.5
s + 0.615 .
Thus, the output voltage u(t) is
u(t) = L−1[U(s)] = (7.98 −0.493e−4.89t + 1.5e−0.615t) ε(t) .
6.8 Analysis method from signal decomposition in the s domain
According to the contents of Section 5.6.3, there is a similar derivation process for the
signal decomposition analysis method in complex frequency domain.
From equation (6.1-4), an aperiodic signal f(t) can be represented as the linear
combination of inﬁnite terms of complex exponential signals just like est. Therefore,
the zero-state response yf1(t) to the basic signal est must be solved ﬁrst. We have
yf1(t) = est ∗h(t) =
∞
∫
−∞
h(τ)es(t−τ)dτ =est
∞
∫
−∞
h(τ)e−sτdτ .
The term ∫
∞
−∞h(τ)e−sτdτ = ∫
∞
−∞h(t)e−stdt is just the Laplace transform H(s) of h(t),
and, therefore,
yf1(t) = H(s)est .
(6.8-1)
Equation (6.8-1) states that the zero-state response to the basic signal est is the product
of the signal itself and a constant coefficient, which is independent of time t. This
coefficient is the Laplace transform H(s) of the impulse response h(t). Thus, we reach
the same conclusion as equation (6.6-8).
The Laplace transform Yf(s) of the zero-state response of a system to any aperiodic
signal f(t) is equal to the product of the Laplace transform F(s) of this signal and the
system function H(s). That is,
Yf(s) = F(s)H(s) .
The derivation process is similar to that shown in 󳶳Figure 5.26.
From the above introduction and Chapters 3–5, we ﬁnd that if the continuous sys-
tem analysis is regarded as a big stage for performance, then the system function is
a leading role. It holds the frequency domain with the left hand and the complex fre-
quency domain with the right hand, steps on the time domain operator, wears the
impulse response as a vest, and frequently appears on the stage to show and attract
the attention of audiences with the zero-state response as a spotlight. Moreover, in
the subsequent contents of the discrete system analysis, audiences can still see his
charming ﬁgure; the difference is just that it is on another stage. Therefore, it is no
exaggeration to say that the system function is a key for system analysis. Readers are
advised to understand its concept and meaning fully in order to judge the whole from
one sample and to draw parallels from its inference.
Authenticated
32 PM

6.9 Relationship among the time, frequency and complex frequency domain methods
|
231
6.9 Relationship among the time, frequency and complex
frequency domain methods
From Chapters 3–6, we have learned some basic analysis methods for an LTI system,
and have built the following three basic mathematical models for solving the zero-
state response.
The model in the time domain is
yf(t) = f(t) ∗h(t) .
(6.9-1)
The model in the frequency domain is
Yf(jω) = F(jω)H(jω) .
(6.9-2)
The model in the complex frequency domain is
Yf(s) = F(s)H(s) .
(6.9-3)
Equations (6.9-2) and (6.9-3) are, respectively, the transform forms of equation (6.9-1)
in the frequency and the complex frequency domains. Their main function is to trans-
form the integral operation in the time domain into the algebraic operation in trans-
form domain, and thus can greatly simplify the system analysis process.
The Laplace transform extends the application range of the Fourier transform and
is widely used in system analysis, but because
(1) the Laplace transform has no obvious physical meaning;
(2) the result of the analysis of a physical system has to be implemented on the time
and frequency characteristics of the system in the end;
So, we ﬁnd that the system analysis methods are essentially divided into only two
types: time domain and frequency domain. The method in the complex frequency do-
main is not only the extension of one in frequency domain (the Laplace transform can
be regarded as the generalized Fourier transform), but also the link between methods
in the time and frequency domains. The relationship between them is shown concep-
tually in 󳶳Figure 6.17.
Note that 󳶳Figure 6.17 emphasizes the relationship between the three methods in
concept, but in the real analysis, the Fourier transform of a signal could not be ob-
tained directly by replacing s with jω in the Laplace transform only when the Laplace
transform ROC contains a jω axis. If the ROC does not contain the jω axis, then the
signal does not have the corresponding Fourier transform. If the boundary of the ROC
is located on the jω axis, the Fourier transform of a signal could not be obtained by
replacing s with jω directly, even though the Fourier transform of the signal exists, but
it could be obtained by
F(jω) = F(s) 󵄨󵄨󵄨󵄨s=jω + π
N
∑
i=1
Kiδ(ω −ωi) ,
(6.9-4)
Authenticated
32 PM

232
|
6 Analysis of continuous-time systems in the complex frequency domain
( )
f t
( )
F s
(j )
F
ω
( )
Y s
( )
( )
f t
h t
∗
( )
( )
F s H s
(j )
(j )
F
H
ω
ω
j
s
ω=
1
−
j
s
ω
=
time-domain analysis
s-domain analysis
frequency-domain analysis
( )
y t
(j )
Y
ω
Fig. 6.17: The bridgefunction of the Laplace transform in system analysis.
where the second term on the right of the expression is the sum of the impulse signals
located on the poles of F(s), such as jω1, jω2, . . . , jωN. The Ki represents all coeffi-
cients of the partial fractions decomposed from F(s).
Example 6.9-1. Using F(jω) = F(s) 󵄨󵄨󵄨󵄨s=jω + π ∑N
i=1 Kiδ(ω −ωi) solve the Fourier trans-
form of signal f(t) = cos ω0tε(t).
Solution. According to Table 6.2, we obtain
F(s) =
s
s2 + ω2
0
,
(6.9-5)
This can be expanded as the partial fractions
F(s) =
s
s2 + ω2
0
= 1
2 (
1
s + jω0
) + 1
2 (
1
s −jω0
) .
(6.9-6)
The poles of the ﬁrst formula are jω0 and −jω0, and we have
K1 = 1
2 ,
K2 = 1
2 .
So,
F(jω) = F(s) 󵄨󵄨󵄨󵄨s=jω + π
N
∑
i=1
Kiδ(ω −ωi) =
jω
ω2
0 −ω2 + π
2 [δ(ω −ω0) + δ(ω + ω0)] .
We should explain that because h(t), H(jω) and H(s) are all deﬁned under the zero-
state condition, when we employ above models to ﬁnd the system response we can
directly write the results as “system response is y(t)” and do not need to consider
whether the response is the zero-state, zero-input or full response.
Authenticated
32 PM

6.10 Solved questions
|
233
6.10 Solved questions
Question 6-1. Find the inverse Laplace transform of
s+3
s2+2s+2e−s.
Solution. Using partial fraction expansion, we have
s + 3
s2 + 2s + 2 =
s + 1
(s + 1)2 + 1 +
2
(s + 1)2 + 1 .
So, the original function of
s+3
s2+2s+2 is e−t[cos t + 2 sin t]ε(t), and the original function
of
s+3
s2+2s+2e−s is e−(t−1)[cos(t −1) + 2 sin(t −1)]ε(t −1).
Question 6-2. For the circuit in 󳶳Figure Q6-2-1 and 󳶳Figure Q6-2-2, iL(0−) = 1 A,
uC(0−) = 1 V are known. Plot the equivalent circuit in its s domain and calculate iR(t).
+
−
( )V
t
ε
1Ω
L ( )
i
t
C ( )
u
t
+
−
0.5F
( )A
t
ε
R ( )
i
t
1H
Fig. Q6-2-1
+
−
1
s
1Ω
s
+
−
1V
−
+
1 s
2 s
1 s
A
R ( )
I
s
R ( )
U
s
Fig. Q6-2-2
Solution. The equivalent circuit of 󳶳Figure Q6-2-1 s domain is shown in 󳶳Figure Q6-
2-2.
Let the node voltage at point A be UR(s). Using node voltage method, the node
voltage equation of point A can be written as
(1 + s
2 + 1
s ) UR(s) = −1
s +
1
s
2
s
+
1
s + 1
s
.
So,
UR(s) =
s2 + 2
s (s2 + 2s + 2) = 1
s +
−2
(s + 2)2 + 1 .
Using the inverse Laplace transform, we obtain uR(t) = 1 −2e−t sin t, t > 0. So,
iR(t) = uR(t)
1
= uR(t) .
Authenticated
32 PM

234
|
6 Analysis of continuous-time systems in the complex frequency domain
Question 6-3. The block diagram of the LTI system in 󳶳Figure Q6-3 is known,
and when f(t)
=
3 (1 + e−t) ε(t), the total response of the system is y(t)
=
(4e−2t + 3e−3t + 1) ε(t).
(1) Find input–output equation of the system.
(2) Find the zero-input response yx(t) of the system.
(3) Find the starting states y(0−) and y󸀠(0−) of the system.
( )
f t
+
∫
∫
( )
y t
−
−
+
+
+
6
5
2
3
1
Fig. Q6-3
Solution. (1) From the diagram, the differential equation of the system is
y󸀠󸀠(t) + 5y󸀠(t) + 6y(t) = f 󸀠󸀠(t) + 3f 󸀠(t) + 2f(t) .
(6.10-1)
(2) With the Laplace transform in the equation (6.10-1), we have
Y(s) = sy(0−) + y󸀠(0−) + 5y(0−)
s2 + 5s + 6
+ s2 + 3s + 2
s2 + 5s + 6F(s) = Yx(s) + s + 1
s + 3 F(s) . (6.10-2)
Finding Laplace transforms of the known excitation and response and putting
them into the equation (6.10-2),
Yx(s) =
4
s + 2 +
3
s + 3 + 1
s −s + 1
s + 3 (1
s +
3
s + 1) =
2s + 8
(s + 2)(s + 3)
=
4
s + 2 −
2
s + 3 .
(6.10-3)
Then, calculating the inverse Laplace transform of the equation (6.10-3), yields
yx(t) = 2 (2e−2t −e−3t) ε(t) .
(6.10-4)
(3) From equation (6.10-2), we have
Yx(s) = sy(0−) + y󸀠(0−) + 5y(0−)
s2 + 5s + 6
.
(6.10-5)
From equation (6.10-3), we have
Yx(s) =
2s + 8
(s + 2)(s + 3) .
(6.10-6)
Comparing equations (6.10-5) and (6.10-6), we obtain
{
{
{
y(0−) = 2
y󸀠(0−) + 5y(0−) = 8
→
{
{
{
y(0−) = 2
y󸀠(0−) = −2
.
Authenticated
32 PM

6.10 Solved questions
|
235
Question 6-4. The differential equation of an LTI causal system is
y󸀠󸀠(t) + 3y󸀠(t) + 2y(t) = 5f 󸀠(t) + 4f(t)
t > 0 .
The inputf(t) = e−3tε(t) and the starting states are y(0−) = 2, y󸀠(0−) = 1. In the s
domain ﬁnd
(1) the zero-input response yx(t) and the zero-state response yf(t) of the system;
(2) the system function H(s), the impulse response h(t), and judge whether the sys-
tem is stable; and
(3) if the input is f(t) = e−3tε(t −2), calculate (1) and (2) again.
Solution. (1) With the Laplace transform on both sides of the differential equation,
the input–output expression in the s domain can be obtained,
s2Y(s) −sy(0−) −y󸀠(0−) + 3 [sY(s) −y(0−)] + 2Y(s) = (5s + 4)F(s) .
So, the total response of the system in the s domain is
Y(s) = sy(0−) + y󸀠(0−) + 3y(0−)
s2 + 3s + 2
+
5s + 4
s2 + 3s + 2 F(s) .
The starting conditions are y(0−) = 2 and y󸀠(0−) = 1, and the total response is
Y(s) =
2s + 7
s2 + 3s + 2 +
5s + 4
s2 + 3s + 2 F(s) = Yx(s) + Yf(s) .
With the inverse Laplace transform in Yx(s) and zero-input response can be ob-
tained by
yx(t) = 5e−t −3e−t
(t ≥0) .
Because the image function of f(t) = e−3tε(t) is F(s) =
1
s+3, the zero-state response
is
yf(t) = (6e−2t −0.5e−t −5.5e−3t) ε(t) .
(2) According to (1), the expression of the zero-state response of the system in the s
domain is
Yf(s) =
5s + 4
s2 + 3s + 2 F(s) .
So,
H(s) = Yf(s)
F(s) =
5s + 4
s2 + 3s + 2 = −
1
s + 1 +
6
s + 2 .
Finding the inverse Laplace transform of the equation, we obtain the unit impulse
response
h(t) = (6e−2t −e−t) ε(t) .
There are a zero z = −4
5 and two poles p1 = −1, p2 = −2 for the system function;
obviously, both poles are on the left half-plane in the s domain, so the system is
stable.
Authenticated
32 PM

236
|
6 Analysis of continuous-time systems in the complex frequency domain
(3) When input is f(t) = e−3tε(t −2), the system function is changeless, so the system
is still stable, and the h(t) and the yx(t) of the system are also changeless.
Because
f(t) = e−3tε(t −2)
L
←→F(s) = e−6
s + 3 e−2s ,
we obtain
Yf(s) =
5s + 4
s2 + 3s + 2 F(s) =
5s + 4
s2 + 3s + 2
e−6
s + 3 e−2s
= e−6 [6
1
s + 2 −1
2
1
s + 1 −11
2
1
s + 3] e−2s .
Using the inverse Laplace transform in the above equation, we obtain the zero-state
response
yf(t) = e−6 [6e−2(t−2) −0.5e−(t−2) −5.5e−3(t−2)] ε(t −2) .
6.11 Learning tips
The complex frequency domain system analysis is the complement and promotion of
that of frequency domain, so we suggest that readers pay attention to the following
points.
(1) The Fourier transform is a special case of the Laplace transform.
(2) The system function and impulse response are the Laplace transform pair.
(3) The Laplace transform of the zero-state response of a system is the product of the
Laplace transform of excitation and the system function.
(4) The s domain model of a circuit is actually a diagram of the Laplace transform
applied on a differential equation of the system.
6.12 Problems
Problem 6-1. Find the unilateral Laplace transforms of the following signals:
(1) (3 sin 2t + 2 cos 3t)ε(t),
(2) 2δ(t) −e−tε(t),
(3) cos2 2tε(t)
(4) e−(t+α) cos ωtε(t),
(5) e−(t−1)ε(t −1),
(6) e−(t−1)ε(t)
(7) t[ε(t) −ε(t −1)],
(8) (t + 1)ε(t + 1),
(9) sin πt[ε(t) −ε(t −2)]
(10) te−(t−1)ε(t −2),
(11) t2ε(t −1),
(12) t2 cos t
Problem 6-2. The unilateral Laplace transform F(s) =
1
s2+3s−5 of a causal signal f(t) is
known. Find the unilateral Laplace transforms of following signals:
(1) e−tf(4t)
(2) f(2t −4)
(3) tf
󸀠󸀠(t)
(4) f(t) sin 2t
(5) ∫
t−2
0
f(τ)eτdτ
(6)
1
t f(t)
Authenticated
32 PM

6.12 Problems
|
237
Problem 6-3. Find the unilateral Laplace transforms of the signals shown in
󳶳Figure P6-3.
)
(
1 t
f
2
)
(
3 t
f
t
ω
sin
1
)
(
2 t
f
0
1
2
1
1
0
0
T
t
t
t
2
T
(a)
(b)
(c)
Fig. P6-3
Problem 6-4. A causal periodic signal f(t) = f(t)ε(t) with periodic T is known, the
image function is F(s), and the image function of the signal during the ﬁrst periodic
f1(t) = f(t)[ε(t) −ε(t −T)] is F1(s). Prove F(s) =
F1(s)
1−e−sT .
Problem 6-5. Using the conclusions of Problem 6-4, ﬁnd the image functions of the
following causal periodic signals shown in 󳶳Figure P6-5.
)
(
1 t
f
t
T
2
T
E
...
...
...
...
)
(
2 t
f
E
)
(
3 t
f
4
2
(1)
6
)
(
4 t
f
1
(a)
(b)
0
t
T
0
2
T
t
0
t
4
2
6
0
(c)
(d)
Fig. P6-5
Problem 6-6. Find the initial values and ﬁnal values of the following inverse Laplace
transforms:
(1) F(s) =
s+3
(s+1)(s+2)2 ;
(2) F(s) = s3+6s2+6s
s2+6s+8 ;
(3) F(s) =
s2+2s+3
(s+1)(s2+4)
(4) F(s) =
2s+1
s3+3s2+2s;
(5) F(s) = 1−e−2s
s(s2+4);
(6) F(s) =
1
s(1+e−s)
Problem 6-7. Find the original functions of the following image functions:
(1)
4s+2
s2+8s+15
(2)
s3+s−3
s2+3s+2
(3)
3s+4
s3+5s2+8s+4
(4)
s+5
s(s2+2s+5)
(5)
4s2+6
s3+s2−2
(6)
s+3
(s+1)3(s+2)
Problem 6-8. Find the inverse Laplace transforms of following functions:
(1)
2−e−3s
s+2
(2)
s(1+e−sT)
s2+π2
(3) ( 1+e−2s
s
)
2
(4)
1−e−s
4s(s2+1)
(5)
s+6e−s
s2+9
(6)
1
1+e−s
Authenticated
32 PM

238
|
6 Analysis of continuous-time systems in the complex frequency domain
(7)
1
s(1+e−s)
(8)
1
1−e−s
(9)
1
s(1−e−s)
(10) ln
s
s+9
Problem 6-9. The excitation is f(t) = e−tε(t), and the corresponding zero-state re-
sponse is yf(t) = ( 1
2e−t −e−2t + 1
2e−3t) ε(t). Find
(1) the impulse response h(t);
(2) the input–output equation.
Problem 6-10. The impulse response and the zero-state response of a circuit are, re-
spectively, h(t) = δ(t) −11e−10tε(t) and yf(t) = (1 −11t)e−10tε(t). Find the excitation
f(t).
Problem 6-11. The system function is H(s) =
s
s2+3s+2. Find the system response to the
following excitation f(t) and point out the free response and the forced response:
(1) f(t) = 10ε(t)
(2) f(t) = 10 sin(t)ε(t)
Problem 6-12. In the circuit shown in 󳶳Figure P6-12, iL(0−) = 2 A, iS(t) = 5tε(t) A.
Find uL(t).
S( )
i t
Ω
3
L( )
u
t
H
2
1
−
+
L( )
i t
Fig. P6-12
Problem 6-13. The circuit shown in 󳶳Figure P6-13 is in steady state when t < 0, and
when t = 0 the switch S turns on. Find the capacitor voltage uC(t) when t ≥0.
2Ω
1H
1F
S
1V
+
−
Fig. P6-13
Problem 6-14. A circuit is shown in 󳶳Figure P6-14. It is known that C = 1 F, L = 1
2 H,
R1 =
1
5 Ω, R2 = 1 Ω, uC(0−) = 5 V, iL(0−) = 4 A and uS(t) = 10ε(t). Find the full
response i1(t) for t ≥0.
S( )
u t
)
(
1 t
i
+
−
1
R
C
2
R
L
L( )
i t
C( )
u
t
+
−
Fig. P6-14
Authenticated
32 PM

6.12 Problems
|
239
Problem 6-15. The starting state of an LTI system is given, an input is f1(t) = δ(t) and
the corresponding full response is y1(t) = −3e−t(t ≥0); another input is f2(t) = ε(t)
and the full response is y2(t) = 1 −5e−t(t ≥0). Find the full response to the input
f(t) = tε(t).
Problem 6-16. The starting states of an LTI system stay the same, when the input
f1(t) = δ(t), the full response y1(t) = δ(t) + e−tε(t), and when the input f2(t) = ε(t),
the full response y2(t) = 3e−tε(t). Find the response of the system y3(t) when
f3(t) =
{
{
{
{
{
{
{
0
t < 0
t
0 < t < 1
1
t > 1
.
Problem 6-17. The system function of a second-order LTI system H(s) =
s+3
s2+3s+2. The
input signal f(t) = e−3tε(t) and the starting conditions y(0−) = 1, y󸀠(0−) = 2. Find
the full response y(t), yx(t) and yf(t), and determine the free response and the forced
response.
Problem 6-18. The differential equation of a system y󸀠(t) + 2y(t) = f(t), the starting
condition y(0−) = 1, and f(t) = sin(2t) ⋅ε(t). Find the full response.
Problem 6-19. The differential equation of a system y󸀠󸀠(t) + 4y󸀠(t) + 3y(t) = 3f(t),
the excitation f(t) = ε(t), and the starting conditions y(0−) = 2, y󸀠(0−) = −1. Using
Laplace transform, ﬁnd the full response of the system.
Problem 6-20. Input–output equation of an LTI system y󸀠󸀠(t) + 5y󸀠(t) + 6y(t) = 6f(t).
Find the zero-input and zero-state responses yx(t) and yf(t) of the system when f(t) =
2e−tε(t), y(0−) = 0, and y󸀠(0−) = 1.
Problem 6-21. The differential equation of a system y󸀠󸀠(t) + 2y󸀠(t) + y(t) = f 󸀠(t), the
starting conditions y(0−) = 1, y󸀠(0−) = 2, and the input signal. Find the zero-input,
zero-state response and the full responses of the system and point out the free and the
forced responses.
Authenticated
32 PM

Authenticated
32 PM

7 Simulation and stability analysis of
continuous-time systems
Questions: Graphs are a very useful tool to analyze and solve. Can we, then, use a
graph instead of a mathematical model to represent a system? Further, how can
we test the stability of a feedback system?
Solution: Use several basic graphs to represent basic mathematical operations →
simulate the system structure with these basic graphic elements →ﬁnd the re-
lationship between system function and system stability.
Results: System representation with the block diagram and the ﬂow graphs, and
pole-zero analysis of the system function.
7.1 System simulation
A simple LTI system (single-input/single-output) can be represented by its mathemat-
ical model, namely the linear constant coefficient differential equation. In reality, peo-
ple would like to use a graph as an auxiliary tool or even as a replacement for the math-
ematical model in order to make a problem clearer and more intuitive. The graphical
representation (simulation) concept of systems was introduced in Chapter 2, and the
related details will be discussed in this chapter.
7.1.1 Basic arithmetic units
The block diagrams of some basic operators in the time domain were given in Chap-
ter 2, and 󳶳Figure 7.1a, b, respectively, show the symbols and the operational rela-
tionships of an adder/summer and a number multiplier in the time and s domains.
The symbols of two units are same in the time and s domains, but the representation
of an integrator is a little complex. We know that the relationship between the output
)
(
1 t
f
)
(
2 t
f
)
(
)
(
)
(
2
1
t
f
t
f
t
y
+
=
)
(
1 s
F
)
(
2 s
F
)
(
)
(
)
(
2
1
s
F
s
F
s
Y
+
=
+
+
+
+
)
(s
F
)
(
)
(
s
aF
s
Y
=
)
(t
f
a
)
(
)
(
t
af
t
y
=
a
Summer models in time-domain and complex frequency-domain
Multiplier models in time-domain and complex frequency-domain 
(a)
(b)
Fig. 7.1: Summer and multiplier models in time domain and complex frequency domain.
https://doi.org/10.1515/9783110419535-007
Authenticated
32 PM

242
|
7 Simulation and stability analysis of continuous-time systems
s
y
)
0
(
−
s
s
F
s
y
s
Y
)
(
)
0
(
)
(
+
=
−
)
(s
F
)
(t
f
∫
)
0
(
−
y
0
( )
(0 )
( )d
t
y t
y
f τ
τ
−
−
=
+∫
+
+
)
(t
f
f
0
( )
( )d
t
y t
f τ
τ
−
= ∫
)
(s
F
f
( )
( )
F s
Y s
s
=
Integrator model in time-domain and complex frequency-domain 
Integrator simplified model in time-domain and complex frequency-domain 
+
+
∫
1
s
1
s
(a)
(b)
Fig. 7.2: Integrator models in time domain and complex frequency domain.
and the input signal of an integrator in the time domain is
y(t) =
t
∫
−∞
f(τ)dτ =
0−
∫
−∞
f(τ)dτ +
t
∫
0−
f(τ)dτ = y(0−) +
t
∫
0−
f(τ)dτ ,
(7.1-1)
and the Laplace transform is
Y(s) = y(0−)
s
+ F(s)
s
.
(7.1-2)
Therefore, the time and s domain block diagrams of an integrator are, respectively,
shown in 󳶳Figure 7.2a. If the starting condition is y(0−) = 0, the simpliﬁed model of
an integrator is as illustrated in 󳶳Figure 7.2b.
7.1.2 Simulating system with block diagrams
We can say that:
(1) The process using basic operation units (integrator, scalar multiplier and adder) to
describe the system function (equation model) of a system is called system simula-
tion.
(2) The simulation based on block diagrams of operation units is called block diagram
simulation, while with ﬂow graphs of operation units it is called ﬂow graph simula-
tion.
The following examples will illustrate processes or methods of block diagram simula-
tion.
Example 7.1-1. Draw the block diagram of the RC circuit shown in 󳶳Figure 7.3, where
ui(t) and uo(t) are, respectively, the excitation and the response.
Authenticated
32 PM

7.1 System simulation
|
243
R
C
o( )
u t
i( )
u t
+
+
−
)
(t
i
−
Fig. 7.3: E7.1-1.
i( )
U s
R
1
)
(s
I
Cs
1
o( )
U
s
+
−
Fig. 7.4: E7.1-1 result.
Solution. The system equations in the complex frequency domain are
I(s) = 1
R [Ui(s) −Uo(s)] ,
(7.1-3)
Uo(s) = 1
Cs I(s) .
(7.1-4)
Equation (7.1-3) shows that there should be a summer to add −Uo(s) and Ui(s) in the
block diagram; the output Uo(s) and the input Ui(s) can merge together to form an in-
termediate function I(s). Then I(s) will be transformed into the output Uo(s) according
to equation (7.1-4), so the ﬁnal result can be shown in 󳶳Figure 7.4.
Example 7.1-2. The input–output equation of a system is given by
y󸀠󸀠(t) + 3y󸀠(t) + 2y(t) = f 󸀠(t) + f(t) .
Plot the system simulation block diagram.
Solution. The zero-state algebraic equation of the system in the s domain is
s2Y(s) + 3sY(s) + 2Y(s) = sF(s) + F(s) .
(7.1-5)
Equation (7.1-5) can be rewritten as
Y(s) =
s + 1
s2 + 3s + 2 F(s) =
1
s + 1
s2
1 + 3
s + 2
s2
F(s) =
1
s
1 + 3
s + 2
s2
F(s) +
1
s2
1 + 3
s + 2
s2
F(s) .
(7.1-6)
Suppose that
X(s) =
1
1 + 3
s + 2
s2
F(s) ,
(7.1-7)
then equation (7.1-6) can be changed into
Y(s) = 1
s X(s) + 1
s2 X(s) .
This means that Y(s) can be seen as an output of a summing point or a sum of which
the two inputs are, respectively, 1
s X(s) and 1
s2 X(s), so equation (7.1-7) can be written as
X(s) = F(s) −3
s X(s) −2
s2 X(s) .
Authenticated
32 PM

244
|
7 Simulation and stability analysis of continuous-time systems
)
(s
F
)
(s
Y
)
(s
X
−
−
2
3
+
+
+
1s
1s
Fig. 7.5: E7.1-2.
This means that X(s) can be seen as the output of a summing point of which the three
inputs are, respectively, F(s), −3
s X(s), and −2
s2 X(s); the simulation block diagram of
the system is shown in 󳶳Figure 7.5.
The expressing form of a system model can be changed, which means a math expres-
sion may have different transformation forms, so, the form of system simulation with
a block diagram should be nonunique. Similarly to the transformation of a mathe-
matical expression, we show some common transformation forms of block diagrams
in Table 7.1, which can improve the understanding of the concept of simulation and
simplify the structures of block diagrams.
Tab. 7.1: Common transformations of block diagrams.
Original
Equivalent
Remark
1. Merge two cascaded boxes
F ⋅G1 ⋅G2 = F(G1 ⋅G2)
2. Move adder to the back of the box
(F ± X)G = FG ± XG
3. Move adder to the front of the box
F ⋅G ± X = (F ± 1
G ⋅X) G
4. Move bifurcation to the back of
the box
5. Move bifurcation to the front of
the box
6. Eliminate feedback loop
7. Special case of 6 (H = 1)
Authenticated
32 PM

7.1 System simulation
|
245
7.1.3 Simulating systems with ﬂow graphs
Although block diagram simulation of systems has been of great convenience in sys-
tem analysis, people are still eager to ﬁnd easier solutions for system simulation. This
is true of ﬂow graph simulation.
1. Concept of the signal ﬂow graph
The ﬂow graph is a short form of the signal ﬂow graph, and its essence is not different
from the block diagram. Thus, it is actually the simpliﬁed form of a block diagram and
is basically similar to a block diagram in the way it is expressed. However, it is char-
acterized by blocks being replaced by directed segments and adders being omitted.
󳶳Figure 7.6 shows the block diagram and ﬂow graph of a system with H(s) as the
transfer function. The basic method of ﬂow graphs is to draw a directed segment from
one node to another according to the ﬂow direction of signals, and mark the transfer
function next to the segment.
A ﬁgure consisting of directed segments with side-noted transfer functions (indicat-
ing transmission direction and the corresponding processing of a signal) and nodes (the
system variables, indicating the start and end points of a signal), is called a signal ﬂow
graph.
In 󳶳Figure 7.6, the branch with H(s) starts from the node representing an excita-
tion signal F(s) and ends the node representing a response signal Y(s). This shows
that the signal at the end point of this branch is equal to the product of the signal at
the starting point and the transfer function of the branch, namely, Y(s) = H(s)F(s).
Note that H(s) is also called the gain of a system.
A node can have many forward (input) and backward (output) branches, so there
is a rule for ﬂow graphs: the signal (variable) on any a node is only equal to the sum
of all signals from the forward branches. In 󳶳Figure 7.7a, for example, the node signal
variables such as x4, x5 and x6 can be given by the following equations:
x4 = H14x1 + H24x2 + H34x3 ,
x5 = H45x4 ,
x6 = H46x4 .
Note that the node plays the role of a adder and can add all signals brought by the
forward branches. If a signal needs to be subtracted, we canset a negative sign in front
of the transfer function of the branch with this signal; this it is shown in 󳶳Figure 7.7b,
)
(s
H
)
(s
F
)
(s
Y
System block diagram expression
)
(s
H
)
(s
F
)
(s
Y
System flow graph expression
(a)
(b)
Fig. 7.6: System block diagram and ﬂow graph expression.
Authenticated
32 PM

246
|
7 Simulation and stability analysis of continuous-time systems
1x
2x
3x
4x
5x
6x
14
H
24
H
34
H
45
H
46
H
1x
2x
3x
13
H
23
H
−
(a)
(b)
Fig. 7.7: Flow graph of the nodes and branchs.
and then
x3 = H13x1 −H23x2 .
It is obvious that the ﬂow graphs can simplify system representation. In a block di-
agram, the interconnection form of subsystems and transfer functions are important
information, and the ﬂow graphs only keep these necessary messages and abandons
other unnecessary things such as boxes and summation symbols, etc., so, it enables
us to focus on the essentials of a system.
2. Mason’s formula
The transfer function of a system can be obtained easily from ﬂow graphs by using
Mason’s formula. This makes it an important tool for system analysis. Moreover, it can
also be used as a mathematical model for system simulation.
First, we will introduce several important buzzwords in Mason’s formula
(1) source point (excitation point) – the node with only output branches;
(2) subordinate point (response point) – the node with at least one input branch;
(3) branch – the directed segment that points directly from one node to another;
(4) opened path – if a signal passes with directed branches from node A to B, and
on the way no one node is met again, the path passed by this signal is called an
opened path from A to B;
(5) loop – if a signal passes with directed branches from node A back to A, and on the
way, no one node is met again except A, this path passed by the signal is called a
loop;
(6) Opened path transfer function T – the product of the transfer functions of all
branches located on an opened path;
(7) loop transfer function L – the product of the transfer functions of all branches
located on a loop;
(8) ﬂow graph determinant ∆–
∆= 1 −∑
i
Li + ∑
i,j
LiLj −∑
i,j,k
LiLjLk + . . .
(7.1-8)
Authenticated
32 PM

7.1 System simulation
|
247
In equation (7.1-8), there are
∑
i
Li – the sum of all the loop transfer functions,
(7.1-9)
∑
i,j
LiLj – the sum of the products of transfer functions of two
loops that do not touch each other,
(7.1-10)
∑
i,j,k
LiLjLk – thesum of theproductsof transfer functionsof each
three loops that do not touch each other.
(7.1-11)
“No touching” means that there are neither common nodes nor branches among dif-
ferent loops.
With the above concept, we can give the form of Mason’s formula.
The system transfer function TFY from excitation point F to response point Y can
be expressed in the form
TFY = ∑M
N=1 TN∆N
∆
,
or
H(s) = ∑M
N=1 TN∆N
∆
,
(7.1-12)
where M represents the number of opened paths from F to Y. The TN represents the
opened path transfer function of the Nth opened path from F to Y. The ∆N represents
the ﬂow graph determinant of the rest ﬂow graphs after the Nth open path is removed.
Mason’s formula is another expression means of a system’s mathematical model
and is also a shortcut to achieve ﬂow graphs. Next, some examples are given to illus-
trate its application.
Example 7.1-3. Find the transfer function of the ﬂow diagram shown in 󳶳Figure 7.8.
Solution. The graph only has one loop, whose transfer function is−H2H3H4. There-
fore, the ﬂow graph determinant ∆is
∆= 1 −∑
i
Li = 1 + H2H3H4 .
There is only one opened path from the input to the output, whose transfer function
is H1H2H3. Once rid of this open path, the loop is also interrupted. Therefore,
∆1 = 1 .
The transfer function of the ﬂow graph is
H(s) = T1∆1
∆
=
H1H2H3
1 + H2H3H4
.
1
H
3
H
2
H
1x
2x
)
(s
F
)
(s
Y
4
H
−
1
Fig. 7.8: E7.1-3.
Authenticated
32 PM

248
|
7 Simulation and stability analysis of continuous-time systems
1
H
3
H
2
H
1x
2x
)
(s
F
)
(s
Y
3x
4x
1
2
G
−
3
G
−
4
G
−
1
G
−
4
H
1
Fig. 7.9: E7.1-4.
Example 7.1-4. Find the transfer function of the ﬂow graph shown in 󳶳Figure 7.9.
Solution. The graph has four loops
(a) x1 −x2 −x1,
(b) x3 −x4 −x3,
(c) x4 −Y −x4,
(d) x1 −x2 −x3 −x4 −Y −x1 .
The transfer function of each loop is
(a) −H2G2,
(b) −H3G3,
(c) −H4G4,
(d) −H2H3H4G1 .
So,
∑
i
Li = −(H2G2 + H3G3 + H4G4 + H2H3H4G1) .
There are two pairs of nontouching loops, such as x1−x2−x1 and x3−x4−x3, x1−x2−x1
and x4 −Y −x4, so,
∑
ij
LiLj = H2G2H3G3 + H2G2H4G4 .
There are no three or more loops that do not touch each other, so,
∆= 1 −∑
i
Li + ∑
ij
LiLj
= 1 + (H2G2 + H3G3 + H4G4 + H2H3H4G1 + H2G2H3G3 + H2G2H4G4) .
There is only an opened path from the input to the output, and the transfer function
is H1H2H3H4, that is,
T1 = H1H2H3H4 .
All loops are in contact with the opened path, and they will break off if the opened
path is taken out. So, we have
∆1 = 1 −0 + 0 −⋅⋅⋅= 1 .
According to Mason’s formula, the system transfer function is
H(s) = T1∆1
∆
=
H1H2H3H4
1 + H2G2 + H3G3 + H4G4 + H2H3H4G1 + H2G2H3G3 + H2G2H4G4
.
Authenticated
32 PM

7.1 System simulation
|
249
3. Flow graph simulation
It has been seen that the system function can be derived from ﬂow graphs by using
Mason’s formula, so we will introduce how to build ﬂow graphs based on the system
function H(s), which is called the ﬂow graph simulation.
Flow graph simulation methods can be divided into three types: the direct, par-
allel and series simulations. Obviously, considering the relationship between ﬂow
graphs and block diagrams, block diagram simulation methods also have these three
forms.
1.) Direct simulation. Flow graphs can be directly built based on the general form of
the system function H(s) in this method.
Example 7.1-5. If the system function of a system is H(s) =
1
s3+a2s2+a1s+a0, simulate
this system with ﬂow graphs.
Solution. The system function is rewritten as H(s) =
1
s3
1−−a2
s −−a1
s2 −−a0
s3 , and a comparison
with Mason’s formula, leads to the following conclusions.
From the numerator, there is only one opened path, and the ﬂow graph determi-
nant without the opened path is 1; this opened path is composed of three integrators.
From the denominator, only three loops touch each other. Their starting points or
ends are, respectively, the output terminals or input terminals of three integrators.
Accordingly, the direct form 1 is depicted in 󳶳Figure 7.10a, and the corresponding
block diagram in 󳶳Figure 7.10b. If 󳶳Figure 7.10a is transposed, that is, the signal
transmission directions of all branches are reversed, and the source point and the re-
sponse point are exchanged, then the direct form 2 can be shown as in 󳶳Figure 7.11a;
󳶳Figure 7.11b is the corresponding block diagram of the system.
1
1
0a
−
1a
−
2
a
−
Direct form 1 of flow-graph
)
(s
F
)
(s
Y
)
(s
F
)
(s
Y
−
−
Direct form 1 of block-diagram
−
1a
2a
0a
+
1s
1s
1s
1s
1s
1s
(a)
(b)
Fig. 7.10: Direct form 1 of simulation diagrams of E7.1-5.
1
1
0a
−
1a
−
2
a
−
Direct form 2 of flow-graph
)
(s
F
)
(s
Y
Direct form 2 of  block-diagram
)
(s
F
−
0a
1a
2
a
−
)
(s
Y
+
+
−
+
1s
1s
1s
1s
1s
1s
(a)
(b)
Fig. 7.11: Direct form 2 of simulation diagrams of E7.1-5.
Authenticated
32 PM

250
|
7 Simulation and stability analysis of continuous-time systems
Clearly, the system function will remain the same after the ﬂow graph is trans-
posed, so the direct simulation can be divided into two forms. Let us see another ex-
ample in the following.
Example 7.1-6. The system function of a system is H(s) =
b2s2+b1s+b0
s3+a2s2+a1s+a0 . Simulate this
system with a ﬂow graph.
Solution. The main difference between this example and the above lies in their nu-
merators. This system function can be rewritten as
H(s) =
b2
s + b1
s2 + b0
s3
1 −−a2
s
−−a1
s2 −−a0
s3
.
Comparing it with Mason’s formula, we obtain the following points.
(1) From the numerator, there are three opened paths, and the ﬂow graph deter-
minants after each opened path is removed are all 1. These three opened paths
which, respectively, point to the subordinate point from the output terminals of
the three integrators or from the excitation point to their input terminals.
(2) From the denominator, only three loops touch each other. The starting points or
ends of the three loops are, respectively, the output terminals or input terminals
of the three integrators. Accordingly, the ﬂow graph and block diagram in direct
forms can be depicted as in 󳶳Figure 7.12 and 󳶳Figure 7.13.
We ﬁnd some simulating rules from the above examples and give the general forms of
the direct simulation after they have been collated and summed up.
Direct form 1 of flow-graph
1
1s
0b
0a
−
1a
−
2
a
−
1b
2b
)
(s
F
)
(s
Y
1
Direct form 1 of block-diagram
2b
)
(s
F
1a
2
a
−
)
(s
Y
0a
−
1b
0b
−
+
+
+
+
1s
1s
1s
1s
1s
(a)
(b)
Fig. 7.12: Direct form 1 of simulation diagrams of E7.1-6.
Authenticated
32 PM

7.1 System simulation
|
251
Direct form 2 of flow-graph
Direct form 2 of block-diagram
1
0a
−
1a
−
2
a
−
0b
1b
2b
)
(s
F
0a
1a
2
a
−
−
)
(s
Y
+
0b
2b
1b
+
)
(s
F
)
(s
Y
+
+
1
1s
1s
1s
1s
1s
1s
(a)
(b)
Fig. 7.13: Direct form 2 of simulation diagrams of E7.1-6.
Direct form 1 of flow-graph
Direct form 2 of flow-graph
1
1s
0b
0a
−
1a
−
2
a
−
1b
2b
m
b
m
a
−
1
−
−
n
a
2
−
−
n
a
1
0b
0a
−
1a
−
2
a
−
)
(s
F
)
(s
Y
1b
2b
m
b
m
a
−
1
−
−
n
a
2
−
−
n
a
)
(s
F
)
(s
Y
1
1
1s
1s
1s
1s
1s
1s
1s
(a)
(b)
Fig. 7.14: General forms of ﬂow graph.
Authenticated
32 PM

252
|
7 Simulation and stability analysis of continuous-time systems
For the systems with the following system function
H(s) = bmsm + bm−1sm−1 + ⋅⋅⋅+ b1s + b0
sn + an−1sn−1 + ⋅⋅⋅+ a1s + a0
= bms−(n−m) + bm−1s−(n−m+1) + ⋅⋅⋅+ b1s−(n−1) + b0s−n
1 + an−1s−1 + ⋅⋅⋅+ a1s−(n−1) + a0s−n
;
(7.1-13)
the denominator is a ﬂow graph determinant consisting of n loops, and each loop
touches the others. The numerator can be regarded as the sum of transfer functions
of (m + 1) opened paths, and each path touches the others. Therefore, the two direct
forms of ﬂow graph simulation can be plotted as in 󳶳Figure 7.14.
2.) Series simulation. The general form of a system function H(s) is always a fraction,
but it can be decomposed into the product of several subfractions. Each subfraction
can be directly simulated into a subﬂow graph, and ﬁnally, these subﬂow graphs can
be connected in series and form a series form of simulation. Note that in some books,
the series form is also called the cascade form.
For example, the system function of a system is H(s) =
5s+5
s3+7s2+10s ; now the system
function will be transformed as follows:
H(s) =
5s + 5
s3 + 7s2 + 10s =
5(s + 1)
s(s + 2)(s + 5)
=
5
s + 2 ⋅s + 1
s + 5 ⋅1
s .
It can be seen that H(s) can become the product of three subsystem functions, which
can be simulated directly, as shown in 󳶳Figure 7.15a–c. Then, they can constitute the
series form of the simulation diagram of the whole system shown in 󳶳Figure 7.15d and
e.
1
2
−
5
1
1
1
5
−
2
5
)
(
+
= s
s
H
5
1
)
(
+
+
= s
s
s
H
1
1
5
−
2
−
)
(s
F
)
(s
Y
5
1
s
s
H
1
)
(
=
s
1
s
1
s
1
)
(s
F
)
(s
Y
5
2
5
+
−
+
−
+
+
1s
1s
1s
1s
1s
1s
(a)
(b)
(c)
(d)
(e)
Fig. 7.15: System series simulation schematic diagram.
Authenticated
32 PM

7.1 System simulation
|
253
1s
12
1
s
s
H
1
2
1 ⋅
=
)
(
56
2
1
6
5
+
⋅
=
s
s
H
)
(
5
−
43
−
5
1
3
4
+
⋅
−
=
s
s
H
)
(
1
2
−
1
1
)
(s
F
1
1
2
−
56
5
−
43
−
12
)
(s
Y
2
3
4
−
)
(s
Y
2
1
6
5
5
1
1
1
1
+
−
+
+
+
+
−
)
(s
F
1
1s
1s
1s
1s
1s
s
1
s
1
s
1
(a)
(b)
(c)
(d)
(e)
Fig. 7.16: System parallel simulation schematic diagram.
3.) Parallel simulation. H(s) is also transformed into the sum of several fractions, then
each of them is directly simulated into a subdiagram. Finally, these subdiagrams are
connected in parallel to form the parallel form of simulation.
If H(s) is decomposed as follows:
H(s) =
5s + 5
s3 + 7s2 + 10s =
5(s + 1)
s(s + 2)(s + 5) = 1
2 ⋅1
s + 5
6 ⋅
1
s + 2 −4
3 ⋅
1
s + 5
It can be seen that H(s) has become a sum of three subsystem functions. After each
subsystem is simulated in direct forms, the parallel form of the diagram of the whole
system can be constituted as shown in 󳶳Figure 7.16.
Example 7.1-7. The system function of a system is H(s) =
2s+3
s(s+3)(s+2)2. Simulate this
system with a block diagram and a ﬂow graph in direct, series and parallel forms.
Solution. (1) H(s) is rewritten as H(s) =
2s+3
s4+7s3+16s2+12s; its direct form diagrams are
shown in 󳶳Figure 7.17.
(2) H(s) is rewritten as H(s) = 1
s ⋅
1
s+2 ⋅2s+3
s+2 ⋅
1
s+3; its series form diagrams are shown
in 󳶳Figure 7.18.
(3) H(s) is rewritten as H(s) = 1
4 ⋅1
s −5
4 ⋅
1
s+2 + 1
2 ⋅
1
(s+2)2 +
1
s+3; its parallel form diagrams
are shown in 󳶳Figure 7.19.
Authenticated
32 PM

254
|
7 Simulation and stability analysis of continuous-time systems
)
(s
F
−
)
(s
Y
1
3
12
−
16
−
7
−
2
−
−
+
+
)
(s
F
)
(s
Y
1
+
7
16
12
2
3
1s
1s
1s
1s
s
1
s
1
s
1
s
1
(a)
(b)
Fig. 7.17: Direct simulation diagram of E7.1-7.
3
2
−
2
−
)
(s
F
)
(s
Y
1
2
s
1
s
1
)
(s
F
3
−
s
1
2
2
+
3
s
1
)
(s
Y
3
2
+
+
+
−
−
−
1
1
s
1
s
1
s
1
s
1
(a)
(b)
Fig. 7.18: Series simulation diagram of E7.1-7.
)
(s
F
1
s−
3
−
1
2
−
)
(s
Y
2
−
54
−
14
12
1
s−
1
s−
1
s−
)
(s
F
)
(s
Y
+
+
+
+
+
+
1
1
1
1
1
s−
1
s−
1/ 4
5 / 4
1
s−
1
s−
3
1/ 2
2
2
(a)
(b)
Fig. 7.19: Parallel simulation diagram of E7.1-7.
Note: From the above examples, we can see that although Mason’s formula is derived
from the ﬂow graph, it is also suitable in system simulation with block diagrams. Ob-
viously, the block diagram can be replaced completely by the ﬂow graph in view of its
functionality.
Authenticated
32 PM

7.2 System stability analysis
|
255
7.2 System stability analysis
System stability as an important conception in the system analysis process is dis-
cussed next.
7.2.1 System stability
We ﬁrst describe stability with the simple physical system in 󳶳Figure 7.20, which
shows the movement of a small ball located at different positions when it is forced.
When the ball is on point A, it will leave the original position and roll to point B or
C if it is forced to the left or the right. This means that A is an instable point and the
small ball located on A is in an unstable state. Suppose that we put the ball on B or C
and apply a leftward or rightward force to move it; the ball will be back in the original
position after the force is canceled. This states that B or C is the stable point and the
ball on it is in a stable state. A cone on the desktop can be used as a similar example,
as shown in 󳶳Figure 7.21. If the cone can go back to the original state after the touch
force is canceled, the cone is in a stable state. Conversely, if it has been inverted,
even a slight disturbance can change its original state, so the upside down cone is in
unstable state.
If a linear system is disturbed, no matter how tiny the disturbance is, there will be
a response that grows with time (even after the disturbance is removed), so this system
is unstable. If the response is limited, the system is boundary stable. If the response
eventually becomes zero, the system is stable.
A
B
C
Force
Fig. 7.20: Ball stability schematic diagram.
Force
Force
Fig. 7.21: Cone stable and unstable state diagram.
Authenticated
32 PM

256
|
7 Simulation and stability analysis of continuous-time systems
Tab. 7.2: The zero-input response stability criterion.
Zero-input response
Stability
Increases with time
Instability
Maintained within a certain limit
Boundary stability or critical stability
Eventually becomes zero
Asymptotic stability
As we know, the whole response of system can be divided into zero-input response
and zero-state response, so the system stability problem can be approached from two
aspects, that is, zero-input response stability and zero-state response stability.
1. Zero-input response stability
The zero-input response is stable, which is also known as asymptotic stability or in-
ternal stability, which means that the response of system caused by the initial energy
storage is gradually reduced to zero with time. That is,
lim
t→∞yx(t) = 0 .
(7.2-1)
Under any a set of initial conditions, if we have
lim
t→∞|yx(t)| ≤M ,
(7.2-2)
where M is a bounded real constant; and then the system is critical or boundary stable.
If under some certain initial conditions,
lim
t→∞|yx(t)| →∞,
(7.2-3)
the system is unstable.
The stability criterion of the zero-input response of a system is listed in Table 7.2.
2. Zero-state response stability
That zero-state response is stable or externally stable refers to that if a system with-
out initial energy storage is driven by a bounded signal, then the output (zero-state
response) is also bounded. This phenomenon is known as BIBO (bounded-input
bounded-output), and the mathematical description is of the form
|f(t)| < M →|yf(t)| < ∞.
(7.2-4)
The BIBO stability of a system can be deﬁned as:
If the response of a system to a bounded excitation is also bounded, the system is a
BIBO system, otherwise, is an unstable one.
Authenticated
32 PM

7.2 System stability analysis
|
257
It can be proved that the necessary and sufficient condition testing the stability of
a CT system is that the h(t) is absolutely integrable,
∞
∫
−∞
|h(t)|dt < ∞.
(7.2-5)
The necessary condition is
lim
t→∞h(t) = 0 .
(7.2-6)
Usually if there are no common factors in the numerator and denominator of a system
function H(s), the characteristics of an LTI system are completely described by the
system function. The asymptotic stability of system is equivalent to BIBO stability. If
there are common factors, the zeros and poles can be canceled, and the system may be
not asymptotically stable but rather BIBO stable. For example, if the system function
of a system is H(s) =
s−1
s2+s−2 =
s−1
(s−1)(s+2), clearly, there is a pole s = 1 on the right
half-plane, so this system is not asymptotically stable (see Section 7.2.2 for details.).
However, if the impulse response is solved by
H(s) =
s −1
s2 + s −2 =
s −1
(s −1)(s + 2) =
1
s + 2 →h(t) = e−2tε(t) →
∞
∫
−∞
|h(t)| dt = 1
2 < ∞,
then, a contradictory conclusion that this system is BIBO stable is results. This phe-
nomenon indicates that the cancellation of zeros and poles covers the essence of sys-
tem inherent instability, so the system function cannot describe fully and correctly the
system characteristics here. Of course, if a system is inherently stable, it still remains
stable after the zeros and poles of H(s) offset each other.
Analysis has shown that:
(1) The asymptotically stable system must be a BIBO stable system.
(2) The asymptotically unstable system must be a BIBO unstable system.
(3) A BIBO stable system is not always an asymptotically stable system.
(4) If the order of the denominator is no less than the order of the numerator, and
there is no cancellation of the zeros and the poles in H(s), the asymptotical sta-
bility is equivalent to BIBO stability.
(5) A real system (except signal generators) must be asymptotically stable and BIBO
stable at the same time.
For convenience, stability in the following discussion will be zero-state response sta-
bility unless stated otherwise.
7.2.2 Pole-zero analysis of the system function H(s)
We know that the stability of a system depends on the characteristics of h(t), but be-
cause h(t) is a function of time, so that ﬁnding and analyzing h(t) directly are usually
Authenticated
32 PM

258
|
7 Simulation and stability analysis of continuous-time systems
more troublesome. Therefore, we naturally wonder whether we can ﬁnd any analyses
methods in the s domain. The answer is “yes”. Since h(t) and H(s) are the Laplace
transform pair, some characteristics of h(t) can be obtained by H(s).
If A(s) and B(s), respectively, represent the denominator and the numerator of
H(s), and B(s) = 0 has m roots such as ξ1, ξ2, . . . , ξm, and A(s) = 0 has n roots
λ1, λ2, . . . , λn, then the general form of the system function can be of the form
H(s) = bmsm + bm−1sm−1 + ⋅⋅⋅+ b1s + b0
sn + an−1sn−1 + ⋅⋅⋅+ a1s + a0
= B(s)
A(s) =
bm
m
∏
j=1
(s −ξj)
n
∏
i=1
(s −λi)
.
(7.2-7)
We name ξ1, ξ2, . . . , ξm and λ1, λ2, . . . , λn, respectively, zeros and poles of H(s). Ac-
cording to mathematical knowledge, there are three forms of the zero and the pole,
such as real, imaginary and complex numbers. The coefficients ai and bj in H(s) are
generally real numbers. Therefore, if a pole or a zero is an imaginary number or a com-
plex number, it must appear in a pair with conjugate form.
From the solution method of the inverse Laplace transform, when H(s) is ex-
panded into partial fraction form, each of its poles will determine a corresponding
function of time. Therefore, the expression of the impulse response h(t) only depends
on the poles of H(s), while the amplitude and phase of h(t) are determined by poles
and zeros together. In other words, h(t) is completely determined by the distribution
of zeros and poles in the s plane.
The locations of poles in the s plane can be divided into three types, such as in
the open left half-plane, on the imaginary axis and in the open right half-plane. The
distribution characteristics of ﬁrst-order and higher-order poles will be introduced in
the following.
1. First-order poles
If the poles of H(s) λ1, λ2, . . . , λn are all of ﬁrst order, H(s) will be expanded into par-
tial fraction form as
H(s) =
n
∑
i=1
Ki
s −λi
.
(7.2-8)
If λi is a real number, the expansion of H(s) will contain the term Hi(s) =
b
s−α. The pole
λi = α is on the real axis. From three different cases of α < 0, α = 0 and α > 0, the pole
may be on the negative real axis, the origin or the positive real axis, which are shown
in 󳶳Figure 7.22a (where “×” represents a pole). The corresponding impulse response
is of the form
hi(t) = beαtε(t) ,
(7.2-9)
and its waveforms are as displayed in 󳶳Figure 7.22b.
From equation (7.2-9), when t →∞, if α < 0, then hi(t) →0; if α = 0, hi(t) is a
ﬁnite value; if α > 0, hi(t) →∞.
Authenticated
32 PM

7.2 System stability analysis
|
259
jω
0
<
α
0
0
=
α
0
>
α
σ
)
(t
hi
0
<
α
0
0
=
α
0
>
α
t
b
Firstorder real poles distribution
Corresponding impulse responses
(a)
(b)
Fig. 7.22: First-order real pole distribution and corresponding impulse responses.
0
0
α =
0
α>
0
α<
jβ
−
jβ
σ
jω
-b
Fig. 7.23: First-order conjugate poles distribution.
If a poleiscomplex, itmustbea pair of conjugatepoles λ1,2 = α±jβ. Theexpansion
of H(s) contains the term Hi(s) =
s+b
(s−α)2+β2 , letting b > 0. It has a ﬁrst-order zero at
point ξ = −b. According to three situations such as α < 0, α = 0 and α > 0, the pole
can be in the open left half-plane, on the imaginary axis or in the open right half-plane,
respectively, as shown in 󳶳Figure 7.23, where “∘” denotes the zero. The corresponding
impulse response is
hi(t) =
√(b + α)2 + β2
β
eαt sin(βt + φ)ε(t) ,
(7.2-10)
where
φ = arctan
β
b + α .
(7.2-11)
If α < 0, the pole is in the open left half-plane, and hi(t) is in the damped oscillation
curve. Thus, when t →∞, hi(t) = 0; if α = 0, the pole is on the imaginary axis, and
hi(t) will be a oscillating wave with a constant amplitude; if α > 0, the pole is in the
open right half-plane, and hi(t) will be in the form of increased oscillation, so when
t →∞, the amplitude of hi(t) →∞. From equations (7.2-10) and (7.2-11), both the
amplitude and the phase of hi(t) are related to the zero positions.
2. Second-order poles
If H(s) has a second-order pole on the real axis as λi = α, the Hi(s) can be written as
Hi(s) =
s + b
(s −α)2 .
(7.2-12)
Since there is a ﬁrst-order zero at ξ = −b, the impulse response is of the from
hi(t) = [(b + α)t + 1]eαtε(t) .
(7.2-13)
If α < 0, the pole is on the negative real axis, so when t →∞, hi(t) →0; if α = 0
or α > 0, the pole is at the origin or on the positive real axis, which can result in an
increasing curve of hi(t) with t, so when t →∞, hi(t) →∞.
Authenticated
32 PM

260
|
7 Simulation and stability analysis of continuous-time systems
If H(s) has second-order conjugate poles λ1,2 = α ± jβ, the denominator of Hi(s)
should be [(s −α)2 + β2]2, so its inverse transform is
[c1teαt cos(βt + θ) + c2eαt cos(βt + φ)]ε(t) ,
(7.2-14)
where c1, c2, θ and φ are constants which relate to the locations of zeros and poles.
If α < 0, the poles are on the open left half-plane, so when t →∞, hi(t) →0. If α = 0
or α > 0, the poles are on the imaginary axis or the open right half-plane, which can
result in an increasing amplitude of hi(t) with t, so when t →∞, the amplitude of
hi(t) →∞.
If there are higher-order poles in Hi(s), the change regulations of corresponding
hi(t)withtime t aresimilar to thecases of second-order poles and willnotbeexplained
here.
Based on the above contents, we can sum up the following points.
(1) The function form for each component hi(t) in h(t) only depends on the corre-
sponding locations of poles of H(s), but its amplitude and phase are codetermined
by the positions of the poles and zeros.
(2) Poles of H(s) in the open left half-plane correspond to the transient components
hi(t) in h(t). When t →∞, the transient component hi(t) →0. A ﬁrst-order
pole on the negative real axis corresponds to the exponential decay curve hi(t) =
e−αtε(t) (α > 0). A pair of conjugate poles λ1,2 = α ± jβ corresponds to a damped
oscillation signal like hi(t) = e−αt sin(βt + φ)ε(t).
(3) A ﬁrst-order pole of H(s) at the origin point corresponds to a step signal like hi(t) =
ε(t). A pair of conjugate poles λ1,2 = ±jβ on the imaginary axis corresponds to a
constant amplitude oscillation signal as hi(t) = sin(βt + φ)ε(t).
(4) For the second-order pole of H(s) at the origin point or on the imaginary axis, or
ﬁrst- and second-order poles in open right half-plane, the corresponding hi(t) all
grow with time.
To make things easier, Table 7.3 shows the typical pole-zero distributions of H(s) and
the corresponding waveforms of h(t).
It can be deduced from Table 7.3 that if all the poles of H(s) are located on the open
left half-plane, the system must be stable; however, it must be unstable if H(s) has one
or more poles on the open right half-plane or the imaginary axis. In addition, if only
the ﬁrst-order pole is on the imaginary axis, the system is often called a boundary
stable or oscillation system.
Example 7.2-1. System functions are as follows:. Draw the pole-zero diagrams and
waveforms of impulse responses.
(1) H(s) =
s+1
(s+1)2+4
(2) H(s) =
s
(s+1)2+4
(3) H(s) =
(s+1)2
(s+1)2+4
Authenticated
32 PM

7.2 System stability analysis
|
261
Tab. 7.3: Pole-zero distributions of H(s) and the corresponding waveforms of h(t).
No.
H(s)
Distribution of poles and
zeros in the s plane
h(t)
Waveform of h(t)
1
1
s
jω
0
σ
ε(t)
( )
h t
0
t
2
1
s−α
(α > 0)
jω
0
σ
α
eαtε(t)
( )
h t
0
t
3
1
s+α
(α > 0)
jω
0
σ
α
−
e−αtε(t)
( )
h t
0
t
4
ω0
s2+ω2
0
jω
0
σ
0
jω
0
jω
−
sin ω0tε(t)
( )
h t
0
t
−
5
s
s2+ω2
0
jω
0
σ
0
jω
0
jω
−
cos ω0tε(t)
( )
h t
0
t
−
6
ω0
(s−α)2+ω2
0
α > 0
jω
0
σ
0
jω
0
jω
−
α
eαt sin ω0tε(t)
( )
h t
0
t
7
ω0
(s+α)2+ω2
0
α > 0
jω
0
σ
0
jω
0
jω
−
α
−
e−αt sin ω0tε(t)
( )
h t
0
t
8
1
(s+α)2
(α > 0)
jω
0
σ
(2) 
α
−
te−αtε(t)
( )
h t
0
t
Authenticated
32 PM

262
|
7 Simulation and stability analysis of continuous-time systems
Tab. 7.3 (continued): Pole-zero distributions of H(s) and the corresponding waveforms of h(t).
No.
H(s)
Distribution of poles and
zeros in the s plane
h(t)
Waveform of h(t)
9
1
s2
jω
0
σ
(2) 
tε(t)
( )
h t
0
t
10
2ω0s
(s2+ω2
0)2
jω
0
σ
0
jω
0
jω
−
(2) 
(2) 
t sin ω0tε(t)
( )
h t
0
t
jω
σ
j2
j2
−
1
−
0
)
(t
h
t
1
−
1
t
e−
0
0.25π
jω
σ
j2
j2
−
1
−
0
)
(t
h
t
1.25
1.25
t
e−
0
0.1π
jω
σ
j2
j2
−
1
−
0
)
(t
h
t
0
2
t
e−
2
0.5π
2
−
)
2
(
)1(
1.25
−
(a)
(b)
(c)
Fig. 7.24: E7.2-1.
Solution. The pole distribution situations of the three system functions are the same,
but the zero distribution situations are different. Therefore, impulse response wave-
forms are also different. The detailed process is as follows.
(1) h(t) = L−1 [
s+1
(s+1)2+4] = e−t cos(2t)ε(t).
The pole-zero diagram of H(s) and impulse response are, respectively, plotted in
󳶳Figure 7.24a.
Authenticated
32 PM

7.2 System stability analysis
|
263
(2)
h(t) = L−1 [
s
(s + 1)2 + 4] = L−1 [
s + 1
(s + 1)2 + 4 −
1
(s + 1)2 + 4]
= e−t[cos(2t) −1
2 sin(2t)]ε(t) = √5
2 e−t cos(2t + 26.57°)ε(t)
The pole-zero diagram of H(s) and the impulse response are, respectively, plotted
in 󳶳Figure 7.24b.
(3)
h(t) = L−1 [
(s + 1)2
(s + 1)2 + 4] = L−1 [1 −
4
(s + 1)2 + 4] = δ(t) −2e−t sin(2t)ε(t) .
The pole-zero diagram of H(s) and the impulse response waveform are, respec-
tively, shown in 󳶳Figure 7.24c.
From this example, we can see that the amplitude and phase of the impulse response
waveform change when the zero is moved from point −1 to the origin. However, when
the order of the zero at point −1 changes from ﬁrst to second order, not only do the
amplitude and phase of the impulse response change, but an impulse signal also ap-
pears in the impulse response. Generally, changing the zero position of H(s) can result
in changes of the amplitude and phase of h(t) and may also produce an impulse signal
in h(t).
Example 7.2-2. H(s) = U(s)
I(s) isthesystem functionof thecircuitshownin󳶳Figure7.25a.
Its pole-zero diagram is shown in 󳶳Figure 7.25b, and H(0) = 1. Solve for the values of
R, L and C.
Solution. From 󳶳Figure 7.25a and b the system function can be written as
H(s) = U(s)
I(s) =
1
sC +
1
Ls+R
=
Ls + R
LCs2 + sCR + 1
(7.2-15)
and
H(s) =
k(s + 2)
(s + 1 −j 1
2)(s + 1 + j 1
2)
.
Since H(0) = 1, and setting s = 0 for the above equation, we have
H(s)|s=0 =
2k
(1 −j 1
2)(1 + j 1
2)
= 8
5 k = 1 ,
Ls
)
(s
U
)
(s
I
+
−
sC
1
R
jω
σ
1
j 2
1
j 2
−
1
−
2
−
0
(a)
(b)
Fig. 7.25: E7.2-2.
Authenticated
32 PM

264
|
7 Simulation and stability analysis of continuous-time systems
and then
k = 5
8 .
Therefore, the system function is
H(s) = 5
8 ⋅
s + 2
(s2 + 2s + 5
4)
=
1
2s + 1
4
5s2 + 8
5s + 1
.
(7.2-16)
Comparing equation (7.2-15) with equation (7.2-16) results in
L = 1
2 H,
R = 1 Ω,
C = 8
5 F .
7.2.3 Relationships between stability and ROC, and poles
1. Relationship between stability and ROC
From Section 7.2.1, an LTI system being stable is the equivalence of h(t) being abso-
lutely integrable; moreover, the Fourier transform of h(t) also exists at this time. This
means that the Fourier transform of a signal is equal to the values taken by its Laplace
transform along with the jω axis. Thus, we reach the following conclusion:
If and only if the ROC of system function H(s) contains jω axis, is the LTI system
stable.
Note that this system may be not a causal system, that means h(t) could be a left-
sided, right-sided or double-sided signal.
2. Relationship between stability and pole positions
From Section 7.2.2, we also know the relationship between stability and pole positions.
If and only if all the poles of system function H(s) lie in the left half of the s plane,
is the LTI causal system stable.
In other words, the ROC of H(s) for a causal system is the right half-plane of a
certain convergence axis in the s plane.
7.2.4 Stability judgment based on the R–H criterion
It is commonly known that before the stability of a system is determined by means of
H(s), we must ﬁrst ﬁnd roots of the equation A(s) = 0. We can then make conclusions
about the stability of the system based on the positions of the roots. Obviously, it is
easy to solve A(s) = 0 when it is with relative lower power (order), but it is very difficult
to solve and determine the stability of a system if it has higher order. Can we ﬁnd other
solution methods for this problem? Fortunately, a relatively simple judging method
has been given by the Routh–Hurwitz criterion.
Authenticated
32 PM

7.2 System stability analysis
|
265
From previous analysis, it is not necessary to know the accurate positions of poles
to determine a system’s stability. In other words, it is not necessary to calculate the
speciﬁc roots of A(s) = 0; the only thing we should do is to test whether the real part
of each pole (root) is greater than, less than or equal to zero. If the system is stable, all
the poles are in the open left half-plane (the real parts of roots are all less than zero);
as long as there is a pole located in the open right half-plane (the real part is greater
than zero), the system will be unstable.
We can deﬁne a Hurwitz polynomial as:
A polynomial with negative real roots, which is with real coefficients and all roots in
the open left half-plane, is named the Hurwitz polynomial.
Obviously, if A(s) is a Horowitz polynomial, the system is stable.
The necessary condition for which A(s) = ansn + an−1sn−1 + ⋅⋅⋅+ a1s + a0 is the
Hurwitz polynomial means that all its coefficients must be nonzero (there is no term
missing) and they all have the same plus or minus sign. Thus, this is also a necessary
condition for a system to be stable.
Note: A system that meets this condition is not always stable. For example, the
system with A(s) = 3s3 + s2 + 2s + 8 is just an unstable system.
E. J. Routh and A. Hurwitz put forward their own additionalconditions in 1877 and
1895, respectively, and their achievements have since been collectively referred to as
the Routh–Hurwitz criterion by later generations, that is:
The necessary and sufficient condition that A(s) is a Hurwitz polynomial is that
signs of elements in the ﬁrst column in the Routh array are the same, otherwise, the
change times of the signs are the number of positive real roots of A(s) = 0.
Thus, the necessary and sufficient condition to determine the system stability in-
cludes:
(1) There is no missing term in A(s), or the coefficients of all terms are not zero;
(2) the plus or minus signs of all coefficients an, an−1, . . . , a1, a0 in the A(s) polyno-
mial are the same;
(3) the plus or minus signs of the elements in the ﬁrst column in the Routh–Hurwitz
array are the same.
Note that if A(s) is a second-order or ﬁrst-order polynomial, the necessary and suf-
ﬁcient condition of system stability can be simpliﬁed as that all the coefficients
a2, a1, a0 (or a1, a0) exist and have the same plus or minus signs.
The arrangement of the Routh array is introduced in the following. For an nth
power equation with real coefficients,
A(s) = ansn + an−1sn−1 + ⋅⋅⋅+ a1s + a0 = 0 ,
Authenticated
32 PM

266
|
7 Simulation and stability analysis of continuous-time systems
the Routh array is of the form
ﬁrst row:
an
an−2
an−4
. . .
second row:
an−1
an−3
an−5
. . .
third row:
bn−1
bn−3
bn−5
. . .
fourth row:
cn−1
cn−3
cn−5
. . .
where
bn−1 = −
1
an−1
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨
an
an−2
an−1
an−3
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨
bn−3 = −
1
an−1
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨
an
an−4
an−1
an−5
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨
bn−5 = −
1
an−1
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨
an
an−6
an−1
an−7
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨
...
cn−1 = −
1
bn−1
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨
an−1
an−3
bn−1
bn−3
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨
cn−3 = −
1
bn−1
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨
an−1
an−5
bn−1
bn−5
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨
cn−5 = −
1
bn−1
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨
an−1
an−7
bn−1
bn−7
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨
...
The array is arranged sequentially in this method until all zero elements appear in
one row, and it can be generally arranged to row n + 1. Usually, any new element in
the array can be obtained by two elements in the ﬁrst column of two rows above the
unknown element, and two elements in the column of the top right of the unknown
element can form a determinant. Then this determinant is divided by the ﬁrst element
in the row above close to the unknown element, and ﬁnally, a negative sign is added
to this determinant. Letting aj,k denote the element in row j and column k, we have
aj,k = −
1
aj−1,1
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨
aj−2,1
aj−2,k+1
aj−1,1
aj−1,k+1
󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨󵄨
Example 7.2-3. Determine the system stability of A(s) = s4 + 7s3 + 17s2 + 17s + 6.
Solution. The Routh array is arranged as follows:
s4
1
17
6
s3
7
17
s2
14.58
6
s1
14.12
s0
6
Authenticated
32 PM

7.2 System stability analysis
|
267
The ﬁrst column element signs are all positive, and the coefficients of A(s) are not
missing, so the system is stable.
Example 7.2-4. With A(s) = s5 −2s4 + 2s3 + 4s2 −11s −10 known, ﬁnd the number
of roots of the characteristic equation which have positive real part.
Solution. Obviously, such a system with A(s) is unstable because there are coeffi-
cients with different signs in A(s). Next, the number of positive real part roots in it
can be found by the Routh–Hurwitz criterion. The Routh array is arranged as follows:
s5
1
2
−11
s4
−2
4
−10
s3
4
−16
s2
−4
−10
s1
−26
s0
−10
It can be seen that element signs in the ﬁrst column change three times between pos-
itive and negative, and therefore,A(s) has three positive real port roots. From the so-
lution for A(s) = 0, we have A(s) = (s + 1)2(s −2)(s −1 + j2)(s −1 −j2) = 0. We
can see it has three roots with positive real parts, such as 2, 1 + j2 and 1 −j2, so the
Routh–Hurwitz criterion is veriﬁed.
Some special problems should appear while a Routh array is arranged, and the corre-
sponding solutions are concluded in the following.
First case: The ﬁrst element of a row is zero, but the remaining elements of the row
are not all zero. Now, the element in the ﬁrst column of the next row will be uncertain.
Considering A(s) = s5 + 2s4 + 2s3 + 4s2 + s + 1, its Routh array is of the form
s5
1
2
1
s4
2
4
1
s3
0
1
2
0
s2
−1
0
Because the 0 appears in the denominator of the element in row 4, the array cannot be
continued. Therearewaystodealwiththisproblem; oneof them istouseanarbitrarily
small number δ to replace the 0 in the ﬁrst column in row 3, and then continue to
arrange the array by normal steps. The term containing δ2 can be ignored unless the
corresponding coefficient is an uncertainty value. In fact, the total changing times of
signs in ﬁrst column elements are independent of the sign of δ.
Continuing to arrange the array, we have
s5
1
2
1
s4
2
4
1
s3
δ
1
2
0
s2
4 −1
δ
1
0
s1
1
2 −
δ2
4δ−1
0
0
Authenticated
32 PM

268
|
7 Simulation and stability analysis of continuous-time systems
Neglecting the δ2 term, we have
δ2
4δ−1 = 0. Because δ is far less than 1, 4 −1
δ ≈−1
δ.
Then the array becomes
s5
1
2
1
s4
2
4
1
s3
δ
1
2
0
s2
−1
δ
1
0
s1
1
2
0
0
s0
1
It can be seen that regardless of whether the sign of δ is positive or negative, the el-
ement signs in the ﬁrst column are changed twice, so A(s) has two zeros in the right
half-plane.
Another way to arrange is to reverse the order of the coefficients, namely from a0,
a0
a2
a4
. . .
a1
a3
a5
. . .
For this example, we have
s5
1
4
2
s4
1
2
1
s3
2
1
0
s2
3
2
1
0
s1
−1
3
0
0
s0
1
The changing times of the signs in the ﬁrst column are the same as previously.
Second case: The elements in a row are all zero. This occurs when the element
numbers of two adjacent rows are the same, and the corresponding elements are pro-
portional. Now, the Routh array need not be arranged sequentially, we can assert that
the equation A(s) = 0 has some roots on the imaginary axis or in the right half-plane,
the system is either boundary stable nor unstable.
Example 7.2-5. Find the range of the gain K to make the system shown in 󳶳Figure 7.26
stable.
Solution. The transfer function of the system is H(s) =
K
s3+6s2+8s+K, and the Routh
array is
1
8
6
K
48−K
6
0
K
0
(
2)(
4)
K
s s
s
+
+
)
(s
F
)
(s
Y
+
Fig. 7.26: E7.2-5.
Authenticated
32 PM

7.2 System stability analysis
|
269
t
0
( )
v t
Fig. 7.27: Unstable system output waveform.
It can be seen that only if 0 < K < 48, the plus or minus signs of the ﬁrst column
elements are all the same, and the system is stable.
An unstable electric system subjected to a disturbance, no matter how small the dis-
turbance, will produce a natural response that increases with time, and the response
will become inﬁnite in theory. If the response becomes very large, the system will be
ruined. However, the amplitude growth of the response can usually cause changes
of other parameters in the system, so that it will limit the continuous growth of the
response and impair the instability of the system. Speciﬁcally, changes in the param-
eters cause make the poles in the right half-plane to move left onto the jω axis, and
the natural response grows to a certain value and is maintained, which will result in
a persistent oscillation waveform, as shown in 󳶳Figure 7.27.
Obviously, an unstable system cannot be used to amplify or process signals, be-
cause the response of the system to a signal will be submerged by gradually increas-
ing natural response components. For example, there is a sound amplifying system
shown in 󳶳Figure 7.28; when we sing a song into a microphone, if the acoustic waves
from the loudspeaker have also entered the microphone and are loud enough (and
have satisﬁed the requirements for phases), the loudspeaker will produce howling.
This phenomenon is called self-excitation, and the sound system becomes unstable.
However, an unstable system is also useful. For example, it can be used as a sinusoidal
signal generator.
An unstable system must have a feedback branch in structure, which can lead the
output signal into the input port of the system. According to its effect, the feedback can
be classiﬁed as positive and negative feedback. The system whose feedback signal off-
sets the original input signal is considered negative feedback, otherwise, it is positive
feedback. Of course, only positive feedback can cause the system to be unstable. In
the above sound amplifying system, the acoustic waves from the loudspeaker are the
amplifier
loudspeaker
microphone
Fig. 7.28: The sound amplifying system schematic diagram.
Authenticated
32 PM

270
|
7 Simulation and stability analysis of continuous-time systems
output signal, but the microphone is the input port for the human voice signal. Once
the acoustic waves from the loudspeaker are fed back to the microphone and result in
the positive feedback effect, self-excitation is more likely to be produced.
7.3 Controllability and observability of a system
In the control technology ﬁeld, in addition to being stable, a system should also be
controllable. In other words, it is necessary to analyze the controllability of the system.
Controllability refers to whether a system can be effectively controlled to reach the
desired state by means of the input signal. Observability is another issue that should
be studied; that is, whether the relevant information about the state of the system can
be obtained in time through observation and measurement of the system’s output sig-
nal, so that the system can be controlled by adjusting the input signal. The objective of
the analysis of the two problems, which are the two basic concepts in modern control
theory, is to achieve optimal control.
The usual requirement of a control system from the human is that the output and
the input of the system are the same as much as possible.
As a result, people would like to adopt different input signals to test and analyze a
system to achieve the control indexes enacted by them. The unit step signal is chosen
as the most representative testing signal, because not only many properties of system
can be reﬂected by using it, but also the responses produced by other testing signals
canbe deduced by its differential or integral. The following example will illustrate this
point.
Example 7.3-1. A feedback system is given in 󳶳Figure 7.29. The gain of the ampliﬁer is
k, HO(s) =
1
s(s+4) and HB(s) = 1.
(1) Judge the stability of the system.
(2) If the input is a unit step signal ε(t), what is the appropriate value of k to make the
output a constant value within a short adjusting time and with good stability?
Solution. (1) The system function is
H(s) =
kHO(s)
1 + kHO(s)HB(s) =
kHO(s)
1 + kHO(s) =
k
s2 + 4s + k .
The poles are λ1,2 = −4±√16−4k
2
. No matter what k is a real number greater than
zero, these two poles will locate on the left half-plane, and the system is stable.
amplifier
o( )
H
s
B( )
H
s
( )
f t
( )
y t
B( )
f
t
( )
F s
( )
Y s
−
+
Fig. 7.29: E7.3-1 (1).
Authenticated
32 PM

7.3 Controllability and observability of a system
|
271
(2) The Laplace transform of the step response is
G(s) = F(s)H(s) = 1
s ⋅
k
s2 + 4s + k =
k
s(s2 + 4s + k) .
It can be seen that G(s) has a pole located at the original point, and that the other
two poles are located in the left half-plane, so increasing values of the step re-
sponse over time necessarily reach a stable value. Three cases based on different
values of k will be discussed.
(a) k > 4, where λ1,2 = −2 ± j√k −4 are a pair of the conjugate complex roots. If
k = 40, λ1,2 = −2 ± j6, and then
G(s) =
40
s(s −λ1)(s −λ2) = 1
s −√5
3√2
e−j18.4°
1
s −λ1
−√5
3√2
ej18.4°
1
s −λ2
.
Finding the inverse Laplace transform of G(s), we have
g(t) = ε(t) −√5
3√2
e−2t[ej6te−j18.4° + e−j6te−j18.4°]ε(t)
= ε(t) −√10
3
e−2t cos(6t + 18.4°)ε(t) .
(7.3-1)
(b) k = 4, where λ1,2 = −2 are the real repeated roots. Then
G(s) =
4
s(s −λ1)(s −λ2) =
4
s(s + 2)2 = 1
s +
−1
s + 2 +
−2
(s + 2)2 .
Finding the inverse Laplace transform of G(s), we have
g(t) = ε(t) −(1 + 2t)e−2tε(t) .
(7.3-2)
(c) k < 4. Suppose k = 2, so λ1,2 = −2 ± √2 are two real roots, then
G(s) =
2
s(s −λ1)(s −λ2) = 1
s +
−1.2
s + 2 −√2
+
0.2
s + 2 + √2
.
Finding the inverse Laplace transform of G(s), we have
g(t) = ε(t) −(1.2e−0.586t −0.2e−3.414t)ε(t) .
(7.3-3)
The g(t) in equations (7.3-1)–(7.3-3), are plotted in 󳶳Figure 7.30.
From the observation of the three waveforms, we can conclude that:
(1) The step response includes an oscillation stage when k > 4, but it can reach a
stable value after attenuation in an interim time ts. This circumstance is called
the underdamping state. The changing time of the response from the initial zero
value to the maximum value is called the peak time tp. The overshoot of the step
response is deﬁned by
σ = g(tp) −g(∞)
g(∞)
100% .
(7.3-4)
Usually, we require the interim and peak times to be short, and the overshoot
smaller or equal to zero, so that the difference between the response and the ex-
citation is minimized.
Authenticated
32 PM

272
|
7 Simulation and stability analysis of continuous-time systems
40
k =
4
k =
2
k =
( )
g t
t
0
1
pt
st
( )t
ε
Fig. 7.30: E7.3-1 (2).
(2) There is no oscillation stage in the step response when k = 4, so the step response
at that time can be called the critical damping state.
(3) There is no oscillation stage in the step response when k < 4, but compared with
k = 4, the transient time ts becomes longer, and the circumstance of the step
response can be called the overdamping state.
(4) Obviously, at the critical damping state, the track feature of the system or the con-
sistency between the input and the output is better. In practical applications, to
reduce the transient time ts, the system should often be allowed to be in the un-
derdamping state with a bit overshoot.
From the above discussions, transient time ts, peak time tp and overshoot σ can be
considered as three important indicators to describe the control properties of a system.
We can regulate k to satisfy different requirements and achieve the optimal control
result in practice.
When we analyze a control system using an external analysis method, generally,
we do not need to discuss controllable and observable problems; as long as the ze-
ros and poles of the system function do not counteract with each other, the system is
controllable and observable, otherwise, it is not controllable and observable.
The usage of knowledge from the signals and systems topic in Automation Control
Systems is brieﬂy introduced here, with the aim of helping readers better understand
the automation control principle and laying the foundation for further research or fur-
ther study.
7.4 Solved questions
Question 7-1. A causal feedback LTI system is shown in 󳶳Figure Q7-1 (1). The system
1 is represented by a differential equation y󸀠(t)−y(t) = ε(t), the system function of the
feedback path is F(s) = K/(s + 2), and K can be adjusted for any real number.
(1) Find the system function H1(s) of system 1, and work out the zeros, poles and ROC
of it, and judge the stability of system 1.
(2) Find an adjustable range of K in F(s) to make the feedback system stable.
Authenticated
32 PM

7.4 Solved questions
|
273
+
System 1
−
( )
y t
( )
x t
( )t
ε
( )
F s
Fig. Q7-1 (1)
Solution. (1) The system function and the ROC of system 1 are, respectively, H1(s) =
1
s−1 and ROC : σ > 1.
A ﬁrst-order pole p = 1, and the ROC does not include imaginary axis; they are
plotted in 󳶳Figure Q7-1 (2). According to the concept of stability, the system is not
stable.
(2) The system function of the feedback system in 󳶳Figure Q7-1 (1) is
H(s) =
H1(s)
1 + H1(s)F(s) =
1
s−1
1 +
K
(s−1)(s+2)
=
s + 2
s2 + s + (K −2) .
Obviously, A(s) is the second-order polynomial, if k −2 > 0 , the system is stable,
so, K > 2.
jω
1
0
s-plane
σ
ROC
Fig. Q7-1 (2)
Question 7-2. For the system shown as 󳶳Figure Q7-2 H(s) = U2(s)
U1(s) = 4 is known.
(1) Find the subsystem function H2(s).
(2) Make the subsystem H2(s) stable and ﬁnd the range of k.
1( )
U s +
1
2
1
s +
k
−
2( )
U
s
+
+
+
2( )
H
s
Fig. Q7-2
Solution. (1) According to 󳶳Figure Q7-2, we have
U1(s) + U2(s)H2(s) (1 −
k
2s + 1) = U2(s) .
Because H(s) = U2(s)
U1(s) = 4, we can conclude that H2(s) =
6s+3+k
4(2s+1−k).
(2) To make H2(s) stable, we obtain k < 1.
Question 7-3. The system equation y󸀠󸀠(t) + 3y󸀠(t) + 2y(t) = 2f 󸀠(t) + f(t). Work out the
ﬂow graph of the system in direct form.
Authenticated
32 PM

274
|
7 Simulation and stability analysis of continuous-time systems
Solution. Based on the zero-state conditions, with the Laplace transform for this dif-
ferential equation, we obtain
s2Yf(s) + 3sYf(s) = 2sF(s) + F(s) .
So, the system function is
H(s) =
2s + 1
s2 + 3s + 1 .
The ﬂow graph of the system in direct form is depicted in 󳶳Figure Q7-3.
2
1
1
−
3
−
1
1
s−
1
s−
( )
F s
( )
Y s
Fig. Q7-3
Question 7-4. The differential equation of an LTI causal continuous system
y󸀠󸀠(t) + 4y󸀠(t) + 3y(t) = 4f 󸀠(t) + 2f(t) .
(1) Find the impulse response h(t) of the system.
(2) Judge whether the system is stable.
Solution. (1) With the Laplace transform on both sides of the equation, we have
s2Yf(s) + 4sYf(s) + 3Yf(s) = 4sF(s) + 2F(s) .
Changing the form of the above equation, yields
H(s) = Yf(s)
F(s) =
4s + 2
s2 + 4s + 3 =
4s + 2
(s + 1)(s + 3) =
5
s + 3 −
1
s + 1 .
With the inverse Laplace transform, the impulse response is
h(t) = (5e−3t −e−t)ε(t) .
(2) From the denominator polynomial of H(s), the system function has two poles,
such as λ1 = −1, λ2 = −3, which are located on the left half-plane, so the system
is stable.
Question 7-5. The differential equation of a LTI causal continuous system y󸀠󸀠(t) +
5y󸀠(t) + 6y(t) = 2f 󸀠(t) + f(t). known f(t) = e−tε(t), y(0−) = 1, y󸀠(0−) = 1. Try to ﬁnd in
s domain:
(1) zero-input and zero-state responses yx(t) and yf(t), the total response y(t).
(2) the system function H(s) and impulse response h(t), and judge system stability.
Authenticated
32 PM

7.4 Solved questions
|
275
Solution. (1) With the Laplace transform on both sides of the differential equation,
s2Y(s) −sy(0−) −y󸀠(0−) + 5sY(s) −5y(0−) + 6Y(s) = (2s + 1)F(s) .
So,
Y(s) = sy(0−) + y󸀠(0−) + 5y(0−)
s2 + 5s + 6
+
2s + 1
s2 + 5s + 6 F(s) .
Two parts on the right side of the equation are representations of zero-input and
zero-stateresponsesinthe s domain; thetotalresponse canbeobtained from their
inverse Laplace transform.
Since y(0−) = 1, y󸀠(0−) = 1, and so the representation of the zero-input response
in the s domain is
Yx(s) = sy(0−) + y󸀠(0−) + 5y(0−)
s2 + 5s + 6
=
s + 6
s2 + 5s + 6 =
4
s + 2 +
−3
s + 3 ,
and so, the zero-input response is
yx(t) = 4e−2t −3e−3t,
t ≥0 .
The representation of zero-state response in the s domain is
Yf(s) =
2s + 1
s2 + 5s + 6 F(s) =
2s + 1
(s2 + 5s + 6)(s + 1) = −1/2
s + 1 +
3
s + 2 + −5/2
s + 3 ,
so the zero-state response
yf(t) = (−1
2 e−t + 3e−2t −5
2e−3t) ε(t) .
The total response
y(t) = yx(t) + yf(t) = −1
2 e−t + 7e−2t −11
2 e−3t,
t ≥0 .
(2) The system function
H(s) = Yf(s)
F(s) =
2s + 1
s2 + 5s + 6 =
−3
s + 2 +
5
s + 3 ,
and so, the impulse response
h(t) = (−3e−2t + 5e−3t) ε(t) .
From H(s), both of the two poles, -2 and -3, are located on the left half-plane, so
the system is stable.
Authenticated
32 PM

276
|
7 Simulation and stability analysis of continuous-time systems
7.5 Learning tips
The graphical system model has incomparable advantages over the analytic formula.
Furthermore, besides the relationship between the response and the excitation, sta-
bility is an important consideration in system analysis. Please pay attention to the
following points.
(1) Besides the mathematical model, a system can also be simulated more intuitively
and concisely by the block diagram and the ﬂow graph.
(2) The stability of a system can be seen from the distribution of the pole-zero of the
system function which is also a type of the mathematical model.
(3) The Rose–Hurwitz criterion.
7.6 Problems
Problem 7-1. Two systems are shown in 󳶳Figure P7-1, ﬁnd the system functions and
prove that the two block diagrams correspond to the same system.
(a)
(b)
-
( )
F s
( )
Y s
2
2
3
1
-
+
+
1
s−
1
s−
+
+
-
( )
F s
( )
Y s
2
1
-
+
+
1
s−
1
s−
+
+
Fig. P7-1
Problem 7-2. Make simulation diagrams in direct form for the following systems:
(1) H(p) =
1
p3 + 3p + 2
(2) H(p) =
p2 + 2p
p3 + 3p2 + 3p + 2
(3) H(s) =
2s + 3
(s + 2)2(s + 3)
(4) H(s) =
s2 + 4s + 5
(s + 1)(s + 2)(s + 3)
Problem 7-3. Find the system function H(s) = Y(s)
F(s) for each system shown in 󳶳Figure
P7-3.
Problem 7-4. For the following system functions, try to draw their simulation block
diagrams in direct, cascade and parallel forms.
(1) H(s) =
5s + 5
s(s + 2)(s + 5),
(2) H(s) = 5s2 + s + 1
s3 + s2 + s ,
(3) H(s) =
3s
s3 + 4s2 + 6s + 4.
Authenticated
32 PM

7.6 Problems
|
277
1
2
H
1
G
−
)
(s
F
)
(s
Y
5
H
1
H
3
H
2
G
−
3
G
−
4
H
)
(s
F
)
(s
Y
1
G
2
G
3
G
5
G
6
G
7
G
4
G
2
H
−
2
G
)
(s
F
)
(s
Y
1
1
H
−
3
G
3
H
−
1
G
4
G
5
G
−
1
1
1
1
−s
2
−
8
1
−s
3
−
2
27
−
1
−s
2
1
−
)
(s
Y
1
−
1
2
G
)
(s
F
)
(s
Y
4
G
1
H
2
H
3
G
3
H
4
H
5
H
6
H
)
(s
F
1
G
(a)
(b)
(c)
(d)
(e)
Fig. P7-3
Problem 7-5. Find H(s) of the system shown in 󳶳Figure P7-5 and its ﬂow graphs in
direct form.
)
(s
F
)
(s
Y
+
1
3
s+
1
1
s+
10
s
s+
+
Fig. P7-5
Problem 7-6. A ﬂow graph in parallel form is shown in 󳶳Figure P7-6. Find H(s) = Y(s)
F(s)
and draw its ﬂow graphs in cascade form and in direct form.
)
(s
F
1
3
1
−s
6
−
1
−s
1
−
1
−s
)
(s
Y
5
−
1
3
−
10
−
1
1
Fig. P7-6
Problem 7-7. A system ﬂow graph is shown in 󳶳Figure P7-7, and f(t) = e−2tε(t) is
known. Find the zero-state response yf(t) of the system.
Authenticated
32 PM

278
|
7 Simulation and stability analysis of continuous-time systems
1
1
−s
s
e−
2
s
3
)
(s
F
)
(s
Y
1
5
−s
Fig. P7-7
Problem 7-8. Find system functions of circuits in 󳶳Figure P7-8 and their pole-zero
plots.
)
(t
i
)
(t
u
+
−
F
3
2
F
2
Ω
2
3
Ω
6
1
Ω
1
)
(t
i
)
(t
u
+
−
H
1
H
1
F
1
F
3
1
)
(
1 t
u
+
−
F
1
Ω
2
)
(
2 t
u
+
Ω
2
F
1
−
(a)
(b)
(c)
Fig. P7-8
Problem 7-9. The poles of a system function are λ1 = 0, λ2 = −1, the zero is z1 = 1,
and the terminal value of the impulse response is h(∞) = −10. Find
(1) the system function H(s);
(2) the stable response ys(t) for the excitation f(t) = 3 sin(3t)ε(t).
Problem 7-10. A circuit is shown in 󳶳Figure P7-10a, the pole-zero diagram of the trans-
fer function H(s) = U2(s)
U1(s) is shown in 󳶳Figure P7-10b, and H(0) = 1. Find L, R and C.
)
(
1 t
u
+
−
R
C
L
+
−
)
(
2 t
u
jω
σ
0
j
j
−
1
−
(a)
(b)
Fig. P7-10
Problem 7-11. The pole-zero diagram is shown in 󳶳Figure P7-11, and h(0+) = 1. If
the excitation f(t) = cos tε(t), ﬁnd the zero-state response, the free response and the
forced response.
jω
σ
0
j4
j4
−
j2
j2
−
Fig. P7-11
Authenticated
32 PM

7.6 Problems
|
279
Problem 7-12. Judge the stability of the following systems and point out the number
of eigenvalues with a positive real part.
(1) The characteristic equation is s4 + 7s3 + 17s2 + 6 = 0.
(2) The characteristic equation is s6 + 7s5 + 16s4 + 14s3 + 25s2 + 7s + 12 = 0.
(3) The system function is H(s) =
4s3+2s2+3s+1
s5+2s4+2s3+4s2+11s+4.
(4) The system function is H(s) =
s+1
s6+5s5+11s4+25s3+36s2+30s+36.
(5) The characteristic equation is s4 + 2s3 + 7s2 + 10s + 10 = 0.
Problem 7-13. For the two feedback systems shown in 󳶳Figure P7-13, ﬁnd the value of
k that can make the systems stable.
(a)
(b)
)
(
1 s
V
+
)
(
2 s
V
4
4
2
+
+ s
s
s
+
k
)
(
1 s
R
+
−
3
1
+
s
k
2
3
1
2
+
+ s
s
)
(
2 s
R
Fig. P7-13
Problem 7-14. A feedback system is shown in 󳶳Figure P7-14; the impulse response of
the subsystem h1(t) = (2e−2t −e−t)ε(t).
(1) If the system is stable, what is the value of the real coefficient k?
(2) Under the condition of boundary stability, ﬁnd the impulse response h(t) of the
whole system.
)
(t
f
+
k
)
(t
y
+
)
(
1 t
h
Fig. P7-14
Problem 7-15. A system is shown in 󳶳Figure P7-15.
(1) What k can make the system is stable?
(2) Under the condition of boundary stability, ﬁnd the impulse response h(t).
1
−s
)
(
1 s
U
)
(
2 s
U
k
1
4
−
4
−
1
−s
1
1
Fig. P7-15
Authenticated
32 PM

Authenticated
32 PM

A Reference answers
Chapter 1
Problem 1-1:
1
2
- 2
0
t
0
1
1
t
-1- 1
0
-3
3
t
( 1 )
( 1 )
0
1
2
t
- 1
- 2
1
- 1
t
0
1
-1
2
-2
t
0
0.5
-0.5
t
0
1
2
-1
-2
1
-1
t
0
1
-1
2
-2
t
0
-3
t
0
1
2
3
4
1
2
3
4
1( )
f t
2 ( )
f
t
4 ( )
f
t
5 ( )
f
t
6 ( )
f
t
3 ( )
f
t
10 ( )
f
t
9 ( )
f
t
8 ( )
f
t
7 ( )
f
t
(1)
1
(1)
(1)
(1)
(-1)
(-1)
(1)
(1)
(1)
(1)
Fig. A1-1
Problem 1-2:
1
1.5
0
t
)
(
1 t
f
1
1
0
t
)
(
2 t
f
1
1
0
t
)
(
3 t
f
-1
1
1
0
t
)
(
4 t
f
-1
1/e
2
1
T
0
t
)
(
5 t
f
2T
-1
Fig. A1-2
Problem 1-3:
The answer is omitted.
https://doi.org/10.1515/9783110419535-app-008
Authenticated
36 PM

282
|
A Reference answers
Problem 1-4:
(1) f(−t0),
(2) f(t0),
(3) 1,
(4) e−4 + 2e−2
(5) −√3
2 e−1,
(6) 2,
(7) 0,
(8) 0,
(9)
1
2,
(10) 1
4 [δ(t) + ε(t)],
(11) 1,
(12) 0
Problem 1-5:
(1) δ󸀠(t) −δ(t),
(2) δ(t) −2e−2tε(t),
(3) δ(t)−2ε(t−1)−2δ(t−1)
Problem 1-6:
(1) f1(t) = (1 + cos πt) [ε(t) −ε(t −2)],
(2) f2(t) = 2t [2ε(t) −ε(t + 1) −ε(t −1)]
(3) f3(t) = sin(t) [ε(t) −ε(t −π)],
(4) f4(t) = sin(πt) ⋅sgn(t)
Problem 1-7:
( )
f t
t
1
-1
(2)
(-2)
1
0
-1
Fig. A1-7
Problem 1-8:
(1) energy
(2) power
(3) power
(4) energy
(5) energy
(6) no energy, no power
Problem 1-9:
1
1
-1
0
t
)
(
1 t
f e
1/2
1
1
-1
0
t
1/2
)
(
1 t
f o
1
-1
0
t
1/2
1
1
-1
0
t
1/2
)
(
2
t
f e
)
(
2 t
f
o
1
-1
0
t
1/2
1
1
-1
0
t
1/2
)
(
3
t
f e
)
(
3 t
f
o
1
-1 0
t
1/2
1
-1
0
t
1/2
)
(
4
t
f e
)
(
4 t
f
o
2
-2
2
-2
Fig. A1-9
Authenticated
36 PM

Chapter 1
|
283
Problem 1-10:
1
1
0
t
τ
τ d
f
t∫∞
−
)
(
1
2
3
4
1
1
0
t
τ
τ d
f
t∫∞
−
)
(
2
2
3
4
2
3
1
1
0
t
τ
τ d
f
t∫∞
−
)
(
3
2
3
4
2
3
Fig. A1-10
Problem 1-11:
f1(t)∗f2(t) =
+∞
∫
−∞
f1(τ)f2(t −τ)dτ =
+∞
∫
−∞
e2τe−(t−τ)ε(t −τ)dτ
= e−t
t
∫
−∞
e3τdτ = 1
3e−te3τ|t
−∞= 1
3e−te3t = 1
3e2t −∞< t < +∞
Problem 1-12:
f1(t) = e−tε(t),
f2(t) = ε(t) −ε(t −1).
Problem 1-13:
1
1
0
2
3
t
-1
1
2
( )
( )
f t
f t
∗
1
2
3
4
0
t
1
2
( )
( )
f t
f t
∗
1
(a)
(b)
Fig. A1-13
(a) f1(t)∗f2(t) =
{
{
{
{
{
{
{
{
{
{
{
{
{
0
else
t −1
1 ≤t < 2
1
2 ≤t < 3
4 −t
3 ≤t < 4
(b) f1(t)∗f2(t) =
{
{
{
{
{
{
{
0
else
1
4 (t + 1)2
−1 ≤t < 1
−1
4t2 + 1
2t + 3
4
1 ≤t ≤3
Problem 1-14:
(1) (1 −e−t)ε(t);
(2) f2(t) =
1
2π [1 −cos 2πt] [ε(t) −ε(t −1)]
Authenticated
36 PM

284
|
A Reference answers
Chapter 2
Problem 2-1:
(1) System does not meet zero input linear and zero state linear, is a nonlinear system.
(2) System does not meet decomposability, is a nonlinear system.
(3) System is a linear system.
(4) System does not meet zero input linear, is a nonlinear system.
(5) System is a nonlinear system.
Problem 2-2:
(1) System is a time invariant system.
(2) System is a time-variant system.
(3) System is a time variant system.
(4) System is a time variant system.
Problem 2-3:
(1) causal
(2) noncausal
(3) noncausal
(4) b < 0 noncausal; b ≥0 causal
(5) causal
Problem 2-4:
(1) linear, time variant, causal
(2) nonlinear, time invariant, noncausal
(3) linear, time variant, causal
(4) linear, time invariant, causal
(5) linear, time-invariant, noncausal
(6) nonlinear, time variant, causal
Problem 2-5:
When τ ≥0, time variant, causal;
when τ < 0, time variant, noncausal.
Problem 2-6:
∵f2(t) = ε(t) −2ε(t −1) + ε(t −2),
∴y2(t) = y1(t) −2y1(t −1) + y1(t −2)
∵f3(t) =
t
∫
−∞
f2(τ)dτ,
∴y3(t) =
t
∫
−∞
y2(τ)dτ
∵f4(t) = f 󸀠
2(t),
∴y4(t) = y󸀠
2(t)
Authenticated
36 PM

Chapter 2
|
285
Problem 2-7:
y1(t) = yx(t) + yf1(t) = 3e−2t + sin 4t,
y2(t) = yx(t) + 2yf1(t) = 4e−2t + 2 sin 4t,
yf1(t) = e−2t + sin 4t,
yx(t) = 2e−2t,
y3(t) = yx(t) + 3yf1(t) = 5e−2t + 3 sin 4t
t > 0
Problem 2-8:
If x1(0−) = 1, zero-input response is yx1(t); x2(0−) = 1, zero-input response is yx2(t);
f(t) =
{
{
{
1
t > 0
0
t < 0
, zero-state response is yf1(t).
∴yx1(t) = te−t + e−t,
yx2(t) = te−t,
yf1(t) = −te−t,
∴yf(t) = 3yf1(t) = −3te−t
t > 0
Problem 2-9:
2u󸀠
C(t) + 2uC(t) = us(t)
Problem 2-10:
u󸀠󸀠
C(t) + 7u󸀠
C(t) + 6uC(t) = 6 sin 2tε(t)
Problem 2-11:
(p2 + 3p + 3)u1 = (2p + 2)i(t),
(p2 + 3p + 3)u2 = 2pi(t)
Problem 2-12:
(1) y󸀠󸀠(t) + 7y󸀠(t) + 12y(t) = f(t),
(2) y󸀠󸀠󸀠(t) + 4y󸀠󸀠(t) + 10y󸀠(t) + 3y(t) = f 󸀠󸀠(t) + 10f(t)
( )
f t +
( )
y t
−
−
12
7
( )
y t
′′
( )
y t
′
∑
∫
∫
( )
x t
′′
( )
x t
′
( )
x
t
′′′
−
+
+
( )
f t +
( )
y t
−
−
10
4
∑
∫
∫
∫
3
( )
x t
10
∑
Fig. A2-12
Authenticated
36 PM

286
|
A Reference answers
Chapter 3
Problem 3-1:
y(t) = 1
2e−3t + 1
t > 0,
Natural response 1
2 e−3tε(t),
forced response ε(t).
Problem 3-2:
y(t) = 4te−2t −3e−2t + 6e−t
t ≥0;
natural 4te−2t −3e−2t,
forced 6e−t
t ≥0
Problem 3-3:
(1) y󸀠(0−) = y󸀠(0+) = 2,
y(0−) = y(0+) = 1,
y(t) = 3e−t −5
2e−2t + 1
2
t > 0
(2) y󸀠(0+) = y󸀠(0−) = 2,
y(0−) = y(0+) = 1,
y(t) = 4e−t −3e−2t −te−2t
t > 0
Problem 3-4:
∵y󸀠󸀠(t) includes δ(t),
y󸀠(t) includes ε(t),
y(t) includes tε(t),
∴∆y󸀠(0) = 1
∆y(0) = 0,
y󸀠(0+) = y󸀠(0−) + ∆y󸀠(0) = 2 + 1 = 3,
y(0+) =
y(0−) + ∆y(0) = 1 + 0 = 1,
y(t) = 2e−t −5
2e−2t + 3
2
t > 0,
yc(t) = 2et −5
2e−2t,
yp(t) = 3
2.
∵yx(t) = c3e−t + c4e−2t,
yx(0+) = yx(0−) = y(0−) = 1,
y󸀠
x(0+) = y󸀠(0−) = 2,
∴yx(t) = 4e−t −3e−2t,
∴yf(t) = y(t) −yx(t) = 2e−t −5
2e−2t + 3
2 −4e−t + 3e−2t = −2e−t + 1
2e−2t + 3
2
Problem 3-5:
yx(t) = c1e(−1−2j)t + c2e(−1+2j)t
{
{
{
yx(0−) = c1 + c2 = 2
y󸀠
x(0−) = (−1 −2j)c1 + (−1 + 2j)c2 = −2
,
yx(t) = 2e−t cos 2t
t ≥0
Problem 3-6:
yx(t) = c1e−3t + c2e−t
{
{
{
c1 + c2 = 1
−3c1 −c2 = 2
,
yx(t) = −3
2e−3t + 5
2 e−t
Problem 3-7:
yf(t) = 5e−t −5e−2t + e−3t
t ≥0
Authenticated
36 PM

Chapter 3
|
287
Problem 3-8:
(1) uC(0+) = 10 V,
i(0+) = 20−uC(0+)
R
= 5 A;
(2) uC(t) = 20 −10e−2t, t ≥0+
Problem 3-9:
iLx(t) = 1
2e−t + 1
2e−2t
t ≥0,
iLf(t) = −3
2 e−t + 1
2e−2t + 1
t ≥0
Problem 3-10:
(a) H(p) =
2(p + 10)
(p + 5)(p + 6);
(b) H(p) =
10(p + 1)
(p + 5)(p + 6)
Problem 3-11:
(1) yx(t) = 5e−t −3e−2t
t ≥0
(2) yx(t) = e−t[A1 cos t + A2 sin t] = e−t[cos t + 3 sin t] t ≥0
(3) yx(t) = 1 −e−t −te−t
t ≥0
Problem 3-12:
h(t) = 0.5e−2tε(t)
Problem 3-13:
h(t) = et−1ε(3 −t)
Problem 3-14:
yf1(t) = 1
π(1 −cos πt)[ε(t) −ε(t −2)]
Problem 3-15:
yf(t) = t[ε(t) −ε(t −T)] −(t −2T)[ε(t −T) −ε(t −2T)]
Problem 3-16:
yf1(t) = df1(t)
dt
∗g(t) = g(t) −2g(t −2) + g(t −3)
yf2(t) = {[ε(t) −ε(t −1)] −δ(t −1)} ∗(2e−2t −1)ε(t)
= (1 −t −e−2t)ε(t) −[1 −t + e−2(t−1)] ε(t −1)
Authenticated
36 PM

288
|
A Reference answers
Problem 3-17:
h(t) = [h1(t) + h2(t)∗h1(t)∗h3(t)] ∗h4(t) = 3 [ε(t) −ε(t −1)]
g(t) = 3ε(t)∗[ε(t) −ε(t −1)] = 3t [ε(t) −ε(t −1)] + 3ε(t −1)
Problem 3-18:
h(t) = ε(t) + ε(t −1) + ε(t −2) −ε(t −3) −ε(t −4) −ε(t −5)
Problem 3-19:
g(t) = 4e−2tε(t),
h(t) = 4δ(t) −8e−2tε(t)
Problem 3-20:
h(t) = e−t cos tε(t)
Chapter 4
Problem 4-1:
(a) only include αn cos nω0t.
(b) only include odd harmonic components.
Problem 4-2:
(a) Fn = 1
T ,
f1(t) = 1
T
+∞
∑
n=−∞
ejnω0t,
(b) a0 = E
2 ,
an =
{
{
{
0
(n = 2, 4, . . . )
−4E
(nπ)2
(n = 1, 3, . . . )
,
bn = 0
f2(t)
=
−4E
π2 (cos ω0t + 1
32 cos 3ω0t + 1
52 cos 5ω0t + . . . + 1
n2 cos nω0t + . . .)
(n = 1, 3, 5, . . . )
(c) F0 = 1
π ,
F1 = 1
4e−j π
2 ,
F−1 = 1
4ej π
2 ,
Fn = −cos2 nπ
2
π(n2 −1) (|n| > 1)
Problem 4-3:
(1) a0 = 1
4,
an =
1
(nπ)2 (cos nπ −1),
bn = −1
nπ cos nπ
Authenticated
36 PM

Chapter 4
|
289
f1(t) = 1
4 + 1
π2
∞
∑
n=1
cos nπ −1
n2
cos nω0t −1
π
∞
∑
n=1
cos nπ
n
sin nω0t
(2) f2(t) = 1
4 + 1
π2
∞
∑
n=1
1 −cos nπ
n2
cos nω0t −1
π
∞
∑
n=1
1
n sin nω0t
f3(t) = 1
4 + 1
π2
∞
∑
n=1
1 −cos nπ
n2
cos nω0t + 1
π
∞
∑
n=1
1
n sin nω0t
f4(t) = 1
2 + 2
π2
∞
∑
n=1
1 −cos nπ
n2
cos nω0t
Problem 4-4:
(a) f1(t) = 1
2 −1
π
∞
∑
n=1
1
n sin nω0t
(b) f2(t) = 1
2 + 2
π
∞
∑
n=1
1
n sin nπt,
n = 1, 3, 5 . . .
Problem 4-5:
0
0
4
3π
4
5π
4
7π
4
π
nc
n
ϕ
0
ω
0
ω
0
3ω
0
3ω
0
5ω
0
5ω
0
7ω
0
7ω
ω
π
•
•
•
•
•
•
•
•
ω
Fig. A4-5 (1)


π
2
π
4
π
6
π
2
π
4
π
6
2
1
2
π
2
π
π
1
2
3π
•
•
•
•
•
•
•
•Ă
nc
n
ϕ
ω
ω
Fig. A4-5 (2)
0
0
π
2π
3π
2
4
π
1
2
•
•
•
•
•
•
…
nc
n
ϕ
1
π
2π
3π
6
π
ω
ω
Fig. A4-5 (3)
Authenticated
36 PM

290
|
A Reference answers
Problem 4-8:
a0 = E
π ,
bn = 0,
an =
{
{
{
{
{
{
{
E
2
(n = 1)
0
(n = 3, . . . )
2E
(1−n2)π cos nπ
2
(n = 2, 4, . . . )
f(t) = E
π + E
2 (cos ω0t + 4
3π cos 2ω0t −
4
15π cos 4ω0t + . . . )
E
π
0
0
2
E
π
4
3π
•
•
•
•
•
nc
n
ϕ
0
ω
0
2ω
0
3ω
0
4ω
•
4
15π
•
0
ω
0
2ω
0
3ω
0
4ω
•
•
•
ω
ω
Fig. A4-8
Problem 4-9:
f(t) = 4
π (1
2 + 1
3 cos 2ω0t −1
15 cos 4ω0t + ⋅⋅⋅−cos nπ
2
n2 −1 cos nω0t + . . . )
(n = 2, 4, 6, . . . )
Problem 4-10:
uc(t) = 6 + 8 cos (103t −36.9°) + 3.33 cos (2 × 103t −56.3°) V
Problem 4-11:
u2(t) = 2
π [0.104 sin (πt + 84°) + 0.063 sin (3πt −79°)] V
Problem 4-12:
i(t) = 0.5 + 0.450 cos (ω0t −45°) + 0.067 cos (3ω0t + 108.4°) A
Chapter 5
Problem 5-1:
F1(jω) =
A
τ −A
τ e−jωτ −jAωe−jωτ
(jω)2
;
F2(jω) = τSa2 ( ωτ
2 );
F3(jω) = 4τSa(ωτ) + 2τSa ( 1
2ωτ) cos(ωτ);
F4(jω) = e−j(2ω−π
2)[Sa(ω + π) −Sa(ω −π)];
F5(jω) = 1
2 [Sa2 (ω + ω0
2
) e−j(ω+ω0) + Sa2 ( ω −ω0
2
) e−j2(ω−ω0)]
Authenticated
36 PM

Chapter 5
|
291
Problem 5-2:
(1) F1(jω) = 1
2 [1 −|ω|
4π ] [ε(ω + 4π) −ε(ω −4π)];
(2) F2(jω) =
π
100 e−j3ωg100(ω);
(3) F3(jω) = πe−2|ω|
Problem 5-3:
(1) F1(jω) = Sa ( ω
2 ) e−j2.5ω −e−j4ω
jω
;
(2) F2(jω) = 2
jω [Sa ( ωT
2 ) −cos ( ωT
2 )];
(3) F3(jω) = 2E
ω Sa [ ω(τ2 −τ1)
4
] ⋅sin [ω(τ2 + τ1)
4
]
Problem 5-4:
(1)
1
2F (j ω
2 ) e−j2.5ω
(2)
1
5F (j −ω
5 ) e−j 3
5 ω
(3)
j
2F󸀠(j ω
2 )
(4)
j
2F󸀠(−j ω
2 ) −2F (j −ω
2 )
(5) −F(jω) −ωF󸀠(jω);
(6) F [j2(ω −4)] ej6(ω−4) + F [j2(ω + 4)] ej6(ω+4)
(7) F(jω)e−j3ω −1
2 {F [j(ω −4)e−j3(ω−4)] + F [j(ω + 4)e−j3(ω+4)]}
Problem 5-5:
(1)
π
α
(2)
2
3πα3
(3)
π
ω0
Problem 5-6:
(1) −1
2 |t|;
(2)
1
πj sin(100t);
(3)
1
2π(α + jt);
(4) e2(t−1)ε(t −1) −e−3(t−1)ε(t −1)
Authenticated
36 PM

292
|
A Reference answers
Problem 5-7:
(a) f(t) = Aω0
π
Sa(ω0t −1);
(b) f(t) = −2A
πt sin 1
2ω0t ⋅cos 1
2ω0t;
(c) f3(t) =
1
jπt2 (sin t −sin 2t + t cos 3t)
Problem 5-8:
yf(t) = (e−3t + e−t −e−2t) ε(t)
Problem 5-9:
yx(t) = 7e−2t −5e−3t,
yf(t) = (−1
2e−t + 2e−2t −3
2 e−3t) ε(t)
y(t) = (−1
2e−t + 9e−2t −13
2 e−3t) ε(t)
Problem 5-10:
yf(t) = [1 −e−2(t−1)] ε(t −1) −[1 −e−2(t−2)] ε(t −2)
Problem 5-11:
uC(t) = (3 −3e−t −t) ε(t) + (t −3)ε(t −3)
Problem 5-12:
(1) i0(t) = 1
8 e−5
8 tε(t) A;
(2) i0(t) = 1
5 (1 −e−5
8 t) ε(t) A;
(3) i0(t) = 1
3 (e−5
8 t −e−t) ε(t) A
Problem 5-13:
h(t) = 1
2e−2tε(t)
if(t) = (5e−t −5.5e−2t + 1
2) ε(t)A
Problem 5-14:
H(jω) =
1
(jω)2 + 6jω + 8
h(t) = 1
2 [e−2t −e−4t] ε(t)
yf(t) = (−1
8 e−4t + 1
8e−2t −1
4 te−2t + 1
4t2e−2t) ε(t)
Authenticated
36 PM

Chapter 6
|
293
Chapter 6
Problem 6-1:
(1)
6
s2 + 4 +
2s
s2 + 9;
(2)
2 −
1
s + 1;
(3)
1
2 (1
s +
s
s2 + 16);
(4)
e−α
s + 1
(s + 1)2 + ω2 ;
(5)
e−s
s + 1;
(6)
e
s + 1;
(7)
1
s2 −e−s
s2 −e−s
s ;
(8)
es
s2 ;
(9)
π
s2 + π2 + πe−2s
s2 + π2 ;
(10)
e−2s−1(2s + 3)
(s + 1)2
;
(11)
e−s
s3 (2 + 2s + s2);
(12)
2s3 −6s
(s2 + 1)3
Problem 6-2:
(1) f(4t) ↔1
4 F ( s
4) = 1
4
1
( s
4)
2 + 3 s
4 −5
=
4
s2 + 12s −80
e−tf(4t) ↔
4
(s + 1)2 + 12(s + 1) −80 =
4
s2 + 14s −67
(2) f(2t) ↔1
2 F ( s
2)
f(2t −4) ↔1
2 F ( s
2) e−2s = e−2s
2
1
( s
2)2 + 3 ( s
2) −5
=
2e−2s
s2 + 6s −20
(3) f 󸀠󸀠(t) ↔s2F(s),
tf 󸀠󸀠(t) ↔−[s2F(s)]
󸀠= −(
s2
s2 + 3s −5)
󸀠
=
10s −3s2
(s2 + 3s −5)2
(4) f(t) sin 2t = f(t)ej2t −e−j2t
2j
= 1
2j [f(t)ej2t −f(t)e−j2t] ↔1
2j [F(s −2j) −F(s + 2j)] =
1
2j [
1
(s −2j)2 + 3(s −2j) −5 −
1
(s + 2j)2 + 3(s + 2j) −5]
(5) f(τ)eτ ↔
1
(s −1)2 + 3(s −1) −5 =
1
s2 + s −7
[
[
t
∫
0
f(τ)eτdτ]
]
= [
[
t
∫
−∞
f(τ)eτdτ]
]
=
1
s .
1
s2 + s −7,
[
[
t−2
∫
0
f (τ)eτdτ]
]
↔
e−2s
s (s2 + s −7)
(6) [1
t f(t)] ↔
∞
∫
s
1
s2 + 3s −5ds
Problem 6-3:
(a) f1(t) = 2ε(t) −ε(t −1) −ε(t −2) ↔2
s −e−s
s
−e−2s
s
= 2 −e−s −e−2s
s
Authenticated
36 PM

294
|
A Reference answers
(b) f2(t) = 1
T t [ε(t) −ε(t −T)] = t
T ε(t)−t −T
T
ε(t−T)−ε(t−T) ↔1
T
1
s2 −1
T
e−Ts
s2 −e−Ts
s
=
1 −e−Ts −Tse−Ts
Ts2
(c) f3(t) = sin(ωt) [ε(t) −ε (t −T
2 )] = sin(ωt)⋅ε(t)−sin(ωt)ε (t −T
2 ) = sin(ωt)ε(t)+
sin [ω (t −T
2 )] ε (t −T
2 ) ↔
ω
s2 + ω2 + ωe−T
2 S
s2 + ω2
Problem 6-4:
f(t) = f(t)ε(t) = f1(t) + f1(t −T) + ⋅⋅⋅=
∞
∑
T=0
f1(t −nT)
F(s) = [f(t)] = F1(s) + e−TsF1(s) + e−2TsF1(s) + ⋅⋅⋅+ e−nTsF1(s) =
F1(s)
1 −e−Ts
Problem 6-5:
(a) f1(t) = E [ε(t) −ε (t −T
2 )] ↔E [1
s −e−T
2 s
s
]
F1(s) =
1
1 −e−sT E (1 −e−T
2 s
s
) = E
s
1
1 + e−T
2 s
(b) f2(t) = E sin(ωt) [ε(t) −ε(t −T)] ↔
Eω
s2 + ω2 (1 −e−Ts)
F2(s) =
1
1 −e−Ts
Eω
s2 + ω2 (1 −e−Ts) =
Eω
s2 + ω2
(c) f3(t) = δ(t)+δ(t−2)+δ(t−4)+⋅⋅⋅+δ(t−2n)+⋅⋅⋅↔1+e−2s +e−4s +⋅⋅⋅+e−2ns+⋅⋅⋅=
1
1 −e−2s
(d) f 󸀠
4T(t) = ε(t) −2ε(t −1) + ε(t −2) ↔1
s −2
s e−s + 1
s e−2s
f4T(t) ↔1
s2 (1 −2e−s + e−2s)
f4(t) ↔
1
s2 (1 −e−s2) (1 −2e−s + e−2s) =
1
s2 (1 −e−2s) (1 −e−s)2 =
1 −e−s
s2 (1 + e−s)
Problem 6-6:
(1) f(0+) = lim
s→∞s
s + 3
(s + 1)(s + 2)2 = 0,
f(∞) = lim
s→0 s
s + 3
(s + 1)(s + 2)2 = 0
(2) F(s) = s +
−2s
s2 + 6s + 8 = s + F1(s)
f(0+) = −lim
s→∞
2s2
s2 + 6s + 8 = −2,
f(∞) = lim
s→0
s4 + 6s3 + 6s2
s2 + 6s + 8
= 0
Authenticated
36 PM

Chapter 6
|
295
(3) f(0+) = lim
s→∞s ⋅
s2 + 2s + 3
(s + 1)(s2 + 2) = 1,
f(∞) no exist,
p = ±2j
(4) f(0+) = 0,
f(∞) = lim
s→0 s ⋅
2s + 1
s3 + 3s2 + 2s = 1
2
(5) f(0+) = lim
s→∞s ⋅1 −e−2s
s (s2 + 4) = 0,
f(∞) no exist.
(6) f(0+) = lim
s→∞
1
1 + e−s = 1,
f(∞) = lim
s→0
1
1 + e−s = 1
2
Problem 6-7:
(1) f(t) = [−5e−3t + 9e−5t] ε(t),
(2) f(t) = δ󸀠(t) −3δ(t) −5e−tε(t) + 13e−2tε(t),
(3) f(t) = e−tε(t) + 2e−2ttε(t) −e−2tε(t),
(4) f(t) = ε(t) −e−t cos 2tε(t),
(5) f(t) = 2etε(t) + 2e−t(cos t −2 sin t)ε(t),
(6) f(t) = e−tt2ε(t) −e−ttε(t) + 2e−tε(t) −e−2tε(t)
Problem 6-8:
(1) 2e−2tε(t) −e−2(t−3)ε(t −3),
(2) cos πt ⋅ε(t) + cos π(t −T) ⋅ε(t −T),
(3) tε(t) + 2(t −2)ε(t −2) + (t −4)ε(t −4),
(4)
1
4 [ε(t) −ε(t −1)] −1
4 cos t ⋅ε(t) + 1
4 cos(t −1)ε(t −1),
(5) cos 3t ⋅ε(t) + 2 sin 3(t −1) ⋅ε(t −1),
(6) ∑∞
n=0 (−1)nδ(t −n),
(7) ∑+∞
n=0 (−1)nε(t −n),
(8) ∑∞
n=0 δ(t −n),
(9) ∑∞
n=0 ε(t −n),
(10) f(t) = −1
t ε(t) + 1
t e−9tε(t)
Problem 6-9:
(1) h(t) = (e−2t −e−3t) ε(t).
Authenticated
36 PM

296
|
A Reference answers
Problem 6-10:
H(s) = 1 −
11
s + 10,
Yf(s) =
1
s + 10 −
11
(s + 10)2 ,
F(s) =
1
s + 10,
f(t) = e−10tε(t)
Problem 6-11:
(1) Yf(s) =
s
s2 + 3s + 2
10
s =
10
s + 1 −
10
s + 2,
yf(t) = (10e−t −10e−2t) ε(t)
(2) Yf(s) =
s
s2 + 3s + 2 ⋅
10
s2 + 1 =
−5
s + 1 +
4
s + 2 + 1
2(1 + 3j) 1
s + j + 1
2(1 −3j) 1
s −j
yf(t) = (4e−2t −5e−t + cos t + 3 sin t) ε(t)
Problem 6-12:
Is(s) = UL(s)
3
+ UL(s) + 1
1
2s
,
UL(s) = 3sIs(s)
s + 6 −
6
s + 6 =
3s
s + 6 ⋅5
s2 −
6
s + 6 = −8.5
s + 6 + 2.5
s
uL(t) = 2.5ε(t) −8.5e−6tε(t)
Problem 6-13:
uC(t) = 1
2 [te−t −e−t + 1] ε(t)
Problem 6-14:
(1
5 + 1
s ) I1(s) −1
5 I2(s) = 15
s ,
−1
5I1(s) + (1
2 s + 6
5) I2(s) = 2
I1(s) = −57
s + 3 + 136
s + 4,
i1(t) = (−57e−3t + 136e−4t) ε(t)
Problem 6-15:
Y1(s) =
−3
s + 1 = Yx(s) + Yf1(s) = Yx(s) + H(s),
Y2(s) = 1
s −
5
s + 1 = Yx(s) + Yf2(s) = Yx(s) + 1
s H(s),
H(s) =
1
s + 1,
Yx(s) =
−4
s + 1,
yx(t) = −4e−tε(t),
Yf3(s) = H(s) ⋅1
s2 =
1
s + 1 + 1
s2 −1
s ,
yf3(t) = (e−t + t −1) ε(t),
y3(t) = yx(t) + yf3(t) = t −1 −3e−t
t ≥0
Authenticated
36 PM

Chapter 6
|
297
Problem 6-16:
Y1(s) = Yx(s)+Yf1(s) = 1+
1
1 + s ,
Y2(s) = Yx(s)+Yf2(s) =
3
1 + s ,
H(s) =
s
s + 1,
Yx(s) =
2
s + 1,
yx(t) = 2e−t,
Yf3(s) = 1 −e−s
s2
⋅
s
s + 1 =
1
s(s + 1) −
e−s
s(s + 1),
f3(t) = t [ε(t) −ε(t −1)] + ε(t −1) = tε(t) −(t −1)ε(t −1),
F3(s) = 1
s2 −e−s
s2 ,
yf3(t) = ε(t) −ε(t)e−t −ε(t −1)e−(t−1),
y3(t) = yx(t) + yf3(t) = (1 + e−t) ε(t) −[1 −e−(t−1)] ε(t −1)
Problem 6-17:
Y(s) =
s + 3
s2 + 3s + 2 F(s) +
s + 5
s2 + 3s + 2;
yx(t) = (4e−t −3e−2t) ε(t)
yf(t) = (e−t −e−2t) ε(t);
y(t) = (5e−t −4e−2t) ε(t), no forced response.
Problem 6-18:
sY(s) −y(0−) + 2Y(s) = F(s),
Y(s) = F(s)
s + 2 + y(0−)
s + 2 ,
F(s) =
2
s2 + 4
Y(s) =
2
(s + 2)(s2 + 4) +
1
s + 2 = 5
4
1
s + 2 + (−1
8 −1
8j)
1
s −2j + (−1
8 + 1
8j)
1
s + 2j ↔
5
4e−2t + 1
4(sin 2t −cos 2t)
(t ≥0)
Problem 6-19:
s2Y(s) −sy(0−) −y󸀠(0−) + 4sY(s) −4y(0−) + 3y(s) = 3F(s)
Y(s) =
3
s2 + 4s + 3 F(s) + sy(0−) + y󸀠(0−) + 4y(0−)
s2 + 4s + 3
= 1
s +
1
s + 1 , y(t) = (1 + e−t)ε(t)
Problem 6-20:
s2Y(s) −sy(0−) −y󸀠(0−) + 5sY(s) −5y(0−) + 6Y(s) = 6F(s)
Y(s) =
6
s2 + 5s + 6 F(s) + sy(0−) + y󸀠(0−) + 5y(0−)
s2 + 5s + 6
yf(t) = 6 (e−t + 2e−3t −2e−2t) ε(t),
yx(t) = (e−2t −e−3t) ε(t)
Problem 6-21:
s2Y(s) −sy(0−) −y󸀠(0−) + 2sY(s) −2y(0−) + Y(s) = sF(s)
Y(s) =
s
s2+2s+1F(s) +
s+4
s2+2s+1;
yx(t) = (1 + 3t)e−tε(t);
yf(t) = (t −t2
2 ) e−tε(t)
Authenticated
36 PM

298
|
A Reference answers
Chapter 7
Problem 7-1:
(a) and (b) H(s) = s2 + 3s + 2
s2 + 2s + 1
Problem 7-2:
∫
∫
∫
-2
-3
( )
x t
'''( )
y t
''( )
y t
'( )
y t
( )
y t
Fig. A7-2 (1)
∫
∫
∫
( )
x t
( )
y t
1
2
-2
-3
-3
'''
q
''q
'q
Fig. A7-2 (2)
7
16
12
2
3
- -
-
( )
X s
( )
Y s
1
S
1
S
1
S
Fig. A7-2 (3)
Problem 7-3:
(a) H(s) =
H1H2H3H5 + H4H5(1 + G2H2)
1 + G1H1 + G2H2 + G3H3 + G1G2G3H4 + G1G3H1H3
,
(b) H(s) =
G1G2G3G7
1 −G2G3G5G6 −G2G4G5
,
Authenticated
36 PM

Chapter 7
|
299
(c) H(s) = G1G2G3G4 −G1G5(1 + G3H1)
1 + (G3H1 + G4H2 + G2G4H3),
(d) H(s) =
s3
s3 + 6s2 + 11s + 6,
(e) H(s) =
H1H2H3H4H5 + H1H5H6(1 −G3)
1 −(H2G2 + H4G4 + H2H3G1 + G3 + H6G1G4) + H2G2(G3 + H4G4)
Problem 7-4:
1
S
1
S
- 7
-10
1
S
5
5
( )
Y s
1
S
1
S
( )
X s
( )
Y s
5
-2
-5
( )
Y s
( )
X s
3
4
−
-5
-2
5
6
( )
X s
1
S
1
S
1
S
1
S
1
2
Fig. A7-4 (1)
5
-1
-1
( )
Y s
( )
X s
5
-1
( )
Y s
( )
X s
-1
-1
( )
Y s
( )
X s
-1
4
1
S
1
S
1
S
1
S
1
S
1
S
1
S
1
S
1
S
Fig. A7-4 (2)
-6
-4
-4
3
-2
-3
3
3
( )
X s
( )
Y s
-2
-2
-2
3
-2
-2
( )
Y s
( )
X s
( )
Y s
( )
X s
1
S
1
S
1
S
1
S
1
S
1
S
1
S
1
S
1
S
Fig. A7-4 (3)
Authenticated
36 PM

300
|
A Reference answers
Problem 7-5:
H(s) =
s2 + s
s3 + 14s2 + 42s + 30
Problem 7-6:
H(s) =
2s + 8
s3 + 6s2 + 11s + 6
Problem 7-7:
yf(t) = −2
5 [2
5 cos(t −1) + 1
5 sin(t −1) −2
5 e−2(t−1)] ε(t −1)
Problem 7-8:
(a) H(s) = (s + 2)(s + 4)
(s + 1)(s + 3);
(b) H(s) = (s2 + 1) (s2 + 3)
2s (s2 + 2)
; (c) H(s) =
2s
4s2 + 6s + 1
Problem 7-9:
(1) H(s) = 10 s −1
s(s + 1);
(2) ys(t) = 6 sin 3t −8 cos 3t
Problem 7-10:
R = 2 Ω,
L = 2 H,
C = 0.25 F
Problem 7-11:
yf(t) = (1
5 sin t + 4
5 sin 4t) ε(t), no free response.
Problem 7-12:
(1) unstable, negative real roots 4, characteristic equation misses term s;
(2) unstable, positive real roots 2;
(3) critical, negative real roots 2;
(4) critical, negative real roots 2;
(5) unstable, positive real roots 2, negative real roots 3.
Problem 7-13:
(1) k < 4;
(2) 0 < k < 60
Authenticated
36 PM

Chapter 7
|
301
Problem 7-14:
(1) k < 3;
(2) when k = 3, critically stable, h(t) = cos √2tε(t)
Problem 7-15:
(1) k < 4;
(2) when k = 4, critically stable, h(t) = 4 cos 2tε(t)
Authenticated
36 PM

Authenticated
36 PM

Bibliography
Weigang Zhang, Wei-Feng Zhang. Signals and Systems (Chinese edition). Beijing, Tsinghua univer-
sity press, 2012.
Weigang Zhang. Circuits Analysis (Chinese edition). Tsinghua university press, January 2015.
Alin V. Oppenheim, Alin S. Willsky, S. Hamid Nawab. Signals and Systems (Second Edition). Beijing,
Electronic industry press, February 2004.
Bernd Girod, Rudolf Rabenstein, Alexander Strenger. Signals and Systems. Beijing, Tsinghua univer-
sity press, March 2003.
Charles L. Phillips, John M. Parr, Eve A. Riskin. Signals, Systems and Transforms (Third Edition).
Beijing, Mechanical industry press, January 2004.
M. J. Roberts. Signals and Systems – Analysis Using Transform Methods and MATLAB. McGrawHill
Companies, 2004.
Weigang Zhang, Lina Cao. Communication Principles (Chinese Edition). Beijing, Tsinghua university
press, 2016.
https://doi.org/10.1515/9783110419535-009
Authenticated
36 PM

Authenticated
36 PM

Index
A
A/D conversion 6
absolutely integrable 115, 149, 193
additivity 51
algebraic equation 58
amplitude spectrum 129, 149
analog signal 4
analogous systems 46
attenuation factor 193
auxiliary tool 241
B
bandpass signals 6
baseband digital signal 7
basic theories 63
behaviors of the system 49
BIBO stability 256
block diagram simulation 242
bridge 138, 175
C
capacitor current 82
characteristic equation 75
classical analysis method 74
complex frequency 13, 194
complex function set 112
complex plane 196
composite function 36
conclusion 125, 131, 177, 178, 218, 230
constant term 11
continuous signal 3
continuous sum 101
contribution 102, 176
convergence 137
core 147
Cramer’s rule 95
D
damped oscillation curve 259
decomposability 115
describe a signal 2
differential equation 74
digital signal 6
direct simulation 249
Dirichlet conditions 115
discrete signal 4
discreteness 136
Duhamel’s integral 101
E
elements in a row are all zero 268
energy spectral density 171
essence 56
essential difference 227
Euler’s relation 12
even harmonic signal 119
extension 150
external analysis method 67
F
feedback signal 59
feedback system 58
ﬁltering concept 122
ﬁxed system 56
ﬂawed 111
ﬂow graph simulation 242
ﬂow graphs 245
forced response 86
Fourier prism 137
frequency band width 171
frequency division multiplexing 171
frequency feature 2
full wave rectiﬁed signal 134
fundamental wave 117
G
gain 245
generalization 194
Gibbs phenomenon 121
graphical representation 65, 241
H
half period overlap 119
halfwave rectiﬁed signal 133
harmonic 117
harmonic response summation 141
harmonics 136
Hilbert ﬁlter 184
Hilbert transform pair 183
homogeneity 50
homogeneous solution 74
Hurwitz polynomial 265
hybrid system 50
https://doi.org/10.1515/9783110419535-010
Authenticated
36 PM

306
|
Index
I
image function 194, 218
important characteristic of an LTI causal system
92
impulse train 206
incrementally linear systems 52
inductor voltage 82
initial condition 76
initial moment 76
initial state 49
input signal 46
invertibility 61
ISI concept 164
J
judgment condition 52, 104
K
key 216
Kirchhoff’s laws 63
L
Lathi deﬁnition 52
Law of Switching 82, 83, 85
linear combination 114, 115
linearity 50
locations of poles 258
long division 208, 212
lowpass signals 6
LTI system 56
M
Mason’s formula 247
master key 122
memory system 58
memoryless 58
modeling to signal 2
modulation 169, 170
motion behavior 63
N
natural frequencies 75
natural response 86
natural type terms 86
new approach 147
new path 194
no essential difference 220
nonlinear devices 50
notation 45
Nth-order linear constant coefficient difference
equation 62
nth-order linear constant coefficient differential
equation 61
Nyquist frequency 6
Nyquist interval 6
Nyquist rate 6
O
odd harmonic signal 119
optimal control 270
original function 149, 194
orthogonal (perpendicular) vectors 113
oscillating wave 259
output signal 47
overlap integral 31
overshoot 271
P
pair of conjugate roots 213
parallel circuit models 227
parallel simulation 253
Parseval’s relation 129
partial fraction expansions 211
particular solution 74
peak time 271
phase spectrum 129, 149
phasor 138
physical meaning 185
points about the ROC 197
power signals 157
prism 137
pseudo-algebraic equation 95
purpose 95, 98, 100, 102, 113, 171
Q
quantization 6
R
ratio 96, 138, 216
real fraction 208
real frequency 194
reappearance characteristic 34
reason 119, 130, 135, 222
relationship between the three methods 231
response decomposability 52
right half-plane 208
ROC 196
Routh array 265
Authenticated
36 PM

Index
|
307
S
s domain mathematic models of Kirchhoff’s
laws 225
sampling function 152
sampling theorem 6, 173
scale coefficient 26
self-excitation 269
series circuit models 227
series simulation 252
shows that 169, 170, 173
sign signal 14
signal analysis 2
signal decomposition methods 28
signal locks 122
signal processing 3
signiﬁcance 117, 122
similarities and differences 151
singularity signal 19
sinusoidal signal generator 269
sinusoidal steady state 58
slacking (at rest) condition 62
solve equations 68
special case 194
spectrum density 147
standard trigonometric form 117
starting state 49
state of system 48
state space analysis method 67
states that 149, 162, 164, 173, 176, 206, 230
steady state response 86
step value 81
superposition 51
switch 15
system analysis 46
system analysis process 73
systems in this book 45
T
three-factor method 47
TI system 55
time constant 47
time feature 2
time inversion theorem 162
train 19
transform method 221
transient process 47
transient response 86
transmit random signals 9
two constraint conditions 63
two purposes 224
typical excitation signals 75
typical pole-zero distributions 260
U
unilateral Laplace transform 195
unit step signal is derivable 19
V
V-A characteristic 63
W
white light 137
white spectrum 155
Z
zero and the pole 212
zero-in/zero-out 52
zero-input linearity 53
zero-input response stability 256
zero-state linearity 53
zero-state response stability 256
Authenticated
36 PM

Authenticated
36 PM


Advanced Information and Knowledge Processing
Sheng Li
Yun Fu
Robust 
Representation 
for Data 
Analytics
Models and Applications

Advanced Information and Knowledge
Processing
Series editors
Lakhmi C. Jain
Bournemouth University, Poole, UK and
University of South Australia, Adelaide, Australia
Xindong Wu
University of Louisiana at Lafayette

Information systems and intelligent knowledge processing are playing an increasing
role in business, science and technology. Recently, advanced information systems
have evolved to facilitate the co-evolution of human and information networks
within communities. These advanced information systems use various paradigms
including artiﬁcial intelligence, knowledge management, and neural science as well
as conventional information processing paradigms. The aim of this series is to
publish books on new designs and applications of advanced information and
knowledge processing paradigms in areas including but not limited to aviation,
business, security, education, engineering, health, management, and science. Books
in the series should have a strong focus on information processing—preferably
combined with, or extended by, new results from adjacent sciences. Proposals for
research monographs, reference books, coherently integrated multi-author edited
books, and handbooks will be considered for the series and each proposal will be
reviewed by the Series Editors, with additional reviews from the editorial board and
independent reviewers where appropriate. Titles published within the Advanced
Information and Knowledge Processing series are included in Thomson Reuters’
Book Citation Index.
More information about this series at http://www.springer.com/series/4738

Sheng Li • Yun Fu
Robust Representation
for Data Analytics
Models and Applications
123

Sheng Li
Northeastern University
Boston, MA, USA
Yun Fu
Northeastern University
Boston, MA, USA
ISSN 1610-3947
ISSN 2197-8441
(electronic)
Advanced Information and Knowledge Processing
ISBN 978-3-319-60175-5
ISBN 978-3-319-60176-2
(eBook)
DOI 10.1007/978-3-319-60176-2
Library of Congress Control Number: 2017945672
© Springer International Publishing AG 2017
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of
the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology
now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or
the editors give a warranty, express or implied, with respect to the material contained herein or for any
errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional
claims in published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
Nowadays high-dimensional and large-scale data can be collected everywhere, due
to the wide deployment of low-cost sensors. Robust representation learning is
considered as an essential problem in data analytics, which can extract informative
features and infer knowledge from big data with uncertainty.
This book presents the concepts and models of robust representation learning
and provides a set of solutions to dealing with real-world data analytics tasks, such
as clustering, classiﬁcation, time series modeling, outlier detection, collaborative
ﬁltering, etc. Particularly, four types of robust feature representations are developed,
which extend the understanding of graph, subspace, dictionary, and latent factor.
Leveraging the theoretical merits of low-rank and sparse modeling, this book
develops robust feature representations under various learning paradigms, including
unsupervised learning, supervised learning, semi-supervised learning, multi-view
learning, transfer learning, and deep learning. As a professional research mono-
graph, this book covers a wide range of applications in the research ﬁelds of big data,
human-centered computing, pattern recognition, digital marketing, Web mining, and
computer vision. Readers from different backgrounds may all beneﬁt from the well-
balanced contents for both theoretical analysis and real-world applications.
This book consists of ten chapters. Chapter 1 introduces the background of robust
data representations. Part I, which includes Chaps. 2, 3, 4, 5, and 6, introduces the
fundamentals of robust representations, and presents a set of robust representation
models involving graph, subspace, multi-view subspace, and dictionary. Chapter 2
overviews the existing techniques on this topic. Chapter 3 presents two robust graph
construction models that integrate low-rank coding and graph sparsiﬁcation. Chap-
ter 4 proposes a robust subspace discovery model that learns a discriminative and
robust subspace from noisy data. Chapter 5 designs a multi-view subspace learning
model and applies it to multi-view time series classiﬁcation. Chapter 6 performs
self-taught learning by training a robust dictionary from both auxiliary domain and
target domain. Part II, which includes Chaps. 7, 8, 9, and 10, presents the real-
world applications of robust data representations in several speciﬁc areas. Chapter 7
develops a collaborative ﬁltering framework based on deep feature learning and
applies the model to movie recommendation and book recommendation. Chapter 8
v

vi
Preface
incorporates the temporal dynamics into collective matrix factorization and applies
the model to conversion prediction. Chapter 9 presents a multi-view low-rank
analysis framework and applies it to outlier detection. Chapter 10 proposes a cross-
view projective dictionary learning model and applies it to person reidentiﬁcation.
Each chapter begins with an overview of its content and ends with references.
In particular, this book can be used by these audiences in the background of
computer science, information systems, data science, statistics, and mathematics.
Other potential audiences can be attracted from broad ﬁelds of science and
engineering since this topic has potential applications in many disciplines.
We would like to thank our collaborators Ming Shao, Kang Li, Jaya Kawale, and
Yaliang Li. We would also like to thank editor Helen Desmond from Springer for
the help and support.
Boston, MA, USA
Sheng Li
Boston, MA, USA
Yun Fu
April 2017

Contents
1
Introduction .................................................................
1
1.1
What Are Robust Data Representations? ...........................
2
1.2
Organization of the Book ............................................
3
Part I
Robust Representation Models
2
Fundamentals of Robust Representations ...............................
9
2.1
Representation Learning Models ....................................
9
2.1.1
Subspace Learning..........................................
9
2.1.2
Multi-view Subspace Learning ............................
10
2.1.3
Dictionary Learning ........................................
11
2.2
Robust Representation Learning.....................................
11
2.2.1
Subspace Clustering ........................................
12
2.2.2
Low-Rank Modeling .......................................
12
References....................................................................
13
3
Robust Graph Construction ...............................................
17
3.1
Overview..............................................................
17
3.2
Existing Graph Construction Methods ..............................
20
3.2.1
Unbalanced Graphs and Balanced Graph .................
20
3.2.2
Sparse Representation Based Graphs ......................
21
3.2.3
Low-Rank Learning Based Graphs ........................
21
3.3
Low-Rank Coding Based Unbalanced Graph Construction........
22
3.3.1
Motivation ..................................................
22
3.3.2
Problem Formulation .......................................
23
3.3.3
Optimization ................................................
25
3.3.4
Complexity Analysis .......................................
27
3.3.5
Discussions .................................................
28
3.4
Low-Rank Coding Based Balanced Graph Construction...........
28
3.4.1
Motivation and Formulation ...............................
28
3.4.2
Optimization ................................................
29
vii

viii
Contents
3.5
Learning with Graphs ................................................
29
3.5.1
Graph Based Clustering ....................................
30
3.5.2
Transductive Semi-supervised Classiﬁcation .............
30
3.5.3
Inductive Semi-supervised Classiﬁcation .................
31
3.6
Experiments ..........................................................
31
3.6.1
Databases and Settings .....................................
32
3.6.2
Spectral Clustering with Graph ............................
33
3.6.3
Semi-supervised Classiﬁcation with Graph ...............
35
3.6.4
Discussions .................................................
38
3.7
Summary..............................................................
41
References....................................................................
41
4
Robust Subspace Learning ................................................
45
4.1
Overview..............................................................
45
4.2
Supervised Regularization Based Robust Subspace (SRRS).......
49
4.2.1
Problem Formulation .......................................
49
4.2.2
Theoretical Analysis........................................
52
4.2.3
Optimization ................................................
53
4.2.4
Algorithm and Discussions ................................
55
4.3
Experiments ..........................................................
57
4.3.1
Object Recognition with Pixel Corruption ................
57
4.3.2
Face Recognition with Illumination and Pose Variation ..
63
4.3.3
Face Recognition with Occlusions.........................
65
4.3.4
Kinship Veriﬁcation ........................................
66
4.3.5
Discussions .................................................
67
4.4
Summary..............................................................
69
References....................................................................
69
5
Robust Multi-view Subspace Learning...................................
73
5.1
Overview..............................................................
73
5.2
Problem Deﬁnition ...................................................
76
5.3
Multi-view Discriminative Bilinear Projection (MDBP) ...........
77
5.3.1
Motivation ..................................................
78
5.3.2
Formulation of MDBP .....................................
78
5.3.3
Optimization Algorithm ....................................
81
5.3.4
Comparison with Existing Methods .......................
83
5.4
Experiments ..........................................................
84
5.4.1
UCI Daily and Sports Activity Dataset ....................
84
5.4.2
Multimodal Spoken Word Dataset .........................
87
5.4.3
Discussions .................................................
88
5.5
Summary..............................................................
91
References....................................................................
91
6
Robust Dictionary Learning...............................................
95
6.1
Overview..............................................................
95
6.2
Self-Taught Low-Rank (S-Low) Coding ...........................
99

Contents
ix
6.2.1
Motivation ..................................................
99
6.2.2
Problem Formulation ....................................... 100
6.2.3
Optimization ................................................ 102
6.2.4
Algorithm and Discussions ................................ 104
6.3
Learning with S-Low Coding........................................ 106
6.3.1
S-Low Clustering ........................................... 106
6.3.2
S-Low Classiﬁcation ....................................... 107
6.4
Experiments .......................................................... 107
6.4.1
Datasets and Settings ....................................... 107
6.4.2
Property Analysis........................................... 110
6.4.3
Clustering Results .......................................... 112
6.4.4
Classiﬁcation Results....................................... 113
6.4.5
Discussions ................................................. 116
6.5
Summary.............................................................. 116
References.................................................................... 117
Part II
Applications
7
Robust Representations for Collaborative Filtering .................... 123
7.1
Overview.............................................................. 123
7.2
Collaborative Filtering ............................................... 125
7.2.1
Matrix Factorization for Collaborative Filtering .......... 125
7.2.2
Deep Learning for Collaborative Filtering ................ 126
7.3
Preliminaries.......................................................... 127
7.3.1
Matrix Factorization ........................................ 127
7.3.2
Marginalized Denoising Auto-encoder (mDA) ........... 128
7.4
Our Approach ........................................................ 129
7.4.1
Deep Collaborative Filtering (DCF): A General
Framework .................................................. 130
7.4.2
DCF Using PMF + mDA................................... 131
7.4.3
Discussion .................................................. 135
7.5
Experiments .......................................................... 136
7.5.1
Movie Recommendation ................................... 137
7.5.2
Book Recommendation .................................... 140
7.5.3
Response Prediction ........................................ 141
7.5.4
Discussion .................................................. 143
7.6
Summary.............................................................. 144
References.................................................................... 145
8
Robust Representations for Response Prediction ....................... 147
8.1
Overview.............................................................. 147
8.2
Response Prediction.................................................. 149
8.2.1
Prediction Models with Temporal Dynamics ............. 150
8.2.2
Prediction Models with Side Information ................. 151

x
Contents
8.3
Preliminaries.......................................................... 151
8.3.1
Notations .................................................... 152
8.3.2
Problem Deﬁnition ......................................... 152
8.4
Dynamic Collective Matrix Factorization (DCMF) with
Side Information...................................................... 153
8.4.1
CMF for Conversion Prediction ........................... 153
8.4.2
Modeling Temporal Dynamics............................. 155
8.4.3
Modeling Side Information ................................ 157
8.4.4
Discussions ................................................. 157
8.5
Optimization.......................................................... 159
8.5.1
Algorithm ................................................... 159
8.5.2
Discussions ................................................. 161
8.6
Experiments .......................................................... 162
8.6.1
Experiments on Public Data................................ 162
8.6.2
Conversion Prediction: Settings............................ 164
8.6.3
Conversion Prediction: Results and Discussions.......... 166
8.6.4
Effectiveness Measurement of Ads ........................ 168
8.6.5
Discussions ................................................. 168
8.7
Summary.............................................................. 170
References.................................................................... 171
9
Robust Representations for Outlier Detection .......................... 175
9.1
Overview.............................................................. 175
9.2
Preliminary ........................................................... 179
9.2.1
Outlier Detection ........................................... 179
9.2.2
Multi-view Outliers......................................... 180
9.3
Multi-view Low-Rank Analysis (MLRA)........................... 181
9.3.1
Cross-View Low-Rank Analysis ........................... 181
9.3.2
Outlier Score Estimation ................................... 185
9.4
MLRA for Multi-view Group Outlier Detection.................... 186
9.4.1
Motivation .................................................. 187
9.4.2
Formulation and Algorithm ................................ 187
9.5
Experiments .......................................................... 189
9.5.1
Baselines and Evaluation Metrics.......................... 189
9.5.2
Synthetic Multi-view Settings on Real Data .............. 190
9.5.3
Real-World Multi-view Data with Synthetic Outliers .... 194
9.5.4
Real-World Multi-view Data with Real Outliers .......... 195
9.5.5
Group Outlier Detection.................................... 195
9.5.6
Discussions ................................................. 196
9.6
Summary.............................................................. 196
References.................................................................... 199
10
Robust Representations for Person Re-identiﬁcation................... 203
10.1
Overview.............................................................. 203
10.2
Person Re-identiﬁcation.............................................. 205

Contents
xi
10.3
Cross-View Projective Dictionary Learning (CPDL)............... 205
10.3.1
Motivation .................................................. 205
10.3.2
Formulation of CPDL ...................................... 206
10.4
CPDL for Person Re-identiﬁcation .................................. 207
10.4.1
Feature Extraction .......................................... 207
10.4.2
CPDL for Image Representation........................... 208
10.4.3
CPDL for Patch Representation............................ 209
10.4.4
Matching and Fusion ....................................... 210
10.5
Optimization.......................................................... 211
10.5.1
Optimizing Image-Level Representations ................. 211
10.5.2
Optimizing Patch-Level Representations.................. 212
10.6
Experiments .......................................................... 213
10.6.1
Settings...................................................... 214
10.6.2
VIPeR Dataset .............................................. 214
10.6.3
CUHK01 Campus Dataset ................................. 215
10.6.4
GRID Dataset ............................................... 217
10.6.5
Discussions ................................................. 218
10.7
Summary.............................................................. 220
References.................................................................... 220
Index............................................................................... 223

Chapter 1
Introduction
Abstract High-dimensional and large-scale data are everywhere. Nowadays, the
wide deployment of low-cost sensors has made it possible to continuously col-
lect measurements for different purposes. Several real-world scenarios are: In
Entertainment, the consumer devices such as Kinect could capture and process
visual information and motion data in real time; In Social Network, people reﬁne,
upload, and comment on images or videos that are captured by mobile devices
with high-deﬁnition (HD) cameras; In Visual Surveillance, the cameras deployed
in public spaces are capturing the pedestrians and objects, in order to identify
abnormal behaviors; In Digital Marketing, the online responses from customers
could be collected from different channels, and integrated together to provide better
recommendations; In Healthcare, the medical sensors for patient monitoring would
provide a comprehensive understanding of patients. Learning representations from
high-dimensional data collected in these scenarios drives intelligent applications. In
this chapter, we introduce some critical issues in representation learning, describe
the robust data representations, and present the organization of this book.
Extracting informative features and inferring knowledge from big data is considered
as an essential objective of data analytics. Particularly, learning feature representa-
tions plays a major role, as it mitigates the gap between low-level observed data and
high-level semantic knowledge. To deal with the high-dimensional and large-scale
data, intelligent and efﬁcient representation learning models are highly desired.
In the past decades, data mining and machine learning researchers have made
signiﬁcant progress toward modeling data for different analytics tasks. Two critical
issues should be carefully addressed when designing the models. The ﬁrst one is
scalability. Data collected in real-world applications are keep growing in terms of
data size and dimensionality. For instance, according to some statistics on social
networks, every minute, over 400 h of videos are uploaded to YouTube, and more
than 50,000 photos are presented to Instagram. A successful data mining model
would be able to deal with a large amount of high-dimensional data efﬁciently. The
second issue is model robustness. Many traditional models, especially the statisti-
cal learning based ones, pose strong assumptions on the underlying distribution of
data. However, the data captured in real-world might be corrupted or contaminated
with severe noise, which violates the underlying assumptions. Therefore, it is of
© Springer International Publishing AG 2017
S. Li, Y. Fu, Robust Representation for Data Analytics, Advanced Information
and Knowledge Processing, DOI 10.1007/978-3-319-60176-2_1
1

2
1
Introduction
great importance to develop robust data mining models that can learn robust data
representations from data with uncertainty. This book presents the concepts, models,
and applications of robust data representations.
1.1
What Are Robust Data Representations?
To understand the robustness of data representations, we ﬁrst discuss the possible
types of uncertainty that might be found in real-world data. In particular, we
consider the uncertain data observations in a general sense. The uncertainty
might be:
•
Gaussian noise;
•
Random corruptions;
•
Missing values in data, due to data loss during transmission, etc.
•
Outliers or anomalies;
•
Uncertainty within one modality;
•
Uncertainty across multiple modalities.
The ﬁrst four types are well aligned with the traditional interpretations of data
uncertainty in the literature, while the last two are considered as special cases of
uncertainty in a more general sense. In common settings of data analytics, one object
usually has multiple instances, such as multiple face images of the same person.
If the multiple instances are from the same modality, variations on appearance
may introduce uncertain information. For example, face images from the same
person may have expression variations or illumination changes. For the last one,
if the object is captured as multiple modalities using different types of sensors, the
variations across different modalities would introduce another level of uncertainty.
It has been extensively demonstrated that exploiting the low-dimensional struc-
ture from high-dimensional data will greatly beneﬁt the data analytics tasks.
Particularly, recent advances on low-rank and sparse modeling have shown promis-
ing performance on recovering clean data from noisy observations, by discovering
the low-dimensional subspace structures. This observation motivate us to develop
new models for extracting robust data representations. The research objectives are
twofold: (1) learning robust data representations from data with uncertainty, by
exploiting the low-dimensional subspace structures; (2) evaluating the performance
of the learned representations on a wide rage of real-world data analytics tasks.
Four categories of data representations are studied in this book, including graph,
subspace, dictionary and latent factor. Robust data representations have been
developed under each of the four categories. First, two novel graph construction
schemes are introduced, by integrating the low-rank modeling with graph sparsiﬁ-
cation strategies. Each sample is represented in the low-rank coding space. And it is
revealed that the similarity measurement in the low-rank coding space is more robust
than that in the original sample space. The robust graphs could greatly enhance the
performance of graph based clustering and semi-supervised classiﬁcation. Second,

1.2
Organization of the Book
3
low-dimensional discriminative subspaces are learned in single-view and multi-
view scenarios, respectively. A single-view robust subspace discovery model is
motivated from low-rank modeling and Fisher criterion, and it is able to accurately
classify the noisy images. In addition, a multi-view subspace learning model is
designed for extracting compact features from multimodal time series data, which
leverages a shared latent space and fuses information from multiple data views.
Third, dictionary serves as expressive bases for characterizing visual data. A robust
dictionary learning method is designed to transfer knowledge from source domain
to a target domain with limited training samples. A cross-view dictionary learning
framework is presented to model the view consistency and extract robust features for
images from two camera views. Fourth, latent factors, as compact representations
of high-dimensional features, are extracted for the tasks of response prediction and
collaborative ﬁltering.
From the perspective of machine learning paradigms, this book covers clustering,
semi-supervised learning, classiﬁcation, multi-view learning, time-series modeling,
graph mining, subspace learning, dictionary learning, transfer learning, and deep
learning. The proposed models have obtained remarkable improvements on many
real-world data analytics tasks, including image clustering, object recognition, face
recognition, kinship veriﬁcation, recommender system, outlier detection, person re-
identiﬁcation, and response prediction.
1.2
Organization of the Book
The rest of this book is organized as follows.
Part I focus on developing the robust representation models by learning robust
graphs, robust subspaces, and robust dictionary. It consists of the following ﬁve
chapters. Chapter 2 presents the fundamentals of robust representations, which
covers the overviews of existing representation learning and robust representation
methods. The advantages and disadvantages of these existing methods are also
discussed.
Chapter 3 presents a robust graph construction approach. Graphs have been
widely applied in modeling the relationships and structures in real-world applica-
tions. By virtue of recent advances in low-rank subspace recovery, we notice that
the similarity between every two samples evaluated in the low-rank coding space
is more robust than that in the sample space. Based on the low-rank codings, we
propose unbalanced and balanced graph construction methods that incorporate the
k-NN constraint and b-matching constraint, respectively. Extensive evaluations on
several benchmark databases demonstrate the superiority of the proposed graphs
over several state-of-the-art graphs in data clustering, transductive and inductive
semi-supervised learning.
Chapter 4 presents a robust subspace discovery approach. Subspace learning
is widely used in extracting discriminative features for classiﬁcation. However,
conventional subspace learning methods usually have strong assumptions on the

4
1
Introduction
data distribution, and therefore they are sensitive to the noisy data. The learned
subspace has limited discriminability. To address this problem, we propose to
exploit a discriminative and robust subspace, which is insensitive to noise or
pose/illumination variations, for dimensionality reduction and classiﬁcation. Our
approach achieves promising performance on noisy image classiﬁcation and noisy
face recognition.
Chapter 5 presents a robust multi-view subspace learning approach. In particular,
we focus on learning discriminative features for classifying multi-view multivariate
time series (m.t.s.) data. Our approach keeps the original temporal structure of
m.t.s. data, and projects m.t.s. from different views onto a shared latent subspace.
It also incorporates discriminative information by minimizing the within-class
separability and maximizing the between-class separability of m.t.s. in the shared
latent subspace. Moreover, a Laplacian regularization term is designed to preserve
the temporal smoothness within m.t.s.. Remarkable performance are observed on
two real-world datasets.
Chapter 6 presents a robust dictionary learning approach for knowledge transfer.
Self-taught learning is a special case of transfer learning, which transfers the useful
knowledge learned from an auxiliary domain to help the learning tasks in target
domain. We focus on building a self-taught coding framework, which can effectively
utilize the rich low-level pattern information abstracted from the auxiliary domain,
in order to characterize the high-level structural information in the target domain. By
leveraging a high quality dictionary learned across auxiliary and target domains, the
proposed approach learns expressive codings for the samples in the target domain.
Extensive experiments on ﬁve benchmark datasets demonstrate the effectiveness of
our approach.
Part II further develops a set of robust representation models on graphs,
subspaces, dictionary and latent factors, and mainly focuses on the real-world
applications of these models in different domains. This part includes the following
ﬁve chapters.
Chapter 7 presents a deep collaborative ﬁltering framework, by exploiting robust
latent factors. Learning effective latent factors plays the most important role in
collaborative ﬁltering. However, the latent factors learned by existing methods may
not be very effective due to the sparse nature of the ratings and the side information.
To tackle this problem, we learn effective latent representations via deep learning.
In particular, our approach integrates probabilistic matrix factorization with deep
feature learning. The combined framework leads to a parsimonious ﬁt over the latent
features as indicated by its improved performance in comparison to prior state-of-
art models over four large datasets for the tasks of movie/book recommendation and
response prediction.
Chapter 8 presents a dynamic collective matrix factorization model based on
robust latent factors for response prediction. In particular, we aim to predict the
conversion response of the users by jointly examining the past purchase behavior
and the click response behavior. To achieve this, a dynamic collective matrix
factorization model is designed to make use of the temporal dynamics between
the click response and purchase activity. Moreover, side information of users,

1.2
Organization of the Book
5
advertisements, and items are also incorporated. The proposed model and several
representative baselines are evaluated on a public dataset and a real-world marketing
dataset.
Chapter 9 presents a multi-view outlier detection framework by exploiting the
low-rank representations. Outlier detection from multi-view data is still a very
challenging problem, as the data in multiple views usually have more complicated
distributions and exhibit inconsistent behaviors. A multi-view low-rank analysis
framework is designed to pursuits outliers from the perspective of robust data
representations. First, the cross-view low-rank coding is performed to reveal the
intrinsic structures of data. Second, the outliers are identiﬁed through an outlier
score estimation procedure. Moreover, the multi-view group outlier detection
problem is also discussed in this chapter. Empirical evaluations on multiple real-
world datasets demonstrate the effectiveness of the proposed framework.
Chapter 10 presents a robust dictionary learning approach in the cross-view
setting. To improve the representation power of features for person re-identiﬁcation,
we learn compact and discriminative representations via robust dictionary learning.
By utilizing the projective dictionary learning strategy, two objectives are designed
to learn low-dimensional representations for each pedestrian in the patch-level and
the image-level, respectively. The proposed objectives can capture the intrinsic
relationships of different representation coefﬁcients in various settings. Experiments
on three benchmark datasets show that the proposed model outperforms the state-
of-the-art methods on person re-identiﬁcation.

Part I
Robust Representation Models

Chapter 2
Fundamentals of Robust Representations
Abstract This chapter presents the fundamentals of robust representations. In
particular, we provide a brief overview of existing representation learning and
robust representation methods. The advantages and disadvantages of these existing
methods are also discussed.
2.1
Representation Learning Models
Representation learning has attracted a lot of attention in the research ﬁelds of
data mining and machine learning. In this section, we brieﬂy review the traditional
representation learning methods, including subspace learning, multi-view subspace
learning, and dictionary learning.
2.1.1
Subspace Learning
Subspace learning is an effective technique in extracting informative features
from data, which reduces the dimensionality of data through linear or nonlinear
projections. It has been extensively studied and widely used in many real-world
applications, such as face recognition, object recognition and visualization. The
basic idea of subspace learning methods is to project high-dimensional samples into
a low-dimensional subspace, in which some speciﬁc properties could be satisﬁed.
According to the availability of class labels, subspace learning methods can be
mainly divided into three groups: unsupervised methods, supervised methods, and
semi-supervised methods.
The unsupervised methods only utilize unlabeled data [23], semi-supervised
methods make use of the partial labeled data [4], and supervised methods learn
subspaces using the fully labeled data [14]. The representative unsupervised
methods include principal component analysis (PCA) [23] and locality preserving
projections (LPP) [19]. PCA, as a parametric model, projects data into a low-
dimensional subspace by maximizing the variance of data, and therefore it is very
© Springer International Publishing AG 2017
S. Li, Y. Fu, Robust Representation for Data Analytics, Advanced Information
and Knowledge Processing, DOI 10.1007/978-3-319-60176-2_2
9

10
2
Fundamentals of Robust Representations
useful in applications like data compression. LPP is a non-parametric method, which
preserves the neighborhood structure of samples on manifold. As a result, LPP
is able to model data with complex distributions. In addition, by utilizing kernel
tricks, the nonlinear extensions of the unsupervised subspace learning methods can
be developed [22].
Supervised subspace learning methods are very effective in extracting discrimi-
native features, and usually achieve promising performance in classiﬁcation tasks.
Linear discriminant analysis (LDA) [3] is developed upon the Fisher criterion,
which aims at ﬁnding a projection to maximize the inter-class scatter and minimize
the intra-class scatter simultaneously. Many supervised subspace methods have
been proposed to improve LDA. Local Fisher discriminant analysis (LFDA) [44]
uses local neighborhood information to construct the weighted between-class and
within-class scatter matrices, and then performs discriminant analysis. Subclass
discriminant analysis [58] models the data using mixture of Gaussians, and redeﬁnes
the scatter matrices used in LDA. LSDA [5] preserves both discriminant and local
geometrical structure in data. Those methods usually obtain promising results on
clean data, since they place speciﬁc assumptions on data distributions. However,
when the data are corrupted by large amount of noise or large variations in real
applications, these assumptions may be invalid, and the noise or variation can
reduce the separability in a classiﬁcation task. Therefore, the performance is heavily
degraded. In addition, traditional methods require the vectorized data as input, while
some advanced methods learn bilinear projections that directly process high-order
data (e.g., images or EEG signals) without vectorization [9, 12].
Semi-supervised subspace learning methods lie between the unsupervised meth-
ods and the supervised ones, which make use of both labeled data and unlabeled
data. The most representative method in this category is semi-supervised discrim-
inant analysis (SDA) [4], which employs a graph based smoothness regularization
term to extend the objective function of LDA. It has the potential to take advantages
of a large amount of unlabeled data, in order to assist the learning tasks with limited
labeled data.
2.1.2
Multi-view Subspace Learning
Multi-view learning has been receiving increasing attention in recent years. One
implicit assumption is that either view alone has sufﬁcient information about the
samples, but the learning complexity can be reduced by eliminating hypotheses from
each view if different views contribute diverse information [49]. Multi-view learning
has been widely applied to many problems, such as clustering [18, 32], classiﬁca-
tion [24, 48], semi-supervised learning [16, 25], person re-identiﬁcation [30], and
outlier detection [31].
Multi-view subspace learning projects data collected from multiple views onto
a shared subspace, which is considered as an effective strategy in many models.

2.2
Robust Representation Learning
11
The classical method, canonical correlation analysis (CCA) [20], projects two sets
of observations onto a subspace by maximizing their correlations, which has been
extended to multiple views [43]. Most recently, Ding et al. incorporated low-rank
constraints in learning common subspace for multi-view data. Kan et al. extended
the linear discriminant analysis method to multi-view setting [24] and obtained
impressive performance on image classiﬁcation. Existing multi-view subspace
learning methods have achieved promising performance in many applications, but
they could not effectively deal with data with uncertainty.
2.1.3
Dictionary Learning
As a powerful technique for learning expressive basis in sample space, dictionary
learning has become an attractive research topic during the past decade [17,
26, 40, 42, 55]. Some popular dictionary learning methods include K-SVD [1],
discriminative K-SVD [52], and projective dictionary pair learning [15].
K-SVD applies SVD decomposition after obtaining the overall reconstruction
error matrix to update each atom of the dictionary. It converges well in practice, but
it is not guaranteed to obtain a global optimal solution [1]. Moreover, K-SVD manly
focuses on the reconstruction of data, and therefore it might not be suitable for the
tasks of clustering and classiﬁcation. Discriminative K-SVD extends the K-SVD
algorithm by incorporating the classiﬁcation error into the objective function [52].
An information-theoretic dictionary learning method is presented in [42], which
selects atoms from an initial dictionary by maximizing the mutual information
measure on dictionary compactness, discrimination and reconstruction. Projective
dictionary learning is designed to reduce the computational cost of traditional
dictionary learning, by reformulating the data approximation as a linear encoding
and reconstruction process [15]. By integrating with sparse coding, many dictionary
learning methods are able to model the sparse noise contained in data, leading to
effective feature representations.
In general, both subspace and dictionary are abstracted from data, and they serve
as bases for representing original samples in a new feature space. Most subspace
learning methods have closed-form solutions, and therefore they are more efﬁcient
than the dictionary learning methods that iteratively update dictionaries.
2.2
Robust Representation Learning
By leveraging the theoretical merits of sparse representation and low-rank matrix
recovery, some robust representation learning models have been developed, such as
the subspace clustering methods and low-rank modeling.

12
2
Fundamentals of Robust Representations
2.2.1
Subspace Clustering
Subspace clustering is an effective technique which can automatically group the
samples into low-dimensional subspace. It has achieved impressive performance in
real-world applications like motion segmentation [36], face clustering [13] and digit
clustering [47]. Sparse subspace clustering (SSC) [13] enforces a sparse constraint
on the coefﬁcients. Least-square regression (LSR) [38] is very efﬁcient by using
Frobenius norm. Sparse additive subspace clustering (SASC) extends SSC to the
additive nonparametric setting [50]. Discriminative subspace clustering (DSC) [59]
incorporates discriminative information into the model. Smooth representation
(SMR) makes use of the grouping effect to further enhance the subspace clustering
performance [21]. In addition, many algorithms have been devised to reduce the
computational cost of subspace clustering [41, 45, 47].
2.2.2
Low-Rank Modeling
Low-rank modeling is becoming popular and practical recently [2], due to its
successful applications in many ﬁelds, such as data compression [6], subspace
clustering [11, 33], image processing [54, 57] and multimedia analysis [8]. Robust
PCA [6] is a representative low-rank modeling method. Given an observed and
usually corrupted sample set XO, Robust PCA decomposes XO into a low-rank,
clean sample set XL and a sparse, noisy sample set E, i.e., XO D XL C E. It shows
impressive performance in background modeling and shadow removal. One major
assumption in RPCA is that data are drawn from a single subspace. In practice, the
underlying structure of data could be multiple subspaces. Low-Rank Representation
(LRR) is designed to ﬁnd underlying structures of noisy data [33].
Given a sample set X D Œx1; x2; : : : ; xn, the objective function of LRR is as
follows
min
Z;E rank.Z/ C 1kEk0
s:t:; X D XZ C E;
(2.1)
where rank./ denotes the rank function, Z 2 RNN is the low-rank coding matrix for
X, E 2 RdN is the reconstruction error matrix, kEk0 denotes the l0 norm of matrix
E, and 1 is a trade-off parameter. The above problem is very difﬁcult to solve due
to the non-convexity of rank function and l0 norm. Usually, they can be converted
to trace norm (i.e., nuclear norm) and l1 norm, respectively, and then numerous
optimization algorithms can be applied to solve the problem.
LRR may suffer from two problems. The ﬁrst one is insufﬁcient data sampling
since LRR simply uses the data matrix itself as the basis for representation.
Second, the optimization of LRR requires multiple SVD calculations that are very
time consuming. In [34], LatLRR is proposed to solve the insufﬁcient sampling

References
13
problem by considering the effects of hidden data for representation. In addition,
active subspace [35] and Divide-Factor-Combine LRR (DFC-LRR) [46] employ
various matrix factorization algorithms to tackle the above problems. Recently, a
structured low-rank representation method [53] is proposed for image classiﬁcation.
A uniﬁed multi-scale low-rank representation approach is designed for image
segmentation [37]. The low-rank constraint can also be employed to learn robust
subspace [27, 29], to construct reliable graphs [28], to learn effective online
metrics [10], or to detect outliers in multi-view settings [31].
Low-rank modeling has been introduced to subspace learning and dictionary
learning. DLRD [39] is a low-rank dictionary learning method, which introduces
low-rank constraints on the sub-dictionaries for each class, and performs sparse
representation for face recognition. The learned dictionary in DLRD is low-rank and
discriminative, which is beneﬁcial for classiﬁcation tasks. Nevertheless, the testing
stage of DLRD is very time consuming, as it has to calculate sparse coefﬁcients for
every test sample. In [7], a low-rank method with structural incoherence is applied
to face recognition. It ﬁrst decomposes raw images into low-rank part and sparse
part, and then applies PCA on the low-rank part to obtain a subspace. Finally,
it employs sparse representation for classiﬁcation. They did not, however, learn
the low-rank representation and a discriminative subspace simultaneously. In this
manner, the low-rank part is expected to be discriminative and beneﬁt classiﬁcation
tasks. In [53], a structured low-rank representation method is presented for image
classiﬁcation. In [51], a LRR-based discriminative projection method (LRR-DP) is
proposed for feature extraction. It ﬁrst applies LRR to recover the data matrix, and
then ﬁnds a discriminative projection by designing a criterion that incorporates both
clean data and noise. In this case, LRR is regarded as a data pre-processing method,
and is performed only once to decompose sample set into two parts, the low-rank
denoised samples and associated sparse noise. However, this decomposition is not
guaranteed to be optimal for classiﬁcation, as it doesn’t make use of any class prior
information. In [56], a discriminant regularization term is incorporated into the
formulation of Robust PCA. It separately learns low-rank data representation and
subspace, which means the obtained subspace cannot be guaranteed to be optimal.
References
1. Aharon, M, Elad, M., Bruckstein A.: K-SVD: an algorithm for designing overcomplete
dictionaries for sparse representation. IEEE Trans. Signal Process. 54(11), 4311–4322 (2006)
2. Bach, F.: Consistency of trace norm minimization. J. Mach. Learn. Res. 9, 1019–1048 (2008)
3. Bellhumeur, P.N., Hespanha, J.P., Kriegeman, D.J.: Eigenfaces vs. ﬁsherfaces: recognition
using class speciﬁc linear projection. IEEE Trans. Pattern Anal. Mach. Intell. 19(7), 711–720
(1997)
4. Cai, D., He, X., Han, J.: Semi-supervised discriminant analysis. In: Proceedings of the IEEE
International Conference on Computer Vision, pp. 1–7. IEEE (2007)
5. Cai, D., He, X., Zhou, K., Han, J., Bao, H.: Locality sensitive discriminant analysis. In:
Proceedings of the 20th International Joint Conference on Artiﬁcial Intelligence, pp. 708–713
(2007)

14
2
Fundamentals of Robust Representations
6. Candès, E.J., Li, X., Ma, Y., Wright, J.: Robust principal component analysis? J. ACM 58(3),
11 (2011)
7. Chen, C., Wei, C., Wang, Y.: Low-rank matrix recovery with structural incoherence for robust
face recognition. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pp. 2618–2625 (2012)
8. Chen, C.Y., Cai, J.F., Lin, W.S., Shi, G.M.: Surveillance video coding via low-rank and sparse
decomposition. In: Proceedings of the 20th ACM International Conference on Multimedia,
pp. 713–716 (2012)
9. Christoforou, C., Haralick, R., Sajda, P., Parra, L.C.: Second-order bilinear discriminant
analysis. J. Mach. Learn. Res. 11, 665–685 (2010)
10. Cong, Y., Liu, J., Yuan, J., Luo, J.: Self-supervised online metric learning with low rank
constraint for scene categorization. IEEE Trans. Image Process. 22(8), 3179–3191 (2013)
11. Deng Y., Dai, Q., Liu, R., Zhang, Z., Hu, S.: Low-rank structure learning via nonconvex
heuristic recovery. IEEE Trans. Neural Netw. Learn. Syst. 24(3), 383–396 (2013)
12. Dyrholm, M., Christoforou, C., Parra, L.C.: Bilinear discriminant component analysis. J. Mach.
Learn. Res. 8, 1097–1111 (2007)
13. Elhamifar, E., Vidal, R.: Sparse subspace clustering. In: CVPR, pp. 2790–2797 (2009)
14. Fisher, R.A.: The statistical utilization of multiple measurements. Ann. Eugen. 8(4), 376–386
(1938)
15. Gu, S., Zhang, L., Zuo, W., Feng, X.: Projective dictionary pair learning for pattern classiﬁ-
cation. In: Proceedings of the Annual Conference on Neural Information Processing Systems,
pp. 793–801 (2014)
16. Günnemann, S., Färber, I., Rüdiger, M., Seidl, T.: SMVC: semi-supervised multi-view
clustering in subspace projections. In: Proceedings of the 20th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, pp. 253–262. ACM (2014)
17. Guo, H., Jiang, Z., Davis, L.S.: Discriminative dictionary learning with pairwise constraints.
In: Proceedings of the Asian Conference on Computer Vision, pp. 328–342. Springer (2013)
18. Guo, Y.: Convex subspace representation learning from multi-view data. In: Proceedings of the
27th AAAI Conference on Artiﬁcial Intelligence, vol. 1, p. 2 (2013)
19. He, X., Niyogi, P.: Locality preserving projections. In: Advances in Neural Information
Processing Systems, pp. 153–160 (2004)
20. Hotelling, H.: Relations between two sets of variates. Biometrika 28(3/4), 321–377 (1936)
21. Hu, H., Lin, Z., Feng, J., Zhou, J.: Smooth representation clustering. In: CVPR (2014)
22. Jing, X.-Y., Li, S., Zhang, D., Yang, J., Yang, J.-Y.: Supervised and unsupervised parallel
subspace learning for large-scale image recognition. IEEE Trans. Circuits Syst. Video Technol.
22(10), 1497–1511 (2012)
23. Jolliffe, I.T.: Principal component analysis and factor analysis. In: Principal Component
Analysis, pp. 150–166. Springer, Berlin/London (2002)
24. Kan, M., Shan, S., Zhang, H., Lao, S., Chen, X.: Multi-view discriminant analysis. IEEE Trans.
Pattern Anal. Mach. Intell. 38(1), 188–194 (2016)
25. Lan, C., Huan, J.: Reducing the unlabeled sample complexity of semi-supervised multi-view
learning. In: Proceedings of the 21th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, pp. 627–634. ACM (2015)
26. Li, L., Li, S., Fu, Y.: Learning low-rank and discriminative dictionary for image classiﬁcation.
Image Vis. Comput. 32(10), 814–823 (2014)
27. Li, S., Fu, Y.: Robust subspace discovery through supervised low-rank constraints. In:
Proceedings of the SIAM International Conference on Data Mining, pp. 163–171 (2014)
28. Li, S., Fu, Y.: Learning balanced and unbalanced graphs via low-rank coding. IEEE Trans.
Knowl. Data Eng. 27(5), 1274–1287 (2015)
29. Li, S., Fu, Y.: Learning robust and discriminative subspace with low-rank constraints. IEEE
Trans. Neural Netw. Learn. Syst. 27(11), 2160–2173 (2016)
30. Li, S., Shao, M., Fu, Y.: Cross-view projective dictionary learning for person re-identiﬁcation.
In: Proceedings of the 24th International Conference on Artiﬁcial Intelligence, pp. 2155–2161
(2015)

References
15
31. Li, S., Shao, M., Fu, Y.: Multi-view low-rank analysis for outlier detection. In: Proceedings of
the SIAM International Conference on Data Mining, pp. 748–756. SIAM (2015)
32. Li, Y., Nie, F., Huang, H., Huang, J.: Large-scale multi-view spectral clustering via bipartite
graph. In: Proceedings of the Twenty-Eighth AAAI Conference on Artiﬁcial Intelligence,
pp. 2750–2756 (2015)
33. Liu, G., Lin, Z., Yan, S., Sun, J., Yu, Y., Ma, Y.: Robust recovery of subspace structures by
low-rank representation. IEEE Trans. Pattern Anal. Mach. Intell. 35(1), 171–184 (2013)
34. Liu, G., Yan, S.: Latent low-rank representation for subspace segmentation and feature
extraction. In: Proceedings of the 13th IEEE International Conference on Computer Vision,
pp. 1615–1622 (2011)
35. Liu, G., Yan, S.: Active subspace: toward scalable low-rank learning. Neural Comput. 24(12),
3371–3394 (2012)
36. Liu, G.C., Lin, Z.C., Yu, Y.: Robust subspace segmentation by low-rank representation. In:
Proceedings of the 27th International Conference on Machine Learning, pp. 663–670 (2010)
37. Liu, X., Xu, Q., Ma, J., Jin, H., Zhang, Y.: MsLRR: a uniﬁed multiscale low-rank representation
for image segmentation. IEEE Trans. Image Process. 23(5), 2159–2167 (2014)
38. Lu, C., Min, H., Zhao, Z., Zhu, L., Huang, D., Yan, S.: Robust and efﬁcient subspace
segmentation via least squares regression. In: ECCV, pp. 347–360 (2012)
39. Ma, L., Wang, C., Xiao, B., Zhou, W.: Sparse representation for face recognition based on
discriminative low-rank dictionary learning. In: Proceedings of the 25th IEEE Conference on
Computer Vision and Pattern Recognition, pp. 2586–2593 (2012)
40. Mairal, J., Bach, F., Ponce, J.: Task-driven dictionary learning. IEEE Trans. Pattern Anal.
Mach. Intell. 34(4), 791–804 (2012)
41. Peng, X., Zhang, L., Yi, Z.: Scalable sparse subspace clustering. In: Proceedings of the 26th
IEEE Conference on Computer Vision and Pattern Recognition, pp. 430–437 (2013)
42. Qiu, Q., Patel, V.M., Chellappa, R.: Information-theoretic dictionary learning for image
classiﬁcation. IEEE Trans. Pattern Anal. Mach. Intell. 36(11), 2173–2184 (2014)
43. Rupnik, J., Shawe-Taylor, J.: Multi-view canonical correlation analysis. In: Conference on Data
Mining and Data Warehouses, pp. 1–4 (2010)
44. Sugiyama, M.: Dimensionality reduction of multimodal labeled data by local Fisher discrimi-
nant analysis. J. Mach. Learn. Res. 8, 1027–1061 (2007)
45. Talwalkar, A., Mackey, L.W., Mu, Y., Chang, S., Jordan, M.I.: Distributed low-rank subspace
segmentation. In: ICCV, pp. 3543–3550 (2013)
46. Talwalkar, A., Mackey, L.W., Mu, Y., Chang, S.-F., Jordan, M.I.: Distributed low-rank subspace
segmentation. In: International Conference on Computer Vision (ICCV), pp. 3543–3550
(2013)
47. Wang, S., Tu, B., Xu, C., Zhang, Z.: Exact subspace clustering in linear time. In: AAAI,
pp. 2113–2120 (2014)
48. Wang, W., Arora, R., Livescu, K., Bilmes, J.: On deep multi-view representation learning. In:
Proceedings of the 32nd International Conference on Machine Learning, pp. 1083–1092 (2015)
49. Xu, C., Tao, D., Xu, C.: A survey on multi-view learning. arXiv preprint arXiv:1304.5634
(2013)
50. Yuan, X., Li, P.: Sparse additive subspace clustering. In: ECCV, pp. 644–659 (2014)
51. Zhang, N., Yang, J.: Low-rank representation based discriminative projection for robust feature
extraction. Neurocomputing 111, 13–20 (2013)
52. Zhang, Q., Li, B.: Discriminative k-SVD for dictionary learning in face recognition. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2691–
2698 (2010)
53. Zhang, Y., Jiang, Z., Davis, L.S.: Learning structured low-rank representations for image
classiﬁcation. In: Proceedings of the 26th IEEE Conference on Computer Vision and Pattern
Recognition, pp. 676–683 (2013)
54. Zhang, Z., Ganesh, A., Liang, X., Ma, Y.: TILT: transform invariant low-rank textures. Int. J.
Comput. Vis. 99(1), 1–24 (2012)

16
2
Fundamentals of Robust Representations
55. Zheng, J., Jiang, Z., Phillips, P.J., Chellappa, R.: Cross-view action recognition via a transfer-
able dictionary pair. In: BMVC, vol. 1, pp. 1–11 (2012)
56. Zheng, Z., Zhang, H., Jia, J., Zhao, J., Guo, L., Fu, F., Yu, M.: Low-rank matrix recovery with
discriminant regularization. In: Proceedings of the 17th Paciﬁc-Asia Conference on Advances
in Knowledge Discovery and Data Mining II, pp. 437–448 (2013)
57. Zhou, X., Yang, C., Yu, W.: Moving object detection by detecting contiguous outliers in the
low-rank representation. IEEE Trans. Pattern Anal. Mach. Intell. 35(3), 597–610 (2013)
58. Zhu, M., Martínez, A.M.: Subclass discriminant analysis. IEEE Trans. Pattern Anal. Mach.
Intell. 28(8), 1274–1286 (2006)
59. Zografos, V., Ellis, L., Mester, R.: Discriminative subspace clustering. In: Proceedings of the
26th IEEE Conference on Computer Vision and Pattern Recognition, pp. 2107–2114 (2013)

Chapter 3
Robust Graph Construction
Abstract Graphs have been widely applied in modeling the relationships and
structures in real-world applications. Graph construction is the most critical part
in these models, while how to construct an effective graph is still an open problem.
In this chapter, we propose a novel approach to graph construction based on two
observations. First, by virtue of recent advances in low-rank subspace recovery,
the similarity between every two samples evaluated in the low-rank code space is
more robust than that in the sample space. Second, a sparse and balanced graph
can greatly increase the performance of learning tasks, such as label propagation
in graph based semi-supervised learning. The k-NN sparsiﬁcation can provide fast
solutions to constructing unbalanced sparse graphs, and b-matching constraint is a
necessary route for generating balanced graphs. These observations motivate us to
jointly learn the low-rank codes and balanced (or unbalanced) graph simultaneously.
In particular, two non-convex models are built by incorporating k-NN constraint
and b-matching constraint into the low-rank representation model, respectively. We
design a majorization-minimization augmented Lagrange multiplier (MM-ALM)
algorithm to solve the proposed models. Extensive experimental results on four
image databases demonstrate the superiority of our graphs over several state-of-the-
art graphs in data clustering, transductive and inductive semi-supervised learning.
3.1
Overview1
Graph based data mining and machine learning has attracted increasing attention
over the last decade, and many graph based learning algorithms have shown great
success in various scenarios, such as classiﬁcation, clustering, semi-supervised
learning, and social network analytics. Graph provides a very effective way of
representing underlying relationships in data. However, how to accurately measure
these relationships during graph construction is always a challenging problem. On
the other hand, sparsity in graphs is also preferred since sparse graphs have much
1This chapter is reprinted with permission from IEEE. “Learning Balanced and Unbalanced Graphs
via Low-Rank Coding”, IEEE Transactions on Knowledge and Data Engineering, 27(5):1274–
1287, 2015.
© Springer International Publishing AG 2017
S. Li, Y. Fu, Robust Representation for Data Analytics, Advanced Information
and Knowledge Processing, DOI 10.1007/978-3-319-60176-2_3
17

18
3
Robust Graph Construction
less misconnections among dissimilar data points. In this chapter, we focus on
addressing these two fundamental problems in graph construction graph construc-
tion, which are similarity metric and graph sparsiﬁcation.
Based on spectral graph theory, graphs have been widely used in data clustering.
Normalized cut is a representative graph based clustering method [41], and many
other clustering algorithms have been presented, such as the constrained graph-
based clustering [1], the bi-stochastic data similarity based clustering [45], and
the efﬁcient spectral clustering on graphs (ESCG) [33]. Another interesting and
successful application of graph is semi-supervised learning (SSL). Due to the fact
that unlabeled data are much easier to obtain than labeled ones, SSL has been
extensively studied in recent years, as it learns from both labeled and unlabeled
samples [9, 57]. Among various SSL techniques, graph based SSL (GSSL) always
achieves inspiring performance on accuracy and speed [6, 17, 19, 34, 51, 59]. Zhu
et al. [58] proposed to use the harmonic property of Gaussian random ﬁeld over the
graph for SSL. Zhou et al. [56] performed SSL with the local and global consistency
(LGC). Wang et al. [46] proposed a linear neighborhood propagation (LNP) method
that considers the linear neighborhood around each data point when constructing the
graph. He et al. [13] developed a generative model for GSSL by estimating priors
and conditional probabilities. The similarity and dissimilarity are incorporated
for graph based semi-supervised classiﬁcation [12]. Cai et al. [4] proposed a
semi-supervised discriminant analysis (SDA) method, which employs a graph
based smoothness regularization term to extend the objective function of linear
discriminant analysis (LDA) [3]. Ni et al. [35] designed a propagable graph for semi-
supervised classiﬁcation and regression. In addition, a generalized optimization
framework for graph-based semi-supervised learning was developed in [43].
Although many graph based machine learning algorithms have been proposed,
limited research has focused on how to construct effective graphs [10, 46, 47].
k-nearest neighbor (k-NN) and -neighborhood are two popular and efﬁcient graph
construction schemes. However, neither methods generate graphs that are balanced
or regular. Unbalanced (or irregular) graphs usually hinder learning performance,
because the high degree nodes may be dominant in the learning process. b-matching
method can solve this problem by learning a symmetric and balanced graph [18]. It
enforces that the degree of every node in the graph is exactly equal to a constant
b. Unfortunately, these resulting graphs are highly dependent on the similarity
function, as the similarity values (e.g., graph weights) are very sensitive to sample
variation or noise. In unsupervised and semi-supervised learning, the algorithms
usually show effective performance on data that obey the smoothness, cluster
or manifold assumptions [7, 53]. However, when the samples contain noise or
large variations, these assumptions are often violated, and therefore the traditional
similarity metrics (e.g., Gaussian function) often fail.
To address those limitations in existing methods, in this chapter, we design a
novel graph construction approach to learn unbalanced or balanced graphs via
a robust similarity metric [21, 23]. Recent advances on low-rank matrix recovery
suggest that noisy data drawn from different subspaces can be correctly recovered by

3.1
Overview
19
Similarity Metric 
Model-I: k-NN
Model-II: b-Matching
Applicatons
Input Images
X
X
Z
d
d
N
N
N
N
»
×
Complete Graph Construcon
A
C
B
D
E
A
C
B
D
E
Unbalanced Sparse Graph: 
Each node selects 2 nearest neighbors.
A
C
B
D
E
Balanced Sparse Graph: 
The degree of each node b = 2.
Iteratve 
learning
Unsupervised Clustering
ÿ Spectral clustering
Transductve Semi-supervised Classifcaton
ÿ Gaussian Harmonic Function(GHF)
Inductve Semi-supervised Learning
ÿ Semi-supervised Discriminant Analysis
A
C
B
D
E
A
C
B
D
E
Cluster 1
Cluster 2
1
1
1
2
2
Labeled data
Unlabeled data
A
C
B
D
E
Graph Regularization
Subspace
Low-Rank Representatons
Sparsifcaton
Label Propagation
Fig. 3.1 Flowchart of our framework. It contains three parts: similarity metric, graph sparsiﬁcation
and applications. (1) Low-rank representation coefﬁcients are utilized to measure the similarities
between all pairs of samples, and a complete graph can then be built, where the bold edges imply
larger graph weights. (2) k-NN and b-matching constraints are employed for graph sparsiﬁcation.
The ﬁrst two parts are learnt iteratively. (3) The applications based on our graph include clustering,
transductive and inductive semi-supervised classiﬁcation
seeking the lowest-rank encoding matrix for all samples [24–26, 28, 31, 32]. Inspired
by this observation, we propose to measure the similarities between different
samples in the low-rank encoding space, instead of original sample space. Another
observation is that a sparse graph can greatly improve the learning performance,
such as the label propagation procedure in GSSL. The k-NN sparsiﬁcation can
provide fast solutions to constructing a sparse graph, and b-matching constraint is a
necessary requirement for generating a balanced sparse graph that is more desired
for learning tasks. These observations motivate us to jointly learn the low-rank codes
and a balanced (or unbalanced) graph simultaneously. Figure 3.1 illustrates our
framework. In particular, two models are built by incorporating k-NN constraint
and b-matching constraint, respectively.
Instead of using the biased estimators adopted in existing low-rank matrix
recovery methods, we ﬁrst relax the matrix rank and l0 norm in our models by two
unbiased estimators, matrix -norm and minimax concave penalty (MCP) norm,
respectively, and then design a majorization-minimization augmented Lagrange
multiplier (MM-ALM) algorithm to solve the proposed non-convex models. We
show that this novel optimization algorithm can better recover the low-rank sub-
space structures that results in robust similarities during graph construction. After
applying a graph re-weighting strategy, our graph can be used in many unsupervised
and semi-supervised learning scenarios including data clustering, transductive and
inductive semi-supervised classiﬁcation. Experimental results on the Extended
YaleB, PIE, ORL and USPS image databases demonstrate the effectiveness of our
graph, compared with several state-of-the-art graphs.

20
3
Robust Graph Construction
In summary, our contributions include:
•
We have proposed a new similarity metric based on low-rank subspace recovery.
The k-NN and b-matching constraints are incorporated into the low-rank learning
models to learn unbalanced or balanced graphs, respectively.
•
We have relaxed the matrix rank and l0 norm in low-rank representation model
by using the matrix -norm and matrix MCP norm, respectively.
•
We have designed a non-convex optimization algorithm to solve the proposed
models, and have shown its convergence properties.
•
We have evaluated the performance of our model on data clustering, transductive
semi-supervised classiﬁcation and inductive semi-supervised learning.2
3.2
Existing Graph Construction Methods
Constructing an effective graph is the most important component in graph based
learning algorithms. As we discussed above, there has been some research speciﬁ-
cally that targeted graph construction. In this section, we give a brief review of these
related works sorted by unbalanced graph and balanced graph, sparse representation
based graphs, and low-rank learning based graphs.
3.2.1
Unbalanced Graphs and Balanced Graph
The k-NN graph and -neighborhood graph are two representative unbalanced
graphs. In the construction procedures, kernel function k./ (e.g., Gaussian kernel)
is usually used to estimate the similarity between samples. Based on the similarity
matrix, k signiﬁcant neighbors for each node are greedily selected to construct
a k-NN graph [41]. However, k-NN graph is unbalanced, as it always requires
a symmetrization process after selecting k neighbors. An unbalanced graph may
lead to a performance degradation in learning algorithms. In the -neighborhood
graph, each node is connected to those nodes within the distance . The linear
neighborhood propagation (LNP) method improves the similarity measurement in
k-NN graph and -neighborhood graph, by utilizing the manifold information [46].
But, both -neighborhood graph and LNP based graph are still unbalanced.
To address this unbalanced problem, Jebara et al. proposed a b-matching method
to construct a balanced graph [18]. They employed a b-matching constraint during
graph construction, and therefore the degree of every node is exactly equal to the
constant b. This method simultaneously ensures both symmetry and balance of the
2Semi-supervised learning can be either transductive or inductive. Transductive model only works
on the labeled and unlabeled training samples, and it cannot deal with unseen data. Inductive model
can naturally handle unseen data [55, 57].

3.2
Existing Graph Construction Methods
21
graph in learning procedures. However, the performance of a k-NN graph and a b-
matching graph is highly dependent on the similarity metric. If the metric is sensitive
to sample variation or noise in the data, the performance greatly reduced.
3.2.2
Sparse Representation Based Graphs
Another class of graph construction methods focuses on weighting the graphs. Spar-
sity could successfully recover signals in noisy scenarios [49], and several graph
construction methods in which the sparse representation coefﬁcients are employed
as graph weights have been proposed [14, 36, 50, 52]. Yan et al. proposed an l1 graph,
which adopts sparse representation coefﬁcients as the weights on a graph [50]. l1
graph is constructed in a parameter-free manner, since the adjacency structure and
graph weights are determined by the l1 optimization algorithm automatically. Cheng
et al. proposed a sparsity induced similarity (SIS) measure for label propagation in
GSSL [8], which utilizes the coefﬁcients of sparse decompositions. Furthermore, He
et al. presented a non-negative sparse probability graph (SPG) to further improve the
classiﬁcation performance of SSL [14]. Nie et al. designed an iterative algorithm to
solve the l1 norm of spectral embedding minimization problem for semi-supervised
classiﬁcation [36]. Recently, the neighborhood graph construction problem was also
modeled as a sparse coding problem with the locality constraint [52].
However, the reason for using sparse representation coefﬁcients as graph weights
is not quite clear. Especially when the labeled samples are very limited, sparse
coefﬁcients are very sensitive to the variation in labeled samples, and then the label
propagation results would be affected negatively. In addition, all these sparse graphs
are not balanced.
3.2.3
Low-Rank Learning Based Graphs
Low-rank matrix recovery has attracted increasing attention in recent years [2],
and it has been successfully applied to many areas [5, 20, 22, 27, 32]. Low-Rank
Representation (LRR) [31, 32] was proposed to recover multiple subspaces in
the presence of noise, and it has shown considerable effectiveness in subspace
segmentation. To capture the global structure of sample set X, LRR determines
a representation Z by minimizing the rank of Z min
Z
rank.Z/ with a constraint
X D XZ. This problem is NP-hard, however, it can be transformed into an equivalent
convex optimization problem using nuclear norm k  k instead of rank./ function.
One drawback of LRR is that the nuclear norm and l2;1 norm are biased estimators
since they over-penalizes large singular values and large entries, respectively [48].
In this chapter, we present an effective solution to this problem by introducing the
matrix -norm, which is a non-convex relaxation of matrix rank.

22
3
Robust Graph Construction
Recently, some graph construction methods that use low-rank representation
coefﬁcients as graph weights have been developed [38–40, 54, 60]. Among them,
non-negative low-rank and sparse (NNLRS) graph [60] is the most relevant work
to ours. NNLRS enforces low-rankness and sparsity simultaneously, and employs
the low-rank representation coefﬁcients as graph weights. There are several key
differences between NNLRS and our approach. First, NNLRS directly utilizes low-
rank representation coefﬁcients as graph weights, our approach, on the other hand,
employs low-rank coefﬁcients of each pair of samples to calculate their similarity.
Second, the optimization techniques are different since we build non-convex
optimization models. Third, our approach jointly learns a sparse and balanced graph
with b-matching constraint, while NNLRS graph is unbalanced, which is undesired
for semi-supervised classiﬁcation.
3.3
Low-Rank Coding Based Unbalanced
Graph Construction
In this section, we ﬁrst describe the motivation of designing robust similarity metric
using low-rank codings. We then build an optimization model to learn the similarity
matrix, and develop a non-convex optimization algorithm to solve this model.
3.3.1
Motivation
Given a sample set X D Œx1; x2; : : : ; xn (each column is a sample, xi 2 Rd), graph
construction models aim at building a weighted graph G. Typically, there are three
steps: similarity measurement between each pair of samples, sparsiﬁcation of the
graph, and graph re-weighting. The output graph can be expressed as G D S ı W,
where S is a binary matrix whose elements indicate the edges in graph, W is a
similarity matrix or other user-deﬁned weighting matrices, and ı is the Hadamard
product, i.e., Gij D .S ı W/ij D SijWij.
We focus on the ﬁrst major step of graph construction: similarity measurement.
Many current similarity metrics, such as Gaussian function, are sensitive to noise or
large intra-class variations. Inspired by the low-rank subspace recovery theory, we
propose to estimate similarity in the low-rank code space to address this problem.
Let Z denote the unknown coefﬁcient matrix for sample set X, low-rank method
learns a matrix Z which has the lowest rank.Z/ and satisﬁes the constraint X D AZC
E, where A is an over-complete dictionary and E is the sparse noise component [31].
The i-th column vector in Z is the low-rank code vector for xi. Due to the fact that Z
can correctly recover multiple subspaces and its low-rankness, the low-rank codes in
Z belonging to one subspace (i.e., samples in the same class or same cluster) should
be highly correlated. Meanwhile, low-rank coefﬁcients are very robust to different
kinds of noises [31]. Thus, it is reasonable to employ low-rank codes to estimate the
similarity between all pairs of samples.

3.3
Low-Rank Coding Based Unbalanced Graph Construction
23
High dimensional 
Low dimensional 
Low-rank Code Space
d
1
2
3
4
5
1
2
3
Sample Space
Digit 2 in USPS dataset
Digit 0 in USPS dataset
Similarity
Original space Low-rank coding space
Intra-class (digit 2)
0.78
0.92
Inter-class (digits 0 and 2)
0.75, 0.32
0.24, 0.10
Fig. 3.2 Similarity measurements in the original space and low-rank code space
To clearly illustrate our new similarity metric, Fig. 3.2 compares two similarity
metrics of digits images in the original space and low-rank code space. It shows
that, in case of large intra-class variation or large inter-class correlation , similarity
values calculated in original space may be unreliable. However, the noise-insensitive
low-rank codes can correctly recover the subspace structures of multiple classes,
and low-rankness means that codes belonging to the same class should have high
correlations. Thus, the similarity metric in low-rank space obtains better results as
shown in Fig. 3.2.
Calculating the inner product of every pair of low-rank representation coefﬁcient
vectors is a straightforward way to obtain similarity value. After obtaining a fully
connected similarity matrix OW, where OWi;j D j.Z>Z/i;jj, we should perform the
second step of graph construction: sparsiﬁcation. k-NN is a simple yet effective
strategy for sparsifying a graph. Thus, we propose to build an unbalanced sparse
graph construction model (Model-I) by integrating the ideas of low-rank coding
and k-NN sparsiﬁcation.
3.3.2
Problem Formulation
Based on the above observations, we propose an optimization model to learn
low-rank codes and sparsify the graph using k-NN simultaneously. The objective
function of our Model-I is:
min
Z;E;S rank.Z/ C 1 kEk0  2
nP
i;jD1
Sij.Z>Z/ij
s:t: X D AZ C E;
nP
jD1
Sij D k; Sii D 0;
(3.1)

24
3
Robust Graph Construction
where A is the dictionary with the size of n  a, Z is the low-rank coefﬁcient matrix,
E is a sparse noise matrix, 1 and 2 are trade-off parameters to balance the effects
of other terms, S is a binary k-NN graph and k is the number of nearest neighbors.
In Eq. (3.1), the ﬁrst two terms rank.Z/ C 1 kEk0 denote the low-rank represen-
tation of noisy data matrix X. The last term indicates the k-NN sparsiﬁcation, which
means k nearest neighbors are selected for each node.
Equation (3.1) is a variant of rank minimization problem. Generally, like LRR,
this kind of problem can be relaxed by using trace norm (nuclear norm) and l1
norm (or l2;1 norm), and then solved by some convex optimization tools, such
as inexact augment Lagrange multiplier (ALM) algorithms [29] and linearized
alternating direction method with adaptive penalty (LADMAP) [30]. However,
Wang et al. noted that the nuclear norm and l1 norm are actually biased estimators
since they over-penalize large singular values and large entries [48]. They devised
a matrix minimax concave penalty (MCP) norm and a matrix -norm to remodel
a non-convex version of the matrix recovery problem that is originally deﬁned in
RPCA. Inspired by the effectiveness of this non-convex relaxation, we propose to
reformulate the low-rank representation problem by using matrix MCP norm and
matrix -norm.
The matrix MCP norm is deﬁned as:
M;.A/ D
X
i;j
;.Ai;j/;
(3.2)
where
;.t/ D 
t
Z
0
Œ1  x
Cdx D
8
<
:
2=2;
if jtj  
jtj  t2
2 ; otherwise:
ŒzC D max.z; 0/. Here, we choose  D 1, and denote M.A/ D M1;.A/ for
simplicity.
The matrix -norm is deﬁned as [48]:
kAk D
rP
iD1
i.A/
R
0
.1  u
r /Cdu D
rP
iD1
1;.i.A// D M..A//;
 > 1;
(3.3)
where .A/ D .1.A/;    ; r.A//> denotes a function from Rmn to Rr
C, r D
min.m; n/. The matrix -norm is non-convex w.r.t A.
Then, problem (3.1) can be reformulated as:
min
Z;E;S kZk1 C 1M2.E/  21>
n .S ı .Z>Z//1n
s:t: X D AZ C E;
nP
jD1
Sij D k; Sii D 0;
(3.4)
where ı is the Hadamard product.

3.3
Low-Rank Coding Based Unbalanced Graph Construction
25
3.3.3
Optimization
In this subsection, we relax the objective function in Eq. (3.4) and design a
majorization-minimization (MM)-ALM algorithm to solve this problem.
The last term in Eq. (3.4), 1T
n.Sı.Z>Z//1n, makes it difﬁcult to solve the objective
function. Fortunately, we can relax it to 1T
n.Sı.Z>J//1n with a new constraint Z D J.
Then (3.4) can be rewritten as:
min
Z;E;S;J kJk1 C 1M2.E/  21>
n .S ı .Z>J//1n
s:t: X D AZ C E;
nP
jD1
Sij D k; Sii D 0; Z D J:
(3.5)
We ﬁrst introduce the generalized singular value shrinkage operator S;	 and
generalized shrinkage operator D;W [48]:
S;	 D UXD;	.˙X/.V/>
X ;
(3.6)
ŒD;W.A/ij D sgn.Aij/.jAij  Wijj/C;
(3.7)
where ˙ and 	 are non-negative matrices.
The MM-ALM algorithm consists of an outer loop and an inner loop. In each
iteration, the outer loop replaces the non-convex problem by its locally linear
approximation (LLA) to form a weighted convex problem, while an inner loop is
an inexact ALM algorithm.
In the outer loop, we reformulate the objective function as follows. Since
the objective function in Eq. (3.5) is concave w.r.t. ..J/; jEj; jZj; jSj/, we can
approximate it by the LLA, and obtain the following objective function:
min
Z;E;S;J f.J; E/ D Q1..J/j.J/old/ C 1Q2.EjEold/
21>
n .S ı .Z>J//1n
s:t: X D AZ C E;
n
X
jD1
Sij D k; Sii D 0; Z D J;
(3.8)
where
Q.AjAold/ D M.Aold/ C P
i;j
.1  jAold
ij j=/C.jAijj C jAold
ij j/:
is the LLA of M.A/ given Aold.
In the inner loop, we utilize the inexact ALM algorithm to solve Eq. (3.8) by
alternately updating different sets of variables. First we update the variables J; Z
and E when S is ﬁxed, and then update S when others are ﬁxed. The augmented
Lagrangian function is:

26
3
Robust Graph Construction
L D Q1..J/j.J/old/ C 1Q2.EjEold/
21>
n .S ı .Z>J//1nC < X  AZ  E; Y1 >
C < Z  J; Y2 > C <
nP
j
Sij  k; Y3 >
C 
2 .kX  AZ  Ek2
F C kZ  Jk2
F/;
(3.9)
where Y1; Y2 and Y3 are Lagrange multipliers and 
 > 0 is a penalty parameter.
In particular, we alternately update the variables J, Z, E and S in the kC1 iteration
as follows:
JkC1 D S1=
;	.Zk C 2Sk ı Z C Y2

/;
(3.10)
ZkC1 D .In C A>A/1.A>X  A>Ek C JkC1 C .A>Y1  Y2 C 2Sk ı JkC1/=
/;
(3.11)
EkC1 D D=
;W.A  AZkC1 C Y1=
/;
(3.12)
SkC1 D arg min
Sk  2

k
1>
n .Sk ı .Z>
kC1JkC1//1n:
(3.13)
s:t:
n
X
j
.Sk/ij D k; .Sk/ii D 0:
Equation (3.13) can be solved by a standard k-NN solver. The details of the
algorithm is outlined in Algorithm 3.1. In this chapter, we use sample set X itself as
dictionary, that is, A D X.
Lemma 3.1 When S is ﬁxed, the objective function values of (3.10) obey
f.J; E; Z/  Q1..J/j.J/old/ C 1Q2.EjEold/  21>
n .S ı .Z>J//1n
 Q1..Jold/j.J/old/ C 1Q2.EoldjEold/  21>
n .S ı .Z>oldJold//1n
D f.Jold; Eold; Zold/:
This lemma can be proved using the Proposition 4 in [48]. It demonstrates the
local convergence property of our algorithm.

3.3
Low-Rank Coding Based Unbalanced Graph Construction
27
3.3.4
Complexity Analysis
Our optimization algorithm contains two parts, LLA and inexact ALM. As sug-
gested in [48], we adopt the one-step LLA strategy, which runs the outer loop
in Algorithm 3.1 only once, to alleviate the computational cost. In particular, the
solutions of LRR are used for initializing the one-step LLA.
The computational complexity of Algorithm 3.1 is mainly dependent on the inner
loop, where the most time-consuming parts are Steps 6–8. In Step 6, the SVT
operator requires singular value decomposition of matrices of size n  n, which
costs O.n3/, where n is the total number of samples. In Step 7 and Step 8, the
matrix inversion and matrix multiplication also cost O.n3/. Assume that the number
of iterations in Algorithm 3.1 is l, the overall computational complexity of this
algorithm would be O.ln3/. Thus, the scalability of Algorithm 3.1 is determined
by the sample size n, like many other low-rank learning methods. [44] presents a
distributed solution to improve the scalability, which would be adopted to accelerate
our algorithm in the future work. We will show the running time of our algorithm
and its competitors in the experiments.
Algorithm 3.1 Solving problem (3.5) via MM-ALM
Input: data matrix X, dictionary A, parameter 1, 2,
Z0 2 Ran, J0 2 Ran, E0 2 Rdn, S0 2 Rnn,
Y1 2 Rdn, Y2 2 Ran, 
0 D 103, 
max D 105,
 D 1:3, k D 0, j D 0,  D 105
Output: S; Z; E
1: while not converged do
2: Initialize 
0 D 103; Y1; Y2; k D 0;
3: 	 D Diag.1n  .Jj/=1/C;
4: W D .1m1>
n  jSjj=2/C;
5: while not converged do
6:
Update JjC1
kC1 using (3.10), given others ﬁxed;
7:
Update ZjC1
kC1 using (3.11), given others ﬁxed;
8:
Update EjC1
kC1 using (3.12), given others ﬁxed;
9:
Update SjC1
kC1 using (3.13), given others ﬁxed;
10:
Symmetrize SjC1
kC1 D max.SjC1
kC1; SjC1T
kC1 /;
11:
Update the multipliers Y1 and Y2
Y1 D Y1 C 
k.X  AZjC1
kC1  EjC1
kC1/;
Y2 D Y2 C 
k.JjC1
kC1  ZjC1
kC1/:
12:
Update the parameter 
kC1 by

kC1 D min.
k; 
max/
13:
Check the convergence conditions
kX  AZ  Ek1 <  and kJ  Zk1 < :
14:
k D k C 1;
15: end while
16: j D j C 1;
17: end while

28
3
Robust Graph Construction
3.3.5
Discussions
Traditionally, graph construction methods either utilize various similarity functions
to estimate the weights [41], or leverage on the manifold information [46]. In
our approach, we propose a new similarity metric by taking advantage of the
subspace structure of sample set. The experimental results will demonstrate that
exploiting such subspace structures in graph construction would greatly beneﬁt the
unsupervised and semi-supervised learning tasks.
Moreover, we unify two steps of graph construction, similarity measurement and
sparsiﬁcation, into a single framework. These two steps can guide each other in
an iterative learning manner. Existing graph construction methods either follow the
two-step strategy (e.g., k-NN graph, -neighborhood graph), or estimate the weights
and sparsify the graph at once (e.g., l1 graph [50], LRR graph [31]). In our approach,
we jointly learn the graph weights and sparsify the graph. Experiments will show
the beneﬁts of our pipeline.
3.4
Low-Rank Coding Based Balanced Graph Construction
In this section, we explain why the b-matching constraint is necessary, and then use
it to replace the k-NN sparsiﬁcation in our Model-I. We build a joint optimization
model Model-II to learn the similarity matrix and the balanced graph simultane-
ously.
3.4.1
Motivation and Formulation
Sparsity of graph is an important requirement for ensuring the effectiveness of learn-
ing algorithms, since sparse graphs have much less misconnections among dissim-
ilar samples. For example, in graph based semi-supervised classiﬁcation, sparsity
helps improve the classiﬁcation accuracy and computational performance [47]. As
we discussed above, k-NN and -neighborhood are two commonly used strategies to
generate a sparse graph, and l1 graphs also meet the sparsity requirement in nature.
However, all these graphs are unbalanced, which hinders learning performance,
since some high degree nodes may dominate the learning results. To address this
problem, we incorporate the b-matching constraint that is designed for learning a
balanced graph .
In particular, we replace the k-NN sparsiﬁcation with b-matching constraint in
Model-I, and then build Model-II as:
min
Z;E;S;J kJk1 C 1M2.E/  21>
n .S ı .Z>J//1n
s:t: X D AZ C E;
nP
jD1
Sij D b; Sij D Sji; Z D J;
(3.14)

3.5
Learning with Graphs
29
where A is the dictionary, Z is the low-rank coefﬁcient matrix, E is a sparse noise
matrix, 1 and 2 are trade-off parameters to balance the effects of other terms, S is
a binary balanced graph and b is the constraint on the degree of each node.
Equation (3.14) differs from Eq. (3.5) in the constraints. The last term in
Eq. (3.14) .21>
n .S ı .Z>J//1n/ and the b-matching constraint Pn
jD1 Sij D b
indicate that each node should have a degree of b, while the total similarity value
should be maximized.
3.4.2
Optimization
In this subsection, we extend the optimization algorithm developed in Sect. 3.3.3 to
solve Eq. (3.14).
The outer loop for Model-II is the same as that for Model-I, and the inner loop
can also be solved by inexact ALM algorithm. The augmented Lagrange function is:
L D Q1..J/j.J/old/ C 1Q2.EjEold/
21>
n .S ı .Z>J//1nC < X  AZ  E; Y1 >
C < J  Z; Y2 > C <
n
X
j
Sij  b; Y3 >
(3.15)
C
2 .kX  AZ  Ek2
F C kJ  Zk2
F/;
where Y1; Y2 and Y3 are Lagrange multipliers and 
 > 0 is a penalty parameter.
In particular, we alternately update the variables J, Z, E and S in each iteration.
The solvers for J, Z and E have been shown in Eqs. (3.10), (3.11), and (3.12). The
subproblem for solving S is:
SkC1 D arg min
Sk  2

k 1>
n .Sk ı .Z>
kC1JkC1//1n:
s:t:
nP
j
.Sk/ij D b; .Sk/ij D .Sk/ji:
(3.16)
Equation (3.16) can be solved by a fast b-matching algorithm that has been
recently proposed in [15]. The time complexity of b-matching solver is O.n2:5/ [15].
One can modify Algorithm 3.1 to solve (3.14), by replacing (3.13) with (3.16) in the
9th step and removing the 10th step.
3.5
Learning with Graphs
In this section, we describe three representative applications based on our graphs,
including clustering, transductive and inductive semi-supervised classiﬁcation.

30
3
Robust Graph Construction
3.5.1
Graph Based Clustering
We extend a classical spectral clustering method, normalized cut (NCut) [41], on
the basis of our graph. According to Sects. 3.3 and 3.4, we can learn binary graphs
S and low-rank representation coefﬁcients Z. As we discussed above, j.Z>Z/i;jj is
a good choice for measuring the similarity between a pair of samples xi and xj.
Thus, we can also re-weight the graph S using it, and ﬁnally obtain a non-negative,
sparse, symmetric, weighted and balanced graph W, where Wi;j D Si;j ı j.Z>Z/i;jj.
In Model-I, S is unbalanced and thus W is also unbalanced, but S and W obtained
by Model-II are balanced.
Let L denote the Laplacian matrix, L D D  W, and D is the diagonal matrix
whose diagonal entry Di;i is the sum of the entries of column i in matrix W. The
normalized cut criterion can be formulated as the following trace maximization
problem [41]:
arg max
Y
1
K trace.Y>WY/;
(3.17)
where Y D R.R>DR/1=2, R is an indicator matrix for the partitions, and K is the
number of clusters.
Let OY D D1=2Y, this problem is rewritten as arg max trace.OYD1=2WD1=2 OY/.
A well known solution to this problem is achieved by setting OY to be the top K
eigenvectors of the matrix D1=2WD1=2.
3.5.2
Transductive Semi-supervised Classiﬁcation
Given a sample set X that contains a set of l labeled samples Xl and a set of u
unlabeled samples Xu, X D Œx1;    ; xl; xlC1;    ; xlCu D ŒXl; Xu; n D l C u.
The aim of graph based semi-supervised classiﬁcation is to infer the missing
labels of Xu with the aid of labeled sample set Xl. We use the same graph re-
weighting strategy as shown in Sect. 3.5.1, and obtain the weighted graph W, where
Wi;j D Si;j ı j.Z>Z/i;jj.
Our graph W can be easily combined with a representative label propagation
method, Gaussian harmonic function (GHF) [58]. We denote Y D ŒYl; Yu, where
Yl contains the probability vectors for the labeled samples and Yu for the unlabeled
samples.
The predicted probability vectors can then be obtained for unlabeled samples by:
Yu D YlLluL1
uu ;
(3.18)
where L is the Laplacian matrix.

3.6
Experiments
31
Algorithm 3.2 Transductive semi-supervised learning
Input: data matrix X D ŒXl; Xu D Œx1; x2; : : : ; xn, dictionary A D X,
parameters 1 and 2
Output: Yu
1. Normalize all the samples xi to unit-norm, xi D xi= kxik.
2. Solve problems (3.4) or (3.14) according to Sects. 3.3 and 3.4,
and obtain optimal solutions S and Z.
3. Graph re-weighting: Wi;j D Si;j ı j.Z>Z/i;jj.
4. Calculate probability vectors Yu for unlabeled samples Xu using (3.18).
These steps are summarized in Algorithm 3.2. Same as [32], we use sample
matrix, X, as the dictionary. Besides GHF, our graph can also be combined with
other label propagation schemes such as local and global consistency (LGC) [56]
and linear neighborhood propagation (LNP) [46].
3.5.3
Inductive Semi-supervised Classiﬁcation
We adopt the semi-supervised discriminant analysis (SDA) method [4] for inductive
semi-supervised classiﬁcation. SDA incorporates a graph based smoothness regular-
ization term to extend the objective function of linear discriminant analysis (LDA).
The objective function of SDA is:
arg max
a
a>SBa
aT.ST C ˛XLX>/a;
(3.19)
where X is the training sample set, a is a transformation matrix, SB and ST are the
between-class and total scatter matrices deﬁned in LDA [3], L D D  W is the
Laplacian matrix, and W is the weighted graph learned by our models.
The optimal solution a that maximizes the objective function is given by the
maximum eigenvalue solution to the generalized eigenvalue problem:
SBa D .ST C ˛XLX>/a:
(3.20)
Finally, we can project all samples onto the learned subspace a, and classify the
test samples using nearest neighbor classiﬁer.
3.6
Experiments
In this section, we ﬁrst introduce four databases used to evaluate our methods,
and then compare our graphs with some state-of-the-art graphs in data clustering,
transductive and inductive semi-supervised learning.

32
3
Robust Graph Construction
3.6.1
Databases and Settings
In our experiments four image databases are used: Extended YaleB [11], PIE [42],
ORL [37] and USPS [16].
Extended YaleB Face Database. This database has 38 subjects and approximately
64 images under different illuminations per subject. We use the images of the ﬁrst
15 subjects, which we crop and resize to the size of 32  32 pixels.
PIE Face Database. The PIE face database consists of 41368 images of 68
subjects. Each subject is shown in different poses, illuminations and expressions.
We use the ﬁrst 15 subjects, ﬁve near front poses (C05, C07, C09, C27, C29), and
all the different illuminations and expressions. Each image is cropped and resized
to the size of 32  32 pixels.
ORL Face Database. The ORL face database contains 400 images of 40 individu-
als. These images were captured at different times, under varying lighting conditions
and showing different facial expressions. We crop and resize each image to 32  32
pixels.
USPS Digit Database. The USPS digit database consists of 9298 handwritten digit
images of 10 numbers (0–9). The size of each image is 16  16 pixels.
In the experiments, we randomly select 50 images of every class in the PIE and
YaleB databases, 100 images from each class in the USPS database, and use all the
images of ORL database. We implement the proposed models in Matlab, and carry
out the experiments on an Intel R
 Xeon R
 3.07 GHz processor with 8 GB memory.
Speciﬁcally, we compare with the following graphs:
(1) k-NN graph. In this graph, two samples are connected if one is among the
k nearest neighbors of the other. The k-NN-graph is constructed under two
conditions. In k-NN-I, the number of nearest neighbors is set to 5; and in k-NN-
II, this number is set to 8. We use the Gaussian kernel to re-weight the edges,
and the parameter ı is adjusted to achieve the best performance on different
databases.
(2) b-matching (BM) graph. We follow the algorithms described in [18], and
construct a weighted and balanced graph. The parameter b is selected to achieve
the best results.
(3) l1 graph. We construct the l1 graph according to [50], and also symmetrize this
graph.
(4) Sparse probability graph (SPG). Following the algorithms in [14], we
construct the non-negative sparse probability graph (SPG).
(5) LRR graph. In accordance with [31], we construct the LRR graph and
symmetrize it. We adopt the same parameters as described in [31].
(6) NNLRS graph. We construct the non-negative sparse and low-rank graph
according to [60]. We also symmetrize this graph. The parameters are set as
described in [60].

3.6
Experiments
33
(7) LRCB graph. We also compare with our previous work, LRCB graph [21].
Two parameters 1 and 2 are separately set as 2 and 0.03 to obtain the best
performance.
3.6.2
Spectral Clustering with Graph
In this subsection, we evaluate the performance of k-NN graph, BM graph [18],
l1 graph [50], LRR graph [32], LRCB graph [21], and our two models on spectral
clustering.
We utilize two metrics to evaluate the clustering performance, which are accuracy
(AC) and normalized mutual information (NMI). Assume that Y is the clustering
result (i.e., label vector) and NY is the ground truth. The AC is deﬁned as:
AC D
NP
jD1
ı.NY.j/; MapY;NY.j//
N
;
(3.21)
where N is the total number of samples, ı.x; y/ equals to 1 if and only if x D y.
MapY;NY denotes the best mapping function that permutes Y to match NY. Here we use
the Hungarian algorithm to ﬁnd the best matching.
The NMI is deﬁned as:
NMI.X; Y/ D
MI.X; Y/
max.H.X/; H.Y//;
(3.22)
where X and Y are two index sets related to Y and NY, respectively. MI.X; Y/
denotes the mutual information between X and Y, MI.X; Y/ D P
y2Y
P
x2X p.x; y/
log2. p.x;y/
p.x/p.y//, p.x/ and p.y/ represent the marginal probability distribution functions
of X and Y, respectively. p.x; y/ is the joint probability distribution function. H.X/
and H.Y/ denote the entropies of p.x/ and p.y/. We can observe that the NMI varies
between 0 and 1. Moreover, NMI does not require the matching of X and Y in
advance.
Figure 3.3 illustrates that our approach converges quickly. The relative error
is calculated by kX  AZ  EkF=kXkF. In our two models, there are two major
parameters, 1 and 2. To choose proper values for them, we adopt a coarse-to-
ﬁne strategy to tune the parameters. We ﬁrst evaluate the parameter sensitivity on
our Model-II. Figure 3.4 shows the AC and NMI of our model under different
settings of 1 and 2, respectively. Here, 1 is used to handle the corruptions or
large noise in the samples, while 2 is used to balance low-rank approximation and
sparsiﬁcation constraint. Since the images in Extended YaleB database are captured
in a relatively controlled environment, there’s not much corruptions and our graph is
not sensitive to 1 over a wide range. Figure 3.4 also shows that our graph achieves

34
3
Robust Graph Construction
0
10
20
30
40
50
60
70
0
1
2
3
4
5
6
Number of Iterations
Relative Error
Fig. 3.3 Convergence curve of our approach on PIE database. ( D 1:2, 
 D 103 and  D
105)
0
0.1 0.5 1
1.5 2
2.5 3
3.5 4
4.5 5
5.5
0
0.01
0.015
0.02
0.025
0.03
0.035
0.04
0.045
0.05
0.055
0.06
60
65
70
75
80
85
90
λ1
λ2
68
70
72
74
76
78
80
82
84
0
0.10.51
1.52
2.53
3.54
4.55
5.5
0
0.01
0.015
0.02
0.025
0.03
0.035
0.04
0.045
0.05
0.055
0.06
0.4
0.5
0.6
0.7
0.8
λ1
λ2
0.55
0.6
0.65
0.7
0.75
(b)
(a)
Fig. 3.4 (a) AC and (b) NMI of our graph under different values of 1 and 2 on Extended YaleB
face database
stable performance when 2 is varied from 0.03 to 0.06, and 1 does not inﬂuence
the results signiﬁcantly. On other three databases, we obtain similar results. Thus,
for all the four databases, 1 and 2 are set to 2 and 0.04, respectively.
In our Model-II, the parameter b is set as 20 on four databases, as a relative larger
b can lead to stable structure of graph in unsupervised learning scenario. If some
labeled samples are available, b is not necessary to be set to large values. In the semi-
supervised classiﬁcation experiments shown in the next subsection, b is set to 10.
We repeat each method 10 times. Tables 3.1 and 3.2 show the average accuracies
and average NMI (with standard deviations) of all compared graphs on four

3.6
Experiments
35
Table 3.1 Average accuracies (with standard deviations) of different graphs for clustering on four
databases
Methods
YaleB
PIE
ORL
USPS
k-NN-I
51.03 ˙ 0.88
65.25 ˙ 6.14
64.07 ˙ 2.11
77.32 ˙ 6.90
k-NN-II
64.17 ˙ 3.47
74.07 ˙ 2.78
81.75 ˙ 3.05
75.98 ˙ 7.89
BM [18]
67.19 ˙ 4.91
74.27 ˙ 2.33
83.65 ˙ 1.59
78.56 ˙ 2.40
l1-graph [50]
81.40 ˙ 3.29
83.35 ˙ 3.24
71.70 ˙ 3.99
80.24 ˙ 4.56
LRR [32]
74.28 ˙ 5.87
73.41 ˙ 3.47
77.50 ˙ 2.59
81.08 ˙ 3.00
LRCB [21]
82.90 ˙ 2.08
85.25 ˙ 2.71
85.62 ˙ 2.25
79.71 ˙ 4.04
Ours-I
84.13 ˙ 4.17
85.86 ˙ 1.12
86.22 ˙ 3.22
80.45 ˙ 3.23
Ours-II
85.45 ˙ 2.26
87.09 ˙ 2.83
88.75 ˙ 2.08
81.64 ˙ 3.51
Table 3.2 Average normalized mutual information (NMI) of different graphs with standard
deviations for clustering on four databases
Methods
YaleB
PIE
ORL
USPS
k-NN-I
0.5988 ˙ 0.0463
0.3269 ˙ 0.0260
0.8127 ˙ 0.0002
0.6972 ˙ 0.0265
k-NN-II
0.4231 ˙ 0.0179
0.2636 ˙ 0.0152
0.7990 ˙ 0.0030
0.7100 ˙ 0.0191
BM [18]
0.4516 ˙ 0.0170
0.5127 ˙ 0.0185
0.8032 ˙ 0.0146
0.7020 ˙ 0.0169
l1-graph [50]
0.5216 ˙ 0.0167
0.4958 ˙ 0.0150
0.7814 ˙ .00294
0.6272 ˙ 0.0249
LRR [31]
0.7122 ˙ 0.0078
0.6060 ˙ 0.0311
0.7799 ˙ 0.0259
0.6693 ˙ 0.0048
LRCB [21]
0.8541 ˙ 0.0104
0.6463 ˙ 0.0078
0.8126 ˙ 0.0125
0.7083 ˙ 0.0155
Ours-I
0.8716 ˙ 0.01387
0.6514 ˙ 0.0146
0.8424 ˙ 0.0216
0.7069 ˙ 0.0095
Ours-II
0.8673 ˙ 0.0166
0.6742 ˙ 0.0107
0.8751 ˙ 0.0094
0.7154 ˙ 0.0102
databases. For simplicity, our Model-I and Model-II are separately denoted as
Ours-I and Ours-II. We can observe that our two models always achieve better
performance than other compared methods. In particular, our balanced graph (Ours-
II) usually outperforms the unbalanced one (Ours-I), which further demonstrates the
merits of the balanced property.
3.6.3
Semi-supervised Classiﬁcation with Graph
Transductive Setting We ﬁrst normalize all the images to be unit-norm as shown
in Algorithm 3.2. All methods are repeated 10 times, and each time we randomly
select a subset of images for each individual to create a labeled sample set. Unlike
most existing semi-supervised learning experiments, we test the performance of all
compared methods with only a small set of labeled samples, because the goal of
semi-supervised learning is to deal with practical tasks that have very limited labeled
samples but a large amount of unlabeled ones. For each individual, the percentage
of labeled samples on the Extend YaleB, PIE and USPS databases is varied from

36
3
Robust Graph Construction
Table 3.3 Average accuracies of different graphs with standard deviations combined with the
GHF label propagation method under different percentages of labeled samples (shown in the
parenthesis) on YaleB and PIE databases
Methods YaleB (10%)
YaleB (20%) YaleB (30%) PIE (10%)
PIE (20%)
PIE (30%)
k-NN-I
65.41 ˙ 1.81
72.10 ˙ 1.68 75.92 ˙ 1.64 51.69 ˙ 2.69
62.30 ˙ 1.71 68.99 ˙ 1.68
k-NN-II 56.03 ˙ 2.27
64.52 ˙ 1.85 69.81 ˙ 2.36 44.93 ˙ 3.88
56.90 ˙ 2.48 62.38 ˙ 1.75
BM [18] 63.45 ˙ 2.23
72.30 ˙ 2.43 76.61 ˙ 2.74 56.84 ˙ 2.86
67.83 ˙ 2.39 74.67 ˙ 1.95
l1-graph
[50]
58.81 ˙ 13.72 80.93 ˙ 2.57 88.11 ˙ 2.38 41.41 ˙ 11.79 72.33 ˙ 5.88 82.25 ˙ 2.28
SPG
[14]
58.90 ˙ 2.18
72.25 ˙ 2.04 81.22 ˙ 2.18 57.04 ˙ 3.28
72.83 ˙ 2.69 80.57 ˙ 1.98
LRR
[31]
75.48 ˙ 4.02
88.67 ˙ 1.75 92.95 ˙ 1.55 59.67 ˙ 6.51
72.83 ˙ 2.69 89.71 ˙ 1.92
NNLRS
[60]
76.89 ˙ 3.54
89.58 ˙ 1.30 93.20 ˙ 1.49 64.43 ˙ 5.12
85.17 ˙ 2.75 90.88 ˙ 1.62
LRCB
[21]
90.67 ˙ 0.93
91.61 ˙ 0.75 94.02 ˙ 0.99 84.06 ˙ 2.06
89.72 ˙ 1.91 91.30 ˙ 1.64
Ours-I
91.05 ˙ 1.60
92.19 ˙ 1.70 94.30 ˙ 1.82 86.25 ˙ 1.27
89.41 ˙ 2.03 92.02 ˙ 1.43
Ours-II
91.56 ˙ 1.05
91.98 ˙ 2.11 94.83 ˙ 1.47 87.66 ˙ 1.90
92.70 ˙ 1.81 93.52 ˙ 1.39
Table 3.4 Average accuracies of different graphs with standard deviations combined with the
GHF label propagation method under different percentages of labeled samples (shown in the
parenthesis) on ORL and USPS databases
Methods ORL (10%)
ORL (20%)
ORL (30%)
USPS (10%) USPS (20%) USPS (30%)
k-NN-I
59.33 ˙ 1.44 70.41 ˙ 2.43 76.21 ˙ 1.76 89.40 ˙ 0.92 90.65 ˙ 0.84 91.31 ˙ 0.66
k-NN-II
48.94 ˙ 2.19 60.69 ˙ 3.43 67.89 ˙ 2.98 88.88 ˙ 1.47 90.51 ˙ 0.82 90.90 ˙ 0.72
BM [18] 58.33 ˙ 2.01 72.40 ˙ 1.69 78.79 ˙ 2.55 88.92 ˙ 0.91 91.30 ˙ 0.92 91.63 ˙ 0.78
l1-graph
[50]
43.06 ˙ 2.74 66.56 ˙ 3.93 73.36 ˙ 2.25 34.43 ˙ 7.47 67.65 ˙ 5.54 77.79 ˙ 3.42
SPG
[14]
62.78 ˙ 3.02 77.50 ˙ 2.69 77.14 ˙ 2.18 61.64 ˙ 0.93 72.35 ˙ 1.36 80.91 ˙ 1.30
LRR
[31]
60.69 ˙ 2.59 76.78 ˙ 1.91 83.04 ˙ 2.59 62.09 ˙ 9.91 83.19 ˙ 1.82 85.91 ˙ 1.54
NNLRS
[60]
61.27 ˙ 2.76 77.81 ˙ 2.94 84.75 ˙ 2.59 80.86 ˙ 5.64 90.85 ˙ 2.71 91.01 ˙ 1.71
LRCB
[21]
76.11 ˙ 2.41 82.57 ˙ 2.23 87.70 ˙ 1.85 89.16 ˙ 0.73 91.41 ˙ 0.79 92.06 ˙ 0.48
Ours-I
83.75 ˙ 0.69 89.04 ˙ 2.33 91.57 ˙ 2.15 89.78 ˙ 0.91 91.60 ˙ 1.05 92.45 ˙ 1.63
Ours-II
85.16 ˙ 0.22 90.42 ˙ 1.91 94.50 ˙ 1.03 89.54 ˙ 1.09 92.03 ˙ 0.96 92.98 ˙ 1.75
5% to 30%. Since there are only 10 images of each individual in ORL database, this
percentage varies from 10% to 50%. In our models, 1 and 2 are empirically set to
2 and 0.03, and b is set to 10.
Tables 3.3 and 3.4 show the average accuracies of different graphs combined with
GHF label propagation strategy on four databases, when the percentages of labeled

3.6
Experiments
37
5%
10%
15%
20%
25%
30%
10
20
30
40
50
60
70
80
90
100
Percentage of labeled samples
Accuracy (%)
k−NN−I
k−NN−II
BM [18]
l1−Graph [44]
SPG [14]
LRR [25]
NNLRS [54]
LRCB [21]
Ours−I
Ours−II
5%
10%
15%
20%
25%
30%
10
20
30
40
50
60
70
80
90
Percentage of labeled samples
Accuracy (%)
k−NN−I
k−NN−II
BM [18]
l1−Graph [44]
SPG [14]
LRR [25]
NNLRS [54]
LRCB [21]
Ours−I
Ours−II
(b)
(a)
10%
20%
30%
40%
50%
40
50
60
70
80
90
Percentage of labeled samples
Accuracy (%)
k−NN−I
k−NN−II
BM [18]
l1−Graph [44]
SPG [14]
LRR [25]
NNLRS [54]
LRCB [21]
Ours−I
Ours−II
5%
10%
15%
20%
25%
30%
20
30
40
50
60
70
80
90
Percentage of labeled samples
Accuracy (%)
k−NN−I
k−NN−II
BM [18]
l1−Graph [44]
SPG [14]
LRR [25]
NNLRS [54]
LRCB [21]
Ours−I
Ours−II
(d)
(c)
Fig. 3.5 Accuracy of transductive semi-supervised classiﬁcation of our models and compared
methods versus different percentages of labeled samples on four databases. (a) Extended YaleB.
(b) PIE. (c) ORL. (d) USPS
samples are 10%, 20% and 30%. Figure 3.5 shows the average accuracies versus
varying percentages of labeled samples. We can observe from Tables 3.3 and 3.4
and Fig. 3.5 that:
1. BM graph usually outperforms k-NN graphs, since BM graph emphasises
sparsity and balance at the same time. But they all use traditional similarity
metrics. In addition, a large number of labeled samples with small variance allow
k-NN graphs to obtain impressive performance on the USPS database.
2. The advanced sparse representation based graph, SPG, outperforms k-NN graph
in many cases on the Extended YaleB and PIE databases, and achieves compara-
ble performance as BM;
3. NNLRS graph, which is a low-rank and sparse graph, performs better than other
graphs in most cases;
4. When the percentage of labeled samples is increased, the performance of all
compared methods is increased. Our two graphs outperform other compared
graphs in almost all cases, and it reaches great performance very quickly. When

38
3
Robust Graph Construction
the labeled samples are very limited, e.g., under 10% of total number of samples,
our graphs can signiﬁcantly improve the accuracy over the state-of-the-art graphs
on three face databases.
5. Results show that Ours-II achieves better results than Ours-I in most cases, which
demonstrates the effectiveness of balanced property.
Inductive Setting Our graphs can be also concatenated with inductive semi-
supervised learning algorithms, such as semi-supervised discriminant analysis
(SDA) [4]. On the YaleB, PIE and USPS databases, we randomly select 20 images
as training samples, and the remaining images as test samples. The percentage of
labeled samples varies from 5% to 30%. On the ORL database, 5 images from each
subject are randomly selected to construct the training set, and the remaining images
are used for testing. The number of labeled samples varies from 1 to 5. Each method
is repeated 10 times. Tables 3.5 and 3.6 and Fig. 3.6 show the average accuracies
of different graphs on four databases. We can observe that our two graphs obtain
better performance than other graphs. Even though Ours-I graph is unbalanced, it
performs better than our previous work LRCB graph that is balanced. The reason is
that Ours-I, as well as Ours-II, reformulates the rank-minimization model to obtain
a new similarity metric, which is the key during graph construction.
3.6.4
Discussions
Our approach is expected to work well when the data have clear subspace
structures, as it estimates the graph weights by taking advantage of the subspace
structure of samples. As we know, face images of different person usually lie on
Table 3.5 Average accuracies of different graphs with standard deviations combined with the
semi-supervised discriminant analysis (SDA) method under different percentages of labeled
samples (shown in the parenthesis) on YaleB and PIE databases
Methods
YaleB (10%) YaleB (20%) YaleB (30%) PIE (10%)
PIE (20%)
PIE (30%)
k-NN-I
57.87 ˙ 4.99 76.11 ˙ 1.80 83.09 ˙ 2.01 64.22 ˙ 1.73 80.42 ˙ 2.13 86.16 ˙ 3.17
k-NN-II
52.87 ˙ 3.84 71.18 ˙ 1.87 79.93 ˙ 2.09 60.31 ˙ 2.01 77.29 ˙ 2.11 83.89 ˙ 2.50
BM [18]
64.96 ˙ 4.54 80.76 ˙ 1.54 86.33 ˙ 2.53 67.00 ˙ 3.07 83.04 ˙ 1.98 87.78 ˙ 1.80
l1-graph
[50]
70.42 ˙ 3.64 80.71 ˙ 2.05 86.04 ˙ 1.85 73.76 ˙ 2.67 84.24 ˙ 2.02 88.04 ˙ 2.11
LRR [31] 70.02 ˙ 3.72 78.89 ˙ 2.45 84.53 ˙ 2.56 70.20 ˙ 3.69 81.82 ˙ 2.02 87.29 ˙ 1.77
NNLRS
[60]
71.54 ˙ 2.52 80.43 ˙ 1.97 84.79 ˙ 2.19 72.73 ˙ 3.47 83.11 ˙ 2.64 87.60 ˙ 1.49
LRCB
[21]
73.05 ˙ 2.01 80.97 ˙ 1.66 85.22 ˙ 2.13 76.47 ˙ 2.39 85.05 ˙ 1.99 88.35 ˙ 2.43
Ours-I
75.58 ˙ 3.02 82.67 ˙ 1.85 86.62 ˙ 1.57 78.36 ˙ 1.61 85.44 ˙ 2.48 88.29 ˙ 2.59
Ours-II
73.67 ˙ 2.15 87.34 ˙ 1.51 89.33 ˙ 1.69 76.55 ˙ 2.52 87.95 ˙ 1.70 90.29 ˙ 2.59

3.6
Experiments
39
Table 3.6 Average accuracies of different graphs with standard deviations combined with the
semi-supervised discriminant analysis (SDA) method under different percentages of labeled
samples (shown in the parenthesis) on ORL and USPS databases
Methods
ORL (20%)
ORL (60%)
ORL (100%) USPS (10%) USPS (20%) USPS (30%)
k-NN-I
83.70 ˙ 2.83 91.40 ˙ 2.82 94.65 ˙ 1.79 69.55 ˙ 2.92 75.51 ˙ 2.16 78.15 ˙ 2.65
k-NN-II
83.10 ˙ 3.25 90.90 ˙ 2.85 94.00 ˙ 1.86 68.31 ˙ 2.01 77.29 ˙ 2.11 80.89 ˙ 2.50
BM [18]
82.50 ˙ 1.42 92.00 ˙ 1.75 94.50 ˙ 1.53 67.00 ˙ 3.07 75.08 ˙ 1.98 80.78 ˙ 1.80
l1-graph
[50]
84.10 ˙ 3.15 92.00 ˙ 2.33 94.95 ˙ 1.44 55.10 ˙ 3.67 67.40 ˙ 3.09 73.35 ˙ 1.25
LRR [31] 83.20 ˙ 2.73 91.55 ˙ 2.54 94.85 ˙ 1.55 57.20 ˙ 4.04 68.70 ˙ 3.84 74.19 ˙ 1.57
NNLRS
[60]
83.75 ˙ 1.52 91.90 ˙ 2.63 94.50 ˙ 1.65 62.50 ˙ 3.47 70.82 ˙ 2.05 76.33 ˙ 1.64
LRCB
[21]
84.25 ˙ 1.86 92.10 ˙ 1.49 94.65 ˙ 1.52 70.15 ˙ 1.68 75.53 ˙ 2.08 77.61 ˙ 3.11
Ours-I
85.25 ˙ 3.06 92.00 ˙ 1.56 94.95 ˙ 1.75 70.63 ˙ 2.66 76.21 ˙ 2.87 78.19 ˙ 1.60
Ours-II
83.75 ˙ 3.06 92.20 ˙ 2.32 94.80 ˙ 1.42 70.04 ˙ 2.87 78.54 ˙ 2.59 79.60 ˙ 1.78
5%
10%
15%
20%
25%
30%
30
40
50
60
70
80
90
Percentage of labeled samples
Accuracy (%)
k−NN−I
k−NN−II
BM [18]
l1−Graph [44]
LRR [25]
NNLRS [54]
LRCB [21]
Ours−I
Ours−II
5%
10%
15%
20%
25%
30%
40
45
50
55
60
65
70
75
80
85
90
Percentage of labeled samples
Accuracy (%)
k−NN−I
k−NN−II
BM [18]
l1−Graph [44]
LRR [25]
NNLRS [54]
LRCB [21]
Our−I
Our−II
(b)
(a)
20%
40%
60%
80%
100%
82
84
86
88
90
92
94
Percentage of labeled samples
Accuracy (%)
k−NN−I
k−NN−II
BM [18]
l1−Graph [44]
LRR [14]
NNLRS [54]
LRCB [21]
Ours−I
Ours−II
5%
10%
15%
20%
25%
30%
40
45
50
55
60
65
70
75
80
Percentage of labeled samples
Accuracy (%)
k−NN−I
k−NN−II
BM [18]
l1−Graph [44]
LRR [25]
NNLRS [54]
LRCB [21]
Ours−I
Ours−II
(d)
(c)
Fig. 3.6 Accuracy of inductive semi-supervised classiﬁcation of our approach and compared
methods versus different percentages of labeled samples on four databases. (a) Extended YaleB.
(b) PIE. (c) ORL. (d) USPS

40
3
Robust Graph Construction
separate subspaces. Therefore, our approach achieved much better results than other
baselines on three face databases (ORL, Extended YaleB and PIE). However, the
subspace structures in digit database are not very clear, as some digits are quite
similar, such as 1 and 7, 0 and 6. Thus, the results of our approach and other
baselines are close. Sometimes, k-NN can get better results than other methods.
Another interesting phenomenon is that, in semi-supervised classiﬁcation, our
method works very well even if the labeled samples are very limited. In this case,
the similarity metrics of some baselines (e.g., Gaussian kernel in k-NN graph) are
not robust to noise, and therefore the estimated graph weights are unreliable. In our
approach, we explicitly model the noise contained in samples, and calculate graph
weights using low-rank codings. Experiments on several face databases showed that,
although the face images contain illumination or pose changes, our approach still
obtains impressive results with a few labeled samples.
To illustrate why our graph outperforms other compared graphs, Fig. 3.7 visu-
alizes several weighted graphs including k-NN graph, l1-graph, LRR graph, and
our two graphs on the PIE face database. We can observe from Fig. 3.7 that k-
NN graph is sparse, but it’s not balanced. l1-graph is not as sparse as k-NN.
LRR produces a very dense graph that is undesirable for graph based learning
algorithms. Ours-I graph in Fig. 3.7d can recover the block diagonal structures
clearly, and the similarity values in diagonal blocks are much higher than those
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
(a)
(b)
(d)
(e)
(c)
Fig. 3.7 Visualization of different weighted graphs on PIE face database. (Please enlarge the
ﬁgure to see details in graphs). (a) k-NN. (b) l1. (c) LRR. (d) Ours-I. (e) Ours-II

References
41
Table 3.7 Average time cost
(seconds) on PIE database
Method
k-NN
l1 [50]
LRR [31]
Ours-I
Ours-II
Time (s)
1.5
144.8
205.9
365.8
516.3
of l1 and LRR graphs, but it’s still not balanced. Figure 3.7e shows that Ours-
II graph is much sparser than the aforementioned graphs. It correctly connects
the samples within each class (diagonal blocks in the graph), and meanwhile the
misconnections between samples in different classes are fewer than other graphs.
The results validate the superiority of our low-rank coding based similarity metric,
as well as the balanced property.
Another consideration is the computational cost of our approach. Although the
MM-ALM algorithm presented in Algorithm 3.1 is more efﬁcient than other solvers,
the b-matching optimization has a high computational cost. Table 3.7 shows the
average computational time of different graph construction methods on the PIE
database. KNN graph is the most efﬁcient one, and l1 and LRR graph have similar
time costs. Ours-II consumes the most time because of the b-matching constraint.
As we can see, Ours-I model offers a good balance between accuracy and efﬁciency.
3.7
Summary
In this chapter, we have proposed a novel graph construction approach for graph
based learning, including data clustering and semi-supervised classiﬁcation. By
taking advantages of low-rank coding and sparsiﬁcation constraints (i.e., k-NN and
b-matching), we jointly learned symmetric and sparse graphs. We also designed
novel optimization algorithms to solve the proposed models. Experimental results
on the Extended YaleB, PIE, ORL and USPS databases demonstrated the effective-
ness of our approach compared with several state-of-the-art methods.
References
1. Anand, R., Reddy, C.K.: Graph-based clustering with constraints. In: The Paciﬁc-Asia
Conference on Knowledge Discovery and Data Mining (PAKDD), pp. 51–62 (2011)
2. Bach, F.: Consistency of trace norm minimization. J. Mach. Learn. Res. 9, 1019–1048 (2008)
3. Bellhumeur, P.N., Hespanha, J.P., Kriegeman, D.J.: Eigenfaces vs. ﬁsherfaces: recognition
using class speciﬁc linear projection. IEEE Trans. Pattern Anal. Mach. Intell. 19(7), 711–720
(1997)
4. Cai, D., He, X., Han, J.: Semi-supervised discriminant analysis. In: International Conference
on Computer Vision (ICCV), pp. 1–7 (2007)
5. Candès, E.J., Li, X., Ma, Y., Wright, J.: Robust principal component analysis? J. ACM 58(3),
11 (2011)
6. Chapelle, O., Schölkopf, B., Zien, A.: Semi-supervised Learning. MIT Press, Cambridge
(2006)

42
3
Robust Graph Construction
7. Chen, K., Wang, S.: Regularized boost for semi-supervised learning. In: Advances in Neural
Information Processing Systems (NIPS) (2007)
8. Cheng, H., Liu, Z., Yang, J.: Sparsity induced similarity measure for label propagation. In:
International Conference on Computer Vision (ICCV), pp. 317–324 (2009)
9. Culp, M., Michailidis, G.: Graph-based semisupervised learning. IEEE Trans. Pattern Anal.
Mach. Intell. 30(1), 174–179 (2008)
10. de Sousa, C.A.R., Rezende, S.O., Batista, G.E.A.P.A.: Inﬂuence of graph construction on semi-
supervised learning. In: The European Conference on Machine Learning and Principles and
Practice of Knowledge Discovery in Databases (ECML/PKDD) (3), pp. 160–175 (2013)
11. Georghiades, A., Belhumeur, P., Kriegman, D.: From few to many: illumination cone models
for face recognition under variable lighting and pose. IEEE Trans. Pattern Anal. Mach. Intell.
23(6), 643–660 (2001)
12. Goldberg, A.B., Zhu, X., Wright, S.J.: Dissimilarity in graph-based semi-supervised classiﬁ-
cation. J. Mach. Learn. Res. Proc. Track 2, 155–162 (2007)
13. He, J., Carbonell, J.G., Liu, Y.: Graph-based semi-supervised learning as a generative
model. In: International Joint Conference on Artiﬁcial Intelligence (IJCAI), pp. 2492–2497
(2007)
14. He, R., Zheng, W., Hu, B., Kong, X.: Nonnegative sparse coding for discriminative semi-
supervised learning. In: IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 2849–2856 (2011)
15. Huang, B., Jebara, T.: Fast b-matching via sufﬁcient selection belief propagation. J. Mach.
Learn. Res. Proc. Track pp. 361–369 (2011)
16. Hull, J.: A database for handwritten text recognition research. IEEE Trans. Pattern Anal. Mach.
Intell. 16(5), 550–554 (1994)
17. Iwata, T., Duh, K.: Bidirectional semi-supervised learning with graphs. In: The European
Conference on Machine Learning and Principles and Practice of Knowledge Discovery in
Databases (ECML PKDD) (2), pp. 293–306 (2012)
18. Jebara, T., Wang, J., Chang, S.-F.: Graph construction and b-matching for semi-supervised
learning. In: International Conference on Machine Learning (ICML), pp. 441–448 (2009)
19. Kveton, B., Valko, M., Rahimi, A., Huang, L.: Semi-supervised learning with max-margin
graph cuts. J. Mach. Learn. Res. Proc. Track 9, 421–428 (2010)
20. Li, L., Li, S., Fu, Y.: Discriminative dictionary learning with low-rank regularization for face
recognition. In: IEEE International Conference on Automatic Face and Gesture Recognition
(FG), pp. 1–6 (2013)
21. Li, S., Fu, Y.: Low-rank coding with b-matching constraint for semi-supervised classiﬁcation.
In: International Joint Conference on Artiﬁcial Intelligence (IJCAI), pp. 1472–1478 (2013)
22. Li, S., Fu, Y.: Robust subspace discovery through supervised low-rank constraints. In: SIAM
International Conference on Data Mining (SDM), pp. 163–171 (2014)
23. Li, S., Fu, Y.: Learning balanced and unbalanced graphs via low-rank coding. IEEE Trans.
Knowl. Data Eng. 27(5), 1274–1287 (2015)
24. Li, S., Fu, Y.: Learning robust and discriminative subspace with low-rank constraints. IEEE
Trans. Neural Netw. Learn. Syst. 27(11), 2160–2173 (2016)
25. Li, S., Fu, Y.: Unsupervised transfer learning via low-rank coding for image clustering. In:
International Joint Conference on Neural Networks, pp. 1795–1802. IEEE (2016)
26. Li, S., Li, K., Fu, Y.: Self-taught low-rank coding for visual learning. IEEE Trans. Neural Netw.
Learn. Syst. (2017)
27. Li, S., Shao, M., Fu, Y.: Locality linear ﬁtting one-class SVM with low-rank constraints
for outlier detection. In: International Joint Conference on Neural Networks (IJCNN),
pp. 676–683 (2014)
28. Li, S., Shao, M., Fu, Y.: Multi-view low-rank analysis for outlier detection. In: Proceedings of
the SIAM International Conference on Data Mining, pp. 748–756. SIAM (2015)
29. Lin, Z., Chen, M., Ma, Y.: The augmented lagrange multiplier method for exact recovery of
corrupted low-rank matrix. In: Technical Report, UIUC (2009)

References
43
30. Lin, Z., Liu, R., Su, Z.: Linearized alternating direction method with adaptive penalty for low-
rank representation. In: Advances in Neural Information Processing Systems (NIPS), pp. 612–
620 (2011)
31. Liu, G., Lin, Z., Yan, S., Sun, J., Yu, Y., Ma, Y.: Robust recovery of subspace structures by
low-rank representation. IEEE Trans. Pattern Anal. Mach. Intell. 35(1), 171–184 (2013)
32. Liu, G., Lin, Z., Yu, Y.: Robust subspace segmentation by low-rank representation. In:
International Conference on Machine Learning (ICML), pp. 663–670 (2010)
33. Liu, J., Wang, C., Danilevsky, M., Han, J.: Large-scale spectral clustering on graphs. In:
International Joint Conference on Artiﬁcial Intelligence (IJCAI), pp. 1486–1492 (2013)
34. Liu, W., Wang, J., Chang, S.-F.: Robust and scalable graph-based semisupervised learning.
Proc. IEEE 100(9), 2624–2638 (2012)
35. Ni, B., Yan, S., Kassim, A.A.: Learning a propagable graph for semisupervised learning:
classiﬁcation and regression. IEEE Trans. Knowl. Data Eng. 24(1), 114–126 (2012)
36. Nie, F., Wang, H., Huang, H., Ding, C.H.Q.: Unsupervised and semi-supervised learning via l1-
norm graph. In: International Conference on Computer Vision (ICCV), pp. 2268–2273 (2011)
37. Samaria, F., Harter, A.: Parameterisation of a stochastic model for human face identiﬁca-
tion. In: Proceedings of the Second IEEE Workshop on Applications of Computer Vision,
pp. 138–142 (1994)
38. Shang, F., Jiao, L., Liu, Y., Tong, H.: Semi-supervised learning with nuclear norm regulariza-
tion. Pattern Recogn. 46(8), 2323–2336 (2013)
39. Shang, F., Jiao, L., Liu, Y., Wang, F.: Learning spectral embedding via iterative eigenvalue
thresholding. In: ACM International Conference on Information and Knowledge Management
(CIKM), pp. 1507–1511 (2012)
40. Shang, F., Jiao, L., Wang, F.: Semi-supervised learning with mixed knowledge information. In:
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),
pp. 732–740 (2012)
41. Shi, J., Malik, J.: Normalized cuts and image segmentation. IEEE Trans. Pattern Anal. Mach.
Intell. 22(8), 888–905 (2000)
42. Sim, T., Baker, S., Bsat, M.: The CMU pose, illumination, and expression database. IEEE
Trans. Pattern Anal. Mach. Intell. 25(12), 1615–1618 (2003)
43. Sokol, M., Avrachenkov, K., Gonçalves, P., Mishenin, A.: Generalized optimization framework
for graph-based semi-supervised learning. In: SIAM International Conference on Data Mining
(SDM), pp. 966–974 (2012)
44. Talwalkar, A., Mackey, L.W., Mu, Y., Chang, S.-F., Jordan, M.I.: Distributed low-rank subspace
segmentation. In: International Conference on Computer Vision (ICCV), pp. 3543–3550
(2013)
45. Wang, F., Li, P., König, A.C., Wan, M.: Improving clustering by learning a bi-stochastic data
similarity matrix. Knowl. Inf. Syst. 32(2), 351–382 (2012)
46. Wang, F., Zhang, C.: Label propagation through linear neighborhoods. IEEE Trans. Knowl.
Data Eng. 20(1), 55–67 (2008)
47. Wang, J., Xia, Y.: Fast graph construction using auction algorithm. In: The Conference on
Uncertainty in Artiﬁcial Intelligence (UAI), pp. 873–882 (2012)
48. Wang, S., Liu, D., Zhang, Z.: Nonconvex relaxation approaches to robust matrix recovery. In:
International Joint Conference on Artiﬁcial Intelligence (IJCAI), pp. 1764–1770 (2013)
49. Wright, J., Yang, A.Y., Ganesh, A., Sastry, S., Ma, Y.: Robust face recognition via sparse
representation. IEEE Trans. Pattern Anal. Mach. Intell. 31(2), 210–227 (2009)
50. Yan, S., Wang, H.: Semi-supervised learning by sparse representation. In: SIAM International
Conference on Data Mining (SDM), pp. 792–801 (2009)
51. Zhang, K., Wang, Q., Lan, L., Sun, Y., Marsic, I.: Sparse semi-supervised learning on low-rank
kernel. Neurocomputing 129:265–272 (2014)
52. Zhang, T., Ji, R., Liu, W., Tao, D., Hua, G.: Semi-supervised learning with manifold ﬁtted
graphs. In: International Joint Conference on Artiﬁcial Intelligence (IJCAI), pp. 1896–1902
(2013)

44
3
Robust Graph Construction
53. Zhang, Z., Chow, T.W.S., Zhao, M.-B.: Trace ratio optimization-based semi-supervised
nonlinear dimensionality reduction for marginal manifold visualization. IEEE Trans. Knowl.
Data Eng. 25(5), 1148–1161 (2013)
54. Zhao, M., Jiao, L., Feng, J., Liu, T.: A simpliﬁed low rank and sparse graph for semi-supervised
learning. Neurocomputing 140, 84–96 (2014)
55. Zhao, M.-B., Zhang, Z., Chow, T.W.S. , Li, B.: A general soft label based linear discriminant
analysis for semi-supervised dimensionality reduction. Neural Netw. 55, 83–97 (2014)
56. Zhou, D., Bousquet, O., Lal, T.N., Weston, J., Schölkopf, B.: Learning with local and global
consistency. In: Advances in Neural Information Processing Systems (NIPS), pp. 321–328
(2003)
57. Zhu, X.: Semi-supervised learning. In: Encyclopedia of Machine Learning, pp. 892–897.
Springer (2010)
58. Zhu, X., Ghahramani, Z., Lafferty, J.D.: Semi-supervised learning using gaussian ﬁelds and
harmonic functions. In: International Conference on Machine Learning (ICML), pp. 912–919
(2003)
59. Zhu, X., Goldberg, A.B., Khot, T.: Some new directions in graph-based semi-supervised
learning. In: IEEE International Conference on Multimedia and Expo, pp. 1504–1507 (2009)
60. Zhuang, L., Gao, H., Lin, Z., Ma, Y., Zhang, X., Yu, N.: Non-negative low rank and sparse
graph for semi-supervised learning. In: IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 2328–2335 (2012)

Chapter 4
Robust Subspace Learning
Abstract Subspace learning is widely used in extracting discriminative features
for classiﬁcation. However, when data are contaminated with severe noise, the
performance of most existing subspace learning methods would be limited. Recent
advances in low-rank modeling provide effective solutions for removing noise or
outliers contained in sample sets, which motivates us to take advantages of low-rank
constraints in order to exploit robust and discriminative subspace for classiﬁcation.
In this chapter, we introduce a discriminative subspace learning method named
Supervised Regularization based Robust Subspace (SRRS) approach, by incor-
porating the low-rank constraint. SRRS seeks low-rank representations from the
noisy data, and learns a discriminative subspace from the recovered clean data
jointly. A supervised regularization function is designed to make use of the class
label information and therefore to enhance the discriminability of subspace. Our
approach is formulated as a constrained rank minimization problem. We design
an inexact augmented Lagrange multiplier (ALM) optimization algorithm to solve
it. Unlike the existing sparse representation and low-rank learning methods, our
approach learns a low-dimensional subspace from recovered data, and explicitly
incorporates the supervised information. Our approach and some baselines are
evaluated on the COIL-100, ALOI, Extended YaleB, FERET, AR, and KinFace
databases. Experimental results demonstrate the effectiveness of our approach,
especially when the data contain considerable noise or variations.
4.1
Overview1
Subspace learning methods have been extensively studied in pattern recognition and
data mining areas during the last two decades [2, 10, 11, 13, 24, 39]. Some represen-
tative subspace learning methods include principal component analysis (PCA) [39],
linear discriminant analysis (LDA) [2], locality preserving projections(LPP) [11],
neighborhood preserving embedding (NPE) [10], locality sensitive discriminant
analysis (LSDA) [3], discriminative locality alignment (DLA) [49]. The basic idea
of subspace learning methods is to ﬁnd a low-dimensional projection which satisﬁes
1This chapter is reprinted with permission from SIAM. “Robust Subspace Discovery through
Supervised Low-Rank Constraints”, SIAM International Conference on Data Mining, 2014.
© Springer International Publishing AG 2017
S. Li, Y. Fu, Robust Representation for Data Analytics, Advanced Information
and Knowledge Processing, DOI 10.1007/978-3-319-60176-2_4
45

46
4
Robust Subspace Learning
some speciﬁc properties [41]. As unsupervised methods, PCA [39] seeks such a
subspace where the variance of projected samples is maximized, while LPP [11]
and NPE [10] aim to ﬁnd subspaces which can preserve the locality relationships
of samples. When class labels are available, supervised subspace methods are
more effective for classiﬁcation tasks. LDA [2] aims at ﬁnding a projection
which maximizes the inter-class scatter and minimizes the intra-class scatter at
the same time. It extracts discriminative features for classiﬁcation tasks. LSDA [3]
preserves both discriminant and local geometrical structure in data. DLA [49] is
designed based on the patch alignment framework which presents the idea of part
optimization and whole alignment. As a discriminative model, it is suitable for the
nonlinear classiﬁcation problem. In [31], two generic frameworks are presented to
implement supervised subspace learning for multi-label classiﬁcation. Note that
the frameworks built in [31, 49] provide us with uniﬁed interpretations of many
subspace learning methods. LPP [11] and NPE [10] can also be extended to
supervised versions. Those methods usually obtain promising results on clean data,
however, when the data are corrupted by considerable noise (e.g., missing pixels or
outliers) or large variations (e.g., pose variations in face images) in real applications,
their performance is heavily degraded [27].
To learn effective features from noisy data, many techniques have been intro-
duced, and sparse representation (SR) is among the most successful ones. SR has
proven to be robust to noise, and has shown impressive results for face recognition
under noisy conditions [43, 47]. The idea of SR has also been considered in
dimensionality reduction and subspace learning [15, 35, 50, 51]. [50] combines
dimensionality reduction and a sparse representation classiﬁer (SRC). A sparsity
preserving projections (SPP) method is proposed in [35], and its improved version
is introduced in [15]. Moreover, a linear subspace learning (LSL) algorithm via
sparse coding is described in [51], which also involves dictionary learning. Most SR
methods seek the sparsest coding vector to represent each test sample by all training
samples. However, the underlying global structure of data is not considered in these
methods, and therefore they may not be robust to noise when extra clean data is not
available [28].
Low-rank modeling has attracted a lot of attention recently, which can recover the
underlying structure of data [5, 6, 17, 25]. It’s an extension of sparse representation.
When data are drawn from a single subspace, robust PCA [5] is able to recover
the corrupted data by minimizing the rank of data matrix. As an extension of
RPCA, low-rank representation (LRR) [28] can recover corrupted data drawn from
multiple subspaces. RPCA has been successfully applied to background modeling,
and LRR achieves impressive performance on subspace clustering. Many improved
versions of LRR have been developed. Latent LRR (LatLRR) [27] considers the
effects of hidden data. Low-rank coding based balanced (LRCB) graph is designed
for clustering [20] and semi-supervised classiﬁcation [18]. In addition, low-rank
modeling has been applied to outlier detection [23], domain adaption [12], transfer
learning [36, 37], and dictionary learning [22, 29, 52]. Low-rank modeling usually
suffers large computational burden, and the idea of divide-and-conquer has been
introduced to solve this problem [33, 38], which makes low-rank modeling scalable
to larger datasets.

4.1
Overview
47
Class 1
Class 2
=
Joint Learning
Original
distribution
Low-dimension 
projection
: Noisy data matrix
: Recovered clean data
: Basis matrix
: Low-rank representations
: Noise
: Low-dimensional data
: Discriminative subspace
+
=
+
=
+
=
~
~
~
Fig. 4.1 Framework of the proposed approach. We jointly remove noise from data X and learn
robust subspace P. The corrupted samples are mixed in the original space, but they are well
separated in the learned subspace
As discussed above, low-rank modeling has shown impressive performance
in various applications [5, 28, 42]. However, only a few of those methods can
take advantages of class label information during low-rank learning, which is key
for classiﬁcation purpose. On the other hand, although the conventional subspace
learning approaches usually obtain good performance for classiﬁcation tasks, they
have strong assumptions on the data distribution, and therefore they are sensitive to
the noisy data. The learned subspace has limited discriminability. Can we leverage
the advantages of both supervised subspace learning and low-rank modeling for
classiﬁcation?
In this chapter, we propose to exploit a discriminative and robust subspace, which
is insensitive to noise or pose/illumination variations, for dimensionality reduction
and classiﬁcation [19, 21]. In particular, we propose a novel linear subspace
approach named Supervised Regularization based Robust Subspace (SRRS) for
pattern classiﬁcation. As illustrated in Fig. 4.1, the core idea of our approach is
to jointly learn low-rank representations from the noisy data, and a discriminative
subspace from the recovered clean data. Moreover, to improve the classiﬁcation
performance of our approach, we naturally incorporate class label information
into our objective function as supervised regularization. This regularization term
enables us to learn a discriminative subspace, which beneﬁts classiﬁcation tasks.
Finally, we formulate our model as a constrained rank minimization problem,
and solve it using the recently proposed ALM algorithm [26]. The convexity of
supervised regularization term is proved theoretically. Experimental results on six
benchmark datasets show that our SRRS approach outperforms the traditional
subspace methods and several state-of-the-art low-rank modeling methods in almost
all cases, especially when the data contain considerable variations or are corrupted
by noise.

48
4
Robust Subspace Learning
Our work is closely related to some existing low-rank learning methods. The
latent LRR (LatLRR) [27] approach could be integrated with some subspace
learning methods. But the representation learnt by LatLRR does not necessarily
guarantee an optimal input for the subsequent subspace learning. While our
approach simultaneously seeks optimal low-rank representations and discriminative
subspaces. In [7], a low-rank method with structural incoherence is applied to face
recognition. It ﬁrst decomposes raw images into low-rank part and sparse part, and
then applies PCA on the low-rank part to obtain a subspace. It does not, however,
learn the low-rank representation and a discriminative subspace simultaneously. In
this manner, the low-rank part is expected to be discriminative and beneﬁt classiﬁ-
cation tasks. In [52], a structured low-rank representation method is presented for
image classiﬁcation. The differences between [52] and our approach include: (1) It
learns a dictionary D to represent the sample set X in the original sample space,
but our approach aims at learning a low-dimensional discriminative subspace to
reduce the dimensionality of samples. (2) It enforces a diagonal structure prior on
the coefﬁcient matrix Z to introduce the supervised information, but our approach
employs the Fisher criterion to learn discriminative features; (3) It uses the ridge
regression model for classifying new samples, but our approach adopts the nearest
neighbor classiﬁer. In [48], a LRR-based discriminative projection method (LRR-
DP) is proposed for feature extraction. In this method, LRR is regarded as a
data pre-processing method, and is performed only once to decompose sample
set into two parts, the low-rank denoised samples and associated sparse noise.
However, this decomposition is not guaranteed to be optimal for classiﬁcation, as
it doesn’t make use of any class prior information. On the contrary, our approach
iteratively learns subspace and decomposes sample set, and it takes full advantage
of class information through supervised regularization. In [53], a discriminant
regularization term is incorporated into the formulation of Robust PCA. This
method differs from our approach in two aspects. First, Robust PCA used in [53]
can only model one single subspace, while our approach is able to discover multiple
subspaces by virtue of LRR, which ﬁts well for multi-class classiﬁcation problems.
Second, the method in [53] separately learns low-rank data representation and
subspace, which means the obtained subspace cannot be guaranteed to be optimal,
while our approach iteratively learns low-rank representations and discriminative
subspaces.
The most relevant method in the literature is low-rank transfer subspace learning
(LTSL) [36, 37], which incorporates low-rank constraint in subspace learning.
However, there are signiﬁcant differences between LTSL and our approach. First,
LTSL is a transfer learning method that seeks a common subspace for two domains,
while our approach lies in supervised learning. Second, LTSL employs low-rank
constraint in low-dimensional subspace in order to transfer knowledge across
two domains. In our approach, the low-rank constraint is enforced in the high-
dimensional feature space in order to preserve more information.

4.2
Supervised Regularization Based Robust Subspace (SRRS)
49
In summary, our contributions include:
•
We have proposed a new feature extraction framework, which smoothly inte-
grates linear subspace learning and low-rank matrix recovery. Supervised regu-
larization is incorporated to improve the classiﬁcation performance.
•
We have designed an optimization algorithm to solve the proposed model, and
have proven the convexity of the supervised regularization term.
•
We have evaluated the model performance on a wide range of real-world appli-
cations, including object recognition, face recognition, and kinship veriﬁcation.
4.2
Supervised Regularization Based Robust
Subspace (SRRS)
In this section, a supervised regularization based robust subspace (SRRS) approach
is proposed. We ﬁrst formulate our approach as a regularized rank-minimization
problem. To solve this problem, we develop an efﬁcient optimization algorithm.
Theoretical analysis on convexity is also provided.
4.2.1
Problem Formulation
Let X denote the sample set that consists of n training samples from c classes, i.e.,
X D Œx1; x2; : : : ; xn. Given a complete basis matrix A D Œa1; a2; : : : ; am 2 Rdm,
we can represent each sample xi as a linear combination of the basis, which is
X D AZ;
(4.1)
where Z 2 Rmn is the coefﬁcient matrix. As suggested in existing subspace
clustering methods, A is usually set as the sample set X, i.e., A D X. We will discuss
the choice of basis matrix A at the end of this section.
To achieve our goal of seeking a robust subspace P 2 Rdp, we ﬁrst denote
the projected low-dimensional sample set as QX D PTX D PTAZ. Then we in
turn incorporate low-rank constraint and supervised regularization to learn the
projection P.
First, due to the fact that n samples belong to c different classes and n  c,
these samples should be drawn from c different subspaces, and therefore the
coefﬁcient matrix Z is expected to be low-rank. In other words, the coefﬁcient
vectors corresponding to samples from the same class should be highly correlated.

50
4
Robust Subspace Learning
Second, since class information is crucial to classiﬁcation problems, we design
a supervised regularization term f.P; Z/ based on the idea of Fisher criterion [2],
that is, f.P; Z/ D Tr.SB.PTAZ//
Tr.SW.PTAZ//, where Tr.K/ is the trace of matrix K. SB.PTAZ/ and
SW.PTAZ/ are the between-class and within-class scatter matrices
SB.PTAZ/ D SB. QX/ D
c
X
iD1
ni.mi  m/.mi  m/T;
SW.PTAZ/ D SW. QX/ D
c
X
iD1
ni
X
jD1
.Qxij  mi/.Qxij  mi/T;
where mi is the mean sample of the i-th class in QX, m is the overall mean sample of
QX, and Qxij is the j-th sample in the i-th class of QX.
By using Fisher criterion, the projected samples from different classes should be
far apart, while projected samples from the same class should be close to each other.
Furthermore, [9] pointed out that this trace-ratio problem can be converted into a
trace difference problem. We then rewrite f.P; Z/ as Nf.P; Z/ D Tr.SW.PTAZ// 
Tr.SB.PTAZ//.
Based on the above observations, we come up with the following objective
function
min
Z;P rank.Z/ C 1Nf.P; Z/;
s:t: X D AZ;
(4.2)
where 1 is a trade-off parameter to balance the low-rank and discriminative terms.
However, the rank minimization problem in objective (4.2) is difﬁcult to solve,
since rank./ is a non-convex function. Fortunately, nuclear norm is a good surrogate
for the rank minimization problem [5, 14, 28], and then (4.2) becomes
min
Z;P kZk C 1Nf.P; Z/;
s:t: X D AZ;
(4.3)
where kZk is the nuclear norm of a matrix (i.e., the sum of singular values of the
matrix) [4].
We also notice that the second term Nf.P; Z/ in (4.3) is not convex to Z because of
the term Tr.SB/, so we add an elastic term to ensure the convexity
Of.P; Z/ D Tr.SW/  Tr.SB/ C 
PTAZ
2
F :
(4.4)
Equation (4.4) can be equivalently expressed as
Of.P; Z/ D
PTAZ.I  Hb/
2
F 
PTAZ.Hb  Ht/
2
F C 
PTAZ
2
F ;
(4.5)

4.2
Supervised Regularization Based Robust Subspace (SRRS)
51
where  is a trade-off parameter, k:kF is the Frobenius norm, I is an identity matrix
in Rnn, and Hb and Ht are two constant coefﬁcient matrices. In detail, Hb.i; j/ D 1
nc
only if xi and xj belong to the same class, where nc is the number of samples in each
class; otherwise, Hb.i; j/ D 0. Ht.i; j/ D 1
n.
The supervised regularization term Of.P; Z/ is convex with respect to Z. We will
provide theoretical analysis to prove it in the next section.
Orthogonality in a subspace means that any two basis vectors in this subspace
are orthogonal to each other, which has the advantages of compactness and reducing
redundancy. To this end, an orthogonal constraint PTP D Ip is incorporated into our
framework, where Ip is an identity matrix in Rpp. By combining equations (4.3)
and (4.5), we obtain the objective function as follows
min
Z;P kZk C 1.
PTAZ.I  Hb/
2
F

PTAZ.Hb  Ht/
2
F C 
PTAZ
2
F/;
s:t: X D AZ; PTP D Ip:
(4.6)
Note that our objective function in (4.6) is not convex with respect to P, because
of the orthogonal constraint PTP D Ip.
In real-world applications, as we discussed in Sect. 4.1, data usually contain
considerable noise. To obtain robust subspaces, we should identify noisy informa-
tion in raw data, and learn reliable subspaces from the recovered noise-free data.
Speciﬁcally, we adopt the l2;1-norm (i.e., kk2;1) to model the noise contained in
data. l2;1-norm is a valid norm as it satisﬁes three conditions for a norm: (1) positive
scalability: k˛Ek2;1 D j˛j kEk2;1, where ˛ is a real scalar; (2) triangle inequality:
kB C Ek2;1  kBk2;1 C kEk2;1; (3) existence of a zero vector: if kEk2;1 D 0, then
A D 0. As kEk2;1 encourages the columns of E to be zero, the assumption in our
work is that some vectors in our data are corrupted while the others are clean. Then,
we have a constraint X D AZ C E, and rewrite the objective function as:
min
Z;E;P kZk C 2 kEk2;1 C 1.
PTAZ.I  Hb/
2
F

PTAZ.Hb  Ht/
2
F C 
PTAZ
2
F/;
s:t: X D AZ C E; PTP D Ip:
(4.7)
where kEk2;1 D
nP
jD1
s
dP
iD1
.ŒEij/2, and 2 is a trade-off parameter.
We have described how to jointly learn discriminative subspace and low-rank
representations. In the next section, we will introduce the optimization algorithm.
Other than Fisher criterion discussed above, other types of objectives, such as local-
ity preserving, can also be easily incorporated into our framework by reformulating
the regularization term Of.P; Z/.

52
4
Robust Subspace Learning
4.2.2
Theoretical Analysis
We theoretically analyze the convexity of supervised regularization term Of.P; Z/
with respect to Z, which is critical to ensure that our model is solvable using
ALM algorithms. In particular, to guarantee the convexity of (4.5), we provide the
following theorem.
Theorem 4.1 If 
>
1, the supervised regularization term
Of.P; Z/
D
PTAZ.I  Hb/
2
F 
PTAZ.Hb  Ht/
2
F C 
PTAZ
2
F is convex to Z when P is
ﬁxed.
Proof Let T D PTAZ, where PTA can be regarded as constant when optimizing Z.
We then can convert Of.P; Z/ to f.T/ as follows
f.T/ D kT.I  Hb/k2
F  kT.Hb  Ht/k2
F C  kTk2
F :
(4.8)
Now we can rewrite T as a column vector, T D Œr1; r2; : : : ; rnT, where ri is the
i-th row vector of T. Then f.T/ is equivalent to
f.T/ D
diag..I  Hb/T/T
2
2 
diag..Hb  Ht/T/T
2
2 C  kTk2
2 ;
(4.9)
where diag.K/ is to construct a block diagonal matrix with each block on the
diagonal being matrix K.
The convexity of f.T/ depends on whether its Hessian matrix r2f.T/ is positive
deﬁnite or not. r2f.T/ will be positive deﬁnite if matrix S is positive deﬁnite.
S D .I  Hb/.I  Hb/T  .Hb  Ht/.Hb  Ht/T C I:
(4.10)
Note that we have the equations HbHt D HtHb D Ht and HtHt D Ht. Then, we
can obtain
S D .1 C /I  2Hb C Ht:
(4.11)
To justify that if matrix S is positive deﬁnite, we employ the following lemma.
Lemma 4.1 (Weyl’s Inequality; Theorem 1 [30])
Let G denote an n by n
Hermitian matrix, the ordered eigenvalues of G are 1.G/      n.G/. If B,
C are n by n Hermitian matrices, then n.B/ C n.C/  n.B C C/.
Lemma 4.1 tells us the smallest eigenvalue of matrix .B C C/ is greater than or
equal to the sum of the smallest eigenvalues of B and C. In our problem, we need to
make S positive deﬁnite, which means the smallest eigenvalue of S should be greater
than 0. Thus, we employ Lemma 4.1 to evaluate the equation (4.11). The minimal
eigenvalues of Hb and Ht are 1 and 0, so we should ensure:
.1 C /  2 C 0 > 0:
(4.12)

4.2
Supervised Regularization Based Robust Subspace (SRRS)
53
Hence, we have  > 1 from the above equation, which could guarantee that f.T/
is convex to T. Recall that T D PTAZ and PTA is a constant. Therefore, we can
further conclude that f.P; Z/ is convex to Z when  > 1 and P is ﬁxed.
4.2.3
Optimization
To solve (4.7), we adopt the recently proposed inexact augmented Lagrange
multiplier (ALM) algorithm [26]. Firstly, we add a variable J and a new constraint
Z D J to relax the original problem
min
Z;E;P;J kJk C 2 kEk2;1 C 1.
PTAZ.I  Hb/
2
F

PTAZ.Hb  Ht/
2
F C 
PTAZ
2
F/;
s:t: X D AZ C E; PTP D Ip; Z D J:
(4.13)
Furthermore, (4.13) can be converted to the following problem
min
Z;E;J;P;Y;R kJk C 2 kEk2;1 C 1.
PTAZ.I  Hb/
2
F

PTAZ.Hb  Ht/
2
F C 
PTAZ
2
F/
CTr.YT.X  AZ  E// C Tr.RT.Z  J//
C 
2 .kX  AZ  Ek2
F C kZ  Jk2
F/;
s:t: PTP D Ip:
(4.14)
where 
 > 0 is a penalty parameter, Y 2 <dn and R 2 <mn are Lagrange
multipliers.
To solve (4.14), we alternately update the variables P, J, Z, and E. First, we
learn a subspace P given an initialized low-rank representation matrix Z. Second,
on the ﬁxed subspace P, we update the low-rank representation matrix J, Z
and the noise matrix E. Although the convergence of inexact ALM algorithm
cannot be guaranteed when there are three or more variables, some theoretical
results have been presented to ensure the convergence with mild conditions [28].
In addition, we demonstrate the convergence properties of our algorithm in the
experiments.

54
4
Robust Subspace Learning
4.2.3.1
Learn Subspace P on Fixed Low-Rank Representations
We ﬁrst discuss how to optimize P while ﬁxing Z, J, and E. Note that kJk C
2 kEk2;1 CTr.YT.X AZ E//CTr.RT.Z J//C 
2 .kX  AZ  Ek2
F CkZ  Jk2
F/
can be regarded as constant.
The objective function w.r.t. P becomes
PkC1 D min
Pk
1.
PT
k AZk.I  Hb/
2
F 
PT
k AZk.Hb  Ht/
2
F C 
PT
k AZk
2
F/
s:t: PT
k Pk D Ip:
(4.15)
For simplicity, let Zwk D AZk.IHb/, Zbk D AZk.HbHt/. We derive the solution
to the projection vectors in Pk one by one. To obtain the i-th column in Pk (denoted
as Pk.W;i/), we rewrite (4.15) as
PkC1.W;i/ D min
Pk.W;i/ 1.kPT
k.W;i/Zwkk2
2  kPT
k.W;i/Zbkk2
2
CkPT
k.W;i/AZkk2
2/ C ˇi.PT
k.W;i/Pk.W;i/  1/;
(4.16)
where ˇi is the corresponding Lagrange multiplier.
By setting the derivative w.r.t. Pk.W;i/ to zero, we have
 1.ZwkZT
wk  ZbkZT
bk C AZkZT
k AT/Pk.W;i/ D ˇiPk.W;i/:
(4.17)
Therefore, Pk.W;i/ is the i-th eigenvector of matrix 1.ZwkZT
wk  ZbkZT
bk C
AZkZT
k AT/, corresponding to the i-th smallest eigenvalue.
4.2.3.2
Learn Low-Rank Representations Z on Fixed Subspace
Here we show how to update JkC1, ZkC1 and EkC1 when ﬁxing PkC1. After dropping
the irrelevant terms w.r.t. J, (4.14) can be rewritten as
JkC1 D min
Jk
kJkk C Tr.RT.Zk  Jk// C 
k
2 kZk  Jkk2
F
D min
Jk
1

k kJkk C 1
2 kJk  .Zk C .Rk=
k//k2
F :
(4.18)
Problem (4.18) can be effectively solved using the singular value thresholding
(SVT) operator [4]. SVT contains two major steps. First, we perform SVD on the
matrix S (S D Zk C .Rk=
k/), and get S D US˙SVS, where ˙S D diag.fig1ir/,
i are the singular values with rank r. Second, we can obtain the optimal
solution JkC1 by thresholding the singular values: JkC1
D US˝.1=
k/.˙S/VS,
where ˝.1=
k/.˙S/
D
diag.fi  .1=
k/gC/, and tC means the positive
part of t.

4.2
Supervised Regularization Based Robust Subspace (SRRS)
55
By ignoring terms independent of Z in (4.14), we have
min
Z;Y;R 1.
PTAZ.I  Hb/
2
F 
PTAZ.Hb  Ht/
2
F
C
PTAZ
2
F/ C Tr.YT.X  AZ  E//
CTr.RT.Z  J// C 
2 .kX  AZ  Ek2
F C kZ  Jk2
F/:
(4.19)
By setting the derivative w.r.t. Z to zero, we have
ZkC1D=
k C .ATPkC1PT
kC1A/1.I C ATA/ZkC1 D .ATPkC1PT
kC1A/1KkC1;
(4.20)
where D D 21..1 C /I  2Hb C HtT/, and KkC1 D JkC1 C AT.X  Ek/ C .ATYk 
Rk/=
k: Problem (4.20) is a standard Sylvester equation, which can be effectively
solved using existing tools [1].
Similarly, after dropping terms independent of E, we can rewrite (4.14) as
EkC1 D min
Ek
2

k kEkk2;1 C 1
2 kEk  .X  AZkC1 C Yk=
k/k2
F :
(4.21)
The solution to problem (22) is presented in [28]. Speciﬁcally, let  D XAZkC1 C
Yk=
k, the i-th column of EkC1 is
EkC1.W; i/ D
8
ˆ<
ˆ:
kik 2

k
kik
i; if 2

k < kik;
0;
otherwise:
(4.22)
As stated in the inexact ALM algorithm, we also need to update the Lagrange
multipliers Y and R, and the parameter 
 after optimizing the variables P, J, Z
and E.
4.2.4
Algorithm and Discussions
The above process is repeated until convergence. The detailed algorithm of our
optimization is outlined in Algorithm 4.1.
After obtaining the optimal solution P and Z, we project both training samples
and test samples onto P, and then utilize nearest neighbor (NN) classiﬁer to predict
the label vector of test samples. The complete procedures of our SRRS approach are
summarized in Algorithm 4.2.

56
4
Robust Subspace Learning
Algorithm 4.1 Solving problem (4.14) by inexact ALM
Input: data matrix X, parameter 1, 2, , Z D J D 0, E0 D 0, Y0 D 0,
R0 D 0, 
0 D 0:1, 
max D 1010,  D 1:3, k D 0, " D 108
Output: Pk; Zk; Ek
1:while not converged do
2: update PkC1 using (4.16), given others ﬁxed
If k D 1, then Zk D I.
3: update JkC1 using (4.18), given others ﬁxed
4: update ZkC1 using (4.20), given others ﬁxed
5: update EkC1 using (4.21), given others ﬁxed
6: update the multipliers YkC1 and RkC1
YkC1 D Yk C 
k.X  AZkC1  EkC1/
RkC1 D Rk C 
k.ZkC1  JkC1/
7: update the parameter 
kC1 by

kC1 D min.
k; 
max/
8: check the convergence conditions
kX  AZkC1  EkC1k1 < "
and
kZkC1  JkC1k1 < ":
9: k D k C 1
10: end while
Algorithm 4.2 SRRS approach
Input: Training sample set X with label vectors LX, test sample set Y,
low-rank coefﬁcients Z
Output: Predicted label vector LY for test samples.
1: Normalize each sample xi to unit-norm, xi D xi= kxik.
2: Use Algorithm 4.1 to solve problem (4.14) and obtain optimal solution P.
3: Project X and Y onto P:
QX D PTXZ, QY D PTY.
4: Predict the label vector LY of NY by using the nearest neighbor (NN) classiﬁer.
The time complexity of our approach mainly depends on the complexity of
Algorithm 4.1. In Algorithm 4.1, the most time-consuming steps are Setps 2–4. Step
2 and Step 3 cost O.n3/ due to the SVD decomposition, where n is the total number
of samples. The matrix inverse calculation in (4.20) costs O.n3/, and the state-of-
the-art solution to a Sylvester equation costs O.n3 C m3/ (In our case, m D n). In
all, the overall time complexity of our approach is O.tn3/, where t is the number of
iterations.
Formula (4) is actually a general framework for robust subspace learning and
feature extraction. In this chapter, we design a supervised regularization term Nf.P; Z/
by virtue of Fisher criterion. Other subspace learning baselines (e.g., LPP, NPE,
and LFDA) could also be extended under our framework by reformulating the
regularization term Nf.P; Z/.
In Algorithms 4.1 and 4.2, sample set X is utilized as dictionary (i.e., A D X).
When the sampling is insufﬁcient, learning an informative dictionary should

4.3
Experiments
57
enhance the classiﬁcation performance, which provides another interesting direction
of future work.
In the Step 3 of Algorithm 4.2, we project the recovered clean training images XZ
onto the subspace P. Ideally, we would also like to project the clean test images onto
P for classiﬁcation. However, it is usually not practical to obtain clean test images
in real applications. In this chapter, to show the robustness of P for noisy data, we
directly project noisy images onto P. To enhance the classiﬁcation performance,
one could apply some image denoising techniques before projecting noisy test data
onto P.
4.3
Experiments
The performance of our SRRS approach is evaluated on six benchmark datasets,
including object datasets [8, 32], face datasets [16, 34], and KinFace dataset [44].
We compare our approach with related methods on the robustness to different kinds
of noise including pixel corruption and large pose/illumination variations. Our code
is publicly available.2
4.3.1
Object Recognition with Pixel Corruption
We use two object datasets, COIL-100 [32] and ALOI [8], in this experiment.
The COIL dataset contains various views of 100 objects with different lighting
conditions. Each object contributes 72 images, which are captured in equally spaced
views. In our experiments, the images are converted to grayscale, resized to 3232,
and then the robustness is evaluated on alternative viewpoints. We normalize the
samples so that they have unit norm that is favorable for optimization. Unlike most
existing subspace learning experiments, we also test the robustness of different
methods to noise by adding 10% pixel corruption to the original images. Some
examples of corrupted object images in COIL dataset can be found in Fig. 4.9.
In the experiments, we compare the proposed approach with PCA [39], LDA
[2], NPE [10], LSDA [3], RPCA[5]+LDA, support vector machine (SVM) [40],
FDDL [46], Latent LRR (LatLRR) [27] and DLRD [29]. PCA and LDA are two
representative unsupervised and supervised subspace learning methods, and we
use them as our baseline. NPE can preserve the neighborhood structure of data,
which is less sensitive to outliers than PCA. Here we compare our method with
the supervised version of NPE. LSDA is a discriminant analysis method which
preserves both discriminant and local geometrical structural in the data. RPCA is
effective in removing noise from corrupted data. Here we incorporate it with LDA
2https://github.com/smilesheng/SRRS

58
4
Robust Subspace Learning
10−3
10−2
10−1
100
101
102
60
65
70
75
80
85
90
95
100
λ1
Recognition Rates (%)
SRRS on original data
SRRS on corrupted data
10−3
10−2
10−1
100
101
102
60
65
70
75
80
85
90
95
100
λ2
Recognition Rates (%)
SRRS on original data
SRRS on corrupted data
Fig. 4.2 Recognition rates of SRRS with different values of 1 and 2 on COIL dataset
as a baseline. SVM is a popular and powerful classiﬁer. Here we compare with the
nonlinear SVM classiﬁer with RBF kernel. FDDL is a dictionary learning method
that learns a discriminative dictionary using Fisher criterion. LatLRR and DLRD are
two low-rank modeling methods. LatLRR can effectively extract salient features for
image recognition, while DLRD learns a low-rank dictionary for face recognition.
Both of them also demonstrate stable performance under noisy conditions.
We randomly select 10 images per object to construct the training set, and the
test set contains the rest of the images. This random selection process is repeated
20 times, and we report the average recognition rates for each compared method. In
addition, we performed scalability evaluations, by increasing the number of objects
from 20 to 100. For our approach and each compared method, the parameters are
tuned to achieve their best performance via 5-fold cross-validation. Figure 4.2 shows
the performance of SRRS with different values of 1 and 2 when the number of
classes is 20. We can also observe that the performance is not very sensitive to the
settings of 2. Further, SRRS obtains its best performance when 1 D 101. It also
shows that, SRRS achieves the best performance on the original data and corrupted
data when 2 D 10 and 2 D 1, respectively.
Figure 4.3 shows the recognition rates of our approach and the compared
subspace methods (PCA, LDA, NPE and LSDA) versus varying feature dimensions.
It shows that our SRRS approach outperforms subspace methods in almost all cases.
When the images contain noise, the recognition rates of compared subspace methods
are severely degraded, but our approach can still obtain good results. Namely, the
subspace derived from our approach is robust to pixel corruption. Table 4.1 shows
the average recognition rates with standard deviations of all compared methods.
It can be observed from Table 4.1 that the recognition rates of our approach vary
slightly when the number of classes increases from 20 to 100.
The total average results are also summarized in Table 4.1. We can see that our
approach and LatLRR have lower deviations than other methods, which demon-
strates good scalability. When the images are corrupted, all traditional subspace
methods have difﬁculty obtaining reasonable results. However, three low-rank
modeling based methods achieve remarkable performance. In most cases, our SRRS
approach achieves the best recognition results. Moreover, we utilize other levels of

4.3
Experiments
59
5
10
15
20
25
30
0
20
40
60
80
100
Dimensions
Recognition Rates (%)
PCA
LDA
NPE
LSDA
Ours
10
20
30
40
50
60
0
20
40
60
80
100
Dimensions
Recognition Rates (%)
PCA
LDA
NPE
LSDA
Ours
(b)
(a)
10
20
30
40
50
60
70
80
0
10
20
30
40
50
60
70
80
90
Dimensions
Recognition Rates (%)
PCA
LDA
NPE
LSDA
Ours
10
20
30
40
50
60
70
80
90 100
0
10
20
30
40
50
60
70
80
90
Dimensions
Recognition Rates (%)
PCA
LDA
NPE
LSDA
Ours
(d)
(c)
5
10
15
20
25
30
0
20
40
60
80
100
Dimensions
Recognition Rates (%)
PCA
LDA
NPE
LSDA
Ours
10
20
30
40
50
60
0
20
40
60
80
100
Dimensions
Recognition Rates (%)
PCA
LDA
NPE
LSDA
Ours
(f)
(e)
10
20
30
40
50
60
70
80
0
10
20
30
40
50
60
70
80
90
Dimensions
Recognition Rates (%)
PCA
LDA
NPE
LSDA
Ours
10
20
30
40
50
60
70
80
90 100
0
10
20
30
40
50
60
70
80
90
Dimensions
Recognition Rates (%)
PCA
LDA
NPE
LSDA
Ours
(g)
(h)
Fig. 4.3 Recognition rates of our approach and compared subspace methods versus varying
feature dimensions on the original ((a)–(d)) and corrupted ((e)–(h)) COIL object database. Note
that LDA and our approach obtains at most c  1 features, where c is the number of classes. (a) 20
classes. (b) 40 classes. (c) 60 classes. (d) 80 classes. (e) 20 classes. (f) 40 classes. (g) 60 classes.
(h) 80 classes

60
4
Robust Subspace Learning
Table 4.1 Average recognition rates (%) with standard deviations of all compared methods on COIL object database
Original images
Methods
20 objects
40 objects
60 objects
80 objects
100 objects
Average
PCA [39]
86.42 ˙ 1.11
83.75 ˙ 1.12
81.01 ˙ 0.92
80.53 ˙ 0.78
82.75 ˙ 0.59
82.89 ˙ 2.36
LDA [2]
81.83 ˙ 2.03
77.08 ˙ 1.36
66.96 ˙ 1.52
59.34 ˙ 1.22
52.29 ˙ 0.30
67.50 ˙ 12.19
NPE [10]
82.24 ˙ 2.25
76.01 ˙ 1.04
63.22 ˙ 1.36
52.18 ˙ 1.44
30.73 ˙ 1.31
60.88 ˙ 20.47
LSDA [3]
82.79 ˙ 1.70
75.01 ˙ 1.14
62.85 ˙ 1.41
51.69 ˙ 2.05
26.77 ˙ 1.05
59.82 ˙ 21.94
RPCA C LDA
83.26 ˙ 1.52
78.39 ˙ 1.15
68.93 ˙ 0.86
60.73 ˙ 0.68
56.44 ˙ 0.73
69.55 ˙ 11.36
SVM [40]
86.52 ˙ 1.51
86.73 ˙ 1.40
82.30 ˙ 0.84
77.42 ˙ 1.12
81.91 ˙ 0.88
82.98 ˙ 3.84
FDDL [46]
87.29 ˙ 1.78
85.18 ˙ 1.10
83.52 ˙ 0.67
78.47 ˙ 1.09
76.23 ˙ 1.46
82.14 ˙ 4.64
LatLRR[27]
88.98 ˙ 0.85
88.45 ˙ 0.64
86.36 ˙ 0.52
84.67 ˙ 0.79
82.64 ˙ 0.60
86.22 ˙ 2.64
DLRD [29]
89.58 ˙ 1.04
86.79 ˙ 0.94
82.60 ˙ 1.06
81.10 ˙ 0.58
79.92 ˙ 0.93
84.00 ˙ 4.06
Ours
92.03 ˙ 1.21
92.51 ˙ 0.65
90.82 ˙ 0.43
88.75 ˙ 0.71
85.12 ˙ 0.33
89.85 ˙ 3.01
10% corrupted images
Methods
20 objects
40 objects
60 objects
80 objects
100 objects
Average
PCA [39]
71.43 ˙ 1.12
70.22 ˙ 1.56
69.80 ˙ 0.65
67.84 ˙ 0.83
65.68 ˙ 0.76
68.99 ˙ 2.26
LDA [2]
47.77 ˙ 3.06
45.89 ˙ 1.12
36.42 ˙ 1.12
27.13 ˙ 0.95
16.79 ˙ 0.34
34.80 ˙ 13.01
NPE [10]
50.75 ˙ 2.37
45.29 ˙ 1.24
31.49 ˙ 1.71
18.49 ˙ 0.98
14.25 ˙ 0.24
32.05 ˙ 16.02
LSDA [3]
53.36 ˙ 2.79
43.27 ˙ 1.94
31.61 ˙ 2.79
18.61 ˙ 1.50
13.10 ˙ 0.22
31.99 ˙ 16.73
RPCAC LDA
49.35 ˙ 1.55
53.26 ˙ 1.84
44.18 ˙ 2.65
29.92 ˙ 0.96
23.55 ˙ 0.46
40.05 ˙ 12.78
SVM [40]
80.44 ˙ 1.76
75.98 ˙ 1.15
72.27 ˙ 0.46
67.38 ˙ 0.81
65.00 ˙ 0.78
72.21 ˙ 6.27
FDDL [46]
70.17 ˙ 1.13
60.46 ˙ 0.79
49.88 ˙ 0.49
41.52 ˙ 0.71
40.24 ˙ 0.54
52.45 ˙ 12.78
LatLRR[27]
81.38 ˙ 1.25
81.93 ˙ 0.92
80.97 ˙ 0.45
77.15 ˙ 0.72
73.47 ˙ 0.62
78.98 ˙ 3.61
DLRD [29]
82.96 ˙ 1.81
80.77 ˙ 0.95
76.93 ˙ 1.25
74.03 ˙ 0.74
73.82 ˙ 0.77
77.70 ˙ 4.07
Ours
86.45 ˙ 1.12
82.03 ˙ 1.31
82.05 ˙ 0.87
79.83 ˙ 0.62
74.95 ˙ 0.65
81.06 ˙ 4.18

4.3
Experiments
61
0
10
20
30
40
50
10
20
30
40
50
60
70
80
90
100
Percentage of Noise (%)
Recognition Rates (%)
PCA [1]
LDA [2]
NPE [4]
LSDA [5]
RPCA+LDA
SVM [54]
LatLRR [9]
DLRD [25]
FDDL [55]
Ours
Fig. 4.4 Average recognition rates of all compared methods on COIL database with different level
of noise
corruption such as 20%, 30%, 40% and 50% on COIL-20 database, and report the
results in Fig. 4.4. It shows that our SRRS approach consistently outperforms other
methods.
We also performed a signiﬁcance test, McNemar’s test, for the results shown
in Table 4.1, in order to demonstrate the statistical signiﬁcance of our approach
compared with several of the most representative state-of-the-art methods. We use
a signiﬁcance level of 0.05. In another word, the performance difference between
two methods is statistically signiﬁcant, if the estimated p-value is lower than 0.05.
Table 4.2 shows the p-values of comparing SRRS with other methods. From this
table, the following conclusions can be reached: (1) The performance differences
between our approach and the methods (PCA, LDA, NPE, LSDA, RPCA C LDA,
SVM and FDDL) are statistically signiﬁcant in all cases. (2) On the original
dataset, the performance differences between our approach and DLRD/LatLRR are
statistically signiﬁcant. (3) On the corrupted dataset, the performance differences
between our approach and DLRD/LatLRR are not statistically signiﬁcant. The
reason is that DLRD and LatLRR are also able to handle the noisy data. But our
approach achieves higher recognition rates than them.
The ALOI dataset contains 1000 general object categories taken at different
viewing angles. There are 72 equally spaced views in each category. In our
experiments, we select the ﬁrst 300 objects from this dataset. All the images are
converted to gray-scale and resized to the size of 36  48. We also add 10% pixel
corruption on the original images to evaluate the performance of different methods.
Some examples of corrupted images in the ALOI dataset can be found in Fig. 4.9.
Ten images of each object are randomly selected as training samples, and the
others as test samples. This random selection process was repeated 20 times.
Figure 4.5a shows the convergence curves of our approach on the original data and
corrupted data. It shows that the relative error on corrupted data (10% noise) is
larger than that on the original data. But, in both cases, our approach converges very

62
4
Robust Subspace Learning
Table 4.2 p-value between SRRS and other methods on the COIL object database. The asterisk * indicates that the difference between Method A and Method B
is statistically signiﬁcant when p D 0:05
Original images
Corrupted images
Method A vs. B
20 objects
40 objects
60 objects
80 objects
20 objects
40 objects
60 objects
80 objects
Ours vs. PCA [39]
1.0106
3.2108
2.3109
2.5107
1.2108
2.31010
3.51010
3.11010
Ours vs. LDA [2]
1.5107
5.5109
4.7109
2.4108
1.1109
1.11010
5.41010
1.71010
Ours vs. NPE [10]
2.3108
2.7108
1.6108
5.2109
1.6109
4.91010
2.91011
5.11010
Ours vs. LSDA [3]
1.3108
2.0108
2.5108
2.3108
1.2109
4.11010
3.81010
2.01010
Ours vs. RPCA C LDA
3.4108
3.2108
5.0108
2.9108
4.7109
2.51010
1.31010
1.11010
Ours vs. SVM [40]
3.1105
5.8106
1.2105
3.3108
1.0109
2.2109
1.8109
1.21010
Ours vs. FDDL [46]
2.7106
4.6107
3.3108
1.2107
2.1109
2.91010
1.4109
1.7109
Ours vs. LatLRR [27]
3.5105
2.1106
4.9105
3.5102
0:0132
0.0511
0.0920
0:0283
Ours vs. DLRD [29]
0:0279
7.0104
2.1108
1.4108
0:1325
3.1105
2.4105
1.3104

4.3
Experiments
63
0
10
20
30
40
50
60
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Number of Iterations
Relative Error
SRRS on original data
SRRS on corrupted data
1.2
1.5
1.8
2.1
2.4
2.7
3
75
80
85
90
95
η
Recognition Rates (%)
(a) Convergence property
(b) Parameter selection (η)
Fig. 4.5 Properties of our approach on ALOI dataset. (a) Convergence curve ( D 1:3, 
 D 0:1
and " D 108). (b) Recognition rates of SRRS with different values of 
Table 4.3 Average
recognition rates (%) on
ALOI object database
Methods
Original images
10% corruption
PCA [39]
84.10 ˙ 0.51
22.99 ˙ 0.29
LDA [2]
83.46 ˙ 0.38
23.97 ˙ 0.37
NPE [10]
84.50 ˙ 0.43
24.83 ˙ 0.25
LSDA [3]
83.96 ˙ 0.35
24.05 ˙ 0.31
RPCA C LDA
84.62 ˙ 0.48
27.58 ˙ 0.41
SVM [40]
84.61 ˙ 0.45
30.29 ˙ 0.33
FDDL [46]
84.77 ˙ 0.72
23.03 ˙ 0.42
LatLRR [27]
84.97 ˙ 0.53
59.35 ˙ 0.48
DLRD [29]
85.53 ˙ 0.55
58.66 ˙ 0.39
Ours
87.91 ˙ 0.34
64.62 ˙ 0.32
well after ten iterations. The relative error is calculated by kX  AZ  EkF=kXkF.
Figure 4.5b shows the recognition rates of SRRS when parameter  is selected
from the range Œ03. We observe that SRRS is not very sensitive to the choice
of  when  > 1. Further, we set  to 1.5 to achieve the best recognition
performance. Table 4.3 shows the average recognition rates with standard deviations
for each compared method. It shows that SVM, FDDL and two low-rank methods
obtain better performance than traditional subspace methods, and our approach
outperforms all these methods on the original dataset and the corrupted dataset.
In addition, by comparing Tables 4.1 and 4.3, we can observe that, for the dataset
with a large number of classes, the classiﬁcation task becomes more difﬁcult when
data are corrupted.
4.3.2
Face Recognition with Illumination and Pose Variation
We also evaluate our approach on the Extended YaleB [16] and the FERET [34]
face databases. The YaleB face dataset consists of 2414 frontal face images of 38
classes, and each of them contains about 64 images. Figure 4.6a shows the examples

64
4
Robust Subspace Learning
Fig. 4.6 Sample images in (a) YaleB, (b) FERET and (c) KinFace datasets.
Table 4.4 Average
recognition rates (%) of all
compared methods on YaleB
and FERET face databases
Methods
YaleB
FERET
PCA [39]
72.57 ˙ 0.58
84.00 ˙ 2.11
LDA [2]
89.09 ˙ 0.91
77.63 ˙ 2.22
NPE [10]
86.01 ˙ 1.37
71.67 ˙ 1.95
LSDA [3]
92.94 ˙ 0.88
73.27 ˙ 3.01
RPCA C LDA
91.29 ˙ 1.16
79.03 ˙ 2.63
SVM [40]
94.93 ˙ 0.75
88.03 ˙ 2.04
FDDL [46]
95.10 ˙ 1.31
86.00 ˙ 2.51
LatLRR [27]
88.76 ˙ 1.26
84.27 ˙ 2.19
DLRD [29]
93.56 ˙ 1.25
83.33 ˙ 2.40
Ours
97.75 ˙ 0.58
89.84 ˙ 2.01
from the YaleB dataset. We crop and resize the images to the size of 28  32, and
normalize the pixel values to [0, 1].
As suggested in [27], we randomly select 30 images per class to construct the
training set, and test set contains the rest of the images. This random selection
procedure is repeated 20 times, and we show the average recognition rates in
Table 4.4. It can be observed that supervised methods perform much better than
the unsupervised method PCA. The reason is that PCA has a high sensitivity to
illumination effects contained in this database. Due to the low-rankness property,
unsupervised method LatLRR greatly improves the recognition rate of PCA. Also,
supervised low-rank method DLRD obtains higher recognition rate than LatLRR.
By incorporating supervised information and low-rankness property, our approach
can achieve an average recognition rate of 97.17% and outperform all the other
methods, which implies that our approach is robust to variation illumination.
To evaluate the robustness to noise of the different methods, we randomly choose
a percentage (from 10% to 50%) pixels and replace their values by random numbers
that are uniformly distributed on [0, 1]. Figure 4.7 shows that, in noisy scenarios,
low-rank modeling based methods (LatLRR, DLRD and our approach) consistently
obtain better performance than other methods. Speciﬁcally, our SRRS approach can
get the best performance.

4.3
Experiments
65
Fig. 4.7 Average recognition
rates of all compared methods
on YaleB database with
different level of noise
0
10
20
30
40
50
10
20
30
40
50
60
70
80
90
100
Percentage of noise (%)
Recognition Rate (%)
PCA [1]
LDA [2]
NPE [4]
LSDA [5]
RPCA+LDA
SVM [54]
FDDL [55]
LatLRR [9]
DLRD [25]
Ours
The FERET database contains 2,200 face images collected from 200 subjects,
and each subject has 11 images. These images were captured under various poses
and expressions. In this experiment, we randomly select the images from 50
individuals. Figure 4.6b provides images of one individual that show large pose
variations. The original size of each image is 384  256. We cropped and resized
them to the size of 30  25.
We randomly select 5 images of each individual as training samples, and
the remaining samples are regarded as test samples. Table 4.4 lists the average
recognition rates of all compared methods over 20 runs. It reﬂects that our approach
can improve the recognition results over existing methods. Interestingly, PCA can
outperform some supervised subspace methods on this database. A likely reason for
this is that large pose changes of one individual produce large intra-class variations,
which highly inﬂuence the performance of supervised methods.
4.3.3
Face Recognition with Occlusions
The AR face database contains over 4,000 facial images collected from 126
subjects. For each subject, there are 26 frontal face images, taken under different
illuminations, expressions, and facial occlusions in two separate sessions. In our
experiments, we strictly follow the experimental settings in [52], and conducted the
following three experiments.
Sunglasses: Some face images contain the occlusion of sunglasses, which are
considered as corrupted samples. To construct the training set, we choose seven
neutral images and one randomly selected image with sunglasses from each subject

66
4
Robust Subspace Learning
Table 4.5 Recognition rates
(%) of all compared methods
on AR face databases
Methods
Sunglasses
Scarf
Sunglasses C scarf
PCA [39]
51.75
48.83
42.00
LDA [2]
82.25
81.75
79.82
NPE [10]
83.75
82.58
80.35
LSDA [3]
83.66
81.83
79.12
SVM [40]
78.37
65.33
67.71
LatLRR [27]
76.52
75.24
76.11
DLRD [29]
85.26
83.01
81.56
LRDL [52]
87.21
83.96
82.15
Ours
85.84
86.98
86.23
(session 1). The test set contains the remaining neutral images (session 2) and the
rest of the images with sunglasses (2 images from session 1 and 3 images from
session 2). Thus, for each individual, there are 8 training images and 12 test images.
The sunglasses cover about 20% of the face image.
Scarf: We utilize the corrupted training images due to the occlusion of scarf. Using
a similar training/test setting as above, we have 8 training images and 12 test images
for each individual. The scarf covers about 40% of the face image.
Sunglasses C Scarf: Moreover, we consider the case where images contain both
sunglasses and scarf. We select all the 7 neutral images and two corrupted images
(one with sunglasses and the other with scarf) at session 1 for training. For each
individual, there are 17 test images in total.
Table 4.5 shows the recognition rates of compared methods in three different
scenarios. We can observe that LRDL obtains the best result in the Sunglasses case,
and our approach obtains the best results in the Scarf case and the mixed case.
Figure 4.9d shows that our approach can correctly recover the clean images from the
occluded face images. Therefore, we can train robust classiﬁers from the recovered
image set AZ.
4.3.4
Kinship Veriﬁcation
Kinship veriﬁcation is a recently investigated research topic, which aims at deter-
mining kin relationships from photos. It is still a very challenging task due to
large variations in different human faces. We also evaluate the performance of
our approach and related methods on kinship veriﬁcation. We conduct the kinship
veriﬁcation experiments on the UB KinFace database Version 2 [44, 45]. This
database contains 600 face images that can be separated into 200 groups, and
each group consists of children, young parents and old parents. Figure 4.6c shows
example images in the KinFace database, in which three columns (from left to right)

4.3
Experiments
67
Table 4.6 Veriﬁcation rates
(%) on UB KinFace database
(5-fold cross validation). “C
vs. Y” and “C vs. O” denote
child-young parent
veriﬁcation and child-old
parent veriﬁcation,
respectively
Methods
C vs. Y
C vs. O
PCA [39]
57.25 ˙ 2.59
56.75 ˙ 2.05
LDA [2]
47.58 ˙ 5.36
49.25 ˙ 7.27
NPE [10]
55.25 ˙ 3.01
55.75 ˙ 3.60
LSDA [3]
56.75 ˙ 3.24
57.00 ˙ 2.88
RPCA C LDA
56.25 ˙ 5.08
52.00 ˙ 4.56
SVM [40]
51.00 ˙ 2.05
49.25 ˙ 1.43
FDDL [46]
48.00 ˙ 2.21
51.46 ˙ 3.51
LatLRR [27]
55.00 ˙ 3.53
52.50 ˙ 2.76
DLRD [29]
53.63 ˙ 3.19
54.27 ˙ 2.88
Ours
61.79 ˙ 2.13
62.15 ˙ 2.23
represent the images of children, young parents and old parents, respectively. Given
two images of faces, our task is to determine whether they are an accurate child-
parent pair.
As suggested in [45], we employ the difference vectors between the child and
the parent as the features rather than directly compare children with their parents.
Speciﬁcally, in the experiments for children and old parents, we build 200 true child-
old parent pairs and 200 false child-old parent pairs. The experiments for children
and young parents are carried out in a similar manner. Then we conduct ﬁve-fold
cross validation for this veriﬁcation problem. At each round, 160 true pairs and 160
false pairs are used for training, and the rest are used for testing. Average veriﬁcation
rates are reported in Table 4.6. Our approach outperforms all the other methods.
In this binary classiﬁcation problem, some traditional supervised methods perform
very poorly.
4.3.5
Discussions
The experimental results show that, compared with traditional subspace learning
methods, our approach is robust to noise and large variations. The reason is that
low-rank property helps us obtain a better estimate of the underlying distribution
of samples from the recovered images, and then our approach learns a robust and
discriminative subspace. The resulting performance is better than the compared low-
rank modeling and dictionary learning methods.
Figure 4.8 illustrates why our approach performs so well by visualizing low-
rank representation coefﬁcients of LatLRR and our approach. In particular, we
show the coefﬁcients for representing two pairs of samples. One pair, samples A
and B, is selected from the same class, while the other pair, samples A and C,
from different classes. Figure 4.8a, b show that, in LatLRR, samples from the
same class contribute more in the representation, as the coefﬁcients within the
same class are a little larger than those in other classes. In some sense, LatLRR

68
4
Robust Subspace Learning
0
20
40
60
80 100 120 140 160 180 200
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Index of Representation Coefficients by LatLRR
Coefficients for Sample A
Coefficients for Sample B
Samples A and B belong to
the same class
0
20
40
60
80 100 120 140 160 180 200
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Index of Representation Coefficients by LatLRR
Coefficients for Sample A
Coefficients for Sample C
Samples A and C belong to
different classes
(a)
(b)
0
20
40
60
80 100 120 140 160 180 200
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Index of Representation Coefficients by Our Approach
Coefficients for Sample A 
Coefficients for Sample B
Samples A and B belong to
the same class
0
20
40
60
80 100 120 140 160 180 200
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Index of Representation Coefficients by Our Approach
Coefficients for Sample A
Coefficients for Sample C
Samples A and C belong to
different classes
(c)
(d)
Fig. 4.8 Visualization of low-rank representation coefﬁcients of two pairs of samples on FERET
database. (a) LatLRR: same class. (b) LatLRR: different classes. (c) Ours: same class. (d) Ours:
different classes
could discover the subspace membership of samples. Compared with LatLRR,
Figure 4.8c, d show that the coefﬁcients of same class are higher than others, which
implies our approach can clearly reveal the subspace structure. Since we incorporate
supervised regularization in our model, the low-rank representations as well as the
resulting subspace learnt by our approach should be more discriminative than that
of LatLRR.
Furthermore, Fig. 4.9 visualizes the corrupted images, recovered images and the
noisy part on ﬁve object and face databases. It shows that, although training images
(i.e., X) have large pose variations and corruptions, the recovered images AZ are
very similar to each other, which helps us learn a robust subspace for classiﬁcation.

References
69
Fig. 4.9 Visualization of corrupted images (X), recovered images (AZ) and noise (E) on ﬁve object
and face databases. (a) COIL. (b) ALOI. (c) FERET. (d) AR. (e) YaleB
4.4
Summary
In this chapter, a novel linear subspace learning approach, supervised regularization
based robust subspace (SRRS), is proposed for feature extraction and classiﬁcation.
The proposed approach iteratively learns robust subspaces from a low-rank learning
model, and naturally incorporates discriminative information. The convexity of the
supervised regularization term has been theoretically proven. Experimental results
on six benchmark datasets demonstrate the effectiveness of our approach com-
pared with the state-of-the-art subspace methods and low-rank learning methods.
Moreover, when the data contain considerable noise or variations, our approach can
improve the classiﬁcation performance.
In our future work, we will develop a divide-and-conquer version of SRRS
approach to make it scalable to larger datasets, and we would also like to design
dictionary learning algorithms to further enhance the classiﬁcation performance.
References
1. Bartels, R.H., Stewart, G.W.: Solution of the matrix equation ax C xb D c. Commun. ACM
15(9), 820–826 (1972)
2. Belhumeur, P.N., Hespanha, J.P., Kriegman, D.J.: Eigenfaces vs. Fisherfaces: recognition using
class speciﬁc linear projection. IEEE Trans. Pattern Anal. Mach. Intell. 19(7), 711–720 (1997)
3. Cai, D., He, X., Zhou, K., Han, J., Bao, H.: Locality sensitive discriminant analysis. In:
Proceedings of the 20th International Joint Conference on Artiﬁcial Intelligence, Vancouver,
pp. 708–713 (2007)
4. Cai, J.F., Candes, E.J., Shen, Z.W.: A singular value thresholding algorithm for matrix
completion. SIAM J. Optim. 20(4), 1956–1982 (2010)
5. Candès, E.J., Li, X.D., Ma, Y., Wright, J.: Robust principal component analysis? J. ACM 58(3),
11:1–11:37 (2011)
6. Candès, E.J., Recht, B.: Exact matrix completion via convex optimization. Found. Comput.
Math. 9(6), 717–772 (2009)
7. Chen, C.F., Wei, C.P., Wang, Y.F.: Low-rank matrix recovery with structural incoherence for
robust face recognition. In: Proceedings of the 25th IEEE Conference on Computer Vision and
Pattern Recognition, Rhode Island, pp. 2618–2625 (2012)

70
4
Robust Subspace Learning
8. Geusebroek, J.M., Burghouts, G.J., Smeulders, A.W.M.: The Amsterdam library of object
images. Int. J. Comput. Vis. 61(1), 103–112 (2005)
9. Guo, Y., Li, S., Yang, J., Shu, T., Wu, L.: A generalized Foley-Sammon transform based
on generalized Fisher discriminant criterion and its application to face recognition. Pattern
Recogn. Lett. 24(1–3), 147–158 (2003)
10. He, X., Cai, D., Yan, S., Zhang, H.: Neighborhood preserving embedding. In: Proceedings of
the 10th IEEE International Conference on Computer Vision, Beijing, pp. 1208–1213 (2005)
11. He, X., Yan, S., Hu, Y., Niyogi, P., Zhang, H.: Face recognition using Laplacianfaces. IEEE
Trans. Pattern Anal. Mach. Intell. 27(3), 328–340 (2005)
12. Jhuo, I., Liu, D., Lee, D.T., Chang, S.: Robust visual domain adaptation with low-rank
reconstruction. In: Proceedings of the 25th IEEE Conference on Computer Vision and Pattern
Recognition, Providence, pp. 2168–2175 (2012)
13. Jing, X., Li, S., Zhang, D., Lan, C., Yang, J.: Optimal subset-division based discrimination and
its kernelization for face and palmprint recognition. Pattern Recogn. 45(10), 3590–3602 (2012)
14. Keshavan, R.H., Montanari, A., Oh, S.: Matrix completion from noisy entries. J. Mach. Learn.
Res. 11, 2057–2078 (2010)
15. Lai, Z., Li, Y., Wan, M., Jin, Z.: Local sparse representation projections for face recognition.
Neural Comput. Appl. 23(7–8), 2231–2239 (2013)
16. Lee, K.C., Ho, J., Kriegman, D.: Acquiring linear subspaces for face recognition under variable
lighting. IEEE Trans. Pattern Anal. Mach. Intell. 27(5), 684–698 (2005)
17. Li, S.: Learning robust representations for data analytics. In: Proceedings of the Twenty-Fifth
International Joint Conference on Artiﬁcial Intelligence, New York, pp. 4010–4011. AAAI
Press (2016)
18. Li, S., Fu, Y.: Low-rank coding with b-matching constraint for semi-supervised classiﬁcation.
In: Proceedings of the 23rd International Joint Conference on Artiﬁcial Intelligence, Beijing,
pp. 1472–1478 (2013)
19. Li, S., Fu, Y.: Robust subspace discovery through supervised low-rank constraints.
In: Proceedings of the SIAM International Conference on Data Mining, Philadelphia,
pp. 163–171 (2014)
20. Li, S., Fu, Y.: Learning balanced and unbalanced graphs via low-rank coding. IEEE Trans.
Knowl. Data Eng. 27(5), 1274–1287 (2015)
21. Li, S., Fu, Y.: Learning robust and discriminative subspace with low-rank constraints. IEEE
Trans. Neural Netw. Learn. Syst. 27(11), 2160–2173 (2016)
22. Li, L., Li, S., Fu, Y.: Learning low-rank and discriminative dictionary for image classiﬁcation.
Image Vis. Comput. 32(10), 814–823 (2014)
23. Li, S., Shao, M., Fu, Y.: Multi-view low-rank analysis for outlier detection. In: Proceedings of
the SIAM International Conference on Data Mining, Vancouver (2015)
24. Li, S., Li, Y., Fu, Y.: Multi-view time series classiﬁcation: a discriminative bilinear projection
approach. In: Proceedings of the 25th ACM International on Conference on Information and
Knowledge Management, Indianapolis, pp. 989–998. ACM (2016)
25. Li, S., Li, K., Fu, Y.: Self-taught low-rank coding for visual learning. IEEE Trans. Neural Netw.
Learn. Syst. (2017)
26. Lin, Z., Liu, R., Su, Z.: Linearized alternating direction method with adaptive penalty for
low-rank representation. In: Proceedings the 25th Annual Conference on Neural Information
Processing Systems, Granada, pp. 612–620 (2011)
27. Liu, G., Yan, S.: Latent low-rank representation for subspace segmentation and feature
extraction. In: Proceedings of the 13th IEEE International Conference on Computer Vision,
Barcelona, pp. 1615–1622 (2011)
28. Liu, G., Lin, Z., Yan, S., Sun, J., Yu, Y., Ma, Y.: Robust recovery of subspace structures by
low-rank representation. IEEE Trans. Pattern Anal. Mach. Intell. 35(1), 171–184 (2013)
29. Ma, L., Wang, C., Xiao, B., Zhou, W.: Sparse representation for face recognition based on
discriminative low-rank dictionary learning. In: Proceedings of the 25th IEEE Conference on
Computer Vision and Pattern Recognition, Providence, pp. 2586–2593 (2012)
30. Merikoski, J.K, Kumar, R.: Inequalities for spreads of matrix sums and products. Appl. Math.
E-Notes 4, 150–159 (2014)

References
71
31. Mu, T., Goulermas, J.Y., Tsujii, J., Ananiadou, S.: Proximity-based frameworks for gener-
ating embeddings from multi-output data. IEEE Trans. Pattern Anal. Mach. Intell. 34(11),
2216–2232 (2012)
32. Nene, S.A., Nayar, S.K., Murase, H.: Columbia object image library (COIL-100). Technical
report CUCS-006-96 (1996)
33. Pan, Y., Lai, H., Liu, C., Yan, S.: A divide-and-conquer method for scalable low-rank latent
matrix pursuit. In: Proceedings of the 26th IEEE Conference on Computer Vision and Pattern
Recognition, Portland, pp. 524–531 (2013)
34. Phillips, P.J., Moon, H., Rozvi, S.: The FERET evaluation methodology for face recognition
algorithms. IEEE Trans. Pattern Anal. Mach. Intell. 22(10), 1090–1104 (2000)
35. Qiao, L., Chen, S., Tan, X.: Sparsity preserving projections with applications to face recogni-
tion. Pattern Recogn. 43(1), 331–341 (2010)
36. Shao, M., Castillo, C., Gu, Z., Fu, Y.: Low-rank transfer subspace learning. In: Proceedings of
the 12th IEEE International Conference on Data Mining, Brussels, pp. 1104–1109 (2012)
37. Shao, M., Kit, D., Fu, Y.: Generalized transfer subspace learning through low-rank constraint.
Int. J. Comput. Vis. 109(1–2), 74–93 (2014)
38. Talwalkar, A., Mackey, L.W., Mu, Y., Chang, S.-F., Jordan, M.I.: Distributed low-rank subspace
segmentation. In: Proceedings of the 14th IEEE International Conference on Computer Vision,
Sydney, pp. 3543–3550 (2013)
39. Turk, M., Pentland, A.: Eigenfaces for face recognition. J. Cogn. Neurosci. 3(1), 71–86 (1991)
40. Vapnik, V.: The Nature of Statistical Learning Theory, 2nd edn. Springer, New York (2000)
41. Wang, X., Tang, X.: A uniﬁed framework for subspace face recognition. IEEE Trans. Pattern
Anal. Mach. Intell. 26(9), 1222–1228 (2004)
42. Wang, Y., Xu, H., Leng, C.: Provable subspace clustering: when LRR meets SSC. In:
Proceedings the 27th Annual Conference on Neural Information Processing Systems, Lake
Tahoe, pp. 64–72 (2013)
43. Wright, J., Yang, A.Y., Ganesh, A., Sastry, S.S., Ma, Y.: Robust face recognition via sparse
representation. IEEE Trans. Pattern Anal. Mach. Intell. 31(2), 210–227 (2009)
44. Xia, S., Shao, M., Fu, Y.: Kinship veriﬁcation through transfer learning. In: Proceedings of
the 22nd International Joint Conference on Artiﬁcial Intelligence, Barcelona, pp. 2539–2544
(2011)
45. Xia, S., Shao, M., Luo, J., Fu, Y.: Understanding kin relationships in a photo. IEEE Trans.
Multimedia 14(4), 1046–1056 (2012)
46. Yang, M., Zhang, L., Feng, X.C., Zhang, D.: Fisher discrimination dictionary learning for
sparse representation. In: Proceedings of the 13th IEEE International Conference on Computer
Vision, Barcelona, pp. 543–550 (2011)
47. Yang, M., Zhang, L., Yang, J., Zhang, D.: Robust sparse coding for face recognition. In:
Proceedings of the 24th IEEE Conference on Computer Vision and Pattern Recognition,
Colorado, pp. 625–632 (2011)
48. Zhang, N., Yang, J.: Low-rank representation based discriminative projection for robust feature
extraction. Neurocomputing 111, 13–20 (2013)
49. Zhang, T., Tao, D., Li, X., Yang, J.: Patch alignment for dimensionality reduction. IEEE Trans.
Knowl. Data Eng. 21(9), 1299–1313 (2009)
50. Zhang, L., Yang, M., Feng, Z., Zhang, D.: On the dimensionality reduction for sparse
representation based face recognition. In: Proceedings of the 20th International Conference
on Pattern Recognition, Istanbul, pp. 1237–1240 (2010)
51. Zhang, L., Zhu, P., Hu, Q., Zhang, D.: A linear subspace learning approach via sparse coding.
In: Proceedings of the 13th IEEE International Conference on Computer Vision, Barcelona,
pp. 755–761 (2011)
52. Zhang, Y., Jiang, Z., Davis, L.S.: Learning structured low-rank representations for image
classiﬁcation. In: Proceedings of the 26th IEEE Conference on Computer Vision and Pattern
Recognition, Portland, pp. 676–683 (2013)
53. Zheng, Z., Zhang, H., Jia, J., Zhao, J., Guo, L., Fu, F., Yu, M.: Low-rank matrix recovery with
discriminant regularization. In: Proceedings of the 17th Paciﬁc-Asia Conference on Advances
in Knowledge Discovery and Data Mining II, Gold Coast, pp. 437–448 (2013)

Chapter 5
Robust Multi-view Subspace Learning
Abstract By virtue of the increasingly large amount of various sensors, informa-
tion about the same object can be collected from multiple views. These mutually
enriched information can help many real-world applications, such as daily activity
recognition in which both video cameras and on-body sensors are continuously
collecting information. Such multivariate time series (m.t.s.) data from multiple
views can lead to a signiﬁcant improvement of classiﬁcation tasks. However, the
existing methods for time series data classiﬁcation only focus on single-view data,
and the beneﬁts of mutual-support multiple views are not taken into account. In light
of this challenge, we propose a novel approach, named Multi-view Discriminative
Bilinear Projections (MDBP), for extracting discriminative features from multi-
view m.t.s. data. First, MDBP keeps the original temporal structure of m.t.s. data,
and projects m.t.s. from different views onto a shared latent subspace. Second,
MDBP incorporates discriminative information by minimizing the within-class
separability and maximizing the between-class separability of m.t.s. in the shared
latent subspace. Moreover, a Laplacian regularization term is designed to preserve
the temporal smoothness within m.t.s. Extensive experiments on two real-world
datasets demonstrate the effectiveness of our approach. Compared to the state-of-
the-art multi-view learning and m.t.s. classiﬁcation methods, our approach greatly
improves the classiﬁcation accuracy due to the full exploration of multi-view
streaming data. Moreover, by using a feature fusion strategy, our approach further
improves the classiﬁcation accuracy by at least 10%.
5.1
Overview1
Nowadays information about one object can be continuously collected from mul-
tiple views in many domains such as health care and entertainment, due to the
increasingly large amount of various sensors. The collected data can be represented
as multi-view multivariate time series, which could lead to signiﬁcant improvement
of data mining tasks like classiﬁcation. For instance, the daily activities of a subject
1This chapter is reprinted with permission from ACM. “Multi-View Time Series Classiﬁcation: A
Discriminative Bilinear Projection Approach”, ACM International Conference on Information and
Knowledge Management, 2016.
© Springer International Publishing AG 2017
S. Li, Y. Fu, Robust Representation for Data Analytics, Advanced Information
and Knowledge Processing, DOI 10.1007/978-3-319-60176-2_5
73

74
5
Robust Multi-view Subspace Learning
can be captured by video cameras, depth cameras, and on-body sensors. These
three sets of heterogeneous and dynamic measurements would provide mutually
enriched information, which are helpful for improving the performance of activity
recognition. In general, it has been well recognized that multi-view data usually
enhance the overall model performance than single-view data, as long as the
different views contain diverse information [3]. In this chapter, we focus on the
classiﬁcation of multi-view multivariate time series, which plays a central role in
extracting useful knowledge from the multi-view streaming data.
Although the classiﬁcation of time series data has been extensively studied during
the past decade, they are only designed for single-view data. Traditional methods
focus on the univariate time series (u.t.s.) classiﬁcation [18, 41], by deﬁning distance
measures (e.g., dynamic temporal wrapping (DTW) [40], recurrence plot [37], edit
distance [34] and elastic distance [31]), or extracting compact and effective features
(e.g., time series shapelets [44] and segment based features [45]). Furthermore, as
the streaming data might be characterized by multiple measurements simultaneously
and represented as multivariate time series (m.t.s.), some recent works try to extract
informative patterns from m.t.s., and have achieved promising results [20, 24, 35,
46]. They can be roughly categorized into three groups: (1) distance metric; (2)
classiﬁer design; (3) dimensionality reduction. The ﬁrst group of methods focus on
designing distance metrics for m.t.s., by considering the temporal dynamics and the
possible misalignment problem in m.t.s. [35]. The classiﬁer design methods usually
adapt the effective classiﬁers from other domains to m.t.s. classiﬁcation, such as
SVM [47], recurrent probabilistic neural network [12], convolutional nonlinear
component analysis [46], etc. The dimensionality reduction methods project high-
dimensional m.t.s. to a low-dimensional subspace by satisfying certain criteria.
Weng et al. employed two-dimensional singular value decomposition [39] and
locality preserving projections [38] for m.t.s. classiﬁcation. Li et al. designed
a common principal component analysis method for m.t.s. classiﬁcation. Other
interesting explorations on m.t.s. classiﬁcation include feature selection [32],
temporal abstraction [36], and tensor factorization [4]. However, these methods
cannot directly handle multi-view data, and the beneﬁts of mutual-support multiple
views are not taken into account.
On the other hand, multi-view learning has attracted increasing attention in recent
years [7, 27, 28, 42], since it sophisticatedly models the consistency and diversity
among multiple data views, and signiﬁcantly boosts the learning performance
than single-view methods. The popular multi-view learning algorithms are usually
categorized as co-training, multiple kernel learning, and subspace learning[42].
However, existing multi-view algorithms are not customized for time series clas-
siﬁcation, as they simply ignore the unique properties of time series, such as the
temporal smoothness.
To address the above challenges, we propose a novel approach, named Multi-
view Discriminative Bilinear Projections (MDBP), for multi-view m.t.s. classiﬁ-
cation [29]. Figure 5.1 illustrates the framework of our approach. MDBP aims to
extract discriminative features from multi-view m.t.s. data, and it models the view

5.1
Overview
75
Audio
Sensor
2
1.5
0.5
-0.5
-1
-1.5
-2
0
1
Video Sequence (Features)
Shared Latent Subspace
P1
Q1
……
Y1
Y2
Yc
P2
P3
Q2
Q3
Fig. 5.1 Framework of our MDBP approach. The Multimodal Spoken Word dataset contains
three data views, including video, audio, and magnetic sensors. MDBP maps multi-view data
onto a shared subspace through a pair of view-speciﬁc bilinear projections (i.e., Pv and Qv),
and incorporates discriminative information by enhancing the between-class separability. Yi is the
compact representation of i-th class in the latent space
consistency and temporal dynamics. First, we assume that a m.t.s. sample and its
counterparts observed in other views could share a compact representation in a low-
dimensional latent subspace, as they indeed represent the same object. To preserve
the original temporal structure in m.t.s., MDBP learns a pair of view-speciﬁc
bilinear projections, which separately reduce the dimensions of measurements and
timestamps. Second, MDBP enforces that samples belong to the same class share
the same latent representation in the shared subspace, which reduces the within-
class separability. Also, the latent representations of different classes are pushed
away from each other, in order to enhance the between-class separability. Third,
a Laplacian regularization term is designed to preserve the temporal smoothness
of m.t.s. after projection. An efﬁcient optimization algorithm based on gradient
descent is designed to solve the problem. We evaluate the classiﬁcation performance
of our approach and baseline methods on two real-world datasets including the
UCI Daily and Sports Activity dataset, and the Multimodal Spoken Word dataset.
Extensive results on both single-view and multi-view test scenarios demonstrate
the superiority of our approach over the state-of-the-art methods. Our approach
greatly improves the classiﬁcation accuracy due to the full exploration of multi-view
m.t.s. data.
Our work is closely related to the research topics of subspace learning, multi-
variate time series classiﬁcation, and multi-view learning. Subspace learning meth-
ods reduce the dimensionality of data through linear or nonlinear projections [8, 13,
15, 16, 21, 22]. The major differences between our approach and existing subspace

76
5
Robust Multi-view Subspace Learning
learning methods are: (1) our approach deals with the multi-view time series
data by modeling view consistency and temporal regularization; (2) our approach
incorporates a novel supervised regularization for classiﬁcation tasks. Multi-view
learning aims to extract shared knowledge from multiple data sources [9, 10, 17, 19,
26, 42]. However, existing multi-view learning algorithms do not take the temporal
information into account, which are not suitable for m.t.s. classiﬁcation. Our MDBP
approach incorporates a temporal smoothness regularization, and its effectiveness
has been validated by extensive experiments.
The main contributions of this chapter are summarized as follows.
•
We propose a discriminative bilinear projection approach, MDBP, for multi-
view multivariate time-series classiﬁcation. To the best of our knowledge, this
work is the ﬁrst attempt to apply multi-view dimensionality reduction for the
m.t.s. classiﬁcation problem.
•
We model the view consistency by projecting multi-view m.t.s. onto a shared sub-
space, and incorporate the discriminative regularization and temporal smoothness
regularization.
•
We conduct extensive experiments on two real-world datasets, which demon-
strate the effectiveness of our approach, compared to the state-of-the-art multi-
view learning methods and m.t.s. classiﬁcation methods.
5.2
Problem Deﬁnition
In this section, we ﬁrst present the deﬁnitions of multivariate time series (m.t.s.) and
multi-view m.t.s., and then formally deﬁne the problems of m.t.s. classiﬁcation and
multi-view m.t.s. classiﬁcation. Table 5.1 summarizes the notations used throughout
this chapter.
Table 5.1 Notations
Notations
Descriptions
Xvij
The j-th m.t.s. sample in i-class, v-th view
Pv
Bilinear projection for the v-th view
Qv
Bilinear projection for the v-th view
Yi
Representation of i-th class in latent space
Lp
Temporal Laplacian matrix
V
Number of views
C
Number of classes
Ni
Number of samples in the i-th class
dv
Dimensionality of m.t.s. sample in v-th view
mv
Length of m.t.s. sample in v-th view

5.3
Multi-view Discriminative Bilinear Projection (MDBP)
77
Deﬁnition 5.1 (Multivariate Time Series (m.t.s.)) A multivariate time series
(m.t.s.) X D Œx1;    ; xm 2 Rdm is an ordered sequence of d-dimensional vectors,
in which xi is the observation at the i-th timestamp, and m is the length of time
series.
Nowadays time series data are usually collected from multiple views or multiple
modalities. We deﬁne the multi-view multivariate time series as follows.
Deﬁnition 5.2 (Multi-View Multivariate Time Series (m.t.s.)) A multi-view
m.t.s. OX D fX.v/g; v D 1;    ; V is a set of time series data collected from multiple
views, where X.v/ 2 Rdvmv denotes the time series observed in the v-th view, dv is
the number of measurements of time series, mv is the length of time series, and V is
the total number of views.
For simplicity, we assume that the time series within the same view have been
synchronized and preprocessed, and therefore they have the same length mv.
We focus on the classiﬁcation task in this chapter. The formal deﬁnition of
multivariate time series classiﬁcation is as follows.
Deﬁnition 5.3 (Multivariate Time Series (m.t.s.) Classiﬁcation) Let C denote
a set of class labels, and C D jC j is the total number of classes. The task of
m.t.s. classiﬁcation is to learn a classier, which is a function F W X ! C , where X
is a set of m.t.s.
The traditional m.t.s. classiﬁcation algorithms are mainly designed for single-
view data, and they cannot directly handle the multi-view data. Although some
practical tricks might be adopted, such as vectorizing multi-view data to single-
view ones, we argue that considerable information might be discarded during this
process. In this chapter, we extend the single-view m.t.s. classiﬁcation problem to
the multi-view setting, and present the formal deﬁnition as follows.
Deﬁnition 5.4 (Multi-View Multivariate Time Series Classiﬁcation) Let OX D
fXi;vji D 1;    ; N; v D 1;    ; Vg denote a set of multi-view m.t.s., where N is the
number of m.t.s. in each view, and C D fC1;    ; Ccg denote a set of class labels
shared by V views. The task of multi-view m.t.s. classiﬁcation is to learn a classier
from OX, and therefore to infer the class label for test m.t.s. Xtest which might be
observed in any view.
We notice that the basic m.t.s. classiﬁcation can be considered as a special case of
multi-view m.t.s. classiﬁcation when there is only one view available. In the multi-
view case, the m.t.s. classiﬁcation problem becomes more challenging. For example,
the consistency between multiple views should be modeled.
5.3
Multi-view Discriminative Bilinear Projection (MDBP)
In this section, we propose the multi-view discriminative bilinear projections
(MDBP) approach for m.t.s. classiﬁcation. We ﬁrst introduce our motivation, and
then present the model details. Finally, the optimization algorithm with discussions
is provided.

78
5
Robust Multi-view Subspace Learning
5.3.1
Motivation
For multi-view m.t.s. classiﬁcation, several key problems should be taken into
account, including:
(1) How to build the consistency and interactions of m.t.s. from multiple views?
(2) How to extract discriminative features from multi-view m.t.s.?
(3) As the data size has to be increased in the multi-view case, how to improve the
computational efﬁciency of training and test?
We aim to address all of the above challenges by designing a multi-view
dimensionality reduction approach. The basic idea is to project multi-view data onto
a common subspace, and then perform the classiﬁcation of m.t.s. using the low-
dimensional representations. Our motivations of using dimensionality reduction are
three-folds. First, as multi-view data are usually drawn from diverse data spaces,
seeking common projections would allow us to bridge the gap between multiple
data views. Learning a shared subspace is also considered as a popular strategy in
multi-view learning [6, 10]. Second, speciﬁcally designed regularization functions,
such as discriminative regularization, can be naturally incorporated into the multi-
view dimensionality reduction framework. Third, we aim to learn linear projections
for each view, and therefore, the training and test would be efﬁcient.
5.3.2
Formulation of MDBP
We assume that a set of m.t.s. OX observed from V views belong to C different classes,
OX D fXvijjv D 1;    ; V; i D 1;    ; C; j D 1;    ; Nig, where Ni is the number of
samples in the i-th class for each view.
The general formulation of MDBP is
min
P;Q;Y ˚. OX; P; Q; Y/ C 1.Y/ C 2˝.P; X/;
(5.1)
where P and Q are bilinear projections, and Y is the low-dimensional representation
of OX. 1 and 2 are two trade-off parameters that balance the effects of different
terms. Equation (5.1) contains three components. The ﬁrst term ˚. OX; P; Q; Y/
represents the multi-view bilinear dimensionality reduction, which characterizes the
connections between different views. The second term .Y/ carries discriminative
regularization, and the last term ˝.P; X/ models the temporal smoothness. We will
detail the three components in the following.
5.3.2.1
Learning Shared Representations Across Views
We propose to learn bilinear projections for reducing the dimensionality of m.t.s.
Let Pv 2 Rdvp and Qv 2 Rmvq denote a pair of linear projections for the v-th
view, and then the m.t.s. Xvij can be transformed by

5.3
Multi-view Discriminative Bilinear Projection (MDBP)
79
Yvij D P>
v XvijQv;
(5.2)
where Yvij 2 Rpq is the low-dimensional representation of Xvij.
The major beneﬁts of employing bilinear projections are two-folds. First, bilinear
projections allow us to preserve the original structure of m.t.s., especially the
temporal structures, which makes it easier to incorporate temporal smoothness
regularizations along the time dimension. Second, compared to other dimensionality
reduction methods, bilinear projections have less computational cost for both
training and test, which is suitable for dealing with long-duration time series data.
Equation (5.2) assumes that each view shares a pair of linear projections. How-
ever, it doesn’t take view correlation into account. A more reasonable assumption is
that, a sample and its counterparts collected from other views could have the same
low-dimensional representation in a common subspace. Moreover, as we focus on
classiﬁcation tasks, we further assume that samples from the same class, no matter
which views they belong to, would share approximately the same representations in
the common subspace. Therefore, we rewrite Eq. (5.2) as: Yi  P>
v XvijQv, which
encourages samples of the same class from all the views to be as close as possible
in the common subspace.
Then we formulate the multi-view dimensionality reduction term ˚. OX; P; Q; Y/ as
˚. OX; P; Q; Y/ D
C
X
iD1
V
X
vD1
Ni
X
jD1
Xvij  PvYiQ>
v
2
F ;
(5.3)
where k  kF is the matrix Frobenius norm. Here we assume that the projections
Pv and Qv are semi-orthogonal matrices, i.e., P>
v Pv D Ip and Q>
v Qv D Iq, where
Ip 2 Rpp and Iq 2 Rqq are two identity matrices
5.3.2.2
Incorporating Discriminative Regularization
For classiﬁcation tasks, the learned low-dimensional representations via dimension-
ality reduction should be discriminative. Actually, ˚. OX; P; Q; Y/ in Eq. (5.3) already
makes use of the label information, as it maps the same-class samples onto a station-
ary point in the low-dimensional common space. It implicitly incorporates discrim-
inative information, however, the separability among classes hasn’t been included,
which is a key for classiﬁcation problems as suggested by the Fisher criterion [8].
Therefore, to explicitly incorporate the discriminative information, we push the
low-dimensional representations of different classes, Yi and Yk .i ¤ k/, far away
from each other. The discriminative regularization term .Y/ is deﬁned as
.Y/ D 
C
X
iD1
C
X
kD1;k¤i
kYi  Ykk2
F :
(5.4)

80
5
Robust Multi-view Subspace Learning
As we need to maximize the summation of pairwise distances between Yi and Yk, a
negative sign is added in order to use .Y/ in the minimization problem Eq. (5.1).
The discriminative regularization shown in Eq.(5.4) is view-independent, as it
is implemented in the shared subspace. This strategy not only simpliﬁes the model
complexity, but also closely relates to the ﬁnal classiﬁcation task that is usually
performed in the low-dimensional subspace.
5.3.2.3
Modeling Temporal Smoothness
In reality, many types of time series data, such as human activities, slightly change
in successive timestamps, such as the smooth transitions of human activities [25]. In
other words, time series data own the property of locally smoothness, which brings
informative prior knowledge for learning models. By using bilinear projections, our
model does not break the temporal structures of input time series X, in which the
temporal smoothness is usually observed. However, after projecting X to a low-
dimensional subspace via Pv, the temporal smoothness might be undermined in the
projected data PvX.
To address this problem, we aim to design a smoothness regularization term on
PvXvk, where Xvk is the k-th sample in the v-th view. In light of the Laplacian
regularization [13], we propose a multi-view temporal Laplacian regularization
˝.Pv; Xvk/ to enforce the smoothness as follows
˝.Pv; Xvk/ D 1
2
N
X
i;jD1
Wij
P>
v Xvk.;i/  P>
v Xvk.;j/
2
2
D
N
X
iD1
P>
v Xvk.;i/DiiX>
vk.;i/Pv 
N
X
i;jD1
P>
v Xvk.;i/WijX>
vk.;j/Pv
(5.5)
D tr.P>
v XvkDX>
vkPv  P>
v XvkWX>
vkPv/
D tr.P>
v Xvk.D  W/X>
vkPv/
D tr.P>
v Xvk.LP/X>
vkPv/;
where Xvk.;i/ 2 Rd1 is the i-th column in Xvk, tr./ denotes the trace of a matrix,
W is a pre-deﬁned weight matrix that carries the smoothness prior, D is a diagonal
matrix whose entries are Dii D P
j Wij, and LP.D D  W/ is the Laplacian matrix.
Let Zvk denote the projected feature of Xvk, Zvk D P>
v Xvk. It is clear that each col-
umn in Xvk or Zvk corresponds to a timestamp. In reality, successive neighbors in Xvk
usually slightly change over time, which can be considered as prior information of
temporal smoothness. By setting a proper weighting matrix W, we can transfer such
temporal smoothness from Xvk to Zvk using the Laplacian regularization ˝.Pv; Xvk/.

5.3
Multi-view Discriminative Bilinear Projection (MDBP)
81
Let s denote the number of successive neighbors, the entry in W is computed as
Wij D
 1; if ji  jj  s
2
0; otherwise:
(5.6)
In this way, the successive columns in Zvk are encouraged to be similar to each
other. Note that we only adopt binary weights in Eq. (5.6). Other sophisticated graph
weighting schemes could also be employed to construct W.
Then, the regularization term ˝.P; X/ used in Eq. (5.1) is deﬁned as a summation
of ˝.Pv; Xvk/ over all of the views and samples
˝.P; X/ D
VP
vD1
NP
kD1
˝.Pv; Xvk/:
(5.7)
5.3.2.4
Objective Function
To sum up, the objective function of our MDBP approach is:
min
Pv;Qv;Yi f.Pv; Qv; Yi/ D
C
X
iD1
V
X
vD1
Ni
X
jD1
Xvij  PvYiQ>
v
2
F
1
C
X
iD1
C
X
kD1;k¤i
kYi  Ykk2
F C 2
V
X
vD1
N
X
kD1
tr.P>
v Xvk.LP/X>
vkPv/
s:t:
P>
v Pv D Ip; Q>
v Qv D Iq; v D 1;    ; V:
(5.8)
In Eq. (5.8), orthogonal constraints P>
v Pv D Ip and Q>
v Qv D Iq are incorporated.
Orthogonality in a projection matrix means that any two basis vectors in this
projection are orthogonal to each other, which has the advantages of compactness
and reducing redundancy.
5.3.3
Optimization Algorithm
We develop an efﬁcient optimization algorithm based on gradient descent to solve
the problem in Eq. (5.8).
Although Eq. (5.8) is not jointly convex to all the variables Pv; Qv and Yi, it is
convex to each of them when the other variables are ﬁxed. We use gradient descent
to alternately update each variable. Given P.t/
v ; Q.t/
v ; Y.t/
i
obtained in the t-th step, the
update rules at the t C 1 step are
P.tC1/
v
D P.t/
v  
@
@Pv f.Pv; Qv; Yi/; v D 1;    ; V;
(5.9)

82
5
Robust Multi-view Subspace Learning
Q.tC1/
v
D Q.t/
v  
@
@Qv f.Pv; Qv; Yi/; v D 1;    ; V;
(5.10)
Y.tC1/
i
D Y.t/
i
  @
@Yi f.Pv; Qv; Yi/; i D 1;    ; C;
(5.11)
where  is the learning rate.
The detailed derivatives are shown below
@
@Pv D 
CP
iD1
Ni
P
jD1
2.Xvij  PvYiQ>
v /QvY>
i C 2
NP
kD1
2P>
v Xvk.LP/X>
vk:
(5.12)
@
@Qv D 
CP
iD1
Ni
P
jD1
2Y>
i P>
v .Xvij  PvYiQ>
v /:
(5.13)
@
@Yi D 
CP
iD1
Ni
P
jD1
2P>
v .Xvij  PvYiQ>
v /Qv  1
CP
kD1;k¤i
2.Yi  Yk/:
(5.14)
Note that the orthogonal constraints shown in Eq. (5.8) are implemented by
a post-processing step during the update. The complete optimization algorithm
is summarized in Algorithm 5.1. We will show the convergence property of our
algorithm in the experiments.
After obtaining the subspaces Pv and Qv, the nearest neighbor classiﬁer can
be employed to classify a test m.t.s. Tv. The complete procedures of MDBP are
provided in Algorithm 5.2.
Algorithm 5.1 Solving problem in Eq. (5.8)
Input: Multi-view m.t.s. sample set OX, parameters 1, 2, s, , maxIter.
Output: Bilinear projections Pv, Qv, class-speciﬁc shared representation Yi.
1: Compute the Laplacian matrix Lp according to Eqs. (5.6) and (5.6);
2: Initialize Pv, Qv and Yi with random matrices;
3: for loop t from 1 to maxIter do
4:
for view v from 1 to V do
5:
Update projection Pv using Eq. (5.9);
6:
Orthogonalize Pv;
7:
Update projection Qv using Eq. (5.10);
8:
Orthogonalize Qv;
9:
end for
10:
for class i from 1 to C do
11:
Update latent presentation Yi using Eq. (5.11);
12:
end for
13:
if the objective converges then
14:
Return Pv, Qv and Yi.
15:
end if
16: end for

5.3
Multi-view Discriminative Bilinear Projection (MDBP)
83
Algorithm 5.2 MDBP approach
Input: Multi-view m.t.s. training sample set OX, single-view m.t.s. test sample Tv.
Output: Predicted class label ct for Tv
1: Normalize each time series sample;
2: Calculate the projections Pv and Qv using Algorithm 5.1;
3: Project Xvi; i D 1;    ; N, to the shared subspace by Zvi D P>
v XviQv;
4: Project Tv to the shared subspace by OZv D P>
v TvQv;
5: Predict the class label of Tv using NN classiﬁer, by comparing OZv with Zvi.
5.3.3.1
Time Complexity Analysis
The computational cost of Algorithm 5.1 mainly depends on the Step 6, Step 8, and
Step 12, which cost O.N.dpq C dqm C pm2//, O.N.dpq C dqm//, and O.N.pdm C
pmq//, respectively. Indeed, our algorithm reduces the dimensionality of time series,
which means p 	 d and q 	 m. Thus, the overall time complexity of the three steps
is simpliﬁed to O.N.dm C m2//.
In addition, our algorithm converges well after several iterations, and there are
usually a few views in reality. It indicates that our approach is approximately linear
to the sample size N when N  max.d; m/, and therefore, our approach can be
easily deployed for large-scale applications.
5.3.4
Comparison with Existing Methods
The ﬁrst term in Eq. (5.1),
CP
iD1
VP
vD1
Ni
P
jD1
Xvij  PvYiQ>
v
2
F, looks similar to the
formulation of matrix tri-factorization [23], which also factorizes a data matrix into
three unknown components. However, our approach is motivated from the multi-
view learning scenario, and the factorized components carry consistency constraints
across views or across classes. For instance, the view-speciﬁc projection Pv is shared
by every sample in the v-th view.
Although some existing multi-view learning algorithms also project multi-view
data to a common subspace [6, 10, 17], our approach differs from them in that:
(1) we employ the bilinear projections to map high-dimensional m.t.s. to a shared
low-dimensional subspace; (2) we design a novel discriminative regularization term
for multi-view dimensionality reduction. (3) we focus on the time series data
classiﬁcation, and design a Laplacian regularization term to enforce the temporal
smoothness.

84
5
Robust Multi-view Subspace Learning
5.4
Experiments
In this section, we conduct extensive experiments to evaluate the classiﬁcation
performance of our approach and baseline methods on two datasets, and perform
quantitative analysis on parameter sensitivity.
5.4.1
UCI Daily and Sports Activity Dataset
The UCI Daily and Sports Activity Dataset [2, 30] contains motion sensor data of
19 daily and sports activities, such as sitting, standing, walking, running, jumping,
etc. Each activity is performed by 8 subjects (4 female and 4 male, between the
ages 20 and 30) for 5 min. In particular, the subjects are asked to perform these
activities in there own styles without any restrictions. As a result, the time series
samples for each activity have considerable inter-subject variations in terms of speed
and amplitude, which makes it difﬁcult for accurate classiﬁcation. During the data
collection, nine sensors are put on each of the following ﬁve units: torso, right arm,
left arm, right leg, and left leg. Thus, there are 45 sensors in total, and each sensor
is calibrated to acquire data at 25 Hz sampling frequency. The 5-min time series
collected from each subject is divided into 5-s segments. For each activity, the total
number of segments is 480, and each segment is considered as a m.t.s. sample of
size 45  125, corresponding to 45 sensors and 125 timestamps.
5.4.1.1
Two-View Setting
We design a two-view experimental setting on the UCI Daily and Sports Activity
dataset. Speciﬁcally, the ﬁrst 27 sensors on torso, right arm and left arm are treated
as View-1, while the rest 18 sensors on right leg and left leg as View-2. The activities
are observed from two distinct views (i.e., two groups of sensors) simultaneously.
Also, the m.t.s. samples in two views have the same number of timestamps.
5.4.1.2
Baselines
Our MDBP approach is a multi-view dimensionality reduction method for time
series classiﬁcation. We mainly compare it with single-view and multi-view dimen-
sionality reduction methods. The single-view methods include principal component
analysis (PCA) [16], linear discriminant analysis (LDA) [8], locality preserving
projections (LPP) [38], and two-dimensional LDA (2DLDA) [43]. The multi-
view methods include canonical correlation analysis (CCA) [14] and multi-view
discriminant analysis (MvDA) [17]. In addition, we also compare our approach
with a popular classiﬁcation method, support vector machine (SVM) [5], and the

5.4
Experiments
85
state-of-the-art time series classiﬁcation method, one-nearest-neighbor dynamic
time warping (1NN-DTW) [40]. For all the baselines except 2DLDA, we have to
vectorize each m.t.s. sample into a single vector. Our approach and 2DLDA learn
linear projections without vectorizing m.t.s.
5.4.1.3
Classiﬁcation Scheme
Given a test time series sample Tv 2 Rdm that is only observed in the v-th view,
our approach maps it to a low-dimensional subspace using the learned projections
Pv and Qv. Then we employ the nearest neighbor (NN) classiﬁer to assign a class
label to Tv.
In addition, if a test time series and its counterparts are available in multiple
views, we could map them to multiple subspaces using the corresponding bilinear
projections Pv and Qv, and then perform NN classiﬁcation by adopting a feature
fusion strategy. We will evaluate the multi-view test cases in Sect. 5.4.3.
5.4.1.4
Results
There are 480 samples for each activity per view. We randomly choose Ntr samples
from each activity (per view) to construct the training set, and the remaining samples
are used to construct the test set. In particular, Ntr 2 f10; 20; 30; 40; 50g. For singe-
view baselines, we separately train two models on the training sets of two views, and
report the classiﬁcation accuracy on each view. For multi-view methods, we train the
model by jointly using samples from two views, and also report the accuracy on each
view. The parameters in our approach and baselines are tuned using 5-fold cross
validation on the training set. The learning rate  in our approach is empirically set
to 0.01. We will analyze the parameter sensitivity of our approach in Sect. 5.4.3.
We randomly choose Ntr training samples from each activity 10 times, and report
the average classiﬁcation accuracy of our approach and baselines in Table 5.2. Our
observations are:
•
For smaller training sets (e.g., Ntr D 10), the supervised dimensionality reduction
methods like LDA usually achieve higher accuracies than unsupervised methods
such as PCA and LPP. The reason is that unsupervised methods cannot accurately
estimate the data distribution without sufﬁcient sampling, while supervised
information used in LDA play a critical role in this scenario. When the training
set grows, PCA achieves comparable results than LDA, and LPP outperforms
LDA signiﬁcantly.
•
By preserving the original temporal structure of m.t.s. data, 2DLDA obtains the
best results among all the single-view methods, but it cannot make use of the
complementary information from multiple views.
•
The multi-view methods usually perform better than single-view methods.
For instance, the unsupervised multi-view method CCA always obtains higher

86
5
Robust Multi-view Subspace Learning
Table 5.2 Classiﬁcation Accuracy (%) on UCI Daily Activity Dataset. Ntr is the number of
training samples randomly chosen from each activity. V1 and V2 denote the View-1 and View-2,
respectively
Ntr D 10
Ntr D 20
Ntr D 30
Ntr D 40
Ntr D 50
Method
V1
V2
V1
V2
V1
V2
V1
V2
V1
V2
PCA [16]
27.63
21.73
31.17
23.34
32.01
24.03
33.04
24.56
33.97
24.86
LDA [8]
31.35
14.29
38.51
13.33
42.27
14.73
42.92
15.75
44.19
16.32
SVM [5]
22.32
20.80
21.45
18.05
21.47
17.58
21.51
17.75
21.18
18.09
LPP [38]
27.60
21.18
39.96
30.39
48.79
36.91
55.14
42.22
59.31
46.05
2DLDA [43]
53.37
55.24
67.59
64.70
73.15
70.55
76.09
72.13
78.93
75.79
DTW [40]
41.05
38.53
43.67
40.33
48.92
45.26
61.55
50.18
63.91
52.70
CCA [14]
28.36
18.05
43.10
20.02
51.61
22.28
56.92
24.07
60.14
26.84
MvDA [17]
56.43
57.98
75.03
74.24
81.20
74.77
80.37
76.95
85.88
81.08
MDBP (Ours)
70.29
67.93
82.58
77.31
87.55
81.81
89.83
83.42
91.35
84.45
Fig. 5.2 Classiﬁcation
accuracy (%) with different
dimensions on View-1 of UCI
Daily and Sports Activity
dataset
0
18
50
75
100
125
150
175
200 220
0
10
20
30
40
50
60
70
80
Dimension
Classification Accuracy (%)
PCA
LDA
LPP
2DLDA
CCA
MvDA
MDBP (Ours)
accuracies than PCA and LPP in the case of View-1; the supervised multi-view
method MvDA performs best among all the baseline methods, which demon-
strates the effectiveness of multi-view learning and supervised regularization.
•
Our approach achieves the highest classiﬁcation accuracy in every case. Com-
pared to MvDA, the accuracy is improved by at least 6% on average. It
demonstrates the superiority of incorporating discriminative information and
temporal smoothness to multi-view dimensionality reduction.
Choosing a proper dimension plays a key role in various dimensionality reduc-
tion methods, which is still an open problem to date. Figure 5.2 shows the
classiﬁcation performance of our approach and baselines with different dimensions,
when Ntr is set to 20, on the View-1 of UCI Daily and Sports Activity dataset. It

5.4
Experiments
87
shows that PCA achieves quite stable results when the dimension is higher than
60. LPP achieves its best performance when the dimension is around 50, and CCA
favors a higher dimension. Although LDA increases the accuracy signiﬁcantly with
more dimensions, it is limited by the maximum number of dimension which is less
than the number of classes. MvDA requires about 200 dimensions to achieve its best
performance. Our approach obtains good performance with only 60 dimensions, and
it consistently outperforms other baselines in each case.
5.4.2
Multimodal Spoken Word Dataset
The Multimodal Spoken Word dataset is collected to study the speaker-dependent
speech recognition problem, which helps us understand the speech translation for
assistive communication. One subject is asked to speak 73 words, and each word
is repeated for eight times. The speech of every word is recorded by three types of
signals, including audio, video, and magnetic sensors. For audio signals, we extract
20 different features from them, such as Linear Predictive Codings (LPC) [11] and
Mel-Frequency Cepstral Coefﬁcients (MFCC) [33]. The videos capture the face of
speaker during speech. We crop the mouth regions in the video, and extract the Local
Binary Pattern (LBP) [1] features from each frame. Twenty-four magnetic sensors
are placed on the tongue of the subject, which track the positions and movement
trajectories of the tongue during speech. Clearly, all of the three modalities can be
represented as multivariate time series.
5.4.2.1
Three-View Setting
A three-view experimental setting is designed on the Multimodal Spoken Word
dataset. The m.t.s. of sensors, video, and audio are separately denoted as View-1,
View-2 and View-3. We compare our approach with the baselines described in
Sect. 5.4.1. The m.t.s. within each view are preprocessed to have the same length.
In addition, MvDA requires that samples in different views should have the same
dimension, while our approach does not have such a constraint. For MvDA, we have
to perform preprocessing to make sure that samples in three views share the same
dimension.
5.4.2.2
Results
We randomly choose Ntr 2 f3; 4; 5g samples from each word (per view) to construct
the training set, and the remaining samples are used for the test. This process is
repeated for 10 times. Table 5.3 shows the average classiﬁcation accuracy of our
approach and baselines in different settings. We observe that traditional subspace

88
5
Robust Multi-view Subspace Learning
Table 5.3 Classiﬁcation accuracy (%) on Multimodal Spoken Word dataset. Ntr is the number of
training samples randomly chosen from each word. V1, V2 and V3 denote the View-1, View-2 and
View-3, respectively. SV and MV denote the single-view and multi-view methods, respectively
Ntr D 3
Ntr D 4
Ntr D 5
Method
V1
V2
V3
V1
V2
V3
V1
V2
V3
SV
PCA [16]
17.73
17.10
12.47
18.49
17.67
13.42
19.36
17.58
13.70
LDA [8]
11.86
13.78
13.97
11.61
14.66
12.88
11.74
15.34
14.06
SVM [5]
14.38
21.26
11.37
14.04
22.29
11.88
12.60
22.28
11.74
LPP [38]
14.71
13.01
12.93
16.20
13.80
13.01
16.55
13.79
12.51
2DLDA [43]
50.08
64.66
21.15
55.27
69.04
37.36
62.83
71.69
50.55
DTW [40]
53.71
65.29
25.45
59.59
58.90
38.47
65.20
72.05
52.33
MV
MvDA [17]
49.73
39.97
18.75
49.93
38.15
23.20
44.02
32.33
21.25
MDBP (Ours)
66.44
69.01
39.51
70.24
76.10
41.08
73.01
78.36
61.14
learning methods, such as PCA, LDA and LPP, obtain very poor performance on
this dataset, due to the small-sample-size problem. Moreover, the classiﬁcation
task on this dataset is more challenging than that on the UCI Daily and Sports
Activity dataset, as there are more classes. 2DLDA keeps the temporal structure of
raw m.t.s., and therefore it outperforms other single-view methods. MvDA obtains
poor performance on View-2 and View-3, due to the following reasons: (1) MvDA
constructs joint scatter matrices across different views, which works well on multi-
view data with similar types of features in each view, such as the UCI dataset used in
Sect. 5.4.1. However, the Multimodal Spoken Word dataset contains three different
types of signals, which can hardly be characterized by a joint scatter matrix. (2)
MvDA requires that samples in different views should have the same dimension,
which results in certain information loss. (3) MvDA breaks the temporal structure
by vectorizing the m.t.s. samples. Table 5.3 shows that our approach achieves
consistently better results than baselines on all the three views.
Figure 5.3 shows the accuracy of our approach and baselines with different
dimensions when Ntr is set to 3. Our approach obtains higher accuracy than other
baselines in most cases.
5.4.3
Discussions
5.4.3.1
Parameter Sensitivity and Convergence
There are three major parameters in our approach, including 1, 2 and s. The ﬁrst
two balance the effects of discriminative regularization and temporal smoothness
regularization, and parameter s denotes the number of sequential neighbors used to
construct the Laplacian matrix. Figure 5.4 shows the sensitivity of 1 and 2 on
the UCI Daily and Sports Activity dataset. We have the following observations: (1)
By setting either 1 or 2 to 0 (i.e., removing the regularization terms in Eq. (5.8)),

5.4
Experiments
89
Fig. 5.3 Classiﬁcation
accuracy (%) with different
dimensions on View-1 of
Multimodal Spoken Word
dataset
0
25
50
75
100
125
150
175
200 220
0
10
20
30
40
50
60
70
80
Dimension
Classification Accuracy (%)
PCA
LDA
LPP
2DLDA
MvDA
MDBP (Ours)
1
3
5
7
9
11
13
1
3
5
7
9
11
13
40
50
60
70
80
λ2
λ1
Accuracy
63
64
65
66
67
68
69
70
71
Fig. 5.4 Parameter sensitivity of 1 and 2 in our approach on UCI Daily and Sports Activity
dataset (View-1). The indexes from 1 to 13 on x/y axis correspond to parameters f0; 104; 5 
104; 103; 5  103; 102; 5  102; 0:1; 0:5; 1; 5; 10; 20g
the accuracy of our approach drops signiﬁcantly. It validates the effectiveness of
incorporating discriminative and temporal information into our approach. (2) Our
approach obtains relatively stable performance with the settings 1 2 Œ5  104; 1
and 2 2 Œ1; 20. Figure 5.5a shows the sensitivity of parameter s. It shows that our
approach is not very sensitive to the setting of s, and s D 2 usually leads to a better
performance.
Figure 5.5b shows the convergence curve of our approach on the UCI Daily and
Sports Activity dataset. Our approach quickly converges with only 25 iterations,
which makes it efﬁcient for large-scale applications.

90
5
Robust Multi-view Subspace Learning
Number of Sequential Neighbors (s)
2
4
6
8
10
12
14
16
Accuracy (%)
60
65
70
75
80
85
90
60
View-1
View-2
Iteration
1
5
10
15
20
25
Objective Value
×104
1.36
1.38
1.4
1.42
1.44
1.46
1.48
1.5
(a) Sensitivity of s
(b) Convergence curve
Fig. 5.5 (a) Parameter sensitivity of s and (b) convergence curve of our approach on UCI Daily
and Sports Activity dataset
Table 5.4 Fusion results on
UCI Daily and Sports
Activity dataset when
Ntr D 10
Method
Data fusion
Feature fusion
Single-view
PCA [16]
31.91
30.76
LDA [8]
18.01
24.40
SVM [5]
31.31
31.41
LPP [38]
9.90
23.15
2DLDA [43]
57.62
58.30
Multi-view
CCA [14]
–
25.90
MvDA [17]
–
67.24
Ours
–
78.96
5.4.3.2
Experiments with Data Fusion and Feature Fusion
In the above experiments, we assume that the test m.t.s. is only available in one
view, as shown in Tables 5.2 and 5.3. In practice, however, test m.t.s. might be
available in multiple views. For single-view methods, strategies like data fusion and
feature fusion can be applied to generate a ﬁnal prediction of class label. Multi-
view methods can adopt the feature fusion strategy. In data fusion, a m.t.s. observed
from multiple views are ﬁrst vectorized, and then concatenated to a long vector. In
feature fusion, the compact features are extracted from each view ﬁrst, and then
those feature vectors can be combined.
Table 5.4 shows the accuracy of our approach and baselines using one or two
available fusion strategies on the UCI Daily and Sports Activity dataset. Comparing
Tables 5.4 and 5.2, we observe that the accuracies of PCA, SVM, 2DLDA, and
MvDA can be improved by fusing data or features. LPP obtains better performance
with the feature fusion strategy. However, LDA cannot take advantages of the
fusion strategies, due to the performance gap between View-1 and View-2. Our

References
91
approach improves the classiﬁcation accuracy by at least 10% with the feature fusion
strategy. It indicates that the features extracted from two views have complementary
information that are useful for m.t.s. classiﬁcation.
5.5
Summary
In this chapter, we propose a multi-view bilinear projection approach named
MDBP for classifying m.t.s. that are collected from multiple views. MDBP projects
multi-view data to a shared subspace through view-speciﬁc bilinear projections
that preserve the temporal structure of m.t.s., and learns discriminative features
by incorporating a novel supervised regularization. The temporal smoothness is
also modeled in MDBP, with the help of Laplacian regularization. An efﬁcient
optimization algorithm based on gradient descent is designed to solve the problem.
We conduct extensive experiments on a daily activity benchmark dataset and a
recently collected multimodal spoken word dataset. Experimental results show
that our approach obtains remarkable improvements over the state-of-the-art multi-
view learning and multivariate time-series classiﬁcation methods. The parameter
sensitivity, convergence property and multi-view fusion are also evaluated and
discussed. In our future work, we will develop an online version of MDBP to deal
with multi-view m.t.s. in a real-time fashion.
References
1. Ahonen, T., Hadid, A., Pietikainen, M.: Face description with local binary patterns: application
to face recognition. IEEE Trans. Pattern Anal. Mach. Intell. 28(12), 2037–2041 (2006)
2. Altun, K., Barshan, B., Tunçel, O.: Comparative study on classifying human activities with
miniature inertial and magnetic sensors. Pattern Recognit. 43(10), 3605–3620 (2010)
3. Blum, A., Mitchell, T.: Combining labeled and unlabeled data with co-training. In: Proceedings
of the Eleventh Annual Conference on Computational Learning Theory, pp. 92–100. ACM
(1998)
4. Cai, Y., Tong, H., Fan, W., Ji, P., He, Q.: Facets: fast comprehensive mining of coevolving
high-order time series. In: Proceedings of the 21th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, pp. 79–88. ACM (2015)
5. Cortes, C., Vapnik, V.: Support vector machine. Mach. Learn. 20(3), 273–297 (1995)
6. Ding, Z., Fu, Y.: Low-rank common subspace for multi-view learning. In: IEEE International
Conference on Data Mining, pp. 110–119. IEEE (2014)
7. Fang, Z., Zhang, Z.: Simultaneously combining multi-view multi-label learning with maximum
margin classiﬁcation. In: Proceedings of IEEE International Conference on Data Mining,
pp. 864–869. IEEE (2012)
8. Fisher, R.A.: The statistical utilization of multiple measurements. Ann. Eugenics 8(4), 376–386
(1938)
9. Günnemann, S., Färber, I., Rüdiger, M., Seidl, T.: SMVC: semi-supervised multi-view
clustering in subspace projections. In: Proceedings of the 20th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, pp. 253–262. ACM (2014)

92
5
Robust Multi-view Subspace Learning
10. Guo, Y.: Convex subspace representation learning from multi-view data. In: Proceedings of the
27th AAAI Conference on Artiﬁcial Intelligence, vol. 1, p. 2 (2013)
11. Harma, A., Laine, U.K.: A comparison of warped and conventional linear predictive coding.
IEEE Trans. Speech Audio Process. 9(5), 579–588 (2001)
12. Hayashi, H., Shibanoki, T., Shima, K., Kurita, Y., Tsuji, T.: A recurrent probabilistic neural
network with dimensionality reduction based on time-series discriminant component analysis.
IEEE Trans. Neural Netw. Learn. Syst. 26(12), 3021–3033 (2015)
13. He, X., Niyogi, P.: Locality preserving projections. In: Advances in Neural Information
Processing Systems, pp. 153–160 (2004)
14. Hotelling, H.: Relations between two sets of variates. Biometrika 28(3/4), 321–377 (1936)
15. Jing, X.-Y., Li, S., Zhang, D., Yang, J., Yang, J.-Y.: Supervised and unsupervised parallel
subspace learning for large-scale image recognition. IEEE Trans. Circuits Syst. Video Technol.
22(10), 1497–1511 (2012)
16. Jolliffe, I.: Principal component analysis. Wiley Online Library (2002)
17. Kan, M., Shan, S., Zhang, H., Lao, S., Chen, X.: Multi-view discriminant analysis. IEEE Trans.
Pattern Anal. Mach. Intell. 38(1), 188–194 (2016)
18. Keogh, E., Kasetty, S.: On the need for time series data mining benchmarks: a survey and
empirical demonstration. Data Min. Knowl. Disc. 7(4), 349–371 (2003)
19. Lan, C., Huan, J.: Reducing the unlabeled sample complexity of semi-supervised multi-view
learning. In: Proceedings of the 21th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, pp. 627–634. ACM (2015)
20. Li, H.: Accurate and efﬁcient classiﬁcation based on common principal components analysis
for multivariate time series. Neurocomputing 171, 744–753 (2016)
21. Li, S., Fu, Y.: Robust subspace discovery through supervised low-rank constraints. In:
Proceedings of the SIAM International Conference on Data Mining, pp. 163–171 (2014)
22. Li, S., Fu, Y.: Learning robust and discriminative subspace with low-rank constraints. IEEE
Trans. Neural Netw. Learn. Syst. 27(11), 2160–2173 (2016)
23. Li, T., Sindhwani, V., Ding, C.H., Zhang, Y.: Bridging domains with words: opinion analysis
with matrix tri-factorizations. In: Proceedings of the SIAM International Conference on Data
Mining, pp. 293–302. SIAM (2010)
24. Li, K., Li, S., Fu, Y.: Early classiﬁcation of ongoing observation. In: IEEE International
Conference on Data Mining, pp. 310–319. IEEE (2014)
25. Li, S., Li, K., Fu, Y.: Temporal subspace clustering for human motion segmentation. In:
Proceedings of the IEEE International Conference on Computer Vision, pp. 4453–4461 (2015)
26. Li, Y., Nie, F., Huang, H., Huang, J.: Large-scale multi-view spectral clustering via bipartite
graph. In: Proceedings of the Twenty-Eighth AAAI Conference on Artiﬁcial Intelligence,
pp. 2750–2756 (2015)
27. Li, S., Shao, M., Fu, Y.: Cross-view projective dictionary learning for person re-identiﬁcation.
In: Proceedings of the 24th International Conference on Artiﬁcial Intelligence, pp. 2155–2161
(2015)
28. Li, S., Shao, M., Fu, Y.: Multi-view low-rank analysis for outlier detection. In: Proceedings of
the 2015 SIAM International Conference on Data Mining, pp. 748–756. SIAM (2015)
29. Li, S., Li, Y., Fu, Y.: Multi-view time series classiﬁcation: a discriminative bilinear projection
approach. In: Proceedings of the 25th ACM International on Conference on Information and
Knowledge Management, pp. 989–998. ACM (2016)
30. Lichman, M.: UCI Machine Learning Repository. School of Information and Computer
Sciences, University of California, Irvine (2013). http://archive.ics.uci.edu/ml
31. Lines, J., Bagnall, A.: Time series classiﬁcation with ensembles of elastic distance measures.
Data Min. Knowl. Disc. 29(3), 565–592 (2015)
32. Liu, R., Xu, S., Fang, C., Y.-w. Liu, Murphey, Y.L., Kochhar, D.S.: Statistical modeling and
signal selection in multivariate time series pattern classiﬁcation. In: The 21st International
Conference on Pattern Recognition, pp. 2853–2856. IEEE (2012)
33. Logan, B., et al.: Mel frequency cepstral coefﬁcients for music modeling. In: ISMIR (2000)

References
93
34. Marteau, P.-F., Gibet, S.: On recursive edit distance kernels with application to time series
classiﬁcation. IEEE Trans. Neural Netw. Learn. Syst. 26(6), 1121–1133 (2015)
35. Mei, J., Liu, M., Wang, Y., Gao, H.: Learning a mahalanobis distance-based dynamic time
warping measure for multivariate time series classiﬁcation. IEEE Trans. Cybern. 46(6), 1363–
1374 (2015)
36. Moskovitch, R., Shahar, Y.: Classiﬁcation of multivariate time series via temporal abstraction
and time intervals mining. Knowl. Inf. Syst. 45(1), 35–74 (2015)
37. Silva, D.F., De Souza, V., Batista, G.E.: Time series classiﬁcation using compression distance
of recurrence plots. In: IEEE 13th International Conference on Data Mining, pp. 687–696.
IEEE (2013)
38. Weng, X., Shen, J.: Classiﬁcation of multivariate time series using locality preserving
projections. Knowl. Based Syst. 21(7), 581–587 (2008)
39. Weng, X., Shen, J.: Classiﬁcation of multivariate time series using two-dimensional singular
value decomposition. Knowl. Based Syst. 21(7), 535–539 (2008)
40. Xi, X., Keogh, E., Shelton, C., Wei, L., Ratanamahatana, C.A.: Fast time series classiﬁcation
using numerosity reduction. In: Proceedings of the 23rd International Conference on Machine
Learning, pp. 1033–1040 (2006)
41. Xing, Z., Pei, J., Philip, S.Y.: Early classiﬁcation on time series. Knowl. Inf. Syst. 31(1), 105–
127 (2012)
42. Xu, C., Tao, D., Xu, C.: A survey on multi-view learning (2013). arXiv preprint
arXiv:1304.5634
43. Ye, J., Janardan, R., Li, Q.: Two-dimensional linear discriminant analysis. In: Advances in
Neural Information Processing Systems, pp. 1569–1576 (2004)
44. Ye, L., Keogh, E.: Time series shapelets: a new primitive for data mining. In: Proceedings of
the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
pp. 947–956. ACM (2009)
45. Zhang, Z., Cheng, J., Li, J., Bian, W., Tao, D.: Segment-based features for time series
classiﬁcation. Comput. J. 55(9), 1088–1102 (2012)
46. Zheng, Y., Liu, Q., Chen, E., Zhao, J.L., He, L., Lv, G.: Convolutional nonlinear neighbourhood
components analysis for time series classiﬁcation. In: Advances in Knowledge Discovery and
Data Mining, pp. 534–546. Springer, Berlin/New York (2015)
47. Zhou, P.-Y., Chan, K.C.: A feature extraction method for multivariate time series classiﬁcation
using temporal patterns. In: Advances in Knowledge Discovery and Data Mining, pp. 409–421.
Springer, New York (2015)

Chapter 6
Robust Dictionary Learning
Abstract The lack of labeled data presents a common challenge in many computer
vision and machine learning tasks. Semi-supervised learning and transfer learning
methods have been developed to tackle this challenge by utilizing auxiliary samples
from the same domain or from a different domain, respectively. Self-taught learning,
which is a special type of transfer learning, has fewer restrictions on the choice of
auxiliary data. It has shown promising performance in visual learning. However,
existing self-taught learning methods usually ignore the structure information in
data. In this chapter, we focus on building a self-taught coding framework, which
can effectively utilize the rich low-level pattern information abstracted from the
auxiliary domain, in order to characterize the high-level structural information in
the target domain. By leveraging a high quality dictionary learned across auxiliary
and target domains, the proposed approach learns expressive codings for the samples
in the target domain. Since many types of visual data have been proven to contain
subspace structures, a low-rank constraint is introduced into the coding objective to
better characterize the structure of the given target set. The proposed representation
learning framework is called Self-taught Low-rank coding (S-Low), which can be
formulated as a non-convex rank-minimization and dictionary learning problem.
We devise an efﬁcient majorization-minimization augmented Lagrange multiplier
(MM-ALM) algorithm to solve it. Based on the proposed S-Low coding mechanism,
both unsupervised and supervised visual learning algorithms are derived. Extensive
experiments on ﬁve benchmark datasets demonstrate the effectiveness of our
approach.
6.1
Overview1
The performance of visual learning algorithms is heavily dependent on the choice of
data representation [2]. Sparse coding [15, 45], dictionary learning [23, 25, 32, 47]
and low-rank learning [8, 20, 28, 30] have been widely used for representing
visual data. Good representations are expressive, meaning that a reasonably
sized dictionary (basis functions) can capture a huge number of possible input
1This chapter is reprinted with permission from IEEE. “Self-Taught Low-Rank Coding for Visual
Learning”, IEEE Transactions on Neural Networks and Learning Systems, 2017.
© Springer International Publishing AG 2017
S. Li, Y. Fu, Robust Representation for Data Analytics, Advanced Information
and Knowledge Processing, DOI 10.1007/978-3-319-60176-2_6
95

96
6
Robust Dictionary Learning
conﬁgurations, and also characterize a given set of data with certain global structural
blueprint (i.e. multiple clusters, subspaces, or manifolds). However, the lack of
training data presents a common challenge in many sophisticated representation
learning algorithms.
Traditionally, this problem was partially tackled by semi-supervised learning [49]
or transfer learning methods [10, 33, 35, 39]. Semi-supervised learning makes use
of the labeled sample set and a larger set of unlabeled samples, which are drawn
from the same domain with same distribution, to train a model. In other words,
semi-supervised learning can only solve learning problems in the same domain.
In transfer learning, this restriction is relaxed to some extent. The labeled samples
and auxiliary samples in transfer learning are drawn from different domains with
different distributions. But transfer learning requires that two domains should be
similar to each other. Most transfer learning methods assume that two domains
share a similar knowledge structure that deﬁnes the domain relatedness. In a word,
both semi-supervised learning and transfer learning usually put strong restrictions
on auxiliary (source) data, which limited their applicability. Recently, an emerging
machine learning topic of self-taught learning (STL) [4, 9, 12, 16, 37, 38, 44]
using unlabeled data with fewer restrictions holds signiﬁcant promise in terms of
enhancing the performance of image clustering and classiﬁcation. Raina et al. ﬁrst
proposed the concept of self-taught learning by applying sparse coding mechanism
to construct a higher-level representation from the unlabeled data [37, 38]. Lee et al.
extended Raina’s work by presenting a generalization of sparse coding module
which could be suited to model other data types drawn from any exponential
family distribution [16]. From the application point of view, Dai et al. proposed
a clustering algorithm in the spirit of self-taught learning by allowing the feature
representation from the auxiliary data to inﬂuence the target data through a common
set of features [4]. Kuen et al. employed the core idea of self-taught learning, and
transferred stacked auto encoders for visual tracking [12]. However, existing self-
taught learning methods do not take advantage of any global structure information
in the target set, as they encode each input signal independently. Besides, a
generalizable schema of self-taught learning for both supervised and unsupervised
learning tasks has not been well studied yet.
Self-taught learning and transfer learning are two related concepts [33]. The
key difference is that they place different restrictions on the auxiliary domain. In
particular, transfer learning only leverages labeled data from related homogenous
tasks (e.g., domain adaptation [34]), while self-taught learning relaxes such restric-
tion by utilizing arbitrary images (e.g., randomly downloaded images) to form
the auxiliary domain. The intuition behind self-taught learning is that randomly
selected visual data in an auxiliary domain can still contain the basic visual patterns
(such as edges, corners, atomic shapes) that are similar to those in the target
domain. The ﬂexibility of self-taught learning makes it particularly potential to
ever-increasing huge amount of unlabeled visual data. Existing self-taught learning
methods, however, simply ignore the structure information in the target domain,
which is critical in the visual learning tasks such as image classiﬁcation.
In this chapter, we propose a novel self-taught low-rank (S-Low) coding frame-
work for visual learning [21, 24]. By leveraging a high quality dictionary abstracted

6.1
Overview
97
from the wealth of information behind the auxiliary domain, we aim to learn expres-
sive high-level representations for the target domain. Since many types of visual data
are well characterized by subspace structure [18, 19, 22, 31, 50], we introduce a low-
rank constraint in our framework to take advantage of global structure information
in the target domain. Emphasizing such kind of structure information through low-
rank constraints in our approach could greatly beneﬁt broad visual learning tasks.
In particular, our approach is very suitable for addressing the tasks that leverage
on the exploitation of underlying data structure, such as object recognition, scene
classiﬁcation, face recognition, image clustering, etc. Especially when the target
data set is small, our approach is still able to extract effective feature representations
by virtue of large-scale unlabeled data in the auxiliary domain. The low-rank
constraint is also capable of removing noise and outliers from data [3, 29], which
helps us learn more robust representations in the target domain.
Figure 6.1 illustrates the diagram of our approach. Intuitively, we extract useful
building blocks from auxiliary domain in terms of a good characterization of under-
lying structure in the target domain. An expressive dictionary is learned by modeling
both auxiliary domain and target domain. In this process, the structure information
in target domain is enforced using low-rank constraints. More speciﬁcally, our
approach can be formulated as a rank-minimization and dictionary learning prob-
lem, and we design an effective majorization-minimization optimization algorithm
to jointly learn the dictionary and low-rank codings. Finally, the learned low-rank
codings correspond to the target domain can be directly used for clustering, or can
be employed to train a supervised model like support vector machines (SVM) for
classiﬁcation.
Moreover, some limitations of existing self-taught learning methods can be
addressed by the proposed method. First, existing methods always either loosely
combine representation learning and ﬁnal visual tasks [38], or tailor the algorithms
to particular applications [44]. Our approach could be easily applied to both
Auxiliary Domain
Target Domain
Classification
Coefficient matrix: 
= [
]
Clustering
Low-Rank Codings
Dictionary: 
Data:
= [
]
SVM
NCut
Airplane
Cow
Test Images
Irrelevant Images
Shared
Dictionary
Applications
=
+
Fig. 6.1 Diagram of the self-taught low-rank (S-Low) coding framework. A small target dataset
XT is usually not sufﬁcient to extract effective features. By utilizing the auxiliary dataset XS, the
proposed S-Low framework learns a shared dictionary D from two domains, and enforces a low-
rank constraint on the coefﬁcient matrix of target domain ZT that is considered as new feature
representations. Finally, the normalized cut (NCut) algorithm can be utilized for image clustering,
and the support vector machine (SVM) can be trained on ZT for image classiﬁcation

98
6
Robust Dictionary Learning
supervised and unsupervised learning tasks in a general way. Second, existing self-
taught methods learn new representations independently for each sample in the
target domain, where the important global structural information in the given set
is simply ignored. Our approach could effectively utilize the rich low-level pattern
information abstracted from the auxiliary domain to intelligently characterize the
high-level structure information in the target domain. It closely link the coding
procedure to the learning tasks.
Our work is closely related to two topics, including self-taught learning and
low-rank modeling. The most relevant method in the literature is the robust
and discriminative self-taught learning (RDSTL) [44]. RDSTL is a classiﬁcation
algorithm with self-taught nature by utilizing supervision information in the target
domain to discover the optimal dictionary basis vectors. There are signiﬁcant
differences between RDSTL and our approach. First, RDSTL does not consider the
global structure information in target domain, which is carefully modeled via low-
rank constraints in our approach. Second, the l2;1 norm used in RDSTL is a biased
estimator, while our approach employs the unbiased estimators including matrix -
norm and minimax concave penalty (MCP) norm. Third, RDSTL is designed for
classiﬁcation. We present both clustering and classiﬁcation algorithms using our
framework.
Some recent works introduced low-rank constraints into transfer learning prob-
lems [10, 39, 46]. Low-rank transfer subspace learning method imposes a low-rank
constraint on a low-dimensional subspace shared by source and target domains [39],
and low-rank domain adaptation method aims to reduce the domain distribution
disparity using low-rank representations [10]. A latent low-rank transfer learning
approach is proposed to tackle the missing modality recognition problem [5]. Most
recently, low-rank constraints are incorporated into deep learning architecture to
achieve transfer learning [6]. Our approach differs from them in three aspects. First,
these methods have strong restrictions in terms of using related homogenous tasks
in source and target domains, while our approach relaxes such restrictions. Second,
they cannot learn dictionaries due to their problem settings. Third, the knowledge
across different domains are transferred via a shared subspace, while our approach
transfers knowledge via a dictionary.
In summary, the major contributions of this chapter include the following:
(1) With the help of rich information from the auxiliary domain, we learn effective
feature representations, S-Low codings, by incorporating low-rank constraints
in the target domain.
(2) The proposed self-taught learning approach is a general framework, which
can be applied to various visual learning scenarios. In the chapter, we present
detailed algorithms for unsupervised learning and supervised learning.
(3) Instead of using the biased estimators like nuclear norm and l1 norm in many
existing low-rank matrix recovery methods, we relax the matrix rank and l0
norm in our model by two unbiased estimators, matrix -norm and mini-
max concave penalty (MCP) norm. An effective majorization-minimization

6.2
Self-Taught Low-Rank (S-Low) Coding
99
optimization algorithm is developed to solve our model. We also empirically
illustrate the convergence property of the optimization algorithm.
(4) Extensive experimental results on ﬁve benchmark datasets demonstrate that our
approach consistently outperforms several representative low-rank learning and
self-taught learning methods.
6.2
Self-Taught Low-Rank (S-Low) Coding
In this section, we formulate the proposed self-taught low-rank coding mechanism,
and develop our approach systematically. Then we present an effective optimization
algorithm to solve the model. Table 6.1 summarizes the notations used throughout
this chapter.
6.2.1
Motivation
Our goal is to take advantages of the abundant unlabeled data, in order to improve
the coding performance for various visual learning tasks. To achieve this goal, we
propose a self-taught low-rank (S-Low) coding framework, by leveraging a high
quality dictionary abstracted from the wealth of information behind the auxiliary
domain. Our intuition is that many types of visual data are well characterized by
subspace structure, and therefore it is possible to leverage on such information from
both auxiliary and target domains, and ﬁnally learn expressive high-level represen-
tations for the target domain. Speciﬁcally, we introduce a low-rank constraint in our
framework to take advantage of global structure information in the target domain.
Table 6.1 Notations
Notations
Descriptions
XS 2 Rdm
Unlabeled samples in auxiliary domain
XT 2 Rdn
Samples in target domain
D 2 Rdr
Dictionary
ZS 2 Rrm
Low-rank codings for auxiliary samples
ZT 2 Rrn
Low-rank codings for target samples
ES 2 Rdm
Sparse noise in auxiliary samples
ET 2 Rdn
Sparse noise in target samples
kk
Matrix -norm
M;./
Matrix concave penalty norm
d
Dimensionality of each sample
m
Number of auxiliary samples
n
Number of target samples
r
Size of dictionary

100
6
Robust Dictionary Learning
Emphasizing such kind of structure information through low-rank constraints could
greatly beneﬁt broad visual learning tasks especially clustering and classiﬁcation,
in which recognizing the underlying structure of a given sample set is our ultimate
goal. The low-rank constraint is also capable of removing noise and outliers from
data [3, 26, 29], which helps us learn more robust representations in the target
domain.
6.2.2
Problem Formulation
Considering the self-taught learning problem, we are given a set of abundant,
unlabeled samples, XS D fxS1;    ; xSmg 2 Rdm, in the auxiliary domain (or
source domain), and we also have limited samples in the target domain, XT D
fxT1;    ; xTng 2 Rdn. Our approach aims to learn expressive codings, in which the
subspace structural information is encoded, for the samples in the target domain.
Like other self-taught learning methods, we do not assume that the data from
auxiliary and target domains share the same (or similar) distributions. Furthermore,
we do not require that the samples are labeled in the target domain. Therefore, our
approach can be performed in either unsupervised or supervised fashions, which
differs from the problem settings in [38] and [44]. We will show that our approach
could deal with clustering problem if labels are unavailable in the target domain, or
classiﬁcation problem with labeled samples.
Traditionally, the sparse coding [15, 45], dictionary learning [32, 47] or low-rank
learning [28, 30] methods approximately represent the samples in a single domain
(i.e., the target domain):
XT  DTZT;
(6.1)
where ZT 2 Rrn is the representation coefﬁcient matrix and DT 2 Rdr is a
dictionary. r is the size of dictionary. Here ZT is usually expected to be sparse
or low-rank, according to the application scenario. Note that the dictionary DT is
often set as the sample set in some sparse representation and low-rank learning
methods [28, 30, 31, 45] (i.e., DT D XT), which may suffer the insufﬁcient sampling
problem.
With the help of auxiliary domain, we are able to learn a more informative
dictionary, and also tackle the insufﬁcient data sampling problem.
First, we can learn the dictionary from all the available samples in two domains.
The whole sample set is X D ŒXS XT. We aim to represent all samples in X using
a dictionary D 2 Rdr. Therefore, we introduce the constraint ŒXS XT D DŒZS ZT
C ŒES ET, where ZS 2 Rrm and ZT 2 Rrn are the coefﬁcient matrices
corresponding to auxiliary domain and target domain, respectively. ES and ET are
the sparse noise matrices that model the reconstruction errors in auxiliary and target
domains. The noise matrices ES 2 Rdm and ET 2 Rdn are often constrained using
the surrogate of l0 norm, such as l1 or l2;1 norms. In reality, target samples may

6.2
Self-Taught Low-Rank (S-Low) Coding
101
contain various types of noise. Considering the sparse noise matrices in the model
enables us to learn a robust dictionary.
Second, for many vision problems like clustering or classiﬁcation, samples in
the target domain usually lie in several underlying subspaces. Many recent research
efforts [18, 19, 27, 30, 31] have shown that enforcing low-rank constraint is an
effective way to discover those underlying subspace structure. Leveraging such
structure information can greatly beneﬁt the visual learning tasks. In light of this
observation, we impose a low-rank constraint on the coefﬁcient matrix ZT in the
target domain, where the learning tasks are performed. Then, our objective function
is formulated as follows:
min
D;ZS;ZT;
ES;ET
rank.ZT/ C 1 kESk0 C 2 kETk0 ;
s:t:
XS D DZS C ES; XT D DZT C ET;
(6.2)
where rank./ denotes the rank function, kk0 is the l0 norm, 1 and 2 are two
trade-off parameters to balance the effects of different terms.
The ﬁrst term in (6.2) characterizes the low-rankness of ZT in the target domain,
and the last two terms model the reconstruction errors. Equation (6.2) is a variant
of rank minimization problem that is NP-hard in general. Therefore, it cannot be
solved directly. In practice, the rank function and l0 norm can be relaxed by the
nuclear norm and l1 norm, respectively. Some convex optimization tools, such as
inexact augment Lagrange multiplier (ALM) algorithm, can achieve acceptable
performance. However, it has been noted that the nuclear norm and l1 norm are
biased estimators, as they over penalize large singular values and large entries [43].
To tackle this problem, we employ the non-convex surrogates of rank function and
l0 norm, which are matrix -norm and minimax concave penalty (MCP) norm,
respectively.
The matrix -norm for a matrix A 2 Rpq is deﬁned as [43]:
kAk D
s
X
iD1
i.A/
Z
0
.1  u
s /Cdu
D
s
X
iD1
1;.i.A// D M..A//;
 > 1;
(6.3)
where .A/ D .1.A/;    ; s.A//T denotes a function from Rpq to Rs
C, s D
min.p; q/. The matrix -norm is non-convex w.r.t A.
The matrix MCP norm is deﬁned as [48]:
M;.A/ D
X
i;j
;.Ai;j/;
(6.4)

102
6
Robust Dictionary Learning
where
;.t/ D 
t
Z
0
Œ1  x
Cdx D
8
<
:
2=2;
if jtj  
jtj  t2
2 ; otherwise:
ŒzC D max.z; 0/. Here, we choose  D 1, and denote M.A/ D M1;.A/ for
simplicity.
By replacing the rank function and l0 norm with matrix -norm and MCP norm,
the objective function (6.2) can be rewritten as:
min
D;ZS;ZT;
ES;ET
kZTk1 C 1M2.ES/ C 2M2.ET/;
s:t: XS D DZS C ES; XT D DZT C ET:
(6.5)
Third, the dictionary is jointly learned from both auxiliary and target domains, in
order to transfer useful knowledge from the auxiliary domain. The two constraints
in (6.5) share the same dictionary D. As the source dataset XS usually contains much
more samples than target dataset XT, the learning of dictionary is easily dominated
by the source data. However, it is more rational to emphasize the reconstruction
power of D in the target domain in which our learning task performs. Therefore, we
introduce an l2;1 norm constraint on the source coefﬁcient matrix ZS. In this way,
some rows in ZS are encouraged to be zero, which enables XS to adaptively select
bases from D. On the other hand, D is fully used to reconstruct samples in the target
domain.
After that, our objective becomes:
min
D;ZS;ZT;
ES;ET
kZTk1 C 1M2.ES/ C 2M2.ET/ C 3 kZSk2;1
s:t:
XS D DZS C ES; XT D DZT C ET;
(6.6)
where 3 is a trade-off parameter, and kZSk2;1 D
nP
jD1
s
dP
iD1
.ŒZSij/2 is the l2;1 norm.
Each column in the learned coefﬁcient matrix ZT corresponds to one sample in
the target domain, which is named low-rank coding in our chapter.
6.2.3
Optimization
In this section, we design a MM-ALM algorithm to solve (6.6). We ﬁrst introduce
the generalized singular value shrinkage operator S;	 and generalized shrinkage
operator D;W [43]:

6.2
Self-Taught Low-Rank (S-Low) Coding
103
S;	 D UXD;	.˙X/.V/T
X;
(6.7)
ŒD;W.A/ij D sgn.Aij/.jAij  Wijj/C;
(6.8)
where ˙ and 	 are non-negative matrices.
To facilitate the optimization, we add a relaxation variable J 2 Rrn to (6.6):
min
D;ZS;ZT;
ES;ET;J
kJk1 C 1M2.ES/ C 2M2.ET/ C 3 kZSk2;1
s:t:
XS D DZS C ES; XT D DZT C ET; ZT D J:
(6.9)
The MM-ALM algorithm consists of an outer loop and an inner loop. In each
iteration, the outer loop replaces the non-convex problem by its locally linear
approximation (LLA) to form a weighted convex problem, while an inner loop is
an inexact ALM algorithm.
In the outer loop, we reformulate the objective function as follows. Since the
objective function in Eq. (6.9) is concave w.r.t. ..J/; jESj; jETj/, we can approxi-
mate kJk1 C 1M2.ES/ C 2M2.ET/ by its LLA at ..J/old; jESjold; jETjold/, and
we obtain the following objective function:
min
D;ZS;ZT;
ES;ET;J
Q1..J/j.J/old/ C 1Q2.ESjEold
S / C 2Q2.ETjEold
T / C 3 kZSk2;1
s:t: XS D DZS C ES; XT D DZT C ET; ZT D J;
(6.10)
where
Q.AjAold/ D M.Aold/ C P
i;j
.1  jAold
ij j=/C.jAijj C jAold
ij j/:
is the LLA of M.A/ given Aold.
In the inner loop, we utilize the inexact ALM algorithm to solve Eq. (6.10) by
alternately updating different sets of variables.
Given an initialized dictionary D, we update other variables J, ZS, ZT, ES and ET.
The augmented Lagrangian function of (6.10) is
L D Q1..J/j.J/old/ C 1Q2.ESjEold
S /
C2Q2.ETjEold
T / C 3 kZSk2;1 C Tr.RT.ZT  J//
CTr.YT.XS  DZS  ES//
CTr.QT.XT  DZT  ET//
C 
2 .kXS  DZS  ESk2
F C kXT  DZT  ETk2
F
C kZT  Jk2
F/;
(6.11)

104
6
Robust Dictionary Learning
where k:kF is the Frobenius norm, Y 2 Rdm, Q 2 Rdn and R 2 Rrn are Lagrange
multipliers and 
 > 0 is a positive penalty parameter.
In particular, we alternately update these variables in the kC1 iteration as follows:
JkC1 D S1=
;	.ZTk C .Rk=
k//;
(6.12)
ZT.kC1/ D .In C DTD/1.DTXT  DTETk C JkC1 C .DTQk  Rk/=
k/;
(6.13)
ZS.kC1/ D min
Z
3

k
kZk2;1 C 1
2
Z  ZSk  DT.XS  DZSk  ESk C Yk=
k/
2
F ;
(6.14)
ES.kC1/ D D1=
;W.XS  DZS.kC1/ C Yk=
k/;
(6.15)
ET.kC1/ D D2=
;W.XT  DZT.kC1/ C Qk=
k/;
(6.16)
When the variables J, ZS, ZT, ES and ET are optimized, we update the dictionary
D using an efﬁcient solver presented in [15]. More speciﬁcally, by ignoring the
irrelevant terms in (6.11), we compute DjC1 by minimizing the following objective:
DjC1 D min
D
Tr.YT.XS  DZS  ES// C Tr.QT.XT  DZT  ET//
C 
2 .kXS  DZS  ESk2
F C kXT  DZT  ETk2
F/:
(6.17)
6.2.4
Algorithm and Discussions
The whole optimization process, including both inner loop and outer loop, is
repeated until convergence. The problem (6.14) can be solved according to the
Lemma 3.2 in [28]. The detailed procedures of our optimization is outlined in
Algorithm 6.1.
Convergence. Lemma 6.1 demonstrates the local convergence property of our
algorithm.
Lemma 6.1 When D is ﬁxed, the objective function values of (6.10) obey
f.J; ES; ET; ZS/  Q1..J/j.J/old/ C 1Q2.ESjEold
S /
C 2Q2.ETjEold
T / C 3 kZSk2;1
 Q1..J/oldj.J/old/ C 1Q2.Eold
S jEold
S /
C 2Q2.Eold
T jEold
T / C 3
Zold
S

2;1
D f.Jold; Eold
S ; Eold
T ; Zold
S /:

6.2
Self-Taught Low-Rank (S-Low) Coding
105
Algorithm 6.1 Solving problem (6.9) via MM-ALM
Input: data matrix X D ŒXS XT, parameters 1, 2, 3,
D0, J0, ZT0, ZS0, ES0, ET0, Y0, Q0, R0,  D 1:2,

0 D 103, 
max D 105, k D 0,  D 106
1: while not converged do
2: 	 D Diag.1n  .Jj/=1/C;
2: W D .1m1T
n  jSjj=2/C;
3: while not converged do
4:
update JjC1
kC1 using (6.12), given others ﬁxed;
5:
update ZjC1
T.kC1/ using (6.13), given others ﬁxed;
6:
update ZjC1
S.kC1/ using (6.14), given others ﬁxed;
7:
update EjC1
S.kC1/ using (6.15), given others ﬁxed;
8:
update EjC1
T.kC1/ using (6.16), given others ﬁxed;
9:
update the multipliers Y, Q and R
YkC1 D Yk C 
k.XS  DjC1ZjC1
S.kC1/  EjC1
S.kC1//;
QkC1 D Qk C 
k.XT  DjC1ZjC1
T.kC1/  EjC1
T.kC1//;
RkC1 D Rk C 
k.JjC1
kC1  ZjC1
T.kC1//:
10:
update the parameter 
kC1 by

kC1 D min.
k; 
max/.
11:
check the convergence conditions
kXS  DZS  ESk1 < ; kJ  ZTk1 < ,
and kXT  DZT  ETk1 < :
12:
k D k C 1;
13: end while
14: update DjC1 using (6.17);
15: j D j C 1;
16: end while
Output: ZS; ZT; ES; ET, D
This lemma can be easily proved using the Proposition 4 in [43]. It demonstrates
the local convergence property of our algorithm.
Initialization. In Algorithm 6.1, the dictionary D is initialized by some randomly
selected samples from X. ZS and ZT are initialized by random normal matrices. All
the other variables are initialized by 0. Our experiments show that both D and Z are
not sensitive to the random initializations.
Time Complexity. Given r < n, the step 4 in Algorithm 6.1 involves SVD
decomposition of a matrix with size r  n that costs O.nr2/, and the multiplication
and inverse of matrices in step 5 also cost O.nr2/. Because the outer loop converges
quickly in practice, which will be illustrated in experiments, we only consider the
inner loop in the time complexity analysis. Let t denote the number of iterations in
the inner loop, the complexity of our algorithm is O.tnr2/.

106
6
Robust Dictionary Learning
6.3
Learning with S-Low Coding
In this section, we present two learning algorithms based on our S-Low coding
approach, including clustering and classiﬁcation.
6.3.1
S-Low Clustering
Given an unlabeled sample set X D ŒXS XT in the self-taught learning scenario,
the goal of our S-Low clustering algorithm is to correctly recover the underlying
subspaces in the target domain.
The low-rank codings ZT for the target domain are utilized to deﬁne an afﬁnity
matrix of an undirected graph G. According to the low-rank subspace recovery
theory, each column in coefﬁcient matrix Z could serve as a new representation
for a sample, and then the correlation coefﬁcient of each pair of samples would be
a good choice for weighting the corresponding edge in the undirected graph [17].
In particular, we calculate the cosine similarity of each pair of samples (i.e., two
vectors) as the graph weights. Given two coding vectors zi; zj 2 ZT, the graph weight
G.i; j/ is deﬁned as
G.i; j/ D
zT
i zj
kzik2kzjk2
:
(6.18)
On the other hand, sparsity is always emphasized during graph construction, and
therefore we prune those edges with small weights to make the graph sparse. Finally,
an effective clustering algorithm, Normalized Cuts [40], is employed to produce the
clustering results. The whole procedures of S-Low clustering are summarized in
Algorithm 6.2.
Algorithm 6.2 S-Low clustering algorithm
Input: data matrix X D ŒXS XT, nearest neighbors K,
number of clusters C
1: Obtain the low-rank representation matrix ZT using
Algorithm 6.1;
2: Build an undirected graph G based on ZT (using (6.18)),
where the edges are weighted using correlation
coefﬁcients of each pair of samples;
3: Prune graph G by removing some edges with small
weights (keep K nearest neighbors for each node);
4: Use NCut to generate C clusters.
Output: clustering index vector L

6.4
Experiments
107
Algorithm 6.3 S-Low classiﬁcation algorithm
Input: data matrix X D ŒXS XT, class labels of XT,
test sample y
1: Obtain the low-rank representation ZT and dictionary
D using Algorithm 6.1;
2: Train a SVM classiﬁer using ZT;
3: Calculate sparse representation of y using (6.19);
4: Predict class label of y.
Output: predicted class label cy
6.3.2
S-Low Classiﬁcation
When label information are available in the target domain, we design a classiﬁcation
algorithm based on our S-Low coding approach to train a classiﬁer. Then, with the
help of the learned dictionary D, our algorithm could classify new test samples. As
discussed in Sect. 6.3.1, low-rank codings ZT can be regarded as new representations
of the target sample set XT. Given a test sample y, we can calculate the representation
coefﬁcients of y 2 Rd1 by solving:
min
a ky  Dak2
2 C kak1;
(6.19)
where a 2 Rr1 is the coefﬁcient vector of y over D.
Without the loss of generality, we can train any classiﬁer using ZT. In this chapter,
we adopt the commonly used classiﬁer support vector machines (SVM) [41] to
predict the class label of y. Algorithm 6.3 summarizes all the procedures in our
S-Low classiﬁcation algorithm.
6.4
Experiments
In this section, we evaluate the performance of the proposed S-Low Coding
method and corresponding learning algorithms. We ﬁrst introduce the auxiliary
dataset, target datasets and basic experimental settings. The convergence property
and parameter sensitivity are then evaluated and discussed. After that, we report
and discuss the results of S-Low clustering and S-Low classiﬁcation algorithms,
compared with some baselines.
6.4.1
Datasets and Settings
Auxiliary Domain Dataset. Following [44], we randomly select 5000 unlabeled
images from the LabelMe website2 to construct the sample set in auxiliary domain.
2http://labelme.csail.mit.edu/Release3.0/

108
6
Robust Dictionary Learning
Fig. 6.2 Sample images in auxiliary domain (above) and target domain (below)
Currently, the LabelMe dataset contains more than 100 thousand images collected
from various resources, which provide us a great auxiliary domain for self-taught
learning. Figure 6.2a shows some images in the LabelMe dataset.
To evaluate how the data size in the auxiliary domain affects the performance
of learning tasks in the target domain, we alter the number of auxiliary samples
from 1000 to 5000, and compare the performance in different settings. In our
experiments, we ﬁnd that increasing the size of auxiliary sample set would improve
the performance of learning tasks, but the improvements are marginal when the size
is over 3000. Due to the space limit, we only report the results of self-taught learning
algorithms under two settings that use 1000 and 3000 auxiliary images, respectively.
Target Domain Datasets. To extensively testify our approach and related methods,
we utilize the following ﬁve benchmark datasets.
•
MSRC-v1 dataset3 contains 240 images of 9 classes. Following [14], we choose
7 classes including airplane, bicycle, building, car, cow, face, tree, and each
class has 30 images. This dataset owns obvious clutter and variability in the
appearances of objects. The MSRC-v2 dataset is an extension of MSRC-v1. It
contains 591 images of 23 object classes. Figure 6.2c shows some images in the
MSRC-v1 dataset.
•
Caltech-101 dataset4 contains 9411 images of objects, belonging to 102 cat-
egories (including one background class). Following [7], we use the 20-class
subset includes Faces, Leopards, Motorbikes, Binocular, Brain, Camera, Car-
Side, Dollar-Bill, Ferry, Garﬁeld, Hedgehog, Pagoda, Rhino, Snoopy, Stapler,
Stop-Sign, Water-Lilly, Windsor-Chair, Wrench, Yin- Yang, and therefore has
1230 images in total. Figure 6.2d shows some images in the Caltech-101 dataset.
•
Caltech-UCSD Birds dataset5 contains the photos of 200 birds species (mostly
North American). There are 6033 images in total. In the experiments, we select
the ﬁrst 30 categories. Figure 6.2e shows some images in the Caltech-UCSD
Birds dataset.
3http://research.microsoft.com/en-us/projects/objectclassrecognition/
4http://www.vision.caltech.edu/Image_Datasets/Caltech101/
5http://www.vision.caltech.edu/visipedia/CUB-200.html

6.4
Experiments
109
Table 6.2 Target datasets for
experiments
Dataset
# Categories
# Samples
MSRC-v1
7
210
MSRC-v2
20
591
Caltech-101
20
1230
Scene-15
15
4485
Caltech-UCSD Birds
30
1622
•
Scene-15 dataset6 contains 4485 images spread over 15 natural scene cate-
gories. The ﬁfteen scene categories contain 200 to 400 images each and range
from natural scenes like mountains and forests to man-made environments
like kitchens and ofﬁces. Figure 6.2b shows some images in the Scene-15
dataset.
Table 6.2 summarizes the details of each target dataset. For each dataset, we
extract the local binary pattern (LBP) features from each image using the VLFeat
package [42],7 and ﬁnally quantize every image as a 928 dimensional feature vector.
LBP was selected due to its simple implementation and good performance on
image representation. Other types of features could also be used in the proposed
framework.
Baselines. We compare our S-Low clustering algorithm with several represen-
tative subspace clustering methods including Scalable Sparse Subspace Cluster-
ing (SSSC) [36], LRR [28], Latent LRR [27] and Fixed Rank Representation
(FRR) [30]. Although the RDSTL [44] method is not designed for clustering
problems, we also testify its performance on subspace clustering to further illustrate
the differences between our approach and RDSTL. We utilize an unsupervised
version of RDSTL, by replacing the classiﬁer in RDSTL with the graph construction
procedure. In detail, the learned dictionary D is used for generating new codings
of each sample, and then a graph is constructed using the codings. As we have two
different auxiliary sample sets, we use RDSTL-A and Ours-A to denote the methods
employing 1000 images from the auxiliary domain, and RDSTL-B and Ours-B use
the auxiliary sample set with 3000 images.
For the image classiﬁcation task, we compare our S-Low classiﬁcation algorithm
with supervised learning method SVM [41], semi-supervised learning method
transductive SVM (TSVM) [11], low-rank learning methods Latent LRR [27]
and FRR [30]. We also compare our approach with the state-of-the-art domain
adaptation and self-taught learning methods, including the Landmarks Selection-
based Subspace Alignment (LSSA) method [1], STL [38], and RDSTL [44].
Evaluation Metrics. For the clustering task, the whole target data set is taken
as input of the algorithm. To evaluate the clustering results, we adopt two widely
6http://www-cvr.ai.uiuc.edu/ponce_grp/data/
7http://www.vlfeat.org/

110
6
Robust Dictionary Learning
used performance measure, clustering Accuracy (Acc) and normalized mutual
information (NMI), which are deﬁned as follows:
Acc D
Pn
iD1 ı.map.ri/; li/
n
;
(6.20)
where ri denotes the cluster label of sample xTi, li denotes the ground truth, n is the
total number of samples, ı.x; y/ equals to 1 if and only if x D y, and map./ is the
permutation mapping function that maps each cluster label ri to the equivalent label
from the data set.
NMI.A; B/ D
MI.A; B/
max.H.A/; H.B//;
(6.21)
where A and B are the predicted clustering index and the ground truth, respectively.
MI.A; B/ denotes the mutual information between A and B. H.A/ and H.B/ denote
the entropies of p.a/ and p.b/.
For the classiﬁcation task, we follow the standard ways to produce training and
test splits on different datasets. Following [44], we conduct 5-fold experiments on
the MSRC-v1 dataset. We randomly select 5, 10, 15, and 30 samples to construct
training set on the Caltech-101 dataset, and the rest samples for testing. On the
Scene-15 dataset, following [13], we randomly select 100 training samples, and the
rest samples are used to construct test sample set. We will compute the classiﬁcation
accuracy, and show the confusion matrices.
6.4.2
Property Analysis
Convergence Analysis. Figure 6.3a, b show our approach converges quickly.
The relative errors in Fig. 6.3a, b are calculated by kZT.kC1/  ZTkkF=kZTkkF and
kDjC1  DjkF=kDjkF, respectively. Especially, Fig. 6.3b shows that our dictionary
converges within a few number of iterations, which is beneﬁcial to some large-scale
applications.
Sensitivity of Parameters. In our approach, there are three main parameters, 1,
2 and 3. To choose proper values for them, we evaluate the parameter sensitivity
on the MSRC-v1 dataset. We conducted an internal 5-fold cross-validation in the
training set to ﬁne tune the parameters. Figure 6.4 shows the accuracy of Ours-B
approach under different settings of 1, 2 and 3, respectively. Here, 1 and 2 are
used to handle the noise in samples, while 3 controls the structural sparsity in ZT,
which allows the source data to select some bases from D. Figure 6.4 also shows
that our approach achieves relatively stable performance when 3 is altered from 1
to 2, and 2 does not affect the results signiﬁcantly in the range Œ2; 3. For all the
databases, we ﬁne tune those parameters to achieve their best performance.

6.4
Experiments
111
0
100
200
300
0
0.5
1
1.5
Number of Iterations
Relative Error
0
5
10
15
0
0.5
1
1.5
Number of Iterations
Relative Error
(a)
(b)
Fig. 6.3 Convergence property of our approach on Caltech-101 dataset, measured by (a) relative
error of ZT; (b) relative error of D
0.1
0.5
1
1.5
2
2.5
3
3.5
40
45
50
55
60
65
70
75
80
The value of parameters
Accuracy (%)
Our method with different λ1, when λ2=2, λ3=1
Our method with different λ2, when λ1=2, λ3=1
Our method with different λ3, when λ1=2, λ2=2
Fig. 6.4 Accuracy of Ours-B approach under different values of 1, 2 and 3 on MSRC-v1
dataset
The size of dictionary D is another parameter that should be predeﬁned.
Figure 6.5a shows the accuracy of different algorithms on the Caltech-101 dataset.
It shows that our approach is not sensitive to the size of dictionary. We empirically
set the size of dictionary as 200, 200, 200, 300, and 400 on the MSRC-v1,
MSRC-v2, Caltech-101, Scene-15 and Caltech-UCSD Birds datasets, respectively.
In Algorithm 6.2, we prune the graph by keeping only K nearest neighbors for
each node. Figure 6.5b shows the NMI with different values of K. In the following
experiments, K is set to 20.
Note that the edge-pruning procedure is not the main reason of the performance
improvement. To verify this, we conduct experiments to evaluate the performance of

112
6
Robust Dictionary Learning
100
150
200
250
300
350
25
30
35
40
45
50
55
Size of Dictionary
Accuracy (%)
LRR
SSSC
RDSTL
Ours
5
10
15
20
25
30
35
0.2
0.3
0.4
0.5
0.6
Number of K 
NMI
LRR
SSSC
RDSTL
Ours
(a) Different size of D
(b) Different number of K
Fig. 6.5 Parameter sensitivity of our approach on Caltech-101 dataset: (a) clustering accuracy
with different dictionary sizes; (b) NMI with different values of K
Latent LRR with edge pruning. On the Caltech-101 dataset, the clustering accuracy
of LatentLRR is 44.39%. The accuracies of Latent LRR with edge pruning are:
44.57% (K D 3), 44.86% (K D 5), 44.12 (K D 10), where K is the number
of neighbors per sample. It shows that edge pruning can slightly enhance the
performance, but it is not the main reason.
6.4.3
Clustering Results
Table 6.3 shows the subspace clustering accuracies of all compared methods on the
ﬁve datasets, and Table 6.4 shows the corresponding NMI for each method. We
can observe that all the low-rank based methods outperform RDSTL, since they
explicitly consider the structure information in sample space. When class labels are
not available, the underlying structure information of data plays an important role in
learning tasks. Latent LRR, which models the effect of hidden data, performs better
than LRR and FRR. By virtue of a more informative dictionary learnt from both
auxiliary domain and target domain, our approach achieves better performance than
other competitors on all the ﬁve datasets.
In addition, by increasing the data size in auxiliary domain, the performance of
self-taught learning methods could be slightly improved, as RDSTL-B and Ours-B
outperform RDSTL-A and Ours-A in most cases, respectively. We also noticed that,
on the Caltech-UCSD Birds dataset, unsupervised clustering is a rather challenging
problem, and the clustering accuracies of all compared methods are a bit low. One
possible reason is that most categories share many common visual elements such as
different birds in the wild.

6.4
Experiments
113
Table 6.3 Subspace clustering accuracies (%) of all compared methods. The version A of RDSTL
and our method uses 1000 auxiliary images, and version B uses 3000 images
Methods
MSRC-v1
MSRC-v2
Caltech-101
Scene-15
Birds
LRR [28]
70.95
32.08
41.22
36.87
13.46
SSSC [36]
69.25
33.25
40.01
28.81
18.25
LatLRR [27]
71.91
31.37
44.39
32.40
15.20
FRR [30]
70.48
32.75
42.67
31.32
17.91
RDSTL-A [44]
52.68
27.16
35.44
27.06
11.05
RDSTL-B [44]
53.26
28.45
37.14
29.65
13.62
Ours-A
74.25
38.42
50.25
43.17
21.63
Ours-B
75.16
43.21
55.47
46.75
23.91
Table 6.4 NMI of all compared methods. The version A of RDSTL and our method uses 1000
auxiliary images, and version B uses 3000 images
Methods
MSRC-v1
MSRC-v2
Caltech-101
Scene-15
Birds
LRR [28]
0.6021
0.3892
0.4697
0.3185
0.2305
SSSC [36]
0.6128
0.3921
0.4832
0.3305
0.2651
LatLRR [27]
0.5939
0.3719
0.4728
0.2932
0.2454
FRR [30]
0.5932
0.4033
0.4489
0.3271
0.2392
RDSTL-A [44]
0.3604
0.2915
0.3109
0.2515
0.2101
RDSTL-B [44]
0.3782
0.2618
0.3675
0.2613
0.2075
Ours-A
0.6725
0.4778
0.5267
0.3795
0.2811
Ours-B
0.6841
0.5132
0.5215
0.4015
0.3091
6.4.4
Classiﬁcation Results
Our S-Low classiﬁcation algorithm is compared with SVM [41], transductive SVM
(TSVM) [11], Latent LRR [27], FRR [30], LSSA [1], STL [38], and RDSTL [44].
We also have two versions for STL, RDSTL, and our method that use two auxiliary
sample sets, respectively.
STL, LSSA and our approach need to train SVM classiﬁers. For SVM and
TSVM, we utilize the Gaussian kernel (i.e., K.xi; xj/ D exp.ˇkxi xjk2/, and tune
the parameter ˇ and regularization parameter C in the range of f105,    ; 101; 1;
101;    ; 105g to obtain their best classiﬁcation results. For each method, an internal
5-fold cross-validation in the training set is conducted to ﬁne tune the parameters.
The dictionary D plays a key role in our approach, and one interesting question
is that, will the size of dictionary greatly inﬂuence the classiﬁcation accuracy?
Figure 6.6 shows the classiﬁcation accuracy of Ours-A approach when the size of
dictionary varies from 100 to 800 on MSRC-v1 and Scene-15 datasets, respectively.
It shows that our approach obtains relatively similar results on the Scene-15 dataset
with different dictionary sizes, and obtains better performance on the MSRC-v1
dataset when the dictionary size is 200. In our experiments, we empirically set the

114
6
Robust Dictionary Learning
Table 6.5 Average classiﬁcation accuracies (%) of all compared methods on three datasets
(a) MSRC-v1 and Scene-15 datasets
Methods
MSRC-v1
Scene-15
SVM [41]
79.62
76.41
TSVM [11]
79.84
75.35
LatLRR [27]
81.90
62.53
FRR [30]
80.45
60.65
LSSA [1]
81.59
72.61
STL-A [38]
83.04
73.70
STL-B [38]
83.62
75.12
RDSTL-A [44]
89.11
77.08
RDSTL-B [44]
89.44
78.52
Ours-A
91.52
82.45
Ours-B
92.36
82.73
(b) Caltech-101 dataset
Methods
5 train 10 train 15 train 30 train
SVM [41]
45.53
53.61
57.72
67.08
TSVM [11]
44.18
52.78
57.35
65.83
LatLRR [27]
46.32
53.29
58.15
68.67
FRR [30]
45.21
53.57
58.63
67.52
LSSA [1]
45.10
54.92
58.25
70.33
STL-A [38]
47.60
54.73
59.06
71.46
STL-B [38]
47.92
55.07
59.54
71.31
RDSTL-A [44] 49.54
56.84
61.26
72.62
RDSTL-B [44] 50.13
57.05
61.73
72.95
Ours-A
53.28
58.92
63.95
74.51
Ours-B
53.16
59.33
65.12
74.78
size of dictionary as 200 on the MSRC-v1 and Caltech-101 datasets, and 400 on the
Scene-15 dataset.
All compared methods are repeated 10 times on the Caltech-101 and Scene-
15 datasets. Table 6.5a shows the average classiﬁcation results on the MSRC-v1
dataset, and Table 6.5b shows the results on the Caltech-101 dataset under different
settings. To take a close look at our success and failure cases, Fig. 6.7b, c show
the confusion matrices of our approach on the MSRC-v1 and Scene-15 datasets,
respectively, and Fig. 6.7a provides examples from classes with high accuracy on
the Caltech-101 dataset, when the number of training samples is 30.
100
200
300
400
500
600
700
800
0
10
20
30
40
50
60
70
80
90
100
Size of Dictionary
Accuracy (%)
MSRC−v1
Scene−15
Fig. 6.6 Classiﬁcation accuracy of our approach with different sizes of dictionary on MSRC-v1
and Scene-15 datasets

6.4
Experiments
115
(b) Scene-15
(a) MSRC-v1
(1) camera, acc: 100%
(2) face, acc: 100%
(3) leopard, acc: 97%
(c) Caltech-101
Fig. 6.7 (a–b) Confusion matrices of our approach on MSRC-v1 and Scene-15 datasets. (c)
Example images from classes with high classiﬁcation accuracy of Caltech-101
We can observe from Table 6.5 that the advantages of self-taught learning are
extensively demonstrated, since all the three self-taught learning methods (i.e., STL,
RDSTL and our approach) outperform other competitors. The domain adaption
methods like LSSA usually assume that the auxiliary and target domains share
similar learning tasks. However, such an assumption does not hold in the setting
of self-taught learning, i.e., the auxiliary domain may contain arbitrary images.
Thus, we observe that LSSA cannot outperform the self-taught learning methods.
Furthermore, our approach consistently performs better than STL and RDSTL, and
the main reason is that our low-rank codings could provide robust representations
for images by using an informative dictionary.

116
6
Robust Dictionary Learning
Fig. 6.8 Visualization of graphs learnt by (a) Latent LRR, (b) RDSTL and (c) Our approach on
MSRC-v1 image dataset. The red color denotes large graph weights, while the blue color indicates
small weights
6.4.5
Discussions
To illustrate how low-rank constraints and auxiliary data help the learning tasks in
the target domain, in Fig. 6.8, we visualize several graphs learned by Latent LRR,
RDSTL and Ours-B approach on the MSRC-v1 image dataset, respectively. We have
the following observations.
•
Due to the low-rank constraint, the graphs learned by Latent LRR and our
approach shown in Fig. 6.8a, c have a block-diagonal structure. However, the
RDSTL graph shown in Fig. 6.8c does not have such a structure, as the global
structure information is not considered in the model.
•
Compared with the Latent LRR graph, our graph is sparser, and has a clearer
block-diagonal structure. The reasons are two-fold. First, Latent LRR uses the
sample set itself as a dictionary, while our approach learns an informative
dictionary for representation. Second, our approach prunes the edges with small
weights to produce a sparse graph, while the graph learned by Latent LRR is very
dense.
•
The weights in our graph (illustrated as red color in Fig. 6.8c) are much higher
than those in Latent LRR (illustrated as blue color in Fig. 6.8a). With the help
of auxiliary dataset, the results in Tables 6.3 and 6.4 validate the effectiveness of
our graph.
6.5
Summary
In this chapter, we propose a novel self-taught low-rank coding approach for
clustering visual data. Our approach jointly learns a dictionary by virtue of rich
information from auxiliary domain, and robust low-rank representations for target
domain. We derive both unsupervised and supervised learning algorithms for sub-
space clustering and image classiﬁcation. Experimental results on ﬁve benchmark

References
117
datasets demonstrate the effectiveness of our approach compared with the state-of-
the-art self-taught learning methods.
There remain several interesting directions for our future work: (1) we would
design S-Low coding based classiﬁcation approaches, (2) given a training set in
target domain, we may automatically choose samples from the auxiliary domain, (3)
we would provide fast solutions to our framework by using the divide-and-conquer
technique.
References
1. Aljundi, R., Emonet, R., Muselet, D., Sebban, M.:
Landmarks-based kernelized subspace
alignment for unsupervised domain adaptation. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 56–63 (2015)
2. Bengio, Y., Courville, A., Vincent, P.: Representation learning: a review and new perspectives.
IEEE Trans. Pattern Anal. Mach. Intell. 35(8), 1798–1828 (2013)
3. Candès, E.J., Li, X.D., Ma, Y., Wright, J.: Robust principal component analysis? J. ACM
58(3), 11 (2011)
4. Dai, W., Yang, Q., Xue, G.-R., Yu, Y.: Self-taught clustering. In: Proceedings of the 25th
International Conference on Machine Learning, pp. 200–207 (2008)
5. Ding, Z., Shao, M., Fu, Y.: Latent low-rank transfer subspace learning for missing modality
recognition.
In: Proceedings of the 28th AAAI Conference on Artiﬁcial Intelligence,
pp. 1192–1198 (2014)
6. Ding, Z., Shao, M., Fu, Y.: Deep low-rank coding for transfer learning. In: Proceedings of the
Twenty-Fourth International Joint Conference on Artiﬁcial Intelligence, pp. 3453–3459 (2015)
7. Dueck, D., Frey, B.J.: Non-metric afﬁnity propagation for unsupervised image categorization.
In: Proceedings of the 11th IEEE International Conference on Computer Vision, pp. 1–8 (2007)
8. Fu, Y., Gao, J., Tien, D., Lin, Z., Hong, X.: Tensor LRR and sparse coding-based subspace
clustering. IEEE Trans. Neural Netw. Learn. Syst. 27(9), 2120–2133 (2016)
9. Hou, C.-A., Yang, M.-C., Wang, Y.-C.: Domain adaptive self-taught learning for heterogeneous
face recognition. In: Proceedings of the 22nd International Conference on Pattern Recognition,
pp. 3068–3073. IEEE, Piscataway (2014)
10. Jhuo, I.H., Liu, D., Lee, D.T., Chang, S.F.: Robust visual domain adaptation with low-rank
reconstruction. In: Proceedings of the 25th IEEE Conference on Computer Vision and Pattern
Recognition, pp. 2168–2175 (2012)
11. Joachims, T.: Transductive inference for text classiﬁcation using support vector machines. In:
Proceedings of the International Conference on Machine Learning, pp. 200–209 (1999)
12. Kuen, J., Lim, K.M., Lee, C.P.: Self-taught learning of a deep invariant representation for
visual tracking via temporal slowness principle. Pattern Recognit. 48(10), 2964–2982 (2015)
13. Lazebnik, S., Schmid, C., Ponce, J.: Beyond bags of features: spatial pyramid matching for
recognizing natural scene categories. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 2169–2178 (2006)
14. Lee, Y.J., Grauman, K.: Foreground focus: unsupervised learning from partially matching
images. Int. J. Comput. Vis. 85(2), 143–166 (2009)
15. Lee, H., Battle, A., Raina, R., Ng, A.Y.: Efﬁcient sparse coding algorithms. In: Proceedings
the 19th Annual Conference on Neural Information Processing Systems, pp. 801–808 (2006)
16. Lee, H., Raina, R., Teichman, A., Ng, A.Y.: Exponential family sparse coding with application
to self-taught learning. In: Proceedings of the 21st International Joint Conference on Artiﬁcial
Intelligence, pp. 1113–1119 (2009)
17. Li, S., Fu, Y.: Low-rank coding with b-matching constraint for semi-supervised classiﬁca-
tion.
In: Proceedings of the 23rd International Joint Conference on Artiﬁcial Intelligence,
pp. 1472–1478 (2013)

118
6
Robust Dictionary Learning
18. Li, S., Fu, Y.:
Robust subspace discovery through supervised low-rank constraints.
In:
Proceedings of the SIAM International Conference on Data Mining, pp. 163–171 (2014)
19. Li, S., Fu, Y.: Learning balanced and unbalanced graphs via low-rank coding. IEEE Trans.
Knowl. Data Eng. 27(5), 1274–1287 (2015)
20. Li, S., Fu, Y.: Learning robust and discriminative subspace with low-rank constraints. IEEE
Trans. Neural Netw. Learn. Syst. 27(11), 2160–2173 (2016)
21. Li, S., Fu, Y.: Unsupervised transfer learning via low-rank coding for image clustering. In:
International Joint Conference on Neural Networks, pp. 1795–1802. IEEE, Piscataway (2016)
22. Li, L., Li, S., Fu, Y.: Learning low-rank and discriminative dictionary for image classiﬁcation.
Image Vis. Comput. 32(10), 814–823 (2014)
23. Li, S., Li, K., Fu, Y.:
Temporal subspace clustering for human motion segmentation.
In:
Proceedings of the IEEE International Conference on Computer Vision, pp. 4453–4461 (2015)
24. Li, S., Li, K., Fu, Y.: Self-taught low-rank coding for visual learning. IEEE Trans. Neural
Netw. Learn. Syst. (2017)
25. Li, S., Shao, M., Fu, Y.: Cross-view projective dictionary learning for person re-identiﬁcation.
In: Proceedings of the 24th International Conference on Artiﬁcial Intelligence, pp. 2155–2161
(2015)
26. Li, S., Shao, M., Fu, Y.: Multi-view low-rank analysis for outlier detection. In: Proceedings of
the SIAM International Conference on Data Mining, pp. 748–756. SIAM, Philadelphia (2015)
27. Liu, G., Yan, S.:
Latent low-rank representation for subspace segmentation and feature
extraction. In: Proceedings of the 13th IEEE International Conference on Computer Vision,
pp. 1615–1622 (2011)
28. Liu, G., Lin, Z., Yu, Y.:
Robust subspace segmentation by low-rank representation.
In:
Proceedings of the 27th International Conference on Machine Learning, pp. 663–670 (2010)
29. Liu, G., Xu, H., Yan, S.:
Exact subspace segmentation and outlier detection by low-rank
representation. J. Mach. Learn. Res. Proc. Track 22, 703–711 (2012)
30. Liu, R.S., Lin, Z.C., Torre, F.D., Su, Z.X.: Fixed-rank representation for unsupervised visual
learning.
In: Proceedings of the 25th IEEE Conference on Computer Vision and Pattern
Recognition, pp. 598–605 (2012)
31. Liu, G., Lin, Z., Yan, S., Sun, J., Yu, Y., Ma, Y.: Robust recovery of subspace structures by
low-rank representation. IEEE Trans. Pattern Anal. Mach. Intell. 35(1), 171–184 (2013)
32. Ma, L., Wang, C.H., Xiao, B.H., Zhou, W.: Sparse representation for face recognition based
on discriminative low-rank dictionary learning. In: Proceedings of the 25th IEEE Conference
on Computer Vision and Pattern Recognition, pp. 2586–2593 (2012)
33. Pan, S.J., Yang, Q.: A survey on transfer learning. IEEE Trans. Knowl. Data Eng. 22(10),
1345–1359 (2010)
34. Patel, V.M., Gopalan, R., Li, R., Chellappa, R.: Visual domain adaptation: a survey of recent
advances. IEEE Signal Process. Mag. 32(3), 53–69 (2015)
35. Patricia, N., Caputo, B.: Learning to learn, from transfer learning to domain adaptation: a
unifying perspective. In: Proceedings of the 27th IEEE Conference on Computer Vision and
Pattern Recognition, pp. 1442–1449 (2014)
36. Peng, X., Zhang, L., Yi, Z.: Scalable sparse subspace clustering. In: Proceedings of the 26th
IEEE Conference on Computer Vision and Pattern Recognition, pp. 430–437 (2013)
37. Raina, R.: Self-taught learning. PhD thesis, Stanford University (2009)
38. Raina, R., Battle, A., Lee, H., Packer, B., Ng, A.Y.: Self-taught learning: transfer learning from
unlabeled data. In: Proceedings of the 24th International Conference on Machine Learning,
pp. 759–766 (2007)
39. Shao, M., Castillo, C., Gu, Z., Fu, Y.: Low-rank transfer subspace learning. In: Proceedings of
the 12th IEEE International Conference on Data Mining, pp. 1104–1109 (2012)
40. Shi, J., Malik, J.: Normalized cuts and image segmentation. IEEE Trans. Pattern Anal. Mach.
Intell. 22(8), 888–905 (2000)
41. Vapnik, V.: The nature of statistical learning theory. Springer, New York (2000)

References
119
42. Vedaldi, A., Fulkerson, B.:
VLFeat: an open and portable library of computer vision
algorithms. In: Proceedings of the 18th ACM International Conference on Multimedia, pp.
1469–1472. ACM (2010)
43. Wang, S., Liu, D., Zhang, Z.:
Nonconvex relaxation approaches to robust matrix recov-
ery.
In: Proceedings of the 23rd International Joint Conference on Artiﬁcial Intelligence,
pp. 1764–1770 (2013)
44. Wang, H., Nie, F.P., Huang, H.: Robust and discriminative self-taught learning. In: Proceedings
of the 30th International Conference on Machine Learning, pp. 298–306 (2013)
45. Wright, J., Yang, A.Y., Ganesh, A., Sastry, S.S., Ma, Y.: Robust face recognition via sparse
representation. IEEE Trans. Pattern Anal. Mach. Intell. 31(2), 210–227 (2009)
46. Xu, Z., Li, W., Niu, L., Xu, D.: Exploiting low-rank structure from latent domains for domain
generalization. In: Proceedings of the 13th European Conference on Computer Vision, pp. 628–
643 (2014)
47. Yang, M., Zhang, L., Feng, X.C., Zhang, D.: Fisher discrimination dictionary learning for
sparse representation. In: Proceedings of the 13th IEEE International Conference on Computer
Vision, pp. 543–550 (2011)
48. Zhang, C.: Nearly unbiased variable selection under minimax concave penalty. Ann. Stat.
38(2), 894–942 (2010)
49. Zhu, X.: Semi-supervised learning literature survey. Comput. Sci. Univ. Wis. Madison 2, 3
(2006)
50. Zografos, V., Ellis, L., Mester, R.: Discriminative subspace clustering. In: Proceedings of the
26th IEEE Conference on Computer Vision and Pattern Recognition, pp. 2107–2114 (2013)

Part II
Applications

Chapter 7
Robust Representations for Collaborative
Filtering
Abstract Collaborative ﬁltering (CF) has been widely employed within recom-
mender systems to solve many real-world problems. Learning effective latent
factors plays the most important role in collaborative ﬁltering. Traditional CF
methods based upon matrix factorization techniques learn the latent factors from
the user-item ratings and suffer from the cold start problem as well as the sparsity
problem. Some improved CF methods enrich the priors on the latent factors by
incorporating side information as regularization. However, the learned latent factors
may not be very effective due to the sparse nature of the ratings and the side
information. To tackle this problem, we learn effective latent representations via
deep learning. Deep learning models have emerged as very appealing in learning
effective representations in many applications. In particular, we propose a general
deep architecture for CF by integrating matrix factorization with deep feature
learning. We provide a natural instantiations of our architecture by combining prob-
abilistic matrix factorization with marginalized denoising stacked auto-encoders.
The combined framework leads to a parsimonious ﬁt over the latent features as
indicated by its improved performance in comparison to prior state-of-art models
over four large datasets for the tasks of movie/book recommendation and response
prediction.
7.1
Overview1
Recommendation is a fundamental problem that has gained utmost importance in
the modern era of information overload. The goal of recommendation is to help
users ﬁnd the item that they maybe potentially interested in from a large repository
of items. Recommender systems are widely used by websites (e.g., Amazon, Google
News, Netﬂix, and Last.fm) in various contexts to target customers and provide them
with useful information. A widely used setting of recommendation system is to
predict how a user would rate an item (such as a movie) if only given the past rating
history of the users. Many classical recommendation methods have been proposed
1This chapter is reprinted with permission from ACM. “Deep Collaborative Filtering via
Marginalized Denoising Auto-encoder”, ACM International Conference on Information and
Knowledge Management, 2015.
© Springer International Publishing AG 2017
S. Li, Y. Fu, Robust Representation for Data Analytics, Advanced Information
and Knowledge Processing, DOI 10.1007/978-3-319-60176-2_7
123

124
7
Robust Representations for Collaborative Filtering
during the last decade. The two broad categories of recommendation systems
are content ﬁltering approaches and collaborative ﬁltering (CF) based methods.
The CF based methods have attracted more attention due to their impressive
performance [39]. Among various CF methods, matrix factorization has emerged
as a powerful tool to perform recommendations in large datasets [4, 16].
Learning effective latent factors plays the most important role in matrix factor-
ization based CF methods. Traditional matrix factorization methods for CF directly
learn the latent factors from the user-item rating matrix [16, 32]. One of the main
challenges faced by these systems is to provide a rating when a new user/item
arrives in the system, which is also known as the cold start scenario. The cold start
problem is circular in nature as – the system will not recommend an item unless
it has some ratings for it and unless the system recommends it will not get ratings
for it. Another practical challenge is learning the appropriate latent factors when the
rating matrix is sparse, which is often the case in many real world scenarios. In order
to overcome these challenges, researchers have suggested to incorporate additional
sources of information about the users or items, also known as the side information.
This side information could be obtained from the user/item proﬁles, for example,
demographics of a user, genre of a movie, etc. The user demographics could be used
to infer the relationships between the users and similarly the item similarity can be
used to automatically assign ratings to new items. The use of side information to
aid matrix factorization has been successfully applied by various prior work, for
example [24, 37, 38]. These methods, however, only utilize the side information as
regularizations in the model, and the learned latent factors may not be very effective
due to the sparse nature of the ratings and the side information. In order to make
matrix factorization based methods effective in such a setting, it is highly desirable
to learn and extract discriminative features from the datasets.
One of the powerful approaches to capture feature interactions and complex
relations that has emerged in the recent past is deep learning [8, 9]. Deep learning
has attracted a lot of attention because of its promising performance to learn
representations on various tasks [22, 35]. Deep neural networks have been shown to
achieve state-of-the-art results in computer vision, speech recognition and machine
translation. The application of deep learning in recommendation systems, however,
is very recent. With large-scale data and rich-side information available, it is now
practicable to learn latent factors through deep architectures. Researchers have
invested in modifying deep learning algorithms like Restricted Botzmann Machines
or Convolutional Neural Networks or Deep Belief Networks directly for the task
of collaborative ﬁltering [7, 27, 34, 40, 42]. However, there are no prior work that
bridge together matrix factorization with deep learning methods with the notable
exception of [43]. In this chapter, we present a deep learning model for collaborative
ﬁltering that tightly couples matrix factorization based collaborative ﬁltering with
deep learning algorithm namely marginalized denoising auto-encoders (mDA) [6].
Unlike [43] which integrates collaborative topic regression and Bayesian stacked
denoising auto-encoders and requires learning of a large number of hyper parame-
ters using an EM style algorithm, our approach uses a much more efﬁcient architec-
ture based upon mDA and stochastic gradient descent and is thus computationally
efﬁcient and highly scalable. This chapter makes the following contributions:

7.2
Collaborative Filtering
125
•
We propose a general deep architecture named deep collaborative ﬁltering
(DCF) [20], which integrates matrix factorization and deep feature learning. It
models the mappings between the latent factors used in CF and the latent layers
in deep models.
•
We present a practical instantiation (i.e., mDA-CF and mSDA-CF) of the
proposed architecture, by utilizing the probabilistic matrix factorization and
mDA. The scalability and low computational cost of the mDA makes it a highly
attractive deep learning tool, which is unlike the prior work [43].
•
We evaluate the performance of our model on three real-world applications,
movie recommendation, book recommendation and response prediction. Our
model outperforms conventional CF methods.
7.2
Collaborative Filtering
In general, our work is closely related to the following topics: matrix factorization
based collaborative ﬁltering, and deep learning based collaborative ﬁltering. We will
discuss the two in the following subsections.
7.2.1
Matrix Factorization for Collaborative Filtering
The importance of accurate recommendation techniques motivated by wide ranging
applications has fueled a great amount of academic as well as industrial research in
this area [31]. Recommender systems are most often based on collaborative ﬁltering
and there are typically two approaches that are widely used. In neighborhood meth-
ods, the similarity between users based on the content they have consumed and rated
is the basis of a new recommendation. A related but intrinsically more powerful
approach has been the use of latent factor models. Matrix factorization (MF) is the
most popular technique to derive latent factor models and their success at the Netﬂix
competition have highlighted their strength [3, 16]. For example, the given matrix
X 2 RNM consisting of the item preferences of the users can be decomposed as
a product of two low dimensional matrices U and V. The decomposition can be
carried out by a variety of methods ranging from SVD based approaches [25] to the
relatively new non-negative matrix factorization approach [17]. One classical MF
method is probabilistic matrix factorization (PMF) [32]. The underlying assumption
behind this method is that the prior probability distribution of the latent factors and
the probability of the observed ratings given the latent factors follows a Gaussian
distribution. Many algorithms have been developed to enhance the performance
of PMF, by designing the Bayesian versions [33, 36, 44], or incorporating side
information, such as social relationships [1, 24, 46].
Although promising, matrix factorization methods suffer from the problem of
cold-start, i.e. what recommendations to make when a new user/item arrives in the

126
7
Robust Representations for Collaborative Filtering
system. Another problem often presented in many real world applications is data
sparsity or reduced coverage. Incorporating side information has shown promising
performance in collaborative ﬁltering in such scenarios. Porteous et al. proposed a
Bayesian matrix factorization (BMF) approach with side information and Dirichlet
process mixtures [30]. A variational BMF method and a hierarchical BMF method
that utilizes side information were also proposed in [14] and [29], respectively.
Hu et al. proposed a cross-domain triadic factorization (CDTF) method [11],
which leverages the information from other domains. The methods discussed
above are proposed for addressing recommendation problems. Recently, MF based
collaborative ﬁltering is also applied to response prediction [21, 26]. The afore-
mentioned approaches can alleviate the problem of cold start and data sparsity but
might still suffer when the side information is sparse. Learning effective features
is critical in matrix factorization. Recently, deep learning based methods have
emerged as a powerful tool for learning representation and are widely used in
many applications ranging from computer vision to speech recognition and machine
translation. In this chapter, our goal is to combine deep learning based methods with
matrix factorization for collaborative ﬁltering. In the next subsection, we survey the
application of deep learning based methods for collaborative ﬁltering.
7.2.2
Deep Learning for Collaborative Filtering
The application of deep learning models to the task of collaborative ﬁltering is very
new and there are not much attempts in this direction. Salakhutdinov et al. [34]
were the ﬁrst to apply deep learning to the task of collaborative ﬁltering. They
modiﬁed the restricted Boltzmann machines as a two-layer undirected graphical
model consisting of binary hidden units and softmax visible units for the task of
collaborative ﬁltering. They designed an efﬁcient learning procedure called the
Contrastive Divergence (CD) to maximize an approximation to the true likelihood
function. They also proposed a conditional RBM model and inference procedures.
They tested the performance of the model on the Netﬂix dataset for movie
recommendation and showed that their model performs well as compared to the
baseline methods.
Truyen et al. [40] proposed ordinal Boltzmann machines for collaborative
ﬁltering. They studied the parameterizations for handling the ordinal nature of
ratings, and presented the integration of multiple Boltzmann machines for user-
based and item-based processes.
Recently, some deep learning models learn latent factors from content informa-
tion such as raw features of audio or articles [12, 28]. Wang et al. [42] utilized deep
belief nets (DBN) for music recommendation, which uniﬁes feature extraction and
recommendation of songs in a joint framework. They assumed that a user has a
feature vector ˇu drawn from a Gaussian prior and the songs have a feature vector
xv. They automatically learned the feature vectors of the songs using a deep belief
network which is a generative probabilistic graphical model with hidden nodes and
observation. It has millions of parameters to be learned from the training data. The

7.3
Preliminaries
127
authors used stacked layers of Restricted Boltzmann Machines for pretraining in
an unsupervised fashion, and then employed the Maximum Likelihood Estimation
(MLE) for supervised learning.
Oord et al. [27] addressed the music recommendation problem using the
convolutional neural networks. They ﬁrst conducted a weighted matrix factorization
to handle implicit feedback and obtained latent factors for all songs. After that
they used deep learning to map audio content to those latent factors. In particular,
they extracted local features from audio signals and aggregated them into a bag-
of-words representation. Finally, the deep convolutional network was employed to
map this feature representation to the latent factors. They tested their algorithm on
the Million song dataset and showed that their model improved the recommendation
performance by augmenting the audio signals.
All the previously mentioned approaches mainly modify the deep learning
algorithms for the task of collaborative ﬁltering and do not directly couple matrix
factorization with deep learning models. Most recently, Wang et al. [43] proposed a
hierarchical Bayesian model called collaborative deep learning (CDL) which tightly
couples stacked denoising auto-encoders (SDA) and collaborative topic regression
(CTR). This work is the closest to our work but differs from ours in many signiﬁcant
ways as follows – (i) CDL utilized a Bayesian formulation of SDA. The generative
process of CDL consists of drawing samples for CDL uses an EM-style algorithm
for obtaining the MAP estimates of Bayesian SDA, and thus it has to learn a
large number of parameters. Our model employs a more efﬁcient architecture,
marginalized SDA (mSDA), which computes the parameters in closed form and
is thus highly efﬁcient and scalable. (ii) CDL only extracts deep features for items,
whereas our model learns deep features for both items and users.
7.3
Preliminaries
Before we describe our general framework, we discuss the preliminaries as follows.
7.3.1
Matrix Factorization
Matrix Factorization (MF) is the most effective collaborative ﬁltering approach. It
allows us to discover the latent factors of user-item interactions by factorizing the
interactions matrix into a joint latent space of user and item features respectively.
It proceeds by decomposing the original rating matrix R 2 Rmn consisting of
ratings by m users for n items into two low-rank matrices U 2 Rmd and V 2 Rnd
consisting of the user and item features respectively of rank d.
The system learns the latent factors by minimizing the following objective
function:
arg min
U;V l.R; U; V/ C ˇ.kUk2
F C kVk2
F/;
(7.1)

128
7
Robust Representations for Collaborative Filtering
where l.R; U; V/ is the loss function of predicting rating using the latent factors U
and V and the last two terms are the regularizations used to avoid overﬁtting. k  kF
denotes the Frobenius norm.
Many MF-based methods have been proposed, by designing a sophisticated loss
function l.R; U; V/. Existing works usually pose some assumptions on the latent
factors U and V in (7.1). Probabilistic matrix factorization (PMF) [32] provides
a probabilistic foundation by assuming a probabilistic linear model with Gaussian
observation noise and Gaussian priors on the latent factors.
p.RjU; V; 2/ D
M
Y
iD1
N
Y
jD1
N .RijjUT
i Vj; 2/Iij
(7.2)
p.Uj2
u/ D
M
Y
iD1
N .Uij0; 2
u / ; p.Vj2
v/ D
N
Y
jD1
N .Vjj0; 2
v/:
(7.3)
The model is ﬁtted by ﬁnding a MAP estimate of the parameters. Maximizing
the log posterior leads to the following objective function that can be solved using
stochastic gradient descent (SGD):
arg minU;V E D kR  UV>k2
F C ˇ.kUk2
F C kVk2
F/:
To improve the recommendation performance of PMF, Bayesian probabilistic
matrix factorization method (BPMF) [33] considers a full Bayesian treatment of the
parameter space instead of a point estimate used in PMF. In the weighted matrix
factorization (WMF), l.R; U; V/ D kC ˇ .R  UV>/k2
F, where C is the weight
matrix [10]. Our model is based upon the probabilistic matrix factorization approach
as it has shown to have a very good performance on several datasets and at the same
time is computationally more efﬁcient as compared to BPMF.
When side information are available, some MF methods make use of these
additional information via regression to predict the ratings [29].
7.3.2
Marginalized Denoising Auto-encoder (mDA)
As a speciﬁc form of neural network, an autoencoder takes a given input and maps
it (encodes) to a hidden representation via a deterministic mapping. Denoising
autoencoders reconstruct the input from a corrupted version of the data with the
motivation of learning a more robust mapping from the data. Various types of
autoencoders have been developed in the literature and have shown promising
results in several domains [13, 18]. Moreover, denoising autoencoders can be
stacked to construct a deep network also known as stacked denoising autoencoder

7.4
Our Approach
129
(SDA) which allows learning higher level representations [41]. Despite their
state-of-the-art performance, one of the main drawbacks of SDA is the high
computational cost of training, as they rely upon the iterative and numerical
optimization techniques to learn a large amount of model parameters.
Marginalized denoising auto-encoder (mDA) [6] is a variant of SDA that avoids
the high computational cost by marginalizing out the random feature corruption
and thus has a closed-form solution to learn model parameters. Therefore, mDA is
highly scalable and faster than SDA. It proceeds as follows:
Given a sample set X D Œx1;    ; xk, mDA considers multiple passes (e.g., c-
times) of random corruptions over X to obtain eX. It then reconstructs the input with
a mapping W that minimizes the squared loss as follows:
L .W/ D
1
2ck
cP
jD1
kP
iD1
kxi  WQxijk2;
(7.4)
where Qxij represents the jth corrupted version of the original input xi and W
represents the mapping that is expected to minimizes the loss function.
The above objective can be rewritten in the matrix form as
L .W/ D k NX  WeXk2
F;
(7.5)
where NX
D
ŒX;    ; X is the c-times repeated version of X, and QX is the
corresponding corrupted version. This problem is similar to the ordinary least
squares problem and has the analytical solution as given by W D SQ1, where
S D QX QXT, Q D NX QXT When c ! 1 in (7.4), we can derive the expectations of Q
and P, and obtain the closed form solution of the mDA [6]. Further, multiple mDAs
can be stacked to form a deep architecture, marginalized stacked denoising auto-
encoder (mSDA). mSDA usually enhances the performance of mDA. Most recently,
a nonlinear version of mDA is presented [5].
7.4
Our Approach
As we noted earlier, deep learning models have been proven to be very effective
in extracting high-level representations from the raw input data in several learning
tasks. The learned features represent high-level knowledge. In the collaborative
ﬁltering problem, we face a similar challenge of inferring effective latent and high-
level knowledge on user preferences from the raw inputs, including the rating
matrix and related features. MF based CF methods are able to capture the implicit
relationship between the users and the items successfully, but they suffer from the
cold start and data sparsity problems. Therefore, it is reasonable to draw strength
from the deep models to assist the collaborative ﬁltering process.

130
7
Robust Representations for Collaborative Filtering
Table 7.1
Summary of
notations
Notation
Description
m
Number of users
n
Number of items
d
Dimension of latent factors
p
Dimension of user features
q
Dimension of item features
R 2 Rmn
Rating matrix
U 2 Rmd
Latent factors of users
V 2 Rnd
Latent factors of items
X 2 Rpm
Side information of users
Y 2 Rqn
Side information of items
W1 2 Rpp
Mapping function for X in auto-encoder
P1 2 Rpd
Projection matrix for U
Table 7.1 summarizes the symbols used in our approach. Next, we describe a
general framework that integrates matrix factorization and deep feature learning.
7.4.1
Deep Collaborative Filtering (DCF): A General
Framework
In this section, we introduce the proposed deep collaborative ﬁltering (DCF)
framework which uniﬁes the deep learning models with MF based collaborative
ﬁltering. Figure 7.1 illustrates the idea of our DCF framework. DCF is a hybrid
model, which makes use of both rating matrix and side information and bridges
together matrix factorization and feature learning.
Given a user-item rating matrix R, the user side information X and the item side
information Y, DCF jointly decomposes R and learns latent factors (i.e., U, V) from
ratings and side information (i.e., X and Y) through the following formulation:
arg min
U;V l.R; U; V/ C ˇ.kUk2
F C kVk2
F/
CL .X; U/ C ıL .Y; V/;
(7.6)
where ˇ,  and ı are the trade-off parameters.
There are two key components in the DCF framework: (i) the function l.R; U; V/
for decomposing the rating matrix R into the two latent matrices; (ii) the functions
L .X; U/ and L .Y; V/ that connect the user/item contextual features with the latent
factors. The ﬁrst component derived through matrix factorization extracts latent
knowledge from the rating matrix. The second component devised using deep
learning models establishes connections of the side information with the latent
factors.

7.4
Our Approach
131
Input layer
Hidden layer
Output layer
Input layer
Hidden layer
Output layer
User 
Feature
Item
Feature
1
1
2
’2
User--Item
Rating matrix ≈
(
,
)
(
,
,
)
( ,
)
Fig. 7.1 Illustration of DCF framework. The inputs are user-item rating matrix R, the user feature
set X and the item feature set Y. Our approach jointly decomposes R and learns latent factors (i.e.,
U, V) from ratings and side information (i.e., X and Y). In particular, the latent factors are extracted
from the hidden layer of deep networks
7.4.2
DCF Using PMF + mDA
A natural instantiation of DCF is combining probabilistic matrix factorization
(PMF) with marginalized denoising auto-encoders (mDA). PMF is a widely applied
CF approach with excellent performance, and mDA is a powerful tool in extracting
high-level features from raw inputs. The combination of the two leverages their
beneﬁts for learning even richer models.
7.4.2.1
mDA Based Collaborative Filtering (mDA-CF)
Let NX 2 Rpcm and NY 2 Rqcn denote the c-times repeated versions of X and Y
respectively and let QX and QY denote their corrupted versions. As discussed before,
we utilize the loss function of PMF to decompose rating matrix R, i.e., l.R; U; V/ D
kAˇ.RUV>/k2
F, where A is the indicator matrix indicating the non-empty entries
in R and ˇ denotes the Hadamard or point-wise product. The objective function of
mDA-CF is formulated as follows:

132
7
Robust Representations for Collaborative Filtering
arg min
U;V;W1;
W2;P1;P2
LU.W1; P1; U/ C LV.W2; P2; V/
C˛kA ˇ .R  UV>/k2
F C ˇ.kUk2
F C kVk2
F/;
(7.7)
where
LU.W1; P1; U/ D k NX  W1 QXk2
F C kP1U>  W1Xk2
F;
LV.W2; P2; V/ D kNY  W2 QYk2
F C kP2V>  W2Yk2
F;
W1 2 Rpp and W2 2 Rqq are reconstructive mappings, P1 2 Rpd and P2 2 Rqd
are projection matrices, ˛, ˇ and  are trade-off parameters. Note that, we set  and
ı in (7.6) to 1 for simplicity.
The ﬁrst term in LU.W1; P1; U/ denotes the learning process in marginalized
denoising auto-encoder. It measures the reconstruction error between input user
features NX and the mapped features of corrupted inputs, i.e., W1 QX. W1 is the learned
mapping that is expected to minimize the loss. The second term connects the hidden
layer feature W1X and the latent factor U. Generally, the latent factor has much lower
dimension than the raw features. Therefore, we add a low-dimensional projection P1
that maps latent factor to the feature space.
Optimization
Although the optimization problem in (7.7) is not jointly convex in all the variables,
it is convex to each of them when ﬁxing the others. Hence, we can alternately
optimize for each of the variables in (7.7). The detailed procedures are provided
below.
First, we derive a solution to solve W1 and W2 using [6]. By ignoring the variables
irrelevant to W1, the objective (7.7) can be rewritten as
arg min
W1 k NX  W1 QXk2
F C kP1U>  W1Xk2
F:
(7.8)
Inspired by mDA, we consider the inﬁnitely many copies of noisy data, and
obtain the optimal solution
W1 D EŒS1EŒQ11;
(7.9)
where S1 D NX QX> C P1U>X> and Q1 D NX QX> C XX>. An efﬁcient solver for
calculating the expectations EŒS1 and EŒQ1 is provided in [6].
Similarly, we can derive the closed-form solution to W2
W2 D EŒS2EŒQ21;
(7.10)

7.4
Our Approach
133
where S2 D NY QY> C P2V>Y> and Q2 D NY QY> C YY>.
Next, by dropping the irrelevant variables w.r.t. P1, the objective function
becomes
arg min
P1 kP1U>  W1Xk2
F:
(7.11)
We can obtain the closed-form solution as
P1 D W1XU.U>U/1:
(7.12)
Similarly, the optimal solution of P2 is
P2 D W2YV.V>V/1:
(7.13)
To solve for the latent factors U and V, we use the popular stochastic gradient
descent (SGD) algorithm. In particular, when other variables irrelevant to U and V
are ﬁxed, we use f.U; V/ to denote the objective in (7.7). The update rules are:
ui D ui   @
@ui f.U; V/;
vj D vj   @
@vj f.U; V/;
(7.14)
where  is the learning rate, and the detailed derivatives are deﬁned as
@f.U;V/
@ui
D .P>
1 .P1ui  .W1X/i// C ˇui
˛
X
.i;j/2A
.Ri;j  uiv>
j /vj:
(7.15)
@f.U;V/
@vj
D .P>
2 .P2vj  .W2Y/j// C ˇvj
˛
X
.i;j/2A
.Ri;j  uiv>
j /ui:
(7.16)
The above steps are repeated until convergence. Finally, we obtain the latent
factors U and V.
Algorithm Complexity
The steps of our mDA-CF approach are summarized in Algorithm 7.1. The learned
latent factors U and V can be used to predict missing entries in the rating matrix.
In Algorithm 7.1, we have analytical solutions of Steps 3–6 which are efﬁcient to

134
7
Robust Representations for Collaborative Filtering
Algorithm 7.1 mDA-CF approach
Input: Rating matrix R, user features X, item features Y,
parameters , ˛, ˇ.
Output: Latent factors U, V
1: Initialize U, V, P1 and P2;
2: while validation error decreases, do
3:
Update W1 using (7.9);
4:
Update W2 using (7.10);
5:
Update P1 using (7.12);
6:
Update P2 using (7.13);
7:
for each observed Rij, do
8:
Update ui using (7.14);
9:
Update vj using (7.14);
10:
end for
11: end while
compute. The matrix multiplication and inversion used in Step 5 and Step 6 cost
O.p2m C pmd C d3/ and O.q2n C qnd C d3/, respectively. The Steps 8–9 are
implemented in a batch-learning fashion, and cost O.tN/ to evaluate the gradients,
where t is the number of iterations and N is the number of training ratings/responses
in R. Considering that N  maxfm; n; dg, the time complexity of Algorithm 7.1
is mainly determined by O.tN/. Hence, our approach owns a good scalability. We
discuss the settings of the parameters in the experimental section. To further reduce
the computational cost, some advanced distributed optimization algorithms could
be applied to our model [45].
7.4.2.2
Stacked mDA Based Collaborative Filtering (mSDA-CF)
Existing literature show that stacking multiple deep learning layers together can
usually generate rich features in the form of hidden layers, and therefore results in
better performance for various learning tasks. Inspired by the marginalized stacked
denoising auto-encoders (mSDA) [6], we stack multiple mDA together, and present
the mSDA-CF approach.
We assume that only one hidden layer should be close to the latent factor.
The reasons are two-fold. First, latent factors are high-level representations, which
should correspond to the deeper layers in deep models. Secondly, latent factors
should be unique, but different hidden layers have various representations. There-
fore, enforcing the similarity between multiple hidden layers and latent factors is
unreasonable.
In our mSDA-CF model, we assume that the latent factors are generated from
the
 lC1
2
˘
layer, given the total number of layers is l. When we train the model
for the rest layers, the parameters , ˛ and ˇ are simply set to 0. In particular, if
i ¤
 lC1
2
˘
, we only need to update Wi
1 and Wi
2 and ignore the other steps, where

7.4
Our Approach
135
Algorithm 7.2 mSDA-CF approach
Input: Rating matrix R, user features X, item features Y
, ˛, ˇ, layers l.
Output: Latent factors U, V
1: for i 1 W l, do
2:
if i D
j
lC1
2
k
, do
3:
Update U and V using Algorithm 7.1, by setting
valid values to , ˛ and ˇ;
4:
otherwise
5:
Update Wi
1 and Wi
2 using Algorithm 7.1, by setting
 D 0, ˛ D 0 and ˇ D 0;
6:
end if
7: end for
Wi
1 and Wi
2 denote the mappings in the i-th layer. One beneﬁt of such setting is the
time efﬁciency, as we do not increase too much computational burden when adding
multiple layers.
Another interesting problem is how to set the number of layers. The number of
layers implies the model complexity, which is usually related to the learning task
and the size of training data. In the experiments we will discuss the inﬂuence of
different number of layers. The detailed procedures of mSDA-CF are summarized
in Algorithm 7.2.
7.4.3
Discussion
We notice that most existing deep learning based collaborative ﬁltering methods
can be uniﬁed in our DCF framework. For example, Oord et al. [27] use deep
convolutional neural networks (CNN) to predict latent factors from music audio
using the following objective function:
min

P
u;i
cu;i.pu;i  x>
u y0
i/2;
(7.17)
where xu, the latent factor of user, is estimated from weighted matrix factorization
beforehand, and y0
i, the latent factors of audio, are learned from CNN, i.e., y0
i D
CNN.fv; ˝/. Here, fv denotes audio contents, and ˝ denotes the model parameters
in CNN. Equation (7.17) can be interpreted in our DCF formulation (i.e., Eq. (7.6)).
First, it utilizes the weighted matrix factorization as the loss function l./. Secondly,
the regularization function L .Y; V/ is implemented by CNN.
Wang et al. [42] utilize deep belief networks (DBN) to build a hybrid model for
collaborative ﬁltering. The objective function is

136
7
Robust Representations for Collaborative Filtering
LHybrid D
X
u;v2I
.ruv  ˇ0
uxv  r0
uyv/2 C ˇkˇ  
k2
F
Ckk2
F C ykyk2
F:
(7.18)
where xv, the latent factor of item, is obtained from DBN, i.e., xv D DBN.fv; ˝/.
Also, we can interpret model (7.18) in our DCF framework. First, loss function
l./ is implemented in a hybrid way, i.e., rating ruv is predicted by the sum of the
CF part r0
uyv and the content part ˇ0
uxv. Secondly, DBN is employed to map the
content features fv and latent factor xv, which can be formulated by L .Y; V/ in
DCF framework. Meanwhile,  in (7.6) is set to 0.
Wang et al. [43] propose a model with Bayesian stacked denoising auto-encoders
(SDAE) and collaborative topic regression are integrated. The objective function is
L D  P
i;j
cij
2 .rij  u>
i vj/2  n
2
P
j
kfr.X0;j; WC/  Xc;jk2
2
 v
2
P
j
kvj  fe.X0;j; WC/>k2
2  freg;
(7.19)
where fe./ and fr./ denote the encoding and decoding functions in SDAE, n and v
denote the trade-off parameters, and freg denote the regularization terms that prevent
overﬁtting.
Obviously, the model (7.19) can also be interpreted in the DCF framework. The
loss function for decomposing rating matrix in (7.19) is in the standard matrix
factorization fashion. Further, the second and the third terms in (7.19) infer the item
latent factor using SDAE, which can be abstracted as L .Y; V/ in DCF.
In summary, the existing deep collaborative ﬁltering methods [27, 42, 43] can be
uniﬁed in a common framework, DCF. We also notice that the existing models only
infer latent factors of items using deep models, whereas the latent factors of users
are generated in traditional ways. Compared to existing works, our DCF framework
provides a more ﬂexible way to explore effective latent factors for users and/or items
via deep models.
7.5
Experiments
We evaluate the performance of our mDA-CF and mSDA-CF approaches on
three challenging tasks that are movie recommendation, book recommendation and
response prediction.

7.5
Experiments
137
7.5.1
Movie Recommendation
For movie recommendation, we conduct experiments on two benchmark datasets
MovieLens-100K and MovieLens-1M,2 which are commonly used for evaluating
collaborative ﬁltering algorithms.
The MovieLens-100K dataset contains 100K ratings of 943 users and 1682
movies, and the MovieLens-1M dataset consists of about 1 million ratings of 6040
users and 3706 movies. Each rating is an integer between 1 (worst) and 5 (best).
The ratings are highly sparse. Table 7.2 summarizes the statistics of datasets. We
extract the features from side information of users and movies to construct X and
Y. To summarize, the user information which consists of the user‘s age, gender and
occupation were encoded into a binary valued vector of length 28. Similarly, the
item feature information which consists of the 18 category of movie genre were
encoded into a binary valued vector of length 18. Ratings were normalized to be
zero-mean.
As our model (7.7) is an extension of the representative collaborative ﬁlter-
ing method PMF, we mainly compare our approach with PMF and its several
variants, such as the Biased PMF [15] and sparse covariance matrix factorization
(SCMF) [36]. PMF and Biased PMF are special cases of our approach mDA-CF. For
example, by adding zero weight to the ﬁrst two terms in (7.7), mDA-CF degrades to
PMF. Further, since our approach takes advantage of the side information of users
and movies, we also compare our approach with the collaborative ﬁltering method
that incorporates side information, such as the Bayesian matrix factorization with
side information (BMFSI) [30].
We employ the root mean squared error (RMSE) as the evaluation metric. RMSE
is deﬁned as:
RMSE D
v
u
u
t 1
N
X
i;j
ZP
ij.Rij  ORij/2;
(7.20)
where Rij is the ground-truth rating of user i for item j, NRij denotes the corresponding
predicted rating, N is the total number of ratings in the test set, and ZP
ij is a binary
matrix that indicates test ratings.
Table 7.2 Statistics of datasets used in our experiments
Dataset
#Users
#Items
Sparsity
User features
Item features
ML-100K
943
1682
93.7%
Age, gender, and occupation
Genres
ML-1M
6040
3706
95.8%
Age, gender, and occupation
Genres
Book-Crossing
278858
271379
99.9%
Age, country, city, etc.
Title, year, etc.
Advertising
448158
737
99.7%
Age, geolocation, domain, etc.
Ad size, etc.
2http://grouplens.org/datasets/movielens/

138
7
Robust Representations for Collaborative Filtering
Table 7.3 Average RMSE
(with standard deviation) of
compared methods with
different percentages of
training data on
MovleLens-100K dataset
99%
Method
d D 10
d D 20
PMF [32]
0:9184 ˙ 0:0265
0:9164 ˙ 0:0261
Biased PMF [15]
0:8953 ˙ 0:0189
0:8923 ˙ 0:0150
BMFSI [30]
0:8912 ˙ 0:0127
0:8905 ˙ 0:0154
SCMF [36]
0:8891 ˙ 0:0146
0:8896 ˙ 0:0198
mDA-CF (Ours)
0:8874 ˙ 0:0142
0:8861 ˙ 0:0153
mSDA-CF (Ours)
0.8852 ˙ 0.0135
0.8849 ˙ 0.0167
80%
Method
d D 10
d D 20
PMF [32]
0:9223 ˙ 0:0056
0:9190 ˙ 0:0052
Biased PMF [15]
0:9135 ˙ 0:0039
0:9087 ˙ 0:0030
BMFSI [30]
0:9114 ˙ 0:0031
0:9065 ˙ 0:0029
SCMF [36]
0:9092 ˙ 0:0033
0:9068 ˙ 0:0036
mDA-CF (Ours)
0:9043 ˙ 0:0043
0:9040 ˙ 0:0045
mSDA-CF (Ours)
0.9035 ˙ 0.0028
0.9024 ˙ 0.0030
50%
Method
d D 10
d D 20
PMF [32]
0:9524 ˙ 0:0023
0:9506 ˙ 0:0024
Biased PMF [15]
0:9388 ˙ 0:0029
0:9337 ˙ 0:0020
BMFSI [30]
0:9371 ˙ 0:0023
0:9335 ˙ 0:0025
SCMF [36]
0:9334 ˙ 0:0025
0:9331 ˙ 0:0021
mDA-CF (Ours)
0:9312 ˙ 0:0026
0:9311 ˙ 0:0025
mSDA-CF (Ours)
0.9309 ˙ 0.0026
0.9308 ˙ 0.0028
For all the compared methods, we set the regularization parameters (e.g., , ˛ and
ˇ) via 5-fold cross validation. Following the experimental settings in [36], we train
each compared method with different percentages (50%, 80%, and 99%) of ratings.
The training data are randomly chosen from each dataset, and the remaining data
are used for testing. This process is repeated ﬁve times, and we report the average
RMSE.
For the MovieLens-100K dataset, the parameters ˛, ˇ and  are set to 0.7, 0.004
and 0.2, respectively. The learning rate used in SGD is set to 0.002. Table 7.3
shows the average RMSE (with standard deviations) of baselines PMF, Biased PMF,
BMFSI, SCMF and our approaches, mDA-CF and mSDA-CF, on the MovieLens-
100K dataset. For each method, we have two settings for the dimensions of latent
factors, including d D 10 and d D 20. We can observe from Table 7.3 that
(a) Our approaches (mDA-CF and mSDA-CF) achieve much better performance
than PMF and Biased PMF, which are special cases of our approach. It
demonstrates the effectiveness of incorporating side information and deep
architectures.
(b) BMFSI is a Bayesian matrix factorization method that utilizes side information,
so it performs better than PMF and Biased PMF that ignore such information.

7.5
Experiments
139
Our approach outperforms BMFSI, which validates the strengths of the latent
factors learned by marginalized denoising auto-encoders.
(c) Usually, deep models with multiple layers lead to better performance. Our
mSDA-CF slightly enhances the performance of mDA-CF. We will show the
inﬂuence of different number of layers in the next section.
(d) Note that the basic component in our approach is PMF. Actually, DCF is a
general framework for collaborative ﬁltering. When we implement l.R; U; V/
in (7.6) as some advanced MF methods (e.g., weighted matrix factorization),
the results could be further improved.
For the MovieLens-1M dataset, the parameters ˛, ˇ and  are set to 0.8, 0.003
and 0.3, respectively. Table 7.4 shows the average RMSE (with standard deviations)
of our approach and compared methods on the MovieLens-1M dataset. Basically,
Table 7.4 shows similar phenomenon to that we observed from Table 7.3. The
proposed mDA-CF and mSDA-CF approaches consistently achieve lower RMSE
than compared methods. As before, we evaluate each method in two settings with
different dimension of latent factors. Usually, d D 20 generates better results than
d D 10 on the two MovieLens datasets.
Table 7.4 Average RMSE
(with standard deviation) of
compared methods with
different percentages of
training data on
MovleLens-1M dataset
99%
Method
d D 10
d D 20
PMF [32]
0:8424 ˙ 0:0071
0:8388 ˙ 0:0059
Biased PMF [15]
0:8408 ˙ 0:0070
0:8367 ˙ 0:0067
BMFSI [30]
0:8391 ˙ 0:0067
0:8340 ˙ 0:0069
SCMF [36]
0:8364 ˙ 0:0065
0:8323 ˙ 0:0065
mDA-CF (Ours)
0:8335 ˙ 0:0064
0:8317 ˙ 0:0062
mSDA-CF (Ours)
0.8320 ˙ 0.0063
0.8304 ˙ 0.0057
80%
Method
d D 10
d D 20
PMF [32]
0:8559 ˙ 0:0022
0:8512 ˙ 0:0017
Biased PMF [15]
0:8531 ˙ 0:0019
0:8493 ˙ 0:0020
BMFSI [30]
0:8503 ˙ 0:0017
0:8478 ˙ 0:0019
SCMF [36]
0:8496 ˙ 0:0019
0:8465 ˙ 0:0018
mDA-CF (Ours)
0:8449 ˙ 0:0015
0:8429 ˙ 0:0013
mSDA-CF (Ours)
0.8416 ˙ 0.0014
0.8407 ˙ 0.0011
50%
Method
d D 10
d D 20
PMF [32]
0:8790 ˙ 0:0009
0:8745 ˙ 0:0011
Biased PMF [15]
0:8766 ˙ 0:0015
0:8722 ˙ 0:0012
BMFSI [30]
0:8742 ˙ 0:0016
0:8703 ˙ 0:0010
SCMF [36]
0:8707 ˙ 0:0013
0:8678 ˙ 0:0007
mDA-CF (Ours)
0:8655 ˙ 0:0007
0:8645 ˙ 0:0006
mSDA-CF (Ours)
0.8628 ˙ 0.0005
0.8613 ˙ 0.0006

140
7
Robust Representations for Collaborative Filtering
Table 7.5 MAE of
compared methods on
MovieLens-100K dataset
Method
Mean Absolute Error (MAE)
PMF [32]
0.793
U-RBM [7]
0.779
I-RBM [7]
0.775
I-RBM+INB [7]
0.699
UI-RBM [7]
0.690
mDA-CF (Ours)
0.683
mSDA-CF (Ours)
0.680
In addition, we compare our approaches with the state-of-the-art deep learning
based collaborative ﬁltering method, a joint user-item based restricted Boltzmann
machine (UI-RBM) [7]. To conduct fair comparisons with UI-RBM, we adopt the
mean absolute error (MAE) used in [7] as evaluation metric. The MAE is deﬁned as
follows
MAE D
LP
i
jgi  pij
L
;
(7.21)
where gi is the ground truth rating, pi is the predicted rating, and L is the total
number of ratings.
We follow the experimental settings in [7], and conduct 5-fold cross-validations.
The dimension of latent factors is set to 20. Table 7.5 shows the average MAE of
compared methods on the MovieLens-100K dataset. U-RBM and I-RBM denote
the user-based model and item-based model, respectively. They are the baselines
used in [7]. Table 7.5 shows that UI-RBM achieves much better performance
than traditional methods like PMF, as it takes advantage of the deep feature
learning. Our mDA-CF and mSDA-CF approaches obtain lower MAE than UI-
RBM, demonstrating the effectiveness of our DCF framework compared to the
RBM based deep learning models.
7.5.2
Book Recommendation
For book recommendation, we utilize the Book-Crossing dataset,3 which contains
1149780 ratings for 271379 books from 278858 users. The rating scale is from 0
to 10 with the higher score indicating the more preference. Some attributes of users
and books are also provided in this dataset. These attributes are encoded to binary
vectors, which form the feature sets for users and books. Table 7.2 shows some
statistics of this dataset.
3http://www2.informatik.uni-freiburg.de/~cziegler/BX/

7.5
Experiments
141
Table 7.6 RMSE of
compared methods on
Book-Crossing data
Method
RMSE (d D 10)
RMSE (d D 50)
PMF [32]
3.7483
3.7452
ISMF [23]
3.7440
3.7415
CIMF [19]
3.7398
3.7372
mDA-CF (Ours)
3.6610
3.6528
mSDA-CF (Ours)
3.6592
3.6513
We follow the settings in [19], and conduct 5-fold cross-validation. The baselines
include PMF, the implicit social matrix factorization (ISMF) [23] and the coupled
item-based matrix factorization (CIMF) [19]. ISMF incorporates the implicit social
relationships between users and between items. CIMF makes use of the attributes of
books in the model.
Table 7.6 shows the RMSE of all compared methods. We can observe that
ISMF and CIMF obtain better results than the conventional PMF method, as they
incorporate side information to their models. Our approaches obtain much lower
RMSE than PMF, ISMF and CIMF in different settings. mSDA-CF achieves the
best results among all competitors.
7.5.3
Response Prediction
Response prediction is another interesting application of collaborative ﬁltering [26].
In particular, we consider a speciﬁc type of response prediction, which is click
prediction. Given an online advertisement, our task is to predict whether a given
user will click it or not in the near future.
Previous research works have proved that collaborative ﬁltering (CF) methods
are suitable for addressing the click prediction problem. Unfortunately, there are
few datasets available for evaluating the click prediction performance of CF models.
To evaluate the performance of our model in real-world applications, we collected
an advertising dataset at a large software company. The dataset is collected from its
website, which contains the click responses for advertisements over 3 million users.
For our purpose, we analyze the data from a 2-month period, from October 1, 2013
to November 30, 2013.
The dataset used in our experiments contains 737 ads and 448,158 users. It also
has the impression and click data of the advertisements. For each click event, we
have the user ID, day, ad ID, page ID, country, browser, advertiser ID, and size of
ad. Each record can be uniquely identiﬁed by a (user, ad, day) triplet. In addition,
we have information about the user proﬁles. For each user, we have some attributes
such as country, domain, etc. Apart from the user information, our dataset contains
the meta-information of the ads such as ad size. In the experiments, we encode the
demographic information (e.g., country, state, domain) into a binary valued vector

142
7
Robust Representations for Collaborative Filtering
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
FAR
FRR
PMF
Biased PMF
BMFSI
SCMF
mDA−CF
mSDA−CF
Fig. 7.2 ROC curves of our approach and compared methods on Advertising dataset (d = 10)
for each user. The attributes of ads (e.g., advertiser, ad size) are also encoded into
binary vectors. Some statistics of this dataset can be found in Table 7.2.
The advertising dataset used in our experiments is very sparse, which contains
880,569 click responses (density: 0.27%). We use the ﬁrst 50% click responses
for training, and the remaining data for testing. Following [2], we use the receiver
operating characteristic (ROC) curve and the area under ROC (AUC-ROC) as our
evaluation metrics. The true positive rate (TPR) and false positive rate (FPR) used
for generating ROC curves are deﬁned as follows:
TPR D
TP
TP C FN ;
FPR D
FP
FP C TN ;
(7.22)
where TP represents the true positives, FN represents the false negatives, TN
represents the true negatives, and FP represents the false positives.
We evaluate the performance of each compared method on the Advertising
dataset. The major parameters ˛, ˇ and  are set to 0.8, 0.02 and 0.12, respec-
tively. Two settings are utilized, by setting the latent factor dimension to 10
and 20, respectively. Figure 7.2 shows the ROC curves of PMF, Biased PMF,
BMFSI, SCMF, mDA-CF and mSDA-CF. Table 7.7 shows the corresponding
AUC of all compared methods. We can observe that our approaches obtain
higher AUC than other methods, which demonstrates the effectiveness of our
framework.

7.5
Experiments
143
Table 7.7 AUC of compared
methods on advertising
dataset
Method
AUC (d D 10)
AUC (d D 20)
PMF [32]
0.7651
0.7716
Biased PMF [15]
0.7692
0.7724
BMFSI [30]
0.7720
0.7805
SCMF [36]
0.7782
0.7866
mDA-CF (Ours)
0.7961
0.8023
mSDA-CF (Ours)
0.8057
0.8115
10
20
30
40
50
60
70
80
90
0.92
0.93
0.94
Percentage of Noise (%)
RMSE
mDA−CF
mSDA−CF
0
50
100
150
200
250
300
350
400
0.94
0.98
1.02
1.06
1.10
Epochs
RMSE
PMF
mDA−CF
mSDA−CF
(b)
(a)
1
2
3
4
5
6
7
8
0.92
0.93
Number of Layers
RMSE
(c)
Fig. 7.3 Experimental analysis on MovieLens-100K dataset: (a) RMSE with different level of
noise; (b) RMSE with epochs; (c) RMSE of mSDA-CF with different number of layers
7.5.4
Discussion
So far, we have seen that our approach outperforms the existing approaches on the
different datasets. We also analyze the convergence property and parameter settings
of our approach on the MovieLens-100K dataset. In Fig. 7.3a, we show the RMSE
of mDA-CF and mSDA-CF with different levels of noise. We observe an interesting

144
7
Robust Representations for Collaborative Filtering
phenomena that the RMSE of mDA-CF slightly decreases when increasing the noise
level for input samples at ﬁrst; mSDA-CF achieves the best performance when
adding 50% noise. However, when the percentage of noise is larger than 50%,
mDA-CF outperforms mSDA-CF. It shows that the latent factors learned from multi-
layer models might be unreliable if there are too much noise contained in the input
samples.
Figure 7.3b shows the RMSE in different iterations of PMF and our approaches.
We see that PMF overﬁts the data after 100 epochs. Although our approaches
have larger RMSE from epoch 40 to epoch 120, they keep reducing the RMSE
even after 400 epochs. It shows that our approach enjoys better stability. Also, by
incorporating the side information of users and items, the learned model has a good
generalization ability.
Another important property in our approach is the setting of stacked layers.
Figure 7.3c shows the RMSE of mSDA-CF with different number of layers.
In general, stacking multiple auto-encoder will lead to better performance. But
Fig. 7.3c shows that the improvement becomes marginal when there are more than
5 layers. The reasons are two-fold. First, the model capacity is related to the size
of training data. In the experiments, training a very complicated deep model (with
many layers) with the employed datasets may be unreasonable. Secondly, accessing
rich features of users is always a challenging issue in reality. In our model, these
features are the inputs of deep models. Our model achieves better performance
than existing works by using these features. However, the features are not extensive
enough to train deep models. In a word, this parameter should be carefully tuned in
practice.
7.6
Summary
In this chapter, we propose a deep collaborative ﬁltering (DCF) framework, which
bridges matrix factorization and deep feature learning. DCF is a hybrid collaborative
ﬁltering model, as it learns effective latent factors from both user-item ratings and
side information. Using this framework, we present the mDA-CF and mSDA-CF
approaches by incorporating the probabilistic matrix factorization and marginalized
denoising auto-encoders. We also design efﬁcient optimization algorithms to solve
the models. Extensive experimental results on the MovieLens-100K, MovieLens-
1M, Book-Crossing and Advertising datasets demonstrate the effectiveness of the
latent factors learned by our model. Our mDA-CF and mSDA-CF approaches
outperform related methods on the tasks of movie recommendation, book recom-
mendation and response prediction. They also achieve better performance than
the existing deep learning based collaborative ﬁltering method such as UI-RBM.
In addition, the convergence property and parameter settings of our model are
discussed in the experiments.

References
145
A part of the future work is to extend other deep learning and matrix factorization
methods using our DCF framework and evaluate their performance for collaborative
ﬁltering. Another future direction is to apply the distributed optimization algorithms
to further reduce the computational costs of our algorithms.
References
1. Adams, R.P., Dahl, G.E., Murray, I.: Incorporating side information in probabilistic matrix
factorization with gaussian processes. In: UAI, pp. 1–9 (2010)
2. Ahmed, A., Das, A., Smola, A.J.:
Scalable hierarchical multitask learning algorithms for
conversion optimization in display advertising. In: WSDM, pp. 153–162 (2014)
3. Bennett, J., Lanning, S.: The netﬂix prize. In: Proceedings of KDD Cup and Workshop, p. 35
(2007)
4. Chatzis, S.: Nonparametric Bayesian multitask collaborative ﬁltering. In: CIKM, pp. 2149–
2158 (2013)
5. Chen, M., Weinberger, K.Q., Sha, F., Bengio, Y.: Marginalized denoising auto-encoders for
nonlinear representations. In: ICML, pp. 1476–1484 (2014)
6. Chen, M., Xu, Z.E., Weinberger, K.Q., Sha, F.:
Marginalized denoising autoencoders for
domain adaptation. In: ICML (2012)
7. Georgiev, K., Nakov, P.:
A non-iid framework for collaborative ﬁltering with restricted
boltzmann machines. In: ICML, pp. 1148–1156 (2013)
8. Hinton, G.E., Salakhutdinov, R.R.: Reducing the dimensionality of data with neural networks.
Science 313(5786), 504–507 (2006)
9. Hinton, G., Osindero, S., Teh, Y.-W.: A fast learning algorithm for deep belief nets. Neural
comput. 18(7), 1527–1554 (2006)
10. Hu, Y., Koren, Y., Volinsky, C.: Collaborative ﬁltering for implicit feedback datasets. In:
ICDM, pp. 263–272 (2008)
11. Hu, L., Cao, J., Xu, G., Cao, L., Gu, Z., Zhu, C.: Personalized recommendation via cross-
domain triadic factorization. In: WWW, pp. 595–606 (2013)
12. Hu, L., Cao, J., Xu, G., Cao, L., Gu, Z., Cao, W.: Deep modeling of group preferences for
group-based recommendation. In: AAAI, pp. 1861–1867 (2014)
13. Kavukcuoglu, K., Ranzato, M., Fergus, R., Le-Cun, Y.: Learning invariant features through
topographic ﬁlter maps. In: CVPR, pp. 1605–1612. IEEE, Los Alamitos (2009)
14. Kim, Y.-D., Choi, S.: Scalable variational Bayesian matrix factorization with side information.
In: AISTATS, pp. 493–502 (2014)
15. Koren, Y.: Factorization meets the neighborhood: a multifaceted collaborative ﬁltering model.
In: KDD, pp. 426–434 (2008)
16. Koren, Y., Bell, R.M., Volinsky, C.: Matrix factorization techniques for recommender systems.
IEEE Comput. 42(8), 30–37 (2009)
17. Lee, D.D., Seung, H.S.: Algorithms for non-negative matrix factorization. In: NIPS, pp. 556–
562 (2001)
18. Lee, H., Pham, P., Largman, Y., Ng, A.Y.: Unsupervised feature learning for audio classiﬁcation
using convolutional deep belief networks. In: NIPS, pp. 1096–1104 (2009)
19. Li, F., Xu, G., Cao, L.: Coupled item-based matrix factorization. In: WISE, pp. 1–14 (2014)
20. Li, S., Kawale, J., Fu, Y.: Deep collaborative ﬁltering via marginalized denoising auto-encoder.
In: Proceedings of the 24th ACM International on Conference on Information and Knowledge
Management, pp. 811–820. ACM, New York (2015)
21. Li, S., Kawale, J., Fu, Y.: Predicting user behavior in display advertising via dynamic collective
matrix factorization. In: Proceedings of the 38th International ACM SIGIR Conference on
Research and Development in Information Retrieval, pp. 875–878. ACM, New York (2015)

146
7
Robust Representations for Collaborative Filtering
22. Liu, H., Shao, M., Li, S., Fu, Y.: Inﬁnite ensemble for image clustering. In: Proceedings
of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
pp. 1745–1754 (2016)
23. Ma, H.: An experimental study on implicit social recommendation. In: SIGIR, pp. 73–82
(2013)
24. Ma, H., Zhou, D., Liu, C., Lyu, M.R., King, I.:
Recommender systems with social
regularization. In: WSDM, pp. 287–296 (2011)
25. Mazumder, R., Hastie, T., Tibshirani, R.: Spectral regularization algorithms for learning large
incomplete matrices. J. Mach. Learn. Res. 11, 2287–2322 (2010)
26. Menon, A.K., Chitrapura, K.P., Garg, S., Agarwal, D., Kota, N.: Response prediction using
collaborative ﬁltering with hierarchies and side-information. In: KDD, pp. 141–149 (2011)
27. Oord, A.V.D., Dieleman, S., Schrauwen, B.: Deep content-based music recommendation. In:
NIPS, pp. 2643–2651 (2013)
28. Ouyang, Y., Liu, W., Rong, W., Xiong, Z.: Autoencoder-based collaborative ﬁltering. In:
ICONIP, pp. 284–291 (2014)
29. Park, S., Kim, Y.-D., Choi, S.: Hierarchical Bayesian matrix factorization with side informa-
tion. In: IJCAI (2013)
30. Porteous, I., Asuncion, A.U., Welling, M.: Bayesian matrix factorization with side information
and dirichlet process mixtures. In: AAAI (2010)
31. Ricci, F., Rokach, L., Shapira, B., Kantor, P.B.: Recommender Systems Handbook, vol. 1.
Springer, New York (2011)
32. Salakhutdinov, R., Mnih, A.: Probabilistic matrix factorization. In: NIPS (2007)
33. Salakhutdinov, R., Mnih, A.: Bayesian probabilistic matrix factorization using Markov chain
Monte Carlo. In: ICML, pp. 880–887. ACM, New York (2008)
34. Salakhutdinov, R., Mnih, A., Hinton, G.E.: Restricted Boltzmann machines for collaborative
ﬁltering. In: ICML, pp. 791–798 (2007)
35. Shao, M., Li, S., Ding, Z., Fu, Y.: Deep linear coding for fast graph clustering. In: Proceedings
of the International Joint Conference on Artiﬁcial Intelligence, pp. 3798–3804 (2015)
36. Shi, J., Wang, N., Xia, Y., Yeung, D., King, I., Jia, J.:
SCMF: sparse covariance matrix
factorization for collaborative ﬁltering. In: IJCAI (2013)
37. Singh, A.P., Gordon, G.J.: Relational learning via collective matrix factorization. In: KDD,
pp. 650–658 (2008)
38. Singh, A.P., Gordon, G.J.: A Bayesian matrix factorization model for relational data. CoRR,
abs/12033517 (2012)
39. Su, X., Khoshgoftaar, T.M.: A survey of collaborative ﬁltering techniques.
Advances in
Artiﬁcial Intelligence. 4, (2009)
40. Truyen, T.T., Phung, D.Q., Venkatesh, S.:
Ordinal Boltzmann machines for collaborative
ﬁltering. In: UAI, pp. 548–556 (2009)
41. Vincent, P., Larochelle, H., Bengio, Y., Manzagol, P.-A.: Extracting and composing robust
features with denoising autoencoders. In: ICML, pp. 1096–1103. ACM, New York (2008)
42. Wang, X., Wang, Y.: Improving content-based and hybrid music recommendation using deep
learning. In: ACM MM, pp. 627–636 (2014)
43. Wang, H., Wang, N., Yeung, D.: Collaborative deep learning for recommender systems. CoRR,
abs/1409.2944 (2014)
44. Xu, M., Zhu, J., Zhang, B.: Fast max-margin matrix factorization with data augmentation. In:
ICML (2013)
45. Yu, Z., Shi, X., Yan, L., Li, W.: Distributed stochastic ADMM for matrix factorization. In:
CIKM, pp. 1259–1268 (2014)
46. Zhao, T., McAuley, J.J., King, I.:
Leveraging social connections to improve personalized
ranking for collaborative ﬁltering. In: CIKM, pp. 261–270 (2014)

Chapter 8
Robust Representations for Response Prediction
Abstract Measuring the performance of display advertising is an important prob-
lem in estimating campaign effectiveness and understanding user behaviors. The
two key performance indicators are the click-through rates (CTR) of the ads
and conversion rates (CVR) on the advertisers website. Existing approaches for
conversion prediction and for click prediction usually look at the two problems
in isolation. However there is considerable beneﬁt in jointly solving the problems
as the two goals are often intertwined. In this chapter, we aim to predict the
conversion response of the users by jointly examining the past purchase behavior
and the click response behavior. To achieve this, we explicitly model the temporal
dynamics between the click response and purchase activity into a uniﬁed framework.
More speciﬁcally, we propose a novel matrix factorization approach named the
dynamic collective matrix factorization (DCMF) to address this problem. Our model
considers temporal dynamics of post-click conversions and also takes advantages of
the side information of users, advertisements, and items. An efﬁcient optimization
algorithm based on stochastic gradient descent is presented in the chapter. We
further show that our approach can be used to measure the effectiveness of
advertisements. Our approach and several representative baselines are evaluated on
a public dataset and a real-world marketing dataset. Extensive experimental results
demonstrate that our model achieves signiﬁcant improvements over the baselines.
8.1
Overview1
Digital marketing has become an effective way to reach out to consumers, in order to
promote brands, attract consumer attention, and increase sales. Typically consumers
can be inﬂuenced via targeted advertisements on various channels, such as websites,
email, social networks, etc. Display advertising is a popularly used medium for
targeting consumers, which allows advertisers to place graphical advertisements on
the publishers’ web pages. Generally the advertisement campaigns fall under two
broad categories: (1) brand advertisements and (2) direct response advertisements.
1This chapter is reprinted with permission from ACM. “Predicting User Behavior in Display
Advertising via Dynamic Collective Matrix Factorization”, the International ACM SIGIR
Conference on Research and Development in Information Retrieval, 2015.
© Springer International Publishing AG 2017
S. Li, Y. Fu, Robust Representation for Data Analytics, Advanced Information
and Knowledge Processing, DOI 10.1007/978-3-319-60176-2_8
147

148
8
Robust Representations for Response Prediction
The goal of brand advertising is to create an awareness of the brand that could
possibly lead to potential increase in sales in the future. The goal of direct
response advertisements on the other hand is to create an impulse leading to a sale
immediately. In this chapter, we mainly focus on understanding the user behavior
for direct response advertisements.
Traditionally the click-through rate (CTR) has been used as a central measure
for evaluating the performance of a direct response advertisement campaign. In
particular, the focus of the advertisement publisher so far has been to maximize
the number of clicks on an ad. An alternate strategy that has gained attention in
the recent past is to maximize the conversion rate (CVR) instead of just ad-clicks,
as many advertisers would prefer not to pay for an ad impression unless it leads
to a conversion. The conversion could either imply revenue generated via buying
a product or subscription on the website, or could mean account creation, etc. In
both strategies it is important to understand the user behavior and predict the user’s
response so as to have better targeting of the ads and as a result higher conversions.
The problem of click prediction and conversion prediction has mainly been
studied in isolation in the past decade. Researchers have successfully applied
novel strategies for click prediction and there has been signiﬁcant work in the
area [1, 2, 12, 18, 25, 43, 52]. They design models from various perspectives to
improve the prediction accuracy of click-through rate. But the click-through rate
cannot be directly linked to conversions. On the other hand, some conversion
prediction has been proposed in [6, 13, 35, 44], which analyze the advertisement
campaigns and predict the conversion rate directly. Actually, the objectives of the
click prediction and conversion prediction are often intertwined together, and there
is a need to study the two problems in conjunction with each other. With the notable
exception of [3], there is not much work analyzing the two objectives together to
understand the user purchase behavior better. Jointly studying the two problems can
help us understand the pathway leading to conversion and can provide answers to
several questions related to marketing. For instance, What ads should be shown to a
particular user so that he generates revenue? Given a limited budget, what ads are
more effective and for which users? and Will a given user generate revenue?
Motivated by the above observations, we aim to jointly study the click behavior
and the purchase activity of users, and to eventually predict their conversion
responses. In this chapter, we present a novel approach named dynamic collective
matrix factorization (DCMF) to address this problem. The DCMF approach is a
substantial extension of the collective matrix factorization (CMF) model used to
jointly factorize two or more matrices [20, 48, 49, 54]. Apart from considering the
two response matrices of click and purchase behavior together as in a CMF model,
our approach also takes into account the temporal inﬂuence of an advertisement
impression or click on conversion. More speciﬁcally, we model the time dependency
into the matrix factorization framework to account for the decay in the inﬂuence of
an advertisement. DCMF is able to take advantages of the temporal relationships
in response data. To the best of our knowledge, this work is the ﬁrst attempt to
introduce temporal information to CMF models.

8.2
Response Prediction
149
Moreover, as the click/conversion response matrices are extremely sparse,
DCMF further incorporates the side information from the user features, advertise-
ment features and the purchased item features in order to improve the robustness of
model. An efﬁcient optimization algorithm based on stochastic gradient descent is
proposed to solve our model, which is able to handle the large-scale datasets. Our
approach is well suited for an interactive setting where the user preferences and
behavior change over time. In addition, DCMF is a general approach for factorizing
multiple temporal response matrices, which can be potentially applied to other
domains as well, such as extracting features from multi-view streaming data.
This chapter is a substantial extension of our previous work [32]. Compared
to [32], we make the following extensions: (1) we add more technical details and
discussions for DCMF; (2) we add more experimental evaluations and applications;
(3) we provide a comprehensive review of related work.
The key contributions of this work can be summarized as follows:
•
We propose a dynamic collective matrix factorization (DCMF) approach for
conversion prediction. DCMF jointly factorizes the click response matrix and
the conversion response matrix, and predicts the missing values. In the display
advertising settings, DCMF could model the temporal relationships between the
click events and purchase events. It also incorporates the side information of
users, ads and items to address the data sparsity problem.
•
The proposed framework is based upon examining the data in time slices
to account for the decayed inﬂuence of an advertisement. In addition, an
optimization algorithm based on stochastic gradient descent is devised to solve
the model. This makes our framework well suited for interactive settings as well
as large datasets.
•
The conversion prediction performance of our approach is evaluated on a public
dataset and a real-world marketing dataset. Extensive experimental results show
that our model performs better as compared to several baseline methods.
8.2
Response Prediction
Response prediction in digital marketing has been widely studied in recent
years [9, 55, 56]. Most of the prior work focuses on predicting the click-through-rate
(CTR) of online ads or other contents [1, 12, 18, 19, 40, 43, 50, 53]. Richardson et
al. designed a model that used features of ads, terms, and advertisers to accurately
predict the CTR of new ads [43]. Agarwal et al. proposed a spatio-temporal model to
estimate the CTR for content recommendation. Graepel et al. presented a Bayesian
algorithm for CTR prediction, which achieved impressive performance in sponsored
search advertising [18]. Cheng et al. built a personalized click prediction model,
which made use of the user-related features [12]. Instead of considering each
advertisement independently, Xiong et al. considered the relationships between
ads, and proposed a continuous conditional random ﬁelds (CRF) method for click

150
8
Robust Representations for Response Prediction
prediction [52]. Yan et al. presented a coupled group Lasso approach for web-scale
CTR prediction [53]. Although these methods usually have good performance,
scalability is always a major concern in real-world response prediction systems.
To address the scalability problem, Chapelle et al. presented a simple and
scalable framework for modeling response prediction in display advertising. They
adopted the map-reduce architecture to improve the scalability of system [10].
Agarwal et al. introduced a scalable response prediction platform for online
advertising in [2]. Based on the standard logistic regression model, they integrated a
modiﬁed alternating direction method of multipliers (ADMM) for optimization, and
presented an explore-exploit serving scheme using Thompson sampling. In [21],
a co-factorization machine (CoFM) was designed for predicting user decisions in
Twitter. It follows the idea of factorization machine [42], a generic approach, to
handle large-scale data. However, these methods still require the full data set for
training, without considering the temporal effects. In our approach, we adopt an
online learning fashion to reduce the latency of system.
The ultimate goal of display advertising is conversion. However, there are only a
few works that investigate the conversion prediction problem [6, 13, 35, 44, 51].
Liu et al. designed a conversion optimization method by taking advantages of
the campaign-speciﬁc criteria and campaign metadata [35]. Lee et al. estimated
the conversion rate by using the past performance observations along with user,
publisher and advertiser data hierarchies [13]. Rosales et al. focused on estimating
the post-click conversion rate, considering the context of users and pages [44]. Bulut
et al. designed a latent topic model for conversion prediction in search advertising.
Unlike these conversion prediction methods, our approach take advantage of
the click response and side information via the collective matrix factorization
technique.
The most relevant method in the literature is the hierarchical multi-task learning
algorithm presented in [3]. It jointly models the conversion, click and unattributed-
conversion problems. There are signiﬁcant differences between [3] and our work.
First, we make use of the explicit temporal relationship between click and purchase
events, which is ignored in [3]. Second, unlike the multi-task learning model used
in [3], we develop the conversion prediction model based on matrix factorization.
8.2.1
Prediction Models with Temporal Dynamics
Collaborative ﬁltering is an effective predictive model [7, 28, 30, 31, 34, 57].
The response prediction methodology considering temporal dynamics was ﬁrst
developed using collaborative ﬁltering in [27]. In [26], Koenigstein et al. utilized
the temporal information for music recommendation. In [29], Li et al. modeled
the user-interest drift over time, and proposed a cross-domain recommendation
framework. In [45], a dynamic NMF approach with temporal regularization was
developed to learn the evolving and emerging topics in social media. In [14],

8.3
Preliminaries
151
a dynamic matrix factorization method was presented to model the temporal
adoption effects in collaborative ﬁltering. Cai et al. presented a dynamic context
matrix factorization algorithm for mining a network of coevolving time series [8].
Devooght. et al. introduced a novel dynamic matrix factorization framework for
collaborative ﬁltering, which allows to set an explicit prior on unknown values in
rating matrix [16]. Different from these methods, our work extends the collective
matrix factorization formulation, and focus on a new application that is user
behavior prediction in digital marketing.
The idea of using temporal dynamics has been explored in online advertising.
Barajas et al. proposed a time series approach for evaluating the effectiveness
of display advertising [4]. Most recently, Oentaryo et al. designed a hierarchical
importance-aware factorization machine (HIFM) for click response prediction in
mobile advertising [37]. It is able to handle the temporal ad response data. Unlike
HIFM, our approach aims to tackle the conversion prediction problem, and model
the temporal relationships between click and purchase events.
8.2.2
Prediction Models with Side Information
Incorporating side information usually improves the performance of prediction
models [41]. Porteous et al. proposed a Bayesian matrix factorization (BMF)
approach with side information and Dirichlet process mixtures [39]. A variational
BMF method and a hierarchical BMF method that utilize side information were
also proposed in [24] and [38], respectively. Hu et al. proposed a cross-domain
triadic factorization (CDTF) method [22] which leverages the information from
other domains.
In online advertising, Agarwal et al. presented a regression-based factor model
(RLFM) for response prediction in the presence of features. The key idea of
RLFM is to associate latent factors or proﬁles to users and items. Menon et al.
employed the collaborative ﬁltering technique for response prediction, by utilizing
the hierarchies and side-information of advertisements [36]. In our approach, we
also model the side information via regression. However, besides the different
application scenarios, a major difference between our work and existing methods is
that, we introduce side information to the collective matrix factorization technique,
and employ the temporal information in modeling user behaviors at the same time.
8.3
Preliminaries
In this section, we ﬁrst introduce the notations used in this chapter. We then formally
deﬁne the conversion prediction problem, and the dynamic collective conversion
prediction problem.

152
8
Robust Representations for Response Prediction
Table 8.1 Summary of
notations
Notation
Description
C
Binary click response matrix
D
Binary purchase activity matrix
U
Latent features of users
V
Latent features of advertisements
P
Latent features of items
M
Transition matrix
X
Side information of users
Y
Side information of advertisements
Z
Side information of items
OU
Regression coefﬁcients of users
OV
Regression coefﬁcients of advertisements
OP
Regression coefﬁcients of items
Nu
Number of users
Na
Number of advertisements
Np
Number of items
r
Dimension of latent factors
k  kF
Frobenius norm
8.3.1
Notations
We mainly consider two response matrices, i.e., click response matrix C and
purchase activity matrix D. We also deﬁne latent factors, side information matrices,
and coefﬁcient matrices. The notations summarized in Table 8.1 will be used
throughout this chapter.
8.3.2
Problem Deﬁnition
Generally, conversion in display advertising can be divided into post-click conver-
sion and post-impression conversion. The former counts the purchase activities after
a user clicks on advertisements, and the latter counts the purchase activities of the
users after the user has seen an ad but has not clicked on it. In this chapter, we aim
to predict both types of conversions and thus look at all the purchase activities of
the user.
Let C 2 RNuNa denote a binary click response matrix, indicating the click
response of Nu users for Na advertisements. We only consider the advertisements
that directly drive conversions. Each advertisement is mapped to a single item. Let
D 2 RNuNp denote a binary response matrix, indicating the purchase decisions of
Nu users for Np items. We deﬁne the traditional conversion prediction problem as
follows.

8.4
Dynamic Collective Matrix Factorization (DCMF) with Side Information
153
Deﬁnition 8.1 (Conversion Prediction)
Given some observed entries in D, pre-
dict the missing values in D.
As we noted earlier, the click response information is ignored in traditional
conversion prediction methods. The click response is important in conversion as
it captures information about a user’s possible interest in the product that could lead
to conversions. The relationship is even reﬂected in our dataset and the probability
of a user converting after if he clicks on an ad is much higher than if he does not.
Further, we observe that most of the post-click conversions happen within 2 weeks
suggesting that there is a temporal “halo” effect of the inﬂuence of the ad. This leads
us to deﬁne the following dynamic collective conversion response problem.
Deﬁnition 8.2 (Dynamic
Collective
Conversion
Prediction)
Given
some
observed values in C and D along with the temporal information, predict the
missing values in D.
8.4
Dynamic Collective Matrix Factorization (DCMF) with
Side Information
Our approach for the dynamic collective conversion problem is based upon col-
lective matrix factorization (CMF) [48]. In this section, we ﬁrst introduce how to
apply CMF to the conversion prediction problem, which is also a baseline in our
experiments. After that, we describe how to incorporate the temporal dynamics and
side information into the CMF model step by step, and ﬁnally present the objective
function our dynamic CMF (DCMF) approach. In the next section, we will devise
an efﬁcient optimization algorithm based on the stochastic gradient descent (SGD)
to solve the problem.
8.4.1
CMF for Conversion Prediction
We propose to deal with the conversion prediction problem using the collaborative
ﬁltering (CF) technique. The fundamental intuition behind CF is that the “similar”
users will have “similar” preferences. As the most successful CF technique, matrix
factorization based methods can learn “similar” latent features for “similar” users by
decomposing the User
Item rating matrix. There has been work on applying the CF
technique to conversion prediction [36], but it models the interactions between pages
and advertisements. In our chapter, we directly model the relationships between
users and ads/items, which enables us to predict the user behaviors. The intuition
is that “similar” users are very likely to click “similar” ads and ﬁnally purchase
“similar” items.
As described in Deﬁnition 8.2, we want to jointly analyze the relational data
that are the click response and purchase activities of users. Collective matrix

154
8
Robust Representations for Response Prediction
factorization (CMF) is an effective method for predicting missing values in rela-
tional data where there is more than one relation matrix present, and it has been
successfully applied to CF [48, 49]. The basic idea of CMF is to jointly factorize
one-side related matrices, such as the User
Movie matrix and Movie
Genre matrix
in movie recommendation. Inspired by the CMF method, we can also devise a
similar mechanism to factorize the click response matrix (User
Advertisement)
and the purchase activity matrix (User
Item) simultaneously, and then predict the
responses of users.
Given a click response matrix C 2 RNuNa and a purchase activity matrix D 2
RNuNp, the entries in C and D are deﬁned as follows:
Cij D
8
<
:
1;
if user i viewed and clicked ad jI
0;
if user i viewed but not clicked ad jI
‹;
Missing values (ad j hasn’t shown to user i/:
(8.1)
Dij D
8
<
:
1;
if user i purchased item jI
0;
if user i viewed but not purchased item jI
‹;
Missing values.
(8.2)
C and D are binary and incomplete matrices. We assume that they can be
approximated by the product of latent factors U 2 RNur, V 2 RNar and P 2 RNpr:
C  UVT; D  UPT;
(8.3)
where Nu is the number of users, Na is the number of advertisements, Np is the
number of items, and r is the dimension of latent features.
In (8.3), C and D share the common latent features of users, U. Thus, the
objective function of CMF is:
arg min
U;V;P f.U; V; P/ D ˛kWC ˇ .C  UVT/k2
F
C .1  ˛/kWD ˇ .D  UPT/k2
F
C .kUk2
F C kVk2
F C kPk2
F/;
(8.4)
where WC is an indictor matrices with WC
ij D 1 if Cij is a valid value and 0 otherwise;
WD is an indictor matrices with WD
ik D 1 if Dij is a valid value and 0 otherwise;
ˇ denotes the Hadamard product (i.e., entry-wise product); ˛ and  are trade-off
parameters.
The ﬁrst two terms in (8.4) denote the approximation errors, and the last three
terms are regularizations used to prevent overﬁtting. Other improved versions of
CMF have been proposed in [5, 17, 20, 23], by introducing additional constraints to
(8.4).

8.4
Dynamic Collective Matrix Factorization (DCMF) with Side Information
155
With the learned latent features U and P, we can predict the conversion score of
user m for item n as:
Score.m; n/ D umpT
n;
(8.5)
where um is the m-th row of U, and pn is the p-th row of P.
8.4.2
Modeling Temporal Dynamics
Temporal information is critical in conversion prediction and in attributing the
conversions to the ad clicks. A key observation is that, the behavior of users may
change over time. For example, if a user has already purchased an item in the
previous week, it is unlikely that he/she will purchase the same item again in the next
week. Also, the inﬂuence of an ad impression or a click may only last for a short
span of time. Prior research has shown the importance of incorporating temporal
information in ad attribution [10]. Further, in real-world systems such as online
advertising, it is important to adapt to the changing scenarios so as to have better
prediction and reduced latency. We propose to incorporate temporal information to
the CMF model. Figure 8.1 illustrates the idea of our approach.
Given T pre-deﬁned time slices t 2 f1; 2;    ; Tg, we use Ct and Dt to denote
the click responses and purchase activities in the time slice t, respectively. Since
the temporal dynamics are ignored in standard CMF, it can only utilize all the
previous data (i.e., fC1;    ; Ctg and fD1;    ; Dtg) for training, and then predict
DtC1. As a result, the size of training data will increase signiﬁcantly over time,
which leads to a heavy computational burden. Moreover, as the user behavior may
Click 
Response
Users
Ads
Conversion
Response
Items
Vt-1
−
Pt-1
−
Click 
Response
Ct
Users
Ads
Conversion
Response
Items
Vt
Pt
Share
Share
Conversion
Response
Training
Prediction
Timeline
(t-1)
(t)
(t+1)
Ad
Feature
Item
Feature
User
Feature
Ad
Feature
Item
Feature
User
Feature
Fig. 8.1 Framework of the proposed DCMF approach. It jointly factorizes the click response and
conversion response matrices in time .t/ to predict the conversions in time t C 1. Different from
existing work, our approach considers the temporal dynamics of user latent features, and also takes
advantages of the side information of users, ads, and items

156
8
Robust Representations for Response Prediction
change signiﬁcantly over a long period of time, the old data are very likely to bring
negative impact to the predictive model. Therefore, it is unnecessary to utilize all
the training data from previous time slices. By exploiting the temporal relationships
between click response and purchase events, we notice that the purchase events in
time t C 1 are mainly related to the click events in time t and hence our model
needs to account for that. (See Sect. 8.4.4 for more details on choosing a proper
time window t.)
Considering such temporal relationships, we introduce a temporal variable t, and
rewrite the objective function (8.4) as:
arg min
Ut;Vt;Pt f.Ut; Vt; Pt/ D ˛kWC ˇ .Ct  UtVtT/k2
F
C .1  ˛/kWD ˇ .Dt  UtPtT/k2
F
C .kUtk2
F C kVtk2
F C kPtk2
F/:
(8.6)
Furthermore, we need to ensure that the latent features of users do not dramat-
ically change in a short period of time, as in reality the user preferences would
evolve smoothly. To address this concern, we leverage the latent features of the
users learned in time t  1 (i.e., Ut1) at time t .t > 1/. Speciﬁcally, we add the
following term in (8.6) to account for the drift in user preferences:
Ut  Ut1M;
(8.7)
where Ut1 is the latent features of users learned from the previous time slice t  1.
In (8.7), we assume that the latent features in time t are closely related to the feature
in time t  1, which is reasonable in real applications. M is a transition matrix of
users’ behavior, which tries to capture the mappings between users’ behavior in
two successive time slices. The intuition is that users’ intention on purchasing items
should be smoothly transited over time. Then, the objective function in (8.6) can be
rewritten as:
arg
min
Ut;Vt;Pt;M f.Ut; Vt; Pt; M/ D ˛kWC ˇ .Ct  UtVtT/k2
F
C .1  ˛/kWD ˇ .Dt  UtPtT/k2
F
C 1kUt  Ut1Mk2
F
C 2.kUtk2
F C kVtk2
F C kPtk2
F C kMk2
F/;
(8.8)
where the latent features Ut1 is given, 1 and 2 are trade-off parameters. The last
regularization term kMk2
F is used to control the complexity of model.

8.4
Dynamic Collective Matrix Factorization (DCMF) with Side Information
157
8.4.3
Modeling Side Information
So far, we have seen that the model in (8.8) is not aware of side information, i.e., the
features of users, ads, and items. We can further exploit the additional information
to improve the prediction performance. The side information is also particularly
useful as the data in conversion and click prediction problems are generally sparse.
For example, we do not have any click responses or conversion responses of some
new users, which lead to the cold-start problem. In this case, the latent features of
new users estimated by (8.8) are not reliable anymore. However, side information
provide useful cues from another perspective, and make it possible to learn robust
latent features in the cold-start scenario. In this section, we incorporate the side
information into (8.8), and present the DCMF method.
Let X, Y and Z denote the feature matrices for users, ads and items, respectively.
We assume that the click response and purchase activity are generated by the inner
product of latent factors, and the side information via linear regression. Thus, we
reformulate the matrix approximation equations in (8.3) as:
Ct  UtVtT C OUtYT C X OVtT;
Dt  UtPtT C OUtZT C X OPtT;
(8.9)
where OUt, OVt and OPt are regression coefﬁcients on user features, ad features and
item features, respectively. We treat the three terms used to approximate Ct (or
Dt) equally for simplicity. The performance can be enhanced by assigning different
weights for them.
By replacing the matrix approximations in (8.8) with (8.9), we can then rewrite
the objective function as:
arg
min
Ut;Vt;Pt;M;
OUt; OVt OPt
˛kWC ˇ .Ct  UtVtT  OUtYT  X OVtT/k2
F
C .1  ˛/kWD ˇ .Dt  UtPtT  OUtZT  X OPtT/k2
F
C 1kUt  Ut1Mk2
F
C 2.kUtk2
F C kVtk2
F C kPtk2
F C kMk2
F/:
(8.10)
8.4.4
Discussions
Size of Time Window. Choosing a proper time window t is crucial in temporal
models and the window size may vary for different datasets. We explored a

158
8
Robust Representations for Response Prediction
6
10
15
20
25
30
0
5
10
15
20
25
30
35
Day of Month
Number of Conversions
Fig. 8.2 The distribution of post-click conversions after clicking ads on Oct. 6, 2013
real-world advertisement campaign dataset,2 and found that a large majority of
conversion events (about 90%) are triggered within 1 day of the click events in our
dataset. This phenomena is consistent to the one described in [10]. Moreover, we
notice that most of the click events that lead to conversion (over 98%) are converted
in the following 2 weeks. Figure 8.2 shows the distribution of post-click conversion
(after Oct. 6) for all the click events on Oct. 6, 2013, which validates our observation.
In our model, we assume that the purchase events in time t C 1 are mainly related
to the click events in time t. Thus, 1 week could be a good choice for the time
window t.
Post-click Conversion and Post-impression Conversion. Clicks and conversions
are both rare events in the marketing data, compared with the huge number of ad
impressions. Generally, the conversions include post-click conversions and post-
impression conversions. The former one counts the conversions right after clicking
the corresponding ad, while the latter one counts the conversions without clicking
the viewed ads. The conversion response data contain both types of conversions, but
existing conversion prediction works usually treat them equally. In reality, different
users exhibit different conversion behaviors, and it is necessary to discover the
intrinsic factors that drive these two types of conversions. Although ad clicks do not
always lead to conversions, clicking an ad is often identiﬁed as positive intention. In
some cases, a user may click one ad multiple times in a short period, which implies
strong intentions. By drawing strengths from the click response data, our approach
has the potential to learn personalized latent features for each user, and generates
accurate predictions.
2The details of this dataset will be presented in Sect. 8.6.2.

8.5
Optimization
159
Advertisements with Different Purposes. Some ads are used for lifting the
brands, by delivering some general messages. It is difﬁcult to directly link them
to the conversions of speciﬁc items, and users rarely click those ads. However, these
ads may potentially change the user behaviors. Thus, the negative click responses
for these ads could not faithfully reﬂect the conversion behaviors of users. In this
chapter, we ﬁlter out the brand-lifting ads to avoid misleading results, and only
employ the ads that directly drive conversions.
8.5
Optimization
In this section, we design an optimization algorithm to solve the objective function
of DCMF in (8.10). Although (8.10) is not jointly convex for all the variables, it
is convex with respect to each variable. As the stochastic gradient descent (SGD)
algorithm is efﬁcient and can be easily parallelized, we adopt SGD to solve (8.10).
8.5.1
Algorithm
First, we ﬁx M, and update other variables, Ut D fut
1;    ; ut
rg, Vt D fvt
1;    ; vt
rg,
Pt D fpt
1;    ; pt
rg, OUt D fOut
1;    ; Out
rg, OVt D fOvt
1;    ; Ovt
rg, and OPt D fOpt
1;    ; Opt
rg.
After selecting a pair of random training points Ct
ij and Dt
ik, we only need to update
ut
i, vt
j, pt
k, Out
i, Ovt
j and Opt
k using:
ut
i D ut
i   @
@ut
i
f.Ut; Vt; Pt; OUt; OVt; OPt; M/;
(8.11)
vt
j D vt
j   @
@vt
j
f.Ut; Vt; Pt; OUt; OVt; OPt; M/;
(8.12)
pt
k D pt
k   @
@pt
k
f.Ut; Vt; Pt; OUt; OVt; OPt; M/;
(8.13)
Out
i D Out
i   @
@Out
i
f.Ut; Vt; Pt; OUt; OVt; OPt; M/;
(8.14)
Ovt
j D Ovt
j   @
@Ovt
j
f.Ut; Vt; Pt; OUt; OVt; OPt; M/;
(8.15)
Opt
k D Opt
k   @
@Opt
k
f.Ut; Vt; Pt; OUt; OVt; OPt; M/;
(8.16)
where  is the learning rate.

160
8
Robust Representations for Response Prediction
The detailed gradients for each variable are provided below. Given that X D
fx1; x2;    ; xNug, Y D fy1; y2;    ; yNag and Z D fz1; z2;    ; zNpg, we obtain the
gradients of variables ut
i, vt
i, pt
i, Out
i, Ovt
i, Opt
i as follows:
@f
@ut
i
D  ˛
X
.i;j/2O
.ct
i;j  ut
ivtT
j  Out
iyT
j  xi OvtT
j /vt
j
 .1  ˛/
X
.i;k/2O
.dt
i;k  ut
iptT
k  Out
izT
k  xiOptT
k /pt
k
C 1.ut
i  ut1
i
M/ C 2ut
i;
(8.17)
@f
@vt
j
D  ˛
X
.i;j/2O
.ct
i;j  ut
ivtT
j  Out
iyT
j  xi OvtT
j /ut
i C 2vt
j;
(8.18)
@f
@pt
k
D  .1  ˛/
X
.i;k/2O
.dt
i;k  ut
iptT
k  Out
izT
k  xiOptT
k /ut
i
C 2pt
k;
(8.19)
@f
@Out
i
D  ˛
X
.i;j/2O
.ct
i;j  ut
ivtT
j  Out
iyT
j  xi OvtT
j /yj
 .1  ˛/
X
.i;k/2O
.dt
i;k  ut
iptT
k  Out
izT
k  xiOptT
k /yk
C 2Out
i;
(8.20)
@f
@Ovt
j
D  ˛
X
.i;j/2O
.ct
i;j  ut
ivtT
j  Out
iyT
j  xi OvtT
j /xi C 2 Ovt
j;
(8.21)
@f
@Opt
k
D  .1  ˛/
X
.i;k/2O
.dt
i;k  ut
iptT
k  Out
izT
k  xiOptT
k /xi
C 2Opt
k:
(8.22)
Next, we ﬁx all the other variables, and update M. By ignoring all the irrelevant
terms with respect to M, the objective (8.10) reduces to:
arg min
M f.M/ D 1kUt  Ut1Mk2
F C 2kMk2
F:
(8.23)
We can then update M using:
M D M   @
@M f.M/:
(8.24)

8.5
Optimization
161
Algorithm 8.1 DCMF Approach
Input: click response Ct, purchase activity Dt, user features X,
advertisement features Y, item features Z, latent features Ut1.
Initialization:  D 0:003, ˛ D 0:6, 1 D 0:001, 2 D 0:02, M D I.
Output: latent features Ut, Vt, Pt, OUt, OVt and OPt.
1: while not converged do
2:
Select a pair of training points ct
ij 2 Ct and dt
ik 2 Dt uniformly at random.
3:
Update latent vector ui using (8.11).
4:
Update latent vector vj using (8.12).
5:
Update latent vector pk using (8.13).
6:
Update regression coefﬁcients Oui using (8.14).
7:
Update regression coefﬁcients Ovj using (8.15).
8:
Update regression coefﬁcients Opk using (8.16).
9:
Update transition matrix M using (8.24).
10: end while
The gradient of variable M is:
@f
@M D  1U.t1/T.Ut  Ut1M/ C 2M:
(8.25)
The above process is repeated until convergence. The detailed procedures
of optimizing objective (8.10) are summarized in Algorithm 8.1. Note that the
initialized values of parameters should be adjusted for different datasets.
8.5.2
Discussions
Convergence. It’s well known that stochastic gradient descent (SGD) algorithms
usually enjoy a very good convergence property. Algorithm 8.1 is a variant of
the standard SGD algorithm that is widely used in matrix factorization. In our
experiments, we will empirically show the convergence property of our algorithm.
Time Complexity. In Algorithm 8.1, the most time-consuming parts are evaluating
the objective function f and its gradients against variables. More speciﬁcally, the
computational cost of evaluating the objective f is O.rNC C rND/, where NC and
ND are the numbers of valid entries in response matrices Ct and Dt, respectively.
The computational costs for calculating gradients in (8.11), (8.12) and (8.13) are
O.rNC CrND/, O.rNC/, and O.rND/, respectively. Thus, in one iteration, the overall
computational complexity is O.rNCCrND/, indicating that our approach has a linear
time complexity with respect to the number of observations in the sparse matrices
Ct and Dt.

162
8
Robust Representations for Response Prediction
8.6
Experiments
The goal of our experimental evaluation is three folds – At ﬁrst we want to analyze
the effectiveness of our approach as compared to the state of the art methods. Next,
we want to check the sensitivity of our approach to the various parameter settings
and ﬁnally we want to check how the approach can be used to ﬁnd out ads that are
effective.
8.6.1
Experiments on Public Data
We conduct experiments on one public dataset and one proprietary dataset. As we
are unable to share the proprietary data outside Adobe, we employ the public data
for evaluation, helping readers get insights into the working of our approach.
Data. We choose a public benchmark dataset, Movielens-1M,3 which is widely
used to evaluate matrix factorization methods in collaborative ﬁltering. The
Movielens-1M dataset contains one million ratings with 6040 users and 3952
movies. It provides side information for both users and items. In particular, the user
information include the user’s age, gender and occupation. We encode them into a
binary valued vector with length 28. The item information include 18 category of
movie genre, which was encoded into a binary valued vector with length 18. The size
of rating matrix R is 6040  3952. To testify the performance on jointly factorizing
multiple matrices, we divide the matrix R into two parts, Ra D R16040;11900 and
Rb D R16040;19013952. In this way, two rating matrices have the same set of users,
but different items.
Evaluation Metrics. We utilize two popular evaluation metrics that are root mean
squared error (RMSE) and mean absolute value (MAE). The RMSE is deﬁned as:
RMSE D
v
u
u
t 1
N
N
X
iD1
.xi  Oxi/2;
(8.26)
and MAE is deﬁned as:
MAE D 1
N
N
X
iD1
jxi  Oxij;
(8.27)
where N is the size of test data, xi and Oxi are the ground truth and the predicted value
for the i-th test data, respectively.
3http://grouplens.org/datasets/movielens/

8.6
Experiments
163
Baselines. Although our approach focuses on response prediction, we cannot
directly compare it with many traditional click or conversion prediction methods
such as [43], as our approach actually predicts user-speciﬁc response. Our approach
belongs to the matrix factorization technique, and therefore we mainly compare
with the representative matrix factorization algorithms. The baselines are listed as
follows.
•
Probabilistic matrix factorization (PMF) [46]. It is a standard baseline for
collaborative ﬁltering, but it can be adapted to the response prediction problem.
As suggested in [46], we use the SGD algorithm for PMF, which is equivalent to
its maximum a posteriori (MAP) solution.
•
LIBMF [58]. It is an implementation of matrix factorization using the fast
parallel SGD algorithm. In addition to the latent user and item features, it also
considers the user bias, item bias for better performance.
•
SVDFeature [11]. SVDFeature is a representative matrix factorization method
that utilizes the auxiliary information. It could incorporate the side information
of users and items into the prediction model.
•
Hierarchical Bayesian matrix factorization with side information (HBMFSI) [38].
It is an improved version of Bayesian matrix factorization [47]. The side
information is utilized to regularize the user and item factor matrices.
•
Collective matrix factorization (CMF) [48]. The CMF model has been described
earlier. It jointly factorization two matrices that share one-side information.
Performance. We aim to predict the ratings in Rb. The single matrix factorization
methods (i.e., PMF, LIBMF, SVDFeature and HBMFSI) are trained only using the
rating matrix Rb. CMF and our methods are trained using both Ra and Rb. The ratings
in Rb are split into training set and test set according to the temporal information.
Ten training/test sets are generated in total. Table 8.2 shows the average RMSE
and MAE with standard deviations. Two collective factorization methods, CMF and
ours, outperforms PMF, LIBMF and SVDFeature. By taking advantages of the side
information, HBMFSI and our DCMF approach achieve much lower RMSE and
MAE than all the other methods, implying that side information are very important
especially when the data are sparse.
Table 8.2 RMSE and MAE
on Movielens-1M data. The
number in parenthesis
represents the standard
deviation. Numbers in
boldface are the best results
Method
RMSE
MAE
PMF [46]
0.9168 (0.0019)
0.7197 (0.0013)
LIBMF [58]
0.9161 (0.0017)
0.7185 (0.0014)
SVDFeature [11]
0.9152 (0.0021)
0.7163 (0.0017)
HBMFSI [38]
0.9089 (0.0025)
0.7146 (0.0016)
CMF [48]
0.9145 (0.0017)
0.7156 (0.0012)
DCMF (Ours)
0.9018 (0.0027)
0.7082 (0.0019)

164
8
Robust Representations for Response Prediction
Table 8.3 Count statistics of
click responses and purchase
activities in 2 months
Entity
Oct. 2013
Nov. 2013
#records
2,170,006
2,635,011
#users
1,562,980
1,909,902
#countries
236
236
#(web) pages
733
728
#items
913
875
#item-types
22
23
8.6.2
Conversion Prediction: Settings
Data. We collected an advertisement campaign dataset at a software company. The
advertising data comprises of ads shown on different websites as a part of different
marketing campaigns. We have behavioral data for the users which includes the ad
impressions shown to the users, the click responses for advertisements and the pur-
chase activities of the items. For the purpose of our study, we examine a subset of the
data comprising of three million users for a 2 month period starting from October 1,
2013 to November 30, 2013. The dataset consists of 1122 ads. For each click event,
we have the user ID, timestamp, ad ID, page ID, site ID, country, browser, campaign
ID, and size of ad. Each record can be uniquely identiﬁed by a (user ID, ad ID,
timestamp) triplet. Along with this, we also have information about the user proﬁles.
For each user, we have information such as country, domain, etc. Apart from the user
information, our system also captures the meta-information of the ads and items
such as ad size, ad placement, item-type and price. The purchase data consists item-
ID, item-type, price and quantity. Table 8.3 shows the count statistics of the purchase
activity table in 2 months. One key observation is that the table is extremely sparse
which makes it very difﬁcult to predict the purchase behavior of users.
To evaluate the performance on conversion prediction, we examine a subset of the
dataset, which constitutes of behavioral characteristics of 448,158 number of users
and 737 ads. We ﬁlter out the ads used for brand-lifting. Along with the impression
records, we also have click and purchase activity information for all the users. As
explained in Sect. 8.4.4, we choose 1 week as the time window t in our model.
To construct the binary response tables, we denote the click events and purchase
events as positive responses, and the impressions (without any following events) as
negative responses. All the other entries are treated as missing values. As the click
and purchase are rare events in reality, our data set is extremely sparse.
To collect the side information, we select some features of users, ads and items,
respectively. For each user, we encode the demographic information (e.g., country,
state, domain) into a binary valued vector. The attributes of ads (e.g., advertiser, ad
size) and items (e.g., type, price) are also encoded into binary vectors, respectively.
To conduct fair comparisons, we set up 6 different training/test cases, as shown
in Table 8.4. Each training set consists of a click events table and a purchase events
table. Each test set only contains a table of purchase events, as our goal is to predict
the conversions.

8.6
Experiments
165
Table 8.4 Training/test
settings for predicting
purchase events in the
experiments
Evaluation
Training period
Test period
Case-1
Oct. 8Oct. 14
Oct. 15Oct. 21
Case-2
Oct. 15Oct. 21
Oct. 22Oct. 28
Case-3
Oct. 22Oct. 28
Oct. 29Nov. 4
Case-4
Oct. 29Nov. 4
Nov. 5Nov. 11
Case-5
Nov. 5Nov. 11
Nov. 12Oct. 18
Case-6
Nov. 12Nov. 18
Nov. 19Oct. 25
Table 8.5 Summarization of
baselines and our approaches
Joint
Side
Temporal
Method
factorization
information
dynamics
PMF [46]
No
No
No
LIBMF [58]
No
No
No
SVDFeature [11]
No
Yes
No
HBMFSI [38]
No
Yes
No
CMF [48]
Yes
No
No
DCMF (Ours)
Yes
Yes
Yes
Baselines. As our approach belongs to the matrix factorization technique, we
mainly compare our approach with the popular matrix factorization methods,
including PMF, LIBMF, SVDFeature, HBMFSI, and CMF. For PMF, LIBMF,
SVDFeature and HBMFSI, we only use the purchase data for training, due to the
intrinsic limitation of these methods. For CMF and our approach, we use both the
click data and purchase data for training. In particular, we use the click events
and purchase events at time t to predict the purchase events in time t C 1 (i.e.,
DtC1). Table 8.5 summarizes the differences between the different baseline methods
and our approach. The DCMF approach is the only one that considers both side
information and temporal dynamics.
Evaluation Metrics. Following [3], we use the ROC curve and the area under
ROC (AUC-ROC) as our evaluation metrics. The true positive rate (TPR) and false
positive rate (FPR) used for generating ROC curves are deﬁned as follows:
TPR D
TP
TP C FN ;
FPR D
FP
FP C TN ;
(8.28)
where TP represents the true positives, FN represents the false negatives, TN
represents the true negatives, and FP represents the false positives.
In addition, Davis et al. showed that the precision-recall (PR) curves give a
more informative picture of the performance of algorithms, when dealing with the
highly skewed datasets [15]. As the clicks and purchase activities are rare events,
the number of negative responses is much higher than that of positive ones, which
makes our dataset highly imbalanced. Therefore, we also adopt the PR curves and

166
8
Robust Representations for Response Prediction
the AUC-PR for evaluation. The deﬁnitions of recall and precision are:
Recall D
TP
TP C FN ;
Precision D
TP
TP C FP:
(8.29)
Parameter Settings. For SGD based matrix factorization methods (e.g., PMF,
LIBMF, SVDFeature and CMF), the major parameters are the learning rate  and
the trade-off parameter  for regularization terms. For Bayesian method HBMFSI,
we follow the settings in [38]. We sample a validation set from the training data,
and tune these parameters empirically. The parameters , 1 and 2 are set to 0.003,
0.001 and 0.02, respectively. In CMF and our approach, another important parameter
is ˛. In the following experiments, ˛ is set to 0.6, and we will show its sensitivity in
the next section. In addition, the dimension of latent features (i.e., r) is empirically
set to 20 for each method. To initialize our approach in Case-1, we borrow the latent
factors of CMF learned from Oct. 1
Oct. 7 as the input Ut1.
8.6.3
Conversion Prediction: Results and Discussions
We evaluate the performance of each compared method in six different settings
shown in Table 8.4. Figure 8.3 shows the ROC in Case-1, and Table 8.6 lists
the AUC-ROC of all cases. Form Fig. 8.3 and Table 8.6, we make the following
observations.
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
False Positive Rate (FPR)
True Positive Rate (TPR)
PMF [17]
LIBMF [18]
SVDFeature [19]
HBMFSI [20]
CMF [11]
DCMF (Ours)
Fig. 8.3 ROC curves of all compared methods in Case-1

8.6
Experiments
167
Table 8.6 AUC-ROC of compared methods on marketing data
Method
Case 1
Case 2
Case 3
Case 4
Case 5
Case 6
Average ˙ Std
PMF [46]
0.7286
0.7180
0.6451
0.7119
0.7143
0.7380
0:7093 ˙ 0:0329
LIBMF [58]
0.7305
0.7197
0.6532
0.7255
0.7229
0.7430
0:7158 ˙ 0:0317
SVDFeature [11]
0.7425
0.7286
0.6605
0.7308
0.7314
0.7511
0:7241 ˙ 0:0323
HBMFSI [38]
0.7498
0.7371
0.6689
0.7397
0.7412
0.7624
0:7330 ˙ 0:0327
CMF [48]
0.7511
0.7464
0.6678
0.7356
0.7369
0.7618
0:7333 ˙ 0:0335
DCMF (Ours)
0.8504
0.8514
0.8411
0.8312
0.8372
0.8617
0.8455 ˙ 0.0111
0
0.2
0.4
0.6
0.8
1
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Precision
PMF [17]
LIBMF [18]
SVDFeature [19]
HBMFSI [20]
CMF [11]
DCMF (Ours)
Fig. 8.4 PR curves of all compared methods in Case-2
•
Incorporating side information improves the prediction results signiﬁcantly.
SVDFeature and HBMFSI employ the features of users and items, they achieve
much better performance than PMF and LIBMF, which do not utilize any side
information. Similarly, our DCMF approach performs very well by virtue of the
side information.
•
Modeling click response is helpful in predicting purchase events, since CMF and
our approach outperforms PMF and LIBMF in each case.
•
Temporal dynamics is critical in conversion prediction. Our approach outperform
all the other competitors, as they leverage the temporal relationships between
click and purchase events. In particular, our DCMF approach obtains better
results than CMF, indicating that the latent features learned from previous time
slice are useful.
•
DCMF achieves the best results in each case, which demonstrates that side
information and temporal dynamic could be complementary to each other.
Figure 8.4 shows the PR curves in Case-2, and the AUC-PR of all cases are
summarized in Table 8.7. We can observe that, the effectiveness of side information
and temporal dynamic is still valid in the PR metric. Our DCMF approach

168
8
Robust Representations for Response Prediction
Table 8.7 AUC-PR of compared methods on marketing data
Method
Case 1
Case 2
Case 3
Case 4
Case 5
Case 6
Average˙Std
PMF [46]
0.2971
0.2762
0.2784
0.3064
0.3110
0.3374
0:3011 ˙ 0:0228
LIBMF [58]
0.3022
0.3018
0.2877
0.3165
0.3245
0.3490
0:3166 ˙ 0:0234
SVDFeature [11]
0.3134
0.3199
0.2901
0.3209
0.3310
0.3522
0:3182 ˙ 0:0220
HBMFSI [38]
0.3358
0.3542
0.3012
0.3358
0.3451
0.3603
0:3312 ˙ 0:0223
CMF [48]
0.3245
0.3325
0.2932
0.3290
0.3318
0.3582
0:3231 ˙ 0:0232
DCMF (Ours)
0.6035
0.6174
0.6363
0.6113
0.6185
0.6366
0.6206 ˙ 0.0134
Table 8.8 Precisions of each
method in Case-3, when
Recall D 0:4
Method
Precision
PMF [46]
0.2271
LIBMF [58]
0.2305
SVDFeature [11]
0.2387
HBMFSI [38]
0.2402
CMF [48]
0.2398
DCMF (Ours)
0.8762
outperform the other compared methods. To take a close look at the results, we
show the precision of each method in Table 8.8, when recall is equal to 0.4. It shows
that, with a reasonable recall rate, our DCMF approach can improve the precision
by 63.60%, compared to the best competitor HBMFSI in this case.
8.6.4
Effectiveness Measurement of Ads
In addition to evaluating the overall performance of conversion prediction, it is also
interesting in digital marketing to understand the effectiveness of different ads [33].
It is very useful to answer Can we accurately predict which ads are more likely
to be converted than others? To answer this, we randomly choose 50 ads from the
data, and study the comparison between the actual conversions and the predicted
conversions. As shown in Fig. 8.5, we see that our approach can accurately predict
the conversions of each ad in most cases. For example, the predicted number of
conversions is quite close to the actual number for Ad#37. This result assumes larger
signiﬁcance and can be very helpful in making advertising strategies and behavioral
targeting.
8.6.5
Discussions
Effect of Temporal Dynamics. As described before, one beneﬁt of our approach is
that the system latency can be reduced because of considering temporal dynamics.

8.6
Experiments
169
0
10
20
30
40
50
0
200
400
600
800
1000
1200
1400
Ad ID
Number of Conversions
Actual Conversions 
Predicted Conversions of DCMF
Fig. 8.5 Actual conversions and predicted conversions of 50 randomly selected ads during the
period Nov. 19Nov. 25, 2013
CMF
CMF−2
DCMF
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
AUC−PR
CMF
CMF−2
DCMF
0
200
400
600
800
Training Time (Seconds)
Fig. 8.6 AUC-PR and training time (seconds) of CMF, CMF-2 and our DCMF approach when
predicting conversions in Case-5
Traditionally, a large amount of data are utilized to train the system, which
introduces heavy computational cost. We add another baseline CMF-2, which
uses all the previous click events (i.e., fC1;    ; CtC1g) and purchase events (i.e.,
fD1;    ; Dtg) to predict DtC1. Figure 8.6 shows the AUC-PR and training time of
CMF-2 and our approach, when predicting the purchase events in Case-5. As before,
our approach only use Ct and Dt for training. It shows that the performance of CMF-
2 and CMF are quite close, indicating that enlarging the data size would not be
helpful. However, the training time of CMF-2 is much more than that of our DCMF
approach and would not be very useful in an interactive setting in real systems. Thus,
the effectiveness of incorporating temporal information in the conversion prediction
problem has been extensively demonstrated.

170
8
Robust Representations for Response Prediction
0
10
20
30
40
50
0.314
0.316
0.318
0.32
0.322
0.324
0.326
0.328
0.33
Iterations
Loss
DCMF
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0.4
0.5
0.6
0.7
0.8
0.9
α
AUC−ROC
CMF
DCMF
(a) Convergence curve
(b) AUC-ROC of CMF and DCMF
Fig. 8.7 (a) Convergence curve of DCMF on marketing data; (b) AUC-ROC of CMF and DCMF
in Case-3 with different parameter ˛
Convergence. Figure 8.7a shows the convergence curve of our approach on the
marketing dataset. We can observe that our approach enjoys a good converge
property and requires only 30 iterations in this case.
Parameter Sensitivity. One major parameter in our approach is ˛. To understand
its sensitivity, we observe the AUC-ROC of CMF and DCMF approach with various
choices of ˛. Figure 8.7b shows the AUC-ROC values in Case-3. We can observe
that our approach is not sensitive to the values of ˛. We achieve better performance
when ˛ falls into the range Œ0:5; 0:8. In our experiments, we set ˛ to 0.6.
8.7
Summary
In this chapter, we presented a novel matrix factorization approach DCMF for
conversion prediction in display advertising. DCMF jointly examines the click
events and purchase events in an online learning fashion by leveraging the temporal
information. It also takes advantage of the side information via regression. Exten-
sive experimental results on a public dataset and a real-world marketing dataset
demonstrate the superiority of our approach over existing methods. Moreover, our
approach can be applied to evaluating the effectiveness of ads, and can be gener-
alized to other domains as well. The future work involves utilizing the distributed
optimization algorithms to further improve the scalability of our approach.

References
171
References
1. Agarwal, D., Chen, B.-C., Elango, P.: Spatio-temporal models for estimating click-through
rate. In: Proceedings of the 18th International Conference on World Wide Web, pp. 21–30
(2009)
2. Agarwal, D., Long, B., Traupman, J., Xin, D., Zhang, L.:
LASER: a scalable response
prediction platform for online advertising.
In: Proceedings of the 7th ACM International
Conference on Web Search and Data Mining, pp. 173–182 (2014)
3. Ahmed, A., Das, A., Smola, A.J.:
Scalable hierarchical multitask learning algorithms for
conversion optimization in display advertising. In: Proceedings of the 7th ACM International
Conference on Web Search and Data Mining, pp. 153–162 (2014)
4. Barajas, J., Akella, R., Holtan, M., Kwon, J., Null, B.: Measuring the effectiveness of display
advertising: a time series approach. In: Proceedings of the 20th International Conference on
World Wide Web (Companion Volume), pp. 7–8 (2011)
5. Bouchard, G., Yin, D., Guo, S.: Convex collective matrix factorization. In: Proceedings of the
16th International Conference on Artiﬁcial Intelligence and Statistics, pp. 144–152 (2013)
6. Bulut, A.: Topicmachine: conversion prediction in search advertising using latent topic models.
IEEE Trans. Knowl. Data Eng. 26(11), 2846–2858 (2014)
7. Cai, Y., Leung, H., Li, Q., Min, H., Tang, J., Li, J.: Typicality-based collaborative ﬁltering
recommendation. IEEE Trans. Knowl. Data Eng. 26(3), 766–779 (2014)
8. Cai, Y., Tong, H., Fan, W., Ji, P.: Fast mining of a network of coevolving time series. In:
Proceedings of the SIAM International Conference on Data Mining. SIAM (2015)
9. Chapelle, O.: Ofﬂine evaluation of response prediction in online advertising auctions. In:
Proceedings of the 24th International Conference on World Wide Web Companion, pp. 919–
922. International World Wide Web Conferences Steering Committee (2015)
10. Chapelle, O., Manavoglu, E., Rosales, R.: Simple and scalable response prediction for display
advertising. ACM Trans. Intell. Syst. Technol. 5(4), (2014)
11. Chen, T., Zhang, W., Lu, Q., Chen, K., Zheng, Z., Yu, Y.: Svdfeature: a toolkit for feature-based
collaborative ﬁltering. J. Mach. Learn. Res. 13, 3619–3622 (2012)
12. Cheng, H., Cantú-Paz, E.: Personalized click prediction in sponsored search. In: Proceedings
of the 7th ACM International Conference on Web Search and Data Mining, pp. 351–360 (2010)
13. chih Lee, K., Orten, B., Dasdan, A., Li, W.: Estimating conversion rate in display advertising
from past performance data.
In: Proceedings of the 18th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, pp. 768–776 (2012)
14. Chua, F.C.T., Oentaryo, R.J., Lim, E.-P.: Modeling temporal adoptions using dynamic matrix
factorization. In: Proceedings of the 13th IEEE International Conference on Data Mining,
pp. 91–100 (2013)
15. Davis, J., Goadrich, M.:
The relationship between precision-recall and ROC curves.
In:
Proceedings of the 23rd International Conference on Machine Learning, pp. 233–240 (2006)
16. Devooght, R., Kourtellis, N., Mantrach, A.:
Dynamic matrix factorization with priors on
unknown values. In: Proceedings of the 21th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pp. 189–198. ACM (2015)
17. Drumond, L.R., Schmidt-Thieme, L., Freudenthaler, C., Krohn-Grimberghe, A.: Collective
matrix factorization of predictors, neighborhood and targets for semi-supervised classiﬁcation.
In: Proceedings of the 18th Paciﬁc-Asia Conference on the Advances in Knowledge Discovery
and Data Mining, Part I, pp. 286–297 (2014)
18. Graepel, T., Candela, J.Q., Borchert, T., Herbrich, R.: Web-scale Bayesian click-through rate
prediction for sponsored search advertising in Microsoft’s bing search engine. In: Proceedings
of the 27th International Conference on Machine Learning, pp. 13–20 (2010)

172
8
Robust Representations for Response Prediction
19. Grotov, A., Chuklin, A., Markov, I., Stout, L., Xumara, F., de Rijke, M.: A comparative study
of click models for web search. In: Experimental IR Meets Multilinguality, Multimodality, and
Interaction, pp. 78–90. Springer, Cham (2015)
20. Gunasekar, S., Yamada, M., Yin, D., Chang, Y.:
Consistent collective matrix completion
under joint low rank structure. In: Proceedings of the Eighteenth International Conference
on Artiﬁcial Intelligence and Statistics (AISTATS) (2015)
21. Hong, L., Doumith, A.S., Davison, B.D.: Co-factorization machines: modeling user interests
and predicting individual decisions in twitter. In: Proceedings of the 6th ACM International
Conference on Web Search and Data Mining, pp. 557–566 (2013)
22. Hu, L., Cao, J., Xu, G., Cao, L., Gu, Z., Zhu, C.: Personalized recommendation via cross-
domain triadic factorization. In: Proceedings of the 22nd International Conference on World
Wide Web, pp. 595–606 (2013)
23. Huang, Y.-J., Xiang, E.W., Pan, R.: Constrained collective matrix factorization. In: Proceedings
of the 6th ACM Conference on Recommender Systems, pp. 237–240 (2012)
24. Kim, Y.-D., Choi, S.: Scalable variational Bayesian matrix factorization with side information.
In: Proceedings of the 17th International Conference on Artiﬁcial Intelligence and Statistics,
pp. 493–502 (2014)
25. King, M.A., Abrahams, A.S., Ragsdale, C.T.: Ensemble learning methods for pay-per-click
campaign management. Expert Syst. Appl. 42(10), 4818–4829 (2015)
26. Koenigstein, N., Dror, G., Koren, Y.: Yahoo! music recommendations: modeling music ratings
with temporal dynamics and item taxonomy. In: Proceedings of the 5th ACM Conference on
Recommender Systems, pp. 165–172 (2011)
27. Koren, Y.: Collaborative ﬁltering with temporal dynamics. In: Proceedings of the 15th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 447–456
(2009)
28. Koren, Y.: Factor in the neighbors: scalable and accurate collaborative ﬁltering. ACM Trans.
Knowl. Discov. Data (TKDD) 4(1), 1 (2010)
29. Li, B., Zhu, X., Li, R., Zhang, C., Xue, X., Wu, X.: Cross-domain collaborative ﬁltering over
time. In: Proceedings of the 22nd International Joint Conference on Artiﬁcial Intelligence,
pp. 2293–2298 (2011)
30. Li, H., Hong, R., Wu, Z., Ge, Y.: A spatial-temporal probabilistic matrix factorization model
for point-of-interest recommendation. In: Proceedings of the SIAM International Conference
on Data Mining, pp. 117–125. SIAM (2016)
31. Li, S., Kawale, J., Fu, Y.: Deep collaborative ﬁltering via marginalized denoising auto-encoder.
In: Proceedings of the 24th ACM International on Conference on Information and Knowledge
Management, pp. 811–820. ACM (2015)
32. Li, S., Kawale, J., Fu, Y.: Predicting user behavior in display advertising via dynamic collective
matrix factorization. In: Proceedings of the 38th International ACM SIGIR Conference on
Research and Development in Information Retrieval, pp. 875–878 (2015)
33. Li, S., Vlassis, N., Kawale, J., Fu, Y.: Matching via dimensionality reduction for estimation
of treatment effects in digital marketing campaigns.
In: Proceedings of the Twenty-Fifth
International Joint Conference on Artiﬁcial Intelligence, pp. 3768–3774. AAAI Press (2016)
34. Li, Y., Zhai, C., Chen, Y.: Exploiting rich user information for one-class collaborative ﬁltering.
Knowl. Inf. Syst. 38(2), 277–301 (2014)
35. Liu, Y., Pandey, S., Agarwal, D., Josifovski, V.: Finding the right consumer: optimizing for
conversion in display advertising campaigns. In: Proceedings of the 5th ACM International
Conference on Web Search and Data Mining, pp. 473–482 (2012)
36. Menon, A.K., Chitrapura, K.P., Garg, S., Agarwal, D., Kota, N.: Response prediction using
collaborative ﬁltering with hierarchies and side-information. In: Proceedings of the 17th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 141–149
(2011)

References
173
37. Oentaryo, R.J., Lim, E.-P., Low, J.-W., Lo, D., Finegold, M.: Predicting response in mobile
advertising with hierarchical importance-aware factorization machine. In: Proceedings of the
7th ACM International Conference on Web Search and Data Mining, pp. 123–132 (2014)
38. Park, S., Kim, Y.-D., Choi, S.: Hierarchical Bayesian matrix factorization with side infor-
mation. In: Proceedings of the 23rd International Joint Conference on Artiﬁcial Intelligence,
pp. 1593–1599 (2013)
39. Porteous, I., Asuncion, A.U., Welling, M.: Bayesian matrix factorization with side information
and dirichlet process mixtures. In: Proceedings of the 24th AAAI Conference on Artiﬁcial
Intelligence, pp. 563–568 (2010)
40. Radinsky, K., Svore, K.M., Dumais, S.T., Shokouhi, M., Teevan, J., Bocharov, A., Horvitz, E.:
Behavioral dynamics on the web: learning, modeling, and prediction. ACM Trans. Inf. Syst.
31(3), 16 (2013)
41. Rafailidis, D., Nanopoulos, A.: Repeat consumption recommendation based on users prefer-
ence dynamics and side information. In: Proceedings of the 24th International Conference on
World Wide Web Companion (Companion Volume), pp. 99–100 (2015)
42. Rendle, S.: Factorization machines with libFM. ACM Trans. Intell. Syst. Technol. 3(3), 57:1–
57:22 (2012)
43. Richardson, M., Dominowska, E., Ragno, R.: Predicting clicks: estimating the click-through
rate for new ads. In: Proceedings of the 16th International Conference on World Wide Web,
pp. 521–530 (2007)
44. Rosales, R., Cheng, H., Manavoglu, E.:
Post-click conversion modeling and analysis for
non-guaranteed delivery display advertising. In: Proceedings of the 5th ACM International
Conference on Web Search and Data Mining, pp. 293–302 (2012)
45. Saha, A., Sindhwani, V.: Learning evolving and emerging topics in social media: a dynamic
NMF approach with temporal regularization. In: Proceedings of the Fifth ACM International
Conference on Web Search and Data Mining, pp. 693–702. ACM (2012)
46. Salakhutdinov, R., Mnih, A.:
Probabilistic matrix factorization.
In: Proceedings of the
Advances in Neural Information Processing Systems 20, pp. 1257–1264 (2007)
47. Salakhutdinov, R., Mnih, A.: Bayesian probabilistic matrix factorization using Markov chain
Monte Carlo. In: Proceedings of the 25th International Conference on Machine Learning,
pp. 880–887 (2008)
48. Singh, A.P., Gordon, G.J.:
Relational learning via collective matrix factorization.
In:
Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, pp. 650–658 (2008)
49. Singh, A.P., Gordon, G.J.: A Bayesian matrix factorization model for relational data. In:
Proceedings of the 26th Conference on Uncertainty in Artiﬁcial Intelligence, pp. 556–563
(2010)
50. Wang, C., Liu, Y., Wang, M., Zhou, K., Nie, J.-y., Ma, S.:
Incorporating non-sequential
behavior into click models. In: Proceedings of the 38th International ACM SIGIR Conference
on Research and Development in Information Retrieval, pp. 283–292. ACM (2015)
51. Wang, J., Yuan, S., Zhang, W.:
Real-time bidding based display advertising: mechanisms
and algorithms. In: Advances in Information Retrieval, pp. 897–901. Springer International
Publishing, Cham (2016)
52. Xiong, C., Wang, T., Ding, W., Shen, Y., Liu, T.-Y.: Relational click prediction for sponsored
search. In: Proceedings of the 5th ACM International Conference on Web Search and Data
Mining, pp. 493–502 (2012)
53. Yan, L., Li, W., Xue, G., Han, D.:
Coupled group lasso for web-scale CTR prediction in
display advertising. In: Proceedings of the 31th International Conference on Machine Learning,
pp. 802–810 (2014)
54. Yang, L., Jing, L., Ng, M.K.: Robust and non-negative collective matrix factorization for text-
to-image transfer learning. IEEE Trans. Image Process. 24(12), 4701–4714 (2015)
55. Zhang, W., Du, T., Wang, J.: Deep learning over multi-ﬁeld categorical data: a case study on
user response prediction. CoRR (2016). arXiv:1601.02376

174
8
Robust Representations for Response Prediction
56. Zhang, X., Zhou, Y., Ma, Y., Chen, B.-C., Zhang, L., Agarwal, D.: GLMix: generalized linear
mixed models for large-scale response prediction. In: Proceedings of the 22nd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, pp. 363–372 (2016)
57. Zhu, K., Wu, R., Ying, L., Srikant, R.:
Collaborative ﬁltering with information-rich and
information-sparse entities. Mach. Learn. 97(1–2), 177–203 (2014)
58. Zhuang, Y., Chin, W.-S., Juan, Y.-C., Lin, C.-J.: A fast parallel SGD for matrix factorization
in shared memory systems. In: Proceedings of the 7th ACM Conference on Recommender
Systems, pp. 249–256 (2013)

Chapter 9
Robust Representations for Outlier Detection
Abstract Detecting outliers or anomalies is a fundamental problem in various
machine learning and data mining applications. Conventional outlier detection
algorithms are mainly designed for single-view data. Nowadays, data can be easily
collected from multiple views, and many learning tasks such as clustering and
classiﬁcation have beneﬁted from multi-view data. However, outlier detection from
multi-view data is still a very challenging problem, as the data in multiple views
usually have more complicated distributions and exhibit inconsistent behaviors.
To address this problem, we propose a multi-view low-rank analysis (MLRA)
framework for outlier detection in this chapter. MLRA pursuits outliers from a new
perspective, robust data representation. It contains two major components. First, the
cross-view low-rank coding is performed to reveal the intrinsic structures of data. In
particular, we formulate a regularized rank-minimization problem which is solved
by an efﬁcient optimization algorithm. Second, the outliers are identiﬁed through an
outlier score estimation procedure. Different from the existing multi-view outlier
detection methods, MLRA is able to detect two different types of outliers from
multiple views simultaneously. To this end, we design a criterion to estimate the
outlier scores by analyzing the obtained representation coefﬁcients. Moreover, we
extend MLRA to tackle the multi-view group outlier detection problem. Extensive
evaluations on seven UCI datasets, the MovieLens, the USPS-MNIST, and the
WebKB datasets demonstrate that our approach outperforms several state-of-the-art
outlier detection methods.
9.1
Overview1
As a fundamental data mining technique, outlier detection (or anomaly detection)
identiﬁes the abnormal samples in a data set. Many effective outlier detection
algorithms have been developed during the past decades, and they have been exten-
sively applied to many safety-critical applications, such as fraud detection, network
intrusion identiﬁcation and system health monitoring [47, 51, 65]. The representa-
tive outlier detection methods include the reference-based approach [48], inductive
1This chapter is reprinted with permission from SIAM. “Multi-View Low-Rank Analysis for
Outlier Detection”, SIAM International Conference on Data Mining, 2015.
© Springer International Publishing AG 2017
S. Li, Y. Fu, Robust Representation for Data Analytics, Advanced Information
and Knowledge Processing, DOI 10.1007/978-3-319-60176-2_9
175

176
9
Robust Representations for Outlier Detection
logic programming based algorithm [2], information-theoretic algorithm [59], and
isolation based algorithm [39]. Recently, some outlier detection methods have been
developed to deal with the high-dimensional data. Pham et al. designed an efﬁcient
algorithm for angle-based outlier detection in high-dimensional data [50]. Zimek et
al. studied the subsampling problem in statistical outlier detection, and provided
effective solutions [66]. Schubert et al. presents a generalization of density-based
outlier detection methods using kernel density estimation [51]. In general, these
existing methods analyze the distribution or density of a dataset, and identify outliers
by using some well-deﬁned criteria. Moreover, these methods were designed for
single-view data like many other conventional data mining methods.
Nowadays, data are usually collected from diverse domains or obtained from
various feature extractors, and each group of features is regarded as a particular
view [62]. Multi-view data provide plentiful information to characterize the prop-
erties of objects. Many algorithms have been designed in the multi-view settings,
by considering the complementary information from different data views [30, 32].
Moreover, some machine learning and data mining problems, such as clustering [57]
and subspace learning [58], have been greatly beneﬁtted from the multi-view data.
Nevertheless, detecting outliers from multi-view data is still a challenging problem
for two reasons: (1) the multi-view data usually have more complicated distributions
than the single-view data; (2) the data points may exhibit inconsistent behaviors in
different views. In other words, outliers may be easily observed as normal data in
one or more views.
In this chapter, we tackle the multi-view outlier detection problem from the
perspective of data representation. We would argue that, by leveraging the repre-
sentation relationship of samples, the outliers contained in data set can be correctly
identiﬁed.
Recently, low-rank matrix recovery has been extensively studied to exploit
the intrinsic structure of data [9, 26, 55]. Many applications have been beneﬁted
from such structural information, such as subspace clustering [43], multi-task
learning [10], subspace learning [25, 27], transfer learning [28, 29, 52] and semi-
supervised classiﬁcation [24, 26]. In low-rank subspace clustering [43], the sample
set is served as bases (or dictionary) to reconstruct itself, which inspires us to explore
the representation relationship of samples. Our intuition is that a normal sample
usually serves as a good contributor in representing the other normal samples,
while the outliers do not. Therefore, it is reasonable to identify outliers from the
representation coefﬁcients in low-rank matrix recovery.
Based on the assumptions above, we propose a novel outlier detection framework
named Multi-view Low-Rank Analysis (MLRA). Figure 9.1 shows the ﬂowchart of
our framework. It contains two successive components: (1) robust data represen-
tation by cross-view low-rank analysis, and (2) the calculation of outlier scores.
In particular, two types of outliers are considered in our framework. The Type-I
outliers are samples that show inconsistent clustering results across different views,
and the Type-II outliers have abnormal behaviors in each view. For example, an
animal dataset might contain two data views, including the image view and text
view. The features extracted from a horse image might be very similar to these

9.1
Overview
177
=
+
=
+
…
…
…
Multi-view Data
Cross-view Low-Rank Analysis
Outlier Score
Index
Score
1
0.9
2
0.3
3
0.9
4
0.9
5
0.9
6
0.2
Type-I outlier
Type-II outlier
Fig. 9.1 Flowchart of the proposed MLRA framework. Given a multi-view sample set X D
fX1; X2;    ; XVg, MLRA ﬁrst performs cross-view low-rank analysis to reveal the reconstruction
relationship of samples, and then calculate outlier scores. Finally, it identiﬁes the outliers contained
in data (i.e., the second and sixth column in data matrix). Zv are representation coefﬁcient matrices
with low-rank structures, and Ev are sparse matrices
from a deer image, as these two species have similar limbs. But they have quite
different features in text view. Thus, they could be the Type-I outliers. In addition,
if some natural scene images are accidentally included in the animal dataset, they
are considered as Type-II outliers. To build effective learning systems, it is crucial
to identify such outliers in advance. In Fig. 9.1, the second column in X1 (marked
by red color) is an outlier in view 1, but it’s a normal sample in other views. So it’s
a Type-I outlier. Moreover, if the last column (marked by blue color) is abnormal in
all of the V views, it’s a Type-II outlier.
By far, only a few methods have been proposed to detect outliers in multi-
view data. Das et al. presented a heterogeneous anomaly detection method using
multiple kernel learning [12]. Muller et al. proposed a multi-view outlier ranking
algorithm using subspace analysis [46]. Hsiao et al. utilized the pareto depth
analysis to develop a multi-criteria anomaly detection algorithm [17]. The most
relevant works to our approach are clustering based multi-view outlier detection
methods, horizontal anomaly detection (HOAD) [15] and anomaly detection using
afﬁnity propagation (AP) [1]. These methods obtained promising results in various
applications. They detect outliers from the perspective of ensemble learning [12]
or clustering [15]. Unlike the existing methods, our approach tackles the multi-
view outlier detection problem from a different perspective, i.e., robust data
representation.
Furthermore, although the two types of outliers discussed above exist in many
real-world applications, traditional single-view and multi-view outlier detection
methods cannot handle them simultaneously. For example, the multi-view methods
proposed in [15] and [1] are only designed for the Type-I outliers. However, our
approach can detect both Type-I and Type-II outliers.

178
9
Robust Representations for Outlier Detection
We formulate the cross-view low-rank analysis in our framework as a constrained
rank-minimization problem, and present an efﬁcient optimization algorithm to solve
it. After that, we devise a criterion to estimate the outlier score for each sample,
considering two types of outliers in multiple views. Moreover, we extend the MLRA
framework to a new problem, multi-view group anomaly detection. Extensive results
on ten benchmark datasets are reported.
Our work is relevant to multi-view learning and low-rank modeling. Multi-view
learning has been receiving increasing attention in recent years [62]. One implicit
assumption is that either view alone has sufﬁcient information about the samples,
but the complexity of learning problems can be reduced by eliminating hypotheses
from each view that tend not to agree with each other [54]. One of the representative
work in multi-view learning is co-training [7], which learns from the samples
that are described by two distinct views. The representative multi-view learning
algorithms include manifold co-regularization [53], and multi-view feature learn-
ing [44]. The basic idea of these methods is to exploit the consistency among multi-
ple views to enhance the learning performance. In our MLRA framework, however,
we exploit the inconsistency information among different views to identify outliers.
Our approach is also related to low-rank matrix learning, which has attracted
increasing attention in recent years [4, 23]. Robust PCA (RPCA) [9] and Low-Rank
Representation (LRR) are two representative low-rank learning methods. RPCA can
recover noisy data from one single space, while LRR is able to recover multiple
subspaces in the presence of noise [40, 43]. The most successful application of
LRR is subspace clustering. It can correctly recover the subspace membership
of samples, even if the samples are heavily corrupted. In addition, LRR shows
promising performance in outlier detection [41, 42]. Liu et al. applied the low-rank
representation model to outlier detection, and achieved promising results [41]. Li
et al. incorporated the low-rank constraint into the support vector data description
model, and detected outliers from image data sets [31]. However, these methods can
only deal with single-view data. Different from LRR and its variants, our approach
performs multi-view low-rank analysis for outlier detection. To the best of our
knowledge, MLRA is the ﬁrst multi-view low-rank learning approach.
This chapter is a substantial extension of our previous conference paper [33]. In
summary, the major contributions of this work are as follows:
•
We design a multi-view outlier detection framework, MLRA. To the best of our
knowledge, our work is the ﬁrst attempt to detect two types of outliers in a joint
framework.
•
We identify the outliers from the perspective of data representation. To this
end, we develop a cross-view low-rank analysis model, and present an efﬁcient
optimization algorithm to solve it.
•
We extend the MLRA to multi-view group outlier detection, by reﬁning the
objective function and devising a criterion for estimating outlier scores.
•
We evaluate our approach and related methods on seven UCI datasets, the
MovieLens, the USPS-MNIST, and the WebKB datasets. Extensive results
demonstrated the effectiveness of our approach.

9.2
Preliminary
179
9.2
Preliminary
9.2.1
Outlier Detection
Many single-view outlier detection algorithms have been developed over the past
decade, such as [14, 39, 56, 66]. Tong et al. proposed a non-negative residual matrix
factorization (NrMF) method for anomaly detection in graph data. It estimates
outlier scores from the residuals, but it is only designed for single-view data. Lee et
al. designed an anomaly detection algorithm via online oversampling PCA [22].
Liu et al. studied a speciﬁc scenario when data have imperfect labels [38]. Du et al.
presented a discriminative metric learning for outlier detection [13]. Perozzi et al.
proposed the focused clustering and outlier detection method in large attributed
graphs [49]. Liu et al. designed the support vector data description (SVDD) based
outlier detection method [37].
To date, only a few methods have been developed to handle the multi-view
outlier detection problem [19, 36]. The most relevant multi-view methods to our
approach are clustering based multi-view outlier detection methods, horizontal
anomaly detection (HOAD) [15] and anomaly detection using afﬁnity propagation
(AP) [1].
HOAD aims to detect outliers from several different data sources that can
be considered as multi-view data. In HOAD, the samples that have inconsistent
behavior among different data sources are marked as anomalies. HOAD ﬁrst
constructs a combined similarity graph based on the similarity matrices in multiple
views and computes spectral embeddings for the samples. Then it calculates
the anomalous score of each sample using the cosine distance between different
spectral embeddings. However, HOAD is only designed for the outliers that show
inconsistent behavior across different views (i.e., the Type-I outlier deﬁned in this
chapter). In addition, the graph constructed in HOAD will be dramatically expanded
for multi-view data, which increases considerable computational cost.
Most recently, Alvarez et al. proposed an afﬁnity propagation (AP) based multi-
view anomaly detection algorithm [1]. This algorithm identiﬁes anomalies by
analyzing the neighborhoods of each sample in different views, and it adopts four
different strategies to calculate anomaly scores. Speciﬁcally, it performs clustering
in different view separately. The clustering-based afﬁnity vectors are then calculated
for each sample. There are signiﬁcant differences between our approach and
Alvarez’s algorithm. First, like HOAD, Alvarez’s algorithm is a clustering based
method that analyze clustering results in different views to detect outliers. However,
our approach models the multi-view outlier detection problem from the perspective
of data reconstruction, and performs low-rank analysis to identify outliers. Second,
Alvarez’s algorithm was only designed for detecting the Type-I outliers. However,
our approach can detect both Type-I and Type-II outliers jointly.
Group anomaly detection is a relatively new topic. Several effective algorithms
have been presented to address this problem [45, 61, 64]. However, these algorithms

180
9
Robust Representations for Outlier Detection
can only handle the single-view data. To the best of our knowledge, our work is the
ﬁrst attempt to address the multi-view group outlier detection problem.
9.2.2
Multi-view Outliers
Given a single-view sample set NX D fx1; x2;    ; xng 2 Rdn that contains a
small amount of outliers, traditional single-view outlier detection methods aim at
identifying those outliers automatically. These methods usually utilize the distance
or density information from sample set [3], and identify outliers using decision
boundaries or outlier scores. In addition, the notations summarized in Table 9.1
will be used throughout this chapter.
When data are collected from multiple views, we have a collection of sample
sets, X D fX.1/; X.2/;    ; X.V/g, where V is the total number of views. For the
sample set observed in view v, we also have X.v/ D fx.v/
1 ; x.v/
2 ;    ; x.v/
n g, where
n is the number of samples in each view. Generally, multi-view outlier detection is
more difﬁcult than single-view outlier detection, as outliers may behave completely
different across multiple views.
In this chapter, we focus on detecting outliers from multi-view data. In particular,
we aim to identify two types of outliers that are deﬁned below.
Deﬁnition 9.1 Type-I Outlier is an outlier that exhibit inconsistent characteristics
(e.g., cluster membership) across different views.
Figure 9.2 illustrates two data views, and each view contains three clusters and
several outliers. The red triangle belongs to different clusters in view 1 and view
2. Thus, it’s a Type-I outlier. Note that the existing multi-view outlier detection
algorithms [1, 15] are designed for the Type-I outlier.
Table 9.1 Summary of
notations
Notation
Description
X
Multi-view sample set
NX
Single-view sample set
n
The number of samples in X
V
Number of views
X.v/
Sample set in view v
Z.v/
Representation coefﬁcients in view v
D.v/
Dictionary in view v
E.v/
Error matrices in view v
oi
Outlier score for the i-th sample in X
k  k
Trace norm (i.e., nuclear norm)
k  kF
Frobenius norm
k  k2;1
l2;1 norm

9.3
Multi-view Low-Rank Analysis (MLRA)
181
View 1
View 2
Type-I outlier
Type-II outlier
Fig. 9.2 Illustration of Type-I outliers (red triangles) and Type-II outliers (blue circles) in two-
view data
Deﬁnition 9.2 Type-II Outlier is an outlier that exhibits consistent characteristics
across different views, but it shows abnormal behavior in each view.
In Fig. 9.2, the blue circle is a Type-II outlier because it does not belong to any
cluster in both views. We also notice that this type of outliers are ignored by existing
multi-view outlier detection methods.
9.3
Multi-view Low-Rank Analysis (MLRA)
In this section, we describe the proposed multi-view low-rank analysis (MLRA)
framework. Our goal is to detect two types of outliers simultaneously. As shown
in Fig. 9.1, our MLRA framework contains two successive components, which are
cross-view low-rank analysis and the calculation of outlier scores.
9.3.1
Cross-View Low-Rank Analysis
We formulate the cross-view low-rank analysis as a constrained rank minimization
problem, and then present an optimization algorithm to solve it.
9.3.1.1
Problem Formulation
Unlike clustering based methods presented in [1, 15], we tackle the multi-view
outlier detection problem from the perspective of data representation. In particular,
for the sample set X.v/ 2 Rdn observed in the v-th view, we can represent it as
X.v/ D X.v/Z.v/ C E.v/;
(9.1)
where Z.v/ 2 Rnn is a coefﬁcient matrix and E.v/ 2 Rdn is a noise matrix.

182
9
Robust Representations for Outlier Detection
Like other outlier detection algorithms, we assume that the (normal) samples
came from K clusters. The samples in the same cluster could be drawn from the
same subspace. Therefore, Z.v/ should be a low-rank coefﬁcient matrix that has the
block-diagonal structure. The coefﬁcient vectors in Z.v/ belong to the same cluster
tend to have high correlations.
On the other hand, outliers are actually “sample-speciﬁc” noises in data matrix.
It’s reasonable to use l2;1 norm to measure the noise matrix, as l2;1 norm makes the
column of the matrix to be zero. Moreover, we consider the cross-view relationships
between coefﬁcient matrices Z.v/. Our intuition is that the representation coefﬁcients
should be consistent for normal data in different views, but should be inconsistent
for outliers.
Based on the observations above, we present the objective function as follows
min
Z.v/;E.v/
VP
vD1
.rank.Z.v// C ˛jjE.v/jj2;1/
Cˇ
V1
P
vD1
VP
pDvC1
jjZ.v/  Z.p/jj2;1
s:t:
X.v/ D X.v/Z.v/ C E.v/; v D 1; 2;    ; V;
(9.2)
where kEk2;1 denotes the l2;1 norm, and kEk2;1 D Pn
iD1
qPd
jD1 .ŒEji/2, ˛ and ˇ
are trade-off parameters to balance different terms.
In (9.2), the ﬁrst two terms PV
vD1.rank.Z.v// C ˛jjE.v/jj2;1/ represent the
low-rank and sparse constraints on each view, respectively. The last term
PV1
vD1
PV
pDvC1 jjZ.v/  Z.p/jj2;1 indicates the summation of pair-wise error of
coefﬁcient matrices Z.v/. Considering the inconsistency columns in Z.v/ and Z.p/, we
utilize the l2;1 norm on .Z.v/  Z.p//. This term ensures robust data representations.
If the coefﬁcient matrix in a speciﬁc view (e.g., Z.2/) are unreliable or corrupted, it
would be ﬁxed by virtue of the last term.
For the sake of simplicity, we only provide detailed derivations and solutions for
the two-view case. They can be extended easily to multi-view cases. In the two-view
case, we have X D fX.1/; X.2/g, and x.v/
i
.v D 1; 2/ denote the i-th sample in view
v. Then we can modify the object function in (9.2) for two-views. However, the
optimization problem in (9.2) is hard to solve, as rank./ function is neither convex
nor continuous. Trace norm is a commonly-used approximation of the non-convex
function rank./ [9, 20]. Then, (9.2) for the two-view case is formulated as
min
Z.v/;E.v/
2P
vD1
.kZ.v/k C ˛jjE.v/jj2;1/ C ˇjjZ.1/  Z.2/jj2;1
s:t:
X.v/ D X.v/Z.v/ C E.v/; v D 1; 2;
(9.3)
where k  k represents the trace norm [9].

9.3
Multi-view Low-Rank Analysis (MLRA)
183
9.3.1.2
Optimization
To solve (9.3), we employ an efﬁcient optimization technique, the inexact aug-
mented Lagrange multiplier (ALM) algorithm [35]. First, we introduce relaxation
variables J.v/ and S to (9.3), and obtain
min
Z.v/;J.v/;E.v/;S
2P
vD1
.kJ.v/k C ˛jjE.v/jj2;1/ C ˇjjSjj2;1
s:t:
X.v/ D X.v/Z.v/ C E.v/;
Z.v/ D J.v/; v D 1; 2;
S D Z.1/  Z.2/:
(9.4)
Furthermore, the augmented Lagrangian function of (9.4) is
L D
2P
vD1
.kJ.v/k C ˛jjE.v/jj2;1/ C ˇjjSjj2;1C
2P
vD1
.hW.v/; X.v/  X.v/Z.v/  E.v/i C 
2 jjZ.v/  J.v/jj2
F
ChP.v/; Z.v/  J.v/i C 
2 jjX.v/  X.v/Z.v/  E.v/jj2
F/
ChQ; S  .Z.1/  Z.2//i C 
2 jjS  .Z.1/  Z.2//jj2
F;
(9.5)
where W.v/, P.v/ and Q are Lagrange multipliers, and 
 > 0 is a penalty parameter.
The objective function is not jointly convex to all the variables, but it is convex to
each of them when ﬁxing the others. Therefore, we update each variable as follows.
9.3.1.3
Update J.v/
By ignoring the irrelevant terms w.r.t. J.v/ in (9.5), we have the objective as follows
J.v/ D arg min
J.v/
2
X
vD1
. 1

jjJ.v/jj C 1
2jjJ.v/  .Z.v/ C P.v/

 /jj2
F/:
(9.6)
The optimal solution to (9.6) can be obtained by using the singular value
thresholding (SVT) algorithm [8]. In detail, we have J D Z.v/ C .P.v/=
/. The
SVD of J is written as J D UJ˙JVJ, where ˙J D diag.fig1ir/, r denotes
the rank, and i denote the singular values. The solution is J.v/ D UJ˝.1=
/.˙J/VJ,
where ˝.1=
/.˙J/ D diag.fi  .1=
/gC/, and .a/C indicates the positive portion
of a.

184
9
Robust Representations for Outlier Detection
9.3.1.4
Update Zv
We ignore the terms independent of Z.v/ in (9.5), and obtain
2P
vD1
.hW.v/; X.v/  X.v/Z.v/  E.v/i C 
2 jjZ.v/  J.v/jj2
F
C hP.v/; Z.v/  J.v/i C 
2 jjX.v/  X.v/Z.v/  E.v/jj2
F/
C hQ; S  .Z.1/  Z.2//i C 
2 jjS  .Z.1/  Z.2//jj2
F;
(9.7)
By setting the derivative w.r.t. Z.1/ and Z.2/ to zero respectively, we obtain the
solutions as follows
Z.1/ D .2I C X.1/>X.1//1.X.1/>.X.1/  E.1//
C J.1/ C S C Z.2/ C X.1/>W.1/P.1/CQ

/:
(9.8)
Z.2/ D .2I C X.2/>X.2//1.X2>.X.2/  E.2//
C J.2/  S C Z.1/ C X.2/>W.2/P.2/Q

/:
(9.9)
9.3.1.5
Update S
By dropping the terms irrelevant to S, Eq. (9.5) is reduced to
S D arg min
S
ˇ

jjSjj2;1
C 1
2jjS  .Z.1/  Z.2/ C Q

/jj2
F:
(9.10)
9.3.1.6
Update E.v/
Similarly, after dropping terms independent of E.v/, we have
E.v/ D arg min
E.v/
2P
vD1
. ˛

jjE.v/jj2;1
C 1
2jjE.v/  .X.v/  X.v/Z.v/ C W.v/

 /jj2
F/:
(9.11)
The solution to problems like (9.10) and (9.11) is discussed in [43]. Take (9.10)
as an example and let  D Z.1/  Z.2/ C Q

, the i-th column of S is
S.W; i/ D
(
kikˇ
kik i; if ˇ < kik;
0;
otherwise:
(9.12)

9.3
Multi-view Low-Rank Analysis (MLRA)
185
Algorithm 9.1 Solving (9.5) using Inexact ALM
Input: data set X D fX.1/; X.2/g, parameters ˛, ˇ,
Z.v/ D J.v/ D 0, E.v/ D 0, W.v/ D 0, P.v/ D 0,
Q D 0,  D 1:2, 
 D 0:1, 
max D 1010,  D 108
1:while not converged do
2: Fix the others and update J.1/ and J.2/ using (9.6).
3: Fix the others and update Z.1/ using (9.8).
4: Fix the others and update Z.2/ using (9.9).
5: Fix the others and update S using (9.10).
6: Fix the others and update E.v/ using (9.11).
7: Update the multipliers W.v/, P.v/ and Q
W.v/ D W.v/ C 
.X.v/  X.v/Z.v/  E.v//;
P.v/ D P.v/ C 
.Z.v/  J.v//;
Q D Q C 
.S  .Z.1/  Z.2///:
8: Update the penalty parameter 
 by

 D min.
max; 
/
9: Examine the conditions for convergence
jjX.v/  X.v/Z.v/  E.v/jj1 < 
and
jjZ.v/  J.v/jj1 < 
and
jjS  .Z.1/  Z.2//jj1 < 
10: end while
Output: Z.v/; E.v/
Finally, the complete optimization algorithm for solving (9.5) is outlined in
Algorithm 9.1. We also show the initializations for each variable in the algorithm.
9.3.1.7
Discussion and Complexity Analysis
The Inexact ALM is a mature optimization technique. It usually converges well in
practice, although proving the convergence in theory is still an open issue [11]. In
the experiments, we will show the convergence property of our algorithm.
Steps 2–4 are the most time-consuming parts in Algorithm 9.1. Let n denote
the sample size. In Step 2, the SVD of n  n matrices is required by the SVT
operator, which costs O.n3/. Steps 3–4 involve the matrix inversion and matrix
multiplication, which usually cost O.n3/. As a result, the time complexity of one
iteration in Algorithm 9.1 is O.n3/. We will show the running time of our algorithm
and its competitors in the experiments.
9.3.2
Outlier Score Estimation
With the optimal solutions Z.v/ and E.v/, we design a criterion to estimate the outlier
score of each sample. To calculate the outlier score vector o, our criterion (for the
two-view case) is formulated as

186
9
Robust Representations for Outlier Detection
o.i/ D
n
X
kD1
.u.i/
k Z.1/
ik Z.2/
ik /  
n
X
kD1
.E.1/
ik E.2/
ik /;
(9.13)
where o.i/ denotes the outlier score of the i-th sample, u.i/ 2 Rn1 is a constant
indictor vector. In detail, the k-th element in u.i/ corresponds to the k-th sample in
X. If samples xi and xk belong to the same class, then u.i/
k D 1; otherwise, u.i/
k D 0.
 is a trade-off parameter (we set  D 0:5 in the experiments).
The criterion (9.13) helps us detect two types of outliers simultaneously. The
ﬁrst term in (9.13) measures the inconsistency of the i-th sample across two views.
From the point of view of data representation, a sample is mainly represented by
those samples came from the same cluster. Therefore, we evaluate the inner-class
representation coefﬁcients by virtue of u.i/. For instance, if the i-th sample is normal
in both views, the coefﬁcients in Z.1/
i
and Z.2/
i
should be consistent. As a result,
the value of Pn
kD1.u.i/
k Z.1/
ik Z.2/
ik / should be relatively large. On the contrary, if the
i-th sample is an outlier that exhibits diverse characteristics in different views, the
inconsistent coefﬁcients Z.1/
i
and Z.2/
i
would lead to a small value. Therefore, this
term is suitable for detecting the Type-I outliers.
The second term in (9.13) contributes to identifying the Type-II outliers. Each
column in E.1/ and E.2/ corresponds to the reconstruction error vectors in view 1
and view 2, respectively. If the i-th sample is normal in at least one of the views,
the value of Pn
kD1.E.1/
ik E.2/
ik / tends to be zero, and then this term won’t affect the
outlier score o.i/ too much. However, if the i-th sample is a Type-II outlier which
shows abnormal behavior in both views, the summation in the second term will be
increased. Finally, the outlier score o.i/ will be further decreased.
Further, a general criterion for the V-view case is
o.i/ D
V1
X
pD1
V
X
qDpC1
.
n
X
kD1
.u.i/
k Z.p/
ik Z.q/
ik /  
n
X
kD1
.E.p/
ik E.q/
ik //;
(9.14)
After calculating the outlier scores for all the samples, the sample xi is marked
as an outlier if the score o.i/ is smaller than the threshold . The complete MLRA
algorithm is summarized in Algorithm 9.2.
9.4
MLRA for Multi-view Group Outlier Detection
In this section, we extend the MLRA framework to a novel application, multi-view
group outlier detection. Different from original outlier detection tasks that identify
individual abnormal data points, we aim to detect a group of abnormal data points
across different views.

9.4
MLRA for Multi-view Group Outlier Detection
187
Algorithm 9.2 MLRA for Outlier Detection
Input: Multi-view sample set X, threshold 
1: Normalize each sample x.v/
i ,
x.v/
i
D x.v/
i =jjx.v/
i jj.
2: Solve objective (9.5) using Algorithm 9.1 and obtain
optimal solution Zv, Ev.
3: Calculate outlier score for each sample using (9.14).
4: Generate binary label vector L
If o.i/ < , L.i/ D 1; otherwise, L.i/ D 0.
Output: Binary outlier label vector L
9.4.1
Motivation
In practice, outlier may not only appear as an individual point, but also as a group.
For example, a group of people collude to create false product reviews in social
media websites [64]. The most challenging part in group anomaly detection is that,
the outliers appear to be normal at the individual level. Existing works on group
anomaly detection mainly deal with the single-view data [45, 61, 64]. In this chapter,
we propose a new problem, multi-view group outlier detection.
The group outlier detection problem becomes more complicated in the multi-
view settings. Our assumption is that the dataset contains several groups, and each
group is considered as a cluster. In other words, we can observe several clusters in
each view. Ideally, the cluster structure in all the views should be consistent. In the
multi-view group outlier detection problem, one cluster might be identiﬁed as an
outlier group, if it exhibits inconsistent behavior in different views.
Formally, an outlier group in the multi-view setting is deﬁned as follows.
Deﬁnition 9.3 An Outlier Group is a set of data points that form as a cluster in each
view, but show inconsistent behavior across different views.
The group outlier is actually a special case of Type-I outlier we deﬁned in
Sect. 9.2. Our goal is to identify such outlier groups from the perspective of data
representation.
9.4.2
Formulation and Algorithm
We extend our MLRA framework for the multi-view group outlier detection
problem. As before, we provide detailed derivations and solutions for the two-view
case. As the individual outlier points are not considered in this problem, we can
simplify (9.3) as

188
9
Robust Representations for Outlier Detection
min
Z.v/
2P
vD1
.kZ.v/k C ˛jjX.v/  X.v/Z.v/jjF/
CˇjjZ.1/  Z.2/jj2;1:
(9.15)
In (9.15), we drop the l2;1 constraints on the reconstruction errors, and utilize the
Frobenius norm. The reason is that we assume the data only contain group outliers.
The group outliers are the Type-I outliers, which show inconsistent behavior across
different views. As discussed in Sect. 9.3, the l2;1 norm is very suitable to detect the
Type-II outliers. Therefore, it is not very necessary to use l2;1 norm in this scenario.
However, if the data contain both individual-level outliers and group-level outliers,
we suggest using the l2;1 norm to model reconstruction errors. The problem (9.15)
can be solved using the same optimization technique described in Sect. 9.3.
Using the optimal solutions Z.v/, we design a criterion to calculate the outlier
score vector og. The criterion for group outlier detection is formulated as
og.i/ D
n
X
kD1
j.U ı .Z.1/>Z.1//
kZ.1/k2
F
 U ı .Z.2/>Z.2//
kZ.2/k2
F
/ikj;
(9.16)
where U is the cluster membership indicator matrix, jaj denotes the absolute value
of a, and A ı B denotes the element-wise product of matrices A and B.
In (9.16), .Z.v/>Z.v// is a robust estimation of the cluster structure in view v,
U ı .Z.v/>Z.v// means that we only count the block diagonal parts in coefﬁcients
matrices Z.v/. In practice, U can be obtained by performing spectral clustering on
Z.v/. We normalize the estimations in each view, and measure the differences in two
views. For instance, if each view contains two groups, and U ı .Z.v/>Z.v// should
have two clear block diagonals. If one group is an outlier group, the corresponding
block diagonal part in j Uı.Z.1/>Z.1//
kZ.1/k2
F
 Uı.Z.2/>Z.2//
kZ.2/k2
F
j should be enlarged, as this group
have inconsistent characteristics in two views.
The sample xi is marked as a member of the outlier group if the score og.i/ is
smaller than the threshold . The MLRA based group outlier detection algorithm is
summarized in Algorithm 9.3.
Algorithm 9.3 MLRA for group outlier detection
Input: Multi-view sample set X, threshold 
1: Normalize each sample x.v/
i ,
x.v/
i
D x.v/
i =jjx.v/
i jj.
2: Solve objective (9.15) and obtain optimal solutions Zv.
3: Calculate outlier score for each sample using (9.16).
4: Generate binary label vector L
If o.i/ < , L.i/ D 1; otherwise, L.i/ D 0.
Output: Binary outlier label vector Lg

9.5
Experiments
189
9.5
Experiments
The performance of our MLRA framework is evaluated on seven UCI datasets [5],
MovieLens-1M dataset,2 the USPS-MNIST dataset [18, 21], and the WebKB
dataset [6].3
9.5.1
Baselines and Evaluation Metrics
Our approach is compared with several state-of-the-art single-view and multi-view
outlier detection methods in the presence of two types of outliers. The compared
methods are listed as follows:
•
Low-Rank Representations (LRR) [41]. LRR is a representative outlier detection
method for single-view data. Thus, we testify its performance on two views
separately.
•
Direct Robust Matrix Factorization (DRMF) [60]. DRMF formulates robust fac-
torization as a matrix approximation problem with constraints on the cardinality
of the outlier set.
•
Outlier Pursuit (OP) [63]. OP is able to recover the optimal low-dimensional
space and identiﬁes outliers. It’s also a single-view method.
•
HOrizontal Anomaly Detection (HOAD) [15]. HOAD is a clustering-based
multi-view outlier detection method. Two parameters m and k in HOAD have
been ﬁne tuned to obtain its best performance.
•
Anomaly detection using Afﬁnity Propagation (AP) [1]. AP is the state-of-
the-art multi-view outlier detection method. The authors employed two afﬁnity
measurements and four anomaly score calculation strategies. In this chapter, we
use the L-2 distance and Hilbert-Schmidt Independence Criterion (HSIC), as they
usually yield better performance than others.
As suggested in [1, 15], we adopt the receiver operating characteristic (ROC)
curves as the evaluation metric, which represents the trade-off between detection
rate and false alarm rate. We also report the area under ROC curve (AUC). The false
positive rate (FPR) and true positive rate (TPR) used for generating ROC curves are
deﬁned as follows:
FPR D
FP
FPCTN ;
TPR D
TP
TPCFN ;
(9.17)
where FP, TN, FN and TP represent the false positives, true negatives, false
negatives and true positives, respectively.
2http://grouplens.org/datasets/movielens/
3http://lig-membres.imag.fr/grimal/data.html

190
9
Robust Representations for Outlier Detection
9.5.2
Synthetic Multi-view Settings on Real Data
9.5.2.1
UCI Datasets
We employ seven benchmark datasets, namely “Iris”, “Letter”, “Waveform”, “Zoo”,
“Ionosphere”, “Pima” and “Wdbc” from the UCI machine learning repository [5].
To conduct fair comparisons, we follow the sample settings in [1]. Since all the seven
datasets are not multi-view datasets, we simulate two views as suggested in [15]. In
particular, the feature representations of each dataset are divided into two subsets,
where each subset is considered as one view of the data. In order to generate a
Type-I outlier, we take two objects from two different classes and swap the subsets
in one view but not in the other. To generate a Type-II outlier, we randomly select a
sample, and replace its features in two views as random values. In total, 15% data are
preprocessed and labeled as outliers. Table 9.2 summarizes the detailed information
of all the UCI datasets used in this chapter.
To illustrate the convergence property of our algorithm, we show in Fig. 9.3a the
relative error on the Iris dataset. The relative error in each iteration is calculated
by max.jjX.1/  X.1/Z.1/  E.1/kF=jjX.1/jjF; jjX.2/  X.2/Z.2/  E.2/kF=jjX.2/jjF/.
Figure 9.3a shows that our algorithms converges quickly, which ensures the less
computational cost of our approach.
As for the parameter selection, we adopted a coarse-to-ﬁne strategy to ﬁnd the
proper range for parameters. There are two major parameters in our approach, ˛ and
ˇ. We tuned their values in the range of f102; 101;    ; 102g. Figure 9.3b shows
the AUC of our approach on the Pima dataset, varying the values of ˛ and ˇ. Note
that we obtained similar results on other datasets. We can observe from Fig. 9.3b
that, as the dataset contain “sample-speciﬁc” noise, the two parameters usually tend
to be small values around 0.04. Also, as we chose ROC and AUC as the evaluation
metrics, we do not need to specify the threshold  in Algorithm 9.2. In fact, different
values of  were employed to generate the ROC curves.
For each dataset, we repeat the random outlier generation procedures for 50
times, evaluate the performance of each compared method on those 50 sets, and
report the average results. We conduct two settings for each method: (1) Type-
I outliers only; (2) Type-I and Type-II outliers. In this way, we can observe the
strengths and limitations of different methods.
Table 9.2 Summary of seven
UCI datasets (n D number of
samples, m1 D number of
Type-I outliers, m2 D number
of Type-II outliers, d D
number of dimensions)
Datasets
n
m1
m2
d
Iris
150
16
8
4
Letter
1300
130
65
16
Ionosphere
351
36
18
34
Zoo
101
10
5
16
Waveform
1200
120
60
21
Pima
768
76
38
8
Wdbc
569
56
28
30

9.5
Experiments
191
0
50
100
150
180
0
0.2
0.4
0.6
0.8
1
Number of Iterations
Relative Error
0.1 0.2
0.4
0.6
0.8
1
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Value of Parameter
AUC
α, when β = 0.4 
β, when α =0.4
(b)
(a)
Fig. 9.3 (a) Convergence curve of our algorithm on Iris dataset. (b) AUC of our approach on Pima
dataset by varying the values of ˛ and ˇ
Table 9.3 Average AUC values (˙ standard deviations) on seven UCI datasets with only Type-I
outliers
Single-view methods
Multi-view methods
Datasets
OP
DRMF
LRR
HOAD
AP
Ours
Iris
0:42 ˙ 0:08 0:46 ˙ 0:07 0:50 ˙ 0:08 0:83 ˙ 0:06 0.96 ˙ 0.03
0:84 ˙ 0:02
Letter
0:39 ˙ 0:05 0:43 ˙ 0:03 0:49 ˙ 0:02 0:53 ˙ 0:04 0:85 ˙ 0:01 0.88 ˙ 0.02
Ionosphere 0:41 ˙ 0:06 0:42 ˙ 0:03 0:46 ˙ 0:05 0:50 ˙ 0:06 0.94 ˙ 0.03
0:87 ˙ 0:03
Zoo
0:49 ˙ 0:08 0:53 ˙ 0:08 0:60 ˙ 0:09 0:55 ˙ 0:10 0.91 ˙ 0.05
0:90 ˙ 0:05
Waveform
0:40 ˙ 0:04 0:45 ˙ 0:03 0:51 ˙ 0:03 0:75 ˙ 0:04 0:62 ˙ 0:02 0.77 ˙ 0.02
Pima
0:45 ˙ 0:03 0:48 ˙ 0:04 0:51 ˙ 0:04 0:56 ˙ 0:03 0:67 ˙ 0:04 0.74 ˙ 0.03
Wdbc
0:47 ˙ 0:04 0:50 ˙ 0:05 0:54 ˙ 0:04 0:45 ˙ 0:06 0:92 ˙ 0:03 0.93 ˙ 0.01
Table 9.3 reports the average area under ROC curve (AUC) values (with standard
deviations) on seven datasets with Type-I outliers. From Table 9.3, we have the
following observations. First, the results of single-view method like LRR are much
lower than the multi-view methods. Secondly, the multi-view method AP performs
better than single-view methods and HOAD in most cases, and it achieves the best
results on the Iris, Ionosphere and Zoo datasets. Thirdly, our approach outperforms
the other compared methods on four datasets, and also obtains competitive results on
the Zoo dataset. In all, it shows that AP and our approach work very well in detecting
the Type-I outliers, and our approach obtains the best results in most cases.
Figure 9.4 shows the detailed ROC curves of compared methods on UCI datasets.
It shows that our approach obtains the best performance in most cases. Table 9.4
shows the average AUC values on seven datasets with both Type-I and Type-II
outliers. We can observe from Table 9.4 that our approach signiﬁcantly outperforms
other competitors in all the cases. In addition, AP still performs better than HOAD
in most datasets except waveform. The results demonstrate that our approach can
detect two types of outliers simultaneously.

192
9
Robust Representations for Outlier Detection
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
False Positive Rate
True Positive Rate
DRMF
LRR
HOAD
AP
Ours
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
False Positive Rate
True Positive Rate
DRMF
LRR
HOAD
AP
Ours
(b)
(a)
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
False Positive Rate
True Positive Rate
DRMF
LRR
HOAD
AP
Ours
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
False Positive Rate
True Positive Rate
DRMF
LRR
HOAD
AP
Ours
(d)
(c)
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
False Positive Rate
True Positive Rate
DRMF
LRR
HOAD
AP
Ours
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
False Positive Rate
True Positive Rate
DRMF
LRR
HOAD
AP
Ours
(f)
(e)
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
False Positive Rate
True Positive Rate
DRMF
LRR
HOAD
AP
Ours
(g)
Fig. 9.4 ROC curves of outlier detection on seven UCI datasets. (a) Iris. (b) Letter. (c) Waveform.
(d) Zoo. (e) Ionosphere. (f) Pima. (g) Wdbc

9.5
Experiments
193
Table 9.4 Average AUC values (˙ standard deviations) on seven UCI datasets with Type-I and
Type-II outliers
Single-view methods
Multi-view methods
Datasets
OP
DRMF
LRR
HOAD
AP
Ours
Iris
0:36 ˙ 0:05 0:38 ˙ 0:04 0:39 ˙ 0:06 0:37 ˙ 0:04 0:70 ˙ 0:02 0.84 ˙ 0.05
Letter
0:32 ˙ 0:02 0:34 ˙ 0:02 0:34 ˙ 0:01 0:34 ˙ 0:01 0:67 ˙ 0:01 0.78 ˙ 0.01
Ionosphere 0:39 ˙ 0:03 0:46 ˙ 0:03 0:43 ˙ 0:04 0:50 ˙ 0:05 0:76 ˙ 0:02 0.79 ˙ 0.03
Zoo
0:35 ˙ 0:06 0:37 ˙ 0:04 0:41 ˙ 0:06 0:58 ˙ 0:07 0:77 ˙ 0:07 0.85 ˙ 0.04
Waveform
0:40 ˙ 0:02 0:43 ˙ 0:03 0:42 ˙ 0:02 0:77 ˙ 0:03 0:42 ˙ 0:01 0.83 ˙ 0.02
Pima
0:32 ˙ 0:03 0:35 ˙ 0:02 0:34 ˙ 0:02 0:37 ˙ 0:02 0:46 ˙ 0:02 0.77 ˙ 0.03
Wdbc
0:31 ˙ 0:02 0:33 ˙ 0:02 0:33 ˙ 0:03 0:33 ˙ 0:07 0:48 ˙ 0:03 0.79 ˙ 0.01
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
False Positive Rate
True Positive Rate
OP
DRMF
LRR
HOAD
AP
Ours
Fig. 9.5 ROC Curves of all compared methods on 2-view USPS dataset
9.5.2.2
USPS Digit Dataset
We construct a two-view dataset by using the USPS dataset [18], which contains
9298 handwritten digit images. We extract two types of features from each image as
two data views, including pixel values and Fourier coefﬁcients.
In the experiments, we randomly select 50 images per digit from each dataset.
Thus, there are 500 samples in each view. We employed the same strategies as in the
UCI datasets to generate 5% Type-I outliers and 5% Type-II outliers. This process
was repeated 20 times, and we evaluated the performance of each method on these
20 sample sets.
Figure 9.5 shows the ROC curves, and Table 9.5 lists the average AUC values
with standard deviations. For single-view outlier detection methods OP, DRMF and

194
9
Robust Representations for Outlier Detection
Table 9.5 Average AUC
values with standard
deviations of compared
methods on 2-view USPS
dataset
Method
AUC (˙ standard deviation)
OP [63]
0:4892 ˙ 0:0746
DRMF [60]
0:6412 ˙ 0:0673
LRR [41]
0:5960 ˙ 0:0461
HOAD [15]
0:5193 ˙ 0:0429
AP [1]
0:6745 ˙ 0:0848
Ours
0.7381 ˙ 0.0702
Table 9.6 Average AUC
values with standard
deviations of compared
methods on WebKB dataset
Method
AUC (˙ standard deviation)
OP [63]
0:4219 ˙ 0:0611
DRMF [60]
0:4624 ˙ 0:0603
LRR [41]
0:4805 ˙ 0:0530
HOAD [15]
0:5027 ˙ 0:0643
AP [1]
0:4965 ˙ 0:0655
Ours
0.5532 ˙ 0.0475
LRR, we simply concatenate the features from two views together as inputs. From
Fig. 9.5 and Table 9.5, we observe that the single-view methods DRMF and LRR
attain even better performance than the multi-view method HOAD. Since DRMF
and LRR are low-rank based methods, they are capable of detecting the Type-II
outliers. Moreover, as our approach can detect two types of outliers effectively, it
outperforms all of the single-view and multi-view outlier detection baselines.
9.5.3
Real-World Multi-view Data with Synthetic Outliers
The WebKB dataset [6] has been widely used for evaluating multi-view learning
algorithms [16, 34]. It contains webpages collected from four universities, including
Cornell, Texas, Washington, and Wisconsin. The webpages can be categorized into
ﬁve classes: student, course, project, faculty, and staff. Each webpage is described by
two views, the content view and the citation view. In the content view, each webpage
is represented by a word vector of length 1703. The citation view characterizes the
number of citation links across pages. In our experiments, we use the Cornell subset
which contains 195 webpages. We follow the procedures described in Sect. 9.5.2
to generate two types of outliers, and evaluate the performance of our approach
and baselines. Table 9.6 shows the average AUC with standard deviations of all
compared methods on the WebKB dataset. In addition, the improvement of our
approach is not very signiﬁcant.We can observe that the true positive rate of all
compared methods are very low, as it is a challenging task in real world. In addition,
the improvement of our approach is not very signiﬁcant. Clearly, our MLRA
approach achieves higher AUC than its competitors.

9.5
Experiments
195
Table 9.7 Movies with high and low outlier scores calculated by the proposed approach
Movie title
Score
Movie title
Score
Quiz show
0.98
Wings of courage
0.15
Dumb & dumber
0.96
Balto
0.13
Forget paris
0.95
GoldenEye
0.09
While you were sleeping
0.95
Jumanji
0.07
Speed
0.93
Toy story
0.02
9.5.4
Real-World Multi-view Data with Real Outliers
We employ the popular MovieLens-1M dataset, which contains one million ratings
for 3883 movies by 6040 users. We consider the movies as samples, and exploit
two perspectives of movies: (1) Genre. There are 18 genres in this dataset, such as
Action, Comedy, and Horror. Each movie was classiﬁed as one or more of these
18 genres, which can be converted as a binary vector. (2) User’s feedback. Each
movie was rated by one or more users, which can also be represented as binary
vectors (across all users). As the ground truth information of outliers in this real-
world dataset are unknown, we mainly perform qualitative analysis to show the
performance of our approach in detecting outliers.
We sample 500 movies and 600 users from the dataset, and perform our MLRA
approach to assign an outlier score to each movie. Table 9.7 shows some movies
with high outlier scores and low outlier scores. The movie “Quiz Show” belongs
to the “Drama” genre. It was considered as an outlier as it receives much more
ratings than other movies in the “Drama” genre. In other words, this movie exhibits
inconsistent behavior in the genre view and the rating view. On the other hand,
the movies “Toy Story” and “Jumanji” are categorized to three different genres, and
they share the same genre of “Children’s”. Meanwhile, both of them received a large
number of ratings, as many other movies belonging to the same genre. Therefore,
they have very low outlier scores, and can be labeled as normal samples. In a word,
the qualitative analysis on the MovieLens-1M shows that our approach is able to
produce meaningful outlier detection results on real-world data.
9.5.5
Group Outlier Detection
In this experiment we test the performance of our method on detecting group
outliers. We use the seven UCI datasets and the USPS digit dataset in the
experiments. First, as described in previous sections, the feature vectors of each
dataset are divided into two subsets to construct the multi-view dataset. Each
subset is considered as one view of the data. In order to generate an outlier group,

196
9
Robust Representations for Outlier Detection
we randomly select a class and a view, remove the data in this class and the
speciﬁc view, and ﬁll in random vectors drawn from a multivariate standard normal
distribution. We generate one outlier group for each dataset. In this way, we can
ensure that the outlier group has a cluster structure, but they are quite different from
the normal samples.
As multi-view methods usually perform better than the single-view ones, we
compare our MLRA framework with two multi-view outlier detection methods
HOAD and AP in this section. In addition to the group outlier detection model
named as MLRA-Group, we also evaluate the performance of our individual-level
outlier detection model (as described in Sect. 9.3), which is named as MLRA-
Individual. Figure 9.6 shows the ROC curves on seven UCI datasets, and Table 9.8
shows the average AUC results. We can observe that: (1) AP performs better than
HOAD on the Iris, Letter and Ionosphere datasets, but HOAD performs much better
than AP on the rest datasets; (2) Both of our MLRA-Individual and MLRA-Group
methods outperform HOAD and AP on all the datasets. As MLRA-Individual does
not consider the group prior information, it obtains lower AUC values than MLRA-
Group. In particular, the AUC of MLRA-Group is quite close to 1.0 on the Letter,
Pima and Wdbc datasets, which demonstrates the strength of our framework.4
9.5.6
Discussions
We evaluate the computational cost of different methods on the Letter dataset. The
machine used in our experiments installs 24 GB RAM and Intel Xeon W3350 CPU.
Figure 9.7 shows the average running time over 50 runs of each compared method.
We can observe that AP took much more computing time than other methods, due
to its afﬁnity propagation procedure. LRR, HOAD and our approach have similar
computational costs, as they are all matrix factorization based methods with similar
time complexities.
To perform an in-depth analysis of the outlier detection results, Fig. 9.8 shows
the number of detected outliers for each type on the USPS-MNIST dataset, when
the false positive rate is equal to 0.8. It shows that two multi-view methods, HOAD
and AP, are only capable of detecting Type-I outliers. However, our approach is able
to detect two types of outliers effectively.
9.6
Summary
We have proposed a multi-view low-rank analysis (MLRA) framework in this
chapter for outlier detection. Our framework performed cross-view low-rank anal-
ysis, and employed a well designed criterion to calculate the outlier score for
4AUC D 1:0 implies that the outlier detector is perfect.

9.6
Summary
197
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
False Positive Rate
True Positive Rate
HOAD
AP
Ours
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
False Positive Rate
True Positive Rate
HOAD
AP
Ours
(b)
(a)
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
False Positive Rate
True Positive Rate
HOAD
AP
Ours
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
False Positive Rate
True Positive Rate
HOAD
AP
Ours
(d)
(c)
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
False Positive Rate
True Positive Rate
HOAD
AP
Ours
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
False Positive Rate
True Positive Rate
HOAD
AP
Ours
(f)
(e)
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
False Positive Rate
True Positive Rate
HOAD
AP
Ours
(g)
Fig. 9.6 ROC curves of group outlier detection on seven UCI datasets. (a) Iris. (b) Letter. (c)
Waveform. (d) Zoo. (e) Ionosphere. (f) Pima. (g) Wdbc

198
9
Robust Representations for Outlier Detection
Table 9.8 Average AUC values on UCI and USPS datasets with group outliers
Datasets
HOAD
AP
MLRA-individual
MLRA-group
Iris
0.5324
0.5720
0.6832
0.7372
Letter
0.6749
0.7247
0.7582
0.9963
Ionosphere
0.7563
0.9127
0.9235
0.9606
Zoo
0.8400
0.5271
0.8522
0.9047
Waveform
0.7568
0.3979
0.7629
0.7846
Pima
0.8274
0.5231
0.8732
0.9934
Wdbc
0.8864
0.4096
0.9129
0.9870
USPS
0.5422
0.7653
0.9015
0.9244
Fig. 9.7 CPU time (seconds)
of all compared methods on
UCI-Letter dataset
LRR
HOAD
AP
Ours
0
5
10
15
20
25
30
CPU Time (Seconds)
Fig. 9.8 Number of detected
outliers (two types) when
FPR D 0:8 on USPS-MINST
dataset
0
10
20
30
40
50
Ours
AP
HOAD
LRR-v2
LRR-v1
Type-I
Type-II
Number of outliers
each sample. We formulated it as a rank-minimization problem, and adopted the
Inexact ALM algorithm to solve it. By analyzing the representation coefﬁcients in
different views, our framework was able to detect two different types of outliers
simultaneously. moreover, MLRA has been extended to multi-view group outlier
detection. Experimental results on seven UCI datasets, USPS-MNIST, MovieLens,
and WebKB datasets showed that the proposed approach outperforms the state-of-

References
199
the-art single-view and multi-view outlier detection methods under various settings.
Especially when the datasets contain both Type-I and Type-II outliers, our approach
can signiﬁcantly boost the performance of outlier detection.
In our future work, we will apply MLRA framework to more outlier detection
applications, and we would also like to develop a divide-and-conquer version of
MLRA to make it more suitable for large scale datasets and further improve its
performance.
References
1. Alvarez, A.M., Yamada, M., Kimura, A., Iwata, T.: Clustering-based anomaly detection in
multi-view data. In: CIKM, pp. 1545–1548 (2013)
2. Angiulli, F., Fassetti, F.: Outlier detection using inductive logic programming. In: ICDM,
pp. 693–698 (2009)
3. Assent, I., Dang, X.H., Micenková, B., Ng, R.T.: Outlier detection with space transformation
and spectral analysis. In: SDM, pp. 225–233 (2013)
4. Bach, F.R.: Consistency of trace norm minimization. J. Mach. Learn. Res. 9, 1019–1048 (2008)
5. Lichman, M.: UCI Machine Learning Repository. School of Information and Computer
Sciences, University of California, Irvine (2013). http://archive.ics.uci.edu/ml
6. Blum, A., Mitchell, T.: Combining labeled and unlabeled data with co-training. In: COLT,
pp. 92–100. ACM (1998)
7. Blum, A., Mitchell, T.M.: Combining labeled and unlabeled data with co-training. In: COLT,
pp. 92–100 (1998)
8. Cai, J.F., Candes, E.J., Shen, Z.W.: A singular value thresholding algorithm for matrix
completion. SIAM J. Optim. 20(4), 1956–1982 (2010)
9. Candès, E.J., Li, X.D., Ma, Y., Wright, J.: Robust principal component analysis? J. ACM 58(3),
11 (2011)
10. Chen, J. Zhou, J., Ye, J.: Integrating low-rank and group-sparse structures for robust multi-task
learning. In: KDD, pp. 42–50 (2011)
11. Cheng, B., Liu, G., Wang, J., Huang, Z., Yan, S.: Multi-task low-rank afﬁnity pursuit for image
segmentation. In: ICCV, pp. 2439–2446 (2011)
12. Das, S., Matthews, B.L., Srivastava, A.N., Oza, N.C.: Multiple kernel learning for heteroge-
neous anomaly detection: algorithm and aviation safety case study. In: KDD, pp. 47–56 (2010)
13. Du, B., Zhang, L.: A discriminative metric learning based anomaly detection method. IEEE
Trans. Geosci. Remote Sens. 52(11), 6844–6857 (2014)
14. Emmott, A.F., Das, S., Dietterich, T., Fern, A., Wong, W.-K.: Systematic construction of
anomaly detection benchmarks from real data. In: KDD Workshop on Outlier Detection and
Description, pp. 16–21 (2013)
15. Gao, J., Fan, W., Turaga, D.S., Parthasarathy, S., Han, J.: A spectral framework for detecting
inconsistency across multi-source object relationships. In: ICDM, pp. 1050–1055 (2011)
16. Guo, Y.: Convex subspace representation learning from multi-view data. In: AAAI, vol. 1, p. 2
(2013)
17. Hsiao, K.-J., Xu, K.S., Calder, J., Hero III, A.O.: Multi-criteria anomaly detection using Pareto
depth analysis. In: NIPS, pp. 854–862 (2012)
18. Hull, J.: A database for handwritten text recognition research. IEEE Trans. Pattern Anal. Mach.
Intell. 16(5), 550–554 (1994)
19. Janeja, V.P., Palanisamy, R.: Multi-domain anomaly detection in spatial datasets. Knowl. Inf.
Syst. 36(3), 749–788 (2013)

200
9
Robust Representations for Outlier Detection
20. Keshavan, R.H., Montanari, A., Oh, S.: Matrix completion from noisy entries. In: NIPS,
pp. 952–960 (2009)
21. LeCun, Y., Bottou, L., Bengio, Y., Haaffner, P.: Gradient-based learning applied to document
recognition. Proc. IEEE 86(11), 2278–2324 (1998)
22. Lee, Y., Yeh, Y., Wang, Y.F.: Anomaly detection via online oversampling principal component
analysis. IEEE Trans. Knowl. Data Eng. 25(7), 1460–1470 (2013)
23. Li, L., Li, S., Fu, Y.: Learning low-rank and discriminative dictionary for image classiﬁcation.
Image Vis. Comput. 32(10), 814–823 (2014)
24. Li, S., Fu, Y.: Low-rank coding with b-matching constraint for semi-supervised classiﬁcation.
In: IJCAI, pp. 1472–1478 (2013)
25. Li, S., Fu, Y.: Robust subspace discovery through supervised low-rank constraints. In: SDM,
pp. 163–171 (2014)
26. Li, S., Fu, Y.: Learning balanced and unbalanced graphs via low-rank coding. IEEE Trans.
Knowl. Data Eng. 27(5), 1274–1287 (2015)
27. Li, S., Fu, Y.: Learning robust and discriminative subspace with low-rank constraints. IEEE
Trans. Neural Netw. Learn. Syst. 27(11), 2160–2173 (2016)
28. Li, S., Fu, Y.: Unsupervised transfer learning via low-rank coding for image clustering. In:
International Joint Conference on Neural Networks, pp. 1795–1802. IEEE (2016)
29. Li, S., Li, K., Fu, Y.: Self-taught low-rank coding for visual learning. IEEE Trans. Neural Netw.
Learn. Syst. (2017)
30. Li, S., Li, Y., Fu, Y.: Multi-view time series classiﬁcation: a discriminative bilinear projection
approach. In: Proceedings of the 25th ACM International on Conference on Information and
Knowledge Management, pp. 989–998. ACM (2016)
31. Li, S., Shao, M., Fu, Y.: Locality linear ﬁtting one-class SVM with low-rank constraints for
outlier detection. In: IJCNN, pp. 676–683 (2014)
32. Li, S., Shao, M., Fu, Y.: Cross-view projective dictionary learning for person re-identiﬁcation.
In: Proceedings of the 24th International Conference on Artiﬁcial Intelligence, pp. 2155–2161
(2015)
33. Li, S., Shao, M., Fu, Y.: Multi-view low-rank analysis for outlier detection. In: Proceedings of
the SIAM International Conference on Data Mining, pp. 748–756. SIAM (2015)
34. Li, S.-Y., Jiang, Y., Zhou, Z.-H.: Partial multi-view clustering. In: AAAI, pp. 1968–1974.
Citeseer (2014)
35. Lin, Z.C., Chen, M.M., Wu, L.Q., Ma, Y.: The augmented lagrange multiplier method for exact
recovery of corrupted low-rank matrices. Technique Report, UIUC (2009)
36. Liu, A., Lam, D.N.: Using consensus clustering for multi-view anomaly detection. In: IEEE
Symposium on Security and Privacy Workshops, pp. 117–124 (2012)
37. Liu, B., Xiao, Y., Cao, L., Hao, Z., Deng, F.: SVDD-based outlier detection on uncertain data.
Knowl. Inf. Syst. 34(3), 597–618 (2013)
38. Liu, B., Xiao, Y., Yu, P.S., Hao, Z., Cao, L.: An efﬁcient approach for outlier detection with
imperfect data labels. IEEE Trans. Knowl. Data Eng. 26(7), 1602–1616 (2014)
39. Liu, F.T., Ting, K.M., Zhou, Z.: Isolation-based anomaly detection. TKDD 6(1), 3 (2012)
40. Liu, G., Lin, Z., Yan, S., Sun, J., Yu, Y., Ma, Y.: Robust recovery of subspace structures by
low-rank representation. IEEE Trans. Pattern Anal. Mach. Intell. 35(1), 171–184 (2013)
41. Liu, G., Xu, H., Tang, J., Liu, Q., Yan, S.: A deterministic analysis for LRR. IEEE Trans.
Pattern Anal. Mach. Intell. 38(3), 417–430 (2016)
42. Liu, G., Xu, H., Yan, S.: Exact subspace segmentation and outlier detection by low-rank
representation. In: AISTATS, pp. 703–711 (2012)
43. Liu, G.C., Lin, Z.C., Yu, Y.: Robust subspace segmentation by low-rank representation. In:
ICML, pp. 663–670 (2010)
44. Memisevic, R.: On multi-view feature learning. In: ICML (2012)
45. Muandet, K., Schölkopf, B.: One-class support measure machines for group anomaly detection.
In: UAI (2013)
46. Müller, E., Assent, I., Sanchez, P.I., Mülle, Y., Böhm, K.: Outlier ranking via subspace analysis
in multiple views of the data. In: ICDM, pp. 529–538 (2012)

References
201
47. O’Reilly, C., Gluhak, A., Imran, M.A.: Adaptive anomaly detection with kernel eigenspace
splitting and merging. IEEE Trans. Knowl. Data Eng. 27(1), 3–16 (2015)
48. Pei, Y., R. Zaïane, O., Gao, Y.: An efﬁcient reference-based approach to outlier detection in
large datasets. In: ICDM, pp. 478–487 (2006)
49. Perozzi, B., Akoglu, L., Sanchez, P.I., Müller, E.: Focused clustering and outlier detection in
large attributed graphs. In: KDD, pp. 1346–1355 (2014)
50. Pham, N., Pagh, R.: A near-linear time approximation algorithm for angle-based outlier
detection in high-dimensional data. In: KDD, pp. 877–885 (2012)
51. Schubert, E., Zimek, A., Kriegel, H.: Generalized outlier detection with ﬂexible kernel density
estimates. In: SDM, pp. 542–550 (2014)
52. Shao, M., Kit, D., Fu, Y.: Generalized transfer subspace learning through low-rank constraint.
Int. J. Comput. Vis. 109(1–2), 74–93 (2014)
53. Sindhwani, V., Rosenberg, D.S.: An RKHS for multi-view learning and manifold co-
regularization. In: ICML, pp. 976–983 (2008)
54. Sridharan, K., Kakade, S.M.: An information theoretic framework for multi-view learning. In:
COLT, pp. 403–414 (2008)
55. Tao, Z., Liu, H., Li, S., Fu, Y.: Robust spectral ensemble clustering. In: Proceedings of the 25th
ACM International on Conference on Information and Knowledge Management, pp. 367–376.
ACM (2016)
56. Tong, H., Lin, C.: Non-negative residual matrix factorization with application to graph anomaly
detection. In: SDM, pp. 143–153 (2011)
57. Tzortzis, G., Likas, A.: Kernel-based weighted multi-view clustering. In: ICDM, pp. 675–684
(2012)
58. White, M., Yu, Y., Zhang, X., Schuurmans, D.: Convex multi-view subspace learning. In: NIPS,
pp. 1682–1690 (2012)
59. Wu, S., Wang, S.: Information-theoretic outlier detection for large-scale categorical data. IEEE
Trans. Knowl. Data Eng. 25(3), 589–602 (2013)
60. Xiong, L., Chen, X., Schneider, J.: Direct robust matrix factorization for anomaly detection.
In: ICDM, pp. 844–853. IEEE (2011)
61. Xiong, L., Póczos, B., Schneider, J.G.: Group anomaly detection using ﬂexible genre models.
In: NIPS, pp. 1071–1079 (2011)
62. Xu, C., Tao, D., Xu, C.: A survey on multi-view learning. CoRR, abs/1304.5634 (2013)
63. Xu, H., Caramanis, C., Sanghavi, S.: Robust PCA via outlier pursuit. In: NIPS, pp. 2496–2504
(2010)
64. Yu, Q.R., He, X., Liu, Y.: GLAD: group anomaly detection in social media analysis. In: KDD,
pp. 372–381 (2014)
65. Zhou, X., Yang, C., Yu, W.: Automatic mitral leaﬂet tracking in echocardiography by outlier
detection in the low-rank representation. In: CVPR, pp. 972–979 (2012)
66. Zimek, A., Gaudet, M., Campello, R.J.G.B., Sander, J.: Subsampling for efﬁcient and effective
unsupervised outlier detection ensembles. In: KDD, pp. 428–436 (2013)

Chapter 10
Robust Representations for Person
Re-identiﬁcation
Abstract Person re-identiﬁcation plays an important role in many safety-critical
applications. Existing works mainly focus on extracting patch-level features or
learning distance metrics. However, the representation power of extracted features
might be limited, due to the various viewing conditions of pedestrian images in
reality. To improve the representation power of features, we learn discriminative
and robust representations via dictionary learning in this chapter. First, we propose
a cross-view projective dictionary learning (CPDL) approach, which learns effective
features for persons across different views. CPDL is a general framework for multi-
view dictionary learning. Secondly, by utilizing the CPDL framework, we design
two objectives to learn low-dimensional representations for each pedestrian in the
patch-level and the image-level, respectively. The proposed objectives can capture
the intrinsic relationships of different representation coefﬁcients in various settings.
We devise efﬁcient optimization algorithms to solve the objectives. Finally, a fusion
strategy is utilized to generate the similarity scores. Experiments on the public
VIPeR, CUHK Campus and GRID datasets show that our approach achieves the
state-of-the-art performance.
10.1
Overview1
Person re-identiﬁcation is the problem of matching pedestrian images observed from
multiple non-overlapping cameras. It saves a lot of human efforts in many safety-
critical applications such as video surveillance. In recent years, many algorithms
have been proposed to tackle this problem [18, 28, 34, 39]. The representative person
re-identiﬁcation methods mainly include the distance learning/metric learning
methods [3, 19, 25, 27, 29, 31, 38], feature learning methods [2, 4, 7, 24, 30, 34].
The distance learning methods aim to learn distance metrics that are expected to
be robust to sample variations [40]. For instance, a logistic metric learning approach
with positive semideﬁnite constraint is proposed to separate the positive sample
pairs from the negative ones [20]. Other effective distance learning methods include
1This chapter is reprinted with permission from IJCAI. “Cross-View Projective Dictionary
Learning for Person Re-Identiﬁcation”, International Joint Conference on Artiﬁcial Intelligence,
2015.
© Springer International Publishing AG 2017
S. Li, Y. Fu, Robust Representation for Data Analytics, Advanced Information
and Knowledge Processing, DOI 10.1007/978-3-319-60176-2_10
203

204
10
Robust Representations for Person Re-identiﬁcation
the Probabilistic Relative Distance Comparison (PRDC) [38], Keep It Simple and
Straightforward Metric Learning (KISSME) [10], etc. The feature learning methods
extract discriminative features from pedestrian images, and then perform matching
in the feature space. Some effective features include salient features [34], mid-level
features [36], salient color features [32], polynomial kernel feature map [2], etc. The
advanced distance learning and feature learning methods have achieved promising
performance on person re-identiﬁcation. However, the representation power of the
learned features or metrics might be limited, due to the various viewing conditions
of pedestrian images in complex real-world scenarios (e.g., illumination changes
and occlusions).
In this chapter, we learn discriminative and robust representations via dictionary
learning to improve the representation power of features. Our motivations are
two-folds. First, dictionary learning is a powerful technique to extract effective
and discriminative features from high-dimensional images, and it has shown
impressive performance in many vision tasks [13, 14], such as face recognition [33],
which motivates us to design novel dictionary learning methods for person re-
identiﬁcation. Moreover, the success of dictionary learning based domain adaptation
inspires us to learn a pair of cross-view dictionaries jointly [26]. The adaptively
learned pairs of dictionaries can generate robust representations for pedestrian
images. Second, existing works either focus on extracting features form image
patches or directly learning global features. However, the complementary infor-
mation resided in patch-level and image-level are usually ignored. We argue that
extracting features from a single level is not sufﬁcient, and it is necessary to design
multi-level models , in order to make use of the complementary information.
Based on the motivations above, we propose a cross-view projective dictionary
learning (CPDL) approach, which is a general framework for the multi-view
dictionary learning problem [16]. We then design two objectives by utilizing the
CPDL framework, which learn low-dimensional representations for each person
in the patch-level and the image-level, respectively. Different from traditional
dictionary learning methods, CPDL adopts the projective learning strategy to avoid
solving the l1 optimization problem in training phase. The proposed objectives can
capture the intrinsic relationships of different representation coefﬁcients in various
settings. We also employ a strategy to fuse the similarity scores estimated in two
levels.
By far, there are few methods proposed to learn effective representations for the
pedestrian images under different views [21]. The basic idea of Liu’s method is
to learn expressive bases to represent the image patches. It assumes that each pair
of patches in two images shares the same representation coefﬁcients. However, it
is not the case in reality, due to the common misalignment problem in person re-
identiﬁcation.
The major contributions of this work are summarized below.
•
We propose a general framework, CPDL, for multi-view dictionary learning,
and apply it to person re-identiﬁcation. CPDL adopts the projective dictionary
learning strategy, which is more efﬁcient than the traditional dictionary learning
methods. We devise efﬁcient optimization algorithms to solve the model.

10.3
Cross-View Projective Dictionary Learning (CPDL)
205
•
We design two objectives using CPDL, which explicitly model the cross-view
interactions in different representation levels, including the patch-level and
image-level. To the best of our knowledge, this work is the ﬁrst attempt to learn
representations at different levels for person re-identiﬁcation.
•
We evaluate the performance of CPDL and related methods on the public VIPeR,
CUHK Campus, and GRID datasets. Extensive experimental results show that
our approach outperforms the state-of-the-art methods.
10.2
Person Re-identiﬁcation
In recent years, many algorithms have been proposed for person re-identiﬁcation.
Some traditional methods focus on learning effective metrics to measure the
similarity between two images captured from different camera views [9, 38]. Other
research works focus on learning expressive features, which usually obtain better
performance that the metric learning methods. They suggest that learning effective
representations is the key in person re-identiﬁcation. Some advanced features
include attributes [11], salience features [34, 35], mid-level features [36], and
salient color features [32]. Although the existing feature learning methods achieve
good performance, the cross-view relationships of pedestrian images haven’t been
extensively studied. Our CPDL approach explicitly models such relationships in
different representation levels, and draws strength from them to enhance the re-
identiﬁcation performance.
As a powerful technique for learning expressive bases in sample space, dictionary
learning has attracted lots of attention during the past decades [12]. Some popular
dictionary learning methods include K-SVD [1], discriminative K-SVD [33], and
projective dictionary pair learning [8]. Most recently, Liu et al. presented a semi-
supervised coupled dictionary learning (SSCDL) method [21], and applied it to
person re-identiﬁcation. The major differences between our approach and SSCDL
are three-folds. First, SSCDL is a semi-supervised method, while our approach
is supervised. Secondly, SSCDL simply assumes that a pair of patches in two
views should have similar codings, which is unreasonable in real scenario due to
the misalignment problem. Our approach models the cross-view interactions in
image-level and patch-level, respectively. Thirdly, SSCDL requires solving the l1
optimization problem that is time consuming. Our approach adopts a more efﬁcient
learning strategy, i.e., projective dictionary learning.
10.3
Cross-View Projective Dictionary Learning (CPDL)
10.3.1
Motivation
Dictionary learning aims to learn expressive feature representations for data, and has
been widely applied in many visual learning tasks. Given a set of samples X 2 Rdn,

206
10
Robust Representations for Person Re-identiﬁcation
traditional dictionary learning methods usually assume that X can be reconstructed
by using sparse coefﬁcients Z 2 Rmn and a dictionary D 2 Rdm:
A  DZ;
(10.1)
where Z is usually constrained by l1 norm that minimizes the sum of entries in Z.
Existing dictionary learning methods based on Eq. (10.1) have shown promising
performance in many applications like image classiﬁcation, but there are still some
drawbacks. First, solving sparse coefﬁcients Z in Eq. (10.1) is computationally
expensive due to the l1 norm constraint, which limits the applicability of dictionary
learning for large-scale problems. Second, traditional dictionary learning methods
mainly focus on single-view data, and therefore they cannot directly handle the
multi-view visual data. Nowadays data can be collected from multiple views [15,
17], and it is of great importance to develop multi-view dictionary learning methods.
As we mainly focus on the person re-identiﬁcation that is essentially a two-view
problem, we only consider the two-view setting in this chapter.
We aim to tackle the above problems by designing an efﬁcient cross-view
dictionary learning (CDL) model. Inspired by the idea of projective dictionary learn-
ing [8], we reduce the computational cost of dictionary learning by reformulating
the approximation in Eq. (10.1) as a linear encoding and reconstruction process. Let
P 2 Rmd .m 	 d/ denote a low-dimensional projection matrix, Eq. (10.1) can be
reformulated as A  DPA. Notice that PA denotes the linear encodings of sample
set A. Moreover, we will consider the dictionary learning process in different views,
and will model the view-consistency in our solution.
10.3.2
Formulation of CPDL
We build a cross-view projective dictionary learning (CPDL) framework in the two-
view settings. Let A1 2 Rd1n and A2 2 Rd2n denote two training sets that are
collected under two different views, respectively. The reconstructions in two views
are formulated as
A1 D D1P1A1;
A2 D D2P2A2;
(10.2)
where D1 (and D2), P1 (and P2) are dictionaries and projections in two views,
respectively.
The objective function of CPDL framework is
min
D1;D2;P1;P2 kA1  D1P1A1k2
F C kA2  D2P2A2k2
F
C1f.D1; D2; P1; P2/
s:t:
d1.W;i/
  1;
d2.W;i/
  1:
(10.3)

10.4
CPDL for Person Re-identiﬁcation
207
where f.D1; D2; P1; P2/ is a regularization function, 1 is a trade-off parameter, and
d1.W;i/ and d2.W;i/ are the i-th columns in D1 and D2, respectively.
The ﬁrst two terms in objective (10.3) indicate reconstruction errors in two views,
respectively. The last term f.D1; D2; P1; P2/ is a regularization function that bridges
two views. It can be customized for speciﬁc problems, such as multi-view image
classiﬁcation or (cross-view) person re-identiﬁcation.
Finally, the obtained optimal dictionary pair fD1; D2g can be used to generate
new representations for test samples. Note that, for simplicity, we only formulate
two views in this chapter, but our model can be extended to the multiple-view case
by extending (10.3).
The
regularization
function
f.D.1/; D.2/; P.1/; P.2//
in
Eq. (10.3)
can
be
customized for speciﬁc problems, such as image classiﬁcation or person re-
identiﬁcation. The function would be able to model the consistency of latent
representations (or dictionaries) in two views. In the next section, we will propose to
learn dictionaries in three different representation levels for person re-identiﬁcation,
and design regularization functions for each of them.
Compared to the existing multi-view or multi-modal dictionary learning meth-
ods, our CPDL model provides a more ﬂexible way to learn dictionaries from two
data views. The dictionary learning methods proposed in [26, 37] are special cases
of our CPDL model.
10.4
CPDL for Person Re-identiﬁcation
In this section, we ﬁrst introduce how to extract low-level dense features from
the pedestrian images. Then we formulate person re-identiﬁcation problem using
CPDL. Figure 10.1 shows the training framework of CPDL.
10.4.1
Feature Extraction
The pedestrian images in different camera views are not usually aligned well.
Extracting dense features from local patches is a widely used strategy to obtain
effective representations, as suggested in [36]. Speciﬁcally, the local patches are
extracted on a dense grid. The size of each patch is 10  10, and the grid step is 5.
Then, for each patch, we extract 32-dimensional color histogram features and 128-
dimensional dense SIFT features in each LAB channel. Further, we also calculate
the color histograms in different sampling scales with the downsampling factors 0.5
and 0.75. All the features of one patch are normalized with l2 norm. Finally, each
patch is represented by a 672-dimensional feature vector.

208
10
Robust Representations for Person Re-identiﬁcation
View 1
View 2
Image-Level
Patch-Level
X1
X2
Y1
Y2
D1
H
P1
H
X1
D2
H
P2
H
X2
D1
L
P1L
Y1
D2
L
P2L
Y2
Similar Dictionary
Similar Codings
D1
H, D1
L: dictionary
P1
HX1, P1
LY1: codings
Fig. 10.1 Training framework of CPDL. The solid boxes represent the variables related to view
1, while the dashed boxes represent the variables related to view 2. In the image-level training
(blue color), two views share the similar codings (i.e., PH
1 X1, PH
2 X2); in the patch-level training
(red color), two views share the similar dictionary (i.e., DL
1, DL
2)
10.4.2
CPDL for Image Representation
Our goal is to learn robust representations for each pedestrian in different camera
views by virtue of dictionary learning. It’s a challenging problem as the same person
under different camera views usually exhibits signiﬁcant differences in appearance.
In this section, we propose to emphasize the feature learning in two levels, patch
level and image level, in order to capture both local and global characteristics
from the pedestrian images. Note that most existing methods only consider feature
learning in one single level [21].
The major beneﬁts of adopting two different levels of representation are two-
folds. First, three representation levels provide informative cues of pedestrian
images at different scales, and capture both local and global characteristics of the
pedestrian images, which will be useful for information fusion. Second, two levels
of representations can jointly address the issues like misalignment and variations.
As patch-level matching is robust to misalignment and pose changes, we aim to
extract effective patch-level representations. The image-level representation ensures
the exact correspondence across views.
Let X1 and X2 denote the training sets of high-dimensional dense features in
two views, respectively. For the i-th training image in view 1, the dense features

10.4
CPDL for Person Re-identiﬁcation
209
of all the patches are concatenated as a high-dimensional vector,2 which is the i-
th column in X1. Clearly, the corresponding columns in X1 and X2 should have
similar codings, since they represent the same pedestrian. Hence, by deﬁning the
regularization function f./ in (10.3), we have the following objective
min
DH
1 ;DH
2 ;
PH
1 ;PH
2
X1  DH
1 PH
1 X1
2
F C
X2  DH
2 PH
2 X2
2
F
C1
PH
1 X1  PH
2 X2
2
F ;
s:t:
kdH
1.W;i/k  1; kdH
2.W;i/k  1;
(10.4)
where DH
1 (and DH
2 /, PH
1 (and PH
2 / denote the dictionaries and projection matrices in
two views, respectively.
The regularization function in (10.4) is
PH
1 X1  PH
2 X2
2
F, indicating that the
codings in two views should be as close as possible. In this way, the learned
dictionaries DH
1 and DH
2 are expected to generate similar codings for the same
pedestrian under two camera views.
10.4.3
CPDL for Patch Representation
In addition to modeling the image representation in (10.4), we also consider the
dictionary learning in patch-level representations. Let Y1 and Y2 denote the training
sets of low-dimensional patch features in two views, respectively. In this case, we
cannot simply assume that the codings in two views are close to each other. In
reality, the i-th patch in view 1 may not match the i-th patch in view 2 due to
the misalignment problem under cross-view settings. One reasonable assumption
is that the patches in different views could share a similar dictionary. Therefore, the
objective function is
min
DL
1 ;DL
2 ;PL
1 ;PL
2
Y1  DL
1PL
1Y1
2
F C
Y2  DL
2PL
2Y2
2
F
C2
DL
1  DL
2
2
F ;
s:t:
kdL
1.W;i/k  1; kdL
2.W;i/k  1;
(10.5)
in which the last term emphasizes the similarity of two dictionaries. In this model,
we assume that the patch-level dictionaries in two views are very similar to each
other. In practice, two different images may still share a lot of similar patches.
Thus, it is reasonable to assume that two dictionaries contain similar bases vectors
2As we have high-dimensional image-level features and low-dimensional patch-level features, we
use superscripts H and L for the image-level and patch-level variables, respectively.

210
10
Robust Representations for Person Re-identiﬁcation
in the image patch space. Another reasonable assumption is that two dictionaries are
exactly the same, i.e., DL
1 D DL
2. This constraint will reduce the model complexity.
And we actually observed very good performance in the experiments by using this
simpliﬁed model. Without loss of generality, we present the optimization algorithm
for the complete model below.
10.4.4
Matching and Fusion
With the learned two pairs of dictionaries, fDL
1; DL
2g and fDH
1 ; DH
2 g, we can obtain
robust representations for the test images in two views, and perform the following
matching and fusion strategy.
In person re-identiﬁcation, we need to match a probe image to a set of gallery
images. As our approach jointly learns the dictionaries in both patch-level and
image-level, we propose a fusion strategy to take full advantages of the robust
representations.
Patch-level Matching. The patch matching methods have been extensively studied
in existing works [34, 36]. We adopt a similar constrained patch matching strategy.
For each patch in the probe image, we can not directly match it to the corresponding
patch in gallery images, due to the well-known misalignment problem. Therefore,
we search the spatial neighbors of the targeted patch in the gallery images, and
calculate the distances between each pairs. Finally, we can estimate the similarity
between a probe image and every gallery image. Instead of comparing the original
patches, we match the representation coefﬁcients over the dictionaries fDL
1; DL
2g for
each pair of patches. The similarity score ScoreP.i/ between the probe image and
the i-th gallery image is generated from the similarities between these patches.
Image-level Matching. The image-level matching between the probe image and
gallery images is more straightforward, as we have already attained the compact
representations for each image. The representation coefﬁcients are calculated using
the dictionaries fDH
1 ; DH
2 g for each pair of patches. We adopt the Gaussian kernel
function to compute the similarity score ScoreI.i/ between the probe image and the
i-th gallery image.
Fusion. We ﬁrst normalize the similarity score vectors ScoreP and ScoreI, and
utilize a simple strategy to perform score fusion:
Score.i/ D ScoreP.i/ C ScoreI.i/;
(10.6)
where  is a user-deﬁned parameter.

10.5
Optimization
211
10.5
Optimization
10.5.1
Optimizing Image-Level Representations
To facilitate the optimization of (10.4), we ﬁrst add two relaxation variables AH
1 and
AH
2 , and rewrite the objective as
min
DH
1 ;DH
2 ;PH
1 ;
PH
2 ;AH
1 ;AH
2
X1  DH
1 AH
1
2
F C
X2  DH
2 AH
2
2
F
C˛.
PH
1 X1  AH
1
2
F C
PH
2 X2  AH
2
2
F/
C1
AH
1  AH
2
2
F ;
s:t:
kdH
1.W;i/k  1; kdH
2.W;i/k  1;
(10.7)
where ˛ is a balance parameter.
Although there are many variables in (10.7), we can alternatively optimize these
variables as follows.
(1) Fix other variables and update AH
1 and AH
2 .
By ignoring the irrelevant variables with respect to AH
1 , the objective (10.7) is
reduced to
min
AH
1
J.AH
1 / D
X1  DH
1 AH
1
2
F C ˛
PH
1 X1  AH
1
2
F
C1
AH
1  AH
2
2
F :
(10.8)
Setting @J.AH
1 /
@AH
1
D 0, we get the solution
AH
1 D .DHT
1 DH
1 C .˛ C 1/I/1
.DHT
1 X1 C 1AH
2 C ˛PH
1 X1/;
(10.9)
where I is an identity matrix. We can obtain solution to AH
2 in a very similar way.
(2) Fix other variables and update PH
1 and PH
2 .
The objective function regarding PH
1 can be written as
min
PH
1
˛
PH
1 X1  AH
1
2
F :
(10.10)
By setting the derivative with respect to PH
1 to zero, we have the solution PH
1 D
AH
1 X1.X1XT
1 C I/1, where  is a regularization parameter. Similarly, the solution
to PH
2 is: PH
2 D AH
2 X2.X2XT
2 C I/1.

212
10
Robust Representations for Person Re-identiﬁcation
(3) Fix other variables and update DH
1 and DH
2 .
By removing the irrelevant terms in (10.7), we can write the objective function
regarding DH
1 as
min
DH
1
X1  DH
1 AH
1
2
F
s:t: kdH
1.W;i/k  1:
(10.11)
Problem (10.11) can be effectively solved using ADMM algorithm as introduced
in [8]. We have similar solutions to DH
2 .
The above procedures are repeated until convergence. Finally, we obtain a pair of
dictionaries fDH
1 ; DH
2 g that are used to represent high-dimensional image features.
The time complexities of updating variables A, P and D are O.mdnCm3 Cm2n/,
O.mdn C d3 C d2n/, and O.t.mdn C m3 C m2d C d2m//, respectively, where t is
the number of iterations. In practice, t is a small number as the algorithm converges
quickly. The dictionary size m is usually much less than the sample size n and the
dimensionality d. Thus, our algorithm is efﬁcient in real-world applications.
The
objective
in
Eq. (10.7)
is
a
bi-convex
problem
for
variables
f.DH
1 ; PH
1 /; AH
1 /g. When DH
1 and PH
1 are ﬁxed, the objective function is convex for AH
1 .
When AH
1 is ﬁxed, the objective function is convex for DH
1 and PH1. The convergence
property of such problems has been extensively studied in [5]. In addition, our
optimization algorithm converges quickly in the experiments.
10.5.2
Optimizing Patch-Level Representations
To solve the problem (10.5), we ﬁrst reformulate the objective as
min
DL
1 ;DL
2 ;PL
1 ;
PL
2 ;AL
1 ;AL
2
Y1  DL
1AL
1Y1
2
F C
Y2  DL
2AL
2Y2
2
F
Cˇ.
PL
1Y1  AL
1
2 C
PL
2Y2  AL
2
2
F/
C2
DL
1  DL
2
2
F ;
s:t:
kdL
1.W;i/k  1; kdL
2.W;i/k  1;
(10.12)
where ˇ is a balance parameter.
We alternatively update the variables in (10.12), and obtain the sub-problems
(with solutions) as follows
min
AL
1
Y1  DL
1AL
1
2
F C ˇ
PL
1Y1  AL
1
2
F :
(10.13)

10.6
Experiments
213
Algorithm 10.1 CPDL for person re-identiﬁcation
Input: Training images in two views A1, A2,
test images T1; T2, parameters 1, 2, , ˛, ˇ.
Output: Matching results.
Training
1: Extract dense features from A1; A2 (Sect. 10.4.1), and
construct feature sets X1; X2; Y1; Y2;
2: Learn dictionaries fDH
1 ; DH
2 g from image-level
features X1; X2 (Sect. 10.5.1);
3: Learn dictionaries fDL
1; DL
2g from patch-level
features Y1; Y2 (Sect. 10.5.2);
Testing
4: Extract dense features from T1; T2 (Sect. 10.4.1), and
construct feature sets Xt1; Xt2; Yt1; Yt2;
5: Encode Xt1; Xt2 using fDH
1 ; DH
2 g, and perform
image-level matching (Sect. 10.4.4);
6: Encode Yt1; Yt2 using fDL
1; DL
2g, and perform
patch-level matching (Sect. 10.4.4);
7: Fuse matching results in two-levels using (10.6).
The solution to (10.13) is AL
1 D .DLT
1 DL
1 C ˇI/1.DLT
1 Y1 C ˇPL
1Y1/:
min
PL
1
ˇ
PL
1Y1  AL
1
2
F :
(10.14)
The optimal solution is PL
1 D AL
1Y1.Y1YT
1 C I/1.
min
DL
1
Y1  DL
1AL
1
2
F C 2
DL
1  DL
2
2
F ;
s:t: kdL
1.W;i/k  1:
(10.15)
We have similar solutions to AL
2, PL
2 and DL
2. The above procedures are repeated
until convergence. We ﬁnally obtain a pair of optimal dictionaries fDL
1; DL
2g that are
used to reconstruct low-dimensional patch features.
The complete algorithm is summarized in Algorithm 10.1.
10.6
Experiments
In this section, we compare our approach with several related methods on three
benchmark datasets, VIPeR [6], CUHK01 Campus [36], and GRID [22].

214
10
Robust Representations for Person Re-identiﬁcation
10.6.1
Settings
Baselines. We compare our approach with three types of person re-identiﬁcation
methods, which are feature learning methods, metric learning methods and dic-
tionary learning methods. The feature learning methods include symmetry-driven
accumulation of local features (SDALF) [4], local descriptors encoded by Fisher
vectors (LDFV) [24], unsupervised salience learning method (eSDC) [35], salience
matching method [34], and mid-level ﬁlters [36]. The compared metric learning
algorithms include probabilistic relative distance comparison (PRDC) [38], large
margin nearest neighbor (LMNN) [29], eBiCov [23], information-theoretic met-
ric learning (ITML) [3], pairwise constrained component analysis (PCCA) [25],
KISSME [10], and local Fisher discriminant analysis (LF) [27]. We also compare
with the dictionary learning method SSCDL [21].
Evaluation Metrics. We employ the standard cumulated matching characteristics
(CMC) curve as our evaluation metric, and report the Rank-k recognition rates.
Parameter Setting. There are ﬁve parameters in our model, including ˛, ˇ, ,
1 and 2. In the experiments, we empirically set these parameters to achieve the
best performance. In particular, ˛ and ˇ are set to 2 and 1, respectively.  used in
the fusion strategy is chosen in the range Œ0 1. Two parameters 1 and 2 control
the effects of cross-view interactions, and we will discuss their settings in the next
section.
10.6.2
VIPeR Dataset
The VIPeR dataset was collected in an outdoor academic environment. It contains
images of 632 pedestrian pairs under two camera views with different viewpoints.
The images in two views have signiﬁcant variations in pose, viewpoint and
illuminations. Figure 10.2a shows some images captured by Camera-1 (ﬁrst row)
and Camera-2 (second row) in the VIPeR dataset. The images are normalized to the
size of 12848 in our experiments.
We follow the evaluation protocol in [7]. In particular, we randomly select 316
pairs of images for training, and the remaining pairs are used for test. Then, two
groups of experiments are conducted. First, the images captured by Camera-1 are
utilized as probe images, and the images captured by Camera-2 as gallery images.
For the probe images, we match each of them to the gallery set, and obtain the Rank-
k rate. The CMC curves are also obtained by using the rates at all ranks. Second,
we exchange the training and test sets, and repeat the above procedures. As the
raw features for image-level training have very high dimensions, we apply PCA to
reduce the dimensionality by keeping the 95% energy. We conduct 10 random tests
and report the average results. Each random test has two groups of evaluations as
described above.

10.6
Experiments
215
Fig. 10.2 Illustration of images in (a) VIPeR dataset, (b) CUHK Campus dataset, and (c) GRID
dataset
Figure 10.3 shows the CMC curves of the compared methods. We can observe
that our approach achieves higher matching rates in each rank. Table 10.1 shows the
detailed Rank-1, Rank-5, Rank-10, and Rank-20 matching rates of all the compared
methods. It shows that the advanced feature learning methods like salience matching
(SalMat) and mid-level ﬁlters obtain much better results than metric learning
methods. The dictionary learning method SSCDL achieves better Rank-5/10/20
rates than the SalMat and Mid-level methods, which shows the merits of dictionary
learning. Our approach achieves the best Rank-1 rate, and signiﬁcantly improves the
Rank-5/10/20 rates, validating the effectiveness of the proposed CPDL framework.
10.6.3
CUHK01 Campus Dataset
The CUHK01 Campus dataset contains pedestrian images of 971 persons in two
camera views. It was collected in a campus environment. This dataset shows
signiﬁcant changes of viewpoints. The frontal or back views are captured by

216
10
Robust Representations for Person Re-identiﬁcation
5
10
15
20
25
0
10
20
30
40
50
60
70
80
90
100
Rank
Matching Rate (%)
19.87% SDALF 
20.66% eBiCov
26.31% eSDC
15.66% PRDC
16.14% aPRDC
19.27% PCCA
19.60% KISSME
24.18% LF
30.16% SalMatch
29.11% Mid−level
33.99% Ours
Fig. 10.3 CMC curves of average matching rates on VIPeR dataset. Rank-1 matching rate is
marked before the name of each approach
Table 10.1 Top ranked
matching rates in (%) with
316 persons on VIPeR dataset
Method
r D 1
r D 5
r D 10
r D 20
PRDC
15.66
38.42
53.86
70.09
PCCA
19.27
48.89
64.91
80.28
SDALF
19.87
38.89
49.37
65.73
eBiCov
20.66
42.00
56.18
68.00
LDFV
22.34
47.00
60.40
71.00
LF
24.11
51.24
67.09
82.01
eSDC
26.31
50.70
62.37
76.36
SalMat
30.16
53.45
65.78
N/A
SSCDL
25.60
53.70
68.10
83.60
Mid-level
29.11
52.50
67.12
80.03
CPDL (Ours)
33.99
64.21
77.53
88.58
Camera-1, while the side views are captured by Camera-2. Figure 10.2b illustrates
some images in view 2 (ﬁrst row) and view 1 (second row). The images are resized
to 160  60 in our experiments.
We follow the evaluation protocol in [36]. For each person, one image is
randomly selected to build the gallery set, and the other one is used to construct
the probe set. We map each image in probe set to every gallery image, and calculate
the correct matched rank and CMC curves. The whole procedure is repeated for 10
times, and the average CMC curves are generated, as shown in Fig. 10.4. Table 10.2
shows the detailed Rank-1/5/10/20 matching rates of the compared methods. We can

10.6
Experiments
217
5
10
15
20
25
0
20
40
60
80
100
Rank
Matching Rate (%)
10.33% L1−norm
9.84% L2−norm
9.90% SDALF
19.67% eSDC
13.45% LMNN
15.98% ITML
20.00% GenericMetric
28.45% SalMatch
34.30% Mid−level
59.47% Ours 
Fig. 10.4 CMC curves of average matching rates on CUHK01 dataset. Rank-1 matching rate is
marked before the name of each approach
Table 10.2 Top ranked
matching rates in (%) on
CUHK01 dataset
Method
r D 1
r D 5
r D 10
r D 20
SDALF
9.90
22.57
30.33
41.03
eSDC
19.67
32.71
40.28
50.57
LMNN
13.45
31.33
42.25
54.11
ITML
15.98
35.22
45.60
59.80
SalMat
28.45
45.85
55.67
68.89
Mid-level
34.30
55.06
64.96
73.94
CPDL (Ours)
59.47
81.26
89.72
93.10
observe that our approach obtains much higher matching rates than other methods.
The Rank-1 matching rate is improved by 25.17%, compared to the mid-level ﬁlter
method.
10.6.4
GRID Dataset
The QMUL underGround Re-IDentiﬁcation (GRID) dataset3 contains 250 pairs of
pedestrian images [22]. All images are captured from eight disjoint camera views
3http://www.eecs.qmul.ac.uk/~ccloy/downloads_qmul_underground_reid.html

218
10
Robust Representations for Person Re-identiﬁcation
Table 10.3 Top ranked
matching rates in (%) with on
GRID dataset
Method
r D 1
r D 5
r D 10
r D 20
ELF6+L1-norm
4.40
–
16.24
24.80
ELF6+RankSVM
10.24
24.60
33.28
43.68
ELF6+PRDC [38]
9.68
22.00
32.96
44.32
ELF6+MRank-
RankSVM
12.24
27.80
36.32
46.56
ELF6-MRank-PRDC
11.12
26.10
35.76
46.56
ELF6+XQDA [19]
10.48
–
38.64
52.56
LOMO+XQDA [19]
16.56
–
41.84
52.40
MLAPG [20]
16.64
–
41.20
52.96
Chen et al. [2]
16.30
35.80
46.00
57.6
CPDL (Ours)
21.60
45.85
61.05
65.80
installed in a busy underground station. For each individual, a pair of images was
captured from two different camera views. In addition, GRID dataset contains 775
additional images that do not belong to the 250 individuals, and these images
can be used to enlarge the gallery set. Figure 10.2c shows six pairs of images,
which demonstrate variations of pose, colors, illumination changes. In addition, the
resolution of images in GRID dataset is very low, which makes it more challenging
to do person re-identiﬁcation.
In the experiments, we randomly choose 125 pairs of images for training,
and use the rest 125 pairs with the additional 775 images for test. The random
selection process is repeated for 10 times. Table 10.3 shows the detailed Rank-
1/5/10/20 matching rates of our approaches and the baseline methods. By comparing
Table 10.3 with the results in Tables 10.1 and 10.2, we can observe that the re-
identiﬁcation task on the GRID dataset is more challenging than that on the VIPeR
and CUHK01 Campus dataset. There are two major reasons. First, the images
in GRID were captured by eight different cameras, while VIPeR and CUHK01
Campus only have two camera views. Second, GRID has a larger test set with
775 additional images, which usually leads to a lower matching rate. We compare
the performance of our approaches with the state-of-the-art results reported on the
GRID dataset. Table 10.3 shows the average Rank-1/5/10/20 matching rates. Our
approach improves the Rank-1 matching rate by at least 9%. It shows that integrating
features extracted from three levels (i.e., image-level, part-level, and patch-level) is
an effective strategy to handle the challenging person re-identiﬁcation problem with
multiple viewpoints.
10.6.5
Discussions
Different from existing methods, the proposed CPDL approach models the interac-
tions between different views, such as the similarities of codings (in the image-level)

10.6
Experiments
219
0.1
1
2
3
4
5
24
26
28
30
32
34
36
38
Value of Parameters
Rank−1 Matching Rate (%) 
Our approach with different λ1, when λ2 =2
Our approach with different λ2, when λ1 =1
1
5
10
15
20
25
0
10
20
30
40
50
60
70
80
90
100
Rank
Matching Rates (%)
Fusion
Image−level
Patch−level
(b)
(a)
30
40
50
60
70
80
90
0
5
10
15
20
25
30
35
40
45
Dictionary Size
Rank−1 Matching Rate (%)
(c)
Fig. 10.5 Experimental analysis on VIPeR dataset. (a) Rank-1 matching rates v.s. different values
of parameters; (b) Matching rates of image-level model, patch-level model and the fusion model;
(c) Rank-1 matching rates v.s. different dictionary size
or dictionaries (in the patch-level). The parameters 1 and 2 control the effects of
the cross-view interactions. Figure 10.5a shows the Rank-1 matching rates of our
approach with different values of 1 and 2. It shows that our approach is not very
sensitive to the choice of parameters in the range Œ0 5. We set 1 D 1; 2 D 2.
Figure 10.5b shows the CMC curves of our approach and its two components,
i.e., image-level model and patch-level model. We can observe that the represen-
tations in image-level and patch-level are complementary to each other, and our
approach takes full advantage of the complementary information.
Another important factor in our approach is the size of dictionary. We use the
same dictionary size in different views. Figure 10.5c shows the Rank-1 matching
rate with different dictionary size. We achieved similar results on the CUHK01
dataset. Accordingly, the dictionary size is set to 50 in our experiments. Also, we
note that the matching process in existing feature learning methods (e.g., SalMat or
Mid-level ﬁlter) is very time consuming. However, our approach adopts a relative
small dictionary, which leads to compact representations of images, and therefore
speeds up the matching process.

220
10
Robust Representations for Person Re-identiﬁcation
10.7
Summary
We proposed a cross-view projective dictionary learning (CPDL) approach for per-
son re-identiﬁcation in this chapter. Our approach learned two pairs of dictionaries
across different views in patch-level and image-level, respectively. The learned
dictionaries can be used to represent probe and gallery images, leading to robust
representations. Experimental results on the public VIPeR, CUHK Campus and
GRID datasets showed that our approach took full advantages of the complementary
information in different views and representation levels, and achieved the state-of-
the-art performance compared with the related methods.
References
1. Aharon, M., Elad, M., Bruckstein, A.:
K-SVD: an algorithm for designing overcomplete
dictionaries for sparse representation. IEEE Trans. Signal Process. 54(11), 4311–4322 (2006)
2. Chen, D., Yuan, Z., Hua, G., Zheng, N., Wang, J.: Similarity learning on an explicit polynomial
kernel feature map for person re-identiﬁcation. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 1565–1573 (2015)
3. Davis, V.J., Kulis, B., Jain, P., Sra, S., Dhillon, I.S.: Information-theoretic metric learning. In:
Proceedings of the International Conference on Machine Learning, pp. 209–216 (2007)
4. Farenzena, M., Bazzani, L., Perina, A., Murino, V., Cristani, M.: Person re-identiﬁcation by
symmetry-driven accumulation of local features. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 2360–2367 (2010)
5. Gorski, J., Pfeuffer, F., Klamroth, K.: Biconvex sets and optimization with biconvex functions:
a survey and extensions. Math. Methods Oper. Res. 66(3), 373–407 (2007)
6. Gray, D., Brennan, S., Tao, H.: Evaluating appearance models for recognition, reacquisition,
and tracking. In: PETS (2007)
7. Gray, D., Tao, H.: Viewpoint invariant pedestrian recognition with an ensemble of localized
features. In: Proceedings of the European Conference on Computer Vision I, pp. 262–275
(2008)
8. Gu, S., Zhang, L., Zuo, W., Feng, X.:
Projective dictionary pair learning for pattern
classiﬁcation. In: Proceedings of the Annual Conference on Neural Information Processing
Systems, pp. 793–801 (2014)
9. Hirzer, M., Roth, M.P., Köstinger, M., Bischof, H.: Relaxed pairwise learned metric for person
re-identiﬁcation. In: Proceedings of the European Conference on Computer Vision, pp. 780–
793 (2012)
10. Köstinger, M., Hirzer, M., Wohlhart, P., Roth, M.P., Bischof, H.: Large scale metric learning
from equivalence constraints. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 2288–2295 (2012)
11. Layne, R., Hospedales, M.T., Gong, S.: Person re-identiﬁcation by attributes. In: Proceedings
of the British Machine Vision Conference, pp. 1–11 (2012)
12. Li, L., Li, S., Fu, Y.: Learning low-rank and discriminative dictionary for image classiﬁcation.
Image Vis. Comput. 32(10), 814–823 (2014)
13. Li, S., Li, K., Fu, Y.:
Temporal subspace clustering for human motion segmentation.
In:
Proceedings of the IEEE International Conference on Computer Vision, pp. 4453–4461 (2015)
14. Li, S., Li, K., Fu, Y.: Self-taught low-rank coding for visual learning. IEEE Trans. Neural
Netw. Learn. Syst. (2017)

References
221
15. Li, S., Li, Y., Fu, Y.: Multi-view time series classiﬁcation: a discriminative bilinear projection
approach. In: Proceedings of the 25th ACM International on Conference on Information and
Knowledge Management, pp. 989–998. ACM (2016)
16. Li, S., Shao, M., Fu, Y.: Cross-view projective dictionary learning for person re-identiﬁcation.
In: Proceedings of the 24th International Joint Conference on Artiﬁcial Intelligence, pp. 2155–
2161. AAAI Press (2015)
17. Li, S., Shao, M., Fu, Y.: Multi-view low-rank analysis for outlier detection. In: Proceedings of
the SIAM International Conference on Data Mining, pp. 748–756. SIAM (2015)
18. Li, W., Zhao, R., Xiao, T., Wang, X.: DeepReID: deep ﬁlter pairing neural network for person
re-identiﬁcation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 152–159 (2014)
19. Liao, S., Hu, Y., Zhu, X., Li, S.Z.:
Person re-identiﬁcation by local maximal occurrence
representation and metric learning. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 2197–2206 (2015)
20. Liao, S., Li, S.Z.:
Efﬁcient PSD constrained asymmetric metric learning for person re-
identiﬁcation.
In: Proceedings of the IEEE International Conference on Computer Vision,
pp. 3685–3693 (2015)
21. Liu, X., Song, M., Tao, D., Zhou, X., Chen, C., Bu, J.: Semi-supervised coupled dictionary
learning for person re-identiﬁcation. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 3550–3557 (2014)
22. Loy, C.C., Xiang, T., Gong, S.:
Multi-camera activity correlation analysis.
In: IEEE
International Conference on Computer Vision and Pattern Recognition, pp. 1988–1995 (2009)
23. Ma, B., Su, Y., Jurie, F.: BiCov: a novel image representation for person re-identiﬁcation and
face veriﬁcation. In: Proceedings of the British Machine Vision Conference, pp. 1–11 (2012)
24. Ma, B., Su, Y., Jurie, F.:
Local descriptors encoded by ﬁsher vectors for person re-
identiﬁcation. In: Proceedings of the European Conference on Computer Vision Workshops
and Demonstration, pp. 413–422 (2012)
25. Mignon, A., Jurie, F.:
PCCA: a new approach for distance learning from sparse pairwise
constraints.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 2666–2672 (2012)
26. Ni, J., Qiu, Q., Chellappa, R.: Subspace interpolation via dictionary learning for unsupervised
domain adaptation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 692–699 (2013)
27. Pedagadi, S., Orwell, J., Velastin, A.S., Boghossian, B.A.: Local ﬁsher discriminant analysis
for pedestrian re-identiﬁcation. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 3318–3325 (2013)
28. Wang, T., Gong, S., Zhu, X., Wang, S.:
Person re-identiﬁcation by video ranking.
In:
Proceedings of the European Conference on Computer Vision, pp. 688–703 (2014)
29. Weinberger, Q.K., Blitzer, J., Saul, L.K.: Distance metric learning for large margin nearest
neighbor classiﬁcation.
In: Proceedings of the Annual Conference on Neural Information
Processing Systems (2005)
30. Wu, Z., Li, Y., Radke, R.J.: Viewpoint invariant human re-identiﬁcation in camera networks
using pose priors and subject-discriminative features. IEEE Trans. Pattern Anal. Mach. Intell.
37(5), 1095–1108 (2015)
31. Xiong, F., Gou, M., Camps, I.O., Sznaier, M.:
Person re-identiﬁcation using kernel-based
metric learning methods. In: Proceedings of the European Conference on Computer Vision,
pp. 1–16 (2014)
32. Yang, Y., Yang, J., Yan, J., Liao, S., Yi, D., Li, S.Z.:
Salient color names for person re-
identiﬁcation. In: Proceedings of the European Conference on Computer Vision, pp. 536–551
(2014)
33. Zhang, Q., Li, B.: Discriminative K-SVD for dictionary learning in face recognition. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2691–
2698 (2010)

222
10
Robust Representations for Person Re-identiﬁcation
34. Zhao, R., Ouyang, W., Wang, X.:
Person re-identiﬁcation by salience matching.
In:
Proceedings of the IEEE International Conference on Computer Vision, pp. 2528–2535 (2013)
35. Zhao, R., Ouyang, W., Wang, X.: Unsupervised salience learning for person re-identiﬁcation.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 3586–3593 (2013)
36. Zhao, R., Ouyang, W., Wang, X.: Learning mid-level ﬁlters for person re-identiﬁcation. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 144–
151 (2014)
37. Zheng, J., Jiang, Z., Phillips, J.P., Chellappa, R.:
Cross-view action recognition via a
transferable dictionary pair. In: BMVC, vol. 1, pp. 1–11 (2012)
38. Zheng, W., Gong, S., Xiang, T.:
Person re-identiﬁcation by probabilistic relative distance
comparison.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 649–656 (2011)
39. Zheng, W., Gong, S., Xiang, T.: Transfer re-identiﬁcation: from person to set-based veriﬁca-
tion. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 2650–2657 (2012)
40. Zhong, G., Zheng, Y., Li, S., Fu, Y.:
Scalable large margin online metric learning.
In:
International Joint Conference on Neural Networks, pp. 2252–2259. IEEE (2016)

Index
A
anomaly detection, 175
augmented Lagrangian function, 25, 103, 183
Auxiliary Domain, 107
auxiliary domain, 96
B
balanced graph, 20, 28
between-class scatter, 50
bilinear projection, 75, 78
book recommendation, 140
C
clustering, 31, 106, 176
collaborative ﬁltering, 124, 150
collective matrix factorization, 148
conversion prediction, 148
convexity, 50
cross-view interaction, 205
D
data sparsity, 129
deep collaborative ﬁltering, 125
deep learning, 124
deep neural networks, 124
denoising auto-encoders, 124
dictionary learning, 95, 204, 209
digital marketing, 147
dimensionality reduction, 47, 76
discriminative subspace, 47
display advertising, 147
dynamic collective matrix factorization, 148
F
face recognition, 49, 97
feature extraction, 49, 207
feature fusion, 90
Fisher criterion, 51, 79
G
gradient descent, 81
graph, 17, 28, 106
graph construction, 18
graph sparsiﬁcation, 18
group anomaly detection, 178
H
Hessian matrix, 52
I
image clustering, 97
inter-class correlation, 23
intra-class variation, 23
K
kinship veriﬁcation, 49
L
label propagation, 30
Laplacian regularization, 80
latent factor, 127, 154
latent subspace, 75
locally linear approximation, 25, 103
© Springer International Publishing AG 2017
S. Li, Y. Fu, Robust Representation for Data Analytics, Advanced Information
and Knowledge Processing, DOI 10.1007/978-3-319-60176-2
223

224
Index
low-rank coding, 23, 96
low-rank matrix recovery, 18, 106, 176
low-rank modeling, 47
low-rank representations, 47
M
majorization-minimization, 25, 102
matching, 210
matrix -norm, 24, 101
matrix factorization, 124, 148
matrix tri-factorization, 83
minimax concave penalty norm, 24, 101
movie recommendation, 137
multi-level model, 204
multi-view dimensionality reduction, 78
multi-view learning, 74, 178
multi-view multivariate time series, 77
multi-view outlier detection, 177
multivariate time series, 73, 77
N
nuclear norm, 50, 98
O
object recognition, 49, 97
orthogonal constraint, 51
outlier detection, 175
P
person re-identiﬁcation, 203
probabilistic matrix factorization , 125
projection, 49, 75, 206
projective dictionary learning, 204
R
rank minimization, 24, 47, 101, 178, 181
recommendation system, 123
response prediction, 141, 149
S
self-taught learning, 96
semi-supervised learning, 31
side information, 130, 149
similarity metric, 20
similarity score, 210
smoothness regularization, 31, 76
sparse coding, 100
speech recognition, 87
stacked denoising auto-encoders, 134
stochastic gradient descent, 128, 159
structure information, 96
subspace learning, 45, 74
subspace structure, 28
supervised regularization, 47, 91
T
target domain, 96
temporal dynamics, 75, 150
time-series classiﬁcation, 74
trace norm, 182
trace-ratio problem, 50
transfer learning, 96
U
unbalanced graph, 20
V
view consistency, 75
W
within-class scatter, 50


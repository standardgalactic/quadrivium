
A MODERN THEORY 
OF RANDOM VARIATION 

A MODERN THEORY 
OF RANDOM VARIATION 
With Applications in Stochastic 
Calculus, Financial Mathematics, 
and Feynman Integration 
Pat Muldowney 
University of Ulster 
Deny, Ireland 
WILEY 
A JOHN WILEY & SONS, INC., PUBLICATION 

Cover illustration: Pavel Khorenyan 
Copyright © 2012 by John Wiley & Sons, Inc. All rights reserved 
Published by John Wiley & Sons, Inc., Hoboken, New Jersey 
Published simultaneously in Canada 
No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or 
by any means, electronic, mechanical, photocopying, recording, scanning or otherwise, except as 
permitted under Section 107 or 108 of the 1976 United States Copyright Act, without either the prior 
written permission of the Publisher, or authorization through payment of the appropriate per-copy fee to 
the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, (978) 750-8400, fax 
(978) 750-4470, or on the web at www.copyright.com. Requests to the Publisher for permission should 
be addressed to the Permissions Department, John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ 
07030, (201) 748-6011, fax (201) 748-6008, or online at http://www.wiley.com/go/permission. 
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best efforts in 
preparing this book, they make no representation or warranties with respect to the accuracy or 
completeness of the contents of this book and specifically disclaim any implied warranties of 
merchantability or fitness for a particular purpose. No warranty may be created or extended by sales 
representatives or written sales materials. The advice and strategies contained herein may not be 
suitable for your situation. You should consult with a professional where appropriate. Neither the 
publisher nor author shall be liable for any loss of profit or any other commercial damages, including 
but not limited to special, incidental, consequential, or other damages. 
For general information on our other products and services please contact our Customer Care 
Department within the United States at (800) 762-2974, outside the United States at (317) 572-3993 or 
fax (317) 572-4002. 
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print, 
however, may not be available in electronic formats. For more information about Wiley products, visit 
our web site at www.wiley.com. 
Library of Congress Cataloging-in-Publication Data: 
Muldowney, P. (Patrick), 1946-
A modern theory of random variation : with applications in stochastic calculus, financial mathematics, 
and Feynman integration / Patrick Muldowney. 
p. cm. 
Includes index. 
ISBN 978-1-118-16640-6 (hardback) 
1. Random variables. 2. Calculus of variations. 3. Path integrals. 4. Mathematical analysis. I. Title. 
QA273.M864 2012 
519.2'3-^dc23 
2012002023 
Printed in the United States of America. 
10 9 8 7 6 5 4 3 2 1 

O b e A mbeAÖ ye 1 ΤΙΟΑΠ DO 
9X IATTI ΟΘΑΓ b e i c pAoi TI-A ceArm 
ll~ oeirbm Ιιοτη nAfib eA5Al bAf DO 
5 o bpÄc nÄ 'Π-Α b e o b e i c cirm. 

Contents 
Preface 
xi 
Symbols 
xiii 
1 Prologue 
1 
1.1 
About This Book 
1 
1.2 
About the Concepts 
3 
1.3 
About the Notation 
6 
1.4 
Riemann, Stieltjes, and Burkill Integrals 
7 
1.5 
The -Complete Integrals 
14 
1.6 
Riemann Sums in Statistical Calculation 
15 
1.7 
Random Variability 
17 
1.8 
Contingent and Elementary Forms 
21 
1.9 
Comparison With Axiomatic Theory 
23 
1.10 What Is Probability? 
25 
1.11 Joint Variability 
26 
1.12 Independence 
31 
1.13 Stochastic Processes 
35 
2 Introduction 
37 
2.1 
Riemann Sums in Integration 
39 
2.2 
The -Complete Integrals in Domain ]0,1] 
41 
2.3 Divisibility of the Domain ]0,1] 
45 
2.4 
Fundamental Theorem of Calculus 
45 
2.5 
What Is Integrability? 
47 
2.6 
Riemann Sums and Random Variability 
49 
2.7 
How to Integrate a Function 
51 
2.8 
Extension of the Lebesgue Integral 
53 
2.9 
Riemann Sums in Basic Probability 
59 
2.10 Variation and Outer Measure 
63 
2.11 Outer Measure and Variation in [0,1] 
64 
2.12 The Henstock Lemma 
67 
2.13 Unbounded Sample Spaces 
69 
2.14 Cauchy Extension of the Riemann Integral 
71 
vii 

viii 
CONTENTS 
2.15 Integrability on ]0,oo[ 
73 
2.16 "Negative Probability" 
78 
2.17 Henstock Integration in R n 
79 
2.18 Conclusion 
82 
3 Infinite-Dimensional Integration 
83 
3.1 
Elements of Infinite-Dimensional Domain 
84 
3.2 
Partitions of R T 
88 
3.3 
Regular Partitions of R T 
90 
3.4 
5-Fine Partially Regular Partitions 
93 
3.5 
Binary Partitions of R T 
95 
3.6 
Riemann Sums in R T 
97 
3.7 
Integrands in R T 
98 
3.8 
Definition of the Integral in R T 
101 
3.9 
Integrating Functions in R T 
106 
4 
Theory of the Integral 
111 
4.1 
The Henstock Integral 
Ill 
4.2 
Gauges for R T 
116 
4.3 
Another Integration System in R T 
119 
4.4 
Validation of Gauges in R T 
120 
4.5 
The Burkill-Complete Integral in R T 
125 
4.6 
Basic Properties of the Integral 
127 
4.7 Variation of a Function 
134 
4.8 Variation and Integral 
146 
4.9 
RTxA/"(T)-Variation 
152 
4.10 Introduction to Fubini's Theorem 
154 
4.11 Fubini's Theorem 
159 
4.12 Limits of Integrals 
165 
4.13 Limits of Non-Absolute Integrals 
174 
4.14 Non-Integrable Functions 
178 
4.15 Conclusion 
181 
5 Random Variability 
183 
5.1 
Measurability of Sets 
186 
5.2 
Measurability of Random Variables 
192 
5.3 
Representation of Observables 
197 
5.4 
Basic Properties of Random Variables 
203 
5.5 
Inequalities for Random Variables 
207 
5.6 
Joint Random Variability 
212 
5.7 
Two or More Joint Observables 
214 
5.8 
Independence in Random Variability 
217 
5.9 
Laws of Large Numbers 
224 
5.10 Introduction to Central Limit Theorem 
233 
5.11 Proof of Central Limit Theorem 
236 
5.12 Probability Symbols 
244 

CONTENTS 
ix 
5.13 Measurability and Probability 
245 
5.14 The Calculus of Probabilities 
251 
6 Gaussian Integrals 
257 
6.1 FresnePs Integral 
257 
6.2 
Evaluation of FresnePs Integral 
261 
6.3 
FresnePs Integral in Finite Dimensions 
265 
6.4 
Fresnel Distribution Function in R n 
270 
6.5 
Infinite-Dimensional Fresnel Integral 
274 
6.6 
Integrability on R T 
276 
6.7 
The Fresnel Function Is VBG* 
282 
6.8 
Incremental Fresnel Integral 
284 
6.9 
Fresnel Continuity Properties 
288 
7 Brownian Motion 
305 
7.1 
c-Brownian Motion 
305 
7.2 
Brownian Motion With Drift 
307 
7.3 
Geometric Brownian Motion 
308 
7.4 
Continuity of Sample Paths 
315 
7.5 
Introduction to Continuous Modification 
316 
7.6 
Continuous Modification 
317 
7.7 Introduction to Marginal Densities 
320 
7.8 
Marginal Densities in R T 
322 
7.9 
Regular Partitions 
326 
7.10 Step Functions in R T 
327 
7.11 c-Brownian Random Variables 
332 
7.12 Introduction to ZY-Observables 
334 
7.13 Construction of Step Functions in R T 
338 
7.14 Estimation of E[fu(XT)] 
339 
7.15 £Y-Observables in c-Brownian Motion 
341 
7.16 Diffusion Equation 
345 
7.17 Feynman Path Integrals 
353 
7.18 Feynman's Definition of Path Integral 
358 
7.19 Convergence of Binary Sums 
360 
7.20 Feynman Diagrams 
366 
7.21 Interpretation of the Perturbation Series 
373 
7.22 Validity of Feynman Diagrams 
377 
7.23 Conclusion 
381 
8 Stochastic Integration 
383 
8.1 
Introduction to Stochastic Integrals 
385 
8.2 
Varieties of Stochastic Integral 
386 
8.3 
Strong Stochastic Integral 
391 
8.4 
Weak Stochastic Integral 
398 
8.5 
Definition of Weak Stochastic Integral 
407 
8.6 
Properties of Weak Stochastic Integral 
410 

x 
CONTENTS 
8.7 
Evaluating Stochastic Integrals 
412 
8.8 
Stochastic and Observable Integrals 
419 
8.9 
Existence of Weak Stochastic Integrals 
421 
8.10 Ito's Formula 
424 
8.11 Proof of Ito's Formula 
426 
8.12 Application of Ito's Formula 
429 
8.13 Derivative Asset Valuation 
433 
8.14 Risk-Neutral Pricing 
436 
8.15 Comments on Risk-Neutral Pricing 
438 
8.16 Pricing a European Call Option 
440 
8.17 Call Option as Contingent Observable 
443 
8.18 Black-Scholes Equation 
444 
8.19 Construction of Risk-Neutral Model 
445 
9 Numerical Calculation 
447 
9.1 
Introduction 
448 
9.2 
Random Walk 
450 
9.3 
Calculation of Strong Stochastic Integrals 
453 
9.4 
Calculation of Weak Stochastic Integrals 
457 
9.5 
Calculation of Ito's Formula 
467 
9.6 
Calculating with Binary Partitions of R T 
473 
9.7 
Calculation of Observable Process in R T 
475 
9.8 
Other Joint-Contingent Observables 
478 
9.9 
Empirical Data 
479 
9.10 Empirical Distributions 
486 
9.11 Calculation of Empirical Distribution 
487 
A Epilogue 
491 
A.l Measurability 
491 
A.2 Historical Note 
501 
Bibliography 
505 
Index 
521 

Preface 
The theory of probability is one of the success stories of twentieth century 
mathematics. Its success was founded on advances in the theory of integration 
associated with Henri Lebesgue, which, in turn, are based on the mathematical 
theory of measure. 
But twentieth century probability theory is constrained by certain features 
of the Lebesgue integral. Lebesgue integration cannot safely be used without 
first mastering the underlying theory of measure—a subtle and difficult subject. 
Furthermore, in Lebesgue integration, as in the Riemann integration that 
it superseded, a function is integrable only if its absolute value is integrable. 
Consequently, some perfectly straightforward functions cannot be integrated 
by Lebesgue's method. This limitation meant that Richard Feynman's mid-
twentieth century discoveries in quantum mechanics, including the theory of 
light for which he received the Nobel Prize, could not be expressed in probability 
terms to which his theory bears a strong formal resemblance. 
A further limitation is manifested in the Itö calculus used in financial math-
ematics and the theory of communication. This is because the Lebesgue version 
of stochastic calculus is relatively complicated and difficult to apply in practice. 
This book overcomes these limitations by formulating probability theory in 
terms of the Stieltjes-complete integral instead of the Lebesgue integral. The 
roots of the Stieltjes-complete integral of this book are in developments in math-
ematical analysis in the 1950s and '60s. 
In the 1950s a new method of integration, using Riemann sums, was discov-
ered independently by Ralph Henstock and Jaroslav Kurzweil. The best-known 
version is the Henstock-Kurzweil integral. In this book the Henstock-Kurzweil 
integral is referred to as the Riemann-complete integral. 
In contrast to Lebesgue theory, Riemann-complete integration uses non-
absolute convergence. In other words, functions may be integrable even if their 
absolute value is not integrable. 
What does this mean? The series 1 — i + i — i~l 
converges if the terms 
are added up in the order in which they appear. But if positive and negative 
terms are first added up separately the series diverges. This is the essential 
difference between non-absolute and absolute convergence. The non-absolute 
method of summation enables cancelations to occur, and many important int-
egrals converge only if their Riemann sums are calculated in this way. 
This is also a key feature of the Stieltjes-complete integral of this book, 
XI 

Xll 
PREFACE 
which enables it to open up new vistas in the theory of probability and random 
variation. These vistas constitute a formulation of probability theory in which 
Lebesgue integration and traditional measure theory are absent. It is a Riemann 
sum approach to the theory of random variation. The concept of measurability 
of sets and functions is expressed in terms of Riemann sums rather than measure 
theory. 
The book opens with elementary numerical calculations of means and var-
iances which demonstrate the role of Riemann sums, and it closes with similar 
numerical Riemann sum calculations of stochastic integrals, Ito's formula, and 
Feynman integrals. The meat in this sandwich is a Riemann sum-based theory 
of random variation using the Stieltjes-complete integral. 
The book has introductory chapters describing the key points of absolute 
and non-absolute integration, and how these differ in dealing with random vari-
ation and probability. It presents in a new, simpler, and more efficient way the 
standard results of probability theory including the laws of large numbers, the 
central limit theorem, and the theory of Brownian motion. 
It provides an account of Feynman integrals within a framework of Brown-
ian motion. One of the striking features of Feynman's theory is its diagram-
matic calculus for analyzing fundamental quantum mechanical phenomena and 
processes—the Feynman diagrams. This book provides an explanation of the 
Feynman diagrammatic calculus and gives conditions which ensure convergence 
of the underlying perturbation series of which Feynman diagrams are a graphical 
representation. 
Also included are a new approach to the Black-Scholes option pricing form-
ula, and a new and simpler formulation of stochastic calculus, including Ito's 
formula. It is shown that many stochastic integrals can be defined in the way 
that ordinary integrals of calculus courses are defined. The reason for this is the 
basic point that the Brownian sample paths, though they have infinite variation, 
have finite integrals when the non-absolute summation method is applied to the 
Brownian increments. 
The main themes of the text are illustrated by numerical calculations using 
the Maple computer program. The gist of the book can be grasped by reading 
the introductory chapters and the concluding numerical calculations. 
The book is fully self-contained in regard to both probability and integra-
tion theory, and it aspires to be accessible and useful to readers who are not 
expert in either field. The title "A Modern Theory of Random Variation" mir-
rors the title A Modern Theory of Integration, a book on Riemann-complete 
integration by Robert Bartle, whose paper Return to the Riemann integral 
(http: //mathdl .maa. org/images/upload _ library/22/Ford/Bartle625-632 .pdf) is 
an introduction to the subject. 
There is a website for commentary on technical issues arising in this book: 
https://sites.google.com/site/StieltjesComplete/ 
Technical communications can be addressed to: 
stieltj es.complete.integralQgmail.com 
Pat Muldowney 

Symbols 
-<, Λ, V : Predicative relations for gauges 7 in R T. 
117 
I5 : Indicator function of a set S. 
43 
(a, 0)-continuity : Continuity of X(S)S£T 
relative to moduli a and Θ. 
290 
BM1-BM7 : Properties of c-Brownian motion. 
305 
C^Q · Set of χτ € R T that are (a, 0)-continuous at r. 
290 
Cle : Set of xT € R T that are (a, 0)-continuous for all t € T. 
291 
Ότ
αθ : The set R T \ C ^ . 
290 
Dle : T h e s e t R T \ C ^ . 
291 
DZ, D ^ : Differentials for partial differentiation. 
347 
dr,d^T : Abbreviated partial differentiation operators. 
346 
£>, ε : Division of a domain or figure. 
42 
δ : Gauge for finite-dimensional domain. 
69 
Τ>δ,ε$ : 5-fine division. 
80 
D 7,£ 7 : 7-fine division. 
104 
d(x) : Dirichlet point function. 
40 
D(J) : Dirichlet increment function. 
178 
E, E F : Expectation with respect to distribution function F. 
24, 185 
Εξτ : Ψ(ζ,τ) = E^r [/(Xp)], marginal density of expectation. 
325 
E : A figure, or finite union of cells. 
39 
E : The union of a figure E and the associated points I* of each I C E. 
57 
f{XT) 
~ f(xT) 
[RT,FXT] 
: Contingent observable. 
22, 184 
(/<*>), (f{x) < v), (f(X) < v) : The set {x e R T : /(x) < v}. 
187 
Xlll 

xiv 
SYMBOLS 
/ ( ^ M ) ? / M ( ^ ) : Function in R M, and corresponding cylinder function. 
280 
f(rq\x),ßrq]^ 
: Binary step function values for a function / in R T. 
328 
fu(xT,N),#q){xT)j£\xT)Ju(xT) 
: Versions of / (JT U(x(t),t)dt). 
335 
F : Distribution function, defined on cells. 
49 
Fx,Fxt,FxT
 
: Distribution function of an observable. 
183 
Fx(s) 
: The characteristic function of X, or Fourier transform of Fx. 
237 
F : Martingale distribution function. 
437 
φ% : Discounting function. 
436 
φα{χ),φη,α{χ) 
· n-Dimensional Fresnel or Gaussian density function. 
264 
φ0(Ι),φη^(Ι) 
: n-Dimensional Fresnel or Gaussian cell function. 
264 
<fin,c(J) - n-Dimensional Fresnel cell function in unbounded cell. 
271 
<7i(x(zs)),...,g$(s,x(ze)) : Stochastic integrands xs> — x8,...,xv
s, 
— xp
s. 
392 
gc(x(N)) 
: Incremental Fresnel density function in R T. 
284 
G^(N,I[N]) 
: Incremental Fresnel distribution function in R T. 
284 
gc9|k, qcqlHK\ 
Gc(Kr^) 
: Values taken by Gc on binary cells of R T. 
326 
g£(x(N)) 
: Fresnel density function in infinite-dimensional R T. 
275 
GC(I[N]) 
: Fresnel distribution function in infinite-dimensional R T. 
275 
QP<J{J[N\) : Geometric Brownian distribution, growth rate p, volatility σ. 314 
7 : Gauge 7 = (L, 8) in RT. 
103 
7-fine : As in 7-fine associated triple in R T. 
103 
= : /i-equivalence, variational equivalence. 
149 
—> : /i-convergence, weak convergence. 
149 
i : The square root of —1. 
73 
z, j : Cells or intervals i n T = ] r ' , r ] . 
51 
/, J : Cells or intervals in R. 
39 
I : The class of cells in a domain. 
44 
J* : The set of points x associated with a cell i\ 
44 
/ : The union of a cell / and its associated points /*. 
57 

SYMBOLS 
xv 
I(N) 
: A cell h x · · · x In of R^. 
86 
I[N] : A cell h x · · · x In x R T\ N of R T. 
86 
k : Permutation of indices for binary partition points in R T. 
95 
Rq\k^ 
Krq\k^ fcrq . Binary partitioning of R T. 
96 
C : Likelihood function. 
25 
t{z) : Lognormal density function. 
191, 309 
L(J) : Lognormal distribution function. 
191, 311 
£(p\z) 
: Lognormal density function with growth rate p. 
312 
I / ( J ) : Lognormal distribution function with growth rate p. 
312 
N : A finite set {ti,... ,£n} of indices t G T; a dimension set. 
84 
Λ/*, Λ/*(Τ) : The class of finite subsets of Γ. 
84 
TV" : J V " = i V \ { i n } = {*i>.--,*n-i}· 
322 
N(J) : Standard normal distribution function, equal to N_'i (7). 
191 
2 
Ν μ σ : Ν ^ , normal distribution, mean μ, standard deviation σ. 
309 
2 
nfJ,(T(y) 
: Normal density function, mean μ, standard deviation σ. 
309 
Ν^σ(7) : c-normal distribution function, with parameters μ and σ. 
191 
N(s) : Fourier transform of the standard normal distribution function. 
238 
Ω : Sample space for random variability. 
183 
u;r<?'k : Expresses fu 
(XT) in terms of binary partition of R T. 
335 
Σ { α / ^ ς ^ : k G tur<?} : Binary Riemann sum for / R T fv
q{xT)GC{I[N\). 
335 
P : Probability function in Kolmogorov probability space (P, A, Ω). 
37 
P : Probability function defined by a Stieltjes-complete integral. 
24 
Ρχ 
: Probability function derived from random variable X. 
24 
V : Partition of a domain of integration. 
39 
p : Projection function. 
86 
wrq,zu,rq,zu-rq 
: Binary partition points in domain R. 
95 
7rt : Risk-free portfolio. 
437 

xvi 
SYMBOLS 
Ψο(ξ,τ), 
^cC^? ^; ^'? τ') : Wave function; marginal density of expectation. 344 
Q : Set of rational numbers. 
51 
R : The set of all real numbers. 
21 
R+ : The set of positive real numbers ]0, oo[. 
69 
R T : Cartesian product of R. 
83 
ΊΖ : Riemann sum functional in stochastic integration. 
390 
p : Riskless interest rate. 
433 
s, S,<S : Observable, strong stochastic, and weak stochastic integrals. 
390 
Ii\N]eKE) · Integral on E with division points labeled x. 
106 
T,T 
: Sets of labels or indices; Τ=]τ', 
τ], ]0,r] or ]0,oo[; T = ]0,i]. 
383 
T- 
: Γ = ] τ ' , τ ] , T~ =y,T[. 
322 
U{xT) 
: β / τ "<*«>* 
315 
U{xT(N)), 
U-c(xT(N)), 
U~c(xT), 
U-q
c{xT), U~c{xT) 
: W-data. 
341 
Uc 
: Step function values ΙΑ~£{χτ)· 
342 
Urqi U r g : Alternative construction of step function U~q
c. 
339 
V : Variation function, corresponding to outer measure. 
64 
Vh[E] : The variation of ft on £; Vh[E] = inf7 V£[£]. 
135 
Vfc[RT] : The variation of 
ft. 
135 
Vfc(5)[J5] : The variation of ft in 5 relative to E\ inf7 VJs/l[£]. 
135 
ν Λ(5) : The variation of ft in 5; inf7 V ^ J R 7 ] . 
136 
Var[/(X)] : The variance of the random variable f(X). 
211 
W : Discounted derivative price martingale. 
437 
x* : The set of cells I[N] associated with x = χτ> 
101 
(x, N, I[N]) : Associated triple in R T. 
103 
x ^ , x r g | k, x : Discrete representation in R M of xT G R T. 
328 
X : Random variable in the Kolmogorov sense; P-random variable. 
245 
XT ~ xT [R T,Fx] : Joint-basic observable. 
184 
Y ~ y[R, Fy] : Elementary observable. 
22 
Z : Discounted price martingale. 
437 

Chapter 1 
Prologue 
1.1 
About This Book 
This is a self-contained study of a Riemann sum approach to the theory of 
random variation, assuming only some familiarity with probability or statistical 
analysis, basic Riemann integration, and mathematical proof. The primary idea 
of the book, and the reason why it is different from other treatments of random 
variation, is its use of non-absolute convergence. The series l + ^ + | + z H 
diverges to infinity. On the other hand, the oscillating series 1 — ^ + ^ — ^Η 
converges—but only on condition that the terms are added up in the order in 
which they are written, without rearranging them. This convergence is called 
conditional or non-absolute. 
What has this got to do with the theory of random variation? Any con-
ception or understanding of the random variation phenomenon hinges on the 
notions of probability and its mathematical representation in the form of prob-
ability distribution functions. The central, recurring theme of this book is that, 
provided a non-absolute method of summation is used, every finitely additive 
function of disjoint intervals is integrable. In other words, every distribution 
function is integrable. 
In contrast, more traditional methods in probability theory exclude signifi-
cant classes of such functions whose integrability cannot be established whenever 
only absolute convergence is considered. Examples of this include: 
■ The Feynman "probability measure" (which is not a measure and not a 
probability)—the probability amplitudes used in the Feynman path inte-
grals of quantum mechanics. This book presents a framework in which 
the Feynman path integrals are actual integrals. In effect, the missing 
pieces of Feynman's original paper [64] are provided here; and then used 
to express Feynman diagrams as convergent series of integrals—as they 
were originally conceived. 
■ The increments in the sample paths of Brownian motion—these have in-
finite variation in every interval, and their integrals (in the usual absolute 
A Modern Theory of Random 
Variation: With Applications 
in Stochastic Calculus, 
1 
Financial Mathematics, 
and Feynman Integration. 
First Edition. By Pat Muldowney 
Copyright © 2012 John Wiley & Sons, Inc. Published by John Wiley & Sons, Inc. 

2 
CHAPTER 1. 
PROLOGUE 
sense) are therefore divergent. But these increments are integrable in the 
non-absolute sense, so the stochastic calculus of Brownian motion can be 
put on a simpler footing. 
Incorporating these innovations in the theory of random variation entails a 
radical reformulation of the subject. It turns out that the standard theory of 
probability or random variation can be simplified and extended provided non-
absolute summation procedures are used. 
Reformulation and extension of the theory involves some changes and re-
interpretations in the standard concepts and notations. Unnecessary changes 
have been avoided, and as far as possible the text is consistent with more trad-
itional versions. Therefore, with due caution and attention to definitions of 
terminology and notation, the text can be read in that spirit. An outline and 
overview are presented in Chapters 1 and 2. 
Chapter 7 is the main part of this book, with Chapter 6 providing intro-
ductory material, and Chapter 8 some consequences. The book presents a new 
sphere of application of probability theory by means of the conception of random 
variation which is elaborated in Chapter 5. 
Ralph Henstock's general theory of integration, as extended in [162] (Mul-
downey, 1987), is the basis for this reformulation of the traditional theory of 
probability and random variation, and is presented in Chapter 4. 
Even though Henstock's theory is different from standard integration theory, 
many of the results are similar. Therefore Chapter 4 can be regarded as a 
kind of appendix to subsequent chapters, providing technical background in the 
manner of many books on probability theory in which measure and integration 
are appended to the main part of the text. Included in this chapter are results 
for non-absolutely integrable functions which are not available in traditional 
integration theory. 
A fundamental modification and extension of the Riemann integral was in-
troduced by R. Henstock and, independently, by J. Kurzweil in the 1950s. In 
Henstock [93] this was designated as the Riemann-complete1 integral. 
The work of Kurzweil has transformed the theory of differential equations— 
see, for instance, Schwabik [129, 207]. Henstock went on to develop a gen-
eral theory of integration [85, 93, 94, 103, 105], which includes as special cases 
the integrals of Riemann, Stieltjes, Lebesgue, Perron, Denjoy, Ward, Burkill, 
Henstock-Kurzweil, and McShane (see [82]). This is the Henstock integral on 
which this book is based. 
The Henstock integral is not so well known as the Lebesgue integral. Also, 
the Riemann sum approach to probability theory is new. Therefore the main 
ideas of this book are introduced in a relatively informal way in Chapters 1 and 
2, while Chapter 3 brings forward some notation and definitions from Chapter 
4, in advance of the fuller exposition of the main theorems and proofs in the 
theory of the integral—the Burkill-complete integral—provided in Chapter 4. 
1This is the Henstock-Kurzweil or HK-integral, also known as the generalized Riemann 
integral, the Kurzweil integral, the Henstock integral, and the gauge integral. 

1.2. ABOUT THE 
CONCEPTS 
3 
Chapter 4 can be read as a stand-alone account of the Stieltjes-complete and 
Burkill-complete versions of the Henstock integral, with emphasis on those parts 
of integration theory which are important in the study of random variation. 
It is possible to get the gist of this book by reading Chapters 1, 2, and 3 
in conjunction with Chapter 9's numerical exploration of observable processes, 
stochastic processes, Brownian motion, and Ito calculus. 
The book contains a new approach to several topics. There have to be good 
reasons for going to the trouble of engaging with a new approach to subjects 
for some of which there already exist tried and tested methods. As the occasion 
arises such reasons are pointed out in the text. 
Much detail is provided in exposition, explanation, commentary, and proof; 
with a view to transparency and, not least, facilitation of error detection, error 
correction, and the like. A degree of repetition is present, for the same purposes. 
The text contains examples which illustrate the material of the text with 
solutions to less difficult issues. They can be regarded as exercises or solved 
problems and can be used as models for devising further exercises and problems. 
The numerical calculations in Chapter 9 are intended to illustrate notation and 
to clarify concepts. Also, as a rich source of insight, motivation, and grounding, 
there is endless scope for further practical numerical exercises of this kind. 
The book builds on the work of numerous authors, many of whom are listed 
in the text and in the bibliography. The generous help of many colleagues in 
bringing the material to publication is gratefully acknowledged. 
1.2 
About the Concepts 
An integrand generally involves a point function f(x) multiplied by an inte-
grator2 function F(I). 
Many treatises on integration focus strongly on the 
properties of /(x), such as continuity and differentiability, or their absence. In 
mathematical analysis the integrator is often taken to be F(I) = \I\, the length 
of the interval /, with less attention given to alternative integrator functions. 
But random variation is not so much concerned with the more difficult man-
ifestations of point function integrands f{x). In this book much more emphasis 
is placed on properties of probability distribution functions F(I). This is one of 
the reasons why the book gives much attention to the properties of variation* 
of interval functions F(I), a concept which it possible to extend distribution 
functions F defined on intervals to outer measure defined on arbitrary sets. 
In addition, the classical form of an integrand function is a product 
f(x)F(I) 
of a point function multiplied by an interval function. But it turns out that 
Henstock integration is most naturally formulated with integrands of the form 
2If the integrator F(I) is additive, but is not the length function \I\, then the integral is 
Stieltjes-type. 
If the integrator is a non-additive function h(I), then the integral is Burkill-
type. 
3The word variation 
(referring to the ranges of actual values taken by a deterministic 
function) also occurs in random variation 
(referring to uncertainty in potential data values 
arising in experiments). To prevent confusion, random variability is substituted for the latter. 

4 
CHAPTER 1. 
PROLOGUE 
h(x,I) which are not necessarily the product of a point function times an interval 
function. 
The wording and symbols used in the theory of random variation, as pre-
sented in this book, are consistent with or similar to those already in general use 
and, for the most part, can be understood in the usual way. A note of caution, 
however. The symbol X is traditionally used to denote a random variable, in 
the sense of a measurable function. But in this book X denotes a mathematical 
representation of an "experiment" for which a range of potential data values x 
is known in advance. And a random variable is a calculation f(X) based on the 
potential data values x. The symbol X will usually denote a process of joint 
observation of several unpredictable occurrences. The occurrences or outcomes 
are actual joint data x, where x is a 'tuple of real numbers such as the observed 
values of an experiment consisting of repeated throws of a die. 
A determination f(X) 
derived from this experiment or joint observation X 
could consist of the value of a payout made on the first occasion when ten succes-
sive sixes are thrown. X can be thought of as an experiment, an "observable", 
or a "random variation". 
Both X and f(X) 
involve potential data, x and /(#), respectively, gener-
ated by an act of measurement—often joint measurement. Thus X (or 
f(X)) 
refers to unpredictable potential data, in advance of actual observation. The 
corresponding x (or f(x)) is the actual datum selected by the process of mea-
surement or observation—in other words, the observed value or occurrence. 
It is therefore helpful to think of X as the experiment, observation or mea-
suring process which selects a datum x. Similarly, f(X) 
represents potential 
data, in advance of actual measurement (or observation or determination), and 
in advance of calculation of a datum f(x). We can think of f(X) 
as consisting 
of potential data in association with their potentialities of occurrence, the latter 
consisting of likelihood that an actual datum x will belong to any set I of poten-
tial values in a sample space Ωχ. Such likelihood or probability will be denoted 
by Fx (I); and it can be thought of as the accuracy potential of the observation 
There is a "before" and "after" aspect to this. There is unpredictability or 
uncertainty before observation, but not after. Therefore part of the meaning of 
x, X (or f(x), f(X)) 
is dependent on the point in time at which they are being 
considered. 
In advance of determination, by measurement or observation, of a datum x 
(or /(#)), we speak of a random variable or observable X (or f(X))] by which is 
meant the potential data values that may be observed, subject to some measure 
Fx of their potentialities or likelihoods or accuracy. 
In light of these various considerations, the expression f(X), 
as used in this 
book, is abbreviation for a notation involving several components: 
■ f(x) represents any deterministic calculation involving a data-value x ob-
served in the experiment; 
■ Ωχ represents the sample space, or domain of potentially observable data-
values x; and 

1.2. ABOUT THE 
CONCEPTS 
5 
■ Fx represents a distribution of probabilities (accuracy potentialities or 
likelihoods) to which potentially observable data-values x are subject. 
Thus, an observable or random variable f(X) 
is denoted by a triple 
f(x)[üx,Fx}; 
and f(X) can be thought of as potential data f(x) in association with their pot-
entialities or likelihoods Fx(J), the latter being the likelihood that the datum 
(or joint datum) x will, in advance of actual observation, belong to any set / of 
potential data-values. The function Fx enables us to quantify the accuracy pot-
ential or degree of unpredictability of prior estimates of datum /(#), in advance 
of actual measurement. 
Since X represents joint observations (possibly infinitely many), stochastic 
processes are subsumed within a general theory of joint variation. 
An important class of stochastic processes, including Brownian motion, is 
defined by the properties of the increments of the process at successive instants 
of time. In the case of Brownian motion, almost all infinite series of successive 
increments diverge absolutely, but all such series are conditionally (or non-
absolutely) convergent. Since the method of summation (or integration) used 
in this book is non-absolute, the stochastic calculus of Brownian motion is sig-
nificantly simplified. 
Other features of this study of random variation may also appear strange, 
initially. For example, random variables are defined here in such a way that 
measurability of the variables is a consequence, and not a pre-condition, of the 
definition. Another unfamiliar aspect of this presentation is that the calculus 
of probabilities, in the usual sense, is not fundamental to it. Instead the basic 
properties of probability are deduced (see Section 5.12) as a consequence of the 
meaning ascribed to random variables. And, in place of probability-measure 
functions defined on measurable subsets of a sample space, the more fundament-
al role is taken by distribution functions defined, not on measurable sets, but 
on intervals. 
When these distribution functions are assumed to take only non-negative 
values the resulting theory is equivalent to the classical or axiomatic theory of 
probability and random variation. But when they are allowed to take complex 
values, a significant extension of the classical theory emerges. Of course, the 
notion that "probability" can manifest itself in anything other than non-negative 
real values is a conceptual challenge; one that is addressed and rationalized at 
various points in the book—in Section 2.16, for instance. 
These amendments to the classical formulation of probability theory make it 
possible to bring the Feynman theory of the path integrals of quantum mechanics 
within the scope of a theory of random variation; and they simplify the theory of 
stochastic calculus. Also, proofs in the basic theory of probability are simplified. 
This is because, instead of P-measurable sets A of a probability space (Ω, A, P), 
probabilities are estimated with finitely additive functions Fx (I) of intervals I. 
It is not the purpose of the book to give exhaustive or in-depth treatments 
of the various themes. Instead, it dwells on the relative simplicity, power, and 

6 
CHAPTER 1. 
PROLOGUE 
versatility of the Riemann sum approach to these subjects. Once the method is 
grasped, it is relatively easy to work out any missing elements. 
1.3 
About the Notation 
Notation for random variables has already been mentioned. Another import-
ant issue is notation for integrals. The integral concept implies the following 
elements. 
■ Domain of integration, for example, 
[0,1], 
[a, 6], 
[a,fc]n, 
R, 
R x R x · · · . 
Traditionally, this is variously written 
JO 
J[0,1] 
Ja 
J[a,b] 
J[a,b]n 
Jn 
^RxRx··· 
and so on. 
■ Expression to be integrated (or integrand), usually involving points 
x and intervals I in the domain of integration. So an integrand could 
have the form f(x)\I\ where x E I and the integrator \I\ is the length of 
the interval I in one dimension. If / is two-dimensional, then integrator 
|/| denotes area of I. Integrals involving \I\ come under the heading of 
Riemann integration. The integrand can also have Stieltjes form 
f(x)F(I) 
where the integrator function F is some additive function defined on inter-
vals of the domain. Or the integrand could be a function f(x)h(I) 
where 
the integrator h is not additive. (With f(x) identically 1, the integral of 
non-additive h(I) is known as the Burkill integral—see [25, 26, 103, 202].) 
Or the integrand could be a joint function h(x,I) 
of points and inter-
vals, a formulation which includes the Riemann, Stieltjes and Burkill in-
tegrands. This suggests a notation of the form J R/i(x,J). Sometimes an 
integrand h(x,I) may, in addition to dependence on x and /, also depend 
on other point and/or interval parameters y and J, say; giving a function 
h(x,y, /, J). In that case the notation J h(x, y, /, J) can be an ambigu-
ous notation for the integral. Which of the parameters are "integrated 
on" ? Which remain fixed4 in the integration? If such ambiguity arises it 
is removed by notation of the form 
px€l* 
Jiei(K 
x£l* 
h(x,y,I,J). 
(R) 
4In [162] (Muldowney, 1987), elements additional to x and / are introduced as Riemann 
sum variables. For the purpose of analyzing random variation, an additional variable of this 
kind is introduced in Chapter 4. 

1.4. RIEMANN, STIELTJES, AND BURKILL 
INTEGRALS 
7 
The meaning of the various parts of this notation is fairly obvious, and 
precise meanings will be given later. But the integral it denotes has a 
meaning different from the following integral: 
ryer 
/ 
h(x,y,I,J), 
Jjei(n) 
in which y and J are "integrated on", while the parameters x and / are held 
constant in the integration. If the integrand h(x, I) is a point function f(x) 
multiplied by an interval function F(I), then the integral of the product 
f(x)F(I) 
may be denoted 
jf(x)F(I) 
or 
Jf(x)dF. 
■ Riemann sums ^2f(x)\I\ 
which approximate to the integral J f(x)\I\. 
Thus, if the domain of integration is the real interval J, the integral 
f f(x)\I\, = f€/ f(x)\I\ 
Jj 
JieKJ) 
rxei* 
IJ 
Jiei(J) 
may be estimated or approximated by Riemann sums 
Σ/(Χ)|/| = Σ { / ( * ) | / | : / € Ρ } , 
where V — {1} denotes a partition of the interval J, and, for each J G P , 
the evaluation point x is contained in I or the closure of i\ 
Riemann sums are the prevailing theme of this book, and a shorter notation on 
the lines of the following is used throughout: 
(V)^2f(x)\I\, 
representing 
£ { / ( x ) | I | : / e V} . 
Occasionally the expression integral of f{x) is used without reference to any 
integrator, weighting function, interval function, or measure. In this case the 
integral should be understood in the traditional way. In other words, integral 
of f(x) should be understood as f f(x)dx, 
Ja f(x)dx, or the like, depending on 
the context. Formally, the integral of f(x) on [a, b] is 
/ f(x)dx, 
= 
/ 
f(x)\I\. 
Ja 
Jlel([a,b]) 
A glossary of symbols is provided in pages xiii-xvi. 
1.4 
Riemann, Stieltjes, and Burkill Integrals 
This section demonstrates simple Riemann sum calculations of Riemann, Stielt-
jes, and Burkill integrals. 

CHAPTER 1. 
PROLOGUE 
Consider JQ f(x)dx with f(x) = Ax3. In basic calculus it is observed that 4x3 
has primitive (or anti-derivative) x4, and the indefinite integral is F(x) = x4 + c 
where c is any constant. Thus basic calculus gives definite integral 
/ 
f(x)dx = [F(x)}l = (1 + c) - (0 + c) = 1. 
(1.1) 
Jo 
If F(x) is written in its incremental or Stieltjes form F(]u,v]) = F(v) — F(u), 
this becomes 
/ ' 
JO 
f(x)dx = F()0,l})=F(l)-F(0) 
= l. 
(1.2) 
This is the calculus integral, also called the Newton integral. 
Example 1 To evaluate this integral by Riemann sums, then, with benefit of 
the preceding calculation (1.1), take 1 as the candidate value5 of the Riemann 
integral, and consider expressions 
1-Σ4χΙ\ΙΓ 
r = l 
|i-0P)J>3|/| 
where V — {Ir} is a partition o/]0,1] with 
J. p 
\(Jip—]■ 
dp 
, 
LLp—Ύ 
^ ^ Jb γ· ^^ Lip* 
-1 γ· \ 
U/p 
LLp—]^, 
for r = 1, 2,..., n, uo = 0, un = 1. Let ε > 0 be given. By uniform continuity 
of the function 4x3 in [0,1], there exists δ > 0 so that, for any interval I = 
]?i, v] C ]0,1] satisfying \I\ = v — u < S, and for any x, y satisfying u < x < v, 
u <y <v, then 
|4x3 - % 2| < e. 
Choose a partition V = {Ir}^=1 
= {]ur-i,ur}}^=1 
satisfying 
\Ir\ = Up — ur-i 
< δ 
for 1 < r < n. Then, by the mean value theorem, for each r there exists yr 
satisfying uT-\ < yr < ur with 
u\ - u\_x = Ay3 (ur - 
Up-i). 
Taking the Riemann sum over the partition V, we have 
n 
1 ^Y^(ui-ut-i) 
and 
A simple application of the triangle inequality (as in Theorem 8) shows that if a pair of 
such "candidates" satisfy the Riemann sum condition then they must be equal. 

1.4. RIEMANN, STIELTJES, 
AND BORKILL 
INTEGRALS 
9 
l-(7>)][>?|Jr| 
= 
u2 
(Ur ~ Ur~l) 
~ 4 x r ( W r ~ ^ r - l ) 
\r=l 
I 
n 
| r = l 
I 
n 
+ Σ (4^ ^ r ~ ^7·-1) ~4a^ ^ r ~ ^-i)) 
| r = l 
I n 
= 
Σ (4^ ^ r ~Wr-^ ~4x^ (Ur ~ ^-i)) 
| r = l 
n 
- Σ l4^ (^r ~Ur-^ ~4x^ ^r ~ ^-i)l 
r = l 
n 
= 
5Z|4^-4a;?|(i/r-Mr_i) 
r=l 
n 
< 
ε y] (ur — ur-i) 
= g. 
r = l 
77MS Zio/ds /or e?;en/ such partition V, so 
f 
Jo 
4x3dx = l, 
or 
/ 
4x3|/| 
J]0,1] 
1, 
as required. 
o 
Thus, in this case, the calculus integral and the Riemann integral give the 
same result. The function defined by (2.13), page 53 of Chapter 2, shows that 
existence of the calculus integral does not guarantee existence of the correspond-
ing Riemann integral. 
Stieltjes integration is "integration of a point function f(x) with respect 
to a point function g(x)". 
Suppose g(x) is a point function defined for real 
numbers x. For intervals I =]w,i>] define the interval function F(I) by 
F(]u,v}) := g(v) - g{u). 
The function F is additive on disjoint, adjoining intervals ]u,v], ]v,w]: 
F(]u,w]) 
= 
g(w)-g(u) 
= 
(g(w) - Φ)) 
+ {g(v) - g(u)) 
= 
F{}u,v]) + FQv,w]). 

10 
CHAPTER 1. 
PROLOGUE 
(Conversely, given an additive interval function F(I), a corresponding point 
function g can be defined by g(x) := F(] — oo, X\). Additivity6 of F ensures that 
g is well defined.) Then the Stieltjes (or Riemann-Stieltjes) integral of / with 
respect to g on ]0,1] is 
/ 
f(x)dg, 
= / 
f(x)F(I). 
JO 
J]0,1] 
Example 2 To illustrate the calculation of a Stieltjes integral using Riemann 
sums, suppose f(x) = 2x2 and g(x) = x2. Then, for I = ]u,v], 
F(I) = 
v2-u2 
[ f(x)dg= [ 
f(x)F(I). 
Jo 
J]o,i] 
Take 1 as the candidate for the value of this integral. To test this candidate 
value, consider Riemann sums 
E/(*r)(fl(«r)-i7K-l)) = Σ 2Xr K " Ul-l) = 
Σ,2*^), 
r=l 
r=l 
r=l 
with a view to establishing a relation 
ι-^^Κ-^ο 
r=l 
< ε 
(1.3) 
for partitions V = {]wr_i, ur] : r = 1,2,..., n}. Choose ε > 0, and note that 
u„ — ur _1 = (u2
r + u2
r_1)( ur — ur _1) = (ul + 
u2
r_1)F(Ir). 
By uniform continuity, δ > 0 can be chosen so that, if ur — ur-\ < δ, then 
\{u2
r Λ-ηΙ_λ) -2x2
r\ 
<ε 
for any xr satisfying ur-\ 
< xr < ur. 
Therefore, for any collection V = {Ir} 
partitioning ]0,1] with \Ir\ < δ for 1 < r < n, 
l-{V)Y^2xlF{Ir) 
J2(W-ut_1)-2x2( 
ur — ur - l ) ) 
j = i 
J2((ul 
+ 
u2
r_1)-2x2)(u2
r-u2
r_1) 
j = i 
< Σ\{{ηΙ + 
ηΙ_1)-2χ2)(ηΙ-ηΙ_1)\ 
3 = 1 
6By Theorem 10, for f(x) constant (with value 1, say), every Stieltjes integrator function 
F(I) is integrable. This fundamental point is a central theme of this book. 

1.4. RIEMANN, STIELTJES, AND BURKILL 
INTEGRALS 
11 
so fQ f(x)dg(x),= 
L· 
χ, 2x2F(I),= 
1. In this case too the Riemann sum cal-
culation (1.3) required that the candidate value 1 for the Stieltjes integral be 
available for testing. The solution to the problem had to be known in advance 
of solving the problem, so to speak. Where did the candidate value come from? 
In this case the integrand 
f{x)F{I) 
= 2x2 (v2 - u2) = 2x2(v + u)(v-u) 
= 2x2(v + u)\I\ 
has a form which is fundamentally similar to the integrand in (1.1). Therefore 
the integral value 1 is worth testing. 
And, as demonstrated, it satisfies the 
required Riemann sum inequality. 
O 
If an interval function F(I) is additive on any finite number of disjoint, adjoining 
intervals i" we designate it as a Stieltjes cell function or Stieltjes integrator. 
The Burkill integral (Burkill [25, 26], Henstock [103]) has integrands of 
the form h(I) which are not additive. 
Example 3 For I =]u,v] let h(I) = Au2v(v — u). 
Then, with u < v < w, 
J =]u,w], I\ =}u, v], I2 =]v,w], we have J = Ii U I2, 
h(J) = 4u2w(w — u), 
h(Ii) = Au2v(v — u), 
h(I2) = Av2w(w — v), 
and h(J) φ h{I\) + hfa). 
A Riemann sum calculation gives L· ^ h(I) = 1. To 
see this, consider a partition V = {]ur-i,ur] 
o/]0,1] (r = Ι,.,.,η), 
so that, 
with 
η = 
we have 
l-(P)5>(I r) 
V 
= 
Σ 
i(Ur 
~ Ur-l) 
- 
4urUl-l(Ur 
- 
ur-l)) 
r=l 
n 
= 
Σ , ((Ur 
+ W ? W r - l + UTU2
r_x + u\_^j 
- 4ΐΖΓΜ*_ι) {ur ~ 
Ur-\). 
Let ε > 0 be given. The expression (v% + u2ur-\ 
+ uru2_x + iiji-i) — ^uru2_± 
is a difference of the functions 
ss+ s2t + st2+t3 
and 
Ast2. 
By uniform continuity in both variables of these two functions, a number δ > 0 
can be found so that 
I (ul + u2
rur-i + uru2
r_x + u;5_i) - 4urul_11 < e 
whenever \Ir\ = ur — ur-i < δ, giving 
η <s^2(ur 
-ur-i) 
=ε 
r=l 
The result follows from this. 
O 

12 
CHAPTER 1. 
PROLOGUE 
Strictly speaking, Burkill integrands h(I) do not contain any element of 
dependence on points x, and depend—in a non-additive way—only on intervals 
(or cells) /. For the purposes of this book, however, it is convenient to extend 
the meaning of Burkill integration to include dependence on points x, so the 
Burkill integrand is 
f(x)h(I) 
or 
h(xj). 
Thus a Burkill integrand can be a product of a point function / multiplied by 
an interval function h. Or it can be an integrand h which depends jointly on 
points x and cells or intervals /. If, further, it is not stipulated that h{I) is non-
additive, then Burkill integrands f(x)h(I) 
include, as a special case, additive 
interval functions of the Stieltjes kind. Viewed this way, Burkill integration is 
a generalization of Stieltjes integration. The latter, in turn, is a generalization 
of Riemann integration, with h(I) = F(I) = \I\. 
Generally speaking, the convention7 in this book is to use a capital letter 
such as F to indicate additive interval functions F(I); while lower case letters 
such as h are used for potentially non-additive interval functions h(I). 
The following is an example of a point-interval Burkill integrand 
h(x,I) 
which is not a product 
f(x)h(I). 
Example 4 For I = ]w, v] and u < x < v, write 
h(x, I) = 2v2 — x(v — u) — u(u + v). 
This function is integrable on ]0,1], with integral value 1. To see this, rewrite 
the integrand as 
h(x, I) = (y2 — u2) + (v — x)(v — u); 
and, with ε > 0 given, take 6 = e. Then, for 
0 = UQ < u\ < U2 < - · - < un = 1, 
η 
1 — 
2^h(xr,Ir) 
with \Ir\ — ur — ur-i < δ, the Riemann sum satisfies 
η 
= 
r=l 
1 ~ Σ ( (Ur ~ Ur-l) + Σ ^ Γ ~ X^Ur ~ Ur-^ 
r-1 
\ 
n 
< 
ε y.(ur 
— Ur-i) 
— ε; 
r = l 
so f]0^h(xj) 
= l. 
O 
7An exception to this convention is to be found in stochastic integration (Chapter 8). In 
that case the incremental, or interval function, form of a point function x(t) is denoted by 
x(]t, s]) = x(s) — x(t), in order to define a Stieltjes integrand with respect to the point function 
x (or with respect to the Stieltjes integrator x). 

1.4. RIEMANN, STIELTJES, AND BURKILL 
INTEGRALS 
13 
Integrable functions do not have to be products of point functions and int-
erval functions. Interval functions h(]u,v\) need not be additive, and need not 
depend explicitly on the numbers u, v or v — u. In fact, h(]u, v]) need not even 
be monotone: it is not required that J D I should imply that h(J) > h(I), as 
the following Bur kill integrand shows. 
Example 5 For I =}u, v] let 
f
l 
if u= h or v = L· 
2 
2> 
0 
otherwise. 
Then 
h(]°>h])+h(]b1])=2> 
M]0,1]) = 0, 
Λ(]0,|]) = 1. 
Let δ be any positive number less than or equal to | . Let 
Ji = ] \ ~ i> \ + f ] ' 
J2 = ] \ - δ> \] > 
and let V\, V2 be partitions o/]0,1] containing I\ and I2, respectively: 
Vl = {···,/!,···}, 
V2 = 
{---,l2,···}· 
Then V2 contains an interval ]\,v], and 
Therefore h(I) is not Burkill integrable on ]0,1] in the basic sense of Riemann 
sums. But in Chapter 2 it is shown that it is possible to constrain the formation 
of partitions V of the domain ]0,1] in such a way that every partition has the 
form Vi. 
(See, for instance, Example 15.) In this constrained system of int-
egration, h(I) is said to be Burkill-complete integrable, with integral 2; that is, 
Jio,i] W) =2. 
O 
Since |/| is an additive function of intervals /, Riemann integrands can be 
taken to be Stieltjes integrands. Also, Stieltjes integrands can be taken to be 
Burkill integrands as presented here. Thus the formulation h(x,I) can represent 
not just a Burkill integrand but also Riemann and Stieltjes integrands. If an 
integrand is Riemann integrable it is Stieltjes integrable; and, likewise, Stieltjes 
integrability implies Burkill integrability. 
In the workings of the examples above, indefinite integrals appear. Letting 
h(x, I) represent, in turn, Riemann, Stieltjes, and Burkill integrands, an indef-
inite integral of h(x, I) is an additive interval function H(J) whose value on every 
interval J equals the integral of h(x, I) on J. The indefinite integral H(I) is thus 
a Stieltjes cell function; and, as a Stieltjes integrand it is itself integrable—in 
the Riemann sum sense—on every bounded interval J, with integral H{J). 

14 
CHAPTER 1. 
PROLOGUE 
0 
1 
2 
1 
0 
for 
for 
for 
for 
for 
I=]u,v], 
v < \, 
J=M], 
I = ]u,v], 
u < \ 
i=\\A, 
I = ]u,v], 
u > \. 
In (1.1) the indefinite integral is F(x) = x4 + c; and, for the same Riemann 
integrand in Example 1, the same indefinite integral is written as the additive 
interval function (or Stieltjes cell function—a cell is an interval) 
F(I)=F(]u,v]) 
= 
v4-u\ 
this being the Stieltjes increment of the point function F(x) for which F'{x) = 
f(x) = 4z3. 
Each of Examples 2 and 3 also has indefinite integral v4 — u4\ while Example 
4 has indefinite integral v2 — u2. In Example 5, the indefinite integral H(I) of 
the Burkill integrand h(x,I) 
does not actually appear in the workings, but a 
few moments' examination should be sufficient to see that the indefinite integral 
in this case is the Stieltjes cell function 
H{I) = { 
Riemann, Stieltjes, and Burkill integrals feature in this book, but mainly in the 
form of Riemann-complete, Stieltjes-complete, and Bur kill-complete integrals. 
This section has focussed on determining the definite and indefinite integrals 
of given integrands. Though it will not feature in this book, another aspect of 
integration is the converse problem of determining an integrand h(x,I) from an 
indefinite integral if (/), or from a differential equation satisfied by an indefinite 
integral. To illustrate simply, if a function F(x) is differentiate then it is an 
indefinite integral of its derivative /(#), = 
F'(x). 
1.5 
The -Complete Integrals 
The Riemann, Stieltjes, and Burkill integrals presented in Section 1.4 are "in-
complete" in various ways. For instance, it is not possible to specify broad 
conditions for which the limit of a sequence of integrands is integrable, with 
the integral of the limit equal to to the limit of the corresponding sequence of 
integrals. This makes it difficult to justify, for instance, differentiation under 
the integral sign, and many other similarly useful calculations on integrals. 
Prom the beginning of the twentieth century the Lebesgue integral has part-
ially remedied this, providing strong conditions under which it is possible to 
take limits under the integral sign. However it was apparent that the Lebesgue 
integral is itself "incomplete" in the sense that, just like the basic Riemann 
integral whose deficiencies needed to be remedied, not every derivative could be 
integrated by the new method. It is possible for a function with an indefinite 
integral to not have a definite integral. This is the case for the function defined 
by (2.13) on page 53 in Chapter 2, which is calculus integrable but not Riemann 

1.6. RIEMANN SUMS IN STATISTICAL 
CALCULATION 
15 
integrable or Lebesgue integrable. In other words the fundamental theorem of 
calculus is not always valid for Lebesgue integration; even though, by definition, 
it is valid for the basic calculus or Newton integral. 
This issue is explored further in Chapters 2 and 4 where it is shown that, in 
the -complete system of integration, an integrand has a definite integral if and 
only if it has an indefinite integral. In advance of that, note the following. 
■ Any interval function which is additive on every finite collection of disjoint, 
adjoining intervals is integrable in a Stieltjes sense based on Riemann sum 
calculation. 
■ Tautologically, every derivative f(x) = Ff(x) has an anti-derivative F(x). 
■ Provided the partitions used to form Riemann sums ^2f(x)\I\ 
are suitably 
constrained (as indicated in Example 5 above), the incremental or Stieltjes 
form of the anti-derivative, F(I) = F(]u,v]) = F(v)—F(u), is an indefinite 
integral for the integrand 
f(x)\I\. 
■ Then the finite additivity (or Stieltjes integrability) of F(I) ensures the 
integrability (i.e., existence of the definite integral) of f(x)\I\. 
Thus, with "constrained" Riemann sum formation the fundamental theorem of 
calculus holds for integrands f(x)\I\. 
Therefore, for integrands f(x)\I\, 
it is 
reasonable to designate this type of integration as Riemann-complete. 
The fundamental theorem of calculus is especially important in areas such as 
differential and integral equations. But it is not so important in investigations 
of random variability, a subject which involves a class of Stieltjes cell functions 
which is broader than the the class of indefinite integrals formed from anti-
derivatives. 
Henstock [93] applied the term Riemann-complete to Stieltjes-complete and 
Bur kill-complete integrands. The reason this book makes a distinction between 
these kinds of integrands is, in part, because of the lesser significance of the 
fundamental theorem of calculus in this subject area, and greater significance 
of other kinds of Stieltjes integrands and Stieltjes integrator functions. 
The evaluations in Section 1.4 show that a key step in integrating any func-
tion is identification of its indefinite integral—an additive interval function or 
Stieltjes cell function. So Stieltjes-complete integration is the link between the 
various kinds of integrand. 
Chapter 4 shows that an integrand f(x)\I\ 
(or 
h(x,I)) 
is integrable if and only if it is "almost" (in some sense) identical to a 
Stieltjes cell function 
H(I). 
1.6 
Riemann Sums in Statistical Calculation 
Elementary statistical calculation is often learned by performing exercises such 
as the following. 
"A sample of 100 individuals is selected, their individual 
weights are measured, and the results are summarized in Table 1.1. 
Estimate 
the mean weight and standard deviation of the weights in the sample." 

16 
CHAPTER 1. 
PROLOGUE 
Weights (kg) 
0 - 2 0 
20- 40 
40- 60 
60- 80 
80 - 100 
Proportion of sample 
0.2 
0.3 
0.2 
0.2 
0.1 
Table 1.1: Relative frequency table of distribution of weights. 
0.3 + 
0.2 
0.14 
0 
20 
40 
60 
80 
100 
Figure 1.1: Histogram for distribution of weights. 
/ 
0 - 2 0 
20- 40 
40- 60 
60- 80 
80 - 100 
F(I) 
0.2 
0.3 
0.2 
0.2 
0.1 
X 
10 
30 
50 
70 
90 
m 
100 
900 
2500 
4900 
8100 
xF(I) 
2 
9 
10 
14 
9 
f(x)F(I) 
20 
270 
500 
980 
810 
Table 1.2: Calculation of mean and standard deviation. 
Figure 1.1 is the histogram for distribution Table 1.1. Sometimes calculation 
of the mean and standard deviation is done by setting out the workings as in 
Table 1.2. The observed weights of the sample members are grouped or classified 
in intervals 7, and the proportion of weights in each interval 7 is denoted by F(I). 
A representative weight x is chosen from each interval 7. The function f(x) is 
x2 since, in this case, these values are needed in order to estimate the variance. 
Completing the calculation, the estimate of the arithmetic mean weight in the 

1.7. RANDOM 
VARIABILITY 
17 
sample is 
5>F(/)=44kg, 
while the variance of the weights is approximately 
Y^x2F(I) 
- (44)2 = 2580 - 1936 = 644. 
The latter calculation, involving Y^x2F(I), 
has the form J2f(x)F(I) 
w ^ n 
f(x) = x2. The expressions Y^xF(I) 
and ^2f{x)F(I) 
have the form of Rie-
mann sums, in which the interval of real numbers [0,100] is partitioned by the 
intervals /, and where each x is a representative data-value in the corresponding 
interval /. Thus the sums 
J2xF(I) 
and 
£ / ( x ) F ( 7 ) 
are approximations to the Stieltjes (or Riemann-Stieltjes) integrals 
/ xdF 
and 
/ f(x)dF, 
respectively; 
Jj 
Jj 
the domain of integration [0,100] being denoted by J. 
1.7 
Random Variability 
If X refers to the potential data-values x arising from an experiment corr-
esponding to a weighing of a single individual member of the population under 
investigation, it can reasonably be declared that the calculation Σ xF(I) above 
is an estimate of the expected value of X, denoted E[X]. The actual datum x 
obtained when the single measurement has been completed is the outcome of 
the experiment. The datum x can also be called an observation or occurrence. 
In that case, each entry in the column headed "Proportion of sample" in Tab-
le 1.1 represents an estimate of the potentiality or probability that the single 
observation x will lie within a particular range of possible values. 
In Table 1.2 a calculation f(x) = x2 is performed on the measured value 
x. Accordingly, denote by f(X) some function of the random variability in the 
experiment X, such as f(X) 
= X2 where x2 = f(x) is the outcome of 
f(X)\ 
and then the calculation Σ f(x)F(I) 
is an estimate of the expected value of 
f(X), 
denoted E[/(X)]. Call f(X) a contingent random variable, dependent on 
the elementary random variable X. 
The expression random variable has been used above without explanation 
or definition. In Kolmogorov's book [123], the expression is used as a synonym 
for experiment. Intuitively "experiments", "trials", or "random variables" can 
be recognized and understood as in the following examples. 
Example 6 Measuring the weight of an individual member of a given popul-
ation. 

18 
CHAPTER 1. 
PROLOGUE 
Example 7 Observing the amount of electric current emitted by a photoelectric 
cell when a beam of light of given intensity is directed on the cell. 
Example 8 Throwing a die and observing which of the numbers 1 to 6 lands 
uppermost. 
Example 9 Throwing a die and noting the square of the number which lands 
uppermost. 
Example 10 Throwing a pair of dice and, whenever the sum of the numbers 
observed exceeds 10, paying out a wager equal to the sum of the two numbers 
thrown, and otherwise receiving a payment equal to the smaller of the two num-
bers observed. 
Example 11 A gambling game in which the gambler pays one cent for each 
successive throw of a single die, and receives a thousand euro if 100 successive 
sixes are thrown. 
In each case there is some experiment or trial involving the observation and 
measurement of some unpredictable value. Underlying factors are the source of 
the unpredictability of the outcome, and this phenomenon is designated random 
variability. 
Example 12 Calculating the maximum value of the end-of-day prices of a bar-
rel of crude oil observed over thirty consecutive days. 
In Example 12, a value is generated by performing a calculation / (the maximum 
value calculation) on 30 observable quantities xi,..., 
£30. So 
f(xu 
..., £30) = max{xi, ■ · ·, x30} 
is the outcome, depending on the unpredictable basic joint outcome # i , . . . , £30, 
each element of the basic joint outcome being itself the elementary outcome of 
an experiment or trial Xj, for each j = 1,..., 30. Thus there are 30 "joint basic 
random variations": X\,..., 
X30 corresponding to observable end-of-day prices 
£ 1 , . . . , £30, and a "contingent (or dependent) random variation" f{X\,..., 
-X30) 
corresponding to the maximum value calculation, and whose value depends on 
the basic joint outcome composed of 30 elementary outcomes. 
Example 11 has a basic joint (or joint-basic) random variation composed of 
an infinite series of elementary basic random variations {Xj} whose observable 
values x j are 1,2, ...,6; and a contingent (or dependent) random variation 
Y = f(Xi,X2, 
X3,. · .)> whose observable value is 
{
1000 if there exists j such that Xj = Xj+i = · · · = #7+99 = 6, 
0 otherwise. 
Example 10 has two "basic random variations" X\ and X2 corresponding to 
the numbers X\ and #2 thrown for each of the pair of dice, and the wager (or 

1.7. RANDOM 
VARIABILITY 
19 
contingent random variation) /(Χι,Χ^) given by the calculation 
{
xi + x2 
if xi + X2 > 10, 
mm{xi,X2/ 
if 
Xi + #2 < 10. 
Example 10 has a contingent random variation f(X) 
where X is a joint-basic 
observable or joint measurement (Χι,Χ^)· Example 12 has /(-X") where X = 
(Xi,..., Χ30). If, as in Example 11, X consists of a joint observation of infinitely 
many values the observable X is traditionally called a process or stochastic 
process. Thus a process can consist of a family (Xt), where each t belongs to 
some infinite domain such as the unit interval [0,1]. 
There is a distinction to be made between joint observation of, on the one 
hand, a finite number of values and, on the other hand, an infinite number of 
values. But in this book both are encompassed in a single theory. 
Thus the intuitive meaning of random variable is, firstly, that it involves the 
generation of a value or datum resulting from measurement (s) or observation(s); 
secondly, in advance of measurement or observation, this value is not certain or 
definite but can be one of a range of possible occurrences or observations; and, 
thirdly, that sometimes it is possible to associate some measure of potentiality 
or likelihood with the possible outcomes or data values that may be observed. 
In other words, in advance of actual measurement or observation, the datum 
can be predicted with a degree of accuracy given by some measure of accuracy 
potentiality or likelihood. 
If the possible outcomes or occurrences are discrete, then the potentialities 
or probabilities are associated with each of the possible values. If the poss-
ible outcomes belong to a continuous domain, as in Table 1.2 above, then the 
potentialities or probabilities are quantities F(I) associated with intervals I of 
possible outcomes of the measurement. Provided the function F is atomic, then 
F(I) can also be used to represent the probabilities of discrete values. 
Thus the intuitive conception of random variation implies a number of el-
ements: 
■ the generation of a value or datum resulting from observation of one of 
■ a set of potential data-values or occurrences combined with 
■ a set of accuracy potentialities or likelihoods. 
The first element will be denoted by a symbol such as x; or by f(x) if some de-
terministic calculation is performed on the measured or observed value x. The 
second element corresponds to the sample space Ωχ for the random variation. 
The third component corresponds to the probability measure (or potentiality dis-
tribution function) Fx(I) for the random variation. The notations for random 
variation adopted in this book make reference to these three elements, with the 
tabular layout and histogram of Table 1.2 and Figure 1.1 as their intuitive basis. 
The sample space corresponds to the original source of the unpredictability 
in the value generated by the experiment. In Example 8 the sample space is the 

20 
CHAPTER 1. 
PROLOGUE 
set Ω = {1, 2,..., 6}. There is often some flexibility in how the sample space 
can be designated. Provided the distribution function value F(I) — | whenever 
the real interval I contains one of the values 1 to 6, then we can, for example, 
take the sample space for this experiment to be the line interval 1 to 6, or the 
whole real line R. Many other choices of sample space are available. Similarly 
in Example 10 the sample space can be taken to be any one of various sets such 
as 
{1,2,..., 6} x {1,2,..., 6}, 
R x R, 
{1,2,3,..., 35,36}, 
or R. 
Specification of the potentiality distribution function for the experiment will 
depend on which set is chosen as sample space. 
An experiment, measurement, or "random variation" can be represented 
or specified by an expression involving factors [Ωχ,^χ], where Ωχ and Fx 
are suitably chosen mathematical constructions which enable us to represent, 
describe, and analyze the random variation in the experiment X. Thus a basic 
random observation or measurement can be denoted by a symbol X and can be 
expressed, in the chosen representation, by 
X~x[ilx,Fx]. 
The random variation in Example 8 could be specified in various alternative 
(but equivalent) ways, such as 
or 
[{i},{Fx({i}=1e}]Li> 
o r 
P , ^ ] ; 
where, in the latter specification, intervals / c R have Fx (I) equal to a sixth if I 
contains just one of the numbers one to six, with Fx(I) equal to zero otherwise. 
In contrast to such a basic random variable X, a random variable 
f(X) 
can be expressed in contingent or dependent form, where some deterministic 
calculation / is performed on the basic observed value x from the sample space 
Ωχ. A contingent random variable is denoted by f(X), 
and written as 
f(x)~f(x)[nx,Fx] 
to specify contingent random variation with outcome f(x). 
The underlying or 
basic random variation involved in this is the basic X ~ ι[Ωχ, Fx]. 
An alternative approach here would be to denote the set of possible outcomes 
or occurrences {f(x) : x G Ωχ} by Ωγ, and deduce a distribution function Fy 
on the intervals of Ωγ, so 
Y~y[QY,FY]. 
Provided Ωy is the set of real numbers R we say that Y is the elementary form of 
the contingent random variable f(X). 
Accordingly, two possible representations 
for the random variation in Example 9 could be 
X^x*[{i},{Fx(i) 
= i}}ll 
(1.4) 
Ωχ 
1 
6 
Fx 
1 
6 
1 
6 

1.8. CONTINGENT 
AND ELEMENTARY 
FORMS 
21 
with underlying X ~ x [{i}, {F(i) = g}] i = 1; or, alternatively, 
y ~ j / [ { l 2 , 2 2 , . . . , 6 2 } , F y ( l ) = F y ( 4 ) = ... = Fy(36) = i ] . 
(1.5) 
The former representation has a "contingent" form (involving the deterministic 
function of squaring an observed basic value i), while the latter has "elementary" 
form y. 
1.8 
Contingent and Elementary Forms 
Now consider an experiment X involving observation of the pair of numbers 
which fall uppermost when a pair of dice is thrown (or when a single die is 
thrown twice). The result is a single outcome x composed of a pair of joint 
occurrences {x\,X2) where x\ is the number falling uppermost for the first die 
and X2 is the number falling uppermost for the second die. Thus X is (XL, X2), 
where Xr is the observation of die r (r = 1,2); with 
Xr^xr[{k}, 
F(k) = ±]6
k=1. 
The joint datum is x = (x\, X2), and experiment can be represented as 
X ~ x [{i}?=1 x {j}«j=1, 
Fx(i,j) 
= JL, i = 1,2,... ,6, j = 1,2,... ,6] , 
the sample space being 
{1,2,...,6}χ{1,2,...,6}. 
With R denoting the set of all real numbers, an alternative way of expressing 
the joint observation is 
X ~ x [R x R, FX(I) = Fx{h x I2) = ^ if h H R = {*}, / 2 n R = {j}} 
where Fx is atomic. Now suppose a single datum is generated from the joint 
observation X by calculating the sum of the two numbers observed to fall 
uppermost when the pair of dice is thrown. The resulting random variable 
f(X) = f(Xi,X2) 
can be represented as follows: 
f(X) ~ f{x) [R x R, Fx(h 
x / 2)], 
(1.6) 
where f(xi +x 2) = xi + #2 (i.e., f(ij) 
= i + j), and 
Fx(h x I2) = ± if hnR={i}, 
7 2 n R = { j } . 
This experiment can also be represented as 
y ^ y [{2,3,..., 12}, 
Fy(2) = ^ , Fy(3) = £ , FY(4) = ±,...] 
. 
Thus, using an atomic form of distribution function, with Fy(J) = -^ if J n R = 
j for j = 2,3,..., 12, the experiment f(X) can be expressed as 
y ~ y [ R , F y ] , 
with j / = /(*); 
(1.7) 

22 
CHAPTER!. 
PROLOGUE 
and we can write 
Y = f(x). 
In representation (1.6), f(X) 
has explicitly contingent form and sample space 
R x R; while (1.7) has elementary form Y with sample space R. In (1.7) the 
contingency or dependence of Y on the joint-basic observation X = (XL, X2) is 
not explicit. Each basic observation Xr is itself an elementary observation since 
its sample space is R for each of r = 1,2. The key relationship between the two 
representations, contingent and elementary, is 
y = f(x) =Xl+x2, 
Y = f(X) = Xx + X2. 
The example demonstrates how this relationship enables the distribution values 
Fy to be deduced from the values of Fx, and vice versa. Also, there is some 
loss of information in converting a contingent form to an elementary form, in 
that the individual components x\ and x2 can no longer be seen. 
This example illustrates an important point in the representation of a ran-
dom variable. Knowledge of the likelihood distribution function enables us to 
glean information about the potential datum values, such as mean and variance. 
In other words, the distribution function carries information about the accuracy 
of estimates of the datum. 
On the other hand, knowledge of data occurrences, obtained, for instance, 
by repeated replication of the experiment or measurement enables us to est-
imate distribution function values, as in Table 1.1 above. And knowledge of the 
functional relationship y = f(x) between different representations of the same 
experiment can sometimes enable us to deduce the corresponding likelihood 
values FY and Fx from each other. 
The function Fx carries information—in advance of occurrence—about acc-
uracy of estimates of the measurement or datum x. And Fy does the same for 
the datum y. The elementary-contingent relationship y — f{x) carries inform-
ation about the relationship between FY and Fx, so the former is deducible 
from the latter. 
The notation is intended to highlight the various perspectives from which 
particular instances of random variability can be viewed. This can be seen 
in (1.6) and (1.7) above. The representation in (1.6) shows the underlying 
random variation as {(i,j)}ij=i, 
with each instance or occurrence having a 
likelihood of 1/36; and the potential data values being then obtained by the 
further deterministic calculation i + j which is shown to the left of the square 
brackets. This is the contingent form. 
The same experiment is represented differently in the elementary-form vers-
ion (1.7). In this representation the manifestations of underlying random vari-
ation (shown inside the square brackets) are the possible totals generated by a 
single throw of a pair of dice. The potential data-values, shown to the left of 
the square brackets, are the same numbers without any further deterministic 
calculation. 
This book makes use of these alternative perspectives in the various areas of 
probability theory. In (1.6) and (1.7) it is easy to deduce the likelihood values 

1.9. COMPARISON 
WITH AXIOMATIC 
THEORY 
23 
Fy from those of Fx. But, for more complicated forms of contingency /, this 
step can be difficult; and it is sometimes necessary to resort to sophisticated 
theory in order to make such deduction. It is helpful to view some of the big 
themes of probability—such as central limit theorems and Ito's formula—from 
this point of view, and this is demonstrated in the text. 
Expressions (1.6) and (1.7) also illustrate those situations where we seek to 
examine features of several (perhaps infinitely many) basic random occurrences 
considered jointly, the occurrences or observations being linked together in some 
way, as when a pair or more of dice are thrown at the same time; or a single 
die is thrown repeatedly, at successive instants of time. Both the elementary 
and explicitly contingent forms of representation are widely used in the study 
of joint random variation, and justification for this is provided in Theorem 82 
of Chapter 5. 
Generally, in this book the contingent representation of joint variation is 
preferred. One reason for this is that, in the contingent representation, it is 
easier to analyze aspects and consequences of the joint variation, such as the 
independence or otherwise of the basic random variables Xr. Conversion of joint-
basic random variability to elementary form involves some degree of concealment 
or masking of information about the joint variability involved. 
Analysis of random variability is concerned with establishing or predicting 
both the datum and its "degree of accuracy"; and relating each one to the other. 
"Degree of accuracy" is given by the distribution function. 
Broadly speak-
ing, establishing the correct distribution function is the key point in specifying 
an observable, and that is why much of this book is concerned with deducing 
the elementary-form distribution functions of observables from their contingent 
form. 
1.9 
Comparison With Axiomatic Theory 
The analysis of random variation in this book is built up from these intuitive 
conceptions. In contrast, the traditional definition of random variable X (or 
f(X)) 
given in many textbooks is that it is a measurable function defined8 on 
an abstract sample space Ω. 
To illustrate the traditional approach, consider Example 4 above—a single 
throw of a single die, the random variable being, intuitively, the observation of 
which of the six sides falls uppermost. Since the outcome is random—that is, 
uncertain or unpredictable—it is possible to envisage or postulate some abstract 
domain Ω which, somehow, generates the various possible outcomes of throwing 
the die. 
Suppose the abstract, mathematical sample space Ω corresponds to a "great 
roulette wheel in the sky", which, for illustrative purposes, has six colors—red, 
green, black, white, pink, and yellow, which determine the real-world outcomes 
In practice, however, the actual sample space is chosen as in the preceding examples. And 
Theorem 76 of Chapter 5 reverses the traditional or axiomatic definition of a random variable, 
by deducing the measurability of a random variable. 

24 
CHAPTER 1. 
PROLOGUE 
of 1 to 6, respectively, whenever the die is thrown. In terms of the axiomatic 
theory of probability, the random variable is the mapping X which makes dice-
throw 1 correspond to red, 2 to green, and so on. Consider the probability 
function P on Ω. First, suppose P is uniform, with P(red) = ^, and so on. 
This corresponds to a fair or balanced die. Now suppose that Ω has a different 
set of probabilities P' defined on it, with 
P'(red) = | , 
and P' (green) = ... = P'(yellow) = ^ . 
This counterposes two different experiments, measurements, or random vari-
ables in the intuitive sense; the first involving a balanced die and the second 
an unbalanced one; corresponding to P and P1'. But formally speaking, and in 
traditional axiomatic terms, we have the same sample space Ω in both cases, 
the same range of values or outcomes n = 1,..., 6 generated by the random 
variation, and hence the same random variable (in the sense of mapping from Ω 
into R). Two intuitively different random variables are, in the axiomatic sense, 
the same. 
Now suppose Ω and P are as described. But suppose we define a different 
mapping X' which sends yellow to 1, pink to 2, and so on. Technically, this is 
a different mapping from X, but it describes exactly the same experiment—a 
single throw of a fair or balanced die. So the formally different (in the axiomatic 
sense) X and X' are intuitively the same random variable. 
But, setting these reservations aside, as long as the probability measure 
P of a probability space (Ω,*4, Ρ) is linked to the sample space Ω in the 
"measurable function f(X)" 
conception of random variable, there is no essen-
tial difference between this conception and the intuitive "set-of-potential-data-
values-linked-with-accuracy-potentiality-distribution" representation f(X) with 
X ~ x[Qx,Fx]. 
As the preceding sections show, the latter approach also allows 
us to easily choose alternative specification of both the sample space Ωχ and 
the potentiality function Fx. 
The relationship between the two conceptions of random variation can be 
demonstrated as follows. If the distribution function Fx (I) is deduced from the 
probability measure P by 
FxW^PiX-^I)), 
then 
E[/PO] = / ί(Χ(ω))άΡ= 
ί 
f(x)dFx, 
where Χ(ω) = x and the latter integral is the Lebesgue-Stieltjes integral. And, 
as will be demonstrated in Section 2.8, the Lebesgue-Stieltjes integral is equal 
to the Stieltjes-complete integral 
f f(x)Fx(I), 
which is the basis of the analysis of random variation presented in this book. 

1.10. WHAT IS 
PROBABILITY? 
25 
1.10 
What Is Probability? 
Probability values are sometimes established by reasoning from the specific de-
tails of actual measurements. For instance, in tossing coins, throwing dice, or 
assessing the motion of a pollen particle released into a gas-filled glass vessel, 
potentialities or likelihoods of particular eventualities may be deduced from 
observation of the behavior and characteristics of the actual phenomena them-
selves. 
This involves an assumption that probability /likelihood is, in some sense, a 
"real" phenomenon; and that it is actually present in the events or measurements 
under consideration—in other words, that it exists, and is "knowable". 
In the traditional axiomatic theory of probability, there is an a priori function 
P defined on an abstract probability space (Ω, A, P) from which we purport to 
deduce the probabilities of actual phenomena by means of mathematical theory. 
Similarly, the Riemann sum approach posits the "reality" or objectivity, in 
some sense, of a notion of "accuracy potentiality"9 or likelihood £, from which 
is deduced a mathematical device—the distribution function Fx. 
This approach may be close to the way in which supposedly objective po-
tentialities or probabilities/likelihoods C are linked to their mathematical man-
ifestation in the form of potentiality distribution functions Fx for actual exper-
iments or observations X. Essentially, 
C[I} = 
FX(I), 
where Fx is somehow estimated or deduced from "real-world" data or real 
physical events in which likelihood C is assumed to exist. This equation is the 
bridge between a supposedly natural manifestation of likelihood or potentiality, 
and its mathematical representation in a context of estimating, measuring, or 
observing a potential datum x. 
In contrast the axiomatic approach has 
FX{I) := / 
dP 
J(xei) 
with probability P postulated mathematically. 
Suppose several experiments Xt are considered jointly. Then there are the 
individual and separate experiments Xt, throws of dice, say; and the joint ex-
periment X = (Xt) with joint outcome x = (a^)—such as throwing 10 sixes in 
10 throws of the dice. Assuming each separate throw has a likelihood £, and 
that the joint measurement or joint observation has its own likelihood, then 
the assumed objectivity or "reality" of C means that the separate and joint 
manifestations of C cannot contradict each other—they are consistent. 
Therefore, in constructing the corresponding mathematical entities Fxt and 
Fx, care must be taken that these too do not contradict each other. In the 
axiomatic theory a similar point applies regarding the postulated probability 
9 As far as possible this book avoids the term "probability", along with its associated symbol 
P, because the burden of meaning they carry in traditional usage may cause confusion here. 

26 
CHAPTER 1. 
PROLOGUE 
space (Ω,*4, Ρ) whenever several random variables Xt have to be considered 
jointly. 
Another complication—one that is not present in the standard, axiomatic 
theory of probability—is that potentiality C is permitted to have negative and 
imaginary values. This is related to those random variation scenarios in nature 
which involve non-absolute convergence, and is discussed further in Section 2.16 
and elsewhere. 
What is probability? No answer to this question is offered in this book. 
But a somewhat broader mathematical conception of accuracy potentiality or 
likelihood is presented here in its place. 
1.11 
Joint Variability 
Understanding of joint variability is one of the primary purposes of the study of 
random variation. Suppose the individuals in the sample of Table 1.1 above had 
their heights measured, in addition to their weights, so there are two random 
variables instead of one. This is described in Table 1.3. 
For i = 1,2, denote the two basic random variations (weight and height) 
by Xi, and their intervals and potentiality distributions by Ii and 
Fx^Ii), 
respectively. Note that the measurements or random occurrences are jointly 
observed. In this case, an observed value in one of the random variations is linked 
to an observed value of the other random variation, since both observations 
pertain to a single individual. Call the pair (X\,X2) 
a joint-basic experiment. 
Then X = (Χχ, Χ2) represents the joint variability or joint estimation of the 
weight and height of a single individual. It is reasonable to ask, for each pair I\, 
J2, what proportion of the sample members jointly have weight in the interval 
Ii and height in the interval I2. In other words, what is the likelihood that, 
after actual measurement of an individual's weight X\ and height X2, the joint 
datum x = (χι,α^) will be contained in the set / = I\ x J2? 
Thus the set / denotes a possible joint outcome of the joint observation X. 
Let the joint potentialities 
F(J), = F(h x I2) = Fx(h 
x h) = 
FX(I), 
correspond to these sample proportions, with twenty possible values correspond-
ing to the twenty possible joint intervals /1X/2. Table 1.4 displays the joint-basic 
Weights (kg) 
Proportion of sample 
0 - 2 0 
0.2 
2 0 - 40 
0.3 
4 0 - 
60 
0.2 
6 0 - 
80 
0.2 
80 - 100 
0.1 
Heights (cm) 
Proportion 
0 - 5 0 
0.3 
50 - 100 
0.2 
100 - 150 
0.4 
150 - 200 
0.1 
Table 1.3: Joint observables. 

1.11. JOINT 
VARIABILITY 
27 
h 
0 - 5 0 
50 - 100 
100 - 150 
150 - 200 
0-20 
0.2 
20-40 
0.3 
40-60 
0.2 
h 
60-80 
Fx(h x J2) 
0.2 
80 - 100 
0.1 
0.3 
0.2 
0.4 
1 o.i 
Table 1.4: A joint-basic observable. 
observable X = (Xi, X2), showing the sample space of potential joint data val-
ues x = (a?i,#2); some of the possible joint events I = (Λ,/2) m the form of 
joint intervals; and indicating the display, or format, of the observable's joint 
likelihood values (or accuracy potentialities) Fx(I). 
The entries for the twenty 
values of Fx{I\ x /2) have been left blank; but the nine marginal distribution val-
ues Fx1(Ii) 
and Fx2(/2) have been included, along the bottom and right-hand 
margins of the table. 
If the height of an individual has no bearing on that individual's weight, we 
would expect that 
Fx(h 
x J2), = F(XliX2)(h 
x J2), = 
FXl(h)FX2(I2). 
If two joint random variations are independent, this property holds for all pos-
sible choices of I\ and I2. 
Two potential events are labeled in Table 1.4, where I\ is the possibility that 
an individual selected weighs between 60 and 80 kg, while I2 is the possibility 
that an individual measures between 100 and 150 cm tall. The joint event I\ x I2 
is the possibility that an individual selected is between 60 and 80 kg in weight 
and between 100 and 150 cm in height. If 
Fx(h 
x J2) =0.2x0.25 
then these two particular outcomes are independent possibilities or independent 
events. 
The marginal potentialities in Table 1.4 are the numbers appearing on the 
right-hand edge (or margin) and along the bottom line (or margin) of the table. 
They satisfy 
FXl(h) = Fx(I1xR), 
Fx2(l2) = 
Fx(Rxh) 
for each 7χ, Ι2. If the joint potentialities were given (i.e., if we filled in the blank 
boxes in the table), then those figures, when added horizontally, should give the 
totals in the right-hand margin; and if added vertically, they should give the 
totals along the bottom margin. This property of joint potentialities is called 
consistency. 

28 
CHAPTER 1. 
PROLOGUE 
200 
150 
100 
50 
~ ~ g 
~ 
20 
40 
60 
80 
100 
Figure 1.2: Cartesian representation of joint-basic observable. 
Independence is a mathematical device or abstraction, rarely or never oc-
curring in practice. In this case, a taller individual is also likely to be heavier, 
so we would expect, for instance, that more than 0.2 x 0.25 of the sample in the 
height range 100 to 150 centimeters will have weight in the range 80 to 100 kg. 
It may happen that these two particular joint events are independent of each 
other; with likelihood of the joint event equal to the product of the two corre-
sponding marginal likelihoods. But independence of the pair of joint observables 
X\ and X2 requires that this relationship should hold for every possible pair of 
joint events I\ and I2, not just for the twenty possibilities displayed in Table 
1.4. While independence of joint observables is entertained for mathematical 
reasons, it seems to be a practical impossibility. 
A geometric sense of the relationships between joint potentialities is conveyed 
by diagrams in which joint observations x — (#1, X2) and joint events (intervals) 
I = l\ x I2 are represented in a system of orthogonal axes. The orientation 
of the vertical axis in Figure 1.2, measuring height, is in the opposite direction 
to the weight column in Table 1.5 which, instead of running upward in the 
geometric manner, runs downward to facilitate numerical calculation. Figure 
1.2 has the usual Cartesian geometric orientation for the ?/-axis. P21 corresponds 
to Fx (l[2) x 4 1 } ) , and P33 corresponds to Fx (l[3) x I&A. 
This diagram shows how the domain of the joint random variables is par-
titioned for Riemann sum calculation of expected value, for instance. Unlike 
the histogram of Figure 1.1, it does not display the distribution function val-
ues, and an act of imagination is required to substitute some visual image of 
Fx (/i m ) x 4 n ) ) for Pmn. 
But Figure 1.3 gives a partial histogram for a pair of joint random variables. 
Even this becomes impractical when there are three or more joint random vari-
ables; and then the helpful histogram description of random variation has to be 
abandoned in favor of purely analytical expressions of the form X ~ #[Ωχ, Fx]. 
For an elementary basic random variation such as the one in Table 1.1, the 

1.11. 
JOINT VARIABILITY 
29 
0 - 5 0 
50 - 100 
100 - 150 
150 - 200 
0-20 
0.06 
0.04 
0.08 
0.02 
0.2 
20-40 
0.09 
0.06 
0.12 
0.03 
0.3 
4 0 - 6 0 
0.06 
0.04 
0.08 
0.02 
0.2 
60-80 
0.06 
0.04 
0.08 
0.02 
0.2 
80 - 100 
0.03 
0.02 
0.04 
0.01 ] 
0.1 
0.3 
0.2 
0.4 
[ 0.1 
Table 1.5: Independent joint-basic observable. 
table and histogram (Figure 1.1) convey a sense of the random variability: the 
potential datum x, the sample space Ωχ, and the values of the potentiality 
distribution function Fx. In other words, all the elements of X ~ 
x[Qx,Fx] 
are displayed, or at least indicated, in the table and histogram. 
It is difficult to provide a similarly intuitive display for joint random vari-
ability. Suppose there is independence in the joint height-weight data of Table 
1.4. In that case the twenty joint likelihoods would be as in Table 1.5. The 
sample space, sample joint data values, and potentiality distribution function 
values on joint interval events can be partially illustrated in a two-dimensional 
histogram, as indicated in Figure 1.3. 
Given joint random variation X = (Xi,X2), 
a random variable contingent 
on (Xi,X2) is 
f(X) = (Xx - E[Xi]) (X2 - E[X2}), 
f(x) = On - E[Xi]) (x2 - E[X2}), 
0.10-
0.08 
0.06 
0.04 
0.02-
0 
20 
40 
60 
80 
100 
Figure 1.3: Part of histogram for two independent joint random variables. 
50τ* 
V^\ 

30 
CHAPTER 1. 
PROLOGUE 
from which is obtained the covariance of X = (Xi,X2); 
the covariance being 
Cov[X] = Cov[(XuX2)} 
= E[f(X)] 
=E[f1(X1)f2(X2)}, 
where 
fj{Xj) 
= Xj-E[Xj], 
j = 1,2. 
For the joint random variable of Table 1.3, the covariance can be estimated by 
choosing a sample value (x\,x2) 
in h x L· fc>r each of the joint intervals /i, I2, 
and then calculating the sum of twenty terms: 
Efteii^Fxih 
x h) 
= 
Σ ( * ι -E[Xi])(x2 -nX2])F(XltX2){h 
x J2), 
= 
Σ (*i " E[Xi]) ( Σ te - E[X2]) Fx(Jx x / 2)); 
where 
E[Xj] = YtxJFXj(Ij), 
j = l,2. 
Again, this estimate of the covariance has the form of a Riemann sum approx-
imation to a Riemann-Stieltjes double integral 
E[/P0] = / 
f(x)dFx] 
Jnxn 
that is, 
E[f(X1,X2)) = J (J f(x2,X2)dF{Xl>x^ 
. 
(1.8) 
If the two joint random variables are independent as indicated in Table 1.5, then 
E[f(X1,X2)} = J (Jf(x2,X2)dFXl\ 
dFX2; 
and the Riemann sum estimate of the covariance can be calculated as 
Σ 
((xi - E[Xi]) ( Σ (Χ2 - E[X2]) FX2(I2)) 
FXl(Ii)) 
, 
which is 
( £ (Xl - nx,]) 
FXl(h)) 
( Σ (Χ2 - E[X2]) FX2(I2)) 
. 
By definition of the expectation E, each of the factors is zero, so independence 
implies covariance zero. 
Example 13 To illustrate numerically, suppose the following sample values are 
used to calculate Riemann sum estimates for the covariance of the joint random 
variables in Table 1.5: 
xi : 10 30 50 
70 
90, 
x2 : 25 75 
125 175. 

1.12. 
INDEPENDENCE 
31 
Then the corresponding sample joint data consist of twenty pairs: 
(10,25) 
(30,25) 
··· 
(90,25) 
(10,75) 
(30, 75) 
· · · 
(90,75) 
(10,175) 
(30,175) 
· · · 
(90,175) 
The expected values of X\ and X<i are obtained from the marginal distributions 
in Table 1.5: 
Άχι\ 
= 
Σ*=ι X[J)FXI 
( / I 0 ) ) 
= 
10 x 0.2 + 
... 
+ 90 x 0.1 
= 
42, 
E[*2] 
- 
Et=i4k)FX2 
(4fc)) 
= 
2 5 x 0 . 3 + 
··· 
+ 1 7 5 x 0 . 1 
= 
90. 
The sample estimate of covariance is then given by the Riemann sum calculation 
Σ;=ι Σ ί = ι (xij) 
~ 4 2) (x2k) - 9θ) F(XuX2) 
(/p) x J<fc)), so independence im-
plies that Cov[(Xi,X2)] is 
t 
(*ω - 42) FXl (&)) 
(f 
(4 f c ) - 90) FXa (/<*>)) . 
3=1 
j 
\fc=l 
/ 
Numerical calculation of the Riemann sum in the format of (1.8) gives 
(10 - 42) x (25 - 90) x 0.06 + · · · + (90 - 42) x (175 - 90) x 0.01 = 0; 
so Cov[(Xi,X2)] = 0, confirming that independence implies covariance zero. Q 
1.12 
Independence 
The notion of joint variability extends to arbitrary collections (Xt)teT of random 
variables, and the notion of independence is extended accordingly. In Table 1.11 
a third measurement—the age of the individual—could be included along with 
joint measurement of an individual's weight and height; giving T = {1, 2, 3}, and 
random variables Xj with intervals 13 and potentiality distributions Fxj (Ij), 
j — 1,2,3. The joint observation X = [Χχ,Χι,Χζ) 
could perhaps be illustrated 
by means of a table like Table 1.4 above, but it is difficult to display three 
variables in tabular format, and when there are more than three variables, tables 
become unmanageable. 
Tables can display a single variable in a vertical direction, as in Table 1.1, 
and two variables can be displayed in vertical and horizontal directions, as in 
Table 1.4. But that is practically the full extent of the tabular format. For three 
or more variables, it is possible instead to use analytic Cartesian formulation, 
and, to a limited extent, the corresponding geometric Cartesian representation. 

32 
CHAPTER!. 
PROLOGUE 
Observations (occurrences or joint data) x = (xi,#2,£3) 
and joint events (in-
tervals) I = Ii x I2 x h can be represented by means of orthogonal axes in 
three dimensions, as in Figure 1.4. Like Figure 1.2, this diagram shows how 
the domain of the joint random variation is partitioned by sets / for Riemann 
sum calculation; and it does not attempt histogram-type display of distribution 
function values 
Fx(I). 
The joint potentialities are 
Fx(I)=F{XliX2iX3)(I1xI2xl3), 
and the triple-joint random variations X = ( X i , ^ ? ^ ) 
a r e said to be indep-
endent if 
F(xux2,x3)(h 
x h x /3) = 
FXl{h)FXl{I2)FXl{h) 
for all possible choices of the intervals i"i, I2, I3 in the domains Ωχ^ of the joint 
random variations. 
In this case there are more marginals: 
Fx^h) 
= Fx(h 
x R x R), 
F(XliX2){I2 
x h) = Fx(h 
x h x R), 
and so on, in various combinations of marginals. As before, consistency condit-
ions apply to these joint potentialities. 
Given a family of joint random variations, sub-families can be independent 
while the family members as a whole are not independent. Here is an illustration. 
Suppose two independent random variables each has potential data values — 1, 
1, with potentialities 0.5, 0.5. The random variation formed by multiplying 
the two potential data values is independent of each of the first two, but the 
Height 
25* 
A S e 
50, 
75r 
200 -
150 -
100 · 
50 -
h 
h— 
f~ 
l·— 
7 
/ 
h_ 
I = 1^ x I2x 
I3 
1 
1 
1 — 
40 
60 
100 
Weight 
Figure 1.4: Framework for joint random variables. 

1.12. 
INDEPENDENCE 
33 
three random variations, considered jointly, are not independent. Expressing 
this more formally, for j = 1, 2 we have Xj ~ Xj [<^xj, Fxj ] with Ωχ^. = {1,-1} 
andF x.(l) = F x . ( - l ) = 0 . 5 . 
The random variable f(X) — f(Xi,X2) 
:= -X1-X2 can be constructed, firstly 
as an observable contingent on the joint observable X = (Xi,^)» a nd sec-
ondly as an elementary observable Y\ Accordingly, X\X2 can be regarded as a 
contingent observable with the following sample space: 
Ω = 
ΩΧι x Ωχ2 
= 
{1,-1} x {1,-1} 
= 
{(1,1), (1,-1), (-1,1), (-1,-1)}· 
The potentiality of each of these four joint observations is, by the independence 
of Xi and X2, 
FXl(i)FX2(j) 
= 0.5x0.5 = 0.25 
for i,j = ±1. The potential outcomes of the contingent observable X1X2 cor-
responding to each of these four joint observations are, respectively, 
1, 
- 1 , - 1 , 1, 
with potentialities, respectively, 
0.25, 
0.25, 0.25, 0.25, 
and expected value: 
E [X1X2] = (1 x 1) x0.25+(l x -1) x0.25-K-l x 1) x0.25 + ( - l x -1) x0.25 = 0. 
The distinct outcomes of f(X) = X\X2 are 
1, 
- 1 
with potentialities 
^{FXl(i)FXa(j) 
: ij = 1} = 0.5, 
Y^{FXl(i)FXi{j) 
'■ V = -1} = 0.5, 
respectively; thus establishing that the contingent random variable f(Xi, X2) = 
X\Xi 
can be expressed10 as an elementary random variable Y whose sample 
space is 
ς\γ = Ω = {1, -1} 
with potentialities 
Fy(l) = 0.5 and FY(-1) 
= 0.5 
10 Theorem 82 of Chapter 5 deals with pointwise equivalence of different representations of 
random variables. A weaker, non-pointwise form of equivalence is introduced in Theorems 44 
and 236. 

34 
CHAPTER 1. 
PROLOGUE 
and expected value 
E[Y] = 1 x 0.5 + (-1) x 0.5 = 0, 
giving 
E[X1X2]=E[Y], 
where each of the expected values is calculated using a different sample space 
and different set of potentialities. 
The joint outcomes of X\ and Y are 
(1,1), (1,-1), (-1,1), (-1,1) 
with potentialities 0.25 in each case, so X\ and Y are independent. Likewise 
X2 and Y. The joint outcomes, with potentialities, of X\,X<i,Y 
are: 
Joint outcome 
(ϊΧϊ) 
(1,1,-1) 
(1,-1,1) 
(1,-1,-1) 
(-1,1,1) 
(-1,1,-1) 
(-1,-1,1) 
( - 1 , - 1 , - 1 ) 
Potentiality 
0.25 
0 
0 
0.25 
0 
0.25 
0.25 
0 
Independence of X\, Χ2 and Y would require that the potentiality of each of 
the joint "possibilities" listed in this table should equal 
FXl(i)FX2(j)FY{k) 
= 0.5 x 0.5 x 0.5 = 0.125, 
for i,j,k 
= ±1. This is not the case. In fact, four of the eight "possibilities" 
or joint "outcomes" listed are not actually possible. Thus Χχ,Χ^,Υ 
are not 
independent. 
Note that if Y is denoted by X3, then we obtain a joint observable X = 
{X\,X<i,X$) with representation 
X~x[Slx,Fx], 
where x — (#1, #2? #3) £ Ωχ. The sample space fix can consist of the outcomes 
in (1.9). Or we can regard the sample space as R x R x R with the outcomes 
of (1.9) embedded in it. The potentiality distribution function Fx consists of 
the potentialities of (1.9) or some equivalent in R x R x R. Then each basic 
Xj can be represented in contingent form by 
X^fiiX), 
xj = fj(x), 
j = 1,2,3, 
where each x = (xi,X2,xz) 
is a joint outcome in (1.9), and the potentiality of 
Xj = a occurring is 
^2{Fx(x) 
: fj(x) = fj(xi,X2,X3) 
=Xj = a } . 

1.13. STOCHASTIC 
PROCESSES 
35 
Thus the contingent variables fj(X) 
have representations 
fj(X)^fj(x){ilx,Fx], 
j = 1,2,3, 
which are equivalent to the representations 
Xj-Xjlilx.tFx.], 
j = 1,2,3. 
1.13 
Stochastic Processes 
For larger (and infinite) collections of joint observables, the Cartesian represen-
tation is more difficult to illustrate in a diagram, but the concepts follow the 
pattern already evident. Suppose we have a joint-basic variation 
X = (Xt)teT 
of basic observables Xt where each t belongs to a (finite or infinite) set T. Then 
the classes of joint events that should be considered are the possibilities that, 
for any finite n and any selection of 
^1 ·> · · - i ^ηι 
the observed values (or joint data-values) xtl,..., 
Xtn of Xtl,..., 
Xtn satisfy 
xtj e Itj, 
1 < j < n, 
which is an instance of a class I of events i" that can be denoted by 
/ = Iu X . · . X Itn X R ^ i i l 
*n}) 
(1>10) 
with potentialities {Fx(I)}. 
The observables Xtj t G T are independent if, for 
each choice of n, ίχ,..., tn, and, correspondingly, all choices of It , we have 
n 
Fx(I) = 
l[FXj(Itj). 
3 = 1 
This definition is consistent with the previous definition of independence of joint 
observables when T consists of just two elements or three elements. 
In a sense, when T is an infinite set, all potentialities are marginal, since an 
infinite product 
oo 
3 = 1 
will usually be zero, so it is more useful to consider potentialities in the joint 
cylindrical intervals I of (1.10) above. Since the potentiality of any cylindrical 

36 
CHAPTER!. 
PROLOGUE 
interval is marginal, consistency conditions apply to all of these joint potentiality 
values. 
A random variable in this context is a real- or complex-valued function 
f(X) 
of the joint random variation X = 
(Xt)teT, 
f(x)*f(x)[nx,Fx(i)], 
where I E I, and, with x = (xt)teT = Χτ € R T = Ωχ, the set of possible values 
of f(X) 
is 
{f{{xt)teT):xtenXt, 
teT}. 
(l.ii) 
Table 1.2 describes a procedure for calculating the expected value of a random 
variable involving a finite number of joint observations (Γ finite). It might be 
anticipated that, with T infinite, the expected value E[/(X)] could be estimated 
by a similar Riemann sum calculation: 
£ / ( ( s t ) t e r ) **(/), or 
J2f(x)Fx(I), 
where the joint intervals I (cylindrical intervals (1.10)) partition the domain 
(1.11) of the random variable. 
In the following chapters the ideas outlined above are developed in more 
detail. One objective, as indicated earlier, is to present the main results of 
probability theory in a Riemann sum format. In other words, the aim is to use 
Riemann sums (corresponding to the intuitive approach of Tables 1.2 and 1.4 
above) and Riemann sum-based integration, instead of the traditional measure 
theory and Lebesgue integration. 
Another objective of this book is to formulate the Feynman path integral 
theory of quantum mechanics as a branch of probability theory. The formal 
similarities between the Feynman theory and the theory of Brownian motion are 
well known. It was demonstrated by Muldowney [162] that these two theories 
could be expressed in a common framework of Stieltjes-type integrals using 
Riemann sum constructions. In this book these subjects are formulated in 
a probability framework based on the Stieltjes-complete and Burkill-complete 
versions of the Henstock integral [94]. 

Chapter 2 
Introduction 
This chapter contains an outline of a Riemann sum theory of random variability. 
Mathematical studies of random variation usually start with probability. In 
the traditional axiomatic approach these studies start with probability spaces 
(Ω, *4, P) where Ω is a sample space, Λ is a sigma-algebra of measurable subsets 
of Ω, and P is a probability measure on Ω relative to A. 
In contrast, Chapter 1 introduces entities X ~ x[ilx,Fx] 
and f(X) — 
f(x)[ftx,Fx], 
called observables, as a basis for the Riemann sum approach of 
this book. 
To begin with, for simplicity, we take a sample space Ω = [0,1], the unit 
interval. In fact, though it is not always the simplest or most convenient choice 
of sample space, most random variables can be represented with [0,1] as sample 
space. For example, in an experiment consisting of a single throw of a fair 
die (Example 8, Chapter 1), the sample space [0,1] can replace {1,2,...,6}, 
provided the potentiality distribution function is defined as follows: 
Fx(i) = l if /n{i,§,...,§} = §, i = i,...,6, 
with Fx(I) 
= 0 otherwise. Wiener [233], represented Brownian motion with 
[0,1] as the sample space.1 
In the Riemann sum approach, any observable X, joint or otherwise, is 
predicated on the existence of a potentiality function Fx defined on intervals I of 
the sample space Ω of the observable. For the purpose of the present discussion 
it is assumed that a distribution function Fx is defined on subintervals of a 
sample space Ωχ = [0,1]. Then Fx is a probability function on subsets of the 
sample space Ωχ, just as in traditional probability theory. It is easy to extend 
the domain of Fx to include figures—that is, finite unions of intervals in [0,1]. 
But, unlike traditional probability theory, in the Riemann sum approach there 
is no a priori requirement that Fx be defined on, for instance, Borel sets in 
[0,1]· 
1This particular construction of Brownian motion is also described in Muldowney [162]. 
A Modern Theory of Random 
Variation: With Applications 
in Stochastic Calculus, 
37 
Financial Mathematics, 
and Feynman Integration. 
First Edition. By Pat Muldowney 
Copyright © 2012 John Wiley & Sons, Inc. Published by John Wiley & Sons, Inc. 

38 
CHAPTER 2. 
INTRODUCTION 
In the axiomatic approach, if X is a P-measurable function defined on the 
sample space [0,1], X is declared to be a random variable, with expected value 
of X defined as 
f XdP= [ 
jQx 
JO 
E[X}= 
/ 
XdP= 
/ XdP, 
(2.1) 
JQx 
JO 
where the integral is interpreted in the Lebesgue sense. 
In the Riemann sum approach presented here, the observable X is taken 
to be a random variable if its expectation exists, where the expectation is a 
Stieltjes-type integral 
E[X] = / 
xFx(I), 
(2.2) 
the exact meaning of which will be given in Chapter 4. The functions P and 
Fx generally have the same value on certain subintervals I of [0,1]. Also the 
integral (2.1) corresponds to the Lebesgue-Stieltjes integral 
/ ' 
Jo 
E[X] = / xdFx. 
(2.3) 
Jo 
When a definition of (2.2) is presented (Definition 5), it will be found to coincide 
with (2.3), and hence with (2.1). Therefore, at this level, the Lebesgue or 
Kolmogorov approach is equivalent2 to the Riemann sum method of this book. 
But surprisingly, the Riemann sum approach actually goes much further when 
deeper problems of stochastic integration and random variability in quantum 
mechanics are tackled. This is demonstrated in, respectively, Chapters 8 and 6. 
The purpose of this chapter is to motivate the Riemann sum approach to 
random variation and to indicate how it relates to the traditional Lebesgue or 
Kolmogorov approach. 
A more detailed demonstration is given in Chapter 5, along the following 
lines. Suppose X is a random variable defined in the traditional way by means 
of a probability space (Ω, A, P). For simplicity, take Ω = R. Then, for A G *4, 
Ρ(Α) = Ί>χ[Α], 
where Ρχ[^4] is defined as L· - 1A(X)FX(I); 
the latter being a Stieltjes-complete 
integral constructed from Riemann sums—without reference to the probability 
space (Ω,*4, P). It will also be shown that if Ai G A are disjoint with A = 
\JiilAii 
t h e n 
/ 
l(x)AFx(I) 
= 
j r i 
lAi(x)Fx(I), 
or 
'X IM 
= Σ,ΡχΙΑ]. 
(2.4) 
This equation, which is a fundamental axiom in the Kolmogorov approach to 
random variation, is a theorem in the Riemann sum approach, and it is obtained 
without reference to the probability space 
(Ω,Α,Ρ). 
2 See Theorem 122. 

2.1. RIEMANN SUMS IN 
INTEGRATION 
39 
2.1 
Riemann Sums in Integration 
Suppose / is a real- or complex-valued function defined on [0,1] and suppose, for 
intervals / C [0,1], \I\ denotes the length of 7. Thus, for I = ]u, v] (or I — [u, v[, 
]u,v[, or ]u, v]), we have 
\I\ = v — u. 
Prom this point onward, for the purposes of integration we will deal almost 
exclusively with a particular class of intervals called cells. Cells are intervals 
used for partitioning a domain of integration in order to construct Riemann 
sums. 
Definition 1 For integration on [a, 6], a cell is an interval of the form [a,u] 
or ]u,v] where a < u < v < b. For integration on R =] — oo,oc[, a cell is an 
interval of the form ] — oo, b] or]u,v] or ]a, oo[ where u < v. A cell will often 
be denoted by the symbol I. A figure is the union of a finite number of cells, 
often denoted by E. 
The Riemann integral of f with respect to x on the domain [0,1] can be 
defined as follows. Choose iti, ^2, ·.., un-\ 
with 
0 = UQ < u\ < U2 < - - - < un-i 
<un — l 
so that Μι,ι/2,... ,ΐΛη-ι form a partition of [0,1]. The partition can also be 
expressed as a finite collection V of disjoint cells I: 
V = {I} = {[0,m],]txi, w2],.. · , K - i , 1]} · 
Given δ > 0, consider partitions satisfying 
\I\ =Ui- 
ui-i < 5, 0 < i < n. 
(2.5) 
If there exists a number a and, for every ε > 0, a positive real number δ so that, 
for every partition of [0,1] satisfying (2.5), we have 
Y^f{xi)(ui 
-Ui-i) 
-a 
\^{f(x)\I\:IeV} 
< £ 
for all x — Xi such that U{-\ < Xi < i ^ , 0 < i < n , then / is Riemann integrable 
on [0,1], and we write 
/ f(x)dx = a. 
Jo 
The Riemann sum can also be written Σν f(x)\I\ or (V) Σί(χ)\Ι\· 
The num-
ber a is the definite integral. 
As a function of cells I in [0,1], |/| is a probability distribution function 
(the uniform distribution on [0,1]), with (V) Σ 1^1 = 1· K F(I) ^s a n arbitrary 
probability distribution function, with (V) Σ ^(^) — 1? tlie- above definition can 
be adapted to give the Riemann-Stieltjes 
integral of f(x) with respect to F: If 

40 
CHAPTER 2. 
INTRODUCTION 
there exists a number a and, for every ε > 0, a positive real number δ so that, 
for every partition V of [0,1] satisfying (2.5), we have 
(V)^f(x)F(I)-a 
< ε 
for all x such that x\-\ < x < Xi, 0 < i < n, then f is Stieltjes integrable on 
[0,1] with respect to F, and we write 
I 
1 
f(x)dF 
= a. 
/o 
Suppose f(X) — f(x) [Ωχ, Fx] is a contingent observable with basic observable 
X~x[Qx,Fx], 
Ωχ = [0,1]. 
Then, as described in Chapter 1, from a partition V of [0,1] the corresponding 
Riemann sum 
(V)J2f(x)Fx(I) 
yields an estimate of the expected value of f(X), 
and it is reasonable to define 
the expected value to be 
E [ / W ] = / 
f{x)dF: 
Jo 
X 
Jo 
if this Stieltjes integral exists. 
In accordance with the intuitive notions outlined in Chapter 1 
on the basis of Tables 1.2 and 1.4, we will consider an observable 
f(X) to be a random variable 
if its expected value can be estimated 
appropriately by Riemann sums (Ρ)Σ 
f(x)Fx(I). 
Historically, however, it was found that the Riemann-Stieltjes integral was 
mathematically inadequate as a basis for further mathematical analysis. The in-
adequacy is illustrated by the following example, known as Dirichlet's function. 
Suppose X ~ x[nx,Fx) 
with Ωχ = [0,1] and FX(I) 
= \I\. If E[X] is defined 
by the Riemann integral J0 xdx, then X is a random variable with Έ[Χ] = \· 
Now suppose a function d (the Dirichlet function) is defined by 
i/ \ _ / 1 
if x is rational, 
, 
, 
^ ' 
1^0 
if x is irrational. 
^ ' ^ 
Consider the observable d(X) ~ d(x)[nx,Fx] 
with Qx = [0,1] and FX(I) 
= 
\I\. If the evaluation point x in each / G V is taken to be a rational number 
then the Riemann sum (V) ^ d(x)|J| yields a total of 1, while if each of the x's 
used in the Riemann sum is irrational then the Riemann sum yields a total of 0. 
Thus d(x)|7| is not Riemann integrable, so the observable d(X) fails to qualify 
as a random variable in this tentative Riemann sense. 
It might seem that the failure of the Riemann integral for the Dirichlet 
function—which is rather exotic, without obvious practical application—should 

2.2. THE -COMPLETE INTEGRALS 
IN DOMAIN ]0,1] 
41 
not deter us from the intuitive Riemann sum approach. But condition (2.4)— 
countable additivity of the probability measure P (or Ρχ)—is used in developing 
much of the standard theory of probability and random variation, such as laws 
of large numbers and central limit theorems; and the basic Riemann integral 
does not give equation (2.4). 
Such considerations indicate that something more than the basic Riemann or 
Riemann-Stieltjes integral is needed. And "exotic" functions such as Dirichlet's 
are very useful3 in testing out systems of mathematical analysis to show whether 
or not they can deliver an effective theory. 
One way ahead is to take (2.4) as an axiom and use it to define Lebesgue 
and Lebesgue-Stieltjes integrals. This is the standard Kolmogorov approach. 
But another way is to adapt the Stieltjes definition in accordance with the 
Stieltjes-complete theory, in which (2.4) is not axiomatic but is deduced as a 
theorem. 
These two alternative approaches give equivalent results for the standard 
probability theory, in which neither approach is essentially "deeper" than the 
other. But there is an extension of probability into the mathematical theory 
of quantum mechanics which can be handled by the Riemann sum or Henstock 
approach, and where the Lebesgue/Kolmogorov approach fails. This is discussed 
in Chapter 7, and a flavor of it is given later in this chapter. 
2.2 
The -Complete Integrals in Domain ]0,1] 
Riemann-complete4 integration, developed independently by J. Kurzweil and 
R. Henstock, is obtained by an apparently small modification of the definition 
of the Riemann integral. 
Consider a Riemann sum (V)^2f(x)\I\^ 
I =]u,v], u < x < v, where the 
finite collection V of adjoining, disjoint intervals I form a partition of the domain 
of integration. In basic Riemann integration, the only condition on the partition 
is that \I\ < δ for each I G V. Provided this condition is satisfied for each 
interval or cell / of the partition, we are free to select each of the evaluation 
points x of the Riemann sum to lie anywhere in /, the closure of /. 
In effect, provided the constraints \I\ < δ and x G I are satisfied, in forming 
a Riemann sum the cells / and the corresponding evaluation points x can be 
chosen independently of each other, with no further conditions applied to them. 
But what if additional conditions are applied? There are many further con-
straints by which the choice of / can be made contingent on the choice of x, 
above and beyond the two requirements that x G I and \I\ < δ. In the formation 
of a Riemann sum the choice of each of the cells / of the partition V of [0,1] 
can be made to depend on the corresponding evaluation point x of the func-
tion /. Also, in the formation of Riemann sums, constraints can be placed on 
3See, for example, Theorem 67. 
Integrals equivalent to the Riemann-complete integral of Henstock and Kurzweil were 
developed, using different methods, by A. Denjoy (1912) and O. Perron (1914). See Gordon 
[82]. 

42 
CHAPTER 2. 
INTRODUCTION 
other parameters or variables in the integrand. In the course of this book (see 
Chapter 4) extensive use is made of ideas of this kind, which can be described 
collectively as Henstock integration or the Henstock integral 
The additional 
constraint which is most important for immediate purposes yields the basic 
generalized Riemann construction of Henstock and Kurzweil—the Riemann-
complete integral. To describe this first additional constraint, suppose V — {1} 
is a partition of Ω = [0,1], and suppose, for each I — Ir =]ur-i,ur] 
G V, we 
have x = xr contained in the closure I of /; that is, ur-\ 
< x — xr < ur for 
1 < r < n. Denoting by V the family of pairs 
{(x,I) = (xrJr) : 1 < r < n}, 
we say that V is a division of Ω = [0,1]. Given a cell / of [0,1], if a point is 
contained in the closure of /, x G Ϊ, we say that x and / are associated, and 
(x, I) is an associated point-cell pair. Let δ be a positive function defined on the 
sample space. Thus, for Ω = [0,1], 
δ: [0,1] H>]0,oc[, 
χ->δ(χ) 
> 0. 
Definition 2 If x and I are associated, and if \I\ < δ(χ), we say I is (5-fine or 
(x, I) is δ-fine. 
Definition 3 A 5-fine division of [0,1] is a finite collection of associated point-
cell pairs V = {(#, /)} such that 
■ each (x, I) is δ-fine and 
■ the cells V = {I : (x,/) G V} form a partition of I. 
These ideas make it possible to define the Riemann-complete integral. 
Suppose a function / is defined on the domain [0,1]. For each division 
V = {(xl, Ρ)}2=ι of [0> 1]> form the corresponding Riemann sum 
n 
^ / ( a v ) I U 
denoted by 
(V)^f(x)\I\. 
r=l 
Definition 4 The function f is Riemann-complete 
integrable on ]0,1], with 
definite integral a, if whenever ε > 0 is given there exists a gauge δ so that, for 
every δ-fine division V of [0,1], the corresponding Riemann sum satisfies 
| ( Ί > ) Σ / θ Γ ) | Ι | - α | < ε . 
(2.7) 
If / is Riemann-complete integrable on ]0,1] with integral a, write 
a= [ 
f(x)\I\. 
J]o,i] 
If it is clear from the context that the Riemann-complete integral is used, then 
the integral can be written as L· χ, f(x)dx or JQ f(x)dx, 
or simply JQ f. 

2.2. THE -COMPLETE INTEGRALS 
IN DOMAIN ]0,1] 
43 
Suppose F(x) is a point function. Then, for 0 < u < v < 1 and I =]u,v], 
the Stieltjes or incremental form of F is defined by: 
F(I) = F(}u,v}) = F(v) - F(u); 
so F(I) is additive on bounded, disjoint, adjoining cells /. 
If / is defined on the domain [0,1] and if F is an additive function of the 
cells / of [0,1], then for each division V — {(xr,Ir)}™=l 
of ]0,1], form the 
corresponding Riemann sum 
n 
Σ f(xr)F(Ir), 
denoted by (V) £ 
f(x)F(I). 
r=l 
Definition 5 The function f(x)F(I) 
is Stieltjes-complete integrable on ]0,1], 
with definite integral a, if whenever ε > 0 is given there exists a gauge δ so that, 
for every δ-fine division V of [0,1], the corresponding Riemann sum satisfies 
(V)J2f(x)F(I)-a\<s. 
(2.8) 
If f(x)F(I) 
is integrable in this sense, we say that / is F-integrable, and we 
write a — L· ±, f(x)F(I); 
or, if the meaning is clear, a = f0 
f{x)dF. 
If a function h is defined on cells /, but is not necessarily additive, then the 
Burkill-complete integral is defined in the same way. 
Definition 6 The function f(x)h(I) 
is Burkill-complete integrable on ]0,1], 
with definite integral a, if whenever ε > 0 is given there exists a gauge δ so 
that, for every δ-fine division V of [0,1], the corresponding Riemann sum sat-
isfies 
\{p)Ydf{x)h{I)-a\<e. 
(2.9) 
Writing L· χ, f(x)h(I) 
= a, we say that / is /i-integrable on ]0,1], if the integral 
exists in this sense. 
Taking h(I) = F(I) and h(I) — \I\ respectively, Definition 6 includes Defi-
nitions 5 and 4. If f(x)\I\ is Riemann-complete integrable then, trivially, it is 
also Stieltjes- and Burkill-complete integrable. If a function f(x)h(I) 
is Burkill 
integrable in the basic sense then it is Burkill-complete integrable. Likewise for 
basic Riemann and Stieltjes integrable functions. 
Intervals of the form ]u,v] are cells. If, additionally, intervals of the form 
[0, v] are admitted as cells, then the domain [0,1] is itself both a cell and a 
figure. Partitions have been defined only for the domain. But, for any given 
cell or figure in the domain, we can define partitions of the cell or figure, and 
hence define the integral on any such cell or figure. The cell function h{I) in 
Definition 6 is called the integrator. For Riemann and Stieltjes integrands, the 
integrators are |/| and F(I), respectively. 
Partitions are not defined for any subset S of the domain other than cells 
or figures. For any set S let the indicator function I5 be defined by ls(x) — 1 

44 
CHAPTER 2. 
INTRODUCTION 
if x G £, ls(#) = 0 otherwise. Then, given any subset S of [0,1], define the 
integral of / in 5, with respect to /i, to be the integral of f(x)ls(x)h(I) 
on 
[0,1]: 
/ f(x)h(I) := / 
f(x)ls(x)h(I) 
(2.10) 
J S 
J[0,1] 
whenever the latter integral exists. 
If the integrand f(x)h(I) 
contains parameters other than x and /, then, to 
avoid ambiguity, it may sometimes be helpful to use a notation such as 
rx£l* 
/ 
f(x)h(I) 
JieWA]) 
to signify that the Riemann sums that approximate the integral are 
Σ{/(χ)Ιι(Ι):(χ,Ι)€ν}. 
The expression I([0,1]) denotes the class of cells in [0,1] used to partition the 
domain; and I* denotes the set of possible division points x associated with any 
partitioning interval / in the Riemann sum. 
The division point x is a component of an associated point-cell pair 
(x,I). 
In the preceding definitions of -complete integrals, the division point x of (x, I) 
can be any point in the closure of /, so I* = /. But sometimes it is convenient 
to use some class I* of division points x which does not coincide with /; and 
that is the approach that will be adopted in this book. 
There is a multifunction relationship between points x and cells I of associ-
ated pairs (x, I) in Riemann sums. Thus each division point x can have many 
different associated cells /, and each partitioning cell / can have many different 
associated division points x. 
In the preceding definitions, the division points (sometimes called tags or 
tag-points) x of divisions V = {(#,/)} are used as the evaluation points of 
the function / in the corresponding Riemann sums. But other arrangements 
of the integrand are possible. Suppose f(x) = 3x2. Then, as in Example 
3, for / =]i/,i;] the integrand f(u)\I\ has as evaluation point for / the left 
hand end point of /, so the division point x need not necessarily coincide with 
the evaluation point. In fact, despite the presence of a term |/|, the integrand 
/(iz)|/|, = 3u2(v — u), is not a Riemann integrand; it is actually the non-additive 
cell function h(I) of Example 3, whose indefinite integral is the additive Stieltjes 
cell function H(I) = v3 — u3, and whose definite integral on ]0,1] is therefore 1. 
Integrands of this type occur in Chapter 8, on stochastic integrals. They can 
be designated as Burkill integrands. 
In Section 4.1 of Chapter 4, each of these versions is located in a single 
general framework of Henstock integration; so each of the Riemann-complete, 
Stieltjes-complete and Burkill-complete integrals can be given blanket designat-
ion Henstock integral. 

2.3. DIVISIBILITY OF THE DOMAIN ]0,1] 
45 
2.3 
Divisibility of the Domain ]0,1] 
The definitions of the -complete integrals in the preceding section pre-suppose 
that for every positive function δ a J-fine division V can be found. The existence, 
character and properties of ^-fine divisions are fundamentally important to the 
theory; and they determine the nature, properties and scope of the resulting 
integration. 
To prove existence5 of (5-fine divisions, suppose an arbitrary positive function 
δ is given, and assume, for contradiction, that no £-fine division of [0,1] = I\ 
exists. Now consider the intervals [0,2-1] and [2_1,1]. If each of these has a 
(5-fine division, then the union of the two divisions is a 5-fine division of [0,1], 
so by the original assumption, at least one of the two intervals (I2, say) must 
fail to have a £-fine division. If I2 is now bisected, we find ^3 C I2 for which no 
J-fine division exists. Thus, by successive bisection, we find 
hD I2D I3D 
--
for each of which there is no £-fine division. The intersection of the closures 
00 
is non-empty, and contains a point x. 
Thus j = f can be chosen so that 
\Ij'\ < δ(χ); and then 
Ij> C]x — δ(χ),χ -f δ(χ)[. 
It follows that V = {(x, Ij>)} is a (5-fine division of Ijt, which is a contradiction. 
Therefore the original assumption, that [0,1] does not have a J-fine division, 
must be false. 
2.4 
Fundamental Theorem of Calculus 
An "area under the curve" sense of the integral is obtained by formulating the 
definition in terms of Riemann sums—that is, rectangular strips which approx-
imate the area bounded by the graph of f(x) and the x-axis. 
An important consequence of using a variable δ(χ) in the definition of the 
integral is the following theorem, which connects the definition of the Riemann-
complete integral to one of the most basic, intuitive, and primary notions of 
what an integral is. If there is a function F{x) such that 
^ 
— f(x) for 
each x G [0,1], then / is integrable on ]0,1] with definite integral 
f(x)\I\ = F(l) - F(0). 
I . 
•'[o.il 
This result is called Cousin's theorem or Cousin's lemma. See also Lemmas 4, 5, and 6 
of Chapter 4. 

46 
CHAPTER 2. 
INTRODUCTION 
To prove this, for each x G [0,1] choose 6(x) > 0 so that, for ]u,v] satisfying 
x — δ(χ) <u<x<v<x 
+ δ(χ), 
we have 
1 F(x) - F(u) 
I 
x — u 
Then 
\F(v)-F(u)-f(x)(v-u)\ 
= 
\F(v) - F(x) - f(x)(v - x) + F(x) - F(u) - f(x)(x - u)| 
< 
|F(v) - F(x) - f(x)(v - x)\ + |F(x) - F(u) - f(x)(x - u)\ 
< ε(υ — x) + ε(χ — u) = e(y — u). 
The positive function δ(χ) is defined for 0 < x < 1, and it is possible to choose 
a £-fine division V of [0,1], so that, with V = {x, ]u, t;]}, 
| ( D ) ^ / ( x ) | 7 | - ( F ( l ) - F ( 0 ) ) | 
= 
| ( P ) ^ / ( x ) ( v - u ) - ( F ( l ) - F ( 0 ) ) | 
= 
\(V)J2(f(x)(v-u)-(F(v)-F(u)))\ 
< 
(1?)X; 1 / ( ^ ( 1 ; - U ) - ( F ( V ) - F ( U ) ) | 
< 
e ( D ) ^ ( t ; - u ) 
= ε, 
(2.11) 
giving the required result. Note that this proof depends on taking the division 
points x of the division V = {(x, /)} as the evaluation points of the function /. 
Also, the proof shows that F(I) := F(v) — F(u) is the indefinite integral6 of /. 
The implications and consequences7 of this and related results concerning 
point functions f(x) with integrator |/| are investigated intensively in the lit-
erature of the subject. The results are particularly significant in the theory of 
differential and integral equations. 
If F(I) is a measure defined not just on intervals or cells / but also on a 
sigma-algebra of measurable subsets of ]0,1], and if / is a point function that 
is integrable with respect to F, in the Lebesgue sense, with Lebesgue integral 
denoted J0 f(x)dF, 
then / is F-integrable in the Stieltjes-complete sense, and 
the two integrals are equal: 
/ 
f(x)F(I) 
= [ 
f(x)dF. 
6If F(I) has value 1 on the domain (i.e., F([0,1]) = 1 in this case), then, in the theory of 
random variability, / is designated as a density function 
for the distribution function 
F. 
7The fundamental theorem of calculus is a relatively specialized area of integration theory, 
and it does not actually arise in the subjects covered in this book. The Henstock integral deals 
with the question of integrability in general, and the Burkill-complete version of the Henstock 
integral deals with the integrability problems arising in particular in random variability. These 
topics are dealt with in Chapter 4. 
< ε , 
F(v) - F(x) 
v — x 
-fix) 
<ε. 

2.5. WHAT IS 
INTEGRABILITY? 
47 
A proof of this is outlined later in this chapter, and details are given in Theo-
rem 122. Thus the class of integrable functions, in the Stieltjes-complete sense, 
includes the class of Lebesgue-integrable functions; in particular those func-
tions which are Stieltjes integrable in the basic sense, with constant δ. So the 
Riemann-complete and Stieltjes-complete integrals are extensions of the cor-
responding Lebesgue integrals. For non-additive integrators h(I) there is no 
corresponding Lebesgue-type integral. 
2.5 
What Is Integrability? 
The preceding section compares the calculus or Newton integral with the Riem-
ann-complete integral. There are many ways of defining the integral of a func-
tion. This section makes some further comparisons. 
A function f(x) is calculus integrable if it has an anti-derivative or primit-
ive function F(x). 
In that case the indefinite integral F(I) is the Stieltjes or 
incremental form of F(x). For / = ]u, v], 
F(I) = F Qu, v}) = F{v) - F(u). 
Riemann integrabilty of a function f{x) does not require8 it to have an anti-
derivative F(x) with Ff(x) = f(x). 
The Riemann method of integration gen-
erally works for continuous functions. Lebesgue's criterion for Riemann inte-
grability (Abbott [1]) says that a real-valued, bounded function / on [a, b] is 
Riemann integrable if and only if it is continuous almost everywhere on [a, b}. 
Lebesgue integrability requires that the integrand / satisfy the stronger pre-
condition of measurability. But, for Lebesgue measurability on [a, 6], this means 
that / is, in some sense, "nearly" continuous—see Royden [200], for instance. 
So continuity or near-continuity of the integrand features strongly in stand-
ard integration theory. Many familiar integration techniques also depend on 
continuity, and possibly differentiability, of the integrands. See, for instance, 
Bartle and Sherbert [8] for integration by parts, and integration by substitution. 
For Lebesgue integrals it is possible to give widely applicable conditions 
under which a convergent sequence of integrable functions has an integrable 
limit function. But some quite "reasonable" functions are not integrable by the 
Riemann method, nor even by the Lebesgue method. For example, the function 
defined by (2.13) in Section 2.7 has an anti-derivative, and is therefore integrable 
by the calculus method. But it is not Riemann integrable, nor is it Lebesgue 
integrable. (It is Riemann-complete integrable.) 
Definitions 4, 5, and 6 of the -complete integrals of f(x)\I\, 
f(x)F(I), 
f(x)h(I), 
respectively, do not impose any pre-conditions on the integrands. 
For the definitions to work, all that is required is that an integrand permits a 
calculation which depends on divisions of the domain—essentially, formation of 
Riemann sums. 
But, as is shown in Section 2.12 and Theorem 18, every integrable function f(x)\I\ 
(or 
h(x,I)) 
possesses an indefinite integral H(I) to which it is "nearly" equal. 

48 
CHAPTER 2. 
INTRODUCTION 
The criterion for integrability—in the -complete sense—is expressed by a 
Riemann sum inequality: (2.7), (2.8), and (2.9), respectively, in Section 2.2. But 
whereas Lebesgue's criterion, for instance, describes "local" properties that must 
be satisfied by a Riemann integrable function /, the Riemann sum inequality 
for -complete integration does not in itself provide a direct sense of what kind 
of familiar and recognizable qualities or characteristics a function f(x)\I\ 
(or 
f(x)F(I), 
or h(x,I), 
respectively) must have in order to be integrable in the 
-complete sense. 
Are there any "local" characteristics, such as differentiability, continuity, 
or measurability, by which it is possible to recognize (or at least surmise with 
some degree of confidence) whether or not a function is integrable? Taking this 
further, what conditions must the integrand satisfy in order to be able to apply 
integration by parts, or by substitution? What about limits of sequences of 
-complete integrable functions? Or Fubini's theorem? 
The -complete integrals are extensions of the more familiar Newton, Riemann 
and Lebesgue integrals, so the familiar local features of integrability carry for-
ward to the -complete system. Also, the evaluations in Section 1.4 provide a 
general sense of what it is that makes a function integrable. In every case there is 
an indefinite integral—a Stieltjes (or additive) interval function which, for every 
interval in the domain of integration, "nearly equals" the integrand. In proofs, 
it is then often possible to replace the integrand, which may be complicated and 
"unmanageable", by a very manageable Stieltjes cell function. 
This point is also demonstrated in the calculation of the calculus or Newton 
integral in Section 2.4. In (2.11), the additive or Stieltjes interval function 
F(I) = F Qu, v}) = F(v) - F(u) 
is "nearly" the same as the integrand f(x)\I\ on every interval 7" = ]u, v}. Funda-
mentally, it is the presence of this finitely additive interval function that yields 
the fundamental theorem of calculus. The student learning integration is taught 
to try to find an anti-derivative or primitive function F(x) for the integrand 
f(x), often by trial and error. But, in finding an anti-derivative, (2.11) shows 
that the student is really finding a Stieltjes cell function F(I) which closely 
approximates the integrand 
f(x)\I\. 
So here we have a local quality which characterizes -complete integrability. 
In the -complete system of integration, every additive (or Stieltjes) interval 
function H(I) is integrable^ and every integrable point-interval integrand is 
"nearly equal to" such a function. (The indefinite integral of H(I) is H(I), 
with exact equality.) 
What is meant by saying the integrand is "nearly" identical to a Stieltjes 
interval function? This point is explored in Section 2.12; and details of Henstock 
integrability criteria are given in Chapter 4, Theorems 18, 19, 20, and 43. 
Theorem 46 gives the -complete version of integration by substitution. The 
basic assumption is integrability—not continuity or differentiability—and the 
proof involves the additive or Stieltjes indefinite integral. Likewise in Fubini's 
9This fundamental point is a central theme of this book. See Theorems 9 and 10. 

2.6. RIEMANN SUMS AND RANDOM 
VARIABILITY 
49 
theorem (Chapter 4), and integration by parts1(? The latter is illustrated in 
Theorem 89 and Example 51. 
Integrability of the limit of a convergent sequence of -complete integrable 
functions is given by theorems such as the dominated convergence theorem: 
Suppose we have a sequence fn of h-integrable functions, n = 1, 2,3,..., which 
converge to a function 
f: 
fn(x) —> f(x) 
for each x G [0,1] as n —>> oo, 
and suppose 
\fn(x)\ < k(x) 
for oil n and for all x G [0,1] 
where k is h-integrable on [0,1]. Then f is h-integrable on [0,1] and 
lim / 
fn(x)h(I) = [ 
f(x)h(I). 
Similar results hold for other kinds of sequences {/n}, such as those which 
converge monotonically. Proofs are given in Chapter 4. Such results are used in 
Section 2.8 to show that, with h(I) = F(I), every Lebesgue integrable function 
f(x)F(I) 
is Stieltjes-complete integrable. 
Much of the literature of the Henstock-Kurzweil generalized Riemann or 
Riemann-complete integral is predicated on the two results described above: 
■ integrability of derivatives; and 
■ extension of the Lebesgue integral. 
In this context, the division points x G /* are also the points in the Riemann 
sum Σί(χ)\Ι\ 
a t which the point-function integrand f(x) is evaluated. But in 
the broader picture of Henstock or -complete integration, as developed in this 
book, a distinction is made between the evaluation points of the integrand and 
the division points used to form a partition of the domain of integration. This 
distinction is particularly significant in the stochastic integrals of Chapter 8. 
2.6 
Riemann Sums and Random Variability 
Within the discipline of mathematical analysis the Riemann-complete integra-
bility of all derivatives has attracted much interest. 
In contrast, in the theory of random variability a central concept is prob-
ability, or potentiality distribution function, defined on certain subsets of the 
sample space. A key fact in this context is the integrability of additive cell 
functions; in particular, the integrability of all distribution functions. 
A figure is a cell or the union of a finite number of cells. Suppose a function 
F is defined on figures (including cells) in R. If F is finitely additive on figures 
See Theorem 5.12 of Henstock [103]. 

50 
CHAPTER 2. 
INTRODUCTION 
it is a Stieltjes cell function. 
We say that F is a distribution function, or a 
potentiality distribution function, if, in addition, its value is 1 on the sample 
space Ω = R, so -F(R) = 1. So if a cell I is the union of disjoint cells V and I"', 
then F{I) = F(V) + F(I"). If a figure £ is the union of disjoint figures E' and 
E", then, for any distribution function F, 
F(£) = F(£') + ^(£")· 
Usually such a function F is also assumed to be non-negative, and it may be 
helpful for the moment to assume non-negativity. But, in further investigations 
of random variability, distribution functions F will be permitted to be negative 
or complex-valued. 
If, for x G R, f(x) is a real-valued point function and g(x) is a real-valued, 
non-decreasing, right-continuous point function, it is possible (see Royden [200]) 
to define the Lebesgue-Stieltjes integral J f(x)dG where 
G(]u, v]) = g(v) — g(u) 
for 
u<v. 
The properties —such as countable additivity—that G must possess for the 
Lebesgue-Stieltjes integral to exist are ensured by the properties ascribed to g. 
In contrast, for any function g(x) (not necessarily real-valued, non-decreas-
ing, or right-continuous) we can define the Stieltjes-complete integral of the 
integrand f(x)G(I). 
In particular, with f(x) identically 1, the function G(I) is 
integrable, and 
/ 
G(I)=g(b)-g(a). 
J]a,b] 
To see this, consider any partition {]xr_i,xr]} of ]a, 6], 
a = xo < x\ < · · · < xn — b. 
The corresponding Riemann sum satisfies 
n 
£ G(I) = Σ (g(xr) - g(xr-i)) = g(b) - g(a), 
r = l 
and since this holds for every partition, and hence every division, of ]a, b], Def-
inition 5 implies that G(I) is Stieltjes-complete integrable, and the integral of 
G(I) on]a,6] is g(b) - g(a). 
It is the finite additivity (and hence Stieltjes-complete integrability) of pot-
entiality distribution functions F(I) that opens the door to a successful theory 
of random variation. 
This point can be illustrated further by looking ahead at the random vari-
ables X(t) that appear in the mathematical theory of Brownian motion (Chap-
ters 7 and 8). One of the issues that arises in this subject is, with X(a) = 0, 
to construct the random variable X(b) from the family of increments (or differ-
ences, or transitions) 
{X(t) - X(t')), 
a<tf 
<t<b. 

2.7. HOW TO INTEGRATE 
A FUNCTION 
51 
This can be done by calculating a Stieltjes-type integral, ί ^ dX or ί fe, X(z), 
where ι = ]t' ,t] and X(z) := X(£) — X(t'). 
This involves a calculation f, 
b] x(z) 
with x(z) := x(t) — #(£') for continuous functions x(i), a < t < 6, x(a) = 0. It 
happens that the continuous paths x(t) of Brownian motion have infinite varia-
tion, so these paths are "infinitely long" even though b — a is finite. This makes 
it impossible to define the Lebesgue-Stieltjes integral of the interval function 
x(z), and the construction known as the ltd integral is used instead. 
Fortunately, the Riemann sum approach can deal directly with this without 
resorting to the Ito integral construction. As we have seen, for any function x 
of £, Jja 61 χ(ϊ) exists and equals x{b) — x(a)1 or x(b) since x(a) = 0. Thus it is 
possible to define the stochastic integral 
f 
X{i) = [ 
dX = X(b), 
J]a,b] 
J]a,b] 
as a Stieltjes-complete integral, without engaging in any further construction. 
This in turn has implications for the formation of stochastic integrals 
/ 
/(X)X(t), = / 
/(X)dX, 
J]a,b] 
J)a,b) 
discussed in Section 8.4 of Chapter 8. 
2.7 
How to Integrate a Function 
This section demonstrates the Riemann-complete integrability of some func-
tions, using the positive functions (or gauges) δ of Definition 4. 
The following example uses the indicator function of a set Q: 
M * ) = { ; 
I I % Q . 
<
2·
ΐ2> 
Let Q denote the set {#i, #2, #3, · · ·} of rational numbers in [0,1]. With d(x) = 
1Q(X)—the 
Dirichlet function (2.6)—and Fx(I) = |/|, consider the existence of 
the expectation E[d(X)] of the observable d(X) where d(X) ~ 
d(x)[üx,Fx] 
and Ωχ — [0,1]. In other words, consider the integrability of the Dirichlet 
function 1Q(X)|/| on [0,1]. It is well known (see Gordon [82]) that this function 
is Lebesgue integrable, but not Riemann integrable. 
Let ε > 0 be given. For x = qj, j = 1,2,3,..., let δ(χ) = ε2~3\ and for 
x £ [0,1] \ Q let δ(χ) be an arbitrary positive number. Choose a 5-fine division 
V = {(xj)} 
of [0,1]. Let (qi^Iii),..., 
(qirJir) 
denote those (x,I) 
<E V for 
which x e Q. Then \Iia \ < ε2~** and lQ(x) = 0 for x e [0,1] \ Q, so 
(2>)£l g(z)|/| 
= ^2{lQ(x)\I\:(xJ)eV, 
xeQ} 
r 
oo 
= Σι^ι < Έε2~3 = ε-
3=1 
3 = 1 

52 
CHAPTER 2. 
INTRODUCTION 
This holds for all ε > 0, so the Riemann-complete integral L· - 1Q(X)|/| exists 
and equals 0. Thus f(X) is a random variable, and E[/(X)] = 0. 
Since the random variable in this case is the indicator function for Q, we 
have proved that, relative to uniform potentiality (or uniform probability) on 
Ωχ = [0,1], the set Q is a null set. In the terminology of random variation, the 
event [0,1] \ Q is almost certain or almost sure. 
What about other integrands? How is it possible to find suitable functions 
δ(χ) and suitable Riemann sums of the kind referred to in Definition 4? And if 
we succeed in that, how do we establish what the actual integral of a function 
is? How do we find candidate values a for the integral? 
Integrating any function in any theoretical framework—Riemann, Lebesgue, 
or Riemann-complete—often requires ingenuity. Notoriously, it helps to know 
the answer before you even start; or, at least, to be able to make an informed and 
intelligent guess which can be tested and verified by differentiation, Riemann 
sum calculation, or some other method. 
There are in practice a variety of standard methods, such as substitution, 
change of integrator-variable, finding an anti-derivative or primitive function, 
and so on. These integration techniques are successful in many cases, and all 
these techniques are available for finding the Riemann-complete integral of a 
given integrand f{x). 
Additionally, as illustrated above in the case of the Dirichlet function, the 
actual "technical machinery" of Definition 4 is itself often useful. In particular, 
the fact that the integral is based on Riemann sums makes it relatively easy 
to obtain approximate values. Also, as in the evaluations in Examples 1 to 5, 
it is sometimes quite simple to deduce the indefinite integral—an additive or 
Stieltjes cell function—from the Riemann sums of the integrand. Every Stieltjes 
cell function is integrable in the -complete system, so an integrand with indefinite 
integral also has a definite integral. 
What about an integrand, less exotic than the Dirichlet function, such as a 
polynomial /(#)? In that case the primitive function or anti-derivative g(x) may 
be known, and then it is possible to use the result (see (2.11)) proved earlier: 
J|o n /(x)l^l = #(-0 ~~ d(ß)- I*1 °ther words, for such functions the Riemann-
complete integral may by found by the standard elementary methods. 
What if we wish to find the Riemann-complete integral of, say, a polynomial 
by direct application of Definition 4, just as we did with the Dirichlet function? 
The following example indicates how the Riemann sums of Definition 4 can be 
constructed for a basic polynomial integrand. 
Example 14 To illustrate this, consider f(x) — x2. The problem then is, with 
ε > 0 given, to find a suitable gauge δ(χ). Sometimes it is helpful to refer to 
(2.11) for guidance in defining a suitable function δ{χ). As a rule of thumb, 
δ{χ) must be "small" if x is in a neighborhood where f(x) is "highly variable". 
However, the integrand f(x) = x2 is uniformly continuous, smooth, and Rie-
mann integrable (in the basic sense) in [0,1]; so to obtain the Riemann sum 
inequality of Definition 4, it is sufficient, with ε > 0 given, to choose δ(χ) < ε 
for each x G [0,1]. 
O 

2.8. 
EXTENSION 
OF THE LEBESGUE 
INTEGRAL 
53 
The following is an example of a function which, though not Lebesgue in-
tegrable (and therefore not Riemann integrable), is, in fact, Riemann-complete 
integrable. For x G [0,1] define f(x) 
as 
{
2xsinx~ 2 — 2χ~Ύ c o s x - 2 
if 0 < x < 1, 
•f 
n ~ 
( 2 - 1 3 ) 
0 
if x = 0. 
With Fx(I) 
= \I\ defined as before, this corresponds to an observable f(X) 
— 
f(x)[üx,Fx] 
where Ωχ = [0,1], and thus raises the question whether the 
expected value E[/(X)] exists. 
In other words, does the Lebesgue integral 
/ 0 f(x)dx 
exist? Does the Riemann-complete integral L· 1,f(x)\I\ 
exist? 
In 
this case it is clear that the integrand f(x) 
is highly oscillatory in a neighbor-
hood of x = 0, and these rapid oscillations indicate that any function δ(χ) must 
be small when x is in a neighborhood of 0. Define 
x 2 s i n x - 2 
if 0 < x < 1, 
if x = 0. 
It is clear that g{x) is differentiable with respect to x for 0 < x < 1, with 
derivative g'{x) = / ( # ) , and g is differentiable from the right at x = 0 with 
derivative 0, so 
5'(rc) = / ( a ; ) f o r x e [ 0 ) l ] . 
From the basic result on the integration of derivatives established in (2.11), / 
must be Riemann-complete integrable, with integral 
/ 
/ ( : r ) | J | = 0 ( l ) - 0 ( O ) = s i n l . 
J[o,i] 
This illustrates the statement that an integrand with indefinite integral also 
has a definite integral. The fundamental theorem of calculus is a particular 
consequence of this general statement. 
Is the function f(x) of (2.13) Lebesgue integrable? This question is answered 
in the next section. 
2.8 
Extension of the Lebesgue Integral 
Consider some features of the Lebesgue integral (as described in Royden [200], 
for example). By definition, every real-valued function f(x) 
which is Lebesgue 
integrable in a domain R is absolutely integrable; that is, it satisfies 
/ f(x)dx= 
/ f+(x)dx- 
/ 
f-(x)da 
JR 
JR 
JR 
where 
r f(X) 
if f{x) > o, 
r -fix) if fix) < o, 
f+{x) := < 
and 
f-(x) 
:= < 
[ 0 
if fix) 
< 0, 
1 0 
if fix) 
> 0. 

54 
CHAPTER 2. 
INTRODUCTION 
The function f(x) defined by (2.13) oscillates between positive and negative 
values. In any interval, not including zero, but with small values of x, the 
contribution from the term 2xsinx - 2 to the area under the curve is relatively 
small, while the corresponding contribution from the term 2x~x cosx - 2 in / is 
relatively large. Therefore, disregarding the term 2xsinx - 2, the zeros of (2.13) 
can, for present purposes, be estimated approximately as 
x — A / 
— as x —> 0 (or integer n —> oo). 
γ(2η + 1)π 
v 
6 
; 
Accordingly we may estimate that, for large and even values of n, 
I ( ^ f+{x)dx ig approximately ^_^_ + 
° V (2η + 3)ττ 
while for large and odd values of n 
I. 
y (2η + 3)ττ 
Writing 
V (2n+l),r 
2
/
1 
f-(x)dx 
is approximately 
— I 
+ 
π V 2n + 1 
2n + 3 
1 
1 
+ 
π \ 2 η + 1 
2n + 3 
each of the two series 
a2 + a4 + a6-\ 
, 
a\ + a3 + a5 H 
diverges, so it is clear that / is not Lebesgue integrable in [0,1]. 
Riemann sum calculations11 like those in Section 2.14 show that the extended 
Riemann integral 
lim / 
f(x)dx 
of / in [0,1] exists, with value equal to the Riemann-complete integral 
/ 
f(x)\I\, 
J[o,i] 
since the series 
—a\ + a2 — as + a4 - · · · 
is non-absolutely convergent. 
For both the Riemann-complete and extended Riemann integrals, conver-
gence is obtained from the cancellation effects produced by successively sum-
ming the positive and negative parts. In the case of the Lebesgue integral 
nFurther details of this argument can be found in Henstock [103] (page 68, Theorem 5.14) 
and Bartle [7] (page 265, Theorem 16.5). 

2.8. EXTENSION 
OF THE LEBESGUE 
INTEGRAL 
55 
this cancellation effect is absent, and therefore there is no convergence by this 
method. 
This establishes that, just as there are Lebesgue integrable functions that 
are not Riemann integrable, there are Riemann-complete integrable functions 
that are not Lebesgue integrable. But to establish that the Riemann-
complete integral is an extension of the Lebesgue integral, it must be 
shown that every Lebesgue integrable function is Riemann-complete 
integrable. 
It is clear from Definition 4 that every Riemann integrable function is Rie-
mann-complete integrable. A direct proof (from Definition 4) that Lebesgue in-
tegrability implies Riemann-complete integrability is given by Davies and Schuss 
[50]; but in the following paragraph a proof of this result is outlined by reference 
to the limit theorems mentioned above for the Riemann-complete integral (for 
which proofs are given in Chapter 4). 
A step function f on [0,1] is a function for which there is a partition V — {1} 
of [0,1] such that, for each I £ V and for all x G /, f(x) is constant. Every 
constant function, and hence step function, is Riemann integrable, Lebesgue 
integrable, and Riemann-complete integrable, and the respective integrals are 
equal. If a function h is Lebesgue integrable then, by definition (Royden [200]) 
of the Lebesgue integral, and writing h = h+ — h- as before, each of h+ and /i_ is 
the limit of an increasing sequence of step functions, and the Lebesgue integrals 
of h+ and h- are, respectively, the limits of the integrals of the sequences of 
step functions converging to /. But, using the Chapter 4 limit theorems for 
Burkill-complete12 integration, the limits of the Riemann-complete integrable 
step functions are themselves Riemann-complete integrable. Therefore every 
Lebesgue integrable function is Riemann-complete integrable, and the integrals 
are equal. 
The Riemann-complete integral is linked in various ways to other methods 
of integration. The important thing in Definition 4 is the choices of x, δ(χ), 
and / for the formation of Riemann sums. It has already been noted that if, 
in Definition 4, the variable δ(χ) is replaced with a constant δ we get the basic 
Riemann integral. 
Another issue is the selection of the division point x which is associated 
with each of the partition intervals / of the Riemann sum. It is laid down in 
Definition 4 that x £ I for each associated / =]u, V] of the division; that is 
x — δ(χ) <u<x<v<x 
+ δ(χ). 
An alternative approach was given by E. J. McShane [158], where the relation 
between the associated (x, I) of the Riemann sums is 
I C]x — δ(χ),χ + δ(χ)[, 
or x — δ(χ) < u < v < x + δ(χ) 
The Burkill-complete integral includes the Riemann-complete integral. 
That is, ev-
ery Riemann-complete integrable function is also Burkill-complete integrable. Likewise, the 
Stieltjes-complete integral. Therefore theorems which are valid for Burkill-complete integra-
tion are also valid for Riemann- and Stieltjes-complete integrals. 

56 
CHAPTER 2. 
INTRODUCTION 
so the division point x is not necessarily contained in, nor even a vertex of, 
the associated partitioning interval /. If the Riemann sums of Definition 4 are 
formed in this manner, the resulting integral is called the McShane integral. 
The McShane and Lebesgue integrals are equivalent, in that every Lebesgue 
integrable function is McShane integrable and vice versa (see Gordon [82]). Thus 
the Riemann-complete integrable function defined by (2.13) is not McShane 
integrable.13 
In Definition 6, it is required that x be either an interior point or boundary 
point (vertex or end point) of /. But if the integrator function F(I) is additive 
(or Stieltjes), then it is sufficient that x be a vertex of /. This means that, in 
Definition 6, with / =]w, v], we can restrict the evaluation point x to be u or v, 
and still obtain the same integral value for the integrand. 
To see this, consider a single term f(x)F(I) 
of the Riemann sum, with x 
not a vertex of I so 
x — δ(χ) <u<x<v<x 
+ δ(χ). 
We can then write I = I1 Ul2 where I1 = ]u, x], I2 = ]x, v] and, by the additivity 
r i 
f(x)F(I) 
= f(x)F(I1 
U I2) = f(x)F(I1) 
+ 
f(x)F(I2) 
with 
I1 C]x- 
δ(χ),χ + δ(χ)[ 
and I2 c]x - £(x),x + δ(χ)[; 
so each of the associated pairs (x, 71), (x,/ 2) is J-fine. Thus each of the terms 
f(x)F(I) 
of the Riemann sum can be replaced by the corresponding 
f(x)F(I1) 
+ 
f(x)F(I2) 
in which x is a vertex of the associated intervals, and the integrability inequality 
of Definition 5 remains valid. 
For an integrand f(x)h(I) 
with h not additive, then restricting each x to 
be a vertex of its associated I in Definition 6 produces a more general kind of 
integral. 
The distribution functions F used to investigate random variability are 
finitely additive on cells and figures. For the purposes of this book it is simplest 
to declare that a point x and a cell I are associated if x is a vertex of /. For 
bounded cells I =]u,v], this means that x — u or x = v. Let I* denote the set 
of associated points of the cell /. So for I =]u,v], we have I* = {w, v}. Then a 
division V of [a, b] consists of a collection (x, /) of point-cell pairs, each of which 
satisfies x G /*, such that the cells / form a partition of [a, b]. Given a function 
δ(χ) > 0, (x, /) is (5-fine if x G /* and |/| < δ(χ). A division V is J-fine if each 
(x, I) G V is (5-fine. 
13For the McShane and Riemann-complete integrals, Riemann sums must be formed in 
which the point function integrand f(y) is evaluated at each division point x of the associated 
point-call pairs (x, I) forming the division of the domain of integration. But in stochastic 
integrals (Chapter 8) the evaluation points y may not necessarily coincide with the division 
points x. 

2.8. EXTENSION 
OF THE LEBESGUE 
INTEGRAL 
57 
Though this produces a change in meaning, there is no change in the wording 
of the definition14 of the integral, repeated here. 
Definition 7 The function f is integrable with respect to integrator h on [a, b], 
and the value of the integral is a, = Γ b, fh, if, whenever ε > 0 is given, there 
exists a gauge δ so that, for every δ-fine division V of [a, 6], the corresponding 
Riemann sum satisfies 
{V)Y^f{x)h{I)-a 
<ε. 
(2.14) 
With this change in the meaning of associated point, /* is no longer the closure 
7 of / in the open interval topology. For any cell / let / denote I U I*. Then, 
in the open interval topology, we have 
/ = / = JUJ*. 
If E is a figure, let E denote (J 
so, with the same meaning of 
associated point, we have 
E = E. 
Example 15 Suppose a$ < a < b < bo. Cells in [αο,&ο] have the form [CLQ,V] 
or ]u, v] where a$ < u < v < bo; and [a, b] is not a cell in [ao, bo]. Suppose u < v 
and 
1 if u < a < v, 
0 if v < a or a < u. 
FQu,v}) 
δι(χ) 
Then F is additive on figures and is an atomic distribution function. 
Define 
δι(χ) > 0 for ao < x < bo as follows. 
( min { \ (x — a), | (b — x)} 
if a < x < b, 
min { \ (x — ao), \ (a — x)} 
if ao < x < a, 
min {^ (x — b), \ (b0 — x)} 
if b < x < b0, 
\ (b — a) if x = a or x = b, 
\ {a- ao) if x = a0, 
{ %{bo-b) 
if x = bo. 
Then, writing [ao,&o] as EQ U Εχ U E2 where Eo = [ao?a]> -ΕΊ = }a,b}, and 
E2 = ]6, 60]? every δχ-fine division V of [ao^o] is the union of δ\-fine divisions 
Ej of Ej for j — 0,1,2. 
In particular, if (x,I) 
is δχ-fine, and if x £ Ej, 
then I c EJ; and we say that δι conforms 
to the figure (cell) Ej. 
Thus the 
definition of δ\ ensures that every δι-fine division E\ of E\ =]a, b] includes a 
Since the Burkill-complete integral includes the Riemann- and Stieltjes-complete integrals, 
this change in the meaning and definition of point-cell association can be equally applied to 
the definition of the latter integrals. 

58 
CHAPTER 2. 
INTRODUCTION 
term (x, I) with x = a; and this is the only non-zero term of the Riemann sum 
(Si)^2F(I). 
Therefore each of these Riemann sums has value 1, giving 
I 
J\a 
F(I) = l = F(]a,b}). 
a,b] 
Then the integral of F on the figure (cell) E\ is equal to F(Ei). 
But the integral of F in the figure (cell) E\ is not equal to F(Ei). 
To see 
this, consider the Riemann sum (<?ι) Σ l]a,b](x)F(I). 
Each term of this 
has value zero, so we get 
0 = / 
l]aM(x)F(I) 
Φ [ 
F(I) = F(E1) = 1. 
(2.15) 
J[a0,b0] 
J]a,b] 
Also, the construction implies that any δ-fine Riemann sum for the integ-
rand l[a0;a](x)F(jT) must contain a term (x,I) with x = a and I = ]a, v], 
contributing value 1 to the Riemann sum. Every other term contributes 
zero, so 
1 = f 
l[o0iO](x)F(I) φ ί 
F(I) = F ( K a}) = 0. 
J[a0,b0] 
J[ao,a] 
The set {a} is not a cell, and F({a}) is not defined. But 
[ F(I)= [ 
l{a}(x)F(I) 
J {a} 
J[a0,bo] 
exists and equals 1. This is because, with δ as above, every δ-fine Riemann 
sum for the integrand l{ay(x)F(I) 
has, as its only non-zero term, 
l{a}(a)F(}a,v}) 
= l. 
The notation j s is ambiguous when S is a cell. It should be avoided in 
favor of the unambiguous notation J Is unless it is clear from the context 
that S is not a cell. 
Note also that, if any function f(x)h(I) 
is integrable on ]a, 6] = E\, then 
it is also integrable in the set [a, b] = E\, which is not a figure or a cell. 
(The converse is also true.) This is because, for every δι-fine division V 
of [a0, bo], 
(V)Y^l[aM(x)f(x)h(I) = (S1)J2f(x)h(I), 
giving 
[ 
f(x)h(I)= 
[ 
l[aM(x)f(x)h(I)= 
[ 
f(x)h(I), 
J[a,b] 
I[ao,b0] 
J]a,b] 
as required. 
O 

2.9. RIEMANN SUMS IN BASIC 
PROBABILITY 
59 
For the atomic distribution function F of Example 15, suppose b — a > 1. 
Consider the cells 
Jn=}a 
+ n-\b], 
In = 
Jn\Jn-\ 
For n — 1,2,3,... the cells In are disjoint, and U^Li In Ha?^]· 
We have 
F(In) = 0 and F ((X=i / n ) = 0 for ra > 1; giving 
Q = J^F()JIA 
^F(\JIA 
= F(]a,b]) = l. 
(2.16) 
2.9 
Riemann Sums in Basic Probability 
Some familiar theorems in basic probability theory are easily established. Sup-
pose Ωχ = [0,1] and Fx(I) = |/|, the uniform probability distribution on [0,1]. 
Then X c± x[Qx,Fx]. 
Suppose j \ and fi are real-valued functions defined on 
[0,1] and integrable on [0,1]; so that, with 
Λ(Χ) ~ h{xWx,Fx\ 
f2(X) 
* 
f2(xWx,Fx], 
fi(X) 
and / 2(^0 are random variables with 
E[/i(*)] = / 
/i(x)|/|, 
E[/2(X)] = / 
f2(x)\I\. 
J[o,i] 
J[o,i] 
Let / = fi + / 2. It is easy to deduce that f(X) 
— f(x)[ilx,Fx] 
is a random 
variable, and 
E[/(X)] = E[/1(A:)] + E[/2(X)]. 
This is the result 
/ 
(/ι(χ) + / 2 ( χ ) ) | / | = / 
/ ι ( * ) | / | + / 
Λ(χ)|/| 
J[0,1] 
7[0,1] 
^[0,1] 
which follows from Definition 4 of the Riemann-complete integral. See Chapter 
4 for proof. Likewise for any finite sum of random variables. 
If A C [0,1] define 
P[A] = Έ>Χ[Α] := / 
lA(x)Fx(I). 
(2.17) 
Mi] 
By (2.16), distribution functions are not necessarily countably additive in their 
domains of definition. But (2.15) shows that, for figures or cells E1, the con-
struction Ρχ[ϋ7] is not necessarily the same as 
Fx(E). 
An easy application of the limit theorems of Chapter 4 gives the countable 
additivity relationship (2.4) for the function P defined in (2.17). To see this, an 
argument involving a countable sum of random variables can be used. Suppose 
the sets A{ c [0,1] are disjoint, i = 1,2,3,..., with A = U S i ^*· Consider 

60 
CHAPTER 2. 
INTRODUCTION 
the contingent observables l^.(X) ~ 1A.(X)[QX,FX]. 
For each i, suppose the 
probability 
P [Ai] = Ρ χ [Ai] := E [1Λ, (X)] = / 
1 Λ ί (x)F x (/) 
J[o,i] 
exists as a Stieltjes-complete integral. Defining A^ to be UlLi ^ » n o t e t n a t the 
integrable function 1A> (X) is monotone increasing and convergent to 1A(X) for 
each x G [0,1], so, by Theorem 57 (monotone convergence theorem), 1A(X) is 
integrable and 
/ 
lA,n(x)Fx(I) 
^ 
[ 
1A(X)FX(I) 
J[0,1] 
J[0,l] 
as n —> oo. This means that P [A'n] = Σ7=ι P [-A^] converges to P [A] as n —> oo. 
That is, 
oo 
ρμι = Σ ρ Ν · 
Note also that if the sets A$ are measurable with respect to the underlying prob-
ability generated by the distribution function Fx, then the Lebesgue-Stieltjes 
integrals of 1^. (a:), ^-A(X) exist and are equal to the corresponding Stieltjes-
complete integrals. Thus the countable additivity relationship (2.4) of the Rie-
mann sum approach has exactly the same meaning and significance as the cor-
responding axiom of the Kolmogorov theory. 
Random variability is typically concerned with joint random variability. 
Consider a pair of joint elementary observables Χι, Χ2 such as those of Table 
1.4 in Chapter 1. Suppose each has [0,1] as sample space, with distribution 
functions Fx0{I) 
(j — 1,2). Consider the joint measurement X = (Χι,Λ^) 
with sample space Ωχ = [0,1] x [0,1] and distribution function Fx defined on 
the two-dimensional intervals / = I\ x I2 of [0,1] x [0,1], where Ij is a subset 
of [0,1] for j = 1,2. 
The issue of marginal potentialities was mentioned in Chapter 1; the re-
lationship between the distribution functions Fx,Fx1,Fx2 
will be addressed 
later. But first consider the forms in which joint random variables manifest 
themselves. 
Suppose f is a real- or complex-valued function defined on [0,1] x [0,1], and 
suppose that, when the joint measurement X = (-ΧΊ,-Χ^) generates a datum 
consisting of a pair of linked random outcomes x = (^1,^2) £ [0,1] x [0,1], 
the value f{x\,X2) 
is then calculated. We say that f(X) 
= /(Χι,Λ^) is an 
observable contingent on the joint observable X = (Χχ,-Χ^)* specified by 
f(X)~f(x)[Qx,Fx] 
where the sample space Ω,χ for the joint variation is [0,1] x [0,1]. The joint-
contingent observable f(X) 
is a (joint) random variable if E [/(-X")] is defined 
and exists. 
For this to be meaningful in the generalized Riemann sense, the Riemann-
complete integral in the domain [0,1] x [0,1] must be defined. 

2.9. RIEMANN SUMS IN BASIC 
PROBABILITY 
61 
First, each cell I in Ωχ = [0,1] x [0,1] is a Cartesian product I\ x J2, each Ij 
being a cell in [0,1]. A figure in Ωχ is a finite union of cells in Ωχ. A partition 
V of [0,1] x [0,1] is a finite collection of disjoint cells / = I\ x I2 whose union is 
[0,1] x [0,1]. A point-cell pair (x, I) of Ωχ is associated if (XJ,IJ) 
is associated 
in [0,1] for j = 1, 2. In other words, x must be a vertex of I in [0,1] x [0,1]. 
Thus, if x — (#i, X2) £ I* x I2 = I*, where Ij =]UJ,VJ], j — 1, 2, we have 
I* = {(u1,u2),(u1,v2),(vi,V2),(vuU2)}i 
I = I\Jl* 
=ϊ. 
A division V of [0,1] x [0,1] is a finite collection of associated point-cell pairs 
(xj) 
= (Οι,χ 2),Λ x h) 
such that the cells / form a partition of [0,1] x [0,1]. A gauge in [0,1] x [0,1] is 
a positive function δ defined for x = (£i,£2) £ [0,1] x [0,1]. An associated pair 
(x,I) — ((#1, #2)7^1 x I2) is ί-fine if 
lJil =V3 ~U3 <δ(χ)ι 
3 = ^ 2 ' 
A division V of [0,1] x [0,1] is 5-fine if each (x, I) G V is i-fine. 
Now suppose / is a real- or complex-valued point function defined on [0,1] x 
[0,1], and suppose h is a real- or complex-valued integrator function (not nec-
essarily additive) defined on the cells J of [0,1] x [0,1]. 
Definition 8 The function f is integrable with respect to h in [0,1] x [0,1], 
with definite integral a, if, whenever ε > 0 is given, there exists a gauge δ in 
[0,1] x [0,1] 50 that, for every δ-fine division V of [0,1] x [0,1], the corresponding 
Riemann sum satisfies 
{V)Y^f{x)h{I)-a\<e; 
(2.18) 
and we write 
( 
f(x)h(I) 
= a. 
J [0,1] X [0,1] 
This definition covers Riemann-complete, Stieltjes-complete and Burkill-comp-
lete15 integrands. Note that Definitions 6 and 7 have the same form as Defin-
ition 8. In Section 4.1 of Chapter 4, each of these versions is located in a single 
general framework of Henstock integration. Each of the Riemann-complete, 
Stieltjes-complete, and Burkill-complete integrals can be given blanket designa-
tion Henstock integral. In one dimension, the Riemann-complete integral gives 
the fundamental theorem of calculus, provided, when forming a Riemann sum, 
the evaluation point of / is also the division point x of the associated pair (x, I) 
in the partition. The Stieltjes-complete and Burkill-complete integrals are used 
in areas of analysis where the fundamental theorem of calculus is largely irrel-
evant. 
15Definition 8 remains valid if product integrand f(x)h(I) 
is replaced by joint integrand 
h{xj). 

62 
CHAPTER 2. 
INTRODUCTION 
Definition 8 validates an understanding, in Riemann sum terms, of the ex-
pectation 
E [/(*)]= / 
MW), 
J[0,l]x[0,l] 
of a joint-contingent 
random variable f(X) 
dependent (or contingent) on a 
joint-basic observable X composed of two jointly-observed, underlying basic 
observables X\, X2, 
f(x) = f(x1,x2)~f(x1,x2)inXl,X2,F{XliX2)} 
= f(x)[nx,Fx}. 
For the pair X = (Χχ,Χ2) to be meaningfully regarded as a joint observable 
in the sample space Ωχ = [0,1] x [0,1], the distribution functions 
Fx,Fx1,Fx2 
must be consistent. 
This means that FXl and Fx2 must each be marginal 
distributions in relation to Fx. That is, 
Fx(I1,[0,l]) 
= FXl(h) 
and FX([0,1],72) - 
FX2(I2) 
for each Ιχ, I2. 
What is the sense or meaning of the equation Fx(Ji, [0,1]) = 
Fx1(Ii)7 
Suppose that, in the joint observation, we are interested in the potentiality that 
the first observable X\ takes a value in the range I\, while the second observable 
X2 has arbitrary value; that is, it may take any value in its domain or sample 
space [0,1]. Essentially, this is the potentiality Fx1(Ii) 
that X\ takes a value 
in Ιχ. Similarly for the second consistency equation Fx([0,1],^) = 
FX2(I2). 
If it happens that 
Fx(I1xI2) 
= 
Fx1(I1)Fx2(I2) 
for all possible intervals or events I\ and I2, then we say that the observables 
X\, X2 are independent. Suppose we have a pair of random variables j \ (X\) 
and f2(X2), 
and suppose that X\ and X2 are independent. Then the joint 
observable 
f(X) 
:= 
f1(X1)f2(X2) 
is a random variable that satisfies 
E[f(X)] 
= 
E[f1(X1)]E[f2(X2)]. 
The proof of this familiar result uses Fubini's theorem, established for Burkill-
complete integrals in Chapter 4. The proof goes as follows: 
E [/(*)] 
= 
/ [ Ο , Ι Μ Ο , Ι Ι / Μ ^ Ο Ο 
= 
/[o,i]x[o,i] f(x^x*)F{XuX2){h 
x h) 
= 
J[o,i]x[o,i] 
h(xi)f2(x2)FXl(h)FX2(I2) 
^ 
/[0,1] (/[0,1] M*l)f2{X2)FXl{h)) 
FX2{I2) 
= 
/[o,i] (/[o,i] f^Fxdh)) 
f2(x2)FX2(I2) 
= 
Et/xiXOlEt/aiXa)]. 

2.10. VARIATION AND OUTER 
MEASURE 
63 
The following, equally familiar, result in joint variability does not require in-
dependence. Given random variables fj(Xj), j = 1,2, the observable f(X) = 
/i(-X"i) + Λ Ρ ^ ) is a random variable and 
Ε[/(Χ)] = Ε[Λ(Χι)]+Ε[/ 2(Χ 2)]. 
This follows, by Fubini's theorem, from the consistency conditions on the joint 
distribution function Fx, since 
E[/PO] = 
Ilo,iMo,i]f^)FxW 
= 
/[o,i]x[o,i] f(x^x2)F{Xllx2)(Ii 
x h) 
= 
/[o,i]x[o,i] (Äfo) + h{x2))F(x^X2){h 
x I2) 
= 
/[0,1]χ[0,1] Λ ^ ΐ ί ^ ι Α ) ^ X 72) 
+ J[0,1]X[0,1] Λ Ο ^ Χ , , Χ ^ Λ X /2) 
= 
/[o,i] /[o,i] fi(xi)F(Xi,X2)(h 
x ^ ) 
+ /[0.1] S[0,1] ΜΧ2)Ρ{Χι,χ2){Ιΐ 
X h) 
= 
/ Μ / ι ί ΐ ι ^ , Λ Ι ^ ι Μ θ , Ι ] ) 
+ /[0jl]/2(x2)F(Xl,X2)([0,l]x/2) 
= 
/ [ M fi(xi)FXl(h) 
+ J[0>1] 
f2(x2)FX2(I2) 
= 
E[/!(Xi)]+E[/ 2(X 2)]. 
The latter two results are easily extended to finite collections of random vari-
ables. 
2.10 
Variation and Outer Measure 
Equation (2.4) above indicates that sets which are "measurable" in a Riemann 
sum sense have the properties that are usually expected of measurable sets. For 
such sets we have 
P[A]=PX[A]= 
[ 
lA(x)Fx(I), 
where P = Ρ χ is the probability measure (defined on measurable sets) gener-
ated by the potentiality distribution function Fx (defined on cells). But (2.15) 
demonstrates that the functions "Ρχ" and "Fx" need not coincide on cells. 
What about sets A which are not "measurable", in that their indicator 
function 1^ is not integrable? 
In the theory of Lebesgue integration (see Royden [200], for instance), an 
"outer measure" function is used to analyze such sets. In one dimension, 
Lebesgue outer measure is an extension of "interval length" concept, so even 
when the "length" of a set cannot be calculated, its outer measure can be 

64 
CHAPTER 2. 
INTRODUCTION 
determined. Likewise for area and volume in two and higher dimensions. Cer-
tain kinds of functions of intervals—other than the length, area, and volume 
functions—can similarly be extended to all sets by a similar construction of 
outer measure. 
In the Henstock theory there is a function V called variation16 which has a 
role comparable to outer measure, and is an extension of the concept of outer 
measure. Later it will be shown that the "variation" function coincides with 
"Ρχ" on measurable sets—that is, on those sets for which P is defined. The 
following discussion explores connections between outer measure and variation. 
2.11 
Outer Measure and Variation in [0, ll 
Consider uniform potentialities in [0,1]; that is, Fx(I) — \I\. This generates an 
outer measure μ* (Lebesgue outer measure) on the family of all subsets A of 
[0,1], with 
(
oo 
the infimum being taken over all countable collections C = {Ij} of intervals such 
that U^=i Ij contains A. 
The variation (or variational measure) of A, relative to uniform likelihood 
or uniform potentialities in [0,1], is 
V(A) = inf {sup(Pi) ^ 
U(x)\I\} 
, 
the supremum of the Riemann sums, for a given gauge δ, being taken over all δ-
fine divisions of [0,1], and the infimum then being taken over all possible gauges 
δ. 
To show that μ*(Α) — V(A) for all subsets A of [0,1], first choose a countable 
interval cover C = {Ij}JLi f°r A such that 
oo 
Σ\Ι3\<μ*(Α) 
+ ε 
for some given ε > 0. Then for each x e A there is an open interval Ij G C such 
that x G Ij. Choose δ(χ) > 0 so that ]x - δ(χ),χ + δ(χ)[ C Ij. For x G [0,1] \ A 
let δ(χ) > 0 be arbitrary. Then, for any (5-fme division V = {(x, I)} of [0,1], 
(Ρ)Χ;ΐΑ(χ)|/|<^|7,|<μ*μ) + ε, 
3 = 1 
sup{(O)Y^lA(x)\I\ 
: V 5-fine} < μ*(Α) + ε 
3 = 1 
so 
16It is unfortunate that this word also appears—with different meaning—in the expression 
random 
variation. 

2.11. OUTER MEASURE AND VARIATION IN [0,1] 
65 
and 
inf |sup{(£>) Σ 
1A(X)\I\ 
: V J-fine} : all gauges δ\ < μ*(Α) + ε. 
Since this holds for all ε > 0, we have V(A) < μ*{Α). 
To demonstrate the 
opposite inequality, for given ε > 0 choose a gauge δ and a δ-üne V1 so that 
(V1)Y^lA(x)\I\<V(A) 
+ 
£-. 
Now let 
S1 
= 
{(x,I):{x,I) 
eX>\ 
xeA}, 
C1 
= 
{I:(x,I)eV\ 
x£A}, 
B1 
= 
\J{I:(x,I)€V1} 
Cx 
= 
\j{I:{x,I)^Vl}\j{x:(xJ)eVi}, 
A2 
= 
An([0,l]\d). 
If A2 = 0, enumerate C1, — J 1,I 2,... (a finite collection), and replace the inter-
vals P =]ui,yi] by open intervals 
J3 =}uj -e2~j,vj 
+e2~j[ 
which contain the elements x of A. Then the collection C = {P} 
satisfies 
^1^1<νμ) + ε, 
so 
inf I y j |i"| : all countable open interval covers C\ < \A\ + ε 
for all ε > 0, giving μ*(Α) < V(A). 
Essentially, we use the partitioning cells /, for which the corresponding di-
vision points x are members of A, to form an open interval cover C for A. But 
they must be adapted for this purpose, since, if (x, I) = (x,]u,v]) is an element 
of the division with x e A, the following issues should be noted: 
■ ]u,v] is not open; 
■ we can have (x, ]u, v]) with x = u, so x φ I; 
■ we can have terms (x, I) in the <5-fine division such that I Π A Φ 0, but 
x £ A; so that the term l^(x)|/| of the Riemann sum is zero. 
If A2 Φ 0 (the latter point), let 
E2 = 
%1]\B1. 
Then it is not difficult (see Chapter 4) to show that V(ADE2) 
< ε, and we can 
choose a gauge 82 so that, for every ^2-fine division D2, 
(V2)YilAnE,(x)\I\<^. 

66 
CHAPTER 2. 
INTRODUCTION 
If the x of the non-zero terms of the Riemann sum do not exhaust A Π E2 the 
procedure can be iterated, if necessary a countable number of times, giving 
(W)J2lAnEJ(x)\I\<-lj, 
j = 2,3,..., 
with 
A c[J{{x 
: (x,I) eVi} 
: j = 1,2,3,...} 
. 
Now enumerate the intervals I of the divisions V^, j = 1, 2,3,..., and denote 
the resulting family of intervals by 
{I1,12,13,...}. 
As before, replace each P =]υ?,υ:>] by 
Ji = }uj -e2~j,vj 
+e2~j[, 
giving an open interval cover C for A, with 
oo 
^\Jj\<V{A) 
+ 2e. 
i=i 
Therefore μ*(Α) < V(A) + 2ε for all ε > 0, and μ*(Α) < V(A). 
Therefore 
μ*(Α) = V(A), so the variation of A relative to uniform probability is the 
Lebesgue outer measure of A. 
If |7| is replaced by an arbitrary non-negative potentiality distribution func-
tion F(I) the same argument can be applied. So, for any A C [0,1], with 
VF(,4) := inf j s u p ( ^ ) ^ 
lA(x)F(I)\ 
we find that Yp(.A) = μ'ρ(Α), the outer measure generated by the probability 
distribution function F; or the F-outer measure of A. 
Thus, a Riemann sum approach to outer measure of A involves forming 
Riemann sums for the point-cell function 1A(X)F(I), 
and the term variation 
designates this. (This is analytical variation or variation of a function, not 
to be confused with random variation or random variability). The Riemann 
sum approach to random variability will involve an extension of this concept of 
outer measure or (analytical) variation, in which F(I) is replaced by an arbitrary 
point-cell function h{x,I). 
Thus the (total) variation of h(x,I) is 
V^:=inf|sup(^)^|/i(^/)|}, 
and, if A is any subset of Ω, the variation of h in A is 
Vh(A) := inf LMVs) 
£ 
lA(x)\h(x, 
I)\\ . 

2.12. THE HENSTOCK 
LEMMA 
67 
This can be thought of as the h-outer measure of A. 
In Chapter 4 the concept of variation will be examined in more detail, as a 
generalization of outer measure using not just non-negative, additive functions 
F, but more general point-cell functions h{x,I) which can be complex-valued 
and non-additive. And in the Epilogue there is a further extension of this 
concept. 
2.12 
The Henstock Lemma 
There is a "natural" and intuitive Riemann sum approach to random variabil-
ity, corresponding to relative frequency tables and histograms, as outlined in 
Chapter 1. The preceding sections of this chapter indicate how this approach 
ties in naturally and intuitively with the Stieltjes-complete theory of integration 
using ^-gauges. 
This section seeks to highlight some features and methods of the ^-gauges 
theory of integration which are specific to that theory and which are less evident 
in other integration theories. This is done with a view to preparing the way for 
more formal presentation, in later chapters, of a Riemann sum approach to the 
analysis of random variability using the Henstock integral itself. 
In the preceding sections the variation function 
V f c:=inf(sup(2? d)5^|h(x,/)l 
was introduced. This can be used like an outer measure, and it exists whether 
or not the function h is or Henstock integrable in the Burkill-complete sense. 
(The term variation here refers to a concept of mathematical analysis, and not 
to random variation, an "approximation" concept of accuracy of estimation in 
measurement or observation.) 
When h is integrable the integral, indefinite integral, and variation of h can 
be linked together by means of a result (Theorem 18) called the Saks-Henstock 
lemma, or Henstock's lemma, which is central to the theory. 
Suppose a point function f(x) is Riemann-complete integrable, in accordance 
with Definition 6, with respect to the cell function |7| in [0,1]. Then, by Theorem 
17 of Chapter 4, the integral exists in every cell J c]0,1]. For each cell J of 
[0,1] let 
H(J):= 
[ 
f(x)\I\, 
so H(J) (J c [0,1]) is the indefinite integral of f(x)\I\. 
Then, in any Riemann 
sum Σ f(x)\I\ 
o v e r [0? 1]5 a single term f(x)\I\ can be thought of as a rectangular 
area approximating the area H(I) which is bounded by the curve /(#), the 
interval I and two ordinates, as in Figure 2.1. The differences (or "errors") 
f(x)\I\ — H(I) can be expected to be small, likewise their absolute values. How 
small are they? Henstock's lemma says that divisions of the domain can be 
chosen so the Riemann sum of the individual differences is arbitrarily small, in 
terms of their absolute values. 

68 
CHAPTER 2. 
INTRODUCTION 
Figure 2.1: Error term in Henstock's lemma. 
Suppose there is a gauge δ(χ) such that the definite integral a = #([0,1]) 
satisfies the integrability inequality (2.9) 
(Vs)^f(x)\I\-H([0,l}) 
< ε 
of Definition 6 is satisfied. The Henstock lemma says that the indefinite inte-
gral H(I) satisfies a related inequality: the sum of the absolute value of the 
differences or error terms is small, and 
(Vs)^\f{x)\I\-H(I)\<8e. 
(2.19) 
Every subset of such error terms satisfies the same inequality. So every Riemann-
complete integrable integrand f(x)\I\ is "nearly" equal to its indefinite integral— 
the additive (or Stieltjes) cell function H(I). (Riemann-type integrands f(x)\I\ 
can be replaced by x-dependent Burkill integrands h(x,I) in this statement.) 
Here are some elementary consequences, dealt with in Chapter 4. 
■ Suppose A is a null subset of [0,1], in the sense that A has |/|-variation 
zero. As shown earlier, this means that the Lebesgue outer measure of A is 
zero. Then, for any point function /, and writing h(x,I) — f(x)\I\, The-
orem 32 will show that the h-variation of A is zero. That is, Vh{A) = 0, 
or, to use a more familiar form of words, the "/i-outer measure" of A is zero. 
If Vh{A) is zero, then JA h exists and equals zero (Theorem 37). With 
h(x, I) = f(x)\I\, this means that if A is a null set then 
f f(x)\I\= 
f 
lA(x)f(x)\I\ 
= 0. 
JA 
J[O,I] 

2.13. UNBOUNDED SAMPLE 
SPACES 
69 
Thus the integral of a point function in a null set is zero—a familiar result. In 
the Henstock theory it is expressed and proved in terms only of Riemann sums. 
In other words the calculations involve finite operations of the kind expressed in 
the inequalities (2.19) and (2.9), generally avoiding "infinite operations" such 
as those expressed in (2.4). 
2.13 
Unbounded Sample Spaces 
In previous sections sample spaces ]0,1] and ]0, l]x]0,1] have been considered; 
and the theory is easily extendible to compact domains 
]a,6], 
]ai,6i] x ]a2,62] x ··· x ]an,fcn], 
respectively. The following is an outline of a Riemann sum approach to random 
variability in unbounded sample spaces such as 
[0, oo[, 
R + = ] 0 , oo[, 
R = ] —oo, oo[, 
and Cartesian products of such spaces. As before, the key to this is formation of 
divisions of the sample space; that is, finite collections of associated point-cell 
pairs (#, I) such that the cells partition the domain. 
The only way that a finite collection of disjoint intervals can partition an 
unbounded domain is by including unbounded intervals in the partition. There-
fore, in addition to pairs of the form 
Or,/) = (x,]tfc,v]), x = u or x = v, 
we may have pairs 
(x, I) with x = — oo and I = } — oo, υ], 
or (x, I) with x = oo and I = ]u, oo[. 
A partition of ]0, oo[ has the form 
0 < u(1) < u{2) < 
< u{n) < oo 
and a division V — {(x, /)} of ]0, oo[ is 
{(xü),J^):j = l,2,...,n} 
with 
I1 
= ]Q,u(% 
XW=0 
or uW; 
P 
= 
}u(j\uü+% 
xW =uW 
or u(j+1\ 
2<j<n-l; 
In 
= 
}u(n\oc[, 
x ^ ^ o o . 
This is illustrated in Figure 2.2. 
A gauge in ]0, oo] is, as before, a positive function δ(χ), defined in this case 
for x G [0, oo]. For x φ oo an associated pair (x,I) 
is (5-fine if, as before, 

70 
CHAPTER 2. 
INTRODUCTION 
\I\ < δ(χ). The innovation here is that associated pairs of the form (oo,]u, oo[) 
are admitted. (In Figure 2.2 the innovation is (oc, ]u^n\oo[).) 
Such a pair is 
said to be δ-fine if 
1 
u > 
Y(—\· 
ό(οο) 
The idea is that, as £(oo) becomes smaller, the lower boundary u (or u^ 
in 
Figure 2.2) of the interval ]iz, oo[ becomes larger, so the interval ]u, oo[, though 
infinite or unbounded, becomes "smaller". Then, as before, a division T> = 
{(#,/)} of ]0,oo[ is δ-fine if each (x,I) is δ-fine. 
Suppose F is a potentiality distribution function defined on the cells / of a 
sample space ]0, oc[, and suppose f(X) 
is an observable, 
f(x)~f(x)[nx,Fx\. 
The expected value E [/(X)] will be given by the integral of f(x)F(I) 
in ]0, oo[, 
E [/(*)]= / 
f(x)F(I), 
provided the integral is defined, and provided the function is integrable. 
Let / and h be real- or complex-valued functions defined on ]0,oo[ and on 
I(]0, oo[), respectively; where I(]0, oo[) is the family of cells in R + =]0, oo[. 
Definition 9 The function f is integrable with respect to h (or, f is h-integrable) 
on ]0, oo[, and the value of the integral is a, if whenever ε > 0 is given there 
exists a gauge δ in [0, oo[ so that, for every δ-fine division V of [0, oo[, the 
corresponding Riemann sum satisfies 
( 2 > ) Σ / ( ζ ) Λ ( / ) - α | < ε ; 
(2.20) 
and we write 
[ 
f(x)h(I) = a. 
J]0,oo[ 
Note that, apart from the domain of integration, the wording of this definition 
is the same as Definitions 6, 7 and 8. 
Fortunately, this congruence of the definition of the integral in various dom-
ains persists. Suppose, for instance, that the sample space is a (finite) Cartesian 
product of ] — oo, oo[ by itself n times, 
Ω=]0,οο[χ ••·χ]0,οο[. 
Figure 2.2: Partition of domain ]0, oo[. 

2.14. CAUCHY EXTENSION 
OF THE RIEMANN 
INTEGRAL 
71 
Figure 2.3: Some elements of a partition of ]0, oo[ x ]0, oo[. 
Then the elements of Ω are 
X = yX\, . . . , X<n) 
and the cells / G Ι(Ω) of Ω are 
I = lx x ... x / n j 
where any of the Ij can be unbounded intervals of the form ]u, oo[. A selection 
of such cells are illustrated in Figure 2.3. In the multi-dimensional domain a 
gauge δ is defined for x € Ω*; that is, Ω with the zeros and the points at infinity 
adjoined. An associated pair (x,I) is ί-fine if, for j = 1,2, ...n, each 
(XJ,IJ) 
is 5-fine in the one-dimensional sense already described. A division V of Ω is a 
finite collection (x,I) such that the cells I partition Ω (i.e., the I are disjoint 
and have union Ω). If we have a point function f(x) and a cell function ft(i"), 
the integral on Ω of / with respect to integrator function h is defined as in 
Definition 9, except that ]0, oo[ is replaced by ]0, oo[ x · · · x ]0, oo[. 
2.14 
Cauchy Extension of the Riemann Integral 
Reverting to the one-dimensional domain ]0, oc[, a term f(oo)h(]u,oo[) 
appears 
in the Riemann sum of (2.20). Such a term may be undefined, and the value 0 
is sometimes assigned to such a term. Taking h(I) = \I\, the Riemann-complete 
integral L· 
Γ/(:Γ)|/| exists and equals the extended Riemann integral 
lim / 
f(x)dx 
a^ooJQ 

72 
CHAPTER 2. 
INTRODUCTION 
whenever the latter exists.17 
To prove this, suppose a = lirria^oo f£ f(x)dx. 
Then, with ε > 0 given, 
there exists αε so that b > a > αε implies that the Riemann integral satisfies 
Ja 
f(x)dx 
< e. 
Let ao = 0, a\ = αε, and choose a2,03,... so that 
as j —>> 00. Then for n > m > 1, the Riemann integral satisfies 
f(x)dx\ 
1 
J a„ 
< ε. 
Write Kj =]aj,a J +i] for j = 0,1,2, 
Choose fixed positive numbers η^ so 
that partitions Vj of Kj satisfy \I\ < 77j for each I G Vj, and 
α-(ν0)Σ/{χ)\Ι\\<ε, 
for j = 1,2,3,.... Define 
Γ+1/(χ)άχ-(ν^/(χ)\Ι\ 
J an 
< e2~j 
(5(0) 
< 
min {ryo, α ε}, 
5(oo) 
< 
ai 
- 1 
ö(cbj) < 
min{ryj_i, r/j, α^ — aj_i, 
CLJ+I — dj} . 
For aj_i < x < a^ let 
£(x) < min {r/j, x — %·_ι, α^ — x} . 
Thus (5 is a gauge on ]0, 00[. Let D b e a 5-fine division of ]0, 00[. Then V contains 
an element (00, ]i>, 00 [) which is £-fine, so v > ae. Let 
k = max {j : aj < v} . 
By construction of £, 
V = V0U/D1U--{J'DkU 
{(00, ]v, oo[)} , 
17If the domain is Ω =]0, oo[ x · · · x ]0, 00 [ then the situation is not so simple, and it is 
not possible to say that the Riemann-complete integral coincides with the extended Riemann 
integral in the multi-dimensional domain. See Muldowney and Skvortsov [174]. Apart from 
a few exceptional cases such as this, all of the standard methods of integrating functions— 
Riemann, Stieltjes, Lebesgue, Denjoy, Perron, Burkill, Kurzweil, McShane—can be expressed 
as Henstock integrals, using Riemann sums whose terms are selected in accordance with some 
rule (or "gauge") which selects associated point-cell pairs (x,I) 
for Riemann sum estimates 
specific to the particular method of integration in question. 

2.15. INTEGRABILITY 
ON ]0, oo[ 
73 
and (p)Y,f{x)\I\ 
= (Α>)Σ/(*)ΙΊ + E J = i ( ^ ) E / W I where V0 is a δ-
fine division of ]0, ae], and, for j = 1, 2,..., k, Vj is a (5-fine division of Kj = 
]aj,ßj+i]. Also, if (#,i") £ Dj then |J| < η^. Then 
£(^)£/0r)|/| = (Σ^θΣ^)!1!- Γ,+1/(χ)ώ] + f/(*)<&, 
and 
£(^)£/0r)|/| 
< 
T —1 
J aj 
Μ θ ε 
(Ι?,·)^/(χ)|7| - Γ + 1 f(x)dx\ + I f/(*)<&> 
«/aj 
|</α ε 
* Σ 
oo 
2ε. 
Therefore 
|a-(2>)£/(s)|J| 
< 
α-(1?ο)Σ/(χ)|Ι|| + 
< ε + 2ε = 3ε, 
Σ(^·)Σ/(χ)ΐ/ι 
7 = 1 
so the Riemann-complete integral of / on ]0, oo[ exists and equals the extended 
Riemann integral. 
The latter is called the Cauchy extension of the Riemann integral. See, for 
instance, Bartle [7], Theorem 16.5. Other versions of this result, such as those 
involving 
pb 
rb 
lim 
/ f(x)dx, 
= 
lim 
/ 
f(x)dx, 
a—>·αο+ Ja 
α—>>αο, a>a0 
Ja 
are proved similarly; see Henstock [103], Theorem 5.14. 
2.15 
Integrability on ]0, oo[ 
Example 16 The function cosx2|i"|, defined on ]0, oo[ x I([0, oo[), is Henstock 
integrable. The function cosx2 is not Lebesgue integrable on ]0, cx)[. 
Since eLX = cos a;2 + isinx2 
(where t = \/—T), this is related to the Presnel 
integral JQ eiX dx, which is considered in more detail in Chapter 6 and which is 
the basis of a random variation interpretation of quantum mechanics that cannot 
easily be accommodated in the traditional, axiomatic theory of probability. 

74 
CHAPTER 2. 
INTRODUCTION 
It is considered here, both as a means of introducing these issues, and as 
a means of further illustrating techniques of Henstock integration such as the 
"constrained partitions" of Section 2.14. The function cosx2|7| has zeros at 
on = \Ι^ψ^, 
n = 0,1,2,...; 
and, writing 
bn := / 
cosx2dx, 
Jan 
the terms bn are alternately positive and negative as n takes alternately odd 
and even values. Also |6n| -» 0 since αη+ι — an —>· 0 as n —> oo. Therefore the 
series Y^=0 bn converges non-absolutely. 
Each of the terms bn exists as a Riemann integral since cos x2 is a continuous 
function in [an, an+i], n = 0,1, 2, — Though the issue will be examined in more 
detail below, it follows directly from this that cosa;2|7| is Henstock integrable 
on [0,oo[. There are various ways of proving Riemann-complete or Henstock 
integrability in this case. 
The Riemann integral is not defined on the unbounded domain ]0,oc[, but 
the convergence of 5^^L0 ^η implies that the Riemann integral fQ cos x2dx con-
verges as a —t oo, so the extended Riemann integral exists. 
Noting that cosx2 is positive (negative, respectively) for an < x < αη+ι, 
n = 1,3, 5,... (n = 0,2,4,..., respectively), the Lebesgue integr ability in [0, oo[ 
of cos x2 is equivalent to the convergence of both of the series 
Σ {t>n : n = 1, 3, 5,...} , 
] T {-bn : n = 0, 2,4,...} . 
For each of the two series, a representative term |6n| has value greater than 
the area of a triangle of height 1 (the maximum value of | cos x2 \) and base 
Figure 2.4: Graph of cos x2. 

2.15. INTEGRABILITY 
ON ]0, oo[ 
75 
[αη,αη+ι], so 
K\ > 5 K + 1 - an) = | ^ f (V2n + 3 - >/2n + l) . 
Since the series 
diverges, both when summed over odd n and even n, it is clear that cos a;2 is 
not Lebesgue integrable on ]0, oo[ . 
Thus comparison with the series Σ^η shows that the integral on ]0,oo[ of 
cosx2 is convergent, but not absolutely convergent. While absolute convergence 
fails, cancellation between alternately positive and negative 6n, 6n+i enables 
non-absolute (or conditional) convergence to take place. 
Accordingly, it is instructive to examine the Henstock integrability (in the 
Riemann-complete sense) of cosx2|/| by direct consideration of Definition 9. In 
Chapter 6, Lemma 133, it is shown that 
/ 
2 | r i 
l 
Γ 
cos a r i = -\ —. 
1 1 
2V 2 
But the immediate purpose is not to determine the value of the integral but to 
show that it exists, in accordance with Definition 9. In other words we seek to 
show that there exists a number a with 
Γ 
Γα° 
°° 
/ 
cosx2\I\ = a, 
where 
a= 
cosx2dx + ^ J bn. 
J]0,oo[ 
JO 
n = 0 
/ 
cosx2|/| = / 
cosx2dx, 
J]0,a0] 
JO 
the first being a Henstock integral and the second a Riemann integral. There-
fore, writing af = J2^=o ^η, it is sufficient to prove that 
We have 
L 
cosaj 2|/|=a ;. 
(2.21) 
a0,oo[ 
This equation is intuitively obvious from inspection of the graph of cos x2, but 
it is instructive to consider it from the point of view of gauges in ]0, oo[. The 
left-hand side of equation 2.21 has meaning in terms of the 5-fine Riemann sums 
(Vs)^2cosx2\I\ 
of Definition 9, while the right-hand side is the limit of partial sums of a non-
absolutely convergent series J]^L0 bn. 
This suggests comparing each term bn of the series with some corresponding 
terms of the Riemann sum. A Riemann sum in this case contains, by definition, 
a term cos£2|/| with x = oo and I =]u,oo[. Neither cos a;2 nor |/| is defined 

76 
CHAPTER 2. 
INTRODUCTION 
in this case. Extend the definition of the integrand so that this term of the 
Riemann sum is assigned the value zero. The corresponding terms of the series 
Σ bn are not zero. But if ε > 0 is given, n' can be chosen sufficiently large that 
Σ*· 
n>n' 
and then i(oo) can be defined so that 
ε 
< 2 ' 
> <S(oo)' 
(2.22) 
The term "cosoo2|]^, oc[|" (= 0) of the Riemann sum must then have u > an>. 
Designating by n" the smallest integer n for which an > £(oo)~ , the (infinite) 
group of terms Ση>η" ^n> though not zero, has value less than ε2 - 1. 
The next step is to compare each of the remaining (finite) number of terms 
bn of the series Σ ^η with the corresponding groups of terms of the Riemann 
sum, in order to use this comparison to define a positive function δ(χ) (x φ οο) 
which will give the Henstock integrability inequality of Definition 9. Crucial 
to this is ensuring that the cancelation effects of the alternately positive and 
negative values of cos x2 come into play. Therefore, if we are seeking to compare 
a group of Riemann sum terms cn = J]cosx 2|/| with 
ro,n+i 
bn= 
/ 
cos x 171 
then, as in Section 2.14, we must ensure that the intervals 7 of cn are contained 
in [αη,αη+ι], in order to ensure that the factors cosx2 are either all positive or 
all negative. Otherwise the cancellation effect may fail to operate fully, making 
the result more difficult to prove. 
Accordingly, for an < x < αη+ι and n = 0,l,2,..., choose δη(χ) to satisfy: 
bn-(V6n)^co^x2\I\ 
< ε2-<η+1) 
for all £n-fine divisions of ]αη,αη+ι]; and define δ(χ) as follows: 
■ if an < x < an+\ then δ(χ) < min{x — an, a n +i — x}; 
• ifx = an, δ(χ) < 
πιίη{δη-1(χ),δη(χ)}; 
■ if an < x < αη+ι, δ(χ) < δη(χ). 
This ensures conformance, meaning that if x Φ an for any n, and if (x, 7) is 
J-fine, then 7 is a subset of some ]αη,αη+ι], and cannot intersect with two of 
them. Therefore any i-fine division V — {(x,7)} must include division points 
x = α 0,αι,... ,a m. 
With i(oo) defined to satisfy (2.22), choose a δ-üne division T>s of [ao,oo[. 
Because of the way δ has been defined, apart from a single interval ]u, oo[, the 

2.15. INTEGRABILITY 
ON]0,oo[ 
77 
intervals I of the division form groups, each group being contained18 in one of 
the intervals ]αη,αη+ι]. Any J-fine division then gives a Riemann sum of the 
form 
n" 
n" 
( P , ) ^ c o s x 2 | / | = ^ 
((Ü t f n)5^cosx 2|/|) ,= 
Y^cn. 
n=0 
n = 0 
Since 
X]&n-(^)X^COSX 2|/| 
| 71 = 0 
this implies that 
< 
n"-l 
n"-l 
n = 0 
n = 0 
+ Σ6-
n"-l Σ
ε 
ε 
ε 
ε 
^ 7 1 4 " ^ 
< 
2 + 2 
= 
ε' 
η = 0 
E^n-(^)X]cOSX2|/| 
|η=0 
so the function cosx2|/| is Henstock integrable on ]0, oo[. 
Example 17 For any cell J in ]0, oo[ write 
H(J) = ί 
cosx2\I\. 
Then H(I) is the indefinite integral of cos x2, and H(I) is defined for every cell 
I including those of the form ]u, oo[ 
o 
Example 18 For I =]u,v] write h(I) — (v — u)cos(uv). 
The integrand h(I) 
is Burkill-complete integrable on ]0,oo[, with integral equal to L· rcosx2 dx. 
This can be proved by a method similar to the one used in Example 5. 
Q 
Example 19 For I —}u^v], suppose the integrand is (cos xy)\I\, and, for a 
division V = {(x,I)} 
o/]0, oo[, suppose the Riemann sum is 
(V)^2(v-u)cos(xy) 
where, for each I, x is the division point, while the other evaluation point y is 
chosen arbitrarily, subject only to u < y < v, as in ordinary Riemann integra-
tion. Superficially, the integrand (cosxy)\I\ 
looks like it might have something 
to do with a two-dimensional integral on ]0, oo[x]0, oo[. That is not the case. 
This integral is a "mixture" of Riemann integration and Riemann-complete 
int-
egration. R could be formally denoted as 
L 
xei*,yei 
/ei(]o,oo[) 
(cos xy)\I\. 
This kind of construction has already been encountered in Section 2.14, in which δ(χ) is 
chosen so that it conforms to each of the bounded intervals ]α η,α η+ι]. 

78 
CHAPTER 2. 
INTRODUCTION 
As in the preceding example, calculating the integral in this way gives a result 
equal to L· rcosx2 dx. However, it is different from the forms of integration 
already encountered. Adding the variable element y to the integration means 
that a new element, additional to the variables x and I, has been introduced. 
This particular form of integration is not actually needed in this book. But in 
Chapter 3, an extra integration variable, denoted N, is introduced. The details 
and implications of this are worked out in Chapter 4- 
O 
2.16 
"Negative Probability" 
For F to be a probability distribution function on a sample space Ω =]0,oo], 
certain conditions on F should be satisfied. One of these conditions is that F 
should be an additive function of cells, with F(]0, oo[) — 1. 
If J e I(]0,oc[) define G(J) by 
Jj cos 
x2\I\ 
G(J) := jj0iOO[coe^i/r 
Then 
- G(]0,oc[) = 1, and 
- G(J) = G(Ji) + G(J2) when J = Jx U J2 with Ju J2 disjoint. 
But G would not usually be regarded as a probability distribution function, since 
G(I) can be negative—for instance, when / = [an, an+i] with n even. Therefore 
G is not a probability distribution function in the usual sense. However, in the 
Feynman path integral formulation of quantum mechanics, a complex-valued 
function analogous to G performs a role analogous to the role of a probability 
distribution function, and it is very useful to have a probability-style calculus 
for such a function. 
The preceding section shows that G cannot be defined using Lebesgue int-
egration, and the extended Riemann integral does not have sufficiently good 
properties to produce a probability-style calculus for Feynman path integrals in 
quantum mechanics. 
But the preceding section also shows that the cell function G can be de-
fined by Henstock integration. This makes it possible to analyze Feynman path 
integrals using the methods of probability theory. Thus, just as the Dirichlet 
function (2.12) marks a stage at which the Lebesgue integral supersedes the 
Riemann integral, the function G, a simplified version of a "Feynman ampli-
tude" or "Feynman probability distribution function", marks a stage at which 
the Henstock integral supersedes the Lebesgue integral in the study of random 
variability. 
Chapter 6 examines Feynman path integrals in more detail, using Henstock 
integration. But it may be worthwhile here to comment on use of the function 

2.17. HENSTOCK INTEGRATION 
IN R N 
79 
G (or some analogue of G) as a "probability distribution function". On the 
face of it, this is counter-intuitive since G(I) can be negative, and a probability 
is usually understood to have value between 0 and 1. Because, if probability 
is thought of in terms of relative frequency, an event either occurs or does not 
occur. There is no such thing as "negative occurrence". In other words an 
occurrence contributes positively to relative frequency, while non-occurrence 
contributes zero. Intuitively, a negative contribution to relative frequency is 
impossible. 
So how can there be a negative contribution to relative frequency or to 
probability? What possible sense can this make? 
The particles of atomic physics exhibit properties of interference or cancel-
lation, just as waves do. If a beam of photons is directed toward some measuring 
device, then the number of particles arriving at the device can be counted. Extra 
particles can make a positive contribution, producing a more intense beam. 
But the photons also manifest wave properties, and, depending on their phase, 
the interaction of waves can produce modulation or cancellation rather than 
intensification. In other words, extra photons can reduce rather than increase 
the intensity of the beam. Thus a "probability calculus" of such particles may 
require distribution functions which take negative as well as positive values. 
2.17 
Henstock Integration in Rn 
The preceding sections give an outline of the Henstock integral of functions 
defined in the domain ]0, l]x]0,1], and it is not difficult to transfer those ideas 
to a finite-dimensional, bounded domain ]αι, bi] x · · · x]an, bn]. The integral in 
the one-dimensional, bounded-below, unbounded-above domain ]0,oo[ has also 
been considered. This section outlines the definition of the Burkill-complete 
integral in a finite-dimensional version of this—in R n with n > 1. 
Denote the elements of R n by 
X = yX\, . . . , Xn)' 
We also need to consider x E R n, in which Xj can be oo or — oo for any j , 
1 < j < n. The class I(R n) of cells / of R n consists of the sets 
I = IX x ··· x I n, 
where each Ij can have the form 
] — oo, a], 
or ]u,v], 
or ]6, oo[, 
including the case ] — oo, oo[. 
In Riemann sums, points x and cells / are linked or associated in particular 
ways. In some presentations of the subject, the association condition is that x 
should be in the interior or on the boundary of I. But, as discussed previously, 
it is convenient to further restrict this by considering only those points x which 

80 
CHAPTER 2. 
INTRODUCTION 
are vertices of the intervals I with which they are associated in Riemann sums. 
Therefore point-cell association means that for each j we have, respectively, 
Xj = — oo, 
or Xj = u or v, 
or Xj = oo. 
If Ij = ] — oo, oo[ then Xj can be —oo or oo. 
A gauge in R n is a positive function δ(χ) defined for x G R n. An associated 
point-cell pair (x, I) of R n is 5-fine if, for each j , the pair (XJ,IJ) is <5-fine in R; 
that is, 
1 
1 
a<-— 
-, 
or \L\ = v - u < δ(χ), 
or b > ——-, 
(5(-oo) 
J 
o(oo) 
respectively. 
A partition of R n is a finite collection V of disjoint cells I whose union is 
R n. A division T> of R n is a finite collection of associated point-cell pairs (x, I) 
whose cells I form a partition of R n. Given a gauge δ in R n, a division V is 
5-fine if each (x, I) G V is <5-fine. A £-fine division is often denoted by T>s. (With 
gauge δ given, the existence of i-fine divisions is proven, as in the compact, one-
dimensional case, by a contradiction obtained by successive bisection in each 
dimension.) 
Examples of partitioning cells in R n can be seen in Figures 2.5, 2.6, 2.7, and 
2.8. A partition of R n must contain some unbounded intervals, since a finite 
number of bounded intervals cannot cover the whole domain. 
An integrand in R n is a function of associated points and cells, usually a 
product f(x)h(I), 
or, more generally, a point-cell function h(x,I). 
Without 
laying down any further conditions on the form or properties of the integrand 
we can give the definition of the integral, the wording of which is, conveniently, 
the same as in the versions given previously. 
Definition 10 A function h(x,I) 
is integrable on Rn
? with integral 
a = 
h(x,I), 
if, given ε > 0 there exists a gauge δ in R n such that, for each δ-fine division 
Ό$ ofHn, the corresponding Riemann sums satisfy 
a-(V6)J2h(x,I) < e. 
The significance of the variation of a function in the generalized Riemann sum 
approach to integration has been discussed. 
Conveniently, the definition of 
variation V/^ on R n also has the same wording as before; namely, 
V U R n ] - i n f { s u p | / i ( x , / ) | | . 
With gauge δ given, we can prove that 5-fine divisions exist for any figure E 
(i.e., finite union of cells) in R n. The definitions of integral on E and variation 
on E (JEh and V/JÜ7]) of any function /i(x,7), are, mutatis mutandis, worded 
in the same way as the definitions for the domain R n, which is itself both a cell 
and figure. 

2.17. HENSTOCK INTEGRATION 
IN R N 
Figure 2.5: Bounded cells of R x R. 
Figure 2.6: Bounded cells of R x R x R. 
□ 
Figure 2.7: Some unbounded cells of R x R. 
£ 
Figure 2.8: Some unbounded cells of R x R x R. 

82 
CHAPTER 2. 
INTRODUCTION 
2.18 
Conclusion 
While the basic Riemann integral is not an adequate basis for probability the-
ory, Riemann sums provide a strongly intuitive framework for the analysis of 
random variability. An adaptation of the definition of the Riemann integral 
provides a technical tool that is comparable to the Lebesgue integral, and that 
is an improvement on the Lebesgue integral in certain respects. In the face of 
challenges which have defeated the Lebesgue integral, the Riemann sum method 
of Henstock integration opens up a prospect of further advances in the theory 
of random variability. 
The literature of the Riemann-complete or ^-gauges method has focussed on 
integration of derivatives, a task that is important in the theory of differential 
and integral equations, but is not so important in random variability. 
The 
Riemann sum method admits non-absolute convergence of Riemann sums, as 
does the extended Riemann integral. It also presents significant advantages in 
the analysis of distribution functions, especially in unbounded domains, and 
this aspect of -complete integration is built on and developed in this book. 
What is taken from the Riemann-complete or Henstock-Kurzweil integral 
is the use of Riemann sums in which partitions are constrained by rules for 
selection and admission of Riemann sum components. An adaptation of Stieltjes 
and Burkill integrals then forms the basis for the Riemann sum method in 
random variability. 

Chapter 3 
Infinite-Dimensional 
Integration 
The previous chapter gives an overview of the Henstock approach to integration 
in progressively more "demanding" domains of integration. This requires char-
acterization of the point-cell pairs (x, I) of the various domains; and of various 
forms of linkage or association between the points and cells used in Riemann 
sums; and of various rules for selection of associated (x, I) in Riemann sums. 
In the basic Riemann-complete integral the selection rule involves a positive 
function δ(χ), often called a gauge. The 5-gauges method in Riemann-complete 
integration is an instance of a system or set of rules for determining variable 
elements in Riemann sums, a system that finds full expression in the Henstock 
integral, as set out in Chapter 4. We will apply the term gauge as a shorthand 
description of any such set of rules, once the variable elements and the relation-
ships between them have been decided upon. Then, depending on the gauge 
chosen, the Henstock integral reduces to the Riemann, Lebesgue, Riemann-
complete (Henstock-Kurzweil) and other integrals. 
Having established, firstly, the basic framework of associated point-cell pairs, 
and, secondly, gauges (or rules) for selecting the terms to be used in Riemann 
sums, the actual definition of the Henstock integral of a function depends on 
a straightforward inequality involving Riemann sums, as in Definition 10 in 
Chapter 2. 
These, then, are the basic building blocks of the theory. There are no special 
a priori or prerequisite properties (continuity, measurability, countable additiv-
ity and the like) for the point- or cell-functions that comprise the integrands. 
Such properties can sometimes be deduced for functions after their integrability 
has been established. 
For the purpose of using the Henstock integral in random variability, the 
domain that this book gives center stage to is R T, where T can have infinitely 
many elements; so the point-cell elements x, / on which integrands h(x,I) 
are 
defined are, respectively, points and subsets of R T. 
A Modern Theory of Random 
Variation: With Applications 
in Stochastic Calculus, 
83 
Financial Mathematics, 
and Feynman Integration. 
First Edition. By Pat Muldowney 
Copyright © 2012 John Wiley & Sons, Inc. Published by John Wiley & Sons, Inc. 

84 
CHAPTER 3. INFINITE-DIMENSIONAL 
INTEGRATION 
But the formulation of integration in this domain includes, as special cases, 
those finite-dimensional domains already considered; and when, in Chapter 4, 
proofs of the properties of the integral and variation in R T are given, these 
proofs apply equally to those simpler kinds of domain. 
The purpose of this chapter is to present and explain the concepts, term-
inology, and notation of the Burkill-complete version of the Henstock integral 
and its Riemann sums in R T, where T can be infinite. A fuller exposition of 
the theory, with proofs, is postponed to Chapter 4. The Henstock integral will 
be referred to as "the integral". 
3·1 
Elements of Infinite-Dimensional Domain 
Let T be an arbitrary set and let λί = λί(Τ) denote the class of finite subsets 
N of T. An element N of Af(T) is designated dimension set. The values taken 
by a function x defined on T can be denoted by 
{x(t) : t e T} , 
or 
(xt)teT 
· 
Denote the family of these sets of function values by R T. 
For example, if 
T = {1,2}, then 
RT = {(x(l),x{2)) : x(l) e R, x{2) e R} = R x R = R2. 
In that case a typical element of R T can be represented in alternative ways, 
such as 
(ar(l),a:(2)), or (xi,x2), 
or (xt)teT, or x T, or x(T), or simply x. 
In the latter case the symbol x is allowed to denote not only the mapping itself 
but also the 'tuple of values taken by x in R. In the first interpretation x is an 
element of T R, the set of real-valued functions defined on T; and in the second 
interpretation x is an element of the Cartesian product 
l[{R:t€T},= 
RT, 
the coordinates of x being x(t), = xt for t £T. No distinction is made between 
the two interpretations. They are used interchangeably, and the notation R T is 
assigned to both. In general, if T is any set and if Q is a subset of T (including 
the case Q = T), elements of R^ can be denoted in any of the following ways: 
If Q', Q" are disjoint subsets of T, with Q = Q' U Q", then 
x(Q) = {x{Q'),x(Q")), 
XQ = 
{XQ>,XQ»), 
and so on. 

3.1. ELEMENTS 
OF INFINITE-DIMENSIONAL 
DOMAIN 
85 
The subsets of T that we are most often concerned with are the finite subsets, 
to be denoted usually by the symbol N. The class of all finite subsets N οϊ Τ 
is λί or λί(Τ). 
The indexing set T that we are most concerned with is the set 
of non-negative real numbers [0, oo[, along with finite and infinite subsets Q of 
[0, oo[. These sets have an ordering "<" of their elements and, while the integral 
(i.e., the Henstock integral) is defined for an arbitrary set T, with or without 
an ordering, the ordering "<" in [0, oo[ will play a significant role in the theory 
of random variation, as will be seen in due course. 
Also, when T is infinite it is often impossible to specify elements x of R T by a 
listing or sequence of coordinates, so representations of the form x = 
(χι,χζ,...) 
cannot be used. Instead we use notations such as x = x(T) or x = (xt)teT when 
T is infinite. 
Let R = RU {—οο,οο}, the real numbers with "points at infinity" adjoined. 
Elements x G R T are 
x = (x(t) 
:teT) 
where some of the coordinates xt, — x(t), can be oo or — oo. 
Integration in any given domain involves: 
■ functions of points of the domain; 
■ functions of subsets of the domain; and 
■ formation of sums involving these functions. 
The domain R T and its elements or points have already been discussed. The 
next step is to consider the subsets of R T that are needed for integration. 
If T is a set consisting of a single element then, as before, the subsets needed 
for Riemann-type integration are cells I G I(R) of R. In this one-dimensional 
case, we choose intervals / of three kinds: 
■ intervals that are unbounded below and bounded above; 
■ intervals bounded below and above; and 
■ intervals bounded below and unbounded above. 
That is, we choose intervals of the form 
]-oo,a], 
]u,v], 
]6,oo[, 
(3.1) 
respectively, where a, u <v,b are arbitrary real numbers. 
If T contains a finite number of elements—for present purposes, let these 
elements be real numbers T = {ti,... 
,tn}, 
say—then, for 1 < j < n and 
arbitrary cells Ij of the form (3.1) in R ^ 
= R, denote an arbitrary cell 
I G I(R T) of 
R T 
= 
R{tl,...,tn} 
by 
Itl x · · · x Jtn, 
or simply Ιλ x · · · x In. 
(3.2) 

86 
CHAPTER 3. INFINITE-DIMENSIONAL 
INTEGRATION 
When n = 1 the definition (3.2) reduces to the definition (3.1). 
If T is an infinite set of real numbers, such as [0, oo[, the cells of R T that 
are required are called cylindrical intervals. Consider the class of finite subsets 
TV = {£i,..., tn} of T, for all possible orders n of TV, and all possible selections 
of elements tj (j = 1,..., n) of T. This class is denoted by λί, or λί(Τ). 
Then, 
for N £λί, with arbitrary cells Ij (of the form (3.1)) in R ^ > , let I{N) denote 
any finite-dimensional cell I\ x · · · x In of HN. 
At this point it is convenient—though not yet strictly necessary—to require 
specification of a particular permutation of TV under the "<" ordering, so we 
write TV = {ίχ,..., tn} with t\ < · · · < tn. 
Now let the cells (or cylindrical intervals) / of R T be 
I = I(N) xKT^N. 
(3.3) 
Whenever it is helpful to emphasize the "restricted" dimensions TV of the cylin-
drical interval / C R T, write i" as /[TV]. Thus, with round brackets, /(TV) is a 
finite-dimensional interval Ιχ x · · · x In G I(RAr), while, with square brackets, 
/[TV] is a cylindrical interval, or infinite-dimensional interval 
11 x · · · x In x 
Whenever a factor of this product is not equal to R we say that the factor is 
restricted. Generally, the set TV in which a cylindrical interval = /[TV] C R T 
is restricted is called the dimension set of /. Thus T \ TV includes all those 
dimensions t in which the cylindrical interval /[TV] is unrestricted.1 
If TV G Af(T) and p = PT/N denotes the projection 
R T 
H+ 
R^, 
x(T) 
-> x(TV), 
then we have 
I[N] = 
p-1(I(N)). 
Figure 3.1 shows an element x contained in a set 
Itl xlt2 
x / t 3 x R T \ ^ ' i 2 ' t 3 > ; 
with x(tj) e Itj for 1 < j < 3, and x(t) arbitrary for t ^ tj. 
The class I = I(R T) of cylindrical intervals /[TV] needed for integration is the 
class given by all possible selections of one-dimensional intervals Ij for tj £ TV, 
and then all possible choices of finite subsets TV of T, including all possible 
values of the positive integer n. In other words, with / = /(TV) denoting an 
xIt may sometimes happen that, for tj £ N, the context requires us to take Ij = I(tj) = R 
(so Ij is unrestricted). But this will be clear from the context, and usually N will denote only 
the set of dimensions t in which the cylindrical interval I[N] is restricted. 

3.1. ELEMENTS 
OF INFINITE-DIMENSIONAL 
DOMAIN 
87 
Figure 3.1: A path x = {x(t) : t € T} in a cell Itl x It2 x IH x RT\{ii.t2,i3} 
I 
I 
I 
I 
Figure 3.2: Three cells in R T. 

88 
CHAPTER 3. INFINITE-DIMENSIONAL 
INTEGRATION 
arbitrary interval in the finite-dimensional R^, and with / = I[N] denoting an 
arbitrary cylindrical interval in R T, 
I(R T) = {I[N} : I(N) £ I (RN) , N G λί(Τ)} . 
Note that if T is a finite set, or a set containing a single element, the definition 
(3.3) reduces to (3.2) and (3.1), respectively; since, without loss of generality 
we can take N = T and I[N] = I(N) = I(T). 
Thus, definition (3.3) can also 
serve for the finite-dimensional and one-dimensional cases. 
Figure 3.2 gives a three-dimensional illustration of some infinite-dimensional 
cells. Only three orthogonal axes can be shown, but others are suggested. Any 
such cell has infinitely many unrestricted dimensions. In contrast to the Carte-
sian product or orthogonal axes picture of Figure 3.2, the displacement-time 
representation of Figure 3.1 can only be used if T is an interval of real numbers. 
Figure 3.1 shows three one-dimensional cells i~i, J2, and Is on the displace-
ment (or vertical) axis, with times (or "dimensions") ti, £2» and ts on the time 
(or horizontal) axis. Figure 3.2 shows £χ, t<i, and £3 as labels for three orthogonal 
Cartesian axes; and 
/1X/2X/3X unities} 
is shown as a "box" in the Cartesian product space R T, with dimensions t Φ tj 
invisible. 
It is a useful visual and mental exercise to switch between the displacement-
time representation and the Cartesian product representation. Figure 3.2 con-
veys the geometry of cells I[N], but elements x are reduced visually to a point 
or dot. Figure 3.1 displays certain properties of x as a "path" in R T, providing 
a better visual display2 of the properties of a function /(#(·))—but at the cost 
of losing geometric sense of cells I[N] and how these cells partition R T. 
3.2 
Partitions of RT 
Integrating functions on the domain R T, with T infinite, involves calculating 
Riemann sums over constrained partitions of R T, in accordance with the meth-
ods outlined in Chapter 2. This in turn involves partitions and divisions of R T 
from which Riemann sums of integrands are constructed. 
A partition of R T is a finite collection V of disjoint cells I[N) whose union 
is R T. This concept is fairly straightforward when the space being partitioned 
has only a finite number of dimension. But it is useful to have some sense of 
the geometry involved when T is infinite, as in Figure 3.1. 
The following result can be understood visually by considering a pair of cells 
in the Cartesian product (orthogonal axes) representation of Figure 3.2. 
2Each element x(t) of any of the straight line segments in Figure 3.1 represents a component 
x(t) of a point x = (x(t))teT 
contained in the infinite-dimensional cell I = I\ x I2 x I3 x 
RT\{*i»*2>*3}a The element x chosen for this particular illustration happens to be continuous 
with respect to t. In fact x G / could be discontinuous at every t. 

3.2. PARTITIONS 
OF R T 
89 
Theorem 1 Suppose M and N belong to λί(Τ) 
and suppose I[M] and J[N] 
are disjoint cells in R T. Then M Π Ν Φ 0, and there exists t G M Π N so that 
itnjt 
= 0. 
Proof. Suppose, for contradiction, that M Γ\ Ν = 0. To visualize this, suppose 
M and TV are represented, respectively, by two of the orthogonal axes in Figure 
3.2; so each of these axes represents, not one, but a finite set of dimensions. 
Then choose any x(M) — XM £ I(M), any x(N) = XJV £ J(N), and, for each 
t G S — T\ (M U iV), choose any xt G R. Then x = (XM,aijv? (^)tes) belongs 
to both I[M] and J[iV], which are assumed disjoint. Therefore 
M f l i V / i 
To show that, for disjoint I[M], J[iV], there must exist t G M Π N for which 
Jt Π Jt ^ 0, we suppose, for contradiction, that for each t G M Π TV there exists 
xt e ItnJt. 
Accordingly, choose x(M Π N) £ I(M Π Ν) Π J(M Π N), and for 
ί G 5' = Γ \ (M Π TV) let x£ G R be arbitrary. Then x = (x(M Π 
N),x(S')) 
is contained in J[iV] Π J[M] which, by assumption, is empty. This gives the 
required contradiction. 
O 
The union of a finite number of cells of I(R T) is not necessarily a cell. As 
before, such a union is called a figure. If the component cells of a figure are 
disjoint they constitute a partition of the figure. 
Suppose ^[N1] 
and I2[N2] are disjoint cells in R T. For their union to be 
a cell I[N] of R T, the following conditions must be satisfied. Firstly, we must 
have N1 = N2,= N. Secondly, there must exist t e N1 = N2 = N such that 
■ the one-dimensional projections satisfy I1(i) Π I2(t) = 0; 
- l\t)Ul2(t) 
Gl(R); and 
- for those t' G Nl = TV2 = N for which t' φ ί, we have Ix{t!) = 
I2(t'). 
Again, this can be understood visually by placing the cells in the Cartesian 
product or orthogonal axes framework of Figure 3.2. 
Next consider the interfaces that can exist between disjoint cells such as the 
cells in a partition. Suppose ^[N1] 
and I2[N2] are disjoint, so N1 Π N2 φ 0. 
Suppose there exist t e N1 Π N2 and x G R T so that 
. I\t) 
=]uxy] 
and I2(t) =]u2,v2], with 
■ x(t) — ul — v2 or x(t) = v1 — u2. 
(Cells P(t) of the form ] — oo,i>] or ]u, CXD[ should also be considered.) If there 
is only one such t and only one such x, then the interface is a vertex common 
to both ^[N1} 
and I2[N2]. 
This interface can be called a "zero-dimensional 
plane". If there is only one such t but more than one x, then J^iV1] and J2[A^2] 
share a common edge or line—a "one-dimensional plane" as interface. Similarly, 
the interface between 71[iV1] and I2[N2] can be a two-dimensional plane, or a 
higher (perhaps infinite-) dimensional hyperplane. 

90 
CHAPTER 3. INFINITE-DIMENSIONAL 
INTEGRATION 
3.3 
Regular Partitions of R T 
This section demonstrates how partitions of a Cartesian product domain can 
sometimes be re-arranged in order to simplify calculation of Riemann sums. 
Each of Figures 2.5, 2.6, 2.7, and 2.8 shows some cells of partitions in domains 
R x R and R x R x R. In each case, the cells are staggered or offset against 
each other. They do not line up in rows or columns, with edges or faces of one 
cell continuing on to form edges or faces of other cells. 
The same applies to the sample of RT-cells in Figure 3.2. A partition of R T 
can be formed with such staggered or offset cells. 
In contrast, the partition on the left in Figure 3.3 is composed of cells which, 
while offset against each in the horizontal direction, are aligned in vertical 
columns of cells. The right hand partition in Figure 3.3 is a partition in which 
the cells are aligned both horizontally and vertically. 
Call the left-hand picture in Figure 3.3 a partially regular partition, the 
right-hand one being a regular partition. 
Figure 3.4 starts with an irregular partition in R x R, with cells offset against 
other cells in both directions or dimensions. But by extending all the edges or 
faces of all the cells of the original partition, the original cells are divided up, 
and the resulting sub-partition consists of cells that line up in both directions. 
In this way, the original partition is regularized. If the extension of edges or 
faces in R x R is performed in one dimension only, then the original partition 
will be partially regularized. 
In general, for Cartesian product spaces R T, subdivision of the cells in a 
partition V 
V = {I[N]}, 
creates a sub-partition V in which the Riemann sum (Ρ') ^h{x^ I[N]) is some-
times easier to calculate than the original Riemann sum {V)^h(x, 
I[N]). Let 
M0 = 
\J{N:I[N]eV}. 
Enumerate MQ as {τχ,... , r m o } , with τ\ < · · · < rmo. If I[N] G V with TV = 
{ii,...,£ n}, write 
I[N] 
= 
Ifa) x · · · x I(tn) x RM°\N 
x RAMo 
C 
Ri4l> x · · · x R ^ > x RMoW x RAMo. 
If M c M0 and I[N] G V, take I(M) to be 
Ι(ΜΠΝ) 
x R M \ N , 
and then the sets I[M] and I[M Π N] are the same; each being equal to 
Ι(ΜΠΝ})χΚτ^ΜηΝ\ 
Write I(tj) =]u,v], G I(R { i^). Then, for tj = rk G M0, let STk denote all such 
partition points 
{u,v:I[N] 
eV}. 

3.3. REGULAR PARTITIONS 
OF R T 
91 
Figure 3.3: The left hand partition is partially regular; the one on the right is 
regular. 
• 
I 
- + -
I 
I 
I 
l·-
Figure 3.4: The picture on the left shows how an irregular partition in R2 can 
be partially regularized by extending the vertical interfaces. The picture on 
the right shows how an irregular partition in R2 can be fully regularized by 
extending all the interfaces. 

92 
CHAPTER 3. INFINITE-DIMENSIONAL 
INTEGRATION 
For k = 1,..., mo, enumerate STk as 
Srk = {Sfclj Sfc2, · · , Sfcpfe} 
W i t h 
5fci < 5fc2 < · * * < Sfcpfc , 
and let PTfc be the following partition of R^Tfc^: 
^T f c = {] - 0 0 , 5 / c i ] , 
] S f c i , S f c 2 ] , . . . , ] S f c P f c , 0 0 [ } . 
Rewrite PTfc as 
^rfc = {Jkrk 'rk = l,2,...,p f e}. 
Now form a partition P ' of R T consisting of the cells 
J[M0] := Jlri x · · · x Jmormo x R T\ M o 
(3.4) 
for all possible choices of Jkrk £ VTk, and for 1 < k < mo- Then each I[N] G V 
is the union of cells J[MQ] G V. Because of the way V has been constructed, 
R T can be expressed as the union of the cells in V' taken in the following order: 
Pm0 
/ 
/ p2 
/ Pl 
\ 
\ \ 
U 
■· 
U 
U 
Jlr1XJ2r2X'"XJm0rrno 
X R^M° 
) ' ) ) , 
(3.5) 
rmo=l \ 
\r2 = l \ri = l 
/ 
/ / 
and because P ' can be ordered in this way we call it regular. Call the construc-
tion above a regularization oiV. 
Sometimes a partial regularization of a given partition V is what is required, 
and not full regularization as expressed in (3.5). The meaning of this is as 
follows. 
With V and MQ as before, suppose Mo is the disjoint union of proper subsets 
Mi and M2, and suppose we wish to "regularize" V in the dimensions M\. In 
this case, instead of the intervals J[Mo] of (3.4), form a partition V" of R T 
consisting of the cells 
J"[M0] := I(M2 DN)x ]J{Jkrk 
: rk G M J x R T\ M° 
(3.6) 
for all possible choices of I[N] G V and for all possible choices of Jkrk G VTk, 
Tk G Mi. In this case, R T can be expressed as the union of cells of the following 
kind: 
U {/(Mi Π TV) x (j[{Jkrk 
: n G M2} x R T\ M°) : I[N] G V, Jkrk 
eVTk}. 
(3.7) 
Regularity, partial regularity, and regularization are illustrated for two dimen-
sions in Figures 3.3 and 3.4. Partial regularization is obtained by extending to 
±00 all the vertical margins or interfaces corresponding to partition points in 
Mi = {ti} = {TI}. Full regularization is obtained by additionally extending all 
the horizontal margins corresponding to partition points in M2 = {£2} = {r2}· 
Classical Lebesgue integration theory involves early engagement with soph-
isticated and exotic constructions such as Cantor sets and non-measurable func-
tions in order to determine the meaning, scope and extent of the theory. Yet 

3.4. δ-FINE PARTIALLY 
REGULAR 
PARTITIONS 
93 
the actual integration of particular functions often requires only elementary 
calculation, such as determining a primitive function or anti-derivative. 
Similarly, the Riemann-complete integral in one dimension requires careful 
choice of partitions to determine the integral of a highly oscillating integrand 
such as the function cos x2 in Section 2.15 in Chapter 2. The expressions sin x~2 
and cosx - 2 in (2.13) require similar attention. And careful selection of Riemann 
sums is needed in the proof of theorems such as the Fubini's theorem. But in 
routine calculations, many functions can easily be integrated using much more 
straightforward partitions. 
This mixture of the complicated and uncomplicated also applies in infinite-
dimensional integration. The Riemann sum theory of Henstock requires use 
of irregular partitions which, though conceptually straightforward enough, can 
be difficult to calculate. But in practice it is often found that the regular and 
partially regular partitions described in this section make it possible to compute 
the corresponding Riemann sum by adding up the terms sequentially in each 
successive dimension. 
3.4 
5-Fine Partially Regular Partitions 
For convenience, label the axes of R2 as Ri x R2. Suppose a gauge δ is defined 
on Ri x R2. An arbitrary £-fine division V will, in general, consist of cells Ιχ χ Ι2 
which are offset against each other, with edges not aligned either horizontally 
or vertically. 
But it is demonstrated below that it is always possible to find a £-fine division 
& = {(*<*>, J<*>) = (*<*>, 4η x (#\4k)) 
■ 1 < * <p} 
which is partially regular, so, as in Figure 3.3, the cells J^ 
can be grouped 
in such a way that, in each vertical group, the vertical edges of the cells are 
aligned. Likewise, a 5-fine division D2 of Ri x R2 can be found so that the 
horizontal edges are aligned. 
This is called the Fubini property, as it is the construction on which the 
Riemann sum proof of Fubini's theorem depends. This is especially important 
for this book, since the central theme of joint random variability is heavily 
dependent on extensive application of Fubini's theorem. 
While it is possible to form J-fine divisions that are aligned (or partially 
regular) in any dimension, it is not in general possible to find a J-fine div-
ision that is fully regular, with edges of the partitioning cells aligned in every 
dimension or direction, as in the right hand picture in Figure 3.3. 
To see how the Fubini property works, suppose a gauge δ{χι,Χ2) is defined 
for all x = (xi,x2) 
G Ri x R2· Choose3 an arbitrary division V of R x R. 
In general, the edges of / = I\ x I2, for (x, /) G V, are not aligned, either 
horizontally or vertically. 
It is advisable, for each step in the argument that follows, to draw a sketch on the lines 
of Figures 3.3 to 3.4. 

94 
CHAPTER 3. INFINITE-DIMENSIONAL 
INTEGRATION 
Choose 2/1 G R. The vertical line {yi} x R2 intersects with a (finite) subset 
Qyi of the (finite) set of pairs 
(χΜ,ιω) = (x[j\i[j)) x (x(
2
i],4j)) e V. 
This selects a finite set Syi of division points or tag-points x^ = (x[J\x<2 M 
for which ( x ^ ) , / ^ ) G Q^. Define a positive number £1(2/1) by 
S1(y1)=mm{5(x^) 
: χ ω = ( Χ ^ , Χ ^ ) G 5 y i} . 
Repeat this argument for every y\ G R. This defines a gauge δχ in Ri, with 
values Si(yi) > 0, y\ G Ri. 
Choose a #i-fine division 
V1 = 
{(zuJ1)} 
of Ri. Then the finite collection 
V' = {((z1,x2),J1xI{
2
j)) 
: ( ^ J O e P i , 
( ι « ) , ΐ ΰ ) ) 6 υ } 
is a (5-fine division of Ri x R2 in which the vertical edges of the cells 
((*ι,ζ 2),.7ιχ/ 2
ϋ )) 
are aligned, so V is a partially regular (5-fine division of R2. 
Note also that V is a refinement of P, in that every cell 1^ of V is parti-
tioned by cells Jx x I^j) of £>'. 
Given a function h(x, I), the Riemann sum (Df) J^ /i(#, J) can be expressed 
as 
ΣίΣΜ^'^ΜιΧ^) : (*Ü),/Ü)) €<?*,} : («i,Ji)ePi}. 
In other words, Riemann summation takes place consecutively in each of the 
vertical slices produced by the <S-fine division T>\ of Ri, as in Figure 3.3. 
In terms of Fubini's theorem, with f(x) = f(x\,x2) 
a nd |/| = |i"i x h\, this 
corresponds to evaluating an integral JR2 f(x)\I\ on R2 = Ri x R2 by means 
of iterated integrals 
/ 
( / (x1,x2)dx2)dx1, 
= / ( / 
(x1,x2)\h\)\Ii\. 
JRI \Jn2 
/ 
^Ri \^R2 
/ 
The proof of Fubini's theorem in Section 4.11 depends heavily on this refinement 
and partial regularization of the division in the product space. 
Starting over again with the same 5-fine division V of Ri x R2, the same 
argument can be repeated to find a 5-fine division V" which is a refinement of V 
and which is partially regular in the horizontal (rather than vertical) direction. 
The Riemann sum that this ordering of terms gives rise to corresponds to turning 
an integral JR2 f(xi,x2)\Ii 
x I2\ on R2 into an iterated integral 
/ 
( / / ( * ! , 
Jn2 
WRi 
x2)dx\ I dx2 

3.5. BINARY PARTITIONS 
OF R T 
95 
3.5 
Binary Partitions of R T 
This section describes a useful system of regular partitions of R T. Suppose q is 
a given positive integer. Define a binary partition 
JCq = iKq^k :fc = 0,l,2,...,g2 9 + 1 + l } 
of R as follows. 
K"\° 
:= 
]-oo,-q], 
K«\«2"+1+1 
:= ]q,oo[, 
and, for 1 < k < ?2«+1, 
Kq\k :=]-q 
+ (k- 
1)2-9, -q + k2~q] . 
(3.8) 
Thus, each of 
/C1 
= 
{ ] - ο ο , - l ] , ] - l , 0 ] , ]0,1], ]l,oc[}, 
K? 
= 
{ ] - o o , - 2 ] , ] - 2 , - | ] , ] - Z , - § ] , . . . , ]|,2], ]2,oo[}, 
is a binary partition of R, as are /C3, /C4 and so on. Let r be a positive integer, 
and, for 1 < j < 2r, let 
r{
J
r)=T/ + (T-Tf)j2-r. 
(3.9) 
Note that τ' = TQ , and write 
Let q be given, and let 
w" = {0,1,2,..., k,..., 
Q2«+1, q2"+1 + 1} , ro'« = {1,2,..., k,..., q2"+1} . 
For given r and 1 < j < 2r, write 
k = 
{ki,...,kj,...,k2r) 
where, for each j , we have kj G wq. Thus k is a permutation, containing 2r 
terms, of the elements of wq, with repetitions allowed. Write 
zuri? 
= 
{k : kj £wq, 
1 < j < 2 r } , 
tz/r<? 
= 
{k : fy G zu/i?, 1 < j < 2r} , 
(3.10) 
w~rq 
= 
{ k i f y G t u 9 , 
l < j < 2 r - l } . 
The collection wrq is the set of permutations, with repetitions, of the elements 
of τυς, taken 2r at a time; and wfrq, w~rq have similar interpretations. 

96 
CHAPTER 3. INFINITE-DIMENSIONAL 
INTEGRATION 
With r and q given, choose k G vorq. This determines a 2r-dimensional cell 
Kq^ 
G I(R M r), and a corresponding infinite-dimensional cell Krq^ 
G I(R T), 
constructed as follows. 
^<?|k . _ TT J^q\kj 
ftrq\\<L 
. _ ^ g | k 
χ j ^ T \ M r 
Thus we get a family of cells in R T: 
1Cq := {iT«|k : kGn; r4, 
(3.11) 
in which the elements Kr<?lk of Krq are in one-to-one correspondence with the 
permutations k of wrq. Similarly, 
£/rg . = (Krq\lc 
. fc ^ ^ / r g 1 
consists of those i^r^lk which have no component intervals of the form ] — oo, — q] 
or ]q, oo[. 
Theorem 2 For ^z^en positive integers r, g, ί/ie finite collection Krq is a parti-
tion ofRT 
= R'T,'r]. 
Proof. This follows from the construction of JCrq. 
o 
Call the collection JCrq a binary (r, g)-partition, an (r, g)-partition of ΐϋτ 
'T\ 
or simply a binary partition of R T. 
The partition fcq of R contains g29+1+2 intervals (or cells), and the partition 
JCrq of R T contains 
( 2r )g2« + 1+2 = 
2rg2« + 1+2r 
( 3 Λ 2 ) 
cells. To illustrate, take r — q — 1. Then /Cr(? = /C1'1 consists of 16 cylindrical 
cells partitioning R^r ,r'. Writing 
M ^ M ^ r ™, 
r ^ } = { i ( r - T ' ) l r} , 
cells are 
^1,11(0,0) 
= 
2^1,11(0,1) 
= 
2^1,1|(0,2) 
= 
2^i,i|(o,3) 
= 
2^i,i|(i,o) 
= 
2fi.i|(i,i) 
= 
2^i,i|(i,2) 
= 
2^i,i|(i,3) 
= 
2^1,l|(3,3) 
= 
] — oo, -
] — oo, -
] — oo, -
] — oo, -
]-i,o] 
]-i,o] 
]-i,o] 
]-i,o] 
]l,oo] 
-i] 
-i] 
-i] 
-i] 
X 
X 
X 
X 
X 
X 
X 
X 
X 
] - o o , -
]-i,o] 
]o,i] 
]l,oo] 
] - o o , -
]-i,o] 
]o,i] 
]l,oo] 
]l,oo] 
-i] 
-i] 
X 
X 
X 
X 
X 
X 
X 
X 
X 
Rl r' 
R'T' 
B)T' 
B)r 
B)T' 
B)T' 
R ] T / 
R 1 T ' 
B)T> 
,τ]\Μλ 
,r]\Mi 
,r]\M^ 
,τ]\ΜΎ 
,τ]\Μι 
,r]\M1^ 
,r]\Mi| 
,T)\M^ 
,T]\MK 

3.6. RIEMANN SUMS IN R T 
97 
When r = 4 and q = 3 the partition /C9, = /C3, contains 50 intervals of R, 
and the partition /Crg, = /C4'3, contains 1650 cylindrical cells of R^r 'rl. The 
following is one of them: 
Ä-4,3|(37,4,...,5,10) 
= 
] _ 3 + |f, _ 3 + 37] 
x 
] _ 3 + J ^ _ 3 + £] 
χ 
... 
··· x ]-3+£,-3+£] x ]-3+Jr,-3+±§] 
X R]T',r]\M45 
where k = (37,4,..., 5,10) has 24 = 16 components and 
M4 = {rj 4 ) = / + (r - T0J2- 4 : j = 1, 2,..., 1β} . 
3.6 
Riemann Sums in R T 
As in domains R and R n, an integrand in R T can be viewed as a function 
h = h(x,I) of points x, and cells /—which, in an infinite-dimensional domain, 
have the cylindrical form I[N]. An integrand h may be a product /(χ)μ(Ι) 
of 
a point function and a cell function. The point x of a point-cell pair (x, I) is a 
division point; sometimes called a tag-point or tag. 
The point x at which an integrand f(x) or h{x, I) is evaluated is called 
an evaluation point. Often the evaluation point is the division point. But in 
stochastic integrals (Chapter 8) this is not usually the case, and a different rule4 
applies. 
In finite dimensions the pairs {{x\,... x n), I\ x · · · x In) on which an integrand 
h operates are said to be associated or linked, and this linkage in R n will be 
used to define point-cell linkage or association in R T. This can mean that the 
point is in the interior, or in the closure, or on the boundary of the cell. But, as 
in previous sections, we restrict the associated points x of the partitioning cells 
/ of Riemann sums in R T so that each point is a vertex of the corresponding 
cell. 
Accordingly, if i" = Ιχ x · · · x In G I(R n) with 
Ij =] — oo, a], 
or 
]u,v], 
or 
]6, oo[ 
for 1 < j < n, 
then (x 1 ;... xn) is associated with I\ x · · · x In if, respectively, 
Xj = —ex), 
or 
u or v, 
or 
oo 
for 1 < j < n. 
In R T with T infinite, we say that a pair (x,I[N]) 
is associated in R T if 
the corresponding finite-dimensional pair (x(N),I(N)) 
are associated in the 
In that case it is found that the evaluation point depends on the partitioning cell J; the 
choice of / determines the evaluation point. If the division point associated with / is denoted 
x, then a different letter, such as y, should be used to denote the evaluation point, so the 
integrand has the form k(I) = f(y)h(I) 
or k(I) = h(y, I), with y determined by the choice of 
/, and not by the rule of point-cell association (x, / ) . 

98 
CHAPTER 3. INFINITE-DIMENSIONAL 
INTEGRATION 
finite-dimensional space KN; that is, if x(N) is a vertex of I(N) in the finite-
dimensional space ΈΙΝ. 
Thus, in R N, division points x(N) are associated, in a multifunction re-
lationship, with cells I(N) of which they are vertices. For the corresponding 
(x,I) G R T x I(R T), we have a corresponding multifunction relation of assoc-
iation between points cells I — I[N] and their vertices x. 
In the next section candidate functions are examined for integrability on R T 
with T infinite. We will see that integrands h may depend explicitly on variable 
elements N G λΓ, the class of finite subsets of T, in addition to dependence on 
points x and cells / = I[N] of R T. So it is also convenient at this stage to 
extend the notion of association to include not just variable points x and cells 
J = I[N], but also the variable dimension sets N that specify the restricted 
dimensions of the variable intervals / = I[N] on which the integrand h depends. 
Thus, an integrand in R T may be expressed as h(x, AT, I[N]), where the triple 
(χ,Ν,Ι) 
are associated if the pair (x(N),I(N)) 
are associated in HN. 
In other words, (x, N, I) are associated in R T if / = I[N] and x(t) is a vertex 
of I(t) in R for each of the finite number oiteN. 
If (x, TV, I) are associated in 
R T, and if t G T \ AT, then x(t) is an arbitrary element of R, = R ^ . 
For any partition V = {/[AT]} of R T, a corresponding Riemann sum can be 
formed: 
(V) Σ Hx, N, I[N}) = ^{h(x, 
N, I[N}) : I[N] € V}. 
For the purpose of defining the integral in R T, it is required that each x, N be 
associated with the corresponding / = I[N] in each term of the Riemann sum. 
This is signified by writing 
xel* 
=I[N]*, 
so x(N) is a vertex of I(N) in R N. This can also be written as I[N] G #*, 
meaning that I and N are associated with x in R T only if N is the set or 
restricted dimensions of / = /[AT], and I(N) has x(N) as a vertex in R^. 
In Section 3.8, further conditions and constraints are placed on the point/dim-
ension set/cell linkage (x, AT, I) in order to define an integral in RT. 
If the integrand h(x,N,I[N]) 
is independent of the points in R T 
and depends only on cells I[N]—for instance, if h is a distribution 
function Fx(I[N]) 
(or Fx(N,I[N]))—then 
it is sometimes possible to 
simplify the computation of the Riemann sum by forming a regular 
sub-partition V' of P, and then summing over V1'. 
3.7 
Integrands in R T 
The next step is to examine some of the integrands encountered in R T. 
Example 20 Take T =]a, 6], and let C be the set of continuous x in Rla'6l. 
Define a real-valued function f{x) for x G R'a>6l; 
fM - ί exP (- la χ(*)ά1) 
if xtC, 
( 
, 
m'\0 
if xeR^\C. 
{ 
} 

3.7. INTEGRANDS 
IN R T 
99 
Next, consider an example of a function μ defined on cells I ofR)a,b\ 
with 
I = I[N] = h x · · · x In x B)a^\N 
G I ( R ] O ' 6 ] ) . 
Suppose \Ij\ is defined to be Vj—Uj when Ij is a bounded cell ]UJ.VJ] and suppose 
\Ij\ := 0 otherwise. Then define 
μ{Ι) = 
\Ι[Ν]\:=Υ[\Ι,\. 
It can legitimately be asked whether or not the point function f is integrable with 
respect to the cell function μ on Έθα^, or, in other words, whether the function 
h(x,I) = /{χ)μ(Ι) 
is integrable on B)a>bl 
O 
To answer this question, integrabihty on an infinite-dimensional domain must 
first be defined. The definition will involve some, but surprisingly little, adapt-
ation of the earlier definitions of integrabihty. This will be done in the next 
section, and in fact the wording will be almost exactly the same as in previous 
definitions of the -complete integral. 
On the face of things, it would appear that any real-valued expression in-
volving /i(I), when summed in R^a'^, could give a very large result, especially if, 
as above, the other factor f(x) is always positive. It would then be surprising 
if the Riemann sums converged to give an integral. Much of the rest of this 
book is concerned with establishing the integrabihty on R T, with T infinite, of 
particular functions and of particular classes of functions. 
The kind of integrands that are important in the context of random variables 
with sample space R T must also be borne in mind. For instance, taking the 
cell function μ(Ι) = |/[Af|| as in Example 20, suppose / is a point function, 
depending on x G R T, but which actually depends only on the values taken by 
x on some fixed, finite set 
M = { r 1 , . . . , r m } c T . 
Such a function is called a cylinder function, and will often be denoted by / M · In 
addition to dependence on the values χ(τχ), χ(τ2),..., 
x(rm), a cylinder function 
/ M will also be allowed to depend explicitly on the elements Tj of M; so / M ( # ) 
has the form/ 
(M,x{M)). 
Example 21 To get an idea of how cylinder functions are integrated, revert for 
a moment to a finite-dimensional scenario, and consider a function 
f(x\.x2) 
defined in [0,1] x [0,1], such that f depends only on x\. 
This means that, for 
any given x\ G [0,1], 
f(xi,x2) 
= f{xi,xf
2) 
for all x2, x'2 G [0,1]. 
Now define a function f1 by 
fl{xl):=f{xux2) 
for 
XiG[0,l]. 

100 
CHAPTER 3. INFINITE-DIMENSIONAL 
INTEGRATION 
This is well defined, since we get the same value for fl{x\) 
no matter which X2 
is chosen. Using the notation of Riemann-complete integration, if f is integrable 
on [0,1] x [0,1] then f1 is integrable on [0,1], and the following holds by FubinVs 
theorem: 
[ 
f(xux2)\Ii 
x h\ = [ 
f{xllx2)\h\\h\ 
= [ 
fH^lhl 
*/[0,l]x[0,l] 
J[0,l]x[0,l] 
J[0,1] 
Conversely, if integrability on [0,1] of f1 is assumed and if some Tonelli-type 
condition is satisfied, then the integrability of f in [0,1] x [0,1] may be deduced. It 
is sometimes helpful to use regularizations of partitions, (3.5) or (3.7), in order 
to compute the Riemann sums for these integrals. To sum up, in conventional 
notation 
/ / f(xux2)dx1dx2 
= 
^{xijdxx 
Jo Jo 
Jo 
if either integral exists. This establishes the connection between the two-dimens-
ional integral and the one-dimensional integral. 
O 
Example 22 Cylinder functions appear in infinite-dimensional spaces such as 
Rj°>°°[. For an example, take a fixed subset M = {ri,...,r m} of ]0, oo[ and 
write Xj = X(TJ) for 1 < j < m. Define functions f1 and fM for x(M) = XM € 
R M and x G RJ0'00^ respectively, as follows. 
f\x(M)) := n(27r(ri-rJ-_1))-*exp(-|j^^)) 
(3.14) 
fM(x) := 
f\x(M)). 
Then the function fM is cylindrical. To see this, write T' =]0, oo[\M, and take 
any two points x — {ΧΜ-,^Τ'), 
Χ' — {ΧΜ,Χ'Τ') 
°f R^0'00^ which coincide in the 
dimensions M. Then 
fM(xM,XTf) 
= ΙΜ(ΧΜ,ΧΤ')Ι 
0r /M(X) = 
ΪΜ{Χ'), 
since f1 depends only on XM and does not depend on χτ' - As in Example 21, 
the question then arises whether the integrability of fM m R^0,oot is related to 
the integrability of f1 in R M . 
Depending on the integrator or cell function μ(Ι) that is chosen, it may be 
possible to convert the integral on Rl0'00! of 
h(x,N,I[N]):=fM(x)ß(I[N]) 
into an integral on the finite-dimensional domain R M ; just as, in Example 
21, the two-dimensional integral was converted into a one-dimensional integral. 
This particular cylinder function is important in random variability and will be 
considered in more detail later. 
O 

3.8. DEFINITION OF THE INTEGRAL IN R T 
101 
Example 23 Now consider 
f(x) = Π (2π(ί,· - ί,·-ι))"* exp (~ ^
Ι
^
) 
, 
(3-15) 
wzi/ι Xj = x(tj) for tj G TV. In this case f depends not just on the variable 
co-ordinates Xj for variable x, but also on the variable N = {ti,... 
,tn}, 
and 
this is something new which does not arise whenever T is a finite set. Since f 
depends explicitly on the variable elements tj G TV as well as on the variables 
Xj = x(tj), it is appropriate to write (3.15) as f(x, N). Our theory of integration 
will enable us to consider the integrability of h(x,I,N) 
= f(x,N)ß(I), 
with 
μ(Ι) = |/[TV] | as in Example 20, where the integrand h depends explicitly on the 
variable finite subsets N G ΛΛ 
O 
The class of integrands that we are interested in consists of real- or complex-
valued functions h defined on associated triples 
(x,N,I[N]): 
(R T,Af,I(R T)) 
A 
R o r C , 
(x,N,I[N]) 
-> 
h(x,N,I[N]), 
(x,N,I[N]) 
G 
(R T,A/\I(R T)), 
x 
G 
I[N]*. 
If T happens to be a finite set, then integrands h(x,N, I[N]) reduce to functions 
of the form h(x,I) where x and I are familiar finite-dimensional objects. 
3.8 
Definition of the Integral in R T 
To establish some procedure by which we can test whether successive Riemann 
sums converge to some limit, the partitioning cells I[N] must "shrink" or become 
successively smaller in successive partitions of R T. As in the finite-dimensional 
integration, in the domain R T this "shrinking" is also accomplished by means 
of gauges. Given x — χτ G R T, let x* denote the class of cells I[N] for which 
x(N) is a vertex of I(N) in the finite-dimensional space HN. 
Just as was done in finite-dimensional domains, in this case too a function 
δ(χ) can be used to place a bound on the lengths of restricted edges Ij of 
the cylindrical intervals or cells I[N] which partition the infinite-dimensional 
domain of integration. And in the formation of Riemann sums, successively 
smaller values can then be chosen for each δ(χ). The upper right-hand cell in 
Figure 3.5 has the same division point x as the upper left-hand cell; but the 
lengths of each of its restricted edges are smaller, so V is a proper subset of /. 
But the lower left-hand cell in Figure 3.5 shows that, for fixed x, and without 
changing the bounding function δ(χ), it is possible to obtain a cylindrical interval 
/ 
which is a proper subset of / = I[N] simply by increasing the number of 

102 
CHAPTER 3. INFINITE-DIMENSIONAL 
INTEGRATION 
I" 
Figure 3.5: Shrinkage of cells of x* in R T, with x fixed. 
restricted dimensions. In other words, if we choose a larger N" containing TV as 
a proper subset, then, without changing the maximum lengths of the restricted 
edges of the cell, I"[Nn] will be properly contained in /[TV]. 
That is the motivation for imposing an extra feature in the definition of 
a gauge in R T. Namely, in addition to a condition δ on the lengths of the 
restricted edges ij, a further condition is imposed: the dimension sets TV (or sets 
of restricted dimensions) of partitioning cells /[TV] should, for each associated 
x, include some minimal set of dimensions L(x). That is, we require TV D L(x), 
where L(x) can be made successively larger, just as δ(χ) is made successively 
smaller in forming Riemann sums. 
Thus, in R T with T infinite, a gauge can be considered as a pair of mappings 
(S,L): 
]0,oo[, 
L 
ί RT 
& 
^ ( T ) > 
δ(χ). 
\ 
x 
-> 
L{x). 
δ: 
R 1 
δ 
-> 

3.8. DEFINITION 
OF THE INTEGRAL 
IN R T 
103 
Then the associated interval-point pairs (x,I[N]) in Riemann sums must, for 
each x appearing in the Riemann sum, satisfy N 2 L(x), and, for tj G N, 
each restricted edge Ij must be 5-fine, in the way that is familiar from the 
finite-dimensional case. 
This approach suggests itself strongly, and a system of integration can in-
deed be built on it. But, in anticipation of problems that will require a more 
delicate approach, something else is added to the concept of a gauge in infinite-
dimensional spaces. The additional condition arises as follows. 
The cells I used to partition R T depend on choices of division points x 
and restricted dimensions N, to give an associated point-cell pair (x, I[N])— or, 
if preferred, the associated point/dimension-set/cell triple (x,N,I[N]). 
Now 
consider again the function δ which sets bounds on the restricted edges Ij of 
I[N\. 
In the preceding paragraphs the bounding function δ is made to depend on 
x in a now-familiar way—the generalization of Riemann integration in which 
associated elements (x, I) in partitions are constrained by δ in respect of the 
length of the edges of /. But, for infinite T, we have extended the notion of 
association to include the restricted dimensions TV of associated x, I[N]. And 
it so happens that there is much advantage in making the ί-constraint depend 
on the points x in association with the dimension sets N of I[N]. 
In fact, if δ is made to depend not just on x G R T but also on TV G Λ/"(Τ), it 
turns out that the resulting theory yields much better results. Why might this 
be? 
An idea of why this matters can be got by examining the point functions 
f(x) in Example 20 and in (3.15) above. These are 
exp (-/>(*)*) if xec, 
and n„ /exp(-^^y 
0 
if x e Ria'6i \ C, 
a n 
j=1 \ <2*<'ί-«*-ι»-* 
respectively. The latter has an integrand which depends on variable x and 
variable JV, so it may make sense to replace functions δ(χ) > 0 by δ(χ, Ν) > 0. 
In the former there is an integrand that includes J x{t)dt where x G Rla»bl is 
continuous. Every finite N G Λί(]α, b}) is a set of partition points of ]a, 6], and, 
for functions x(t) which are Riemann integrable in ]a,6], we may want to use 
partition points N to form Riemann sum approximations to f(x). 
Functions 
L(x) and δ(χ, Ν) may then be helpful in calculating both the Riemann sums 
for Ja and the Riemann sums for JR]a,b] · So that is how we proceed. 
Definition 11 In R T with T infinite, a gauge 7 is a pair of mappings (L, δ): 
L 
λί(Τ), 
f RTxAf(T) 
Λ 
-> 
L{x); 
\ 
(x,N) 
-+ 
δ(χ,Ν). 
L 
ί RT 
A 
Af(T), 
6_ffLTxAf(T) 
A 
]0,oo[, 
Definition 12 With N = {ti,..., 
tn} e N{T), an associated triple (x, N, I[N}) 
is 7-fine if N D L(x) and (XJ,IJ) 
is δ-fine for tj G N, 1 < j < n. 

104 
CHAPTER 3. INFINITE-DIMENSIONAL 
INTEGRATION 
The kind of "shrinking" produced by a gauge defined in this manner is 
demonstrated in the diagrams of Figure 3.5. In these diagrams, x is the same 
point in all four pictures, but the attached or associated cell in the second, 
third, and fourth picture is a subset of the cell attached to x in the first picture. 
The idea is that, for each x G R T, the set N giving the restricted edges of 
the attached or associated cell i" = I[N] becomes successively larger as the 
set L(x) G λί(Τ) increases, while the lengths \Ij\ = Vj — Uj of the restricted 
edges of I[N] become successively smaller as δ(χ, Ν) decreases, in the manner 
of finite-dimensional Riemann integration. In Figure 3.5, 
Ϊ C /, 
I" C /, 
/'" C I1 
Ϊ" C I", 
but neither of i7, I" is a proper subset of the other. 
If it happens that Xj = oo for a particular tj G iV, then the corresponding 
Ij has the form [6j,oo[, and, as δ(οο,Ν) decreases, bj must satisfy 
1 
j > J(oo, NY 
while if Xj — —oo and Ij has the form ] — oo, a^[, then, as δ(—oo, N) decreases, 
UJ must satisfy 
1 
aj 
< ~<S(-oo,/V)· 
A partition of R T is a finite collection V of disjoint cells I[N] whose union is 
R T. 
Definition 13 A division of R T is a finite collection V of point-cell pairs 
(x,I[N]) 
such that the corresponding triples (x,N,I[N]) 
are associated, and 
the cells I[N) form a partition V 
ofHT. 
Note that, if a division V is given, the corresponding partition V can be regul-
arized or partially regularized as in (3.5) or (3.7), and a Riemann sum can be 
formed from the resulting regular or partially regular partition V'. 
Definition 14 // a gauge 7 = (L, δ) is given, then a division V is a 7-fine if 
each (x,I[N]) G V is j-fine. 
In that case, we can denote the η-fine division V 
byV1. 
Given a 7-fine division Ί)Ί and a division V formed from a regularizing sub-
partition V\ then V is not necessarily 7-fine. 
The next question is, given a gauge 7, whether a 7-fine division Τ>Ί of R T 
exists. The answer is yes; but when T is an infinite set the proof is not so easy 
as it is in finite-dimensional domains. A proof is given in Theorem 4 of Chapter 
4. 
Having equipped ourselves with all the necessary technical concepts of points, 
cells, dimension sets, association, partitions, divisions, and gauges, we are in 

3.8. DEFINITION 
OF THE INTEGRAL 
IN R T 
105 
a position to formulate a Riemann sum definition of integration in infinite-
dimensional spaces. Suppose h = fo(x, TV, /) is a real- or complex-valued func-
tion of associated elements (x, TV, /[TV]) in the infinite-dimensional domain R T. 
Suppose V = {(x, /[TV])} is a division of R T. Then the corresponding Riemann 
sum for h is 
(2>)5> 
:= Yih(x,NJ[N]) 
= 
^i{h(x,N,I[N]):{xJ[N])£V}. 
V 
This is used to define the integral of h. 
Definition 15 A function h of associated triples (x, TV, /[TV]) is integrable on 
R T, with integral 
a= 
[ 
/i(x,TV,/[TV]), 
if, given ε > 0, there exists a gauge 7 so that, for each η-fine division Τ>Ί of 
R T, the corresponding Riemann sum satisfies 
(P7)^/i(x,TV,/[TV])-a < e. 
This definition is similar in form to Definition 6 and the other definitions of 
Henstock integrability already given, and it is equivalent to these if T is finite. 
(When T is finite, then, by taking L(x) = T for all x, Definition 15 reduces to 
Definition 10 of the Burkill-complete integral in finite-dimensional spaces.) 
The integral of a function is defined on domains that can be partitioned by 
cells. These are elementary sets or figures; that is, cells, or finite unions of 
cells. The definition of the integral of a function on any figure other than R T 
is similar to one already given: 
Definition 16 A function h of associated triples (x,TV,/[TV]) is integrable on a 
figure E, with integral 
ß 
[ /i(x,TV,/[TV]), 
JE 
if, given ε > 0, there exists a gauge 7 «so that, for each j-fine division ΕΊ of E, 
the corresponding Riemann sum satisfies 
(εΊ)Σίι(χ,Ν,Ι[Ν])-β 
< ε. 
Since R T is itself a figure or elementary set, this definition includes the previous 
one. The definition is predicated on the existence of 7-fine divisions of any figure. 
The proof of this is similar to the proof of Theorem 4. 
Sometimes an integrand h(x,I[N]) 
= h(x,y,I[N],J[M]) 
may depend on 
other parameters, such as a fixed y e R T or a fixed J[M] e I(R T). Then it 
may not be obvious from notation JE h(x, y, /[TV], J[M]) which parameters—x, 
y, /, J—are the variables in the integration, corresponding to x and / in the 

106 
CHAPTER 3. INFINITE-DIMENSIONAL 
INTEGRATION 
Riemann sums of Definition 16. In that case the notation can be expanded as 
necessary: 
/ 
h(x,y,I[N],J[M]) 
or 
f 
h(x,y,I[N],J[M]). 
Jiei(E) 
Ji[N]ei(E) 
The notation /* is used to denote the set of division points associated with a 
division cell /. The preferred rule of association in this book is that a division 
point x associated with a cell I[N] must be a vertex of I[N], so x(N) is a vertex 
ofl(N) 
mRN. 
Though the division points x determine the associated partitioning cells /, 
they need not necessarily determine the values5 taken by the integrand function 
h. The integrand h may be independent of the division point x, while depending 
on a point y which is not necessarily a division point but which is determined 
by the division cell /. For instance, if / is an interval ]n, v] in R, y might be the 
mid-point of ]u, v]. If h depends only on (for instance) the left-hand vertex u of 
/ =]tt, v], then this amounts to dependence on the division cell / rather than 
on the division point x. 
This is a challenge to the intuition or preconception that an integrator func-
tion h must be a kind of "volume" function; perhaps bearing some continuity 
relation to actual volume \I\. But there is nothing in Definition 16, or in any 
of the preceding definitions, which implies that h has anything other than arb-
itrary dependence on /. Such integrands arise in stochastic integrals and are 
discussed in Chapter 8. 
3.9 
Integrating Functions in R T 
Some candidate integrands were presented in Section 3.7. How can their integral 
values on the infinite-dimensional domain R T be calculated? 
Example 24 Suppose T =]0, oo[. Suppose to = 0 and N = {ti,... 
,tn} 
€ 
λί{Τ), 
with 0 = t0 < ti < t2 < · · · < tn. Suppose I = I(N) <E I(R N) is given. 
Consider the expression 
Π (2π& - f,·-!))-* / 
exp -KVl 
νγλ) 
\J(N)\. 
(3.16) 
~J{ 
JJ(N)el(I(N)) 
\ 
Ζ\13-13-1)/ 
In more familiar notation, this is 
Π (2π(ί,· - ί,·_ι)Γ* ji exp (~ ^ Γ . ^ ) ) ^ 1^ 2 · · - ^ ' 
5In fact, the notation h(x,I) 
or h(x, TV, I[N]) for the integrand is not perfect, and can 
be misleading. It is inherited from work on the Fundamental Theorem of Calculus, where a 
point function integrand / must, in Riemann sum calculations, be evaluated at division points 
x. But the notation can be safely used provided it is understood that, while the choice of 
individual terms h(x, TV, I[N]) of Riemann sums may depend on the elements x o f a division 
T> = {(x, /[TV])}, the evaluation of h in this term need not necessarily depend explicitly on x, 
and may, indeed, depend on some other point y. 

3.9. INTEGRATING 
FUNCTIONS IN R T 
107 
an integral that exists for every I C R n = R . The integrand is a form of 
the familiar Gaussian one arising in normal distributions. 
Its properties are 
examined in Chapter 6. 
O 
For any x G R]0'°°[, any N G Af (Rl0>oo[), and any associated cell I[N] G 
I (R]°'°°[), let the value of the function h(x, TV, I[N]) be given by the calculation 
(3.16). Since the division point x does not appear explicitly in (3.16), h does 
not depend on x. The expression (3.16) depends explicitly on TV and /[TV], so 
h can be written h(N, /[TV]), omitting the variable x. 
However, to integrate h in R]°'°°[, Riemann sums have to be formed from 
7-fine {(a;,/[TV])} divisions of R^0'00^ in which the choice of elements N and 
I[N] depends on the associated points x. While values of h are independent of 
x, the terms of Riemann sums formed from h depend on the choice of associated 
(x, TV, I[N}). Therefore, in a case like this, it is sometimes helpful to regard x as 
a "hidden variable" in the designation of h—at least for the purpose of forming 
Riemann sums for h. 
We show that JR]0,oc[ h = 1. In fact, for any partition V = {I[N}} of Rl0'00!, 
the terms of the corresponding Riemann sum for h add up to the value 1. That 
is, (V)'^2h(x,N,I[N]) 
= 1. Looked at probabilistically, this is obvious. It is 
probabilistically obvious because the expression in (3.16), which defines h, is 
the joint probability that a Brownian process takes values in Ij at times tj (j = 
1, 2,..., n), and since the finite set of events I[N] in V are mutually exclusive and 
exhaust R]°'°°[, the sum of their probabilities is 1. Thus {V)YJg(N,I[N}) 
= 1 
for all divisions V of 
B)0^. 
But this point can also be seen without appealing to probabilistic ideas. Any 
division V of R]°'°°[ contains partitioning cells 
V = 
{l1[N1],...,Im[Nm]}. 
Figure 3.2 and Theorem 1 show how the disjoint cells, and their restricted 
dimension sets TV7, relate to each other. Using (3.5), a regularized sub-partition 
V = {/'[MQ]} of V can be formed. A sub-partition of V formed by partitioning 
each of the cells I[N] of V is sometimes called a refinement of V. 
By the regularization construction (3.5), each cell of V has the same set Mo 
of restricted dimensions; and each /[TV] G V is the union of a finite number of 
cells /'[M0] from V. 
Then the additivity of the integrals in RM° implies that, for each /[TV] G V, 
the corresponding term /i(TV,/[TV]) of the Riemann sum is itself the sum of a 
finite number of terms ]Γ /ι(/7[Μ0], so 
(P)EKNJ[N]) 
= 
EiKNJ[N]):I[N]eV} 
= 
E{M^[Mo]):/,[Mo]GP}· 
The left-hand Riemann sum has variable TV, while the right-hand one has fixed 
M0 = { r i , . . . , r m o } , 

108 
CHAPTER 3. INFINITE-DIMENSIONAL 
INTEGRATION 
and the construction (3.5) means the second Riemann sum can easily be com-
puted, giving 
e x D 
(_(yj-y?-i)2\ 
e X P ^ 
2(t J-t J_ 1) >) 
Jn\ 
\JnfJi 
(2π(τ ί-τ,·_ 1))» 
|/(τι)Ι· 
This is a standard Gaussian integral iterated mo times, and it can be expressed 
as 
/
OO 
/ 
/ 
/»OO 
m 0 
/ n 
OO 
\ 
\ «/ — OO · _ i 
oo j = 1 (2π(τ7· - r ^ - i ) ) 5 r-d2/mo 
··· 
dyi, = 1. 
Thus, for any division V of Rl°'°°[, we have 
(V)^h(x,N,I[N]), 
=(V)J2HN,I[N}), 
= 1 , 
so the infinite-dimensional integral 
/ 
Λ ( * , ^ Μ ) , - f 
h(N,I[N}), 
= 1 . 
This is our first calculation of an integral in R T with T infinite. The 
result, though simple enough, is an important one, perhaps the second6 most 
important in this book. The method used, involving calculation of the Rie-
mann sums of Definition 15, is especially significant and is the foundation of the 
analysis of random variability presented in this book. 
It is another illustration of a point which has already been made—every 
distribution function is Stieltjes-complete integrable. And because the 
above argument works for additive cell functions F even if they are complex-
valued, the analysis and method can be extended into areas of quantum me-
chanics which are not amenable to traditional probabilistic analysis. 
Example 25 The following cylindrical integrand looks, in part, like (3.14) of 
Example 22 above. Let 
M = {ri,...,T m}c]0,oo[ 
be fixed, and let N e I(]0, oo[) with N D M. 
Let G(I[N]) be defined by the 
calculation (3.16), and suppose a function / ( X M ) is such that 
h(xM,I(M)):=f(xM)G(I(M)) 
is Riemann integrable on R M with integral a. 
That means that, with ε > 0 
given, there exists a number 6M > 0, the same for every XM £ R M, so that, for 
each 8M -fine division T>s of R M, we have 
\(VsM)Y^h(xMJ(M))-a 
< e. 
sThe most important is Theorem 4, which makes Definition 15 work. 

3.9. INTEGRATING 
FUNCTIONS IN R T 
109 
For example, if f is continuous for x m G R M , and is zero outside a bounded 
domain, 
f(x) = 0 for x e R M \ (Λ x · · · x J m) 
where each Jj is a fixed, bounded real interval, then h is Riemann integrable on 
KN. 
Consider any N eJ\f(]0,oo[), 
N D M, N = {ifco+i,... ,tkm}, 
r0 = tko = 
0, and 
Tj = tkj < tkj + 1 <"' 
< tkj + 1 = Tj + i, 
for j — 0,..., m. For each associated (x, TV, I[N]) in R T define ΛΜ(#> AT, I[N\) 
by ΗΜ{Χ,Ν,Ι[Ν\) 
:— f(xM)G(I[N]), 
where the value of the latter is given by 
j [xTl,..., 
xTj, xTj+i · · · ? ^rm J / 
J/(iV) 
/ 
( 
(ytkr~ytkr.-iT\ 
\ 
1 
exp(~ 2(X-«;;.!;, j 
* 
A1* pir^-t^))* *** 
(3.17) 
H^e would like to show that ΗΜ{Χ,Ν,Ι[Ν\) 
is integrable on R^0'00^ with 
[ 
hM(x, N, I[N}) = [ 
h(xMl I(M)) = a. 
jR]0,oo[ 
J R M 
Define a gauge 7 = (L, 6) in R]°'°°[ as /o//ow;5. For each x G Rl0'00!, Zei L(x) D 
M, and for each (x, N) e R]°'°°[ X I(R]°'°°[), let δ(χ, N) be less than or equal to 
8M- NOW choose any η-fine division £>7 0/R'0'00', and construct the regularized 
partition V' (3.5) from the partition V corresponding to Ί)Ί. It is then found, 
by iterated integration in dimensions Mo \ M, that 
(Z>7) Σ ^(x, N, I\N]) = (V) Σ H*M, I(M)), 
so there is a OM-fine division T)$M ofHM 
such that 
\(Vy)^hM(x,NJ[N]) 
- a\ = \(VsM)^2h(xMJ(M)) 
- a\ < e, 
giving the result. If it is assumed that h is generalized Riemann integrable, but 
not Riemann integrable (so a gauge SM for R M has different values for different 
points XM 6 R MA then this argument will not work. But in that case the result 
can be obtained by using a partial regularization (3.7). 
These matters will be 
examined further in Chapter 6. 
O 
Example 26 Sometimes it is possible to express a function in R T as a limit of 
integrable functions like those in Example 25. Here is an example. Let τ > 0 be 
fixed. Let T =]0, r]. Let x(0) := 0. Let C(]0,r])) denote the set of continuous 
x e Rl°'rl. Let M = { n , . . . , Tm} be a finite set, with 
jr 
Tj — —, 
0 < 7 < TO. 
m 

110 
CHAPTER 3. INFINITE-DIMENSIONAL 
INTEGRATION 
Suppose V : R *-» [0, oo[ is a continuous function with compact support (so for 
real numbers y in the complement of a fixed interval J =]a, 6], V(y) = 0). For 
xM = χ(Μ) e R M
; let 
(
m 
and, for this f, and for each N £ Af(T) with N D M, define /IM(#> N, I[N]) as 
in (3.17) above; and, since it depends on m, write 
hM(x,NJ[N}) 
= 
hm(x,N,I[N]). 
Define g on R T = R^°'Ti as follows: 
( exp(-^V(x(t))dt) 
t/xGC(]0,r]), 
g{x) = < 
[ 0 
otherwise. 
Then, for each choice of M, f(xM)G(I(M)) 
is Riemann integrable in R M, 
and, as shown in Example 25, hm{x, N, I[N\) is integrable on R^0,r^. Also, for 
x G C(]0,r[), / M ( # ) —* g(x) as M expands, becoming dense in ]0, r], so 
f(xM)G(I[N}) 
= hm(x,N,I[N]) 
-+ g(x)G(I[N}) 
as m —> oo. It would be nice to be able to deduce from this that g(x)G(I[N]) 
is 
integrable on R'°'T], with 
lim / 
hm(x,NJ[N])= 
[ 
g(x)G(I[N}). 
m ^ ° ° j R ] 0 , r ] 
jR]0,r] 
But to be able to make such a deduction, a theorem such as the dominated 
convergence theorem is needed to justify taking limits under the integral sign. 
Therefore some integration theory must be produced for the integral of Def-
inition 15. This is the subject of Chapter 4- 
O 

Chapter 4 
Theory of the Integral 
In this chapter, using Definition 15 of the Burkill-complete integral in Chapter 
3, theorems needed for study of random variability are stated and proved. The 
basic features and structure of the Henstock integral have been introduced in 
preceding chapters and are formally presented here. 
4.1 
The Henstock Integral 
A system, called a division system1 is posited, with the following components 
and relationships. 
■ There is a domain W, consisting of points x, with a class I(W) of subsets, 
called cells of W. The domain W belongs to I(W). 
■ The union E of a finite number of cells / is called a figure. The class of 
figures in W is denoted by E(W). 
■ A finite collection of disjoint cells / whose union is E is called a partition 
of E (often denoted V). 
DS1 Association Axiom: For each / G I(W) there is a class of points /*, 
and if x G /* the pair (x, /) are said to be associated. 
■ Equivalently, for each x G W* there is a class x* of cells /, and if / G x* 
the pair (x, I) are associated. 
' The set / U /* is denoted by /. For E G E(W), the set 
\j{i:iei(W), 
ICE} 
is denoted by E. 
1The term integration basis is also used for this. 
A Modern Theory of Random 
Variation: With Applications 
in Stochastic Calculus, 
111 
Financial Mathematics, 
and Feynman Integration. 
First Edition. By Pat Muldowney 
Copyright © 2012 John Wiley & Sons, Inc. Published by John Wiley & Sons, Inc. 

112 
CHAPTER 4. THEORY OF THE 
INTEGRAL 
■ A finite collection E of associated (x, 7) such that the cells 7 form a par-
tition of E is called a division of E (often denoted ί or D). 
DS2 Gauge Axiom: In forming divisions of E G E(W) a selection is made 
from W* x I(W) as follows. There is a rule 7 which, for each x G W*, 
determines a selection σ7(χ) C x* of cells 7 associated with x, and (x, 7) is 
permitted to belong to an admissible division ΕΊ only if 7 G σ7(χ). Given 
x G W*, an associated pair (x, 7) is said to be η-fine if 7 G σ7(χ). A 
division ΕΊ is said to be j-fine if each (x,7) G £7 is 7-fine. Let σ, = σ7, 
denote 
{σ7(χ) 
:x£W*}; 
and let Ξ denote the family of collections σ — σ7 for all possible choices 
of the rule 7. 
■ If σ G Ξ, and if σ contains a division of E, σ is said to divide E. 
DS3 Division Axiom: For each figure E G E(VF) there exists σ G Ξ such 
that σ divides E. 
■ Definition 17 Suppose a real- or complex-valued function h is defined on 
the set of associated pairs (x,I). 
The function h is Henstock integrable 
on a figure E, with integral a = JE h(x, I), if for every ε > 0, there exists 
σ G Ξ dividing E so that, for each division E of E with E C σ, we have 
a-Y^{h(x,I):{xJ)e£}\<e. 
(4.1) 
If S is a subset of E then h is integrable in S ifh(x, 1)1 s(x) is integrable 
on E. In that case we write fs h = JE his· 
■ Note that E is a subset of itself, and that integrability of h in E is not 
the same as integrability of h on E. See Example 28. 
■ The integral defined by (4.1) may or may not possess properties or theo-
rems such as Fubini's theorem or a dominated convergence theorem. The 
properties possessed by the integral depend on which of the foll-
owing axioms (DS4 to DS8) are satisfied by the division system. 
DS4 Joint Precedence Axiom: If σ', σ" G Ξ both divide E then there exists 
σ G Ξ that divides E, with σ C σ' Π σ". 
DS5 Additivity Axiom: For each pair of disjoint figures E\ and E2, and each 
σ containing a division of E\ Ui?2, the set σ' of (x, 7) G σ for which 7 C E\ 
contains a division of E\. 
DS6 Restriction Axiom: Suppose E\, E2 are disjoint figures with σ^1) G Ξ 
dividing E\ and with σ^ 
G Ξ dividing E2. 
Suppose that, for each of 
3 = 1 and 3 — 2, (x, 7) G σ ^ implies that 7 C Ej. Then there exists 
σ G Ξ dividing Ex U E2 such that σ C σ(1) U σ(2). 

4.1. THE HENSTOCK INTEGRAL 
113 
DS7 Diagonalization Axiom: If there is a collection σ(χ) G Ξ, one for each 
x G W*, then there exists σ G Σ such that if (#, I) e σ then (#, I) G σ(χ). 
DS8 Joint Divisions Axiom: Suppose W1 = W2 x W3, with division systems 
Ξ' on figures Ej c f , j = l,2,3. Suppose E1 = E2 x £ 3 and σ ^ G Ξ1. 
Then for each x2 G i£2 there exists σ^3) = σ^\χ2) 
G Ξ3 dividing E3 such 
that, for every division V3(x2) C σ^3\χ2) 
of E3, there is a σ^ 
G Ξ2 
dividing E 2 so that if V2 C σ^ 
is a division of E2, then 
P 1 - { ( ( ^ , Χ Β ) , / 2 x / 3) : (x3,I3) 
€ 2>3(*2), (z 2,/ 2) € 2?2} C a«1) 
is a division of E2. 
In Henstock [94,105] this framework is elaborated into a theory of integration 
which elucidates the fundamental principles and processes underlying the vari-
ous systems of integration used in mathematics, such as the integrals of Cauchy, 
Riemann, Lebesgue/McShane, Ward, Burkill, and Denjoy/Perron/Kurzweil. 
Depending on the meanings given to the statements "x G /*" and "σ G Ξ", 
each of these integration or Riemann sum systems is a particular instance of 
the Henstock integral. Also, Henstock integration (the division system frame-
work) can be used, as in this book, to generate integration systems suitable for 
particular purposes, such as the theory of random variability. 
The meaning assigned to association of point-cell pairs (x, I) £ I* x x* is 
a fundamental determinant of the properties of the integration system. In this 
book, with W = R n, for instance, then x G I* (or I G x*) means x is a vertex 
of/. 
For the purpose of comparison, suppose we have a different point-cell asso-
ciation rule in Rn—denoted # to distinguish it—on the following lines: 
x £ I# if x is in the closure of I (using the open-interval 
topology). 
Now suppose x and / are given and x is in the closure of / (i.e., x G /#) but x is 
not a vertex of / (i.e., x £ I*). Then / can be expressed as the union of a finite 
number of disjoint cells J, each of the cells J having x as a vertex (x G J*). If 
the integrand h(x,I) 
satisfies 
h(xj) = ^2h(x, J), 
the additivity of h ensures that any Riemann sum based on ^-association can 
be converted into a Riemann sum based on *-association. In that case h is 
#-integrable if and only if it is *-integrable. On the other hand, if h is not 
additive in the manner described, then #-integrability implies *-integrability 
but not conversely. 
Since potentiality distribution functions are additive, the Stieltjes-complete 
theory of random variability described in this book is valid if *-association is 
replaced throughout by ^-association. 
As described in the preceding chapter, and in Muldowney [162], the division 
system framework permits extension of the multifunction relation of association 
of pairs of elements (x, I) G W x I(W), as follows. If W = R T, then a triple 
(x, N, I) 
G R T x λί(Τ) x I(RT) 

114 
CHAPTER 4. THEORY OF THE 
INTEGRAL 
is associated (an associated triple) if 
■ / is restricted in the dimensions N and unrestricted in dimensions 
T\N, 
and 
■ (x(N),I(N)) 
is an associated pair in HN. 
Thus, when T is an infinite set, each N G λί(Τ) can be in a multifunctional 
relationship with the elements of R T and with the elements of I(RT). When 
W = RT, Definition 17 is then extended to functions h(x, iV, J[iV]), sim-
ply by replacing the expression h(x,I) 
by the expression 
h(x,N,I[N]). 
With this amendment, Definition 17 makes reference to associated elements 
x, N, and /, which can be called, respectively, division point, dimension set, 
and division cell. (The division point x is also called tag-point.) Even when 
the integrand h does not explicitly depend on each of these associated elements, 
Definition 17 remains valid. In particular, h may depend on points y, which are 
not necessarily division points or tag-points, but which depend on the cell I in 
accordance with some fixed rule for determining y from /. For instance, in one 
dimension y could be the mid-point u -f ^(v — u) of the cell / =]u,v]. In that 
case, if there is no dependence on division points x, the integrand h(x, N, I) 
reduces to h(I[N]). 
Given σ G Ξ, denote σ by σ7 where 7 G Γ is an identifying mark to disting-
uish σ, = σ7, from any other σ' — σΊ> G Ξ (7' G Γ); and we declare that the 
associated cell-point pair (x, I) is η-fine if and only if (x, I) G σΊ = σ. Apply 
a different identifying mark 7 G Γ to each σ = σ7 Ε Ξ , = ΞΓ· Then interpret 
(tautologically) each of the identifiers 7 G Γ as a rule or gauge, which determines 
whether or not (x, I) (with x G /*) belongs to σ = σΊ. Thus 
σΊ 
= 
{(χ,Ι) : (χ,Ι) 
is 7 _ n n e} ? 
ΞΓ 
= 
{σ7 : all choices of gauge 7} 
- 
{σΊ:ΊΕΤ}. 
When W = R T, an associated triple (x, N, I[N]) is said to be 7-fine if (x, I[N]) G 
σ7. If S is a division of a figure E G E(R T), with S C σ7, then S is said to 
be 7-fine, indicated by writing S = £7. Definition 17 can then be expressed as 
follows. A real- or complex-valued function h of associated (χ,Ν,Ι) 
is integrable 
on E, with integral ct — j E h(x, N, I), if, for every ε > 0, there exists a gauge 7 
so that, for each division η-fine division £Ί of E, 
α-(£Ί)Σ?ι(χ,Ν,Ι) 
< ε. 
The Cauchy version of the definition is: A real- or complex-valued function h 
of associated triples (x, iV, /) is integrable on E if, for every ε > 0, there exists 
a gauge 7 so that, for each pair E^, £^ of η-fine divisions of E, 
\(£}l)Y/h(x,N,I)-(S^J2h(x,N,I) 
<ε. 

4.1. THE HENSTOCK 
INTEGRAL 
115 
The axioms DS1, DS2, and DS3 are all that are needed for the definition of the 
Henstock integral (Definition 17). The properties that an integration system 
will possess depend on: 
■ which subsets of the domain qualify as cells; 
■ what is the rule of association DS1 between division points, dimension 
sets, and division cells; and 
■ which combination of the axioms DS4, DS5, DS6, DS7, and/or DS8 is 
valid in the system. 
For instance, the Riemann integral on a domain W =]a, b] has / =]M,V] C 
]a, b] and I* = [u,v] (the closure of I in the open-interval topology); and, with 
6>0,xeW*, 
x* 
= 
{]u, v] : a < u < v < 6, u < x, 
v > x} , 
σδ{χ) 
= 
{]u, v] : u < x < v, v — u < δ} , 
σδ 
= 
{{xj):xel*, 
| / | < * } , 
Ξ 
= 
{σ : σ = σ$, 
δ > 0} . 
Axiom DS7 does not hold for this system. As will be evident from Section 
4.6, DS7 corresponds to what is needed to provide an integration system with 
theorems such as the dominated convergence theorem for taking limits under 
the integral sign. The Henstock-Kurzweil or Riemann-complete integral on 
W =]a, b] is usually presented with /* equal to the closure of / =]?/,v] in the 
open interval topology; and, for δ(χ) > 0, 
σδ 
= 
{(x,I) : a < x < 6, lex*, 
\I\ < δ(χ)} , 
Ξ 
= 
{σδ : δ(χ) > 0, x e [a, b}} . 
However, for various reasons of convenience, the version of the Riemann-complete 
integral used in this book has /* = {u,v} where / =]u,v]. 
Axioms DS1, DS2, and DS3 are needed in order to formulate the integral in 
Definition 17, but the rule of association between points and cells in DS1 may 
be different for different integrals. DS4, DS5, DS6, DS7, and DS8 are valid for 
the Riemann-complete2 integral. Likewise, all eight of the axioms are valid for 
this book's integral on R T. The latter developed out of the Henstock integral or 
division system and includes several features of the Riemann-complete integral. 
Historically, however, the Riemann-complete integral preceded the Henstock 
integral, and, as illustrated in earlier chapters of this book, provided several of 
2 The apparent simplicity of the Riemann-complete solution to the problem of integrating 
derivatives sometimes gives rise to a naive anticipation of overall simplicity in that theory. 
This book provides an even simpler solution (Theorem 9 below) to the question of integrability 
of distribution functions, and of additive (or Stieltjes) cell functions generally—such as the 
strong integrability of Brownian motion sample path increments. But the underlying issues 
are non-trivial, many of the problems are difficult, and the subject should be approached with 
some degree of seriousness. 

116 
CHAPTER 4. THEORY OF THE 
INTEGRAL 
the basic ingredients of division systems. Prompted in part by Jessen [116], 
a version of this book's integral on R T was presented in Henstock [94] as an 
illustration of the division system theory. Inspiration for the division system 
idea also came from the work of W.H. Young, Saks, Ward, Burkill, and other 
authors. 
The Riemann-complete integral finally resolved outstanding issues of int-
egrating derivatives. Integrating derivatives is an important issue in integration 
theory. But it plays only a marginal part in the main themes of this book. 
Broader issues of 
■ what constitutes an integrand, 
■ what constitutes an integral, and 
■ conditions for convergence of Riemann sums of an integrand to its integral, 
are resolved by the Henstock integral division system theory. 
In conjunction with further developments of the Henstock integral or division 
system theory, the integral in domain R T (with T infinite) was developed further 
in Henstock [96, 97, 100, 105], Henstock et al. [107], and Muldowney [162, 164]. 
The McShane integral (equivalent to the Lebesgue integral) has the same 
J-gauges as the Riemann-complete theory, but the rule of association of a pair 
(x,I) is 
I C]x — δ(χ), x — δ(χ)[ , 
so the class of x associated with a cell / need not belong to the closure of the 
interval /. 
Denote the classes of Riemann integrable functions, Lebesgue/McShane int-
egrable functions, and Riemann-complete/Perron/Denjoy integrable functions 
in R n by, respectively, S(ft), 3(C), and S(/C). It is well known that S(1Z) C 
S(£). In Gordon [82] it is shown that S(£) C S(/C). Integration systems in R n, 
in which cells / are not the usual Cartesian products of one-dimensional intervals 
]u,v], are described in Ostaszewski [191]. The relationships between many of 
the integration systems mentioned in this section are described in Gordon [82] 
and Henstock [105]. Four different integration systems for R T are compared in 
Henstock et al. [107] using gauges 7. Features of 7-gauges are reviewed in the 
next section. Another class of gauges (denoted ζ) in R T is given in Section 4.3 
of this chapter. 
4.2 
Gauges for R T 
In preceding chapters, with domain W = R T, the (x, /) association relationship 
of the Henstock integral division system is extended to include the additional 
variable TV in associated triples 
(x, TV, /) 
G R T x λί(Τ) x I(R T); 

4.2. GAUGES FOR R T 
117 
and a gauge 7 := (L, δ) defines a collection σ = σΊ G Ξρ, where 
Γ = {7 = (L, δ) : all choices of L and 5} . 
Then 
σ7 
= 
{(j?,JV,J[JV]):(x,iV,/[JV]) is 7-fine}, 
ΞΓ 
= 
{σ7 : all choices of gauge (L, <5) = 7 } . 
Notation -<, V, and Λ is defined as follows. 
Definition 18 Suppose 7 = (L, δ) and 7' = (!/,£'), and suppose L'{x) D £(#) 
and £'(£, iV) < £(#, iV). Tften σ7/ C σ7, and we write 
w/ien £/ws /io/ds. Suppose 71 = (Li,ii) and 72 = (Ζ/2,<Ϊ2)> wii/i corresponding 
σ7ι and σ72. Define σ73 fry 
L3(x) = Li(x)nL 2(x), 
δ3(χ,Ν) 
= max{53(x,A0,<53(z,iV)}. 
T/ien σ73 = σ7ι U σ72 and we write 
73 = 
71V72. 
With 71 and 72 as before, define 74 oy 
L4(x) = L1(x)öL2(x), 
δ4(χ,Ν) 
= min{i3(x, 
Ν),δ3(χ,Ν)}. 
Then σ74 = σ7ι Π σ72, and we write 
74 
= 
71Λ72· 
Note that, for j = 1,2, 
7j 
-< 71V72, 
71Λ72 
-< 7j. 
Theorem 4 below shows that, for gauges 7 in R T, e?;en/ σ7 contains a division 
of R T, so axiom DS3 of Definition 17 holds for this system. Other axioms of 
Section 4.1 are established at the points at which they are required in proving 
the properties of this system of integration. 
Axiom DS7 of the general Henstock integral holds for gauges 7. To see this, 
suppose 7y and aly are defined for each y G R T: 
7y = 
(Ly,Sy) 
with Ly(x) G λί(Τ) for each x G R T and δν(χ,Ν) 
> 0 for each x G R T and 
each N G Af(T), N D L(x). Now define 7 = (L, δ) and σ7 by 
L(x) = Lx(x), 
δ(χ, Ν) = δχ(χ, Ν). 

118 
CHAPTER 4. THEORY OF THE 
INTEGRAL 
It is easy to see that (x, I[N]) G σΊ implies (x, I[N]) G <77x, so axiom DS7 holds 
for this division system. DS4, DS5, and DS6 are easily established for gauges 
7, using Theorem 3 (below) when necessary. DS8—the Fubini condition—also 
holds for products R T = R T x R T 
and will be dealt with when proving 
Fubini's theorem. 
A proof of DS3 for this division system is given below in Theorem 4. In 
anticipation of this result, designate the division system formed by 
■ triples (x,N,I[N]) 
with *-association and 
■ gauges 7 = (L, δ) on R T 
as the 7- division system, and let the resulting Burkill-complete integral be des-
ignated as the 7-integral, or simply the integral 
Before commencing discussion of DS3 for the 7-division system, define the 
interior of cylindrical intervals and elementary sets in R T as follows. 
Definition 19 Given a cell I[N] G I(R T), int(J[iV]) is the subset of those 
x G I[N] such that, for each t G N, there exist real numbers y(t), y'{t) satisfying 
y(t) < x(t) < y'(t), 
y(t) G /(*), 
y'(t) G I(t). 
Definition 20 Given a figure E G E(R T), 'mt(E) is 
\J{int(I[N]):I[N]cE}. 
Since E is the union of a finite set of cells I[N], int(E') is well-defined. 
Definition 21 Given a figure E G E(R T) and a gauge 7 in R T, we say that 7 
conforms to E if, whenever (x,I[N]) is η-fine and x G int(E), then I[N] C E. 
The association relationship (x, I) means that the point x G R T is associated 
with I G I(R T) whenever x is a vertex of I. Adding the vertices of intervals 
J C I to the set / gives the following. 
Definition 22 For I G I(RT) define 
I:=\J{J* 
:JCI, 
JGl(R T)}. 
In the 7 division system, I is the closure of / in the open-interval topology. This 
is extended to figures E G E(R T) as follows. 
Definition 23 For any E G E(R T), 
E := {x : / = I[N] C £7, x G Γ } = [j {Γ : / C E} . 
This corresponds to the closure of E in the open-interval topology, with E — E. 
For the general Henstock integral, starting with some selection of the ax-
ioms DS4 to DS8 of Section 4.1, properties of a resulting integration (such as 

4.3. ANOTHER INTEGRATION 
SYSTEM IN R T 
119 
Riemann, Lebesgue, or Riemann-complete) are then deduced as theorems from 
Definition 17. 
Proofs of the main properties of the 7-integration system (or 7-division sys-
tem) in R T (for which all of the axioms DS4 to DS8 hold) are given in this 
chapter, including the proofs of such theorems as are required in the study of 
random variation. The proofs follow the lines of the proofs given for the general 
Henstock integral in Henstock [94, 105], and therefore they are valid for many 
other integration systems, depending on 
■ the meaning of cells in W, 
■ the meaning of association in DS1, and 
■ on which of Axioms DS4 to DS8 are valid. 
But first we look at another instance of the general Henstock integral. 
4.3 
Another Integration System in R T 
In many applications in the theory of random variation, the set T in R T is a 
real interval. In consideration of this, it is reasonable to regard T as a cell or 
figure in R. Prom this viewpoint3 the elements tj of TV = {t 1 ?... £n_i} can be 
regarded as partition points of the figure T. And then N G λί(Τ) also describes 
a partition and a division of T: 
{]*i-l> *,·]}?=!> 
{(t>]*i-l.*j])}?=l» 
respectively, with either t = tj-ι or t = tj for each ]tj-i,tj], 
and with to — a, 
tn = b if, for instance, T = [a,b]. Regarding T as a cell in R we can let TV 
signify any of 
{tjYj=i' 
{]^-i>^']}j=i > 
{(*» ]^-ι»^'])}"=ι ί 
that is, points, partition, or division, depending on the context. If there is a 
positive function η(ί) defined for t G T, we can say that N is η-fine if each 
(t,]tj-i,tj]) 
is 77-fine, and then denote the corresponding division N by Νη. 
Now suppose functions 77(i) > 0, L(x) C T, and δ(χ, Ν) > 0 are given. For 
associated triples (x, N, I) define a gauge ζ in R T as follows: 
C = (V,L,6) 
(4.2) 
where an associated (x, A/", I) in R T is ζ-fine provided 
■ L(x) is an 77-fine division of the elementary set T; 
■ N = Νη D L(x) is an 77-fine division of T; 
• (x(N),I(N)),= 
(χ(Νη),Ι(Νη)), 
is 5-fine in R ^ = R ^ . 
This is the viewpoint underlying the study of stochastic integrals (see Chapter 8). 

120 
CHAPTER 4. THEORY OF THE 
INTEGRAL 
In other words, the gauge ζ only uses 77-fine sets L(x) and N D L(x). All other 
elements of N{T) are ignored. 
If we have Ci = (771, Zq, $2) and C2 = (^2, £2^3) then 0 -< ζ2 in E if 771(2) < 
η2(ί) forall t G T, Li(x) D L2(a;) for all x e R T, and δι(χ,Ν) 
< δ2(χ,Ν) 
for 
all x e R T and all 7Vm G Af(T). (Note that when 771(f) < 772(2) for all 2 then 
r/i-fine iVr7l is also 772-fine.) Likewise, £3 ~< Ci Λ C2 if C3 — (V2,^3^3) 
satisfies 
773(*) < min{r;i(2), η2(2)}, 
^ ( x ) ^ ! ( x ) U L2(x), 
with I/3(x) r/i-fine and 772-fine, and 
δ3(χ,Νη3) 
< 
τηϊη{δι(χ,Νη3),δ2(χ,Nm)} 
for all 773-fine iV^ 
£λί(Τ). 
Given a function /i(x, A/", 7[iV]) of associated triples (x, iV, /[iV]), the integral 
of /i in R T, in the sense of gauges £, is given by Definition 17 with collections 
σ — σζ. This is meaningful only if axiom DS3 is valid for such collections. The 
validation will be given in the next section. 
The ζ-integral in R T is a further illustration of Henstock's Riemann sum 
or division system framework of integration, in which different methods of int-
egration are obtained by choosing different kinds of association of points and 
cells (and, where necessary, other variable elements such as TV); and by choosing 
different kinds of gauges for forming Riemann sums in Definition 17. 
There is little merit in choosing cells, association, and gauges in a capricious 
or arbitrary manner. Example 58 (Chapter 7) shows that both 7-gauges and 
ζ-gauges have a role to play in the Riemann sum theory of random variability. 
4.4 
Validation of Gauges in R T 
Axiom DS3 of Section 4.1 is the basis of the Henstock integral. Without it, 
Definitions 15 and 17 would be vacuous since every function could be integrable. 
But if DS3 is the foundation of the system, DS6 is its engine. 
It is usually obvious which of axioms DS4 to DS8 are valid when cells are 
the usual real intervals, or Cartesian products of these—as discussed in previous 
chapters. 
The following result is used extensively, and, with Definition 18 above, shows 
that DS6 is valid for the 7-division system. 
Theorem 3 Given a gauge 7 = (L, δ) in R T and a figure E C R T, there exists 
a gauge 7^ -< 7 such that 7# conforms to E. 
Proof. If T consists of just one element then R T = R, and the gauge 7 reduces 
to the function δ(χ) > 0. If the figure E is a proper subset of R, then E is 
a finite union of disjoint one-dimensional cells I 1,... ,/fc having the property 
that, if any cell J has any of the Ir as a proper subset, then 
J H ( R \ £ ) ^ 0 . 

4.4. VALIDATION OF GAUGES IN R T 
121 
Thus, if x G int(E), we have x G int(7r) for some r. Let 
e1(x) 
= 
sup{x-y 
:y <x, 
y e Ir} , 
e2{x) 
= 
sup {y1 - x : y' > x, y' G Γ} , 
e(x) 
= 
min{ei(x), e2(x)}, 
fe(#) 
= 
min{e(x), £(#)}. 
This gives the result in the one-dimensional case. If T contains more than one 
element (possibly infinitely many), write 7 = (L,6). Since E is a figure, then, 
as in the one-dimensional case, E can be partitioned by cells Ι1[Λ/1],..., Ik[Nk] 
which are "maximal", in the sense that none of the cells Ir[Nr] is properly 
contained in a cell J satisfying 
Ir[Nr]cJcE. 
Let M = Ni U · · · U Nk. 
Consider any x G int(JE). For each t G M, x(t) G 
int (Ir(t)) for some r, 1 < r < k, and we can find e(x(t)) > 0 as above. Let 
e(x) = mm{e(x(i)) 
: t G M} . 
This is positive since M is finite. Then, for each N G Λ/"(Τ), let 
SE(x,N) 
=min{e(a:), 5(x,7V)}, 
and let 
This gives the required result. 
O 
A corresponding result holds for ζ-gauges. The following result is the found-
ation stone of this book; everything else rests on it. It asserts the validity of 
division axiom DS3 for the 7-di vision system. That is, for any gauge 7 the 
domain R T (or any figure E G E(R T)) has a 7-fine division; or is 7-divisible. 
Theorem 4 Given any gauge 7 = (L,6) in R T, there exists a η-fine division 
ofRT. 
Proof. Write To = T, 70 = 7, Lo — L, and SQ = δ. Assume, for contradiction, 
that there does not exist a 70-fine division of RT°. Choose t\ G To and let 
Τ _ 1 = Γ 0 \ { ί 1 } . 
(We will construct domains T_i,T_ 2,T_3,... decrementally, denoting them 
with negative subscripts as a prompt or reminder that elements ^1,^2,^3,··· 
are successively removed.) 
For r = 1,2,3,..., let Ir G I(R) represent, for 
- r 2 r < s < (r - l)2 r, any of the cells 
] - o o , - r ] , 
]r,oo[, 
}s2~r, (s + l)2" r]. 

122 
CHAPTER 4. THEORY OF THE 
INTEGRAL 
We then find I1 D I2 D I3 D · · · so that Γ x RT-X is not 70-divisible. Choose 
00 
V(*i) € Π /r. 
If Lo((y(ii),XT_i)) Π T_i — 0 for at least one point χτ_λ € R T _ 1, so that 
io((2/(*i),a:r_1))={ii} 
for this point, then choose r so that 
2 - r < 5 o ( ( ^ i ) , x T _ 1 ) , { i i } ) . 
If y(ti) is a vertex of 7r, one point-cell pair [[y{ti)^XT_1)^Ir 
x R T _ 1) forms a 
7o-fine division of Ir x R T _ 1. If y{t\) is not a vertex of 7r, write J r = J 1 U J 2 
with 2/(^1) a vertex of each of J 1, J2; and then the two cell-point pairs 
((yfa^xT-^J1 
x RT"0 , 
((yih^xT^J2 
x R71"1) 
form a 70-fine division of Ir x RT°. Thus in both cases we get a desired contra-
diction. So we can proceed, supposing that 
L0((?/(t1),^T_1))nT_1 φ 0 
for all points #τ_ι £ RT_1; and then define 7-1 for R T _ 1, depending on y(t\) 
and 70, as follows, with N0 = {ti}UiV_i for each N-\ G Λ/*(Γ_ι): 
7_i := ( L _ i , i _ i ) , 
where 
i - i ^ T - J 
:= 
L 0(( 2/(ii),x T_ 1))nr_ 1, 
VXT., € R T - S 
ί - ι ί χ τ - χ , ^ - ι ) 
:= 
ί ο ^ ί ι ) , ^ ) , ^ ) , 
V i r ^ 6R31-1, VJV.i e ^ T . i ) . 
We show that the original assumption of the non-70-divisibility of RT° implies 
that the space R T l is not 7_i-divisible. Indeed, if R T _ 1 is 7_i-divisible, let 
V = {(#τ_ι ? ^[^-1])} be a 7_i-fine division of RT-*, and choose r so that 
2~r <min{(5_1(xr_1,iV_1) : ( ^ . ^ [ J V . i ] ) € V) . 
Then, with {/r} as before, if y(t\) is a vertex of 7r, the collection 
{((2/(*i),*r_1),/r x /[^-i]) : (xr.x./lJV-i]) G ^ } 
is a 70-fine division of 7r x R T _ 1. 
And if y{t\) is not a vertex of 7r, write 
Ir — J 1 U J 2 with y(ti) a vertex of each of J 1, J2; so that the finite collection 
{((y(«i),*T_1), J1 x J[W-i]) : (^^./[iV-i]) € 2>; f = 1,2} 
is a 70-fine division of Ir x R T _ 1. Either way we get a contradiction, proving 
that R T l is not 7_i-divisible. Now choose t2 € ^"-l and let 
Γ _ 2 = Τ _ ι \ { ί 2 } = Γο\{ίι,ί 2}. 
We make the following inductive assumption for 2 < j < k: 

4.4. VALIDATION OF GAUGES IN R T 
123 
■ tj has been chosen with T_j := T_(j_i) \{tj} 
= TQ \ {ίχ, t^, · · · > tj}; 
- iV-^+i = {fy}U JVLj for each N-j G Ν(Τ-ό) 
and each iV-j+i G A/*(T_j+i); 
■ y(ij) has been found, with 
L_ i + 1 ((j/fo), χτ_,·)) Π T.j = L0 ((yiii), j/(i 2),..., yfe), XT_,)) ^ 0 
for each xT_ . G R T _ J; 
■ 1-ό{χτ_ό) 
:= L_ j +i ((y^JjXT.,·)) η Γ _ ; for each xT_. G R T ^ ; 
■ £_^· (xr.jiN-j) 
:= 5-j+i ((2/(^·),χτ_,),^-^) for each χτ_, G RT-^ and 
each iV-j GJV(RT-0; 
■ 7_j := (L-j,6-j) 
is a gauge for RT~J, and RT_J is not 7_j-divisible. 
Now proceed as in the first inductive step. Choosing tk+\ G T_^, again define 
Γ_/,_ι := T_fc \ {ijt+i} and find I 1 D 72 D 73 D · · · so that Γ_χ R7-*-1 is not 
7_fc-divisible for each r. Now, having taken y(tk+i) G H^Li-^ there are two 
possibilities. Firstly, 
Lo ((2/(*i),2/(i2), - - ·, i/(ifc), ),^T_fc_1)) Π T_fc_x = 0 
for at least one point xrk+1 G RTfc+1; and secondly, 
Lo ((y(*i), y(t 2),.. ·, y(f*),), *τ-*_!)) n T_fc_! ^ 0 
(4.3) 
for all points £T_fc_i G R^-*-1. In the first case, repeating the argument in the 
first inductive step above, we come to an immediate contradiction and stop the 
construction, as the theorem is proved. In the case of the second possibility, 
proceed by repeating the construction of the first inductive step. That is, for 
the fixed point y(tk) G R and the gauge ^-k in RT-fc, construct a gauge 7_&_ι 
for RT-1*-1 by defining 
L_fc_! : R^*- 1 -+ Ar(T_fc_!), 
<U_! : R 7 ^ " 1 x M{T-k-i) 
->]0, oo[ 
as follows. With JV_fc = {tk} U N-k-i 
for each N-k-i 
G Af(T-k-i), 
define 
L-fc-iixT-fc.J 
:= 
L-k ((y(tk),xT^k^)) 
ΠΓ-fe-i; 
S-k-iixT.k.^N-k-x) 
:= δ-k 
((y(tk),xT_k_1),N-k). 
Then, exactly as in the first inductive step, we prove that RT-fc~1 is not 7_^_i-
divisible, completing the induction. If, at each inductive step, we get the second 
of the two possibilities described above, then there is an infinite sequence of 
indices 
tk+i G T-k = T0 \ {ti, t 2,.. .,t/c} 
and the corresponding series of points y(tk) such that (4.3) holds for each 
XT_fc_! G RT-fe~1, k = 1, 2,3, 
Now choose any point 
i T u e R T - : = R W ' « ' - > , 

124 
CHAPTER 4. THEORY OF THE 
INTEGRAL 
and observe that, for the point 
XT = XT0 := (2/(*i),y(h),y(*i),...,^Tw) £ R T, 
we have LO(XT) Π T_fc ^ 0 for fc — 1,2,3,..., since 
XT = (2/(ii),y(*2),i/(i3),...) 
if 
Τω = 0. 
This contradicts the fact that LQ(XT) is a finite set. Therefore the assumption 
that R T is not 7-divisible must be false. 
O 
Example 27 Suppose T is a real interval, such as [0,1]. 
Then an element 
x = (x(t))0<t<1 
o/R T is either continuous or discontinuous with respect to t. 
Suppose y G R T is discontinuous at some point ry £ T. We can define a gauge 
7 so that, if x G R T is continuous and (a;,/) is η-fine, then y eRT\I. 
Thus, 
any η-fine division {(#,/)} must include some terms (x,I) for which x is not 
continuous. Details of this are given in Muldowney [162], Proposition 7, page 
24. 
O 
Theorem 5 Given a gauge 7 = (L,S) in R T and a figure E C R T, there exists 
a η-fine division of E. 
This can be proved in the same way as Theorem 4. Alternatively, it can be 
deduced from Theorem 4 using Theorem 3. 
The basic properties of ζ-gauges can similarly be established. The first step 
is to prove that C_rme divisions exist. 
Theorem 6 Suppose T is a figure in R. Given a gauge ζ = (η,Σ,δ) 
in R T, 
there exists a ζ-fine division 
ofHT. 
Proof. The proof of Theorem 4 can be adapted for this. As in that proof, write 
T = T0, 
r/ = r/o, L = L0, 
δ = δ0, 
ζ = Co-
Assume, for contradiction, that there is no Co-fine division of RT°. Choose an 
770-fine division Μχ of To, and choose t\ G M\. Let T_i = To \ {t\}. For each 
t G Mi let rt be a positive integer, and let Ir(t) be one of the cells Ir of Theorem 
4. Partition R M l by cells 
Jr(M1) = 
l[{F(t):teM1}, 
giving J ^ M i ) D J 2(Mi) D J 3(Mi) D ··· with each J r(Mi) x R7-* non ζ-
divisible. Choose 
yMief]{Jr(M1):r 
= 1,2,3,...}. 

4.5. THE BURKILL-COMPLETE 
INTEGRAL 
IN R T 
125 
As before, if L0 ((ί/Μι^τ.ι)) Π Γ_Ι = 0 for some #τ_ι € R T _ 1 we have 
Lo ((2/Mi,a?T_i)) = Afi, 
so we can choose r satisfying 
2" r <Αο((2/Μι,»τ_1),Μι); 
and for this r we have Jr(M\) x R T _ 1 ζ-divisible and the proof stops here. Thus 
we can proceed, as before, on the assumption that L0 ((ί/Μι^τ.ι)) Π Γ_Ι ^ 0 
for all χτ_ι £ R T _ 1. Then, using Theorem 3, we can define 
0 < T M W <**>(*) 
with η-ι conforming to the figure T_i in R; and, for all χτ-λ £ R T - 1 and all 
r/_i-fine Ντ_Ύ G Λ/"(Τ_ι), we can define 
L-i(xT^) 
= 
L 0((y(ii),x T_ 1))nr_ 1, 
δ.^χτ.^Ντ^) 
= 
δοϋυΙΜ,χτ.ΛΛίύνΝτ.!). 
Let 
C-i = ( T 7 _ I , L _ I , J _ I ) . 
The original assumption of the non-C-divisibility of R T now implies that R T l is 
not C_i-divisible. This is established by an argument similar to that in Theorem 
4. The next step is to choose an ry_i-fine (and hence 770-fine) division M2 of the 
elementary set T_i in R, then choose a point t2 £ M2, and remove t2 from T_i, 
letting 
T-2=T-1\{t2} 
= 
T0\({t1}\J{t2}). 
The proof then continues by induction as in Theorem 4. 
O 
If h(x,N,I) 
is Henstock integrable (in the sense of Definition 17) in R T 
using gauges ζ, we say that h is ζ-integrable. Likewise for gauges 7. 
Theorem 7 If h(x,N, I) is ζ-integrable then h is 7-integrable. 
Proof. Given a gauge ζ = (r/,L,(5), the pair (L, δ) define a gauge 7, so every 
Riemann sum of h over a C~fine division is also a 7-fine Riemann sum. The 
result then follows from Definition 17. 
O 
4.5 
The Burkill-Complete Integral in R T 
The system of axioms in Section 4.1 is concerned, not with properties of an 
integrand function h(x,I), 
but with elements x and / of the domain Ω of inte-
gration which vary with respect to the partitioning of the domain for formation 
of Riemann sums of the integrand function h. In the basic Henstock integral, 

126 
CHAPTER 4. THEORY OF THE 
INTEGRAL 
for which these partition-dependent variables are division points x and their 
associated division cells /, this is expressed by writing the integrand for the 
domain Ω as 
h(x,I). 
But it is possible to have explicit dependence of h on partition-dependent 
elements or parameters in Ω other than, and additional to, the points x and 
cells /. A simple illustration of this is in Example 19 of Chapter 2 
In Muldowney [162], two additional partition-dependent variable elements— 
denoted N and N*—were introduced into the Burkill-complete integral in the 
infinite-dimensional Cartesian product domain R T. 
This book utilizes an extension consisting of just one of these additional 
variables; namely, the variable dimension set N of the cylindrical cells I[N] 
used to partition R T. 
Many of the integration theorems or properties dealt with here are those 
properties that are familiar from basic integration in finite dimensions. But 
they are expressed in terms of Burkill-complete integrands h(x,N,I) 
rather 
than products of point functions by set functions. Nothing is lost by doing 
this, since the usual integrand consisting of product of point function and set 
function—without any variable parameter N—is a special case of 
h(x,N,I). 
Moreover, this form of integrand h provides access to more "extreme" or "exotic" 
counterexamples whenever these are needed in order to locate the limitations 
or extent of the theory. 
But the primary reason is that, in the study of joint variation of infinitely 
many observables, integrands depend in a natural way on variable parameters 
N. 
Theorems in this chapter are proved, for the most part, by examining Rie-
mann sums. This often means examining closely the partitions or divisions of 
the domain, out of which the Riemann sums are constructed. Thus there is 
an emphasis on cylindrical intervals I[N] which, in the Cartesian co-ordinate 
representation, have a rectangular shape; and this sometimes gives the analysis 
a geometric flavor. Also, Riemann sums contain only a finite number of terms, 
and this often simplifies the calculations. 
In addition to basic properties of the integral, we prove theorems on taking 
limits under the integral sign. If T is expressed as the disjoint union of sets T' 
and T", then R T = R T x R T , leading to iteration of integrals and Fubini's 
theorem. The definition of the integral in Chapter 3, Definition 17, and else-
where makes no reference to measure theory. But, as outlined in Chapter 3, the 
theory has a Riemann-type analog of outer measure, called the variation of a 
function. These are some of the main points in the Riemann sum or division 
system approach to integration in infinite-dimensional spaces. 
The proofs given in this chapter remain valid for the corresponding results 
for Burkill-complete integrals in finite-dimensional spaces. Mutatis 
mutandis, 
they are also valid for instances of the Henstock integral corresponding to other 
choices of (#, I) association, of the gauges, or of both. In other words, depending 
on which of division system axioms DS1 to DS8 is satisfied by the integration 
in question, the properties described in this chapter also hold for integrations 
such as those of Cauchy, Riemann, Lebesgue/McShane, Ward, Burkill, and 

4.6. BASIC PROPERTIES 
OF THE 
INTEGRAL 
127 
Denjoy/Perron/Riemann-complete. And in that case, with gauge appropriately 
altered, the proofs in this chapter remain valid. 
Proofs are given for Burkill-complete integrals defined by gauges 7. But be-
cause the method of proof is valid for the general Henstock integral of Definition 
17, the results also hold for Stieltjes-complete integrals, and Riemann-complete 
integrals in R T, whether defined by gauges 7 or ζ. 
In principle the results can be deduced from the relevant division system 
axioms DS4 to DS8, as is done in Henstock [94, 105]. That approach is efficient, 
illuminating, and intellectually satisfying. It shows how the integrals of Cauchy, 
Riemann, Lebesgue, Denjoy, Perron, Ward, Burkill, Kurzweil, and McShane 
relate to each other. It reaches the pure and irreducible core of integration, 
avoiding incidental features of particular domains, and demonstrating how the 
familiar properties of integrals—linearity, additivity, limit theorems, Fubini, and 
so on—flow from the division system axioms. 
But that is not the purpose of this book. Its purpose is to shed light on 
random variability, and many of the "incidental" features of domains such as 
]0,1], ] — 00, oo[, and R T are relevant to this. By locating the proofs of integra-
tion theory within R T rather than the axiomatic division system W, a focus is 
retained on specific characteristics of R T which feature strongly in the theory 
of random variability. 
4.6 
Basic Properties of the Integral 
Descriptions of the basic properties of integrals generally start with Theorems 
12 and 13 below. (These state that a non-negative integrable function has non-
negative integral, and that the sum of two integrable functions is integrable.) 
First we show, in the division system terminology, that the integral in Def-
inition 17 is well defined. 
Theorem 8 // two numbers a\ and a^ satisfy (4-1) then a\ = 0L2. 
Proof. Let ε > 0 be given. By hypothesis there exist σ\ and 02 dividing E 
such that, for all Sl G σ\ and all E2 G σ2, we have 
^-(ε^Σπχ,η 
<ε, 
a2-(S2)J2h(x,I) < ε. 
By axiom DS4 there exists σ € Ξ dividing E, with σ C σ\ Π σ-ι. Choose £ G σ. 
Then 
|αχ - a 2 | 
I (ai - {€) Σ M*, i)) - («1 - (£) Σ M*. i)) 
< j (ttl - (S) £ h(x, I)) I - I (ai - {£) Σ h(x, I)) 
and since this holds for every ε > 0 the result follows. 
< 2ε, 
o 
Non-negative functions F defined on E(R T) (and hence on I(R T)) which are 
finitely additive on disjoint figures are sometimes called distribution functions. 

128 
CHAPTER 4. THEORY OF THE 
INTEGRAL 
But we extend the term distribution function, or potentiality distribution func-
tion, to additive cell functions F, real- or complex-valued, such that F(R T) = 1. 
The next result is especially important in the theory of random variability, 
and it does not hold when the Lebesgue integral is used. This result is a central 
theme of this book. 
Assume that the integrand h(x,N, I[N]) is a distribution function F—whether 
non-negative, real-valued, or complex-valued. 
Theorem 9 Any distribution function F is integrable. 
Proof. Suppose E G E(R T) is given. Let £ be a division of E, so 
V = 
{I[N]:(x,I[N})eS} 
is a partition of E. Then, with h{x,NJ[N]) 
= F(I[N]), the additivity of F 
over the finite union of disjoint cells I[N] of V, with [Jv {/[iV]} = E, gives 
OP) Σ F(W}) = (S) Σ h(x, N, I[N}) = F(E). 
Since this holds for every division E of E, the distribution function F is int-
egrable on E with integral F(E). 
In particular, F is integrable on R T, with 
integral value 1. 
O 
The proof of Theorem 9 shows that every Stieltjes integrator (i.e., every 
additive cell function) is Stieltjes-complete integrable: 
Theorem 10 Any additive cell function H(I) is integrable on bounded figures. 
A cell function k(I[N]) is finitely sub-additive if, for each cell J G I(R T) and 
each partition V = {I[N]} of J, 
When a boundedness condition is satisfied, real-valued finitely sub-additive cell 
functions are integrable. 
Theorem 11 Suppose a cell function k(I[N]) is real-valued. Ifk is finitely sub-
additive, and if, for some gauge 7 and all η-fine divisions ΕΊ of E, the collection 
of Riemann sums 
{(£7)5>(/[iV])} 
is bounded above with supremum a, then k is integrable on E with integral a; 
[ k(I[N}) = a. 
JE 
If on the other hand, the above collection of Riemann sums is unbounded, then, 
for each integer m there is a gauge 7 m such that 
(^)Efc(/[ArD^m 
for all ^m-fine divisions Elrn of E. 

4.6. BASIC PROPERTIES 
OF THE 
INTEGRAL 
129 
Proof. Suppose £ is a division of E. Let 
ß = CP) Σ mm), v = {I\N\ ■. (*, im e ε}. 
With ε > 0 given, there exists a 7-fine division £, with corresponding partition 
P, so that 
a — e < β < a. 
Choose a gauge 7' conforming to each of the finite number of cells I[N] of this 
partition V, and choose a 7/-fine division of each of these I[N], Then the union 
of these divisions is a 7/-fine division £' = {(#', /')} of E, and 
et - ε < ß < (e')^k(I') 
< a. 
Since this holds for every 7/-fine division of E, k is integrable on 25, and JEk = a. 
Now suppose the collection {/?} has no upper bound. Then for each integer m 
there is a value ß = ßm > m, with ßm corresponding to the partition V = Vm. 
Choose a gauge 7^ conforming to the intervals of V. Then 
m < ßm = (V) Σ k(I) < (£') Σ k(I') 
for all 7^-fine divisions E' of E. This gives the result. 
O 
Theorem 12 If a non-negative function h is integrable on a figure E 
eE(RT), 
then JE h is non-negative. 
Proof. 
Each Riemann sum Y^h(x,N,I[N]) 
is non-negative, and the result 
follows from this. 
O 
Theorem 13 If functions hi and /i2 are integrable on E G E(R T), then h = 
hi + /i2 is integrable on E, and 
h= 
hi+ 
h2 
JE 
JE 
JE 
ε 
< 2 · 
Proof. We prove this for E = R T. The proof for any figure E e E(R T) is 
similar. Write aj = J R T hj and a = ai + a2- Given ε > 0 there exists a gauge 
7 so that, for j = 1,2 and each 7-fine division Ί)Ί of R T, 
\{νΊ)Σ^{χ,Ν,Ι\Ν))-αί 
Then 
\(V7)Y:h(x,N,I[N})-a\ 
= 
\<ΡΊ) Z(hi(x,N,I[N]) 
+ h2(x,N,I[N})) 
- {a, + a2)\ 
< \CD1)Zh1(x,N,I\N})-a1\ 
+ \(Vy)Zh2(x,N,I[N})-a2\ 
< e, 
giving the result. 
Q 

130 
CHAPTER 4. THEORY OF THE 
INTEGRAL 
Theorem 14 If a is a real or complex constant and h is integrable on E, then 
fEah 
= 
afEh. 
Proof. This follows from (V) Σ ah(x, N, /[TV]) = a{V) £ ft(x, TV, /[TV]). 
Q 
Theorem 15 If hi andh^ are real-valued, with hi(x,N,I[N)) 
< 
h,2(x,N,I[N]) 
for each associated (χ,Ν,Ι), 
and if hi and /12 are integrable on E, then 
[ hi < [ h2. 
JE 
JE 
Proof. For j = 1,2, choose jj so that, for each £Ίά, 
\{εΊά)ΣΗ3 -αΑ 
<ε 
Choose 7 -< 71 Λ 72. Then, for all 7-fine divisions £7, 
&i — ε < (£7) 22 ^1 — (^τ) zl ^2 < ct2~l· ε, 
so θίι < OL2 + 2ε for all ε, giving αι < c*2. 
O 
Theorem 16 // J R T /i exists, then the indefinite integral of h, H{E) — JE h, 
exists as an additive or Stieltjes function of the figures E G E(R T). 
Proof. With ε > 0 given, choose 7 so that |(£>7) Σ h - H(RT)\ 
< ε for all £>7. 
Write E = E1. The set E2 = R T \ ^ 1 is a figure. By Theorem 3, there exist 
7i> 72 with 71 -< 7, 72 -< 7, so that jj conforms to E1·7, j = 1,2. For any 7j-fine 
divisions Sj of .E·7, their union 8i U £2 is a 7-fine division of E1, with 
( ί ι υ £ 2 ) Σ Λ = = ( 5 ι ) Σ / ι + ( 5 2 ) Σ Λ , = αι+α2. 
Consider any two 71-fine divisions Si and S[ of E"1, and write the corresponding 
Riemann sums of h as 
«! = (£!)!> 
αί = (£ί)ΣΛ· 
Then 
|ai+o 2--H"(R T)| < ε 
and 
Κ + α 2 - i7(R T)| < ε. 
This gives 
Ιαχ-οΊΙ 
= 
| ( a i + a 2 - / / ( R T ) ) - ( a i + a 2 - t f ( R T ) ) | 
< 
|( a i + a2 - tf(RT))| + | K + a2 - i/(R T))| 
< 
2ε. 
Thus the Cauchy condition for integrability of h is satisfied, and we can write 
H(E)= 
f 
h(x,N,I[N}) 
JE 

4.6. BASIC PROPERTIES 
OF THE 
INTEGRAL 
131 
for any figure E C R T. Let D1, D2 be disjoint figures in R T. Then D = D1UD2 
is a figure and the integrals Ηφ1), 
H(D2) and H(D) exist. Let V\, V2 be 
7-fine divisions of D1, D2, and let c?i, d2 be the corresponding Riemann sums 
for h. Using the previous argument, 
|di - Η(ΌΧ)\ < 2ε, 
\d2 - H(D2)\ < 2ε. 
Then D* U D^ is a 7-fine division of D, which we can denote by P 7 , and we can 
denote the corresponding Riemann sum by d. Therefore 
d = d i + d 2 
and 
\d - H(D)\ < 2ε, 
so 
\H(D)-{H(D1) 
+ H(Di))\ 
= \(H(D)-d)-{H(D1)-d1 
+ 
H(D2)-d2)\ 
< \H(D)-d\ + \H(D1)-d1\ 
+ \H(D2)-d2\ 
< 6ε. 
Thus H(D) = H(D2) + H{D2). 
0 
Theorem 17 If h is integrable on a figure E, so that, with ε > 0 given, there 
exists 7 satisfying 
(εΊ)Σίι(χ,Ν,Ι[Ν])-Η(Ε) 
< ε 
for all £Ί, and if E1 is any figure which is a subset of E, then the indefinite 
integral H satisfies 
\(ε1
Ί)Υ/Φ,Ν,Ι[Ν])-Η(Ε1)\<2ε 
for all η-fine divisions £* of E1. 
Proof. See proof of Theorem 16. 
O 
Theorem 16 links integrable point-cell functions h(x,N, I[N]) with their in-
definite integrals H which are additive on cells I and figures E. The Henstock 
lemma (Theorem 18 below) provides information on the linkage between h and 
its indefinite integral H. It shows that any Bur kill-complete integrable func-
tion /ι(χ, Ν, i"), not necessarily additive on cells /, is "nearly" equal to a Stieltes 
or additive cell function. By Theorem 10 every Stieltjes cell function H(I) is 
integrable, and the indefinite integral of H(I) is exactly equal to a Stieltjes cell 
function—namely, H(I) itself. 
Theorem 18 examines the difference h(x,N,I[N]) 
— H(I[N]) between the 
Burkill-complete integrand h and the additive cell function ϋί, and shows that 
Riemann sums of the difference function are "absolutely small". 

132 
CHAPTER 4. THEORY OF THE 
INTEGRAL 
Figure 2.1 indicates that this is not too surprising. Consider a product 
f(x)\I\ in one dimension, and its indefinite integral 
H(J) = jf(x)\I\, 
defined for one-dimensional cells J. If the interval / is "small", the difference 
d(I) = f(x)\I\ — H(I) should be relatively small. The question is whether 
a gauge can be chosen so that Riemann sums of the absolute values of the 
difference, J^ |d(/)|, are arbitrarily small; so f(x) can be regarded as a density 
function for its indefinite integral H(I). 
The following result, known as the 
Henstock lemma, gives an affirmative answer. 
Theorem 18 If h(x,N,I[N]) 
is integrable on R T, and if a gauge 7 satisfies 
I (X>7) J2 M*> N, I[N}) - H(R
T) I < ε 
for all Ί)Ί, then, for all Τ>Ί, 
(V7)^2\h(x,N,I[N])-H(I)\<Se. 
Furthermore, if E is a figure in R T and £7 is a η-fine division of E, then 
\(εΊ)ΣΗχ,Ν,Ι[Ν])-Η(Ε)\ 
< 
2ε and 
(&,)Σ,\Ηχ,Ν,Ι[Ν\)-Η(Ι)\ 
< 
16ε. 
Proof. For any complex number c, let 3t(c),9?(c) denote, respectively, the real 
and imaginary parts of c. Suppose Τ>Ί is a 7-fine division of R T. Let 
E1 
= 
[){I[N] 
E2 
= 
\J{I[N] 
E3 
= 
\J{I[N] 
E
A = UUM 
(a;, I[N]) € 2>7, St{h{x, N, I[N}) - H(I[N})) > 0}, 
(a;, I[N]) e X>7, M(h(x, N, I[N}) - H(I[N])) < 0}, 
(x, I[N}) e P 7 , Z(h(x, N, I[N}) - H(I[N})) > 0}, 
(x, I[N}) € 2>7, Z(h{x, N, I[N]) - H{I[N])) < 0}, 
and let £% denote 
{(χ,Ι[Ν]):Ι[Ν]θΕ\ 
(χ,Ι[Ν\) € P 7} . 
For each i, ΕΧ
Ί is a 7-fine division of Ex. By Theorem 17, 
(St,) £ 
|ft(x, TV, I[N}) - H(I[N})\ < 2ε, 
giving 
(2>7) £ 
\h(x, N,I[N}) - H(I[N})\ < 8ε. 
Using this argument again, the rest follows from Theorem 17. 
Q 

4.6. BASIC PROPERTIES 
OF THE 
INTEGRAL 
133 
Thus, if an arbitrary function h(x, TV, I[N]) is integrable, there is an additive 
or Stieltjes interval function H whose integral equals the integral of h. This can 
be expressed as a criterion4 for integrability of a function h. 
Theorem 19 A function /i(x, N, I[N]) is integrable on E G E(R T) if and only 
if there is an additive function H defined in E(R T) such that 
\h(x,N,I[N})-H(I[N})\ 
is integrable on E with fE \h - H\ = 0. Then, for E1 G E(R T) and E1 C E, 
we have JE1 h = 
Hfö1). 
Proof. If h is integrable on E, then, by Theorem 17, h is integrable on each 
figure E1 C E. Denote JE1 h by H{El). 
By Theorem 16, H is additive on 
figures E1 C E. By Theorem 18, given ε > 0 there exists a gauge 7 so that, for 
each 7-fine division £Ί of E, 
(£,) X ; \h(x, N, I[N] - H(I[N})\ < e. 
(4.4) 
Therefore the function \h — H\ is integrable on E, with integral 0. For the 
converse, suppose (4.4) holds, with H additive. Then 
\(εΊ)ΣΗχ,Ν,ΐ[Ν))-Η(Ε)\ 
= 
\(εΊ)ΣΗχ,Ν,ΐ[Ν})-(εΊ)ΣΗ(ΐ[Ν})\ 
= 
\(εΊ)Σ(Ηχ,Ν,Ι[Ν])-Η(Ι[Ν}))\ 
< 
(εΊ)Σ\Ηχ,Ν,Ι[Ν])-Η(Ι[Ν})\ 
< 
ε, 
which, with Theorem 17, gives the converse. 
O 
Theorem 20 If h is integrable on R T, then its indefinite integral H is an add-
itive function of cells, and JE H(I[N]) = fE h{x, N, I[N]) for all E G E(R T). 
Proof. This follows from Theorems 16 and 9. Alternatively, with ε > 0 given, 
there exists 7 so that, for any 7-fine division £7 of E, 
\(εΊ)ΣΗ(ΐ[Ν))-Η(Ε)\ 
< 
\(εΊ)ΣΗ(ΐ[Ν])-(ε,)ΣΗχ,Ν,ΐ[Ν])\ 
+ 
\{εΊ)ΣΚχ,Ν,Ι[Ν])-Η{Ε)\ 
< 
(εΊ)\ΣΗ(Ι[Ν])-Ηχ,Ν,Ι[Ν])\+ε 
< 
16ε + ε = 17ε. 
This gives the result. 
O 
4This is a "local" property of the Burkill function h ensuring integrability, just as there 
are local properties—of continuity, measurability, existence of primitive—for integrability in, 
respectively, Riemann, Lebesgue and calculus integration. 

134 
CHAPTER 4. THEORY OF THE 
INTEGRAL 
By Theorems 9 and 10, a function H(I) which is finitely additive for I £ I(£7) 
is integrable on E. The definite integral of h on E exists if and only if its 
indefinite integral on E exists. Note that if the indefinite integral H satisfies 
H (R T) = 1 then H is a distribution function. 
Theorems 18, 19, and 20 show that Burkill-complete integrabihty of a func-
tion h is closely related to Stieltjes-complete integrabihty, the link being the 
indefinite integral H which is an additive cell function. The Henstock lemma 
can be regarded as a local criterion of integrabihty analogous to Lebesgue's 
criterion for Riemann integrabihty. 
The Henstock lemma is concerned with Riemann sums of the absolute value 
of the difference h(x,N,I[N]) 
- H(I[N]), with h and H both integrable. The 
next section deals with Riemann sums of the absolute values of arbitrary func-
tions /i, whether integrable or not, and the results, combined with the Hen-
stock lemma, give further properties of the integral whenever it exists. 
4.7 
Variation of a Function 
The proofs in the preceding section are based on DS4, DS5, and DS6 of Section 
4.1. This section uses these axioms, and also DS7. 
An important step in the development of integration theory has been the 
identification and characterization of "negligible" sets in the domain of integrat-
ion of a function. This issue is often illustrated by the Dirichlet function (2.6) 
defined on the unit interval [0,1], which takes value 1 at the rational points in 
[0,1], and zero otherwise. The rational points of [0,1] turn out to be negligible in 
some sense. When this function is integrated with respect to the length function 
|/| of the sub-intervals 7 of [0,1], its integral has value 0. 
Investigation of "negligibility" phenomena leads to the concept of outer mea-
sure of a set. In the case of the Dirichlet function, the outer measure is the 
Lebesgue outer measure obtained from the length function h(I) = |J|, and the 
rationale are negligible or null, in the sense of having Lebesgue outer measure 
zero. 
The corresponding concept in Henstock integration is called variation. It 
is defined for arbitrary functions h(x,N,I[N]), 
and we denote the h-variation 
of an arbitrary set S C R T by V/l(5). Just as other branches of this subject 
have theories of outer measure and measure, in Henstock integration there is a 
theory of variation. This section demonstrates that V/^*?) is an outer measure 
of subsets S of R T. 
The characteristic function or indicator function of a set S C R T is defined 
by 
{ l if 
xeS, 
y J 
\ 0 if 
xeRT\S. 
Definition 24 A function h{x, N, I[N]) is absolutely integrable on E G E(R T) 
if \h(x,N,I[N])\ 
is integrable on E. 
A function h(x,N,I[N}) 
is absolutely 
integrable if\h(x,N,I[N])\ 
is integrable on R T. 

4.7. VARIATION OF A FUNCTION 
135 
Definition 25 For any function h(x,N,I[N]), 
let E G E(R T) and gauge 7 be 
given, and let £Ί denote η-fine divisions of E. Taking the supremum over all 
such divisions, write 
VllE] = sup {(^ΣΜΧ,Ν,ΙΙΝ])]} 
, 
and define the variation of h relative to E (or variation of h on E) as 
Vh[E]=m{Vl[E}. 
With E = R T, Vfc [RT] is the variation of h. 
Definition 26 Given S C R T, replace h(x,N,I[N]) 
in Definition 25 by 
h(x,N,I[N])ls(x), 
and define the variation of h in S, relative to E (or variation of h in S on E), 
a i V u s [ 4 written ~Vh(S)[E], so 
V, XS)[E] = inf Vl(S)[E} = inf jsup {(£,) £ 
\h(x,N,I[N})\ 
ls(x)} 
1 . 
Here are some observations implied directly by the definition of the functional 
Vfc. 
■ If 72 -< 7i, every 72-fine division £72 of E is also a 71-fine division of E1, 
and then 
V?{S)[E\<V£{S)[E\. 
■ For any Ελ,Ε2 
G E(R T) and 5 C E1 C E2, consideration of the Riemann 
sums gives 
VHWIE1] 
= 
Vh(S)[E2}. 
■ Suppose V^fE1] is finite. Given ε > 0, there exists 7 so that 
\l[E}<Vh[E} 
+ e, 
and there exists a 7-fine division S7 of E so that 
(S^\h\>Vl[E}-s. 
Thus there exist 7 and £Ί so that 

136 
CHAPTER 4. THEORY OF THE 
INTEGRAL 
Definition 27 With E = R T in Definition 26, the variation of h in S (or 
h-variation of S) is V^(5)[RT]? written V/l(5). So for gauges 7 and η-fine 
divisions Τ>Ί ofHT, 
Vh(S) = iniVl(S) 
= inf I sup {(P 7) £ 
\h(x, N,I[N})\ 
ls(x)} 
If S C R T and V/l(5) = 0, we say that S is h-null. In other words, S is /i-null 
if h has variation 0 in 5, or S has h-variation 0; and then R T \ S is h-full in 
R T. If E e E(R T) and Vfc(S)[£] = 0 we say that h has variation 0 in S on E. 
If a property or condition holds for values x other than those x belonging to a 
/i-null set, we say that the condition holds h-almost everywhere. 
In that case, 
if h is a potentiality distribution function F, we say that the condition holds 
F- almost certainly, or F-almost surely. 
This section addresses the properties implied by Definitions 25, 26, and 27. 
Definition 27 selects one of the associated elements of the associated triples 
{x, N, I[N]) 
e 
R T x λί{Τ) χ I(R T) 
that constitute the argument or domain of the function h. Section 2.10 indi-
cated, in advance of actual proof, that variation has outer-measure-type prop-
erties. The domain of outer measure is generally the space of points on which 
an integrand is defined. On those grounds it is natural to be interested in the 
function Vh(S) defined on sets 5 of x e R r . 
But the integration elements, x, N and i" = I[N] in the division systems 
described in Section 4.1 are in a mutual relationship of association, with 
x e l \ 
I[N] Ex*. 
The variation of h in subsets S of R T, V^(5), is of immediate interest. But the 
definitions above can easily be extended5 from elements S to elements 
Θ 
- 
SxMxJ 
C 
R T x λί(Τ) x I(R T); 
in which S C R T is a set of points x, M. C λί(Τ) is a family of finite subsets M 
of T, and J C I(R T) is a family of cells J of the domain R T. Thus, an element 
Θ = (x, M, J) belongs t o 0 = S x . M x J i f 
x e 5, 
M eM, 
J 
ej. 
And 
( l if xeS, 
M eM, 
J 
ej, 
[ 0 otherwise. 
Then define V)l(©), the variation of h in Θ, by 
ν Λ(θ) = inf ΥΧ(Θ) = inf J sup {(2>7) V \h(x,N,I[N})\ 
1Θ(χ,Ν,Ι[Ν\) 
(4.5) 
5For further details of how to formulate such extensions see Section A.l of the Epilogue. 

4.7. VARIATION OF A FUNCTION 
137 
Whenever it is necessary to distinguish this kind of variation from that of Def-
inition 27, call it R T x λί{Τ) χ I(RT)-variation. This definition is meaningful 
only if the set of Θ = (x, M, J) E Θ for which 
J = J[M], 
J[M] 
ex*, 
is non-empty. In other words, at least some of the elements (x,M,J) 
of Θ must 
satisfy the mutual relationship of association, since Riemann sums only involve 
associated elements. 
Also, if Θ = S x λί(Τ) x I(R T), then 
Vh(S)=\h(S); 
so in this case R T x λί(Τ) χ I(RT)-variation reduces to that of Definition 27. 
Of particular interest is R T x A/*(T)-variation, in which a set Θ has the form 
S x M x I(R T) where M may be a proper subset οίλί(Τ), 
but J = I(R T). This 
will be considered in Section 4.9. But first we investigate the outer-measure-type 
properties of the V/l(5) of Definition 27. 
Theorem 21 If S1 C S2 are subsets of E e E(R T) then 
Vh(Sl)[E] 
< 
Vh(S2)[E\. 
Proof. Since S1 C S2, 
\h(x,N,I[N])\lSl(x) 
< 
\h(x,N,I[N])\ls2(x)> 
For any gauge 7 and any division f7 of E, for i = 1,2 let ^(5 7) denote 
(^J^IMaJ.JV./fJVDIl^ia;). 
Then 
νι{εΊ) < ν2{εΊ), 
Vl{S^[E\ 
= 
sup^Mfi,)} 
< s u p ^ i ^ ^ ) } 
= 
Vl(S2)\E}, 
Vh(S')[E] 
= 
ini,{Vl(S')[E]} 
< inf7{V2(S2)[£]} 
= 
Vh(S2)[E], 
giving the result. 
O 
Theorem 22 If S1 C 5 2 ί/ien ν^(5χ) < Vh(S2). 
Proof. V^(5J) = V^(^)[R T], so the result follows from Theorem 21 on taking 
E = RT. 
O 
IfEe 
E(R T) then E is a subset S of R T. Note that Vh(E), = Vh(E) [R T], 
is not necessarily the same as V^fE1]. In other words, the variation of h in E 
is not necessarily the same as the variation οϊ h on E. The following example 
demonstrates this. 

138 
CHAPTER 4. THEORY OF THE 
INTEGRAL 
Example 28 Suppose E is a cell J = J[M] and y G J* \ J. That is, y is a 
vertex of J but y ^ J. Define h(x, N, I[N]) to be 1 if x = y and 0 otherwise (so 
h is atomic/ We have h(x,N,I[N])lj(x) 
= 0 for all x G J, giving V^(J) = 0 
for all 7, so V^( J) = 0. Choose a gauge 7 which conforms to J, so every j-fine 
division J1 of J must include a term (y,I[N]). 
Then 
(Jy)Y^h(x,N,I[N]) 
= 1 for all 
J 7, 
giving V^[J] = 1. This holds for all η' -< η, so V/JJ] = 1. Note that h is 
integrable on J, with integral 1, and that h is integrable in J, with integral 0: 
[h(x,NJ[N]) 
= l, 
[ 
h(x,N,I[N])lj(x) 
= 0. 
Note also that, for any E G E(R T), we have E C E (the latter being the set of 
associated points x of all I C E); so Theorem 21 implies Vh(E) < "Vh(E). 
O 
The variation of a function h exists whether or not h is integrable. While 
Definition 17 of the integral of a function h does not admit infinite values, the 
variation of h may be infinite. But the implications of a function h possessing 
bounded variation are of particular interest and will be encountered frequently. 
The usefulness of additivity was mentioned in connection with the Henstock 
lemma (Theorem 18). Integrability of additive cell functions (or Stieltjes in-
tegrator functions) is a central theme of this book. This section shows that 
when the variation of an arbitrary function h(x,N, I[N]) is calculated we get a 
functional V^ which is additive or sub-additive in several different ways. 
As in the one-dimensional case (Section 2.11), V/l(5) gives an /i-outer meas-
ure of subsets S of R T, as the ensuing discussion will show. The following 
lemmas are interim steps in expressing variation as outer measure. 
Lemma 1 For disjoint elementary sets E1 and E2 and any gauge 7, 
Proof. Choose any 7-fine divisions Sj of E\ j = 1,2. Then Si U £2 is a 7-fine 
division of E1 U E2 and 
{S1)Yj\h{x,NJ)\^{S2)Yj\h{x,NJ)\ 
= 
{S1^82)Yj\h{x,NJ)\. 
We are interested in the respective suprema relative to 7 of these three classes 
of Riemann sums. In the two left-hand classes we would expect any pair of 
Riemann sums, which are "close to" their respective suprema, to provide a 
Riemann sum for the right-hand side which is "close to" the supremum of the 
latter. But the disjoint El and E2 may contain intervals I1 and I2 which adjoin 
each other while remaining disjoint, so there may be an interval J straddling 
the disjoint E1 and E2; that is, J C E1 U E2 with JnEj 
φ§ for j = 1,2. Thus 

4.7. VARIATION OF A FUNCTION 
139 
the class of 7-fine divisions £ of E1 U E2 may include divisions other than those 
of the form £1 U £2· Therefore 
sup(f0 5 3 |M*, W, /)l + sup(£2) 5 3 |Λ(χ, ΛΓ, 7)| < sup(£) 5 3 \h(x, 
N,I)\. 
7 
7 
7 
Since 
supt^lM^-OI-V^Ui? 2] 
7 
the result follows. 
O 
Lemma 2 Given 7 and disjoint elementary sets E1 and E2, there exists 7' -< 7 
Proof. For j = 1,2, let 7^ -< 7 be chosen, in accordance with Theorem 3, so 
that jj conforms to E^\ and then take 7' = η[ Λ 72. Each 7;-fine division £' of 
E1 U E12 is the union of 7^-fine divisions £j of Ej, j = 1, 2. Therefore 
(50ΣΐΜ^^ί)ΐ + (ί2)Σι/ι^ΛΓ'/)ΐ = (5')Σι^.^^)ΐ· 
The suprema (found separately) of each of the two sums on the left, when added 
together, cannot be less than the joint supremum of two sums added together in 
the right-hand term. Unlike the preceding result, our choice of 7J prevents the 
occurrence of a 7J-fine interval J C E1 U E2 straddling E1 and E2. Therefore 
sup(f J) 5 3 \h(x, N,I)\+ 
sup(£2) 5 3 \h(x, N,I)\> 
sup(£') 5 3 
\h(x,N,I)\, 
7' 
7' 
Ί' 
giving the result. 
O 
Lemma 3 Given disjoint elementary sets E1 and E2, there exists a gauge 7' 
such that, for all 7" -< η', 
Vi'[E'} 
+ Vi'{E2} = 
\h
Y,[E1UE2}. 
Proof. This follows from the preceding two lemmas. 
O 
Theorem 23 For disjoint elementary sets E1 and E2, 
Vh[E1UE2}=Vh[E1}+Vh[E2}. 
Proof. Given e > 0 choose 7^ so that V^[Ej] < Vh[Ej] + \ε· In accordance 
with Theorem 3, choose 7' -< 71 Λ 72, with 7' conforming to both E1 and E2. 
Then 
V f c ^ u E 2 ] < VliE'uE2} 
= VilE^+V^E2} 
< Vfc[£?i]+Vfc[J5y+e. 

140 
CHAPTER 4. THEORY OF THE 
INTEGRAL 
Since this holds for all ε > 0, 
In particular, if the right-hand side is finite, then the left-hand side is finite. To 
prove the opposite inequality, choose a gauge 70 so that 
VI0[E1 U E2] < VHIE1 U E2} + e. 
For j = 1,2, take 7^ = 70 and 7' -< 71 Λ 72 as before, with 7' conforming to 
both E1 and E2. Then 
V ^ l + V ^ 2 ] < Vi[E2] + V([E2] = VIIE'UE2} 
< 
Wh[ElUE2]+e. 
Since this holds for all e > 0, the result follows. 
O 
Theorem 24 7/V^[RT] is finite then V/J.E] is a Stieltjes cell function. 
Proof. If Vh[RT] is finite then Vh[E] is finite for each E G E(R T), and finite 
additivity on disjoint figures follows from Theorem 23. 
O 
By Theorem 9, VjJJ] is integrable provided V^[RT] is finite. 
Theorem 25 //V^[RT] = 1 then Vh[·} is a distribution function. 
Proof. This follows from the definition of a distribution function. 
O 
Theorem 26 Suppose E1 G E(R T) is a proper subset of E G E(R T) with 
ν*(£) finite. With ε > 0 given, if η satisfies \l[E] < Vh[E] + e, then 
VZ[B1]<Vfc[£71]+e. 
Proof. Using Lemma 1, 
V^E1] < V^W-VllEXE1} 
< V ^ + s - V ^ X B 1 ] = V ^ J + e , 
as required. 
O 
Theorem 27 7/i?1 and E2 are disjoint figures and S C E1 U E2, 
Yh{S)[El 
U ^]=V f c(S)[£ 1] + Vh(S)[£2]. 
Proof. In Theorem 23 replace h by his- 
O 

4.7. VARIATION OF A FUNCTION 
141 
Theorem 28 Suppose 
oo 
S1 C 5 2 C · . . , 
S = U S J C R T 
T/ien lim^oo Vh(S') = Vh(S). 
Proof. SJ C 5·7'"1"1 implies V/l(S·7') is monotone increasing as j -4· oo, so the 
sequence converges to a finite limit or diverges to +oo. As S 5 5 j we have 
Vfc(S) > V/^S·7') and V/l(5) > lim^oo Vh(SJ). 
Therefore we can assume that 
the latter limit is finite. Choose 7j = (Lj,öj) so that, for j — 1, 2,3,..., 
V ^ ' ) = s u p { ^ 
Then, if a figure E contains Sj, and if £7. denotes T^-fine divisions of E, Theorem 
26 implies 
Vh
i(S*)[E\=s*p{(Z1J)Yi\h(x,N,I[N])\lsi(x)} 
<Vh(S^[E) 
+ 
e2'\ 
Taking S° to be the empty set, define a gauge 7 = (L, 8) so that, for x G 
Si\Si~\ 
L(x) = Lj(x), 
and, for x G S3' \ SJ~X and N 2 L(x) — Lj(x), choose 
Ä(x,iV) = 
Sj(x,N). 
(This is our first use of the property of gauges 7 in R T corresponding to axiom 
DS7 of the integral, Section 4.1.) Consider a 7-fine division Τ>Ί of R T. Let 
£7 
= 
{(x, I[N}) G νΊ : x G S} 
and 
£2, = 
{{x,I[N})eV7:xeSJ\Si-1}. 
then 
Because Τ>Ί is a finite collection, there is a largest integer /c, depending on 2λ 
such that E* is not empty. If 
& ={J{I: 
(x,I[N\) e ej} , j = 
l,2,...,k, 
(νΊ)Σ\Ηχ,Ν,Ι[Ν])\13(χ) 
= 
(εΊ)Σ\Ηχ,Ν,Ι[Ν})\ 
= 
Σ*=ιΦ)Σ\Φ,Ν,Ι[Ν])\ 
< E ^ i W A ^ " 1 ) ^ ' ] 
< Σί=ι^(^')[^'] 
< 
E * = i C W ) [ £ ' ] + e 2 - > ' ) 
< 
Ei=iV/,(5J')[^']+e 
= 
Vfc(Sfe)+e 
< 
l i m ^ V ^ + e . 

142 
CHAPTER 4. THEORY OF THE 
INTEGRAL 
Therefore, for any 7 and all ε > 0, 
sup{(2?7)S2\h(x,N,I[N])\ls(x)} 
= Vl(S) < lim Vh(Sk) + s, 
so V^(5) < limj-^oo Vh(SJ), giving the result. 
Q 
Theorem 29 Suppose E G E(R T) and S1 C S2 C ·.., 5 = U ^ i ^ ' £ E> 
Then 
UmVh(Si)[E}=Vh(S)[E}. 
Proof. The proof of this is similar to the previous one. 
O 
The next result establishes that, for any function ft, the functional V/j, is an 
outer measure for subsets S of R T. 
Theorem 30 // S^ is a countable family of subsets of R T with union S, then 
00 
vfc(S)<£vfc(S'-). 
Proof. Replace Ä·7 by 
&\{J{Sk:k<j}. 
Thus, without loss of generality, we can assume that the S^ are disjoint, and 
we can take the right-hand side finite. The proof is similar to the second part 
of the proof of Theorem 28. Choose 7j so that 
Vr
h
i(S^<Vh(Si) 
+ e2-\ 
and, as in Theorem 28, define 7 so that if (x,I[N]) is 7-fine and x e 5 J, then 
(x,I[N]) is Tj-fine. Let Τ>Ί be a 7-fine division of R T, and let 
E> = ( J {I[N} : (x,I[N]) G P 7 , X G S J'} , £7 = {(x, J[JV]) € P 7 : x € S J} . 
Since there are only a finite number of points x in Τ>Ί, there is a largest integer 
k, depending on Τ>Ί, such that Ek is not empty. Each Ö is a 7-fine division of 
EK Then 
(νΊ)Σ\Ηχ,Ν,ΐ[Ν})\ 
= 
E"=i((^EIM^/[iv])|i s,(x)|) 
< Σί=ι^(^[^]) 
< Σ£=ι(νΛ(#)+ε2-·0 
< Σ" = ιν Λ^)+ε. 

4.7. VARIATION OF A FUNCTION 
143 
Thus, for all ε > 0, 
oo 
Vh(S)<VKS)<J2vh(S>) 
+ e 
3 = 1 
soVh(S)<E7=iVh(Si). 
O 
Theorem 31 If E is a given figure and S^ is a countable family of subsets of 
E with union S, then 
oo 
Vh(S)[E\<Y^Vh(Sl)[IS\. 
Proof. Similar to previous proof. 
O 
It was mentioned earlier that the variation of a function in a set is analogous 
to outer measure of the set, helping to characterize "negligible" or null sets 
in integration. The following results illustrate further the connection between 
integral and variation. Suppose f(x) is a real- or complex-valued function, S is 
a subset of R T, and h(x,N, I[N]) is as before. 
Theorem 32 IfVh(S)[E] 
= 0 then Vfh(S)[E) 
= 0. Conversely, if S' C S and 
f(x) φ o for x e S', then Vfh(S)[E] = 0 implies Vh(Sf)[E] = 0. 
Proof. (The converse concludes that the support of / in S on E is /i-null. See 
also Theorem 48 below.) For j = 1,2,... let S^ be the set of x G 5 for which 
| / 0 r ) | < j . Then 
S1 C S2 C S3 C · · ·, a n d S j c 5 for each j . 
Therefore Vh(Sj)[E] = 0 for each j , and 
oo 
oo 
Vfh(S)[E\ < ΣVfH(S*)[E\ 
< ^JVh(S>)[E] 
= 0. 
For the converse, the function g(x) := (/(x)) _ 1 is defined in S" and is non-zero 
there. Then, by the first part, Yfh(S')[E] 
= 0 implies 
0 = 
V9fh(S')[E}=Vh(S'){E}, 
giving the result. 
O 
Taking E — R T, Theorem 32 says that, if S is h-null, then for any point 
function /, S is /ft-null; and if 5 is //z-null, then the support of / in S is ft-null. 
Returning to the comparison of ν^(^) and V/JÜ7], = V/l(JE), Figure 4.1 illus-
trates the set E\E. 
Part of the boundary of figure E (indicated by the thinner 
lines of the left-hand picture) does not belong to £*, but is contained in the set 
(not a figure) E on the right. But provided h satisfies a continuity condition, 
the following results show that Vh(E) and Vh[E] are the same. 

144 
CHAPTER 4. THEORY OF THE 
INTEGRAL 
—' 
Π 
'—' UZ2 
Figure 4.1: Comparison of sets E and E in R x R. 
Theorem 33 Given E G E(R T) and an arbitrary function h, if E\E 
is h-null 
thenVh(E)=Vh[E}. 
Proof. Let ε > 0 be given. Choose 70 so that 
E\E\XJ 
< 
9 
for all 70-fine divisions £7o of E. Choose one such £7o, and let 
E 1-(J{7[iV]:( a ;,/[7V]) €f 7 o, 
xeE} 
so £7o — {{x,I[N}) G £7o : x G E} is a 70-fine division of E1. Theorem 17 then 
implies 
(^)^|h(x,JV,J[7V])| 
ΐ£\£ΐ(χ) < 
ε· 
Note also that, by Theorem 23, 
Vh[E\E1]+Vh[E1]=Vh[E}. 
Choose 7i = (Li,S[) -< 70, with 7^ conforming to both of the elementary sets 
E1 and Ε\Ελ. 
Now choose 
ii(a;,JV)<min{Äi(a;,JV), | } , 
and, as we arranged for 70, let 71 = (Li,ii) also satisfy, in accordance with 
Lemma 3, the following three conditions: 
V£[Ei}<Vh[E'}+e, 
and 
V£[E\ 
E1} + \ft\E1] 
= 
\
W 
Note that, by construction, V^1 [i?1] = V^1 (E1). Choose a 71-fine division £7l of 
E1, so, by construction, £7l consists of 71-fine divisions of E1 and Ε\Ελ 
C 
E\EX. 
Let 
E 2 = U{/[7V]:(x,/[iV])Gi 7 l, , G ^ } . 
Proceed inductively for r = 2,3, 
Choose 7^ = (L r,^) -< 7 r-i, with jf
r 
conforming to both of the figures Er and E\E2. 
Choose 
5r{x,N)<mm{ö'r{x,N), 
2~r) , 

4.7. VARIATION OF A FUNCTION 
145 
and choose £7r and Er, with Vh[E\Er] 
+ Vh[Er) = Vh[E], \lr[Er] 
= V ^ ( £ r ) , 
and with ητ = {Lr,6r) 
satisfying 
{εΊτ)Σ\Η{χ,Ν,Ι[Ν})\1έ^{χ) 
<ε, 
VX[E\Er]<Yh[E\Er]+e, 
V£lEr}<Vh[Er]+e, 
and 
V^[E\Er} 
+ V^[Er} = V^[E]. 
For 7r = (Lr, Sr), 6r(x, N) < 2 - r , r = 1,2,3,..., we have 
oo 
E1 C E 2 C £ 3 C · · · , 
£ = | j £ r , 
r=l 
and Theorem 28 implies 
Vh(E) 
= lim Vh(Er),= 
lim Vh[£r] = lim (Vh[E] - Vh[E \ 
Er\). 
r—»oo 
r—»oo 
r—»oo 
We can choose ro so that, for r > ro and 7r-fine divisions Slr of 2£, 
Vh(E) 
> (£7r) ] Γ |/ι| - ε > νΛ[£] - 2ε, 
so νΛ(Β) > Vh[E\. 
Combined with the opposite inequality, this gives the result. 
O 
Example 29 Suppose a G R and h(x,I) 
is defined on R x I(R), with h(x,I) = 
I if I =]a,v] for any v > a, and h(x,I) 
= 0 otherwise. Then h is an atomic 
distribution function and is independent of x. 
Defining δ(χ) to conform to 
} — oo, a] and ]a, oo[, we find that, for any J =]a, b], J = [a, b], 
V£(J) = 0; V*[J] = \ * ( J ) = \ * ( J \ J) = V*({a}) = 1; 
Vh(J) = 0; Vfc[J] = Vh(J) = V f c(J\ J) = V^({a}) = 1. 
Example 30 If h(x,N,I[N]) 
= \I[N]\ for I[N] C £ tfien continuity gives 
Vh(E)=Vh[E] 
for all £ G E (R T) . 
~y2 
Example 31 If h(x,N,I[N}) 
= fj,N\ 
IYJ=I ^T/T^Vj then, by continuity, 
Vh(E)=Vh[E] 
for all £ G E (R T) . 
Theorem 34 IfVh(E\E) 
= 0 for each E G E(R T), and ifVh[RT] 
< oo? then 
\h(E) 
is additive on figures (and, so, a Stieltjes cell function in HT), with 
[ νΛ(/)=νΛ[ΐΙτ]. 
If\h[R>T] = 1 then V^(·) is a distribution function. 
Proof. This follows from Theorems 33, 24, and 9. See also Theorem 39. 
Q 

146 
CHAPTER 4. THEORY OF THE 
INTEGRAL 
4.8 
Variation and Integral 
The Riemann sum structure of the variation functional gives it distinctive qual-
ities. 
This section demonstrates that integral and variation relate to each 
other in ways which correspond to the relationship between integral and meas-
ure/outer measure in Lebesgue integration. 
Theorem 35 If fE, h(x,N,I[N]) 
= 0 for every figure E' C E € E(R T), then 
Vh[E}=0. 
Proof. Using the proof and notation of the Henstock lemma (Theorem 18), the 
hypothesis gives H(I[N]) = 0 for each I[N] C E. Therefore, for i = 1,2,3,4, 
(εί,)Σ\Κχ,Ν,Ι[Ν])\<2ε,*> 
(£ 7)]Γ|/>(Χ,ΛΓ,/[^ 
2 = 1 
The result follows from this. 
O 
Theorem 36 IfWh[E] = 0 then h is integrable on E and j E h = 0. 
Proof. Given e > 0 there is a gauge 7 so that, for each division £7 of E, 
|(£r)5>|<(£y)5>|<e, 
giving the result. 
O 
The following is a key statement about sets of /i-variation zero, or /i-null 
sets. It corresponds to a similar theorem involving outer measure and follows 
from the preceding result if E and h are replaced by R T and his, respectively. 
The convention "fs ft" is used to denote f hls(x) 
when S £ E(R T). 
Theorem 37 If S C R T with V/l(5) = 0 then his is integrable on R T with 
Jsh = 0. 
Proof. Choose 7 so that (D7) ]T \h(x, N, I[N])\ls(x) 
< £ for all 7-fine divisions 
νΊ of R T. Then 
giving / R T h(x,N,I[N])ls(x) 
= 0. That is, fsh 
= 0. 
O 
If the function h is now multiplied by a point function f(x) then the /i-null 
set 5 is also fh-mi\\. 
Theorem 38 If S C R T with V/l(5) = 0, and if f is a real- or complex-valued 
function of x G R T, then V//l(5) = 0, and fhls 
is integrable on R T with 
Jsf(x)h(x,N,I{N]) 
= 0. 

4.8. VARIATION AND INTEGRAL 
147 
Proof. 
The first part follows from Theorem 32, and the second part then 
follows from Theorem 37. See also Theorem 49 below. 
O 
Next we show that if h is absolutely integrable its integral equals the varia-
tion. 
Theorem 39 // \h\ is integrable on E G E(RT), then fE \h\ = Vh[E}. 
Proof. Let ε > 0 be given. Since \h\ is integrable, there exists a G R so that, 
for all £7, 
\(εΊ)^2\Φ,Ν,Ι[Ν})\-α\<ε. 
Choose η' -< η so that, for all £y, 
\(£Ί,)Σ\Ηχ,Ν,Ι[Ν])\\>νίΐΕ}-ε. 
Now choose 7" -< 7' so that, for all 7"' -< 7", 
Vf[E\<Vh[E\+e. 
Then, writing β = (£y») Σ \h(x, N, I[N])\, 
\a-Vh[E}\ 
<\a-ß\ 
+ \ß-V£"[E}\ 
+ \v£"[E\ - Vh[E] 
giving the result. 
<3ε, 
o 
Theorem 40 If \h\ is integrable on R T
; then V/Ji] is integrable on R T and 
[ 
Vh[I\= 
[ 
\h(x,NJ[N])\-
JnT 
JE 
Proof. This follows from Theorem 39 above, with Theorem 20. 
o 
Definition 28 A function h(x,N,I[N\) 
has bounded variation on E G E(RT) 
if Vh[E] is finite. 
Theorem 41 // \h\ is integrable on R T then h has bounded variation on R T. 
Proof. This follows from Theorem 39. 
O 
Conversely, the next result, based on the Henstock lemma (Theorem 18), 
summarizes much of the relationship between integrability, absolute integrabil-
ity, and the variation functional. 

148 
CHAPTER 4. THEORY OF THE 
INTEGRAL 
Theorem 42 If h is integrable on E with integral H(E), and if h has bounded 
variation on E, then \h(x,I,N)\ 
and \H(I)\ are integrable on each figure E1 C 
E, with 
[ 
\h(x,N,I[N})\= 
[ 
\H(I[N]) = Vh[El], 
JE1 
JE1 
and we have 
Vh[E1]=VH[E1] 
and ^ ( S 1 ) ! < V^E 1]. 
Proof. For any division £ of any figure E1 C E we have 
ΚΟΕΙΜχ,/,ΛΟΙ-^ΣΙ^ΟΟΙΙ < 
{εχ)ΈΜ-\H\\ 
By the Henstock lemma (Theorem 18), with ε > 0 given we can choose a gauge 
7 so that, for each 7-fine division 8* of E1, 
(^ΣΐΜχ,/,^Ι-ί^Σΐ^)! < ε. 
Thus 
VH[E1]=Vh[E1]. 
Since H is a finitely additive function of intervals, \H\ is finitely sub-additive, 
and since H and \H\ have bounded variation, Theorem 11 implies \H\ is int-
egrable; and its integral is V#[E1] because \H\ > 0. Finally, by sub-additivity, 
| i i ( ^ ) | < / 
iHWm-VhiE1]. 
JE1 
This completes the proof. 
O 
The following results are useful in the analysis of random variability and are 
cited frequently in proofs. To begin, the concept of variational equivalence of a 
pair of functions is introduced. 
Definition 29 Two functions hi and hi are variationally equivalent if their 
difference h = hi — /12 has variation zero; that is, Vh [RT] = 0. 
Theorem 43 // a real- or complex-valued function h(x,N,I[N]) 
is integrable 
on E with integral H(E), then the indefinite integral H(I[N]) is variationally 
equivalent to h(x, N, I[N]). 
Proof. This follows from Henstock's lemma (Theorem 18). 
O 
Theorem 44 Let f be a point function. 
Suppose hi and hi are variationally 
equivalent, and suppose f(x)hi(x, 
N, I[N]) is integrable on E. Then 
f(x)h2(x,N,I[N}) 

4.8. VARIATION AND 
INTEGRAL 
149 
is integrable on E and 
[ /(x)/n(x,iV,/[7V] = ί 
f(x)h2(x,N,I[N]. 
JE 
JE 
Proof. 
(See also Theorem 50 below.) Using Theorem 32 and writing h = 
h1-h2, 
V^[RT] = 0 implies V)h[RT] = 0 so Vfh[E] = 0 for all E, and fh is 
integrable with JE fh — 0. By Theorem 13, fh2 = fh — fh\ is integrable and 
ΙΕ^ 
= ΙΕ^- 
O 
Definition 30 Given real- or complex-valued functions fi(x,N), 
f2(x,N), 
and 
h(x,N,I), 
we say that f\ and f2 are h-equivalent if writing 
k(x,N,I) 
= (Μχ,Ν) 
- 
f2(x,N))h(x,N,I), 
we have Vfc[RT] = 0. In that case write 
h = h-
Theorem 45 If fi = f2 and if either of fj(x)h(x,N,I) 
is integrable on R T, 
j — 1,2, then both are integrable and 
[ h(x)h(x,NJ) 
= [ 
f2(x)h(x,N,I) 
JE 
JE 
IE 
JE 
for all E G E(RT). 
Proof. This follows from Theorems 37 and 44. See also Theorem 51 below. O 
Further to /i-equivalence of point functions, the notion of /i-weak convergence 
of point functions is useful in the theory of random variability. 
Definition 31 Given real- or complex-valued functions fj(x, N), j = 1, 2,3,..., 
we say that fj(x,N) 
converges h-weakly to f(x,N) 
(or fj(x,N) 
/i-converges to 
f(x,N)) 
as j —)· oo if writing 
kj(x,N,I) 
= 
(/ι(χ,Ν)-Κχ,Ν))Η(χ,Ν,Ι), 
we have limj^oo Vj.. [RT] = 0. In that case we write 
fj A /. 
Theorem 46 Suppose f\{x), 
h(x), 
and h(x,N,I) 
are real- or complex-valued 
functions. 
Suppose H is the indefinite integral of f2h. 
Then the existence of 
either of 
[ h(x)H(I[N}), 
[ 
f1(x)f2(x)h(x,N,I), 
JE 
JE 
implies the existence and equality of both. 

150 
CHAPTER 4. THEORY OF THE 
INTEGRAL 
Proof. By hypothesis, H(E) = JE f2(x)h(x,N,I) 
exists for each E e E(R T). 
By Theorem 43 the function f2(x)h(x,N,I) 
is variationally equivalent to its 
indefinite integral H(I), and V/2h-ij[RT] = 0. Therefore, by Theorem 32, 
Vf1(f2h-H)[R<T] = 0; so, by Theorem 38, the function fi(f2h — H) is integrable 
on R T with integral 0, 
/ 
(Λ(x)f2(x)h(x, 
N, I) - h(x)H(I)) 
= 0. 
Therefore, by Theorem 13 (finite additivity of integrals), if either of f\f2h 
and 
fiH is integrable then the other is, and their integrals are equal. 
O 
Theorem 46 is the justification for integration by substitution, also known as 
the change of variable method of integration. (Change of integrator is a more 
informative title.) 
It demonstrates again how a Burkill-complete integrable 
function can be expressed as a Stieltjes-complete integrable function. 
Suppose f(x) is a monotone increasing function defined on a compact interval 
]a, b] C R. Define the increments /(/) on subintervals / =]u, v] C]a, b] by 
/(/) := f(v) - f(u). For all partitions a = tx < tx < · · · < tp = 6, £ / ( / ) = 
^ | / ( / ) | = b — a. Thus, on domain ]a, 6], /(/) has finite variation, and has 
Stieltjes-complete integral, with both equal to b — a. 
Example 32 Many familiar, 
"non-pathological" functions do not have finite 
variation; for instance, the function 
f(x) = x 
2cosx 
2, 
0<x< 
This continuous function has finite variation on each interval 
]ur+i,ur] 
where 
ur — y ^ v 
<w§ 
+ΐ)π· 
but is not Lebesgue integrable 
The function is Henstock integrable on 
there. This can be seen by substituting y — x~l, so 
rur 
rurl\ 
I 
x~2cosx~2dx 
= 
/ 
cosy2dy, 
ry/ΐ 
/*oo 
/ 
x~2cosx~2dx 
= 
/ 
cos y2dy, 
Jo 
Jy/% 
and it was shown in Section 2.15 that the latter integral exists as a Henstock 
(but not Lebesgue) integral. A similar argument shows that the variation of the 
corresponding Stieltjes increment function f(]u,v]) = f(v) — f{u) on domain 
1 
oo 
r=0 
is infinite. 
O 

4.8. VARIATION AND 
INTEGRAL 
151 
Functions that are otherwise difficult and complicated may be amenable to 
analysis if, like the function f{x) of Example 32, they have a structure in which 
it is possible to distinguish or identify sets in which their variation is finite. 
This is the property of generalized bounded variation. It is useful to develop an 
understanding of this property in the domain R T. 
Example 33 In the theory of Brownian motion [120] the sample paths x G R T 
(T a real interval) are almost surely not of bounded variation in any sub-interval 
of T. Writing % = ]s, s'] G 1(T), and x(z) = x(sf) — x(s), this means that, for 
any given j G I(T), the variation Vx[j] is infinite for G-almost all x G R T, 
where G(I) is a potentiality distribution function defined for I G I(R T). (See 
Theorem 173.) By Theorem 1^1, this means that the cell function x(z) is G-
almost surely not absolutely 
integrable 
on T. In particular, x(z) is almost 
surely not Lebesgue integrable. However, x is an additive cell function, and 
therefore, by Theorem 9, it is integrable on T in the Stieltjes-complete or non-
absolute sense. If, for instance, T — [a, b], then 
f x(i) =x(T) 
= x(b) - x(a). 
Example 34 In Example 33, the sample path x is continuous. But x can be 
an arbitrary element ofHT. 
Even if x is everywhere discontinuous the equation 
JT x(z) = x(T) = x(b) — x(a) remains valid. 
O 
Definition 32 A function h(x,N,I[N]) 
has generalized bounded variation on 
E G E(R T) if there is a countable family of sets {Sj C E} such that Vh(Sj)[E] 
is finite, j = 1,2,3, 
Then we say that h is VBG* on E. 
Our main interest will be in the case E = R T, so we will be considering functions 
h for which sets 5 J C R T exist with V/l(5·7) finite, j = 1,2,3, — In this case, 
when h is VBG* on R T, we simply say that h is VBG*. 
Theorem 47 The function h(x,N,I[N]) 
is VBG* if and only if there is a 
positive function f(x) for which the product fh has bounded variation 
(i.e., 
V/h[RT] is finite). 
Proof. If V / h[R T] is finite, let 
Sj = {x:i<f(x)<1^}, 
j = l,2,3,.... 
Then, for x G Sj, we have 
\h(x,NJ[N])\<jf(x)\h(x,NJ[N])\. 
Thus 
Vh(Sj)<jVfh(&)<jVfh\RT]. 

152 
CHAPTER 4. THEORY OF THE 
INTEGRAL 
Since the latter is finite, V/l(5·7) is finite for each j , so h is VBG*. Conversely, 
if R T = \J°°=1 5·7, with the sets 5 J mutually disjoint, and with V/l(5J) finite for 
each j , then, given ε > 0, we can find jj so that 
If η- = (Lj,ij) (j = 1,2,3,...), choose 7 so that, when x e S^, L{x) = Lj(x) 
and £(x,7V) = Sj(x,N) 
for each TV G Λ/"(Τ). This ensures that if (xJ[N]) is 
7j-fine, then (x, I[N]) is also 7-fine. For j = 1,2,3,... and x e Sj, let 
2i(Vfc(SJ) + e)· 
Then, as in the proof of Theorem 30, 
oo 
V/fc[Rr] 
< 
V}h[RT] < 
*£,*&(&) 
OO 
OO 
giving the converse. 
O 
4.9 
R T xA/"(T)-Variation 
The preceding sections describe the properties of RT-variation. The definition 
in (4.5) gives an extension which we call R T x λί(Τ) χ I(RT)-variation, and 
which has the same properties as RT-variation, in the sense that, when sets 
Θ = 5 x Λ4 x J are substituted for sets 5, then theorems corresponding to 
those of Section 4.7 hold; and, mutatis mutandis, the proofs are the same. 
See also Epilogue, Section A.l, which has an account of R T x λί(Τ) χ I(R T)-
measurability. 
For applications in this book, only sets Θ = S x M x I(R T) are needed, 
and, for ease of reference, some of the more important results are stated in this 
section. As indicated earlier, when J = I(R T) the latter becomes redundant in 
S x M x I(R T), and we can write Θ = 5 x M. Thus, with 
Θ = SxM 
C RTxA/"(T), 
we have 
(z, N) β Θ if and only if a: € 5, N e M; 
and definition (4.5) turns into the definition of V/l(0) [R T]. That is, 
Vfc(0) = 
infVfte) 
= 
iniLup{(Vy)Y;\h(x,NJlN})\le(x,N)}\, 
(4.6) 

4.9. RTxA/"(T)-VARIATION 
153 
where V7 denotes 7-fine divisions of R T. The definition of V^ (©)[£"] is analog-
ous. 
Note that every x G R T is associated with every TV G Af(T) in the sense 
that, for each x and each TV, there is I[N] G I(R T) with I[N] Ex*. In contrast, 
for sets Θ = S x M x J with J a proper subset of I(R T), it is not necessarily 
the case that an arbitrary (x,M,J) 
G θ is an associated triple; in other words, 
it is not necessarily the case that x(M) is a vertex of J(M). 
The distinction between the R T x A/"(T)-variation of a function on a figure, 
and the R T x Ar(T)-variation of a function in a subset Θ of R T x λί(Τ) 
is 
clear. For instance, the R T x A/*(T)-variation of a function h on the figure R T is 
simply V^[RT], the variation (or RT-variation) of h; and there is no particular 
need to extend this to R T x λί(Τ)-variation 
on E x M where E G E(R T) is an 
arbitrary figure in R T. 
Definition 33 The R T x λί(Τ)-variation 
of h(x,N,I[N}) 
on R T is 
inf|sup{(P7)^|/i(x,Ar,/[7V])|}|. 
Definition 34 Ife = SxMcKTx 
λί(Τ) then h has bounded R T x λί(Τ)-
variation if 
ν Λ(θ), = inf j sup {(P 7) Σ 
\h(x, Ν, Ι[Ν])\ 1θ(χ, Ν)}\ 
, 
is finite. 
Definition 35 Suppose, for j = 1, 2,3,..., 
00 
θ^" = Sj x Mj c RT x ΛΤ{Τ), 
\J (Sj x Mj) = RT x AT(T), 
3 = 1 
with Vh(Oj) finite for each j . Then we say that h is R T x 
N(T)-VBG*. 
The proofs of basic results are obvious and are not essentially different from the 
corresponding results given in Sections 4.7 and 4.8. For instance, if Θ1 C Θ2 
then 
v^©1) < ν^(θ2). 
And if Oj C 9j+1 
for each j = 1,2,3,..., then, with Θ = U£Li ©J, 
lim ν„(θ*) = νΛ(θ). 
The proof of the latter is not especially easy, but it is essentially the same as 
the proof of Theorem 28. 
There is a R T x λί(Τ) analogue to the theory of /i-null sets S C R T. A set 
e = SxMcKT 
x λί(Τ) 

154 
CHAPTER 4. THEORY OF THE 
INTEGRAL 
is /i-null if V/l(©) = 0. This holds if, given ε > 0, there exists 7 such that, for 
every 7-fine division Τ>Ί of R T, 
(νΊ)ΣΐΘ(χ,Ν)\Η(χ,Ν,Ι[Ν])\<ε. 
The proof of the following theorem is similar to the proof of Theorem 32. It 
features functions f(x,N) 
instead of /(#). 
Theorem 48 Suppose functions f(x, N) and h(x, N, I[N]) are real- or complex-
valued. IfVh(G)[E} 
= 0 then Vfh(S)[E] 
= 0. Conversely, if Θ' c θ and 
f(x,N) 
φ 0 for (x,JV) G θ', then Yjh(ß)\E\ 
= 0 implies Vh(&)[E\ = 0. 
Theorem 49 If Vh{&) — 0 and if f(x,N) 
is real- or complex-valued, then fh 
is integrable in Θ and Js f(x,N)h(x,N,I[N]) 
= 0. 
Theorem 50 For given f(x,N), 
suppose hi and /12 are variationally equiv-
alent, and suppose /(x, N)h\(x, N, I[N]) is integrable on E. Then 
f(x,N)h2(x,N,I[N}) 
is integrable on E and 
ί /(χ,Ν^χ,Ν,ΙΙΝ] 
= [ 
f(x,N)h2(x,N,I[N}. 
JE 
JE 
Theorem 51 If fi = fa and if either of fj(x, N)h(x, iV, /) is integrable on R T, 
j = 1,27 then both are integrable and 
ί Μχ,ΝΜχ,Ν,Ι) 
= ί 
f2{x,N)h{x,NJ) 
JE 
JE 
forallEeE(RT). 
The following result corresponds to Theorem 47, and its proof is similar. 
Again, the point function f(x) of Theorem 47 is replaced here by a function 
f(x,N). 
Theorem 52 The function h(x,NJ[N]) 
is R T x N{T)-VBG* 
if and only if 
there is a positive function f(x,N) 
for which the product 
f(x,N)h(x,N,I[N]) 
has bounded variation (i.e., Vf/JR7"] is finite). 
There is further extension of the theory along these lines in Section A.l. 
4.10 
Introduction to Fubini's Theorem 
The Fubini-Tonelli group of theorems is used in analyzing the variability propert-
ies of joint observables: to determine independence, covariance, and correlation, 
for example. The focus of this book is on joint random variability, and there is 

4.10. INTRODUCTION 
TO FUBINFS 
THEOREM 
155 
hardly a single statement that can be made about joint random variability that 
does not depend in some way on Fubini's theorem. 
The basic idea is as follows. In two dimensions R2 = R x R, a function 
f{x\,X2) of two variables can be integrated with respect to a function \I\ x 72| = 
|ii||i21 of intervals I = I\ x h in R2, where \Ij\ is the length function of one-
dimensional intervals. In the standard notation this is / R 2 f {x\,X2)dx\dx2- 
The 
Fubini-Tonelli theorems are then concerned with the relationships between the 
integrals 
/ 
f{x\,X2)dxidx2, 
/ ( / f{xuX2)dxAdx2, 
\ ( / 
f(xi,X2)dx2\dx1. 
These relationships can be regarded as double limits, in the following manner. 
Suppose, for each #1, f(xi,x2) 
is integrable with respect to Χ2 (i.e., with respect 
to integrator function I/2I) in R, with integral 
a(ari) = / f{xux2)dx2= 
/ 
f(xi,x2)\l2\· 
Jn 
Jn 
Then there exists a sequence Vj of divisions of R so that, defining 
/,(*!) :=(P,) £ / ( £ ! , X 2 ) | I 2 | , 
we have 
lim fj(xi) 
= a(x1). 
Tonelli-type theorems are concerned with the existence of 
f(xi,x2)dxidx2 
L 
R2 
whenever the existence of J R a{x\)dx\ is assumed. Examples are given in Natan-
son [182], pages 91-92, of functions /(χι,α^) for which the iterated integrals 
(with respect to integrators |/i|, |i2|, respectively) exist, but the double integral 
with respect to |ii x J2I d° e s not exist. 
Fubini's theorem assumes the existence of the double integral 
/ 
f(x1,x2)dx1dx2 
Jn2 
and is concerned with the integrabihty of a{x\) with respect to X\ (i.e., with 
respect to |ii |). In standard accounts of Fubini's theorem it is also assumed that 
/ is measurable. The theorem is proved here without this extra assumption. 
This is much more difficult, but a much deeper version is achieved. 
Tonelli-type theorems can often be subsumed within the framework of limit 
theorems such as the dominated convergence theorem, which will be addressed 
later in this chapter. But Fubini's theorem has a logic which is the opposite 
of, say, the dominated convergence theorem. In contrast to the latter, Fubini's 

156 
CHAPTER 4. THEORY OF THE INTEGRAL 
theorem starts with the existence of the double limit and works backwards from 
that to the existence of the other limits. 
Because of this, Fubini's theorem uses a particular kind of argument which 
does not appear in the framework of the other limit theorems for integrals; so 
this subject is considered first. 
Consider the double sequence of real numbers 
A = {aij:i 
= 1,2,3,...; j = 1,2,3,...}. 
This sequence is convergent, in the Cauchy sense, if, with ε > 0 given, there 
exist i0, jo, so that i > i0, ϊ > io, j > jo, f > jo imply 
Prom this, single sequences Bl and C·7 arise as follows. 
■ For each fixed z, Bl = {α^ : j = 1, 2,3,...}. 
■ For each fixed j , Cj = {a^· : i = 1, 2,3,...}. 
What connections, if any, exist between convergence of the double sequence A 
and the various single sequences B% and C·7? Here are some examples to show 
that, without extra conditions, there can be convergence or divergence of A in 
combination with convergence or divergence of sequences Bl and CJ. 
Example 35 Take A to be the double sequence 
_ f 1 
if 
i=J, 
aij~{ 
0 
if 
Ϊφ3> 
Then A is divergent as a double sequence, while, for all i, j , each of the seq-
uences Bl, & converges. 
O 
Example 36 Let 
_ (-l)i-n 
a*, - 
. 
. 
Then A converges, each Bl diverges, and each 0 
converges. 
Q 
Example 37 Let 
/ 
i 
-.\i-t-l 
if 
i < j , 
(-DJ+1 
y' + i 
±ψ- 
V 
i>j. 
Then A converges and each of the sequences of B% and C·7 diverges. 
Q 

4.10. INTRODUCTION 
TO FUBINFS 
THEOREM 
157 
Replacing sequences by suitably arranged collections of Riemann sums, Fub-
ini's theorem shows that, when the double limit of A exists as a joint integral, 
each of the marginal limits of Bl exists as an integral, and the marginal limits 
converge to the limit of A. Likewise, each of the marginal limits of CJ exists as 
an integral, and these marginal limits also converge to the limit of A. 
The core of the Fubini argument is the axiom DS8 (see Section 4.1) of gauges 
and divisions in product spaces. Suppose T = T U T" with T Π T" = 0, so 
R T = R T x R T . Then, writing χτ = x, XT1 — χ\ 
χτ" 
— x", there is a relation 
x = (χ', x") for each x 
and the families of finite subsets of T', T", T are λί(Τ'), Ν{Τ"), N(T), 
with 
N = N' U N" 
for N' G ΛΤ(Τ'), N" G N{T"), 
N G λί(Τ) 
where some of the finite sets are allowed to be empty. Similarly, given cells 
Γ = I'[N'} G I(R T /) and I" = I"[N"} G I(R(T"), then 
I = I[N] =Γχ 
I" = I[Nf U N"} G I(R T). 
If we have divisions V = {(*',/'[TV'])}, V" = {(χ»\Γ[Ν"})} 
of R T', R T", 
respectively, then 
V x V" = {((χ',χ"),Γ 
x I") : {Χ',Ι'ΙΝ']) 
G V, 
(x",I"[N"]) 
G D"} 
is a division V of R T. In fact, V x V" is a regular division of R T = R T x R T , 
as described in Section 3.3. 
But at this stage a difficulty is encountered. For a given division P, we 
cannot "travel in the opposite direction". In tackling Fubini's theorem, we 
start with the existence of an integral J R T h(x,N,I[N]) 
and are therefore led 
to consider divisions V = {(x,I[N])} 
of R T, and such a division is not usually 
expressible as a product V x V" of divisions of R T and R T . 
In other words, fully regular divisions are not available. 
A related difficulty can be illustrated by means of finite-dimensional gauges 
δ. Suppose a positive function δ is defined for points (#i, x<i) G R x R. Suppose 
we try to deduce from this a gauge δ\ defined for points X\ in the one-dimensional 
projection R of R x R. If we say that δ\{χ\) is the same as δ(χι,χ2), 
then δι 
is not well defined, since there are infinitely many possible values of x2 and 
therefore infinitely many possible values of <5i(#i). 
The Riemann sum or Henstock integral version of Fubini's theorem has a 
strategy that overcomes this difficulty. The strategy, based on the construction 
in axiom DS8, is outlined in Section 3.4. Figure 3.3 may be helpful in assessing 
it. 
The strategy is as follows. Given a gauge 7 in R T, we can find gauges η"(χ') 
in R T , one for each x' G R T', and 7//(x/)-fine divisions V"{xf) of R T \ and 
then we can find a gauge η' in R T so that, for every 7/-fine division V of R T , 
the finite collection 
{(x,I{N])} = 
{((x',x"),I[N'uN"))} 

158 
CHAPTER 4. THEORY OF THE 
INTEGRAL 
is a 7-fine division of R T, with (x',I'[N']) 
G V and (x",I"[N"}) 
G V»(x') for 
each (χ',/'[TV']) 
e&. 
Denote this collection by V x (\JX, V"{xf)). 
This is a partially regular 
division. The proof is as follows. 
Theorem 53 Suppose a gauge 7 in R T is given. For each xf G R T there is a 
gauge 7^, for which, given any collection of^n{x')-fine 
divisions V"{x') o/R T 
(all x' G R T ) , there is a gauge 7' in R T such that, ifV 
is any η' -fine division 
of R T , then the finite collection 
V = V x ( ( J P " 0 O : {*'J'[H']) € D'Y) 
is a η-fine division 
ofHT. 
Proof. Essentially, we must show that division system axiom DS8 is valid for 
gauges 7 in product spaces. Each x' G R T determines a subset ( a "slice" or 
hyperplane) 
{x = (x',x"):x" 
G R T " } C R T . 
In R T , define a gauge η"(χι) = (L", δ") that depends on x'\ 
R T" 
& 
Λ/·(Τ"), 
x" 
-+ L"{x") 
:= 
L((x ,,x , ,))nr , ,
; 
Έίτ"χλί(Τ") 
& 
]0,oo[, 
(χ",ΛΓ") 
-> 
<ϊ"(χ",ΛΓ") 
:= 
£((*', χ"),ΛΓ"). 
Now let i'(x') 
= (L", 5")· Let V"(x') be a 7,/(a;/)-fiiie division of R T ' . Define 
L'(x') 
:= 
U {^ ((*', *")) : (*", W " ] ) € P"} , 
δ'{χ', Ν') 
:= 
min {5 ((a/, z"), ΛΓ' U N") : (x", J"[J\T"]) G P"} . 
For any given #', the collection V"(x') (depending on x') is finite. Because of 
this fact we now have Lf(xf) 
G Af(Tf) (i.e., a finite subset L'(x') of T') and 
δ'(χ' ,Ν') a positive number. Now define a gauge η' = (£/,£') in R T : 
R T' 
K 
M(T'), 
x' 
-+ 
L'(x'); 
R T'xAf(T') 
Ä 
]0,oo[, 
{χ',Ν') 
-+ 
δ'(χ',Ν'). 
Then, for any 7/-fine division V of R T , the construction above ensures that 
V = V x (|J{2>'V) : (x'J'[N']) 
^ £'}) 
is a 7-fine division of R T. 
O 

4.11. FUBINFS 
THEOREM 
159 
The proof permits T' and T" to be interchanged in this theorem. So, for 
any gauge 7 in R T, there is a 7-fine division V of R T, and gauge 7' or 7", such 
that the corresponding Riemann sum (V) ]T · · · is expressible as, respectively, 
(ν)Σ,{(Ρ"^))Σ·--} or θΡ'θΣί^ν'^Σ···}· 
Consider a function 
h(x,N,I[N]) = 
ti(x,,N',r[N'))ti'(x",N",I"[N"];x'), 
assumed to be integrable on E e E(R T), with integral H(E). 
(In fact we take 
E = R T in order to reduce the burden of notation, but the proof is valid for any 
E £ E(RT).) The proof of Fubini's theorem falls into several parts involving 
investigation of Riemann sums. It is found that iteration of Riemann sums, as 
permitted by DS8 (Theorem 53), gives the iteration of integrals that constitutes 
Fubini's theorem. 
Example 38 Consider a two-dimensional integral 
f(xi,x2)dxidx2 
where E =]0, l]x]0,1]. In the notation used in this book, this integral can be 
written JEf(xux2)\h 
x h\ or fE f(xux2)\Ii\\I2\. 
Then h'(x',Ν',Γ[Ν'}) 
cor-
responds to the interval function \I\\, and h"(x",N",I"[N")\x') 
corresponds 
to f(xi,x2)\I2\; 
and in the course of the proof of Fubini's theorem we show 
that, if f(xi,x2)\Ii 
x I2\ is integrable on [0,1] x [0,1] then, for each x\ G [0,1], 
Jo f(xi,x2)dx2 
exists except for a null set of x\ in [0,1]. 
O 
4.11 
Fubini's Theorem 
Denote 
/ 
ti'{x",N",I"[N"]-x') 
by a{x') whenever this integral exists. Assuming integrability of h on R T, we 
deduce the existence of the integral a{x') on R T , and then go on to establish 
the integrability of a(x')h' in R T on R T. When it is shown that the iterated 
integral JRT/ a(x')h"(x') 
is the same as J*RT /i, = iJ(R T), the proof of Fubini's 
theorem is complete. Having demonstrated, in Theorem 53, the validity of 
axiom DS8 for gauges 7 in R T, the only assumption is the integrability of the 
function h on R T. 
To establish6 existence of the two integrals a(xf) and JRT/ a(x')h!', we invoke 
the criterion of Theorem 19; that is, existence of the integral of a function is 
/ 
The origins and antecedents of this proof are outlined in Henstock [103]. 

160 
CHAPTER 4. THEORY OF THE 
INTEGRAL 
equivalent to the existence of a Stieltjes cell function, such that the absolute 
value of the difference of these two functions is integrable, with integral zero. 
The indefinite integral H(E) = JE h in R T provides a suitable Stieltjes cell 
function, as required by the integrability criterion. Integrability of the Burkill-
complete integrand h gives access to Stieltjes-complete integrable H. 
Theorem 54 (Fubini's theorem) Assume 
h{x,NJ[N]) 
= 
h\x\N'Jf[N'})tif{x",N"J"[N"};xf) 
is integrable on R T. Then: 
1. The integral 
a(x')= 
[ 
h"{x'\N"J"[N%x') 
ϋτσ" 
exists except for a set S' of x' G R T with Vh>{S') = 0. 
2. The product a(x')h'(xf, 
N',V[N']) 
is integrable on R T' and 
[ 
h(x,N,I[N])= 
[ 
a ^ ^ V ^ ' , / ' ^ ' ] ) . 
Proof. Since h is integrable there is an additive function H(J) (J G I(R T) 
with H(HT) = J R T h. Let ε > 0 be given. Using Theorem 18 (the Henstock 
lemma), choose 7 so that, for each 7-fine division Τ>Ί of R T, 
\(νΊ)ΣΗχ,Ν,Ι[Ν})-Η(ΈΙτ)\ 
< 
f, 
(νΊ)Σ\Ηχ,Ν,Ι[Ν])-Η(Ι[Ν])\ 
< ε. 
Use Theorem 53 (axiom DS8) to select, from 7, 
■ for each x'\ a gauge η"{χ'~)\ 
■ for each x'\ a 7//(x/)-fine division V"(xr) of R T ; and 
■ corresponding to the family of divisions I V"(xf) : x' G R T k a gauge η' 
so that: 
■ for each 7/-fine division V of R T , the division 
V 
= 
{(χ,Ι[Ν}) = 
((χ,,χ"),Γ[Ν'υΝ"}): 
(χ',Γ[Ν'}) 
G V, 
(x"J"[N"]) 
G V»{x')} 
= 
{Vf xV"(x') 
: (χ',Γ[Ν'\) 
eV'} 
is 7-fine. 

4.11. FUBINFS 
THEOREM 
161 
Thus DS8 provides a 7-fine division T> of R T in which iteration of Riemann 
sums is possible, so (P) Σ ' ' ' can be expressed as 
(ρ')Σ{(ρ'ν))Σ···}· 
With a view to producing a Cauchy-type argument for convergence of Riemann 
sums of terms 
h"(x",N"J"[N,,]-xt) 
on R T , choose 7//(x/)-fme divisions 
pi" = pi" {x>) = pi;;(i/) and P
2" = P
2" (x>) = u?/,^ 
so that, for each V = V, we can form iterated Riemann sums from 
V1 = V x ( U p l V ) ) 
and 
V2=V'x 
(\JV?\x')) 
· 
Then, for each (^/'[iV7]) 6 X>', the components if(I' x J") can be added up, 
giving Η(Γ x R T ) by additivity of H. Likewise when V2 is considered. Having 
done that, we will consider the difference of the resulting expressions, to show 
that, for each #', the Riemann sums in R T 
converge, in the Cauchy sense, to 
the integral a(xf) = / R T„ h"(x",N" J"[N"};x'). 
The details of the argument 
are as follows. 
e 
> 
(V^ZWh" 
- H(I)\ 
= 
(V x VY'{x')) Σ \h'h" - Η(Γ x I")\ 
= (V) Σ {p 1 V ) ) Σ Wh" - H(i' x i")\} 
> {V) Σ {I (P
1" (*')) Σ (h'h" - H(I' x /")) I} 
= <P) Σ {IP
1" (*')) Σ h'h" - (P
1" 00) Σ # U' x n I} 
= 
(T>')E{\(Vl"(x'))Eh'h" 
- H(I> x*RT")\} 
. 
The first inequality is from Theorem 19 (which is a consequence of our ass-
umption of integrability of h = h!h" in R T, combined with Henstock's lemma 
(Theorem 18)). The second inequality, 
(V) £ (I^V)) Σ (h'h" - wx 
7"))|} <
ε> 
(4·7) 
follows from the triangle inequality. The same argument gives 
(V')^2{\(V2"(x'))J2h'h" -H(I' xRT")\} <ε. 

162 
CHAPTER 4. THEORY OF THE INTEGRAL 
These two inequalities imply that 
(V) Σ \h'(x>, N', I')11(D1"(a:')) Σ h"{x') - (V2"(x>)) Σ h"(x')\ 
= (V')E\h'(x',N',I') 
({V^{x'))Y,h"{x') 
- {V*'{x>))Y,h"{x>))\ 
= (V) Σ {| (V1" (*')) Σ Λ'Λ" - (2>2" (a:')) Σ h'h"|} 
= 
( 2 y ) E { | ( p 1 " ( a ; , ) ) E ^ " - Ä ( / , x R T " ) ) 
- ((V2"(x'))J2h'h" 
- Η(Γ xR T"))|} 
< 
( ^ ) Σ { | ( ( © 1 " ( χ , ) ) Σ Λ ' Λ " - ^ ( / , χ κ Γ " ) ) | } 
+ 
( ^ ) E { | ( ( ^ V ) ) E M " - # ( / ' X R T " ) ) | } 
< 2ε. 
Summarizing, this says that 
(V')^\h'\ I(2?1"(*'))£>"(*') - (^"(^E^'V) < 2ε, 
(4.8) 
and, with a view to forming an inequality for Cauchy convergence, we now try 
to deduce that the difference 
(V'"(x'))Y^h"(x') 
- 
{ν*\χ>))Σΐι"{χ') 
is "small", giving integrability of h" (except perhaps in some Zi'-null set). The 
outer measure property (Theorem 30) of the variation function Vw will accomp-
lish this. Let Sf
v be the set of x' G RT such that, for all 7 and all 7",, there 
exist Vl"{x') and V2" (χ') with 
\{&'\Χ>))ΣΗ"{Χ') - (2^V))E*'V) 
Then, for all 7,-fine £>', 
> 77 > 0. 
2ε 
(Vf)J2W(x',N',I')\Vls,(xf) 
<2ε, and (Ρ') £ | / ^ A T , J')|ls;(x') < A 
so 
2ε 
2ε 
2ε 
sup(P1)X;|Ä'(x',iV',/')|l5{,(x/) < A 
K U ^ ) < Α 
Vv(S;) < -J 
ν 
— 
' 
η 
η 
η 
Taking η — y/ε we get 
Vhr ( 5 ^ ) < 2>/£. 
Replacing ε by ε4~·7, j = 0,1,2,..., this implies that the integral 
a(x')~ 
[ 
ti,(x"1N",r,[N");x') 

4.11. FUBINFS 
THEOREM 
163 
exists except in a set 
oo 
*0' = U fe 
j=0 
with 
oo 
oo 
Vh,(S0')<^Vh/(s'^J)=2Y^V^J^4V~e. 
j=0 
j=0 
Since this holds for all ε > 0 we have Vw (S° ) = 0; so 
\{νι"{χ'))Σΐι"{χ') 
- (V2"(x'))^2h"(x')\ 
<v = V~e 
(4.9) 
except for x' G 5° . That is, 
a(xf) = f 
h"{x') 
JUT" 
exists /i'-almost everywhere. This completes the proof of the first part 
of the theorem. Now define a(xf) to be zero for x' G 5° . For j = 1,2,3,..., 
take ε = j ~
l and denote the corresponding 7 by 7^. Then, for x' G R T \ S° , 
and writing 
&(*') = ( ^ ( r r ' ) ) ^ ^ ^ " , ^ " , / " ^ " ] ; ^ ) , 
■ choose 7" = 7j(xf) to satisfy 7J/
+1(x/) -< 7"(#'); and 
■ choose 7^(x')-fine divisions V"(x') so that 
\βό{χ') - a{x')\ < 2 ^ 7 ^ - 2y/i. 
(4.10) 
Let 5)' be the set7 of x' G R T' \ S0' for which 
\ßj+1(x,)-ßj(xf)\>0. 
Let 
00 
As 7J/
+1(x/) -< η"(χ'), inequality (4.8) above now implies that 
(Ρ') J ] |Λ'|151, (ζ') \ßj+1(x>) - ßj(x')\ < 
2Γ\ 
SO 
Vh,l0j+1_ß3](S1')<2j 
- 1 
Leaving aside certain special cases, such as when h" is a constant function of x", Riemann 
sums ßj(x') 
can often be arranged to yield different values for j — 1,2,3,.... 
Therefore 
R T ' \ (S 1' U 5 0 will often be empty. 

164 
CHAPTER 4. THEORY OF THE 
INTEGRAL 
Therefore, by Theorem 47, ti is VBG* in S)' for j = 1,2,3,.... Since a count-
able family of countable families is itself countable, 
tilsl*(x') 
is VBG*; that is, ti is VBG* in S1'. 
(4.11) 
Inequality (4.7) gives an inequality of the form 
(V) Σ 
\iP'\x')) 
Σ 
fcViN''Ofc'V, 
N",I"\x') 
~ H(I' x / " ) | < e 
for selected 7//(x/)-fme divisions V (xf) and all 7'-fine divisions V. The proof 
of the final part of the theorem consists of replacing h"(x') by a(xf) in this 
inequality, enabling us to deduce integrability of a{x')h" from Theorem 19. Let 
S 2 ' = R r , \ ( 5 ° ' u S 1 ' ) . 
For p = 0,1,2, write 
hp(x,N,I[N}) 
= 
h'(x',N',I'[N'})h"(x",N",I"[N"})lSp/(x'), 
so 
2 
h(x,N,I[N}) 
= 
ΣΗΡ(Χ,Ν,Ι[Ν}). 
The function a(xf) has been defined to be zero for each x' in the Zi'-null set 5° . 
For x' e S2 we have 
(V"(x') Σ h"{x", N", I'[N"];x') = a(x') 
for each V"{x'). 
Write Η'1(χ',Ν',Γ[Ν']) 
= Η'(χ',Ν',Γ[Ν'])131,(χ'), 
and note 
that, by (4.11), h[ is VBG*. Then, by Theorem 47, there is a positive function 
f(x') for which Vfh,{S1') < 1, and a gauge j[ for which V ^ S 1 ' ) < 1. Write 
ti{(x",N",I"[N"};x') 
= 
h"(x",N",I"{N"};x')lsl,(x'). 
For each x' € S 1 choose r = r(x'), 
so that for every 7"(a;')-fine division 
Vr(x')(x') 
°^ -^-T * n e corresponding Riemann sum 
ßr(x') = W^)(x>))J£ih'{(x>',N"J"[N''];x') 
satisfies 
(since we have taken ε = J _ 1). 
Using diagonalization axiom DS7, define 
η"(χ') = 7;/(x/) for each α' G S1'. Then choose 7"(ic/)-fine divisions V"{x') 
as follows: 
^ ( ζ ' ) 
for 
xeS1', 
V"(x') = { arbitrary 
for 
x G S0', 
V'{(x') 
for 
xeS2'. 

4.12. LIMITS OF 
INTEGRALS 
165 
Now choose 7' in accordance with axiom DS8 (Theorem 53), and choose 
f 
-< 71Λ7'· 
Then, for any -γ'-ίίηβ division V of R T , (4.7) becomes 
(V) Σ 
\a(x') {{V"{x')) Σ h"(x", N», I"[N"}) ~ H(I' x /")} < 2ε, 
and the second part of the theorem follows by Theorem 19. 
O 
Theorem 55 Using the notation of the Theorem 54, if h is integrable on R T 
thenh'(x',N'J[N'])lsl<(x') 
is 
VBG*inRr. 
Proof. This is a corollary of the preceding theorem; see (4.11). 
O 
The above proof of Fubini's theorem fills several pages. In contrast, the 
Lebesgue integral version of the proof is quite short and simple. But the latter 
assumes measur ability of the integrand, while the former does not. If measur-
ability of the integrand h is assumed, then the proof for Henstock integrals is 
equally short and simple. But the assumption of measur ability of h conceals 
the full meaning of Fubini's theorem. 
4.12 
Limits of Integrals 
This section deals with integrability of the limit function of a convergent seq-
uence of integrable functions—"taking limits under the integral sign". 
Suppose functions hj are integrable on E to Hj(E) for each j , and suppose 
that, for each 
(x,N,I[N]), 
hj{x, N, I[N]) -> h(x, AT, I[N]) as j -> oo. 
Several questions arise from this: 
1. Under what conditions will the sequence Hj(E) = fEhj(x,N,I[N]) 
con-
verge? 
2. Under what conditions8 will the limit function /ι(χ, Ν, I[N]) be integrable? 
3. Under what conditions will JE h(x, iV, I[N]) be equal to lim^oo Hj(E)? 
Separate conditions can be given that ensure an affirmative answer to each of 
these questions in turn. Or, a single condition can be given that ensures that all 
three questions receive an affirmative response. Such a condition is the existence 
of an integrable function g such that 
\Ηά(χ,Ν,Ι[Ν])\<9(χ,Ν,Ι[Ν]) 
for all j . 
8The assumption of measurability of the integrands in the standard Lebesgue integration 
theory amounts to an assumption of integrability, except in the case of divergence to ±oo. 

166 
CHAPTER 4. THEORY OF THE 
INTEGRAL 
This is the dominated convergence theorem. 
As in the overview of the Pubini-Tonelli theorems in the previous section, 
the analogy of limits of double sequences provides insights into these theorems 
for convergence of sequences of integrals and integrability of limit functions. 
The assumptions in these theorems are: 
■ convergence of functions hj to a function h, and 
■ integrability of each hj. 
Thus, for each hj there exists a sequence of divisions V1 such that 
α^:=(νή^2^(χ,Ν,Ι[Ν}) 
converges to 
bj := / hj 
as i —> oo. Furthermore, for each z, 
d := lim (P<) Thiix,N,I[N}) 
= (P<) 
Vh(x,N,I[N}) 
exists. The integrability of the limit function h is suggested by (but is not quite 
the same as) the existence of a limit 
c = lim Ci. 
i—»·οο 
The convergence of the integrals J hj to J h is then expressed by 
lim bj — c. 
In contrast to Fubini's theorem, here we are assuming the existence of limits of 
single sequences and, under conditions to be determined, deducing a possible 
limit of a related double sequence. 
Some of the issues involved in this topic are illustrated by the following 
examples. For x G [0,1] define 
Example 39 Define fj (0) = 0 for all j and 
J'W-\ 
0 
if 
)<x<\. 
The sequence fj(x) converges to a function integrable with respect to \I\ in [0,1] 
(or on ]0,1]), but the sequence of integrals L· χ, fj(x)dx 
does not converge. 
Example 40 Define fj (0) = 0 for all j and 
( j 
if 0<x 
< i, 
A ' 
\ 0 if 
)<x<\. 
Then ffQ1]fj(x)dx 
= 1 for each j , fj(x) 
—> f(x) 
= 0 for all x G [0,1] but 
ifo li fjix)dx 
does not converge to L· ^ f(x)dx 
as j —> CXD. 

4.12. LIMITS OF 
INTEGRALS 
167 
Example 41 
f 0 
if 
0 < ζ < 2 - ' ' , 
3 
\ k2k 
if 2~k < x < 2 " f c + 1 , 0 < Ä ; < j . 
Each fj is integrable, and fj(x) converges to a function f(x) for each x, but f 
is not integrable, and the sequence of integrals of fj is not convergent. 
Example 42 
0 
if 
0 < x < 2 - ^ , 
2i-i -2·?' 
if 2~i 
<x<2~i+\ 
{ 0 
if 2~i+1 
<x<l. 
fj(x) = \ 
The sequence fj converges to an integrable function; each fj is integrable and 
the sequence of integrals converges; but the limit of the integrals is not equal to 
the integral of the limit. 
O 
Conditions such as uniform convergence of fj(x), or dominated convergence (i.e., 
existence of an integrable function g(x) such that \fj(x)\ < g(x) for all x and 
all j) are sufficient for convergence of the sequence of integrals of fj to a limit 
equal to the integral of /; and it is evident by inspection how such conditions 
could play a role in the examples above. 
Henstock integral versions of the familiar uniform and dominated conver-
gence theorems are presented in this section. The next section presents Rie-
mann sum criteria for existence and equality of the limits involved. These are 
particularly important in problems involving non-absolute integrability of limits 
of functions which are non-absolutely integrable. 
We will be dealing, not with bounded domains such as the ]0,1] of the 
examples above, but with unbounded product domains R T. 
As usual, it is 
convenient to present the integrands in Burkill-complete form 
h(x,N,I[N]), 
rather than 
f(x)\I\. 
When dealing with convergence of fj(x) to /(#), we form an inequality 
\fj(x) - f(x)\ < ε, 
equivalent to 
\fj(x)\I\ - f(x)\I\\ < ε\Ι\. 
The latter formulation of the inequality involves the integrator function |/|. 
This function is integrable on bounded domains ]0,1] or ]a, 6], but for integrals 
in an infinite domain such as R the function |7| is not sufficient. For unbounded 
one-dimensional domains the convergence inequality requires something extra, 
on the lines of a positive function go, integrable on R, such as go(x, I) = e~x \I\. 
Then the convergence inequality appears as 
|/,.(z)|I| - /(x)|J|| < e~x'\I\e 
or 
\fj(x) - f(x)\ < 
e'^e. 
Correspondingly, in R T an appropriate formulation for the convergence 
hj(x,N,I[N]) 
-> 
h(x,N,I[N}) 

168 
CHAPTER 4. THEORY OF THE 
INTEGRAL 
is 
\hj(x, N,I[N}) - h{x, TV, I[N})\ < g0(x, N,Ι[Ν])ε 
(4.12) 
where go(x, iV, I[N]) > 0 is integrable on R T. (In fact, it will be seen from the 
proofs that the requirement that go be integrable can be relaxed. It is generally 
sufficient that go have bounded Riemann sums.) 
The limit theorems are for integrals on R T, but the proofs are valid for 
integrals on any figure E in R T. With small modifications they are also valid 
for any Henstock integral division system for which the relevant division system 
axioms are satisfied. 
The first convergence theorem deals with uniform convergence. 
Theorem 56 Suppose the real- or complex-valued hj is integrable on R T for 
each j = 1,2,3,..., and suppose that, for each associated (x, N, I[N]), 
hj(x,N,I[N])->h(x,N,I[N]) 
as j -> oo. 
Suppose there exists a positive function go, integrable on R T, and a gauge 70 so 
that, with ε > 0 given, there is a positive integer jo for which 
\hj(x,N,I[N})-h(x,N,I[N])\<ego(x,N,I[N}) 
for every j > jo and every 70-/me (x,N,I[N]). 
Then h is integrable on R T, 
and 
lim / 
hj = I 
h. 
i-*°° JnT 
JnT 
Proof. Let ε > 0 be given. Choose j > jo. Let Hj and Go denote, respectively, 
/ 
hj 
and 
/ 
g0. 
Choose 7 -< 70 so that for each 7-fine division Vg of R T, 
Ι ( ^ Σ ^ · - ί ^ · | < ε 
and 
\{Vg ^9o 
~ G0\ < e. 
Then, for any 7-fine divisions Τ>Ί and Ό'Ί, 
| ( 2 ? 7 ) Σ Λ - ( Ρ ; ) Σ Λ | < (^ΣΙΛ-Λ,-Ι + Κ^ΣΛί-^-Ι 
+ Ι^·-(^;)Σ^Ί + (^)ΣΙ^-^Ι 
< 
e(2 + 2G0 + 2s) < 
Αχε 
where A\ is a fixed positive number. Thus h is integrable, and 
\H-Hj\ 
< IH-iVJZhl + iVJZlh-hjl 
+ lCDJZhj-Hjl 
< 
e{A1 + (Vy)Y,g0 
+ l) 
< 
Α2ε 
where A? is a fixed positive number. Therefore Hj —► H as j —> CXD. 
O 
The next theorem is known as Levi's monotone convergence theorem. 

4.12. LIMITS OF 
INTEGRALS 
169 
Theorem 57 Suppose the real-valued functions hj satisfy 
h!(x,N,I[N]) 
< h2(x,N,I[N]) 
< h3(x,NJ[N}) 
< ... 
and suppose the functions are bounded above with supremum h(x,N,I[N]) 
for 
each associated (x,N,I[N]). 
Suppose each hj is integrable on R T with integral 
Hj, and suppose the numbers Hj are bounded above with supremum H. Suppose 
go(x,N,I[N]) 
> 0 is integrable on R T with integral Go, and suppose there exist 
7o and, for each x, an integer jo = jo(x) > 0 so that 
h(x, N, I[N}) - hjix, N, I[N}) < eg0(x, N, I[N]) 
for each j > jo(x) and each ^fo-fine (x,N,I[N]). 
Then h is integrable and 
h = H. 
j 
Proof. For any figure E C R T, let Hj(E) denote fEhj. 
It can be assumed 
without loss of generality that hj > 0. Let ε > 0 be given, with ε < Go- Choose 
r so that 
Η-ε<ΗΓ<Η. 
For j = 1,2,3,..., there exist jj = (Lj,Sj) so that each 7j-fine V10 satisfies 
(^Σ^·-^ <ε2~Κ 
Then, for any figure E, with 7^-fine division £Ί , 
|(^)E^-^l<£2_j+1· 
(4.13) 
For each x G R T choose 70-fine (x,M,J[M\). 
Let r(x) be the smallest integer 
> max{r, jo(x)} satisfying 
hr(x)(x,M,J[M}) 
> 
h(x,M,J[M])-ego(x,M,J[M}). 
Then, for each 70-fine (x,NJ[N]) 
with I[N] C J[M], 
hr{x)(x,NJ[N]) 
> 
h(x,N,J[N])-£g0(x,N,J[N}). 
For j = 1,2,3,..., let Sj = {x : r(x) = j}. Define a gauge 7 = (L, δ) to satisfy 
the following conditions: 
■ 7 ~< 7o and 7 conforms9 to J[M]; 
■ if x e Sj then L(x) D Lj(x) and δ(χ) < Sj(x); 
' (^7) Σ 9o > G0 - e for all 7-fine ΌΊ. 
9That is, (x,N,I[N}) 
is 7-fine implies (x,N,I[N]) 
is 70-fine and I[N] C J[M]). 

170 
CHAPTER 4. THEORY OF THE 
INTEGRAL 
Thus, if (x,NJ[N]) 
is 7-fine and x G Sj, then (x,N,I[N]) 
is 7rfine. Let Τ>Ί 
be a 7-fine division of R T, and let p and q be the smallest and largest values of 
r(x) for (xJ[N]) 
G 2>7. Then 
Η-ε<Ηρ 
= (2> 7)£ff p(J) < (νΊ)ΣΗτ{χ){Ι) 
< (2> 7)£ff f f(J) < H.m 
For p < & < g, let 
£7
fc 
:= 
{(x,I[N\):(x,I[N\)eVy,.r(x) 
= k}, 
Ek 
:= \J{I[N]:(x,I[N])eV^,r(x) 
= k}. 
Then, using (4.13), 
(£*) 53fffc(£7fc) - - ^ - < (£*) 5 3 Λ*(χ,TV,/[TV]) < (£*) 53Hk(Ek) + 
^ 
and 
(£7
fc)53//p(£fc) - ^ τ < (4)53^(χ,τν,/[ΛΓ]) < (£7
fc)53/fg(£fc) + JL·. 
Thus, by grouping together the (x, JV, J[iV]) having the same values of r(x), and 
using the finite unions 
we get 
# - 2ε < (£>7) ] Γ ΛΓ(ίΒ)(x, TV, I[N]) < H + ε, 
so 
i f - 2 ε 
< 
(Ρ7)Σ/ι(χ,]ν,/[ΑΓ]) 
< 
( ^ 7 ) Σ ( ^ ( χ ) ( χ , ^ ^ ] ) + ε ^ 0 ( ^ ^ / [ ^ ] ) 
< 
# + ε + ε(<3 0-ε). 
Thus h is integrable and J R T h = H. 
O 
The next result deals with the integrability of the minimum function and 
maximum function of a finite number of integrable functions. 
Theorem 58 // the function hj is real-valued and integrable on R T, with int-
egral H, for 1 < j < r, and if there are real numbers c\, c<i (c\ < c<i) and a 
gauge 70 50 that 
c\ < (Vl0)"^2hj(x,N,i[N])(x,N,I[N}) 
< c2 
for all choices j(x,I[N]) 
of j , 1 < j < r, then the functions 
hi{x,NJ[N]) 
:= 
πιίη{/ι,·(χ,7ν,/[ΛΠ):1<^<Γ}, 
hu(x,N,I[N\) 
:= 
m^{hj{x,N,I[N]) 
: 1 < j < r} 
are integrable. 

4.12. LIMITS OF 
INTEGRALS 
171 
Proof. (νΊ)Σ^(χΑΝ]){χ,Ν,Ι[Ν]) 
< c2 implies (ΏΊ)Σ^(Χ,Ν,Ι[Ν]) 
< c2. 
Take r = 2. For any figure E, JE hj = Hj(E) exists. Then, for j = 1,2, 
hj(x, N, I[N}) - max{H^I), H2(I)} < \hj(xt N, I[N}) - 
Ηό{1)\, 
< 
\hux,N,I[N\)-Hx{I)\ 
+ 
\h2(x,N,I[N})-H2(I)\, 
so 
max{h1(x,N,I[N]),h2(x,N,I[N])}-max{Hi(I),H2(I)} 
< ^(χ,Ν,ΙΙΝϋ-Η^ηΐ 
+ 
ΙΗ^χ,Ν,ΙΐΝΰ-Η^ηΐ. 
Using Theorem 18, choose 71 -< 70 so that 
K ^ i ) Σ (max {Λι(χ, N, I[N\), h2(x, N, I[N})} - m a x { ^ ( J ) , ff2(J)})| 
< 
( Ρ 7 Ι ) Σ ( | Λ Ι ( Ϊ , ^ , / [ Λ Π ) - Η ! ( / ) | + |/ι2(ζ,ΛΓ,/[ΛΠ) - # 2 ( Ι ) | ) 
< 
8ε. 
Thus (P 7 l) ^2iaax.{Hi(I),H2(I)} 
is bounded above for 71-fine divisions T>7l. 
Let 
s = sup{(2? 7 l)^max{ii 1(J),ii 2(/)}}. 
Let P be a division of R T such that 
s-ε 
< ( P ) ^ m a x { # i ( J ) , # 2 ( J ) } < 5. 
(4.14) 
The objective is to combine this inequality with the previous one to give the 
result. If (x, I[N]) £ V and S = {(#, J)} is a division of J[iV], then 
(£)£max{ffi(J), ff2(J)} > ( 5 ) ^ ^ ( J ) = #,(/), j = 1,2, 
so 
(£) Σ 
m a x {ffi (J)> ^2(J)} > max {Hx (I), H2(I)} . 
Choose 72 -< 7i so that each 72-fine division D72 of R T is a sub-division of V; 
that is, if (j, J) eV and (x, J) £ X>72 with / D J not empty, then I C J. Then, 
combining inequality (4.14) with the inequality which preceded it, we find, for 
all such Vl2, 
5 - 9 ε < (Vl2)Y^m&yL{hl(x,NJ[N}),h2{x,NJ[N\)}<s 
+ %e. 
This gives the required result for r = 2, and the result for any r follows by 
induction. 
O 
The next theorem will be used to prove a version of Lebesgue's dominated 
convergence theorem. 

172 
CHAPTER 4. THEORY OF THE 
INTEGRAL 
Theorem 59 // the real-valued functions h\,h2, · · ·>hr are integrable and if 
there exist integrable functions g\ and g2 and a gauge 70 so that 
9l{x,NJ[N\) 
< 
hj(x,N,I[N})<g2(x,NJ{N}) 
for each go-fine (x, N, I[N]), j = 1,2,..., r, then the functions 
hi(x,N,I[N]) 
= 
min{hj(x,N,I[N]) 
: 1 < j < r}, 
hu(x,N,I[N}) 
= 
max{hj(x,N,I[N]) 
: 1 < j < r} 
are integrable. 
Proof. Replace hj and #2 by hj = hj — g2 and 0, and then 
(Ί>Ίο)ΣΗ'ί{χΆΝ])(χ,Ν,Ι[Ν])<0 
for all choices j(x,I[N]) 
of j , 1 < j < r. 
Thus, by the previous result, 
max{hj(x,N,I[N]) 
: 1 < j < r} is integrable, so 
max{hj(x,NJ[N]) 
:l<j<r} 
= max{^(x,Ar,/[AT]) : 1 < j < r} - 
g2(x,N,I[N}) 
is integrable. Similarly for the min function. 
O 
Theorem 57 (Levi's monotone convergence theorem) in conjunction with 
Theorem 58 provide the means of proving the following result. 
Theorem 60 Suppose hj is real and integrable on R T for j = 1,2,3,..., and 
hj{x,N,I[N}) 
-> 
h(x,N,I[N\) 
for each (x,N,I[N]). 
Suppose go > 0 is integrable, and, given ε > 0, suppose 
there exist 70 and jo — jo(#) depending on x so that jf > j > jo(x) implies 
\hr(x,N,I[N}) 
- hj(x,N,I[N})\ 
< 
ε90(χ,Ν,Ι[Ν}) 
for all jo-fine (x,N,I[N]). 
Suppose there exist real numbers c\,c2 and a gauge 
71 so that, for all 2>7l, 
ci < (νΊι)^2^{χΑΝ])(χ,Ν,Ι[Ν}) 
< c2 
for all choices j(x,I[N]) 
of j corresponding to (x,I[N]) 
G P 7 l . 
Then h is 
integrable, and the sequence Hj = J R T hj converges to a limit H as j —> oc, and 
H = 
fRTh. 
Proof. Choose 7 -< 70 Λ 71. Let 
krs(x,N,I[N]) 
:= min {hj(x,N,([N]) 
: r < j < r + s} . 

4.12. LIMITS OF 
INTEGRALS 
173 
By Theorem 58, krs is integrable. If r > jo, P > s, and (#, iV,/[AT]) is 7-fine, 
then 
krs(x,N,I[N\) 
- krp(x,N,I[N}) 
< 
eg0(x,N,I[N]). 
As s —> 00, krs is monotone decreasing to a function kr(x,N,I[N]), 
so the 
condition c\ < (ΡΊ)Σ^(χ,ΐ[Ν])(%ιΝ,Ι[Ν]), 
with Theorem 57, implies kr is 
integrable and 
/ 
kr(x,N,I[N]) 
= lim / 
krs{x,N,I[N}). 
Since kra{x, A", I[N\) < hj(x, N, I[N]) for r < j < r + s, we have 
lim / 
krs < 
hj, 
/ 
Av < inf / 
hj, 
/ 
/cr < liminf / 
hj, 
S->°°JTIT 
JnT 
JRT 
J>rJnT 
JnT 
i->°° JnT 
and the latter is finite because of the condition 
(P7)^/ij(x,/[iV])(x,7V,/[7V]) < c2. 
If q > r > jo and (x, A", I[N]) is 7-fine, then 
kqs(x,N,I[N})-krs(x,N,I[N}) 
< 
ε9ο(χ,Ν,Ι[Ν]), 
kq(x,N,I[N})-kr(x,N,I[N}) 
< 
eg0(x,N,I[N]). 
As r —>· oc, Αν(#, A", /[A]) is monotone increasing to a function fc(x, A",/[AT]), 
and Theorem 57 implies k is integrable, with 
Since 
/ 
k = lim / 
kr. 
k = lim kr = liminf hj = lim hj = h, 
r—>oo 
j—>oo 
j—>·οο 
the latter is integrable and 
/ 
h = 
lim /cr = lim / 
kr < lim inf / 
hj. 
Similarly, 
/ 
h> lim sup / 
hj. 
JnT 
j-+00 JnT 
Therefore limbec J R T hj exists and equals J R T h. 
O 
The next result is the dominated convergence theorem. 
Theorem 61 Suppose hj is real and integrable on R T for j = 1, 2,3,..., and 
hj(x,N,I[N])^h(x,NJ[N)) 

174 
CHAPTER 4. THEORY OF THE INTEGRAL 
for each (x,N,I[N}). 
Suppose go > 0 is integrable, and, given ε > 0, suppose 
there exist jo and jo = 3ο{χ) so that f > j > jo(x) implies 
\hr(x,N,I[N}) 
- hj(x,N,I[N})\ 
< 
ε9ο(χ,Ν,Ι[Ν}) 
for all jo-fine (x,N,I[N]). 
Suppose there exist integrable functions g\ and g2, 
and a gauge 71 so that, for all Ί)Ίι, 
9l(x,N,I[N]) 
< 
hd(x,NJ[N])<g2(x,N,I[N]) 
for all 71-/me (x, N, I[N]). Then h is integrable, and the sequence Hj = J R T hj 
converges to a limit H as j —> 00, and H = J R T h. 
Proof. This follows from Theorem 60. 
O 
4.13 
Limits of Non-Absolute Integrals 
While the theorems above are given for sequences of integrable functions, they 
can easily be expressed in terms of families of functions which depend on a real-
valued parameter taking uncountably many values. Also, Cauchy conditions for 
existence of limits can be substituted. 
Similar considerations apply to the following theorems, which are crucially 
important in dealing with sequences or families of real- or complex-valued func-
tions that are integrable but not absolutely integrable. It is assumed that a 
sequence of integrable, real- or complex-valued functions hj converges to h in 
the same sense as above. That is, there exists a positive-valued, integrable func-
tion go(x,N,I[N]) 
so that, given ε > 0, for each associated (x, iV, I[N]) there 
is a gauge 70 and an integer jo = jo(x, N, I[N\) depending on (x, TV, I[N\) such 
that 
\hj(x, N, I[N}) - h(x, TV, I[N})\ < eg0(c, N,/[TV]) 
(4.15) 
for every j > jo and for every 70-fine (x, N, I[N}). 
As previously mentioned, it will be seen from the proofs that the requirement 
that #0 be integrable can be relaxed. It is sufficient that the Riemann sums 
(ΡΊΟ) Σ # Ο Ο , N, I[N]) be bounded for all £>7o. 
Where the previous theorems used fixed, real, bounding constants c\ and 
C2, equivalent to a ball B of fixed radius in R, the following results use balls of 
arbitrarily small radius in R or C. Let B(a, b) denote a ball with center a and 
radius 6, and let B\ + Β<χ denote 
{ci + c 2 : ci G J5i, c2 G B2} . 
The next result gives a criterion for integrability of the limit function h. 
Theorem 62 Suppose the integrable hj(x,N,I) 
converges to h(x,N,I) 
in R T 
in the sense of (4-15). Then h is integrable if and only there exist a ball B\ 

4.13. LIMITS OF NON-ABSOLUTE 
INTEGRALS 
175 
of arbitrarily small radius and, correspondingly, a gauge 7 and integers p — 
p(x,N,I[N]) 
depending on B\ so that 
(νΊ)Υ^ΗΜΧίΝΑΝ]){χ,Ν^[Ν]) 
G Si 
for all choices of m = m(x,N,I[N]) 
> p(x,N,I[N]) 
in the Riemann sum, and 
for all j-fine T>7. 
Proof. 
If h is integrable, with J R T h = H, then let ε > 0 be given, take 
7 = 70, take p(x,N,I[N]) 
= jo(x,N,I[N]), 
and let a be an upper bound for 
(νΊο)Σ9ο(χ,Ν,Ι[Ν}) 
for all Z>70, so that 
{V10)^2hm{XiNJlN]){x,NJ[N]) 
G Β{Η,αε). 
Conversely, if there exist B\, 7, and p(x, N, I[N] so that 
{Vy)^hmiXiNJlN])(x,NJ[N]) 
G Bu 
then, with a as before, take 7' -< 7o Λ 7 and take 
m(x,7V,/[AT]) > max{p(x,7V,/[AT]), 
j0(x,N,I[N})} 
for each 7/-fine (x, TV, I[iV]). Then, for each Dy, 
(P70^/i(x,7V,/[AT]) G Si + Β(Ο,αε), 
giving the integrability of h. 
O 
The following partial version of Theorem 62 is often useful. 
Theorem 63 Suppose, for x G R T and N G Λί(Τ), the real- or complex-valued 
fj(x,N) 
—l· f(x,N) 
as j —> 00. Suppose the real- or complex-valued 
h(x,N,I) 
is VBG*. If each fj(x,N)h(x,N,I) 
is integrable on R T then 
f(x,N)h(x,N,I) 
is integrable on R T. 
Proof. Let ε > 0 be given. There exist a gauge 7 and, for r = 1,2,3,..., 
constants ar > 0, and sets Sr C R T whose union is R T, so that, for each 7-fine 
division Τ>Ί of R T, 
( P 7 ) ^ | / i ( x , A T , / ) | l 5 r ( x ) < a r , 
r = 1,2,3,.... 
(4.16) 
For x e Sr and TV G Λί(Τ), choose j£ = je(x,N) 
so that, for j = j(x,N) 
> 
Je(x,N), 
\fj{x,N)(x,N)-f(x,N)\<ea;12-r. 
(4.17) 
For each x, N choose j = j(x, N) > je(x, TV), j ' = j'(x, N) > je(x, TV), and £>7. 
Now consider the difference 
d = <Pt) Σ Μ*,Ν)(χ> N)h(x> 
N>7) - <Ρι) Σ fr(^N)(x, 
N)h(x, N, I). (4.18) 

176 
CHAPTER 4. THEORY OF THE 
INTEGRAL 
This can be expressed as 
(P-r) Σ (fn*,N)(x,N) 
- f(x, N)) h(x, N, I) 
~ P-y) Σ (fj'{x,N){x, N) - f(x, N)) (x, N)h(x, N, I), 
= a — 6, say. 
We have 
|o| 
< 
(V^)J2\fj{x,N)(x,N)-f(x,N)\\h(x,N,I)\ 
oo 
= Σ (ΡΜΣ 1*·^) \fj(X,N)(^N)-f(^N)\ 
\Ηχ,Ν,η\) 
r=l 
oo 
< 
Y^ea-l2-rar 
= e. 
(4.19) 
r = l 
Similarly |6| < e. Therefore \d\ < 2ε, and the result follows by Theorem 62. O 
Note that Theorem 63 is valid if the condition "ft is VBG*" is replaced by the 
condition "ft is R T XjV(T)-VBG*". To see this, make the following amendments 
to the proof: 
- In (4.16), use (2>7) Σ \h(x,N,I)\ler(x,N) 
< ar. 
- In (4.17), use \fj(x,N)(x,N) 
- f(x,N)\ 
< ε α " ^ . 
■ In (4.19), use l@r(x,N) 
instead of 
lsr(x). 
The next result is a criterion for convergence of J R T hj as j —> oo. 
Theorem 64 Suppose the integrable hj(x,N,I) 
converges to h(x,N,I) 
in R T 
in the sense of (4-15). Then lim^oo J R T hj exists if and only if there exist a 
ball JE?2 of arbitrarily small radius, a corresponding integer jo = jo{B2), and a 
gauge 7j depending on j so that 
PJj)J2hj(x,N,I[N})eB2 
for each j > jo and for each η$ -fine Vj. 
Proof. 
If linij^oo J R T ftj, = H', exists, then there exists jo so that j > jo 
implies 
/ 
hjtB{H\e). 
In other words, there exists 7j so that, for all P 7 j, 
(V7j)J2hj(x,N,I[N}) 
e 
Β(Η',2ε). 
Conversely, suppose there exist JE?2, jo? and 7^, with j > j 0 , so that 

4.13. LIMITS OF NON-ABSOLUTE 
INTEGRALS 
111 
Since each hj is integrable there exist gauges 7J so that, for all P y , 
|(^·)Σ^·- / h\<e-
I 
3 
JnT 
I 
Now take 7" -< jj A 7J and consider 7^-fine divisions. Then, for any j > jo, 
I 
hj e £ 2 + £(0,e), 
so linij^oo J R T /ij exists. 
O 
The following refers to B\ and B2 of Theorems 62 and 64 and gives a criterion 
for equality of J R T h and lim^oo J R T /ij whenever both of them exist. 
Theorem 65 Suppose the integrable hj{x,N,I) 
converges to h(x,N,I) 
in R T 
in the sense of (4-15). Given the existence of J R T h and lim^oo J R T hj, the 
two are equal if and only if 
ΒχΓ\Β2φ 
0. 
Proof. Let ε be an arbitrary positive number. If both 
H= 
[h 
and 
H' = lim / 
hj 
J 
0-^00 
JKT 
exist, then H = Hf if and only if 
Bi = B{H, as) 
and B2 = B(H', 2ε) 
have non-empty intersection. 
O · 
To illustrate the above criteria, consider the following examples in R. 
Example 43 If hn(x) = 1[0,η](χ), then h(x) = limn-^ hn(x) = 1 for x e R. 
The function hn(x)\I\ is integrable on R for each n, with 
/ hn(x)\I\= 
/ 
l[o,n](x)dz = n, 
£mi ft(x)|/| ^5 noi integrable (f£° dx does not exist), and fRhn(x)\I\ 
does not 
converge as n —> 00. Regarding the first criterion above, it is clear that there 
do not exist a gauge δ(χ) or integers p(x) for which integers m(x) > p(x) imply 
that 
(^)Σ>™(χ)(*)ΐ'ΐ = Σ l[0,m(x)](x)\I\ 
belong to an interval of arbitrarily small length, since, for any choice of δ(χ) 
there will be divisions £«$ for which, no matter how large we make the integers 
p(x), the points x in the Riemann sum may occur in [0,m(x)] or in ]ra(#),oo[. 
Regarding the second of the criteria, the Riemann sums 
(εδη)ΣΗη(χ)\Ι\ = Σΐ[0,η](χ)\Ι\ 

178 
CHAPTER 4. THEORY OF THE 
INTEGRAL 
will fail to be contained in any interval of arbitrarily small length, no matter 
what values are chosen for gauges δη. 
O 
Example 44 If hn(x) = nl]o)n-i[(x), then h(x) = limn^oo hn(x) = 0 for x G 
[0, oo[. The function hn{x)\I\ is integrable on R for each n, with 
/ hn(x)\I\ = / 
nl]o,n-i[(a;)ac = 1; 
Jn 
Jo 
the limit h(x)\I\ is integrable on H, with JRh(x)\I\ 
— 0; and J R/i n(x)|/| con-
verges as n —> oo. But 
lim / hn{x)\I\ φ ί 
h(x)\I\. 
n-+°° Jn 
Jn 
To see that the first of the criteria above is satisfied, choose a gauge δ to satisfy 
δ(χ) < | for x > 0, and choose p(x) > | . Then we can make S$ satisfy 
(Ss)J2hm{x)\I\ e [0,e[ 
for all choices of m(x) > p(x). Regarding the second criterion, gauges δη can 
be chosen so that, for each n, 
(ßsn)Y,Kix)\i\ 
e]i-e,i]. 
However the third criterion fails, because if e < \ then 
[0,e[ Π ] l - e , l ] = 0. 
4.14 
Non-Integrable Functions 
Theorem 67 of this section appears to be first piece of published10 work in 
generalized Riemann (or -complete) integration. It is reproduced here because it 
also happens to be an informative comparator and motivator for the integration 
of functions in Chapters 6, 7, and 8 of this book. 
The Dirichlet function d(x), defined in (2.6), has value 1 if x is a rational 
number, and value 0 if x is irrational. For any cell / =]u, v] define a Stieltjes or 
incremental version of d as follows 
O(I)=d(v)-d(u). 
(4.20) 
Thus D(JT) = 0 if u and v are both rational or both irrational, D(J) = 1 if v is 
rational and u is irrational, and D(J) = — 1 if v is irrational and u is irrational. 
Theorem 10 implies that the incremental Dirichlet function is integrable on 
every bounded figure. Here is the proof of integrability of D for bounded cells. 
10Theorem 1 of Henstock [85]. 

4.14. NON-INTEGRABLE 
FUNCTIONS 
179 
Theorem 66 The incremental Dirichlet function D(7) is integrable on each 
bounded cell J in R, with indefinite integral D(J), whose value is 1, 0, or —1. 
Proof. If J = ]a, b] and if V is a partition of ]a, b] with 
a = «<°> < «W < uW ■■< w<"+1> = b, 
then, for / = }u^~l\u^}, 
j = 1,2,...,n,n + 1, cancellation gives 
(P) X] D(J) = ]T (d (u«)) - d (uö-1))) = d(6) - d(a). 
i=i 
Thus, depending on whether a and b are rational or irrational, the Riemann 
sum is 1, 0, or —1 for all partitions. 
O 
If f(x) has constant value κ then, for each J =]a,6], f(x)O(I) 
is integrable 
on J, with integral value κ, 0 or — κ. Theorem 1 of Henstock [85] shows that, 
if f(x) is not constant, then f(x)O(I) 
is not integrable on J. In other words, 
no non-constant point function / is Stieltjes-complete integrable with respect 
to the Dirichlet point function d(x). This is because the extreme oscillation of 
d(x) prevents Riemann sums from converging unless / is constant. 
The proof given in Henstock [85] has the following preliminary results. Sup-
pose J = ]a, 6] and suppose a gauge δ(χ) > 0 is defined for a < x < b. Let Q de-
note the set of rational numbers, and, for numbers a < u^-1^ < y^ < u^ < b 
satisfying 
y(J) - UU-V < S (yW) , 
ιχϋ) - 2/ϋ) < δ (yW) , 
a = u(°> < u™ < v.™ 
< uW 
=w<b, 
let MQ denote {u^l\u^2\ 
. . . , u ^ } C Q. Let MQ denote the family of all such 
finite sets MQ for all values of n. The family MQ is not empty since there exists 
a rational number u^ such that 
u(1) <6, 
u(1) <α + δ{α). 
Lemma 4 The supremum of the set {w : MQ G MQ} is b. 
Proof. 
Write β — sup {it; : MQ G MQ). 
It is clear that ß < b, since, by 
definition, w = un < b for all MQ G MQ. 
Suppose, for contradiction, that 
β < b. Choose MQ for which w > β — δ(β). Choose w' G Q satisfying 
ß<w' <β + δ(β), 
w' <b. 
Write MQ = MQ U {U/}, SO MQ G MQ. Since u/ > /3, this is a contradiction. 
Therefore β = b. 
O 

180 
CHAPTER 4. THEORY OF THE 
INTEGRAL 
Lemma 5 There is a δ-fine division 
V = {(x{j\lU)) 
: j = 0,l,2,...,ra +1} 
o/]a,b], with x^ 
= a, χ(πι+1) = b; and so that, for each j , if 1^ =]u, v] and 
χθ') = u then v G Q, while if 1^ =]u,v] and x^ 
= v then u G Q. 
Proof. 
By Lemma 4 MQ can be chosen so that w = -um > b — 6(b). Let 
j(m+i) =]w,b], x(m+1) = b. In (4.21), let /ü" 1) = }u^-l\y^% 
x^~^ 
= 
y^\ 
and let I& =]yV\uW], 
xü) = y^j). Take χ(°> - a , j(°> =]α,τχ(1)]. 
O 
It can similarly be shown11 that there is a δ-fine division of ]a, b] with irrat-
ional partition points 
u^. 
Lemma 6 There is a δ-fine division 
V = {(xV\lW) 
: j = 0 , l , 2 , . . . , r a + l } 
of]a,b], with x^ 
= a, x(m_,_1) = b; and so that, for each j , if 1^ =]u,v] and 
#0') = u then v G R \ Q, while if 1^ =]u,v] and x^ 
= v then u G R \ Q. 
The next theorem uses Lemmas 5 and 6, with (4.21), to demonstrate that the 
incremental or Stieltjes form of the Dirichlet function is a "bad" integrator. 
Theorem 67 If f{x) is defined for a < x < b, and if f(x)O(I) 
is integrable on 
J =]a, b], then f(x) is constant for a < x <b. 
Proof. By hypothesis there exist c, d, a < c < d < 6, such that /(c) φ 
f(d). 
By Theorem 17, integrability on ]a,b] implies integrability on ]c, d], so take 
a = c, b — d and assume f(a) φ f(b). 
Writing κ = /(α), by Theorem 66 
the function κΏ(Ι) is integrable, so the integrand f(x)O(I) 
can be replaced by 
(/(#) — κ)Ό(Ι). 
Therefore, without loss of generality, it can be assumed that 
f(a) = 0 and f(b) φ 0. Take ε — \ |/(6)|, and let δ be an arbitrary gauge, 
defined for a < x < b. First, suppose b is an irrational number. Using Lemma 
5, choose a J-fine partition V1 of ]a, b] such that, for 1 < j < m, the partition 
points Uj of (4.21) are rational numbers. Then (V1) Σ f(x)O(I) 
— OL\ +#2 + ^3, 
where 
ai 
= /(a) ( d ^ - d («<»>)), 
a2 
= 
■■■ + f{y^O 
(jü-i>) + f{y{j)O 
(jü>) + · · · 
m 
a 3 
= 
/ ( 6 ) ( d ( 6 ) - d ( ^ m ) ) ) ; 
11 These results can be deduced directly from Cousin's theorem (page 45), and amount to 
an alternative proof of it. See Bartle [7], Exercise 1.1. 

4.15. 
CONCLUSION 
181 
so ct\ = Q.2 = 0 and a^ = f(b). Using Lemma 6, choose a δ-üne partition V2 of 
]a, b] such that, for 1 < j < m, the partition points Uj are irrational numbers. 
In this case (V2) Σ /(z)D(-O = 0. Therefore 
(V^fixMI) 
- 0D 2)]T/(z)D(J)| = |/(6)| > e. 
Since this holds for every gauge δ, f(x)O(I) 
is not integrable on ]a, b] unless / 
is constant. If b is a rational number the same argument can be re-arranged in 
accordance with Lemma 6. 
O 
Example 45 What about integrability ofO on'R? Consideration of this point 
requires that D(J) be defined for I =] — οο,υ] and I = [u, oo[. IfO is assigned 
value 0 for these sets, it is easy to see that D is not integrable on R. Alterna-
tively, suppose the Dirichlet function d is assigned a value (0 or I, say) at oo; 
likewise at —oo; and suppose, for I = [u, oo[, D(7) is defined as d(oo) — d(u). 
Similarly for I =] — οο,ν]. Then the incremental Dirichlet function D is addi-
tive on disjoint cells of'I(R), and is accordingly integrable on R. This example 
demonstrates that, when extending to unbounded domains, some care should be 
taken to preserve the additivity of a Stieltjes cell function. 
O 
The function cos x2, though it appears to be non-integrable on R, was shown 
in Section 2.15 to be integrable. The Feynman integrand of Chapter 6 and the 
Brownian increments of Chapter 7, despite being deemed non-integrable, are 
shown to be integrable. In contrast, Theorem 67 specifies a class of functions 
which actually are non-integrable, thereby providing a sense of what really 
constitutes or determines integrability. 
This theorem shows that, in terms of integrability, D(7) is a "bad" integrator 
function. The rest of this book deals mostly with integrators (Feynman and 
incremental Brownian) which, though highly oscillatory, are not actually as 
"bad" as D(I). 
4.15 
Conclusion 
Except where stated otherwise, the results of this chapter are valid for functions 
defined in a domain R T where T can be countably or uncountably infinite, or 
finite. 
The proofs in this chapter employ Riemann sum construction sometimes 
involving quite delicate partitioning of the domain of integration. But these 
abstract proofs are designed to be valid for the integration of functions h(x,N, I) 
of great generality, without any assumption12 of measurability. 
12When measurability of the integrand is introduced, Theorems 79, 122, and 123 show 
how the integration of functions defined on abstract measure spaces can be interpreted as 
Henstock integration in R, and Lebesgue integration theory's simple functions 
in abstract 
measure space are transformed into step functions in R. 

182 
CHAPTER 4. THEORY OF THE 
INTEGRAL 
The 7-fine partitions used to calculate expected values of random variables 
are not necessarily the regular or partially regular partitions of (3.4), (3.5), 
(3.6), (3.7), and Figures 3.3 and 3.4. In general they are irregular partitions, 
consisting of cells which are unaligned in some or all of the restricted dimensions 
of the partition, such as the cells in Figures 2.5 and 3.2. 
But for particular integrands the partitioning of the domain R T for Riemann 
sum construction can be relatively straightforward in practice. A comparison 
can be made with the integration in R of familiar functions such as polynomials 
which are Riemann integrable in the basic sense. A more sophisticated kind of 
analysis is required in order to justify, for instance, differentiating them under 
the integral sign. But relatively uncomplicated partitioning of the domain is 
sufficient to give meaning to the integration of such functions. 
The calculation of the integral of a function in R T can be greatly simpli-
fied if and when the calculation can be done by means of Riemann sums over 
regular partitions. As is the case for integration in one dimension, the integra-
bility of functions (i.e., convergence of their Riemann sums) in R T can often be 
established using relatively uncomplicated partitions of R T. 
In Chapter 3 binary partitions of R T were demonstrated. In Chapter 7 
these binary partitions are used to construct step function approximation of 
integrands which, in conjunction with the limit theorems of this chapter, make 
possible the integration of functions that are important in the analysis of random 
variability. 

Chapter 5 
Random Variability 
Suppose T is a finite or infinite labeling set. For the purposes of this book the 
domain R T is structured by means of a number of features: 
■ points x = χτ', 
- cells 1 = I[N) Gl(R T); 
• figures E G E(R T); 
■ point-cell association x G I[N]* and I[N] G x*; 
■ partitions V, divisions X>, gauges 7; 
■ integral f h, and variation V^, defined for functions h(x,N,I) 
of assoc-
iated triples 
(x,N,I[N]). 
A real- or complex-valued function F defined on the figures E(R T) of R T is an 
additive (or Stieltjes) cell function if F is finitely additive on disjoint figures. 
A Stieltjes cell function F is a distribution function or potentiality 
distribution 
function if F(R T) = 1. Every Stieltjes cell function is integrable (in the Stieltjes-
complete or Henstock sense) on figures. 
If an experiment (measurement or observation) Y produces a datum y, and 
if Ωγ is the set of potential data values, then Ωγ is the sample space for the 
experiment. If, for cells I G Ι(Ωγ), the likelihood (potentiality or accuracy 
potentiality) C[(y G /)] is given by a potentiality distribution function 
Fy(I), 
then 
Y~y[nY,FY} 
is an observable. If Ωγ = R then Y is an elementary observable. 
Suppose, for each t G T, Xt is a measurement or experiment with sample 
space fixt = R and potentiality distribution function Fxt defined on I(R); so 
Xt~xt\R,FXt]. 
A Modern Theory of Random 
Variation: With Applications 
in Stochastic Calculus, 
183 
Financial Mathematics, 
and Feynman Integration. 
First Edition. By Pat Muldowney 
Copyright © 2012 John Wiley & Sons, Inc. Published by John Wiley & Sons, Inc. 

184 
CHAPTER 5. RANDOM 
VARIABILITY 
This is an elementary observable since Q,xt = R. Viewed jointly, the collection 
{Xt : t € Γ} 
are joint observables provided the distribution functions Fxt satisfy consistency 
conditions. If the joint observation X = XT = (Xt)teT 
produces a joint datum 
x = XT = (xt)ter' 
a nd if? f°r e a c n cen< I — I[N) G I(R T), the likelihood 
£[(z Te/[JV])] 
is given by a potentiality distribution function Fx(I[N]), 
then 
X = 
XT^XT[BT,FX] 
is a jom£ observable or joint-basic observable. For this to be well defined Fx 
must be consistent. That is, for 
M = N1UN2e 
AT(T), 
I = J(JVi) x J(A^2) x R n M , 
J(N2) = R^2, 
Fx must satisfy 
Fx(/) = FX(J[7V1]). 
To qualify as a component of a joint-basic observable X = XT — XT [RT
5 FX\ , 
the distribution functions Fxt of an elementary observable Xt ~ xt [R, Fxt ] 
must satisfy the consistency requirement. Then, for t G T, the elementary 
observable Xt is a oaszc observable. 
Given an observable Y ~ y [Ωγ,,Ργ], if a datum z is generated by a deter-
ministic calculation z = f(y) on the observed datum ?/, then Z ~ z [Ωγ, Fy] is 
an observable contingent on the observable Y\ or simply a contingent observable. 
If Xt is a basic observable for each t G T, so X T is a joint-basic observable, 
then, with datum z = / ( # T ) J the contingent observable 
f(XT)~f(XT)[RT,FXT} 
is a joint-contingent observable. 
Theorem 68 Given a joint-basic observable X = XT, the corresponding Xt 
(t G T) are elementary observables. 
Proof. For each cell It G I(R{i}) = I(R), take 
FXt(It) = 
Fx(ltxRT\M). 
The definition of Fxt can easily be extended to figures in R. The additivity of 
Fx implies that Fxt is additive, and Fx(RT) 
= 1 implies Fxt(R) 
= 1. 
O 
The contingent observable concept can be extended to include functions dep-
endent on elements N G Λί(Τ) in addition to dependence on random occurrences 
or data χτ'-
f(X,N)~f(x,N)[RT,Fx]. 

RANDOM VARIABILITY 
185 
Definition 36 If f(X) is a joint-contingentobservable, 
and if 
f(x)Fx(I[N]) 
is integrable on R T, then f(X) is a random variable, and the expectation or 
expected value of f(X) is1 
E [/(*)]= / 
f(x)Fx(I[N}). 
Similarly, if the contingent observable has the form f(X, N), then it is a random 
variable ifE[f(X, N)], = / R T f(x, N)FX(I[N}), 
exists. The following discussion 
deals with the more familiar random variables f(X), but can be extended easily 
to include joint-contingent observables of the form f(X, N). 
Taking / to be the identity function on R, a basic observable X ~ x[R, Fx] 
can be a random variable in this sense, since J RxFx(7) is defined for x G R, 
I G I(R). In that case, E[X] = JRxFx(I) 
if the integral exists. But if X = XT 
is a joint-basic observable (so T has more than one element) then the integral 
is not defined, and X? cannot be a random variable in this sense. 
If both f(x)Fx(I) 
and \f(x)\Fx(I) 
are integrable on R T (so E[f(X)] 
and 
E [1/POI] exist) then the observable 
f(X)~f(x)[RT,Fx] 
is an absolute random variable. 
With T infinite, if a collection of basic random variables {Xt : t G T} satisfies 
E[Xt] = Ε[Χ^/] for all t,t' G T, then we say that the collection is a martingale. 
Note that a martingale XT = (Xt)teT 
ls n o t necessarily an observable, since 
there is no requirement that a joint distribution function for XT should exist. 
Suppose T = {^1,^2^3?···} with ti < h < ^3 * · *· If the joint-basic experi-
ment X = (Xj)°<izl is observable, with 
(X,-)~ i = X ^ x [RT, FX] = (Xj)™=1 [RT, FX] , 
and if it satisfies 
E[Xtn\Xtj,j<n]=E[Xtn\Xtn_1} 
(5.1) 
for all n, then we say that X = (Xj)c*L1 is a Markov chain. A Markov chain 
is a joint-basic observable, since there must be a joint distribution function, 
satisfying consistency, in order to determine whether (5.1) holds. 
Theorem 82 shows that when / is measurable (see Definition 41 below), 
contingent and elementary representations of the same experiment, 
f(X)~f(x)\RT,Fx], 
Y~yiR,FY], 
Y = f(X), 
have the same likelihood values £, and yield the same expected value, 
E[/W] = / 
f(xT)f(I[N}) = [ yFY(I) = E[F], 
lrTo emphasize the distribution function F = Fx this can be written E F [f(X)]. 

186 
CHAPTER 5. RANDOM 
VARIABILITY 
so the expected value of an observable is well defined. Reconciliation of other 
choices of alternative sample spaces and potentiality distribution functions for 
observables usually presents little difficulty. 
Any statement about an experiment (or observable or random variable) X, 
f(X), 
Y, .. · should generally be understood as a statement about the corr-
esponding random data values x, /(x), y, ..., respectively, in advance of actual 
occurrence but in association with their accuracy potential (their potentiality 
distribution function). 
In other words it is a statement that has meaning prior to actual observ-
ation or measurement taking place, and where information about the (as yet 
undetermined) data values x, /(#), y, ... is of the kind indicated in Tables 1.1 
and 1.2. 
On the one hand, the potentiality distribution function Fx is a source of 
information about the relative accuracy of the potential data value, such as its 
expectation and variance. On the other hand, knowledge of potential data values 
can help to determine potentiality distribution functions or accuracy measure. 
To illustrate, suppose T — {1,2,3,...} and 
(Xj)JL 1 = X^x 
[RT,FX] = (Xj)?=1 [RT,FX] 
is an observable. Then, to say that n~1{X\ + · · · + Xn) converges to 0 almost 
surely as n —> oo means that, for Fx-almost all x G R T, the random outcome 
n~1(xi H 
h xn) converges to 0 as n —► oo. In other words, n _ 1 (x± H 
h xn) 
converges to 0 except for a set S G R T with Vpx (S) = 0. 
Likewise, to say that the sequence of observables Xj converges to an observ-
able Y means that the sequence of observed random outcomes Xj converges to 
a random outcome y of the observable Y. 
5.1 
Measurability of Sets 
Measurability is not required in Definition 36 above, nor in Chapter 4 on the 
Henstock integral. In that chapter, results such as Fubini's theorem and the 
dominated convergence theorem are expressed without invoking the powerful 
property of measurability. 
In contrast the traditional theory of probability is predicated on measurabil-
ity. The Lebesgue integral is defined by means of measurable simple functions; 
while the method of this book is based not on measurability but on conver-
gence of Riemann sums. If the additional property of measurability is assumed, 
further results are obtained. 
The following notation is commonly used. If X ~ x[RT, Fx] is an elementary 
or joint-basic observable (i.e., T consists, respectively, of one or more than one 
element) and if A is a subset of R T, then the statement "X G A" means that the 
potential datum x belongs to A and the set indicated by the notation (X G A) 
is simply the set A. If B is a subset of the (real or complex) range of /, the 
statement uf(X) 
G 5 " means that the contingent datum f(x) belongs to B. 

5.1. MEASURABILITY 
OF SETS 
187 
Thus if f(X) 
is a contingent observable, with f(x) in R or C, and if B is a 
subset of, respectively, R or C, then the set {f(X) £ B) is 
(f(X) €B) = {xGQx: 
f{x) €B} = 
f~\B). 
If / is real-valued and v G R, then 
(f(X)<v), 
(f(X)<v), 
(f(X)>v), 
(f(X)>v) 
are, respectively, the sets 
{x e R T : f(x) <v}, 
{xeRT 
: f(x) < v) , 
{xeKT 
: f(x) >v}, 
{x e R T : f(x) > v} . 
Notation (/ < v), (f(x) <v), ..., is also used for the latter sets. 
Definition 37 Given a real- or complex-valued function h(x,N,I[N]), 
a set 
A C R T is /i-measurable (in the Riemann sum sense) if 1A(X)II(X,N,I[N]) 
is 
integrable on R T. 
If h is integrable on R T and if A is /i-measurable, then, by Theorem 13, the 
complement of A is /i-measurable, and 
/ 
lBr\A(x)h(x,N,I[N])= 
[ 
h(x,N,I[N])- 
[ 
lA(x)h(x,NJ[N]). 
Measurability with respect to arbitrary real- or complex-valued functions h is 
examined further in Section A.l of the Epilogue. But for now we are concerned 
with the case where h is a distribution function. 
Definition 38 Given a real- or complex-valued function distribution 
function 
Fx(I[N]), 
a set A c R T is Fx-measurable (in the Riemann sum sense) if the 
integral J R T 1A(X)FX(I[N]) 
exists. 
Suppose an observable X has representation X ~ x[RT, Fx], By Definition 38, 
a set A c R T is Fx-measurable if 
E[1A(X)}= 
[ 
lA(x)Fx(I[N}) 
exists; that is, if 1A(X) is a random variable. 
Definition 39 If X ~ x[KT,Fx] 
define 
Px [A] := E [1A(X)\ = / 
lA(x)Fx(I) 
if the integral exists. 
According to this definition, the function Ρ χ is complex-valued if Fx is complex-
valued. If 1A(X) is a random variable, call A a random event, or, simply, an 
event. 

188 
CHAPTER 5. RANDOM 
VARIABILITY 
Definition 40 If a statement is true except for potential occurrences x G A with 
VFX(A) 
— 0 then the statement is true almost surely, or Εχ-almost surely. 
Theorem 69 If a set A C R T is Εχ -null then every subset A' of A is Fx -null 
with Ρχ [Α'] = 0. 
Proof. If VFx(A) 
= 0, then VFx(A') 
= 0 for each Af C A, and Theorem 37 
implies that each such A! is an Fx-measurable event with Ρχ \A!\ = 0. 
O 
The F^-variation of a set A C R T is Vpx (A), the variation of Fx in A. The 
Fx-variation exists for every set A provided the possibility of an infinite value 
for VFx(A) is admitted. In contrast, Ρχ [A] only exists for Fx-measurable sets 
or events A. Also, the calculation of VFx(A) 
uses absolute values |Fx(/)|, so 
VFx(A) 
must be non-negative, and can be infinite. In contrast, calculation of 
Ρχ [A] is based on Fx(7); so Ρ χ [A] can be negative or complex-valued, and 
infinite values are excluded by the definition of the integral. 
Theorem 70 If A is measurable and if Fx is non-negative, then 
Px[A]=VFx(A). 
Proof. This follows from Theorem 39. 
O 
Thus, if the potentiality distribution function Fx is real-valued and non-
negative, the Fx-variation—defined for all sets—coincides with Ρχ on mea-
surable sets. In particular, if Fx is non-negative and if A is an Fx-null event 
then 
VFx(A) 
= [ 
1A(X)FX(I) 
= Px [A] = 0, 
which is a partial converse to Theorem 69. 
The following example, similar to Example 28, shows how the Ρ χ function 
may fail to be a potentiality distribution function. 
Example 46 Define F on cells I G I(R) as follows: 
{
1 
if I has the form ]u. oof, 
0 
otherwise. 
IfEe 
E(R) define F{E) = 1 if E contains an interval of the form ]u, oo[? with 
F(E) = 0 otherwise. Then F is finitely additive on disjoint figures, and 
F(R) = F(]-oo,oo[) = l, 
so F is a potentiality distribution function. Every subset A ofH is F-measurable, 
with 
P[A]= 
f lA(x)F(I) 
= 0. 
JR 

5.1. MEASURABILITY 
OF SETS 
189 
To see this, consider any division V = {(#,/)} of H, and consider Riemann 
sums 
(2>)5>(J), 
(V)"£lA(x)F(I). 
The only term (x,I) GD that contributes a non-zero value in the former is the 
term (oo,]it, oo[). But this term is excluded from the second Riemann sum since 
x = oo is not contained in any subset A ofH =] — oo, oo[. Thus P is additive 
on figures E E E(R) and is therefore a Stieltjes cell function. But it is not a 
potentiality distribution function since P [R] = J R 1 R ( X ) F ( 7 ) = 0. 
O 
Example 46 above is similar in some respects to Example 28. We can expect 
Ρ χ to be a potentiality distribution function provided Fx is not too "unusual" 
or "exotic". The more important potentiality distribution functions Fx generate 
probability functions Ρ χ that are themselves potentiality distribution functions; 
see Example 47 below. 
Theorem 71 Given a real- or complex-valued distribution function Fx, if A 
is an Fx-measurable subset ο/Ωχ = R T, then its complement R T \ A is Fx-
measurable. 
Proof. By the integrability of Fx on R T, Theorem 13 implies that 
/ 
1RT\A{X)FX{I{N\) 
= [ 
FX(I[N]) - [ 
lA(x)Fx(I[N}), 
and this gives the result. 
O 
Theorem 72 Suppose the distribution function Fx is non-negative. If A\ and 
A2 are Fx-measurable subsets of Ωχ = R T
; then A\ Π A<i and A\ U A2 are 
Fx-measurable. 
Proof. 
Let f(x) := 1A1(X) + 1AI(#)· 
Since 1A1(X) and 1A2(X) 
a r e Fx-
integrable, their sum f(x) is integrable, and \f(x)\ is integrable since |/(x)| = 
f(x). 
Therefore (/ > v) is Fx-measurable2 for all v G R, and, in particular, 
(/ > 2) is Fx-measurable. But 
( / > 2 ) = ΑιΠΑ 2, 
so A\ Π Α2 is Fx-measurable. The Fx-integrability of 1A1(X)^ 
1A2(X)I 
a nd 
1Α1ΠΑ2(Χ) 
then implies that 
1A1UA2(X), 
= 1ΑΛΧ) 
+ 
1Α2(Χ)-1Α1ΠΑ2(Χ), 
is integrable, and therefore A\ U A2 is Fx-measurable. 
O 
This step uses Theorem 76 in advance of its actual statement and proof in the next section. 

190 
CHAPTER 5. RANDOM 
VARIABILITY 
Theorem 73 Suppose the distribution function Fx is non-negative. If' A\, A2, 
..., An are disjoint Fx-measurable subsets ο/Ωχ = R T, then the set A = 
U?=i A? is Fx-measurable and 
n 
Px[A} = 
J2VxlAj}· 
i=i 
Proof. We have 
n 
1A{X) = 
Y^lAj(x), 
3=1 
and, by Theorems 72 and 13, 
/ 
lA(x)Fx(I[N]) 
= J2 [ 
lAj{x)Fx{I[N]). 
This gives the result. 
O 
This result can be extended to the union of a countable family of disjoint 
Fx-measurable sets. See Theorem 129 of Section 5.12 below. The next result 
shows that if a continuity condition is satisfied by Fx, then Ρ χ is a distribution 
function. 
Theorem 74 Suppose the distribution function Fx is non-negative. If for each 
E G E(R T) we have Ypx(E\E) 
= 0, then each E is Fx-measurable and Ρχ is 
a potentiality distribution function coinciding with Fx on figures E G E(R T). 
Proof. Since FX(E \ E) = 0, Theorem 69 implies that the set E \ E is im-
measurable with 
P x 
By Theorem 13, 
E\E]= 
[ 
l^E(x)Fx(I[N]) 
= 0. 
[ 
lE(x)Fx(I[N})= 
[ FX(I[N})- 
[ 
l^E(x)Fx(I[N]), 
JnT 
JE 
JnT 
so E is immeasurable; with Px [E] = FX(E) for each E G E(RT). 
Q 
Example 47 Note that, with E = R, the condition Vpx (E\E) 
= 0 fails for 
the distribution function Fx in Example J±6 above. The following distribution 
functions are widely used in applications. 
■ Binomial 
Distribution. 
Suppose n is a positive integer and r is an 
integer satisfying 0 < r < n. Suppose p is a real number with 0 < p < 1. 
If I =]u, v] G I(R) and r — l<u<r<v<r 
+ l define 
n\ 
F « ^ 
= H ( ^ ) i p r ( 1 " - p ) n ' 

5.1. MEASURABILITY 
OF SETS 
191 
If I C [rc,oo[, or if I C] - οο,Ο], /ei ^ W 7 ) = °> and 
let 
Fbin(T) 
be 
defined by additivity for other cells I. Then F^n is a binomial distribution 
function and 
X^x[R,Fbm] 
is a binomial observable. It has expected value E [X] = np, so X is a 
binomial random variable. 
■ Poisson 
Distribution. 
Given X > 0 let r represent any non-negative 
integer. If I = }u, v] E I(R) and r — l<u<r<v<r 
+ \ define 
e~xXr 
^poiss\*) ~ 
^j 
' 
with Frp0iss(I) defined by additivity for other I E I(R). This is the Poisson 
distribution function. 
■ Normal 
Distribution. 
Suppose μ G R and σ > 0. For I G I(R) the 
normal or Gaussian distribution function is 
N ^ ( / ) = — \ = ( 
e-^-rtdy. 
2 
σ\/2π Ji 
More generally, with c = a -f- ώ (L = Λ/—T; « < 0 7 b > 0, c ^ 0), define 
σ V 7Γ Jj 
■ Lognormal 
Distribution. 
Writing 
1 
1 
/ 
( l n y - μ ) 2λ 
i{z) — — j = - exp I — 
aV2^y 
\ 
2σ2 
J' 
the lognormal distribution function, defined for i" E I (]0, oo[), is 
L(J) = j i(y)dy. 
The standard normal distribution function N has mean μ = 0 and standard 
deviation σ = 1. (The standard deviation of a real-valued random variable 
is the square root of its variance.) Therefore a standard normal distribution 
function has 
N°'1
1(/) = N(/). 
2 
A normal distribution function N ^ has mean μ and standard deviation σ. 
2 
The normal and lognormal distributions are discussed in Chapter 7. The 
properties of the binomial and Poisson distributions are described in Feller [62]. 
Continuity of the exponential function ensures that the normal and lognormal 
distribution functions satisfy the condition VF (E \ E) = 0 for figures E. 

192 
CHAPTER 5. RANDOM 
VARIABILITY 
5.2 
Measurability of Random Variables 
This section shows that, if a real-valued f(X) 
is an absolute random variable 
(i.e., f(x) and \f(x)\ are Fx-integrable on R T), then / is a measurable function. 
If f(X) is a real-valued, or complex-valued, observable contingent on X, and 
if B C R, or, respectively, B C C, then 
P x [(f(X) G B)} := E [lf-HB)(X)] 
= [ 
lf-HB)(x)Fx(I) 
if the integral exists (i.e., if lf-i^(X) 
is a random variable, or f~l(B) 
is an 
event). 
Definition 41 Suppose a function h(x,N,I[N]) 
is real- or complex-valued. A 
real-valued function f is h-measurable if, for each real number v, the sets 
(f>v), 
(f>v), 
(f<v), 
(f<v) 
are h-measurable. A complex-valued function f is h-measurable if both the real 
part and the imaginary part of f are h-measurable. 
Example 48 Suppose the function h(x,N, I[N]) is given. For any set A C R T, 
the indicator function 1 A is real-valued. If A is a subset 
ofHT, 
(1A > v) = < 
A 
if 
0 < v < 1, 
R T 
if 
v < 0, 
1 0 
if 
v > 1. 
Similar identities also apply, such as (1A < v) = R T \A if 0<v<l. 
Thus, 
according to Definition 39, the function 1 A is h-measurable if and only if the set 
A is h-measurable. If A is h-measurable and h is integrable then the complement 
of A is h-measurable. 
O 
We are primarily concerned with F-measurable functions where F is a dis-
tribution function. The following result shows that a point function / is im-
measurable if any one of the conditions in Definition 41 holds. 
Theorem 75 Suppose Fx is a non-negative distribution function. 
If the set 
(f{X) < a) is Fx-measurable for each a G R then f(X) 
is Fx-measurable. 
Proof. Given a G R, 
(/(*) 
> a) = R\ (f(X) 
< a); 
l(/(x)>a)(*) = 1 - 1(/ W<a)(*); 
so 
lmx)>a)(x)Fx(I[N]) 
= FX(I[N}) - 
l{f{x)<a)(x)Fx(I[N}). 

5.2. MEASURABILITY 
OF RANDOM 
VARIABLES 
193 
Each of the two right-hand functions is integrable on R T. Therefore the left-
hand function is integrable, so the set (f(X) 
> a) is Fx-measurable for each 
a e R. For j = 1,2,3,..., let 
Sj= 
(ί(Χ)>α+-λ 
=ix€RT 
:f(x)>a+-\. 
For each j the latter set is Fx-measurable, 
so ls(x)Fx(I[N]) 
is integrable on 
R T. If 
S = (f(X) 
>a) = {xenT: 
f(x) > a} 
then, for each x G R T, 
lSj(x) 
-»· l 5(x); 
lSj(x)Fx(I[N}) 
-»■ 
ls(x)Fx(I[N}) 
as j —^ oo. For each j , 
lSj(x)Fx(I[N})<Fx(I\N}), 
so Theorem 61 (dominated convergence theorem) implies ls(x)Fx(I[N]) 
is int-
egrable; that is, S is Fx-measurable, and this holds for each a G R. This in turn 
implies that (f(X) 
< o) is Fx-measurable for each a. Thus all the conditions 
of Definition 41 are satisfied. 
O 
Theorem 76 Suppose the real-valued potentiality distribution function Fx is 
non-negative. 
If real-valued f(X) 
is an absolute random variable, then f is 
Fx-measurable. 
Proof. 
The proof is in stages. By hypothesis, /(#), along with |/(#)|, is 
Fx-integrable on R T. 
1. The first stage is to prove that, for any real number v, the function 
min{f(x),v} 
is Fx-integrable. For this we use Theorem 58 with 
hx{x,N,I) 
= f(x)Fx(I[N}), 
h2(x,N,I) 
= 
vFx(I[N}). 
Both hi and /i2 are integrable, with 
and 7 can be chosen so that, with ε > 0 given, 
|(2> 7)ΣΛι-Ε[/(Χ)]| 
< 
ε, 
|(Ρ 7)ΣΙΛι|-Ε[|/(Α:)|]| 
< 
ε, 
( Ρ 7 ) Σ Λ 2 
= 
ν, 

194 
CHAPTER 5. RANDOM 
VARIABILITY 
for all 7-fine divisions Ί)Ί of R T. Let ΕΊ be any subset of Τ>Ί and let 
E = [j{I: 
(z, I) e εΊ. Then Theorem 17 implies that 
(£y) Σ 
1 ^ 1 - / ^ ΙΛιΙ) 
< 2ε, 
so 
( £ γ ) Σ > ι 
^ 
- ( ^ Σ Ι ^ Ι 
^ 
- Ε [ | / ( Χ ) | ] - 2 ε . 
Since Fx(I) is non-negative, 
For each (x, 7[iV]) G Τ>Ί choose j(x, I[N]) = 1 or 2 , and then consider the 
expression 
of Theorem 58. Let 
Εχ
Ί = {(x,I[N\) 
: j(x,I[N}) 
= 1} , £* = {(x,I[N\) 
■ 3&W}) 
= 2} , 
and let 
d 
= 
- Ε [ | / ( Χ ) | ] - 2 ε - Η . 
Then 
ci < 
( P 7 ) ^ ^ w [ i Y ] ) ( x , i V , / ) 
= 
( £ * ) 5 > i + ( £ ? ) 5 > 2 
for all D 7 and for all choices of j(x,I[N]) 
= 1 or 2 for (χ, J[iV]) G £>7. 
Therefore, by Theorem 58, the function min{f(x),v} 
is Fx-integrable. 
2. Now let u be any real number less than v, and consider the Fx-integrability 
of the function 
max {min{/(#), v}, u} . 
With 
^(χ,Ν,Ι) 
:=mm{f(x),v}Fx(I[N}) 
and h2{x,N,I) 
:=uFx(I[N]), 
an argument similar to the preceding one gives existence of a gauge 7 and 
a real number c2 so that 
for all Τ>Ί and all choices j(xJ[N]) 
= 1 or 2 for (ar,/[iV]) G £>7. Thus 
Theorem 58 gives the integrability of the function max {min{/(x), v},u}. 

5.2. MEASURABILITY OF RANDOM VARIABLES 
195 
3. We have 
( u 
when 
f(x) < u, 
meLx{min{f(x),v} 
— u} = < f(x) 
when 
u < f(x) < v, 
( v 
when 
f(x) > v. 
Define 
max{mm{ f(x),v},u} 
— u 
v — u 
so g is Fx-integrable, 
{
0 when 
f(x) < u, 
1 
when 
f(x) > v, 
and 
l(f>v)(x) 
= 
lim 
g(x;u,v). 
The latter is the limit of a monotone decreasing function bounded by 0 
and 1, so Levi's monotone convergence theorem (Theorem 57) gives the 
Fx-integrability of l(j>v)(x). 
We have 
1(f<v)(x) 
= 
1-!(/>«)0*0 
so l(f<v)(x) 
if Fx-integrable. The integrability of l(f<v)(x) 
and l(f>v)(x) 
is 
proved similarly. 
O 
Theorem 77 Suppose the distribution function Fx is non-negative. If the real-
valued function f is Fx-measurable, then, for I =]u,v] G I(R), the set 
is Fx-measurable. 
Proof. We have (u < f < v) = (u < f) Π (/ < v). In other words, 
rHi) = r 1 (]«,c»[ jnr^i-c»,!;]). 
The result then follows from Definition 41 and Theorem 72. 
O 
A similar proof gives the Fx-measurability of f~l(I) 
for intervals / that are 
not cells ]u,v]. 
Theorem 78 Suppose the non-negative distribution function Fx is continuous 
in the sense that 
VFx(E\E)=0 
for each E G E(R T) 7 and suppose the function f is real-valued and Fx-measurable. 
Then the composite function Ρ χ o f~l 
is a distribution function on cells J G 
I(R). 

196 
CHAPTER 5. RANDOM 
VARIABILITY 
Proof. If J and J' are disjoint cells in R then / 
l(J) and / 
1(J /) are disjoint 
Fx -measurable sets in R T, with 
r\j) + r\j') = r\Jvj'). 
Then, by Theorem 77, 
px [r\j) u r\J')] = Ρχ [rV)] + Ρχ [r V)] · 
In other words, 
Pxof-^JUJ') 
= P x o ^ f j j + P ^ o / - 1 ^ , 
so Ρ χ o f~l 
is an additive cell function. Since Vpx(i? \ E) = 0 for each E, 
Theorem 74 implies that 
Pxof-\R) 
= Px[f-\R)] 
= PX[RT] 
= 1, 
so Ρ χ o f~l is a distribution function. 
0 
The following is a partial converse to Theorem 76. 
Theorem 79 Suppose the real-valued distribution function Fx is non-negative. 
If 9 > 0 is Fx-integrable on R T, and if f is real and Fx-measurable with \f\ < g, 
then f and \f\ are Fx-integrable on R T. 
Proof. To begin with, assume f(x) > 0 for all x. Given a positive integer ra, 
let 
and, for each x, let 
ί 0 
if 
0<f(x) 
<uu 
fm(x)'=l 
Uj 
if 
Uj< f{x)<Uj+u 
j = l,...,m2 m, 
(5.2) 
{ 0 
if 
f(x) > m. 
Then 
fm(x) < f(x) 
<g{x) 
and, for each x, fm(x) 
is monotone increasing, with limit /(#), as m -» oo. 
Denote ]UJ,UJ+I] by J·7 G I(R). Since the set 
Aj = (UJ <f< 
uj+1) = f~\Jj) 
C R T 
p 
m 2 m 
« 
/ 
fm{x)Fx{I) 
= 
V 
/ 
Ujl{u<f<u 
)(x)Fx( 
is Fx-measurable for each j , 
ί=ι · , η τ 
m 2 m 
m 2 m 
= Σ ^p* ^ < -f - ^+^ = Σ u^x l·4'] 
3=1 
3=1 
m 2 m 
m 2 m 
= Y^ujPxif-1^)} 
= 
Υ^η,ΡχοΓ1^)-
j=l 
j = l 

5.3. REPRESENTATION 
OF 
OBSERVABLES 
197 
Then Levi's monotone convergence theorem (Theorem 57) gives the integrability 
of / when / > 0. If / is not > 0, replace / by / + <7, so / + g is integrable, and, 
since g is integrable, (f + g) — g = f is integrable. The proof of the integrability 
of \f\ is similar. 
O 
Example 49 The expressions 
m 2 m 
m 2 m 
3=1 
3=1 
from the preceding proof are, respectively, a simple function and a step function. 
The Lebesgue integral of a function whose domain is an abstract measure space 
can be obtained from simple functions defined on the measure space. The proof 
of Theorem 79 indicates how to prove that a Lebesgue integral of a function f 
defined on a measure space endowed with measure function μ is equal to the 
corresponding Henstock integral defined on the range space R of f. This is how 
the Henstock or -complete integral of a real-valued function f is defined when 
the domain of f is not R or R T. This point is addressed more fully in Theorem 
122 of Section 5.13, in which f has an abstract measure space as domain instead 
o/RT. 
O 
Theorem 80 Suppose the real-valued distribution function Fx is non-negative. 
If f is real and measurable, and if the Fx -integrals of the functions / m (defined 
by (5.2)) are bounded as m —> oo7 then f and \f\ are Fx -integrable. 
Proof. The proof is similar to the previous one. 
O 
Note also that Theorem 42 shows that integrable functions f(x)Fx(I) 
which 
have bounded variation are absolutely integrable, and hence—when the distribu-
tion function Fx is non-negative—Lebesgue integrable; see Theorem 122 below. 
The expressions UjPx [/_1(J·7)] are the simple functions used to define the 
Lebesgue integral of point function / with respect to measure Ρ χ . 
Theorem 81 If Fx is non-negative and \f(x)\ is Fx-integrable, then 
E[\f(X)\] 
= 
VlflFx[RT}. 
Proof. This follows from Theorem 39. 
O 
5.3 
Representation of Observables 
This section returns to the issue of alternative representations of random vari-
ables. Given a contingent observable /(Χρ), an equivalent observable Y in 
elementary form has been posited, with 
f(X) ~ /(x)[R T, Fx], 
y = /(*), 
Y ~ y[R, FY], 

198 
CHAPTER 5. RANDOM 
VARIABILITY 
so Y = f(X)· 
If f(X) 
and Y represent the same experiment, then calcul-
ations performed on each of the representations should give the same result. 
For instance, is the expectation functional E well defined? 
Given f(X), 
the following is the definition of Fy: 
FY(J) := Ρ χ o Γ1 (J) = P x [ Γ V ) ] = / 
l(f-HJ))(x)Fx{I[N]), 
(5.3) 
whenever the integral exists. For this to be meaningful, the same physical 
event should have the same likelihood when calculated in any mathematical 
representation. 
Theorem 78 gives sufficient conditions to ensure that this holds for the dis-
tribution functions of (5.3). Theorem 79 can also be invoked for sufficient con-
ditions. 
To illustrate this further, here is a direct proof of equality of Έ[Υ] and 
E[/(X)], with Y and f(X) 
as in (5.3). 
Theorem 82 Suppose the distribution function Fx is non-negative. 
Suppose 
f(x) and \f(x)\ are Fx-integrable on R T. Then y and \y\ are Fy-integrable on 
R and 
E[Y] 
= 
JRyFY(J) 
= 
JRTf(x)Fx(I) 
= 
E[f(X)}, 
m\\ 
= 
IR\y\Fy(J) 
= 
JRT\f(x)\Fx(I) 
= 
E[\f(X)\]. 
Proof. Using the notation of Theorem 79, for y G R define 
[ 0 
if 
0<y<uu 
9m(y) '·= I Uj if Uj <y < 
uj+u 
t o 
if y = f(x) > m. 
Thus, for m = 1, 2, 3,..., if 
xeA* = r 1 (Jj) - {χ : f(x) = y e]uj,uj+1] = Jj}, 
j = o, 1,... ,m2m, 
then gm(y) = UJ; and gm(y) is Fy-integrable on R for m = 1, 2,3, — As in 
Theorem 79, 
- 
m 2 m 
^ 
/ 9m(y)FY(J) 
- V 
UjPx 
[u3 < f{x) < uj+1] = / 
fm(x)Fx(I[N]), 
and the integrals are bounded above by E [|/(X)|] = J R T \f(x)\Fx(I). 
Because 
9m(y) converges monotonically to y for each ?/, the function yFy(J) is integrable 
on R. For each m, 
/ 9m(y)FY(J) 
= [ 
fm(x)Fx(I), 
so 
[ yFY(J) 
= f 
f(x)Fx(I). 
Similarly for \y\FY(J). 
Thus E[Y] = E[f(X)} and E[\Y\] = E[\f(X)\}. 
Q 

5.3. REPRESENTATION 
OF 
OBSERVABLES 
199 
The result holds in particular when / is the indicator function of a set. 
Also, it shows that the elementary and contingent forms of random variables 
are consistent in the sense that they yield the same expected values. 
Example 50 If X ~ x[R, Fx], so T consists of a single element and X is an 
elementary observable, then, taking f to be the identity mapping, application of 
Theorem 82 gives 
Y ~ y\R,FY] 
where FY(J) = P x [ / " V ) ] = Ρ χ [J] = [ 
lj(x)Fx(I). 
Jn 
Example 15 shows that fnlj(x)Fx(I) 
is not always equal to Fx{J). 
As in 
Example 15, let w G R be fixed, and define Fx(]u,v]) = 1 if u < w < v, with 
Fx(]u,v]) = 0 if v < w or u > w; so Fx is an atomic distribution 
function. 
Theorem 82 predicts that 
E[Y] = E[X], 
/ yFY{I) = [ 
xFx(I), 
Jn 
Jn 
so the fact that FY and Fx are different in this case should not affect the result 
of the expectation calculation. To verify this, choose a function δ(χ) > 0 con-
forming to ] — oo, a] and ]a, oo[, and choose a δ-fine division o/R. This choice 
ensures that each such division includes two point-cell pairs of the form 
(w,]u,w]), 
(w,]w,v\). 
The Riemann sum approximation for J RxFx(7) includes just one non-zero 
term, wFx (]w,v]), so 
E[X}= 
[ 
xFx(I)=w. 
Jn 
For any a < w, the Riemann sum approximation for J R l]afW](x)Fx(I) 
includes 
a term 
so 
FY(]a,w})= [ l]aM(x)Fx(I) 
= l. 
Jn 
For any b > w, the Riemann sum approximation for J R l]Wi^(x)Fx(I) 
includes 
terms 
l]Wib](w)FxQu,w]), 
l]w,b](w)FxQu>,v]), 
each of which contributes zero to the Riemann sum for this integral. Therefore 
FYQw,b])= 
[ 
l]wM(x)Fx(I)=0. 
Jn 
The Riemann sum approximation to the integral J yFY(I) 
includes one non-
zero term, 
wFY(]u,w]), 
=W, 

200 
CHAPTER 5. RANDOM 
VARIABILITY 
so 
E[Y] = JyFY(I) = w = f xFx(I) = E[X], 
as predicted by Theorem 82. 
O 
Next we show that if distinct point functions f\ and $2 are 
Fx-equivalent, 
and if the corresponding contingent observables fi(X) 
and f2(X) are expressed 
in elementary form Y\ = f(X\), 
Yz = /(^2), then the distribution functions 
Fyx and Fy2 are equal. 
Theorem 83 Suppose the non-negative distribution function Fx is continuous 
in the sense that VFX(E 
\ E) = 0 for each E G E(R T), and suppose the real-
valued functions /i and fa are Fx -measurable; and suppose /i and /2 are Fx -
equivalent; that is, 
Then, for for J G I(R), the distribution functions Ρ χ ο / " 1 and¥xof2~l 
satisfy 
P x o / f 1 ( J ) = P x o / 2 - 1 ( J ) . 
In other words, writing yj = fj{x), j = 1,2, the elementary forms 
Y1^y1\R,FYl], 
Y2~y2[R,FYl} 
of the contingent observables 
/ p d ) ^ h{x) [RT, Fx] , 
f(X2) 
~ f2(x) [RT, Fx] , 
respectively, satisfy 
FYl(J) = Fy2(J) 
for J e l ( R ) . 
Proof. 
The Fx-equivalence of /i and f2 means that V^-^Fx 
[RT] = 0· 
Theorem 37 then implies that, for every figure E € E(R T, 
/ (Λ(χ) - f2(x)) Fx(I[N}) = 0, 
f hWFxiW}) 
= ( 
h{x)Fx{I[N)). 
JE 
JE 
JE 
By Theorem 77 the F^-measurability of f\ and fa implies that, for J G I(R), 
Px°/r1W=Pxo/2-1(J). 
The final part follows from FYj (J) = Px o / _ 1 ( J ) for J £ I(R), j = 1,2. 
Q 

5.3. REPRESENTATION 
OF 
OBSERVABLES 
201 
Theorem 84 Suppose Fx is non-negative. If f\ = f^ then 
E [ | / i P 0 - f2(X)\] 
= Vih-h\Fx[RT] 
= 0. 
If in addition, either of \fi\ or I/2I is Fx-integrable then 
E[\f1(X)\]=E[\f2(X)\}. 
Proof. These results follow from Theorem 81. 
O 
Definition 42 Given a basic observable X ~ x [R T,Fx] and functions 
fj(x), 
we say that fj -4 0 ifVfjFx 
[RT] —^0 as j ^ 00. 
Theorem 85 If fj -4 0 as j —> 00 7 and if the contingent observables fj(X) 
are 
absolute random variables, then 
E[\fj(X)\]—>0. 
Proof. This follows from Theorem 81. 
O 
Definition 42 describes a possible form of weak convergence3 of sequences 
{fj} which may hold in situations where, for x G R T, the sequences 
{fj(x):j 
= 
1,2,3,...} 
fail to converge for individual x. 
In that case the next result gives conditions for convergence, for each J G 
I(R), of the distribution function values 
{FYj(J):j 
= 1,2,3,...} 
of the elementary observables Yj ~ yj [R, Fy.] whose contingent form is fj(X) c± 
fj(x) [RT, Fx]; with yj = fj(x) for x e RT. 
In other words, the values FY0(J) 
may converge even when the values fj(x) 
fail to converge. The following result gives sufficient conditions for this. 
Theorem 86 Suppose the non-negative distribution function Fx is continuous 
in the sense that VFx(E \ E) — 0 for each E e E(R T). Suppose fj is Fx-
measurable for j — 1,2,3,... and suppose fj -4 0. 
Write yj = fj{x) 
and 
Yj ~ yj [R,FYJ], SO YJ = fj{X), 
the former being the elementary form of 
the contingent observable fj(X). 
Then, for J =]iz,v] G I(R), the distribution 
functions Fyö satisfy 
Urn FYj(J) = F0(J) 
where Fo is an atomic distribution function with FQ(]U,V]) — I 
ifu<0<v. 
3This is used in Chapter 8 on stochastic integrals. 

202 
CHAPTER 5. RANDOM 
VARIABILITY 
Proof. By Theorem 77, FYj(J) = Ρ χ o f~x(J) 
for each j and each J G I(R). 
By Theorem 78, Fyj is a distribution function, with i^Fy.{J) 
= 1. Suppose 
tx > 0 and J = ]u, oo[ G I(R). Then, for each j , 
VfjF [RT] > u / 
l{fj>u)(x)Fx(I[N}) 
= uPx o / - i ( j ) = 
uFYj{J). 
Therefore, for fixed u > 0, we have Fy^J) 
< u~lYf0F 
[RT]> and 
^ £ - U o , 
Fyj(J)-+0 
as j —> oo. If v < 0 and J — ] — oo, v], a similar argument gives 
FYj(J) = / 
l(/i<t;)(x)Fx(/[iV]) < V / j i; [ R r ] -► 0 
JnT 
\v\ 
as j —> oo. For r = 1, 2,3,..., consider a sequence Jr = ]ur, vr] with ur < 0 < vr, 
ur increasing monotonically to 0 and vr decreasing monotonically to 0 as r —> oo. 
Since the functions Fy. are distribution functions, for each r we have 
FYj (]ur,vr]) -> 1 
as j —>> oo. Taking FQ to be an atomic distribution function, with 
i
l 
if u < 0 < v, 
0 if u < t> < 0 or 0 < u < t>; 
then, for J = ]u, v] with ?x < v < 0, 0 < u < i>, or w < 0 < f, we have 
FYj(J)-> 
F0(J) 
as j —>· oo. 
O 
This result can be expressed in terms of observables. Given a basic observ-
able X ~ x [RT, F x ] , let f0(x) = 0 for a? G R T, and let y0 = /oPO denote the 
(degenerate) observable 
Fo^0[R,F o] 
where Fo is the atomic distribution function of Theorem 86. 
Then Theorem 86 states that, for continuous Fx and Fx-measurable fj, if 
fj -4 0 then fj(X) 
—>· 0 as j —>· oo, in the sense that the distribution functions 
Fy^ converge to FQ. 
Now consider weak convergence of a sequence fj, not to 0, but to a function 
/. In accordance with Definition 31, given a basic observable X ~ x [R T,Fx] 
and functions f(x) and fj(x), we say that 
fi ^ f 
(5-4) 
if V(fj-f)Fx 
[ R T ] -^ 0 as j ^ oo. 

5.4. BASIC PROPERTIES 
OF RANDOM 
VARIABLES 
203 
Theorem 87 Suppose the non-negative distribution function Fx is continuous 
with VFX(E 
\ E) = 0 for each E £ E(R T). 
Suppose fj and f are Fx-
measurable for j = 1,2,3,... and suppose fj -4 / . 
Write yj = fj(x) — f(x) 
and Yj ~ yj [R, Fy.], so Yj — fj(X) 
— f{X), 
and Yj is the elementary form 
of the contingent observable fj{X) — f(X)· 
Then, for J =]u,v] £ I(R), the 
distribution functions Fyj satisfy 
lim FYj(J) = F0(J) 
where Fo is an atomic distribution function with FQ(]U, V]) = 1 if u < 0 < v. 
Proof. This follows from Theorem 86 when we replace fj by fj — f. 
Q 
5.4 
Basic Properties of Random Variables 
Suppose we have an observable X ~ x[R T,Fx], and suppose two different cal-
culations /i and /2 are performed on £, giving a pair of contingent observables 
h(X) 
and 
f2(X). 
Theorem 88 If fi(X) 
and $2{Χ) o,re random variables, then 
f(X) = f1(X) + f2(X) 
is a random variable, and 
E[f(X)} = E[f1(X)} + E[f2(X)}. 
Proof. We have 
krf(x) 
= 
JRT(h(x) 
+ 
f2(x))Fx(I) 
= 
JRT h(x)Fx(I) 
+ JRT 
h(x)Fx(I), 
giving the result. 
O 
A random variable is an observable with expected value. Theorem 89 below 
shows that the expected value of a non-negative random variable with non-
negative distribution function has an alternative method of calculation. Sup-
pose f(X) 
is a random variable and / is measurable, so it can be expressed in 
elementary form Y ~ y[R, FY\. Define 
s(v) := / 
FY(I). 
The existence of this integral, called the survival function, follows from the 
integrability of the distribution function Fy, and from Theorem 17. Suppose 

204 
CHAPTER 5. RANDOM 
VARIABILITY 
f(x) is non-negative and FY(] — oo,0[) = 0 so s(y) = 0 for y < 0. Theorem 89 
establishes that 
E[f(X)]=E[Y]= 
f s(y)\I\= 
[ 
s(y)\I\. 
Jn 
J[o,oo[ 
In actuarial use the random outcome y may represent the present age of a human 
subject, and use of the function s(y) in the calculation of expected lifetime may, 
in anticipation of death, seem counter-intuitive, prophetic, or even menacing. 
For such reasons, this calculation is designated in Muldowney et al. [176] as the 
Darth Vader rule. 
The proof in Hewitt [109] is quite difficult. To prove it by the Riemann 
sum method some lemmas are needed, in which f(x) and FY are assumed to be 
non-negative. 
Lemma 7 If 0 < v < oc, then, with 
μ(ν) := / 
yFY{I), 
μ(ν) := / 
yFY(I), 
J[0,v] 
J]v,oo[ 
these integrals exist and satisfy 
μ(ν) + μ(ν) = Ε[Υ}. 
(5.5) 
Proof. By Theorem 82, yFY(I) 
is integrable on [0, oo[ and 
E[f(X)] = E[Y] = [ 
yFY(I). 
J[0,oo[ 
The result then follows from Theorems 17 and 16 of Chapter 4. 
O 
Lemma 8 The function μ{ν) is monotone increasing to E[Y] as v —> oc, and 
μ(ν) is monotone decreasing to 0 as v —>· ex). 
Proof. We can write 
μ{υ)= 
l[0M(y)yFY(I) 
and μ(ν) = / 
l[ViQo](y)yFY(I), 
J[0,oo] 
J[0,oo] 
and since 1[ο,υ](ν)υ^γ(Ι) 
converges monotonically to yFY(I) in [0, oo[ as v —>> oo, 
the results then follow from (5.5) and Theorem 57 (monotone convergence). O 
Lemma 9 If μ(ν) < ε then the product vs (v) < e. 
Proof. Forming Riemann sums over any division of [v, oo[ gives 
ΣυΡγ(ΐ)>νΣΡγ(ΐ) 
and the result follows from this. 
O 
The proof of the "Darth Vader" rule uses the integration by parts technique. 
(For more details of this technique see Henstock [103].) 

5.4. BASIC PROPERTIES 
OF RANDOM 
VARIABLES 
205 
Theorem 89 Suppose the distribution function FY(I) is non-negative. If the 
random variable Y ~ y [Ωγ, Fy] has Ωy = [0, oo[, then 
E[Y] = / 
s(y)\I\. 
Proof. For cells J =}u,v] the survival function s(v) = f, 
riV(J) gives 
s(u) - s(v) = [ 
FY(I) - [ 
FY(I) = [ 
FY(I). 
J]u,oo[ 
J]v,oo[ 
JJ 
Every Riemann sum estimate of Jj FY(I) has the form ]T ^VW where the cells 
/ partition J. Therefore, by the additivity of Fy, 
^2FY{I) 
= FY(J) 
= J FY(I) 
and s(u) - s(v) = FY(J). 
Now suppose J has the form ]6, oo[. If s(oo) is defined to be zero, then, using 
Riemann sum estimates as before, 
s(b)-s(oo) 
= 
FY(J). 
Define 
(
s(v) — s(u) 
if 
I =]u,v], 
s(oo) — s(b) 
if 
/ =]&, oo [, 
so s(I) = —FY(I) 
for all intervals. Now define 
h(y, I) := -ys(I) 
- s(y)\I\ with h{y, I) := 0 if y = oo. 
Recall that |7| = v — u whenever / has the form ]u,v], and take \I\ = 0 whenever 
I has the form ]6, oo[. Since h(y, I) is the same as yFy(J) — s(y)|J|, the objective 
is to prove that L· < h(y, I) = 0. If / =]u, v] and y = u, then 
KvJ) 
= 
y(s{u)-s(v))-s(y)(v-u) 
— (v — u) (s(v) — s(u)) + us(u) — vs(v). 
If / =]u, v] and y = v, then 
HyJ) 
= 
2/(s(w)-s(v))-s(y)(i;-iA) 
= 
(v — M) (S(W) — s(v)) + us(u) — vs(v). 
If / =]6, oo[, with y = oo, then /i(y, J) = 0 by definition. Thus, for any division 
©of [0,oo[, 
(2>)£%,!)|<(2>)£|(t;-u^ 
(5.6) 

206 
CHAPTER 5. RANDOM 
VARIABILITY 
With ε > 0 given, choose υε so that μ(νε) < ε. Now define a gauge δ so that 
( e 
if y = 0, 
(%) < < min{y,e} 
if 0 < y < oo, 
I v'1 
if y = oo; 
so (5 conforms to [0, oo[, and any ί-fine division V of [0, oo[ includes a term 
(y,[0,v]) with y = 0. Then 
2> = {(0, [0, vi]),..., (j/, R ϋ]), (oo, ]v, oo[)} 
where v is the largest of those v for which the partitioning intervals I are 
bounded (that is have the form ]tx,v]), and satisfies v > νε, so, by the mono-
tonicity of β(υ), we have 
ε > μ(υε) > μ(ϋ), 
and hence, by Lemma 9, 
vs(v) < e. 
Let £ denote 
{(0,[0,t;i]),...,(2/,]ü,t;])}. 
Then ε is a ί-fine division of [0, v], and inequality (5.6) gives 
\(Ό)ΣΚχ,Ι)\ 
< (£)E\(v-u)(s(u)-S(v))\ 
+ 
\(£)E(us(u)-vs(v))\. 
The first Riemann sum on the right satisfies 
{8)Yt\(v-u)(a(u)-8(v))\ 
= 
{S)'£l\I\FY(I)<eYiFY(I)<e, 
and by cancelation the second Riemann sum on the right satisfies 
\(S) Y^ (us(u) — vs(v))\ = vs(v) < e. 
Thus, for every ί-fine division V of [0, oo[, 
(2>)5>(ff,/) < 2ε, 
giving the result. 
O 
At first sight, the idea that the expected value of a random variable can be 
established from the properties of the survival function s{x) suggests precog-
nition. It has "something of the night" about it. The following example [176] 
illustrates this. 
Example 51 "Use the Force, Luke!" 
Consider two random variables X 
and Y jointly uniformly distributed on the unit disk; that is, the region where 
x2+y2 
< 1. Pick a random point (x, y) in the unit disk, and let R be the random 

5.5. INEQUALITIES 
FOR RANDOM 
VARIABLES 
207 
variable for distance from the origin, so, with R ~ r[R, FR], r = 
f(x,y) 
\Jx2 + y2, then 
R = f(X,Y) 
= ^Jx2 + Y2. 
The expected value of R is given by 
m = / . ' , / 
1 
r ^ ^ 
^x2 
+ 
y2 
dydx. 
This, of course, can be calculated by transformation into polar coordinates. But 
it requires the student to expend effort, care, and time. On the other hand, for 
0 < r < 1, let A(r) denote the area within the unit disk but outside the disk of 
radius r centered at (0,0), so the probability that R exceeds r is A{r) divided by 
the area of the unit disk. That is, 
sR{r) = PR [R > r] = FR(R > r) = Ä 
= l^lfi 
= 1 - r2, 
π 
π 
with SR^r) = 0 for r > 1. Since R is non-negative with probability 1, 
E[R] = j 
sR(r)dr = J 
(1 - r2)dr = 1 - | = | -
Is this not conclusive evidence of the power of the Dark Side ? 
Q 
5.5 
Inequalities for Random Variables 
If p > 1 and q — 1 + (p + 1)_1 so p~l + q~1 = 1, and if x > 0 and y > 0, then 
— + - 
> xy, 
(5.7) 
p 
q 
with equality if and only if 
y = xp~\ 
that is, yq = xp. 
(5.8) 
This is Holder's inequality, proved in Littlewood [143]. The following theorem 
gives Holder's inequality for expected values of random variables. 
Theorem 90 (Holder inequality) 
Suppose Fx is a non-negative potentiality 
distribution function. 
Let f and g be non-negative functions defined for x G 
Ωχ = R T, let p > 1, and let q be defined by p~l + q~l = 1. If (f(X))p 
and 
(g(X))q 
are random variables then f(X)g(X) 
is a random variable, and 
E[f(X)g(X)} 
< (E[/(X)n* (EM*)]*)* , 
(5.9) 
with equality in (5.9) when 
(f(X))p 
. 
is constant Fx-almost surely. 
(5.10) 
(g(x))q 

208 
CHAPTER 5. RANDOM 
VARIABILITY 
Proof. Suppose αι,..., a m, ß\,..., 
/3m, a, b are non-negative numbers. Then 
(5.7) implies 
w, 
x 
(axj)p 
(byi)q 
{aXj){byj) 
< ^
^ 
+ 
^J-
p 
q 
with equality if and only if 
oPx* = b«y« 
so 
b« 
abJ2xjyj < ^ Σχ* + - Σν] 
j=l 
y 
3 = 1 
qu 
with equality if and only if 
apxP = bqyq-, for 
1 < j < m. 
The right-hand side of (5.11) becomes ρ~λ + q~x = 1 ii we take 
(5.11) 
(5.12) 
and then 
3=1 
j=l 
with equality when 
Ι/Ϊ 
(5.13) 
(5.14) 
This is Holder's inequality for series, from which the inequality for integrals is 
deduced. For each b > a > 0 and d > c > 0, the set 
( a < / < 6 , 
c < < ? < d ) ; 
K < / p < ^ , 
cq<gq<dP), 
is measurable by Theorems 76 and 72, so its indicator function is Fx-integrable. 
For each positive integer m let 
αά 
= 
j 2 " m , j = 0,l,2,...,m2™, 
f 0 
if 0 < / ( z ) 
<au 
fm(x) 
= 
< aj 
if dj <f(x)<a,j+i, 
1,2, ...,m, 
I 0 
if f(x) > m2m, 
( 0 
if 0 <g(x) < au 
9m(x) 
= 
< dj 
if dj < g(x) < aj+i, j = 1, 2,..., m, 
I 0 
if 0(a) > m2 m. 

5.5. INEQUALITIES 
FOR RANDOM 
VARIABLES 
209 
Then the product fm{x)g<m(x) is Fx-integrable since, by Theorem 72, the indic-
ator function of 
(a< f < 6, c<g<d), 
= (a < f < b) Π (c < g < d), 
is integrable. Also, for each x, fm(x)9m(x) 
is monotone increasing and con-
vergent to f(x)g(x) 
as m —► oo. By (5.7), 
and, by the hypotheses, the left-hand side is Fx-integrable. Therefore, by The-
orem 57 (monotone convergence theorem), f(x)g(x) 
is Fx-integrable, 
and 
obi 
f(x)g(x)Fx(I) 
< ^ 
/ 
(/(*))* Fx(J) + ^ / 
fo(s))*Fx(J), 
(5.15) 
which is the analog of (5.11). Since f(x) > 0 for x G R T, 
/ 
FFx 
= / 
(f(x))pFx(I) 
= 0 implies V / P F x(R T) = 0, 
and hence, by Theorem 32, (f(x))p 
= 0 Fx-almost everywhere, so f{x) = 0 
Fx-almost everywhere. Thus we can assume that J R T fpFx 
> 0, and similarly 
JRT gqFx > 0. Write 
h{x) = ^ (/(*))" + j {g{x))q - abf{x)g{x\ 
(5.16) 
so / R T hFx > 0. Thus, by Theorem 39, 
/ 
h(x)Fx(I) 
= 
VhFx(KT) 
with equality in (5.15) if and only if h(x) = 0 Fx-almost everywhere—that is, 
by (5.16) and (5.8), 
oP (f(x))p 
= bq (g(x))q 
Fx-almost surely. 
(5.17) 
This result holds for all values of a > 0 and b > 0. Taking the particular values 
given by 
a-P = [ 
fPFx, 
b-* = [ 
g«Fx, 
(5.15) becomes (5.9), and equality in (5.9) occurs if and only if (5.17) is true for 
the particular values of a and 6, giving (5.10). 
O 

210 
CHAPTER 5. RANDOM 
VARIABILITY 
Theorem 91 (Minkowski 
inequality) 
Suppose Fx(I) > 0 (I G 
1(HT)), 
f(x) > 0 and g(x) > 0 (x G RTy)7 and suppose p > 1 is constant. If 
(f(X))p 
and (g(X))p are random variables, then 
(f(X) + g{X)f, 
f(X) (f(X) + giX))"-1, 
g(X) (f(X) + g{X)f~*, 
are random variables, and 
(E\f{X)+g(X)]')t 
< (E[f(X)]»)i 
+ (E\g(X)]»)K 
(5.18) 
with equality in (5.18) when 
fix) 
9(X) 
is constant Fx-almost surely. 
(5.19) 
Proof. If p-1 + q~l = 1 then p(p - 1) = p. If / R T ( / + g)pFx = 0 then, since 
/ > 0 and g > 0, Theorem 39 and Theorem 32 imply that / = 0 and g = 0 
Fx-almost everywhere. Hence we can assume J R T (/ + g)pFx > 0. Assuming 
Fx-integrability of / ( / + g) p _ 1 and g(f + g)p~l, 
then, by Holder's inequality, 
!*TU 
+ 9)PFX 
= 
SKTfU + gy-'Fx 
+ S^gU 
+ 
gy-'Fx 
< (fnTfpFxy*(fnT(f 
+ 9)pFxy 
+ (kT9pFxy(fnT(f 
+ 9)pFx)<. 
Dividing by (/ R T(/ + g)pFx)~q gives (5.18). Equality occurs when / p ( / + #)~p 
is constant and gp{f + g)~p is constant, so f(x) (g(x))~ 
is constant Fx-almost 
everywhere. Therefore all that has to be proved is Fx-integrability of 
f{f-\-g)p~1 
and g(f + #)ρ~1. This is done by the methods of Theorems 79 and 90, using 
the functions fm(x) and gm(x) of Theorem 90. By inequality (5.7), 
fmifm+gruf-1 
< — + ( / " + ^ 
< F + 2P max{f, 5
P} < / P ( / P + 5 P ) · 
Hence 
Σ™*=ο ai( ai + afc) p _ l px [(aj < / < aj+i> ak<g< 
ajfe+i)] 
- 
/ R r / m i / m + ^ r 1 
< (1 + 2P)/ R T/P + / R T ^ F X < 2 P . 
By Theorem 57 (monotone convergence theorem), / ( / -f- g)p~l is integrable. 
Similarly for g(f + g)p~l · 
Q 
The following is known as Markov's inequality, and applies to non-negative 
random variables. 
Theorem 92 (Markov 
inequality) 
Suppose X ~ x[HT,Fx] 
is a joint-basic 
observable with Fx non-negative, and suppose f(X) > 0 is a joint-contingent 
random variable. Suppose a > 0. Then 

5.5. INEQUALITIES 
FOR RANDOM 
VARIABLES 
211 
Proof. By Theorem 76, the set 
A = (f(X) 
>a) = {xeRT: 
f(x) > a} 
is Fx-measurable; so the contingent random variable 1A(X) satisfies 
Ά 
> lA{x) 
for all x G R T; 
or ^ - 
> 1Λ(Χ). 
a 
a 
Taking expectations of both sides, 
^ C T > Ε[1Λ(Χ)] = / 
U(x)Fx(I) = Px[f(X) > a], 
as required. 
O 
Definition 43 If f(X) 
is a random variable then Var[/(Jf)], the variance of 
f{X), 
if it exists, is the expected value of the observable (f(X) — E[f(X)]) 
; 
that is, 
Var[/(X)]=E[(/(X)-E[/(X)]) 2 
Note that this permits both / and Fx to be complex-valued; and this is also 
the case in the following result. 
Theorem 93 If f(X) 
is a random variable and ifVai[F(X)] 
exists, then 
Var[/(X)]=E[(/(X)) 2]-(E[/(X)]) 2. 
Proof. By additivity of integrals on R T, 
Var[/(X)] 
= 
E [(/(*)-E[/(X)]) 2] 
= 
E [(/(X))2 - 2f(X)E[f(X)] 
+ E[/(X)]2] 
= 
E[(/(X)) 2]-2E[/(X)] 2+E[/(X)] 2 
= 
E[(/(X)) 2]-E[/(X)] 2, 
as required. 
O 
Theorem 94 If f and Fx are real-valued, if f(X) 
is a random variable, and 
ifVai[f(X)] 
exists, then 
E[(/PO)2] > (E[/(X)])2 , 
Var[/(X)] < E[(/(X))2]. 
Proof. 
The first of these is called the Cauchy inequality. Both of them 
follow from the preceding theorem. 
O 

212 
CHAPTER 5. RANDOM 
VARIABILITY 
Theorem 95 Iff and Fx are non-negative and if(f(X))2 
is a random variable 
then f(X) 
is a random variable and Var[/(X)] exists. 
Proof. In Theorem 90, take g(x) — 1 for all x and take p = q = 2. 
O 
Note that, for non-negative / and Fx, Cauchy's inequality is a special case 
of Holder's inequality. 
Theorem 96 (Chebyshev 
inequality) 
Suppose Fx is non-negative and 
f(X)^f(x)[RT,Fx] 
is an absolute random variable whose variance exists, and suppose a > 0. Then 
P*[(I/W-MI>«)] < 
^ ψ 1 
where μ = Ε[/(Χ)]. 
Proof. Using Theorems 76 and 72, the set 
A 
= 
( | / ( Χ ) - μ | > ο ) 
= 
ν(Χ)>μ 
+ 
α)η(/(Χ)<μ-α) 
= 
{x : f(x) > μ + a} Π {x : f{x) < μ — a} 
= 
{x:\f{x)-ß\>a2} 
is measurable and 
Ρ χ [ ( | / ( Χ ) - μ | > α ) ] 
= 
/ 
1A(X)FX(I). 
Then 
Var[/(X)] = 
/ 
(f(x)-ß)2Fx(I) 
> a2 [ 
lA(x)Fx(I), 
and the result follows from this. 
O 
5.6 
Joint Random Variability 
With several (perhaps infinitely many) jointly observed random occurrences we 
can have a contingent random value or datum (a real or complex number) which 
depends on the unpredictable data-values taken jointly by the underlying joint 
occurrences. This section examines essential features of the joint distribution 
function of a joint observable. 
Suppose, for j = 1,2, elementary observables Xj ~ ^-[Ωχ.,^χ.] have 
fi>Xj = R with distribution functions Fx. defined on I(R). Considered jointly, 
calculations can be performed on their joint outcome x = (χι,α^); such as 

5.6. JOINT RANDOM 
VARIABILITY 
213 
their sum f(x) = x\ + x2. Then, provided there is an accuracy potential or 
potentiality distribution function—call it Fx, or F(x1?x2)—for the joint occur-
rence x = (xi,x2), 
we have a contingent observable which can be denoted by 
f{X) 
= X\ + X2. 
Then the three individual observations can be viewed as 
follows: 
Xi 
^ 
x^ilx^FxJ, 
Ω Χ ι = Κ , 
X2 
~ 
x2 [Ωχ2, Fx2], 
Ωχ2 = R, 
f(X) 
~ 
f(x)[ilx,Fx], 
Ωχ =ΩΧι 
χ Ω χ 2 = Κ χ Κ . 
For this to have the required meaning—that is, for f(Xi,X2) 
to be contingent 
on the joint random variability of X\ and X2—the distribution function Fx of 
f(X\,X2) 
must reduce to the distribution function of X\ when we examine the 
potentiality that an observation of /(Xi,X2) yields a datum x\ in an interval 
I\ regardless of the value of the datum x2. In other words, the marginal distrib-
utions of f(Xi,X2) 
are the distributions of X\ and X2. Thus, for X = (Χχ, Χ2) 
to be an observable or joint observable, Fx must satisfy the consistency cond-
itions 
Fx(I1xR) 
= FXl(I1), 
Fx(RxI2) 
= Fx2(I2) 
for all /i, I2, and that is what is implied when by the statement that X = 
(Xi,X2) 
is & joint-basic observable. 
Theorem 97 Suppose X = (Χχ,Χ2) 
is a joint-basic observable. If fi(Xi) 
and 
/2(^2) are random variables then fi(Xi) 
+ 72(^2) is a random variable and 
E[Y}=E[f1(X1)} 
+ 
E[f2(X2)]. 
Proof. The observable X = (Xi,X2) 
has representation 
(xux2)[RxR,Fx] 
where Fx satisfies the above consistency conditions. Then, using Theorem 54 
(Fubini's theorem), 
E[/i(Xi) + /2(X2)] 
= 
JRxR(fx(xi) 
+ 
f2(x2))Fx(I) 
= 
/ R X R ( / I ( * I ) + h[x2))Fx{h 
x h) 
= 
JRXR fi(*i)FxVi 
x h) + fRxRh{x2)Fx(h 
x h) 
= 
JRxRfi(xi)Fx(hxR) 
+ 
JRxRf2(x2)Fx(RxI2) 
= 
JRfl(xi)FXl(h) 
+ 
JRf2(X2)FX2(l2) 
= 
E[f1(X1)]+E[f2(X2)}. 
O 
Having formulated the joint sample space and joint distribution function for 
the joint observable X — (ΛΊ, X2), we can, if we wish, treat each of Xi and X2 

214 
CHAPTER 5. RANDOM 
VARIABILITY 
not as elementary observables but as contingent observables in the joint sample 
space, by defining 
gi(X):=Xi, 
2 = 1,2. 
In other words, gi(x) = gi(x\,X2) = %i for x G Ωχ = Ωχ1 χ Ωχ2, and then the 
distribution function for both of the contingent observables h{(X) (i = 1,2) is 
Fx. 
Theorem 97 can be proved in these terms with little change in the steps 
of the proof. Alternatively, Theorem 97 can be regarded as a consequence of 
Theorem 88, by taking X = (X1,X2) 
and hi(x) = fi{g%{xi,X2)) = fi(xi), so 
hi(X) = fi(gi(X)), 
i = l,2. 
In any of these formulations, the essential step is the use of Theorem 54 (Fubini's 
theorem, Chapter 4). 
5.7 
Two or More Joint Observables 
The above is an indication of how a pair of joint observables is handled. What 
if there are three or more—or infinitely many—joint elementary observables? 
Suppose T is any finite or infinite set, and suppose {Xt : t G T} is a linked 
collection of elementary observables with 
Xt~xt[R,FXt\ 
where the real- or complex-valued Fxt is additive on disjoint figures Et G E(R), 
withF X t(R) = 1. 
In order to obtain a representation of such a joint measurement, a sample 
space and distribution must be defined for it. The joint measurement X — 
XT = (Xt)teT has possible joint occurrences, or joint data-values, 
x = XT = (xt)ter G Y[{nXt 
: t G T} = RT; 
so let 
Ωχ = Ωχτ = Y[{nXt : t € T} = RT 
be the sample space for the joint measurement. 
In order to qualify as a joint potentiality distribution function, a real- or 
complex-valued function Fx^— FxT, must be defined on the cells I = I[N] 
of I(R T) and on the figures E G E(R T); and it must be additive on disjoint 
figures, with 
F*(R T) = 1. 
In addition, for every subset T' of T, the linked collection XT1 = (Xt)teTf must 
be representable as a joint-basic observable. 
Thus Fx must have marginal distribution functions that are consistent, as 
in the case of the pair X = (Λχ,Λ^) of the previous section. This means 

5.7. TWO OR MORE JOINT 
OBSERVABLES 
215 
that there must exist a family T of distribution functions FxN defined on the 
finite-dimensional intervals Ι(Κ^) for N G Af(T), satisfying, firstly, 
FXN 
= FXt whenever TV = {t} 
(5.20) 
for any t G T. 
Secondly, if M and N both belong to λί(Τ) and if M = {tu 
proper subset of ΛΓ, then, for any cells 7χ,..., 7m of R, with Im+i 
R, we have 
1\ X · · · X i m X R T \ M = (h x · · · x Im x Im+i x · · · x 7n) x 
The set on the left can be denoted by I[M] = I(M) x R T\ M, while the equivalent 
one on the right can be denoted by I[N] = I(N) x HT\N. 
Since they are 
the same, both can be denoted as /. Then the following further consistency 
condition must be satisfied by T on such intervals: 
Fx(I) = FXM (I(M)) = FXN (I(N)) 
(5.21) 
for all M,N G M{T) with M C N, and all intervals I. If a family T satisfies 
this condition, then FxT(I), 
or FxT(I[M]), 
can, without ambiguity, be defined 
tobeF X M(/(M)). 
The above describes the necessary properties of a function FxT in order 
for it to qualify as a joint distribution function* of a joint measurement Χτ> 
The description is valid whether T is finite or infinite. (If T is finite, just take 
T = M0 (fixed) in conditions (5.20) and (5.21) above.) 
Definition 44 IfT contains more than one element and if FxT satisfies (5.20) 
and (5.21) then it is consistent. 
Definition 45 // T contains more than one element and if a real- or complex-
valued function FxT is consistent and additive on disjoint elements of E(R T) 
with FxT(HT) 
= 1, then FxT is a joint potentiality distribution function. 
Definition 46 // FxT is a (joint potentiality) distribution function, then 
X = 
XT~XT[RT,FXT] 
is a (joint) observable, with x = χτ as the (joint) observation (Outcome, or 
datum,). 
Suppose X = XT is a joint observable and suppose we have a function 
/ : R T h > R o r C , 
so f(X) = f(Xr) 
is a contingent observable with representation 
, i m ) is a 
• · = In = 
/ ( X T ) ~ / ( X T ) [ R T , F X T ] . 

216 
CHAPTER 5. RANDOM 
VARIABILITY 
In accordance with Definition 36, f(Xr) 
is a random variable if the expected 
value (or expectation) 
E [/(*)]:= / 
f(x)Fx(I[N]) 
exists. Definition 36 of a random variable dependent on joint occurrences is valid 
whether T is finite or infinite. (If T has more than one element then E [Χτ] is 
not defined—a joint-basic observable cannot be a random variable.) 
This definition differs from the definition of a random variable given by the 
traditional, axiomatic theory of probability, in that it does not require that / 
be measurable. All that is required is that / be an Fx-integrable function (in 
the non-absolute Henstock sense) on R T. So it is not required that |/| be Fx-
integrable on R T. This relaxation of the meaning of "random variable" makes 
it possible to extend the theory in Chapters 6 and 7. 
If a random variable f(X) is also an absolute random variable (so 
\f(x)\Fx(I) 
is integrable on R T), and if the distribution function Fx is non-negative, then 
Theorem 76 implies that the function / is Fx-measurable. Therefore measur-
ability of / is a consequence, not a pre-condition. 
It will be useful to further extend the notion of contingent observable by 
allowing dependence, not just on x G R T but also on TV G Λ/*(Τ), as follows. 
Definition 47 If 
/ : R T x M{T) H> R or C, 
then f(X,N) 
= /(Χτ,Ν) 
is a contingent observable with representation 
f(XT,N)~f(xT,N)[RT,FXT}. 
The observable f(Xr,N) 
is a random variable if the expected value 
E [/(*,#)]:= / 
f(x,N)Fx(I[N]) 
exists. 
The proofs of basic properties of random variables of the form f{X) are gen-
erally valid also for the form f(X, N). When Fx-measurability of / is required, 
then results from Section A.l can be used. 
Sometimes a joint distribution Fx is defined by means of given marginal 
distribution functions T. To illustrate, suppose a pair of random variables X\ 
and X2 each has sample space R or a subset of R, 
Xx ~ Xl\R,FXl], 
X2 ^ 
x2[R,FX2}. 
With T consisting of just two elements, a joint distribution function defined on 
E(R x R) can be formed by multiplying the two marginal distribution functions: 
F{h*h):=FXl{h)FX2{h) 
for / = h x h € I(R2)· 

5.8. INDEPENDENCE 
IN RANDOM 
VARIABILITY 
217 
This gives consistency (and also independence—see next section), since 
F ( / l X R ) 
= 
F X l(/i)Fx a(R) 
= 
F X l(/i), 
F ( R x / 2 ) 
= 
FXl(R)FX2(I2) 
= 
FX2(I2). 
The result is a joint observable X = (Xi,X2), 
with 
Ωχ = ilXl x Ωχ2 = R x R, 
FX(I) = F(I) = 
FXl(h)Fx2(I2)\ 
so we can write X ~ x[H x R, Fx]. 
Suppose T has two or more elements and suppose the elementary joint ob-
servables Xt are random variables. Then all of the observables, including the 
basic random variables Xt, can be regarded as having the same sample space R T 
and the same distribution function FxT. This is because contingent observables 
can be defined by 
9ί{%τ) 
= 
Xt 
for each 
XT G R T, 
so 
gt(XT) 
= 
Xt 
for each 
t G T. 
Then, for fixed r G T we have E [§τ(Χτ)\ = E [-XV]> because consistency of Fx 
implies that 
/ 
gr{xT)FxT(I[N])= 
f 
XTFXT(I[N})= 
[ 
XTFXT(IT). 
5.8 
Independence in Random Variability 
Definition 48 Two observables X\, X2 are independent if the distribution 
function Fx of the joint observable X = (X\,X2) 
satisfies 
F x ( / i x / 2 ) : - F x 1 ( 7 1 ) F x 2 ( / 2 ) 
for all I = h x h 6 Ι ( Ω Λ Λ ) ) . 
Theorem 98 Suppose X\andX2 
are independent observables, and fi(Xi) 
and 
f2(X2) are random variables. Then, with 
X = (XUX2), 
Fx(I) 
= 
FXl(I1)FX2(I2), 
the joint-contingent observable f(X) := fi(Xi)f2{^2) 
is a random variable and 
E [f(X)} = E \f1(X1)f2(X2)] 
= E [fi(Xi)} E 
[f2(X2)\. 
Proof. Since fi(Xi) 
and f2(X2) 
are random variables, the integrals 
/ fi(xi)FXl(h) 
and 
/ 
/ 2(x2)ix a(/ 2) 

218 
CHAPTER 5. RANDOM 
VARIABILITY 
exist. Then, using Fubini's theorem, 
E [ / W ] 
= 
Saxf{x)Fx(I) 
= 
IuXl xnX2 
h(xi)f2(x2)FXl(h)FX2(I2) 
= 
!nX2 h{x2) (faxi fi(xi)FXl(h)) 
FX2(I2) 
= 
Jnx2Mx2)E[f(X1)]FX2(I2) 
= 
E [/!(*!)] E[/ 2(X 2)]. 
O 
Definition 49 Given two random variables fi(Xi) 
and 72(^2) with expected 
values μι = E [/(X^)], 2 = 1,2, £/ie covariance of fi(X\) 
and /2PG) ^ 
Cov[/i(*i)./2(*2)] 
= ΕΚ/^Χχ) - μ ι ) (/2(Χ2) - μ2)] 
i/ ί/ie expected value of the product exists. 
Theorem 99 Suppose Χχ and X2 are independent observables. If fi(Xi) 
and 
h{X2) o,re random variables then their covariance exists and equals 0. 
Proof. By the independence of X\ and X2 the distribution function of the 
contingent observable (fi(Xi) 
- μι) (/2p^) - μο) is FXl(I1)FX2(I2), 
and 
Coy[f1(X1),f2(X2)] 
= 
Ε[(ΜΧ1)-μ1)(ΜΧ2)-μ2)} 
= 
JRXR ( / I ( « I ) - Mi) (h(x2) ~ M2) 
FXl(h)FX2(I2) 
= 
JR (/i(*i) - Mi) FXl(h) 
JR (f2(x2) - μ2) 
FX2(I2) 
= 
(JRf1(x1)FXl(I1) 
- μι) (JRf2(x2)FX2(I2) 
- μ2) 
= 
0. 
O 
Independence is useful as an abstract concept, but it is rarely—if ever— 
present in practice. In its absence, the correlation of observables /i(Xi), f2(X2) 
is one of a number of measures of interdependence in joint random variability. 
The correlation of observables /i(Xi), f2{X2) is measured by 
COV [/!(*!),/2(X2)] 
vAfiu-[/!(*!)] Var[/2(X2)f 
If the conditions of Theorem 99 are satisfied then the correlation is zero. 
Theorem 100 If Xi and X2 are independent observables and if fi{Xi) 
and 
f2{X2) are random variables with variances then the random variable 
f(X):=f1(X1) 
+ 
f2(X2) 
has a variance and 
Var [/!(*!) + /2(X2)] = Var [ / j ^ ) ] + Var 
[f2(X2)). 

5.8. INDEPENDENCE 
IN RANDOM 
VARIABILITY 
219 
Proof. We have /(*) = f{xux2) 
= h(xi) 
+ / a(x 2). Write μά = Ε[/,·(Χ,)], 
j = 1,2. Then 
(/(*)-E[/(x)]) 2 
= 
(/1(X1) + / 2 ( X 2 ) - E [ / 1 ( X 1 ) + /2(X2)])2 
= 
(fi(Xi) 
+ hiXi) 
~ E [/i(Xi)] + E [/2(X2)])2 
= 
((fl(Xl) 
- μι) + (f2(X2) 
- 
μ2))2 
= 
(MX,) 
- μι)2 + (f2(X2) 
- μ2)2 
+ 
2 ( / 1 ( Χ 1 ) - μ 1 ) ( / 2 ( Χ 2 ) - μ 2 ) 
= 
(/ι(Χι) - μι) 2 + (f2(X2) 
- μ2)2 + 
2f1(X1)f2(X2) 
- 2/1(Χ1)μ2 - 2/2(X2)/x1 + 2μχμ2. 
Therefore 
Var[/1(X1) + /2(X2)] 
= 
E [(/(*)-E[/(x)]) 2] 
= 
Vax[/i(Xi)]+Var[/2(X2)] 
- 2Ε[/1(Χ1)/2(Χ2)] + 2μ1μ2. 
Because Xi and X2 are independent, E [/i(Xi)/2(X2)] = μιμ2, and this com-
pletes the proof. 
0 
Suppose we wish to consider independence in the joint random variability 
of more than two (possibly infinitely many) basic observables {Xt : t G T} with 
representations 
Xt~xt\R,FXt], 
teT. 
Consider the joint outcomes x = XT = (xt)teT'<> and, for each N G λί(Τ) 
and 
each / = I[N] G I(R T), consider the accuracy potential or likelihood that x 
belongs to I. Denoting this potentiality by Fx(I), Fx is a distribution function 
if it satisfies the consistency conditions (5.20) and (5.21). In that case the 
measurement X is an observable with representation X ~ x[HT,Fx] 
and we 
can write 
X~x[RT,Fx}. 
If, in addition, Fx satisfies 
Fx(I[N)) = H FXt(It), 
V TV G Af(T), 
V I[N] G RT, 
(5.22) 
teN 
then the observables {Xt : t G T} are independent 
If T is a finite set, then, taking N = T above, a finite collection of basic ob-
servables Xj = Xj[R, Fx.} (j = 1,...,m) is independent if X = (ΛΊ,..., X m) 
is a joint observable (or joint-basic observable) X = X[R m,Fx] whose distrib-
ution function satisfies 
771 
ί χ ( / ι x · · · x Im) 
= Π 
Fxi (Ji) 
f o r e a c h / = -ϊι * · · · x Λη € I(R m). 
J'=l 

220 
CHAPTER 5. RANDOM 
VARIABILITY 
The results already proved for random variables contingent on m = 2 indepen-
dent joint-basic observables can be extended easily to random variables contin-
gent on any finite number m of independent joint-basic observables. 
Theorem 101 Suppose Χχ,..., Xm are independent joint-basic observables with 
X = (Xi,... ,Xm), 
and suppose / I ( X L ) , ..., fm(Xm) 
are random variables. 
Then 
f(X) 
:= /i(Xi)/ 2(X 2).../mW 
is a random variable and 
m 
E[f(X)\ = E[f1(X1)MX2)-'fm(Xm)] 
= Π Ε [ Λ Μ · 
Proof. Since X\,..., 
X m are independent the joint distribution function is 
771 
FX(I) 
= 
HFXM), 
3 = 1 
and therefore, using Theorem 54 (Fubini's theorem), 
E [/(*)] = JRm f(x)Fx(I) 
= 
/ R m fl (Xl)f2(x2) 
■ ■ ■ fm(Xm)FXl 
(h)Fx2 
(/ 2) · · · FXm 
(Im) 
= ΠΓ=ι/κΛ·(^)^(^·) 
= 
E [f1(X1)} E [f2(X2)} · · · E [fm(Xm)} , 
as required. 
O 
The following result does not require independence. 
Theorem 102 Suppose, for j = l,...,ra 7 fj(Xj) 
is a random variable cont-
ingent on the elementary observable Xj ~ Xj [R, Fx. ]. // 
f(X) 
:= Λ(Χ!) + f2(X2) 
+ · · · + fm(Xm) 
is contingent on the joint-basic observable 
X = 
(Xu...,Xrn)~x[Rn,Fx}, 
then f(X) 
is a random variable and 
m 
E[f(X)]=E[f1(X1) + f2(X2) + -^frn(Xrn)) 
= Y^E[fj(Xj)}. 
3 = 1 
Proof. For joint outcome x = (#i,..., xm) we have 
f(x) 
= fi(xi) 
+ · · · + 
fm(Xm)· 

5.8. INDEPENDENCE 
IN RANDOM 
VARIABILITY 
221 
Using Theorem 54 (Fubini's theorem), consistency of Fx implies 
JRmf(x)Fx(I) 
= 
fRm(h(xi) 
+ --- + 
fm(xm))Fx(I) 
= 
JRm fl(xi)FX(I) 
+ · ■ · + / R r a 
fm(Xm)FX(I) 
= 
JR h(xi)FXl 
(h) + ■■■ + JRm 
fm(xm)FXm(Jm) 
= 
E[f1(X1)] + --- + 
E[fm(Xm)}, 
as required. 
O 
Theorem 103 Suppose Χχ,Χϊ,..., 
Xm are independent basic observables, and 
suppose fj(Xj) 
(j = 1,2, ...,m) 
are random variables whose variances exist, 
then the random variable fx(Xi) + ■ · · + fm(Xm) has variance satisfying 
Var [MXi) + · · · + fm(Xm)} 
= Var [MXJ] + · · · + Var [fm(Xi)} ■ 
Proof. Given Xj ~ Xj [R, FXj ], by independence we have 
(X1,X2,...,Xm) 
= 
X~x[B?n,Fx} 
where 
m 
Fx(J) = n F * i ( J i ) 
f o r ^€l(R m). 
3 = 1 
Let f(x) = Σ™=1 fj(xj) so f(X) denotes the random variable Σ™=ι fj(Xj) 
with 
outcome or datum f(x). Then 
Var [/(*)] = 
E [ ( / P 0 - E [ / ( X ) ] ) 2 ] 
= 
E [(fx(Xl) 
+ ■■■ + fm(Xm) 
~ E [ / l ( X l ) + · · · + 
fm(Xm)})2} 
= 
Ε[(Σ,=1(Λ·(^·)-Ε[Λ·(Χ,·)]))2] 
= 
Ε[ΣΓ=1(Λ·(^·)-Ε[/^Χ,·)])2] 
+ E [ 2 ^ ( / , W ) - E \fi(Xi)])(fi(Xi) ~ E Ifi(Xj)})] 
= 
Σ ; = Ι Ε [ ( Λ · ( Χ , · ) - Ε [ / , · ( Χ , · ) ] ) 2 ] 
+ 2Ei^j E Kfi(Xi) - E IMXi)}) (fjiXj) - E [fjiXj)])] 
= 
Σ"=ι Var [fj(X3)} + 2 Σ ^ (Ε [/,(Χ;)Λ(^)] 
- 2E [/,(Χ,)] E [fjiXj)} + E [/,(*<)] E [fjiXj)}} 
= 
E ^ i V a r t / , · ^ · ) ] 
since E [£(*<)/,·(*,·)] = E [/»(Xi)] E [£(*,·))] by independence. 
O 
In Theorems 102 and 103 the sample space and potentiality distribution 
function for the calculation of E [/(X)] and Var [/(X)] are not the same as 

222 
CHAPTER 5. RANDOM 
VARIABILITY 
the sample space and potentiality function for the calculation of E [fj(Xj)] and 
Var [fj(Xj)]· But it is possible to formulate such results in such a way that the 
same sample space and potentiality distribution function are used throughout. 
To see this, suppose Xt (t G T) are basic observables with representations 
Xt ~ #t[R, Fxt] for t G T, and suppose X — XT is the joint observable 
XT = X ~ x[RT,Fx] 
with distribution function Fx satisfying the consistency 
conditions (5.20) and (5.21). 
Let M = {ti,..., 
tm} be any fixed finite subset of T. For brevity, write Xj 
for XT- and Xj for xt . Suppose fj(Xj) 
are random variables, 1 < j < m. Then, 
by the above results, 
E 
and if the observables X^ are independent with Fx(I[N]) = Y\teN FxtIt, then 
Var Σ/>■(**) 
For each t G T and M G N(T) define the projection functions 
pt(x)=xt 
and pM(x) = (xu · · · ,ffm), 
so 
R T 
n 
R M, 
XT 
^ 
(x!,...,x m). 
For j = 1,..., m and x = X T £ RT> define 
#(z) := fjiptjfaj)), 
9M(X) := f(pM{x)) 
= fi(xi) + · · · + fm(Xm)· 
Using the consistency conditions (5.20) and (5.21), 
fnTgj(x)Fx(I) 
= 
i^fj{xj)FXi{I0) 
and 
JnT9M(x)Fx(I) 
= 
/ R m(/i(xi) + --- + / m ( x m ) ) n ^ i ^ W , 
giving 
Σ»(*) = ΣΕ^-(^)] 
where the expectations on both sides of the equation are calculated using the 
same sample space R T and distribution function Fx for all the observables 
involved. Theorem 103 can be similarly re-formulated. 
The following result uses consistency of joint distribution functions, but does 
not require independence. 

5.8. INDEPENDENCE 
IN RANDOM 
VARIABILITY 
223 
Theorem 104 // Χχ,Χϊ,... 
are absolute random variables with non-negative 
distribution functions Fx0, then, for each n, X\ + Χ<ι Λ 
l· 
Xn is an absolute 
random variable. 
Proof. For j = l,2,...n the functions XjFx^Ij) 
and \XJ\FX.{IJ) 
are inte-
grable. Let T = {1, 2,3,...} and let 
(Xu...,Xn) 
~ 
(xi,...,a; n)[R n,F ( X l f... f X n )], 
XT 
= (XUX2,...) 
= X 
~ 
x[RT,Fx], 
denote the joint random observables. Write kj(x) = \XJ\. Then, by consistency, 
the contingent observables pj(X) = Xj and kj(X) = \Xj\ are random variables 
in R T since Xj ~ Xj[HT,Fx] 
and \Xj\ ~ |^j|[RT,Fx] are random variables. 
Therefore, by Theorem 13, 
Χι + ··· + Χ η, 
|Χ!| + ·.· + |Χη| 
are random variables, since 
/ 
(xi + --+Xn)Fx(I[N\) 
= J2 [ 
XjFXj(Ij), 
I {\xi\ + - + \xn\)Fx{I[N\) = J2 I \^\FXj{Ij). 
For fixed n, if either of the integrals 
/ 
\Xl + · · · + Xn\FiXlim.mtxn)(Ii 
X ' · ' X Jn), 
/ 
\X1 + ' ' ' + Χη|^χ(/[ΛΠ) 
exists, then, by consistency of the distribution function, the other also exists and 
they are equal; and this implies that X\-\ 
\-Xn is an absolute random variable. 
Existence is proved by expressing the integrand as the limit of integrable step 
functions, and then applying Theorem 61 (dominated convergence theorem). 
For r = 1, 2,3,..., let Γ e I(R) represent, for -r2r < s < (r - l)2 r, any of the 
cells 
] - o o , - r ] , 
]r,oo[, 
)s2~r, (s + 1)2^]. 
If Xj > 0, define yy 
as 
y{p=r 
if Xj>r 
and y{p = s2~r 
if 
Xje]s2-r, 
(s + l)2~r]; 
while if Xj < 0, define y^ 
as 
Vjr) = -r 
if Xj<r 
and yj.r) = (* + l)2 _ r 
if Xj e]s2~r, 
(s + l)2~r]. 
For r = 1,2,3,..., define fr(x\,... 
,xn) by 
/ r(xi,...,x n) := \y[r) + - - - + ^ r ) l 

224 
CHAPTER 5. RANDOM 
VARIABILITY 
For fixed n let Jr denote a cell of R n obtained by forming a product of Ir = Ij 
for any choice of JJ in each dimension j , j = 1,2,... n, and let P denote the 
partition {Jr} for all choices of components i j . Then, for fixed n, / r is Fx-
integrable, with 
/ 
fr(xu...,xn)Fx(I) 
= (V)Y,\y[r) + 
and, for each (xi,..., xn), 
fr(xi,...,Xn) 
-> \X1 + ' 
a s r - > o o . Since Fx is non-negative, 
n 
fr(xu 
.. .,xn)Fx(I[N]) 
< Σ 
\xj\FX(I[N})· 
and the latter is integrable. The result follows by Theorem 61. 
O 
5.9 
Laws of Large Numbers 
Suppose an experiment or measurement X has expected value E [X] = μ, and 
suppose the measurement is repeated successively in such a way that the out-
come of any instance of the experiment has no bearing on the conduct or 
outcome of any other instance. Each repetition then constitutes an indep-
endent measurement Xj, and the repetitions yield successive outcomes Xj, 
j = 1,2,3, 
It would not be surprising if, for each j , the sample mean 
xi H 
h xn 
μη = 
n 
of the outcomes were an approximation of E [X], the expected value of the 
measurement, and, if the number n of repetitions were to be increased, that 
this approximation should become increasingly accurate. In other words, the 
"sequence of sample means" {μη} may converge in some sense to μ as n —> oo. 
Laws of large numbers deal with the convergence of the partial sums of 
series of random outcomes Xj. Therefore they can be expressed in terms of 
the convergence of the partial sums of series of random variables, as follows: If 
Xi, X2J X3,... 
are independent random variables, each having the representation 
Xj ~ Xj [R,FXj] 
with FXl 
=Fx2=Fx2=.·, 
so each random variable has the same expected value μ, then the sequence of 
random variables Yj given by 
n 
converges in some sense to μ as n -> 00. 
■ + yP F(Xl,...,Xn)(Jry, 
+ Xn\ 

5.9. LAWS OF LARGE 
NUMBERS 
225 
A later section addresses a central limit theorem. In their usual forms the 
laws of large numbers and central limit theorems are concerned with sequences 
of independent, identically distributed random variables. In our terms, these 
are sequences Xj ~ xj [R, Fxj ] with non-negative distribution function FXl — 
Fx2 = · · ·; for which, writing T = {1,2,...}, the joint observable 
satisfies 
X = χτ 
= (χλ,χ2, 
·. ·) 
X ~ x[RT,Fx] 
with FX(I[N]) = J ] 
FXj(Ij) 
jeN 
for all TV e N(T) 
and all I[N] G I(R T). The assumption of independence 
ensures the joint observable is well defined. 
Without loss of generality each Xj can be replaced by 
A , · - E [Χ3·] 
In other words, it is assumed that the Xj are standardized, with E [Xj] = 0 and 
Var [Xj] — 1 for j = 1,2, 
Then the following summaries apply. 
1. Weak Law of Large Numbers: For every e > 0, 
lim Ρ χ 
> ε 
= 0. 
2. Strong Law of Large Numbers 
lim 
n—>·οο 
71 
1. 
3. Central Limit Theorem: 
' X i + ·■· + *„ 
lim P x 
n—foo 
-\/n 
€ J 
— 
/ 
y/2nJj 
exp(-y2)\I\. 
Suppose each of the observables 
ΑΊ + ···+Χ η 
lim 
n—>CXD 
-X"l H" ' * * -f ^"n 
-^1 + ' ' ' + Xn 
Π 
n->oo 
n 
' 
Λ/Ü 
is expressed in elementary form: 
Yn - j / n [R,F yJ , 
y - j/ [R,F y], 
Zn~zn 
[R,F ZJ , 
respectively. Then the above statements are expressible as statements about 
the distribution functions Fyn, Fy, and FZn of the elementary forms. 

226 
CHAPTER 5. RANDOM 
VARIABILITY 
1. Weak Law of Large Numbers: 
For any ε > 0, 
Fyn( ] — oo, — ε[) + Fyn( ]ε, oo[ ) —► 0 as n —>> oo. 
2. Strong Law of Large Numbers: 
FY(I) = 1 if and only if 
OeL 
3. Central Limit Theorem: 
lim FZn(J) 
= -^= 
[ exp(-y2)\I\ 
for each 
J e l ( R ) . 
n->oo 
^ τ τ 
J j 
With T = {1, 2,3, · · ·}, suppose that, as n -> CXD, the sequence 
2/n = Λι(ζτ) = ~ 
fal 
+ 
h 
Xn) 
n 
is convergent, with limit value y = f(xr), 
say. Then we have contingent ob-
servables 
fn(Xr) 
^ fn(xr) 
[RT, Fx] , 
f(XT) 
^ f(xT) 
[RT, Fx] 
with Fx{Ij 
x R T\^>) = Fx.(Ij) 
for j - 1,2,3,.... The elementary form of 
these contingent observables is 
with 
y n - 2 / n [ R , F y n ] , 
y - y [ R , F y ] 
y<n = fn(xT), 
y = f{xr) 
for x T G RT. 
The laws of large numbers provide information about relations between the 
distribution functions Fyn, Fy; as do the central limit theorems. 
Theorem 105 (Weak law of large numbers) Suppose {Xj : j — 1, 2,3,...} 
is a sequence of independent, elementary, absolute random variables, each having 
the same non-negative distribution function Fxj — F for j = 1,2,3,..., and 
each having the same variance σ2 and mean μ. Writing T = {1,2,3,...}, let 
X = XT — XT [RT> FX] be the joint-basic observable. Then, for any ε > 0, 
lim P x 
Χι + ·· 
Xn 
n 
μ >ε 
= 0. 
(5.23) 
Proof. We have a sequence of elementary, absolute random variables Xj, j = 
1,2,3,..., and a further sequence of contingent observables n~l{X\ + ··· + 
Xn), n — 1> 2,3, 
By Theorem 104, each member of the latter sequence of 
contingent observables is an absolute random variable. By independence, the 
distribution function Fx of the joint-basic observable X ~ x[RT, Fx] satisfies 
WlN])=UFXj(Ij)=YlF(Ij). 
j€N 
jeN 

5.9. LAWS OF LARGE 
NUMBERS 
227 
For given n define 
MX) ■■--
χ1 + ... + χη 
Since F is non-negative and fn(X) 
is an absolute random variable, 
'x 
>η 
is defined for any η > 0 and any i / £ R . Write pj(X) := Xj. Then 
E [Pj(X)} = [ 
Pj{x)Fx(I[N]) 
= [ x3FXj(I3) 
= E [Χό] = μ 
for all j , and similarly 
E [ / „ ( X ) ] = E 'Χι + ··· + Χη -it«™-^-. 
i = i 
for all n. Theorem 103 implies 
Var [fn(X)} = 5 1 Var [n" 1^·] = n" 2 ^ 
Var [X,·] = n" 2 ^ σ 2 = -
since the basic random variables Xj are independent. Given any ε > 0, Cheby-
shev's inequality (Theorem 96) implies 
0 < Ρχ[(|/ ηΡ0-μ|>ε)] < Vax[/n(X)] 
ηε 2* 
The result follows from this, since ηε 
2σ2 —» 0 as n —> oo. 
(5.24) 
O 
An n-dimensional version of the Chebyshev inequality can be used to prove 
the strong law of large numbers. 
Theorem 106 (Kolmogorov 
inequality) 
Suppose Xi,X2,...,Xn
 
are inde-
pendent, joint, elementary, absolute random variables whose variances exist, and 
whose distribution functions FXi are non-negative. Writing X = (Xi,..., Xn), 
then, for any e > 0, 
max 
l<p<n Σχ,-Σ,ν&' 
/ J 
j=1 
Proof. Write M = { 1 , . . . , n) and X = (Xu ..., Xn) with 
X~(Xl,...xn)\RM,Fx], 

228 
CHAPTER 5. RANDOM 
VARIABILITY 
where FX(I) = Π?=ι^(^) f o r I e ^B·")* 1J e J( R)· D e f i n e t h e following 
subsets of R M: 
sP ■■= {χ··\ΣΪ=ιΧί-Σΐ=ιν[χΑ 
with 
< ε, for 1 < i < p, 
Σ?=1^-Σ?=1Ε[^·]|>ε}. 
S 
''— Up=l *%» 
the sets 5 P being disjoint. The hypotheses imply that, for each j , Xj — E [Xj] is 
an absolute random variable. Therefore, by Theorem 104, J ^ = 1 (Xj ~ ^ [Xj]) 
is an absolute random variable, and, by Theorem 76, the sets Sp and S are 
Fx-measurable. 
Thus 
x 
max 
l<p<n 
3 = 1 
3 = 1 
> ε 
[jsp 
P=I 
. 
Px [S], 
exists. We have 
EU^lXj] 
= 
/ΗΜ(Σ;=Ι^'-Σ;=ΙΕ[Χ,·]) 2^(/) 
> 
/ R M ls(x) (Σ"=ι *ί - Σ"=ι E 
[Xj\)2Fx(I) 
= 
ΣΡ
η
=1 / RM iSp (x) ( Σ ; = 1 xi - Σ ; = 1 E [x,·])2 F X (/) 
= 
a + 6 + c, 
where 
a = 
Σ ^ ι ( / κ Μ ΐ 5 ρ ( χ ) ( Σ ^ ι ^ · - Σ ^ 1 Ε [ ^ ] ) ν χ ( / ) ) ) 
6 = 
Σ Ρ
η= 1[2/κΛ ίΐ5 Ρ(χ)(Σ^ 1^·-Σ^ 1Ε[χ,·]) 
x (Σ;=Ρ+1 ^- - Σ"=Ρ+ι Ε [^]) Fx(/)] , 
« = 
Σ Ρ " = 1 ( / Κ Μ 1 5 Ρ ( Χ ) ( Σ ; = Ρ + 1 ^ - Σ ; = Ρ + 1 Ε [ ^ ] ) 2 ^ ( / ) ) . 
Each term of expression c is non-negative. Each term of expression b is zero 
because of the independence assumption. Therefore 
EUy<*[Xj} > Σί=ι/κΜΐ5Ρ(^)(Σ?=ι^·-Σ?=ιΕ[^])2^(/) 
> 
e2EUPx[SP}=e2Px[S}, 
giving the result. 
O 
As in Theorem 96 (Chebyshev inequality), this result can be formulated for 
contingent random variables fj (Xj) instead of basic random variables Xj. 

5.9. LAWS OF LARGE 
NUMBERS 
229 
The Kolmogorov inequality is now used to prove two versions of the strong 
law of large numbers. 
Theorem 107 (First Kolmogorov) 
Suppose {Xj} is a sequence of independ-
ent, joint-basic, absolute random variables such that J^jli j _ 2Var [Xj] is finite. 
Then {Xj} satisfies the strong law of large numbers. 
Proof. Let T = {1,2,3,...} and let X = (XUX2,X3,...) 
with 
Xj ~ XJ [R, FXj] , 
X ~ x[RT, 
FXT}. 
Let gj(X) := Xj - E [Xj] and let 
M*) == \ (Σ*; - Σ Ε ^ ) = ^ί>(*)· 
The strong law of large numbers says that, as n —> oo, hn(X) 
tends to 0 Fx-
almost surely, and this is what must be proved. Given ε > 0, define the following 
subset of R T: 
B(e) := {x : 3 q = q(x) such that |/ιη(#)| < ε for all n > q(x)} . 
We have 
oo 
oo 
B(e)=[j 
f] {x : \hn(x)\ < ε} . 
q=l n=q 
Define 
Bm(e) := {x : max \hn(x)\ > ε, 2 m" 1 < n < 2 m} . 
As in the proof of Theorem 106, this set is Fx-measurable, so Ρ χ [ßm(s)] exists. 
Using Kolmogorov's inequality (Theorem 106), 
P x [Bm{e)\ 
= 
Ρχ [{x : max ± |^" = 19j(X)\ > ε, 2 — 1 <n< 
2m}] 
< 
P 
>x [{a; : max \Σ"=19j 
(X)\ > en, 2™"1 <n< 
2™}] 
x [{x : 
max.\Y™=1gj(X) >ε2 771—1 
o m - 1 < n < 2 7 ■)] 
< 
Px [{x : max | ^ = 1 9 j ( X ) \ 
> e2m~\ 
1 < n < 2 m}] 
ESiVar^·]· 
< 
£ - 2 2 2 - 2 m \^ 2 
Therefore 
Σ,p* [ß-(£)J ^ Σ £_222_2m ΣVar tei · 
m = l 
77i=l \ 
j ' = l 
/ 
As each term of the series is positive the terms can be rearranged, giving a series 
oo 

230 
CHAPTER 5. RANDOM 
VARIABILITY 
where, for m = 2,3,... and 2 m _ 1 - 1 < j < 271 
_.. 
16 
o™ 
16 
otj = ε 
so 
E
92-2(m+r) _ 
1 U p-2m ^ 
~ 3 ε 2 
< 3 e 2 j 2 ' 
oo 
-jß 
oo 
^ P x [ S m ( £ ) ] < ^ ^ r 2 V a r [ X , · ] , 
3 ε 2 · , 
771=1 
J = l 
which, by hypothesis, is finite. Therefore, by Theorem 131, there exists rao 
mo(x) so that 
max 
|ftn(aOI < £ 
2 m - 1 < n < 2 m 
for all m > rao- Thus Ρ χ [.Β(ε)] = 1 for every ε > 0. In particular, 
P X [Bik-1)} = 1 
for every positive integer A:, so 
x Π ^(
fc_1) 
. / e = l 
1. 
But if x G HfcLi B{k~l) then x G B(k~1) for any fc; so there exists p = p(x, k) 
such that, for all n > p(x, k), we have |/in(x)| < fe_1. In other words, for such 
x, /in(x) tends to zero. 
O 
If it is assumed, further, that the random variables Xj have the same distri-
bution function, the following result shows that the strong law of large numbers 
holds even when Σ°°=1 j - 2Var [Xj] is infinite or undefined. 
Theorem 108 (Second Kolmogorov) 
Suppose, for j = 1,2,3,..., the joint-
basic, absolute random variables Xj ~ Xj [R, Fxj ] are independent and identi-
cally distributed, with 
FXl(I) 
= FX2(I) = FX3(I) = ---
for each I G I(R). Then the sequence {Xj} satisfies the strong law of large 
numbers. 
Proof. The random variables Xj can be replaced by the gj(X) of Theorem 
107, so, without loss of generality we can assume E [Xj] = 0 for j = 1,2,3, — 
For J G I(R) let F(J) denote the common value of Fx.(J), each j . Then, with 
f(y) denoting an arbitrary function of values y G R, we have, for each j , 
f f(xj)FXj(Ij)= 
[ 
f(y)F(J) 

5.9. LAWS OF LARGE 
NUMBERS 
231 
whenever / is F-integrable on R. Let T, X, and hn(X) be as in Theorem 107. 
Define 
f
Xj 
if \XJ\ < j , 
0 
otherwise. 
Then for each x G R T, |ej(x)| < \XJ\ and ej(x) converges to Xj as j —)> oo, and, 
since \XJ\ is F-integrable on R, Theorem 61 (dominated convergence theorem) 
implies that Ε[ε^(Χ)] converges to lim^oo E [Xj] = 0 as j —> oo. Writing 
E [ej(X)] as μ^ we have 
^ Σ ^! = n Σ e^x) -^Σ^· + ^ Έ^ - e&)) + - Σ ^· 
For any given j , μη < j n _ 1 , so //j; —>· 0 as n —>> oo. Therefore n - 1 Σ™=1 μ^ —>· 0 
a s n - ^ o o . Given j G T we now show that, except for a set of x G R T of Fx-
potentiality 0, Xj φ ej(x) only for a finite number of values of j , which implies 
the convergence to zero, Fx-almost surely, of the term n~l Y^-I(XJ 
— ej(x)), 
and which, in turn, implies that, if either of 
1 
n 
i 
n 
3=1 
3=1 
converges as n -» oo , then for Fx-almost all x the other also converges, and 
then the two limits are equal Fx-almost surely. This is proved as follows. The 
hypotheses imply that, for each j G T, the functions Xj and ej(x) are Fx-
measurable in R T. Therefore 
oo 
oo 
^PxKXj-ejiX))] 
= 
£P*[(|Xil>j)] 
3=1 
3=1 
oo 
/ oo 
= Σ 
Σρ^[(Α^ι^·ι<*+1)] 
j=l 
\k=j 
= ] [ > + 1 ) Ρ χ Ρ < | ^ | < £ + 1)] 
fc=0 
oo 
= 
1 + ^λ;Ρχ[(Α;<|Χ,·| < * + !)]. 
fc=0 
Letting fj(x) := /c for /c < |XJ| < k + 1, /c = 0,1,2,..., we have 
oo 
E M*)] = Σfc p* Kfc ^ i^i <fc+!)] · 
fc=0 
Since / ^ s ) < | ^ | for all x, E []Χ^| - f3(X)] 
> 0 so E [|J^|] > E [£(Χ)]. Since, 
by hypothesis, \Xj\ is a random variable, E [\Xj\] < oo, so 
oo 
ΣΡχ[(Χΐ-βΐ(Χ))] 
3 = 1 

232 
CHAPTER 5. RANDOM 
VARIABILITY 
converges. Therefore, by Theorem 131 (first Borel-Cantelli) we can now, with-
out loss of generality, substitute ej(X) for Xj in hn{X) and seek to prove that 
hn(X) tends to 0 Fx-almost surely. We will show that 
oo 
^ r ' V a r f e p O ] 
is finite; and then Theorem 107 (first Kolmogorov), with (5.25), will give the 
result we require in the present theorem. Note that the ej(X) are independent 
absolute random variables since the Xj are, but the ej(X) are contingent and are 
not identically distributed. While Theorem 107 does not require the variables 
to have a common distribution function, it was proved for elementary random 
variables, not contingent ones such as ej(X). However, Theorem 82 says that a 
contingent absolute random variable can be given elementary form. Accordingly, 
represent the random variables ej(X) as elementary Yj ~ y3;[R, Fy. ], so Yj = 
ej(X). 
(Explicitly, 
FY(I)=lFxAln[-m 
^ 
H I ' 
3 
\ FXj (I n [-j,j]) + FXj (] - oo, -j[\I) 
+ FXj (]j, oo[\I) 
if 0 G I. 
Theorem 107 can easily be formulated for contingent variables.) First we show 
that Var [Yj], = Var [ej(X)], exists for each j . This follows from 
VarfopO] = E [(Yj - μj)2] < E [(Yj)2} < [ l{^j}(z)F(J) 
< j . 
Finally, we show that 
oo 
oo 
£ r 2 V a r K · ] , 
^ ^ r 2 V a r [ e , - ( X ) ] , 
3=1 
3=1 
is finite, so Theorem 107 gives the result required in the present theorem. To 
prove this, for k = 0,1, 2,... let 
afc+i = / 
l ] M + i ] ( z ) z F ( J ) + / 
l]_(fc+1)^fc](z)|z|F(J), 
so a,k > 0, Σ™=ι dk = fn\z\F(J), 
and, by the hypotheses, the series converges 
to a finite value. Also, 
/ 
l]k,k+i)(z)z2F(J) 
+ f 
l^k+^qWlzfFiJ) 
< (k + 
\)ak+1. 
Since Var [βό(Χ)\ < E [(e^X))2], we have 
oo 
oo 
« 
oo 
j 
£r 2Var MX)] < ΣΓ2 
Ιί-jJiWfFiJ) < 
Σ,Γ'Σ 
j=l 
7 = 1 
J
n 
j=l 
k=l 
3 
kak. 

5.10. INTRODUCTION 
TO CENTRAL LIMIT 
THEOREM 
233 
As each term is positive the terms can be rearranged without changing the value 
of the series. Thus, for the latter series, 
CO 
j 
CO / 
CO 
Σ^Σ^ 
= Σ [kakJ2r2 
j=l 
fc=l 
k=l \ 
j=k 
CO 
/ 
CO 
< Σ (ka^UU + l))-1 
fc=l \ 
j=k 
- f:(*£(j-jTi' 
k=l \ 
j=k XJ 
J 
= ^2kakk 
1 = y^Qfc 
fc=l fc=l 
which, by hypothesis, is finite. Therefore Σ^ΐχ j _ 2Var [ej(X)] is finite, and the 
result follows. 
O 
5.10 
Introduction to Central Limit Theorem 
The preceding sections were concerned with the nature and degree of uncert-
ainty or random variability exhibited by n~l(X\ + · · · + Xn) as n —>> oo. This 
corresponds to the question of the level of measurement precision or accuracy— 
the accuracy potential—that can be achieved by taking successive measurements 
XL, X2,...; 
and to the accuracy of estimates obtained by taking the arithmetic 
mean of the first n of them. 
It has been shown that, under certain conditions, n_1(XiH 
VXn) exhibits 
no random variability at all in the limit. Therefore division by n does not provide 
a "delicate" sense of the "variability" or accuracy potential of X\ Λ l· Xn as 
n increases. 
But, under some basic assumptions, the Chebyshev inequality (Theorem 96) 
implies that the "variability" of n~l(X\ H 
\-Xn) is of the order of -4=, which 
tends to zero as n tends to infinity. Thus, if n~1(Xi + · · · + Xn) is multiplied 
by y/n, some understanding of the behavior of X\ + · · · + Xn may be gained. 
Accordingly, one way to address the "variability" or inexactness in the sam-
ple mean is to examine the joint-contingent observable 
V^ (~(Xl + · · · + Xn)) =-^=(Χ1 + ·..+ Xn), 
\n 
) 
y/n 
which we denote by Yn, with Yn in elementary form; so 
4=(*i + · · · + Xn) = Yn * yn\R, FYn}. 

234 
CHAPTER 5. RANDOM 
VARIABILITY 
The following is information about the accuracy potential of the datum yn. 
Suppose {Xj}JL\ 
a r e independent, each having the same distribution function 
F = Fx, and hence the same mean μ, and suppose each of them has (the same) 
finite variance σ2. Then, for each J G /(R), 
lim F y n ( J ) = N ( J ) = N r i ( J ) = - 4 = 
/ V * < W | J | , 
™->°° 
2 
σν2π Jj 
(5.26) 
the normal distribution function with mean μ and variance σ2 which was intro-
duced in Example 47. 
This is the basic version of the central limit theorem, and it is proved in the 
next section. 
Laws of large numbers are concerned with the convergence of sequences of 
random occurrences y\, y2? 2/3> · · · of random variables Y\, Υ2,13, 
Central 
limit theorems are concerned with convergence of the distribution functions 
Fyx (J), Fy2 (I), FY3 (/),... of these random variables. 
First we give an example showing that convergence of sequences of random 
data values does not imply convergence of the distribution functions of the 
corresponding observables of which the data values are occurrences. 
The scenario is as follows. Suppose there is a sequence of random observables 
Xj ~ Xj[R, Fx·], with joint observable X = (Xi, ^2» -^3? · · ·)> 
X = 
{XuX2,X3,...)~x[RT,Fx], 
Fx,= 
FxT, being the joint distribution function. 
Suppose the sequence of 
observations or data values Xj converges Fx-almost surely to a value y G R. 
Then the datum y may represent a random occurrence of an observable Y ~ 
y[n,FY\. 
The following example shows that, in this scenario, the potentiality distrib-
ution functions Fxj do not necessarily converge to the distribution function 
FY. 
Example 52 For j — 1, 2,3,... define the distribution functions Fxj of indep-
endent observables Xj ~ Xj [R, Fxj ] by 
i
l 
if I =]u,v] 
with 
u<j~1(—iy<v, 
0 
otherwise. 
The joint probability distribution Fx is assumed, as such, to satisfy consistency. 
For instance, we could have 
Fx(I[N}) := J ] FxA'd) 
for 
each 
/ ( 7 V ) e X( R i V)> 
N 
e ^ ( T ) ' 
jeN 
which is the independence condition. Let Y be the observable with representation 
2/[R, Fy] where 
( 1 if I =]u,v) 
with 
u<0<v, 
FY(J) 
= < 
0 
otherwise. 

5.10. INTRODUCTION 
TO CENTRAL LIMIT 
THEOREM 
235 
The sequence of atomic distribution functions Fx1, Fx2, Fx3,... 
have support at 
1 
_1 
2 ' 
3 ' 
points —1, i, —-J, — Similarly Fy is atomic with support at point 0. Therefore 
Xj is deterministic with value j~1(—iy, 
and Y is deterministic with value 0. 
The joint datum (—1, | , — §>···) is certain; the probability of any other out-
come is zero. The joint occurrence or datum x = ( x i , ^ , ^ , · · ·) with Xj = 
j~x{—l)·7 
has Xj converging to 0 as j —> oo. 
Thus the sequence of observables {Xj} 
converges to the observable Y as 
j -» oo. On the other hand, any interval J =]0, a] (a > 0) has Fy{J) = 1, but 
the sequence 
FXl(}0,a[), 
FX2(}0,a[), 
FXs(]0,a[),... 
diverges. In particular, this sequence does not converge to Fy(]0,a[) = 1. Like-
wise, any interval J = ]&, 0] has Fy(J) = 1, but Fx^J) 
does not converge as 
j -> oo. 
O 
The following is a more detailed and formalized exploration of the notation 
and reasoning involved in Example 52. Let 
Λ 
\ ^ ' 
2 ' 3 ' * ' ' ' 
j 
->'—)' 
Thus x' is a potential joint datum of the observable X ~ X[R T,Fx] where 
T = {1,2,3,...} and, for each TV G λί(Τ) and each I[N] G I(R T), 
FX(I[N}) = FxMN)) 
= Π 
FXj{Ij). 
jeN 
Let S = {xf}. First we show that Ρ χ [S] = 1. Using Theorem 3, define a gauge 
7 in R T so that 7 conforms to both of the unbounded cells 
{x eKT 
: Xj > x'j, j G T} , 
{x G R T : Xj < x'p j eT} . 
Then x = x' if (x, I[N]) is 7-fine and x' G I[N}*. Let Τ>Ί be a 7-fine division of 
RT, 
νΊ = 
{{χ\ΐ1[Νι)),...,{χ^Γ'[Ν''])}. 
Then there is an integer p between 1 and q for which 
x' eP[N% 
x' = xp, 
FX(P>[N*]) = 1, 
FX(I*[N*]) 
=0forq^p. 
(V,)"£ls(x)Fx(I[N]) 
= FX(I*[N*]) = 1, 
/ 
l5(x)Fx(/[AT]) = 1; 
that is, P x [5] = 1. 
so 
Thus 
so 

236 
CHAPTERS. 
RANDOM 
VARIABILITY 
Therefore, almost surely (i.e., except for a set of Fx-potentiality zero), a random 
occurrence x = (a?i, χ<ι, Χ3,...) of X satisfies 
lim Xj = 0. 
ji->oo 
Let SQ- be the set of x G R T for which lim^oo Xj exists. Define 
_ / l[mj^ooXj 
itxeSg, 
[ 1 
otherwise. 
Then f(x) is an occurrence of the contingent observable /(:c)[RT,Fx], with 
f(x) = 0 Fx-almost surely, or 
Px [(f(x) = o)] = 1. 
Write 
SX = (Y = 0) = {0}. 
For I / G R define a gauge S(y) > 0 which conforms to ] — 00,0] and ]0, oo[; so 
that if (y, J) is 5-fine and 0 G J = J U J*, then y = 0. Then, for any (5-fine 
division T>s of R, 
(D5) J ] lsv(y)FY(J) = 1, 
P y [50
y] = f lsv(y)FY(J) = 1. 
Thus, except for a set of Fy-potentiality 
zero, a random occurrence y of the 
observable Y has y = 0. Therefore Y = 0 Fy-almost surely, or 
p y [(r = o)] = l. 
Therefore 
x 
lim Xj = Y 
= rY 
lim Xj - y 
1. 
5.11 
Proof of Central Limit Theorem 
In a reversal of Example 52—in which the data values converge but the distri-
bution functions do not—we now give conditions for the distribution functions 
of sequences of random variables to converge while the corresponding sequences 
of random data values or occurrences do not necessarily converge. 
Suppose Xj (j = 1,2,3,...) are independent, identically distributed elemen-
tary random variables with variance, each having the same distribution func-
tion Fxj = F, and having the same mean E [Xj] = μ and the same variance 
Var [Xj]= σ2. Then , with T = {1,2,3,...} and X = XT, X - X[R T, Fx] is 
a joint-basic observable. For n = 1,2,3,... define the contingent observables 
(ay/n)-1 
(Χλ + · · · + Xn - ημ). 

5.11. PROOF OF CENTRAL LIMIT 
THEOREM 
237 
The function / ( x i , . . . , xn) = (ay/n)'1 
(xi + · · · + xn) is measurable, so Theo-
rem 79 implies that (σ^/n)"1 (X\ + · · · + Xn) can be represented as an elemen-
tary observable Yn, 
Yn~Yn\R,FYJ. 
As n -» oo the sequence of random values (σ^/η)_1 (#ι -f · · · + xn) does not 
necessarily converge (though the strong law of large numbers tells us that 
(ση) - 1 (x\ + · · · + xn) converges to the deterministic value μ). 
The central limit theorem says that, for each J G I(R), the sequence of 
distribution functions FYn(J) converges to a distribution function Fz(J) where 
FZ(J) = N(J) = N ^ ( J ) = σ-1(2ττ)-ί j exp ( ^ ) 
\I\-
To prove this we use one of the standard methods—essentially, deducing the 
result from the convergence of the Fourier transform of Fyn(J) to the Fourier 
transform of Fz(J). 
Suppose F is a distribution function for I = I\ x · · · x In G I(R n). With 
ι — \[—\, define the Fourier transform of F to be F : R n i-)· R where 
F O i , . . . , sn) = / 
exp (L(S1X1 H 
+ snxn)) F{h x · · · x In) 
provided the integral exists. If X ~ x[Rn,Fx] 
is a joint-basic observable, with 
X — (Xi,... ,X n), let f{X) denote the contingent observable 
exp(^(5iXi H 
h 
snXn)). 
Definition 50 The characteristic function of X, denoted Fx(s) 
(s G Hn), is 
defined as E [/(X)] provided the expectation exists. 
Thus the characteristic function of X is the Fourier transform of Fx. 
Theorem 109 If Xi,... 
,Xn are independent random variables then 
n 
3 = 1 
for each s G R n. 
Proof. For each I G I(R n) we have 
n 
FXW^WFX^), 
i=i 
and the result then follows from Definition 50, using Fubini's theorem. 
Q 

238 
CHAPTER 5. RANDOM 
VARIABILITY 
Theorem 110 With n = 1, the Fourier transform N(s) of the standard normal 
distribution function 
N(J) = N°»1(J) = 
^ ^ e - ^ a | / | 
is N(s) = exp (-^s 2), s G R . 
Proof. Using Theorem 46, we have 
rxel* 
N(s) 
= 
/ 
eiSXN(I) 
/ei(R) 
exp (*,s:r) exp ( -^— ) \I\ 
■r*f 
\/2π Λ 
*er 
/ 
^2 
JGl(R) 
2 
—== exp ( — — ) / exp(^s:r)exp ( — ~(x — LS)2 ) m 
c.2 
= «P^-yJ- 
O 
Theorem 111 Suppose Fx is non-negative and X ~ x[R, Fx] is a random 
variable with variance. 
Differentiating with respect to s, the first derivative 
F'(0) = ιμ and the second derivative F"(0) = —σ2 — μ2. 
Proof. By hypothesis, μ := E [X] and σ2 := E [(X — μ)2] exist, and we have 
σ2 
= 
Ε[(Χ-μ)2] 
= E [X2 - 2μΧ + μ2] 
= 
E [X2] - 2μΕ[Χ] + μ2 = E [X2] - μ2, 
soE[X2] = σ 2 + μ 2 . We have 
exp(^x)Fx(7) = cos(sx)Fx(7) + £sm(s£)Fx(J), 
with 
cos{sx)Fx{I) 
< FX(I), 
sm(sx)Fx(I) 
< 
FX{I). 
Differentiating cos(xs) with respect to s involves taking a limit as a —» 0 of 
cos(xs + xa) — cosxs 
Λ . f x(2s + a)\ 
sin ^ 
i 
^ 
, = - 2 sin 
- ^ — 
'- 
*-, 
α 
\ 
2 
/ 
a 
whose absolute value is bounded above by a positive constant β as a —> 0. 
As ßFx(I) 
is a non-negative integrable function we can apply the dominated 
convergence theorem (Theorem 61) and differentiate with respect to s under the 
integral sign in JRcos(sx)Fx(I). 
Likewise for fnsm(sx)Fx(I), 
and, therefore, 
also for F(s) = Jnexp(tsx)Fx(I), 
giving 
dF(s) 
dt 
./R 
= i I xexp(tsx)Fx(I) 
= LE [X 
exp(tsX)]. 
Jn 

5.11. 
PROOF OF CENTRAL LIMIT 
THEOREM 
239 
Letting s = 0 gives the first result. If, instead of iterating the differentiation 
with respect to s, we treat the second derivative as a double limit, then we can 
again use a positive multiple of Fx (I) as the non-negative dominating function 
in the dominated convergence theorem, and get 
d2F(s) = - [ x2exp(isx)Fx(I) 
= - E [X2exp(tsX)] . 
JR 
dt* 
JR 
Letting 5 = 0 gives the second result. 
O 
Theorem 112 If X ~ x[R, Fx] is an elementary random variable such that 
Ρχ exists, and if a and b are real numbers, the contingent random variable 
aX + b can be represented as elementary random variable Y ~ ?/[R, FY], and 
FY(S) = 
ex.p(tsb)Fx(as). 
Proof. This follows from Definition 50. 
O 
Theorem 113 // Xi,X2,X$, 
· · · are independent random variables with com-
mon non-negative distribution function F = Fx. and with common mean μ and 
variance σ2, then, for each n, the contingent random variable 
{ay/n^)-1{X1 + · · · + Xn - ημ) 
has elementary representation as Yn ~ ?/n[R, Fyn], satisfying 
i^ n(0)=0, 
F£(0) = - 1 . 
Proof. By Theorems 82, 97, and 100, Yn has mean zero and variance 1, and 
the result then follows from Theorem 111. 
O 
Let Z ~ z[R, Nz] denote a standard normal random variable, so μ = 0, 
σ2 = 1), 
N z(J) = - l = ^ e - i * 2 | / | 
and N z ( s ) = exp ( - ± * 2 ) . 
Theorem 114 // Χ ι , Χ ^ - ^ , · · · ^re independent, identically distributed ran-
dom variables with mean μ and variance σ2, and, with 
Yn = (σνη)" 1 (Xi + · · · + Xn ~ ημ) 
as before, then, for each s G R , linin^oo FYU(S) — N^(s). 
Proof. 
Let F(I) denote the common distribution function Fx.(I), 
for j = 
1, 2,3,..., and let F(s) denote its Fourier transform. Since Yn has mean zero 
we can assume without loss of generality that μ = 0. Then 
^-jptesM'tes))' 

240 
CHAPTER 5. RANDOM 
VARIABILITY 
Again using Theorem 61 (the dominated convergence theorem) to take a limit 
under the integral sign, we have F(s) continuous in s, so a Taylor expansion of 
F(s) about s = 0 gives 
where ^i(s) ^ 0 a s s - > 0 . Therefore 
= 
e x p ( n l n ( l - | i + ^ ( ^ - ) ) ) 
= exp ( - I S
2 + g2 ( n " * ) ) , 
where g2(n~i) - ^ O a s n ^ o o . Letting n - ^ o o w e get the result. 
O 
To complete the proof of the central limit theorem, convergence of the dis-
tribution functions must be deduced from the convergence of the characteristic 
functions. This step is really the essence of this proof of the central limit the-
orem, and it is now addressed. Using the Riemann sum method of this book, 
the proof invokes some properties of monotone increasing functions. 
Theorem 115 (Inverse 
Fourier transform 
formula) 
If J is any interval 
ofH and with the degenerate intervals [u,u] denoted by Ju, then, for J = [u,v], 
the Fourier transform of an interval function F satisfies 
i 
i 
λ 
Γ p~LUt 
— 
P~LVt 
F(J) + -F{JU) + -F(JV) 
= - j R 
F(t)\I\. 
Proof. The latter integral exists as an improper Riemann integral, and hence 
as a Riemann-complete integral; but not as a Lebesgue integral. For a proof of 
the Fourier inversion formula using the Riemann-complete integral, see Theorem 
18 (page 1219) of Talvila [218]. 
O 
This formula implies that if two distribution functions have the same char-
acteristic function, the distribution functions are the same. 
If F(I) is a non-negative distribution function on cells I G I(R), then the 
cumulative 
distribution 
function 
F(x) := F(] — οο,χ]) is non-negative, 
monotone increasing, has an at most countable set of points of discontinuity, 
and F(x) —> 1 as x -> oo. The discontinuity points are jump points of F(x) and 
atoms of F(I). See Chung [41] for details of these properties. 
The following is a version of Helly's selection principle. 
Theorem 116 (Helly's theorem) If{Fn} 
is a sequence of non-negative distrib-
ution functions, there exists a non-negative monotone increasing point function 
f{x) and a sub-sequence {Fnk} such that the cumulative distribution functions 
{Fnk(x)} 
converge to f(x) at each continuity point of f. 

5.11. PROOF OF CENTRAL LIMIT 
THEOREM 
241 
Proof. Note that f(x) need not itself be a cumulative distribution function, 
because if Fn{x) = l[n?0o[(^) then f(x) = 0 for all x. Let Q = {qi,q2,q3, · · ·} be 
a countable dense subset of R. Then {Fn(qi)}^=1 
is a bounded subset of R, and 
therefore contains a convergent sub-sequence {Fni(qi)}™=1. 
Let y\ denote the 
limit as k —>· oo of this sub-sequence. Likewise {Fni (^2)}^=i is bounded so there 
exists a sub-sequence {n\} C {n\} so that Fn2(q2) —>· 2/2 as k —>· 00 and also 
Fn2 (qi) —>· y1 as fc —>> 00. Continuing, for j = 1,2,3,..., we find sub-sequences 
"-D {n3^1} D {n°k} D · · · for which 
Fnfaj) ~> Va a s k "^ °°> i ^ *■ 
Now consider Fnk{x) := Fnk{x). Define a point function / Q in Q by 
and for x G R, x Φ q, define f(x) by 
f(x):=mi{FQ(q):qeQ, 
q>x}. 
It is easy to see that / is non-negative, monotone increasing, and bounded above 
by 1. Suppose x is not one of the set of discontinuity points of /, a set which is 
at most countable. To show that Fnk (x) converges to /(#), choose 7*1, Γ2, r% G Q 
with 7*1 < Γ2 < x < Γ3 so that 
/(a) - ε < / ( n ) < /(*) < /(r 3) < /(*) + e. 
Since Fnk{r2) —► /Q(^2) > /( ri) w e c a n n n d sufficiently large A: so that 
/ ( x ) - e < F n f e ( r 2 ) . 
Since Fnk{r%) -> / Q ^ ) > f(rs) we can find sufficiently large k so that 
/(*) + 5>F n f c(r 3). 
By monotonicity of Fnk, 
KM) 
< Fnk(x) < Fnk(r3); 
so f{x)-e< 
Fnk{x) < /(*) + ε , 
giving the result. 
O 
Definition 51 A sequence of distribution functions 
{ίΊ(/),Γ 2(/),ρ· 3(/),...}, 
defined for each I € I(R), is tight if, for each ε > 0, ί/iere ezisis M so that 
F„(R \ [-M, M]) < e /or a// n. 

242 
CHAPTER 5. RANDOM 
VARIABILITY 
Theorem 117 (Prokhorov's 
theorem) 
If {Fn(I)} 
is tight, then there exist 
a distribution function F(I) and a sub-sequence {Fn]c(I)} 
such that, as k —>► 
oo, the cumulative distribution functions {Fn]c(x)} 
converge to the cumulative 
distribution function F(x) at each continuity point of F(x). 
Proof. By Helly's theorem {Fnk(x)} 
converges to f(x) at each continuity point 
x of /. Find M from the definition of tightness and find a continuity point y 
of / such that f(y) > M. (There must be such a y because otherwise the set 
of discontinuity points of / would be uncountable.) Then for all n we have 
Fn(y) = Fn(] - oo, y]) > 1 - ε, so 
f(y) = lim Fnk(y) > 1 - ε, 
lim f(y) = 1, 
k—»oo 
y—>oo 
so / is a cumulative distribution function and, for any interval 7, such as 
I = [u,v], we can define the distribution function F(I) of the theorem by, 
for instance, F(I) = f(v) — f(u). This gives the result. 
O 
Definition 52 A sequence {Fn(I)} 
converges vaguely if there exists a dense 
subset DofH 
such that {Fn(I)} 
converges whenever the endpoints of I are in 
D. 
Theorem 118 (Convergence 
theorem 
of Levy-Kramer) 
If {Fn(t)} 
con-
verges everywhere in R to f(t), 
and if f(t) is continuous at t = 0, then the 
sequence of distribution functions {Fn} converges vaguely to a distribution func-
tion F with \imn-+oo Fn(I) = F(I) whenever the end-points of I are continuity 
points of F; and F = f. 
Proof. For positive a let I x J denote intervals of the two-dimensional domain 
R x [—a, a]. We show that, for all n, 
Fn(R\[-ß,ß\)<U 
(l-Fn(t))\J\, 
and we show that the latter integral converges to 
f/ 
(i -/(*)) | j\ 
as n —> oo, and that f(t) —> /(O) — 1 as t —> 0. Let 
Hn(I x J) := Fn(I)\J\. 
Then Hn and eLXtHn are Stieltjes-complete integrable in S, and, by Fubini's 
theorem, 
f e^Hn(I 
x J) = / 
( [ 
e^\J\] 
Fn{I) = [ 
^ ^ F „ ( / ) . 
JS 
JR \Jl-a,a] 
j 
JR 
X 

5.11. PROOF OF CENTRAL LIMIT 
THEOREM 
243 
Therefore 
- / 
(l - Fn(t)) \J\ 
= 
2 - - [ Hn(I x J) 
" J[-a,a] V 
J 
a JS 
> 
[ 
ln\[-2,2](ax)Fn(I) 
= 
Fn(n\ 
2 2 
a' a 
We have Fn(0) = fKeL'°'xFn(Ix) 
= 1 for all n, so Fn(0) -> 1 as n ->- oo, giving 
/(0) = 1. The continuity of / at 0 implies there exists β > 0 such that 
\i-f(t)\<£-
if |i| < | . Therefore 
f / 
(WW)W <§/ 
7^1 = 1-
2 ■/[-■!,-II 
2 ^[-1,41 4 
2 
'?»?i 
We have 
l-^nWI = I / e^Fn(4)| < / Fn(Ix) = 1, 
so, by Theorem 61 (the dominated convergence theorem), 
lim / 
( l - F n ( i ) ) | J | = / 
(1-/(*))|J|· 
n—HX) 
/r 
2 2i \ 
/ 
/f 
2 
2l 
Therefore there exists no so that n > rto implies 
so 
and 
/ 
(l-Fn(tj)\J\- 
f 
(1-/(«)) |J| 
L 
n ' n J 
L 
n ' n J 
j 2 2 (l-Fn(t))|J| 
<ejff, 
2 
/ 
(l - F„(t)) \J\ < e. 
Therefore F n ( R \ [—/3,/3]) < z and it follows that the sequence of distribution 
functions is tight. By Helly's theorem and Prokhorov's theorem, there exists a 

244 
CHAPTER 5. RANDOM 
VARIABILITY 
sub-sequence {Fnk} converging vaguely to Fz(I), the distribution function of 
the standard normal variable. But since Fz(I) is continuous with respect to 
each of the endpoints of the intervals /, {Fnk(I)} 
converges to Fz(I) for all 
/. Now consider any convergent sub-sequence {Fmj (I)} converging to H(I) for 
every I. By the dominated convergence theorem the characteristic functions 
{Fmj (t)} converge to H(t), and H = /. Then the uniqueness property of the 
characteristic functions of distribution functions gives H(I) = Fz(I). 
Thus 
every convergent sub-sequence of {Fn} converges to Fz(I), so {Fn} converges 
to Fz- This completes the proof of the central limit theorem. 
Q 
5.12 
Probability Symbols 
Textbooks on probability theory usually use two or three basic symbols to denote 
probabilities; namely, P (and alternates such as Q), with symbols such as F and 
/ denoting probability distributions and their densities. In contrast, this book 
uses £, Fx, Ρ χ , and Vp x. 
There is a rationale for what might seem like a proliferation of symbols. The 
symbol "£" is intended to designate some underlying, "natural" phenomenon of 
likelihood or accuracy potentiality manifested in association with actual mea-
surement exercises X which generate unpredictable or uncertain data-values x; 
while "Fx" is a mathematical representation of this potentiality phenomenon. 
In other words, whenever "Fx" appears, it can be understood as "£". 
Also, it is difficult to formulate any conception of potentiality or probability 
except in association with observation of some generator (such as X) of random 
variability. In this book such "observables" are primary, while "potentiality" is 
a characteristic of measurement or observation and is not itself primary. That 
is why the notation Fx for potentiality includes reference to X. 
The function Ρ χ is the extension of Fx from cells (intervals) to measurable 
sets. Various examples have shown that these two functions do not always 
coincide on cells, and that is another reason for making a distinction between 
them. But it will be shown below that, for non-negative Fx, the function Ρ χ 
is equivalent to the probability function P of classical probability theory. 
The function Vpx(·) is a non-negative outer measure defined on all subsets 
of the sample space R T, whether they are Fx-measurable or not. 
To sum up: 
■ C denotes a natural phenomenon of likelihood or accuracy potential. 
■ Fx is the mathematical expression of potentiality associated with accuracy 
in measurement of data. It can be negative- or complex-valued. 
■ Ρχ is a mathematical construction that extends Fx to measurable sets, 
and, as will be seen in Section 5.13, it corresponds to classical probability 
P whenever Fx is a non-negative function of cells. 

5.13. MEASURABILITY 
AND 
PROBABILITY 
245 
■ Vpx is a mathematical construction that aids analysis. When potentiality 
Fx is non-negative, Vpx coincides with Fx on cells and figures, and, in 
that case, YFX is an extension of Fx to all sets. It coincides with Ρχ 
(and P) on Fx-measurable sets when Fx is a non-negative function. 
5.13 
Measurability and Probability 
Suppose Ω is a sample space in the classical Kolmogorov or axiomatic sense, and 
suppose (Ω,*4, Ρ) is a probability space. In other words, A is a sigma-algebra 
of subsets of Ω, and P is a non-negative, additive function defined on A, with 
Ρ(Ω) = 1 and 
(
oo 
\ 
oo 
3 = 1 
/ 
i = l 
for disjoint Aj G A. Since Ω and 0 both belong to A, this implies P(0) = 0 and 
0 < P(A) < 1 for each A e A. 
Then a function 
# : f i i - > R 
is a random variable in the classical, axiomatic sense if X is measurable relative 
to the probability space (Ω,,Λ, Ρ); that is, 
X-\l)eA 
for every interval I in R, and in particular for each cell / G I(R). To distinguish 
this from the usage of this book, we will describe A' as a P-random variable. 
With (Ω,Α,Ρ) 
given, and with P-random variable X given, define the cell 
function 
FX[T):=P{X-\I)) 
for each I G I(R). If X is a P-random variable then its P-expected value E(X) 
is defined as a Lebesgue integral: 
E(X) := f 
X(u)dP, 
perhaps with value equal to ±oo. 
The first task is to establish a relationship between these concepts and the 
meaning that has already been ascribed to them in this book. It will be shown 
that, when the values ±oo are excluded, a random variable in the traditional 
Kolmogorov sense is also a random variable in the Riemann sum sense of this 
book. 
Theorem 119 // (Ω, A, P) is a probability space, and if X is a P-random vari-
able, then X ~ x[R, Fx] is an elementary observable. 

246 
CHAPTER 5. RANDOM 
VARIABILITY 
Proof. We must prove that the cell function 
Fx : E(R) h4 [0,1] 
is a distribution function. If /' and /" are cells of I(R) such that I' Li I" = I ζ 
I(R), then, by P-measurability, X~X(I') 
and A"_1(/") are members of Λ, and 
if/', I" are disjoint then X~l{I') 
and A'_1(/") are disjoint; and, by additivity 
ofP, 
FX(I>) + FX(I") 
= 
Ρ{Χ-1{Γ))+Ρ{Χ-\Ι")) 
= 
Ρ{Χ-1{ΓΌΓ1)) 
= 
p(x-Hi)) 
= 
FX{I). 
This can be extended to finite unions of disjoint figures. Also, 
FX(R) = Ρ (ΛΤ 1^)) = 1, 
Fx{%) = P {X-\®)) 
= P(0) = 0, 
as required. 
O 
The next result shows that if X is a P-random variable then X ~ x[R, F%\ is 
a random variable in the Riemann sum sense. Consider any non-negative mea-
sure P defined on the measurable domain Ω with sigma-algebra of measurable 
sets Λ. We show that a measurable function X defined on Ω is Stieltjes-complete 
integrable provided the Lebesgue integral of X is finite. The method used is 
similar to the proof of Theorem 79. Consider the integrability of the function 
h{xj) 
:=xFx(I), 
(R,I(R)) A R . 
It is shown that if X is integrable with respect to the measure P, with finite 
Lebesgue integral JQ Χ(ω)άΡ, then the function h is Henstock integrable on R, 
and 
[ h{x,I)= 
[ X(uo)dP. 
Theorem 120 Suppose (Ω, A, P) is a measure space. Suppose X : Ω >—>- R is 
P-measurable, and suppose the Lebesgue integral / Ω X{uo)dP is finite. Then the 
function xFx(I), 
= Χ(ω)Ρ (Α' _ 1(/)) ; is integrable on R and 
[ Χ(ω)άΡ = ί Χ{ω)Ρ 
(X^il)) 
Proof. The first integral is obtained by the Lebesgue definition, and the sec-
ond one is to be read as a Henstock or Stieltjes-complete integral, equal to 
fnxFx(I). 
For ω G Ω, define 
f Χΐώ) 
if Χ{ω) > 0, 
Χ+{ω) = 1 
~ 
0 
if 
Χ(ω) < 0, 

5.13. MEASURABILITY 
AND 
PROBABILITY 
247 
and let X (ω) = \Χ(ω)\ — Χ+{ω). 
Given a positive integer ra, let 
aj=j2~m, 
j = . . . , - 2 , -1,0,1,2,..., 
and, for each x, let 
( dj 
if 
0 < α^< x < a j +i, 
j = 0 , 1 , . . . , ra2m, 
^ 0 
it 
x > m. 
Let gm(x) — ~aj+i 
if cij < x < a^+i < 0, and g^(x) = 0 if x < —m. Then for 
each ω G Ω and each x = A'(a;) G R, 
O < 0 + ( s ) < * + M , 
0<ff-(x)<AT-(a;). 
For x G R, both <7^(x) and g^(x) are monotone increasing as m —>· oo; and as 
m —> oo, for each x = Af(a;), 
^+(α:)=ρ+(ΑΤ(α;))^^+(χ)=χ = ^+(α;) 
if 
a: > 0, 
S W = 0m(#(cj)) -> <T(*) = M = ΛΤ (ω) 
if 
x < 0. 
Writing 
then, for each x G R, gm(x) —> g{%) = # as m —» oo. Write P m =]ÖJ, ÖJ+I] and 
771 
TM 
α+ 
= 
^ α , · Ρ ( α , · < χ < α , · + 1 ) 
= £ f t U * ) W i m ) , 
(5.27) 
i=o 
j=i 
0 
m 
a" 
= 
£ 
α, + 1Ρ(α,<ζ<α,· + 1) 
= ^ " ( ^ ( I ' " " * ) . 
j—-m 
j=l 
By existence and finiteness of the Lebesgue integral of X with respect to P, the 
suprema over m of α+, α^ exist and are finite, equal to a + and a - , respectively, 
and the Lebesgue integral satisfies 
/ Χ{ω)άΡ = α+ 
-a~. 
The Henstock or Stieltjes-complete integrals on R of the step functions 
g+(x)Fx(I), 
g-(x)Fx(I), 
exist; and, as m —> oo, 
/ g+(x)Fx(I) 
= 
5 > + ( * ) W " ) 
"»· <*+, 
/ g-(x)Fx(I) 
= 
J2g-(x)Fx(Pm) 
-»· 
a". 

248 
CHAPTER 5. RANDOM 
VARIABILITY 
As these integrals are bounded above by a + and a , respectively, monotonicity 
and Theorem 57 imply that the Henstock or Stieltjes-complete integrals 
/ 
lim g+{x)Fx{I), 
f 
lim 
g^(x)Fx(I) 
exist and are equal to 
lim [ g+{x)Fx(I), 
lim [ 
g-(x)Fx(I), 
which, in turn, are equal to a +, a - , respectively. Thus the function 
lim 0+(x) + lim g^(x) = g+(x) - g~(x) = g{x) = x 
ra—>oo ra—»oo 
is F^-integrable in the Henstock sense, and 
/ xFx(I) 
= a+ -a~ 
= / 
Χ(ω)άΡ, 
as required. 
O 
This proof works for all measurable domains Ω in which the Lebesgue integral 
is defined. The main point is (5.27) in which the simple functions used to 
define the Lebesgue integral of X with respect to P are used here to yield the 
Stieltjes-complete integral of X with respect to P(X~1(I)). 
Thus, in forming 
the division system, not in the domain Ω of X, but instead in its range space 
R, we are following the method of the Lebesgue integral itself. 
In other words, just as the Lebesgue integral / Ω XdP is obtained from the 
calculation (5.27) in the range space of X, the Henstock integral 
" f Χ{ω)Ρ{Α), 
= ί 
Χ(ω)Ρ(χ-\ΐ))," 
is obtained by essentially the same calculation (5.27) in the range space of X. 
But the more usual notation for the latter integral is fnxFx(I), 
and we retain 
this notation.4 
However, if Ω = R we can, instead, form a division system in the domain of 
X rather than the range, and, provided the measure P is defined on intervals 
and cells J C Ω = R, it can then be proved that the resulting Henstock integral 
/ X(OÜ)P(J), 
= [ 
X(x)P(J), 
exists, and equals the value given by the Lebesgue integral / Ω Χ(ω)άΡ. This is 
done in Davies and Schuss [50]. In Muldowney [164] the latter proof is extended 
to domains Ω = R T. 
4The calculation (5.27) gives the Lebesgue version of the integral of X with respect to P, 
as well as its Henstock counterpart. Thus there would be a degree of consistency in using 
the Lebesgue-Stieltjes notation " J 
xdFx" 
(instead of J X(u)dP) 
for the Lebesgue integral 
itself. 

5.13. MEASURABILITY 
AND 
PROBABILITY 
249 
Theorem 121 Suppose (Ω,*4, Ρ) is a probability space. Suppose X : Ω \-t R 
is a P-random variable with finite P-expectation Ε(Λ') Then the elementary 
observable 
X~x\R,Fx] 
is an absolute random variable, with expected value E[X] = E(X). 
Proof. By definition Fx = Ρχ. The result follows from Theorem 120. 
O 
If / : R i-> R is Fx -measurable, then / o X : Ω \-> R is P-measurable. When 
/ is the identity function on R, the P^-measurability of / o X is equivalent to 
the P-measurability of X. We now show that, for any P-measurable function /, 
the P-expectation of a P-random variable / o X is equal to the expectation (in 
the Henstock or Riemann sum sense) of the observable f(X), 
where Fx = F#. 
Theorem 122 Suppose X is P-measurable and f : R i-> R is Fx-measurable. 
If the Lebesgue integral fQ f(X)dP 
is not infinite, then the Stieltjes-complete 
integral Jnf(x)Fx 
(I) exists, and 
[ f(X(w))dP 
= f 
f(x)Fx(I). 
JQ 
JR. 
Proof. 
Again we use the method of Theorem 79. By hypothesis, / o X is 
P-measurable, and the function 
r f{x) if f{x) > o, 
f ix) := < 
\ 0 
if fix) < 0, 
is Lebesgue-Stieltjes integrable with respect to Fx(I). 
Given a positive integer 
m, let 
aj=j2-m, 
j - 0 , 1 , 2 , . . . , 
and, for each x, let 
fm(x) ~ I 
Then 
dj 
if 
dj < f(x) < öj+i, 
j — 0 , 1 , . . . , ra2m, 
0 
if 
f(x) > m. 
fm(x)<f+(x) 
and, for each x, fm(x) 
is monotone increasing as m —» oo. The set 
(dj < f o X < aj+i) = {ω : αό < /(Χ(ω)) < a j +i} 
is P-measurable in Ω for each j ; and, with 
(dj < f(x) < aj+1) = {x e R : αό < f(x) < a j + i } , 

250 
CHAPTER 5. RANDOM 
VARIABILITY 
we have existence of the following Stieltjes-complete integral on R: 
- 
m 2 m
 
r 
/ fm(x)FX{I) 
= 
] T / 
α31{α3<Ι{χ)<α^1φ)Ρχ{Ι) 
ra2m 
= 
Σα3ρ((α3 
<foX<aj+1)). 
3 = 1 
A s m ^ o o the right-hand side converges to the Lebesgue integral Jn / +(X)dP, 
by definition of the latter. 
The Henstock integral on the left-hand side is 
bounded above, for all m, by /Ω f+(X)dP. 
Then Levi's monotone convergence 
theorem (Theorem 57) gives the F^-iritegrability, in the Henstock sense, of /+. 
The P^-integrability (in the Henstock sense) of / " is proved similarly, and the 
result follows. 
O 
For the following, suppose a real-valued function R has domain R, so 
/ : R ^ R . 
Theorem 123 Suppose X is P-measurable and f is Fx-measurable. If the P-
expected value E(/ o X) is finite, then the observable f(X) — /(x)[R, Fx] is an 
absolute random variable (in the Henstock or Riemann sum sense), with 
E[f(X)}=E(foX). 
Proof. This follows from Theorem 122. 
O 
A consequence of Theorem 123 is that, given a probability space 
(Ω,Α,Ρ) 
of the axiomatic theory, and a P-random variable X : Ω i-> R, then sets A! G A 
yield subsets A — X(A') of R which are Fx -measurable in the Riemann sum 
sense. 
Theorem 124 Suppose (Ω,Α,Ρ) 
is a given probability space. If A! G A then 
A — X{A') is an Fx-measurable subset ofH and 
P(A') = PX[A]= 
[ 
lA(x)Fx(I), 
the latter integral being Stieltjes-complete 
Proof. This follows from Theorem 123 on taking / to be the indicator function 
1A(Z). 
O 
A converse to Theorem 124 would be, for non-negative distribution function 
Fx defined on I(R T), to construct a measure space (Ωχ,Α,Ρ) 
so that each 
Fx -measurable subset A of R T corresponds to a set A' C R, A! G A, with 
P(A') = PX[A]= 
[ 
lA(x)Fx(I). 
This is established in Theorem 130 below. 

5.14. THE CALCULUS OF 
PROBABILITIES 
251 
5.14 
The Calculus of Probabilities 
There is a familiar calculus of probabilities for probability spaces (Ω,,Α, Ρ). 
But, as indicated in Example 47, potentialities are in practice measured not 
by probability measures P in abstract5 probability spaces but by distribution 
functions Ρχ or Fx in sample spaces R T. Having established a relationship in 
Theorem 124, we now dispense with probability spaces and prove some results 
in the calculus of probabilities using the notation and methods of the Henstock 
integral. 
Throughout this section it is assumed that X = XT is a joint-basic ob-
servable XT — XT [&T,FxT] 
a nd the distribution functions Fx, = FxT, are 
non-negative on E(R T). Recall that 
'x [A]= 
[ 
1A(X)FX(I) 
J\RT 
for immeasurable sets A C R T. 
Theorem 125 For every Fx-measurable set A7 Ρχ[Α] > 0. 
Proof. This follows from Theorem 12. 
O 
Theorem 126 For the empty set 0, Ρχ[0] = 0. 
Proof. 10O) = 0 for all x £ RT, so (V) Σ U(x)Fx(I) 
= 0 for all division V 
of R T, giving Px[0] = E[10(X)] = / R T U{x)Fx{I) 
= 0. 
O 
Theorem 127 // Fx(E\E)=0 
for E e E(RT) then 
Ρχ[Ωχ}=Ρχ[ΈΙτ}^1. 
Proof. This follows from Theorem 74. 
O 
Theorem 128 If A\,.. .Am 
are disjoint Fx-measurable sets then Uj=i A? ^s 
Fx-measurable and 
x 
5Historically, in advance of the Henstock integral concepts, the simple function or step 
function calculation in (5.27) needed the Lebesgue measure space structure (Ω, A, P) to justify 
"passage to the limit". But, as Theorem 120 shows, the "passage to the limit" required by 
(5.27) can now be accomplished within the scope of the finitely additive cell functions 
Fx, 
and the intermediate step involving abstract measure space can be dispensed with. 

252 
CHAPTER 5. RANDOM 
VARIABILITY 
I 
Proof. Consider the case m — 2. For x £ R T, 
1A1UA2(X) 
= 1ΑΛΧ) 
+ 1 A 2 W 5 
and Theorem 13 implies 
PX[A1UA2] 
= 
[ 
lAlUA2(x)Fx(I[N}) 
{lAl{x) 
+ 
lA2{x))Fx(I[N]) 
= 
/ 
lAl(x)Fx(I[N])+ 
[ 
lA2(x)Fx(I[N]) 
= 
P x [ i i ] + P x [ A 2 ] · 
The argument can be easily extended to any finite union of disjoint Fx-meas-
urable sets. 
O 
The following theorem is an axiom in the classical theory of probability. 
Theorem 129 // the Fx-measurable sets Aj are disjoint for j = 1,2,3,..., 
then A = IJjli A7 ^s Fx-measurable and 
0 0 
τ>χ[Α} = Στ>χ[Α3). 
3 = 1 
Proof. Let Bm = |JJLi Aj. By Theorem 128, 
m 
As m —» 00, lß m(x) is monotone increasing to lA(x) 
for each x £ R T. There-
fore, by Theorem 57, the function lA(x) 
is Fx-integrable on R T and 
lim / 
lBrn(x)Fx(I[N})= 
[ 
lA(x)Fx(I[N}), 
so 
00 
ΣΡχ[ΑΑ=Ρχ[Α], 
as required. 
O 
The above results are valid for any non-negative, finitely additive distribution 
function FxT defined on E (R T)· For such a function there is a corresponding 
function Ρ χ defined on those subsets A of R T which are Fx-random events. 
That is to say, the indicator function lA(xT) 
is Fx-integrable on R T, implying 
in turn that A is Fx-measurable. Denote the class of Fx-measurable subsets of 
R T by Αχ. The following is a converse to Theorem 124. 

5.14. THE CALCULUS OF 
PROBABILITIES 
253 
Theorem 130 Suppose X = Χτ — XT [ R T
? ^ X T ] is a basic observable, and 
suppose Fx = FxT is non-negative and satisfies Vpx (E\E) = 0 for E G E(R T). 
Then the tuple (R T,*4x,Px) is a probability space. 
Proof. This follows from the preceding results. 
o 
The following pair of results is called the zero-one law. The Fx-measurable 
sets Aj are not assumed to be disjoint. 
Theorem 131 (First Borel-Cantelli) 
Given the hypotheses of Theorem 130, 
suppose the sets Aj G Αχ 
satisfy 
^ ρ χ μ , · ] < ο ο . 
V 
A := {x £ R T : 3 sequence jk = jk(%) for which x G Ajk, 
k = 1,2,3,...} 
thenPx[A] 
=0. 
Proof. We have 
oo 
/ oo 
*=1 \3=* 
SO 
Ρχ[Λ]<Ρχ IM <£ΡΧ[Α,·], 
3=1 
and the latter tends to zero as i tends to infinity. 
o 
Theorem 132 (Second Borel-Cantelli) 
Given the hypotheses of Theorem 
ISO, let {Aj}JL1 be a sequence of independent events in Αχ with 
£Pjr[A,-] = oo. 
J'=l 
If 
A := {x G R T : 3 sequence jk = jk(x) for which x G Ajk, 
k = 1, 2,3,...} 
thenPx[A] 
= 1. 
Proof. We have 
oo 
/ oo 
V4:=RT\A=(J ΓΙΟΛ) 
i—1 
\j=i 

254 
CHAPTER 5. RANDOM 
VARIABILITY 
so 
ρ χ Μ ] < £ ρ χ f](\AM 
i=l 
The events \Aj are independent since Aj are, and therefore, for any i, 
'x 
Π(\Λ·) 
3=1 
since Σ7=ι p * \Ai\ = °°-
0 
Example 53 Conditional 
expectation 
and conditional 
probability 
Theorem 130 enables us to use the function Ρχ as the probability measure P 
traditionally used in the familiar results and proofs of standard probability theory; 
though, as we have seen, some results can more easily be established from the 
finitely additive cell function 
Fx. 
For example, the probability calculus outlined above is extended by means of 
the concept of conditioning. Given an observable f(X) 
= f(X)[HT,Fx] 
and 
an event A G Λχ for which Ρχ[Α] > 0, the expected value of f(X) 
conditional 
on event A is 
E[/(X)|A]:= E[f(X)lA(X)} 
_E[f(X)lA(X)} 
E [ 1 A W ] 
PxlA] 
provided /(Χ)1Α(Χ) 
is a random variable. Conditional expectation defined in 
this manner satisfies familiar rules, such as: 
■ f(X) 
> 0 implies E [f(X)\A] > 0; 
■ for constant c, E [cf{X)\A] = cE [f(X)\A]; 
- E \f(X) + g(X)\A] = E [f(X)\A] + E [g(X)\A]; 
■ IfHT 
is expressible as the union of disjoint events Ai, ^ 2 , ^ 3 , . . . , then 
Ε[ΚΧ)} = 
ΣΡχ[Α3)Ε[ΚΧ)\Αι}. 
3 = 1 
These rules are proved by expressing the expected values as integrals in R T. 
Taking f(X) = 1B{X) in the definition of conditional expectation above, Ρχ 
is extended to conditional Ρ χ as follows. 
Given B C R T and an event A for which Ί*χ[Α\ > 0, the conditional 
potentiality that B occurs if A occurs is 
Px[B\A]:=E[lB(X)\A} 
E[1B(X)1A(X)} 
E[UP0] 
' 

5.14. THE CALCULUS OF PROBABILITIES 
255 
With AB = ΑΠ Β, we then have 
E[lAnB(X)} 
PX[AB}_ 
Px[m 
= 
EM 
= ~PxW' 
Ρχ[ΑΒ] 
= 
PX[A]PX[B\A]; 
Px\ABC] 
= 
Px[A}Px[B\A]Px[C\AB}; 
_ 
Ρχ\Α)Ρχ[Β\Α). 
P
x
[
m 
~ 
Ρχ[Β] 
' 
and ifHT is expressible as the union of disjoint events Ai, A2,A%,..., 
then 
Ρχ\Αί]Ρχ[Β\Αί) 
*χ[Α,\Β) 
= E£iP*[^]P;diW 

Chapter 6 
Gaussian Integrals 
This chapter deals with properties of exponential functions which are den-
sity functions for the distribution functions of important classes of observables. 
(Density function is a point function for which the distribution function is the 
Riemann-complete indefinite integral.) 
6.1 
FresnePs Integral 
The integrals 
/ exp(-z2)|J|, 
/ xexp(-x 2)|/|, 
/ x 2exp(-* 2)|J|, 
Jn 
Jn 
Jn 
are well known in their familiar notation, 
JZoe~x2(ix 
= Iv^, 
f°° xe 
x dx 
= 
0, 
J — oo 
' 
JZ0x2e-x2dx 
= 
\yfi. 
The latter integrals exist as improper or extended Riemann integrals. 
The 
familiar proof of the first one runs as follows. 
Lemma 10 
e y dy = -y/π. 
Proof. Let p > 0 and let 
Sp := {x e R x R : x = (χι,α^), 
x\ + x\ < p) · 
By Theorem 46 (integration by substitution), the Riemann integral 
ap= 
e~^+x22)dx1dx2 
Jsp 
A Modern Theory of Random 
Variation: With Applications 
in Stochastic Calculus, 
257 
Financial Mathematics, 
and Feynman Integration. 
First Edition. By Pat Muldowney 
Copyright © 2012 John Wiley & Sons, Inc. Published by John Wiley & Sons, Inc. 

258 
CHAPTER 6. GAUSSIAN 
INTEGRALS 
can be evaluated using polar co-ordinates (r, Θ): 
ap = IoW 
(loe~r2'rdr)de 
= 27r[-Klo 
^ 
Letting p —> oo, 
JC 
e-(x2i+x22)dXldx2 = π. 
(6.2) 
RxR 
For λ = 1,2,3,... let Jx = [-λ, λ] χ [-λ, λ]. Then 
= 
4 ( / 0
λ β - ^ ) . 
If equation (6.2) is correct, it implies that βχ converges as λ —> CXD, and 
lim /?Λ = 7Γ. 
λ^-οο 
Therefore JQ e~y dy converges as λ —> oo and 
Jo 
e 
y dy = -Λ/ΤΓ? 
as required. 
O 
Lemma 11 The Riemann-complete integral // G l/ R) e_2/ |/| exists and 
i 
/ei(R)' 
yer 
/€I(R) 
Proof. This follows from the preceding result since, as demonstrated in Section 
2.14 of Chapter 2, in one dimension every extended Riemann integrable function 
is Riemann-complete integrable, and the integrals are equal. 
O 
The result follows from the preceding one. But the proof used in obtaining 
the previous result, while valid for extended Riemann integrals, is not valid if 
the Riemann integrals in it are replaced by Henstock integrals. The problem 
lies in (6.2). The existence of this limit as p —>· oo is taken as being equivalent 
to the existence of the extended Riemann integral on R x R. If this is merely 
an interim construction used to prove Lemma 10 then there is no problem. 

6.1. 
FRESNEL'S 
INTEGRAL 
259 
But (6.2) cannot be interpreted as a Henstock integral without further proof. 
In Muldowney and Skvortsov [174] it is shown that, in domains R n with n > 1, 
the existence of such a limit does not, in general, guarantee the existence of the 
corresponding Riemann-complete integral on R n. 
However, the proof can be adapted so that it is valid for the Riemann-
complete integral on R x R in this particular case. The following amendment 
is the essential one. Write 
ap= 
[ 
ls p(x 1,x 2)e- ( x ? + x' )|/ix/ 2|, 
i R x R 
and note that the expression lsp(xi,Z2)e~^Xl+X2^\Ii 
x h\ 
■ is monotone increasing as p increases; 
■ is integrable on R x R for each p; 
■ has integrals which are bounded for all p. 
Therefore Theorem 57 (monotone convergence theorem) implies that 
lim lSp{xuX2)e-^+x')\h 
x h\, 
= e~^+x^\h 
x h\, 
is integrable on R x R, with integral equal to 
lim π f 1 — e~p ) = π. 
p-HX) 
\ 
/ 
With this amendment1 the "interim construct" is a Riemann-complete integral. 
Using Theorem 19, the Riemann-complete integrand e~x \I\ has indefinite 
integral H(I) = ν / π Ν ^ ( / ) , and H(I) has density function e~x . 
Here is a version of this result which is valid for the extended Riemann (also 
Henstock) integral, with a < 0: 
J-oo 
JieiCR) 
V ~α 
//GI(R) 
The following is FresneVs integral, an extended Riemann integral: 
Il
eLv2dy=\i^
=^ 
(6.3) 
where ι = Λ/—Τ· The following version of (6.3) will be needed: 
//ei(R) 
V ~c 
/
oo 
ry^ii 
ΓΤΓ 
ecy2dy= 
/ 
ecy2\I\ = J —, 
(6.4) 
-oo 
Jie 
1This illustrates the point mentioned in Section 4.10 of Chapter 4, about deducing a double 
integral from iterated integrals in a Tonelli-type scenario. 

260 
CHAPTER 6. GAUSSIAN 
INTEGRALS 
where c = a + ώ, with a < 0, 6 > 0, c ^ O . Before proceeding to proof of this 
version of Presnel's integral, consider aspects of (6.3). Write 
cos x2 + ι sin x2. 
Then (6.3) gives 
Λ/2ΤΓ 
/ 
cos£2|/| = / 
sinx 2|/| 
J[0,oo[ 
J[0,oo[ 
These integrals exist as extended Riemann integrals and as Riemann-complete 
integrals, but not as Lebesgue integrals. This is fairly obvious when the graphs 
of cos x2, sinx2 are considered—the graphs (see Figure 2.4) oscillate periodically 
with constant amplitude 2 but with period decreasing to zero as x —> oo. This 
is demonstrated in the following example. 
Example 54 The zeros of cos x2 are 
v/(2j + l)f, j = 0,l,2,.... 
Letting P =]UJ,VJ] where Uj = y/(2j + 1)§ and Vj = y/(2j + 3) ξ, consider 
ctj = / 
cosx2dx. 
Ja 
The series Σφ=ι aj ^s alternating, with \GCJ\ —>· 0, so the series converges. 
■ To show that the Riemann-complete integral exists on the domain [0, oo[, 
for each j and for each interval 
choose δ(χ) > 0 to conform to these intervals; that is, let 
δ(χ) < min {x - x/(2j + 1 ) | , W(2j + 3 ) | - rr }>; 
and for x = y/(2j + 1)§ let δ(χ) > 0 satisfy, additionally, 
and 
otj — (£<$) y^cosx 2|/| <e2~j. 

6.2. EVALUATION 
OF FRESNEL'S 
INTEGRAL 
261 
To see that the Lebesgue integral of cos x2 does not exist, consider the two 
series consisting, respectively, of the positive terms and the negative terms 
of Σ°1χ OLj- The area of an isosceles triangle with base P and vertex at 
±1 is 
Hy
(2i+3)W
(iy+i)i 
and 
Thus 
> i(V(2j + 3 ) f - y ( 2 j + l)§) 
= IvT (vWT3)-vW+^)) 
1^1 > 2 
1 
/TFI 
2 V 2 J 
for j > 4, and since Σ3~l 
diverges, J^Li ctj is not absolutely convergent. 
6.2 
Evaluation of FresnePs Integral 
To prove (6.4) with c = a + ώ (a < 0, b > 0, c ^ 0) and z = x + ty, the function 
exp cz2 is integrated along a closed contour. The integrals involved are Riemann 
integrals and extended Riemann integrals (in which a limiting value is obtained 
by letting a parameter in the domain tend to infinity), and can therefore be 
interpreted as Henstock integrals. We use the standard Riemann notation for 
the contour integrals of complex variable theory. The latter are Riemann, and 
therefore Riemann-complete, integrals. 
Theorem 133 If a < 0, b > 0, c φ 0, 
Proof. Write c — ßeL(*, let 77 = | ( π — £), and z = rei6'. Consider a contour 
consisting of the following boundary lines of a circular sector re10: 
1. with Θ = 0, the straight line z = 0 to z = r; 
2. with r fixed, the circular arc z = reL° to z = reir]\ 
3. and with Θ = 77 fixed, the straight line 2 = reiri to 2 = 0. 
On the second of these, the arc z = reLe, 0 < Θ < 77, we get 
[~\Cz2dz 
= 
Γ 
ecr2(cos2e+Lsin2e)recede 
Je=o 
Jo 
— 
/ 
e a r 2 (cos 29+L sin 20)+*,6r2 (cos 2θ-\-ι sin 
2e)re
LeM 
JO 

262 
CHAPTER 6. GAUSSIAN 
INTEGRALS 
Therefore 
I Π 
2 
I 
f 
/ 
e°z dz\ ^ \ 
\Jo 
\ 
Jo 
eCz2dz\ 
< / 
rer2(acos29-bsm2e)de 
and this tends to zero as r tends to infinity since a < 0, b > 0, c Φ 0. On the 
third segment peLT}, r > p > 0, 
Γ ecz2ciz = eLr> Γ exp (ßel<p2eL2ri) dp = el7] Γ 
e~ßp2dp, 
Jo 
Jo 
Jo 
and this tends to 
6ιη 
as r —» cxD. 
Lemma 12 7/ a < 0, 6 > 0, c = a + ώ Φ 0, 
ec((l/i-2/2)2 + (y2-2/3)2) 
o 
X 
dy2 = 
ec({yi~y2)2). 
Proof. This follows from Theorem 133 on completing the square in y2- 
O 
Here is another useful version of this. 
Lemma 13 Suppose, for i = 1,2, Ci = ai + ubi with 
Oi < 0, 
bi> 0, a φ 0. 
Then 
7 R V 
7Γ 
V π 
£ l p c i ( u - y ) 2
 4 / Z £ 2 
cafo-t;)5 dy 
exp 
-(w-v) 5 
π 
" 
V _ 7 rv ci+ C2) 
\ C i + c 2 
Proof. Proved similarly, by completing the square. 
O 
Lemma 14 Suppose c = a + ώ with a < 0. Then 
f 
J —< 
xecx dx = 0. 
Proof. 
Then 
Note that xecx 
= 
fv xecx dx 
Ju 
- ( -
= 
= 
= 
= 
-x)ec (-*r . Let u and v b epositive numbers.
±Γηβ™\2αχ)<Ιχ 
1 
2c 
1 
2c 
1 
2c 
ecx 2 
u 
Γ 
2 
, 
2 l V 
pOX +LOX 
\u 
eav2- \-ibv2 _ pau2+i-bw2 

6.2. EVALUATION 
OF FRESNEL'S 
INTEGRAL 
263 
which, because a < 0, tends to 0 as u —> —oo, v —> oo. Thus the extended 
Riemann integral / 
xecx dx = 0, so the Henstock integral exists and 
-oo 
rxei* 
[ xe
c*
2\I\= f 
as required. 
O 
xecx \I\ = 0, 
'Jei(R) 
Lemma 15 Suppose c = a + ώ with a < 0. Then 
/ 
x2ecx2dx 
= —-A — . 
Proof. If the Riemann integral JJ x2ecx 
dx converges as w-> — oo, I> —>> oo, 
then the extended Riemann integral J^° x2ecx dx exists (and hence the Rie-
mann-complete integral fnx2ecx 
\I\ = fjeim)x2eCX 
\I\ exists) and 
/
°° 
2 
fV 
2 
x2ecx dx = lim / x2ecx 
dx 
-oo 
Ju 
a s u - ^ —oo and v —> oo. Using integration by parts (Henstock [103], page 64), 
Γ 
2 
1 
/ ^ 
2 
/ x2ecx dx 
= 
— / z(ecx 2cx)cte 
- ΪΚ-/ Λ[· 
2 
2 
As u —> —oo and i; -> oo the term t>eCi; — weCM converges to 0 since a < 0; and 
JJ ecx dx converges to J-~ 
by Theorem 133, giving the result. 
O 
Without the smoothing convergence factor eax 
(a < 0), the function xeihx 
is not integrable. 
Lemma 16 The functions xexp (tx2) \I\ and x2 exp (tx2) \I\ are not integrable 
on R. 
Proof. Consider the function xcosx2, which is the real part of xeLbx 
with 
6 = 1 . The zeros of xcosx2 are 
(2j + l ) | , 
j = 0,l,2,..., 
and the integral of xcosx2 on [0, oo[, if it exists, must equal 
ri 
~ 
r(2j+3)f 
/ 
xcosx2dx + V^ / 
xcosx2dx 
Jo 
j==QJ(2j + l)% 

264 
CHAPTER 6. GAUSSIAN 
INTEGRALS 
if the series converges. But, by writing y = x2 and \dy = xdx, we get 
/ > ( 2 j + 3 ) i 
1 
(27+3)* 
/ 
xcosxdx 
= - [sini/jL., 1x? = ±1 
«/(2j+i)f 
2 
K 
)2 
depending on whether j is odd or even. This alternating series diverges, so 
xcosx2\I\ 
is not integrable. It can similarly be proved that xelbx \I\ and 
x2eLbx \I\ are not integrable on R. 
O 
Variants of the next theorem will play a big part in our investigations. It 
gives the indefinite integral representation of the Gaussian or Fresnel integrand, 
in Stielt j es or interval function form. Suppose n is a positive integer. With 
x — (#ι,..., xn) and /, J denoting points and cells of R n, write 
<Pc(x) = ΨηΑχ) 
= β°χ2 
= ecxi+-+c:r-; 
ΨΟ{Χ)\Ι\ 
= <Pn,c{x)\I\ = z™2^ 
= e c a 5?+- + c a sn|/ 1x... xj n| ; 
Vc(J) 
= 
ΨηΛΐ) 
= fj<Pc{x)\I\ = 
/ ; | ί κ η ) β ^ + ···+^|/|. 
(6.5) 
Theorem 134 For n = 1, a < 0, b > 0, and c φ 0, the functions φ0{1) and 
φ0(χ)\Ι\ are variationally equivalent in R. 
Proof. The cell function φα{1) is well defined, since the (extended) Riemann 
integral involved exists for each /, in consequence of Theorem 133. The result 
then follows from Theorem 18. 
O 
Theorem 135 If n = 1, a < 0, b > 0, and c Φ 0, the function φ0(Ι) = 
Jjecx 
dx is integrable on R, and /R<£cC0 = \/~^Z' 
Proof. This follows from Theorem 18 and Theorem 133. 
O 
Theorem 136 If n = 1, a < 0, b > 0, and c^O, 
then /Rx<pc(J) = 0. 
Proof. This follows from Theorem 44, with Lemma 14. 
O 
Theorem 137 lfn = l,a<0,b> 
0, and c^O, 
then ^χ2φα{1) 
= ^Jl^-
Proof. This is proved similarly. 
O 
Theorem 138 If n = 1, a = 0 and b Φ 0, the functions χφ€(Ι) 
and χ2φ0{1) 
are not integrable on R. 

6.3. FRESNEL'S INTEGRAL 
IN FINITE 
DIMENSIONS 
265 
Proof. If these functions were integrable then Theorem 44 would imply xeLX 
and x2eLX 
are integrable on R, contradicting Lemma 16. 
O 
As in Example 47, for n = 1 and J € I(R) define 
Nc(J) 
= / *«M|j| = [™?ß 
CX* , 
ax, 
N-(J) = i^ß\I\= 
/eXPC(Zf)2&, 
(6.6) 
N(J) 
= 
N _ i ( J ) , 
N"CT(J) 
= 
N ^ ( J ) , 
2 
whenever the integrals exist. N( J) is the standard normal distribution function, 
so X ~ x[R, N] is a normal or Gaussian random variable with mean 0 and 
variance 1, and 
Χ~χ[Κ,Νμσ] 
is a normally distributed random variable with mean μ and variance σ2. 
Theorem 139 If c = a + ώ, with a > 0, b < 0, c φ 0, and if μ and σ are given 
complex numbers, then the elementary observable X ~ x [R, Ν£σ] is a random 
variable with 
Ε[Χ]=μ, 
Var(X) = a2. 
Proof. This follows from Theorems 136 and 137. 
O 
6.3 
Fresnel's Integral in Finite Dimensions 
Next consider the integrability on R n of 
n 
φηΛ*)\Ι\ =e c^+- +^|/i|...|/„| = n e" ?l^l 
3 = 1 
where c = a + ώ with a < 0, b > 0, c Φ 0. 
Lemma 17 The iterated integrals 
exist and 

266 
CHAPTER 6. GAUSSIAN INTEGRALS 
Proof. We have 
= (k^\h\) 
(fRe^\I2\)--.(fRe^\In\) 
= ( Π ο β-ϊώΐ) (fZ 
tCXldXl) ■ ■ ■ (/Too ****») 
{ 
i—\n 
= (yfS) -
by Theorem 133. 
O 
Deducing the integrability on R n of φη^(χ)\Ι\ 
from this result requires use 
of some Tonelli-type arguments.2 We apply an extended-Riemann-integral app-
roach to the Presnel integrand <pniC(x) with x G R n. For r = 1,2,3,..., let 
Jr = ] — r, r] x · · ■ x]—r,r] G I(R n). First consider the case b = 0, so c = a < 0. 
Lemma 18 If a < 0 the positive function φη^α(χ)\Ι\ is integrable on Hn, and 
Proof. For r = l,2,3,..., Theorem 54 (Fubini's theorem) implies 
/ 
ljr<pnta(x)\I\=( 
[ 
e ^ l A - i i 
e~»|J„|), 
Jnn 
\J)-r,r] 
) 
\J\-r,r\ 
) 
(
I 
\ n 
r -> oc. Since ljripn^a(x)\I\ 
is monotone increasing as r —>· oo, Theorem 57 
(monotone convergence theorem) implies 
lim 13τφηα(χ)\Ι\,= 
φηα(χ)\Ι\, 
r—too 
is integrable on R n. Another application of Fubini's theorem gives 
as required. 
O 
Lemma 19 For n > 1 and c = a + ώ (a < 0, b > 0, c Φ 0), the function 
φη^(χ)\Ι\ 
is integrable on Jr G I(R n), and 
J ^ φη^χ)\Ι\ 
= j ^ ljr(x)<pniC(x)\I\ 
= ^ 
I · · · I fT 
f[ ecx* I dxn · 
2Fubini's theorem makes it possible to deduce iterated integrability from integrability on 
R n . In this case we wish to travel in the opposite direction. Theorems that justify this receive 
the designation "Tonelli". 

6.3. FRESNEL'S INTEGRAL 
IN FINITE 
DIMENSIONS 
267 
Proof. Integrability follows from the continuity of the integrand in the bounded 
domain J r, and Theorem 54 (Pubini's theorem) gives the iterated integrals. O 
Lemma 20 For n > 1 and c = a -j- ώ (a < 0, b > 0, c φ 0), the function 
ψη,ο{Ι) is integrable on Jr € I(Rn), and 
/ ψηΜ) 
= / 
1^{χ)ψηΑΙ) 
= 
\ " \ 
^l.c(-il) ) · · · ) <Pl,c(Jn). 
Proof. This is proved similarly. 
O 
Lemma 21 If a <0, b>0, 
and c ψ 0 then 
lim / 
ljr(x)<pnc(x)\I\ 
= ( J— 
) . 
Proof. For each j , Theorem 133 implies that the Riemann integrals 
Therefore 
J ljr{x)<pc{X)\I\ = f[Jr^Uxj 
-> (^l^j 
as r —>· oo. 
O 
Lemma 22 If a <0, b >0, and c φ 0 i/ien 
-^L
ijr(a:Vn'
c(/)=(\/5) · 
Proof. This is proved similarly. 
O 
Thus, the extended Riemann integrals of φη^{χ)\Ι\ and φη^(Ι) 
exist on R n. 
Also, it has already been proved that the iterated one-dimensional integrals of 
<Pn,c(x)\I\ and φη^(Ι) 
on R x · · · x R exist, with value 
π 
ΓτΓ 
( I π 
ο
χ · · ·
χ ν ^ = 1ν^ 
But integrability on R n of y?c(x)|/| and/or φη^(Ι) 
is not the same as either of 
these. 
To establish that φ€(χ)\Ι\ is integrable on R n requires further effort. The 
next result shows that, whenever a is not zero, this result can be obtained by 
familiar methods. But it should be noted that these methods do not work when 
a = 0. 

268 
CHAPTER 6. GAUSSIAN 
INTEGRALS 
Theorem 140 If a < 0 and b > 0 then φ€(χ)\Ι\ is integrable on R n and 
[ 
ljr(x)Vc(x)\I\ 
-+ /" 
Vc(ar)|J| = 
as r -» oo. 
Proof. For r = 1,2,3,..., the function ljr(x)(pc(x)\I\ 
satisfies 
\1^{χ)φ0(χ)\Ι\\ 
<φα(χ)\Ι\, 
and since the latter function is integrable on R n, Theorem 61 (the dominated 
convergence theorem) implies that the function 
lim lJr(x)(pc(x)\I\,= 
<pc(x)\I\, 
r—>oo 
is integrable on R n. Then, by Theorem 54 (Fubini's theorem), 
using Theorem 133. 
O 
These methods do not work when a = 0, and cannot be used to infer int-
egrability on R n of <pc(:r)|J| when c = ώ. Indeed, Muldowney and Skvortsov 
[174] give an example of a function h(x,I) 
in R n for which J R n 
ljr(x)h(x,I) 
converges as r —> oo, (so / is extended Riemann-integrable in some sense), but 
h is not Henstock integrable on R n (n > 1). Thus, when n > 1 extended Rie-
mann integrability (in the sense used here) does not necessarily imply Henstock 
integrability on R n. If, for n > 1, we wish to establish the integrability on R n 
of (pc(x)\I\, including the case a = 0, then other methods must be used. 
Figure 6.1 displays the graph, for values of x\ and #2 from — | π to | π . 
To display its oscillatory features the graph is tilted forward and away from 
the vertical. It is obtained by rotating Figure 2.4 through 360°. (A higher 
dimensional graphical analog applies to cos(a?f + · · · + x2^) when n > 2.) 
Figure 6.1 shows that, if polar co-ordinates (p, Θ) are used with x\ +x\ = p2, 
then the real part of β ^ + ^ ) ? 
SR (V(*?+^)) = c o s( x2 + χ2^ 
= c o s p2 5 
describes volumes that alternate between positive and negative values, so condi-
tional or non-absolute convergence in R2 may be expected. Instead of following 
this approach we continue with rectangular or Cartesian co-ordinates x^ and 
use the method of Lemma 18, adapted for non-absolutely convergent integrals. 
This means using Theorem 63, which is a version of Theorem 62; along with 
Theorem 64. 
Theorem 141 For any n, with a < 0, b > 0, c Φ 0, the function φη^{χ)\Ι\ 
is 
VBG*. 
m 

6.3. FRESNEUS INTEGRAL 
IN FINITE 
DIMENSIONS 
269 
Figure 6.1: Graph of cos (x\ + χζ). 
Proof. This is intuitively clear from inspection of the relevant graphs. To prove 
it analytically, note that 
\φη,ο(χ)\ = Π e x P a ^ y c°s2 bx* + sin2 bxj, 
3 = 1 
and by continuity of the functions involved, disjoint cells Kj £ I(R n) can be 
found, in which 
/ 
|<pn,c(aOI|/| < oo, j = 1,2,3,.... 
JKj 
The result follows from this (even when a = 0). 
O 
Theorem 142 For any n, with a < 0, b > 0, c ^ 0, the function φη^(χ)\Ι\ 
is 
integrable on R n. 
Proof. Write Jr =} - r, r] x · · · x] - r, r] G I(R n). As r -> oc, 
ljr(x)(pn,c(x)\I\ 
-> φηι0(χ)\Ι\ 
for each x e R n and each J e I(R n). Since <pn>c(a:)|J| is VBG*, Theorem 63 
gives the result. 
O 
Theorem 143 For any n, with a <0, b>0, 
c^O, 

270 
CHAPTER 6. GAUSSIAN 
INTEGRALS 
( 
i—\n 
Proof. By Lemma 21, J R n 1^{χ)φη^{χ)\Ι\ 
-» L / ^ J 
as r —> oo. Let ε > 0 
be given. Choose r = r£ so that r > r£ implies 
< ε. 
For each r choose a gauge £r on R n so that, for each <5r-fine division T>sr of R n, 
( ^ Γ ) Υ ] ΐ ^ ( ^ η , ο ( ^ ) | ί | - 
/ 
ljr(aj)y?n,c(a;)|/| 
Then, for r >r£, 
< e. 
|(%Γ)Σι^(χ)νηιο(χ)|/|-(^) 
Therefore, by Theorems 64 and 65, 
/ 
ljr(x)ipniC{x)\I\ 
-» / 
<pn>c(a;)|J 
<2ε. 
and 
as required. 
L *
n A x m = { ^ ' 
o 
Theorem 144 For any n, ifc — α + ώ with a < 0, b > 0, c φ 0, then φη^{χ)\Ι\ 
is variationally equivalent to 
φη^(Ι). 
Proof. 
For each E G E(R n), φη^(χ)\Ι\ 
is integrable on E1, with integral 
φη^(Ε). 
The result then follows from Theorem 18. 
O 
6.4 
Fresnel Distribution Function in Rn 
In this section a re-scaled or normalized version GC(J) of the Fresnel integrand 
φ0(Ι), — φη,ο(Ι), is formed so that GC(I) can be considered to be a distribution 
function, where φ0(Ι) is a Stieltjes or additive cell function. 
The first step is to construct modified versions of the functions φη^(χ)\Ι\ and 
φη^{1). 
If / G I(R n) has an associated point x e R n \ R n (i.e., if x has some 
component Xj = ±oo), then a value zero is sometimes assigned, by convention, 
to such cases. But in this case a distribution function in R n is the objective, 
so non-zero values are assigned. The resulting functions are denoted by <£n>c. 
They are used to define a "normalized" function GC(J); and the latter is shown 
to be a distribution function. 

6.4. FRESNEL DISTRIBUTION 
FUNCTION IN R] 
271 
Theorem 145 For any n, if c = a + ώ with a < 0, b > 0, c φ 0, and if 
E G E(R n), then φη^(χ)\Ι\ and φη,ο(Ι) are integrable on E. 
Proof. This follows from Theorem 17. 
O 
If x and J are associated in R n with x G R n \ R n (so x has some infinite 
component (s) and J has some unbounded one-dimensional component (s)), de-
fine each of the expressions (£>n,c(J), ΨηΑχ)\^\ 
to be ^φη^{1), 
which equals 
ί3φη,α{χ)\ΐν 
Thus> 
<if>nAJ) = / ΨηΑ1) 
= / ¥>n,c(ζ)|-Γ| = <Pn,c(X)\J\' 
(6'7) 
For all other J G I(R n) take 
<£nAJ) = VnAJ)i 
ΨηΑΧ)\Ά = <PnAX)\Jl 
To illustrate, consider the case n = 1. If J =]u,oo[ with u > 0, then 
and if J = ] — oo, υ] with v < 0, then 
viAJ) = J/y2dy 
= \^c-Pi,c(]vA) 
= l^c- 
£ ecy2dy. 
If J € I(R"), with n > 1, 
n 
J = l 
Theorem 146 For any n, ifc = a + tb with a < 0, b > 0, c Φ 0, then each of 
the functions 
is variationally equivalent to each of the others. 
Proof. Suppose n = 1. Consider the pair ψιΑχ)\Λ 
an<^ ΨιΑ^)- 
The function 
<Pi}C(ic)|J| is variationally equivalent to <£i)C(J). Given ε > 0, choose a gauge δ 
on R n so that 
(Ί>δ)Σ\φ1<(:(χ)\Ι\-<ΡιΛΙ)\<ε 
and so that, for I =] — oo, v] and v < — (£(—oo))- , 

272 
CHAPTER 6. GAUSSIAN 
INTEGRALS 
and so that, for / =]u, oo[ and u > (5(oo)) 
, 
Then 
(^Α) Σ 
IVi,c(a:)|/| - Vi,c(/)| < 3e, 
giving variational equivalence. Other cases are proved similarly. 
O 
We are now in a position to define a Presnel distribution function in Rn. 
Definition 53 If J e I(Rn) then 
GC(J) = G n, c(J) := ( Y ^ J 
VnAJ)· 
For E e E(R n) define GC(E) by additivity. 
Likewise, define 
gcO*0|J\ = gnAX)\J\ 
:= f Y -^ ] 
ΨηΑΧ)\Α' 
Theorem 147 For any n, if c = a + ώ with a < 0, b > 0, c φ 0, then G c is a 
distribution function in R n. 
Proof. Additivity follows from the definition. If V is a division of R n then 
(P)5>(/) = (^)V)^>,C(/) = ffi (^)" = i, 
as required. 
O 
Theorem 148 For any n, if c = a + ώ with a < 0, b > 0, c φ 0, then gc(a;)| J| 
and GC(J) are variationally equivalent. 
Proof. This follows from the variational equivalence of the functions <£n>c(o;)| J| 
and φηΑ1)' 
O 
Theorem 149 For a < 0, b > 0, and c = a + ώ φ 0, the function gc(x)\I\ 
is 
integrable on R n and 
[ 
gc(x)|/| = i· 
Proof. This follows from the preceding results. 
O 

6.4. FRESNEL DISTRIBUTION 
FUNCTION IN R N 
273 
Theorem 150 For a < 0, b > 0, c = a + ώ φ 0, and 1 < j < n, the functions 
Xjf£c{x\) · - - ·>%η)\Ι\ and #jgc(#i> · · · »#n)|-f"| are variationally equivalent in R n 
to XjGc{x\,... 
,xn)\I\ and #jG c(#i,... ,x n)|J|, respectively. 
Proof. This follows from Theorems 148 and 32. 
Q 
Theorem 151 For a < 0 and b > 0, the functions 
XjGc(xi,... 
,xn)\I\ 
and 
XjGc(xi,...,xn)\I\ 
are integrable on R n and 
[ 
x iG c(/) = 0, 
/ 
x)Gc{I) 
= 
^-. 
Proof. Using Theorem 149 with Lemmas 14 and 15, the functions 
SjgcO&i,.. · ,Xn)\I\ 
and 
x*g c(xi,... ,x n)|^| 
are integrable on R n and 
[ 
xjSc(x)\I\ 
= 0, 
f 
x2
jSc(x)\I\ 
= ^ . 
The result then follows from the preceding one, and it says that E [Xj] = 0 
and Var(Xj) = — (2c)_1 relative to the distribution function GC(J). It is not 
possible to include a = 0 in this result—see Lemma 16. 
O 
Theorem 152 Suppose a < 0, b > 0, c = a + ώ ψ 0, suppose T is finite, and 
suppose Ji and Jk are cells in R. Then 
r(Xl,...,xn)er 
tffvLljiMe^hlil 
/ 
ΐφ^'^"'^\ι\ 
= J/ieI(*} ^ΐί, 
' ', 
Jiei(Rn) 
f 
jL·) 
VV ~c) 
and 
•/R" 
(V^) 
Proof. The existence of the integrals in R n follows from Theorem 143 using 
Theorem 17. Their values are obtained by iterated integration. 
Q 

274 
CHAPTER 6. GAUSSIAN 
INTEGRALS 
6.5 
Infinite-Dimensional Fresnel Integral 
This section extends the Fresnel construction to a domain R T for any set T, 
including T infinite; and investigates integrability of particular functions on this 
domain, with a view to applications3 in the study of random variability. 
The integrator function will often involve some "volume" function defined 
for cells 
7[7V]GI(RT), 
N = 
{tu...,tn}eAT(BT). 
The most basic of such functions is obtained by multiplying the lengths of the 
finite edges Ij =]UJ,VJ] 
(UJ and Vj both finite): 
0 otherwise. 
An integrand in R T might then take the form 
h(x,N,I[N]) 
= 
f(x)\I[N]\ 
for some point function /(#). Because R T is unbounded in each dimension it is 
easy to see that constant functions f(x) are not integrable on R T with respect 
to \I[N]\ unless, for instance, f(x) is equal to zero for all x. 
To build up a stock of functions which, for infinite T, are non-trivially int-
egrable on R T, we start with Riemann-complete integrands using Fresnel func-
tions. Suppose (XT, I[N]) are associated in R T, with N = {ίχ,..., tn} G Af(T), 
Ij = Itj, and Xj = x(tj) G R for each tj G N and each N G Λ/"(Τ). Then the 
finite-dimensional object (x(N),I(N)) 
corresponds to the infinite-dimensional 
(xTJ[N]), 
and gc(*(A0), = g(x(N)), 
is defined by 
£{*m:=(^~ntnA*T{N)), 
= ( ^ ) 
ηβ^ϊ+-+«ϊ). 
(6.8) 
If T is itself the finite set {ίι,...,ί η}» then χτ(Ν) 
— (xi,...,x n) and the 
function gc coincides with the function 
π'^(-rf{pn,c(xi,... 
,xn). 
Since \I[N}\ = |J(JV)|, multiply the left-hand side of (6.8) by |J[7V]|, and the 
right-hand side by |7(iV)|, giving 
gc(x(N))\I{N}\ 
= gZ(x(N))\I[N}\ 
= (^β^ 
φηΑχ(Ν))\Ι(Ν)\. 
Similarly, given N, I(N), and I[N], 
GC(I[N}) = Gj(I[N}) := ( ^ j 
%„, C(/W). 
3For the moment, T is any infinite set. Later some ordering of the elements of T will be 
required. In the most important applications the elements of T will be an interval of real 
numbers ordered by the "<" relation. 

6.5. INFINITE-DIMENSIONAL 
FRESNEL 
INTEGRAL 
275 
Sc(x(N))\I[N}\ 
T 
Summarizing this linkage of infinite- and finite-dimensional entities, 
ί (J^)~nec^2^'-'+x^\I[N}\ 
if 
x G R T , 
< 
I (^ynnukecxUxJ 
if ^ftr\R: 
Gc(I[N}) 
= 
i E - i (/,, ^ Λ * ) . 
(6.9) 
If it needs to be emphasized that the domain is R T (and not R n), notations gj 
and G^T can be used. The domain of G^ is extended from I(R T) to E(R T) by 
additivity. With C denoting the set of complex numbers we then have 
g c: 
Κτχλί(Τ) 
^ 
C; 
G c : 
E(R T) 
H> C. 
The first task is to consider the integrability of these functions on R T. In the 
case of gc, the integrand is 
g< ..(x(N))\I[N]l 
= 
( ^ ) 
e c ( ^ ) 2 + - + a ; ^ ) a ) | / ( i i ) x · · · x 
I(tn)l 
(
i 
\ ~n 
i 
Though the latter form of the integrand gj in R T is suggestive of the integrand 
φη^ in R n (and, indeed, acquires its meaning from it), care should be taken to 
distinguish these two integrands from each other. In the former, the parameter 
n is variable, because iV G M{T) is variable. With this cautionary proviso, when 
T is infinite we will often make use of n-dimensional integration even though 
the infinite-dimensional context is very different from R n. 
To illustrate further the gulf between finite- and infinite-dimensional int-
egrands, it is clear, for instance, that the one-dimensional integration J R ecx dx 
is different from two-dimensional 
L 
e<*2+y2)dxdy. 
R x R 
Despite superficial similarities in notation, the differences between the two in-
tegrals 
rx(N)eI(N)* 
rxeI[N}* 
/ 
ec(*?+-+*£) \i(N) I, 
/ 
βΦι+-+*η) 
μ ^ ] i 
Ji(N)ei(nN) 
Ji\N]ei(nT) 
are equally profound. For instance, the latter is approximated by Riemann sums 
^e c^i + ," + xn)|/[iV']| in which a pair of terms can have the form 
gc(x(N))\I[N)\ 
= 
e c H t l ) 2 + - ' - + ^ ) 2 ) | / ( i i ) | x - - - x | J ( i n ) l · 
gc(v(N'))\J[N']\ 
= 
e c ( ^ ) 2 + - + ^ ^ 2 ) | J ( t i ) | x - . . x | J ( C ) l , 

276 
CHAPTER 6. GAUSSIAN 
INTEGRALS 
with different sets N and N'. Unlike integration in R n, integration in R T uses 
variables sets N G λί(Τ) when T is infinite4. 
6.6 
Integrability on R T 
First consider the integrability of the function Gc(J[iV]), defined on cells / = 
I[N] G I(R T), in which the parameter N G λί(Τ) is variable. (From this the 
integrability of g£(x(N))\I[N]\ 
will be deduced. This is a reversal of the order 
in which analogous results were proved in finite-dimensional domains R n.) 
Theorem 153 If a <0,b>0,c 
= α-\-ώ φ 0, and T is infinite, then GC(I[N]) 
is integrable on R T, and 
[ 
GC(/[JV]) = 1. 
Proof. Consider any division V of R T. Choose any (x,I[N]) 
G V. If TV = 
{£i,... ,tn}, so I = I[N] is restricted in n dimensions, the cell I[N] is bounded 
by (n — 1)-dimensional "surfaces" or faces, each of which is bounded by (n — 2)-
dimensional "edges"; and so on. Ultimately we reach one-dimensional edges, 
which are restricted in the dimensions N. For the chosen I[N], extend each 
of these faces and edges to infinity. Then do the same for each of the other 
cells I of the division V, so the cells of V are subdivided. As in (3.4), this 
"regularization" of V gives a partition V of R T such that each I[N] of V is the 
union of a finite number of cells J[M] G V, and each cell J of V is restricted in 
the same dimensions M = {ri,..., τρ}. In other words, while V is composed of 
cells I[N] with variable iV, the partition V consists of cells J[M], each having 
the same M. By additivity of the finite-dimensional Riemann and extended 
Riemann integrals involved, 
( P ) £ G C ( J [ A T ] ) = 
( 7 > ) £ G C ( J [ M ] ) . 
The latter Riemann sum is easy to compute, because M is the same for each 
J £V. Rearranging the Riemann sum in accordance with (3.5), we get 
(2>)£GC(I) = Π (fjCXUxi) 
= *' 
giving the result. 
O 
This argument works because the individual terms Gj(I[N] 
of the Rie-
mann sum (T^)^2G^(I) 
do not depend explicitly on the division points x of 
the (#, I[N]) G V. It is an argument that cannot be applied directly to the corr-
esponding integrand g£(x(N))\I[N]\ 
in which the dependence on the elements 
x is explicit. The following is fundamental to the main results of this book. 
4Just as δ "decreases", the sets N "increase". Thus, if T is finite, N can only "increase" 
to T, so in finite-dimensional spaces N is simply taken to be T. With T infinite, candidate 
integrands for the infinite-dimensional domain R T have been presented. Though superficially 
similar to integrands in R n , their infinite-dimensional character must be kept in mind. 

6.6. INTEGRABILITY 
ON R T 
277 
Theorem 154 If a < 0, b > 0, c = a + ώ φ 0, then G^ is a distribution 
function on RT. 
Proof. The integrability of G c on R T has been proved. Therefore, by Theorem 
17, G c is integrable on each E G E(R T), and, by Theorem 18, 
/ . 
Gi(I) 
= Gc(E) 
E 
for each E G E(R T). Thus, by Theorem 16, G c is finitely additive on disjoint 
sets E G E(R T). Since 1 = / R T Gc(J[iV]) = GC(RT), the result follows. 
O 
With T finite it is possible to deduce the variational equivalence of φη^{χ)\Ι\ 
and its indefinite integral φη^{1) from the integrability of the former—see The-
orem 148. When T is infinite the order of these two steps is reversed in the 
following results. 
Theorem 155 lfa<0, 
b>0, c = α + ώ Φ 0, and T infinite, then the function 
frc{x{N))\I[N}\ 
is variationally equivalent to the function Gc(J[iV]) in R T. 
Proof. Let ε > 0 be given. Given x and TV, choose δ(χ,Ν) 
> 0 so that if 
(x(N)J(N)) 
is 5-fine, then, for all y{N) G I(N), 
\gc(y(N)) - gc(x(N))\ 
< 
ieg-i(x(JV)) 
and 
\ttc(v(N))\ = ga(y(N)) 
> 
|g-i(s(JV)). 
This is possible because of the continuity of the functions involved. Then for 
any given I = I[N] G I(R T) we have 
|Gc(/[Ar])-gc(*(A0)|J[JV]|| 
I 
fy(N)eJ{N)· 
= 
\ 
(gc(y(N))-Sc(x(N)))\J(N)\ 
\Jj(N)eI(I(N)) 
< 
[ 
\&c(y(N))-gc(x(N))\\J(N)\ 
Jl(N) 
1 
ry(N)eJ(N)' 
< l e 
g^(x(N))\J(N)\ 
Δ 
Jj(N)ei(i(N)) 
fy(N)eJ(N)-
Jj(N)€l(I(N)) 
U(N)€l(I(N)) 
= 
eG-X{I\N\). 
With L(x) arbitrary for each #, choose a gauge 7 = (L, δ). Choose any 7-fine 
division X>7 of R T. By Theorems 153 and 154, 
(2>7) £ 
\Ge(I[N\) - tSc(x(N))\I[N]\\ < ε{νΊ) £ 
G^(I[N]) 
= e. 
This gives the result. 
O 

278 
CHAPTER 6. GAUSSIAN 
INTEGRALS 
The function g_i (i.e., gc with c = —1) is integrated in a finite-dimension-
al domain I(N) above. Strictly speaking, this is contrary to the meaning of 
the definition (6.8) of gc in R T. But provided these alternative meanings are 
carefully distinguished from each other, further proliferation of notation can be 
avoided by doing this. 
Theorem 156 Suppose a<0,b>0,c 
= a + ib^O,Tis 
infinite, and f(x, N) 
is a function defined on R T x λί(Τ). 
If either of 
f(x,N)Gc(I[N]), 
f(x,N)gc(x(N))\I[N}\ 
is integrable on R T then the other is also integrable, and the integrals are equal. 
Proof. This follows from the preceding result, using Theorem 44. 
O 
Theorem 157 Suppose a < 0, b > 0, c = a + ώ φ 0. Then gc(x(N))\I[N]\ 
is 
integrable on HT, and 
[ 
gc(x(7V))|/[iV]| = l. 
Proof. Taking f(x,N) 
= 1, this follows from Theorems 156 and 153. 
O 
Let f(x(N)) 
be a point function defined for xeRT,N 
e Λ/*(Τ), x(N) G RN. 
Define the function H(I[N]) for I[N] G I(R T) as follows. 
ry(N)eJ(N)· 
H(I[N}) := / 
f(y(N))gc(y(N))\J(N)\ 
(6.10) 
Jj(N)eI(I(N)) 
whenever the finite-dimensional integral exists. In other words, 
Η(Ι[Ν})=(]Ιψ] 
j j N f(y(N))VnAy(N))dy(N) 
for each N G λί(Τ) and each / = I[N] G I(R T)· Finite-dimensional calculations 
are used here to define an infinite-dimensional object. 
Theorem 158 Suppose a < 0, b > 0, c = α + ώ φ 0, T infinite, and H(I[N]) is 
defined by (6.10). If f(x(N)) 
is continuous in HN, then H(I[N]) is variationally 
equivalent to f(x(N))Gc(I[N]) 
in R T. 
Proof. Having established conditions for the finite additivity/integrability of 
G j in Theorems 153 and 154, we now take c = — 1 in this function and employ 
the integrable functions G^1(/[A/r]) and g_i(x(JV)|7"[iV]| to prove this result. 
With ε > 0 and N = {t\,..., 
tn}J\f(T) given, use continuity of the functions to 
choose δΝ(χ(Ν)) 
> 0 so that, for SN-üne 
(χ(Ν),Ι(Ν)), 
\f(y(N))-f(x(N))\\g.c(y(N))\ 
< 
|eg-i(x(JV)), 
S-i(y(N)) 
> 
±g-i(*(A0). 

6.6. INTEGRABILITY 
ON R T 
279 
Now choose a gauge 7 = (L, δ) in R T so that δ(χ, Ν) < <5JV- Consider any assoc-
iated and 7-fine (x,I[N\). 
Holding N fixed, we consider inequalities involving 
the integral5 //(N)gc(2/(-/V))|^(-/V)| with variable J(N) € I(I(N)) 
and variable 
y{N) 6 J(N)*. 
Then 
\H(I[N])-f(x(N))Gc(I[N})\ 
= 
\li(N) f(y(N))gc(y(N))\J(N)\ 
- f(x(N)) 
JJ(N) 
gc(y(N))\J(N)\\ 
= 
\Sim 
(/(»(*)) ~ 
f(x(N)))gc(y(N))\J(N)\\ 
< yf^g^xiNWm 
< 
eJI{N)g^(y(N))\J(N)\ 
= 
eG^(I[N}). 
Therefore, for every 7-fine division Ί)Ί of R T, 
(P7) X ; \H(I[N}) - f(x(N))Gc(I\N})\ 
< ε{ΌΊ) ^ 
G^(I[N}) 
= e, 
giving the result. 
O 
The above results show that, for c = a + tb (a < 0, b > 0, c ^ 0), GC(/[A^]) is 
the indefinite integral in R T of the Riemann-complete integrand gc(xT)|/[A/']|. 
By analogy with the finite-dimensional situation, we can say that gc(#r) is a 
density function in R T for the distribution function 
GC(I[N]). 
In definition (6.8), the function gc(x(N)) 
is defined for points x e R T. 
To avoid introducing further notation, in the proof of Theorem 158 above this 
expression was treated as a function of x(N) 
G HN with TV fixed. We will 
continue this practice, allowing the context to indicate which meaning is present. 
Likewise, the function G c can be treated as a distribution function in RN with 
N fixed, and gc(x(iV)) is its density function in R^. Again the context will 
establish which of the following meanings is intended for Gc: 
GC(I[N)), 
J[iV]€l(R T); 
or 
GC(I(N)), 
I(N) 
eI(RN). 
Next consider the integration of cylinder functions, in which an integral in an 
infinite-dimensional domain is reduced to an integral in a finite-dimensional 
domain. The underlying idea is given by the following example. 
Example 55 If g(x) is integrable for x G [0,1], and if f(x,y) 
= g(x) for all 
{x,y) G [0,1] x [0,1], then 
/ 
f(x,y)dxdy= 
/ 
g(x)dx. 
O 
</[0,1] x [0,1] 
./[ο,ι] 
5Since I(N) 
is finite-dimensional a notation ί γ ~ ) Jr(N\ 
(Pn,c(y{N))dy(N) 
would be 
more accurate here. 

280 
CHAPTER 6. GAUSSIAN 
INTEGRALS 
Suppose M = {TI, ..., r m} G Af(T) is a fixed finite set, and suppose a real-
or complex-valued function f(x) defined for x G R T is such that, if x, y G R T 
with x φ y and x{t) = y(t) for t G M, then /(x) = f(y). 
In other words the 
value of f(x) depends only on the variables x(t) for t G M. Then we describe 
the function / as a cylinder function, and we write f(x) as / M ( # ) · Given a 
cylinder function / M in RT? define a corresponding function / in R M by 
J{XM) = f{x(n),. 
· ·, x(rm)) = / M ( 4 
(6.11) 
Theorem 159 Suppose a < 0, b > 0, c = a + ώ φ 0. If f(xM)Gc(xM)I(M) 
is 
integrable on R M, £/ien /M(^)Gc(/[iV]) is integrable on R T and 
/ 
fM(x)Gc(I[N}) 
= [ 
f(xM)Gc(xM)I(M). 
Proof. Let a denote J R M /(xM)G C(XM)I(M). 
Let ε > 0 be given. Choose a 
gauge SM in R M so that, for every <$M-fine division VM of R M, 
{VM) Σ f(xM)Gc{xM)I(M) 
- a < ε. 
Choose a gauge 7 = (L, δ) for R T so that, for each x, L(x) D M and, for each 
N 2 Z/(a:), δ(χ,Ν) 
< SM{%(M)). 
Suppose Ί)Ί is a 7-fine division of R T. As in 
Theorem 153, construct a regularized partition V of R T by extending the faces 
and edges of I[N] for each (x,I[N]) G Τ>Ί as described in (3.6); so that 
(V,)YjfM(x)Gc(I[N}) 
= 
('D')J£fM(x)Gc(I[N}). 
Summing the latter in accordance with (3.7), 
(V')YifM(x)Gc(I[N}) 
= 
{VM)Y^f{xM)Gc{xM)I{M). 
Therefore 
\(V1)^2fM(x)Gc(I[N])-a 
<ε, 
giving the result. 
O 
The following is a converse to Theorem 159. 
Theorem 160 Suppose a < 0, b > 0, c = a + ώ Φ 0. If .fM(x)Gc(/[iV]) is 
integrable on R T then f(xM)Gc(xM)I(M) 
is integrable on R M, and 
[ 
fM(x)Gc(I[N]) 
= [ 
f(xM)Gc(xM)I(M). 
Proof. 
Consider any division V of R T. 
Extending the faces and edges of 
each I[N] of (x, I[N}) G V gives a partial regularization of V which allows easy 
computation of (£>) Σ fM(x)Gc(I[N}); 
so 
(V)J2fM(x)Gc(I[N}) 
= ( P M ) ^ / ( x M ) G c ( x M ) / ( M ) 

6.6. INTEGRABILITY 
ON R T 
281 
where VM = {(xMJ(M))} 
is a division of R M. Thus, if ε > 0 is given, 
integrability of fM(x)Gc(I[N]) 
on R T implies that there is a gauge 6M on R M 
such that 
ε > a-{VM)Y^f(xM)Gc(xM)I{M) 
for any ^M-fine division VM of R M . This gives the result. 
O 
Theorem 161 If a < 0, b > 0, and τ G T, then the function x(r)Gc(I[N]) 
is 
integrable on R T, with integral 0. 
Proof. 
The case a = 0 cannot be included in this—see Lemma 16. Take 
M = {T} and / M ( # ) — X(T)- Then Theorems 151 and 159 give the result. O 
Theorem 162 If a < 0, b > 0, and τ e T, then the function (x(r))2 GC(I[N]) 
is integrable on R T, with 
[ 
(x(r))2Gc(7[Ar])^zl. 
Proof. This is proved similarly. 
O 
Suppose τ',τ" eT ( τ ' φ τ") and J'Tf,J^ 
G I(R). Write 
J' = J'[T'] 
= { x G R T : x ( r , ) G j ; , } , 
J" = J»[T"] 
= {xeRT 
:x(r")e 
J^',,}, 
(6.12) 
j = jfnj" 
= {xeRT: 
x{r') G J;„ X(T") G j;',,}. 
The idea here is that each of the cells J' and J" is restricted in one dimension 
only, so their intersection in R T must be non-empty, and is restricted in just 
two dimensions. The following result shows the Gaussian integral on the latter 
set reduces to an integral in domain R x R, and it is the product of a pair of 
one-dimensional integrals. 
Theorem 163 Suppose a < 0, b > 0, c = a + ώ Φ 0. Suppose τ,τ' G T are 
fixed, and J'T,, J"„ are given cells in R. Then 
fa» - (ι/ϋ„ *"<'->) (i/^4 *-<'->) 
= ί/«ίΐ)ίίθ,(1]); 
(6.13) 
and 
>)\IT"\\ 
J &c(x)\i\ = [\lzrc I <ριΛχτ')\ΐτ'\\ (γζ^ / ψιΛχτ 
J &(χ(Ν))\Ι\)(Ι fScMNWW-

282 
CHAPTER 6. GAUSSIAN 
INTEGRALS 
Proof. The existence of the integrals follows from Theorem 149, using Theorem 
17. Their values are obtained by iterated integration, since Theorem 54 holds for 
both integrands. Note that, in each case, we have equality of three entities: an 
integral on R T, the product of two one-dimensional integrals, and the product 
of two integrals on R T. 
O 
6.7 
The Fresnel Function Is VBG* 
Even with a = 0 and c = ώ, the Fresnel function gJ(x(N))^ 
though not abso-
lutely integrable on unbounded domains, is continuous. Unlike the incremental 
Dirichlet function (4.20) of Chapter 4, it is not patently "exotic" or pathological. 
Does this function have any other "helpful" properties, in addition to continuity 
and smoothness? In one dimension, with x £ R, it reduces to 
ecx2 __ eax2 (cogfo.2 _j_ LS[nox^ 
m 
On the real intervals whose end points are the zeros of cos ax2 and sin bx2, these 
functions are continuous and absolutely integrable. In other words, their var-
iation6 is finite on the intervals bounded by y/(2j + 1)§ and y/jir, respectively 
(j = 0,1, 2,...). This shows that, though their variation in [0, oo[ is infinite, 
each of these functions has generalized bounded variation in [0, oo[, since this 
domain can be resolved into a countable number of disjoint sets (the real in-
tervals already mentioned) in each of which each of the functions has bounded 
variation. 
2 
2 
This does not mean that the function eLX (or eLX \J\) has generalized bounded 
variation on R. For this function we have 
\j\\ = \j\, 
(2>)EkxVi = ( P ) 5 > I , 
and it must be established that the function h{x,I) := \I\ is VBG* in R. But 
that is not difficult, since R can be resolved into a countable union of bounded 
intervals in each of which h{x,I) has bounded Riemann sums. 
By similar reasoning, the function <pn,c(#i H 
h α^)| Ji x · · · x Jn| is VBG* 
in R n, even when a = 0, c = ώ. 
What about the functions g^(x(JV))|/(iV)|, G^(I[N]) in R T with T infinite? 
These functions are variationally equivalent, by Theorem 155, so if either one is 
VBG* the other must also be VBG*. 
If T is countably infinite it is not too hard to adapt the earlier argument 
used to prove this for finite T—provided we switch from the basic meaning of 
VBG* (or RT-VBG*) to the broader concept of R T x Af(T)-VBG* 
introduced 
in Section 4.9. 
The argument runs as follows. The countable set T can be resolved into a 
countable union of finite subsets Mj? and gc(x(M7))|/(Mj)| is VBG* in R M' for 
6That is, using the conventions and terminology of this book, the variation of the point-cell 
functions cosax2\J\ 
and sinbx 2|J|, J G I(R)· 

6.7. THE FRESNEL FUNCTION IS VBG* 
283 
j = 1,2,3,..., with gc(x(Mj))\I(Mj)\ 
having finite variation in sets S^. C RMj, 
k = 1,2,3, 
Prom this, form sets 
( s ^ . x R ^ j x M , 
c 
RTxAf(T), 
j,k = 1,2,3,..., 
in each of which the function gc(x(Mj))\I(Mj)\ 
has finite variation. 
If T is uncountably infinite, Theorem 52 of Section 4.9 can be invoked. 
Consider first the case c = a + ώ with a < 0, b > 0. 
Theorem 164 Ifc = α+ώ witha < 0, b > 0, then the function 
gLb(x(N))\I[N}\ 
isRT 
xAf(T)-VBG*. 
Proof. We have 
and 
if-thfi-hYthf. 
Define 
Then 
\N}\\ = 
f(x,N) 
π 
—c 
= 
1 2 
π 1 
—c\ 
1 π 
1 ~a 
n 
2 
7 
2 
Γ 
a 
ga(x(N))\I[N}\. 
f(x,N)\Sc(x(N))\I[N}\\=ga(x(N))\I[N]\, 
and, writing h(x,N,I[N]) 
= f(x,N) 
|gc(x(AT))|/[iV]||, we have 
Vh [RT] = [ 
ga(x(N))\I[N}\ 
= 1. 
The result then follows from Theorem 52. 
o 
Theorem 165 Ifb > 0 the function glb(x(N))\I[N}\ 
is R T x 
Af(T)-VBG*. 
Proof. Choose a = —b and, with N = {t\,..., 
tn} and x(tj) = Xj, let 
/ ( ζ , Λ 0 = β ° ( χ ι + - + χ * ) . 
Then /(x, JV) \glb(x(N))\I[N}\\ 
= go(x(W))|/[JV]|. Writing 
h(x,N,I[N}) 
= 
f(x,N)glb(x(N))\I[N}\, 
we get Vh [RT] = fnT ga{z(N))\I[N]\ 
= 1, and the result then follows from 
Theorem 52. 
O 

284 
CHAPTER 6. GAUSSIAN 
INTEGRALS 
Theorem 166 Suppose a < 0, b > 0, and c = a -f ώ Φ 0. Then each of the 
functions 
GC(I[N]), 
gc(x(N))\I[N]\ 
isKT 
xAf(T)-VBG* 
inRT. 
Proof. This follows from the preceding results. 
O 
6.8 
Incremental Fresnel Integral 
This section gives the preceding results in a form which will enable them to be 
used in the theory of Brownian motion. Again, take a and b to be real numbers 
such that a < 0, b > 0, with a, b not both zero; ι = \f—l\ and c = a + ώ. 
Prom this point onwards T will no longer be an arbitrary labeling set, infinite 
or otherwise. Instead, T will be a set of real numbers, so elements of T can 
be ordered by the "<" relation. Take T C ]0, oo[ = R+ and consider any 
N = {ίι,ί 2,...,ίη} € M(T) with 0 = t0 < h < t2 < ··· < tn. Writing 
x(tj) = Xj define 
9c(xj\xj-i) ■= (-c)iff"i (tj - i,-_i)-* exp (ϊ&ΖΞϊζϊΙ^ 
, 
(6.14) 
n 
gc(x(N)) =gc(xi,X2,'.>,xn) 
·= Π ^ ( ^ Ι ^ - ι ) , 
(6-15) 
i=i 
where #o = 0· The differences Xj — Xj-i, = x(tj) — x(tj-i), 
are the increments 
or transitions of the incremental (transitional) Fresnel integrand. 
Theorem 133, and Lemmas 12 and 13, imply that, with Xj-\ given and Ij 
denoting any real interval corresponding to some tj (1 < j < n), 
f 
rxoerj 
/ 9c{xj\xj-i)\Ijl = / 
gdxjlxj-^lljl 
= 1 
(6.16) 
./R 
Jijei(R) 
since (—c)~^n^ is the integral value in Theorem 133, with (tj —tj-\)~* 
as a 
further scaling factor corresponding to Lemma 13. 
Let T be ]0, oo[, or any subset of ]0, oo[. For any positive integer n, for any 
N = {tu ... ,i n} G ΛΤ(Τ), and for any / = I[N] = h x ■ · · x / n x R n i v G I(R T), 
define a cell function Gc as follows: 
GC(I[N]) 
- 
GC(JV,/[JV]) = GT
C(N,I[N}) 
(6.17) 
r2/(iV)€J(iV)',' 
:= / 
flc(yW)kWI 
= / 
5c(2/W)|J(iV)|. 
^/(iv) 
Jj(N)ei(i(N)) 
[Xj — Xj-l) 
IHN) 
Jj(N)el(I(N)) 
Note that if we write 
tj 
^j -1 

6.8. INCREMENTAL 
FRESNEL 
INTEGRAL 
285 
then gc(x(N)) 
= gc(y(N)). 
This relationship enables the incremental Fresnel 
functions gc and Gc to inherit properties of gc and Gc, using the link provided 
by Lemma 13. 
Different meanings can be given to the set T, such as T finite, or T = ]0, oo[, 
or T =]0,r]. For clarity, write G]
C
0'°°[(/[7V]) or G]
c°'r](J[7V]) or the like, whenever 
such a distinction needs to be made. 
Theorem 167 With a < 0, b > 0, and c = a + ώ φ 0, the function G^ is well 
defined in I (R T). 
Proof. Consider I = I[N] G I(R T). If, for each tj G TV, each Ιά = I(tj) has the 
form ]u,v] with both u and v finite, then GC(I[N]) is defined since the integral 
in (6.17) exists. If some of these intervals are unbounded, with u = —oc or 
v = oo, then we can appeal to Theorem 149, in conjunction with Lemma 13 
and Theorem 54 (Fubini's theorem), so the integral in (6.15) can be evaluated 
by iterated integrals. A similar method can be applied if, for some tj G AT, we 
have Ij = ] — oo, oo[. 
O 
Theorem 168 With a < 0, b > 0, and c = a + ώ Φ 0, the function 
G^(I[N]) 
is integrable on R T, with J R T G^(I[N]) 
= 1; so G^(I[N]) 
is a distribution 
function. 
Proof. 
This follows from Theorem 149, using Lemma 13 and Theorem 54 
(Fubini's theorem). 
O 
Theorem 169 With a < 0, b > 0, and c = a + ώ φ 0, the functions 
G^(I[N]) 
and gc(x(N))\I[N)\ 
are variationally equivalent in RT. 
Proof. 
This follows from Theorem 155, using the change of variables with 
Theorem 54 (Fubini's theorem) as before. Alternatively it can be proved directly 
using the method of Theorem 155. 
O 
Theorem 170 Suppose a <0, b>0, c = a + ώ Φ 0, and the real- or complex-
valued function /(#, TV) is given. If either of /(#, N)Gcx(I) 
or /(#, N)gc(x)\I\ 
is integrable on R T then the other is also integrable, and the integrals are equal 
In particular, gc(x(N))\I[N]\ 
is integrable on R T, and 
[ gc(x(N))\I[N]\ = l. 
Proof. This follows from the two preceding results, using Theorem 44. 
O 
Let f(x(N)) 
be a point function defined for x(N) G R N, TV G λί{Τ) with T 
infinite. Define the function H(I[N]) for /[TV] G I(R T) as follows. 
H(I[N}) := / 
f(y(N))gc(y(N))\J(N)\ 
Jj(N)el(I(N)) 

286 
CHAPTER 6. GAUSSIAN 
INTEGRALS 
whenever the finite-dimensional integral exists. 
Theorem 171 Suppose a < 0, b > 0, c = a + ώ φ 0, and H(I[N]) is de-
fined. If f(x(N)) 
is continuous then H(I[N]) is variationally equivalent to 
f(x(N))Gc(I[N\) 
inRT. 
Proof. With the usual transformation of variables, this follows from Theorems 
158 and 46. It can also be proved directly, by the method of Theorem 158. O 
These results show that GC(I[N]) is the indefinite integral of gc(xT)\I[N]\, 
and QC(XT) is a density function for GC(I[N)). 
Suppose M = { n , . . . , r m} G λί(Τ) is a fixed finite set, and suppose a real-
or complex-valued function f(x) defined for x G R T is such that, if x,y G R T, 
x φ y, x(t) — y(t) for t G M, then f(x) = f(y). In other words the value of 
f(x) depends only on the variables x(t) for t G M. Thus the function / is a 
cylinder function as described in (6.9); and, as before, we write f(x) as / M ( # ) · 
Given a cylinder function / M on R T we can, as before, define a corresponding 
function / on R M by 
/(XM) 
= /(x(ri),...,a;(r m)) = / M ( S ) . 
Theorem 172 If f(xM)Gc(xM)I(M) 
is integrable on R M , then 
fM(x)Gc(I) 
is integrable on R T and 
[ 
fM(x)Gc(I) 
= [ 
f(xM)Gc(xM)I(M). 
Proof. This follows from Theorem 159. 
O 
Theorem 173 Suppose M G λί(Τ) and J[M] G I(R T) are given. If a < 0, 
b>0, and c = a -f ώ Φ 0, then GC(I[N]) is integrable on J[M] and 
[ 
GC(I[N]) = GC(J[N}). 
(6.18) 
Jj[N] 
Proof. This holds since Gc is a distribution function. 
O 
If V is a partition of R T (so V = {J[N]} where J[N] are disjoint elements 
of I(R T) whose union is R T), and if a real- or complex-valued function 
f(x,N) 
is constant for x G J[N] G V, then we call / a step function. 
Theorem 174 If a <0, b >0, and c = a + ώ φ 0, then every step function is 
Gc-integrable on RT. 
Proof. This follows from Theorem 173. 
O 
Investigation of Gc-observables f(X) or /(X, N) means that we are inter-
ested in establishing if and when functions f(x) or f(x, N) are Gc-integrable. 
Here is an example of a function that is not Gc-integrable. 

6.8. INCREMENTAL 
FRESNEL 
INTEGRAL 
287 
Example 56 Let g*(x(N)) 
denote the complex conjugate of gc(x(N)). 
Then 
g*(x(N)) 
is not Gc-integrable if a = 0, b > 0. Because if it were, then, by 
Theorem 170, the function g*(x(N))gc(x(N))\I[N]\ 
would be integrable on R T, 
and this is false since 
g*c(x(N))gc(x(N)) 
= 
\gc(x(N))\2 
is always positive, so no cancellation takes place when Riemann sums are formed. 
It is then easy to see that the Riemann sums diverge. 
O 
Theorem 175 lfa<0,b>0 
and τ,τ' eT with τ' < τ fixed, then the function 
(X(T) — X(T')) 
GC(I[N]) 
is integrable on R T, with 
[ 
(X(T)-X{T'))GC{I[N]) 
= 0. 
Proof. This result follows from Theorem 161. 
O 
Note that the case a = 0 (see Lemma 16) cannot be included. Likewise in 
the following result. 
Theorem 176 lfa<0,b>0 
and τ,τ' eT with r' < τ fixed, then the function 
(X(T) — X{T')) 
GC(I[N]) 
is integrable on R T, with 
j ^ (x(r) - X{T'))2 GC(I[N]) = 
~(Τ~/]. 
Proof. This follows from Theorem 162. 
O 
The following version of Theorem 176 is useful. 
Theorem 177 If a < 0, b > 0, and τ,τ' 
G T with τ' < τ fixed, then the 
function 
( ( S ( T ) - x(r')f 
- ~(Τ~/]) 
GC(I[N]) 
is integrable on R T, with 
J^T 
((x(r) - x{r')f 
- ~{Τ~/]) 
GC{I[N\) = 0. 
Proof. This follows from Theorem 176. 
O 
The following is used in Chapter 7 to prove that Brownian increments are 
independent. 
Theorem 178 Suppose a < 0, b > 0, and c = a + ώ φ 0. Suppose τ,τ' e T 
are fixed, with cells 

288 
CHAPTER 6. GAUSSIAN 
INTEGRALS 
as defined in (6.12). Then 
I GC(I) = (J GC{I)\ (j GC{I)\ ; and 
ljgc(x)\I\ = (f 9c(x(N))\I\)(f 
9cMN))\I\y 
(6-19) 
Proof. 
This is implied by Theorem 163. As in that result, the left-hand 
integral in R T reduces to an integral in R x R, while each of the two right-hand 
integrals in R T reduces to an integral in R. As before, the result is essentially 
an application of Fubini's theorem. 
O 
Theorem 179 Suppose a < 0, b > 0, c = a + ώ ^ 0, and T is countably 
infinite. Then GC(I[N]) and gc(x)\I[N]\ are R T x Af{T)-VBG* 
in R T. 
Proof. This is implied by Theorem 166, and the remark that follows it. 
O 
Theorem 174 can be used to extend the list of functions which are Gc-
integrable on the domain R T, by means of step functions. The step functions 
in question are constant on each element of the binary partitions of R T in-
troduced in Section 3.5. But in order to be able to use step functions in this 
way some properties of continuity for the incremental Fresnel integrand must 
be established. 
6.9 
Fresnel Continuity Properties 
Up to this point the incremental Fresnel integral has yielded nothing not al-
ready encountered in the Fresnel integral itself. But this section derives some 
properties that depend on the "transition" factors or increments 
Jb j 
Jb j — \ — JU\~L ή j 
iL [L"j — \ ) 
in the integrand. 
Take T C R + = ]0, oo[ with x0 = x{t0) = x(0) = 0. Let τ',τ eT be given, 
and let r be a positive integer. Consider the following subset of R T: 
<x:xeRT, 
\X{T)-X{T')\ 
> - > . 
(6.20) 
If and when we want to let r' —» r, then we will require that T be an interval 
in R + = ]0, oo[ or a dense subset of such an interval. In the following we take 
a < 0, b = 0. 
Lemma 23 For r, τ' G T, each of the integrands 
lA^(x)GT
a(I[N}), 
(X(T)-X(T'))2GI(I[N}) 
in R T is a cylinder function, and each can be replaced by an integrand in R x R. 

6.9. FRESNEL CONTINUITY 
PROPERTIES 
289 
Proof. For fixed τ, τ' G T, each of the two point functions in question is a 
cylindrical function of x G R T, so the result follows from Theorem 159. 
Q 
Theorem 180 If a < 0 and τ, τ' G Γ then 
Proof. By Theorem 176 the cylinder function (x(r) — x{r')) 
is G^-integrable, 
and 
JRT(X(T)-X(T'))2GJ(I[N}) 
= - ^ ^ . 
Therefore 
1 / 
lAfT(x)Gl(I[N\) 
< ί 
1Α,τ(χ)(χ{τ)-χ{τ'))2(Ζ(Ι[Ν]) 
r 
JUT 
r 
JnT 
< [ 
(x(r)-x(rf)fG^(I[N}) 
2a ' 
Since the integrand is positive the result follows from Theorem 39. 
O 
In other words, with r given, we can choose a gauge for which, in any Rie-
mann sum (V) Σ Ga(I[N]), we get a contribution of approximate size 
V 
r2\r 
2a 
from those terms (x,I[N]) 
G V in which the transition x{r') — x(r) is large 
relative to r. 
Suppose x G R T fails to be continuous at some r G T. Then for any 
given r, an element τ' G T (r; φ r) can be chosen so that η is as small as 
we please. Now define a gauge 7 = (L, δ) (depending on the positive integer 
r) with τ,τ' G L(x). Consider a 7-fine division Τ>Ί = {(x,I[N])} of R T. In 
the corresponding Riemann sum (Τ>Ί) Υ^ G'ailW]), consider those terms of the 
Riemann sum for which (x, I[N]) G Τ>Ί has x discontinuous. If it can be shown 
that the aggregate of such terms contributes an arbitrarily small amount to the 
Riemann sum, it may be possible to deduce from Theorem 180 that the set D 
of discontinuous x in R T is G^-null; that is, ν ^ α (D) — 0. If this holds we will 
be able to say that every x G R T is Ga-almost certainly continuous; that is, 
the set of continuous functions is Ga-full in R T. This program of proof can be 
accomplished—provided T is countable. 
Suppose T G T is given, and suppose 
QT = 
{ T I , T 2 , T 3 , . . . } C T 

290 
CHAPTER 6. GAUSSIAN 
INTEGRALS 
is a sequence in T with Tj —> r G T as j —> oo. Consider any a: G R T. Let ε > 0 
be given. If there exists j e so that j > j£ implies 
\χ{τ)-χ{τό)\ 
< e, 
(6.21) 
then x is continuous at r relative to QT. If (6.21) holds for every sequence 
converging to r then x is continuous at r. Now suppose a and Θ are given real 
numbers, with 0 < a < \ and Θ > 0. If there exists jT = jr{&, Θ) so that j > jT 
implies 
|*(τ)-*(τ,·)| < β | τ - τ , · | α , 
(6-22) 
then we say that x is (a,Θ)-continuous at r relative to QT. Condition (6.22) 
implies condition (6.21). If (6.22) holds for every such sequence then a; is (α, Θ)-
continuous at r. 
Because (6.22) is sometimes useful in proofs we now prove another version 
of Theorem 180. Suppose r is a positive integer. Write 
Air 
._ 
{x.rGBT 
\x{r)-x{Tj)\ 
I ] 
(6.23) 
Theorem 181 // a < 0, τ, τ' G T, 0 < a < \, Θ > 0, and r is a positive 
integer, then 
Proof. The proof is essentially the same as that of Theorem 180, but it is 
repeated here for comparison. By Theorem 176, 
f 
(X(T)-X(TJ))2
 
T 
_ 
I r - r . f - 2 " 
JRT 
ö2(T_Tj.)2a Ga(I[N]) 
~ 
202|a| 
' 
Therefore 
K 
i l - 2 a 
2θ2\α\ 
Jl 
By Theorem 39 the result follows from this since the integrand is positive. 
O 
Note that, as before, the integrands are cylindrical, and the integrals in the 
proof are each equal to the corresponding integrals in R x R. 
Suppose T is a dense set of real numbers. For instance, T = R +, or T is 
some interval in R +. If r G T let C^Q denote the set of x G R T which are 
(a, #)-continuous at r relative to T (i.e., relative to every sequence in T), and 
let 
Df 
= RT\C^ 
(6.24) 
the set of x G R T which are (α, Θ)-discontinuous at r relative to T. 

6.9. FRESNEL CONTINUITY 
PROPERTIES 
291 
With C^e and D^e denoting, respectively, the set of x G R T which are 
(a, #)-continuous at r relative to T for each τ E T, and the set of x G R T which 
are (a, ^-discontinuous at r relative to T for some r G T, we have 
C& = Π °«Θ> 
Dle = U ^ < 
Dle = RT \ <&· 
(6-25) 
If T is a countable set then the intersection and union are countable. The aim 
is to establish conditions for which 
VGT(Z£e) = 0, 
V G r ( 0 = l· 
Note that Ό^θ includes as a proper subset those x G R T for which, for some 
reT, 
the sequence X(TJ) fails to converge to χ(τ) as Tj -> r. 
Suppose r is a positive integer. If x G R T is (a, ^-discontinuous at r, then, 
with ε > 0 given, there exists a sequence 
QT = {τι,τ 2,τ 3,...} C T 
such that, for j = 1, 2, 3,..., 
Γ 
V - T / - 2 * 
< e2~j, 
(6.26) 
202|a 
since 0 < a < \ implies that 1 — 2a > 0. 
With reference to AJJdr in (6.23) above, define the following auxiliary sets. 
For positive integer r, write 
TDklr 
— 
I \l 
AJr 
jDkr 
— 
I loo 
AJT 
B; 
.= nr=iSr
fcT. BT ■■= ur=i^
r· 
If T is a countable set there is only a countable number of sequences QT in T. 
We then have 
D^ = [j{B^.QTcT}^ 
DT
ae= (J D^ 
both unions being countable. 
Thus, if it can be shown that VGT(BT) 
= 0, then countability may imply 
that 
YGr{Dle) 
= Q, 
VGr(Dle) 
= 0. 
If we replace AJJ0r by AJ
r
T, as defined in (6.20), the proofs below give the corr-
esponding results with (a, #)-continuity replaced by continuity. 
Theorem 182 Suppose T is dense in R+ and suppose T G T , 
α < 0 , 0 < α < 
\, 
and θ>0. 
Then VGT(BT) 
= 0. 

292 
CHAPTER 6. GAUSSIAN 
INTEGRALS 
Proof. Given any sequence of elements of T converging to r, we can, without 
loss of generality, replace it with a sub-sequence 
Qr = {Ti,T 2,T 3,...}cr 
satisfying (6.26). Theorem 181 and additivity of the integrals give 
lBi,r(x)Gl(I[N}) 
< 
-f 
J lB>,r(x)(%(I[N}) < ^ L £ | r - r , | i - ^ < e£ 2-i. 
j=k 
j=k 
Note that each integrand is a cylinder function. The sequence of integrands 
lßkiT(x)G^(I[N]) 
is monotone increasing as I increases, and the preceding in-
equality shows that the sequence of integrals is bounded above. Therefore The-
orem 57 implies that 
lim lB^(x)Gl(I[N}), 
= 
lBr{x)GT
a{I[N\), 
I—>oo 
r 
r 
is integrable and 
r. 
OO 
/ 
lB^{x)GT
a{I[N]) 
< 
e^2 3 < ε 
j=k 
for all ε > 0, so 
/ 
1Β*τ(χ)(%(Ι[Ν]) 
= 0. 
Then Theorem 39 implies 
Note that, for any e > 0, there exist k and / and a gauge 7 = 7(7*, /c, I) so that the 
cylinder function lBkir(x)G^(I[N]) 
has integral and Riemann sum satisfying, 
respectively, 
/ 
1Β^(χ)(%(Ι[Ν]) 
< ε, 
(2\) £ 
lB^{x)GT
a(I[N\) 
< 2ε; 
and since the integrand is a cylinder function there are, corresponding to the 
integral and Riemann sum in R T, an integral and Riemann sum in R x R for 
which the same inequalities hold. Since B^ C B^r for each /c, 
VGT(B;) 
= O= ί 
ιΒ;(χ)(%(ΐ[Ν]). 
Then B\ C Βζ C B\ C · · ·, so 1Βτ (x)G^(I[N]) 
is monotone increasing as r 
increases, with 
lim lBr(x)Gl(I[N}) 
= 
lBr(x)Gl(I[N}); 

6.9. FRESNEL CONTINUITY 
PROPERTIES 
293 
and Theorem 57 (with Theorem 39) gives 
ie.(x)Gl(j[«]) = o = vo;r(ir). 
/ 
JnT 
Instead of monotone convergence, dominated convergence (Theorem 61) can be 
used in the above proof. 
O 
Since the integrand is non-negative Theorem 17 implies further that, for any 
E G E(R T), 
/ 1Β(χ)(%(Ι[Ν]) 
= 0, 
YGI[E](BT) 
= 0. 
JE 
The following result is a corollary of the above. 
Lemma 24 The function 1Βτ(χ) 
is the limit of cylinder functions 
lBkw(x), 
satisfying 
ί 
lBMr(x)Ga(I[N])-> 
f 
lBr(x)Gl(I[N]) 
= 0, 
k < Z, 
JnT 
JnT 
as l,k,r —► oo. 
Proof. This follows as in Lemma 23. Note that each of the left-hand integrals 
has a cylinder function lBkir (x) as integrand, so Theorem 160 implies that each 
of them is equal to a corresponding finite-dimensional integral. 
O 
Now suppose, as above, that T is a dense set of real numbers, such as R+ 
itself or some interval in R +; and suppose α < 0 , 0 < α < ^ , and Θ > 0. 
Suppose S C T. Define 
Cf £ = 
{x : x G Οι
αθ for each t G 5} , 
Daθ 
= 
{x : x e Όι
αΒ for some t G S} , 
Cs 
= 
{x-.xeC1 
for each t G S} , 
Ds 
= 
{x : x G Dt for some t e S} . 
Suppose m is a positive integer and τχ,..., r m G T are fixed, giving 
M = { n , . . . , r m } G ^ ( T ) . 
Taking S = M, we have C%, D™0, CM and D M where C M and D M consist of 
those x G R T which are, respectively, continuous relative to T at each Tj G M 
(in the sense of (6.21)); and discontinuous relative to T at some Tj G M. Then 
L>^ = R T \ C ^ , 
D M = R T \ C M , 
D M c D a
M
ö , 
C M D C ^ . 
The objective is to show that D^e is a Ga-null set; that is, VGT (D^e) = 0. If 
true, this implies further that 
V G. (CÜ5) = 1, 
Vor (DM) = 0, 
WGr (CM) = 1. 
(6.28) 

294 
CHAPTER 6. GAUSSIAN 
INTEGRALS 
Theorem 183 Suppose T is a dense set of real numbers; and suppose a < 0, 
0 < a < \, Θ > 0, and m is a positive integer. Suppose M = { n , . . . ,r m} G 
λί(Τ) is fixed. Then V GT (D%) = 0. 
Proof. We have 
m 
so, by Theorems 30 and 181, 
m 
v GiW)<E^K)=°. 
as required. 
O 
As in Lemmas 23 and 24, there is here a similar consequence of Theorem 
183. Recall that G^ is the incremental Gaussian distribution function defined 
on I(R T), so G^{I(M)) 
is defined for cells I(M) in R M. 
Lemma 25 The function 1DM (X) is the limit as k —» oc of a sequence of 
αθ 
positive functions $Mk {x) (x G R T) which are cylindrical on RMfc (Mk G λί(Τ)), 
with the following properties. 
1. Mk C M for each k. 
2. J R T fMk{x)GT
a{I[N)) 
-+ / R T 1DM(X)GZ(I[N}) 
ask^oc. 
3. With f defined by f(x(Mk)) 
= fMk(x) 
(x € R T > x(Mk) G RMk), 
we have, 
for each k, 
[ 
fMk(x)GT
a{I[N}) 
= [ 
f(x(Mk))G^(I(Mk). 
By Fubini's theorem (Theorem 54), each integral on RMfc is computable 
by iteration of a finite number of integrals on R ; and, since each of the 
integrands is continuous except at a finite number of points of R, the 
iterated integrals on R are extended Riemann integrals. 
4- Given ε > 0, for each k there exists a gauge ÖMk on RMfc so that, for each 
ÖMk-fine division T>k ofHT, 
the corresponding Riemann sum satisfies 
(Vk)J2f(x(Mk))G™*(I(Mk))<e. 
Proof. This follows from Theorem 183 and Lemma 24. To see how $Μ0 is 
constructed, suppose m = 2. Then 
BM 
= βΤι 
y Br2^ 
1ΒΜ(χ) = i B n (χ) + lBr2(x) 
- 1 βτ ι η β Τ 2(χ). 

6.9. FRESNEL CONTINUITY 
PROPERTIES 
295 
Since BTl Π BT2 C BTl, we have V G T ( £ T I f l F 2 ) = 0, and each of the indicator 
functions involved is the limit of cylindrical indicator functions as in Lemma 
24. When m > 2 a similar argument produces a finite linear combination of 
cylindrical indicator functions, and sequences of these combinations converge 
pointwise to 1DM (X). The result then follows by applying Lemma 24. 
O 
αθ 
Lemma 26 Each of the sets Mk in Lemma 25 can be replaced by Mk G λί(Τ) 
provided M'k D Mk. 
Proof. For each x(M'k) G RM*, define / ' by 
f'(x(M'k)) = f(x(Mk)). 
Then, as in Theorems 159 and 160, 
/ 
f(x(M'k))Gf*(I(M'k) 
= / 
f(x(Mk))G*f 
*(J(Mfc), 
giving the result. 
O 
For each &, take Mk = M, with M as in Theorem 183. Again, each of 
the finite-dimensional integrals J R M f(x(M))G^[(I(M) 
can, by Theorem 54 
(Fubini's theorem), be expressed as a finite number of iterations of extended 
Riemann integrals (since the latter are Henstock integrals): 
Foe UZ (" ■ / I UWMW'VM) ··■))· 
Lemmas 23 to 26 help to give a sense of the finite-dimensional iterated extended 
Riemann integrals which can be used to estimate ν^τ(£)^) and VQT(C^Q) 
for 
different sets 5. 
Theorem 183 establishes that those elements x of R T which are (a, ^-con-
tinuous (or simply continuous) at each of a fixed, finite set of r G T form a 
G0-full subset of R T. It would be interesting to investigate whether the much 
smaller set of those x in R T which are (a, 0)-continuous (or continuous) at each 
r G T form a Ga-full subset of R T when T is an uncountable set such as an 
interval of real numbers; or whether, on the other hand, V"G;T(£>^0) > 0 for such 
T. (Since ^G^(I[N]) 
— 1 over every partition {/[iV]} of R T we must have 
0 < V G . ( D ^ ) < 1 . ) 
However, we will take the traditional approach of extending Theorem 183 
from a fixed finite set of possible points of discontinuity to a fixed countable set 
of possible points of discontinuity. 
Let Q be a fixed, countable, dense subset of a set T, where T is (as previously) 
a dense subset of R + such as R + itself or an interval in R +. If T is itself 
countable we can take Q = T. Define D%, c£ e, £>Q, C Q as in (6.28) above, 
but with Q replacing 5. 

296 
CHAPTER 6. GAUSSIAN 
INTEGRALS 
Theorem 184 Suppose T is a dense set of real numbers with countable, dense 
subset Q = {n, T2, T3,...}; and suppose a <0, 0 < a < | , 0 > 0. Then 
V G - ( ^ ) = 0 ' 
*V
GT(DQ)=0. 
Proof. As in Theorem 183 we have 
oo 
D
Q
a6 = U *>2» 
so, by Theorems 30 and 181, 
oo 
νσϊ(^)<ΣνσϊΜ)=°· 
3 = 1 
Since DQ c £>^ the result follows. 
O 
For a < 0, b > 0, and c = α + ώ φ 0, G^ is a distribution function on R T 
where T c R . It has been established above that, provided b = 0, under certain 
conditions those x G R T which do not satisfy certain continuity conditions on 
T are a Ga-null subset of R T. 
Does a corresponding result hold for Gc whenever b is not zero? 
As before, let r', r be given elements of T, and let r be a positive integer. 
And, as before, consider the following subsets of RT: 
Ay 
= 
{x:xeRT, 
\χ(τ)-χ(τ')\>λ}, 
tfX 
= 
{x-.xeRr, 
\χ{τ)-χ{τ')\>θ-\τ-τΤ}> 
Then, if a, c, and β are given, with 0 < β < 1, then for any real number r a 
number τ' can be chosen so that 
< ψ^ρψ- 
(6-29) 
Theorem 185 Suppose a < 0, b > 0, c = a + ώ, 0 < β < 1, and τ,τ' 
e T 
satisfy (6.29). Then 
\[ 
lK>T(x)GT
c{I[N}) < 
2\a\ 
Proof. The function 1AT/T(x) is a cylinder function, so Theorem 159 can be 
used. Write y = x(r) - x(rf), so that 

6.9. FRESNEL CONTINUITY 
PROPERTIES 
297 
Therefore, following the method of proof of Theorem 180, 
i 
*yer 
/ei(R) 
r 
l 
—— rv^i* 
/RTV.(*)<£(/M) < |||(^)-ay 
\^>_φ)^^\ι\ 
< 
< 
— 1 
a 1 
C 
a 
< 
r l r - τ Ί 1 -
2\a\ 
2\a\ 
as required. 
O 
Theorem 186 Suppose a < 0, b > 0, c = α+ώ, Θ > 0, 0 < a < \, 0 < β < 2a; 
and suppose τ,τ' G T satisfy (6.29). Then 
f 
1AT,■ (x)G?(I[N)) 
Γ ^ Τ - Τ Ί 1 - 2 * -
2|α| 
Proof. This is proved similarly. (Note that 1 — 2a — β is positive.) 
O 
Theorem 187 Suppose T is R+ or an interval of R+, and suppose Q is a 
countable, dense subset ofT. 
If a < 0, b > 0, c = a + tb, then 
[ 
lDQ(x)Gj(I[N}) 
= 0, 
[ 
1DQ(X)GJ(I[N]) 
= 0. 
Proof. The proof follows the same steps as used for the case b = 0 in the 
preceding section. (We cannot deduce a result corresponding to Theorem 184 
for VGT since G^(I[N]) is not a positive-valued function.) 
O 
In the following, let B denote either of D® or D^e. Letting a —> 0, 
lB(x)Gj(I[N}) 
-»■ 
lB(x)Gjb(I[N}), 
and the question then arises whether it is possible to prove that 
/ 
lB(x)(%(I[N]) 
-+ ί 
1Β(χ)(%(Ι[Ν\). 
If this can be done then the right-hand integral (if it exists) is zero, since each 
of the left-hand integrals is zero by Theorem 187. 
The usual methods of integration theory suggest using some limit theorem, 
such as dominated or monotone convergence, to try to prove the existence of 
the right-hand integral with value equal to the common value of the left-hand 
integrals. 
Regarding monotonicity, it is not hard to see that, for any given (x, I[N]), the 
real (or imaginary) part of the left hand integrand is monotonically convergent 
to the real (or imaginary) part of the right hand integrand. 
Unfortunately, 
for different (x,I[N]) the monotonicity may be increasing or decreasing. And 

298 
CHAPTER 6. GAUSSIAN 
INTEGRALS 
if these two classes of (x,I[N]) 
are separated out from each other, thereby 
expressing the real (or imaginary) parts of lB(x)Ga+Lb(I[N]) 
as the sums of 
monotone increasing and monotone decreasing functions, the resulting functions 
are not integrable on R T. 
Regarding dominated convergence, it is not possible to find an integrable 
function that dominates the convergence of these integrands. 
But this is not the end of the road. Convergence of non-absolute integrals 
can be tested by the methods of non-absolute integration. 
Continuing on these lines, Q is again a countable dense subset of real in-
terval T, and B denotes either of D® or D®9. The aim is to examine the 
G^-measurability of B. In other words, to find out whether lB(x)GLb(I[N]) 
is 
integrable on R T, in some sense, with value 0. That is to say, whether or not 
the equation P[B] — 0 holds for the underlying process. 
Lemma 27 For each x{N) G R^, ga+Lh{x(N)) ->> gLh(x(N)) 
as a ->► 0. 
Proof. This follows from the continuity of gc(x(N)) 
with respect to the para-
meter c. Specifically, with TV = {i 1 ?..., tn}, and 
^-(^'(jH 5^))· 
we have 
ga+Lb(x(N)) = f(x, iV; a)gLh(x(N)). 
(6.30) 
Suppose b > 0 is fixed. Then, for any given (x, N), n is fixed and 
n
(a(xj 
-Xj-i)2\ 
(a + Lb\* 
as a —> 0. 
O 
The first step is to deduce the integrability of linia^o 
lB{x)9a+ib(x(N))\I[N]\. 
Some earlier results are repeated here for convenience. 
Suppose a < 0, b > 0, and c = a + ώ φ 0. Then, by Theorems 169 and 179, 
the functions gc(x(N))\I[N]\ 
and GC(I[N]) are 
■ variationally equivalent, and 
- R T x A/"(T)-VBG* in R T. 
The same holds for the pair of functions 
1B(X)9C(X(N))\I[N}\, 
lB(x)Gc(I[N}). 
In particular, the function h(x, N, I[N}) = lB(x)glb(x(N))\I[N]\ 
is R T 
xAf(T)-
VBG* in R T for each b > 0. 

6.9. FRESNEL CONTINUITY 
PROPERTIES 
299 
With b > 0 given, choose a sequence a\ < Ü2 < as < · · · < 0 with cij —» 0 as 
j —> oo. Write lB(x)f(x,N;a,j) 
= fj(x,N) 
and 
hd(x,N,I[N]) 
= lB(x)ga.+ih(x(N))\I[N]\ 
= 
^(χ,Ν)9Λ(χ(Ν))\Ι[Ν]\, 
so for each 
(x,N,I[N]), 
hj(x,NJ[N}) 
-+ ft(x,W,/[JV]) = 1β(χ)^6(χ(ΑΓ))|/[ΑΓ]| 
as j —>> oo. 
Lemma 28 For 6 > 0 the function h(x,N,I[N]), 
= lB(x)gLb(x(N))\I[N]\, 
is 
integrable on R T. 
This follows7 from Theorem 63 and integrability of hj, since gLb(x(N))\I[N]\ is 
RTxA/*(r)-VBG*. 
O 
Lemma 29 For b > 0 ί/ie set B is GLb -measurable in RT. 
Proof. 
This follows from Lemma 28 and the variational equivalence of the 
distribution function GLb(I[N]) and its density version ^(a;(iV))|7[iV]|. 
O 
The final step is to deduce the equation 
/ 
lB(x)gtb(x(N))\I[N]\= 
[ 
lB(x)GLb(I[N}) 
= 0 
from the relation 
l i m / 
lB(x)gaj+lh(x(N))\I[N]\ 
= lim / 
lB(x)Gaj+Lh(I[N]) 
= 0. 
This deduction can be made from the distribution function or indefinite integral 
form, 
/ 
lB(x)Gaj+lb{I[N])^0, 
or from the density function form J R T lB(x)gaj+Lf)(x(N))\I[N]\ 
-> 0. But the 
relation Gaj+Lb(I[N]) —> GLb(I[N]) has not actually been established in the 
preceding steps, so here is a proof. 
Lemma 30 For b > 0 and each I[N), Gaj+ib(I[N]) -> Gib(I[N]) as j ->- oo. 
Proof. With I[N] given (so N is fixed, not variable as in R T) the proof can be 
carried out in the finite-dimensional space R N; so full advantage can be taken 
of the "good" properties of the incremental Gaussian function 
gaj+Lb{x(N)), 
which is continuous and repeatedly differentiable with respect to Xj for j = 
1, 2,..., n. The density form ga.+Lb(x(N))\I(N)\ 
is defined only for cells I(N) 
7 See the note appended to Theorem 63. 

300 
CHAPTER 6. GAUSSIAN 
INTEGRALS 
which are bounded above and below in R^. 
Therefore 
gaj+Lb(x(N))\I(N)\ 
converges uniformly to gib(x(N))\I(N)\ 
as j —> oo; and Theorem 56 then implies 
that 
/ 
gaj+ib(x(N))dx(N) 
-> / 
gaLb(x(N))dx(N) 
Jl(N) 
Jl(N) 
as j -► oo. That is, Gaj+Lb{I[N\) 
-> Glh(I[N]) for any such I(N) e 
I(RN), 
and hence for any corresponding I[N] G I(R T). To deal with cells I(N) which 
are unbounded in any dimension tj, a gauge δ(χ(Ν)) > 0 can be chosen so that 
each of \Gaj+i,b(I[N])\ and \GLb(I[N})\ is less than ε for ί-fine I(N). 
Q 
Lemma 31 There exist sets Ok C R T x Af(T), positive numbers ßk (k = 
1, 2,3,...), and a gauge 70 such that, for every jo-fine division VlQ 
ofHT, 
(V10)J2lek(x,N)lB(x)GlbI[N})\ 
< (p^)Yileh(x,N)GlbI[N])\< 
ßk· 
Proof. The result follows from Theorem 169. 
O 
Theorem 188 lfb>0, 
then 
[ 
1B(X)9*(X(N))\I[N]\= 
f 
lB(x)GLb(I[N}) 
= 0. 
Proof. Let ε > 0 be given, and let θ/e, /?&, and 70 be as in Lemma 31. Write 
a = J R T lB(x)GLb(I[N]). 
Choose 7 -< 70 so that, for every 7-fine division Ί)Ί 
of the domain R T, 
(νΊ)ΣΐΒ(χ)ΰώ(Ι[Ν])-α 
< ε. 
The objective is to show that a = 0. If we can find a single T>7 such that 
1(^7) Σ ^B(x)GLb(I[N})\ 
is less than some multiple of ε, then a must be zero. 
By Theorem 188, J R T 1B(X)Gaj+Lb(I{N]) 
= 0 for each j . Therefore there exist 
gauges 7j -< 7, j = 1, 2,3,..., such that, for every 7j-fine division V7j of R T, 
(ΌΊ,)ΣΐΒ(χ)βαί+Λ(Ι[Ν]) 
<η. 
where 77 is any given positive number. Each (x, N) G R T x λΓ(Τ) belongs to θ^ 
for some /c, and, in each case, there exists j = j(x, N) such that 
-io-fc 
\Glb(I[N}) - Gaj^N)+Lb(I[N})\ 
< 
εβ?2 
Taking η = sß^12-k, 
let Τ>Ί.(χΝ),= 
{(y, J[M])}, be a 7j(X)JV)-fine division of 
R T, so 
\(P^.sy) 
Σ 
lB(y)Gaj^N)+ib(J{M})\ 
< 
εβ?2-\ 

6.9. FRESNEL CONTINUITY 
PROPERTIES 
301 
For fc = 1,2,3,..., let£* ( x J V ) = {J[M] : ( 2 / , J [ M ] ) e D 7 j M ) , ( * , Λ θ € θ * } 
and let 
Ek ={J{J[M] : (y,J[M}) e P7j (*,*), (*,#) G Θ*} ; 
so £^ 
N is a 7J-(a.)jv)-fine (and hence 7-fine) division of i£fc. Then, for each &, 
Theorem 17 implies 
|(^,»>)Σ^)^ ( χ, Ν ) +^[Μ]) <2η = 
εβζ12~''+1. 
Now let 
-D = {j{(y,J[M}) 
: ( l i , J [ M ] ) 6 ^ ( > f f ) ) 
(x,iV) € Θ*. fc = 1,2,3,...} . 
By construction, V is a 7-fine division of R T, and (D) Σ GLb(I[N}) = QL\ + a2, 
where 
ttl 
= 
(D) £ 
lB(x)Gl6(J[JV]) - (P) £ 
lB(x)Gaj,XiN)+lfc(/[iV]), 
a2 
= 
(P)^l B(x)G a. ( x J V ) + t 6(/[iV]). 
Write B = ß x _/V(T), so l Bne,(z,iV) = l B(x)l e i(a:,iV). Then 
l«il 
= 
\(f>)Y,1B^)G,b(I[N])-(V)J£lB(x)GajiX:N)+Lb(I[N})\ 
I °° 
I 
= 
Σ (WE^ne.^.iV) (Gt6(/[iV]) - 
Ga^N)+ib(I[N})))\ 
U=i 
I 
00 
^ Σ ((^Σ^ηθΛζ,ΛΟ |Gtb(/[AT]) - Gaj(XiN)+t6(/[iV])|) 
k=l 
00 
< 
5 ^ (e/3^12-*/3fc) 
= ε; 
and 
fe=l 
|Q2| = 
\(V)J2lB(x)Gaj(XiN)+lb(I[N])\ 
I °° 
I 
= 
\£leh(x,N) 
(s^N))^2lB(x)Ga^N)+Lb(I\N})\ 
\k=l 
I 
I °° 
I 
U = i 
I 
00 
^ 
E|(£i(..w,)E1B(*)G.i(..W)+t6(/[Jv])| 
00 
< ^ Γ ε / ? ^ - ^ 1 = 2ε. 
fc=l 
Therefore |(P) Σ)G?t6(/[JV])| < |αχ| + |α2| = 3ε, giving the result. 
Q 

302 
CHAPTER 6. GAUSSIAN 
INTEGRALS 
I J 
Figure 6.2: Area in tail of normal curve. 
A 
/ 
! i / 
\\ 
\ 
v y 
Figure 6.3: Positive and negative area in tail of cos x2 
/ 
\ 
i 
M 
Figure 6.4: Positive and negative area in tail of e x cos a:2 

6.9. FRESNEL CONTINUITY 
PROPERTIES 
303 
This establishes that, relative to the underlying process, B is a measurable 
set, of measure zero, where B is D® or D®e. 
Is there some plausible reason for credibility of these continuity results? 
Their fundamental source is Chebyshev's inequality, Theorem 96. In review of 
the argument, consider first the case a < 0, b — 0, then the case a — 0, b > 0; 
and finally consider a < 0, b > 0. 
In the first case suppose z is a datum of a normally distributed observable 
Z = X(tj) — X(tj-i). 
Figure 6.2 shows that there is only a small likelihood 
that a transition or increment z = x(tj) — x(tj-i) 
will be large in comparison 
with tj 
—tj-i. 
In the second case (a = 0, b > 0), such likelihoods are manifested as in 
Figure 6.3. In this case the cancellation of positive and negative potentialities 
ensures that the aggregate potentiality of large z is small. 
Suppose likelihoods are based on a Fresnel-type distribution exp(cx2) with 
c = a + tb, a < 0, b > 0. Suppose a = — 1, b = 1. Then 
3?(exp(cx2)) = e~x cosx2, 
9(exp(cx2)) = e~x sinx2, 
and complex-valued potentialities have real and imaginary parts from compo-
nents of this kind. Figure 6.4 shows how Chebyshev's inequality comes into 
play. 

Chapter 7 
Brownian Motion 
Previous chapters can safely—indeed beneficially—be read on the understanding 
that the distribution functions are of the familiar non-negative kind. But, except 
where otherwise stated, those distribution functions are actually permitted to 
be complex-valued. 
In contrast, this chapter deals with a specific, complex-valued distribution 
function (and hence a specific observable X) based on the Presnel-type integ-
rands discussed in Chapter 6. 
7.1 
c-Brownian Motion 
The subject of this chapter is an observable XT with complex-valued (or c-
valued) distribution function G^, the indefinite integral or Stieltjes version of 
the incremental Fresnel density function g^ described in Chapter 6. 
Take T — [0, oo[ or any subset or interval of it; for instance, T = ]0, r] with 
to = x(to) := 0. The list BM1, ..., ΒΜ7 below is a summary of properties 
associated with Brownian motion. 
Taking c = — | gives the standard version of Brownian motion. But some 
of these properties are present in observables or processes other than standard 
Brownian motion, and, in anticipation of this, it is not stated that c = — \ in 
the following list of properties. To indicate this distinction the term c-Brownian 
motion is used, leaving the complex number c unspecified for the moment. 
BM1 For t G T, t > 0, the observables Xt are random variables with 
E[X t]=0, 
Vax[Xt] = -(2c)- 1i. 
BM2 For t, s e T, 0 < s < t, the observables 
Y = X8-XU 
y = 
xs-xt, 
are random variables with 
E[Y] = E [Xt - Xs] - 0, 
Var[y] = Var [Xt - Xs] = -{2c)-\t 
- s). 
A Modern Theory of Random 
Variation: With Applications 
in Stochastic Calculus, 
305 
Financial Mathematics, 
and Feynman Integration. 
First Edition. By Pat Muldowney 
Copyright © 2012 John Wiley h Sons, Inc. Published by John Wiley & Sons, Inc. 

306 
CHAPTER 
7. BROWNIAN 
MOTION 
BM3 The observable Xo has sample space R and a distribution function whose 
value on any interval / of R is 1 if 0 G /, and 0 if 0 ^ /. 
BM4 For t > 0 the observable Xt has sample space R and distribution function 
such that the potentiality C [(Xt G /)] for / G I(R) is given by 
•^-^/-(Yh-^jC-m 
\J\. 
J€I(I) 
If c = — i this is the Gaussian or normal distribution function N 'Y* with 
2 
mean 0 and variance t. 
BM5 For any s, £, with 0 < 5 < £, the observable Y = Xt— Xs has distribution 
function such that the potentiality C [(Y G /)] for / G I(R) is given by 
•^»■^/-(Ä)*-y?C-(Ä)w· 
If c = — i this is the normal distribution function N ,\t~s 
with mean 0 
and variance t — s. 
BM6 If Xt' - XS' is an observable with 0<s<t<s'<t', 
then Xt - XS and 
Xt' — XS' are independent. 
BM7 If T is an interval in [0, oo[ the sample paths x = {x(t) : t > 0} of XT are 
almost surely continuous. 
These conditions can be simplified a little, without any essential loss of gen-
erality, by prescribing that Xo be deterministic with value zero. This takes 
care of condition BM3. With this simplification, instead of [0, oo[, take T to be 
]0, oo[ = R+ (or a subset of R +, or interval ]0, r]) with to = x(to) := 0. 
Also, BM1 and BM4 are redundant since, with 5 = 0, they are implied by 
BM2 and BM5, respectively. They are included in the list for emphasis. The 
first task is to show that 
XT~XT[RT,G?] 
is a well defined joint-basic observable. Because of conditions BM1 and BM4, 
the observable 
Pt(XT)^Pt(xT)[nT,Gc] 
must be a random variable for each t, the function pt being the projection 
pt(xr) = xt for each XT G R T. Whenever c = — | , the cell function in BM4 is 
the distribution function of a normal random variable with mean 0 (Lemma 14) 
and variance t (Lemma 15), so in that case Xt is indeed a random variable and 
condition BM1 is satisfied. This also holds for any a < 0, with b > 0, as we will 
see. In the case of BM5 the observable is 
Xt-Xs^fst{xT)[RT,GT
c], 
where fst(x) 
= 
xt-x8. 
But it must first be confirmed that XT is itself an observable. 

7.2. BROWNIAN 
MOTION WITH 
DRIFT 
307 
Theorem 189 //a < 0; b > 0, and c = a + ώ Φ 0, iften X T ^ ^τ [RT,GT
C\ is 
an observable. 
Proof. This follows from Theorems 167 and 168, which imply that G^ is a well 
defined distribution function in I(RT). 
O 
Theorem 190 If a < 0, b > 0, c = a + tb, and T C ]0, oo[, then the observable 
XT CZ XT [ R T , G ^ ] satisfies conditions BM1 to BM6. 
Proof. Theorem 175 gives BM2 and, with τ' = 0, BMI. With the amendment 
mentioned above BM3 is satisfied. Theorems 176 and 178 give BM4, BM5, and 
BM6. 
O 
If the case a = 0 is allowed, we have c = a + ώ with a < 0, b > 0, c Φ 0. 
The following result holds for such c. 
Theorem 191 If c = ώ with b > 0, then the observable XT — 
XT[^T^G^] 
satisfies conditions BMS to BM6. 
Proof. 
The proof is the same as for the preceding result, except that the 
observables Xt and Xt — Xs are not random variables since their expected 
values do not exist. 
O 
Section 7.4 shows that, with continuous modification of the integral, this 
observable has continuous sample paths (property BM7). 
7.2 
Brownian Motion With Drift 
Suppose T =]0,oo[ and c — — | , so 6 = 0 and a = c = ^. The distribution 
function G = G^\ generates an observable process X = (Xt)teT for which 
2 
E [Xt] = 0 and Var [Xt] = t for each t € T. This process is standard Brownian 
motion. Suppose μ G R and σ2 > 0 are constant. By altering the distribution 
function G it is possible to construct an observable Xt having continuous sample 
paths x(t) (t e Γ), and satisfying properties corresponding to BM1 to BM5; 
subject to the amendment that, for t > t' > 0, 
E [Xt - Xt,\ = μ(ί - 0 , 
Var [Xt - Xt,] = a2(t - t'). 
We say that this observable is drifting Brownian motion with dnft rate μ and 
variance rate σ2. 
To produce drifting Brownian motion a distribution function ο μ σ on cells 
I[N] G I(R T) is constructed as follows. The standard normal distribution 
function N(J), = N°'i (J), has mean zero and standard deviation 1. The normal 
distribution Ν μ σ, = Ν ^ , is defined on cells J G I(R), and 
2 

308 
CHAPTER 7. BROWNIAN 
MOTION 
The corresponding elementary observable X ~ x [R, Ν μ σ] is a random variable 
with expected value E[X] = μ and variance Var[X] — σ1. In (6.14), a similar 
adjustment is made to the density function gc(xj\xj-\), 
with c = — ^, so 
1 
( 
1 fxj - (xj-i + ß(tj - tj-i)) 
2> 
(7.1) 
Subject to this adjustment of the underlying density function, the construction 
of Ωμσ is the same as the construction of G^i {I[N}). The observable Χτ — 
2 
χτ [RT,GM<7] satisfies the following Brownian motion properties: 
DB1 
E [Xt] = μί, 
Var [Xt] = σΗ 
DB2 
E [Xt - Xs] = μ{ΐ - s), 
Var [Xt - Xs] = a2(t - s). 
DB4 The random variables Xt have distribution functions N^V7 
. 
2 
DB5 The random variables Xt — Xs have distribution functions Ν ^ - 5 ) ' σ ν ί - 3 
2 
The method of proof of these properties is the same as for the correspond-
ing properties of standard Brownian motion in the preceding section. As be-
fore, DB2 and DB5 imply, respectively, DB1 and DB4. Property DB4 con-
fers drift rate μ and variance rate σ2 on the drifting Brownian motion XT — 
xT[RT,G^]. 
The methods of Section 7.4 of this chapter show that, with continuous mod-
ification of the integral, this observable has continuous sample paths (property 
DB7). 
7.3 
Geometric Brownian Motion 
This section introduces another offshoot of the transitional or incremental Pres-
nel distribution of the preceding sections. It is possible to form a well defined 
joint-basic observable X ~ x [Rn, Gc], because, with change of variable—that 
is, change of integrator function—Theorem 46 (integration by substitution) and 
Theorem 154 imply that J R n GC(I) = 1, where a < 0, b > 0, c = α + ώ φ 0, and 
n 
ftGR, 
σ, > 0 , 
J=Y[jj, 
Jj e I(R), 
J<El(R n); 
so G c is a distribution function on I(R n). This uses properties of a version of 
Presnel's integral, 
L
exp{
c(^))
dy=a^ 

7.3. GEOMETRIC BROWNIAN 
MOTION 
309 
which is an extended Riemann integral, and therefore a Henstock integral. This 
is the starting point for investigating c-Brownian motion of both standard and 
drifting kind. With 
6 = 0, 
a = -—^, 
σ > 0, 
μ G R, 
J G I ( R ) , 
the normal or Gaussian distribution function is 
N ^ ( J ) = N ^ (J) = - 1 = / e~^{y-»)2dy, 
(7.2) 
2 
σ\/2π Jj 
for which the density function is 
n(y) = (2π)"* exp (-\σ~2(ν 
- μ) 2) . 
(7.3) 
We now examine a variant of this, with a view to constructing another version 
of Brownian motion. Let p, σ G R be given, with σ > 0, and, for z G R, let 
t is the lognormal density function. By replacing the function n with the func-
tion £, and then retracing the steps involved in formulating Brownian motion, 
the process known as geometric Brownian motion emerges. 
The following are some standard results (see, e.g., Kwok [134]) which we use 
to formulate geometric Brownian motion. 
Lemma 32 
/»OO 
/ 
i{z)dz = 1. 
Jo 
Proof. Let y = In2, so ^ = \, and 
as required. 
O 
Lemma 33 
zt(z)dz = exp ( p + — 
/ 
./o 

310 
CHAPTER 7. BROWNIAN MOTION 
Proof. Choose a > 0. Letting y = In 2, 
[zi{z)dz = 
L
ev^
vA-
{1^f)
evdy 
[°° 
( 
y 2 _ 2 
+ 
2_ 2 < T2 ^ 
i^Lexp 
& 
]dy 
= 
exp 
Writing Ja = ]lna,oo 
[ 
zi{z)dz = Γ 
z£(z)dz = exp (p + ?-) Ν'+ σ 2' σ(J a). 
(7.5) 
J]a,oo[ 
Ja 
\ 
2 
/ 
The left-hand integral on ]a, oc[ can be interpreted as either an extended Rie-
mann or Henstock integral. In the former interpretation it becomes the integral 
πτι R + by letting a —> 0. Either way, the result follows from (7.5). 
Q 
on 
Lemma 34 
/•OO 
/ 
z2£(z)dz = exp (2p + 2σ 2). 
Jo 
Proof. Letting y = In z, we have 
J=-£exp(2v)exp(-i(^) 2) d t, 
s? /„exp (" h(j!"2ra+"2 
-
iah))
iy-
Since 
y2 - 2py + p2 - 4σ2?/ = τ/2 - 2 (p + 2σ2) y -f (p + 2σ2)2 - 4ρσ2 - 4σ4, 
the latter integral can be expressed as 
-<*♦** )fe/:-(-K(^r^)>»)) 
whose value is exp (2p -f- 2σ2). 
O 

7.3. GEOMETRIC BROWNIAN 
MOTION 
311 
For J e I(R+) = I(]0, oo[), define 
L(J» - / / < * · = ^ / > p ( 4 ( ^ ) 1 *·■ 
(7·6) 
Given p G R and σ > 0, Lemma 32 and Theorem 18 imply L is integrable on 
R+ with integral 1, so it is a potentiality distribution function in R +, and 
Z ~ 2 [ R + , L ] 
is an observable, with lognormal distribution function L . 
For our purposes the parameter σ will generally be held at the same con-
stant value. On the other hand p will be allowed to assume different constant 
values. Therefore, wherever necessary, the notation will be extended to keep 
track of such differences. While the generalized normal distribution function N 
is allowed to depend on a complex-valued parameter c, in the lognormal distri-
bution function L the parameter c will take only the traditional value of — ^, so 
the subscript — ^ is omitted. 
Theorem 192 Given p G R and σ > 0, the observable Z ~ z[R+,L] is a 
random variable with mean value and variance, respectively, 
exp ( p + - σ 2 j , 
exp (2p + σ2) (exp σ2 - l) . 
Proof. By Lemma 33 and Theorem 18, the expected value of Z exists, so Z 
is a random variable, and its mean value is exp (p + \o~2). By Lemma 34 and 
Theorem 18, the variance of Z exists and equals 
/ 
z2t(z)dy- 
[ j 
zt(z)dz\ 
, 
which equals 
exp (2p + 2σ2) - ί exp (p + y ) ) > = e xP (2P + 2ij2) ~ e xP (2P + σ 2) > 
and this gives the result. 
O 
With Z ~ z [R+, L], Theorem 192 shows that the contingent random variable 
expZ~e*[R+,L] 
is normally distributed with mean p and variance σ2. Therefore 
Y ~ y [R, Ν ρ σ], 
In Y ~ In y [R, Ν^σ] 
are a pair of corresponding random variables Y, Z which are related by 
y = expZ, 
Z = ]nY. 

312 
CHAPTER 
7. BROWNIAN 
MOTION 
So £(z), defined by (7.4), gives E[Z] = exp(p + \σ2). It will be useful to have a 
lognormally distributed random variable Z that has E[Z] = expp. A variant of 
the function £ gives this. Let 
«"M - ^«»(-""-'S'·'"')· 
(TT) 
L'(J) 
= 
fj£^(z)dz 
for cells J C R+. Whenever it is helpful to indicate, in addition, the parameter 
σ, we will write Lp as LP<T. With this amendment we get the following version 
of Theorem 192. 
Theorem 193 Given p G R and σ > 0; the observable 
Z~z[R+,Lp] 
is a random variable with mean value and variance, respectively, 
exp p, 
exp 2p (exp σ2 — l) . 
In this case the underlying normally distributed random variable is 
Y 
-y 
for which 
R , N P " ~ 
E[Y] =p- 
\σ2, 
Y = expZ, 
Z = lnF. 
With t > t', and replacing p and σ2 by p(t — t7) and a2(t — ί'), respectively, the 
following form of this result is useful: 
E[Z] = exp (p(t - t7))» 
Z = \nY, 
E[Y] = (p - |σ 2) (t - t')· 
(7.8) 
The above properties of Lp are deduced from the corresponding properties of 
the distribution function Ν ρ σ in one dimension, and can be extended to finitely 
many, and infinitely many dimensions, as the Presnel integrals <p, g, and G were 
extended in Sections 6.2, 6.3, 6.5, and 6.7. 
Rather than repeat the proofs in detail for the lognormal distribution func-
tion Lp, we list the main results for a "transition" or "increment" version of 
L, corresponding to the transitional Presnel integrands g and G discussed in 
Sections 6.8, 6.9, and 7.4. The detailed proofs are, mutatis mutandis, the same 
as before. 
As before, let T C R be an indexing set, such as T = ]0, oo[ or a bounded 
sub-interval of it. The aim is to define variables z(t) G R+ (t G T) and as-
sociated distribution functions, such that the corresponding observables Zt are 
lognormally distributed random variables with increments Zt — Zt> having mean 
value p(t — t') for all t > t'. In other words, the intention is to construct a dis-
tribution function 
0 : Ι ( ΐ φ - > [ Ο , 1 ] 

7.3. GEOMETRIC BROWNIAN 
MOTION 
313 
and a corresponding joint observable 
(Zt)t€T = Zczz[Rl,g] 
such that, writing Ztt> = Zt — Zt', we have 
E [Zu* \Zt>]= p(t - t') 
for all 
t>t'. 
The required distribution function and observable are constructed as follows. 
With p € R and σ € R+ given, form an observable 
Y 
~y Ητ,σΓιτ 
that is, Brownian motion with c = — | , drift rate p—\, 
and variance rate σ1 
This means that, for any t, t' G T, t > £', we have 
E[rt|iv] = /"„(i) 
JR 
e X P 
2σ 3(ί-ί') 
a^2n(t 
- V) 
-dy 
Similarly, Var(Yt\Yti) =a2(t- 
tf). Thus, if t - tf = 1 and if Yt/ has given value 
y(t'), the expected value of Y* is 2/(£') + p— \ 
a nd ^s variance is σ2, confirming 
2 
that the drift rate of the Brownian motion Y is p — ^-, and its variance rate is 
σ2. Writing 77 = v(yj,yj-utj,tj-i,p,a) 
where 
v(yj,yj-utj,tj-i,p,<7) 
= 
- - 
— 
2a2(t--t--i) 
the distribution function for Y ~ 2/ R T,G_; 
IS 
^•Ί/iiv])=/ (ft 
/ 9 Γ \ ^ *w 
for /[TV] e I(R T), iV = {ii,... ,<„} £ Λ/"(Τ), % = y(tj). 1 < J < n. Therefore 
the distribution function Q for the observable Z ~ z [R+,£] that, for each 
ί € T, satisfies 
Yt = In Zu 
y(t) = In z(t), 

314 
CHAPTER 7. BROWNIAN MOTION 
is G(J[N]), = QP°(J[N}), 
where g»"{J[N\) equals 
/ 
JJ{N) 
M 
zja^nitj-tj.,) 
\ 
) 
dz(N) 
(7.9) 
for J[N] e I(R+). With this specification οϊ G = Gpa in Z ~ z [R^, 0] , we say 
that the geometric Brownian process Z has growth rate p and volatility σ. 
The source of expression (7.9) is in Section 7.2, which gives ΰμσ as the joint 
distribution function on R T for drifting Brownian motion with arbitrary drift 
rate μ. Thus it is possible to assign an arbitrary growth rate to geometric 
Brownian motion by assigning the parameter p in the distribution function 0μρ. 
This is summarized as follows. 
Theorem 194 For any p > 0 and any fixed τ, τ' (τ > τ'), the observable 
Z~z 
\ÜF+,Gpa\ satisfies 
Έ{Ζτ\Ζτ,] 
= ζ(τ') + 
ρ(τ-τ'). 
Proof. Take M = {τ,τ'} 
with z(r') given. As in the proofs for Brownian mot-
ion (and Brownian motion with drift), the integrand z(t)Gp<T(I[N]) is cylindrical, 
so Theorem 159 can be applied (either directly using change of variable, or 
by applying the argument of regular partitions of R+). This establishes the 
existence of 
/ 
z(t)G»°(I[N}), 
with value equal to the value of an integral on R ^ , and since z(rf) is fixed or 
constant, this reduces again to an integral on R+. The value of this integral is 
then deduced from Theorem 193, giving the result. 
O 
This result demonstrates how a geometric Brownian motion can be con-
structed with growth rate p where p is any real number. In Black-Scholes 
pricing theory (Section 8.16), the price z(t) of a financial asset at any instant t 
is assumed to be an occurrence of a geometric Brownian motion 
Ζ~ζ[Κΐ,ΰμσ] 
with actual growth rate μ and volatility σ. In order to determine the variable 
values of some related financial asset, this theory requires construction of a 
geometric Brownian process 
z~z{nl,Gpa] 
whose volatility σ is the same as the volatility of the observable Z, but whose 
growth rate p is the risk-free interest rate. Theorem 194 makes it possible to 
define geometric Brownian observables in R^, each having the same volatility 
σ, but with any growth rate p that we wish to assign. 

7.4. CONTINUITY 
OF SAMPLE 
PATHS 
315 
7.4 
Continuity of Sample Paths 
This section reverts to c-Brownian motion with joint distribution function G^ 
on I(R T), where a < 0, b > 0, c = a + ώ φ 0, and T is ]0, oo[ or a bounded 
sub-interval of it; and with x(to) = to := 0. The discussion is also valid for G£a 
(Brownian motion with non-zero drift) and Q (geometric Brownian motion). 
Some observables f(Xr) — / ( # T ) [ R T , G^] are meaningful only if the sample 
paths 
XT = 
(Xt)t€T 
are continuous. To illustrate, suppose U is a real-valued, continuous function of 
y G R, and suppose 
U(xT) = exp ( / 
U{x(t))dt 
If T is a real interval, and if the sample path xτ is continuous, the integral in the 
exponential function can be understood as a Riemann (or extended Riemann, 
or Riemann-complete) integral. 
Consider a function / of the process Xr? with f(Xr) = U(XT), 
where 
U(XT)^U(XT)[RT,G^}. 
The function U(XT) is a limit1 of a function e^-^ 3^'*· of Riemann sums, the 
intervals ι being cells in T. Can U{XT) be regarded as a random variable, or 
observable, in this case? 
In Chapter 6 the symbols D, DT, or Ό^θ were used to denote the com-
plements in R T of, respectively, the sets C, (7T, or C^e of x G R T which are 
"continuous relative to T" in some sense. The element x can be included in C 
if, for instance, for each x G R T \ B T, with ε > 0 given, there exists δ > 0 for 
which 
t,t'eT 
and 
\t-t'\<5 
imply 
\x(t) - x(t')\ < e; 
(7.10) 
so that x G D if such a continuity condition fails. If it is the case that 
lD(x)Gc(I[N}) 
= 0 
L 
then the definition of U(XT) can be extended as follows: 
u{xT)..= [^U(x{t))dt 
it χε*τ\ϋ, 
(7.η) 
v 
' 
\ o 
if i e D , 
and in that case W(XT) is defined for x € R T; and U(XT) — U{XT) [ R T , G%] is 
an observable. Also, if it happens that 
E[U(XT))= 
[ 
U(xT)GT
c{I[N)) 
1ln Chapter 8 some meaning is sought for stochastic integrals—observables constructed by 
taking a limit of Riemann sums J ^ h(X(tj-\)) 
(X(tj) 
— X(tj-i)). 
In such cases the continuity 
or otherwise of the sample paths x(t), t ET, is also of interest. 

316 
CHAPTER 7. BROWNIAN 
MOTION 
exists, we can say that U(XT) is a random variable. 
Theorems 187 and 188 provide the first steps in this direction, summarized 
as follows. 
Theorem 195 If T is a dense, countable set of real numbers, and if c = a + ώ 
with a < 0 and b>0, 
then the observable X ~ x [R T,G^] satisfies conditions 
BM1 to BM7. 
Proof. By Theorem 190 conditions BM1 to BM6 hold. Theorem 187 gives 
BM7. 
O 
Theorem 196 If T is a dense, countable set of real numbers, and if b > 0, 
then the observable X ~ x[R T,G^] satisfies conditions BM3 to BM7. 
Proof. By Theorem 191 conditions BM3 to BM6 hold. Theorem 188 gives 
BM7. 
O 
7.5 
Introduction to Continuous Modification 
However, the preceding results do not meet the requirements of, for instance, 
definition (7.11); in which T should be a real interval, not a countable set. 
Thus the question arises whether it is possible to give some workable meaning, 
as joint-contingent observables or joint-contingent random variables, to entities 
such as 
U(XT)· 
Suppose T is a real interval. One way to proceed is to choose a countable, 
dense subset T' C T. Let DT denote the set of χτ> € R T which are discont-
inuous relative to T' (in the sense that condition (7.10) fails, with X" replacing 
T). Consider the projection of R T to R T : 
RT 4 
RT, 
XT = (xt)t£T 
~> P{XT) = XT' = {xt)teT>-
We have 
DT C p - 1 
(Dr) 
as a proper subset; but if χτ> € R T \ DT then, by continuity, the set 
Ρ _ 1({*τ'}) 
contains only a single element, which is XT £ R T \ DT, since ρ(χτ) — XT' · 
Now suppose a function / (such as U) can be meaningfully defined on the set 
R T \ DT. Then, setting /(#τ) = 0 for XT € DT\ consider whether the following 
can be given some meaning: 
E[f(XT)}= 
f 
f{xT)GT
c{I{N])= 
( 
lRT\DT(xT)f(xT)G?(I[N}). 
JUT\DT 
JnT 

7.6. CONTINUOUS 
MODIFICATION 
317 
Because of one-to-one correspondence between 
xT eRT\ 
DT 
and xT, G R T \ 
Dr, 
we can define 
f f(xT) 
for 
xT,eRT'\DT\ 
f(*T>):={ 
r 
' 
(7.12) 
I 0 
for 
xT, e DT . 
Then define 
rxT,ei[N}* 
Ji[N]ei(nT') 
rxT,ei[N]* 
E' [f(XT)} := E [/(Xr,)] - / 
/ ( s T O G f i ' M ) . 
(7.13) 
■//ΓΑΠ€ΐ(Κτ') 
Theorem 197 If a < 0, b > 0, and c = a + ώ φ 0 then E'[f(XT)] 
zs we/Z 
defined. 
Proof. This follows from Theorems 195 and 196. 
O 
Thus, if E' [f(XT)] = / R T' /(xrOGT'^f^]) exists^ t h e observable f(XT) 
is 
a random variable in this modified sense. 
7.6 
Continuous Modification 
In the preceding section, a modified expectation calculation was defined by 
replacing the sample space R T with a sample space R T where T' is a countable 
set. If R T is preferred as the sample space a different approach can be adopted; 
one that gives the same modified expectation value for the random variable in 
question. 
To illustrate the ideas involved, here is a finite-dimensional example. 
Example 57 The following is a finite-dimensional analog of the above problem. 
In place ofHT 
and R T, consider R and R x R. Suppose f{x\,X2) 
is defined 
in R x R ; and suppose a distribution function F\2{I\ x I2) is defined on the 
two-dimensional intervals of R x R. The integral fnxnf 
(x 1^X2)Fi2{h x ^2) 
corresponds to integration in R T. 
Suppose a function f is defined on the set Z = {(x,x)} 
c R x R , but f is 
meaningless for values outside Z. 
Corresponding to the attempt above to integrate a function f(x) with respect 
to G^ in R T \ DT, we seek, in the analog, to integrate the function /(#,#) in 
the set Z. Since Z is a null set in R x R this is problematic, but we can try to 
give it meaning by modifying the definition of the integral. 
Define a function fi(xi) 
in R by fi(xi) 
:= f(x,x), 
define a distribution 
function Fi(Ii) 
:= F{h x R), and then define E7[/] to be 
Jnfi(xi)Fi(Ii) 
whenever this integral and expectation are well defined, and whenever the integral 
exists. This corresponds to the modification described in the preceding section. 

318 
CHAPTER 7. BROWNIAN 
MOTION 
The modified expectation E'[/]—an integral in one dimension—can be in-
terpreted as J R X R / ( ^ ? ^ ) ^ ( ^ I 
x R)> «^ integral in two dimensions. 
For this, 
partitions of R x R must be defined by using intervals I\ x R, where the sets 
{Ii} partition the one-dimensional R. 
Accordingly, multiply representative values f(x,x) 
by weights F{I\ x R) 
which add up to 1. Then, if f is F-integrable on R x R, using these modi-
fied intervals and partitions, denote the resulting modified expectation by 
E · [ / ] = / 
f(x,x)F(hxR), 
JUxH 
and again it is reasonable to regard E*[/] as the mean value in Z of f. 
By 
comparing the Riemann sum estimates in each case it is found that 
E'[/]=E*[/]. 
In contrast, if we attempt to obtain the average by means of Riemann sums 
Y^f(xi,X2)lz{xi,X2)F(h 
x L·), the weights F(Ii x ^2) would not add up to 1, 
so the result could not be interpreted as an average value for f in Z. 
O 
Using the ideas in Example 57, we now construct another version of continu-
ous modification in R T, leading to a modified expectation E*. For the modified 
integration in R T, re-define the intervals I[N] of the associated point-cell pairs 
(xJ[N])eRT 
xI(RT) 
which are used to form Riemann sums for the integral in R T. As before, with 
T an interval of ]0, 00[, choose a countable dense subset T' C T. To form the 
required cells, let 
I'(T) := {I[N] : N G N{T')} . 
(7.14) 
In other words, each cylindrical interval /, to be used in the modified integration 
in the sample space R T, has the form 
I = Itl χ / ί 2 χ · · · χ / ί η 
χΒη{*ι,...,*»} 
where 
{*i, · · ·, tn} C T' instead of T. 
The meaning of a gauge 7* = (L, δ) is accordingly modified so that only those 
(x,I[N}) that satisfy I[N] e Γ(Γ), as defined by (7.14), are subject to the 7* 
gauge restrictions. Nothing else in the definition of the integral of a function 
h(x,N,I[N]) 
is to be changed. Thus h is integrable on R T, with integral 
rxei[N]m 
a= 
h(x,NJ[N]), 
(7.15) 
Ji[N]ei'(nT) 

7.6. CONTINUOUS 
MODIFICATION 
319 
if, with ε > 0 given, there exists a gauge 7* so that, for each 7*-fine division 
£>, = {(x,I[N) 
: I[N] G Γ(Τ)}, of R T, we have 
a-(V)J2h(x,N,I[N}) 
< £. 
For this modified definition of the integral to have meaning, 7*-fine divisions 
of R T must exist. But a small modification of the proof of Theorem 4 ensures 
existence of such divisions. (The elements (L, δ) of a gauge 7* can be restricted 
to R T , and any resulting division of R T corresponds to a 7*-fine division of 
R T. The integration theory of Chapter 4 holds for this construction.) 
Given a function f(xr) 
that is meaningful only for XT G R T \ DT, define 
f(xT) 
:= 0 for x G DT. 
Then, using (7.12), modified integrability on KT\DT 
can be obtained as follows. 
Theorem 198 Suppose T is a real interval and T' is a countable, dense subset 
ofT. Suppose h(I[N]) is a function depending only on It fort £ N, and suppose 
pxT,ei[N}* 
/ 
f(xT,)h(I[N]) 
exists, with 
pxT,ei[N']* 
/ 
lDT,(xT>)f(xT>)h(I[N'])=Q. 
Then the modified integral 
rxTei[Ny 
exists and equals 
Γ 
f(xT)h(I[N\) 
Ji[N]ei'(nT) 
/ 
f{xT>)h{I[N']). 
Ji[N>)ei(nT') 
Proof. Consider a gauge 7 = (L, δ) in R T \ For xT eRT\BT 
and N' G N{T') 
let L(xT) 
= L(xT>) and δ(χτ,Ν') 
= δ(χτ',Ν'). 
Let L(xT) 
and δ(χτ,Ν) 
be 
otherwise arbitrary. Thus there is a gauge 7* corresponding to 7. If χτ> G 
R T \DT', I[Nf] G I(R T ) and (χτ',Ι[Ν']) 
is 7-fine, then there is a corresponding 
7*-fine (xT,I[N]) 
with xTGRT\ 
DT, I[N) G I ' ( R T ) , and 
I[N] = 
I[N']xRT\N'. 
Therefore, for χτ> G R T \ Ότ> , XT G R T \ DT and, for any 7- and 7*-fine 
divisions D 7 and P7* of R T' and R T, respectively, 
(V7)J2f(xT,)h(I[N'}) 
= (Vy.)J2f(xT)h(I[N}). 
O 

320 
CHAPTER 7. BROWNIAN 
MOTION 
If h is also a potentiality distribution function, h(I[N]) = F(I[N]), then, 
provided / has the properties prescribed in Theorem 198, a modified expectation 
of the observable f(Xr) 
can be defined by means of the modified integral2 
rxTei[N}* 
[f(XT)} := / 
f(xT)F{I[N\), 
Ji\mei'(B.T) 
rxTeI[N]* 
!i[N]ei'(RT) 
and then Theorem 198 implies 
E [f(XT,)} = E' [f(XT)} = E* [f(XT)} ■ 
The distribution functions G;T and G^ have the properties mentioned in The-
orem 198. Therefore the following holds. 
Theorem 199 Suppose T is a real interval and T' is a countable, dense subset 
ofT. 
Suppose a < 0, b > 0, and c = a + ώ Φ 0. Suppose f(xr) 
is defined for 
x G R T \ DT and suppose f(xrf), 
defined by (7.12), is integrable on R T , with 
L 
lDT,{xT,)f{xT,)GT
c{I[N'])=ü. 
Taking f(xr) 
= 0 for XT G DT, the observable 
f(XT)^f(xT)[RT,Gj] 
is a random variable in the modified sense, and 
E[f(XT,)]=E'[f(XT)]=E*[f(XT)). 
Proof. This follows from Theorem 198. 
O 
Note that, when 6 = 0 and a < 0, we have V^ (DT ) = 0; so, for any point 
function /(xjv)> Theorem 38 implies 
/ 
lDT>(xT')f{xT>)Cf?(W']) 
= 0. 
JnT' 
In other words this need not be a hypothesis in Theorem 199 whenever 6 = 0, 
a < 0 . 
7.7 
Introduction to Marginal Densities 
This section introduces versions of the observables, distribution functions, and 
their expectations in which the integration in one of the dimensions is omitted. 
For that reason we will speak of marginal distribution densities and marginal 
expectation densities. The rationale for this terminology is as follows. 
2The * symbol in E* indicates continuous modification of the integral, whereas I[N]* 
denotes associated points or vertices of I[N]. 

7.7. INTRODUCTION 
TO MARGINAL 
DENSITIES 
321 
If Fip(I) = Fi^(h 
x h) is a distribution function in R x R, and if there is 
a function g{x\,X2) such that, for each J = J\ x J2 € I(R x R), 
Fh2(Ji 
x J 2 ) = / 
^(a?i,x2)|/i||/2|, 
then g is a density function for Fi^· Note that F is the indefinite integral3 of 
g. Then, by Theorem 54 (Pubini's theorem), 
F2{Ji,x2) = / 
g(xi,x2)\Ii 
exists, and 
Fh2(Ji 
x J 2 ) = i 
F2{Jux2)\I2\. 
(7.16) 
By analogy with the calculations on the numbers at the "margins" of Table 1.4, 
call the function F2(Ji,x2) 
the marginal density function for the distribution 
function Fi^> 
Now suppose 
f{X) = /(Xi,X 2) ^ /(xi,x 2) [R x R,*if2] 
is a joint-contingent random variable. By Theorems 18 and 44, 
E [/(*)] = / 
f(xi,X2)Fia{h 
xh)= 
[ 
f{xi,X2)9(xi,X2)\Ii\\I: 
i R x R 
i R x R 
Then, by Fubini's theorem, for each X2 £ R, 
2 · 
exists, and 
Ψ{Χ2) ·= 
fBtf(xi,X2)9(xi,X2)dxi 
= 
fKf(xi,X2)g(xiiX2)\h\ 
= 
fRf(XuX2)F2(Il,X2) 
E[f(X)}= 
( 
ψ{χ2)\Ι2\. 
Jn 
We call ψ(χ2) the marginal density function for the expectation E[f(X)], 
or 
simply marginal density of expectation. To summarize, 
E[f(X)]= 
/ <ψ(χ2)άχ2= 1(1 
f(xi,X2)9(xuX2)dxi)dx2· 
If F and # are non-negative then g is a probability density function, since, in that case, F 
is a probability distribution function in the traditional sense. 

322 
CHAPTER 7. BROWNIAN 
MOTION 
7.8 
Marginal Densities in R T 
For 0 < τ' < r and real numbers £, £', let T denote the interval ]r'',τ] closed 
on the right; and let T~ denote the interval )r'\r[ open on the right. Let 
R T 
= RJT ,r^ denote the following set of x: 
]T',T[ 
A 
R 
t 
—l· x(t), with 
x(r') 
= 
ξ' and 
x(r) 
= 
f; 
while R T = R^T'^ 
denotes 
]τ',τ] 
A 
R 
t 
—ϊ x(t), with 
x(r') 
= 
ξ'. 
The domain Ή)τ ,Tt can be interpreted as the set {(xt) ' t £]τ',τ[}, with xr> — 
X(T') — £' a nd xr = x(j) — £, for x G R^r ,rL In contrast, the domain R^T 'rl 
has variable xT. 
In other words, viewed as Cartesian product space, IÜr ,rt has dimension r 
removed, along with the corresponding integration on the variable χ(τ). 
This 
corresponds to the removal of the integration on variable X2 in the marginal 
densities for R x R of the preceding section. 
c-Brownian motion can be defined in R^T ,rl. In this case, instead of a;(to) — 
x(0) = 0 we have χ{τ') = £', and the joint observable is 
XT~xT[RT,G?}, 
where T =]τ',τ]. 
Choosing 
r' = t0 < ti < t2 < · · · < tn = r, 
and with gc(xj\xj-i) 
defined as in (6.14), define a corresponding version of 
9c(x(N)): 
n 
gc(x(N))=gc(x(N);£,r') 
:= 
T[gc(xj\xj-i), 
i=i 
the difference being that now we have to = τ' and tn = r. With T =]r', r] and 
W = {ti,..., U ^ W ) , 
let T~ denote ]τ',τ[ and, removing the element tn from the set TV, let 
TV- :={ίι,...,ί„_!}€ΛΓ(Γ-). 

7.8. MARGINAL 
DENSITIES IN R T 
323 
This corresponds to the removal of the variable x2 to form densities in R x R 
in the preceding Section 7.7. Now suppose, for the moment, that the elements 
£i,...,£ n-i of N~ are fixed constants. (The elements to = τ', tn — r are 
already fixed.) Suppose J(N) G I(RiV) is given. Define the cell function Gc on 
cylindrical cells in the infinite-dimensional domain R T as follows: 
ry(N)eJ(Ny 
GC(J[N}) = Gc(J[N];£,Tf) 
:= / 
9c(y(N)^y)\I{N)\. 
Jl(N)el(J(N)) 
The integral on the right hand side exists, as a Riemann or extended Riemann 
integral in the finite-dimensional domain R^. By the consistency and integra-
bility arguments used in Theorems 167 and 168, the cell function Gc on the left 
is well defined on each of the cells I[N] G I(R T). 
Now suppose ξ is a given real number, and χ(τ) = ξ is fixed. Again suppose 
the elements £i,...,£ n_i of N~ are fixed constants; and suppose J(N~) 
G 
I(R i v ) is given. Define the marginal distribution density 
GC(J[N~]) 
= 
Gc(J[N-];?,T'rt,T) 
:= 
/ 
9MN-y,ey)\I{N-)\ 
(7.17) 
Ji(N-)ei(J{N-)) 
= 
[ 
gc(y(N-);ti',Tf)dy(N-) 
JJ(N-) 
= 
/ 
9c(y(N~)]£,T')dyidy2'-dyn-i. 
Jj(N-) 
Since the integrands involved are Riemann integrable or extended-Riemann int-
egrable, the finite-dimensional integral on the right defines the value of Gc for 
each cylindrical cell J(N~) of the infinite-dimensional domain R T . The latter 
can then be written as 
r 
"LZ! 
exp c(^-^--i) 2 
Gc{I[N-WS^r) 
= / 
Π 
/ , 
, uj~\ 
=dx(N~). 
Jl(N-) fj[ V ^ - C ) " 1 ^ - * i - l ) 
Theorem 200 If a < 0, b > 0, and c = a + ώ Φ 0, then GC(I[N}) and 
GC(I[N~}) are well defined and exist. 
Proof. This follows by the methods used to prove Theorems 167 and 168. 
O 
Theorem 201 If a < 0, b>0, 
and c = a + ώ φ 0, and if 
M G Λ/ΧΓ), Af- G Λ/*(Τ"), 
J[M] G I(R T), 
J[M~] e I ( R T _ ) 
are fixed, then 
GC(J[M])= 
/ 
Gc(J[M-te,y]t,T)\IT\= 
/ G c(J[M-];0de. 
^/TeI(R) 
«/R 

324 
CHAPTER 7. BROWNIAN 
MOTION 
Proof. This follows by application of Fubini's theorem (Theorem 54, finite-
dimensional version). 
O 
Theorem 202 lfa<0,b>0, 
and c = α+ώ φ 0, then Gc(I[N 
]) is integrable 
on RT~ = B)T'^, 
and 
/ 
Gc(/[7V-]);^,r';e,r) = 
/ 
^ 
(7.18) 
7RkV[ 
^ ( - c ) _ 1 ( r - τ') 
Proof. Integrability follows from Theorems 168 and 54; and evaluation of the 
integral is similar to the evaluation in Theorem 168. 
O 
Theorem 203 lfa<0,b>0, 
αηάο = α + ώφ0, 
and if E e E (B)T'>T(\ 
is a 
given figure, then GC(I[N~]) is integrable on E. 
Proof. This follows from Theorems 202 and 17. 
O 
These results establish that GC(I[N~]) is a density function, and, whenever 
T has more than one element, the analogy with (7.16) demonstrates the rationale 
for the terminology marginal density function. 
Results for GC(I[N~]) and gc(x(N))\I[N~]\ 
which are analogous to those 
obtained earlier for GC(I[N]) and gc(x(N))\I[N]\ 
can be proved by similar meth-
ods. For ease of reference some of them are listed in Theorem 204 below. 
As before, use the notation N~ to represent members of the collection 
Λ/"(]τ',τ[), = λί(Τ~). 
The density relationship connects R]r''r[ to R^''^, and 
hence the elements N~ of Λ/"(]τ',τ[) to the elements N of 
Λί(]τ',τ]). 
Theorem 204 Suppose T~ =]r ;, r[, and a < 0, b > 0 with c = a + ώ φ 0. 
1. gc(x(N~))\I[N~])\ 
is variationally equivalent to GC(I[N~]). 
2. If f{x) is a given point function* 
and if either of 
f{x)gc{x{N))\I[N~}\, 
f(x)Gc(I[N-}) 
is integrable, then the other is also integrable and the two integrals are 
equal. In particular, taking f(x) to be 1, gc{x(N))\I[N~])\ 
is integrable 
on RlT 'rt; with integral equal to 
exp \ _ r / 
v M - c ^ H r - r ' ) 
4This property also has a version with f(x,N) 
instead of f(x)· 
See Theorem 50. 

7.8. MARGINAL 
DENSITIES IN R T 
325 
3. Suppose f(x(N 
)) is defined for each x(N 
) € R'T ' r' and each N 
€ 
Λί(]τ',τ[), 
and suppose 
H(I[N-]) 
= / 
f{x{N-))gc(y(N-))dy(N-) 
Jl(N-) 
exists for each I[N~] G I(RJr 'rf). If f(x(N~)) 
is a continuous function 
then H(I[N~]) 
is variationally equivalent to 
f(x(N~))Gc(I[N~]). 
4- Suppose M G Λί(]τ',τ[) 
is fixed, and f(x) = / ( # M ) = / M ( # ) is a cylinder 
function. If f (x M)G C{I (M)) is integrable on R M then 
fM(x)Gc(I[N~]) 
is integrable on Rj r ,T^ and 
[ 
f(xM)Gc(I(M)) 
= 
= 
[ 
fM(x)gc(x(N-)\I[N~}\. 
5. GC(I[N-}) 
and gc(x(N-))\I[N-]\ 
are R T x 
Af(T)-VBG*. 
Because of these results, if f(Xr) 
is a random variable with T = ]rf,r], 
T/ = ]r /,r[ and 
E [ / ( X T ) ] = / 
f(xT)Gc(I[N}), 
then, writing x(r) = £, we can describe J R T- f(xT-)Gc(I[N~]) 
as an expect-
ation density, or marginal density of expectation, and denote it by 
ψ(ξ,r) = Εξτ [f(XT)} = [ 
f{xT)GT
c-(/[TV"]), 
(7.19) 
since Pubini's theorem (Theorem 54) implies 
Eif(XT)} = j (J 
f(xT)Gc(I[N-]f) άξ = JREiT [f(XT)\άξ. 
Similarly GC(I[N~]; ξ)—in which there is no integration on just one of the var-
iables—is a marginal density function for the distribution function 
GC(I[N]). 
The function gc(x(N)) has no integration on any of the variables x(t) (t G N), 
and can be regarded as a density function for the distribution function (or 
indefinite integral) Gc. 
As pointed out at the beginning of the discussion in Section 7.4, the prop-
erties established for Gc and its density gc apply equally to the distribution 
functions G£a (Brownian motion with non-zero drift) and Q (geometric Brow-
nian motion) and their respective densities and marginal densities. 
Note that, as it stands, item 5 of Theorem 204 requires the continuous 
modification of Gc. 
JTi)r 
fM(x)Gc(I[N-}) 

326 
CHAPTER 7. BROWNIAN MOTION 
7.9 
Regular Partitions 
Regularized partitions made it possible to establish the integrability of Gc in 
Theorem 168. Binary partitions were introduced in Section 3.5, with (3.8): 
K^k := } -q + (k - 1)2~9, -q + k2~q] 
and (3.9): 
Mr = {τ[Γ\τ£\ 
... ,τ£> = r } , 
rj r ) = r' + (r - 
r')j2^. 
Successive bisections give the binary partitions Krq of (3.11) of R T, with cells 
Kr^k 
denoted by 
Krq\k 
e 
yjq 
These partitions will be used to determine the integrability of Stieltjes-complete 
integrands of the form 
f(x(N))Gc(I[N}). 
Theorem 205 Suppose a < 0, b > 0, and c = a + ώ Φ 0. Then, for each 
Krq\k 
£ fcrq^ 
the 
functions 
GC(I[N]) and gc(xT,N)\I[N]\ 
are integrable on 
Kr^, 
with 
[ 
GC(I[N}) = [ 
gc(xT,N)\I[N)\ 
= 
Gc(Kr^). 
Proof. This follows from Theorems 168 and 17. 
O 
For K = Krq^ 
e Krq, write 
Theorem 206 Given positive integers r and q,ifa<0,b>0, 
and c = α+ώ Φ 
0, then 
Σ, fr9|k = 1· 
k€^ r g 
Proof. This follows from Theorem 168, since Krq is a partition of R T, giving 
Σ 
?c9|k = / 
GC(I[N\) = 1, 
as required. 
O 
Since the sum has a finite number of terms we can add up the terms in any 
order we choose without changing the value of the sum. This Riemann sum can 

7.10. STEP FUNCTIONS IN R T 
327 
be represented in various ways, such as 
gr9|k(K) . 
K e A C r g j 
= s?:i(E{^l(fcl,-,fci,-,fcar) : M ^ } ) 
c ( » j - v j - i ) 2 ) \ 
T(0_T(r) 
I 
J 
J - l / 
/ e x p ( 4 ^ 
J-OO 
J-OO H j = l (-(rr-ro) T 
^ 2 / 1 · · · ^2/2'· · 
/ 
The latter (finite-dimensional integral) version holds because the finite-dimen-
sional integrals in the original Riemann sum are additive in each of their 2 r 
dimensions. 
7.10 
Step Functions in R T 
Suppose / is a real- or complex-valued function defined for ΧΜΓ G R M r, and 
suppose f^r\x) 
is defined for x = χχ G R T = R' r 'T', so that 
/ ( r ) ((xMr,XT\Mr)) 
= / ( r ) ({XMr,VT\Mr)) 
= 
f(XMr) 
for all Xr\Mr
 a n d Vr\Mr £ R T\ M r. In other words, / ( r \ defined on R T, is a 
cylinder function.5 
Given x G R]T''T), for 1 < j < 2r define x^} as follows. 
-9 
if 
x\TjJ 
G I " 0 0 ' ~ ^ ] ' 
-q + k2-« if x (rj r )) G ] - ς + ^ , - ς + £ ] and k G tu'«, 
9 
if *(yj r )) € ]</, oo[. 
(7.20) 
The symbol "x" (to be distinguished from "x") denotes a "discretization" and 
mapping into finite dimensions of the infinite-dimensional, continuously varying 
x, and the various symbols attached to x denote the kind of discretization and 
projection involved. This construction is a version of random walk. Note that, 
for each r and each j , x^· -» x (TJ J as q —> oo. 
Given k G wTq corresponding to Krq\k G /Cr9, then to each χχ G Krq\k there 
corresponds ί x p , . . . , x^? ), and we write 
„rq|k _ ( (q) 
(q)\ 
v(«) — 
5In the notation of Theorem 204, / ( Γ ) ( * Τ ) = ΙΜΓ(ΧΤ) 
= 
f(xhfr)· 

328 
CHAPTER 7. BROWNIAN 
MOTION 
This gives a mapping 
R T 
H-> 
R Mr 
X 
—y 
x 
= 
_ 
vrg|k 
(7.21) 
The mapping x —> x is not quite a projection—each of the finite number of 
components of xr<3,lk may be, in value, close to the value of the corresponding 
component of #, but the components will not usually be exactly equal. 
Now define f{rq\x) 
by 
/(^)(x T) := / M (x'«" k,* n A, r) = / M ((x<*\ ... ,x£>) , * n A , r ) = / (xr*|k) 
for x G K r 9 G K,rq, with x^} defined by (7.20). Thus, as xT varies, 
f{rq){xT) 
takes only a finite number of possible values, not exceeding the number 
9Τ·ς2 9 + 1+2Γ 
of possible choices of kj G tuq in the permutations6 
k = k(xT) 
= 
(fej)i<i<2'· 
= 
(ki,...,kj,...,k2r). 
Denote the real or complex values of f(rq) by 
f(rq\x) 
= f(rq){xT)=ßrq\* 
for xeKrqlk 
and k G zz7r«. 
(7.22) 
Theorem 207 If a < 0, b > 0, and c = a + ώ Φ 0, ί/ien, /or positive integers 
r and q, the observable f{rq)(XT) 
^ f{rq\xT) 
[RT,GC] is a random variable, 
with 
E f{rq)(XTj\ 
= Σ j/r9|k£9|k : k € wrq} . 
Proof. Using Theorem 205, 
E /(r«>(Xr)l 
= 
/ 
f{r9)(xT)Gc(I[N}) 
= Σ {/ 
rg f(rg)(xT)Gc(I[N}) : Krq 6 Krq\ 
= Σ {ßr9]k f 
Gc(I[N\) ■ Rrq € Krq\ 
= 
\~* {eri\kcrg\k 
: KTq € K.rq\ 
= Σ {ßr9lVk ■■k e ™rq), 
as required. 
o 
6These are "permutations with repetition". See Section 9.7 for numerical illustration. 

7.10. STEP FUNCTIONS IN R T 
329 
Since the latter is a finite sum of real or complex constants, we can perform 
the summation in any order we like without altering the final value of the sum. 
Therefore 
E[/(r«)(Xr)] = E{r|kgc9|k:kG^} 
— 
^Γ2Γ 
ίχ^ 
arq\(k1,...,kj,...,k2r)r)
rq\(k1,...,kj,...,k2r)\ 
— Z^j=i yZ^kjezuv v 
vc 
y 
As before, the latter can be read as 
/
oo 
/»oo 
2 
- OO 
J — OO 
· _ 1 
/ 
/ c(y. 
' 
exp ' - ^ 
Vj-Vj-i) 
\ 
.(r)__(r) 
I \ 
(i(-
.(') 
■ 
*
>
,
)
)
' 
rM 
dyi · · · G?2/2-
/ 
where fMr(xMr) 
·= f^rq\xT) 
is a step function with constant value in each 
of the finite-dimensional cells of the binary partition, and where / M r ( % r ) , = 
f(rq\xT), 
is calculated from the finite-dimensional, discretized version x of χτ> 
Thus the expectation, an infinite-dimensional integral, can be expressed as a 
finite-dimensional integral in this case—which is no surprise, since the integrand 
is cylindrical. 
Theorem 208 Suppose r is given, and suppose a real- or complex-valued func-
tion f is defined for XM £ R-Mr · If a < 0, b > 0, and c = a + ώ φ 0, and 
if 
f(xMr)gc(xMr)\i(Mr)\ 
is integrable on HMr, then the observable 
f^(XT)~f(r\xT)[RT,Gc] 
is a random variable, with 
E /<r>(Xr)l = / 
f(xMr)gc(xMr)\i(Mr)\. 
Proof. This follows from Theorem 159. 
o 
As before, the regularity of the partition K,rq means that the expectation in 
this case can be expressed in the form of the finite-dimensional integral 
,οο 
.oo 
r 
( exp (c%y^f 
/ 
- / 
/ ( « ι , . . . , « τ ) Π Ι 
V - ' - " ' - 1 -
J— OO 
J— OO 
· 
1 
\ 
V 
/ 
.(iW0-*))1 
If f(xMr) is continuous then, for each XMT = (xi, ■ ■ ■ ,X2r) € RMr, 
f{rq){xT)^f{r\xT) 
= f{xMT) as H o c . 
dyi---dy2r. 
(7.23) 

330 
CHAPTER 7. BROWNIAN MOTION 
Does this imply that 
E /(r'>(#r)]->E [/<*·> (Xr)] 
as q —> oo? Since the expectation E [/(τς\Χτ)] 
exists for every q and every r, 
it is useful to know of any circumstances in which it may be valid to take this 
limit. And this leads to further questions. 
■ Is it then possible to let r —> oo? 
■ What does it mean to increase the number of variables in a function? For 
example, a function of two variables is very different from a function of one 
variable. Viewed in this way, can "lim^oo / ( X M J " have any meaning? 
■ Integrating these functions creates further problems of interpretation. An 
integral in two dimensions is very different from an integral in three dimen-
sions. So what happens if we let r tend to infinity in the 2r-dimensional 
integral (7.23) above? 
The idea behind the finite-dimensional integral (7.23) is E [f^(Xr)] 
whose 
basic meaning is an integral, not in 2r dimensions, but in infinitely many 
dimensions. Perhaps the observable f^r\XMr) 
c a n al s o De given an infinite-
dimensional interpretation which will allow us to let r —> oo. 
But first we return to the question of what it means to let q —y oo in 
E [/(»■«) (XT)]. 
Theorem 209 With r given, suppose the numbers 
[\ß^*\ 
: k , 6 ^ , 9 = 1,2,3,...} 
are bounded above by a real number κτ. Then, if a < 0, the observables 
f^\XT) 
~ f^(xT) 
[RT, Ga] , f{r)(Xr) 
* f{r\xT) 
[RT, Ga] 
satisfy 
E / ^ ) ( X T ) ] ^ E [ / W ( X T ) ] 
as q 
oo. 
Proof. Theorem 207 implies that, for each q, /(r<3)(xT)^c(x(AT))|/[7V]| is int-
egrable on R T. We have 
f^HxT)\9a(x(N))\I[N}\ 
< 
Kr9a(x(N))\I[N}\ 
for each (x,N,I[N]) 
and each q. The function on the right-hand side is int-
egrable, so Theorem 61 (dominated convergence theorem) gives the result, since, 
for each xT e R T, f(rq)(xT) 
-> / ( Γ ) ( ^ Τ ) as ς -> OO. 
O 

7.10. STEP FUNCTIONS IN R T 
331 
Note that the dominated convergence theorem makes it possible to deduce 
the Ga-integrability of f^(xr) 
from the Ga-integrability of / ^ ( X T ) · 
Also, 
since /(r) is cylindrical, Theorems 159 and 209 imply 
E[/<r>(Xr)] 
= 
j J(r\xT)Ga{I[N}) 
= J ^ 
/«(xT)ga(xT(N))\I[N}\ 
/(2/MO Π 
3 
J - l 
/ 
= 
lim V |^ r 9 | kg: 9 | k : k e wrq) . 
q—)·οο ^—' I 
J 
dy 
J 
The next theorem states alternative conditions for 
E [/ ( r 9 )(*r)] -> E [/(Γ)(*τ)] 
(b = 0, a < 0). 
Again, ί^\χτ) 
·= f{xMr), %τ £ R T, ΧΜΓ € R M r, and again the hypotheses 
are of a finite-dimensional character. 
Theorem 210 Suppose a real- or complex-valued function f{y), 
defined for 
y = %Mr € R M r, is uniformly continuous in R M r. If a < 0, and if 
\f(xMr)\9a(xMr)\I(Mr)\ 
is integrable on R M r, then the observable f^r\Xr) 
— f^r\%T) 
[R T
?G a] is an 
absolute random variable, and 
E [/<'«> (X r)]->E[/< r>(X r) 
as q —» oo. 
Proof. Let η > 0 be given. Then there exists qo so that q > qo implies 
f™(xT) 
< 
| / W ( X T ) | + /?, 
|/ ( r 9 )(*r)| 
< 
\f{r)(xr)\ 
+ V, 
f^(xT)9a(x(N))\I[N]\ 
< 
(\f^(xT)\+V)ga(x(N))\I[N)\, 
\f^\xT)\9a(x(N))\I[N}\ 
< 
(\/^(χτ)\ 
+ 
η)9α(χ(Ν))\Ι[Ν)\ 
for all (x, N, I[N]). The latter function is integrable on R T, so the result follows 
from Theorem 61. 
O 
Next we seek corresponding results when 6 ^ 0 . 
Theorem 211 Suppose a < 0, b > 0, and c = a + ώ ψ 0. // the real- or 
complex-valued function f(xMr) is continuous for xMr £ R M r, and if f^ 
is the 
cylinder function given by / ^ ( x r ) ·= f{^Mr), 
then the observable 
f^(XT)~f(r\xT)[RT,Gc] 

332 
CHAPTER 7. BROWNIAN 
MOTION 
is a random variable, with 
E [ / M ( X T ) 1 = / 
f^(xT)G?(I[N})= 
[ 
f(xMr)9c(xMr)\I(Mr)\. 
Proof. This follows from Theorems 155 and 63. 
O 
In this theorem the contingent observable function is cylindrical, so the ex-
pectation (fundamentally infinite-dimensional) has a finite-dimensional repres-
entation. It has been established that, with the given hypotheses, the observ-
able is a random variable relative to Gc. Therefore its expectation—an infinite-
dimensional integral—can be estimated or approximated by an appropriate Rie-
mann sum. 
In this case, such an approximation consists conveniently of a finite sum 
of "finite-dimensional" terms. But sometimes it may be no simple matter to 
determine a Riemann sum approximation to the expected value of an observable. 
For instance, it may require a delicate choice of gauge 7 if the integrand is highly 
oscillatory. 
Unlike Theorem 209 (where b = 0, a < 0), it is not yet established that the 
relatively uncomplicated binary7 Riemann sums 
can be used 
to estimate the expectation when b φ 0. But we will see that the latter approx-
imation can be valid even when b φ 0, provided the integrand or observable is 
not too highly oscillating. 
7.11 
c-Brownian Random Variables 
A c-Brownian observable is a Gc-observable; that is, 
f(XT) 
^ f(xT) 
[RT, Gc] 
or 
f(XT, 
N) ~ f(xT, N) [RT, Gc] . 
The former is the predominant form in this book. A fuller study of the latter 
is intimated in Sections 4.9 and A.l. Because the Gc-integration in R T of both 
forms is well defined in the Riemann sum terms of this book, the meaning of 
both, as random variables, is well defined. 
Rather than continue to investigate functions / of a general kind, the rest 
of this book will be concerned mainly with two particularly important kinds of 
function /. The most widely used Gc-observables are stochastic integrals, and 
a Riemann sum method of analyzing these is presented in Chapter 8. But a 
different and perhaps more important class of Gc-observables will be the subject 
of the rest of the present chapter. These will be called observable integrals, as 
distinct from stochastic integrals. 
Both of these types of Gc-observable have wide application, and both involve 
joint-contingent observables of the form f{Xr, N) ~ ί{χτ·> N) [RT, Gc], where 
f{xT, N) involves, in turn, calculation of Riemann sums of some integrand g in 
the domain T 
=]τ',τ]. 
7That is, Riemann sums over binary partitions of R T . 

7.11. C-BROWNIAN 
RANDOM 
VARIABLES 
333 
To see what is involved, suppose a function g depends on differences 
sf — 5, 
and/or 
x(s') — x(s), 
where s,sf £ T, s < s', and x = χτ £ R-T- Write zs = ]s,s'] £ I(T) and 
x(zs) = x{s') — x(s), so g(z,x(z)) is defined. Suppose N £ N(T) 
is a finite 
subset {ti,..., i n} of T, with tn = τ and to = τ'. Then TV can be regarded as a 
partition VN of T. For simplicity write VN = N. 
Write ij =]tj-i,tj] 
and x^·) = x(tj) — x(tj-\). 
Then the following ex-
pressions both have the form f(xr,N), 
and both involve Riemann sums over 
partitions PN, (= N), of 
Τ=]τ',τ]ι 
( 7 V ) ^ ^ , x ^ · ) ) , 
e((AT)^^(z„x(z,))) , 
(7.24) 
where e is some function of the real or complex numbers that emerge from the 
calculation of the Riemann sum (N)Y^g 
in domain T. In fact, for the main 
applications considered in the rest of this chapter, the function e will be some 
version of the exponential or trigonometric functions. 
Each of these expressions in (7.24) is a datum of a joint-contingent observable 
of the form f(XT, 
N) ~ f(xT, N) [RT, Gc]. 
Note that the function g is a Stieltjes-type integrand in a Riemann sum on 
domain T. As expressed above, the integrand g depends on a particular choice 
of χτ £ RT- 
In other words, the Riemann sum depends on a joint datum 
of a joint-basic observable Χχ — #t[RT,Gc]. Generally, g will also depend 
on elements x(s) as well as on increments x(ze); but, to avoid overloading the 
notation, such additional variable parameters are omitted for the moment. 
Formation of a Riemann sum of an x-dependent integrand g leads on to the 
question whether, for8 all χτ, such Riemann sums converge to an integral (an 
observable integral) on domain T. If they do, then the expressions in (7.24) lead 
to the expressions 
Jg(iM*)), 
e(jg(t,x(t))y, 
(7.25) 
which, in turn, are data values for joint-contingent observables of the form 
f(Xt)~f(XT){XT,Gc]. 
The expressions g in (7.24) have the form h(i) (usually with further depen-
dence on x), so their Henstock (Stielt jes- or Bur kill-complete) integrals on real 
intervals in T are well defined. The partitions N can be subject to the choice 
of L(XT) 
in gauges 7 = (L, 5); in other words, iV D L(XT)· 
Existence of the 
Henstock integral on T of g would then require that g should actually be int-
egrable in some basic Riemann sense. In other words g has to be a particularly 
"tractable" function. In fact, this book only deals with such "tractable" func-
tions g. But if a deeper approach is required, then gauges 7 can be replaced by 
the gauges ζ of (4.2) in Chapter 4. In that case, instead of Riemann integrability 
of g, Henstock integrability of g on T can be stipulated. 
'Or for any χγ. 

334 
CHAPTER 7. BROWNIAN 
MOTION 
It is generally helpful when the Riemann sums (7.24) actually converge to int-
egrals on T. But, regardless of whether they converge or not, the Riemann sum 
theory of random variability, as developed in this book, permits investigation of 
observables f(XT,N) 
in the forms (7.24). 
However, rather than engaging with further development of the concept of 
R T xA/*(T)-measurability, or with the form of measurability described in Section 
A.l, this book deals mainly with integrable forms of g. 
The rest of this chapter is concerned with Gc-observables 
e ((^) Σ
g(
x^
 Z sv' 
e ( /
 
9(
Xs>
i^)' 
with g depending on is =]s,s'] and xs, but not on the Brownian increments 
x(zs). The latter case belongs to the theory of stochastic integrals, the subject 
of Chapter 8. 
7.12 
Introduction to 
U-Observables 
This section examines in more detail the observable introduced in (7.11): 
f exp(LU(x(s),s)ds) 
if x = 
xTeRT\DT, 
U{XT):=\ 
ft
 
P U T 
} 
., 
n T 
(7.26) 
[ 0 
if x £ D
T, 
corresponding to e (fTg(xs,is)) 
in the preceding section, with 
g(xs,is) 
= 
U(x(s),s)\i8\. 
This notation allows the function U to depend on time s as well as position 
y = x(s). 
Except when otherwise stated, U can be real- or complex-valued. 
The set DT consists of those x G R T which fail to be continuous at every t G T. 
The continuous modification introduced in Sections 7.4 and 7.6, and in Theo-
rem 199, describes a modification of the definition of the integral on R T which 
makes it possible to consider (7.26) as a valid integrand with respect to G^ 
when T is a real interval. And if T is a dense countable subset of a real interval 
(in which case we denote it by T;), then Theorems 187 and 188 enable us 
to calculate the Gc-integral of the function (7.26) without appealing to the 
continuous modification of the integral. 
Suppose a real- or complex-valued function / is defined on the set of real 
numbers or the set of complex numbers. As before, let 
Mr = {rir) 
r « } , x * ^ ! } « 1 
^ ) 6 R * 
where, as before, x is a discretized, finite-dimensional, binary-form, random-
walk-type image of x. With variables including χτ £ R T and N = {ti,..., tn} G 

7.12. INTRODUCTION TO U-OBSERVABLES 
335 
Λ/"(Τ), define real- or complex-valued functions fu on R T, or on R T x Af(T), 
as follows. 
ΜΧΤ,Ν) 
:= 
/ ( E ^ i ^ i ^ - O . i j - i ) ^ - ^ · - ! ) ) . 
ίυ{%τ) 
'= 
f (f^, U(x(t),t)dt) 
whenever J^f Udt exists. 
(7.27) 
Thus ίυ{χτ) 
— U{XT) when / is the exponential function and χτ G R T \ DT. 
Both f\jq' 
and / ^ are cylindrical, and f^ 
is a step function. (In x^· , the 
step function x is evaluated at Tj rather than Tj-\ because of the discontinuity 
of x at each Tj-\.) For each k G vorq and each χτ G Krq\k G /Crg, write 
ω ^ = //Γ9)(^τ). 
Because of their dedicated form, designate the joint-contingent Gc-observables, 
Μχτ,Ν), 
$9)(χτ), 
f(u\xT), 
MXT), 
as ZY-observables. 
Theorem 212 If a < 0, b > 0 and c = a + ώ Φ 0 then 
zs a random variable and 
E [/^(Xr)] 
= 
Σ {/ ( ^ | k ) Gc(ifr«lk) : k = k(Kr«lk) G C7r«} 
= E{^|kfc*|k:kG^} 
= 
/RM, /(2/I> · · ·»y2r)gc(yi,. ·. »^OMWOl· 
Proof. This is similar to the proof of Theorem 207. 
O 
While the effect of letting q —>· oo can be examined, it is also of interest to 
let r -» oo. But first note some connections between the different versions of 
fu. Take Τ=]τ',τ] and 
oo 
T' = U Mr, 
r=l 
so T' is a countable, dense subset of T. 
Theorem 213 Suppose a > 0, b > 0, and c = a + ώ ^ 0. If the real- or 
complex-valued function f is a continuous function of its real or complex argu-
ment, and ifU(y,t) 
is continuous for y G R and t G [τ',τ], then: 

336 
CHAPTER 
7. BROWNIAN 
MOTION 
1. for each Χτ € R'T 'T\ and for each χτ' € R T , 
fp\xT) 
-+ fP(xT), 
fp\xT.) 
-+ 
fP(xT,), 
as q —► oo; 
2. for each χτ> £ R T \ DT 
(i.e., for each x which is continuous relative to 
t€T' 
= U ~ ! Mr), 
f{j\xT.) 
-> 
fu{xT,). 
as r —> oo. 
Proof. The first result is a consequence of continuity. The second result follows 
from Theorem 188. 
O 
Theorem 214 Suppose a < 0, b > 0, c = a + ώ φ 0, and T" zs a countable, 
dense subset of T =]r',r] 
(such as Tr = [£!LiMr). 
If f is continuous and if, 
for each χτ> € R T \ DT , the function U(xTf(t),t) 
is Riemann integrable on 
[τ',τ], then the functions 
MxT,,N)C£{I[N\), 
fu{xT')GT
c'{I[N\), 
Μχτ,,Ν)9Τ(χτ'(Ν))\Ι[Ν}\, 
MxT')gT(xT'(N))\I[N]\ 
are variationally equivalent in R T . 
Proof. Riemann integrability of U(xT'(t),t) 
means fTU(x(t),t)dt 
exists. By 
Theorem 188 the additional factor 1 RT'\ DT> (ΧΤ') c a n be included in each of the 
listed functions. By Theorem 179, G% (I[N]) is R T' x Af(T')-VBG*; 
so there 
are disjoint sets 
θ^ = Sj x Mj C R T' x Af(Tf) 
whose union is R T χΛ/'(Τ/), positive real numbers α^, and a gauge 70 = (Lo, 5o) 
such that, for each 70-fine division P 7 o of R T and j — 1,2,3,..., 
(νΊ0)ΣΐΘ,(χτ>)\ΟΪ(Ι[Ν))\ 
< e. 
Using the continuity of / and the Riemann integrability of U in [τ', r], for each 
XT' 6 R T , and for any given 771 > 0 and 772 > 0, we can choose L\(XT>) 
£ M(T') 
so that LI(XT') 
D LO(XT') 
and so that, for each N = {ti,...,tn} 
£ Af(T') 
satisfying N D 
L\(XT'), 
|Σ"=1 ^(*(<i-l).*J-l) (*i - tj-l) - fr> U(x(t),t)dt 
\f (Σ"=1 U (Xltj-!),^-!) 
(tj - tj-!)) 
- / ( / ; 
U(x(t),t)dt) 

7.12. INTRODUCTION 
TO 
U-OBSERVABLES 
337 
For χτ' £ Sj and N G Mj let 772 = ea- λ2 j, j = 1,2,3,.... Then, for gauge 
7i = (£i,^o) an<i a n v 7i-fine division D 7 l of R T , group together the terms of 
the Riemann sum in which χψ> G Sj, j = 1,2,3,... and iV 6 ΛΊ^, so that 
(*>■») Σ \fu(xT>,N)Gc(I[N}) 
- 
MXT>)GC{I[N])\ 
= 
(*>τι)Σ ( Σ £ ι ΙΘ,ΟΓ,ΛΟ |Mzr',iV)Gc(/[JV]) - /i/(«rOGc(/[iV])|) 
= 
Σ ~ ι (P7i) Σ 1θ,(*, W) l/c/^τ', iV) - Λ,(*Γ,)| 
\GC{I[N])\) 
< ΣΤ=ι eaj^-latj 
= ε. 
This gives variational equivalence. The other variational equivalences in the 
statement of the theorem follow from Theorem 158. 
O 
In the following results both q and r tend to infinity. 
Theorem 215 // / and U are continuous then, for G^ -almost all χχ' in R T , 
fiq\xT>) ^ 
MXT>) 
as q and r tend to infinity jointly. 
Proof. This follows from Theorem 188. 
O 
The next result has 6 = 0. 
Theorem 216 Suppose a < 0, / and U are continuous, and f is bounded. 
Then 
MXT')*MXT') 
[*?,(£] 
is an absolute random variable and 
lim E[fr
u«(XT,)} = Elfu(XT,)}, 
lim Ε[\%·(Χτ,\)] 
= 
Ε[\ΜΧτ>\)]. 
ςτ,Γ—>oc 
q,r—too 
Proof. Theorem 61 (dominated convergence) gives this, since there exists a 
constant κ for which 
f{Jq\xT>)GT
a\l[N}) 
< \K\GT
a\l[N}), 
\Üq\xT>)\GT
a\l[N]) 
< N ^ V M ) 
for all q, r and {xT,, N, I[N}). 
O 
Taking T' = IJ^Li Mr, T =]τ',τ], and using Theorem 169 and the continuous 
modification of the integral (with expectation denoted E*), this result can be 
written as 
E'\fu(XT)\ 
= 
E[fu(XT,)} 
2r 
lim / 
fP{yMr)Y[ 
r-Kx> JRMr 
" 
/ expf^Se#^ ^ 
T.· 
—fA 
(Μ-Τ-ί?.)) 
dy. 
]Mr 
I 

338 
CHAPTER 7. BROWNIAN 
MOTION 
This provides a number of convenient ways of estimating or approximating the 
expected value of the random variable /t/(Xr) without having to calculate Rie-
mann sums from a gauge 7. Also, the seemingly strange notion of letting r tend 
to infinity, where 2r is the number of dimensions in the domain of calculation 
of the integral, is given a reasonable meaning in a context where the overall 
calculations are located in an infinite-dimensional domain to begin with. 
The next result is applicable when b > 0. 
Theorem 217 Suppose a < 0, b > 0, and c = a + ώ φ 0. If f and U 
are continuous then the observable fui^T') 
— fu(%T') 
variable, with 
R T ' , G f 
is a random 
E*[fu(XT)] 
= 
E(fu(XT>)) 
= 
fRTfu(xT')G?(I\N}) 
= 
fRT,fu(xT>)9T(xN)\I(N)\. 
Proof. For χτ> £ R T \ DT we have 
ur^ 
= 
f™{xT,)-+MxT,) 
as q,r -» 00. Theorem 179 says that the integrator G^'(I[N]) is R T / x 
N(T')-
VBG*. Therefore, using the continuous modification of the integral, the result 
follows from Theorems 155 and 63. 
O 
This establishes Gc-integrability of fu for a broad class of functions / and 
U—continuous functions, which, in other contexts, might be expected to be 
unproblematically integrable. Most importantly, it gives integrability for the 
cases b > 0 even when a = 0. The proof, though simple enough, uses methods of 
non-absolute integration theory which are not available in traditional integration 
theory. 
The class of / and U for which the result is valid is a "tractable" class 
of continuous functions. The objective is to keep the treatment simple. On 
that account, more general and more far-reaching results of this type have been 
avoided here. 
7.13 
Construction of Step Functions in R T 
A real- or complex-valued function in R T takes the form g(xr)· 
This can be 
viewed as a function g(-) of a function #(·), where x(-) can often be restricted 
to the class of continuous x G C^e C R T, for some fixed a, 0, 0 < a < \, Θ > 0. 
Suppose g(xr) has the iV-dependent form g(x(N)). 
The construction 
ΛίΡ = { τ ί Ρ > , . . . , τ « } , 
x ^ k = ( x ^ , . . . l X ^ ) G R ^ , 

7.14. ESTIMATION 
OF E[Fu(X T)] 
339 
provides a step function version of each of the functions x(t) for χτ £ R T. A 
function grq(xr) 
can be defined as a cylinder function 
9rq(xr) 
= 9rq(x(Mr)) 
= g 
(xr^,xT\Mr) 
where xr?lk = xr^(xT) 
for 
χτ G K £ Krq is a "discretized" or step function 
version of x(-). Then grq is constant in each of the finite number of cells K of 
the binary partition KLrq of R T; so grq is a step function in R T. 
Sometimes g(xr) takes the form h{f{xr)) 
where / is real- or complex-valued. 
In that case, instead of replacing x(-) by x, a step function approximation to 
g(xr) can be constructed by converting / into step function form. 
In this chapter the functions of most interest are various formulations of 
U-%x(N)) 
= 
e x p ( - c ( E ; = 1 t / ( x f e - i ) , ^ - i ) f e - ^ - i ) ) ) , 
U-c(x(T) 
= 
exp(-cfTU(x(t))dt). 
Rather than forming discretized step functions x from the functions χτ, suitable 
step functions in R T can be formed from step function versions (in R x T) of 
the function U{y,t). For 
-Q+^T'-Q 
+ Y« 
y € K^k = 
define Urq(y, τ') = U (y, τ'), 
k-1 
k 
el(R), 
te 
TJ:\,TJr) C T = ]r',r], 
Urq(y,t) = u(-q+^,T^ 
(7.28) 
for 0 < k < 2q, 1 < j < 2r, with Urq(y,t) = 0 otherwise. Then, for xT € K € 
Kr", define 
^{XT) 
= - p i - c f e u ( - q + A r j : \ ) ( r j V f )X\ 
. 
(7.29) 
By forming a step function Urq from U in R x T it is possible to construct a 
Jrq in R T. Bu 
struct step fu 
functions x(t). 
step function \Jrq in R T. But even when this method is not available, it is still 
possible to construct step functions in R T by "discretizing" the intermediate 
7.14 
Estimation of E [ίυ(Χτ)] 
Theorem 217 confirms G^-integrability of fu when b > 0 and a < 0, so the 
expected value Έ[/υ(Χτ)] 
can be approximated by appropriate Riemann sums. 
But, depending on the degree of accuracy required, the Riemann sums must 
be appropriately 7-fine. And the result does not, in itself, provide any direct 

340 
CHAPTER 7. BROWNIAN 
MOTION 
guidance on how to find an appropriate gauge 7, nor how to otherwise construct 
any of the Riemann sums involved. 
Theorem 217 does not say whether the Gc integrals on R T of functions 
fjjQ* converge as r and q tend to infinity; nor—if they do happen to converge— 
whether they converge to the Gc-integral on R T of the function fu. 
Therefore Theorem 217 does not make it possible to use the more convenient 
methods of approximation by Riemann sums Σ uurq^qr
c 
over binary partitions 
of R T, nor even the finite-dimensional integrals which result from letting q —> 00 
in binary Riemann sum approximation. 
Continuity of / and U(y,t) is insufficient, by itself, to permit use of these 
convenient methods of approximation whenever a < 0, b > 0. 
When U(y,t) is a step function, in the sense that U is constant for 
y e Kq\k = 
k-1 
k 
G I(R), t e T(r) r W cr = 
the corresponding binary sum gives the exact value of the G<T-expectation for 
all values of a < 0, b > 0, c = a + ώ Φ 0. And when 6 = 0, continuity of U 
provides access to binary Riemann sum approximations. 
But when b > 0 it is more difficult to find convergent Riemann sums, even 
if U is uniformly continuous. However, if U is sufficiently close to step function 
approximations, then it may be possible to prove that 
■ the binary Riemann sums converge, and 
■ the limit of the binary Riemann sums is J R T fu{%T)dGc, or E [/υ(Χτ)]· 
The results in Section 7.12 apply to functions with domains R T and R T , T' 
being a countable, dense subset of T. There are corresponding versions of these 
results for domains R T = R^T 'rt and the analogous R T 
with T'~ a countable 
dense subset of T1 —]rf^r[. Then, as described in Section 7.8, marginal density 
of expectation replaces expectation. For a given observable /(Χρ)? the marginal 
density of expectation and the expectation are, respectively, 
Efr[/(Xr)] = / 
ί(χτ)0Γ(Ι[Ν-}), 
E [ / ( X T ) ] 
= 
/ 
f{xT)GT
c{I[N]), 
where x(r) = ξ. There are corresponding versions which use T'~ and T'. The 
rationale for the terminology "marginal expectation density" is the relationship 
E[f(XT)} 
= 
/ 
f(xT)C%(I[N\) 
fx(T)ei(T)' 
/ 
r 
\ 
= / 
/ 
ί(χτ)0Γ(Ι[Ν-]))\Ι(τ)\ 
r*(r)e/(T)* 
//(r)Gl(R) 
/ 
Εξτ[/(Χτ)]άξ, 

7.15. U-OBSERVABLES 
IN C-BROWNIAN 
MOTION 
341 
which, by Fubini's theorem, holds whenever the observable 
f(XT) ~ f(xT) 'B)T'>T\G%' 
is a random variable. Integration on the final variable x(r) = ξ converts the 
marginal density of expectation into the expectation itself. 
The density version of the potentiality distribution function G^ (I[N~]) is 
gT(xT(N~)), 
with variational equivalence between 
ί,Γ(*Γ(ΛΓ-))|ΐμν-]|, 
οΓ(ΐ[Ν-}). 
We will use "density", "marginal density", "countable domain", and "contin-
uously modified" versions of the preceding results as and when required. 
7.15 
ZY-Observables in c-Brownian Motion 
Observables that are broadly of the form /υ(Χτ), 
corresponding to 
fu(xT) 
= exp (f 
U{x(t),t)dt\ 
, 
play an important role in applications. In relation to the discussion in Section 
7.12, this amounts to letting the function / be the exponential function. This 
section introduces some particular versions of fu· The relevant results from 
Section 7.12 are noted here, for ease of reference. 
As before, T = ]r',r], c = a + ώ, N = {ti,..., 
tn} G λί(Τ) where 
τ' = to < t\ < · · · < tn = r, 
r and q are positive integers; and U(y,t) is a real- or complex-valued function 
on R x [τ',τ]. Writing x(tj) = Xj and Uj = U(xj,tj), 
define 
U{xT{N)) 
:= exp ^ U ( χ , - ι , ^ - ι ) (tj - ^_i) 
n 
i=i 
U-c(xT(N)) := 
expl-cj^Uj-iitj-tj^) 
= (U(xT(N)))-\ 
U~c(xT) := exp(-c ί U(x(t),t)dt\ 
(7.30) 
whenever this integral exists, 

342 
CHAPTER 7. BROWNIAN 
MOTION 
U-q\xT) := e x p f - c £ t f ( i ^ 
U-%XT) := e x p ( - c j > ( ^ 
The first two functions are defined in R T x Λ/*(Τ); while the other three are 
defined in R T. Both U~£{XT) 
and U~C{XT) 
are cylindrical; and ΙΑ~£{χτ) is 
a step function since "x" is a discretized version of "#". (In other words, the 
values of variable x £ R M r are the values of a step function defined for x £ R T.) 
Note that, by construction, for each χτ £ R T, 
U-c(xT) 
= U- c(x T), 
(7.31) 
the latter step function being defined by (7.28) and (7.29) without using the 
intermediate, discrete function x. 
To summarize the notation, each permutation k £ wrq corresponds uniquely 
to: 
■ a cell K £ I(R T) where each K — Krq\k is an element of a binary partition 
/Cr«ofRT, and to 
■ a unique point x of R M r given by xr<3,'k. 
Given a permutation k, all of the points χτ £ K are mapped to 
x = ( χ ^ , χ ^ , . . . , χ ^ ) = xr<?|k £ RM^. 
The form of the integrator function or distribution function that is needed in 
the main applications of the theory is G^(I[N]). 
For each permutation k £ vorq 
and each xT £ Krq^ 
£ Krq, write 
u - ' k = 
Ur7(xT), 
^ I k 
= 
G T ( ^ | k ) = / ^ | k G T ( / [ i V ] ) . 
Thus I Uc | isa finite array of observable constants or data values given by 
the cylindrical step function U~Q
C(XT), the elements of which are in one to one 
correspondence with the elements of the arrays {uH k} and {ßrq^} 
used earlier. 
Therefore the array of potential occurrences or potential data values 
of the joint observable U~q
c(Xr) corresponds exactly to the array of potentiality 
constants < Qc \ which are the values given by the distribution function G^ 
on the members Krq^ 
of the binary partition fCrq of the sample space R T. 
The following is a summary of some properties of U which are immediate 
consequences of the discussion of fu in Section 7.12. 

7.15. U-OBSERVABLES 
IN C-BROWNIAN 
MOTION 
343 
FBI Suppose a < 0, b > 0, and c = a + ώ φ 0, and suppose U (x^_l5 r^-i j is a 
step function defined on discrete-valued x € R M r, with x determined by 
x, r, and q. Then the observable 
U-c(XT)^U-q
c(xT)[RT,Gj] 
is a random variable, with 
E (W"c (XT)) = JRT Kq
c (xT) G?(I[N}) = Σ {<"kS?|k : k e ^ } . 
Let a(c,q,r), 
= a(c,q,r,x(Mr)), 
= a(c, <?,r, χχ,... ,^2^), denote 
πΓ-ι^^-'ο))* 
E (U~q
c (XT)) = I 
a(c, q, r, 
x(Mr))dx(Mr), 
Then 
or 
/
OO 
/-OO 
··· / 
α(ο,ς,Γ,χι,...,χ 2'·)ώι···^2'·· 
-OO 
«/—OO 
FB2 For this property we either replace T by T' = IJ^li -^r o r u s e the con~ 
tinuous modification of the integral. Taking 6 = 0 and a < 0, suppose a 
real-valued £/ is continuous and non-negative. Then the observable 
U-a(XT,)~U-a(xT,)[RT',GT] 
—or, in the modified sense, U~a (Χτ) — K~a (#τ) [RT> G%]—is a random 
variable, with 
E \U~a (Χτ')] = E* \U~a (XT)] = lim E \U-Q
a(XT)] . 
L 
J 
L 
J 
q,r->oo 
L 
* 
J 
Thus the expectation JRT/ £Υ~α (χτ') G% (I[N]) is representable as 
lim 
/ 
α(α, g, r, x(Mr))c&r(Mr), 
g,r-^oo 7 R M r 
or as 
lim / 
/3(a,r,x(Mr))dx(Mr), 

344 
CHAPTER 7. BROWNIAN 
MOTION 
where ß(a,r,x(Mr)) 
is9 
e*p(«E?:, ((#e&)*-f (*>-.·&)) W-ft)) 
iC-ii^-^.))' 
FB3 If α < 0, fr > 0, and c = a+*i> ^ 0, and if U(y, t) is continuous in R x [τ', r], 
then the observable 
U-c{XT>)^U-c(xT>) 
T' 
nT' 
& ,G; 
—or, in the modified sense, U c (Χτ) —Uc (χχ) [RT, Gj]—is a random 
variable, with 
Ep-°{XT·)] 
= 
E*[U-C(XT)} 
= 
fRT,U-<(xT,)GT(I{N}) 
= 
fRT,U-HxT')gT(xT'(N)\I[N}\. 
Take T~ =]τ/,τ[ 
and ξ = χ(τ) fixed, so integration is not performed on x(r) 
in the dimension r. Then results analogous to FBI, FB2, and FB3 hold for the 
marginal density G^ (I[N~]) and for the marginal density of expectation 
Εξτ {U-C{XT)) 
= / 
U-\xT-, 
Ν)ΰΓ 
(/[TV]), 
(and also for each of the variationally equivalent formulations—with continuous 
modification if required) whenever the function U~c is G^ -integrable. Note 
that, by Fubini's theorem, if the function U~c is Gc-integrable on R T then U~c 
is Gc-integrable on R T . 
On a point of notation and terminology, in applications the marginal density 
of expectation expression E^r [U~C(XT)] 
is sometimes called the wave function 
or state function ψ or φ, with notation of the form 
Μϊ,τ) 
or 
ψο&τιξ',τ'). 
The latter notation can be used if, in addition to x(r) = £, it is also necessary 
to keep track of x(r') = £'. 
Results analogous to the preceding ones include Theorems 218 and 219, in 
which 
Λ2 
exp i c χ_ 
exP [°Σ3 
ctii-r-«))' 
' 
(7.33) 
2/o = £', V2- = £, and M~ = {yu ...,y 2r- 1}. 
9In this case a step function is not introduced. 

7.16. DIFFUSION 
EQUATION 
345 
Theorem 218 Suppose a < 0 and the real-valued function U(y, t) is continuous 
and non-negative. Then, with T~ = ]τ',τ[, the following hold: 
E € T[ZT e(Xr-)] 
= 
/ 
U-a{xT-)GT
a-(I[N}) 
= 
/ 
ν-α(χτ-(Ν))θΓ(Ι[Ν}) 
JB)T',T[ 
= 
[ 
U-a{xT-)gT\xT-{N))\I[N\\ 
- 
/ 
U-a(xT-(N))9r(xT-(N))\I[N}\ 
= 
lim / 
_Fa{yM-)dy{M-) 
= 
Φα(ξ,τ). 
Theorem 219 If a < 0, b > 0, c = a + ώ φ 0, and if U(y,t) is continuous, 
then, withT~ = ]τ',τ[, the following hold: 
EsT[U-°(XT-)} 
= 
[ 
U-%xT-)GT
c-[I[N]) 
U-%xT-{N))GT
c-(I[N}) 
L 
= 
/ 
U-c(xT-)9r(xT-(N))\I[N]\ 
= 
/ 
U-%xT-(N))9r(xT-(N))\I[N]\ 
= 
ΦΛξ,τ). 
Using Theorem 54 (Fubini's theorem), Theorems 218 and 219 are conseq-
uences of Theorems 216 and 217. Or they can be proved directly, by the same 
argument as before. 
Note that Theorem 219 does not assert that, if U is 
continuous and r —► oo, JL M- •7r
c(2/M~)c^(^r~) converges to ψ0(ξ,τ) whenever 
b > 0. 
7· 16 
Diffusion Equation 
This section gives a verification10 of a partial differential equation satisfied by 
the marginal density Εξτ [U~C(XT)) 
of the expectation of the random variable 
U-C(XT)~U-C(XT)[RT,GJ}. 
There are various methods by which the diffusion equation can be derived. Instead of 
exploring these methods we take the equation itself as given, and verify that the marginal 
expectation density function satisfies this equation. 

346 
CHAPTER 
7. BROWNIAN 
MOTION 
Because of traditional practice and usage, we use the wave function notation 
ψο{ζ,τ) for the marginal density of the expectation, so 
Vc(£,r)=E C T[Zr c(Xr)]. 
The equation is 
Οφ0(ξ,τ) 
, 1 
δ2φο(ξ,τ) 
-ου(ξ,τ)φ0(ξ,τ). 
(7.34) 
t/7 
<±c; 
c/ς-
The integrand U(xT-(N))gc(xT-,N)\I[N]\ 
depends on ξ and τ, so it can be 
represented as 
Ηξ,τ)=14(χτ-(Ν))9ο(χτ-,Ν)\Ι[Ν]\. 
(7.35) 
The method of proof is to show that the integrand /(£, r) satisfies an equation 
similar to the one we want to prove for the integral ψ0(ξ,τ), 
and then use int-
egration theory to justify reversing the order of differentiation and integration. 
The steps in the proof are as follows. Write (7.34) as 
&τφε(ξ,τ) 
= 
-ου(ξ,τ)φ0(ξ,τ). 
Next, show that 
Οξτί(ξ,τ) 
= -οΙ7(χ„_ι,ί η_ι)/(ξ,τ). 
Then, from the equation 
/ 
# T / ( £ , T ) = / 
-ct/(a; n-i,i n-i)/(e,r),= -c£7(i,r)^c(i,r), 
deduce that 
&T [j 
Πξ,τή 
= 
-ου(ξ,τ)φ0(ξ,τ); 
the latter equation being (7.34). In other words it is shown that reversal of the 
order of differentiation (c^r) and integration (J R T-) is possible. To start, here 
is some elementary differentiation of functions. Let /ι(ξ, r) denote 
( r - i n _ i ) " ^ e x p (c 
- ^ 
cU(xn-i,tn-i)(T 
- t n-i) 
V 
T - t n _ i 
Lemma 35 
ο^Η(ξ,τ) 
= f + £ @ 
= 
-ctf(xn-i,fn_i)fc. 
Proof. By differentiation we have 
dh 
dr 
Λ«,r) ί - i ( r - in-i)" 1 - c ί * _ ^ ~ Λ 
- cU(xn-U 
tn-i) 
= 
Λ(ξ,τ) 2c-
ξ± = ^ , r ) L 2 f ^ ^ V + 2c(r-tn_1)-1 
and the result follows by addition. 
O 

7.16. DIFFUSION 
EQUATION 
347 
Lemma 36 
9ί(ξ,τ) 
1 θ 2/(ξ,τ) 
— ä r 
4c—ä?2— = - ^ ( ^ η - ι , ί η - ϋ / ΐ ς , τ ) . 
(7.36) 
Proof. This follows on multiplying both sides of the equation of Lemma 35 by 
For c = a -f ώ and any function fo(£, τ), use the following notation, where ζ, 
77, 0 are real numbers: 
DJ/(i,r) = / « ^ + 0-/(e,r)t 
DS/(i,r) = /tt + ^ - ) - / « ^ ) , 
ηξ ,,, , 
ί(ξ + ν + θ,τ)-/(ξ + η,τ)-/(ξ + θ,τ) + ί(ξ,τ) 
0^/(ξ,τ) 
= 
— 
, 
Df^/(i,r) 
= ϋί/(ξ,τ) + 1ϋ;β/(ξ,τ). 
(7.37) 
Lemma 37 
as ζ, ry, and 0 tena7 £o zero. 
Proof. This follows from Lemma 36. 
O 
The integrand in equation (7.34) has ξ and r as parameters that remain 
fixed in the Riemann sum approximations. But the domain of integration is 
R T 
= Rl r ,Tt, and each evaluation point xT- 
of the Riemann sums satisfies 
χ(τ) = ξ. How can the domain of integration be allowed to vary like this? After 
all, if some analysis is being done on variables in a domain R x R, switching to 
a completely different domain R x R x R is a significant complication. 
Therefore, since the objective is to take joint limits of variable Riemann sums 
and variable r and £, the status of elements r and ξ needs to be altered so that 
they no longer appear as parameters which define the domain of integration. The 
domain itself should remain the same while other parameters in the Riemann 
sums are allowed to vary. 
In general, given real parameters ξ and r, and an integrand h(x, iV, I[N]; £, r) 
whose domain of integration has 
(x,7V,/[iV])G(R] T ,' T [,^(]T ,,T[),l(R^^)) 
with 
a ( r ) = £ , 

348 
CHAPTER 7. BROWNIAN 
MOTION 
the removal of ξ and r from the definition of the domain of integration can be 
accomplished as follows. 
Choose a real number τ+ > τ, and let 
(x+,N+,I[N+}) 
e (R]^ +],Af(]r',T +]) ,1 ( R 1 ^ + ) ) ) . 
Now define an integrand h(x+, N+,I[N+]) 
in the domain RJT ,T ': 
h(x+, N+, I[N+}) := 
0 
ΐίχ+(τ)φξ, 
0 
if t € N+, t > T, 
and x+(t) φ 0, 
I h(x, N, I[N}) 
iit€N+,t> 
τ, 
and x+(t) = 0. 
(7.38) 
Then the Riemann sums satisfy 
(V+)J2h{x+,N+,I[N+)) = {V)Yih(^N,I[N\) 
where the left-hand sum has domain RjT ' r 1, and the right-hand sum has do-
main RJT 'rL The key point is that, with tn = r, and given 
ΑΓ = {ί 1,...,ί η_ 1}€ΛΓ(]τ /,τ[), 
and 
A^+ = { i i , . . . , i n _ i , i n , i n + i , . . . , i m } € A / ' ( ] r / , r + ] ) , 
then, provided tn satisfies £n_i < tn < t n+i, the element r = tn can be all-
owed to vary in the Riemann sum (D+)Y^h 
without altering the domain of 
integration. 
These are the means by which precise meaning is given to the interchange of 
integration and differentiation limits in this case. The reasoning in the following 
theorems is to be understood in the light of the preceding remarks; though in 
order to avoid further complication of notation they are not made explicit in 
the proofs. The continuous modification—of the meaning, or definition, of the 
integral—is used as and when required. 
Here are some further preliminary results. 
Lemma 38 Suppose a < 0, b > 0, and c = a + tb φ 0, and U{y, t) is continuous 
at the point (y,t) = (ξ,τ). IfU~c(x(N)) 
is Gc-integrable, with 
[ 
U-c(x(N))Gc(I[N}) 
= i>c(t,T), 
then U(xn-i,tn-i)U-c(x{N))Gc(I[N]) 
is integrable on RjT''T[ and 
U(xn-i,tn.1)U-c(x(N))Gc(I[N]) 
= 
υ(ξ,τ)φ0{ξ,τ). 
L 
RK>-[ 

7Λ6. DIFFUSION 
EQUATION 
349 
Proof. By Theorem 179, GC(I[N)) is R T x A/*(T)-VBG*. As in Theorems 
52 and 179, the Gc-integrability of U~c{x{N)) implies U-c(x(N))Gc(I[N]) 
is 
VBG* in the same sense. Therefore there exist positive numbers α^, sets θ^ 
(j = 1,2,3,...) whose union is R^T 'r^ x A/"(]T', r[), and a gauge 7 = (L, δ) such 
that, for each 7-fine division Τ>Ί of R^r 'r', 
{ΌΊ)Υ^1^{χ,Ν) 
\U-c(x(N))Gc(I[N})\ < aj, 
for j = 1,2,3,— 
As before, let Dr denote the set of x G R^T ,rl such 
that x(t) does not converge to x(r) as t —>· r. 
Theorem 184 implies that 
/HJT'.T] IDT (x)Gc(/[iV]) = 0. Let ε > 0 be given. For ^ Ε ^ Π (R) T'' T1 \ D T) 
choose 
i'W = {fi 
C } 2 I ( i ) 
so that, if N = {ίχ,... ,ί η} D L'(a;), 
l ^ n - ! , ^ - ! ) - υ(ξ,τ)\ < 
2-jajh. 
For i ? B r let L'(x) be arbitrary. Now consider the gauge 7' = (L',S). For any 
7'-fine division X>y of RlT''rt, 
( * V ) E |tf(*n-i,in-i)W-c(z(JV)X?c(/[iV]) - £/(ξ, T)W-c(z(iV))Gc(J[JV])| 
< 
E J . D , , { l e i ( x , i V ) | ^ n - i , i n - i ) - ^ ) r ) | | W - c ( a ; ( i V ) ) G c ( / [ i V ] ) | } 
SO 
U(xn.utn^)U~c(x,N)Gc(I[N}) 
and 
C/(^,r)W-c(x,iV)Gc(/[iV]) 
are variationally equivalent, and then the result follows from Theorem 44. Note 
that, if b = 0, a < 0, the proof is simpler since Ga has bounded variation. 
O 
Lemma 39 Suppose a < 0, b > 0, and c = α + ώ φ 0, and U(y,t) is continuous 
at the point (y,t) = (ξ,τ). If either ofU~c(x(N)) 
orU~c(x) 
is Gc-integrable or 
gc-integrable on R^T 'rt, then the following hold: 
υ(ξ,τ)Μξ,τ) 
= 
/R]T,,T[i7(x„_1,in_1)W-c(a;(JV))Gc(/[7V]) 
= 
/R]T,,T[ C/(x„_1,i„_1)W-c(a;)Gc(/[Ar]) 
= 
/ Λ τ,, τ [ 
Uixn-utn-M-'MNVgcMNWWl 
= 
^T,M 
U{xn-l,tn^)U-c{x)gc{x{N))\I[N)\. 
Proof. This follows from variational equivalence of integrands. 
O 
The next result has b = 0, a < 0. The partial differential equation in this 
case, with a = —1, is a version of the heat equation (diffusion equation, Fokker-
Planck equation, Kolmogorov forward equation). 

350 
CHAPTER 7. BROWNIAN 
MOTION 
Theorem 220 Let ξ £ R and r > τ' be given. Suppose a < 0, b = 0, and the 
function U(y,t) is real and non-negative for all y £ R and all t €]r\r[. 
If U 
is continuous at y = ξ, t — r, and if 
Ψα&τ) = [ 
U-a(x(N))ga(x(N))\I[N}\ 
exists for each (ξ",τ") 
in a neighborhood of (£,r), then ψα(ζ,τ) 
satisfies equa-
tion (7.34), 
Proof. Regarding the hypotheses, if U(y, t) is continuous for all y, t then by 
Theorem 218, ψα{ζ,τ) exists for all ξ,τ. With 
N = {ii,...,ί η_ι}, 
ΛΓ+ = {ti,..., i n,..., i m} 
as in (7.38), choose ζ in (7.37) so that tn(C) = r + C satisfies 
in_i < £n(C) < ^η+ι 
and write 
1\Ιζ = \ t l , · · · , tn—i, t n ( s j» &n+l j · · · j *mj· 
Then, with τ = tn and £ fixed, and with N~ — {ti,..., 
£ n-i} variable, D£/(£, r) 
is integrable on Ή)τ 'Tl, as it is a linear combination of integrable functions. 
Likewise, D^/(£,r), ϋ ^ / ( ξ , τ ) , and D ^ / ( ^ » r ) a r e integrable for each fixed 77 
and 0. Let ε > 0 be given. Denote the function 
^ ( χ η - ι , ί η - ι ^ - ^ χ ί Λ Γ ) ) ^ ^ ^ ) ) ! / ^ ! 
+ 
ε9α(χ(Ν))\Ι[Ν}\ 
by fte(:r, A/", 7[iV]). This is positive and, by the preceding results, it is integrable 
on Ή)τ 'rL For any (x, JV, /[iV]), the numbers C, ^ and 0 can be chosen so that 
Therefore by Theorem 61 (the dominated convergence theorem), 
lim D g , / 
/(ί,τ) 
= 
/ 
lim 
D ^ / ( i , r ) , 
giving the result. 
O 
When a = 0 and b = L = y/—l, (7.34) is Schrödinger's equation. In this 
case Theorem 61 (dominated convergence) does not work. Instead we use the 
methods of Theorem 64 which are applicable to non-absolutely convergent int-
egrals. It is not assumed that the interaction function U is non-negative, or 
even real-valued, so the following is a stronger result than the preceding one. 

7.16. DIFFUSION 
EQUATION 
351 
Theorem 221 Let ξ G R and τ > τ' be given. Suppose a < 0, b > 0 and 
c = a + ώ φ 0, and suppose the function U(y, t) is continuous at y = ξ, t — r. 
if 
ΦΛξ,τ) 
= 
/ 
U-c(x(N))gc(x(N))\I[N}\ 
JB)T'.T[ 
exists for each (ξ", τ") in a neighborhood o/(£, r), then φε{ξ, τ) satisfies equation 
(7-34), 
δΜξ,τ) 
1 
Ο2ψε(ξ,τ) 
Proof. Regarding the hypotheses, if U(y, t) is continuous for all y, t then by 
Theorem 219, ψ€(ζ,τ~) exists for all ξ,τ. As in Theorem 220, the functions 
D^/(^,r), DjL/(£, r), and ^ ^ / ( ξ ? ^ ) are linear combinations of integrable 
functions and are therefore integrable for each fixed ζ, η, and Θ. But in this 
case we have b > 0, so a function such as /ιε is no help. We must prove that 
1. lim c^0^ oDf^/(i;,T) is integrable; 
2. the sequence of integrals JR]T',T[ ^ ^ / ( ξ , T) converges when the numbers 
ζ, 77,0 -> 0 jointly; and 
3. the integral of the limit equals the limit of the integrals: 
Regarding 1: 
lim ϋ|; θ/(ξ,τ) 
= 
U(xn^,tn^)U-cgc(x(N))\I[N}\, 
and this is integrable, by Lemma 39. 
Regarding 2: Let ε > 0 be given. By the integrability of /(£, τ') (for r' in a 
neighborhood of r), there is ζο > 0 such that, for any given ζ < ζΌ, the function 
Ε^τ/(ξ, r) is integrable (since it is a linear combination of integrable functions) 
and 
/ 
D f 7 & r ) 
= 
DfVc&T). 
Therefore, using the Cauchy condition for integrability, given any such ζ there 
exists a gauge 7^ so that, for any pair V1VV 
, 
I (2>7<) Σ ΡζΊ(ξ, r) - (Vy() Σ DT f& r) 
Theorem 64 then implies that 
< 
e. 
[ 
limDf/(£,T) 

352 
CHAPTER 7. BROWNIAN 
MOTION 
exists. In other words, 
limD*Vc(e,T), = 
C->o 
dr 
' 
exists. Similarly, 
exists. Thus 
fen/ , D^/(i,r), 
&MZ,r) 
dt 
■2 
' 
/ 
, 
C U ? L n D c V ^ T ) ) ' 
= 
. ' f e n / 
, 
OW>f&r) 
Or 
4c 
d£2 
' 
exists. 
Regarding 3: For any division V of Rj r ,T^ we have 
With ε > 0 given, Lemma 39 implies that there exists 7 so that, for all P 7 , 
I (I>7) Σ ("<#(*„_!, ί„-ΐ)/(ξ, T)) - (-<#(£, T)Vc(i, T)) I 
Also, we can find ζο, ryo, #0 > 0 s o that 
ICI,M,|0| < Co,%,ö0 
imply 
< ε. 
| D & ( ( ί ? 7 ) Σ / ^ ' Τ ) ) - (Ι>7)Σ(-<^(Ζη-1,ί„-ΐ)/(ξ,τ)) 
Therefore 
giving the result. 
< 
ε. 
o 
Using the notation of random variability, this result is an equation satisfied 
by the marginal density of the expectation of a joint random variable: 
3 
1 32 
_ ( E , r ( W - c ( X T ) ) ) + - — 
(E,T(U-C(XT))) 
= 
-ου(ξ,τ)Εξτ(ΐ(-%ΧΤ)). 
Note that Theorem 221 (with a < 0, b > 0, c = a + (A ^ 0) includes Theorem 
220 (a < 0, 6 = 0) as a special case. 

7.17. FEYNMAN 
PATH 
INTEGRALS 
353 
7.17 
Feynman Path Integrals 
With 6 = 0 and a = c = —\, Theorem 220 describes the evolution, over time 
and over one dimension of space, of an aspect of the physical density of particles 
in Brownian motion. The physical density in question is the number of particles, 
per unit volume, to be found at a particular location in space and at a particular 
time. 
The physical density in question can therefore be interpreted as a relative 
frequency or probability of observing one of the particles at a particular location 
£, at a particular time r. The probability function used here is G_i. 
Taking a = 0 and 6 = | , s o c = ^ = :y-f^, Theorem 221 describes the evolu-
tion over time and over one dimension of space of the potentiality of observing 
a particle at a particular location £, at a particular time r, the potentiality 
being measured in this case by the complex-valued distribution function GT, 
with T = ]T',T]. 
Interpreted in this way, the preceding sections describe the basic features of 
Brownian motion and the quantum mechanics of a single particle. In the latter 
case, the calculation emerges from Richard Feynman's theory of path integrals. 
This section contains an outline of the ideas, described in Feynman [64] and 
Feynman and Hibbs [67], on which the Feynman approach to basic quantum 
mechanics is based. Many of the points here have been dealt with in the pre-
ceding sections of this chapter. They are rehearsed again in order to reconcile 
them with other presentations of Feynman's theory, and in particular with the 
formulations in Feynman [64] and Feynman and Hibbs [67]. 
Feynman path integration is concerned with the mechanical action of a phys-
ical system, represented as the integral of the Lagrangian function (the difference 
between total kinetic energy and total interaction energy of the system) along 
each of the paths traversable by the system. These paths correspond to the 
sample paths x of c-Brownian motion. Given a particular path x = 
(x(t))teT, 
the mechanical action of the system can be represented as 
/ 
(Kinetic Energy — Interaction Energy) dt, 
Λ(·) 
where the intrinsic kinetic energy and externally induced potential energy or 
interaction energy are variable and depend on x(t) at different times t. This 
can be approximated by a Riemann sum along any given path or history #(·), 
Y^ (Kinetic Energy — Interaction Energy) (tj — tj-ι), 
where, in each term of the sum, the kinetic energy and potential energy terms 
are estimated at time t, tj-ι 
< t < tj. In simple systems kinetic energy at a 
given instant t of time may be proportional to half the square of velocity, which 
can be approximated as 

354 
CHAPTER 7. BROWNIAN 
MOTION 
where tj-\ <t < tj, and a is a number involving mass and Planck's constant. 
With appropriate scaling of variables it may be assumed that a = 1. 
If the system interacts with some other physical system additional forces 
may be observed and the interaction energy (perturbation or potential energy) 
thereby introduced may be represented by a term 
V(x(t)) 
where tj-i 
<t<tj. 
The notation can be simplified by taking V(x(t)) = ^U(x(t)), with 
U(x(tj-i)) 
taken as the representative value of the function; so the action for a given sample 
path x may be approximated in Riemann terms as 
or 
■j 
lj—i 
2 
(x{tj) - 
Xjtj^)) 
U 
-t 
Σ J ^ζ?-1" 
-vWi-M fc-*i-o. 
°J 
°3~ 
Feynman [64, 67] argued that the state function ψ of quantum mechanics can 
be expressed as the aggregate or average over all possible paths or histories #(·) 
of the exponential of mechanical action, provided the latter is multiplied by the 
factor i = \[—\, the averaging being effected by multiplying this function of the 
action by a weighting proportionate to "volume" in the domain of paths. In 
other words, the sum involves an estimate or approximation 
(7.39) 
Here, \I[N]\ denotes11 "volume" in Rjr''r[, with N = {ίι,...,ί η-ι} € 
λί(Τ). 
Also, scaling of variables allows the value 1 to be assigned to each of the other 
physical constants present in the system. When U is identically zero (7.39) may 
be seen intuitively as an Riemann sum approximation, formed from a partition 
V of Rj r 'rt, of an "integral" involving the following calculation, 
jL-p('jCKi)>*)fa<-)· 
where δχ(·) is some kind of volume or measure in Rl r , r' corresponding to |7[iV]|. 
When U is zero (no perturbation or interaction energy), the estimate (7.39) 
above reduces to a Riemann sum in 
1 (xjtj) - 
xjtj^Y 
2 
tj ~ 
tj-i 
Σ 
«p 
' Σ Ι ? 
i-f. 
))I JMI· 
i[N]ev 
\ 
j 
1 Though { t i , . . . , tn-i} 
has previously been denoted by AT , N is used here for simplicity. 

7.17. FEYNMAN 
PATH 
INTEGRALS 
355 
If the partition V is regularized (see (3.4) and (3.5)), then, if we write Xj as 
x(tj), this Riemann sum may be understood as an approximation to 
This argument is incomplete—there are convergence issues to be dealt with if 
these calculations are to be more than merely suggestive. Suppose the iterated 
integrals · · · J^ 
f^ 
· · · are evaluated by successive one-dimensional integrat-
ions f_00- This procedure generates a product which will converge only if each 
of the one-dimensional integrals has modulus 1. (If the modulus of each of the 
one-dimensional integrals is greater than 1, then the modulus of the infinite 
product diverges to +oo; and if the modulus of each of the one-dimensional 
integrals is less than 1, then the modulus of the infinite product converges to 
zero. In either case all information about the evolution of the system is lost as 
the number of iterations tends to infinity.) 
As we have seen when investigating the Presnel integral (Chapter 6), intro-
ducing additional factors 
^-(tj-tj^ 
\ 
(7.40) 
corresponding to each of the transitions or increments Xj—Xj-i in the estimate, 
ensures that each of the iterated integrals has value 1, so the sequence of iterated 
integrals converges to a value determined by the other parameters of the system. 
Thus, replacing (—t)~l by L, a Riemann sum approximation for the average of 
the function of action over the space of paths should have the form 
Σ
 
('
Σ ;}',"-;-', JVw 
And, on regularization of the partition, this approximates to 
J-oo 
J-oo 
Π yl^iitj 
— 
tj-i) 
with no integration on XQ = ξ' or on xn = ξ. Inserting additional ^-values 
in τ' = to < tj < tn = r makes n successively larger. This gives a success-
ively larger number of dimensions in this finite-dimensional iterated integral, 
producing a sequence of numbers which are values obtained when the iteration 
of one-dimensional integrals is performed. 
If and when this sequence of values converges, their limit is sometimes de-
noted by a formulation such as the following: 
L 
--δχ(-). 
(7.41) 
R>''T' Π*6]τ',τ[ 
^2πιdt 

356 
CHAPTER 7. BROWNIAN 
MOTION 
Feynman [64] called the terms (7.40) normalization factors and determined their 
form, not by actually integrating successive Fresnel integrals, but in the course 
of deriving Schrödinger's equation for his integral over paths. 
In a "perturbed" system, with U φ 0, we are no longer dealing with simple 
Fresnel integrands, though we are still seeking to find the "average" of the 
exponential of >/—T times the action; that is, the average or expectation, with 
respect to some weighting function, of 
X(tj) 
- X{tj-i) 
2 
exp U L· I ( 
t.-£\ 
) ~ υ{Χ^-ΐ)}) 
{tj ~ tj~l] 
the average being taken over the domain RjT 'Tt of paths. If volume \I[N]\ is 
used as weighting in the Riemann sum approximations, this would give 
Σ exp U Σ (("^tZf^)2-c(*(*i-i))) 
(*J -*i-o)\*m-
i[N]ev 
\ 
3 
\ \ 
J 
3 ; 1 
' 
/ 
) 
Regularization of the partitions (in accordance with (3.5) and (3.7)) again con-
verts this Riemann sum into an approximation to the finite-dimensional iterated 
integral 
/
OO 
/»C 
-co «/—c 
Cx I . . · ,«^τ — \Ί "^ Ί Ί ' · ') ' ' ' U"*-17 — 1U/JL n ' ' ' , 
where a (..., Xj_i, Xj,...) 
is the expression 
with Xj = x(tj) for each j . If the integration of this expression is iterated in 
successively more dimensions, the resulting sequence of values may converge to 
some limiting value. This limit, if it exists, may be denoted by an expression 
such as 
e(i/;((»)a-^w))*) 
/R]T',T[ 
n 
JB) 
where some scaling factor κ may be related in some way to ILGIT' T[ V27ndt. 
In this case (U Φ 0) it is not so obvious whether we can be guided by 
Fresnel integrals to decide what kind of normalization or scaling factors κ may 
be required in order to ensure convergence, nor is it obvious that the normalizing 
factors (7.40) will do the trick. In the course of proving, in Feynman [64], that 
the path integral satisfies Schrödinger's equation, Feynman produces the factors 
(7.40) for non-zero U. But his argument does not help in establishing conditions 
for the convergence of either his iterated integrals, or of other formulations which 
have subsequently been devised in order to estimate his path integrals. 

7.17. FEYNMAN 
PATH 
INTEGRALS 
357 
Various ad hoc methods are employed, sometimes invoking physical rather 
than mathematical considerations, to ensure convergence of these estimates. 
From the mathematical point of view presented in this book, the integrand 
eHT, (*(*)>) 
rit€]r',r[V2^Ä 
in (7.41) should be thought of as a "marginal density of potentiality distrib-
ution" ; while the integrand 
Λ-ιβ(Α^(Κ»)2-ί^«))Λ) 
is best thought of as a function exp (—| f*, U(x(t))dt) multiplied by the poten-
tiality density of (7.41), so (7.42) becomes the "marginal density of expectation" 
with normalization factor κ = Π*€ΐτ' r[ V^ntdt^ as in (7.40). When this is 
expressed in terms of the concepts and notation of this book, it becomes 
L 
U~Hx)9i(x(N))\I[N}\, 
R ] T ' , T [ 
or, in variationally equivalent version, 
U-Hx(N))G,(I[N]). 
JB) 
Therefore the following meaning is ascribed to Feynman's theory: 
:= 
E € T ( W - * ( A · ) ) 
= 
/rtT,.T[W-4(x)fl4(s(AO)|/MI 
= 
/ ^ . ^ ( ^ ( / [ A T ] ) 
= 
/rtT,,T[W-4(s(J\0)G4(I[JV]). 
To switch from marginal density of distribution and marginal density of ex-
pectation in Rj r 'r! to distribution function and expectation in Rj r 'r^ we must 
include an integration dx(r), or άξ, in dimension r. Thus the convergence issue 
in (7.42) is, essentially, the issue of whether or not 
e(-4 Γτ, u{x{t))dt) 
(-4 /;, u{X{t))dt) 

358 
CHAPTER 7. BROWNIAN 
MOTION 
can be regarded as, respectively, datum and contingent observable, in the sample 
space Rj r ,T', relative to the potentiality distribution function 
e(*£(H*)» 
δχ(·)άχ(τ), 
with 
E ;H £>(*<*»*)' 
(_4 ru(X(t))dt) /βο/;(κ&)>Λ 
the extra "άχ(τ)η being tacked on at the end because the domain is now 
instead of Rjr''TL 
In order to preserve the meaning of this as an expected value, its distribution 
function should, being of the incremental Presnel variety, have normalization 
factor of the form 
κ= Y[ V2ntdt. 
te]r',r[ 
Convergence is a consequence of the cancellation effects produced by the oscil-
lations of the Fresnel factors in the integrand, as pointed out by Feynman in 
his original investigation [64] of the subject. But only a non-absolute averaging 
technique, such as Henstock's division system integration, is sensitive enough 
to enable the cancellations to occur. Separating the alternating positive and 
negative values prevents the cancellations, and the estimates produced by that 
method are accordingly divergent. 
Theorems 219 and 221 of the preceding section are the basis of this in-
terpretation of Feynman path integration. Theorem 219 shows that, if U is 
continuous, Feynman's path integral exists in the Henstock integral sense, and 
Theorem 221 shows that when Feynman's path integral exists in this sense, it 
satisfies Schrödinger's equation. 
7.18 
Feynman's Definition of Path Integral 
Presenting his path integral 
e H J ; w
) lMW*m ■ 
j 
Feynman [64] gives an expression of the following form as its estimated or ap-
proximate value: 
/
oo 
/*oo 
-oo J — oo 
(7.43) 

7.18. FEYNMAN'S 
DEFINITION 
OF PATH 
INTEGRAL 
359 
with integrand 77 (... 
,) = a x f t where 
1 
n j V^I(tHi-i) 
and where io = ΤΊ ^η = r> and tj — tj-i have the same value e for each j . With 
c = L, the integrand in this expression corresponds to the function Tc of (7.33): 
^ ( V M - ) - -
nf-i^r-M))* 
Feynman took lime_^o of (7.43) as the definition of the path integral represen-
tation of ψ±(ξ,τ), so in effect Feynman [64] declares that 
Φ: 
,(ξ,τ):= lim / 
T,(y(M-))dy1dy2...dy2r_1. 
(7.44) 
In a footnote Feynman [64] wrote: "There are very interesting 
mathematical 
problems involved in [this declaration]. Some sort of complex measure is being 
associated with the space of functions x(t). 
Finite results can be obtained un-
der unexpected circumstances because the measure is not positive everywhere, 
but the contributions from most of the paths largely cancel out. These curious 
mathematical problems are sidestepped by the subdivision process [of slicing the 
time interval ]τ',τ[ into equal subintervals of length e]. However, one feels as 
Cavalieri must have felt calculating the volume of a pyramid before the invention 
of calculus." 
Feynman's definition of the path integral is essentially the equation 
Va&T) = lim / 
_ 
?a(y(M-))dy(M-) 
of Theorem 218 above, the difference being that, in place of \i, the latter has 
c = a + ώ with 6 = 0 and a < 0. 
It is pointed out in Theorem 219 that, with c = a + ώ, a < 0, 6 > 0, the 
hypotheses of that theorem do not imply either of the following: 
Ε ξ τ [W-* (X)] = ψ, (ξ, r) 
= 
lim^oc J 
- Th 
(y(M-))dy(M-), 
R 
r 
7.45 
Εξτ{Κ-<(Χ)}=ψ0(ξ,τ) 
= 
limr^ 
fRM-Tc(y(M-))dy(M-). 
In other words, Theorem 219 does not provide a justification or validation for 
(7.44)—that is, for Feynman's definition of the path integral. 

360 
CHAPTER 7. BROWNIAN 
MOTION 
But, according to Theorem 218, such a passage to the limit is valid whenever 
b = 0, a < 0, and U is continuous and non-negative, and this provides some 
hope. 
Therefore the next step is to search for conditions under which the passage 
to the limit may be valid, not just when 6 = 0 and a < 0, but also when b > 0 
and a < 0. This may also help to make a connection with Feynman's own 
definition of the path integral. 
7.19 
Convergence of Binary Sums 
Take τ' = 0 and T =]0,r]. As in Section 3.5, take 
r(r) 
_ 
(7.46) 
K*b 
= 
]-g + ( p - l ) 2 - « , -q + p2~q], 
l<p<q2q+\ 
r)r) 
= 
jr2~r, 
l<j<2r 
Suppose r and q are given. Suppose a function i/(y, £) takes constant values for 
y 
€ 
#-<?b? 
1 < ρ < $ 2 « + 1 , 
ί G 
(7.47) 
1 < j < 2^, 
and is zero for all other values of y, with U(y, 0) = U(y,r2~r), 
all y. Thus, as 
in (7.28) and (7.29), U(y,t) is itself a step function, and, for the partition K, rq 
of R T = R'°'r], consisting of disjoint cells K £ K Γς, the step function U~q
c(x) 
of (7.30) and (7.32) is the same as U~c{x)\ that is, 
V-q
c(x)=U-c(x)=U~c(x) 
for all x G R T. Then, as in Property FBI of Section 7.15, the marginal density 
of the expectation of U~q
c(X) is 
ΨΛξ,τ) 
= 
/ 
U~c(x)Gc(I[N}) 
= 
/ 
U-q
c(x)Gc(I[N}) 
= E { u c # ^ # : kG^-
r < ?}, 
where w~rq is defined12 in (3.10), with χ(τ) = ξ fixed. 
In this case, the right-hand side of equations (7.45) is the same for all q' > q 
and all r' > r, so Feynman's definition [64] of the path integral gives the marginal 
expectation density ψ€(ζ, τ). (For a proof of this, a gauge 7 in Rl r ,rt must be 
chosen which conforms to each cell Krq of the regular partition /C 
rq.) 
Can any broader set of conditions be established, applicable to a class of 
functions U wider than the class of step functions, for which Feynman's defin-
ition coincides with the Riemann sum interpretation of the path integral, as 
12 As a reminder of the meaning of notation: uj 
is a version of the symbol u;r<?lk applied in 
a more general context on page 335. The superscript - in w-rq 
indicates that no summation 
takes place in dimension r = TA^· · 

7.19. CONVERGENCE 
OF BINARY 
SUMS 
361 
presented in this book? Recall that, by Theorem 217, continuity of U is sufficient 
to ensure existence of ψ0(ζ,τ) = J R T - U~C(X)GC(I[N]). 
It would be interesting 
to know if there are any further conditions for U which make U sufficiently like 
a step function, so to speak, to give the relation 
Μξ,τ)= 
lim V { u ^ k
Q ^ k 
: k e ^ ' l . 
Theorems 217, 218, and 219 show that this holds when 6 = 0 and a < 0. But 
they do not establish that, if b > 0, the numbers on the right converge to the 
number on the left—nor even that they converge at all—as r and q tend to 
infinity. 
The first step is to investigate any local properties of U—other than being 
a step function—which, whenever b > 0 and a < 0, might ensure convergence, 
as q, r —> oo, of the sequence of numbers Εξτ \U~q
c(X)], where 
Suppose Γ2 > ri and #2 > qi- Then, since r' = 0, for j = 1,2 we have 
Mr. = {jr2- r' : 0 < j < 2r> } , 
Mr2 D Mri. 
With ε > 0 given, and for non-step function t/(y,i), the aim is to declare (see 
Definition 54) a continuity-type condition for [/, which we call BM-continuity,13 
and which, if satisfied, gives a Cauchy inequality for the binary Riemann sums 
ßrq = Εξτ [U-q
c(X)} . 
A suitable Cauchy condition is existence of rE and q£ so that 
\Priqi ~ Pr2q2\ ^ ^ 
for all V2 > ri > r£ and all q2 > q\ > qe. The form of this Cauchy condition 
shows that it will generally be the case that q and r cannot tend to infinity 
independently, and that choice of qE depends on n. 
BM-continuity oiU(y, t) is expected to deliver an inequality that will make 
it possible to invoke results such as Theorem 64, ensuring convergence of the 
binary Riemann sums. Theorem 64 can be invoked if there exist gauges 72 -< 71 
so that, for all 72-fine 
(x,I[N]), 
\Kic
qi(*) -K^{x)\ 
\GC(I[N})\ < eG^(I[N}). 
(7.48) 
Then, for all 72-fine division Vl2 of R T, 
Κ^) ΣΚά(*)Gc{I[N)) - (V,2)Urrq2(x)Gc(I[N})\ 
< e(Vl2)G^(I[N}) 
= ε, 
1 3 "BM" is a reference to Brownian motion. 

362 
CHAPTER 7. BROWNIAN 
MOTION 
and Theorem 64 then gives convergence of the binary Riemann SUmS Prq &S 
r, q —► oo. 
The program of proof is easier to express if G(I[N}) is replaced by its dens-
ity version g(x(N))\I[N]\, 
and this is readily accommodated because of the 
variational equivalence of the two expressions. Then (7.48) becomes 
fe» 
- * C » ( * ) | ISc(*(W))l < eg-iMN)) 
(7-49) 
after cancellation of \I[N]\ on both sides. At first sight, continuity of U suggests 
that, in (7.49), uniform continuity of U and uniform continuity (with respect to 
t) of x = x(t)teT € Ca0 C R T might deliver an inequality 
\Κ^{χ)-Κ^{χ)\<ε, 
leaving \gc(x(N))\ to be related somehow to the real-valued 
g-i(x(N)). 
The trouble with the latter point can be easily seen if c has a = 0 and 6 = 1 . 
Then, for N = {h,... 
,in-i}> *o = 0, t n = r, 
n 
\gc(x(N))\ = \gMN))\ = τΤ* [ J ^ " *i-ij"*, 
i=i 
while 
9-i (x(N)) = π-t JJ(ii - t^r* ft«P (-(Xf._X/"l)2) · 
j=l 
j=l 
V 
J 
J 1 
/ 
In the context of inequality (7.49) the first two factors of each of these express-
ions cancel each other out. But, for x G Ca$, each factor exp (— l~*j 
) 
tends to zero as tj — tj-\ tends to zero. Thus, uniform continuity of U(y,t) is 
not enough to give inequality (7.49). 
Nonetheless, with suitable choice of 7, (7.49) is trivially valid for step func-
tions [/, since the left hand side is zero. Therefore the class of BM-continuous 
functions U(y, t) is not empty. 
In following the Cauchy approach to convergence of binary sums of step 
functions, the aim of BM-continuity is that U should be sufficiently "close" to 
step function status to deliver (7.49). This means that U must be such that an 
inequality of the form 
\K^{x)-UZM < ^Πβχρ (JXl~_X
t
J-_f) 
(7.50) 
holds. A closer examination of this point is worthwhile. Retain the "worst case" 
scenario, a = 0, 6 = 1. This also keeps the notation a bit simpler. For p = 1,2, 
*C» = ^ί-*Σ^(^*0(τ^)-τί:')) 
2rP 
- Π^^^^Β"'-^)) 
3 = 1 

7.19. CONVERGENCE 
OF BINARY 
SUMS 
363 
It is assumed that V2 > r\ so, for each j , Mr2 D Mri implies that 
and 
Π β χ ρ ^ ί / , ί ) ^ - ^ ) ) , 
fc=l 
where y = x$gi) and t = r } ^ for r j ^ < r^ 2 ) < r j r i ) . Because W-^(x) is 
cylindrical for p = 1,2, inequality (7.50) can be used in the form 
2 Γ 2 
/ 
/ 
\2S 
(Xk - 
Xk-l) 
l*c» -*c*(*)l <ε Π H - L 
tl) · 
<
7·
51) 
fc=l 
\ 
Tfc 
Tfc-1 
where xk = x \T~k )' 1 — ^ — ^ r 2· Another way of writing (7.51) is 
I 2r2 
2r2 
J J exp(^u) - J J exp(^fc) 
fc=l fc=l 
2r2 
fc=l 
\ 
rfc 
Tk-\ J 
where 
with y = x^ l } and t = rpl 
for each j such that τ ^ 
< τ£2) < r j r i ) . By taking 
the absolute value of the left-hand term of inequality (7.52), the difference of 
exponential functions is converted into a combination of sine and cosine terms. 
The latter are uniformly continuous, and the function x, being step-wise con-
stant, is a step-wise uniformly continuous function of the underlying function 
x(t)—which is itself uniformly continuous with respect to t provided x G Cae 
(0<a< 
§, 
θ>0). 
The right-hand side of inequality (7.52) can be expressed as 
Π
2-^2 
/ 
(xk-xk-i)2\ 
εζ 
exp ( 
» 
(r2) 
(r2) 
fc=l 
\ 
Tk 
Tk-\ 
Therefore, to see how the local behavior of the function U(y, t) might ensure the 
validity of (7.52), consider corresponding factors of the three product expressions 
in (7.52): 
2"ra 
/ 
{Xk-Xk-l)2\ 
Vik, 
V2k, 
ε 
exp 
- 
. 
\ 
Tk 
~ Tk-1 
/ 

364 
CHAPTER 7. BROWNIAN 
MOTION 
For 0 < ε, the 2r2th root of ε tends to 1 as ri tends to infinity with r2 > v\. 
Therefore e2 r2 can be replaced by a positive number λ < 1 which does not 
have to be a small number. (By taking r2 > v\ > τε sufficiently large, the 
product of 2r2 of these relatively large factors λ can be made as small as needed.) 
By Theorem 184, those x which fail to satisfy x G Cae form a G_i-null set 
(0 < a < | , 0 > 0). For any given a,#, the numbers r and q can be jointly 
chosen to ensure that the factor (xk — Xk-i)2 
is bounded above by (2_<72) , 
= 2~2q2, so that 
{xk-Xk-if 
< 2~2q2 
< 
2~2qi 
for q2>qi. 
The factor (T£2) - τ£*Λ 
equals 2r2. Therefore 
™4JXL)Xkt!)) 
> -P(-2"29l2-) = exp(-2"2-^) 
' f c - 1 
Since r^2' — r^2{ = 2 
r2, a sense of what is involved in inequality (7.52) can 
then be gained by considering the pair of expressions 
\U(y,t) - U(y',t')\ 2-r>, 
Aexp ( - ( ^ r ~ ^ ) ■ 
To link the behavior of these two expressions, note that 
y,y' € K«b = ]-qi + ( p - l)2-«,-qi 
+p2"*] , 
ί,ί' ε ΐ η ^ , τ ? ] 
imply that 
\y-y'\<2-*\ 
\y-y'\2-^<2-^^\ 
\t - t'\ < 2"r2, 
\t - i;|2-r2 < 2~2r2; 
while 
λ - ρ ( - ^ 7 ^ ) 
> Aexp ( - 2 — ) . 
V 
Tk 
Tk~\ 
/ 
By continuity of the functions, the linkage or comparison is, loosely speaking, 
therefore reduced to comparison of the values 
2-(<?i+r2) i n combination with 2~2r2, 
Aexp (_2" 2^+ r 2). 
(7.53) 
Provided both <?i and T\ are sufficiently large (so r2 > r\ is sufficiently large), 
then, as r2 increases, the value exp (—2~2ςι+Γ2) decreases faster than 2~^1+r2^ 
in combination with 2 - 2 r 2. 
It should not be assumed that the binary sums ßrq converge when v and q 
increase independently. But (7.53) indicates that, for given τε, r\ and r2, the 
numbers q\ and q2 (q2 > Qi) can be arbitrarily large; so BM-continuity of U 
implies that, for any fixed r > re, lim^oo ßrq exists. 

7.19. CONVERGENCE 
OF BINARY 
SUMS 
365 
While BM-continuity is more demanding than uniform continuity, a reason-
ably "flat" function U(y,t) will deliver inequality (7.48). If U is assumed to be 
differentiable, the mean value theorem 
is useful in assessing BM-continuity. If U(y, t) is a step function then 
dU(y,t) 
dt 
dU(y,t) 
dU{y,t) 
dt 
dU(yyt) 
dy 
need not 
are both zero. The idea is that, while 
necessarily be zero, they should be locally "small" in order to produce step 
function-like "flatness". Thus, there are various ways in which sufficient cond-
itions for relative "flatness" can be expressed. 
The preceding discussion provides motivation for the following definition. 
Definition 54 Suppose c = a + ib, a < 0, b > 0, c φ 0. A real-valued function 
U(y,t) is BM-continuous if, given ε > 0, there exist re and qe, and if for each 
p = 1, 2, the pair (rp, qp) is chosen jointly and not necessarily independently, to 
satisfy Γ2 > r\ > r£ and q2 > Qi > Qe, then 
\l^c
qi(x)-U^qi(x)\\Gc(I[N])\<eG.i(I[N]) 
(7-54) 
for all associated 
(x,I[N]). 
Note that U~p
c
qp(x) and V~p
c
qp(x) are the same for p = 1,2. 
Theorem 222 Suppose T =]τ',τ[, 
c = a + ώ, a < 0, b > 0, c φ 0; and 
suppose the real-valued, continuous function U(y,t) is BM-continuous. 
Then 
U~c{x) = \iuir^q^00U~q
c(x) 
is Gc-integrable on R T, and 
M&T) 
= 
fnTU-c(x)G-c(I[N}) 
= 
limr^oc 
fKTU-q
c(x)G-c(I[N\) 
= 
E^[U-C(X)} 
= 
hmr^ooE^lU-^X)]. 
Proof. This follows from Theorem 56. 
O 
The function Tc(yM-) 
is defined in (7.33), and Theorem 218 gives a result 
corresponding to Feynman's definition (7.44), or (7.45), of the path integral, 
but with c = a < 0 instead of c = | . The following result gives conditions for 
which Feynman's definition is valid. 
Theorem 223 Suppose T =)τ',τ[, 
c = a + ώ, a < 0, b > 0, c φ 0, and 
suppose the real-valued, continuous function U(y, t) is BM-continuous. 
Suppose 
inequality (7.54) remains valid as qi,q2 —> oo. Then 
Φ, : ( £ , T ) = l i m / 
_Fc{yM-)dy{Mr). 

366 
CHAPTER 7. BROWNIAN 
MOTION 
Proof. This follows from Theorem 222. 
O 
With c = | , Sections 9.7 and 9.8 of Chapter 9 provide an indication of how 
the expressions ψ±.(ξ,τ) and E [W"~3pQ] can be calculated numerically. 
Example 58 Rather than seeking the most exhaustive version of the theory, 
the main consideration has been to illustrate key elements of the Riemann sum 
method. With T =]τ',τ[, existence of the state function (or marginal density of 
expectation) ^ c(^,r), = E^r [U~~C(X)], amounts to Gc-integrability on R T of the 
function U~C(XT)> 
This depends in turn on the character and properties of the 
function U(y,t). 
It has been convenient to impose conditions on U which imply 
Gc-almost certain existence of fTU(x(i),t)dt. 
This means that, in advance of 
seeking a limit of Riemann sums in R T, it is assumed that a limit of different 
kinds of Riemann sums in T exists. But, despite the convenience of such an 
assumption, no such a priori restriction of the integrand is implied in Definition 
17, which defines the integral on R T. In Chapter 8 on stochastic integrals, a 
form of weak integrability on T of an integrand in R T is introduced. 
This 
contrasts with the strong integrability of U(x(t),t) 
on T which is a key feature 
of this chapter. By writing % —\u^v\ G I(T), and with (t,i) denoting associated 
point-cell pairs in the domain T, the assumption has been that U(x(t),t)\i\ 
is 
Riemann integrable on T for Gc-almost all XT G R T. This in turn means that, 
for Gc-almost all χτ G R T, accurate Riemann sum estimates of JT 
U(x(t),t)dt 
can be obtained simply by choosing a gauge 7 = (L,S) with a suitable L(x) G 
λί(Τ). 
From this, Gc-integrability ofU~c(xr) 
may be deduced. 
But Riemann integrability—or, indeed, any other kind of integrability—of 
U(x(t),t) 
on T is not a pre-condition of Gc-integrability ofU~c{x(N)) 
on R T. 
And what ifU(x(t),t)\i\ 
is Riemann-complete integrable on T, but not Riemann 
integrable? Would it still be possible to use the methods of this chapter? 
In that case the partitioning of T by sets L(x) needs to be done more care-
fully. In other words, gauges 7 will not be enough. But gauges ζ of the ζ-division 
system of Section J^.l permit η-fine divisions of T to be formed; and then the 
methods of this chapter can be applied in order to establish Riemann sum con-
vergence in RT. 
O 
7.20 
Feynman Diagrams 
The path integral theory gives rise to a kind of graphical calculus—the Feynman 
diagrams—for analyzing quantum mechanical interactions. Feynman diagrams 
are explained in detail in, for instance, Feynman and Hibbs [67] and Schulman 
[206]. Also, a film of a lecture on the subject by Feynman is available [68]. 
The basic idea is that the exponential function in ZY, = U~c, is expanded 
as a power series, each term of which is then integrated with respect to Gc in 
R^r 'rL In Feynman and Hibbs [67] this is called the perturbation expansion 

7.20. FEYNMAN 
DIAGRAMS 
367 
Figure 7.1: Feynman diagrams. 
or perturbation series, and the perturbation method is applied to a number of 
physical problems. 
The term by term integration of the perturbation expansion yields an infinite 
series of integrals, each term of which is given a diagrammatic representation 
describing one specific aspect of the quantum mechanical processes taking place 
in the interaction. These representations are the Feynman diagrams. 
Four examples of Feynman diagrams are portrayed in Figure 7.1. The pur-
pose of such diagrams is to provide a visual analysis of the process as a whole, 
indicating the various kinds of physical phenomena which may occur in the in-
teraction. The variable t (for time) is represented on the horizontal14 axis, with 
displacement x(t) on the vertical axis, as in Figure 3.1. The latter corresponds 
to the usual "jagged path" diagram for Brownian motion. 
Feynman diagrams can be thought of as a resolution or disaggregation of 
the dynamic elements of interaction which constitute the motion of quantum 
particles. Figure 6-2 on page 123 of Feynman and Hibbs [67] shows this dis-
aggregation of quantum mechanical "Brownian motion" into component parts 
represented by Feynman diagrams. Figure 6-3 on page 127 of the same book 
shows a partial re-aggregation of the Feynman diagrams into a "Brownian mot-
ion". 
This section begins with an outline of the theory of Feynman diagrams as 
presented in Feynman and Hibbs [67] and Schulman [206]. The latter have 
a = 0, & = ^ , c = ^ = :^ΞΪ; and they do not address underlying mathematical 
issues such as term by term integration of the power series expansion of the 
state function or marginal expectation density. 
In order to supply these missing elements of the theory, this section considers 
In physics textbooks, time is usually pictured on the vertical axis of a Feynman diagram. 

368 
CHAPTER 7. BROWNIAN 
MOTION 
the underlying mathematical issues of convergence. The analysis presented here 
is valid for c = a -f ώ, a < 0, b > 0, c φ 0; so it includes the more familiar 
Brownian motion as well as quantum mechanical "c-Brownian motion". 
The presentations in Feynman and Hibbs [67] and Schulman [206] employ the 
following preliminary result. Suppose f(s) is a real- or complex-valued function 
defined for s G [τ',τ]. Let p be a positive integer. Write 
}τ',τ)ΐ>= 
] τ ' , τ ] χ ] τ ' , τ ] χ . . · χ ] τ ' , τ ] , 
so]T',Tp>eI(Rp). Then 
f (s) = f (si, s 2,..., sp) := f{s1)f(s2) 
· · · f{sp) 
is defined for s = (si, $2> · · · > sp) £ ]r', T]p- Let si, S2,..., sp satisfy 
T' < si < S2 < · · · < sp-i < sp < r. 
(7.55) 
Then 
ί = ] τ , , β ι ] χ ] τ , , β 2 ] χ · · · χ ] τ / , 5 ρ - ι ] χ ] τ , , τ ] G I ( ] r ' , 7 f ) . 
For each r denote elements of I (]r7, sr]) by z^Sr\ so when condition (7.55) holds, 
we have z^Sr^ G I (z(Sr+1)) (1 < r < p — 1). In other words i^Sr^ is a member of 
the family of cells which are subsets of ζ^+^. 
Thus the set 1 is 
1 
= 
ZM 
x ζ(*2) x . . . x 2(*Ρ-Ι) x 
iM 
G l(i^) 
x l ( ^ ) x . . . x I ( f W ) x l ( ] r ' , r ] ) . 
Lemma 40 Suppose p is a positive integer. If a real- or complex-valued function 
f(s) is integrable on ]τ'',τ] and is continuous for τ' < s <τ, then the expression 
(ί'Ή' 
is equal to 
, ( β ρ - ΐ ) 
,( 5p) 
where iM €ΐ(]τ',τ]), z(Sr) Gl(z ( S r + l )) /or r = l , 2 , . . . , p - 1, and 
f(s) = f(si,8 2,...,s p) = 
f(s1)f(s2)'-f(sp). 
Proof. With τ' < sr < sr+i, and with s r +i fixed and sr variable, the notation 
L 
j ( e r + i ) 
1 ( 5 1 , . . . , 5 r , 5r-j_i, · . . , Sp) 
represents 
/ · β Ρ € ι ( β ί , ) · 
Jl^r)el(l(s 
t(sr)£l(i(sr+l)") 
*\sli 
· · · j S r , 5r_(-i, . . · , 5 p j 
,(«r) 
,K) 
or 

7.20. FEYNMAN 
DIAGRAMS 
369 
pSr+l 
I 
I (^Si, . . . , 5 r , 5r_|_i, . . . , Spja5 r. 
JT' 
In traditional notation, this lemma says that (/^ f(s)ds)P 
is expressible as 
P}' I 
[ I 
■' ■( / , /( Sl)/( s2)-'-/(5p_l)/(5p)ii8i j .-.dSp-ij dsp. 
Using induction on p, the statement is obviously true for p = 1. Assume that, 
for any £ with τ' < t < τ and r ; < 5i < «2 < · · · < sp_i < £, the expression 
( J i , ti f(s)ds) 
is equal to (p — 1)! multiplied by the value 
/ 
(7 
··'(/ 
/(*I)"-/(*P-I) 
J\r'A 
\J)r',sp^) 
\J]T>,SI] 
, ( * I ) 
, ( * P - I ) 
, ( s P - i ) 
Write 
fe(*)=(/r'tJ/WM) 
={£mdS)P' 
Then <7P(£) is differentiable for p = 1,2,3,... and, for τ' <t < r, 
%(*) 
di 
= pf(t)([tf(s)ds)P 
=pf(t)(r% 
/( 
p - 1 
*)|t| 
The latter, as a function of t Ε]τ',τ), 
is Riemann-complete integrable (since it 
is a derivative), and 
9P(r)= 
\pf(t)[ 
/( 
^ € l ( ] r ' , r ] ) \ 
W.el(]r',t]) 
p - l \ 
*)|t| 
bl· 
The result then follows by induction. 
o 
This can also be proved using a geometric argument in Rp, which has the 
advantage of providing more visual insight into the result. 
Lemma 41 Suppose, for τ' < s < r, f(s) is a real- or complex-valued step 
function. Then, writing f (s) = /(si)/(s2) · · * f(sp)> 
a: 
f(s)ds 
is equal to 
J]T',T] \Jl(sp) 
\Jl(°3) \Jl(°2) 
I 
1/ 
,(*2) 
, ( « P - I ) 
, ( S P ) 

370 
CHAPTER 7. BROWNIAN 
MOTION 
Proof. Since / is a step function there are cells κ^ 
G Ι(]τ',τ]), forming a 
partition {κ^}^=1 
of ]r', r], in which, for each r — 1, 2,..., p, f has constant 
value ar; and there are numbers 
^r) I such that 
/ 
/(β)Μ = £α Ρ. 
P h 
With 
}T\T]P 
= 
}τ',τ]χ}τ',τ}χ·--χ}τ',τ] 
and 
3 = 3i XJ2 x ··· x j P € I(]r',r] p), 
we have 
([Tf(s)ds)P 
= 
( Γ1 
f(s)\z\] 
\JT' 
J 
\Λ(ΞΙ(]Τ',Τ]) 
/ 
= 
/ 
f(si,52,...,5p)|.?| 
•/j€l(]r',r]P) 
- (P-J 
= 
I 
f{si)f{s2)---f{sP)\3i\\32\---h 
JT',T)P 
and, summing in the appropriate order, this equals 
/ 
(7 
' ( f 
fMfM · · ■ / Μ Λ Ι J W · · ·) IJPI. 
Take p = 2 and, for simplicity, take ]τ',τ] = ]0,1], Let 
Δ 2 = {(βι,β 2):0<βι < s 2 < l } . 
Then ]0,1] x ]0,1] divides into two identically shaped parts, one of which is Δ 2. 
By the symmetry of the integrand, ΣΓ=Ι 
ar divides into two equal parts, so 
that 
/ 
f{82)([ 
f(s1)\Jl\)\j2\ 
= 
[ 
lA2(suS2)f(s1)f(S2)\j1\\j2\ 
J}0,1] 
\«/]0,e2] 
/ 
^]0,1]2 
= \ [ 
/(*ΐ)/(*2)|Λ|, 
and this is the required result for p = 2. For p = 3 let 
A3 = {(si, 52, S3) : 0 < 5i < s2 < s3 < 1}. 
This set trisects the set 
{(5i,52,53) : 0 < 5i < 5 2 < 1 } = Δ 2χ]0,1], 

7.20. FEYNMAN 
DIAGRAMS 
371 
which, in turn, bisects ]0, l]x ]0, l]x ]0,1]. Again, by the symmetry of the int-
egrand, the iterated integral 
/ 
f(s3)(f 
f(s2)([ 
/( Si)|ji|) |ja| ) |j 3| 
equals 
lA3(«l,S2,53)/(5i)/(s2)/(s3)bl|b2|b3| 
which equals 
/ 
J\0,1\ 
\ l\ (jo i 3 f(sl)f(S2)f(s3)\jl 
X J2 X J3| j j · 
3 
This proves the case p = 3. Higher values of p are proved similarly. 
O 
Lemma 42 Suppose a real- or complex-valued function f(s) has f and \f\ int-
egrable on [τ',τ]. Then, using the same notation, the expression 
J]T' m\t\] 
is equal to the iterated integral 
pi / 
( [ 
( [ 
■ ■ ■ ( 7 
f(s)b(si)l) · · ) b(Sp-l}l) \j{Sp)\-
J]r',r) 
\J]T>,SP] 
\./]τ' | β ρ_ι] 
\JW,s2] 
J 
J 
J 
Proof. In other words, continuity of / need not be assumed. By Theorem 
76, / is measurable (i.e., measurable with respect to the cell function |z|), so it 
is the limit of a sequence of step functions; and its integral is the limit of the 
integrals of the step function approximations. The preceding lemma then gives 
the result. 
O 
If / is Riemann integrable then / and |/| are integrable, and / is measurable. 
With this lemma out of the way the outline of the theory of Feynman diagrams, 
as presented in Feynman and Hibbs [67] and Schulman [206], can be resumed. 
Suppose a < 0, b > 0, c = α + ώ Φ 0, T =]τ',τ], Τ~ =]τ'',r[, x(rf) = £', and 
xiT) — ζ- (Application of the theory to quantum mechanics has a = 0, b = ^, 
c = |.) Suppose an interaction function U is given. The joint-basic observable 
is 
XT ^ xT [RT, Fx] 
where FX(I) = GC{I) 
for I = I[N] G I ( R T ) , 
and the joint-contingent observable is 
Z Y - c ( X T ) - ^ - c ( x T ) [ R T , G c ] . 

372 
CHAPTER 7. BROWNIAN 
MOTION 
If the various integrals exist, including those requiring the continuous modific-
ation of the integral, then the marginal density of expectation 
E(T[U-C(X)}, 
= Vc(i,r), 
is given by 
/ 
U-c(x)dGc 
= 
/ 
U-c(x)Gc(I[N]) 
= 
f ^ τ[ (exp (-cjT
fU{x{s),s)ds\\ 
GC(I[N}). 
In more formal notation the latter integral is 
fx€l[N]* 
( 
( 
rsei* 
\ \ 
/ 
exp i-c 
U(x(s),s)\i\) 
)GC(I[N}). 
Λ[ΛΓ]6Ι(Κ1Τ,·ΤΪ) \ 
V 
^*€I(]T',T]) 
/ / 
Power series expansion of the exponential function gives 
e x p f - c f 6 * 
U { x i s ) , s ) \ t A = f ^ t ^ i 
ί 
U(x(s),s)\z\] 
. 
V 
Λ6Ι([τ',τ]) 
) 
^Ό 
P! 
VV.r] 
/ 
If term by term integration of the infinite series can be justified, this gives 
EiT[W-c(X)] = Vc(£,r) = £ 
^ 
/ 
( / 
tf(*(*),*)M 
j 
GC(I[N}). 
(7.56) 
If U and \U\ are integrable (so £/ is measurable), Lemma 42 implies that 
/ 
U(x(s),s)\i 
can be replaced by 
P\ [ 
(f 
· ( / 
f(B)|t^)|) 
J]T',T] 
\J}T>,SP] 
ν/]τ'|β1] 
' 
7 
, ( β ρ - ΐ ) 
, ( S P ) 
where 
f (s) = J7(x(Si), 5i)t/(x(52), S2) ' ' * #ΌΦ ρ), SP) 
and where, with sp+i = r, we have 
, ( * r ) 
}r',sr] 
e I(]r',s r + 1]) 
C Ι(]τ',τ]), 
1 < r < p. 
In other words, 
(Γ 
U(x(s), s)ds\ 
=P[ Γ ( Γ ' ~ ( 
Γ 
f ( s ) ^ i ) ' * * dsP-i) 
dsP-

7.21. INTERPRETATION 
OF THE PERTURBATION 
SERIES 
373 
The perturbation series, or perturbation expansion, 
oo 
oo 
Μ.τ) 
= Σ^' τ)' 
ΟΓ Μξ,τ;ξ',τ') 
= Σψ0,ρ(ξ,τ;ξ',τ'), 
(7.57) 
is deduced from (7.56) as follows. By cancellation of the factors p!, and if 
reversal of the order of integration can be justified, each term ψ0,ρ(ζ,τ), 
= 
^,ρ(£,τ;ξ',τ'), is 
{~C)P L (Γ (''' 0Γ (L· r[
 fWWW)) ώι) · · ·) ώρ-ι) dspy 
(7.58) 
with f (s) = Π Γ = Ι U(x(sr), sr) as before. If the reversal of order of integration 
is valid, (7.58) is the (p + l)th term of series (7.56) and (7.57). 
7.21 
Interpretation of the Perturbation Series 
Starting from the series (7.57), Lemmas 41 and 42 can be used to facilitate a 
physical interpretation of each term (7.58) of the series, p — 0,1,2, — This 
section provides an outline of the physical interpretation. 
To begin with, consider the term p = 0. By Theorem 168, 
V>C,O(£,T;£',T') = *Φ (*£*£) 
The second and third terms, ψ0,ι{ζ,τ"Λ'·>?') and ^c,2(£jτ;ξ',τ'), are 
r si£C 
-c Γ lS1 
( [ 
U(xSl,Sl)Gc 
Λ βΐ€ΐ(]τ',τ]) 
\ . / R ] T V [ 
(W}) 
,(*l) 
(7.59) 
~ c ) 2 / 
( / 
( / 
^ - i ^ i ) ^ ( x e a , e 2 ) G £ CM) 
, ( * i ) 
,(82) 
and so on, for p = 3,4,5, 
In traditional notation, the second, third, and 
(p + l)th terms 
<Μ£,τ;ξ',τ'), 
^,2(ξ,τ;ξ',τ'), 
^,ρ(ί,τ;£',τ') 
are, respectively, 
: 
I / 
£/(x(3i),5i)dGcjd5i, 

374 
CHAPTER 7. BROWNIAN 
MOTION 
(~ C ) 2 f< ( / ' 2 ( / W n U^81^ 
8^υ(χ(8*)> 
s*)dGc) 
dsi) 
ds2, 
( ~ C ) P / " [Γ 
' ' ' \f> 
( / ir' π Π V(x(sr),sr)dGc\ 
dsA . · ·ώρ-ι J ώ Ρ . 
(7.60) 
(The integrator function GC(I[N]) in R^r ,Ti is indicated by the notation dGc.) 
Feynman and Hibbs [67] and Schulman [206] state that the case p = 0 describes 
the motion of a free particle with no interaction occurring. The upper left-hand 
Feynman diagram in Figure 7.1 describes the case p — 0. This is a straight line, 
without emission or absorption of an interaction particle. In Brownian motion 
it corresponds to motion of a particle in a vacuum, so there are no molecular 
impacts on the particle, and it maintains uniform motion in a straight line, 
without any "jagged path" features. 
For p = 1,2,3,..., consider the possible meaning and validity of each of the 
terms ^ ( ί , τ ι ξ ' , τ ' ) of (7.57). Write 
Sp = {«I, 52, · · · , Sp} , 
r' < Si < S2 < " ' < Sp < T. 
Given Sp G Λ/^τ',τ]), the function 
p 
J J U(x(sr), 
Sr) = U(XS1, 8i)U(xa2,S2) 
' ' ' 
U(xSp,Sp) 
r=l 
can be regarded both as a function defined for x(Sp) G R 5 p and as a cylinder 
function defined for x G R^T 'rL If it happens that this function is Gc-integrable 
on HSp (with respect to the finite-dimensional distribution function GC(/(5P))), 
then Theorem 159 implies that, as a cylinder function in R^r ,rf, it is also integ-
rable on Ή)τ ^ (with respect to the infinite-dimensional distribution function 
GC(I[N})). In that case, for 
Sp = (si,...,5 p) £)τ',τ]ρ, 
r' < si < · ·· < sp < r, 
the function 
h(Sp) = ft(si,...,5p) = / 
I TTi7(x(sr),5r) ) G!GC 
is well defined, and, whenever it exists, the existence and meaning of the terms 
(7.58) of series (7.56), (7.57), can be addressed, where the term 
ψ€,ρ{ζιΤ",ζ',τ') 
of (7.60) is now represented as 
(-c)p f (["'"(I2 
M*i> · · · ,*p)<ki) · •·ώΡ-ι) dsp. 
(7.61) 
Some initial comments can be made on the validity of some of the above steps. 
For instance, if U(y,t) (y G R, t €]τ',τ]) 
is a "well-behaved" function, such as 

7.21. INTERPRETATION 
OF THE PERTURBATION 
SERIES 
375 
a step function, then, for any given (si,..., sp), the function Π Γ = Ι U(xSriSr) 
is 
Gc-integrable on HSP. Theorem 159 then implies that, as a cylinder function in 
RjT ,rf, the function Π Γ = Ι U(xsrisr) 
is Gc-integrable on ~R)T 'r'; and this holds 
for each value of p. 
In the case p = 1, with si £ ]r', r] given, we have 
{Sl}eAf(}r',T}), 
J S l = / ( { S l } ) e I ( R ) = l(R{ S l>), 
J [ { S l } ] e l ( R ] ^ t ) . 
Theorem 54 (Fubini's theorem) then implies that /Κ]Τ',τ[ U(x3l, si)Gc(I[N]) 
is 
€I(R) 
, ( • 1 ) 
or, in traditional notation, 
/
CO 
-co 
and, with z(*i) el(]r',r]), ^ c , i ( ^ ; £ ' , 0 is 
/•si€t ( s i )* 
/ 
/* 
\ 
Λ(βι>€ΐ([τ',τ]) \./R 
/ 
(7.62) 
In traditional notation this says that ψ0,ι(ζ,τ",ζ')Τ~') is 
-c I 
( 
^c,o(Cϊτ;xβl,βl)l7(xβl,5l)^c,o(xβl,5l;ξ^r/)cteSl J dsi, 
which, by Theorem 168, equals 
- c / 
/ 
— 
/-t/(a:<,1,3i)—. 
-cfo8l 
ds t. 
Λ' \Λ-οο ^ ( τ - β ι ) 
y/^isi-r') 
) 
In Feynmand and Hibbs [67] and Schulman [206], this formula is interpreted as 
follows. 
■ The particle experiences no interaction from (ξ',τ') up to 
(x3l,si). 
■ At (x5l,5i) the particle is subjected to an interaction determined by U. 
■ The particle experiences no interaction from (xSl,si) up to (ξ,τ). 
■ This history is aggregated for all (x3l,si) 
G R x [τ',τ]. 
Corresponding to the term p = 1 in (7.57), the Feynman diagram is a short-
hand device to signify and bring to mind elements of this process. In Figure 

376 
CHAPTER 7. BROWNIAN 
MOTION 
7.1, the upper right-hand diagram shows emission/absorption of a single inter-
action particle (such as a photon), represented by a wavy line. Like molecular 
impact on a Brownian particle, the quantum mechanical emission/absorption is 
manifestation of an interaction "force". 
Now consider the term p = 2 in the perturbation expansion (7.57). Using 
the same argument as before, this third term ψ0,2(ζ,τ",ζ',τ') is 
/ _ x2 rs 2G2 ( s 2 )* 
/ r e i € t ( e i ) * 
/ rx°2eIs2 
/ pXs1€i;i 
y C) 
J2(s2)€l(]r',r])U2(«i)6l(]r',S2])U/S2Gl(R)U/SlGl(R) 
V>c,o(£,r;x8l,si)t/(xei, si)^c,0(aei, si;x82, s2)?7(xS2, 
s2)^c^{x82,s2;ξ',τ) 
I^|)|/ S 2|)K
( S 1 )I)K
( S 2 )|, 
(7.63) 
- 
Η 2 / ; ^ ( Γ Ο Ο ( Γ . 
^C)o(£, T; ar8l, si)?7(xSl, 8ι)ψΟι0(χ8ι, si; xS2, s2)U(x82,52)^c,o(^2, 
s2; ξ', τ') 
dx Sl)dx 
S2)dsi)ds2. 
In Feynman and Hibbs [67] and Schulman [206] this formula, with c = | = 
^γ^, 
is interpreted as describing possible histories of free movement except for 
interactions taking place at (xS2,S2) and (x8l,si), 
the possible histories being 
aggregated for all 
{x82,s2) e R x [τ',θι], 
(χβ1,5ι) G R x [τ',τ]. 
The corresponding Feynman diagram for p — 2 symbolizes this interaction pro-
cess. In Figure 7.1, the lower left-hand diagram shows two interactions. 
Comparing (7.63) with (7.62), observe that ψο^ί,τ',ξ',τ') 
is 
- c / 
I / ^c,0(^r;xS2,52)l7(xS2,52)^c,i(^2,S2;^,r,)|/S2M |*(s2)|. 
Thus, in traditional notation ψ€,2(ξ, τ\ζ',τ') 
ls 
ΨοΛο&τ\Χ82ΐ32)υ(χ82,82)ψοΑχ82ΐ32\ξ\τ')άχ82 
) ds2. 
Similarly, taking p = 3,4,..., the (p + l)th term ψβ,ρ(ζ, r ; £'>r') of the perturb-
ation expansion is, in the alternative notations, 
- c / 
( / V;c,o(^r;xSp,5p)i7(a:Sp,5p)^c,p-i(^p,5p;^/,r/)|/Sp|J |ι(βρ)|, 
(7.64) 
- c / ( / 
^c^{i,T\x8p,sp)U{x8p,sp)^c^-i{x8p,sp\^,r')dxs\ 
dsp. 

7.22. VALIDITY OF FEYNMAN 
DIAGRAMS 
377 
This provides a recursive method for deducing ψ€ιΡ from ^ C j P-i, where the 
initial term ψ0ιο is given by Theorem 168. 
Furthermore, the terms (7.64) of series (7.57) are interpreted in diagram-
matic terms (Feynman diagrams) which, for p = 1,2,3,..., describe a corr-
esponding sequence of physical interactions, to be aggregated over 
(x S l,x S 2,...,x S p,si,s 2,...,5 p) e RPX]T',S2} 
x [T',S3] x ··· χ]τ',β ρ_ι]χ]τ',τ]. 
In Feynman and Hibbs [67] and Schulman [206], (7.64) is combined with 
(7.57) to deduce an integral equation satisfied by ψ0, and this in turn is used 
to deduce or derive the diffusion equation/Schrödinger's equation. (In contrast, 
Theorem 221 verifies this equation by a direct method.) 
This is an outline of the theory of Feynman diagrams for a basic interaction. 
7.22 
Validity of Feynman Diagrams 
The graphical calculus described in Section 7.21 is used successfully in physics. 
But, while giving Kac [118] as a reference for the case c = — | , Feynman and 
Hibbs [67] and Schulman [206] do not give existence conditions for the Feynman 
path integral (c = \L). They also leave unresolved the issues mentioned above 
in (7.56), (7.57), and (7.58). 
This section examines the missing points of mathematical justification of 
these issues. To summarize, mathematical proof or justification are needed for 
the following reasons: 
■ to justify reversal of the order in which the calculations Σ^ο 
ano^ JRIT'.TI 
are carried out in (7.56), and 
■ having expressed (JJ, U(x(s), s)ds)P as f^, f*? · · · dsp-idsp, 
to justify re-
versal of the order in which the calculations 
I 
('-')dGc 
and 
/ 
/ 
. . . / 
(· · ·) dsi.. 
.dsp-\dsp 
imT'.T[ 
JT' JT' 
JT' 
are carried out in (7.58). 
If justification of these points can be supplied, the physical interpretation of the 
perturbation series, in the form of Feynman diagrams, has reliable mathematical 
foundation in this basic case. 
Theorem 219 establishes that ψ€(ξ, τ), = E^r [U~C(X)], exists if U is contin-
uous. To further establish the validity of the Feynman diagrams derived from 
the path integral, the issues involved in (7.56), (7.57), and (7.58) are examined 
in this section. 
First suppose U(y,t), = Urq((y,t), 
is a step function relative to space y and 
time t, with Urq and U~g
c defined by (7.47) and (7.29). Let U have the form 
given in (7.30) and (7.32), with binary sums calculated as in Section 7.19. Then 
XJ~q
c(x) — U~c{x) = U~q
c(x). (Other step functions U can be handled similarly.) 

378 
CHAPTER 7. BROWNIAN 
MOTION 
For any particular permutation k = k(if) we have xr<?lk = ί x^ ,..., x^r ), 
the "discretized" image of each x G K = K 
r q' k. Write 
r9|k(x) 
(7.65) 
i = i 
where Urq(y,t) is constant for y G if9'fc, 1 < fc < g29+1, and ί G 
1 < J < 2r, with frq\k(x) 
= 0 otherwise. For step functions Urq write 
Εξτ[Κ-*(Χ)]=ψ?(ξ,τ), 
so the expression J^^Lo Ψ1#(ζ, τ; ξ', τ') is the decomposition of Ψ19(ζ, τ) required 
in the Feynman diagram interpretation of V£9(£>r)· For (r, #)-step functions 
U(y,t), = Urq(y,t), this decomposition is established in the following result. 
Theorem 224 Suppose positive integers r and q are given, and a < 0, b > 0, 
c = a + ώ φ 0. IfU = Urq(y, t) is a step function (7.47), then 
oo 
Ε ξ τ [U-C{X)} = φβ(ξ,τ) 
= 5>c, P(£,r;£',T') 
where ψ0,ρ has the form given in (7.64). 
Proof. Using the notations of (7.30) and (7.32) and taking U~c(x) = \5~q
c{x) 
to be (7.65), the wave function (state function or marginal expectation density) 
is, in the various equivalent notations, the (r, g)-binary Riemann sum 
- 
/ 
φα(ξ,τ) 
= 
E C T [ U " C ( X ) ] 
u;g
c(x)dGc 
= 5^{<«'kQ^lk : kew-rq\ 
= 
] T {exp ( / ^ " ( ζ ) ) Qc9|k = k e r o " " } . 
For the given r, g, denote this by V£9(£>r)· The binary Riemann sum has a 
finite number of terms, and the existence of each ς£9' is ensured by Theorem 
203 provided a < 0 and 6 > 0 in c = a i tö / 0. Expand the exponential 
function as in (7.56), so 
u?lk = exp (/""><(*)) = Σ i=f- (/"lk(*))P , 
p=0 

7.22. VALIDITY OF FEYNMAN 
DIAGRAMS 
379 
and Εξτ \Urq
c(X)]—which, 
for binary step function Urg, can be denoted by 
ψζ9(ξ,τ)—can 
be expanded as 
Μξ,τ),= 
ψ^(ξ,τ), 
= 
Σ 
{exP (/ r 9 | k(*)) ?c9|k : k € «7"^} 
p=0 
since the collection of permutations w-rq 
is a finite set. Thus, since U(y,t) = 
Urq(y,t) 
is a step function, term by term integration of the power series ex-
pansion of the exponential function in (7.56) is valid. The next step is to check 
whether the reversal of the order of integration in (7.58) is valid when U, = Urqi 
is a step function. If it is, then 
Μξ,τ-,ξ',τ'^^Λξ,τ-,ξ',τ'), 
—that is, the marginal density of expectation of the step function observable 
U~q
c(X)—is representable as the series Y^^L0 Ψο,ρ{ζ,τ',ξ',τ') 
where each term 
ΨΖ&τ-,ξ',τ') = (ply1 Σ {(/r9|k(*)) V k : k € w'Tq} 
can be expressed in the form (7.58), in which the reversal of order of integration 
makes the Feynman diagram interpretation possible. To see that it is possible 
to reverse the order of integration for step function (7, take a fixed p > 0 and 
note that the finite sum 
£{(/r*|k(a:))V|k : k e ^ } , =ρ\ΨΖ(ξ,τ), 
is the integral with respect to Gc, in R T, of 
-cE^(^)(rjr)-rj:\) 
2 r 
-E^WW^i-0 
i = i 
For any one of the finite number of permutations k G zu-rq, the fact that U is 
a step function implies that 
( r " k ( * ) ) P = ( - c f > r ^ , 0 ( r j r M - i ) ) 
' 

380 
CHAPTER 7. BROWNIAN 
MOTION 
and Lemma 41 implies that, for each k, this can be decomposed into a finite 
sum of terms ]Γ\ α\ζ!, where each of the finite number of terms ofä in the 
decomposition has the form 
m=0 
This expression is, in turn, a step function in R T. Therefore 
&Vc
q
P{t,T) = 
Ρ\φΖ(ξ,τ;ξ',τ') 
- E{(E«ä?<£"k 
= E(EK )^ | k : k e w" r'})
 
(7·
66) 
i 
since the sums involved in the two calculations are finite. Thus, because the 
interaction function is a step function, the reversal of the order of summation 
translates into the required reversal of the order of integration. 
O 
When U is the step function (7.47), Theorem 224 establishes the validity 
(with a < 0, 6 > 0 , c = a + ώ ψ 0) of the mathematical expressions ^c,p(£> r)> 
= Ψα%(ζιτ)ι which, for p = 0,1,2,..., and after reversal (7.58) of the order of 
integration, are represented graphically by Feynman diagrams; and it establishes 
that Ε ξ τ [U-g
c(X)] = ψ?(ξ,τ) 
= Σ™=οΨο%(ξ>τ)- T h e r e s u l t c a n b e similarly 
proved for other step functions U. For non-step functions, the following holds. 
Theorem 225 Suppose T =]τ',τ[, c = a + tb, a < 0, b > 0, c φ 0, and suppose 
U(y,t) is BM-continuous. 
Then 
Μξ,τ) 
= Εξτ[υ-%Χ)}= 
hm 
( f > c l ( £ , r ) ) . 
Proof. Urq(y, t) is the step function approximation to U(y, t) defined by (7.29). 
By Theorem 222, ψ0(ξ,τ) = limg^^oo ΨΙς(ξ,τ). 
The result then follows from 
Theorem 224. 
' 
O 
This shows that, for any BM-continuous perturbation function Ϊ7, the cor-
responding state function value ψ0(ξ,τ) can, by choosing r and q large enough, 
be estimated to any degree of accuracy by the calculation which is represented 
graphically by Feynman diagrams. Theorem 225 does not establish that ψο(ζ, τ) 
equals 
Ρ' 
lim Σ^>(^;Μ, 
ρ=0 
nor that the latter limit exists. 
Numerical calculation of path integrals is intimated in Section 9.7. 

7.23. 
CONCLUSION 
381 
7-23 
Conclusion 
For both Brownian motion (c = —\) and quantum mechanics (c = | = ^τ^), 
the function E^T[U~C(X)], = ψο(ξιΐ~), contains information about the proper-
ties of a particle described by this function, including the state of motion of 
the particle in consequence of interaction with another particle or system of 
particles. 
In Brownian motion a particle of smoke or pollen is subjected to collision 
with molecules of air or water, producing the erratic motion which is represented 
in diagrams by "jagged paths". 
The particle interaction, or "force exerted", is represented mathematically by 
the function U(y,t). 
In quantum mechanics, interaction "force" is manifested 
by the emission or absorption of another particle or particles by the subject 
particle. Thus an electron (the subject particle) interacts with another electron 
by means of photon exchange, a photon itself being viewed as a particle—the 
interaction particle. 
Each term of the perturbation expansion (in a view "typical" of a particular 
path x, but aggregated in the path integral over all x) describes a single such 
event; though only a small number of these particle exchanges—photon ab-
sorption/emission—is mathematically or physically significant. Each integer p 
produces such a "view", visualized in a Feynman diagram of the interaction. 
There is a Feynman diagram for each p — 0,1,2, — 
Thus Brownian motion can be visualized as individual smoke particles or 
pollen particles bombarded by (and bombarding) air molecules, while electron 
phenomena can be thought of as individual electrons being bombarded by (or 
absorbing) photons, and emitting them. 
When interaction takes place a perturbation function U is introduced. Sup-
pose the integral is calculated, on Rj r 'Tl, of the function 
exp I -cY^U(xj-i,tj-i) 
(tj -tj-ι) 
J , 
with respect to the distribution function 
GC(J[N]) = ft {-At, -t3-,)Y 
I 
f[ewC{"J~y/~l)2dy(N). 
The result is the expected15 value E[U~C(X)]. Theorem 158 implies that this is 
equivalent to calculating the integral on R T of the function h(I[N]) defined as 
/ 
15If the integration is done on the space R j r 'Tt, the result is an expectation density 
Εξτ[Κ~°(Χ)], 
equal to the state function or wave function ψ(ζ,τ). 
With c = ^^-, 
a version 
of the expectation density is called probability amplitude [64, 67] in the physics literature. 

382 
CHAPTER 7. BROWNIAN 
MOTION 
The latter is not generally a distribution function. But if we take c = — | , and 
if h(I[N]) is compared with the construction (7.1) of Section 7.2, then (7.67) is 
the distribution function for Brownian motion with drift rate μ if it so happens 
that the function 
n 
S^(i/j-i»*i-i)(*i-*i-i) 
.7 = 
1 
is capable of being represented as 
for some μ. If these two are equal, then (7.67) is simply ο μ σ, with drift rate 
μ and variance rate σ2 = —2c-1, and, with c = — | , the ^/-interaction then 
converts standard Brownian motion into drifting Brownian motion. 

Chapter 8 
Stochastic Integration 
In Chapter 1 examples of observables expressed in elementary, basic, and con-
tingent forms are given. 
For instance, the contingent observable (1.4) has 
elementary form (1.5); and the joint-contingent observable (1.6) has elemen-
tary form (1.7). Theorem 82 of Chapter 5 examines conditions under which 
an elementary form Y ~ y[R, Fy] might be deduced from a contingent form 
f(X) 
^ f(x)[Qx,Fx], 
where Y and f(X) 
represent the same observable—so 
the datum satisfies y = f(x), and the sample space Ωγ is R. This is done by 
deducing a distribution function Fy from Fx. 
When we have knowledge of the distribution function of an observable, then, 
in advance of actually carrying the experiment or measurement we can, by 
performing particular calculations with the distribution function, acquire know-
ledge of the datum, such as its expected value or variance. Conversely, if we 
have repeated instances of the measurement obtained in nearly identical exper-
imental conditions, then we can use the experimental or sample data to estimate 
the distribution function under which the sample data were generated. This can 
be done, for instance, by calculating the relative frequencies of sample data. 
Likewise, if we know the functional form / of a contingent observable f(X) — 
/(χ)[Ωχ, Fx], we can sometimes deduce from this the distribution function FY 
for the elementary form Y = f(X) of the contingent observable. 
Much of the analysis of random variability consists of the search for the dis-
tribution functions which describe alternative elementary and contingent repre-
sentations of observables. At the beginning of the book, this point is illustrated 
in (1.4) and (1.5); and in (1.6) and (1.7). This chapter provides further illus-
tration. 
For example, if a contingent observable has the form of a stochastic integral, 
then Itö's formula, also known as Ito's lemma, can help to determine alternative 
forms of the observable, forms for which a distribution function can be deduced. 
Different experiments may have the same representation as observables. The-
orem 227 below shows that two different contingent observables Y\ = h\(X) and 
^2 = h2(X) defined on the same underlying joint-basic observable X may have 
the same distribution function, Fy1 = Fy2. 
A Modern Theory of Random Variation: With Applications 
in Stochastic Calculus, 
383 
Financial Mathematics, 
and Feynman Integration. 
First Edition. By Pat Muldowney 
Copyright © 2012 John Wiley & Sons, Inc. Published by John Wiley & Sons, Inc. 

384 
CHAPTER 8. STOCHASTIC 
INTEGRATION 
Suppose r £ R+ and suppose 0 < t < r. Let T, T denote ]0,r] and ]0,t], 
respectively. 
Given a joint-basic observable Χτ — XT [RT
5 F\, and contingent observables 
hi(Xr) 
and Η2(Χτ)^ we say that /ΐχ and /i2 are F-equivalent if the functions 
/i1(^r)F(7[7V]), 
/n(xT)F(/[iV]) 
are variationally equivalent in R T; and then we write 
Λι(Χτ) = Λι(*τ). 
With distribution function Gc defined by (6.17), take c = — \ and write 
G_h(I[N]) 
= G(I[N}) 
for cells I[iV]. Then Χχ — #τ [RT,G] is standard Brownian motion. Let 
hi(Xr) 
and h2{Xr) 
be joint-contingent G-observables. 
Theorem 226 If hi(Xq-) and h2{Xr) 
are G-equivalent random variables then 
Ε(Λι(Χ τ))=Ε(Λ 2(Χτ)). 
Proof. This follows from Theorem 44. 
O 
Alternative versions of contingent observables may have the same distrib-
ution function, and may therefore represent the same observable. A contingent 
observable can be represented in elementary form, as explained in Theorem 82; 
and can also be represented in alternative contingent versions, as explained in 
Theorem 83. For ease of reference this is restated here. 
Theorem 227 Suppose, for each t £ T, Ηι(Χχ) 
and h2{Xr) 
are real-valued, 
absolute G-random variables with h\(Xq-) 
= h2(Xj-). 
Then 
Ρχτο h^(J), 
Pjc-ofcjV) 
are distribution functions on the cells J £ I(R), and, for each such J, 
P x r o / i " 1 ( J ) = P X r o 
h^(J). 
Proof. 
The assumptions imply hi and h2 are G-measurable. 
Theorem 78 
implies that each of Ρ χ τ ο /ij~1(J) and Ρ χ τ ο h J ^ J ) is a distribution function 
on I(R). The result follows from Theorem 44. 
O 
The above results are valid not only when the underlying process (or joint-
basic observable) is Brownian motion X ~ x [RT, G], but also when the under-
lying process is any other joint-basic observable I ^ x [RT> Fx] · 
Sections 7.2 and 7.3 introduced drifting c-Brownian motion with drift rate 
μ and variance rate σ2, and geometric Brownian motion with growth rate p and 
volatility σ, using joint-basic representations 
XT ~ xT [RT, &"] , 
ZT~zT 
[RT, GP°\ ■ 

8.1. INTRODUCTION 
TO STOCHASTIC 
INTEGRALS 
385 
Traditionally, however, it is more common to use contingent observables in the 
form of stochastic integrals, in conjunction with Ito's formula, for this purpose. 
This chapter examines alternative representations using stochastic integrals 
in a Riemann sum setting. For simplicity it is assumed that all distribution 
functions F are real and non-negative. 
8.1 
Introduction to Stochastic Integrals 
Given T = ]0, t] and a joint-basic observable X ~ x [R r, G], a stochastic integ-
ral is an entity involving a form of integration on T which has traditionally been 
symbolized by formulations such as JT f(Xs)dXs. 
If the process Xq- is standard 
Brownian motion (so F = G), examples are 
/ XsdXs = \X2
t - \t, 
f (dXs)2 = t, 
/ X2
sdXs = \X* - f 
X8ds. 
JT 
JT 
JT 
JT 
(8.1) 
In the latter case, JTXsds 
is not considered to be a stochastic integral since 
it does not involve dXs. 
An important result in the theory is Ito's formula. 
This says that under certain conditions a contingent observable f(Xt,t) 
can be 
expressed as 
η^)^πο,ο)+Ιτ%ά8+ΙΙτ^α3+Ιτ§-άχ8. 
(8.2) 
The final term of the latter expression involves dXs, and is therefore regarded 
as a stochastic integral. 
To begin, we give some notation, and ways of interpreting the notation. If 
T is a real interval such as ]τ',τ] or ]0, oo[, denote cells of the interval T by 
ι G I(T). For any s,s' GT, S < sf, let is =]s, s']; and write 
X(2S) = X(s') - X{8), 
X(ls) 
= X(S') ~ X(S). 
Then the notation x(z) (i G I(T)) can be viewed in various ways: 
■ For any given is =]s, s'} € I(T), x(*s) is the value taken by a function 
/(#, N) = /(#, {5, s'}) defined for each x G R T, 
R T 
^ 
R, 
x 
-> 
f(x,{s,sf}) 
=x(s')-x(s) 
=x(is). 
Alternatively, 
■ For any given x G R T, x is a function defined for is =]s, s'} G I(T), 
I(T) 
^ 
R, 
is = ]s, s') 
-> 
f(x, {s, s'}) = x(s') - x(s) = x(ze). 
The next section gives some simple examples that are applicable to any process, 
Brownian or otherwise. 

386 
CHAPTER 8. STOCHASTIC 
INTEGRATION 
8.2 
Varieties of Stochastic Integral 
Suppose T =)τ',τ] 
and F x , = FxT, is a distribution defined on I(R T), so 
X ~ x [RT, FX\ is a joint-basic observable. Suppose 2, = is, = ]s, s'] G I(T) and 
/(#, {5, 5'}) = x(s') — x(s) = x.(is)> We then have a contingent joint observable 
f(X,{s,s'})~f(x,{s,s'})\RT,Fx}, 
or 
X(,e) ~ x(te) [RT,FX] 
. 
Suppose g is a function of the elements x(z) for 1 G I(T). For instance, p(x(z)) 
could be the function 
#(x(z)), = 0(x(z e)), = X ( l e ) 2 = ( φ ' ) - X(s)) 2 . 
If N = {ti,t2, · · · ?^n} £ Λ/"(Γ) with to = τ' and i n = r, we can1 write ij = 
]tj_i,£j]. Thus the cells {ij} form a partition of the domain T. For simplicity, 
let the symbol TV denote: 
■ partition points {tj}, or 
■ partition {ij}, or 
■ division {(s;/, }tj-i,tj])} 
(with division point s" = tj-\ G 1* or s" = tj G 
z*, j = l,2,...,n). 
The first of these is the primary meaning of TV, but for simplicity we allow it, in 
appropriate contexts, to also represent the corresponding partition or division 
of T. For the given function g and for some fixed x, the function % —>· g(x(i)) 
can be thought of as a function 
0:I(T)->R, 
Φ(ι) = g{x(t)). 
Thus, for any given #, g depends ultimately on cells % G I(T); and, since it is not 
necessarily additive on adjoining disjoint cells z, it is generally a Burkill- rather 
than Stieltjes-type function. Denote Riemann sums of φ(ί), = g(x(i)), by 
n 
n 
3 = 1 
3 = 1 
For this particular x, if φ(ι), = p(x(z)), is integrable in a Burkill sense, or in 
a Bur kill-complete sense, then this Riemann sum is an approximation to, or 
estimate of, the integral fTg(x(i)), 
— $Τφ(ι). 
This integral (if it exists) depends on the given element χτ\ and if it exists 
for variable χτ G R T the integral can be regarded as a contingent datum of a 
contingent observable. That is, writing 
f(x) = ί φ(ι) = I 5(x(*)), 
JT 
JT 
1There is some inconsistency in writing is =]s,s'] 
and ij =]tj-i,tj}. 
The first notation 
uses the left-hand end point of the cell to label the cell, while the second notation makes 
reference to the j of the right-hand end point tj. 
However, s generally belongs to the set 
T, while j is an integer used to label elements tj of T, so there is a distinction between the 
usages. 

8.2. VARIETIES 
OF STOCHASTIC 
INTEGRAL 
387 
f(X) 
is a contingent observable 
f{X) ~ f{x) [RT, FX] 
or 
/ g(X(i)) ~ / «,(x(t)) [RT, F*] . 
JT 
JT 
Because the integrator function is x(z), which is dependent on the stochastic 
process X, this is a stochastic integral. In due course2 stochastic integrals will 
be given dedicated notations s(X), 
S(X), and S(X) 
to replace the general 
purpose symbols / or f. 
In addition to dependence on the elements x(fcs), = x(s') — x(s), the function 
# can be allowed to depend on other parameters such as x(s), 5, and |zs|, = s' — s. 
In fact, what can be expected in a stochastic integrand g are elements s and xs, 
and differences s' — s and av ~Xs- I n order to make these elements explicit and 
visible, write the integrand as 
g(xa, s, x(z8), |zs|), 
or simply 
g(xa, s, x(zs), zs). 
As an integrand, g has the integrand form laid down in Definition 17, so the 
Henstock integral of g on domain T is well defined. 
In fact, for any given 
XT € R T, the integration of g on T is covered by Definition 6, so g is a Burkill-
complete integrand. If g happens to be an additive function of adjoining, disjoint 
cell z, it can be considered to be a Stieltjes-complete integrand. 
What about integrability of g on T? A Riemann sum for integrand g has 
the form 
n 
{N)^g{xs,s,y.{is),is), 
= 
^ ^ ( ^ . , , , ^ . ι , χ ^ ) , ^-|), 
n 
This formulation of integrand and Riemann sum is sufficient to cover those cases 
that generally arise in applications. Notable features of the usual stochastic 
integrands g are: 
■ absence of dependence on division points s" of the point-cell pairs (s", ij) 
of divisions of T; 
■ dependence on Xt _λ where the tj-\ 
are the left-hand end points of the 
partitioning cells ij. 
Chapter 7 features a contingent G-observable U~C(X), 
which also involves integration on 
domain T. The difference is that the integrator function in that case is |z|, as in ί_ · · · \ι\ or 
JT - · · dt, and not x(i) as in J 
- · · x(z) or f · · · dx(t)). 
To distinguish an observable-dependent 
integral JT · · · dt from stochastic integrals—which are a special category of observable, widely 
investigated in their own right—we can call it an observable integral. 

388 
CHAPTER 8. STOCHASTIC 
INTEGRATION 
In this chapter these features are demonstrated in evaluation of particular 
stochastic integrals. 
The presence of Xtu_1 as the evaluation point in the integrand g is intended 
to convey that the integrand depends on the value that x takes at the left-hand 
vertex of the cell ι =]tj-i,tj]. 
This is a form of functional dependence, not on 
the division point s" G z*, but on the cell i, with ι G s"*. 
In place of tj-i, some versions of the theory include dependence on 
, 
Xtj 
~ 
S t j - i 
**,-! + 
a 
· 
In fact a stochastic integrand can have the form g(xs,...), 
where s = s+X(sf — s) 
with 0 < λ < 1 fixed. In terms of Henstock integration, none of the commonly 
used variants involves dependence on division points s"', or on x(s"). But each 
of these variants can be handled by the method presented in this section. 
Using the formal notation in full, the integral is 
/ « 
g{x8,s,x(i8),ia). 
ei(T) 
This notation indicates, on the one hand, dependence of the integrand g on xs 
(with xs depending, in turn, on χτ and on the partitioning cell is =]s, s']); and, 
on the other hand, the fact that g does not depend on the division points s" 
used to form Riemann sums. 
The T-integrand g includes dependence on x G R T, which itself will be 
allowed to vary; so the T-integral of g (the integral of g on T) thereby becomes 
a function defined on R T. At that point the T-integral of g, if it exists, enters 
the theoretical framework of Henstock integrands of associated elements in a 
domain R T, as described in Chapter 4. In other words, if, for each x G R T, the 
T-integral of g is multiplied by a distribution or weighting function i*x, then 
the Τχ-mean in R T of the T-integral of g, 
L(l gjFx(I[N\), 
if it exists, is defined by Definition 17. 
For some T-integrands g, and for certain classes of elements x, fT g may not 
exist. This means that the T-Riemann sums (Riemann sums on T) of g do not 
converge. 
Nonetheless, it is always possible to form Riemann sums 1lg
T{x,N) of the 
T-integrand g. And, for certain x^-dependent T-integrands g, it is possible that 
the following integral exists: 
/ 
U9
T(x,N)Fx(I[N]). 
That is, the Fx-mean in R T of such (iV-dependent) T-Riemann sums Ί19
τ(χ, Ν) 
may exist, in accordance with Definition 17 of the Henstock integral in R T. In 

8.2. VARIETIES OF STOCHASTIC 
INTEGRAL 
389 
that case, TZ^(x,N) is Fx-observable as a Riemann-sum-type joint function of 
χτ G R T and partitions N G λί(Τ). 
This makes it possible to investigate the 
Fx-random variability of such Riemann sum functions even when, for particular 
elements χτ G R T, these Riemann sums do not converge to an integral on T. 
In other words, the methods of this book make it possible to actually avoid 
altogether the issue of integrability of g on T. The idea of random variability, 
in sample space R T, of the integral on T of g, can be abandoned in favor of 
random variability, in sample space R T, of the Riemann sum function 1Zj,(x, N) 
which depends jointly on χτ and N. This may be the best way of analyzing 
the Fx-random variability of integral-like functions "fT g" of x G R T when the 
functions g are not actually integrable on T. 
However, the approach of Section 8.4 and subsequent sections of this chapter 
is closer to the traditional (and perhaps more difficult) theory of stochastic int-
egration, involving weak integrability of χτ-dependent functions g on T. While 
this is not really satisfactory, it side-steps a more radical departure from the 
traditional theory of stochastic integrals. Such a departure would be contrary 
to the purposes of this book. 
We then have three distinct forms of the integral on T of g, as follows. 
■ If g depends on x but not on x(z), then, as x varies, denote the integral 
of g by &T(X). In other words, letting x G R T vary in R T, the integral on 
T of the integrand g(xs,s,is) 
can be regarded as a function of x G R T. 
This interpretation is prompted by introducing the notation 
ST(X) 
= / g{xs,s,is); 
(8.3) 
which, writing φ(ι) = g(xs, s,zs), is simply3 the integral fT φ{ί) of the cell 
function φ on T. 
■ If g depends on x(z), then, as x varies, denote the integral of g by 
S^(x), 
S9
T(x)= 
/ g(xs,s,x(i8),ia). 
(8.4) 
We call this the strong stochastic integral. The traditional theory of 
stochastic integration does not include this concept. 
■ If g depends on x(z), and if the integral of g on T exists only in some weak 
sense as adverted to above, then, for variable x G R T, denote the weak 
integral of g by «Sf.(#), 
Sg
T(x) = " j 
ng(xa,8,x(is),ia), 
(8.5) 
whose exact meaning will be established in the next section. This corr-
esponds to the traditional idea of the stochastic integral. 
3The integrand U(x(t),t)\i\ 
of Chapter 7 has this form; and I U{x(t),t)dt 
is termed an 
observable integral, as distinct from stochastic 
integral 

390 
CHAPTER 8. STOCHASTIC 
INTEGRATION 
With T = ]0, t] and is = ]s, s'] G I(T), an example of a stochastic integrand g is 
g(xs,s,x.(is),is) 
= s3|zs| + ζ ^ β | + x(*) + z5x(zs). 
(8.6) 
The final two terms of the right-hand expression are the ones that make the 
function g a stochastic integrand, since they contain the factor x(z). 
Consider each of the four right hand terms of (8.6) separately. The first term 
contains no random element; and it is integrable on T = ]0, t] with integral equal 
to \tA. The second term has random component xs\ and it is integrable on T 
if, for instance, X is a Brownian motion. (If X is Brownian then x(s) can be 
assumed to be a continuous function of s). The third and fourth terms include a 
random increment x(z), and therefore, if they are integrable in some sense, their 
integrals will be called stochastic integrals. We will see that the third term is 
Henstock integrable (in the Stieltjes-complete sense) on T for all cc, with integral 
equal to x(t). In Section 8.4 it will be shown that, if the process X happens to 
be standard Brownian motion, the fourth and final term is integrable in a weak 
sense. 
Taking the domain T to be T =]0,i], denote the Riemann sums of an int-
egrand g on T by 
U9
r{x, TV), = (N) J2 9(xs, e, x(*),»). 
If, for each x G R T, the function g(xs,s,x.(i),i) 
is integrable on T, denote the 
integral by 
S W , = //(x.,s,x(,),„. 
Allowing x to vary in R T, then 
n9
r : R T x Λ/*(Τ) H> R or C, 
S^- : R r H-> R or C 
are functions defined on R7". Therefore, for each t, 
S9
r(X)~Sa
T(x)[RT,Fx} 
is a joint-contingent observable with respect to the joint-basic observable 
X~x[Rr,Fx}. 
Similarly, for any given N, Tlg
T{X, N) ~ 1lg
T{x, N) [R r, Fx] is a contingent 
observable. 
The symbols f, s, S, and *S are each versions of the letter S, indicating 
summation. The first three symbols denote Henstock integrals, but "s" and 
"S" have been introduced because "/" does not usually carry the connotation 
of a function "/(«)" of a variable x. The symbols "s(ar)w, "π(χ,Ν)η, 
"S(x)", 
and "S(x)n are intended to convey such a connotation. The symbol "11" further 
indicates "Riemann sum", which, in addition to dependence on variable x, is 
also dependent on variable N. For an integrand g that does not depend on x(z), 
the expression sg
r{x) denotes JTg. 

8.3. STRONG STOCHASTIC 
INTEGRAL 
391 
8.3 
Strong Stochastic Integral 
Definition 55 IfX ~ x [RT, Fx], and ifSg
T(x) exists for each x € R r (except 
perhaps for an Fx-null set), then S^(X), = JTg(xs,s1x(i),t), 
is the strong 
stochastic 
integral of g(Xs, s, X(z), 2). 
Existence of the strong stochastic integral depends on the point-wise conver-
gence of the Riemann sums 7Zj-(x,N), = (N) £] #(:rs,x(z),z), to the stochastic 
integral S?j-(x) for each x G R ' (except perhaps for a null set of x). When this 
fails we may appeal to a weaker form of convergence, to be introduced in Section 
8.4, which is similar to the more traditional approach to stochastic integrals, as 
presented, for instance, in Chung and Williams [42], Karatzas and Shreve [120], 
or 0ksendal [190]. 
Consider the function g of (8.6): 
g(xs,s,x(is),is) 
= s3\is\ + x2\is\ + x ( 0 + xsx(2s). 
Write 
hi=s3\is\, 
h2=x2
s\is\, 
gi=x(i), 
g4 = 
xsx(is)-
In the course of this chapter it will be shown that each of the terms of g is 
integrable in a different way: 
i : : ^ . _ 
/ g1 = S^ (x) = / dxs = xt — xo for each x, 
I 94 = S9
r*(X) = f XsdXs = \X2 - \t. 
The latter integral is not valid for individual paths x, but is valid only in some 
weak, aggregate sense, or "in G-mean" when X is Brownian motion. Thus the 
function g has weak stochastic integral 
J g(Xs,s,X(is),t.) = Sf (X) = i*4 + j X2
sds + Xt + \X\ - \t. 
The meaning and validity of results such as this one will be established. Here 
is a further list of stochastic integrands: 
9i(x(is)) 
= x(^), 
#2(x(*s)) = 
x(*s)2, 
#3(x(*s)) = 
x(^) 3, 
g^(xs^M) 
= 
xs*M, 
g5(s,x(is)) 
= sx(zs), 
g6(x3^M) 
= xs*M2, 
(8.7) 
g7(xs,xM) 
= ^x(z), 
gs(xs,Xs') = x^-xl, 
g$(xs,Xs>) 
= 
χζ,-χξ· 

392 
CHAPTER 8. STOCHASTIC 
INTEGRATION 
With is =]s, sf] G I(T), these can be written as follows: 
9i(*M) 
= Xs>-x8, 
92{xM) 
= 
( a v - x s ) 2 , 
<fe(x W ) = (av - z s) 3 , 
#4(zs, x(«e)) = z s (av - xa), 
£ 5(s,x(z s)) 
= 
5 ( χ β / - Χ θ ) , 
g6(xs,ls) 
= 
XaiXs'-Xs)2, 
g7{xs,yL{is)) 
= x2
s(xs>-xs), 
gs(xs,xs
f) 
= x*8> - x\, 
g${xs,xs>) 
= xP
s>-xVs· 
In the course of this chapter integrals on T will be found for each of these 
integrands. In the traditional accounts of the subject #2? #3, and g± are found to 
be weakly integrable, while g\ is not integrable. Using non-absolute integration 
it will be shown that g<z, #3, and g± are weakly integrable, but g\ is strongly 
integrable for all x G R T, regardless of which distribution function FxT operates 
on the data #7-; and #5 is strongly integrable if X is Brownian. 
At first sight #8 and g$ do not appear to depend on the increment x(zs), so 
how can they be stochastic integrands if the definition of the latter is strictly 
adhered to? But 
a factor in both, so g$ can be written 
#8(xs,av,x(zs)) =x(z e) (x2
sr +xs>xs + x2
s) , 
with similar factorization for #9 provided p is any positive integer. 
The integrand #4 is an example of a class of stochastic integrands of the 
form h(x8) (xs
f — xs) in which h(xs) = h(x(s)) is the value that the composite 
function hox takes at s, the left-hand vertex of the cell % = ]s, s'\. Thus, for this 
integrand, h o x is not evaluated at a division point s" G z*; and such functions 
(including #4) depend only on the cells % = is and not on any associated division 
point. 
Example 59 IfT = ]0,£], gi(x(i)) = x(fc), andN = {^1,^2» · · · >*η}> with to = 0 
and £n = t, i/ien 
n 
K9J(x,N) = (AT) 5^x(z) = ] T (xt. - £*._,) = xt - x0 = xt. 
77ws can 6e expressed in terms of δ-gauges. Given ε > 0, a gauge S(s) > 0 (s G 
[0,t]) can be chosen so that, for every δ-fine division {(s, )tj-\,tj]} 
(s = ij_i 
or s = tj), with \ij\ = tj — tj-ι < 5(s), the Riemann sum satisfies 
[R%(x,N)-xt\ 
= 0 < ε . 
In fact this condition is satisfied for every gauge S(s). 
While it is possible to 
make the partition N conform to the gauge values S(s) at tag-points or division 
points s" (so, viewed as a division, N is δ-fine), the integrand itself does not 
depend on the tag-points s". Also the inequality is satisfied regardless of whether 

8.3. STRONG STOCHASTIC 
INTEGRAL 
393 
the partition N—and any corresponding division—is δ-fine. Therefore, for each 
x £ R T, the function x(i) is integrable on T with 
S«i(rr) = J 
x(t)=xt; 
and 
n9j(x,N) = xt, 
sg^(x) = xt. 
That is, the strong stochastic integral of the contingent joint observable 
X(z)~x(z)[Rr,Fx] 
exists4 and equals Xt. In traditional notation, 
/ dXs = Xt. 
This is valid for any joint distribution function Fx. 
O 
Any observable Y ~ y[Cl, Fy] has two basic components: 
■ the datum y, and 
■ the distribution function Fy. 
The sample datum is the "carrier" of information about the distribution func-
tion, in the sense that, if the characteristics of the data are understood, it may 
be possible to deduce a characterization of the distribution function. For in-
stance, if the experiment can be repeated many times without change to the 
circumstances in which it is conducted, then, as in Tables 1.1 and 1.4 of Chap-
ter 1, by counting the relative frequencies of the data values y we can estimate 
the values taken by the distribution function Fy. 
On the other hand, when 
the distribution function for the experiment is known, then key features of the 
datum—such as its expected value, variance, and so on—can be established in 
advance of actually conducting the experiment. 
Example 59 demonstrates that, with g(x3,s,x.(i),i) 
= #i(x(z)) = x(z), the 
evaluation 
ST(X)= 
/ 0 i ( x M ) = x W 
(or / 
dx8=xt) 
provides alternative perspectives on the resulting observable. 
4The Lebesgue integral method cannot be used to define the stochastic integral S ^ ( X ) , 
or J 
dXs. 
For instance, if X is a standard Brownian motion then, for any given x G R T 
outside a null set, x has unbounded total variation. In other words the function x(i) defined 
on I(T) has unbounded variation and does not generate a measure of the Lebesgue kind on 
the measurable subsets of the real interval T. It is straightforward to prove that Brownian 
paths have unbounded variation in every interval j G I(T). 
But a digression into this topic 
is unnecessary since, as Example 59 shows, unbounded variation of x(z) does not prevent 
definition and evaluation of the Stieltjes-complete integral of x(i). 

394 
CHAPTER 8. STOCHASTIC 
INTEGRATION 
■ The contingent observable 
s£po,=/ $(*.,*, x(»).*).=y j7i(x(»)),=y xw, 
can be regarded as 
S£(Xr)-S^(zr)[Rr,^r]> 
where, for AT = {ίι,... ,f„-i,i„ = ί} € 1(7"), 
FXr(/[7V]) = FXt(7t), 
with 
X t ~ x ( [ R , F x J . 
■ Writing y = xt = S^{xy), 
S^{Xf) 
has elementary-form representation 
Y ~ j/[R, F y], where, for J = It e I(R), 
F y(J) = FXr(J[7V]). 
Thus the stochastic integral of the increment of a process simply reproduces or 
replicates the process itself. 
Given a stochastic integrand g(xs,s,x(z),z), if y = S^-(x) can be evaluated 
for any joint datum x £ R r , it is sometimes possible to deduce the distribution 
function Fy of the observable Y = S?r(X). Example 59 is a simple illustration 
of this point. 
Itö's formula is sometimes used as a means of deducing such distribution 
functions. Theorems 82 and 227 are often the basis for such a deduction. 
The following result gives further examples of strong stochastic integrals, in 
which the deduction of the distribution function of the contingent observable 
(stochastic integral) is straightforward. 
Theorem 228 Suppose μ and σ are non-zero real or complex constants, and 
T = %t]· If 
g(x(i),i) = 
μ\ι\+σχ(ί) 
then g is integrable on T with 
g,= 
s
g
T(x), = ßt + axt. 
For any real- or complex-valued distribution function Fx, the contingent observ-
able Sg
T{X) ~ S9
r(x) [ R r , F x ] satisfies 
S9
r(X)=ßt 
+ aXt. 
Proof. The proof is similar to that given in Example 59. By cancellation, each 
Riemann sum Σ?=ι ax(lj) 
n a s ^ n e v a m e °~xt, so 
S9
r(x) = / μ\ι\+ 
/ σχ(ζ) = 
μάβ + 
adxs = μί + axu 

8.3. STRONG STOCHASTIC INTEGRAL 
395 
and the corresponding contingent observable is 
S9
T(X) ~ S9
r(x) [RT, Fx] =μί + aXt. 
In traditional notation this is 
I 
(μ ds -f σ dXs) = μί + oXt· 
r 
Note that the first term, fTßdt, 
is not itself a stochastic integral since its 
calculation does not involve the cell function x(z). 
O 
Because this calculation is widely used we give it the particular designation 
Sf a(X). The point-wise evaluation of the integral implies that the likelihood 
distribution function of the contingent observable S^a(X) 
is, for It =]u,v] G 
I(R), 
C[(S^(X)eIt))=FXt(Jt), 
where Jt ^ησ'1 
- /it, νσ~ι - μί]. 
Again, evaluation of a stochastic integral as a contingent observable provides 
information about the distribution function of the stochastic integral. 
Example 60 Suppose 0 = To < T\ < · · · < r m = t. If the real- or complex-
valued functions μ($) and a(s) are step functions with constant values on cells 
JTJ-IJTJ], then the stochastic integral Sj?(X) 
is 
„
m
m 
3?{X) 
= / (μ|ι| + aX(t)) = £ μ, (TJ - Tj-x) + £ ^ (*fo) ~ ^ f o - i ) ) ■ 
JT 
j=1 
j=1 
(8.8) 
This is because, within each Tj 
=]TJ~I,TJ], 
/ 
(μ\ι\ + σχ(ζ)) = μό (τό - τ,-ι) + σύ {χ(τά) - xfo-i)), 
using the cancellation argument. 
In this case the strong stochastic integral 
S^(X) 
is a contingent observable 
St?(XT)^SF(xr)[KT,FXT} 
which depends jointly on the n elementary, basic observables XTl, XTl,..., 
ΧΤτι, 
and does not depend on the outcome of Xt for t φ Tj, j = 1,2,..., n. Writing 
y = S^(xq-), 
and provided Fy is well defined, this contingent observable has 
elementary form 
Y~y[R,FY}. 
The evaluation shows that, for J G I(R), the value Fy(J) of the distribution 
function for the stochastic integral can be calculated from values of Fxr., j = 
1,2,..., n. To see this, write (for brevity) 
f{x) = S £ » , 
(x e TT). 

396 
CHAPTER 8. STOCHASTIC 
INTEGRATION 
Then, for J € I(R), we have / _ 1 ( ^ ) = I € I ( R r ) , and 
FY{J) 
= 
Px[(8?(x)eJ)] 
= 
Px[(f(x)eJ)\ 
= PxorHJ) 
= 
FXTIXT2...XTJI). 
Since the integral fTß(s)\i\ 
in (8.8) is not a stochastic integral, it is possible to 
extend this argument, allowing the function μ{β) to be continuous or bounded 
Borel measurable. But in order to take a similar course with the function a(s) 
different kind of argument, involving the theory of -complete or non-absolutely 
convergent integrals, must be undertaken. This is because the stochastic integr-
ator function x(z) is oscillatory. 
O 
Theorem 229 Suppose Xr — %r[frT> G] is Brownian motion. With is = ]s, s'} 
the function 
#5(s,x(zs)) = s(av 
-xs) 
is strong-stochastic integrable; and 
S?f(X) = / Xs\is\ 
(or 
ί sdXs = f 
Xsds). 
J I 
J i 
JI 
Proof. Since X is Brownian the sample path χ-γ — (x s) s e T can be taken to be 
a uniformly continuous function of s G T. Then fTx3\is\, 
= fj.x(s)ds, 
exists 
for each such x, and, writing 
s(xr) 
= / 
XsM, 
the contingent observable (or observable integral) 
s(Xr)^s(xT)[B-T,G] 
is well defined. Let 
N = {Si, S2, · · · , 5 n_i, Sn} , 
0 = S0 < 5i < S2 < ' ' ' < Sn_i < Sn = t, 
be a partition of T. Compare Riemann sums 
a = a(7V), 
β = β(Ν) 
for, respectively, JTxsds 
and fTsdxs. 
Write Xj = X(SJ) and 
a 
= 
#i(si - s0) + %2(s2 - si) + z 3(s 3 - s2)-\ 
h xn(sn - 
sn-i) 
= 
-S0Xi + Si(xi - X2) + S2{X2 - X3J H 
h Sn_i(xn_i - Xn) -f SnXn 
/? = 
s0(a:i - xo) + si(x2 - ^1) + «2(^3 - £2) H 
l· 
s n_i(x n - x n_i). 
Then 
/3 = a —sn#n, = a — txt, 

8.3. STRONG STOCHASTIC 
INTEGRAL 
397 
and, given ε > 0, a gauge 6(s) > 0 can be chosen so that, for every 5-fine 
partition 7V of T, 
< e. 
\ß - [ 
xs\is\ -txt)\ 
< \ß - (a- txt)\ + a - / x3[i 
Since this holds for all continuous xq- the result follows. 
O 
The function S(XT), 
= fq-xs\is\, 
is G-measurable in R T, since for each 
continuous x it is expressible as a limit of (r, g)-binary Riemann sums, each of 
which is a step function in R T. Also, G is continuous in the sense of Theorem 
78. With 
y = s(xr) = / xs\zs\ 
for x G R T, 
by Theorem 78 the distribution function 
FY*(J) = PxoS-1(J) 
(8.9) 
is well defined; so the observable Ys ~ y[R, Fys] is well defined. This establishes 
the existence of an elementary version of the contingent observable s(X), = 
JTXsds, 
where X is standard Brownian motion. 
The following example contrasts stochastic integrals with the observable int-
egral function U~C(XT) 
of Section 7.15. 
Example 61 Suppose T =]0,τ], Τ =]0,i] forteT;c 
= a + ώ with a < 0, 
b > 0, c φ 0; FXT(I[N}) 
= GC(I[N}); μ ( φ ) , β ) = ^(χ,,β) = ^ ( φ ) , β ) . 
5Ίίρρο5β 
/ ( ί , χ ) = / 
U(x(s),s)ds 
is an observable integral, where U is the function described in (7.11). This is not 
a stochastic integral since it does not involve integration with respect to process 
increments x(z). While it does not have JT · · · x(z), = fT · · · dx, nonetheless it 
depends on the random joint occurrences (x(s)), and is therefore not a deter-
ministic function. Let g be the function 
a —> exp(—ca) = (expa)~c = (expc) _ a 
where a is a real or complex number. For each t £ T take 
ht(xr) 
=g(f(t,xT))· 
Then 
hT(xT) = 9 ((/(*, χτ))ΙΕτ) 
= 
^~°(χτ) 
specifies a Gc-observable, U~C(XT), 
whose properties are investigated in Chap-
ter 7. See (7.11) and consequent results. Though not itself a drifting Brownian 
motion, the observable U~C(XT) 
is a process that can be described as "the recip-
rocal of the cth power of the exponential of a drifting Brownian motion which 

398 
CHAPTER 8. STOCHASTIC 
INTEGRATION 
has drift rate U(x(s),s) 
and variance rate 1". Theorem 222 gives conditions 
for U~C(XT) 
to be Gc-random variable. Section 7.23 shows how an interac-
tion function U(x(s),s) 
can convert Brownian motion into drifting Brownian 
motion. 
O 
The next section examines stochastic integrals that are not point-wise con-
vergent. 
8.4 
Weak Stochastic Integral 
A contingent observable f{Xr) 
has an outcome or datum f{xr) 
which may-
depend on each potential outcome xs of the family of basic elementary obs-
ervables (Xs)seT. 
This entails the task of deducing, from the collection of 
distribution functions Fxs (s G T)—or from the joint distribution function 
FxT—the distribution function Fy of the elementary observable Y ~ y\R, Fy] 
where y = 
f(xr)-
The kinds of contingent observables f(Xr) 
to be considered here are those 
involving some kind of limit of Riemann sums5 whose terms contain Stieltjes 
or incremental factors X(z). Various examples in Section 8.3 demonstrate that 
point-wise (or path-wise) convergence of these Riemann sums can be very helpful 
in establishing corresponding distribution functions Fy. In particular, Section 
8.3 deals with JTdXs 
(or fT~K(i)) 
and related strong stochastic integrals. 
This section considers the stochastic integral J r X(z)2, or J r dX2, which has 
sample path counterpart fTx(i)2. 
Unlike JTx(z), this does not have a simple 
evaluation. 
The integrand g2(x(i)) = x(^)2 is not additive on disjoint, adjoining cells i. 
Therefore it is a Burkill-type integrand rather than Stieltjes-type. Is g<i Burkill 
integrable, or Burkill-complete integrable, on T? Since this integrand is different 
for different x, perhaps it is Burkill integrable, or Burkill-complete integrable, 
for some x but not for others? 
For any given xj- G R7", take partition points N G J V ( T ) , SO a Riemann 
sum for JTx(z)2 has the form 
πψ(χ,Ν) 
= (JV) ] ζ χ ( ζ ) 2 = (Xl - x0)2 + (x2 - Xlf 
+ · · · + {xn - x n_i) 2 . 
Each term of this Riemann sum is non-negative, so the terms do not cancel or 
"telescope" in the manner of 7?J^ (x, N) in Example 59. 
But perhaps the Riemann sums can converge in a way that does not involve 
cancellation of terms, or even in some other less direct way? Suppose £ is a 
5 Such a Riemann sum involves a datum χη- and partition points N C T, so it has the form 
f(xr,N) 
with variable N. As outlined in Section 8.3, f{XT,N) 
~ f(xr,N)[Rr 
x 
N,FX] 
can be treated as an observable, regardless of whether there is any kind of Riemann sum 
convergence in which the variable N disappears in the limit. Section A.l gives an indication 
of how this approach can be worked out in more detail. But substantial development of the 
theory in this direction is outside the scope of this book. 

8.4. WEAK STOCHASTIC 
INTEGRAL 
399 
datum of standard Brownian motion X ~ x [R r, G]. If the sample path (or 
potential joint datum) x = (x(s) : s G T) is everywhere differentiable6 with 
derivatives bounded by «, then, choosing any partition {SJ} of T such that 
Sj — Sj-i < ε, and using the mean value theorem, the Riemann sum satisfies 
yjx(z) 2 < « y j (SJ — Sj-i) < ^max{«Sj — Sj_i} / J ( s j — Sj-ι) < net. 
3 
The terms κ and £ are fixed, so for this sample path the Riemann sums for 
JT (x.(i)) converge to 0. Therefore, for such xq-, the Burkill-complete integral 
of x(^)2 exists and equals zero. 
But the value of JT dX^ is usually7 given as £, not 0. So what is the meaning 
of the traditional result JTdX^ = t (or J r X(z)2 = t)l The following example 
shows that there are x G R7" for which the Riemann sums ^ x ( z ) 2 diverge. 
Example 62 Let S = (SJ) be an increasing sequence of rational numbers in T, 
si < S2 < S3 < · - -. Take so = 0. Consider x G R T with 
, , 
( l 
if 
teS, 
\ 0 
if 
teT\S. 
Let k be a positive integer, and choose Sj E]SJ-I,SJ[ 
for j = 1, 2,... k. Let 
N = 
{s1,...,sk}U{sf
1,...,s'k}. 
Then 
(A05>W)2 = 2*· 
Therefore the function x^) 2 is not integrable on T. 
O 
Thus the evaluation of J rx(z) 2 for different sample data xq- may yield various 
different results; and for some paths x the integral J rx(z) 2 does not exist. 
With g2{x) = xW 2 we can define a function ΊΖψ(χ,Ν) 
on R r x λί(Τ) 
as 
a Riemann sum (N) l^x(^)2· But since the Riemann sums do not converge in 
any obvious way, it is not clear that the strong stochastic integral approach 
can be used to define a function S^ (x) as a datum of a contingent observable 
S?j2(X) = J7-X(z)2 (i.e., frdXg 
in the traditional notation). 
Also, unlike the evaluation fT~K(i) 
= Xt (valid for all x), the usual value 
given for fT~K(i)2 
(or JTdXg) 
is f, which is not even a random variable— 
though it can be thought of as a degenerate contingent random variable 
f(X)~ 
t [Rj Fx] with Fx an arbitrary distribution function. 
It is known that, if the joint-basic observable X ~ x lüT, G\ is standard Brownian 
motion, the subset of potential joint data 1 7 , each of which is differentiable at some s 6 T, 
constitutes a G-null set in H7', so this integration of functions of such x is not in itself 
significant. 
7See, e.g., 0ksendal [190]. 

400 
CHAPTER 8. STOCHASTIC 
INTEGRATION 
So how can the stochastic integral of the integrand-observable X(fl)2 be de-
fined? 
In contrast to the approach taken in Section 8.3, the standard or traditional 
method (see [190], for instance) defines the stochastic integral of certain inte-
grands h(Xs, X(z)) as a form of weak limit. We take a similar approach in the 
case of integrand X(z)2. 
Section 8.3 dealt with observables in the form of Riemann sums on T. This 
section takes a broadly similar approach, but, for simplicity, and to keep in step 
with the traditional approach to stochastic integration, special partitions of T 
are used. For T = ]0, £], r = 1, 2,3,... and j = 1,2,... 2r, form binary partitions 
of T as follows. Let 
r = 1 U ~ 1)* 
Jt] 
j 
2r 
' 
2 r I 
so for any given x G 
*(ir)2 = 
{x{j2-rt)-x{{j-l)2-rt))\ 
which we denote by (^(x^J))· Let Mr G λί(Τ) denote: 
■ partition points {j2~r£}, 
■ partition {z^}, and, when appropriate, 
■ division {(s",i^)} 
(with s" G zj*, each j). 
For each integer r construct an observable 
fr(Xr)^fr(xr)[RT,G] 
as follows. Form the binary-Riemann-sum-of-squared-Brownian-increments 
ob-
servable fr(Xj-) 
= ΊΖτ{Χη-) from the integrand-on-T observable X(zJ)2 (or 
ifc(X(t;))): 
2 r 
2 r 
7 ^ X T ) , = 7^ 2(X,M r), = £<fc(X(tJ)), = $ > ^ ) 2 . 
(8.10) 
J = l 
.7 = 1 
Then a weak limit, as r —> oo, of the observable 7?^'r(X) will be designated as 
the (weak) stochastic integral of X(zp2. The following results make it possible 
to define a weak limit that is suitable for this purpose. For given r and j let 
fjr (xr) = 92 (*(,;)) = (* (£) - x ( i ^ ) ) 2 · 
Theorem 230 For Brownian motion Xj- ~ x<j-[HT, G], 
f 
fjr (xT) G(I[N}) = VfjrG [R r] = 2~rt. 

8.4. WEAK STOCHASTIC 
INTEGRAL 
401 
Proof. For brevity write this result as 
_x^)2G(7[iV]) = V , 2 G [ R r ] = ^ 
ϋτσ 
Note that, in the integration on R T, the element i7^ is fixed. Theorem 180 gives 
JRT fjr (xr) G(I[N]) = 2~rt. Theorem 39 gives 
L 
fjr(xr)G(I[N)) 
= 
VfjrG[Rr} 
R7" 
since the integrand is non-negative. 
O 
For any zrj let hjr(xr) 
= fjr(xr) 
~ |«J| = *(^) 2 - 1^1· 
Theorem 231 As r -> oo, hjr(xr) 
-> 0. 
Proof. This result states that (g2(x(ir
j)) - \irj\) 4 0, or x ^ ) 2 - |zj| 4 0. To 
prove it we must show that V^.rG?[Rr] -> 0 as r -> oo. Theorem 230 implies the 
function hjr(xq-) is G-integrable on R r , with integral 0, and hence, by Theorem 
76, hjr(xj-) is G-measurable. Let η > 0 be given, and let 
Sv>1 
= 
{χ:χ($)*-\%ϊ\>η}, 
Sn,2 
= 
{a::|x(t$) 2-|*5l| 
<η}, 
5„,3 
= 
{* : x ^ ) 2 - \irj\ <-η} 
= {χ:0< 
x(*p2 < - η + |ij|} . 
The sets SVik depend on r for k = 1,2,3. The set 6^3 is empty if 77 > |zj|. Thus 
$η,ι U 5^,2 U 5^,3 = R7"; and lsVfk(xT)hjr(%r) 
1S G-integrable for k = 1,2,3. 
By Theorem 230, 
/ 1sv>3(xr)hjr(xr) = - / lSr]>1(xr)hjr(xr) - / lsv,2(xT)hjr(xT)· 
Jn 
Jn 
Jn 
Therefore, for |zj| > 77, 
fRlsvA(xT)hjr(xT)G(I[N}) 
= 
IRlsnA^r)\hjr(xT)\G(I[N}) 
= 
\fRlsnA(xT)hjr(xT)G(I[N})\ 
^ 
\fKlsvAxT)hjr(xT)G(I[N})\ 
+ 
\JRlSt)^T)hjr(xr)G(I[N))\ 
^ 
IK W>(*T) 
\hjr(xT)G(I[N})\ 
+ / R l s „ , ( x r ) \hjr(xT)\ 
G(I[N}) 
< 
η^Ο(Ι[Ν}) 
+ 
(η+^\)^0(Ι[Ν}) 
= 
2»/ + |*;|; 
while, for \i^\ < η, 
f 
lSr!l(xr)\hjr(xr)\G(I[N})<v. 
JR 

402 
CHAPTER 8. STOCHASTIC 
INTEGRATION 
Therefore, for 1 < k < 3, lsv k(xj-)hjr(xr) 
is absolutely G-integrable on R T, 
with 
3 
/ 
\hjr(xT)G(I[N})\ 
= 
W 
lsv.k(xT)\hjr(xT)W[N\) 
< 
(2η+\ιϊ\)+η+(η+\ι*;\) 
= 
4η + 2\ιτ]\ 
for \ij\ > η, and 
/ \hjr(xT)\G(I[N}) 
< 2η 
JB. 
for \ιΤ\ < η. By Theorem 39, 
/ 
\hjr(xr)G(I[N})\=VhjrG[RT}, 
and therefore, since η > 0 is arbitrary, V^.rG[Rr] —>· 0 as r —> oo. 
O 
Given a positive integer r we have 
r=]o, *] = U *;. 
Σ 1*51 = *' 
fr(xr) = Σ ^-(^) = π52,Γ(^τ), 
in accordance with (8.10). Write 
hr{xT)=n9^r(xr)-t. 
Theorem 232 For any given r the functions ΊΖψ,τ(χτ) 
and hr{xq-) are G-
integrable on R r with integrals t and 0, respectively. 
Proof. This follows from Theorem 230. 
O 
This result can be expressed in terms of random variability or observables 
as follows. For r = l,2,3,..., the contingent observable 
Krw=Ex^)2-Ex(^)2[Rr'G] 
i = i 
j = i 
= t. 
is a random variable, and E [Jl?*>r(X)} = E \^j=1 
X (iff 
Theorem 233 For given r, the function TZ?^,r(xr) has G-variation t; that is 
vTC??.-G[Rr] = t. 
Proof. This follows from Theorems 232 and 39. 
O 

8.4. WEAK STOCHASTIC 
INTEGRAL 
403 
Theorem 234 Given Brownian motion Xr — #7-[R,r, G], the function 
hr(xr)=n9^r{xT)-t 
is G-convergent to 0 as r —► oo; that is, 
πψ'Γ(χτ) 
- t 4 0, 
or V h r G[R r] -» 0. 
Proof. For any 77 > 0, there exists rv so that, for r > Rv, we have |zj| < 77. 
The result then follows from Theorem 231. 
O 
Taking f{x) = t for all x G R T, the constant t can be treated as a (degen-
erate) random variable: 
f(X)~t[RT,G}. 
In Section 8.3, evaluation of a stochastic integral as a contingent observable led 
on to information about the likelihood distribution of the corresponding sample 
datum. The following result also illustrates this in the case of the (prospective) 
stochastic integral fT dX%. It contains a summary of properties of the binary-
Riemann-sum-of-squared-Brownian-increments observable Σ^· = 1 ^ ifj) · 
Theorem 235 The binary-Riemann-sum 
random variable 
Σχ(.;)2*Σ*(*Σ)2[»τ.σ] 
3 = 1 
i = l 
is G-convergent to the (degenerate) random variable 
f(X)~t[RT,G] 
as r -* 00; that is, 
ν(π«·'- 4)ο [ R T ] "► °> 
o r ft£,P(*r)3i. 
WVzie £/ie binary-Riemann-sum 
random variable in elementary form: 
2r 
ITien, for each J e I(R), 
f 1 if 
teJ, 
FY(r)(J) 
= 
Pxoh;\J)^{ 
[ 0 
if t f J. 
Proof. This follows from Theorems 234 and 227. 
O 

404 
CHAPTER 8. STOCHASTIC 
INTEGRATION 
To recapitulate, for x G R T and ^2(x(*)) = XM2> the binary Riemann sums 
*?■-(*) = f>(.;)a 
do not in general converge as r —> oo. But, for each binary partition M r, the 
G-random variable 
7 ^ ' r ( X , M r ) , = 
^ X ^ ) 2 , 
i=i 
is G-convergent to the (degenerate) random variable t as r —>· oo. Thus we can 
say that the sequence of binary-Riemann-sum 
contingent observables ΊΖψ'Γ(Χ) 
converges weakly to t as r —► oo. 
This makes it possible to define, in a weak sense, the stochastic integral of the 
observable X (zj) as a (degenerate) contingent observable δψ(Χ) 
with respect 
to an underlying standard Brownian motion X. This definition corresponds to 
Definition 42 in Chapter 5. 
Definition 56 Given T = ]0, t] and X ~ x [RT, G], the stochastic integral (or 
weak stochastic integral,) of the observable ^ ( X W ) = X (i) is the (degenerate) 
random variable f(X) —t [RT, G], and we denote this by 
„ « * , . * . , — . / <Dfs
2 = t. 
r 
To sum up the preceding discussion, if xj- is a function differentiate with 
respect to s G T, then the Burkill-complete integral of (72(*W) = 
x( z) 2 ^s 
zero. If χχ is an arbitrary function in R T, then the integral of x(fc)2 does not 
generally exist. And if xj- is a joint datum of Brownian motion X7-, then the 
weak stochastic integral of ^(X(*)) = XW 2 is t. These statements can be 
expressed, respectively, as 
/ dx2
s = 0 , 
/ dx2
s does not exist, 
/ dX2
s = t. 
r 
The distinction between the notations / , s, S, and S should be noted. All 
four are variants of the letter S, and represent some limit of Riemann sums of 
an integrand summed over partitions of a domain. The first three represent 
integration proper. Furthermore, if x G R T is a variable parameter in the 
integrand, each can be thought of as notation for a function operating on a 
calculation involving variable x in domain R r . 
The symbol S is applicable when there is a convergent Riemann sum calcul-
ation involving stochastic integrand x(z) (or dxs in traditional notation). 
The symbol S represents a form of weak integration or weak limit of Riemann 
sums, and is applied to particular kinds of stochastic integrand, involving x(z), 
for which the Riemann sums do not converge. 

8.4. WEAK STOCHASTIC 
INTEGRAL 
405 
Thus S symbolizes a function of x G R r , the function value being the strong 
stochastic integral of an integrand g(xs, s, x(z), %)\ and S is similarly a function of 
x G R r in which the function value is a weak stochastic integral of an integrand 
g(xs,s,x.(i),i). 
The different notations have been introduced to highlight the different mean-
ings of the calculations involved. These notations may be contrasted with the 
traditional notation which does not make these distinctions. In traditional not-
ation the symbol J is used indiscriminately and carries a variety of different 
meanings depending on the context in which it appears. 
In addition, the ΊΖ notation8 (in place of ^ ) is a prompt that Riemann sums 
can be interpreted as functions on domain R T, so the value of a Riemann sum 
can therefore be regarded as a datum of a contingent observable. 
Similarly the symbols s, S, and <S, applied to elements xj- G R r , are a 
reminder that these integrals (or quasi-integrals) are functions of variable χη-
and thus define contingent observables. 
Example 63 Suppose X ~ x [R r, G] is standard Brownian motion. For x G 
R T and ι = is =]s, s'} G I(T) let 
9*(ΧΜ) 
= x8xM 
= Xs(x8> ~ xa)> 
Note that the function g± does not depend on division points (associated points 
or tag-points) s" G z*. The integrand values depend only on cells i, and it is 
not additive on disjoint, adjoining cells; so it is a Burkill-type integrand. With 
0 = so < si < 52 < · · · < sn = t, write N = {si, S2,..., sn}. For x G R r write 
ij =]sj-i,Sj] 
and X(SJ) = xSj, so 
n 
π9
τ*(χ) = (Λθ£>4(χ(0) = Σ^-χ (xsj-xs^). 
Now let N be the binary partition Mr. For r = l,2,3,..., 
Mr = {j2~rt :j = l,2,...2r}, 
ι' = ](j - l)2~rt, 
j2~rt] , 
as before. Then, with Xj = xp = 
x(jt2~r), 
πψ\χ) 
= (ΜΓ)Σ>(χ(*;)) 
= 
(ΜΡ)Σ^·-ΐχ(*Ρ 
= Σ?1ΐ 5 (χ3 + X3-l) Χ(*5) - Σ?1ΐ 5 (Χ3 ~ Χ3-ΐ) χ(*Ρ 
= 
Σ ί = 1 2 \Xj ~ Xi-l) 
~ Σ^=1 2Χ(*Ρ 
= Η*)2-|Σ?1ιχ(»5)2 
8 Redundancy or proliferation of different notation describing more or less the same thing 
can produce confusion. But this is a lesser risk than the ambiguity and loss of understanding 
and meaning resulting from use of the same notation to describe slightly different things. 

406 
CHAPTER 8. STOCHASTIC 
INTEGRATION 
since the sum Σί=ι (x? ~~ χ<]-ι) 
"telescopes" to the value x\, = x(t)2. 
By 
Theorem 234, the expression 
2r 
2 
i*(t)2-i5>(ij) 
3 = 1 
is G-convergent to \x(t)2 
— \t as r —>· oo; that is, 
U9^r(X)$\X{t)2-\t, 
or 
V h r G[R T]-*0 
as r -» oo; where 
hr(xT) = I \x{tf - \ f>(^)2 j - (H*)2 - ii) = i I ί - Ex(»J)2 j · 
i s i n Theorem 234 and Definition 56 we deduce from this (in advance of actual 
definition) that the weak stochastic integral of the integrand g± exists, with 
S^{XT) = 
\X{tf-\t. 
In other words, the stochastic integral of Χ81ί(ι8) 
is \X(t)2 
— \t. This is 
j XsX{is) 
= \X{t)2 
- £*; 
or 
ί XsdXs = \X{t)2 
- \t 
in the traditional notation. What the evaluation tells us is that, for Y = S^ 
{X), 
J =]u,v] G I(R), and Y ~ i/[R, Fy] in elementary form, the potentiality dist-
ribution function for Y is 
FY(J) = C [(S? (X) € J)] = N (]«',«']) 
where 
v! = 2^/ü + \t, 
v' = 2y/v + \t; 
N being the standard normal distribution function. 
O 
This example of a weak stochastic integral is based on Definition 42 and 
Theorem 86. 
At this point—in advance of a general definition of the concept—we have 
plausibly evaluated, from first principles (i.e., the convergence behavior of Rie-
mann sums), two instances of weak stochastic integrals. The key point in the 
argument is convergence to zero of the variation on R7" of functions of the form 
hr(xqr)G(I[N\). 
To put it another way, and speaking loosely, the key point is 
convergence to zero (as r —>· oo) of the supremum of Riemann sums 
(Vy)J2\hr(xr)\Fx(I[N}), 

8.5. DEFINITION OF WEAK STOCHASTIC 
INTEGRAL 
407 
where, for any given r and any given gauge 7, the supremum of the Riemann 
sums is taken over all 7-fine divisions Τ>Ί of R r . The particular distribution 
function considered here is standard Brownian, but the general idea is applicable 
to any process Xj- ~ xj-[RT, Fx] where Fx is non-negative. 
A numerical and graphical sense of these concepts can be obtained from the 
calculations in Chapter 9. 
8.5 
Definition of Weak Stochastic Integral 
The underlying idea of the (weak) stochastic integral is illustrated in the exam-
ples of the preceding section and in the numerical calculations of Chapter 9. It 
can be summarized as follows. 
If each term fr (χη-) in a sequence of random data has the form of a Riemann 
sum of some integrand on T, and if the sequence converges weakly in the sense 
of Definition 31, then Theorem 87 can be used to define an observable which is 
to be designated as the weak stochastic integral. For stochastic integrand g and 
binary partitionsf Mr of T, an rth binary Riemann sum fr(xr) 
is 
ng/{xr), 
=ng
r(xT,Mr), 
= ( M r ) ^ ( z s , s , x ( z ) , z ) . 
Definition 57 For T = ]0,i]? let Xj- be an arbitrary stochastic process 
XT~xr[RT,Fx] 
with non-negative distribution function Fx. 
Suppose, for each xj- G R7", the 
function g(xs, «s, x(^), i) depends on the process increments x(z) where % = ]s, s') G 
I(T); and suppose, for each positive integer r, Mr is a binary partition {tj2~r : 
j = 1,2,..., 2r} of T. 
Thus, for r = 1, 2,3,..., and for each x G R T, the 
function 
ng/{xT),= 
{Mr)Y^g(xs,s,*(i),i), 
is a sequence of Riemann sums on T for the integrand g. If there is a function 
f(xr) 
such that the difference 
hr(xT):=ng/(xr)-f(xr) 
satisfies 
lim 
VhrFx[RT}=0, 
r—>oo 
then we say that f(Xr) 
— f(xr) 
[R7"?^:] is the (weak) stochastic integral of 
^(Xs,X(z),z); denoted by 
f(XT) 
= 
S9
T{XT). 
For simplicity, consider only standard Brownian processes Xq-. Thus, with 
FxT = G, the stochastic integral of the integrand g is the contingent observable 
S9
T{XT)^S9
T{xT){KT,G\, 

408 
CHAPTER 8. STOCHASTIC 
INTEGRATION 
where Sq-{xj-) is the function f(xr) 
of Definition 57 above. In terms of the 
alternative notations, Sj-{Xq~) c a n De written as 
/ g(Xs,s,X(is),is) 
or 
/ 
g(Xs,s,dXs,ds). 
If, for r = 1,2,3,..., the observables hr(X) = 7Z?pr(X) — f(X) 
are expressed in 
elementary form Y^ 
= hr(X) withY^ 
~ y(r) [R, FY(r)], then, by Theorem 87, 
the statement "Sq-(Xq-) = f(Xr)" 
implies that the sequence of distribution func-
tions jPy(r) converges to the atomic distribution function Fo as r —> oo. This 
is the meaning we ascribe to the (weak) stochastic 
integral. 
This weak 
form of convergence of binary Riemann sums !ZjT(xq-) to a function S^-{xq-) is 
expressed by 
lim V/^s,r_5fl\G [R r] = 0, 
or 
VSf -> Sj- 
as r —> oo; 
and convergence in this weak sense can sometimes occur in cases when the strong 
limit, linv-^oo (ΊΖγΤ(χη-) — Sj-(xq-)) = 0, fails to exist9 for xj- € R7". 
This version of (weak) stochastic integral can be applied to processes other 
than Brownian motion. When Xq- ~ xj-[HT,G] 
is standard Brownian motion 
the traditional version of the weak stochastic integral is sometimes called the 
ltd integral. For the pair of contingent observables 
πγ(χτ) 
^ n9/{xT) [RT, G] , 
s9
T{xT) =* sg
T(xT) [Rr, G] , 
we have the equivalent notations 
TZ3/(xr) 
4 S9
T(xr), 
Ua/(XT) 
-> 5 f 
(XT). 
The first of these is based on (5.4). In the second one, the distribution function 
G (or FxT) 
is implied in Xq-, and the fact that the convergence is G-weak 
convergence is implied in the symbol S for weak stochastic integral. 
Definition 57 is consistent with Definition 56, and also with the evaluations 
in Theorem 234 and Example 63. 
In the case of the stochastic integrand 
g2(X(is)), 
= X W 2 , 
= (x(s') - x(s)f 
, 
the datum y^ = 1Z?p'r(xr)—t is a member of a sequence {y^ 
: r = 1, 2,3,...}. 
This sequence does not in general converge to the value 0, or to any other value. 
But, as Theorem 234 shows, the G-expectation of 
\n9r(Xr) -1\ 
9For "well-behaved" integrands g, binary Riemann sums on T can be used to estimate 
the corresponding Henstock integral on T. But if the integrals are interpreted in the sense 
of absolute convergence (e.g., if Lebesgue instead of Henstock integration is used) then, for 
Brownian paths x7-, the strong limit never exists outside a G-null set of xj- G R T . 

8.5. DEFINITION 
OF WEAK STOCHASTIC 
INTEGRAL 
409 
converges to 0 as r —>■ oo. 
Expressing the contingent observable ΤΖψ'Γ(Χη-) — t in elementary form, 
y(r) = 
U9*,r{x) 
_ ^ 
y(r) „ y(r) [ R > py(r)] 
? 
the evaluation S^ (Xr) — t means that the sequence of distribution functions 
FY{T) (J) converges to the atomic distribution function F0. Thus, as r —> oo, the 
sequence 
(
1 
if u < 0 < v, 
0 
otherwise. 
In other words, the datum ΤΖψ'Γ(χ) satisfies 
C [(ΤΖψ'Γ{χτ) < t, Κψτ(χτ) 
> t)} ->· 0 
as r —> oo. This is demonstrated numerically and graphically in Chapter 9. 
Likewise for Example 63. Summarizing this example, 
is 
= 
]s,s'], 
g4{is) 
= 
xs(xs'-xs), 
Mr 
= 
{tj2~r:l<j<2r}, 
i* = 
]C/-l)2- pi, j2~rt] e I(T), 
Κψ\χ) 
= 
Tty{x,Mr) 
= 
Ef=i9A(trj) 
= Σ?1ι*(ϋ-ΐ)2-ρ*)χ(*5) 
= 
Σ?1ι x (Ü ~ 1)2-Γί) (x (j2'rt) 
- x ((j - 
l)2~rt)), 
f(xr) 
= 
\x\-\t; 
and the evaluation S^(Xr) 
= JrXsdXs 
= \X^ — \t means that 
V ( ^ - / ) G [ R l ^ 
(8.11) 
as r —>► oo. Each of the two functions ΊΖ^'Γ(χτ), 
\x\ — \t is G-measurable in 
R T, so Theorem 87 can again be applied in order to further elucidate (8.11). 
That is, the observable 1Z?f'r(X) — f(X) has datum 1Z?f,r(x) — /(#), whose value 
tends to zero, in an "approximate" sense, as r —> oo. This means that, when we 
write ?/r) = 1Z?f,r(x) — f(x) and form the corresponding contingent observable 
y W ~ y W [ R , F y r ] , 
then, for any u < 0, v > 0, 
C\{Y{T) <U, y(r) >«)] 
->o 
as r —> oo. 

410 
CHAPTER 8. STOCHASTIC 
INTEGRATION 
8.6 
Properties of Weak Stochastic Integral 
As in Definition 57, assume that Xq- ~ x r [ R r , FxT] is an arbitrary process. 
The next result shows that the weak stochastic integral is unique up to i n -
equivalence; so the weak stochastic integral is well defined. 
Theorem 236 In Definition 57, if there is another function k(x) such that 
kr(x) = 1Z?/(x) - k{x) satisfies VkrFx [RT] -> 0, then kF= f. 
Proof. We must show that V ^ . ^ ) ^ [RT] = 0. We have 
hr(x) = 7l9/(x) - /(*), 
VhrFx [R r] -> 0, 
kr(x) = n9/(x) - k(x), 
vkrFx [RT] -► o. 
Also, 
hr(x) - kr(x) = {n9/(x) - f(x)) - (n9/(x) 
- k(x)) = k(x) - f(x). 
Then 
Vik-f)Fx [RT] = Vifr-kr)Fx [R r] < V / r F x[R T] + YkrFx [RT] -* 0 
as r —> oo. Therefore V(k-f)Fx 
[R r] = 0 since it is independent of r. 
O 
Definition 57 is consistent with the definition of the strong stochastic integral 
(Definition 55), in the following sense. 
Theorem 237 Suppose Xq- ~ xr [RT, Fx\ is a process with Fx > 0; suppose 
the strong stochastic integral ofg(Xs, s, X(zs), zs) exists; and suppose, for binary 
partitions Mr, the corresponding sequence of Riemann sums ΊΖ?/ (XT) converges 
uniformly, for xq- G R T, to S?j-(x), as r —>► oo. Then the weak stochastic integral 
of g exists, and 
S9
r(XT)=S^(XT). 
Proof. Let ε > 0 be given. Existence of the strong stochastic integral of g 
implies that for each x G R r there exists rE so that, for r > re and for all 
z G R r , 
\n9/(x)-S9
T{x)\ 
< ε . 
Then, for any division V of R7", 
(2?) Σ \TZ9/(x) - S?r(x)\Fx(I[N}) 
< e(V) £F X(I[N}) 
= ε, 
so, for r >r£, 
y(n9
1ir-s9
T)Fx 
[ R T ] < 2 ε· 
This gives the result. 
O 

8.6. PROPERTIES 
OF WEAK STOCHASTIC 
INTEGRAL 
411 
In (8.3), (8.4), and (8.5), a contingent observable style of notation was 
given for different kinds of stochastic integrals. Unlike δη-(Χη-), contingent 
observables s?j-(Xq-) and S?J-{XJ-) are themselves actual integrals. However, 
weak stochastic "integrals" S?j-{Xq-)—though not actually integrals in the usual 
sense—are defined by means of Riemann sums, so it is not surprising that they 
have some integral-like properties, such as the following. 
Theorem 238 If hj(xs,s,x.(is),is) 
is weakly stochastic integrable for j = 1,2, 
then h = hi -\- h2 is weakly stochastic integrable, and 
Proof. In traditional notation, this states that the weak stochastic integrals 
satisfy 
/ {h1+h2) 
= / h1(Xs,s)X(is),zs) 
+ 
h2(Xs,s,X(is),is). 
The distribution function Fx is assumed to be non-negative. Let ε > 0 be given. 
Weak integrability of hj implies that, for j = 1,2, there exist gauges 7 ^ and 
integers rjt£ so that for any r > rj,£ and any T^-fine division Ί)Ίά ε of R r , the 
binary Riemann sums satisfy 
(^,.) Σ (<ΜΌ Σ | ^ r ' » - Sr(x)\) 
FX(I[N}) 
< ε. 
Take 
re =max{r l i £,r 2, e}, 
ηε -< 7ι,ε Λ η2,ε· 
We have 
π¥{χ) = nh
T
ur{x) + nh^'r{x), 
nh/(x) - (s£(x) + a£?(x)) = ΣU (ητ'τ(χ) ~ST (*)). 
\nh/{x)-(s^{x) + s^{x))\ < 
Y:)=1\nh^r{x)-s^{x)\. 
For j — 1,2 write 
<*i = (*>7,,J Σ ( ^ ) Σ \nr ' » - Sr^)\) Fx{IW\)-
Then, for any r >r£ and any 7£-fine division Ί)Ίε of R T, 
" 
= 
(^)E((Mr)E\^r(x)-(s^(x)+^(x))\)Fx(I[N}) 
< ai + a2 < 2ε. 
Therefore, as r ->- 00, V^, r_/ 5h 1 + 5h 2\ F j f [RT] -> 0, as required. 
Q 

412 
CHAPTER 8. STOCHASTIC 
INTEGRATION 
Theorem 239 Suppose a is a real or complex constant. Ifh = h(xs, s, x.(i3),is) 
is weakly stochastic integrable then ah is weakly stochastic integrable, and 
S?{XT) 
= 
aS${XT). 
Proof. Similar to preceding proof. 
0 
8.7 
Evaluating Stochastic Integrals 
The notation for stochastic integrals can be a source of misdirection. If X is a 
Brownian motion (or any other process on T), "/0 Xs dXs" is a suggestion or 
prompt for " JQ xdx = \t2" 
in basic calculus. The form in which each of these 
two integrands is expressed suggests superficially some equivalence between the 
variable x in the latter, and the variable Xs in the former, and once this intuitive 
link is established, the extra term — \t in the evaluation "fQ Xs dXs = 
^(X2—t)" 
may seem counter-intuitive. 
But the true correspondence or linkage is between, on the one hand, x G 
T =]0,1] in "/ 0
£χώ"; and, on the other hand, s G T =]0,1] in 
uf*XsdXs". 
In the latter, Xs, = X(s), is a quite challenging function of s, and the notation 
dXs,= 
dX(s), means that the integration is a rather complicated Stieltjes-
type10 integration with respect to increments dX(s) of the difficult function 
X(s). 
Viewed this way, the surprise is not the extra term —\t. Suppose the in-
tegrand is #4 = xs(xs
f 
— Xs)- Then estimates of ΤΖψίΤ(Χψ) must be formed— 
a function which depends on variable sets of elements tj = jt2~r 
and Xj = 
x(jt2~r). 
What is then surprising is not the "unexpected" extra term in the 
result, but that the stochastic integral actually evaluates at all; and that it 
evaluates to a simple analytic expression depending only on t and x(t) when 
r —>· oo. 
And in the case of integrands such as g± the elements Xj and tj do not 
conveniently cancel out or "telescope". Instead, they have to be "averaged out" 
in a weak limit procedure. Looked at in this light it should be no surprise that 
evaluation of weak stochastic integrals11 can take quite a bit of effort. 
In Example 64 below, with g7(xs,x.(is)) 
= x2
s{xs' —xs) and standard Brown-
ian motion Χχ, the weak stochastic integral SJ?(XT) 
— f-j-XsdXs 
is evaluated. 
Before that, here are some preliminary results involving other evaluations. 
It has been established in Sections 8.3 and 8.4 that 
/ dXs = Xt, 
I (dX3) 
= t, 
10In fact, though superficially resembling a Stieltjes integrand it is actually Burkill-type, 
since the integrand x(s) (x(s') — x(s)) is not additive on disjoint, adjoining cells ]s,s'] in 7". 
n I f we admit observables /(X, N) ~ f(x,N)[RT 
x λί, i*x], then the presence of variable 
elements Xj and tj is no barrier to analysis which does not require these variables to disappear 
by cancellation, or "averaging out", or any other such device. But such analysis needs a 
development of the concept of measurability outlined in Section A.l. 

8.7. EVALUATING 
STOCHASTIC 
INTEGRALS 
413 
the first being a strong stochastic integral S, and the second a weak stochastic 
integral S. For higher powers of dXs (or X(zs)), the stochastic integrals are 
zero. Using Riemann sum constructions such as the one in Example 62, it can 
be seen that the strong version of the stochastic integral does not exist in those 
cases. 
Theorem 240 IfXr 
— %T [RT>G\ i*s standard Brownian motion onT — ]0, t], 
then, with gs{x.(t)) = x(03> then 
Sf3(XT), = j XW3, = I dXl = 0. 
Ifn>3 
and gM(i) = x(z)n then S9^\x), 
= frX(i)n, 
= JTdX?, 
= 0. 
Proof. The proof uses Theorem 234. Note that 
2r 
2r 
3=1 
j=l 
where Xj = x{tj), tj = jt2~r, j = 0 , 1 , . . . , 2r. By G-measurability of 
ΊΖψ,Γ(χτ) 
and llg^r(xT), 
Theorem 40 implies12 that 
V ^ G [ R r ] 
= 
/ \-R**r{xT)\G{I[N\), 
^(n^-t)alRT} 
= [ 
\n^'r(xT)-t\G(I[N}). 
Define r-dependent subsets S+ and 5_ of R T as follows: 
S+ 
= 
{xr:n9^r(xT)-t>0}, 
= {xr:n9^r(xr)>t}, 
(8.12) 
5_ 
= 
{ χ τ : ^ τ ' Γ ( χ τ ) - ^ < 0 } , = 
{xr^0<n9^r{xr)<t}. 
By Theorem 234, for ε > 0 we can choose re so that r > r£ implies 2~rt < ε, 
|XJ - iCj-il < ε for χη- G C ^ , and13 
/ 
\-R$'r{xT) 
~ t\ G(I[N}) = V.w.a.r_t|G[Rr] 
^ R 
I T 
I 
< ε. 
By Theorem 17 a gauge 7 can be chosen so that, for every 7-fine division Ί)Ί of 
R r, 
wE 
2 r 
Y^{XJ -Xj-if 
-t 
3 = 1 
G(I[N]) 
< 
2ε, 
12Applying this theorem to any absolute random variable f(X) 
^ / ( x ) [ R T , F x ] for which 
F is non-negative, measurability of / gives V / F [ R T ] = J' 
\f\F(I[N]). 
1 3The set C j ö is denned in (6.28). 

414 
CHAPTER 8. STOCHASTIC INTEGRATION 
^ ) Σ ( Σ > ; - ^ - Ι )
2 - Μ W*r)G(I[7V]) < 4ε, 
{V^lt-^ixj-Xj-ifUsA^GinN]) 
< 4ε; 
so 
( ^ ) Σ ( ί > ; - ^ - ι ) 2 ) W*T)G(/[iV]) < 4e + t(O,)Y^G(I[N}) 
= 4ε +1, 
( ^ Σ ί Σ ^ - ^ - ι ) 2 ) 1 ^ ^ ) ^ / ^ ) 
< t(2>7)£G(I[JV]) 
= t. 
Write 5 ; - S+ n Cje, 5L = 5_ ΓΊ C^, and 
ß+ = 
(V^i^lxj-Xj-iAls^XTMIiN}), 
ß. 
= 
(V^^l^Xj-Xj-xAls'Jx^GillN])-
For xT € S'+, 
ß '+ = fl\) Σ ( Σ \*i ~ ^-11 & ~ ^-i) 2) H 
(*T)G(I[N\) 
< ε(Ι? 7)ΣίΣ(^-χ,-ι) 2)ΐ 5 ;(χτ)ο(/[7ν]) 
< ε(4ε + ί), 
0_ = 
( ^ ^ [ ^ l ^ - ^ - x K ^ - x ^ f l l s L ^ r M / I i V ] ) 
^ = 1 
< ε(Ι> 7)ΣίΣ(^·-^·-ι)Ίΐ5ΐ(*τ)σ(/[ΛΠ) 
= ei. 
Then 
/ r 
\ 
(^ΣΐΣι^-^-ιΐ3)^1^) < ε(4ε+2ί), 
and the result follows. For powers of x(z) higher than 3 the proof is similar. Q 

8.7. EVALUATING 
STOCHASTIC 
INTEGRALS 
415 
To increase our store of stochastic integrable functions we show that, for 
stochastic integrand geixs^i^s)) 
= ^ x f e ) 2 , 
S?{X) = J X.\ta\, 
or, in other notation, JTXsdX2 
= JTXsds. 
The following result is preliminary 
to this. Suppose 5 E]0,t] is fixed. For integer r let 
S = tj-χ = (j - l)t2~r, 
lr
0 = ]tj-Utj], 
x(lj) = X(tj) - 
X(tj-i), 
and, with Xj = x(tj), let 
fr(xT) 
= Xj-Mirj)2 
- Xj-i\irj\ = Xj-i {{XJ - Xj-if 
- (tj - tj^yj 
. 
Theorem 241 If Xr — xr [R>T, G\ is standard Brownian motion onT = ]0, t], 
then YfrQ[RT] —y 0 as r —» oo. 
Proof. If z = ]s, s'], s = tj-i and s' == tj, then a?j_i = x(s). For any positive 
integer k let 
x£ 
= 
max{|xj_i|,fc : 1 < j < 2r} , 
/£(ζ τ) 
= 
x ^ x ^ ) 2 - ^ ! ) , 
so for each xj- G R r , \/ζ(χτ)\ 
~> \fr(xr)\ 
as k —► oo, with 
Ι/Σ(*τ)|<|/*+ι(*τ)|, 
Ä = 1,2,3,.... 
For every positive integer q the function 2/9exp(—y2) is absolutely integrable 
(with respect to y) on R. Then, by Theorem 159, the cylinder functions 
fr(xr) 
and \fr(xr)\ 
are G-integrable on R r ; as are fk{xr) 
and |/£(#τ)|· Absolute 
G-integrability gives 
VrkG[Rr}= 
[ 
\rk(xr)\G(I[N]), 
VrG[RT}= 
[ 
\r(xr)\G(I[N}). 
JUT 
JUT 
For each r, Theorem 57 implies 
/ 
\fr(xr)\G(I[N}) 
= lim / 
|/£(:rr)|G(/[iV]). 
Letting r —> oo, Theorem 234 implies that, for each fc, 
V / i G[R T] = / 
\rk(xr)\ 
G(I[N)) -> 0. 
Theorem 64 then gives 
VrG[KT}= 
[ 
\r(xT)\G(I[N}), 
= lim / 
|/£(zr)|G(7[7V]), -> 0 
7RT 
fc->°° 
J R T 
as r —► oo. 
O 

416 
CHAPTER 8. STOCHASTIC 
INTEGRATION 
In effect, Theorem 234 enables dX2 to be replaced by ds. A version of this 
is called basic isometry in traditional accounts of the subject, such as 0ksendal 
[190]. 
The next result gives JrXsdX2 
= JTXsds. 
As before, write ge{K(t3)) = 
xsx(zs)2 and write 
S(XT) = / #(s)|fcs| = / 
x{s)ds 
when the latter integral exists. 
Theorem 242 If X ~ x [RT, G] is standard Brownian motion on T =]0,£] 
thenS^(Xr)=s(Xr). 
Proof. In the other notations this result states that 
/ XsX(is)2 
= f Xs\is\, 
f XsdX2
s = / 
X(s)ds. 
To prove it we must show that, as r —> oo, 
V( W 6,r_ s ) G[R T]-+0. 
The proof is a continuation of the argument in the proof of Theorem 241. Except 
for the G-null set of discontinuous x-j-, the integral s(xj-) exists. Write 
2r 
Sr(xT) = 
Y^Xj-l\trjl 
By uniform continuity of xq- G C ^ , 
sr(xr) 
-> / xsds = s(x r), 
V ( S r_ s ) G[R r] -» 0, 
(8.13) 
as r —► 00. The objective is to deduce the statement Sfj?(Xq-) = s(Xj·) from 
Theorems 234 and 241. Writing Xj_i5fc = max{|xj_i|, A:}, define 
/ί(^τ) = Σχ5-ι.*(χ(*5)2-*ί)> 
so that, as A: —>► 00, 
ΙΛ(*τ)Ι -+ ΙΓ(χτ)Ι = E l^-il Ιχ(*5)2 -1*511 = Σ I*J-IX(*5)2 - *;-il*;il · 
Each of fr{xq-) 
and /[(17) is a sum of absolutely G-integrable functions, and 
therefore each is absolutely G-integrable. By Theorem 57, for each r, 
VnG\RT]= 
[ 
\rk(xT)\G(I[N}) 
-+ / 
|r(xr)|G(/[JV]) = V r G[R r] 

8.7. EVALUATING 
STOCHASTIC 
INTEGRALS 
417 
as k ->■ oo. By definition of fl(xr), 
Y T G [ R - T ] < fcV(7js2,r_t)G[RT], so for each 
k, Theorem 234 implies 
0 < / 
\fr
k(xT)\G(I[N}) 
= VfrG[RT} 
< 
fcV(*92,_t)G[Rr] 
-+ 0 
as r —> oo. Theorem 64 then implies that 
0< [ 
\rk(xT)\G(I[N})=VrG[RT]^0 
{ s u ) 
as r -> oo. The result follows from (8.13), (8.14), and Theorem 238. 
O 
Example 64 Suppose X ~ x [RT, G] zs standard Brownian motion. For ι = 
]s,s'] write 
g7(xs,x(is)) 
=x2
sx(is), 
= x2
s(xs> 
-xs). 
We show that 
S?(X) 
= 
| X t
3 - / r X e | . . | , 
or 
fTX2X(is) 
= 
i3Xf-JTXs\is\, 
or 
fTX2dXs 
= 
±Xf-fTXsds; 
these being alternative ways of denoting the same stochastic integral. The prosp-
ective stochastic integral value includes a term fTXs\is\ 
or fTXsds, 
on the 
right-hand side. Note that, for Brownian X, the set of discontinuous x G R T is 
G-null. As in (8.9), when such x are excluded the contingent observable 
s(X T) ~ s(xT) [RT, G] , 
s(xT) = / 
xsds, 
along with its elementary form, are well defined; though, in general, there is no 
closed analytical representation of the contingent datum JTxsds. 
To evaluate 
JTX2dXs 
use the identity 
a2(ß - a) = | ((/33 - a3) - (/? - a) 3 -3a(ß- 
a)2) . 
For irj =}(j - l)2~rt,j2-rt] 
and x G R r , let 
a 
= 
Xj-! 
= 
x((j-l)2-rt), 
ß = 
Xj 
= 
x(j2'rt), 
ß-a 
= 
x( tj) 
= 
x(j2-rt)-x((j-l)2-rt); 
so the r-th binary Riemann sum 
= I ( Σ (*? - ^3-i) - Σ (** - ^ ) 3 - 3l>i-i*(.j)2) · 

418 
CHAPTER 8. STOCHASTIC 
INTEGRATION 
By cancellation, 
/ > \X3 
~ 
Xj-l) 
= 
Xt-
Note that, for is =]s, s'], the function 
g8(x(is)) 
= 
x(s')3 - x(s)3 
= 
(x(s') - x(s)) (x(s')2 + x(s')x(s) + x(s)2) 
= 
x(zs) (x(s')2 + x(s')x(s) + x(s)2) 
is a stochastic integrand, and its strong stochastic integral exists, with 
S9
r
s(x) = x(tf, 
S??(X) ~ x* [R r, G] . 
To complete the evaluation it is required that, as r —> oo, 
2 r 
(M r)]T(x^)) 3 
= 
^ f e - ^ i f ' ö 
and 
i=i 
( Μ Γ ) 5 ^ ^ _ ι χ ( ^ ) 2 
4 
[ xs\is\,= 
( x(s)ds. 
(8.15) 
These weak convergences are given, respectively, by Theorems 240 and 242. 
Therefore <Sf7(X) = \X* - fr Xsds. 
O 
Example 65 Suppose p is a positive integer, and 
g9(*M) 
= 
x(s'Y-x{sY 
= 
(x(sf) - x(s)) ( φ ' ) * " 1 + x(sf)P-2x(s) 
+ · · · + X(Ä) P _ 1) 
= 
χ(ιβ) (s(«/)p~1 + a?(e,)p"2^(«) + ' ' ' + ; φ ) ρ - 1 ) · 
For any real- or complex-valued distribution function Εχτ 
and process Xj~ ~ 
xr[R>T,FxT}, 
the strong stochastic integral S^{Xj-) 
exists, with value Xf: 
This is because, for any partition N = {t\,..., 
tn = t} 
ofT, 
Yt(x(tjr-x(tj-1r)=x(tr. 
The contingent observable S?f(Xq-) ~ S^(x7-)[Rr, FxT] can then be converted 
to elementary form: 
y = Saf(xT)=xp
t, 
Y~y\R,FY]; 

8.8. STOCHASTIC 
AND OBSERVABLE 
INTEGRALS 
419 
where, for u, v G R, u < v, 
C[(Ye}u,v})] 
= 
FXtQuKvi]). 
This completes the evaluation of the stochastic integrals of the list (8.7) of nine 
stochastic integrands g\ to gg. 
Q 
In traditional notation, making no distinction between strong stochastic and 
weak stochastic integrals, the results are: 
JTdXs 
= Xu 
ST(dXs)2 
= f, 
Jr(dXsf 
= 0, 
JTXsdXs 
= 
\*2t-\t, 
JTsdXs 
= 
frXsds, 
JTXs{dXs)2 
= 
frXsds, 
(8.16) 
!Tx2
sdxs = lx?-frxsds, 
$Td{xD = xi 
frd(x?) 
= 
xi 
Also, the proofs of Theorems 234 and 240 show that, for p > 2, fT (dXs)p = 0. 
8.8 
Stochastic and Observable Integrals 
Joint-basic observables Χτ — XT[RT,FXT] 
and related joint-contingent observ-
ables f(Xr) — / ( ^ T ) [ R T ? ^ X T ] a r e present throughout this book. The integral 
(expectation) on R T of the latter is defined in Definition 17, so f(Xr) has 
meaning as a random variable. But if T has more than one element and / is 
the identity function—the joint-basic case—Definition 17 does not apply. 
The book is mainly concerned with infinite sets T, particularly with T a real 
interval. In that case, it is possible for the function values or contingent data 
values f(xr) to be determined by integration on T of some integrand function 
g which depends on χτ· 
Numerous examples demonstrate the point that an 
integrand f(xr) can have a form involving an integral on T. For example, 
f(xr) 
could be taken as one of the following functions: 
U~C(XT) 
= 
exp(—cfTU(x(s),s)ds), 
S9
T
7(xT) 
= 
fTx{s)2dx(s), 
(8.17) 
Sff(xT) = /Τ<*0Φ)3). 
By tradition, the term "stochastic integral" is applied to the second and third 
of these, but not the first since it does not involve the integrator function x(z). 
However, since all three involve integration on T, and all depend on random 
joint-basic data values x(t) for t G T, the term observable integral or joint-
observable integral can be applied to such functions. 

420 
CHAPTER 8. STOCHASTIC 
INTEGRATION 
Because of the Riemann sum/integral construction on domain T, the value 
taken by such a function depends, in principle, on every value x(t) for t G T; 
and may also depend, implicitly or explicitly, on all of the elements t £T, with 
consequences for subsequent calculation in R T, such as the FxT -expected value 
on R T of the function. 
Such dependence is expressed in the joint-contingent notation f(Xr) 
— 
f(xT)[RT,FxT]. 
In examples (8.17), dependence on each x(t) (and possibly 
on each t, t G T =]0, r]) is expressed in 
exp ( — c I U(x(s),s)ds 
1 , 
-x3 — / 
xsds, 
the values taken, respectively, by the first and second functions of (8.17). But if 
T =]0,r] the third function JTd(x(s))3 
evaluates as x(r)3 where, conveniently, 
for t φ T all dependence on elements x{t) and t has disappeared. This ensures 
that the joint-contingent observable 
Si»{XT) 
= [ d(X(sf) 
~ / d(x(s))3 
[RT,FXT] 
JT 
JT 
is easily expressible in elementary form (i.e., a form that has sample space 
R rather than R T). The joint-observable notation f(Xr) 
— / ( # T ) [ R T ? ^ X T ] 
implies that the joint distribution function FxT is well defined, and therefore 
consistent. Thus, writing y = fTd(x(s))3 
= x(r)3, consistency of FxT implies 
that 
Sf? (XT) = Y ^ y [R, FY], 
FY = FXr; 
the latter being the distribution function for the elementary basic observable 
XT ~ xT[R, FxT). Then, as demonstrated earlier, for outcomes 5 the likelihoods 
£[(Sl?(XT)eS)}, 
C[(YeS)} 
are quite easy to determine. Apart from g$ and #7, this applies to all the 
evaluations listed in (8.16). This is particularly convenient, since some of these 
stochastic integral evaluations are of some importance in, for instance, applic-
ations such as the Black-Scholes formula (8.54). 
But what about the less amenable #5, #7, and W. Suppose a joint-contingent 
observable f(Xr), 
perhaps having joint-observable integral form, depends ex-
plicitly on values x(t), and/or on t, for infinitely many t eT. 
How is it possible 
to analyze such an observable? 
The key to this is the definition of the Bur kill-complete integral in R T, which 
says the following. 
Suppose, in domain R T, a real- or complex-valued function h is defined 
on the set of associated triples (χτ,Ν,Ι). 
The function h is Burkill-complete 
integrable on a figure E, with integral a = fEh(xr,N,I), 
if for every ε > 0, 
there exists a gauge 7 so that, for each η-fine division £Ί of E, 
α-(εΊ)ΣΗχτ,Ν,η 
<ε. 

8.9. EXISTENCE 
OF WEAK STOCHASTIC 
INTEGRALS 
421 
In a variety of examples already considered, the integrand h depended on 
variable elements xt for elements t of variable finite subsets N of T. In other 
words, the integrand h often has the form h(x(N),N,I[N]). 
But, as it stands, 
Definition 17 actually allows the integrand h to depend on each xt for t £ T. The 
key point of the definition is that the associated cell / = I[N] of the integrand 
must belong to a class χϊρ = {I[N}} depending on the joint element χτ· 
The phrasing of the definition does not show h depending explicitly on el-
ements t of T \ N. Nonetheless, just as h is allowed to depend on elements xt 
for t G T \ TV, Definition 17 also allows the integrand h to depend on elements 
t G T \ N, while x, N, and I[N] vary in the Riemann sum estimates of the 
integral. If required, the notation can be amended to express this dependence 
explicitly, with no further requirement to amend the theory and properties of 
the integral. 
Thus an integrand / in R T may involve calculation in T involving perhaps 
infinitely many elements t G T, as in the case of the integrand U~C{XT) 
= 
exp 
(—cfTU(x(s),s)ds). 
Existence of such an integral on T, in advance of calculating Riemann sums 
of the integrand in R T, can make the analysis in R T easier. But, for Riemann 
sum-type integrands in R T, convergence—strong or weak—of the Riemann sums 
on T is not essential, and Definition 17 and its extensions are valid whether or 
not the preliminary convergence of Riemann sums on T holds. Likewise for any 
integrand function f(xr,Q) 
which may depend not just on variable elements 
XT G R T but also on variable (perhaps infinite) subsets 
QofT. 
8.9 
Existence of Weak Stochastic Integrals 
It is possible to continue in the vein of (8.16) and seek, for instance, to evaluate 
further stochastic integrals JTq(Xs)dXs, 
Jrq(Xs)dXg 
and the like, where q(y) 
is a polynomial function of y G R; perhaps using the Henstock integral's non-
absolute convergence criteria (Theorems 62, 64, and 65). But the following 
results—existence theorems for weak stochastic integrals—are easier to prove 
and have more immediate practical use. 
Suppose we have a function 
φ : R x T t-> R, 
φ{τυ, s) G R. 
Let h be the stochastic integrand 
h(X8,s,X(i8),i8) 
= </>(X8,s)X(i8)\i8)\. 
Theorem 243 Suppose Xj- ~ x-j-[HT, G) is standard Brownian motion. If φ 
is bounded for w eK 
and s eT, 
then S^(Xr) 
= 0. 
Proof. 
This says that the weak stochastic integral Jr 0(Xs,s)c?Xsds = 0. 

422 
CHAPTER 8. STOCHASTIC 
INTEGRATION 
Writing 
2 r 
Hr(xT) 
= ^ 0 ( ^ _ i , ^ _ i ) x ^ ) | z ^ | , 
we must prove that 
lim V „ r G [ R r ] = 0 . 
r—>·οο 
By hypothesis there exists a constant κ > 0 so that |0(w, s)\ < κ for all w G R 
and s € T. Except for the G-null set of xr G R T \ G^, Theorem 184 (with 
continuous modification of G) implies that there exists r$ so that, for r > ro, 
1x^)1 < |,;r 
where 0 < a < \. Let ε > 0 be given. Choose r\ > r*o so that r > r\ implies 
|x(zp| < ε, and 
2 r 
|ffr(xr)l 
< Σμ(χ,·-ι,*,·-ι)||χ(ι;)||*5Ι 
2r 
Then, for any division V of R7", 
(V) Σ \Hr(xT)\ G(I[N}) < (nte) ((D) £G(J[JV])) = «fe, 
and this gives the result. 
O 
Suppose, for each χη- G R T and each is =]s,s'] G I(T), the point ς € T 
depends on x(ze) and satisfies s < ς < sf. Write 0ς = φ(χς,ς), 
and define 
stochastic integrands 
h 
= 
Λ(χ(**)) 
= 
0(xs,s)x(2s)2, 
= 
0x(zs)2, 
h+ 
= 
Λς(χ(ζ8)) 
= 
φ(χς,ς)χ(ι8)2, 
= 
0ςχ(ζβ)2. 
Theorem 244 Suppose φ{ιν, s) is uniformly continuous. 
If X is Brownian 
motion then the weak stochastic integrals of h(Xq-) and Ης(Χη-) exist, and 
S^(XT)=S^(XT). 
Proof. By uniform continuity of both φ(ιν, s) for w G R and s £T, and of xs 
for xj- G C^0, there exists κ > 0 so that \φ(χ8, s)\ < κ for all s and x8, except 
for a G-null set of xq-. Let ε > 0 be given. For any xq- G C^e and any binary 
partition Mr = {tj} of T write 
s r(x r) = (M r)]P</>(zj-i)^l· 
s(xT) = / 0(χβ)|ιβ|. 

8.9. EXISTENCE OF WEAK STOCHASTIC INTEGRALS 
423 
Then, by uniform continuity of φ and χη- G C ^ , there exists ro so that r > r$ 
implies \sr{xj-) — s{xr)\ < ε· As before, write g2('x.(i)) = x(^)2· We must prove 
that an inequality of the form 
(^)Σ ^ 0 ( x ( i J _ 1 ) ) x ^ ) 2 - s ( x r ) 
J'=l 
G(I[N}) < e, 
is satisfied, and also a similar inequality with the same s(xj-) but with ς^ re-
placing tj-i in <j>(x(tj-i)). If such conditions hold as r -» oo, then 
S{XT) = S^-{XT) = 
S^{XT). 
Theorem 234 implies that v\ and 7 can be chosen so that, for any r > r\ and 
any 7-fine division X>7 of R T, 
(Ρ-r) Σ \n9Tr^r) 
~ (Mr) Σ l*5l| G(I[N}) < e. 
Write 
M*r) = ^ ^ - . O x i t J ) 2 , ft. = (2>7) 5 ] |M*r) - s(a;T)|G(J[JV]). 
Then, for r > max{ro,ri}, 
A- < 
(P^^IArixrJ-SrixrJIGi/lJVD + i P ^ ^ l s r ^ r J - s i a r r J I G i J ^ ) 
2 r 
EW*5)a-l*JD 
j = l 
G(/[iV]) + 
s(V,)G(I[N]) 
< κ(Ρ7)53 
< 
κε + ε. 
Therefore J r 0(Xs)ds = <S^(XT) = jT<j>{Xs)dX2
s. To show that 
I ^(X,)dXs
2 = y φ(Χ.)άβ, 
write 
KM 
= Σφ(χ{ίς3)))κ(ι))\ 
ß+ = (νΊ) Σ \Κ(*τ) - s(*r)| G{I[N])> 
3 = 1 
and the result follows by further application of the triangle inequality. 
O 
The calculation of stochastic integrals, both strong and weak, can be illus-
trated numerically and graphically. Some of the stochastic integrals evaluated 

424 
CHAPTER 8. STOCHASTIC 
INTEGRATION 
in this and preceding sections are estimated with Maple mathematical software 
in Chapter 9. 
Up to this point our calculation of stochastic integrals has been based on the 
definitions of strong and weak stochastic integration. The stochastic integrals 
have been worked out from first principles, so to speak. But in ordinary calculus, 
evaluation of integrals—Riemann, Lebesgue, or Riemann-complete—is usually 
accomplished not so much by reference to definition but by working out or 
guessing an anti-derivative or primitive function; or by substitution or change 
of integrator; or by some other such indirect or secondary method. 
Thus, instead of expanding our store of stochastic integrals on a case by 
case basis using the primary definition method, the next section examines an 
indirect method of evaluating stochastic integrals. 
8.10 
Itö's Formula 
Standard Brownian motion is defined by the condition that the increments 
X(zs), = Xst —Xs, are independent, normally distributed observables with mean 
zero and variance sf — s. At every instant s of time, an unpredictable change, 
step, or increment is caused by the process dynamic, whose effect at the later 
time sf is measured as the datum x(is) = x{sf) — x(s). For T =]τ',τ] the 
aggregation of these random increments constitutes Brownian motion and is 
described mathematically as 
X r ~ x T [ R T , G ] , 
where the joint potentiality distribution function G is deduced from the distrib-
ution functions of the increments. The additivity of the increments is expressed 
in the equation 
X(j) = Jx(i), 
or X(t')-X(t) = jdXs 
or all j =]t, tf] G I(T). The integral in this case is the strong stochastic integral 
S^(Xr) 
where, as before, £i(x(z)) = x(z) for % G I(T). 
While we are mainly concerned with Brownian motion, suppose for the mo-
ment that X is an arbitrary process Χχ — XT [ R T , . F X ] . Suppose, at times 
s G T some unpredictable value ys depends in a deterministic way on the dat-
um xs and on the time s, 
Vs = 
f{xs,s); 
so the unpredictable increments in the values of ys are 
yOs), =ys>-ys, 
= f{xs>,s') - / ( x e , s ) , 
(8.18) 
and each ys is a datum of a contingent observable 
f(Xs,s)~f(xs,s)[R,FXs}. 

8.10. ITO'S 
FORMULA 
425 
When X is standard Brownian motion it is possible to deduce, from the prop-
erties of the increments X(zs), = Xs' — Xs, the joint distribution function G of 
the joint-basic observable Χτ- Similarly, by investigating the properties of the 
increments 
f(Xs,,s')-f(Xs,s), 
we hope to determine the properties of the joint measurements (Ys)seT> 
If the underlying process X is Brownian motion it may be possible to deduce 
the joint properties of the contingent process f(Xs, s) by relating the increments 
f(Xs',s') 
— f(Xs, s) to the Brownian increments X(zs) = Xs
f — Xs whose prop-
erties have been investigated in detail. However, while f(Xs',s') 
— 
f(Xs,s) 
relates implicitly to X(zs), until the relationship is made explicit it may be 
more difficult to draw on our knowledge of Brownian motion. 
This is where Ito's formula (also known as Ito's lemma) comes in. Brownian 
motion is an additive process defined by its increments, which, provided non-
absolute integration is used, are strongly integrable. (The increments of any 
process are strongly integrable in this way.) By expressing the increments of the 
contingent process so that their dependence on Brownian increments is explicit, 
as in Ito's formula, it is possible to draw on knowledge of Brownian increments 
for information about the joint behavior of the contingent observables defined 
by / . Application of Ito's formula sometimes gives results involving some of 
the small, but important, group of stochastic integrals (both strong and weak) 
evaluated in preceding sections, for which the distribution functions are obvious. 
Ito's formula is comparable to the "integration by substitution" or "change 
of integrator" method given in Theorem 46 of Chapter 4. With T =]0,t] and 
is =]s,s'], the formula can be used to transform the stochastic integrand 
g(f;x8,s,x(ia),i) 
= f(xs>,sf) - 
f(xs,s) 
into a G-equivalent form from whose (strong or weak) stochastic integral Sj-(Xq-) 
distribution functions Fyt and/or FyT may be deducible. 
With Xj- ~ xT [R r, G], Ito's formula, in traditional notation, is 
f dYs = J f(X8, s)ds + f 
\f"{Xs, 
s)ds + j 
f'(X8, s)dXs, 
(8.19) 
often abbreviated by equating the left-hand and right-hand stochastic integrands: 
dYa = f(X8, s)ds + !/"(X e, s)ds + f'(X8, s)dXs-, 
where ys = f(x3, s); and, for w e R and s G T, 
d/(w>s) 
o,( 
x 
df{w,s) 
· 
d2f(w,s) 
„ 
dw 
= f (™> *)> 
ds 
= /(™> *)' 
dw2 
= f K *) 
(8·20) 
In the notation of this book, Ito's formula is 
fT(f(Xs,,s>)-f(Xs,s)) 
(8.21) 
= 
fTf(X.,s)\ia\+fT±f»(Xe,s)\t.\+fTf(Xa,s)X(i.). 

426 
CHAPTER 8. STOCHASTIC 
INTEGRATION 
On the right hand side the only stochastic integral is JT f'(Xs, s)X(zs), because 
this is the only term containing the factor X(zs) = Xs
f —Xs- Thus the stochastic 
integral JT (f(Xs>,sf) 
— f(Xs, s)), which has an integrator function 
g(f;X(is),is) 
= f(Xs,,s') 
- 
f(Xs,s) 
whose distribution function is unknown, is replaced by an expression involving 
an integrator function X(zs)—Brownian motion increments, whose distribution 
function is known. This is the "change of integrator" idea. 
Writing ys = f(xs, s), we have 
yM = ys> -ys = /(av,s') - f(xs,s) 
and 
j Y(is) = Yt-Yo 
= f(Xut) 
- f(X0,0) 
= f(Xt,t) 
- /(0,0) 
since x$ = 0 for standard Brownian motion. Therefore, provided non-absolute 
integration is used, (8.21) is 
Yt = mO) 
+ J f(Xs,s)\is\ 
+ f 
\f"{Xs,s)\is\ 
+ j 
f'(Xs,s)X(is). 
(8.22) 
And provided the stochastic integral J7-//(Xs,s)X(zs) is not too complicated, 
it may then be possible to deduce, for any £, the distribution function for Yt = 
f(Xt,t). 
With luck, fT f'(Xs,s)X.(zs) 
will turn out to be some combination of 
the stochastic integrals already calculated in the preceding sections. 
In place of the variable Ys, for which distribution functions may be difficult 
to establish, Ito's formula makes it possible to substitute the better-known int-
egrator dXs for the integrator dYs. The formula differs from the non-stochastic 
change of integrator method, or "integration by substitution" (of a function of 
two variables), because of the presence of an extra term ^f,f(Xs,s)\is\. 
The 
reason for this extra term will be clear from the proof of the formula. 
8.11 
Proof of Ito's Formula 
Suppose, for the moment, that X is an arbitrary process Xj- ~ xj- [R r, FXT\, 
and suppose14 f(Xs,s) 
is a contingent observable, where / is a deterministic 
function of two variables w and s, with w EH and 0 < s < t < r. 
To accomplish change of variable or integrator in f(Xs/,s') 
— f(X3, s), apply 
Taylor's theorem, with remainder, to the function f(w,s). 
Assume that the 
(m + l)-th order partial derivatives of / exist. Suppose 0 < s < sf < t and 
χτ £ R T · Taylor's theorem states that f(xs>,sf) — f(xs, s) is equal to 
14To conform to previous usage, the s-dependence of / could be denoted by fs(Xs) 
~ 
/ s(x s)[R, Fxs] instead of f(Xs,s) 
~ f(xs,s)[H,Fxg]. 
This would be more consistent, but 
less convenient. 

8.11. 
PROOF OF ITO'S 
FORMULA 
427 
Ux..-xe)-^ 
+ (a'-a)-^)f(xa,8) 
+ ··· 
(8.23) 
··· + ±({χ,-χ.)Α- 
+ μ-8)£\ 
f{Xs,s) + Rn, 
with 
ß - = (^TT)l(^'-^^ + ( s'- s )^J 
'«·«>· 
(8·24) 
and 
ξ = χ8 + \(χ8, -xa), 
u = s + X(s' -s), 
0 < λ < 1 , 
(8.25) 
where λ depends on /, and on the values xs and xs> for is = ]s,s']. 
(The 
parameter λ does not depend on values χς for ς φ s, ς ^ s'.) With m = 1, write 
df(xs,s) 
_ 
f, 
df(xs,s) 
_ 
; 
ftr, 
d2/(£,«) 
,„ 
<9ξ2 
/ λ ' 
Then, writing ys = 
f(xs,s), 
Us' 
Vs 
= 
(Es' ~ &8 )fs 
ds 
d2m,u) 
du
2 
+ (S' - 8)ft 
d2m,u) 
d£du 
— J\, 
—57H 
— 
J\-
+ \{xs> - xs?n+(«.' - *.)(*' - s)h+w -
 s)
2h. 
(8.27) 
In other words, 
y(t.) = A*s)f: + \is\fs + H*s)2f'x+xM\is\A + hM2h> 
(o.Zo) 
Y(*.) = x(ls)/^ + ki/s + ix(is)
2/r+xwKi/; + |ki
2/A. 
The strong stochastic integral of the left-hand side exists, with 
/ y(*e) = yt-yo 
= yt- /(o, o). 
Therefore the strong stochastic integral of the right hand side exists, and 
Yt = /(o,o) + J (/.l».l + \fx*M2 + ffrM + /AX(*.)I*.I + ^AW2) · 
(8.29) 
This gives a change of variable, expressing Y3 in terms of Xs. 
It is a strong 
stochastic integral, valid for each χη- £ R r , and for arbitrary distribution func-
tions FXT in 
Xr^xT[RT,FXT}. 

428 
CHAPTER 8. STOCHASTIC 
INTEGRATION 
The first three terms of the integrand on the right-hand side of (8.29) are similar 
to those in Ito's formula (8.19)—provided X(2S)2 can be replaced by \is\ in ac-
cordance with Theorem 234, in which Xj- is assumed to be standard Brownian. 
Because of the factor \is\ in each of the final pair of terms, it may be the 
case that these two terms contribute 0 to the integral, so perhaps they can be 
removed. However, while the integral on the right exists as a strong stochastic 
integral, we cannot declare that each term in the integrand is separately integ-
rable on T. Therefore, as it stands, the right-hand integral is not likely to be 
helpful in determining the distribution function of Yt. 
Replacement of X(zs)2 by \is\ involves reference to Theorem 234, in which 
X is assumed to be standard Brownian. Therefore we now take X to be 
Xr ~ xr [Rr, G] . 
Then, with FxT = G, another version of Ito's formula is obtained. 
Theorem 245 Suppose Χτ — # T [ R T ? G ] ; suppose f(w,s) 
satisfies Taylor's 
theorem with remainder, for m = 1; and suppose f 
and f are bounded for 
w G R and s G T. If ys = f(xs, s) for xs G R and s G T, then, for t > 0, the 
contingent observable f(Xt,t) 
satisfies 
f(Xt,t) = /(0,0) + 1 (Λ|ιβ| + §/ί'Χ(ι.)2 + /;X(*.)) , 
(8.30) 
where the integral is weak stochastic. If, additionally, f(w,t) 
is continuous then 
f(Xt,t) = /(0,0) + j f(Xs,s)\is\ + I {y&iis? + f'sXM), 
(8.31) 
where the final integral is weak stochastic. 
Proof. The integral fT f(Xs,s)\is\ 
describes a contingent observable but does 
not involve X(z) and is not a stochastic integral. By Theorem 243 the weak 
stochastic integrals JT ί f'xX.(i3)\is\ 
J and JT ifx^\is\2) 
exist and equal 0. Sub-
tracting them from (8.29), and using Theorem 238, we get (8.30). Existence of 
frf(Xs,s)\ia\ 
gives (8.31). 
O 
Theorem 246 Suppose xr — %r [RT?G] i>s standard Brownian motion; sup-
pose f(w,t) 
satisfies Taylor's theorem with remainder, for m = 1; and suppose 
f and f are bounded, f is continuous, and f and f" are uniformly continuous. 
Then JTdYs = fTf(Xs,s)\is\ 
+ \ JT f"(Xs,s)\is\ 
+ JT 
f'(Xs,s)X(is). 
Proof. The hypotheses allow Theorem 244 to be used, so the weak stochastic 
integral fT fxdX^ 
exists and equals Jr f"(Xs,s)ds. 
By Theorem 238 we can 
subtract the former from (8.31), and add the latter. The final integral is weak 
stochastic. 
O 

8.12. APPLICATION 
OF ITÖ 'S FORMULA 
429 
Theorem 246 gives Ito's formula, traditionally written as 
f(Xut) 
= /(0,0) + j 
f(Xs,s)ds+\J 
f"{Xs,s)ds 
+ J 
f'(Xa,s)dXa, 
or 
dYs = f(Xa, s)ds + \f'{Xs, 
s)ds + f'(Xa, 
s)dXs 
where Ys = 
f(Xa,s). 
While Theorem 246 implicitly gives conditions for the existence of the weak 
stochastic integral of an integrand h(Xs, s)X(zs), this chapter does not attempt 
to give exhaustive existence conditions for stochastic integrals. Also, in this 
chapter functions fr(x) — f(x,Mr) 
have been treated as data of different ob-
servables fr(X) for r = 1, 2, 3,.... But problems of random variability can be 
approached in a variety of different ways; and the spirit and method of this book 
suggest that variable elements (x, N) of the class 
{{χ,Ν) : z e R r , 
NeAf{T)} 
may be regarded as potential data-values of a single observable 
f(X, N) ~ f(x, N) [ R r x λί(Τ), Fx] . 
Such an approach could evade the necessity for subtle stochastic integral proce-
dures for "averaging out" partition points N of observables taking the form of 
Riemann sums in T. Developing the theory in this direction requires some ex-
tension of the notion of measurability of sets and functions, as in the Epilogue. 
8.12 
Application of Ito's Formula 
Given Χη- ~ X7-[RT,G], cells %s =]s,sr] G I(T), and a twice differentiate 
function f(y, s) defined for y e R and for 0 < s < t, write 
/i(xs,s,x(zs),zs) = f(xs,s)\is\ 
+ |/"(a;e,s)|2e| + 
f'(x3,s)X(ia). 
Under the conditions of Theorem 246, Ito's formula states that 
S!}-(X) = f(Xt,t) 
- f (0,0). 
Without loss of generality we can write f(Xt,t) 
— /(0,0) as f(Xt,t)—or 
simply 
rename the right hand side—so that 
S!HX) = 
f(Xt,t). 
Ito's formula then states that 
Y{Uh
J:r-f)G [ R T] -^ °5 

430 
CHAPTER 8. STOCHASTIC 
INTEGRATION 
or, in other words, as r —> oo the rth-binary-Riemann-sum-observables 
1Z^r(X) 
converge weakly to f(Xt,t) 
as r -> oo. In this way, the partition points N (or 
Mr) disappear in the limit. 
In terms of the datum 1Z^r(xr) 
of the observable ΊΖγΤ(Χτ) 
{r = 1,2,3,...), 
and the corresponding datum /(#*,£) of the observable f(Xu 0> this m e a n s that 
(nh/{XT) - f(xt,t)) 4 o, 
nh/{xT) 4 f(xut) 
for the possible joint outcomes xr of the sample space R T. 
Thus, for any particular xj- € R r , the sequence of binary Riemann sums 
whose rth term is 
Khrr{xr) 
= Σ 
(/(*i-i,*i-i)l*JI + £Α*;-ι,*;-ι)|χ5Ι + /'(*,·-ι,*;-ι)χ(ι;)) 
will generally not actually converge as r -» oo. (Recall that Sj = j2~rt, Xj = 
x(j2~rt), 
irj =](j — l)2 - rt, j2~~rt], x(«p = Xj — Xj-i·) 
But, loosely speaking, 
for any "large" value of r, and for "most" of the possible joint outcomes £7-, 
the Riemann sum will "usually" add up to a value that differs "not too greatly" 
from the corresponding datum value f(xt,t). 
And as r increases this behavior 
becomes "more pronounced". 
This can be expressed in more formal terms, as follows. 
Under the hypotheses of Theorem 246, the functions !Z^r(x) 
and 
f(xt^t) 
are G-measurable. Therefore, by Theorem 78, for each r the elementary form 
Yr of the contingent observable ΊΖ^(Χψ) 
— f(Xt,t) 
has distribution function 
Fyr. The preceding discussion can be summarized by the statement that, for 
any J — ]u, v], u < 0 < v, the datum 
yr=nh/(xT)-f(xut) 
satisfies 
Fyr{J) 
= 
C[(u<yr<v)) 
-+ 1, 
Fy r(R \J) 
= 
C \{yr < u or v < yr)] 
-> 0 
as r —> 00. That is what Itö's formula says. The point is illustrated numer-
ically and graphically in Section 9.5 of Chapter 9. The following examples are 
intended to illustrate further the meaning of the formula. (The process Xq- is 
assumed to be standard Brownian motion.) 
Example 66 Suppose a and b are constant and f(y, s) = ay + b for y £ R; 50 
/Ü/, s) = 0, f'(y, s) = a, f"(y, s) = 0. Then 
f f'(Xs, s)dXs = f adXs = a f X(is) = a(Xt - 0), 
J1 
J 7~ 
J T 
and ltd 's formula reduces to 
aXt + b = b + aXt. 

8.12. APPLICATION 
OF ITO'S 
FORMULA 
431 
Example 67 Suppose f(y, s) = asy + bs for 0 < s < t and for y € R. Then 
f(y, s) = ay + 6, /'(?/, s) = as, /"(y, s) = 0; ί/ie left-hand side of Ito's formula 
is 
atXt + to; 
and £ae right-hand side is 
bt+ 
aXs\is\ + \ x 0 + / asX(zs), = bt + a 
(Xs\is\ + sX(ie)). 
5mce X zs standard Brownian motion Theorem 229 confirms that the latter 
integral exists as a strong stochastic integral with value tXt, and this instance 
of ltd 's formula is thereby verified. The elementary version of the contingent 
observable atXt + btisY~ 
2/[R+, Fy] where y = at + btxt and 
FY(J) = N°'1(}u',v'}) 
/or J =]«,«] GI(R), 
u' = ^
, 
v' = 
^
, 
at 
at 
where N 0 , 1,= N, is the standard normal distribution function with mean zero 
and standard deviation 1. 
O 
The following application of Ito's formula is used in mathematical finance. 
Example 68 Given standard Brownian motion Xq- and constant σ, find a con-
tingent observable f(Xs,s), 
s GT, satisfying the stochastic integral equation 
f(Xu t) - f(X0,0) 
= σ j 
f(Xs, s)X(%s). 
(8.32) 
(Since X is Brownian, f(Xo,0) = /(0,0)J Traditionally (see, e.g., Baxter and 
Rennte [9]), this is formulated as the "stochastic differential equation" dZs = 
aZsdXs, 
to be solved for Z. Using Ito's formula, the solution is found to be 
Zs = f(Xa, s) = exp (σΧ8 - \a2s) , (s G T). 
(8.33) 
The equation dZs = aZsdXs is an abbreviation of 
j dZs=a 
j ZsdXs, 
or Zt - Z0 = σ J 
ZsdXs 
since dZs is strong stochastic integrable, and this is equivalent to (8.32). To 
verify that (8.33) is a solution to (8.32), apply Ito's formula to the function 
f. 
Differentiating f(y, s), 
f = -\°2f, 
/' = */, 
/" = σ2/, 
and Itö 's formula gives 

432 
CHAPTER 8. STOCHASTIC 
INTEGRATION 
f(Xu 
t) = /(0,0) + j fds + \ j fds + j 
f'dXa 
= 
/(Ο,Ο)-^σ 2 J fds+\a2 
J fds + aj 
fdXs 
= 
/(0,0)+(j / 
exp(aXs-±a2s)dXs 
which is (8.32). 
O 
The latter equation, 
f(Xut) 
- /(Χο,Ο) = σ j exp (aXs - ±σ2
5) <2XS, 
(8.34) 
can be re-written as follows. For is =]s, s'] G I(T), let 
/ii(x(2e),2e) 
= f{xs',s') - f(xs,s) 
= 
aexp (σαν — ^cr2^) — aexp (σχ5 — \cr2s) , 
^2(x(«s),^) = crexp (axs - \&2s) x(zs) 
(8.35) 
= 
aexp (σχ3 — ^σ2^) (av — x s), 
/i(x(zs),2s) 
= 
fti(x(2e),2e) 
- Λ2(χ(«β),*β). 
The function /*i is strong-stochastic integrable, with 
/ fti(x(2e),ze) = /(xt,*) - / ( z o , 0 ) = aexp(ax t - |σ 2ί) - 1. 
Then (8.34) is equivalent to any of the following: 
S^(XT) 
= 0, 
n^r(xr) 
4 0, 
lim V ^ , . G [ R r ] = 0. 
(8.36) 
The latter representation of (8.34) is illustrated numerically and graphically in 
Section 9.5 of Chapter 9. 
The following is the form in which Example 68 sometimes appears in math-
ematical finance. With μ constant let 
f{y, s) = exp (ay + (μ - \a2)s) , 
so 
/ = ( M-ia 2)/, 
f' = af, 
f" = a2f; 
and, by Itö's formula, 
f(xut) 
- / ( 0 , 0 ) = μ 
f(xs,s)\is\+a 
f(xs,s)x(is). 

8Λ3. DERIVATIVE 
ASSET 
VALUATION 
433 
In traditional notation, and writing Zs — exp (σΧ3 + (μ — ^σ2)δ), this is 
/ άΖ3=μ 
Zsds + σ 
ZsdXs, 
which, in turn, has traditional "stochastic differential equation" versions: 
dZs = μΖ8άβ + aZsdXs, 
—^ = μ ds + adXs, 
d (In Zs) = μάδ + σ dXs. 
Zs 
This is the standard or traditional method (see, e.g., Baxter and Rennie [9]) 
by which Ito's formula is used to establish that a process Zq- = (Zs)se-j- whose 
logarithm is drifting Brownian motion, with growth rate μ and variance rate 
σ2, is given by 
Zs = exp (σΧ8 + (μ - ^a2)s) . 
In fact, without reference to Ito's formula, this has already been established in 
Sections 7.2 and 7.3. The key to the latter approach is representation of random 
variables in their contingent form f(X,N) 
~ /(#, N)[HT, ί χ τ ] , so that we do 
not have to take limits of Riemann sums in the time domain T in advance of 
carrying out analysis of random variability in the process domain R7\ A fuller 
development of this approach would require the ideas presented in Section A.l. 
8.13 
Derivative Asset Valuation 
In the following sections some basic financial and probabilistic ideas concerning 
the pricing of financial assets are outlined, with a view to expressing them 
in Riemann sum terms. This approach is illustrated by an explanation of the 
Black-Scholes formula for the price of a European call option, using the Riemann 
sum methods of this book, and without having to invoke the stochastic integrals 
of the preceding sections. 
First consider the calculation of compound interest, and the related issue 
of the discounted value of money. If an amount a of money is invested at a 
fixed rate p (usually an annual rate) of compound interest for a period of time 
of duration At (usually measured in years), the final value of the investment is 
aexp(pAt). 
Suppose the rate of interest varies step-wise over successive time 
intervals, so, for r0 < T\ < · · · < r m = r, we have 
p(t) = pj 
for Tj-i <t 
<Tj. 
Then a sum α(το) of money invested at time To will, at time r, have value 
a(r) 
= 
(((a(To)e^ri-T°)) ep^T*-T^) 
· · ·) e^T™-T™-^ 
= 
a{r0) exp fe^Li Pj-i (rj ~ Tj-i)) 
· 

434 
CHAPTER 8. STOCHASTIC 
INTEGRATION 
If p(t) is a continuous15 function of t, then, with continuous compounding, 
a(r) = a(ro) exp I / 
p(s)ds 
a{t) = a(t')exp[ 
/ p(s)ds). 
(8.37) 
For any t' < £, 
Conversely, a sum a of money accruing at time t is, in these terms, equivalent 
to a sum 
/** 
a exp (-J 
ρ{8)ώ?) 
(8.38) 
accruing at the earlier time £'. We say that the discounted value of a(t), dis-
counting from time t to earlier time t', is 
a(t') = a(t)exp (- 
/ p(s)dsj . 
(8.39) 
If the latter amount is invested at time t' at a compound interest rate p(s), 
continuously compounded, its value at the later time t will be a(t). 
As presented here, the function p{t) does not depend on any random data 
x(t), and can therefore, for present purposes, be considered to be a deterministic 
function. For illustrative purposes we will often take p to be a fixed positive 
value. Call p the risk-free interest rate. 
The next step is to consider financial variables that are not deterministic, 
and whose values vary randomly or unpredictably at successive instants t of 
time. 
Suppose zt,= z(t), is the value at time t of a particular financial asset, such 
as the observed market price of a share, as recorded in a stock exchange. With 
p constant, the value of the share, discounted from time r to an earlier time t, 
is Z(T) exp (—p(r — t)). 
Taking time t = 0 to be the present, and T =]0,r], a claim at time r on 
this asset is a contract, involving some transaction in the asset, which entitles 
the holder of the contract or claim to a financial benefit (payment or award), 
received at time r, whose amount depends in some deterministic way on the 
observed market values 
(zt)ter 
of the asset. This entitlement can itself be traded at times £, 0 < t < r; 
and the purchaser of the entitlement or contract then acquires the claim on 
the underlying asset. As a tradeable entity, possessing monetary value, the 
entitlement is itself a financial asset. Accordingly, the entitlement is called a 
derivative asset, or derivative. 
Step-wise variation is more realistic. 

8.13. DERIVATIVE 
ASSET 
VALUATION 
435 
Example 69 European call option: Suppose the underlying asset is a share, 
whose market price at time t is denoted z(t) or Zt. Suppose the owner of the 
share makes a contract at time t = 0 with another party, the terms of the contract 
entitling the counter-party to purchase the share for a declared sum κ, at time 
T > 0. The counter-party thereby possesses the option (i.e., the right, but not 
the obligation) to purchase the share at time τ, for the price n, regardless of the 
actual price z(r) of the share on that date. This is the claim on the underlying 
asset; that is, the claim that the holder of the option has against the holder 
of the share. If, at time r, the market price z(r) of the share exceeds κ, then 
the value of the claim to the counter-party in the contract is z(r) — κ; because 
the share can be purchased for an amount κ, and re-sold immediately for the 
greater amount z(r). On the other hand, if z(r) < κ, then the contract is of no 
advantage to the counter-party, and the option to purchase the share for price κ, 
is not exercised. In that case the claim has value 0 to the counter-party. 
Thus, 
at time r, the claim has a monetary value, to the counter-party, amounting to 
W(T) = max {z(r) - κ, 0} . 
(8.40) 
The holder of the option or claim possesses a certain advantage against the 
owner of the share. To acquire this contractual advantage at time t = 0, the 
counter-party to the contract must pay the owner of the share an amount w(0); 
and then the "advantage" or option can, at any time t, 0 < t < r, be sold on to a 
third party for an amount w(t). It seems that w{t) must be related in some way 
to z(t). 
Obviously, w(t) < z(t), because a right to purchase the share at some 
time in the future cannot cost more than the current value of the share. Since, by 
assumption, z(t) varies randomly or unpredictably, it would seem that w(t) must 
vary randomly. However, (8.40) shows that w(r), though random, depends on 
the random ζ(τ), but in a deterministic way. So if z(r) is a potential occurrence 
or datum of an observable Zr ~ zT [R+, F^J, then wr is a potential occurrence 
of a contingent observable Wr. We wish to show that, under certain conditions 
and assumptions, there is a deterministic function giving the dependence of w{t) 
on z(t) for 0 < t < r, a function that reduces to (8.40) when t — r. This is the 
Black-Scholes formula (8.54) for the price, at time t, of a European call option. 
In the case t = 0 the Black-Scholes formula gives the initial cost (payable, by the 
counter-party, to the holder of the share or underlying asset) of the European 
call option on the share. 
O 
To establish this formula for the value w{t) of the derivative asset described 
in Example 69, here is an outline of some financial principles and methods of 
reasoning. 
Consider an asset whose value ß(t) at time t varies in accordance with (8.37) 
above, with tf = 0; so with p constant, 
ß(t) = /3(0)e<*. 
For t e T = ]0,r], let φξ(β(ή) = ß{t)e~/o 
p{s)ds as in (8.39), so with p constant, 

436 
CHAPTER 8. STOCHASTIC 
INTEGRATION 
#(/?(*)) = ß(t)e-<* = ß(0). 
Since φζ(β(ί)) is constant for t £ T it can be regarded as a martingale, no 
matter what distribution function applies. Technically it can be regarded as an 
observable—but of a degenerate kind, and not variable, either in the random or 
deterministic sense. In other words, given any distribution function FT, write 
Yt = ht(XT) 
~ ht(xT)\Rl,FT] 
with yt defined by yt = ht(xT) = 
φξ(β(ί)), 
satisfying 
E \Yt,] = E[hf(XT)] 
= E MXT)} 
= E [Yt] = β(0) 
(8.41) 
for all i, t' e T. 
The discounting function φρ can be extended as follows. For 0 < t' < t, 
tf,t(/3(t)) = /ϊ(ί)β-'<«-*'> = 0(f), 
^t(/3(i)) = ß(t)e~ X' "(5)ds = /3(ί'), 
for constant and variable p, respectively. (It is assumed that, if p(s) is variable, 
its integral exists.) 
8.14 
Risk-Neutral Pricing 
This section gives an outline of some further financial ideas and arguments 
which, under certain assumptions, imply the Black-Scholes pricing formula 
(8.54) for the European call option of Example 69. (Detailed exposition of the 
financial reasoning involved can be found in, for instance, Baxter and Rennie 
[9] andlngersoll [111].) 
With T =]0, r] and R+ =]0, oo[, suppose ζτ = (zt)teT 
represents the pot-
ential joint values (or prices) at times t of an asset; where, for the moment, 
we regard t = 0 as the "present". If these joint values are occurrences of an 
observable, by definition this means that there is a potentiality distribution 
function Fz and an observable Z with 
Z = ZT*zT[Rl,Fz]. 
(8.42) 
For the moment it need not be assumed that such a distribution function Fz is 
actually known. 
Now suppose there is a derivative asset, based on the potential underlying 
asset values zt, whose potential joint values are denoted WT = (w*)tGT. Suppose 
the potential derivative asset value wT depends in a deterministic way on one 
or more of the potential underlying asset values, so 
WT = fr ((Zs)0<s<r) 
= fr(zT). 
(8.43) 
An example of this is equation (8.40) of a European call option, in which the 
potential terminal (t = r) values of the option depend deterministically on the 
potential values of the datum zr. 
It would not be too surprising if random changes in the value zt of the 
underlying asset were tracked by, or reflected in, corresponding changes in the 

8.14. RISK-NEUTRAL 
PRICING 
437 
values wt of the derivative asset. In that case, it may be possible to combine 
quantities—positive and/or negative—of the underlying asset and the derivative 
asset in such a way that the random variability in the value of one cancels out 
the random variability in the value of the other. 
Accordingly, at any time t, suppose such a combination (or portfolio) of 
assets is composed of pt units of the underlying asset zt and qt units of the 
derivative asset wt. This gives a combined asset value (or portfolio value) of 
7Γ* =ptzt 
+ qtwt. 
At each instant of time £, the quantities pt and qt are chosen so that the portfolio 
value 7Γ£ is not subject to any random variability, and is therefore predictable, 
or riskless. Under certain conditions (see, e.g., Baxter and Rennie [9] or Hull 
[110]) it is possible to do this because of the way the underlying asset value is, 
in broad terms, reflected in the derivative asset value. 
An argument from finance (impossibility of arbitrage or riskless profit, or 
no-arbitrage) then tells us that 
■ the value of the riskless portfolio 
of the combined assets 
changes, 
or grows, at the risk-free 
interest 
rate p. 
This means that the discounted portfolio value 
is constant for t G T; and is therefore a martingale in the degenerate sense of 
(8.41). 
It is supposed that a distribution function F can be found for the domain 
R+ such that the discounted potential joint asset values (zt)teT are joint data 
values of a martingale Zt; so we may write 
E*[#(Zt)],= E* Zte-fo^ds] 
= [zte-fo^dsF(I[N]),= 
z0 
for all t e T. This implies that, in F-mean, the asset prices zt increase at the 
risk-free rate p. 
Furthermore, the discounted joint derivative values (wt)ter form a F-mart-
ingale Wt, and we may write 
E*[tf(Wi)],=E* 
Wte ■ Jo p ( s ) d s 
, = wo 
for all t G T. In other words, in F-mean, the claim value wt increases at the 
risk-free rate p. 
The important case is t = r, because (8.43) gives a deterministic function 
fT connecting the potential value wT with the potential value ζτ· We have 
Ερ[φΡ(πτ)], = Ε* WTe ■ fo P^ds 
w0. 

438 
CHAPTER 8. STOCHASTIC 
INTEGRATION 
Then, by (8.43), 
w0 = Ep [Φ? (WT)] = / 
fT(zT)e-fo^dsF(I[N}). 
(8.44) 
For 0 < τ' < τ let Τ' denote ]τ',τ] and let FT' be the restriction of the distrib-
ution function F to I(R+ ). Now assume r' is the "present", so the asset values 
zs are no longer merely potential prices but are known for 0 < s < τ'. Then, as 
before, for τ' < t < r, 
Ε^' [#,t (Zt)}, = Ep*' \zte~C »W] = J 
Ζφ~ C ^d°FT,(/[TV]), = zT, 
because of the martingale characteristic of F. (In this case, I[N] denotes cells 
in R+ , not R+.) Similarly, 
wT. = Ε^' ψτ,τ (WT)] = j 
fT(zT,)e-f>{s)dsFT,(I[N}). 
(8.45) 
8.15 
Comments on Risk-Neutral Pricing 
The introduction of F amounts to a "thought-experiment", an artificial con-
struction or device that allows us to postulate observables ZT and WT whose 
discounted values are F-martingales, thereby making it possible to perform the 
calculations (8.44) and (8.45). 
In analogy, it is conceivable that an inverse cube law of gravity, or some 
other intellectual conception of gravity, could be postulated instead of the actual 
inverse square law, in order to prove some point about planetary motion. This 
is not to deny that actual planetary motion is accurately described by Newton's 
inverse square law. However, the thought-experiments of the pre-Newtonian 
astronomer Johannes Kepler envisaged complex arrangements of cosmological 
spheres and polyhedra guiding the paths of heavenly bodies; and this geometric 
reasoning produced quite good estimates of the motion of some planets. 
As a further illustration, suppose a fair and balanced coin is tossed a hundred 
times. It is possible—but most16 unlikely—that a sequence of a hundred heads 
occurs. However, if a different probability distribution is postulated for the 
experiment, one that assumes the coin is engineered so that it almost always 
falls to one side only, then a sequence of a hundred heads is not unlikely. But 
this is a thought-experiment, an imagined scenario contrary to reality, since the 
coin is actually fair and balanced. 
The risk-neutral pricing argument requires that we find 
■ a distribution 
function 
F which generates 
discounted 
data 
values 
(ßt(zt) that are occurrences 
of a 
martingale. 
16This particular outcome is no more unlikely or improbable than any other. And it is 
practically certain that some such outcome will ensue in consequence of tossing the coin in 
this fashion. 

8.15. COMMENTS ON RISK-NEUTRAL 
PRICING 
439 
What does this mean? Suppose there exists an actual distribution function 
FzT which fairly accounts for the likelihoods of potential asset prices. Assume 
further that we have a martingale distribution function F for the discounted 
price values. 
We then have two observables ZT and ZT with identical joint data values 
ZT ~ zT [Rl, Fz] , 
ZT ~ zT [R£, F] ; 
the first being the actual asset price observable and the second one a thought-
experiment for which the discounted values 4>t(zt) are potential occurrences of 
a martingale φΡ(Ζ). 
Suppose time t = 0 is the "present", and suppose we calculate the F-
expectation of the asset price zu discounted to the present, at some future 
time t. By the martingale property, 
Ερ[φ?,(Ζτ,)]=ζο. 
But when time t arrives, the actual observed value zt need not satisfy 
φρ
τ, (zt) = z0. 
As a thought-experiment, this relationship need only hold in the F-mean. And, 
of course, the actual distribution function Fz regulating the actual price occ-
urrences—if it exists and if it can be found—need not satisfy E \φρ
τ, (Ζτ>)\ = ZQ 
(ίβ.,Ερζ[φ?,(Ζτ,)] 
= ζ0). 
Now suppose r' is the "present" (0 < τ' < r), so the actual asset prices 
zs are known for 0 < s < τ'. When, for any t (rf < t < r), we calculate the 
F-expectation of the potential asset values zt discounted to time τ', we get 
EP[K,t{Zt)\(za)o<s<T>]=Zr,. 
In reality, dealing with an actual asset and any related derivative asset, the 
following questions arise: 
■ whether such a distribution function F actually exists; 
■ whether it can actually be specified; and 
■ whether the derivative pricing calculation (8.45) is valid. 
In principle, discovering F seems to be a task comparable to finding Kepler's 
cosmological spheres. But in the next section an argument is presented for it. 
To conclude this section, here is a summary of the main points of financial 
theory needed. A distribution function F on I(R+) is posited so that, for t £ T, 
Ft(It) = FT [lt x R ^ { t } ) 
for each It e I(R+). Also, for 0 < t' < t < τ, contingent F-observables 
<#, t(Z t)~<#, t(2 ()[R +,F (] 

440 
CHAPTER 8. STOCHASTIC 
INTEGRATION 
are posited, satisfying 
E[#,t(Zt)|*t,],= / 
<t>p
t(zt)dFu=zt, 
for all tf,t G T. Furthermore, there is a riskless portfolio of pt units of the 
underlying asset and qt units of the derivative asset whose value 
Mter 
= (Ptzt + qtWt)teT 
grows at the risk-free rate p, so its discounted values φζ (nt) form a (degener-
ate) martingale for all distribution functions. The discounted derivative values 
φ% (wt) are data values of an F-martingale Wt, t G T. Using the deterministic 
relation 
WT = fr ((Zs)o<s<r) 
= 
fr(zT), 
and taking p constant, we then find that 
wo = e~^Ep 
[fT (ZT)} = e~»T [ 
fT (zT) F(I[N]); 
(8.46) 
and, writing V = ]i,r], 
Wt = e-^T-^Ep 
[fT (ZT,) \zt] = β-"(τ-*) ί 
fT (zT,)F(I[N\) 
(8.47) 
with J[iV] G l ( R ^ ) . 
8.16 
Pricing a European Call Option 
These general ideas on derivative valuation are now applied to the problem of 
pricing the European call option described in Example 69. 
A key step in the financial arguments of the preceding sections is the thought-
experiment requiring us to find a distribution function F such that the data-
values φζ(ζί) generated by an observable φζ(Ζί) — Φ%(ζί) [R+,^ί] are occur-
rences of a martingale under the new distribution function F. 
On the face of it, this would appear to be a difficult task. In the literature 
of the subject [9, 58] a first step in finding a solution is to assume17 that the 
joint asset data-values (zt)teT 
are occurrences of a geometric Brownian motion. 
The pricing strategy of Sections 8.14 and 8.15 then proceeds on this basis. 
Thus it is assumed that the asset values ζτ are generated by an observable 
ΖΓ-*Γ[Βί,0£σ]> 
(8·48) 
where Qi^7 is the geometric Brownian or lognormal distribution function (7.9) 
of Section 7.3, giving constant asset value growth rate μ and constant asset 
For investigation of this assumption, see Section 9.9. 

8.16. PRICING A EUROPEAN 
CALL OPTION 
441 
value volatility σ. (The notation used in (7.9) is Gca,= G^°i, but for present 
2 
purposes the parameter c will remain equal to — \; and since the set T of the 
domain R+ may be variable it is more convenient to adopt notation Gj? for 
the distribution function.) For variable growth and/or volatility, (7.9) must be 
amended appropriately. Other amendments are necessary for the growth rate, 
and/or the volatility, to be stochastic (or, in other words, dependent on Zt—or 
on some random variables or processes other than or additional to Zt). 
The basic financial hypothesis is that the underlying asset prices zt are gen-
erated by a distribution function Fz with 
FZ{I[N\) = Q^ (I[N\) = 
g^(I[N\) 
2 
for I[N] G I(R+). This hypothesis implies that: 
■ the asset price observable ZT has growth rate μ and volatility σ; 
■ the increments lnZt — \wZt> are independent and normally distributed.18 
According to the financial theory outlined in Sections 8.13, 8.14, and 8.15, 
what is needed in order to apply the no-arbitrage risk-neutral calculations (8.44) 
and (8.45) is to find a distribution function F such that the discounted asset 
price observable </>£(^) is an F-martingale. 
Theorem 194 of Section 7.3 provides such a distribution function. For sim-
plicity assume that p, μ, and σ are constant. All we have to do is change μ to p 
in £/^σ, giving QT°. When this modified distribution function is applied to the 
discounted asset values </>£(ζί), = zt exp(—pt), Theorem 194 shows that, with 
T =]0,£] and taking Fj- = G^-> the resulting contingent observables 
e-^Zt-e-rtztlRliGF] 
(8.49) 
form a martingale. 
The next task is to determine the initial value w$ of the European call option 
of Example 69, whose value at time r is given by (8.40), 
WT = ίτ{ζτ) 
= fr(zr) 
= m a x {zT ~ ft, 0} , 
where zT, = p r(z^), is a random occurrence or datum of the joint-basic observ-
able 
ZT~*r[R*:,0£"]. 
However, the financial reasoning outlined in Sections 8.13, 8.14, and 8.15 means 
that, for the purpose of calculation, we must engage in the "thought-experiment" 
in which zT, = ρτ(ζτ), 
is a potential random occurrence of a different joint-basic 
observable 
ZT~zT[Rl,GT
a]. 
(8.50) 
These assumptions are based on an economic doctrine—the efficient market 
hypothesis— 
rather than factual data. They are reviewed in Section 9.9. 

442 
CHAPTER 8. STOCHASTIC 
INTEGRATION 
Using (8.46) with constant p, and noting that wT depends only on zt for t = r, 
(8.44) gives 
w0 
= 
Ep[tf(WT)]=E*[<l4(fr(ZT))] 
= 
e-' T 
/ 
MZT)QF{I[N]) 
[ 
max{zr 
- 
K,0}G!!?(I[N]). 
= 
e 
Since max {zT — κ,, 0} is a cylinder function the geometric Brownian analog of 
Theorem 159 can be applied to the latter integral; and, referring to (7.9), we 
find that wo is equal to 
1 
Γ 
~ ( l n ^ ~ \lnz° 
~ \PT ~ ^Γ ) ) ) 
dz 
σν2τπ- 7 R + 
2σ2τ 
ζτ 
(8.51) 
If ί is the present, so the data-values zj- = (zs)0<s<t 
are given, then a similar 
argument using (8.45) gives 
wt 
= 
Ep[<&(fT{ZT))\zT] 
= 
e-^-Q 
f τ τ max {zT - κ, 0} 
G^r(I[N\) 
J R , 
= 
e~p{T-
T ^ 
. 
/ 
max {zT - K, 0} 
σ^2π(τ 
-1) JR+
 
X 
* 
2 
- ( l ^ -(■„;,- {„ir - t) - dpa))) 
fe 
Using (7.5) of Lemma 33, (8.51) and (8.52) give, respectively, 
(8.53) 
and 
Wi = ,,N.,f'°» + W ( ^ A 
_ «e-Pir-W.i ^ ^ 
+ ( P - J ^ ) ( r - « A 
( 8 5 4 ) 
y 
Gy/T-t 
J 
where N0'1, = N j i ? denotes the standard normal distribution function. This is 
2 
the Black-Scholes formula for the value of a European call option. 

8.17. CALL OPTION AS CONTINGENT 
OBSERVABLE 
443 
8.17 
Call Option as Contingent Observable 
Section 7.2 shows that a geometric Brownian joint observable with growth rate p 
and volatility σ presupposes an underlying drifting Brownian motion with drift 
rate p— | σ 2 and variance rate σ2. In (8.48) the representation of the underlying 
asset price process or joint-basic observable ZT is such that the individual asset 
price observables are lognormally distributed, with representation, in elementary 
observable form, 
as described in (7.7), so 
E [Zt] = pt. 
However, this is not the usual approach. Most presentations of this subject use 
a stochastic integral representation of the asset price: 
exp ( β Γ 4 < T V ( * T ) ) * exp ( s f i < j V ( * r ) ) [RT, G0·1] . 
(8.55) 
In this representation the asset price is a contingent observable, dependent on 
an underlying standard Brownian motion Χψ. The contingency is expressed in 
■ the deterministic calculation of a stochastic integral, and 
■ taking the exponential of the result—also a deterministic calculation. 
Assuming p and σ are constant and XQ = x(0) = 0, (8.55) reduces to 
e(p-^)t+aXt 
„ e(p-i* a)t+**t [ RT ? £0,1] 
(g^g) 
Writing 
Zt — β(ρ-έ σ' 2) ί+ σ χί 
for 0 < t < r, it is easy to confirm that the contingent—or stochastic integral— 
representation of the underlying asset gives the same result (8.53) for the Eu-
ropean call option value; and likewise for (8.54). To see this, re-calculate the 
distribution function C [(zT G J r)], where JT G I(R+), subject to the change of 
variable.19 Writing f(zT) = σ~ι (lnzT - (p - ^σ 2)), we have 
i 
/ 
1 2 \ 
, 
l n z r - (p- 
\σ2)τ 
lnz r = Ι ρ - - σ ' \ τ + σχτ, 
xT = 
^—^—'—, = 
f(zr). 
Since 
1JT(*T), 
= ljT ( e ( " - i ' a ) T + ~ T ) , = 1 / ( J T ) ( X T ) 
is cylindrical for ZT G R+, χτ G R T, Theorem 159 implies 
19Bear in mind that in this case C [{zT G JT)] does not have the supposedly objective "real-
world" meaning that we have ascribed to £, since it is an element of a "thought-experiment"— 
a theoretical contrivance of financial reasoning. 

444 
CHAPTER 8. STOCHASTIC 
INTEGRATION 
'[(zTeJT)} 
= 
I 
lf(jT)(xT)G(I[N}) 
U (β("-* σ ί) τ + σ Χ τ) G(I[N}) 
L 
RT 
xT€RT" 
1. 
/[W]6I(RT) 
! / l n z T - ( P - i ^ ) r \ 
\ 
σ\/2πτ 
zT 
so Z r is lognormally distributed, the underlying normal distribution function 
for xr being N ^ " ^ 2 ) ^ ^ . (Note that, by the continuity of G, Theorem 74 
implies that P [(zT G Jr)] is a distribution function.) Prom this point onward 
the derivation of (8.53) proceeds as before; likewise for (8.54). 
8.18 
Black-Scholes Equation 
The integrand in (8.52) is 
φζΤ (fT (ZT)) FT\r(I[N}), = e~ Γ p(s)dsfT (zT) 
FnT(W}). 
This is conditional upon the given value zt which we denote by ζ. Writing the 
integrand as h(C,t), (8.52) is 
wt = w(C,t)= 
/ 
ft(C,i). 
Using the method of Theorem 220 it can be proved that 
MC,*) 
, n? dw(C,t) 
l . / t » ( ( , t ) _ 
- ^ - - f ^ — _ _ + _ σ Ζ ( _ _ _ _ _ ^ ( ζ , ί ) , 
(8.57) 
the Black-Scholes partial differential equation. The method of proof is to show 
that h((,t) satisfies a similar equation, and then use Theorem 61 (the dominated 
convergence theorem) to reverse the order of differentiation and integration. 
Since no new ideas are involved in this the proof is omitted here. In Muldowney 
[166] this result is proved for variable p and σ. Some computational errors are 
corrected in Muldowney [170]. 
In Bonotto et al. [17, 18] a version of this is given for a system in which the 
underlying geometric Brownian process ZT is geometric Brownian except at a 
given time f G T, at which time a given real value (or "impulse") a is added to 
zt, with appropriate modification Οψ of the distribution function. 
The resulting process 
ZT — ZT \ R+, Q£σ' 

8.19. CONSTRUCTION 
OF RISK-NEUTRAL 
MODEL 
445 
is then designated as geometric Brownian with impulsive action. 
A further development on these lines would allow f and/or a to be random 
occurrences, or observables subject to some empirical or analytical distribution 
functions. 
8-19 
Construction of Risk-Neutral Model 
In the conventional presentations such as Baxter and Rennie [9], the asset price 
process is constructed from a probability space (Ω, A, P) by means of a random 
variable 
X : Ω *-+ R T, 
such that A* is a standard Brownian motion. A stochastic integral construction 
S is then used to produce a geometric Brownian motion 
Ζ = δμσ(Χ), 
Ζ ι Ω ^ Κ ^ , 
with volatility σ and "real world" growth rate μ. This corresponds to the real-
world asset price model discussed earlier. But risk-neutral arbitrage pricing 
requires construction of a probability measure Q in a probability space (Ω, A, Q) 
which generates a geometric Brownian motion 
Z = Spa(X), 
Ζ : Ω ^ Κ ^ , 
with volatility σ and risk-free growth rate p—the risk-neutral "thought-exper-
iment" model. The construction is complicated, requiring use of the Radon-
Nikodym theorem, Ito's formula for stochastic integrals, and the Cameron-
Martin-Girsanov theorem. See Elliott and Kopp [58], for example. Deducing a 
risk-neutral probability measure Q from "real-world" probability measure P in 
the abstract domain Ω is quite complicated and delicate. 
This outline of the conventional construction relates to the Riemann sum 
theory of the preceding sections in the following way. For i" = I[N] G I(R+), 
P (Z-1 (I[N})) = P (x-1 
((<Sn _ 1 (I[N]))) 
= QF(I[N])· 
Likewise, Q (Ζ~λ {Ι[Ν})) = 
Q^(I[N\). 
In contrast to the traditional theory, the Riemann sum method takes R+ 
as the sample space, and only requires us to define the distribution function 
0 on the cells I[N] of R+, and not on measurable subsets of R/£ or Ω. The 
function Q has an explicit and relatively simple analytical form of dependence on 
the cells I[N]—which, in turn, have a straightforward analytical representation. 
To specify the appropriate form of the function Q, all that is required is to 
directly insert the appropriate parameter μ or p in Q. This simplifies the subject 
considerably. 
The key point is that, unlike the Riemann sum approach of preceding chap-
ters of this book, the traditional stochastic integral approach requires a weak 

446 
CHAPTER 8. STOCHASTIC 
INTEGRATION 
form of passage to the limit before the calculations in the process domain R+ 
can be done. In contrast, the Riemann sum approach enables these two opera-
tions to be calculated as a single calculation of limits of Riemann sums20 in the 
process domain. 
'In other words, integrals on R T . 

Chapter 9 
Numerical Calculation 
A theme of this book is that Riemann sums are an effective means of analyzing 
random variability phenomena, enabling a comprehensive theory to be con-
structed. The traditional theory of probability is based on measurable sets 
formed by operations involving infinitely many steps. In contrast, Riemann 
sums have only a finite number of terms. On the face of it, such sums should be 
relatively easy to calculate. This chapter contains a number of such calculations, 
using Maple 15 computer software. 
Numerical and empirical investigations warrant detailed study in their own 
right. But such an project is beyond the scope of this book. All that can be 
given are a few indications and pointers. 
In preceding chapters Riemann sums have been used to calculate and analyze 
measurability of sets and functions, expectation values of random variables, 
state functions of diffusion systems and quantum mechanics, Feynman diagrams, 
valuation of share options, and strong and weak stochastic integrals. 
In order to produce a robust theory, considerable subtlety has been built 
into the construction of Riemann sums. By making the Riemann sums conform 
to construction rules called gauges it has been possible to establish rules and 
criteria for sophisticated mathematical operations by which many classical re-
sults could be deduced, and also many new results which are beyond the reach 
of traditional theory. 
But the essential simplicity of Riemann sums is a constant feature. Further-
more, in the various themes discussed in this book, the expressions encountered 
in the construction of Riemann sums were the familiar polynomial, exponential, 
trigonometric, and logarithmic functions. These are not expressions of an exotic 
or pathological kind which require very delicately constructed partitions. 
In one dimension, integrals of polynomial functions can be estimated with 
Riemann sums of a straightforward kind, without having to resort either to 
the simple functions of Lebesgue theory, nor even to the δ-gauges of Riemann-
complete theory. Similarly, good estimates of the Henstock integrals of the func-
tions investigated in this book can often be obtained with regular partitions—or 
even binary partitions—of the infinite-dimensional domain of integration. 
A Modern Theory of Random 
Variation: With Applications 
in Stochastic Calculus, 
447 
Financial Mathematics, 
and Feynman Integration. 
First Edition. By Pat Muldowney 
Copyright © 2012 John Wiley & Sons, Inc. Published by John Wiley & Sons, Inc. 

448 
CHAPTER 9. NUMERICAL 
CALCULATION 
To sum up, the calculations involve finite Riemann sums. And in many cases 
the partitions involved are particularly amenable to numerical calculation. So it 
should not be a surprise that many of the steps involved in numerical calculation 
of the themes of this book can be illustrated with Maple software. 
9-1 
Introduction 
This chapter presents Maple estimates of some of the calculations encountered 
in Chapters 7 and 8. With T = ]0, t] and 
x = xr = (xs)8er
 
e R T > 
consider deterministic calculations / performed on the unpredictable or random 
elements x8 which, for s £ T, are the potential joint outcomes of an experiment 
or joint observation denoted by X—that is, 
X = χτ 
= (Xs)s€T 
e RT. 
where, for each s, Xs denotes the measurement or process of determination of 
the individual datum xs. For any potential joint outcome x, the deterministic 
calculation f(xr) 
is subject to the random variability of each xs for s £T. 
The 
random variability in the final outcome /(#r)> induced by the joint random 
variability of xs (s e T), is measured by a joint potentiality distribution function 
FXl= 
F X r , defined on the events I = I[N] for I G I ( R r ) . 
For real-valued /, a potential final outcome f(xr) 
can be regarded as a 
potential value y £H. 
Then y is the unpredictable outcome of an observation 
Y. 
And, provided / is measurable and Fx is continuous, the likelihoods of 
values y are measured by a potentiality distribution function Fy defined for 
cells J e I(R), with 
FY(J) = 
Pxof-\j). 
This is the elementary form Y ~ j/[R, Fy] of the contingent joint-basic observ-
able f(Xr), 
as described in Sections 5.2 and 5.3. 
We are primarily concerned with Brownian motion X, and, except where 
otherwise stated, will take Fx to be the Brownian distribution function G. 
A key point in these illustrations is the representation of the elementary-
form observables Y by means of histograms. Using Maple, we will, for particular 
calculations / on values in R r , demonstrate with histograms both the possible 
values y of Y and also the distribution function Fy (depending on Fx and /) 
which measure the likelihoods in R. 
Histograms are a useful way of visualizing elementary observables Y ~ 
2/[R, Fy], because they exhibit the range of potential values y € R of the ob-
servable, and they indicate the values Fy(J) of cells J C R. Constructed, as 
they are, on a finite number of cells J partitioning the domain R, histograms 
are particularly apt for representing observables in the Riemann sum frame-

9.1. 
INTRODUCTION 
449 
work of this book. Histograms capture and display the distinguishing features 
of elementary observables.1 
Of particular interest are the various kinds of calculation / discussed in 
Chapter 8. These calculations involve 
■ selection of partition points TV = {..., s, s',...} 
in T; 
■ for any potential outcome xj-, forming terms composed of differences or 
increments such as 
xs(zs) = xe(]s,s']) =xs> -xs, 
or 
g{xa{ia),ia))\ 
■ calculating Riemann sums on domain T using these terms. 
Such calculation depends not just on the potential joint occurrence xq- € R7", 
but also on elements N C T, often in the form of differences Sj — Sj-i. This 
is the joint-contingent view, in contrast to the elementary view, and developing 
the theory more fully in this direction requires ideas from Section A.l of the 
Epilogue. 
But if the Riemann sums converge for each £7-, then the limit does not 
depend on any particular N C T, and when the limit of the Riemann sums is 
taken, the outcome can be treated as a potential datum of a strong stochastic 
integral—a well defined elementary observable. The features of such a calcul-
ation are displayed in the histogram of Figure 9.4. 
When the Riemann sums do not converge, and in default of a suitable theory 
involving joint-contingent observables with variable JV, the "weak stochastic 
integral" device is available. To visualize an observable defined by this method, 
take several successive partitions, each one refining the previous one. Then, 
for each such partition, perform the Riemann sum calculation using a sample of 
potential joint outcomes xq-, and draw a histogram of the resulting real numbers. 
By examining the histograms for successive partitions it is possible get a visual 
sense of the range of possible values y of the weak stochastic integral in each case, 
along with the "shape" of the corresponding likelihood values Fy(J); and also a 
sense of how these elements change when the partitions are refined successively. 
Figures 9.6, 9.7, and 9.8 illustrate this for JTdX^ 
where X is standard 
Brownian motion. Maple allows us to create samples of thousands of Riemann 
sums, and the histogram of any such sample constitutes a visualization of the 
particular elementary observable Y ~ ?/[R, Fy] whose potential datum is 
These diagrams demonstrate what is meant by "weak stochastic integral", in 
which the Riemann sums do not generally converge for particular outcomes 
y = f{xr)· 
The same device is applied to Ito's formula in Section 9.5. 
1 Unfortunately, the corresponding joint-contingent observables f(X) 
cz f(x)[HT, 
Fx] do 
not have such a convenient and familiar visual representation in multiple dimensions. Figure 
1.3 shows the limitations, even when there are only two random variables involved. 

450 
CHAPTER 9. NUMERICAL CALCULATION 
9.2 
Random Walk 
As an introduction, random walk and Brownian path diagrams can easily be 
produced with Maple. 
The following Maple code simulates a random walk whose steps or increments 
are normally distributed with mean zero, standard deviation 0.1, and variance 
0.12 = 0.01. Provided the increments are independent, Theorem 100 says that 
the variance of the sum of the increments equals the sum of the variances of the 
increments. Therefore the unit interval T =]0,1] is partitioned by 100 equal 
intervals of length 0.01. 
Calculation 1 
restart: 
with(Statistics): 
randomize(): 
Brownianlncrements := Sample(Normal(0, 0.1), 100): 
ListBrownianlncrements := [seq(Brownianlncrements[j], j=1..100)]: 
ordinate[0] := 0: 
for j from 1 to 100 do 
ordinate[j] := ordinate[j — 1] + ListBrownianlncrements[j]: 
end do: 
for j to 100 do 
abscissa := [seq(0.01*j, j=1..100)]: 
end do: 
RandomWalk := [[0, 0], seq([abscissa[j], ordinate[j]], j=1..100)]: 
plot(RandomWalk); 
Line 1 of Calculation 1 cancels any preceding Maple code. Line 2 activates 
Maple's statistics functions, such as Sample. Line 3 resets the random number 
generation algorithm of the program. Line 4 produces a random sample of 100 
numbers, each of them sampled from a normal distribution with mean zero, 
standard deviation 0.1, and variance 0.12 = 0.01. Line 5 converts these 100 
A, 
Λ„ I\ 
ί \ f 
Λ/"\ .. J 
\ I 
V 
v 
\ r 
u 
V 
I 
Figure 9.1: Random walk with normal increments. 

9.2. RANDOM 
WALK 
451 
items into a Maple list structure, indexed from 1 to 100. Line 6 assigns 0 as the 
ordinate (vertical) of the zeroth item. Lines 7 to 9 assign the sum of the first 
j Brownian increments as the jth ordinate, for j from 1 to 100. Lines 10 to 12 
assign the value 0.01 j as the jth abscissa (horizontal coordinate), for j from 1 
to 100. Line 13 assembles 101 abscissa-ordinate pairs into a Maple indexed-list 
structure. Line 14 plots the 101 points of this list, joining each consecutive pair 
of points with a straight line segment. 
In Figure 9.1, the points are the important thing. The straight line segments 
joining up the points have no particular significance, but they are helpful in 
drawing the eye from one point to the next. 
For j = 0 to 100, the jth point of the graph is (SJ,X(SJ)), 
where s = O.Olj 
and xs is the sum of the first j random normal increments; that is, 
X(SJ) = xSj = ordinate [j]. 
Thus X\ = #(sioo) = #(1) = ordinate [100]. One particular execution of Calcul-
ation 1 produced 
xx = 0.49372516204936334. 
If the Maple program is re-run, 100 random increments produce a different 
random walk, and a different datum x\. Here is an example: 
xx = -2.268719956985433. 
Calculation 4 calculates a sample of 1000 values of x(l). The histogram confirms 
that, subject to the limitations of the sampling process (which is not truly ran-
dom) , the data values produced by this program are from a normal distribution 
with mean zero and standard deviation 1. 
The following program has 10,000 increments, each with standard deviation 
0.01, so their variance is 0.0001; giving us a partition of domain T =]0,1] 
consisting of 10,000 intervals or steps. The graph of the sample path conforms 
to the customary representation of Brownian motion. 
Calculation 2 
restart: 
with(Statistics): 
randomize(): 
Brownianlncrements := Sample(Normal(0, 0.01), 10000): 
ListBrownianlncrements := [seq(BrownianIncrements[j], j=l..10000)]: 
ordinate[0] := 0 : 
for j from 1 to 10000 do 
ordinate[j] := ordinate[j — 1] + ListBrownianlncrements[j]: 
end do: 
for j from 1 to 10000 do: 
abscissa := [seq(0.0001*j, j=l..10000)]: 
end do: 
BrownianPath := [[0, 0], seq([abscissa[j], ordinate[j]], j=l..10000)]: 
plot(BrownianPath); 

452 
CHAPTER 9. NUMERICAL 
CALCULATION 
it 
1.6 
ΙΛ 
L2 
OS 
IM 
rv 
l*HI 
yg\ Λν^ 
Figure 9.2: Sample path of Brownian motion. 
Figure 9.2 has the familiar2 jagged path shape. By using a bigger sample 
it is possible to produce a graph that looks less like the random walk of Figure 
9.1, and more like the traditional picture of standard Brownian motion. 
The graph consists of points joined by straight line segments, just like Figure 
9.1. The points have both mathematical and physical significance. The straight 
line segments joining the points have neither. The only substantial difference 
between Figure 9.1 and Figure 9.2 is that the latter has more sample points. 
With T =]0,1], each of Figure 9.1 and Figure 9.2 is a sample datum of a 
Riemann sum estimate of the strong stochastic integral 
S£(JC) = / X(ifl) = ( dXs= 
X(l). 
In each case, the partition of T is regular (since each cell has the same length), 
though not binary. The Riemann sum observable in Figure 9.1 is 
100 
100 
3=1 
j = l 
where ta = jlO - 2, and i3- =}(j - 1)10-2, 
jl0~2]. 
The observables ΤΖ^ (X) and S^ (X) have joint-contingent representation 
Ugf {X) - TV} (x) [R r, G] , 
S^-1 (X) ~ S^-1 (x) [R r, G] , 
respectively, where G is the standard Brownian distribution function. The rep-
resentation of each of them in elementary form is particularly easy to establish, 
because of the "telescopic" cancellation of terms in the Riemann sum (in every 
Riemann sum, in fact). Cancellation gives 
Κτ(χτ) 
= P(t=i)(sr) =xi= 
s(l); 
2 This is the displacement-time picture of a point χτ of the Cartesian product R T . 

9.3. CALCULATION 
OF STRONG STOCHASTIC 
INTEGRALS 
453 
and 
1φ(Χτ) 
= 
ρ{Χτ)=Χι. 
The latter has elementary form. Then, writing y — x\, we have 
X,=Y, 
y~tf[R,N]; 
where N is the standard normal distribution function N 0' 1 with mean 0 and 
standard deviation 1. 
These calculations are confirmed by applying Theorems 159 and 160 to the 
observable 1Z?^(XT)· 
They are also confirmed in Maple Calculation 4, and in 
the larger sample of 10,000 Riemann sums of 100 terms each. 
9.3 
Calculation of Strong Stochastic Integrals 
In Calculation 3 Maple's Sample command is used to select 100,000 random 
values from a normal distribution with mean zero and standard deviation 0.1. 
So the variance is 0.01. In Calculation 4 these are taken in groups of 100 values 
at a time, giving 1000 samples of size 100. If the 100 sampled values were really 
instances of 100 independent basic normal observables (which, strictly speaking, 
cannot necessarily be guaranteed by computer software) then the variance of 
the sum of each group of 100 values would be 100 x 0.01, =1. The following 
examines the output of 100,000 standard normal increments. 
Calculation 3 
restart: 
with(Statistics): 
Brownianlncrements := Sample(Normal(0, 0.1), 100000): 
dx := [seq(Brownianlncrements[i], i=l..100000)]: 
Histogram(dx); 
Line 1 of Calculation 3 gets rid of the stored results of any previous Maple 
calculations. Line 2 activates the Maple Statistics functions. Line 3 generates a 
random sample of 100,000 values of a normal distribution with mean zero and 
standard deviation 0.1. Line 4 converts this output into a Maple list whose 
individual elements are indexed from 1 to 100,000. Line 5 produces a histogram 
of these values, displayed in Figure 9.3. 
The theory of random variation can be thought of as a theory of estimation 
or approximation. Given a single datum (estimate or approximate value), it is 
not necessarily obvious what it is an estimate of; nor what is the "true value" 
to which the datum approximates. But if, as in Calculation 3, a great number 
of estimates can be produced, it may be possible to deduce the "true value". 
Calculation 3 provides 100,000 estimates. To see these numbers, just change 
the colons to semicolons in the Maple code. The program will then display a 
list of 100,000 numbers—something it may be preferable to avoid. 
What is actually needed is not such a list, but a "sense", or summary, of these 
numbers as estimates of a "true value", and that is provided by the histogram, 
which indicates that: 

CHAPTER 9. NUMERICAL 
CALCULATION 
-< 
r -J 
L 
-: -
-o.i 
U ^ 
C.2 
-
Figure 9.3: Histogram for 1000 samples, Riemann sum step 0.01. 
■ the estimates cluster around the value 0; or 
■ the likelihood that an estimate will be close to zero is much greater than 
the likelihood it will not be close to zero. 
In other words, the histogram is a summary or picture of the observable or 
random variable for which the numbers produced by Calculation 3 are data. 
Inspection of the histogram suggests that the mean value of the sample is 
zero. Most of the values lie within the range —0.35 to +0.35, suggesting a 
standard deviation of 0.1. This is confirmed by applying Maple functions to the 
sample data generated by Calculation 3 : 
Mean(dx); 
For one particular sample this gave result —0.0003180143098. 
StandardDeviation(dx); 
This gave output 0.0995857853503455, close to 0.1. Combined with Figure 9.3, 
this gives some broad confirmation that, for a large sample (100,000), Maple's 

9.3. CALCULATION 
OF STRONG STOCHASTIC 
INTEGRALS 
455 
random normal increment generator can be relied on, within reason. While 
statistical independence of the data generated by the Sample command cannot 
be guaranteed, this histogram has the appearance of a normally distributed 
observable with the appropriate mean and standard deviation, allowing us to 
proceed with due caution. 
Calculation 4 below selects 100 consecutive values at a time from the list 
of 100,000 values produced by Calculation 3, giving 1000 groups of 100 values. 
The 100 values in each group are taken to be independent Brownian increments. 
These increments are added in each of the 1000 groups, so the total of the 
increments in each group can be regarded as: 
■ the outcome of a random walk of 100 steps, or 
■ a Riemann sum estimate ΊΖ?^ (χτ) of the stochastic integral JT dxs (or 
fTx.(is)) 
of the integrand g\ — dxs on the domain T =]0,t], with t = 1; 
the Riemann sum consisting of 100 terms. 
The Maple code produces a sample of 1000 of the potential values of the ob-
servable Y ~ i/[R, Fy] where each sample value y is obtained by calculating the 
Riemann sum V . X(ZJ); 
3 
Thus the joint-contingent and elementary forms are, respectively, 
n<$ (xT) * K% (xT) [R T, G], 
y ~ J/[R, N] . 
Since the strong stochastic integral of integrand g\ exists, with value equal to 
IZj^ (Xf) fc>r every partition of T, this gives 
/ dXs ~ / dxs[Rr, 
G], 
r - 2/[R, N], 
Y= 
ί dXs = SgJ (X r). 
When Maple produces the histogram Figure 9.4 of the list of 1000 values of y, 
it can be expected to confirm the theoretical prediction of Example 59—that 
occurrences y are sample data of a normally distributed observable with mean 
0 and standard deviation 1. 
Prom the latter point of view, the Maple code in Calculation 4 produces 
Riemann sum estimates of 1000 "independent" instances of the strong stochastic 
integral 
/ dXs~ ( dxs [Rr,G]. 
The elementary form of this reduces simply to Y ~ y[R, N0,1], a normal random 
variable with mean zero and standard deviation 
t= v/lOOx (0.1)2,= 1. 
The histogram of Figure 9.4 provides some empirical confirmation of this, but 
perhaps not very convincing in visual terms. 

456 
CHAPTER 9. NUMERICAL 
CALCULATION 
Calculation 4 
for r from 1 to 1000 do 
k := 100*r: 
j :- k - 99: 
xl[r] := add(dx[i], i =j..k): 
end do: 
XI := [seq(xl[r], r-1.,1000)]: 
Histogram(Xl); 
Provided Calculation 4 is run immediately after Calculation 3, the data from 
the latter feeds into the former. Otherwise Calculation 3 must be re-run. The 
output of Calculation 4 is the histogram in Figure 9.4. 
To check the parameters of this histogram, the command Mean(XI) gave a 
result —0.03180143098 from a Maple sample, while StandardDeviation(Xl) gave 
1.00075019279754. So the mean of a sample of 1000 Riemann sums came to ap-
proximately zero, and their standard deviation was approximately 1; while their 
Figure 9.4: Histogram for 1000 sums of 100 normal increments. 

9.4. CALCULATION 
OF WEAK STOCHASTIC 
INTEGRALS 
457 
histogram is at least vaguely reminiscent of the shape of a normal distribution. 
The cancellation or "telescoping" argument of Section 8.3 establishes that 
Brownian variability of each joint-data component xs of the contingent observ-
able L· o X(zs) has no effect on the variability of Xt. In other words, L· t, X(zs) 
simply replicates Xt. This is suggested (even if only faintly) in the histogram 
of Figure 9.4. 
The fact that L· ±, X(ze) exists as a strong stochastic integral, equal to its 
Riemann sum estimates regardless of which partitions of ]0,1] are chosen, makes 
the preceding remarks unsurprising. It is obvious that the distribution function 
Fxx of the elementary observable is N 0 ' ^ , with t = 1 in this case. So the 
histogram above provides partial confirmation of the obvious. 
The differences in shape between the histograms of Figures 9.4 and 9.3 sug-
gest that data generated by Maple should be treated with caution. Randomness 
and independence are ideal mathematical concepts, and perfect manifestations 
of them cannot be expected to appear in practice. 
In order to get a better simulation, amend the code in Calculation 3 to 
produce 1,000,000 Brownian increments instead of 100,000, and then amend 
Calculation 4 to deliver 10,000 (instead of 1000) Riemann sums of 100 terms 
each. The latter produces the histogram of the Riemann sum values in Figure 
9.5. This histogram more plausibly conforms to the theoretical result; that 
if Xq- is standard Brownian motion, then, for t = 1, the elementary form of 
the strong stochastic integral fT dXs has standard normal distribution function 
N 0 , 1, with mean 0 and standard deviation 1. 
Quite large samples are used here as a practical means of compensating 
for any lack of randomness and independence in Maple-generated sample data. 
There is a penalty for this, in that the running time of the programs can be 
relatively long. 
9.4 
Calculation of Weak Stochastic Integrals 
With T = ]0,1] this section investigates numerically the weak stochastic integral 
Sq?{Xr)i — J}0 i] X(*s)2? f°r which the Riemann sum estimates do not generally 
converge whenever the partitions of the domain ]0,1] are successively refined. 
Section 8.4 based the meaning of weak stochastic integrals on rth binary 
partitions of ]0,1] and the corresponding rth binary Riemann sum contingent 
observables given by the sum of the squares of the rth binary Brownian incre-
ments. 
For any given r the rth binary Riemann sum contingent observable has 
elementary form given by 
The potential data-values ζ^ 
= Σ \ = ι x( 2j) 2 a r e unbounded above, and have 
zero as lower bound. This holds for all r. Therefore, unlike L· 1nX(2s), none 

458 
CHAPTER 9. NUMERICAL 
CALCULATION 
Figure 9.5: Histogram for 10,000 sums of 100 normal increments. 
of the observables Z^ 
= Σ ι = ι ^( zj) 2 is normally distributed. (The potential 
data-values in a normal distribution are unbounded above and below.) 
Loosely speaking, the idea behind the weak stochastic integral is as follows. 
As r —> oc the rth binary Riemann sums may converge for some, but not all, of 
the potential joint outcomes χη- of the Brownian motion Xq-. But when the set 
of rth binary Riemann sums is formed for each x-j- G R T, the Maple calculations 
below show that, for integrand #2 = x(0 2> these sets tend to cluster ever more 
closely around some target value as r increases to infinity. This "target value" 
is the weak stochastic integral of #2 on T. 
The test for weak convergence of the rth binary Riemann sums of any 
stochastic integrand #(XS) is as follows. 
■ Find the "target value"; that is, the observable k(Xr) which is to be candi-
date for the weak stochastic integral of #(XS). (In the case of L· χ, X(zs)2, 
the candidate is the constant 1.) 

9.4. CALCULATION 
OF WEAK STOCHASTIC 
INTEGRALS 
459 
■ For each r calculate the difference kr{xj-) between rth binary Riemann 
sum of #(XS) and k(xq-)\ 
K{XT) = Y,9{*$))-KXT). 
(9.1) 
■ Determine whether V^cpR/7"] —> 0 as r —>· oo. If it does, then k(Xq-) is 
the weak stochastic integral of g(X(is) on T. 
The function VkrG(S) is an outer measure—a kind of volume—defined on sub-
sets S of R r . If Sr is the set {kr(xj-) 
: # r £ RT}> then continuity of G 
implies 
V, r G(fc- 1(5 r))=V f e r G[R r] 
provided kr is also continuous; so, under these conditions, YkrG [RT] ~~> 0 
implies that the "volume" VkrG (k~1(Sr)) 
—> 0. The aim of this section is to 
produce numbers and diagrams which illustrate this. 
The distribution function of the contingent rth binary Riemann observable 
ΊΖγ-τ(Χη-) is the Brownian distribution function G. What about FZ(r), the 
distribution function of the elementary form Z^ ~ z(r)[R, FZ(r)], with z^ — 
πγ{χτγ 
Denote the inverse function of V3f by Q. Provided g is G-measurable, 
Theorem 227 then gives 
FziMJ) 
= 
C[(z^eJ)}=PXT[CR.9/(xT)€J)} 
= PXT[Q(J)} = 
JRTlQ(J)(xr)G(I{N}). 
If the stochastic integrand g is g\ = X^ s) = dXs, then, for every r, FZ(r) is 
simply the standard normal distribution function N. 
Other stochastic integrands g may be dealt with on a case by case basis, 
as in Chapter 8. But without striving3 to find analytical form for distribution 
functions F z ( r), numerical calculations can provide a sense of them. 
In the calculations below it is more convenient to use decimal rather than 
rth binary partitions. The Maple code in Calculation 5 is intended to give 
1000 Riemann sum estimates, each containing 25 terms, of the weak stochastic 
integral of dx2
s on ]0,1]. 
Calculation 5 selects 25,000 random Brownian increments, each having stan-
dard deviation 0.2 and variance 0.04. With t = 1 this is equivalent to partition-
ing the domain T =]0, t] into 25 equal steps of length 0.04. In each case Maple 
calculates the Riemann sum of integrand 
02 W = ^ , = x(2s)2; 
As previously mentioned, the extension of measurability described in Section A.l of the 
Epilogue may provide a simpler and more efficient way of dealing with this. 

460 
CHAPTER 9. NUMERICAL 
CALCULATION 
so the sample of 25,000 increments generates a sample of 1000 Riemann sums, 
each containing 25 terms. (A Riemann sum containing 25 terms is not binary— 
the closest binary partition has r = 5, giving a 25 = 32 term Riemann sum. 
Either way, the Riemann sum calculation gives an idea of how the resulting 
values are distributed.) Maple then constructs the histogram, Figure 9.6, of the 
25,000 Riemann sum values. 
Calculation 5 
restart: 
with(Statistics): 
Brownianlncrements := Sample(Normal(0, 0.2), 25000): 
dx := [seq(Brownianlncrements[i], i=l..25000)]: 
for r from 1 to 1000 do 
k := 25*r: 
j := k - 24: 
RiemannSum[r] := add(dx[i] 2, i = j . . k ) : 
end do: 
RiemannSums := [seq(RiemannSum[r], r=1..1000)]: 
Histogram(RiemannSums); 
This histogram in Figure 9.6 gives a sense of the elementary form Z^ 
of 
1.5 -| 
Figure 9.6: Histogram of 1000 25-term Riemann sums Y^dX^. 

9.4. CALCULATION 
OF WEAK STOCHASTIC 
INTEGRALS 
461 
the contingent observable ΤΖψ'Γ(Χτ-); which are, respectively, 
Z(r) „ ,w [ R i F y ( r ) ] i 
πψ'Γ(χτ) * πψ'Γ(χτ)[Β.τ,ο}, 
with r = 5. (Taking r = 5 requires partitioning T into 32 parts, while, for 
convenience, the partitioning in Calculation 5 and Figure 9.6 involves only 25 
terms.) 
The observable Z^ 
does not appear to be normally distributed. If the 
Maple command 
Mean(RiemannSums); 
is now executed, then, depending on the particular Riemann sum values pro-
duced by the random sample or simulation of Calculation 5, a result such as 
0.9930280578 is obtained. Since this is close to 1, it may constitute some kind of 
confirmation that (9.1) converges in some sense to 1 for this particular stochastic 
integrand. 
The values of dx[i], given by the random sample in Calculation 5, are norm-
ally distributed Brownian increments. Subject to Maple programming con-
straints, it is hoped that they are also independent. Therefore the elementary-
form distribution function FZ(r) indicated in Figure 9.6 corresponds to the dis-
tribution function FZ(r) given by (9.2). In other words, Z^ 
"inherits", in some 
sense, its distribution function FZr in domain R, from the standard Brownian 
distribution function G of the standard Brownian joint-basic process Xj- in R T. 
The following calculations use successively finer partitions of T =]0,1] in 
order to see the shape of the distribution functions FZ(r) as r increases. For 
convenience, binary partitions are avoided. 
Calculation 6 
restart: 
with(Statistics): 
Brownianlncrements := Sample(Normal(0, 0.1), 100000): 
dx := [seq(Brownianlncrements[i], i=l..100000)]: 
for r from 1 to 1000 do 
k := 100*r: 
j := k - 99: 
RiemannSum[r] := add(dx[i] 2, i=j..k): 
end do: 
RiemannSums := [seq(RiemannSum[r], r=1..1000)]: 
Histogram(RiemannSums); 
Calculation 6 produces the histogram in Figure 9.7. It indicates a clustering 
of Riemann sum values around a "target" value 1. The range of Riemann sum 
values obtained in this case is less than the range demonstrated in Figure 9.6 
for Calculation 5, indicating "tighter" clustering as r increases. 
Calculation 7 has 1000 Riemann sums ^ dx[i]2, each containing 400 squares 
of Brownian increments dxs, each of which in turn has mean zero with standard 
deviation 0.05 and variance 0.0025; corresponding to random walks of 400 steps 
in the domain ]0,1]. 

462 
CHAPTER 9. NUMERICAL CALCULATION 
Figure 9.8: Histogram of 1000 400-term Riemann sums Σ dX% 
Figure 9.7: Histogram of 1000 100-Riemann sums Ó dX% 

9Λ. 
CALCULATION 
OF WEAK STOCHASTIC 
INTEGRALS 
463 
Calculation 7 
restart: 
with(Statistics): 
Brownianlncrements := Sample(Normal(0, 0.05), 400000): 
dx := [seq(Brownianlncrements[i], i=l..400000)]: 
for r from 1 to 1000 do 
k := 400*r: 
j := k - 399: 
RiemannSum[r] := add(dx[i] 2, i=j..k): 
end do: 
RiemannSums := [seq(RiemannSum[r], r=l..1000)]: 
Histogram(RiemannSums); 
The output from Calculation 7 is the histogram in Figure 9.8. 
The three histograms in Figures 9.6, 9.7, and 9.8 can be seen to provide some 
empirical support for (9.1) converging to zero as r —> oo. They provide some 
confirmation that the integrand g = #2 = dX2 has (weak) stochastic integral 
f{Xr) = 1. 
To test further for weak convergence of the Riemann sums to 1, change the 
preceding Maple calculations to calculate 
VarSum = ^ d r [ z ] 2 - 1 
instead of RiemannSum. 
Calculation 8 
restart: 
with(Statistics): 
Brownianlncrements := Sample(Normal(0, 0.2), 25000): 
dx := [seq(BrownianIncrements[i], i=l..25000)]: 
for r from 1 to 1000 do 
k := 25*r: 
j :- k - 24: 
VarSum[r] :- abs(add(dx[i] 2, i=j..k) - 1): 
end do: 
VarSums := [seq(VarSum[r], r=1..1000)]: 
Histogram(VarSums); 
Mean(VarSums); 
Calculation 9 
restart: 
with(Statistics): 
Brownianlncrements := Sample(Normal(0, 0.1), 100000): 
dx := [seq(BrownianIncrements[i], i=l..100000)]: 
for r from 1 to 1000 do 
k := 100*r: 
j :- k - 99: 

464 
CHAPTER 9. NUMERICAL 
CALCULATION 
10-1 
6-\ 
H 
Figure 9.9: Histogram for 25-term Riemann sum observable with \i\ = 0.04. 
VarSum[r] := abs(add(dx[i] 2, i=j..k) 
end do: 
VarSums := [seq(VarSum[r] , r=l..1000)]: 
Histogram(VarSums); 
Mean(VarSums); 
- 1): 
Calculation 10 
restart: 
with(Statistics): 
Brownianlncrements := Sample(Normal(0, 0.05), 400000): 
dx : = [seq(BrownianIncrements[i], i=l..400000)]: 
for r from 1 to 1000 do 
k := 400*r: 
j :» k - 399: 
VarSumCr] := abs(add(dx[i] 2, i=j..k) - 1) 
end do: 
VarSums := [seq(VarSum[r], r=1..1000)]: 
Histogram(VarSums); 
Mean(VarSums); 
Each of these three (r = 1,2,3) Maple calculations returns two pieces of 
output. These are, in each case, a histogram of the rth sample, and the mean 

9.4. CALCULATION 
OF WEAK STOCHASTIC 
INTEGRALS 
465 
10 -i 
sH 
0.4 
0,6 
o.s 
Figure 9.10: Histogram for 100-term Riemann sum observable with \i\ =0.01. 
0.4 
0.6 
0.8 
Figure 9.11: Histogram for 400-term Riemann sum observable, |z| = 0.0025. 

466 
CHAPTER 9. NUMERICAL 
CALCULATION 
of the rth sample. For r = 1,2,3, the rth sample consists of 1000 data values 
y = y{r) = \ΣάχΜ2 -χ\ = Ι^2,Γ(*τ) - 1 | , = Λτ(χτ), 
say. (For convenience of calculation, the partitions of T for the Riemann sum 
are not rth binary.) The rth sample can be thought of as consisting of the 1000 
data values returned by 1000 observables: 
y<r> ~ yW [R, FY(r)}, 
hr{Xr) 
^ hr(xr) 
[R r, G] , 
Υ^ 
= 
hr(Xr) 
being the corresponding elementary and contingent forms. 
The terms dx[i] in the Riemann sums are selected by Maple as independent 
random Brownian increments; so the sample of contingent data elements hr(xq-) 
are distributed in accordance with the Brownian distribution function G and 
the deterministic function hr. In other words, the elementary data-values y^ — 
hr(x-]-) inherit from G and hr a distribution function FY(r) in R. Therefore, 
using (9.2), for any J G I(R) the proportion of sample data-values y in J is 
approximately 
C[J] = FY„(J) 
= Px o K\J) 
= f 
lh-1(J){xT)G{I[N]); 
and, using Theorems 39 and 79, 
Ε[ΥΜ] = / y^FYM(J) 
= [ 
hr(xT)G(I[N}) 
= V h r G[R r]. 
For r = 1,2,3 the histograms are displayed in Figures 9.9, 9.10, and 9.11, 
respectively. 
Each time Calculations 8, 9, and 10 are executed a particular random sample 
of Brownian increments dx[i] is generated. For a trio of such samples, the three 
estimates of VhrG [R7"] returned by 
Mean(VarSums); 
were 
0.2180014861, 
0.1101283834, 
0.05825507338, 
for partitions of T using cells of length 
0.04, 
0.01, 
0.0025, 
respectively. Though only three terms of the series are examined, the results 
provide some intuitive support for the statement that \hrG [RT] ^ 0 as r ^ oo. 

9.5. CALCULATION 
OF ITÖ'S 
FORMULA 
467 
9-5 
Calculation of Ito's Formula 
For σ £ R, T =]0,i], and Xq- ~ xj-[RT
1 G], Ito's formula states that 
f(Xut)-f(0,0) 
= J f(Xs,s)ds+^2 
j f"(Xs,s)ds 
+ a j 
f'(X8,8)dX8, 
the final integral being weak stochastic. This can be written as 
f df(Xs,s) 
= j f(Xs,s)ds 
+ ±a2 j f"(Xs,s)ds 
+ a f 
f'(X8,s)dX8. 
The "=" sign in Ito's formula amounts to a statement about the value of the 
weak stochastic integral on the right-hand side and is therefore a statement 
about weak convergence of Riemann sums. 
In Section 8.12 the formula was verified for the function 
f(y,s)^e^+(^^). 
(9.3) 
For this particular function /, the objective here is to demonstrate numerically 
and visually the meaning of the weak convergence of Riemann sum estimates of 
the observables in Ito's formula. To simplify the Maple calculations take μ = 0. 
As before, the calculations are done by constructing large samples of the 
potential values of the Riemann sum observables and drawing histograms of 
these samples. The process is repeated, using finer partitions in each case, and 
the resulting histograms illustrate successively closer clustering of the sample 
Riemann sum values around a weak limit. 
The intention is to provide some numerical and visual sense of what is in-
volved in Ito's formula, as expressed in (8.36). That is, 
lim Vph.r^iR7"] = 0 , 
where, with integrands from (9.3), 
h(x(is),is) 
= hi(x(i8),i8) - /i 2(x(^),^), 
hi(-x.(is),ia) 
= (σβχρ (σχ8> - \c2s') - σβχρ (σχ3 - \cr2s)) , (9.4) 
h2{-x(is),is) 
= crexp (σχ8 - ^σ2δ) {χ8* - x8). 
The following Maple code is used to estimate 
Vnh,rG[Ilr]. 
Calculation 11 
restart; 
with(Statistics); 
Brownianlncrements := Sample(Normal(0, 10), 10000); 
dx := [seq(BrownianIncrements[i], i=l..10000)]; 
for q from 1 to 1000 do 
k := 10*q: 
j :- k - 9: 

468 
CHAPTER 9. NUMERICAL 
CALCULATION 
s[j - 1] := 0: 
xs[j - 1] := 0: 
RS[j - 1] := 0: 
for i from j to k do 
s[i] := 100*i: 
xs[i] := add(dx[q], q = j . . i ) : 
hl[i] := 10*exp(10*xs[i] - 
(1/2*100)*s[i] 
- 10*exp(10*xs[i - 1] - 
(1/2*100)*s[i - 1 ] ) : 
h2[i] := 10*exp(10*xs[i - 1] 
- 
(1/2*100) *s[i - l])*(xs[i] - xs[i - 
1]): 
RS[i] := RS[i - 1] + abs(hl[i] - h2[i]): 
end do: 
IT0[q] := RS[k]: 
end do: 
Ito := [seq(IT0[q], q-1..1000)]: 
Histogram(Ito); 
Mean(Ito); 
This simulation uses standard Brownian increments dx = xs' — xs where 
y/s' -s = σ = 10. 
Sample (Normal (0, 10), 10000) selects a random sample of 10,000 "independ-
ent" normal increments. The domain T =]0,1000] is partitioned into 10 equal 
intervals of length σ2 = 100, with partition points Si = s [i]. The process value 
x(si) = xs[i] is obtained by adding up i normal increments dx. The values of 
h\ and /12 at partition point Si are given by hi [i] and h2 [i] in accordance with 
(9.4). Calculation 11 has r = 10 (so the partition is not binary). For any fc, 
1 < k < 1000, the fcth sample Riemann sum value RS[k] of ΊΖγΤ(χτ) 
is given 
by 
which the Maple code finds by accumulating, in successive cycles, the absolute 
value of terms (hi [i] — h2 [i]) in the r-partition: 
RS[i] := RS[i - 1] + abs(hl[i] - h2[i]). 
A sample of 1000 of these Riemann sum estimates of V^rG[RT] is listed in the 
Maple list denoted by Ito. 
The histogram (Figure 9.12) of the list Ito gives an impression of the Rie-
mann sum observable for this instance of Ito's formula. 
As explained in Section 9.4, the Mean(Ito) command provides a Riemann 
sum estimate of V^rG![RT] for a particular integer r. According to the theory, 
Vfc r G[R T]^0 
as r -+ 00. With r = 10 in T =]0,1000], a sample of 1000 Riemann sum 
estimates of V/1T,G[RT] produced for Calculation 11 returned an estimated value 
Mean(Ito) = 76^86002866. 

9.5. CALCULATION 
OF ITÖ'S 
FORMULA 
469 
To see what happens when T = ]0,1000] is partitioned with smaller subint-
ervals, Calculation 12 selects 1,000,000 Brownian increments with σ = σ2 = 1. 
A sample of 1000 Riemann sums is formed from partitions of T 
=]0,1000] 
consisting of 1000 equal intervals of length 1. 
Calculation 12 
restart; 
with(Statistics); 
Brownianlncrements := Sample(Normal(0, 1), 1000000); 
dx := [seq(BrownianIncrements[i], i=l..1000000)]; 
for q from 1 to 1000 do 
k := 1000*q: 
j := k - 999: 
s[j - 1] := 0: 
xs[j - 1] := 0: 
RSCj - 1] := 0: 
for i from j to k do 
s[i] := 100*i: 
xs[i] := add(dx[q], q=j..i) : 
hl[i] := l*exp(l*xs[i] - 
( | * l ) * s [ i ] 
- l*exp(l*xs[i - 1] - 
( | * l ) * s [ i 
- 1 ] ) : 
h2[i] := l*exp(l*xs[i - 1] 
- 
( | * l ) * s [ i - l])*(xs[i] - xs[i - 
1]): 
RS[i] := RS[i - 1] + abs(hl[i] - h2[i]): 
end do: 
IT0[q] := RS[k]: 
end do: 
Ito := Cseq(IT0[q], q-l..1000)]: 
Histogram(Ito); 
Mean(Ito); 
The histogram representation of the elementary form of the Riemann sum 
observable is in Figure 9.13. Mean(Ito) returned 1.152977886 as estimate of 
The following two calculations are done on domain T = ]0,1]. Calculation 
13 has σ = 0.2, so T — ]0,1] is partitioned into 1/0.04 = 25 subintervals; 
Maple simulates 1000 Riemann sum estimates for (9.4), with histogram output 
in Figure 9.14, and Mean(Ito) estimate 0.2185890853 for 
VhrG[RT]. 
Calculation 13 
restart; 
with(Statistics); 
Brownianlncrements := Sample(Normal(0, 0.2), 25000); 
dx := [seq(Brownianlncrements[i], i=l..25000)]; 
for q from 1 to 1000 do 
k := 25*q: 
j := k - 24: 
s[j - 1] := 0: 

470 
CHAPTER 9. NUMERICAL 
CALCULATION 
Figure 9.12: Histogram for Ito formula, 10-term approximation, T = ]0,1000]. 
0.5 
0.4 
0.3 
0.2 
0.1 
Figure 9.13: Ito formula, 1000-term approximation, T =]0,1000]. 

9.5. CALCULATION 
OF ITÖ'S 
FORMULA 
471 
xs[j - 1] := 0: 
RS[j - 1] := 0: 
for i from j to k do 
s[i] := 0.04*i: 
xs[i] : = add(dx[q], q = j . . i ) : 
hl[i] := 0.2*exp(0.2*xs[i] - 
(±*0.04)*s[i] 
- 0.2*exp(0.2*xs[i - 1] - (^*0.04)*s[i - 1]): 
h2[i] := 0.2*exp(0.2*xs[i - 1] 
- 
(±*0.04)*s[i - l])*(xs[i] - xs[i - 1]): 
RS[i] := RS[i - 1] + abs(hl[i] - h2[i]): 
end do: 
ITOCq] := RS[k]: 
end do: 
Ito := [seq(IT0[q], q=1..1000)]: 
Histogram(Ito); 
Mean(Ito); 
Calculation 14 has σ = 0.05, so T =]0,1] is partitioned into 1/0.0025 = 
400 subintervals, and Maple simulates 1000 Riemann sum estimates for (9.4), 
with histogram output in Figure 9.15, and Mean(Ito) estimate 0.1893459552 for 
V^ G[R T]. 
Calculation 14 
restart; 
with(Statistics); 
Brownianlncrements := Sample(Normal(0, 0.05), 400000); 
dx := [seq(BrownianIncrements[i], 
i=l..400000)]; 
for q from 1 to 1000 do 
k := 400*q: 
j := k - 399: 
s[j - 1] := 0: 
xs[j - 1] := 0: 
RS[j - 1] := 0: 
for i from j to k do 
s[i] := 100*i: 
xs[i] := add(dx[q], q = j . . i ) : 
hl[i] := 0.05*exp(0.05*xs[i] - 
(^*0.0025)*s[i] 
- 0.05*exp(0.05*xs[i-l] - (l*0.0025)*s[i-l]) : 
h2[i] := 0.05*exp(0.05*xs[i - 1] 
- 
(§*0.0025)*s[i - l])*(xs[i] - xs[i - 1]): 
RS[i] := RS[i - 1] + abs(hl[i] - h2[i]): 
end do: 
IT0[q] := RS[k]: 
end do: 
Ito := [seq(IT0[q], q=1..1000)]: 
Histogram(Ito); 
Mean(Ito); 

472 
CHAPTER 9. NUMERICAL 
CALCULATION 
s1 
H 
4-J 
2 
0.1 
0.3 
0.4 
0.5 
0,6 
D." 
— i — — T 
O.S 
Figure 9.14: Histogram for Ito formula, 25-term approximation, T =]0,1]. 
12-
10-
:H 
0.1 
0.2 
O.S 
0.4 
0.5 
0.6 
0." 
O.S 
Figure 9.15: Histogram for Ito formula, 400-term approximation, T =]0,1]. 

9.6. CALCULATING 
WITH BINARY PARTITIONS 
OF R T 
473 
9.6 
Calculating with Binary Partitions of R/1 
The calculations in preceding sections have been done, essentially, for element-
ary-form observables Y ~ y[R, Fy], a form which lends itself to visualization 
by histograms. In contrast, a joint-contingent observable with sample space R T 
has representation 
f(XT)~f(xT)[RT,FXT], 
and its expected value is 
E [ / ( * r ) ] = / 
f(xT)FxT(I[N]). 
This calculation is done by forming partitions and Riemann sums in R T. 
Binary (r, </)-partitions of infinite-dimensional domains R T are described in 
Chapter 3, Section 3.5. With T = ]0, t], 2r is the number of partition points 
rj r ) = j2~rt of T, and q2q+1 + 1 is the number of partition points k2~q of R 7 / 
for each j . As detailed in (3.12), the number of cells in an (r, g)-partition K,rq of 
R T i g 2r*2«^+2r 
w h i c h increases greatly as r and q increase. Since this is also 
the number of terms in the corresponding Riemann sum, the scale and amount 
of calculation involved quickly escalate as r and q increase. 
Take T =]0,1] and FxT = G, so the process is standard Brownian motion 
on ]0,1]. The binary partition points of T — ]0,1] are 
(r) 
JT 
In Section 7.9 a binary cell K partitioning R T is denoted by 
K = Krq^ e K,rq, 
and the value of G on K is denoted by 
qrq\k 
= 
qrq\\L(K) 
= G(Krq\^ 
= 
Q(K^ 
where, for c : 
I 
' 2 ' 
G(K) 
[ GC(I[N})= [ 
gc(xT,N)\I[N]\ 
JK 
JK 
rqu[l] 
i>qu[2r] *_ ' 
Jalll] 
Jal\2-\ 7_i 
(9.5) 
exp 
c(yj-Vj-iY 
T ( r ) ( r ) 
\ 
(iW'-^V 
dyi'" 
dy2r 
the lower and upper limits of integration (ql[j] and qu[j], respectively) being 
„(r) 
drawn from the cells Kq\k which, for each j , partition R r 
Kq\k = ]-q + (k-l)2-q,-q 
+ k2-q] 
= ]ql\j], qu\j]]. 
(9.6) 

474 
CHAPTER 9. NUMERICAL 
CALCULATION 
With r = 2 the partition points of T = ]0,1] are r· \ j = 1,2,3,4; that is, 
I I 3 -j 
4' 2' 4' -1, 
With g = 2, then, in accordance with (9.6), and for each j , the one-dimensional 
(2) 
domain R / J 
has partition points 
_ 0 0 _ 9 - 1 
- 3 
_ 5 
_ i 
_ 3 
_ 1 
_ 1 n 1 1 3 i 5 3 7 9 oo 
(Q 7) 
^^5 
^5 
45 
2 ' 
4 ' 
' 
4 ' 
2 ' 
4 ' 
' 4 ' 2 ' 4 ' 
' 4 ' 2 ' 4 ' 
' 
V 
/ 
For 1 < j < 4, the lower and upper integration limits in (9.5), ql[j] and qu\j], 
respectively, are drawn from (9.7). Therefore, for each j , the corresponding 
integral in (9.5) can be any one of 
f, r\ /■·, .. ,r, r. 
7-oo 
J-2 
J-l 
Jl 
h 
By separating (9.7) into two separate lists, 
_nn _ 9 _ 1 _ 3 _ 5 _ i 
_ 3 _ I 
_ 1 
n I 
I 
3 -, 5 3 7 
9 
{J<J'> 
*"> 
4? 
2 ' 
4 ' 
' 
4 ' 
2 ' 
4 ' 
' 4 ' 2 ' 4 ' 
' 4 ' 2 ' 4 ' 
' 
_ 9 _ 1 _ 3 
_ 5 
_-i _ 3 
_ I 
_ I n 1 I 3 i 5 3 7 o 
^ 
^? 
45 
9 ' 
4 ' 
-1' 
4 ' 
9 ' 
4 ' u> 4 ' 2 ' 4 ' 
' 4 ' 2 ' 4 ' 
' 
' 
(9.8) 
the following Maple code establishes limits ql[j] and qu[j] in domain R^0'1! for 
the iterated integrals in (9.5). 
Calculation 15 
r e s t a r t : 
r := 2: 
q := 2: 
for j from 1 to 2r do S[j] := j*2 r : end do; 
s := [seq(S[j], j = 1 .. 
2 r)] ; 
kmax := q*2(<?+1): 
for k from 1 to kmax do qList[k] := — q + (k — l)*2 - < ?: 
end do: 
ql := [—oo, seq(qList [k] , k = 1 .. 
kmax), q] ; 
qu := [seq(qList[k], k = 1 .. 
kmax), q, oo]; 
This returns the pair of lists in (9.8). In (9.5), with c——\ 
and τ^ 
~τ^_1 
= 
2~r, write 
3 = 1 V 
7 
rqu[l] 
rqu[2r] ^ 
( 
, . _ 
. 
γ \ 
Jql[l] 
Jql[2'] fJi 
\ 
2(2 
) 
J 
Then 
G(K) = βα. 
This is calculated in the next section. 

9.7. CALCULATION 
OF OBSERVABLE PROCESS IN R T 
475 
9.7 
Calculation of Observable Process in R T 
Section 7.10 shows how to use binary partitions Krq to estimate the expectation 
of a joint-contingent observable 
f(XT)~f(xT)[RT,G}. 
In Theorem 207, a Riemann sum estimate for E[/(Xr)] is 
Σ 
( / 
f{rqHxT)Gc(I[N}) 
: IT* € KT" 
(9.9) 
where f^rq\xT) 
is a step function approximation of f(xr)- 
This section calcul-
ates a single term of this Riemann sum. Other terms are calculated similarly. 
To simplify the Maple calculation, instead of a binary (r, g)-partition suppose 
T =]0,1] is partitioned as 
° l 
5 
1 2' 
3 ' 3 
5 
M 
and, at each of the partition points of T, suppose R is partitioned as 
] - 10, -0.6], 
] - 0.6, -0.3], 
] - 0.3,0], ]0,0.3], 
]0.3,0.6], ]0.6,10] 
in order to calculate the integral in (9.9). (With slight re-wording, Theorem 209 
ensures convergence of the step functions for any regular partition, not just the 
binary ones.) 
Lower and upper limits of —10 and 10 are used here in place of — oo and oo. 
This makes little difference to the calculated values of the Brownian distribution 
function G(I[N]), as the variance of the Brownian increments is 
-Hi)-*G)K 
The corresponding partition of R T consists of 63 = 216 cells K\ for example, 
K = ]0,0.3] x ]-0.3,0] x ]0.3,0.6] x R ^ M i i 1 } . 
Then, with yo = 0 and 
ß = Π(2π(ί,·-ί,·_1)Γ* = (2n)-i(-
3 = 1 
■ - r WJCHK-^)) 
HVj 
Vj-tf))dy1\dy2\dy3 
Maple can calculate the distribution function value G(K) = βα, as follows. 

476 
CHAPTER 9. NUMERICAL 
CALCULATION 
Calculation 16 
restart: 
yCO] := 0: 
g := mul(exp(- (3/2)*(y[j] - y[j - l ] ) 2 ) , j - 1 . , 3 ) : 
a := evalfi 0(int(g, y[l]=0. .0.3, y [2] = - 0 . 3 . .0, y[3]«0.3. .0.6)) : 
ß :» evalfio((2*Pi)"ä * ( | ) - § ) : 
G := 
ß*a; 
0.004336025795 
Thus G{K) = 0.004336025795. The Brownian likelihood of other cells can be 
similarly calculated. For any cell K of the regular partition, any one of the 
three lower limits of integration in Line 4 of Calculation 16 is any one of the six 
numbers 
10 
- 0 . 6 
- 0 . 3 0 0.3 0.6; 
and in each case, the corresponding upper limit of integration in Line 4 of 
Calculation 16 is 
-0.6 
- 0 . 3 
0 
0.3 
0.6 
10. 
Once the lower limits of the integration are established, the corresponding upper 
limits are then determined. 
To make a list of the lower limits, it is necessary to list all possible per-
mutations, with repetition, of the numbers 10, 
—0.6, —0.3, 0, 
0.3, 0.6. 
Maple has a permute command which lists all permutations of n items, taken 
r at a time without repetition. The following sequence of commands will 
generate the listings needed for permutations of six items taken three at a time, 
with repetition. 
Calculation 17 
restart; 
1[1] := - 10: 
1[2] := - 0.6: 
1 [3] := - 0.3: 
1[4] := 0: 
1[5] := 0.3: 
1[6] := 0.6: 
u[l] := - 0.6: 
u[2] := - 0.3: 
u[3] := 0: 
u[4] := 0.3: 
u[5] := 0.6: 
u[6] := 10: 
L := [seq(l[j], j = 1 .. 
6 ) ] ; 
[- 10, - 0.6, - 0.3, 0, 0.3, 0.6] 
U := [seq(u[j], j = 1 .. 
6 ) ] ; 
[- 0.6, - 0.3, 0, 0.3, 0.6, 10] 
p := 0: 
for rl from 1 to 6 do 
for r2 from 1 to 6 do 
for r3 from 1 to 6 do 
p :- p + 1: 
PCp] := [L[rl], L[r2] , L[r3]]: 
QCp] := [U[rl], U[r2] , U[r3]] : 
end do: 
end do: 
end do: 

9.7. CALCULATION 
OF OBSERVABLE PROCESS IN R T 
477 
This generates a list of 216 lower integration limits L, and the list Q of corr-
esponding upper integration limits. For instance, the 100th cell K of the 216-
member partition of ϊϋ0,1^ is determined by the lists 
P[100] = [-0.3, -0.3,0.3], 
Q[100] = [0,0,0.6], 
so 
K[100] = ]-0.3,0] x ]-0.3,0] x ]0.3,0.6] x 
R j ^ M i ' ä · 1 } . 
A Riemann sum estimate of the expected value of the observable f(Xr) 
is then 
given by 
the sum being taken over the 216-member partition. Suppose 
f(xT) 
= exp (- 
f x{tfdt\ 
. 
With the continuous modification of G, f(xr) 
can be taken to be zero if χτ 
is not continuous. Theorem 209 ensures that E [/(Χτ)] 
exists, and that it 
can be approximated to any degree of accuracy by Riemann sums over regular 
partitions of R T. For the cell K in Calculation 16, Theorem 214 implies that 
f(xr) 
can be taken to be 
exp I - £ 
x^itj 
- tj-i) 
I , 
= exp ί "
U 
x2^ 
I , 
(9.10) 
since XQ = 0, and the values x\ and #2 can be taken to be 0.3 and 0. This 
reduces to exp(—0.03), or 0.9704455335. Therefore this particular term of the 
Riemann sum reduces to 
exp(-0.03)G(iQ = 0.9704455335 x 0.004336025795 = 0.004207876866. 
If this calculation is replicated for the other 215 cells partitioning R^0'1^ then 
the resulting Riemann sum calculation gives an estimated value for E [/(Xp)]. 
The same method can be used to obtain estimates of the diffusion function 
Ψ{ξ,τ), given by the marginal density of expectation 
Εξτ [f(XT)] · 
In this case τ = 1. Suppose ξ = 0.5. Then, in Calculation 16, take 
y[3] = ξ = 0.5, 
and perform the integration on variables y[l] and y[2] only. The result given by 
Maple in this case is 
G(K~)= 
0.03999644118, 
and the corresponding Riemann sum term is exp(—0.03)G(K~) 
= 
= 0.9704455335 x 0.03999644118 = 0.03881436770. 
(9.11) 
The approximate numerical value of Ε0.5,ι(/(Χτ)) = ^(0.5,1) can be built up 
from calculations like this one. 

478 
CHAPTER 9. NUMERICAL 
CALCULATION 
9.8 
Other Joint-Contingent Observables 
As further illustration of numerical calculation in sample space R T, suppose the 
Brownian observable is 
f(XT) = 
sup{x(t),te)0,l}=T}. 
This function of χτ is not continuous as x{t)teT varies continuously in its sep-
arate components. Nor is this function bounded. Therefore Theorem 209 does 
not guarantee convergence of its Riemann sums over regular partitions, and 
further investigation into convergence is required. 
For similar reasons it is not possible to apply Theorem 214, as it stands, 
in order—as in (9.10)—to produce a version of / which is more amenable to 
numerical calculation. But if conditions were such that devices like these could 
be applied, then, for the cell K of Calculation 16, this particular term of the 
Riemann sum estimate of E[/(Xr] has a value of 0.3 or 0.6—depending on the 
method of calculation—for the potential data-value f{xr) 
for that particular 
cell K. Potential data-values f(xr) 
for other cells can be obtained similarly. 
Much of this book is concerned with potentiality distribution functions Fx 
which can take negative or complex values, such as the Feynman distribution 
function G±, with ι = y/^ϊ. 
To perform computations such as those in Calc-
ulation 16 and (9.11), the integrand can be separated into real and imaginary 
parts. For the cell K of Calculation 16, with T = ]0,1], the real part » (G± (K)) 
is oti + c*2, where 
αι = 
( T ) ~ ' C O S ( X ) / O ° " V - O . 3 ^ 
a2 = ( τ ) ~ ' 8 ί η ( χ ) / ο ° ' 3 / \ 3 / ο ° 3 6 Σ , 3 = ι 8 ί η ( | (yj - yj-if) 
dyidy2dy3. 
This is obtained by separating 
G* (I\N}) = Π; = 1 (2«r)-* JI{N) exp (* Σ ? . ι ^ ϊ ^ ) 
Μ*) 
into real and imaginary parts, with n = 3 and tj —tj-\ 
= | . These iterated 
integrals can be calculated as in Calculation 16, with the appropriate trigono-
metric function replacing the exponential function. As in (9.11), a term of the 
Riemann sum estimate of E [Z//~s(XT)] can be calculated; and again the po-
tential datum-value U~%(XT) for each such term can be separated into real and 
imaginary parts. 
For any given ξ G R and r G R+, Riemann sum estimates of the quantum 
mechanical state function ψ±(ξ,τ) 
can similarly be obtained, by removing the 
integration on ξ = yn as in Calculation 16. Likewise, Riemann sum estimates 
of the values of terms ψ±ίΡ(ξ,τ) 
of the series expansion of ψ%(ξ,τ) on which 
Feynman diagrams are based—see Section 7.22. Theorem 222 then provides 
convergence conditions for these Riemann sums over regular partitions such as 
those in Calculation 16. 

9.9. EMPIRICAL 
DATA 
479 
9.9 
Empirical Data 
At the start of this book Table 1.1 gives simple numerical data from which an 
estimate of the distribution function for the observable is deduced. No claim is 
made regarding the accuracy or reliability of the estimate. 
Is it possible to estimate a joint distribution function from sample data χτ 
of a joint observable or random process Χχ? And why should the attempt be 
made? This section seeks to provide support for empirical approaches. 
In contrast, the basic assumption of Section 8.16 is (8.48). This states that 
asset price processes Ζτ satisfy Ζτ — ζτ [Κ+,{?^σ]; so the increments lnZt — 
In Zf are assumed to be 
■ independent, and 
■ normally distributed. 
In Ross [198], the end-of-day price of a barrel of crude oil, as traded on the New 
York Mercantile Exchange, is listed for each trading day from 3 January 1995 to 
19 November 1997. A chi-square statistical test of the independence hypothesis 
is applied to increments ΙηΖ^ — lnZt/, and the independence hypothesis fails. 
In general, independence is intuitively implausible. Realistically it is hard to 
conceive of any pair of real-world events for which there is no possible connecting 
chain of causal events, however remote, linking the two together. 
Does this mean that the financial theory presented in Section 8.16 is worth-
less? Independence is a useful mathematical abstraction. The fact that, in 
geometry, the mathematically perfect circular shape rarely if ever actually occ-
urs in physical reality does not detract from the crucial role of the mathematical 
circle in understanding the world. But it is unwise, in the "real" world, to gam-
ble on occurrence of a mathematically perfect abstraction. 
The assumption that the increments of the logarithms of asset prices are 
normally distributed has also been challenged. It is argued that large changes 
in asset prices are much more common than the theory of Section 8.16 predicts. 
This section presents series of asset prices from4 Thomson Reuters Data-
stream [224] in order to assess these issues further. Datastream provides data 
in Excel format. To import an Excel file FileName. xls into Maple the following 
Maple commands should be executed: 
with(ExcelTools): 
Import(FileLocation/FileName.xls): 
Figure 9.16 is a Maple graph displaying a series of daily end-of-day share 
prices of Glanbia, a food processing company. Prices are in pounds sterling. 
The series consists of 5219 terms, running from 8 March 1991 to 9 March 2011, 
beginning and ending as follows: 
0.8, 0.817, 0.836, ··· ,3.782, 3.699, 3.692. 
See also: https://sites.google.com/site/StieltjesComplete/ 

480 
CHAPTER 9. NUMERICAL 
CALCULATION 
-H 
H 
2-J 
-i 
1 
1 
1 
1 
1 
1 
1 
1 
r~ 
2 
4 
6 
8 
10 
-
l
—
i
—
ι
—
i
—
i
—
■
—
i
—
i
—
i 
12 
14 
16 
18 
20 
Figure 9.16: Twenty-year graph of Glanbia daily share prices. 
Take the logarithm of each of the daily share prices, and calculate the 
daily increments by subtraction. This gives a list of 5218 log-increments, with 
graph Figure 9.17 and histogram Figure 9.18. Maple returns a mean value μ = 
0.0002930839150 for the 5218 increments of the logarithms of the share prices, 
with standard deviation σ = 0.0234953419046857. 
If the Glanbia share prices satisfy the assumptions of Section 8.16, Figure 
9.18 should approximate to a normal distribution with mean zero and standard 
deviation 0.0235. 
Does Figure 9.18 look like a histogram of a normally distributed random 
variable? That is what is assumed in Section 8.16. 
A chi-square statistical test can be applied to the data to check whether the 
Glanbia data satisfy the assumptions of Section 8.16. The null hypothesis is 
that the sample of 5218 data items of Figure 9.18 are observations of a normally 
distributed random variable with mean μ and standard deviation σ. Then the 
expected frequencies e are obtained, in accordance with the null hypothesis, by 

9.9. EMPIRICAL 
DATA 
481 
0.05 
-Ö.05H 
-0.10 
-0.15 
-0.2.0-
Figure 9.17: Graph of log-increments of Glanbia share prices. 
-o. 
_ , 
j _ _ 
-0.2 
Figure 9.18: Histogram of log-increments of Glanbia share prices. 

482 
CHAPTER 9. NUMERICAL 
CALCULATION 
calculating 
-8*^/>(-K^)> 
A simple Maple calculation gives the actual, or observed, frequencies o of the 
Glanbia data. The observed and expected frequencies are given in Table 9.1. 
The tabulation confirms the impression given by Figure 9.16, that extreme 
changes in share prices are more common than what is predicted under the 
lognormality assumption. 
The chi-square value is 
9 
v ^ (o — e)2 
_ Λ 
x = Σ — 
= 85β· 
With seven degrees of freedom, the critical values of χ2 for significance levels 
0.05, 0.01, and 0.001 are 14.07, 18.48, and 24.32, respectively. Therefore the null 
hypothesis should be rejected. The evidence of the Glanbia data contradicts the 
hypotheses of Section 8.16. The Glanbia data demonstrate the "black swan" 
phenomenon—events which are rare in theory are common in practice. 
Figures 9.19 to 9.24 refer to daily series of share price data, each series of 
twenty years' duration, selected miscellaneously from Thomson Reuters Data-
stream [224]. 
Figures 9.21 and 9.24 show significant numbers of extreme outlying values. 
Superficial inspection of this miscellaneous selection of share price data indic-
ates that the hypotheses of Section 8.16 cannot be assumed to be valid. In 
particular, it cannot be assumed, without verification, that a geometric Brown-
ian distribution function 0μσ is applicable to series of share prices. And even 
if some modified version of 0μσ is used, it is still necessary to verify, test, or 
otherwise ensure that the distribution function fits the data. 
Data range 
] — oo, μ — 3σ] 
]μ — 3σ, μ — 2σ] 
]μ-2σ,μ-σ] 
]μ-σ,μ] 
]μ, μ + σ] 
]μ + σ,μ + 2σ] 
]μ + 2σ, μ + 3σ] 
]μ + 3σ, οο[ 
0 
45 
92 
350 
2295 
1925 
369 
103 
39 
e 
7.04376792905 
111.666420577 
709.152926509 
1781.13688499 
1781.13688499 
709.152926509 
111.666420577 
7.04376792905 
Table 9.1: Calculation of chi-square value for Glanbia data. 

9.9. EMPIRICAL 
DATA 
483 
-
5 0 -
4 0 -
3 0 -
2 0 -
1 0 -^^--^y^ 
1 
| 
1 
| 
1 
| 
1 
| 
1 
| 
1 
.2 
4 
6 
S 
10 
1 
I 
12 
1 
1 
14 
1 
1 
r-
16 
1/ 
If 
— i 
i S 
2 0 
Figure 9.19: Graph of Rio Tinto share prices. 
Figure 9.20: Graph of log-increments of Rio Tinto share prices. 

484 
CHAPTER 9. NUMERICAL 
CALCULATION 
- 0 . 4 
-0.3 
-0.1 
Figure 9.21: Histogram of log-increments of Rio Tinto share prices. 
22-
.20-
!S-
16-
14-
12-
10-
s-
s-
- i 
1 
1 
1 
1 -
2 
4 
—j 
1 
1 
1 
1 
1 
1 
1 
1 
1 
\ 
1 
1 
1 
1 
6 
S 
10 
12 
14 
16 
IS 
2 0 
Figure 9.22: Graph of GlaxoSmithKhne share prices. 

.9. EMPIRICAL 
DATA 
485 
-
0.02 
-α.ΰ'. 
-0.04 
•0.06 
-0.ÜS 
Figure 9.23: Graph of log-increments of GlaxoSmithKline share prices. 
—i 
1 
1 
1 
I —
i
—
"
^
^ 
1 
1 
1 
1 
1 
1 
1 
1 1— 
-0.10 
-0.05 
0 
0.05 
0.10 
0.15 
Figure 9.24: Histogram of log-increments of GlaxoSmithKline prices. 

486 
CHAPTER 9. NUMERICAL 
CALCULATION 
9.10 
Empirical Distributions 
Even if the finance theory of Section 8.16 cannot be relied on, it may still be 
possible to analyze the random variability properties of empirical data. 
Section 8.16 is predicated on a particular analytic distribution function 0μσ· 
Various modifications of this function are sometimes invoked to deal with the 
mismatch between the theory and the empirical data such as those in Section 
9.9. But it is difficult to imagine a reason why the likelihood distribution—if it 
exists—of any series of empirical data must have some analytic representation. 
There seems to be no a priori justification for assuming that there is an "alg-
ebraic formula" for such likelihoods, nor that any algebraic formula will give 
accurate values of the joint likelihoods. 
Does this mean that it is impossible to carry out accurate analysis of the 
random variability of such data? 
If there is no underlying likelihood C operating on the joint data series, then 
the series is not an observable, and the methods of this book have nothing to 
offer in the form of guidance. The entrails of a goat would be just as good. 
But if there is an underlying joint likelihood £, then it can be estimated 
numerically, and the likelihood estimates can then be used, as in Sections 9.7 
and 9.8, to estimate expected values, marginal expectation densities, and the 
like—just as we sought to use the distribution function ζ}μσ. The accuracy of 
the results will then depend on the accuracy of the estimates of C 
In Brownian motion there is a theoretical foundation—such as Theorem 
217—which gives some guarantee of the ultimate accuracy of the results of 
estimates based on regular and binary partitions of R T. Such estimates are 
good when the processes involved are Brownian or geometric Brownian, and 
when the contingent observable integrand function / satisfies the conditions of 
Theorem 217. But what guarantee of accuracy is there when these assumptions 
are dropped? 
Brownian and geometric Brownian processes (XtjteT 
a r e defined for contin-
uous t, and the data-values xt are continuous and unbounded. Therefore results 
such as Theorem 217 are needed as a basis for discrete-time, discrete-value est-
imates from regular or binary partitions of R T. 
But the empirical price processes (Xt) of Section 9.9 do not have continuous 
t. As recorded in Thomson Reuters Datastream [224], t is discrete, and each da-
tum value xt is constant from one end-of-day observation to the next. And even 
if the constantly changing moment-to-moment prices are taken into account, 
these too are momentarily constant from one "time-tick" to the next—unlike 
the values taken by a Brownian or geometric Brownian process. 
Also, the prices xt are discrete, not continuous, since these are units of money 
and are not infinitely divisible. The value of Xt is bounded below by zero, and 
while theoretically there is no upper bound, in practice some very large upper 
cutoff value can be applied. So in any finite interval of time there is only a finite 
number of time-ticks t, and there is, in effect, only a finite number of possible 
values xt. 
In other words, while the insights and perspectives of advanced mathematics 

9.11. CALCULATION 
OF EMPIRICAL 
DISTRIBUTION 
487 
can be helpful, investigation of the random variability of share prices is inher-
ently simpler than Brownian motion and quantum mechanics. In fact, results 
like Theorem 217 are irrelevant if, when analyzing price processes, the false 
assumption of continuous time/continuous values is abandoned, and the more 
realistic discrete view is adopted. 
Thus, if numerical estimates of joint likelihood C are used, the accuracy of 
any final calculation depends on the accuracy of the estimates. In other words, 
accuracy of results depends on the amount of effort, insight, and skill used in 
estimating joint likelihoods. 
9.11 
Calculation of Empirical Distribution 
Though detailed investigation of such estimates of joint likelihood is beyond 
the scope of this book, some rudimentary calculation is presented here for the 
Glanbia series from Thomson Reuters Datastream [224]. 
The series starts on Friday 8 March 1991, with Glanbia share price standing 
at 0.8 at the end of that day, and concludes on Wednesday 9 March 2011, with 
share price 3.692. Let τ', τ denote the start and finish date, respectively, of this 
series, and write T 
=]τ'',τ]. 
Assuming the existence of joint likelihood potentiality C for the Glanbia 
prices, the series χτ from Thomson Reuters Datastream [224] is a joint datum 
of basic observable 
XT~XT[RT,FXT}. 
Now suppose Wednesday 9 March 2011 is the present, and suppose a contract 
in Glanbia shares matures at a point four months in the future (in fact, sixteen 
weeks later), at end of Wednesday 29 June: in other words, after sixteen five-day 
trading weeks. 
Assuming continued existence of joint likelihood potentiality C for the Glan-
bia prices, let t represent the "future" date Wednesday 29 June 2011 (end-of-
day), so T =]r,t] represents the eighty-day trading period 9 March 2011 to 29 
June 2011. Then the Glanbia joint observable for the period is 
XT~xr[Rr,FXT], 
where, for I[N] E I (R T), the distribution function values are 
FXT(I[N]) 
= 
C[(xreI[N})}. 
To perform some analysis of the joint random variability of the share prices in 
the forthcoming sixteen weeks, Thursday 10 March 2011 to Wednesday 29 June 
2011, estimates of the joint distribution function Fx, = FxT, are needed. 
Suppose T is partitioned by times si, 82, and 53, each time increment being 
twenty days or four five-day trading weeks. Then so = r is the present (end of 
9 March 2011), s4 = t is 29 June, βι represents end of Wednesday 6 April 2011; 
and so on. 

488 
CHAPTER 9. NUMERICAL 
CALCULATION 
Suppose the daily end-of-day prices are partitioned by intervals of length 
one-tenth of a pound. Then the domain R7" = Rj7"'*! is partitioned by cells 
K = h x I2 x h x h x RT\{Sl'S2'S3'S4>, 
where each Ij has the form ]d, cZ + 0.1]. If marginal densities at time t = 54 are 
required, then a typical partitioning cell for domain R7" = Rjr'*i is 
Γ - / ι Χ /
2 Χ /
3 Χ 
R T \ { S I , * 2 , S 3 } 5 
where x(t) is now some given, fixed positive number. "Present time" is s = r; 
and present price is xT = x(r) = 3.692. Using intervals of one-tenth of a pound, 
the components of K could be 
h 
= J3.692,3.792], 
I2 
= ]3.592,3.692], 
h 
= ]3.692,3.792], 
I4 
= ]3.792,3.892]. 
The next task is to find an estimate of the joint distribution function value 
Fx(K), 
using the historic joint data-values of the series χχ from Thomson 
Reuters Datastream [224]. 
One way of making such an estimate is, by examining the series values from 
8 March 1991 to 9 March 2011, to find the proportion of occasions when, at 
four-week intervals, the prices had increments or transitions corresponding to 
those in the partitioning cell K. 
Subtracting 80 from 5219, the number of such tests or comparisons that can 
be made in the component elements of χτ is 5139. Thus, for each of 5139 days, 
count the number of occasions that, after a further twenty days, the comparison 
price increases by no more than 0.1, followed by a decrease of no more than 0.1 
from the comparison price after 40 days from the comparison date; and so on. 
The Maple code in Calculation 18 below evaluates this proportion. 
Fgb is the estimated value of Fx(K). 
In this case, fgb = 25, so Maple finds 
25 occurrences of joint increments or decrements of the kind appearing in K, 
and returns 
FX{K) = 0.004864759681 
as the value of Fgb. Other cells K in the partition can similarly be calculated. A 
Maple listing of the cells in the partition can be generated by permutation-with-
repetition, as in Calculation 17. Once the distribution function estimates Fx(K) 
are found for the cells of the partition, then expected value of a contingent 
observable f(Xr) 
c a n be estimated; also marginal density of the expectation. 
To obtain a risk-neutral model it is fairly simple to adjust the distribution 
function estimates in order to simulate, in expectation estimates, a specified 
growth rate such as the riskless rate of interest p. 
Calculation 18 is not necessarily suitable5 for every series, and there are many 
ways in which estimates can be improved. But whatever method is used, care 
5Calculation 18 is, essentially, the relative frequency counting procedure of Table 1.1 with 
which this book commences. 

9.11. 
CALCULATION 
OF EMPIRICAL 
DISTRIBUTION 
489 
must be taken to ensure that the distribution function values satisfy consistency. 
The counting procedure in Calculation 18 ensures consistency in this case. 
To see how consistency is ensured in this example, suppose dimension s\ is 
partitioned, with 
R+ = \j4 
i=l 
where the cells J{ are disjoint. The relative frequency counting argument ensures 
that Fx (J2 xhxUx 
R l M U ^ s , ^ } ) is gi v e n by 
Q 
ΣΓ* 
(Ji x J2 x h x h x R]0'1^^1'^,^,^}^ 
i=l 
Conditioning is built into Calculation 18, so dubious assumptions of the inde-
pendence of log-increments kind are averted. 
What about other potentially bogus assumptions? It is possible the assump-
tions (what are they?) in Calculation 18 are invalid, in which case some other 
estimation method must be tried. But assumptions that might perhaps be valid 
in some circumstances are preferable to assumptions that are demonstrably 
false. 
Calculation 18 
for j from 1 to 5139 do 
if gb[j + 20] > gbCj] and gb[j + 20] < gb[j] + 0.1 
and gb[j + 40] > gb[j] - 0.1 and gb[j + 40] < gb[j] 
and gb[j + 60] > gb[j] and gb[j + 60] < gb[j] + 0.1 
and gb[j + 80] > gb[j] + 0.1 and gb[j + 80] < gb[j] + 0.2 
then fgb := fgb + 1: 
end if: 
end do: 
Fgb := evalfio(fgb/5139); 

Appendix A 
Epilogue 
A. 1 
Measur ability 
Without invoking a theory of measure, Chapter 4 demonstrates that Riemann 
sums, constructed from point/cell/dimension-set association and regulated by 
gauges, deliver a theory of integration which has the characteristics, properties, 
and theorems required for the purposes of real analysis, and in particular for 
the purposes of analysis of random variability. 
Sections 4.7 and 4.8 of Chapter 4 introduced variation as a form of outer 
measure of subsets S of R T. In Section 4.9 this concept was extended from 
sets S in R T to sets S x M in R T x M(T), and the extended concept of outer 
measure was used in Theorem 52. This in turn was needed when establishing 
the integrability of functions that arise in quantum mechanics—in Theorem 219, 
for instance. 
The Henstock or Burkill-complete integral of a function h of elements x, 
iV", I[N], structured by a multifunctional relation of association between these 
elements, is defined by forming Riemann sums of the function values, the terms 
of the Riemann sums being selected by "gauge" rules. Thus the concept of 
measurability is not required in order to define the integral. However, in proving 
certain properties of the integral, or in calculating the integrals of particular 
functions, the analytical device of step functions can be very useful. 
Measurability is related to the construction of step functions and limits of 
step functions. These are formed from measurable sets, and they are measur-
able functions. 
Existence of the integrals of particular functions can often be 
established by means of step functions, using measurable sets and measurable 
functions. Accordingly, this section considers measurability of sets and functions 
in R T, in a further extension of the meaning given to the measurability concept 
in Section 4.9. 
Suppose h(x,N,I[N]) 
is a real- or complex-valued function, and suppose A 
is a subset of R T x Λί(Τ) χ I(R T) defined as follows. Given 
A c R T , 
McAf(T), 
J c I ( R T ) , 
A Modern Theory of Random 
Variation: With Applications 
in Stochastic Calculus, 
491 
Financial Mathematics, 
and Feynman Integration. 
First Edition. By Pat Muldowney 
Copyright © 2012 John Wiley & Sons, Inc. Published by John Wiley & Sons, Inc. 

492 
APPENDIX A. 
EPILOGUE 
an element (a?, TV, I[M]) of A satisfies x e A, 
TV G M, 
I[M] G J\ so 
(x,TV,/[M]) 
G A 
= 
A x ^ x J 
c 
R T x M x I(R T). 
Some of the elements (#, TV, J[M]) of A may be associated, with M = TV and 
x(TV) a vertex of /(TV) in R^; where I(t) is a proper subset of R for t G TV, and 
I(t) = R for t G Γ \ TV. But it is not assumed that the elements (x, TV, /[M]) 
of A are associated. 
Measurability has been defined and used in preceding chapters. The follow-
ing definition extends the meaning of measurability. 
Definition 58 Suppose a function h of associated elements (x, TV, /[TV]) is given. 
Then a set A is h-measurable if the function 
lA{x,NJ[N])h(x,N,I[N\) 
is integrable on RT. 
Example 70 To relate this definition to preceding chapters, take h to be a 
distribution function F defined on I(R T), and let A be a subset o/R T. 
Then 
A is F-measurable if and only if 1A(X)F(I) 
is integrable on R T. In terms of 
Definition 58, take 
A = A x Af{T) x I(R T) 
so M. = λί(Τ) 
and J = I(R T). Then 1A(X)F(I) 
is integrable if and only if 
lA{x,N,I[N])F(I) 
is integrable. 
O 
Example 71 If none of the triples (x, TV, I[N]) £ A = AxMxJ 
is associated, 
then, for any function h(x,N,I[N]), 
the set A is not h-measurable. 
This is 
because the function 1A(X)II(X, 
N, I[N]) is not integrable if A has no associated 
elements. 
O 
If lA(x,N,I[N])h(x,N,I[N]) 
is integrable on R T, write 
Ρ,* [A] = / 
lA(ar, TV, I[N])h(x, TV, /[TV]). 
If h(x, TV, /[TV]) is integrable on R T, write 
P ^ [ R T ] - / 
h(x,NJ[N]). 
The following results show that the function P^ has some probability-like prop-
erties. 
Theorem 247 If A is h-measurable and if h is integrable on R T then \A is 
h-measurable and 
P h[\A] 
= P h [RT] - Pft [A]. 

A.l. 
MEASURABILITY 
493 
Proof. We have 
\ A 
= 
(RT xM(T) 
xl(RT))\ 
(Ax 
MxJ), 
l\A(x,N,I[N}) 
= 
l-lA(x,N,I[N}), 
l\A(x,N,I[N])h(x,N,I[N\) 
= 
h(x,N,I[N}) 
- 
lA(x,N,I[N])h(x,N,I[N\). 
By assumption, both functions on the right are integrable on R T, so the result 
follows by Theorem 13. 
O 
Theorem 248 If A.\, A 2 are disjoint h-measurable subsets ofHT, then A1UA2 
is h-measurable, and 
Ρ^[ΑχυΑ 2] 
= 
Ph [Ax] + Ph [A2]. 
Proof. Write A = Ai U A2. We have 
lA(x,N,I[N)) 
= 
lAl(x,N,I[N}) 
+ 
lM(x,N,I[N}), 
lA(x,N,I[N})h(x,N,I[N}) 
= 
lAl(x,N,I[N])h(x,N,I[N)) 
+ 
lA2(x,N,I[N])h(x,N,I[N]). 
By Theorem 13, 
SRTlA(x,N,I[N])h(x,N,I[N}) 
= JRT lAl (x, N, I[N])(x)h(x, 
N,I[N}) 
+ JRT 
lA2(x,N,I[N])(x)h(x,N,I[N}), 
giving the result. 
O 
This result can be extended to any finite union of disjoint sets. For the union 
of a countably infinite family of disjoint sets the following holds. 
Theorem 249 Suppose h and \h\ are integrable on R T. 
/ / A i , A2, A3,... 
are disjoint h-measurable subsets o / R T x Λ/"(Τ) χ I(R T), then Ujii -A,· *s h-
measurable, and 
IK 
7 = 1 
= ΣΡ"[Α; 
j = l 
Proof. Let Bfe = [fj=1 A., and let A = ( J ^ i A.j- Then, for each 
{x,N,I[N}), 
as k —>· 00, 
lBk(x,N,I[N]) 
-► 
lA(x,N,I[N}), 
1B„(X, N,I[N])h{x,N,I[N]) 
-► 
lA(x,N,I[N})h(x,N,I[N}). 

494 
APPENDIX A. 
EPILOGUE 
Given ε > 0, for each (x, N, I[N]) there exists k£ so that k > k£ implies 
\lA(x,N,I[N])h(x,N,I[N}) 
- lBk(x,N,I[N])h(x,N,I[N})\ 
< 
\h(x,N,I[N})\, 
with 
lBk(x,N,I[N])h(x,N,I[N]) 
< 
\h(x,NJ[N})\ 
for each k. By hypothesis, \h(x,N,I[N])\ 
is integrable on R T, so Theorem 61 
(dominated convergence theorem) gives the result. 
O 
Example 72 For each j , suppose Aj = Aj x λί(Τ) χ I(R T) as in Example 70, 
the Aj being disjoint. Then Theorem 249 gives 
IM 
3 = 1 
3 = 1 
The countable additivity of the function P^ on disjoint measurable sets is, ult-
imately, a consequence of the properties of Riemann sums restricted by gauges. 
Next, we define /i-measurability of a function /. 
Definition 59 Given a real-valued function f(x,N,I[N]) 
and a real- or com-
plex-valued function h(x,N,I[N}), 
we say that f is h-measurable if, for each 
a EH, each of the sets 
{(x, N, I[N}) : /(*, N, I[N}) < a} , {(x, N, I[N}) : /(*, N, I[N}) > a} , 
{(x, N, I[N}) : f(x, N, I[N}) > a} , {(x, N, /[TV]) : f(x, N, I[N}) < a} 
is h-measurable. 
If / is complex-valued then its /i-measurability is defined by taking the real and 
imaginary parts of /. 
Theorem 250 Suppose h(x,N,I[N]) 
is real-valued, with h and \h\ integrable 
on R T. Suppose /(x, TV, 7[TV]) is real-valued, with fh, f\h\, and \fh\ integrable 
on R T. Then f is h-measurable. 
Proof. Suppose v G R . Write 
k(x,N,I\N}) 
= \f(x,N,I[N])h(x,N,I[N})\ 
+ 
\vh(x,N,I[N})\. 
The function k is integrable because, by hypothesis, each of the two functions 
of which it is composed is integrable. We have 
-k(x,N,I[N}) 
< 
f(x,N,I[N])h(x,N,I[N}) 
< 
k(x,N,I[N}), 
-k(x,N,I[N}) 
< 
vh(x,N,I[N}) 
< 
k(x,N,I[N\). 

A.l. 
MEASURABILITY 
495 
By Theorem 59, the functions 
max{f(x, N, I[N])h(x, N, I[N]), vh(x, N, I[N})} , 
min {f{x, TV, I[N])h(x, TV, I[N]), vh{x, TV, I[N])} 
are integrable for every v G R . Thus, for u < v, the function p(x, TV, I[N]) given 
by 
ma,x{mm{f(x,N,I[N})h(x,N,I[N}), 
vh(x,N,I[N})}, 
uh(x,N,I[N})} 
is integrable. We have 
r «Ä(x,jv,/[iv]), 
p(ar, N, I[N]) = j /(*, TV, /[JV])Mx, JV, /[JV]), 
ί 
vh(x,N,I[N}), 
whenever, respectively, 
f{x, N, I[N}) <u, 
u< f(x, N, I[N] <v, 
v< f(x,N, 
I[N}). 
Let S denote 
{(x,N,I[N}):f(x,N,I[N])>v}. 
Write 
q(x,N,I[N}) 
= 
p(x,N,I[N])-uh(x,N,I[N}). 
For u —v v—, 
q{x'N'I[N]) 
-> 
ls(x,N,I[N])h(x,N,I[N}) 
v — u 
monotonically; so for any ε > 0 there exists u£ so that v > u > ue implies 
\q(x,N,I[N}) 
ls(x,N,I[N})h(x,N,I[N}) 
< 
e\h(x,N,I[N]\. 
υ — u 
Therefore, by Theorem 57, ls(x, N, I[N]) is /i-integrable for every « e R . Then 
(R T x λί(Τ) x I(R T)) \ S = 
{(*, N, I[N]) : f(x, N, I[N}) < v} 
is /i-measurable for every v. The other conditions required for /i-measurability 
of / are proved similarly. 
O 
Theorem 251 Suppose h and \h\ are integrable on R T. Suppose Ai, Α2 are 
h-measurable subsets ofHT x λί(Τ) x I(R T). Then Ai Π A2 and Ai U A2 are 
h-measurable subsets o/R T x Af(T) x I(R T). 

496 
APPENDIX A. 
EPILOGUE 
Proof. The functions 1A,· (z, N,I[N])h(x, 
N,I[N]) 
are integrable on R T for 
j = 1,2. Therefore, with 
f(x,N,I\N}) 
= lAl(x,N,I[N}) 
+ 
lA2(x,N,I[N}), 
the function f(x, N, I[N])h(x, N, I[N]) is integrable. Since this function satisfies 
the conditions of Theorem 250, the set 
{(x,N,I[N]):f(x,N,I[N})>2} 
is /i-measurable. In other words Ai Π Α2 is /i-measurable. Then 
1A.UA, (X,N,I[N}) 
= 
f(x,N,I[N])-lAinA2(x,N,I[N}), 
and this is h-integrable, giving the second result. 
O 
Theorem 252 Suppose h and \h\ are integrable on R T. If, for each a G R, 
any one of the sets in Definition 59 is h-measurable, then the other sets are also 
h-measurable; and the function f(x,N,I[N]) 
is h-measurable. 
Proof. Denote the sets in Definition 59 by Si, S2, S3, S4, respectively. Assume 
Si is /i-measurable. Then 
S2 = (RTxAf(T) 
x I(R T)) \ S i , 
lS2(x,N,I[N}) 
= 1 - 
lSl(x,N,I[N]), 
ls2(x, N, I[N])h(x, N, I[N}) = h(x,N,I[N}) 
- l S l(x,N, I[N])h(x, N, /[TV]). 
Since h and l s ^ are integrable on R T, so is ls 2^· As this holds for each a G R, 
52 is /i-measurable. Given a G R, for j = 1,2,3,..., let 
A,· = 
i(x,N,I[N)):f(x,N,I[N})>a 
+ 
j \ , 
and, for k = 1,2,3,..., let B^ = U,-=1 Aj. As k -> oo, 
lBk(x,N,I[N)) 
-»· 
lS3(x,N,I[N]). 
Let 
gk(x,N,I[N}) 
= 
lBk(x,N,I[N])h(x,N,I[N\). 
For each /c, g^ is integrable on R T, with 
\gk(x,N,I[N})\ 
< 
\h(x,N,I[N])\. 
Given ε > 0, there exists k£ so that k > k£ implies 
\ls3(x,N,I[N])h(x,N,I[N)) 
- 
lBk(x,N,I[N])h(x,N,I[N})\ 
= 
\lS3(x,N,I{N])h(x,N,I[N})-gk(x,N,I[N})\, 
< 
e\h(x,N,I[N})\. 

A.l. 
MEASURABILITY 
497 
As \h\ is integrable, Theorem 61 (dominated convergence theorem) implies that 
lim gk(x,N,I[N\), 
= 
lS3(x,N,I[N])h(x,N,I[N}), 
k—too 
is integrable on R T, so S3 is /i-measurable; and this holds for each a G R. 
Finally, 
S4 = (R T x Af(T) x I(R T)) \ S3, 
lSi(x,N,I[N]) 
= 1 - lS3(x,/V, J[/V]), 
1S4(x, N,I[N})h(x, 
N, I[N}) = h{x, N,/[TV]) - l s , ( s , TV, I[N))h{x, N, I[N}); 
giving /i-measurability of S4. Thus /i-measurability of Si implies /i-measurability 
of Sj; for j = 2,3,4. The other cases are proved similarly. 
O 
Theorem 253 Suppose h(x,N,I[N]) 
is real- or complex-valued, with h and \h\ 
integrable on HT, and suppose /(x,iV, I[N]) is h-measurable. Given a £ R, the 
functions f(x,N, I[N]) -f- a and af(x, N, I[N]) are h-measurable. 
Proof. For each a e R, f(x, N,I[N]) 
< a implies f(x,N,I[N]) 
+ a < a + a 
and 
af(x, N, I[N]) < aa 
or 
a/(x, TV, I[N]) > aa 
depending on whether a and a are, respectively, positive, negative, or zero. The 
result then follows from Theorem 252. 
O 
Theorem 254 Suppose h(x,N,I[N]) 
is real- or complex-valued, with h and 
\h\ integrable on R T, and suppose f(x,N,I[N]) 
is h-measurable. 
Then the 
functions (f(x,N,I[N])) 
and \f(x,N,I[N])\ 
are h-measurable. 
Suppose, in 
addition, that f φ 0. Then \ is h-measurable. 
Proof. If a < 0 then |(x, AT, I[N]) : (f{x, AT, I[N}))2 > a\ is the set 
{(x,N,I[N]):\f(x,N,I[N])\>a], 
= R, 
which is /i-measurable since h is integrable. Suppose a > 0. Denote the sets 
{(x,N,I[N]):f(x,N,I[N])>^}, 
{{x,N,I[N}) 
: f(x,N,I[N}) 
< -y/Έ} , 
{(x,N,I[N]):f(x,N,I[N])>a}, 
{(x,N,I[N]):f(x,N,I[N])<-a} 
by Ai, A2, A3, A4, respectively. Each of these sets is /i-measurable by hypoth-
esis. Then 
{(x,N,I[N]):(f(x,N,I[N}))2>a} 
= 
A j U ^ , 
{(x,N,I[N]):\f(x,N,I\N})\>a} 
= 
A3UA4, 

498 
APPENDIX A. 
EPILOGUE 
f(x,N,I[N])>0}, 
f(x,N,I[N})<0}, 
/(χ,ΛΓ,7[ΛΓ])<Ι}. 
each of which is /i-measurable. Write 
Si 
= 
{{x,N,I[N}) 
5 2 
= 
{{x,N,I[N}) 
5 3 
= 
{(x,N,I[N\) 
With / φ 0, the set 
( Si 
if a = 0, 
{(^nN]):f{xJJ[N])>a} 
= 
SlnS3 
if 0 0 , 
i ( S i U S 2 ) n S 3 
if 
a < 0 , 
so j is /i-measurable. 
O 
Theorem 255 Suppose h and \h\ are integrable on R T and suppose each of the 
functions f(x,N,I[N]) 
and g(x, N, I[N]) is h-measurable. Then the set 
A = 
{(x,N,I[N]):f(x,N,I{N})<g(x,N,I[N))} 
is h-measurable. 
Proof. Let {αι, 02,03,...} be an enumeration of the rational numbers in R, 
and let 
k 
Aj = {(x,N,I[N]) 
: f(x,N,I[N]) 
< a3- < g(x,NJ[N])}, 
Bk = ( J A,, 
3=1 
Then A = Ujli A7 an<l· as A: -> 00, 
lBk(x,N,I[N}) 
-+ 
lA(x,N,I[N}), 
lBk(^N, 
I[N])h(x,NJ[N]) 
-+ 
lA(x,N,I[N])h{x,N,I[N]); 
with \lBk{x,N,I[N])h(x,N,I[N])\ 
< \h(x,N,I[N])\ 
for each fc, where both 
l&k(x,N,I[N])h(x,JV,I[N]) 
and \h\ are integrable. Given e > 0 we can find k£ 
so that, for k > h£, 
\lA(x,N,I[N])h(x,N,I[N}) 
- lBk(x,N,I[N])h(x,N,I[N})\ 
< 
e\h(x,N,I\N})\. 
Therefore by Theorem 61 (dominated convergence theorem), 
lA(x,N,I[N])h(x,N,I[N]) 
is integrable on R T, giving the result. 
O 

A.l. 
MEASURABILITY 
499 
Theorem 256 Suppose h and \h\ are integrable on R T, and suppose the func-
tions f and g are h-measurable. Then the functions f — g, f + g, and fg are 
h-measurable. If g φ 0 then £ is h-measurable. 
Proof. For any a G R, the function a + g(x,N,I[N]) 
is /i-measurable. Then, 
by Theorem 255, the set 
{(x, N, I[N}) : f(x, N, I[N}) > g(x, N, I[N}) + a}, 
= {(x, N, I[N}) : f(x, N, I[N}) - g(x, N, I[N}) > a} , 
is /i-measurable, so the function / — g is /i-measurable. Since / -f g = f — (—g), 
the function / + g is /i-measurable. Since 
fg = l((f 
+ 
9)2-(f-9)2), 
the preceding results imply that the function fg is /i-measurable. For the final 
part use Theorem 254. 
O 
Theorem 257 Suppose h and \h\ are integrable on R T, and suppose the real-
valued functions fj are h-measurable for j = 1,2,3, — //, for each (x, N, I[N]), 
fj(x,N,I[N]) 
-> 
f(x,N,I[N}) 
as j —» oo, then f(x,N,I[N]) 
is h-measurable. 
Proof. Given e > 0 there exists j£ so that, for j > j £ l 
\/(χ,Ν,Ι[Ν])-Μχ,Ν,Ι[Ν\)\ 
< ε. 
For any a G R write A = {(x,N,I[N]) 
: f(x,N,I[N]) 
> a}. Then, for each 
(x,N,I[N]), 
\lAfjh-lAfh\ 
< 
e\h(x,NJ[N])\; 
the function lA(x,N,I[N])fj(x,N,I[N])h(x,N,I[N]) 
is integrable for each j ; 
and, for each 
(x,N,I[N]), 
lAfjh 
-> 
lAfh 
as j —>· oo. Theorem 61 then implies that the function 
lA(x, N, I[N))f(x, N, I[N])h(x, N, I[N}) 
is integrable. Since this holds for each a, / is /i-measurable. 
O 
The following is a partial converse to Theorem 252. 
Theorem 258 Suppose k(x,N,I[N]) 
> 0 is h-integrable on R T. If real-valued 
f(x,N,I[N]) 
is h-measurable with f < k then f is h-integrable on R T. 

500 
APPENDIX A. 
EPILOGUE 
Proof. We have 
f(x,N,I[N}) 
+ k(x,N,I[N}) 
< 
2k(x,N,I[N]), 
so, if we replace / by / + fc, we can take f(x, N, I[N]) > 0. For each positive 
integer m write 
j 
aJ = 2 ^ ' 
j = 0,1,2,3,..., 
and define 
fm(x,N,I[N]) 
= aj 
for 
aj<f(x,NJ[N])<aj+u 
j = 0,1,... ,m2 m, 
with fm(x, N, I[N]) = 0 if /(x, N, I[N}) > m2" m. Then 
fm(x,N,I[N\) 
< f(x,N,I[N]) 
< 
fm+1(x,NJ[N]) 
and, for each (x, N, I[N]), / m(x, N, I[N]) is monotone increasing as m increases. 
By measurability of /, fm is /i-integrable, and, writing 
A,· = 
{(x,N,I[N]):aj<f(x,N,I[N})<aj+1}, 
we have 
/ 
fm{x,NJ[N])h(x,N,I[N]) 
= 
Σαί1Α,(χ,Ν,Ι[Ν]Μχ,Ν,Ι[Ν]). 
Then fm—tf 
monotonically as m —> oo, and, given ε > 0, for each (#, AT, 7[iV]) 
there exists πιε so that m> τηε implies 
\f(x,N,I[N])h(x,NJ[N}) 
- fm(x,N,I[N])h(x,N,I[N])\ 
< 
ek(x,NJ[N}). 
Provided / > 0 Theorem 57 then implies fh is integrable on R T. If / does not 
satisfy / > 0 we can apply the preceding argument to the function / + k. Then 
the /i-integrability of k and f + k implies that (/ + k) — k is /i-integrable. 
O 
Instead of assuming |/| < k the preceding result remains valid if it is assumed 
that 
■ / > 0 and 
■ J R T fmh is bounded as m -> oo. 
Theorem 259 Suppose f(x,N,I[N]) 
is real-valued and f is h-measurable, and 
suppose g is a continuous real-valued function defined on domain R. Then the 
composite function g o / is h-measurable. 
Proof. If J G I(R), then, by continuity of g, # _ 1(J) G I(R). Write g~x{J) = 
K. By /i-measurability of /, the set 
Γ\Κ), 
= 
{{x,N,I[N}):f(x,N,I[N))eK}, 
is ^-measurable. Thus, for each J G I(R), the set 
( j o / r ' W , 
= 
{(x,N,I[N]):g(f(x,N,I[N]))eJ}, 
is /i-measurable. 
O 

A.2. 
HISTORICAL 
NOTE 
501 
A.2 
Historical Note 
The background to most of the themes in this book is amply documented in 
books and articles. The purpose of this note is to draw together some of the 
newer themes. 
The neo-Riemannian approach to the theory of integration associated with 
R. Henstock, J. Kurzweil, and E.J. McShane originated in the 1950s. The con-
cepts of generalized Riemann (or -complete) integration appeared in 1955 [85], 
where they provided a solution1 to a problem of integrals converging or diverging 
in the presence of convergence factors, a problem first posed by Henstock to his 
Ph.D. supervisor Paul Dienes in 1944. Using his new techniques of Riemann-, 
Stieltjes- and Burkill-complete integration, Henstock solved a number of prob-
lems in papers published in the 1950's and 1960's, culminating in 1963 [93], 
Theory of Integration, which gives a comprehensive account of an early version2 
of the Burkill-complete integral. 
Addressing the 1962 International Congress of Mathematicians in Stock-
holm, Henstock declared: "Lebesgue is dead!". Nonetheless, Henri Lebesgue 
(1875-1941) set the standard [137, 138] for integration in the twentieth cent-
ury. But Henstock's flamboyant announcement that a new, deeper, and more 
effective theory of integration had arrived was already being given additional 
substance by J. Kurzweil, who independently discovered Riemann-complete int-
egration, and whose 1957 paper [129], Generalised ordinary differential equations 
and continuous dependence on a parameter, was creating a new paradigm for 
the theory of differential equations. 
However, this was not the end of the story, just the beginning. The Riemann-
complete integral provides a resolution of the problem of integrating derivatives, 
a significant but relatively small area of the subject—one that had already been 
resolved [82] by Denjoy and Perron, using other methods. 
Integrability of derivatives is part of the broader subject of integrability of 
functions, strands of which can be found [101, 102] in the work of authors such 
as C-J. de la Vallee Poussin, W.H. Young, J.C. Burkill* L.C. Young, A.J. Ward, 
and S. Saks. Prom the point of view of this book a particularly important strand 
is the role of integration in the theory of approximation. 
About 1962, a physics researcher introduced Henstock to Feynman integrals. 
These constructs had been used successfully in quantum electrodynamics and 
quantum mechanics since 1948. But no adequate mathematical method had 
been found to establish whether or not they "converge"; that is, whether or not 
they actually exist in a specifically mathematical sense. 
Feynman "integrals" worked in practice but not in theory, so to speak. The 
1 Theorem 67 of this book is, essentially, Theorem 1 of Henstock [85]. 
2Henstock's Theory of Integration uses the term "Riemann-complete" to designate integrals 
which, in this book, are distinguished from each other as Riemann-complete, Stieltjes-complete 
and Burkill-complete. 
3In [103] Henstock cites the work of W.H. Young, as developed in lectures by J.C. Burkill, 
as a source of the inspiration for his proof by Riemann sums of Pubini's theorem. Henstock 
attended a course of lectures by J.C. Burkill in 1942. His Ph.D. thesis, Interval Functions and 
Their Integrals (1948), is an investigation and extension of J.C. Burkill's idea of the integral. 

502 
APPENDIX A. 
EPILOGUE 
Lebesgue integral method could not resolve this issue. Could the new develop-
ments in integration theory do it? 
Independently, E.J. McShane [11] engaged with this problem. His search for 
a rigorous mathematical foundation for quantum field theory led him to sus-
tained and productive involvement in the theory of integration. Seminal works 
by McShane include Order-Preserving Maps and Integration Processes in 1953 
[153], Integrals devised for special purposes in 1963 [155], and Stochastic Calcu-
lus and Stochastic Models in 1974 [158] which has influenced much subsequent 
research in this area. 
With such issues in mind, over the following decade Henstock embarked on 
construction of a more complete and comprehensive theory of integrability. Page 
221 of Henstock [94] has a sketch of the historical background to some of these 
developments. His paper [101] (The Lebesgue syndrome) is also useful. 
In 1968 Henstock published Linear Analysis [94], containing an initial expo-
sition of the division system (or integration basis) theory described in Chapter 4 
of the present book, and designated here as the Henstock integral. The Riemann-
complete integral is a modification of the nineteenth century Riemann integral 
with more delicate selection of Riemann partitions. The Henstock integral is 
based on 
■ classes of objects to be selected for partitions, and 
■ classes of rules for determining selections. 
This system ties the integral to partitions (or divisions), while setting it free 
from any particular method of integration such as Riemann's or Lebesgue's. The 
basic idea can be described simply as the Riemann sums method in integrability. 
Linear Analysis [94] also included an outline (Exercises 43.12 and 43.13, 
pages 223-224) of integration in infinite-dimensional spaces, partly inspired by 
a 1934 paper [116] by B. Jessen. 
The Henstock integral, or division system, was presented more fully [96] in 
1969. Henstock's 1973 paper [97] showed how the system can be used to define 
the Feynman integral as a manifestation of an extended version of Brownian 
motion, thus bringing this construct into the realm of mathematical integration 
and probability—which is the spirit in which it was originally presented by 
Feynman in 1948 [64]. 
Henstock [97] is a historic landmark in the theory of integration and prob-
ability. But definition is one thing, existence is another. It was not shown that 
the Feynman integrals exist. And if they do happen to exist, means of perform-
ing more than a finite number of operations on them were not provided. The 
question of taking limits under the integral sign for highly oscillating integrands 
such as these was broached in this paper, but not resolved there. Henstock con-
cluded [97] with the statement: "This shows how difficult it will be to introduce 
Lebesgue-type limit theorems into Feynman integration theory. ... The present 
paper is only the beginning of the theory." 
A General Theory of Integration in Function Spaces (Muldowney [162], 1987) 
extended the division system idea, providing a broader structure of associated 

A.2. 
HISTORICAL 
NOTE 
503 
elements of integration with the capacity to deal with a large class of integrands 
in infinite-dimensional spaces. This is, essentially, the extension of the Henstock 
integral used in the present book. It was established in Muldowney [162] that the 
Feynman integrals of step function integrands exist. But to go further requires 
taking limits under the integral sign. 
Criteria for taking limits under the integral sign—actually a departure from 
the "Lebesgue-type limit theorems" mentioned in the conclusion of Henstock 
[97]—were constructed by Henstock for the division system theory in his 1991 
book The General Theory of Integration [105]. These criteria (similar to The-
orems 62, 64, and 65 in this book) were used by Muldowney in 2000 [165] to 
establish properties of the Feynman integral. 
The partitioning theorem (Theorem 4) used in this book was proved by Hen-
stock, Muldowney, and Skvortsov [107]. This result simplifies and strengthens 
the definition of the Henstock integral in infinite-dimensional spaces. 
The following assessment of the Feynman integral problem was given by 
Gian-Carlo Rota in his 1997 book Indiscrete Thoughts [199]: 
The Feynman path integral is the mathematicians' pons asinorum. Attempts 
to put it on a sound footing have generated more mathematics than any subject 
in physics since the hydrogen atom. To no avail. The mystery remains, and it 
will stay with us for a long time. 
The Feynman integral, one of the most useful ideas in physics, stands as a 
challenge to mathematicians. 
While formally similar to Brownian motion, and 
while admitting some of the same manipulations as the ones that were made 
rigorous long ago for Brownian motion, it has withstood all attempts at rigor. 
Behind the Feynman integral there lurks an even more enticing (and even less 
rigorous) concept: that of an amplitude which is meant to be the quantum-
mechanical analog of probability ... 
Rota went on to say: A concept similar to that of a sample space should be 
brought into existence for amplitudes and quantum mechanics should be devel-
oped starting from this concept. 
The brief historical outline in this section is a summary of developments in 
the agenda described by Rota—a work-in-progress but no longer a mystery. 

Bibliography 
[1] Abbott, S., Understanding Analysis, Springer-Verlag, New York, 2001. 
[2] Adams, P., Smith, K., and Vyborny, R., Introduction to Mathematics with 
Maple, World Scientific, Singapore, 2004. 
[3] Adams, P., and Vyborny, R., Maple tools for the Kurzweil integral, Math-
ematica Bohemica 131(4) (2006), 337-346. 
[4] Ash, R.B., Doleans-Dade, CA., Probability and Measure Theory, Acad-
emic Press, San Diego, 2000. 
[5] Bachelier, L., Theorie de la speculation, Annales Scientifiques de l'Ecole 
Normale Superieure 17 (1900), 21-86. 
[6] Bartle, R.G., Return to the Riemann integral, American Mathematical 
Monthly 103(8) (1980), 625-632. 
[7] Bartle, R.G., A Modern Theory of Integration, John Wiley & Sons, Hob-
oken, 2001. 
[8] Bartle, R.G., and Sherbert, D.R., Introduction to Real Analysis, John 
Wiley & Sons, Hoboken, 1999. 
[9] Baxter, M., and Rennie, A., Financial Calculus, Cambridge University 
Press, Cambridge, 1996. 
[10] Black, F., and Scholes, M., The pricing of options and corporate liabilities, 
Journal of Political Economy 81 (1973), 637-659. 
[11] Berkovitz, L.D., and Fleming, W.H., Edward James McShane 1904-1989, 
The National Academy Press, Biographical Memoirs 80, Washington, 
2001. 
www. nap. edu/readingroom. php?book-biomems&page=emcshane. html 
(accessed 21 December 2011). 
[12] Bongiorno, B., Un nuovo integrale per il problema delle primitive, Le 
Matematiche (Catania) 51 (1996), 299-313. 
[13] Bongiorno, B., Di Piazza, L., and Musial, K., A characterization of the 
Radon-Nikodym 
property by finitely additive interval functions, Illinois 
Journal of Mathematics 53(1) (2009), 87-99. 
A Modern Theory of Random 
Variation: With Applications 
in Stochastic Calculus, 
505 
Financial Mathematics, 
and Feynman Integration. 
First Edition. By Pat Muldowney 
Copyright © 2012 John Wiley & Sons, Inc. Published by John Wiley & Sons, Inc. 

506 
BIBLIOGRAPHY 
[14] Bongiorno, B., Di Piazza, L., and Preiss, D., 
A constructive 
minimal 
integral which includes the Lebesgue integrable functions and derivatives, 
Journal of the London Mathematical Society 62 (2000), 117-126. 
[15] Bongiorno, B., Di Piazza, L., and Skvortsov, V., A new full descriptive 
characterization of Denjoy-Perron 
integral, Real Analysis Exchange 21 
(1995-96), 256-263. 
[16] Bongiorno, B., Giertz, M., and Pfeffer, W.F., Some nonabsolutely con-
vergent integrals in the real line, Bollettino Unione Matematica Italiana 
7(6-B) (1992), 371-402. 
[17] Bonotto, E. de Mello, A Equacäo de Black-Scholes com Αςάο Impulsiva, 
PhD thesis, University of Sao Paolo, Sao Carlos, 2008. 
[18] Bonotto, E. de Mello, Federson, M., and Muldowney, P., A Feynman-
Kac solution to a random impulsive equation of Schrödinger type, Real 
Analysis Exchange 36(1) (2010-11), 107-148. 
[19] Borkar, V.S., Probability Theory: An Advanced Course, Springer, New 
York, 1995. 
[20] Burk, F., A Garden of Integrals, Mathematical Association of America, 
Washington, 2007. 
[21] Bullen, P.S., Axiomatisations 
of various non-absolute integrals, Bulletin 
of the South East Asia Mathematical Society, Special Issue, 2 (1979), 
173-189. 
[22] Bullen, P.S., Non-absolute integrals; a survey, Real Analysis Exchange 5 
(1979-80), 195-259. 
[23] Bullen, P.S., Lee, P.Y., Mawhin, J.L., Muldowney, P., and Pfeffer, 
W.F. (editors), New Integrals: Proceedings of the Henstock Conference, 
Coleraine, Northern Ireland, 1988, Lecture Notes in Mathematics 1419, 
Springer-Verlag, Berlin-Heidelberg, 1990. 
[24] Bullen, P.S., Nonabsolute integration in the twentieth century, Ameri-
can Mathematical Society Special Session on Nonabsolute Integration, 
Toronto, 23-24 September, 2000, 
www.emis.de/proceedings/Toronto2000/papers/bullen.pdf 
(accessed 
12 
November 2011). 
[25] Burkill, J.C., Functions of intervals, Proceedings of the London Mathe-
matical Society, 22(2) (1924), 275-310. 
[26] Burkill, J.C., The expression of area as an integral, Proceedings of the 
London Mathematical Society, 22(2) (1924), 311-336. 
[27] Burkill, J.C., Henri Lebesgue, Journal of the London Mathematical Soci-
ety, 19 (1944), 56-64. 

BIBLIOGRAPHY 
507 
[28; 
[29 
Burkill, J.C., Obituary: Henri Lebesgue 1875-1941, Obituary Notices of 
Fellows of the Royal Society of London 4 (1944), 483-490. 
Burkill, J.C., and Burkill, H., A Second Course in Mathematical Analysis, 
Cambridge University Press, Cambridge, 1970. 
[30] Cameron, R.H., and Storvick, D.A., A Simple Definition of the Feynman 
Integral, With Applications, Memoirs of the American Mathematical Soc-
iety, No. 288, Providence, 1983. 
[31] Cao, S.S., The Henstock integral for Banach-valued functions, South East 
Asia Bulletin of Mathematics 16 (1992), 35-40. 
[32] Capinski, M., and Ekkehard, K., Measure, Integral and Probability, 
Springer, London, 1999. 
[33] Caratheodory, C, Mass und Integral und ihre Algebraisierung, Birkhäuser 
Verlag, Basel and Stuttgart, 1956. 
[34] Celidze, V.G., and Dzarseisili, A.G., The Theory of the Denjoy Integral 
and Some Applications, Series in Real Analysis Volume 3, World Scien-
tific, Singapore, 1989, translated by P.S. Bullen. (The original Russian 
edition: Tbilisi University Press, Tbilisi, 1978.) 
[35] Chatterji, S.D., and Wefelscheid, H. (editors), G.C. Young, W.H. Young: 
Selected Papers, Presses Polytechniques et Universitaires Romandes, Laus-
anne, 2000. 
[36] Chew, T.S., and Toh, T.L., The Riemann approach to stochastic inte-
gration using non-uniform mesh, Journal of Mathematical Analysis and 
Applications 280(1) 2003, 133-147. 
[37] Chew, T.S., and Toh, T.L., The Riemann approach to multiple Wiener 
integral, Real Analysis Exchange 29(1) (2003-04), 275-289. 
[38] Chew, T.S., and Toh, T.L., On Henstock-Fubini's 
theorem for multiple 
stochastic integral, Real Analysis Exchange 30(1) (2004-05), 295-310. 
[39] Chew, T.S., Huang, Z.Y., and Wang, C.S., The non-uniform 
Riemann 
approach to anticipating stochastic integrals, Stochastic Analysis and App-
lications 22(2) (2005), 429-442. 
[40] Chew, T.S., and Toh, T.L., Henstock's version of the Ito formula, Real 
Analysis Exchange 35(2) (2009-10), 1-20. 
[41] Chung, K.L., A Course in Probability Theory, Academic Press, San Diego, 
2001. 
[42] Chung, K.L., and Williams, R.J., Introduction to Stochastic Integration, 
Birkhäuser, Boston, 1990. 

508 
BIBLIOGRAPHY 
[43] Cousin, P., Sur les fonctions de n variables complexes, Acta Mathematica 
19 (1895), 1-62. 
[44] Cyganowski, S., Kloeden, P., and Ombach, J., Prom Elementary Prob-
ability to Stochastic Differential Equations 
With MAPLE, 
Springer, 
Berlin, 2002. 
[45] Daniell, P.J., Integrals in an infinite number of dimensions, Annals of 
Mathematics 20 (1919), 281-288. 
[46] Das, A.G., and Das, U., Convergence in bounded k-th variation and RSk 
integrals, Journal of the Australian Mathematical Society (Series A) 31 
(1981), 163-174. 
[47] Das, A.G., and Ray, S.K., A new definition of generalized Riemann integ-
ral, Bulletin of the Institute of Mathematics Academia Sinica 18 (1990), 
273-282. 
[48] Das, A.G., Nath, M.C., and Sahu, G., Generalized Henstock-Stieltjes 
int-
egral, Bulletin of the Institute of Mathematics Academia Sinica 26(1) 
(1998), 61-75 
[49] Das, A.G., The Riemann, Lebesgue, and Generalized Riemann Integrals, 
Narosa, New Delhi, 2008. 
[50] Davies, R.O., and Schuss, Z., A proof that Henstockys integral includes 
Lebesgue's, Journal of the London Mathematical Society 2 (1970), 561-
562. 
[51] Dellacherie, C., and Meyer, P.A., Probabilites et Potential, Theorie des 
Martingales, Hermann, Paris, 1980. 
[52] Di Piazza, L., Varational measures in the theory of the integration in R m, 
Czechoslovak Mathematical Journal 51 (2001), 95-110. 
[53] Di Piazza, L., and Maraffa, V., The McShane, PU and Henstock integ-
rals of Banach valued functions, Czechoslovak Mathematical Journal 52 
(2002), 609-633. 
[54] Dirac, P.A.M., The Lagrangian in quantum mechanics, Physikalische 
Zeitschrift der Sowjetunion 3 (1933), 64-72. 
[55] Dirac, P.A.M., The Principles of Quantum Mechanics, Oxford University 
Press, Oxford, 1947. 
[56] Einstein, 
A., 
Über die von der molekularkinetischen 
Theorie der 
Warme geforderte Bewegung von in ruhenden Flüssigkeiten suspendierten 
Teilchen, Annalen der Physik 17(4) (1905), 549-560. 
[57] Doob, J.L., Stochastic Processes, John Wiley h Sons, New York, 1953. 

BIBLIOGRAPHY 
509 
[58; 
[59 
Elliott, R.J., and Kopp, P.E., Mathematics 
of Financial 
Markets, 
Springer, New York, 2005. 
Federson, M., The monotone convergence theorem for 
multidimensional 
abstract Kurzweil vector integrals, Czechoslovak Mathematical Journal 
52(2) (2002), 429-437. 
[60] Federson, M,, A constructive integral equivalent to the integral of Kurzweil, 
Czechoslovak Mathematical Journal 52(2) (2002), 365-367. 
[61] Federson, M., and Taboas, P., Impulsive retarded differential equations in 
Banach spaces via Bochner-Lebesgue and Henstock integrals, Nonlinear 
Analysis: Theory, Methods and Applications 50(3) (2002), 389-407. 
[62] Feller, W., An Introduction to Probability Theory and Its Applications, 
Volume 1, John Wiley & Sons, New York, 1950. 
[63] Feller, W., An Introduction to Probability Theory and Its Applications, 
Volume 2, John Wiley & Sons, New York, 1966. 
[64] Feynman, R., Space-time approach to non-relativistic quantum mechanics, 
Reviews of Modern Physics 20 (1948), 367-387. 
[65] Feynman, R., Mathematical formulation of the quantum theory of electro-
magnetic interaction, Physical Review 80 (1950), 440-457. 
[66] Feynman, R., An operator calculus having applications in quantum elec-
trodynamics, Physical Review 84 (1951), 108-128. 
[67] Feynman, R.P., and Hibbs, A.R., Quantum Mechanics and Path Integrals, 
McGraw-Hill, New York, 1965. 
[68] Feynman, R.P., Electrons and their Interactions, 
www.vega.org.uk/video/programme/47 (accessed 12 November 2011). 
[69] Fremlin, D.H., On the Henstock and McShane integrals of vector-valued 
functions, Illinois Journal of Mathematics 38(3) (1994), 471-479. 
[70] Fremlin, D.H., The generalized McShane integral, Illinois Journal of Math-
ematics 39(1) (1995), 39-67. 
[71] Fremlin, D.H., Vector-valued Saks-Henstock indefinite integrals, Univers-
ity of Essex, www.essex.ac.uk/maths/people/fremlin/nl0805.pdf (accessed 
20 December 2011). 
[72] Fremlin, 
D.H., 
Products of gauge integrals, University of Essex, 
www. essex. ac. uk/maths/people/fremlin/n07k24. pdf (accessed 20 Decem-
ber 2011). 
[73] Friedrichs, K. O., et al., Integration of Functionals, New York University 
Institute of Mathematical Sciences, New York, 1957. 

510 
BIBLIOGRAPHY 
[74] Ganguly, D.K., Lee, P.Y., and Pal, S., Henstock-Stieltjes 
integrals not 
induced by measure, Real Analysis Exchange 26(2) (2000), 853-860. 
[75] Ganguly, D.K., and Mukherjee, R., The generalized approximate Perron 
integral, Mathematica Slovaca 58(1) (2008), 31-42. 
[76] Ganguly, D.K., and Mukherjee, R., On primitive for the GAP-integral, 
Mathematica Slovaca 61(4) (2011), 665-673. 
[77] Ganguly, D.K., and Mukherjee, R., Two important extension theorems for 
the GAP-integral, Journal of the Indonesian Mathematical Society 17(1) 
(2011), 49-58. 
[78] Gelfand, I.M., and Yaglom, A.M., Integration in function spaces, Journal 
of Mathematical Physics 1 (1960), 48-69. 
[79] Gikhman, I.I., and Skorokhod, A.V., Introduction to the Theory of Ran-
dom Processes, Saunders, Philadelphia, 1969. 
[80] Gill, T.L., and Zachary, W.W., Banach spaces for the Feynman integral, 
Real Analysis Exchange 34(2) (2008), 267-310. 
[81] Glimm, J., and Jaffe, A., Quantum Physics: A Functional Integral Point 
of View, Springer, New York, 1981. 
[82] Gordon, R. A., The Integrals of Lebesgue, Denjoy, Perron, and Henstock, 
American Mathematical Society, Providence, 1994. 
[83] Gordon, R. A., The use of tagged partitions in elementary analysis, Amer-
ican Mathematical Monthly 105(2) (1998), 774-781. 
[84] Grattan-Guinness, 
I., 
Mathematical 
bibliography for 
W.H. 
and 
G.C. Young, Historia Mathematica 2 (1975), 43-58. 
[85] Henstock, R., The efficiency of convergence factors for functions of a 
continuous real variable, Journal of the London Mathematical Society 30 
(1955), 273-286. 
[86] Henstock, R., On Ward's Perron-Stieltjes 
integral, Canadian Journal of 
Mathematics 9 (1957), 96-109. 
[87] Henstock, R., The summation by convergence factors of Laplace-Stieltjes 
integrals outside their half plane of convergence, Mathematische Zeitschrift 
67 (1957), 10-31. 
[88] Henstock, R., A new description of the Ward integral, Journal of the 
London Mathematical Society 35 (1960), 43-48. 
[89] Henstock, R., The use of convergence factors in Ward integration, Pro-
ceedings of the London Mathematical Society 3(10) (1960), 107-121. 

BIBLIOGRAPHY 
511 
[90] Henstock, R., The equivalence of generalized forms of the Ward, vari-
ational, Denjoy-Stieltjes, 
and Perron-Stieltjes 
integrals, Proceedings of 
the London Mathematical Society 3(10) (1960), 281-303. 
[91] Henstock, R., N-variation and N-variational integrals of set functions, 
Proceedings of the London Mathematical Society 3(11) (1961), 109-133. 
[92] Henstock, R., Definitions of Riemann type of the variational integrals, 
Proceedings of the London Mathematical Society 3(11) (1961), 402-418. 
[93] Henstock, R., Theory of Integration, Butterworth, London, 1963. 
[94] Henstock, R., Linear Analysis, Butterworth, London, 1968. 
[95] Henstock, R., A Riemann-type integral of Lebesgue power, Canadian Jour-
nal of Mathematics 20 (1968), 79-87. 
[96] Henstock, R., Generalised integrals of vector-valued functions, Proceedings 
of the London Mathematical Society 19 (1969), 509-536. 
[97] Henstock, R., Integration in product spaces, including Wiener and Feyn-
man integration, Proceedings of the London Mathematical Society 27 
(1973), 317-344. 
[98] Henstock, R., Additivity and the Lebesgue limit theorems, Proceedings of 
the Greek Mathematical Society Caratheodory Symposium, 1974, 223-
241. 
[99] Henstock, R., Integration, variation and differentiation in division spaces, 
Proceedings of the Royal Irish Academy 78A(10) (1978), 69-85. 
[100] Henstock, R., Division spaces, vector-valued functions and backwards mar-
tingales, Proceedings of the Royal Irish Academy 80A(2) (1980), 217-233. 
[101] Henstock, R., The Lebesgue syndrome, Real Analysis Exchange 9 (1983-
84) 96-110. 
[102] Henstock, R., A short history of integration theory, South East Asia Bull-
etin of Mathematics 12(2) (1988), 75-95. 
[103] Henstock, R., Lectures on the Theory of Integration, World Scientific, 
Singapore, 1988. 
[104] Henstock, R., Stochastic and other functional integrals, Real Analysis Ex-
change 16 (1990-91), 460-470. 
[105] Henstock, R., The General Theory of Integration, Clarendon, Oxford, 
1991. 
[106] Henstock, R., The integral over product spaces and Wiener formula, Real 
Analysis Exchange 17(1) (1991-92), 737-744. 

512 
BIBLIOGRAPHY 
[107] Henstock, R., Muldowney, P., and Skvortsov, V.A., Partitioning 
infinite-
dimensional spaces for generalized Riemann integration, Bulletin of the 
London Mathematical Society 38 (2006), 795-803. 
108] Henstock Archive, University of Ulster, Coleraine, 2007. 
109] Hewitt, E., Integration by parts for Stieltjes integrals, The American Math-
ematical Monthly 67(5) 1960, 419-423. www.jstor.org/stable/2309287 
(accessed 1 October 2008.) 
1101 Hull, J.C., Options, Futures and Other Derivatives, Pearson, New Jersey, 
1988. 
Uli Ingersoll, J.E., Theory of Financial Decision Making, Rowmann and 
Littlefield, Savage, 1987. 
112] Ito, K., and McKean, H.P., Diffusion Processes and their Sample Paths, 
Academic Press, New York, 1965. 
113] Jacod, J., and Protter, P., Probability Essentials, Springer, Berlin, 2000. 
1141 Jarnik, J., and Kurzweil, J., A nonabsolutely convergent integral which 
admits transformation 
and can be used for integration on manifolds, 
Czechoslovak Mathematical Journal 35 (1986), 116-139. 
1151 Jarrow, R., and Protter, P., A short history of stochastic integration and 
mathematical finance: the early years, 1880-1970, 
http://people.orie.Cornell.edu/ protter/WebPapers/historypaper7.pdf 
(accessed 7 March 2011). 
1161 Jessen, B., The theory of integration in a space of an infinite number of 
dimensions, Act a Mathematica 63 (1934), 249-323. 
1171 Johnson, G.W., and Lapidus, M.L., The Feynman Integral and Feynman's 
Operational Calculus, Oxford University Press, Oxford, 2000. 
1181 Kac, M., On distributions of certain Wiener functionals, Transactions of 
the American Mathematical Society 65(1) (1949), 1-13. 
1191 Kac, M., Probability and Related Topics in the Physical Sciences, Inter-
science, New York, 1957. 
1201 Karatzas, I., and Shreve, S. E., Brownian Motion and Stochastic Calculus, 
Springer-Verlag, New York, 1991. 
1211 Karatzas, I., and Shreve, S.E., Methods of Mathematical 
Finance, 
Springer-Verlag, New York, 1998. 
1221 Kestelman, H., Modern Theories of Integration, Clarendon Press, Oxford, 
1937. 

BIBLIOGRAPHY 
513 
123] Kolmogorov, 
A.N., 
Grundbegriffe 
der 
Wahrscheinlichkeitreichnung, 
Ergebnisse der Mathematik, Springer, Berlin, 1933 (Foundations of the 
Theory of Probability, Chelsea Publishing Company, New York, 1950). 
124] Kubota, Y., A direct proof that the RC-integral is equivalent to the D*-
integral, Proceedings of the American Mathematical Society 80 (1980), 
293-296. 
125] Kubota, Y., Notes on integration, Mathematica Japonica 31 (1986), 617-
621. 
126] Kurtz, D.S., and Swartz, C.W., Theories of Integration: The Integrals of 
Riemann, Lebesgue, Henstock-Kurzweil, 
and McShane, World Scientific, 
Singapore, 2004. 
127] Kurzweil, J., Generalised ordinary differential equations and continuous 
dependence on a parameter, Czechoslovak Mathematical Journal 7(82) 
(1957), 418-449. 
128] Kurzweil, J., On Fubini theorem for general Perron integral, Czechoslovak 
Mathematical Journal 23(2) (1973), 286-297. 
129] Kurzweil, J., On multiplication of Perron-integrable functions, Czechoslo-
vak Mathematical Journal 23(4) 1973, 542-566. 
130] Kurzweil, J., Nichtabsolut Konvergente Integrale, Teubner-Texte zur 
Mathematik, Teubner, Leipzig, 1980. 
131] Kurzweil, J., Henstock-Kurzweil Integration: Its Relation to Topological 
Vector Spaces, World Scientific, Singapore, 2000. 
132] Kurzweil, J., Integration Between the Lebesgue Integral and the Henstock-
Kurzweil Integral: Its Relation to Local Convex Vector Spaces, World Sci-
entific, Singapore, 2002. 
133] Kurzweil, J., The revival of the Riemannian approach to integration, Ban-
ach Center Publications 64 (2004), 159-172. 
134] Kwok, Y.K., Mathematical Models of Financial Derivatives, Springer-
Verlag, Singapore, 1998. 
135] Lamperti, J.W., Probability: A Survey of the Mathematical Theory, John 
Wiley k Sons, Hoboken, 1996. 
136] Leader, S., The Kurzweil-Henstock Integral and Its Differentials, Marcel 
Dekker, New York, 2001. 
137] Lebesgue, H., Sur une generalisation de Vintegrale definie, Comptes Ren-
dus de l'Academie des Sciences 132 (1901), 1025-1028. 

514 
BIBLIOGRAPHY 
138] Lebesgue, H., Integrale, longueur, aire, Annali di Matematica Pura ed 
Applicata 7 (1902), 231-359. 
139] Lee, T.W., On the generalized Riemann integral and stochastic integral, 
Journal of the Australian Mathematical Society 21A (1976), 64-71. 
140] Lee, P.Y., Lanzhou Lectures on Henstock Integration, World Scientific, 
Singapore, 1989. 
141] Lee, P.Y., and Vyborny, R., The Integral: An Easy Approach After 
Kurzweil and Henstock, Australian Mathematical Society, Lecture Series 
14, Cambridge University Press, Cambridge, 2000. 
142] Lee, P.Y., and Xu, J.G., Stochastic integrals of Ito and Henstock, Real 
Analysis Exchange 18(2) (1992-93), 352-366. 
143] Littlewood, J.E., Lectures on the Theory of Functions, Oxford University 
Press, Oxford, 1944. 
144] Lukashenko, T.P., Skvortsov, V.A., and Solodov A.P., Generalized Integ-
rals (in Russian), URSS, Moscow, 2nd edition, 2011. 
145] Marraffa, V., A descriptive characterization of the variational Henstock 
integral, Matimyas Matematika 22(2) (1999), 73-84. 
146] Marraffa, V., The variational McShane integral in locally convex spaces, 
The Rocky Mountain Journal of Mathematics 39(6) (2009), 1993-2013. 
147] Mawhin, J., Introduction a VAnalyse, Cabay, Louvain-la-Neuve, 1983. 
148] Mawhin, J., Analyse: Fondements, Techniques, Evolution, De Boeck Uni-
versite, Brussels, 1992. 
149] McKean, H.P., Stochastic Integrals, Academic Press, New York, 1969. 
150] McLeod, R. M., The Generalized Riemann Integral, Mathematical Asso-
ciation of America, Washington, 1980. 
151] McShane, E.J., Integration, Princeton University Press, Princeton, 1944. 
152] McShane, E.J., Remark concerning integration, Proceedings of the Nat-
ional Academy of Sciences of the U.S.A. 35 (1949), 46-49. 
153] McShane, E.J., Order-Preserving Maps and Integration Processes, Prince-
ton University Press, Princeton, 1953. 
154] McShane, E.J., Stochastic integrals and non-linear processes, Journal of 
Mathematics and Mechanics 11 (1962), 235-284. 
155] McShane, E. J., Integrals devised for special purposes, Bulletin of the Amer-
ican Mathematical Society 69 (1963), 597-627. 

BIBLIOGRAPHY 
515 
1561 McShane, E.J., A Riemann Type Integral that Includes 
Lebesgue-Stieltjes, 
Bochner and Stochastic Integrals, Memoirs of the American Mathematical 
Society No. 88, Providence, 1969. 
1571 McShane, E.J., A unified theory of integration, American Mathematical 
Monthly 80 (1973), 349-359. 
158] McShane, E.J., Stochastic Calculus and Stochastic Models, Academic 
Press, New York, 1974. 
159] McShane, E.J., Unified Integration, Academic Press, New York, 1983. 
1601 Merton, R.C., Theory of rational option pricing, Bell Journal of Economics 
and Management Sciences 4 (1973), 141-183. 
1611 Montroll, E.W., Marko ff chains, Wiener integrals and quantum theory, 
Communications in Pure and Applied Mathematics 5 (1952), 415-453. 
1621 Muldowney, P., A General Theory of Integration in Function Spaces, Pit-
man Research Notes in Mathematics, Longman, Harlow, 1987. 
163] Muldowney, P., Infinite-dimensional 
generalised Riemann 
integration, 
New Integrals, Lecture Notes in Mathematics 1419, Springer-Verlag, Heid-
elberg, 1990. 
1641 Muldowney, P., Topics in probability using generalised Riemann 
inte-
gration, Mathematical Proceedings of the Royal Irish Academy 99A(1) 
(1999), 39-50. 
1651 Muldowney, P., Feynman's path integrals and Henstock's non-absolute int-
egration, Journal of Applied Analysis 6(1) (2000), 1-24. 
1661 Muldowney, P., The Henstock integral and the Black-Scholes theory of 
derivative asset pricing, Real Analysis Exchange 26(1) (2000-01), 117-
132. 
1671 Muldowney, P., The infinite dimensional Henstock integral and problems 
of Black-Scholes expectation, Journal of Applied Analysis 8 (2002), 1-21. 
1681 Muldowney, P., Helly's selection principle and the central limit theorem, 
Atti del Seminario Matematico e Fisico dell Universitä di Modena e Reggio 
Emilia 54(1-2) (2006), 183-190. 
1691 Muldowney, P., A Riemann approach to random variation, Mathematica 
Bohemica 131(2) (2006), 167-188. 
1701 Muldowney, P., A generalized Black-Scholes equation without ltd calculus, 
Integration: Mathematical Theory and Applications 1(2) (2008), 179-182. 
1711 Muldowney, P., Henstock on random variation, Scientiae Mathematicae 
Japonicae, No. 247 67(1) (2008), 51-69. 

516 
BIBLIOGRAPHY 
172] Muldowney, P., Ralph Henstock 1923-2007, Scientiae Mathematicae 
Japonicae, No. 247 67(1) (2008), 1-5. 
173] Muldowney, P., and Skvortsov, V.A., Lebesgue integrability implies gen-
eralized Riemann integrability in Rj0'1!, Real Analysis Exchange 27(1) 
(2001-02), 223-234. 
174] Muldowney, P., and Skvortsov, V.A., An improper Riemann integral and 
the Henstock integral in R n (in Russian), Matematicheskie Zametki 78(2) 
(2005), 251-258 (translation in Mathematical Notes 78 (2005), no. 1-2, 
228-233). 
175] Muldowney P., and Wojdowski, W., Nonabsolute integration in Black-
Scholes pricing, Proceedings of AMS Special Session on Non-Absolute 
Integration, Toronto, 2000. 
www.emis.de/proceedings/Toronto2000/papers/wojdowski.pdf (accessed 12 
November 2011). 
176] Muldowney, P., Ostaszewski, K., and Wojdowski, W., The Darth Vader 
Rule, Tatra Mountains Mathematical Publications 12 (2012). 
177] Nakanishi, S., The Denjoy integral defined as the completion of simple 
functions, Mathematica Japonica 37 (1992), 89-101. 
178] Nakanishi, S., A remark for convergence of Denjoy integrals, Mathematica 
Japonica 37 (1992), 473-478. 
179] Nakanishi, S., The Henstock integral for functions with values in nuclear 
spaces, Mathematica Japonica 39 (1994), 309-335. 
180] Nakanishi, S., Approach to non-absolute integration by successive approx-
imations, Scientiae Mathematicae Japonicae 54 (2001), 301-426. 
181] Nakanishi, S., Convergences on the Henstock-Kurzweil integral, Scientiae 
Mathematicae Japonicae 67(1) (2008), 83-90. 
182] Natanson, I.P., Theory of Functions of a Real Variable, Volume II, 1957 
(English translation by L.F. Boron, Frederick Ungar, New York, 1960). 
183] Nelson, E., Feynman integrals and the Schrödinger equation, Journal of 
Mathematical Physics 5 (1964), 332-343. 
184] Neumann, J. von, Mathematische 
Grundlagen der 
Quantenmechanik, 
Springer, Berlin, 1932. 
185] O'Connor, J.J., and Robertson, E.F., Grace Chisholm Young, University 
of St. Andrews, Scotland, 2005. 
http://www.gap-system.org/~history/Biographies/Chisholm_Young.html 
(accessed 16 February 2012). 

BIBLIOGRAPHY 
517 
[186] O'Connor, J.J., and Robertson, E.F., Henri Leon Lebesgue, University of 
St. Andrews, Scotland, 2004. 
http://www-history.mcs.st-andrews.ac.uk/Biographies/Lebesgue.html 
(accessed 28 February 2012). 
[187] O'Connor, J.J., and Robertson, E.F., John Charles Burkill, University of 
St. Andrews, Scotland, 2005. 
http://www-history.mcs.st-andrews.ac.uk/Biographies/Burkill.html 
(accessed 16 February 2012). 
[188] O'Connor, J.J., and Robertson, E.F., Thomas Jan Stieltjes, University of 
St. Andrews, Scotland, 1999. 
http://www-history.mcs.st-andrews.ac.uk/Biographies/Stieltj es.html 
(accessed 16 February 2012). 
[189] O'Connor, J.J., and Robertson, E.F., William Henry Young, University 
of St. Andrews, Scotland, 2003. 
http://www-history.mcs.st-andrews.ac.uk/Biographies/Young.html 
(accessed 16 February 2012). 
[190] 0ksendal, B., Stochastic Differential Equations, Springer-Verlag, Berlin, 
1985. 
[191] Ostaszewski, K., Henstock Integration in the Plane, Memoirs of the Amer-
ican Mathematical Society, no. 353, Providence, Rhode Island, 1986. 
[192] Ostaszewski, K., The space of Henstock integrable functions of two vari-
ables, International Journal of Mathematics and Mathematical Sciences 
11 (1988), 15-22. 
[193] Pfeffer, W., The Riemann Approach to Integration, Cambridge University 
Press, London, 1993. 
[194] Pfeffer, W., The Lebesgue and Denjoy-Perron integrals from a descriptive 
point of view, Ricerche di Matematica XLVIII (1999), 211-223. 
[195] Pfeffer, W., Derivation and Integration, Cambridge University Press, 
Cambridge, 2001. 
[196] Popov, V.N., Functional Integrals in Quantum Field Theory and Statist-
ical Physics, Reidel, Dordrecht, 1983. 
[197] Rao, M.M., Measure Theory and Integration, Marcel Dekker, New York, 
2004. 
[198] Ross, S., An Introduction to Mathematical Finance, Cambridge University 
Press, Cambridge, 1999. 
[199] Rota, G.-C, Indiscrete Thoughts, Birkhäuser, Boston, 1997. 
[200] Royden, H.L., Real Analysis, Macmillan, New York, 1968. 

518 
BIBLIOGRAPHY 
[201] Rudin, W., Real and Complex Analysis, McGraw-Hill, New York, 1974. 
[202] Saks, S., 
Theory of the Integral, Warszawa-Lwow; 
translated by 
L.C. Young, Stechert, New York, 1937. 
[203] Sarkhel, D.N., 
A criterion for Perron integrability, Proceedings of the 
American Mathematical Society 7 (1978), 109-112. 
[204] Sarkhel, D.N., The Riemann Integral, Calcutta Mathematical Society, 
Kolkata, 2006. 
[205] Schechter, E., Handbook of Analysis and Its Foundations, Academic Press, 
San Diego, 1997. 
[206] Schulman, L.S., Techniques and Applications of Path Integration, John 
Wiley & Sons, New York, 1981. 
[207] Schwabik, S., Generalized Ordinary Differential Equations, World Scient-
ific, Singapore, 1992. 
[208] Simon, B., Functional Integration and Quantum Physics, Academic Press, 
New York, 1981. 
[209] Sinai, Y.G., Probability Theory: An Introductory Course, Springer, Berlin, 
1992. 
[210] Skvortsov, V.A., Henstock integral in harmonic analysis, Scientiae Math-
ematicae Japonicae, 67(1) (2008), 71-82. 
[211] Skvortsov, V.A., and Solodov, A.P., A variational integral for Banach-
valued functions, Real Analysis Exchange 24(2) (1998-99), 799-805. 
[212] Skvortsov, V.A., and Solodov, A.P., A descriptive characterization of 
the Denjoy-Bochner integral and its generalizations, Moscow University 
Mathematics Bulletin 57 (2002), 36-39. 
[213] Skworcow, W. (V.A. Skvortsov), and Sworowski, P., Calki uogolnione, 
Wydawnictwo Uniwersytetu Kazimierza Wielkiego, Bydgoszcz, 2010. 
[214] Solodov, A.P., Riemann-type definition for the restricted 
Denjoy-Bochner 
integral, Fundamentalnaya i Prikladnaya Matematika 7 (2001), 887-895. 
[215] Swartz, W., Introduction to Gauge Integrals, World Scientific, Singapore, 
2001. 
[216] Talvila, E., 
Limits and Henstock integrals of products, Real Analysis 
Exchange 25 (1999-2000), 907-918. 
[217] Talvila, E., Necessary and sufficient conditions for differentiating under 
the integral sign, American Mathematical Monthly 108 (2001), 544-548. 

BIBLIOGRAPHY 
519 
Talvila, E., Henstock-Kurzweil 
Fourier transforms, Illinois Journal of 
Mathematics 46(4) (2002), 1207-1226. 
Talvila, E., Estimates of the remainder in Taylor's theorem using the 
Henstock-Kurzweil 
integral, Czechoslovak Mathematical Journal 55(4) 
(2005), 933-940. 
Talvila, E., The distributional Denjoy integral, Real Analysis Exchange 
33(1) (2007-08), 51-82. 
Thompson, H.B., Taylor's theorem using the generalized Riemann integral, 
American Mathematical Monthly 96 (1989), 346-350. 
Thompson, H.B., Taylor's theorem with the integral remainder under very 
weak differentiable assumptions, Australian Mathematical Society Gazette 
12 (1985), 1-6. 
Thomson, B.S., Outer measures and total variation, Canadian Mathemat-
ical Bulletin 24 (1981), 341-345. 
Thomson Reuters Datastream, 
http://online.thomsonreuters.com/datastream/ (accessed 1 March 2011). 
Varadhan, S.R.S., Probability Theory, American Mathematical Society, 
Providence, 2001. 
Vyborny, R., Elementary evaluation of FresneVs integrals, Mathematica 
Bohemica 116(4) (1991), 401-404. 
Vyborny, R., Some applications of Kurzweil-Henstock integration, Math-
ematica Bohemica 118(4) (1993), 425-441. 
Vyborny, R., Kurzweil-Henstock absolute integrable means McShane int-
egrable, Real Analysis Exchange 20(1) (1994-95), 1-4. 
Vyborny, R., A remark on the definition of the Kurzweil-Henstock 
int-
egral, Real Analysis Exchange 31(2) (2005-06), 465-468. 
Ward, A.J., The Perron-Stieltjes 
integral, Mathematische Zeitschrift 41 
(1936), 578-604. 
Wiener, N., Differential space, Journal of Mathematics and Physics 2 
(1923), 131-174. 
Wiener, N., The average value of a functional, Proceedings of the London 
Mathematical Society 23 (1924), 452-467. 
Wiener, N., Generalised harmonic analysis, Acta Mathematica 55 (1930), 
117-258. 
Wong, E., Stochastic Processes in Information and Dynamical Systems, 
Mcgraw-Hill, New York, 1971. 

520 
BIBLIOGRAPHY 
[235] Xu J., Lee P.Y., Stochastic integrals of Itö and Henstock, Real Analysis 
Exchange 18(2) (1992-93), 352-366. 
[236] Xu J., Lee P.Y., The stochastic integral of Henstock, South East Asia 
Bulletin of Mathematics 26(2) (1996), 101-106. 
[237] Yeh, J., Stochastic Processes and the Wiener Integral, Dekker, New York, 
1973. 
[238] Young, L.C., An inequality of Holder type connected with Stieltjes integrat-
ion, Acta Mathematica 67 (1936), 251-282. 
[239] Young, W.H., On non-uniform convergence and term-by-term 
integration 
of series, Proceedings of the London Mathematical Society 2(1) (1904), 
89-102. 
[240] Young, W.H., Open sets and the theory of content, Proceedings of the 
London Mathematical Society 2(2) (1904), 16-51. 
[241] Young, W.H., On upper and lower integration, Proceedings of the London 
Mathematical Society 2(2) (1904), 52-66. 
[242] Young, W.H., On the general theory of integration, Philosophical Trans-
actions of the Royal Society of London 204 (1905), 221-252. 
[243] Young, W.H., and Young, G.C., On the reduction of sets of intervals, 
Proceedings of the London Mathematical Society, 14(2) (1915), 111-130. 
[244] Zygmund, A., Trigonometric Series, Cambridge University Press, Cam-
bridge, 1959. 

Index 
absolute random variable, 185 
accuracy potentiality, 183 
action 
mechanical, 353 
additive 
sub-additive, 128 
almost certain, 52 
almost everywhere 
F-almost everywhere, 136 
/i-almost everywhere, 136 
almost sure, 52 
almost surely 
F-almost surely, 136, 188 
anti-derivative, 52 
arbitrage, 437 
risk-neutral pricing, 445 
arbitrage pricing, 445 
asset 
underlying, 434 
asset value 
derivative, 436 
growth rate, 440 
underlying , 436 
volatility, 441 
associated triple, 98, 103, 116 
association, 111, 136 
associated triple, 101 
in R T, 116-119 
point-cell, 42, 79, 97 
point-cell-dimension set, 98 
atomic distribution function, 409 
basic observable, 184 
binary partition, 95, 96, 342, 400 
binary Riemann sums 
weak convergence, 408 
black swan, 482 
Black-Scholes equation, 444 
Black-Scholes formula, 433, 442 
European call option, 435 
BM-continuity, 360-366 
definition, 365 
Borel sets, 37 
Borel-Cantelli, 230, 232 
first, 253 
second, 253 
Brownian motion, 50, 381 
c-Brownian motion, 305-307 
continuous modification, 316 
discretization, 327 
drift rate, 307 
drifting, 382, 397 
geometric, 308-314 
increments, 50 
infinite variation, 51 
properties, 305 
random walk, 327 
standard, 307 
step function, 327 
transitions, 50 
unbounded variation, 393 
variance rate, 307 
with drift, 307 
Burkill integral, 6, 7, 11 
Burkill-complete integral, 3, 14 
calculus integral, 8 
call option 
European, 435 
Cameron-Martin-Girsanov, 445 
Cauchy extension, 73 
Cauchy inequality, 211 
Cavalieri, 359 
cell, 39, 111 
A Modern Theory of Random 
Variation: With Applications 
in Stochastic Calculus, 
521 
Financial Mathematics, 
and Feynman Integration. 
First Edition. By Pat Muldowney 
Copyright © 2012 John Wiley & Sons, Inc. Published by John Wiley & Sons, Inc. 

522 
INDEX 
cell function, 128 
sub-additive, 128 
central limit theorem, 225, 233-244 
proof, 236 
characteristic function, 237 
Chebyshev inequality, 212 
claim, 434, 435 
compound interest, 433 
compounding, 435 
continuous, 434 
conditional potentiality, 254 
conditioning, 254 
conformance, 57, 76, 118, 206 
consistency, 184, 213 
contingent form, 20 
contingent observable, 184 
elementary form, 409 
continuity 
(a, #)-continuous, 290 
continuous 
BM-continuous, 360-366 
continuous modification, 316, 334 
contract, 434 
convergence 
/i-weak, 149 
absolute, 1 
conditional, 1, 75 
criteria, 174-178 
non-absolute, 1, 74, 75 
weak, 149 
correlation, 218 
countable additivity, 41 
Cousin's Theorem, 45 
covariance, 30 
cumulative distribution function, 240 
cylinder function, 99, 279, 329 
integrability, 280 
cylindrical interval, 86, 101 
Dark Side, 207 
Darth Vader Rule, 204 
datum, 17, 183 
contingent, 386 
joint, 184 
definite integral, 8, 14, 39, 134 
degenerate random variable, 403 
density, 321 
probability density function, 321 
density function, 46, 132, 279, 286 
lognormal, 309 
derivative 
financial, 434 
derivative asset, 434, 435 
derivative asset valuation, 433-446 
diffusion equation, 345-352, 377 
dimension set, 86 
Dirichlet function, 40 
integrability, 51 
discounted value of money, 433 
discounting, 435 
distribution 
binomial, 190 
lognormal, 191 
marginal, 62 
normal, 191 
Poisson, 191 
distribution function, 3, 50, 128, 183 
atomic, 57, 145, 409 
complex-valued, 5, 305 
consistent, 214 
continuity, 145 
cumulative, 240 
density, 341 
empirical, 486 
finite additivity, 50 
Presnel, 270 
Gaussian, 309 
integrability, 49, 108 
lognormal, 311 
marginal, 60, 214 
marginal density, 320 
normal, 265, 309 
potentiality, 136 
divisibility, 45 
division, 42, 88 
5-fine, 42 
cell, 114 
dimension set, 114 
of R T, 116-119 
point, 114 
unbounded domain, 69 
division axiom, 112 

INDEX 
523 
division point, 44, 49, 55, 97, 386 
division system, 111, 126 
dominated convergence, 49 
dominated convergence theorem, 173 
drift rate 
Brownian motion, 443 
drifting Brownian motion, 397 
drift rate, 307 
variance rate, 307 
elementary form, 20 
elementary observable, 183 
elementary set, 105 
equivalence 
F-variational, 384 
/i-equivalent, 149 
/i-variational, 384 
variational, 384 
European call option, 433, 435 
pricing, 440-442 
evaluation point, 49, 97, 388, 427 
events 
independent, 27 
joint, 35 
random, 187 
expectation, 34, 36, 38, 40, 185 
additivity, 59 
conditional, 254 
marginal density, 321, 340, 344 
expectation density, 325 
expectation functional, 198 
expected value, 34, 36, 38, 40, 185 
experiment, 17 
Feynman diagrams, 366-382 
Feynman integration, 353-366 
Feynman path integral, 5, 78 
figure, 39, 49, 105, 111 
fine 
5-fine, 42 
ί-fine division, 80 
7-fine, 103, 104 
7-fine division, 114 
7-fine pair, 112 
7-fine triple, 114 
Fokker-Planck equation, 345-352 
Fourier transform, 240 
Fresnel integral, 73, 257-276, 356 
continuity in R T, 288 
evaluation, 261 
incremental, 284 
infinite-dimensional domain, 274 
integrability on R T, 286 
Stieltjes version, 264 
transitional, 284 
variationally equivalent forms, 286 
Fresnel integrand 
VBG*, 282 
Fubini's theorem, 62, 154-165 
fundamental theorem of calculus, 61 
gauge, 51, 80, 83, 101, 103 
7-di visibility, 121 
7-fine, 112 
axiom, 112 
existence of 7-fine division, 121 
in R T, 103, 116-119 
Gaussian distribution function, 309 
generalized bounded variation, 151 
geometric Brownian motion, 314, 440 
growth rate, 314 
volatility, 314 
with impulsive action, 445 
growth rate of asset value, 443 
Holder inequality, 207 
Helly's theorem, 240 
Henstock integral, 46, 49, 111-182 
division elements, 120 
division system, 120 
extension, 125 
integration elements, 120 
Henstock lemma, 67-69, 132 
Henstock, Ralph, 501 
indefinite integral, 8, 13, 14, 130, 134 
independence, 219 
independent occurrences, 27 
indicator function, 43, 51 
integrability 
absolute, 53, 134 
additive cell functions, 128 

524 
distribution functions, 128 
Henstock-Kurzweil, 55 
in a set, 138 
Lebesgue, 55 
McShane, 55 
non-absolute, 54 
on a figure, 138 
Riemann, 55 
Riemann-complete, 55 
sub-additive cell functions, 128 
integral 
Burkill, 3, 7, 72, 127, 386 
Burkill-complete, 3, 14, 36, 386 
calculus, 8 
Cauchy, 113, 127 
definite , 8 
Denjoy, 2, 41, 72, 113, 116, 127 
extended Riemann, 54, 74, 257 
gauge, 41, 42 
generalized Riemann, 41, 42 
Henstock, 2, 36, 42, 83, 111, 491 
Henstock-Kurzweil, 2, 41, 42, 47, 
115, 116 
improper, 257 
in, 112 
indefinite, 13, 130 
indefinite , 8 
infinite-dimensional, 83 
Itö, 408 
iterated, 159 
joint-observable, 419 
Kurzweil, 2, 41, 72, 83, 113, 127 
Lebesgue, 2, 38, 46, 47, 53, 72, 83, 
113, 115, 116, 127 
Lebesgue-Stieltjes, 24, 38 
McShane, 2, 55, 72, 113, 116, 127 
Newton, 8 
observable, 332, 396, 397, 419 
on, 112 
Perron, 2, 41, 72, 113, 116, 127 
Riemann, 2, 7, 39, 41, 47, 72, 83, 
113, 116, 127 
Riemann-Stieltjes, 17, 30, 38, 39 
Riemann-complete, 2, 14, 41, 42, 
45, 47, 49, 83, 115 
Stieltjes, 2, 3, 7, 17, 38, 39, 72 
Stieltjes-complete, 3, 14, 24, 36 
stochastic, 97, 106, 332, 387 
strong, 392 
strong stochastic, 386-398 
Ward, 127 
weak, 389, 392 
weak stochastic, 391, 398-424 
integrand 
cylindrical, 329 
integration 
in, 58 
on, 58 
integration basis, 111 
integration by change of integrator, 149 
integration by change of variable, 149 
integration by parts, 204, 263 
integration by substitution, 149 
integrator, 3, 43, 52, 56, 61 
Stieltjes, 11 
interaction energy, 353 
interest rate 
riskless, 437, 440 
inverse Fourier transform, 240 
isometry, 416 
Ito's formula 
numerical calculation, 467-471 
Ito integral, 51, 408 
Ito's formula, 383, 385, 424-433 
application, 429-433 
change of integrator, 425 
change of variable, 427 
drifting Brownian motion, 431 
integration by substitution, 425 
proof, 426-429 
Ito's lemma, 383, 424-433 
iterated integrals, 159 
iterated Riemanns sums, 159 
joint datum, 184 
joint occurrences, 35 
joint variability, 26-35 
joint-basic observable, 184 
joint-contingent observable, 184 
joint-observable integral, 419 
kinetic energy, 353 

INDEX 
Kolmogorov, 38, 41 
Kolmogorov forward equation, 345-352 
Kolmogorov inequality, 227 
Kramer convergence theorem, 242 
Kurzweil, Jaroslav, 501 
Lagrangian, 353 
law of large numbers, 224-233 
first Kolmogorov, 229 
second Kolmogorov, 230 
strong, 226, 229, 230 
weak, 225, 226 
Levy convergence theorem, 242 
Levy-Kramer theorem, 242 
likelihood, 4, 25, 183 
uniform, 64 
limits of integrals, 165-178 
boundedness condition, 168 
dominated convergence, 173 
monotone convergence, 169 
uniform convergence, 168 
lognormal asset value, 443 
lognormal density function, 309 
lognormal distribution function, 311 
mean, 312 
variance, 312 
Maple, 424, 447 
marginal density 
of distribution function, 320 
of expectation, 320 
marginal density function, 321 
marginal density of expectation, 321, 
340, 344, 357 
marginal distribution density, 323 
marginal expectation density, 340, 345, 
360, 372 
marginal potentiality, 60 
Markov chain, 185 
Markov inequality, 210 
martingale, 185, 436 
F-martingale, 437 
degenerate, 436, 437, 440 
McShane integral, 55 
McShane, Edward James, 501 
mean value, 16 
525 
measurability, 63, 186-197, 491-500 
F-measurable, 187 
Fx-measurability, 188 
/i-measurable, 187 
measurable function, 192-197 
measurable sets, 46 
mechanical action, 353 
Minkowski inequality, 210 
monotone convergence, 49 
monotone convergence theorem, 169 
Newton integral, 8 
no-arbitrage, 437 
no-arbitrage pricing, 441 
non-absolute integrals 
limit theorems, 174-178 
normal distribution function, 309 
normalization factor, 356, 357 
null, 52 
ft-null, 136 
null set, 143, 146 
observable, 4, 37, 183 
basic, 40, 62, 184, 213, 383 
contingent, 40, 62, 184, 383 
elementary, 183, 383 
independent, 27 
joint, 36 
joint-basic, 19, 26, 62, 184, 213 
joint-contingent, 36, 62, 184 
representation, 197-203, 214 
observable integral, 332, 396, 397 
observables 
consistency, 62 
independence, 35 
observation, 17 
occurrences, 17 
independent, 27 
joint, 35 
option, 435 
European, 435 
option pricing, 433-446 
oscillatory, 53, 54 
outcomes, 17 
independent, 27 
joint, 35 

526 
INDEX 
outer measure, 63-67, 136, 137 
Lebesgue, 64 
partially regularized partition, 280 
particle exchange, 381 
partition, 17, 39, 88 
binary, 96, 342, 400 
partial regularization, 92, 280 
refinement, 107 
regular, 92, 100, 104, 326, 342 
regular sub-partition, 104, 107 
regularization, 92 
regularized, 280, 326 
unbounded domain, 69 
unbounded sample space, 69 
partitions 
binary, 95 
simple regular, 95 
path integrals, 36 
Feynman, 353-366 
perturbation, 354 
perturbation expansion, 367, 373 
perturbation function, 381 
perturbation method, 367 
perturbation series, 367, 373 
photon 
absorption, 381 
emission, 381 
portfolio, 437 
riskless, 440 
value, 437 
potential energy, 353 
potentiality, 4, 183 
accuracy, 183 
conditional, 254 
joint, 26 
marginal, 27, 32, 35 
uniform, 64 
potentiality distribution function, 19, 
25, 37, 50, 128, 183 
potentiality function, 37 
price growth rate, 445 
price volatility, 445 
pricing 
risk-neutral, 438 
pricing theory, 433-446 
primitive function, 52 
probability, 25-26 
calculus, 244-255 
complex-valued, 78 
conditional, 254 
measure, 5 
negative, 78-79 
theory, 244-255 
probability amplitude, 381 
probability distribution function, 37 
probability function, 37 
probability measure, 19 
probability space, 5, 23-24 
process, 19, 35-36 
Prokhorov's theorem, 242 
quantum mechanics, 38, 41, 73, 78, 108, 
353, 367, 371, 381 
Radon-Nikodym theorem, 445 
random variability, 3, 18 
independent, 27, 32, 63 
joint, 31, 60, 63, 212-217 
random variable, 4, 40, 185, 216 
random variables 
absolute, 185, 193 
contingent, 17, 22 
degenerate, 403 
elementary, 17, 22 
expectation, 216 
independence, 35 
independent, 27, 217-224 
joint, 36 
joint-contingent, 60 
measurability, 192 
random variation, 3 
joint-basic, 18 
random walk, 327, 450 
Brownian motion, 327 
refinement partition, 107 
regular partition, 326, 342, 355 
regularized partition, 280 
Riemann integral, 7 
Riemann sums, 7, 30, 39-41 
infinite-dimensional, 97 
iterated, 161 

INDEX 
527 
Riemann-complete integral, 2, 14, 42, 
45,49 
risk-free interest rate, 434 
risk-neutral 
arbitrage pricing, 445 
risk-neutral pricing, 436-440 
risk-neutrality, 441, 445 
riskless interest rate, 440 
riskless portfolio, 440 
riskless profit, 437 
sample space, 19, 37, 50, 183 
unbounded, 69-73 
Schrödinger's equation, 345-352, 377 
share price, 434 
simple functions, 186, 197 
simple regular partitions, 95 
standard deviation, 16, 191 
state function, 344, 354 
step function, 55, 197, 335, 375, 377 
Brownian motion, 327 
Stieltjes cell function, 11, 50 
Stieltjes integral, 6, 7, 17, 38, 39, 264 
Stieltjes integrator, 11, 12 
Stieltjes-complete integral, 3, 14 
stochastic integral, 49, 51, 332, 387 
evaluation, 412-424 
existence conditions, 429 
numerical calculation, 453-466 
weak convergence, 430 
stochastic integrand, 387 
stochastic integration, 38, 383-433 
stochastic process, 19, 35-36 
strong stochastic integral, 386-398 
sub-partition 
regular, 98 
support 
of a function, 143 
survival function 
Darth Vader Rule, 203 
tag-point, 114 
Taylor's theorem, 426 
thought-experiment, 438 
Tonelli, 100, 154, 155 
unpredictability, 19 
variance, 17 
variance rate 
asset value, 443 
variation, 3, 63-67, 126, 134-154 
h- variation, 136 
additivity, 138, 139 
and integral, 143 
bounded, 147 
countable sub-additivity, 142 
equals integral of absolute value, 
147 
extension of, 137 
finite additivity, 140 
generalized bounded, 151 
in, 137 
in a set, 135 
infinite, 150 
null, 146 
on, 137 
on a figure, 135 
outer measure, 134, 138, 146 
sub-additivity, 138 
unbounded, 150 
variational equivalence, 148 
variational measure, 64 
zero, 146 
variational equivalence, 148, 149 
F-variational equivalence, 384 
G-variational equivalence, 384 
h-variational equivalence, 384 
VBG*, 151, 165 
Fresnel integrand, 282 
volatility, 441 
asset value, 443 
wave function, 344 
weak integral, 389 
weak stochastic integral, 398-424 
definition, 407 
underlying asset value, 436 


C H A P T E R  1
Introduction
1.1 Logic
Logic is one of the oldest intellectual disciplines in human history. It dates back to Aristotle; it has
been studied through the centuries; and it is still a subject of active investigation today.
We use Logic is just about everything we do. We use it in our personal lives. We use it in our
professional activities. We use the language of Logic to state observations, to define concepts, and
to formalize theories. We use logical reasoning to derive conclusions from these bits of
information. We use logical proofs to convince others of these conclusions.
And we are not alone! Logic is increasingly being used by computers - to prove mathematical
theorems, to validate engineering designs, to diagnose failures, to encode and analyze laws and
regulations and business rules.
This chapter is an overview of Logic as presented in this book. We start with an introduction to the
key concepts of Logic using sentences written in English. We then talk about the value of using a
formal language for expressing logical information, and we talk about the formal rules for
manipulating sentences expressed in this language. After this, we discuss how formalization of
this sort enables automation of logical reasoning, and we summarize the current state of logic
technology and its applications. Finally, we conclude with a reading guide for the remainder of the
book.
1.2 Elements of Logic
Consider the interpersonal relations of a small sorority. There are just four members - Abby, Bess,
Cody, and Dana. Some of the girls like each other, but some do not. Figure 1 shows one set of
possibilities.
 
Figure 1 - One state of Sorority World.

Let's assume that we do not know this information ourselves, but we have informants who know
the girls and are willing to share what they know. Each informant knows a little about the likes
and dislikes of the girls, but no one knows everything. Here is where Logic comes in. By writing
logical sentences, each informant can express exactly what he or she knows - no more, no less. For
our part, we can combine these sentences into a logical theory; and we can use this theory to draw
logical conclusions, including some that may not be known to any one of the informants.
Figure 2 shows one such theory. The first sentence is straightforward; it tells us directly that Dana
likes Cody. The second and third sentences tell us what is not true,without saying what is true. The
fourth sentence says that one condition holds or another but does not say which. The fifth sentence
gives a general fact about the girls Abby likes. The sixth sentence expresses a general fact about
Cody's likes. The last sentence says something about everyone.
Dana likes Cody.
Abby does not like Dana.
Dana does not like Abby.
Bess likes Cody or Dana.
Abby likes everyone that Bess likes.
Cody likes everyone who likes her.
Nobody likes herself.
Figure 2 - Logical sentences describing Sorority World.
Sentences like these constrain the possible ways the world could be. Each sentence divides the set
of possible worlds into two subsets, those in which the sentence is true and those in which the
sentence is false. Believing a sentence is tantamount to believing that the world is in the first set.
Given two sentences, we know the world must be in the intersection of the set of worlds in which
the first sentence is true and the set of worlds in which the second sentence is true. Ideally, when
we have enough sentences, we know exactly how things stand.
Unfortunately, this is not always the case. Sometimes, a collection of sentences only partially
constrains the world. For example, there are four different worlds that are consistent with the our
Sorority World sentences, viz. the ones shown in figure 3.
Figure 3 - Four states of Sorority World.
Even though a set of sentences does not determine a unique world, it is often the case that some
sentences are true in every world that satisfies the given sentences. A sentence of this sort is said
to be a logical conclusion from the given sentences. Said the other way around, a set of premises
logically entails a conclusion if and only if every world that satisfies the premises also satisfies the
conclusion.

What can we conclude from the bits of information in Figure 2? Quite a bit, as it turns out. For
example, it must be the case that Bess likes Cody. Also, Bess does not like Dana. There are also
some general conclusions that must be true. For example, in this world with just four girls, we can
conclude that everybody likes somebody. Also, everyone is liked by somebody.
One way to make this determination is by checking all possible worlds. For example, in our case,
we notice that, in every world that satisfies our sentences, Bess likes Cody, so the statement that
Bess likes Cody is a logical conclusion from our set of sentences.
Unfortunately, determining logical entailment by checking all possible worlds is impractical in
general. There are usually many, many possible worlds; and in some cases there can be infinitely
many.
The alternative is logical reasoning, viz. the application of reasoning rules to derive logical
conclusions and produce a logical proofs, i.e. sequences of reasoning steps that leads from
premises to conclusions.
As an example, consider the following informal proof that starts with the premises shown above
and proves that Bess likes Cody.
We know that Abby likes everyone that Bess likes, and we know that Abby does not
like Dana. Therefore, Bess must not like Dana either. (If Bess did like Dana, then
Abby would like her as well.) At the same time, we know that Bess likes Cody or
Dana. Consequently, since Bess does not like Dana, she must like Cody.
The concept of proof, in order to be meaningful, requires that we be able to recognize certain
reasoning steps as immediately obvious. In other words, we need to be familiar with the reasoning
"atoms" out of which complex proof "molecules" are built.
One of Aristotle's great contributions to philosophy was his recognition that what makes a step of
a proof immediately obvious is its form rather than its content. It does not matter whether you are
talking about blocks or stocks or sorority girls. What matters is the structure of the facts with
which you are working. Such patterns are called rules of inference.
As an example, consider the reasoning step shown below. We know that all Accords are Hondas,
and we know that all Hondas are Japanese cars. Consequently, we can conclude that all Accords
are Japanese cars.
All Accords are Hondas.
All Hondas are Japanese.
Therefore, all Accords are Japanese.
Now consider another example. We know that all borogoves are slithy toves, and we know that all
slithy toves are mimsy. Consequently, we can conclude that all borogoves are mimsy. What's
more, in order to reach this conclusion, we do not need to know anything about borogoves or
slithy toves or what it means to be mimsy.
All borogoves are slithy toves.
All slithy toves are mimsy.
Therefore, all borogoves are mimsy.

What is interesting about these examples is that they share the same reasoning structure, viz. the
pattern shown below.
All x are y.
All y are z.
Therefore, all x are z.
The existence of such reasoning patterns is fundamental in Logic but raises important questions.
Which patterns are correct? Are there many such patterns or just a few?
Let us consider the first of these questions. Obviously, there are patterns that are just plain wrong
in the sense that they can lead to incorrect conclusions. Consider, as an example, the (faulty)
reasoning pattern shown below.
All x are y.
Some y are z.
Therefore, some x are z.
Now let us take a look at an instance of this pattern. If we replace x by Toyotas and y by cars and z
by made in America, we get the following line of argument, leading to a conclusion that happens
to be correct.
All Toyotas are cars.
Some cars are made in America.
Therefore, some Toyotas are made in America.
On the other hand, if we replace x by Toyotas and y by cars and z by Porsches, we get a line of
argument leading to a conclusion that is questionable.
All Toyotas are cars.
Some cars are Porsches.
Therefore, some Toyotas are Porsches.
What distinguishes a correct pattern from one that is incorrect is that it must always lead to correct
conclusions, i.e. conclusions that are logically entailed by the premises. As we will see, this is the
defining criterion for what we call deduction.
Now, it is noteworthy that there are patterns of reasoning that are sometimes useful but do not
satisfy this strict criterion. There is inductive reasoning, abductive reasoning, reasoning by
analogy, and so forth.
Induction is reasoning from the particular to the general. The example shown below illustrated
this. If we see enough cases in which something is true and we never see a case in which it is false,
we tend to conclude that it is always true.
I have seen 1000 black ravens.
I have never seen a raven that is not black.
Therefore, every raven is black.
Now try red Hondas.

Abduction is reasoning from effects to possible causes. Many things can cause an observed result.
We often tend to infer a cause even when our enumeration of possible causes is incomplete.
If there is no fuel, the car will not start.
If there is no spark, the car will not start.
There is spark.
The car will not start.
Therefore, there is no fuel.
What if the car is in a vacuum chamber?
Reasoning by analogy is reasoning in which we infer a conclusion based on similarity of two
situations, as in the following example.
The flow in a pipe is proportional to its diameter.
Wires are like pipes.
Therefore, the current in a wire is proportional to diameter.
Now try price.
Of all types of reasoning, deductive reasoning is the only one that guarantees its conclusions in all
cases. It has some very special properties and holds a unique place in Logic. In this book, we
concentrate entirely on deduction and leave these other forms of reasoning to others.
1.3 Formalization
So far, we have illustrated everything with sentences in English. While natural language works
well in many circumstances, it is not without its problems. Natural language sentences can be
complex; they can be ambiguous; and failing to understand the meaning of a sentence can lead to
errors in reasoning.
Even very simple sentences can be troublesome. Here we see two grammatically legal sentences.
They are the same in all but the last word, but there structure is entirely different. In the first, the
main verb is blossoms, while in the second blossoms is a noun and the main verb is sank.
The cherry blossoms in the Spring.
The cherry blossoms in the Spring sank.
As another example of grammatical complexity, consider the following excerpt taken from the
University of Michigan lease agreement. The sentence in this case is sufficiently long and the
grammatical structure sufficiently complex that people must often read it several times to
understand precisely what it says.
The University may terminate this lease when the Lessee, having made application
and executed this lease in advance of enrollment, is not eligible to enroll or fails to
enroll in the University or leaves the University at any time prior to the expiration of
this lease, or for violation of any provisions of this lease, or for violation of any
University regulation relative to resident Halls, or for health reasons, by providing
the student with written notice of this termination 30 days prior to the effective data of
termination, unless life, limb, or property would be jeopardized, the Lessee engages in
the sales of purchase of controlled substances in violation of federal, state or local
law, or the Lessee is no longer enrolled as a student, or the Lessee engages in the use

or possession of firearms, explosives, inflammable liquids, fireworks, or other
dangerous weapons within the building, or turns in a false alarm, in which cases a
maximum of 24 hours notice would be sufficient.
As an example of ambiguity, suppose I were to write the sentence There's a girl in the room with a
telescope. See Figure 4 for two possible meanings of this sentence. Am I saying that there is a girl
in a room containing a telescope? Or am I saying that there is a girl in the room and she is holding
a telescope?
Figure 4 - There's a girl in the room with a telescope.
Such complexities and ambiguities can sometimes be humorous if they lead to interpretations the
author did not intend. See the examples below for some infamous newspaper headlines with
multiple interpretations. Using a formal language eliminates such unintentional ambiguities (and,
for better or worse, avoids any unintentional humor as well).
Crowds Rushing to See Pope Trample 6 to Death
Journal Star, Peoria, 1980
Scientists Grow Frog Eyes and Ears
British Left Waffles On Falkland Islands
The Daily Camera, Boulder, 2000
Food Stamp Recipients Turn to Plastic
Indian Ocean Talks
The Miami Herald, 1991
The Plain Dealer, 1977
Fried Chicken Cooked in Microwave Wins Trip
The Oregonian, Portland, 1981
Figure 5 - Various newspaper headlines.
As an illustration of errors that arise in reasoning with sentences in natural language, consider the
following examples. In the first, we use the transitivity of the better relation to derive a conclusion
about the relative quality of champagne and soda from the relative quality of champagne and beer
and the relative quality or beer and soda. So far so good.
Champagne is better than beer.
Beer is better than soda.
Therefore, champagne is better than soda.
Now, consider what happens when we apply the same transitivity rule in the case illustrated

below. The form of the argument is the same as before, but the conclusion is somewhat less
believable. The problem in this case is that the use of nothing here is syntactically similar to the
use of beer in the preceding example, but in English it means something entirely different.
Bad sex is better than nothing.
Nothing is better than good sex.
Therefore, bad sex is better than good sex.
Logic eliminates these difficulties through the use of a formal language for encoding information.
Given the syntax and semantics of this formal language, we can give a precise definition for the
notion of logical entailment. Moreover, we can establish precise reasoning rules that produce all
and only logically entailed conclusions.
In this regard, there is a strong analogy between the methods of Formal Logic and those of high
school algebra. To illustrate this analogy, consider the following algebra problem.
Xavier is three times as old as Yolanda. Xavier's age and Yolanda's age add up to
twelve. How old are Xavier and Yolanda?
Typically, the first step in solving such a problem is to express the information in the form of
equations. If we let x represent the age of Xavier and y represent the age of Yolanda, we can
capture the essential information of the problem as shown below.
x - 3y = 0
x + y = 12
Using the methods of algebra, we can then manipulate these expressions to solve the problem.
First we subtract the second equation from the first.
x - 3y = 0
x + y = 12
-4y = -12
Next, we divide each side of the resulting equation by -4 to get a value for y. Then substituting
back into one of the preceding equations, we get a value for x.
x = 9
y = 3
Now, consider the following logic problem.
If Mary loves Pat, then Mary loves Quincy. If it is Monday and raining, then Mary
loves Pat or Quincy. If it is Monday and raining, does Mary love Quincy?
As with the algebra problem, the first step is formalization. Let p represent the possibility that
Mary loves Pat; let q represent the possibility that Mary loves Quincy; let m represent the
possibility that it is Monday; and let r represent the possibility that it is raining.
With these abbreviations, we can represent the essential information of this problem with the

following logical sentences. The first says that p implies q, i.e. if Mary loves Pat, then Mary loves
Quincy. The second says that m and r implies p or q, i.e. if it is Monday and raining, then Mary
loves Pat or Mary loves Quincy.
p ⇒q
m ∧ r ⇒p ∨ q
As with Algebra, Formal Logic defines certain operations that we can use to manipulate
expressions. The operation shown below is a variant of what is called propositional resolution.
The expressions above the line are the premises of the rule, and the expression below is the
conclusion.
p1 ∧ ... ∧ pk
⇒q1 ∨ ... ∨ ql
r1 ∧ ... ∧ rm
⇒s1 ∨ ... ∨ sn
p1 ∧ ... ∧ pk ∧ r1 ∧ ... ∧ rm ⇒q1 ∨ ... ∨ ql ∨ s1 ∨ ... ∨ sn
There are two elaborations of this operation. (1) If pi on the left hand side of one sentence is the
same as qj in the right hand side of the other sentence, it is okay to drop the two symbols, with the
proviso that only one such pair may be dropped. (2) If a constant is repeated on the same side of a
single sentence, all but one of the occurrences can be deleted.
We can use this operation to solve the problem of Mary's love life. Looking at the two premises
above, we notice that p occurs on the left-hand side of one sentence and the right-hand side of the
other. Consequently, we can cancel the p and thereby derive the conclusion that, if is Monday and
raining, then Mary loves Quincy or Mary loves Quincy.
p ⇒q
m ∧ r ⇒p ∨ q
m ∧ r ⇒q ∨ q
Dropping the repeated symbol on the right hand side, we arrive at the conclusion that, if it is
Monday and raining, then Mary loves Quincy.
m ∧ r ⇒q ∨ q
m ∧ r ⇒q
This example is interesting in that it showcases our formal language for encoding logical
information. As with algebra, we use symbols to represent relevant aspects of the world in
question, and we use operators to connect these symbols in order to express information about the
things those symbols represent.
The example also introduces one of the most important operations in Formal Logic, viz. resolution
(in this case a restricted form of resolution). Resolution has the property of being complete for an
important class of logic problems, i.e. it is the only operation necessary to solve any problem in the
class.
1.4 Automation

The existence of a formal language for representing information and the existence of a
corresponding set of mechanical manipulation rules together have an important consequence, viz.
the possibility of automated reasoning using digital computers.
The idea is simple. We use our formal representation to encode the premises of a problem as data
structures in a computer, and we program the computer to apply our mechanical rules in a
systematic way. The rules are applied until the desired conclusion is attained or until it is
determined that the desired conclusion cannot be attained. (Unfortunately, in some cases, this
determination cannot be made; and the procedure never halts. Nevertheless, as discussed in later
chapters, the idea is basically sound.)
Although the prospect of automated reasoning has achieved practical realization only in the last
few decades, it is interesting to note that the concept itself is not new. In fact, the idea of building
machines capable of logical reasoning has a long tradition.
One of the first individuals to give voice to this idea was Leibnitz. He conceived of "a universal
algebra by which all knowledge, including moral and metaphysical truths, can some day be
brought within a single deductive system". Having already perfected a mechanical calculator for
arithmetic, he argued that, with this universal algebra, it would be possible to build a machine
capable of rendering the consequences of such a system mechanically.
Boole gave substance to this dream in the 1800s with the invention of Boolean algebra and with
the creation of a machine capable of computing accordingly.
The early twentieth century brought additional advances in Logic, notably the invention of the
predicate calculus by Russell and Whitehead and the proof of the corresponding completeness and
incompleteness theorems by Godel in the 1930s.
The advent of the digital computer in the 1940s gave increased attention to the prospects for
automated reasoning. Research in artificial intelligence led to the development of efficient
algorithms for logical reasoning, highlighted by Robinson's invention of resolution theorem
proving in the 1960s.
Today, the prospect of automated reasoning has moved from the realm of possibility to that of
practicality, with the creation of logic technology in the form of automated reasoning systems,
such as Vampire, Prover9, the Prolog Technology Theorem Prover, Epilog, and others.
The emergence of this technology has led to the application of logic technology in a wide variety
of areas. The following paragraphs outline some of these uses.
Mathematics. Automated reasoning programs can be used to check proofs and, in some cases, to
produce proofs or portions of proofs.
Engineering. Engineers can use the language of Logic to write specifications for their products
and to encode their designs. Automated reasoning tools can be used to simulate designs and in
some cases validate that these designs meet their specification. Such tools can also be used to
diagnose failures and to develop testing programs.
Database Systems. By conceptualizing database tables as sets of simple sentences, it is possible to
use Logic in support of database systems. For example, the language of Logic can be used to
define virtual views of data in terms of explicitly stored tables, and it can be used to encode
constraints on databases. Automated reasoning techniques can be used to compute new tables, to

detect problems, and to optimize queries.
Data Integration The language of Logic can be used to relate the vocabulary and structure of
disparate data sources, and automated reasoning techniques can be used to integrate the data in
these sources.
Logical Spreadsheets. Logical spreadsheets generalize traditional spreadsheets to include logical
constraints as well as traditional arithmetic formulas. Examples of such constraints abound. For
example, in scheduling applications, we might have timing constraints or restrictions on who can
reserve which rooms. In the domain of travel reservations, we might have constraints on adults
and infants. In academic program sheets, we might have constraints on how many courses of
varying types that students must take.
Law and Business. The language of Logic can be used to encode regulations and business rules,
and automated reasoning techniques can be used to analyze such regulations for inconsistency and
overlap.
1.5 Reading Guide
Although Logic is a single field of study, there is more than one logic in this field. In the three
main units of this book, we look at three different types of logic, each more sophisticated than the
last.
Propositional Logic is the logic of propositions. Symbols in the language represent "conditions" in
the world, and complex sentences in the language express interrelationships among these
conditions. The primary operators are Boolean connectives, such as and, or, and not.
Herbrand Logic expands upon Propositional Logic by providing a means for explicitly talking
about individual objects and their interrelationships (not just monolithic conditions). In order to do
so, we expand our language to include object constants, function constants, relation constants,
variables, and quantifiers.
First Order Logic is a variant of Herbrand Logic that provides greater flexibility in encoding
information. First Order Logic supports synonyms and unnamed objects and allows one to define
abstract concepts, which apply across worlds of different size.
Each chapter brings new issues and capabilities to light. Despite these differences, there are many
commonalities among these logics. In particular, in each case, there is a language with a formal
syntax and a precise semantics; there is a notion of logical entailment; and there are legal rules for
manipulating expressions in the language.
These similarities allow us to compare the various logics and to gain an appreciation of the
tradeoff between expressiveness and computational complexity. On the one hand, the introduction
of additional linguistic complexity makes it possible to say things that cannot be said in more
restricted languages. On the other hand, the introduction of additional linguistic flexibility has
adverse effects on computability. As we proceed though the material, our attention will range from
the completely computable case of Propositional Logic to a variant of Logic that is not at all
computable.
One final comment. In the hopes of preventing difficulties, it is worth pointing out a potential
source of confusion. This book exists in the meta world. It contains sentences about sentences; it

contains proofs about proofs. In some places, we use similar mathematical symbology both for
sentences in Logic and sentences about Logic. Wherever possible, we will try to be clear about
this distinction, but the potential for confusion remains. Unfortunately, this comes with the
territory. We are using Logic to study Logic. It is our most powerful intellectual tool.
Recap
Logic is the study of information encoded in the form of logical sentences. Each logical sentence
divides the set of all possible world into two subsets - the set of worlds in which the sentence is
true and the set of worlds in which the set of sentences is false. A set of premises logically entails
a conclusion if and only if the conclusion is true in every world in which all of the premises are
true. Deduction is a form of symbolic reasoning that produces conclusions that are logically
entailed by premises (distinguishing it from other forms of reasoning, such as induction,
abduction, and analogical reasoning). A proof is a sequence of simple, more-or-less obvious
deductive steps that justifies a conclusion that may not be immediately obvious from given
premises. In Logic, we usually encode logical information as sentences in formal languages; and
we use rules of inference appropriate to these languages. Such formal representations and methods
are useful for us to use ourselves. Moreover, they allow us to automate the process of deduction,
though the computability of such implementations varies with the complexity of the sentences
involved.


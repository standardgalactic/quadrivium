


Essential Textbooks In Mathematics
ISSN: 2059-7657
Published:
A Sequential Introduction to Real Analysis
by J M Speight (University of Leeds, UK)
A Friendly Approach to Functional Analysis
by Amol Sasane (London School of Economics, UK)


Published by
World Scientific Publishing Europe Ltd.
57 Shelton Street, Covent Garden, London WC2H
9HE
Head office: 5 Toh Tuck Link, Singapore 596224
USA office: 27 Warren Street, Suite 401-402, Hackensack, NJ 07601
Library of Congress Cataloging-in-Publication Data Names: Sasane, A. (Amol), 1976– Title: A friendly
approach to functional analysis / by Amol Sasane 
(London School of Economics, UK).
Description: New Jersey : World Scientific, 2017. | Series: Essential textbooks in mathematics | 
Includes bibliographical references and index.
Identifiers: LCCN 2017000443 | ISBN 9781786343338 (hardcover : alk. paper) | 
ISBN 9781786343345 (softcover : alk. paper) Subjects: LCSH: Functional analysis--Textbooks.
Classification: LCC QA320 .S235 2017 | DDC 515/.7--dc23
LC record available at https://lccn.loc.gov/2017000443
British Library Cataloguing-in-Publication Data
A catalogue record for this book is available from the British Library.
Copyright © 2017 by World Scientific Publishing Europe Ltd.
All rights reserved. This book, or parts thereof, may not be reproduced in any form or by any means,
electronic or mechanical, including photocopying, recording or any information storage and retrieval
system now known or to be invented, without written permission from the Publisher.
For photocopying of material in this volume, please pay a copying fee through the Copyright Clearance
Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, USA. In this case permission to photocopy is not
required from the publisher.

Printed in Singapore

To Sara

Preface
What is Functional Analysis?
Functional Analysis is Calculus in the setting of (typically infinite dimensional)
vector spaces, just like
Why the adjective “functional”?
There is a historical reason behind this: the subject arose with considerations of
problems in vector spaces of functions: for example in C[a, b], the vector space
of continuous functions on the interval [a, b] ⊂ R, with pointwise operations.
What do we mean by Calculus?
It is the study of concepts involving “limiting processes”, such as convergence of
sequences, continuity of functions, differentiation, integration, etc.
Why study Functional Analysis?
Functional analysis plays an important role in the applied sciences as well as in
mathematics itself. The impetus came from applications: problems related to
ordinary and partial differential equations, calculus of variations, approximation
theory, numerical analysis, integral equations, and so on.
In ordinary calculus, one dealt with limiting processes in finite-dimensional
vector spaces (R or Rd), but problems arising in the above applications required
a calculus in spaces of functions (which are infinite-dimensional vector spaces).

For instance, we mention the following optimisation problem as a motivating
example.
Example 0.1. Imagine a copper mining company which is mining in a mountain,
that has an estimated amount of Q tonnes of copper, over a period of T years.
Suppose that x(t) denotes the total amount of copper removed up to time t ∈ [0,
T]. Since the operation is over a large time period, we may assume that this x is a
function living on the “continuous-time” interval [0, T]. The company has the
freedom to choose its mining operation: x can be any nondecreasing function on
[0, T] such that
x(0) = 0 (no copper removed initially) and
x(T) = Q (all copper removed at the end of the mining regime).
The cost of extracting copper per unit tonne at time t is
Here a, b are given positive constants. The expression is reasonable, since the
term a · x(t) accounts for the fact that when more and more copper is taken out,
it becomes more and more difficult to find the leftover copper, while the term b ·
x′(t) accounts for the fact that if the rate of removal of copper is high, then the
costs increase (for example due to machine replacement costs). We don’t need to
follow the exact reasoning behind this formula; this is just a model that the
optimiser has been given. If the company decides on a particular mining
operation x : [0, T] → R, then the total cost f(x) ∈ R over the whole mining
period [0, T] is given by
Indeed, x′(t)dt is the incremental amount of copper removed at time t, and if we
multiply this by c(t), we get the incremental cost at time t. The total cost should
be the sum of all these incremental costs over the interval [0, T], and so we

obtain the integral expression for f(x) given above.
Hence the mining company is faced with the following natural problem:
Which mining operation x incurs the least cost? In other words, minimise f : S →
R, where S denotes the set of all (continuously differentiable) functions x : [0, T]
→ R such that x(0) = 0 and x(T) = Q.
Exercise 0.1.
In Example 0.1, find f(x1), f(x2) where 
, 
.
Which is smaller among f(x1) and f(x2)?
Which mining operation among x1 and x2 will be preferred?
How do we solve optimisation problems in function spaces?
Suppose that instead of an optimisation problem in a function space, we consider
a much simpler problem:
Then we know how to solve this. Indeed, from ordinary calculus, we know the
following two facts.
Fact 1 allows us to narrow our choice of possible minimisers, since we can
calculate f′(x) = 2x − 2, and note that 2x − 2 = 0 if and only if x = 1. So if at all
there is a minimiser, then it must be x∗ = 1. On the other hand, f″(x) = 2 > 0 for
all x ∈ R, and so Fact 2 confirms that x∗ = 1 is a minimiser. Thus using these
two facts, we have completely solved the optimisation problem: we know that x∗
= 1 is the only minimiser of f.

Fig. 0.1 Fact 1 says that at a minimiser x∗, the tangent to the graph of f must be horizontal. Fact 2 says that
if f′ is increasing (f″  0), and if f′(x∗) = 0, then for points x to the left of x∗, f′ must be nonpositive, and so f
must be decreasing there, and similarly f must be increasing to the right of x∗. This has the consequence that
x∗ is a minimiser of f.
On the other hand our optimisation problem from Example 0.1 does not fit into
the usual framework of calculus, since there we have a real-valued function that
lives on a subset of an infinite dimensional function space, namely continuously
differentiable functions on the interval [0, T].
Thus the need arises for developing calculus in such infinite dimensional
vector spaces. Although we have only considered one example, problems
requiring calculus in infinite dimensional vector spaces arise from many
applications, and from various disciplines such as economics, engineering,
(hysics, and so on. Mathematicians observed that different problems from varied
fields often have related features and properties. This fact was used for an
effective unifying approach towards such problems, the unification being
obtained by the omission of unessential details. Hence the advantage of an
abstract approach is that it concentrates on the essential facts, so that these facts
become clearly visible and one’s attention is not disturbed by unimportant
details. Moreover, by developing a box of tools in the abstract framework, one is
equipped to solve many different problems (that are really the same problem in
disguise!) in one go, all at once.
In the abstract approach, one usually starts from a set of elements satisfying
certain axioms. The theory then consists of logical consequences which result
from the axioms and are derived as theorems once and for all. These general
theorems can then later be applied to various concrete special sets satisfying the
axioms. The reader is presumably already familiar with such a programme for
example in the context of elementary undergraduate linear algebra.
What will we learn in this book?
This aim of this book is to familiarise the reader with the basic concepts,
principles and methods of functional analysis and its applications.
We will develop such an abstract scheme for doing calculus in function
spaces and other infinite-dimensional spaces, and this is what this book is about.
Having done this, we will be equipped with a box of tools for solving many
problems, and in particular, we will return to the optimal mining operation
problem again and solve it.
The book contains many exercises, which form an integral part of the text, as

some results relegated to the exercises are used in proving theorems. The
solutions to all the exercises appear at the end of the book.
In this book we have described a few topics from functional analysis which
find widespread use, and by no means is the choice of topics “complete”.
However, equipped with this basic knowledge of the elementary facts in
functional analysis, the student can undertake a serious study of a more
advanced treatise on the subject, and the bibliography lists a few textbooks
which might be suitable for further reading.
Who is the book for?
The book is meant for final year undergraduate students. No prerequisites
beyond knowledge of linear algebra and ordinary calculus (with -δ arguments)
are needed to read this book. Knowledge of the Lebesgue integration theory or
topology is not assumed. Nevertheless, we will include illustrative examples that
involve Lebesgue integrals, with the naive under-standing that the Lebesgue
integral is simply a generalisation of the usual Riemann integral, which is rather
more amenable to limiting processes. We will give a short introduction to the
Lebesgue integral, entirely sufficient for treating our subsequent discussion,
when we encounter the first such example. parts of the book marked with an
asterisk (∗) may be more challenging as compared to the average level of
difficulty of the book, and may be skipped, skim-read or studied, depending on
the goals of the student. Thus the book should be accessible to a wide spectrum
of students, and may also serve to bridge the gap between linear algebra and
advanced functional analysis.
Acknowledgements
I am indebted to the late prof. Erik Thomas (University of Groningen), for many
useful comments and suggestions on the lecture notes used at the London School
of Economics, from which this book has grown. I would like to thank Dr. Sara
Maad Sasane for going through parts of the manuscript, pointing out typos and
mistakes, and offering insightful suggestions. Thanks are also due to prof.
Raymond Mortini (University of Metz) and to MSc student Edvard Johansson
(Lund University) for several useful comments. Finally, it is a pleasure to thank
the staff at World Scientific; in particular, Laurent Chaminade for his help, and
Eng Huay Chionh for overseeing the production of the book, and for valuable
copy editorial comments which improved the quality of the book. The book
relies on many sources, which are listed in the bibliography. This also applies to

the exercises.
Amol Sasane 
Lund, 2016

Contents
Preface
1. Normed and Banach spaces
1.1
Vector spaces
1.2
Normed spaces
1.3
Topology of normed spaces
1.4
Sequences in a normed space; Banach spaces
1.5
Compact sets
2. Continuous and linear maps
2.1
Linear transformations
2.2
Continuous maps
2.3
The normed space CL(X, Y)
2.4
Composition of continuous linear transformations
2.5
(∗) Open Mapping Theorem
2.6
Spectral Theory
2.7
(∗) Dual space and the Hahn-Banach Theorem
3. Differentiation
3.1
Definition of the derivative
3.2
Fundamental theorems of optimisation
3.3
Euler-Lagrange equation
3.4
An excursion in Classical Mechanics
4. Geometry of inner product spaces
4.1
Inner product spaces
4.2
Orthogonality

4.3
Best approximation
4.4
Generalised Fourier series
4.5
Riesz Representation Theorem
4.6
Adjoints of bounded operators
4.7
An excursion in Quantum Mechanics
5. Compact operators
5.1
Compact operators
5.2
The set K(X, Y) of all compact operators
5.3
Approximation of compact operators
5.4
(∗) Spectral Theorem for Compact Operators
6. A glimpse of distribution theory
6.1
Test functions, distributions, and examples
6.2
Derivatives in the distributional sense
6.3
Weak solutions
6.4
Multiplication by C∞ functions
6.5
Fourier transform of (tempered) distributions
Solutions
The Lebesgue integral
Bibliography
Index

Chapter 1
Normed and Banach spaces
As we had discussed in the introduction, we wish to do calculus in vector spaces
(such as C[a, b], whose elements are functions). In order to talk about the
concepts from calculus such as differentiability, we need a notion of closeness
between points of a vector space.
Recall for example, that a real sequence (an)n∈N is said to converge with limit
L ∈ R if for every  > 0, there exists an N ∈ N such that whenever n > N, |an − L|
< . In other words, the sequence converges to L if no matter what distance  > 0
is given, one can guarantee that all the terms of the sequence beyond a certain
index N are at a distance of at most  away from L (this is the inequality |an − L|
< ). So we notice that in this notion of “convergence of a sequence”, indeed the
notion of distance played a crucial role. After all, we want to say that the terms
of the sequence get “close” to the limit, and to measure closeness, we use the
distance between points of R. A similar thing happens with continuity and
differentiability. Recall that a function f : R → R is said to be continuous at c ∈
R if for every  > 0, there exists a δ > 0 such that whenever |x − c| < δ, |f(x) −
f(c)| < . Roughly, given any distance , I can find a distance δ such that
whenever I choose an x not farther than a distance δ from c, I am guaranteed that
f(x) is not farther than a distance of  from f(c). Again notice the key role played
by the distance in this definition. The distance between points x, y ∈ R is taken
as |x − y|, where | · | : R → [0, ∞) is the absolute value function, given by
If we imagine the real numbers depicted on a “number line”, then |x − y| is the
length of line segment joining x, y visualised on the number line. See the
following picture.

But now if one wants to also do calculus in a vector space X (for example C[a,
b]), there is so far no ready-made available notion of distance between vectors.
One way of creating a distance in a vector space is to equip it with a “norm” || ·
||, which is the analogue of absolute value | · | in the vector space R. The distance
function is then created by taking the norm ||x − y|| of the difference between
vectors x, y ∈ X, just like in R the Euclidean distance between x, y ∈ R was
taken as |x − y|.
Having done this, we have the familiar setting of calculus, and we can talk
about notions like the derivative of a function living on a normed space. (Later
on, in Chapter 3, we will then also have analogues of the two facts from ordinary
calculus relevant to optimisation, namely the vanishing of the derivative for
minimisers, and the sufficiency of this condition for minimisation when the
function is convex.) Thus the outline of this chapter is as follows.
First of all, we will learn the notion of a “normed space”, that is a vector
space equipped with a “norm”, enabling one to measure distances between
vectors in the vector space. This makes it possible to talk about concepts from
calculus, and in particular the notion of differentiability of functions between
normed spaces, as we shall see later on. Next, we will see lots of examples of
normed spaces: we will see that1
are all normed spaces, enabling us to do Calculus in each case.
Finally, we will introduce Banach spaces, which are special types of normed
spaces, namely ones in which “Cauchy sequences converge”. We will also
motivate this, and see why Banach spaces are nicer than having merely a normed
space.
We begin by recalling the notion of vector space.
1.1 Vector spaces
Roughly speaking it is a set of elements, called “vectors”. Any two vectors can
be “added”, resulting in a new vector, and any vector can be multiplied by an

element from R (or C, depending on whether we consider a real or complex
vector space), so as to give a new vector. The precise definition is given below.
Definition 1.1. (Vector space) Let K = R or C (or more generally2 a field). A
vector space over K, is a set X together with two functions, + : X × X → X,
called vector addition, and · : K × X → X, called scalar multiplication that
satisfy the following:
(V1) For all x1, x2, x3 ∈ X, x1 + (x2 + x3) = (x1 + x2) + x3.
(V2) There exists an element, denoted by 0 (called the zero vector) such
that for all x ∈ X, x + 0 = x = 0 + x.
(V3) For every x ∈ X, there exists an element, denoted by −x, such that x +
(−x) = (−x) + x = 0.
(V4) For all x1, x2 in X, x1 + x2 = x2 + x1.
(V5) For all x ∈ X, 1 · x = x.
(V6) For all x ∈ X and all α, β ∈ K, α · (β · x) = (αβ) · x.
(V7) For all x ∈ X and all α, β ∈ K, (α + β) · x = α · x + β · x.
(V8) For all x1, x2 ∈ X and all α ∈ K, α · (x1 + x2) = α · x1 + α · x2.
Example 1.1. (R). R is a vector space over R, with vector addition being the
usual addition of real numbers, and scalar multiplication being the usual
multiplication of real numbers. (R is also a vector space over the field Q of
rational numbers, but we will always consider the real vector space R unless
stated otherwise.)
Example 1.2. (Rd). Rd = R × · · · R (d times) is the set of all ordered d-tuples
(x1, · · · , xd) of real numbers x1, · · · , xd . Then Rd is a vector space over R, with
addition and scalar multiplication defined =component-wise”:
for (x1, · · · , xd), (y1, · · · , yd) ∈ Rd and α ∈ R.

Example 1.3. (C[a, b]). Let a, b ∈ R and a < b. Consider the vector space
consisting of all continuous functions x : [a, b] → K, with addition and scalar
multiplication defined in a “pointwise” manner as follows.
If x1, x2 ∈ C[a, b], then x1 + x2 ∈ C[a, b] is the function given by
If α ∈ K and x ∈ C[a, b], then α · x ∈ C[a, b] is the function given by
It can be checked that the vector space axioms (V1)-(V8) are satisfied. C[a, b] is
referred to as a ‘function space’, since each vector in C[a, b] is a function (from
[a, b] to K). The zero vector in C[a, b] is the zero function 0, given by 0(t) = 0
for all t ∈ [a, b].
Example 1.4. (C1[a, b]). Let C1[a, b] denote the space of continuously
differentiable functions on [a, b]:
(Recall that a function x : [a, b] → R is continuously differentiable if for all t ∈
[a, b], the derivative of x at t, namely x′(t), exists, and the map t  x′(t) : [a, b]
→ R is a continuous function.) We note that
because whenever a function x : [a, b] → R is differentiable at a point t in [a, b],
then x is continuous at t. In fact, C1[a, b] is a subspace of C[a, b] because it is
closed under addition and scalar multiplication, and is nonempty:
(S1) For all x1, x2 ∈ C1[a, b], x1 + x2 ∈ C1[a, b].
(S2) For all α ∈ R, x ∈ C1[a, b], α · x ∈ C1[a, b].
(S3) 0 ∈ C1[a, b].
Thus C1[a, b] is a vector space with the induced operations from C[a, b], namely
the same pointwise operations as defined in (1.1) and (1.2).

Example 1.5. (Sequence spaces). For any real p such that 1  p < ∞,
(Here we take the sequences (an)n∈N with values in K.) We define vector
addition and scalar multiplication termwise:
for elements (an)n∈N, (bn)n∈N ∈ ℓp and α ∈ K. It is not yet clear whether the sum
of two elements in ℓp, defined in the manner above, delivers an element in ℓp.
That this is indeed true is shown by the elementary chain of inequalities below:
We can use these inequalities termwise, and the Comparison Test for
convergence of real series, to conclude that (an)n∈N + (bn)n∈N ∈ ℓp whenever
(an)n∈N, (bn)n∈N ∈ ℓp.
By ℓ∞ we denote the vector space of all bounded sequences with values in K,
once again with termwise operations. It is easy to see that the sum of two
elements from ℓ∞ is again an element of ℓ∞.
It is clear that ℓp ⊂ ℓ∞ for all p ∈ [1, ∞]: if (an)n∈N ∈ ℓp, then
and so 
 |an|p = 0. In particular, (an)n∈N is a bounded sequence.
So all the ℓp spaces with a finite p are subspaces of ℓ∞. Some other important
subspaces are:

Example 1.6. (Lp[a, b]). For p ∈ [1, ∞], define
where the integral is the “Lebesgue integral” rather than the riemann integral.
What do we need to know about Lebesgue integrals? Firstly, every Riemann
integrable function x on an interval [a, b] is also Lebesgue integrable on [a, b],
and moreover, the Lebesgue integral then coincides with the usual Riemann
integral. However, the class of Lebesgue integrable functions is much larger than
the class of continuous functions. For instance, it can be shown that the function
is Lebesgue integrable, but not Riemann integrable on [0, 1]. For computation
aspects, one can get away without having to go into technical details about
Lebesgue integration. (In an appendix called “The Lebesgue Integral” on page
359, we have outlined the key definitions and a few relevant results on the
Lebesgue Integral, which the reader might wish to read if so desired, in order to
get a better feeling for the Lp spaces.)
We also note that in the above definition of Lp[a, b], we have put quotes
around the equality sign. What is that supposed to mean? Strictly speaking, each
element of Lp[a, b] is not a function x, but rather an equivalence class [x] of
functions, where
(The reason for wanting Lp[a, b] to be this set of equivalence classes [x], rather
than the functions x itself, will become clear when we discuss “norms”. It is tied
to demanding that the only vector in Lp[a, b] with 0 norm must be the zero
vector.) Of course, if x, y ∈ C[a, b], then

and thanks to the continuity of x, y, we could then conclude that
But it may happen for functions x, y ∈ Lp[a, b] that they are not equal as
functions, but nevertheless
In fact if x is the function given by (1.3), and y = 0, then it turns out that
but clearly x ≠ y! Note however that in this example, “almost everywhere”, that
is, for “almost all” t ∈ [0, 1], (only the rational t are excluded!) we do
have x(t) = y(t). These phrases can be made precise using the theory of Lebesgue
integral, in that it turns out that if
then x(t) = y(t) for all t ∈ [a, b]\N, where N has “Lebesgue measure” 0. We
won’t go into this, but we’ll simply bear in mind that for
So we view elements of Lp[a, b] through “fuzzy glasses”, and treat two functions
as being identical whenever the integral above is 0.
Analogous to the space ℓ∞, one can also introduce the space L∞[a, b]:

Since this example relies on the notion of Lebesgue measure, we won’t discuss
this any further now.
Exercise 1.1. True or false? The set V = (0, ∞) (positive reals) is a vector space
with addition and scalar multiplication given by x + y = xy and α · x = xα for all
positive x, y, and for all α ∈ R.
Exercise 1.2. (C[0, 1] is not finite dimensional.) Show that C[0, 1] with the usual
pointwise operations is not a finite dimensional vector space.
Hint: One can prove this by contradiction. Let C[0, 1] be a finite dimensional
vector space with dimension d, say. First show that the set B = {t, t2, ···, td} is
linearly independent. Then B is a basis for C[0, 1], and so the constant function 1
should be a linear combination of the functions from B. Derive a contradiction.
Exercise 1.3. Let S := {x ∈ C1[a, b] : x(a)= ya and x(b)= yb}, where ya, yb ∈ R.
Prove that S is a subspace of C1[a, b] if and only if ya = yb = 0. (So we see that S
is a vector space with pointwise operations if and only if ya = yb = 0.)
1.2 Normed spaces
We would like to develop calculus in the setting of vector spaces (for example,
in function spaces like C[a, b]). Underlying all the fundamental concepts in
ordinary calculus, is the notion of closeness between points. So in order to
generalise the notions from ordinary calculus (where we work with real
numbers, and where the absolute value is used to measure distances), to the
situation of vector spaces, we need a notion of distance between elements of the
vector space. This is done by introducing an additional structure on a vector
space, namely, a “norm”, which is a real-valued function || · || defined on the
vector space, and the norm plays a role analogous to the one played by the
absolute value in R. Once we have a norm on a vector space X (in other words a
“normed space”), then the distance between x, y ∈ X will be taken as ||x − y||.
Definition 1.2. (Norm; normed space). Let X be a vector space over K (R or C).
A norm on X is a function || · || : X → [0, +∞) such that:
(N1) (Positive definiteness).
For all x ∈ X, ||x||  0. If x ∈ X and ||x|| = 0, then x = 0.

(N2) For all α ∈ K (R or C) and for all x ∈ X, ||αx|| = |α|||x||.
(N3) (Triangle inequality) For all x, y ∈ X, ||x + y||  ||x|| + ||y||.
A normed space is a vector space X equipped with a norm.
Distance in a normed space. Just like in R, with the absolute value, and where
the distance between x, y ∈ R is |x − y|, now in a normed space (X, || · ||), we
have for x, y ∈ X, that the number ||x − y|| is taken as the distance between x, y ∈
X. Thus ||x|| = ||x − 0|| is the distance of x from the zero vector 0 in X.
Remark 1.1. (Metric spaces). A metric space is a set X together with a function
d : X × X → R satisfying the following properties:
(D1) (Positive definiteness) 
For all x, y ∈ X, d(x, y)  0. For all x ∈ X, d(x, x) = 0. 
If x, y ∈ X are such that d(x, y) = 0, then x = y.
(D2) (Symmetry) For all x, y ∈ X, d(x, y) = d(y, x).
(D3) (Triangle inequality) For all x, y, z ∈ X, d(x, y) + d(y, z)  d(x, z).
The reader familiar with “metric spaces” may notice that in a normed space (X, ||
· ||), if we define d : X × X → R by d(x, y) = ||x – y|| for x, y ∈ X, then it is easily
seen that d satisfies (D1)-(D3), and so (X, d) is a metric space with the
metric/distance function d. This distance d is referred to as the induced distance
in the normed space (X, || · ||). Then ||x|| = ||x − 0|| = d(x, 0), and so the norm of a
vector x in the normed space (X, || · ||) is the induced distance of x to the zero
vector.
We now give a few examples of normed spaces, by reconsidering the vector
space examples from the previous section, and equipping each of them with
norms.
Example 1.7. (R, | · |). R is a vector space over R. Define || · || : R → R by ||x|| =
|x|, for x ∈ R. Then (R, |·|) is a normed space. (No surprise, since wanting to
generalise the situation from ordinary calculus in R to the case of vector spaces, |
· | is what motivated the definition of the norm || · ||!)
Example 1.8. (Rd, || · ||p). Rd is a vector space over R. Let us define the

Euclidean norm || · ||2 by
Then Rd is a normed space (see Exercise 1.8.(1) on page 16).
(The motivation behind calling (N3) the triangle inequality is now evident.
Indeed, for triangles in Euclidean Geometry of the plane, we know that the sum
of the lengths of two sides of a triangle is at least as much as the length of the
third side. If we now imagine the points 0, –x, y ∈ R2 as the three vertices of a
triangle, then this is what (N3) says for the || · ||2 norm; see the following
picture.)
|| · ||2 is not the only norm3 that can be defined on Rd. For example,
are also examples4
Note that (Rd, || · ||2), (Rd, || · ||1) and (Rd, || · ||∞) are all different normed
spaces. This illustrates the important fact that from a given vector space, we can
obtain various normed spaces by choosing different norms. What norm is
considered depends on the particular application at hand. We illustrate this in the
next paragraph.
Imagine a city (like New York) in which there are streets and avenues with
blocks in between, forming a square grid as shown in the picture below. Then if
we take a taxi/cab to go from point A to point B in the city, it is clear that it isn’t
the Euclidean norm in R2 which is relevant, but rather the || · ||1-norm in R2. (It
is for this reason that the || · ||1-norm is sometimes called the taxicab norm.)

So what norm one uses depends on the situation at hand, and is something that
the modeller decides. It is not something that falls out of the sky!
Example 1.9. (C[a, b] as a normed space). Consider the vector space C[a, b]
defined earlier. Define
Then || · ||∞ is a norm on C[a, b], and is referred to as the “supremum norm.” The
second equality above, guaranteeing that the supremum is attained, that is, that
there is a c ∈ [a, b] such that
follows from the Extreme Value Theorem5 for continuous functions.
Exercise 1.4. In C[0, 1] equipped with the || · ||∞-norm, calculate the norms of t,
–t, tn and sin(2πnt), where n ∈ N.
Let us check that || · ||∞ on C[a, b] does satisfy (N1), (N2), (N3).
(N1) For x ∈ C[a, b], |x(t)|  0 for all t ∈ [a, b]. So ||x||∞ = 
 |x(t)|  0.
Also, if x ∈ C[a, b] is such that ||x||∞ = 0, then for each t ∈ [a, b],
So for all t ∈ [a, b], |x(t)| = 0, and so x(t) = 0.
In other words, x = 0, the zero function in C[a, b].
(N2) If α ∈ R and x ∈ C[a, b], then |(α · x)(t)| = |αx(t)| = |α||x(t)|, for t ∈ [a, b],
and so ||α · x||∞ = 
 |α||x(t)| = |α| 
 |x(t)| = |α|||x||∞.

(N3) Let x1, x2 ∈ C[a, b]. If t[a, b], then
As this holds for all t ∈ [a, b], 
 |(x1 + x2)(t)|  ||x1||∞ + ||x2||∞.
Thus ||x1 + x2||∞  ||x1||∞ + ||x2||∞.
So C[a, b] with the supremum norm || · ||∞ is a normed space. Thus we can use
||x1 − x2||∞ as the distance between x1, x2 ∈ C[a, b].
Geometric meaning of the distance in C[a, b] equipped with the supremum
norm. We ask the question: what does it mean geometrically when we say that x
is close to x0? In other words, what does the set of points x that are close to (say
within a distance of  from) x0 look like?
In (R, | · |), we know that the set of points x whose distance to x0 is less than 
 is an interval:
and so {x ∈ R : |x − x0| < } = (x0 − , x0 + ).
Now we ask: can we visualise the set {x ∈ C[a, b] : ||x − x0||∞ < }? We have
that
We can imagine translating the graph of x0 upward by a distance of , and
downward through a distance of , so as to obtain the shaded strip depicted in the
following picture. Then the graph of x has to lie in this shaded strip, because at
each t, x0(t) −  < x(t) < x0(t) + . So for example at the particular t indicated in

the following picture, x(t) has to lie on the line segment AB. Since this has to
happen at each t ∈ [a, b], we see that the graph of x lies in the shaded strip.
Fig. 1.1 The set of all continuous functions x whose graph lies between the two dashed curves is the “ball”
B(x0, ) = ||x ∈ C[a, b] : {x − x0||∞ < }.
Here are examples of some other frequently used norms in C[a, b]:
for x ∈ C[a, b]. The || · ||1-norm can be thought of as a continuous analogue of
the taxicab norm, while the || · ||2 norm is the continuous analogue of the
Euclidean norm. The verification that || · ||1 is indeed a norm on C[a, b] will be
done in Exercise 1.10. We’ll postpone checking that || · ||2 is also a norm on C[a,
b] until Chapter 4, where we will first check that C[a, b] can be endowed with an
“inner product”
and then
will automatically become a norm! right now we’ll just accept the fact that || · ||2
is a norm on C[a, b].
We will see later on that (C[a, b], || · ||∞) is “complete”, that is, {Cauchy

sequences} = {convergent sequences}, while (C[a, b], || · ||2) is not complete. On
the other hand, (C[a, b], || · ||2) has a “nicer geometry”, allowing one to talk
about orthogonality6. What is the remedy? This motivates the consideration of
(L2[a, b], || · ||2), which besides allowing the nice geometry, also turns out to be
complete. We will introduce this normed space in Example 1.12 below.
Example 1.10. C1[a, b]. recall our optimal mining problem from Example 0.1,
where the function to be minimised was defined on a subset of the subspace
C1[a, b]. So we see that the space C1[a, b] also arises naturally in applications.
What norm do we use in C1[a, b]? In general, if X is a normed space and Y is a
subspace of the vector space X, then we can make Y into a normed space by
simply using the restriction of the norm in X to Y. This is called the induced
norm in Y, and in Exercise 1.7, we will see that this does give a norm on Y. So
surely C1[a, b], being a subspace of C[a, b] (which is a normed space with the
supremum norm), is also a normed space with the supremum norm || · ||∞.
However, it turns out that in applications, this is not a good choice, essentially
because the differentiation map
is not “continuous” (we will see this later on). There is a different norm on C1[a,
b], denoted by || · ||1,∞, given below, which we shall use:
In C1[0, 1], for example
Roughly, two functions in (C1[a, b], || · ||1,∞) are regarded as close together if
both the functions themselves and their first derivatives are close together.
Indeed, ||x1 − x2||1,∞ <  implies that

and conversely, (1.7) implies that ||x1 – x2||1,∞ < 2 . We will see later (when
discussing continuity of maps between normed spaces), that the differentiation
mapping from C1[a, b] to C[a, b] is continuous if C1[a, b] is equipped with the ||
· ||1,∞-norm and C[a, b] is equipped with the || · ||∞-norm.
Example 1.11. (Sequence spaces). For 1  p < ∞, ℓp is a normed space with the ||
· ||p norm, given by
Checking that the triangle inequality holds can be done using an inequality
called Hölder’s Inequality; see Exercise 1.8.
When p = ∞, that is, for the sequence space ℓ∞, we define
Then it is easy to check that || · ||∞ is a norm, and so (ℓ∞, || · ||∞) is a normed
space.
Example 1.12. For 1  p < ∞, Lp[a, b] is the normed space with the || · ||p norm,
given by
We won’t check the validity of (N3) here. Also, the space L∞[a, b] is a normed
space with the norm given by
Again, we won’t try to make “almost all” precise, as it relies of the notion of
Lebesgue measure. The || · ||∞-norm here is referred to as the “essential
supremum norm”.

We remark that even more generally, if (Ω, M, μ) is any “measure space”,
where μ is a positive measure, then Lp(Ω) denotes the collection of all real-
valued measurable functions x on Ω with
It turns out that (Lp(Ω), || · ||p) is a normed space, which is moreover, complete.
This normed space arises in applications, for example when (Ω, M, μ) is a
probability space, where the x are “random variables”, and ||x||pp then has the
interpretation of being the expected value E(|xp).
Exercise 1.5. (Triangle Inequality). Let (X, || · ||) be a normed space. Prove that
for all x, y ∈ X, |||x|| – ||y|| –  ||x − y}.
Exercise 1.6. If x ∈ R, then let ||x|| = |x| . Is || · || a norm on R?
Exercise 1.7. Let X be a normed space with norm || · ||X, and Y be a subspace of
X. Prove that Y is also a normed space with the norm || · ||Y defined simply as the
restriction of the norm || · ||X to Y. This norm on Y is called the induced norm.
Exercise 1.8. Let 1 < p < ∞ and q be defined by 
.
Then Hölder’s inequality says that if x1, ···, xd, y1, ···, yd ∈ C, then
Let’s quickly establish this inequality. Suppose that a, b ∈ R and a, b  0. We
begin by showing that
If a = 0 or b = 0, then the conclusion is clear, and so we assume that both a and b
are positive. We will use the following result.

Claim: If α ∈ (0, 1), then for all x ∈ [1, ∞), α(x − 1) + 1  xα.
Given α ∈ (0, 1), define fα : [1, ∞) → R by fα(x) = α(x − 1) – xα + 1, for x  1.
Note that fα(1) = α · 0 − 1α + 1 = 0, and for all x  1,
By the Fundamental Theorem of Calculus, for any x > 1,
and so we obtain fα(x)  0 for all x ∈ [1, ∞), completing the proof of the claim.
As p ∈ (1, ∞), it follows that 1/p ∈ (0, 1). Applying the above with α = 1/p and
we obtain inequality (1.8).
Holder’s inequality is obvious if 
 = 0 or 
 = 0.
So we assume that neither is 0, and proceed as follows.
Define am = |xm|p/
 |xn|p and bm = |ym|q/
 |yn|q, 1  m  d.
Applying the inequality (1.8) to am, bm, we obtain for each m that:
Adding these d inequalities, we obtain Hölder’s inequality.
If 1  p  ∞, and d ∈ N, then for x = (x1, ···, xd) ∈ Rd, define

(1) Show that the function x  ||x||p is a norm on Rd.
Hint: In the case when 1 < p < ∞, use Hölder’s inequality to obtain
and use ||x + y||pp = 
 |xn + yn||xn + yn|p–1
(2) Let d = 2. For 1  p  ∞, the “open unit ball” Bp(0, 1), is defined by Bp(0, 1)
:= {x ∈ R2 : ||x||p < 1}. Sketch Bp(0, 1) for p = 1, 2, ∞.
(3) (Explanation of the notation for the maximum norm “|| · ||∞”.) Let x ∈ R.
Prove that (||x||p)p∈N is a convergent sequence in R, and 
 ||x||p = ||x||∞.
Describe qualitatively what happens to the sets Bp(0, 1) as p tends to ∞.
Exercise 1.9. A subset C of a vector space is said to be convex if for all x, y ∈ C,
and all α ∈ (0, 1), (1 − α)x + αy ∈ C; see the following picture for examples of
convex and nonconvex sets in R2.
(1) In any normed space (X, || · ||), show that the “closed unit ball” 
defined by 
 := {x ∈ X : ||x||  1} is convex.
(2) Depict the set 
 in the plane.
(3) (Explanation of why we’ve been taking p in [1, ∞) rather than just all p > 0).
Prove that 
 does not define a norm on R2.
Exercise 1.10. Show that (1.5) on page 12 defines a norm on C[a, b].
Exercise 1.11. Let Cn[a, b] be the set of n times continuously differentiable

functions on [a, b]: Cn[a, b] = {x : [a, b] → R such that x′, x″, ···, x(n) ∈ C[a,
b]}, equipped with pointwise operations, and the norm
Show that || · ||n,∞ is a norm on Cn[a, b].
Exercise 1.12. (“p-adic norm”). Consider the vector space of the rational
numbers Q over the field Q. Let p be a prime number. If the integer q divides the
integer n, we write q | n, and if not, then we write q  n.
Define the p-adic norm  · p on Q as follows:
0p := 0, and if r ∈ Q\{0}, then rp :=  where 
So in this context, a rational number is close to 0 precisely when it is “highly
divisible” by p.
(1) Show that  · p is well-defined on Q.
(2) If r ∈ Q, then prove that rp  0, and that if rp = 0 then r = 0.
(3) For all r1, r2 ∈ Q, show that 
.
(4) For all r1, r2 ∈ Q, prove that 
. 
In particular, for all r1, r2 ∈ Q, 
.
Exercise 1.13. Consider the vector space Rm×n of matrices with m rows and n
columns of real numbers, with the usual entrywise addition and scalar
multiplication. Let the entry in the ith row and jth column of M be denoted by
mij. For M ∈ Rm×n, define 
. Show that || · ||∞ is a norm on Rm×n.
1.3 Topology of normed spaces
In a normed space, we can describe “neighbourhoods” of points by considering
sets which include all points whose distance to the given point is not too large.
Definition 1.3. (Open ball).
Let (X, || · ||) be a normed space, x ∈ X, and r > 0.
The open ball B(x, r) with centre x and radius r is defined by

Thus B(x, r) is the set of all points in X whose distance to the centre x is strictly
less than r.
We’ll keep the following picture in mind.
In the sequel, for example in our study of continuous functions, open sets will
play an important role. Here is the definition.
Definition 1.4. (Open set). Let (X, || · ||) be a normed space. A set U ⊂ X is said
to be open if for every x ∈ U, there exists an r > 0 such that B(x, r) ⊂ U.
Note that the radius r may depend on the choice of the point x. See the following
picture. roughly speaking, no matter which point you take in an open set, there is
always some “room” around it consisting only of points of the open set.
Example 1.13. Let us show that the “open interval” (a, b) is open in R. Given
any x ∈ (a, b), we have a < x < b. Motivated by the following picture, let us take
r = min{x − a, b − x}. Then r > 0, and if |y − x| < r, then −r < y − x < r. So a = x
− (x − a)  x − r < y < x + r  x + (b − x) = b, that is, y ∈ (a, b). Hence B(x, r) ⊂
(a, b). Consequently, (a, b) is open.

On the other hand, the interval [a, b] is not open: with x := a ∈ [a, b], we have
that no matter how small an r > 0 we take, the set
contains points that do not belong to [a, b]: for example,
The picture above illustrates this.
Example 1.14. The set X is open, since given an x ∈ X, we can take any r > 0,
and notice that B(x, r) ⊂ X trivially.
The empty set ∅ is also open (“vacuously”). Indeed, the reasoning is as
follows: can one show an x for which there is no r > 0 such that B(x, r) ⊂ ∅?
And the answer is no, because there is no x in the empty set (let alone an x which
has the extra property that there is no r > 0 such that B(x, r) ⊂ ∅!).
Exercise 1.14. Let (X, || · ||) be a normed space, x ∈ X and r > 0. Show that the
open ball B(x, r) is an open set.
Exercise 1.15. We know that the segment (0, 1) is open in R. Show that the
segment (0, 1) considered as a subset of the plane, that is, the set
is not open in (R2, || · ||2).
Exercise 1.16. (Euclidean, taxicab, and maximum norm topologies coincide).
Recall the three norms || · ||2 (Euclidean), || · ||1 (taxicab) and || · ||∞ (maximum)
on R2 from Example 1.8 on page 9. Give a pictorial “proof without words” to
show that a set U is open in R2 in the Euclidean metric if and only if it is open
when R2 is equipped with the metric d1 or the metric d∞. Hint: Inside every
square you can draw a circle, and inside every circle, you can draw a square!
Lemma 1.1. Any finite intersection of open sets is open.

Proof. It is enough to consider two open sets, as the general case follows
immediately by induction on the number of sets.
Let U1, U2 be two open sets. Let x ∈ U1  U2. Then there exist r1 > 0, r2 > 0
such that B(x, r1) ⊂ U1 and B(x, r2) ⊂ U2. Take r = min{r1, r2}. Then r > 0, and
we claim that B(x, r) ⊂ U1  U2. To see this, let y be an element of B(x, r). Then
||x − y|| < r = min{r1, r2}, and so ||x − y|| < r1 and ||x − y|| < r2. So y ∈ B(x, r1) 
B(x, r1) ⊂ U1  U2.
Example 1.15. The finiteness condition in the above lemma cannot be dropped:
In R, consider the open sets Un := (−1/n, 1/n), n ∈ N. Then we have 
 Un =
{0}, which is not open in R.
Lemma 1.2. Any union of open sets is open.
Proof. Let Ui, i ∈ I, be a family of open sets indexed7 by the set I.
If x ∈  Ui, then we have that x ∈ Ui∗ for some i∗ ∈ I.
But as Ui∗ is open, there exists a r > 0 such that B(x, r) ⊂ Ui∗.
Thus B(x, r) ⊂ Ui∗ ⊂  Ui. So the union  Ui is open.
Definition 1.5. (Closed set). Let (X, || · ||) be a normed space. A set F is closed if
its complement X\F is open.
Example 1.16. The “closed interval” [a, b] is closed in R. Indeed, its
complement R\[a, b] is the union of the two open sets (−∞, a) and (b, ∞). Hence
R\[a, b] is open, and so [a, b] is closed.
The set (−∞, b] is closed in R. (Why?)
The sets (a, b], [a, b) are neither open nor closed in R. (Why?)
Example 1.17. X, ∅ are closed.
Exercise 1.17. Show that arbitrary intersections of closed sets are closed. Prove
that a finite union of closed sets is closed.
Can the finiteness condition be dropped in the previous claim?

Exercise 1.18. Let (X, || · ||) be a normed space, x ∈ X and r > 0.
Show that the “closed ball” 
 is a closed set.
Exercise 1.19. Determine if the following statements are true or false.
(1) If a set is not open, then it is closed.
(2) If a set is open, then it is not closed.
(3) There are sets which are both open and closed.
(4) There are sets which are neither open nor closed.
(5) Q is open in R.
(6) (∗) Q is closed in R.
(7) Z is closed in R.
Exercise 1.20. Let (X, || · ||) be a normed space.
Show that the unit sphere S := {x ∈ X : ||x|| = 1} is closed.
Exercise 1.21. Let (X, || · ||) be a normed space.
Show that a singleton (a subset of X having exactly one element) is always
closed.
Conclude that every finite subset F of X is closed.
Exercise 1.22. (∗) A subset D of a normed space (X, || · ||) is said to be dense in
X if for all x ∈ X and all  > 0, there exists a y ∈ D such that ||x − y|| < c.
That is, if we take any x ∈ X and consider any ball B(x, ) centred at x, it
contains a point from D. In everyday language, we may say for example that
“These woods have a dense growth of birch trees”, and the picture we then have
in mind is that in any small area of the woods, we find a birch tree. A similar
thing is conveyed by the above: no matter what “patch” (described by B(x, ) we
take in X (thought of as the woods), we can find an element of D (analogous to
birch trees) in that patch.
Show that Q is dense in R by proceeding as follows.
If x, y ∈ R and x < y, then show that there is a q ∈ Q such that x < q < y. (By the
Archimedean Property8 of R, there is a positive integer n such that n(y − x) > 1.
Next there are positive integers m1, m2 such that m1 > nx and m2 > −nx so that
−m2 < nx < m1. Hence there is an integer m such that m − 1 
 nx < m.
Consequently nx < m  1 + nx < ny, which gives the desired result.)

Exercise 1.23. Is the set R\Q of irrational numbers dense in R? Hint: Take any x
∈ R. If x is irrational itself, then we may just take y to be x and we are done;
whereas if x is rational, then take y = x + √2/n with a sufficiently large n.
Exercise 1.24. Show that c00 is dense in ℓ2.
Exercise 1.25. (Separable spaces.) A normed space X is called separable if it has
a countable dense set, that is, there exists a set D := {x1, x2, x3, ···} in X such that
for every r > 0 and every x ∈ X, there exists an xn ∈ D such that ||xn − x|| < r. For
example R is separable, since we can simply take D = Q.
Show that ℓ1 is separable. (Analogously it can be shown that ℓp is separable for
all 1  p < ∞.)
On the other hand, ℓ∞ is not separable. Suppose that D = {x1, x2, x3, ···} is a
dense subset of ℓ∞. Consider the set A of all sequences with all terms equal to
either 0 or 1. If (an)n∈N, (bn)n∈N are distinct elements of A, then their mutual
distance is 1, since an ≠ bn for at least one n. Now by the density of D in ℓ∞, it
follows that for each a ∈ A, we can choose an element xn(a) ∈ B(a, 1/3). As the
balls B(a, 1/3), a ∈ A, are all mutually disjoint, it follows that we get an injective
map A ∋ a  n(a) ∈ N, a contradiction, since A is uncountable (as it is in one-to-
one correspondence with all real numbers between 0 and 1 via binary
expansion).
Separability is a sort of a topological limitation on size. It plays a role in
constructive mathematics, since many theorems have constructive proofs only
for separable spaces even though the theorem is true for nonseparable ones. Such
constructive proofs can sometimes be turned into algorithms for use in numerical
analysis.
Exercise 1.26. (Weierstrass’s Approximation Theorem).
The aim of this exercise is to show that polynomials are dense in (C[a, b], || · ||
∞). By considering the map x  x(a + ·(b − a)) : C[a, b] → C[0, 1], we see that
there is no loss of generality in assuming that a = 0 and b = 1. For x ∈ C [0, 1]
and n ∈ N, let Bnx be the polynomial9 given by
Let us introduce the auxiliary polynomials

Show that:
The proof of Weierstrass’s Approximation Theorem can now be completed as
follows. For δ > 0, we have
where we used the observation
in order to obtain the last inequality.
Now for δ > 0, set ωδ(x) := 
 |x(t) − x(s)|.
Then we have

Let  > 0. Since x is uniformly continuous10, we can choose δ > 0 such that ωδ(x)
< /2. Next choose n > ||x||∞/( δ2). Then it follows from the above that ||Bnx − x||
∞ < , completing the proof of the Weierstrass Approximation Theorem.
Remark 1.2. (Topology). If we look at the collection O of all open sets in a
normed space (X, || · ||), we notice that it has the following three properties:
(T1) ∅, X ∈ O.
(T2) If Ui ∈ O for all i ∈ I, then 
 Ui ∈ O.
(T3) If U1, ···, Un is a finite collection of sets from O, then 
 Ui ∈ O.
More generally, if X is any set (not necessarily a normed space), then any
collection O of subsets of X that satisfy properties (T1), (T2), (T3) is called a
topology on X and (X, O) is called a topological space. Elements of O are called
open sets in (X, O). So for a normed space X, if we take O to be the family of
open sets in (X, || · ||), then we obtain a topological space. The following picture
displays the hierarchy of structures11.
It turns out that one can in fact extend some of the notions from Calculus (such
as convergence of sequences and continuity of maps) in the even more general
set-up of topological spaces, devoid of any metric or norm, where the notion of
closeness is specified by considering arbitrary open neighbourhoods provided by
elements of O. In some applications this is exactly the right thing needed, but we
will not go into such abstractions here. In fact, this is a very broad subdiscipline
of mathematics called Topology.

1.4 Sequences in a normed space; Banach spaces
In a normed space, we have a notion of “distance” between vectors, and we can
say when two vectors are close by, and when they are far away. So we can talk
about convergent sequences. In the same way as in R or C, we can define
convergent sequences and Cauchy sequences in a normed space:
Definition 1.6. (Convergent sequence). Let (xn)n∈N be a sequence in X and let L
∈ X. The sequence (xn)n∈N is said to be convergent (in X) with limit L if
In the above, we have used the symbol “∀”, which is read “for every”. Also the
symbol “∃” means “there exists a/an”.
Note that the definition says that the convergence of (xn)n∈N to L is the same
as the real sequence (||xn − L||)n∈N converging to 0:
that is the distance of the vector xn to the limit L tends to zero, and this matches
our geometric intuition. One can show in the same way as with R, that the limit
is unique: a convergent sequence has only one limit.
We write 
 xn = L.
Theorem 1.1. A convergent sequence has a unique limit.
Proof. Let (xn)n∈N be convergent with limits L1 and L2, with L1 ≠ L2. Let  := ||L1
− L2||/3 > 0, where the positivity of the  follows from the fact that L1 ≠ L2. Since
L1 is a limit of the sequence (xn)n∈N, there exists an N1 ∈ N such that for all n >
N1, ||xn − L1|| < . Since L2 is a limit of the sequence (xn)n∈N, there exists an N2 ∈
N such that for all n > N2, ||xn − L2|| < . So for n > N1 + N2, we have n > N1 and
n > N2, and

So we arrive at the contradiction that 1 < 2/3. Hence our assumption was
incorrect, and so a convergent sequence must have a unique limit.
Example 1.18. Consider the sequence (xn)n∈N in the normed space (C[0, 1], || · ||
∞), where xn = 
, t ∈ [0, 1].
The first few terms of the sequence are shown in the following picture.
From the figure, we see that the terms seem to converge to the zero function.
Indeed we have
Given  > 0, let N ∈ N be such that N > 1/ . Then for all n > N,
So (xn)n∈N is convergent in the normed space (C[0, 1], || · ||∞) to 0.
Definition 1.7. (Cauchy sequence). A sequence (xn)n∈N in a normed space (X, || ·
||) is called a Cauchy sequence if for every  > 0, there exists an N ∈ N such that
for all m, n ∈ N satisfying m, n > N, ||xm − xn|| < .
Roughly speaking, we can make the terms of the sequence arbitrarily close to
each other provided we go far enough in the sequence.

Proposition 1.1. Every convergent sequence is Cauchy.
Proof. Let (xn)n∈N be a sequence in (X, || · ||) that converges to L ∈ X. Let  > 0.
(We want to find N which guarantees for n, m > N that ||xn − xm|| < . But we do
know that the terms xn, xm can both be made close to L if n, m are large enough.
So we introduce L artificially: ||xn − xm|| = ||xn − L + L − xm|| and use the triangle
inequality to complete the argument. The details are given below.)
Then there exists an N ∈ N such that for n > N, we have ||xn − L|| < .
Thus for n, m > N, we have
So the sequence (xn)n∈N is a Cauchy sequence.
We recall from ordinary calculus that in R,
(We will recall the proof of this fact below, in Theorem 1.4 on page 31.) This
raises the tempting question of whether this equality is true in general normed
spaces too:
If the two sets coincide, then one can conclude that a sequence is convergent by
just checking Cauchyness. This is the basis of many existence results in
Analysis: for example, the convergence tests in Calculus, the existence results
for differential equations, the Riesz representation Theorem12, etc. Once

existence is known, (and after showing uniqueness, if valid), one can justify and
use numerical approximations. So this prompts the question:
Q. Is it true in all normed spaces that
Answer: No. It is true in some normed spaces, for example
but not true in others, for example
(We will soon justify these claims.)
In light of the above answer, it makes sense to give normed spaces in which
a special name. These are called Banach spaces, after the Polish mathematician
Stefan Banach (1892–1945), who laid the foundations of the study of such
spaces in his doctoral dissertation from 1920.
Definition 1.8. (Banach space). A normed space in which the set of Cauchy
sequences is equal to the set of convergent sequences is called a Banach space.
Sometimes, we also call it a complete normed space.
Thus in a complete normed space, or Banach space, the Cauchy condition is
sufficient for convergence: the sequence (xn)n∈N converges if and only if it is a
Cauchy sequence. So we can determine convergence a priori without the
knowledge of the limit. Just as it was possible to introduce new numbers in R as
the limits of Cauchy sequences, now in a Banach space, it is possible to show the
existence of elements with some property of interest, by making use of the
Cauchyness. In this manner, one can sometimes show that certain equations

possess a solution. In many cases, one cannot write the solution explicitly. But
after existence and uniqueness of the solution is demonstrated, one can do
numerical approximations.
(R, | · |) is a Banach space
The completeness of R will be used fundamentally in checking all of our other
examples of Banach spaces. While the fact that real Cauchy sequences are
always convergent may be familiar to the reader, we reprove this here for the
sake of completeness. We will first establish the following elementary lemma,
which is valid in all normed space, not just in R.
Lemma 1.3. Every Cauchy sequence in a normed space is bounded13.
Proof. Suppose that (xn)n∈N is a Cauchy sequence in the normed space (X, || · ||).
Choose any positive , say  = 1. Then there exists an N ∈ N such that for all n,
m > N, ||xn − xm|| < . In particular, with m = N + 1 > N, and n > N, ||xn − xN+1|| < 
. By the Triangle Inequality, for all n > N, ||xn|| = ||xn − xN+1 + xN+1||  ||xn −
xN+1|| + ||xN+1|| < 1 + ||xN+1||. On the other hand, for n  N, ||xn||  maxt{||x1||, ···,
||xN||, 1 + ||xN+1||} =: M. So ||xn||  M for all n ∈ N, that is, the sequence (xn)n∈N is
bounded.
Next we’ll show that:
Theorem 1.2. Every real sequence has a monotone14 subsequence.
Before giving the formal proof, we give an illustration of the idea behind this
proof15. If (xn)n∈N is the given sequence, then imagine that there is an infinite
chain of hotels along a line, where the nth hotel has height xn, and at the horizon,
there is a sea. A hotel is said to have the seaview property if it is higher than all
hotels following it (so that from the roof of the hotel, one can view the sea).
There are only two possibilities:

Proof. Let (xn)n∈N be a real sequence, and let
(This is the collection of indices of hotels with the seaview property.) Then we
have the following two cases.
1° S is infinite. 
Arrange the elements of S in increasing order: n1 < n2 < n3 < .... Then (xnk)k∈N
is a decreasing subsequence of (xn)n∈N.
2° S is finite.
If S is empty, then define n1 = 1, and otherwise let n1 = max S + 1.
Define inductively nk+1 = min{m ∈ N : m > nk and xm  xnk}. (nk+1 is the
index of the first hotel blocking the view from the top of the nkth hotel.) The
minimum exists as {m ∈ N : m > nk and xm  xnk} is a nonempty subset of N.
(Otherwise if it were empty, then nk ∈ S, and this is not possible if S was
empty, and also impossible if S was not empty, since nk > max S.) Then
(xnk)k∈N is an increasing subsequence of (xn)n∈N.
Theorem 1.3.

If a real sequence is monotone and bounded, then it is convergent.
Proof.
1° We will first consider the case of increasing sequences which are bounded.
Let (xn)n∈N be an increasing and bounded sequence. We want to show that
(xn)n∈N is convergent. But with what limit?
The picture above suggests that the limit should be the smallest number bigger
than each of the terms of this sequence, that is, the supremum of the set {xn : n ∈
N}. Since (xn)n∈N is bounded, it follows that the set S := {xn : n ∈ N} has an
upper bound and so sup S exists. We show that in fact (xn)n∈N converges to sup
S. Let  > 0. Since sup S −  < sup S, it follows that sup S −  is not an upper
bound for S, and so there exists an xN ∈ S such that sup S −  < xN, that is sup S −
xN < . Since (xn)n∈N is an increasing sequence, for n > N, we have xN  xn. Since
sup S is an upper bound for S, xn  sup S and so |xn − sup S| = sup S − xn, Thus
for n > N we obtain |xn − sup S| = sup S − xn  sup S − xN < .
2° If (xn)n∈N is a decreasing and bounded sequence, then clearly (−xn)n∈N is an
increasing sequence. Furthermore if (xn)n∈N is bounded, then (−xn)n∈N is
bounded as well (|−xn| = |xn| 
 M). Hence by the case considered above, it
follows that (−xn)n∈N is a convergent sequence with limit sup{–xn : n ∈ N} =
−inf{xn : n ∈ N} = −inf S, where S = {xn : n ∈ N}. So given  > 0, there exists an
N ∈ N such that for all n > N, |−xn − (−inf S)| < , that is, |xn − inf S| < . Thus
(xn)n∈N is convergent with limit inf S.
Corollary 1.1. (Bolzano-Weierstrass Theorem).
Every bounded real sequence has a convergent subsequence.
Proof. Let (xn)n∈N be a bounded real sequence. The sequence (xn)n∈N has a
monotone subsequence, say (xnk)k∈N. Then (xnk)k∈N is bounded too. We have that

(xnk)k∈N is monotone and bounded, and hence it is convergent in R.
We are now ready to prove that (R, | · |) is a Banach space.
Theorem 1.4. Every real Cauchy sequence in R is convergent.
Proof. Let (xn)n∈N be Cauchy in R. Then (xn)n∈N is bounded. By the Bolzano-
Weierstrass Theorem, (xn)n∈N has a convergent subsequence, say (xnk)k∈N, with
limit, say L ∈ R. We will now show that (xn)n∈N is also convergent with limit L.
Let  > 0. Then there exists an N ∈ N such that for all n, m > N,
Also, since (xnk)k∈N converges to L, we can find an nK > N such that
Thus we have for all n > N that
Thus (xn)n∈N is also convergent with limit L.
Example 1.19. (Q is not complete). Consider the sequence (xn)n∈N in Q defined
by x1 = 3/2, and for n > 1, recursively by
Then it can be shown by induction that (xn)n∈N is bounded below by √2, and that
(xn)n∈N is monotone decreasing.
(A) xn  √2 or all n.
If n = 1, then x1 =   √2 (as   2). If xn  √2 or some n, then

So this gives, since xn+1  0, that xn+1  √2, and the claim follows.
(B) xn  xn+1 for all n.
We have
where the last inequality follows from part (A).
So this sequence is convergent in R. Hence it is also Cauchy in R. But as each
term xn is a rational number for all n ∈ N, it follows that (xn)n∈N is also Cauchy
in Q. However, we now show that (xn)n∈N is not convergent in Q. Suppose, on
the contrary, that (xn)n∈N converges to L ∈ Q. Then from the recurrence relation,
we obtain using the Algebra of Limits that
and so L2 = 2. As L must be positive (the sequence is bounded below by √2), it
follows that L = √2. But this is a contradiction, since we know that there is no
rational number whose square is 2.
(Alternately, consider the real number c with the decimal expansion
This number c is irrational because it has a nonterminating and nonrepeating
decimal expansion. If we consider the sequence of rational numbers 0.1, 0.101,
0.101001, 0.1010010001, 0.101001000100001, ···, obtained by truncation, then
this sequence converges with limit c.)
Example 1.20.
 converges in R, as it is Cauchy: for n > m,

which can be made as small as we please by taking m large enough. We remark
that it is not yet known if the limit is rational or irrational!
The completeness of R is the basis for the completeness of other normed spaces,
and we’ll see this now.
Finite-dimensional normed spaces are Banach
Theorem 1.5. (Rd, || · ||2) is a Banach space.
Proof.
(Essentially, this is because R is complete, and one has d copies of R in Rd.)
Suppose that (xn)n∈N is a Cauchy sequence in Rd; xn = (xn(1), ···, xn(d)). We have 
, from which it follows that each of
the real sequences (xn
(k))n∈N, k = 1, ···, d, is Cauchy in R, and hence convergent,
with respective limits, say L(1), ···, L(d) ∈ R. So given  > 0, there exists a large
enough N such that whenever n > N, we have 
.
Set L = (L(1), ···, L(d))∈ Rd.
Then for n > N, ||xn − L||2 = 
.
Consequently, the sequence (xn)n∈N converges to L.
Corollary 1.2. (C, | · |) is a Banach space.
Proof. This follows from the fact that (R2, || · ||2) is a Banach space.
Exercise 1.27. (∗)16 (Equivalent norms).
Let X be a vector space, and let || · ||a, || · ||b be norms on X. || · ||a is said to be

equivalent to || · ||b, denoted by || · ||a ~ || · ||b, if there exist positive constants m
and M such that m||x||b  ||x||a  M ||x||b.
(1) Show that ~ defines an equivalence relation on the set of all norms on X.
(2) Prove for equivalent norms on X, their respective collections of open sets,
convergent sequences, and Cauchy sequences coincide.
One can show that all norms are equivalent on Rd as follows. (It follows from
here that all finite dimensional normed spaces are Banach since Rd is complete!)
In view of the fact that ~ is an equivalence relation, it is enough to show that any
norm || · || ~ || · ||2, the Euclidean norm. We do this in three steps:
Step 1. First we will show that there is a positive M such that ||x||  M||x||2 for all
x ∈ Rd. Let e1, ···, ed be the standard basis in Rd. Then every x ∈ Rd can be
decomposed uniquely as x = x1e1 + ··· + xded, where x1, ···, xd are scalars. So ||x||
= ||x1e1 + ··· + xded||  |x1| ||e1|| + ··· + |xd| ||ed|| (using (N2) and (N3))
Step 2. Let K := {y ∈ Rd : ||y||2 = 1}. Then K is a compact set in the || · ||2 norm
topology since it is closed and bounded. The map || · || : K → R is continuous
from (K, || · ||2) to (R, | · |): ∀y1, y2 ∈ K, | ||y1|| – ||y2| ||  ||y1 – y2||  M||y1 – y2||2.
By Weierstrass’s Theorem, || · || : K → R attains a minimum value m on K. But
this m can’t be zero, since if ||y|| = 0, then y = 0 ∉ K. So this m ought to be
positive. Conclusion: ||y||  m for all y’s with ||y||2 = 1.
Step 3. Now we will show that m||x||2  ||x|| for all x ∈ Rn. This is obvious if x =
0, since both sides of the inequality are zero in this case.
If x ≠ 0, then y := x/||x||2 satisfies ||y||2 = 1, so that y ∈ K.
Thus m  ||y|| = ||x/||x||2|| = ||x||/||x||2. Rearranging, we obtain m||x||2  ||x||.
So we’ve shown that for all x ∈ Rn, m||x||2  ||x||  M||x||2, that is, || · || ~ || · ||2.
(C[a, b], || · ||∞) is a Banach space
The following theorem is an important result, and lies at the core of several

results, for example the result on the existence of solutions for Ordinary
Differential Equations (ODEs).
Theorem 1.6. (C[a, b], || · ||∞) is a Banach space.
Proof. The idea behind the proof is similar to the proof of the completeness of
Rd. If (xn)n∈N is a Cauchy sequence, then we think of the xn(t) as being the
“components” of xn indexed by t ∈ [a, b]. We first freeze a t ∈ [a, b], and show
that (xn(t))n∈N is a Cauchy sequence in R, and hence convergent to a number
(which depends on t), and which we denote by x(t). Next we show that the
function t  x(t) is continuous, and finally that (xn)n∈N does converge to x in the
supremum norm.
Let (xn)n∈N be a Cauchy sequence. Let t ∈ [a, b]. We claim that (xn(t))n∈N is a
Cauchy sequence in R. Let  > 0. Then there exists an N ∈ N such that for all n,
m > N, ||xn − xm||∞ < . But
for n, m > N. This shows that indeed (xn(t))n∈N is a Cauchy sequence in R. But
R is complete, and so the Cauchy sequence (xn(t))n∈N is in fact convergent, with
a limit which depends on which t ∈ [a, b] we had frozen at the outset. To
highlight this dependence on t, we denote the limit of (xn(t))n∈N by x(t). (Thus
for example x(a) is the number which is the limit of the convergent sequence
(xn(a))n∈N, x(b) is the number which is the limit of the convergent sequence
(xn(b))n∈N, and so on.) So we have a function
We call this function x. This will serve as the limit of the sequence (xn)n∈N. But

first we have to see if it belongs to C[a, b], that is, we need to check that this x is
continuous on [a, b].
Let t ∈ [a, b]. We will show that x is continuous at t. Recall that in order to
do this, we have to show that for each  > 0, there exists a δ > 0 such that
whenever |τ − t| < δ, we have |x(τ) – x(t)| < . Let  > 0.
Choose N large enough so that for all n, m > N, ||xn − xm||∞ < /3.
Let τ ∈ [a, b]. Then for n > N, |xn(τ) – xN+1(τ)|  ||xn − xN+1||∞ < /3.
Now let n → ∞: |x(τ) – xN+1(τ)| = 
 |xn(τ) – xN+1(τ)|  /3.
The choice of τ ∈ [a, b] was arbitrary, and so for all τ ∈ [a, b]
Now xN+1 ∈ C[a, b]. So there exists a δ > 0 such that whenever |τ − t| < δ,
Thus whenever |τ − t| < δ, we have
This shows that x is continuous at t. As the choice of t ∈ [a, b] was arbitrary, x is
continuous on [a, b].
Finally, we show that (xn)n∈N does converge to x. Let  > 0. Choose N large
enough so that for all n, m > N, ||xn − xm||∞ < . Fix n > N. Let t ∈ [a, b]. Then
for all m > N, |xn(t) − xm(t)|  ||xn − xm||∞ < . Thus
But t ∈ [a, b] was arbitrary. Hence ||xn − x||∞ = 
 |xn(t) − x(t)|  .
But we could have fixed any n > N at the outset and obtained the same result. So
for all n > N, ||xn – x||∞  . Thus 
 xn = x in (C[a, b], || · ||∞).
This completes the proof.
Example 1.21. (C[a, b] is not a a Banach space with the || · ||2-norm.)
We will work with [a, b] = [0, 2] for computational ease. Consider the sequence
(xn)n∈N in C[0, 2], where xn has a graph as shown below.

(xn)n∈N is a Cauchy sequence in (C[0, 2], || · ||2): indeed, for n > m,
Suppose that (xn)n∈N converges to x ∈ C[0, 2] in (C[0, 2], || · ||2). Then:
As x ∈ C[0, 1], 
 implies that x(t) = 0 for t ∈ [0, 1].
Let N ∈ N. Then for all n > N,
and so 
 As x ∈ C[1 + , 2], this implies

Since N ∈ N was arbitrary, it follows that x(t) = 1 for all t ∈ (0, 1].
Conclusion:
But then x ∉ C[0, 2] (as it has a discontinuity at t = 1), a contradiction.
The following is an instance where one uses “Cauchyness 
 convergence” in
Banach spaces.
Theorem 1.7. In a Banach space, absolutely convergent series converge, that is:
If(xn)n∈N is a sequence in a Banach space(X, || · ||) such that 
 < ∞, then 
 converges in X. Moreover, 
Proof. Let sn = x1 + ··· + xn, n ∈ N. We want to show that 
 converges, that
is, the sequence (sn)n∈N of partial sums converges in X.
As X is a Banach space, it is enough to show that (sn)n∈N is Cauchy.
We are given that the real series 
 converges, that is its sequence (σn)n∈N of
partial sums converges, where σn = ||x1|| + ··· + ||xn||, n ∈ N.
In particular, (σn)n∈N is Cauchy. For n > m,

and this can be made as small as we please for all n > m > N with a large enough
N. (The rightmost equality above follows from the leftmost inequality.) Thus
(sn)n∈N is Cauchy in X, and hence convergent in X (as X is a Banach space), to,
say, L ∈ X. Let  > 0. Then there exists an n such that ||sn − L|| < . Thus
As the choice of  > 0 was arbitrary, 
Example 1.22. 
 converges in (C[0, 2π], || · ||∞).
(Here sin(n·) means the function t  sin(nt) : [0, 2π] → R.)
Indeed, we have 
 and 
So 
 defines a continuous function on [0, 2π].
We can get a good idea of the limit by computing the first N terms (with a large
enough N) and plotting the resulting function; the error can then be bounded as
follows:
For example, if N = 100, then the error is bounded above by
Using Maple, we have plotted the partial sum of x with N = 100.
Thus the sum converges to a continuous function that lies in the strip of width
0.01 around the graph shown in the figure.
Later on, we will use this theorem to show that eA converges, where A belongs to

CL(X). Here CL(X) denotes a certain Banach space, namely the space of all
“continuous linear transformations” from X to itself, with the “operator norm”.
For example, when X = Rd, CL(X) turns out to be the space of all square d × d
real matrices. Why fuss over eA? The answer is that it plays a role in differential
equations: the initial value problem
has the unique solution x(t) = etAx0, t ∈ R.
Also, using the fact that (C[a, b], || · ||∞) is a Banach space, one can show the
Fundamental Theorem of Ordinary Differential Equations (ODEs):
Theorem 1.8. (Existence and Uniqueness of ODEs).
If there exists an r > 0 and an L > 0 such that f : R × R → R satisfies
then for all x0 ∈ R, there exists a T > 0 and there exists an x ∈ C1[0, T] solving
the Initial Value Problem
on [0, T], and moreover (IVP) has a unique solution.
Condition (L) on f is expressed as: f is “Lipschitz in x, uniformly in t”.
Proof. (Uniqueness) Let x1, x2 be two solutions to (IVP) on [0, T] for some T >
0. Let 
Then

So 
Let17 
 and 
Note that 
. Then for all 
Thus
and so 
, that is, N  1, a contradiction. This shows the uniqueness.
(Existence) We will write down a sequence of recursively defined functions,
which are not solutions, but serve as “good approximations”:
We will show that (xn)n
0 converges to x in 
 and this x solves
(IVP)! (So, in particular, we’ll take 
.)

We note that xn+1 = 
Also, for 0  t  
,
Thus 
 and so 
.
Hence 
So 
 converges in (C[0, T], || · ||∞), to, say, x ∈ C[0, T].
We know that 
Passing the limit as n → ∞, we have (see the explanation below):
(Here’s the justification. Define the continuous gn(n = 0, 1, 2, 3, ···) by:
Then the sequence g0, g1, g2, ··· is the sequence of partial sums of the series
We have

So (1.12) converges absolutely to some g in (C[0, T], || · ||∞). We have
We’ll now use the fact that if 
 converges to f in (C[a, b], || · ||∞), then
and this is precisely the content of Exercise 2.14 on page 73, which will be dealt
with after discussing continuity of linear transformations. Using this,
that is, we have proved (1.11).)
Thus x(0) = x0 + 0 = x0, and by the Fundamental Theorem of Calculus, x′(t) = 0
+ f(x(t), t) for all t ∈ [0, T].
Exercise 1.28. (Nonuniqueness when non-Lipschitz).
(1) Let f(x) := 
, x ∈ R. Show that f is not Lipschitz, that is, there is no
constant L > 0 such that for all x, y ∈ R, |f(x) − f(y)|  L|x − y|.
(2) Check that x1  0 and x2(t) = t2/4 are solutions to the Initial Value Problem 
(ℓp, || ·||p) are Banach spaces
Theorem 1.9. Let 1  p  +∞. Then(ℓp, || · ||p) is a Banach space.
Proof. We had already seen that ℓp is a vector space, and the fact that || · ||p
defines a norm will be established in Exercise 1.35 (page 44). We must now
show that ℓp is complete. Let (xn)n∈N be a Cauchy sequence in ℓp. Denote the kth

term of xn by 
. The proof will be carried out in 3 steps.
Step 1. We have 
 and so 
 is a Cauchy sequence in
K (= R or C), and consequently, it is convergent, with limit, say, x(k). Set x =
(x(k))k∈N.
Step 2. We show that x belongs to ℓp. Let  > 0. Then there exists an N ∈ N such
that for all n, m > N, ||xn − xm||p < . Fix any n > N.
If K ∈ N, then for p < ∞, 
Passing the limit as m goes to ∞ yields 
As the choice of K was arbitrary,
So xn − x belongs to ℓp. But xn ∈ ℓp. Hence (x − xn) + xn = x ∈ ℓp too. The p = ∞
case can be seen as follows. Fix n > N and k ∈ N. Then for all m > N, 
. Passing the limit as m goes to ∞ yields 
. As k was
arbitrary,
that is, xn − x belongs to ℓ∞. As xn ∈ ℓ∞, it now follows that x ∈ ℓ∞ too.
Step 3. Finally, we’ll show that (xn)n∈N converges to x. In the case when p < ∞,
proceeding as in Step 2, (1.13) gives for all n > N, ||xn − x||p  .
When p = ∞, (1.14) gives ||xn − x||∞   for all n > N.
Exercise 1.29. (Characterisation of closed sets).
Let X be a normed space and F be a subset of X. Show that the following two
statements are equivalent:
(1) F is closed.
(2) For every sequence (xn)n∈N in F ( n ∈ N, xn ∈ F), which is convergent in X
with limit x ∈ X, we have that x ∈ F.
Exercise 1.30. Show that c00, the set of all sequences with compact support (that

is sequences which have all terms equal to zero eventually), is a subspace of ℓ2
which is not closed.
Exercise 1.31. (Closure of a set).
Let X be a normed space and S be a subset of X. A point L ∈ X is a limit point of
S if there exists a sequence (xn)n∈N in S\{L} with limit L. The set consisting of
all points and limit points of S is denoted by S, and is called the closure of S.
(1) Prove that S is the smallest closed set which contains S.
(2) Show that if Y is a subspace of X, then Y is also a subspace of X.
(3) Prove that if C is a convex subset of X, then C is also convex.
(4) Show that a subset D of X is dense if and only if D = X.
Exercise 1.32. Show that ℓ1  ℓ2.
Is ℓ1 a Banach space with the topology induced from ℓ2?
Exercise 1.33. Let c0 be the set if all sequences convergent with limit 0. Then c0
is a subspace of the normed space ℓ∞. Prove that c0 is a Banach space.
Exercise 1.34. Let (X, || · ||) be a normed space, and let (xn)n∈N be a convergent
sequence in X with limit x. Prove that (||xn||)n∈N is a convergent sequence in R
and that 
.
Exercise 1.35. Show that if 1  p  ∞, then ℓp is a normed space.
Exercise 1.36. Show that (C1[a, b], || · ||1,∞) is a Banach space.
Exercise 1.37. (∗) We have seen that if X is a Banach space, then every
absolutely convergent series is convergent. The aim of this exercise is to show
the converse. That is, prove that if X is a normed space with the property that
every absolutely convergent series converges, then X is a Banach space. Hint:
Construct a subsequence (xnk)k∈N of a given Cauchy sequence (xn)n∈N possessing
the property that if n > nk, then ||xn − xnk|| < 1/2k. Define u1 = xn1, uk+1 =xnk+1 − xnk,
k ∈ N, and consider the series with terms uk.
Exercise 1.38. (Finite product of normed spaces).
If X, Y are normed spaces, then X × Y is a vector space with component-wise

operations. Show that ||(x, y)|| := max{||x||, ||y||}, (x, y) ∈ X × Y, defines a norm
on X × Y. Prove that if X, Y are Banach, then so is X × Y.
1.5 Compact sets
In this section, we study an important class of subsets of a normed space, called
compact sets. Before we learn the definition, let us give some motivation for this
concept.
Of the different types of intervals in R, perhaps the most important are those
of the form [a, b], where a, b are finite real numbers. Why are such intervals so
important? We know of an important result, the Extreme Value Theorem18 ,
where such intervals play a vital role. recall that the Extreme Value Theorem
asserts that any continuous function f : [a, b] → R attains a maximum and a
minimum value on [a, b]. This result does not hold in general for continuous
functions f : I → R with I = (a, b) or I = [a, b) or I = (a, ∞), and so on. Besides
its theoretical importance in Analysis, the Extreme Value Theorem is also a
fundamental result in Optimisation Theory. It turns out that when we want to
generalise this result, the notion of “compact sets” is pertinent, and later on, we
will learn the following analogue of the Extreme Value Theorem: If K is a
compact subset of a normed space X and f : K → R is continuous, then f assumes
a maximum and a minimum on K. Here is the definition of a compact set.
Definition 1.9. (Compact set). Let (X, || · ||) be a normed space. A subset K of X
is said to be compact if every sequence in K has a convergent subsequence with
limit in K, that is, if (xn)n∈N is a sequence such that xn ∈ K for each n ∈ N, then
there exists a subsequence (xnk)k∈N which converges to some L ∈ K.
Example 1.23. (Compact intervals in R). The interval [a, b] is a compact subset
of R. Indeed, every sequence (an)n∈N contained in [a, b] is bounded, and thus by
the Bolzano-Weierstrass Theorem, possesses a convergent subsequence, say
(ank)k∈N, with limit L. But since a  ank  b, for all k’s, by letting k → ∞, we
obtain a  L  b, that is, L ∈ [a, b]. Hence [a, b] is compact.
On the other hand, (a, b) is not compact, since the sequence

is contained in (a, b), but it has no convergent subsequence whose limit belongs
to (a, b). Indeed this is because the sequence is convergent, with limit a, and so
every subsequence of this sequence is also convergent with limit a, which
doesn’t belong to (a, b).
R is not compact since the sequence (n)n∈N cannot have a convergent
subsequence. Indeed, if such a convergent subsequence existed, it would also be
Cauchy, but the distance between any two distinct terms, being distinct integers,
is at least 1, contradicting the Cauchyness.
In the above list of nonexamples, note that R is not bounded, and that (a, b) is
not closed. On the other hand, in the example [a, b], we see that [a, b] is both
bounded and closed. It turns out that in Rd, having the property “closed and
bounded” is a characterisation of compact sets, and we will show this below.
Theorem 1.10.
A subset K of Rd is compact if and only if K is closed and bounded.
Before showing this, we prove a technical result, which besides being interesting
on its own, will also somewhat simplify the proof of the above theorem.
Lemma 1.4. Every bounded sequence in Rd has convergent subsequence.
Proof. As all norms on Rd are equivalent, it suffices to work with the || · ||2
norm. We prove this using induction on d. Let us consider the case when d = 1.
Then the statement is precisely the Bolzano-Weierstrass Theorem!
Suppose that the result has been proved in Rd for a d  1. We’ll show that it
holds in Rd+1. Let (xn)n∈N be a bounded sequence. We split each xn into its first
d components and its last component in R, and write xn = (αn, βn), where αn ∈
Rd and βn ∈ R. Since 
 we see that (αn)n∈N is a
bounded sequence in Rd. By the induction hypothesis, it has a convergent
subsequence, say (αnk)k∈N which converges to, say α ∈ Rd. Now consider the
sequence (βnk)k∈N in R. Then (βnk)k∈N is bounded, and so by the Bolzano-
Weierstrass Theorem, it has a convergent subsequence (βnkℓ)ℓ∈N, with limit, say β
∈ R. Then we have

So the bounded sequence (xn)n∈N has (xnkℓ)ℓ∈N as a convergent subsequence.
Also, we note that the “only if ” part of Theorem 1.10 holds in all normed
spaces.
Proposition 1.2. Let(X, || · ||) be a normed space, and K ⊂ X be compact. Then K
must be closed and bounded.
Proof.
(K is closed:) Let (xn)n∈N be a sequence in K that converges to L. Then there is a
convergent subsequence, say (xnk)k∈N that is convergent to a limit L′ ∈ K. But as
(xnk)k∈N is a subsequence of a convergent sequence with limit L, it is also
convergent to L. By the uniqueness of limits, L = L′ ∈ K. Thus K is closed.
(K is bounded:) Suppose that K is not bounded. Then given any n ∈ N, we can
find an xn ∈ K such that ||xn} > n. But this implies that no subsequence of
(xn)n∈N is bounded. So no subsequence of (xn)n∈N can be convergent either. This
contradicts the compactness of K. Thus our assumption was incorrect, that is, K
is bounded.
Now we return to the task of proving of Theorem 1.10.
Proof. It remains to just prove the “if ” part. Let K be closed and bounded. Let
(xn)n∈N be a sequence in K. Then (xn)n∈N is bounded, and so it has a convergent
subsequence, with limit L. But since K is closed and since each term of the
sequence belongs to K, it follows that L ∈ K. Consequently, K is compact.
Example 1.24. If a, b ∈ R and a < b, then the intervals (a, b], [a, b) are not
compact in R, since although they are bounded, they are not closed.
The intervals (−∞, b], [a, ∞) are not compact, since although they are closed,
they are not bounded.
Let us consider an interesting compact subset of the real line, called the Cantor

set.
Example 1.25. (Cantor set). The Cantor set is constructed as follows. Let F1 :=
[0, 1], and delete from F1 the open interval 
 which is its middle third, and
denote the remaining set by F2. Thus we have that 
 Next, delete
from F2 the middle thirds of its two pieces, namely the open intervals 
 and 
 and denote the remaining set by F3. It can be checked that 
 Continuing this process, that is, at each stage
deleting the open middle third of each interval remaining from the previous
stage, we obtain a sequence of sets Fn, each of which contains all of its
successors.
The Cantor set is defined by 
C is contained in [0, 1], and consists of those points in the interval [0, 1] which
“ultimately 
remain” 
after 
the 
removal 
of 
all 
the 
open 
intervals 
 What points do remain? C clearly contains the end-points
of the intervals which make up each set Fn:
Does C contain any other points? Actually, C contains many more points than
the above list of end points. After all, the above list of endpoints is countable,
but it can be shown that C is uncountable, see Example 7.1 on page 362.
As C is an intersection of closed sets, it is closed. Moreover it is contained in
[0, 1] and so it is also bounded. Consequently it is compact. (It turns out that the
Cantor set is a very intricate mathematical object, and is often a source of
interesting examples/counterexamples in Analysis. For example, it can be shown
that the Lebesgue measure of C is 0, and so C is an example of an uncountable
set with Lebesgue measure 0; see Example 7.1 on page 362.)
Remark 1.3. Since all norms on a finite dimensional normed space are
equivalent, we have the following consequence of Theorem 1.10.

Corollary 1.3. Let X be a finite dimensional normed space.
A subset K ⊂ X is compact if and only if K is closed and bounded.
However, in infinite dimensional normed spaces, although compact sets continue
to be necessarily closed and bounded, it turns out that closed and bounded sets
may fail to be compact. We give two examples below, the closed unit ball in ℓ2
(Example 1.26) and in C[0, 1] (Example 1.28).
Example 1.26. (The closed unit ball in ℓ2 is not compact.)
Consider the closed unit ball with centre 0 in the normed space ℓ2:
Then K is bounded, it is closed (since its complement is easily seen to be open),
but K is not compact, as shown below.
For example, take the sequence (en)n∈N, where en is the sequence with only
the nth term equal to 1, and all other terms are equal to 0:
Then this sequence (en)n∈N in K ⊂ ℓ2 can have no convergent subsequence.
Indeed, whenever n ≠ m, ||en − em||2 = 
, and so any subsequence of (en)n∈N
must be non-Cauchy, and hence also not convergent!
Example 1.27. (∗)(The Hilbert cube in ℓ2 is compact.)
Let C denote the set of all real sequences (xn)n∈N, whose nth term satisfies 0  xn
 1/n for all n ∈ N. Then it is clear that C is a subset of the real vector space ℓ2.
It can be shown that C is a compact subset of ℓ2, and we include a proof below,
even though it is somewhat technical. The proof relies on creating subsequences
of subsequences, and eventually using a “diagonal” sequence created in this
process. We will also use a similar process in the proof of Theorem 5.4 (page
213) in Chapter 5. If the reader so wishes, he/she can skip the proof below for
now, and move on to Example 1.28.
Let (xm)m∈N be a sequence in C. The task is to produce a subsequence of this
sequence which converges in ℓ2 to an x in C. Let 
 Then for all n
and m, 
In particular, 
 and as [0, 1] is compact, there is a

subsequence m1(1), m1(2), m1(3), ··· of 1, 2, 3, ··· such that 
 is
convergent, with limit, say, x(1) ∈ [0, 1].
Now 
 and as [0, 1/2] is compact, there is a subsequence
(m2(j))j 2 of (m1(j))j 2 such that 
 is convergent, with limit, say, x(2) ∈
[0, 1/2].
Proceeding in this manner, we get for all ℓ that there is a subsequence (mℓ(j))j ℓ
of (mℓ−1(j))j ℓ such that 
 is convergent, with limit x(ℓ) ∈ [0, 1/ℓ]. We
claim that 
 converges in ℓ2 to the sequence 
. See the
schematic diagram below.
First, let us note that x ∈ C because for all n ∈ N, 0  x(n)  1/n.
Now the plan is to show that ||xmj(j) − x||2 is small for all js sufficiently large. To
do this, we will split this quantity into two parts, and estimate them separately:
Let us see how to handle the second summand on the right-hand side above.
Let  > 0. Let N be such that 
 Then we have
Having accomplished this, let us now estimate the first summand in (1.15).
For all 
. As (mj(j))j N is a subsequence of (mN(j))j N, and
hence also of each (mn(j))j n for n  N, it follows that for all 
So we can find a J such that for all 
 Thus

As  > 0 was arbitrary, we have indeed shown that (xmj(j))j∈N converges in ℓ2 to x
∈ C. This completes the proof of the compactness of the Hilbert cube C.
Example 1.28. (The closed unit ball in (C[0, 1], || · ||∞) is not compact.) Consider
the closed unit ball with centre 0 in (C[0, 1], || · ||∞):
Then K is bounded, and also it is closed (since its complement is open). But K is
not compact, and this can be demonstrated by considering the sequence (xn)n∈N,
where the graphs of the xns have “narrowing” tents of height 1, with the supports
of the tents moving to the right, on half of each remaining interval, as shown in
the following picture:
Then this sequence does not have a convergent subsequence, since if it did, then
the convergent subsequence would be Cauchy, but whenever n ≠ m, ||xn − xm||∞ =
1, a contradiction to the Cauchyness.
Exercise 1.39. Let K be a compact subset of Rd (with the Euclidean norm || · ||2),
and F ⊂ Rd be a closed subset. Show that F  K is compact in Rd.
Exercise 1.40. Show that the unit sphere Sd−1 := {x ∈ Rd : ||x||2 = 1} with centre
0 in Rd, is compact in Rd.
Exercise 1.41. (∗)
Consider the normed space (R2×2, || · ||∞) from Exercise 1.13, page 17.
(1) Show that the set O(2) := {R ∈ R2×2 : R R = I} of orthogonal matrices is a
compact set.

(2) Is the indefinite orthogonal group O(1, 1) := {R ∈ R2×2 : R  JR = J}, 
where 
 also compact? 
Hint: Consider “hyperbolic rotations” 
Exercise 1.42. Show that 
 is compact in R.
Remark 1.4. (Definition of compactness.) The notion of a compact set that we
have defined is really sequential compactness. In the context of the more general
topological spaces, one defines the notion of compactness as follows.
Definition 1.10. Let X be a topological space with the topology given by the
family of open sets O. Let K ⊂ X. A collection C = {Ui : i ∈ I} of open sets is
said to be an open cover of K if 
K ⊂ X is said to be a compact set if every open cover of K has a finite subcover,
that is, given any open cover C = {Ui : i ∈ I} of K, there exist finitely many
indices i1, ···, in ∈ I such that K ⊂ Ui1  ···  Uin.
In the case of normed spaces, it can be shown that the set of compact sets
coincides with the set of sequentially compact sets. But in general topological
spaces, these two notions may not be the same; see for instance [Steen and
Seebach (1995)].
1 All of this notation will be explained in the course of this chapter.
2 Unless stated otherwise, the underlying field is always assumed to be R or C.
3 However, unless otherwise stated, we will always use the Euclidean norm on Rd.
4 Just like the || · ||1 and || · ||2-norms, more generally, for any p ∈ [1, ∞], one can define the || · ||p-norm
on Rd, given by ||x||p := (|x1| + ··· + |xd|, x ∈ Rd. of norms (see Exercise 1.8.(1) on page 16).
5 See for example [Sasane (2015), §3.4].
6 This is in turn useful in applications, for example to solve shortest distance problems via projections.
7 This means that we have a set I, and for each i ∈ I, there is a set Ui. The set I is referred to as the index
set, and any particular i ∈ I as the index of Ui.
8 The Archimedean Property of R says that if x, y ∈ R and x > 0, then there exists an n ∈ N such that y <
nx. See for example [Sasane (2015), page 18].
9 The notation B is after Sergei Bernstein, 1880–1968, who did fundamental work in constructive
function theory, where smoothness properties of a function are related to its approximability by
polynomials.

10 Recall that every continuous function on a compact interval is uniformly continuous there; see for
example [Sasane (2015), Proposition 3.11, page 113].
11 We remark here that every vector space can be made into a normed space. For the details, see Remark
4.3 on page 162.
12 To be studied in Chapter 5.
13 That is, the norms of the terms of the sequence form a bounded real sequence.
14 That is, either the terms are increasing, or the terms are decreasing.
15 This illustrative analogy stems from [Bryant (1990)].
16 This exercise assumes familiarity with the notion of continuity of real-valued maps on compact sets in
(Rd, || · ||2) and Weierstrass’s Theorem saying that such a map assumes a minimum value. We will prove
Weierstrass’s Theorem in Chapter 2; see page 66.
17 We will see the rationale behind these seemingly strange choice of N soon enough.
18 See for example [Sasane (2015), §3.4].

Chapter 2
Continuous and linear maps
A normed space has two structures: a linear one (the underlying vector space),
and a topological one (the norm). So when we study maps between normed
spaces, it is natural to focus on maps which are well-behaved with these
structures, and we’ll do this now. In particular, we’ll study:
(1) linear transformations 
(well-behaved with respect to the linear structure),
(2) continuous maps 
(well-behaved with respect to the topological structure),
(3) continuous linear transformations 
(well-behaved with respect to both structures).
In the context of normed spaces, continuous linear transformations are most
important, and these are sometimes also called bounded linear operators.
The reason for this terminology will become clear in Theorem 2.6 (page 67).
We’ll see that the set of all bounded linear operators is itself a vector space, with
obvious pointwise operations of addition and scalar multiplication, and it also
has a natural notion of a norm, called the operator norm. Equipped with the
operator norm, the vector space of bounded linear operators is a Banach space,
provided that the co-domain is a Banach space. This is a useful result, which we
will use in order to prove the existence of solutions to integral and differential
equations.
2.1 Linear transformations

Linear transformations are maps that respect vector space operations.
Definition 2.1. (Linear transformation).
Let X and Y be vector spaces over K (R or C).
A map T : X → Y is called a linear transformation if:
(L1) For all x1, x2 ∈ X, T(x1 + x2) = T(x1) + T(x2).
(L2) For all x ∈ X and all α ∈ K, T(α · x) = α · T(x).
Example 2.1. (Linear galore!)
(1) D : C1[a, b] → C[a, b] given by Dx = x′, x ∈ C1[a, b] is a linear
transformation, since
(L1) D(x + y) = (x + y)′ = x′ + y′ = Dx + Dy for all x, y ∈ C1[a, b];
(L2) D(αx) = (αx)′ = α · x′ for all α ∈ R and x ∈ C1[a, b].
(2) Let m, n ∈ N and X = Rn and Y = Rm.
is a linear transformation from Rn to Rm. Indeed,
and so (L1) holds. Moreover,
and so (L2) holds as well. Hence TA is a linear transformation.
(3) Let X = Y = ℓ2. Define the left/right shift operators L, R as follows: if x =
(xn)n∈N ∈ ℓ2, then

Then it is easy to see that R and L are linear transformations.
(4) Let X := c(⊂ ℓ∞), the space of all real valued convergent sequences, and Y =
R. The map L : c → R, L((an)n∈N) := 
 for (an)n∈N, is a linear
transformation (using the algebra of limits).
Recall that given a linear transformation T : X → Y, we can associate with T
two natural subspaces, of X and Y, respectively,
the kernel of T, ker T := {x ∈ X : Tx = 0Y} ⊂ X, and
the range of T, ran T := {y ∈ Y : ∃x ∈ X such that y = Tx} ⊂ Y .
In the above example of the linear transformation L, we have
ker L = c0 (set of sequences convergent with limit 0),
ran L = R (since for every r ∈ R, the constant sequence (r)n∈N)
converges to r).
(5) The map I : C[a, b] → R, given by 
 for all x ∈ C[a, b], is a
linear transformation.
(6) Let S := {h ∈ C1[a, b] : h(a) = h(b) = 0}. From Exercise 1.3 (page 7), we see
that S is a subspace of C1[a, b]. Let A, B ∈ C[a, b] be fixed functions.
Let L : S → R be given by 
Let us check that L is a linear transformation. We have:
(L1) For all h1, h2 ∈ S,

(L2) For all h ∈ C1[0, 1] and all α ∈ R,
Thus L is a linear transformation.
(7) Let C1(R), C2(R) denote the vector spaces of once, respectively twice
continuously differentiable real-valued functions in R with pointwise
operations. For f ∈ C2(R) and g ∈ C1(R), consider the initial value problem
for the one (spatial) dimensional wave equation:
Let C2(R × [0, ∞)) denote the vector space of all twice continuously
differentiable functions (x, t) 
 u(x, t) : R × [0, ∞] 
 R, again with
pointwise operations. Then it can be shown that the unique solution uf,g in
C2(R × [0, ∞)) to (IVP) is given by d’Alembert’s Formula,
Then the map (f, g) 
 uf,g : C2(R) × C1(R) → C2(R × [0, ∞)) is a linear
transformation.
Here are some non examples.
Example 2.2. (Not quite linear!)
(1) If ·∗ denotes complex conjugation, then the complex conjugation map z 
z∗ : C → C is not a linear transformation, since although (L1) is satisfied: (z
+ w)∗ = z∗ + w∗(z, w ∈ C), we see that (L2) isn’t: indeed, (i · 1)∗ = i∗ = –i ≠
i · 1∗.

(2) Consider the map T : R2 → R2 defined by
Then T is not a linear transformation since (L1) is not satisfied.
Indeed, 
while 
If α ∈ R\{0} and 
 then we have
If α = 0 and 
So for all 
 Thus (L2) holds.
Notation 2.1. We will denote the set of all linear transformations from the vector
space X to the vector space Y by L(X, Y). Recall from elementary linear algebra
that L(X, Y) is itself a vector space (over the common field K for X, Y) with
pointwise operations: if T, S ∈ L(X, Y), then we define T + S ∈ L(X, Y) by (T +
S)(x) = Tx + Sx, for all x ∈ X, and if α ∈ K and T ∈ L(X, Y), then we define α · T
∈ L(X, Y) by (α · T)(x) = α · (Tx), for all x ∈ X. What is the zero vector in this
vector space L(X, Y)? It is the “zero linear transformation” 0 : X → Y, given by
0x = 0Y, for all x ∈ X, where 0Y denotes the zero vector in Y.

If X = Y, then we write L(X) instead of L(X, X).
Exercise 2.1. Consider the two maps S1, S2 : C[0, 1] → R given by
Show that S1 is not a linear transformation, while S2 is.
Exercise 2.2. Let a, b be nonzero real numbers, and consider the two real-valued functions f1, f2 defined on
R by f1(t) = eat cos(bt) and f2(t) = eat sin(bt), t ∈ R. f1 and f2 are vectors belonging to the infinite
dimensional vector space C1(R), consisting of all continuously differentiable functions from R to R. Denote
by Sf1,f2 the span of the two functions f1 and f2.
(1) Prove that f1 and f2 are linearly independent in C1(R).
(2) Show that the differentiation map, 
 is a linear transformation.
(3) What is the matrix [D]B of D with respect to the (ordered) basis B = (f1, f2)?
(4) Prove that D is invertible, and write down the matrix corresponding to the inverse of D.
(5) Compute the indefinite integrals 
2.2 Continuous maps
Let X and Y be normed spaces. As there is a notion of distance between pairs of
vectors in either space (provided by the norm of the difference of the pair of
vectors in each respective space), one can talk about continuity of maps. Within
the huge collection of all maps, the class of continuous maps form an important
subset. Continuous maps play a prominent role in functional analysis since they
possess some useful properties.
Before discussing the case of a function between normed spaces, let us first
of all recall the notion of continuity of a function f : R → R.
Continuity of functions from R to R
In everyday speech, a ‘continuous’ process is one that proceeds without gaps of
interruptions or sudden changes. What does it mean for a function f : R → R to
be continuous? The common informal definition of this concept states that a
function f is continuous if one can sketch its graph without lifting the pencil. In

other words, the graph of f has no breaks in it. If a break does occur in the graph,
then this break will occur at some point. Thus (based on this visual view of
continuity), we first give the formal definition of the continuity of a function at a
point below. Next, if a function is continuous at each point, then it will be called
continuous. If a function has a break at a point, say x0, then even if points x are
close to x0, the points f(x) do not get close to f(x0).
This motivates the definition of continuity in calculus, which guarantees that if a
function is continuous at a point x0, then we can make f(x) as close as we like to
f(x0), by choosing x sufficiently close to x0.
Definition 2.2. A function f : R → R is continuous at x0 if for every  > 0, there
exists a δ > 0 such that for all x ∈ R satisfying |x – x0| < δ, we have that |f(x) –
f(x0)| < .
f : R → R is continuous if for every x0 ∈ R, f is continuous at x0.
Continuity of functions between normed spaces
We now define the set of continuous maps from a normed space X to a normed
space Y.
We observe that in the definition of continuity in ordinary calculus, if x, y are
real numbers, then |x–y| is a measure of the distance between them, and that the
absolute value | · | is a norm in the finite (one) dimensional normed space R. So
it is natural to define continuity in arbitrary normed spaces by simply replacing
the absolute values by the corresponding norms, since the norm provides a

notion of distance between vectors.
Definition 2.3. (Continuity of maps between normed spaces).
Let X and Y be normed spaces over K (R or C). Let x0 ∈ X. A map f : X → Y is
continuous at x0 if for every  > 0, there exists a δ > 0 such that for all x ∈ X
satisfying ||x – x0|| < δ, we have ||f(x) – f(x0)|| < . f : X → Y is continuous if for all
x0 ∈ X, f is continuous at x0.
We will soon study when linear transformations are continuous, but first let us
consider some examples of nonlinear maps.
Example 2.3. Consider the map S : C[0, 1] → R, given by
We’ll show that S is continuous. (As usual, C[0, 1] is endowed with the
supremum norm.) Suppose that x0 ∈ C[0, 1]. Let  > 0. As we would like to
make |S(x) – S(x0)| small, let us first consider this expression. We have
if ||x – x0||∞ < δ, where δ > 0 is some number. We ought to choose δ > 0 suitably
so as to make the right-hand side above smaller than . There is no unique way to
do this, and anything one can justify works. We set
Whenever ||x – x0||∞ < δ, in light of the above computation, we have

Thus S is continuous at x0. As the choice of x0 was arbitrary, it follows that S is
continuous (on C[0, 1]).
Example 2.4. c00 is the subspace of ℓ∞ of all finitely supported sequences. c00 is
a normed space with the supremum norm inherited from ℓ∞.
Consider the map s : c00 → R given by 
We’ll show that s is not continuous at 0. Suppose on the contrary that it is. With 
= 1/4 > 0, there exists a δ > 0 such that if ||a||∞ = ||a – 0||∞ < δ, then we are
guaranteed that |s(a) – s(0)| = |s(a) – 0|= |s(a)| < 
 = 1/4. If 
 then 
 So for all m
sufficiently large, we must have ||am||∞ < δ, giving in turn that |s(am)| < 1/4. But
for all m we have
a contradiction. Hence s is not continuous at 0.
Exercise 2.3. (Rationale for the C1[a, b] norm.)
This exercise concerns the norm on C1[a, b] we have chosen to use. Since we want to be able to use
ordinary analytic operations such as passage to the limit, then, given a function f : C1[0, 1] → R, it is
reasonable to choose a norm such that f is continuous. As our f, let us take the arc length function given by
We show in the following sequence of exercises that f is not continuous if we equip C1[0, 1] with the
supremum norm || · ||∞ induced from C[0, 1].
(1) Calculate f(0). (The arc length of the graph of the constant function taking value 0 everywhere on [0, 1]
is obviously 1, and check that the above formula delivers this.)
(2) Now consider 
 Using 
 for all 
 and the
periodicity of sin(2πnt) (the graph of sin(2πnt) on 
 is repeated n times in [0, 1]), conclude that

(3) Show that f is not continuous at 0. (Prove this by contradiction. Note that by taking larger and larger n,
||xn – 0||∞ can be made as small as we please, but f(xn) doesn’t stay close to f(0).)
Show that the arc length function f is continuous if we equip C1[0, 1] with the norm || · ||1,8. It may be useful
to note that by using the triangle inequality in (R2, || · ||2), we have for a, b ∈ R that
Exercise 2.4. Let (X, || · ||) be a normed space. Show that the norm || · || : X → R is a continuous map.
Continuity and open sets
We’ll now learn an important property of continuous maps:
“inverse images” of open sets under a continuous map are open.
In fact, we shall see that this property is a characterisation of continuity. First
let’s some notation. Let f : X → Y be a map between the normed spaces X and Y,
and let V ⊂ Y. We set f–1(V) := {x ∈ X : f(x) ∈ V}, and call it the inverse image
of V under f. Clearly f–1(Y) = X and f–1(∅) = ∅.
Exercise 2.5. Let f : R → R be given by f(x) = cos x(x ∈ R).
Find f–1(V), where V = {–1, 1}, V = {1}, V = [–1, 1], V = R, 
On the other hand if U ⊂ X, then we set f(U) := {f(x) ∈ Y : x ∈ U}, and call it the image of U under f.

Exercise 2.6. Let f : R → R be given by f(x) = cos x (x ∈ R).
Find f(U), where U = R, U = [0, 2π], U = [δ, δ + 2π] where δ > 0.
Theorem 2.1. Let X, Y be normed spaces and f : X → Y be a map.
Then f is continuous on X if and only if
for every V open in Y, f–1(V) is open in X.
Proof.
(If) Let c ∈ X, and let  > 0. Consider the open ball B(f(c), ) with center f(c) and
radius  in Y . We know that this open ball V := B(f(c), ) is an open set in Y. Thus
we also know that f–1(V) = f–1(B(f(c), )) is an open set in X. But the point c ∈ f–
1(B(f(c), )), because f(c) ∈ B(f(c), ) (||f(c), f(c)|| = 0 < !). So by the definition of
an open set, there is a δ > 0 such that B(c, δ) ⊂ f–1(B(f(c), )). In other words,
whenever x ∈ X satisfies ||x – c|| < δ, we have x ∈ f–1(B(f(c), )), that is, f(x) ∈
B(f(c), ), which implies ||f(x) – f(c)|| < . Hence f is continuous at c. But the
choice of c ∈ X was arbitrary. Consequently f is continuous on X. See the picture
on the left.
(Only if) Now let f be continuous, and let V be an open subset of Y. We would
like to show that f–1(V) is open. So let c ∈ f–1(V. Then f(c) ∈ V. As V is open,
there is a small open ball B(f(c), ) with center f(c) and radius  that is contained
in V. By the continuity of f at c, there is a δ > 0 such that whenever ||x – c|| < δ,
we have ||f(x) – f(c)|| < , that is, f(x) ∈ V. But this means that B(c, δ) ⊂ f–1(V).
Indeed, if x ∈ B(c, δ), then ||x – c|| < δ and so by the above, f(x) ∈ V, that is, x ∈
f–1(V). Consequently, f–1(V) is open in X. See the picture on the right above.
 Note that the theorem does not claim that for every U open in X, f(U) is open
in Y. Consider for example X = Y = R equipped with the Euclidean norm, and the
constant function f(x) = c (x ∈ R), which is clearly continuous. But note that
direct images of open sets are not always open under f : indeed X = R is open in

X = R, but f(X) = {c} is not open in Y = R.
Corollary 2.1. Let X, Y be normed spaces and f : X → Y be a map.
Then f is continuous on X if and only if
for every F closed in Y, f–1(F) is closed in X.
Proof. If F ⊂ Y, then f–1(Y\F) = X\(f–1(F)).
Exercise 2.7. Fill in the details of the proof of Corollary 2.1.
Theorem 2.2. Let X, Y, Z be normed spaces, and f : X → Y, g : Y → Z be
continuous maps. Then the composition map g  f : X → Z, defined by (g  f)(x)
:= g(f(x)) (x ∈ X), is continuous.
Proof. Let W be open in Z. Then since g is continuous, g–1(W) is open in Y. Also,
since f is continuous, f–1(g–1(W)) is open in X. Finally, we note that (g  f)–1(W) =
f–1(g–1(W)). So g  f is continuous.
Exercise 2.8. In the proof of Theorem 2.2, we used (g  f)–1(W) = f–1(g–1(W)). Check this.
Exercise 2.9. Let X be a normed space and f : X → R be a continuous map. Determine if the following
statements are true or false.
(1) {x ∈ X : f(x) < 1} is an open set.
(2) {x ∈ X : f(x) > 1} is an open set.
(3) {x ∈ X : f(x) = 1} is an open set.
(4) {x ∈ X : f(x)  1} is a closed set.
(5) {x ∈ X : f(x) = 1} is a closed set.
(6) {x ∈ X : f(x) = 1 or f(x) = 2} is a closed set.
(7) {x ∈ X : f(x) = 1} is a compact set.
Continuity and convergence
We have the following characterisation of continuous maps in terms of
convergence of sequences: “Continuous maps preserve convergent sequences”.
Theorem 2.3. Let X, Y be normed spaces, c ∈ X, and let f : X → Y. Then the
following two statements are equivalent:
(1) f is continuous at c.
(2) For every sequence (xn)n∈N in X such that (xn)n∈N converges to c, (f(xn))n∈N
converges to f(c).

Proof.
(1) ⇒ (2): Suppose that f is continuous at c. Let (xn)n∈N be a sequence in X such
that (xn)n∈N converges to c. Let  > 0. Then there exists a δ > 0 such that for all x
∈ X satisfying ||x – c|| < δ, we have ||f(x) – f(c)|| < . As the sequence (xn)n∈N
converges to c, for this δ > 0, there exists an N ∈ N such that whenever n > N,
||xn – c|| < δ. But then by the above, ||f(xn) – f(c)|| < . So we have shown that for
every  > 0, there is an N ∈ N such that for all n > N, ||f(xn) – f(c)|| < . In other
words, the sequence (f(xn))n∈N converges to f(c).
(2) ⇒ (1): Suppose that f is not continuous at c. Thus there is an  > 0 such that
for every δ > 0, there is an x ∈ X such that ||x – c|| < δ, but ||f(x) – f(c)|| > . We
will use this statement to construct a sequence (xn)n∈N for which the conclusion
in (2) does not hold. Let δ = 1/n, for n ∈ N, and denote a corresponding x as xn:
thus, ||xn – c|| < δ = 1/n, but ||f(xn) – f(c)|| > . Clearly the sequence (xn)n∈N is
convergent with limit c, but (f(xn))n∈N does not converge to f(c) since ||f(xn) –
f(c)|| >  for all n ∈ N. Consequently if (1) does not hold, then (2) does not hold.
In other words, we have shown that (2) ⇒ (1).
Exercise 2.10. Let X, Y be normed spaces. Find all continuous maps f : X → Y such that for all x ∈ X, f(x) +
f(2x) = 0. Hint: 
Exercise 2.11. (∗)(Continuity of the determinant; {invertible matrices} is open). Show that the determinant
M → det M : (Rn×n, || · ||∞) → (R, | · |) is continuous. Prove that the set of invertible matrices is open in
(Rn×n, || · ||∞). Hint: det–1{0}.
Continuity and compactness
In this section we will learn about a very useful result in Optimisation Theory, on
the existence of global minimisers of real-valued continuous functions on
compact sets.
Theorem 2.4.
If (1) K is a compact subset of a normed space X,
(2) Y is a normed space, and
(3) f : X → Y is function that is continuous at each x ∈ K, 
then f(K) is a compact subset of Y.

Proof. Suppose that (yn)n∈N is a sequence contained in f(K). Then for each n ∈
N, there exists an xn ∈ K such that yn = f(xn). Thus we obtain a sequence (xn)n∈N
in the set K. As K is compact, there exists a convergent subsequence, say
(xnk)k∈N, with limit L ∈ K. As f is continuous, it preserves convergent sequences.
So (f(xnk))k∈N = (ynk)k∈N is convergent with limit f(L) ∈ f(K). Consequently, f(K)
is compact.
Now we prove the aforementioned result which turns out to be very useful in
Optimisation Theory, namely that a real-valued continuous function on a
compact set attains its maximum/minimum on the compact set. This is a
generalisation of the Extreme Value Theorem we had learnt earlier, where the
compact set in question was just the interval [a, b].
Theorem 2.5. (Weierstrass).
If (1) K is a nonempty compact subset of a normed space X, and
(2) f : X → R is a function that is continuous at each x ∈ K, then there exists
a c ∈ K such that f(c) = sup{f(x) : x ∈ K}.
We note that since c ∈ K, f(c) ∈ {f(x) : x ∈ K}, and so the supremum above is
actually a maximum:
Also, under the same hypothesis of the above result, there exists a minimiser in
K, that is, there exists a d ∈ K such that
This follows from the above result by just looking at –f, that is by applying the
above result to the function g : X → R given by g(x) = –f(x) (x ∈ X).
Proof. (Of Theorem 2.5.) We know that the image of K under f, namely the set
f(K) is compact and hence bounded. So {f(x) : x ∈ K} is bounded. It is also
nonempty since K is nonempty. But by the least upper bound property of R, a
nonempty bounded subset of R has a least upper bound. Thus M := sup{f(x) : x
∈ K} ∈ R. Now consider M – 1/n (n ∈ N). This number cannot be an upper
bound for {f(x) : x ∈ K}. So there must be an xn ∈ K such that f(xn) > M – 1/n. In

this manner we get a sequence (xn)n∈N in K. As K is compact, (xn)n∈N has a
convergent subsequence (xnk)k∈N with limit, say c, belonging to K. As f is
continuous, (f(xnk))k∈N is convergent as well with limit f(c). But from the
inequalities f(xn) > M – 1/n (n ∈ N), it follows that f(c)  M. On the other hand,
from the definition of M, we also have that f(c)  M. So f(c) = M.
Example 2.5. Since the set 
 is compact in R3 and
since the function x 
 x1 + x2 + x3 is continuous on R3, it follows that the
optimisation problem
has a minimiser.
Remark 2.1. (∗) In Optimisation Theory, one often meets necessary conditions
for a minimiser, that is, results of the following form:
(Here 
 are certain mathematical conditions, such as the Lagrange multiplier
equations.) Now such a result has limited use as such since even if we find all 
which satisfy 
, we can’t conclude that there is one that is a minimiser. But
now suppose that we know that f is continuous on F and that F is compact. Then
we know that a minimiser exists, and so we know that among the  that satisfy 
, there is at least one which is a minimiser.
Notation 2.2. We will denote the set of all continuous maps from the normed
space X to the normed space Y by C(X, Y).
2.3 The normed space CL(X, Y)
In this section we study those linear transformations from a normed space X to a
normed space Y that are also continuous.

Notation 2.3. We denote the set of all continuous linear transformations from the
normed space X to the normed space Y by CL(X, Y), that is, CL(X, Y) := C(X, Y)
∩ L(X, Y). If X = Y, then we denote CL(X, X) simply by CL(X).
We begin by giving a characterisation of continuous linear transformations.
When is a linear transformation continuous?
Theorem 2.6.
Let X and Y be normed spaces, and T : X → Y be a linear transformation. Then
the following properties of T are equivalent:
(1) T is continuous.
(2) T is continuous at 0.
(3) There exists an M > 0 such that for all x ∈ X, ||Tx||Y  M ||x||X.
We’ll see the proof below. But let us first remark that the useful part is the
equivalence of (1) and (3), since by just showing the existence/lack of of the
bound M, we can conclude the continuity/lack of continuity of the given linear
transformation. So we don’t have to go through the rigmarole of verifying the -δ
definition: rather, a simple estimate, as stipulated in (3), suffices. Note also that
it seems miraculous that continuity at just one point (at 0) delivers continuity
everywhere on X! This miracle happens because the map T is not any old map,
but rather a linear transformation. Here is an elementary example.
Example 2.6. (The left shift and right shift operators).
The left shift operator, L : ℓ2 → ℓ2, given by
is a linear transformation. We have for all (an)n∈N ∈ ℓ2 that
and so L ∈ C(ℓ2, ℓ2). The right shift operator R : ℓ2 → ℓ2, given by R(a1, a2, a3,
···) := (0, a1, a2, ···), (an)n∈N ∈ ℓ2, is also a linear transformation which is
continuous, thanks to the equality

for all (an)n∈N ∈ ℓ2.
Proof. (Of Theorem 2.6.) We will show the three implications (1)⇒(2), (2)⇒(3),
and (3)⇒(1), which are enough to get all the three equivalences (and six
implications) given in the statement of the theorem.
(1)⇒(2). This is just the definition of continuity on X. Indeed, T has to be
continuous at each point in X, and in particular at 0 ∈ X.
(2)⇒(3). Take  := 1 > 0. Then there exists a δ > 0 such that whenever ||x – 0|| =
||x|| < δ, we have that ||Tx – T0|| = ||Tx – 0|| = ||Tx|| < 1. Let’s check that this
yields:
First consider x = 0. Then
And so the claim in (2.2) holds because we have in fact an equality.
On the other hand, now suppose that x ≠ 0. Set 
 Then
and so ||Ty|| < 1, that is 
Upon rearranging, we obtain (2.2). So the claim in (3) holds with 
(3)⇒(1). Let M > 0 be such that for all x ∈ X, ||Tx||  M ||x||. Let x0 ∈ X, and  >
0. Set δ := /M > 0. Then whenever ||x – x0|| < δ, we have
So T is continuous at x0. But as x0 ∈ X was arbitrary, T is continuous.
Example 2.7. (Norm on C1[a, b] revisited). Consider the differentiation mapping

D : C1[0, 1] → C[0, 1] defined by (Dx)(t) = x′(t), t ∈ [0, 1], x ∈ C1[0, 1]. We had
seen that D is a linear transformation. Let’s now investigate if D is also
continuous.
(1) We will show that D is not continuous if both C1[0, 1] and C[0, 1] are
equipped with the || · ||∞ norm. Suppose on the contrary that the map D is
continuous. Because D is a linear transformation, it follows from Theorem
2.6 that there exists an M > 0 such that for all x ∈ C1[0, 1],
But if we take x = tn (n ∈ N), then we have
and so ||Dx||∞ = ||x′||∞ = n  M ||x||∞ = M · 1, that is, n  M for all n ∈ N,
which is clearly not true. So D is not continuous.
(2) However, D is continuous if C1[0, 1] is equipped with the || · ||1,∞ norm:
while C[0, 1] has the usual supremum norm || · ||∞. Indeed, we have for all x
∈ C1[0, 1], that ||Dx||∞ = ||x′||∞  ||x||∞ + ||x′||∞ = ||x||1,∞.
Example 2.8. (If X, Y are finite dimensional, then L(X, Y) = CL(X, Y).) Let X =
(Rn, || · ||2), Y = (Rm, || · ||2) and let A ∈ Rm×n be given by
Let TA : Rn → Rm be the linear transformation given by TAx := Ax for all x ∈ Rn.
Then for all x ∈ Rn,

and so 
 Hence TA is continuous.
Remark 2.2. We know that every linear transformation on finite dimensional
vector spaces X, Y can be represented by TA once bases for X, Y have been
chosen. Also we know that all norms on finite-dimensional normed spaces are
equivalent to each other. It follows from these two facts that every linear
transformation between finite dimensional normed spaces is continuous.
Example 2.9. Let 
 and let 
For x = (xj)j∈N ∈ ℓ2, set 
We claim that TA : ℓ2 → ℓ2 is a continuous linear transformation on ℓ2. Firstly,
TAx ∈ ℓ2, since
Moreover, it is easily seen that TA ∈ L(ℓ2). Moreover, by Theorem 2.6, the
computation above shows that TA ∈ CL(ℓ2).
Example 2.10. (Integral operators).
Suppose that A : [0, 1] × [0, 1] → R be such that 
We think of A as a “non-discrete/continuous” analogue of a square matrix: the
indices i, j are replaced by the “non-discrete/continuous” indices t, τ.
Then the map TA : L2[0, 1] → L2[0, 1], defined by

is a continuous linear transformation. The following picture illustrates the action
of TA on x schematically, highlighting the analogy with matrix multiplication.
We note that for x ∈ L2[0, 1],
(The inequality in the second line above, is the Cauchy-Schwarz inequality in
L2[0, 1], and it follows from the general Cauchy-Schwarz inequality in inner
product spaces, which will be shown in Theorem 4.1, page 157; see also
Example 4.3, page 159. We’ll accept this for now.) So TAx ∈ L2[0, 1], and TA ∈
CL(L2[0, 1]).
Operators TA are called integral operators. It used to be common to call the
function A that plays the role of the matrix, as the “kernel”1 of the integral
operator. Many variations of the integral operator are possible.
Example 2.11. We had seen on page 55 that with
and A, B ∈ C[a, b], the map L : S → R given by

is a linear transformation. Now we ask: is L continuous? Here we equip S ⊂
C1[0, 1] with the norm || · ||1,∞. For h ∈ S,
where 
 In the above, we have used
Hence L is continuous.
Example 2.12. (∗)(Fourier transform).
Let L1(R) be the space of all complex valued Lebesgue integrable functions on R, with the usual L1-norm:
Its Fourier transform is the function  : R → C defined by
Then  is a continuous function on R, and it is also bounded because
The vector space Cb(R) of all complex-valued continuous functions on R that are bounded, is a normed

space with the supremum norm:
(We won’t check this; the proof is analogous to Example 1.9, page 10.) Thus from the above, we have  ∈
Cb(R). It is also easy to check that 
 : L1(R) → Cb(R) is a linear transformation, and it is continuous,
thanks to the estimate above, giving || ||∞  ||f||1.
Remark 2.3. Owing to the characterisation of continuous linear transformations by the existence of a bound
as in item (3) of Theorem 2.6 above, they are sometimes called bounded linear operators.
Exercise 2.12. Show that if A ∈ Rm×n, then ker A = {x ∈ Rn : Ax = 0} is a closed subspace of Rn.
Exercise 2.13. (∗) Prove that every subspace of Rn is closed.
Hint: Construct a linear transformation whose kernel is the given subspace.
Exercise 2.14. Let C[a, b] be endowed with the || · ||∞-norm.
(1) Show that 
 is a continuous linear transformation.
(2) Prove that if 
 converges to f in C[a, b], then 
Exercise 2.15. (Convolution operator).
If f ∈ L1(R), then the corresponding convolution operator f∗ : L∞(R) → L∞(R) is given by
Show that f∗ is well-defined and that f∗ ∈ CL(L∞(R)).
Exercise 2.16. Let Y = {f ∈ L2(R) : f =  := f(–·)} be the set of all even functions in L2(R). Show that Y is a
closed subspace of L2(R).
Hint: View Y as the kernel of a suitable map in CL(L2(R)).
Operator norm and the normed space CL(X, Y)
Consider the set CL(X, Y) of all continuous linear transformations from a normed
space X to a normed space Y. We will show that CL(X, Y) is a normed space,
with pointwise operations (inherited from L(X, Y)), and the “operator norm” || · ||
: CL(X, Y) → R given by
Let us first show that CL(X, Y) is a subspace of L(X, Y), making it a vector space
in its own right.

Proposition 2.1. CL(X, Y) is a subspace of L(X, Y).
Proof. We have:
(S1) Let S, T ∈ CL(X, Y). Then there exist MS, MT > 0 such that for all x ∈ X,
||Sx||  MS ||x|| and ||Tx||  MT ||x||. So
Thus S + T ∈ CL(X, Y) too.
(S2) Let α ∈ R and T ∈ CL(X, Y). There exists an M > 0 such that for all x ∈ X,
||Tx||  M||x||. So ||(αT)x|| = ||α(Tx)|| = |α| ||Tx||  |α|M||x||. Hence αT ∈ CL(X,
Y).
(S3) The zero linear transformation 0 ∈ CL(X, Y) because for all x ∈ X, ||0x|| =
||0Y|| = 0  1 · ||x||.
Consequently, CL(X, Y) is a subspace of L(X, Y).
Next we show that the operator norm || · || : CL(X, Y) → R given by
is indeed a norm on CL(X, Y). First let us check that is a well-defined number. If
we set S := {||Tx|| : x ∈ X, ||x||  1}, then we note that this is a subset of the real
numbers. Let us observe that this is a nonempty bounded set:
(1) S ≠ ∅ because if we take x = 0X ∈ X, then ||x|| = ||0X|| = 0  1, and so ||Tx|| =
||T0X|| = ||0Y|| = 0 ∈ S.
(2) S is bounded above. As T ∈ CL(X, Y), there is an M > 0 such that for all x ∈
X, ||Tx||  M||x||. We claim that M is an upper bound of S. Indeed, if x ∈ X
and ||x||  1, then ||Tx||  M||x||  M · 1 = M.
Since S is a nonempty subset of R which is bounded above, it follows from the
Least Upper Bound Property of R that the supremum of S exists: so for all T ∈
CL(X, Y), ||T|| := sup{||Tx|| : x ∈ X, ||x||  1} < ∞. In order to do our verification
that this operator norm || · || is a norm on CL(X, Y), the following two results will
be useful.
Lemma A. Let T ∈ CL(X, Y).

If M > 0 is such that for all x ∈ X, ||Tx||  M||x|| , then ||T||  M.
Proof. If x ∈ X and ||x||  1, then ||Tx||  M||x||  M · 1 = M. So M is an upper
bound of S = {||Tx|| : x ∈ X, ||x||  1}. Thus sup S  M, that is, ||T||  M.
Lemma B. Let T ∈ CL(X, Y). Then for all x ∈ X, ||Tx||  ||T|| ||x||.
Proof.
1  x = 0. Then ||Tx|| = ||T0|| = ||0|| = 0 = ||T||0 = ||T|| ||0|| = ||T|| ||x||.
2  Suppose that 0X ≠ x ∈ X. Let 
 Then 
Thus ||Ty|| ∈ S, and so ||Ty||  sup S = ||T||, that is,
Rearranging, we get ||Tx||  ||T|| ||x||.
Lemmas A and B together tell us that for a T ∈ CL(X, Y), ||T|| is allowed as an
“M” in
and moreover it is the smallest possible such number M, in the sense that any
other allowed M has got to be at least as large as ||T||.
Theorem 2.7. The operator norm, || · || : CL(X, Y) → R, given by
is a norm on CL(X, Y).
Proof. We have:
(N1) For T ∈ CL(X, Y), 
 since ||Tx||  0 for all x.
If T ∈ CL(X, Y) and ||T|| = 0, then ||Tx||  ||T|| ||x|| = 0||x|| = 0, and so ||Tx|| =
0, that is, Tx = 0Y for all x ∈ X. So T = 0, the zero linear transformation.
(N2) For α ∈ K and T ∈ CL(X, Y),

(N3) Let T, S ∈ CL(X, Y). Then for all x ∈ X,
from which it follows (Lemma A) that ||T + S||  ||T|| + ||S||.
Example 2.13. Recall Example 2.8, page 69.
Let Rn and Rm be equipped with the Euclidean || · ||2-norm.
Let A = [Aij] ∈ Rm×n, and TA ∈ CL(Rn, Rm) be the continuous linear
transformation given by TAx = Ax, x ∈ Rn.
Then we’d seen that for all x ∈ Rn, 
 So
So we have an estimate for ||TA|| in terms of the matrix coefficients aij. But there
does not exist a general “formula” for ||TA|| in terms of the matrix coefficients
except in the special cases n = 1 or m = 1, when ||TA|| = |a11|. It can be seen that
the map
is also a norm on Rm×n, and is called the Hilbert-Schmidt norm of A.
Exercise 2.17. (Diagonal operator norm; operator norm needn’t be attained.) Let (λn)n∈N be a bounded
sequence in K, and let Λ ∈ CL(ℓ2) be given by Λ(a1, a2, a3, ···) = (λ1a1, λ2a2, λ3a3, ···) for all (a1, a2, a3,
···) ∈ ℓ2. Show that Λ ∈ CL(ℓ2) and 
Now let 
 Show that there is no x ∈ ℓ2 such that ||x||2  1 and ||Λx||2 = ||Λ||. This gives an
example showing that the operator norm need not be attained.
Exercise 2.18. (Schauder basis). Let X be a Banach space. A sequence of vectors (en)n∈N in X is a Schauder

basis for X if for every x ∈ X, there exists a unique sequence of numbers (ξn)n∈N such that 
Let 1  p < ∞, and en = (0, ···, 0, 1, 0, ···) be the sequence in ℓp with nth term equal to 1 and all others 0.
Show that {en : n ∈ N} is a Schauder basis for ℓp.
Hint: For uniqueness use the continuity of the “coordinate map” φn : x 
 xn, selecting the nth term of the
sequence x.
Remark. A Banach space X that has a Schauder basis is separable, that is, there exists a countable dense
subset in X (for example the linear combinations of the en with rational coefficients). The converse of the
above, namely if every separable Banach space had a Schauder basis, was an open problem for a long time.
In 1973, the Swedish mathematician Per Enflo finally constructed an example of a separable Banach space
that does not have a Schauder basis.
Exercise 2.19. (Invariant subspace, and the Invariant Subspace Problem)
(1) Prove that the averaging operator2 A : ℓ∞ → ℓ∞, defined by
is a continuous linear transformation. What is the operator norm of A?
(2) (∗) A subspace Y of a normed space X is said to be an invariant subspace with respect to a linear
transformation T : X → X if TY ⊂ Y. Let A ∈ CL(ℓℓ) be the averaging operator from part (1). Show that
the subspace c of ℓ∞, consisting of all convergent sequences, is an invariant subspace of the averaging
operator A. Hint: Show that if x ∈ c has limit L, then Ax has limit L.
Remark. Invariant subspaces are useful since they are helpful in studying complicated operators by
breaking them down into smaller operators acting on invariant subspaces. This is already familiar to the
student from the diagonalisation procedure in linear algebra, where one decomposes the vector space into
eigenspaces, and in these eigenspaces the linear transformation acts trivially. One of the open problems in
functional analysis is the invariant subspace problem:
Does every T ∈ CL(H) on a separable complex Hilbert space H have a non-trivial invariant
subspace?
Hilbert spaces are just special types of Banach spaces, in which the norm is induced by an inner product,
and we will learn about Hilbert spaces in Chapter 4. Non-trivial means that the invariant subspace must be
different from {0} or H. In the case of Banach spaces, the answer to the above question is “no”: during the
annual meeting of the American Mathematical Society in Toronto in 1976, Per Enflo (again!) announced
the existence of a Banach space and a bounded linear operator on it without any non-trivial invariant
subspace.
Now that we know CL(X, Y) is a normed space with the operator norm, it is
natural to ask if CL(X, Y) is complete, that is, if CL(X, Y) is a Banach space. It
turns out that CL(X, Y) is a Banach space if and only if Y is a Banach space, and
we’ll show this in the next section.
When is CL(X, Y) complete?
We’ll see that CL(X, Y) is a Banach space if and only if Y is a Banach space. In

this section the “if” part will be shown, and the “only if” part will be done in
Remark 2.9, page 109.
Theorem 2.8. If Y is a Banach space, and X is any normed space, then CL(X, Y)
is a Banach space.
Proof. Let (Tn)n∈N be a Cauchy sequence in CL(X, Y). Let x ∈ X. Claim:
(Tnx)n∈N is Cauchy in Y.
Indeed, for all n, m, ||Tnx – Tmx||  ||Tn – Tm|| ||x||.
As Y is Banach, (Tnx)n∈N converges in Y, with limit, say Tx ∈ Y.
So we get a map x → Tx : X → Y.
Questions: (a) Is T ∈ CL(X, Y)?
(b) Does Tn 
 T in CL(X, Y)?
(a) Is T a linear transformation?
If x1, x2 ∈ X, then (Tnx1)n∈N converges to Tx1 in Y, and (Tnx2)n∈N converges
to Tx2 in Y. Thus (Tnx1 + Tnx2)n∈N = (Tn(x1 + x2))n∈N converges to Tx1 + Tx2
in Y. But we know that (Tn(x1 + x2))n∈N converges to T(x1 + x2) in Y. By the
uniqueness of limits, T(x1 + x2) = Tx1 + Tx2.
Let α ∈ K and x ∈ X. Then (Tnx)n∈N converges to Tx in Y. So we have (α ·
(Tnx))n∈N = (Tn(α · x))n∈N converges to α · (Tx) in Y. But (Tn(α · x))n∈N
converges to T(α · x) in Y. So α · T(x) = T(α · x).
Is T continuous? Let  = 1. Then there exists an N ∈ N such that for all n, m
> N, ||Tn – Tm||   = 1. So for all n > N, ||Tn – TN+1||  1. Thus for n > N and x
∈ X, ||Tnx – TN +1x||  ||Tn – TN+1|| ||x||  1 · ||x||. Passing the limit n → ∞, we
obtain ||Tx – TN+1x||  ||x|| for all x ∈ X. So for all x ∈ X, ||Tx||  ||Tx – TN+1x||
+ ||TN+1x|| = (1 + ||TN+1||) ||x||. Conclusion: T ∈ CL(X, Y).
(b) Is it true that 
 Tn = T in CL(X, Y)?
Let  > 0. Then there exists an N ∈ N such that for all n, m > N, we have ||Tn
– Tm||  . So for all n, m > N and all x ∈ X, we obtain that ||Tnx – Tmx||  ||Tn
– Tm || · ||x||  ||x||. Passing to the limit as m → ∞, we get that for all n > N
and x ∈ X, ||Tnx – Tx||  ||x||. Hence for all n > N, ||Tn – T||  .
Corollary 2.2. If X is a normed space over K, then the dual space of X, X′ :=

CL(X, K), is a Banach space with the operator norm.
Corollary 2.3. If X is a Banach space, then CL(X) := CL(X, X) is a Banach
space with the operator norm.
Remark 2.4. (“Hilbert” versus Banach spaces). In Chapter 4, we’ll meet Hilbert
spaces: a Hilbert space is a special type of a Banach space in which the norm is
induced by an “inner product”. If instead of Banach spaces, we are interested
only in Hilbert spaces, then the notion of a Banach space is still indispensable,
since for a Hilbert space H, the normed space CL(H) is typically only a Banach
space, and not a Hilbert space in general.
(∗) Strong and weak operator topologies on CL(X, Y)
Many claims in this section won’t be proved, but are included to provide the
reader with a “road map”. The main content of the section are the definitions of
the three operator topologies and the illustrative examples. One who wants to
know more could embark on a deeper study, as offered for example in [Pedersen
(1989)] or [Rudin (1976)].
Let a set X be equipped with two topologies, and let X1 (respectively X2)
denote the set X equipped with the first (respectively second) topology. If the
identity map x → x : X1 → X2 is continuous, namely if every set open in X2 is
open in X1, one says that first topology is stronger than the second, or that the
second topology is weaker/coarser/smaller than the first. Of all the topologies on
the set X, there is a strongest one (discrete topology), namely the one for which
all subsets of X are open, and there is a weakest one (trivial topology), namely
the one for which only X, ∅ are open.
Now suppose we have a set X, and a family F = {fi : X → R | i ∈ I} of maps.
Then of course there exists at least one topology on X with respect to which all
the maps fi are continuous, namely the discrete topology on X. However, there is
also a “less wasteful/more efficient/weakest” topology on X that makes all the

maps fi, i ∈ I, continuous, characterized by the following: U is open in this
topology on X if for every x ∈ U, there exist a finite number of indices i1, ···, in
∈ I and intervals (a1, b1), ···, (an, bn) such that x ∈ {y ∈ X : fik (y) ∈ (ak, bk), k =
1, ···, n} ⊂ U. It can be shown that this gives a topology T on X, and for any
other topology T′ on X that makes the maps fi, i ∈ I, continuous, we have T ⊂ T′.
We had seen that CL(X, Y) is a normed space with the operator norm ||T|| :=
sup{||Tx|| : x ∈ X, ||x||  1}, T ∈ CL(X, Y). We call the resulting topology the
uniform operator topology on CL(X, Y), and is the weakest topology making
each map in the family
continuous. A subset U ⊂ CL(X, Y) is open in the uniform operator topology on
CL(X, Y) if for each T ∈ U, there exists an  > 0 such that {S ∈ CL(X, Y) : ||S –
T|| < } ⊂ U. A sequence (Tn)n∈N converges to T ∈ CL(X, Y) in the uniform
operator topology if 
 ||Tn – T|| = 0.
We remark that besides the uniform operator topology on CL(X, Y), there are
weaker topologies (with fewer open sets), on CL(X, Y), called the Strong
Topology and the Weak Topology. Here are the definitions, although in this basic
introduction, we won’t use these useful alternative topologies much.
Definition 2.4. (Strong Operator Topology)
Let X, Y be normed spaces. Then the weakest topology on CL(X, Y) which makes
each map in the family
continuous, is called the strong operator topology on CL(X, Y). A subset U ⊂
CL(X, Y) is open in the strong operator topology on CL(X, Y) if for each T ∈ U,
there exists an  > 0 and finitely many x1, ···, xn ∈ X such that {S ∈ CL(X, Y) :
||Sxk – Txk|| < , k = 1, ···, n} ⊂ U. A sequence (Tn)n∈N converges to T ∈ CL(X,
Y) in the strong operator topology if for all x ∈ X, 
 ||Tnx – Tx|| = 0.
Example 2.14. (Strong but not uniform convergence).
For n ∈ N, let Pn ∈ CL(ℓ2) be the “projection operator” given by

We claim that (Pn)n∈N converges to the identity operator I ∈ CL(ℓ2) in the strong
operator topology.
Indeed, ||Ia – Pna||22 = ||(0, ···, 0, an+1, an+2, ···)||22 = 
But the sequence (Pn)n∈N does not converge to the identity I ∈ CL(ℓ2) in the
uniform operator topology. Let’s show this by contradiction.
Suppose it does converge to I with respect to the operator norm. With  :=
1/2 > 0, there exists an N ∈ N such that ||PN – I|| < 1/2. So if eN+1 ∈ ℓ2 is the
sequence with the (N + 1)st term 1 and all others 0, then we have
a contradiction!
A yet weaker topology than the strong operator topology is the weak operator
topology, defined below.
Definition 2.5. (Weak Operator Topology)
Let X, Y be normed spaces. Let Y′ := CL(Y, K). Then the weakest topology on
CL(X, Y) which makes each map in the family
continuous, is called the weak operator topology on CL(X, Y). A subset U ⊂
CL(X, Y) is open in the weak operator topology on CL(X, Y) if for all T ∈ U,
there exists an  > 0, finitely many x1, ···, xn ∈ X, and φ1, ···, φn ∈ Y′ such that
A sequence (Tn)n∈N converges to T ∈ CL(X, Y) in the weak operator topology if
for all φ ∈ Y′ and for all 
The following table summarises this:

Example 2.15. (Weak but not strong convergence).
Let R ∈ CL(ℓ2) be given by ℓ2 ∋ (a1, a2, a3, ···) 
 (0, a1, a2, a3, ···), the right
shift operator. We claim that (Rn)n∈N converges to 0 ∈ CL(ℓ2) in the weak
operator topology. We’ll use a result which will be proved later on in Theorem
2.14, page 104 (and also in Chapter 4, Theorem 4.10, page 189):
For each φ ∈ CL(ℓ2, C) =: (ℓ2)′, there is an xφ = (xφ(k)k∈N ∈ ℓ2, such that
(Here ·∗ denotes complex conjugation.)
Using the Cauchy-Schwarz inequality (page 159), for all a ∈ ℓ2, φ ∈ (ℓ2)′,
Thus (Rn)n∈N converges to 0 ∈ CL(ℓ2) in the weak operator topology.
If e1 := (1, 0, 0, ···) ∈ ℓ2, then Rne1 = (0, ···, 0, 1, 0, ···), the sequence with
(n + 1)st term 1 and all others 0. So ||Rne1||2 = 1, n ∈ N. Thus it is not the case
that 
 Rne1 = 0 = 0e1.
So (Rn)n∈N does not converge to 0 in the strong operator topology.

2.4 Composition of continuous linear transformations
If T ∈ CL(X, Y),S ∈ CL(Y, Z), then the composition ST : X → Z of T, S is defined
by (ST) (x) = S(T(x)), x ∈ X.
It is easily checked that ST is linear. Moreover, it is continuous too, since for all
x ∈ X, we have ||(ST)(x)|| = ||S(T(x))  ||S|| ||Tx||  ||S|| ||T|| ||x||. Moreover, the
above inequality shows that ||ST||  ||S|| ||T||.
In particular, if X is a normed space, then CL(X), besides possessing a natural
addition and scalar multiplication (both defined pointwise), also possesses a
natural multiplication of elements of CL(X), namely composition (S, T) 
 ST :
CL(X) × CL(X) → CL(X). So CL(X) is an “algebra”. Loosely speaking, an
algebra is a vector space in which there is also available a nice way of
multiplying vectors and producing new vectors.
Definition 2.6. (Algebra). An algebra is a vector space V in which an associative
and distributive multiplication is defined, that is,
for all u, v, w ∈ V, and which is related to scalar multiplication so that
for all u, v ∈ V and all α ∈ K. We call e ∈ V a multiplicative identity element if
for all v ∈ V, one has ev = v = ve.
The algebra V := CL(X) has a multiplicative identity element, namely the identity
operator I. The identity operator is the map I : X 
 X, given by Ix = x, x ∈ X.
The operator I clearly belongs to CL(X) (with ||I|| = 1), and I serves as the
multiplicative identity element of the algebra CL(X): IT = T = TI for all T ∈
CL(X).
Definition 2.7. (Normed and Banach algebras).
A normed algebra is an algebra V equipped with a norm || · || that satisfies:

A Banach algebra is a normed algebra which is complete.
We note that V := CL(X) is a normed algebra. We’d seen earlier that CL(X) is a
Banach space if X is a Banach space. So CL(X) is a Banach algebra if X is a
Banach algebra.
Let us note that as opposed to vector addition in CL(X), vector multiplication
(that is, composition) in CL(X) is in general not commutative. Here is an
example. Take X = R2. Let T be clockwise rotation by π/2, and S be reflection in
the x-axis, that is,
Then one can check that TS ≠ ST. This can also be observed visually by
observing the distinct fates of the point (1, 0) under TS and under ST :
The commutator of A, B ∈ CL(X) is defined by [A, B] = AB – BA, and
“measures” the lack of commutativity of A and B. The above example shows that
the commutator may not be necessarily 0. In Exercise 2.22, page 86, we will
investigate the “largeness” of the commutator in finite and infinite dimensional
spaces X. This plays a role in Quantum Mechanics. We’ll show in Chapter 4
(page 204) that for “observables” A, B, the Heisenberg Uncertainty Relation
holds:
We won’t explain this3 right now, but we simply notice that the commutator
makes an appearance on the right-hand side.
If dim X = d < ∞, and T, S ∈ CL(X) are such that TS = I, then ST = I too. So
TS = I ⇒ TS = ST = I. (Let us show this. First of all, if TS = I, then ker S = {0}.

Indeed, if Sx = 0, then
Next observe that if {v1, ···, vd} is a basis for X, then {Sv1, ···, Svd} are linearly
independent: if αks are scalars such that α1Sv1 + ··· + αdSvd = 0, then S(α1v1 + ···
+ αdvd) = 0, and so α1v1 + ··· + αdvd = 0, making all αks zeros. Hence {Sv1, ···,
Svd} must be a basis for X. For x ∈ X, there exist βks in K such that x = β1Sv1 +
··· + βdSvd = S(β1v1 + ··· + βdvd); and so STx = STS(β1v1 + ··· + βdvd) = SI(β1v1 +
··· + βdvd) = x.)
However, if dim X = ∞, then it can happen that TS = I, but ST ≠ I. Consider
for example the left/right shift operators on ℓ2. We have LR = I as LR(a1, a2, a3,
···) = L(0, a1, a2, ···) = (a1, a2, ···) = I(a1, a2, a3, ···), for all (a1, a2, a3, ···) ∈ ℓ2.
But RL ≠ I since
This prompts the following definition.
Definition 2.8. (Invertible operator) Let X be a normed space. An element A ∈
CL(X) is said to be invertible if there exists a B ∈ CL(X) such that AB = I = BA.
Inverses are unique. This follows from the associativity of composition.
Proposition 2.2. If A ∈ CL(X) is invertible, then there exists a unique B ∈ CL(X)
such that AB = I = BA.
The unique inverse of an invertible A ∈ CL(X) is denoted by A–1 ∈ CL(X).
Proof. If B1, B2 ∈ CL(X) satisfy AB1 = I = B1A and AB2 = I = B2A, then B1 = IB1
= (B2A)B1 = B2(AB1) = B2I = B2.
Proposition 2.3. If A ∈ CL(X) is invertible, then A is bijective.
Proof. If x, y ∈ X are such that Ax = Ay, then A–1(Ax) = A–1(Ay), that is, Ix = Iy,
and so x = y. Thus A is injective/one-to-one.
If y ∈ X, then x := A–1 y ∈ X, and so Ax = A(A–1y) = Iy = y. Hence A is
surjective/onto too.

If A ∈ CL(X) is bijective, then the inverse map is automatically a linear
transformation. In the case when dim X < ∞, we have L(X) = CL(X). So in this
case the inverse is automatically continuous too. So if dim X < ∞, then A ∈
CL(X) is invertible if and only if A is a bijection.
In the infinite dimensional case, is it still true that if A ∈ CL(X) is a bijection,
then A must be invertible? The answer is “yes” if X is a Banach space. The proof
is not immediate, and we will show this below, using a deep result called the
“Open Mapping Theorem”. But first, let us see an example showing that in non-
Banach spaces, the inverses of continuous bijections may fail to be continuous.
Example 2.16. (Bijection, but not invertible.)
Recall that c00 is the subspace of ℓ∞ of all finitely supported sequences. Consider
the map A : c00 → c00 given by
Then A is linear, and continuous (because ||Ax||∞  ||x||∞ for all x ∈ c00). It is also
easily seen that A is injective and surjective. So A is a bijection. However, it is
not invertible. Indeed, if otherwise, B ∈ CL(c00) is the inverse, then we would
have, with em := (0, ···, 0, 1, 0, ···) (mth term 1, all others 0), that
giving m  ||B|| for all m ∈ N, a contradiction. But we aren’t shocked by this
example, since c00 is not complete with the supremum norm, and the
equivalence of bijectivity with invertibility is supposed to hold for operators in a
Banach space.
Exercise 2.20. (When is the diagonal operator invertible?)
Let (λn)n∈N be a bounded sequence in K, and consider Λ ∈ CL(ℓ2) given by
Show that Λ is invertible in CL(ℓ2) if and only if 
Exercise 2.21. Let X be a normed space, and suppose that A, B ∈ CL(X).
Show that if I + AB is invertible, then I + BA is also invertible, with the inverse (I + BA)–1 given by I – B(I +

AB)–1A.
Remark. This identity can be used to show that the nonzero spectrum of AB and BA coincide. λ is said to be
in the spectrum of an operator T if λI – T is not invertible in CL(X).
Exercise 2.22. ([A, B] can’t be “large” for A, B ∈ CL(X).)
(1) The trace, tr(A), of a square matrix A = [aij] ∈ Cd×d is the sum of its diagonal entries: tr(A) = a11 + ··· +
add. It can be shown that tr(A + B) = tr(A) + tr(B) and that tr(AB) = tr(BA). Prove that there cannot exist
A, B in Cd×d such that AB – BA = I, where I denotes the d × d identity matrix.
(2) Let X be a normed space, and A, B be in CL(X). Show that if AB – BA = I, then for all n ∈ N, ABn – Bn
A = nBn–1, where we set B0 := I. Taking the operator norm on both sides of ABn – Bn A = nBn–1,
conclude that we can never have AB – BA = I with A, B ∈ CL(X).
(3) Let C∞(R) denote the set of all functions f : R → R such that for all n ∈ N, f(n) exists. It is clear that
C∞(R) is a vector space with pointwise operations. Consider the operators A, B : C∞(R) → C∞(R) given
as follows:
(The operators A and B appear as the momentum operator and the position operator in Quantum
Mechanics.) Show that AB – BA = I, where I denotes the identity on C∞(R).
The Neumann Series Theorem.
Theorem 2.9. (Neumann4 Series Theorem).
Let X be a Banach space, and A ∈ CL(X) be such that ||A|| < 1.
Then (1) I – A is invertible in CL(X),
In particular, I – A : X → X is bijective: for each y ∈ X, there exists a unique
solution x ∈ X of the equation x – Ax = y, and moreover,
so that x depends continuously on y.
This plays a role in integral equation theory:

where y, k are given, and x is the unknown function.
(This is called the Fredholm equation of the second type.)
Proof. (Of the Neumann Series Theorem). For all n ∈ N, ||An||  ||A||n.
As ||A|| < 1, 
 ||A||n converges. By comparison, 
||An|| converges too.
As X is Banach, so is CL(X). Since all absolutely convergent series in the Banach
space CL(X) converge, it follows that
converges in CL(X). Is this S the inverse of I – A? For n ∈ N, define
Then we know that 
 Sn = S in CL(X). We have
Since ||ASn – AS||  ||A|| ||Sn – S|| and ||SnA – SA||  ||A|| ||Sn – S||, it follows that SA
= AS = S – I. This gives (I – A)S = I = S(I – A). Hence I – A is invertible in CL(X)
and
Moreover, 
Exercise 2.23. Consider the system
in the unknown variables (x1, x2) ∈ R2. If I denotes the 2 × 2 identity matrix, then this system can be
written as (I – K)x = y, where

(1) Show that if R2 is equipped with the norm || · ||2, then ||K|| < 1. 
Conclude that (2.3) has a unique solution (denoted by x in the sequel).
(2) Find out the unique solution x by computing (I – K)–1.
(3) Write a computer program to compute xn = (I + K + ··· + Kn)y and the relative error ||x – xn||2/||x||2 for
various values of n (say, until the relative error is less than 1%). Note the slow convergence of the
Neumann series.
Exercise 2.24. Let X be a Banach space, and let A ∈ CL(X) be such that ||A|| < 1.
For n ∈ N, let Pn := (I + A)(I + A2)(I + A4) ··· (I + A2).
(1) Using induction, show that (I – A)Pn = I – A2n+1 for all n ∈ N.
(2) Prove that (Pn)n∈N is convergent in CL(X) to (I – A)–1.
Exercise 2.25. (∗)(The set of invertibles is open, and ·–1 is continuous.)
Let X be a Banach space and GL(X) denote the set of all invertible continuous linear transformations on X.
(1) Prove that GL(X) is an open subset of CL(X) in the usual operator norm topology.
(2) Prove that T 
 T–1 is continuous on GL(X), that is, for all T0 ∈ CL(X) and each  > 0, there exists a δ >
0 such that if T ∈ CL(X) satisfies ||T – T0|| < δ, then T ∈ GL(X) and ||T–1 – T0
–1|| < .
The exponential of an operator. Let X be a Banach space and let A ∈ CL(X).
We will now study the exponential operator eA ∈ CL(X).
For a ∈ R, one defines the exponential ea ∈ R by
The exponential function e· is useful, because it provides a solution to the initial
value problem for the most basic differential equation
(Here x(t) ∈ R and x0 ∈ R.) The unique solution is given by x(t) = etax0, t ∈ R.
This fundamental differential equation arises in all sorts of applications, for
example, radioactive decay, Newton’s law of cooling, continuous compound
interest, population growth, etc.
For A ∈ CL(X), we will show that an analogous definition,
(where we have simply replaced the little a by capital A!) works, and the series

converges in CL(X). Then the map t  etA x0 provides a solution to the analogous
initial value problem, but now in the Banach space X, with the initial condition
x0 ∈ X.
Theorem 2.10. Let X be a Banach space, and A ∈ CL(X).
Then 
 converges in CL(X).
Proof. The real series 
 converges (to e||A||). Since for n ∈ N we have 
 by the Comparison Test, 
 converges absolutely. So 
 converges in the Banach space CL(X).
Remark 2.5. (∗) Recall that when a ∈ R, we have 
Similarly, it can be shown that when A ∈ CL(X), 
The last equality is not superfluous, since commutativity of multiplication in
CL(X) is not always guaranteed, but it turns out that A does commute with etA.
Formally, the above result is not surprising, as can be seen by differentiating the
series for etA termwise with respect to t:
A rigorous justification can be given using the fact that e(t+s)A = etA esA for all s, t
∈ R. In general, if A, B ∈ CL(X) commute, that is, AB = BA, then eA+B = eA eB.
This shows that eA is always invertible in CL(X). Indeed, since A commutes with
–A, we have e–AeA = eA–A = e0 = I = eA e–A.
Now let x0 ∈ X, A ∈ CL(X), and consider the initial value problem:

Then x(t) := etA x0, t ∈ R, solves the initial value problem because
with x(0) = e0Ax0 = e0x0 = Ix0 = x0.
Moreover, the solution is unique, since if  is any solution, then
so that e–tA (t) = e–0A (0) = Ix0 = x0 for all t, giving
for all t ∈ R. Hence the solution t  etA x0, t ∈ R, is unique.
Initial value problems in Banach spaces of the above type arise from initial
boundary value problems for partial differential equations and their
discretisations. More generally, the operator A in the initial value problem is then
“unbounded”, and similar to t 
 etA, one can then associate a “C0-semigroup 
 generated by the infinitesimal operator A”. The solution to the initial
value problem is given by x(t) = etA x0 for t  0. For example, the initial value
problem for the diffusion equation with the homogeneous Dirichlet boundary
conditions
gives the initial value problem for the following ordinary differential equation in
the Banach space L2[0, 1]:
where x(t) = u(·, t) ∈ L2[0, 1], and A : D(A)(⊂ L2[0, 1] → L2[0, 1] is an

unbounded operator given by
and 
This completes our (rather long!) Remark 2.5.
Example 2.17. (Computing eA for diagonalisable A). Consider the system
With x = (x1, x2), this system can be written as x′(t) = Ax(t), where
We know that given the initial condition x0 = (x1(0), x2(0)) ∈ R2, the unique
solution is x(t) = etAx0. This raises the question:
There are several ways, but let us consider a method which works for
diagonalisable As. First we note that if
and so
Note in particular that e0 = I, and so calculating eA cannot be the same as taking
exponentials of the entries of A!
Now suppose that A is diagonalisable, that is, A = PDP–1 where D is
diagonal and P is invertible. Then An = PDnP–1 and so

Let’s see this method in action when 
 where a, b ∈ R.
By computing the eigenvalues and eigenvectors of A, we can write
and so
In particular, our initial value problem for (2.4) has the solution (putting a = 1
and b = 2 above)
So we’ve seen how to compute eA if the matrix A is diagonalisable. However,
not all matrices are diagonalisable. For example, consider the matrix
The eigenvalues of this matrix are both 0, and so if it were diagonalisable, say A
= PDP–1, then the diagonal matrix D must be the zero matrix. But then A =
PDP–1 = P0P–1 = 0, and we have arrived at a contradiction since A ≠ 0! So this A
is not diagonalisable.
In general, however, every matrix has what is called a Jordan canonical
form, that is, there exists an invertible P such that P–1AP = D + N, where D is
diagonal, N is nilpotent (that is, there exists an n ∈ N such that Nn = 0), and D
and N commute. Then the exponential of A is:

But the computation of a P taking A to its Jordan form requires some
sophisticated linear algebra, and we won’t treat this here. The interested reader is
referred to [Hirsch and Smale (1974), Chapter 6].
Exercise 2.26. (eA+B ≠ eA eB).
Compute eA and eB, where A, B are the nilpotent matrices 
Give an example of matrices A, B ∈ R2×2 for which eA+B ≠ eA eB.
2.5 (∗) Open Mapping Theorem
In this section, we will show Theorem 2.11, the “Open Mapping Theorem”. The
proofs in this section are somewhat more technical than the rest of the sections
of this chapter.
Definition 2.9. (Open map) Let X, Y be normed spaces.
T ∈ CL(X, Y) is called open if for all open sets U ⊂ X, T(U) is open in Y.
Proposition 2.4.
Let X, Y be normed spaces, T ∈ CL(X, Y), and B := {x ∈ X : ||x||  1}. Then the
following are equivalent:
(1) T is open.
(2) There exists a δ > 0 such that B(0Y, δ) ⊂ T(B).
Proof.
(2)⇒(1): Suppose that there exists a δ > 0 such that B(0Y, δ) ⊂ T(B). Let U be
open in X. If y0 ∈ T(U), then y0 = Tx0 for some x0 ∈ U. As U is open, there exists
a r > 0 such that the open ball B(x0, r) with centre x0 and radius r is contained in
U. We claim that the open ball B(y0, δr/2) is contained in T(U). If y ∈ B(y0, δr/2),
then ||y – y0|| < δr/2, that is, ||(2/r)(y – y0)|| < δ, and so (2/r)(y – y0) ∈ B(0Y, δ) ⊂
T(B). Hence there exists an x ∈ B such that (2/r)(y – y0) = Tx, that is, we have y
= T((r/2)x + x0). But as ||((r/2)x + x0) – x0|| = (r/2)||x||  (r/2) · 1 < r, we see that
(r/2)x + x0 ∈ B(x0, r) ⊂ U. Consequently, y ∈ T(U), as desired.
(1)⇒(2): Suppose that T is open. Then T(B(0X, 1)), the image of the open set
B(0X, 1), must be open. But 0Y = T0X ∈ T(B(0X, 1)), and so, there must exist a δ
> 0 such that the open ball B(0Y, δ) ⊂ T(B(0X, 1)) ⊂ T(B), as wanted. 0

Lemma 2.1. (Baire Lemma)
Let (1) X be a Banach space, and
(2) (Fn)n∈N be a sequence of closed sets in X such that X = 
 Fn.
Then there exist an n ∈ N and a nonempty open set U such that U ⊂ Fn.
Proof. We assume none of the sets Fn contain a nonempty open subset and
construct a Cauchy sequence that converges to a point, which lies in none of the
Fn, contradicting the fact that the Fns cover X.
First let us observe that whenever a closed set F in X does not contain any
open set, we have that F is dense in X. (To see this, let x ∈ X, and r > 0. We’d
like to show that B(x, r) ∩ F ≠ ∅. If x ∈ F, then x ∈ B(x, r) ∩ F, and we are
done. On the other hand, if x ∉ F, then x ∈ F. But as F doesn’t contain any
open set, it won’t, in particular, contain B(x, r). So there must be an element y in
B(x, r) which is not in F. But this means that y ∈ F, and so we’ve got y ∈ B(x,
r) ∩ F, as wanted.) By our assumption, it follows that Fn is dense in X for all n
∈ N.
Let x1 be any element in the nonempty (dense!) open set F1. Let r1 > 0 be
such that 
 ⊂ F1. As F2 is dense in X, there exists an x2 ∈ B(x1, r1) ∩ 
F2. As B(x1, r1) ∩ F2 is open, we can find an r2 < r1/2 such that 
 ⊂ B(x1,
r1) ∩ F2. As F3 is dense in X, there exists an x3 ∈ B(x2, r2) ∩ F3. As B(x2, r2)
∩ F3 is open, we can find an r3 < r1/4 such that 
 ⊂ B(x2, r2) ∩ F3.
Proceeding in this manner, we obtain a sequence (xn)n∈N, with the term xn+1 ∈
B(xn, rn). If n > m, then B(xn, rn) ⊂ B(xm, rm), and so we have ||xn – xm|| < rm <
r1/2m–1 
 0. Thus (xn)n∈N is Cauchy, and as X is Banach, also convergent, say,
to x ∈ X. With a fixed m, in the inequality above, if we pass the limit as n → ∞,
then we obtain ||x – xm||  rm, that is, x ∈ 
 ⊂ Fm. As the choice of m ∈
N was arbitrary, for all m ∈ N, x ∉ Fm. But this contradicts the fact that the Fms

cover X.
Exercise 2.27.
Show that the Hamel basis5 of a Banach space can only be finite or uncountable.
Before proving the Open Mapping Theorem, we’ll give some notation and a
useful technical result. For subsets A, B of a normed space X and a scalar α, we
set αA := {αa : a ∈ A}, and A + B := {a + b : a ∈ A, b ∈ B}.
Lemma 2.2. Let X be a normed space, and A ⊂ X satisfy
(1) A is symmetric, that is, –A = A,
(2) A is mid-point convex, that is, for all x, y ∈ A, 
 ∈ A, and
(3) there is a nonempty open set U ⊂ A.
Then there exists a δ > 0 such that B(0, δ) ⊂ A.
Proof. First note that for a fixed scalar α ≠ 0, and an a ∈ X, the maps x  x + a :
X → X and x  αx : X → X, are both continuous, with the continuous inverses (x 
 x – a and x  α–1x).
Hence if U is open in X, then U + {–a} is open in X.
So U + (–A) = 
 (U + {–a}) is open in X. Thus 
 is open in X.
If a ∈ U, then 0 = 
Thus there exists a δ > 0 such that B(0, δ) ⊂ 
.
Theorem 2.11. (Open Mapping Theorem).
Let X, Y be Banach spaces, and T ∈ CL(X, Y) be surjective.
Then T is open.
Proof. Let B := {x ∈ X : ||x||  1}. Then X = 
 nB. Thanks to the surjectivity of
T, we have Y = 
 T(nB). Thus certainly Y = 
 T(nB). It can be checked that
T(nB) = nT(B). By the Baire Lemma, there exists an n ∈ N such that nT(B)
contains a nonempty open set. But since the map x 
 nx : X → X is continuous
with a continuous inverse, it follows that T(B) contains a nonempty open set too.
By Lemma 2.2, there exists a δ > 0 such that B(0Y, δ) ⊂ T(B). We will now show
that this implies

giving the required openness of T by Proposition 2.4. Let y such that ||y|| < δ/2.
We must show that there exists a x ∈ B with y = Tx. Using B(0Y, δ) ⊂ T(B), it
can be seen that
From (2.6), with n = 1, it follows that we can arbitrarily closely approximate y
by elements from T(B/2). Thus there exists an x1 with ||x1||  1/2 such that ||y –
Tx1||  δ/4 that is, y – Tx1 ∈ B(0, δ/4). From (2.6) again it follows (with n = 2)
that we can arbitrarily closely approximate y – Tx1 by an element Tx2 with ||x2|| 
1/4: ||y – Tx1 – Tx2||  δ/8. Proceeding in this manner, we can inductively
construct a sequence (xn)n∈N such that: ||xn||  1/2n and ||y – Tx1 – Tx2 – ··· – Txn–
1||  δ/2n.
As ||xn||  
 xn is absolutely convergent, and 
.
If we denote the sum of the series 
 xn by x, then
thanks to the continuity of T. Since ||x||  1, this proves the desired inclusion
(2.5).
Corollary 2.4. If X, Y are Banach spaces, and T ∈ CL(X, Y) is bijective, then T–1
∈ L(Y, X) is continuous.
We then refer to T as a normed space isomorphism, and say that X, Y are
isomorphic (as normed spaces), written X  Y.
Proof. T is open, and so if U is open in X, T(U) is open in Y. But (T–1)–1(U) = {y
∈ Y : T–1y ∈ U} = {y ∈ Y : y ∈ T(U)} = T(U). Thus the inverse images of open
sets under T–1 are open, showing that T–1 is continuous.
Exercise 2.28. Construct a continuous and surjective, but not open, f : R → R.
Exercise 2.29. (Closed Graph Theorem).
The aim of this exercise is to prove the Closed Graph Theorem:
Let X, Y be Banach spaces and T : X → Y be a linear transformation.

Then T is continuous if and only if its graph G(T) is closed in X × Y.
Here X × Y has the norm ||(x, y)} := max{||x||, ||y||}, (x, y) ∈ X × Y, and the set G(T) := {(x, Tx) : x ∈ X} ⊂ X
× Y is the graph of T.
The “only if” part is easy to see. If (xn, Txn) → (x, y), then xn → x, and as T is continuous, ||Txn – Tx||  ||T||
||xn – x||, so that Txn → Tx. But Txn → y, and so, by the uniqueness of limits, Tx = y. Thus (xn, Txn) → (x,
Tx) ∈ G(T), showing that G(T) is closed.
Show the “if” part. Hint: Consider p : G(T) → X, where p((x, Tx)) = x, x ∈ X.
Uniform Boundedness Principle.
We give below another important application of the Baire Lemma.
Theorem 2.12. (Uniform Boundedness Principle).
Suppose that
(1) X and Y are Banach spaces,
(2) Ti ∈ CL(X, Y), i ∈ I, is a “pointwise bounded” family, that is,
Then the family is “uniformly bounded”, that is, 
 ||Ti|| < + ∞.
Proof. For n ∈ N, Fn := {X ∈ X : 
 ||Tix||  n} = 
{x ∈ X : ||Tix||  n} is mid-
point convex, symmetric, and closed, as Fn is the intersection of the mid-point
convex, symmetric, and closed sets {x ∈ X : ||Tix||  n}, i ∈ I.
From (2), we have X = 
 Fn, and so by the Baire Lemma, there exists an n such
that Fn contains a nonempty open set. By Lemma 2.2, there exists a δ > 0 such
that the ball B(0, δ) with center 0 and radius δ is contained in Fn, that is, if ||x|| <
δ, then for all i ∈ I we have ||Tix||  n. We claim that ||Tix||  (2n/δ)||x|| for all x ∈
X and all i ∈ I. Clearly this is true if x = 0, since then both sides of the inequality
are equal to 0. On the other hand, if x ≠ 0, then y := 
x has norm ||y|| = δ/2 < δ,
and so we must have ||Tiy||  n, which, using the linearity of Ti and the positive
homogeneity of the norm, delivers, upon a rearrangement, the desired inequality.
Thus ||Ti||  2n/δ for all i ∈ I, and thus 
 ||Ti||  2n/δ.
Corollary 2.5. (Banach-Steinhauss Theorem).
Let (1) X, Y be Banach spaces, and
(2) (Tn)n∈N in CL(X, Y) be such that 
 Tnx exists for all x ∈ X.

Then x  
 Tnx : X → Y belongs to CL(X, Y).
Proof. It is clear that the map x  
 Tnx : X → Y is linear.
It remains to show that it is continuous too. Set Tx := 
 Tnx, x ∈ X. For each x
∈ X, (Tnx)n∈N is convergent, and in particular, bounded:
Hence by the Uniform Boundedness Principle, there exists an M such that for all
n ∈ N, ||Tn||  M. This gives, for each fixed x ∈ X, that
Passing the limit n → ∞ yields ||Tx||  M||x||. As the choice of x was arbitrary,
this holds for all x, and consequently, the linear transformation T is continuous.
2.6 Spectral Theory
For a linear transformation T ∈ L(X) on a finite dimensional vector space X over
C, the set of eigenvalues of T is known as its spectrum σ(T), and has cardinality
at most dim X. But in infinite dimensional complex vector spaces, strange things
may happen, for example linear transformations may have no eigenvalues at all
or finitely many or (countably/uncountably) infinitely many! First of all, here is
a natural definition of eigenvectors and eigenvalues, extending our prior
familiarity with eigenvalues from elementary linear algebra. We remind the
reader that the prefix eigen is derived from German, meaning “one’s own”.
Definition 2.10. (Eigenvalues and eigenvectors). Let X be a normed space and T
∈ CL(X). Then λ ∈ C is called an eigenvalue of T if there exists a nonzero vector
x ∈ X such that Tx = λx. Such a nonzero vector x is then called an eigenvector of
T corresponding to the eigenvalue λ.
Example 2.18. (Uncountably many eigenvalues).
Let λ ∈ D := {z ∈ C : |z| < 1}. If x := (1, λ, λ2, λ3, ···), then as |λ| < 1,

and so x ∈ ℓ2. Clearly x ≠ 0 too.
We see that x is an eigenvector of the left shift operator L ∈ CL(ℓ2) because
Thus each point in the open unit disk6 is an eigenvalue of L.
Example 2.19. (No eigenvalues). On the other hand, the right shift operator R ∈
CL(ℓ2) has no eigenvalues. Suppose that λ ∈ C is such that Rx = λx for some x =
(xn)n∈N ∈ ℓ2. Then
Suppose first that λ ≠ 0. Then from the above, λx1 = 0 gives x1 = 0. Next, λx2 = x1
now gives x2 = 0. Proceeding in this manner, we obtain x1 = x2 = x3 = ··· = 0, and
so x = 0.
On the other hand, if λ = 0, then (0, x1, x2, x3, ···) = (λx1, λx2, λx3, ···) shows
immediately that x1 = x2 = x3 = ··· = 0, and so x = 0.
Consequently, R has no eigenvalues.
Note that when dim X < ∞, and T ∈ CL(X), then
λ ∈ C is an eigenvalue of T if and only if λI – T is not invertible.
So the points in the spectrum σ(T) are exactly the ones where λI – T fails to be
invertible in σ(T). This prompts the following natural concept in the general
case, that is, when dim X  ∞.
Definition 2.11. (Spectrum and resolvent).
Let X be a normed space and T ∈ CL(X).
We say that λ ∈ C belongs to the spectrum σ(T) of T if λI – T is not invertible in
CL(X). Thus
The set ρ(T) is called the resolvent set of T.
The set σp(T) of all eigenvalues of T is called the point spectrum of T.

We have that σp(T) ⊂ σ(T), since if λ ∈ σp(T), then there exists a nonzero vector
x such that Tx = λx, that is, (λI – T)x = 0, showing that λI – T is not injective, and
hence can’t be invertible either!
We’ll now show that if X is Banach and T ∈ CL(X), then σ(T) is a compact
nonempty subset of C.
Theorem 2.13. Let X be a Banach space and T ∈ CL(X). 
Then
(1) σ(T) ⊂ {λ ∈ C : |λ|  ||T||}.
(2) ρ(T) is an open subset of C.
(3) σ(T) is a compact subset of C.
(4) σ(T) is nonempty.
Proof.
(1) Let |λ| > ||T||  0. Then 
 < 1, and so I – 
 is invertible in CL(X). Thus,
as λ ≠ 0, we have that
is invertible in CL(X) too.
(2) Let λ0 ∈ ρ(T). Then for λ ∈ C,
So I – 
.
For λ0 ≈ λ, A has small norm, in particular, < 1. Hence it follows that (λ0I –
T)–1(λI – T) =: S is invertible in CL(X). So we conclude that λI – T = (λ0I –
T)S (being the product of two invertible operators in CL(X)) is also invertible

in CL(X).
(3) σ(T) is bounded (as σ(T) ⊂ B(0, ||T||) := {z ∈ C : |z|  ||T||}), and also it is
closed (because its complement C\σ(T) = ρ(T) is open). So σ(T) is compact.
(4) (∗)7 Let σ(T) = ∅. Then f(z) := (zI – T)–1 ∈ CL(X) for all z ∈ C.
In particular, T–1 exists, and is not 0.
Let φ ∈ (CL(X))′ be such that φ(T–1) ≠ 0.
Such a φ exists by the Hahn-Banach Theorem (Exercise 2.38, page 109).
Let g : R2 → C be given by g(r, θ) = φ(f(reiθ)), for all (r, θ) ∈ R2.
We will show that g ∈ C1 (R2, C) by showing that it has continuous first
order partial derivatives (which will in turn be used in the calculations, and
also to justify a differentiation under the integral sign).
Using the resolvent identity (Exercise 2.30, page 102), we have
Using continuity of φ and that of the inverse operation (Exercise 2.25, page 88),
it follows from the above calculation, that
Similarly, 
.
By differentiating under the integral sign, we obtain
Consequently,

Hence F is constant, and we have
Now
Fix r such that |φ(f(reiθ))| < 
. Then
giving 2 < 1, a contradiction. This completes the proof.
Example 2.20. (Spectrum of the left shift operator).
Consider the left shift operator L ∈ CL(ℓ2). Then ||L||  1. So it follows that σ(L)
⊂ {z ∈ C : |z|  1}. As {z ∈ C : |z| < 1} ⊂ σp(L) ⊂ σ(L), and because σ(L) is
closed, it follows that {z ∈ C : |z|  1} ⊂ σ(L) too. So σ(L) = {z ∈ C : |z|  1}.
We now claim that σp(L) = {z ∈ C : |z| < 1}. We had seen earlier that {z ∈ C : |z|

< 1} ⊂ σp(L). Now we’ll show the reverse inclusion.
To this end, let λ ∈ σp(L) with eigenvector x = (xn)n∈N.
Then (x2, x3, ···) = L(x1, x2, x3, ···) = λ(x1, x2, x3, ···).
So λxn = xn+1 for all n, giving (by induction) xn = λn–1x1 for all n.
As ℓ2 ∋ x ≠ 0, we have
so that x1 ≠ 0, and the geometric series with common ratio |λ|2 converges. So |λ| <
1, and we get the reverse inclusion σp(T) ⊂ {z ∈ C : |z| < 1}.
We will return to this topic on spectral theory when we deal with operators on a
Hilbert space, and also in the context of compact operators.
Exercise 2.30. (Resolvent Identity). Let X be a normed space, T ∈ CL(X) and λ, μ ∈ ρ(T). Prove that (λI –
T)–1 – (μI – T)–1 = (μ – λ)(λI – T)–1(μI – T)–1.
Exercise 2.31. (Spectral radius). Let X be a Banach space, and T ∈ CL(X). Define the spectral radius of T
by rσ(T) := 
 |λ|.
(1) Prove that rσ(T)  ||T||.
(2) Show that for TA ∈ CL(R2), A := 
, then rσ(TA) < ||TA||.
Here R2 has the usual Euclidean || · ||2-norm.
Remark. In this connection, the Gelfand-Beurling Formula8 says that:
If X is Banach and T ∈ CL(X), then rσ(T) = 
 ||Tn||1/n.
Exercise 2.32. Let X be a Banach space, T ∈ CL(X), and λ ∈ σ(T).
Prove that λ2 belongs to the spectrum of T2.
Hint: Use (λ2I – T2) = (λI – T)(λI + T) = (λI + T)(λI – T).
Remark. More generally, the Spectral Mapping Theorem9 says that:
If X is a Banach space, T ∈ CL(X), p = c0 + c1z + · · · + cdzd ∈ C[z] (a polynomial with complex
coefficients), and p(T) := c0I + c1T + · · · + cdTd, then we have σ(p(T)) = p(σ(T)) := {p(λ) : λ ∈ σ(T)}.
Exercise 2.33. (Spectrum of the diagonal operator).
Let (λn)n∈N be sequence in C which is convergent to 0, and consider Λ ∈ CL(ℓ2) given by Λ(a1, a2, a3, · ·
·) = (λ1a1, λ2a2, λ3a3, · · ·) for all (a1, a2, a3, · · ·) ∈ ℓ2.
Show that {λn : n ∈ N) ⊂ σp(Λ) ⊂ {λn : n ∈ N) {0} = σ(Λ).
Remark. (Spectral Theorem for Compact Operators).
In Chapter 5, we will learn that this Λ is an example of a “compact operator”; see Example 5.3 on page 214.
More generally, one can show the Spectral Theorem for Compact Operators, which says that for a compact

operator K on an infinite dimensional Hilbert space H,
(1) σ(K\{0} = σp(K\{0}, and σ(K) is countable,
(2) 0 is the only accumulation point of σ(K),
(3) For all λ ∈ σp(K\{0}, dim kerp(λI – K) = dim kerp(λ*I – K*) < ∞.
Exercise 2.34. (Approximate spectrum).
(1) Let X be a Banach space, and T ∈ CL(X). A number λ ∈ C is said to belong to the approximate
spectrum σap(T) of T if there exists a sequence (xn)n∈N of vectors from X such that ||xn|| = 1 for all n ∈
N, and Txn – λxn 
 0 in X. Prove that σap(T) ⊂ σ(T).
(2) Let Λ ∈ CL(ℓ2) be the diagonal operator corresponding to a convergent (and hence bounded) sequence
(λn)n∈N. Prove that 
 λn ∈ σap(Λ).
Exercise 2.35. (Point spectrum of the position operator).
Let X be a normed space, and let A : DA → X be an “unbounded operator10”, where the domain DA is a
subspace of X. Then the point spectrum of the unbounded operator A is defined in an analogous manner as
before: σp(A) := {λ ∈ C : there exists an x ∈ DA\{0} such that Ax = λx}.
Now consider the position operator Q : DQ → L2(R), arising in Quantum Mechanics, where DQ := {Ψ ∈
L2(R) : (x → xΨ(x)) =: QΨ ∈ L2(R)}, and (QΨ)(x) := xΨ(x), for almost all x ∈ R, and all Ψ ∈ DQ.
Show that σp(Q) = ∅.
Remark. So Q has no eigenvectors in DQ ⊂ L2(R). However, when we learn elementary distribution theory
later on in Chapter 6, we’ll see that xδλ = λδλ for all λ ∈ R, where δλ is the “Dirac distribution” with support
at λ ∈ R. See Example 6.11 on page 251.
2.7 (∗) Dual space and the Hahn-Banach Theorem
Definition 2.12. (Dual space of a normed space).
Let X be a normed space over K. Then the normed space CL(X, K), equipped
with the operator norm, is called the dual space of X. One denotes the dual space
of X simply by X′. Elements of the dual space are sometimes called bounded
linear functionals.
Recall that a consequence of Theorem 2.8 (page 78) was Corollary 2.2, which
says that X′ is always a Banach space, even if X isn’t. This is because K = R or C
are both Banach spaces.
Given a concrete X, like Rd or ℓp, it is sometimes possible to “recognize” X′,
that is to establish a (normed space) isomorphism from X′ to some other Banach
space, for example:
Such results are called representation theorems, and we will see a few such

results now, and also later on in the chapter on Hilbert spaces (Chapter 4), when
we will learn about the Riesz Representation Theorem, page 189.
Theorem 2.14. For 1  p < ∞, (ℓp)′ » ℓp, where 
(Here the understanding is that if p = 1, then q = ∞.)
Proof. (Sketch). We consider K = R for simplicity. Let 1 < p < ∞.
By Hölder’s Inequality, |a1b1 + · · · + anbn|  ||(a1, · · ·, an)||p||b1, · · ·, bn)||q, with
equality if 
 is a multiple of 
. Let T ∈ CL(ℓp, R). Let ek ∈ ℓp be
the sequence (0, · · ·, 0, 1, 0, · · ·) with kth term 1, and all others 0. Fix n ∈ N.
Let a = (a1, · · ·, an, 0, · · ·) ∈ ℓp be such that 
 is a multiple of ((Te1)q, ·
· ·, (Ten)q) (i.e., ak := (Tek)p/q, k = 1, · · ·, n).
Then ||T||  
 = ||(Te1, · · · , Ten)||q.
Passing the limit n → ∞, we get (Te1, Te2, Te3, · · ·) ∈ ℓq. So we get a
continuous linear transformation T  (Te1, Te2, Te3, · · ·) : CL(ℓp, R) → ℓp. It
can be checked that this map ι is injective and surjective. As ι is bijective, it is an
isomorphism.
If p = 1, then let us now show that (ℓ1)′  ℓ∞. This is easier to see, since if T ∈
CL(ℓ1, R), then we get immediately that for all k, |Tek| = 
  ||T||, giving
(Tek)k∈N ∈ ℓ∞.
Remark 2.6. (The dual space (ℓ∞)′  ℓ′.)
If a = (an)n∈N ∈ ℓ1, then define the functional φa ∈ CL(ℓ∞, R) = (ℓ∞)′ by
Then a 
 φa : ℓ1 → (ℓ∞)′ is an injective linear transformation. It is continuous
since |φa(b)|  ||b||∞||a||1 for all b ∈ ℓ∞, giving ||φa||  ||a||1. However it is not
surjective, and this can be shown by using the Hahn-Banach Theorem (see
Theorem 2.15 on page 108), which says that a continuous linear functional on a
subspace of a normed space can be extended to the whole normed space while
preserving the operator norm of the functional. To see how this gives the non-

surjectivity of the map a  φa : ℓ1 → (ℓ∞)′ above, let us consider the subspace c
⊂ ℓ∞ of all convergent subsequences, and the “limit functional” λ : c → R, given
by
Then λ ∈ CL(c, R) = c′, and ||λ|| = 1. By the Hahn-Banach Theorem, this
functional λ on the subspace c of ℓ∞ has an extension Λ ∈ CL(ℓ∞, R). But now
we see that Λ can’t be φa for some a ∈ ℓ1. Otherwise, with en ∈ c ⊂ ℓ∞ being the
sequence with nth term 1 and all others 0, we have
for all n, showing that a = 0, and so Λ = φa = 0, which is clearly false, since Λ(1,
1, 1, · · ·) = λ(1, 1, 1, · · ·) = 1 ≠ 0! So (ℓ∞)′ is “bigger” than ℓ1.
Remark 2.7. If 1  p < ∞, then it can be shown that
where 
Exercise 2.36. Consider the subspace c0 ⊂ ℓ∞ consisting of all sequences that converge to 0. Prove that ℓ1 
(c0)′.
Exercise 2.37. (∗) (Dual of C[a, b]). In this exercise we will learn a representation of the dual space of C[a,
b]. A function μ : [a, b] → R is said to be of bounded variation on [a, b] if its total variation var(μ) on [a,
b] is finite, where
Here P is the set of all partitions of [a, b]. A partition of [a, b] is a finite set P = {t0, t1, · · ·, tn–1, tn} with t0
:= a < t1 < · · · < tn–1 < b =: tn.
(1) Show that the set of all functions of bounded variations on [a, b], with the usual pointwise operations
forms a vector space, denoted BV[a, b].
Define || · || : BV[a, b] → [0, +∞) by ||μ|| := |μ(a)| + var(μ), for μ ∈ BV[a, b].
(2) Prove that || · || is a norm on BV[a, b].
The Riemann-Stieltjes integral: Let x ∈ C[a, b] and μ ∈ BV[a, b]. For a partition of [a, b], say P = {t0, t1, · ·
·, tn–1, tn}, let δP be the length of a largest interval [tj–1, tj], that is, δP := max{t1 – t0, · · ·, tn – tn–1}, and set

Then it can be shown that there exists a unique real number, denoted by
called the Riemann-Stieltjes integral of x over [a, b] with respect to μ, such that for every  > 0 there is a δ >
0 such that if P is a partition of [a, b] satisfying δP < δ, then
The usual linearity of the integral (as with the ordinary Riemann integral) holds:
(3) Prove that 
  ||x||∞ var(μ), where x ∈ C[a, b] and μ ∈ BV[a, b].
(4) Conclude that every μ ∈ BV[a, b] gives rise to a φµ ∈ CL(C[a, b], R),
and that ||φµ||  var(μ).
The following converse result was proved by F. Riesz: For all φ ∈ CL(C[a, b], R), there exists a μ ∈ BV[a,
b] such that
and ||φ|| = var(μ). In other words, every element (C[a, b]′ can be represented by a Riemann-Stieltjes integral.
(5) For the functional x 
 x(a) on C[a, b], find a corresponding μ ∈ BV[a, b].
Dual spaces are important because, among other things, they allow us to define
dual operators. Here is the definition.
Definition 2.13. (Dual operator).
Let X, Y be normed spaces, and T ∈ CL(X, Y). We define the dual operator (of
T), T′ ∈ CL(Y′, X′), by (T′ψ)(x) = ψ(Tx), for all x ∈ X and ψ ∈ Y′.
Several things need to be checked here:
(1) For ψ ∈ Y′, does T′ψ belong to X′?
(2) Does T′ ∈ CL(Y′, X′)?

Let us begin with (1). If ψ′ ∈ Y′, then we have that
(L1) for all x1, x2 ∈ X,
(L2) for all α ∈ K and x ∈ X,
Hence T′ψ ∈ L(Y, K). Moreover T′ψ is continuous because for all x ∈ X,
Now let’s check (2), that is, that T′ ∈ CL(Y′, X′). We have
(L1) for all ψ1, ψ2 ∈ Y′, for all x ∈ X,
and so T′(ψ1 + ψ2) = T′(ψ1) + T′(ψ2),
(L2) for all α ∈ K, for all ψ ∈ Y′, for all x ∈ X,
and so T′(αψ) = α(T′ψ).
Thus T′ is linear. It is also continuous, because (2.7) gives ||T′ψ||  ||ψ||||T|| for all
ψ, that is T′ ∈ CL(Y′, X′) and ||T′||  ||T||.
Example 2.21. Consider x 
 x′ : C1[0, 1] → C[0, 1], x ∈ C1[0, 1]. Then D′ :
(C[0, 1]) → (C1[0, 1])′ is given by (D′ψ)(x) = ψ(Dx) = ψ(x′), ψ ∈ (C[0, 1])′, x ∈
C1[0, 1]. But (C[0, 1])′ ⊂ BV[0, 1], and so every ψ ∈ (C[0, 1])′ can be
represented by some element μψ ∈ BV[0, 1], so that
Thus if ψ ∈ (C[0, 1])′, then

where μψ ∈ BV[0, 1] is such that ψ(y) = 
 y(t)dμψ, y ∈ C[0, 1].
Sometimes problems for an operator can be simplified by looking at the dual
operator, making the consideration of dual spaces and dual operators a useful
endeavour.
Remark 2.8. (Dual versus adjoint operators). When we learn about Hilbert
spaces, we will learn about the notion of the adjoint T* ∈ CL(Y, X) of an
operator T ∈ CL(X, Y), where X, Y are Hilbert spaces, and we can use the Riesz
Representation Theorem (which we will also learn there) to represent elements
of Y′, X′ by elements of Y, X. (The next sentence should be read after the
discussion of the adjoint operator and the Riesz Representation Theorem.) If X, Y
are Hilbert spaces, and T ∈ CL(X, Y), then for Y ∋ yψ ≡ ψ ∈ Y′, we have for all x
∈ X that
where we identified T*yψ ∈ X with the functional x → x, T*yψ  : X → K in X′.
In this sense the notions of adjoint and dual “coincide” in this context of
operators on Hilbert spaces.
Hahn-Banach Theorem11. Finally, we will learn a fundamental result, known as
the Hahn-Banach Theorem, which says that X′ always contains sufficiently many
elements to separate points of X : for x ≠ y in X, there exists a φ ∈ X′ such that
φ(x) ≠ φ(y). In this sense, the elements of X′ play the role of “coordinates” for the
points of X (which is the kind of thinking one is used to in elementary linear
algebra when X = Kd).
Theorem 2.15. (Hahn-Banach).
Let (1) X be a normed space,
(2) Y ⊂ X be a linear subspace,
(3) φ ∈ CL(Y, K).
Then there exists a Φ ∈ CL(X, K) such that Φ|Y = φ and ||Φ|| = ||φ||.

In other words: “Every continuous linear functional on a subspace Y of a normed
space X possesses a norm-preserving extension to the entire normed space X”.
Before proving the Hahn-Banach Theorem, we will now list a few important
consequences one obtains from it.
Corollary 2.6. Let X be a normed space and x0 ∈ X. Then there exists an
element Φ ∈ X′ such that Φ(x0) = ||x0|| and ||Φ|| = 1.
Proof. Let Y := span {x0} and φ : Y → K be given by φ(y) = α||x0|| for y = αx0 ∈
Y, α ∈ K. Then φ is linear. Moreover, φ is a continuous map because |φ(y)| = |
φ(αx0)| = |α||x0||| = ||αx0|| = ||y||, that is, |φ(y)| = ||y|| for all y ∈ Y. Hence ||φ|| = 1.
By Hahn-Banach there now exists an extension Φ with the desired property.
As mentioned earlier, once we have the Hahn-Banach Theorem, one has the
ability of distinguishing elements of X using elements from X′. This is shown in
the two corollaries below.
Corollary 2.7. Let x and y be elements in a normed space X with x ≠ y. Then
there exists a Φ ∈ X′ such that Φ(x) ≠ Φ(y). (In other words, X′ separates the
points of X.)
Proof. Take Φ ∈ X′ with Φ(xq– Φ(y) = Φ(x – y) = ||x – y|| ≠ 0.
Exercise 2.38. Let X be a complex normed space and x∗ ∈ X\{0}. Show that there exists a φ∗ ∈ CL(X, C)
such that φ∗(x∗) ≠ 0.
Remark 2.9. (CL(X, Y) Banach ⇒ Y Banach).
Fix any nonzero x∗ ∈ X. By Exercise 2.38, there exists a φ ∈ CL(X, C), such that
φ(x∗) ≠ 0. Let (yn)n∈N be a Cauchy sequence in Y. For n ∈ N, define Tn ∈ CL(X,
Y) by
Then using the linearity of φ, it follows that Tn is linear. Also, Tn is continuous
because

A similar computation also gives that for n, m ∈ N,
showing that (Tn)n∈N is a Cauchy sequence (since (yn)n∈N is Cauchy). As CL(X,
Y) is Banach, the Cauchy sequence (Tn)n∈N is convergent, with limit, say, T ∈
CL(X, Y). But for x ∈ X,
and so we have that for all x ∈ X, (Tnx)n∈N converges to Tx.
In particular, with x = x∗, (Tnx∗)n∈N = (yn)n∈N converges to Tx∗ = y.
Hence Y is a Banach space!
Since X′ is itself a normed space, we know that X′ too has a dual space (X′)′ =: X
″, and X″ called the bidual of X. For x ∈ X, now consider the map φx : X′ → K,
given by
It is clear that φx is linear. Moreover, it is continuous too, since
We thus see that ||φx||  ||x||.
If x = 0, the zero vector in X, we have φx = 0, the zero linear transformation in
CL(X′, K). So ||φx|| = 0 = ||x|| in this case.
If x ≠ 0, then from Corollary 2.6 it follows that there exists a ψ ∈ X′\{0} for
which |ψ(x)| = ||x|| and ||ψ|| = 1. So we get the reverse inequality ||φx||  ||x|| too.
Hence ||φx|| = ||x|| in this case as well.
So we have the following third consequence of the Hahn-Banach Theorem:
Corollary 2.8. Let X be a normed space and x ∈ X. Then the map φx on X′, given
by 
 has the operator norm ||φx|| = ||x||.
Thus map x 
 φx : X 
 X″ is a linear isometric embedding of X in X″. If we
consider the elements of X as bounded linear functionals on X′ (by identifying x

with φx), then:
where the norm of x on X agrees with the norm of φx in X″. Sometimes, the map
x 
 φx from X into X″ is also surjective. In that case, the space X is called
reflexive and the inclusion (2.8) is replaced by the equality:
For proving the Hahn-Banach theorem, we will need a few preliminaries. We
will first prove the theorem in the case K = R, and then show how the result for
the case K = C can be derived from the real case.
In the following lemma, we consider a normed space X over R, and instead of a
norm, we consider a more general function p : X → R such that
that is, a subadditive and positive-homogeneous functional.
Lemma 2.3. (Hahn-Banach Lemma).
Let X be a normed space over R and ( : X → R satisfy (2.9) and (2.10).
Furthermore, let Y ⊂ X be a subspace and φ : Y → R be a linear map such that
Then there exists a linear map Φ : X → R such that Φ|Y = φ, and
(That is, there exists a linear extension of φ to X preserving the estimate.)
Proof. (∗) This is a rather technical proof, but the idea of the proof is to extend φ
“one dimension at a time”. Let x0 ∈ X\Y. Every vector x ∈ Y + (span {x0}) has a
unique decomposition x = αx0 + y, with y ∈ Y and α ∈ R. An extension Φ of φ to
Y + (span {x0}) is given by Φ(x) = αr + φ(y), where r, which ought to be Φ(x0),
will be chosen so that (2.12) holds, that is:

Owing to the positive homogeneity of p, it is sufficient to choose r such that
(2.13) is satisfied with α = 1 and α = –1:
Indeed, once these hold, then multiplication with t > 0 yields
which, in light of the fact that every element from Y can be written in the form ty
with y ∈ Y, gives (2.13) with α = ±t ≠ 0. For α = 0, (2.13) is already satisfied
according to the hypothesis. Now the inequalities (2.14) and (2.15) are
equivalent to the statement:
But there exists such a r precisely if all numbers φ(y) – p(–x0 + y), with y ∈ Y, lie
to the left of all numbers –φ(z) + p(x0 + z), with z ∈ Y, that is,
that is, if for all y, z ∈ Y, φ(y) + φ(z)  p(–x0 + y) + p(x0 + z). But this is indeed
the case since we have for all y, z ∈ Y that
Now from (2.16) it now follows that:
and it is sufficient to choose, for instance, 
(In general, the sup and the inf here are unequal, and we can choose r arbitrarily
from an interval.) Now the number r also satisfies (2.14) and (2.15) and thus
from (2.13), we have obtained an extension to Y + (span {x0}) such that (2.12)
holds.
Now the idea is that we extend φ one dimension at a time in order to get an
extension to the space X. If X were finite dimensional, then it is clear that this
can be done. After dim X – dim Y steps we will have obtained a linear

transformation Φ : X → R that satisfies (2.12).
In the general case, the proof goes through, in essentially the same manner,
by successive one-dimensional extensions, but we won’t be able to get an
extension to X in finitely many steps. In order to complete the process, we will
use Zorn’s Lemma.
Zorn’s Lemma
Zorn’s Lemma says that a partially ordered set P with the property that
every chain has an upper bound in P possesses a maximal element. The
terms are explained below.
A partial order  on a set P is a relation on P satisfying
• (transitivity) for all x, y, z ∈ P, x  y, y  z ⇒ x  z,
• (antisymmetry) for all x, y ∈ P, x  y, y  x ⇒ x = y,
• (reflexivity) for all x ∈ P, x  x.
A set with a partial order is called a partially ordered set.
A familiar example is R with the usual  relation, but the situation can be
much more general: for example, consider R2 with the order: (a, b) 
 (c,
d) if a  c and b  d. This latter example justifies the terminology partial.
Indeed, 
 is not a total order because not every pair of elements can be
compared with 
 : we have neither (0, 1) 
 (1, 0) nor (1, 0)  (0, 1).
A subset C of P is said to be bounded above if there exists an element u ∈
P such that x  u for all x ∈ C. The element u ∈ P is then called an upper
bound of C.
A subset C of P is said to be chain if for all x, y ∈ C, there holds x  y or y 
 x. Thus on a chain C,  forms a total order since any two elements in C
can be compared with . The set R2 with the above order 
 is not a chain,
since neither (0, 1) 
 (1, 0) nor (1, 0) 
 (0, 1). However, the diagonal
{(x, x) : x ∈ R} is a chain.
An element m ∈ P is called maximal if whenever x ∈ P and m  x we have
that x = m.
Zorn’s Lemma (named after the mathematician Max Zorn) is an axiom in
Set Theory. It can be shown that it is equivalent with the Axiom of Choice:
for every family Ai, i ∈ I, of nonempty sets Ai, there exists a map I ∋ i → xi
∈ Ai.

In order to apply Zorn’s Lemma to complete the proof of the Hahn-Banach
Lemma, we proceed as follows.
Consider the set P of all pairs (Z, ψ), where Z is a subspace of X with Y ⊂ Z
⊂ X, and ψ : Z → R is a linear transformation extending φ such that ψ(z)  p(z)
for all z ∈ Z.
We define the partial order  on P by defining (Z, ψ)  (Z′, ψ′) if Z ⊂ Z′ and
ψ = ψ′|Z. Then every chain in P has an upper bound, as explained below.
If C is a chain in P, then we can construct an upper bound (ZC, ψC) of C as
follows: Let ZC be the union of all subspaces Z, with (Z, ψ) ∈ C and let ψC be the
common extension of the linear transformations ψ. More precisely, for z ∈ ZC,
there exists a (Z, ψ) ∈ C such that z ∈ Z, and we define ψC(z) = ψ(z). This
definition of ψC(z) is independent of the choice of (Z, ψ). Indeed, if (Z′, ψ′) also
belongs to C, and z ∈ Z′, then we have (Z, ψ)  (Z′, ψ′) or (Z′, ψ′)  (Z, ψ), and
so ψ is the restriction of ψ′ or vice versa. In either case, we have ψ(z) = ψ′(z).
The map ψC : ZC → R so defined is linear: Indeed, if z, z′ belong to ZC, then
there exists a (Z, ψ) such that z ∈ Z and there exists a (Z′, ψ′) such that z′ ∈ Z′.
We have Z ⊂ Z′ or Z′ ⊂ Z. Suppose that Z′ ⊂ Z. Then also z′ ∈ Z so that for α, α′
∈ R, we have αz + α′z′ ∈ Z ⊂ ZC, and so it follows that ZC is subspace of X, and
ψC(αz + α′z′) = ψ(αz + α′z′) = αψ(z) + α′ψ(z′) = αψC(z) + α′ψC(z′). Finally, ψC
satisfies the inequality ψC(z)  p(z), z ∈ ZC, since indeed ψ(z)  p(z) for all z ∈ Z,
for all (Z, ψ) ∈ C. Thus we see that (ZC, ψC) belongs to P and that (Z, ψ)  (ZC,
ψC) for all (Z, ψ) ∈ C. This completes the proof that every chain in P has an
upper bound.
By Zorn’s Lemma, P has a maximal element (Z∗, Φ). Then Z∗ = X. Indeed, if
Z∗  X, then there exists an x∗ ∈ X\Z∗, and then from the first part of the proof
of the Hahn-Banach Lemma, it follows that we can extend Φ to Z∗ + (span{x∗})
with the same estimate given by p, contradicting the maximality of (Z∗,Φ). Thus
we have a linear Φ : X → R that extends φ : Y → R, while satisfying the estimate
(2.12). This completes the proof of the Hahn-Banach Lemma!
We will now apply this Hahn-Banach Lemma to prove the Hahn-Banach
Theorem, first of all in the case when K = R.
Proof. (Of the Hahn-Banach Theorem; real case.)

Let φ : Y → R be a continuous linear transformation. Then we have:
Now we apply the Hahn-Banach Lemma with p(x) := ||φ|| ||x||, x ∈ X. From
(2.17), we certainly have for all y ∈ Y, φ(y)  |φ(y)|  ||φ|| ||x|| = p(y). Thus, by the
Hahn-Banach Lemma, there exists a linear map Φ : X → R, extending φ to X,
that moreover satisfies the estimate that for all x ∈ X, Φ(x)  p(x) = ||φ|| ||x||.
Replacing x by –x, we obtain –Φ(x)  ||φ|| ||x||, and so for all x ∈ X, |Φ(x)|  ||φ||
||x||. Hence it follows that Φ is continuous and that ||Φ||  ||φ||. Since φ is the
restriction of Φ, we have, on the other hand, also that
This proves the Hahn-Banach Theorem in the real case.
The proof for complex scalars can be derived from the real case. We remark that
real versions of the Hahn-Banach Theorem were first proved independently by
Hahn and by Banach. The complex version was given by Bohnenblust and
Sobcyzk, following the ideas of Murray.
Proof. (Of the Hahn-Banach Theorem; complex case.)
Let X be a normed space over C. By restricting the multiplication with scalars to
real numbers, we obtain a normed space over R, which we denote simply by XR.
If Φ : X → C is a linear transformation, then ΦR : XR → R, given by ΦR(x) =
Re(Φ(x)), x ∈ XR, is also a linear transformation. We now observe below that Φ
is completely determined by its “real part” ΦR. For complex z = a + ib, with a, b
∈ R, we have iz = –b + ia, and hence Im(z) = –Re(iz). So Im(Φ(x)) = –Re(iΦ(x))
= –Re(Φ(ix)) = –ΦR(ix). Thus
Now if ΦR : XR → R is R-linear, then the right-hand side expression of (2.18)
determines a C-linear map Φ : X → C:
(1) It is clear that Φ is R-linear.

(2) We have Φ(ix) = ΦR(ix) – iΦR(–x) = i(ΦR(x) – iΦR(ix) = iΦ(x). 
Since every complex number is of the form a + ib, with a, b ∈ R, it follows
from here and the above part (1) that Φ is also C-linear.
Finally we show that Φ continuous if and only if its real part ΦR is continuous,
and moreover, ||Φ|| = ||ΦR||.
(1) If Φ is continuous, then since |ΦR(x)| = |Re(Φ(x))|  |Φ(x)|  ||Φ|| |x||, we have
that ΦR is continuous and moreover ||ΦR||  ||Φ||.
(2) Now suppose that ΦR is continuous and that Φ is given by (2.18). For x ∈ X,
let θ ∈ R be such that Φ(x) = eiθ |Φ(x)|. As |Φ(x)| is real,
so that Φ is continuous, and moreover ||Φ||  ||ΦR||.
The proof of the Hahn-Banach theorem in the complex case can now be
completed as follows. Let φ ∈ CL(Y, C) and let φR ∈ CL(YR, R) be the real part
of φ. Then there exists an extension ΦR ∈ CL(XR, R) of φR to XR with ||ΦR|| = ||
φR||. Let Φ ∈ CL(X, C) defined by (2.18). Then Φ is an extension of φ, and ||Φ||
= ||ΦR|| = ||φR|| = ||φ||.
Exercise 2.39. (Hamel basis). Let X be a vector space over any field F. Show that there exists a subset B ⊂
X such that B is linearly independent, and span B = X. Such a set is called a Hamel basis of X.
Exercise 2.40. Let X, Y be vector spaces over a field F. Show that any function f : B → Y defined on a
Hamel basis B of X can be extended to a linear transformation F : X → Y, that is, F |B = f. Hint: Every
vector in X can be uniquely expressed as a linear combination of vectors from B.
Exercise 2.41. Let X be an infinite dimensional normed space, and let Y be a nontrivial normed space. Prove
that there exists a linear transformation from X to Y which is not continuous.
Exercise 2.42. R is a vector space over Q, and hence has a Hamel basis B. Prove that B is necessarily
uncountable.
Exercise 2.43. (Additive discontinuous F : R → R). Show that there exists a function F : R → R such that
for all x, y ∈ R, F(x + y) = F(x) + F(y), but F is not continuous on R.

Exercise 2.44. (∗)(Banach limits).
Consider the subspace c of ℓ∞ comprising convergent sequences.
Let l : c → K be the limit functional given by 
(1) Show that l is an element in the dual space CL(c, K) of c, when c is equipped with the induced norm
from ℓ∞.
Let Y ⊂ ℓ∞ be given by 
(2) Show that Y is a subspace of ℓ∞.
(3) Prove that for all x ∈ ℓ∞, x – Sx ∈ Y, where S : ℓ∞ → ℓ∞ denotes the left shift operator: S(x1, x2, x3, ···)
= (x2, x3, x4, ···), (xn)n∈N ∈ ℓ∞.
(4) Prove that c ⊂ Y.
(5) Show that there exists a L ∈ CL(ℓ∞, K) such that L|c = l and LS = L.
This gives a generalisation of the concept of a limit, and the number Lx is called a Banach limit of a
(possibly divergent!) sequence x ∈ ℓ∞.
Hint: First observe that L0 : Y → K defined by
is an extension of the functional l from c to Y. Now use the Hahn-Banach Theorem to extend L0 from Y
to ℓ∞.
(6) Find the Banach limit of the divergent sequence ((–1)n)n∈N.
Notes
The proof of the Open Mapping Theorem given in §2.5, and the proof of the
Hahn-Banach Theorem given in §2.7 are based on [Thomas (1997)].
1 This has nothing to do with the null space: ker TA := {x ∈ L2[0, 1] : TAx = 0}, which is also called the
kernel of the integral operator.
2 In Fourier/Harmonic Analysis, this is sometimes called the Cesáro summation operator.
3 That is, what Δψ, · ψ, etc. mean.
4 The “geometric” series in (2) is called the Neumann series, after the German mathematician Carl
Neumann, who used it in connection with the Dirichlet problem.
5 For the definition/existence of a Hamel basis, see Exercise 2.39, page 115.
6 We remark that if we look at the “matrix” corresponding to L, while thinking of vectors in ℓ2 as an
“infinite columns”, then the action of L is described by

a matrix with all diagonal entries equal to 0, and with 1s along an “upper” diagonal. So in this case, our
“matricial intuition” would have led us astray, since based on the above matrix, reminiscent of a Jordan
block in finite dimensional linear algebra, one would be tempted to hastily guess that L has the only
eigenvalue 0!
7 The usual proof of this is by using some tools from complex analysis. We will instead follow the proof
from [Singh (2006)] relying on real analysis techniques.
8 See for example [Taylor and Lay (1980), page 287, Theorem 3.2].
9 See for example [Taylor and Lay (1980), page 279, Theorem 3.4].
10 By an unbounded operator, we mean a linear transformation that is not continuous.
11 Named after the mathematicians Hans Hahn and Stefan Banach.

Chapter 3
Differentiation
In the previous chapter we studied continuity of operators from a normed space
X to a normed space Y. In this chapter, we will study differentiation: we will
define the (Fréchet) derivative of a map f : X → Y at a point x0 ∈ X. Roughly
speaking, the derivative f′(x0) of f at a point x0 will be a continuous linear
transformation f′(x0) : X → Y that provides a linear approximation of f in the
vicinity of x0.
As an application of the notion of differentiation, we will indicate its use in
solving optimisation problems in normed spaces, for example for real valued
maps living on C1[a, b]. At the end of the chapter, we’ll apply our results to the
concrete 
case 
of 
solving 
the 
optimisation 
problem 
Setting the derivative of a relevant functional, arising from (P), equal to the zero
linear transformation, we get a condition for an extremal curve x∗, called the
Euler-Lagrange equation. Thus, instead of an algebraic equation obtained for
example in the minimisation of a polynomial p : R → R using ordinary calculus,
now, for the problem (P), the Euler-Lagrange equation is a differential equation.
The solution of this differential equation is then the sought after function x∗ that
solves the optimisation problem (P).
At the end of this chapter, we will also briefly see an application of the
language developed in this chapter to Classical Mechanics, where we will
describe the Lagrangian equations and the Hamiltonian equations for simple
mechanical systems. This will also serve as a stepping stone to a discussion of
Quantum Mechanics in the next chapter.
3.1 Definition of the derivative
Let us first revisit the situation in ordinary calculus, where f : R → R, and let us

rewrite the definition of the derivative of f at x0 ∈ R in a manner that lends itself
to generalisation to the case of maps between normed spaces. Recall that for a
function f : R → R, the derivative at a point x0 is the approximation of f around
x0 by a straight line.
Let f : R → R and let x0 ∈ R. Then f is said to be differentiable at x0 with
derivative f′(x0) ∈ R if 
that is, for every  > 0, there exists a δ > 0 such that whenever x ∈ R satisfies 0 <
|x − x0| < δ, there holds that 
that is,
If we now imagine f instead to be a map from a normed space X to another
normed space Y, then bearing in mind that the norm is a generalisation of the
absolute value in R, we may try mimicking the above definition and replace the
denominator |x − x0| above by ||x − x0||, and the numerator absolute value can be
replaced by the norm in Y, since f(x) − f(x0) lives in Y. But what object must
there be in the box below?
Since f(x), f(x0) live in Y, we expect the term f′(x0)(x − x0) to be also in Y. As x −
x0 is in X, f′(x0) should take this into Y. So we see that it is natural that we should
not expect f′(x0) to be a number (as was the case when X = Y = R), but rather it
we expect it should be a certain mapping from X to Y. We will in fact want it to
be a continuous linear transformation from X to Y. Why? We will see this later,
but a short answer is that with this definition, we can prove analogous theorems

from ordinary calculus, and we can use these theorems in applications to solve
(e.g. optimisation) problems. After this rough motivation, let us now see the
precise definition.
Definition 3.1. (Derivative).
Let X, Y be normed spaces, f : X → Y be a map, and x0 ∈ X.
Then f is said to be differentiable at x0 if there exists a continuous linear
transformation L : X → Y such that for every  > 0, there exists a δ > 0 such that
whenever x ∈ X satisfies 0 < ||x − x0|| < δ, we have 
(If f is differentiable at x0, then it can be shown that there can be at most one
continuous linear transformation L such that the above statement holds. We will
prove this below in Theorem 3.1, page 124.) The unique continuous linear
transformation L is denoted by f′(x0), and is called the derivative of f at x0.
If f is differentiable at every point x ∈ X, then f is said to be differentiable.
Before we see some simple illustrative examples on the calculation of the
derivative, let us check that this is a genuine extension of the notion of
differentiability from ordinary calculus. Over there the concept of derivative was
very simple, and f′(x0) was just a number. Now we will see that over there too, it
was actually a continuous linear transformation, but it just so happens that any
continuous linear transformation from R to R is simply given by multiplication
by a fixed number. We explain this below.
Coincidence of our new definition with the old definition when we have X =
Y = R, f : R → R, x0 ∈ R.
(1) Differentiable in the old sense ⇒ differentiable in the new sense. 
Let 
 exist and be the number 
 
Define the map L : R → R by 
 
Then L is a linear transformation as verified below.
(L1) For every v1, v2 ∈ R,
(L2) For every α ∈ R and every v ∈ V,

L is continuous since |L(v)| = |
(x0) · v| = |
(x0)||v| for all v ∈ R. We know
that 
that is, for every  > 0, there exists a δ > 0 such that whenever x ∈ R satisfies
0 
< 
|x 
− 
x0| 
< 
δ, 
we 
have 
that is, 
So f is differentiable in the new sense too, and 
(x0) = L, that is, we have (
(x0)) (v) = 
(x0) · v, v ∈ R.
(2) Differentiable in the new sense ⇒ differentiable in the old sense.
Suppose there is a continuous linear transformation 
(x0) : R → R such
that for every  > 0, there exists a δ > 0 such that whenever x ∈ R satisfies 0
< |x − x0| < δ, we have 
Define 
(x0) := (
x0))(1) ∈ R. Then if x ∈ R, we have 
So there exists a number, namely 
(x0), such that for every  > 0, there
exists a δ > 0 such that whenever x ∈ R satisfies 0 < |x − x0| < δ, 
Consequently, f is differentiable at x0 in the old sense, and furthermore, 
(x0) = (
(x0))(1).
The derivative as a local linear approximation. We know that in ordinary
calculus, for a function f : R → R that is differentiable at x0 ∈ R, the number f′
(x0) has the interpretation of being the slope of the tangent line to the graph of
the function at the point (x0, f(x0)), and the tangent line itself serves as a local
linear approximation to the graph of the function. Imagine zooming into the
point (x0, f′(x0)) using lenses of greater and greater magnification: then there is
little difference between the graph of the function and the tangent line. We now
show that also in the more general set-up when f is a map from a normed space X
to a normed space Y, that is differentiable at a point x0 ∈ X, f′(x0) can be
interpreted as giving a local linear approximation to the mapping f near the point

x0, and we explain this below. Let  > 0. Then we know that for all x close
enough to x0 and distinct from x0, we have 
that is, ||f(x) − f(x0) − f′(x0)(x − x0)|| < ||x − x0||. So for all x close enough to x0, 
that is, f(x) − f(x0) − f′(x0)(x − x0) ≈ 0 ∈ X, and upon rearranging, 
The above says that near x0, f(x) − f(x0) looks like the action of the linear
transformation f′(x0) acting on x − x0. We will keep this important message in
mind because it will help us calculate the derivative in concrete examples. Given
an f, for which we need to find f′(x0), our starting point will always be to start
with calculating f(x) − f(x0) and trying to guess what linear transformation L
would give that f(x) − f(x0) ≈ L(x − x0) for x near x0. So we would start by
writing f(x) − f(x0) = L(x − x0) + error, and then showing that the error term is
mild enough so that the derivative definition can be verified. We will soon see
this in action below, but first let us make an important remark.
Remark 3.1. In our definition of the derivative, why do we insist the derivative f
′(x0) of f : X → Y at x0 ∈ X should be a continuous linear transformation—that is,
why not settle just for it being a linear transformation (without demanding
continuity)? 
The 
answer 
to 
this 
question 
is 
tied 
to 
wanting 
We know this holds with the usual derivative concept in ordinary calculus when f
: R → R. If we want this property to hold also in our more general setting of
normed spaces, then just having f′(x0) as a linear transformation won’t do, but in
addition we also need the continuity.
On the other hand, for solving optimisation problems, even if one doesn’t
have differentiability at a point implying continuity at the point, one can prove
useful optimisation theorems using the weaker notion of the derivative. The
weaker notion is called the Gateaux derivative1, while our stronger notion is the
Fréchet derivative. As we’ll only use the Fréchet derivative, we refer to our
“Fréchet derivative” simply as “derivative.”
Example 3.1. Let X, Y be normed spaces, and let T : X → Y be a continuous
linear transformation. We ask: 
Let us do some rough work first. We would like to fill the question mark in the
box 
below 
with 
a 
continuous 
linear 
transformation 
so 
that 

for x close to x0. But owing to the linearity of T, we know that for all x ∈ X, 
and (the right-hand side) T is already a continuous linear transformation. So we
make a guess that T′(x0) = T! Let us check this now.
Let  > 0. Choose any δ > 0, for example, δ = 1. Then whenever x ∈ X
satisfies 
0 
< 
||x 
− 
x0|| 
< 
δ 
= 
1, 
we 
have 
Hence T′(x0) = T. Note that as the choice of x0 was arbitrary, we have in fact
obtained that for all x ∈ X, T′(x) = T! This is analogous to the observation in
ordinary calculus that a linear function x  m · x has the same slope at all points,
namely the number m.
Example 3.2. Consider f : C[a, b] → R,
Let x0 ∈ C[a, b]. What is f′(x0)?
As before, we begin with some rough work to make a guess for f′(x0) and we
seek a continuous linear transformation L so that for x ∈ C[a, b] near x0, f(x) −
f(x0) ≈ L(x − x0). We have 
where L : C[a, b] → R is given by
This L is a continuous linear transformation, since it is a special case of Example
2.11, page 72 (when A := 2·x0 and B = 0). Let us now check that the “ -δ
definition of differentiability” holds with this L. For x ∈ C[a, b], 

and so
So if 0 < ||x − x0||∞, then
Let  > 0. Set δ := /(b − a). Then δ > 0 and if 0 < ||x − x||∞ < δ, 
So f′(x0) = L. In other words, f′(x0) is the continuous linear transformation from
C[a, b] to R given by 
So as opposed to the ordinary calculus case, one must stop thinking of the
derivative as being a mere number, but instead, in the context of maps between
normed spaces, the derivative at a point is itself a map, in fact a continuous
linear transformation. So the answer to the question 
should always begin with the phrase
“f′(x0) is the continuous linear transformation from X to Y given by ···”.
To emphasise this, let us see some particular cases of our calculation of f′(x0)
above, for specific choices of x0.
In particular, we have that the derivative of f at the zero function 0, namely f′
(0), is the zero linear transformation 0 : C[a, b] → R that sends every h ∈ C[a,
b] to the number 0: 0(h) = 0, h ∈ C[a, b].
Similarly, 

Exercise 3.1. Consider f : C[0, 1] → R given by 
Let x0 ∈ C[0, 1]. What is f′(x0)? What is f′(0)?
We now prove that something we had mentioned earlier, but which we haven’t
proved yet: if f is differentiable at x0, then its derivative is unique.
Theorem 3.1. Let X, Y be normed spaces. If f : X → Y is differentiable at x0 ∈ X,
then there is a unique continuous linear transformation L such that for every  >
0, there is a δ > 0 such that whenever x ∈ X satisfies 0 < ||x − x0|| < δ, there holds
Proof. Suppose that L1, L2 : X → Y are two continuous linear transformations
such that for every  > 0, there is a δ > 0 such that whenever x ∈ X satisfies 0 <
||x − x0|| δ, there holds 
Suppose that L1(h0) ≠ L2(h0) for some h0 ∈ X. Clearly h0 ≠ 0 (for otherwise L1(0)
= 0 = L2(0)!). Take  = 1/n for some n ∈ N. Then there exists a δn > 0 such that
whenever x ∈ X satisfies 0 < ||x − x0|| < δn, the inequalities (3.1), (3.2) hold.
with 
 we have that x ≠ x0, and
So (3.1), (3.2) hold for this x. The triangle inequality gives
Upon rearranging, we obtain 
As the choice of n ∈ N was arbitrary, it follows that ||L1(h0) − L2(h0)|| = 0, and so
L1(h0) = L2(h0), a contradiction. This completes the proof.
Exercise 3.2. (Differentiability ⇒ continuity).
Let X, Y be normed spaces, x0 ∈ X, and f : X → Y be differentiable at x0.
Prove that f is continuous at x0.

Exercise 3.3. Consider f : C1[0, 1] → R defined by f(x) = (x′(1))2, x ∈ C1[0, 1].
Is f differentiable? If so, compute f′(x0) at x0 ∈ C1[0, 1].
Exercise 3.4. (Chain rule).
Given distinct x1, x2 in a normed space X, define the straight line γ : R → X
passing through x1, x2 by γ(t) = (1 − t)x1 + tx2, t ∈ R.
(1) Prove that if f : X → R is differentiable at γ(t0), for some t0 ∈ R, then f  γ :
R → R is differentiable at t0 and (f γ)′(t0)) = f′(γ(t0))(x2 − x1).
(2) Deduce that if g : X → R is differentiable and g′(x) = 0 at every x ∈ X, then
g is constant.
3.2 Fundamental theorems of optimisation
From ordinary calculus, we know the following two facts that enable one to
solve optimisation problems for f : R → R.
Fact 1. If x∗ ∈ R is a minimiser of f, then f′(x∗) = 0.
Fact 2. If f″(x)  0 for all x ∈ R and f′(x∗) = 0, then x∗ is a minimiser of f.
The first fact gives a necessary condition for minimisation (and allows one to
narrow the possibilities for minimisers — together with the knowledge of the
existence of a minimiser, this is a very useful result since it then tells us that the
minimiser x∗ has to be one which satisfies f′(x∗) = 0). On the other hand, the
second fact gives a sufficient condition for minimisation.
Analogously, we will prove the following two results in this section, but now
for a real-valued function f : X → R on a normed space X.
Fact 1. If x∗ ∈ X is a minimiser of f, then f′(x∗) = 0.
Fact 2. If f is convex and f′(x∗) = 0, then x∗ is a minimiser of f.
We mention that there is no loss of generality in assuming that we have a
minimisation problem, as opposed to a maximisation one. This is because we can
just look at −f instead of f. (If f : S → R is a given function on a set S, then
defining −f : S → R by (−f)(x) = −f(x), x ∈ S, we see that x∗ ∈ S is a maximiser
for f if and only if x∗ is a minimiser for −f .) Optimisation: necessity of
vanishing derivative

Theorem 3.2. Let X be a normed space, and let f : X → R be a function that is
differentiable at x∗ ∈ X . If f has a minimum at x∗, then f′(x∗) = 0.
Let us first clarify what 0 above means: 0 : X → R is the continuous linear
transformation that sends everything in X to 0 ∈ R: 0(h) = 0, h ∈ X.
So to say that “f′(x∗) = 0” is the same as saying that for all h ∈ X, (f′(x∗))(h) = 0.
Proof. Suppose that f′(x∗) ≠ 0. Then there exists a vector h0 ∈ X such that (f′(x∗))
(h0) ≠ 0. Clearly this h0 must be a nonzero vector (because the linear
transformation f′(x0) takes the zero vector in X to the zero vector in R, which is
0). Let  > 0. Then there exists a δ > 0 such that whenever x ∈ X satisfies 0 < ||x
− x∗|| < δ, we have 
Thus whenever 0 < ||x − x∗|| < δ, we have
Hence whenever 0 < ||x − x∗|| < δ, 
Now we will construct a special x using the h0 from before. Take
Then x ≠ x∗ and 
Using the linearity of f′(x0), we obtain

Thus, |(f′(x∗))(h0)| < ||h0||. As  > 0 was arbitrary, |(f′(x∗))(h0)| = 0, and so (f′(x∗))
(h0) = 0, a contradiction.
We remark that the condition f′(x∗) = 0 is a necessary condition for x∗ to be a
minimiser, but it is not sufficient. This is analogous to the situation to
optimisation in R: if we look at f : R → R given by f(x) = x3, x ∈ R, then with x∗
:= 0, we have that f′(x∗) = 3x∗2 = 3 · 02 = 0, but clearly x∗ = 0 is not a minimiser
of f.
Example 3.3. Let f : C[0, 1] → R be given by f(x) = (x(1))3, x ∈ C[0, 1]. Then f′
(0) = 0. (Here the 0 on the left-hand side is the zero function in C[0, 1], while the
0 on the right-hand side is the zero linear transformation 0 : C[0, 1] → R.)
Indeed, given  > 0, we may set δ := min{ , 1}, and then we have that whenever
x 
∈ 
C[0, 
1] 
satisfies 
0 
< 
||x 
− 
0||∞ 
< 
δ, 
But 0 is not a minimiser for f. For example, with x := −α · 1 ∈ C[0, 1], where α >
0, we have f(x) = (−α)3 = −α3 < 0 = f(0), showing that 0 is not2 a minimiser.
Exercise 3.5. Let f : C [a, b] → R be given by 
In Example 3.2, page 122, we showed that if x0 ∈ C [a, b], then f′(x0) is given by
(1) Find all x0 ∈ C[a, b] for which f′(x0) = 0.

(2) If we know that x∗ ∈ C[a, b] is a minimiser for f, what can we say about x∗?
Optimisation: sufficiency in the convex case
We will now show that if f : X → R is a convex function, then a vanishing
derivative at some point is enough to conclude that the function has a minimum
at that point. Thus the condition “f″(x)  0 for all x ∈ R” from ordinary calculus
when X = R, is now replaced by “f is convex” when X is a general normed space.
We will see that in the special case when X = R (and when f is twice
continuously differentiable), convexity is precisely characterised by the second
derivative condition above. We begin by giving the definition of a convex
function.
Definition 3.2. (Convex set, convex function) Let X be a normed space.
(1) A subset C ⊂ X is said to be a convex set if for every x1, x2 ∈ C, and all α ∈
(0, 1), (1 − α) · x1 + α · x2 ∈ C.
(2) Let C be a convex subset of X. A map f : C → R is said to be a convex
function 
if 
for 
every 
x1, 
x2 
∈ 
C, 
and 
all 
α 
(0, 
1), 
The geometric interpretation of the inequality (3.3), when X = R, is shown
below: the graph of a convex function lies above all possible chords.
Exercise 3.6. Let a < b, ya, yb be fixed real numbers.
Show that S := {x ∈ C1[a, b] : x(a) = ya and x(b) = yb} is a convex set.

Exercise 3.7. (||·||) is a convex function).
If X is a normed space, then prove that the norm x  ||x|| : X → R is convex.
Exercise 3.8. (Convex set versus convex function).
Let X be a normed space, C be a convex subset of X, and let f : C → R. Define
the epigraph of f by 
Intuitively, we think of U(f) as the “region above the graph of f ”. Show that f is a
convex function if and only if U(f) is a convex set.
Exercise 3.9. Suppose that f : X → R is a convex function on a normed space X.
If n ∈ N, x1, ···, xn ∈ X, then 
Convexity of functions living in R.
We will now see that for twice differentiable functions f : (a, b) → R, convexity
of f is equivalent to the condition that f″(x)  0 for all x ∈ (a, b). This test will
actually help us to show convexity of some functions on spaces like C1[a, b].
If one were to use the definition of convexity alone, then the verification can
be cumbersome. Consider for example the function f : R → R given by f(x) = x2,
x ∈ R. To verify that this function is convex, we note that for x1, x2 ∈ R and α ∈
(0, 1), 
On the other hand, we will now prove the following result.
Theorem 3.3. Let f : (a, b) → R be twice continuously differentiable. Then f is
convex if and only if for all x ∈ (a, b), f″(x)  0.
The convexity of x  x2 is now immediate, as 
Example 3.4. We have 
 and so x 
 ex is convex.

Consequently, for all x1, x2 ∈ R and all α ∈ (0, 1), we have the inequality e(1−α)x1
+αx2  (1 − α)ex1 + αex2 .
Exercise 3.10. Consider the function f : R → R given by 
 Show
that f is convex.
Proof. (Of Theorem 3.3.) Only if part: Let x, y ∈ (a, b) and x < u < y.
Set 
 Then α ∈ (0, 1), and 
 As f is convex, 
that is,
From (3.4), (y − x) f(u) 
 (u − x) f(y) + (y − x + x −y) f(x), that is, 
and so
From (3.4), we also have (y − x)f(u)  (u − y + y − x)f(y) + (y − u)f(x), that is, (y
− x)f(u) − (y − x)f(y) 
 (u − y)f(y) − (u − y)f(x), and so 
Combining (3.5) and (3.6), 
Passing the limit as u  x and u  y, 
Hence f′ is increasing, and so 
Consequently, for all x ∈ R, f″(x)  0.
If part: Since f″(x)  0 for all x ∈ (a, b), it follows that f′ is increasing. Indeed by
the Fundamental Theorem of Calculus, if a < x < y < b, then 
Now let a < x < y < b, α ∈ (0, 1), and u := (1 − α)x + αy. Then x < u < y.
By the Mean Value Theorem, 
 for some v ∈ (x, u).
Similarly, 
 for some w ∈ (u, y).
As 
w 
> 
v, 
we 
have 
f′(w) 
 
f′(v), 
and 
so 

Rearranging, we obtain
Thus f is convex.
Example 3.5. Consider the function f : C[a, b] → R given by
Is f convex? We will show below that f is convex, using the convexity of the map
ξ  ξ2 : R → R. Let x1, x2 ∈ C[a, b] and α ∈ (0, 1). Then for all a, b ∈ R, ((1 −
α)a + αb)2  (1 − α)a2 + αb2. Hence for each t ∈ [a, b], with a := x1(t), b := x2(t),
we obtain 
Thus
Consequently, f is convex.
Exercise 3.11. (Convexity of the arc length functional.)
Let f : C1[0, 1] → R, be given by 
Prove that f is convex.
Example 3.6. Let us revisit Example 0.1, page viii.
There S := {x ∈ C1[0, T] : x(0) = 0 and x(T) = Q}, and f : S → R was given by 
where a, b, Q > 0 are constants. Let us check that f is convex. The convexity of
the map

follows from the convexity of η 
 η2 : R → R. The map 
is constant on S because
and so this map is trivially convex. Hence f, being the sum of two convex
functions, is convex too.
We now prove the following result on the sufficiency of the vanishing derivative
for a minimiser in the case of convex functions.
Theorem 3.4.
Let X be a normed space and f : X → R be convex and differentiable. If x∗ ∈ X is
such that f′(x∗) = 0, then f has a minimum at x∗ .
Proof. Suppose that x0 ∈ X and f(x0) < f(x∗). Define φ : R → R by φ(t) = f(tx0 +
(1 − t)x∗), t ∈ R. The function φ is convex, since if α ∈ (0, 1) and t1, t2 ∈ R,
then we have 
Also, from Exercise 3.4 on page 125, φ is differentiable at 0, and
We have φ(1) = f(x0) < f(x∗) = φ(0). By the Mean Value Theorem, there exists a θ

∈ (0, 1) such that 
But this is a contradiction because φ is convex (and so φ′ must be increasing; see
the proof of the “only if” part of Theorem 3.3).
Thus there cannot exist an x0 ∈ X such that f(x0) < f(x∗).
Consequently, f has a minimum at x∗.
Exercise 3.12. Consider 
Let x0 ∈ C[0, 1]. From Example 3.2, page 122, f′(x0) : C[0, 1] → R is given by 
Prove that f′(x0) = 0 if and only if x0(t) = 0 for all t ∈ [0, 1].
We have also seen that in Example 3.5, page 131, that f is a convex function.
Find all solutions to the optimisation problem 
3.3 Euler-Lagrange equation
Theorem 3.5. Let
(1) x∗ ∈ S = {x ∈ C1[a, b] : x(a) = ya, x(b) = yb}, (2)
(3)
(4) X := {h ∈ C1[a, b] : h(a) = 0, x(bh = 0}, (5)
 : X → R be given by (h)
= f(x∗ + ), h ∈ X .
Then ′(0) = 0 if and only if x∗ ∈ S satisfies the Euler-Lagrange equation: 
Definition 3.3. Such an x∗ ∈ S, which satisfies the Euler-Lagrange equation, is
said to be stationary for the functional f.
Note that X defined above in Theorem 3.5 is a vector space, since it is a subspace
of C1[0, 1] (Exercise 1.3, page 7), and it inherits the ||·||1,∞-norm from C1[0, 1].

To prove Theorem 3.5, we will need the following result.
Lemma 3.1. (“Fundamental lemma of the calculus of variations”). If k ∈ C[a, b]
is such that 
then there exists a constant c such that k(t) = c for all t ∈ [a, b].
Of course, if k ≡ c, then by the Fundamental Theorem of Calculus,
for all h ∈ C1[a, b] that satisfy h(a) = h(b) = 0. The remarkable thing is that the
converse is true, namely that the special property in the box forces k to be a
constant.
Proof. Set 
(If k ≡ c, then
so the c defined above is the constant that k “is supposed to be”.)
Define h0 : [a, b] → R by 
Thus 
 Since h′0(t) = k(t) − c, t ∈ [a,
b], we obtain 
Thus k(t) − c = 0 for all t ∈ [a, b], and so k ≡ c.

Proof. (Of Theorem 3.5). We note that h ∈ X if and only if x∗ + h ∈ S.
and so x∗ + h ∈ S.
Vice versa, if x∗ + h ∈ S, then
Consequently, h ∈ X .) Thus  is well-defined.
What is ′(0)? For h ∈ X, we have
By Taylor’s Formula for F, we know that
for some θ such that 0 < θ < 1. We will apply this for each fixed t ∈ [a, b], with
ξ0 := x∗(t), p := h(t), η0 := x′∗(t), q := h′(t), τ0 := t, r := 0, and we will obtain a θ ∈
(0, 1) for which the above formula works. If I change the t, then I will get a
possibly different θ ∈ (0, 1). So we have that the θ depends on t ∈ [a, b]. This
gives 
rise 
to 
a 
function 
Θ 
: 
[a, 
b] 
→ 
(0, 
1) 
so 
that 

where
and HF (·) denotes the Hessian of F:
From the above, we make a guess for 
′(0): define L : X → R by 
We have seen that L is a continuous linear transformation in Example 2.11, page
72. For h ∈ X,
where
We note that for each t ∈ [a, b], the point

in R3 belongs to a ball with centre (x∗(t), x′∗(t), t) and radius ||h||1,∞. But x∗, x′∗
are continuous, and so these centres (x∗(t), x′∗(t), t), for different values of t ∈ [a,
b], lie inside some big compact set in R3. And if we look at balls with radius, say
1, around these centres, we get a somewhat bigger compact set, say K, in R3.
Since the partial derivatives 
are all continuous, it follows that their absolute values are bounded on K. Hence
M is finite.
Let  > 0, and 
 If h ∈ X satisfies 0 < ||h − 0||1,∞ = ||h||1,∞ < δ, then 
Consequently, ′(0) = L.
(Only if part). So far we’ve calculated 
′(0) and found out that it is the
continuous linear transformation L. Now suppose that ′(0) = L = 0, that is, for
all h ∈ X, Lh = 0, and so for all h ∈ C1[a, b] with h(a) = h(b) = 0, 
We would now like to use the technical result (Lemma 3.1) we had shown. So
we rewrite the above integral and convert the term in the integrand which
involves h, into a term involving h′, by using integration by parts: 
because h(a) = h(b) = 0. So for all h ∈ C1[a, b] with h(a) = h(b) = 0, we have 
By Lemma 3.1, 
 for some constant c.
By differentiating with respect to t, we obtain
(If part). Now suppose that x∗ satisfies the Euler-Lagrange equation, that is,
A(t) − B′(t) = 0 for all t ∈ [a, b]. For h ∈ X, we have 

Thus for all h ∈ X,
Consequently, ′(0) = L = 0
Corollary 3.1.
Let 
(1) S = 
{x 
∈ 
C1[a, 
b] : 
x(a) = 
ya, 
x(b) 
= 
yb}, 
(2)
(3) f : S → R be given by 
Then we have:
(a) If x∗ is a minimiser of f, then it satisfies the Euler-Lagrange equation: 
(b) If f is convex, and x∗ ∈ S satisfies the Euler-Lagrange equation, then x∗ is a
minimiser of f .
Proof. Let X := {x ∈ C1[a, b] : x(a) = 0, x(b) = 0}, and  : X → R be given by 
(h) = f(x∗ + ), h ∈ X. Then  is well-defined.
(a) We claim that  has a minimum at 0 ∈ X. Indeed, for h ∈ X, we have 
So by Theorem 3.2, page 126, ′(0) = 0. From Theorem 3.5, page 134, it
follows that x∗ satisfies the Euler-Lagrange equation.
(b) Now let f be convex and x∗ ∈ S satisfy the Euler-Lagrange equation. By
Theorem 3.5, it follows that ′(0) = 0. The convexity of f makes  convex as
well. 
Indeed, 
if 
h1, 
h2 
∈ 
X, 
and 
α 
∈ 
(0, 
1), 
then 

Recall that in Theorem 3.4, page 132, we had shown that for a convex
function, the derivative vanishing at a point implies that that point is a
minimiser for the function. Since  is convex, and because ′(0) = 0, 0 is a
minimiser of . We claim that x∗ is a minimiser of f.
Indeed, if x ∈ S, then x = x∗ + (x – x∗) = x∗ + h, where h := x – x∗ ∈ X.
Hence
This completes the proof.
Let us revisit Example 0.1, page viii, and solve it by observing that it falls in the
class of problems considered in the above result.
Example 3.7. Recall that S := {x ∈ C1[0, T] : x(0) = 0 and x(T) = Q}, so that we
have a = 0, b = T, ya = 0 and yb = Q. The cost function f : S → R was given by 
where a, b, Q > 0 are constants, and F : R3 → R is given by 
So this problem does fall into the class of problems covered by Corollary 3.1. In
order 
to 
apply 
the 
result 
to 
solve 
this 
problem, 
we 
compute 
The Euler-Lagrange equation for x∗ ∈ S is:
that is, 
 for all t ∈ [0, T]. Thus
By the Fundamental Theorem of Calculus, it follows that there is a constant A

such that x′(t) = A, t ∈ [0, T], and integrating again, we obtain a constant B such
that x∗(t) = At + B, t ∈ [0, T]. But since x∗ ∈ S, we also have that x∗(0) = 0 and
x∗(T) = Q, which we can use to find the constants A, B: A · 0 + B = 0, and A · T
+ B = Q so that B = 0 and A = Q/T. Consequently, by part (a) of the conclusion in
Corollary 3.1, we know that if x∗ is a minimiser of f, then 
On the other hand, we had checked in Example 3.6 that f is convex. And we
know that the x∗ given above satisfies the Euler-Lagrange equation.
Consequently, by part (b) of the conclusion in Corollary 3.1, we know that this
x∗ is a minimiser. So we have shown, using Corollary 3.1, that 
So we have solved our optimal mining question.
And we now know that the optimal mining operation is given by the humble
straight line!
Exercise 3.13. (Euclidean plane).
Let P1 = R2 with ||(x, t)||1 := 
 for (x, t) ∈ P1.
Set S := {x ∈ C1[a, b] : x(a) = xa, x(b) = xb}.
Given x ∈ S, the map 
 is a curve in the Euclidean
plane P1, and we define its arc length by 
Show that the straight line joining (xa, a) and (xb, b) has the smallest arc length.
Exercise 3.14. (Galilean spacetime).
Let P0 = R2 with ||(x, t)||0 := 
 for (x, t) ∈ P0.
Set S := {x ∈ C1[a, b] : x(a) = xa, x(b) = xb}.
Given x ∈ S, the map 
 is a curve in the Euclidean
plane P0, and we define its arc length by 
Show that all the curves γx joining (xa, a) and (xb, b) have the same arc length. (If
we think of P0 as the collection of all events (=“here and now”), with the

coordinates provided by an “inertial frame3” choice, then this arc length is the
pre-relativistic absolute time between the two events (xa, a) and (xb, b).) Exercise
3.15. (Minkowski spacetime).
Let P−1 = R2 with ||(x, t)||−1 := 
 for (x, t) ∈ P0.
Set S := {x ∈ C[a, b] : x(a) = xa, x(b) = xb, for all a  t  b, |x′(t)| < 1}.
Given x ∈ S, the map 
 is a curve in the Euclidean
plane P−1, and we define its arc length by 
Show4 that among all the curves γx joining (xa, a) and (xb, b), the straight line
has the largest(!) arc length.
(P−1 can be thought of as the special relativistic spacetime of all events, with the
coordinates provided by an “inertial frame” choice. Then the arc length L(γx) is
the proper time between the two events (xa, a) and (xb, b), which may be thought
of as the time recorded by a clock carried by an observer along its worldline γx.
The fact that the straight line has the largest length accounts for the aging of the
travelling sibling in the famous Twin Paradox: Imagine two twins, say Seeta and
Geeta, who are separated at birth, event (0, 0) in an inertial frame, and meet
again in adulthood at the event (0, T). Seeta, the meek twin, doesn’t move in the
inertial frame, described by the straight line γS joining the events (0, 0) and (0,
T). Meanwhile, the other feisty twin, Geeta, travels in a spaceship (never
exceeding the speed of light, 1), with a worldline given by γG, starting at (0, 0),
and ending to meet the twin at (0, T) as shown.
There is no longer any surprise that Seeta has aged far more than Geeta, thanks
to our inequality that L(γG) < L(γS). Resting is rusting!) Exercise 3.16. (Euler-
Lagrange Equation: vector valued case).
The results in this section can be generalised to the case when f has the form

where (ξ1, ···, ξn, η1, ···, ηn, τ) 
 F(ξ1, ···, ξn, η1, ···, ηn, τ) : R2n+1 → R is a
function with continuous partial derivatives of order  2, and x1, ···, xn are n
continuously differentiable functions of the variable t ∈ [a, b].
Then following a similar analysis as before, we obtain n Euler-Lagrange
equations to be satisfied by the minimiser (x1∗, ···, xn∗): for t ∈ [a, b], and k ∈
{1, ···, n}, 
Let us see an application of this to the planar motion of a body under the action
of the gravitational force field (planet around the sun).
If x1(t) = r(t) (the distance to the sun), and x2(t) = φ(t) (radial angle), then the
function to be minimised is 
Show that the Euler-Lagrange equations give
(The latter equation shows that the angular momentum, L(t) := mr(t)2 φ′(t), is
conserved, and this gives Kepler’s Second Law, saying that a planet sweeps
equal areas in equal times.) Exercise 3.17. (Euler-Lagrange Equation: several
independent variables). Suppose that Ω ⊂ Rd is a “region” (an open, path-
connected set), and that 
is a given C2 function (called the Lagrangian density).
We are interested in finding u ∈ C1(Ω) which minimise I : C1(Ω) → R given by 
(Here subscripts indicate respective partial derivatives: for example, 
It can be shown that a necessary condition for u to be a minimiser of I is that it
satisfies 
the 
Euler-Lagrange 
equation 
below: 
(Note that the Euler-Lagrange equation above is now a Partial Differential
Equation (PDE), rather than the Ordinary Differential Equation (ODE) we had

met in Theorem 3.5, page 134.) Let us consider examples of writing the Euler-
Lagrange equation.
(1) (Minimal area surfaces).
Consider a smooth surface in R3 which is the graph of (x, y) 
 u(x, y)
defined on an open set Ω ⊂ R2.
The area of the surface is given by:
Show that if u is a minimiser, then u must satisfy the PDE
Verify that the following solve this PDE: 
Also, in the case of the helicoid, show that a parametric representation of the
surface is given by x(s, t) = s · cos t, y(s, t) = s · sin t, z(s, t) = t, by setting 
 and t = tan−1(y/x). Plot the surface5 with Maple using: 
(2) (Wave equation).
Consider a vibrating string of length 1, whose ends are fixed.
If u(x, t) denotes the displacement at position x and time t, where 0  x  1,
then the potential energy at time t is given by 
and the kinetic energy is 
For u : [0, 1] × [0, T] → R, set
Prove that if u∗ minimises I, then it satisfies the wave equation
Show that if f : R → R is
- twice continuously differentiable,
- odd (f(x) = −f(–x) for all x ∈ R), and - periodic with period 2 (that is, f(x
+ 2) = f(x) for all x ∈ R), then u given by 
 is such
that

- it solves the wave equation,
- with the boundary conditions u(0, ·) = 0 = u(1, ·) and
- the initial conditions u(·, 0) = f (position) and 
 (velocity).
Interpret the solution graphically.
3.4 An excursion in Classical Mechanics
The aim of this section is to apply the Euler-Lagrange equation to illustrate some
basic ideas in classical mechanics. Also, this brief discussion will provide some
useful background for discussing Quantum Mechanics later on, as an application
of Hilbert spaces and their operators.
Newtonian Mechanics. Consider the motion t 
 q∗(t) of a classical point
particle of mass m along a straight line. Here q∗(t) denotes the position of the
particle at time t.
Then the evolution of q∗ is described by Newton’s Law, which says that the
“mass times the acceleration equals the force acting”, that is, if F(x) is the force
at position x, then 
Together with the particle’s initial position q∗(ti) = qi, and initial velocity 
(ti) =
vi, the above equation determines a unique q∗.
Principle of Stationary Action. An alternative formulation of Newtonian
Mechanics is given by the “Principle of Stationary6 Action”, which is more
useful because it lends itself to generalisations for other types of physical
situations, for example in describing the electromagnetic field (when there are no
particles). In that sense it is more fundamental as it provides a unifying
language.
First, let us define the potential V : R → R as follows. Choose any x0 ∈ R,
and set 
V is thought of as the work done against the force to go from x0 to x. (Because of
the fact that x0 was chosen arbitrarily, the potential V for a force F is not unique.
By the Fundamental Theorem of Calculus, we have 
and so it can be seen that if V,  are potentials for F, then as 

there is a constant c ∈ R such that (x) = V(x) + c, x ∈ R.) We define the kinetic
energy of the particle at time t as 
Consider for q ∈ C1[ti, tf] with q(ti) = xi and q(tf) = xf, the action 
where L is called the Lagrangian, given by L(x, v) = mv2 − V(x).
Note that along an imagined trajectory q of a particle,
The Principle of Stationary Action in Classical Mechanics says that the motion
q∗ of the particle moving from position xi at time ti to position xf at time tf is
such 
that 
Ã′(0) 
= 
0, 
where 
Ã 
: 
X 
→ 
R 
is 
given 
by 
By Theorem 3.5, page 134, the Euler-Lagrange equation is equivalent to Ã′(0) =
0, and so the motion q∗ is described by 
that is,
Using 
 we obtain Newton’s equation of motion,
Here are a couple of examples.
Example 3.8. (The falling stone).

Let x  0 denote the height above the surface of the Earth of a stone of mass m.
Then its potential energy is given by V(x) = mgx. Thus 
Suppose the stone starts from initial height x0 > 0 at time 0, with initial speed 0.
Then the height q∗(t) at time t is described by 
 that is, 
Using the initial conditions, we obtain 
 and so
Example 3.9. (The Harmonic Oscillator).
The harmonic oscillator is the simplest oscillating system, where we can imagine
a body mass m attached to a spring with spring constant k oscillating about its
equilibrium position. For a displacement of x from the equilibrium position of
the mass, a force of kx is imparted on the mass by the spring. So 
The equation of motion is
describing the displacement q∗(t) from the equilibrium position at time t. If v0 is
the velocity at time t = 0, and the initial position is q∗(0) = 0, then the unique
solution is 
(It can be easily verified that this q∗ satisfies the equation of motion 
 as well as the initial conditions 
 and 
The maximum displacement is
and the period of oscillation is 
“Symmetries” of the Lagrangian give rise to “conservation laws”:
Law of Conservation of Energy. Since the Lagrangian L(x, v) does not depend
on t (that is, it possesses “the symmetry of being invariant under time
translations”), we will now see that this results in the Law of Conservation of

Energy. 
Define 
the 
energy 
E(t) 
along 
q∗ 
at 
time 
t 
by 
Then we have
Hence the energy E is constant, that is, it is conserved.
Law of Conservation of Momentum. Now suppose that the Lagrangian does
not depend on the position, that is, L(x, v) = l(v) for some function l. Define the
momentum p∗t along q∗ at time t by 
Then
and so p∗ is constant, that is, the momentum is conserved.
Remark 3.2. (Noether’s Theorem).
The above two results are special cases of a much more general result, called
Noether’s Theorem, roughly stating that every differentiable symmetry of the
action has a corresponding conservation law. This result is fundamental in
theoretical physics. We refer the interested reader to the book [Neuenschwander
(2011)].
Example 3.10. (Particle in a Potential Well).
Consider a particle of mass m moving along a line, and which is acted upon by a
force
generated by a potential V. The associated Lagrangian is
Suppose that the motion of the particle is described by q∗ for t 
 0. If 

then for all t  0, we have by the Law of Conservation of Energy, that
and so 
 This implies that V(q∗(t))  E. Hence the
particle cannot leave the potential well if V(x) → ∞ as x → ±∞.
If the velocity of the particle is always positive while moving from initial
position x0 at time t = 0 to a position x > x0 at time t, then by integrating, 
If in this manner, the particle reaches x1, where E = V(x1) (see the previous
picture), then we may ask if the travel time t1 from x0 to x1 is finite.
The above expression reveals that t1 < ∞ if and only if
In particular, in the case of the harmonic oscillator, where
we have that the time of travel from the initial condition x0 to the maximum
displacement 
xmax 
is 
finite, 
and 
is 
given 
by 

which is, as expected, one-fourth of the period of oscillation.
Hamiltonian Mechanics.
The momentum p∗ is defined by 
 Since 
 we have 
The Euler-Lagrange equation, 
 can be re-
written as
It turns out that the above two equations can be expressed in a much more
symmetrical 
manner, 
with 
the 
introduction 
of 
the 
Hamiltonian, 
as follows. Note that
Thus
These two equations are equivalent to 
 and the Euler-Lagrange
equation. The space {(q, p) ∈ R2} is called the phase plane, where the position-
momentum pairs live. Each point (q, p) in the phase plane is thought of as a
possible state of the particle. Given an initial state (q0, p0), the coupled first
order differential equations, describing the evolution of the state, namely the
Hamiltonian equations 
for t  0, describe a curve t 
 (q∗(t), p∗(t)) in the phase plane, called a phase
plane trajectory. The collection of all phase plane trajectories, corresponding to

various initial conditions, is called the phase portrait. The following picture
shows the phase portrait for the harmonic oscillator.
We also observe that the Hamiltonian H, evaluated along a phase plane trajectory
t  (q∗(t), p∗(t)), is 
the energy, which by the Law of Conservation of Energy, is a constant. So the
phase plane trajectories are contained in level sets of the Hamiltonian H.
Another proof of this constancy of the function H along phase plane trajectories,
based on the Hamiltonian equations, is given below (where we have suppressed
writing 
the 
argument 
t): 
This sort of a calculation can be used to calculate the time evolution of any
“observable” (q, p)  f(q, p) along phase plane trajectories in the phase plane, as
explained in the next paragraph.
Poissonian Mechanics.
All the (mechanical) physical characteristics are functions of the state. For
example in our one-dimensional motion of the particle, the coordinate functions
(q, p) 
 q and (q, p) 
 p give, for a state (q, p) of the particle, the position,
respectively the momentum, of the particle. Similarly, 
gives the energy. Motivated by these considerations, we take
as the collection of all observables.
We now introduce a binary operation {·, ·} : C∞;(R2) × C∞(R2) → C∞(R2),
which is connected with the evolution of the mechanical system.

Given two observables F and G in C∞(R2), define the new observable {F, G} ∈
C∞(R2), 
called 
the 
Poisson 
bracket 
of 
F, 
G, 
by 
The Poisson bracket can be used to express the evolution of an observable F.
Suppose that our particle moving along a line, evolves along the phase plane
trajectory (q∗, p∗) in the phase plane according to Hamilton’s equations for a
Hamiltonian H. Then the evolution of the observable F ∈ C∞(R2) along the
trajectory 
(q∗, 
p∗) 
is 
given 
by 
(again 
suppressing 
t): 
In particular, if {F, H} = 0 (as for example is the case when F = H!), then F is a
conserved quantity.
It can be shown that C∞(R2) forms a Lie algebra with the Poisson bracket,
that 
is, 
the 
following 
properties 
hold: 
for α, β ∈ R and any F, G, H ∈ C∞(R2). (H may not be the Hamiltonian!) We
will see in the next chapter, that the role of the Poisson bracket in classical
mechanics,
of observables F, G ∈ C∞(R2), is performed by the commutator 
of observables A, B (which are operators on a Hilbert space H) in quantum
mechanics.
Exercise 3.18. Prove (3.7)-(3.9).
Exercise 3.19. (Position and Momentum).
Let Q ∈ C∞(R2) be the position observable, 
 and P ∈ C∞(R2) be
the momentum observable 
 Show that {Q, P} = 1.

1See for example [Luenberger (1969)].
2In fact, not even a “local” minimiser because ||x − 0||∞ = α can be chosen as small as we please.
3A coordinate system is inertial if particles which are “free” that is, not acted upon by any force, move in
straight lines with a uniform speed.
4Here we tacitly ignore the fact that the set S doesn’t quite have the form that we have been considering,
since we have the extra constraint |x′(t)| < 1 for all ts. Despite this extra condition, a version of Corollary
3.1 holds, mutatis mutandis, with an appropriately adapted proof: instead of X := {h ∈ C1[a, b] : h(a) = 0 =
h(b)}, we work in the open subset X0 := {h ∈ X :|h′(t)| < 1 for all t ∈ [a, b]} of X. We won’t spell out the
details here, but we simply use the Euler-Lagrange equation in this exercise.
5This surface has the least area with a helix as its boundary.
6It is standard to use “Least” rather than “Stationary” because in many cases the action is actually
minimised.

Chapter 4
Geometry of inner product spaces
In a vector space we can add vectors and multiply vectors by scalars. In a
normed space, the vector space is also equipped with a norm, so that we can
measure the distance between vectors. The plane R2 or the space R3 are
examples of normed spaces. However, in the familiar geometry of the plane or of
space, we can also measure the angle between lines, using the “dot product” of
two vectors. Then cos θ = (  · 
)/(|| || || ||). In particular, nonzero vectors 
and  are perpendicular if and only if  ·  = 0.
We wish to generalise this notion to abstract spaces, so that we can talk about
perpendicularity or orthogonality of vectors even in vector spaces of sequences,
functions, etc., and this is what we will do in this chapter, by equipping vector
spaces with an “inner product” (analogous to the dot product from R2, R3).
Why do we care about orthogonality in more general spaces? There are
several reasons for this. It gives a natural generalisation of the familiar R2, R3
case, and allows one to solve shortest distance and best approximation problems.
Consider the figure on the left in the following picture. We are given a convex
set C and a point X ∉ C. Then the closest point P∗ in C to X is characterised by a
geometric property involving angles: XP∗ makes an obtuse angle with P∗ Q,
where Q is any point in C. Similarly, let us now consider the figure on the right
of the following picture, where we are given a plane P and a point X ∉ P. Then
the closest point P∗ ∈ P is the unique point such that XP∗ is perpendicular to
P∗Q, where Q is any point in P. We will learn about the generalisation of these

results in inner product spaces. These results in turn find applications in Fourier
Analysis, and in more general “orthonormal basis” expansions. Finally, inner
products will also simplify duality theory, and we will learn this later on, when
we discuss the Riesz Representation Theorem in Hilbert spaces.
4.1 Inner product spaces
Definition 4.1. (Inner product space; inner product).
Let X be a vector space X over K (= R or C). A function ·, ·  : X × X → K is
called an inner product on the vector space X over K if:
(IP1) (Positive definiteness)
For all x ∈ X, x, x   0. If x ∈ X and x, x  = 0, then x = 0.
(IP2) (Linearity)
For all x1, x2, y ∈ X, x1 + x2, y  = x1, y  + x2, y .
For all x, y ∈ X and all α ∈ K, αx, y  = α x, y .
(IP3) (Conjugate symmetry)
For all x, y ∈ X, x, y  = y, x ∗. (Here ·∗ = complex conjugation).
A vector space X, equipped with an inner product ·, ·  : X × X → K, is called an
inner product space.
It follows, from (IP2) and (IP3), that the inner product is also antilinear with
respect to the second variable, that is, it is additive, and such that x, αy  = α∗x,
y  for all x, y ∈ X and α ∈ K. Moreover, for a real vector space X, note that (IP3)
asserts that for all x, y ∈ X, x, y  = y, x .
We will now see that every inner product space is automatically a normed
space, with the norm given by
We emphasise again that by (IP1), we know that x, x   0 for all x ∈ X, and so,
the unique nonnegative real square root ||x|| of x, x  exists. We will call this || · ||,
given by ||x|| = 
 for x ∈ X, the norm induced from the inner product on X.

Let us check that || · || satisfies (N1), (N2), (N3).
(N1) For all x ∈ X, ||x|| = 
  0.
If x ∈ X and ||x|| = 0, then x, x  = 0, and by (IP1), x = 0.
(N2) For all x ∈ X and α ∈ K, we have
To show (N3), we will first show the following result, which is an important
inequality in its own right.
Theorem 4.1. (Cauchy-Schwarz In/equality).
If X is an inner product space, then for all x, y ∈ X, | x, y |  ||x|| ||y||. For x, y ∈
X, | x, y | = ||x|| ||y|| if and only if x, y are linearly dependent.
Proof. Let x ∈ X.
If y = 0, then | x, 0 | = | x, 0 · 0 | = |0 x, 0 | = 0 = ||x||0 = ||x|| ||0||.
Now consider y ∈ X\{0} and let α ∈ K. Then we have
Write x, y  = | x, y  |eiθ for some θ ∈ R. Take α = reiθ, where r ∈ R. Then |α| = r,
α∗ = re−iθ, and
Thus we obtain (from 0  ||x||2 + 2Re(α∗x, y ) + |α|2||y||2) that for all r ∈ R,
r2||y||2 + 2| x, y |r + ||x||2  0. Taking

we obtain 
, that is, | x, y |  ||x|| ||y||.
Next, we’ll show that the equality holds if and only if x, y are dependent.
(If part).
If y = 0, then both sides, | x, y  | and ||x|| ||y||, are equal to 0.
If y ≠ 0, then since x, y are dependent, there exist c1, c2 ∈ K with c1 ≠ 0, such
that c1x + c2y = 0, and so x = λy for some λ ∈ K. Hence
(Only if part).
If y = 0, then 0 · x + 1 · y = 0, and so x, y are linearly dependent.
If y ≠ 0, then define α = −x, y /||y||2. We have
and so 1 · x + α · y = 0, that is, x, y are linearly dependent.
Now that we’ve obtained this important inequality, let us complete our
verification that every inner product space is a normed space with the induced
norm ||x|| := 
, x ∈ X. We had already checked (N1) and (N2), and it only
remains to verify (N3).
(N3) For x, y ∈ X, we have
and so ||x + y||  ||x|| + ||y||.

Example 4.1. Let K = R or C. Then Kd is an inner product space with the inner
product x, y  := x1y∗1 + ··· + xdy∗d, for x = (x1, ···, xd) ∈ Kd and y = (y1, ···, yd) ∈
Kd. Note that the induced norm is
the Euclidean 2-norm || · ||2. The Cauchy-Schwarz Inequality in the inner product
space Cd gives
for complex x1, ···, xd, y1, ···, yd.
Example 4.2. The vector space ℓ2 of square summable (real or complex)
sequences is an inner product space with the inner product
for x = (xn)n∈N, y = (yn)n∈N ∈ ℓ2. The series converges since
We have 
.
Also, the Cauchy-Schwarz inequality gives

Example 4.3. The space of continuous K-valued functions on [a, b] can be
equipped with the inner product
for x, y ∈ C[a, b]. The induced norm matches the || · ||2 norm:
The Cauchy-Schwarz inequality gives, for real-valued x, y ∈ C[a, b] that
But we had seen (page 36) that C[a, b] is not a Banach space with the || · ||2-
norm. L2[a, b] remedies the noncompleteness of (C[a, b], || · ||2):
is an inner product space with the inner product
for [x], [y] ∈ L2[a, b]. The induced norm on L2[a, b] is
and L2[a, b] is complete with this norm.
In light of the previous example, it makes sense to introduce the following.
Definition 4.2. (Hilbert space). An inner product space which is a Banach space
with the induced norm is called a Hilbert space.

Thus Rd, Cd, ℓ2, L2[a, b] are all Hilbert spaces (with the aforementioned
respective inner products), while (C[a, b], || · ||2) isn’t. We can ask: What about
(C[a, b], || · ||∞)? We’d seen it is a Banach space. Is it also a Hilbert space, that is,
is the || · ||∞-norm induced by some inner product? The answer is “no”! And the
reason is that an inner product endows the vector space with a certain geometry,
and forces the distance function (≡ norm) to behave in a certain “rigid” manner.
To see what we mean, let us show the following result.
Theorem 4.2. (Parallelogram Law).
If X is an inner product space with induced norm || · ||,
then for all x, y ∈ X, ||x + y||2 + ||x − y||2 = 2||x||2 + 2||y||2.
The name is justified, since the identity is a generalisation of the situation in
Euclidean R2, where we know that the sum of the squares of the lengths of the
diagonals of a parallelogram equals the sum of the squares of the lengths of the
sides of the parallelogram.
Proof. We have
Adding these, we obtain ||x + y||2 + ||x − y||2 = 2||x||2 + ||y||2, as wanted.
Remark 4.1. (The inner product is recovered from the induced norm via the
Polarisation Identity). We have, upon subtracting (4.1) and (4.2), that Re( x, y )
=  (||x + y||2 − ||x − y||2). In the case of real scalars:
In the complex case, Im( x, y ) = Re(−i x, y ) = Re( x, iy ), so that

This is called the Polarisation Formula. The inner product is thus uniquely
determined by the induced norm.
It can also be shown that if X is a vector space with a norm || · || that satisfies
the Parallelogram Law, then the norm is induced by the inner product ·, ·  given
by the Polarisation Identity! The interested reader is referred to [Day (1973),
page 153].
Remark 4.2. (Pythagoras’s Theorem).
If x and y are orthogonal, that is, x, y  = 0, then, using (4.1),
Example 4.4. (ℓp is an inner product space ⇔ p = 2).
With x = e1 := (1, 0, ···) and y = e2 := (0, 1, 0, ···) in ℓp, we have
So we have ||x + y||2
p + ||x − y||2
p = 2||x||2
p + 2||y||2
p if and only if
that is, if and only if p = 2. So if p ≠ 2, the Parallelogram Law fails for the || · ||p
norm on ℓp, showing that this || · ||p norm is not induced from any inner product
on ℓp. We had already seen that the || · ||2 norm on ℓ2 is induced from an inner
product on ℓ2.
Exercise 4.1. Show that the supremum norm || · ||∞ on C[0, 1] is not induced by
any inner product on C[0, 1].
Exercise 4.2. (Appollonius Identity). Let X be an inner product space.
Prove that for all x, y, z ∈ X, ||z − x||2 + ||z − y||2 = 
.
This is called the Appollonius Identity.

Give a geometric interpretation when X = R2.
The hierarchy of spaces, with a few key examples, is depicted below.
Remark 4.3. Although we’ve shown a strict containment of normed spaces
within vector spaces, we remark that every vector space V can be made into a
normed space with a norm devised from a Hamel basis B for V, as follows: for
each x ∈ V, there exists an n ∈ N, scalars c1, ···, cn ∈ K, and vectors b1, ···, bn ∈
B such that x = c1b1 + ··· + cnbn. Set ||x|| = |c1| + ··· + |cn|. Then it can be checked
that || · || is a well-defined norm on V. But this may not be the
“appropriate/natural” norm in the vector space.
Exercise 4.3. (Continuity of ·, · ).
Let X be an inner product space and suppose that (xn)n∈N, (yn)n∈N are sequences
in X which converge to x, y ∈ X, respectively. Show that ( xn, yn )n∈N is
convergent in K with limit x, y .
Exercise 4.4. Prove, using the Cauchy-Schwarz Inequality, that given an ellipse
and a circle having equal areas, the perimeter of the ellipse is larger.
Hint: Consider a 90° rotation of the ellipse.
Exercise 4.5. If A, B ∈ Rm×n, then define A, B  = tr(A B), where A  denotes the
transpose of the matrix A. Prove that ·, ·  defines an inner product on the space
of m × n real matrices. The norm induced by this inner product on Rm×n is called

the Hilbert-Schmidt norm. Is (Rm×n, ·, · ) a Hilbert space?
Exercise 4.6.
(1) Let X be an inner product space over C, and let T ∈ CL(X) be such that for
all x ∈ X, T x, x  = 0. Prove that T = 0. Hint: Consider T(x + y), x + y , and
also T(x + iy), x + iy . Finally take y = T x.
(2) On the other hand, consider the continuous linear transformation T ∈
CL(R2) corresponding to an anticlockwise rotation by 90°: T = 
.
Check that Tx, x  = 0 for all x ∈ R2, but clearly T ≠ 0.
Have we got a contradiction to the previous part?
Exercise 4.7. (∗)(Completion of inner product spaces).
If an inner product space (X, ·, · X) is not complete, then this means that there
are some “holes” in it, as there are Cauchy sequences that are not convergent–
roughly speaking, the “limits that they are supposed to converge to”, do not
belong to the space X. One can remedy this situation by filling in these holes,
thereby enlarging the space to a larger inner product space (X, ·, · X) in such a
manner that:
(C1) X can be identified with a subspace of X and ∀x, y ∈ X, x, y X = x, y X.
(C2) X is complete.
Given an inner product space (X, ·, · X), we now give a construction of an inner
product space (X, ·, · X), called the completion of X, that has the properties (C1)
and (C2).
Let C be the set of all Cauchy sequences in X.
If (xn)n∈N, (yn)n∈N are in C, then define the relation1 R on C as follows:
Prove that R is an equivalence relation on C.
Let X be the set of equivalence classes of C under the equivalence relation R.
Suppose that the equivalence class of (xn)n∈N is denoted by [(xn)n∈N].

Define vector addition + : X × X → X and scalar multiplication · : K × X → X by
[(xn)n∈N] + [(yn)n∈N] = [(xn + yn)n∈N] and α · [(xn)n∈N] = [(αxn)n∈N].
Show that these operations are well-defined. (It can be verified that X is a vector
space with these operations, but we will accept these straightforward
verifications on faith.)
Define ·, · X : X × X → K by [(xn)n∈N], [(yn)n∈N] X = 
xn, yn X.
Prove that ·, · X is well-defined, and is an inner-product on X.
Define the map ι : X → X as follows: if x ∈ X, then ι(x) = [(x)n∈N], that is, ι takes
x to the equivalence class of the (constant) Cauchy sequence (x, x, x, . . .).
Show that ι is an injective bounded linear transformation (so that X can be
identified with a subspace of X), and that for all x, y in X, x, y X = ι(x), ι(y) X.
We now show that X is a Hilbert space. Let 
 be a Cauchy sequence in
X. For each k ∈ N, let nk ∈ N be such that for all n, m  nk, 
.
Define the sequence (yk)k∈N by 
, k ∈ N. We claim that (yk)k∈N ∈ C.

Let  > 0. Choose N ∈ N such that 1/N < . Let K1 ∈ N be such that for all 
, that is, 
.
Define K = max{N, K1}, and let k, l > K. Then for all n > max{nk, nl},
So ||yk − yl||X   + 
 +  <  +  +  = 3 .
This shows that (yn)n∈N ∈ C, and so [(yn)n∈N] ∈ X. We will prove that 
 converges to [(yn)n∈N] ∈ X as k → ∞. Given  > 0, let K1 be such
that 1/K1 < . As (yk)k∈N is a Cauchy sequence, there exists a K2 ∈ N such that
for all k, l > K2, ||yk − yl||X < . Define K = max{K1, K2}. Then for all k > K and
all m > max{nk, K}, we have
Hence 
Remark 4.4. We had seen that the inner product space C[a, b] with the induced
norm || · ||2 is not complete. However, it can be completed by the process
discussed above. The completion is isomorphic to L2[a, b]. The new inner
product as constructed above is expressible as an integral, namely the Lebesgue
integral. As we had mentioned earlier, for continuous functions on [a, b], the

Lebesgue integral is the same as the Riemann integral, that is, it gives the same
value. The L2 Hilbert spaces arise naturally in Probability Theory (see the
remark made in Example 1.12 on page 14). The space of random variables X on
a probability space (Ω, M, μ) for which E(X2) < +∞, where E(·) denotes
expectation, is a Hilbert space with the inner product X, Y  = E(XY).
4.2 Orthogonality
Two vectors  and  in R2 are perpendicular/orthogonal2 if their dot product 
· 
 is 0. Since an inner product on a vector space is a generalisation of the
notion 
of 
dot 
product, 
we 
can 
analogously 
talk 
about
perpendicularity/orthogonality of vectors in the general setting of inner product
spaces.
Definition 4.3. (Orthogonal vectors; orthonormal set).
Let X be an inner product space.
(1) Vectors x, y ∈ X are said to be orthogonal if x, y  = 0.
(2) A subset S of X is said to be an orthonormal set if
(a) for all x, y ∈ S with x ≠ y, x, y  = 0, and
(b) for all x ∈ S, x, x  = 1.
Example 4.5.
(1) In 
 is an orthonormal set.
(2) In ℓ2, 
 is an orthonormal set.
If δij denotes the Kronecker delta, that is,
then we have ei, ej  = δij.
(3) Consider C[0, 1] with the usual inner product.
For n ∈ Z, and let Tn ∈ C[0, 1] be given by

Then the set {Tn : n ∈ Z} is an orthonormal set. Indeed, if m, n ∈ Z and n ≠
m, then we have
On the other hand, for all n ∈ Z,
So {···, e−4πit, e−2πit, e0 = 1, e2πit, e4πit, ···} is an orthonormal set in C[0, 1].
For vectors belonging to the span of orthonormal set, there is a special relation
between the coefficients obtained in the relevant linear combinations, the norms
and the inner products. Indeed if
and the uks are orthonormal, then we have
and
Theorem 4.3. (Orthonormality⇒linear independence).
Let X be an inner product space, and S ⊂ X be an orthonormal set.
Then S is linearly independent.
Proof. Let x1, ···, xn ∈ S and α1, ···, αn ∈ K be such that

For j ∈ {1, ···, n}, we have
Consequently, S is linearly independent.
So every orthonormal set in X is linearly independent. Conversely, given a
linearly independent set in X, we can construct an orthonormal set such that the
span of this new constructed orthonormal set is the same as the span of the given
independent set. We explain this below, and this algorithm is called the Gram-
Schmidt orthonormalisation process.
Theorem 4.4. (Gram-Schmidt orthonormalisation).
Let {x1, x2, x3, ···} be a linearly independent subset of an inner product space X.
Define
Then {u1, u2, u3, ...} is an orthonormal set in X and for all n ∈ N,
span{x1, ···, xn}= span{u1, ···, un}.
Proof. We’ll use induction to show that for all n ∈ N,
n = 1: {x1} being independent, we know that x1 ≠ 0. So u1 = x1/||x1|| is well-
defined, and ||u1|| = ||x1||/||x1|| = 1. Also as u1 = x1/||x1|| and x1 = ||x1||u1, it follows
that span{x1}= span{u1}.
Suppose that the claim is true for some n, and suppose that {x1, ···, xn+1} is a
linearly independent set. Then in particular, {x1, ···, xn} is linearly independent

too. By the induction hypothesis {u1, ···, un} is an orthonormal set with span{x1,
···, xn} = span{u1, ···, un}. Since {x1, ···, xn+1} is a linearly independent set, xn+1
∉ span{x1, ···, xn} = span{u1, ···, un}. Hence v := xn+1 − ( xn+1, u1  u1 + ··· +
xn+1, un  un) ≠ 0. So the vector un+1 = v/||v|| is well-defined. Trivially, ||un+1|| = 1.
For all k  n we have
So un+1, uk  = 0 for all k  n. Hence {u1, ···, un, un+1} is an orthonormal set.
Finally, as 
, we have
By mathematical induction, the proof is complete.
Exercise 4.8. Let X be an inner product space, and let (xn)n∈N be a sequence in X
such that {xn : n ∈ N} is a linearly independent set. If (vn)n∈N is a sequence such
that {vn : n ∈ N} is an orthonormal set and
then show that for all n ∈ N, there exists a scalar αn with |αn| = 1 and vn = αnun,
where u1, u2, u3, ··· is the sequence of vectors obtained by carrying out the
Gram-Schmidt Orthogonalisation Procedure on the sequence x1, x2, x3, ···.
Example 4.6. (Legendre’s Polynomials).
The set of monomials {1, t, t2, t3, ···} form a linearly independent system in
C[−1, 1]. If Pn is the subspace of all polynomials of degree at most n, then Pn =
span{1, t, ···, tn}. Let us apply the Gram-Schmidt Orthonormalisation Procedure
to the set of monomials. This results in a sequence {un)n∈N of polynomials.
Since un ∈ Pn and un is orthogonal to every element of Pn−1, it follows that the
degree of un is precisely n. Thus there exists a sequence of polynomials (un)n
0

such that deg un = n, n  0 and
By Exercise 4.8, it follows that any sequence {Pn)n
0 with the properties that
deg Pn = n, n  0, and
is the same as the sequence (un)n
0 except for an overall multiplying numerical
constant. We’ll produce such a sequence {Pn)n
0, called the sequence of
Legendre’s Polynomials. We shall see that Pn(1) ≠ 0. Classically, these
polynomials Pns are not normalised by demanding that ||Pn||2 = 1, but rather by
arranging that Pn(1) = 1. We define P0(t) = 1 and for n ∈ N:
This is known as Rodrigues’s Formula. Then the polynomials Pn, with n  0,
have the following properties:
(1) Pn is a polynomial of degree n, and Pn(1) = 1.
(2) The polynomials are orthogonal: 
 for n ≠ m.
(3) The norm of Pn is given by 
Since (t2 − 1)n is a real polynomial with the leading term t2n, it is clear that Pn is
a real polynomial of degree n.
Let us now calculate Pn(1). Consider in general the derivative of order m  n of
the function (t2 − 1)n. Thanks to Leibniz’s Formula, we have
It is clear that if m = n, then all terms vanish for t = 1, except the one with k = n,
which yields

This gives Pn(1) = 1 if n  1.
For 1  k  m < n, 
 has a factor t −1, and so it is 0 for t = 1.
Also, 
 is zero for t = −1, for 1  k  m < n.
Consequently,
Now we show that Pn is orthogonal to Pn−1 = span{1, ···, tn−1}.
After m integrations by parts, we get for 1  m < n:
For m = 0, this follows from the Fundamental Theorem of Calculus.
In particular, it follows that if m < n, then 
So this also for n ≠ m by interchanging the roles of n and m.
To calculate the norm of Pn, we use integration by parts to first note that
The last integral can again be calculated using integration by parts, where the
boundary terms containing factors (1 − t) and (t + 1) become zero:

Thus we have: 
.
Using Rodrigues’s Formula, we obtain:
We won’t treat some less evident properties of Legendre’s polynomials, such as
the inequality |Pn(t)|  1 for all t ∈ [−1, 1]. Legendre’s polynomials also appear
naturally as a solution to an ODE (see Exercise 4.10), for instance in Physics
(when solving the Laplace equation and related PDEs in spherical coordinates—
in particular, in Quantum Mechanics for orbital angular momentum
calculations).
Exercise 4.9. Show that Pn is an even function if n is even, and odd if n is odd. In
particular: Pn (−1) = (−1)n.
Exercise 4.10. Show that the Legendre polynomial Pn solves
or equivalently,

Hint: Let y(t) = (t2 − 1)n. Verify that (t2 − 1)y′(t) = 2nty(t), and differentiate this
equation n + 1 times using Leibniz’s Formula.
Exercise 4.11. Show that Pn has n real zeros, and that these belong to (−1, 1).
Hint: We have seen that 
 is equal to zero at ±1 for 0  k  n − 1.
By induction on k, using Rolle’s Theorem show that 
 has at least k
zeros in (−1, 1).
Exercise 4.12. Recall Exercise 4.5, page 163, where we defined an inner product
on Rm×n by setting A, B  := tr[(A  B), A, B ∈ Rm×n.
Find an orthonormal basis3 for (Rm×n, ·, · ).
Exercise 4.13. (Hermite functions and the quantum harmonic oscillator).
(1) For nonnegative integers n, set Hn(x)= (−1)nex2 (d/dx)ne−x2, x ∈ R.
These are called the Hermite polynomials.
Justify the name “polynomial” as follows. First, note that H0 = 1.
Next, establish the recursion relation Hn+1(x)= 2xHn(x) − H′n(x).
Conclude that each Hn is indeed a polynomial.
Show that the leading term of Hn is 2nxn. In particular Hn has degree n.
Determine H0, H1, H2, H3.
(2) Show that φn := e−x2/2 Hn, n  0, belong to L2(R) and are orthogonal.
What is the L2-norm of Hn? (φnare called Hermite functions.)
(3) Show that (x − d/dx)φn = φn+1 for all n  0.
In particular, φn = Dnφ0, where D is the differential operator x − d/dx.
(4) Show that (x + d/dx)φn= 2nφn−1 for all n  1.
(5) Show that the φn satisfy ( − (d/dx)2 + x2)φn = (2n + 1)φn, n  0.
Hint: Expand (x + d/dx)(x − d/dx) using (d/dx)xφ − x(d/dx)φ = φ.
(6) Let a > 0 and let ψn(x)= φn 
, n  0.
Show that ( − (d/dx)2 + a2x2)ψn= (2n + 1)aψn, n  0.
Quantum harmonic oscillator: Find values of En such that the ψn, n 
 0,
satisfy the Schrödinger equation with ω > 0:

Exercise 4.14. Let {un : n ∈ N} ⊂ H be an orthonormal set in a Hilbert space H.
Show that 
 converges, but not absolutely.
Exercise 4.15. (Bessel’s Inequality).
Let {un : n ∈ N} be an orthonormal set in an inner product space X.
Show that for all x ∈ X, 
Orthogonal complement of a subspace
Definition 4.4. (Orthogonal complement).
Let Y be a subspace of an inner product space X.
The orthogonal complement Y⊥ of Y is defined by
Proposition 4.1. If Y is a subspace of an inner product space X, then Y⊥ is a
closed subspace of X.
Proof. Y⊥ is a subspace of X:
(S1) 0 ∈ Y⊥ since 0, y  = 0 for all y ∈ Y.
(S2) If x1, x2 ∈ Y⊥, then x1 + x2, y  = x1, y  + x2, y  = 0 + 0 = 0 for all y ∈ Y,
and so x1 + x2 ∈ Y⊥.
(S3) If α ∈ K and x ∈ Y⊥, then α · x, y  = α x, y  = α · 0 = 0 for all y ∈ Y, and
so α · x ∈ Y⊥.
Let (xn)n∈N be a sequence in Y⊥, which converges in X to x ∈ X.
Then for all 
Hence x ∈ Y⊥. Consequently Y⊥ is closed.
Example 4.7.
(1) Let {e1, e2, e3} be the standard orthonormal basis in R3 with the usual
Euclidean inner product. Then we have (span{e1, e2})⊥ = span{e3}.

(2) If e1 := (1, 0, ···) and e2 = (0, 1, 0, ···) in ℓ2, then
Exercise 4.16. Let Y be a subspace of an inner product space X.
Show that Y ∩ Y⊥ = {0}.
Exercise 4.17. Let Y be a subspace of an inner product space X.
(1) Prove that Y ⊂ (Y⊥)⊥.
(2) Show that if Y ⊂ Z, where Z is another subspace of X, then Z⊥ ⊂ Y⊥.
(3) Prove that Y⊥ = 
⊥, where  denotes the closure of Y.
(4) Show that if Y is dense in X, then Y⊥ = {0}.
(5) Let Yeven be the subspace of ℓ2 of all sequences whose oddly indexed terms
are zeros. Describe 
 Show that 
.
(6) What is 
 in ℓ2? Show that 
.
(Thus for a subspace Y, it can happen that Y ≠ Y⊥⊥.
We’ll see later that for a closed subspace Y in a Hilbert space H, Y = Y⊥⊥.
But for general subspaces Y, we can only say that Y⊥⊥ = .)
4.3 Best approximation
Finite dimensional subspace
One reason that orthonormal sets are useful is that it enables us to compute the
best approximation in a given finite dimensional subspace to a given vector in an
inner product space X. Thus, the following optimisation problem can be solved:
Let Y = span{u1, ···, un}, where u1, ···, un ∈ X is an orthonormal basis for Y.
Given x ∈ X, find y∗ ∈ Y that minimises ||x − y||, subject to y ∈ Y.

Theorem 4.5.
Let (1) X be an inner product space,
(2) x ∈ X, and
(3) Y = span{u1, ···, un}⊂ X,
where {u1, . . . , un} is an orthonormal basis for Y.
Define 
Then y∗ ∈ Y is such that for all y ∈ Y, ||x − y||  ||x − y∗||.
Proof.
If y ∈ Y, then there exist scalars c1, ···, cn such that 
.
So for 
.
Then we have
In particular, for all y ∈ Y, x − y∗, y∗ − y  = 0. So for all y ∈ Y,
So for all y ∈ Y, ||x − y||  ||x − y∗||.
“Least square approximation problems” that arise in applications can be cast as
best approximation problems in appropriate inner product spaces. Here is an
example, and we’ll see more in the exercises.

Example 4.8. Suppose that f is a continuous function on [a, b] and we want to
find a polynomial p∗ of degree at most m such that the ‘error’
is minimised. Let X = C[a, b] with the usual inner product, and let Pm be the
subspace of X comprising all polynomials of degree at most m. Then Theorem
4.5 gives a method of finding such a polynomial p∗. Let us take a concrete case.
Let a = −1, b = 1, f (t) = et for t ∈ [−1, 1], and m = 2. As
by a Gram-Schmidt orthonormalisation process (Example 4.6, page 168),
where P0, P1, P2 are orthogonal, and their span equals P2. We have
and so from Theorem 4.5, we obtain
This polynomial p∗ is a polynomial of degree at most 2 that minimises the L2-
norm error (4.3) on the interval [−1, 1] when f is the exponential function.

In the picture above, the dots indicate the graph of the exponential function,
while the solid line is the graph of the polynomial p∗.
Exercise 4.18. (Least Squares Regression). Suppose we are interested in two
variables x and y, which are related via a linear function y = mx + b. Suppose that
m and b are unknown, but one has measurements (x1, y1), ···, (xn, yn) from
experiments. However there are errors, so that the measured values yi are related
to the measured values xi by yi = mxi + b + ei, where ei is the (unknown) error in
measurement i. What are the “best” values of m and b? This is a very common
situation occurring in the applied sciences.
If the m and b we guessed were correct, then most of the errors ei := yi − mxi + b
should be reasonably small. So to find the “best” m and b, we should find the m
and b that make the ei’s collectively the smallest in some sense. So we introduce
the error
and seek m, b such that E(m, b) is minimised. Convert this problem into the
setting of a best approximation problem to a vector in a subspace.

The table shows the data on energy consumption and mean temperature in the
various months of the year. Draw a scatter chart, and fit a regression line of
energy consumption on temperature. What is the intercept, and what does it
mean in this case? What is the slope, and how could one interpret it? Use the
regression line to forecast the energy consumption for a month with mean
temperature 9°C.
Convex set case
What if the subspace Y in the previous section was not necessarily finite
dimensional? We will consider this in the next subsection, but first let us
consider the far more general case of a closed convex set, (not just in an inner
product space, but rather) in a Hilbert space.
Recall that a subset C of a vector space X is said to be convex if for all α ∈
(0, 1) and all x, y ∈ C, we have that (1 − α)x + αy ∈ C. Now suppose that we are
given: a Hilbert space H, a closed convex subset C ⊂ H, and a point x ∈ H. We
consider the following optimisation problem:
In Theorem 4.6 below, we give a characterisation of c∗ ∈ C that solves this
optimisation problem.
Theorem 4.6. Let H be a Hilbert space, C be a closed, convex, nonempty subset
of H, and x ∈ H. Then:
(1) There exists a unique c∗ ∈ C such that for all c ∈ C, ||x − c∗||  ||x − c||.
(2) The point c∗ ∈ C can be characterised by the following property:

Remark 4.5. (Geometric interpretation of condition (4.4) in Theorem 4.6):
Cauchy-Schwarz gives, for all x, y ∈ H\{0}: 
The angle θx,y ∈ [0, π] between the nonzero vectors x and y is defined by
Thus the condition (4.4) in Theorem 4.6 has the geometric interpretation that the
angle between the vectors x − c∗ and c − c∗ is obtuse, that is, it belongs to [π/2,
π].
Proof.
(1) Existence: Let 
 (the distance of x to C).
Then there exists a sequence (cn)n∈N in C such that 
.
Such a sequence is called a minimising sequence.
Now we show that a minimising sequence is a Cauchy sequence.
To do this, we apply the Parallelogram Law to the vectors x − cn and x − cm,
and divide by 4. This gives:
Since C is convex, 
, so that 
.
Thus 
Given  > 0, there is an N such that for all n > N, 
.
Hence, by (∗), for n, m > N, ||cn − cm|| < . Because H is complete, it follows
that the Cauchy sequence (cn)n∈N has a limit c∗, and this belongs to C since
C is closed. Now we have d = 
 ||x − cn|| = ||x − c∗||.
Consequently, ||x − c∗ || = d  
 ||x − c′||  ||x − c|| for all c ∈ C.

Uniqueness: Assume that ∗ ∈ C is such that ||x − ∗||  ||x − c|| for all c ∈
C. Then it follows that d := 
 ||x − c|| = ||x − ∗||.
From the previous part, we also know that ||x − ∗|| = d.
So it follows that the sequence c∗, ∗, c∗, ∗, ··· is a minimising sequence.
But from the above, it follows that this sequence is a Cauchy sequence, and
hence convergent. But this sequence has subsequences c∗, c∗, c∗, ··· and ∗, 
∗, ∗, ··· that converge, respectively, to c∗, and to ∗.
Consequently, c∗ = ∗.
(2) Suppose that c∗ is the unique vector from part (1). Let c ∈ C.
Then for α ∈ (0, 1), αc + (1 − α)c∗ ∈ C, and so
By taking squares, we obtain:
or Re x − c∗, c − c∗  α||c∗ − c||2/2. Letting α go to 0, in the limit we obtain
Re x − c∗, c − c∗  0.
Now suppose that c∗ ∈ C is a vector that satisfies (4.4).
We wish to show that for all c ∈ C, ||x − c∗||  ||x − c||. Let c ∈ C.
We write x − c = x − c∗ + c∗ − c. Then
So c∗ is the point in C closest to x.
Exercise 4.19. Let H = L2(R) be the real Hilbert space of square integrable
functions on R, and let L2+(R) = {f ∈ H : f  0} (strictly speaking the set of
equivalence classes of nonnegative functions). Suppose that f ∈ H. Show that the
element in L2
+(R) closest to f is precisely f+ = max{f, 0} (that is, the simplest
nonnegative function associated with f!).

Subspace case
A special case of a convex set is a subspace. Thus we can apply Theorem 4.6 to
closed subspaces. Note that earlier we had seen the finite dimensional subspace
case in inner product spaces, whereas now we consider the case of (possibly
infinite dimensional) subspaces in a Hilbert space.
Theorem 4.7.
Let Y be a closed subspace of a Hilbert space H and let x ∈ H .
Then there exists a unique y∗ ∈ Y such that for all y ∈ Y, ||x−y∗||  ||x−y||.
The point y∗ ∈ Y can be characterised by: for all y ∈ Y, x − y∗, y = 0.
(We write x − y∗ ⊥ Y.)
Proof. The existence of y∗ ∈ Y follows from Theorem 4.6. Also, we know that
this unique y∗ ∈ Y is such that for all y′ ∈ Y, Re x − y∗, y′ − y∗  0. Now let y ∈
Y. Let θ ∈ [0, 2π) be such that x − y∗, y  = eiθ| x − y∗, y  |. Then with y′ := y∗ +
eiθy, we have y′ ∈ Y (because Y is a subspace). So
Hence x − y∗, y  = 0 for all y ∈ Y.
Next let ∗ ∈ Y be such that x − ∗, y  = 0 for all y ∈ Y. Let y′ ∈ Y. Then y := y′
− ∗ ∈ Y too (as Y is a subspace). So x − ∗, y′ − ∗ = 0. In particular, Re x − 
∗ y′ ∈ ∗ = 0 too. Thus ∗ = y∗.
Definition 4.5. (Orthogonal projection). Let Y be a closed subspace of the
Hilbert space H, and let x ∈ H. Then the unique y∗ ∈ Y (given by Theorem 4.7)
is called the orthogonal projection of x to Y. We will denote the orthogonal
projection of x to Y by PY x.

Theorem 4.8. Let Y be a closed subspace of the Hilbert space H .
(1) The map x 
 PY x : H → H belongs to CL(H).
(2) ran PY = Y.
(3) ||PY|| = 1 except if Y = {0}, in which case PY = 0 and ||PY|| = 0.
(4) ker PY = Y⊥.
(5) Every x ∈ H has a unique decomposition x = y + z, with y ∈ Y, z ∈ Y⊥.
(6) PY⊥ = I − PY.
(7) P2
Y = PY.
(8) PY is symmetric, that is, for all x, x′ ∈ H, PY x, x′  = x, PY x′ .
Proof.
(1) Let x1, x2 ∈ H. Then PY x1, PY x2 ∈ Y, and so is their sum.
Moreover, for all y ∈ Y,
Thus PY(x1 + x2) = PY x1 + PY x2.
Let x ∈ H and α ∈ K. Then PYx ∈ Y and so αPY x ∈ Y too.
Moreover, for all y ∈ Y, αx − αPY x, y  = α x − PY x, y  = α · 0 = 0.
So PY(αx) = αPY x.
Hence PY ∈ L(H). Next we’ll show continuity of PY.
For x ∈ X, and all y ∈ Y, x − PY x, y  = 0, that is, x − PY x ∈ Y⊥.
Thus ||x||2 = ||x − PY x + PY x||2 = ||x − PY x||2 + ||PY x||2  ||PY x||2, showing
that PY ∈ CL(H) and that ||PY ||  1.
(2) For all x ∈ H, PY x ∈ Y, and so ran PY ⊂ Y. Now suppose that y′ ∈ Y. Then y
′ − PY y′ ∈ Y too. But for all y ∈ Y, we have y′ − PY y′, y  = 0. In particular,
with y := y′ − PY y′, ||y′ − PY y′||2 = y′ − PY y′, y  = 0. Thus PY y′ = y′. Hence
Y ⊂ ran PY.
(Alternatively, one could just observe that for y ∈ Y, PY y = y, since y is the
closest member of Y to y, and this shows that y ∈ ran PY.)
(3) If Y = {0}, then PY = 0, and so ||PY|| = 0.
Suppose that Y ≠ {0}. Then there exists a y′ ∈ Y\{0}, and as we had seen
above, PY y′ = y′. So ||PY|| ||y′||  ||PY y′|| = ||y′||, giving ||PY||  1. Also, we had

established in part (1) that ||PY||  1. Thus ||PY|| = 1.
(4) Let x ∈ ker PY. Then PY x = 0. For all y ∈ Y,
and so x ∈ Y⊥. Thus ker PY ⊂ Y⊥.
Let x ∈ Y⊥. Then for all y ∈ Y, x − 0, y  = x, y  = 0. Thus PY x = 0. Hence
Y⊥ ⊂ ker PY too.
(5) If x ∈ X, then x − PY x ∈ Y⊥. Thus, taking y := PY x and z := x − PY x, we get
x = y + z with y ∈ Y and z ∈ Y⊥. Also, if y′ ∈ Y and z′ ∈ Y⊥ are such that x =
y′ + z′, then y + z = y′ + z′, and so we obtain that Y ∋ y − y′ = z′ − z ∈ Y⊥. But
Y ∩ Y⊥ = {0}. Hence y − y′ = 0 = z′ − z, that is, y = y′ and z = z′.
(6) We have seen that x = y + z, where y = PY x ∈ Y and z = x−PY x ∈ Y⊥. As Y⊥
is also a closed subspace and using Exercise 4.17(1) (page 173), it follows
from part (5) that PY⊥ x = z. Hence PY⊥ = I − PY.
(7) We have 
, and so by part (5), PY (PY x)= PY x.
(8) Let x, x′ ∈ H. Then we have
Thus PY is symmetric4.
Corollary 4.1. Let Y be a closed subspace of a Hilbert space H.
Then (Y⊥)⊥ = Y.
Proof. We have P(Y⊥)⊥ = I − PY⊥ = I − (I − PY) = PY.
So the ranges of P(Y⊥)⊥ and PY⊥ are equal, that is, (Y⊥)⊥ = Y.

Exercise 4.20. Let Y be a subspace of a Hilbert space H. Show that Y⊥⊥ = .
Exercise 4.21. Let H = L2(R) and let Y = {f ∈ L2(R) : f =  := f (−·)} be the
closed subspace of even functions in L2(R). Show that PY f = {f + )/2. Show
that Y⊥ is equal to the space of odd functions in L2(R). Determine PY⊥, and using
this, verify the fact that every element of L2(R) has a unique decomposition into
the sum of an even and an odd function in L2(R).
Exercise 4.22. Let H be a Hilbert space. Suppose that S ∈ CL(H) is such that S2
= I and S is symmetric (that is, Sx, y  = x, Sy  for all x, y ∈ H). Let Y = {x ∈ H :
Sx = x} and let Z = {x ∈ H : Sx = −x}. Show that Z = Y⊥, Y = Z⊥, and determine
the projections PY and PZ in terms of S.
Exercise 4.23. Let H = L2(R). Let A ⊂ R be a measurable set and let YA be the
set of elements f ∈ L2(R) such that f(x) = 0 for almost all x ∈ A = R\A. Show
that YA is a closed linear subspace, and determine the projection PA onto YA.
Exercise 4.24. Let D ⊂ H be a subspace of the Hilbert space H. Show that D is
dense in H if and only if D⊥ = {0}, that is, there does not exist a nonzero x ∈ H
that is orthogonal to D. Hint: Use the fact that  is a closed linear subspace.
4.4 Generalised Fourier series
Recall that using Zorn’s Lemma, we’d seen that every vector space X has a
Hamel basis, that is, a set B ⊂ X such that span(B) = X (so that every vector in X
can be written as a linear combination of elements of B) and (this representation
is not “wasteful”, and is “efficient” in the sense that) B is linearly independent.
However, if X is an inner product space, then the Hamel basis suffers from the
following two “flaws”:
(1) The notion of a Hamel basis is purely “algebraic”, that is, it refers only to
the vector space structure of X. But if X is an inner product space, then more
structure is available, and it would be great if the basis reflects this structure
too.
(2) A Hamel basis of an infinite dimensional Hilbert space is uncountable, and
there is no constructive procedure to find it. So it is “inconvenient” in this

context.
We’ll now see that as opposed to the Hamel basis, there is much nicer concept of
orthonormal basis in the context of inner product spaces, which does away with
the above deficiencies. Indeed, an orthonormal basis has good algebraic
properties (being independent) and good analytic properties (since it is an
orthonormal set, and hence it is nicely behaved with respect to the inner
product). Moreover, in the infinite dimensional case, for most examples one
meets in applications, it is a countable and an explicit set. As opposed to Hamel
basis, where we insist on using finite number of vectors from the Hamel basis at
a time to generate vectors (by taking linear combinations), we’ll see that an
orthonormal basis is more “efficient” in that limits of linear combinations are
also allowed for generation of vectors.
Definition 4.6. (Orthonormal basis). Let X be an inner product space. A set B ⊂
X is called an orthonormal basis for X if
(a) span(B) is dense in X, and
(b) B is an orthonormal set.
Example 4.9. Consider Rd with the usual Euclidean inner product.
Then 
 is an orthonormal basis for Rd.
Example 4.10. Consider ℓ2 with the usual inner product. For i ∈ N, let ei := (0,
···, 0, 1, 0, ··· ), the sequence with the ith term equal to 1 and all other terms
equal to 0. Then B := {ei : i ∈ N} is an orthonormal set, and span(B) = c00, which
is dense in ℓ2 (Exercise 1.24, page 21). Hence B is an orthonormal basis of ℓ2.
Exercise 4.25. Show that the polynomials are dense in C[−1, 1] with the usual
inner product. Hint: Weierstrass’s Theorem from Exercise 1.26, page 22.
Example 4.11. (Legendre’s Polynomials).
Let us revisit Example 4.6, page 168. From Exercise 4.25, it follows that the set
of polynomials 
 forms an orthonormal basis of C[−1, 1] (and of

L2[−1, 1]).
Example 4.12. (Trigonometric Polynomials).
Consider the example in item (3) on page 166. It can be shown that the set B :=
{Tn : n ∈ Z} is an orthonormal basis for X = C[0, 1] (or L2[0, 1]). We had seen
earlier that the Tns are orthonormal. The proof of the density of span B in X is
somewhat tedious, and we will not prove this here5.
The result below justifies the terminology “basis” in “orthonormal basis”.
Theorem 4.9. Let X be an inner product space with a countable orthonormal
basis {u1, u2, u3, ···}. Then:
(1) For all x ∈ X, 
.
(2) For all x, y ∈ X, 
.
(3) For all x ∈ X, 
.
If X = H is a Hilbert space with a countable orthonormal basis {u1, u2, ···}, then
for all (cn)n∈N ∈ ℓ2, we have 
.
Remark 4.6.
(1) It is tempting to say “the right-hand side of the decomposition in (1) is a
linear combination of the uns”, but this remark is faulty because a linear
combination by definition is a finite sum.
And “infinite linear combination” is considered troublesome terminology
owing to the persistent query about convergence. Instead we’ll call the
decomposition in part (1) of Theorem 4.9 a generalised Fourier series,
because of the similarity with the classical Fourier series, which will be
explored in Example 4.13 below.
(2) Recall that in elementary (finite dimensional) linear algebra, one learns the
following “representation result”:

Analogously, the theorem above contains an important representation result,
saying that
Indeed, consider the map
It can be checked that this map ι is linear, and thanks to the result in
Theorem 4.9, also continuous, one-to-one and onto. So it is an element of
CL(H, ℓ2), and as the map ι is bijective, it has a continuous inverse ι−1 ∈
CL(ℓ2, H). Thus ι is an isomorphism. As ||x||H = ||ι(x)||2, we call ι an isometric
isomorphism.
Exercise 4.26.
If H is a Hilbert space with a countable orthonormal basis {u1, u2, u3, ···}, then
using Theorem 4.9, show that the map x  ( x, u1 , x, u2 , x, u1 , ···): H → ℓ2 is
indeed an isometric isomorphism.
Proof. (Of Theorem 4.9)
(1) Let x ∈ X, and  > 0. As span{u1, u2, u3, ···} is dense in X, there exists an N
∈ N and scalars c1, ···, cN ∈ K such that
Let n > N, and Y = span{u1, ···, un}. As 
, by Theorem 4.5,
Thus 
 is convergent in X with limit x, that is,

(2) If x, y ∈ X, then
So
Thus x, y  = 
 x, un y, un
∗.
(3) This follows from (2) by taking y = x.
Now suppose that X = H, a Hilbert space.
Let (cn)n∈N ∈ ℓ2. Then 
 is a Cauchy sequence.
We will show that 
 is a Cauchy sequence. For n > m,
Thus 
 is a Cauchy sequence in H.
As X = H is a Hilbert space, 
 is convergent in H.
The identity
is called Parseval’s Identity.

Example 4.13. Consider the Hilbert space C[0, 1] with the usual inner product,
and the orthonormal basis {Tn : n ∈ N} from page 166.
By Theorem 4.9, every x ∈ C[0, 1] has the expansion: 
.
Note that x, Tn  is precisely the nth Fourier coefficient of x:
Hence the above expansion becomes 
.
But we should bear in mind that the above does not necessarily mean pointwise
convergence! All we know is that the sequence of partial sums sN, N ∈ N, given
by
converges to x in the || · ||2-norm: 
.
Exercise 4.27. Using t  t ∈ C[0, 1], show that 
.
Exercise 4.28. Show that an inner product space is dense in its completion.
Exercise 4.29. (Isoperimetric Theorem).
Among all simple, closed smooth curves of length L in the plane, the circle
encloses the maximum area. This can be proved by proceeding as follows.
Suppose that s 
 x(s), y(s)) is a representation of the curve, where s is the arc
length parameter. We may consider x, y as L-periodic functions.
So x, y have Fourier series expansions

Then
Using Parseval’s Identity, it can be shown that
We have 
.
So L2 = 2π2 
 n2(a2
n + b2
n + c2
n + d2
n).
The area A is given by 
.
Prove that L2 − 4πA  0 and that equality holds if and only if
which describes the equation of a circle.
Exercise 4.30. Which of the following two sets is an orthonormal basis for ℓ2?
Exercise 4.31. Show that an inner product space with a countable orthonormal
basis is separable.
Exercise 4.32. (Nonseparable Hilbert spaces).
Although an analogue of Theorem 4.9 can be shown for spaces with an
uncountable orthonormal basis, we won’t discuss this. One reason is that most
examples met in practice are separable.

A notable exception is the class of Besicovitch Almost Periodic Functions,
which is the completion of the span X of the exponentials {eiλx : λ ∈ R} under
the inner product
We won’t check that this does define an inner product. Rather, show that the
space of Besicovitch Almost Periodic Functions is not separable by calculating
the distance between eiλx and eiμx when λ ≠ μ.
Exercise 4.33.
Let {ui : i ∈ I} be an orthonormal set in an inner product space X, and let x ∈ X.
Show that x, ui  is nonzero for at most a countable number of the ui’s.
Hint: Using Bessel’s Inequality, for each n ∈ N, Un := {ui : | x, ui |2 > ||x||2/n} has
at most n − 1 elements. Conclude that U := {ui : x, ui  ≠ 0} must be at most
countable.
4.5 Riesz Representation Theorem
Let H be a Hilbert space, and y ∈ H. Then the map φy : H → C given by
is a continuous linear transformation on H. Indeed, we have:
(L1) For all x1, x2 ∈ H,
(L2) For all α ∈ C and x ∈ H, φy(α · x) = α · x, y  = α x, y  = αφy(x).
Continuity: For all x ∈ H, |φy(x)| = | x, y  |  ||y|| ||x||, by the Cauchy-Schwarz
Inequality.
Thus φy is an element of the dual space H′ = CL(H, C).
Now we will see that every element of the dual space is of this type!
So H′ can be “identified” with H.
This is called the Riesz Representation Theorem after (Frigyes)6 Riesz.
Theorem 4.10. (F. Riesz Representation Theorem).

If H is a Hilbert space, and φ ∈ CL(H, C),
then there exists a unique y ∈ X such that φ = φy, that is, for all x ∈ H, φ(x) = x,
y .
Proof. Let Y = ker φ. Then Y is closed. (Indeed, if Y ∋ xn → x, then φ(x) = φ(lim
xn)= lim φ(xn)= lim 0 = 0.)
If Y = H, then we may take y = 0.
If Y ≠ H, then let x0 ∈ H\Y.
Write x0 = PY x0 + PY⊥ x0, where PY⊥ x0 ≠ 0 because x0 ∉ Y.
Let us note that φ(PY⊥ x0) ≠ 0. Indeed,
For x ∈ H, 
 PY⊥ x0 ∈ ker φ = Y.
Set 
.
We have
that is, x, y  = φ(x).
Finally, we prove the claimed uniqueness. Suppose that y, y′ ∈ H are such that
for all x ∈ H, φ(x) = x, y  = x, y′ . Then for all x ∈ H, x, y − y′  = 0. In
particular, taking x = y − y′, we obtain
and so y = y′.
The above theorem characterises linear functionals on Hilbert spaces: they are
precisely inner products with a fixed vector! In this sense we can “identify” the
dual space H′ = CL(H, C) with H itself, at least set theoretically.

Exercise 4.34. Let H be a complex Hilbert space, y ∈ H, and φy ∈ CL(H, C) be
given by φy(x) = x, y .
(1) Show that ||φy|| in CL(H, C) equals ||y|| as an element of H.
(2) Prove that φiy = −iφy, and so the map y  φy : H → CL(H, C) is not linear
when H has nonzero vectors. (Although for real Hilbert spaces, y 
 φy is
linear.)
4.6 Adjoints of bounded operators
With every operator T ∈ CL(H, K), where H, K are Hilbert spaces, one can
associate another operator, called its adjoint, T∗ ∈ CL(K, H), which is
geometrically related to T.
Theorem 4.11. Let H, K be Hilbert spaces, and T ∈ CL(H, K).
Then there exists a unique T∗ ∈ CL(K, H) such that
Definition 4.7. (Adjoint operator). This T∗ is called the adjoint of T.
Proof.
Step 1: Fix k ∈ K. The map h  T h, k  : H → K is a linear functional on H. We
verify this below.
(L1) If h1, h2 ∈ H, then
(L2) If α ∈ C and h ∈ H, then
Continuity: For all h ∈ H, |φk(h)| = | Th, k |  ||Th|| ||k||  ||T || ||k|| ||h||, and so ||φk|| 
 ||T || ||k||< ∞.
Hence φk ∈ CL(H, C), and by the Riesz Representation Theorem, it follows that
there exists a unique vector, which we denote by T∗k, such that for all h ∈ H,
φk(h)= h, T∗k , that is, for all h ∈ H, T h, k  = h, T∗k .
In this manner, we get a map k 
 T∗k : K → H from K to H.

We ask: Is T∗ ∈ CL(K, H)?
We will show in the next step that the answer is “yes”.
Step 2: T∗ ∈ CL(K, H).
(L1) If k1, k2 ∈ K, then for all h ∈ H we have
Thus h, T∗(k1 + k2) − (T∗k1 + T∗k2)  = 0 for all h ∈ H.
In particular, taking h = T∗(k1 + k2) − (T∗k1 + T∗k2), we obtain that T∗(k1
+ k2) = T∗k1 + T∗k2.
(L2) If α ∈ C and k ∈ K, then for all h ∈ H,
In particular, taking h = T∗(αk) − α(T∗k), we get T∗(αk) = α(T∗k).
This completes the proof of the linearity of T∗.
Continuity: By Exercise 4.34(1), we have ||φk|| = ||Tk||, k ∈ K.
Thus for all k ∈ K, ||T∗k|| = ||φk||  ||T || ||k||.
Consequently, T∗ ∈ CL(K, H) (and ||T∗||  ||T ||).
Uniqueness: If S ∈ CL(K, H) is another operator satisfying for all h ∈ H and k ∈
K, Th, k  = h, Sk ,
then we have for all h ∈ H and k ∈ K, h, T∗k  = Th, k  = h, Sk , and so h, T∗k
− Sk  = 0. Let k ∈ K. Taking h := T∗k − Sk ∈ H in the above, we obtain T∗k = Sk.
As k ∈ K was arbitrary, we conclude that S = T∗. Consequently T∗ is unique.
Example 4.14. Let H = Cn and K = Cm with the usual respective Euclidean inner
products. Let TA : Cn → Cn be the continuous linear transformation
corresponding to multiplication by the n × n matrix A of complex numbers:
What is the adjoint (TA)∗? We will show that (TA)∗ = TA*, where A∗ is the matrix

obtained by transposing the matrix A and by taking the complex conjugates of
each of the entries. Thus
For all h = (h1, ···, hn) ∈ Cn and all k = (k1, ···, km) ∈ Cm, we have
Thus (TA)∗ = TA*.
Example 4.15. (Right/left shift operators).
For the right shift operator R ∈ CL(ℓ2), one has that R∗ = L.
For h = (hn)n∈N and k = (kn)n∈N, we have
Thus R∗ = L.
What about L∗? Clearly L∗ = (R∗)∗, and from the following result, we can
conclude that (R∗)∗ = R. Thus L∗ = R.
Theorem 4.12. Let H, K be Hilbert spaces, T, S ∈ CL(H, K) and α ∈ C. Then:
(1) (T∗)∗ = T .
(2) ||T∗||= ||T ||.
(3) (αT)∗ = α∗T.
(4) (S + T)∗ = S∗ + T∗.
(5) If T1 ∈ CL(H1, K) and T2 ∈ CL(K, H2), then (T2T1)∗ = T∗
1T∗
2, where H1,
H2, K are Hilbert spaces.

Proof.
(1) For all h ∈ H and k ∈ K, T*k, h  = h, T*k * = Th, k * = k, Th , and so
(T*)* = T.
(2) In the proof of Theorem 4.11, we had seen that ||T*||  ||T||. 
Also, ||T|| = ||(T*)*||  ||T*||. Consequently ||T|| = ||T*||.
(3) For all h ∈ H and k ∈ K, we have
and so (αT)* = α*T*.
(4) For all h ∈ H and k ∈ K, we have
and so (T + S)* = T* + S*.
(5) For all h1 ∈ H1 and h2 ∈ H2, we have
and so (T2T1)* = 
.
Corollary 4.2.
If H is a Hilbert space and T ∈ CL(H) is invertible in CL(H), 
then T* is invertible in CL(H) and (T*)−1 = (T−1)*.
Proof. We have for all h, k ∈ H that Ih, k  = h, k  = h, Ik , and so I* = I. Since
TT−1 = I = T−1T, by taking adjoints, we obtain that (TT−1)* = I* = (T−1T)*, that
is, (T−1)*T* = I = T*(T−1)*. Hence T* is invertible, and its inverse is (T−1)*.
The definition of the adjoint operator T shows that T* is geometrically related to
T. A manifestation of this is the following result.
Theorem 4.13. Let H, K be Hilbert spaces and T ∈ CL(H, K). Then:
(1) kerT = (ranT*)⊥.
(2) ranT = (kerT*)⊥.

Proof.
(1) kerT ⊂ (ranT*)⊥: If h ∈ kerT, then Th = 0.
So for all k ∈ K, h, T*k  = Th, k  = 0, k  = 0. Thus h ∈ (ranT*)⊥. (ranT*)⊥
⊂ ker T: Let h ∈ (ran(T*))⊥. Then for all k ∈ K, we have 0 = h, T*k  = Th,
k , and in particular, with k = Th, we obtain that Th, Th  = 0, that is Th = 0.
So h ∈ ker T, and (ran(T*))⊥ ⊂ ker (T). Consequently, ker T = (ran T*)⊥.
(2) From part (1), and using Exercise 4.20, page 182, for T ∈ CL(H, K), we
have that
With T replaced by T*, we obtain (ker T*)⊥ = ran(T*)* = ranT.
In our discussion on adjoints, we’ve been working with complex spaces, but
analogous results also hold for real spaces.
Example 4.16. Take H = K = R2.
Let P = PY be the projection operator onto Y = span 
 (the “x-axis”).
Then P = TA, where A = 
. As A* = A, we see that P* = P.
Also, ker P = span 
 and ran P = span 
.
In the above example, we saw that for the projection operator, its adjoint was
itself. This is no coincidence!
Theorem 4.14. Let Y be a closed subspace of a Hilbert space H, and let PY be

the orthogonal projection operator onto Y (Theorem 4.8, page 180). Then (PY)*
= PY.
Proof. For all h, k ∈ H, we have
and so (PY)* = PY.
Definition 4.8. (Self-adjoint or Hermitian operators).
Let H be a Hilbert space. An operator T ∈ CL(H) is called self-adjoint or
Hermitian if T* = T.
Such operators are often encountered in applications, for example in physics and
in PDE theory. We’ll see a glimpse of this in our discussion on Quantum
Mechanics later in this chapter. Note that if T is a Hermitian operator on a
Hilbert space H, then for all x ∈ H, Tx, x  is real because
Exercise 4.35. (Idempotent self-adjoints are orthogonal projections.)
Let H be a Hilbert space, and P ∈ CL(H) be such that P2 = P = P*.
Prove that Y := ran P is closed and that P = PY.
Exercise 4.36.
An operator T ∈ CL(H) on a Hilbert space H is called skew-adjoint if T = −T*.
Show that every T ∈ CL(H) can be written as a sum of self-adjoint operator and
a skew-adjoint operator in a unique manner.
Exercise 4.37. Let (λn)n∈N be a bounded sequence in C, and consider Λ ∈ CL(ℓ2)
given by Λ(a1, a2, a3, · · ·) = (λ1a1, λ2a2, λ3a3, · · ·) for all (a1, a2, a3, · · ·) ∈ ℓ2.
Determine Λ*.
Exercise 4.38.

Let I : L2[0, 1] → L2[0, 1] be the operator on L2[0, 1] (real version), given by
From Example 2.10 (page 70), with A(t, τ) = 
, I ∈ CL(L2[0, 1]).
Determine I*.
Exercise 4.39. Consider the anticlockwise rotation through angle θ in R2, given
by the operator TA : R2 → R2 corresponding to the matrix A = 
. Give
a geometric interpretation of 
. (Note that 
. We call U ∈ CL(H), where
H is a Hilbert space, unitary if UU* = U*U = I.)
Exercise 4.40. Let {un : n ∈ N} be an orthonormal basis for a Hilbert space H.
For n ∈ N, consider the subspace Yn = span{u1, · · ·, un}.
Show that the projection Pn := PYn ∈ H is given by Pnx = 
 x, uk uk, x ∈ H.
Prove that (Pn)n∈N converges strongly to the identity operator I, that is, for all x
∈ H, Pnx 
 x in H. (Although ||Pn − I|| = 1 for all n ∈ N.)
Exercise 4.41. (Hilbert-Schmidt operators).
Let B = {un : n ∈ N} be an orthonormal basis for a Hilbert space H.
An operator T ∈ CL(H) is said to be Hilbert-Schmidt if 
 < ∞. || ·
||HS is called the Hilbert-Schmidt norm of T.
(1) Show that ||T||HS does not depend on the choice of the orthonormal basis.
Hint: If B′ = {  : n ∈ N} is another orthonormal basis, then show that
by using the fact that for a finite double sum of nonnegative terms, the order
of summation may be interchanged.
(2) Show that the collection S2(H) of all Hilbert-Schmidt operators is a subspace
of CL(H), and that || · ||HS defines a norm on S2(H).
(3) Prove that ||T||  ||T||HS for all Hilbert-Schmidt T ∈ CL(H).
The importance of Hilbert-Schmidt operators stems from the fact that they are
“compact” (to be studied in the next chapter), and hence they can be

approximated by finite rank operators (finite matrices). This is useful in
numerical analysis. An important example of Hilbert-Schmidt operators are the
integral operators TA : L2[0, 1] → L2[0, 1] on L2[0, 1] (real version), given by
where the kernel A ∈ L2([0, 1] × [0, 1]), that is, 
 (A(t, τ))2dτdt < ∞.
Indeed, if (un : n ∈ N} is an orthonormal basis for L2[0, 1], then
and so
Thus TA is Hilbert-Schmidt with ||TA||HS = 
.
Exercise 4.42. Let H be a Hilbert space, and let A ∈ CL(H) be fixed.
We define Λ : CL(H) → CL(H) by Λ(T) = A*T + TA for T ∈ CL(H).
Show that Λ ∈ CL(CL(H)), and that Λ(T) is self-adjoint if T is self-adjoint.
Exercise 4.43. Let H be a Hilbert space. Show that the set of all self-adjoint
operators on H is a closed subspace of CL(H).
Some more spectral theory
We will begin with a few elementary exercises, and move on to discuss the
spectral theory for self-adjoint operators.
Exercise 4.44. Let H be a Hilbert space and T ∈ CL(H).
Prove that μ ∈ ρ(T) if and only if μ* ∈ ρ(T*).
What is the spectrum of the right shift operator on ℓ2?
Exercise 4.45. Let Y be a proper closed subspace of a Hilbert space H, and PY be

the projection onto Y. If |λ| > ||PY||  0, then we can write by the Neumann Series
Theorem that
Show that for λ ∈ C\{0, 1}, (λI − PY)−1 = 
.
What is the spectrum of PY? What is its point spectrum?
Exercise 4.46. Let H be a Hilbert space and U ∈ CL(H) be a unitary operator,
that is UU* = I = U*U. Show that eigenvalues λ ∈ σp(A) lie on the unit circle
with center 0 in the complex plane. Also show that the eigenvectors
corresponding to distinct eigenvalues are orthogonal.
Recall that the finite-dimensional spectral theorem from linear algebra states that
a Hermitian linear transformation T on Cd has all eigenvalues real, with
corresponding eigenvectors forming an orthogonal basis. A similar result holds
for Hermitian continuous linear transformations on a Hilbert space H.
Theorem 4.15. Let H be a Hilbert space and T ∈ CL(H) satisfy T = T*. Then
σ(T)⊂ R.
Proof. We will show that for all λ ∈ C\R, λI − T is invertible, by showing that it
is one-to-one and onto. Set λ = a + ib, where a, b ∈ R and b ≠ 0.
One-to-one: ||(λI − T)x||2 = ||(aI − T)x||2 − 2Re(ib x, (aI − T)x ) + ||ibx||2 for x ∈
H. As (aI − T)* = aI − T* = aI − T, x, (aI − T)x  is real, and the term Re(· · ·) is
0. So ||(λI − T)x||2 = ||(aI − T)x||2 + |b|2||x||2  |b|2||x||2. Hence if (λI − T)x = 0, then
|b|2||x||2 = 0, giving x = 0. Thus λI − T is one-to-one whenever Im(λ) ≠ 0.
Onto: First we note that the range of λI − T is closed. To see this, let (yn)n∈N be a
sequence of vectors in λI − T that converges in H to y. Then there is a sequence
(xn)n∈N in H such that (λI − T)xn = yn, n ∈ N. But the inequality ||(λI − T)x|| 
|b|||x||, x ∈ H, established above gives with x = xn − xm that ||yn − ym||  |b|||xn −
xm||, n, m ∈ N. As (yn)n∈N is Cauchy, so is (xn)n∈N, and since H is Hilbert, there

is some x ∈ H to which (xn)n∈N converges. But then
So y belongs to the range of λI − T. Consequently, the range of λI − T is closed.
Next,
So T is onto.
Just as in the finite dimensional case, one can also show orthogonality of
eigenvectors.
Theorem 4.16.
Let H be a Hilbert space and T ∈ CL(H) be such that T = T*.
Then the eigenvectors corresponding to distinct eigenvalues are orthogonal.
Proof. We already know that σp(T) ⊂ σ(T) ⊂ R. Let λ1, λ2 be distinct real
eigenvalues with corresponding eigenvectors v1, v2 respectively. Then
This gives (λ1 − λ2) v1, v2  = 0, and so v1, v2  = 0.
Remark 4.7. In the finite dimensional case, we can construct the Hermitian
operator given the knowledge of the eigenvalues and eigenvectors. In the infinite
dimensional case, an analogous construction can be given. To see this, let us first
revisit the finite-dimensional case, and write a procedure for constructing the
operator in a way that lends itself to a generalisation to the infinite-dimensional
case.
Arrange the spectrum of the Hermitian T ∈ CL(Cd) in increasing order: λ1 <
· · · < λk, and let the corresponding eigenspaces be denoted by Y1, · · ·, Yk. Thus
Yj = ker(λjI − T), j = 1, · · ·, k. Then the Yjs are mutually orthogonal, and

together, they span Cd. If Pj is the projection onto Yj, then
With E0 := 0, we have
Also, 
 = Ei and Ek = I. One thinks of the family of projections Eis as “growing
from 0 to I”. We have
This is essentially the content of the finite dimensional spectral theorem for
Hermitian linear transformations.
Analogously, for a Hermitian operator T on a Hilbert space H, there exists an
increasing family of projection operators (Eλ)λ∈R such that Eλ = 0 if λ < inf σ(T),
Eλ = I if λ > sup σ(T), and
The above integral can be used in the following ordinary Riemann-Stieltjes
integral sense to reconstruct T:
We refer the reader to [Kreyszig (1978), Theorem 9.9-1, page 505] for details.
With the extra assumption of compactness, we’ll see this in Chapter 5.
Exercise 4.47. (Cayley transform).
Let H be a Hilbert space, and let T = T* ∈ CL(H).
Conclude that T + iI is invertible, and that (T + iI)−1 commutes with T − iI.
Show that (T − iI)(T + iI)−1 =: U is unitary.
Moreover, show that T = i(I + U)(I − U)−1.

Exercise 4.48. If A, B are Hermitian operators on a Hilbert space H, we write A 
B if for all x ∈ H, Ax, x   Bx, x . Show that if Y, Z are closed subspaces of H,
then PY  PZ if and only if Y ⊂ Z.
4.7 An excursion in Quantum Mechanics
We’ve seen that the Hilbert space theory grows naturally out of a need to
generalise the setting of finite-dimensional Euclidean space. Another motivation
for their study comes from Physics, where it provides a framework for Quantum
Mechanics. The determinism of Newtonian Physics, even after Einstein’s
revolution with the special and general theories of relativity, was still the
prevalent view at the beginning of the 20th century. However, this started getting
challenged by experimental evidence in the quantum world, and the
deterministic view of the universe started giving way to the probabilistic
description of the behaviour of quantum particles. The two seemingly disparate
formulations of the fundamental laws of quantum mechanics, developed in the
1920s, the Matrix Mechanics of Heisenberg, versus the Wave Mechanics of
Schrödinger, were shown to be in fact equivalent, both falling under the theory
of linear operators on Hilbert spaces. We will briefly discuss the axiomatics of
Quantum Mechanics in order to give a flavour of this application of Hilbert
space theory to Physics.
State space and states. Every physical system is assumed to correspond to a
complex separable Hilbert space, and is called the state space. A state of the
system is a one-dimensional subspace of H, and is represented by a nonzero
vector ψ ∈ H such that ||ψ|| = 1. We note that eiρψ, ρ ∈ R represents7 the same
state as ψ.
Observables. In classical mechanics, recall that observables are real-valued
functions on the state space. On the other hand, in quantum mechanics,
observables are self-adjoint operators on the Hilbert space H.
Although we have been considering continuous linear transformations, to
include physically realistic examples, one needs to consider “unbounded linear
operators”, that is, (not necessarily continuous) linear transformations T : DT(⊂
H) → H, where DT is a subspace of H. A thorough discussion of self-
adjointness8 of unbounded operators will take us too far afield in these
elementary notes, and so we will assume continuity in our arguments, but relax
this assumption in the discussion of examples! We hope that despite this

“schizophrenic” approach, much of the spirit of the application of Hilbert space
theory to quantum mechanics will nevertheless be conveyed to an uninitiated
reader.
In the classical case, if the state space is X = R2, and F ∈ C∞(R2) is an
observable, then this observable imparts a structure on the state space X by the
consideration of the fibres of F, which partition X into disjoint subsets, namely
the level sets of F. Indeed, we define the equivalence relation ~ on X by setting
x1 ~ x2 if F(x1) = F(x2). Then equivalence classes of this equivalence relation
partition X into disjoint subsets, and F labels each subset with a distinct real
number λi, i in some index set I. The quantum mechanical analogue of a partition
into disjoint subsets is a decomposition into orthogonal subspaces, and so the
analogue of the classical observable should decompose H into orthogonal
subspaces. But this is what self-adjoint operators do (for example, we know this
by the spectral theorem at least when dim H < ∞, and the following figure
illustrates this).
In classical mechanics, the result of an experiment measuring an observable F on
a classical system in a state x is the real number F(x) which belongs to the range
of the observable at hand. In quantum mechanics, the result of an experiment
measuring an observable T on a quantum system in state ψ is an eigenvalue of T.
Which eigenvalue of T? The answer is no longer deterministic, but rather
probabilistic. The act of measuring the observable T “collapses” the state ψ of
the system into a state φ, where φ is a normalised eigenvector of T. The
probability with which this happens is | ψ, φ |2. If the state has collapsed to φ,
and Tφ = λφ then the measured value of the observable T is λ. If a quantum
mechanical system is in a state φ, where φ is an eigenvector of the observable T,

then what is the probability that the state stays in the same state φ after T is
measured? The answer is | φ, φ |2 = ||φ||2 = 1. This feature of Quantum
Mechanics, where the result of an experiment is not deterministic, but random,
and the fact that the state of the system is in general no longer the same as that
prior to the measurement is one of the key differences between classical and
quantum mechanics.
We now introduce some terminology from probability theory. The expected
value of an observable T of a system which is in a state ψ is given by
Note that since T is self-adjoint, this is a real number. This number is to be
interpreted as follows: if we do multiple experiments on a quantum system in the
same state ψ, and we measure the observable T in each instance, this is the
average of the measured value we would obtain. The root mean square deviation
of an observable T of a system which is in a state ψ is given by
Let us simplify this somewhat. First note that T − T ψI is self adjoint, and so is
its square. So the quantity under the square root is well-defined and moreover, it
is nonnegative because
Moreover, we have
Note that if ψ is a normalised eigenvector of T with eigenvalue λ, then T ψ = λ,
and T2
ψ = λ2, so that ΔψT = 0. Vice versa, if ΔψT = 0, then (T − T ψI)ψ = 0, that
is, Tψ = T ψψ, so that ψ is an eigenvector of T corresponding to the eigenvalue
T ψ. So we’ve shown that a state ψ is an eigenvector of an observable T if and

only if ΔψT = 0 (“dispersion-free”). Such states are called pure states of the
observable T.
Example 4.17. (Position and momentum operators). Let H = L2(R), and define
the position operator Q : DQ(⊂ L2(R)) → L2(R) by
Then Q is a linear transformation. Moreover, for Ψ1, Ψ2 ∈ DQ, we have
The expected value of Q for a system in a state Ψ is
So if we think of R as space, then |Ψ|2 can be thought of as the probability
density function for the random variable of the result of the experiment of
measuring the position of the particle in a state Ψ. The probability of finding the
particle in a region (a, b)⊂ R is then
The momentum operator P : DP(⊂ L2(R)) → L2(R) is given by
Then P is a linear transformation. Moreover, for Ψ1, Ψ2 ∈ DP, we have, using
integration by parts, that

We had seen in Exercise 2.35, page 103, that Q has no eigenvectors. The
momentum operator doesn’t have any eigenvectors either, since if Ψ ∈ L2(R)\
{0} is such that PΨ = λΨ for some λ ∈ R, then
giving Ψ(x) = eiλx/ħΨ(0), and as Ψ ≠ 0, Ψ(0) ≠ 0. But then
a contradiction to Ψ ∈ L2(R)!
Exercise 4.49.
(1) Let f : [0, ∞) → [0, ∞) be differentiable and 
 Show
that 
 f(x) = 0.
(2) Prove that if Ψ, Ψ′ ∈ L2(R), then 
 Ψ(x) = 0.
Heisenberg Uncertainty Principle. The famous Heisenberg Uncertainty
Principle that one learns in elementary physics courses states that the position
and momentum of a quantum particle cannot be measured simultaneously, with
their respective dispersions satisfying (σx)(σp) 
 ħ/2, where ħ is the reduced
Planck’s constant. (This famously resolves the question of the stability of the
atom, namely why the negatively charged electrons don’t just radiate
electromagnetic energy and fall into the positively charged nucleus. The
uncertainty principle guarantees that when the electron tries to do so, it would be
confined in a smaller space, thus reducing the dispersion in position, and
increasing the dispersion in momentum, and hence creating a tendency in the
electron to not want to do so! So it settles in a happy cloud around the nucleus,

giving the atom its “size”.) We will now show an Uncertainty Principle not just
for the position-momentum pair of observables, but for any pair, and this is just
an application of the Cauchy-Schwarz Inequality!
Theorem 4.17. (Uncertainty Principle).
Let A, B be self-adjoint operators on a Hilbert space H and ψ ∈ H. Then
where [A, B] := AB − BA is the commutator of A, B.
Proof. We have
where Im(·) denotes the imaginary part of a complex number. If a := A ψ and b
:= B ψ, then a, b ∈ R, and we have
Thus we have
This completes the proof.
Example 4.18. (Position-momentum uncertainty).
For Ψ ∈ DP ∩ DQ (for example Ψ can be any compactly supported smooth
function), we have

and so (ΔΨP)(ΔΨQ)  .
Exercise 4.50. Show that the commutator of two observables is skew-adjoint.
Exercise 4.51. (Jacobi Identity).
Show that for observables A, B, C, [A, [B, C]] + [B, [C, A]] + [C, [A, B]] = 0.
Exercise 4.52. Show that for n ∈ N, [Qn, P] = iħnQn−1.
The uncertainty relation of the position-momentum type given in Example 4.18
is obtained for any two observables satisfying the commutation relation [A, B] =
iħI, and such observables are said to be complementary. The need for
considering unbounded operators in Quantum Mechanics is thus further
supported by Exercise 2.22, page 86, from which it follows that there do not
exist continuous linear transformations A, B on H such that [A, B] = −iħI. Thus
the beloved continuity of mathematicians appears quite at odds with the
“unbounded” requirements of physicists for realistic applications. Nevertheless,
CL(H) still has its role to play in quantum mechanics, for these unbounded
operators do “generate” groups of unitary operators, which play a role in
dynamics, to be discussed below.
Quantum dynamics and Schrödinger’s equation. In classical mechanics, the
evolution equations for the state (q*(t), p*(t)) in Hamiltonian form are:
where (q, p)  H(q, p) : R2 → R is the Hamiltonian observable9. With
we get a quantum mechanical Hamiltonian observable H(Q, P), and the
evolution equation for the state ψ is Schrödinger’s equation,

Here we assume that H is time-independent. What does the solution look like?
We elaborate on this in the following paragraph.
Just as real numbers are related to unit-modulus complex numbers via the
exponential map τ 
 eitτ, t ∈ R, (even unbounded) self-adjoint operators are
related to (bounded) unitary ones. One has the following result due to Stone
from 1932:
Theorem 4.18. Every self-adjoint operator T on a Hilbert space H generates a
strongly continuous one-parameter group of unitary operators eitT, t ∈ R, on H.
Conversely, every such one-parameter group (U(t))t∈R is generated by a unique
self-adjoint operator, given by
If ψ(0) ∈ DH, then the solution to Schrödinger’s equation is given by
The evolution of the expected value of an observable T is given by
that is,
which is reminiscent of the Poissonian equation in classical mechanics for the

evolution of an observable F:
Thus the role of the Poisson bracket {·, ·} in Classical Mechanics is replaced by
commutator
in Quantum Mechanics.
Exercise 4.53. (Quantisation).
In our discussion of Classical Mechanics in Chapter 3, recall that in one space
dimension, the position observable Q ∈ C∞(R2) was given by
and the momentum observable P ∈ C∞(R2) was given by
Show that the classical Poisson bracket {Q2, P2} = 4QP, but that this does not
correspond to the quantum mechanical commutator [Q2, P2]/(iħ) of the quantum
mechanical observables Q2 and P2.
By rewriting the classical Poisson bracket {Q2, P2} = 4QP = 2(QP + PQ) (which
is allowed, since the order or writing does not matter classically), this expression
does correspond to the quantum mechanical case.
Check that QP is not self-adjoint, but that QP + PQ satisfies the symmetry
condition (PQ + QP)Φ, Ψ  = Φ, (PQ + QP)Ψ  for all Ψ, Φ.
(The process of replacing classical observables by quantum mechanical ones is
called “quantisation”, and this example shows that there is no unique way of
doing this owing to the lack of commutativity of the quantum mechanical
observables: a helpful device available is that one should rearrange the Poisson
bracket in a manner such that when quantised, it results in a self-adjoint
operator.)
Exercise 4.54. (Probability is conserved).
Let t 
 ψ(t) be the solution to Schrödinger’s equation for some selfadjoint

Hamiltonian H. Show that if ||ψ(0)||2 = 1, then ||ψ(t)||2 = 1 for all t ∈ R.
Exercise 4.55. The one-dimensional Schrödinger equation in H = L2(R) is
where V is the potential energy.
One method to find solutions is to assume that variables separate, that is, the
solution has the form Ψ(x, t) = X(x)T(t).
Substituting Ψ(x, t) = X(x)T(t) in the Schrödinger equation gives
As the left-hand side is constant along vertical lines in the (x, t)-plane, and the
right-hand side is constant along horizontal lines, it follows that their common
value is a constant function in the (x, t)-plane, with constant value, say E (for
“energy”). So we obtain the following equation for T,
which has the solution T(t) = C exp 
.
The equation for X is 
 X″(x) + (V(x) − E)X(x) = 0.
Consider a free particle of mass m confined to the interval 0 < x < π (so that V ≡
0 in (0, π)), and suppose that Ψ(0, t) = Ψ(π, t) = 0 for all t. (Imagine the particle
to be in an “infinite potential well”.)
Show that this problem has a nontrivial solution if and only if
Sketch the probability density function |Ψ|2 when n = 1, 2, and compute the
probability that the particle is in the interval [0, 1/4] in each case.
Notes
Example 4.6 and Theorem 4.6 are based on [Thomas (1997)].

1Recall that a relation on a set S is simply a subset of the Cartesian product S × S. A relation R on a set S
is called an equivalence relation if it is reflexive (for all x ∈ S, (x, x) ∈ R), symmetric (if (x, y) ∈ R, then (y,
x) ∈ R), and transitive (if (x, y), (y, z) ∈ R, then (x, z) ∈ R). If x ∈ S, then the equivalence class of x, denoted
by [x], is defined to be the set {y ∈ S : (x, y) ∈ R}. It is easy to see that [x] = [y] if and only if (x, y) ∈ R.
Thus equivalence classes are either equal or disjoint. They partition the set S, that is the set can be written as
a disjoint union of these equivalence classes.
2From Greek, where “ortho” = straight/erect and “gonia” = angle.
3In this finite dimensional context, by an orthonormal basis, we simply mean a (finite, Hamel) basis of
orthonormal vectors.
4After we introduce the notion of the adjoint of an operator, we will see that this fact means that PY is
self-adjoint, that is, 
 = PY.
5The interested reader is referred to [Rudin (1976), Theorem 8.16, page 191].
6His brother, Marcel, is also famous for results in functional analysis.
7The “angular freedom”, eiρ, is sometimes called the phase of the state.
8Among other things, a self adjoint operator T : DT(⊂ H) → H should first of all be densely defined (that
is, DT should be dense in H) and symmetric (that is, it should satisfy the symmetric condition that for all φ,
ψ ∈ DT, Tψ, φ  = ψ, Tφ ).
9We use upright H for the Hamiltonian here in order to avoid confusion with the letter H used for the
underlying Hilbert space in our quantum mechanical system.

Chapter 5
Compact operators
In this chapter, we study a special class of linear operators, called compact
operators.
Why should we study compact operators? One important reason is that they
can be approximated by finite rank operators. So they play an important role in
the numerical approximation of solutions to operator equations. We had seen that
if H is an infinite-dimensional Hilbert space and A ∈ CL(H) with ||A|| < 1, then
given y ∈ H, there is a unique x ∈ H such that
which is given by the Neumann series
But all of the operators A, A2, A3, ··· act on the infinite-dimensional H, so that
computing these powers may not at all be feasible, and the convergence of the
series may be “slow” (see Exercise 2.23, page 87). But now imagine that we can
approximate A by finite matrices An and consider instead
where the yn → y as n → ∞, and the unknown xn are obtained by solving the
finite linear algebraic equation (I – An)xn = yn. Then we can easily compute xn =
(I – An)−1yn, and if xn → x, then we are able to determine x approximately. This
wishful thinking can be made a reality if A is “compact”, as we shall see later on
in this chapter when we learn Theorem 5.6 (page 218) on Galerkin
approximations.
We begin by giving the definition of a compact operator.
5.1 Compact operators
Definition 5.1. (Compact operators). Let X, Y be normed spaces. A linear

transformation T : X → Y is said to be compact if
for every bounded sequence (xn)n∈N contained in X, (Txn)n∈N has a
convergent subsequence.
We will denote the set of all compact operators from X to Y by K(X, Y).
Why do we call such operators compact? The following result answers this
question. Recall firstly that a closed and bounded set in an infinite dimensional
normed space may fail to be compact (Example 1.26, page 48, and Example
1.28, page 50). So if T ∈ CL(X, Y), and B is the closed unit ball in X with centre
0, then although we know that T(B) is closed and bounded, it needn’t be
compact. However, compact operators T are special in the sense that T(B) is
guaranteed to be compact!
Theorem 5.1.
Let X, Y be normed spaces, and T : X → Y be a linear transformation. Then the
following are equivalent:
(1) T is compact.
(2) The closure T(B) of the image under T of the closed unit ball, B := {x ∈ X
: ||x||  1}, is compact.
Proof.
(1)  (2): Let (zn)n∈N be a sequence in T(B). Then there is a sequence (yn)n∈N in
T(B) such that
Let yn = Txn, xn ∈ B, n ∈ N. Since for all n we have ||xn||  1, and because T is
compact, (yn)n∈N has a convergent subsequence, say (ynk)k∈N, converging to, say
y. As each yn ∈ T(B), we have y ∈ T(B). From (5.1), (znk)k∈N converges to y too.
Hence T(B) is compact.
(2)  (1): Let (xn)n∈N be a bounded sequence in X, and let M > 0 be such that for
all n ∈ N, ||xn||  M. Then (xn/M)n∈N is in B, and (T(xn)/M)n∈N is in T(B) ⊂ T(B).
As T(B) is compact, (T(xn)/M)n∈N has a convergent subsequence. Thus (Txn)n∈N
has a convergent subsequence. Consequently, T is compact.

5.2 The set K(X, Y) of all compact operators
Corollary 5.1.
Every compact operator is continuous, that is K(X, Y) ⊂ CL(X, Y).
Proof. Let B := {x ∈ X : ||x||  1}. If T is compact, then T(B) is compact, and in
particular, bounded. So T(B) ⊂ T(B) is bounded too. So there is some M > 0 such
that ||Tz||  M for all z ∈ B. But this gives ||Tx||  M ||x|| for all x ∈ X. (This is
trivially true when x = 0, and if x ≠ 0, then by taking z = x/||x|| ∈ B, we have ||Tz|| 
 M, yielding the desired inequality.) So T ∈ CL(X, Y).
Is it true that K(X, Y) = CL(X, Y)? No:
Example 5.1. (Not all continuous linear transformations are compact). Let X be
any infinite dimensional inner product space, for example ℓ2. We will show that
the identity operator I ∈ CL(X) is not compact.
Let {u1, u2, u3, ···} be any orthonormal set in X. (Start with any countably
infinite independent set, and use Gram-Schmidt.) Then ||un|| = 1 for all n ∈ N,
and so the sequence (un)n∈N is bounded. However, the sequence (Iun)n∈N has no
convergent subsequence, since for all n, m ∈ N with n ≠ m, we have ||Iun – Ium||2
= ||un – um||2 = 1 + 0 + 0 + 1 = 2, and this can’t be made as small as we please.
Hence I is not compact, but is continuous.
In contrast to the above, it turns out that all finite rank operators are compact.
Recall that an operator T is called a finite rank operator if its range, ran(T), is a
finite-dimensional vector space. For ease of exposition, we will just prove this
when Y is an inner product space.
Theorem 5.2. Let X be a normed space and Y be an inner product space.
Suppose that T ∈ CL(X, Y) is such that ran(T) is finite dimensional.
Then T is compact.
Proof. Let {u1, ···, um} be an orthonormal basis for ran(T). Let (xn)n∈N be a

bounded sequence in X. Suppose that M > 0 is such that ||xn||  M for all n ∈ N.
We want to show that (Txn)n∈N has a convergent subsequence. (We will show
that
for some subsequence (xnk)k∈N.) For all n ∈ N and each ℓ ∈ {1, . . . , m},
(xn)n∈N has some subsequence (xn(1))n∈N such that 
.
(xn(1))n∈N has some subsequence (xn(2))n∈N such that 
.
(xn
(m–1))n∈N has some subsequence (xn
(m))n∈N such that 
.
Claim: (Txn
(m))n∈N converges to α1u1 + ··· αmum.
it follows that (Txn
(m))n∈N is a convergent subsequence of the sequence (Txn)n∈N.
Consequently T is compact.
In elementary linear algebra, not only were all linear transformations from Cn to
Cm continuous, they were even compact!
Example 5.2. (L(Cn, Cm) = CL(Cn, Cm) = K(Cn, Cm)).
If A ∈ Cn×m, then TA ∈ CL(Cn, Cm) given by TAx = Ax, x ∈ Cn, is finite-rank
because ran TA ⊂ Cm, and so TA is compact. In particular, the identity map I : Cd
→ Cd is compact.
We had seen that K(X, Y) ⊂ CL(X, Y). But CL(X, Y) is a vector space, with the

usual pointwise operations. So it is natural to ask if K(X, Y) is a subspace of
CL(X, Y). The answer is “yes”, and this is what we show next.
Theorem 5.3. 
.
If X, Y are normed spaces, then K(X, Y) is a subspace of CL(X, Y).
Proof.
(S1) 0 is compact since (0xn)n∈N = (0)n∈N is convergent for all bounded
sequences (xn)n∈N in X.
(S2) If T, S are compact, and (xn)n∈N is bounded, then (Tn)n∈N has some
subsequence (Txnk)k∈N that is convergent, and (Sxnk)k∈N has some
subsequence (Sxnkℓ)ℓ∈N that is convergent. So ((T + S)xnkℓ)ℓ∈N is convergent.
Thus T + S is compact.
(S3) If T, S are compact, α ∈ K and (xn)n∈N is bounded, then (Tn)n∈N has some
subsequence (Txnk)k∈N that is convergent, and so it follows that 
 is convergent. Thus αT is compact.
Since CL(X, Y) is a normed space, we can even ask if K(X, Y) is a closed
subspace of CL(X, Y). We now show that if Y is a Banach space, then K(X, Y) is
a closed subspace of CL(X, Y), or briefly:
“Limits of compact operators are compact.”
Theorem 5.4. Let X be a normed space, Y a Banach space, and (Tn)n∈N be a
sequence in K(X, Y) that converges in CL(X, Y) to T ∈ CL(X, Y). Then T is
compact.
Proof. Suppose that (xn)n∈N is a bounded sequence in X, and let M > 0 be such
that for all n ∈ N, ||xn||  M. Since T1 is compact, (T1xn)n∈N has a convergent
subsequence (T1xn(1))n∈N, say. Again, since (xn(1))n∈N is a bounded sequence, and
T2 is compact, (T2xn(1))n∈N has a convergent subse-quence, say (T2xn(2))n∈N. We
continue in this manner to obtain the following:

Consider the diagonal sequence x1, x2(1), x3(2), ··· .
By meditating on the above picture, one can convince oneself that
, ··· is a subsequence of 
.
As 
, ··· converges, its subsequence,
converges too, and so (
 converges.
For n, m ∈ N, we have
Hence 
 is a Cauchy sequence in Y and since Y is complete, it converges
in Y. So, starting from the bounded sequence (xn)n∈N in X, we have found a
subsequence 
 of the sequence (Txn)n∈N, that converges in Y.
Consequently, T is compact.
Corollary 5.2. Let X be a normed space, Y a Hilbert space, and (Tn)n∈N be a

sequence of finite rank operators in CL(X, Y) that converges in CL(X, Y) to T ∈
CL(X, Y). Then T is compact.
Example 5.3. (When is a diagonal operator on ℓ2 compact?)
Let X = Y = ℓ2, (λn)n∈N be a bounded in K, and Λ ∈ CL(ℓ2) be “given by”
Then we had seen in Exercise 2.17 (page 76) that ||Λ|| = 
|λn|.
Claim: Λ is compact if and only if 
 λn = 0.
(If part): Consider for n ∈ N, the operators Λn ∈ CL(ℓ2) given by
Each Λn is a finite rank operator because ran Λn ⊂ span{e1, ··· , en}, where ek is
the sequence with kth term equal to 1, and all others equal to 0. Hence Λn is
compact. Then
Consequently, Λ, being the uniform limit of a sequence of compact operators, is
compact.
(Only if part): Suppose that Λ is compact, but it is not the case that
(λn)n∈N is convergent with limit 0, that is,

that is,
Taking N = 1, there exists n1 > 1 such that |λn1|  .
Taking N = n1, there exists n2 > n1 such that |λn2|  .
· · ·
Proceeding in this manner, we can construct inductively a subsequence (λnk)k∈N
of (λn)n∈N such that for all k ∈ N, |λnk|  . Now consider the bounded sequence
(enk)k∈N in ℓ2. 
We 
have 
(Λenk)n∈N 
= 
(λnk 
enk)k∈N. 
But 
for 
all 
. This shows that (λnk enk)k∈N
has no convergent subsequence, contradicting the compactness of Λ.
Exercise 5.1. (Hilbert Schmidt operators are compact.)
Let H be a Hilbert space with an orthonormal basis (u1, u2, u3, ···}.
Let T ∈ CL(H) be Hilbert-Schmidt, that is, 
.
(1) If m ∈ N, then define Tm : H → H by Tmx = 
.
Prove that Tm ∈ CL(H) and that 
.
Hint:
. and use the Cauchy-
Schwarz inequality in ℓ2.
(2) Show that every Hilbert-Schmidt operator T is compact.
Hint: Using (1), conclude that T is the limit in CL(H) of the sequence of
finite rank operators Tm, m ∈ N.
Exercise 5.2. Let H be a Hilbert space, and x0, y0 ∈ H be fixed.
Define x0  y0 : H → H by (x0  y0)(x) = x, y0 x0, x ∈ H.
(1) Show that x0  y0 ∈ CL(H) and that ||x0  y0||  ||x0||||y0||.
(2) Is x0  y0 compact?
(3) Let A, B ∈ CL(H). Show that A(x0  y0)B = (Ax0)  (B*y0).
Recall that CL(H) has the structure of a complex algebra with multiplication of

T, S ∈ CL(H) taken as composition T  S ∈ CL(H). What is the relation of K(H q
as a subset of CL(H) with respect to this operation of multiplication? The answer
is that K(H) forms an “ideal” in CL(H).
Definition 5.2. (Ideal in an algebra).
An ideal I of an algebra R is a subset I of R having the properties:
(I1) 0 ∈ I.
(I2) If a, b ∈ I, then a + b ∈ I.
(I3) If a ∈ I and r ∈ R, then ar ∈ I and ra ∈ I.
For example, if R = Z, the set of all integers, then I = 2Z, the set of all even
integers, is an ideal in R. In algebra, ideals are important, since they serve as
kernels of algebra homomorphisms.
Theorem 5.5. Let H be a Hilbert space. Then we have:
(1) If T ∈ K(H) is compact and S ∈ CL(H), then TS is compact.
(2) If T ∈ CL(H) is compact, then T* is compact.
(3) If T ∈ CL(H) is compact and S ∈ K(H), then ST is compact.
Proof.
(1) Let (xn)n∈N be a bounded sequence in H. Suppose M > 0 is such that ||xn|| 
M for all n ∈ N. Since S ∈ CL(H), it follows that (Sxn)n∈N is also a bounded
sequence (||Sxn||  ||S||||xn||  ||S||M). As T is compact, (T(Sxn))n∈N = (TSxn)n∈N
has a convergent subsequence. Thus TS is compact.
(2) As T ∈ K(H) and T* ∈ CL(H), by part (1) above, TT* is compact.
Let (xn)n∈N be a bounded sequence in X and ||xn||  M for all n.
Then (TT*xn)n∈N has some convergent subsequence, say (TT*xnk)k∈N. Hence,
given an  > 0,
So (T*xnk)k∈N is a Cauchy sequence, and as H is a Hilbert space, it is
convergent. Consequently, T* is compact.
(3) Since T is compact, by part (2), it follows that T* is also compact.
Moreover, as S* ∈ CL(H), we have T*S* is compact, using part (1). From
part (2) again, we get (T* S*)* = S**T** = ST is compact.

Summary: The set K(H) is a closed ideal of CL(H).
Example 5.4. (Compact operators on infinite dimensional Hilbert spaces are
never invertible.) Let H be an infinite dimensional Hilbert space, and T ∈ K(H).
If T were invertible in CL(H), then T−1 ∈ CL(H), so that I = TT−1 ∈ K(H), which
is false, since we had seen that the identity operator on an infinite dimensional
Hilbert space is not compact.
Exercise 5.3. Let T ∈ CL(H), where H is an infinite-dimensional Hilbert space.
(1) Give an example of H and T such that T2 is compact, but T isn’t.
(2) Show that if T is self-adjoint and T2 is compact, then T is compact.
Exercise 5.4. Determine if the following statements are true for all S, T ∈ CL(H),
where H is an infinite dimensional Hilbert space.
(1) If S and T are compact, then S + T is compact.
(2) If S + T is compact, then S or T is compact.
(3) If S or T is compact, then ST is compact.
(4) If ST is compact, then S is compact or T is compact.
Exercise 5.5. Let H be a Hilbert space. Let A ∈ CL(H) be fixed.
We define Λ ∈ CL(CL(H)) by Λ(T) = A*T + TA, T ∈ CL(H).
Show that the subspace K(H) of CL(H) is Λ-invariant, that is, ΛK(H) ⊂ K(H).
5.3 Approximation of compact operators
Compact operators play an important role in numerical analysis since they can
be approximated by finite rank operators. This means that when we want to
solve an operator equation involving a compact operator, then we can replace the
compact operator by a sufficiently good finite-rank approximation, reducing the
operator equation to an equation involving finite matrices. The solution can then
be found using linear algebra. In this section we will prove Theorems 5.6 and
5.7, which form the basis of the Galerkin Method in numerical analysis.
Consider the equation (I – K)x = y, where K is a given operator on a Hilbert
space H, y ∈ H is a given vector, and x ∈ H is the unknown. Suppose we
consider instead the equation (I – K0)x0 = y0, where K0 is close to K, and y0 is
close to y. The following result describes how big ||x – x0|| can get.

Theorem 5.6.
Let
(1) H be a Hilbert space,
(2) K ∈ CL(H) be such that I – K is invertible in CL(H),
(3) K0 ∈ CL(H) be such that  := ||(K – K0)(I – K)−1|| < 1.
Then for every y, y0 ∈ H, there exist unique x, x0 ∈ X such that
(a) (I – K)x = y,
(b) (I – K0)x0 = y0, and
(c) 
.
Note that from part (c) we see that the upper estimate on ||x – x0|| is small when y
≈ y0 and K ≈ K0. So the result is telling us that if we have a scheme of
approximating the operator K and the vector y, then we can solve the equation
approximately by solving instead the equation
and moreover, we have a handle on how large the error ||x – x0|| can get. Later
on, in Theorem 5.7 we will see that for compact operators K, such an
approximating scheme for producing K0 does exist.
Proof. As ||(K – K0) (I – K)−1|| < 1, by the Neumann Series Theorem, we have I +
(K – K0)(I – K)−1 is invertible, and so
is invertible as well. Moreover,

Furthermore, we have
and so
Let y, y0 ∈ X. Since I – K and I – K0 are invertible, there are unique x, x0 ∈ X
such that (I – K)x = y and (I – K0)x0 = y0. Also,
and so 
, as desired.
Question: If K is compact, y ∈ H, then how do we find approximations K0 to K
and y0 to y?
Answer: Via projections.
Theorem 5.7. (Galerkin approximation).
Let
(1) H be a Hilbert space,
(2) K be a compact operator on H,
(3) (Pn)n∈N be a sequence of projections (P2
n = P*n = ∈ CL(H)) of finite rank
such that Pn converges strongly to I (for all x ∈ H, 
 Pnx = x).
Then PnKPn 
 K in CL(H).

We remark that a mere strong convergence assumption results in uniform
convergence, and this miracle happens since we have a compact operator at
hand. We also remark that a standard way of producing such a sequence of
projections is via choosing an orthonormal basis {u1, u2, u3, ···} for H, and then
we can take Pn to be the projection onto the closed finite dimensional subspace Y
= span{u1, ···, un}:
Proof. We’ll prove the following claims:
(1) PnK 
 K in CL(H) (projection approximation),
(2) KPn 
 K in CL(H) (Sloan approximation),
(3) PnKPn 
 K in CL(H) (Galerkin approximation).
(1): For all x ∈ H, we have
and so ||Pnx||  ||x|| for all x, that is, ||Pn||  1. Suppose that it is not the case that
PnK – K converges to 0 in CL(H) as n → ∞. This means that
Thus there exists an  > 0 such that for all N ∈ N, there exists an n > N, such that
||PnK – K|| > .
Hence there exists an  > 0 such that for all N ∈ N, there exists an n > N, such
that sup{||(PnK – K)x|| : x ∈ H, ||x||  1} > .
So there exists an  > 0 such that for all N ∈ N, there exists an n > N, such that
there exists an x ∈ H with ||x||  1, but ||(PnK – K)x|| > .
The last statement allows us to construct a sequence (xnk)k∈N in X such that ||xnk|| 
 1 and ||(PnkK – K)xnk|| >  as follows.
Taking N = 1, there exists an n1 > 1 and an xn1 ∈ H with ||xn1||  1 but ||(Pn1K –
K)xn1|| > .
Taking N = n1, there exists an n2 > n1 and an xn2 ∈ H with ||xn2||  1 but ||(Pn2K –
K)xn2|| > .

Taking N = n3, there exists an n3 > n2 and an xn3 ∈ H with ||xn3||  1 but ||(Pn3K –
K)xn3|| > .
Thus (xnk)k∈N is bounded and ||(PnkK – K)xnk|| >  for all ks.
As (xnk)k∈N is bounded and K is compact, there exists a subsequence, say
(Kxnkℓ)ℓ∈N, of (Knk)k∈N, that is convergent to y, say. Then we have
a contradiction. This completes the proof of (1).
(2): As K is compact, so is K*. Thus by (1), 
 in CL(H). But 
, and so KPn 
 K in CL(H).
(3): Finally,
This completes the proof
So, in Theorem 5.6, what is y0, K0? We can take K0 = PnKPn, where Pn is the
orthogonal projection onto span{u1, ···, un}, and y0 = Pny. We note that ||y0 – y||
= ||Pny – y|| is small for large n, and ||K – PnKPn|| is small for large n. Thus  = ||
(K – K0)(I – K)−1|| is small. So if we look at the equation (I – PnKPn)x0 = Pny
instead of (I – K)x = y, then ||x – x0|| can be made as small as we please by taking
n large enough. We give a simple toy example.
Example 5.5.
Consider the operator 
 on ℓ2. For all x = (xn)n∈N ∈ ℓ2,

So ||K||  , and I – K is invertible in CL(ℓ2).
K is Hilbert-Schmidt as 
. So K is compact.
Let 
. To find approximate solutions of the equation
we fix an n ∈ N, and solve instead x – PnKPnx = Pny, that is, the system
The approximate solutions for n = 1, 2, 3, 4, 5 are given (correct up to four
decimal places) by
while the exact unique solution to the equation (I – K)x = y is given by
To 4 decimal places, this is x = (0.5, 0.3333, 0.2500, 0.2000, 0.1667, ···).
5.4 (∗) Spectral Theorem for Compact Operators
In elementary linear algebra, one learns about the Spectral Theorem, which says

that every Hermitian matrix T ∈ Cd×d is diagonalisable, with a basis of
orthonormal eigenvectors u1, ··· , ud ∈ Cd, and corresponding real eigenvalues
λ1  ···  λd, so that
Towards seeking a generalisation to the Hilbert space case, we’ll now show that
while the spectrum of a general self-adjoint operator may be quite complicated,
for a compact self-adjoint operator, things are quite similar to the finite-
dimensional case.
Theorem 5.8. (Spectral Theorem for compact, self-adjoint operators).
Let H be a Hilbert space and T = T* ∈ K(H) have infinite rank.
Then there exist orthonormal eigenvectors un, n ∈ N, with corresponding
eigenvalues λn, n ∈ N, such that 
 λn = 0, and for all x ∈ H,
We had already seen that the eigenvalues of a self-adjoint operator must be real,
and that the eigenvectors corresponding to distinct eigenvalues are orthogonal. It
is also clear that for any eigenvalue λ of T, we have that |λ|  ||T||, since if v ∈ H\
{0} is a corresponding eigenvector, then
Let us make a few more observations which will be used in proving the spectral
theorem.
Lemma 5.1. If T = T* ∈ CL(H), then ||T|| = 
.
Proof. Let M := 
.
If x ∈ H is such that ||x|| = 1, then we have by Cauchy-Schwarz that 
. Thus M  ||T||.
It remains to show the reverse inequality. For any x, y ∈ H,

and so 
. We note that by the definition
of M,
and so from the above, together with the Parallelogram Law, we obtain
Let θ ∈ R be such that T x, y  = | T x, y |eiθ. Replacing y by eiθy yields
If Tx = 0 or x = 0, then ||Tx||  M ||x|| is trivially true.
If Tx ≠ 0 and x ≠ 0, then with 
 in the above, we obtain
||Tx||||x||  M ||x||2 and so ||Tx||  M ||x||. Thus ||T||  M.
Moreover if T is compact, then this bound M is achieved, thanks to the following
result. Indeed, if x is the unit-norm eigenvector corresponding to the eigenvalue
λ whose modulus |λ| = ||T||, then
Lemma 5.2. If H is a nontrivial Hilbert space and T = T* ∈ K(H), then either
||T|| or −||T|| is an eigenvalue of T.
Proof. If T = 0, then this is trivial. So let us suppose that T is nonzero. From the
previous lemma, it follows that there is a sequence (x′n)n∈N of unit norm vectors
in H such that 
. But as Tx′n, x′n  is real, we have Tx′n, x′n  is
either | Tx′n, x′n | or −| Tx′n, x′n |. Thus either for infinitely many n, Tx′n, x′n  is
positive (and then the subsequence ( Txn, xn )n∈N with these ns converges to
||T||), or for infinitely many n, Tx′n, x′n  is negative (and then the subsequence
( Txn, xn )n∈N with these ns converges to −||T||). So Txn, xn  
 λ, where λ is
either ||T|| or −||T||. We have

So Txn – λxn 
 0. As T is compact, there is a subsequence, say (Txnk)k∈N of
(Txn)n∈N that converges, say, to y ∈ H. Then (xnk)k∈N converges to y/λ, because
λxnk = Txnk – (Txnk – λxnk) 
 y – 0 = y.
Thanks to the continuity of T, we obtain
Hence Ty = λy, and y ≠ 0 since 
.
Lemma 5.3. Let H be a Hilbert space, T = T* ∈ CL(H), and Y be a T-invariant
closed subspace of H. Then:
(1) Y⊥ is also T-invariant.
(2) The restriction T|Y⊥ : Y⊥ → Y⊥ of T to the Hilbert space Y⊥ is also self-
adjoint.
(3) If T is in addition compact, then T|Y⊥ is also compact.
Proof.
(1) Let z ∈ Y⊥. For all y ∈ Y, we have that Ty ∈ Y (Y is T-invariant!), and so
Tz, y  = z, T*y  = z, Ty  = 0. Thus Tz ∈ Y⊥.
(2) Y⊥, being a closed subspace of a Hilbert space, is itself a Hilbert space. As
Y⊥ is T-invariant, the restriction T|Y⊥ : Y⊥ → Y⊥ is well-defined. Let us
denote this restriction by . For z1, z2 ∈ Y⊥,
Thus T|Y⊥ is self-adjoint.
(3) Finally, suppose that T is compact. Let (zn)n∈N be a bounded sequence in
Y⊥. Then ||zn||Y⊥ = ||zn||. So (zn)n∈N is a bounded sequence in H. As T is
compact, (Tzn)n∈N = ( zn)n∈N has a subsequence ( xnk)k∈N that is convergent

in H. In particular, ( xnk)k∈N is Cauchy in H, and hence also Cauchy in Y⊥
(because || zn – zm|| = || zn – zm||Y⊥). As Y⊥ is complete, it follows that (
xnk)k∈N is convergent in Y⊥. So T|Y⊥ is compact.
Proof. (Of the spectral theorem). Let H1 := H and T1 := T.
By Lemma 5.2, there exists an eigenvalue λ1 of T1 and a corresponding
eigenvector u1 such that ||u1|| = 1 and |λ1| = ||T1||.
Set H2 = (span{u1})⊥. Then H2 is a closed subspace of H1, and it is also T-
invariant: TH2 ⊂ H2. Let T2 := T|H2. Then T2 is self-adjoint and compact. There
exist an eigenvalue λ2 of T2 and a corresponding eigenvector u2 such that ||u2|| =
1 and |λ2| = ||T2||. So
Clearly, {u1, u2} are orthonormal, Tu1 = λ1u1 and Tu2 = λ2u2.
Now let H3 := (span{u1, u2})⊥. Then H3 is a closed subspace of H, H3 ⊂ H2,
and as span{u1, u2} is T-invariant, we obtain TH3 ⊂ H3. Let T3 := T|H3. Then T3
is self-adjoint and compact. Thus there exist an eigenvalue λ3 of T3 and a
corresponding eigenvector u3 ∈ H3, such that ||u3|| = 1 and |λ3| = ||T3||. As u3 ∈
H3 ⊂ H2, we have that
Continuing in this manner, we get a sequence λ1, λ2, λ3, ··· of eigenvalues of T,
and a corresponding set of eigenvectors u1, u2, u3, ··· , such that
The process would stop at some n if Hn := (span{u1, ···, un−1})⊥ would become
{0}, but we will now show that thanks to the infinite rank assumption on T, this
case is impossible. Suppose, on the contrary, that Hn = {0}.
For any x ∈ H, we have 
.
As Hn = {0}, we obtain 
.

So ran T is spanned by Tu1, ··· , Tun, a contradiction to the assumption that T has
infinite rank. Thus one has an infinite sequence of eigenvectors u1, u2, u3, ···
with eigenvalues λ1, λ2, λ3, ··· .
Let us now show that (|λn|)n∈N converges to 0. As it is decreasing, it converges to
. If  > 0, then for n ≠ m,
But this contradicts the fact that T is compact, since the sequence (Tun)n∈N
should have some convergent (and hence Cauchy) subsequence. So (|λn|)n∈N
converges to 0, and hence (λn)n∈N also converges to 0.
For all x ∈ H, we have 
, and so
The last inequality above follows from Bessel’s Inequality (Exercise 4.15, page
172). Hence 
 for all x ∈ H.
Exercise 5.6.
Let H be an infinite dimensional Hilbert space, and T = T* ∈ K(H) be one-to-
one. Show that the eigenvectors of T form an orthonormal basis for H.
Exercise 5.7. Let H be a Hilbert space. Suppose that T = T* ∈ K(H) has infinite
rank, and is positive, that is, T x, x   0 for all x ∈ H.
Prove that T has a square root, that is, an operator 
 ∈ CL(H) such that (
)2 =
T.

Chapter 6
A glimpse of distribution theory
In this last chapter of the book, we will see a generalisation of functions
and their ordinary calculus to “distributions” or “generalised functions”. These
distributions will be “continuous linear functionals on the vector space of test
functions D(Rd)”:
Why study distributions? We list three main reasons:
(1) To mathematically model the situation when one has an impulsive force
(imagine a blow to an object which changes its momentum, but the force
itself is supposed to act “impulsively”, that is the time interval when the
force is applied is 0!). Similar situations arise in other instances in
mathematics and the applied sciences.
(2) To develop a calculus which captures more general situations than the
classical case. For example, what is the derivative of |x| at x = 0? 
It will turn out that this is also useful to talk about weaker notions of
solutions of Partial Differential Equations (PDEs).
(3) To extend the Fourier transform theory to functions that may not be
absolutely integrable. For example, what1 is the Fourier transform of the
constant function 1?
It turns out that the theory of distributions solves all of these three problems in
one go. This seems like a miracle, and naturally there is a price to pay. The price
is that everything classical is now replaced by a weaker notion. Nevertheless this
is useful, since it is often sufficient for what one wants to do. An example is that,
as opposed to functions on R, which have a well-defined value at every point x
∈ R, we can no longer talk about the value of a distribution at a point of R.

Let us elaborate on reason (2) above, in the context of PDEs. An example we
met earlier in Exercise 3.17, page 143, is that of the wave equation (which
describes the motion of a plucked guitar string), and we had checked that for a
twice continuously differentiable f
gives a solution with the initial condition described by f (and zero initial
velocity). However, when we pluck a guitar string, the initial shape needn’t be
C2, and in fact, it could have a corner like this:
Nature of course doesn’t care about the lack of classical differentiability of our
candidate solution to the PDE model, and produces a travelling wave solution,
with time snapshots that look like the ones shown in Figure 6.1. We must have at
sometime witnessed choppy waves on the surface of the sea on a windy day.
Thus it is desirable to weaken the notion of differentiability, to allow for such
(classically) non-differentiable solutions to nevertheless serve as solutions to the
wave equation. We will see that such a weaker notion is provided by viewing our
function as a more general object, namely a distribution, and with its weaker
distributional calculus, the wave equation will be satisfied!
So with this motivation, we will begin to learn the very basics of the theory
of distributions in this chapter, and also see a glimpse of its applications to
PDEs. For a firm foundation of the theory of distributions, one needs
preliminaries on topological vector spaces2. Here we will adopt a “working”
approach, in which we will learn rigorous, but (seemingly) ad hoc definitions
about continuity and convergence, in the spaces related to distributions. We will
make a few remarks that will serve as a guide to the reader who wishes to delve
into the subject deeper.

Fig. 6.1 Time snapshots of a plucked guitar string.
We first make a brief historical remark about the story of the development of
distributions. The prime example of a distribution, the “delta function” δa, was
introduced3 in the 1930s in order to do quantum mechanical computations (as
eigenstates of the position operator). However, a firm mathematical foundation
for this and other generalised functions had to wait till the 1950s when the
French mathematician Laurent Schwartz introduced the concept of distributions
and developed its theory. For this, he was awarded the Fields medal.
6.1 Test functions, distributions, and examples
It will turn out that distributions are functionals on a vector space, namely the
vector space of “test functions”. Just like a function f : Rd → C can be evaluated
at a point x ∈ Rd giving a number f(x), we will see later on that a distribution T
on Rd can be “tested against” a test function φ, giving a number T, φ . Let us
first begin by describing this vector space D(Rd) of test functions.

Definition 6.1. (Test function). A test function φ : Rd → C is an infinitely
differentiable function, for which there exists a compact set, outside which φ
vanishes. The set of all test functions is denoted by D(Rd). Equipped with
pointwise operations, D(Rd) is a vector space. Thus:
Example 6.1. Let φ : R → R be given by
We claim that φ is a test function, that is, it is an element of D(R).
It is clear that φ vanishes outside the compact interval [−1, 1].
Moreover, it is also infinitely many times differentiable:
Indeed, it can be seen that the function f : R → R given by
is clearly infinitely many times differentiable outside 0, and also
showing that f is infinitely many times differentiable everywhere on R. (See
Exercise 6.1 where the details are spelt out.)
The graph of f is shown in the following picture.
The function φ is just the composition of f with the polynomial x  1 − x2.

The picture below shows the graph of φ.
Similarly, we could have composed f with the function
and obtained a function in D(Rd) that is C∞ and is zero outside the closed unit
ball B(0, 1) := {x ∈ Rd : ||x||2  1} in Rd. Note that as B(0, 1) is closed and
bounded in Rd, it is compact.
Based on the above example, it might seem that test functions are rather special,
and are few and far between. But let us note that whenever we have a φ ∈
D(Rd), then for every λ > 0 and every a ∈ Rd, also the functions
belong to D(Rd). Moreover, it is easy to see that D(Rd) is closed under partial
differentiation. (In the following, it will be convenient to introduce the following
notation: if k = (k1, · · · , kd) is a multi-index of nonnegative integers, then
In this notation, we have: φ ∈ D(Rd)  Dkφ ∈ D(Rd) for all k.) By taking linear
combinations, we see that we thus get a huge abundance of functions in D(Rd).

Exercise 6.1. (∗)(A C∞ function which is not analytic.)
(1) Suppose that f : R → R is continuous on R, continuously differentiable on
R∗ := R\{0}, and such that 
 f′(x) exists.
Show that f is continuously differentiable on R.
(2) Let f : R → R be n − 1 times continuously differentiable, n times
continuously differentiable on R∗, and such that 
 f(n)(x) exists.
Show that f is n times continuously differentiable on R.
(3) Let f : R → R given by 
Show that f is infinitely many times differentiable.
Hint: Using induction on n, show that for x > 0, f(n)(x) = Rn(x)f(x), where Rn
is a rational function. Conclude that 
 x−n f(x) = 0.
Exercise 6.2. Solve 
 = 0 in D(R2).
Exercise 6.3. Show that {Φ′ : Φ ∈ D(R)} = {φ ∈ D(R) : 
 φ(x)dx = 0} =: Y.
Also, show that if φ ∈ Y, then the Φ ∈ D(R) such that Φ′ = φ is unique.
Definition 6.2. (Convergence in D(Rd)).
We say that a sequence (φn)n∈N converges to φ in D(Rd) if
(1) there exists a compact set K ⊂ Rd such that all the φn vanish outside K,
and
(2) φn converges uniformly4 to φ, and for each multi-index k, Dkφn
converges uniformly to Dkφ.
We then simply write 
.
Remark 6.1. (Topology on D(Rd)).
It turns out that D(Rd) is a topological space with a certain topology (with a
collection of open sets denoted by, say O), allowing one to then talk about
convergent sequences in this topology given by O. It can be shown that (φn)n∈N

converges to φ in the topological space (D(Rd), O) if and only if it converges in
the above sense. See for example [Bremermann (1965), Appendix 1, pages 37–
40].
Exercise 6.4. Let (φn)n∈N be a sequence in D(R) such that for each n ∈ N,
From Exercise 6.3, it follows that for each n ∈ N, there exists a unique Φn ∈
D(R), such that 
. Suppose moreover that 
. Prove that 
.
Now that we have a vector space D(Rd), with a topology, one can talk about
continuous linear transformations D(Rd) from C, and these are called
distributions. The precise definition is as follows.
Definition 6.3. (Distribution).
A distribution T on Rd is a map T : D(Rd) → C such that
(1) (Linearity) For all φ, ψ ∈ D(Rd) and all α ∈ C,
(2) (Continuity) If 
, then T(φn) → T(φ).
The set of all distributions is denoted by D′(Rd).
With pointwise operations, D′(Rd) is a vector space.
We will usually denote T(φ) for φ ∈ D(Rd), by T, φ .
Remark 6.2. It is enough to check the continuity requirement with φ = 0, since
from the linearity of T, it follows that T(φn) − T(φ) = T(φ − φn), and it is clear
that 
 if and only if 
.
Remark 6.3. It can be shown that a linear transformation from D(Rd) to C is

continuous, with respect to the aforementioned (Remark 6.1) topology on D(Rd)
given by O, if and only if it is continuous in the above sense (given by item (2)
in the above definition). See for example [Rudin (1991), Theorem 6.6 on page
155].
When first encountered, distributions may seem very strange objects indeed, far
removed from the world of ordinary functions that we are used to. But now we’ll
see that practically all functions one meets in practice, can be viewed as
distributions. This will enable us later, to apply the distributional calculus we
develop, also to functions (by viewing them as distributions), and it will be in
this sense, that we’ll be able check that even with a non-C2 function f, the u
given by (6.1) satisfies the wave equation.
Example 6.2. (
(Rd) functions are distributions.)
Let f : Rd → C be a locally integrable function (written f ∈ 
(Rd), that is, for
every compact set K,
Here dx stands for the volume element dx1 · · · dxd in Rd.
Then f defines a distribution Tf as follows:
The integral exists since φ is bounded and zero outside a compact set, and so we
are actually integrating over a compact set.
It is also easy to see that Tf is linear.
Continuity: Let 
. Then there is a compact set K such that all φn, n ∈ N,
vanish outside K. We have
since φn converges to 0 uniformly on K (the derivatives of φn play no role here).

Consequently, Tf ∈ D′(Rd).
Distributions Tf, where f is locally integrable, are called regular distributions. An
example of a function f ∈ 
(R) is the Heaviside function, whose graph is
displayed in the picture on the left below.
The Heaviside5 function H is given by 
The value H(0) of the function H at x = 0 can be arbitrarily assigned, since for
any φ ∈ D(R), the value of the integral
won’t change, no matter what choice we make for H(0)! So the distribution TH
will be the same, irrespective of what number we set H(0) to be.
We denote the distribution TH simply by the symbol H from now on.
Theorem 6.1. The following are equivalent for f, g ∈ 
(Rd) :
(1) Tf = Tg.
(2) For almost all x ∈ Rd, we have f(x) = g(x).
In particular, if f and g are continuous, then Tf = Tg if and only if f = g.
Theorem 6.1 means that the map f  Tf, from 
(Rd) (which is the space of
equivalence classes of locally integrable functions that are equal almost
everywhere), to D′(Rd), is injective. In practice, one identifies (the equivalence
class of) f ∈ 
(Rd) with the distribution Tf, and one considers the map f  Tf : 
(Rd) → D′(Rd) as an inclusion: 
(Rd) ⊂ D′(Rd). So just like we identify
each integer as a rational number, we may think of all locally integrable

functions as distributions. Since the map f 
 Tf is linear (that is, Tf+g = Tf + Tg
and Tαf = αTf), this identification respects addition and scalar multiplication. As
the space C(Rd) of continuous functions can be considered as a subspace of 
(Rd) (two continuous functions on Rd that are equal almost everywhere are
identical), we get the following inclusions:
We won’t include the proof of Theorem 6.1 here. The interested reader is
referred to [Schwartz (1966), Theorem 2, page 74]. The following example
shows that the inclusion 
(Rd) ⊂ D′(Rd) is strict, that is, not all distributions
are regular.
Example 6.3. (Dirac delta distribution).
The distribution δ ∈ D′(Rd) is defined by
More generally, one defines, for a ∈ Rd, a distribution δa by
It is evident that δa is linear and continuous on D(Rd), that is, it is a distribution.
The delta distribution is not regular: there is no function f ∈ 
(Rd) such
that δa = Tf. Nevertheless, in a huge amount of literature, one encounters a
manner of writing that suggests that δa is a regular distribution. In place of δ, φ ,
one writes
One then talks about delta “functions” instead of delta distributions. This is of
course incorrect (see Exercise 6.5 below), but in some sense useful if one wants
to do formal manipulations in order to guess answers, or in order to get physical
insights etc.

With this fallacious viewpoint, one often depicts the “graph of δa ∈ D′(R)” as a
spike positioned at a, with the intuitive feeling that the “δa function is
everywhere 0, but is infinity at x = a, and has integral over R equal to 1”!
Exercise 6.5. Show that there is no function δ : R → R such that for all a > 0:
(1) δ is Riemann integrable on [−a, a], and
(2) for every C∞ function φ vanishing outside [−a, a], 
 δ(x)φ(x)dx = φ(0).
Example 6.4. pv .
Although the function x 
  (defined almost everywhere on R) is not locally
integrable, nevertheless one can associate a distribution T with this function: for
all φ ∈ D(R),
(pv stands for “principal value”).
That the limit above exists, and that T defines a distribution can be seen as
follows. Suppose that φ = 0 on R\[−a, a], where a > 0. We have:
Since 
, we have
Thus 
 exists, and 
.

Continuity: Let 
, and suppose that a > 0 is such that for all n ∈ N, φn = 0
outside [−a, a]. Then | T, φn |  2a · 
.
This distribution is denoted by pv , so that 
.
(In quantum mechanics (perturbation theory) one encounters
having the property δ = δ + + δ−.)
Exercise 6.6.
(1) For φ ∈ D(R), set 
. Show that T ∈ D′(R).
(2) Give an example of a sequence of test functions (φn)n∈N such that:
(a)
 uniformly, and for all k ∈ N, also 
 uniformly, but
(b)
.
Hint: Consider a sequence of test functions of the type 
 with an
appropriate choice of φ.
(3) Does the construction in part (2) contradict our conclusion in (1) that T is a
distribution?
6.2 Derivatives in the distributional sense
We will now develop a calculus for distributions.
Let us first consider the case when d = 1.
Definition 6.4. (Distributional derivative, d = 1). Let T ∈ D′(R).
Then the distributional derivative 
 of T is defined by
Note that if φ ∈ D(R), then clearly φ′ ∈ D(R). So the right-hand side above is
well defined.
Moreover, the map φ  −T, φ′  is linear: for all φ, ψ ∈ D(R) and α ∈ C,

Continuity of T′: If 
, then also 
, and so 
. Thus T′ ∈ D′
(R).
Q. Is distributional differentiation an extension of classical differentiation?
A. Yes. The result below (Lemma 6.1) shows that our new definition is a
sensible generalisation from the classical case: If we have a continuously
differentiable function, and if we choose to put on our “distributional
glasses”, then the distribution we get, by distributionally differentiating the
corresponding regular function, is a regular distribution corresponding to the
classical derivative.
Lemma 6.1. If f ∈ C1(R), then (Tf)1 = Tf′.
Proof. Let φ ∈ D(R) be such that it vanishes outside [a, b]. Then using
integration by parts,
(Here we have used that φ, φ′ are zero outside [a, b], and φ(a) = φ(b) = 0.) This
completes the proof.
The above result means that whenever one identifies the function f with the
distribution Tf, then the two possible interpretations of the derivative which arise
– the classical sense versus the new distributional sense – coincide. In fact, this
is the motivation behind our definition of the distributional derivative given in
Definition 6.4.
The next example shows that now, endowed with our notion of the
distributional derivative, we can differentiate functions which we couldn’t earlier
(albeit only in the distributional sense).
Example 6.5. (H′ = δ).

Let us show that the Dirac distribution is the distributional derivative of regular
distribution corresponding to the Heaviside function H ∈ 
(R). For any test
function φ ∈ D(R), we know that φ(x) = 0 for all sufficiently large x, and so
and so H′ = δ.
Example 6.6. (Dipole).
The derivative δ′ of δ, is called the dipole, and is given by
for all φ ∈ D(R).
In Example 6.5, the classical derivative of H for the regions x < 0 and x > 0 is
equal to 0 everywhere, and it is right to think philosophically that the δ appeared
owing to the jump in the values of H from 0 to 1 as x went from negative to
positive values.
So we can write
where T0 is the regular distribution corresponding to the zero function 0, and the
coefficient 1 multiplying δ0 is the “jump” in the value of H at 0. This is no
coincidence, and the observation can be generalised as follows.
Proposition 6.1. (Jump Rule).
Let f be continuously differentiable on R except at the point a ∈ R, where the
limits f(a+), f(a−), f′(a+), f′(a−) exist.
Then f, f′ are locally integrable, and

We think of f(a+) − f(a−) as the jump in f at the point a. One can formulate this
result by saying:
The derivative of f in the sense of distributions is
the classical derivative plus δa times the jump in f at a.
Proof. Let φ ∈ D(R), and suppose that φ is 0 outside [α, β], and that a ∈ [α, β].
Then
Example 6.7. (|x|′ = 2H − 1). In the sense of distributions,
Indeed, the jump in |x| at x = 0 is 
 |x| − 
 |x| = 0 − 0 = 0.

For x ≠ 0, |x| is differentiable with derivative 
 = 2H(x)−1.
Finally, 
(2H(x) − 1) = 1 and 
(2H(x) − 1) = −1.
So by the Jump Rule, (T|x|)′ = T2H−1, or briefly |x|′ = 2H − 1 in the sense of
distributions.
Remark 6.4. This result can be extended to the case when f is continuously
differentiable everywhere except for a finite number of points ak, and at these
points ak, the function satisfies the same assumptions as stipulated above. This
then leads to
The proof is analogous.
In fact the result even extends to the case when f has infinitely many, but locally
finite, jump discontinuities: in any compact interval, one finds only finitely many
discontinuities. The sum on the right-hand side is the distribution defined by
where, for a given test function φ, only finitely many terms on the right-hand
side are nonzero.
Example 6.8. 
. See the pictures below.

Here ·  is the greatest integer function, that is, for x ∈ R, x  is the greatest
integer less than or equal to x.
Exercise 6.7.
Show that 
H(x) cos x = −H(x) sin x + δ and 
H(x) sin x = H(x) cos x.
One can define higher order distributional derivatives by iteratively setting T(n)
:= (Tn−1)′ for n  2.
Exercise 6.8. (Fundamental Solution to the 1D Laplace equation).
Show that the equation 
 is satisfied by 
.
Exercise 6.9. (Zero distributional derivative implies constancy.)
The aim of this exercise is to show that if T ∈ D′(R), and T′ = 0, then there exists
a constant c such that T = Tc, the regular distribution associated with the constant
function taking value c everywhere on R. To prove this result, we will proceed
as follows.
(1) Let V be a complex vector space. If ℓ, L ∈ L(V, C) are such that ker ℓ ⊂ ker
L, then there exists a constant c ∈ C such that L = cℓ.
Hint: If v0 ∈ V is such that ℓ(v0) ≠ 0, then show that every vector v ∈ V can
be decomposed as v = cvv0 + w for some cv ∈ C and some w ∈ ker ℓ.
(2) Prove, using part (1) and Exercise 6.3, page 232, that if the derivative of the
distribution T is zero, then T must be constant.
Exercise 6.10. Show that if T ∈ D′(R), then there exists an S ∈ D′(R) such that S
′ = T. Moreover, show that such an S is unique up to an additive constant.
Hint: Fix any φ0 ∈ D(R)\{0} which is nonnegative everywhere. For ψ ∈ D(R),
belongs to the subspace Y of D(R) from Exercise 6.3, page 232.
Hence there exists a unique Φ ∈ D(R) such that Φ′ = φ. Set S, ψ  = −T, Φ .
Exercise 6.11. Show that for all n ∈ N, δ(n) ≠ 0.

Exercise 6.12. Show that {δ, δ′, · · · , δ(n), · · ·} is linearly independent in D′(R).
When d > 1, the definition of the distributional derivative is analogous.
Definition 6.5. (Distributional partial derivatives).
For T ∈ D′(Rd), the ith-partial derivative 
 of T, 1  i  d, is defined by
Exercise 6.13. Show that for all T ∈ D′(Rd) and all i, j, 
.
Exercise 6.14. The Heaviside function in two variables, H : R2 → R, is defined
by
(That is, H is the indicator function 1[0,∞)2 of the “first quadrant”.)
Show that 
.
Exercise 6.15. (Fundamental solution of the Laplacian on R2).
Verify that 
, where 
 and 
.
Remark 6.5. (Sobolev spaces).
There exist Hilbert spaces analogous to the spaces Cn[a, b] defined by:
equipped with the norm || · ||n defined by:
where || · ||2 denotes the L2 norm. This norm is induced by the inner product

For example: H1(a, b) = {f ∈ L2(a, b) : f′ ∈ L2(a, b)} and
Since f merely belongs to L2(a, b), we are aware that f(k) may not have any
meaning in general, and so one ought to make the definition precise. This can be
done with the theory of distributions. The space Hn(a, b) is the space of
functions f ∈ L2(a, b) such that f′, · · · , f(n), defined in the sense of distributions,
belong to L2(a, b). Then it can be showed that the space Hn(a, b) is a Hilbert
space. The Sobolev spaces Hn(a, b) are named after the Russian Mathematician
Sergei Sobolev.
6.3 Weak solutions
A function satisfying a PDE in the sense of distributions will be called a weak
solution to a PDE.
Example 6.9. Let u(x, t) := H(t)ex. Then u is locally integrable.
We’ll show that u is a weak solution of the PDE 
.
We have for all φ ∈ D(R2) that

and so 
 in the sense of distributions.
The following picture shows the graph of this u.
Far from being a classical solution (for which we would want u ∈ C1) to the
PDE, we see that u isn’t even continuous! Nevertheless we accept it as a solution
to PDE, since it satisfies the PDE, albeit in the weak sense.
Remark 6.6. (Other notions of weak solutions). Besides distributional solutions,
there are other notions too of weak solutions in PDE theory, for example
“viscosity solutions” (which are natural in certain contexts, such as the
Hamilton-Jacobi equation for optimal control).
Remark 6.7. (Why are weak solutions important?) Weak solutions are
important, since as mentioned before, many initial/boundary value problems for
PDEs encountered in real world may not possess sufficiently smooth solutions,
but only weak solutions, which should not be dismissed. We will see two
examples below.
Another reason is “theoretical”: even when there exist classical solutions, it
might be easier to find/show the existence of distributional solutions first, and
then show later that the solution is in fact sufficiently smooth (and such results
are called “regularity results”).
Weak solution to the transport equation
The transport equation is given by

where f is the initial condition, and c ∈ R is a constant.
This equation arises for instance when one models fluid flow6.
It is easy to check that if f ∈ C1, then a solution is given by
Indeed, u(x, 0) = f(x + c · 0) = f(x + 0) = f(x), and
But now, we’ll show that even when f ∈ 
(R), the same formula, namely u(x,
t) := f(x + ct), still gives a solution u to the transport equation. The only change is
that it will be a weak solution, that is, the PDE will be satisfied in the
“distributional sense”. To see this, let φ ∈ D(R2). Then:
Hence to prove our claim, it must be shown that the above integral is zero. To do
this, we will make the following change of variables7:
Recall that for a double integral, one has the following “change of variables”
formula under the change of variables given by the map 
:
where 
.
In our case, the derivative of the map 
 is

whose determinant is 1.
We have
Thus
where we have used the Fundamental Theorem of Calculus to simplify the inner
integral, and used the fact that φ has compact support to obtain

See the following picture.
This shows that u(x, t) := f(x + ct) is indeed a weak solution to the transport
equation.
Weak solution to the wave equation
Recall that if f ∈ C2(R), then
is a classical solution to
with the initial condition u(x, 0) = f(x) and with zero initial speed ut(x, 0) = 0.
Let us now show that even when f is merely locally integrable, u given by
(6.5) satisfies the wave equation, but in the sense of distributions. In order to do
this, we will use our result from the previous section, where we considered the
transport equation.
Let f ∈ 
(R). Putting c = 1, we have seen that u+ given by
satisfies, in the sense of distributions, the transport equation

Similarly, putting c = −1, we also see that u− given by
is a weak solution to
For any distribution 
 (Exercise 6.13, p. 242). So
Using this observation, we will find a weak solution to the wave equation too.
Let u be given by (6.5), and φ ∈ D(R2). Then
To see the equality (∗) in the last line, we note that since φ ∈ D(R2), also
But as

we see that the equality (∗) above holds.
Exercise 6.16. (Weak solution exists, but no classical solution).
Show that 
 is a weak solution of the ODE u′ = H,
where H is the Heaviside function.
6.4 Multiplication by C∞ functions
In general, it is not possible to define the product of two distributions. For
example, the product of two locally integrable functions is not in general locally
integrable. (f := 1/
 is locally integrable, but f2 = 1/|x| isn’t!) So the product of
two regular distributions in general may not define a distribution.
However, one can define the product of a function α ∈ C∞(Rd) with a
distribution T ∈ D′(Rd) as follows.
Definition 6.6. (Multiplication of a distribution by a smooth function). Let α ∈
C∞(Rd) and T ∈ D′(Rd). Then αT ∈ D′(Rd) is defined by
Note that if φ ∈ D(Rd), then it is in particular in C∞(Rd), and so it is clear that αφ
is infinitely many times differentiable. Moreover, as φ vanishes outside a
compact set, so does αφ. Hence αφ ∈ D(Rd), and the right-hand side makes
sense. It is also easy to see that the map
is linear, thanks to the linearity of T. Finally, the continuity of αT can be
established by using the multivariable Leibniz Rule for differentiating the
product of two functions, which we recall here first:
Leibniz Rule: For a multi-index n = (n1, · · · , nd) of nonnegative integers n1, · ·
· , nd, define its
– order |n| by n1 + · · · + nd, and
– factorial by n! = n1! · · · nd!.

Then the (multivariable) Leibniz Rule states that for every multi-index n := (n1, ·
· · , nd), and functions f, g ∈ C∞(Rd),
where 
, and 
.
(We will omit the cumbersome, although straightforward, proof of the Leibniz
Rule, which proceeds by induction on the order |n| of Dn, and by using the one
variable mth derivative formula for the product of two functions.)
Using the Leibniz Rule, it can be seen that if 
, and if α ∈ C∞(Rd), then
also 
. Consequently, αT ∈ D′(Rd).
This product of distributions with smooth functions extends the usual pointwise
product of a function with a smooth function.
Proposition 6.2. If f ∈ 
(Rd) and α ∈ C∞(Rd), then αTf = Tαf.
Proof. α is bounded on every compact set, and so it follows that αf is locally
integrable. For φ ∈ D(Rd), we have
This completes the proof.
The above result means that, whenever we identify as usual the elements of 
(Rd) with distributions, then the two a priori different manners of forming the
product with α lead to the same result.
Example 6.10. One can think of the distribution H(x) cos x as the product of the
C∞ function cos x with the distribution H(x).
Proposition 6.3. The following calculation rules hold.

For T, T1, T2 ∈ D′(Rd), α1, α2, α, β ∈ C∞(Rd), we have
(1) α(T1 + T2) = αT1 + αT2
(2) (α1 + α2)T = α1T + α2T
(3) (αβ)T = α(βT)
(4) 1T = T. (Here 1 ∈ C∞(Rd) is the constant function Rd ∋ x  1.)
(Thus D′(Rd) is a C∞(Rd)-module8.)
Proof. All of these follow from the definition of multiplication of distributions
by C∞ functions. For example, to check (3), note that for all φ in D(Rd), (αβ)T,
φ  = T, (αβ)φ  = T, β(αφ)  = βT, αφ  = (α(βT)), φ , proving the claim.
The product rule for differentiation is valid in the same manner as for functions.
Theorem 6.2. (Product Rule). For T ∈ D′(Rd) and α ∈ C∞(Rd),
Proof. When d = 1 and φ ∈ D(R), we have (αφ)′ = α′φ + αφ′, and so
The proof is analogous when d > 1. 0
Theorem 6.3. If a ∈ Rd and α ∈ C∞(Rd), then αδa = α(a)δa .
Proof. For φ ∈ D(Rd), we have

Example 6.11. (δa, a ∈ R, are eigenvectors of the position operator).
Let us recall Exercise 2.35, page 103, where we showed that the position
operator Q : DQ(⊂ L2(R)) → L2(R) given by (Qf)(x) = xf(x), x ∈ R, f ∈ DQ, has
empty point spectrum, and so it has no eigenvectors.
But we can “extend” the operator Q to act not just on functions on R, but
also distributions:
Then Q is a linear transformation from D′(R) to itself.
The result in Theorem 6.3 above shows that, for all a ∈ R,
and so δa ∈ D′(R), serves as an eigenvector, with corresponding eigenvalue a ∈
R, of the position operator Q ∈ L(D′(R)). (The physicist Paul Dirac used this in
1926 for Quantum Mechanical computations.)
Example 6.12. We have xδ = 0, (cos x)δ = δ, (sin x)δ = 0.
Exercise 6.17. Redo Exercise 6.7, page 241, using the Product Rule.
Exercise 6.18. (Fundamental Solutions).
Show the following, where λ ∈ R, n ∈ N, ω ∈ R\{0}:
Exercise 6.19. Show that if α ∈ C∞(R), then αδ′ = α(0)δ′ − α′(0)δ. Conclude that
xδ′ = −δ.
Exercise 6.20. For T ∈ D′(R), define 
.

Show that for all T ∈ D′(R), 
.
(Thus the commutant of 
, namely 
Exercise 6.21. Show that u(x, y) := e−3yxH(y) is a weak solution of
Exercise 6.22. Show that on D′(R) it is impossible to define an associative and
commutative product such that for α ∈ C∞(R) and T ∈ D′(R), it agrees with
Definition 6.6. Hint: Consider the product of δ, x and pv .
Exercise 6.23.
(1) Let T be a distributional solution to the differential equation 
.
Show that T is a classical solution: T = ceλx. Hint: Differentiate e−λxT.
(2) (Hypoellipticity9 of 
 
Let f ∈ C∞(R), and T ∈ D′(R) be a solution to
Show that T is equal to a classical solution, and that T = F + ceλx, where F is
a classical (namely C∞) solution of (∗).
(3) Consider an ordinary differential operator with constant coefficients:
Let f ∈ C∞(R) and let T be a distributional solution of DT = f.
Show that T is a classical solution, namely T ∈ C∞.
Hint: If λ is a root of the polynomial P(ξ) = a0 + aξ + · · · + anξn, then D can
be written as the product 
, where D1 is a differential operator
of order n − 1.
(4) Let E∗ be a fundamental solution of the differential operator D, that is, let
DE∗ = δ. What can one say about the set of all fundamental solutions of D?

Exercise 6.24.
Show that the distributional solutions T to xT = 0 are scalar multiplies of δ = δ0.
Hint: Show that ker δ = {xφ : φ ∈ D(R)} and use Exercise 6.9(1), page 241.
6.5 Fourier transform of (tempered) distributions
We make a few final parting remarks for this chapter, which are somewhat
sketchy, but aim to give a glimpse of what lies ahead. One would like to extend
the classical Fourier transform theory to distributions. From our previous
definitions (for example that of differentiation of a distribution), we know that
the philosophy is, to transpose the stuff we want to do to a distribution, to an
appropriate related thing on the test function, so that the new definition matches
with the classical one. Continuing in this spirit, we would like to define the
Fourier transform of “nice” distributions T ∈ D′(R) in such a manner, so that if T
= Tf, with f ∈ L1(R) (say10), then one has 
. Proceeding formally, we ought
to have for test functions φ that
So motivated by this, one could hope to define the Fourier transform of a
distribution T by setting
where  is the classical Fourier transform of φ ∈ D(R), defined by
But the above calculation is all wrong! Indeed, for a test function φ ∈ D(R), the
Fourier transform  may not have compact support11, and so  does not belong

to D(R) (unless φ = 0). In light of this problem (that the Fourier transform of test
functions are no longer test functions), it makes sense to work with a bigger
class of test functions that are closed under Fourier transformation, and then
work with only those distributions that are well-behaved with this larger class of
test functions (and then these distributions will be deemed to be “Fourier
transform-able”). With this little motivation, we will consider the Schwartz class
S(R) of test functions, defined below. Although this story can be developed in
Rd with d  1 in general, we will just work with d = 1 here for simplicity.
Definition 6.7. (The Schwartz space S(R) of test functions).
The Schwartz space S(R) of test functions is the set of all functions φ : R → C
such that:
(a) φ is infinitely many times differentiable, and
(b) for all nonnegative integers ℓ, m, 
.
With pointwise operations, S(R) is a vector space.
S(R) is closed under differentiation, and multiplication by polynomials.
It is also immediate that D(R) ⊂ S(R).
An example of a function in S(R)\D(R) is e−x2.
Exercise 6.25. Show that e−x2 ∈ S(R).
Definition 6.8. (Convergence in S(R)).
A sequence (φn)n is said to converge to 0 in S(R), written 
, if for all
nonnegative integers ℓ, m, we have 
.
Exercise 6.26.
Show that if (φn)n∈N is a sequence of test functions in D(R) such that 
,
then we have 
The following result can be shown, but it will take us a bit far afield, and so we

skip its somewhat technical proof.
Proposition 6.4.
 : S(R) → S(R) is a (linear and) continuous map, that is,
(1) If φ ∈ S(R), then  ∈ S(R).
(2) If (φn)n∈N is a sequence in S(R) such that 
 as n → ∞, then 
.
Definition 6.9. (Tempered distribution).
A tempered distribution T on R is a map T : S(R) → C such that
(1) T is linear, and
(2) if (φn)n∈N is a sequence in S(R) such that 
 as n → ∞, then T, φn
→ 0.
The vector space of all tempered distributions (with pointwise operations) is
denoted by S′(R).
Also, since D(R) ⊂ S(R), and since the inclusion is continuous in the sense of
Exercise 6.26, it follows that S′(R)⊂ D′(R).
However, it can be shown that the inclusion S′(R) ⊂ D′(R) is strict, as shown
below:
Example 6.13. (ex2 ∈ D′(R)\S′(R)).
ex2, being continuous, is locally integrable, and hence Tex2 ∈ D′(R).
But ex2 does not define a tempered distribution, since, for example, its action on
the test function e−x2 ∈ S(R), is not finite:
Thus Tex2 ∉ S′(R).

Example 6.14. (L1(R) ⊂ S′(R)).
Let f ∈ L1, that is, ||f||1 := 
 |f(x)|dx < ∞.
We claim that the regular distribution Tf is tempered, that is, T ∈ S′(R).
For φ ∈ S(R), | Tf, φ | =  
 f(x)φ(x)dx   
 |f(x)||φ(x)|dx  ||f||1||φ||∞.
From here it follows that if (φn)n∈N is a sequence in S(R) such that 
 as n
→ ∞, then T, φn  → 0. Hence Tf ∈ S′(R).
Example 6.15. (δ ∈ S′(R)).
The map 
 : S(R) → C is clearly linear.
It is also continuous, since if 
, then, in particular, φn(0) → 0.
Exercise 6.27. (L∞(R) ⊂ S′(R)).
Show that if f is a bounded function on R, then Tf ∈ S′(R).
Derivative of tempered distributions.
If T ∈ S′(R), then we define T′ : S′(R) → C by
It is easy to see that T′ ∈ S′(R).
Multiplication of tempered distributions by polynomials.
Recall that elements of D′(R) could be multiplied by C∞ functions. This luxury
is not available for tempered distributions: just think of multiplying the constant
function 1 ∈ L∞(R) ⊂ S′(R) by ex2 ∈ C∞(R): their product is not tempered, since
by Example 6.13, we know that ex2 ∉ S′(R)!
But while elements in S′(R) can’t be multiplied by general α ∈ C∞(R), they
can nevertheless be multiplied with polynomials as follows:

For T ∈ S′(R), we define xT : S(R) → C by
Then it is easy to see that xT ∈ S′(R).
Fourier transformation of tempered distributions.
Definition 6.10. (Fourier transform of tempered distributions).
If T ∈ S′(R), then its Fourier transform is the tempered distribution
, defined by 
, for all φ ∈ S(R).
Using Proposition 6.4, we can see that 
 : S(R) → C defines a tempered
distribution.
Exercise 6.28. Show that if f ∈ L1(R), then 
.
Example 6.16. (Fourier transform of the Dirac δ). 
For φ ∈ S(R), we have that
So the Fourier transform  of the tempered distribution δ is the regular tempered
distribution corresponding to the constant function 1.
Much of the classical Fourier transform theory, can be extended appropriately
for the class of tempered distributions. For example, for tempered distributions T
∈ S′(R), we have
This plays an important role in linear differential equation theory: by taking
Fourier transforms, the analytic operation of differentiation is converted into the
algebraic operation of multiplication by the Fourier transform variable. Another
instance is “convolution theorem”: we know that if f, g ∈ L1(R), then

multiplication on the Fourier transform side corresponds to convolution:
Under certain technical conditions12 on distributions T, S ∈ D′(R), one can
define their convolution T ∗ S ∈ D′(R). Then a distributional analogue of the
convolution theorem is often available: for example, for T ∈ S′(R), and for a
“compactly supported” S ∈ D′(R), one has13
These results allow one to rigorously justify some of the formal calculations met
in engineering. It also gives rise to some important auxiliary concepts, useful in
the theory of PDEs. One such notion, related to convolution of distributions, is
the concept of a fundamental solution for a linear PDE.
Definition 6.11. (Fundamental Solution).
Given a linear partial differential operator with constant coefficients,
a fundamental solution is a distribution E ∈ D′(Rd) such that
In the above, for a multi-index k = (k1, · · · , kd), |k| := k1 + · · · + kd.
Fundamental solutions are useful, since they allow one to solve the
inhomogeneous equation
For suitable g, it can be shown that u := E ∗ g does the job:
There is also a deep result14 saying that every nonzero operator D with constant

coefficients has a fundamental solution in D′(Rd). Fundamental solutions with
appropriate boundary conditions specific to a PDE problem are sometimes
referred to as Green’s functions.
Example 6.17. It follows from Exercise 6.8, page 241, that a fundamental
solution for the one-dimensional Laplacian operator
is Ep := |x|/2. In fact if we add to Ep any solution to the homogeneous equation u
″ = 0, then it will also be a fundamental solution. So the functions ax + b + |x|/2,
with arbitrary constants a and b, are all fundamental solutions of the one-
dimensional Laplacian operator.
Notes
Some parts of this chapter are inspired by the lectures on Distribution Theory
given by Professor Erik G.F. Thomas at the University of Groningen [Thomas
(1996)].
1Although the classical Fourier transform does not exist, it can be shown that in the sense of distributions,
the Fourier transform of the constant function 1 is the Dirac delta distribution δ.
2That is, vector spaces with a topology making the vector space operations of addition and scalar
multiplication continuous. Normed- and inner product-spaces are particular examples of topological vector
spaces, where the topology is given by a norm, but it turns out that there are weaker ways of specifying a
topology, which aren’t generated by a norm.
3There were, however, earlier usages of such an object; for example an infinitely tall, unit impulse
function was used by Cauchy in the early 19th century. The Dirac delta function as such was introduced as a
“convenient notation” by the English physicist Paul Dirac in his book, The Principles of Quantum
Mechanics, where he called it the “delta function”, as a continuous analogue of the discrete Kronecker
delta.
4That is, for every  > 0, there exists an N ∈ N such that for all n > N and all x ∈ K, |φn(x) − φ(x)| < .
5Named after Oliver Heaviside (1850–1925), self-taught English physicist, who among other things,
developed an “operational calculus” to solve linear differential equations. His methods were not rigorous,
and he faced criticism from mathematicians. The Heaviside operational calculus can be justified using
distribution theory; see for example [Schwartz (1966), pages 128–130].
6See for example [Pinchover and Rubinstein (2005), page 8].
7A motivation for this particular change of variables comes from hindsight, see equation (6.3) on page
246: the aim is to view the integrand in (6.2) as a derivative in one of the variables so that one can apply the

fundamental theorem of calculus.
8A module is just like a vector space, except that the underlying field is replaced by a ring. For the
module D′(Rd) we consider, the underlying ring is C∞(Rd), with pointwise addition and multiplication. We
note that not every nonzero element in C∞(Rd) has a multiplicative inverse; for example a function which is
zero on a strict subset of Rd such as x 
 x1. This is the only thing which is missing for a ring from the list
of satisfied field axioms.
9D is hypoelliptic if u ∈ D′ and Du ∈ C∞ implies u ∈ C∞.
10We start with such functions, since we know that for absolutely integrable functions f, the classical
Fourier transform  is a well-defined function, which is bounded and continuous on R.
11In fact, it can be shown that  belongs to D if and only if φ = 0.
12See for example, [Hörmander (1990), Chapter IV].
13See for example, [Hörmander (1990), Theorem 7.1.15, page 166].
14Due to Malgrange and Ehrenpreiss.

Solutions
Solution to Exercise 0.1, page ix
We have 
.
On the other hand, 
.
Clearly f(x2) > f(x1), and so the mining operation x1 is preferred to x2 because it
incurs a lower cost.
Solutions to the exercises from Chapter 1
Solution to Exercise 1.1, page 7
True. Indeed we have:
(V1) For all x, y, z > 0, x  (y  z) = x  (yz) = x(yz) = (xy)z = (xy)  z = (x  y)  z.
(V2) For all x > 0, x  1 = x1 = x = 1x = 1  x.
(So 1 serves as the zero vector in this vector space!)
(V3) If x > 0, then 1/x > 0 too, and x  (1/x) = x(1/x) = 1 = (1/x)x = (1/x)  x.
(Thus 1/x acts as the inverse of x with respect to the operation .)
(V4) For all x, y > 0, x  y = xy = yx = y  x.
(V5) For all x > 0, 1 · x = x1 = x.
(V6) For all x > 0 and all 
.
(V7) For all x > 0 and all 
.
(V8) For all x, y > 0, 
.
We remark that V is isomorphic to the one dimensional vector space R (with the
usual operations): indeed, it can be checked that the maps log : V → R and exp :
R → V are linear transformations, and are inverses of each other.
Solution to Exercise 1.2, page 7
We prove this by contradiction. Suppose that C[0, 1] has dimension d. Consider
functions xn(t) = tn, t ∈ [0, 1], n = 1, ··· , d. Since polynomials are continuous,
we have xn ∈ C[0, 1] for all n = 1, ··· , d.
First we prove that xn, n = 1, ··· , d, are linearly independent in C[0, 1].
Suppose not. Then there exist αn ∈ R, n = 1, ··· , d, not all zeros, such that α1 ·
x1 + ··· + αd · xd = 0. Let m ∈ {1, ··· , d} be the smallest index such that αm ≠ 0.

Then for all t ∈ [0, 1], αmtm + ··· + αdtd = 0. In particular, for all t ∈ [0, 1], we
have 
.
Thus for all n ∈ N we have 
.
Passing the limit as n → 8, we obtain αm = 0, a contradiction. So the functions
xn, n = 1, ··· , d, are linearly independent in C[0, 1].
Next, we get the contradiction to C[0, 1] having dimension d. Since any
independent set of cardinality d in a d-dimensional vector space is a basis for this
vector space, {xn : n = 1, ··· , d} is a basis for C[0, 1]. Since the constant
function 1 (taking value 1 everywhere on [0, 1]) belongs to C[0, 1], there exist βn
∈ R, n = 1, ··· , d, such that 1 = β1 · x1 + ··· + βd · xd. In particular, putting t = 0,
we obtain the contradiction that 1 = 0: 1 = 1(0) = (β1 · x1 + ··· + βd · xd)(0) = 0.
Solution to Exercise 1.3, page 7
(“If ” part.) Suppose that ya = yb = 0. Then we have:
(S1) If x1, x2 ∈ S, then x1 + x2 ∈ S. As x1, x2 ∈ C1[a, b], also x1 + x2 ∈ C1[a,
b]. Moreover, x1(a) + x2(a) = 0 + 0 = 0 = ya and x1(b) + x2(b) = 0 + 0 = 0 =
yb.
(S2) If x ∈ S and α ∈ R, then α · x ∈ S. Indeed, as x ∈ C1[a, b], and α ∈ R,
we have α·x ∈ C1[a, b], and (α·x)(a) = α0 = 0 = ya, (α·x)(b) = α0 = 0 = yb.
(S3) 0 ∈ S, since 0 ∈ C1[a, b] and 0(a) = 0 = ya = yb = 0(b).
Hence, S is a subspace of a vector space C1[a, b]
(“Only if ” part.) Suppose that S is a subspace of C1[a, b]. Let x ∈ S.
Then 2 · x ∈ S. Therefore, (2 · x)(a) = ya, and so ya = (2 · x)(a) = 2x(a) = 2ya.
Thus ya = 0. Moreover, (2 · x)(b) = yb, and so yb = (2 · x)(b) = 2x(b) = 2yb.
Hence also yb = 0.
Solution to Exercise 1.4, page 10
Solution to Exercise 1.5, page 14

From the triangle inequality, we have that ||x|| = ||y + x − y||  ||y|| + ||x − y||, for
all x, y ∈ X. So for all x, y ∈ X, ||x|| − ||y||  ||x − y||.
Interchanging x, y, we get 
.
So for all x, y ∈ X, − (||x|| − ||y||)  ||x − y||.
Combining the results from the first two paragraphs, we obtain |||x|| − ||y|||  ||x −
y|| for all x, y ∈ X.
Solution to Exercise 1.6, page 14
No, since for example (N2) fails if we take x = 1 and α = 2:
Solution to Exercise 1.7, page 15
We verify that (N1), (N2), (N3) are satisfied by || · ||Y:
(N1) For all y ∈ Y, ||y||Y = ||y||X  0.
If y ∈ Y and ||y||Y = 0, then ||y||X = 0, and so y = 0 ∈ X.
But 0 ∈ Y, and so y = 0 ∈ Y.
(N2) If y ∈ Y and α ∈ R, then α · y ∈ Y and 
.
(N3) If y1, y2 ∈ Y, then y1 + y2 ∈ Y.
Also, 
.
Solution to Exercise 1.8, page 15
(1) We first consider the case 1  p < ∞, and then p = ∞. Let 1  p < ∞.
(N1) If x = (x1, ··· , xd) ∈ Rd then 
.
If x ∈ Rd and ||x||p = 0, then ||x||pp = 0. that is, 
.
So |xn| = 0 for 1  n  d, that is, x = 0.
(N2) Let x = (x1 , ··· , xd) ∈ Rd, and α ∈ R.
Then 
.
(N3) Let x = (x1, ··· , xd) ∈ Rd and y = (y1 , ··· , yd) ∈ Rd.
If p = 1, then we have |xn + yn|  |xn| + |yn| for 1  n  d.
By adding these, ||x + y||1  ||x||1 + ||y||1, establishing (N3) for p = 1.
Now consider the case 1 < p < ∞.
If x + y = 0, then ||x + y||p = ||0||p = 0  ||x||p + ||y||p trivially.
So we assume that x + y ≠ 0. By Hölder’s Inequality, we have

where we used q(p − 1) = p in order to obtain the last equality.
Similarly, 
. Consequently,
Dividing throughout by 
, we obtain 
. This
completes the proof that (Rd, || · ||p) is a normed space for 1  p < ∞.
Now we consider the case p = ∞.
(N1) If x = (x1 , ··· , xd) ∈ Rd, then ||x||∞ = max{|x|, ··· , |xd|}  0.
If x ∈ Rd and ||x||∞ = 0, then max{|x1|, ··· , |xd|} = 0, and so |xn| = 0 for 1 
n  d, thart is, x = 0.
(N2) Let x = (x1 , ··· , xd) ∈ Rd, and α ∈ R.
Then 
.
(N3) Let x = (x1, ··· , xd) ∈ Rd and y = (y1, ··· , yd) ∈ Rd.
We have 
 for 1  n  d.
So it follows that ||x + y||∞  ||x||∞ + ||y||∞, establishing (N3) for p = ∞.
(2) See the following pictures.
(3) We have for x = (a, b) ∈ R2 that

So 
.
We have 
. We have
giving 1/p  hp  0 for all p, and so hp → 0 as p → ∞.) So it follows by the
Sandwich Theorem1 that 
.
The balls Bp(0, 1) grow to B∞(0, 1) as p increases.
Solution to Exercise 1.9, page 16
(1) If x, y ∈ B(0, 1), then for all α ∈ (0, 1), (1 − α) · x + α · y ∈ B(0, 1) too,
since
(2) See the following picture.
(3) B(0, 1) is not convex: taking x = (1, 0), y = (0, 1) and α = 1/2, we obtain 
, and so 
.
Solution to Exercise 1.10, page 16
We’ll verify that (N1), (N2), (N3) hold.
(N1) If x ∈ C[a, b], then |x(t)|  0 for all t ∈ [0, 1], and so 
.
Let x ∈ C[a, b] be such that ||x||1 = 0. If x(t) = 0 for all t ∈ (a, b), then by
the continuity of x on [a, b], it follows that x(t) = 0 for all t ∈ [a, b] too,

and we are done! So suppose that it is not the case that for all t ∈ (a, b),
x(t) = 0. Then there exists a t0 ∈ (a, b) such that x(t0) ≠ 0. As x is
continuous at t0, there exists a δ > 0 small enough so that a < t0 − δ, t0 + δ
< b, and such that for all t ∈ [a, b] such that t0 − δ < t < t0 + δ, |x(t) − x(t0)|
< |x(t0)|/2. Then for t0 − δ < t < t0 + δ, we have, using the “reverse”
Triangle Inequality from Exercise 1.5, page 14, that
So 
.
This is a contradiction. Hence x = 0.
(N2) For x ∈ C[a, b], α ∈ R, 
.
(N3) Let x, y ∈ C[a, b]. Then
Solution to Exercise 1.11, page 17
(N1) For x ∈ Cn[a, b], clearly 
.
If x ∈ Cn[a, b] is such that ||x||n,∞ = 0, then ||x||∞ + ··· + ||x(n)||∞ = 0, and
since each term in this sum is nonnegative, we have ||x||∞ = 0, and so x = 0.
(N2) Let x ∈ Cn[a, b] and α ∈ R. Then
(N3) Let x, y ∈ Cn[a, b]. For all 0  k  n, ||x(k) + y(k)||∞  ||x(k)||∞ + ||y(k)||∞, by
the Triangle Inequality for || · ||∞ Consequently,

Solution to Exercise 1.12, page 17
(1) Let k1, k2, m1, m2, n1, n2 ∈ Z, p ł m1, m2, n1, n2 and 
.
If k1 > k2, then pk1−k2m1n2 = m2n1, which implies that p | m2n1, and as p is
prime, this would mean p | m1 or p | n1, a contradiction. Hence k1  k2.
Similarly, we also obtain k2  k1.
Thus k1 = k2. Consequently, 
, and so | · |p is well-defined.
(2) If 0 ≠ r ∈ Q, then we can express r as 
, with k, m, n ∈ Z, and p  m, n.
We see that 
. If r = 0, then |r|p = |0|p = 0 by definition.
Thus |r|p  0 for all r ∈ R. Also if r ≠ 0, then |r|p > 0. Hence |r|p = 0 implies
that r = 0.
(3) The claim is obvious if r1 = 0 or r2 = 0. Suppose that r1 ≠ 0 and r2 ≠ 0.
Let 
 and 
.
So 
. As p  m1, p  m2, and p is prime, we have p  m1m2.
Similarly p  n1n2. Thus 
.
(4) The inequality is trivially true if r1 = 0 or r2 = 0 or if r1 + r2 = 0.
Assume r1 ≠ 0, r2 ≠ 0, and r1 + r2 ≠ 0.
Let 
, with k1, k2, m1, m2, n1, n2 ∈ Z, p  m1, m2, n1, n2. We
have
where  := pk1−min{k1,k2} m1n2 + pk2−min{k1,k2} n1m2 (≠ 0, since r1 + r2 ≠ 0). By
the Fundamental Theorem of Arithmetic, there exists a unique integer   0
and an integer m such that 
 and p  m. Clearly p  n1n2.
Hence r1 + r2 = 
, with p  m, n1n2.
So 
This yields the Triangle Inequality:

Solution to Exercise 1.13, page 17
(N1) Clearly 
 for all M = [mij] ∈ Rm×n.
If ||M||∞ = 0, then |mij| = 0 for all 1  i  m, 1  j  n, that is, M = [mij] = 0,
the zero matrix.
(N2) For M = [mij] ∈ Rm×n and α ∈ R, we have
(N3) For P = [pij], Q = [qij] ∈ Rm×n, |pij + qij|  |pij| + |qij|  ||P||∞ + ||Q||∞.
As this holds for all i, j, ||P + Q||∞ = 
.
Solution to Exercise 1.14, page 19
Consider the open ball B(x, r) = {y ∈ X : ||x − y|| < r} in X. If y ∈ B(x, r), then ||x
− y|| < r. Define r′ = r − ||x − y|| > 0. We claim that B(y, r′) ⊂ B(x, r). Let z ∈ B(y,
r′). Then ||z − y|| < r′ = r − ||x − y|| and so ||x − z||  ||x − y|| + ||y − z|| < r. Hence z
∈ B(x, r). The following picture illustrates this.
Solution to Exercise 1.15, page 19
The point 
, but for each r > 0, the point 
 belongs to the ball
B(c, r), but not to I, since ||y − c||2 = 
, but  ≠ 0. See the
following picture.

Solution to Exercise 1.16, page 19
Using the following picture, it can be seen that the collections O1, O2, O∞ of
open sets in the normed spaces (R2, || · ||1), (R2, || · ||2), (R2, || · ||∞), respectively,
coincide.
Solution to Exercise 1.17, page 20
If Fi, i ∈ I, is a family of closed sets, then X\Fi, i ∈ I, is a family of open sets.
Hence 
 is open. So 
 is closed.
If F1, ··· , Fn are closed, then X\F1, ··· , X\Fn are open, and so the intersection 
 of these finitely many open sets is open as well.
Thus 
 is closed.
For showing that the finiteness condition cannot be dropped, we’ll consider the
normed space X = R, and simply rework Example 1.15, page 20, by taking
complements.
We know that Fn := R\(−1/n, 1/n), n ∈ N, is closed and the union of these, 
 which is not closed, since if it
were, its complement R\(R\{0}) = {0} would be open, which is false.
Solution to Exercise 1.18, page 20
Consider the closed ball B(x, r) = {y ∈ X : ||x − y||  r} in X. To show that B(x, r)
is closed, we’ll show its complement, U := {y ∈ X : ||x − y|| > r}, is open. If y ∈
U, then ||x − y|| > r. Define r′ = ||x − y|| − r > 0. We claim that B(y, r′) ⊂ U. Let z
∈ B(y, r′). Then ||z − y|| < r′ = ||x − y|| − r and so ||x − z||  ||x − y|| − ||y − z|| > ||x −
y|| − (||x − y||− r) = r. Hence z ∈ U.
Solution to Exercise 1.19, page 20
(1) False.
For example, in the normed space R, consider the set [0, 1). Then [0, 1) is

not open, since every open ball B with centre 0 contains at least one negative
real number, and so B has points not belonging to [0, 1).
On the other hand, this set [0, 1) is not closed either, as its complement is
C := (−∞, 0) ∪ [1, ∞), which is not open, since every open ball B′ with
centre 1 contains at least one positive real number strictly less than one, and
so B′; contains points that do not belong to C.
(2) False. R is open in R, and it is also closed.
(3) True. Ø and X are both open and closed in any normed space X.
(4) True. [0, 1) is neither open nor closed in R.
(5) False.
0 ∈ Q, but every open ball centred at 0 contains irrational numbers; just
consider 
/n, with a sufficiently large n.
(6) False.
Consider the sequence (an)n∈N given by a1= , and for n > 1, an+1 = 
.
Then it can be shown, using induction on n, that (an)n∈N is bounded below
by 
, and that (an)n∈N is monotone decreasing. (Example 1.19, page 31.) So
(an)n∈N is convergent with a limit L satisfying 
, and so L2 = 2.
As L must be positive (the sequence is bounded below by 
), it follows that
L = 
. So every ball with centre 
 and a positive radius contains elements
from Q (terms an for large n), showing that R\Q is not open, and hence Q is
not closed.
(Alternately, let c ∈ R have the decimal expansion c = 0.101001000100001
···. The number c is irrational because2 it has a nonterminating and
nonrepeating decimal expansion. The sequence of rational numbers obtained
by 
truncation, 
namely 
0.1, 
0.101, 
0.101001, 
0.1010010001,
0.101001000100001, ··· converges with limit c, and so every ball with
centre c and a positive radius contains elements from Q, showing again that
R \ Q is not open, and hence Q is not closed.)
(7) True. 
 As each (n, n + 1) is open, so is their union. 
Hence Z = R\(R\Z) is closed.
Solution to Exercise 1.20, page 21
We have already seen in Exercise 1.14, page 19, that the interior of S, namely the

open ball B(0, 1) = {x ∈ X : ||x|| < 1} is open. Also, it follows from Exercise
1.18, page 20, that the exterior of the closed ball B(0, 1), namely the set U = {x
∈ X : ||x|| > 1} is open as well. Thus, the complement of S, being the union of the
two open sets B(0, 1) and U, is open. Consequently, S is closed.
Solution to Exercise 1.21, page 21
If X = {0}, then {0} is clearly closed, since X\{0} = Ø is open.
Now suppose that X ≠ {0}, and let x ∈ X. We want to show that U := X\{x} is
open. Let y ∈ U := X\{x}, and set r := ||x – y|| > 0. We claim that the open ball
B(y, r) is contained in U. If z ∈ B(y, r), then ||y – z|| < r, and so ||z – x||  ||x – y|| –
||y – z||  r – ||y – z|| > r – r = 0. Hence z ≠ x, and so z ∈ X\{x} = U. Consequently
U is open, and so {x} = X\U is closed.
If F is empty, then it is closed.
If F is not empty, then F = {x1, ··· , xn} = 
{xi}, for some x1, ···, xn ∈ X.
As F is the finite union of the closed sets {x1}, ···, {xn}, F is closed too.
Solution to Exercise 1.22, page 21
Let x, y ∈ R and x < y. By the Archimedean property of R, there is a positive
integer n such that n > 1/(y – x), that is n(y – x) > 1. Also, there are positive
integers m1, m2 such that m1 > nx and m2 > –nx, so that –m2 < nx < m1. Thus we
have nx ∈ [–m2, –m2 + 1) ∪ [–m2 + 1, –m2 + 2)∪···∪[m1 – 1, m1). Hence there
is an integer m such that m – 1  nx < m. We have nx < m  1 + nx < ny, and so
dividing by n, we have x < q := m/n < y. Consequently, between any two real
numbers, there is a rational number.
Let x ∈ R and let  > 0. Then there is a rational number y such that x –  < y <
x + , that is, |x – y| < . Hence Q is dense in R.
Solution to Exercise 1.23, page 21
Let x ∈ R and let  > 0. If x ∈ R\Q, then taking y = x, we have |x – y| = 0 < . If
on the other hand, x ∈ Q, then let n ∈ N be such that n > 
/  so that with y := x
+ 
/n, we have y ∈ R\Q, and |x – y| = 
/n < . So R\Q is dense in R.

Solution to Exercise 1.24, page 21
Let x = (xn)n∈N ∈ ℓ2, and  > 0. Let N ∈ N be such that 
Then y := (x1, ···, xN, 0, ···) ∈ c00, and 
Thus ||x – y||2 < . Consequently, c00 is dense in ℓ2.
Solution to Exercise 1.25, page 21
Consider the set D of all finitely supported sequences with rational terms. Then
D is a countable set since it is a countable union of countable sets. We now show
that D is dense in ℓ1. Let x := (xn)n∈N ∈ ℓ1 and let r > 0.
Let N ∈ N be large enough so that 
As Q is dense in R, there exist q1, ···, qN ∈ Q such that 
With x′ := (q1, ···, qN, 0, ···) ∈ D, 
Solution to Exercise 1.26, page 22
By the Binomial Theorem, we have
Putting s = t, we get 1 = (t + (1 – t))n 
Keeping s fixed, and differentiating (7.1) with respect to t yields
Multiplying throughout by t gives
With 

Differentiating (7.2) with respect to t yields
Multiplying throughout by t yields
Setting s = t now gives
Hence
Solution to Exercise 1.27, page 33
(1) We check that the relation ~ is reflexive, symmetric and transitive.
(ER1) (Reflexivity) If ||·|| is a norm on X, then for all x ∈ X, we have that 1 ·
||x|| = ||x|| = 1 · ||x||, and so ||·|| ~ ||·||.
(ER2) (Symmetry) If ||·||a ~ ||·||b, then there exist positive m, M such that for
all x ∈ X, m||x||b  ||x||a  M ||x||b. A rearrangement of this gives
(1/M)||x||a  ||x||b  (1/m)||x||a, x ∈ X, and so ||·||2 ~ ||·||1.
(ER3) (Transitivity) If ||·||a ~ ||·||b and ||·||b ~ ||·||c, then there exist positive
constants Mab, Mbc, mab, mbc such that for all x ∈ X, we have that
mab||x||b  ||x||a  Mab||x||b and mbc ||x||c  ||x||b  Mbc ||x||c.
Thus mabmbc||x||c  mab||x||b  ||x||a  Mab||x||b  MabMbc ||x||c, and so ||
·||a ~ ||·||c.
(2) Suppose that ||·||a ~ ||·||b. Because ~ is an equivalence relation, it is enough to
just prove that if U is open in (X, ||·||b), then U is open in (X, ||·||a) too, and

similarly, if (xn)n∈N is Cauchy (respectively) convergent in (X, ||·||b), then it
is Cauchy (respectively convergent) in (X, ||·||a) as well. Let m, M > 0 be
such that for all x ∈ X, m||x||b  ||x||a  M||x||b.
Let U be open in (X, ||·||b), and x ∈ U. Then as U is open in (X, ||·||b), there
exists an r > 0 such that Bb(x, r) := {y ∈ X : ||y – x||b < r} ⊂ U. But if y ∈ X
satisfies ||y – x||a < mr, then ||y – x||b  (1/m)||y – x||a < (1/m)mr = r, and so y
∈ Bb(x, r) ⊂ U. Hence Ba(x, mr) := {y ∈ X : ||y – x||a < mr} ⊂ U. So it
follows that U is open in (X, ||·||a) too.
Now suppose that (xn)n∈N is a Cauchy sequence in (X, ||·||b). Let  > 0. Then
there exists an N ∈ N such that for all n > N, ||xn – xm||b < /M. Hence for all
n > N, ||xn – xm||a  M ||xn – xm||b < M · ( /M) = .
Consequently, (xn)n∈N is a Cauchy sequence in (X, ||·||a) as well.
If (xn)n∈N is a convergent sequence in (X, ||·||b) with limit L, then for  > 0,
there exists an N ∈ N such that for all n > N, ||xn – L||b < /M. Thus for all n >
N, ||xn – L||a  M ||xn – L||b < M · ( /M) = . So (xn)n∈N is convergent with
limit L in (X, ||·||a) too.
Solution to Exercise 1.28, page 42
(1) Let L > 0 be such that for all x, y ∈ R, |f(x) – f(y)| = 
Then in particular, with 
 n ∈ N, and y = 0, we obtain 
Thus n  L for all n ∈ N, which is absurd. So f is not Lipschitz.
(2) x1(0) = 0 and x2(0) = 02/4 = 0, and so x1, x2 satisfy the initial condition.
For all t  0, 
So x1, x2 are both solutions to the given Initial Value Problem.
Solution to Exercise 1.29, page 43
Let F be closed, and (xn)n∈N be a sequence in F which converges to x. Suppose
that x ∉ F. Since F is closed, there is an open ball B(x, r) := {x ∈ X : ||x – x|| < r}
with r > 0, which is contained in X\F. But with  := r > 0, there exists an N ∈ N
such that for all n > N, ||xn – x|| < r. In particular, ||xN+1 – x|| < r, so that F ∋ xN+1

∈ B(x, r) ⊂ X\F, a contradiction. Hence x ∈ F.
Now suppose that for every sequence (xn)n∈N in F, convergent in X with a limit x
∈ X, we have that the limit x ∈ F. We want to show that X\F is open. Suppose it
isn’t. Then3 ¬[∀x ∈ X\F, ∃r > 0 such that B(x, r) ⊂ X\F]. In other words, ∃x ∈
X\F such that ∀r > 0, B(x, r) ∩ F ≠ Ø. So with r = 1/n, n ∈ N, we can find an xn
∈ B(x, r) ∩ F. Then we obtain a sequence (xn)n∈N in F satisfying ||xn – x|| < 1/n
for all n ∈ N. Thus (xn)n∈N converges to x. But x ∉ F, contradicting the
hypothesis. Hence X\F is open, that is, F is closed.
Solution to Exercise 1.30, page 43
Let (xn)n∈N in c00 be given by 
 n ∈ N.
Then with 
 we have
showing that c00 is not closed.
Solution to Exercise 1.31, page 43
(1) Suppose that F is a closed set containing S. Let L be a limit point of S.
Then there exists a sequence (xn)n∈N in S\{L} which converges to L.
As each xn ∈ S\{L} ⊂ S ⊂ F, and since F is closed, it follows that L ∈ F.
So all the limit points of S belong to F. Hence S ⊂ F.
S is closed. Suppose that (xn)n∈N is a sequence in S that converges to L.
We would like to prove that L ∈ S. If L ∈ S, then L ∈ S, and we are done.
So suppose that L ∉ S. Now for each n, we define the new term x′n as
follows:
1° If xn ∈ S, then x′n := xn.
2° If xn ∉ S, then xn must be a limit point of S, and so B(xn, 1/n) must
contain some element, say x′n, of S.

Hence we have 
Thus (x′n)n∈N is a sequence in S\{L} which converges to L, and so L is a limit
point of S, that is, L ∈ S. Consequently S is closed.
(2) We first note that if y ∈ Y, then there exists a (yn)n∈N in Y that converges to
y. Indeed, this is obvious if y is a limit point of Y, and if y ∈ Y, then we may
just take (yn)n∈N as the constant sequence with all terms equal to y. We have:
(S1) Let x, y ∈ Y. Let (xn)n∈N, (yn)n∈N be sequences in Y that converge to x,
y, respectively. Then xn + yn ∈ Y ⊂ Y for each n ∈ N, and (xn + yn)n∈N
converges to x + y. But as Y is closed, it follows that x + y ∈ Y too.
(S2) Let α ∈ K, y ∈ Y. Let (yn)n∈N be a sequence in Y that converges to y.
Then α · yn ∈ Y ⊂ Y for each n ∈ N, and (α · yn)n∈N converges to α · y.
But as Y is closed, it follows that α · y ∈ Y too.
(S3) 0 ∈ Y ⊂ Y.
Hence Y is a closed subspace.
(3) The proof is similar to part (2). Let x, y ∈ C. Then there exist sequences
(xn)n∈N and (yn)n∈N in C that converge to x, y, respectively. If α ∈ (0, 1),
then (1 – α)x + αy = (1 – α) 
 xn + α 
 yn = 
 ((1 – α)xn + αyn).
As (1 – α)xn + αyn ∈ C ⊂ C for all n ∈ N, and since C is closed, it follows
that (1 – α)x + αy ∈ C too.
(4) Suppose that D is dense in X. Let x ∈ X\D. If n ∈ N, then the ball B(x, 1/n)
must contain an element dn ∈ D. The sequence (dn)n∈N converges to x
because ||x – dn|| < 1/n, n ∈ N. Hence x is a limit point of D, that is, x ∈ D.
So X\D ⊂ D. Also D ⊂ D. Thus X = D ∪ (X\D) ⊂ D ⊂ X, and so X = D.
Now suppose that X = D. If x ∈ X\D = D\D, then x is a limit point of D, and
so there is a sequence (dn)n∈N in D that converges to x. Thus given an  > 0,
there is an N such that ||x – dN|| < , that is, dN ∈ D ∩ B(x, ).
On the other hand, if x ∈ D, and  > 0, then x ∈ B(x, ) ∩ D.
Hence D is dense in X.
Solution to Exercise 1.32, page 43
Let (xn)n∈N) ℓ1. Then 
 and so 
 |xn| = 0.

Thus there exists an N ∈ N such that |xn|  1 for all n  N. For all n  N, |xn|2 =
|xn| · |xn|  |xn| · 1 = |xn|. By the Comparison Test4, 
Hence (xn)n∈N ∈ ℓ2.
 while the Harmonic Series 
 diverges.
(ℓ1, ||·||2) is not a Banach space: Let us suppose, on the contrary, that it is a
Banach space, and we will arrive at a contradiction by showing a Cauchy
sequence which is not convergent in (ℓ1, ||·||2).
Consider for n ∈ N, 
 Then (xn)n∈N converges in
ℓ2 to 
 because 
So (xn)N is a Cauchy sequence in (ℓ2, ||·||2), and so it is also Cauchy in (ℓ1, ||·||2).
As we have assumed that (ℓ1, ||·||2) is a Banach space, it follows that the Cauchy
sequence (xn)n∈N must be convergent to some element x′ ∈ ℓ1 ⊂ ℓ2. But by the
uniqueness of limits (when we consider (xn)n∈N as a sequence in ℓ2), we must
have x = x′ ∈ ℓ1, which is false, since we know that the Harmonic Series
diverges. This contradiction proves that (ℓ1, ||·||2) is not a Banach space.
Solution to Exercise 1.33, page 43
Let (an)n∈N be a Cauchy sequence in c0. Then this is also a Cauchy sequence in
ℓ∞, and hence convergent to a sequence in ℓ∞, say a. We’ll show that a ∈ c0. We
write 
 and 
 Let  > 0. Then there exists an N ∈ N such
that ||aN – a||∞ < . In particular, for all m ∈ N, 
 < . But as aN ∈ c0, we
can find an M such that for all m > M, 
 Consequently, for m > M, we
have from the above that 
 Thus a ∈ c0 too.
Solution to Exercise 1.34, page 44
Given  > 0, let N ∈ N be large enough so that for all n > N, ||xn – x|| < . Then
for all n > N, we have ||xn|| – ||x||  ||xn – x|| < , and so it follows that the
sequence (||xn||)n∈N is R is convergent, with limit ||x||.

Solution to Exercise 1.35, page 44
First consider the case 1  p < ∞.
(N1)
 for all x = (x1, x2, x3, ···) ∈ ℓp.
If 
 then |xn| = 0 for all n, and so x = 0.
(N2) ||α · x||p = 
 = |α| ||x||p, for x ∈ ℓp, α ∈ K.
(N3) Let x = (x1, x2, ···) and y = (y1, y2, ···) belong to ℓp. Let d ∈ N.
By the Triangle Inequality for the ||·||p-norm on Rd,
Passing the limit as d tends to ∞ yields ||x + y||p  ||x||p + ||y||p.
Now consider the case p = ∞.
(N1)
 for all x = (x1, x2, x3, ···) ∈ ℓ∞.
If 
 then |xn| = 0 for all n, that is, x = 0.
(N2)
 for x ∈ ℓ∞, α ∈ K.
(N3) Let x = (x1, x2, ···) and y = (y1, y2, ···) belong to ℓ∞.
Then for all k, |xk + yk|  |xk| + |yk|  ||x||∞ + ||y||ℓ, and so
Solution to Exercise 1.36, page 44
From Exercise 1.11, page 17, taking n = 1, (C1[a, b], ||·||1,∞) is a normed space.
We show that (C1[a, b], ||·||1,∞) is complete. Let (xn)n∈N be a Cauchy sequence in
C1[a, b]. Then ||xn – xm||∞  ||xn – xm||∞ + ||x′n – x′m||1,∞, and so (xn)n∈N is a Cauchy
sequence in (C[a, b], ||·||∞), and hence convergent to, say, x ∈ C[a, b]. Also, ||x′n –
x′m||∞  ||xn – xm||∞ + ||x′n – x′m||1,∞ = ||xn – xm||1,∞, shows that (x′n)n∈N is a Cauchy
sequence in (C[a, b], ||·||∞), and hence convergent to say, y ∈ C[a, b]. We will
now show that x ∈ C1[a, b], and x′ = y. Let t ∈ [a, b]. By the Fundamental
Theorem of Calculus, 
 and so

Passing the limit as n goes to ∞ gives, for all t ∈ [a, b], 
By the Fundamental Theorem of Calculus, x′ = y ∈ C[a, b]. So x ∈ C1[a, b].
Finally, we’ll show that (xn)n∈N converges to x in C1[a, b]. Let  > 0, and let N
be such that for all m, n > N, ||xn – xm||1,∞ < . Then for all t ∈ [a, b], we have
|xn(t) – xm(t)| + |x′n(t) – x′m(t)|  |xn – xm|∞ + |x′n – x′m|∞ = ||xn – xm||1,∞ < . Letting m
go to ∞, it follows that for all n > N, |xn(t) – x(t)| + |x′n(t) – x′(t)   As the choice
of t ∈ [a, b] was arbitrary, it follows that
that is, ||xn – xm||1,∞  2 .
Solution to Exercise 1.37, page 44
Let (xn)n∈N be any Cauchy sequence in X. We construct a subsequence (xnk)k∈N
inductively, possessing the property that if n > nk, then ||xn – xnk|| < 1/2k, k ∈ N.
large enough so that if n, m  n1, then ||xn – xm|| < 1/2. Suppose xn1, ···, xnk have
been constructed. Choose nk+1 > nk such that if n, m  nk+1, then ||xn – xm|| <
1/2k+1. In particular for n  nk+1, ||xn – xnk+1|| Z 1/2k+1.
Now define u1 = xn1, uk+1 = xnk+1 – xnk, k ∈ N.
We have 
 Thus 
 converges.
But the partial sums of 
 are 
So (xnk)k∈N converges in X, to, say x ∈ X. As (xnk)k∈N is a convergent
subsequence of the Cauchy sequence (xn)n∈N, it now follows that (xn)n∈N is itself
convergent with the same limit x. Indeed, given  > 0, first let N be such that for
all n, m > N, ||xn – xm|| < /2, and next let nK > N be such that ||xnK – x|| < /2,
which yields that for all n > N,

Solution to Exercise 1.38, page 44
(N1) For (x, y) ∈ X × Y, ||(x, y)|| = max{||x||, ||y||}  0. 
If ||(x, y)|| = 0, then 0  ||x||  max{||x||, ||y||} = {||x, y)|| = 0, and so ||x|| = 0,
giving x = 0. Similarly, y = 0 too, and so (x, y) = 0X×Y.
(N2) For α ∈ K, and (x, y) ∈ X × Y,
(N3) Let (x1, y1), (x2, y2) ∈ X × Y. Then
and so max{||x1 + x2||, ||y1 + y2||}  ||(x1, y1)|| + {||x2, y2)||. Thus
Hence (x, y)  max{||x||, ||y||}, (x, y) ∈ X × Y, defines a norm on X × Y.
Let ((xn, yn))n∈N be Cauchy in X × Y. As ||x||  max{||x||, ||y||} = ||(x, y)||, (xn)n∈N
is Cauchy in X. As X is Banach, (xn)n∈N converges to some x ∈ X. Similarly
(yn)n∈N converges to a y ∈ Y. Let  > 0. Then there exists an Nx such that for all n
> Nx, ||xn – x|| < , and there is an Ny such that for all n > Ny, ||yn – y|| < . So with
N := max{Nx, Ny}, for all n > N, we have ||xn – x|| <  and ||yn – y|| < . Thus ||(xn,
yn) – (x, y)|| = ||(xn – x, yn – y)|| = max{||xn – x||, ||yn – y||} < , showing that ((xn,
yn))n∈N converges to (x, y) in X × Y. So X × Y is Banach.
Solution to Exercise 1.39, page 50
Since K is compact in Rd, it is closed and bounded. Let R > 0 be such that for all
x ∈ K, ||x||2  R. In particular, for every x ∈ K ∩ F, we have ||x||2  R. Thus K ∩
F is bounded. Also, since both K and F are closed, it follows that even K ∩ F is

closed. Hence K ∩ F is closed and bounded, and so by Theorem 1.10, page 45,
we conclude that K∩F is compact.
Solution to Exercise 1.40, page 50
Clearly Sd–1 is bounded. It is also closed, and we prove this below. Let (xn)n∈N
be a sequence in Sd–1 which converges to L in Rd. Let L = (L1, ···, Ld) and 
 for n ∈ N. Then 
 xn(k) = Lk (k = 1, ..., d).
Since xn ∈ Sd–1 for each n ∈ N, we have 
 Passing
the limit as n → ∞, we obtain 
 Hence L ∈ Sd–1. So Sd–1 is
closed. As Sd–1 is closed and bounded, it follows from Theorem 1.10, page 45,
that it is compact.
Solution to Exercise 1.41, page 50
(1) Let (Rn)n∈N be a sequence in O(2). 
Using 
 then 
 and 
So each of the sequences (an)n∈R, (bn)n∈R, (cn)n∈R, (dn)n∈R is bounded.
By successively refining subsequences of these sequences, we can choose a
sequence of indices n1 < n2 < n3 <···, such that the sequences (ank)k∈N,
(bnk)k∈N, (cnk)k∈N, (dnk)k∈N are convergent, to, say, a, b, c, d, respectively.
Hence (Rnk)k∈N is convergent with the limit 
From (Rn)  Rn = I (n ∈ N), it follows that also R R = I, that is, R ∈ O(2).
(2) The hyperbolic rotations 
 belong to O(1, 1) because
But ||R(t)||∞  | cosh(t)| = cosh t → ∞ as t → ∞, showing that O(1, 1) is not

bounded. Hence O(1, 1) can’t be compact (as every compact set is
necessarily bounded).
Solution to Exercise 1.42, page 51
Let 
 Since K ⊂ [0, 1], clearly K is bounded.
Moreover, 
Thus R\K, being the union of open intervals, is open, that is, K is closed. Since K
is closed and bounded, it is compact.
Solutions to the exercises from Chapter 2
Solution to Exercise 2.1, page 58
If 1 ∈ C[0, 1] denotes the constant function taking value 1 everywhere, then
and so 
So (L2) is violated, showing that S1 is not a linear transformation.
On the other hand, S2 is a linear transformation. For all x1, x2 ∈ C[0, 1],
and so (L1) holds. Moreover, for all α ∈ R and x ∈ C[0, 1] we have
and so (L2) holds as well.
Solution to Exercise 2.2, page 58
(1) Let α1, α2 ∈ R be such that α1f1 + α2f2 = 0, that is,

In particular, with t = 0, we obtain α1 = 0. Thus α2eat sin(bt) = 0 for all t ∈ R.
With t = π/2b, we see that 
 and so α2 = 0. Consequently, f1, f2
are linearly independent.
(2) First of all, D is a well-defined map from Sf1,f2 to itself, since
Thus DSf1,f2 ⊂ Sf1,f2.
Furthermore, it is clear that D(g1 + g2) = D(g1) + D(g2) for all g1, g2 ∈ C1(R)
(and in particular for g1, g2 ∈ Sf1,f2 ⊂ C1(R)), and also D(α · g) = α · D(g)
and all g ∈ (R) (and in particular, for all g ∈ Sf1,f2).
Hence D is a linear transformation from Sf1,f2 to itself.
(3) We have Df1 = aeat cos(bt) – eatb sin(bt) = af1 – bf2, and 
Df2 = aeat sin(bt) + eatb cos(bt) = bf1 + af2.
So the matrix of D with respect to the basis B = (f1, f2) is 
(4) As det[D]B = a2 + b2 ≠ 0, [D]B is invertible, and 
 
Hence D is invertible, and the inverse D–1 : Sf1,f2 → Sf1,f2 has the matrix [D–
1]B (with respect to B) given by [D–1]B = [D]–1]B found above.
(5) We note that 
 and so
By the definition of D, 
So 
 any constant.
Similarly, as 
 we have

and so 
So 
 any constant.
Solution to Exercise 2.3, page 61
(1) We have 
(As expected, the arc length is simply the length of the line segment [0, 1].)
(2) We have 
 and so
(3) Suppose that f is continuous at 0. Then with  := 1 > 0, there exists a δ > 0
such that whenever x ∈ C1[0, 1] and ||x – 0|| < δ, we have |f(x) – f(0)| < 1.
We have 
 for all 
Hence for such n there must hold that |f(xn) – f(0)| = |f(xn) – 1| < 1.
So for all 
 we have 
  |f(xn)|  |f(xn) – 1| + 1 < 1 + 1 = 2,
which is a contradiction. Hence f is not continuous at 0.
Let x0, x ∈ C1[a, b]. Using the triangle inequality in (R2, ||·||2), we obtain
and so

Thus given  > 0, if we set δ := , then we have for all x ∈ C1[0, 1] satisfying ||x
– x0||1,∞ < δ that |f(x) – f(x0)|  ||x – x0||1,∞ < δ = .
So f is continuous at x0. As the choice of x0 was arbitrary, f is continuous.
Solution to Exercise 2.4, page 62
Let x0 ∈ X. Given  > 0, set δ := . Then for all x ∈ X satisfying ||x – x0|| < δ, we
have | ||x|| – ||x0|| |  ||x – x0|| < δ = . Thus ||·|| is continuous at x0. As x0 ∈ X was
arbitrary, it follows that ||·|| is continuous on X.
Solution to Exercise 2.5, page 62
f–1({–1, 1}) = {nπ : n ∈ Z}, f–1({1}) = {2nπ : n ∈ Z}, f–1([–1, 1]) = R, and 
Solution to Exercise 2.6, page 62
Since cos is periodic with period 2π (that is, f(x) = f(x + 2π) for all x ∈ R), we
have f(R) = f([0, 2π]) = f([δ, δ + 2π]) = [–1, 1].
Solution to Exercise 2.7, page 64
(“If” part) Suppose that for every closed F in Y, f–1(F) is closed in X.
Now let V be open in Y. Then Y\V is closed in Y.
Thus f–1(Y\V) = f–1(Y)\f–1(V) = X\f–1(V) is closed in X.
Hence f–1(V) = X\(X\f–1(V)) is open in X.
So for every open V in Y, f–1(V) is open in X.
By Theorem 2.1, page 63, f is continuous on X.
(“Only if” part) Suppose that f is continuous.

Let F be closed in Y, that is, Y\F is open in Y.
Hence f–1(Y\F) = f–1(Y\f–1(F) = X\f–1(F) is open in X.
Consequently, we have that f–1(F) is closed in X.
Solution to Exercise 2.8, page 64
If x ∈ (g  f)–1(W), then (g  f)(x) ∈ W, that is, g(f(x)) ∈ W. So f(x) ∈ g–1(W), that
is, x ∈ f–1(g–1(W)). Thus (g  f)–1(W) ⊂ f–1 (g–1(W)).
If x ∈ f–1(g–1(W)), then f(x) ∈ g–1(W), that is, (g  f)(x) = g(f(x)) ∈ W. Hence x ∈
(g  f)–1(W). So we have f–1(g–1(W)) ⊂ (g f)–1(W).
Consequently, (g  f)–1(W) = f–1(g–1(W)).
Solution to Exercise 2.9, page 64
(1) True.
Since (–∞, 1) is open in R and f : X → R is continuous, it follows that {x ∈
X : f(x) < 1} = f–1(–∞, 1) is open in X by Theorem 2.1, page 63.
(2) True.
Because (1, ∞) is open in R, and f : X → R is continuous, it follows by
Theorem 2.1, page 63, that {x ∈ X : f(x) > 1} = f–1 (1, ∞) is open in X.
(3) False.
Take for example X = R with the usual Euclidean norm, and consider the
continuous function f(x) = x for all x ∈ R. Then {x ∈ X : f(x) = 1} = {1},
which is not open in R.
(4) True.
(–∞, 1] is closed in R because its complement is (1, ∞), which is open in R.
As f : X → R is continuous, {x ∈ X : f(x)  1} = f–1 (–∞, 1] is closed in X by
Corollary 2.1, page 64.
(5) True.
Since {1} is closed in R and since f : X → R is continuous, it follows by
Corollary 2.1, page 64, that {x ∈ X : f(x) = 1} = f–1{1} is closed in X.
(6) True.
Each of the sets f–1{1} and f–1{2} are closed, and so their finite union,
namely {x ∈ X : f(x) = 1 or 2} is closed as well.
(7) False.
Take for example X = R with the usual Euclidean norm, and consider the

continuous function f(x) = 1 (x ∈ R). Then {x ∈ X : f(x) = 1} = R, which is
not bounded, and hence can’t be compact.
Solution to Exercise 2.10, page 65
For all x ∈ X, we have f(2x) = –f(x), and so
Since the sequence 
 converges to 0, it follows that
So we obtain that ((–1)nf(x))n∈N is convergent with limit f(0). Thus the
subsequence (f(x))n∈N = ((–1)2nf(x))n∈N of ((–1)f(x))n∈N is also convergent with
limit f(0). Hence f(x) = f(0) for all x ∈ X. As f(0) = f(2 · 0) = –f(0), it follows that
f(0) = 0. Hence f(x) = 0 for all x ∈ X. So if f is continuous and it satisfies the
given identity then it must be the constant function x  0 : X → Y.
Conversely, the constant function x 
 0 : X → Y is indeed continuous and also
f(2x) + f(x) = 0 + 0 = 0 for all x ∈ X.
Solution to Exercise 2.11, page 65
The determinant of M = [mij] is given by the sum of expressions of the type
where p : {1, 2, 3, ···, n} → {1, 2, 3, ···, n} is a permutation. Since each of the
maps M  m1p(1) m2p(2) m3p(3) ... mnp(n) is easily seen to be continuous using the
characterisation of continuous functions provided by Theorem 2.3, page 64, it
follows that their linear combination is also continuous.
{0} is closed in R, and so its inverse image det–1{0} = {M ∈ Rn×n : det M = 0}
under the continuous map det is also closed. Thus its complement, namely the
set {M ∈ Rn×n : det M ≠ 0}, is open. But this is precisely the set of invertible
matrices, since M ∈ Rn×n is invertible if and only if det M ≠ 0.

Solution to Exercise 2.12, page 73
We’d seen in Exercise 1.21, page 21, that a singleton set in any normed space is
closed. So {0} is closed in Rm. As the linear transformation TA : Rn → Rm is
continuous, its inverse image under TA, T–1A({0}) = {x ∈ Rn : Ax = 0} = ker A, is
closed in Rn.
Solution to Exercise 2.13, page 73
Let V be a subspace of Rn, and let {v1, · · ·, vk} be a basis for V. Extend this to a
basis {v1, · · ·, vk, vk+1, · · ·, vn} for R. By using the Gram-Schmidt
orthogonalisation procedure, we can find an orthonormal5 set of vectors {u1, · ·
·, un} such that for each k ∈ {1, · · ·, n}, the span of the vectors v1, · · ·, vk
coincides with the span of u1, · · ·, uk. Now define A ∈ R(n–k)×n as follows:
It is clear from the orthonormality of the ujs that Au1 = · · · = Auk = 0, and so it
follows that also any linear combination of u1, · · ·, uk lies in the kernel of A. In
other words, V ⊂ ker A.
On the other hand, if x = α1u1 + · · · + αnun, where α1, · · ·, αn are scalars and if
Ax = 0, then it follows that
So x = α1u1 + · · · + αkuk ∈ V. Hence ker A ⊂ V.
Consequently V = ker A, and by the result of the previous exercise, it now
follows that V is closed.
Solution to Exercise 2.14, page 73
(1) The linearity of T follows immediately from the properties of the Riemann
integral. Continuity follows from the straightforward estimate

(2) The partial sums sn of the series converge to f. Thus, since the continuous
map T preserves convergent sequences, it follows that
Solution to Exercise 2.15, page 73
We have for all t ∈ R that
Thus ||f ∗ g||∞  ||g||∞||f||1 for all g ∈ L∞(R). So f∗ is well-defined. Linearity is
easy to see. From the above estimate, it follows that the linear transformation f∗
is continuous as well.
Solution to Exercise 2.16, page 73
Consider the reflection map 
 : L2(R) → L2(R). Then it is straightforward to
check that R ∈ L(L2(R)), and moreover it is continuous since ||f||2 = 
2 for all f
∈ L2(R). Clearly Y = ker(I – R), and so, being the inverse image of the closed set
{0} under the continuous map I – R, it follows that Y is closed.
Solution to Exercise 2.17, page 76
For 

and so Λ ∈ CL(ℓ2) and ||Λ||  
|λn|.
Moreover, for ℓ2 ∋ en := (0, · · ·, 0, 1, 0, · · ·) ∈ ℓ2 (sequence with all terms
equal to 0 and nth term equal to 1), we have
for all n, and so ||Λ|| is an upper bound for {|λn| : n ∈ N}. Hence ||Λ||  
 |λn|.
From the above, it now follows that ||Λ|| = 
|λn|.
If λn = 1 –,  n ∈ N, then ||Λ|| = 
 
 = 1.
Suppose that x = (an)n∈N ∈ ℓ2 is such that ||x||2  1 and ||Λx||2 = ||Λ|| = 1.
If 0 = a2 = a3 = · · ·, then Λx = 0, and this contradicts the fact that ||Λx||2 = 1.
So at least one of the terms a2, a3, · · · must be nonzero.
a contradiction. So the operator norm is not attained for this particular Λ.
Solution to Exercise 2.18, page 76
Let x = (xn)n∈N ∈ ℓp, and let  > 0.
Then there exists an N such that 
 |xk|p < p. Let sn := 
 xkek.
Then for n > N, x – sn = (0, · · ·, 0, xn+1, xn+2, xn+3, · · ·).
So 
 giving ||x – sn||p < .
So (sn)n∈N converges in ℓp to x, that is, x = 
 xnen.
The map x = (x1, x2, x3, · · ·)  xn : ℓp → K is easily seen to be linear.
It’s continuous as for all x ∈ ℓp, |φn(x)| = |xn| = (|xn|p)1/p 
 = |x|p.
If x = 
, where the ξis and s are scalars, then applying φn,
As the choice of n was arbitrary, ξn =  for all n.

Solution to Exercise 2.19, page 77
(1) Let x = (xn)n∈N ∈ ℓ∞. Then for all n ∈ N, |xn|  ||x|∞.
Thus 
 = ||x||∞.
Consequently Ax ∈ ℓ∞. So A is a well-defined map.
The linearity is easy to check.
Also, we see that for all x ∈ ℓ∞ that ||Ax||∞ = 
  ||x||∞.
So A ∈ CL(ℓ∞), and ||A||  1. Also, with 1 := (1, 1, 1, · · ·) ∈ ℓ∞, we have
Consequently, ||A|| = 1.
(2) Let x = (xn)n∈N ∈ c, and let its limit be denoted by L.
We’ll show that Ax ∈ c as well.
We will prove that Ax is convergent with the same limit L! (Intuitively, this
makes sense since for large n, all xns look alike, L, and the average of these
is approximately L, since the first few terms do not “contribute much” if we
take a large collection to take an average.)
Let  > 0. Then there exists an N1 ∈ N such that for all n > N1, |xn – L| < /2.
Since (xn)n∈N is convergent, it is bounded, and so there exists an M > 0 such
that for all n ∈ N, |an|  M.
Choose N ∈ N such that N > max 
(This ghastly choice of N is arrived at by working backwards. Since we wish
to make 
 less than  for n > N, we manipulate this, as shown
in the chain of inequalities below, and then choose N large enough to achieve
this.)
So N > N1 and 
 Then for all n > N, we have:

So 
 is a convergent sequence with limit L.
Hence Ax ∈ c. Consequently Ac ⊂ c, and c is an invariant subspace of A.
Solution to Exercise 2.20, page 85
(If part:) Since 
|λn| > 0, we have |λk|  
|λn| > 0, and so λk ≠ 0 for all k.
Moreover, 
 < ∞, and so V : ℓ2 → ℓ2 given by
belongs to CL(ℓ2). Moreover for all (an)n∈N we have
and so VΛ = I = ΛV. Hence Λ is invertible in CL(ℓ2), with Λ–1 = V.
(Only if part:) Let Λ be invertible in CL(ℓ2). Then there exists a Λ–1 ∈ CL(ℓ2)
such that Λ–1Λ = I = ΛΛ–1. So ||x||2 = ||Λ–1Λx||2  ||Λ–1||||Λx||2, for all x ∈ ℓ2.
Hence ||Λx||2  
 for all x ∈ ℓ2. So with x := ek (kth term 1, others 0),
Thus 
Solution to Exercise 2.21, page 86

We have
Similarly,
Solution to Exercise 2.22, page 86
(1) If there exist matrices A, B such that AB – BA = I, then
a contradiction.
(2) If n = 1, then ABn – BnA = AB – BA = I = 1 · B0 = nBn–1.
If for some n ∈ N, we have ABn – BnA = nBn–1, then
and so the result follows by induction.
Suppose that AB – BA = I. Then for all n ∈ N, ABn – BnA = nBn–1. Taking
operator norm on both sides yields
We claim that Bn–1 ≠ 0 for all n ∈ N. Indeed, if n = 1, then B0 := I ≠ 0. If Bn–
1 ≠ 0 for some n ∈ N, then Bn = 0 gives the contradiction that
and so we must have Bn ≠ 0 too. By induction, our claim is proved. Thus in
(7.3), we may cancel ||Bn−1|| > 0 on both sides of the inequality, obtaining n 

 2||A||||B|| for all n ∈ N, which is absurd. Consequently, our original
assumption that AB − BA = I must be false.
(3) If Ψ ∈ C∞(R), then
and so AB − BA = I.
Solution to Exercise 2.23, page 87
(1) For x = (x1, x2) ∈ R2, we have, using the Cauchy-Schwarz inequality, that
So 
By the Neumann Series Theorem, (I − K)−1 exists in CL(R2).
So there is a unique solution x ∈ R2 to (I − K)x = y, given by x = (I − K)−1y.
(2) We have 
 and so 
Thus 
(3) A computer program yielded the following numerical values:
Solution to Exercise 2.24, page 88
If n = 1, then (I − A)P1 = (I − A)(I + A)(I + A2) = I − A4 = I − A21 + 1.

If the claim is true for some k ∈ N, then
So the claim follows by induction for all n ∈ N.
(I − A2n+1)n∈N converges to I in L(X) since ||A|| < 1 and
Also, since ||A|| < 1, I − A is invertible in CL(X). We have
and so ((I − A)−1 (I − A2n+1))n∈N = ((I − A)−1(I − A)Pn)n∈N = (Pn)n∈N is convergent
with limit (I − A)−1.
Solution to Exercise 2.25, page 88
(1) Let T0 ∈ GL(X). Then T0−1 ∈ CL(X), and also r := ||T0−1|| ≠ 0.
If T ∈ 
, and in particular,
and so by the Neumann Series Theorem, I + (T − T0)T0−1 belongs to GL(X).
But as T0 ∈ GL(X) too, it now follows that
This completes the proof that GL(X) is an open subset of CL(X).
(2) Let T0 ∈ GL(X) and  > 0. Set 
Let T ∈ CL(X) be such that ||T − T0|| < δ.
Then in particular ||T − 
 and so by part (1), T ∈ GL(X), with
Moreover, we have

Thus using the estimate from the Neumann Series Theorem,
Solution to Exercise 2.26, page 92
A2 = B2 = 0, and so A, B are nilpotent.
Hence 
 and 
We note that 
Also, 
We have 
 and 
 Thus
and so 
Solution to Exercise 2.27, page 94
Suppose that the Banach space has an infinite countable Hamel basis {x1, x2, x3,
··· }. We can ensure that for all n ∈ N, we have ||xn|| = 1. Let Fn := span{x1, x2,
···, xn}. Then each Fn is a finite dimensional normed space (with the induced
norm from X), and so it is a Banach space. It follows that Fn is a closed subspace
of X. By the Baire Lemma, there is an n ∈ N such that Fn contains an open set U,
and in particular, an open ball B(X, 2r) for some r > 0. The vector y := rxn+1 + x

belongs to B(X, 2r) since ||y − x|| = ||rxn+1|| = r < 2r. Since y, x ∈ B(X, 2r) ⊂ Fn,
and as Fn is a subspace, we conclude that (y − x)/r ∈ Fn too, that is, xn+1 ∈ Fn =
span{x1, ···, xn}, a contradiction.
Solution to Exercise 2.28, page 96
In light of the Open Mapping Theorem, such a function must necessarily be
nonlinear. If the function is constant on an open interval I, then the image f(I)
will be a singleton, which is not closed. The following function does the job:
If I := (−1, 1), then f(I) = {0}, which is not open. f is surjective and continuous,
and its graph is depicted in the following picture.
Solution to Exercise 2.29, page 96
From Exercise 1.38, page 44, X × Y is a Banach space. Since G(T) is a closed
subspace of the Banach space X × Y, it is a Banach space too. Let us now
consider the map p : G(T) → X defined by p(X, Tx) = x for x ∈ X. Then p is a
linear transformation:
for α ∈ K, x, x1, x2 ∈ X. Moreover, p continuous because
p is also injective since if p(X, Tx) = 0, then x = 0.
Furthermore, if x ∈ X, then x = p(x, Tx), showing that p is surjective too.

Thus, p ∈ CL(G(T), X) is bijective, and hence invertible in CL(G(T), X), with
inverse p−1 ∈ CL(X, G(T)). Hence for all x ∈ X,
showing that T ∈ CL(X, Y).
Solution to Exercise 2.30, page 102
We have
Solution to Exercise 2.31, page 102
(1) We know that σ (T) ⊂ {λ ∈ C : |λ|  ||T||}, and so ||T|| is an upper bound for
{|λ| : λ ∈ σ(T)}. Thus 
(2) We have σ(TA) = {eigenvalues of A} = {1}, and so rσ(TA) = 1.
On the other hand, with 
 we have ||x1||2 = 1, and so
Solution to Exercise 2.32, page 103
Suppose that λ2 ∉ σ(T2). Then λ2 ∈ ρ(T2), that is, λ2 I − T2 is invertible in CL(X).
From the identity (λ2I − T2) = (λI − T)(λI + T) = (λI + T)(λI − T), we then obtain 
But then Q = QI = Q(λI − T)P = IP = P, and so P = Q ∈ CL(X) is the inverse of
λI − T, a contradiction to the fact that λ ∈ σ(T).
Solution to Exercise 2.33, page 103
If en ∈ ℓ2 denotes the sequence with the nth term equal to 1, and all others equal
to 0, then Λen = λnen, and so each λn is an eigenvalue of Λ with eigenvector en ≠

0. Thus {λn : n ∈ N} ⊂ σp(Λ).
Next we will show that σ(Λ) ⊂ {λn : n ∈ N} {0}. To this end, suppose that μ ∉
{λn : n ∈ N} {0}. Then we claim that μI − Λ is invertible in CL(ℓ2). By a
previous exercise, we know that in order to show the invertibility of
it is enough to show that |μ − λn| is bounded away from 0. To see this, note that
since 
 there is an N large enough such that |λn| < |μ|/2 for all n > N, and so
But also |μ − λ1|, ···, |μ − λN| are all positive, so that we do have
Hence μI − Λ ∈ CL(ℓ2) is invertible in CL(ℓ2), that is, μ ∈ ρ(Λ).
Thus σ(Λ) ⊂ {λn : n ∈ N} {0}.
But the spectrum σ(Λ) is closed, and since it contains σp(Λ) ⊃ {λn : n ∈ N}, it
must contain the limit of (λn)n∈N, which is {0}.
So we also obtain {λn : n ∈ N} {0} ⊂ σp(Λ) {0} ⊂ σ(Λ).
Thus σ(Λ) = {λn : n ∈ N} {0}.
Consequently, {λn : n ∈ N} ⊂ σp(Λ) ⊂ {λn : n ∈ N} {0} = σ(Λ).
Solution to Exercise 2.34, page 103
(1) Suppose that λ ∈ σap(T). Then there exists a sequence (xn)n∈C of vectors in X
such that ||xn|| = 1 for all n ∈ N, and 
We will just prove that λ ∉ ρ(T), and so by definition it will follow that then
λ ∈ σ(T). Suppose, on the contrary, that λ ∈ ρ(T). Then T − λI is invertible in
CL(X). Thus

a contradiction. Consequently, λ ∉ ρ(T), that is, λ ∈ σ(T).
(2) For k ∈ N, let ek denote the sequence in ℓ2 whose kth term is 1 and all other
terms are zeros. Then ||ek||2 = 1, and Λek = λkek, so that
that is, 
 Consequently, 
Solution to Exercise 2.35, page 103
Let λ ∈ C and Ψ ∈ DQ be such that xΨ(x) = λΨ(x) for almost all x ∈ R, that is, (x
− λ)Ψ(x) = 0 for almost all x ∈ R. Now x − λ ≠ 0 for all x ∈ R\{λ}. Hence for
almost all x ∈ R, Ψ(x) = 0, that is, Ψ = 0 in L2(R). Consequently, λ can’t be an
eigenvalue of Q, and so σp(Q) = ∅.
Solution to Exercise 2.36, page 105
For simplicity we’ll assume K = R. If a = (an)n∈N ∈ ℓ1, then define the
functional φa ∈ CL(c0, R) = (c0)′ by
Then a 
 φa : ℓ1 → (c0)′ is an injective linear transformation, and it is also
continuous because |φa(b)|  ||b||∞ ||a||1 for all b ∈ c0, and ||φa||  ||a||1. To see the
surjectivity of this map, we need to show that given φ ∈ (c0)′, there exists an a ∈
ℓ1 such that φ = φa. Let en ∈ c0 being the sequence with nth term 1 and all others
0. Set a = (φ(e1), φ(e2), φ(e3), ···). We’ll show that a ∈ ℓ1, and that φ = φa.
Define the scalars αn, n ∈ N, by 
Then for all n we have αnφ(en) = |φ(en)|.
We have ||(α1, ···, αn, 0, ···)||∞  1, and so

for all n ∈ N. Hence a ∈ ℓ1.
Finally, we need to show φ = φa . Let b = (bn)n∈N ∈ c0 and  > 0. Then there
exists an N such that for all n > N, |bn| < . Set b  = (b1, ···, bN, 0, ···) ∈ c0. Then
||b − b ||∞ = ||(0, ··· , 0, bN+1, ···)||∞  . Moreover, we have that
Hence
As the choice of  > 0 was arbitrary, it follows that φ(b) = φa(b) for all b ∈ c0,
that is, φ = φa.
Solution to Exercise 2.37, page 105
(1) BV [a, b] is a vector space: We prove that BV [a, b] is a subspace of the
vector space R[a,b] of all real valued functions on [a, b] with pointwise
operations.
(S1) The zero function 0 belongs to BV [a, b].
Indeed, for any partition 
 and so var(0) = 0 < ∞.
(S2) Let μ1, μ2 ∈ BV [a, b]. Then we have
and so μ1 + μ2 ∈ BV [a, b].
(S3) Let α ∈ R and μ ∈ BV [a, b]. Then

and so αμ ∈ BV [a, b].
(2) We show that μ  ||μ|| defines a norm on BV [a, b].
(N1) If μ ∈ BV [a, b], then ||μ|| = |μ(a)| + var(μ)  0.
Let μ ∈ BV [a, b] be such that ||μ|| = 0. Then var(μ) = 0, and |μ(a)| = 0.
Hence μ(a) = 0. Suppose that μ ≠ 0. Then there exists a c ∈ [a, b] such that
μ(c) ≠ 0. Clearly c ≠ a, since μ(a) = 0. Now consider the partition
Then var 
a contradiction. Hence μ = 0.
(N2) Let α ∈ R and μ ∈ BV [a, b]. Then αμ ∈ BV [a, b], and we have seen
earlier that varαμ = |α|var(μ). Hence
(N3) Let μ1, μ2 ∈ BV [a, b]. Then μ1 + μ2 ∈ BV [a, b], and we’ve seen above
that var(μ1 + μ2)  var(μ1) + var(μ2). Thus
Consequently BV [a, b] is a normed space with the norm ||·||.
(3) Let x ∈ C[a, b] and μ ∈ BV [a, b]. Given  > 0, let δ > 0 be such that for
every partition P satisfying δP < δ, we have

Then
As the choice of  > 0 was arbitrary, it follows that 
(4) For all x ∈ C[a, b], |φµx|  ||x||∞ var(μ).
From the linearity of the Riemann-Stieltjes integral, it follows that φµ is a
linear transformation from C[a, b] to R. From the above estimate, we also
see that φµ is continuous. Consequently φµ ∈ CL(C[a, b), R) = (C[a, b])′.
Moreover ||φµ||  var(μ).
(5) We will show that (x  x(a)) = φµ, where 
First of all, μ ∈ BV [a, b], since var(μ) = 1 < ∞.
Let x ∈ C[a, b], and  > 0. Let δ > 0 be such that for all t such that t − a < δ,
we have |x(t) − x(a)| < .
Then for all partitions P with δP < δ, we have
where the last inequality follows from the fact that |a − t1|  δP < δ.
So 
 (μ is not unique: for any c ∈ R, μ + c also works!)
Solution to Exercise 2.38, page 109
On the one dimensional subspace Y :=span{x∗} ⊂ X, we have a continuous
linear map φ : Y → C. (Simply define φ(αx∗) = α, then |φ(αx∗)| = |α| = ||
αx∗||/||x∗||, and so ||φ|| = 1/||x∗|| < ∞.) By the Hahn-Banach Theorem, there exists
an extension φ∗ ∈ CL(X, C) of φ, and so φ∗(x∗) = φ(x∗) = 1 ≠ 0. (Alternatively,
one could just use Corollary 2.7, page 109, with x = x∗ and y = 0: there exists a

functional φ∗ ∈ CL(X, C) such that φ∗(x∗) ≠ φ∗(0) = 0.)
Solution to Exercise 2.39, page 115
Consider the collection P of all linearly independent subsets S ⊂ X. Consider the
partial order which is simply set inclusion ⊂. Then every chain in P has an upper
bound, as explained below.
If C is a chain in P, then 
 is an upper bound of C.
We just need to show the linear independence of this set U. To this end, let v1,
···, vn be any set of vectors from U for which there exist scalars α1, ···, αn in F
such that α1v1 + ··· + αnvn = 0. Let the sets S1, ···, Sn ∈ C be such that v1 ∈ S1,
···, vn ∈ Sn. As C is a chain, we can arrange the finitely many Sks in “ascending
order”, and there exists a k∗ ∈ {1, ···, n} such that S1, ···, Sn ⊂ Sk∗. Then v1, ···,
vn ∈ Sk∗. But by the linear independence of Sk∗, we conclude that α1 = ··· = αn =
0. Thus U is linearly independent, showing that every chain in P has an upper
bound.
By Zorn’s Lemma, P has a maximal element B. We claim that span B = X.
For if not, then there exists an x ∈ X\span B. We will show B′ := B ∪ {x} is
linearly independent. Suppose that α1, ···, αn, α ∈ K and v1, ···, vn ∈ B are such
that αx + α1v1 + ··· + αnvn = 0. First we note that α = 0, since otherwise
which is false. As α = 0, the equality αx + α1v1 + ··· + αnvn = 0 now becomes
α1v1 + ··· + αnvn = 0. But by the independence of the set B, we conclude that α1
= ··· = αn = 0 too. Hence B′ is linearly independent, and so B′ belongs to P. As B′
= B ∪ {x} 
 B, we obtain a contradiction (to the maximality of B).
Consequently, span B = X, and as B ∈ P, B is also linearly independent.
Solution to Exercise 2.40, page 115
Let B = {vi : i ∈ I}. Every x ∈ X has a unique decomposition
for some finite number of indices i1, ···, in ∈ I and scalars α1, ···, αn in F. Define
F(x) = α1f(vi1) + ··· + αnf(vin). It is clear that F(vi) = f(vi), i ∈ I. Let us check that
F : X → Y is linear.

(L1) Given x1, x2 ∈ X, there exist scalars α1, ···, αn and β1, ···, βn (possibly
several of them equal to zero) and indices i1, ···, in ∈ I, such that
(L2) Let α ∈ F. Given x ∈ X, there exist β1, ··· , βn ∈ F and i1, ···, in ∈ I, such
that x = β1vi1 + ··· + βnvin. Then αx = (αβ1)vi1 + ··· + (αβn)vin.
Solution to Exercise 2.41, page 115
Let B be a Hamel basis for X. As X is infinite dimensional, B is an infinite set.
Let {vn : n ∈ N} be a countable subset of B. Let y∗ ∈ Y be any nonzero vector.
Let f : B → Y be defined by 
By the previous exercise, this f extends to a linear transformation F from X to Y.
We claim that F ∉ CL(X, Y). Suppose that it does. Then there exists an M > 0
such that for all x ∈ X, ||F(x)||  M||x||. But if we put x = vn, n ∈ N, this yields
n||vn||||y∗|| = ||f(vn)|| = ||F(vn)||  M ||vn||, and so for all n ∈ N, n  M/||y∗||, which
is absurd. Thus F is a linear transformation from X to Y, but is not continuous.
Solution to Exercise 2.42, page 115
If R were finite dimensional, say d-dimensional over Q, then there would exist a
one-to-one correspondence between R and Qd. But Qd is countable, while R
isn’t, a contradiction. So R is an infinite dimensional vector space over Q.
Suppose that R has a countable basis B = {vn : n ∈ N} over Q.
We will define an injective map 
 yielding a contradiction.
Set f(0) := 0 ∈ Q1. If x ≠ 0, then x has a decomposition x = q1v1 + ··· + qnvn,
where q1, ···, qn ∈ Q and qn ≠ 0. In this case, set f(x) = (q1, ···, qn) ∈ Qn. It can
be seen that if f(x) = f(y), for some x, y ∈ R, then x = y. So f is injective.
As 
 is countable, follows that R is countable too, a contradiction.
Hence B can’t be countable.

Solution to Exercise 2.43, page 115
The set R is an infinite dimensional vector space over Q. Let {vi : i ∈ I} be a
Hamel basis for this vector space. Fix any i∗ ∈ I.
We define a function f : B → R on the basis elements: 
Let F be an extension of f from B to R, as provided by Exercise 2.40, page 115.
Then F is linear, and in particular, additive. So F(x + y) = F(x) + F(y) for all x, y
∈ R.
We now show that F is not continuous on R: for otherwise, for any vi ≠ vi∗, if
(qn)n∈N is a sequence in Q converging to the real number vi/vi∗ (vi∗ ≠ 0 since it is
a basis vector), then we would have
a contradiction!
Solution to Exercise 2.44, page 116
(1) By the Algebra of Limits, the map l is linear.
Let (xn)n∈N ∈ c. For all n ∈ N, |xn|  ||(xn)n∈N||∞.
Passing the limit as 
Thus l ∈ CL(c, K).
(2) Y is a subspace of ℓ∞. Indeed we have:
(S1) Clearly (0)n∈N ∈ Y, since 
(S2) Let (xn)n∈N, (yn)n∈N ∈ Y.
Then 
 and 
 exist.
As 
we conclude that 
 exists as well.
Thus (xn)n∈N + (yn)n∈N ∈ Y too.
(S3) Let (xn)n∈N ∈ Y and α ∈ K. Then 
 exists.

As 
 it follows that
 exists, and so α · (xn)n∈N ∈ Y.
Consequently, Y is a subspace of ℓ∞.
(3) For all x ∈ ℓ∞, x − Sx ∈ Y : Let x = (xn)n∈N ∈ ℓ∞. Then we have
We have 
As x ∈ ℓ∞, it follows that 
 and so x − Sx ∈ Y.
(4) If x = (xn)n∈N ∈ c, then Ax ∈ c, where A denotes the averaging operator
(Exercise 2.19, page 77).
Hence 
 exists, and so x ∈ Y. Consequently, c ⊂ Y.
(5) Define L0 : Y → K by L0(xn)n∈N = 
Then it is easy to check that L0 : Y → K is a linear transformation.
Moreover, if x ∈ Y, then 
But 
Hence |L0x|  ||x||∞. Consequently, L0 ∈ CL(Y, K).
We had seen that if x ∈ c, then Ax ∈ c, and that l(Ax) = l(x).
Hence for all x ∈ c, L0(x) = l(Ax) = l(x), that is, L0|c = l.
Using the Hahn-Banach Theorem, there exists an L ∈ CL(ℓ∞, K) such that
L|Y = L0 (and ||L|| = ||L0||).
In particular, if x ∈ c, then x ∈ Y and so Lx = L0x = lx. Thus L|c = l.
Also, if x = (xn)n∈N ∈ ℓ∞, then x − Sx ∈ Y.
Hence 
Thus Lx = LSx for all x ∈ ℓ∞, that is, L = LS.
(6) We have 
Consequently, 
Solutions to the exercises from Chapter 3

Solution to Exercise 3.1, page 124
f is a continuous linear transformation. Thus it follows that f′(x0) = f for all x0,
and in particular also for x0 = 0.
Solution to Exercise 3.2, page 125
Suppose that f′(x0) = L ∈ CL(X, Y). Let M > 0 be such that ||Lh||  M||h||, for all h
∈ X. Let  > 0. Then there exists a δ1 > 0 such that whenever x ∈ X satisfies 0 <
||x − x0|| < δ1, we have
So if x ∈ X satisfies ||x − x0|| < δ1, then ||f(x) − f(x0) − L(x − x0)||  ||x − x0||.
Let 
 Then for all x ∈ X satisfying ||x − x0|| < δ, we have
Hence f is continuous at x0.
Solution to Exercise 3.3, page 125
(Rough work: We have for x ∈ C1[0, 1] that
where L : C1[0, 1] → R is the map given by Lh = 2x′0(1)h′(1), h′ ∈ C1[0, 1]. So
we make the guess that f′(x0) = L.)
Let us first check that L is a continuous linear transformation. L is linear
because:
(L1) For all h1, h2 ∈ C1[0, 1], we have
(L2) For all h ∈ C1 [0, 1] and α ∈ R, we have

Also, L is continuous since for all h ∈ C1[0, 1], we have
So L is a continuous linear transformation. Moreover, for all x ∈ C1[0, 1],
so that 
Given  > 0, set δ = . Then if x ∈ C1[0, 1] satisfies 0 < ||x − x0||1, ∞ < δ, we have
Solution to Exercise 3.4, page 125
Given  > 0, let ′ > 0 be such that ′||x2 − x1|| < . Let δ′ > 0 such that whenever 0
< ||x − γ(t0)|| < δ′, we have
Let δ 0 be such that δ ||x2 − x1|| < δ′. For all t ∈ R satisfying 0 < |t − t0| < δ,
and so ||γ(t) − γ(t0)|| = |t − t0|||x2 − x1||  δ||x2 − x1|| < δ′. Thus for all t ∈ R
satisfying 0 < |t − t0| < δ, we have
Thus f  γ is differentiable at t0 and 
Let x1, x2 ∈ X be such that g(X1) ≠ g(X2). With γ the same as above, we have for
all t ∈ R that

So g  γ is constant. Thus (g  γ)(1) = g(x2) = g(x1) = (g  γ)(0), a contradiction.
Consequently, g is constant.
Solution to Exercise 3.5, page 128
Suppose that f′(x0) = 0. Then for every 
In particular, setting h = x0, we have 
 giving x0 = 0 ∈ C[a, b].
Vice versa, if x0 = 0, then
for all h ∈ C[a, b], that is, f′(0) = 0.
Consequently, f′(x0) = 0 if and only if x0 = 0.
So we see that if x∗ is a minimiser, then f′(x∗) = 0, and so from the above x∗ = 0.
We remark that 0 is easily seen to be the minimiser because
Solution to Exercise 3.6, page 129
If x1, x2 ∈ S, α ∈ (0, 1), then x1, x2 ∈ C1[a, b]. So (1 − α)x1 + αx2 ∈ C1[a, b].
Moreover, as x1(a) = x2(a) = ya and x1b = x2(b) = yb, we also have that
Thus (1 − α)x1 + αx2 ∈ S. Consequently, S is convex.
Solution to Exercise 3.7, page 129
For x1, x2 ∈ X and α ∈ (0, 1) we have by the triangle inequality that
Thus || · || is convex.

Solution to Exercise 3.8, page 129
(If part:) Let x1, x2 ∈ C and α ∈ (0, 1). Then we have that (x1, f(x1) ∈ U(f) and
(x2, f(x2)) ∈ U(f). Since U(f) is convex,
Consequently, (1 − α)f(x1) + αf(x2) = y  f(x) = f((1 − α) · x1 + α · x2). Hence f is
convex.
(Only if part:) Let (x1, y1), (x2, y2) ∈ U(f) and α ∈ (0, 1). Then we know that y1 
f(x1) and y2  f(x2) and so
Consequently, 
 that is,
So U(f) is convex.
Solution to Exercise 3.9, page 129
We prove this using induction on n. The result is trivially true when n = 1, and in
fact we have equality in this case. Suppose the inequality has been established
for some n ∈ N. If x1, ···, xn, xn+1 are n + 1 vectors, and 
 then
and so the claim follows for all n.

Solution to Exercise 3.10, page 130
We have for all x ∈ R
Thus f is convex.
(Alternately, one could note that 
 is a norm on R2, and so
it is convex. Now fixing y = 1, and keeping x variable, we get convexity of 
Solution to Exercise 3.11, page 132
For x1, x2 ∈ C1[0, 1] and α ∈ (0, 1), we have, using the convexity of function 
 (Exercise 3.10, page 130), that
Solution to Exercise 3.12, page 133
(If:) Suppose that x0(t) = 0 for all t ∈ [0, 1]. Then we have that for all h ∈ C[0,
1],
and so f′(x0) = 0.
(Only if:) Now suppose that f′(x0) = 0. Thus for every h ∈ C[0, 1], we have

In particular, taking h := x0 ∈ C[0, 1], we obtain 
So 
 As x0 is continuous on [0, 1], it follows that x0 = 0.
By the necessary condition for x0 to be a minimiser, we have that f′(x0) = 0 and
so x0 must be the zero function 0 on [0, 1]. Furthermore, as f is convex and f′(0)
= 0, it follows that the zero function is a minimiser. Consequently, there exists a
unique solution to the optimisation problem, namely the zero function 0 ∈ C[0,
1]. The conclusion is also obvious from the fact that for all x ∈ C[0, 1],
Solution to Exercise 3.13, page 141
We have 
 Then 
 and 
The Euler-Lagrange equation is 
Upon integrating, we obtain 
 on [a, b] for some constant C.
Thus 
, for all t ∈ [a, b].
So A  0, and 
 for each t ∈ [a, b]. As 
 is continuous, we can
conclude that 
 must be either everywhere equal to 
, or everywhere equal to
−
. In either case, 
 is constant, and so x∗ is given by x∗(t) = αt + β, t ∈ [a, b].
Since x∗(a) = xa and x∗(b) = xb, we have
and 
 for all t ∈ [a, b].
That this x∗ ∈ S is indeed a minimiser can be concluded by noticing that the
map x  L(γx) : S → R is convex, thanks to the convexity of 
 
 for all η ∈ R (Exercise 3.10, page 130).
(The fact that x∗ is a minimiser, is of course expected geometrically, since the
straight line is the curve of shortest length between two points in the Euclidean
plane.)
Solution to Exercise 3.14, page 141

We have 
Solution to Exercise 3.15, page 141
With 
 we have
Then 
 and 
The Euler-Lagrange equation is 
Upon integrating, we obtain 
 on [a, b] for some constant C.
Thus 
So A  0, and 
 for each t ∈ [a, b]. As 
 is continuous, we can
conclude that 
 must be either everywhere equal to 
, or everywhere equal to
−
. In either case, 
 is constant, and so x∗ is given by x∗(t) = αt + β, t ∈ [a, b].
Since x∗(a) = xa and x∗(b) = xb, we have
and 
 for all t ∈ [a, b]
We will now show that this x∗ is a maximiser of x  L(γx) : S → R, that is, it is a
minimiser of x 
 −L(γx). Note that the map 
 is convex
because
Hence x  −L(γx) : S → R is convex too, and this proves our claim.
Solution to Exercise 3.16, page 142
We have 
 Thus

So the Euler-Lagrange equations are
that is,
Solution to Exercise 3.17, page 143
(1) With 
 we have that
We have 
So the Euler-Lagrange equation is
We have
Similarly 
Thus the Euler-Lagrange equation becomes (using uxy = uyx)
If u = Ax + By + C, then uxx = 0, uxy = 0 and uyy = 0, so that all the three

summands on the left-hand side of the Euler-Lagrange equation vanish, and so
we see that the Euler-Lagrange equation is satisfied.
If u = tan–1 (y/x), then we have
Thus uxx = 
, uxy = uyx = 
, and uyy = 
.
Hence
With s := 
 and t = tan–1(y/x) = u, we have tan t = , and so
Thus x = 
 · cos t = s · cos t. Then
Vice versa, if x = s · cos t, y = s · sin t and u = t, then
and so s = 
. Also 
 = tan t, and so u = tan–1(y/x) = t.
Using the Maple command given in the exercise we obtain the following:
(2) If L(X1, X2, U, V1, V2) := 
, then I(u) = 
.

We have 
.
So the Euler-Lagrange equation is:
Thus u∗ satisfies the wave equation 
 = 0.
We can check this by direct differentiation that the given u in terms of f satisfies
the wave equation. We have
Differentiating again with respect to t, we obtain
Similarly, by differentiating u with respect to x we obtain
Differentiating again with respect to x, we obtain
It follows from (∗) and (∗∗) that 
 = 0.
Let us check that the boundary conditions are satisfied.
Note that u(0, t) = 
 = 0 since f is odd.
Now we would like to check u(1, t) = 0 too.
Using the oddness and 2-periodicity of f, we have
So u(1, t) = 
 = 0.
Finally, we can check if the initial conditions is satisfied.
We have u(x, 0) = 
 = f(x) for all x.
Also, from our previous calculation, we have

for all x.
For a fixed t, the graph of f(· –t) is just a shifted version of the graph of f by t
units to the right. As t increases, the graph travels to the right, representing a
travelling wave, moving to the right with a speed 1. Similarly the graph of f(·+t)
with increasing t represents a travelling wave moving to the left with speed 1.
The solution of the wave equation is an average of these two travelling waves
moving in opposite directions, and the shape of the wave is determined by the
initial shape of the string.
Solution to Exercise 3.18, page 153
We have (suppressing the argument (q, p) everywhere)
Also,
Finally, we will prove the Jacobi Identity. In order to simplify the notation, we
will use subscripts to denote partial derivatives, for example Fp will mean 
.First we note that
Similarly, by making cyclic substitutions F → G → H above, we obtain

Thanks to the symmetry of the left-hand side of the expression in Jacobi’s
Identity in F, G, H, it is enough to show that after collecting all the Fq, Fp terms,
their overall coefficients are zero.
The overall coefficient of Fq is
Since Gpq = Gqp and Hpq = Hqp, we see that the above expression is 0.
The overall coefficient of Fp is
This completes the proof of the Jacobi Identity.
Solution to Exercise 3.19, page 153
We have {Q, P} = 
 = 1 · 1 – 0 · 0 = 1.
Solutions to the exercises from Chapter 4
Solution to Exercise 4.1, page 162
With x := 1 = (t  1), and y := (t  t), 2||x||2∞ + 2||y||2∞ = 2 · 12 + 2 · 12 = 4, while
||x + y||2
∞ + ||x – y||2
∞ = ||1 + t||2
∞ + ||1 – t||2
∞ = 22 + 12 = 5. So ||·||∞ does not obey the
Parallelogram Law, and hence ||·||∞ cannot be a norm induced by some inner
product on C[0, 1].
Solution to Exercise 4.2, page 162
Let x, y, z ∈ X. Then
Adding these, we obtain

Geometric interpretation in R2: If x, y, z are the vertices of a triangle ABC, then 
 is the length of the median AD (see the picture).
The Appollonius Identity gives AB2 + AC2 = BC2 + 2AD2.
Solution to Exercise 4.3, page 162
Let  > 0. Let N1 ∈ N be such that for all n > N1, ||xn – x|| < 
.
Let N2 ∈ N be such that for all n > N2, ||yn – y|| < 
, where the number M
:= 
 ||xn|| < ∞ (this exists since (xn)n∈N, being convergent, is bounded).
Consequently, for all n > N := max{N1, N2},
Hence ( xn, yn )n∈N is convergent in K, with limit x, y .
Solution to Exercise 4.4, page 162
If the ellipse has major and minor axis lengths as 2a and 2b, respectively, then
observe that the perimeter is given by

where the last expression is obtained by rotating the ellipse through 90°,
obtaining a new ellipse with the same perimeter.
Using Cauchy-Schwarz Inequality we obtain
Thus P  2π√ab. Since the areas of the circle and the ellipse are equal, it follows
that πr2 = πab, where r denotes the radius of the circle. Hence r = √ab. So we
have P  2π√ab = 2πr, that is, the perimeter P of the ellipse is at least as large as
the circumference of the circle.
Solution to Exercise 4.5, page 163
(IP1) If A ∈ Rm×n, then A, A  = tr(A A) = 
 aki aki = 
 a2
ki  0. 
If A ∈ Rm×n and A, A  = 0, then 
 a2
ki = 0, and so for all 
k ∈ {1, ···, m} and all i ∈ {1, ···, n}, aki = 0, that is, A = 0.
(IP2) For all A1, A2, B ∈ Rm×n,
For all A, B ∈ Rm×n and α ∈ R,

(IP3) For all A, B ∈ Rm×n,
This is a Hilbert space, since finite-dimensional normed spaces are complete.
Solution to Exercise 4.6, page 163
Let x, y ∈ X. Then
Also,
From (∗) and (∗∗) it follows that for all x, y ∈ X, Tx, Ty  = 0.
In particular, with y = Tx, we get Tx, Tx  = 0, that is, ||Tx||2 = 0.
Hence for all x ∈ X, Tx = 0, that is, T = 0.
We have Tx, x  = 
 = –x2x1 + x1x2 = 0, for all x = 
 ∈ R2.
There is no contradiction to the previous part since the vector space R2 is a
vector space over the real scalars.
Solution to Exercise 4.7, page 163
R is an equivalence relation on C:
(ER1) If x = (xn)n∈N ∈ C, then 
 ||xn – xn||X = 
 0 = 0, and so (x, x) ∈ R.
(ER2) If x = (xn)n∈N, y = (yn)n∈N ∈ C, and (x, y) ∈ R, then 
 ||xn –yn||X = 0. 
So 
 ||yn – xn||X = 
 |–1| ||xn – yn||X = 
 ||xn – yn||X = 0. 
Hence (y, x) ∈ R.

(ER3) Let x = (xn)n∈N, y = (yn)n∈N, z = (zn)n∈N be in C, such that (x, y) ∈ R and
(y, z) ∈ R. Then 
 ||xn – yn||X = 0 and 
 ||yn – zn||X = 0.
As 0  ||xn – zn||X  ||xn – yn||X + ||yn – zn||X , we get 
 ||xn – zn||X = 0. 
So (x, z) ∈ R.
Consequently, R is an equivalence relation on C.
 is well-defined:
If [(xn)n∈N] = [(x′n)n∈N] and [(yn)n∈N] = [(y′n)n∈N], then we wish to show that [(xn
+ yn)n∈N] = [(x′n + y′n)n∈N]. We have that (xn + yn)n∈N ∈ C, since (xn)n∈N, (yn)n∈N
∈ C and ||xn + yn – (xm + ym)||X  ||xn – xm||X + ||yn – ym||X.
Similarly, (x′n + y′n)n∈N ∈ C.
Furthermore, 0  ||(xn + yn) – (x′n + y′n)||X + ||xn + x′n||X + ||yn + y′n)||X, and so
that is, ((xn + yn)n∈N, (x′n + y′n)||n∈N) ∈ R. So [(xn + yn)n∈N] = [(x′n + y′n)||n∈N].
 is well-defined:
Let α ∈ K and [(xn)n∈N] = [x′n)n∈N]. Since ||αxn – αxm||X = |α|||xn – xm||X, clearly
(αxn)n∈N ∈ C. Similarly, (αx′n)n∈N ∈ C. We have
and so ((αxn)n∈N, (αx′n)n∈N) ∈ R. So [(αxn)n∈N] = [(αx′n)n∈N].
 is well-defined:
Since Cauchy sequences are bounded, given (xn)n∈N, (yn)n∈N in C, we have that
Mx := 
 ||xn||X < ∞ and My := 
 ||yn||X < ∞.
Let N be large enough so that if m, n > N, then
Thus for m, n > N,

So ( xn, yn X)n∈N is a Cauchy sequence in K, and as K (= R or C) is complete, it
follows that 
 xn, yn X exists.
Now suppose that [(xn)n∈N] = [(x′n)n∈N] and [(yn)n∈N] = [(y′n)n∈N].
Given  > 0, let N be such that for all n > N,
where Mx′ = 
 ||x′n||X < ∞. For n > N, we have
Passing the limit as n → ∞, we obtain 
·, ·  defines an inner product on X:
(IP1) If 
, then 
.
Let 
 be such that 
 = 0.
Then 
 xn, xn X = 
 ||xn||2X = 0.
(0)n∈N ∈ C and 
 ||xn – 0||X = 
 ||xn||X = 0 (using the above).
Thus [(xn)n∈N] = [(0)n∈N].
(IP2) For all x1, x2, y ∈ X,

For all α ∈ K and x, y ∈ X, we have
(IP3) For all x, y ∈ X, x, y X = 
.
ι is a linear transformation:
ι is injective:
If ι(x) = [(x)n∈N] = [(0)n∈N], then ||x|| = 
 ||x – 0|| = 0, and so x = 0.
ι preserves inner products: For x, y ∈ X, ι(x), ι(y) X = 
 x, y X = x, y X.
Solution to Exercise 4.8, page 168
As span{v1} = span{x1} = span{u1}, it follows that v1 = α1u1.
Thus 1 = ||v1|| = |α1|||u1|| = |α1| · 1 = |α1|.
For n > 1, vn ∈ span{v1, ···, vn} = span{x1, ···, xn} = span{u1, ···, un}.
So there are scalars β1, ···, βn–1, αn such that vn = β1u1 + ··· + βn–1un–1 + αnun.
We also know that for all k < n, vn, vk  = 0. So it follows that vn, v  = 0 for all v
∈ span{v1, ···, vn–1} = span{x1, ···, xn–1} = span{u1, ···, un–1}. Thus vn, uk  = 0
for all k < n. This gives β1 = ··· = βn–1 = 0, and vn = αnun. Moreover, 1 = ||vn|| = |
αn| ||un|| = |αn| · 1 = |αn|.
Solution to Exercise 4.9, page 171
Let us first note that the derivative of an even monomial t2k is odd, and that of an
odd monomial t2k+1 is even. From here it follows that the derivative of a
polynomial with only even monomials is a polynomial consisting of only odd
monomials, while that of a polynomial with only odd monomials is a polynomial
with only even monomials.
By the Binomial Theorem, we see that the polynomial (t2 – 1)n is the sum of
even monomials of the form ckt2k, for suitable scalars ck, k = 0, ···, n.
So 
 (t2 – 1)n will be a polynomial p with:
(1) only even monomials if n is even,

(2) only odd monomials if n is odd.
In the former case, when n is even, p, being the sum of even functions will be
even, while in the latter case, p, being the sum of odd functions, will be odd.
Thus Pn is even when n is even, and odd if n is odd.
If n is odd, then each of the terms ckt2k–n is an odd polynomial, and hence so is
their sum. Consequently, Pn is odd if n is odd.
We have Pn(–1) = (–1)nPn(1) = (–1)n · 1 = (–1)n for all n  0.
Solution to Exercise 4.10, page 171
With y(t) := (t2 – 1)n, we have y′(t) = n(t2 – 1)n–1 · 2t. So
By differentiating the left-hand side of (∗), we obtain
and by differentiating the right-hand side of (∗), we have
Equating the final expressions from the above calculations, we obtain
Multiplying by 
, we get (1 – t2)P″n(t) – 2tP″n(t) + n(n + 1)Pn(t) = 0.
Solution to Exercise 4.11, page 171
t2 – 1 is zero at ±1. By Rolle’s Theorem, it follows that (d/dt)(t2 – 1) is zero at
some t(1) ∈ (–1, 1). But we had seen that (d/dt)(t2 – 1) is also zero at the end
points ±1. So by Rolle’s Theorem applied to the function (d/dt)(t2 – 1) on the
two intervals [–1, t(1)] and [t(1), 1], we get the existence of points t1
(2) ∈ (–1, t(1))
and t2
(2) ∈ (t(1), 1), where (d/dt)2(t2 – 1) is zero. Proceeding in this manner, we get

the existence of points t1(n), ···, tn(n) ∈ (–1, 1) where (d/dt)n(t2 – 1)n vanishes. So
Pn has at least n zeros on (–1, 1). But Pn has degree n, and hence it can have at
most n zeros in C. This shows that all the zeros of Pn are real, and all of them lie
in the open interval (–1, 1).
Solution to Exercise 4.12, page 171
The set {eij : 1  i  m, 1  j  n}, where eij is the matrix with 1 in the ith row
and jth column, and all other entries 0, is a basis for Rm×n. To see that this basis
is in fact orthonormal, observe that the map ι : Rm×n → Rmn given by A = [aij] 
(a11, ···, a1n, a21, ···, a2n, ···, am1, ···, amn) (that is, lay out the rows of A next to
each other in one long row), is an isomorphism that preserves inner products:
{ι(eij) : 1  i  m, 1  j  n} is orthonormal, and so it follows that the set {eij : 1 
 i  m, 1  j  n} is orthonormal as well.
Solution to Exercise 4.13, page 172
(1) We have H0 = ex2e–x2 = 1. For n  0,
Thus if Hn is a polynomial, then 2xHn, H′n are polynomials too, and so is
Hn+1 = 2xHn – H′n. Since H0 = 1 is a nonzero polynomial of degree 0, it
follows by induction on n that each Hn, n  0 is a polynomial. Moreover, if
Hn has degree d, and its leading term is cdxd, then H′n has degree d – 1, while
2xHn has degree d + 1 with the leading term 2cdxd+1. Consequently, the
recurrence relation together with H0 = 1 also reveals that Hn has the leading

term 2nxn, and in particular has degree n.
Using the recursion relation, we get H1 = 2x, H2 = 4x2 – 2, H3 = 8x3 – 12x.
(2) Let m < n. Then we have
As (d/dx)n–1e–x2 is a sum of terms of the form ckxke–x2, and because Hm is a
polynomial, it follows that the first summand in the right-hand side is 0. 
So we have φm, φn  = (–1)n+1 
.
We can continue this process of integration by parts, until we arrive at
But as Hm has degree m < n, (d/dx)n Hm = 0, so that φm, φn  = 0.
The case m > n also follows from here, since the inner product is conjugate
symmetric. Finally,
(The last equality can be justified as follows. With I := 
, we have
So I = 
(3) For n  0, we have

(4) First let us note that if n  1, then we have
Hence for n  1,
(5) We have for all φ
Hence for all n  0,
(6) We have 
 and 
.
From the previous part, we have (–(d/dx)2 + x2)φn = (2n + 1)φn, giving
We have

In Schrödinger’s equation, a2 = 
, and so 
 = a(2n + 1).
So En = 
, for n  0.
Solution to Exercise 4.14, page 172
Since 
 diverges, 
 does not converge absolutely.
If sn is the nth partial sum of 
, then for n > m, we have
and this can be made as small as we please since 
.
Hence (sn)n∈N is Cauchy in H, and since H is a Hilbert space, it converges.
Solution to Exercise 4.15, page 172
For all N ∈ N, we have
Thus 
, and as N was arbitrary, 
.
Solution to Exercise 4.16, page 173
Let y ∈ Y ∩ Y⊥. As y ∈ Y⊥, we know that for all y′ ∈ Y , y, y′  = 0. Taking y′ :=
y ∈ Y, we obtain 0 = y, y′  = y, y  = ||y||2, and so ||y|| = 0, giving y = 0. So Y ∩

Y⊥ ⊂ {0}. Also, since Y, Y⊥ are subspaces, it follows that each contains the zero
vector 0. So Y ∩ Y⊥ = {0}.
Solution to Exercise 4.17, page 173
(1) If y ∈ Y, then for each x ∈ Y⊥, y, x  = x, y ∗ = 0, and so y ∈ (Y⊥)⊥. Thus Y
⊂ (Y⊥)⊥.
(2) Let x ∈ Z⊥. Then x, z  = 0 for all z ∈ Z. As Y ⊂ Z, we also have x, y  = 0
in particular for all y ∈ Y. Hence x ∈ Y⊥. This shows that Z⊥ ⊂ Y⊥.
(3) As Y ⊂ Y, it follows from part (2) that 
.
Now let x ∈ Y⊥. Then x, y  = 0 for all y ∈ Y.
If y′ ∈ Y, then there exists a sequence (yn)n∈N in Y such that 
 yn = y′.
Thus x, y′  = 
 = 0.
Hence 
, showing that 
 as well.
(4) Suppose that x ∈ Y⊥.
As Y is dense in X, there is a sequence (yn)n∈N in Y converging to x in X.
Thus x, x  = 
 x, yn  = 
 0 = 0.
(5) Suppose x = (xn)n∈N ∈ 
. Since e2n ∈ Yeven for each N, x2n = x, e2n  = 0.
Hence the subspace 
 ⊂ Yodd, where Yodd denotes the subspace of ℓ2 all
sequences whose evenly indexed terms are 0.
Vice versa, if x ∈ Yodd, it is clear that for all y ∈ Yeven, x, y  = 0. Thus Yodd
⊂ 
.
Consequently, 
 = Yodd.
Similarly, 
 = Yeven. And so, 
.
(6) We know that c00 is dense in ℓ2. (Just truncate the series to the desired
accuracy to get a finitely supported approximation!)
So 
. But then 
.
Solution to Exercise 4.18, page 176
Let 
.
Then E(m, b) = 
.

Thus the problem of finding the least square regression line is:
It follows from Theorem 4.5, page 174, that a minimiser Y∗ is given by
where {U1, U2} is any orthonormal basis for the subspace Y := span{Y1, Y2} of
Rn with the usual Euclidean inner product. By the Gram-Schmidt
Orthonormalisation Procedure, U1 = 
, and U2 = 
.
For the given data, using the above formulae, we obtain m = –0.3184 million
tonnes coal per °C, and b = 10.4667 million tonnes of coal. The y-intercept is b =
10.4667 million tonnes of coal, and this is the inland energy consumption when
the mean temperature is 0°C (that is when it is freezing!). The x-intercept is

10.4667/0.3184 = 32.8728, which is the mean temperature when the inland
consumption is 0 (that is, no heating required). The slope is m = –0.3184 million
tonnes of coal per °C. Thus for each °C drop in temperature, the inland energy
consumption increases by 0.3184 million tonnes of coal. Finally, the forecast of
the energy consumption for a month with mean temperature 9°C is given by y =
mx + b = (–0.3184)(9) + 10.4667 = 7.6011 million tonnes of coal.
Solution to Exercise 4.19, page 179
Let C := L2+(R). Then C is convex. Thus C is convex too. We will show that g∗
:= max{f, 0} ∈ L2
+(R) = C ⊂ C satisfies: for all g ∈ C, f – g∗, g – g∗  0.
We have f = max{f, 0} + min{f, 0}. So f – g∗ = min{f, 0}. Also,
Hence we obtain for all g ∈ C that
So for all g ∈ C, ||f – g∗||  ||f – g||. In particular, for all g ∈ L2
+(R) = C ⊂ C, we
also have ||f – g∗||  ||f – g||.
Solution to Exercise 4.20, page 182
We’d seen in Exercise 4.17, page 173, that 
. So 
, where
the last equality follows from Corollary 4.1, page 182, since Y is closed.
Solution to Exercise 4.21, page 182
For all f ∈ L2(R), it is easy to check that fe := (f + )/2 is even, and fo := (f – )/2
is odd. Thus for all g ∈ Y, we have

Thus, by Theorem 4.7, page 180, PYf = fe for all f ∈ L2(R).
By Theorem 4.8, page 180, we have
PY⊥ = I – PY, and so for all f ∈ L2(R), PY⊥f = f – 
.
We have f = If = PYf + PY⊥f = 
.
Moreover, by Theorem 4.8, this decomposition is unique.
Solution to Exercise 4.22, page 182
Y = ker(I – S), and so Y is a closed subspace of H.
For all x ∈ H, 
.
So 
 for all x ∈ H. Moreover, for all y ∈ Y, we have
Thus, by Theorem 4.7, page 180, PYx = 
 for all x ∈ H.
By Theorem 4.8, page 180, we have
Thus Z⊥ = (Y⊥)⊥ = Y.
PY⊥ = I – PY, and so for all x ∈ H, PY⊥x = x – 
Solution to Exercise 4.23, page 182
Consider the map 
, where 
 is the indicator function of 
.
As 
 < ∞, Mf ∈ L2(R).
It is also easy to see that M is linear. The above inequality then establishes that
M ∈ CL(L2(R)). We have

Thus YA is closed.
For f ∈ L2(R), 1Af ∈ YA, and moreover, for any g ∈ YA,
Thus PAf = 1Af for all f ∈ L2(R).
Solution to Exercise 4.24, page 182
Suppose that D⊥ = {0}. Then D = (D⊥)⊥ = {0}⊥ = H. So D is dense in H.
Now suppose that D is dense in H. Then D = H. Thus D⊥ = (D)⊥ = H⊥ = {0}.
Solution to Exercise 4.25, page 184
Let x ∈ C[–1, 1] and  > 0. By Weierstrass’s Approximation Theorem (Exercise
1.26, page 22), there is a polynomial p ∈ C[–1, 1] such that ||x – p||∞ < 
.
Then 
Hence ||x – p||2 < . Consequently the polynomials are dense in C[–1, 1] (with
the usual inner product).
Solution to Exercise 4.26, page 185
Moreover ι is continuous because ||ι(x)||2 = 
 for all x ∈ H.
If x ∈ H is such that ι(x) “ 0, then ||x|| = ||ι(x)|| = 0, and so x = 0.
Hence ι is injective.
If (cn)n∈N ∈ ℓ2, then x := 
 cnun ∈ H, and for all k ∈ N,

So ι(x) = (cn)n∈N, showing that ι is surjective too.
As ι ∈ CL(H, ℓ2) is a bijection, it has a continuous inverse ι–1 ∈ CL(ℓ2, H) (by
Corollary 2.4 on page 96). Moreover, ||ι(x)|| = ||x|| for all x ∈ H, and so ι is an
isometry.
Solution to Exercise 4.27, page 187
Let x ∈ C[0, 1] be the function t  t.
For n ≠ 0, we have x, Tn  = 
, using integration by parts.
Also x, T0  = 1/2. By Parseval’s Identity,
which yields 
Solution to Exercise 4.28, page 187
Let [(xn)n∈N] ∈ X. Consider the sequence (xn)n∈N in X. Since (xn)n∈N ∈ C, given
any  > 0, there exists an N ∈ N such that for all m, n > N, ||xn – xm|| < .
Consequently, for all m > N, ||ι(xm) – [(xn)n∈N]||X = 
 ||xm – xn||  .
Hence 
 ι(xn) = [(xn)n∈N].
Solution to Exercise 4.29, page 187
We have

with equality if and only if 
.
Thus the curve enclosing the maximum area is given by
with 
.
Let α ∈ [0, 2π) be such that cos α = 
 and sin α = 
. Then
Hence (x∗(s) – a0)2 + (y∗(s) – c0)2 = 
.
Consequently, s  (x∗(s), y∗(s)) : [0, L] → R2 is the parametric representation of
a circle with centre at (a0, c0) ∈ R2 and radius equal to 
.
Solution to Exercise 4.30, page 188
(1) Call un the nth vector in the list. If {un : n ∈ N} were an orthonormal basis,
then

a contradiction. So the given set is not an orthonormal basis.
(2) Let us call the evenly indexed vectors as vn, and the oddly indexed ones as
wn. Then clearly vi, vj  = wi, wj  = vi, wj  = 0 whenever i ≠ j, since there
are no overlapping nonzero terms. Also vi, wi  = 0.
Finally ||vi|| = ||wi|| = 1. This shows that the given set B is orthonormal. In
order to show density, we note that 
 and 
. Thus
span B = span{en : n ∈ N}, and the latter is dense in ℓ2.
Solution to Exercise 4.31, page 188
If X is a real vector space, then let KQ := Q, while if X is a complex vector
space, then let KQ := Q + iQ. Set
Then D is countable. Let x ∈ X, and  > 0.
Then there exists an N such that 
Let cn ∈ KQ, n = 1, ···, N, be such that 
.
Then with y := 
 cnun ∈ B, we have
Thus X is separable.
Solution to Exercise 4.32, page 188
We have for λ ≠ μ that
On the other hand, ||eiλx||2 = 1. Thus

Hence 
Suppose now that X is separable, with a dense subset D = {d1, d2, d3, ···}. Then
for each λ ∈ R, there exists a dλ ∈ D such that ||eiλx – dλ|| < 1/√2.
This gives us the existence6 of a map λ  dλ : R → D.
This map is injective since if λ ≠ μ, then
giving ||dλ − dμ|| > 0, and in particular dλ ≠ dμ.
But this is absurd, since R is uncountable, while D is countable!
So  is not separable.
Solution to Exercise 4.33, page 189
For n ∈ N, set 
If Un has more than n − 1 elements, then for any distinct ui1, · · · , uin ∈ Un,
(where the former inequality is by virtue of the fact that the uik ’s belong to Un,
and the latter is Bessel’s Inequality). So we obtain ||x||2 < ||x||2, which is absurd.
Thus Un has at most n − 1 elements. Hence each Un is finite. But
and as each Un is finite, their union U is at most countable.
Consequently, x, ui  is nonzero for at most a countable number of the ui ’s.
Solution to Exercise 4.34, page 190
(1) We have for all x ∈ H that |φy(x)| = | x, y |  ||x|| ||y||, and so ||φy||  ||y||.
If y = 0, then ||φy||  ||y|| = 0, and so ||φy|| = 0 = ||y||.
If y ≠ 0, then define z = 
, and observe that ||z|| = 1, so that

Hence it follows that ||φy|| = ||y||.
(2) Let y ∈ H\{0}. Then for x ∈ H,
and so φiy = −iφy.
Also ||φy|| = ||y|| ≠ 0, so that φy ≠ 0, the zero linear functional.
If the map η  φη : H → CL(H, C) were linear, then in particular, we would
have φiy = iφy, and from the above, we would then get iφy = −iφy, giving φy =
0, which is absurd.
Solution to Exercise 4.35, page 195
We will show that Y := ran P = ker(I − P), and since the kernel of the continuous
linear transformation I − P is closed, it follows that Y is closed.
That ran P = ker(I − P): If y ∈ ran P, then y = Px for some x ∈ H. Then
So y ∈ ker(I − P). Hence ran P ⊂ ker(I − P).
On the other hand, if y ∈ ker(I − P), then (I − P)y = 0 and so y = Py ∈ ran P.
Thus ker(I − P) ⊂ ran P as well.
It remains to show that P = PY. We will use (ran P)⊥ = ker(P∗) = ker P, where
the last equality follows thanks to the self-adjointness of P. Let x ∈ H. Then x =
PY x + PY ⊥ x. But PY ⊥ x ∈ Y⊥ = ker P, and so
As PY x ∈ Y = ran P, PY x = Px1 for some x1 ∈ H.
Thus P (PY x) = P (Px1) = P2 x1 = Px1 = PY x. Hence Px = P (PY x) = PY x.
Solution to Exercise 4.36, page 195

and so T1 is self-adjoint, while T2 is skew-adjoint.
Moreover, 
In order to show uniqueness, suppose that T′1, T′2 are self-adjoint and skew-adjoint
respectively such that T = T′1 + T′2. Then T1 + T2 = T′1 + T′2, and so we obtain T1 −
T′1 = T′2 − T2. As the left-hand side is self-adjoint, and the right-hand side is skew-
adjoint, both sides must be zero. (Indeed, if S := T1 − T′1 = T′2 − T2 is the common
value, then S = S∗ = −S, and so 2S = 0, that is, S = 0.)
Solution to Exercise 4.37, page 195
. Define T : ℓ2 → ℓ2 by Tk = 
.
Then T is well-defined and T ∈ CL(ℓ2). We will show that Λ∗ = T.
For all h = (hn)n∈N and k = (kn)n∈N in ℓ2, we have
Thus Λ∗ = T.
Solution to Exercise 4.38, page 195
We’ll show that I∗ is given by 
I∗ ∈ CL(L2 [0, 1)] by Example 2.10 (page 70), with 
For h, k ∈ L2 [0, 1], we have
and so I∗ ∈ CL(L2 [0, 1]) is given by 
Solution to Exercise 4.39, page 195

T*A = TA∗, where 
Thus T∗A is clockwise rotation through an angle θ in the plane.
Solution to Exercise 4.40, page 196
For x ∈ H, we have 
(Note that x′ ∈ Y⊥
n because x′, ui  = 0 for all i = 1, · · · ,n.)
So 
 for all x ∈ H. For all x ∈ H, we have
since 
Solution to Exercise 4.41, page 196
(1) If B′ = {u′n: n ∈ N} is another orthonormal basis, then
On the other hand, we also have
and so 
(2) We will verify simultaneously the norm and subspace axioms:
(N1/S3) For all T ∈ S2 (H) that 
Now let T ∈ S2 (H) and ||T||HS = 0. Then 
So Tun = 0 for all n. But then for all x ∈ H, we have

Consequently T = 0.
Clearly 0 ∈ S2 (H) since ||0||HS = 0 < ∞.
(N2/S2) For all T ∈ S2 (H) and α ∈ K, we have
and so ||α · T||HS = |α| ||T||HS.
Note that we’ve also shown for all T ∈ S2 (H), α ∈ K, that α · T ∈ S2
(H).
(N3/S1) Finally, if T1, T2 ∈ S2 (H), then we have
and so ||T1 + T2||HS  ||T1||HS + ||T2||HS.
Also, this shows that for all T1, T2 ∈ S2 (H), T1 + T2 ∈ S2(H).
(3) We have for all x ∈ H that
and so ||T || = ||T∗||  ||T||HS.
Solution to Exercise 4.42, page 197
As CL(H) is an algebra, Λ(T) ∈ CL(H). We verify linearity:
(L1) For T1, T2 ∈ CL(H),

(L2) Λ(αT) = A∗ (αT)+(αT)A = α(A∗ T + T A) = αΛT, T ∈ CL(H), α ∈ K.
Continuity: For T ∈ CL(H),
and so Λ ∈ CL(CL(H)).
If T ∈ CL(H) is such that T = T∗, then
So Λ(T) is self-adjoint.
Solution to Exercise 4.43, page 197
Let (Tn)n∈N be a sequence of self-adjoint operators in CL(H) that converges to T
∈ CL(H). We’d like to show that for all x, y ∈ H, T x, y  = x, Ty . As we have
||Tnx − T x||  ||Tn − T || ||x||, it follows that (Tnx)n∈N converges to Tx, and
similarly, (Tny)n∈N converges to Ty. Thus
Solution to Exercise 4.44, page 197
Let μ ∈ ρ(T). Then there is an S ∈ CL(H) such that S(μI − T) = I = (μI − T)S.
Taking adjoints, we obtain
Thus μ∗ I − T∗ is invertible in CL(H), and so μ∗ ∈ ρ(T∗).
So we have proved that 
 for all T ∈ CL(H).
Applying this to T∗ instead of T gives:

Consequently for all T ∈ CL(H), μ ∈ ρ(T) if and only if μ∗ ∈ ρ(T∗).
We had seen that R = L∗ and that σ(L) = {z ∈ C : |z|  1}.
From the above, we obtain σ(R) = C\ρ(R) = C\(ρ(L))∗ = C\ρ(L) = σ(L).
Consequently the spectrum of R is the same as that of L, namely the closed unit
disc {z ∈ C : |z|  1} in the complex plane.
Solution to Exercise 4.45, page 197
We have for λ ∉ {0, 1},
and similarly 
The previous part shows that σ(PY) ⊂ {0, 1}.
We now show that both 0 and 1 are eigenvalues, so that σ(PY) = σp(PY) = {0, 1}.
As Y is a proper subspace, Y ≠ {0}. So there exist nonzero vectors y in Y, and all
of these are eigenvectors of PY with eigenvalue 1: PY y = y = 1 · y.
Also, as Y is a proper subspace, Y ≠ H.
If Y⊥ = {0}, then we have that Y = (Y⊥ qK = {0}⊥ = H, a contradiction.
Thus Y⊥ ≠ {0}. But this means that there exist nonzero vectors x in Y⊥.
All of these are eigenvectors of PY with eigenvalue 0, since PY x = 0 = 0 · x.
Solution to Exercise 4.46, page 197
Let λ ∈ σp (U) with eigenvector v ≠ 0.
Then Uv = λv, and so |λ|2 ||v||2 = λv, λv  = Uv, Uv  = U* Uv, v  = Ivv  = ||v||2.
Thus |λ| = 1, that is, λ lies on the unit circle with centre 0 in the complex plane.
If v1, v2 ∈ H\{0} are eigenvectors of U corresponding to distinct eigenvalues λ1,
λ2, then we have

and so v1, v2  = 0.
Solution to Exercise 4.47, page 199
The spectrum of T is real, and hence T + iI is invertible in CL(H). Since (T + iI)
(T − iI) = T2 + I = (T − iI)(T + iI), it follows by pre- and post-multiplying with (T
+ iI)−1 that (T − iI)(T + iI)−1 = (T + iI)−1(T − iI) =: U. Hence we have
So
Thus U is unitary. We have
Hence I − U is invertible in CL(H) with inverse 
. Similarly,
So 
Solution to Exercise 4.48, page 199
(1) Suppose that PY  PZ. If y ∈ Y, then
So PZ⊥ y = 0, giving y = PZ y + PZ⊥ y = PZ y + 0 = PZ y ∈ Z. Thus Y ⊂ Z.
(2) Now let Y ⊂ Z and x ∈ H. We have PZ x = PY x + (PZ x − PY x).

We first show that PZx − PYx is perpendicular to PYx.
As x = PY x + PY⊥x = PZ x + PZ⊥x, we have PZ x − PY x = PY⊥x − PZ⊥x.
So PY x, PZ x − PY x  = PY x, PY⊥x − PZ⊥x  = 
 = 0.
Hence
Consequently, PY  PZ.
Solution to Exercise 4.49, page 204
(1) By the Fundamental Theorem of Calculus, 
So 
.
As f (x)  0 for all x, we must have that L  0. Suppose that L > 0.
Then there exists an R > 0 such that for all x > R, L −f (x)  |f (x) − L| < ,
and in particular, f (x) >  for all x > R. Hence for all x > R,
which is absurd. Hence L = 0.
(2) We apply part (1) with f (x) := |Ψ(x)|2.
We note that f′ = (|Ψ|2)′ = (ΨΨ*)′ = Ψ′Ψ* + Ψ(Ψ′)*, and so |f′|  2||Ψ|| ||Ψ′||.
Thus 
So 
To show that 
, we apply the above to x 
 Ψ(−x), and note that if
Ψ, Ψ′ ∈ L2 (R), then so do Ψ(−·), (Ψ(−·))′ = −Ψ′(−·).
Solution to Exercise 4.50, page 205
We have for self-adjoint A, B that
[A, B]∗ = (AB − BA)∗ = B∗ A∗ − A∗B∗ = BA − AB = −(AB − BA) = −[A, B].
Solution to Exercise 4.51, page 205
We have

Similarly, Hence
Hence
Solution to Exercise 4.52, page 205
If n = 1, then [Q, P] = −[P, Q] = −(−i I) = i 1Q1−1, and so the claim is true. If
[Qn, P] = i nQn−1 for some n ∈ N, then we have
and so the claim follows for all n ∈ N by induction.
Solution to Exercise 4.53, page 207
We have in the classical case that
Thus {Q2, P2} = 4QP.
In the quantum mechanical case, we have, using Exercise 4.52, page 205, that
Thus 
 (since otherwise QP = PQ, which is false since
[Q, P] = i I ≠ 0).
QP is not self-adjoint, since if it were self-adjoint, then for all compactly

supported Ψ and Φ, we would have
which would give i Φ = [Q, P] Φ = 0, which is clearly false for nonzero Φ! On
the other hand, for all Ψ and Φ, we have
Solution to Exercise 4.54, page 207
We have
and so t  ||ψ(t)||2 is constant, giving ||ψ(t)||2 = ||ψ(0)||2 = 1.
Solution to Exercise 4.55, page 207
As V ≡ 0 for x ∈ (0, π), we have 
, that is, 
.
Depending on the sign of E, the solution is given by
If E = 0, then the conditions X(0) = X(π) = 0 give A = B = 0. So X ≡ 0.
If E < 0, then the conditions X(0) = X(π) imply that A = B = 0 so that X ≡ 0.
So only the case E > 0 remains. The condition X(0) = 0 gives A = 0.
The condition X(π) = 0 implies B sin 
As we want nontrivial solutions, we know B ≠ 0 (otherwise X ≡ 0).
So sin 
, giving 
Thus 
 (discrete/“quantised” energy levels!).

We have |Ψ(x, t)| = |X(x)||T (t)| = |X(x)| · |C| = |C| · |B| · | sin(nx)|.
The plots of |Ψ|2 = constant · sin(nx))2 when n = 1, 2 are shown below.
When n = 1, the probability is
When n = 2, the probability is
Solutions to the exercises from Chapter 5
Solution to Exercise 5.1, page 215
(1) Tm is linear:
(L1) For all x1, x2 ∈ H,
(L2) For all x ∈ H and α ∈ K,
So Tm is a linear transformation. Next we prove continuity: for all x ∈ H,

Conclusion: Tm ∈ CL(H).
For x ∈ H we have
(2) As 
, we have 
Thus (Tm)m∈N converges to T in CL(H). Since the range of Tm is contained in
the span of Tu1, · · ·, Tum, it follows that Tm has finite rank, and so Tm is
compact. As T is the limit in CL(H) of a sequence of compact operators, it
follows that T is compact.
Solution to Exercise 5.2, page 216
(1) (L1) For x1, x2 ∈ H, we have
(L2) For α ∈ K and x ∈ H, we have
Continuity: For x ∈ H, we have

So x0  y0 ∈ CL(H), and ||x0  y0||  ||x0 || ||y0||.
(2) As ran(x0  y0) ⊂ span{x0}, we have that x0  y0 has finite rank, and so it is
compact.
(3) For all x ∈ H,
Since this is true for all x ∈ H, we conclude that A(x0 y0)B = (Ax0) (B∗ y0).
Solution to Exercise 5.3, page 217
(1) Let H = ℓ2, and T be diagonal with 2 × 2 nilpotent blocks 
More explicitly, T (a1, a2, a3, a4, a5, a6, · · ·) = (a2, 0, a4, 0, a6, 0, · · ·), for
all (an)n∈N ∈ ℓ2. Thus T ∈ CL(ℓ2). Also, T2 = 0 is compact.
But if we take the bounded sequence (e2n)n∈N, then (Te2n)n∈N = (e2n−1)n∈N,
and this has no convergent subsequence. Hence T is not compact.
(2) Suppose that (xn)n∈N is a bounded sequence in H, and ||xn||  M for all n.
Since T2 is compact, (T2xn)n∈N has a convergent subsequence, say (T2
xnk)k∈N. We will show that (T xnk)k∈N is also convergent, by showing that it is
Cauchy. We have for j, k that
and so (T xnk)k∈N is Cauchy. As H is a Hilbert space, it follows that (T xnk)k∈N
is convergent. Hence T is compact.
Solution to Exercise 5.4, page 217
(1) True.
(2) False.
Neither I nor −I is compact, but their sum is 0, which is compact.
(3) True.
(4) False.

See the example in the solution to Exercise 5.3, part (1), page 217.
Alternately, we could take two diagonal operators on ℓ2 corresponding to the
sequences (1, 0, 1, 0, 1, 0, · · ·) and (0, 1, 0, 1, 0, 1, · · ·).
Solution to Exercise 5.5, page 217
If T ∈ K(H), then as A∗ ∈ CL(H), we have A∗T ∈ K(H). Also, TA ∈ K(H)
because T ∈ K(H) and A ∈ CL(H). Since A∗T and TA are in K(H), also their sum
A∗T + TA ∈ K(H), that is, Λ(T) ∈ K(H). Thus K(H) is Λ-invariant.
Solution to Exercise 5.6, page 226
We have ker T = {0}. So ran T = (ker T∗)⊥ = (ker T)⊥ = {0}⊥ = H.
So T has infinite rank. Let x ∈ H = ranT, and  > 0. Then there exists a y ∈ ran T,
such that ||x − y|| < /2.
As y ∈ ran T, we have y = T x′, for some x′ ∈ H, and 
So there exists an N Such that with
we have ||y − z|| < /2. Consequently, ||x − z||  ||x − y|| + ||y − z|| < /2 + /2 = ,
and so span{un : n ∈ N} is dense in H. Since {un : n ∈ N} is also an orthonormal
set, it follows that it is an orthonormal basis for H.
Solution to Exercise 5.7, page 226
We note that each eigenvalue λ of T is nonnegative because if u is a
corresponding unit-norm eigenvector, then λ = λ · 1 = λ u, u  = T u, u   0. By
the spectral theorem, we know that there exists a sequence of orthonormal
eigenvectors u1, u2, u3, · · · of T with corresponding eigenvalues λ1  λ2  λ3  ·
· ·  0.
We will show that for all x ∈ H, 
 converges in H.
For N > M, we have,

In the above, we have used Bessel’s Inequality to get the last inequality.
Hence 
 is Cauchy in H. As H is a Hilbert space,
converges in H. Consequently 
x is well-defined for all x ∈ H.
Also, it is easy to see that 
 is a linear transformation.
Continuity: For all N ∈ N,
Passing the limit N → ∞, we obtain ||
x||2  λ1 ||x||2, and so 
 ∈ CL(H).
We have for all x ∈ H that
So (
)2 = T.
Solutions to the exercises from Chapter 6
Solution to Exercise 6.1, page 232
(1) Since 
 exists, given an  > 0, there exists a δ > 0 such that
whenever 0 < |h| < δ. Consider the interval [0, h] for some h which satisfies 0
< h < δ. Since f is differentiable in (0, h) and continuous on [0, h], it follows
from the Mean Value Theorem that

Thus |θh| < δ and so 
So for all h ∈ (0, δ), 
Applying the Mean Value Theorem on [−h, 0], where 0 < h < δ, we also get
for all h ∈ (−δ, 0). Consequently, for all h satisfying 0 < |h| < δ, we have
that is, f is differentiable at 0, and 
shows that f′ is continuous at 0. It was given that f′ is also continuous on R∗.
So f is continuously differentiable on R.
(2) Applying the result from part (1) above, to the function f(n−1) : R → R, we
obtain that f(n−1) is continuously differentiable on R, that is, f is n times
continuously differentiable on R.
(3) We’ll show that for x > 0, 
 where pk is a polynomial.
This holds for k = 1: f(x) = e−1/x for x > 0, and so 
If the claim holds for some k, then
where 
 is a polynomial.
Now e1/x e−1/x = 1, and since we have 
 it follows that 
 for x > 0. So 0 < x−2n e−1/x < (2n + 1)!x for x > 0. Thus 
 Consequently
By the previous part, it follows that f ∈ C∞(R).

Solution to Exercise 6.2, page 232
The equation 
 says that u is constant along the lines parallel to the x-axis.
So for each fixed y, there is a number Cy such that u(x, y) = Cy for all x ∈ R. But
u ∈ D(R2) must have compact support, and so it is zero outside a ball B(0, R)
with a large enough radius R. So Cy is forced to be 0 for all y! Hence u ≡ 0 is the
only solution.
Solution to Exercise 6.3, page 232
It is clear that if Φ ∈ D(R), then Φ′ ∈ D(R). Moreover,
So we have 
Now suppose that φ ∈ D(R) is such that 
Define Φ by 
 for x ∈ R. Then Φ′ = φ, and so Φ ∈ C∞.
If a > 0 is such that φ is zero outside [−a, a], then we have for x < −a that
On the other hand, for 
So φ also vanishes outside [−a, a], and hence Φ ∈ D(R).
Finally, let φ ∈ Y, and suppose that Φ1, Φ2 ∈ D(R) are such that 
Then (Φ1 − Φ2)′ = 0, and so Φ1 − Φ2 = C, where C is a constant. But as Φ1, Φ2
both have compact supports, it follows that C must be zero. Hence Φ1 = Φ2.
Solution to Exercise 6.4, page 233
From the solution to Exercise 6.3, page 232, we know that the Φns are given by

As 
 there is some a > 0 such that all the φn vanish olutside [−a, a].
Then it follows that each Φn also vanishes outside [−a, a]. Also,
Hence it follows that (Φn)n∈N converges uniformly to 0 as n → ∞. Since 
it follows that 
 for k  1. Thus for each k  1, we have that 
converges uniformly to 0 (thanks to the fact that 
 This completes the
proof that 
Solution to Exercise 6.5, page 236
Suppose that such a function δ exists. Let 
For n ∈ N, and let φn : R → R be defined by φn(x) := φ(nx), x ∈ R.
Then φn is smooth, takes values in [0, 1], and vanishes outside [−1/n, 1/n]. So we
have
a contradiction.
Solution to Exercise 6.6, page 237
(1) For all φ ∈ D(R), there exists an N ∈ N such that φ = 0 on R\[−n, n].
So the sum in the definition of T, φ  is finite: 
Hence T, φ  is well defined for each φ ∈ D(R). The linearity is obvious.
Now suppose that 
 Then there exists an K ∈ N such that each φn,
vanishes outside [−K, K]. Also, for all |k|  K, 
Thus 
 and so T ∈ D′(R).
(2) Take any φ ∈ D(R) that is positive in (0, 1q and zero outside [0, 1].

(From Example 6.1, page 230, there is a ψ ∈ D(R) that is positive on (−1, 1)
and zero outside [−1, 1]. By shifting and scaling, we see that the function φ
defined by φ(x) := ψ(2x − 1), x ∈ R, is one such function.)
Now define φn ∈ D(R), n ∈ N, by 
We have for k ∈ N that 
Thus for all 
Hence for all k  0, we have 
 uniformly. However, we have
(3) There is no contradiction to our conclusion from (1) that T is a distribution,
since we observe that there is no compact set K ⊂ R such that for all n ∈ N,
φn is zero outside K: Indeed, 
Solution to Exercise 6.7, page 241
The function 
 is continuously differentiable on R\{0}, and has a
jump f(0+) − f(0−) = 1 at 0. For x < 0, H(x) = 0 and so (H(x) cos x)′ = 0.
For x > 0, H(x) = 1, and so (H(x) cos x)′ = (cos x)′ = − sin x.
Moreover, 
Consequently, 
The function 
 is continuously differentiable on R\{0}, and has a
jump g(0+) − g(0−) = 0 at 0. For x < 0, H(x) = 0 and so (H(x) sin x)′ = 0.
For x > 0, H(x) = 1, and so (H(x) sin x)′ = (sin x)′ = cos x.
Moreover, 
Consequently, 
Solution to Exercise 6.8, page 241
The function 
 is continuously differentiable on R\{0}, and has a jump
f(0+) − f(0−) = 0 at 0. Moreover, for x > 0, |x|/2 = x/2, and so we have (|x|/2)′ =
(x/2)′ = 1/2 for x > 0. On the other hand, for x < 0, |x|/2 = −x/2, and so we obtain
(|x|/2)′ = (−x/2)′ = −1/2 for x < 0.
Also, 

Hence 
 where 
Again, g is continuously differentiable on R\{0}.
g has a jump of g(0+) − g(0−) =  − (− ) = 1 at 0.
Also g is constant for x > 0 (respectively for x < 0), and so g′(x) = 0 for x > 0
(respectively for x < 0).
Also, 
Hence 
Solution to Exercise 6.9, page 241
(1) Let us first consider the case when ℓ ≡ 0.
Then V = ker ℓ ⊂ ker L implies that ker L = V too, and so L = 0 as well.
Thus we may simply take c = 0, and then clearly L = 0 = 0ℓ is valid.
Now let us suppose that ℓ ≠ 0.
Then there is a vector v0 ∈ V such that ℓ(v0) ≠ 0.
This vector v0 must be nonzero, for otherwise ℓ(v0) = 0.
(To show the desired decomposition of an arbitrary vector as v = cvv0 + w,
with w ∈ ker ℓ, we need to find the appropriate scalar cv, because then we
can set w := v − cvv0. To find what cv might work, we apply ℓ on both sides
to obtain ℓ(v) = cvℓ(v0) + ℓ(w) = cvℓ(v0) + 0 = cvℓ(v0).
So it seems that 
 should do the trick!)
Given v ∈ V, we now proceed to show that 
We have 
 and so w ∈ ker ℓ.
As w ∈ ker ℓ ⊂ ker L, we have L(w) = 0, and
Hence with 
 we have L = cℓ.
(2) For φ ∈ D(R), 0 = 0, φ  = T′, φ  = −T, φ . So {φ′ : φ ∈ D(R)} ⊂ ker T.
Let 1 denote the constant function R ∋ x  1. By Exercise 6.3, page 232

Finally, by part (1), applied to the vector space V = D(R), with L := T and ℓ
:= T1, we get the existence of a c ∈ C so that T = cT1 = Tc.
(Here Tc denotes the regular distribution corresponding to the constant
function taking value c everywhere on R.)
Solution to Exercise 6.10, page 242
Fix any φ0 ∈ D(R)\{0} which is nonnegative everywhere. For ψ ∈ D(R), set
As ψ and φ0 belong to D(R), so does φ. Moreover,
Thus 
By Exercise 6.3, page 232, there is a unique Φ ∈ D(R) such that Φ′ = φ.
We define S : D(R) → C by S, ψ  = −T, Φ . Let us check that S is linear.
Let ψ1, ψ2 ∈ D(R), and let Φ1, Φ2 ∈ D(R) be such that
Then 
So 
Similarly, S, αψ  = α S, ψ  for all ψ ∈ D(R) and all α ∈ C.
Now we check the continuity of S. Let (ψn)n∈N be a sequence in D(R) such that 
. Then there exists an a > 0 such that all the ψn vanish outside [−a, a], and
(ψn)n∈N converges uniformly to 0 as n → ∞, giving

Now set 
Then there exists a b > 0 such that each φn vanishes outside [−b, b].
Also, for k  0, 
So for each k  0, 
 converges uniformly to 0. Thus 
Let Φn be the unique element in D(R) such that 
From Exercise 6.4, page 233, we can conclude that 
Consequently, S, ψn  = −T, Φn  → 0 as n → ∞. Hence S ∈ D′(R).
Finally, we’ll show that S′ = T.
If Φ ∈ D(R), then 
Thus 
Solution to Exercise 6.11, page 242
Let φ ∈ D(R) be such that φ(0) ≠ 0. (For example we can simply take the test
function from Example 6.1, page 230.) Then xn φ ∈ D(R) too, and we have
So δ(n) ≠ 0.
Solution to Exercise 6.12, page 242
It is enough to show the linear independence of δ, δ′, · · ·, δ(n) for each n.
Suppose that there are scalars c0, c1, · · ·, cn such that 
 Let
φ ∈ D(R), and for λ > 0, set φλ(x) := φ(λx), for all x ∈ R. Then
The polynomial 
 is zero on {λ : λ > 0},

and hence must be identically zero. So c0φ(0) = · · · = cn φ(n)(0) = 0. As the
choice of φ was arbitrary, we have that for all test functions φ ∈ D(R),
But if we look at the φ from Example 6.1, page 230, then φ(0) ≠ 0, and also xnφ,
n ∈ N, belongs to D(R), which moreover satisfies
So using φ, xφ, · · ·, xnφ as the test functions in (∗), we obtain c0 = · · · = cn = 0.
Solution to Exercise 6.13, page 242
For any φ ∈ D(Rd), we have
So 
Solution to Exercise 6.14, page 242
 and so it defines a regular distribution on R2.
For φ ∈ D(R2), with a > 0 such that φ ≡ 0 on R2\(−a, a)2, we have

Thus 
Solution to Exercise 6.15, page 242
If u : R2 → R is a radial function, say u(x) = f(r), where r = ||x||2, then
Thus 
 Since for all R > 0 we have
we conclude that 
For φ ∈ D(R2) which vanishes outside the ball B(0, R), we have
(log r)(Δφ) is integrable, as logr is locally integrable, and Δφ = 0 outside a ball.
Let  > 0.
Using Green’s formula in the annulus Ω := {x ∈ R2 :  < ||x||2 < R} (with the
boundary ∂Ω being the union of the two circles S( ) = {x : ||x||2 = } and S(R) =
{x : ||x||2 = R}), for the functions u = log r and v = φ, we obtain
We’ll show below that the first integral on the right-hand side is O( ), and thus it

tends to 0 as  → 0.
As 
 and ||n(x)||2 = 1, the Cauchy-Schwarz Inequality gives
where 
 Finally, 
Next we will look at the second integral 
First, 
 Moreover,
Given η > 0,
where first 0 > 0 is chosen small enough so that |φ(x) − φ(0)|  η if ||x||2  0,
and  satisfies 0 <   0.
Thus 
So 
 Hence 
Solution to Exercise 6.16, page 248
u is continuous on R, and continuously differentiable on R\{0}.
For x < 0, we have u′(x) = 0. For x > 0, u′(x) = 1.
Also, 
Thus by the Jump Rule, 
 in the sense of distributions.
So u is a weak solution of u′ = H.
Solution to Exercise 6.17, page 251
We view H(x) cosx as the product of the C∞ function cos with the regular
distribution H. Using the Product Rule, we have

Similarly,
Solution to Exercise 6.18, page 251
(1) We have
Hence 
(2) When 
If the claim is true for some n ∈ N, then
So the claim follows for all n ∈ N by induction.
(3) We have
Thus
Consequently, 
Solution to Exercise 6.19, page 252

We have for all φ ∈ D(R) that
So αδ′ = α(0)δ′ − α′(0)δ. In particular, xδ′ = 0δ′ − 1δ = −δ.
Solution to Exercise 6.20, page 252
For all φ ∈ D(R), we have that
So 
Solution to Exercise 6.21, page 252
With u := e−3yxH(y), we have
For all φ ∈ D(R2), we have
So 
 Hence 
Moreover, u(0, y) = e−0 H(y) = 1 · H(y) = H(y).
Solution to Exercise 6.22, page 252

First we note that for all φ ∈ D(R), we have
where 1 is the constant function R ∋ x  1.
Suppose on contrary, it is possible to define an associative and commutative
product such that for α ∈ C∞(R) and T ∈ D′(R), it agrees with Definition 6.6,
page 249. Then
whereas
and so 
 violating associativity.
Solution to Exercise 6.23, page 252
(1) Let 
 Then we have
From Exercise 6.9, page 241, there exists a c ∈ C such that e−λxT = c, that is,
T = ceλx.
(2) Since f ∈ C∞, there exists an F ∈ C∞ such that 
(In fact, an explicit expression for one such F (for which F(0) = 0), is given
by 
 This can be checked by differentiation using the
Product Rule and the Fundamental Theorem of Calculus.)

Hence we obtain 
From part (1), T − F = ceλx for some c ∈ C. Hence T = F + ceλx ∈ C∞.
(3) Let 
 with an ≠ 0.
Then 
So P(ξ) = (ξ − λ)Q(ξ), where λ = λn, and a suitable polynomial Q.
Correspondingly, with 
We’ll use induction (on the order n of D) to prove
This is true for n = 1, from part (2) above.
Suppose that the claim is true for all differential operators of order n.
Let D have order n + 1, and write 
 where D1 is order n.
If DT = f ∈ C∞, then 
 and so D1T = Tg for some g ∈ C∞.
But by the induction hypothesis, it now follows that T = TF, with F ∈ C∞.
(4) If E is also a fundamental solution, then DE = δ.
But also DE∗ = δ, and so D(E − E∗) = 0.
Thus E − E∗ = F, where F is a classical solution of the homogeneous
equation DF = 0. So E = E∗ + F.
Conversely, if F is a classical solution of the homogeneous equation DF = 0,
then E := E∗ + F is a fundamental solution of D too: indeed, we have that
DE = DE∗ + DF = δ + 0 = δ.
So we conclude that: E ∈ D′(R) satisfies DE = δ if and only if
Solution to Exercise 6.24, page 252
If T = cδ, where c ∈ C, then clearly xT = x(cδ) = 0.
Now suppose that T ∈ D′(R) is such that xT = 0.

This means that for all φ ∈ D(R), we have 0 = xT, φ  = T, xφ .
Hence {xφ : φ ∈ D(R)} ⊂ ker T. We will now identify the set on the left-hand
side as ker δ = {ψ ∈ D(R) : ψ(0) = 0}, and then use part (1) of Exercise 6.9, page
241.
First, let us note that if ψ = xφ, where φ ∈ D(R), then ψ ∈ D(R), and moreover,
ψ(0) = 0φ(0) = 0. So we have {xφ : φ ∈ D(R)} ⊂ {ψ ∈ D(R) : ψ(0) = 0}.
Next, let us show the reverse inclusion. Let ψ ∈ D(R) be such that ψ(0) = 0.
We have, by the Fundamental Theorem of Calculus:
Set 
 Then ψ(x) = xφ(x).
By differentiating under the integral sign we see that φ ∈ C∞.
If ψ is zero outside [−a, a] for some a > 0, then as 
it follows that φ also vanishes outside [−a, a]. Thus φ ∈ D(R).
So we have {ψ ∈ D(R) : ψ(0) = 0} ⊂ {xφ : φ ∈ D(R)} as well.
Thus ker δ = {xφ : φ ∈ D(R)} ⊂ ker T, and by part (1) of Exercise 6.9, page 241
there exists a c ∈ C such that T = cδ.
Solution to Exercise 6.25, page 254
First, we prove by induction that 
 where pn is a polynomial.
This is indeed true for n = 0 and 
If it is true for some n, then
where 
 is a polynomial.

This finishes the proof of our claim.
Now to show e−x2 ∈ S(R), it is enough to show that 
 for all
nonnegative integers ℓ. For ℓ = 0, this is clear since |e−x2|  1 for all x ∈ R.
We have 
 and so for 
Since 
 is a continuous function, there is an M > 0 such that 
for x ∈ [−1, 1]. Consequently, 
Solution to Exercise 6.26, page 254
Since 
 we know that there exists an a > 0 such that all the φn vanish
outside [−a, a], and moreover, φn and all its derivatives converge uniformly to 0
on [−a, a]. So for any nonnegative integers m, k, we have that
So 
Solution to Exercise 6.27, page 255
We have for φ ∈ S(R) that
From here it follows that if (φn)n∈N is a sequence in S(R) such that 
 as n
→ ∞, then Tf, φn  → 0. Thus Tf ∈ S′(R).
Solution to Exercise 6.28, page 256
For φ ∈ S(R), we have that

Note that in the last step, we have used the fact that the Fourier transform of an L
′(R) function is bounded on R, and hence it defines a tempered distribution.
1 See for example [Sasane (2015), §2.4].
2 See for example [Sasane (2015), Chapter 6].
3 The symbol ¬ stands for “negation”. It is read as: “It is not the case that · · ·”.
4 See for example [Sasane (2015), page 311].
5 
 uj = 0, unless i = i, in which case 
 ui = 1. Here ·  denotes transpose.
6 By the Axiom of Choice!

The Lebesgue integral
In this appendix, we give a summary of the Lebesgue integral in one dimension,
to give the reader some feeling for some of the examples we have treated in the
book. This is of course no substitute for a thorough exposition to the subject,
which can be found for example [Smith (1983)] or [Apostol (1974)]. We will
exclusively work in one dimension, that is in R, although one can more
generally work in Rd in an analogous manner. Also, we will just work with real-
valued functions, instead of complex-valued functions. Again, by decomposing a
complex-valued function into its real and imaginary parts, one can carry over the
definitions and results to this more general setting.
Measurable sets
The length of an interval I ⊂ R is defined to be
We shall now associate a “measure” to more general subsets of R following a
method originally due to Henri Lebesgue (1875–1941). The more general sets
which possess a measure will be called measurable sets, and we will denote the
measure of a measurable set A ⊂ R by λ(A). The associated integral, which we
will define in the next section is called the Lebesgue integral.
Step 1: Compact sets. Let K ⊂ R be a compact set, that is, closed and bounded.
Let K be covered by intervals I1, ··· , In ⊂ R, n ∈ N. Then we expect λ(K) to
satisfy

This should hold for every such cover of K, and we expect the right-hand side
above to be close to the left-hand side when the “overlap” of the covering
intervals becomes smaller. This motivates the following definition. We define
where the infimum is taken over all covers of K by a finite number of intervals
Ik’s. We note that if K = [a, b], then our definition above delivers λ(K) = b − a,
which is indeed the length of the interval [a, b]. Also, we note that λ(K) < ∞ for
compact K.
Step 2: Open sets. The measure of an open set U ⊂ R is defined by
For open sets U, 0  λ(U)  ∞. Also, if U = (a, b), then λ(U) = b − a for finite a,
b, and is ∞ if a = ∞ or b = ∞.
Step 3: Bounded measurable sets. Let A ⊂ R be a bounded set. Consider all
compact sets K ⊂ A and all open sets U ⊃ A. Then we have λ(K) ⊂ λ(U). Thus
We say that the bounded set A is measurable if there is equality above, and
define its measure λ(A) to be the common value, that is,
If A is compact, then this definition coincides with the ones from Step 1. Also, if
A is open and bounded, then this definition coincides with the one from Step 2. It
can be shown (invoking Zorn’s Lemma) that there exist bounded subsets A ⊂ R
that are not measurable; see [Apostol (1974), Exercise 10.36, page 304].
Step 4: Measurable sets. Let A ⊂ R be an arbitrary set. We call A measurable if
for every compact set K⊂ R, the bounded set A ∩ K is measurable, and we
define the measure λ(A) of A by

If A is bounded, then this definition coincides with the one from Step 3.
This is how the (Lebesgue) measure λ(A) is defined for (Lebesgue) measurable
subsets A of R. The following result can be shown.
Theorem 7.1. (Properties of measurable sets).
(1) If A is measurable, then R\A is also measurable.
(2) Let A be measurable and x ∈ R. Set
Then x + A and xA are measurable, and
(3) If A1, A2 are measurable and A1 ⊂ A2, then λ(A1)  λ(A2).
Now suppose that (An)n∈N is a sequence of measurable sets.
(4)
 An is measurable, and 
.
If 
.
If 
.
(5)
 An is measurable.
Sets of measure 0. Sets of measure zero play an important role in measure
theory (for example, they underly the notions of “almost everywhere” and “for
almost all”, as we shall see). Examples of sets A with λ(A) = 0 are:
(1) A = {a}, a singleton, because A is then an interval in R, with
(2) A = {a1, a2, a3, ···} = 
{an}, a countable set. Then

We remark that there are uncountable sets with Lebesgue measure 0, for
example, the standard Cantor set, recalled below.
Example 7.1. (Cantor set revisited). Recall the Cantor set C from Example 1.25
on page 47. Let us show that C is uncountable.
We will prove that there is a one-to-one correspondence between points of C
and the points of [0, 1]. First note that any point x in C is associated with a
sequence of letters “L” or “R” as follows. Let x ∈ C. Then for any n, x ∈ Fn, and
when the middle thirds of each subinterval in Fn is removed, x is present either
in the left part or the right part of the subinterval, and the nth term in the
sequence of letters is L or R accordingly. For example, the points
But points in [0, 1] are also in one-to-one correspondence with such sequences.
Indeed,
If x ∈ [0, 1], then for each n, we can look at the nth equality, and see if x falls in
the left or the right part of the new subintervals created when the each
subinterval on the right-hand side of the nth equality is divided into two parts,
and this gives the [n + 1)st term of the sequence of Ls and Rs associated with x:
for example,

As [0, 1] is uncountable, it follows that so is C.
In the construction of C, since the sum of the lengths of the intervals removed is
(factor out  and sum the resulting geometric series), the measure of F is 1 − 1 =
0. So this is an example of an uncountable set with measure 0.
Any subset of a measurable set of measure 0 is also measurable with measure 0.
We say that two functions x1, x2 : A → R defined on a measurable set A are
equal almost everywhere if there exists a measurable set N with λ(N) = 0 such
that x1(t) = x2(t) for all t ∈ A\N. Sometimes then we also say that x1(t) = x2(t) for
almost all t ∈ A.
Measurable functions
Let A be a measurable subset of R. A function x : A → R∪{−∞, +∞} is called
measurable if x has any of the following equivalent properties:
(M1) For all y ∈ R, {t ∈ A : x(t) < y}.
(M2) For all y ∈ R, {t ∈ A : x(t)  y}.
(M3) For all y ∈ R, {t ∈ A : x(t) > y}.
(M4) For all y ∈ R, {t ∈ A : x(t)  y}.
Practically all functions are measurable, and they are abundant:
(1) All continuous functions are measurable.
(2) All functions that are continuous outside a set of measure 0. For example
is measurable.
Such functions are called continuous almost everywhere.
(3) All monotone functions are measurable.

(4) If A is a measurable set, then its indicator function 1A, given by
is a measurable function. Indeed, we have
(5) The sum, product and (if well-defined) the quotient of measurable
functions are all measurable.
(6) If x is measurable, then so is |x|.
Hence1 if x1, x2 are measurable, then max{x1, x2} and min{x1, x2} are
also measurable.
(7) If (xn)n∈N is a sequence of measurable functions, which converges
pointwise to a function x, then x is measurable.
The integral of measurable functions
While defining the Riemann integral, we consider upper and lower sums
corresponding to a partition P = {a = t0, t1, ··· , tn−1, tn = b} of the domain [a, b]
of the function x, for example the lower sum
The above is really the Riemann integral of a step function, which assumes
finitely many values, and is constant on intervals.
While defining the Lebesgue integral, we shall consider simple functions. A
simple function assumes finitely many values (just as before, with step

functions), but now is constant (more generally than the case of step functions)
on measurable sets (instead of mere intervals).
Roughly speaking, such simple functions arise from a partition of the range
(rather than a partition of the domain for the step functions considered when
defining the Riemann integral).
Every step function is a simple function (since every interval is measurable),
but not every simple function is a step function (because not every measurable
set is an interval).
Now let A be a measurable set, and let s : A → R be a simple function. This
means that s assumes finitely many values, which we arrange in creasing order:
and let Ak = {t ∈ A : s(t) = yk}, 1  k  n. Thus we may write
If s(t)  0 for all t ∈ A, then y1  0, and in this case, we define2
The right-hand side is either a nonnegative real number or ∞ (if one of the Ak’s
has infinite measure).
The collection of all nonnegative simple functions on A is denoted by S+(A).
For each s ∈ S+(A), we have defined
If A is a set of measure 0, then for all s ∈ S+(A),

Indeed, since every subset of a set of measure 0 is also a measurable set of
measure 0, it follows, with the notation from (7.1), that λ(Ak) = 0 for all 1  k 
n. The claim follows by the definition of the integral.
Now, let x : A → R∪{−∞, ∞} be a measurable function, and suppose that
x(t)  0 for all t ∈ A. Then we define
The right-hand side is either a nonnegative real number or +∞. In this sense, we
can say that for nonnegative measurable functions, their Lebesgue integral
always exists, but this is not the case with Riemann integrals.
Example 7.2. Let A = [0, 1], and let x : [0, 1] → R be defined by
The sets
are measurable. Since A0 is countable, λ(A0) = 0. On the other hand,
Since x = 1A1 is a simple function,
But x is not Riemann integrable, as shown below. For every partition
we have that each subinterval [tk, tk+1] contains both rational as well as irrational
points, and this observation shows that

Thus S(x, P) − S(x, P) = 1 for all partitions P, and so x is not Riemann
integrable.
Let A be a measurable set. Suppose that all the functions appearing in the list
below are defined on A, take values in [0, ∞)∪{+∞}, and are measurable. Then
we have:
(1)
(2) For 
(3) If for all 
(4) (Monotone Convergence Theorem). If 0  x1(t)  x2(t)  ··· , and
Then 
(5) If λ(A) = 0, then 
.
(6) If 
, then there exists a set N of measure zero such that x(t) < ∞
for all t ∈ A\N.
Now let x : A → R∪{−∞, +∞} be a measurable function defined on the
measurable set A. Note that x is no longer assumed to be nonnegative. We can,
nevertheless, write x as a difference, x = x+ − x−, of the two nonnegative (and
measurable) functions x+ := max{x, 0} and x− := max{−x, 0} = − min{x, 0}. See
the following picture.
We say that x is (absolutely) integrable on A if

and, in this case, we define
We note that since 0  x+(t)  |x(t)|, and thanks to assumption (7.2), it follows
from item (3) from the list of properties of the integrals of nonnegative
measurable functions given on page 367, that
and so their difference, 
, is finite too.
We’ll denote the set of all absolutely integrable functions on A by L1(A).
For x1, x2, x ∈ L1(A) and α ∈ R, we have the following:
(1) x1 + x2 ∈ L1(A) and 
.
(2) α · x2 ∈ L1(A) and 
.
(3) |x| ∈ L1(A) and 
.
(4) If 
 then there exists a set N ⊂ A of measure 0 such that x(t) = 0
for all t ∈ A\N.
(5) Let A = B∪C, where B, C are measurable too and B∩C = Ø. Then x ∈
L1(B), x ∈ L1(C) and 
.
(6) If y : A → R∪{−∞, +∞} is measurable and
then 
The parts (1), (2) assert that L1(A) is a real vector space, and the integral

is a linear transformation (or a linear functional, since the co-domain is the field
of scalars R).
We also remark that in part (4), under the given hypothesis, we cannot in
general conclude that x ≡ 0 on all of A. Indeed,
as Q is countable, however the integrand is not identically zero: for example, its
value at 1/
 is 1. On the other hand, if in (4), we are also given that x is
continuous, then we can safely conclude that x ≡ 0 on A.
The Dominated Convergence Theorem. The Dominated Convergence
Theorem says that if all the terms xn in a sequence of functions has an L1-
majorant, then assuming that their pointwise limit x exists almost everywhere, is
also an element of L1(A).
(Dominated Convergence Theorem). Let A be measurable. Let (xn)n∈N be a
sequence in L1(A), and x : A → R, be such that
Suppose that y ∈ L1(A) is such that for all n ∈ N,
Then x ∈ L1(A), and 
We remark that the hypothesis of the existence of an L1 majorant is essential, as
demonstrated by the following two examples.
Example 7.3. (Lacking an L1 majorant). Let A = R.
(1) Let xn = 1[−n,n], x = 1. Then xn ∈ L1(R), (xn)n∈N converges pointwise

everywhere on R to x, but x ∉ L1(R).
(2) Let xn = 1[n,n+1], x = 0. Then xn ∈ L1(R), (xn)n∈N converges pointwise
everywhere on R to x, but
Link with the Riemann integral. Let x ∈ C[a, b]. Then x ∈ L1[a, b] and
That x ∈ L1[a, b] follows from the fact that x is measurable (since it is
continuous), and it is bounded (Extreme Value Theorem).
The Lp spaces. We will just consider the case p = 1, and with A = [0, 1]. More
general situations can be handled in an analogous manner. Consider on L1[0, 1]
the candidate for the norm
This map || · ||1 fails to be a norm because functions that are almost everywhere 0
(e.g. 1Q∩[0,1]) have zero norm. Hence we should essentially “consider such
functions to be also the zero vector in the vector space L1[0, 1]”. This intuitive
remark can be made rigorous by considering the following relation on L1[0, 1].
We say that
if there exists a set3 N ⊂ [0, 1] of measure 0, such that
It can be seen that ~ is an equivalence relation on L1[0, 1], that is,

(ER1) (Reflexivity). x ~ x for all x ∈ L1[0, 1].
(ER2) (Symmetry). If x, y ∈ L1[0, 1] and x ~ y, then y ~ x.
(ER3) (Transitivity). If x, y, z ∈ L1[0, 1], x ~ y and y ~ z, then x ~ z.
Let [x] denote the equivalence class of x:
Thus [x] is the collection of all elements of L1[0, 1] that are almost everywhere
equal to x on [0, 1]. Define
Then we can endow a vector space structure on L1[0, 1] by setting
for [x], [y] ∈ L1[0, 1] and α ∈ R. It can also be seen that the above operations +,
· are well-defined, that is they do not depend on the chosen representatives x, y
∈ L1[0, 1] for the equivalence classes [x], [y], respectively.
We now define the map || · ||1 : L1[0, 1] → R by
Then it can be checked that || · ||1 defines a norm on L1[0, 1]. In particular, now if
||[x]||1 = 0, then it follows that x(t) = 0 for almost all t ∈ [0, 1], and so [x] = [0],
that is, [x] is the zero vector from the vector space L1[0, 1], as desired.
We recall that we had mentioned that L1[0, 1] is complete, and in this sense
the space L1[0, 1] was “better” than C[0, 1]. We supply a sketch of the proof
below. Suppose that ([xn])n∈N is a Cauchy sequence in L1[0, 1]. In order to prove
its convergence, it is enough to show the convergence of a subsequence. Hence
we may assume (by passing to a subsequence if necessary) that

Let x0 := 0, and set
By the Triangle Inequality, we have
By the Monotone Convergence Theorem,
Hence the function y is finite almost everywhere on [0, 1]. So the series
is absolutely convergent for almost all t ∈ [0, 1]. For these t’s, we set
But 
 for all t ∈ [0, 1], and so
Furthermore, 
 for almost all t ∈ [0, 1].
By the Dominated Convergence Theorem,

Hence [x] ∈ L1[0, 1]. Also, note that [|x|] + [y] ∈ L1[0, 1], and furthermore |x −
xn|  |x| + y for all n. The Dominated Convergence Theorem again gives
showing that ([xn])n∈N converges to [x] in (L1[0, 1], || · ||1).
Consequently, (L1[0, 1], || · ||1) is complete.
Notes
The path to defining the Lebesgue measure we have adopted, stems from the
notes on Integration Theory by Erik Thomas, University of Groningen [Thomas
(1998)], [Dijksma (1997)]. This is equivalent to alternative standard definitions
of the Lebesgue measure used in the literature, for example, the Caratheodory
approach, giving the same set of measurable sets as well as the measure. The
author is grateful to Raymond Mortini, University of Metz, for pointing this
equivalence out.
The proof of the completeness of L1[0, 1] is based on [Limaye (1996),
Theorem 4.6, page 51].
1 For real a, b, max{a, b} = (a + b + |a − b|)/2, and min{a, b} = a + b − max{a, b}.
2 We will write 
3 Depending in general on x and y.

Bibliography
Apostol, T. (1974). Mathematical Analysis, Second Edition (Narosa Publishing
House).
Bremermann, H. (1965). Distributions, Complex Variables, and Fourier
Transforms (Addison-Wesley).
Bryant, V. (1990). Yet Another Introduction to Analysis (Cambridge University
Press).
Day, M.M. (1973). Normed Linear Spaces, Third Edition (Springer-Verlag).
Dijksma, A. (1997). Diktaat bij Wiskund 4, 5 en 6 Natuurkunde en Sterrenkunde
(Rijksuniversiteit Groningen, Vakgroep Wiskunde).
Hirsch, M. and Smale, S. (1974). Differential Equations, Dynamical Systems,
and Linear Algebra (Academic Press).
Hörmander, L. (1990). The Analysis of Linear Partial Differential Operators I,
Second Edition (Springer).
Kreyszig, E. (1978). Introductory Functional Analysis with Applications
(Wiley).
Limaye, B. (1996). Functional Analysis, Second Edition (New Age International
Limited Publishers).
Luenberger, D. (1969). Optimization by Vector Space Methods (John Wiley).
Neuenschwander, D. (2011). Emmy Noether’s Wonderful Theorem (Johns
Hopkins University Press).
Pedersen, G. (1989). Analysis Now. (Springer).
Pinchover, Y. and Rubinstein, J. (2005). An Introduction to Partial Differential
Equations (Cambridge University Press).
Rudin, W. (1976). Principles of Mathematical Analysis, Third Edition (McGraw-
Hill).
Rudin, W. (1991). Functional Analysis, Third Edition (McGraw-Hill).
Singh, D. (2006). The Spectrum in a Banach Algebra. American Mathematical
Monthly 113, 8, pp. 756–758.
Sasane, A. (2015). The How and Why of One Variable Calculus (Wiley).
Sasane, A. (2016). Optimization in Function Spaces (Dover).

Schwartz, L. (1966). Mathematics for the Physical Sciences (Addison-Wesley).
Smith, K. (1983). Primer of Modern Analysis (Springer).
Steen, L.A. and Seebach, J.A. (1995). Counterexamples in Topology, Second
Edition (Dover).
Taylor, A. and Lay, D. (1980). Introduction to Functional Analysis, Second
Edition (John Wiley).
Thomas, E.G.F. (1996). Distributietheorie (Rijksuniversiteit Groningen).
Thomas, E.G.F. (1997). Lineaire Analyse (Rijksuniversiteit Groningen).
Thomas, E.G.F. (1998). Integraalrekening (Rijksuniversiteit Groningen).

Index
B(x, r), 18
C(X, Y), 67
CL(X), 67
CL(X, Y), 67
C[a, b], 3
C1[a, b], 4, 44
Cn[a, b], 17
Dk, 231
H(x), 234
Hn(a, b), 242
K(X, Y), 211
L(X), 57
L(X, Y), 57
(Rd), 233
L∞[a, b], 7, 14
Lp(Ω), 14
Lp[a, b], 5, 14
O(1, 1), 50
O(2), 50
[·, ·], 84
D′(Rd), 233
D(Rd), 230
L1(A), 368
S′(R), 255

S(R), 254
ℓ∞, 4
ℓp, 4
S2, 21
Sd−1, 50
{·, ·}, 152
c, 5
c00, 5, 21, 43
c0, 5
p-adic norm, 17
absolute time, 141
absolutely convergent series, 37
absolutely integrable function, 367
action, 146
adjoint operator, 108, 190
algebra, 83
almost all, 363
almost everywhere, 363
Appollonius identity, 162
approximate spectrum, 103
arc length, 61
Archimedean property, 21
averaging operator, 77
Axiom of Choice, 113
Baire Lemma, 93
Banach algebra, 83
Banach limit, 116
Banach space, 27
Banach, Stefan, 27
Banach-Steinhauss Theorem, 97
Bernstein polynomials, 22
Bernstein, Sergei, 22
Besicovitch almost periodic function, 188
Bessel’s inequality, 172
bidual of a normed space, 110

Bolzano-Weierstrass Theorem, 30
bounded above, 112
bounded linear operator, 73
bounded variation, 105
Cantor set, 47
Cauchy sequence, 26
Cauchy-Schwarz inequality, 157
Cayley transform, 199
Cesáro summation, 77
chain, 112
chain rule, 125
classical mechanics, 145
closed ball, 20
Closed Graph Theorem, 96
closed interval, 20
closed set, 20, 43
closed unit ball, 16
closure (of a set), 43
coarser topology, 79
commutant, 252
commutator, 84, 153
compact operator, 103, 210
compact set, 45, 51
complementary observables, 205
complete normed space, 27
completion of an inner product space, 163
composition of maps, 64
composition of operators, 82
conservation of angular momentum, 143
continuity at a point, 60
continuous map, 60
continuously differentiable function, 4
convergence in D, 232
convergence in S, 254
convergent sequence, 24
convex function, 128

convex set, 16, 128
convolution, 257
Convolution Theorem, 256
d’Alembert’s formula, 56
delta distribution, 235
delta function, 229
dense set, 21, 43
derivative, 119
diagonal operator, 103, 214
diagonalizable matrix, 91
differentiability at a point, 119
differentiable, 119
diffusion equation, 90
dipole, 238
Dirac delta distribution, 235
Dirac distribution, 103
Dirac, Paul, 229, 251
Dirichlet problem, 86
discrete topology, 79
distance, 8
distribution, 233
distributional derivative, 237
dual operator, 106
dual space, 79, 104
dual space of a normed space, 104
eigenvalue, 98
eigenvector, 98
energy, 148
Enflo, Per, 77
epigraph, 129
equivalence class, 163
equivalence relation, 163
equivalent norms, 33
essential supremum norm, 14
Euclidean norm, 9
Euclidean plane, 141
Euler-Lagrange equation, 134

Extreme Value Theorem, 10
factorial of a multi-index, 249
finite rank operator, 211
Fréchet derivative, 119
Fredholm integral equation, 87
functional analysis, vii
functional, bounded linear, 104
fundamental solution, 241, 242, 251, 252, 257
Fundamental Theorem of ODEs, 39
Galerkin approximation, 219
Galerkin method, 217
Galilean spacetime, 141
Gateux derivative, 122
Gelfand-Beurling formula, 102
generalised Fourier series, 185
Gram-Schmidt orthonormalisation, 167
graph, 96
Green’s function, 257
Hölder’s inequality, 14, 15
Hahn-Banach Lemma, 111
Hahn-Banach Theorem, 108
Hamel basis, 94, 115, 183
Hamiltonian, 150
Hamiltonian equations, 151
Hamiltonian mechanics, 150
harmonic oscillator, 172
Heaviside function, 234
Heaviside, Oliver, 234
Heisenberg’s uncertainty principle, 84, 204
helicoid, 144
Hermite functions, 172
Hermitian operator, 195
Hilbert cube, 48
Hilbert space, 160
Hilbert-Schmidt norm, 76, 163, 196
Hilbert-Schmidt operator, 196, 215

hyperbolic rotation, 50
hypoelliptic differential operator, 252
ideal of an algebra, 216
identity operator, 83
image of a set, 62
index, 20
index set, 20
induced norm, by an inner product, 157
induced norm, in a subspace, 15
initial value problem, 39, 89
inner product, 156
inner product space, 156
integrable function, 367
integral equation, 87
integral operator, 70
invariant subspace, 77
invariant subspace problem, 77
inverse image of a set, 62
invertible operator, 84
isometric isomorphism, 185
isomorphic normed spaces, 96
isomorphism, 96
Isoperimetric Theorem, 187
Jacobi identity, 153
Jordan canonical form, 92
jump rule, 239
Kepler’s second law, 143
kernel of an integral operator, 71
kinetic energy, 146
Lagrangian, 146
Lagrangian density, 143
Laplace equation, 171, 241, 242, 257
Laplacian, 242
law of conservation of energy, 148
law of conservation of momentum, 148

least squares approximation problem, 174
least squares regression, 176
Lebesgue integral, 6, 165
left shift operator, 54, 68, 102
Legendre polynomial, 168, 184
Leibniz rule, 249
Leibniz’s formula, 169
Lie algebra, 152
limit point of a set, 43
linear isometric embedding, 110
linear transformation, 54
Lipschitz condition, 39
Lipschitz function, 42
locally integrable function, 233
matrix mechanics, 200
maximal element, 113
measurable function, 363
measurable set, 360
measure, 360
metric space, 8
mid-point convexity, 94
minimising sequence, 178
minimum surface area, 144
Minkowski spacetime, 141
module, 250
momentum, 148
multi-index notation, 231
multiplication, of a distribution by a smooth function, 249
multiplicative identity element, 83
Neumann series, 86
Neumann Series Theorem, 86
Neumann, Carl, 86
Newton’s second law, 145
Newtonian mechanics, 145
nilpotent matrix, 92
Noether’s Theorem, 148
norm, 8

normed algebra, 83
normed space, 8
numerical analysis, 217
observables, 84
ODE, 39
open ball, 17
open cover, 51
open interval, 18
Open Mapping Theorem, 95
open operator, 93
open set, 18
open set in a topological space, 23
open unit ball, 16
operator norm, 74
order of a multi-index, 249
ordinary differential equation, 39
orthogonal, 161, 166
orthogonal complement, 172
orthogonal projection of a vector, 180
orthonormal basis, 183
parallelogram law, 160
Parseval’s identity, 186
partial order on a set, 112
partially ordered set, 112
partition of an interval, 105
perturbation theory, 237
phase plane, 151
phase plane trajectory, 151
phase portrait, 151
Poisson bracket, 152
Poissonian mechanics, 152
polarisation formula, 161
position operator, 103, 251
positive operator, 226
potential energy, 145
principle of stationary action, 145
product of normed spaces, 44

projection approximation, 220
proper time, 142
Pythagoras’s Theorem, 161
quantum mechanics, 84, 103, 153, 171, 172, 200, 229, 237, 251
reflexive space, 110
regular distribution, 234
relation, 163
representation theorems, 104
resolvent, 99
resolvent identity, 102
Riesz Representation Theorem, 104, 189
Riesz, Frigyes, 106, 189
Riesz, Marcel, 189
right shift operator, 54, 68
Rodrigues’s formula, 169
scalar multiplication, 3
Schauder basis, 76
Schrödinger equation, 172
Schwartz space of test functions, 254
seaview property, 28
self-adjoint operator, 195
separable normed space, 21
separable space, 77
simple function, 365
skew-adjoint operator, 195
Sloan approximation, 220
smaller topology, 79
Sobolev spaces, 242
special relativity, 142
Spectral Mapping Theorem, 103
spectral radius, 102
Spectral Theorem for Compact Operators, 103
spectrum, 99
sphere, 50
square root operator, 226
state, 151

stationary for a functional, curve/function/solution, 134
step function, 364
strong operator topology, 80
stronger topology, 79
subcover, 51
subspace, 4
supremum norm, 10
symmetric set, 94
taxicab norm, 10, 19
tempered distribution, 254
test function, 230
topological space, 23
topology, 23
total variation, 105
triangle inequality, 8, 9, 14
trivial topology, 79
twin paradox, 142
unbounded operator, 103
Uniform Boundedness Principle, 96
uniform operator topology, 80
unit sphere, 21
unitary operator, 195
upper bound, 112
vector addition, 3
vector space, 3
wave equation, 56, 144
wave mechanics, 200
weak operator topology, 81
weaker topology, 79
Weierstrass’s Approximation Theorem, 22
Weierstrass’s Theorem, 66
zero linear transformation, 57
zero vector, 3
Zorn’s Lemma, 112, 183


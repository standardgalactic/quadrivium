Compact Textbooks in Mathematics
Gisbert Stoyan
Agnes Baran
Elementary Numerical 
Mathematics for 
Programmers and 
Engineers

Compact Textbooks in Mathematics

Compact Textbooks in Mathematics
This textbook series presents concise introductions to current topics in math-
ematics and mainly addresses advanced undergraduates and master students.
The concept is to offer small books covering subject matter equivalent to 2- or
3-hour lectures or seminars which are also suitable for self-study. The books pro-
vide students and teachers with new perspectives and novel approaches. They
feature examples and exercises to illustrate key concepts and applications of the
theoretical contents. The series also includes textbooks speciﬁcally speaking to
the needs of students from other disciplines such as physics, computer science,
engineering, life sciences, ﬁnance.
•
compact: small books presenting the relevant knowledge
•
learning made easy: examples and exercises illustrate the application of the
contents
•
useful for lecturers: each title can serve as basis and guideline for a 2-3 hours
course/lecture/seminar
More information about this series at http://www.springer.com/series/11225

Gisbert Stoyan • Agnes Baran
Elementary Numerical
Mathematics for
Programmers and
Engineers

Gisbert Stoyan
Faculty of Informatics
ELTE University
Budapest, Hungary
Agnes Baran
Faculty of Informatics
University of Debrecen
Debrecen, Hungary
Revised
translation from
the
Hungarian
language
edition:
Numerikus
matematika
mérnökönek és programozóknak by Gisbert Stoyan, © Typotex kft, 2007 All Rights
Reserved.
MATLABr is a registered trademark of The MathWorks, Inc.
ISSN 2296-4568
ISSN 2296-455X
(electronic)
Compact Textbooks in Mathematics
ISBN 978-3-319-44659-2
ISBN 978-3-319-44660-8
(eBook)
DOI 10.1007/978-3-319-44660-8
Library of Congress Control Number: 2016958653
Mathematics Subject Classiﬁcation (2010): 65D30, 65F15, 65F05
© Springer International Publishing AG 2007, 2016
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of
the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology
now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or
the editors give a warranty, express or implied, with respect to the material contained herein or for any
errors or omissions that may have been made.
Printed on acid-free paper
This book is published under the trade name Birkhäuser, www.birkhauser-science.com
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Foreword to the English Edition
This elementary introduction to numerical mathematics arose from lectures and
exercises which were held by the two authors jointly at Debrecen University for
students of informatics. It contains material for one semester and was ﬁrst issued
quite successfully in Hungary. The English translation is the joint work of the two
authors as are the MATLAB exercises and Chap. 10 which have been added for this
edition in English.
The ﬁrst author has taught similar topics to students of mathematics, informatics,
physics and meteorology at ELTE University; also he has written (in Hungarian)
a three-volume textbook on numerical methods, embracing more material, from
rounding errors to methods for partial differential equations; the last two volumes
are now mostly used in PhD studies.
The present text in its Hungarian form was much more successful; due to its
shortness and lower requirements on mathematical knowledge, more than 900
copies were sold in few years.
We thank the two unknown referees for their helpful remarks.
Budapest, Hungary
Gisbert Stoyan
Debrecen, Hungary
Agnes Baran
February 2015
v

Contents
1
Floating Point Arithmetic..................................................
1
1.1
Integers .................................................................
1
1.2
Floating Point Numbers ...............................................
2
1.3
Floating Point Arithmetic, Rounding .................................
4
1.4
Accumulation of Errors ...............................................
9
1.5
Conclusions ............................................................
12
1.6
Exercises ...............................................................
13
2
Norms, Condition Numbers ...............................................
15
2.1
Norms and Their Elementary Properties .............................
16
2.2
The Induced Matrix Norm ............................................
19
2.2.1
Deﬁnition and Properties......................................
19
2.2.2
Computation of the Induced Matrix Norm for
the Vector p-Norms, p D 1; 1...............................
20
2.2.3
Computation of the Induced Matrix Norm (p D 2) .........
23
2.3
Error Estimations ......................................................
26
2.3.1
The Right-Hand Side of the Linear System is Perturbed ....
26
2.3.2
Condition Number.............................................
27
2.3.3
The Matrix of the Linear System is Perturbed ...............
31
2.4
Exercises ...............................................................
33
3
Solution of Systems of Linear Equations ................................
37
3.1
Gaussian Elimination ..................................................
37
3.2
When Can Gaussian Elimination be Performed? ....................
40
3.3
The LU Factorization ..................................................
42
3.4
Algorithms, Cost of Solving ..........................................
46
3.5
Inﬂuence of Rounding Errors .........................................
50
3.6
LU Factorization for General Matrices ...............................
51
3.6.1
Algorithm of the LDU Factorization, Test Examples........
54
3.7
Cholesky Factorization ................................................
56
3.7.1
Algorithm of the LDLT Factorization, Test Examples.......
60
3.8
Band Matrices..........................................................
61
3.8.1
Tridiagonal Systems of Equations ............................
62
3.8.2
The Tridiagonal Algorithm, Test Examples ..................
64
3.9
Exercises ...............................................................
65
vii

viii
Contents
4
The Least Squares Problem ...............................................
69
4.1
Linear Regression......................................................
70
4.1.1
Algebraic Description .........................................
71
4.1.2
The Method of Least Squares.................................
72
4.2
Normal Equations......................................................
76
4.3
Solution Algorithm, Test Examples...................................
80
4.4
Exercises ...............................................................
82
5
Eigenvalue Problems .......................................................
85
5.1
Fundamental Properties ...............................................
85
5.1.1
Normal Matrices...............................................
87
5.1.2
The Characteristic Polynomial................................
89
5.1.3
Localization of the Eigenvalues ..............................
90
5.2
Power Iteration.........................................................
92
5.2.1
Conditions of Convergence ...................................
93
5.2.2
The Rayleigh Quotient ........................................
97
5.2.3
Algorithm of the Power Iteration, Test Examples ...........
98
5.2.4
The Shift ....................................................... 100
5.3
The Inverse Iteration................................................... 101
5.3.1
Conditions of Convergence ................................... 102
5.3.2
Algorithm of the Inverse Iteration, Test Examples .......... 103
5.4
The QR Algorithm..................................................... 105
5.5
Exercises ............................................................... 107
6
Interpolation ................................................................ 111
6.1
Interpolation Problems ................................................ 112
6.2
Lagrangian Interpolation .............................................. 112
6.2.1
Lagrange Interpolation Problem .............................. 112
6.2.2
Newton’s Divided Differences Method ...................... 115
6.2.3
The Difference Scheme ....................................... 117
6.2.4
Algorithm of Lagrangian Interpolation, Test Examples ..... 122
6.2.5
Error Estimations .............................................. 123
6.3
Hermite Interpolation.................................................. 125
6.4
Piecewise Polynomial Interpolation .................................. 129
6.5
Exercises ............................................................... 133
7
Nonlinear Equations and Systems ........................................ 135
7.1
Bisection Method, Fixed Point Iterations............................. 136
7.2
Newton’s Method ...................................................... 138
7.2.1
Damped Newton Method ..................................... 143
7.2.2
Secant Method................................................. 145
7.3
Solution of Systems of Equations..................................... 148
7.3.1
Newton’s Method.............................................. 148
7.3.2
Algorithm of Damped Newton Method, Test Examples..... 150
7.3.3
Approximation of the Jacobian Matrix ....................... 152
7.3.4
Broyden’s Method ............................................. 153

Contents
ix
7.4
Gauss–Newton Method................................................ 155
7.4.1
Description..................................................... 155
7.4.2
Algorithm of the Gauss–Newton Method,
Test Examples ................................................. 156
7.5
Exercises ............................................................... 159
8
Numerical Integration ..................................................... 161
8.1
Elementary Quadrature Formulae..................................... 162
8.2
Interpolational Quadrature Formulae ................................. 165
8.3
Composite Quadrature Rules.......................................... 168
8.3.1
Construction of Composite Formulae ........................ 168
8.3.2
Convergence of Composite Formulae ........................ 171
8.4
Practical Points of View ............................................... 174
8.5
Calculation of Multiple Integrals ..................................... 177
8.5.1
Reduction to Integration of Functions of One Variable...... 177
8.5.2
Approximation of the Domain ................................ 179
8.5.3
Algorithm of the Two-Dimensional Simpson
Rule, Tests ..................................................... 182
8.6
Exercises ............................................................... 183
9
Numerical Solution of Ordinary Differential Equations ............... 185
9.1
Motivation.............................................................. 185
9.2
Initial Value Problems ................................................. 188
9.3
Euler’s Method......................................................... 191
9.3.1
Algorithm of Euler’s Method, Test Examples ............... 192
9.4
Error Analysis of Euler’s Method..................................... 193
9.5
The Modiﬁed Euler Method, Runge–Kutta Methods ................ 196
9.6
The Implicit Euler Method ............................................ 201
9.6.1
The Implicit Euler Method for Linear Systems .............. 201
9.6.2
Nonlinear Systems ............................................ 204
9.6.3
Algorithm of the Implicit Euler Method, Test Examples .... 206
9.7
Exercises ............................................................... 208
10
Practical Error Estimation ................................................ 211
Bibliography ...................................................................... 217
Index ............................................................................... 219

1
Floating Point Arithmetic
You know from secondary school:
If the base of a power is a > 0, then the following relations are valid:
abac D abCc;
ab
ac D abc:
If q ¤ 1
1 C q C q2 C    C qn D
n
X
iD0
qi D 1  qnC1
1  q
holds, moreover, if q D 1, then Pn
iD0 qi D n C 1:
1.1
Integers
Integers are represented on a computer in the form of signed binary numbers. Often
2-, 4- and 8-byte integers are available where a byte possesses eight binary digits.
In many computers 4 bytes are the smallest available—addressable—unit of the
memory. It may turn out that we can work with one- and 16-byte integers, too.
The arithmetical operations performed with integers may be and often are faster
than the operations performed with ﬂoating point numbers (which will be discussed
in the following section), but this depends on their word length, the processor and
the translator. Such operations can be considered as error-free, hence, using integers
a given algorithm can be faster on the computer. However, all steps of a calculation
with integers have to be carefully planned, because actually in this case we work in
“residue classes”. As an example consider the addition of 1-byte integers:
11010101
+
10101010
=1
01111111
213
+
170
=
383  127 mod(256)
© Springer International Publishing AG 2016
G. Stoyan, A. Baran, Elementary Numerical Mathematics for Programmers and
Engineers, Compact Textbooks in Mathematics, DOI 10.1007/978-3-319-44660-8_1
1

2
1
Floating Point Arithmetic
The processing unit cuts off—without any warning!—the 1 which cannot be placed
in the frame (it would produce an overﬂow), and in this way the sum is replaced by
its residual with respect to 28.
Thus, we move to the residue class of 28 and, e.g., in the case of two-byte
numbers to the residue class of 216, etc.
1.2
Floating Point Numbers
This type of numbers is fundamental in the use of computers for numerical
computations. The form of nonzero ﬂoating point numbers is
˙ ak m1
a C m2
a2 C    C mt
at

;
(1.1)
where a > 1 is the base of the representation of numbers, “+” or “” is the sign,
t > 1 is the number of digits, k is the exponent. The digit m1 is normalized, that is,
it satisﬁes the inequality
1  m1  a  1:
This ensures the uniqueness of the representation and the full exploitation of the
available digits. For the remaining digits
0  mi  a  1;
2  i  t
holds. The number zero is not normalized: in this case k D 0; m1 D m2 D    D
mt D 0, and its sign is usually “+”.
It often happens that the base a of the representation of ﬂoating point numbers
is not equal to 2. Instead of 2 it can be, e.g., 10 or 16. Some computers have a
decimal CPU. However, usually the corresponding arithmetic is implemented by
the programming language, and when, e.g., the main purpose is data processing,
it can be more economical to use the decimal number system. We may view the
computations as being done in the decimal number system and can count on about
t D 8 using single precision and about t D 16 using double precision.
We can imagine the storage of ﬂoating point numbers in the following form
(reality may be different from this, but here we do not describe these details of
the technical implementation which do not affect our numerical calculations):
Œ˙; k; m1; : : : ; mt;
where the vector .m1; : : : ; mt/ DW m is the mantissa (or signiﬁcand). The exponent
k is also called the characteristic of the number. Depending on the machine and
on the precision (single, double, quadruple precision) four, eight or 16 bytes are
available to store m. Simultaneously, the range of k increases but, depending on the

1.2
Floating Point Numbers
3
given precision,
k./  k  k.C/ ;
where k./ < 0; k.C/ > 0 and jk./j  jk.C/j (see Exercises 1 and 2). Then the
largest representable number is
M1 WD ak.C/
tX
iD1
a  1
ai
D ak.C/
a  1
a
C a  1
a2
C    C a  1
at

D ak.C/.1  at/;
while the smallest number is M1. The ﬂoating point numbers form a discrete
subset of the rational numbers from ŒM1; M1, and this subset is symmetric
about zero.
We denote the positive ﬂoating point number nearest to zero by "0,
"0 WD ak./
 1
a C 0 C    C 0

D ak./1:
Thus, besides zero there is no other (normalized) ﬂoating point number in the
interval ."0; "0/. (Here we disregard the fact that on a computer denormalized
nonzero numbers can also be used, but to ensure the uniqueness of the representation
only in the case k D k./. Practically, this means that then the formula for "0
changes to ak./t.) The interval ."0; "0/ appears as a huge gap if we represent
the ﬂoating point numbers on the real axis (see Fig. 1.1 and Exercise 3.) since the
positive ﬂoating point number nearest to "0 is
ak./
 1
a C 0 C    C 0 C 1
at

D "0 C ak./t D "0.1 C a1t/;
and
"0  a1t  "0 :
The number 1 always belongs to the ﬂoating point numbers:
1 D ŒC; 1; 1; 0; : : : ; 0:
0 ε0
1
2
0.5
Fig. 1.1 Floating point numbers on the real axis

4
1
Floating Point Arithmetic
After the number 1 the next ﬂoating point number
ŒC; 1; 1; 0; : : : ; 0; 1 D 1 C "1;
follows, where
"1 WD a1t:
This number is often called the relative precision of the machine, or the machine
epsilon (see Exercise 4). The importance of the numbers "0 and "1 is that in the case
of the input and of the four basic operations they give an absolute and a relative
error bound, respectively (see the next section).
If we take any ﬂoating point number satisfying
0 < x D ŒC; k; m D ak m1
a C m2
a2 C    C mt
at

< M1 ;
then the nearest ﬂoating point number to x larger than x is x WD x C akt:
x D x C ak

0 C 0 C    C 0 C 1
at

D x C akt:
Thus, the spacing between the two numbers is ıx WD x  x D akt. The smallest
possible value x for the present characteristic k is equal to ak 1
a D ak1, hence,
x  x D ıx D akt D ak1C1t D ak1a1t D ak1"1  x"1:
(1.2)
If x < 0 and x is the left-hand side ﬂoating point neighbour of x then, similarly,
jx  xj  jxj"1 :
(1.3)
1.3
Floating Point Arithmetic, Rounding
First consider the input.
Imagine a real number x to be read in, jxj  M1. Then, most often, the ﬂoating
point number Qx assigned to x by the machine is
Qx WD
(
0;
if jxj < "0 ;
the ﬂoating point number nearest to x
if "0  jxj  M1 :
Thus, in the ﬁrst case the number is rounded towards zero (the gap around 0 appears
as a “black hole”) while in the second case the number is rounded properly. Then

1.3
Floating Point Arithmetic, Rounding
5
(see relations (1.2) and (1.3))
j Qx  xj 
(
"0;
if jxj < "0 ;
1
2"1jxj;
if jxj  "0 :
(1.4)
If the machine always rounds the numbers towards zero (since truncation of the
non–signiﬁcant digits is simpler than proper rounding), then in (1.4) "1 replaces 1
2"1
and so j Qx  xj < "1jxj is true always. Disregarding the gap around zero we obtain
a simpliﬁed version of (1.4), i.e.
j Qx  xj  1
2"1jxj
(1.5)
in the case of rounding, or
j Qx  xj  "1jxj
(1.6)
in the case of truncation.
The numbers 1
2"1 or "1 arising here can be described as follows: they are the
smallest positive ﬂoating point numbers " for which the result of the addition 1 C "
is greater than 1—after rounding and truncation, respectively (see Exercise 4).
Next, consider the ﬂoating point operations. As an example, ﬁrst examine the
addition. Take, e.g., a D 10; t D 3; k./ D 3; k.C/ D 3 (then "1 D 102)
and x D 3:45; y D 0:567 as the two numbers to be added. As the ﬁrst step of the
addition the characteristics of the two numbers (k1 and k2) have to be made equal,
and the mantissa of the smallest number has to be shifted right by k1  k2 digits:
+
1
345
+
+
0
567
=
+
1
345
+
+
1
0567
10*0.345 ( D x)
+
1*0.567 ( D y)
=
10*0.345
+
10*0.0567=4.017=x C y DW z
Here immediately the question arises whether the CPU uses t digits to place the
mantissa.
In the case of addition at least one more digit (a so-called guard digit) is necessary
to have an acceptable error. (In the case of multiplication it is best if there are 2t
digits available to place the product of the mantissas because then the product can
be placed without loss.)
If we have this guard digit, then the addition from the above example can be
continued in the following way:
+
1
345
+
+
0
567
=
+
1
345
+
+
1
0567
=
+
1
4017

6
1
Floating Point Arithmetic
Up to this point the result is error-free but usually one has to put it back into the
storage where in our example there are only three digits to place the mantissa. Then
the last digit has to be truncated or rounded. If the latter was chosen when the CPU
(or the programming language which implements the arithmetic) was planned, then
the result of the computation, i.e. Qz D A
x C y is
+
1
402
Hence, the result is loaded by error, and for the error the following is valid—in
accordance with (1.6), writing z; Qz instead of x; Qx:
jz  Qzj D 0:003 D "1 	 0:3 D 1
2"1 	 0:6 < 1
2"1 	 jzj .D 0:020085/:
(1.7)
Let us examine one more example of addition (we take a D 10; t D 3;
k./ D 3; k.C/ D 3 as before and further assume that in the CPU one guard
digit is available): x D 0:987 and y D 0:567.
Now, there is no need to shift the numbers before the addition:
+
0
987
+
+
0
567
=
+
0
1554
Š
+
1
155
0.987 ( D x)
+
0.567 ( D y)
=
1.554 ( D z)
Š
10*0.155=:Qz
Here, in the last step the result had to be normalized and rounded, and after these
steps the result (once more in accordance with (1.6)) can be put into the storage.
Now, examine the rounding errors occurring during the basic operations, in
general. According to the example above, we assume that ﬁrst the machine performs
exactly any of the four basic operations .C; ; 	; =, denoting them regardlessly by
˘), and after that it assigns to the result a ﬂoating point number using truncation or
rounding. Then in the case of rounding according to (1.4)
jA
x ˘ y  x ˘ yj 
(
"0 ;
if jx ˘ yj < "0
1
2"1jx ˘ yj;
if jx ˘ yj  "0 ;
(1.8)
or, in a simpliﬁed version, omitting the case corresponding to "0,
jA
x ˘ y  x ˘ yj  "1jx ˘ yj 	
(
1;
in the case of truncation
1
2;
in the case of rounding.
(1.9)
This inequality can be described as
A
x ˘ y D .x ˘ y/  .1 C "˘/;
j"˘j  "1 	
(
1;
truncation
1
2;
rounding.
(1.10)

1.3
Floating Point Arithmetic, Rounding
7
Indeed, relation (1.10) can be obtained from (1.9) in the following way (considering
the case of truncation): inequality (1.9) means that
"1jx ˘ yj  A
x ˘ y  x ˘ y  "1jx ˘ yj:
Therefore, the expression A
x ˘ y  x ˘ y can take any value between "1jx ˘ yj and
"1jx ˘ yj. This can be written as
A
x ˘ y  x ˘ y DW "1jx ˘ yjs; where  1  s  1:
Thus,
A
x ˘ y D x ˘ y C "1jx ˘ yjs D .x ˘ y/.1 C sign.x ˘ y/"1s/;
and here j sign.x ˘ y/"1sj

"1. Now, we have only to substitute "˘ for
sign.x ˘ y/"1s.
Our example concerning the addition illustrates the estimation (1.10) for ˘ D C:
"˘ D "C D A
x C y  .x C y/
x C y
D 0:003
4:017  0:000747 < 1
2 	 "1 D 0:005:
The estimation (1.10) is not valid in the gap around zero, moreover, it is also not
valid in the case when the result of the operation is in absolute value larger than M1.
This would mean overﬂow which is a serious error for ﬂoating point calculation,
and—depending on the programming language and on the operating system—after
one or more errors of this kind the machine terminates its work (more precisely, it
works with the next program).
If the result of an operation is not equal to zero, but it lies in the interval ."0; "0/,
then according to the above reasoning underﬂow occurs; the machine considers
the result to be zero, usually without an error message. In this case important
information may be annihilated, the last sign of life of some phenomenon.
Using ﬂoating point subtraction, valuable digits often disappear. This means the
following: as earlier consider the case a D 10; t D 3 and, e.g., x D 1:234,
y D 1:233. Then there is no need to shift the digits, and in the second step of
the subtraction we get x  y D 0:001, without error. After this the number has to
be normalized; the result is 102. 1
10 C 0 C    C 0/ without error. However, there
is a problem: the numbers x; y had four digits, which resulted, e.g., from an earlier
calculation, and after the subtraction only one digit remains. From the point of view
of the ﬂoating point calculation, we cannot take any responsibility for the accuracy
of the zeros after the leading digit 1.
The other dangerous phenomenon of the ﬂoating point arithmetic is the possible
cancellation by subtraction.
This means the following. The machine does not correspond to the simple model
above when in the CPU the exact result is ﬁrst produced. In this case there is no

8
1
Floating Point Arithmetic
guard digit, and before the subtraction of two ﬂoating point numbers x; y with
different exponents .k1 ¤ k2/ the digits are not only shifted by k1  k2 digits
(compare with the above example for addition), but the digits moving on the
right-hand side out of the frame are immediately truncated. For example,
x D ŒC; k; 1; 0; : : : ; 0 D ak1;
y D ŒC; k  1; a  1; : : : ; a  1 D ak
 1
a 
1
atC1

:
After shifting and truncating the last digit which has the value ak a1
atC1 , the
machine continues working with the number Qy D ak  1
a  1
at
 instead of y, and
x  Qy D akt DW Qz appears as the result of the machine subtraction. However, the
exact result is z WD akt1. (See Exercise 6.)
This example calls for the distinction of two errors. If Qz is the result of a ﬂoating
point calculation and z the exact result (in our example z D akt1 and Qz D akt),
then jQz  zj is called the absolute error of the calculation. If z ¤ 0, then the value
j.Qzz/=zj is the relative error. Applying this to our example, the absolute error of the
subtraction performed under the above circumstances is jQz  zj D akt  akt1 D
akt1.a  1/, while the relative error is
akt1.a  1/=akt1 D a  1 D a  1
a1t "1  at"1 
 "1;
which is considerably larger than "1 D a1t, nearly at times larger. This relative
error is smallest when a D 2, but it is even then not acceptable, since if the relative
error can be equal to 1, then the result can be confused with its error. This is the
possible fatal loss of signiﬁcant digits on a (badly planned) computer.
To avoid this phenomenon of machine subtraction we need at least one guard
digit in the CPU, on the .t C 1/th place which, however, is missing in several
machines. By subtraction, signiﬁcant digits can also disappear when a guard digit is
available, and the exponents of the two numbers are equal, in this case not in a fatal
way, but according to (1.8).
In practice, in connection with the evaluation of the accuracy of computational
results, one works not only with absolute and relative errors. It is often practical to
apply a (“mixed” absolute-relative) test like
jz  Qzj  "

1 C 1
2.jzj C jQzj/
	
(1.11)
where " is the given accuracy. If the inequality is not true, the result Qz is rejected as
a too inaccurate result; if it is true, then we accept the result. Using the above test,
if jzj C jQzj is small, absolute accuracy is measured by ", while if jzj C jQzj is large,
relative accuracy is measured. The analogy with (1.4) and with (1.8) is striking. We
advise you to apply such a test during programming.

1.4
Accumulation of Errors
9
The innocent-looking operation of rounding can lead to large errors even when
a guard digit is present as the examples in the following section show. However,
rounding is unavoidable after almost all ﬂoating point arithmetical operations (but
see Exercise 6), hence a possible solution is to replace the simple rounding described
by (1.4) and (1.10) by one-sided roundings. After each arithmetical operation, when
the result is not a ﬂoating point number, one produces two results by rounding up
and rounding down. As a consequence the complete calculation changes: instead of
numbers one always has to work with intervals. Hence, either the machines have
to be reconstructed to perform the roundings always with the actual arithmetical
operation (i.e. instead of each basic operation one has two operations) or one has to
build the interval arithmetic into the programming language. The latter was done,
e.g., in the case of Pascal-SC.
1.4
Accumulation of Errors
How do errors accumulate when one performs many operations? Suppose we have
to add n C 1 ﬂoating point numbers,
Sn WD x0 C x1 C    C xn D
n
X
iD0
xi;
that is, we have to perform not one, but n additions. For each addition, the CPU
ﬁrst adds to the previous partial sum Sk1 the following number xk without errors:
Sk WD Sk1 C xk; k D 1; 2; : : :, then it applies, e.g., truncation. Here we disregard
the fact that some partial or the ﬁnal result can be in the interval ."0; "0/, or it can
overﬂow.
Then based on (1.10) the ﬁrst partial sum obtained on the computer is
e
S1 WD B
x0 C x1 D .x0 C x1/.1 C "0;1/ D S1 C "0;1S1 where j"0;1j  "1 :
Similarly:
e
S2 WD B
e
S1 C x2
D .e
S1 C x2/.1 C "1;2/
D .S1 C "01S1 C x2/.1 C "1;2/
D .S1 C x2 C "0;1S1/.1 C "1;2/
D .S2 C "0;1S1/.1 C "1;2/
D S2 C "0;1S1 C "1;2S2 C "1;2"0;1S1

10
1
Floating Point Arithmetic
where "1;2 is the corresponding value of the rounding error, j"1;2j  "1. In order
to have a better view of the essence of the problem, and using that "1;2"0;1S1 is of
second order in "1:
j"1;2"0;1S1j  "2
1jS1j;
we neglect this term (and use the notation Š for equalities up to second order terms):
S2 Š S2 C "0;1S1 C "1;2S2 :
Continuing the above reasoning you see that
e
Sn Š Sn C
n
X
kD1
"k1;kSk :
(1.12)
Let us express the partial sums Sk with the help of the xi:
n
X
kD1
"k1;kSk D x0
n
X
kD1
"k1;k C
n
X
iD1
xi
n
X
kDi
"k1;k :
Observe that the estimation of the sum multiplying x0 and x1 is
ˇˇˇˇˇ
n
X
kD1
"k1;k
ˇˇˇˇˇ  n"1 ;
since j"k1;kj  "1 holds for all k and we have n terms. The estimation of the sum
multiplying x2 is
ˇˇˇˇˇ
n
X
kD2
"k1;k
ˇˇˇˇˇ  .n  1/"1 ;
and, generally, the estimation of the sum multiplying xi (i  1) is
ˇˇˇˇˇ
n
X
kDi
"k1;k
ˇˇˇˇˇ  .n  i C 1/"1 :
Returning to (1.12) we can write that (neglecting all second order terms)
je
Sn  Snj  "1
 
njx0j C
n
X
iD1
.n  i C 1/jxij
!

1.4
Accumulation of Errors
11
is valid. From this you see that it is worth beginning the summation with the numbers
xi having the smallest absolute value: in the error term the multiplier of x0 and x1
is n"1, but xn has only an "1 multiplier.
The following estimation is a little bit rougher but it shows better that the error
increases linearly as the number of operations n increases:
je
Sn  Snj  n"1
n
X
iD0
jxij:
(1.13)
In the special case when all xi are positive (and neglecting the second order terms)
we obtain an estimation of the relative error:
ˇˇˇˇˇ
QSn  Sn
Sn
ˇˇˇˇˇ  n"1 :
This relation shows that when n"1  1 an accurate result cannot be expected.
We come to a similar conclusion when we multiply n C 1 ﬂoating point numbers
which are not only positive but lie around 1, i.e. the n multiplications do not involve
the danger of overﬂow and underﬂow. If
P1 WD x0 	 x1; Pn D Pn1 	 xn; n D 2; 3; : : : ;
that is
Pn D
n
Y
kD0
xn ;
then on the computer (with corresponding but possibly different from (1.12) error
quantities "k1;k)
QP1 D P1 	 .1 C "0;1/;
QP2 D B
QP1 	 x2 D .P1 	 .1 C "0;1/ 	 x2/ 	 .1 C "1;2/
D P2 	 .1 C "0;1/ 	 .1 C "1;2/
D P2 	 .1 C "0;1 C "1;2 C "0;1"1;2/
Š P2 	 .1 C "0;1 C "1;2/;
and generally
QPn Š Pn.1 C
n
X
kD1
"k1;k/:

12
1
Floating Point Arithmetic
This means that
ˇˇˇˇˇ
QPn  Pn
Pn
ˇˇˇˇˇ Š
ˇˇˇˇˇ
n
X
kD1
"k1;k
ˇˇˇˇˇ  n"1;
(1.14)
since an upper bound of the sum is n"1. Thus, when the latter quantity does not
exceed 1=a then we can expect at least one accurate digit.
To what extent can this reasoning be accepted as we have neglected second order
terms? The worst case is when the error is maximal in each multiplication: "1. Then
Pn  QPn  Pn.1 C "1/n and
ˇˇˇˇˇ
QPn  Pn
Pn
ˇˇˇˇˇ  .1 C "1/n  1:
Recall from mathematical analysis that 1 C x  ex (for all x), and for this reason
on the one hand .1 C "1/n  1  en"1  1, on the other hand one can verify that
ex  1 
x
1 x
2 if 0  x < 2. Thus, in the case of x D n"1 < 2:
ˇˇˇˇˇ
QPn  Pn
Pn
ˇˇˇˇˇ 
n"1
1  n"1
2
:
If we would like the relative error of QPn to be less than 1
a (i.e. at least one accurate
digit in QPn), then solving the relation
x
1 x
2  1
a for x we obtain that
ax  1  x
2 H) .a C 1
2/x  1 H) x 
1
a C 1
2
:
This means the following: from relation (1.14) obtained by neglecting the second
order quantities you can see that the relative error of QPn is less than 1
a if n"1  1
a.
Without neglecting the second order terms this is not valid—but it is sufﬁcient to
require n"1 
1
aC 1
2 .
1.5
Conclusions
On computers we can work with two fundamental types of numbers: integers and
ﬂoating point numbers. While the operations with integers are error-free (in fact here
we work in residue classes), for ﬂoating point numbers rounding is unavoidable. As
a consequence of this (also from other sources like measuring) errors will occur,
and accumulation of errors can strongly bias the ﬁnal result. For this reason it is
important to estimate the ﬁnal errors. While after one operation the error will be of
the order of "1, the relative machine accuracy, after n operations it will be of the
order of n"1.

1.6
Exercises
13
Most probably, the speed of computers will increase in the future. (This will
happen not by increasing the “clockrate” but rather by increasing the number of
cores available and newer technical developments.) If parallel with this the length of
the mantissa of ﬂoating point numbers used in programs and the domain Œk./; k.C/
of the characteristic do not increase, then the danger of unreliable arithmetic results
also increases: if the relative error obtained after n operations is not less than one,
then the result is unacceptable.
1.6
Exercises
1. Assuming that a D 10, determine the values of t, k.C/ and k./ on your
computer (laptop, mobile etc.). How does the machine react to overﬂow and
to underﬂow? Does a guard digit exist on your computer?
2. Why is it often true that jk.C/j ¤ jk./j, for normalized numbers? Start from
the question how many characteristics can be represented with n binary digits
in general, how many when the ﬁrst digit is used for the sign, and how many
when the exponent k is stored in form k C 2n1, without sign.
3. Represent on the real axis the non-negative ﬂoating point numbers correspond-
ing to a D 2, t D 4, k./ D 3, k.C/ D 2.
(In the following exercises we always assume that the values a; t; k.C/,
k./ characterizing the representation of ﬂoating point numbers, are given,
moreover, k./  k.C/ > t  2 is valid.)
4. Determine in an experimental way the numbers "0 and "1.
5. Which is the smallest natural number not contained among the ﬂoating point
numbers?
6. Which arithmetical operations do you know which can be performed error-free
for ﬂoating point numbers?
7. Name two ﬂoating point numbers x; y, different from zero and one, such that
x C y or x 	 y are (a) ﬂoating point or (b) not ﬂoating point.
8. We have seen that the right-hand side ﬂoating point number adjacent to 1 is
1 C "1. However, what is the left-hand side ﬂoating point neighbour of 1?
9. How many positive ﬂoating point numbers exist for a ﬁxed characteristic k?
10. Give a reason for the fact that in the case of ﬂoating point calculation the
sequence fsng1
nD1, s1 D 1, sn D sn1 C 1=n, n D 2; 3; : : :, known from
mathematical analysis, becomes a constant sequence when n is large enough—
instead of diverging.
Estimate approximately how large Qsn will be, when n is large enough. For
this, use that for the exact values of sn
log.n C 1/ < sn < 1 C log n
holds. Check your result with the help of your computer (using the shortest
“real” type).

14
1
Floating Point Arithmetic
11. Imagine you have a program to calculate the determinant of a second order
matrix based on the well-known mathematical formula. In the CPU there is a
guard digit, after the arithmetical operations the number with t C 1 digits will
be normalized and rounded, if necessary. If the matrix is
A D A."/ D
 1 1  "
1
1

; " > 0;
then starting from " D 1 and decreasing the value of ", what is the smallest
positive " for which the determinant is positive?
12. Examine the value of the logical expression
(1/45)*15==1/3
in MATLAB. Try to explain the phenomenon.
13. Choosing x D 1=3 as the initial value, the expression 4x 1 should return with
1=3. Run the command x D 4 	 x  1 several (e.g., 27, 30, 40) times in a loop
and check the ﬁnal value of x.
14. Consider the MATLAB commands
4-5+1
0.4-0.5+0.1
and give a comment on the outcomes.
15. The equation
 1
5x C 1

x  x D 0:2
is true for all x ¤ 0. How many times does MATLAB ﬁnd the above equation
valid, if x D 1; : : : ; 100?
16. Read the text on “Representation of Floating-Point Numbers” in the help-part
of MATLAB and look on the internet for “IEEE standard 754”.
17. Find a simple algebraic relation between the following MATLAB numbers:
realmin; eps.0/; eps by reading the explanations in the MATLAB help—
or by direct experimentation.

2
Norms, Condition Numbers
You remember from Algebra:
If A 2 IRnn is a matrix with (real) elements aij , and x D .x1; x2; : : : ; xn/T is a column vector,
then Ax is a vector where the ith component is
.Ax/i D
n
X
jD1
aij xj ; i D 1; : : : ; n:
Moreover, if for some vector v and number 
Av D v; v ¤ 0;
holds, then  is an eigenvalue of the matrix A, and v is the corresponding eigenvector. Both 
and v may turn out to be complex.
Let us show that as generalization of jx C yj  jxj C jyj there holds
ˇˇˇˇˇ
n
X
iD1
xi
ˇˇˇˇˇ 
n
X
iD1
jxij:
This follows from
ˇˇˇˇˇ
n
X
iD1
xi
ˇˇˇˇˇ  jx1j C
ˇˇˇˇˇ
n
X
iD2
xi
ˇˇˇˇˇ  jx1j C jx2j C
ˇˇˇˇˇ
n
X
iD3
xi
ˇˇˇˇˇ
etc.
© Springer International Publishing AG 2016
G. Stoyan, A. Baran, Elementary Numerical Mathematics for Programmers and
Engineers, Compact Textbooks in Mathematics, DOI 10.1007/978-3-319-44660-8_2
15

16
2
Norms, Condition Numbers
You surely know and understand the following operations with sums (here m; n  1 are
integers):
n
X
iD1
m
X
jD1
aibj D
n
X
iD1
ai
m
X
jD1
bj D
m
X
jD1
n
X
iD1
aibj D
m
X
jD1
bj
n
X
iD1
ai :
These equalities hold because in each case all possible combinations of ai and bj are multiplied
and summed:
n
X
iD1
m
X
jD1
aibj D a1b1 C a1b2 C    C a1bm C a2b1 C a2b2 C    C a2bm
C    C anb1 C    C anbm ;
n
X
iD1
ai
m
X
jD1
bj D a1.b1 C b2 C    C bm/ C a2.b1 C b2 C    C bm/
C    C an.b1 C b2 C    C bm/;
m
X
jD1
n
X
iD1
aibj D a1b1 C a2b1 C    C anb1 C a1b2 C a2b2 C    C anb2
C    C a1bm C    C anbm ;
m
X
jD1
bj
n
X
iD1
ai D b1.a1 C a2 C    C an/ C b2.a1 C a2 C    C an/
C    C bm.a1 C a2 C    C an/:
2.1
Norms and Their Elementary Properties
In the following chapter we are going to discuss the solution of a system of linear
equations
Ax D b
(2.1)
where A is an n  n real matrix: A 2 IRnn; b 2 IRn is a given vector and x 2 IRn
is the unknown vector. In practice both vector b and matrix A are often given with
uncertainties: they are perturbed by errors, e.g. by rounding errors. Hence, in this
chapter we study the magnitude of the error in the vector x caused by errors in A
and b. As a result of this examination we will understand the following: on what
does it depend that on a given computer a linear system with a given error can be
solved with an acceptable error, or not?
To answer this question we shall introduce norms and condition numbers.
If, for example, bCıb is given instead of b, where ıb is the vector of errors in b, then
we would like to know, how large is the error of the solution y of the “perturbed”
system Ay D b C ıb, i.e. how large is y  x where x is the unknown error-free

2.1
Norms and Their Elementary Properties
17
solution? To answer this we need to review (or introduce) some deﬁnitions, as we
have to measure the distance of two vectors and two matrices.
In what follows, even if we write complex vectors: x 2 ICn, it is usually possible
to think only of real ones. Complex vectors occur as a solution of the eigenvalue
problem since a real matrix can also have complex eigenvectors and -values.
A function d W ICn ! R is called a norm if the following four conditions hold:
1. d.x/  0 for all x 2 ICn;
2. d.x/ D 0 , x D 0, so the nullvector can uniquely be recognized;
3. d.a  x/ D jajd.x/ where a is an arbitrary number which can also be complex
(i.e., if a D b C ci; i D
p
1, then jaj D
p
b2 C c2);
4. d.x C y/  d.x/ C d.y/ (this is the so-called triangle inequality).
The value d.x/ is the norm of the vector x and since it is a generalization of the
absolute value, instead of d.x/ the usual notation is kxk.
Notice that in the second property 0 has two different meanings: as a number and
in the case of x D 0 as a vector. In what follows we always write 0, both in the case
of numbers and vectors, and even for matrices and functions.
In the space ICn the following p-norms are useful for us:
d.x/ D kxkp WD
 n
X
iD1
jxijp
!1=p
;
p D 1 (Manhattan norm),
p D 2 (Euclidean norm),
p D 1 (maximum norm).
The maximum norm could be interpreted as the limit for p ! 1 but it can also
be deﬁned in the following way:
kxk1 WD max
1in jxij:
This follows from the inequality
max
1in jxij 
 n
X
iD1
jxijp
!1=p
 .n/1=p max
1in jxij
which is always true because max1in jxij is present among the values jxij (this
gives the lower estimation on the left-hand side). Moreover, jxij can take the
maximal value at most n times which explains the upper estimation on the right-
hand side. Further, recall from mathematical analysis that n1=n tends to 1 as n ! 1.
So much the more n1=p ! 1, if n is ﬁxed and p ! 1, as here. For example, if
n D 256 and p D 2 then n1=p D 16, if p D 4 then n1=p D 4, but in the case of
p D 16 we have n1=p D
p
2  1:414.
We often omit the subscript p when any of the above-mentioned three norms can
be used, but in general, these norms behave rather differently (Fig. 2.1).

18
2
Norms, Condition Numbers
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
1
1
−1
−1
p=∞
p=2
p=1
Fig. 2.1 The unit ball in three p-norms: the points x satisfying jjxjjp D 1
The Euclidean norm corresponding to p D 2 is probably known from your
previous studies. If the vector x has real components, then we can simply write
kxk2 D
q
x2
1 C x2
2 C    C x2n D
 n
X
iD1
x2
i
!1=2
:
Consider an example: if x is the row vector .1; 2; 3; 4/, then
kxk1 D 1 C j  2j C j  3j C 4 D 10;
kxk2 D
p
1 C 4 C 9 C 16 D
p
30  5:477;
kxk1 D max.1; j  2j; j  3j; 4/ D 4:
In the case of the maximum and the Manhattan norm, the triangle inequality follows
from the fact that for the absolute value the inequality ja C bj  jaj C jbj holds.
In the case of the Euclidean norm it is a consequence of the Cauchy-inequality (see
the beginning of Chap.4).
We remark that the parameter p of the p-norms can be any number from the
interval Œ1; 1, but 0 < p < 1 is not possible because in this case the fourth property
does not hold, that is, kxkp does not deﬁne a norm. As a counterexample consider
n  2, moreover, let x D .1; 0 : : : ; 0/T and y D .0; 1; 0 : : : ; 0/T . Thus, according
to the deﬁnition, kxkp D 1; kykp D 1, but from 0 < p < 1 we have
kx C ykp D .1 C 1/1=p D 21=p > 2 D kxkp C kykp :

2.2
The Induced Matrix Norm
19
2.2
The Induced Matrix Norm
2.2.1
Deﬁnition and Properties
In what follows we shall denote column vectors by x; y. As an example,
x D
0
BBB@
1
0
:::
0
1
CCCA 2 IRn
is the ﬁrst column unit vector, denoted also as e1 and often written as a transposed
row vector .1; 0; : : : ; 0/T .
Now, if A 2 IRnn is a real matrix, with the help of a vector norm we can deﬁne
a matrix norm:
kAk WD max
x¤0
kAxk
kxk :
(2.2)
It is the matrix norm induced by the given vector norm (or subordinate to the given
vector norm). We need to check that (2.2) deﬁnes indeed a norm on the linear space
of the matrices:
1. The non-negativity of the expression on the right-hand side follows from the
non-negativity of the vector norms.
2. If A D 0, then kAxk D 0 for all vectors x, so kAk D 0. In the opposite direction:
if A ¤ 0, then A has at least one nonzero element, for example in the jth column.
If we multiply the matrix A by the jth unit vector x D ej, then Ax is the jth
column vector of A. The norm of this vector is positive, so the maximum is also
positive.
3. In the case of the matrix c  A for any real or complex number c, it follows
again from the property of the vector norm that kc  Axk D jcjkAxk. Since the
maximum does not concern c, there holds kc  Ak D jcjkAk.
4. Finally, if B is also a matrix on IRnn, then k.A C B/xk D kAx C Bxk 
kAxk C kBxk. Hence, with the help of deﬁnition (2.2) we obtain
kA C Bk WD max
x¤0
k.A C B/xk
kxk
 max
x¤0
kAxk C kBxk
kxk
 max
x¤0
kAxk
kxk C max
x¤0
kBxk
kxk
D kAk C kBk;
where the second inequality is valid because on its right-hand side we take the
maxima of kAxk
kxk and kBxk
kxk separately.
Therefore, the triangle inequality is true, too.

20
2
Norms, Condition Numbers
Since in (2.2) the matrix norm is deﬁned as the maximum over all nonzero vectors
x, for a ﬁxed x ¤ 0 the quotient kAxk=kxk can be less than the norm, but it can
never be greater:
kAxk
kxk  kAk; that is kAxk  kAk kxk:
(2.3)
Later this inequality will be used in several estimations. Conversely, if M is a (non-
negative) number such that for all vectors x
kAxk  M  kxk;
(2.4)
then kAk  M and the norm of the matrix A is the smallest number M for
which (2.4) is always valid.
Using (2.3) we can also obtain an upper bound of the norm of a vector multiplied
by several matrices, e.g.
kABxk D kA.Bx/k  kAk kBxk  kAk kBk kxk:
By (2.2) this means that for all matrices A; B 2 IRnn
kA  Bk  kAk  kBk
(2.5)
holds.
2.2.2
Computation of the Induced Matrix Norm for the Vector
p-Norms, p D 1; 1
We compute the matrix norm (2.2) directly from the elements aij of the matrix A.
For this aim
1. we have to verify an estimation of the form (2.4),
2. we need to ﬁnd a vector x ¤ 0 for which one has equality in (2.4).
Consider ﬁrst the case p D 1:
First step:
The inequality
kAxk1 D
n
X
iD1
j.Ax/ij D
n
X
iD1
ˇˇˇˇˇˇ
n
X
j D1
aij xj
ˇˇˇˇˇˇ

n
X
iD1
n
X
j D1
jaij jjxjj D
n
X
j D1
jxjj
n
X
iD1
jaij j
holds for all x.

2.2
The Induced Matrix Norm
21
In fact the last double sum is an expression of the form Pn
j D1 jxjjcj , where
cj WD Pn
iD1 jaij j. We take from these n non-negative numbers the largest one:
maxj
Pn
iD1 jaij j

, and then
kAxk1  max
j
 n
X
iD1
jaij j
!

n
X
j D1
jxj j:
(2.6)
Since Pn
j D1 jxj j D kxk1, this means that kAk1  max1j n
Pn
iD1 jaij j

.
Second step:
If j0 is an index where the maximum occurs, i.e.
n
X
iD1
jaij0j D max
j
 n
X
iD1
jaij j
!
;
and x is the vector for which the j0th coordinate is equal to 1 and the other
coordinates are equal to 0 so that x D ej0 is the j0th unit vector, then one has
equality in (2.6). Hence,
kAk1 D max
j
 n
X
iD1
jaij j
!
:
According to this easily evaluated expression the matrix norm induced by the
Manhattan vector norm is also called the column sum norm.
Example
A D
0
@
3 4
1
2
0 1
2 5
3
1
A :
(2.7)
Here Pn
iD1 jaij j takes the values 7; 9; 5, if j D 1; 2; 3, respectively. Hence, kAk1 D 9. We
also ﬁnd a vector for which the equality holds in (2.4): according to the previous reasoning the
corresponding vector is x D e2, and then
Ax D
0
@
4
0
5
1
A ; so kAxk1 D j  4j C 0 C j  5j D 4 C 5 D 9 D 9kxk1 :
Now, let us consider the case p D 1:
First step: The equality
kAxk1 D max
1in j.Ax/ij D max
1in
ˇˇˇˇˇˇ
n
X
j D1
aij xj
ˇˇˇˇˇˇ

22
2
Norms, Condition Numbers
holds for all vectors x. Moreover, for all ﬁxed i
ˇˇˇˇˇˇ
n
X
j D1
aij xj
ˇˇˇˇˇˇ

n
X
j D1
jaij jjxj j 
0
@
n
X
j D1
jaij j
1
A max
1j njxj j D
0
@
n
X
j D1
jaij j
1
A kxk1
is valid. Summarizing both statements above we get
kAxk1 D max
1in
ˇˇˇˇˇˇ
n
X
j D1
aij xj
ˇˇˇˇˇˇ

0
@ max
1in
n
X
j D1
jaij j
1
A kxk1 :
(2.8)
This upper bound means, that
kAk1  max
i
0
@
n
X
j D1
jaij j
1
A :
Second step: If i0 is an index where the maximum occurs, that is,
n
X
j D1
jai0jj D max
i
0
@
n
X
j D1
jaij j
1
A ;
and x is a vector with its coordinates xj deﬁned as
xj WD
(
C1; if ai0j  0;
1; if ai0j < 0;
(2.9)
then kxk1 D 1 and .Ax/i0 D Pn
j D1 jai0jj. In addition, for i ¤ i0
j.Ax/ij 
n
X
j D1
jaij xj j D
n
X
j D1
jaij j 
n
X
j D1
jai0jj
holds, hence max1in j.Ax/ij D Pn
j D1 jai0jj D Pn
j D1 jai0jjkxk1 which means
kAk1 D max
i
0
@
n
X
j D1
jaij j
1
A :
This is why the maximum norm is also called the row sum norm.

2.2
The Induced Matrix Norm
23
Example
Consider again the matrix A in (2.7). Then the sum Pn
jD1 jaij j takes the values 8; 3; 10, if i D
1; 2; 3, respectively. Hence, kAk1 D 10.
Let us clarify which vector implies equality in (2.8): according to the arguments above
(see (2.9)) the corresponding vector is x D .1; 1; 1/T , and then
Ax D
0
@
8
3
10
1
A ; so kAxk1 D max.8; j  3j; 10/ D 10 D 10kxk1 :
If you are in doubt as to which p the column sum or the row sum induced
matrix norms correspond, then the following simple rule can help you: the 1
stands—as a column, the 1 lies—as a row.
2.2.3
Computation of the Induced Matrix Norm (p D 2)
Finally, consider the case p D 2. The derived matrix norm is useful for theoretical
considerations but—as it turns out—in practice it is difﬁcult to compute.
The norm kxk2 is connected with the Euclidean inner product
.x; y/ WD
n
X
iD1
xiyi ;
x; y 2 ICn;
(2.10)
(where yi is the conjugate complex value of yi) in the following way:
kxk2
2 D .x; x/:
With the inner product (2.10) the connection
.Ax; y/ D .x; AT y/
(2.11)
is valid for all real matrices A. We get this identity if we change the order of
summations, that is,
.Ax; y/ D
n
X
iD1
0
@
n
X
j D1
aij xj
1
A  yi
D
n
X
j D1
 n
X
iD1
aij yi
!
 xj
D
n
X
j D1
xj 
 n
X
iD1
aij yi
!
D .x; AT y/;

24
2
Norms, Condition Numbers
where AT is the transpose of A. In the expression Pn
iD1 aij yi the components of
the vector y are multiplied by the elements of the jth column of the matrix A—i.e.,
by the jth row of the transpose of A—but not by the elements of the jth row of A
since this would yield the product .Ay/j D Pn
iD1 ajiyi .
Applying the relation (2.11) to y D Ax we get
kAxk2
2 D .x; AT Ax/:
(2.12)
The matrix AT A is symmetric, that is, .AT A/T D AT A. Recall now an important
concept: a symmetric matrix B 2 IRnn is positive semideﬁnite, if the inequality
.Bx; x/  0
(2.13)
holds for all x 2 ICn. Accordingly, equality (2.12) shows that the matrix AT A is
positive semideﬁnite, and from this it follows that all eigenvalues of the matrix AT A
are non-negative. Denote by max the largest eigenvalue of the matrix AT A: max D
max.AT A/. We below derive the inequality
kAxk2
2 D .x; AT Ax/  maxkxk2
2
(2.14)
for all x 2 ICn—which implies 0  max and kAk2 
p
max.AT A/ .
The inequality (2.14) follows from a fact of linear algebra: since AT A is a
symmetric matrix, it can be diagonalized with an orthogonal matrix. This means
that there exists a matrix Q 2 IRnn satisfying QT Q D I D QQT , such that
QAT AQT D D D diag1in.i.AT A//
holds, where D is a diagonal matrix, and its main diagonal contains the eigenvalues
i of AT A. In the case of an orthogonal matrix Q, the equation
kxk2
2 D .x; x/ D .QT Qx; x/ D .Qx; Qx/ D kQxk2
2 ;
(2.15)
is valid, so the Euclidean length of a vector does not change when we multiply it by
Q. In our case
.x; AT Ax/ D .QT Qx; AT Ax/ D .Qx; QAT Ax/ D .y; QAT AQT y/
where y WD Qx, so that
.x; AT Ax/ D .y; Dy/ D
n
X
iD1
dijyij2  dmax
n
X
iD1
jyij2
D maxkyk2
2 D maxkQxk2
2 D maxkxk2
2

2.2
The Induced Matrix Norm
25
which gives (2.14). In the previous estimation, equality holds if x D vmax is
the eigenvector of AT A corresponding to the eigenvalue max having the largest
absolute value. Hence, for the Euclidean norm of a real matrix A we have
kAk2 D
p
max.AT A/:
(2.16)
The spectral radius is connected to the Euclidean matrix norm. Let A 2 IRnn
be a matrix, and denote by i.A/ the eigenvalues of A; i D 1; : : : ; n. The absolute
value of the eigenvalue having the largest absolute value is called spectral radius,
.A/ WD max
1in ji.A/j:
If the matrix A is symmetric, then its eigenvalues are real numbers, moreover, the
eigenvalues of the matrix AT A D A2 are easy to get from the eigenvalues  D .A/
of the original matrix: AT Ax D A2x D A.Ax/ D A.x/ D Ax D 2x. Thus in
this case
kAk2 D
p
max.AT A/ D
q
2max.A/ D jmax.A/j D .A/;
meaning that now the spectral radius is also a norm.
As an example consider the matrix
A D
3 4
0 0

:
Here you can immediately obtain that kAk1 D 4; kAk1 D 7. Moreover, because
of the special form of A (which is a so-called upper triangle matrix), it is known
that the eigenvalues are the elements of the main diagonal: one of the eigenvalues is
equal to 3, the other is 0, and therefore .A/ D 3.
Calculating the matrix
AT A D
 9 12
12 16

;
we can ﬁnd its eigenvalues, which are 25 and 0 so that kAk2 D
p
25 D 5. The
eigenvector of AT A corresponding to the eigenvalue 25 is x D .3; 4/T , and indeed
Ax D
25
0

; kAxk2 D 25 D 5  5 D 5kxk2 D kAk2kxk2 :
As a second example take again the matrix A in (2.7). Then kAk1 D 9 and
kAk1 D 10.

26
2
Norms, Condition Numbers
In this case the determination of the eigenvalues of A requires a longer calcu-
lation. The eigenvalues are approximately 6.425, 1.608, 2:033, and so .A/ 
6:425. Moreover,
AT A D
0
@
17 22
11
22
41 19
11 19
11
1
A ;
and the eigenvalues of this matrix are approximately 63:279; 3:962; 1:759, hence,
ﬁnally, kAk2 
p
63:279  7:955.
This example shows that the maximum and Manhattan norms are often more
practical for direct calculations than the Euclidean norm.
Also, it can be seen in both examples that the spectral radius is less than the three
norms. As the reasoning below shows, this is not accidental. First, let k  k be an
arbitrary vector norm, moreover, let  D .A/ be some eigenvalue of A, and v be
the corresponding eigenvector. Then
jj  kvk D kAvk  kAk  kvk H) jj  kAk;
(2.17)
since kvk > 0. This is also true for the maximal (by absolute value) of the
eigenvalues so that we get
.A/  kAk;
and here the norm may be any induced norm of A.
2.3
Error Estimations
We continue our way to the numerical solution of a linear system Ax D b by
tackling ﬁrst the question whether it makes sense to try it at all.
Having norms already at our disposition, consider the linear system (2.1) in the
case of a regular n  n matrix A, and of a vector b ¤ 0. Taking the norms of both
sides we get
0 < kbk D kAxk  kAk kxk:
(2.18)
2.3.1
The Right-Hand Side of the Linear System is Perturbed
Let us estimate the error of the solution if instead of b a perturbed vector b C ıb is
given, ıb representing the error of the right-hand side. Let y D xC.yx/ DW xCıx
be the solution of the equation. Hence, ıx D y  x and
A.x C ıx/ D b C ıb:
Then b C ıb D A.x C ıx/ D b C Aıx, and in this way ıb D Aıx. Because of
the regularity of A, that is, it has an inverse, we can also write (at least formally: the

2.3
Error Estimations
27
calculation of the inverse matrix is usually much more expensive than solving the
system) ıx D A1ıb. Taking again the norm we obtain
kıxk D kA1ıbk  kA1k kıbk:
(2.19)
This inequality shows that kA1k kıbk is an upper bound for the absolute error of
the solution x.
To estimate the relative error (which usually is more interesting) we use ﬁrst
inequality (2.19) and then (2.18) (where kAk > 0 since A ¤ 0 being regular):
kıxk
kxk  kA1k kıbk
kxk
 kA1k kıbk
kbk=kAk
D kAk  kA1k  kıbk
kbk :
(2.20)
On the right-hand side stands the relative error of the vector b (it can be, e.g., the
rounding error), while on the left-hand side the relative error of the solution appears.
The estimations (2.18), (2.19) are strict because there exist both a nonzero vector
x (in other words: a vector b) and a nonzero vector ıb such that
kAxk D kAk kxk; and kA1ıbk D kA1k kıbk
hold where the second relation means strictness of (2.19). Further,
kıxk
kxk
D
kA1k kıbk
kxk
D kAk  kA1k  kıbk
kbk which shows that estimation (2.20) is also strict.
2.3.2
Condition Number
The expression kAk  kA1k appearing in Eq.(2.20) is called the condition number
of the matrix A and denoted by
cond.A/ WD kAk  kA1k:
According to the calculation in the preceding subsection, the condition number tells
us exactly how much larger the relative error of the solution of the linear system
can be than the relative error of the right-hand side vector. Hence, the knowledge
of the condition number will be critical from the point of view of the numerical
solvability of the system. However, it is hard to compute because ﬁrst we would
have to compute the inverse matrix.
Properties:
1. The condition number depends on the matrix norm, and as the examples below
show, it also depends on the vector norm inducing it. In this way, if necessary,
we use a corresponding subscript: condp.A/ D kA1k.p/kAk.p/, p D 1; 2; 1.
2. The condition number cannot be less than 1. In the case of the induced norms
kIk D 1, because from (2.2) it follows that
kIk D max
x¤0
kxk
kxk D 1:

28
2
Norms, Condition Numbers
Hence, taking into account also (2.5), 1 D kIk D kAA1k  kAk kA1k D
cond.A/. In other words, we usually must expect that the relative error of the
solution increases compared to the relative error of the right-hand side.
3. In the special case if A D Q is an orthogonal matrix, cond2.Q/ D 1, see (2.15).
Then the relative error corresponding to the Euclidean norm cannot increase
during the solution of the linear system.
4. For a regular matrix A, 0 is not an eigenvalue. Then, for any eigenvector v,
Av D v H) 1
v D A1v; i.e. 1.A/ D .A1/:
Now, denote by min.A/ and max.A/ the eigenvalues of A which have the
smallest and largest absolute values, respectively. We can apply the reasoning
of (2.17) for the inverse matrix too:
kAk  jmax.A/j; kA1k  jmax.A1/j D
1
jmin.A/j:
From this it follows that
cond.A/ 
ˇˇˇˇ
max.A/
min.A/
ˇˇˇˇ :
On the basis of the above inequality the condition number can be estimated
from below using the methods described in the chapter dealing with eigenvalue
problems.
5. We take the case that the relative error of the right-hand side vector be equal to
the rounding error: kıbk=kbk D "1 D a1t, see (1.6) in Sect. 1.3, and
cond.A/  1
"1
:
Then (2.20) shows that the relative error of the solution can be unacceptably large:
since cond.A/ kıbk
kbk  1, it may occur that kıxk  kxk. In this case we call the linear
system (and the matrix) ill-conditioned.
To guarantee at least one correct digit in the solution, the inequality
cond.A/ 
1
a"1
has to be satisﬁed, because then from (2.20) and from kıbk=kbk  "1 it follows
kıxk
kxk  1
a:

2.3
Error Estimations
29
Examples
If the matrix is a regular 2  2 matrix, then the condition number can be calculated without any
problem. Let the determinant of the matrix
A D
 a b
c d

be different from zero: det.A/ D ad  cb ¤ 0. Then the inverse of A is
A1 D
1
ad  cb
 d b
c
a

;
and hence, for example,
cond1.A/ D max.jaj C jbj; jcj C jdj/max.jdj C jbj; jcj C jaj/
jad  cbj
D kAk1kAk1
j det.A/j
:
But for the example A D
 3 4
0 0

, the condition number is not deﬁned (independently of the vector
norm used) because of the singularity of the matrix.
If the matrix is
A D

3 4
1 2

;
then kAk1 D 7; det.A/ D 10 and
A1 D 1
10
 2 4
1
3

:
So kA1k1 D
6
10 D 3
5 and cond1.A/ D 21
5 .
For which vector x does the equality kAxk1 D kAk1kxk1 hold? According to our result
in Sect. 2.2.2, the corresponding vector is x D .1; 1/T —and then b D Ax D .7; 1/T . Similarly,
kA1ıbk1 D kA1k1kıbk1, if ıb D .1; 1/T —and then ıx D A1ıb D
1
10
 6
2

D 1
5
 3
1

.
If instead of ıb we have t  ıb and instead of ıx the vector t  ıx with some parameter t, the
equalities remain valid: A.t  ıx/ D t  ıb and kA1t  ıbk1 D kA1k1kt  ıbk1. Hence, if the
original and the perturbed systems are
Ax D

3 4
1 2

x D
 
7
1
!
;
and
A.x C t  ıx/ D

3 4
1 2

.x C t  ıx/ D
 
7 C t
1  t
!
;
(2.21)
respectively, we have the equality
kt  ıxk
kxk
D kAk  kA1k  kt  ıbk
kbk
:

30
2
Norms, Condition Numbers
Substituting the calculated values into the equation we get
kt  ıxk
kxk
D jtj3=5
1
D jtj3
5 D 21
5  jtj
7 D 21
5  kt  ıbk
kbk
:
This means that—while t ¤ 0—during the solution of the system (2.21) the relative error of the
solution may increase by 21=5, independently of t, i.e. independently of having a small or large
relative error on the right-hand side.
In the case of the matrix (2.7) you can readily check that
A1 D 1
21
0
@
5 7 4
4 7 1
10 7
8
1
A ; so kA1k1 D 1; kA1k1 D 25
21:
Hence, cond1.A/ D 9  1 D 9; cond1.A/ D 10  25
21 D 250
21  11:905, and after a
longer calculation we get that cond2.A/  7:955 	 0:754  5:998.
Even small matrices can have large condition numbers as the following example
shows. Let A.t/ be the matrix
A.t/ WD
1
t
t 1 C t2

;
(2.22)
which depends on the parameter t. Then det.A/ D 1 holds, independently of t, and
A.t/1 WD
1 C t2 t
t
1

:
Hence, kA.t/k1 D max.jtj C 1 C t2; 1 C jtj/ D 1 C jtj C t2 D kA.t/1k1, so
cond1.A.t// D .1 C jtj C t2/2. For a proper t this can be very large, for example
if t D 100, then cond1.A.t// D 102;030;201 > 108.
A famous example of this phenomenon is the so-called Hilbert matrix
Hn D

1
i C j  1
n
i;j D1
:
As a speciﬁc case take n D 5, so
H5 D
0
BBBBB@
1
1=2 1=3 1=4 1=5
1=2 1=3 1=4 1=5 1=6
1=3 1=4 1=5 1=6 1=7
1=4 1=5 1=6 1=7 1=8
1=5 1=6 1=7 1=8 1=9
1
CCCCCA
:

2.3
Error Estimations
31
Then cond1.H5/  9:437105; cond2.H5/  4:766105 (here min  3:288106,
max  1:567) and cond1.H12/  3:798  1016:
In the case of the matrix H12 the condition number is so large that the rounding
error (regardless whether using single or double precision computation) of the right-
hand side vector itself can cause a huge error in the exact solution of the Eq. (2.1).
Obviously, in this case no solution algorithm can give an acceptable result; the
matrix is indeed ill-conditioned.
We have arrived at the most important observation of the present chapter:
for a given computer, linear system and error level, it is not the determinant
but the condition number that determines the numerical solvability of the
system.
Surely, if the determinant is equal to zero, the condition number is not deﬁned.
On the other hand, if det.A/ differs from zero, then the magnitude of the condition
number of A does not depend on the value of the determinant: for example multiply-
ing the system (2.1) by the power 10k yields a 10kn times larger determinant, while
the condition number does not change. We can see the same in example (2.22): the
determinant is equal to 1, independently of t, while the condition number can take
arbitrarily large values.
The matrices arising in practice are typically ill-conditioned, while the matrices
constructed from random numbers are most often well-conditioned.
A good program package, like MATLAB, will issue a warning if encountering
a linear system with an ill-conditioned matrix.
2.3.3
The Matrix of the Linear System is Perturbed
Now we examine the case where the elements of the matrix are perturbed:
B WD A C ıA; By D b; y D x C ıx:
(2.23)
Here, we need a condition for the matrix A which ensures that the perturbation ıA
is small enough such that both A and B are non-singular.
Consider ﬁrst a simpler question: what kind of sufﬁcient condition can be given
for the matrix R to ensure regularity of I C R ?
The answer to this question is the perturbation lemma: when S WD I C R and
kRk DW q < 1 in some induced norm then S is regular, and
kS1k  1=.1  q/:

32
2
Norms, Condition Numbers
To understand that this is true, we proceed as follows:
(a) First, from the triangle inequality kx C yk  kxk C kyk you have
kxk  kx C yk  kyk:
Replacing x C y by z (that is x D z  y) you get the so-called inverse triangle
inequality
kz  yk  kzk  kyk
where vectors x; y, and hence vectors y and z, are arbitrary.
(b) In the inverse triangle inequality, take z WD x, y WD Rx:
kSxk D kx C Rxk  kxk  kRxk  kxk  qkxk D .1  q/kxk:
(Here the second inequality is a consequence of kRxk  kRkkxk.) Hence, there
does not exist a vector x ¤ 0 for which Sx D 0 holds (otherwise kSxk D 0 and
.1q/kxk  0 would follow). But this means that S is regular. Substitute x D S1z
(that is Sx D z) into the inequality to get:
kzk  .1  q/kS1zk;
and you arrive at the estimation of kS1k using (2.4).
When 1 is an eigenvalue of the matrix R and the corresponding eigenvector is
v ¤ 0, then
q D kRk  .R/ D jmax.R/j  1;
so the condition of the perturbation lemma is not fulﬁlled. And really, in this case 0
is an eigenvalue of I C R: .I C R/v D v C .v/ D 0, and I C R is singular.
Next, with the help of the perturbation lemma we obtain a sufﬁcient condition
which ensures that in case of regularity of the error-free matrix A the matrix B D
A C ıA of the perturbed system (2.23) is also regular.
Since B D A.I C A1ıA/ holds and so
det.B/ D det.A/ det.I C A1ıA/;
matrix B is regular if and only if so is matrix I C A1ıA. Moreover, based on the
perturbation lemma, the condition
kıAk < 1=kA1k
(holding for some induced matrix norm) is sufﬁcient to ensure the regularity of I C
A1ıA because in this case from (2.5) it follows that kA1ıAk  kA1k kıAk < 1.

2.4
Exercises
33
We can also estimate the norm of matrix B1 starting from B1 D .I C
A1ıA/1A1. Then
kB1k  k.I C A1ıA/1k kA1k 
kA1k
1  kA1k kıAk:
(2.24)
After this preparation we can estimate the difference of the solutions of sys-
tems (2.1) and (2.23). Since
Ax D b D By D Bx C Bıx D b C ıAx C Bıx;
there holds ıx D B1ıAx, and according to (2.24)
kıxk  kB1kkıAkkxk 
kA1k kıAk
1  kA1k kıAkkxk;
that is
kıxk
kxk 

1   ;  WD cond.A/kıAk
kAk :
(2.25)
In this way the relative error of the solution can be expressed by the condition
number and by the relative error of the data (in our case the latter is kıAk=kAk).
Later we will see that the condition number plays an important role whenever
we work with matrices: for example in eigenvalue problems and in the solution of
systems of ordinary linear differential equations.
2.4
Exercises
1. Compute the maximum and Manhattan norm of the following matrix:
A D
0
@
1
2
3
4 5
6
7
8 9
1
A :
Find also the vectors x satisfying kAxkp D kAkpkxkp, p D 1; 1.
2. Compute the maximum norm and the condition number of the matrix
A D
2 7
6 13

:
For which vectors x; y do the equalities kAxk1
D
kAk1kxk1 and
kA1yk1 D kA1k1kyk1 hold?

34
2
Norms, Condition Numbers
3. You know the condition for
A D
 a b
c d

to be regular. Give an expression for cond1.A/.
4. Compute the condition numbers of the following two matrices. (The norm can
be chosen arbitrarily.)
A D
2 5
0 0

;
B D
 " 0
0 "1

; 0 < " < 1:
5. Let the matrix A be
A D
0
@
5 3 6
6 2 3
4 2 5
1
A : Then A1 D 1
10
0
@
4
3
3
18 1 21
4 2
8
1
A :
Compute the maximum and Manhattan norms of A, and the corresponding
condition numbers.
6. For a real n  n matrix A D .aij / 2 IRnn, derive the following inequalities:
kAkp  n  max
i;j jaij j;
p D 1; 1:
7. If A is the regular matrix given by (2.7) and ıA is a perturbation of it, then give
an upper bound for kıAk1 ensuring the regularity of A C ıA !
8. The following matrix depends on a real parameter t:
A D A.t/ WD I4 C t 
0
BB@
3
2 7 2
0 5
3 1
4
0 9 6
2
8 4 5
1
CCA ;
and I4 is the 4  4 unit matrix. For which values of t can you be sure of the
regularity of A.t/ ?
9. Show that if
0 < m  kAxk
kxk  M;
8x ¤ 0 2 IRn;
holds for some vector norm, then the condition number of A is well deﬁned,
and cond.A/  M=m.
10. Find out which norm corresponds to the MATLAB function norm when called
norm(x) for a vector x, and norm(A,inf) for a matrix A.
11. Compare the MATLAB values obtained from norm(A)*norm(x) and
norm(A*x) when x=[1 -1]’ and A=[1 2; 3 4].

2.4
Exercises
35
12. Take the 6  6 “built-in” Hilbert matrix as A and explain and compare the
MATLAB numbers cond(A,1),rcond(A),cond(A,1)*rcond(A).
13. Consider a 1000  1000 random matrix and using the commands tic and
toc examine how long it takes to compute its 2-, 1-, 1- and Frobenius-norm,
respectively.
14. Construct the following matrix A 2 IR100100 and vector b 2 IR100, and using
the backslash operator in MATLAB solve the system Ax D b (i.e., apply the
command x D Anb). After that perturb the vector b, e.g. let b.100/ D 1:00001
instead of 1 and solve the system again. Compute the condition number of A
(also give an exact formula for cond1.A/).
aij D
8
ˆˆ<
ˆˆ:
1;
if i D j;
1;
if i < j;
0;
otherwise,
b D .98; 97; : : : ; 0; 1/T :

3
Solution of Systems of Linear Equations
You know from Algebra:
If A 2 IRnn is a regular matrix, then the linear system Ax D b can be uniquely solved (e.g.,
with the help of Cramer’s rule).
The following conditions are equivalent to the non-regularity (that is, to the singularity) of A:
1. det.A/ D 0,
2. system Ax D b cannot be solved for all vectors b,
3. there exists a vector b for which the linear system has inﬁnitely many solutions,
4. there is a vector x ¤ 0 such that Ax is the zero vector: Ax D 0,
5. 0 is an eigenvalue of the matrix A: Ax D 0  x, where x ¤ 0.
3.1
Gaussian Elimination
In this chapter we deal with solving the linear system Ax D b. Assume that A 2
IRnn is a given regular matrix, b 2 IRn is a given (column) vector, and x 2 IRn is
the unknown (column) vector to be found.
In practice Cramer’s rule is not used even in the case of n D 2, because it costs
more operations than Gaussian elimination which we discuss below. First, we write
the system Ax D b in detailed form:
a11x1 C a12x2 C    C a1nxn D b1
a21x1 C a22x2 C    C a2nxn D b2
(3.1)
:::
:::
:::
an1x1 C an2x2 C    C annxn D bn
To start, we aim to eliminate x1 and assume that a11 ¤ 0. Then, we subtract the
ﬁrst equation multiplied by .ai1=a11/ from the ith equation, i > 1. As a result,
© Springer International Publishing AG 2016
G. Stoyan, A. Baran, Elementary Numerical Mathematics for Programmers and
Engineers, Compact Textbooks in Mathematics, DOI 10.1007/978-3-319-44660-8_3
37

38
3
Solution of Systems of Linear Equations
the coefﬁcient of x1 in the ith row will be zero, that is, the unknown x1 is indeed
eliminated there, while the ﬁrst row is suitable to solve later for x1 in terms of the
other unknowns.
Denote by aij DW a.1/
ij
the elements of A and by a.2/
ij , i > 1, the new elements
obtained after the subtraction. Then the previous operations can be described using
the following expressions:
a.2/
ij
WD a.1/
ij  `i1a.1/
1j ;
i; j D 2; : : : ; n;
(3.2)
where `i1 WD a.1/
i1 =a.1/
11 ;
i D 2; : : : ; n:
(3.3)
Here, i D 1 is excluded because our purpose was just to obtain a.2/
i1 D 0 for i > 1.
Further, it makes no sense to calculate these zeros in a loop so that j > 1.
Then, if
b.1/
1
WD b1 and b.2/
i
WD b.1/
i
 `i1b.1/
1 ;
i D 2; : : : ; n;
(3.4)
after the ﬁrst step of this “Gaussian” elimination we have the following system:
a.1/
11 x1 C a.1/
12 x2 C    C a.1/
1n xn D b.1/
1
a.2/
22 x2 C    C a.2/
2n xn D b.2/
2
(3.5)
:::
:::
:::
a.2/
n2 x2 C    C a.2/
nn xn D b.2/
n
Now, if a.2/
22
¤ 0 holds, then you can perform similar steps as before for the
.n  1/  .n  1/ system standing under the ﬁrst row: with the help of the second
row, you can eliminate the unknown x2 from the following rows.
To write this down similarly as above, denote the elements of the new submatrix
and the components of the right-hand side as follows:
a.3/
ij
WD a.2/
ij  `i2a.2/
2j ;
i; j D 3; : : : ; n;
where `i2 WD a.2/
i2 =a.2/
22 ;
i D 3; : : : ; n;
and b.3/
i
WD b.2/
i
 `i2b.2/
2 ;
i D 3; : : : ; n:
As earlier j D 1, now j D 2 was omitted. Moreover, the unknown x2 can be
expressed with the help of the second row and unknowns x3; : : : ; xn.

3.1
Gaussian Elimination
39
In general, the kth step of Gaussian elimination can be described by the
following formulae—if a.k/
kk ¤ 0 holds:
a.kC1/
ij
WD a.k/
ij
 `ika.k/
kj ;
i; j D k C 1; : : : ; n;
where `ik WD a.k/
ik =a.k/
kk ;
(3.6)
b.kC1/
i
WD b.k/
i
 `ikb.k/
k ;
i D k C 1; : : : ; n:
These relations can be obtained from (3.2)–(3.4) by writing k instead of all “1”s, and
kC1 instead of all “2”s. After the n1 steps described by (3.6) for k D 1; : : : ; n1,
we have the following system:
a.1/
11 x1 C a.1/
12 x2 C : : :
   C a.1/
1n xn D b.1/
1
a.2/
22 x2 C : : :
   C a.2/
2n xn D b.2/
2
a.3/
33 x3 C    C a.3/
3n xn D b.3/
3
(3.7)
:::
:::
:::
a.n/
nn xn D b.n/
n
In fact, with these steps the elimination is completed and we have obtained an “upper
triangular matrix” U and a right-hand side eb such that Ux D eb.
If in the last equation of (3.7) unn WD a.n/
nn ¤ 0 holds, then we easily get the
solution x: ﬁrst the unknown xn D b.n/
n =a.n/
nn , then one after the other the unknowns
xn1; : : : ; x1. This straightforward procedure of computing U 1eb is called back
substitution.
By the previous two steps, i.e. the elimination of the unknowns x1; : : : ; xn and
back substitution, we have described Gaussian elimination—in a narrow sense.
As an example consider the following system:
Ax D b;
A D
0
@
4 1 1
1 4 1
1 1
4
1
A ; b D
0
@
9
12
11
1
A :
(3.8)
Performing Gaussian elimination we use the same row operations for the
elements of the matrix and for the right-hand side, hence, we can use the augmented
matrix:
ŒA; b D
0
@
4 1 1
9
1 4 1 12
1 1
4
11
1
A :

40
3
Solution of Systems of Linear Equations
First, you subtract the ﬁrst row multiplied by 1
4 D 1
4 (i.e. `21 D 1
4) from the second
row, then you subtract the ﬁrst row multiplied by
1
4 D  1
4 (`31 D  1
4) from the
third row getting
ŒA; b
`21D 1
4 ; `31D 1
4
H)
0
@
4 1 1 9
0 15
4  3
4  39
4
0
5
4
15
4
35
4
1
A :
Next, you take the (transformed) second row and subtract it, multiplied by 5=4
15=4 D 1
3
(i.e. `32 D 1
3), from the third row:
0
@
4 1 1 9
0 15
4  3
4  39
4
0
5
4
15
4
35
4
1
A
`32D 1
3
H)
0
@
4 1 1 9
0 15
4  3
4  39
4
0 0
4
12
1
A D ŒU;eb:
Now, the elimination of the unknowns is completed, and you can begin the back
substitution, i.e. compute x D U 1eb :
x3 D 12
4 D 3;
x2 D 4
15

39
4 C 3
4  x3

D 30
15 D 2;
(3.9)
x1 D 1
4 .9  x2 C x3/ D 1
4 .9 C 2 C 3/ D 4
4 D 1:
This gives
Ax D
0
@
4 1 1
1 4 1
1 1
4
1
A
0
@
1
2
3
1
A D
0
@
9
12
11
1
A :
3.2
When Can Gaussian Elimination be Performed?
Above we needed the conditions a.k/
kk ¤ 0 to perform the elimination. Let us clarify
what these conditions mean. According to our formulae, e.g.,
u22 D a.2/
22 D a.1/
22  a.1/
21 a.1/
12
a.1/
11
D a.1/
11 a.1/
22  a.1/
21 a.1/
12
a.1/
11
:

3.2
When Can Gaussian Elimination be Performed?
41
On the right-hand side is the ratio of the second and ﬁrst leading principal minor,
that is, the ratio of the following two determinants:
det
 a11 a12
a21 a22

and det.a11/:
Now remember that determinants are left unchanged if one takes linear combina-
tions of their rows—and we took such a combination of the ﬁrst and second row of
system (3.1) to obtain the second row of the system (3.5). Hence, conﬁning to the
second leading principal minor,
det
 a11 a12
a21 a22

D det
 
a.1/
11 a.1/
12
0 a.2/
22
!
D a.1/
11 a.2/
22 D a.1/
11 u22:
Assuming that a.1/
11 .D u11/ ¤ 0 the element a.2/
22 can be expressed from here, and
this condition is fulﬁlled if the second leading principal minor det
a11 a12
a21 a22

is not
equal to zero. Then we can continue the elimination.
Similarly, after the .k  1/th step we can use that the kth leading principal minor
is unchanged,
det
0
B@
a11 : : : a1k
:::
:::
:::
ak1 : : : akk
1
CA D det
0
BBBB@
a.1/
11 : : : : : : a.1/
1k
0
:::
:::
:::
0
0
:::
:::
0 : : : 0 a.k/
kk
1
CCCCA
D
k
Y
iD1
a.i/
ii
D a.k/
kk 
k1
Y
iD1
a.i/
ii D a.k/
kk  det
0
B@
a11
: : : a1;k1
:::
:::
:::
ak1;1 : : : ak1;k1
1
CA ;
and a.k/
kk D ukk .
In order to reach this point we require k  1 conditions to be fulﬁlled: all the
ith leading principal minors .i D 1; : : : ; k  1/ have to differ from zero. If the
kth leading principal minor is nonzero, too, then a.k/
kk
¤ 0, and we can continue
the elimination. Hence, for the complete elimination, conditions a.k/
kk
¤ 0; k D
1; : : : ; n1 are needed, while the regularity of A is necessary only for the beginning
of the back substitution because
det.A/ D a.n/
nn  det
0
B@
a11
: : : a1;n1
:::
:::
:::
an1;1 : : : an1;n1
1
CA :

42
3
Solution of Systems of Linear Equations
We see from here that if the .n  1/th leading principal minor is not equal to zero,
then det.A/ differs from zero if and only if a.n/
nn ¤ 0.
To summarize, we can state that a linear system can be solved by Gaussian
elimination exactly in the case when all the leading principal minors are
different from zero:
det
0
B@
a11 : : : a1k
:::
:::
:::
ak1 : : : akk
1
CA ¤ 0;
k D 1; : : : ; n:
(3.10)
Here the ﬁrst n  1 conditions ensure that the elimination can be completed and the
last one ensures that the back substitution can be started. The back substitution can
then be continued, once more due to the ﬁrst n  1 conditions.
It is not difﬁcult to calculate that in example (3.8) the leading principal minors
are in order 4; 15; 60, so each of them is nonzero.
According to our result (3.10), Gaussian elimination—in the form described
above—often cannot be completed, even when the system (3.1) has a unique
solution, since in the latter case only the nth condition is necessary.
In practice the n conditions described above are hard to check before solving the
system. For this reason, during the computations we always check whether a.k/
kk ¤ 0.
However, there exist classes of matrices for which the conditions (3.10) are always
satisﬁed, see, e.g., Sect. 3.7.
3.3
The LU Factorization
From the point of view of programming it is advantageous to describe the steps of
Gaussian elimination using matrix operations.
The transition from (3.1) to (3.5) can be described in the following way:
Ax D b
H) A2x D L1b; A2 WD L1A1; A1 WD A;
(3.11)
where matrix L1 has a special form: it differs from the unit matrix only in the ﬁrst
column—namely, it contains the quotients `i1 deﬁned in (3.3):
L1 WD
0
BBBBBBB@
1
0 : : : 0
`21 1 0 : : 0
:
0 1 0 : 0
:
: : : : :
:
: : : 1 0
`n1 0 : : 0 1
1
CCCCCCCA
:

3.3
The LU Factorization
43
We see from this lower triangular form of L1 that it is non-singular since
det.L1/ D 1. If we write A D A1 as a block matrix:
A1 D
 
a11 a1
c1 A.1/
22
!
(3.12)
where a1 resp. c1 are the corresponding (.n  1/-dimensional) row resp. column
vectors:
a1 WD .a12; : : : ; a1n/;
c1 WD .a21; : : : ; an1/T
and A.1/
22 2 IR.n1/.n1/ contains the elements standing in the right lower part of A,
then the relations (3.2) take a simple matrix form:
A.2/
22 D A.1/
22  c1a1=a11 :
(3.13)
Thus, A.2/
22 contains the new elements arising in place of A.1/
22 .
The continuation of the Gaussian elimination can be described similarly as
in (3.11):
A2x D L1b
H) A3x D L2L1b; A3 WD L2A2 D L2L1A
(3.14)
where
L2 WD
0
BBBBBBB@
1
0
: : : : 0
0
1
0 : : : 0
0 `32 1 0 : : 0
:
:
: : : : :
:
:
: : : 1 0
0 `n2 0 : : 0 1
1
CCCCCCCA
(3.15)
is non-singular like L1, and
`i2 WD a.2/
i2 =a.2/
22 ;
i D 3; : : : ; n:
The ﬁrst row and column of the matrix L2 are from the unit matrix corresponding to
the fact that we ignore the ﬁrst row of A2 and the unknown x1. The remaining part
of the matrix is the same as L1 but its size is only .n  1/  .n  1/.
Generally, in the kth elimination step (k D 1; 2; : : : ; n  1)
Akx D Lk1    L1b
H) AkC1x D Lk    L1b;
(3.16)
AkC1 WD LkAk D Lk    L1A:

44
3
Solution of Systems of Linear Equations
That is, from Ak we produce new n  n matrices: Lk and AkC1 where
Lk WD
0
BBBBBBB@
Ik1
0
: : : 0
0
1
0 : : 0
:
`kC1k 1 0 : 0
:
:
0 1 : 0
:
:
: : : :
0
`nk
0 : 0 1
1
CCCCCCCA
and Ik1 is the .k  1/  .k  1/ unit matrix, moreover
`ik WD a.k/
ik =a.k/
kk ;
i D k C 1; : : : ; nI k D 1; : : : :n  1:
Lk is non-singular for the same reason as L1, and its inverse differs from Lk only
in the signs of the quotients `ik:
L1
k
WD
0
BBBBBBB@
Ik1
0
: : : 0
0
1
0 : : 0
:
`kC1k 1 0 : 0
:
:
0 1 : 0
:
:
: : : :
0
`nk
0 : 0 1
1
CCCCCCCA
:
(3.17)
Now, assume, that the Gaussian elimination can be completed, that is the leading
principal minors differ from zero. The result (3.7) of the elimination considering
only the changes in A is
An D
0
BBBB@
a.1/
11 : : : : : : a.1/
1n
0 a.2/
22
::: a.2/
2n
0
0
:::
:::
0
: : : 0 a.n/
nn
1
CCCCA
DW U D
0
BBBB@
u11 : : : : : : u1n
0 u22
::: u2n
0
0
:::
:::
0 : : : 0 unn
1
CCCCA
;
(3.18)
the upper triangular matrix, and looking back you see that
An D U D Ln1    L1A:
(3.19)
Using (3.17) and direct calculations, you get
.L1/1    .Ln1/1 D
0
BB@
1 0
:
0
`21 1
:
:
:
:
:
0
`n1 : `nn1 1
1
CCA DW L:
(3.20)

3.3
The LU Factorization
45
The matrix L is called a lower triangular matrix. This matrix is normalized: `ii D 1
for all i. Finally, from (3.19) and (3.20) we obtain
A D LU
(3.21)
which is the LU factorization of the matrix A. Thus, if during the elimination we
store the multipliers `ik then at the end we have both matrix U and matrix L, that is,
both parts of the LU factorization. In this sense the elimination step of the Gaussian
algorithm is equivalent to LU factorization. Hence, for the computation of the LU
factorization the ﬁrst n  1 conditions of (3.10) are sufﬁcient.
In example (3.8) above
U D
0
@
4 1 1
0 15
4  3
4
0 0
4
1
A ;
(3.22)
and taking into consideration the quotients `ij used by the elimination we get
L D
0
@
1 0 0
1
4 1 0
 1
4
1
3 1
1
A :
(3.23)
With these matrices, the decomposition A D LU is valid.
Now, using the LU factorization, we get the solution of the system
Ax D LUx D L(Ux) D b
by solving two systems with triangular matrices:
Ly D b;
(3.24)
Ux D y:
(3.25)
For the Gaussian elimination, we obtain the solution y of the ﬁrst system at the same
time as the multiplier U , and y D eb of (3.7).
To solve the second system, the last condition in (3.10) is also necessary.
Equations (3.16) and (3.24)–(3.25) show a way to solve the system of equa-
tions subdividing the task between subprograms:
the ﬁrst module performs the LU decomposition according to (3.16) for k D
1; : : : ; n  1, the second and third modules solve (3.24) and (3.25), correspond-
ingly.
This method is useful not only during the testing phase of the whole program
but also if we have to solve several systems with the same matrix A and different
right-hand sides b.

46
3
Solution of Systems of Linear Equations
In the latter case, since Gaussian elimination—taken in the narrow sense—is not
storing the multiplier L but producing U andeb, it will compute the same U for every
new b and hence will perform a costly procedure—see the next section—again and
again.
3.4
Algorithms, Cost of Solving
Now, we are going to deal with the practical part of the solution of systems,
assuming that conditions (3.10) are fulﬁlled.
(Pseudo-code of the solution algorithm, after getting rid of conditions (3.10),
follows in Sect. 3.6.1.)
One of the possibilities we have already shown is the Gaussian elimination when
we produce the LU factorization with normalized lower triangular matrix L and
upper triangular matrix U and then solve the triangular systems.
The main feature of this approach is that in the kth step we compute the elements
`ik .i > k/ and ukj .j  k/, and we go over all remaining elements of the matrix
Ak (i.e. a.k/
ij ; k < i; j/ once.
There are further algorithmic solutions creating the LU factorization directly
starting from the formula
aij D
n
X
kD1
`ikukj D
min.i;j /
X
kD1
`ikukj :
(3.26)
Here the minimum comes from the zeros `ik D 0 if k > i, and ukj D 0 if k > j.
Consider the formula ﬁrst in the case i D j. Then
aii D
iX
kD1
`ikuki D `iiuii C
i1
X
kD1
`ikuki ;
from which you can see that the LU factorization is not uniquely determined:
you can choose, e.g., either `ii D 1 (which corresponds to the usual Gaussian
elimination), or uii D 1 for all i. In the ﬁrst case you get uii from the formula,
while in the second case you obtain `ii.
In addition to the method of the Gaussian elimination there are two further
obvious possibilities (each with its merits) to use formulae (3.26):
(a) calculating the elements of L and U column by column;
(b) calculating the elements row by row.
For the realization of the ﬁrst possibility, take `ii D 1. Then for a ﬁxed j ﬁrst you
calculate from (3.26) the elements uij .i  j/ and then the elements `ij .i > j/.
In the second version, assume ujj D 1, and for a ﬁxed i you ﬁrst calculate
from (3.26) the elements `ij .j  i/ and after them the elements uij .j > i/.

3.4
Algorithms, Cost of Solving
47
Let us observe that in the ﬁrst (or second) case we do not need the columns
(or rows) of A with indices greater than j (i) to calculate the ith columns (rows)
of L and U . This is important if the matrix is too large to be stored in the primary
(working) memory and is stored in secondary storage, or in the case if these columns
(rows) are still to be determined or even are unknown yet.
In both cases it is worth storing the multipliers 1=ujj and 1=`ii appearing during
the computation. The values `ij and uij can be stored in the place of the aij if the
original matrix A is not needed anymore.
In the following matrix we show the position of the information needed to
calculate the element `ij .i > j/ for row-by-row calculation:
0
BBBBBBBBBBBBBBBBBBB@
u11 u12   
u1j
  
u1n
`21 u22
u2j
u2n
`31
:::
:::
:::
:::
:::
ujj
:::
`i1 `i2    `i;j1 aij ! `ij ai;j C1    aii   
ain
aiC1;1
aiC1;j
aiC1;n
:::
:::
:::
an1
anj
ann
1
CCCCCCCCCCCCCCCCCCCA
In the matrix below the same can be seen for the computation of elements
uij .i  j/.
0
BBBBBBBBBBBBBBBBBBBBB@
u11 u12   
u1j
  
u1n
`21 u22
u2j
u2n
`31
:::
:::
:::
:::
:::
ui1;j
`i1 `i2    `i;i1 uii    aij ! uij ai;j C1   
ain
aiC1;1
aiC1;j
aiC1;n
:::
:::
:::
ajj
:::
:::
:::
an1
anj
ann
1
CCCCCCCCCCCCCCCCCCCCCA

48
3
Solution of Systems of Linear Equations
For large matrices, an important question is how many operations the LU
factorization requires, if an operation means one multiplication and one addition.
From the formulae above we get the same as in the case of the Gaussian
elimination—which requires
.n  1/2 C .n  2/2 C    C 1 D n.n  1/.2n  1/=6 D n3=3 C O.n2/
operations (where O.n2/ means “of the order of n2”, and where we neglect the
n  1 divisions and the cost of the calculation of the multipliers `ij which amounts
to .n  1/ C .n  2/ C    C 1 D .n  1/n=2 operations).
After performing the LU factorization, the determinant of A can be calculated
by taking the product of the elements in the main diagonal of U , and in this way
instead of nŠn operations using a direct approach, the calculation requires only about
n3=3 operations. (For example, if n D 10, these values are nŠn  3:6  107 and
n3=3  333.)
For large values of n during the multiplications of the elements uii you have to
be prepared for both overﬂow and underﬂow.
Naturally, our main purpose is to solve the system Ax D b with the help
of (3.21)—considered above—and (3.24)–(3.25). From these latter equations the
solution of Ly D b (if L is normalized) is obtained from
yi D bi 
i1
X
kD1
`ikyk ;
i D 1; : : : ; n:
Here the calculation of the ith component requires i  1 operations, hence, in total
.n  1/n=2 operations are needed (and that many elements are in L under its main
diagonal).
For the back substitution, solving the equation Ux D y, n more multiplications
are needed,
xi D
 
yi 
n
X
kDiC1
uikxk
!
1
uii
;
i D n; n  1; : : : ; 1;
where the 1=uii are already available. Hence, here in total .nC1/n=2 operations are
required (and that many elements are in the upper triangular part of U ).
Thus, solving the systems (3.24) and (3.25) altogether means n2 operations.
Hence, the solution of a new system of equations (with the same matrix A, and
having the LU factorization already done) costs n2 operations, just as many as would
be needed for a multiplication by the matrix A1.

3.4
Algorithms, Cost of Solving
49
How can we produce the inverse matrix and how many operations does it require?
First, we compute the LU factorization, then based on AA1 D I taken columnwise:
Ax.j / D ej; j D 1; : : : ; n, we solve the corresponding systems (3.24), (3.25) for
b D ej where ej is the jth coordinate unit vector, j D 1; : : : ; n.
This process would require n3=3 C n  n2 C O.n2/ operations, but during the
solution of the systems Ly.j / D ej we take into account that
y.j /
i
D 0;
1  i  j  1I
y.j /
j
D 1;
y.j /
j C1 D `j C1;j
without arithmetical operations. Next, for i > j C 1 only i  j  1 multiplications
and additions are required:
y.j /
i
D `ij 
i1
X
kDj C1
`iky.j /
k ;
and together
n
X
iDj C2
.i  j  1/ D
nj 1
X
kD1
k D .n  j/.n  j  1/=2
operations are needed (that many elements are in L under the main diagonal, to the
right of the jth column). Therefore, the calculation of all vectors y.j / costs
1
2
n
X
j D1
.n  j/.n  j  1/ D 1
2
n2
X
kD1
.k C 1/k D 1
6n.n  1/.n  2/;
that is about n3=6 operations (i.e. neglecting an O.n2/ part).
During the solution of the systems Ux.j / D y.j / there is no more essential simpli-
ﬁcation, and, in total, the computation of A1 requires about
 1
3 C 1
6 C 1
2

n3 D n3
operations.
As you see, the computation of the inverse of a matrix costs three times more
than its LU factorization.
In other words, it is usually not worth determining the inverse matrix because
the 2n3=3 C O.n2/ operations of extra work are never justiﬁed—even if we
have to solve systems with the same matrix and many different right-hand
sides.

50
3
Solution of Systems of Linear Equations
More complicated expressions including A1 can also be obtained for a cheaper
price using LU factorization. For example, to determine AB1Cx we ﬁrst calculate
the product b D Cx (n2 operations), then we solve the system LUy D b where
B D LU (which costs n3=3 C O.n2/ operations), ﬁnally, Ay is the vector we want
to ﬁnd (n2 more operations).
If instead we ﬁrst calculated the matrix AB1C, then this would cost about 3n3
operations. Then, the n2 operations required for the multiplication by vector x are
negligible.
3.5
Inﬂuence of Rounding Errors
In view of the often large condition numbers and the many operations needed to
perform the Gaussian elimination (or LU factorization) as seen above, here it is
worthwhile to brieﬂy consider these factors and their inﬂuence on the solution of
linear systems—remembering Sect. 2.3 and, especially, Sect. 2.3.2.
We ﬁrst consider the situation that x is some approximate solution of Ax D b
with regular A 2 IRnn. Then x is the exact solution of Ax D bbCAx D br DW
b  ıb, where r WD b  Ax is the residual vector to x. Then, for ıx WD x  x we
have Aıx D ıb and from (2.20) we obtain
kx  xk
kxk
 cond.A/  krk
kbk:
(3.27)
Now, if krk is of the order of rounding errors, krk D O."1/, we get kxxk
kxk

cond.A/  O."1/:
But (3.27) allows us to judge the accuracy of x in general—and mostly leads to
rather pessimistic results.
A practically satisfying criterion is delivered by considering the following
question: what is the (“perturbed”) ıA matrix, instead of A, having been LU
decomposed exactly when taking into account the many rounding errors committed
during the decomposition, and how far is it in some norm from the original A ?
This question is practically satisfying since usually the original matrix has been
obtained by previous computations or by measurements and clearly is charged with
errors, and this way gives less pessimistic results.
This (so-called backward) error analysis leads to the following answers:
1. It can be proved that kıAk.1/ is of the order of "1a  n2.
2. Numerical experiments show that kıAk.1/ is of the order of "1a  n.
Here a D maxk maxi;j ja.k/
ij j where k refers to the k-th step in the LU decomposi-
tion.

3.6
LU Factorization for General Matrices
51
3.6
LU Factorization for General Matrices
Up to this point we have always assumed that a.k/
kk ¤ 0, k D 1; : : : ; n. However,
with suitable modiﬁcations the LU factorization can be performed for an arbitrary
matrix A. Namely, for every A there is an upper triangular matrix U , a normalized
lower triangular matrix L and a permutation matrix P such that A D PLU holds.
(A permutation matrix has all the columns of the unit matrix, but in some—possibly
different—order which here, for P , depends on A.)
To verify this we use Gaussian elimination. If a11 ¤ 0, the ﬁrst step of the
elimination can be performed without changes, and then A ! A2 D L1A holds.
However, imagine the worst case ai1 D 0, 1  i  n. Then A is singular, but it
already has the same form as A2, compare with (3.12):
A D
 
0 a1
0 A.1/
22
!
:
Therefore, in this case we have L1 D I and A2 D A1 in (3.11) and immediately
we can continue by applying to the block A.1/
22 a transformation (3.16) (writing
formally A1 WD A.1/
22 ; k D 1). For A3 D L2A2 this means applying just an L2
of the form (3.15), etc.
However observe, as the ﬁrst row of A D A1 is the ﬁrst row of the ﬁnal matrix
U , a zero will appear on the beginning of the main diagonal of U .
There remains the case that a11 D 0 but a`1 ¤ 0 for some `; 1 < `  n. Then
let us interchange the `th row with the ﬁrst row. This means to multiply A from the
left by the corresponding permutation matrix P1 where
P1 D .e`; e2; : : : ; e`1; e1; e`C1; : : : ; en/:
Thus, we get from matrix A D A1 to matrix P1A1, and then the elements on the
positions .i; 1/, i  2 can be eliminated in the usual way:
L1P1A1 D
 
a11 c1
0 A
.2/
!
:
Continuing the elimination in this way, we ﬁnally obtain Ln1Pn1 L1P1A D U ,
which is again an upper triangular matrix. Let us compute the product PmLk when

52
3
Solution of Systems of Linear Equations
m > k and Pm is the matrix which interchanges the mth and the ith row .i > m/:
PmLk D Pm
0
BBBBBBBBBBBBB@
1 0
:
: : : : : 0
: 1
:
: : : : : :
0 :
1
0 : : : : 0
: :
:
1 : : : : :
: : `m;k 0 1 : 0 : 0
: :
:
: : 1 : : 0
: : `i;k 0 0 : 1 : 0
: :
:
: : : : 1 :
0 : `n;k 0 : : : 0 1
1
CCCCCCCCCCCCCA
D
0
BBBBBBBBBBBBB@
1 0
:
: : : : : 0
: 1
:
: : : : : :
0 :
1
0 : : : : 0
: :
:
1 : : : : :
: : `i;k 0 0 : 1 : 0
: :
:
: : 1 : : 0
: : `m;k 0 1 : 0 : 0
: :
:
: : : : 1 :
0 : `n;k 0 : : : 0 1
1
CCCCCCCCCCCCCA
D
0
BBBBBBBBBBBBB@
1 0
:
: : : : : 0
: 1
:
: : : : : :
0 :
1
0 : : : : 0
: :
:
1 : : : : :
: : `i;k 0 1 : 0 : 0
: :
:
: : 1 : : 0
: : `m;k 0 0 : 1 : 0
: :
:
: : : : 1 :
0 : `n;k 0 : : : 0 1
1
CCCCCCCCCCCCCA
Pm DW LkPm :
The lower triangular matrix Lk has the same structure as Lk, only the elements in
the .m; k/ and .i; k/ positions are interchanged. In this way we have moved Pm one
position to the right. Moving all permutation matrices to the right we obtain that
U D Ln1  Ln2    L1
n1
Y
iD1
PiA;
(where in Lk at most n  k  1 interchanges occurred, all in the kth column), and
then using the notations
L WD .Ln1  Ln2    L1/1;
P WD
 n1
Y
iD1
Pi
!1
PLU D A holds. We add to this that the inverse of a permutation matrix is its
transpose, hence, it is a permutation matrix, too, and the product of permutation
matrices is again a permutation matrix.
To produce the LU factorization, in the reasoning above we have used partial
pivoting: if the element a.k/
kk needed for step k is equal to zero then we look for a
nonzero element—the pivot element—from the part of the kth column below the
main diagonal and by a row change bring it into position .k; k/. If such a pivot
element does not exist, then the matrix is singular and we take Lk D I.
Therefore, in both cases the Gaussian elimination can be continued. However, in
practice during the partial pivoting of the Gaussian elimination one searches not for

3.6
LU Factorization for General Matrices
53
an arbitrary pivot element but for the element a.k/
ik ; i  k, with the largest absolute
value from the currently considered kth column—because we will have to divide by
a.k/
ik if it is nonzero. We then interchange the kth and ith rows, and the elimination
can be continued.
If we consider the determinants of the matrices on both sides of the equality
PLU D A, then it turns out that A is singular if and only if there are zero elements
on the main diagonal of U . However, if A is non-singular, then U is non-singular
as well. Hence, in the case of an arbitrary non-singular matrix A the system
Ax D b can be solved with the help of PLU factorization—and, essentially, in the
same way as before (see (3.24), (3.25)): the appearance of the permutation matrix
means only that in the ﬁrst step we have to solve Ly D P 1b D P T b instead of
Ly D b.
We consider an example of this PLU factorization:
A D
0
BB@
0 0 1 1
2 2 2 2
1 2 1 2
2 4 1 1
1
CCA H) P1A D
0
BB@
2 2 2 2
0 0 1 1
1 2 1 2
2 4 1 1
1
CCA DW A1
(3.28)
where P1 interchanges the ﬁrst and the second rows: P1 D
0
BB@
0 1 0 0
1 0 0 0
0 0 1 0
0 0 0 1
1
CCA. Then
A1
`21D0; `31D 1
2 ; `41D1
H)
0
BB@
2 2
2
2
0 0
1
1
0 1
0
1
0 2 1 1
1
CCA D L1A1 DW A2 :
Now we interchange the second and the third rows (that is P2 D
0
BB@
1 0 0 0
0 0 1 0
0 1 0 0
0 0 0 1
1
CCA), and the
elimination can be continued:
P2A2 D
0
BB@
2 2
2
2
0 1
0
1
0 0
1
1
0 2 1 1
1
CCA
`32D0; `42D2
H)
0
BB@
2 2
2
2
0 1
0
1
0 0
1
1
0 0 1 3
1
CCADL2P2A2 DWA3 :

54
3
Solution of Systems of Linear Equations
After this, no further interchanges are needed, i.e. formally P3 D I 2 IR44, and
A3
`43D1
H)
0
BB@
2 2 2
2
0 1 0
1
0 0 1
1
0 0 0 2
1
CCA D L3A3 D U:
To summarize, U D L3L2P2L1P1A D L3L2L1P2P1A, where
L1 D
0
BB@
1 0 0 0
1
2 1 0 0
0 0 1 0
1 0 0 1
1
CCA ;
that is
P2P1A D .L1/1.L2/1.L3/1U D
0
BB@
1 0
0 0
1
2 1
0 0
0 0
1 0
1 2 1 1
1
CCA
0
BB@
2 2 2
2
0 1 0
1
0 0 1
1
0 0 0 2
1
CCA :
Taking into account that here P 1
i
D P T
i
D Pi; i D 1; 2; the equality
.P2P1/1 D P1P2 D
0
BB@
0 1 0 0
1 0 0 0
0 0 1 0
0 0 0 1
1
CCA
0
BB@
1 0 0 0
0 0 1 0
0 1 0 0
0 0 0 1
1
CCA D
0
BB@
0 0 1 0
1 0 0 0
0 1 0 0
0 0 0 1
1
CCA
holds, and ﬁnally the PLU factorization is
A D
0
BB@
0 0 1 0
1 0 0 0
0 1 0 0
0 0 0 1
1
CCA
0
BB@
1 0
0 0
1
2 1
0 0
0 0
1 0
1 2 1 1
1
CCA
0
BB@
2 2 2
2
0 1 0
1
0 0 1
1
0 0 0 2
1
CCA :
It is worth checking that this product really gives the original matrix.
3.6.1
Algorithm of the LDU Factorization, Test Examples
Below, in Sect. 3.7, we will discuss a variant of the LU factorization for general
matrices (using row permutations if necessary), the so-called LDU factorization.

3.6
LU Factorization for General Matrices
55
Assuming that A is non-singular, we now give the pseudo-code of this LDU
factorization (with permutations, that is exactly: PLDU factorization) solving the
system Ax D b. However, it would be a waste to store the permutation matrix
P explicitly. Instead, a single vector remembering the permutations appears. The
elements of A are loaded row-wise, the elements of L and U are produced row-
wise, too.
1.
i WD 1.1/n Œ pi WD i i
The vector p stores the interchanges
2.
i WD 1.1/n
The factorization loop
3.
Œ m WD i; s WD bi, j WD 1.1/n Œ rj WD aij j
Reading the
ith row of A
4.
k WD 1.1/i  1
5.
Œ `ik WD rk, s WD s  xk 	 `ik
6.
j WD k C 1.1/n Œ rj WD rj  `ik 	 ukj j
7.
`ik WD `ik 	 dk k
The ith row of L is completed
8.
‹ri D 0‹ Œ m WD m C 1; ‹m > n‹ [ stop: “singular matrix”, i index ]
9.
s WD bm; bm WD bi; bi WD s
Interchange in the right side
vector
10.
k WD pm; pm WD pi; pi WD k
Interchange in the permutation
vector
11.
j WD 1.1/n Œ rj WD amj; amj WD aij; aij WD rj j
Row
interchange in A
12.
! 4. ]
13.
di WD 1=ri, j WD i C 1.1/n Œ uij WD rj 	 di j
Storing 1=dii in di,
the ith row of U is completed
14.
xi WD s 	 di i
End of the factorization loop, xn is completed
15.
i WD n  1.1/1
Starts the back substitution
16.
Œ s WD xi, j WD i C 1.1/n Œ s WD s  uij 	 xj j
17.
xi WD s i
18.
stop [ result: solution x, matrices L; U , vectors d; p ]
Let us point out that the loops of steps 4, 13, 15 and 16 can be empty, too.
As a ﬁrst example facilitating the test of the program, consider the system (3.8) with exact
solution (3.9). We have computed the matrix L in (3.23), and the matrix U given in (3.22)
corresponds now to D  U , where
D D
0
@
4
0 0
0 3:75 0
0
0 4
1
A ; U D
0
@
1 0:25 0:25
0
1 0:2
0
0
1
1
A :
At the end of the ﬁrst loop (step 14) we have to get the vector x D .2:25; 2:6; 3/T .

56
3
Solution of Systems of Linear Equations
Our second example requires three interchanges:
Ax D
0
BB@
2 2 0 0
1 1 0 2
2 2 1 0
0 1 0 0
1
CCA
0
BB@
1
2
0
1
1
CCA D
0
BB@
6
1
6
2
1
CCA D b:
(3.29)
After the ﬁrst i-loop we get x D .3; 2; 0; 1/T , d D . 1
2; 1; 1; 1
2/T and p D .1; 4; 3; 2/,
accordingly the permutation matrix is the following:
P D
0
BB@
1 0 0 0
0 0 0 1
0 0 1 0
0 1 0 0
1
CCA :
Then A D P LDU holds, where
L D
0
BB@
1 0 0 0
0 1 0 0
1 0 1 0
1
2 0 0 1
1
CCA ; D D
0
BB@
2 0 0 0
0 1 0 0
0 0 1 0
0 0 0 2
1
CCA ; U D
0
BB@
1 1 0 0
0 1 0 0
0 0 1 0
0 0 0 1
1
CCA :
Finally, the third example is the matrix A examined in detail in (3.28) and the right-hand side
vector b D .4; 6; 4; 4/T . In this case the exact solution is x D .2; 3; 0; 4/T .
3.7
Cholesky Factorization
Here we ﬁrst deal with the LDU factorization and the symmetric Gaussian elimina-
tion. If the conditions (3.10) are true, then all elements on the main diagonal of U
differ from zero, and then they can be moved out of U in the sense that U D DU 0
where
D WD diag.uii/ D diag.a.i/
ii /:
Thus, A
D
LDU0. Substituting for U 0 the notation U we obtain the LDU
factorization of the matrix A,
A D LDU:
(3.30)
Since here the multiplier U is also normalized, this so-called LDU factorization is
uniquely determined.
For general matrices (in accordance with Sect. 3.6 using a permutation matrix
P ), instead of (3.30) we will obtain A D P LDU. The pseudo-code of this PLDU
factorization was given in Sect. 3.6.1. Of course, MATLAB “knows” all these
factorizations, see the exercises, and in fact applies the PLU decomposition (and
not the inverse matrix) when you solve your system Ax D b by simply writing the
command x D Anb.

3.7
Cholesky Factorization
57
Now consider the special case when matrix A is symmetric and positive deﬁnite,
that is, on the one hand
A D AT ; in other words: aij D aji for all i and j
is valid, on the other hand
.Ax; x/ > 0 for all vectors x ¤ 0
(3.31)
holds in the Euclidean inner product.
Such matrices are often met in applications: for example, mechanical problems or
least square problems (see Chap. 4) lead to symmetric and positive deﬁnite matrices.
For such matrices A there exists such a factorization (3.30) of A for which
U D LT holds, moreover, the main diagonal of the lower triangular matrix L is
normalized .`ii D 1/, and the elements on the main diagonal of D are positive.
To verify this it is sufﬁcient to examine the ﬁrst step of the Gaussian elimination.
Take in (3.31) x D e1 to be the ﬁrst coordinate unit vector. Then .Ae1; e1/ > 0, but
Ae1 is the ﬁrst column of A, and .Ae1; e1/ D a11. Hence, this element is positive,
and the Gaussian elimination can be started. After this, matrix A.2/ (see (3.12))
contains the elements a.2/
ij
calculated according to (3.2) and (3.3) on which the
Gaussian elimination operates next. Let’s have a look at a.2/
ij  a.2/
ji :
a.2/
ij  a.2/
ji D a.1/
ij  `i1a.1/
1j 

a.1/
ji  `j1a.1/
1i

D

a.1/
ij  a.1/
ji



`i1a.1/
1j  `j1a.1/
1i

D 0 
 
a.1/
i1 a.1/
1j
a.1/
11

a.1/
j1 a.1/
1i
a.1/
11
!
D 0:
This means that together with A the matrix A.2/ is also symmetric.
Further, imagine that in x the lower part x.2/ D .x2; : : : ; xn/T is an arbitrary
nonzero vector, moreover,
x1 D  1
a11
n
X
j D2
a1j xj ;

58
3
Solution of Systems of Linear Equations
and therefore .Ax/1 D 0. Then the inner product turns out to be .Ax; x/ D
Pn
iD2.Ax/ixi, and further
.Ax; x/ D
n
X
iD2
.Ax/ixi D
n
X
iD2
0
@ai1x1 C
n
X
j D2
aijxj
1
A xi
D
n
X
iD2
0
@ ai1
a11
n
X
j D2
a1j xj C
n
X
j D2
aijxj
1
A xi
D
n
X
iD2
n
X
j D2

 ai1
a11
a1j C aij

xj xi
D
n
X
iD2
n
X
j D2
a.2/
ij xj xi D .A.2/x.2/; x.2//;
see (3.13). From here it is obvious that together with A the matrix A.2/ is also
positive deﬁnite, and our statement follows by induction. Hence, we have a class
of matrices for which conditions (3.10) are automatically satisﬁed: this is the class
of symmetric and positive deﬁnite matrices.
You can see directly from (3.12) that in the case of a symmetric matrix A the
relation cT
1
D a1 is valid, and c1=a11 appears in the multiplier L of the LDU
factorization while a1=a11 appears in U , and a11 is the ﬁrst entry of D. Continuing
by induction, in (3.30) you get
LT D U and D D diag1kn.a.k/
kk /:
This is the symmetric LDU factorization: A D LDU D LDLT .
Since D is diagonal and the elements on the main diagonal of D are positive, the
square root of D is real and readily computed:
D1=2 D diag1kn
q
a.k/
kk

:
Now, using the notation QL WD LD1=2, you see that
A D LDLT D .LD1=2/.D1=2LT / D QL QLT :
(3.32)
This is called the Cholesky factorization of A. According to the reasoning above,
on the main diagonal of the lower triangular matrix QL we have the positive numbers
q
a.k/
kk , which means also that the matrix QL is non-singular. In what follows, we
simply write L instead of QL.

3.7
Cholesky Factorization
59
The matrix L is uniquely determined—disregarding the signs of the elements on
the main diagonal which were chosen positive.
The Cholesky (or square root) algorithm serves for the computation of the LLT
factorization. From equality A D LLT it follows that (compare with (3.26))
aij D
min.i;j /
X
kD1
`ik`jk ;
1  i; j  n;
hence, using the expressions
j D 1; : : : ; n W `jj D
 
ajj 
j 1
X
kD1
`2
jk
!1=2
;
(3.33)
`ij D
 
aij 
j 1
X
kD1
`ik`jk
!
=`jj; i D j C 1; : : : ; n;
(3.34)
we can calculate the elements of L column-wise. This computation requires in total
n3=6 C O.n2/ operations since compared to the Gaussian elimination here we have
to produce only half of the elements, and the storage requirement is only n.nC1/=2.
Consider an example:
A D
0
@
5 7 3
7 11 2
3 2 6
1
A ;
(3.35)
which is obviously a symmetric matrix, moreover, it is positive deﬁnite since
conditions (3.10) are satisﬁed:
5 > 0; det
5 7
7 11

D 6 > 0; det
0
@
5 7 3
7 11 2
3 2 6
1
A D 1 > 0:
We illustrate the different factorizations considered starting from the LU factor-
ization:
A
`21D 7
5 ; `31D 3
5
H)
0
@
5
7
3
0
6
5  11
5
0  11
5
21
5
1
A
`32D 11
6
H)
0
@
5 7
3
0 6
5  11
5
0 0
1
6
1
A D U;
that is
A D LU D
0
@
1
0 0
7
5
1 0
3
5  11
6 1
1
A
0
@
5 7
3
0 6
5  11
5
0 0
1
6
1
A :

60
3
Solution of Systems of Linear Equations
Collecting the elements from the main diagonal of U in the diagonal matrix D:
U D DU 0 and writing U instead of U 0, we obtain the LDU factorization which is
in fact the LDLT factorization:
A D LDU D
0
@
1
0 0
7
5
1 0
3
5  11
6 1
1
A
0
@
5 0 0
0 6
5 0
0 0 1
6
1
A
0
@
1 7
5
3
5
0 1  11
6
0 0
1
1
A D LDLT :
(3.36)
Taking now LD1=2 D QL where D1=2 D diag
p
5;
q
6
5;
q
1
6

, we get the Cholesky
factorization:
A D QL QLT D
0
BB@
p
5
0
0
7
5
p
5
q
6
5
0
3
5
p
5  11
6
q
6
5
q
1
6
1
CCA
0
BB@
p
5 7
5
p
5
3
5
p
5
0
q
6
5  11
6
q
6
5
0
0
q
1
6
1
CCA :
(3.37)
With the help of (3.33)–(3.34) this can also be computed directly and if this is not
a classroom exercise as here but the result of a computer program, we get of course
ﬂoating point numbers approximating the roots.
In practice the LDLT factorization given by (3.32) is more useful than the
Cholesky factorization produced from it. Firstly, we can economize on the calcu-
lation of the square roots, and secondly, using the LDLT factorization there are n
multiplications fewer when solving the equations
Ly D b;
LT x D D1y:
Finally, the LDLT factorization can be feasible even when the matrix is symmetric
but not positive deﬁnite. This turns out during the factorization because then there
occurs an i such that dii  0. But if here di D 0 then A is singular, and when dii < 0
there may be numerical difﬁculties.
3.7.1
Algorithm of the LDLT Factorization, Test Examples
Now, we give a pseudo-code which solves the symmetric system of equations Ax D
b using the LDLT factorization. This algorithm produces the elements of the lower
triangular part of L columnwise and it requires only the elements from the lower
triangular part of A.
Thus, below A is a symmetric and positive deﬁnite matrix (or some symmetric
matrix for which the LDLT factorization exists) and b is a given column vector.
1.
j WD 1.1/n
2.
Œi WD j.1/n Œri WD aiji, xj WD bj
3.
k WD 1.1/j  1

3.8
Band Matrices
61
4.
Œi WD j.1/n Œri WD ri  `ik 	 rk 	 `jki; xj WD xj  `jk 	 xkk
5.
If rj=0, then issue an error message containing j, and stop.
Otherwise dj WD 1=rj
6.
i WD j C 1.1/n Œ`ij WD ri 	 djij
End of factorization cycle.
7.
i WD n.1/1 Œxi WD di 	 xi
Back substitution starts.
8.
k WD i C 1.1/n Œxi WD xi  `ki 	 xkki
9.
stop [result: x]
Notice that after line 6, i.e. at the end of the j-loop, the array r contains
the elements of the main diagonal of the matrix D while array d contains their
reciprocals.
A test example is the matrix (3.35) and its LDLT factorization (3.36), while the right-hand side
vector is b D .0; 9; 17/T . Using double-precision ﬂoating point numbers rounded here to six
digits, the matrices are
ADLDLT ; where LD
0
@
1
0 0
1:4
1 0
0:6 1:833333 1
1
A ; D D
0
@
5 0
0
0 1:2
0
0 0 0:133333
1
A :
According to this we have d D .0:2; 0:833333; 6/, and completing the factorization loop we
obtain x D .0; 9; 0:5/T , while at the end x D .1; 2; 3/T which is the exact solution.
3.8
Band Matrices
In practical problems the matrices often have many zero entries, and it is typical that
the nonzero elements lie in a rather narrow band containing the main diagonal.
To describe these band matrices we introduce the notion of the bandwidth: the
half bandwidth of the matrix A D .aij/ 2 IRnn is the value s for which
ji  jj > s ) aij D 0; 1  i; j  n
holds, and there is an index pair .k; `/ such that jk  `j D s and ak` ¤ 0.
In MATLAB such matrices can conveniently be constructed by calling the built-
in program spdiags (see “help spdiags”) as A D spdiags.B; d; n; n/ where B
contains the elements of the nonzero diagonals of A in columns which are identiﬁed
by the integer vector d. Here d.i/ D 0 refers to the main diagonal; diagonals above
(or below) the main diagonal have d.i/ > 0 (or d.i/ < 0), and jd.i/j gives the
distance from the main diagonal (see the example in Sect. 3.8.1). Then, the maximal
jd.i/j is the (half) bandwidth s.
Assume that the half bandwidth of the matrix A is 1  s  n2 and that systems
with matrix A are solvable with the help of Gaussian elimination. Then the Gaussian
elimination preserves the bandwidth: the half bandwidths of both multipliers L and
U are s.
To understand this, as always, it is sufﬁcient to examine the ﬁrst step of Gaussian
elimination.

62
3
Solution of Systems of Linear Equations
If 1  i; j  1 C s, then we are in the band, so it is not necessary to check
this index domain: its elements will change according to (3.2)–(3.3). However, if
i > 1 C s, then (3.2)–(3.3) gives
a.2/
ij
D aij  ai1a1j
a11
and a.2/
ij
D aij holds due to ai1 D 0. Similarly, when j > 1 C s, as a1j D 0 we
have a.2/
ij
D aij. Therefore, during the ﬁrst step of the elimination for i; j > 1 C s
no change is made in the matrix, that is, the zeros remain where they were.
The Gaussian elimination can be modiﬁed (using an appropriate re-indexing) to
avoid calculations with the elements outside the band where the elements would be
multiplied only by zero, or zero would be added to them. In this way, we get the
band Gaussian elimination. It requires approximately ns2 operations, since in each
step we have to change at most s2 elements (in the ﬁrst step, e.g., the elements on
positions 2  i; j  1 C s). This operation count is orders of magnitude smaller
than the n3=3 arising for full matrices since typically n 
 s D 1; 2; 3.
3.8.1
Tridiagonal Systems of Equations
Here we consider the important special type of linear systems having band matrices
with s D 1. The best choice to solve such systems is the band Gaussian elimination
because it enables a solution with O.n/ operations:
Ax D f;
(3.38)
A WD
0
BBBBB@
b1 c1 0
:
0
a2 b2 c2 :
:
0
:
:
:
0
:
:
:
: cn1
0
: 0 an bn
1
CCCCCA
DW tridiag1in.ai; bi; ci/:
(3.39)
According to their form these matrices are called tridiagonal, and the MATLAB
command to get (3.39) is
A=spdiags([a b c],[-1 0 1],n,n)
provided that the n-dimensional vectors are a
D
.a2; a3; : : : ; an; 0/T ; b
D
.b1; b2; : : : ; bn/T ; c D .0; c1; c2; : : : ; cn1/T , see the example below in Sect. 3.8.2.
Assume that the following conditions (where a1 D cn WD 0) are satisﬁed:
jbij  jaij C jcij; i D 1; : : : ; n;
(3.40)
there exists an index j for which jbjj > jaj j C jcj j holds:
(3.41)

3.8
Band Matrices
63
Since the half bandwidth is equal to 1 (according to the previous section and if
Gaussian elimination can be applied), the half bandwidths of U and L will also be
equal to 1. So these triangular matrices have only two nonzero diagonals, and from
the ith row of the system Ux D y there remains only
uiixi C ui;iC1xiC1 D yi ;
uii ¤ 0:
By reordering we obtain
xi D ˛iC1xiC1 C ˇiC1 ;
i D n  1; : : : ; 1;
(3.42)
where ˛iC1 WD ui;iC1=uii; ˇiC1 WD yi=uii. However, the values ˛i and ˇi can be
determined directly from the ith row of the system (3.38)–(3.39):
fi D aixi1 C bixi C cixiC1 D ai.˛ixi C ˇi/ C bixi C cixiC1 ;
that is
.bi C ai˛i/xi D fi  aiˇi  cixiC1 :
Now, assume that bi Cai˛i ¤ 0. (This condition will be examined below.) Then we
obtain relations (3.42), where
˛iC1 WD 
ci
bi C ai˛i
;
ˇiC1 WD fi  aiˇi
bi C ai˛i
; i D 1; : : : ; n  1:
(3.43)
The ﬁrst equation of the system (b1x1 C c1x2 D f1) shows that in the formulae
above we can start from
˛1 WD 0 and ˇ1 WD 0:
Hence, the calculation can start, and we obtain from (3.43) in turn all the ˛i; ˇi. At
the end, for i D n we get the following system:
xn1 D ˛nxn C ˇn ;
anxn1 C bnxn D fn D an.˛nxn C ˇn/ C bnxn ;
from which
xn D fn  anˇn
bn C an˛n
DW ˇnC1:
(3.44)
Now, using xn the back substitution (3.42) can be started: we obtain the values xn1,
..., x1. Algorithm (3.42)–(3.44) is known as tridiagonal algorithm, sometimes it is
called “double sweep” or “Thomas algorithm”.
As you see, the above formulae are usable when bi C ai˛i ¤ 0. Now assume
that a stronger form of our conditions (3.40) holds: jbij > jaij C jcij for all i.

64
3
Solution of Systems of Linear Equations
Then from ˛1 D 0 it follows that j˛2j D
jc1j
jb1j < 1. Moreover, assuming that
j˛ij < 1, by induction we obtain
jbi C ai˛ij  jbij  jaijj˛ij > jcij C jaij  j˛ijjaij D jcij C jaij.1  j˛ij/  jcij:
Then, according to (3.43) inequality j˛iC1j < 1 also holds, hence it is valid for all
i. Together with this jbi C ai˛ij > jcij  0, i D 1; : : : ; n, holds—which includes
that ˇnC1 is well deﬁned. We see that in the formulae (3.43)–(3.44) there is no zero
divisor.
Starting now from our original conditions (3.40), similarly as before we verify
that j˛ij  1, but as soon as for some i in (3.40) “>” is valid then beginning from
here j˛iC1j < 1 holds and ˇnC1 can be calculated, too.
On the other hand, if, e.g., bi D ai Cci holds for all i, then the matrix is singular
(because Ae D 0; e D .1; 1; : : : ; .1/n/T and the system might be unsolvable.
The tridiagonal algorithm requires approximately 3n operations (in the previous
sense: one multiplication + one addition) and 2n divisions, that is, if we do not
distinguish between the four operations it requires 8n arithmetical operations.
3.8.2
The Tridiagonal Algorithm, Test Examples
Finally, we describe the pseudo-code which implements formulae (3.42)–(3.44), i.e.
the tridiagonal algorithm.
So, our system is (3.38) with matrix (3.39) where n  2, and conditions (3.40)–
(3.41) are valid.
1.
d WD 1=b1; ˛2 WD c1 	 d; f1 WD f1 	 d
2.
i WD 2.1/n  1
3.
Œd WD 1=.bi C ai 	 ˛i/,
˛iC1 WD ci 	 d; fi WD .fi  ai 	 fi1/ 	 di
4.
xn WD .fn  an 	 fn1/=.bn C an 	 ˛n/
5.
i WD n.1/2 Œxi1 WD ˛i 	 xi C fi1i
6.
stop [result: x]
In this algorithm the components fi of the right-hand side are overwritten by the
auxiliary values ˇi, hence, it is advantageous to shift the indices. Division by zero
should have been tested in steps 1, 3 and 4.
First test example:
Ax D f;
A D
0
@
2 1
0
1 3 1
0 1
1
1
A ; f D
0
@
1
12
2
1
A ;
when in (3.39)
a D
0
@
0
1
1
1
A ; b D
0
@
2
3
1
1
A ; c D
0
@
1
1
0
1
A :

3.9
Exercises
65
(For the MATLAB construction of A by spdiags we need a D .1; 1; 0/T , b D .2; 3; 1/T ; c D
.0; 1; 1/T , according to Sect. 3.8.1.) In this case the results are
˛ D
0
@
0
1=2
2=7
1
A ; ˇ D
0
BB@
0
1=2
23=7
1
1
CCA ; x D
0
@
2
3
1
1
A :
Second test example (where n  2 is a free parameter): Ax D f 2 IRn,
A D tridiag1in.1; 2; 1/ D
0
BBBBBBBBB@
2 1
0 : : : : : :
0
1
2 1
0 : : :
0
0 1
2 1
0
0
: : : : : : : : : : : : : : : : : :
0
0 1
2 1
0
0 : : :
0 1
2 1
0 : : : : : :
0 1
2
1
CCCCCCCCCA
;
Here
a D .0; 1; 1; : : : ; 1/T ; b D .2; : : : ; 2/T ; c D .1; 1; : : : ; 1; 0/T ;
and f
D .1; 2; 3; : : : ; n  2; n  1; n  n.nC1/.nC2/
6
/T . Then the exact solution is x D
.0; 1; 4; 10; : : : ; .n1/3.n1/
6
; n3n
6
/T .
3.9
Exercises
1. Take a general non-singular matrix A and some vector x ¤ 0 considered as
an approximate solution of the system Ax D b ¤ 0. Therefore krk  0 for
r WD Ax  b and some vector norm k  k.
Estimate the relative error kxxk
kxk
with the help of krk
kbk taking into account
that x D x C .x  x/ DW x C ıx and Ax D b C .Ax  b/ D b C r DW b C ıb
are true.
2. Find the LU factorization of the matrices
A1 D
0
@
4 1 2
1
4 1
1 2
4
1
A ;
A2 D
0
@
3
1 1
1
4
1
2 1
6
1
A :
3. Find the PLU factorization of the matrices
A1 D
0
BB@
0 1 0 0
2 0 1 0
0 2 0 1
0 0 2 0
1
CCA ;
A2 D
0
BB@
2 2 0 0
2 2 1 0
1 1 0 2
0 1 0 0
1
CCA :

66
3
Solution of Systems of Linear Equations
4. Decide without calculations whether the following matrix is positive deﬁnite or
not:
A D
0
@
4 2 1
2 5 1
1 1 1
1
A :
5. Imagine that the size of An D tridiag1in.ai; bi; ci/ 2 IRnn is large and the
sufﬁcient conditions (3.40)–(3.41) of applicability of the tridiagonal algorithm
are satisﬁed.
What is the largest value of n for which a linear system with matrix An
can be solved when in the numerical processor of your computer a memory of
1024 Mb is available to store the numbers, double precision arithmetic is used,
and
(a) full Gaussian elimination,
(b) the tridiagonal algorithm
is applied?
6. Apply the MATLAB commands
[L U]=lu(A)
[P L U]=lu(A)
to the matrix (3.29) and explain the connection between the matrices obtained
in the ﬁrst and second line.
7. For the matrix (3.35), get the results of the previous two lu-calls and compare
with (3.36), then look for the Cholesky decomposition using the MATLAB
help and apply it, too, comparing now with (3.37).
8. Use spdiags to construct, in MATLAB and for a given n  3, the tridiagonal
matrix (3.39) in case ai D .1/i; i D 2; : : : ; n, bi D i2; i D 1; : : : ; n and
ci D .1/i; i D 1; : : : ; n1—all this without using cycles!
9. Read the MATLAB help about the rref command. Applying rref to the
augmented matrix, what can you tell about the solutions of the systems (3.8),
Ax D b and Ax D c, where
A D
0
@
1 2 3
4 5 6
7 8 9
1
A ;
b D
0
@
7
19
31
1
A ;
c D
0
@
7
19
32
1
A‹
10. Consider the linear system Ax D b,
A D
1017 1
1
1

;
b D
1
2

:

3.9
Exercises
67
To solve the system ﬁrst apply the MATLAB commands
c=A(2,2)-A(1,2)*A(2,1)/A(1,1);
d=b(2)-b(1)*A(2,1)/A(1,1);
x(2)=d/c
x(1)=(b(1)-A(1,2)*x(2))/A(1,1)
(which corresponds to the Gaussian elimination for the 22 system Ax=b), and
after that the MATLAB command x=Anb. Try to explain the obtained results.
11. Let A=pascal(10) (i.e. A is the 1010 Pascal matrix, which is a symmetric,
positive deﬁnite matrix), x=ones(10,1) and deﬁne the vector b as b=A*x.
Solve the system Ax D b by applying the MATLAB functions lu, chol and
the command Anb (use format long).
12. Construct the following regular matrix A
2
IRnn and vector b
D
.1; 1; : : : ; 1/T 2 IRn for different values of n (for example, n D 5; 20; 100,
500; 2000), and solve the system Ax D b using the command x D Anb. Solve
also the system Cx=b, where C=rand(n). Try to explain the result. Here
aij D
8
ˆˆ<
ˆˆ:
1;
if i D j or j D n;
1;
if i > j;
0;
otherwise:

4
The Least Squares Problem
1. You know from Linear algebra:
A linear system Bx D c 2 IRm (where B 2 IRmn) is solvable exactly in the case
when for all row vectors y satisfying yB D 0 we have yc D 0.
2. We have already referred to the Cauchy inequality in connection with the
triangle inequality of the Euclidean norm, however, it will be important in this
chapter, too. It states that for arbitrary real numbers a1; : : : ; an; b1; : : : ; bn the
inequality
 n
X
iD1
aibi
!2

n
X
iD1
a2
i
n
X
iD1
b2
i
holds. Moreover, here equality is valid in the case when the vectors a D
.a1; : : : ; an/ and b D .b1; : : : ; bn/ are collinear: this means that there exists such
a constant c ¤ 0, that either a D cb, or b D ca.
3. If f
is a function of several variables, f
D
f .x1; x2; : : : ; xn/, then
the partial derivative @f.x1;:::;xn/
@xk
can be obtained by considering the variables
x1; : : : ; xk1; xkC1; : : : ; xn as constants and then taking the usual derivative with
respect to xk.
Example: If f .a; b/ D .a  c1 C b  c2/2, then
@f .a; b/
@a
D 2.a  c1 C b  c2/c1; @f .a; b/
@b
D 2.a  c1 C b  c2/c2:
© Springer International Publishing AG 2016
G. Stoyan, A. Baran, Elementary Numerical Mathematics for Programmers and
Engineers, Compact Textbooks in Mathematics, DOI 10.1007/978-3-319-44660-8_4
69

70
4
The Least Squares Problem
4.1
Linear Regression
In this section we look at a basic technique of processing measurement data with
the aim of a deeper understanding of a technical, physical, biological or social
phenomenon being investigated and measured.
Given m points on the plane: fti; figm
iD1, where ti is the moment of time in which
the ith measurement was done and fi is the result of the measurement, e.g. we have
measured the temperature f D f .t/ of a numerical processor ti minutes after being
switched on and have obtained the value fi. We do not preclude the coincidence of
some ti, in this case several measurements were made at the same moment of time
and we are aware that there may be f .ti/ ¤ fi due to measurement errors.
These points .ti; fi/ form a “point cloud” on the plane and can be plotted as in
Fig. 4.1. Based on the measurement results we now try to determine—in the form
of a simple expression roughly ﬁtting the data—a connection between t (the time
passing since switching on) and the temperature f .t/ of the processor. A connection
between t and f .t/ could be formulated by connecting the consecutive data points
by straight lines. However, we do not see the physical reality of such a method: in
this case the velocity of heating up would change suddenly at our time points as if
our measurements were strongly reacting to the process.
Instead of this we think that the connection can approximately be given with the
help of a smooth (several times continuously differentiable) curve F.t/, and if the
data points do not exactly ﬁt this curve then this is a consequence of measurement
errors. In other words, it would be best for us if, with acceptable approximation,
F.ti/  fi ; i D 1; 2; : : : ; m;
(4.1)
representing that in this case the function F.t/—in spite of the errors being present
in the values fi—will be near the real (but unknown) function f D f .t/. So we
hope that F.t/  f .t/ in the time interval Œt1; tm. The expression for F.t/ is
called the model of the process. The most simple form of the curve imagined is
a straight line:
F.t/ D a C bt:
(4.2)
o
o
o
o
tm
t3
t1
f2
f3
f1
f4
. . .
t
f
fm
t4
o
t2
Fig. 4.1 Measurement results

4.1
Linear Regression
71
In this case the problem—to determine a linear model from measurement results—
is called linear regression. The numbers a and b determining the straight line F.t/
are called the parameters of the model.
4.1.1
Algebraic Description
The model (4.2) would be really successful if instead of (4.1) the equations
F.ti/ D a C bti D fi; i D 1; 2; : : : ; m
(4.3)
were true. However, when m > 2 we usually cannot expect this because a straight
line is determined by two different points in the plane, and any further point does not
necessarily ﬁt the line (see Fig. 4.2: the third point can lie anywhere on the straight
line x D x3). This geometric fact can be described in the language of linear algebra.
Using the following matrix and two vectors:
A D
0
BBB@
1 t1
1 t2
::: :::
1 tm
1
CCCA 2 IRm2; f D
0
BBB@
f1
f2
:::
fm
1
CCCA 2 IRm;
x D
a
b

;
(4.4)
Eq. (4.3) can be written shortly as
Ax D f:
(4.5)
If m D 2 and t1 ¤ t2, then this system of linear equations is uniquely solvable for
all vectors f . However, when m > 2 it is usually overdetermined and unsolvable,
since if we have two different values, e.g. tk and t`, then based on the kth and `th
rows using Cramer’s rule we could calculate the parameters a and b, but for the
remaining indices i it is not necessary that the equations a C bti D fi hold.
o
o
2.
1.
y
x
o 3.
x3
x2
y1
y2
x1
Fig. 4.2 Three points and a straight line

72
4
The Least Squares Problem
4.1.2
The Method of Least Squares
In the previous situation the method of least squares is advantageous which realizes
the conception (4.1) with the help of an easily solvable minimization problem,
introducing the function
J.x/ WD kAx  f k2
2 ;
which assigns to each vector x (i.e. to each pair a; b) a non-negativenumber, namely,
the Euclidean norm of the vector Ax  f of residuals. After that it is obvious to
consider a; b as the best pair if J.x/ has a minimum at this pair:
J.x/ ! min
x Š
Describing the idea formally:
x D .a; b/T ;
J.x/ D
m
X
iD1
..Ax/i  fi/2 ! min
x Š;
(4.6)
where .Ax/i D F.ti/ D a C bti. Now, ..Ax/i  fi/2 is the squared distance from
the data point .ti; fi/ to the point .ti; a C bti/ ﬁtting the line. In other words, it
is the squared distance between the measurement data and the line, measured in
the vertical direction. Figure 4.3 shows two straight lines with the corresponding
squares; one of the lines is rather good (solid line), the other is not so good
(dashed line).
o
o
o
o
m
t
3
t
t1
f3
f1
f4
. . .
t
f
fm
t4
o
t2
f2
Fig. 4.3 Two straight lines and the squares corresponding to the measurement data

4.1
Linear Regression
73
To solve the minimization problem we start from the fact that the differentiable
function J can have a minimum only at the points where
@J.x/
@a
D 0; @J.x/
@b
D 0
(4.7)
hold. In the case of a general J a point x D .a; b/T satisfying these equations
could be not only a minimum point, but also a maximum point or a saddle point.
However, below we are returning to the fact that the function J deﬁned by (4.6)
assures that (4.7) yields a minimum. To calculate the partial derivatives we describe
J in the form
J.x/ D J.a; b/ D
m
X
iD1
.a C bti  fi/2:
(4.8)
Considering b to be constant we obtain that
@J.x/
@a
D 2
m
X
iD1
.a C bti  fi/;
since @.aCbtifi /2
@a
D 2.a C bti  fi/, and similarly, considering a to be constant
we get
@J.x/
@b
D 2
m
X
iD1
.a C bti  fi/ti ;
since @.aCbtifi /2
@b
D 2.a C bti  fi/ti. Hence, a rearranged form of (4.7) is
ma C b
m
X
iD1
ti D
m
X
iD1
fi ;
a
m
X
iD1
ti C b
m
X
iD1
t2
i D
m
X
iD1
tifi ;
and in matrix form

m
Pm
iD1 ti
Pm
iD1 ti
Pm
iD1 t2
i
  
a
b
!
D
 Pm
iD1 fi
Pm
iD1 tifi
!
:
(4.9)

74
4
The Least Squares Problem
The determinant of this matrix is non-negative:
m
m
X
iD1
t2
i 
 m
X
iD1
ti
!2
 0;
which is a consequence of the Cauchy inequality mentioned earlier. We add that the
left-hand side can be equal to zero only when all ti are equal:
ti D const DW t0; i D 1; : : : ; m:
(4.10)
In this case the determinant is zero and the system of equations simpliﬁes to
 m mt0
mt0 mt2
0
  
a
b
!
D
 Pm
iD1 fi
t0
Pm
iD1 fi
!
;
(4.11)
which is singular. However, the system is solvable because its second row equals t0
times the ﬁrst one, that is, it can be omitted. The solution is then
a D 1
m
m
X
iD1
fi  bt0 ;
where b is arbitrary. Substituting this into the model (4.2) we obtain
F.t/ D a C bt D 1
m
m
X
iD1
fi C b.t  t0/:
We see that this is an adequate result: in our special case the measurements were
made at one single moment of time t0. At this time point the model gives the mean of
the values fi, while the slope of the straight line is b, which can be chosen arbitrarily,
e.g. b D 0. With this choice the model simpliﬁes to F.t/ D a D const:
F.t/  const D 1
m
m
X
iD1
fi :
This has the following interpretation: if we don’t have any information about the
temporal behaviour of the process, then we will have to be satisﬁed with a time
independent model.
If there are at least two different values among the time points ti, then we will
not get the singular case (4.10)–(4.11), the determinant of the system (4.9) will be
positive, and the system will have only one solution .a; b/, that is, the model is
uniquely determined.

4.1
Linear Regression
75
To summarize: while the system (4.5) is usually unsolvable, that is, it does
not help in the choice of the linear model, the method of least squares always
leads to the solution of the linear regression. However, a singular case can
appear, but this is unavoidable. In this case a simpliﬁcation of the model is
recommended after which the solution is again uniquely determined.
We consider an example. The following table contains our data, corresponding
to m D 4:
i
1
2
3
4
ti
0
1
1
2
fi
4
3
2
0
Then (4.4) takes the form
A D
0
BB@
1 0
1 1
1 1
1 2
1
CCA 2 IR42; f D
0
BB@
4
3
2
0
1
CCA 2 IR4;
x D
a
b

:
If we tried to solve the system Ax D f , e.g. using its ﬁrst two rows, then the
solution would be a D 4; b D 1; F.t/ D 4  t. Then F.0/ D 4; F.1/ D 3 hold,
however, neither F.1/ D 2 nor F.2/ D 0 can be true. In this case the vector Ax f
of residuals is equal to .0; 0; 1; 2/T , and the square of the Euclidean length of this
vector equals 5.
However, the method of least squares yields a special case of the system (4.9):
4 4
4 6
  
a
b
!
D
 
9
5
!
which has the solution a D 17
4 ; b D 2, that is, the model is F.t/ D 17
4  2t. The
corresponding vector of residuals is . 1
4;  3
4; 1
4; 1
4/T , and the square of the Euclidean
length of this vector is 3
4. A value less than 3
4 cannot be reached because in our case
after some work on (4.8) you can calculate that
J.a; b/ D 4a2 C 8ab C 6b2  18a  10b C 29
D 4.a  17
4 C b C 2/2 C 2.b C 2/2 C 3
4  3
4:

76
4
The Least Squares Problem
4.2
Normal Equations
Now, we can deal with the more general case: as a model of a technological,
economical, etc. process we can try the following formula:
F.t/ D
n
X
j D1
xj 'j.t/:
(4.12)
Here xj ; j D 1; : : : ; n, are the unknown parameters (n  1), while f'jgn
j D1 is an
appropriate system of functions, e.g. it consists of polynomials: 'j.t/ D tj 1, or of
trigonometrical or exponential functions.
As an example we refer to Problem 5 (see Sect. 4.4 below), where n D 3; m D
12, and '1.t/  1; '2.t/ WD cos.2
t
365/; '3.t/ WD sin.2
t
365/.
Assume the data fti;figm
j D1 (containing measurement errors) are available, and
m > n holds. (In practice it is recommended to have about ten times more
measurement results than the number of unknown parameters in order to avoid the
dependence of the model on measurement errors.) We would like the model to ﬁt
the data: F.ti/  fi;
i D 1; : : : ; m.
Now, let
A D .aij / D .'j.ti// 2 IRmn;
(4.13)
f D .f1; : : : ; fm/T ;
x D .x1; : : : ; xn/T :
Then, again F.ti/ D .Ax/i. Therefore, our model would be the most successful if
the system of linear equation (4.5)—in this case with the n-dimensional vector x
and with the re-deﬁned matrix A—held. In this case it is even more important to
take into account that this system can be overdetermined and unsolvable. Hence,
we use again the method of least squares due to Gauss, calculating the unknown
parameters from the solution of the minimization problem
J.x/ WD kAx  f k2
2 D
m
X
iD1
0
@
n
X
j D1
xj 'j.ti/  fi
1
A
2
! min
x Š
(4.14)
We will see that the solution of the minimization problem exists, and under
certain conditions it is unique, moreover, we show its numerical realization.
In the case of m 
 n the whole process can be understood as compression
and interpretation of the information: we replace the large number of data
points fti; figm
j D1 by the model describing the essence of the phenomenon,
and by the few parameters fxj g of the model.

4.2
Normal Equations
77
A minimum can be attained only at a point x where the partial derivatives of
J.x/ disappear:
@J.x/
@xk
D 0;
k D 1; : : : ; n;
that is
@J.x/
@xk
D 2
m
X
iD1
0
@
n
X
j D1
xj 'j.ti/  fi
1
A 'k.ti/ D 0;
k D 1; : : : ; n;
(4.15)
as
@
@xk
Pn
j D1 xj 'j.ti/  fi
2
D 2
Pn
j D1 xj'j .ti/  fi

'k.ti/.
In this way we again obtain a system of linear equations with respect to the
unknown parameters fxj g. But, here too, it is worth clarifying the connection of the
system with the matrix A and the vector f .
First, we examine the expression connected with the values fi in (4.15). We
can write
m
X
iD1
fi'k.ti/ D .AT f /k ;
(4.16)
because according to (4.13) the kth row of the matrix AT consists of the values
'k.t1/; : : : ; 'k.tm/. Then it is not difﬁcult to see that
m
X
iD1
'j.ti/'k.ti/ D .AT A/kj ;
(4.17)
and so
m
X
iD1
n
X
j D1
xj 'j.ti/'k.ti/ D
n
X
j D1
.AT A/kjxj D .AT Ax/k :
(4.18)
Equations (4.15), (4.16) and (4.18) together mean that the minimum can be attained
only on a vector x where
AT Ax D AT f:
(4.19)
This n  n system of linear equations is called the normal equations.

78
4
The Least Squares Problem
Example
Take the polynomial system f'j gn
jD1, that is 'j .ti/ D t j1
i
. Then
.AT A/kj D
m
X
iD1
t k1
i
t j1
i
D
m
X
iD1
t kCj2
i
;
1  k; j  n;
moreover, when n D 2 also holds, then you obtain exactly the entries of the matrix (4.9).
We have a good chance of solving the system (4.19): both sides are linear
combinations of columns of AT . To verify that the normal equations are indeed
always solvable, our starting point is that when B D AT A (for all row vectors y)
equation
yByT D .ByT ; yT / D .AT AyT ; yT / D .AyT ; AyT / D kAyT k2
2
(4.20)
is valid, since, on the one hand, yByT can be written as a scalar product, and,
on the other hand, we have applied the identity (2.11) from Chap. 2, from which
.AT AyT ; yT / D .AyT ; AyT / follows.
Now, let us apply the algebraic theorem mentioned in the ﬁrst part of the chapter:
In our case B D AT A and c D AT f . If yB D 0 holds, then by (4.20)
kAyT k2 D 0 is valid which according to the properties of the norm means that
AyT D 0, and together with this equation yAT D 0T also holds. Then yc D
yAT f D 0 is true—as required by the theorem.
We have veriﬁed therefore that the system (4.19) is always solvable. Since the
system is a generalization of (4.9), it can be singular, too.
Denote by x the solution of the system. (In the case of singularity it is not
uniquely determined.) In what follows, we will show that in this way, by solving the
normal equations, we obtain the minimum of J.x/:
J.x/ D kAx  f k2
2 D kAx  f C .Ax  Ax/k2
2
D kAx  f k2
2 C kA.x  x/k2
2 C 2.Ax  f; A.x  x//
D J.x/ C kA.x  x/k2
2  J.x/;
where .Ax  f; A.x  x// D 0 holds, because x is a solution of (4.19):
.Ax  f; A.x  x// D .AT .Ax  f /; x  x/ D .0; x  x/ D 0:
Here we again used the identity (2.11) from Chap. 2.
Let us also make use of the equation
J.x/ D J.x/ C kA.x  x/k2
2
(4.21)

4.2
Normal Equations
79
in another way: ﬁrst, if there exists a column vector y ¤ 0 for which Ay D 0 holds
(which means that the column vectors of A are linearly dependent), then AT Ay D 0,
that is, the matrix AT A is singular. Then adding this vector to the solution x of the
system, for the vector x D xCy equation A.xx/ D Ay D 0 is valid, see (4.21),
and in this case J.x/ D J.x/ in spite of the fact that x ¤ x. Thus neither the
solution of the system (4.19) nor the minimum point of J is uniquely determined.
The inverse direction is also true: if for some vector x ¤ x there holds J.x/ D
J.x/, that is, x is a minimum point, too, then it follows from (4.21) that kA.x 
x/k2 D 0, and hence A.x  x/ D 0. This means that columns of A are linearly
dependent and AT A is a singular matrix.
To summarize: the system (4.5) is usually unsolvable in the case of the
matrix (4.13), too. However, the method of least squares (4.14) always leads
to a solution. Every minimum point of function J is also a solution of (4.19)
and conversely. The matrix (4.19) is singular exactly when the matrix (4.13)
has linearly dependent columns. Then the minimization problem (4.14) and
the system (4.19) has several (inﬁnitely many) solutions.
In the singular case the simpliﬁcation of the model can again be recommended:
the linearly dependent columns of A should be omitted. This means that the
corresponding function(s) 'j should be deleted from the model, after which the
solution is uniquely determined again.
Example 1
A suitable model of a simple periodic process can be the following
F.t/ D x1 C x2 cos t C x3 sin t:
(4.22)
Imagine that measurement results are given for t D 0; 1
2; 2; 5
2; 4; 9
2, and from these we would like
to determine the parameters x1; x2; x3. Then the matrices (4.13) and (4.19) are
A D
0
BBBBBB@
1 1 0
1 0 1
1 1 0
1 0 1
1 1 0
1 0 1
1
CCCCCCA
;
AT A D
0
@
6 3 3
3 3 0
3 0 3
1
A ;
det.AT A/ D 0:
Hence, the columns of A are linearly dependent. Actually, the third row can be obtained as the
difference of the ﬁrst and second rows. Although the solution can be determined from (4.19), it
is not unique. Because of the linear dependence we can decide that the function '3.t/ D sin t
is not required in the model, it can be omitted (choosing x3 D 0). After that we obtain x1 and
x2 uniquely from (4.19) where the matrix is now regular: AT A D
6 3
3 3

. Another possibility is
to ask for more measurements, e.g. at the time point t D 1 or t D 3, while it is not worthwhile
requiring more measurements, e.g., at even time points.

80
4
The Least Squares Problem
350
300
250
200
150
100
50
0
20
15
10
5
0
t
F
t
a+b*cos(2*Pi*(t-t0)/365)
Fig. 4.4 Two two-parameter least squares solutions of Problem 6 in Sect. 4.4: linear regression
and cos-model
Example 2
Solving Problem 6 (Sect. 4.4) in the way discussed there is no problem with the singularity, and
(choosing t0 D 14, which can be found by trial and error) the solution leads to the following
normal equations:
AT Ax D

12
0:0442338
0:0442338 5:9847069

x D AT f D
 
121:0
66:926409
!
;
which have the solution x D .a; b/T  .10:124831; 11:257739/T , see Fig. 4.4. The ﬁgure shows
the result of the linear regression, too, illustrating that here this is an unsuitable model.
Example 3
Problem 5 in Sect. 4.4 corresponds to n D 3 and avoids the choice of t0, determining it implicitly.
4.3
Solution Algorithm, Test Examples
Before describing the numerical solution of the least squares problem we have to
clarify two properties of the matrix B D AT A.

4.3
Solution Algorithm, Test Examples
81
On the one hand, the matrix is symmetric: B D AT A D .AT A/T D BT , which
can also be seen directly from (4.17).
On the other hand, B is positive semideﬁnite (see (2.13) in Chap.2) as you
understand from (4.20): .Bx; x/  0 is valid for all (column) vectors x (in (4.20)
take x instead of yT ). Moreover, if the columns of A are linearly independent (that
is, there does not exist a vector x ¤ 0 such that Ax D 0), then B is positive deﬁnite:
.Bx; x/ > 0 when x ¤ 0.
In the latter case the Cholesky and LDLT factorization of (3.32) exist. However,
when B is only positive semideﬁnite then this fact will be noticed during the
factorization as in this case some entries on the main diagonal of the multiplier
QL or D under construction will be equal to zero or have the magnitude of rounding
errors, and a good program will give a singularity warning.
To describe the algorithm we assume that the value m (the number of mea-
surements), the measured data fti; figm
iD1 (time points and measurement results),
and the number n of parameters are available. We formulate the algorithm for a
polynomial model. Then the entries of the matrix A are aij D tj 1
i
and the degree
of the polynomial is (at most) n  1.
To be found are the parameters x1; : : : ; xn which ensure the best ﬁt in the sense
of (4.14).
1. j WD 1.1/n
2. i WD 1.1/m Œaij WD tj 1
i
i
jth column of matrix (4.13)
3. cj WD 0; i WD 1.1/m Œcj WD cj C aij 	 fii Computation of vector c D AT f
4. k WD 1.1/j Œbjk WD 0
5. i WD 1.1/m Œbjk WD bjk C aij 	 aikikj
Computation of the lower
triangular part of B D AT A
6. Solution of system Bx D c using such a Cholesky or LDLT factorization which
reports in the case of singularity the number of the row where the singularity
was detected (see the corresponding algorithm on page 61 where j denotes this
number).
7. In the case of singularity, issue the message “Data are not enough to determine
uniquely the model with n parameters, try to determine the model simpliﬁed by
xj D 0.” Stop.
8. Otherwise, return x, calculate of Ax  f and kAx  f k2, plot fti; figm
iD1 and
fti; .Ax/igm
iD1 on a common ﬁgure ( “measurement results” and “best ﬁt”). Stop.
As test examples consider the data on page 75 (regular case), and the above example (4.22)
where we calculate the data fi before running the algorithm, e.g., for x1 D x3 D 1; x2 D 1 at
the time points ti given there (singular case):
i
1
2
3
4
5
6
ti
0
1
2
2
5
2
4
9
2
fi
0
2
0
2
0
2
Then c D AT f D .6; 0; 6/T , and a symmetric decomposition of the matrix AT A D B exists:
AT A D
0
@
6 3 3
3 3 0
3 0 3
1
A D QL QLT ;
QL D
p
6
2
0
@
2 0 0
1 1 0
1 1 0
1
A ;

82
4
The Least Squares Problem
but this is not a Cholesky factorization because the multiplier QL is not regular. However, if we use
the model corresponding to n D 2 (that is, we choose x3 D 0), then we obtain the parameters
uniquely: x1 D 2; x2 D 2, and in this case the vector of residuals is the nullvector. The other
possibility is to complete our data with t7 D 1; f7 D 2, then the corresponding system is regular
and its solution is x1 D 1; x2 D 1; x3 D 1.
As a third test example we suggest the following regular example. The data are
i
1
2
3
4
5
6
ti
1
1
2
2
3
4
fi
8
0
7
9
6
1
and we ﬁt polynomial models of degree n  1 for several small n. The results are
n
x1
x2
x3
x4
1
5.16666
–
–
–
2
6.80488
0:75610
–
–
3
4:2
10.65
2:35
–
4
9
18.5
6
0.5
In the last case, when n D 4, the polynomial (of degree 3) ﬁts exactly the mean of the data (for
t D 1; 2) and the values fi (for t D 3; 4). There is no sense in choosing n D 5 since we have
measurements only at four points and would get a singular matrix B.
4.4
Exercises
1. Determine the parameters of the straight line F.t/ D aCbt which gives the best
ﬁt to the data in least squares sense:
i
1
2
3
4
ti
0
1
1
2
fi
1
10
2
23
2. What is the highest degree of the polynomial model which ﬁts uniquely the
following data? Calculate the parameters of this model.
i
1
2
3
4
5
ti
1
1
2
2
2
fi
1
1
2
0
3
3. Why is the system of equations Ax D f usually unsolvable, when A 2 IRmn
and m > n ? Why is the system AT Ax D AT f always solvable?
4. What action can you suggest to the user when during the Cholesky factorization
of the normal equations, in the kth step (k > 1), a singularity occurs?
5. For the data below determine the parameters a; b; c of the following model using
least squares ﬁtting:
F.t/ D a C b cos

2 t
365

C c sin

2 t
365


4.4
Exercises
83
where t denotes the time in days. The data:
Monthly mean minimal values f .t/ of the temperature at New Delhi as
obtained from the years 1961–1990 (t day, f .t/ temperature in C ı):
i
1
2
3
4
5
6
7
8
9
10
11
12
ti
15
45
74
105
135
166
196
227
258
288
319
349
fi
7.7
10.3
15.4
21.5
26.2
28.3
27.1
26.3
24.7
19.4
12.8
8.4
Here for ti we took the 15th day of each month. Having found the parameters,
rewrite the model as F.t/ D a C d cos

2 tt0
365

by using appropriate trigono-
metric identities and compute d; t0.
6. Determine parameters a; b of the following model using least squares ﬁtting:
F.t/ D a C b cos

2 t  t0
365

where t denotes the time in days. The data:
Monthly average temperature in Budapest; means of the years 1901–1950:
i
1
2
3
4
5
6
7
8
9
10
11
12
ti
15
46
74
105
135
166
196
227
258
288
319
349
fi
1.7
0.1
5.2
10.3
15.8
18.9
21.1
20.3
16.1
10.2
4.2
0.5
Take t0 D 14, but also experiment with values nearby. (Determining t0 directly
would make the problem nonlinear. We will discuss such problems in Sect. 7.4.)
Give an interpretation of the resulting parameters a; b.
7. Solve the last two least squares problems using their matrices A2 IR122 and
column vectors f by the MATLAB commands:
[L U]=lu(A’*A);
x=L\(A’*f);x=U\x;
and display the results using the obtained parameters x in their corresponding
models.
Read the help about qr decomposition and ﬁnd the parameter vector on the
basis of this decomposition.
8. Using the same MATLAB commands as in the previous example, determine the
parameters of the model F.t/ D a C b
t ﬁtting (in least squares sense) the data
i
1
2
3
4
5
6
7
8
9
10
ti
0.5
0.6
0.7
0.9
1
1.2
1.4
1.6
1.8
2
fi
8.10
7.00
6.30
5.30
5.00
4.52
4.14
3.90
3.70
3.51
Read the MATLAB help about the command polyfit and solve the problem
using this command. Apply this method also to Exercise 6.

84
4
The Least Squares Problem
9. Using the same MATLAB commands as in Exercise 7 try to ﬁnd the straight line
which ﬁts the data
i
1
2
3
4
5
ti
1  2"
1  "
1
1 C "
1 C 2"
fi
1
2
3
4
5
where " D 109. Try to explain the result (calculate the exact value of matrix
AT A and use the knowledge from Chap. 1).

5
Eigenvalue Problems
What you know about the complex numbers:
The general form of them is
a C bi; where i WD
p
1; that is i2 D 1; and a; b are real numbers:
The absolute value of a complex number a C bi is
ja C bij D
p
a2 C b2;
which is the Euclidean norm of the vector .a; b/.
If a real polynomial has a complex root: z D a C bi, then the complex conjugate z D a  bi
will also be a root of the polynomial. Moreover, jzj2 D zz is valid.
For complex n-dimensional vectors x; y, in short x; y 2 ICn, the Euclidean inner product also
contains the complex conjugate:
.x; y/ D
n
X
iD1
xiyi :
(5.1)
The Euclidean inner product can also be written in the form yx where y D .y/T is the
transpose and conjugate of the complex vector y. Two fundamental relations in connection with
the Euclidean inner product are
.˛x; y/ D ˛.x; y/
and
.x; ˛y/ D ˛.x; y/;
(5.2)
where ˛ is a complex number. You can verify them by looking at (5.1).
5.1
Fundamental Properties
We have met eigenvalues several times already in this book, e.g. in the case of norms
and condition numbers. However, eigenvalues play an important role not only in
mathematics but also in internet searching by ranking pages, or in technological
sciences by examination of stability and oscillation problems.
© Springer International Publishing AG 2016
G. Stoyan, A. Baran, Elementary Numerical Mathematics for Programmers and
Engineers, Compact Textbooks in Mathematics, DOI 10.1007/978-3-319-44660-8_5
85

86
5
Eigenvalue Problems
Consider a real matrix: A 2 IRnn. The eigenvalue problem is the following: we
want to ﬁnd a vector x ¤ 0 and a number  for which
Ax D x
(5.3)
holds. In other words, in the case of this vector x the multiplication by the matrix A
can be substituted simply by a multiplication with an appropriate number. Vector x
is the eigenvector, number  is the eigenvalue, while .; x/ together form an eigen-
pair. This pair can even be complex, and it is the solution of a nonlinear problem:
Equation (5.3) can also be considered as a system of linear equations: .A 
I/x D 0, and this has a nontrivial solution only in the case when
0 D det.A  I/ DW pA./:
Here I denotes the unit matrix and pA./ denotes the characteristic polynomial.
The constant term of this polynomial does not depend on , hence it can be obtained
by substituting  D 0, thus its value is det.A/. However, the leading coefﬁcient of
the polynomial cannot depend on A, so it can be obtained by substituting A D 0.
Since det.I/ D ./n, the leading coefﬁcient is .1/n. This means that the
degree of pA is equal to n, that is equal to the order of the matrix A, and it has
exactly n roots (counting with multiplicity): these roots are the eigenvalues of A.
Even in the case of a polynomial of degree 5 one cannot guarantee that its roots
can be expressed in terms of its coefﬁcients using a ﬁnite formula involving only
the four basic operations and radicals. Hence, usually the eigenvalues can only be
approximated with the help of iterative methods.
Moreover, even the explicit formulae of the roots of a third order polynomial may
cause difﬁculties. As an example consider the polynomial
p3.x/ WD x3  2x  5
which has a real root and a complex conjugate root-pair. These roots can be written
as follows:
 r
643
108 C 5
2
!1=3
C
2
3
q
643
108 C 5
2
1=3 ;

1
3
q
643
108 C 5
2
1=3

q
643
108 C 5
2
1=3
2
˙i
p
3
2
0
BBB@
 r
643
108 C 5
2
!1=3

2
3
q
643
108 C 5
2
1=3
1
CCCA :
Since a singular, homogeneous system of equations has at least one nontrivial
solution, to each different eigenvalue belongs at least one eigenvector. If the matrix
and the eigenvalue are real, then the eigenvector can be chosen to be real.
It is important to emphasize that eigenvectors, as nontrivial solutions of the
singular system .A  I/x D 0, are not uniquely determined: if x is a solution

5.1
Fundamental Properties
87
of (5.3), then cx (where c D const ¤ 0) will also be a solution. Hence, it is usual to
normalize the eigenvectors in some norm.
Consider an example for these properties:
A."/ D
0
@
1 " 0
0 1 0
1 1 2
1
A ;
(5.4)
where " is a real number. You can easily calculate that pA./ D .1  /2.2  /,
independently of ", that is 1 is an eigenvalue with multiplicity two, and 2 is
an eigenvalue with multiplicity one. Also independently of ", to the eigenvalue
3 D 2 there belongs the eigenvector v.3/ D .0; 0; 1/T . However, the linear system
corresponding to 1 D 2 D 1 is
.A."/  I/x D
0
@
0 " 0
0 0 0
1 1 1
1
A x D 0;
and from this you can see that x2 D 0; x1 C x3 D 0 when " ¤ 0. These
equations have only one nontrivial solution (disregarding a constant multiplier):
v.1/ D .1; 0; 1/T . Thus, there is no other eigenvector corresponding to the
eigenvalue 1. Otherwise, if " D 0, then v.1/ D .1; 0; 1/T and v.2/ D .0; 1; 1/T
will be two independent solutions of the system, and these will be eigenvectors
corresponding to the double eigenvalue 1.
5.1.1
Normal Matrices
There exists a special class of matrices which is important from the point of view of
eigenvectors, this is the class of normal matrices—which is often met in mechanical
problems. A matrix A is normal if and only if it can be diagonalized with the help
of a unitary matrix. (Later we will give a condition that is easier to check.) A matrix
U is unitary, if U U D I. The matrix U can be complex, and U  denotes the
conjugate transpose of U . Let us describe the part of the matrix equation U U D I
corresponding to the ith row and to the jth column, denoting the latter by uj, j D
1; : : : ; n:
u
i uj D .uj; ui/ D ıij D
(
1; if i D j;
0; if i ¤ j:
That is, ıij is the Kronecker delta, and we have expressed the product u
i uj
equivalently with the help of the Euclidean inner product (5.1), too. As we see,
the column vectors of U form a complete orthonormal set. The fact that A can be

88
5
Eigenvalue Problems
diagonalized with the help of the matrix U can be described with the following
expression:
U AU D D;
where D is a diagonal matrix (that is, dj—the jth column of D—is a multiple of
the jth unit vector ej: dj D djjej).
Then the numbers djj on the main diagonal of D are the eigenvalues of A, and
the columns of U are the eigenvectors of A. We can verify this by considering the
equation
AU D UD
and taking its jth column:
.AU /j D Auj D Udj D djjuj D j.A/uj :
We obtained the last equality from (5.3); comparing the ﬁrst and third parts of the
previous equality with (5.3) we see that entry djj of the main diagonal is equal to
an eigenvalue of the matrix A, which is denoted by j .A/.
Moreover, the columns uj of U are the eigenvectors of A and thus form a
complete orthonormal set.
On the other hand, a system consisting of n linearly independent eigenvectors
of a normal matrix A is not necessarily orthogonal, as in the case of multiple
eigenvalues it would be possible to use non-orthogonal eigenvectors. However, in
this case an orthogonal eigenvector system corresponding to the multiple eigenvalue
can be chosen. As an example it is sufﬁcient to mention the unit matrix, which is
normal, 1 is its eigenvalue with multiplicity n, but any system consisting of linearly
independent vectors is an eigenvector system of this matrix.
As well as the advantageous properties of the normal matrices listed above there
is a further one: it is not difﬁcult to decide, whether a matrix is normal or not.
Namely, a matrix A is normal exactly if
AA D AA
holds. This can be checked for small matrices by manual calculations, and for larger
matrices (disregarding rounding errors) with the use of a computer.
In several cases there is no need to calculate: Hermitian (A 2 ICnn; A D A),
skew-Hermitian (A D A), symmetric (A 2 IRnn; AT D A), skew-symmetric
(AT D A), unitary (see above) and orthogonal matrices (Q 2 IRnn; QT Q D I)
belong to the class of normal matrices.
However, many matrices are not normal, for example the matrix (5.4) is not
normal, independently of ". It sufﬁces to calculate that .A."/T A."//33 D 4 and
.A."/A."/T /33 D 6. (Since A."/ is real A."/ D A."/T holds.)

5.1
Fundamental Properties
89
In the case of " ¤ 0 the matrix A."/ cannot be diagonalized, because it has only
2 eigenvectors. However, in the case of " D 0 it has a complete eigenvector set and
the matrix can be diagonalized. To show this using the eigenvectors we construct
the matrix S D .v.1/; v.2/; v.2// D
0
@
1
0 0
0
1 0
1 1 1
1
A. It is a lower triangular matrix, so
it is not difﬁcult to calculate its inverse: S1 D
0
@
1 0 0
0 1 0
1 1 1
1
A. From here we obtain that
S1A.0/S D
0
@
1 0 0
0 1 0
0 0 2
1
A is indeed a diagonal matrix.
Consider also the following more simple example:
A D
 3 4
0 0

; AT A D
 9 12
12 16

; AAT D
25 0
0 0

:
Here, A is an upper triangular matrix, hence pA./ D Qn
iD1.aii / D .3/./,
that is the eigenvalues stand on its main diagonal: 1 D 3; 2 D 0. You can
check directly that the corresponding eigenvectors are v.1/ D 1
0
; v.2/ D  4
3
.
Constructing the matrix S from these vectors you obtain
S D
 1
4
0 3

; AS D
 3 0
0 0

; S1 D 1
3
 3
4
0 1

; S1AS D
 3 0
0 0

:
Thus, the matrix A is not normal, but it can be diagonalized—as can each n  n
matrix which has n different eigenvalues. You also see that here ST D S ¤ S1,
that is S is not a unitary matrix.
5.1.2
The Characteristic Polynomial
It seems that the simplest way to calculate the eigenvalues is to produce the
characteristic polynomial and to determine its roots. However, this method is
recommended only for small values of n:
1. Generally the calculation of the polynomial needs an arithmetic work of order of
magnitude O.n3/, and we get the coefﬁcients only with rounding errors.
2. Although the roots of a polynomial depend on the coefﬁcients of the polynomial
continuously, this is a week dependence. We have to face the situation that an
error " in the coefﬁcients can cause an error "1=n in the eigenvalues, which means
that, for example, if n is only 10 but in the matrix we have an error " D 1010,
then in the roots of the characteristic polynomial this error may increase up to 0.1.

90
5
Eigenvalue Problems
5.1.3
Localization of the Eigenvalues
It is often difﬁcult to calculate the eigenvalues, and this is especially true for multiple
eigenvalues. Besides this, if we want to use iterative methods to determine the
eigenvalues (see later), then we will need a ﬁrst guess. But, advantageously, we
can easily obtain some information about the localization of the eigenvalues.
Firstly, the eigenvalues of a Hermitian matrix (the symmetric matrix is a special
case of this) lie on the real axis, while the eigenvalues of a skew-Hermitian matrix
lie on the imaginary axis.
Next, the eigenvalues of an arbitrary matrix lie on the complex plane on a disc of
radius kAk centred at the origin (here k  k is an arbitrary induced matrix norm), see
relation (2.17) in Chap. 2.
We can tell more about the localization of the eigenvalues with the help of the
so-called Gershgorin discs:
If v ¤ 0 is an eigenvector of A belonging to some eigenvalue  and vi is a
component of v for which
jvij D max
j
jvjj D kvk1 ;
then by reordering the ith row of Av D v we will obtain that
.aii  /vi D 
n
X
j D1
j ¤i
aij vj ;
and from this
jaii  j jvij D
ˇˇˇˇˇˇˇ
n
X
j D1
j ¤i
aij vj
ˇˇˇˇˇˇˇ

X
j ¤i
jaij j jvij:
Since jvij D kvk1 > 0, we can omit this factor from the above inequality so that
jaii  j 
n
X
j D1
j ¤i
jaij j DW ri :
In this inequality the index i depends on the chosen eigenvalue and eigenvector—
that is together with  and v the index i is also unknown. But the inequality has
the geometrical meaning that the eigenvalue  lies within a disc on the complex
plane centred at aii and of radius ri. This set of the complex plane is called the ith
Gershgorin disc of the matrix A:
Gi WD fz 2 IC; jz  aiij  rig:

5.1
Fundamental Properties
91
Applying the reasoning for all eigenvalues we obtain that the eigenvalues of an
arbitrary n  n matrix A lie in the union of all Gershgorin discs of A.
This result is Gershgorin’s theorem.
Example
Let us apply Gershgorin’s theorem to the matrix (2.7) examined in Sect. 2.1. We described the
eigenvalues of this matrix on page 26.
ﬁrst Gershgorin disc: G1 WD fz 2 IC; jz  3j  5g
second Gershgorin disc: G2 WD fz 2 IC; jzj  3g
third Gershgorin disc: G3 WD fz 2 IC; jz  3j  7g
Thus, the eigenvalues lie in the union of these discs, and this information required very few
calculations. We observe that the considered matrix is not normal. We have already described the
matrix AA on page 26 and, e.g. its entry on the position .1; 1/ is equal to 17 while the entry of
the matrix AA on the same position is equal to 26, so AA ¤ AA. However, the matrix can be
diagonalized, because it has three different eigenvalues.
The following stronger form of Gershgorin’s theorem is also valid: If the union
of j (1  j < n) Gershgorin discs is disjoint from the remaining discs, then in the
union of these j discs lie exactly j eigenvalues.
To verify this let us apply the theorem to the matrix At WD D C t.A  D/ where
0  t  1. Then the centres aii of the Gershgorin discs of A are independent of
t, but ri depends on t: ri D t Pn
j D1
j ¤i jaij j. Hence, in the case of t D 0 the discs
reduce to points and they coincide with the eigenvalues of A0 D D, that is with the
entries aii of the main diagonal. When t moves towards 1 the eigenvalues (which
are continuous functions of t) can not leave the separate discs—as long as the latter
do not meet. For t D 1 we get back the original matrix and then there are still as
many eigenvalues in each disjoint group of discs as many discs belong to that group.
Consider an example of this. If the matrix is
A D
0
@
4 2 0
1
6 2
0 1 3
1
A
(5.5)
then
ﬁrst Gershgorin disc: G1 WD fz 2 IC; jz C 4j  2g
second Gershgorin disc: G2 WD fz 2 IC; jz  6j  3g
third Gershgorin disc: G3 WD fz 2 IC; jz  3j  1g
see Fig. 5.1. Now, in the union of G2 and G3 lie 2 eigenvalues (in our case a complex
conjugate eigenvalue-pair: 2;3  4:401˙0:309i). However, G1 is disjoint from G2
and G3, thus it can contain only one eigenvalue. This eigenvalue 1 has to be real:

92
5
Eigenvalue Problems
3
6
4
−
G1
G2
G3
Fig. 5.1 Gershgorin discs of matrix (5.5) and the location x of the eigenvalues
as the disc is symmetric about the real axis, if 1 were complex then its conjugate
pair would also be an eigenvalue and lie in G1. Indeed, in our case 1  3:802.
Moreover, based on the discs we can also establish that the matrix is not singular,
otherwise zero would be an eigenvalue of the matrix and it would lie in some
Gershgorin disc.
5.2
Power Iteration
With the help of an iterative procedure this method produces the eigenvalue with
the largest absolute value and the corresponding eigenvector. In each step of the
iteration a new vector is calculated, and the sequence of these vectors approximates
the eigenvector we are looking for. The corresponding eigenvalue is a by-product of
the iteration.
Essentially, each step of the power iteration is only a matrix–vector multiplica-
tion. Therefore, it is advantageous when the matrix is of large size and sparse. The
latter property means that many elements of the matrix are zero—which is often the
case in practical applications.
As before, we denote the eigenvalues and the eigenvectors of the matrix by k
and v.k/, respectively.
To illustrate the power iteration starting from some vector y0 ¤ 0, ﬁrst let us
imagine that 1 D
1
10; 2 D 1; 3 D 10, moreover,
y0 D a1v.1/ C a2v.2/ C a3v.3/;
where ai ¤ 0; i D 1; 2; 3.
Now, using (5.3) let us multiply y0 by A:
y1 WD Ay0 D A.a1v.1/ C a2v.2/ C a3v.3// D a1Av.1/ C a2Av.2/ C a3Av.3/
D a11v.1/ C a22v.2/ C a33v.3/ D 1
10a1v.1/ C a2v.2/ C 10a3v.3/;

5.2
Power Iteration
93
Then we repeat this operation with the vector y1:
y2 WD Ay1 D A. 1
10a1v.1/ C a2v.2/ C 10a3v.3//
D
1
10a1Av.1/ C a2Av.2/ C 10a3Av.3/ D
1
102 a1v.1/ C a2v.2/ C 102a3v.3/:
Repeating this in total m times:
ym WD Aym1 D
1
10ma1v.1/ C a2v.2/ C 10ma3v.3/:
(5.6)
It can be seen that (for increasing m) the ﬁrst term of this sum can be neglected,
moreover, compared with the last term the second term can also be neglected, and
we arrive at the same result when the orders of magnitude of the ai are sufﬁciently
different. For increasing m the vectors ym get closer and closer to a multiple of v.3/
and we only have to avoid an overﬂow. We can ensure this if sometimes, or in all
steps, we work with ym=kymk (using some vector norm) instead of ym:
y0 ¤ 0; m D 1; 2; : : : W x WD Aym1; ym WD x=kxk:
(5.7)
5.2.1
Conditions of Convergence
What kind of conditions are sufﬁcient for getting the same result as in the previous
example for more general matrices, i.e. under what conditions does the sequence
fymg1
mD0 converge to the eigenvector corresponding to the eigenvalue having the
largest absolute value?
1. Let the matrix A be diagonalizable, e.g. normal.
2. Let .y0; v.n// ¤ 0.
3. Let the inequality
j1j  j2j      jn1j < jnj
(5.8)
be valid.
The ﬁrst condition ensures that an arbitrary initial vector y0 can be expressed as
a linear combination of the eigenvectors.
The second condition is written with the help of the Euclidean inner prod-
uct (5.1), and it ensures that y0 contains the eigenvector we are looking for. (This
is really a matter of chance since we don’t know v.n/. In mechanical problems, this
condition is often satisﬁed by taking y0 D .1; : : : ; 1/T .)
The third condition guarantees that if the exponents of the powers of n are
increasing then these powers will differ more and more in absolute value from the
corresponding powers of the other eigenvalues.

94
5
Eigenvalue Problems
We are going to verify that our three conditions are indeed sufﬁcient to ensure
convergence when A is normal, i.e. its eigenvectors form an orthonormal system.
Remember that the latter means that
.v.i/; v.j // D ıij ; 1  i; j  n;
hold in the Euclidean inner product.
First, we express y0 as a linear combination of the vectors v.i/:
y0 D
n
X
iD1
civ.i/:
(5.9)
Then we determine the coefﬁcients ci of the linear combination. For this purpose let
us take the inner product of the relation (5.9) and v.j /, for some j, 1  j  n:
.y0; v.j // D
 n
X
iD1
civ.i/; v.j /
!
D
n
X
iD1
ci.v.i/; v.j // D
n
X
iD1
ciıij D cj :
(5.10)
It is also useful to calculate the Euclidean norm of y0:
ky0k2
2 D .y0; y0/ D
 n
X
iD1
civ.i/; y0
!
D
n
X
iD1
ci.v.i/; y0/:
In view of the calculations to follow, in the linear combination (5.9) of y0 it is
advantageous to change the summation index to j because we have already used
the index i in the previous equation:
ky0k2
2 D
n
X
iD1
ci.v.i/; y0/ D
n
X
iD1
ci
0
@v.i/;
n
X
j D1
cj v.j /
1
A D
n
X
iD1
n
X
j D1
cicj .v.i/; v.j //:
Here we have taken into consideration the relation (5.2). Finally, this implies
ky0k2
2 D
n
X
iD1
n
X
j D1
cicj.v.i/; v.j // D
n
X
iD1
n
X
j D1
cicj ıij D
n
X
iD1
jcij2:
(5.11)
For Ay0 we obtain from (5.9)
Ay0 D
n
X
iD1
ciAv.i/ D
n
X
iD1
ciiv.i/:

5.2
Power Iteration
95
Now, if we write in (5.11) Ay0 instead of y0 and cii instead of ci, then the result
is
kAy0k2
2 D
n
X
iD1
jciij2:
Similarly, we can calculate that
Amy0 D
n
X
iD1
cim
i v.i/ és kAmy0k2
2 D
n
X
iD1
jcim
i j2;
thus
kAmy0k2 D
" n
X
iD1
jcim
i j2
#1=2
:
Up to this point we have used only the ﬁrst condition of convergence. Based
on (5.10) the second condition means that cn ¤ 0. To use the third condition it
is advantageous to move out the nth terms from the sums:
Amy0 D m
n
"
cnv.n/ C
n1
X
iD1
ci
 i
n
m
v.i/
#
;
kAmy0k2 D jnjm
"
jcnj2 C
n1
X
iD1
jcij2
ˇˇˇˇ
i
n
ˇˇˇˇ
2m#1=2
:
From this relation you can see that for large m the third condition implies
Amy0 D m
n

cnv.n/ C O
n1
n
m	
 m
n cnv.n/;
kAmy0k2 D jnjm
"
jcnj2 C O
 ˇˇˇˇ
n1
n
ˇˇˇˇ
2m!#1=2
 jnjmjcnj:
Thus,
Amy0
kAmy0k2
 mv.n/;
where m D m
n cn=jm
n cnj is a complex number with absolute value 1. Disregarding
this scalar multiplier m we have obtained exactly the eigenvector we were looking
for. However, a nonzero scalar multiplier is irrelevant in the case of eigenvectors: as
a result mv.n/ is as good as v.n/.

96
5
Eigenvalue Problems
In short, we have veriﬁed the convergence. We also see that the rate of
convergence can be characterized by O

n1
n

. If the ratio jn1=nj is close to
1, then the convergence is very slow.
In what follows we discuss the importance of the three conditions of conver-
gence.
The iteration converges also in the case of a non-normal or non-diagonalizable
matrix A, that is the ﬁrst condition is less important (however, it has facilitated our
previous calculations). The second condition has been mentioned earlier, and if we
don’t detect convergence, we try another starting vector.
The important condition is the third one, which shortly means that n has to be
dominant.
A counterexample shows that without the third condition we cannot expect
convergence. Consider the matrix
A D
 0 1
1 0

; and e.g. y0 D
 
2
3
!
:
(5.12)
This matrix is symmetric, hence diagonalizable, 1;2 D ˙1 and the orthonormal
system of the eigenvectors is v.1/ D
p
2
2
1
1

; v.2/ D
p
2
2
 1
1

.
In the case of the given y0 the second condition is also satisﬁed: .y0; v.2// D

p
2
2 . However, the third condition is not satisﬁed, because j1j D j2j. The
sequence fymg does not converge: for even m we have ym D y0, while for odd m
we obtain ym D
3
2

because A only permutes the components of vectors multiplied
by it. The assumption that the two vectors appearing in the sequence inﬁnitely many
times are eigenvectors is also false.
The reason why the inequality jn1j < jnj does not hold is often the fact that
A is real but n1; n are complex conjugate: n1 D n. An example of this is the
matrix (5.5), where j1j < j2j D j3j.
The problem with the third condition is similar to that with the second; both
concern unknown quantities: we look exactly for v.n/ and n. However, the third
condition depends on the matrix and it will not help to start with another initial
vector.
Finally, we deal once more with the second condition. If .y0; v.n// D 0 or
it is small, then during the iteration (because of rounding errors) a component
with direction v.n/ arises in the vectors ym. Then the iteration converges to v.n/,
although with a delay. The time when this effect is felt depends on the applied
precision (sooner if we compute in single precision: here rounding errors are
advantageous!), moreover it depends on the rate jn1=nj of domination of
n, and ﬁnally the magnitudes of the remaining coefﬁcients ci also effect the
phenomenon.

5.2
Power Iteration
97
5.2.2
The Rayleigh Quotient
Using the power iteration we can obtain not only the eigenvector but also the
corresponding eigenvalue.
A less good but simple possibility is the following: expecting that for m large
enough Aym  nym can be valid, we obtain the approximation of n from the
components of two consecutive vectors. If
y.m/
i
¤ 0; then ymC1
i
=y.m/
i
D .Aym/i=y.m/
i
 n
can be valid for large m. However, varying i we can get n different values and it
is not obvious which we should prefer, or from which weighted sum of the latter
values a good approximation could be calculated.
The preferable method is to use the Rayleigh quotient of ym. The Rayleigh
quotient of a vector x ¤ 0 is deﬁned as
.x/ D .Ax; x/
.x; x/ :
(5.13)
We can obtain this by asking for which number  does kAx  xk2
2 attain its
minimum for a given ﬁxed vector x? Having such a number  we can hope that
if x is close to an eigenvector then  is close to the corresponding eigenvalue.
This is a least squares problem:
J./ D kAx  xk2
2 D
n
X
iD1
..Ax/i  xi/2;
whose solution—as in Chap.4—can be obtained by differentiation:
0 D dJ./
d
D 2
n
X
iD1
..Ax/i  xi/.xi/ D 2Œ.Ax; x/  .x; x/;
from which we have  D .x/, since .x; x/ D kxk2
2 > 0.
Thus, the advantage of the Rayleigh quotient  D .x/ is that it guarantees the
minimal value of kAx  xk2
2.
Let us clarify that if x is an eigenvector of A, then .x/ is the corresponding
eigenvalue  (that is, the Rayleigh quotient “does not cheat”).
In this case Ax D x and x ¤ 0, hence
.x/ D .Ax; x/
.x; x/ D .x; x/
.x; x/ D .x; x/
.x; x/ D :

98
5
Eigenvalue Problems
To summarize: The power iteration is nothing else than starting from a suitable
vector y0 ¤ 0 we obtain a sequence of vectors fymg1
mD0:
m D 1; 2; : : : W x WD Aym1; ym WD x=kxk:
This sequence fymg1
mD0 converges to the eigenvector v.n/ corresponding to the
eigenvalue n having the largest absolute value if the following (sufﬁcient)
conditions are satisﬁed:
1. The matrix A is diagonalizable, e.g. normal,
2. .y0; v.n// ¤ 0,
3. j1j  j2j      jn1j < jnj is valid.
We get (an approximation of) the corresponding eigenvalue n using the Rayleigh
quotient:
n  .m/ WD .Aym; ym/
.ym; ym/ :
The third condition is the most important among the above conditions. And
remember, a matrix A is normal if and only if AA D AA holds, or if (as an
equivalent condition) it can be diagonalized with the help of a unitary matrix U ,
that is, there exists a U such that
U U D I; U AU D D D diag.i.A//:
5.2.3
Algorithm of the Power Iteration, Test Examples
Now, it is worth describing the algorithm of the power iteration.
There are given n, the matrix A 2 IRnn, an initial vector x ¤ 0, the maximal
number of iterations maxit and an accuracy ". The numbers .m/ calculated during
the algorithm are expected to tend to the eigenvalue n having the largest absolute
value as m increases, and in the case of such success x will be the corresponding
eigenvector.
0.
m WD 0; xn WD kxk2, ? xn D 0 ?
[stop: “give a nonzero initial vector”]
1.
x WD x=xn; y WD A 	 x; .m/ WD .y; x/
2.
m:=1(1)maxit
3.
[yn WD kyk2, ? yn D 0 ?
[stop: x “eigenvector”,  D 0 is an
eigenvalue”]
4.
x WD y=yn
as a result of the normalization, kxk2 D 1
5.
y D A 	 x; .m/ WD .y; x/
6.
? j.m/  .m1/j  ".1 C j.m/j/ ? [goto 8.]]m

5.2
Power Iteration
99
7.
[stop: “the maximal number of iterations is reached,
present results:” x “eigenvector”, .m/ “eigenvalue”]
8.
? ky  .m/ 	 xk2
2  " [stop: “success!” x “approximating eigenvector”,
.m/ “approximating eigenvalue”, m “th iteration”]
9.
[stop: “No solution is found.
Current results:” x “eigenvector”, .m/ “eigenvalue”]
We draw attention to the fact that the test of step 8 concerns the square of the
norm.
Test examples:
(1) Let n D 3,
A WD
0
@
2 1
0
1
2 1
0 1
2
1
A :
Then 3 D max D 2 C
p
2  3:41421356; v.3/ D 1
2.1; 
p
2; 1/T .
Let us start the algorithm with the vector x D .1; 2; 1/T , with maximal number of iterations
maxit D 20 and with accuracy " D 0:001. Then before the loop:
y D Ax D .4; 6; 4/T ; .0/ D .x/ D 10
3 D 3:333 : : :,
moreover, in the m-loop:
mD 1 W ynD 8:24621125; x D .0:48507125; 0:72760687; 0:48507125/T ;
y D .1:69774937; 2:42535625; 1:69774937/T ; .1/ D 3:41176470
mD 2 W ynD 3:41277875; x D .0:49746833; 0:71066905; 0:49746833/T ;
y D .1:70560573; 2:41627478; 1:70560573/T ; .2/ D 3:41414141:
Then both the tests of step 6 and of step 8 have already been satisﬁed. Let us notice that the sign of
the approximation x of the eigenvector is not false: in the case of eigenvectors a nonzero constant
multiplier (here 1) does not matter!
(2) Now consider matrix (5.5), initial vector x D .1; 1; 1/T , and again, let maxit D 20; " D
0:001. Then for m D 18 the test of step 6 is satisﬁed, but in step 8 we don’t accept the result .18/ D
4:50242832 as an eigenvalue and vector x D .0:18877986; 0:82393254; 0:53431949/T as an
eigenvector. We would not get convergence even if we repeated the computation with a smaller
". This can be explained by Fig. 5.1 and the conditions of convergence: on the one hand here
j2j D j3j, because they are complex conjugate numbers, on the other hand our computation runs
on real numbers, so we could not obtain a complex result. Observe that the numbers .m/ are close
to the real part of 3.
In such a situation it is also possible that the sequence f.m/g converges (also in the case of
decreasing "), but not to the eigenvalue, as the test of step 8 would reveal. An example of this
is (5.12): then .m/ D
12
13 independently of m, in spite of the fact that this is not an eigenvalue
(1;2 D ˙1). However, in this way the test of step 6 is already satisﬁed for m D 1, and in step 8
we get ky .1/ xk2
2 D
25
169 D 0:1479 : : : Thus, it might happen (e.g. if " D 0:2), that we accept
the wrong result.
Based on the above here is some advice: the successful computation should be repeated using
the obtained approximation of the eigenvector as a new initial vector and a smaller " as new
accuracy.

100
5
Eigenvalue Problems
(3) Let n D 4,
A WD
0
BB@
4 2 0 0
1
4 2 0
0 2 4 1
0
0 2 4
1
CCA ; when 1;2 D 3 ˙ i; 3;4 D 5 ˙ i:
To start the algorithm, we take the vector x D .1; 1; 1; 1/T , maximal number of iterations 20 and
accuracy " D 0:00001. In the 19th iteration we obtain .19/ D 4:99999564 (then we are very close
to the real part of 3) and the test of step 6 is satisﬁed, but after that in step 8 ky  .19/  xk2
2 D
0:99986931, hence we do not accept the result ..19/; x/ as an eigenpair.
(4) Now we give a counterexample when A is diagonalizable, it has a dominant eigenvalue and the
power iteration converges, but not to the eigenvector corresponding to the dominant eigenvalue,
although condition .x; v.3// ¤ 0 also holds:
AD
0
@
6 2 2
15 5 7
21 3 9
1
A ; 1 D 0; 2 D 4; 3 D 12; v.3/ D
0
@
0
1
1
1
A ; x D
0
@
1
0
2
1
A :
Then .x; v.3// D 2, but Ax D
0
@
2
1
3
1
A D v.2/, which is an eigenvector belonging to the eigenvalue
4, hence from here on the multiplication by A gives only the multiples of this vector. As an
explanation we list three reasons: on the one hand in the case of exact calculation it is true that the
eigenvector belonging to the dominant eigenvalue does not occur any more, on the other hand the
eigenvectors are not orthogonal, and ﬁnally, a zero eigenvalue is also present.
In step 8 of our above algorithm, the eigen-pair .4; v.2// would be accepted and is a solution
to the eigenproblem, only j4j is not maximal.
5.2.4
The Shift
Up to this point, using the power iteration we obtain only the eigenvalue of largest
absolute value and the corresponding eigenvector.
We can help with this problem using a shift. This means that instead of A we
examine the matrix A  0I where we choose an appropriate number for 0. The
latter matrix has the same eigenvectors as A, while the eigenvalues change in a
simple way. If (5.3) holds, then
.A  0I/x D x  0x D .  0/x:
From here we see that the eigenvalues are shifted exactly by 0. If now we apply the
power iteration to the matrix A  0I, then the eigenvalues will be i  0 instead
of i, and then it well may happen that n  0 will not dominate though n was
dominating.

5.3
The Inverse Iteration
101
For example, if the matrix A is symmetric and positive deﬁnite, n > 1, then its
eigenvalues lie in the interval Œ1; n, the spectral radius of the matrix is n D .A/:
0 < 1  2      n ;
and we assume that 1 < n. Let us choose 0 D kAk1 because this can be easily
calculated. (Likewise, we can take the row sum norm of A as a shift.) Then the
eigenvalues of A  0I are in the interval Œ1  0; n  0, namely
1  0  2  0      n  0  0;
as n D .A/  kAk1 D 0. This means that the eigenvalue having the smallest
absolute value is shifted to become the eigenvalue having the largest absolute value.
Hence, iterating with the shifted matrix we obtain the approximation of the
smallest eigenvalue and the corresponding eigenvector. Denoting by .1/ the
calculated approximating eigenvalue, .1/  1  0 holds, and from here we
obtain the value 1  .1/ C 0. This method sometimes has a disadvantage—
when the largest and smallest eigenvalues differ from each other by several orders
of magnitude, the addition .1/ C 0 is in fact a subtraction where we can lose
signiﬁcant digits: 0  n >> 1, and
.1/ C 0  .1  n/ C n D n  .n  1/:
It will be worth trying the shift 0 D kAk (taking the 1- or the maximum norm) also
in the case of a general symmetric matrix if the condition of dominance (5.8) holds.
However, now the dominant eigenvalue can also be negative, and then it remains
negative after such a shift which can be noticed from the unchanged computational
result. Then we can try with the shift 0 D kAk.
Finally, we want to point out that to choose 0 D ˙kAk as the shift is not the
only reasonable possibility. If our program is able to work with complex numbers
and we suspect that the reason for the missing convergence is a dominating complex
eigenvalue pair, then it will be worth choosing a purely imaginary shift because then
only one of the two complex conjugate eigenvalues will be maximal in absolute
value.
For general nonsymmetric matrices we can also use the power iteration and
calculate approximations to the eigenvectors with eigenvalues farther away from
the origin of the complex plane.
5.3
The Inverse Iteration
Theoretically, we obtain this method by applying the power iteration to the inverse
matrix. However, instead of the previous iteration (5.7) in fact we do not perform the
operation x D A1ym1; ym WD x=kxk but we rather solve the linear system Ax D
ym1 using LU factorization. For this purpose we compute the LU factorization of

102
5
Eigenvalue Problems
A once before the start of the iteration and then we work in the usual way by solving
inside the m-loop with unchanged L and U the systems
Lz D ym1; Ux D z
where z is an auxiliary vector. Then ym WD x=kxk, like in (5.7).
5.3.1
Conditions of Convergence
Following the above idea, we get the process below called inverse iteration.
Take an initial vector x0 ¤ 0, compute L and U from A and the sequence fxmg
by
m D 1; 2; : : : W Lz D xm1; Uy D z; xm D y=kyk:
The assumptions and conditions of convergence of the power iteration are now
reformulated as follows:
If A is normal and
.x0; v.1// ¤ 0; j1j < j2j      jnj
holds, then the sequence fxmg converges to (a multiple of) v.1/, i.e. to the eigenvector
corresponding to the eigenvalue having the smallest absolute value.
The explanation of this is that if A is regular and we multiply Eq. (5.3) by A1,
then we will obtain x D A1x. Because of the regularity A cannot have a zero
eigenvalue, hence this relation can be divided by :
1
x D A1x; x ¤ 0:
This means that the eigenvalues .A1/ of the inverse matrix are the reciprocals of
the eigenvalues .A/ of the original matrix:
.A1/ D
1
.A/;
while the eigenvectors remain unchanged. Hence, if 1.A/ is an eigenvalue of A
having the smallest absolute value, then
1
1.A/ will be an eigenvalue of A1 having
the largest absolute value.
Applying the inverse iteration we obtain an approximation xm of the correspond-
ing eigenvector v.1/ and then from its Rayleigh quotient we get an approximation
.m/ of the eigenvalue 1.A/.

5.3
The Inverse Iteration
103
The great advantage of the inverse iteration compared to the power iteration is
that—applying a shift—it can be used to determine not only maximal and minimal,
but also intermediate eigenvalues.
In the case of a shift the method converges to the eigenvector which corresponds to
the eigenvalue closest to the shift 0, because if
min
i
ji  0j DW ji0  0j > 0;
then 1=.i0  0/ will be the eigenvalue of .A  0I/1 having the largest absolute
value.
Now, the rate of convergence depends on the quantity
max
i¤i0
ji0  0j
ji  0j
which is close to zero provided that 0 is a good approximation of the eigenvalue
i0 we want to ﬁnd.
Therefore, after having reached a good approximation of the eigenvalue i0 using
some trial shift 0, it is worth restarting the iteration using the just calculated
approximation of the eigenvalue as new shift.
Applying the inverse iteration it is important to know that a shift 0 which is
very close to or equals an eigenvalue will not cause difﬁculties—but we have to pay
attention to these possibilities. In the ﬁrst case it is true that we get the solution of
the equation .A  0I/y D xm1 with a large error, however the error arises almost
completely in the direction of the eigenvector belonging to the close eigenvalue,
which is just favourable for our purpose.
The second case may occur only exceptionally but we have to prepare for it with
a small auxiliary program: If during the LU factorization we detect a singularity,
then we will usually not be able to solve the original system of equations, however,
we will be able to ﬁnd a nontrivial solution of the system .A  0I/xm D 0. This is
an eigenvector and 0 turns out to be the corresponding eigenvalue.
Finally, it may happen that there are two eigenvalues of equal distance to the shift
0: think of a real matrix having a pair of complex conjugate eigenvalues—and you
are trying a real shift.
In this case the same problems arise as in the case of the power iteration.
Hence, here also it is recommended to check at the end of the iteration whether
the sequences f.m/g; fxmg have converged to an eigenpair ; x.
5.3.2
Algorithm of the Inverse Iteration, Test Examples
There are given the matrix A, the initial vector x D x0 ¤ 0, an accuracy ", the
maximal number of iterations maxit and the shift 0. We want to ﬁnd the eigenvalue
nearest to 0 and the corresponding eigenvector.

104
5
Eigenvalue Problems
0. LU WD A  0I
LU factorization with partial pivoting.
If during this a singularity occurs, then compute a nontrivial solution of .A 
0I/x D 0, [stop: x “eigenvector”,  D 0 “eigenvalue”]
1. m WD 0; xn WD kxk2, ? xn D 0 ?
[stop: “give a nonzero initial vector”]
2. x WD x=xn; .0/ WD .A 	 x; x/
3. m:=1(1)maxit
4.
[solve Lz D x and Uy D z; x WD y=kyk2; a WD A 	 x; .m/ WD .a; x/
5.
? j.m/  .m1/j  ".1 C j.m/j/ ? [goto 7.]]m
6. [stop: “The max. number of iterations is reached,
Current result:” x “vector”, .m/ “value”]
7. ? ka  .m/ 	 xk2
2  " [stop: “success!”, x “approximating eigenvector”,
.m/ “approximating eigenvalue”, m“th iteration”]
8. [stop: “No solution is found.
Current result:” x “vector”, .m/ “value”]
Instead of step 4 above one could compute there the Rayleigh quotient correspond-
ing to the inverse matrix, take its reciprocal and account for the shift:
4.’
[Lz D x; Uy D z; .m/ WD 1=.y; x/ C 0; x WD y=kyk2
This method economizes the calculation of the product a WD A 	 x, however, it is
safer to proceed according to step 4.
Test examples:
(1) Let n D 3,
A WD
0
@
2 1
0
1
2 1
0 1
2
1
A :
Then 1 D 2 
p
2  0:58578643; v.1/ D . 1
2;
p
2
2 ; 1
2/T  .0:5; 0:70710678; 0:5/T .
Let us start the algorithm with the vector x D .1; 2; 1/T , the shift 0 D 0, maximally with 2
iterations and taking the accuracy " D 0:001. Then before the loop:
xn D 2:44948974; x D .0:40824829; 0:81649658; 0:40824829/T ;
.0/ D 0:66666667;
A  x D .0:0; 0:81649658; 0:0/T ;
further, in the m-loop:
m D 1 W y D .0:81649658; 1:22474487; 0:81649658/T ;
x D .0:48507125; 0:72760687; 0:48507125/T ;
a D .0:24253562; 0:48507125; 0:24253562/T ; .1/ D 0:58823529
m D 2 W y D .0:84887468; 1:21267812; 0:84887468/T ;
x D .0:49746833; 0:71066905; 0:49746833/T ;
a D .0:28426762; 0:42640143; 0:28426762/T ; .2/ D0:58585858:

5.4
The QR Algorithm
105
Then the test of the step 5 is almost satisﬁed. (For m D 3 it would be satisﬁed, when x D
.0:49956653; 0:70771926; 0:49956653/T ; .3/ D 0:58578856, and the test of step 7 would be
true. .3/ would give the eigenvalue to 5 correct digits.)
However, next we restart the algorithm with the vector x and with the shift 0 D 0:58585858
obtained above for m D 2. First we get back this value as .0/, then there follows
mD1 W y D .6930:0892; 9800:6237; 6930:0892/T ;
x D .0:50000006; 0:70710669; 0:50000006/T ;
a D .0:29289343; 0:41421325; 0:29289343/T ; .1/ D0:58578643;
when the tests in steps 5 and 7 are satisﬁed and .1/ is equal to the eigenvalue to all visible digits.
(2) Consider the matrix (5.5) and start the algorithm with the initial vector x D .10; 1; 0/T , using
the shift 0 D 0, maximally 100 iterations and accuracy " D 0:001.
Then the convergence is rather slow, the tests are satisﬁed for m D 34 and we obtain
x D .0:99499089; 0:09893994; 0:01428307/T ; .34/ D 3:80082331:
After this last vector x and the shift 0 D 3:80082331 we restart the algorithm. Now, for m D 1
the result is
x D .0:99502591; 0:09855702; 0:01448966/T ; .1/ D 3:80190062:
Then kA  x  .m/  xk2
2 computed in step 7 is already of the order of magnitude of the rounding
errors.
It is natural to wonder if at the end we obtained such a good result only because x0 was already
close to the eigenvector. Hence, let us try an initial vector which is “innocent” from this point of
view by taking x0 D .1; 1; 1/T . And indeed, in this case the tests are satisﬁed only in the 46th
iteration, when
x D .0:99499152; 0:09866244; 0:01604948/T ; .46/ D 3:80110235:
However, let us use the information obtained easily from the Gershgorin discs, that is there
exists an eigenvalue in the interval Œ5; 3, and based on this let us choose 0 D 3. Restarting
with the vector x0 D .1; 1; 1/T in the fourth iteration we obtain
x D .0:99502705; 0:09854041; 0:01452408/T ; .4/ D 3:80193963;
and the tests of steps 5 and 7 are satisﬁed.
5.4
The QR Algorithm
This algorithm, practically, combines the power method and the inverse iteration,
and, together with an appropriate shift, it represents one of the fasted algorithms in
Numerical Linear Algebra, see [4], realized, e.g., in MATLAB’s function eig, see
also Exercises 9 and 10 below.
Comment: The MATLAB function eig is a result of several decades of math-
ematical work. It delivers (in nearly all cases) all eigenvalues and eigenvectors.
To understand it, you must understand the power method, the inverse iteration,
and the effect of shifts.

106
5
Eigenvalue Problems
The QR algorithm uses the fact that any matrix A can be decomposed (see
[4]) into an orthogonal matrix Q and an upper triangular matrix R. The algorithm
proceeds in its basic version as follows:
A D A0 D QR;
RQ D A1:
This transformation A0 ! A1 obeys the following properties:
1. A0 and A1 have the same eigenvalues because of
QA1Q1 DQ.RQ/Q1 D QR D A0.D A/; and
det.A0  I/Ddet.QA1Q1  QQ1/Ddet.Q/ det.A1  I/ det.Q1/
Ddet.QQ1/ det.A1  I/ D det.A1  I/:
2. If A D A0 is symmetric, then so is A1 since Q1 D QT for orthogonal matrices
and AT
1 D .RQ/T D QT RT D QT RT QT Q D QT .QR/T Q D QT AT
0 Q D
A1, where for the last equality see Property 1.
3. If A is of so-called Hessenberg form, i.e. its second and all further lower
diagonals are zero, then so is A1, see [4].
4. If A is symmetric and tridiagonal, then A1 also is symmetric and tridiagonal, as
follows from Properties 2 and 3.
The QR algorithm for the computation of (all) eigenvalues consists in repeating
the above transformation A0 ! A1 !    ! Ak !    and gains the eigenvalues
from the main diagonal of the matrices Ak.
Because the Hessenberg or the tridiagonal form result in less numerical work
and faster convergence it is worth transforming the original A matrix into such a
form—which can be done in about 2
3n3 arithmetical operations.
The convergence can be speeded up by calculating the eigenvalues from the ﬁrst
(or last) 2  2 minor matrix of Ak and using the largest (or smallest) eigenvalue of
these submatrices as shift, see once more [4].
Let’s take the simple example
A D
0
BB@
1 2 0
0
2 3 3 0
0 3 5 4
0
0 4 7
1
CCA
and perform two transformations using MATLAB getting A2 D
0
BB@
6:76
2:72
0
0
2:72 8:251764705882351 2:171377188038403
0
0
2:171377188038403 0:698313777294275 1:306239801072344
0
0
1:306239801072345 0:289921516823373
1
CCA :

5.5
Exercises
107
Then the left upper (right lower) 2  2 minor matrix of A2 has the maximal
(minimal) eigenvalue 10:326297301921034 (resp. 0:827986160328724) whereas
the corresponding maximal resp. minimal eigenvalues of the original matrix A are
already near: 10:640531330826247 resp. 1:094838522524648.
Let us remark that the basis of the MATLAB procedure eig, the QR decompo-
sition, could be used instead of the LU decomposition for solving linear systems:
Ax D b
! QRx D b;
Rx D QT b
and here again R is upper triangular (and its condition number equals that of A).
This approach has the advantage of not enlarging the errors contained in A and
b during A ! QT R and b ! QT b in the Euclidean norm—but at the same
time it would cost twice as many arithmetical operations and therefore is seldom
used in this context. However, for sensitive least squares problems this method can
be recommended. Then, A and R are rectangular, the condition number of R is
the same as that of A (in the Euclidean norm) and not squared like in the normal
equation approach considered in Chap. 4—remember that there the transformation
was A ! AT A.
5.5
Exercises
1. Why can we expect that in the case of the triple eigenvalue  D 4 of the matrix
below there exist 3 orthogonal eigenvectors? Find these eigenvectors.
A WD
0
BB@
5 1 1 1
1 5 1 1
1 1 5 1
1 1 1 5
1
CCA
2. Calculate the row sum norm of the following matrix and give a lower and upper
estimation for the absolute value of the eigenvalue having the smallest absolute
value. Using this information, show that A is regular and estimate the value of
kA1k1 and the condition number.
A WD
0
@
3
1 1
1
4
1
2 1
6
1
A
3. Apply the power iteration (without normalization during the iteration) to the
matrix
A WD
 2 1
1
2

;

108
5
Eigenvalue Problems
starting from the vector y0 D .1; 0/T . Compute the sequences fymg and
f.m/ D .ym/g. What can you say about convergence (look also at the
normalized vectors), where are the eigenvalues, why does the shift 0 D 2
not help?
4. Find an approximation of the eigenvalue corresponding to the approximating
eigenvector x D .8; 1; 1/T of the matrix
A WD
0
@
4 1 1
1 4 1
1 1
4
1
A :
5. What can you tell about the localization of all eigenvalues of the previous
matrix?
6. What can you tell about the properties of the following matrices and about the
localization of their eigenvalues?
A WD
0
BB@
3
1
1
0
1
8
1
0
0 1
8
1
0 1 1 3
1
CCA ;
B WD
0
BB@
3 0
1 1
2 4 1
0
1 0
3
1
1 0 2
4
1
CCA :
7. Find an approximation of the eigenvalue corresponding to the approximating
eigenvector x D .2; 3; 2/T of the matrix
A WD
0
@
5 1
2
1
5 1
2 1
6
1
A :
Based on Gershgorin discs, what do you know about the eigenvalues?
8. Find an approximation of the eigenvalue corresponding to the approximating
eigenvector x D .8; 2; 6; 0; 1/T of the matrix
A WD
0
BBBBB@
7
3
1
0
0
3 17 1
1
0
1 1
7 1
1
0
1 1 5 2
0
0
1
2 7
1
CCCCCA
:
Based on Gershgorin discs, what can you say about the eigenvalues?

5.5
Exercises
109
9. We would like to illustrate that the eigenvalues of some matrices are very
sensitive to perturbations. Compute the eigenvalues of the matrix
A D
0
BBBBB@
41;493 38;838
36;656 47;940 56;646
32;667
30;575 28;866
37;731
44;577
10;372
9707
9167 11;984 14;159
1700
1591
1502
1967
2317
153
143
136
175
201
1
CCCCCA
using the MATLAB function eig, then apply the command
A(3,3)=A(3,3)-0.0001;
and compute the eigenvalues of A again.
10. What can you observe if you apply the following MATLAB commands?
A=pascal(3)
[V,L]=eig(A)
V*L/V
11. In exact arithmetic, if " D 0 and n D 10, the following n  n matrix A D
A."/ (which you can construct using the MATLAB function diag) has one
eigenvalue equal to 0, of multiplicity n, but it has only one eigenvector,whereas,
if " ¤ 0, it has n simple eigenvalues and, correspondingly, n eigenvectors. By
experimenting with MATLAB, ﬁnd out by varying the positive integer m the
following:
For which biggest (but small) number epsm (where eps is known to
MATLAB and represents the machine relative accuracy considered in Chap. 1
and named there "1) are there, as a numerical result, less than n eigenvectors?
Further, for which biggest such (but even smaller) number there is only one
eigenvector?
Compute the number of eigenvectors using the MATLAB function rank on
the output matrix V in [V,D]=eig(A).
A."/ D
0
BBBBBBB@
0 1
0 : : : : : : 0
0 0
1
0 : : : 0
0 : : : : : : : : : : : : 0
0 : : : : : : 0
1 0
0 : : : : : : : : : 0 1
" 0 : : : : : : : : : 0
1
CCCCCCCA
:
If you reach rank(V)=1, have a look at V.

6
Interpolation
About substituting into expressions:
If f; g; h are three functions deﬁned on the interval Œa; b such that
f .x/ D g.x/ C h.x/;
and we want to substitute the value x0 2 Œa; b into f , then we have to do it in the following
way:
f .x0/ D g.x0/ C h.x0/:
It would not be a substitution to write here, e.g., h.x/ in place of h.x0/.
Example: if g.x/ D a C b.x  2/ and h.x/ D c.x  2/.x  3/, then
f .x/ D g.x/ C h.x/ D a C b.x  2/ C c.x  2/.x  3/
and here the substitution of x D 2 results in
f .2/ D g.2/ C h.2/ D a C b.2  2/ C c.2  2/.2  3/ D a:
You have studied in Calculus the Taylor polynomial with a remainder term: If a < b and
the real function f W
Œa; b ! IR is k times continuously differentiable (k  1), then for all
x 2 Œa; b
f .x/Df .a/ C .x  a/f 0.a/ C .x  a/2
2
f ".a/ C    C .x  a/k
kŠ
f .k/.a C .x  a/#.x//;
where # is a continuous function which maps the interval Œa; b into .0; 1/. If, for example,
k D 1 and f is once continuously differentiable, then
f .x/Df .a/ C .x  a/f 0.a C .x  a/#.x//; x 2 Œa; b; 0 < #.x/ < 1:
In the form f .x/f .a/
xa
D f 0.a C .x  a/#.x// this is the mean value theorem.
© Springer International Publishing AG 2016
G. Stoyan, A. Baran, Elementary Numerical Mathematics for Programmers and
Engineers, Compact Textbooks in Mathematics, DOI 10.1007/978-3-319-44660-8_6
111

112
6
Interpolation
6.1
Interpolation Problems
We know from the previous chapter that to calculate an intermediate eigenvalue 
of a large-size matrix A can be quite expensive. It often occurs that the matrix A
depends continuously on a parameter x and consequently  is a continuous function
of x. Hence, it seems to be practical to calculate the function  only for some values
x and after that to connect—interpolate—smoothly in some way the obtained -
values, using a method requiring signiﬁcantly fewer operations. Then we have to be
aware that the price of the decreased cost is the following: moving away from the
exact values we obtain the value of .x/ with an increasing error.
The essence of the problem does not change if .x/ is not an eigenvalue, but
some other costly computable function of the variable x.
Similarly to Chap.3, we look again for a simply computable function which ﬁts
the values fi D f .xi/ given at speciﬁed points xi. However, now we assume that
the values f .xi/ are available error-free, therefore we wish the function to ﬁt the
data exactly.
From the point of view of the solution of interpolation problems, polynomials
form one of the most important classes of functions because we can calculate easily
with polynomials. For example, we can obtain a value of the polynomial pn with
the help of the Horner scheme. If
p.x/ D a0 C a1x C    C anxn D
n
X
j D0
aj xj
(6.1)
then the algorithm of the Horner scheme can be described in the following way:
Given x, n and the coefﬁcients faign
iD0 , we want to ﬁnd pn.x/.
1.
p D an
2.
i WD n  1.1/0 Œp D p 	 x C aii
3.
stop [result: p D pn.x/]
6.2
Lagrangian Interpolation
6.2.1
Lagrange Interpolation Problem
Imagine we know the numbers fxign
iD0 and ffign
iD0 , moreover, there holds
xi ¤ xj ;
0  i ¤ j  n:
(6.2)
We call the points fxig nodes, and the pairs .xi; fi/ are our data points. We denote
by Pn the class of polynomials whose degrees are at most n.

6.2
Lagrangian Interpolation
113
y
y0
y2
y1
yn
y3
x0
x1
x2
x3
xn
x
.... .... ....
Fig. 6.1 Lagrange interpolational problem
We want to ﬁnd a polynomial p 2 Pn which satisﬁes the interpolation conditions
p.xi/ D fi ;
i D 0; : : : ; n:
(6.3)
Conditions (6.2)–(6.3) deﬁne the Lagrange interpolation problem, see Fig. 6.1. Let
us verify that conditions (6.3) are equivalent to a system of linear equations.
We look for a polynomial p in the form (6.1) where the coefﬁcients ai are
unknown so far. We expect that these unknowns can be determined from the
following linear equations which correspond to conditions (6.3):
p.xi/ D a0 C a1xi C    C anxn
i D fi ;
i D 0; 1; : : : ; n:
(6.4)
We summarize this system in matrix-vector form:
VaDf; V WD
0
BBB@
1 x0 x2
0 : : : xn
0
1 x1 x2
1 : : : xn
1
::: :::
:::
:::
1 xn x2
n : : : xn
n
1
CCCA ; a WD
0
BBB@
a0
a1
:::
an
1
CCCA ; f WD
0
BBB@
f0
f1
:::
fn
1
CCCA :
(6.5)
From the point of view of the computer aided solution it is important that for the
conditions (6.2) there is only one polynomial in Pn which satisﬁes relations (6.3)
for arbitrary values fi. If (6.2) is true, then V is a regular (so-called Vandermonde)
matrix (and it can be veriﬁed by induction that for its determinant det.V / D
Q
0i<j n.xj  xi/ holds). Then we can calculate the solution, e.g., using Gaussian
elimination with pivoting.
We can also understand the unique solvability of the problem using different rea-
soning, without referring to the Vandermonde matrix. Consider two polynomials—g

114
6
Interpolation
and h—in Pn which solve the Lagrange interpolation problem. Then q D g  h
is a polynomial of degree (at most) n which is equal to zero in all the n C 1
nodes. However, according to the fundamental theorem of algebra a polynomial
of degree n has at most n roots—or the polynomial vanishes identically. Hence,
there remains only the possibility p.x/  0, that is g D h. Accordingly, the
Lagrange interpolation problem has at most one solution. However, based on the
above reasoning this problem is equivalent to a system of linear equations, and if
this system has at most one solution for arbitrary right-hand side vectors then the
matrix cannot be singular, see property 3 of singular matrices on page 37. In other
words, the system always has only one solution.
The formula of the solution is known. Let us begin with the fact that for n > 1
the polynomial
n
Y
j D0
j ¤i
.x  xj /
is of degree n and disappears in all nodes but xi. Then
q.n/
i .x/ WD
n
Y
j D0
j ¤i
x  xj
xi  xj
satisﬁes the relations q.n/
i .xk/ D ıik D
(
1;
i D k;
0;
i ¤ k:
For n D 1 take q.1/
1 .x/  1. Hence, the polynomials q.n/
i
form a basis of the
Lagrange interpolation, see also Fig. 6.2. With the help of these polynomials the
solution of the problem (6.2)–(6.3) can be written in the form
Ln.x/ D Ln.f; x/ WD
n
X
iD0
fiq.n/
i .x/ D
n
X
iD0
fi
n
Y
j D0
j ¤i
x  xj
xi  xj
:
(6.6)
It is sufﬁcient to substitute x D xk here, and from the sum there remains only one
term which is exactly fk:
Ln.xk/ D
n
X
iD0
fiq.n/
i .xk/ D
n
X
iD0
fiıik D fk; k D 1; : : : ; n:
Unfortunately, it is difﬁcult to work with the formula (6.6). If, for example, we wish
to determine the maximum of the polynomial, then we would have to differentiate
the polynomial (6.6) and we would obtain a long expression.
Moreover, the above-mentioned way, the application of Gaussian elimination
cannot be recommended either.
The Vandermonde matrix is often rather ill-conditioned (e.g. in the important
case of equidistant nodes: xi  xi1 D const > 0; i D 1; 2; : : : ; n). Also, the
solution would require O.n3/ operations.

6.2
Lagrangian Interpolation
115
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
−1
−0.5
0
0.5
1
1.5
2
q1
(7)(x)
its data points
q7
(7)(x)
its data points
Fig. 6.2 Two basis polynomials of Lagrangian interpolation when the nodes are xi
D
i=7; i
D
0; : : : ; 7.
For
better
visibility
of
the
captions,
the
MATLAB-command
set(gca,’fontweight’,’bold’) has been used
6.2.2
Newton’s Divided Differences Method
However, there exists a recursive method for the calculation of the interpolation
polynomial which requires considerable fewer operations: using the so-called
Newton’s interpolation polynomial.
In order to derive this, denote by Nk the polynomial of degree k, 0  k  n,
which interpolates the data fxi; figk
iD0. Then Nk.x/ D Lk.x/ due to the unique
solvability of the interpolation problem, but we will write Nk in another form
than (6.6).
When k D 0,
N0 D b0 (where b0 WD f0/
(6.7)
is the polynomial of degree zero we are looking for. Further, building on this
we look for the polynomial of degree at most one interpolating the data points
.x0; f0/; .x1; f1/ in the following form:
N1.x/ D N0 C b1.x  x0/:

116
6
Interpolation
This formula, where b1 is a constant, already ensures N1.x0/ D N0 D f0. After that
b1 can be determined such that N1.x1/ D f1 will be valid. To this end we substitute
x D x1 into the formula of N1, and we demand the interpolation condition
N1.x1/ D N0 C b1.x1  x0/ D f1 :
(6.8)
Since x1  x0 ¤ 0, b1 can be calculated from here:
b1 D .f1  N0/=.x1  x0/ D f1  f0
x1  x0
:
(6.9)
We remember the obtained formula
N1.x/ D N0 C b1.x  x0/ D f0 C f1  f0
x1  x0
.x  x0/
(6.10)
from the secondary school. If f1 D f0, then the above polynomial will be of order
zero.
Now, consider the case k D 2. Knowing N1 we want to ﬁnd the polynomial of
degree at most two interpolating the data points .x0; f0/; .x1; f1/; .x2; f2/ in the
following form:
N2.x/ D N1.x/ C b2.x  x0/.x  x1/:
Here the term added to N1 disappears at the nodes x D x0 and x D x1, hence N2
also possesses the property of N1 that at these points it takes the required values
f0 and f1 independently of the still unknown constant b2. This constant can be
determined so that N2.x2/ D f2 will be true. Namely, we substitute x D x2 into the
formula for N2 and demand the result to be equal to f2:
N2.x2/ D N1.x2/ C b2.x2  x0/.x2  x1/ D f2 :
(6.11)
Here the situation is similar as before: the multiplier of b2 is .x2 x0/.x1 x0/ ¤ 0,
consequently b2 can be calculated:
b2 D
f2  N1.x2/
.x2  x0/.x2  x1/ D
1
x2  x0
f2  f1
x2  x1
 f1  f0
x1  x0

;
(6.12)
where we have used the formula (6.10) for N1 from which
N1.x2/ D f0 C f1  f0
x1  x0
.x2  x0/ D f0
x1  x2
x1  x0
C f1
x2  x0
x1  x0
D f0
x1  x2
x1  x0
C f1

1
x2  x1
C
1
x1  x0

:
If from (6.12) we obtain b2 D 0, then N2 D N1 and its order is less than two.

6.2
Lagrangian Interpolation
117
Consider the general case. In the kth step of the recursion we look for the
polynomial Nk of degree at most k in the form
Nk.x/ D Nk1.x/ C bk!k.x/;
k D 1; : : : ; n;
(6.13)
where !k is the polynomial of degree k deﬁned by
!0  1; !k.x/ WD
k1
Y
j D0
.x  xj /; k  1:
(6.14)
Due to the form of !k, for i D 0; : : : ; k1 the interpolation conditions Nk.xi/ D fi
are satisﬁed. Substituting x D xk into the formula (6.13) and demanding Nk to have
at the node xk the value fk, that is,
Nk.xk/ D Nk1.xk/ C bk!k.xk/ D fk ;
we obtain
bk D .fk  Nk1.xk//=!k.xk/;
(6.15)
since !k.xk/ D Qk1
j D0.xk  xj / ¤ 0. Looking back to (6.13) we see that
Nk.x/ D
k
X
iD0
bi!i.x/; k D 0; 1; : : : ; n:
In particular, the polynomial Nn is in this form, too:
Nn.x/ D
n
X
iD0
bi!i.x/:
(6.16)
6.2.3
The Difference Scheme
Now we need a simple method for calculation of the coefﬁcients bk. Since bk is the
leading coefﬁcient of Nk and Nk coincides with Lk, by substituting n D k into (6.6)
we obtain that
Lk.x/D
k
X
iD0
fi
k
Y
j D0
j ¤i
x  xj
xi  xj
Dxk
k
X
iD0
fi
k
Y
j D0
j ¤i
1
xi  xj
C low-order terms in x;

118
6
Interpolation
that is
bk D
k
X
iD0
fi
k
Y
j D0
j ¤i
1
xi  xj
(6.17)
is the leading coefﬁcient of Lk D Nk. In order to obtain a convenient method of
calculation, for 0  m  k  n we introduce the so-called divided differences
Œxm; : : : ; xkf WD
k
X
iDm
fi
k
Y
j Dm
j ¤i
1
xi  xj
; m < kI
Œxmf WD fm; k D m (6.18)
connected to the vector f D .f0; : : : ; fn/. Then the unknown coefﬁcients bk can be
obtained as kth divided differences
bk D Œx0; : : : ; xkf
(6.19)
corresponding to the case m D 0 in (6.18). It is important to remark that the
divided differences do not depend on the ordering of the data fxi; fign
iD0, since
interchanging .xr; fr/ and .xs; fs/ in (6.18) does not change the sum, only the sth
and rth terms interchange. Moreover, the following equality is valid:
Œxm; : : : ; xkf D ŒxmC1; : : : ; xkf  Œxm; : : : ; xk1f
xk  xm
.k > m/:
(6.20)
This equality generalizes the formulae (6.9) and (6.12), and you can verify it with
some patience by a direct calculation based on (6.18).
With the help of the recursion (6.20) we can compute all divided differences
required for (6.16) using the following scheme:
x0
Œx0f &
Œx0; x1f &
x1
Œx1f %
Œx0; x1; x2f &
&
Œx1; x2f %
. . .
. . .
. . .
&
. . .
. . .
Œx0; : : : ; xnf
. . .
. . .
. . . %
xn1
Œxn1f &
Œxn1; xnf %
xn
Œxnf %
The arrows show the order of calculation after which the needed quantities bk
stay on the upper side of the triangle. We can obtain the coefﬁcients of Newton’s
interpolation formula with this divided difference scheme in a comfortable way.

6.2
Lagrangian Interpolation
119
Example
Given ﬁve data points: .1; 1/, .1; 1/, (2,13), (3,69), (4,221), we want to ﬁnd the polynomial of
degree at most four which ﬁts the data. Then the difference scheme is in details:
xi
fi
1
1
11
1.1/ D 1
1
1
14.1/
2.1/ D 5
13.1/
21
D 14
215
3.1/ D 4
2
13
5614
31 D 21
94
4.1/ D 1
6913
32 D 56
4821
41 D 9
3
69
15256
42
D 48
22169
43
D 152
4
221
Now we read the coefﬁcients b0; b1; : : : ; bn from the upper side of the triangle:
1; 1; 5; 4; 1;
(6.21)
and using these, from (6.16) we obtain the polynomial N4:
N4.x/ D 1  .x C 1/ C 5.x C 1/.x  1/ C 4.x C 1/.x  1/.x  2/
C.x C 1/.x  1/.x  2/.x  3/:
Since the interpolation polynomial does not depend on the order of data we can
also read coefﬁcients b0; b1; : : : ; bn in another way, e.g. from the lower side of the
triangle:
221; 152; 48; 9; 1:
But this means that we ﬁrst have used the data .xn; fn/ ending up with .x0; f0/,
therefore for this coefﬁcient sequence we shall construct the polynomial N4 in the
following way (“backwards”):
N4.x/ D 221 C 152.x  4/ C 48.x  4/.x  3/ C 9.x  4/.x  3/.x  2/
C.x  4/.x  3/.x  2/.x  1/:

120
6
Interpolation
If we are interested only in the values of the polynomial, then it will not be worth
calculating the explicit form of the polynomial, because the form of the obtained
Newton’s polynomial is also suitable for determining the values. The following
algorithm (a generalization of the Horner scheme) gives the value of Nn.x/ at the
point x with O.n/ operations:
1.
p WD bn
2.
i WD n  1.1/0 Œp WD p 	 .x  xi/ C bii
3.
[stop, result: p D Ln.x/ D Nn.x/]
Before this, to produce the triangular difference scheme 1
2n2 C O.n/ operations are
needed (1 operation = 2 subtractions and 1 division).
The simplicity of the above calculation of the coefﬁcients using the difference
scheme can be appreciated more from the fact that it can be performed with two
program lines where the vector f ﬁrst contains the values f0; f1; : : : ; fn but at the
end the coefﬁcients b0; b1; : : : ; bn:
1.
i D 1.1/n
2.
Œj D n.1/i Œfj WD .fj  fj 1/=.xj  xj i/j i
Using the following algorithm we can calculate with further 1
2n2 C O.n/ operations
(multiplications and divisions) the coefﬁcients aj of the interpolation polynomial
of the form (6.1) from the divided differences b0; : : : ; bn of the Newton’s polyno-
mial (6.16):
1.
snC1 WD 1
2.
i WD 0.1/n  1
3.
Œsni WD 0; k WD 0
4.
j WD n  i.1/n
5.
Œsj WD sj  xi 	 sj C1; bk WD bk C sj 	 biC1; k WD k C 1j i
6.
[stop, result: ak D bk; k D 0; : : : ; n]
Then it costs in total n2 C O.n/ operations to produce the interpolation polynomial
in the form (6.1)—while to solve the system of equations (6.5) with the help of
Gaussian elimination, n3=3 C O.n2/ operations are needed.
In the above example, for the expanded form of the polynomial we obtain
L4.x/ D p4.x/ D 3 C 2x2  x3 C x4:
Observe that using the difference scheme we can handle the additional data
.xnC1; fnC1/, etc. easier than starting from the form (6.6) of the Lagrange inter-
polation. In order to take the new data into account we supplement the difference

6.2
Lagrangian Interpolation
121
scheme by the new data point and calculate only one further (e.g. lower) diagonal of
it. We illustrate this in the case examined above when the new data point is .0; 3/:
xi
fi
1
1
1
1
1
5
14
4
2
13
21
1
56
9
0
3
69
48
1
152
8
4
221
32
56
0
3
The zero at the end of the scheme means that the interpolation polynomial does
not change: in our case the previous polynomial p4 also ﬁts the new data since the
constant term of it is equal to 3, see also Fig. 6.3. But when adding new data, the
degree of the interpolation polynomial usually increases.
3,75
2,5
1,25
0
-1,25
200
150
100
50
0
x
y
x
y
Fig. 6.3 Lagrange
interpolation for the data
.1; 1/, .0; 3/, .1; 1/,
.2; 13/, .3; 69/, .4; 221/:
L4.x/ D 3C2x2 x3 Cx4

122
6
Interpolation
To summarize: The Lagrange interpolation problem is the following: given
the values fxign
iD0 and ffign
iD0 where (6.2) holds, i.e.
xi ¤ xj ;
0  i ¤ j  n;
we want to ﬁnd the polynomial p 2 Pn which satisﬁes the interpolation
conditions (6.3), that is
p.xi/ D fi;
i D 0; : : : ; n:
We solve this problem with the help of Newton’s recursion (6.13), i.e.
N0 D f0; Nk.x/ D Nk1.x/Cbk!k.x/; k D 1; : : : ; n; !k.x/ WD
k1
Y
j D0
.xxj /:
Here, the coefﬁcients bk can be calculated using the difference scheme in
O.n2/ operations.
6.2.4
Algorithm of Lagrangian Interpolation, Test Examples
Connecting the different programs we obtain the algorithm of Lagrange interpola-
tion.
Given n, the sequence of the nodes fxign
iD0 and the corresponding function values
ffign
iD0, we want to ﬁnd the values of the Lagrange polynomial at speciﬁed points
fxigm
iD1.
1. i D 1.1/n
2.
[ j D n.1/i
3.
[ ? xj  xj i D0 ? [stop: “error in the points: xj D xj i”, j, j-i ]
4.
fj WD .fj  fj 1/=.xj  xj i/j i
5. j D 1.1/m
6.
[ p WD fn
7.
i WD n  1.1/0 Œp WD p 	 .xj  xi/ C fii
8.
f j WD pj
9. [stop, result: ff jgm
j D1 ]
As a ﬁrst test example we recommend the example on page 119 to which belong the coefﬁcients
bk from (6.21) (which appear in the algorithm above as the fj produced in step 4), see also Fig. 6.3.
If m D 4 and
x1 D 0; x2 D 1:5; x3 D 2:5; x4 D 3:5;

6.2
Lagrangian Interpolation
123
then
f 1 D 3; f 2 D 3:1875; f 3 D 32:9375; f 4 D 128:6875:
As a second test example, it will be interesting to extend the previous example by the data point
.x5; f5/ WD . 1
7;  7111
2401/, which is a value of the polynomial obtained there, that is, it already ﬁts the
graph of the polynomial. However, using ﬂoating point computation (starting, e.g., from the point
.0:1428571429; 2:961682632/), because of the rounding errors an interpolation polynomial of
degree 5 can arise with leading coefﬁcient 8:75  1011.
6.2.5
Error Estimations
Our original purpose was to ﬁnd a polynomial p 2 Pn which takes at the points
fxig the values ffig. Now, after producing the polynomial in the form Ln or Nn,
the result can be interpreted as follows: there is a function f deﬁned on the interval
Œa; b, where
a  min
0in xi ;
b  max
0inxi ;
moreover, it satisﬁes f .xi/ D fi, i D 0; 1; : : : ; n, and we approximate this function
f by the polynomial Ln.
Then, the following question arises: how much does Ln.x/ differ from f .x/?
This is an important question because the approximation obtained by a Lagrange
polynomial is the core of several numerical methods (e.g. numerical integration, see
Chap. 8, or solution of ordinary differential equations, see Chap. 9).
If the function f is n C 1 times continuously differentiable, then we can answer
the previous question with the help of Taylor’s theorem:
f .x/ D f .a/ C .x  a/f 0.a/ C .x  a/2
2
f ".a/ C    C .x  a/n
nŠ
f .n/.a/
C.x  a/nC1
.n C 1/Š f .nC1/.a C .x  a/#.x//;
where # is a continuous function which maps the interval Œa; b into .0; 1/. On
the right-hand side of this formula stand the nth order Taylor polynomial and the
remainder term.
We can write this polynomial in several forms, e.g. using our data points
.x0; f .x0//; : : : ; .xn; f .xn//, that is, with the help of Ln.f; x/. In the latter case the
remainder term changes also giving:
f .x/ D Ln.f; x/ C f .nC1/.nC1.x//
.n C 1/Š
!nC1.x/
(6.22)

124
6
Interpolation
where !nC1 has been deﬁned in (6.14), and nC1.x/ D nC1.x0; : : : ; xn; x/ satisﬁes
the conditions
a  min.x; min
0in xi// < nC1.x/ < max.x; max
0in xi//  b:
(6.23)
In (6.22)—as we expect—on the one hand the difference between f and Ln
equals zero at the nodes, on the other hand it is identically zero in the case when f
itself is a polynomial of degree at most n.
We can rarely use the important error-formula (6.22) directly, however, an
estimation of the .n C 1/th derivative is more often available, that is,
max
x2Œa;b jf .nC1/.x/j  MnC1 :
Then we can estimate the error with the help of this upper bound:
jf .x/  Ln.f; x/j 
MnC1
.n C 1/Š.b  a/nC1; x 2 Œa; b;
(6.24)
since for x 2 Œa; b in all multipliers x  xi of !nC1.x/ the value of xi is at least a
and at most b. Hence, j!nC1.x/j  .b  a/nC1 is true.
We show an example for the application of this estimation: if we interpolate
the function sin.x/ on the interval Œa; b with a Lagrange polynomial, then based
on (6.22) for the difference we will obtain that
j sin.x/  Ln.sin; x/j  .b  a/nC1
.n C 1/Š ; x 2 Œa; b;
since the .nC1/th derivative of this function is sin.x/, cos.x/,  sin.x/ or  cos.x/,
depending on n, but jf .nC1/j  1 is always true.
Based on (6.6), we can also express the error f  Ln in another way:
f .x/ Ln.x/ D
n
Y
j D0
.x  xj /
"
f .x/
Qn
j D0.x  xj / C
n
X
iD0
fi
.xi  x/ Q
j ¤i.xi  xj/
#
D !nC1.x/  Œx0; : : : ; xn; xf;
where we have used formulae (6.17)–(6.19), too.
The previous formula together with (6.22) means that
Œx0; : : : ; xn; xf D f .nC1/.nC1.x//
.n C 1/Š
D f .nC1/.nC1.x0; : : : ; xn; x//
.n C 1/Š
:
(6.25)

6.3
Hermite Interpolation
125
When n D 0 then the relation gives the mean value theorem known from Calculus:
Œx0; xf D f .x/  f .x0/
x  x0
D f 0.1.x//:
However, if we knew only the latter relation, then in the case of the second order
differences (see (6.12) and (6.19)) we would obtain no more than
Œx0; x1; xf D
1
x  x0

 f .x/  f .x1/
x  x1
 f .x1/  f .x0/
x1  x0

D
1
x  x0
˚
f 0.1.x1; x//  f 0.1.x0; x1//

;
although corresponding to (6.25)
Œx0; x1; xf D f 00.2.x0; x1; x//
2
:
The relation (6.25) is useful if we get a new data point .xnC1; f .xnC1// where
xnC1 ¤ xi; i D 0; 1; : : : ; n, after solving an interpolation problem on x0; : : : ; xn.
In this case, continuing the difference scheme with the new data we calculate
the divided difference Œx0; : : : ; xn; xnC1f , and according to (6.25) get a value
of the expression
f .nC1/.nC1.x//
.nC1/Š
from the remainder term. Multiplying this by
!nC1 we obtain the polynomial bnC1!nC1 leading to the next term NnC1 of the
sequence (6.13), which gives an impression about the possible magnitude of the
interpolation error (practically in this case we consider the polynomial NnC1 as the
exact f ). The polynomial bnC1!nC1 will be near to the real error mainly in the
neigbourhood of the new point.
Finally, we have to remark that we cannot recommend the use of Lagrangian
interpolation in the case of large numbers n. Although the polynomial will take
the given values, it usually oscillates between the nodes (which is a consequence
of the oscillation of the basis polynomials, see Fig. 6.2, and which is expressed also
by (6.22)), and this becomes very disturbing especially near the endpoints x0 and xn.
6.3
Hermite Interpolation
Let us examine a more general problem than (6.2)–(6.3), the Hermite interpolation
problem where values of derivatives can be taken into account. In detail, we want to
ﬁnd a polynomial H which satisﬁes the following conditions:
i D 0; 1; : : : ; n W
H .j /.xi/ D fij; j D 0; 1; : : : ; mi  1;
(6.26)

126
6
Interpolation
o
o
y
x
o
x1
x2
x3
Fig. 6.4 Hermite interpolation problem, when at the points xi the function values and the ﬁrst
derivatives are given
where the xi and fij are given real numbers, the mi are natural numbers and H .j /
denotes the jth derivative of the function H. We assume that (6.2) is also valid. Pay
attention to the requirement contained in (6.26) that at a node xi all derivatives must
be given between the zeroth (the function value) and the mi  1-th derivative.
The solution of such a problem can be interesting, for example, in the case
when, besides the function values f .xi/, at some nodes we also know the values
of the derivatives, see Fig. 6.4. The Hermite interpolation polynomial is useful for
numerical integration and solution of differential equations which will be discussed
in Chaps. 8 and 9.
To get information about the solvability of the Hermite interpolation problem we
denote by m WD Pn
iD0 mi the number of conditions required by (6.26). Then there
exists a unique solution of this problem in Pm1, that is, in the set of polynomials
whose degrees are at most m  1. This assertion can be veriﬁed exactly in the same
way as in the case of the Lagrange problem, based on the fundamental theorem of
algebra, but here the polynomial can have multiple roots:
Assume that H1 and H2 are two polynomials (whose degrees are at most m  1)
satisfying conditions (6.26). Then xi is a root of the polynomial h D H1  H2 2
Pm1 with multiplicity mi. If, e.g., mi D 2, then not only h.xi/ D H1.xi/ 
H2.xi/ D 0 will be satisﬁed but also h0.xi/ D H 0
1.xi/  H 0
2.xi/ D 0 since in
this case the derivatives of H1 and H2 are equal at xi. In total there are m roots
(counting with multiplicity), but the degree of h is at most m  1 what is possible
only when h.x/  0. This means that more than one solution cannot exist. However,
the problem (6.26), (6.2) is again equivalent to a system of linear equations: we look
for the solution in the form
Hm1.x/ D
m1
X
kD0
akxk;

6.3
Hermite Interpolation
127
where the coefﬁcients ak are still unknown. Then for a given function value fi0 at
xi, similarly as before, we obtain the equation
Hm1.xi/ D
m
X
kD0
akxk
i D fi0
which is linear in the unknown ak. If the ﬁrst derivative fi1 is also given at xi, then
it will mean again a linear equation in the ak:
H 0
m1.xi/ D
m
X
kD1
kakxk1
i
D fi1 :
If the second derivative fi2 is also given, we will obtain one more equation:
H 00
m1.xi/ D
m
X
kD2
k.k  1/akxk2
i
D fi2 ;
etc. To summarize: we get a linear system which always has (independently of the
numbers fij) at most one solution. However, then it has exactly one solution.
In the solution of the Hermite interpolation problem the Lagrange polynomial
lends us a hand, namely, we will use its Newton form obtained from the difference
scheme. For this reason we assume that the given values fij can be calculated
from some sufﬁciently many times differentiable function: fi0 D f .xi/; fij D
f .j /.xi/; j D 1; : : : ; mi  1, for all i.
To understand the method let us imagine for a moment the following.
If for some i the inequality mi1 > 0 holds, then let us supplement the point xi with
mi 1 points xik whose distance from xi are k", k D 1; : : : ; mi 1, where " > 0 is
sufﬁcient small so as to make all the points xi and xik pairwise different. After that
we construct from the data f.xi; f .xi/ D fi0/gn
iD0 and f.xik; f .xik//giDn;kDmi1
iD0;kD1
the difference scheme in the usual way, we read the coefﬁcients bk D bk."/ of
the Newton polynomial from the upper diagonal, and from these we obtain the
interpolation polynomial Nm1."; x/ in the previously discussed way.
Now assume that " tends to zero. Then observe what happens with the divided
differences in the difference scheme, according to (6.23) and (6.25):
lim
"!0Œxi; xi1f D f 0.xi/; lim
"!0Œxi; xi1; xi2f D 1
2f 00.xi/;
etc. However, the Hermite problem contains exactly the numbers f 0.xi/ D fi1,
f 00.xi/ D fi2, etc., when mi D 2, mi D 3, etc. Based on this,
lim
"!0 Nm1."; x/ D Hm1.x/:
Now we can describe the direct construction of the interpolation polynomial.

128
6
Interpolation
When " D 0, in the original difference scheme each node xi appears mi times.
Let us write fi1 in the third column of the difference scheme (where the ﬁrst order
divided differences Œxi; xiC1f appear) instead of the meaningless expression 0/0
arising from the multiple listing of our data points. In a similar way, instead of 0/0
in the fourth column (where the second order divided differences Œxi1; xi; xiC1f
appear) we write 1
2fi2, and so on. In the other cases we calculate in the usual way.
Example
Given
.x0; f00 D f .0/; f01 D f 0.0// D .0; 0; 0/;
.x1; f10 D f .1/; f11 D f 0.1/; f12 D f 00.1// D .1; 1; 3; 6/;
that is m0 D 2; m1 D 3, and we want to ﬁnd the polynomial of degree at most 4 which
interpolates these data.
Solution. First we show only the repeated points and the places where an expression 0/0 would
appear:
xi
fi
0
0
0/0
0
0
*
*
*
1
1
*
*
0/0
*
1
1
0/0
0/0
1
1
, then ﬁlling in correctly:
xi
fi
0
0
0
0
0
1
1
1
1
1
2
0
3
1
1
1
6
2
3
1
1
From this we read the coefﬁcients bk in the usual way:
0; 0; 1; 1; 0
and after that we construct the interpolation polynomial:
H4 D 0 C 0  x C x2 C x2.x  1/ C 0  x2.x  1/2 D x3:
The interpolation polynomial is only of third order but it satisﬁes all the interpolation conditions.
So, it is the only solution of the problem.

6.4
Piecewise Polynomial Interpolation
129
Using once more the idea to take the limit of " ! 0, for the difference of the
m times differentiable function f and of the Hermite interpolation polynomial we
obtain
f .x/  Hm1.x/ D f .m/.m.x//
mŠ
!m.x/; m.x/ 2 .a; b/;
(6.27)
where now
!m.x/ D
n
Y
iD0
.x  xi/mi:
From this we can see that similarly to (6.24) inequality
jf .x/  Hm1.x/j  Mm
mŠ .b  a/m; x 2 Œa; b
(6.28)
is valid if maxx2Œa;b jf .m/.x/j  Mm.
The Hermite interpolation locally gives a better approximation than the Lagrange
interpolation, but near the endpoint nodes, because of the higher order of the
polynomial, one can expect even stronger oscillations.
6.4
Piecewise Polynomial Interpolation
We have previously seen that the polynomial interpolation can be easily produced,
but when the number of nodes is large it is not free from problems. Moreover, near
the end nodes and for rapid changes in the data it often gives unwanted results.
The simplest solution of this problem is piecewise polynomial interpolation. To
get this, divide the interval into sections:
a D x0 < x1 <    < xn1 < xn D b;
hi WD xiC1  xi ; h WD
max
0in1 hi :
If only the function values ff .xi/gn
iD0 are available, then for all sections Œxi1; xi
you easily produce the ﬁrst order Lagrange interpolation Li;1. This means piecewise
linear interpolation, see Fig. 6.5, and gives a continuous function due to the common
value fi for the inner point xi—seen either from Œxi1; xi or from Œxi; xiC1. Based
on (6.24), if f is two times continuously differentiable on the interval Œa; b, the
estimation of the error is
jf .x/  Li1.x/j  M2
2  h2; x 2 Œxi1; xi:
(6.29)

130
6
Interpolation
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
−1
−0.5
0
0.5
1
1.5
2
Lagrange interpolation
piecewise linear interpolation
data points
Fig. 6.5 Piecewise linear interpolation and Lagrange interpolation for the step function-like data
.0; 1/, . 1
7; 1/, . 2
7; 1/, . 3
7; /, . 4
7; 0/, . 5
7; 0/, . 6
7; 0/,.1; 0/
If we want to obtain higher order interpolation, then we should have an
appropriate number of values of derivatives at xi1 and xi. From these we can
construct the piecewise Hermite interpolation.
It is often advantageous to consider section-wise odd order Hermite interpolation
polynomials, which can be obtained by specifying at each node xi the values of the
function f and the corresponding values of the ﬁrst p derivatives. Since in this case
the order will be .2p C 1/, denote the result by Hi;2pC1; i D 1; : : : ; n.
The error bound (6.28) shows that for ﬁxed m D 2pC2 and using subintervals of
length h we can obtain an arbitrarily small distance between f and the interpolating
function by taking h sufﬁciently small:
jf .x/  Hi;2pC1.x/j 
M2pC2
.2p C 2/Š  h2pC2; x 2 Œxi1; xi:
Thus the error of the piecewise Hermite interpolation is of .2p C 2/th order in h.
The collection of functions Hi;2pC1 taken for all i deﬁnes a function on the interval
Œa; b whose ﬁrst p derivatives are continuous, but its .p C1/th derivative is usually
not continuous.
Example
Consider the case of p D 1, that is, for all i f .xi/ and f 0.xi/ are given. From these values we
can construct the piecewise third order Hermite interpolations Hi;3.x/. According to the previous
results, if f is four times continuously differentiable on the interval Œa; b the errors of these
interpolations will be O.h4/,

6.4
Piecewise Polynomial Interpolation
131
However, there is another idea for the piecewise polynomial interpolation which
uses only function values, but in spite of this it ensures an even higher order
differentiability than the piecewise Hermite interpolation:
We produce the piecewise interpolation on the whole interval Œa; b simultane-
ously, whose order will be g on each section Œxi1; xi. Since we have n sections,
we dispose over .g C 1/n free parameters. We demand the interpolation conditions
(concerning the function values) in all nodes (these are n C 1 equations), moreover,
in all inner points (x1; : : : ; xn1) the continuity of the piecewise interpolation and of
its ﬁrst d derivatives (these are in total .d C 1/.n  1/ equations). These conditions
lead to a system of linear equations (to which we will turn back later). For the sake of
the solvability of the system of these linear equations the number of free parameters
may not be less than the number of conditions, that is,
.g C 1/n  n C 1 C .d C 1/.n  1/ D .d C 2/n  d:
The most obvious solution of this inequality is d D g  1. Then we have d more
free parameters than conditions, hence we can require d further conditions. It is
practical to use these conditions at the boundary points x0 and xn.
Consider now two examples illustrating the meaning of the requirements of
continuity. Let us choose g D 3, i.e. we can write the ith polynomial as
p.i/
3 .x/ WD a.i/
3 x3 C a.i/
2 x2 C a.i/
1 x C a.i/
0 ; x 2 Œxi1; xi; 1  i  n:
For the function values, we do not use the continuity requirements p.i/
3 .xi/ D
p.iC1/
3
.xi/ at the inner points xi (1  i  n  1), but the interpolation conditions
for all different sections Œxi1; xi (1  i  n):
f .xi1/ D a.i/
3 x3
i1Ca.i/
2 x2
i1Ca.i/
1 xi1Ca.i/
0 ; f .xi/ D a.i/
3 x3
i Ca.i/
2 x2
i Ca.i/
1 xiCa.i/
0 :
(These 2n conditions have the same effect as the n C 1 interpolation conditions and
the n  1 continuity conditions.) With respect to the coefﬁcients these are linear
equations, but, at the moment, there is no connection between the coefﬁcients of the
ith and .i C 1/th sections.
However, the continuity of the ﬁrst derivatives means that
.p.i/
3 /0.xi/ D .p.iC1/
3
/0.xi/; 1  i  n  1;
or in detail:
3a.i/
3 x2
i C 2a.i/
2 xi C a.i/
1
D 3a.iC1/
3
x2
i C 2a.iC1/
2
xi C a.iC1/
1
:
These are again linear equations for the unknown coefﬁcients but now they link
the different sections. The equations of the continuity of the second derivatives are
found similarly (d D g  1 D 2).

132
6
Interpolation
This is the spline interpolation. Thus, the basic idea is that having prescribed
function values we construct the piecewise higher order interpolation not by using
values of the derivatives but by requiring their continuity.
For the spline interpolation even the g  1 D dth derivative is continuous, and,
if g > 1, to determine the free parameters we have to solve a special system of
equations. The size of this linear system is .g C 1/n, but row-wise it has only a few
nonzero coefﬁcients, and it turns out that the solution of the system can be found
with a cost that is linear in n.
The piecewise linear interpolation is a special case of the spline interpolation
(g D 1, d D 0). Besides its simplicity a further advantage is that it preserves
both non-negativity, see Fig. 6.5, and convexity. We can also call it ﬁrst order spline
interpolation.
An often used spline interpolation is that of third order when g D 3. Then d D 2,
that is, two conditions remain after specifying the function values. We can use one of
these at the point x0 and the other at the point xn. The linear system corresponding
to the third order spline interpolation can be written in tridiagonal form, if we start
piecewise from a Hermite interpolation problem where at both points xi1; xi, two
conditions are given: the data are .xi1; fi1; yi1/, .xi; fi; yi/ and the (unknown)
coefﬁcients yi correspond to values of the derivatives. These conditions ensure
the continuity of the piecewise Hermite interpolation and the continuity of its ﬁrst
derivative on the whole interval Œa; b. However, we can choose uniquely the values
yi in a way that ensures the continuity of the second derivatives of the piecewise
interpolation, too. This requirement gives the following equations:
y0 D f 0.x0/;
i D 1; 2; : : : ; n  1 W
hiyi1 C 2.hi C hi1/yi C hi1yiC1 D 3fhiŒxi1; xif C hi1Œxi; xiC1f g;
yn D f 0.xn/;
and this system can be solved with the help of the algorithm (3.42)–(3.44), because
conditions (3.40)–(3.41) hold. If the values f 0.x0/, f 0.xn/ are not available, then in
the case of the ﬁrst and last four nodes we will calculate the Lagrange interpolation
(whose degree is at most 3), and then by differentiating it and substituting x D x0
and x D xn we will obtain expressions for y0 and yn in terms of the ﬁrst and last
four function values, respectively.
Based on this construction even the second derivatives of the obtained piecewise
polynomial function—of the spline—are continuous, while the second derivative of
the piecewise third order Hermite interpolation is usually discontinuous.
In general the piecewise .2p C1/th order Hermite interpolation has a continuous
pth derivative, while in the case of a g D .2p C1/th order spline interpolation even
the g  1 D .2p/th derivative is continuous.

6.5
Exercises
133
6.5
Exercises
1. Which interpolation polynomial of degree at most 3 ﬁts the data (0,0), (1,1),
(2,8), (3,27)?
2. Find the Lagrange interpolation polynomial which ﬁts the following data: (0,4),
(2,24), (3,79), .2; 16/, (1,5).
3. Solve the following Lagrange interpolation problem:
.1; 5/, (2,25), (3,215), .0; 1/, .2; 45/.
After calculating the interpolation polynomial add the new data point
.3; 305/ to the previous data.
4. How many operations costs the calculation of the coefﬁcients of Newton’s
polynomial (6.16) in the case when we work using (6.15), obtaining the values
of the intermediate Newton’s polynomials by the generalized Horner scheme
while the values of the !k-s are found according to the deﬁnition (6.14)?
5. Find the Lagrange interpolation polynomial which ﬁts the following data:
.1; 10/, (0,1), .1; 2/, .2; 23/.
Having done this, specify the derivative at x D 2 to be 1 and calculate the
corresponding Hermite polynomial extending the former difference scheme.
6. Find the Hermite interpolation polynomial which ﬁts the data:
(a) (1,4), (3,8,0), (4,6), (7,2).
(b) (0,0,0), (1,1,3,6).
7. You are given the following numbers n0; : : : ; n5 as the ﬁrst several terms of a
sequence fnig: 1,3,1,19,105,331. Find a sensible continuation of the sequence.
8. Improve the error bound (6.29) of the piecewise linear interpolation in the
following way.
Formulate (6.22) for the case of two nodes xi1 and xi. Find the maximum of
the absolute value of the corresponding polynomial !2;i.x/ WD .x  xi1/.x 
xi/ over Œxi1; xi and express it with the help of hi1 WD xi  xi1. Finally,
assume jf 00.2.x//j to be bounded from above by M2.
9. Read the MATLAB help about interp1 and spline and write programs
for the solution of the above Exercise 2 and 3, including the graphical repre-
sentation using plot, displaying the data by markers and the interpolation
obtained as continuous curves.
10. Read the MATLAB help about polyfit and apply it to Exercise 1.
11. Using the MATLAB functions polyfit and polyval plot the Lagrange
polynomials of degree n, n D 2; 4; : : : ; 10, for the function f .x/ D
1
1C25x2 on
the interval Œ1; 1 (choose equidistant nodes). All functions (including f .x/)
should be plotted on the same ﬁgure. What do you observe?
12. Using the MATLAB functions spline and ppval plot the third order spline
corresponding to data:
xi
2
1
0
1
2
3
f .xi/
4
1
7
4
12
9
f 0.xi/
15
–
–
–
–
8

7
Nonlinear Equations and Systems
1. Remember the formula of the roots of a second order polynomial p.x/ WD ax2 C bx C c
where a ¤ 0: :
p.x/ D 0; if x D x1;2 D 1
2a
n
b ˙
p
b2  4ac
o
:
The formula of the roots of a third order polynomial (and even more of a fourth order
polynomial) can be very complicated, see the example on page 86.
For a polynomial of degree 5 one cannot guarantee that its roots can be expressed in terms of its
coefﬁcients using a ﬁnite formula involving only the four basic operations and radicals. In this
case we can get an approximate result only with the help of numerical methods.
2. We have already dealt with the partial derivatives of a function of several variables in Chap. 4.
Here, in connection with the solution of nonlinear equations, these will be important again.
Consider, for example, the function f .x; y/ D x  sin y C cos.x  y/. Then
@f .x; y/
@x
D sin y  sin.x  y/  y;
@f .x; y/
@y
D x  cos y  sin.x  y/  x:
The mathematical models of processes in technology, economics, and nature are
often nonlinear.
Nonlinear problems often also arise in mathematics, an example of this is ﬁnding
roots of a general function (e.g. of a high order polynomial). As the case of
polynomials shows, nonlinear equations mostly have several roots. And often the
user does not accept our tediously obtained solution and asks us to ﬁnd another one.
© Springer International Publishing AG 2016
G. Stoyan, A. Baran, Elementary Numerical Mathematics for Programmers and
Engineers, Compact Textbooks in Mathematics, DOI 10.1007/978-3-319-44660-8_7
135

136
7
Nonlinear Equations and Systems
7.1
Bisection Method, Fixed Point Iterations
Consider a function f W IR ! IR and assume that it is continuous on the interval
Œa; b, b > a. If the signs of the values of f change between a and b, e.g. f .a/ < 0,
f .b/ > 0, then the equation
f .x/ D 0
(7.1)
will have a root in the interval .a; b/. We can rather efﬁciently ﬁnd such a root with
the help of the bisection method:
Given the accuracy ", and the maximal number maxit of the bisections, assume
that f .a/  f .b/ < 0 holds.
1.
m WD 0, ım WD jb  aj, xm WD a, ym WD b
2.
zm WD 1
2.xm C ym/
3.
? f .xm/  f .zm/ < 0 ? ŒxmC1 WD xm, ymC1 WD zm, ! 6.]
4.
? f .zm/  f .ym/ < 0 ? ŒxmC1 WD zm, ymC1 WD ym, ! 6.]
5.
stop [results: zm, f .zm/]
6.
m WD m C 1, ım WD ım1=2
7.
? m D maxit ? [stop, information: m, zm, f .zm/, ım]
8.
? ım > "
2.1 C jxmj C jymj/? [! 2.]
9.
stop [results: m, 1
2.xm C ym/, f . 1
2.xm C ym//, ım]
In step 8 of this algorithm we have applied a mixed stopping criteria, cf. (1.11)
in Sect. 1.3: in the case of small jxmj C jymj the value " is used for checking the
absolute accuracy, while in the case of large values it ensures relative accuracy.
The weak point of the previous algorithm is the creation of the starting situation.
A possible solution is to choose several points in the interval Œa; b and after
determining the function values at these points we look for the places of sign
changes. Another possibility is to plot the graph of function f .x/ on the screen
and to locate the roots; the root-ﬁnding algorithms called thereafter are then used
only for improving the accuracy of the roots.
Another disadvantage of the method is that it gives only a stepwise approxima-
tion of the solution even when f is a linear function of x, that is, when the exact
solution could immediately be produced.
In all steps of the bisection method we halve the interval .xm; ym/ of uncertainty.
Hence, after m steps the length of this is ım D jb  aj  2m, that is, the number of
steps needed to reach the accuracy "
2.1 C jaj C jbj/ depends on " logarithmically.
We assume now that f W IRn ! IRn, which means that both f and its argument
x are vectors. Then we can consider the solutions of the system f .x/ D 0 as, e.g.,
the ﬁxed points of the function g.x/ WD x !f .x/, where ! ¤ 0 is an appropriately
chosen parameter.
Here the expression “ﬁxed point” refers to the fact that this point remains
unchanged under the mapping g, that is, g.x/ D x. In this case x D g.x/ D
x  !f .x/, which means f .x/ D 0.

7.1
Bisection Method, Fixed Point Iterations
137
In Calculus one can learn ﬁxed point theorems about the solvability of ﬁxed point
problems like the above, for example the Banach ﬁxed point theorem. This theorem
is valid also in IRn (which is now interesting for us).
If in some norm
kg.x/  g.y/k  qkx  yk for all x; y 2 IRn
(7.2)
is satisﬁed for some 0  q < 1, then the solution of the equation g.x/ D x can be
found with the help of iteration
k D 0; 1; : : : W x.kC1/ D g.x.k// D x.k/  !f .x.k//;
(7.3)
starting from an arbitrary vector x.0/. The sequence fx.k/g1
kD0 converges to the
solution of the equation, so x.k/ will be closer and closer to the solution (which
we shall denote by x), moreover in this case the solution exists and it is unique.
The advantage of the previous iteration is that it requires only substitutions. It is
called (simple or) ﬁxed point iteration.
Condition (7.2) means a continuity property. If with an appropriate constant L
kg.x/  g.y/k  Lkx  yk for all x; y 2 IRn;
then we call g Lipschitz continuous and L is its Lipschitz constant. According to
this the Banach ﬁxed point theorem requires the Lipschitz constant of g to be less
than 1: L D q < 1.
Example
Let n D 1 and x ¤ y. Then based on the mean value theorem, for continuously differentiable g,
g.x/  g.y/
x  y
D g0.y C 	  .x  y//; 	 D 	.x; y/ 2 .0; 1/;
hence, from g.x/ D x  !f .x/ there follows g0.x/ D 1  !f 0.x/ and
g.x/  g.y/ D g0.y C 	  .x  y//.x  y/ D .1  !f 0.y C 	  .x  y/// .x  y/:
Assume that the ﬁrst derivative of f does not change sign and is bounded, e.g.
0 < m1  f 0.x/  M1
for all x:
(7.4)
Then choosing a positive !
1  !M1  1  !f 0.y C 	  .x  y//  1  !m1 < 1

138
7
Nonlinear Equations and Systems
holds. Hence, it is sufﬁcient to have
! 2

0; 2
M1

;
e.g. ! D
1
M1
;
(7.5)
since in this case
1 < 1  !M1  1  !f 0.y C 	.x  y/  1  !m1 < 1
and the ﬁxed point iteration converges: (7.2) is satisﬁed, where now
q D max .j1  !m1j ; j1  !M1j/ < 1:
We call the convergence of the ﬁxed point iteration linear and its order of convergence equals 1
because in each step the error (the difference between the exact solution x and the approximation
x.k/) decreases at least to one qth of the previous error. Subtracting the two relations
x D g.x/; x.k/ D g.x.k1//
from each other we obtain that x  x.k/ D g.x/  g.x.k1//, so based on (7.2)
jx  x.k/j  qjx  x.k1/j
(7.6)
holds.
The bisection algorithm also converges linearly, and it is often faster than the
ﬁxed point iteration (when q >
1
2). However, the ﬁxed point iteration (while in
its form (7.3) necessitating a condition on the sign of f 0 like (7.4)) does not need
the sign change of f and it converges in the case of an arbitrary initial point (if
n D 1) or vector (if n > 1). This latter important property is usually called global
convergence.
7.2
Newton’s Method
First we assume that n D 1, dealing with nonlinear systems later.
We obtain Newton’s method if starting from an approximation x.0/ of the root of
Eq. (7.1) we want to determine a correction ıx for which x.0/ C ıx is a root of the
equation. With this aim, assuming that f is twice continuously differentiable and
using the Taylor polynomial with remainder term, we obtain
0 D f .x.0/ C ıx/ D f .x.0// C f 0.x.0//ıx C ıx2
2 f 00.x.0/ C 	ıx/:
(7.7)
Neglecting the second order term and using the notation x.1/ WD x.0/ C ıx we have
0 D f .x.0// C f 0.x.0//.x.1/  x.0//:
(7.8)
Since we have dropped the second order term we cannot ensure anymore that x.1/
is a root of f , but we hope that it approximates the root better than x.0/.

7.2
Newton’s Method
139
When f 0.x.0// ¤ 0 the value of x.1/ can be calculated from (7.8):
x.1/ D x.0/  f .x.0//
f 0.x.0//:
(7.9)
Continuing similarly, we obtain the following iterative method:
given x.0/;
k D 0; 1; : : : W x.kC1/ D x.k/  f .x.k//
f 0.x.k//:
(7.10)
This is Newton’s method.
It is useful to know another approach to Newton’s method: Let us substitute the
function f locally, in the neighbourhood of x.0/, by its tangent. This line takes at
x.0/ the function value f .x.0//, its slope is f 0.x.0//, hence, the determination of the
tangent line can be considered as a Hermite interpolation problem. The solution of
this may be already known from secondary school:
y D f .x.0// C f 0.x.0//.x  x.0//:
Now, we consider the root x.1/ of this straight line as a hopefully better approxima-
tion of the root of f (see Fig. 7.1):
0 D f .x.0// C f 0.x.0//.x  x.0// ) 0 D f .x.0// C f 0.x.0//.x.1/  x.0//
which gives (7.9).
1
0,5
0
-0,5
-1
1,5
1
0,5
0
-0,5
-1
x
y
x
y
x1   x0=1,5
Fig. 7.1 Newton’s method in the case of function f .x/ D x3  x. Dashed lines: function values
and tangent lines of three points

140
7
Nonlinear Equations and Systems
Finally, we can also say that Newton’s method is a ﬁxed point iteration where in
the kth step depending on x we choose ! to be
1
f 0.x.k//, see also (7.2) and (7.5).
The following (sufﬁcient) conditions ensure the convergence of Newton’s
method.
1. Function f is twice continuously differentiable;
2. for its ﬁrst derivative jf 0.x/j  m1 > 0 holds;
3. the second derivative satisﬁes jf 00.x/j  M2;
4. the starting approximation x.0/ is sufﬁciently close to the root x:
jx.0/  xj < 2m1
M2
:
(7.11)
Let us examine where these conditions are used. The ﬁrst one is needed in the series
expansion (7.7), the second one ensures that the method (7.10) is well deﬁned for
all values of k, excluding zero divisors.
From the second condition it follows that f has exactly one root because its
slope nowhere equals zero: f 0 is either positive or negative. Hence, f itself is not
identically zero, too, and e.g. if f 0 > 0, moreover, if for some x the inequality
f .x/ > 0 (or f .x/ < 0) holds, then we reach the unique root by going backward
(or forward).
The sense of the third and fourth conditions will become clear from the following
developments.
For simplicity, we will use the notations fk WD f .x.k//, f 0
k WD f 0.x.k//, f WD
f .x/ D 0. Then based on (7.10)
x.kC1/  x D x.k/  x  fk
f 0
k
(7.12)
D x.k/  x  fk  f
f 0
k
D f  fk  f 0
k.x  x.k//
f 0
k
:
(7.13)
The expression in the numerator can be reformulated with the help of the methods
described in Chap. 6. For this aim we consider the Hermite interpolation problem
where .x.k/; fk; f 0
k/ are given and we are looking for an at most ﬁrst order
polynomial which interpolates these data. The solution is
H1.x/ WD fk C f 0
k  .x  x.k//;
while its error formula is
f .x/ D H1.x/ C .x  x.k//2 f 00.2.x//
2
;

7.2
Newton’s Method
141
see (6.27). Substituting x D x for f .x/ D f we obtain the following:
f D fk C f 0
k.x  x.k// C .x  x.k//2 f 00.2.x//
2
:
Using this we can continue (7.13):
x.kC1/  x D f  fk  f 0
k.x  x.k//
f 0
k
D .x  x.k//2
2f 0
k
f 00.2.x//:
(7.14)
From here and from our second condition we get
jx.kC1/  xj  Cjx.k/  xj2;
C WD M2
2m1
:
(7.15)
This result is valid for all k, hence for k D 0, too. In the latter case our fourth
condition gives q WD Cjx.0/  xj < 1, so we have
Cjx.1/  xj  C 2jx.0/  xj2 D q2  q < 1:
This means that the property Cjx.0/  xj  q < 1 of x.0/ is inherited by x.1/.
Similarly, Cjx.k/  xj  q < 1 is valid for all k. However, then together with
(7.15) we also have
jx.kC1/  xj  qjx.k/  xj; 0  q < 1:
Based on this
jx.kC1/xj  qjx.k/xj  q2jx.k1/xj      qkC1jx.0/xj ! 0;
(7.16)
when k ! 1—which means convergence.
Introducing the errors "k WD Cjx.k/  xj we obtain from (7.15) that
"kC1  "2
k ; and generally "k  "2k
0 ; k  0:
(7.17)
The latter expression shows better how fast the convergence is when "0  q < 1.
In the case of the above-listed four conditions only one root is possible, but f
usually has more roots. Then it is sufﬁcient for the conditions to hold locally, in such
a neighbourhood of the actual root x which contains the interval deﬁned by (7.11).
Consider some examples.
1. Let f .x/ D x32x5. Then f has only one real root, see page 86. Since f .2/ D
1; f .3/ D 16, let us choose the starting value x.0/ D 2. For comparison we also

142
7
Nonlinear Equations and Systems
use iteration (7.3) where, according to (7.5), we choose the parameter ! D
1
25
because on the interval Œ2; 3 the bounds (7.4) are m1 D 10; M1 D 25.
We obtain the following approximations x.k/ (computing with many digits
and underlining those which are exact):
Newton’s method
ﬁxed point iteration
x.0/ D 2
x.0/ D 2
x.1/ D 2:1
x.1/ D 2:04
x.2/ D 2:094568 : : :
x.2/ D 2:06361 : : :
x.3/ D 2:09455148169 : : :
x.3/ D 2:07718 : : :
x.4/ D 2:094551481542326591496 : : :
x.4/ D 2:08486 : : :
x.5/ D 2:09455148154232659148238654057930296385741 : : :
x.5/ D 2:08916 : : :
These results ﬁrst of all illustrate the estimation (7.15): because of the square
standing on its right-hand side we speak about quadratic convergence, or we
say that the order of convergence of Newton’s method is 2. We can see above
the effect of that square in (7.15): the number of the accurate (underlined) digits
doubles in each step.
Due to its fast convergence Newton’s method is very popular—however, the
main part of the work is often devoted to obtaining an appropriate initial value
(fulﬁlling (7.11)).
The results also illustrate the linear convergence of the ﬁxed point iteration,
see estimation (7.6). Now, after 5 (resp. 20) iterations there are 2 (resp. 5)
accurate digits after the decimal point.
2. Consider the function f .x/ D x3  x. Then there are two initial values leading
Newton’s method into an inﬁnite loop (see on Fig. 7.1 the right and left vertices
of the parallelogram on the x axis): if x.0/ D ˙
p
5
5 , then x.1/ D x.0/, so x.2/ D
x.0/ etc., because in this case f .x.0// D  4
5x.0/, f 0.x.0// D  2
5, and from this
we have x.1/ D x.0/  f.x.0//
f 0.x.0// D x.0/  2x.0/, see (7.9).
However, if 
p
5
5
< x.0/ <
p
5
5 , then it can be checked that the iteration
remains inside of the parallelogram of the Fig. 7.1, moreover the sequence
fx.k/g1
kD0 tends to zero, that is, to the root.
Here, on the interval Œ
p
5
5 ;
p
5
5 , the inequalities 1  f 0.x/   2
5 and
jf 00.x/j 
6
p
5 hold, hence, according to the condition (7.11) the convergence
can be guaranteed when jx.0/j <
2 2
5
6
p
5
D 2
3
p
5
5 . This means that the length of the
interval of x.0/-values leading to convergence to the root x D 0 (the diameter of
the ball of attraction of x) which can be theoretically guaranteed is two-thirds
of the diameter of the real ball of attraction in our special case, which is a rather
good result.
The points x.0/
1;2 D ˙
p
3
3 at which f has extrema cannot be chosen as initial
points, since at these points f 0.x.0/
1;2/ D 0.

7.2
Newton’s Method
143
Finally, for the same function f let us examine the case when x.0/ > 1. Then
f .x.0// > 0; f 0.x.0// > 0, so x.1/ < x.0/, see (7.9). However, in this case, due
to f 00.x.0// > 0, the tangent line lies under the curve (cf. (7.7)) which also means
that the root of the tangent is on the right-hand side of 1.
In summary, 1 < x.1/ < x.0/, see also the right-hand side of Fig. 7.1. Continuing
similarly, we see that 1 < x.kC1/ < x.k/. This means that the convergence is
monotone (we continuously approach the root from the right), and condition
(7.11), i.e. a condition that on the (actually inﬁnite) interval Œ1; 1/ there would
hold jf 00.x/j  M2, is not needed. Since the function is odd, the convergence
is monotone also if x.0/ 2 .1; 1 (when we continuously approach the root
from the left).
To summarize: Newton’s method is a fundamental tool to solve nonlinear
equations. Its formula is
given x.0/ W x.kC1/ D x.k/  f .x.k//
f 0.x.k//;
k D 0; 1; : : : :
(7.18)
Newton’s method is not always convergent. The following properties ensure the
convergence:
1. The function f is twice continuously differentiable,
2. for its ﬁrst derivative there holds jf 0.x/j  m1 > 0,
3. its second derivative satisﬁes jf 00.x/j  M2 ,
4. the starting approximation x.0/ is sufﬁciently close to the root x:
jx.0/  xj < 2m1
M2
:
(7.19)
Then the convergence of Newton’s method is fast:
jx.kC1/  xj  Cjx.k/  xj2; C WD M2
2m1
:
(7.20)
Based on this, the number of accurate digits can be doubled in each step.
7.2.1
Damped Newton Method
Since in the case of a multiple root x for the ﬁrst derivative f 0.x/ D 0 holds, up to
this point we have excluded the possibility of such roots. Observing the behaviour of
the method when approaching a multiple root, we see that the convergence becomes
slower. Let us try to ﬁnd the reason for this phenomenon.

144
7
Nonlinear Equations and Systems
Assume that f is sufﬁciently differentiable and x is a root of f of multiplicity
t  1. Then due to Taylor’s theorem:
f .x/ D .x  x/t
 1
tŠf .t/.x/ C x  x
.t C 1/Šf .tC1/.x/ C   
	
;
where f .t/.x/ ¤ 0.
Using the notation ek WD x.k/  x we ﬁnd
f .x.k// D .ek/t
tŠ

f .t/.x/ C
ek
t C 1f .tC1/.x/ C O.e2
k/
	
;
f 0.x.k// D .ek/t1
.t  1/Š
h
f .t/.x/ C ek
t f .tC1/.x/ C O.e2
k/
i
D .ek/t1f .t/.x/
.t  1/Š

1 C
ek
tf .t/.x/f .tC1/.x/ C O.e2
k/
	
:
Together with ekC1 D ek  f .x.k//=f 0.x.k// (see (7.12)) this implies
ekC1 D ek  ek
t

f .t/.x/ C
ek
t C 1f .tC1/.x/ C O.e2
k/
	
	.f .t/.x//1

1 
ek
tf .t/.x/f .tC1/.x/ C O.e2
k/
	
;
because for sufﬁciently small ek

1 C
ek
tf .t/.x/f .tC1/.x/ C O.e2
k/
	1
D

1 
ek
tf .t/.x/f .tC1/.x/ C O.e2
k/
	
holds (compare this with the equation
1
1Cq D 1  q C O.q2/ for small jqj). Hence,
ekC1 D ek 
ek
tf .t/.x/

f .t/.x/ 
ek
t.t C 1/f .tC1/.x/ C O.e2
k/
	
D .1  1=t/ek C e2
k
f .tC1/.x/
t2.t C 1/f .t/.x/ C O.e3
k/:
For t D 1 this means the already known quadratic convergence, but for t > 1 it
shows that Newton’s method converges only linearly.
If t is known, the following modiﬁcation of Newton’s method ensures the
quadratic convergence again:
given x0I
x.kC1/ D x.k/  t f .x.k//
.f 0.x.k/// ; k D 0; 1; : : : :
(7.21)

7.2
Newton’s Method
145
In other words, in (7.21) we had to increase (the absolute value of) the original
Newton step ıx.k/ D  f.x.k//
.f 0.x.k/// to improve the convergence. But in general,
increasing the step is not advantageous, on the contrary, it is recommended to
decrease it:
The local (but fast) convergence can be changed into global (but slow) con-
vergence, if in (7.21) t is not an integer, but a sufﬁciently small positive number.
Obviously, we will choose a small t only when encountering problems, otherwise
we will strive to have t D 1. We call this method—(7.21) completed with a
strategy to select t—damped Newton method. Its algorithm is given in Sect. 7.3.1
for systems.
7.2.2
Secant Method
One of the problems in the application of Newton’s method is the computation of
the derivative; think of the possibility that the values of f are the output of a large
amount of code.
Generally, the simplest way is to substitute the difference quotient for the
derivative, that is,
f 0.x.k//  f .x.k/ C h/  f .x.k//
h
DW Ak:
(7.22)
Then instead of (7.10) the iteration is the following (assuming Ak ¤ 0):
given x.0/; x.kC1/ D x.k/  f .x.k//
Ak
; k D 0; 1; : : : :
An advantage of this iteration is that it can converge only to a root of f because
from x.k/ ! x.1/; Ak ! A1 ¤ 0; k ! 1, there follows
x.1/ D x.1/  f .x.1//
A1
; that is f .x.1// D 0:
If—according to our previous assumptions—f is twice continuously differen-
tiable, then the difference quotient Ak approximates the derivative to ﬁrst order:
jf 0.x.k//  Akj  1
2M2jhj; M2 WD max
Œa;b jf 00.x/j:
Also, by the mean value theorem we have Ak D f 0.x.k/C#kh/ and from the second
condition jAkj  m1 > 0.
An obvious possibility is to apply, as Ak, the divided difference
Œx.k1/; x.k/f D
fk  fk1
x.k/  x.k1/ :

146
7
Nonlinear Equations and Systems
0,5
0
-0,5
-1
1,5
1
0,5
0
x
y
x
y
x0
x1
x2
Fig. 7.2 Illustration of the secant method: two secants
The advantage is that in this case we do not need to calculate an auxiliary value of
f (like f .x.k/ C h/):
given x.0/ and x.1/ ¤ x.0/I
x.kC1/ D x.k/ 
f .x.k//
Œx.k1/; x.k/f ; k D 1; 2; : : : :
We call this iteration the secant method, see Fig. 7.2.
Its convergence can be examined similarly to the case of Newton’s method.
Namely, (compare with (7.12))
x.kC1/  x D x.k/  x 
fk  f
Œx.k1/; x.k/f
D f  fk  Œx.k1/; x.k/f  .x  x.k//
Œx.k1/; x.k/f
:
(7.23)
We can reformulate the expression in the numerator with the help of the Lagrange
problem, where .x.k1/; fk1/ and .x.k/; fk/ are given and we want to ﬁnd the
polynomial of degree at most one interpolating these data points:
L1.x/ D fk C Œx.k1/; x.k/f  .x  x.k//:

7.2
Newton’s Method
147
Its error formula is
f .x/ D L1.x/ C f 00.2.x//
2
.x  x.k//.x  x.k1//;
see (6.22). Substituting x D x we obtain a formula for f .x/ D f :
f D fk C Œx.k1/; x.k/f  .x  x.k// C .x  x.k//.x  x.k1//f 00.2.x//
2
:
Using this we can continue (7.23):
x.kC1/  x D .x  x.k//.x  x.k1//
2Œx.k1/; x.k/f
f 00.2.x//:
Now, from the second and third condition of convergence of Newton’s method we
have
jx.kC1/  xj  Cjx.k/  xjjx.k1/  xj; C WD M2
2m1
:
Introducing the error quantities "k WD Cjx.k/  xj, the result can be written in
the short form "kC1  "k"k1. If both "1  q < 1 and "0  q < 1 hold then
"2  q2  q < 1 follows, and similarly we get "k  q < 1 for all k. Returning to
the inequality "kC1  "k"k1 we obtain that "kC1  q"k which means convergence,
similarly to the case of Newton’s method.
Thus, for the secant method the conditions of the convergence are the same as for
Newton’s method with only one difference; now, besides (7.19), we have to require
condition jx.1/  xj < 2m1
M2 , too.
If both latter conditions are satisﬁed, then ı WD max."0; "1/  q < 1, and then
"kC1  ık; k  1;
can be veriﬁed. Here  WD
1
2.1 C
p
5/  1:618 : : : , the number known from
the golden section is the order of convergence of the secant method. In the
corresponding estimation (7.17) of Newton’s method, "0 and 2 are to be replaced
by ı and , respectively.
Let us clarify which method leads faster to a result with a given accuracy.
For this aim let us take into account that Newton’s method requires in each
step one function value and one derivative value and with these it decreases the
previous error bound "2k1
0
to "2k
0 . On the other hand, the secant method uses only
one function value and with this it decreases the previous error bound ık1 to ık.
We can assume that "0 and ı have the same order of magnitude, and computing the
derivative costs the same time as computing the function value. Then during one
Newton-step two steps of the secant method can be performed, meanwhile the error

148
7
Nonlinear Equations and Systems
bound of Newton’s method is reduced by a power 2, while the bound of the secant
method by a power 2, where 2 D 1 C   2:618    > 2. We can conclude
that the secant method is deﬁnitely faster—which can also be observed in practical
computations.
Finally, we mention that third order methods are usually employed only for the
determination of roots of nth order polynomials. An example of a third order method
is Müller’s method, when with the help of three subsequent values fx.k/; f .x.k//g
we compute a second order interpolation and as following approximation we choose
the root of this polynomial which is closer to the present best approximation; after
that we continue the iteration.
7.3
Solution of Systems of Equations
7.3.1
Newton’s Method
In connection with ﬁxed point iterations we have already talked about the solution
of systems of nonlinear equations. Now, we are going to deal with Newton’s method
which is often more efﬁcient than the simple ﬁxed point iteration.
Consider a vector function f which maps a column vector x 2 IRn to the column
vector f .x/ 2 IRn. We want to ﬁnd a solution of the system f .x/ D 0.
Example
n D 3 W x D .x1; x2; x3/T ; f .x/ D .f1.x/; f2.x/; f3.x//T ;
namely
f1.x/ WD x2
1 C x3 C 3;
f2.x/ WD x1 C 2x2
2  x2
3  3;
(7.24)
f3.x/ WD x2  3x2
3 C 2:
In other words, we want to solve the system
 x2
1 C x3 C 3 D 0; x1 C 2x2
2  x2
3  3 D 0; x2  3x2
3 C 2 D 0:
(7.25)
This system has eight solutions, some of them are
(1) x1 D 2:0;
x2 D 1:0;
x3 D 1:0;
(7.26)
(2) x1  2:030141344;
x2  1:773110967;
x3  1:121473876;
(7.27)
(3) x1  1:55175117;
x2  0:9483653595;
x3  0:592068307:

7.3
Solution of Systems of Equations
149
1
0,5
0
-0,5
-1
0
-5
-10
-15
-20
x
y
x
y
250*x^2+x-805*x^4+900*x^6-324*x^8-22
Fig. 7.3 Polynomial determining the solutions of the system (7.25)
With some patience you can substitute the equations into each other and obtain a polynomial of
degree 8 with respect to x3 (see Fig. 7.3). The roots of this polynomial give the third coordinates
of the solutions of the system, while the third equation gives the corresponding second coordinates
and the second gives the ﬁrst coordinates.
Now, consider a general function f which is twice continuously differentiable.
Then the Taylor polynomial with remainder term is formally the same as for n D 1:
0 D f .x.0/Cıx/ D f .x.0//Cf 0.x.0//ıxC second order terms in ıx;
(7.28)
but actually f 0 is a matrix, the Jacobian matrix J.x/ of the vector function f , that is
f 0.x/ D J.x/ WD .fij/;
fij D @fi
@xj
:
Namely: the ith row of the Jacobian matrix contains the partial derivatives of the ith
coordinate function fi.x/. For the above example
J.x/ D
0
@
2x1
0
1
1 4x2 2x3
0
1 6x3
1
A :
(7.29)

150
7
Nonlinear Equations and Systems
Neglecting the second order terms in (7.28) and reordering the obtained linear
system
x.1/ D x.0/  .J.x.0//1f .x.0//
gives the improvement of x.0/, when J.x.0// is regular (in the case of singularity we
choose another x.0/). In fact, we don’t calculate the inverse matrix, but we use LU
factorization with pivoting to solve the system
J.x.0//.x.1/  x.0// D f .x.0//:
Further improving x.1/ we obtain an iteration which is practically organized in the
following way:
given x.0/;
J.x.k//ıx.k/ D f .x.k//;
x.kC1/ D x.k/ C ıx.k/; k D 0; 1; : : : :
Fundamentally, the same 4 conditions ensure the convergence of the iteration as in
the one-dimensional case. For example, now the second and fourth conditions take
the form
k.J.x//1k  1=m1 ; and kx  x.0/k < M2
2m1
in some norm. Thus, in order to obtain a convergent sequence fx.k/g1
kD0, now we
have to choose the initial vector from the attracting ball around the root.
In the practical realization of the multidimensional Newton method the applica-
tion of damping has a special importance since together with the increase of the
dimension n the diameters of the attracting balls around the roots can be shown to
decrease like 1=n2.
7.3.2
Algorithm of Damped Newton Method, Test Examples
Given the initial vector x.0/, the maximal step size ıx (a sufﬁciently large number,
e.g. the radius of the ball (measured in the row sum norm) in which—according to
our expectation—the root lies), the maximal number maxit of the iterations and
the accuracy " (which have to be chosen according to the input, the model and
measurement error level of the system), we require the procedures computing the
vector f .x/ and the matrix J.x/ as well as a program for the LU factorization
with pivoting. This latter has to deliver three outputs: L; U; sing where sing is a
logical variable which will be ‘true’ if during the LU factorization a singularity is
detected.
1. t WD 1
2. f k WD f .x.0//; nf k WD kf kk1; nf 0 WD nf k
3. k WD 0.1/maxit
4.
[ ? nf k  " 	 .1 C nf 0/ ?

7.3
Solution of Systems of Equations
151
5.
[ stop: success: root x.k/, norm of remaining vector nf k,
number of iterations k ]
6.
Jk WD J.x.k//, Jk H) ŒL; U; sing
7.
? sing ? [ stop: singularity at x.k/, norm nf k ]
8.
t WD min.ıx 	 kJkk1=nf k; t/, ? t < 103 ? [ ` WD 0; ! 17. ]
9.
LUy WD f k
H) y
10.
` WD 1.1/8
11.
[ z WD x.k/  t 	 y; f z WD f .z/; nf z WD kf zk1
12.
? nf z < nf k ?
13.
[x.kC1/ WD z; f k WD f z; nf k WD nf z
14.
? ` D 1 ? [ t WD min.1; t 	 1:5/ ], ! 16. ]
15.
t WD t=2 ]`, ! 17.
16.
]k
17. [stop: unsuccessful halt at x.k/, nf k, k, `]
Steps 4 and 5 denote the successful exit from the root-ﬁnding algorithm if the
function values become small enough: a small norm of f .x/ convinces the user
that we have solved the nonlinear system.
In step 8 we reach inequality t  ıx 	 kJkk1=nf k for the damping parameter
t. Otherwise, if t > ıx  kJkk1=nf k were true, then
t > ıx  kJkk1
nf k
 ıx  kJkk1
kJkk1kyk1
D
ıx
kyk1
would hold where y is the still unknown solution of the system f k D f .x.k// D
LUy D J.x.k//y (step 9) for which
nf k D kf .x.k//k1 D kJ.x.k//yk1  kJ.x.k//k1kyk1 D kJkk1kyk1
always holds. As (in step 11) we will have the step size t  y, it would be an
unexpectedly large step, contradicting the choice of ıx: tkyk1 > ıx.
In step 15 we decrease the parameter t because the test of step 12 (“descent
criterion”) is not satisﬁed. Here we use that the unknown root x will also minimize
kf .x/k. If the test of step 12 is satisﬁed, then the step is successful, and in the case
when it happened without trial: ` D 1, we increase the parameter t carefully, until
it reaches the value 1 (step 14).
We consider it as a failure if t becomes too small (step 8), or its decrease in the
`-loop does not give a result (see step 15)—or the number of iterations is too large
(the end of the k-loop, step 16).
In fact, with the help of damping the fast but uncertain (locally) convergence
of Newton’s method can be changed into a slow but sure (globally) convergence,
however, under appropriate circumstances the damped method becomes the original
one (that is t will be equal to 1).
It is often useful to restart the algorithm with the obtained (approximate)solution.

152
7
Nonlinear Equations and Systems
A typical property of nonlinear systems is that they can have several different
solutions. Because of this it is worth doing the following: over the domain, where
we expect the roots, we lay a grid and we start the algorithm from each point of the
grid.
Test example. Consider the system (7.24) the Jacobian matrix of which is (7.29). Firstly,
we choose " D 0:0001; ıx D 1; maxit D 10; x.0/ D .2; 2; 1/T , that is, we start from the
neigbourhood of the root (7.27).
Then f .x.0// D .0; 2; 1/T ; kf .x.0//k1 D 2,
J.x.0// D
0
@
4 0 1
1 8 2
0 1 6
1
A D LU D
0
@
1
0
0:25
1
0
0
0:125 1
1
A
0
@
4 0
1
0 8
2:25
0 0 5:71875
1
A :
After this we get
ıx.0/ D .0:03278689; 0:21311475; 0:13114754/T ;
x.1/ D .2:03278689; 1:78688525; 1:13114754/T ; kf .x.1//k1 D 0:073636119;
ıx.1/ D .0:0026315528; 0:013716540; 0:0096237920/T ;
x.2/ D .2:03015533; 1:77316871; 1:12152375/T ; kf .x.2//k1 D 0:00028366955:
and the algorithm successfully ﬁnishes with the test of step 4. The correct digits of the found
solution are underlined.
For the starting vector x.0/ D .0:5; 0:25; 0:25/T the algorithm ﬁnishes in step 7 due to the
singularity of the Jacobian matrix.
As second test example we describe a case where the t-strategy gets working.
We start the algorithm using " D 0:0001; ıx D 1; maxit D 20; x.0/ D .0; 1:4; 1/T . Then we
reach the `-loop (steps 10–15), and as the descent criterion is not satisﬁed, we perform the `-loop
six times, always halving the value of t. Then t D 0:015625 and
x.1/ D .2:01125; 1:01875; 0:9375/T ; kf .x.1//k1 D 0:38203125;
and now the descent criterion is satisﬁed. In this way we get near to the solution (7.26) of the
system, but we cannot reach it fast because t is small. Continuing, we go on in the k-loop, and
the descent criterion is always satisﬁed even in the case of ` D 1, so in each step of the loop the
value of t increases by 50 %. After computing x.12/ D .2:00012640; 1:00019218; 0:99940784/
and kf .x.12//k1 D 0:00374409 we obtain t D 1, and then
x.13/ D .2:00000005; 1:00000001; 1:00000018/T ; kf .x.13//k1 D 0:00000105;
when the algorithm ﬁnishes successfully.
7.3.3
Approximation of the Jacobian Matrix
In each step of Newton’s method we have to calculate the Jacobian matrix, that is
n2 function values. In applications this often causes problems, especially when the
calculation of f cannot be done using analytical formulae but needs the help of a
further program. Then, similarly to the one-dimensional case, a ﬁrst idea is to work

7.3
Solution of Systems of Equations
153
with difference quotients. The n-dimensional equivalent of (7.22) is the following:
@fi.x/
@xj
 fi.x C hej/  fi.x/
h
(7.30)
where ej is the jth coordinate unit-vector and h ¤ 0 is an appropriate step size
which can depend on j. This means that in (7.30)
fi.x C hej/ D fi.x1; x2; : : : ; xj 1; xj C h; xj C1; : : : ; xn/:
Here x is usually the actual approximation x.k/ of the root at which we anyway have
to calculate fi, however, we have to calculate the value at x.k/ C hej only for the
approximation of the derivative. Hence, we need in total n2 extra function values.
It often occurs that the Jacobian matrix is a sparse matrix, e.g. a band matrix. In
this case, essentially fewer function values will be sufﬁcient.
7.3.4
Broyden’s Method
Here we are going to deal with the generalization of the secant method due to
Broyden, which is recommended when the determination of the partial derivatives
is a problem and the computation of the n2 extra function values mentioned before
costs too much.
Broyden’s method approximates the Jacobian matrix only with the help of the
already available vectors f .x.k//.
At the beginning of the iteration the approximation A0 of the Jacobian matrix
(or only its main diagonal) is calculated using the difference quotients (7.30) (or we
choose A0 D I).
So we obtain x.1/. In the kth step we take into account the following information:
x.k1/; x.k/; f .x.k1// DW fk1 ; f .x.k// DW fk; Ak1 :
Here we have got the latest vector x.k/ using the relation
x.k/ D x.k1/  A1
k1fk1:
(7.31)
We may assume that x.k/ differs from x.k1/, otherwise fk1 D 0, that is, x.k1/
would be the root we are looking for.
Our purpose is to produce Ak and after that to calculate x.kC1/ D x.k/ 
.Ak/1fk. Therefore, we require the linear function
lk.x/ WD fk C Ak.x  x.k//

154
7
Nonlinear Equations and Systems
to interpolate not only the point .x.k/; fk/ (which it automatically does), but the
point .x.k1/; fk1/, too:
lk.x.k1// D fk C Ak.x.k1/  x.k// D fk1 ;
that is,
Akv.k/ D fk  fk1; v.k/ WD x.k/  x.k1/:
(7.32)
This condition is the so-called quasi–Newton equation which results in one dimen-
sion in the secant method where fk; fk1; x.k1/; x.k/ are numbers and, according
to (7.32), Ak D
fkfk1
x.k/x.k1/ D Œx.k1/; x.k/f is valid. In several dimensions the
relation (7.32) is not sufﬁcient to determine Ak as Ak has n2 entries, moreover,
the quasi–Newton equation concerns n-dimensional vectors and yields only n
conditions.
We shall complete our condition (7.32) with the requirement that Ak has to be
close to Ak1 in the following way:
.Ak  Ak1/y D 0 for all vectors y orthogonal to v.k/:
(7.33)
This looks like only one condition (the orthogonality .v.k//T y D 0), but y has n
components and there are n  1 linearly independent vectors y satisfying (7.33).
With the choice
Ak  Ak1 D u.k/.v.k//T ; u.k/ is an unknown nonzero vector,
condition (7.33) is satisﬁed: if .v.k//T y D 0, then
.Ak  Ak1/y D u.k/.v.k//T y D 0:
Observe that the dyadic product u.k/.v.k//T gives an n  n matrix while .v.k//T y is
another form of the Euclidean inner product, that is, its result is a number.
To determine u.k/ we use that
.Ak  Ak1/v.k/ D u.k/.v.k//T v.k/ D u.k/kv.k/k2
2 ;
and now from (7.32) there follows
.Ak  Ak1/v.k/ D fk  fk1  Ak1v.k/ D u.k/kv.k/k2
2 ;
or, because according to (7.31) equation Ak1v.k/ D fk1 holds,
u.k/ D
fk
kv.k/k2
2
:

7.4
Gauss–Newton Method
155
Herewith the vector u.k/—and consequently the matrix Ak—are determined.
Having the matrix Ak we calculate the vector x.kC1/, the next approximation of
the solution of the nonlinear system f .x/ D 0.
We remark that under the conditions of Newton’s method the rate of convergence
of Broyden’s method is not worse than 1C1=2n. By the way, it does not necessarily
follow that Ak ! J.x/, k ! 1.
By also using a damping strategy we can try to change the local convergence of
Broyden’s method into global convergence:
x.kC1/ D x.k/  tk.Ak/1fk :
7.4
Gauss–Newton Method
7.4.1
Description
In Chap. 4 we dealt with ﬁtting linear models to measurement results, however, in
practice nonlinear models often arise, too. In the model
F.t/ D a C b cos

2 t  t0
365

of problem 6 of Sect. 4.4 (which we would like to ﬁt to the temperature data given
there) the parameter t0 represents a nonlinear dependence and not a linear one like
the coefﬁcients a and b; compare to (4.12). Hence, we were able to determine it
only by trial and error.
In such nonlinear cases the Gauss–Newton method is useful.
In order to describe the method the starting point is the following: we want to ﬁnd
an n-dimensional parameter vector x D .x1; : : : ; xn/T such that the model F.x; t/
ﬁts best the given data fti; figm
iD1 in the sense that the difference between the m-
dimensional vector function
G.x/ WD .F.x; t1/; F.x; t2/; : : : ; F.x; tm//T
and the m-dimensional vector f
D .f1; f2; : : : ; fm/T is small. Here n  m,
and typically n is much less than m. Hence, the system G.x/ D f usually is
overdetermined and unsolvable. Based on the knowledge described in Chap. 4 and
in the present chapter we proceed as follows.
1. Starting from an appropriate approximation x.0/ of the best parameter vector
x we are looking for, we organize an iteration in which we update the actual
parameter vector x.i/:
x.iC1/ D x.i/ C ıx.i/; i D 1; 2; : : : :

156
7
Nonlinear Equations and Systems
2. In the neigbourhood of the parameter vector x.i/ we substitute for the nonlinear
system G.x/ D f its linear approximation:
G.x.i// C Jiıx D f; Ji WD J.x.i//;
where J.x/ is the Jacobian matrix of the vector function G.
3. We obtain the solution of this linear, but usually overdetermined system with the
help of the corresponding normal equations:
.J T
i Ji/ıx D J T
i

f  G.x.i//

;
Ji D J.x.i//:
We use Cholesky factorization to solve this symmetric n  n system, and the
obtained vector ıx D ıx.i/ will be used in the iteration to get x.iC1/.
Thus, in the temperature example we have n D 3; m D 12; x1 D a; x2 D
b; x3 D t0 and
F.x; t/ D x1 C x2 cos

2 t  x3
365

; Gi.x/ D F.x; ti/; i D 1; 2; : : : ; 12:
Therefore, the entries of the ith row of the Jacobian matrix are
ji1 WD 1; ji2 WD cos

2 ti  x3
365

; ji3 WD x2
2
365 sin

2 ti  x3
365

:
7.4.2
Algorithm of the Gauss–Newton Method, Test Examples
Supplementing the previous description with a damping parameter we obtain the
following algorithm of the Gauss–Newton method.
Given the dimensions n; m, the initial guess x.0/, an accuracy ", the maximal
number of iterations maxit, the vector f of the measurement results, the procedures
computing the vector function G and its Jacobian matrix J . We are looking for a
parameter vector x for which kG.x/  f k2
2 is minimal.
1. k WD 0; t WD 1
2. [g WD G.x.k//  f; gn WD kgk2, ? k D 0 ? [ gn0 WD gn ]
3. ? gn  " 	 .1 C gn0/ ? [stop: kth iteration, parameter vector x.k/,
remainder vector g, its norm gn ]
4. J WD J.x.k//; w WD J T 	 g
5. solution of J T J v D w using Cholesky factorization,
in the case of singularity: stop
6. ` WD 1.1/5
7.
[computing y WD x.k/  tv, G.y/  f
8.
? kG.y/  f k2 < gn ? [ ? ` D 1 ? [ t WD min.1; 1:2 	 t/ ], ! 11: ]

7.4
Gauss–Newton Method
157
9.
t D max.0:7 	 t; 103/ `
10. [stop: unsuccessful halt at x.k/, remainder vector g, its norm kgk2 ]
11. k WD k C 1; x.k/ WD y, ? k < maxit ?, ! 2: ]
12. [stop: exiting due to maxit iterations;
parameter vector x.k/, remainder vector g, its norm kgk2 ]
As ﬁrst test example consider the following: at the points
ti WD 2 C 5  .i  1/=.m  1/; i D 1; 2; : : : ; 12 DW m
and with the parameter vector .a; b; c; d; e/T D .0:2; 0:1; 2; 2; 1/T we calculate the values
fi WD Gi.x/ D F.x; ti/ where F.x; t/ WD a C b  t C c  exp.d  .t  e/2/:
After that we “forget” the original parameter vector and we try to determine it from the vector
.f1; : : : ; fm/T (reconstructing with this the exact formula of the function F ), starting from an
approximate parameter vector.
Thus, n D 5; m D 12; x D .x1; x2; x3; x4; x5/T D .a; b; c; d; e/T , that is
F.x; t/Dx1 C x2  t C x3  exp.x4  .t  x5/2/; Gi.x/DF.x; ti/; i D1; 2; : : : ; 12:
From here we get the Jacobian matrix J of G.x/. Its ith row is
J.x/i WD.1; ti; exp.x4.ti  x5/2/; x3.ti  x5/2  exp.x4.ti  x5/2/;
2  x3  x4  .ti  x5/  exp.x4  .ti  x5/2//:
Starting the previous algorithm with the initial approximation x.0/ WD .0:1; 0:2; 1; 1, 1:5/T , we
obtain (rounding to six digits after the decimal point)
g.x.0// D.0:299995; 0:254456; 0:208195; 0:162662; 0:181508;
 0:545378; 1:200423; 0:950147; 0:155423; 0:629291; 0:472918; 0:304728/T ;
whose norm is gn D 1:906672. Instead of the 60 entries of the matrix J we give, to test the code,
only the eighth row:
J.x.0//8 D .1; 1:181818; 0:903716; 0:091492; 0:575092/;
while the matrix .J T J /.x.0// is
.J T J /.x.0//D
0
BBBB@
12
6
3:874168 1:845999 0:101916
6
32:545455
5:760294 2:556057 3:539124
3:874168
5:760294
2:756801 0:687433 0:001923
1:845999 2:556057 0:687433 0:509667
0:007435
0:101916 3:539124
0:001923 0:007435
2:749734
1
CCCCA
:
Then we obtain the vector w
w D .2:240403; 2:900298; 0:865987; 0:300277; 2:903115/T ;

158
7
Nonlinear Equations and Systems
0,5
1,5
1
2
1,25
2,5
0
−1,25
0
Fig. 7.4 Upper line: the unknown curve of test Example 1 (and the result of the algorithm), lower
line: ﬁrst approximation
and the solution of the system .J T J /.x.0//v D w:
v D .0:098065; 0:061251; 0:259491; 0:176973; 0:972651/T :
Since t D 1, to this belongs the ﬁrst approximation
x.1/ D .0:198065; 0:138749; 1:259491; 0:823027; 0:527349/T ;
and the second iteration can start. We summarize the results of the present and the following
iterations in the table below (see also Fig. 7.4).
iter
gn
x1 D a
x2 D b
x3 D c
x4 D d
x5 D e
2
1.482564
0.186724
0.044118
1.403366
0.828971
1.316018
3
1.155566
0.342548
0.209447
1.396324
1.665572
0.734403
4
1.103511
0.197415
0.095760
1.748663
1.438514
1.108142
5
0.506068
0.210206
0.109477
1.924154
1.905157
0.972508
6
0.134831
0.200181
0.099820
1.996581
1.994197
1.001470
7
0.006891
0.200001
0.100001
1.999989
1.999984
0.999996
8
0.000019
0.2
0.1
2.0
2.0
1.0
In fact, gn given in the table always belongs to the previous vector x, see the algorithm. To the
exact result obtained in the last row corresponds gn D 0.
It is important to notice the considerable acceleration of the convergence at the end. This
property is a feature of good models; if the model is unsuitable, then the Gauss–Newton method
possibly does not converge.
Second test example is the temperature example where the ﬁnal result is
x1 D a D 10:12507951; x2 D b D 11:25764305; x3 D t0 D 14:27828254;
(7.34)
compare with page 80.

7.5
Exercises
159
7.5
Exercises
1. The bisection method can be accelerated by including a step where (if, e.g.,
f .xm/ < 0; f .ym/ > 0) the algorithm will continue not with 1
2.xm C ym/ but
with the root zm of the straight line connecting .xm; f .xm// and .ym; f .ym//.
Find a formula for zm.
2. Find a starting point x.0/ that leads Newton’s method applied to f D x3  2x
into an inﬁnite loop.
3. Apply Newton’s method to the function f .x/ WD x2  a, where a > 0, and
describe in this particular case the connection between the errors x.k/  x and
x.kC1/  x.
4. Calculate the Jacobian matrix of the following vector function:
n D 3;
f WD .f1; f2; f3/T ;
f1.x/ WD sin.x1 	 x2/ C x4
3 ;
f2.x/ WD cos.x1/ 	 sin.x3/; f3.x/ WD x2
1 
p
1 C x2 	 x3 :
5. Find another solution (completely different from the solution characterized by
the value t0 from (7.34)) for problem 6 of Sect. 4.4.
6. Read the MATLAB help about fsolve and optimset and solve the above
problem 4 and the system (7.25) using MATLAB. Try to reach full machine
accuracy of the solutions.
7. In Sect. 8.5.2 we shall consider several formulae (being exact for ﬁrst and second
degree polynomials in x and y) for the approximate calculation of integrals over
triangles. To have a formulae which is exact for third degree polynomials, we
propose to take the triangle S WD f0  x  1  y  1g and to consider the
approximation
4
X
iD1
aif .xi; yi/ 
Z
S
f .x; y/ dx dy
(where R
S f .x; y/ dx dy represents the volume between S and the surface f , and
fai; xi; yig4
iD1 are our unknowns). Further, we require that this approximation be
exact for the polynomials
1; x; y; x2; xy; y2; x3; x2y; xy2; y3:
To simplify the problem, we require symmetry in the sense that a1 D a3; a2 D
a4; x1 D y3; x3 D y1; x2 D y4; x4 D y2. This means that we have six unknowns
(a1; a2; x1; y1; x2; y2) which we shall denote by z1; : : : ; z6. As six equations we
then take
2.z1 C z2/ D 1
2;
which corresponds to f D 1;

160
7
Nonlinear Equations and Systems
z1.z3 C z4/ C z2.z5 C z6/ D 1
6;
which corresponds to f D x;
z2.z2
3 C z2
4/ C z2.z2
5 C z6/2/ D
1
12;
corresponding to f D x2;
2 	 .z1z3z4 C z2z5z6/ D
1
24;
corresponding to f D xy;
z1.z3
3 C z3
4/ C z2.z3
5 C z3
6/ D
1
20;
for f D x3;
z1z3z4.z3 C z4/ C z2z5z6.z5 C z6/ D
1
60;
for f D x2y:
Therefore we get the following m-ﬁle to be called from fsolve:
function f=fsolveint3(z)
% system to be solved calling fsolve
% The meaning of the unknowns:
% z(1)=a1,z(2)=a2,z(3)=x1,z(4)=y1,z(5)=x2,z(6)=y2.
f(1)=2*z(1)+2*z(2)-1/2;
f(2)=z(1)*(z(3)+z(4))+z(2)*(z(5)+z(6))-1/6;
f(3)=z(1)*(z(3)^2+z(4)^2)+z(2)*(z(5)^2+z(6)^2)-1/12;
f(4)=2*z(1)*z(3)*z(4)+2*z(2)*z(5)*z(6)-1/24;
f(5)=z(1)*(z(3)^3+z(4)^3)+z(2)*(z(5)^3+z(6)^3)-1/20;
f(6)=z(1)*z(3)*z(4)*(z(3)+z(4))+z(2)*z(5)*z(6)*(z(5)+z(6))-1/60;
Then make a ﬁrst call of fsolve, e.g.:
z0=[0.25,0.25,0.25,0.15,0.35,0.15];
[z,fz]=fsolve(’fsolveint3’,z0),fn=norm(fz,inf)
In this way you get a result using the default options of fsolve. Therefore next
read about optimset and take smaller values for TolFun,TolX and a larger
value for MaxIter, and start fsolve from the z-values obtained from the ﬁrst
call with the aim of getting fn=norm(fz,inf) of the order 1016.

8
Numerical Integration
Remember the deﬁnite integrals of some fundamental functions:
Z b
a
xk dx D
 xkC1
k C 1
	b
a
D bkC1  akC1
k C 1
D b  a
k C 1 .bk C abk1 C    C ak1b C ak/:
Z ˇ
˛
sin x dx D Œ cos xˇ
˛ D cos ˛  cos ˇ;
Z ˇ
˛
cos x dx D Œsin xˇ
˛ D sin ˇ  sin ˛;
Z x
1
1
t dt D ln x;
Z x
0
et dt D ex  1:
Moreover, some important properties of the integral:
– the integral is additive:
if f D f1 C f2 are (Riemann integrable) functions deﬁned over the interval Œa; b, then
Z b
a
f .x/ dx D
Z b
a
f1.x/ dx C
Z b
a
f2.x/ dxI
– the integral is homogeneous:
if f D ˛f1, where ˛ is a constant, then
Z b
a
f .x/ dx D ˛
Z b
a
f1.x/ dx:
© Springer International Publishing AG 2016
G. Stoyan, A. Baran, Elementary Numerical Mathematics for Programmers and
Engineers, Compact Textbooks in Mathematics, DOI 10.1007/978-3-319-44660-8_8
161

162
8
Numerical Integration
The integral is the area under a curve, to be more precise
I.f / WD
Z b
a
f .x/ dx
is the area under the real function y D f .x/ (and between the straight lines y D
0; x D a and x D b > a, resulting in a positive value for f .x/ > 0 and a negative
value for f .x/ < 0). If f is a function of two variables, then the double integral
I.f / WD
Z b
a
Z d
c
f .x; y/ dx dy
is the volume under the surface z D f .x; y/ and over the rectangle a  x  b; c 
y  d of the x; y-plane.
It is important to have in mind that not all integrals can be expressed in terms
of elementary or known transcendental functions. On the contrary, we can deﬁne
functions using integrals, e.g. when the upper bound is a variable. An example—
where the integral cannot be expressed in terms of elementary functions—is the
“error function” which plays an important role in statistics:
ˆ.x/ WD
2
p
Z x
0
ez2 dz:
These are cases when the numerical approximation of the integral is advantageous
(but it is also useful if the primitive function (or “antiderivative”) is known but too
complex).
A practically important problem connected to the calculation of integrals is the
building of motorways. Along the path of the motorway many height values of
the surface of the earth (e.g. at points .xi; yi/ of a grid on a plane basis surface)
are available. In order to avoid steep slopes and transportation of soil over long
distances, one has to calculate how much soil is needed in a valley which the
motorway will cross on a dam, and how much soil can be obtained from the hill
before and behind the valley when we dig the motorway partially into the hill. In
this case the function f is unknown, we have only values f .xi; yi/, so the integral
can only be approximated numerically.
8.1
Elementary Quadrature Formulae
In the interpretation due to Riemann the integral is the limit limn!1 Sn.f / of the
sum
Sn.f I a; b/ WD
n
X
iD1

xif .xi/; 
xi WD xixi1; xi 2 Œxi1; xi; i D1; 2; : : : ; n;
(8.1)

8.1
Elementary Quadrature Formulae
163
where the points xi form a partition of the interval Œa; b depending on n: a D x0 <
x1 <    < xn D b.
Similarly, we will look for the approximation of I.f / in the form of the sum
In.f; a1; x1; : : : ; an; xn/ WD
n
X
iD1
aif .xi/;
xi 2 Œa; b;
(8.2)
which is natural both when f is a given real function and when it is known only at
points xi.
We call the numbers ai weights (comparing to (8.1) we can think about small
parts of the interval Œa; b) and the points xi nodes—which are always assumed to
be in Œa; b. In.f; a1; x1; : : : ; an; xn/ is the quadrature formula which has 2n C 1
free parameters: n, the nodes and the weights. For simplicity, we usually shorten
this denotation to In.f / or In.
Some simple quadrature formulae come directly from the geometrical interpre-
tation (Figs. 8.1 and 8.2):
1. n D 1: The midpoint rule or rectangle rule:
Z b
a
f .x/ dx  I1.f / WD .b  a/f
a C b
2

:
2. n D 2: The trapezoidal rule:
Z b
a
f .x/ dx  I2.f / WD b  a
2
Œf .a/ C f .b/:
(8.3)
3. n D 3: Simpson’s rule:
Z b
a
f .x/ dx  I3.f / WD b  a
6

f .a/ C 4f
a C b
2

C f .b/
	
:
(8.4)
x
y
a
(a+b)/2
b
f(x)
x
a
b
y
f(x)
Fig. 8.1 The midpoint rule—and the trapezoidal rule

164
8
Numerical Integration
x
y
a
(a+b)/2
b
f(x)
Fig. 8.2 Simpson’s rule
Simpson’s rule is a very frequently used formula.
In the case n D 1 we mention the left- and right-hand rectangle rules, too:
Z b
a
f .x/ dx  I1.f / WD .ba/f .a/; and
Z b
a
f .x/ dx  I1.f / WD .ba/f .b/:
From the point of view of testing these quadrature formulae, it is worth asking
for which simple functions do we obtain the exact value of the integral instead of its
approximation?
First of all we think about polynomials, as their integrals are well known. One
can check one after another:
The left- and right-hand rectangle rules are, in general, exact only for constant
functions.
The midpoint and trapezoidal rules are exact for linear functions.
Finally, Simpson’s rule is exact in the case of third order polynomials.
Most of these statements can be seen from the ﬁgures illustrating the formulae. We
will check two cases:
(a) Midpoint rule: Let f .x/ D c0 C c1x C c2x2, then
I.f / D Œc0x Cc1
x2
2 Cc2
x3
3 b
a D .b a/

c0 C c1
b C a
2
C c2
b2 C ab C a2
3
	
;
but according to the midpoint rule
I1.f / D .b  a/
 
c0 C c1
a C b
2
C c2
a C b
2
2!
;

8.2
Interpolational Quadrature Formulae
165
hence, the difference is
I.f /  I1.f / D .b  a/ c2
12.b2  2ab C a2/ D c2.b  a/3
12
which equals zero only in the case c2 D 0.
(b) Simpson’s rule: Calculating with the previous f we obtain that
I2.f / D b  a
6

c0 C c1a C c2a2 C 4
 
c0 C c1
a C b
2
C c2
a C b
2
2!
Cc0 C c1b C c2b2
D b  a
6

6c0 C 3.a C b/c1 C c2.a2 C .a C b/2 C c2b2/

D I.f /:
Hence, we see that Simpson’s rule is exact for second order polynomials. Let
us observe that the third order polynomial p3.x/ WD .x  a/.x  aCb
2 /.x  b/
disappears at the nodes of the formula I2.f /, that is the quadrature formula “fails
to notice” it: I2.p3/ D 0; I2.f C ˛p3/ D I2.f / C I2.˛p3/ D I2.f /, where ˛
is an arbitrary real number. Otherwise, due to symmetry I.p3/ D 0 holds, that is,
I.f C ˛p3/ D I.f / D I2.f /. Now, it is sufﬁcient to observe that an arbitrary
third order polynomial can be written in the form f C˛p3, and we have veriﬁed our
earlier statement.
In the above reasoning we have used two common properties of the integrals
I and of the quadrature formulae In, namely both the integral and the quadrature
formula are additive and homogeneous:
if f D f1 C f2 are functions on Œa; b then In.f / D In.f1/ C In.f2/,
and if f D ˛f1, where ˛ is a constant, then In.f / D ˛In.f1/.
8.2
Interpolational Quadrature Formulae
For the approximate calculation of the integral
R b
a f .x/ dx the following method is
used:
we substitute for f the interpolational polynomial corresponding to the nodes
xi 2 Œa; b (i D 1; : : : ; n), that is, the Lagrange interpolational polynomial Ln1,
and then we calculate the integral of Ln1 exactly. In this way we obtain a quadrature
formula of the form (8.2).

166
8
Numerical Integration
In fact, disposing of the data fxi; f .xi/gn
iD1 the Lagrange polynomial is well
deﬁned:
Ln1.x/ D
n
X
iD1
f .xi/q.n/
i .x/ D
n
X
iD1
f .xi/
n
Y
j D1
j ¤i
x  xj
xi  xj
;
see relation (6.6) in Sect. 6.2. If n D 1, then q.1/
1 .x/  1 and Ln1.x/ D f .x1/.
Integrating this over the interval Œa; b (using again the additivity and homogenity
of the integral) we have
I.f /  I.Ln1.f // D
n
X
iD1
f .xi/ai DW In.f /;
where ai WD I.qi/ D
Z b
a
q.n/
i .x/ dx; and q.n/
i .x/ D
n
Y
j D1
j ¤i
x  xj
xi  xj
:
The simplest case is when only one node is given: x1, and we substitute the constant
f .x1/ for the function f . Then, if x1 D a; x1 D b or x1 D aCb
2
we obtain the left-
or right-hand rectangle rule or the midpoint rule, respectively.
Now, we can take two nodes, and the choice x1 D a; x2 D b seems to be self-
evident. From this arises the trapezoidal rule.
Continuing with n  3 and choosing equidistant nodes: .xi WD a C i ba
n1; i D
0; : : : ; n  1/, we obtain the so-called Newton–Cotes formulae.
We already know these formulae for n  3, and n D 3 corresponds to Simpson’s
rule (8.4).
Together with the interpolational polynomials we also studied their error formu-
lae (see (6.22) in Sect. 6.2):
f .x/ D Ln1.f; x/ C f .n/.n.x//
nŠ
!n.x/:
Now, we integrate the whole formula:
Z b
a
f .x/ dx D I.f / D I.Ln1.f // C 1
nŠI.f .n/.n/!n/:
(8.5)
Since I.Ln1.f // D In.f /, the error term Rn.f / of the Newton–Cotes formula is
Rn.f I a; b/ D Rn.f / WD I.f /  In.f / D 1
nŠ
Z b
a
f .n/.n.x//!n.x/ dx:
(8.6)

8.2
Interpolational Quadrature Formulae
167
From here we can see that if f is a polynomial of degree at most n  1 then
f .n/.x/  0, and so Rn.f / D 0, that is, the Newton–Cotes formula using n nodes
is exact. This just means that our plan is complete: if f satisﬁes the above condition
then it coincides with its own Lagrange polynomial and the error Rn.f / is zero.
However, from (8.6) we can also understand why, when using the midpoint rule
I1.f /, besides the integral of a constant f also that of a linear f can be obtained
without error—in spite of the fact that in this case n D 1 and the derivative of f is
constant but not necessarily zero. Namely, then !1.x/ D x  bCa
2
and
R1.f / D
Z b
a
c1

x  b C a
2

dx D c1
x2
2  x b C a
2
	b
a
D c1.b  a/.b C a
2
 b C a
2
/ D 0:
In fact, along with (8.5), for n D 1, we have here also—due to I1.f / D I.L0.f /C
˛!1.x//—that
I.f / D I.L0.f /C˛!1.x//C 1
2ŠI.f .2/.2/!2/ D I1.f /C 1
2Š
Z b
a
f .2/.2/!2.x/ dx
for ˛ chosen so that H1 D L0.f / C ˛!1 is the Hermite interpolation polynomial
speciﬁed by f .x1/ and f 0.x1/, see Sect. 6.3 and integrate (6.27) for m D 2. Here, in
general,
R b
a f .2/.2/!2.x/ dx ¤ 0, e.g. for f .2/ D const ¤ 0, and
R b
a j!2.x/j dx 
.b  a/3 follows from (6.28).
As in this case, we often encounter the situation that for an n-point formula,
along with Ln1.f /, there is also a d  1-degree (Hermite) polynomial Pd1.f /
with d > n such that In.f / D I.Ln1.f // D I.Pd1.f //. Then, correspondingly,
we get
Rn.f I a; b/ WD I.f /  In.f / D 1
dŠ
Z b
a
f .d/.d.x//!d.x/ dx; d  n;
(8.7)
for some !d.x/ satisfying
R b
a j!d.x/j dx  .b  a/dC1. The case d D n is always
true and expressed by (8.6), with !d D !d and the estimate
R b
a j!d.x/j dx  .b 
a/dC1 obtained from (6.24).
Therefore, having d . n/ and a d-times differentiable function f ,
jRn.f I a; b/j  1
dŠ max
axb jf .d/.x/j.b  a/dC1;
(8.8)
where we have also used that, according to (6.27), a < n.x/ < b.

168
8
Numerical Integration
Applying this estimation to the midpoint rule (n D 1; d D 2) and the trapezoidal
rule (n D d D 2), we get for their errors R1 and R2:
jR1.f /j; jR2.f /j  1
2 max
axb jf 00.x/j.b  a/3:
8.3
Composite Quadrature Rules
8.3.1
Construction of Composite Formulae
In practice, the higher-order Newton–Cotes formulae exhibit certain problems and
the low-order formulae are applied not to the whole interval Œa; b, but to the
subintervals of this interval. Remember here that in Sect. 6.4 it turned out to be
advantageous to work with piecewise polynomial interpolation.
In order to describe the composite quadrature rules we assume that the starting
quadrature formula (not necessarily of Newton–Cotes type) is given in the following
standard form:
In.F I 0; 1/ WD
n
X
iD1
˛iF.ti/ 
Z 1
0
F.t/ dt;
(8.9)
indicating that it is applied to the interval Œ0; 1, and ti 2 Œ0; 1. Then, in the ﬁrst
place it is worth dealing with the problem of how to transform the integral and the
quadrature formula when we change the interval.
The interval Œ0; 1 can be mapped onto the interval Œa; b using the transformation
x D a C t.b  a/: if t D 0, then x D a, if t D 1, then x D b. When a < b, the
inverse transformation is t D
xa
ba . First, we are going to use these relations to
clarify how the Riemann sum (8.1) can be applied to a different interval, e.g. Œ0; 1.
Using the inverse transformation, from the partition fxig of (8.1) we obtain the
partition of the interval Œ0; 1:
ti WD xi  a
b  a ; i D 1; : : : ; n:
It is obvious that in this case the lengths of these subintervals of Œ0; 1 are

ti WD ti  ti1 D xi  a  .xi1  a/
b  a
D 
xi
b  a:
We obtain the new function using the original transformation: F.t/ WD f .x.t// D
f .a C t.b  a//, and ﬁnally, the inverse transformation deﬁnes the nodes at which
we calculate the values of the function F :
F.ti/ D f .xi/;
if t i WD xi  a
b  a ;
i D 1; : : : ; n;

8.3
Composite Quadrature Rules
169
where fxig are the nodes of (8.1). Therefore, we see that the Riemann sum
corresponding to the interval Œ0; 1 takes the form
Sn.F I 0; 1/ WD
n
X
iD1

tiF.ti/;

ti WD ti  ti1; ti 2 Œti1; ti:
In the opposite case when the starting formula is Sn.F I 0; 1/ we have
x WD a C t.b  a/;
f .x/ D F.t.x// D F.x  a
b  a /;
xi WD a C ti.b  a/; xi WD a C t i.b  a/; 
xi WD .b  a/
ti; i D 1; : : : ; n;
which give the transformation to the sum Sn.f I a; b/.
We can obtain the transformation of the standard quadrature formula (8.9) to the
general interval Œa; b in exactly the same way:
In.f I a; b/ WD
n
X
iD1
aif .xi/ 
Z b
a
f .x/ dx; f .x/ WD F.t.x// D F
x  a
b  a

;
xi WD a C ti.b  a/; ai WD .b  a/˛i; i D 1; : : : ; n;
as here ˛i and 
xi correspond to ai and 
ti, respectively.
Now, from the transformations of the Riemann sums, taking the limit, follow the
transformations of the integrals:
Z 1
0
F.t/ dt
xDaCt.ba/
D
Z b
a
f .x/
dx
b  a;
f .x/ D F.t.x// D F
x  a
b  a

D
1
b  a
Z b
a
f .x/ dx:
The previous relation cannot be correct without the multiplier
1
ba as from F.t/  1
it follows that f .x/  1, and in this case
R 1
0 F.t/ dt D 1;
R b
a f .x/ dx D b  a.
We are going to apply the above reasoning to the approximate calculation of the
integral I.f I a; b/ WD
R b
a f .x/ dx using composite quadrature formulae.
For this aim we divide the interval Œa; b uniformly into m subintervals :
a D x0 < x1 <    < xm D b; xj  xj 1 DW h; h WD b  a
m
;

170
8
Numerical Integration
and accordingly, we subdivide the integral, too:
I.f I a; b/ D
m
X
j D1
Z xj
xj 1
f .x/ dx
t D .x  xj 1/=h
D
m
X
j D1
h
Z 1
0
Fj .t/ dt
D
m
X
j D1
hI.FjI 0; 1/;
where Fj .t/ WD f .xj 1Cth/ D f .x/; and the multiplier h compensates for the fact
that if f  1 the value of the integral
R xj
xj 1 f .x/ dx before the transformation t D
t.x/ equals h. However, without h after the transformation the value of
R 1
0 Fj.t/ dt
would be equal to 1.
Now, it still remains to apply the standard quadrature to the approximation of the
integrals I.FjI 0; 1/:
I.FjI 0; 1/ D
Z 1
0
f .xj 1 C th/ dt 
n
X
iD1
˛iFj .ti/ D
n
X
iD1
˛if .xij /;
where xij WD xj 1 C tih.
Hence, we have obtained the composite rule corresponding to (8.9) on the
interval Œa; b:
Imn.f / WD
m
X
j D1
hIn.Fj I 0; 1/ D
m
X
j D1
h
n
X
iD1
˛if .xij / 
Z b
a
f .x/ dx:
Example
the composite trapezoidal rule:
Im2.f / D
m
X
jD1
hI2.Fj I 0; 1/ D
m
X
jD1
h
f .xj1/ C f .xj /
2

D h
1
2f .a/ C
m1
X
jD1
f .a C jh/ C 1
2f .b/
	
; h WD b  a
m
:
(8.10)
This formula can also be derived by producing the linear interpolation of f over each interval
Œxj1; xj, thus, with respect to the whole interval Œa; b we use the ﬁrst order spline to approximate
f , and then we integrate it.

8.3
Composite Quadrature Rules
171
Example
the composite Simpson rule, using the notation xj1=2 WD
xj1Cxj
2
:
Im3.f /D
m
X
jD1
hI3.Fj I 0; 1/ D
m
X
jD1
h
6

f .xj1/ C 4f .xj1=2/ C f .xj /

D h
6

f .a/ C 2
m1
X
jD1
f .aCjh/ C 4
m
X
jD1
f .aC.j  1
2/h/ C f .b/
	
:
(8.11)
8.3.2
Convergence of Composite Formulae
We are now going to examine the convergenceof the composite quadrature formulae
Imn.f / for m ! 1.
To get our result, in the formula of Imn.f / we interchange the order of
summations:
Imn.f / D
m
X
j D1
h
n
X
iD1
˛if .xij / D
n
X
iD1
˛i
m
X
j D1
hf .xij / DW
n
X
iD1
˛iS.i/
m ;
where
S.i/
m D
m
X
j D1
hf .xij /; h D b  a
m
; xij D xj 1 C tih; i D 1; : : : ; n;
are special Riemann sums of the kind Sm.f I a; b/, thus in the case of a Riemann
integrable function f , for all i we have
lim
m!1 S.i/
m D
Z b
a
f .x/ dx
and therefore ﬁnd
lim
m!1 Imn.f / D
n
X
iD1
˛i
Z b
a
f .x/ dx:
What can we say about this multiplier Pn
iD1 ˛i? It is obtained from the standard
quadrature formula (8.9) for F  1. If in this special case the standard formula
gives an exact result (a natural expectation), i.e.
R 1
0 dt D 1 D Pn
iD1 ˛i, then the

172
8
Numerical Integration
value of the multiplier will be equal to 1 and
lim
m!1 Imn.f / D
Z b
a
f .x/ dx:
Hence, we can see that even a minimal set of conditions is sufﬁcient to ensure the
convergence of a composite quadrature formula:
if m ! 1,
the standard formula In.F; 0; 1/ gives an exact result for constant functions,
and if f is a Riemann integrable function,
then the composite quadrature formula Imn.f / converges to I.f I a; b/, the exact
value of the integral.
It is worth supplementing the previous list of sufﬁcient convergence conditions
(where the convergence can be arbitrarily slow) with the fact that for a sufﬁciently
many times differentiable function the convergence can be characterized by powers
of h.
For this purpose it is advantageous to assume that along with Ln1.F / there is a
polynomial Pd1.F //; d  n, such that In.F / D I.Ln1.F // D I.Pd1.F // and
to take (8.7) and its estimation (8.8) (adding to the above-mentioned cases that of
n D 3; d D 4 for Simpson’s rule).
We begin by considering the difference between I.f I a; b/ and Imn.f /:
I.f I a; b/  Imn.f / D
m
X
j D1
 Z xj
xj 1
f .x/ dx  h
n
X
iD1
˛if .xj 1 C tih/
!
D
m
X
j D1
Rn.f; xj 1; xj /:
We estimate the obtained sum using (8.8), assuming that f is d times continuously
differentiable:
jI.f I a; b/  Imn.f /j 
m
X
j D1
jRn.f; xj 1; xj /j 
m
X
j D1
1
dŠ
max
xj 1xxj jf .d/.x/jhdC1
 1
dŠ max
axb jf .d/.x/j
m
X
j D1
hdC1 D 1
dŠMd.b  a/hd;
as mh D b  a. Here we have used again the notation Md WD maxaxb jf .d/.x/j.
If the exact calculation of maxaxb jf .d/.x/j is complicated, then instead of Md
we can use an upper bound of the maximum.

8.3
Composite Quadrature Rules
173
Applying this result to the trapezoidal rule and to Simpson’s rule you see that for
a twice resp. four times continuously differentiable function f
jI.f I a; b/  Im2.f /j  h2
2 .b  a/M2;
(8.12)
jI.f I a; b/  Im3.f /j  h4
24.b  a/M4:
That is, the composite trapezoidal rule and Simpson’s rule are of second and fourth
order in h, respectively.
These results are useful in the following case: f is a function whose second (or
fourth) derivative can be calculated and the maximum of its absolute value can be
estimated over Œa; b, but the integral of f is complicated or unknown. Then using
the previous estimations we can calculate such a value of h which guarantees a given
accuracy ".
Example
Let us calculate the integral
Z 1
0
1
1 C x4 dx
with accuracy " D 104 using the composite trapezoidal rule. Here
f .x/ D
1
1 C x4 ; f 0.x/ D
4x3
.1 C x4/2 ; a D 0; b D 1;
f 00.x/ D 12x2.1 C x4/  2  4x3  .4x3/
.1 C x4/3
D 12x2 C 20x6
.1 C x4/3
:
Since 0  x  1, we have 1  1 C x4  2, and g.x/ WD 12x2 C 20x6 D 4x2.5x4  3/
takes the values 0 and 8 at the boundaries of the interval, but inside the interval it has a minimum
point at which g0.x/ D 24x C 120x5 D 0, that is, at which x4 D
1
5. At this point g.x/ D
4
p
5.1  3/ D  8
p
5 which means that the absolute value of the minimum does not reach the value
of the maximum. In summary, jf 00.x/j  8 DW M2.
Now, we can apply the estimation (8.12) from which we see that the error of the composite
trapezoidal rule is not larger than h2
2 .b  a/M2 D 4h2. Hence, if 4h2  " D 104, then due
to jI.f I a; b/  Im2.f /j  4h2  " the required accuracy is guaranteed. From this we obtain
h 
1
200, i.e. m D 1
h D 200 is sufﬁcient.
In this example real calculations give I2002 D 0:866970904, while I.f / D 0:8669729873 is
the exact result to ten digits. Roughly, the error reached is  2  106 and thus by two orders of
magnitude better than the prescribed 104.
Our example illustrates the role mathematics can play: it helps us to be
responsible for our computational results.

174
8
Numerical Integration
Let us summarize the most important points:
The elementary quadrature formulae In (rectangle, midpoint, trapezoidal, Simp-
son’s rule) can be characterized by the following data:
– how many nodes they use (this is the number n);
– which maximal polynomial order they give the exact integral for (this is the
number d  1).
Mostly, we apply these formulae in the composite form Imn, dividing the interval
Œa; b into m subintervals.
The composite formulae Imn are convergent: Imn ! I.f /, if the following
conditions are satisﬁed:
– the number m of the subintervals tends to inﬁnity;
– the standard quadrature formula In calculates the integral of constant functions
exactly;
– the integrand f is Riemann integrable.
If the integrand is d times continuously differentiable, then the following error
estimation is valid
jI.f /  Imn.f /j  1
dŠ max
axb jf .d/.x/j.b  a/hd; where h D b  a
m
;
that is, the convergence is of order d in h.
8.4
Practical Points of View
The number of nodes of the composite quadrature formulae Imn is `  nm, where
n is ﬁxed. In what follows we write I` instead of Imn. Then the typical form of
error estimations of the composite formulae is
jI`.f /  I.f /j  cdhd  cd..b  a/n/d`d; cd WD .b  a/Md
dŠ ;
(8.13)
because h D ba
m  .ba/n
`
.
The ﬁrst of our practical remarks is that in addition to Imn D I` let us calculate
the value I2mn D I2` corresponding to the doubled number of nodes, and, if
required, the values I4`; I8`, etc.
For the difference of I` and I2` we have that
jI`.f /  I2`.f /j  jI`.f /  I.f /j C jI2`.f /  I.f /j 
 .1 C 2d/cdhd;
as h D ba
m and ba
2m D h
2, moreover, hd C .h=2/d D hd.1 C 2d/.

8.4
Practical Points of View
175
The estimation of Md can be obtained from the values f .xi/ calculated during
the integration using divided differences (see (6.25)).
The inequalities above show that for a d times continuously differentiable
function f the upper bounds of the differences jI`.f /I2`.f /j and jI`.f /I.f /j
differ only in the multiplier 1C2d. Hence, the value jI`.f /I2`.f /j=.1C2d/ or
more simply the value jI`.f /  I2`.f /j can be used as a so-called error indicator.
This should not be confused with an error estimation because we have started only
from an upper bound of the errors.
A second remark can help you in practice: if jI`  I2`j  "Œ1 C jI`.f /j is
satisﬁed, then you should calculate the value I4`, too, and should accept the new
result only when jI2`  I4`j  "Œ1 C jI2`.f /j. Otherwise you should continue the
calculation with a further value of `. (As in the other cases when we have to measure
the accuracy we recommend here the previous absolute-relative mixed test instead
of jI`.f /  I2`.f /j  ".)
In the case of numerical integration one of the problems is that with the increase
in the number of nodes the rounding errors also increase. As the quadrature formula
is, mathematically, an inner product, we can refer to Chap. 1. According to this, if
the errors of the weights ai and of the function values fi are in order of magnitude
of the rounding errors, the exact value of I` differs from the value eI` obtained using
a computer in the following way:
jeI`  I`j  2"1.`  1/
`
X
j D1
aijfij < 2`"1  I`.jf j/;
where "1 is the machine epsilon, compare to (1.13). Here we have assumed that the
weights ai are positive and we have calculated them with high accuracy, while the
values of f have been produced with relative error "1, and in the estimation of the
accumulation of errors we have neglected the quantities which are of second order
in "1.
Now, the following situation occurs: the difference between the exact value I.f /
and the calculated value A
I`.f / divides into two parts. The ﬁrst part is the rounding
error estimated above, which increases linearly in `, while the second part is the
error of the dth order formula decreasing as `d. For an illustration see pp. 213 and
215. In summary:
jeI`  Ij  jeI`  I`j C jI`  Ij  c1` C c2`d;
where c1 D 2"1I`.jf j/ and c2 D cd..b a/n/d. It follows from this estimation that
there exists a value of ` such that the total error is minimal (see Fig. 8.3 where x
stands instead of `):
g.`/ WD c1` C c2`d  c1`opt.1 C 1=d/ D gmin; `opt WD .c2d=c1/1=.1Cd/:

176
8
Numerical Integration
10
7,5
5
2,5
0
1
0,75
0,5
0,25
0
x
y
x_opt
Fig. 8.3 The total error y D g.x/ (solid line), a rounding error (starting from the origin), error of
the formula (decreasing curve), the minimal error (the line parallel to the x-axis)
The dependence of `opt on d is quite sensitive. c2=c1 is often a large number and
then one can expect that `opt decreases as d increases.
When the number of nodes is `opt, the level of rounding errors reaches the level
of the error of the formula, from this point on, for ` > `opt, it is not worth continuing
the calculation of the values I`. Observe that the lower error bound depends on the
problem (through f and d), on the quadrature formula and on the computer (the
latter through "1 in c1).
Finally, we are going to deal with the possibility of acceleration of the con-
vergence, namely with Aitken correction. In order to obtain it we assume that
concerning the quadrature formula I`.f / we have no more information than that
for a sufﬁciently smooth f
I`.f / D I.f / C chd;
h WD .b  a/=`
(8.14)
holds, where c D c.f; h/ varies only slowly with h and where c; I.f / and d are
unknown. Then considering c to be constant we can determine these three unknowns
from the three values I`, I2`, I4`:
I`  I2`
I2`  I4`
D
1  2d
2d  4d D 2d; chd D I`  I2`
1  2d D
.I`  I2`/2
I`  2I2` C I4`
;

8.5
Calculation of Multiple Integrals
177
that is,
I.f / D I` 
.I`  I2`/2
I`  2I2` C I4`
:
(8.15)
When c is not a constant then the expression on the right-hand side does not give
the exact value I.f /, however, it often gives a better approximation of this than any
of the three values I`, I2` I4`.
This is an extrapolational method in the sense that we have three values I`, I2`,
I4`, which correspond to the values h; h
2; h
4, respectively: I` D I`.h/, I2` D I`. h
2/,
I4` D I`. h
4/, and we want to know the value I.f / D I`.0/.
8.5
Calculation of Multiple Integrals
In this section, as an example for multi-dimensional cases, we are going to deal with
the two-dimensional case, when the purpose is to determine the volume over some
domain  of the x; y-plane and under the surface z D f .x; y/:
I.f; / WD
Z

Z
f .x; y/ dx dy:
As a generalization of (8.2) an approximation of this double integral can be taken in
the form
In.f; / WD
n
X
iD1
aif .xi; yi/; .xi; yi/ 2 :
A sum like this is called a cubature formula. The weights ai can be imagined as
subdomains of  which contain the points .xi; yi/.
Compared to the one-dimensional case, an important difference is that now the
domain often has to be approximated, too.
8.5.1
Reduction to Integration of Functions of One Variable
The simplest possibility for the application of two-dimensional numerical integra-
tion arises when we have such a (closed, bounded) domain   IR2 that the integral
of (a continuous function) f can be written in the form (Fig. 8.4)
Z

Z
f .x; y/ dx dy D
Z b
a
Z d.x/
c.x/
f .x; y/ dy dx D
Z b
a
g.x/ dx;
(8.16)

178
8
Numerical Integration
x
y
d(x)
b
a
c(x)
Fig. 8.4 Example for a domain of a double integral
where c and d are continuous functions of x 2 Œa; b. Then the inner integral
g.x/ WD
Z d.x/
c.x/
f .x; y/ dy
depends on x at three places: at the lower and upper boundaries, moreover in the
integrand.
To get an approximation of the integral, we start from a family of standard
quadrature formulae:
Im.F I 0; 1/ WD
m
X
iD1
˛.m/
i
F.t.m/
i
/ 
Z 1
0
F.t/ dt;
where the weights fa.m/
i
gm
iD1 and nodes ft.m/
i
gm
iD1 are available for all natural
numbers m.
We already know from the preceding point how to transform this to the interval
Œa; b and to g.x/ D F.t.x//:
Z b
a
g.x/ dx 
m
X
iD1
a.m/
i
g.x.m/
i
/ DW Im.gI a; b/;
where x.m/
i
WD a C .b  a/t.m/
i
and a.m/
i
WD .b  a/˛.m/
i
.
We have yet to obtain the values of g at x.m/
i
.
We do this approximately by choosing a quadrature formula from the same
family, applying it to the integral deﬁning the function g; however, the number of
nodes can differ from m, say it is n:
g.x.m/
i
/ 
n
X
j D1
bijf .x.m/
i
; y.n/
j /;

8.5
Calculation of Multiple Integrals
179
where
bij WD .di  ci/˛.n/
j ; ci WD c.x.m/
i
/; di WD d.x.m/
i
/; y.n/
j
WD ci C .di  ci/t.n/
j :
In order to simplify the notations we have not shown the dependence of the
coordinates y.n/
j
on m and i (through ci; di) in an explicit way.
Now we combine the quadrature formulae with respect to the directions x and y:
I.f; /  Im.gI a; b/ D
m
X
iD1
a.m/
i
g.x.m/
i
/

m
X
iD1
a.m/
i
n
X
j D1
bij f .x.m/
i
; y.n/
j / DW Im;n.f I /:
Example
If the domain is the rectangle Q WD fa  x  b; c  y  dg then for the formula Im;n we have
I.f; Q/ WD
Z b
a
Z d
c
f .x; y/ dy dx  Im;n.f /;
Im;n.f / WD jQj
m
X
iD1
n
X
jD1
f .x.m/
i
; y.n/
j /˛.m/
i
˛.n/
j ;
jQj WD .b  a/.d  c/:
We draw attention to the fact that now y.n/
j
WD c C .d  c/t .n/
j
depends neither on i nor on m (in
contrast to the case of a general ), as c and d are constants.
We call these formulae tensor product integration. It is important to know that if the formulae
for Im and In are exact for all polynomials of (maximal) degrees r in x and s in y, respectively,
then the tensor product quadrature rule is exact for all polynomials in two variables which have the
maximal degrees r in x and s in y.
8.5.2
Approximation of the Domain
If the domain is more general so that the integral cannot be written in the
form (8.16), then in order to obtain the cubature formula we have to divide the
domain, moreover, we often have to approximate it by simpler pieces.
The roughest possibility is to cover the domain with a rectangular grid which
has step sizes h and k in the directions x and y, respectively, and to approximate the
boundary of the original domain  by straight lines of the grid. In this way we obtain
a domain 0 approximating , and we can apply the tensor product integration on
each rectangle of 0.

180
8
Numerical Integration
Another (more accurate) method is an approximation 0 of  by a set of non-
degenerate triangles. We denote the triangles by 
i and their number by n. Then
0 WD [n
iD1
i :
We interpolate the function f over each triangle 
i. Taking, e.g., linear interpo-
lation, denote by fi1, fi2, fi3 the values of f at the three vertices of the triangle.
These three values uniquely determine a plane z D Fi.x; y/ which provides a linear
interpolation of f over 
i. The volume of the prism I3.
i; fi1; fi2; fi3/ between
our plane and the x; y-plane is already known from the secondary school:
I3.
i; fi1; fi2; fi3/ D 1
3.fi1 C fi2 C fi3/j
ij:
Here j
ij is the area of the triangle 
i.
Adding these prisms we get the following cubature formula:
In3.0; f / WD 1
3
n
X
iD1
.fi1 C fi2 C fi3/j
ij:
(8.17)
In summary, we have applied the following approximations:
Z

Z
f .x; y/ dx dy 
Z
0
Z
f .x; y/ dx dy 
n
X
iD1
Z

i
Z
Fi.x; y/ dx dy DWIn3.0; f /:
Formula (8.17) can be considered as a generalization of the trapezoidal rule.
Otherwise, we mention a formula generalizing the midpoint rule :
In1.f / D
n
X
iD1
f .si; ti/j
ij
(8.18)
where .si; ti/ is the centroid of the triangle 
i.
Taking a second order interpolation of f .x; y/ over each triangle we ﬁnd the
following cubature formula:
I 0
n3.f / WD 1
3
n
X
iD1
.fi1 C fi2 C fi3/j
ij;
(8.19)
where now fij denotes the value at the midpoint of the jth side of the ith triangle.
Let us check this formula. It is sufﬁcient to consider only one triangle 
i, and
we can assume that it is half of the unit square: 0  x; y  1; x C y  1. The
function values at the midpoints of the three sides determine a plane again, and
the formula gives exactly the volume under this plane, that is, it is exact for an

8.5
Calculation of Multiple Integrals
181
arbitrary ﬁrst order polynomial. Therefore, we have to deal only with the second
order polynomials: x2; xy; y2. Due to symmetry it is sufﬁcient to examine only one
of the two squares, hence it is enough to check xy; y2.
(a) The calculation of the exact value of the integral for f .x; y/ D xy:
I.
i; f / D
Z

i
Z
xy dx dy D
Z 1
0
Z 1y
0
xy dx dy D
Z 1
0
y
Z 1y
0
x dx dy
D
Z 1
0
y
x2
2
	1y
0
dy D
Z 1
0
y .1  y/2
2
dy D 1
2
Z 1
0
Œy  2y2 C y3 dy
D 1
2
y2
2  2y3
3 C y4
4
	1
0
D 1
2
1
2  2
3 C 1
4

D 1
24:
In order to calculate the value of the cubature formula we produce a small table
containing the coordinates of the midpoints and the values of f at these points:
j
1
2
3
xj
1
2
1
2
0
yj
0
1
2
1
2
f .xj ; yj/ D xj  yj
0
1
4
0
According to this, I 0
3.f / D 1
3.0 C 1
4 C 0/j
ij D
1
12  1
2 D
1
24 , which means that
for f .x; y/ D xy we have checked the coincidence.
(b) For f .x; y/ D y2 the calculation of the integral proceeds similarly:
I.
i; f / D
Z

i
Z
y2 dx dy D
Z 1
0
Z 1y
0
y2 dx dy D
Z 1
0
y2
Z 1y
0
dx dy
D
Z 1
0
y2Œx1y
0
dy D
Z 1
0
y2.1  y/ dy D
Z 1
0
Œy2  y3 dy
D
y3
3  y4
4
	1
0
D 1
3  1
4 D 1
12 :
In order to calculate the value of the cubature formula we produce a table again,
however, now the coordinates xj are not needed, as f does not depend on them:
j
1
2
3
yj
0
1
2
1
2
f .xj ; yj/ D y2
j
0
1
4
1
4
Hence, I 0
3.f / D 1
3.0 C 1
4 C 1
4/j
ij D 1
6  1
2 D
1
12 . This means we have checked
the coincidence for f .x; y/ D y2, too. In other words, the cubature formula (8.19)
is exact for arbitrary second order polynomials.

182
8
Numerical Integration
We mention another possibility for partitioning the domain: the inner part is
divided into rectangles, while near the boundary it is divided into triangles. On these
subdomains the approximation of the integral can be obtained using tensor product
integration and the above formulae, respectively.
8.5.3
Algorithm of the Two-Dimensional Simpson Rule, Tests
At the end of this chapter we describe the algorithm of the two-dimensional Simpson
rule. (The algorithm for the two-dimensional trapezoidal rule is similar but shorter.)
Given m and n (the number of the subintervals in directions x and y, respec-
tively), interval Œa; b and functions c.x/, d.x/, moreover the integrand f .x; y/, we
want to ﬁnd the approximate value Im;n of the integral
I WD
Z b
a
Z d.x/
c.x/
f .x; y/ dy dx
using Simpson’s rule in both coordinate directions.
1. H WD .b  a/=m, ? H D 0 ? [ stop: result Im;n D 0 ]
2. x WD a C H, s WD .Sim1.a/ C Sim1.b//=2
3. i WD 1.1/m  1
4.
Œs WD s C Sim1.x/, x WD x C Hi
5. s WD s=2, x WD a C H=2
6. i WD 1.1/m
7. Œs WD s C Sim1.x/, x WD x C Hi
8. s WD s 	 H 	 2=3
9. stop [result: s D Im;n]
Here the description of the code for Sim1.x/ (the one-dimensional Simpson’s
rule) is not needed, as it contains the same steps 1–9 with the following substitutions:
b ! d.x/; a ! c.x/; Sim1.x/ ! f .x; y/; m ! n; Im;n ! In:
(We remark that the two-dimensional trapezoidal sum is obtained at the end of
step 4 if Sim1.x/ is changed correspondingly.)
First test example: a D 1; b D 2; c 	 2; d 	 1; f D x3y3 C x2  y3 C 2.
Then the tensor product interpolation is realized and we obtain the exact value independently
of m and n (e.g. also in the case of m D n D 1): I.f / D Im;n D I1;1 D 387
16 D 24:1875.
Second test example: a D 1; b D 2; c.x/ D 2x; d.x/ D 1Cx; f D xyCxyC2.
As a result of our procedure we obtain exactly also this integral: I.f / D I1;1 D 141
4 D 35:25.
As a third test example we give two cases when the program provides only approximate
results. Here again a D 1; b D 2:
(a) c
	
2; d
	
1; f
D
x4y4 C x2  y3, where I1;1
D
94:640625; I2;2
D
65:49664307; I.f / D 6381
100 .

8.6
Exercises
183
(b) c.x/ D 2  x; d.x/ D 1 C x; f
D x2y2 C x  y, where I1;1 D 80:5; I2;2 D
71:640625; I.f / D 1421
20 D 71:05.
8.6
Exercises
1. Consider the pairwise different numbers x0; : : : ; xn from the interval Œa; b,
moreover, use our customary notations
I.f / WD
Z b
a
f .x/ dx;
In.f / D
n
X
iD0
aif .xi/:
Write the linear equations for the weights ai obtained from the requirement
In.p/ D I.p/ when p equals the monomi 1; x; x2; : : : xn. What kind of matrix
do you obtain and what can you say about the solvability of the system for the
weights faig ?
2. How large a value of m (the number of subintervals m D ba
h ) should be applied
to approximate the integral
Z 1
0
1
1 C x3 dx
with precision " D 3  104 using the composite trapezoidal formula (8.10) and
error bound (8.12) ?
3. Write a program to calculate the composite Simpson formula (8.11) having a
single loop and in the body of the loop, besides x and function evaluations, using
additions of function values only.
4. What is the limit of the sequence fsig1
iD1 if si WD a C bqi and jqj < 1 ? Find the
value I.f / that the formula (8.15) delivers if I`; I2`; I4` are replaced by three
values si; siC1; siC2 of the sequence fsig1
iD1 for some i (that is, siCk DW I2k`).
5. What is the order of polynomials of two variables for which the cubature
formula (8.18) is exact?
6. What is the order of the two-dimensional tensor product version of Simpson’s
rule?
7. Consider the possibilities of numerical integration in MATLAB reading the
help about trapz and quad. Then solve the above problem 2 for different
accuracies, checking your former estimate about the necessary number of points.
8. Write a cycle to compute, using quad in MATLAB, the integral of sin.2mx/
over Œ0; 1. You know that the result should be zero for natural m. Let the cycle
run over m D 20; 200; 2000; 20;000 and think about improving your results by
selecting appropriate options.
9. Read about dblquad and solve the third test example for the 2-dimensional
Simpson rule in both cases (a) and (b).

9
Numerical Solution of Ordinary Differential
Equations
The inverse operation of integration is differentiation, hence, if
y.x/ D c C
Z x
x0
f .t; y.t// dx;
where c D const, then after differentiating we obtain
dy
dx D f .x; y.x//;
because one has to differentiate the integral with respect to its upper bound by substituting the
upper bound into the integrand.
According to the chain rule, if y D f .x/ and x D g.t/, that is, y D f .x.t//, then
dy
dt D dy
dx  dx
dt D dy
dx  dg
dt D f 0.x/  g0.t/:
Moreover, if y D f .x; z/ and z D g.x/, then
dy
dx D @f
@x C @f
@z
dg
dx :
Here, the partial derivatives @f
@x (or @f
@z ) are obtained by considering z (or x) as constant and then
differentiating as usual.
9.1
Motivation
In a lot of models providing a mathematical description of time dependent tech-
nological, scientiﬁc and economical processes, ordinary differential equations play
an important role—so much so that for many experts the word “simulation” means
solving differential equations.
© Springer International Publishing AG 2016
G. Stoyan, A. Baran, Elementary Numerical Mathematics for Programmers and
Engineers, Compact Textbooks in Mathematics, DOI 10.1007/978-3-319-44660-8_9
185

186
9
Numerical Solution of Ordinary Differential Equations
Consider the spread of verbal information by personal contacts. The spread of
diseases or the sale of a new product in a given country are similar processes.
In order to spread the new information it is necessary that a person who knows it
(denoting by y the density of these people in the whole population) meets a person
who does not know it (the remainder, that is, obviously, their density is 1  y).
The probability of this event is y.1  y/. The effectivity of such a contact can
be characterized by a positive constant (say ˛): by how much does the density of
persons who know the information after the contact increase? We expect that the
longer the time interval 
t given for the contact the larger will be the increment of
the density: in ﬁrst approximation the density increases linearly as 
t increases.
Thus, if in a moment of time t the density was y.t/ then after the contact, in the
moment t C 
t, the new density will be
y.t C 
t/ D y.t/ C 
t˛y.t/.1  y.t//;
that is,
y.t C 
t/  y.t/

t
D ˛y.t/.1  y.t//:
Thinking here about smaller and smaller values 
t and considering y to be
differentiable, by taking the limit 
t ! 0 we get the differential equation
y0.t/ D ˛y.1  y/:
(9.1)
When in the initial time moment y.t/ D 0, then no new information was present,
so it cannot spread: y.t/ D 0 remains for all further t. However, when 1 >y.0/ > 0,
we expect an increase of y.t/ as t increases.
The validity of the previous reasoning can be checked using the analytical
solution of the differential equation which is
y.t/ D
1
1 C ˇe˛t ;
where 1 > y.0/ D
1
1 C ˇ > 0 if ˇ D 1  y.0/
y.0/
> 0:
(9.2)
Let us check the solution:
dy
dt D 
1
.1 C ˇe˛t/2
d.1 C ˇe˛t/
dt
D 
1
.1 C ˇe˛t/2 .˛ˇe˛t/:
On the other hand,
1  y.t/ D 1 C ˇe˛t  1
1 C ˇe˛t
D
ˇe˛t
1 C ˇe˛t ;

9.1
Motivation
187
and so in fact,
˛y.1  y/ D ˛
ˇe˛t
.1 C ˇe˛t/2 :
According to this, formula (9.2) provides the solution of the differential equa-
tion (9.1), and the constant ˇ can be determined using the initial value y.0/.
In what follows we also mention a simple mechanical problem: a mass hanging
on a spring. The independent variable is the time t and x is the unknown function,
the displacement of the mass from the normal position, and Px WD dx
dt is its derivative
with respect to t (the velocity of the mass) while Rx is the second derivative with
respect to t (the acceleration of the mass). We also take into account the friction
using a term r Px, moreover, an external force f .t/:
m Rx C r Px C kx D f .t/:
We assume that the constants m (mass) and k (spring constant) are positive.
Using the notations
y1.t/ WD x.t/; y2.t/ WD Px.t/;
the previous second order equation can be described as a system of ﬁrst order
equations in the following way:
Py1 D Px D y2;
Py2 D Rx D 1
m.r Px  kx C f / D 1
m.ry2  ky1 C f /:
Summarizing in vector form:
y WD
 
x
Px
!
;
A WD  1
m
0 m
k
r

; g WD
 
0
f
m
!
;
we obtain a system of differential equations:
Py D Ay C g:
Chemical processes can also be described with the help of systems consisting
of differential equations, however, these are—disregarding the simplest cases—
nonlinear. As an example one can consider smog models that are important from
the point of view of environmental protection where possibly we have to take into
account about 60 reactions and components.

188
9
Numerical Solution of Ordinary Differential Equations
Our examples show two problems of ordinary differential equations (which make
the analytical solution more difﬁcult, or practically impossible):
(1) typically one has to consider systems of equations;
(2) the equations are often nonlinear in the unknown variables.
9.2
Initial Value Problems
As the previous examples showed, the differential equation gives information about
the derivative of the unknown function y.x/, but often not directly. The typical
form is
y0 D dy
dx D f .x; y/;
(9.3)
where f .x; y/ is a given function, and we want to ﬁnd the function y.x/, e.g., on
the interval Œ0; 1. If another interval is of interest, e.g. the interval Œx0; xL, then we
can reduce it to the previous one using the transformation x D x0 C .xL  x0/t
(where t 2 Œ0; 1). In this case
y.x/ D y.x0 C .xL  x0/t/ DW z.t/; f .x; y/ D f .x0 C .xL  x0/t; z/;
hence,
dz
dt D dy
dt D dy
dx
dx
dt Df .x; y/.xLx0/Df .x0 C.xLx0/t; z/.xLx0/ DW g.t; z/:
Essentially, this is the same differential equation as the original one.
Equation (9.3) itself is not sufﬁcient to determine the unknown y uniquely, even
in the simplest case, when f .x; y/ is identically zero. In the latter case it is obvious
that the general solution is y.x/ D const, as the derivative of a (differentiable)
function equals zero only if the function is constant, however, the value of the
constant cannot be determined from the differential equation y0 D 0. Thus, we
need further information.
A piece of information, which we often have, is the initial value y.0/. In the
solution (9.2) of the differential equation (9.1) this value is present as a parameter.
Taking it into account the solution is uniquely determined.
In the mechanical example y.0/ is a vector which contains the initial position
x.0/ and the initial velocity dx
dt .0/. Here an explicit formula of the solution would
be rather complicated, but it is physically obvious that without the initial position
and velocity we do not have all the information about the state of the hanging mass.

9.2
Initial Value Problems
189
However, mathematically it follows that the initial value determines the solution
because the integral of Eq.(9.3) is
y.x/  y.x0/ D
Z x
x0
f .t; y.t// dt:
(9.4)
The latter formula shows that we can choose one of the possible solutions in such
a way that at some point x0—e.g. at x0 D 0—we prescribe the value y.x0/ of the
solution.
Together with this the initial value problem is complete:
For the differential equation (9.3),
dy
dx D f .x; y.x//;
given y.0/;
we want to ﬁnd y.x/ for x 2 Œ0; 1:
(9.5)
After prescribing y.0/, if f does not depend on y, (9.4) shows the formula of
the solution directly: y.x/ D y.0/ C R x
0 f .t/ dt. Obviously, it is possible that the
primitive function (or “antiderivative”) of the integrand is not known—so we have
to think about a numerical approximation. This is even more important in general,
when f can depend on y.
In order that (9.4) be mathematically well deﬁned, and also for its numerical
solution, we need some conditions on f .
It is practical to start from the following (sufﬁcient) conditions:
(1) we require the function f W Œ0; 1IR ! IR to be continuous in its ﬁrst argument
(which is x), and
(2) “Lipschitz-continuous” in the second argument (which is y), namely satisfying
jf .x; y/  f .x; z/j  Ljy  zj for all x 2 Œ0; 1 and y; z 2 IR:
In this second condition, y or z are not solutions of the initial value problem, they
are not functions, but two arbitrary real numbers. (However, it is sufﬁcient to assume
that this Lipschitz condition holds for all values y and z in a neighbourhood of the
solution.)
The previous two sufﬁcient conditions—continuity with respect to x and
Lipschitz continuity with respect to y—ensure not only the existence of the
solution of the initial value problem (9.5) but also its uniqueness.

190
9
Numerical Solution of Ordinary Differential Equations
Examples
First let us examine the right-hand side of Eq. (9.1). The ﬁrst one of the above two conditions holds:
the function f .x; y/ D ˛y.1  y/ is constant with respect to x, thus it is continuous in x. Let us
also examine the Lipschitz continuity of the function with respect to y:
f .y/  f .z/ D ˛.y.1  y/  z.1  z// D ˛.y  z  y2 C z2/ D ˛.y  z/.1  y  z/;
hence,
jf .y/  f .z/j  ˛jy  zjj1  y  zj;
that is: f is usually not Lipschitz continuous, because j1y zj can be arbitrarily large. However,
if we know that 0 < y; z  1 (as it follows from the explicit formula of the solution that 0 < y  1,
when 0 < y.0/  1), then j1  y  zj  1, hence
jf .y/  f .z/j  ˛jy  zj:
In this way f is Lipschitz continuous for these arguments and the Lipschitz constant is ˛ DW L.
Knowing the exact solution (9.2) this is comforting as for 0 < y.0/  1 it is the only solution of
the equation.
As second example consider the case when f .x; y/ D py, the interval is Œ0; 1 and the initial
value is y.0/ D 0. Thus, the initial value problem is
y0 D py; 0  x  1; y.0/ D 0:
This problem has (at least) two solutions:
(1) y 	 0, which is obvious, as in this case y0 	 0 and f .x; y/ 	 0;
(2) y D
 x
2
2. Now y0 D d
dx
x2
4 D 2x
4 D x
2 , which is exactly py.
Both solutions satisfy the initial condition, as well. From the above two conditions the ﬁrst one is
satisﬁed: being constant with respect to x, the function f .x; y/ D py is continuous. The problem
is again the Lipschitz continuity with respect to y. If, e.g., y > 0 and z  0, then
f .y/  f .z/ D py  pz D
y  z
py C pz;
hence,
jf .y/  f .z/j D
jy  zj
py C pz  jy  zj
py
;
and
1
py  L D const should be satisﬁed to ensure the Lipschitz continuity of this function f .
However, this inequality holds only when y  y0 > 0 for some positive y0, but in our example the
initial value of y equals 0.
From this latter example we see that without the Lipschitz continuity the
uniqueness is not ensured.

9.3
Euler’s Method
191
9.3
Euler’s Method
Now, we start to deal with the numerical solution of differential equations. We will
assume that f is Lipschitz continuous in its second variable, otherwise the problem
of lack of uniqueness can arise. Besides this we will need further differentiability
conditions, similarly as in the case of numerical integration.
The simplest method of providing a numerical solution to the initial value
problem (9.5) is Euler’s method. In order to introduce the method we assume that
n D 1.
The following idea is straightforward: from the initial value problem of the
differential equation the derivative of the unknown function y.x/ can be calculated
at the point x0 WD 0: y0.0/ D f .0; y.0//. If we take a small step h in this direction
(see Fig. 9.1):
x1 D x0 C h D h; y1 D y0 C hf.0; y0/; y0 WD y.0/;
then we usually make only a small error, namely of second order in h (we return to
this later).
Then we are in a similar situation as at the beginning: we have a point on the
x; y-plane crossed by the unknown function (now only approximately), and with
the help of the differential equation we get the value of the derivative at the given
point. Using this derivative, we can again approximate the solution after a second
step of length h:
x2 D x1 C h D 2h; y2 D y1 C hf.x1; y1/;
again making an error of second order. In general, we work according to the
formulae
xiC1 D xi C h; yiC1 D yi C hf.xi; yi/:
(9.6)
y(0)
h
y1
hf(0,y(0))
}
y(x)
x1=h
0
x
y
Fig. 9.1 The ﬁrst step of Euler’s method

192
9
Numerical Solution of Ordinary Differential Equations
In the worst case the second order errors accumulate (there will be further second
order errors due to the fact that the value yi is only an approximation for i > 0) and
after making a number of steps comparable with 1=h, the total error will be only
of ﬁrst order in h (we will return to this question later). For small h this total or
“global” error may turn out to be acceptable.
9.3.1
Algorithm of Euler’s Method, Test Examples
Formulating the complete method in pseudocode we obtain the following steps as
the algorithm of the “explicit” Euler method, when we divide the interval Œ0; 1 into
N subintervals of length h and for all points xi D ih we calculate a value yi which—
we hope—approximates the corresponding value y.xi/ of the exact solution:
Given x0 D 0; y0 D y.0/ and a procedure computing the right-hand side
f .x; y/ of the differential equation, moreover, the number N  1 of intervals.
1.
x0 WD 0; h WD 1=N
2.
i D 0; 1; : : : ; N  1
3.
[ xiC1 D xi C h
4.
yiC1 D yi C hf.xi; yi/ i
5.
[ stop: result .x0; y0/; .x1; y1/; : : : ; .xN ; yN / ]
As a ﬁrst test example we propose a case when Euler’s method provides the values of the exact
solution without error as f is constant:
y0 D 1; x 2 Œ0; 1; y.0/ D 0:
(9.7)
The general solution of this differential equation is y D x C a where a D const. However, due
to the initial value this constant must equal 0. That is, y D x is the solution of the initial value
problem, and using the program we should obtain this—disregarding rounding errors.
As a second test example consider the initial value problem
y0 D Ly; x 2 Œ0; 1; y.0/ D 1;
(9.8)
where L D const > 0. Its solution is y.x/ D eLx, so it is not difﬁcult to compare the numerical
and the exact solution (remember that the former is denoted by yi and the latter by y.xi/). If, e.g.,
L D 10, then using six digits
(a) h D 0:1:
(b) h D 0:05:
xi
y.xi/
yi
0.1
2.71828
2
0.2
7.38906
4
0.3
20.0855
8
:::
:::
:::
1.0
22,026.5
1024
xi
y.xi/
yi
0.05
1.64872
1.50000
0.1
2.71828
2.25000
0.15
4.48169
3.37500
:::
:::
:::
1.0
22,026.5
3325.26
Thus, the results are far from the exact solution, however, they are suitable for testing the
program. Let us continue the computation using smaller values h D 1=N , always doubling
the number N of the intervals (which is a recommended strategy). The following table shows

9.4
Error Analysis of Euler’s Method
193
in parallel the numerical values obtained for xN D 1 D N h and their global errors eN WD
jy.xN /yN j. In all cases the rounded value of the exact solution is y.xN / D 22;026:5. In the last
column of the table we show the ratio of the current and of the previous error:
N
h
yN
eN
eN =eN=2
10
0.1
1024
21,002.5
–
20
0.05
3325.26
18,701.24
0.8904
40
0.025
7523.16
14,503.34
0.7755
80
0.0125
12,365.2
9661.3
0.6661
160
0.00625
16,316.6
5709.9
0.5910
320
0.003125
18,900.3
3126.0
0.5475
640
0.0015625
20,387.5
1639.0
0.5243
From this table you can see that the convergence is slow but, in fact, of ﬁrst order: the ratios
of the actual and the previous errors approaches 0.5, that is, it behaves in the same way as the step
size h which is halved in each computational step. Expressing the two last errors with the help of
the current h, we have
e320  1;000;384  h; e640  1;048;960  h:
(9.9)
Euler’s method can also be applied to systems: then y 2 IRn, f 2 IRn and
in (9.6) both the values yi and f .xi; yi/ are vectors, that is, the formula yiC1 D
yi Chf.xi; yi/ means a loop through the corresponding components where the index
j (say) runs from 1 to n.
Then, if we are interested only in the last value x of the result, the old x and y
values can be overwritten by the new ones and in the case of systems this means a
large difference with respect to the storage needed.
9.4
Error Analysis of Euler’s Method
Euler’s method can also be considered in the following way: in the differential
equation (9.3) we replace the derivative of y with the so-called forward difference
quotient:
y.xi C h/  y.xi/
h
 y0.xi/ D f .xi; y.xi//:
The difference between the left- and right-hand sides of this expression is the local
discretization error of Euler’s method:
g.xi; h/ D gi WD y.xiC1/  y.xi/
h
 f .xi; y.xi//:

194
9
Numerical Solution of Ordinary Differential Equations
The local discretization error corresponding, e.g., to the differential equation (9.7)
is gi  0, because in this case y.x/ D x; f .x; y/ D 1 and the forward difference
quotient provides the derivative without error since the latter is constant:
gi D xiC1  xi
h
 1 D .i C 1/h  ih
h
 1 D 0:
However, the local discretization error corresponding to the differential equa-
tion (9.8) does not equal zero. Then y.x/ D eLx; f .x; y/ D Ly D LeLx and
the forward difference quotient gives the derivative only with error:
gi D eLxiC1  eLxi
h
 LeLxi D eLxi
eLh  1
h
 L

D eLxi
 
1 C Lh C 1
2.Lh/2 C O.h3/  1
h
 L
!
D eLxi

L C 1
2L2h C O.h2/  L

D O.h/:
The importance of the local discretization error is that with its help we can estimate
the global error ei WD y.xi/  yi of the numerical solution:
jeiC1j  eLxiC1
"
je0j C
iX
kD0
jgkjh
#
:
(9.10)
In order to obtain this estimate of the global error it is sufﬁcient to assume that f is
Lipschitz continuous in its second argument with Lipschitz constant L.
Using this we can obtain the convergence of the method assuming more
conditions on f : let f .x; y/ be continuously differentiable with respect to both its
arguments. This also ensures the existence and uniqueness of the analytical solution,
because in this case f is Lipschitz continuous in y, moreover, based on the chain
rule the solution y is twice continuously differentiable:
y0 D f .x; y.x//;
y00 D @f
@x C @f
@y
dy
dx :
(9.11)
Hence, you see that y00 is continuous. We use this in the Taylor series expansion of
y.xiC1/:
y.xi C h/ D y.xi/ C hy0.xi/ C h2
2 y00.xi C #ih/;

9.4
Error Analysis of Euler’s Method
195
where 0 < #i < 1. From this expression we obtain the Taylor series expansion of
the local discretization error g.xi; h/ at the point xi:
hgi D y.xi C h/  y.xi/  hf.xi; y.xi//
D Œy.xi/ C hy0.xi/ C h2
2 y00.xi C #ih/  y.xi/  hy0.xi/;
where we have taken into account that according to the differential equation (9.3),
f .x; y.x// D y0.x/. Thus,
jgij D h
2 jy00.xi C #ih/j  h
2 M2;
M2 WD max
Œ0;1 jy00.x/j:
The latter relation shows that the local discretization error of Euler’s method tends
to zero as h ! 0. Due to this we say that Euler’s method is consistent.
Now, we substitute the estimation of gi into (9.10):
jeiC1jeLxiC1
"
je0j C
iX
kD0
jgkjh
#
eL
"
je0j C h
2 M2
iX
kD0
h
#
eL

je0j C h
2 M2
	
;
(9.12)
as Pi
kD0 h D .i C 1/h D xiC1  1:
Pay attention to the fact that eiC1 D y.xiC1/  yiC1 is the error, while eLxiC1 
eL is the value of the exponential function.
From (9.12) one can see that in the case of e0 D y.0/  y0 ¤ 0 the convergence
cannot be guaranteed: if y0 differs from y.0/, then later this difference can even
increase. This remark is important when the exact value of y.0/ is not available,
because it is the result of another problem and so it may contain an error.
But, if e0 D 0, then from (9.12) there remains jeiC1j  eL h
2M2 D O.h/,
implying ﬁrst order convergence in h.
Now, let us apply the estimation (9.12) to the numerical solution of the
differential equation (9.8).
Then L D 10; M2 D y00.1/ D L2eL  2;202;646:6; eL M2
2  2:4  1010.
Comparing this value to our computational result (9.9) we get that the error bound
is approximately eL  22;026:6 times the value obtained there, however, it conﬁrms
that the error is of ﬁrst order in h.
Let us draw attention to the following fact: whether the method converges does
not depend on the particular value of y0, only y0 D y.0/ has to be valid, neither
does it depend on the particular f : it has only to be continuously differentiable.
Our numerical result is the sequence fyigN
iD0, and we expect its convergence to
the function y.x/, to be more precise, to the values fy.xi/gN
iD0 of y.x/. In order
to check the convergence we ﬁx, for a sequence of partitionings of Œ0; 1, the point
xi D ih D x, where i D i.h; x/. Here i.h; x/ ! 1 as h ! 0. In practical
computations the most suitable way is to double the number of intervals of the

196
9
Numerical Solution of Ordinary Differential Equations
partitioning, i.e., after computing with step size h we continue the computation with
h=2, etc.
Summary: properties of Euler’s method.
(a) If the function f is Lipschitz continuous in y with Lipschitz constant L, then
Euler’s method is stable: the estimation
jeij  eLxi
 
je0j C
i1
X
kD0
jgkjh
!
is valid.
(b) If y 2 C 2 (e.g. when f 2 C 1fŒ0; 1  IRg), then Euler’s method is consistent:
for the local errors gk we have
jgkj  h
2 M2 D O.h/;
M2 WD max
Œ0;1 jy00.x/j:
(c) “consistency + stability = convergence”: If e0 D 0 and y 2 C 2, then Euler’s
method is convergent on the interval Œ0; 1:
jeij D jy.xi/  yij  eL h
2 M2 D O.h/:
According to this the order of convergence equals 1, which can be described as
“slow but sure”.
9.5
The Modiﬁed Euler Method, Runge–Kutta Methods
Due to its slow convergence (see the computational results for the second test
example above) Euler’s method is rarely applied, e.g. in the case when the problem
is nonlinear and of a very large size. Then the application of a better (higher order)
method may be impossible, but in general the method can be improved in the
following way.
First we make only a half Euler-step, then at the obtained point we calculate again
the value of f —the derivative of the solution—and using this slope we perform a
whole step starting from .xi; yi/. With the half step we arrive at the midpoint of the
interval Œxi; xiC1, and based on the symmetry we hope that the slope calculated at
this point results in an accuracy of second order.
Formally, this idea gives (see Fig. 9.2):
(a) the half Euler-step:
xiC1=2 D xi C h
2 ; yiC1=2 D yi C h
2 f .xi; yi/;

9.5
The Modiﬁed Euler Method, Runge–Kutta Methods
197
x
y
0
y(0)
x1=h
h/2
}
y(x)
y1
hf(0,y(0))/2
Fig. 9.2 The ﬁrst step of the modiﬁed Euler method
(b) the whole step using the new slope:
xiC1 D xi C h; yiC1 D yi C hf.xiC1=2; yiC1=2/:
The local discretization error of this method, similarly to the case of Euler’s
method discussed in Sect. 9.4, can be deﬁned in the following way:
g.xi; h/ D gi WD y.xiC1/  y.xi/
h
 k2;
(9.13)
k1 WD f .xi; y.xi//;
k2 WD f

xi C h
2 ; y.xi/ C h
2 k1

:
In order to examine the local discretization error of the modiﬁed Euler method
we assume that f is twice continuously differentiable in both its arguments,
consequently y is three times continuously differentiable with respect to x.
In what follows we need the Taylor series expansion of the function f of two
variables. However, it can be reduced to the case of functions of one variable. What
we are interested in is the series expansion of f .x C ıx; y C ıy/ for ﬁxed .x; y/.
We deﬁne the following auxiliary function F.t/:
F.t/ WD f .x C tıx; y C tıy/;
where for simplicity we write x instead of xi and y instead of y.xi/, moreover
h
2 DW ıx; h
2 k1 D h
2 f .xi; y.xi// DW ıy:

198
9
Numerical Solution of Ordinary Differential Equations
Then the series expansion of k2 D F.1/ D f .x C ıx; y C ıy/ at the point t D
0 will be useful for us. By expressing the remainder term using the second order
derivatives, we get the following:
F.t/ D F.0/ C tF 0.0/ C t2
2 F 00.#t/
(where 0 < # D #.t/ < 1/
D f .x; y/ C t

fxıx C fyıy

.x; y/
Ct2
2

fxxıx2 C 2fxyıxıy C fyyıy2
.x C #tıx; y C #tıy/:
Here we have denoted the partial derivatives by subscripts, e.g. fxy D
@2f
@x@y ,
moreover we have used that
d.x C tıx/
dt
D ıx; d.y C tıy/
dt
D ıy
and hence, according to the chain rule
F 0.t/ D

fx
d.x C tıx/
dt
C fy
d.y C tıy/
dt
	
.x C tıx; y C tıy/
D Œfxıx C fyıy.x C tıx; y C tıy/;
F 00.t/ D dF 0
dt
D

fxxıx2 C 2fxyıxıy C fyyıy2
.x C tıx; y C tıy/:
Substituting the expressions we are interested in:
t D 1; x D xi; y D y.xi/; ıx D h
2 ; and ıy D h
2 k1 D h
2 f .xi; y.xi//
into the previous relations we obtain
k2 D F.1/ D F.0/ C F 0.0/ C 1
2F 00.#.1//
D f .xi; y.xi// C h
2

fx C fyf

.xi; y.xi// C O.h2/;
as
F 00.#/ D h2
4

fxx C 2fxyf C fyyf 2
.x C #ıx; y C #ıy/ D O.h2/

9.5
The Modiﬁed Euler Method, Runge–Kutta Methods
199
when, according to our assumption, f is twice continuously differentiable. Taking
into account (9.11), ﬁnally we have
k2 D f .xi; y.xi//Ch
2
fx C fyf  .xi; y.xi//CO.h2/ D y0.xi/Ch
2 y00.xi/CO.h2/:
Now, we can composite the series expansion of gi deﬁned in (9.13) at the point
.xi; y.xi//:
gi D 1
h

y.xi/ C hy0.xi/ C h2
2 y00.xi/ C O.h3/  y.xi/



y0.xi/ C h
2 y00.xi/ C O.h2/

D O.h2/:
This means that the modiﬁed Euler method is consistent.
Since for this method a similar stability estimation is valid as for Euler’s method,
we have second order convergence.
We illustrate the accuracy of the modiﬁed Euler method by the same computational example
corresponding to problem (9.8) which we have used in the case of Euler’s method:
(a) h D 0:1:
(b) h D 0:05:
xi
y.xi/
yi
0.1
2.71828
2.50000
0.2
7.38906
6.25000
0.3
20.0855
15.6250
:::
:::
:::
1.0
22,026.5
9536.74
xi
y.xi/
yi
0.05
1.64872
1.62500
0.1
2.71828
2.64063
0.15
4.48169
4.29102
:::
:::
:::
1.0
22,026.5
16,484.2
We also show a table (similarly as in the case of Euler’s method) which illustrates the behaviour
of the error eN WD jy.xN /  yN j in terms of N :
N
h
yN
eN
eN =eN=2
10
0.1
9536.74
12,489.8
–
20
0.05
16,484.2
5542.29
0.4437
40
0.025
20,200.2
1826.29
0.3295
80
0.0125
21,510.1
516.357
0.2827
160
0.00625
21,890.0
136.423
0.2642
320
0.003125
21,991.5
34.9928
0.2565
640
0.0015625
22,017.6
8.85644
0.2531
The table shows the second order convergence: the ratio of the actual and previous errors tends
to 0.25, so it behaves as h2. The error which is reached by the modiﬁed Euler method for N D 40

200
9
Numerical Solution of Ordinary Differential Equations
can be reached by the simple Euler method for (approximately) N D 640. To perform these steps,
the modiﬁed Euler method requires 80 function evaluations while Euler’s method requires 640,
which means that for this example the modiﬁed Euler method is eight times more efﬁcient.
The example of the modiﬁed Euler method shows that a method of order greater
than one can be constructed without using the formulae of the derivatives of f .
Instead, for the second argument of f , an expression containing f is substituted as
shown by the following method of writing the two steps (a) and (b) of the modiﬁed
Euler method as one step:
xiC1 D xi C h; yiC1 D yi C hf.xiC1=2; yi C h
2 f .xi; yi//:
(9.14)
This construction can be generalized and leads to the famous class of methods of
Runge and Kutta—of which the modiﬁed Euler method was a ﬁrst example, often
also denoted by RK2 because of its second order. Continuing the substitution of
f into f one can obtain methods of any prescribed order. For example, one of the
most popular methods for the numerical solution of ordinary differential equations
is the fourth order Runge–Kutta method, or RK4:
First we deﬁne the numbers kj below (in the case of systems they are vectors):
k1 WD f .xi; yi/;
k2 WD f .xi C h
2 ; yi C h
2 k1/;
k3 WD f .xi C h
2 ; yi C h
2 k2/;
k4 WD f .xi C h; yi C hk3/:
Observe that k2 appears in the above one-step formula (9.14) of RK2.
Now, taking a linear combination of these numbers kj , we calculate the new
y-value as follows:
yiC1 D yi C h
6 .k1 C 2k2 C 2k3 C k4/ :
It is not accidental that the f -values belonging to the points xiC1=2 (namely k2
and k3) are present in total with quadruple weight, while the f -values belonging to
the endpoints of the interval Œxi; xiC1 only have single weight: this corresponds to
Simpson’s formula.

9.6
The Implicit Euler Method
201
We also show computational results for the problem (9.8) in the case of the fourth order Runge–
Kutta method RK4:
(a) h D 0:1:
(b) h D 0:05:
xi
y.xi/
yi
0.1
2.71828
2.70833
0.2
7.38906
7.33507
0.3
20.0855
19.8658
:::
:::
:::
1.0
22,026.5
21,233.5
xi
y.xi/
yi
0.05
1.64872
1.64844
0.1
2.71828
2.71735
0.15
4.48169
4.47938
:::
:::
:::
1.0
22,026.5
21,950.8
Here, it is worth underlining the correct digits of the numerical results.
9.6
The Implicit Euler Method
Now, let us consider the approximate solution of the equation y0 D f .x; y/ using
the so-called implicit or backward Euler method, that is, with the help of the formula
yiC1 D yi C hf.xiC1; yiC1/;
(9.15)
in which the unknown value yiC1 appears on the right-hand side. Similarly to the
original Euler method (which is also called explicit), the implicit Euler method is
consistent and its order of convergence equals 1.
In contrast to the explicit Euler method it has the disadvantage that its appli-
cation involves the solution of a (generally nonlinear) equation. This disadvantage
decreases in the case of a system of linear differential equations.
9.6.1
The Implicit Euler Method for Linear Systems
Let y 2 IRn and f W Œ0; 1IRn ! IRn, namely f .x; y/ D AyCg.x/; A 2 IRnn:
y0 D Ay C g.x/:
The implicit Euler method is very important when the matrix A is ill-conditioned
and its eigenvalues are negative or have negative real part.
For the linear system, the method takes the following form:
yiC1 D yi C h

AyiC1 C g.xiC1/

;

202
9
Numerical Solution of Ordinary Differential Equations
and reordering the still unknown vector yiC1 we obtain
ŒI  hAyiC1 D yi C hg.xiC1/
(9.16)
where I is the n  n unit matrix.
We compute the LU factorization of the matrix on the left-hand side and then
we solve the system. As A does not depend on x, it is sufﬁcient to perform the
factorization only once.
We are going to explain the advantage of the implicit over the explicit Euler
method in the simplest case when n D 1, that is, we have only one differential
equation, and g does not depend on x, in other words, A and g are real numbers.
We assume that g > 0 and A is negative: A D q; q > 0. Hence, our differential
equation is (now I D 1 because of n D 1):
y0 D g  qy; its solution in the case of y.0/ D 0 W y.x/ D g
q .1  eqx/ :
(9.17)
This function is monotonously increasing and tends to g
q when x ! 1. In our case
of n D 1, (9.16) takes the form
Œ1 C hqyiC1 D yi C hg; that is yiC1 D yi C hg
1 C hq :
From this you can see that there is no problem for the computation of yiC1,
moreover, together with yi  0 and g > 0 the value yiC1 is also positive.
Now, we try to solve the same differential equation using the explicit Euler
method:
yiC1 D yi C h.g  qyi/ D yi.1  qh/ C hg:
(9.18)
According to our previous results the ﬁrst order convergence is guaranteed, but the
formula shows that when 1  qh < 1 increasing oscillations arise. This does not
disprove the convergence, because for h ! 0 we have 1  qh ! C1, however, on
a computer we work with a ﬁnite h and in this way it can easily occur that hq > 2,
particularly if q is large, see Fig. 9.3.
Here, the numerical solution is unacceptable for two reasons: it does not remain
bounded for i ! 1, and it shows oscillations—however, the exact solution tends
to a constant in a monotone way when x ! 1.
When, as in the example above, there is only one equation we can get rid of
the large q using a transformation: let us choose t D qx as the new independent
variable. Then
x D t
q ; y.x.t// DW z.t/;
dz
dt D dy
dx
dx
dt D y0.x/1
q D 1
q .g  qy/ D g
q  z:

9.6
The Implicit Euler Method
203
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
−5
0
5
10
x
y
explicit Euler
implicit Euler
exact solution
Fig. 9.3 Exact solution, results of explicit and implicit Euler method: y0 D g  qy, g D q D 25,
h D 0:1
However, a transformation like this does not provide the required effect in the case
of systems. Consider an example. Let the initial value problem be the following:
y0 D
501 500
500 501

y C
 
6
7
!
; y.0/ D
 
3
2
!
:
(9.19)
The matrix here corresponds to the matrix we mentioned at the beginning as a
problematical case for Euler’s (explicit) method: its eigenvalues are negative (to
be more precise, its eigenvalue are 1 and 1001) and the matrix is slightly ill-
conditioned, as cond A D 1001. Using the above transformation we get a multiplier
before the matrix A—however, this does not change its condition number.
Taking Euler’s method, we will have no difﬁculties if h is sufﬁciently small:
– the numerical solution remains bounded if h 
2
jmaxj D
2
1001, however, in this
case oscillations still may occur;
– there are no oscillations in the numerical solution when h 
1
jmaxj D
1
1001.
The latter is a rather unpleasant restriction, which can be even worse (if 5001 and
5000 stand in the matrix instead of 501 and 500, then the latter bound changes to
h 
1
10;001, etc.). Since the exact solution is
y.x/ D 38
77
 
1
1
!
e1001x C 3
 
1
1
!
ex  1
77
 
38
39
!
;

204
9
Numerical Solution of Ordinary Differential Equations
where the part connected to the eigenvalue 1001 for growing t soon becomes
insigniﬁcant, we might hope that a larger step size could be used in the explicit
Euler method, too. However, this is a mistake because the matrix did not change
and increasing oscillations will start here, too.
The problem that the numerical solution does not remain bounded for i ! 1
does not disappear even when we use other explicit methods. Consider, e.g., the
modiﬁed Euler method. If we apply this to (9.17), then instead of (9.18) the formula
will be
yiC1 D yi Ch

g  q.yi C h
2 .g  qyi//
	
D yi

1  qh C q2h2
2

Cg

1  qh
2

:
Here, yi is multiplied by a polynomial of z WD qh: p.z/ D 1zC z2
2 D 1
2.1C.1z/2/
whose values are larger than one, if z D qh > 2. Hence, yi increases rapidly with
i—apart from the case h  2
q . This is the same restriction as in the explicit Euler
method.
Similar observations can be made for general systems of differential equations.
Those systems whose Jacobi matrix is ill-conditioned or singular and the eigenval-
ues are non-positive (or their real parts are non-positive) and for which the implicit
Euler method works signiﬁcantly better than the explicit method (even if of higher
order) are called stiff differential equations.
When solving stiff differential equations, the bound on h (preventing oscillations
like those arising with Euler’s method) may slow down the computation much
more than the fact that each step of the implicit Euler method involves solution
of algebraic equations.
9.6.2
Nonlinear Systems
If f W
Œ0; 1  IRn ! IRn and it is not necessarily linear in y, then in each step of
the implicit Euler method (9.15) we obtain the equation
Fi.y/ WD y  hf.xiC1; y/  yi D 0;
(9.20)
whose solution y DW yiC1 is to be found. We apply Newton’s method to this
equation. It is obvious to use as a starting approximation of yiC1 the previous vector
yi: yiC1;0 WD yi.
In this case the formula of Newton’s method is (see Sect. 7.3.1)
0 D Fi.yi/ C F 0
i .yi/ıy;
or
F 0
i .yi/ıy D Fi.yi/ D .yi  hf.xiC1; yi/  yi/ D hf.xiC1; yi/;
(9.21)

9.6
The Implicit Euler Method
205
where ıy D yiC1;1yiC1;0 D yiC1;1yi is the update of the starting approximation.
Here the form of F 0
i is
F 0
i .yi/ D I  hJ.xiC1; yi/;
(9.22)
with the Jacobi matrix J.x; y/ of the vector function f .x; y/. In order to understand
the formula (9.22) we examine the case n D 2, when in (9.20) yi D .y.1/
i ; y.2/
i /T
and y D .y.1/; y.2//T , that is
Fi.y/ D
 
y.1/
y.2/
!
 h
 
f1.xiC1; y/
f2.xiC1; y/
!

 
y.1/
i
y.2/
i
!
:
The partial derivative of the ﬁrst row of this relation with respect to y.1/ consists of
two parts (because the third term on the right-hand side is constant, its derivative
equals 0). However, the derivatives of the ﬁrst and second terms are 1 and h @f1
@y.1/ ,
respectively. Moreover, the derivative of the ﬁrst row with respect to y.2/ is h @f1
@y.2/
since for the calculation of this the variable y.1/ of the ﬁrst term has to be considered
as a constant.
Similarly, the partial derivative of the second row with respect to y.1/ is h @f2
@y.1/ ,
but the derivatives of the ﬁrst and second term with respect to y.2/ equals 1 and
h @f2
@y.2/ , respectively. In summary, we see that in fact
F 0
i D
1 0
0 1

 h
 @f1
@y.1/
@f1
@y.2/
@f2
@y.1/
@f2
@y.2/
!
D I  hJ:
Now, we take into account the formula (9.22) in (9.21), moreover, we accept the
ﬁrst approximation yiC1;1 from (9.21) as yiC1, that is, as the ﬁnal approximation for
the solution of (9.20). Namely, we can check that by computing yiC1;1 we obtain
a method for solving the differential equation in ﬁrst order with respect to h and
further Newton iterations do not improve this order.
Also, from the point of view of stiff differential equations the most important
thing has already happened: in the approximation the Jacobi matrix of f appeared
as the multiplier of the unknown new value.
Therefore, we recommend the following linear system as a computational
formula:
ŒI  hJ.xiC1; yi/ıy D hf.xiC1; yi/; yiC1 D yi C ıy:
(9.23)
When f .x; y/ D Ay C g.x/, the Jacobi matrix is exactly J D A and then (9.23) is
the same as (9.16). Here, for nonlinear systems, the matrix I hJ.xiC1; yi/ changes
together with i, that is, we have to perform an LU factorization in each step.

206
9
Numerical Solution of Ordinary Differential Equations
9.6.3
Algorithm of the Implicit Euler Method, Test Examples
In the algorithm of the implicit Euler method we divide the interval Œ0; 1 into N
intervals of length h, but now y.x/ is an n-dimensional vector function and for each
node xi D ih we have to ﬁnd a vector yi which approximates the corresponding
vector y.xi/ of the exact solution.
Given the initial vector y0 D y.0/, a procedure computing the right-hand side
f .x; y/ of the differential equation, a procedure computing the Jacobi matrix, and
N  1, which is the number of the intervals, do the following:
1.
x0 WD 0; h WD 1=N
2.
i WD 0; 1; : : : ; N  1
3.
[xiC1 WD xi C h; Ji WD J.xiC1; yi/
4.
I  h 	 Ji H) ŒL; U; sing
5.
? sing ? [ stop: “singular matrix”, x D xiC1; y D yi ]
6.
LUıy D h 	 f .xiC1; yi/ H) ıy
(solved by LU-decomposition)
7.
yiC1 WD yi C ıy i
8.
[stop: result .x0; y0/; .x1; y1/; : : : ; .xN ; yN /]
If we ﬁnd the matrix to be singular let us start the computation again using a larger
N , because the perturbation lemma in Sect. 2.3.3 guarantees that for a sufﬁciently
small h the matrix I  h 	 J is regular.
As a ﬁrst test example let us complete the simple Eq. (9.7) to a system:
d
dx
 
y1
y2
!
D
 
1
2
!
;
 
y1.0/
y2.0/
!
D
 
3
4
!
;
because the program has to provide its exact solution, y1 D 3 C x; y2 D 4 C 2x, without
error—disregarding rounding errors. Thus,
f .x; y/ D
 
f1
f2
!
D
 
1
2
!
;
J.x; y/ D
0 0
0 0

:
Our second test example is a heat conduction example: we join a rod of 100ı and a rod of 0ı at
the moment of time t D 0, and then on the free end of the warm rod we set a temperature of 0ı
and on the free end of the cold rod a temperature of 50ı, and at N inner points we observe the
temperature y D .y1.t/; : : : ; yN .t//T . A discrete model of this process is the following system of
linear differential equations:
dy
dt D .N C 1/2 
0
BBBB@
2 1 0
0
: : :
0
1 2 1 0
: : :
0
: : : : : : : : : : : : : : : : : :
0
: : :
0 1 2 1
0
: : :
0
0 1 2
1
CCCCA
y C
0
BBBB@
0
0
: : :
0
50.N C 1/2
1
CCCCA
;
the initial value is y.0/ D .100; : : : ; 100; 0; : : : ; 0/T .
The matrix is symmetric, that is, each eigenvalue is real, moreover, with the help of the
Gershgorin discs one can check that the eigenvalues lie in the interval Œ0; 4.N C 1/2—that is,

9.6
The Implicit Euler Method
207
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
10
20
30
40
50
60
70
80
90
100
x
y
y(0)
y(1)
y(2)
y(3)
Fig. 9.4 The heat distribution after the ﬁrst three time-steps: N D 6; h D 0:01
the system will be stiff if N is large. (For larger N we obtain the heat distribution with higher
accuracy.) The Jacobi matrix equals the tridiagonal matrix given in the system of equations,
including the multiplier .N C 1/2.
We show the results of the ﬁrst and second time-step (rounded to six digits) for N D 6 in the
case when the time-step is h D 0:01, see also Fig. 9.4, where we have complemented the results
with the values 0 and 50 taken on the boundaries.
y.1/ D .0:721672; 0:875330; 0:774557; 0:213697; 0:088954; 0:145751/;
y.2/ D .0:547329; 0:738856; 0:651865; 0:314483; 0:182787; 0:242584/:
As a third test example we consider a system describing a particular chemical process, where
y1; y2 and y3 are the concentrations of some species A; B and C, respectively, changing in time.
Gathering the concentrations in the solution vector y.t/ D .y1.t/; y2.t/; y3.t//T , the following
nonlinear system of equations models the process:
dy
dt D
0
@
k1
0
k2y2
k1 k3y2 k2y2
0
k3y2
0
1
A y;
y.0/ D
0
@
1
0
0
1
A :
As values of the parameters we take
k1 D 0:04; k2 D 104; k3 D 3  107:

208
9
Numerical Solution of Ordinary Differential Equations
The Jacobi matrix of this nonlinear system is
J D J.y/ D
0
@
k1
k2y3
k2y2
k1 2k3y2  k2y3 k2y2
0
2k3y2
0
1
A :
The eigenvalues of J.y.0// (substituting yi D yi.0/, i D 1; 2; 3;) are
1 D 2 D 0; 3 D k1;
that is, at the beginning the system is not stiff—but after a short time it becomes stiff. Due to the
large k3 we can expect that soon k3y2 
 k1 and k3y2 
 k2y3 are valid, and then
1 D 0; 2 < 0; 3  2k3y2
where 2 is close to zero. Starting the computation with the step size h D 0:1 we obtain the
following results (rounded to six digits):
xi
y1i
y2i
y3i
0.1
0.996016
0.003984
0.0
0.2
0.996808
0.001992
0.001200
0.3
0.996538
0:9961  103
0.002465
1.0
0.978334
0:3270  104
0.021633
Then the eigenvalues of J are
1 D 0; 2  0:3306; 3  2178:07:
In order to check the numbers obtained using h D 0:1 we also show results for the time-step
h D 0:01. You can see that the previous computation was stable, but not very accurate:
xi
y1i
y2i
y3i
0.1
0.996122
0:3581  104
0.003842
0.2
0.992356
0:3513  104
0.007609
0.3
0.988729
0:3449  104
0.011237
1.0
0.966536
0:3076  104
0.033434
Now the eigenvalues of J are
1 D 0; 2  0:2943; 3  2179:58:
9.7
Exercises
1. Decide whether the following functions are Lipschitz continuous or not, when
y 2 Œ0; 1.
(a) f .y/ D ey;
(b) f .y/ D
3py;
(c) f .y/ D
1
1y .

9.7
Exercises
209
2. Write down the formula of the explicit Euler method for the solution of the initial
value problem
y0 D f .x; y/; y.0/ is given;
and draw a ﬁgure on the x; y-plane around the origin to explain the meaning of
the terms appearing in the formula.
3. Estimate the relative efﬁciency of the fourth order Runge–Kutta method com-
pared to Euler’s method for the example (9.8), L D 10, in the following way:
Denote by eh the error of Euler’s method and by h the actual step size.
Compute the coefﬁcients c1; c2 of the expression
eh WD c1h C c2h2
to ﬁt the two data points (corresponding to h D 1=320 and h D 1=640) given
in (9.9).
From this obtain the step size hE which provides (using the explicit Euler
method) the same error as the fourth order Runge–Kutta method in the case
hRK4 D 0:1 at x D 1 (see page 201).
Denote by NE the number of function evaluations on the unit interval
corresponding to hE in the case of Euler’s method and by NRK4 the number
corresponding to hRK4 in the case of the fourth order Runge–Kutta method.
Then the relative efﬁciency is NE=NRK4 because the ﬁrst order method
provides the same accuracy slower by this factor than the fourth order method.
4. Try the MATLAB-functions ode113 and ode45 (reading ﬁrst the correspond-
ing help) on the differential equation y0 D cos.x/ 	 y starting from y.0/ D 1.
Check that its solution is y.x/ D esin x and write an m-ﬁle (say) odetest1
to compute the right-hand side cos.x/ 	 y for given x and y. Then take
the solution interval Œ0; 40 and call ode113 for the following option values
(read now about odeset) ’reltol’,1e-1,’abstol’, 1e-3. Plot the
output ŒT; Y  as a curve. Simultaneously (in the same plot, using hold on,
hold off) show also the results of ode45, selecting another colour for its
output curve. Repeat this for ’reltol’,1e-2, ’abstol’,1e-4, and for
’reltol’,1e-3,’abstol’,1e-6—which is the default.
5. Explore the MATLAB possibilities to numerically solve stiff systems of differ-
ential equations by reading ﬁrst the help of ode15s. Then for the (mildly stiff)
system (9.19) write an m-ﬁle (say) odetest2 (the output of which must be a
column vector containing the two components of the right-hand side of (9.19)),
taking the solution interval Œ0; 0:05 and the initial values (9.19). Next call,
simultaneously, ode15s and ode113 for all three option combinations of the
preceding exercise.

10
Practical Error Estimation
Imagine the following situation: Your boss asks you to get a measure of the
reliability and convergence speed of some code that solves a complicated problem.
Your company has spent a lot of money on it and the modules of the code consist
of many thousands of lines. The documentation accompanying the code emphasizes
its superiority and shows a lot of convincing ﬁgures, but says nothing about the
reliability and accuracy. Of course the license sells the software “as is” and expressly
disclaims any warranties.
You know which system of equations is numerically solved by the code (it
contains integrals and partial derivatives, like most physically founded systems).
Moreover, this system, being nonlinear, has no known exact solution, except
in uninteresting, simple cases. Further, you have only limited possibilities (also
juridical prohibitions) to get into the code; perhaps you can vary some numerical
parameters like step sizes in space (if the code uses ﬁnite differences to approximate
the derivatives) and time (probably only upper and lower values for those step
sizes) or approximation degrees and element side lengths (when the code uses ﬁnite
elements or ﬁnite volumes).
Your task to get believable information about reliability and accuracy is not a
“mission impossible”. But you will need much more working and computing
time than for a routine run.
First, let us determine what we mean by reliability:
This is the percentage, among all code runs performed, of those runs which
produce useable results: numbers, tables, ﬁgures, videos, etc. The experts tell you
that the interpretable contents of these results may be reﬂecting the process modelled
by the code, even when they possibly have doubts about certain details. This “good”
part includes runs in which the code stops outputting the message that you did not
pay attention to some user instructions.
© Springer International Publishing AG 2016
G. Stoyan, A. Baran, Elementary Numerical Mathematics for Programmers and
Engineers, Compact Textbooks in Mathematics, DOI 10.1007/978-3-319-44660-8_10
211

212
10
Practical Error Estimation
But there may be “bad” cases in which the code ends with an error message
which is not connected to its misuse, there may be clearly false results, or abnormal
ends without any message.
Next, let us determine what we mean by accuracy. When looking at the results,
the human eye is a rather severe judge, and so are expert opinions. But it is also
convincing to determine numbers and numerical measures of deviations. Remember
Chap. 2 on norms and condition numbers—which was not exhaustive. For important
parts of the solution, take the maximum norm. But possibly there are regions
or components of the solution which are less important where you may prefer a
Euclidean norm weighted appropriately, e.g., if available, by the number of points
falling into those regions.
Start your work by planning a list of scenarios to examine the code, discuss it with
your colleagues, your boss and, perhaps, the legal department (erroneous decisions
in your company may have dire consequences).
When working down the approved list of scenarios it is very helpful to have the
support of software for producing reports in parallel with your computations: see,
e.g., in MATLAB the help under “publishing”.
Now we come to the task itself of judging reliability and accuracy.
In the uncommon case where you can manipulate the right-hand sides of the
equations solved by the program you can apply the method of test functions:
Helped by a mathematician you select a set of functions which satisfy, if possible,
the initial, boundary and further conditions (e.g., positivity) of the solutions. Let a
formula manipulation program perform the operations required by the equations
of your system. In this way you obtain artiﬁcial new solutions which satisfy all
conditions except the given right-hand sides. Then you change the original into the
calculated right-hand sides and run the program. Comparing the original and new
solutions you get a measure of the possible error of the original solutions.
If the former approach is impossible but you can change the number of points,
(maximal) length of elements or the order of approximating polynomials you may
turn to a version of Aitken’s correction (8.15), see also Exercise 3 in Chap.9:
Assume that in a selected, ﬁxed point or element of the solution domain there
holds a relation (like in (8.14))
yh D u C ch
(10.1)
where u is the true but unknown solution at the considered point, yh is its
calculated approximation and h represents numerical parameters like the (max-
imal) step sizes you can change. Here, if you can vary 
t; 
x; 
y; 
z, take
h
t0; h
x0; h
y0; h
z0. Then apply this h to compute yh. Observe that in (10.1)

10
Practical Error Estimation
213
0
100
200
300
400
500
600
−15
−10
−5
0
Number n of points in one direction
log10(error)
↓ n=384
2D Simpson error
cumulated rounding errors
Fig. 10.1 Error of the 2D Simpson rule for different n
there are two known values (yh and h) but three unknown values: u; c; , and c is in
general not zero and depends on the selected point or element, and on h (Fig. 10.1).
According to your task you are more interested in the error ch than in the
solution u. And in the expression of the former,  tells you the most interesting
characteristics: the convergence speed of the (unknown) method.
You obviously lack more information, but two more such relations should do. So
you compute with numerical parameters hi ¤ h; i D 1; 2, the solutions yhi which
ideally refer to the same considered point or element. If not, interpolate yhi from the
surrounding points. Then, with unknown u and considering c independent of h (we
return to this assumption),
yhi D u C ch
i ;
i D 1; 2;
(10.2)
allowing to eliminate u and to obtain yh  yh1 D c.h  h
1/ and yh1  yh2 D
c.h
1  h
2/. The ﬁrst relation gives an error estimate, namely
ch D
yh  yh1
1  .h1=h/ :
(10.3)
To get an estimate of the convergence speed  we further obtain
yh  yh1
yh1  yh2
D h  h
1
h
1  h
2
or
d WD yh  yh1
yh1  yh2
D
1  .h1=h/
.h1=h/  .h2=h/ :

214
10
Practical Error Estimation
Now we have one equation in the one unknown . We can try to solve it in a few
steps with Newton’s method (Chap.7), but if
h2=h1 D h1=h;
(10.4)
we have h2=h D .h1=h/2 and therefore
d D
1  .h1=h/
.h1=h/  .h2=h/ D
1  .h1=h/
.h1=h/.1  .h1=h// D .h=h1/:
This gives then, ﬁnally,
 D log d= log h
h1
:
(10.5)
Going back we get ch D
yhyh1
11=d D yh  u or
yh  u D yh  yh1
1  1=d D
yh  yh1
1 
yh1 yh2
yhyh1
D
.yh  yh1/2
yh  2yh1 C yh2
;
(10.6)
that is the Aitken correction (8.15) from page 177—now obtained under the more
general condition (10.4).
Our advice is then to plot the values of the (estimated) convergence speed  and
the (estimated) error (10.3) or (10.6), depending on the points or elements of the
solution domain. These ﬁgures constitute your main output to judge the accuracy of
the code, see, e.g., Fig. 10.2 below.
Let us discuss some problems in this Aitken correction approach.
1. A numerical algorithm usually has three phases, when looking at its errors: ﬁrst it is “attuning”
to the problem, and during this period the error can well enlarge (and then it may happen that
the logarithm in (10.5) gets a negative argument d); next, there comes the regular, theoretically
described asymptotic behaviour which is the basis of the Aitken correction. Finally, the error
reaches the level of rounding errors (remember Chap. 1) which may be estimated as number of
arithmetical operations times the “machine epsilon” "1). From then on, the error may behave
randomly.
2. The assumption that yh D u C ch where c does not depend on h (though varying with the
selected point or element) simpliﬁes the real situation. More realistic is the relation yh D u C
ch C dhC1 C    . But then the above formulae lead to an approximate result for , u and
yh  u. The latter will be at least of one order higher than in (10.1) whereas the exact value u
remains unknown. In other words, the exact value is not needed.
3. There are a lot of further approaches for error estimation. But these methods are similar in that
they are based on local use of higher order approximations. For example, it is possible to use,
for the actual step in integrating a function, Simpson’s rule along with the trapezoidal rule. This
means also computing the function value at the midpoint of the considered interval.

10
Practical Error Estimation
215
x
y
−1
0
1
2
3
4
−1
0
1
2
3
x
y
−1
0
1
2
3
4
−1
0
1
2
3
0
2
4
x 10−3
x 10−3
0
1
2
3
0
200
400
600
−16
−14
−12
−10
−8
−6
−4
−2
0
n
log10(error)
↓ n=256
↑ n=64
x
y
−1
0
1
2
3
4
−1
−0.5
0
0.5
1
1.5
2
2.5
3
−1
0
1
2
Fig. 10.2 Above, n D 96: left: error (10.3) of the 2D trapezoidal rule, right: error (10.6). Below,
left: error (10.3) of the 2D trapezoidal rule for different n, right: order (10.5) of convergence,
n D 96
To illustrate part of the above discussion we look at a picture showing the use of
Simpson’s rule in the two-dimensional case for the domain f1  x  4; 1 
y  3g and the integrand (with unknown integral)
f .x; y/ D10exp.4..x  1
 /2 C .y 
1
p
11
/2//
C 30exp.20..x  /2 C .y 
p
2/2//
 25exp.100..x  1=pe/2 C .y 
p
5/2//:

216
10
Practical Error Estimation
Subdividing the solution domain into nn rectangles, we ﬁnd that the (fourth-order)
Simpson method attunes to the problem until about n D 16, then rapidly reaches a
small error of about 1010—and the level of rounding errors. Beyond the point of
cross-over (at about n D 384) there is only a short interval (until about n D 631)
where the error surely diminishes. The random region seems to start from n D 2048
on.
The error of the (second-order) trapezoidal rule (blue; see the lower left part
of Fig. 10.2, on the previous page) is far from the (red) rounding errors and from
those of Simpson’s rule, and attunes to the problem until about n D 64. Aitken
acceleration (blue stars) improves remarkably the trapezoidal error reaching (for
n D 256) the level of the corresponding rounding errors (red stars) and the best
Simpson error of about 1010.
The absolute value of the local error (10.3) of the trapezoidal rule lies in Œ0; 5:21
103 and is shown on the upper left of Fig. 10.2 whereas the right upper ﬁgure
exhibits the error (10.6) which lies in Œ9:41  104; 3:47  103. This ﬁgure is more
detailed since we allowed negative values.
The estimated convergence order (10.5) is very near to 2, see the right lower part
of Fig. 10.2 for n D 96 where its mean value is  1:996. But locally it reﬂects the
surface of f (like the upper row of Fig. 10.2) and takes values in Œ1:547; 2:937.
For the parts of Fig. 10.2 which constitute our main result on the error investi-
gation of numerically integrating the above f , the exact value of the integral is not
needed. However, to be able to draw the real error on the lower left ﬁgure, we took
the approximation 11:779305207266269 obtained by averaging the Simpson values
to n D 2048, 4096; 8192.
To conclude, we may report that the 2D Simpson and trapezoidal codes showed a
reliability of 100 % (we never encountered problems in running those few lines, see
p. 182) and the order of convergence of the trapezoidal rule turned out to be mostly
2, apart from the steep parts of the integrand.

Bibliography
1. Axelsson, O. (1994). Iterative methods for large linear systems. Amsterdam: North Holland.
2. Cohen, H. (2011). Numerical approximation methods. New York: Springer.
3. Deuﬂhard, P. (2011). Newton methods for nonlinear problems: Afﬁne invariance and adaptive
algorithms. Berlin: Springer.
4. Golub, G. H., & van Loan, C. F. (1996/2013). Matrix computations (3rd ed., 4th ed.).
Baltimore: John Hopkins University Press.
5. Grifﬁths, D. F., & Higham, D. J. (2010). Numerical methods for ordinary differential equations:
Initial value problems. London: Springer.
6. Hohmann, A., & Deuﬂhard, P. (2003). Numerical analysis in modern scientiﬁc computing: An
introduction (2nd ed.). Berlin: Springer.
7. Moler, C. (2004). Numerical computing with MATLAB. Philadelphia: SIAM Books.
8. Nocedal, J., & Wright, S. J. (2000). Numerical optimization. New York: Springer.
9. Quarteroni, A., Saleri, F., & Gervasio, P. (2010). Scientiﬁc computing with MATLAB and
Octave (3rd ed.). Berlin: Springer.
10. Strang, G. (1988). Linear algebra and applications. San Diego: Harcourt, Brace, Jovanovich.
11. Wilkinson, J. H. (1964). Rounding errors in algebraic processes. Englewood Cliffs: Prentice-
Hall [or recently Dover Publications, 1994; Rundungsfehler. Springer 1969 (German ed.)].
© Springer International Publishing AG 2016
G. Stoyan, A. Baran, Elementary Numerical Mathematics for Programmers and
Engineers, Compact Textbooks in Mathematics, DOI 10.1007/978-3-319-44660-8
217

Index
A
accuracy, 15, 19, 59, 182, 185, 209, 222
accuracy estimation, 222
accuracy, mixed test, 15, 144, 185
Aitken correction, 186, 222, 224
B
backward error analysis, 59
Banach ﬁxed point theorem, 145
band matrices, 70
bisection method, 144
C
cancellation, 14
Cauchy inequality, 77
characteristic, 9
characteristic polynomial, 94, 98
condition number, 36, 39
cubature formula, 187
D
divided differencies, 126
E
efﬁciency, relative, 219
error, absolute, 14
error, relative, 14
error estimation, 221
Euler’s method, explicit, 201, 202
Euler’s method, implicit, 211, 216
extrapolational method, 187
F
factorization, Cholesky, 67
factorization, LU, 53
factorization, LDLT , 67, 69
factorization, LDU, 65
factorization, PLU, 61
factorization, PLDU, 63, 65
ﬁxed point iteration, 145
G
Gaussian elimination, 47
Gauss-Newton method, 164, 165
Gershgorin disc, 98, 99
Gershgorin’s theorem, 99
global convergence, 146
H
Horner scheme, 120
I
initial value problem, 199
inverse iteration, 110, 112
© Springer International Publishing AG 2016
G. Stoyan, A. Baran, Elementary Numerical Mathematics for Programmers and
Engineers, Compact Textbooks in Mathematics, DOI 10.1007/978-3-319-44660-8
219

220
Index
J
Jacobian matrix, 157, 161
M
machine epsilon ("1), 10, 11, 19
mantissa, 9
matrix, Hilbert, 39
matrix, ill-conditioned, 37
matrix, inverse, 57
matrix, normal, 95, 97, 106
matrix, orthogonal, 32
matrix, positive semideﬁnite, 32
midpoint rule, 173
Müller’s method, 156
N
Newton’s interpolation polynomial,
123
Newton’s method, 147, 152, 157
Newton method, damped, 153,
159
Newton-Cotes formulae, 176
norm, 25
norm, column sum, 29
norm, Euclidean, 25, 26
norm, induced matrix, 27
norm, Manhattan, 25
norm, maximum, 25
norm, p-, 25
norm, row sum, 31
normal equations, 85
O
overﬂow, 13
P
perturbation lemma, 40
pivoting, 61
power iteration, 106, 107
Q
QR algorithm, 114
QR decomposition, 115, 116
R
Rayleigh quotient, 105, 106
reliability, 221
Runge-Kutta methods, 210
rounding errors, 12, 16, 37, 58, 105, 185, 224,
225
S
secant method, 154
Simpson’s rule, 173, 180, 192, 224
spectral radius, 33
spline interpolation, 140
T
Taylor polynomial, 119
tensor product integration, 189
trapezoidal rule, 173, 180, 225
U
underﬂow, 13



Variational Bayesian Learning Theory
Variational Bayesian learning is one of the most popular methods in machine learning.
Designed for researchers and graduate students in machine learning, this book summa-
rizes recent developments in the nonasymptotic and asymptotic theory of variational
Bayesian learning and suggests how this theory can be applied in practice.
The authors begin by developing a basic framework with a focus on conjugacy, which
enables the reader to derive tractable algorithms. Next, it summarizes nonasymptotic
theory, which, although limited in application to bilinear models, precisely describes
the behavior of the variational Bayesian solution and reveals its sparsity-inducing
mechanism. Finally, the text summarizes asymptotic theory, which reveals phase
transition phenomena depending on the prior setting, thus providing suggestions on
how to set hyperparameters for particular purposes. Detailed derivations allow readers
to follow along without prior knowledge of the mathematical techniques speciï¬c to
Bayesian learning.
Shinichi Nakajima is a senior researcher at Technische UniversitÂ¨at Berlin. His
research interests include the theory and applications of machine learning, and he
has published papers at numerous conferences and in journals such as The Journal
of Machine Learning Research, The Machine Learning Journal, Neural Computation,
and IEEE Transactions on Signal Processing. He currently serves as an area chair for
Neural Information Processing Systems (NIPS) and an action editor for Digital Signal
Processing.
Kazuho Watanabe is an associate professor at Toyohashi University of Tech-
nology. His research interests include statistical machine learning and information
theory, and he has published papers at numerous conferences and in journals such
as The Journal of Machine Learning Research, The Machine Learning Journal, IEEE
Transactions on Information Theory, and IEEE Transactions on Neural Networks and
Learning Systems.
Masashi Sugiyama is the director of the RIKEN Center for Advanced Intelligence
Project and professor of Complexity Science and Engineering at the University of
Tokyo. His research interests include the theory, algorithms, and applications of
machine learning. He has written several books on machine learning, including Density
Ratio Estimation in Machine Learning. He served as program cochair and general
cochair of the NIPS conference in 2015 and 2016, respectively, and received the Japan
Academy Medal in 2017.


Variational Bayesian Learning Theory
SHINICHI NAKAJIMA
Technische UniversitÂ¨at Berlin
KAZUHO WATANABE
Toyohashi University of Technology
MASASHI SUGIYAMA
University of Tokyo

University Printing House, Cambridge CB2 8BS, United Kingdom
One Liberty Plaza, 20th Floor, New York, NY 10006, USA
477 Williamstown Road, Port Melbourne, VIC 3207, Australia
314â€“321, 3rd Floor, Plot 3, Splendor Forum, Jasola District Centre,
New Delhi â€“ 110025, India
79 Anson Road, #06â€“04/06, Singapore 079906
Cambridge University Press is part of the University of Cambridge.
It furthers the Universityâ€™s mission by disseminating knowledge in the pursuit of
education, learning, and research at the highest international levels of excellence.
www.cambridge.org
Information on this title: www.cambridge.org/9781107076150
DOI: 10.1017/9781139879354
Â© Shinichi Nakajima, Kazuho Watanabe, and Masashi Sugiyama 2019
This publication is in copyright. Subject to statutory exception
and to the provisions of relevant collective licensing agreements,
no reproduction of any part may take place without the written
permission of Cambridge University Press.
First published 2019
Printed and bound in Great Britain by Clays Ltd, Elcograf S.p.A.
A catalogue record for this publication is available from the British Library.
Library of Congress Cataloging-in-Publication Data
Names: Nakajima, Shinichi, author. | Watanabe, Kazuho, author. | Sugiyama,
Masashi, 1974- author.
Title: Variational Bayesian learning theory / Shinichi Nakajima (Technische
UniversitÂ¨at Berlin), Kazuho Watanabe (Toyohashi University of
Technology), Masashi Sugiyama (University of Tokyo).
Description: Cambridge ; New York, NY : Cambridge University Press, 2019. |
Includes bibliographical references and index.
Identiï¬ers: LCCN 2019005983| ISBN 9781107076150 (hardback : alk. paper) |
ISBN 9781107430761 (pbk. : alk. paper)
Subjects: LCSH: Bayesian ï¬eld theory. | Probabilities.
Classiï¬cation: LCC QC174.85.B38 N35 2019 | DDC 519.2/33â€“dc23
LC record available at https://lccn.loc.gov/2019005983
ISBN 978-1-107-07615-0 Hardback
Cambridge University Press has no responsibility for the persistence or accuracy
of URLs for external or third-party internet websites referred to in this publication
and does not guarantee that any content on such websites is, or will remain,
accurate or appropriate.

Contents
Preface
page ix
Nomenclature
xii
Part I
Formulation
1
1
Bayesian Learning
3
1.1
Framework
3
1.2
Computation
10
2
Variational Bayesian Learning
39
2.1
Framework
39
2.2
Other Approximation Methods
51
Part II
Algorithm
61
3
VB Algorithm for Multilinear Models
63
3.1
Matrix Factorization
63
3.2
Matrix Factorization with Missing Entries
74
3.3
Tensor Factorization
80
3.4
Low-Rank Subspace Clustering
87
3.5
Sparse Additive Matrix Factorization
93
4
VB Algorithm for Latent Variable Models
103
4.1
Finite Mixture Models
103
4.2
Other Latent Variable Models
115
5
VB Algorithm under No Conjugacy
132
5.1
Logistic Regression
132
5.2
Sparsity-Inducing Prior
135
5.3
Uniï¬ed Approach by Local VB Bounds
137
v

vi
Contents
Part III
Nonasymptotic Theory
147
6
Global VB Solution of Fully Observed Matrix Factorization
149
6.1
Problem Description
150
6.2
Conditions for VB Solutions
152
6.3
Irrelevant Degrees of Freedom
153
6.4
Proof of Theorem 6.4
157
6.5
Problem Decomposition
160
6.6
Analytic Form of Global VB Solution
162
6.7
Proofs of Theorem 6.7 and Corollary 6.8
163
6.8
Analytic Form of Global Empirical VB Solution
171
6.9
Proof of Theorem 6.13
173
6.10 Summary of Intermediate Results
180
7
Model-Induced Regularization and Sparsity Inducing
Mechanism
184
7.1
VB Solutions for Special Cases
184
7.2
Posteriors and Estimators in a One-Dimensional Case
187
7.3
Model-Induced Regularization
195
7.4
Phase Transition in VB Learning
202
7.5
Factorization as ARD Model
204
8
Performance Analysis of VB Matrix Factorization
205
8.1
Objective Function for Noise Variance Estimation
205
8.2
Bounds of Noise Variance Estimator
207
8.3
Proofs of Theorem 8.2 and Corollary 8.3
209
8.4
Performance Analysis
214
8.5
Numerical Veriï¬cation
228
8.6
Comparison with Laplace Approximation
230
8.7
Optimality in Large-Scale Limit
232
9
Global Solver for Matrix Factorization
236
9.1
Global VB Solver for Fully Observed MF
236
9.2
Global EVB Solver for Fully Observed MF
238
9.3
Empirical Comparison with the Standard VB Algorithm
242
9.4
Extension to Nonconjugate MF with Missing Entries
247
10
Global Solver for Low-Rank Subspace Clustering
255
10.1 Problem Description
255
10.2 Conditions for VB Solutions
258
10.3 Irrelevant Degrees of Freedom
259
10.4 Proof of Theorem 10.2
259

Contents
vii
10.5 Exact Global VB Solver (EGVBS)
264
10.6 Approximate Global VB Solver (AGVBS)
267
10.7 Proof of Theorem 10.7
270
10.8 Empirical Evaluation
274
11
Efï¬cient Solver for Sparse Additive Matrix Factorization
279
11.1 Problem Description
279
11.2 Efï¬cient Algorithm for SAMF
282
11.3 Experimental Results
284
12
MAP and Partially Bayesian Learning
294
12.1 Theoretical Analysis in Fully Observed MF
295
12.2 More General Cases
329
12.3 Experimental Results
332
Part IV
Asymptotic Theory
339
13
Asymptotic Learning Theory
341
13.1 Statistical Learning Machines
341
13.2 Basic Tools for Asymptotic Analysis
344
13.3 Target Quantities
346
13.4 Asymptotic Learning Theory for Regular Models
351
13.5 Asymptotic Learning Theory for Singular Models
366
13.6 Asymptotic Learning Theory for VB Learning
382
14
Asymptotic VB Theory of Reduced Rank Regression
385
14.1 Reduced Rank Regression
385
14.2 Generalization Properties
396
14.3 Insights into VB Learning
426
15
Asymptotic VB Theory of Mixture Models
429
15.1 Basic Lemmas
429
15.2 Mixture of Gaussians
434
15.3 Mixture of Exponential Family Distributions
443
15.4 Mixture of Bernoulli with Deterministic Components
451
16
Asymptotic VB Theory of Other Latent Variable Models
455
16.1 Bayesian Networks
455
16.2 Hidden Markov Models
461
16.3 Probabilistic Context-Free Grammar
466
16.4 Latent Dirichlet Allocation
470

viii
Contents
17
Uniï¬ed Theory for Latent Variable Models
500
17.1 Local Latent Variable Model
500
17.2 Asymptotic Upper-Bound for VB Free Energy
504
17.3 Example: Average VB Free Energy of Gaussian Mixture Model
507
17.4 Free Energy and Generalization Error
511
17.5 Relation to Other Analyses
513
Appendix A
Jamesâ€“Stein Estimator
516
Appendix B
Metric in Parameter Space
520
Appendix C
Detailed Description of Overlap Method
525
Appendix D
Optimality of Bayesian Learning
527
Bibliography
529
Subject Index
540

Preface
Bayesian learning is a statistical inference method that provides estimators
and other quantities computed from the posterior distributionâ€”the conditional
distribution of unknown variables given observed variables. Compared with
point estimation methods such as maximum likelihood (ML) estimation and
maximum a posteriori (MAP) learning, Bayesian learning has the following
advantages:
â€¢ Theoretically optimal.
The posterior distribution is what we can obtain best about the unknown
variables from observation. Therefore, Bayesian learning provides most
accurate predictions, provided that the assumed model is appropriate.
â€¢ Uncertainty information is available.
Sharpness of the posterior distribution indicates the reliability of
estimators. The credible interval, which can be computed from the posterior
distribution, provides probabilistic bounds of unknown variables.
â€¢ Model selection and hyperparameter estimation can be performed in a
single framework.
The marginal likelihood can be used as a criterion to evaluate how well a
statistical model (which is typically a combination of model and prior
distributions) ï¬ts the observed data, taking account of the ï¬‚exibility of the
model as a penalty.
â€¢ Less prone to overï¬tting.
It was theoretically proven that Bayesian learning overï¬ts the observation
noise less than MAP learning.
On the other hand, Bayesian learning has a critical drawbackâ€”computing
the posterior distribution is computationally hard in many practical models.
This is because Bayesian learning requires expectation operations or integral
computations, which cannot be analytically performed except for simple cases.
ix

x
Preface
Accordingly, various approximation methods, including deterministic and
sampling methods, have been proposed.
Variational Bayesian (VB) learning is one of the most popular deterministic
approximation methods to Bayesian learning. VB learning aims to ï¬nd the
closest distribution to the Bayes posterior under some constraints, which are
designed so that the expectation operation is tractable. The simplest and most
popular approach is the mean ï¬eld approximation where the approximate
posterior is sought in the space of decomposable distributions, i.e., groups
of unknown variables are forced to be independent of each other. In many
practical models, Bayesian learning is intractable jointly for all unknown
parameters, while it is tractable if the dependence between groups of parame-
ters is ignored. Such a case often happens because many practical models have
been constructed by combining simple models in which Bayesian learning is
analytically tractable. This property is called conditional conjugacy, and makes
VB learning computationally tractable.
Since its development, VB learning has shown good performance in many
applications. Its good aspects and downsides have been empirically observed
and qualitatively discussed. Some of those aspects seem inherited from full
Bayesian learning, while some others seem to be artifacts by forced indepen-
dence constraints. We have dedicated ourselves to theoretically clarifying the
behavior of VB learning quantitatively, which is the main topic of this book.
This book starts from the formulation of Bayesian learning methods. In
Part I, we introduce Bayesian learning and VB learning, emphasizing how
conjugacy and conditional conjugacy make the computation tractable. We also
brieï¬‚y introduce other approximation methods and relate them to VB learning.
In Part II, we derive algorithms of VB learning for popular statistical models,
on which theoretical analysis will be conducted in the subsequent parts.
We categorize the theory of VB learning into two parts, and exhibit them
separately. Part III focuses on nonasymptotic theory, where we do not assume
the availability of a large number of samples. This analysis so far has been
applied only to a class of bilinear models, but we can make detailed discus-
sions including analytic forms of global solutions and theoretical performance
guarantees. On the other hand, Part IV focuses on asymptotic theory, where
the number of observed samples is assumed to be large. This approach has
been applied to a broad range of statistical models, and successfully elucidated
the phase transition phenomenon of VB learning. As a practical outcome,
this analysis provides a guideline on how to set hyperparameters for different
purposes.

Preface
xi
Recently, a lot of variations of VB learning have been proposed, e.g., more
accurate inference methods beyond the mean ï¬eld approximation, stochastic
gradient optimization for big data analysis, and sampling based update rules for
automatic (black-box) inference to cope with general nonconjugate likelihoods
including deep neural networks. Although we brieï¬‚y introduce some of those
recent works in Part I, they are not in the central scope of this book. We rather
focus on the simplest mean ï¬eld approximation, of which the behavior has
been clariï¬ed quantitatively by theory.
This book was completed under the support by many people. Shinichi
Nakajima deeply thanks Professor Klaus-Robert MÂ¨uller and the members in
Machine Learning Group in Technische UniversitÂ¨at Berlin for their direct and
indirect support during the period of book writing. Special thanks go to Sergej
Dogadov, Hannah Marienwald, Ludwig Winkler, Dr. Nico GÂ¨onitz, and Dr. Pan
Kessel, who reviewed chapters of earlier versions, found errors and typos,
provided suggestions to improve the presentation, and kept encouraging him
in proceeding book writing. The authors also thank Lauren Cowles and her
team in Cambridge University Press, as well as all other staff members who
contributed to the book production process, for their help, as well as their
patience on the delays in our manuscript preparation. Lauren Cowles, Clare
Dennison, Adam Kratoska, and Amy He have coordinated the project since its
proposal, and Harsha Vardhanan in SPi Global has managed the copy-editing
process with Andy Saff.
The book writing project was partially supported by the following orga-
nizations: the German Research Foundation (GRK 1589/1) by the Federal
Ministry of Education and Research (BMBF) under the Berlin Big Data Center
project (Phase 1: FKZ 01IS14013A and Phase 2: FKz 01IS18025A), the
Japan Society for the Promotion of Science (15K16050), and the International
Research Center for Neurointelligence (WPI-IRCN) at The University of
Tokyo Institutes for Advanced Study.

Nomenclature
a, b, c,. . . , A, B,C,. . .
: Scalars.
a, b, c, . . . (bold-faced small letters)
: Vectors.
A, B, C, . . . (bold-faced capital letters)
: Matrices.
A, B, C, . . . (calligraphic capital letters)
: Tensors or sets.
(Â·)l,m
: (l, m)th element of a matrix.
âŠ¤
: Transpose of a matrix or vector.
tr(Â·)
: Trace of a matrix.
det (Â·)
: Determinant of a matrix.
âŠ™
: Hadamard (elementwise) product.
âŠ—
: Kronecker product.
Ã—n
: n-mode tensor product.
|Â·|
: Absolute value of a scalar. It applies element-wise for a vector
or matrix.
sign(Â·)
: Sign operator such that sign(x) =
â§âªâªâ¨âªâªâ©
1
if x â‰¥0,
âˆ’1
otherwise.
It applies
elementwise for a vector or matrix.
{Â· Â· Â· }
: Set consisting of speciï¬ed entities.
{Â· Â· Â· }D
: Dfold Cartesian product, i.e.,
XD â‰¡{(x1,. . . , xD)âŠ¤; xd âˆˆX for d = 1,. . . , D}.
# (Â·)
: Cardinality (the number of entities) of a set.
R
: The set of all real numbers.
R+
: The set of all nonnegative real numbers.
R++
: The set of all positive real numbers.
RD
: The set of all D-dimensional real (column) vectors.
xii

Nomenclature
xiii
[Â·, Â·]
: The set of real numbers in a range, i.e.,
[l, u] = {x âˆˆR; l â‰¤x â‰¤u}.
[Â·, Â·]D
: The set of D-dimensional real vectors whose entries are in a
range, i.e., [l, u]D â‰¡{x âˆˆRD; l â‰¤xd â‰¤u for d = 1,. . . , D}.
RLÃ—M
: The set of all L Ã— M real matrices.
RM1Ã—M2Ã—Â·Â·Â·Ã—MN
: The set of all M1 Ã— M2 Ã— Â· Â· Â· Ã— MN real tensors.
I
: The set of all integers.
I++
: The set of all positive integers.
C
: The set of all complex numbers.
SD
: The set of all D Ã— D symmetric matrices.
SD
+
: The set of all D Ã— D positive semideï¬nite matrices.
SD
++
: The set of all D Ã— D positive deï¬nite matrices.
DD
: The set of all D Ã— D diagonal matrices.
DD
+
: The set of all D Ã— D positive semideï¬nite diagonal matrices.
DD
++
: The set of all D Ã— D positive deï¬nite diagonal matrices.
HKâˆ’1
N
: The set of all possible histograms for N samples and
K categories, i.e., HKâˆ’1
N
â‰¡{x âˆˆ{0,. . . , N}K; K
k=1 xk = N}.
Î”Kâˆ’1
: The standard (K âˆ’1)-simplex, i.e.,
Î”Kâˆ’1 â‰¡{Î¸ âˆˆ[0, 1]K; K
k=1 Î¸k = 1}).
(a1,. . . , aM)
: Column vectors of A, i.e., A = (a1,. . . , aM) âˆˆRLÃ—M.
(a1,. . . ,aL)
: Row vectors of A, i.e., A = (a1,. . . ,aL)âŠ¤âˆˆRLÃ—M.
Diag(Â·)
: Diagonal matrix with speciï¬ed diagonal elements, i.e.,
(Diag(x))l,m =
â§âªâªâ¨âªâªâ©
xl
if l = m,
0
otherwise.
diag(Â·)
: Column vector consisting of the diagonal entries of a matrix, i.e.,
(diag(X))l = Xl,l.
vec(Â·)
: Vectorization operator concatenating all column vectors of a matrix
into a long column vector, i.e., vec(A) = (aâŠ¤
1 ,. . . , aâŠ¤
M)âŠ¤âˆˆRLM
for a matrix A = (a1,. . . , aM) âˆˆRLÃ—M.
ID
: D-dimensional (D Ã— D) identity matrix.
Î“
: A diagonal matrix.
Î©
: An orthogonal matrix.
ek
: One of K expression, i.e., ek = (0,. . . , 0,
kth
	

1
, 0,. . . , 0

	
K
)âŠ¤âˆˆ{0, 1}K.
1K
: K-dimensional vector with all elements equal to one, i.e.,
ek = (1,. . . , 1

	
K
)âŠ¤.

xiv
Nomenclature
GaussD(Î¼, Î£)
: D-dimensional Gaussian distribution with mean
Î¼ and covariance Î£.
MGaussD1,D2(M, Î£ âŠ—Î¨)
: D1 Ã— D2 dimensional matrix variate Gaussian
distribution with mean M and covariance Î£ âŠ—Î¨.
Gamma(Î±, Î²)
: Gamma distribution with shape parameter Î±
and scale parameter Î².
InvGamma(Î±, Î²)
: Inverse-Gamma distribution with shape parameter
Î± and scale parameter Î².
WishartD(V, Î½)
: D-dimensional Wishart distribution with scale
matrix V and degree of freedom Î½.
InvWishartD(V, Î½)
: D-dimensional inverse-Wishart distribution with
scale matrix V and degree of freedom Î½.
Multinomial(Î¸, N)
: Multinomial distribution with event probabilities
Î¸ and number of trials N.
Dirichlet(Ï†)
: Dirichlet distribution with concentration
parameters Ï†.
Prob(Â·)
: Probability of an event.
p(Â·), q(Â·)
: Probability distribution (probability mass function for discrete
random variables, and probability density function for
continuous random variables). Typically p is used for a
model distribution and q is used for the true distribution.
r(Â·)
: A trial distribution (a variable of a functional) for approximation.
âŸ¨f(x)âŸ©p(x)
: Expectation value of f(x) over distribution p(x), i.e.,
âŸ¨f(x)âŸ©p(x) â‰¡

f(x)p(x)dx.
Â·
: Estimator for an unknown variable, e.g., x and A are estimators
for a vector x and a matrix A, respectively.
Mean(Â·)
: Mean of a random variable.
Var(Â·)
: Variance of a random variable.
Cov(Â·)
: Covariance of a random variable.
KL(Â·||Â·)
: Kullbuckâ€“Leibler divergence between distributions, i.e.,
KL(p||q) â‰¡

log p(x)
q(x)

p(x).
Î´(Î¼;Î¼)
: Dirac delta function located at Î¼. It also denotes its
approximation (called Pseudo-delta function) with its
entropy ï¬nite.
GE
: Generalization error.
TE
: Training error.
F
: Free energy.

Nomenclature
xv
O( f(N))
: A function such that lim supNâ†’âˆ|O( f(N))/f(N)| < âˆ.
o( f(N))
: A function such that limNâ†’âˆo( f(N))/f(N) = 0.
Î©( f(N))
: A function such that lim infNâ†’âˆ|Î©( f(N))/f(N)| > 0
Ï‰( f(N))
: A function such that limNâ†’âˆ|Ï‰( f(N))/f(N)| = âˆ.
Î˜( f(N))
: A function such that lim supNâ†’âˆ|Î˜( f(N))/f(N)| < âˆ
and lim infNâ†’âˆ|Î˜( f(N))/f(N)| > 0.
Op( f(N))
: A function such that lim supNâ†’âˆ
Op( f(N))/f(N)
 < âˆ
in probability.
op( f(N))
: A function such that limNâ†’âˆop( f(N))/f(N) = 0 in probability.
Î©p( f(N))
: A function such that lim infNâ†’âˆ
Î©p( f(N))/f(N)
 > 0
in probability
Ï‰p( f(N))
: A function such that limNâ†’âˆ
Ï‰p( f(N))/f(N)
 = âˆ
in probability.
Î˜p( f(N))
: A function such that lim supNâ†’âˆ
Î˜p( f(N))/f(N)
 < âˆ
and lim infNâ†’âˆ
Î˜p( f(N))/f(N)
 > 0 in probability.


Part I
Formulation


1
Bayesian Learning
Bayesian learning is an inference method based on the fundamental law
of probability, called the Bayes theorem. In this ï¬rst chapter, we introduce
the framework of Bayesian learning with simple examples where Bayesian
learning can be performed analytically.
1.1 Framework
Bayesian learning considers the following situation. We have observed a set
D of data, which are subject to a conditional distribution p(D|w), called the
model distribution, of the data given unknown model parameter w. Although
the value of w is unknown, vague information on w is provided as a prior
distribution p(w). The conditional distribution p(D|w) is also called the model
likelihood when it is seen as a function of the unknown parameter w.
1.1.1 Bayes Theorem and Bayes Posterior
Bayesian learning is based on the following basic factorization property of the
joint distribution p(D, w):
p(w|D)

	
posterior
p(D)

	
marginal
= p(D, w)

	
joint
= p(D|w)

	
likelihood
p(w)

	
prior
,
(1.1)
where the marginal distribution is given by
p(D) =

W
p(D, w)dw =

W
p(D|w)p(w)dw.
(1.2)
Here, the integration is performed in the domain W of the parameter w.
Note that, if the domain W is discrete, integration should be replaced with
3

4
1 Bayesian Learning
summation, i.e., for any function f(w),

W
f(w)dw â†’

wâ€²âˆˆW
f(wâ€²).
The posterior distribution, the distribution of the unknown parameter w
given the observed data set D, is derived by dividing both sides of Eq. (1.1) by
the marginal distribution p(D):
p(w|D) = p(D, w)
p(D)
âˆp(D, w).
(1.3)
Here, we emphasized that the posterior distribution is proportional to the joint
distribution p(D, w) because the marginal distribution p(D) is a constant (as
a function of w). In other words, the joint distribution is an unnormalized
posterior distribution. Eq. (1.3) is called the Bayes theorem, and the posterior
distribution computed exactly by Eq. (1.3) is called the Bayes posterior when
we distinguish it from its approximations.
Example 1.1
(Parametric density estimation) Assume that the observed data
D = {x(1),. . . , x(N)} consist of N independent and identically distributed (i.i.d.)
samples from the model distribution p(x|w). Then, the model likelihood is
given by p(D|w) = N
n=1 p(x(n)|w), and therefore, the posterior distribution
is given by
p(w|D) =
N
n=1 p(x(n)|w)p(w)
 N
n=1 p(x(n)|w)p(w)dw
âˆ
N

n=1
p(x(n)|w)p(w).
Example 1.2
(Parametric regression) Assume that the observed data D =
{(x(1), y(1)),. . . , (x(N), y(N))} consist of N i.i.d. inputâ€“output pairs from the
model distribution p(x, y|w) = p(y|x, w)p(x). Then, the likelihood function
is given by p(D|w) = N
n=1 p(y(n)|x(n), w)p(x(n)), and therefore, the posterior
distribution is given by
p(w|D) =
N
n=1 p(y(n)|x(n), w)p(w)
 N
n=1 p(y(n)|x(n), w)p(w)dw
âˆ
N

n=1
p(y(n)|x(n), w)p(w).
Note that the input distribution p(x) does not affect the posterior, and accord-
ingly is often ignored in practice.
1.1.2 Maximum A Posteriori Learning
Since the joint distribution p(D, w) is just the product of the likelihood
function and the prior distribution (see Eq. (1.1)), it is usually easy to

1.1 Framework
5
compute. Therefore, it is relatively easy to perform maximum a posteriori
(MAP) learning, where the parameters are point-estimated so that the posterior
probability is maximized, i.e.,
wMAP = argmax
w
p(w|D) = argmax
w
p(D, w).
(1.4)
MAP learning includes maximum likelihood (ML) learning,
wML = argmax
w
p(D|w),
(1.5)
as a special case with the ï¬‚at prior p(w) âˆ1.
1.1.3 Bayesian Learning
On the other hand, Bayesian learning requires integration of the joint distri-
bution with respect to the parameter w, which is often computationally hard.
More speciï¬cally, performing Bayesian learning means computing at least one
of the following quantities:
Marginal likelihood (zeroth moment)
p(D) =

p(D, w)dw.
(1.6)
This quantity has been already introduced in Eq. (1.2) as the normalization
factor of the posterior distribution. As seen in Section 1.1.5 and subsequent
sections, the marginal likelihood plays an important role in model selection
and hyperparameter estimation.
Posterior mean (ï¬rst moment)
w = âŸ¨wâŸ©p(w|D) =
1
p(D)

w Â· p(D, w)dw,
(1.7)
where âŸ¨Â·âŸ©p denotes the expectation value over the distribution p, i.e., âŸ¨Â·âŸ©p(w) =

Â·p(w)dw. This quantity is also called the Bayesian estimator. The Bayesian
estimator or the model distribution with the Bayesian estimator plugged in (see
the plug-in predictive distribution (1.10)) can be the ï¬nal output of Bayesian
learning.
Posterior covariance (second moment)
Î£w =

(w âˆ’w)(w âˆ’w)âŠ¤
p(w|D) =
1
p(D)

(w âˆ’w)(w âˆ’w)âŠ¤p(D, w)dw, (1.8)

6
1 Bayesian Learning
where âŠ¤denotes the transpose of a matrix or vector. This quantity provides
the credibility information, and is used to assess the conï¬dence level of the
Bayesian estimator.
Predictive distribution (expectation of model distribution)
p(Dnew|D) = p(Dnew|w)
p(w|D) =
1
p(D)

p(Dnew|w)p(D, w)dw,
(1.9)
where p(Dnew|w) denotes the model distribution on unobserved new data Dnew.
In the i.i.d. case such as Examples 1.1 and 1.2, it is sufï¬cient to compute the
predictive distribution for a single new sample Dnew = {x}.
Note that each of the four quantities (1.6) through (1.9) requires to compute the
expectation of some function f(w) over the unnormalized posterior distribution
p(D, w) on w, i.e.,

f(w)p(D, w)dw. Speciï¬cally, the marginal likelihood,
the posterior mean, and the posterior covariance are the zeroth, the ï¬rst, and
the second moments of the unnormalized posterior distribution, respectively.
The expectation is analytically intractable except for some simple cases, and
numerical computation is also hard when the dimensionality of the unknown
parameter w is high. This is the main bottleneck of Bayesian learning, with
which many approximation methods have been developed to cope.
It hardly happens that the ï¬rst moment (1.7) or the second moment (1.8)
are computationally tractable but the zeroth moment (1.6) is not. Accordingly,
we can say that performing Bayesian learning on the parameter w amounts to
obtaining the normalized posterior distribution p(w|D). It sometimes happens
that computing the predictive distribution (1.9) is still intractable even if the
zeroth, the ï¬rst, and the second moments can be computed based on some
approximation. In such a case, the model distribution with the Bayesian
estimator plugged in, called the plug-in predictive distribution,
p(Dnew|w),
(1.10)
is used for prediction in practice.
1.1.4 Latent Variables
So far, we introduced the observed data set D as a known variable, and the
model parameter w as an unknown variable. In practice, more varieties of
known and unknown variables can be involved.
Some probabilistic models have latent variables (or hidden variables) z,
which can be involved in the original model, or additionally introduced for

1.1 Framework
7
computational reasons. They are typically attributed to each of the observed
samples, and therefore have large degrees of freedom. However, they are just
additional unknown variables, and there is no reason in inference to distinguish
them from the model parameters w.1 The joint posterior over the parameters
and the latent variables is given by Eq. (1.3) with w and p(w) replaced with
w = (w, z) and p(w) = p(z|w)p(w), respectively.
Example 1.3
(Mixture models) A mixture model is often used for parametric
density estimation (Example 1.1). The model distribution is given by
p(x|w) =
K

k=1
Î±kp(x|Ï„k),
(1.11)
where w = {Î±k, Ï„k; Î±k â‰¥0, K
k=1 Î±k = 1}K
k=1 is the unknown parameters. The
mixture model (1.11) is the weighted sum of K distributions, each of which
is parameterized by the component parameter Ï„k. The domain of the mixing
weights Î± = (Î±1,. . . , Î±K)âŠ¤, also called as the mixture coefï¬cients, forms the
standard (K âˆ’1)-simplex, denoted by Î”Kâˆ’1 â‰¡{Î± âˆˆRK
+; K
k=1 Î±k = 1} (see
Figure 1.1). Figure 1.2 shows an example of the mixture model with three
one-dimensional Gaussian components.
The likelihood,
p(D|w) =
N

n=1
p(x(n)|w),
=
N

n=1
â›âœâœâœâœâœâ
K

k=1
Î±kp(x|Ï„k)
ââŸâŸâŸâŸâŸâ ,
(1.12)
Î±1
Î±2
Î±3
Î±1 + Î±2 + Î±3 = 1
Figure 1.1 (K âˆ’1)-simplex, Î”Kâˆ’1, for K = 3.
1 For this reason, the latent variables z and the model parameters w are also called local latent
variables and global latent variables, respectively.

8
1 Bayesian Learning
â€“2
â€“1
0
1
2
0
0.2
0.4
0.6
0.8
1
Figure 1.2 Gaussian mixture.
for N observed i.i.d. samples D = {x(1),. . . , x(N)} has O(KN) terms, which
makes even ML learning intractable. This intractability arises from the summa-
tion inside the multiplication in Eq. (1.12). By introducing latent variables, we
can turn this summation into a multiplication, and make Eq. (1.12) tractable.
Assume that each sample x belongs to a single component k, and is drawn
from p(x|Ï„k). To describe the assignment, we introduce a latent variable
z âˆˆZ â‰¡{ek}K
k=1 associated with each observed sample x, where ek âˆˆ{0, 1}K is
the K-dimensional binary vector, called the one-of-K representation, with one
at the kth entry and zeros at the other entries:
ek = (0,. . . , 0,
kth
	

1
, 0,. . . , 0

	
K
)âŠ¤.
Then, we have the following model:
p(x, z|w) = p(x|z, w)p(z|w),
(1.13)
where
p(x|z, w) =
K

k=1
{p(x|Ï„k)}zk ,
p(z|w) =
K

k=1
Î±zk
k .
The conditional distribution (1.13) on the observed variable x and the latent
variable z given the parameter w is called the complete likelihood.
Note that marginalizing the complete likelihood over the latent variable
recovers the original mixture model:
p(x|w) =

Z
p(x, z|w)dz =

zâˆˆ{ek}K
k=1
K

k=1
{Î±kp(x|Ï„k)}zk =
K

k=1
Î±kp(x|Ï„k).
This means that, if samples are generated from the model distribution (1.13),
and only x is recorded, the observed data follow the original mixture model
(1.11).

1.1 Framework
9
In the literature, latent variables tend to be marginalized out even in
MAP learning. For example, the expectation-maximization (EM) algorithm
(Dempster et al., 1977), a popular MAP solver for latent variable models,
seeks a (local) maximizer of the posterior distribution with the latent variables
marginalized out, i.e.,
wEM = argmax
w
p(w|D) = argmax
w

Z
p(D, w, z)dz.
(1.14)
However, we can also maximize the posterior jointly over the parameters and
the latent variables, i.e.,
(wMAPâˆ’hard,zMAPâˆ’hard) = argmax
w,z
p(w, z|D) = argmax
w,z
p(D, w, z).
(1.15)
For clustering based on the mixture model in Example 1.3, the EM algorithm
(1.14) gives a soft assignment, where the expectation value zEM âˆˆÎ”Kâˆ’1 âŠ‚
[0, 1]K is substituted into the joint distribution p(D, w, z), while the joint
maximization (1.15) gives the hard assignment, where the optimal assignment
zMAPâˆ’hard âˆˆ{ek}K
k=1 âŠ‚{0, 1}K is looked for in the binary domain.
1.1.5 Empirical Bayesian Learning
In many practical cases, it is reasonable to use a prior distribution parame-
terized by hyperparameters Îº. The hyperparameters can be tuned by hand or
based on some criterion outside the Bayesian framework. A popular method of
the latter is the cross validation, where the hyperparameters are tuned so that
an (preferably unbiased) estimator of the performance criterion is optimized.
In such cases, the hyperparameters should be treated as known variables when
Bayesian learning is performed.
On the other hand, the hyperparameters can be estimated within the
Bayesian framework. In this case, there is again no reason to distinguish the
hyperparameters from the other unknown variables (w, z). The joint posterior
over all unknown variables is given by Eq. (1.3) with w and p(w) replaced
with w = (w, Îº, z) and p(w) = p(z|w)p(w|Îº)p(Îº), respectively, where p(Îº) is
called a hyperprior. A popular approach, called empirical Bayesian (EBayes)
learning (Efron and Morris, 1973), applies Bayesian learning on w (and z) and
point-estimate Îº, i.e.,
ÎºEBayes = argmax
Îº
p(D, Îº) = argmax
Îº
p(D|Îº)p(Îº),
where
p(D|Îº) =

p(D, w, z|Îº)dwdz.

10
1 Bayesian Learning
Here the marginal likelihood p(D|Îº) is seen as the likelihood of the hyperpa-
rameter Îº, and MAP learning is performed by maximizing the joint distribution
p(D, Îº) of the observed data D and the hyperparameter Îº, which can be seen as
an unnormalized posterior distribution of the hyperparameter. The hyperprior
is often assumed to be ï¬‚at: p(Îº) âˆ1.
With an appropriate design of priors, empirical Bayesian learning combined
with approximate Bayesian learning is often used for automatic relevance
determination (ARD), where irrelevant degrees of freedom of the statistical
model are automatically pruned out. Explaining the ARD property of approxi-
mate Bayesian learning is one of the main topics of theoretical analysis in Parts
III and IV.
1.2 Computation
Now, let us explain how Bayesian learning is performed in simple cases. We
start from introducing conjugacy, an important notion in performing Bayesian
learning.
1.2.1 Popular Distributions
Table 1.1 summarizes several distributions that are frequently used as a model
distribution (or likelihood function) p(D|w) or a prior distribution p(w) in
Bayesian learning. The domain X of the random variable x and the domain
W of the parameters w are shown in the table.
Some of the distributions in Table 1.1 have complicated function forms,
involving Beta or Gamma functions. However, such complications are mostly
in the normalization constant, and can often be ignored when it is sufï¬cient
to ï¬nd the shape of a function. In Table 1.1, the normalization constant is
separated by a dot, so that one can ï¬nd the simple main part. As will be
seen shortly, we often refer to the normalization constant when we need to
perform integration of a function, which is in the same form as the main part
of a popular distribution.
Below we summarize abbreviations of distributions:
GaussM(x; Î¼, Î£) â‰¡
1
(2Ï€)M/2 det (Î£)1/2 Â· exp

âˆ’1
2 (x âˆ’Î¼)âŠ¤Î£âˆ’1(x âˆ’Î¼)

,
(1.16)
Gamma(x; Î±, Î²) â‰¡
Î²Î±
Î“(Î±) Â· xÎ±âˆ’1 exp(âˆ’Î²x),
(1.17)

Table 1.1 Popular distributions. The following notation is used: R : The set of all real numbers, R++ : The set of all
positive real numbers, I++ : The set of all positive integers, SM
++ : The set of all M Ã— M positive deï¬nite matrices,
HKâˆ’1
N
â‰¡{x âˆˆ{0,. . . , N}K; K
k=1 xk = N} : The set of all possible histograms for N samples and K categories,
Î”Kâˆ’1 â‰¡{Î¸ âˆˆ[0, 1]K; K
k=1 Î¸k = 1} : The standard (K âˆ’1)-simplex, det (Â·) :Determinant of matrix, B(y, z) â‰¡
 1
0 tyâˆ’1(1 âˆ’t)zâˆ’1dt : Beta function, Î“(y) â‰¡
 âˆ
0 tyâˆ’1 exp(âˆ’t)dt : Gamma function, and Î“M(y) â‰¡

TâˆˆSM
++ det (T)yâˆ’(M+1)/2 exp(âˆ’tr(T))dT : Multivariate Gamma function.
Probability distribution
p(x|w)
x âˆˆX
w âˆˆW
Isotropic Gaussian
GaussM(x; Î¼, Ïƒ2IM) â‰¡
1
(2Ï€Ïƒ2)M/2 Â· exp

âˆ’1
2Ïƒ2 âˆ¥x âˆ’Î¼âˆ¥2 
x âˆˆRM
Î¼ âˆˆRM, Ïƒ2 > 0
Gaussian
GaussM(x; Î¼, Î£) â‰¡
1
(2Ï€)M/2 det(Î£)1/2 Â· exp

âˆ’1
2 (x âˆ’Î¼)âŠ¤Î£âˆ’1 (x âˆ’Î¼)
 
x âˆˆRM
Î¼ âˆˆRM, Î£ âˆˆSM
++
Gamma
Gamma(x; Î±, Î²) â‰¡
Î²Î±
Î“(Î±) Â· xÎ±âˆ’1 exp(âˆ’Î²x)
x > 0
Î± > 0, Î² > 0
Wishart
WishartM(X; V, Î½) â‰¡
1
(2Î½|V|)M/2Î“M( Î½
2 ) Â· det (X)
Î½âˆ’Mâˆ’1
2
exp
!
âˆ’tr(Vâˆ’1X)
2
"
X âˆˆSM
++
V âˆˆSM
++, Î½ > M âˆ’1
Bernoulli
Binomial1(x; Î¸) â‰¡Î¸x(1 âˆ’Î¸)1âˆ’x
x âˆˆ{0, 1}
Î¸ âˆˆ[0, 1]
Binomial
BinomialN(x; Î¸) â‰¡

N
x

Î¸x(1 âˆ’Î¸)Nâˆ’x
x âˆˆ{0,. . . , N}
Î¸ âˆˆ[0, 1]
Multinomial
MultinomialK,N(x; Î¸) â‰¡N! Â· K
k=1(xk! )âˆ’1Î¸xk
k
x âˆˆHKâˆ’1
N
Î¸ âˆˆÎ”Kâˆ’1
Beta
Beta(x; a, b) â‰¡
1
B(a,b) Â· xaâˆ’1(1 âˆ’x)bâˆ’1
x âˆˆ[0, 1]
a > 0, b > 0
Dirichlet
DirichletK(x; Ï†) â‰¡
Î“(K
k=1 Ï†k)
K
k=1 Î“(Ï†k) Â· K
k=1 xÏ†kâˆ’1
k
x âˆˆÎ”Kâˆ’1
Ï† âˆˆRK
++

12
1 Bayesian Learning
WishartM(X; V, Î½) â‰¡
1
(2Î½|V|)M/2Î“M
 Î½
2
 Â· det (X)
Î½âˆ’Mâˆ’1
2
exp

âˆ’tr(Vâˆ’1X)
2

,
(1.18)
BinomialN(x; Î¸) â‰¡
N
x

Â· Î¸x(1 âˆ’Î¸)Nâˆ’x,
(1.19)
MultinomialK,N(x; Î¸) â‰¡N! Â·
K

k=1
(xk! )âˆ’1Î¸xk
k ,
(1.20)
Beta(x; a, b) â‰¡
1
B(a, b) Â· xaâˆ’1(1 âˆ’x)bâˆ’1,
(1.21)
DirichletK(x; Ï†) â‰¡Î“(K
k=1 Ï†k)
K
k=1 Î“(Ï†k)
Â·
K

k=1
xÏ†kâˆ’1
k
.
(1.22)
The distributions in Table 1.1 are categorized into four groups, which are
separated by dashed lines. In each group, an upper distribution family is a
special case of a lower distribution family. Note that the following hold:
Gamma(x; Î±, Î²) = Wishart1

x; 1
2Î², 2Î±

,
BinomialN(x; Î¸) = Multinomial2,N

(x, N âˆ’x)âŠ¤; (Î¸, 1 âˆ’Î¸)âŠ¤ 
,
Beta(x; a, b) = Dirichlet2

(x, 1 âˆ’x)âŠ¤; (a, b)âŠ¤ 
.
1.2.2 Conjugacy
Let us think about the function form of the posterior (1.3):
p(w|D) = p(D|w)p(w)
p(D)
âˆp(D|w)p(w),
which is determined by the function form of the product of the model likeli-
hood p(D|w) and the prior p(w). Note that we here call the conditional p(D|w)
NOT the model distribution but the model likelihood, since we are interested
in the function form of the posterior, a distribution of the parameter w.
Conjugacy is deï¬ned as the relation between the likelihood p(D|w) and the
prior p(w).
Deï¬nition 1.4
(Conjugate prior) A prior p(w) is called conjugate with a
likelihood p(D|w), if the posterior p(w|D) is in the same distribution family
as the prior.

1.2 Computation
13
1.2.3 Posterior Distribution
Here, we introduce computation of the posterior distribution in simple cases
where a conjugate prior exists and is adopted.
Isotropic Gaussian Model
Let us compute the posterior distribution for the isotropic Gaussian model:
p(x|w) = GaussM(x; Î¼, Ïƒ2IM) =
1
(2Ï€Ïƒ2)M/2 Â· exp

âˆ’1
2Ïƒ2 âˆ¥x âˆ’Î¼âˆ¥2

.
(1.23)
The likelihood for N i.i.d. samples D = {x(1),. . . , x(N)} is written as
p(D|w) =
N

n=1
p(x(n)|w) =
exp

âˆ’1
2Ïƒ2
N
n=1 âˆ¥x(n) âˆ’Î¼âˆ¥2 
(2Ï€Ïƒ2)MN/2
.
(1.24)
Gaussian Likelihood As noted in Section 1.2.2, we should see Eq. (1.24),
which is the distribution of observed data D, as a function of the parameter
w. Naturally, the function form depends on which parameters are estimated in
the Bayesian way. The isotropic Gaussian has two parameters w = (Î¼, Ïƒ2),
and we ï¬rst consider the case where the variance parameter Ïƒ2 is known,
and the posterior of the mean parameter Î¼ is estimated, i.e., we set w = Î¼.
This case contains the case where Ïƒ2 is unknown but point-estimated in the
empirical Bayesian procedure or tuned outside the Bayesian framework, e.g.,
by performing cross-validation (we set w = Î¼, Îº = Ïƒ2 in the latter case).
Omitting the constant (with respect to Î¼), the likelihood (1.24) can be
written as
p(D|Î¼) âˆexp
â›âœâœâœâœâœââˆ’1
2Ïƒ2
N

n=1
âˆ¥x(n) âˆ’Î¼âˆ¥2
ââŸâŸâŸâŸâŸâ 
âˆexp
â›âœâœâœâœâœââˆ’1
2Ïƒ2
N

n=1
âˆ¥(x(n) âˆ’x) + (x âˆ’Î¼)âˆ¥2
ââŸâŸâŸâŸâŸâ 
= exp
â›âœâœâœâœâœââˆ’1
2Ïƒ2
â›âœâœâœâœâœâ
N

n=1
âˆ¥x(n) âˆ’xâˆ¥2 + Nâˆ¥x âˆ’Î¼âˆ¥2
ââŸâŸâŸâŸâŸâ 
ââŸâŸâŸâŸâŸâ 
âˆexp
!
âˆ’N
2Ïƒ2
###Î¼ âˆ’x
###2"
âˆGaussM

Î¼; x, Ïƒ2
N IM

,
(1.25)
where x =
1
N
N
n=1 x(n) is the sample mean. Note that we omitted the factor
exp $ âˆ’
1
2Ïƒ2
N
n=1 âˆ¥x(n) âˆ’xâˆ¥2% as a constant in the fourth equation.

14
1 Bayesian Learning
The last equation (1.25) implies that, as a function of the mean parameter Î¼,
the model likelihood p(D|Î¼) has the same form as the isotropic Gaussian with
mean x and variance Ïƒ2
N . Eq. (1.25) also implies that the ML estimator for the
mean parameter is given by
Î¼ML = x.
Thus, we found that the likelihood function for the mean parameter of the
isotropic Gaussian is in the Gaussian form. This comes from the following
facts:
â€¢ The isotropic Gaussian model for a single sample x is in the Gaussian form
also as a function of the mean parameter, i.e., GaussM(x; Î¼, Ïƒ2IM) âˆ
GaussM(Î¼; x, Ïƒ2IM).
â€¢ The isotropic Gaussians are multiplicatively closed, i.e., the product of
isotropic Gaussians with different means is a Gaussian: p(D|Î¼) âˆ
N
n=1 GaussM(Î¼; x(n), Ïƒ2IM) âˆGaussM

Î¼; x, Ïƒ2
N IM
 
.
Since the isotropic Gaussian is multiplicatively closed and the likelihood
(1.25) is in the Gaussian form, the isotropic Gaussian prior must be conjugate.
Let us choose the isotropic Gaussian prior,
p(Î¼|Î¼0, Ïƒ2
0) = GaussM(Î¼; Î¼0, Ïƒ2
0IM) âˆexp
â›âœâœâœâœââˆ’1
2Ïƒ2
0
âˆ¥Î¼ âˆ’Î¼0âˆ¥2
ââŸâŸâŸâŸâ ,
for hyperparameters Îº = (Î¼0, Ïƒ2
0). Then, the function form of the posterior is
given by
p(Î¼|D, Î¼0, Ïƒ2
0) âˆp(D|Î¼)p(Î¼|Î¼0, Ïƒ2
0)
âˆGaussM

Î¼; x, Ïƒ2
N

GaussM(Î¼; Î¼0, Ïƒ2
0)
âˆexp
â›âœâœâœâœââˆ’N
2Ïƒ2
###Î¼ âˆ’x
###2 âˆ’
1
2Ïƒ2
0
###Î¼ âˆ’Î¼0
###2
ââŸâŸâŸâŸâ 
âˆexp
â›âœâœâœâœâœâœââˆ’NÏƒâˆ’2 + Ïƒâˆ’2
0
2
######Î¼ âˆ’NÏƒâˆ’2x + Ïƒâˆ’2
0 Î¼0
NÏƒâˆ’2 + Ïƒâˆ’2
0
######
2ââŸâŸâŸâŸâŸâŸâ 
âˆGaussM
â›âœâœâœâœâÎ¼; NÏƒâˆ’2x + Ïƒâˆ’2
0 Î¼0
NÏƒâˆ’2 + Ïƒâˆ’2
0
,
1
NÏƒâˆ’2 + Ïƒâˆ’2
0
ââŸâŸâŸâŸâ .
Therefore, the posterior is
p(Î¼|D, Î¼0, Ïƒ2
0) = GaussM
â›âœâœâœâœâÎ¼; NÏƒâˆ’2x + Ïƒâˆ’2
0 Î¼0
NÏƒâˆ’2 + Ïƒâˆ’2
0
,
1
NÏƒâˆ’2 + Ïƒâˆ’2
0
ââŸâŸâŸâŸâ .
(1.26)

1.2 Computation
15
Note that the equality holds in Eq. (1.26). We omitted constant factors in the
preceding derivation. But once the function form of the posterior is found,
the normalization factor is unique. If the function form coincides with one of
the well-known distributions (e.g., ones given in Table 1.1), one can ï¬nd the
normalization constant (from the table) without any further computation.
Multiplicative closedness of a function family of the model likelihood
is essential in performing Bayesian learning. Such families are called the
exponential family:
Deï¬nition 1.5
(Exponential families) A family of distributions is called the
exponential family if it is written as
p(x|w) = p(t|Î·) = exp

Î·âŠ¤t âˆ’A(Î·) + B(t)
 
,
(1.27)
where t = t(x) is a function, called sufï¬cient statistics, of the random variable
x, and Î· = Î·(w) is a function, called natural parameters, of the parameter w.
The essential property of the exponential family is that the interaction
between the random variable and the parameter occurs only in the log linear
form, i.e., exp $Î·âŠ¤t%. Note that, although A(Â·) and B(Â·) are arbitrary functions,
A(Â·) does not depend on t, and B(Â·) does not depend on Î·.
Assume that N observed samples D = (t(1),. . . , t(N)) = (t(x(1)),. . . , t(x(N)))
are drawn from the exponential family distribution (1.27). If we use the
exponential family prior p(Î·)
=
exp

Î·âŠ¤t(0) âˆ’A0(Î·) + B0(t(0))
 
, then the
posterior is given as an exponential family distribution with the same set of
natural parameters Î·:
p(Î·|D) = exp
â›âœâœâœâœâœâÎ·âŠ¤
N

n=0
t(n) âˆ’Aâ€²(Î·) + Bâ€²(D)
ââŸâŸâŸâŸâŸâ ,
where Aâ€²(Î·) and Bâ€²(D) are a function of Î· and a function of D, respectively.
Therefore, the conjugate prior for the exponential family distribution is the
exponential family with the same natural parameters Î·.
All distributions given in Table 1.1 are exponential families. For example,
the sufï¬cient statistics and the natural parameters for the univariate Gaussian
are given by Î· = ( Î¼
Ïƒ2 , âˆ’1
2Ïƒ2 )âŠ¤and t = (x, x2)âŠ¤, respectively. The mixture model
(1.11) is a common nonexponential family distribution.
Gamma Likelihood Next we consider the posterior distribution of the vari-
ance parameter Ïƒ2 with the mean parameter regarded as a constant, i.e.,
w = Ïƒ2.

16
1 Bayesian Learning
Omitting the constants (with respect to Ïƒ2) of the model likelihood (1.24),
we have
p(D|Ïƒ2) âˆ(Ïƒ2)âˆ’MN/2 exp
â›âœâœâœâœâœââˆ’1
2Ïƒ2
N

n=1
âˆ¥x(n) âˆ’Î¼âˆ¥2
ââŸâŸâŸâŸâŸâ .
If we see the likelihood as a function of the inverse of Ïƒ2, we ï¬nd that it is
proportional to the Gamma distribution:
p(D|Ïƒâˆ’2) âˆ(Ïƒâˆ’2)MN/2 exp
â›âœâœâœâœâœââˆ’
â›âœâœâœâœâœâ
1
2
N

n=1
âˆ¥x(n) âˆ’Î¼âˆ¥2
ââŸâŸâŸâŸâŸâ Ïƒâˆ’2
ââŸâŸâŸâŸâŸâ 
âˆGamma
â›âœâœâœâœâœâÏƒâˆ’2; MN
2
+ 1, 1
2
N

n=1
âˆ¥x(n) âˆ’Î¼âˆ¥2
ââŸâŸâŸâŸâŸâ .
(1.28)
Since the mode of the Gamma distribution is known as argmaxx
Gamma (x; Î±, Î²) =
Î±âˆ’1
Î² , Eq. (1.28) implies that the ML estimator for the
variance parameter is given by
Ïƒ2 ML =
1
Ïƒâˆ’2 ML =
1
2
N
n=1 âˆ¥x(n) âˆ’Î¼âˆ¥2
MN
2 + 1 âˆ’1
=
1
MN
N

n=1
âˆ¥x(n) âˆ’Î¼âˆ¥2.
Now we found that the model likelihood of the isotropic Gaussian is
in the Gamma form as a function of the inverse variance Ïƒâˆ’2. Since the
Gamma distribution is in the exponential family and multiplicatively closed,
the Gamma prior is conjugate.
If we use the Gamma prior
p(Ïƒâˆ’2|Î±0, Î²0) = Gamma(Ïƒâˆ’2; Î±0, Î²0) âˆ(Ïƒâˆ’2)Î±0âˆ’1 exp(âˆ’Î²0Ïƒâˆ’2)
with hyperparameters Îº = (Î±0, Î²0), the posterior can be written as
p(Ïƒâˆ’2|D, Î±0, Î²0) âˆp(D|Ïƒâˆ’2)p(Ïƒâˆ’2|Î±0, Î²0)
âˆGamma
â›âœâœâœâœâœâÏƒâˆ’2; MN
2
+ 1, 1
2
N

n=1
âˆ¥x(n) âˆ’Î¼âˆ¥2
ââŸâŸâŸâŸâŸâ Gamma(Ïƒâˆ’2; Î±0, Î²0)
âˆ(Ïƒâˆ’2)MN/2+Î±0âˆ’1 exp
â›âœâœâœâœâœââˆ’
â›âœâœâœâœâœâ
1
2
N

n=1
âˆ¥x(n) âˆ’Î¼âˆ¥2 + Î²0
ââŸâŸâŸâŸâŸâ Ïƒâˆ’2
ââŸâŸâŸâŸâŸâ ,
and therefore
p(Ïƒâˆ’2|D, Î±0, Î²0) = Gamma
â›âœâœâœâœâœâÏƒâˆ’2; MN
2
+ Î±0, 1
2
N

n=1
âˆ¥x(n) âˆ’Î¼âˆ¥2 + Î²0
ââŸâŸâŸâŸâŸâ . (1.29)

1.2 Computation
17
Isotropic Gauss-Gamma Likelihood Finally, we consider the general case
where both the mean and variance parameters are unknown, i.e., w = (Î¼, Ïƒâˆ’2).
The likelihood is written as
p(D|Î¼, Ïƒâˆ’2) âˆ(Ïƒâˆ’2)MN/2 exp
â›âœâœâœâœâœââˆ’
â›âœâœâœâœâœâ
1
2
N

n=1
âˆ¥x(n) âˆ’Î¼âˆ¥2
ââŸâŸâŸâŸâŸâ Ïƒâˆ’2
ââŸâŸâŸâŸâŸâ 
= (Ïƒâˆ’2)MN/2 exp
â›âœâœâœâœââˆ’
â›âœâœâœâœâ
Nâˆ¥Î¼ âˆ’xâˆ¥2
2
+
N
n=1 âˆ¥x(n) âˆ’xâˆ¥2
2
ââŸâŸâŸâŸâ Ïƒâˆ’2
ââŸâŸâŸâŸâ 
âˆGaussGammaM
â›âœâœâœâœâÎ¼, Ïƒâˆ’2
x, NIM, M(N âˆ’1)
2
+ 1,
N
n=1 âˆ¥x(n) âˆ’xâˆ¥2
2
ââŸâŸâŸâŸâ ,
where
GaussGammaM(x, Ï„|Î¼, Î»IM, Î±, Î²)
â‰¡GaussM(x|Î¼, (Ï„Î»)âˆ’1IM) Â· Gamma(Ï„|Î±, Î²)
=
exp

âˆ’Ï„Î»
2 âˆ¥x âˆ’Î¼âˆ¥2 
(2Ï€(Ï„Î»)âˆ’1)M/2
Â·
Î²Î±
Î“(Î±) Ï„Î±âˆ’1 exp(âˆ’Î²Ï„)
=
Î²Î±
(2Ï€/Î»)M/2Î“(Î±) Ï„Î±+ M
2 âˆ’1 exp

âˆ’
Î»âˆ¥x âˆ’Î¼âˆ¥2
2
+ Î²

Ï„

is the isotropic Gauss-Gamma distribution on the random variable x âˆˆRM,
Ï„ > 0 with parameters Î¼ âˆˆRM, Î» > 0, Î± > 0, Î² > 0.
Note that, although the isotropic Gauss-Gamma distribution is the product
of an isotropic Gaussian distribution and a Gamma distribution, the random
variables x and Ï„ are not independent of each other. This is because the
isotropic Gauss-Gamma distribution is a hierarchical model p(x|Ï„)p(Ï„), where
the variance parameter Ïƒ2 = (Ï„Î»)âˆ’1 for the isotropic Gaussian depends on the
random variable Ï„ of the Gamma distribution.
Since the isotropic Gauss-Gamma distribution is multiplicatively closed, it
is a conjugate prior. Choosing the isotropic Gauss-Gamma prior
p(Î¼, Ïƒâˆ’2|Î¼0, Î»0, Î±0, Î²0) = GaussGammaM(Î¼, Ïƒâˆ’2|Î¼0, Î»0IM, Î±0, Î²)
âˆ(Ïƒâˆ’2)Î±0+ M
2 âˆ’1 exp

âˆ’
Î»0âˆ¥Î¼ âˆ’Î¼0âˆ¥2
2
+ Î²0

Ïƒâˆ’2

with hyperparameters Îº = (Î¼0, Î»0, Î±0, Î²0), the posterior is given by
p(Î¼, Ïƒâˆ’2|D, Îº) âˆp(D|Î¼, Ïƒâˆ’2)p(Î¼, Ïƒâˆ’2|Îº)
âˆGaussGammaM
â›âœâœâœâœâÎ¼, Ïƒâˆ’2
x, NIM, M(N âˆ’1)
2
+ 1,
N
n=1 âˆ¥x(n) âˆ’xâˆ¥2
2
ââŸâŸâŸâŸâ 
Â· GaussGammaM(Î¼, Ïƒâˆ’2|Î¼0, Î»0IM, Î±0, Î²)

18
1 Bayesian Learning
âˆ(Ïƒâˆ’2)MN/2 exp
â›âœâœâœâœââˆ’
â›âœâœâœâœâ
Nâˆ¥Î¼ âˆ’xâˆ¥2
2
+
N
n=1 âˆ¥x(n) âˆ’xâˆ¥2
2
ââŸâŸâŸâŸâ Ïƒâˆ’2
ââŸâŸâŸâŸâ 
Â· (Ïƒâˆ’2)Î±0+ M
2 âˆ’1 exp

âˆ’
Î»0âˆ¥Î¼ âˆ’Î¼0âˆ¥2
2
+ Î²0

Ïƒâˆ’2

âˆ(Ïƒâˆ’2)M(N+1)/2+Î±0âˆ’1
Â· exp
!
âˆ’
!
Nâˆ¥Î¼âˆ’xâˆ¥2+Î»0âˆ¥Î¼âˆ’Î¼0âˆ¥2
2
+
N
n=1 âˆ¥x(n)âˆ’xâˆ¥2
2
+ Î²0
"
Ïƒâˆ’2
"
âˆ(Ïƒâˆ’2)Î±+ M
2 âˆ’1 exp
â›âœâœâœâœâœâœââˆ’
â›âœâœâœâœâœâœâ
Î»
###Î¼ âˆ’Î¼
###2
2
+ Î²
ââŸâŸâŸâŸâŸâŸâ Ïƒâˆ’2
ââŸâŸâŸâŸâŸâŸâ ,
where
Î¼ = Nx + Î»0Î¼0
N + Î»0
,
Î» = N + Î»0,
Î± = MN
2
+ Î±0,
Î² =
N
n=1 âˆ¥x(n) âˆ’xâˆ¥2
2
+ NÎ»0âˆ¥x âˆ’Î¼0âˆ¥2
2(N + Î»0)
+ Î²0.
Thus, the posterior is obtained as
p(Î¼, Ïƒâˆ’2|D, Îº) = GaussGammaM(Î¼, Ïƒâˆ’2|Î¼,Î»IM,Î±,Î²).
(1.30)
Although the Gauss-Gamma distribution seems a bit more complicated
than the ones in Table 1.1, its moments are known. Therefore, Bayesian
learning with a conjugate prior can be analytically performed also when both
parameters w = (Î¼, Ïƒâˆ’2) are estimated.
Gaussian Model
Bayesian learning can be performed for a general Gaussian model in a
similar fashion to the isotropic case. Consider the M-dimensional Gaussian
distribution,
p(x|w) = GaussM(x; Î¼, Î£) â‰¡
1
(2Ï€)M/2 det (Î£)1/2 Â· exp

âˆ’1
2(x âˆ’Î¼)âŠ¤Î£âˆ’1(x âˆ’Î¼)

(1.31)
with mean and covariance parameters w = (Î¼, Î£). The likelihood for N i.i.d.
samples D = {x(1),. . . , x(N)} is written as
p(D|w) =
N

n=1
p(x(n)|w) =
exp

âˆ’1
2
N
n=1(x(n) âˆ’Î¼)âŠ¤Î£âˆ’1(x(n) âˆ’Î¼)
 
(2Ï€)NM/2 det (Î£)N/2
.
(1.32)

1.2 Computation
19
Gaussian Likelihood Let us ï¬rst compute the posterior distribution on the
mean parameter Î¼, with the covariance parameter regarded as a known
constant. In this case, the likelihood can be written as
p(D|Î¼) âˆexp
â›âœâœâœâœâœââˆ’1
2
N

n=1
(x(n) âˆ’Î¼)âŠ¤Î£âˆ’1(x(n) âˆ’Î¼)
ââŸâŸâŸâŸâŸâ 
âˆexp

âˆ’1
2
N

n=1

(x(n) âˆ’x) + (x âˆ’Î¼)
 âŠ¤Â· Î£âˆ’1 
(x(n) âˆ’x) + (x âˆ’Î¼)
 
= exp

âˆ’
1
2Ïƒ2

N

n=1
(x(n) âˆ’x)âŠ¤Î£âˆ’1(x(n) âˆ’x) + N(x âˆ’Î¼)âŠ¤Î£âˆ’1(x âˆ’Î¼)

âˆexp
!
âˆ’N
2 (Î¼ âˆ’x)âŠ¤Î£âˆ’1(Î¼ âˆ’x)
"
âˆGaussM

Î¼; x, 1
N Î£

.
(1.33)
Therefore, with the conjugate Gaussian prior
p(Î¼|Î¼0, Î£0) = GaussM(Î¼; Î¼0, Î£0) âˆexp

âˆ’1
2(Î¼ âˆ’Î¼0)âŠ¤Î£âˆ’1
0 (Î¼ âˆ’Î¼0)

,
with hyperparameters Îº = (Î¼0, Î£0), the posterior is written as
p(Î¼|D, Î¼0, Î£0) âˆp(D|Î¼)p(Î¼|Î¼0, Î£0)
âˆGaussM

Î¼; x, 1
N Î£

GaussM(Î¼; Î¼0, Î£0)
âˆexp
â›âœâœâœâœââˆ’N(Î¼ âˆ’x)âŠ¤Î£âˆ’1(Î¼ âˆ’x) + (Î¼ âˆ’Î¼0)âŠ¤Î£âˆ’1
0 (Î¼ âˆ’Î¼0)
2
ââŸâŸâŸâŸâ 
âˆexp
â›âœâœâœâœâœâœââˆ’
$Î¼ âˆ’Î¼%âŠ¤Î£
âˆ’1 $Î¼ âˆ’Î¼%
2
ââŸâŸâŸâŸâŸâŸâ ,
where
Î¼ =

NÎ£âˆ’1 + Î£âˆ’1
0
 âˆ’1 
NÎ£âˆ’1x + Î£âˆ’1
0 Î¼0
 
,
Î£ =

NÎ£âˆ’1 + Î£âˆ’1
0
 âˆ’1 .
Thus, we have
p(Î¼|D, Î¼0, Î£0) = GaussM

Î¼;Î¼, Î£
 
.
(1.34)

20
1 Bayesian Learning
Wishart Likelihood If we see the mean parameter Î¼ as a given constant,
the model likelihood (1.32) can be written as follows, as a function of the
covariance parameter Î£:
p(D|Î£âˆ’1) âˆdet

Î£âˆ’1 N/2 exp
â›âœâœâœâœââˆ’
N
n=1(x(n) âˆ’Î¼)âŠ¤Î£âˆ’1(x(n) âˆ’Î¼)
2
ââŸâŸâŸâŸâ 
âˆdet

Î£âˆ’1 N/2 exp
â›âœâœâœâœâœâœââˆ’
tr
N
n=1(x(n) âˆ’Î¼)(x(n) âˆ’Î¼)âŠ¤Î£âˆ’1 
2
ââŸâŸâŸâŸâŸâŸâ 
âˆWishartM
â›âœâœâœâœâœâœâœâÎ£âˆ’1;
â›âœâœâœâœâœâ
N

n=1
(x(n) âˆ’Î¼)(x(n) âˆ’Î¼)âŠ¤
ââŸâŸâŸâŸâŸâ 
âˆ’1
, M + N + 1
ââŸâŸâŸâŸâŸâŸâŸâ .
Here, as in the isotropic Gaussian case, we computed the distribution on the
inverse Î£âˆ’1 of the covariance parameter. With the Wishart distribution
p(Î£âˆ’1|V0, Î½0) = WishartM(Î£âˆ’1; V0, Î½0)
=
1
(2Î½0 det (V0))M/2 Î“M
 Î½0
2
 Â· det

Î£âˆ’1 Î½0âˆ’Mâˆ’1
2
exp
â›âœâœâœâœââˆ’tr(Vâˆ’1
0 Î£âˆ’1)
2
ââŸâŸâŸâŸâ 
for hyperparameters Îº
=
(V0, Î½0) as a conjugate prior, the posterior is
computed as
p(Î£âˆ’1|D, V0, Î½0) âˆp(D|Î£âˆ’1)p(Î£âˆ’1|V0, Î½0)
âˆWishartM
â›âœâœâœâœâœâœâœâÎ£âˆ’1;
â›âœâœâœâœâœâ
N

n=1
(x(n) âˆ’Î¼)(x(n) âˆ’Î¼)âŠ¤
ââŸâŸâŸâŸâŸâ 
âˆ’1
, M + N + 1
ââŸâŸâŸâŸâŸâŸâŸâ 
Â· WishartM(Î£âˆ’1; V0, Î½0)
âˆdet

Î£âˆ’1 N
2 exp
â›âœâœâœâœâœâœââˆ’
tr
N
n=1(x(n) âˆ’Î¼)(x(n) âˆ’Î¼)âŠ¤ 
Î£âˆ’1 
2
ââŸâŸâŸâŸâŸâŸâ 
Â· det

Î£âˆ’1 Î½0âˆ’Mâˆ’1
2
exp
â›âœâœâœâœâœâœââˆ’
tr

Vâˆ’1
0 Î£âˆ’1 
2
ââŸâŸâŸâŸâŸâŸâ 
âˆdet

Î£âˆ’1 Î½0âˆ’M+Nâˆ’1
2
exp
!
âˆ’
tr((N
n=1(x(n)âˆ’Î¼)(x(n)âˆ’Î¼)âŠ¤+Vâˆ’1
0 )Î£âˆ’1)
2
"
.
Thus we have
p(Î£âˆ’1|D, V0, Î½0)
= WishartM
â›âœâœâœâœâœâœâœâÎ£âˆ’1;
â›âœâœâœâœâœâ
N

n=1
(x(n) âˆ’Î¼)(x(n) âˆ’Î¼)âŠ¤+ Vâˆ’1
0
ââŸâŸâŸâŸâŸâ 
âˆ’1
, N + Î½0
ââŸâŸâŸâŸâŸâŸâŸâ . (1.35)

1.2 Computation
21
Note that the Wishart distribution can be seen as a multivariate extension of
the Gamma distribution and is reduced to the Gamma distribution for M = 1:
Wishart1 (x; V, Î½) = Gamma (x; Î½/2, 1/(2V)) .
Gauss-Wishart Likelihood When both parameters w
=
(Î¼, Î£âˆ’1) are
unknown, the model likelihood (1.32) is seen as
p(D|Î¼, Î£âˆ’1) âˆdet

Î£âˆ’1 N/2 exp
!
âˆ’
N
n=1(x(n)âˆ’Î¼)âŠ¤Î£âˆ’1(x(n)âˆ’Î¼)
2
"
âˆdet

Î£âˆ’1 N/2 exp
!
âˆ’
tr(N
n=1(x(n)âˆ’Î¼)(x(n)âˆ’Î¼)âŠ¤Î£âˆ’1)
2
"
âˆdet

Î£âˆ’1 N/2 exp

âˆ’
tr
N
n=1((x(n)âˆ’x)+(xâˆ’Î¼))((x(n)âˆ’x)+(xâˆ’Î¼))
âŠ¤Î£âˆ’1 
2

âˆdet

Î£âˆ’1 N/2 exp
!
âˆ’
tr(N(Î¼âˆ’x)(Î¼âˆ’x)âŠ¤+N
n=1(x(n)âˆ’x)(x(n)âˆ’x)âŠ¤)Î£âˆ’1)
2
"
âˆGaussWishartM

Î¼, Î£âˆ’1; x, N,
N
n=1(x(n) âˆ’x)(x(n) âˆ’x)âŠ¤ âˆ’1 , M + N
 
,
where
GaussWishartM(x, Î›|Î¼, Î», V, Î½)
â‰¡GaussM(x|Î¼, (Î»Î›)âˆ’1)WishartM(Î›|V, Î½)
=
exp

âˆ’Î»
2 (x âˆ’Î¼)âŠ¤Î› (x âˆ’Î¼)
 
(2Ï€)M/2det(Î»Î›)âˆ’1/2
Â·
det (Î›)
Î½âˆ’Mâˆ’1
2
exp
!
âˆ’tr(Vâˆ’1Î›)
2
"
(2Î½ det (V))M/2Î“M
 Î½
2
 
=
Î»M/2
(2Î½+1Ï€ det(V))M/2Î“M( Î½
2) det (Î›)
Î½âˆ’M
2 exp
!
âˆ’
tr((Î»(xâˆ’Î¼)(xâˆ’Î¼)âŠ¤+Vâˆ’1)Î›)
2
"
is the Gaussâ€“Wishart distribution on the random variables x âˆˆRM, Î› âˆˆSM
++
with parameters Î¼ âˆˆRM, Î» > 0, V âˆˆSM
++, Î½ > M âˆ’1.
With the conjugate Gaussâ€“Wishart prior,
p(Î¼, Î£âˆ’1|Î¼0, Î»0, Î±0, Î²0) = GaussWishartM(Î¼, Î£âˆ’1|Î¼0, Î»0, V0, Î½0)
âˆdet

Î£âˆ’1 Î½âˆ’M
2 exp

âˆ’
tr

(Î»0(Î¼âˆ’Î¼0)(Î¼âˆ’Î¼0)
âŠ¤+Vâˆ’1
0 )Î£âˆ’1 
2

with hyperparameters Îº = (Î¼0, Î»0, V0, Î½0), the posterior is written as
p(Î¼, Î£âˆ’1|D, Îº) âˆp(D|Î¼, Î£âˆ’1)p(Î¼, Î£âˆ’1|Îº)
âˆGaussWishartM

Î¼, Î£âˆ’1; x, N,
N
n=1(x(n) âˆ’x)(x(n) âˆ’x)âŠ¤ âˆ’1 , M + N
 
Â· GaussWishartM(Î¼, Î£âˆ’1|Î¼0, Î»0, V0, Î½0)

22
1 Bayesian Learning
âˆdet

Î£âˆ’1 N/2 exp
!
âˆ’
tr(N(Î¼âˆ’x)(Î¼âˆ’x)âŠ¤+N
n=1(x(n)âˆ’x)(x(n)âˆ’x)âŠ¤)Î£âˆ’1)
2
"
Â· det

Î£âˆ’1 Î½0âˆ’M
2
exp

âˆ’
tr

(Î»0(Î¼âˆ’Î¼0)(Î¼âˆ’Î¼0)
âŠ¤+Vâˆ’1
0 )Î£âˆ’1 
2

âˆdet

Î£âˆ’1 Î½âˆ’M
2 exp
â›âœâœâœâœâœâœâœâœâœâœâ
âˆ’tr
â›âœâœâœâœâœâœâœâœâœâœâ
!
Î» $Î¼ âˆ’Î¼% $Î¼ âˆ’Î¼%âŠ¤V
âˆ’1"
Î£âˆ’1
2
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
,
where
Î¼ = Nx + Î»0Î¼0
N + Î»0
,
Î» = N + Î»0,
V =
 N
n=1(x(n) âˆ’x)(x(n) âˆ’x)âŠ¤+
NÎ»0
N+Î»0 (x âˆ’Î¼0)(x âˆ’Î¼0)âŠ¤+ Vâˆ’1
0
 âˆ’1,
Î½ = N + Î½0.
Thus, we have the posterior distribution as the Gaussâ€“Wishart distribution:
p(Î¼, Î£âˆ’1|D, Îº) = GaussWishartM

Î¼, Î£âˆ’1|Î¼,Î», V,Î½
 
.
(1.36)
Linear Regression Model
Consider the linear regression model, where an input variable x âˆˆRM and
an output variable y âˆˆR are assumed to satisfy the following probabilistic
relation:
y = aâŠ¤x + Îµ,
(1.37)
p(Îµ|Ïƒ2) = Gauss1(Îµ; 0, Ïƒ2) =
1
âˆš
2Ï€Ïƒ2 Â· exp

âˆ’Îµ2
2Ïƒ2

.
(1.38)
Here a and Ïƒ2 are called the regression parameter and the noise variance
parameter, respectively. By substituting Îµ = y âˆ’aâŠ¤x, which is obtained from
Eq. (1.37), into Eq. (1.38), we have
p(y|x, w) = Gauss1(y; aâŠ¤x, Ïƒ2) =
1
âˆš
2Ï€Ïƒ2 Â· exp

âˆ’(y âˆ’aâŠ¤x)2
2Ïƒ2

.
The likelihood function for N observed i.i.d.2 samples,
D = (y, X),
2 In the context of regression, i.i.d. usually means that the observation noise Îµ(n) = y(n) âˆ’aâŠ¤x(n) is
independent for different samples, i.e., p({Îµ(n)}N
n=1) = N
n=1 p(Îµ(n)), and the independence
between the input (x(1),. . . , x(N)), i.e., p({x(n)}N
n=1) = N
n=1 p(x(n)), is not required.

1.2 Computation
23
is given by
p(D|w) =
1
(2Ï€Ïƒ2)N/2 Â· exp

âˆ’âˆ¥y âˆ’Xaâˆ¥2
2Ïƒ2

,
(1.39)
where we deï¬ned
y = (y(1),. . . , y(N))âŠ¤âˆˆRN,
X = (x(1),. . . , x(N))âŠ¤âˆˆRNÃ—M.
Gaussian Likelihood The computation of the posterior is similar to the
isotropic Gaussian case. As in Section 1.2.3, we ï¬rst consider the case where
only the regression parameter a is estimated, with the noise variance parameter
Ïƒ2 regarded as a known constant.
One can guess that the likelihood (1.39) is Gaussian as a function of a, since
it is an exponential of a concave quadratic function. Indeed, by expanding the
exponent and completing the square for a, we obtain
p(D|a) âˆexp

âˆ’âˆ¥y âˆ’Xaâˆ¥2
2Ïƒ2

âˆexp
!
âˆ’(aâˆ’(XâŠ¤X)âˆ’1XâŠ¤y)
âŠ¤XâŠ¤X(aâˆ’(XâŠ¤X)âˆ’1XâŠ¤y)
2Ïƒ2
"
âˆGaussM

a; (XâŠ¤X)âˆ’1XâŠ¤y, Ïƒ2(XâŠ¤X)âˆ’1 
.
(1.40)
Eq. (1.40) implies that, when XâŠ¤X is nonsingular (i.e., its inverse exists), the
ML estimator for a is given by
aML = (XâŠ¤X)âˆ’1XâŠ¤y.
(1.41)
Therefore, with the conjugate Gaussian prior
p(a|a0, Î£0) = GaussM(a; a0, Î£0) âˆexp

âˆ’1
2(a âˆ’a0)âŠ¤Î£âˆ’1
0 (a âˆ’a0)

for hyperparameters Îº = (a0, Î£0), the posterior is Gaussian:
p(a|D, a0, Î£0) âˆp(D|a)p(a|a0, Î£0)
âˆGaussM

a; a0, 1
N Ïƒ2(XâŠ¤X)âˆ’1 
GaussM(a; a0, Î£0)
âˆexp
â›âœâœâœâœâœâœââˆ’
(aâˆ’(XâŠ¤X)âˆ’1 XâŠ¤y)
âŠ¤XâŠ¤X(aâˆ’(XâŠ¤X)âˆ’1 XâŠ¤y)
Ïƒ2
+(aâˆ’a0)âŠ¤Î£âˆ’1
0 (aâˆ’a0)
2
ââŸâŸâŸâŸâŸâŸâ 
âˆexp
â›âœâœâœâœâœâœââˆ’
$a âˆ’a%âŠ¤Î£
âˆ’1
a
$a âˆ’a%
2
ââŸâŸâŸâŸâŸâŸâ ,

24
1 Bayesian Learning
where
a =
 XâŠ¤X
Ïƒ2
+ Î£âˆ’1
0
âˆ’1  XâŠ¤y
Ïƒ2 + Î£âˆ’1
0 a0

,
Î£a =
 XâŠ¤X
Ïƒ2
+ Î£âˆ’1
0
âˆ’1
.
Thus we have
p(a|D, a0, Î£0) = GaussM

a;a, Î£a
 
.
(1.42)
Gamma Likelihood When only the noise variance parameter Ïƒ2 is unknown,
the model likelihood (1.39) is in the Gamma form, as a function of the
inverse Ïƒâˆ’2:
p(D|Ïƒâˆ’2) âˆ(Ïƒâˆ’2)NM/2 exp

âˆ’âˆ¥y âˆ’Xaâˆ¥2
2
Ïƒâˆ’2

âˆGamma

Ïƒâˆ’2; NM
2
+ 1, âˆ¥y âˆ’Xaâˆ¥2
2

,
(1.43)
which implies that the ML estimator is
Ïƒ2 ML =
1
Ïƒâˆ’2 ML =
1
MN
N

n=1
âˆ¥y âˆ’Xaâˆ¥2 .
With the conjugate Gamma prior
p(Ïƒâˆ’2|Î±0, Î²0) = Gamma(Ïƒâˆ’2; Î±0, Î²0) âˆ(Ïƒâˆ’2)Î±0âˆ’1 exp(âˆ’Î²0Ïƒâˆ’2)
with hyperparameters Îº = (Î±0, Î²0), the posterior is computed as
p(Ïƒâˆ’2|D, Î±0, Î²0) âˆp(D|Ïƒâˆ’2)p(Ïƒâˆ’2|Î±0, Î²0)
âˆGamma

Ïƒâˆ’2; MN
2
+ 1, 1
2 âˆ¥y âˆ’Xaâˆ¥2

Gamma(Ïƒâˆ’2; Î±0, Î²0)
âˆ(Ïƒâˆ’2)MN/2+Î±0âˆ’1 exp

âˆ’
1
2 âˆ¥y âˆ’Xaâˆ¥2 + Î²0

Ïƒâˆ’2

.
Therefore,
p(Ïƒâˆ’2|D, Î±0, Î²0) = Gamma

Ïƒâˆ’2; MN
2
+ Î±0, 1
2 âˆ¥y âˆ’Xaâˆ¥2 + Î²0

.
(1.44)
Gauss-Gamma Likelihood When we estimate both parameters w = (a, Ïƒâˆ’2),
the likelihood (1.39) is written as

1.2 Computation
25
p(D|a, Ïƒâˆ’2) âˆ(Ïƒâˆ’2)NM/2 exp
!
âˆ’âˆ¥yâˆ’Xaâˆ¥2
2
Ïƒâˆ’2
"
âˆ(Ïƒâˆ’2)NM/2 exp

âˆ’

aâˆ’aML âŠ¤XâŠ¤X

aâˆ’aML 
+âˆ¥yâˆ’XaMLâˆ¥2
2
Ïƒâˆ’2

âˆGaussGammaM
!
a, Ïƒâˆ’2;aML, XâŠ¤X, M(Nâˆ’1)
2
+ 1, âˆ¥yâˆ’XaMLâˆ¥2
2
"
,
where aML is the ML estimator, given by Eq. (1.41), for the regression
parameter, and
GaussGammaM(x, Ï„|Î¼, Î›, Î±, Î²)
â‰¡GaussM(x|Î¼, (Ï„Î›)âˆ’1) Â· Gamma(Ï„|Î±, Î²)
=
exp(âˆ’Ï„
2 (xâˆ’Î¼)âŠ¤Î›(xâˆ’Î¼))
(2Ï€Ï„âˆ’1)M/2 det(Î›)âˆ’1/2 Â·
Î²Î±
Î“(Î±) Ï„Î±âˆ’1 exp(âˆ’Î²Ï„)
=
Î²Î±
(2Ï€)M/2 det(Î›)âˆ’1/2Î“(Î±) Ï„Î±+ M
2 âˆ’1 exp

âˆ’
 (xâˆ’Î¼)âŠ¤Î›(xâˆ’Î¼)
2
+ Î²
 
Ï„
 
is the (general) Gauss-Gamma distribution on the random variable x âˆˆRM,
Ï„ > 0 with parameters Î¼ âˆˆRM, Î› âˆˆSM
++, Î± > 0, Î² > 0. With the conjugate
Gauss-Gamma prior
p(a, Ïƒâˆ’2|Îº) = GaussGammaM(a, Ïƒâˆ’2|Î¼0, Î›0, Î±0, Î²0)
âˆ(Ïƒâˆ’2)Î±0+ M
2 âˆ’1 exp

âˆ’
 (aâˆ’Î¼0)âŠ¤Î›0(aâˆ’Î¼0)
2
+ Î²0
 
Ïƒâˆ’2 
for hyperparameters Îº = (Î¼0, Î›0, Î±0, Î²0), the posterior is computed as
p(a, Ïƒâˆ’2|D, Îº) âˆp(D|a, Ïƒâˆ’2)p(a, Ïƒâˆ’2|Îº)
âˆGaussGammaM
!
a, Ïƒâˆ’2;aML, XâŠ¤X, M(Nâˆ’1)
2
+ 1, âˆ¥yâˆ’XaMLâˆ¥2
2
"
Â· GaussGammaM(a, Ïƒâˆ’2|Î¼0, Î›0, Î±0, Î²0)
âˆ(Ïƒâˆ’2)NM/2 exp

âˆ’

aâˆ’aML âŠ¤XâŠ¤X

aâˆ’aML 
+âˆ¥yâˆ’XaMLâˆ¥2
2
Ïƒâˆ’2

Â· (Ïƒâˆ’2)Î±0+ M
2 âˆ’1 exp

âˆ’
 (aâˆ’Î¼0)âŠ¤Î›0(aâˆ’Î¼0)
2
+ Î²0
 
Ïƒâˆ’2 
âˆ(Ïƒâˆ’2)Î±+ M
2 âˆ’1 exp
â›âœâœâœâœââˆ’
â›âœâœâœâœâ
$a âˆ’Î¼%âŠ¤Î› $a âˆ’Î¼%
2
+ Î²
ââŸâŸâŸâŸâ Ïƒâˆ’2
ââŸâŸâŸâŸâ ,
where
Î¼ = (XâŠ¤X + Î›0)âˆ’1 
XâŠ¤XaML + Î›0Î¼0
 
,
Î› = XâŠ¤X + Î›0,
Î± = NM
2 + Î±0,
Î² = âˆ¥yâˆ’XaMLâˆ¥2
2
+ (aMLâˆ’Î¼0)âŠ¤Î›0(XâŠ¤X+Î›0)âˆ’1XâŠ¤X(aMLâˆ’Î¼0)
2
+ Î²0.

26
1 Bayesian Learning
Thus, we obtain
p(a, Ïƒâˆ’2|D, Îº) = GaussGammaM(a, Ïƒâˆ’2|Î¼,Î›,Î±,Î²).
(1.45)
Multinomial Model
The multinomial distribution, which expresses a distribution over the his-
tograms of independent events, is another frequently used basic component
in Bayesian modeling. For example, it appears in mixture models and latent
Dirichlet allocation.
Assume that exclusive K events occur with the probability
Î¸ = (Î¸1,. . . , Î¸K) âˆˆÎ”Kâˆ’1 â‰¡
â§âªâªâ¨âªâªâ©Î¸ âˆˆRK; 0 â‰¤Î¸k â‰¤1,
K

k=1
Î¸k = 1
â«âªâªâ¬âªâªâ­.
Then, the histogram
x = (x1,. . . , xK) âˆˆHKâˆ’1
N
â‰¡
â§âªâªâ¨âªâªâ©x âˆˆIK; 0 â‰¤xk â‰¤N;
K

k=1
xk = N
â«âªâªâ¬âªâªâ­
of events after N iterations follows the multinomial distribution, deï¬ned as
p(x|Î¸) = MultinomialK,N(x; Î¸) â‰¡N! Â·
K

k=1
Î¸xk
k
xk!.
(1.46)
Î¸ is called the multinomial parameter.
As seen shortly, calculation of the posterior with its conjugate prior is
surprisingly easy.
Dirichlet Likelihood As a function of the multinomial parameter w = Î¸, it is
easy to ï¬nd that the likelihood (1.46) is in the form of the Dirichlet distribution:
p(x|Î¸) âˆDirichletK(Î¸; x + 1K),
where 1K is the K-dimensional vector with all elements equal to 1. Since
the Dirichlet distribution is an exponential family and hence multiplicatively
closed, it is conjugate for the multinomial parameter. With the conjugate
Dirichlet prior
p(Î¸|Ï†) = DirichletK(Î¸; Ï†) âˆ
K

k=1
Î¸Ï†âˆ’1
k
with hyperparameters Îº = Ï†, the posterior is computed as

1.2 Computation
27
p(Î¸|x, Ï†) âˆp(x|Î¸)p(Î¸|Ï†)
âˆDirichletK(Î¸; x + 1K) Â· DirichletK(Î¸; Ï†)
âˆ
K

k=1
Î¸xk
k Â· Î¸Ï†kâˆ’1
k
âˆ
K

k=1
Î¸xk+Ï†kâˆ’1
k
.
Thus we have
p(Î¸|x, Ï†) = DirichletK(Î¸; x + Ï†).
(1.47)
Special Cases For K = 2, the multinomial distribution is reduced to the
binomial distribution:
p(x1|Î¸1) = Multinomial2,N

(x1, N âˆ’x1)âŠ¤; (Î¸1, 1 âˆ’Î¸1)âŠ¤ 
= BinomialN(x1; Î¸1)
=
!N
x1
"
Â· Î¸x1
1 (1 âˆ’Î¸1)Nâˆ’x1.
Furthermore, it is reduced to the Bernoulli distribution for K = 2 and N = 1:
p(x1|Î¸1) = Binomial1(x1; Î¸1)
= Î¸x1
1 (1 âˆ’Î¸1)1âˆ’x1.
Similarly, its conjugate Dirichlet distribution for K = 2 is reduced to the
Beta distribution:
p(Î¸1|Ï†1, Ï†2) = Dirichlet2

(Î¸1, 1 âˆ’Î¸1)âŠ¤; (Ï†1, Ï†2)âŠ¤ 
= Beta(Î¸1; Ï†1, Ï†2)
=
1
B(Ï†1, Ï†2) Â· Î¸Ï†1âˆ’1
1
(1 âˆ’Î¸1)Ï†2âˆ’1,
where B(Ï†1, Ï†2) =
Î“(Ï†1)Î“(Ï†2)
Î“(Ï†1+Ï†2) is the Beta function. Naturally, the Beta distri-
bution is conjugate to the binomial and the Bernoulli distributions, and the
posterior can be computed as easily as for the multinomial case.
With a conjugate prior in the form of a popular distribution, the four quan-
tities introduced in Section 1.1.3, i.e., the marginal likelihood, the posterior
mean, the posterior covariance, and the predictive distribution, can be obtained
analytically. In the following subsections, we show how they are obtained.

28
1 Bayesian Learning
Table 1.2 First and second moments of common distributions.
Mean(x) = âŸ¨xâŸ©p(x|w), Var(x) =

(x âˆ’Mean(x))2
p(x|w),
Cov(x) = (x âˆ’Mean(x))(x âˆ’Mean(x))âŠ¤
p(x|w), Î¨(z) â‰¡d
dzlog Î“(z) :
Digamma function, and Î¨m(z) â‰¡dm
dzm Î¨(z): Polygamma function of order m.
p(x|w)
First moment
Second moment
GaussM(x; Î¼, Î£)
Mean(x) = Î¼
Cov(x) = Î£
Gamma(x; Î±, Î²)
Mean(x) = Î±
Î²
Var(x) =
Î±
Î²2
Mean(log x)
Var(log x) = Î¨1(Î±)
= Î¨(Î±) âˆ’log Î²
WishartM(X; V, Î½)
Mean(X) = Î½V
Var(xm,mâ€²) = Î½(V2
m,mâ€² + Vm,mVmâ€²,mâ€²)
MultinomialK,N(x; Î¸)
Mean(x) = NÎ¸
(Cov(x))k,kâ€² =
)NÎ¸k(1 âˆ’Î¸k)
(k = kâ€²)
âˆ’NÎ¸kÎ¸kâ€²
(k  kâ€²)
DirichletK(x; Ï†)
Mean(x) =
1
K
k=1 Ï†k Ï†
(Cov(x))k,kâ€² =
â§âªâªâ¨âªâªâ©
Ï†k(Ï„âˆ’Ï†k)
Ï„2(Ï„+1)
(k = kâ€²)
âˆ’Ï†kÏ†kâ€²
Ï„2(Ï„+1)
(k  kâ€²)
Mean(log xk)
where
Ï„ = K
k=1 Ï†k
= Î¨(Ï†k) âˆ’Î¨(K
kâ€²=1 Ï†kâ€²)
1.2.4 Posterior Mean and Covariance
As seen in Section 1.2.3, by adopting a conjugate prior having a form of one
of the common family distributions, such as the one in Table 1.1, we can have
the posterior distribution in the same common family.3 In such cases, we can
simply use the known form of moments, which are summarized in Table 1.2.
For example, the posterior (1.42) for the regression parameter a (when the
noise variance Ïƒ2 is treated as a known constant) is the following Gaussian
distribution:
p(a|D, a0, Î£0) = GaussM

a;a, Î£a
 
,
where
a =
 XâŠ¤X
Ïƒ2
+ Î£âˆ’1
0
âˆ’1  XâŠ¤y
Ïƒ2 + Î£âˆ’1
0 a0

,
Î£a =
 XâŠ¤X
Ïƒ2
+ Î£âˆ’1
0
âˆ’1
.
3 If we would say that the prior is in the family that contains all possible distributions, this family
would be the conjugate prior for any likelihood function, which is however useless. Usually, the
notion of the conjugate prior implicitly requires that moments (at least the normalization
constant and the ï¬rst moment) of any family member can be computed analytically.

1.2 Computation
29
Therefore, the posterior mean and the posterior covariance are simply given by
âŸ¨aâŸ©p(a|D,a0,Î£0) = a,

(a âˆ’âŸ¨aâŸ©)(a âˆ’âŸ¨aâŸ©)âŠ¤
p(a|D,a0,Î£0) = Î£a,
respectively. The posterior (1.29) of the (inverse) variance parameter Ïƒâˆ’2 of
the isotropic Gaussian distribution (when the mean parameter Î¼ is treated as a
known constant) is the following Gamma distribution:
p(Ïƒâˆ’2|D, Î±0, Î²0) = Gamma
â›âœâœâœâœâœâÏƒâˆ’2; MN
2
+ Î±0, 1
2
N

n=1
âˆ¥x(n) âˆ’Î¼âˆ¥2 + Î²0
ââŸâŸâŸâŸâŸâ .
Therefore, the posterior mean and the posterior variance are given by

Ïƒâˆ’2
p(Ïƒâˆ’2|D,Î±0,Î²0) =
MN
2 + Î±0
1
2
N
n=1 âˆ¥x(n) âˆ’Î¼âˆ¥2 + Î²0
,
*
Ïƒâˆ’2 âˆ’

Ïƒâˆ’2 2+
p(Ïƒâˆ’2|D,Î±0,Î²0) =
MN
2 + Î±0
( 1
2
N
n=1 âˆ¥x(n) âˆ’Î¼âˆ¥2 + Î²0)2 ,
respectively.
Also in other cases, the posterior mean and the posterior covariances can be
easily computed by using Table 1.2, if the form of the posterior distribution is
in the table.
1.2.5 Predictive Distribution
The predictive distribution (1.9) for a new data set Dnew can be computed
analytically, if the posterior distribution is in the exponential family, and hence
multiplicatively closed. In this section, we show two examplary cases, the
linear regression model and the multinomial model.
Linear Regression Model
Consider the linear regression model:
p(y|x, a) = Gauss1(y; aâŠ¤x, Ïƒ2) =
1
âˆš
2Ï€Ïƒ2 Â· exp

âˆ’(y âˆ’aâŠ¤x)2
2Ïƒ2

,
(1.48)
where only the regression parameter is unknown, i.e., w = a âˆˆRM, and the
noise variance parameter Ïƒ2 is treated as a known constant. We choose the
zero-mean Gaussian as a conjugate prior:
p(a|C) = GaussM(a; 0, C) =
exp

âˆ’1
2 aâŠ¤Câˆ’1a
 
(2Ï€)M/2 det (C)1/2 ,
(1.49)
where C is the prior covariance.

30
1 Bayesian Learning
When N i.i.d. samples D = (X, y), where
y = (y(1),. . . , y(N))âŠ¤âˆˆRN,
X = (x(1),. . . , x(N))âŠ¤âˆˆRNÃ—M,
are observed, the posterior is given by
p(a|y, X, C) = GaussM

a;a, Î£a
 
=
1
(2Ï€)M/2det
Î£a
 1/2 Â· exp
â›âœâœâœâœâœâœââˆ’
$a âˆ’a%âŠ¤Î£
âˆ’1
a
$a âˆ’a%
2
ââŸâŸâŸâŸâŸâŸâ ,
(1.50)
where
a =
 XâŠ¤X
Ïƒ2
+ Câˆ’1
âˆ’1 XâŠ¤y
Ïƒ2
= Î£a
XâŠ¤y
Ïƒ2 ,
(1.51)
Î£a =
 XâŠ¤X
Ïƒ2
+ Câˆ’1
âˆ’1
.
(1.52)
This is just a special case of the posterior (1.42) for the linear regression model
with the most general Gaussian prior.
Now, let us compute the predictive distribution on the output yâˆ—for a
new given input xâˆ—. As deï¬ned in Eq. (1.9), the predictive distribution is the
expectation value of the model distribution (1.48) (for a new inputâ€“output pair)
over the posterior distribution (1.50):
p(yâˆ—|xâˆ—, y, X, C) = âŸ¨p(yâˆ—|xâˆ—, a)âŸ©p(a|y,X,C)
=

p(yâˆ—|xâˆ—, a)p(a|y, X, C)da
=

Gauss1(yâˆ—; aâŠ¤xâˆ—, Ïƒ2)GaussM

a;a, Î£a
 
da
âˆ

exp

âˆ’(yâˆ—âˆ’aâŠ¤xâˆ—)2
2Ïƒ2
âˆ’(aâˆ’a)
âŠ¤Î£
âˆ’1
a (aâˆ’a)
2

da
âˆexp
!
âˆ’yâˆ—2
2Ïƒ2
" 
exp
â›âœâœâœâœâœââˆ’
aâŠ¤
!
Î£
âˆ’1
a + xâˆ—xâˆ—âŠ¤
Ïƒ2
"
aâˆ’2aâŠ¤
!
Î£
âˆ’1
a a+ xâˆ—yâˆ—
Ïƒ2
"
2
ââŸâŸâŸâŸâŸâ da
âˆexp
â›âœâœâœâœâœâœââˆ’
Ïƒâˆ’2yâˆ—2âˆ’
!
Î£
âˆ’1
a a+ xâˆ—yâˆ—
Ïƒ2
"âŠ¤!
Î£
âˆ’1
a + xâˆ—xâˆ—âŠ¤
Ïƒ2
"âˆ’1!
Î£
âˆ’1
a a+ xâˆ—yâˆ—
Ïƒ2
"
2
ââŸâŸâŸâŸâŸâŸâ 
Â·

exp
â›âœâœâœâœâœââˆ’
(aâˆ’Ë˜a)âŠ¤!
Î£
âˆ’1
a + xâˆ—xâˆ—âŠ¤
Ïƒ2
"
(aâˆ’Ë˜a)
2
ââŸâŸâŸâŸâŸâ da,
(1.53)
where
Ë˜a =

Î£
âˆ’1
a + xâˆ—xâˆ—âŠ¤
Ïƒ2
âˆ’1 
Î£
âˆ’1
a a + xâˆ—yâˆ—
Ïƒ2

.

1.2 Computation
31
Note that, although the preceding computation is similar to the one for the
posterior distribution in Section 1.2.3, any factor that depends on yâˆ—cannot
be ignored even if it does not depend on a, since the goal is to obtain the
distribution on yâˆ—.
The integrand in Eq. (1.53) coincides with the main part of
GaussM
â›âœâœâœâœâœâa; Ë˜a,

Î£
âˆ’1
a + xâˆ—xâˆ—âŠ¤
Ïƒ2
âˆ’1ââŸâŸâŸâŸâŸâ 
without the normalization factor. Therefore, the integral is the inverse of the
normalization factor, i.e.,

exp
â›âœâœâœâœâœââˆ’
(aâˆ’Ë˜a)âŠ¤!
Î£
âˆ’1
a + xâˆ—xâˆ—âŠ¤
Ïƒ2
"
(aâˆ’Ë˜a)
2
ââŸâŸâŸâŸâŸâ da = (2Ï€)M/2det
!
Î£
âˆ’1
a + xâˆ—xâˆ—âŠ¤
Ïƒ2
"âˆ’1/2
,
which is a constant with respect to yâˆ—. Therefore, by using Eqs. (1.51) and
(1.52), we have
p(yâˆ—|xâˆ—, y, X, C)
âˆexp
â›âœâœâœâœâœâœââˆ’
Ïƒâˆ’2yâˆ—2âˆ’
!
Î£
âˆ’1
a a+ xâˆ—yâˆ—
Ïƒ2
"âŠ¤!
Î£
âˆ’1
a + xâˆ—xâˆ—âŠ¤
Ïƒ2
"âˆ’1!
Î£
âˆ’1
a a+ xâˆ—yâˆ—
Ïƒ2
"
2
ââŸâŸâŸâŸâŸâŸâ 
âˆexp

âˆ’
yâˆ—2âˆ’(XâŠ¤y+xâˆ—yâˆ—)
âŠ¤(XâŠ¤X+xâˆ—xâˆ—âŠ¤+Ïƒ2Câˆ’1)
âˆ’1(XâŠ¤y+xâˆ—yâˆ—)
2Ïƒ2

âˆexp

âˆ’
1
2Ïƒ2
,
yâˆ—2
1 âˆ’xâˆ—âŠ¤
XâŠ¤X + xâˆ—xâˆ—âŠ¤+ Ïƒ2Câˆ’1 âˆ’1 xâˆ— 
âˆ’2yâˆ—xâˆ—âŠ¤
XâŠ¤X + xâˆ—xâˆ—âŠ¤+ Ïƒ2Câˆ’1 âˆ’1 XâŠ¤y
-
âˆexp

âˆ’
1âˆ’xâˆ—âŠ¤(XâŠ¤X+xâˆ—xâˆ—âŠ¤+Ïƒ2Câˆ’1)
âˆ’1xâˆ—
2Ïƒ2
Â·

yâˆ—âˆ’
xâˆ—âŠ¤(XâŠ¤X+xâˆ—xâˆ—âŠ¤+Ïƒ2Câˆ’1)
âˆ’1XâŠ¤y
1âˆ’xâˆ—âŠ¤(XâŠ¤X+xâˆ—xâˆ—âŠ¤+Ïƒ2Câˆ’1)
âˆ’1xâˆ—
2 
âˆexp
â›âœâœâœâœââˆ’(yâˆ—âˆ’y)2
2Ïƒ2y
ââŸâŸâŸâŸâ ,
where
y =
xâˆ—âŠ¤
XâŠ¤X + xâˆ—xâˆ—âŠ¤+ Ïƒ2Câˆ’1 âˆ’1 XâŠ¤y
1 âˆ’xâˆ—âŠ¤
XâŠ¤X + xâˆ—xâˆ—âŠ¤+ Ïƒ2Câˆ’1 âˆ’1 xâˆ—
,
Ïƒ2
y =
Ïƒ2
1 âˆ’xâˆ—âŠ¤
XâŠ¤X + xâˆ—xâˆ—âŠ¤+ Ïƒ2Câˆ’1 âˆ’1 xâˆ—
.

32
1 Bayesian Learning
â€“4
â€“2
0
2
4
â€“4
â€“2
0
2
4
True
Estimated
Credible interval
Figure 1.3 Predictive distribution of the linear regression model.
Thus, the predictive distribution has been analytically obtained:
p(yâˆ—|xâˆ—, y, X, C) = Gauss1

yâˆ—;y, Ïƒ2
y
 
.
(1.54)
Figure 1.3 shows an example of the predictive distribution of the linear
regression model. The curve labeled as â€œTrueâ€ indicates the mean y = aâˆ—x
of the true regression model y = aâˆ—x + Îµ, where aâˆ—= (âˆ’2, 0.4, 0.3, âˆ’0.1)âŠ¤,
x = (1, t, t2, t3)âŠ¤, and Îµ âˆ¼Gauss1(0, 12). The crosses are N = 30 i.i.d.
observed samples generated from the true regression model and the input
distribution t âˆ¼Uniform(âˆ’2.4, 1.6), where Uniform(l, u) denotes the uniform
distribution on [l, u]. The regression model (1.48) with the prior (1.49) for
the hyperparameters C = 10000 Â· IM, Ïƒ2 = 1 was trained with the observed
samples. The curve labeled as â€œEstimatedâ€ and the pair of curves labeled as
â€œCredible intervalâ€ show the mean y and the credible interval y Â± Ïƒy of the
predictive distribution (1.54), respectively.
Reï¬‚ecting the fact that the samples are observed only in the middle region
(t âˆˆ[âˆ’2.4, 1.6]), the credible interval is large in outer regions. The larger
interval implies that the â€œEstimatedâ€ function is less reliable, and we see that
the gap from the â€œTrueâ€ function is indeed large. Since the true function is
unknown in practical situations, the variance of the predictive distribution is
important information on the reliability of the estimated result.
Multinomial Model
Let us compute the predictive distribution of the multinomial model:
p(x|Î¸) = MultinomialK,N(x; Î¸) âˆ
K

k=1
Î¸xk
k
xk!,

1.2 Computation
33
p(Î¸|Ï†) = DirichletK(Î¸; Ï†) âˆ
K

k=1
Î¸Ï†kâˆ’1
k
,
with the observed data D = x = (x1,. . . , xK) âˆˆHKâˆ’1
N
and the unknown
parameter w = Î¸ = (Î¸1,. . . , Î¸K) âˆˆÎ”Kâˆ’1.
The posterior was derived in Eq. (1.47):
p(Î¸|x, Ï†) = DirichletK(Î¸; x + Ï†) âˆ
K

k=1
Î¸xk+Ï†kâˆ’1
k
.
Therefore, the predictive distribution for a new single sample xâˆ—âˆˆHKâˆ’1
1
is
given by
p(xâˆ—|x, Ï†) = âŸ¨p(xâˆ—|Î¸)âŸ©p(Î¸|x,Ï†)
=

p(xâˆ—|Î¸)p(Î¸|x, Ï†)dÎ¸
=

MultinomialK,1(xâˆ—; Î¸)DirichletK(Î¸; x + Ï†)dÎ¸
âˆ

K

k=1
Î¸
xâˆ—
k
k Â· Î¸xk+Ï†kâˆ’1
k
dÎ¸
=

K

k=1
Î¸
xâˆ—
k+xk+Ï†kâˆ’1
k
dÎ¸.
(1.55)
In the fourth equation, we ignored the factors that depend neither on xâˆ—
nor on Î¸.
The integrand in Eq. (1.55) is the main part of DirichletK(Î¸; xâˆ—+ x+Ï†), and
therefore, the integral is equal to the inverse of its normalization factor:

K

k=1
Î¸
xâˆ—
k+xk+Ï†kâˆ’1
k
dÎ¸ =
K
k=1 Î“(xâˆ—
k + xk + Ï†k)
Î“(K
k=1 xâˆ—
k + xk + Ï†k)
=
K
k=1 Î“(xâˆ—
k + xk + Ï†k)
Î“(N + K
k=1 Ï†k + 1)
.
Thus, by using the identity Î“(x + 1) = xÎ“(x) for the Gamma function, we have
p(xâˆ—|x, Ï†) âˆ
K

k=1
Î“(xâˆ—
k + xk + Ï†k)
âˆ
K

k=1
(xk + Ï†k)xâˆ—
kÎ“(xk + Ï†k)

34
1 Bayesian Learning
âˆ
K

k=1
(xk + Ï†k)xâˆ—
k
âˆ
K

k=1
â›âœâœâœâœâ
xk + Ï†k
K
kâ€²=1 xkâ€² + Ï† kâ€²
ââŸâŸâŸâŸâ 
xâˆ—
k
= MultinomialK,1(xâˆ—;Î¸),
(1.56)
where
Î¸k =
xk + Ï†k
K
kâ€²=1 xkâ€² + Ï†kâ€² .
(1.57)
From Eq. (1.47) and Table 1.2, we can easily see that the predictive mean
Î¸, speciï¬ed by Eq. (1.57), coincides with the posterior mean, i.e., the Bayesian
estimator:
Î¸ = âŸ¨Î¸âŸ©DirichletK(Î¸;x+Ï†) .
Therefore, in the multinomial model, the predictive distribution coincides with
the model distribution with the Bayesian estimator plugged in.
In the preceding derivation, we performed the integral computation and
derived the form of the predictive distribution. However, the necessary infor-
mation to determine the predictive distribution is the probability table on the
events xâˆ—âˆˆHKâˆ’1
1
= {ek}K
k=1, of which the degree of freedom is only K.
Therefore, the following simple calculation gives the same result:
Prob(xâˆ—= ek|x, Ï†) = MultinomialK,1(ek; Î¸)
DirichletK(Î¸;x+Ï†)
= âŸ¨Î¸kâŸ©DirichletK(Î¸;x+Ï†)
= Î¸k,
which speciï¬es the function form of the predictive distribution, given by
Eq. (1.56).
1.2.6 Marginal Likelihood
Let us compute the marginal likelihood of the linear regression model, deï¬ned
by Eqs. (1.48) and (1.49):
p(D|C) = p(y|X, C)
= âŸ¨p(y|X, a)âŸ©p(a|C)
=

p(y|X, a)p(a|C)da

1.2 Computation
35
=

GaussN(y; Xa, Ïƒ2IN)GaussM(a; 0, C)da
=
 exp
!
âˆ’âˆ¥yâˆ’Xaâˆ¥2
2Ïƒ2
"
(2Ï€Ïƒ2)N/2
Â·
exp

âˆ’1
2 aâŠ¤Câˆ’1a
 
(2Ï€)M/2 det (C)1/2 da
=
exp
!
âˆ’âˆ¥yâˆ¥2
2Ïƒ2
"
(2Ï€Ïƒ2)N/2(2Ï€)M/2 det (C)1/2
Â·

exp
â›âœâœâœâœâœâœâœââˆ’
âˆ’2aâŠ¤XâŠ¤y
Ïƒ2 + aâŠ¤ XâŠ¤X
Ïƒ2 + Câˆ’1 
a
2
ââŸâŸâŸâŸâŸâŸâŸâ da
=
exp
!
âˆ’1
2
!
âˆ¥yâˆ¥2
Ïƒ2 âˆ’aâŠ¤Î£
âˆ’1
a a
""
(2Ï€Ïƒ2)N/2(2Ï€)M/2 det (C)1/2
Â·

exp
â›âœâœâœâœâœâœââˆ’
$a âˆ’a%âŠ¤Î£
âˆ’1
a
$a âˆ’a%
2
ââŸâŸâŸâŸâŸâŸâ da,
(1.58)
where a and Î£a are, respectively, the posterior mean and the posterior
covariance, given by Eqs. (1.51) and (1.52).
By using

exp
â›âœâœâœâœâœâœââˆ’
$a âˆ’a%âŠ¤Î£
âˆ’1
a
$a âˆ’a%
2
ââŸâŸâŸâŸâŸâŸâ da =
.
(2Ï€)Mdet
Î£a
 
,
and Eq. (1.58), we have
p(y|X, C) =
exp
!
âˆ’1
2
!
âˆ¥yâˆ¥2
Ïƒ2 âˆ’yâŠ¤XÎ£aXâŠ¤y
Ïƒ4
""
(2Ï€Ïƒ2)N/2(2Ï€)M/2 det (C)1/2
.
(2Ï€)Mdet
Î£a
 
=
exp

âˆ’
âˆ¥yâˆ¥2âˆ’yâŠ¤X(XâŠ¤X+Ïƒ2Câˆ’1)
âˆ’1XâŠ¤y
2Ïƒ2

(2Ï€Ïƒ2)N/2det(CXâŠ¤X + Ïƒ2IM)1/2 ,
(1.59)
where we also used Eqs. (1.51) and (1.52).
Eq. (1.59) is an explicit expression of the marginal likelihood as a function
of the hyperparameter Îº = C. Based on it, we perform EBayes learning in
Section 1.2.7.
1.2.7 Empirical Bayesian Learning
In empirical Bayesian (EBayes) learning, the hyperparameter Îº is estimated
by maximizing the marginal likelihood p(D|Îº). The negative logarithm of the
marginal likelihood,

36
1 Bayesian Learning
FBayes = âˆ’log p(D|Îº),
(1.60)
is called the Bayes free energy or stochastic complexity.4 Since log(Â·) is
a monotonic function, maximizing the marginal likelihood is equivalent to
minimizing the Bayes free energy.
Eq. (1.59) implies that the Bayes free energy of the linear regression model
is given by
2FBayes = âˆ’2 log p(y|X, C)
= N log(2Ï€Ïƒ2) + log det(CXâŠ¤X + Ïƒ2IM)
+
âˆ¥yâˆ¥2 âˆ’yâŠ¤X

XâŠ¤X + Ïƒ2Câˆ’1 âˆ’1 XâŠ¤y
Ïƒ2
.
(1.61)
Let us restrict the prior covariance to be diagonal:
C = Diag(c2
1,. . . , c2
M) âˆˆDM.
(1.62)
The prior (1.49) with diagonal covariance (1.62) is called the automatic
relevance determination (ARD) prior, which is known to make the EBayes
estimator sparse (Neal, 1996). In the following example, we see this effect by
setting the design matrix to identity, X = IM, which enables us to derive the
EBayes solution analytically.
Under the identity design matrix, the Bayes free energy (1.61) can be
decomposed as
2FBayes = N log(2Ï€Ïƒ2) + log det(C + Ïƒ2IM) +
âˆ¥yâˆ¥2 âˆ’yâŠ¤
IM + Ïƒ2Câˆ’1 âˆ’1 y
Ïƒ2
= N log(2Ï€Ïƒ2) + âˆ¥yâˆ¥2
Ïƒ2 +
M

m=1

log(c2
m + Ïƒ2) âˆ’
y2
m
Ïƒ2 $1 + Ïƒ2câˆ’2
m
%

=
M

m=1
2Fâˆ—
m + const.,
(1.63)
where
2Fâˆ—
m = log

1 + c2
m
Ïƒ2

âˆ’y2
m
Ïƒ2

1 + Ïƒ2
c2m
âˆ’1
.
(1.64)
In Eq. (1.63), we omitted the constant factors with respect to the hyperpa-
rameter C. As the remaining terms are decomposed into each component m,
we can independently minimize Fâˆ—
m with respect to c2
m.
4 The logarithm of the marginal likelihood log p(D|Îº) is called the log marginal likelihood or
evidence.

1.2 Computation
37
0
1
2
3
â€“0.5
0
0.5
1
Figure 1.4 The (componentwise) Bayes free energy (1.64) of linear regression
model with the ARD prior. The minimizer is shown as a cross if it lies in the
positive region of c2
m/Ïƒ2.
The derivative of Eq. (1.64) with respect to c2
m is
2âˆ‚Fâˆ—
m
âˆ‚c2m
=
1
c2m + Ïƒ2 âˆ’
y2
m
$1 + Ïƒ2câˆ’2
m
%2 c4m
=
1
c2m + Ïƒ2 âˆ’
y2
m
$c2m + Ïƒ2%2
= c2
m âˆ’(y2
m âˆ’Ïƒ2)
(c2m + Ïƒ2)2
.
(1.65)
Eq. (1.65) implies that Fâˆ—
m is monotonically increasing over all domain c2
m > 0
when y2
m â‰¤Ïƒ2, and has the unique minimizer in the region c2
m > 0 when
y2
m > Ïƒ2. Speciï¬cally, the minimizer is given by
c2
m =
â§âªâªâ¨âªâªâ©
y2
m âˆ’Ïƒ2
if y2
m > Ïƒ2,
+0
otherwise.
(1.66)
Figure 1.4 shows the (componentwise) Bayes free energy (1.64) for dif-
ferent observations, y2
m = 0, Ïƒ2, 1.5Ïƒ2, 2Ïƒ2. The minimizer is in the positive
region of c2
m if and only if y2
m > Ïƒ2.
If the EBayes estimator is given by c2
m â†’+0, it means that the prior
distribution for the mth component am of the regression parameter is the Dirac
delta function located at the origin.5 This formally means that we a priori
5 When y2
m â‰¤Ïƒ2, the Bayes free energy (1.64) decreases as c2
m approaches to 0. However, the
domain of c2
m is restricted to be positive, and therefore,c2
m = 0 is not the solution. We express
this solution asc2
m â†’+0.

38
1 Bayesian Learning
knew that am = 0, i.e., we choose a model that does not contain the mth
component.
By substituting Eq. (1.66) into the Bayes posterior mean (1.51), we obtain
the EBayes estimator:
aEBayes
m
= c2
m

c2
m + Ïƒ2 âˆ’1 ym
=
â§âªâªâªâ¨âªâªâªâ©
!
1 âˆ’Ïƒ2
y2m
"
ym
if y2
m > Ïƒ2,
0
otherwise.
(1.67)
The form of the estimator (1.67) is called the Jamesâ€“Stein (JS) estimator
having interesting properties including the domination over the ML esti-
mator (Stein, 1956; James and Stein, 1961; Efron and Morris, 1973) (see
Appendix A).
Note that the assumption that X = IM is not practical. For a general design
matrix X, the Bayes free energy is not decomposable into each component.
Consequently, the prior variances {c2
m}M
m=1 that minimize the Bayes free energy
(1.61) interact with each other. Therefore, the preceding simple mechanism is
not applied. However, it is empirically observed that many prior variances tend
to go toc2
m â†’+0, so that the EBayes estimator aEBayes is sparse.

2
Variational Bayesian Learning
In Chapter 1, we saw examples where the model likelihood has a conjugate
prior, with which Bayesian learning can be performed analytically. However,
many practical models do not have conjugate priors. Even in such cases,
the notion of conjugacy is still useful. Speciï¬cally, we can make use of
the conditional conjugacy, which comes from the fact that many practical
models are built by combining basic distributions. In this chapter, we introduce
variational Bayesian (VB) learning, which makes use of the conditional
conjugacy, and approximates the Bayes posterior by solving a constrained
minimization problem.
2.1 Framework
VB learning is derived by casting Bayesian learning as an optimization
problem with respect to the posterior distribution (Hinton and van Camp,
1993; MacKay, 1995; Opper and Winther, 1996; Attias, 1999; Jordan et al.,
1999; Jaakkola and Jordan, 2000; Ghahramani and Beal, 2001; Bishop, 2006;
Wainwright and Jordan, 2008).
2.1.1 Free Energy Minimization
Let r(w), or r for short, be an arbitrary distribution, which we call a trial
distribution, on the parameter w, and consider the Kullbackâ€“Leibler (KL)
divergence from the trial distribution r(w) to the Bayes posterior p(w|D):
KL (r(w)âˆ¥p(w|D)) =

r(w) log
r(w)
p(w|D)dw =
/
log
r(w)
p(w|D)
0
r(w)
.
(2.1)
39

40
2 Variational Bayesian Learning
Since the KL divergence is equal to zero if and only if the two distributions
coincide with each other, the minimizer of Eq. (2.1) is the Bayes posterior, i.e.,
p(w|D) = argmin
r
KL (r(w)âˆ¥p(w|D)) .
(2.2)
The problem (2.2) is equivalent to the following problem:
p(w|D) = argmin
r
F(r),
(2.3)
where the functional of r,
F(r) =

r(w) log
r(w)
p(w, D)dw =
/
log
r(w)
p(w, D)
0
r(w)
(2.4)
= KL (r(w)âˆ¥p(w|D)) âˆ’log p(D),
(2.5)
is called the free energy. Intuitively, we replaced the posterior distribution
p(w|D) in the KL divergence (2.1) with its unnormalized versionâ€”the joint
distribution p(D, w) = p(w|D)p(D)â€”in the free energy (2.4). The equivalence
holds because the normalization factor p(D) does not depend on w, and
therefore log p(D)
r(w) = log p(D) does not depend on r. Note that the free
energy (2.4) is a generalization of the Bayes free energy, deï¬ned by Eq. (1.60):
The free energy (2.4) is a functional of an arbitrary distribution r, and equal
to the Bayes free energy (1.60) for the Bayes poterior r(w) = p(w|D). Since
the KL divergence is nonnegative, Eq. (2.5) implies that the free energy F(r)
is an upper-bound of the Bayes free energy âˆ’log p(D) for any distribution r.
Since the log marginal likelihood log p(D) is called the evidence, âˆ’F(r) is also
called the evidence lower-bound (ELBO).
As mentioned in Section 1.1.2, the joint distribution is easy to compute
in general. However, the minimization problem in Eq. (2.3) can still be
computationally intractable, because the objective functional (2.4) involves the
expectation over the distribution r(w). Actually, it can be hard to even evaluate
the objective functional for most of the possible distributions. To make the
evaluation of the objective functional tractable for optimal r(w), we restrict the
search space to G. Namely, we solve the following problem:
min
r
F(r)
s.t.
r âˆˆG,
(2.6)
where s.t. is an abbreviation for â€œsubject to.â€
We can choose a tractable distribution class directly for G, e.g., Gaussian,
such that the expectation for evaluating the free energy is tractable for any
r âˆˆG. However, in many practical models, a weaker constraint restricts the
optimal distribution to be in a tractable class, thanks to conditional conjugacy.

2.1 Framework
41
2.1.2 Conditional Conjugacy
Let us consider a few examples where the model likelihood has no conjugate
prior. The likelihood of the matrix factorization model (which will be discussed
in detail in Section 3.1) is given by
p(V|A, B) =
exp
!
âˆ’1
2Ïƒ2
###V âˆ’BAâŠ¤###2
Fro
"
(2Ï€Ïƒ2)LM/2
,
(2.7)
where V âˆˆRLÃ—M is an observed random variable, and A âˆˆRMÃ—H and B âˆˆRLÃ—H
are the parameters to be estimated. Although Ïƒ2 âˆˆR++ can also be unknown,
let us treat it as a hyperparameter, i.e., a constant when computing the posterior
distribution.
If we see Eq. (2.7) as a function of the parameters w = (A, B), its
function form is the exponential of a polynomial including a fourth-order
term
###BAâŠ¤###2
Fro = tr(BAâŠ¤ABâŠ¤). Therefore, no conjugate prior exists for this
likelihood with respect to the parameters w = (A, B).1
The next example is a mixture of Gaussians (which will be discussed in
detail in Section 4.1.1):
p(D, H|w) =
N

n=1
K

k=1
â§âªâªâªâªâªâ¨âªâªâªâªâªâ©
Î±k
exp
!
âˆ’âˆ¥x(n)âˆ’Î¼kâˆ¥
2
2Ïƒ2
"
(2Ï€Ïƒ2)M/2
â«âªâªâªâªâªâ¬âªâªâªâªâªâ­
z(n)
k
,
(2.8)
where D = {x(n)}N
n=1 are observed data, H = {z(n)}N
n=1 are hidden variables, and
w = (Î±, {Î¼k}K
k=1) are parameters. For simplicity, we here assume that all Gaus-
sian components have the same variance Ïƒ2, which is treated as a hyperparam-
eter, i.e., we compute the joint posterior distribution of the hidden variables
{z(n)}N
n=1 and the parameters w, regarding the hyperparameter Ïƒ2 as a constant.
If we see Eq. (2.8) as a function of ({z(n)}N
n=1, Î±, {Î¼k}K
k=1), no conjugate
prior exists. More speciï¬cally, it has a factor N
n=1
K
k=1 Î±kz(n)
k , and we cannot
compute

z(n)
k âˆˆ{ek}K
k=1

N

n=1
K

k=1
Î±k
z(n)
k dÎ±k
analytically for general N, which is required when evaluating moments.
1 Here, â€œno conjugate priorâ€ means that there is no useful and nonconditional conjugate prior,
such that the posterior is in the same distribution family with computable moments. We might
say that the exponential function of fourth-order polynomials is conjugate to the likelihood
(2.7), since the posterior is within the same family. However, this statement is useless in
practice because we cannot compute moments of the distribution analytically.

42
2 Variational Bayesian Learning
The same difï¬culty happens in the latent Dirichlet allocation model (which
will be discussed in detail in Section 4.2.4). The likelihood is written as
p(D, H|w) =
M

m=1
N(m)

n=1
H

h=1
â§âªâªâ¨âªâªâ©Î˜m,h
L

l=1
Bl,h
w(n,m)
l
â«âªâªâ¬âªâªâ­
z(n,m)
h
,
(2.9)
where D = {{w(n,m)}N(m)
n=1 }M
m=1 are observed data, H = {{z(n,m)}N(m)
n=1 }M
m=1 are hidden
variables, and w = (Î˜, B) are parameters to be estimated. Computing the
sum (over the hidden variables H) of the integral (over the parameters w) is
intractable for practical problem sizes.
Readers might ï¬nd that Eqs. (2.7), (2.8), and (2.9) are not much more
complicated than the conjugate cases: Eq. (2.7) is similar to the Gaussian
form, and Eqs. (2.8) and (2.9) are in the form of the multinomial or Dirichlet
distribution, where we have unknowns both in the base and in the exponent.
Indeed, they are in a known form if we regard a part of unknowns as ï¬xed
constants.
The likelihood (2.7) of the matrix factorization model is in the Gaussian
form of A if we see B as a constant, or vice versa. The likelihood (2.8) of a
mixture of Gaussians is in the multinomial form of the hidden variables H =
{z(n)}N
n=1 if we see the parameters w = (Î±, {Î¼k}K
k=1) as constants, and it is the
(independent) product of the Dirichlet form of Î± and the Gaussian form of
{Î¼k}K
k=1 if we see the hidden variables H = {z(n)}N
n=1 as constants. Similarly, the
likelihood (2.9) of the latent Dirichlet allocation model is in the multinomial
form of the hidden variables H = {{z(n,m)}N(m)
n=1 }M
m=1 if we see the parameters
w = (Î˜, B) as constants, and it is the product of the Dirichlet form of the row
vectors {Î¸m}M
m=1 of Î˜ and the Dirichlet form of the column vectors {Î²h}H
h=1 of B
if we see the hidden variables H = {{z(n,m)}N(m)
n=1 }M
m=1 as constants.
Since the likelihoods in the Gaussian, multinomial, and Dirichlet forms
have conjugate priors, the aforementioned properties can be described with
the notion of conditional conjugacy, which is deï¬ned as follows:
Deï¬nition 2.1
(Conditionally conjugate prior) Let us divide the unknown
parameters w (or more generally all unknown variables including hidden
variables) into two parts w = (w1, w2). If the posterior of w1,
p(w1|w2, D) âˆp(D|w1, w2)p(w1),
(2.10)
is in the same distribution family as the prior p(w1) (where w2 is regarded as a
given constant or condition), the prior p(w1) is called a conditionally conjugate
prior of the model likelihood p(D|w) with respect to the parameter w1, given
the ï¬xed parameter w2.

2.1 Framework
43
2.1.3 Constraint Design
Once conditional conjugacy for all unknowns is found, designing tractable VB
learning is straightforward.
Let us divide the unknown parameters w into S groups, i.e., w
=
(w1,. . . , wS ), such that, for each s
=
1,. . . , S , the model likelihood
p(D|w) = p(D|ws, {wsâ€²}sâ€²s) has a conditionally conjugate prior p(ws) with
respect to ws, given {wsâ€²}sâ€²s as ï¬xed constants. Then, if we use the prior
p(w) =
S
s=1
p(ws),
(2.11)
the posterior distribution
p(w|D) âˆp(D|w)p(w)
is, as a function of ws, in the same distribution family as the prior p(ws).
Therefore, moments of the posterior distribution are tractable, if the other
parameters {wsâ€²}sâ€²s are given.
To make use of this property, we impose on the approximate posterior the
independence constraint between the parameter groups,
r(w) =
S
s=1
rs(ws),
(2.12)
which allows us to compute moments with respect to ws independently from
the other parameters {wsâ€²}sâ€²s. In VB learning, we solve the minimization
problem (2.6) under the constraint (2.12). This makes the expectation com-
putation, which is required in evaluating the free energy (2.4), tractable (on
any stationary points for r). Namely, we deï¬ne the VB posterior as
r = argmin
r
F(r)
s.t.
r(w) =
S
s=1
rs(ws).
(2.13)
Note that it is not guaranteed that the free energy F(r) =

log
r(w)
p(D|w)p(w)

r(w)
is tractable for any r satisfying the constraint (2.12). However, the constraint
allows us to optimize each factor {rs}S
s=1 separately. To optimize each factor,
we rely on calculus of variations, which will be explained in Section 2.1.4.
By applying calculus of variations, the free energy is expressed as an explicit
function with a ï¬nite number of unknown parameters.

44
2 Variational Bayesian Learning
2.1.4 Calculus of Variations
Calculus of variations is a method, developed in physics, to derive conditions
that any optimal function minimizing a (smooth) functional should satisfy
(Courant and Hilbert, 1953). Speciï¬cally, it gives (inï¬nitely many) stationary
conditions of the functional with respect to the variable.
The change of the functional F(r) with respect to an inï¬nitesimal change of
the variable r (which is a function of w) is called a variation and written as Î´I.
For r to be a stationary point of the functional, the variation must be equal to
zero for all possible values of w. Since the free energy (2.4) does not depend
on the derivatives of r(w), the variation Î´I is simply the derivative with respect
to r. Therefore, the stationary conditions are given by
Î´I = âˆ‚F
âˆ‚r = 0,
âˆ€w âˆˆW,
(2.14)
which is a special case of the Eulerâ€“Lagrange equation. If we see the function
r(w) as a (possibly) inï¬nite-dimensional vector with the parameter value w as
its index, the variation Î´I = Î´I(w) can be interpreted as the gradient of the
functional F(r) in the |W|-dimensional space. As the stationary conditions in
a ï¬nite-dimensional space require that all entries of the gradient equal to zero,
the optimal function r(w) should satisfy Eq. (2.14) for any parameter values
w âˆˆW.
In Section 2.1.5, we see that, by applying the stationary conditions (2.14) to
the free energy minimization problem (2.13) with the independence constraint
taken into account, we can ï¬nd that each factor rs(ws) of the approximate
posterior is in the same distribution family as the corresponding prior ps(ws),
thanks to the conditional conjugacy.
2.1.5 Variational Bayesian Learning
Let us solve the problem (2.13) to get the VB posterior
r = argmin
r
F(r)
s.t.
r(w) =
S
s=1
rs(ws).
We use the decomposable conditionally conjugate prior (2.11):
p(w) =
S
s=1
p(ws),
which means that, for each s = 1,. . . , S , the posterior p(ws|{wsâ€²}sâ€²s, D) for ws
is in the same form as the corresponding prior p(ws), given {wsâ€²}sâ€²s as ï¬xed
constants.

2.1 Framework
45
Now we apply the calculus of variations, and compute the stationary
conditions (2.14). The free energy can be written as
F(r) =
 â›âœâœâœâœâœâ
S
s=1
rs(ws)
ââŸâŸâŸâŸâŸâ 
â›âœâœâœâœâlog
S
s=1 rs(ws)
p(D|w) S
s=1 p(ws)
ââŸâŸâŸâŸâ dw.
(2.15)
Taking the derivative of Eq. (2.15) with respect to rs(ws) for any s = 1,. . . , S
and ws âˆˆW, we obtain the following stationary conditions:
0 = âˆ‚F
âˆ‚rs
=
 â›âœâœâœâœâœâ

sâ€²s
rsâ€²(wsâ€²)
ââŸâŸâŸâŸâŸâ 
â›âœâœâœâœâlog
S
sâ€²=1 rsâ€²(wsâ€²)
p(D|w) S
sâ€²=1 p(wsâ€²)
+ 1
ââŸâŸâŸâŸâ dw
=
/
log
S
sâ€²=1 rsâ€²(wsâ€²)
p(D|w) S
sâ€²=1 p(wsâ€²)
0

sâ€²s rsâ€²(wsâ€²)
+ 1
=
/
log

sâ€²s rsâ€²(wsâ€²)
p(D|w) 
sâ€²s p(wsâ€²)
0

sâ€²s rsâ€²(wsâ€²)
+ log rs(ws)
p(ws) + 1
=
/
log
1
p(D|w)
0

sâ€²s rsâ€²(wsâ€²)
+ log rs(ws)
p(ws) + const.
(2.16)
Note the following on Eq. (2.16):
â€¢ The right-hand side is a function of ws (wsâ€² for sâ€²  s are integrated out).
â€¢ For each s, Eq. (2.16) must hold for any possible value of ws, which can
fully specify the function form of the posterior rs(ws).
â€¢ To make Eq. (2.16) satisï¬ed for any ws, it is necessary that
âˆ’log p(D|w)
sâ€²s rsâ€²(wsâ€²) + log rs(ws)
p(ws)
is a constant.
The last note leads to the following relation:
rs(ws) âˆp(ws) exp log p(D|w)
sâ€²s rsâ€²(wsâ€²) .
(2.17)
As a function of ws, Eq. (2.17) can be written as
rs(ws) âˆexp log p(D|w)p(ws)
sâ€²s rsâ€²(wsâ€²)
âˆexp log p(ws|{wsâ€²}sâ€²s, D)
sâ€²s rsâ€²(wsâ€²)
âˆexp

log p(ws|{wsâ€²}sâ€²s, D)

sâ€²s
rsâ€²(wsâ€²)dwsâ€².
(2.18)
Due to the conditional conjugacy, p(ws|{wsâ€²}sâ€²s, D) is in the same form as the
prior p(ws). As the intergral operator g(x) =

f(x; Î±)dÎ± can be interpreted as
an inï¬nite number of additions of parametric functions f(x; Î±) over all possible
values of Î±, the operator h(x) = exp

log f(x; Î±)dÎ± corresponds to an inï¬nite

46
2 Variational Bayesian Learning
number of multiplications of f(x; Î±) over all possible values of Î±. Therefore,
Eq. (2.18) implies that the VB posterior rs(ws) is in the same form as the prior
p(ws), if the distribution family is multiplicatively closed.
Assume that the prior p(ws) for each group of parameters is in a multi-
plicatively closed distribution family. Then, we may express the corresponding
VB posterior rs(ws) in a parametric form, of which the parameters are called
variational parameters, without any loss of accuracy or optimality. The last
question is whether we can compute the expectation value of the log-likelihood
log p(D|w) for each factor rs(ws) of the approximate posterior. In many cases,
this expectation can be computed analytically, which allows us to express the
stationary conditions (2.17) as a ï¬nite number of equations in explicit forms of
the variational parameters.
Typically, the obtained stationary conditions are used to update the varia-
tional parameters in an iterative algorithm, which gives a local minimizerr of
the free energy (2.4). We call the minimizerr the VB posterior, and its mean
w = âŸ¨wâŸ©r(w)
(2.19)
the VB estimator.
The computation of predictive distribution
p(Dnew|D) = p(Dnew|w)
r(w)
can be hard even after ï¬nding the VB posteriorr(w). This is natural because we
need approximation for the function form of the likelihood p(D|w), and now
we need to compute the integral with the integrand involving the same function
form. In many practical cases, the plug-in predictive distribution p(Dnew|w),
i.e., the model distribution with the VB estimator plugged in, is substituted for
the predictive distribution.
2.1.6 Empirical Variational Bayesian Learning
When the model involves hyperparameters Îº in the likelihood and/or in the
prior, the joint distribution is dependent on Îº, i.e.,
p(D, w|Îº) = p(w|Îº)p(D|w, Îº),
and so is the free energy:
F(r, Îº) =

r(w) log
r(w)
p(D, w|Îº)dw
=
/
log
r(w)
p(w, D|Îº)
0
r(w)
(2.20)
= KL (r(w)âˆ¥p(w|D, Îº)) âˆ’log p(D|Îº).
(2.21)

2.1 Framework
47
Similarly to the empirical Bayesian learning, the hyperparameters can be
estimated from observation by minimizing the free energy simultaneously with
respect to r and Îº:
(r,Îº) = argmin
r,Îº
F(r, Îº).
This approach is called the empirical VB (EVB) learning.
EVB learning amounts to minimizing the sum of the KL divergence to the
Bayes posterior and the marginal likelihood (see Eq. (2.21)). Conceptually,
minimizing any weighted sum of those two terms is reasonable to ï¬nd the VB
posterior and the hyperparameters at the same time. But only the unweighted
sum makes the objective tractableâ€”under this choice, the objective is written
with the joint distribution as in Eq. (2.20), while any other choice requires
explicitly accessing the Bayes posterior and the marginal likelihood separately.
2.1.7 Techniques for Nonconjugate Models
In Sections 2.1.2 through 2.1.5, we saw how to design tractable VB learning by
making use of the conditional conjugacy. However, there are also many cases
where a reasonable model does not have a conditionally conjugate prior. A
frequent and important example is the case where the likelihood involves the
sigmoid function,
Ïƒ(x; w) =
1
1 + eâˆ’wâŠ¤x ,
(2.22)
or a function with a similar shape, e.g., the error function, the hyperbolic
tangent, and the rectiï¬ed linear unit (ReLU). We face such cases, for example,
in solving classiï¬cation problems and in adopting neural network structure
with a nonlinear activation function.
To maintain the tractability in such cases, we need to explicitly restrict the
function form of the approximate posterior r(w;Î»), and optimize its variational
parameters Î» by free energy minimization. Namely, we solve the VB learning
problem (2.6) with the search space G set to the function space of a simple
distribution family, e.g., the Gaussian distribution r(w;Î») = GaussD(w; w, Î£)
parameterized with the variational parameters Î» = (w, Î£) consisting of the
mean and the covariance parameters. Then, the VB learning problem (2.6) is
reduced to the following unconstrained minimization problem,
min
Î»
F(Î»),
(2.23)

48
2 Variational Bayesian Learning
of the free energy
F(Î») =

r(w;Î») log r(w;Î»)
p(w, D)dw =
/
log r(w;Î»)
p(w, D)
0
r(w;Î»)
,
(2.24)
which is a function of the variational parameters Î».
It is often the case that the free energy (2.24) is still intractable in
computing the expectation value of the log joint probability, log p(w, D) =
log p(D|w)p(w), over the approximate posterior r(w;Î») (because of the
intractable function form of the likelihood p(D|w) or the prior p(w)). In
this section, we introduce a few techniques developed for coping with such
intractable functions.
Local Variational Approximation
The ï¬rst method is to bound the joint distribution p(w, D) with a simple
function, of which the expectation value over the approximate distribution
r(w;Î») is tractable.
As seen in Section 2.1.1, the free energy (2.24) is an upper-bound of the
Bayes free energy, FBayes â‰¡âˆ’log p(D), for any Î». Consider further upper-
bounding the free energy as
F(Î») â‰¤F(Î», Î¾) â‰¡

r(w;Î») log r(w;Î»)
p(w; Î¾)dw
(2.25)
by replacing the joint distribution p(w, D) with its parametric lower-bound
p(w; Î¾) such that
0 â‰¤p(w; Î¾) â‰¤p(w, D)
(2.26)
for any w âˆˆW and Î¾ âˆˆÎ. Here, we introduced another set of variational
parameters Î¾ with its domain Î. Let us choose a lower-bound p(w; Î¾) such that
its function form with respect to w is the same as the approximate posterior
r(w;Î»). More speciï¬cally, we assume that, for any given Î¾, there exists Î»
such that
p(w; Î¾) âˆr(w;Î»)
(2.27)
as a function of w.2 Since the direct minimization of F(Î») is intractable, we
instead minimize its upper-bound F(Î», Î¾) jointly over Î» and Î¾. Namely, we
solve the problem
min
Î»,Î¾
F(Î», Î¾),
(2.28)
2 The parameterization, i.e., the function form with respect to the variational parameters, can be
different between p(w; Î¾) and r(w;Î»).

2.1 Framework
49
to ï¬nd the approximate posterior r(w;Î») such that F(Î», Î¾) (â‰¥F(Î»)) is closest
to the Bayes free energy FBayes (when Î¾ is also optimized).
Let
q(w; Î¾) =
p(w; Î¾)
Z(Î¾)
(2.29)
be the distribution created by normalizing the lower-bound with its normaliza-
tion factor
Z(Î¾) =

p(w; Î¾)dw.
(2.30)
Note that the normalization factor (2.30) is trivially a lower-bound of the
marginal likelihood, i.e.,
Z(Î¾) â‰¤

p(w, D)dw = p(D),
and is tractable because of the assumption (2.27) that p is in the same simple
function form as r.
With Eq. (2.29), the upper-bound (2.25) is expressed as
F(Î», Î¾) =

r(w;Î») log r(w;Î»)
q(w; Î¾)dw âˆ’log Z(Î¾)
= KL

r(w;Î»)âˆ¥q(w; Î¾)
 
âˆ’log Z(Î¾),
(2.31)
which implies that the optimal Î» is attained when
r(w;Î») = q(w; Î¾)
(2.32)
for any Î¾ âˆˆÎ (the assumption (2.27) guarantees the attainability). Thus, by
putting this back into Eq. (2.31), the problem (2.28) is reduced to
max
Î¾
Z(Î¾),
(2.33)
which amounts to maximizing the lower-bound (2.30) of the marginal likeli-
hood p(D). Once the maximizer Î¾ is obtained, Eq. (2.32) gives the optimal
approximate posterior.
Such an approximation scheme for nonconjugate models is called local
variational approximation or direct site bounding (Jaakkola and Jordan, 2000;
Girolami, 2001; Bishop, 2006; Seeger, 2008, 2009), which will be discussed
further with concrete examples in Chapter 5. Existing nonconjugate models
applied with the local variational approximation form the bound in Eq. (2.26)
based on the convexity of a function. In such a case, the gap between
F(Î¾) and F turns out to be the expected Bregman divergence associated
with the convex function (see Section 5.3.1). A similar approach can be

50
2 Variational Bayesian Learning
applied to expectation propagation, another approximation method introduced
in Section 2.2.3. There, by upper-bounding the joint probability p(w, D), we
minimize an upper-bound of KL

p(w|D)âˆ¥r(w;Î»)
 
(see Section 2.2.3).
Black Box Variational Inference
As the available data size increases, and the beneï¬t of using big data has
been proven, for example, by the breakthrough in deep learning (Krizhevsky
et al., 2012), scalable training algorithms have been intensively developed, to
enable big data analysis on billions of data samples. The stochastic gradient
descent (Robbins and Monro, 1951; Spall, 2003), where a noisy gradient of the
objective function is cheaply computed from a subset of the whole data in each
iteration, has become popular, and has been adopted for VB learning (Hoffman
et al., 2013; Khan et al., 2016).
The black-box variational inference was proposed as a general method
to compute a noisy gradient of the free energy in nonconjugate models
(Ranganath et al., 2013; Wingate and Weber, 2013; Kingma and Welling,
2014). As a function of the variational parameters Î», the gradient of the free
energy (2.24) can be evaluated by
âˆ‚F
âˆ‚Î»
= âˆ‚
âˆ‚Î»

r(w;Î») log r(w;Î»)
p(D, w)dw
=
 âˆ‚r(w;Î»)
âˆ‚Î»
log r(w;Î»)
p(D, w)dw +

r(w;Î») âˆ‚
âˆ‚Î»

log r(w;Î»)
 
dw
=

r(w;Î»)âˆ‚log r(w;Î»)
âˆ‚Î»
log r(w;Î»)
p(D, w)dw + âˆ‚
âˆ‚Î»

r(w;Î»)dw
=
/âˆ‚log r(w;Î»)
âˆ‚Î»
log r(w;Î»)
p(D, w)
0
r(w;Î»)
.
(2.34)
Assume that we restrict the approximate posterior r(w;Î») to be in a
simple distribution family, from which samples can be easily drawn, and
its score function, the gradient of the log probability, is easily computed,
e.g., an analytic form is available. Then, Eq. (2.34) can be easily computed
by drawing samples from r(w;Î»), and computing the sample average. With
some variance reduction techniques, the stochastic gradient with the black box
gradient estimator (2.34) has shown to be useful for VB learning in general
nonconjugate models. A notable advantage is that it does not require any model
speciï¬c analysis to implement the gradient estimation, since Eq. (2.34) can be
evaluated as long as the log joint probability p(D, w) = p(D|w)p(w) of the
model can be evaluated for drawn samples of w.

2.2 Other Approximation Methods
51
2.2 Other Approximation Methods
There are several other methods for approximate Bayesian learning, which are
brieï¬‚y introduced in this section.
2.2.1 Laplace Approximation
In the Laplace approximation, the posterior is approximated by a Gaussian:
r(w) = GaussD(w; w, Î£).
VB learning ï¬nds the variational parametersÎ» = (w, Î£) by minimizing the free
energy (2.4), i.e., solving the problem (2.6) with the search space G restricted
to the Gaussian distributions. Instead, the Laplace approximation estimates the
mean and the covariance by
wLA(= wMAP) = argmax
w
p(D|w)p(w),
(2.35)
Î£
LA = F
âˆ’1,
(2.36)
where the entries of F âˆˆSD
++ are given by
Fi, j = âˆ’âˆ‚2 log p(D|w)p(w)
âˆ‚wiâˆ‚wj
w=wLA.
(2.37)
Namely, the Laplace approximation ï¬rst ï¬nds the MAP estimator for the
mean, and then computes Eq.(2.37) at w = wLA to estimate the inverse
covariance, which corresponds to the second-order Taylor approximation to
log p(D|w)p(w). Note that, for the ï¬‚at prior p(w) âˆ1, Eq. (2.37) is reduced to
the Fisher information:
Fi, j =
/âˆ‚log p(D|w)
âˆ‚wi
âˆ‚log p(D|w)
âˆ‚wj
0
p(D|w)
= âˆ’
/âˆ‚2 log p(D|w)
âˆ‚wiâˆ‚wj
0
p(D|w)
.
In general, the Laplace approximation is computationally less demanding
than VB learning, since no integral computation is involved, and the inverse
covariance estimation (2.36) is performed only once after the MAP mean
estimator (2.35) is found.
2.2.2 Partially Bayesian Learning
Partially Bayesian (PB) learning is MAP learning after some of the unknown
parameters are integrated out. This approach can be described in the free
energy minimization framework (2.6) with a strnger constraint than VB
learning.

52
2 Variational Bayesian Learning
Let us split the unknown parameters w into two parts w = (w1, w2), and
assume that we integrate w1 out and point-estimate w2. Integrating w1 out
means that we consider the exact posterior on w1, and MAP estimating w2
means that we approximate the posterior w2 with the delta function. Namely,
PB learning solves the following problem:
min
r
F(r)
s.t.
r(w) = r1(w1) Â· Î´(w2; w2),
(2.38)
where the free energy F(r) is deï¬ned by Eq. (2.4), and Î´(w; w) is the Dirac
delta function located at w.
Using the constraint in Eq. (2.38), under which the variables to be optimized
are r1 and w2, we can express the free energy as
F(r1, w2) =
/
log
r1(w1) Â· Î´(w2; w2)
p(D|w1, w2)p(w1)p(w2)
0
r1(w1)Â·Î´(w2;w2)
=
/
log
r1(w1)
p(D|w1, w2)p(w1)p(w2)
0
r1(w1)
+ log Î´(w2; w2)
Î´(w2;w2)
=
/
log
r1(w1)
p(w1|w2, D)
0
r1(w1)
âˆ’log p(D|w2)p(w2) + log Î´(w2; w2)
Î´(w2;w2) ,
(2.39)
where
p(D|w2) = p(D|w1, w2)
p(w1) =

p(D|w1, w2)p(w1)dw1.
(2.40)
The free energy (2.39) depends on r1 only through the ï¬rst term, which is
the KL divergence, KL $r1(w1)âˆ¥p(w1|w2, D)%, from the trial distribution to the
Bayes posterior (conditioned on w2). Therefore, the minimizer for r1 is trivially
the conditional Bayes posterior
r1(w1) = p(w1|w2, D),
(2.41)
with which the ï¬rst term in Eq. (2.39) vanishes. The third term in Eq. (2.39) is
the entropy of the delta function, which diverges to inï¬nity but is independent
of w2. By regarding the delta function as a distribution with its width narrow
enough to express a point estimate, while its entropy is ï¬nite (although it is
very large), we can ignore the third term. Thus, the free energy minimization
problem (2.38) can be written as
min
w2
âˆ’log p(D|w2)p(w2),
(2.42)
which amounts to MAP learning for w2 after w1 is marginalized out.

2.2 Other Approximation Methods
53
This method is computationally beneï¬cial when the likelihood p(D|w) =
p(D|w1, w2) is conditionally conjugate to the prior p(w1) with respect to w1,
given w2. Thanks to the conditional conjugacy, the posterior (2.41) of w1 is
in a known form, and its normalization factor (2.40), which is required when
evaluating the objective in Eq. (2.42), can be obtained analytically.
PB learning was applied in many previous works. For example, in the
expectation-maximization (EM) algorithm (Dempster et al., 1977), latent
variables are integrated out and parameters are point-estimated. In the ï¬rst
probabilisitic interpretation of principal component analysis (PCA) (Tipping
and Bishop, 1999), one factor of the matrix factorization was called a latent
variable and integrated out, while the other factor was called a parameter and
point-estimated.
The same idea has been adopted for Gibbs sampling and VB learning, where
some of the unknown parameters are integrated out based on the conditional
conjugacy, and the other parameters are estimated by the corresponding learn-
ing method. Those methods are called collapsed Gibbs sampling (Grifï¬ths and
Steyvers, 2004) and collapsed VB learning (Kurihara et al., 2007; Teh et al.,
2007; Sato et al., 2012), respectively. Following this terminology, PB learning
may be also called collapsed MAP learning. The collapsed version is in
general more accurate and more computationally efï¬cient than the uncollapsed
counterpart, since it imposes a weaker constraint and applies a nonexact
numerical estimation to a smaller number of unknowns.
2.2.3 Expectation Propagation
As explained in Section 2.1.1, VB learning amounts to minimizing the KL
divergence KL (r(w)âˆ¥p(w|D)) from the approximate posterior to the Bayes
posterior. Expectation propagation (EP) is an alternative deterministic approx-
imation scheme, which minimizes the KL divergence from the Bayes posterior
to the approximate posterior (Minka, 2001b), i.e.,
min
r
KL (p(w|D)âˆ¥r(w))
s.t.
r âˆˆG.
(2.43)
Clearly from its deï¬nition, the KL divergence,
KL (q(x)âˆ¥p(x)) =

q(x) log q(x)
p(x)dx,
diverges to +âˆif the support of q(x) is not covered by the support of p(x),
while it remains ï¬nite if the support of p(x) is not covered by the support
of q(x). Due to this asymmetric property of the KL divergence, VB learning
and EP can provide drastically different approximate posteriorsâ€”VB learning,

54
2 Variational Bayesian Learning
â€“2
â€“1
0
1
2
0
0.2
0.4
0.6
0.8
1
Bayes posterior
VB posterior
EP posterior
Figure 2.1 Bayes posterior, VB posterior, and EP posterior.
minimizing KL (r(w)âˆ¥p(w|D)), tends to provide a posterior that approximates
a single mode of the Bayes posterior, while EP, minimizing KL (p(w|D)âˆ¥r(w)),
tends to provide a posterior with a broad support covering all modes of the
Bayes posterior (see the illustration in Figure 2.1).
Moment Matching Algorithm
The EP problem (2.43) is typically solved by moment matching. It starts with
expressing the posterior distribution by the product of factors,
p(w|D) = 1
Z

n
tn(w),
where Z = p(D) is the marginal likelihood. For example, in the parametric
density estimation (Example 1.1) with i.i.d. samples D = {x(1),. . . , x(N)},
the factor can be set to tn(w) = p(x(n)|w) and t0(w) = p(w). In EP, the
approximating posterior is also assumed to have the same form,
r(w) = 1
Z

n
tn(w),
(2.44)
where Z is the normalization constant and becomes an approximation of the
marginal likelihood Z. Note that the factorization is not over the elements of w.
EP tries to minimize the KL divergence,
KL(pâˆ¥r) = KL
â›âœâœâœâœâœâ
1
Z

n
tn(w)
####
1
Z

n
tn(w)
ââŸâŸâŸâŸâŸâ ,
which is approximately carried out by reï¬ning each factor while the other
factors are ï¬xed, and cycling through all the factors. To reï¬ne the factor tn(w),
we deï¬ne the unnormalized distribution,
rÂ¬n(w) = r(w)
tn(w),

2.2 Other Approximation Methods
55
and the following distribution is used as an estimator of the true posterior:
pn(w) = tn(w)rÂ¬n(w)
Zn
,
where Zn =

tn(w)rÂ¬n(w)dw is the normalization constant. That is, the new
approximating posterior rnew(w) is computed so that it minimizes KL(pnâˆ¥rnew).
Usually, the approximating posterior is assumed to be a member of the
exponential family. In that case, the minimization of KL(pnâˆ¥rnew) is reduced
to the moment matching between pn and rnew. Namely, the parameter of rnew
is determined so that its moments are matched with those of pn.
The new approximating posterior rnew yields the reï¬nement of the factor
tn(w),
tn(w) = Zn
rnew(w)
rÂ¬n(w) ,
where the multiplication of Zn is derived from the zeroth-order moment
matching between pn and rnew,

tn(w)rÂ¬n(w)dw =
 tn(w)rÂ¬n(w)dw.
After several passes through all the factors, if the factors converge, then
the posterior is approximated by Eq. (2.44), and the marginal likelihood is
approximated by Z =
 
ntn(w)dw or alternatively by updating it as Z â†ZZn
whenever the factor tn(w) is reï¬ned. Although the convergence of EP is not
guaranteed, it is known that if EP converges, the resulting approximating
posterior is a stationary point of a certain energy function (Minka, 2001b).
Local Variational Approximation for EP
In Section 2.1.1, we saw that VB learning minimizes an upper-bound (the free
energy (2.4)) of the Bayes free energy FBayes â‰¡âˆ’log p(D) (or equivalently
maximizing the ELBO). We can say that EP does the opposite. Namely, the EP
problem (2.43) maximizes a lowerbound of the Bayes free energy:
max
r E(r)
s.t.
r âˆˆG,
(2.45)
where
E(r) = âˆ’

p(w, D)
p(D)
log p(w, D)
r(w)
dw
(2.46)
= âˆ’

p(w|D) log p(w|D)
r(w) dw âˆ’log p(D)
= âˆ’KL (p(w|D)âˆ¥r(w)) âˆ’log p(D).
(2.47)
The maximization form (2.45) of the EP problem can be solved by local
variational approximation, which is akin to the local variational approximation
for VB learning (Section 2.1.7). Let us restrict the search space G for the
approximate posterior r(w;Î½) to the function space of a simple distribution

56
2 Variational Bayesian Learning
family, e.g., Gaussian, parameterized with variational parameters Î½. Then, the
EP problem (2.45) is reduced to the following unconstrained maximization
problem,
max
Î½
E(Î½),
(2.48)
of the objective function written as
E(Î½) = âˆ’

p(w, D)
p(D)
log
p(w, D)
p(D)r(w;Î½)dw âˆ’log p(D).
(2.49)
Consider lower-bounding the objective (2.49) as
E(Î½) â‰¥E(Î½, Î·) â‰¡âˆ’

p(w; Î·)
p(D) max
)
0, log
p(w; Î·)
p(D)r(w;Î½)
1
dw âˆ’log p(D)
(2.50)
by using a parametric upper-bound p(w; Î·) of the joint distribution such that
p(w; Î·) â‰¥p(w, D)
(2.51)
for any w âˆˆW and Î· âˆˆH, where Î· is another set of variational parameters
with its domain H.3 Let us choose an upper-bound p(w; Î·) such that its
function form with respect to w is the same as the approximate posterior
r(w;Î½). More speciï¬cally, we assume that, for any given Î·, there exists Î½
such that
p(w; Î·) âˆr(w;Î½)
(2.52)
as a function of w.
Since the direct maximization of E(Î½) is intractable, we instead maximize
its lower-bound E(Î½, Î·) jointly overÎ½ and Î·. Namely, we solve the problem,
max
Î½,Î· E(Î½, Î·),
(2.53)
to ï¬nd the approximate posterior r(w;Î½) such that E(Î½, Î·) (â‰¤E(Î½)) is closest to
the Bayes free energy FBayes (when Î· is also optimized).
Let
q(w; Î·) = p(w; Î·)
Z(Î·)
(2.54)
be the distribution created by normalizing the upper-bound with its normaliza-
tion factor
Z(Î·) =

p(w; Î·)dw.
(2.55)
3 The two sets,Î½ and Î·, of variational parameters play the same roles as Î» and Î¾, respectively, in
the local variational approximation for VB learning.

2.2 Other Approximation Methods
57
Note that the normalization factor (2.55) is trivially an upper-bound of the
marginal likelihood, i.e.,
Z(Î·) â‰¥

p(w, D)dw = p(D),
and is tractable because of the assumption (2.52) that p is in the same simple
function form as r.
With Eq. (2.54), the lower-bound (2.50) is expressed as
E(Î½, Î·) = âˆ’
 Z(Î·)q(w; Î·)
p(D)
max
â§âªâ¨âªâ©0, log Z(Î·)q(w; Î·)
p(D)r(w;Î½)
â«âªâ¬âªâ­dw âˆ’log p(D)
= âˆ’Z(Î·)
p(D)

q(w; Î·) max
â§âªâ¨âªâ©0, log q(w; Î·)
r(w;Î½) + log Z(Î·)
p(D)
â«âªâ¬âªâ­dw âˆ’log p(D).
(2.56)
Eq. (2.56) is upper-bounded by
âˆ’Z(Î·)
p(D)

q(w; Î·)
â›âœâœâœâœâlog q(w; Î·)
r(w;Î½) + log Z(Î·)
p(D)
ââŸâŸâŸâŸâ dw âˆ’log p(D)
= âˆ’Z(Î·)
p(D)KL $q(w; Î·)âˆ¥r(w;Î½)% âˆ’Z(Î·)
p(D) log Z(Î·)
p(D) âˆ’log p(D),
(2.57)
which, for any Î· âˆˆH, is maximized whenÎ½ is such that
r(w;Î½) = q(w; Î·)
(2.58)
(the assumption (2.52) guarantees the attainability). With this optimal Î½,
Eq. (2.57) coincides with Eq. (2.56). Thus, after optimization with respect to
Î½, the lower-bound (2.56) is given as
max
Î½
E(Î½, Î·) = âˆ’Z(Î·)
p(D) log Z(Î·)
p(D) âˆ’log p(D).
(2.59)
Since x log x for x â‰¥1 is monotonically increasing, maximizing the lower-
bound (2.59) is achieved by solving
min
Î· Z(Î·).
(2.60)
Once the minimizer Î· is obtained, Eq. (2.58) gives the optimal approximate
posterior.
The problem (2.60) amounts to minimizing an upper-bound of the marginal
likelihood. This is in contrast to the local variational approximation for VB
learning, where a lower-bound of the marginal likelihood is maximized in the
end (compare Eq. (2.33) and Eq. (2.60)).

58
2 Variational Bayesian Learning
â€“2
â€“1
0
1
2
0
0.2
0.4
0.6
0.8
1
Bayes posterior
Lower-bound
Upper-bound
Figure 2.2 Bayes posterior and its tightest lower- and upper-bounds, formed by a
Gaussian.
Remembering that the joint distribution is proportional to the Bayes
posterior, i.e., p(w|D) = p(w, D)/p(D), we can say that the VB posterior
is the normalized version of the tightest (in terms of the total mass) lower-
bound of the Bayes posterior, while the EP posterior is the normalized version
of the tightest upper-bound of the Bayes posterior. Figure 2.2 illustrates the
tightest upper-bound and the tightest lower-bound of the Bayes posterior,
which correspond to unnormalized versions of the VB posterior and the EP
posterior, respectively (compare Figures 2.1 and 2.2). This view also explains
the tendency of VB learning and EPâ€”a lower-bound (the VB posterior) must
be zero wherever the Bayes posterior is zero, while an upper-bound (the EP
posterior) must be positive wherever the Bayes posterior is positive.
2.2.4 Metropolisâ€“Hastings Sampling
If a sufï¬cient number of samples {w(1),. . . , w(L)} from the posterior distribution
(1.3) are obtained, the expectation

f(w)p(w|D)dw required for computing
the quantities such as Eqs. (1.6) through (1.9) can be approximated by
1
L
L

l=1
f(w(l)).
The Metropolisâ€“Hastings sampling and the Gibbs sampling are most popular
methods to sample from the (unnormalized) posterior distribution in the
framework of Markov chain Monte Carlo (MCMC).
In the Metropolisâ€“Hastings sampling, we draw samples from a simple
distribution q(w|w(t)) called a proposal distribution, which is conditioned on
the current state w(t) of the parameter (or latent variables) w. The proposal
distribution is chosen to be a simple distribution such as a Gaussian centered
at w(t) if w is continuous or the uniform distribution in a certain neighborhood

2.2 Other Approximation Methods
59
of w(t) if w is discrete. At each cycle of the algorithm, we draw a candidate
sample wâˆ—from the proposal distribution q(w|w(t)), and we accept it with
probability
min

1, p(wâˆ—, D)
p(w(t), D)
q(w(t)|wâˆ—)
q(wâˆ—|w(t))

.
If wâˆ—is accepted, then the next state w(t+1) is moved to wâˆ—, w(t+1) = wâˆ—;
otherwise, it stays at the current state, w(t+1) = w(t). We repeat this procedure
until a sufï¬ciently long sequence of states is obtained. Note that if the proposal
distribution is symmetric, i.e., q(w|wâ€²) = q(wâ€²|w) for any w and wâ€², in which
case the algorithm is called the Metropolis algorithm, the probability of
acceptance depends on the ratio of the posteriors,
p(wâˆ—, D)
p(w(t), D) = p(wâˆ—, D)/Z
p(w(t), D)/Z = p(wâˆ—|D)
p(w(t)|D),
and if wâˆ—has higher posterior probability (density) than w(t), it is accepted with
probability 1.
To guarantee that the distribution of the sampled sequence converges to the
posterior distribution, we discard a ï¬rst part of the sequence, which is called
burn-in. Usually, after the burn-in period, we retain only every Mth sample
and discard the other samples so that the retained samples can be considered
as independent if M is sufï¬ciently large.
2.2.5 Gibbs Sampling
Another popular MCMC method is Gibbs sampling, which makes use of the
conditional conjugacy. More speciï¬cally, it is applicable when we can compute
and draw samples from the conditional distribution of a variable of w âˆˆRJ,
p(wj|w1,. . . , wjâˆ’1, wj+1,. . . , wJ, D) â‰¡p(wj|wÂ¬j, D),
conditioned on the rest of the variables of w.
Assuming that w(t) is obtained at the tth cycle of the Gibbs sampling
algorithm, the next sample of each variable is drawn from the conditional
distribution,
p(w(t+1)
j
|w(t)
Â¬j, D),
where
w(t)
Â¬j = (w(t+1)
1
,. . . , w(t+1)
jâˆ’1 , w(t)
j+1,. . . , w(t)
J )
from j = 1 to J in turn.
This sampling procedure can be viewed as a special case of the Metropolisâ€“
Hastings algorithm. If the proposal distribution q(w|w(t)) is chosen to be

60
2 Variational Bayesian Learning
p(wj|w(t)
Â¬j, D)Î´(wÂ¬j âˆ’w(t)
Â¬j),
then the probability that the candidate wâˆ—is accepted is 1 since wâˆ—
Â¬j = w(t)
Â¬j
implies that
p(wâˆ—, D)
p(w(t), D)
q(w(t)|wâˆ—)
q(wâˆ—|w(t)) = p(wâˆ—|D)
p(w(t)|D)
p(w(t)
j |w(t)
Â¬j, D)
p(wâˆ—
j|w(t)
Â¬j, D)
=
p(wâˆ—
Â¬j|D)p(wâˆ—
j|wâˆ—
Â¬j, D)
p(w(t)
Â¬j|D)p(w(t)
j |w(t)
Â¬j, D)
p(w(t)
j |w(t)
Â¬j, D)
p(wâˆ—
j|w(t)
Â¬j, D)
= 1.
As we have seen in the Metropolisâ€“Hastings and Gibbs sampling algo-
rithms, MCMC methods do not require the knowledge of the normalization
constant Z =

p(w, D)dw. Note that, however, even if we have samples
from the posterior, we need additional steps to compute Z with the samples.
A simple way is to calculate the expectation of the inverse of the likelihood by
the sample average,
/
1
p(D|w)
0
p(w|D)
â‰ƒ1
L
L

l=1
1
p(D|w(l)).
It provides an estimate of the inverse of Z because
/
1
p(D|w)
0
p(w|D)
=

1
p(D|w)
p(D|w)p(w)
Z
dw = 1
Z

p(w)dw = 1
Z .
However, this estimator is known to have high variance. A more sophisticated
sampling method to compute Z was developed by Chib (1995), while it
requires multiple runs of MCMC sampling.
A new efï¬cient method to compute the marginal likelihood was recently
proposed and named a widely applicable Bayesian information criterion
(WBIC), which requires only a single run of MCMC sampling from a
generalized posterior distribution (Watanabe, 2013). This method computes
the expectation of the negative log-likelihood,
âˆ’log p(D|w)
p(Î²)(w|D) ,
over the Î²-generalized posterior distribution deï¬ned as
p(Î²)(w|D) âˆp(D|w)Î²p(w),
with Î² = 1/ log N, where N is the number of i.i.d. samples. The computed
(approximated) expectation is proved to have the same leading terms as those
of the asymptotic expansion of âˆ’log Z as N â†’âˆ.

Part II
Algorithm


3
VB Algorithm for Multilinear Models
In this chapter, we derive iterative VB algorithms for multilinear models with
Gaussian noise, where we can rely on the conditional conjugacy with respect
to each linear factor. The models introduced in this chapter will be further ana-
lyzed in Part III, where the global solution or its approximation is analytically
derived, and the behavior of the VB solution is investigated in detail.
3.1 Matrix Factorization
Assume that we observe a matrix V âˆˆRLÃ—M, which is the sum of a target
matrix U âˆˆRLÃ—M and a noise matrix E âˆˆRLÃ—M:
V = U + E.
In the matrix factorization (MF) model (Srebro and Jaakkola, 2003; Srebro
et al., 2005; Lim and Teh, 2007; Salakhutdinov and Mnih, 2008; Ilin and Raiko,
2010) or the probabilistic principal component analysis (probabilistic PCA)
(Tipping and Bishop, 1999; Bishop, 1999b), the target matrix is assumed to be
low rank, and therefore can be factorized as
U = BAâŠ¤,
where A âˆˆRMÃ—H, B âˆˆRLÃ—H for H â‰¤min(L, M) are unknown parameters
to be estimated, and âŠ¤denotes the transpose of a matrix or vector. Here, the
rank of U is upper-bounded by H. We denote a column vector of a matrix by a
bold lowercase letter, and a row vector by a bold lowercase letter with a tilde,
namely,
A = (a1,. . . , aH) = $a1,. . . ,aM
%âŠ¤âˆˆRMÃ—H,
B = (b1,. . . , bH) =
b1,. . . ,bL
 âŠ¤âˆˆRLÃ—H.
63

64
3 VB Algorithm for Multilinear Models
3.1.1 VB Learning for MF
Assume that the observation noise E is independent Gaussian:
p(V|A, B) âˆexp

âˆ’1
2Ïƒ2
###V âˆ’BAâŠ¤###2
Fro

,
(3.1)
where âˆ¥Â·âˆ¥Fro denotes the Frobenius norm.
Conditional Conjugacy
If we treat B as a constant, the likelihood (3.1) is in the Gaussian form of A.
Similarly, if we treat A as a constant, the likelihood (3.1) is in the Gaussian
form of B. Therefore, conditional conjugacy with respect to A given B, as
well as with respect to B given A, holds if we adopt Gaussian priors:
p(A) âˆexp

âˆ’1
2tr

ACâˆ’1
A AâŠ¤ 
,
(3.2)
p(B) âˆexp

âˆ’1
2tr

BCâˆ’1
B BâŠ¤ 
,
(3.3)
where tr(Â·) denotes the trace of a matrix.
Typically, the prior covariance matrices CA and CB are restricted to be diag-
onal, which induces low-rankness (we discuss this mechanism in Chapter 7):
CA = Diag(c2
a1,. . . , c2
aH),
CB = Diag(c2
b1,. . . , c2
bH),
for cah, cbh > 0, h = 1,. . . , H.
Variational Bayesian Algorithm
Thanks to the conditional conjugacy, the following independence constraint
makes the approximate posterior Gaussian:
r(A, B) = rA(A)rB(B).
(3.4)
The VB learning problem (2.13) is then reduced to
r = argmin
r
F(r)
s.t.
r(A, B) = rA(A)rB(B).
(3.5)
Under the constraint (3.4), the free energy is written as
F(r) =
/
log
rA(A)rB(B)
p(V|A, B)p(A)p(B)
0
rA(A)rB(B)
=

rA(A)rB(B) log
rA(A)rB(B)
p(V|A, B)p(A)p(B)dAdB.
(3.6)

3.1 Matrix Factorization
65
Following the recipe described in Section 2.1.5, we take the derivatives of
the free energy (3.6) with respect to rA(A) and rB(B), respectively. Thus, we
obtain the following stationary conditions:
rA(A) âˆp(A) exp log p(V|A, B)
rB(B) ,
(3.7)
rB(B) âˆp(B) exp log p(V|A, B)
rA(A) .
(3.8)
By substituting the likelihood (3.1) and the prior (3.2) into Eq. (3.7), we
obtain
rA(A) âˆexp

âˆ’1
2tr

ACâˆ’1
A AâŠ¤ 
âˆ’
1
2Ïƒ2
*###V âˆ’BAâŠ¤###2
Fro
+
rB(B)

âˆexp

âˆ’1
2tr
!
ACâˆ’1
A AâŠ¤+ Ïƒâˆ’2 
âˆ’2VâŠ¤BAâŠ¤+ ABâŠ¤BAâŠ¤
rB(B)
"
âˆexp
â›âœâœâœâœâœâœâœâœâœâœâ
âˆ’
tr
!
(A âˆ’A)Î£
âˆ’1
A (A âˆ’A)âŠ¤
"
2
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
,
(3.9)
where
A = Ïƒâˆ’2VâŠ¤âŸ¨BâŸ©rB(B) Î£A,
(3.10)
Î£A = Ïƒ2 !
BâŠ¤B

rB(B) + Ïƒ2Câˆ’1
A
"âˆ’1
.
(3.11)
Similarly, by substituting the likelihood (3.1) and the prior (3.3) into Eq. (3.8),
we obtain
rB(B) âˆexp

âˆ’1
2tr

BCâˆ’1
B BâŠ¤ 
âˆ’
1
2Ïƒ2
*###V âˆ’BAâŠ¤###2
Fro
+
rA(A)

âˆexp

âˆ’1
2tr
!
BCâˆ’1
B BâŠ¤+ Ïƒâˆ’2 
âˆ’2VABâŠ¤+ BAâŠ¤ABâŠ¤
rA(A)
"
âˆexp
â›âœâœâœâœâœâœâœâœâœâœâ
âˆ’
tr
!
(B âˆ’B)Î£
âˆ’1
B (B âˆ’B)âŠ¤
"
2
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
,
(3.12)
where
B = Ïƒâˆ’2V âŸ¨AâŸ©rA(A) Î£B,
(3.13)
Î£B = Ïƒ2 !
AâŠ¤A

rA(A) + Ïƒ2Câˆ’1
B
"âˆ’1
.
(3.14)
Eqs. (3.9) and (3.12) imply that the posteriors are Gaussian. More speciï¬-
cally, they can be written as
rA(A) = MGaussM,H(A; A, IM âŠ—Î£A),
(3.15)
rB(B) = MGaussL,H(B; B, IL âŠ—Î£B),
(3.16)

66
3 VB Algorithm for Multilinear Models
where âŠ—denotes the Kronecker product, and
MGaussD1,D2(X; M, Ë˜Î£) â‰¡GaussD1Â·D2(vec(XâŠ¤); vec(MâŠ¤), Ë˜Î£)
(3.17)
denotes the matrix variate Gaussian distribution (Gupta and Nagar, 1999).
Here, vec : RD2Ã—D1 â†’RD2D1 is the vectorization operator, which concatenates
all column vectors of a matrix into a long column vector. Note that, if the
covariance has a speciï¬c structure expressed as Ë˜Î£ = Î£ âŠ—Î¨ âˆˆRD2D1Ã—D2D1,
such as Eqs. (3.15) and (3.16), the matrix variate Gaussian distribution can be
written as
MGaussD1,D2(X; M, Î£ âŠ—Î¨) â‰¡
1
(2Ï€)D1D2/2 det (Î£)D2/2 det (Î¨)D1/2
Â· exp

âˆ’1
2tr

Î£âˆ’1 (X âˆ’M) Î¨âˆ’1 (X âˆ’M)âŠ¤ 
.
(3.18)
The fact that the posterior is Gaussian is a consequence of the forced
independence between A and B and conditional conjugacy. The parameters,
,A, B, Î£A, Î£B
-
, deï¬ning the VB posterior (3.15) and (3.16), are the variational
parameters.
Since rA(A) and rB(B) are Gaussian, the ï¬rst and the (noncenterized) second
moments can be expressed with variational parameters as follows:
âŸ¨AâŸ©rA(A) = A,

AâŠ¤A

rA(A) = A
âŠ¤A + MÎ£A,
âŸ¨BâŸ©rB(B) = B,

BâŠ¤B

rB(B) = B
âŠ¤B + LÎ£B.
By substituting the preceding into Eqs. (3.10), (3.11), (3.13), and (3.14), we
have the following relations among the variational parameters:
A = Ïƒâˆ’2VâŠ¤BÎ£A,
(3.19)
Î£A = Ïƒ2 !
B
âŠ¤B + LÎ£B + Ïƒ2Câˆ’1
A
"âˆ’1
,
(3.20)
B = Ïƒâˆ’2VAÎ£B,
(3.21)
Î£B = Ïƒ2 !
A
âŠ¤A + MÎ£A + Ïƒ2Câˆ’1
B
"âˆ’1
.
(3.22)
As we see shortly, Eqs. (3.19) through (3.22) are stationary conditions for
variational parameters, which can be used as update rules for coordinate
descent local search (Bishop, 1999b).

3.1 Matrix Factorization
67
Free Energy as a Function of Variational Parameters
By substituting Eqs. (3.15) and (3.16) into Eq. (3.6), we can explicitly write
down the free energy as (not a functional but) a function of the unknown
variational parameters
,A, B, Î£A, Î£B
-
:
2F = 2
/
log
rA(A)rB(B)
p(V|A, B)p(A)p(B)
0
rA(A)rB(B)
= 2
/
log rA(A)rB(B)
p(A)p(B)
0
rA(A)rB(B)
âˆ’2 log p(V|A, B)
rA(A)rB(B)
=
/
M log det (CA)
det
Î£A
 + L log det (CB)
det
Î£B
 + tr

Câˆ’1
A AâŠ¤A + Câˆ’1
B BâŠ¤B
 
âˆ’tr
!
Î£
âˆ’1
A (A âˆ’A)âŠ¤(A âˆ’A) + Î£
âˆ’1
B (B âˆ’B)âŠ¤(B âˆ’B)
"
+ LM log(2Ï€Ïƒ2) + âˆ¥V âˆ’BAâŠ¤âˆ¥2
Fro
Ïƒ2
0
rA(A)rB(B)
= M log det (CA)
det
Î£A
 + L log det (CB)
det
Î£B
 âˆ’tr
!
MÎ£
âˆ’1
A Î£A + LÎ£
âˆ’1
B Î£B
"
+ tr
!
Câˆ’1
A
!
A
âŠ¤A + MÎ£A
"
+ Câˆ’1
B
!
B
âŠ¤B + LÎ£B
""
+ LM log(2Ï€Ïƒ2) +
/
âˆ¥(Vâˆ’BA
âŠ¤)+(BA
âŠ¤âˆ’BAâŠ¤)âˆ¥2
Fro
Ïƒ2
0
rA(A)rB(B)
= M log det (CA)
det
Î£A
 + L log det (CB)
det
Î£B
 âˆ’(L + M)H
+ tr
!
Câˆ’1
A
!
A
âŠ¤A + MÎ£A
"
+ Câˆ’1
B
!
B
âŠ¤B + LÎ£B
""
+ LM log(2Ï€Ïƒ2) +
âˆ¥Vâˆ’BA
âŠ¤âˆ¥2
Fro
Ïƒ2
+
/
âˆ¥BA
âŠ¤âˆ’BAâŠ¤âˆ¥2
Fro
Ïƒ2
0
rA(A)rB(B)
= LM log(2Ï€Ïƒ2) +
####V âˆ’BA
âŠ¤####
2
Fro
Ïƒ2
+ M log det (CA)
det
Î£A
 + L log det (CB)
det
Î£B
 
âˆ’(L + M)H + tr
2
Câˆ’1
A
!
A
âŠ¤A + MÎ£A
"
+ Câˆ’1
B
!
B
âŠ¤B + LÎ£B
"
+ Ïƒâˆ’2 !
âˆ’A
âŠ¤AB
âŠ¤B +
!
A
âŠ¤A + MÎ£A
" !
B
âŠ¤B + LÎ£B
""3
.
(3.23)
Now, the VB learning problem is reduced from the function optimization
(3.5) to the following variable optimization:

68
3 VB Algorithm for Multilinear Models
Given
CA, CA âˆˆDH
++,
Ïƒ2 âˆˆR++,
min
A,B,Î£A,Î£B
F,
(3.24)
s.t.
A âˆˆRMÃ—H, B âˆˆRLÃ—H,
Î£A, Î£B âˆˆSH
++,
where R++ is the set of positive real numbers, SD
++ is the set of D Ã— D
(symmetric) positive deï¬nite matrices, and DD
++ is the set of D Ã— D positive
deï¬nite diagonal matrices.
We note the following:
â€¢ Once the solution
,A, B, Î£A, Î£B
-
of the problem (3.24) is obtained, Eqs.
(3.15) and (3.16) specify the VB posteriorr(A, B) = rA(A)rB(B).
â€¢ We treated the prior covariances CA and CB and the noise variance Ïƒ2 as
hyperparameters, and therefore assumed to be given when the VB problem
was solved. However, they can be estimated through the empirical Bayesian
procedure, which is explained shortly. They can also be treated as random
variables, and their VB posterior can be computed by adopting conjugate
Gamma priors and minimizing the free energy under an appropriate
independence constraint.
â€¢ Eqs. (3.19) through (3.22) coincide with the stationary conditions of the
free energy (3.23), which are derived from the derivatives with respect to
A, Î£A, B, and Î£B, respectively. Therefore, iterating Eqs. (3.19) through
(3.22) gives a local solution to the problem (3.24).
Empirical Variational Bayesian Algorithm
The empirical variational Bayesian (EVB) procedure can be performed by
minimizing the free energy also with respect to the hyperparameters:
min
A,B,Î£A,Î£B,CA,CA,Ïƒ2
F,
(3.25)
s.t.
A âˆˆRMÃ—H, B âˆˆRLÃ—H,
Î£A, Î£B âˆˆSH
++,
CA, CA âˆˆDH
++,
Ïƒ2 âˆˆR++.
By differentiating the free energy (3.23) with respect to each entry of CA
and CB, we have, for h = 1,. . . , H,
c2
ah =
###ah
###2/M +
Î£A
 
h,h ,
(3.26)
c2
bh =
####bh
####
2
/L +
Î£B
 
h,h .
(3.27)

3.1 Matrix Factorization
69
Algorithm 1 EVB learning for matrix factorization.
1: Initialize the variational parameters (A, Î£A, B, Î£B), and the hyperparame-
ters (CA, CB, Ïƒ2), for example, Am,h, Bl,h âˆ¼Gauss1(0, Ï„), Î£A = Î£B = CA =
CB = Ï„IH, and Ïƒ2 = Ï„2 for Ï„2 = âˆ¥Vâˆ¥2
Fro/(LM).
2: Apply (substitute the right-hand side into the left-hand side) Eqs. (3.20),
(3.19), (3.22), and (3.21) to update Î£A, A, Î£B, and B, respectively.
3: Apply Eqs. (3.26) and (3.27) for all h = 1,. . . , H, and Eq. (3.28) to update
CA, CB, and Ïƒ2, respectively.
4: Prune the hth component if c2
ahc2
bh < Îµ, where Îµ > 0 is a small threshold,
e.g., set to Îµ = 10âˆ’4.
5: Evaluate the free energy (3.23).
6: Iterate Steps 2 through 5 until convergence (until the energy decrease
becomes smaller than a threshold).
Similarly, by differentiating the free energy (3.23) with respect to Ïƒ2, we have
Ïƒ2 =
âˆ¥Vâˆ¥2
Fro âˆ’tr
!
2VâŠ¤BA
âŠ¤"
+ tr
!
(A
âŠ¤A + MÎ£A)(B
âŠ¤B + LÎ£B)
"
LM
.
(3.28)
Eqs. (3.26)â€“(3.28) are used as update rules for the prior covariances CA, CB,
and the noise variance Ïƒ2, respectively.
Starting from some initial value, iterating Eqs. (3.19) through (3.22) and
Eqs. (3.26) through (3.28) gives a local solution for EVB learning. Algorithm 1
summarizes this iterative procedure. If we appropriately set the hyperpara-
meters (CA, CB, Ïƒ2) in Step 1 and skip Steps 3 and 4, Algorithm 1 is reduced
to (nonempirical) VB learning.
We note the following for implementation:
â€¢ Due to the automatic relevance determination (ARD) effect in EVB
learning (see Chapter 7), c2
ahc2
bh converges to zero for some h. For this
reason, â€œpruningâ€ in Step 4 is necessary for numerical stability (log det (C)
diverges if C is singular). If the hth component is pruned, the corresponding
hth column of A and B and the hth column and row of Î£A, Î£B, CA, CB
should be removed, and the rank H should be reduced accordingly.
â€¢ In principle, the update rules never increase the free energy. However,
pruning can slightly increase it.
â€¢ When computing the free energy by Eq. (3.23), log det (Â·) should be
computed as twice the sum of the log of the diagonals of the Cholesky
decomposition, i.e.,

70
3 VB Algorithm for Multilinear Models
log det (C) = 2
H

h=1
$log(Chol(C))h,h
% .
Otherwise, det (Â·) can be huge for practical size of H, causing numerical
instability.
Simple Variational Bayesian Learning (with Columnwise Independence)
The updates (3.19) through (3.22) require inversion of an H Ã— H matrix. One
can derive a faster VB learning algorithm by using a stronger constraint for the
VB learning. More speciï¬cally, instead of the matrixwise independence (3.4),
we assume the independence between the column vectors of A = (a1,. . . , aH)
and B = (b1,. . . , bH) (Ilin and Raiko, 2010; Nakajima and Sugiyama, 2011;
Kim and Choi, 2014):
r(A, B) =
H

h=1
rah(ah)
H

h=1
rbh(bh).
(3.29)
By applying the same procedure as that with the matrixwise independence
constraint, we can derive the solution to
r = argmin
r
F(r)
s.t.
r(A, B) =
H

h=1
rah(ah)
H

h=1
rbh(bh),
(3.30)
which is in the form of the matrix variate Gaussian:
rA(A) = MGaussM,H(A; A, IM âŠ—Î£A) =
H

h=1
GaussM(ah;ah, Ïƒ2
ah IM),
rB(B) = MGaussL,H(B; B, IL âŠ—Î£B) =
H

h=1
GaussL(bh;bh, Ïƒ2
bhIL),
with the variational parameters,
A = (a1,. . . ,aH),
B = (b1,. . . ,bH),
Î£A = Diag(Ïƒ2
a1,. . . , Ïƒ2
aH),
Î£B = Diag(Ïƒ2
b1,. . . , Ïƒ2
bH).
Here Diag(Â· Â· Â· ) denotes the diagonal matrix with the speciï¬ed diagonal entries.
The stationary conditions are given as follows: for all h = 1,. . . , H,
ah =
Ïƒ2
ah
Ïƒ2
â›âœâœâœâœâœâœâV âˆ’

hâ€²h
bhâ€²aâŠ¤
hâ€²
ââŸâŸâŸâŸâŸâŸâ 
âŠ¤
bh,
(3.31)

3.1 Matrix Factorization
71
Ïƒ2
ah = Ïƒ2
####bh
####
2
+ LÏƒ2
bh + Ïƒ2
c2ah
âˆ’1
,
(3.32)
bh =
Ïƒ2
bh
Ïƒ2
â›âœâœâœâœâœâœâV âˆ’

hâ€²h
bhâ€²aâŠ¤
hâ€²
ââŸâŸâŸâŸâŸâŸâ ah,
(3.33)
Ïƒ2
bh = Ïƒ2
â›âœâœâœâœâœâ
###ah
###2 + MÏƒ2
ah + Ïƒ2
c2
bh
ââŸâŸâŸâŸâŸâ 
âˆ’1
.
(3.34)
The free energy is given by Eq. (3.23) with the posterior covariances Î£A and
Î£B restricted to be diagonal. The stationary conditions for the hyperparameters
are unchanged, and given by Eqs. (3.26) through (3.28). Therefore, Algorithm
1 with Eqs. (3.31) through (3.34), substituted for Eqs. (3.19) through (3.22),
gives a local solution to the VB problem (3.30) with the columnwise indepen-
dence constraint.
We call this variant simple VB (SimpleVB) learning. In Chapter 6, it will
be shown that, in the fully observed MF model, the SimpleVB problem
(3.30) with columnwise independence and the original VB problem (3.5) with
matrixwise independence actually give the equivalent solution.
3.1.2 Special Cases
Probabilistic principal component analysis and reduced rank regression are
special cases of matrix factorization. Therefore, they can be trained by
Algorithm 1 with or without small modiï¬cations.
Probabilistic Principal Component Analysis Probabilistic principal com-
ponent analysis (Tipping and Bishop, 1999; Bishop, 1999b) is a probabilisitic
model of which the ML estimation corresponds to the classical principal
component analysis (PCA) (Hotelling, 1933). The observation v âˆˆRL is
assumed to be driven by a latent vector a âˆˆRH in the following form:
v = Ba + Îµ.
Here, B âˆˆRLÃ—H speciï¬es the linear relationship between a and v, and Îµ âˆˆRL
is a Gaussian noise subject to GaussL(0, Ïƒ2IL).
Suppose that we are given M observed samples V = (v1,. . . , vM) generated
from the latent vectors AâŠ¤= (a1,. . . ,aM), and each latent vector is subject to
a âˆ¼GaussH(0, IH). Then, the probabilistic PCA model is written as Eqs. (3.1)
and (3.2) with CA = IH. Having the prior (3.2) on B, it is equivalent to the MF
model.

72
3 VB Algorithm for Multilinear Models
Figure 3.1 Reduced rank regression model.
If we apply VB or EVB learning, the intrinsic dimension H is automatically
selected without additional procedure (Bishop, 1999b). This useful property is
caused by the ARD (Neal, 1996), which makes the estimators for the irrelevant
column vectors of A and B zero. In Chapter 7, this phenomenon is explained in
terms of model-induced regularization (MIR), while in Chapter 8, a theoretical
guarantee of the dimensionality estimation is given.
Reduced Rank Regression Reduced rank regression (RRR) (Baldi and
Hornik, 1995; Reinsel and Velu, 1998) is aimed at learning a relation between
an input vector x âˆˆRM and an output vector y âˆˆRL by using the following
linear model:
y = BAâŠ¤x + Îµ,
(3.35)
where A âˆˆRMÃ—H and B âˆˆRLÃ—H are parameters to be estimated, and
Îµ âˆ¼GaussL(0, Ïƒâ€²2IL) is a Gaussian noise. RRR can be seen as a linear neural
network (Figure 3.1), of which the model distribution is given by
p(y|x, A, B) =

2Ï€Ïƒâ€²2 âˆ’L/2 exp

âˆ’1
2Ïƒâ€²2
###y âˆ’BAâŠ¤x
###2
.
(3.36)
Thus, we can interpret this model as ï¬rst projecting the input vector x
onto a lower-dimensional latent subspace by AâŠ¤and then performing linear
prediction by B.
Suppose we are given N pairs of input and output vectors:
D =
,
(x(n), y(n))|x(n) âˆˆRM, y(n) âˆˆRL, n = 1,. . . , N
-
.
(3.37)
Then, the likelihood of the RRR model (3.36) is expressed as
p(D|A, B) =
N

n=1
p(y(n)|x(n), A, B)p(x(n))
âˆexp
â›âœâœâœâœâœââˆ’1
2Ïƒâ€²2
N

n=1
###y(n) âˆ’BAâŠ¤x(n)###2
ââŸâŸâŸâŸâŸâ .
(3.38)

3.1 Matrix Factorization
73
Note that we here ignored the input distributions N
n=1 p(x(n)) as constants (see
Example 1.2 in Section 1.1.1). Let us assume that the samples are centered:
1
N
N

n=1
x(n) = 0
and
1
N
N

n=1
y(n) = 0.
Furthermore, let us assume that the input samples are prewhitened (HyvÂ¨arinen
et al., 2001), i.e., they satisfy
1
N
N

n=1
x(n)x(n)âŠ¤= IM.
Let
V = Î£XY = 1
N
N

n=1
y(n)x(n)âŠ¤
(3.39)
be the sample cross-covariance matrix, and
Ïƒ2 = Ïƒâ€²2
N
(3.40)
be a rescaled noise variance. Then the exponent of the likelihood (3.38) can be
written as
âˆ’1
2Ïƒâ€²2
N

n=1
###y(n) âˆ’BAâŠ¤x(n)###2
= âˆ’1
2Ïƒâ€²2
N

n=1
2###y(n)###2 âˆ’2tr

y(n)x(n)âŠ¤ABâŠ¤ 
+ tr

BAâŠ¤x(n)x(n)âŠ¤ABâŠ¤ 3
= âˆ’1
2Ïƒâ€²2
â§âªâªâ¨âªâªâ©
N

n=1
###y(n)###2 âˆ’2Ntr

VABâŠ¤ 
+ Ntr

ABâŠ¤BAâŠ¤ â«âªâªâ¬âªâªâ­
= âˆ’1
2Ïƒâ€²2
â›âœâœâœâœâœâN
###V âˆ’BAâŠ¤###2
Fro +
N

n=1
###y(n)###2 âˆ’N âˆ¥Vâˆ¥2
Fro
ââŸâŸâŸâŸâŸâ 
= âˆ’1
2Ïƒ2
###V âˆ’BAâŠ¤###2
Fro âˆ’
1
2Ïƒ2
â›âœâœâœâœâœâ
1
N
N

n=1
###y(n)###2 âˆ’âˆ¥Vâˆ¥2
Fro
ââŸâŸâŸâŸâŸâ .
(3.41)
The ï¬rst term in Eq. (3.41) coincides with the log-likelihood of the MF model
(3.1), and the second term is constant with respect to A and B. Thus, RRR is
reduced to MF, as far as the posteriors for A and B are concerned.
However, the second term depends on the rescaled noise variance Ïƒ2,
and therefore, should be considered when Ïƒ2 is estimated based on the free
energy minimization principle. Furthermore, the normalization constant of the

74
3 VB Algorithm for Multilinear Models
likelihood (3.38) differs from that of the MF model. Taking these differences
into account, the VB free energy of the RRR model (3.38) with the priors (3.2)
and (3.3) is given by
2FRRR = NL log(2Ï€NÏƒ2) +
1
N
N
n=1
###y(n)###2 âˆ’âˆ¥Vâˆ¥2
Fro
Ïƒ2
+
####V âˆ’BA
âŠ¤####
2
Fro
Ïƒ2
+ M log det (CA)
det
Î£A
 + L log det (CB)
det
Î£B
 
âˆ’(L + M)H + tr
2
Câˆ’1
A
!
A
âŠ¤A + MÎ£A
"
+ Câˆ’1
B
!
B
âŠ¤B + LÎ£B
"
+Ïƒâˆ’2 !
âˆ’A
âŠ¤AB
âŠ¤B +
!
A
âŠ¤A + MÎ£A
" !
B
âŠ¤B + LÎ£B
""3
.
(3.42)
Note that the difference from Eq. (3.23) is only in the ï¬rst two terms.
Accordingly, the stationary conditions for the variational parameters A, B, Î£A,
and Î£B, and those for the prior covariances CA and CB (in EVB learning) are
the same, i.e., the update rules given by Eqs. (3.19) through (3.22), (3.26), and
(3.27) are valid for the RRR model. The update rule for the rescaled noise
variance is different from Eq. (3.28), and given by
(Ïƒ2)RRR =
1
N
N
n=1
###y(n)###2 âˆ’tr
!
2VâŠ¤BA
âŠ¤"
+ tr
!
(A
âŠ¤A + MÎ£A)(B
âŠ¤B + LÎ£B)
"
NL
,
(3.43)
which was obtained from the derivative of Eq. (3.42), instead of Eq. (3.23),
with respect to Ïƒ2.
Once the rescaled noise variance Ïƒ2 is estimated, Eq. (3.40) gives the
original noise variance Ïƒâ€²2 of the RRR model (3.38).
3.2 Matrix Factorization with Missing Entries
One of the major applications of MF is collaborative ï¬ltering (CF), where
only a part of the entries in V are observed, and the task is to predict missing
entries. We can derive a VB algorithm for this scenario, similarly to the fully
observed case.
3.2.1 VB Learning for MF with Missing Entries
To express missing entris, the likelihood (3.1) should be replaced with
p(V|A, B) âˆexp

âˆ’1
2Ïƒ2
####PÎ› (V) âˆ’PÎ›

BAâŠ¤ ####
2
Fro

,
(3.44)

3.2 Matrix Factorization with Missing Entries
75
where Î› denotes the set of observed indices, and PÎ› (V) denotes the matrix of
the same size as V with its entries given by
(PÎ› (V))l,m =
â§âªâªâ¨âªâªâ©
Vl,m
if (l, m) âˆˆÎ›,
0
otherwise.
Conditional Conjugacy
Since the likelihood (3.44) is still in a Gaussian form of A if B is regarded as
a constant, or vise versa, the conditional conjugacy with respect to A and B,
respectively, still holds if we adopt the Gaussian priors (3.2) and (3.3):
p(A) âˆexp

âˆ’1
2tr

ACâˆ’1
A AâŠ¤ 
,
p(B) âˆexp

âˆ’1
2tr

BCâˆ’1
B BâŠ¤ 
.
The posterior will be still Gaussian, but in a broader class than the fully
observed case, as will be seen shortly.
Variational Bayesian Algorithm
With the missing entries, the stationary condition (3.7) becomes
rA(A) âˆexp

âˆ’1
2tr

ACâˆ’1
A AâŠ¤ 
âˆ’
1
2Ïƒ2
*####PÎ› (V) âˆ’PÎ›

BAâŠ¤ ####
2
Fro
+
rB(B)

âˆexp

âˆ’1
2tr

ACâˆ’1
A AâŠ¤ 
+ Ïƒâˆ’2 
(l,m)âˆˆÎ›
/
âˆ’2Vl,m
H

h=1
Bl,hAm,h +
H

h=1
H

hâ€²=1
Bl,hBl,hâ€²Am,hAm,hâ€²
0
rB(B)
ââŸâŸâŸâŸâŸâŸâŸâ 
âˆexp
â›âœâœâœâœâœâœâœâœâœâœâ
âˆ’
M
m=1
!
(am âˆ’am)âŠ¤Î£
âˆ’1
A,m(am âˆ’am)
"
2
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
,
(3.45)
where
am = Ïƒâˆ’2Î£A,m

l:(l,m)âˆˆÎ›
Vl,m
bl

rB(B) ,
(3.46)
Î£A,m = Ïƒ2
â›âœâœâœâœâœâœâ

l:(l,m)âˆˆÎ›
*
blb
âŠ¤
l
+
rB(B) + Ïƒ2Câˆ’1
A
ââŸâŸâŸâŸâŸâŸâ 
âˆ’1
.
(3.47)
Here, 
(l,m)âˆˆÎ› denotes the sum over l and m such that (l, m) âˆˆÎ›, and 
l:(l,m)âˆˆÎ›
denotes the sum over l such that (l, m) âˆˆÎ› for given m.

76
3 VB Algorithm for Multilinear Models
Similarly, we have
rB(B) âˆexp

âˆ’1
2tr

BCâˆ’1
B BâŠ¤ 
âˆ’
1
2Ïƒ2
*####PÎ› (V) âˆ’PÎ›

BAâŠ¤ ####
2
Fro
+
rA(A)

âˆexp
â›âœâœâœâœâœâœâœâœâœâœâœâ
âˆ’
L
l=1
!
(bm âˆ’bl)âŠ¤Î£
âˆ’1
B,l(bl âˆ’bl)
"
2
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
,
(3.48)
where
bl = Ïƒâˆ’2Î£B,l

m:(l,m)âˆˆÎ›
Vl,m
am

rA(A) ,
(3.49)
Î£B,l = Ïƒ2
â›âœâœâœâœâœâœâ

m:(l,m)âˆˆÎ›

amaâŠ¤
m

rA(A) + Ïƒ2Câˆ’1
B
ââŸâŸâŸâŸâŸâŸâ 
âˆ’1
.
(3.50)
Eqs. (3.45) and (3.48) imply that A and B are Gaussian in the following
form:
rA(A) = MGaussM,H(A; A, Ë˜Î£A) =
M

m=1
GaussH(am;am, Î£A,m),
rB(B) = MGaussL,H(B; B, Ë˜Î£B) =
L

l=1
GaussH(bm;bl, Î£B,l),
where
Ë˜Î£A =
â›âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ
Î£A,1
0
Â· Â· Â·
0
0
Î£A,2
...
...
...
0
0
Â· Â· Â·
0
Î£A,M
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
,
Ë˜Î£B =
â›âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ
Î£B,1
0
Â· Â· Â·
0
0
Î£B,2
...
...
...
0
0
Â· Â· Â·
0
Î£B,L
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
.
Note that the posterior covariances cannot be expressed with a Kronecker
product, unlike the fully observed case. However, the posteriors are Gaussian,
and moments are given by
am

rA(A) = am,

amaâŠ¤
m

rA(A) = ama
âŠ¤
m + Î£A,m,
bl

rB(B) = bl,
*
blb
âŠ¤
l
+
rB(B) = blb
âŠ¤
l + Î£B,l.

3.2 Matrix Factorization with Missing Entries
77
Thus, Eqs. (3.46), (3.47), (3.49), and (3.50) lead to
am = Ïƒâˆ’2Î£A,m

l:(l,m)âˆˆÎ›
Vl,mbl,
(3.51)
Î£A,m = Ïƒ2
â›âœâœâœâœâœâœâ

l:(l,m)âˆˆÎ›

blb
âŠ¤
l + Î£B,l

+ Ïƒ2Câˆ’1
A
ââŸâŸâŸâŸâŸâŸâ 
âˆ’1
,
(3.52)
bl = Ïƒâˆ’2Î£B,l

m:(l,m)âˆˆÎ›
Vl,mam,
(3.53)
Î£B,l = Ïƒ2
â›âœâœâœâœâœâœâ

m:(l,m)âˆˆÎ›
!
ama
âŠ¤
m + Î£A,m
"
+ Ïƒ2Câˆ’1
B
ââŸâŸâŸâŸâŸâŸâ 
âˆ’1
,
(3.54)
which are used as update rules for local search (Lim and Teh, 2007).
Free Energy as a Function of Variational Parameters
An explicit form of the free energy can be obtained in a similar fashion to the
fully observed case:
2F = # (Î›) Â· log(2Ï€Ïƒ2) + M log det (CA) + L log det (CB)
âˆ’
M

m=1
log det
Î£A,m
 
âˆ’
L

l=1
log det
Î£B,l
 
âˆ’(L + M)H
+ tr
â§âªâªâ¨âªâªâ©Câˆ’1
A
â›âœâœâœâœâœâA
âŠ¤A +
M

m=1
Î£A,m
ââŸâŸâŸâŸâŸâ + Câˆ’1
B
â›âœâœâœâœâœâB
âŠ¤B +
L

l=1
Î£B,l
ââŸâŸâŸâŸâŸâ 
â«âªâªâ¬âªâªâ­
+ Ïƒâˆ’2 
(l,m)âˆˆÎ›

Vl,m âˆ’2Vl,ma
âŠ¤
mbl + tr
)!
ama
âŠ¤
m + Î£A,m
" 
blb
âŠ¤
l + Î£B,l
1
,
(3.55)
where # (Î›) denotes the number of observed entries.
Empirical Variational Bayesian Algorithm
By taking derivatives of the free energy (3.55), we can derive update rules for
the hyperparameters:
c2
ah =
###ah
###2 +
M
m=1 Î£A,m
 
h,h
M
,
(3.56)
c2
bh =
####bh
####
2
+
L
l=1 Î£B,l
 
h,h
L
,
(3.57)

78
3 VB Algorithm for Multilinear Models
Algorithm 2 EVB learning for matrix factorization with missing entries.
1: Initialize the variational parameters (A, {Î£A,m}M
m=1, B, {Î£B,l}L
l=1), and the
hyperparameters (CA, CB, Ïƒ2), for example, Am,h, Bl,h âˆ¼Gauss1 (0, Ï„),
Î£A,m = Î£B,l = CA = CB = Ï„IH, and Ïƒ2 = Ï„2 for Ï„2 = 
(l,m)âˆˆÎ› V2
l,m/# (Î›).
2: Apply (substitute the right-hand side into the left-hand side) Eqs. (3.52),
(3.51), (3.54), and (3.53) to update {Î£A,m}M
m=1, A, {Î£B,l}L
l=1, and B, respec-
tively.
3: Apply Eqs. (3.56) and (3.57) for all h = 1,. . . , H, and Eq. (3.58) to update
CA, CB, and Ïƒ2, respectively.
4: Prune the hth component if c2
ahc2
bh < Îµ, where Îµ > 0 is a threshold, e.g., set
to Îµ = 10âˆ’4.
5: Evaluate the free energy (3.55).
6: Iterate Steps 2 through 5 until convergence (until the energy decrease
becomes smaller than a threshold).
Ïƒ2 =

(l,m)âˆˆÎ›

Vl,m âˆ’2Vl,ma
âŠ¤
mbl + tr
)!
ama
âŠ¤
m + Î£A,m
" 
blb
âŠ¤
l + Î£B,l
1
# (Î›)
. (3.58)
Algorithm 2 summarizes the EVB algorithm for MF with missing entries.
Again, if we appropriately set the hyperparameters (CA, CB, Ïƒ2) in Step 1 and
skip Steps 3 and 4, Algorithm 2 is reduced to (nonempirical) VB learning.
Simple Variational Bayesian Learning (with Columnwise Independence)
Similarly to the fully observed case, we can reduce the computational burden
and the memory requirement of VB learning by adopting the columnwise
independence (Ilin and Raiko, 2010):
r(A, B) =
H

h=1
rah(ah)
H

h=1
rbh(bh).
(3.59)
By applying the same procedure as the matrixwise independence case, we can
derive the solution to
r = argmin
r
F(r)
s.t.
r(A, B) =
H

h=1
rah(ah)
H

h=1
rbh(bh),
(3.60)
which is in the form of the matrix variate Gaussian,

3.2 Matrix Factorization with Missing Entries
79
rA(A) = MGaussM,H(A; A, Ë˜Î£A) =
M

m=1
H

h=1
Gauss1(Am,h; Am,h, Ïƒ2
Am,h),
rB(B) = MGaussL,H(B; B, Ë˜Î£B) =
L

l=1
H

h=1
Gauss1(Bl,h; Bl,h, Ïƒ2
Bl,h),
with diagonal posterior covariances, i.e.,
Ë˜Î£A =
â›âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ
Î£A,1
0
Â· Â· Â·
0
0
Î£A,2
...
...
...
0
0
Â· Â· Â·
0
Î£A,M
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
,
Ë˜Î£B =
â›âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ
Î£B,1
0
Â· Â· Â·
0
0
Î£B,2
...
...
...
0
0
Â· Â· Â·
0
Î£B,L
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
,
for
Î£A,m = Diag(Ïƒ2
Am,1,. . . , Ïƒ2
Am,H),
Î£B,l = Diag(Ïƒ2
Bl,1,. . . , Ïƒ2
Bl,H).
The stationary conditions are given as follows: for all l = 1,. . . , L,
m = 1,. . . , M, and h = 1,. . . , H,
Am,h =
Ïƒ2
Am,h
Ïƒ2

l;(l,m)âˆˆÎ›
â›âœâœâœâœâœâœâVl,m âˆ’

hâ€²h
Bl,hâ€² Am,hâ€²
ââŸâŸâŸâŸâŸâŸâ Bl,h,
(3.61)
Ïƒ2
Am,h = Ïƒ2
â›âœâœâœâœâœâœâ

l;(l,m)âˆˆÎ›
B2
l,h + Ïƒ2
Bl,h
 
+ Ïƒ2
c2ah
ââŸâŸâŸâŸâŸâŸâ 
âˆ’1
,
(3.62)
Bl,h =
Ïƒ2
Bl,h
Ïƒ2

m;(l,m)âˆˆÎ›
â›âœâœâœâœâœâœâVl,m âˆ’

hâ€²h
Am,hâ€²Bl,hâ€²
ââŸâŸâŸâŸâŸâŸâ Am,h,
(3.63)
Ïƒ2
Bl,h = Ïƒ2
â›âœâœâœâœâœâœâ

m;(l,m)âˆˆÎ›
A2
m,h + Ïƒ2
Am,h
 
+ Ïƒ2
c2
bh
ââŸâŸâŸâŸâŸâŸâ 
âˆ’1
.
(3.64)
The free energy is given by Eq. (3.55) with the posterior covariances
{Î£A,m, Î£B,l} restricted to be diagonal. The stationary conditions for the hyper-
parameters are unchanged, and given by Eqs. (3.56) through (3.58). Therefore,
Algorithm 2 with Eqs. (3.61) through (3.64) substituted for Eqs. (3.51) through
(3.54) gives a local solution to the VB problem (3.60) with the columnwise
independence constraint.
SimpleVB learning is much more practical when missing entries exist. In
the fully observed case, the posterior covariances Î£A and Î£B are common
to all rows of A and to all rows of B, respectively, while in the partially

80
3 VB Algorithm for Multilinear Models
observed case, we need to store the posterior covariances Î£A,m and Î£B,l for
all m = 1,. . . , M and l = 1,. . . , L. Since L and M can be huge, e.g., in
collaborative ï¬ltering applications, the required memory size is signiï¬cantly
reduced by restricting the covariances to be diagonal.
3.3 Tensor Factorization
A matrix is a two-dimensional array of numbers. We can extend this notion to
an N-dimensional array, which is called an N-mode tensor. Namely, a tensor
V âˆˆRM(1)Ã—Â·Â·Â·Ã—M(N) consists of N
n=1 M(n) entries lying in an N-dimensional
array, where M(n) denotes the length in mode n. In this section, we derive VB
learning for tensor factorization.
3.3.1 Tucker Factorization
Similarly to the rank of a matrix, we can control the degree of freedom of
a tensor by controlling its tensor rank. Although there are a few different
deï¬nitions of the tensor rank and corresponding ways of factorization, we here
focus on Tucker factorization (TF) (Tucker, 1996; Kolda and Bader, 2009)
deï¬ned as follows:
V = G Ã—1 A(1) Â· Â· Â· Ã—N A(N) + E,
where V âˆˆRM(1)Ã—Â·Â·Â·Ã—M(N), G âˆˆRH(1)Ã—Â·Â·Â·Ã—H(N), and {A(n) âˆˆRM(n)Ã—H(n)} are
an observed tensor, a core tensor, and factor matrices, respectively. E âˆˆ
RM(1)Ã—Â·Â·Â·Ã—M(N) is noise and Ã—n denotes the n-mode tensor product. Parafac
(Harshman, 1970), another popular way of tensor factorization, can be seen as
a special case of Tucker factorization where the core tensor is superdiagonal,
i.e., only the entries Gh(1),...,h(N) for h(1) = h(2) = Â· Â· Â· = h(N) are nonzero.
3.3.2 VB Learning for TF
The probabilistic model for MF is straightforwardly extended to TF (Chu and
Ghahramani, 2009; MÃ¸rup and Hansen, 2009). Assume Gaussian noise and
Gaussian priors:
p(V|G, {A(n)}) âˆexp
â›âœâœâœâœâœâœââˆ’
###V âˆ’G Ã—1 A(1) Â· Â· Â· Ã—N A(N)###2
2Ïƒ2
ââŸâŸâŸâŸâŸâŸâ ,
(3.65)

3.3 Tensor Factorization
81
p(G) âˆexp

âˆ’vec(G)âŠ¤(CG(N) âŠ—Â· Â· Â· âŠ—CG(1))âˆ’1 vec(G)
2

,
(3.66)
p({A(n)}) âˆexp
â›âœâœâœâœâœââˆ’
N
n=1 tr(A(n)Câˆ’1
A(n) A(n)âŠ¤)
2
ââŸâŸâŸâŸâŸâ ,
(3.67)
where âŠ—and vec(Â·) denote the Kronecker product and the vectorization
operator, respectively. {CG(n)} and {CA(n)} are the prior covariances restricted
to be diagonal, i.e.,
CG(n) = Diag

c2
g(n)
1 ,. . . , c2
g(n)
H(n)

,
CA(n) = Diag

c2
a(n)
1 ,. . . , c2
a(n)
H(n)

.
We denote Ë˜CG = CG(N) âŠ—Â· Â· Â· âŠ—CG(1).
Conditional Conjugacy
Since the TF model is multilinear, the likelihood (3.65) is in the Gaussian form
of the core tensor G and of each of the factor matrices {A(n)}, if the others
are ï¬xed as constants. Therefore, the Gaussian priors (3.66) and (3.67) are
conditionally conjugate for each parameter, and the posterior will be Gaussian.
Variational Bayesian Algorithm
Based on the conditional conjugacy, we impose the following constraint on the
VB posterior:
r(G, {A(n)}) = r(G)
N

n=1
r(A(n)).
Then, the free energy can be written as
F(r) =

r(G)
â›âœâœâœâœâœâ
N

n=1
r(A(n))
ââŸâŸâŸâŸâŸâ 
â›âœâœâœâœâœâlog p(V, G, {A(n)}) âˆ’log r(G) âˆ’
N

n=1
log r(A(n))
ââŸâŸâŸâŸâŸâ 
Â· dG
â›âœâœâœâœâœâ
N

n=1
dA(n)
ââŸâŸâŸâŸâŸâ .
(3.68)
Using the variational method, we obtain
0 =
 â›âœâœâœâœâœâ
N

n=1
r(A(n))
ââŸâŸâŸâŸâŸâ 
â›âœâœâœâœâœâlog p(V, G, {A(n)}) âˆ’log r(G) âˆ’
N

n=1
log r(A(n)) âˆ’1
ââŸâŸâŸâŸâŸâ 
Â·
â›âœâœâœâœâœâ
N

n=1
dA(n)
ââŸâŸâŸâŸâŸâ ,

82
3 VB Algorithm for Multilinear Models
and therefore
r(G) âˆexpâŸ¨log p(V, G, {A(n)})âŸ©r({A(n)})
âˆp(G) expâŸ¨log p(V|G, {A(n)})âŸ©r({A(n)}).
(3.69)
Similarly, we can also obtain
0 =

r(G)
â›âœâœâœâœâœâ

nâ€²n
r(A(n))
ââŸâŸâŸâŸâŸâ 
â›âœâœâœâœâœâlog p(V, G, {A(n)}) âˆ’log r(G) âˆ’
N

n=1
log r(A(n)) âˆ’1
ââŸâŸâŸâŸâŸâ 
Â·
â›âœâœâœâœâœâ

nâ€²n
dA(n)
ââŸâŸâŸâŸâŸâ ,
and therefore
r(A(n)) âˆexpâŸ¨log p(V, G, {A(n)})âŸ©r(G)r({A(nâ€²)}nâ€²n)
âˆp(A(n)) expâŸ¨log p(V|G, {A(n)})âŸ©r(G)r({A(nâ€²)}nâ€²n).
(3.70)
Eqs. (3.69) and (3.70) imply that the VB posteriors are Gaussian. The
expectation in Eq. (3.69) can be calculated as follows:
âŸ¨log p(V|G, {A(n)})âŸ©r({A(n)})
= âˆ’1
2Ïƒ2
*###V âˆ’G(Ã—1 A(1)) Â· Â· Â· (Ã—N A(N))
###2+
r({A(n)}) + const.
= âˆ’1
2Ïƒ2

âˆ’2vec(V)âŠ¤(A(N) âŠ—Â· Â· Â· âŠ—A(1))vec(G)
+ vec(G)âŠ¤(A(N)âŠ¤A(N) âŠ—Â· Â· Â· âŠ—A(1)âŠ¤A(1))vec(G)

r({A(n)}) + const.
Substituting the preceding calculation and the prior (3.66) into Eq. (3.69) gives
log r(G) = log p(G)âŸ¨log p(V|G, {A(n)})âŸ©r({A(n)}) + const.
= âˆ’1
2(Ë˜g âˆ’Ë˜g)âŠ¤Ë˜Î£
âˆ’1
G (Ë˜g âˆ’Ë˜g) + const.,
where
Ë˜g = vec(G),
Ë˜v = vec(V),
Ë˜g =
Ë˜Î£G
Ïƒ2
!
A
(N) âŠ—Â· Â· Â· âŠ—A
(1)"âŠ¤
Ë˜v,
(3.71)
Ë˜Î£G = Ïƒ2 !
A(N)âŠ¤A(N) âŠ—Â· Â· Â· âŠ—A(1)âŠ¤A(1)
r({A(n)}) + Ïƒ2 Ë˜C
âˆ’1
G
"âˆ’1
.
(3.72)

3.3 Tensor Factorization
83
Similarly, the expectation in Eq. (3.70) can be calculated as follows:
âŸ¨log p(V|G, A(n))âŸ©r(G)r({A(nâ€²)}nâ€²n)
= âˆ’1
2Ïƒ2
*###V âˆ’G(Ã—1A(1)) Â· Â· Â· (Ã—N A(N))
###2+
r(G)r({A(nâ€²)}nâ€²n) + const.
= âˆ’1
2Ïƒ2
)
tr
!
âˆ’2VâŠ¤
(n) A(n)G(n)(A
(N) âŠ—Â· Â· Â· âŠ—A
(n+1) âŠ—A
(nâˆ’1) Â· Â· Â· âŠ—A
(1))âŠ¤"
+ tr

A(n) 
G(n)(A(N)âŠ¤A(N) âŠ—Â· Â· Â· âŠ—A(n+1)âŠ¤A(n+1)
âŠ—A(nâˆ’1)âŠ¤A(nâˆ’1) Â· Â· Â· âŠ—A(1)âŠ¤A(1))GâŠ¤
(n)

r(G)r({A(nâ€²)}nâ€²n) A(n)âŠ¤" 1
.
Substituting the preceding calculation and the prior (3.67) into Eq. (3.70) gives
log r(A(n)) = log p(A(n)) expâŸ¨log p(V|G, {A(n)})âŸ©r(G)r({A(nâ€²)}nâ€²n) + const.
= âˆ’1
2tr
!
(A(n) âˆ’A
(n))Î£
âˆ’1
A(n)(A(n) âˆ’A
(n))âŠ¤"
,
where
A
(n) = 1
Ïƒ2 V(n)(A
(N) âŠ—Â· Â· Â· âŠ—A
(n+1) âŠ—A
(nâˆ’1) Â· Â· Â· âŠ—A
(1))G
âŠ¤
(n)Î£ A(n),
(3.73)
Î£ A(n) = Ïƒ2 
G(n)(A(N)âŠ¤A(N) âŠ—Â· Â· Â· âŠ—A(n+1)âŠ¤A(n+1)
âŠ—A(nâˆ’1)âŠ¤A(nâˆ’1) âŠ—Â· Â· Â· âŠ—A(1)âŠ¤A(1))GâŠ¤
(n)

r(G)r({A(nâ€²)}nâ€²n) + Ïƒ2Câˆ’1
A(n)
"âˆ’1
.
(3.74)
Thus, the VB posterior is given by
r(G, {A(n)}) = GaussN
n=1 H(n)
!
Ë˜g;Ë˜g, Ë˜Î£G
"
Â·
N

n=1
MGaussM(n),H(n)
!
A(n); A
(n), IM(n) âŠ—Î£ A(n)
"
,
(3.75)
where the means and the covariances satisfy
Ë˜g =
Ë˜Î£G
Ïƒ2
!
A
(N) âŠ—Â· Â· Â· âŠ—A
(1)"âŠ¤
Ë˜v,
(3.76)
Ë˜Î£G = Ïƒ2
 !
A
(N)âŠ¤A
(N) + M(N)Î£ A(N)
"
âŠ—Â· Â· Â· âŠ—
!
A
(1)âŠ¤A
(1) + M(1)Î£A(1)
"
+ Ïƒ2 Ë˜C
âˆ’1
G
âˆ’1
,
(3.77)

84
3 VB Algorithm for Multilinear Models
A
(n) = 1
Ïƒ2 V(n)(A
(N) âŠ—Â· Â· Â· âŠ—A
(n+1) âŠ—A
(nâˆ’1) Â· Â· Â· âŠ—A
(1))G
âŠ¤
(n)Î£ A(n),
(3.78)
Î£ A(n) = Ïƒ2
*
G(n)
!
(A
(N)âŠ¤A
(N)+M(N)Î£ A(N)) âŠ—Â· Â· Â· âŠ—(A
(n+1)âŠ¤A
(n+1)+M(n+1)Î£ A(n+1))
âŠ—(A
(nâˆ’1)âŠ¤A
(nâˆ’1)+M(nâˆ’1)Î£ A(nâˆ’1)) âŠ—Â· Â· Â· âŠ—(A
(1)âŠ¤A
(1)+M(1)Î£ A(1))
"
GâŠ¤
(n)
+
r(G)
+ Ïƒ2Câˆ’1
A(n)
âˆ’1
.
(3.79)
The expectation in Eqs. (3.79) is explicitly given by
*
G(n)
!
(A
(N)âŠ¤A
(N)+M(N)Î£ A(N)) âŠ—Â· Â· Â· âŠ—(A
(n+1)âŠ¤A
(n+1)+M(n+1)Î£ A(n+1))
âŠ—(A
(nâˆ’1)âŠ¤A
(nâˆ’1)+M(nâˆ’1)Î£ A(nâˆ’1)) âŠ—Â· Â· Â· âŠ—(A
(1)âŠ¤A
(1)+M(1)Î£ A(1))
"
GâŠ¤
(n)
+
r(G)
= G(n)
!
(A
(N)âŠ¤A
(N)+M(N)Î£ A(N)) âŠ—Â· Â· Â· âŠ—(A
(n+1)âŠ¤A
(n+1)+M(n+1)Î£ A(n+1))
âŠ—(A
(nâˆ’1)âŠ¤A
(nâˆ’1)+M(nâˆ’1)Î£ A(nâˆ’1)) âŠ—Â· Â· Â· âŠ—(A
(1)âŠ¤A
(1)+M(1)Î£ A(1))
"
G
âŠ¤
(n) + Î(n),
(3.80)
where the entries of Î(n) âˆˆRH(n)Ã—H(n) are speciï¬ed as
Î(n)
h(n),hâ€²(n) =

(h(1),hâ€²(1)),...,(h(nâˆ’1),hâ€²(nâˆ’1)),(h(n+1),hâ€²(n+1)),...,(h(N),hâ€²(N))
(A
(N)âŠ¤A
(N)+M(N)Î£ A(N))h(N),hâ€²(N)
Â· Â· Â· (A
(n+1)âŠ¤A
(n+1)+M(n+1)Î£ A(n+1))h(n+1),hâ€²(n+1)(A
(nâˆ’1)âŠ¤A
(nâˆ’1)+M(nâˆ’1)Î£ A(nâˆ’1))h(nâˆ’1),hâ€²(nâˆ’1)
Â· Â· Â· (A
(1)âŠ¤A
(1) + M(1)Î£ A(1))h(1),hâ€²(1)(Î£G)(h(1),hâ€²(1)),...,(h(N),hâ€²(N)).
Here, we used the tensor expression of Î£G âˆˆR
N
n=1 2H(n) for the core posterior
covariance Ë˜Î£G.
Free Energy as a Function of Variational Parameters
Substituting Eq. (3.75) into Eq. (3.68), we have
2F = 2
/
log r(G) +
N

n=1
log r(A(n))
âˆ’log p(V|G, {A(n)})p(G)
N

n=1
p(A(n))
0
r(G)(N
n=1 r(A(n)))

3.3 Tensor Factorization
85
=
â›âœâœâœâœâœâ
N

n=1
M(n)
ââŸâŸâŸâŸâŸâ log(2Ï€Ïƒ2) + log
det
 Ë˜CG
 
det
!Ë˜Î£G
" +
N

n=1
M(n) log det (CA(n))
det
Î£ A(n)
 
+ âˆ¥Vâˆ¥2
Ïƒ2
âˆ’
N

n=1
H(n) âˆ’
N

n=1
(M(n)H(n))
+ tr
!
Ë˜C
âˆ’1
G (Ë˜gË˜g
âŠ¤+ Ë˜Î£G)
"
+
N

n=1
tr
!
Câˆ’1
A(n)(A
(n)âŠ¤A
(n) + M(n)Î£ A(n))
"
âˆ’2
Ïƒ2 Ë˜vâŠ¤(A
(N) âŠ—Â· Â· Â· âŠ—A
(1))Ë˜g
+ 1
Ïƒ2 tr
2!
(A
(N)âŠ¤A
(N) + M(N)Î£ A(N)) âŠ—Â· Â· Â· âŠ—(A
(1)âŠ¤A
(1) + M(1)Î£ A(1))
"
Â·(Ë˜gË˜g
âŠ¤+ Ë˜Î£G)
3
.
(3.81)
Empirical Variational Bayesian Algorithm
The derivative of the free energy (3.81) with respect to Ë˜CG gives
2 âˆ‚F
âˆ‚Ë˜CG
= M(n) !
Ë˜C
âˆ’1
G âˆ’Ë˜C
âˆ’2
G
!
Ë˜gË˜gâŠ¤+ Ë˜Î£G
""
.
Since it holds that
âˆ‚Ë˜CG
âˆ‚(CG(n))h,h
= CG(N) âŠ—Â· Â· Â· âŠ—CG(n+1) âŠ—E(H(n),h,h) âŠ—CG(nâˆ’1) âŠ—Â· Â· Â· âŠ—CG(1),
where E(H,h,hâ€²) âˆˆRHÃ—H is the matrix with the (h, hâ€²)th entry equal to one and
the others equal to zero, we have
2
âˆ‚F
âˆ‚(CG(n))h,h
= 2tr
 âˆ‚F
âˆ‚Ë˜CG
âˆ‚Ë˜CG
âˆ‚(CG(n))h,h

= M(n)
######vec

IH(N) âŠ—Â· Â· Â· âŠ—IH(n+1) âŠ—(CG(n))âˆ’1
h,hE(H(n),h,h) âŠ—IH(nâˆ’1) âŠ—Â· Â· Â· âŠ—IH(1)
âˆ’Câˆ’1
G(N) âŠ—Â· Â· Â· âŠ—Câˆ’1
G(n+1) âŠ—(CG(n))âˆ’2
h,hE(H(n),h,h) âŠ—Câˆ’1
G(nâˆ’1) âŠ—Â· Â· Â· âŠ—Câˆ’1
G(1)
!
Ë˜gË˜gâŠ¤+Ë˜Î£G
"" ######1
= M(n)(CG(n))âˆ’2
h,h
######vec

IH(N) âŠ—Â· Â· Â· âŠ—IH(n+1) âŠ—(CG(n))h,hE(H(n),h,h)âŠ—IH(nâˆ’1) âŠ—Â· Â· Â· âŠ—IH(1)
âˆ’Câˆ’1
G(N) âŠ—Â· Â· Â· âŠ—Câˆ’1
G(n+1) âŠ—E(H(n),h,h) âŠ—Câˆ’1
G(nâˆ’1) âŠ—Â· Â· Â· âŠ—Câˆ’1
G(1)
!
Ë˜gË˜gâŠ¤+ Ë˜Î£G
"" ######1
= M(n)(CG(n))âˆ’2
h,h
 
nâ€²n H(nâ€²) 
(CG(n))h,h âˆ’diag
!
Ë˜gË˜gâŠ¤+ Ë˜Î£G
"âŠ¤
diag

Câˆ’1
G(N) âŠ—Â· Â· Â· âŠ—Câˆ’1
G(n+1) âŠ—E(H(n),h,h) âŠ—Câˆ’1
G(nâˆ’1) âŠ—Â· Â· Â· âŠ—Câˆ’1
G(1)
 
,
(3.82)

86
3 VB Algorithm for Multilinear Models
Algorithm 3 EVB learning for Tucker factorization.
1: Initialize the variational parameters (Ë˜g, Ë˜Î£G, {A
(n)}, {Î£ A(n)}), and the hyper-
parameters ({CG(n)}, {CA(n)}, Ïƒ2), for example, Ë˜gh âˆ¼Gauss1(0, Ï„), A
(n)
m,h âˆ¼
Gauss1

0, Ï„1/N 
, Ë˜Î£G = Ï„IN
n=1 H(n), CG(n) = Ï„IH(n), Î£ A(n) = CA(n) = Ï„1/NIH(n),
and Ïƒ2 = Ï„2 for Ï„2 = âˆ¥Vâˆ¥2/N
n=1 M(n).
2: Apply (substitute the right-hand side into the left-hand side) Eqs. (3.77),
(3.76), (3.79), and (3.78) to update Ë˜Î£G,Ë˜g, {Î£ A(n)}, and {A
(n)}, respectively.
3: Apply Eqs. (3.83) through (3.85) to update {CG(n)}, {CA(n)}, and Ïƒ2, respec-
tively.
4: Prune the hth component if c2
g(n)
h c2
a(n)
h < Îµ, where Îµ > 0 is a threshold, e.g.,
set to Îµ = 10âˆ’4.
5: Evaluate the free energy (3.81).
6: Iterate Steps 2 through 5 until convergence (until the energy decrease
becomes smaller than a threshold).
where âˆ¥Â· âˆ¥1 denotes the â„“1-norm, and diag(Â·) denotes the column vector
consisting of the diagonal entries of a matrix. Thus, the prior covariance for
the core tensor can be updated by
c2
g(n)
h =
diag
!
Ë˜gË˜gâŠ¤+Ë˜Î£G
"âŠ¤
diag

Câˆ’1
G(N)âŠ—Â·Â·Â·âŠ—Câˆ’1
G(n+1)âŠ—E(H(n),h,h)âŠ—Câˆ’1
G(nâˆ’1)âŠ—Â·Â·Â·âŠ—Câˆ’1
G(1)
 

nâ€²n H(nâ€²)
.
(3.83)
The derivative of the free energy (3.81) with respect to CA(n) gives
2 âˆ‚F
âˆ‚c2
a(n)
h
= M(n)
â›âœâœâœâœâœâcâˆ’2
a(n)
h âˆ’câˆ’4
a(n)
h
â›âœâœâœâœâœâ
âˆ¥a(n)
h âˆ¥2
M(n)
+ (Î£ A(n))h,h
ââŸâŸâŸâŸâŸâ 
ââŸâŸâŸâŸâŸâ ,
which leads to the following update rule for the prior covariance for the factor
matrices:
c2
a(n)
h = âˆ¥a(n)
h âˆ¥2
M(n)
+ (Î£ A(n))h,h.
(3.84)
Finally, the derivative of the free energy (3.81) with respect to Ïƒ2 gives
2 âˆ‚F
âˆ‚Ïƒ2 =
N
n=1 M(n)
Ïƒ2
âˆ’1
Ïƒ4

âˆ¥Vâˆ¥2 âˆ’2Ë˜vâŠ¤(A
(N) âŠ—Â· Â· Â· âŠ—A
(1))Ë˜g
+ tr
2!
(A
(N)âŠ¤A
(N)+M(N)Î£ A(N)) âŠ—Â· Â· Â· âŠ—(A
(1)âŠ¤A
(1)+M(1)Î£ A(1))
"
(Ë˜gË˜g
âŠ¤+Ë˜Î£G)
3
,

3.4 Low-Rank Subspace Clustering
87
which leads to the update rule for the noise variance as follows:
Ïƒ2 =
1
N
n=1 M(n)

âˆ¥Vâˆ¥2 âˆ’2Ë˜vâŠ¤(A
(N) âŠ—Â· Â· Â· âŠ—A
(1))Ë˜g
+ tr
2!
(A
(N)âŠ¤A
(N)+M(N)Î£ A(N)) âŠ—Â· Â· Â· âŠ—(A
(1)âŠ¤A
(1)+M(1)Î£ A(1))
"
(Ë˜gË˜g
âŠ¤+Ë˜Î£G)
3 
.
(3.85)
Algorithm 3 summarizes the EVB algorithm for Tucker factorization. If we
appropriately set the hyperparameters ({CG(n)}, {CA(n)}, Ïƒ2) in Step 1 and skip
Steps 3 and 4, Algorithm 3 is reduced to (nonempirical) VB learning.
3.4 Low-Rank Subspace Clustering
PCA globally embeds data points into a low-dimensional subspace. As more
ï¬‚exible tools, subspace clustering methods, which locally embed the data into
the union of subspaces, have been developed. In this section, we derive VB
learning for subspace clustering.
3.4.1 Subspace Clustering Methods
Most clustering methods, such as k-means (MacQueen, 1967; Lloyd, 1982)
and spectral clustering (Shi and Malik, 2000), (explicitly or implicitly) assume
that there are sparse areas between dense areas, and separate the dense areas as
clusters (Figure 3.2 left). On the other hand, there are some data, e.g., projected
trajectories of points on a rigid body in 3D space, where data points can be
assumed to lie in a union of small dimensional subspaces (Figure 3.2 right).
Note that a point lying in a subspace is not necessarily far from a point lying in
another subspace if those subspaces intersect each other. Subspace clustering
methods have been developed to analyze this kind of data.
Figure 3.2 Clustering (left) and subspace clustering (right).

88
3 VB Algorithm for Multilinear Models
Let V = (v1,. . . , vM) âˆˆRLÃ—M be L-dimensional observed samples of size
M. We assume that each vm is approximately expressed as a linear combination
of Mâ€² words in a dictionary, D = (d1,. . . , dMâ€²) âˆˆRLÃ—Mâ€², i.e.,
V = DU + E,
where U âˆˆRMâ€²Ã—M is unknown coefï¬cients, and E âˆˆRLÃ—M is noise. In subspace
clustering, the observed matrix V itself is often used as a dictionary D. Then,
a convex formulation of the sparse subspace clustering (SSC) (Soltanolkotabi
and Cand`es, 2011; Elhamifar and Vidal, 2013) is given by
min
U âˆ¥V âˆ’VUâˆ¥2
Fro + Î» âˆ¥Uâˆ¥1 , s.t. diag(U) = 0,
(3.86)
where U âˆˆRMÃ—M is a parameter to be estimated, Î» > 0 is a regularization
coefï¬cient. âˆ¥Â·âˆ¥1 is the â„“1-norm of a matrix. The ï¬rst term in Eq. (3.86)
together with the constraint requires that each data point vm is accurately
expressed as a linear combination of other data points, {vmâ€²} for mâ€²  m. The
second term, which is the â„“1-regularizer, enforces that the number of samples
contributing to the linear combination should be small, which leads to low-
dimensionality of each obtained subspace. After the solution U to the problem
(3.86) is obtained, the matrix abs(U)+abs(U
âŠ¤), where abs(Â·) takes the absolute
value elementwise, is regarded as an afï¬nity matrix, and a spectral clustering
algorithm, such as the normalized cuts (Shi and Malik, 2000), is applied to
obtain clusters.
Another popular method for subspace clustering is low-rank subspace
clustering (LRSC) or low-rank representation (Liu et al., 2010; Liu and Yan,
2011; Liu et al., 2012; Vidal and Favaro, 2014), where low-dimensional
subspaces are sought by enforcing the low-rankness of U with the trace norm:
min
U âˆ¥V âˆ’VUâˆ¥2
Fro + Î» âˆ¥Uâˆ¥tr .
(3.87)
Since LRSC enforces the low-rankness of U, the constraint diag(U) = 0 is not
necessary, which makes its optimization problem (3.87) signiï¬cantly simpler
than the optimization problem (3.86) for SSC. Thanks to this simplicity, the
global solution of Eq. (3.87) has been analytically obtained (Vidal and Favaro,
2014).
Good properties of SSC and LRSC have been theoretically shown (Liu
et al., 2010, 2012; Soltanolkotabi and Cand`es, 2011; Elhamifar and Vidal,
2013; Vidal and Favaro, 2014). It is observed that they behave differently
in different situations, and each of SSC and LRSC shows advantages and
disadvantages over the other, i.e., neither SSC nor LRSC is known to dominate
the other in the general situations. In the rest of this section, we focus on LRSC
and derive its VB learning algorithm.

3.4 Low-Rank Subspace Clustering
89
3.4.2 VB Learning for LRSC
We start with the following probabilistic model, of which the maximum
a posteriori (MAP) estimator coincides with the solution to the convex
formulation (3.87) under a certain hyperparameter setting:
p(V|A, B) âˆexp

âˆ’1
2Ïƒ2
###V âˆ’VBAâŠ¤###2
Fro

,
(3.88)
p(A) âˆexp

âˆ’1
2tr(ACâˆ’1
A AâŠ¤)

,
(3.89)
p(B) âˆexp

âˆ’1
2tr(BCâˆ’1
B BâŠ¤)

.
(3.90)
Here, we factorized U as U = BAâŠ¤, where A âˆˆRMÃ—H and B âˆˆRMÃ—H for
H â‰¤min(L, M) are the parameters to be estimated (Babacan et al., 2012a). This
factorization is known to induce low-rankness through the MIR mechanism,
which will be discussed in Chapter 7. We assume that the prior covariances are
diagonal:
CA = Diag(c2
a1,. . . , c2
aH),
CB = Diag(c2
b1,. . . , c2
bH).
Conditional Conjugacy
The model likelihood (3.88) of LRSC is similar to the model likelihood (3.1)
of MF, and it is in the Gaussian form with respect to A if B is regarded as a
constant, or vice versa. Therefore, the priors (3.89) and (3.90) are conditionally
conjugate for A and B, respectively.
Variational Bayesian Algorithm
The conditional conjugacy implies that the following independence constraint
on the approximate posterior leads to a tractable algorithm:
r(A, B) = r(A)r(B).
In the same way as MF, we can show that the VB posterior is Gaussian in the
following form:
r(A) âˆexp
â›âœâœâœâœâœâœâœâœâœâœâ
âˆ’
tr
!
(A âˆ’A)Î£
âˆ’1
A (A âˆ’A)âŠ¤
"
2
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
,
r(B) âˆexp
â›âœâœâœâœâœâœâœâœââˆ’(Ë˜b âˆ’Ë˜b)âŠ¤Ë˜Î£
âˆ’1
B (Ë˜b âˆ’Ë˜b)
2
ââŸâŸâŸâŸâŸâŸâŸâŸâ ,
(3.91)

90
3 VB Algorithm for Multilinear Models
where Ë˜b = vec(B) âˆˆRMH. The variational parameters satisfy the following
stationary conditions:
A = 1
Ïƒ2 VâŠ¤VBÎ£ A,
(3.92)
Î£ A = Ïƒ2 !
BâŠ¤VâŠ¤VB

r(B) + Ïƒ2Câˆ’1
A
"âˆ’1
,
(3.93)
Ë˜b =
Ë˜Î£B
Ïƒ2 vec

VâŠ¤VA
 
,
(3.94)
Ë˜Î£B = Ïƒ2 !
(A
âŠ¤A + MÎ£ A) âŠ—VâŠ¤V + Ïƒ2(Câˆ’1
B âŠ—IM)
"âˆ’1
,
(3.95)
where the entries of BâŠ¤VâŠ¤VB
r(B) in Eq. (3.93) are explicitly given by
!
BâŠ¤VâŠ¤VB

r(B)
"
h,hâ€² =
!
B
âŠ¤VâŠ¤VB
"
h,hâ€² + tr
!
VâŠ¤VÎ£
(h,hâ€²)
B
"
.
(3.96)
Here Î£
(h,hâ€²)
B
âˆˆRMÃ—M is the (h, hâ€²)th block matrix of Ë˜Î£B âˆˆRMHÃ—MH, i.e.,
Ë˜Î£B =
â›âœâœâœâœâœâœâœâœâœâœâœâœâœâ
Î£
(1,1)
B
Â· Â· Â·
Î£
(1,H)
B
...
...
...
Î£
(H,1)
B
Â· Â· Â·
Î£
(H,H)
B
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
.
Free Energy as a Function of Variational Parameters
The free energy can be explicitly written as
2F = LM log(2Ï€Ïƒ2) +
####Vâˆ’VBA
âŠ¤####
2
Fro
Ïƒ2
+ M log det(CA)
det
Î£ A
 + log det(CBâŠ—IM)
det
!Ë˜Î£B
"
âˆ’2MH + tr
2
Câˆ’1
A
!
A
âŠ¤A + MÎ£ A
"3
+ tr
2
Câˆ’1
B B
âŠ¤B
3
+ tr
2
(Câˆ’1
B âŠ—IM)Ë˜Î£B
3
+ tr
)
Ïƒâˆ’2VâŠ¤V

âˆ’BA
âŠ¤AB
âŠ¤+
*
B(A
âŠ¤A + MÎ£ A)BâŠ¤+
r(B)
1
,
(3.97)
where the expectation in the last term is given by
*
B(A
âŠ¤A + MÎ£ A)B
+âŠ¤
r(B)

m,mâ€² =
!
B(A
âŠ¤A + MÎ£ A)B
âŠ¤"
m,mâ€²
+ tr

(A
âŠ¤A + MÎ£ A)Â´Î£
(m,mâ€²)
B

.
(3.98)

3.4 Low-Rank Subspace Clustering
91
Algorithm 4 EVB learning for low-rank subspace clustering.
1: Initialize the variational parameters (A, Î£ A, B, Ë˜Î£B), and the hyperpara-
meters (CA, CB, Ïƒ2), for example, Am,h, Bm,h âˆ¼Gauss1(0, 12), Î£ A = CA =
CB = IH, Ë˜Î£B = IMH, and Ïƒ2 = âˆ¥Vâˆ¥2/(LM).
2: Apply (substitute the right-hand side into the left-hand side) Eqs. (3.93),
(3.92), (3.95), and (3.94) to update Î£ A, A, Ë˜Î£B, and B, respectively.
3: Apply Eqs. (3.99) through (3.101) to update CA, CB, and Ïƒ2, respectively.
4: Prune the hth component if c2
ahc2
bh < Îµ, where Îµ > 0 is a threshold, e.g., set
to Îµ = 10âˆ’4.
5: Evaluate the free energy (3.97).
6: Iterate Steps 2 through 5 until convergence (until the energy decrease
becomes smaller than a threshold).
Here, Â´Î£
(m,mâ€²)
B
âˆˆRHÃ—H is deï¬ned as
Â´Î£
(m,mâ€²)
B
=
â›âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ
!
Î£
(1,1)
B
"
m,mâ€²
Â· Â· Â·
!
Î£
(1,H)
B
"
m,mâ€²
...
...
...
!
Î£
(H,1)
B
"
m,mâ€²
Â· Â· Â·
!
Î£
(H,H)
B
"
m,mâ€²
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
.
Empirical Variational Bayesian Algorithm
By differentiating the free energy (3.97) with respect to the hyperparameters,
we can obtain the stationary conditions for the hyperparameters:
c2
ah =
###ah
###2 /M +
Î£ A
 
h,h ,
(3.99)
c2
bh =
!####bh
####
2
+ tr
!
Î£
(h,h)
B
""
/M,
(3.100)
Ïƒ2 =
tr

VâŠ¤V

IM âˆ’2BA
âŠ¤+
*
B(A
âŠ¤A + MÎ£ A)BâŠ¤+
r(B)

LM
.
(3.101)
Algorithm 4 summarizes the EVB algorithm. If we appropriately set the
hyperparameters (CA, CB, Ïƒ2) in Step 1 and skip Steps 3 and 4, Algorithm 4 is
reduced to (nonempirical) VB learning.
Variational Bayesian Algorithm under the Kronecker
Product Covariance Constraint
The standard VB algorithm, given in Algorithm 4, for LRSC requires the
inversion of an MH Ã— MH matrix, which is prohibitively huge in practical

92
3 VB Algorithm for Multilinear Models
applications. As a remedy, Babacan et al. (2012a) proposed to restrict the
posterior r(B) for B to be the matrix variate Gaussian with the Kronecker
product covariance (KPC) structure, as Eq. (3.18). Namely, we restrict the
approximate posterior to be in the following form:
r(B) = MGaussM,H(B; B, Î¨ B âŠ—Î£B)
âˆexp

âˆ’1
2tr
!
Î¨
âˆ’1
B (B âˆ’B)Î£
âˆ’1
B (B âˆ’B)âŠ¤"
.
(3.102)
Under this additional constraint, the free energy is written as
2FKPC = LM log(2Ï€Ïƒ2) +
####V âˆ’VBA
âŠ¤####
2
Ïƒ2
+ M log det (CA)
det
Î£ A
 + M log det (CB)
det
Î£B
 
+ H log
1
det
Î¨ B
 âˆ’2MH
+ tr
2
Câˆ’1
A
!
A
âŠ¤A + MÎ£ A
"3
+ tr
2
Câˆ’1
B
!
B
âŠ¤B + tr(Î¨ B)Î£B
"3
+ tr
2
Ïƒâˆ’2VâŠ¤V
!
MBÎ£ AB
âŠ¤+ tr
!
(A
âŠ¤A + MÎ£ A)Î£B
"
Î¨ B
"3
. (3.103)
By differentiating the free energy (3.103) with respect to each variational
parameter, we obtain the following update rules:
A = 1
Ïƒ2 VâŠ¤VBÎ£ A,
(3.104)
Î£ A = Ïƒ2 !
B
âŠ¤VâŠ¤VB + tr

VâŠ¤VÎ¨ B
 Î£B + Ïƒ2Câˆ’1
A
"âˆ’1
,
(3.105)
B
new = B
old âˆ’Î±

B
oldCâˆ’1
B + 1
Ïƒ2 VâŠ¤V
!
âˆ’A + B
old(A
âŠ¤A + MÎ£ A)
"
,
(3.106)
Î£B = Ïƒ2
â›âœâœâœâœâœâœâ
tr

VâŠ¤VÎ¨ B
 
M
(A
âŠ¤A + MÎ£ A) + Ïƒ2tr(Î¨ B)
M
Câˆ’1
B
ââŸâŸâŸâŸâŸâŸâ 
âˆ’1
,
(3.107)
Î¨ B = Ïƒ2
â›âœâœâœâœâœâœâœâœâœâœâ
tr
!
(A
âŠ¤A + MÎ£ A)Î£B
"
H
VâŠ¤V + Ïƒ2tr(Câˆ’1
B Î£B)
H
IM
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
âˆ’1
,
(3.108)
c2
ah = (A
âŠ¤A + MÎ£ A)h,h/M,
(3.109)
c2
bh =
!
B
âŠ¤B + tr(Î¨ B)Î£B
"
h,h /M,
(3.110)
Ïƒ2 =
1
LM tr
!
VâŠ¤V
!
IM âˆ’2BA
âŠ¤+ tr
!
(A
âŠ¤A + MÎ£ A)Î£B
"
Î¨ B
+ B(A
âŠ¤A + MÎ£ A)B
âŠ¤""
.
(3.111)

3.5 Sparse Additive Matrix Factorization
93
Algorithm 5 EVB learning for low-rank subspace clustering under the Kro-
necker product covariance constraint (3.102).
1: Initialize the variational parameters (A, Î£ A, B, Î£B, Î¨ B), and the hyper-
parameters (CA, CB, Ïƒ2), for example, Am,h, Bm,h âˆ¼Gauss1(0, 12), Î£ A =
Î£B = CA = CB = IH, Î¨ B = IM, and Ïƒ2 = âˆ¥Vâˆ¥2/(LM).
2: Apply (substitute the right-hand side into the left-hand side) Eqs. (3.105),
(3.104), (3.107), and (3.108) to update Î£ A, A, Î£B, and Î¨ B, respectively.
3: Apply Eq. (3.106) T times (e.g., T = 20) to update B.
4: Apply Eqs. (3.109) through (3.111) to update CA, CB, and Ïƒ2, respectively.
5: Prune the hth component if c2
ahc2
bh < Îµ, where Îµ > 0 is a threshold, e.g., set
to Îµ = 10âˆ’4.
6: Evaluate the free energy (3.103).
7: Iterate Steps 2 through 6 until convergence (until the energy decrease
becomes smaller than a threshold).
Note that Eq. (3.106) is the gradient descent algorithm for B with the step size
Î± > 0.
Algorithm 5 summarizes the EVB algorithm under the KPC constraint,
which we call KPC approximation (KPCA). If we appropriately set the
hyperparameters (CA, CB, Ïƒ2) in Step 1 and skip Steps 4 and 5, Algorithm 5
is reduced to (nonempirical) VB learning.
3.5 Sparse Additive Matrix Factorization
PCA is known to be sensitive to outliers in data and generally fails in their
presence. To cope with outliers, robust PCA, where spiky noise is captured by
an elementwise sparse term, was proposed (Cand`es et al., 2011). In this section,
we introduce a generalization of robust PCA, called sparse additive matrix
factorization (SAMF) (Nakajima et al., 2013b) and derive its VB learning
algorithm.
3.5.1 Robust PCA and Matrix Factorization
In robust PCA, the observed matrix V âˆˆRLÃ—M is modeled as follows:
V = Ulow-rank + Uelement + E,
(3.112)
where Ulow-rank
âˆˆ
RLÃ—M is a low-rank matrix, Uelement
âˆˆ
RLÃ—M is an
elementwise sparse matrix, and E âˆˆRLÃ—M is a (typically dense) noise matrix.

94
3 VB Algorithm for Multilinear Models
Given the observed matrix V, one can infer each term in the right-hand side of
Eq. (3.112) by solving the following convex problem (Cand`es et al., 2011):
min
Ulow-rank,Uelement
###V âˆ’Ulow-rank âˆ’Uelement###2
Fro + Î»1
###Ulow-rank###tr + Î»2
###Uelement###1 ,
where the trace norm âˆ¥Â·âˆ¥tr induces low-rank sparsity, and the â„“1-norm âˆ¥Â·âˆ¥1
induces elementwise sparsity. The regularization coefï¬cients Î»1 and Î»2 control
the strength of sparsity.
In Bayesian modeling, the low-rank matrix is commonly expressed as the
product of two matrices, A âˆˆRMÃ—H and B âˆˆRLÃ—H:
Ulow-rank = BAâŠ¤=
H

h=1
bhaâŠ¤
h .
(3.113)
Trivially, low-rankness is forced if H is set to a small value. However, when
VB learning is applied, the estimator can be low-rank even if we adopt the full-
rank model, i.e., H = min(L, M). This phenomenon is caused by MIR, which
will be discussed in Chapter 7.
3.5.2 Sparse Matrix Factorization Terms
SAMF (Nakajima et al., 2013b) was proposed as a generalization of robust
PCA, where various types of sparsity are induced by combining different types
of factorization. For example, the following factorization implicitly induces
rowwise, columnwise, and elementwise sparsity, respectively:
Urow = Î“E D = (Î³e
1d1,. . . , Î³e
LdL)âŠ¤,
(3.114)
Ucolumn = EÎ“D = (Î³d
1e1,. . . , Î³d
MeM),
(3.115)
Uelement = E âŠ™D,
(3.116)
where Î“D = Diag(Î³d
1,. . . , Î³d
M) âˆˆRMÃ—M and Î“E = Diag(Î³e
1,. . . , Î³e
L) âˆˆRLÃ—L
are diagonal matrices, and D, E âˆˆRLÃ—M. âŠ™denotes the Hadamard product,
i.e., (E âŠ™D)l,m = El,mDl,m. The reason why the factorizations (3.114) through
(3.116) induce the corresponding types of sparsity is explained in Section 7.5.
As a general expression of sparsity inducing factorizations, we deï¬ne
a sparse matrix factorization (SMF) term with a mapping G consisting of
partitioning, rearrangement, and factorization:
U = G({Uâ€²(k)}K
k=1; X), where Uâ€²(k) = B(k) A(k)âŠ¤.
(3.117)
Here, {A(k), B(k)}K
k=1
are parameters to be estimated, and G(Â·; X)
:
R
K
k=1(Lâ€²(k)Ã—Mâ€²(k)) â†’RLÃ—M is a designed function associated with an index
mapping X (explained shortly).

3.5 Sparse Additive Matrix Factorization
95
U
A
A
A
A
A
B
B
B
B
B
U
U
U
U
U
Figure 3.3 An example of SMF-term construction. G(Â·; X) with X : (k, lâ€², mâ€²) â†’
(l, m) maps the set {Uâ€²(k)}K
k=1 of the PR matrices to the target matrix U, so that
Uâ€²(k)
lâ€²,mâ€² = UX(k,lâ€²,mâ€²) = Ul,m.
Figure 3.3 illustrates how to construct an SMF term. First, we partition the
entries of U into K parts. Then, by rearranging the entries in each part, we form
partitioned-and-rearranged (PR) matrices Uâ€²(k) âˆˆRLâ€²(k)Ã—Mâ€²(k) for k = 1,. . . , K.
Finally, each of Uâ€²(k) is decomposed into the product of A(k) âˆˆRMâ€²(k)Ã—Hâ€²(k) and
B(k) âˆˆRLâ€²(k)Ã—Hâ€²(k), where Hâ€²(k) â‰¤min(Lâ€²(k), Mâ€²(k)).
In Eq. (3.117), the function G(Â·; X) is responsible for partitioning and
rearrangement: it maps the set {Uâ€²(k)}K
k=1 of the PR matrices to the target matrix
U âˆˆRLÃ—M, based on the one-to-one map X : (k, lâ€², mâ€²) â†’(l, m) from the
indices of the entries in {Uâ€²(k)}K
k=1 to the indices of the entries in U such that

G({Uâ€²(k)}K
k=1; X)
 
l,m = Ul,m = UX(k,lâ€²,mâ€²) = Uâ€²(k)
lâ€²,mâ€².
(3.118)
When VB learning is applied, the SMF-term expression (3.117) induces
partitionwise sparsity and low-rank sparsity in each partition. Accordingly,
partitioning, rearrangement, and factorization should be designed in the fol-
lowing way. Suppose that we are given a required sparsity structure on a
matrix (examples of possible side information that suggests particular sparsity
structures are given in Section 3.5.3). We ï¬rst partition the matrix, according to
the required sparsity. Some partitions can be submatrices. We rearrange each
of the submatrices on which we do not want to impose low-rank sparsity into a
long vector (Uâ€²(3) in the example in Figure 3.3). We leave the other submatrices
which we want to be low-rank (Uâ€²(2)) and the original vectors (Uâ€²(1) and Uâ€²(4))
and scalars (Uâ€²(5)) as they are. Finally, we factorize each of the PR matrices to
induce sparsity.
Let us, for example, assume that rowwise sparsity is required. We ï¬rst make
the rowwise partition, i.e., separate U âˆˆRLÃ—M into L pieces of M-dimensional
row vectors Uâ€²(l) = uâŠ¤
l âˆˆR1Ã—M. Then, we factorize each partition as Uâ€²(l) =
B(l) A(l)âŠ¤(see the top illustration in Figure 3.4). Thus, we obtain the rowwise
sparse term (3.114). Here, X(k, 1, mâ€²) = (k, mâ€²) makes the following connection
between Eqs. (3.114) and (3.117): Î³e
l = B(k) âˆˆR,dl = A(k) âˆˆRMÃ—1 for
k = l. Similarly, requiring columnwise and elementwise sparsity leads to

96
3 VB Algorithm for Multilinear Models
Table 3.1 Examples of SMF terms.
Factorization
Induced sparsity
K
(Lâ€²(k), Mâ€²(k))
X : (k, lâ€², mâ€²) â†’(l, m)
U = BAâŠ¤
low-rank
1
(L, M)
X(1, lâ€², mâ€²) = (lâ€², mâ€²)
U = Î“E D
rowwise
L
(1, M)
X(k, 1, mâ€²) = (k, mâ€²)
U = EÎ“D
columnwise
M
(L, 1)
X(k, lâ€², 1) = (lâ€², k)
U = E âŠ™D
elementwise
L Ã— M
(1, 1)
X(k, 1, 1) = vec-order(k)
G
=
U1,1
U1,2
U1,3
U2,1
U2,2
U2,3
(1) = U1,1
U1,2
U1,3 = B (1)A(1)
(2) = U2,1
U2,2
U2,3 = B (2)A(2)
G
=
U1,1
U1,2
U1,3
U2,1
U2,2
U2,3
(1) =
U1,1
U2,1
= B (1)A(1)
(2) =
U1,2
U2,2
= B (2)A(2)
(3) =
U1,3
U2,3
= B (3)A(3)
G
=
U1,1
U1,2
U1,3
U2,1
U2,2
U2,3
(1) = U1,1 = B (1)A(1)
(6) = U2,3 = B (6)A(6)
(2) = U2,1 = B (2)A(2)
(3) = U1,2 = B (3)A(3)
(4) = U2,2 = B (4)A(4)
(5) = U1,3 = B (5)A(5)
U
U
U
U
U
U
U
U
U
U
U
U
U
U
Figure 3.4 SMF-term construction for the rowwise (top), the columnwise
(middle), and the elementwise (bottom) sparse terms.
Eqs. (3.115) and (3.116), respectively (see the bottom two illustrations in
Figure 3.4). Table 3.1 summarizes how to design these SMF terms, where
vec-order(k) = (1 + ((k âˆ’1) mod L), âŒˆk/LâŒ‰) goes along the columns one after
another in the same way as the vec operator forming a vector by stacking the
columns of a matrix (in other words, (Uâ€²(1),. . . , Uâ€²(K))âŠ¤= vec(U)).
Now we deï¬ne the SAMF model as the sum of SMF terms (3.117):
V =
S
s=1
U(s) + E,
(3.119)
where
U(s) = G({B(k,s) A(k,s)âŠ¤}K(s)
k=1; X(s)).
(3.120)
3.5.3 Examples of SMF Terms
In practice, SMF terms should be designed based on side information. Suppose
that V âˆˆRLÃ—M consists of M samples of L-dimensional sensor outputs.
In robust PCA (3.112), we add an elementwise sparse term (3.116) to the
low-rank term (3.113), assuming that the low-rank signal is expected to be

3.5 Sparse Additive Matrix Factorization
97
B
F
Figure 3.5 Foreground/background video separation task.
contaminated with spiky noise when observed. Here, we can say that the
existence of spiky noise is used as side information.
Similarly, if we expect that a small number of sensors can be broken, and
their outputs are unreliable over all M samples, we should add the rowwise
sparse term (3.114) to separate the low-rank signal from rowwise noise:
V = Ulow-rank + Urow + E.
If we expect some accidental disturbances occurred during the observation,
but do not know their exact locations (i.e., which samples are affected), the
columnwise sparse term (3.115) can effectively capture such disturbances.
The SMF expression (3.117) enables us to use side information in a more
ï¬‚exible way, and its advantage has been shown in a foreground/background
video separation problem (Nakajima et al., 2013b). The top image in
Figure 3.5 is a frame of a video available from the Caviar Project website,1
and the task is to separate moving objects (bottom-right) from the background
(bottom-left). Previous approaches (Cand`es et al., 2011; Ding et al., 2011;
Babacan et al., 2012b) ï¬rst constructed the observation matrix V by stacking
all pixels in each frame into each column (Figure 3.6), and then ï¬tted it by
the robust PCA model (3.112). Here, the low-rank term and the elementwise
1 The European Commission (EC)-funded CAVIAR project/IST 2001 37540, found at URL:
http://homepages.inf.ed.ac.uk/rbf/CAVIAR/.

98
3 VB Algorithm for Multilinear Models
V
T
P
Figure 3.6 The observation matrix V is constructed by stacking all pixels in each
frame into each column.
sparse term are expected to capture the static background and the moving
foreground, respectively. However, we can also rely on the natural assumption
that the pixels in a segment sharing similar intensities tend to belong to the
same object. Under this assumption as side information, we can adopt a
segmentwise sparse term, for which the PR matrix is constructed based on
a precomputed oversegmented image (Figure 3.7). The segmentwise sparse
term has been shown to capture the foreground more accurately than the
elementwise sparse term in this application. Details will be discussed in
Chapter 11.
3.5.4 VB Learning for SAMF
Let us summarize the parameters of the SAMF model (3.119) as follows:
Î˜ = {Î˜(s)
A , Î˜(s)
B }S
s=1,
where
Î˜(s)
A = {A(k,s)}K(s)
k=1,
Î˜(s)
B = {B(k,s)}K(s)
k=1.
As in the MF model, we assume independent Gaussian noise and priors. Then,
the likelihood and the priors are given by
p(V|Î˜) âˆexp
â›âœâœâœâœâœâœâœââˆ’1
2Ïƒ2
#######V âˆ’
S
s=1
U(s)
#######
2
Fro
ââŸâŸâŸâŸâŸâŸâŸâ ,
(3.121)

3.5 Sparse Additive Matrix Factorization
99
P
P
T
U
Figure 3.7 Construction of a segmentwise sparse term. The original frame is
presegmented, based on which the segmentwise sparse term is constructed as an
SMF term.
p({Î˜(s)
A }S
s=1) âˆexp

âˆ’1
2
S
s=1
K(s)

k=1
tr

A(k,s)C(k,s)âˆ’1
A
A(k,s)âŠ¤ 
,
(3.122)
p({Î˜(s)
B }S
s=1) âˆexp

âˆ’1
2
S
s=1
K(s)

k=1
tr

B(k,s)C(k,s)âˆ’1
B
B(k,s)âŠ¤ 
.
(3.123)
We assume that the prior covariances of A(k,s) and B(k,s) are diagonal:
C(k,s)
A
= Diag(c(k,s)2
a1
,. . . , c(k,s)2
aH
),
C(k,s)
B
= Diag(c(k,s)2
b1
,. . . , c(k,s)2
bH
).
Conditional Conjugacy
As seen in Eq. (3.121), the SAMF model is the MF model for the parameters
(Î˜(s)
A , Î˜(s)
B ) in the sth SMF term, if the other parameters {Î˜(sâ€²)
A , Î˜(sâ€²)
B }sâ€²s are
regarded as constants. Therefore, the Gaussian priors (3.122) and (3.123) are
conditionally conjugate for each of Î˜(s)
A and Î˜(s)
B in each of the SMF terms.
Variational Bayesian Algorithm
Based on the conditional conjugacy, we solve the VB learning problem under
the following independence constraint (Babacan et al., 2012b):

100
3 VB Algorithm for Multilinear Models
r(Î˜) =
S
s=1
r(s)
A (Î˜(s)
A )r(s)
B (Î˜(s)
B ).
(3.124)
Following the standard procedure described in Section 2.1.5, we can ï¬nd that
the VB posterior, which minimizes the free energy (2.15), is in the following
form:
r(Î˜) =
S
s=1
K(s)

k=1

MGaussMâ€²(k,s),Hâ€²(k,s)(A(k,s); A
(k,s), Î£
(k,s)
A )
Â· MGaussLâ€²(k,s),Hâ€²(k,s)(B(k,s); B
(k,s), Î£
(k,s)
B
)

=
S
s=1
K(s)

k=1
 Mâ€²(k,s)

mâ€²=1
GaussHâ€²(k,s)(a(k,s)
mâ€² ;a
(k,s)
mâ€² , Î£
(k,s)
A )
Â·
Lâ€²(k,s)

lâ€²=1
GaussHâ€²(k,s)(b
(k,s)
lâ€²
;b
(k,s)
lâ€²
, Î£
(k,s)
B
)

(3.125)
with the variational parameters satisfying the stationary conditions given by
A
(k,s) = Ïƒâˆ’2Zâ€²(k,s)âŠ¤B
(k,s)Î£
(k,s)
A ,
(3.126)
Î£
(k,s)
A
= Ïƒ2 !
B
(k,s)âŠ¤B
(k,s) + Lâ€²(k,s)Î£
(k,s)
B
+ Ïƒ2C(k,s)âˆ’1
A
"âˆ’1
,
(3.127)
B
(k,s) = Ïƒâˆ’2Zâ€²(k,s)A
(k,s)Î£
(k,s)
B
,
(3.128)
Î£
(k,s)
B
= Ïƒ2 !
A
(k,s)âŠ¤A
(k,s) + Mâ€²(k,s)Î£
(k,s)
A
+ Ïƒ2C(k,s)âˆ’1
B
"âˆ’1
.
(3.129)
Here, Zâ€²(k,s) âˆˆRLâ€²(k,s)Ã—Mâ€²(k,s) is deï¬ned as
Zâ€²(k,s)
lâ€²,mâ€² = Z(s)
X(s)(k,lâ€²,mâ€²),
where
Z(s) = V âˆ’

sâ€²s
U
(s).
(3.130)
Free Energy as a Function of Variational Parameters
The free energy can be explicitly written as
2F = LM log(2Ï€Ïƒ2) + âˆ¥Vâˆ¥2
Fro
Ïƒ2
+
S
s=1
K(s)

k=1

Mâ€²(k,s) log
det

C(k,s)
A
 
det
!
Î£
(k,s)
A
" + Lâ€²(k,s) log
det

C(k,s)
B
 
det
!
Î£
(k,s)
B
"

+
S
s=1
K(S )

k=1
tr
2
C(k,s)âˆ’1
A
(A
(k,s)âŠ¤A
(k,s) + Mâ€²(k,s)Î£
(k,s)
A
)
+ C(k,s)âˆ’1
B
(B
(k,s)âŠ¤B
(k,s) + Lâ€²(k,s)Î£
(k,s)
B
)
3

3.5 Sparse Additive Matrix Factorization
101
+ 1
Ïƒ2 tr
â§âªâªâ¨âªâªâ©âˆ’2VâŠ¤

S
s=1
G({B
(k,s)A
(k,s)âŠ¤}K(s)
k=1; X(s))

+ 2
S
s=1
S
sâ€²=s+1
GâŠ¤({B
(k,s)A
(k,s)âŠ¤}K(s)
k=1; X(s))G({B
(k,sâ€²)A
(k,sâ€²)âŠ¤}K(sâ€²)
k=1 ; X(sâ€²))
â«âªâªâ¬âªâªâ­
+ 1
Ïƒ2
S
s=1
K(S )

k=1
tr
2
(A
(k,s)âŠ¤A
(k,s) + Mâ€²(k,s)Î£
(k,s)
A
)(B
(k,s)âŠ¤B
(k,s) + Lâ€²(k,s)Î£
(k,s)
B
)
3
âˆ’
S
s=1
K(S )

k=1
(Lâ€²(k,s) + Mâ€²(k,s))Hâ€²(k,s).
(3.131)
Empirical Variational Bayesian Algorithm
The following stationary conditions for the hyperparameters can be obtained
from the derivatives of the free energy (3.131):
c(k,s)2
ah
=
####a(k,s)
h
####
2
/Mâ€²(k,s) + (Î£
(k,s)
A )hh,
(3.132)
c(k,s)2
bh
=
#####b
(k,s)
h
#####
2
/Lâ€²(k,s) + (Î£
(k,s)
B
)hh,
(3.133)
Algorithm 6 EVB learning for sparse additive matrix factorization.
1: Initialize the variational parameters {A
(k,s), Î£
(k,s)
A , B
(k,s), Î£
(k,s)
B
}K(s)
k=1,
S
s=1, and
the hyperparameters {C(k,s)
A
, C(k,s)
B
}K(s)
k=1,
S
s=1, Ïƒ2, for example, A(k,s)
m,h , B(k,s)
l,h
âˆ¼
Gauss1(0, Ï„), Î£
(k,s)
A
= Î£
(k,s)
B
= C(k,s)
A
= C(k,s)
B
= Ï„IHâ€²(k,s), and Ïƒ2 = Ï„2 for Ï„2 =
âˆ¥Vâˆ¥2
Fro/(LM).
2: Apply (substitute the right-hand side into the left-hand side) Eqs. (3.127),
(3.126), (3.129), and (3.128) for each k and s to update Î£
(k,s)
A
, A
(k,s), Î£
(k,s)
B
,
and B
(k,s), respectively.
3: Apply Eqs. (3.132) and (3.133) for all hâ€² = 1,. . . , Hâ€²(k,s), k and s, and
Eq. (3.134) to update C(k,s)
A
, C(k,s)
B
, and Ïƒ2, respectively.
4: Prune the hth component if c(k,s)2
ah
c(k,s)2
bh
< Îµ, where Îµ > 0 is a threshold,
e.g., set to Îµ = 10âˆ’4.
5: Evaluate the free energy (3.131).
6: Iterate Steps 2 through 5 until convergence (until the energy decrease
becomes smaller than a threshold).

102
3 VB Algorithm for Multilinear Models
Ïƒ2 =
1
LM
)
âˆ¥Vâˆ¥2
Fro âˆ’2
S
s=1
tr
â›âœâœâœâœâœâU
(s)âŠ¤
â›âœâœâœâœâœâV âˆ’
S
sâ€²=s+1
U
(sâ€²)
ââŸâŸâŸâŸâŸâ 
ââŸâŸâŸâŸâŸâ 
+
S
s=1
K(s)

k=1
tr

(B
(k,s)âŠ¤B
(k,s) + Lâ€²(k,s)Î£
(k,s)
B
) Â· (A
(k,s)âŠ¤A
(k,s) + Mâ€²(k,s)Î£
(k,s)
A
)
 1
.
(3.134)
Algorithm 6 summarizes the EVB algorithm for SAMF. If we appropriately
set the hyperparameters {C(k,s)
A
, C(k,s)
B
}K(s)
k=1,
S
s=1, Ïƒ2 in Step 1 and skip Steps 3 and
4, Algorithm 6 is reduced to (nonempirical) VB learning.

4
VB Algorithm for Latent Variable Models
In this chapter, we discuss VB learning for latent variable models. Starting
with ï¬nite mixture models as the simplest example, we overview the VB
learning algorithms for more complex latent variable models such as Bayesian
networks and hidden Markov models.
Let H denote the set of (local) latent variables and w denote a model
parameter vector (or the set of global latent variables). In this chapter, we
consider the latent variable model for training data D:
p(D|w) =

H
p(D, H|w).
Let us employ the following factorized model to approximate the posterior
distribution for w and H:
r(w, H) = rw(w)rH(H).
(4.1)
Applying the general VB framework explained in Section 2.1.5 to the preced-
ing model leads to the following update rules for w and H:
rw(w) = 1
Cw
p(w) exp log p(D, H|w)
rH(H),
(4.2)
rH(H) =
1
CH
exp log p(D, H|w)
rw(w).
(4.3)
In the following sections, we discuss these update rules for some speciï¬c
examples of latent variable models.
4.1 Finite Mixture Models
A ï¬nite mixture model p(x|w) of an L-dimensional input x âˆˆRL with a
parameter vector w âˆˆRM is deï¬ned by
103

104
4 VB Algorithm for Latent Variable Models
p(x|w) =
K

k=1
Î±kp(x|Ï„k),
(4.4)
where integer K is the number of components and Î± = (Î±1,. . . , Î±K)âŠ¤âˆˆÎ”Kâˆ’1
is the set of mixing weights (Example 1.3). The parameter w of the model is
w = {Î±k, Ï„k}K
k=1.
The ï¬nite mixture model can be rewritten as follows by using a hidden
variable z = (z1,. . . , zK)âŠ¤âˆˆ{e1,. . . , eK},
p(x, z|w) =
K

k=1
4Î±kp(x|Ï„k)5zk .
(4.5)
Here ek âˆˆ{0, 1}K is the K-dimensional binary vector, called the one-of-K
representation, with one at the kth entry and zeros at the other entries:
ek = (0,. . . , 0,
k-th
	

1
, 0,. . . , 0

	
K
)âŠ¤.
The hidden variable z is not observed and is representing the component from
which the data sample x is generated. If the data sample x is from the kth
component, then zk = 1, otherwise, zk = 0. Then

z
p(x, z|w) = p(x|w)
holds where the sum over z goes through all possible values of the hidden
variable.
4.1.1 Mixture of Gaussians
If the component distribution in Eq. (4.4) is chosen to be a Gaussian distribu-
tion,
p(x|Ï„) = GaussL(x; Î¼, Î£),
the ï¬nite mixture model is called the mixture of Gaussians or the Gaussian
mixture model (GMM).
In some applications, the parameters are restricted to the means of each
component, and it is assumed that there is no correlation between each input
dimension. In this case, since L = M, the model is written by
p(x|w) =
K

k=1
Î±k
(2Ï€Ïƒ2)M/2 exp

âˆ’âˆ¥x âˆ’Î¼kâˆ¥2
2Ïƒ2

,
(4.6)
where Ïƒ > 0 is a constant.

4.1 Finite Mixture Models
105
In this subsection, the uncorrelated GMM (4.6) is considered in the VB
framework by further assuming that Ïƒ2 = 1 for simplicity. The joint model for
the observed and hidden variables (4.5) is given by the product of the following
two distributions:
p(z|Î±) = MultinomialK,1(z; Î±),
(4.7)
p(x|z, {Î¼k}K
k=1) =
K

k=1
6GaussM(x; Î¼k, IM)7zk .
(4.8)
Thus, for the set of hidden variables H = {z(n)}N
n=1 and the complete data set
{D, H} = {x(n), z(n)}N
n=1, the complete likelihood is given by
p(D, H|Î±, {Î¼k}K
k=1) =
N

n=1
K

k=1
,
Î±kGaussM(x(n); Î¼k, IM)
-z(n)
k .
(4.9)
ML learning of the GMM is carried out by the expectation-maximization
(EM) algorithm (Dempster et al., 1977), which corresponds to a clustering
algorithm called the soft K-means (MacKay, 2003, ch. 22).
Because of the conditional conjugacy (Section 2.1.2) of the parameters Î± =
(Î±1,. . . , Î±K)âŠ¤âˆˆÎ”Kâˆ’1 and {Î¼k}K
k=1, we assume that the prior of the parameters
is the product of the following two distributions:
p(Î±|Ï†) = DirichletK(Î±; (Ï†,. . . , Ï†)âŠ¤),
(4.10)
p({Î¼k}K
k=1|Î¼0, Î¾) =
K

k=1
GaussM(Î¼k|Î¼0, (1/Î¾)IM),
(4.11)
where Î¾ > 0, Î¼0 âˆˆRM and Ï† > 0 are the hyperparameters.
VB Posterior for the Gaussian Mixture Model
Let
Nk =
N

n=1

z(n)
k

rH(H)
(4.12)
and
xk = 1
Nk
N

n=1

z(n)
k

rH(H) x(n),
(4.13)
where z(n)
k
= 1 if the nth data sample x(n) is from the kth component; otherwise,
z(n)
k
= 0. The variable Nk is the expected number of times data come from
the kth component, and xk is the mean of them. Note that the variables Nk
and xk satisfy the constraints K
k=1 Nk = N and K
k=1 Nkxk = N
n=1 x(n). From

106
4 VB Algorithm for Latent Variable Models
(4.2) and the respective priors (4.10) and (4.11), the VB posterior rw(w) =
rÎ±(Î±)rÎ¼({Î¼k}K
k=1) is obtained as the product of the following two distributions:
rÎ±(Î±) = DirichletK

Î±; (Ï†1,. . . ,Ï†K)âŠ¤ 
,
(4.14)
rÎ¼({Î¼k}K
k=1) =
K

k=1
GaussM

Î¼k;Î¼k, Ïƒ2
kIM
 
,
(4.15)
where
Ï†k = Nk + Ï†,
(4.16)
Ïƒ2
k =
1
Nk + Î¾
,
(4.17)
Î¼k = Nkxk + Î¾Î¼0
Nk + Î¾
.
(4.18)
From Eq. (4.3), the VB posterior rH(H) is given by
rH(H) =
1
CH
N

n=1
exp
â›âœâœâœâœâœâz(n)
k
â§âªâªâ¨âªâªâ©Î¨(Ï†k) âˆ’Î¨
â›âœâœâœâœâœâ
K

kâ€²=1
Ï†kâ€²
ââŸâŸâŸâŸâŸâ 
âˆ’âˆ¥x(n) âˆ’Î¼kâˆ¥2
2
âˆ’M
2

log 2Ï€ +
1
Nk + Î¾
1
,
where Î¨ is the di-gamma (psi) function, and we used
log Î±k

rÎ±(Î±) = Î¨(Ï†k) âˆ’Î¨
â›âœâœâœâœâœâ
K

kâ€²=1
Ï†kâ€²
ââŸâŸâŸâŸâŸâ .
(4.19)
That is, rH(H) = rz({z(n)}N
n=1) is the multinomial distribution:
rz({z(n)}N
n=1) =
N

n=1
rz(z(n))
=
N

n=1
MultinomialK,1

z(n);z(n) 
,
wherez(n) âˆˆÎ”Kâˆ’1 is
z(n)
k
=

z(n)
k

rH(H) =
z(n)
k
K
kâ€²=1 z(n)
kâ€²
,
(4.20)
for
z(n)
k
= exp
â›âœâœâœâœâœâÎ¨(Ï†k) âˆ’Î¨
â›âœâœâœâœâœâ
K

kâ€²=1
Ï†kâ€²
ââŸâŸâŸâŸâŸâ âˆ’1
2
###x(n) âˆ’Î¼k
###2 + MÏƒ2
k
ââŸâŸâŸâŸâŸâ .
(4.21)

4.1 Finite Mixture Models
107
The free energy as a function of variational parameters is expressed as
follows:
F =
/
log rH(H)rw(w)
p(w)
0
rH(H)rw(w)
âˆ’log p(D, H|w)
rH(H)rw(w)
=
/
log
(z(n)
k )z(n)
k
Î“(K
k=1 Ï†k)
K
k=1 Î“(Ï†k)
K
k=1 Î±
Ï†kâˆ’1
k
exp

âˆ’
âˆ¥Î¼kâˆ’Î¼kâˆ¥2
2Ïƒ2
k

(2Ï€Ïƒ2
k)M/2
Î“(KÏ†)
(Î“(Ï†))K
K
k=1 Î±Ï†âˆ’1
k
 Î¾
2Ï€
 M/2 exp

âˆ’Î¾âˆ¥Î¼kâˆ’Î¼0âˆ¥2
2
 
0
rH(H)rw(w)
âˆ’
/
log
N

n=1
K

k=1
â§âªâªâªâªâªâ¨âªâªâªâªâªâ©
Î±k
exp
!
âˆ’âˆ¥x(n)âˆ’Î¼kâˆ¥2
2
"
(2Ï€)M/2
â«âªâªâªâªâªâ¬âªâªâªâªâªâ­
z(n)
k 0
rH(H)rw(w)
= log
â›âœâœâœâœâœâ
Î“(K
k=1 Ï†k)
K
k=1 Î“(Ï†k)
ââŸâŸâŸâŸâŸâ âˆ’log
 Î“(KÏ†)
(Î“(Ï†))K

âˆ’M
2
K

k=1
log

Î¾Ïƒ2
k
 
âˆ’KM
2
+
N

n=1
K

k=1
z(n)
k logz(n)
k +
K

k=1
Ï†k âˆ’Ï† âˆ’Nk
 
Î¨(Ï†k) âˆ’Î¨(K
kâ€²=1 Ï†kâ€²)
 
+
K

k=1
Î¾

âˆ¥Î¼k âˆ’Î¼0âˆ¥2 + MÏƒ2
k
 
2
+
K

k=1
Nk

M log(2Ï€) + MÏƒ2
k
 
2
+
K

k=1
Nkâˆ¥xk âˆ’Î¼kâˆ¥2 + N
n=1z(n)
k âˆ¥x(n) âˆ’xkâˆ¥2
2
.
(4.22)
The prior hyperparameters, (Ï†, Î¼0, Î¾), can be estimated by the EVB learning
(Section 2.1.6). Computing the partial derivatives, we have
âˆ‚F
âˆ‚Ï† = K (Î¨(Ï†) âˆ’Î¨(KÏ†)) âˆ’
K

k=1

Î¨(Ï†k) âˆ’Î¨
K
kâ€²=1 Ï†kâ€²
  
,
(4.23)
âˆ‚F
âˆ‚Î¼0
= Î¾
K

k=1
$Î¼0 âˆ’Î¼k
% ,
(4.24)
âˆ‚F
âˆ‚Î¾ = âˆ’M
2
â›âœâœâœâœâœâ
K
Î¾ âˆ’
K

k=1
âˆ¥Î¼k âˆ’Î¼0âˆ¥2
M
+ Ïƒ2
k
ââŸâŸâŸâŸâŸâ .
(4.25)
The stationary conditions âˆ‚F
âˆ‚Î¼0 = 0 and âˆ‚F
âˆ‚Î¾ = 0 yield the following update rules:
Î¼0 = 1
K
K

k=1
Î¼k,
(4.26)

108
4 VB Algorithm for Latent Variable Models
Algorithm 7 EVB learning for the Gaussian mixture model.
1: Initialize the variational parameters ({z(n)}N
n=1, {Ï†k}K
k=1, {Î¼k, Ïƒ2
k}K
k=1), and the
hyperparameters (Ï†, Î¼0, Î¾).
2: Apply (substitute the right-hand side into the left-hand side) Eqs. (4.21),
(4.20), (4.12), (4.13), (4.16), (4.17), and (4.18) to update {z(n)}N
n=1, {Ï†k}K
k=1,
and {Î¼k, Ïƒ2
k}K
k=1.
3: Apply Eqs. (4.29), (4.26), and (4.27) to update Ï†, Î¼0 and Î¾, respectively.
4: Evaluate the free energy (4.22).
5: Iterate Steps 2 through 4 until convergence (until the energy decrease
becomes smaller than a threshold).
Î¾ =
â§âªâªâ¨âªâªâ©
1
K
K

k=1
âˆ¥Î¼k âˆ’Î¼0âˆ¥2
M
+ Ïƒ2
k
â«âªâªâ¬âªâªâ­
âˆ’1
.
(4.27)
Since the stationary condition
âˆ‚F
âˆ‚Ï†
= 0 is not explicitly solved for Ï†, the
Newtonâ€“Raphson step is usually used for updating Ï†. With the second deriva-
tive,
âˆ‚2F
âˆ‚Ï†2 = K

Î¨(1)(Ï†) âˆ’KÎ¨(1)(KÏ†)
 
,
(4.28)
the update rule is obtained as follows:
Ï†new = max
â›âœâœâœâœâœâ0, Ï†old âˆ’
âˆ‚2F
âˆ‚Ï†2
âˆ’1 âˆ‚F
âˆ‚Ï†
ââŸâŸâŸâŸâŸâ 
= max

0, Ï†old âˆ’
K(Î¨(Ï†)âˆ’Î¨(KÏ†))âˆ’K
k=1

Î¨(Ï†k)âˆ’Î¨
K
kâ€²=1 Ï†kâ€²
  
K(Î¨(1)(Ï†)âˆ’KÎ¨(1)(KÏ†))

,
(4.29)
where Î¨m(z) â‰¡dm
dzm Î¨(z) is the polygamma function of order m.
The EVB learning for the GMM is summarized in Algorithm 7. If the prior
hyperparameters are ï¬xed and Step 3 in the algorithm is omitted, the algorithm
reduces to the (nonempirical) VB learning algorithm.
4.1.2 Mixture of Exponential Families
It is well known that the Gaussian distribution is an example of the exponential
family distribution:
p(x|Ï„) = p(t|Î·) = exp

Î·âŠ¤t âˆ’A(Î·) + B(t)
 
,
(4.30)
where Î· âˆˆH is the natural parameter, Î·âŠ¤t is its inner product with the vector
t = t(x) = (t1(x),. . . , tM(x))âŠ¤, and A(Î·) and B(t) are real-valued functions

4.1 Finite Mixture Models
109
of the parameter Î· and the sufï¬cient statistics t, respectively (Brown, 1986)
(see Eq. (1.27) in Section 1.2.3). Suppose functions t1,. . . , tM and the constant
function, 1, are linearly independent and the number of parameters in a single
component distribution, p(t|Î·), is M.
The VB framework for GMMs in Section 4.1.1 is generalized to a mixture
of exponential family distributions as follows. The conditional conjugate prior
distributions of Î± âˆˆÎ”Kâˆ’1 and {Î·k}K
k=1 are given by
p(Î±|Ï†) = DirichletK(Î±; (Ï†,. . . , Ï†)âŠ¤),
(4.31)
p({Î·k}K
k=1|Î½0, Î¾) =
K

k=1
1
C(Î¾, Î½0) exp

Î¾(Î½âŠ¤
0 Î·k âˆ’A(Î·k))
 
,
(4.32)
where the function C(Î¾, Î½) of Î¾ âˆˆR and Î½ âˆˆRM is deï¬ned by
C(Î¾, Î½) =

H
exp

Î¾(Î½âŠ¤Î· âˆ’A(Î·))
 
dÎ·.
(4.33)
Constants Î¾ > 0, Î½0 âˆˆRM, and Ï† > 0 are the hyperparameters.
VB Posterior for Mixture-of-Exponential-Family Models
Here, we derive the VB posterior rw(w) for the mixture-of-exponential-family
model using Eq. (4.2).
Using the complete data {x(n), z(n)}N
n=1, we put
Nk =
N

n=1

z(n)
k

rH(H) ,
(4.34)
tk = 1
Nk
N

n=1

z(n)
k

rH(H) t(n),
(4.35)
where t(n) = t(x(n)). Note that the variables Nk and tk satisfy the constraints
K
k=1 Nk = N and K
k=1 Nktk = N
n=1 t(n). From Eq. (4.2) and the respec-
tive prior distributions, Eqs. (4.10) and (4.32), the VB posterior rw(w) =
rÎ±(Î±)rÎ·({Î·k}K
k=1) is obtained as the product of the following two distributions:
rÎ±(Î±) = DirichletK

Î±; (Ï†1,. . . ,Ï†K)âŠ¤ 
,
rÎ·({Î·k}K
k=1) =
K

k=1
1
C(Î¾k,Î½k)
exp
Î¾k(Î½âŠ¤
k Î·k âˆ’A(Î·k))
 
,
(4.36)
where
Ï†k = Nk + Ï†,
(4.37)

110
4 VB Algorithm for Latent Variable Models
Î½k = Nktk + Î¾Î½0
Nk + Î¾
,
(4.38)
Î¾k = Nk + Î¾.
(4.39)
Let
Î·k = Î·k

rÎ·(Î·k) = 1
Î¾k
âˆ‚logC(Î¾k,Î½k)
âˆ‚Î½k
.
(4.40)
It follows that
A(Î·k)
rÎ·(Î·k) = Î·âŠ¤
kÎ½k âˆ’âˆ‚logC(Î¾k,Î½k)
âˆ‚Î¾k
.
(4.41)
From Eq. (4.3), the VB posterior rH(H) is given by
rH(H) =
N

n=1
rz(z(n))
=
N

n=1
MultinomialK,1

z(n);z(n) 
,
wherez(n) âˆˆÎ”Kâˆ’1 is
z(n)
k
=

z(n)
k

rH(H) =
z(n)
k
K
kâ€²=1 z(n)
kâ€²
,
(4.42)
for
z(n)
k
= exp
â›âœâœâœâœâœâÎ¨(Ï†k) âˆ’Î¨
â›âœâœâœâœâœâ
K

kâ€²=1
Ï†kâ€²
ââŸâŸâŸâŸâŸâ +Î·âŠ¤
k t(n) âˆ’A(Î·k)
rÎ·(Î·k) + B(t(n))
ââŸâŸâŸâŸâŸâ .
(4.43)
To obtain the preceding expression of z(n)
k , we used Eq. (4.19).
The free energy as a function of variational parameters is expressed as
F = log
â›âœâœâœâœâœâ
Î“(K
k=1 Ï†k)
K
k=1 Î“(Ï†k)
ââŸâŸâŸâŸâŸâ âˆ’log
 Î“(KÏ†)
(Î“(Ï†))K

âˆ’
K

k=1
logC(Î¾k,Î½k) + K logC(Î¾, Î½0)
+
N

n=1
K

k=1
z(n)
k logz(n)
k +
K

k=1
Ï†k âˆ’Ï† âˆ’Nk
 
Î¨(Ï†k) âˆ’Î¨(K
kâ€²=1 Ï†kâ€²)
 
+
K

k=1
â¡â¢â¢â¢â¢â£Î·âŠ¤
k
,
Î¾ $Î½k âˆ’Î½0
% + Nk

Î½k âˆ’tk
 -
+
Î¾k âˆ’Î¾ âˆ’Nk
 âˆ‚logC(Î¾k,Î½k)
âˆ‚Î¾k
â¤â¥â¥â¥â¥â¦
âˆ’
N

n=1
B(t(n)).
(4.44)

4.1 Finite Mixture Models
111
The update rule of Ï† for the EVB learning is obtained by Eq. (4.29) as in
the GMM. The partial derivatives of F with respect to the hyperparameters
(Î½0, Î¾) are
âˆ‚F
âˆ‚Î½0
= K âˆ‚logC(Î¾, Î½0)
âˆ‚Î½0
âˆ’Î¾
K

k=1
Î·k,
(4.45)
âˆ‚F
âˆ‚Î¾ =
K

k=1
â§âªâªâ¨âªâªâ©Î·âŠ¤
k
$Î½k âˆ’Î½0
% âˆ’âˆ‚logC(Î¾k,Î½k)
âˆ‚Î¾k
â«âªâªâ¬âªâªâ­+ K âˆ‚logC(Î¾, Î½0)
âˆ‚Î¾
.
(4.46)
Equating these derivatives to zeros, we have the following stationary condi-
tions:
1
Î¾
âˆ‚logC(Î¾, Î½0)
âˆ‚Î½0
= 1
K
K

k=1
Î·k,
(4.47)
âˆ‚logC(Î¾, Î½0)
âˆ‚Î¾
=
â›âœâœâœâœâœâ
1
K
K

k=1
Î·k
ââŸâŸâŸâŸâŸâ 
âŠ¤
Î½0 âˆ’1
K
K

k=1
A(Î·k)
rÎ·(Î·k) ,
(4.48)
where we have used Eq. (4.41). If these equations are solved for Î½0 and Î¾,
respectively, we obtain their update rules as in the case of the GMM.
Otherwise, we need the Newtonâ€“Raphson steps to update them.
The EVB learning for the mixture of exponential families is summarized in
Algorithm 8. If the prior hyperparameters are ï¬xed and Step 3 in the algorithm
is omitted, the algorithm reduces to the (nonempirical) VB learning algorithm.
Algorithm 8 EVB learning for the mixture-of-exponential-family model.
1: Initialize the variational parameters ({z(n)}N
n=1, {Ï†k}K
k=1, {Î½k,Î¾k}K
k=1), and the
hyperparameters (Ï†, Î½0, Î¾).
2: Apply (substitute the right-hand side into the left-hand side) Eqs. (4.43),
(4.42), (4.34), (4.35), (4.37), (4.38), and (4.39) to update {z(n)}N
n=1, {Ï†k}K
k=1,
and {Î½k,Î¾k}K
k=1. Transform {Î½k,Î¾k}K
k=1 to {Î·k, A(Î·k)
rÎ·(Î·k)}K
k=1 by Eqs. (4.40)
and (4.41).
3: Apply Eqs. (4.29), (4.47), and (4.48) to update Ï†, Î½0 and Î¾, respectively.
4: Evaluate the free energy (4.44).
5: Iterate Steps 2 through 4 until convergence (until the energy decrease
becomes smaller than a threshold).

112
4 VB Algorithm for Latent Variable Models
4.1.3 Inï¬nite Mixture Models
In 2000s, there was a revival of Bayesian nonparametric models to estimate the
model complexity, e.g., the number of components in mixture models, by using
a prior distribution for probability measures such as the Dirichlet process (DP)
prior. The Bayesian nonparametric approach ï¬ts a single model adapting its
complexity to the data. The VB framework plays an important role in achieving
tractable inference for Bayesian nonparametric models. Here, we introduce the
VB learning for the stick-breaking construction of the DP prior by instantiating
the estimation of the number of components of the mixture model.
For the ï¬nite mixture model with K components, we had the discrete latent
variable,
z âˆˆ{e1, e2,. . . , eK},
indicating the label of the component. We also assumed the multinomial
distribution,
p(z|Î±) = MultinomialK,1(z; Î±) =
K

k=1
Î±zk
k .
In the nonparametric Bayesian approach, we consider possibly an inï¬nite
number of components,
p(z|Î±) = lim
Kâ†’âˆMultinomialK,1(z; Î±),
and the following generation process of Î±k, called the stick-breaking pro-
cess (Blei and Jordan, 2005; Gershman and Blei, 2012):
Î±k = vk
kâˆ’1

l=1
(1 âˆ’vl),
vk âˆ¼Beta(1, Î³),
where Beta(Î±, Î²) denotes the beta distribution with parameters Î± and Î², and Î³
is the scaling parameter.
To derive a tractable VB learning algorithm, the truncation level T is usually
introduced to the preceding process, which enforces vT = 1. If the truncation
level T is sufï¬ciently large, some components are left unused, and hence T
does not directly specify the number of components.
Then, the VB posterior r(H, v) for the latent variables and v = {vk}Tâˆ’1
k=1 is
assumed to be factorized, rH(H)rv(v), for which the free energy minimization
implies further factorization:
r(H, v) =
N

n=1
rz(z(n))
Tâˆ’1

k=1
rv(vk),

4.1 Finite Mixture Models
113
where rz(z(n)) is the multinomial distribution as in the case of the ï¬nite mixture
model, and rv(vk) is the beta distribution because of the conditional conjugacy.
To see this and how the VB learning algorithm is derived, we instantiate the
GMM discussed in Section 4.1.1.
The free energy is decomposed as
F =
/
log rz({z(n)}N
n=1)rv(v)rÎ¼({Î¼k}K
k=1)
p(v)p({Î¼k}T
k=1)
0
rz({z(n)}N
n=1)rv(v)rÎ¼({Î¼k}K
k=1)
âˆ’

log p(D|{z(n)}N
n=1, w)

rz({z(n)}N
n=1)rv(v)rÎ¼({Î¼k}K
k=1)
âˆ’

log p({z(n)}N
n=1|v)

rz({z(n)}N
n=1)rv(v) .
These terms are computed in the same way as in Section 4.1.1 except for the
last term,

log p({z(n)}N
n=1|v)

rz({z(n)}N
n=1)rv(v) = N
n=1

log p(z(n)|v)

rz(z(n))rv(v).
Let c(n) be the index k such that z(n)
k
= 1 and Î¸ be the indicator function.
Then, we have

log p(z(n)|v)

rz(z(n))rv(v)
=
/
log
âˆ

k=1
(1 âˆ’vk)Î¸(c(n)>k)vÎ¸(c(n)=k)
k
0
rz(z(n))rv(v)
=
âˆ

k=1
,
rz(c(n) > k) log(1 âˆ’vk)
rv(v) + rz(c(n) = k) log vk

rv(v)
-
=
Tâˆ’1

k=1
,
rz(c(n) > k) log(1 âˆ’vk)
rv(v) + rz(c(n) = k) log vk

rv(v)
-
,
where we have used log vT = 0 and rz(c(n) > T) = 0.
Since the probabilities rz(c(n) = k) and rz(c(n) > k) are given by
rz(c(n) = k) =z(n)
k ,
rz(c(n) > k) =
T

l=k+1
z(n)
l ,
it follows from Eq. (4.12) that
N

n=1
rz(c(n) = k) = Nk,
N

n=1
rz(c(n) > k) =
T

l=k+1
Nl = N âˆ’
k

l=1
Nl.

114
4 VB Algorithm for Latent Variable Models
Now Eq. (4.3) in this case yields that
rv(v) âˆ
N

n=1

log p(z(n)|v)

rz(z(n)) p(v).
It follows from similar manipulations to those just mentioned and the condi-
tional conjugacy that
rv(v) =
Tâˆ’1

k=1
Beta(vk;Îºk,Î»k),
(4.49)
i.e., for a ï¬xed rH(H), the optimal rv(v) is the beta distribution with the
parameters,
Îºk = 1 + Nk,
(4.50)
Î»k = Î³ + N âˆ’
k

l=1
Nl.
(4.51)
The VB posterior rH(H) is computed similarly except that the expectation
log Î±k

rÎ±(Î±) = Î¨(Nk + Ï†) âˆ’Î¨(N + KÏ†)
in Eq. (4.19) for the ï¬nite mixture model is replaced by
log vk

rv(vk)+
kâˆ’1

l=1
log(1 âˆ’vl)
rv(vl) = Î¨(Îºk)âˆ’Î¨(Îºk+Î»k)+
kâˆ’1

l=1
{Î¨(Î»l)âˆ’Î¨(Îºl+Î»l)},
since
log vk

rv(vk) = Î¨(Îºk) âˆ’Î¨(Îºk +Î»k),
log(1 âˆ’vk)
rv(vk) = Î¨(Î»k) âˆ’Î¨(Îºk +Î»k),
for rv(vk) = Beta(vk;Îºk,Î»k). In the case of the GMM, z(n)
k
in Eq. (4.21) is
replaced with
z(n)
k
= exp
â›âœâœâœâœâœâœâÎ¨(Îºk) âˆ’Î¨(Îºk +Î»k) +
kâˆ’1

l=1
{Î¨(Î»l) âˆ’Î¨(Îºl +Î»l)}
âˆ’1
2
###x(n) âˆ’Î¼k
###2 + MÏƒ2
k

.
(4.52)
The free energy is given by
F =
Tâˆ’1

k=1
log
â›âœâœâœâœâ
Î“(Îºk +Î»k)
Î“(Îºk)Î“(Î»k)
ââŸâŸâŸâŸâ âˆ’(T âˆ’1) log Î³ âˆ’M
2
T

k=1
log

Î¾Ïƒ2
k
 
âˆ’T M
2
+
N

n=1
T

k=1
z(n)
k logz(n)
k +
Tâˆ’1

k=1

Îºk âˆ’1 âˆ’Nk
 ,
Î¨(Îºk) âˆ’Î¨(Îºk +Î»k)
-

4.2 Other Latent Variable Models
115
+
Tâˆ’1

k=1
â§âªâªâ¨âªâªâ©Î»k âˆ’Î³ âˆ’
â›âœâœâœâœâœâœâN âˆ’
k

l=1
Nl
ââŸâŸâŸâŸâŸâŸâ 
â«âªâªâ¬âªâªâ­
,
Î¨(Î»k) âˆ’Î¨(Îºk +Î»k)
-
+
T

k=1
Î¾

âˆ¥Î¼k âˆ’Î¼0âˆ¥2 + MÏƒ2
k
 
2
+
T

k=1
Nk

M log(2Ï€) + MÏƒ2
k
 
2
+
T

k=1
Nkâˆ¥xk âˆ’Î¼kâˆ¥2 + N
n=1z(n)
k âˆ¥x(n) âˆ’xkâˆ¥2
2
.
(4.53)
The VB learning algorithm is similar to Algorithm 7 for the ï¬nite GMM
while the number of components K is replaced with the truncation level T
throughout, the update rule (4.21) is replaced with Eq. (4.52), and {Îºk,Î»k}Tâˆ’1
k=1
are updated by Eqs. (4.50) and (4.51) instead of {Ï†k}K
k=1.
By computing
âˆ‚F
âˆ‚Î³ and equating it to zero, the EVB learning for the
hyperparameter Î³ updates it as follows:
Î³ =
â¡â¢â¢â¢â¢â¢â¢â£
âˆ’1
T âˆ’1
Tâˆ’1

k=1
,
Î¨(Î»k) âˆ’Î¨(Îºk +Î»k)
-â¤â¥â¥â¥â¥â¥â¥â¦
âˆ’1
,
(4.54)
which can replace the update rule of Ï† in Step 3 of Algorithm 7.
4.2 Other Latent Variable Models
In this section, we discuss more complex latent variable models than mixture
models and derive VB learning algorithms for them. Although we focus on the
models where the multinomial distribution is assumed on the observed data
given latent variables, it is straightforward to replace it with other members of
the exponential family.
4.2.1 Bayesian Networks
A Bayesian network is a probabilistic model deï¬ned by a graphical model
expressing the relations among random variables by a graph and the condi-
tional probabilities associated with them (Jensen, 2001). In this subsection,
we focus on a Bayesian network whose states of all hidden nodes inï¬‚uence
those of all observation nodes, and assume that it has M observation nodes
and K hidden nodes. The graphical structure of this Bayesian network is called
bipartite and presented in Figure 4.1.
The observation nodes are denoted by x = (x1,. . . , xM), and the set of states
of observation node x j = (xj,1,. . . , xj,Y j)âŠ¤âˆˆ{el}
Y j
l=1 is {1,. . . , Yj}. The hidden

116
4 VB Algorithm for Latent Variable Models
. . .
.  .  .
 .  .  .
x1
x2
xM
z1
z2
zK
Figure 4.1 Graphical structure of the Bayesian network.
nodes are denoted by z = (z1,. . . , zK), and the set of states of hidden node
zk = (zk,1,. . . , zk,Tk)âŠ¤âˆˆ{ei}Tk
i=1 is {1,. . . , Tk}.
The probability that the state of the hidden node zk is i (1 â‰¤i â‰¤Tk) is
expressed as
a(k,i) = Prob(zk = ei).
Then, ak = (a(k,1),. . . , a(k,Tk))âŠ¤âˆˆÎ”Tkâˆ’1 for k = 1,. . . , K.
The conditional probability that the jth observation node xj is l (1 â‰¤l â‰¤Yj),
given the condition that the states of hidden nodes are z = (z1,. . . , zK), is
denoted by
b(j,l|z) = Prob(xj = el|z).
Then, b j|z = (b(j,1|z),. . . , b(j,Y j|z))âŠ¤âˆˆÎ”Y jâˆ’1 for j = 1,. . . , M. Deï¬ne bz =
{b j|z}M
j=1 for z âˆˆZ = {z; zk âˆˆ{ei}Tk
i=1, k = 1,. . . , K}. Let w = {{ak}K
k=1, {bz}zâˆˆZ}
be the set of all parameters. Then, the joint probability that the states of
observation nodes are x = (x1,. . . , xM) and the states of hidden nodes are
z = (z1,. . . , zK) is
p(x, z|w) = p(x|bz)
K

k=1
Tk

i=1
azk,i
(k,i),
where
p(x|bz) =
M

j=1
Y j

l=1
b
xj,l
(j,l|z).
Therefore, the marginal probability that the states of observation nodes are
x is
p(x|w) =

zâˆˆZ
p(x, z|w)
=

zâˆˆZ
p(x|bz)
K

k=1
Tk

i=1
azk,i
(k,i),
(4.55)

4.2 Other Latent Variable Models
117
where we used the notation 
zâˆˆZ for the summation over all states of hidden
nodes. Let
Mobs =
M

j=1
(Yj âˆ’1),
which is the number of parameters to specify the conditional probability
p(x|bz) of the states of all the observation nodes given the states of the hidden
nodes. Then, the number of the parameters of the model, D, is
D = Mobs
K

k=1
Tk +
K

k=1
(Tk âˆ’1).
(4.56)
We assume that the prior distribution p(w) of the parameters w
=
{{ak}K
k=1, {bz}zâˆˆZ} is the conditional conjugate prior distribution. Then, p(w) is
given by
,K
k=1 p(ak|Ï†)
- ,
zâˆˆZ
M
j=1 p(b j|z|Î¾)
-
, where
p(ak|Ï†) = DirichletTk

ak; (Ï†,. . . , Ï†)âŠ¤ 
,
(4.57)
p(b j|z|Î¾) = DirichletY j

b j|z; (Î¾,. . . , Î¾)âŠ¤ 
,
(4.58)
are the Dirichlet distributions with hyperparameters Ï† > 0 and Î¾ > 0.
VB Posterior for Bayesian Networks
Let {D, H} be the complete data with the observed data set D = {x(n)}N
n=1
and the corresponding hidden variables H = {z(n)}N
n=1. Deï¬ne the expected
sufï¬cient statistics:
N
z
(k,ik) =
N

n=1

z(n)
k,ik

rH(H) ,
N
x
(j,lj|z) =
N

n=1
x(n)
j,ljrz(z(n) = z),
(4.59)
where
rz(z(n) = z) =
/ K

k=1
z(n)
k,ik
0
rH(H)
(4.60)
is the estimated probability that z(n) = z = (ei1,. . . , eiK). Here x(n)
j
indicates
the state of the jth observation node and z(n)
k
indicates the state of the kth
hidden node when the nth training sample is observed. From Eq. (4.2), the
VB posterior distribution of parameters w = {{ak}K
k=1, {bz}zâˆˆZ} is given by
rw(w) =
â§âªâªâ¨âªâªâ©
K

k=1
ra(ak)
â«âªâªâ¬âªâªâ­
â§âªâªâªâ¨âªâªâªâ©

zâˆˆZ
M

j=1
rb(b j|z)
â«âªâªâªâ¬âªâªâªâ­,

118
4 VB Algorithm for Latent Variable Models
ra(ak) = DirichletTk

ak;Ï†k
 
,
(4.61)
rb(b j|z) = DirichletY j

b j|z;Î¾ j|z
 
,
(4.62)
where
Ï†k = (Ï†(k,1),. . . ,Ï†(k,Tk))âŠ¤
(k = 1,. . . , K),
Ï†(k,i) = N
z
(k,i) + Ï†
(i = 1,. . . , Tk),
(4.63)
Î¾ j|z = (Î¾(j,1|z),. . . ,Î¾(j,Y j|z))âŠ¤
( j = 1,. . . , M, z âˆˆZ),
Î¾(j,l|z) = N
x
(j,l|z) + Î¾
(l = 1,. . . , Yj).
(4.64)
Note that if we deï¬ne
N
x
z =
N

n=1
rz(z(n) = z) =
N

n=1
/ K

k=1
z(n)
k,ik
0
rH(H)
,
for z = (ei1,. . . , eiK) âˆˆZ, we have
N
x
z =
Y j

l=1
N
x
(j,l|z),
(4.65)
for j = 1,. . . , M, and
N
z
(k,i) =

zâˆ’k
N
x
z,
(4.66)
where 
zâˆ’k denotes the summation over zkâ€² (kâ€²  k) other than zk = ei.
It follows from Eqs. (4.61) and (4.62) that
log a(k,i)

ra(ak) = Î¨(Ï†(k,i)) âˆ’Î¨
Tk
iâ€²=1 Ï†(k,iâ€²)
 
(i = 1,. . . , Tk),
for k = 1,. . . , K and

log b(j,l|z)

rb(bj|z) = Î¨(Î¾(j,l|z)) âˆ’Î¨
Y j
lâ€²=1Î¾(j,lâ€²|z)
 
(l = 1,. . . , Yj),
for j = 1,. . . , M. From Eq. (4.3), the VB posterior distribution of the hidden
variables is given by rH(H) = N
n=1 rz(z(n)), where for z = (ei1,. . . , eiK),
rz(z(n) = z) =

z(n)âˆˆZ
rz(z(n))
K

k=1
z(n)
k,ik
âˆexp
â›âœâœâœâœâœâ
K

k=1
2
Î¨(Ï†(k,ik)) âˆ’Î¨
!Tk
iâ€²
k=1 Ï†(k,iâ€²
k)
"3
+
M

j=1
2
Î¨(Î¾(j,l(n)
j |z)) âˆ’Î¨
Y j
lâ€²=1Î¾(j,lâ€²|z)
 3ââŸâŸâŸâŸâŸâŸâ ,
(4.67)
if x(n)
j
= el(n)
j .

4.2 Other Latent Variable Models
119
The VB algorithm updates {N
x
(j,lj|z)} using Eqs. (4.59) and (4.67) iteratively.
The other expected sufï¬cient statistics and variational parameters are com-
puted by Eqs. (4.65), (4.66) and Eqs. (4.63), (4.64), respectively. The free
energy as a function of the variational parameters is given by
F =
K

k=1
â§âªâªâ¨âªâªâ©log
â›âœâœâœâœâœâ
Î“(Tk
i=1 Ï†(k,i))
Tk
i=1 Î“(Ï†(k,i))
ââŸâŸâŸâŸâŸâ âˆ’log
 Î“(TkÏ†)
(Î“(Ï†))Tk

+
Tk

i=1
Ï†(k,i) âˆ’Ï† âˆ’N
z
(k,i)
 
Î¨(Ï†(k,i)) âˆ’Î¨
Tk
iâ€²=1 Ï†(k,iâ€²)
  â«âªâªâ¬âªâªâ­
+

zâˆˆZ
M

j=1
â§âªâªâ¨âªâªâ©log
â›âœâœâœâœâœâœâ
Î“(Y j
l=1Î¾(j,l|z))
Y j
l=1 Î“(Î¾(j,l|z))
ââŸâŸâŸâŸâŸâŸâ âˆ’log
 Î“(YjÎ¾)
(Î“(Î¾))Y j

+
Y j

l=1
Î¾(j,l|z) âˆ’Î¾ âˆ’N
x
(j,l|z)
 
Î¨(Î¾(j,l|z)) âˆ’Î¨
Y j
lâ€²=1Î¾(j,lâ€²|z)
  
â«âªâªâªâ¬âªâªâªâ­
+
N

n=1

zâˆˆZ
rz(z(n) = z) log rz(z(n) = z).
(4.68)
The following update rule for the EVB learning of the hyperparameter Ï† is
obtained in the same way as the update rule (4.29) for the GMM:
Ï†new = max

0, Ï†old âˆ’
K
k=1
,
Tk(Î¨(Ï†)âˆ’Î¨(TkÏ†))âˆ’Tk
i=1

Î¨(Ï†(k,i))âˆ’Î¨
Tk
iâ€²=1 Ï†(k,iâ€²)
  -
K
k=1 Tk(Î¨(1)(Ï†)âˆ’TkÎ¨(1)(TkÏ†))

.
(4.69)
Similarly, we obtain the following update rule of the hyperparameter Î¾:
Î¾new = max
â›âœâœâœâœâœâ0, Î¾old âˆ’

zâˆˆZ
M
j=1
2
Y j(Î¨(Î¾)âˆ’Î¨(YjÎ¾))âˆ’Y j
l=1

Î¨(Î¾(j,l|z))âˆ’Î¨
Yk
lâ€²=1 Î¾(j,lâ€²|z)
  3
(K
k=1 Tk) M
j=1 Y j(Î¨(1)(Î¾)âˆ’Y jÎ¨(1)(Y jÎ¾))
ââŸâŸâŸâŸâŸâ .
(4.70)
Let 
S = {rz(z(n) = z)}N
n=1,zâˆˆZ =
2K
k=1 z(n)
k,ik

rH(H)
3N
n=1,zâˆˆZ, Î¦ = {Ï†k}K
k=1, and
Î = {Î¾ j|z}M
j=1,zâˆˆZ be the sets of variational parameters. The EVB learning for the
Bayesian network is summarized in Algorithm 9. If the prior hyperparameters
are ï¬xed and Step 3 in the algorithm is omitted, the algorithm reduces to the
(nonempirical) VB learning algorithm.
4.2.2 Hidden Markov Models
Hidden Markov models (HMMs) have been widely used for sequence modeling
in speech recognition, natural language processing, and so on (Rabiner,

120
4 VB Algorithm for Latent Variable Models
Algorithm 9 EVB learning for the Bayesian network.
1: Initialize the variational parameters (
S, Î¦, Î) and the hyperparameters
(Ï†, Î¾).
2: Apply (substitute the right-hand side into the left-hand side) Eqs. (4.67),
(4.59), (4.65), (4.66), (4.63), and (4.64) to update 
S, Î¦, and Î.
3: Apply Eqs. (4.69) and (4.70) to update Ï† and Î¾, respectively.
4: Evaluate the free energy (4.68).
5: Iterate Steps 2 through 4 until convergence (until the energy decrease
becomes smaller than a threshold).
1989). In this subsection, we consider discrete HMMs. Suppose a sequence
D = (x(1),. . . , x(T)) was observed. Each x(t) is an M-dimensional binary vector
(M-valued ï¬nite alphabet):
x(t) = (x(t)
1 ,. . . , x(t)
M) âˆˆ{e1,. . . , eM},
where if the output symbol at time t is m, then x(t)
m = 1, and otherwise 0.
Moreover, x(t) is produced in K-valued discrete hidden state z(t). The sequence
of hidden states H = (z(1),. . . , z(T)) is generated by a ï¬rst-order Markov
process. Similarly, z(t) is represented by a K-dimensional binary vector
z(t) = (z(t)
1 ,. . . , z(t)
K ) âˆˆ{e1,. . . , eK},
where if the hidden state at time t is k, then z(t)
k = 1, and otherwise 0.
Without loss of generality, we assume that the initial state (t = 1) is the ï¬rst
one, namely z(1)
1 = 1. Then, the probability of a sequence is given by
p(D|w) =

H
M

m=1
bx(1)
m
1,m
T

t=2
K

k=1
K

l=1
a
z(t)
l z(tâˆ’1)
k
k,l
M

m=1
b
z(t)
k x(t)
m
k,m ,
(4.71)
where 
H is taken all over possible values of hidden variables, and the model
parameters, w = (A, B), consist of the state transition probability matrix A =
(a1,. . . ,aK)âŠ¤and the emission probability matrix B = (b1,. . . ,bK)T satisfying
ak = (ak,1,. . . , ak,K)âŠ¤âˆˆÎ”Kâˆ’1 and bk = (bk,1,. . . , bk,K)T âˆˆÎ”Mâˆ’1 for 1 â‰¤k â‰¤K,
respectively. ak,l represents the transition probability from the kth hidden state
to the lth hidden state and bk,m is the emission probability that alphabet m is
produced in the kth hidden state. Figure 4.2 illustrates an example of the state
transition diagram of an HMM.

4.2 Other Latent Variable Models
121
Figure 4.2 State transition diagram of an HMM.
The log-likelihood of the HMM for a sequence of complete data {D, H} is
deï¬ned by
log p(D, H|w) =
T

t=2
K

k=1
K

l=1
z(t)
k z(tâˆ’1)
l
log ak,l +
T

t=1
K

k=1
M

m=1
z(t)
k x(t)
m log bk,m.
We assume that the prior distributions of the transition probability matrix
A and the emission probability matrix B are the Dirichlet distributions with
hyperparameters Ï† > 0 and Î¾ > 0:
p(A|Ï†) =
K

k=1
DirichletK

ak; (Ï†,. . . , Ï†)âŠ¤ 
,
(4.72)
p(B|Î¾) =
K

k=1
DirichletM
bk; (Î¾,. . . , Î¾)âŠ¤ 
.
(4.73)
VB Posterior for HMMs
We deï¬ne the expected sufï¬cient statistics by
Nk =
T

t=1

z(t)
k

rH(H) ,
(4.74)
N
[z]
k,l =
T

t=2

z(t)
l z(tâˆ’1)
k

rH(H) ,
(4.75)
N
[x]
k,m =
T

t=1

z(t)
k

rH(H) x(t)
m ,
(4.76)
where the expected count Nk is constrained by Nk = 
l N
[z]
k,l. Then, the VB
posterior distribution of parameters rw(w) is given by

122
4 VB Algorithm for Latent Variable Models
rA(A) =
K

k=1
DirichletK

ak; (Ï†k,1,. . . ,Ï†k,K)âŠ¤ 
,
rB(B) =
K

k=1
DirichletM
bk; (Î¾k,1,. . . ,Î¾k,M)âŠ¤ 
,
where
Ï†k,l = N
[z]
k,l + Ï†,
(4.77)
Î¾k,m = N
[x]
k,m + Î¾.
(4.78)
The posterior distribution of hidden variables rH(H) is given by
rH(H) =
1
CH
exp
â›âœâœâœâœâœâ
T

t=2
K

k=1
K

l=1
z(t)
k z(tâˆ’1)
l
log ak,l

rA(A)
+
T

t=1
K

k=1
M

m=1
z(t)
k x(t)
m
log bk,m

rB(B)
ââŸâŸâŸâŸâŸâ ,
(4.79)
where CH is the normalizing constant and
log ak,l

rA(A) = Î¨(Ï†k,l) âˆ’Î¨
â›âœâœâœâœâœâ
K

lâ€²=1
Ï†k,lâ€²
ââŸâŸâŸâŸâŸâ ,
log bk,m

rB(B) = Î¨(Î¾k,m) âˆ’Î¨
â›âœâœâœâœâœâ
M

mâ€²=1
Î¾k,mâ€²
ââŸâŸâŸâŸâŸâ .
The expected sufï¬cient statistics

z(t)
k

rH(H) and

z(t)
l z(tâˆ’1)
k

rH(H) in Eqs.
(4.74) through (4.76) can be efï¬ciently computed in the order of O(T) by the
forwardâ€“backward algorithm (Beal, 2003). This algorithm can also compute
CH. Thus, after the substitution of Eq. (4.3), the free energy is given by
F =
K

k=1
â§âªâªâ¨âªâªâ©log
â›âœâœâœâœâœâ
Î“(K
l=1 Ï†k,l)
K
l=1 Î“(Ï†k,l)
ââŸâŸâŸâŸâŸâ +
K

l=1
Ï†k,l âˆ’Ï†
 
Î¨(Ï†k,l) âˆ’Î¨(K
lâ€²=1 Ï†k,lâ€²)
 
+ log
â›âœâœâœâœâœâ
Î“(M
m=1Î¾k,m)
M
m=1 Î“(Î¾k,m)
ââŸâŸâŸâŸâŸâ +
M

m=1
Î¾k,m âˆ’Î¾
 
Î¨(Î¾k,m) âˆ’Î¨(M
mâ€²=1Î¾k,mâ€²)
 â«âªâªâ¬âªâªâ­
âˆ’K log
 Î“(KÏ†)
(Î“(Ï†))K

âˆ’K log
 Î“(MÎ¾)
(Î“(Î¾))M

âˆ’logCH.
(4.80)
The following update rule for the EVB learning of the hyperparameter Ï† is
obtained in the same way as the update rule (4.29) for the GMM:
Ï†new = max

0, Ï†old âˆ’
K2(Î¨(Ï†)âˆ’Î¨(KÏ†))âˆ’K
k=1
K
l=1

Î¨(Ï†k,l)âˆ’Î¨
K
lâ€²=1 Ï†k,lâ€²
  
K2(Î¨(1)(Ï†)âˆ’KÎ¨(1)(KÏ†))

.
(4.81)

4.2 Other Latent Variable Models
123
Algorithm 10 EVB learning for the hidden Markov model.
1: Initialize the variational parameters (
S, Î¦, Î), and the hyperparameters
(Ï†, Î¾).
2: Apply the forwardâ€“backward algorithm to rH(H) in Eq. (4.79) and
compute CH.
3: Apply (substitute the right-hand side into the left-hand side) Eqs. (4.74),
(4.75), (4.76), (4.77), and (4.78) to update Î¦, and Î.
4: Apply Eqs.(4.81) and (4.82) to update Ï† and Î¾, respectively.
5: Evaluate the free energy (4.80).
6: Iterate Steps 2 through 5 until convergence (until the energy decrease
becomes smaller than a threshold).
Similarly, we obtain the following update rule of the hyperparameter Î¾:
Î¾new = max

0, Î¾old âˆ’
KM(Î¨(Î¾)âˆ’Î¨(MÎ¾))âˆ’K
k=1
M
m=1

Î¨(Î¾k,m)âˆ’Î¨
M
mâ€²=1 Î¾k,mâ€²
  
KM(Î¨(1)(Î¾)âˆ’MÎ¨(1)(MÎ¾))

.
(4.82)
Let

S =
)2
z(t)
k

rH(H)
3K
k=1 ,
2
z(t)
l z(tâˆ’1)
k

rH(H)
3K
k,l=1
1T
t=1
,
Î¦ = {Ï†k,l}K
k,l=1, and Î = {Î¾k,m}K,M
k,m=1 be the sets of variational parameters.
The EVB learning for the HMM is summarized in Algorithm 10. If the prior
hyperparameters are ï¬xed, and Step 4 in the algorithm is omitted, the algorithm
reduces to the (nonempirical) VB learning algorithm.
4.2.3 Probabilistic Context-Free Grammars
In this subsection, we discuss probabilistic context-free grammars (PCFGs),
which have been used for more complex sequence modeling applications
than those with the Markov assumption in natural language processing,
bioinformatics, and so on (Durbin et al., 1998). Without loss of generality, we
can assume that the grammar is written by the Chomsky normal form. Let the
model have K nonterminal symbols and M terminal symbols. The observation
sequence of length L is written by X = (x(1),. . . , x(L)) âˆˆ{e1,. . . , eM}L. Then,
the statistical model is deï¬ned by
p(X|w) =

ZâˆˆT(X)
p(X, Z|w),
(4.83)

124
4 VB Algorithm for Latent Variable Models
Figure 4.3 Derivation tree of PCFG. A and B are nonterminal symbols and a and
b are terminal symbols.
p(X, Z|w) =
K

i, j,k=1

aiâ†’jk
 cZ
iâ†’jk
L

l=1
K

i=1
M

m=1
(biâ†’m)z(l)
i x(l)
m ,
w = {{ai}K
i=1, {bi}K
i=1},
ai = {aiâ†’jk}K
j,k=1 (1 â‰¤i â‰¤K),
bi = {biâ†’m}M
m=1 (1 â‰¤i â‰¤K),
where T(X) is the set of derivation sequences that generate X, and Z
corresponds to a tree structure representing a derivation sequence. Figure 4.3
illustrates an example of the derivation tree of a PCFG model. The derivation
sequence is summarized by {cZ
iâ†’jk}K
i, j,k=1 and {z(l)
i }L
l=1, where cZ
iâ†’jk denotes the
count of the transition rule from the nonterminal symbol i to the pair of
nonterminal symbols ( j, k) appearing in the derivation sequence Z and z(l) =
(z(l)
1 ,. . . ,z(l)
K ) is the indicator of the (nonterminal) symbol generating the lth
output symbol of X. Moreover the parameter aiâ†’jk represents the probability
that the nonterminal symbol i emits the pair of nonterminal symbols (j, k) and
biâ†’m represents the probability that the nonterminal symbol i emits the terminal
symbol m. The parameters, {{ai}K
i=1, {bi}K
i=1}, have constraints
aiâ†’ii = 1 âˆ’

(j,k)(i,i)
aiâ†’jk, biâ†’M = 1 âˆ’
Mâˆ’1

m=1
biâ†’m,
i.e., ai âˆˆÎ”K2âˆ’1 and bi âˆˆÎ”Mâˆ’1, respectively.
Let D
=
{X(1),. . . , X(N)} be a given training corpus and H
=
{Z(1),. . . , Z(N)} be the corresponding hidden derivation sequences. The log-
likelihood for the complete sample {D, H} is given by

4.2 Other Latent Variable Models
125
log p(D, H|w) =
N

n=1
â¡â¢â¢â¢â¢â¢â¢â£
K

i, j,k=1
cZ(n)
iâ†’jk log aiâ†’jk +
L

l=1
K

i=1
M

m=1
z(n,l)
i
x(n,l)
m
log biâ†’m
â¤â¥â¥â¥â¥â¥â¥â¦,
where x(n,l) and z(n,l) are the indicators of the lth output symbol and the
nonterminal symbol generating the lth output in the nth sequences, X(n) and
Z(n), respectively.
We now turn to the VB learning for PCFGs (Kurihara and Sato, 2004).
We assume that the prior distributions of parameters {ai}K
i=1 and {bi}K
i=1 are the
Dirichlet distributions with hyperparameters Ï† and Î¾:
p({ai}K
i=1|Ï†) =
K

i=1
DirichletK2

ai; (Ï†,. . . , Ï†)âŠ¤ 
,
(4.84)
p({bi}K
i=1|Î¾) =
K

i=1
DirichletM

bi; (Î¾,. . . , Î¾)âŠ¤ 
.
(4.85)
VB Posterior for PCFGs
We deï¬ne the expected sufï¬cient statistics as follows:
N
z
iâ†’jk =
N

n=1
L

l=1

cZ(n)
iâ†’jk

rz(Z(n)) ,
(4.86)
N
z
i =
K

j,k=1
N
z
iâ†’jk,
N
x
iâ†’m =
N

n=1
L

l=1

z(n,l)
i

rz(Z(n)) x(n,l)
m ,
(4.87)
N
x
i =
M

m=1
N
x
iâ†’m.
Then the VB posteriors of the parameters are given by
rw(w) = ra({ai}K
i=1)rb({bi}K
i=1),
ra({ai}K
i=1) =
K

i=1
DirichletK2

ai; (Ï†iâ†’11,. . . ,Ï†iâ†’KK)âŠ¤ 
,
(4.88)
rb({bi}K
i=1) =
K

i=1
DirichletM

bi; (Î¾iâ†’1,. . . ,Î¾iâ†’M)âŠ¤ 
,
(4.89)
where
Ï†iâ†’jk = N
z
iâ†’jk + Ï†,
(4.90)
Î¾iâ†’m = N
x
iâ†’m + Î¾.
(4.91)

126
4 VB Algorithm for Latent Variable Models
The VB posteriors of the hidden variables are given by
rH(H) =
N

n=1
rz(Z(n)),
rz(Z(n)) =
1
CZ(n) exp $Î³Z(n)% ,
(4.92)
Î³Z(n) =
K

i, j,k=1
cZ(n)
iâ†’jk

log aiâ†’jk

ra({ai}K
i=1)
+
L

l=1
K

i=1
M

m=1
z(n,l)
i
x(n,l)
m
log biâ†’m

rb({bi}K
i=1) ,
where CZ(n) = 
ZâˆˆT(X(n)) exp(Î³Z) is the normalizing constant and

log aiâ†’jk

ra({ai}K
i=1) = Î¨
Ï†iâ†’jk
 
âˆ’Î¨
K
jâ€²=1
K
kâ€²=1 Ï†iâ†’jâ€²kâ€²
 
,
log biâ†’m

rb({bi}K
i=1) = Î¨
Î¾iâ†’m
 
âˆ’Î¨
M
mâ€²=1Î¾iâ†’mâ€²
 
.
All the expected sufï¬cient statistics and CZ(n) can be efï¬ciently computed
by the insideâ€“outside algorithm (Kurihara and Sato, 2004). The free energy,
after the substitution of Eq. (4.3), is given by
F =
K

i=1
â§âªâªâ¨âªâªâ©log
â›âœâœâœâœâœâœâ
Î“(K
j,k=1 Ï†iâ†’jk)
K
j,k=1 Î“(Ï†iâ†’jk)
ââŸâŸâŸâŸâŸâŸâ 
+
K

j,k=1
Ï†iâ†’jk âˆ’Ï†
 
Î¨
Ï†iâ†’jk
 
âˆ’Î¨
K
jâ€²,kâ€²=1 Ï†iâ†’jâ€²kâ€²)
  
+ log
â›âœâœâœâœâœâœâ
Î“
M
m=1Î¾iâ†’m
 
M
m=1 Î“
Î¾iâ†’m
 
ââŸâŸâŸâŸâŸâŸâ +
M

m=1
Î¾iâ†’m âˆ’Î¾
 
Î¨
Î¾iâ†’m
 
âˆ’Î¨
M
mâ€²=1Î¾iâ†’mâ€²
  
â«âªâªâªâ¬âªâªâªâ­
âˆ’K log
 Î“(K2Ï†)
(Î“(Ï†))K2

âˆ’K log
 Î“(MÎ¾)
(Î“(Î¾))M

âˆ’
N

n=1
logCZ(n).
(4.93)
The following update rules for the EVB learning of the hyperparameters Ï†
and Î¾ are obtained similarly to the HMM in Eqs. (4.81) and (4.82):
Ï†new = max

0, Ï†old âˆ’
K3(Î¨(Ï†)âˆ’Î¨(K2Ï†))âˆ’K
i=1
K
j,k=1

Î¨
Ï†iâ†’jk
 
âˆ’Î¨
K
jâ€²,kâ€²=1 Ï†iâ†’jâ€²kâ€²
  
K3(Î¨(1)(Ï†)âˆ’K2Î¨(1)(K2Ï†))

,
(4.94)
Î¾new = max

0, Î¾old âˆ’
KM(Î¨(Î¾)âˆ’Î¨(MÎ¾))âˆ’K
i=1
M
m=1

Î¨
Î¾iâ†’m
 
âˆ’Î¨
M
mâ€²=1 Î¾iâ†’mâ€²
  
KM(Î¨(1)(Î¾)âˆ’MÎ¨(1)(MÎ¾))

.
(4.95)

4.2 Other Latent Variable Models
127
Algorithm 11 EVB learning for probabilistic context-free grammar.
1: Initialize the variational parameters (
S, Î¦, Î) and the hyperparameters
(Ï†, Î¾).
2: Apply the insideâ€“outside algorithm to rz(Z(n)) in Eq. (4.92) and compute
CZ(n) for n = 1,. . . , N.
3: Apply (substitute the right-hand side into the left-hand side) Eqs. (4.86),
(4.87), (4.90), and (4.91) to update Î¦, and Î.
4: Apply Eqs. (4.94) and (4.95) to update Ï† and Î¾, respectively.
5: Evaluate the free energy (4.93).
6: Iterate Steps 2 through 5 until convergence (until the energy decrease
becomes smaller than a threshold).
Let

S =
)2
cZ(n)
iâ†’jk

rz(Z(n))
3K
i, j,k=1 ,
2
z(n,l)
i

rz(Z(n))
3L
l=1
1N
n=1
,
Î¦ = {Ï†iâ†’jk}K
i, j,k=1, and Î = {Î¾iâ†’m}K,M
i,m=1 be the sets of variational parameters. The
EVB learning for the PCFG model is summarized in Algorithm 11. If the prior
hyperparameters are ï¬xed and Step 4 in the algorithm is omitted, the algorithm
reduces to the (nonempirical) VB learning algorithm.
4.2.4 Latent Dirichlet Allocation
Latent Dirichlet allocation (LDA) (Blei et al., 2003) is a generative model
successfully used in various applications such as text analysis (Blei et al.,
2003), image analysis (Li and Perona, 2005), genomics (Bicego et al., 2010;
Chen et al., 2010), human activity analysis (Huynh et al., 2008), and collab-
orative ï¬ltering (Krestel et al., 2009; Purushotham et al., 2012). Given word
occurrences of documents in a corpora, LDA expresses each document as a
mixture of multinomial distributions, each of which is expected to capture a
topic. The extracted topics provide bases in a low-dimensional feature space,
in which each document is compactly represented. This topic expression was
shown to be useful for solving various tasks, including classiï¬cation (Li and
Perona, 2005), retrieval (Wei and Croft, 2006), and recommendation (Krestel
et al., 2009).
In this subsection, we introduce the VB learning for tractable inference in
the LDA model. Suppose that we observe M documents, each of which consists
of N(m) words. Each word is included in a vocabulary with size L. We assume
that each word is associated with one of the H topics, which is not observed.

128
4 VB Algorithm for Latent Variable Models
We express the word occurrence by an L-dimensional indicator vector w, where
one of the entries is equal to one and the others are equal to zero. Similarly,
we express the topic occurrence as an H-dimensional indicator vector z. We
deï¬ne the following functions that give the item numbers chosen by w and z,
respectively:
Â´l(w) = l if wl = 1 and wlâ€² = 0 for lâ€²  l,
Â´h(z) = h if zh = 1 and zhâ€² = 0 for hâ€²  h.
In the LDA model (Blei et al., 2003), the word occurrence w(n,m) of the
nth position in the mth document is assumed to follow the multinomial
distribution:
p(w(n,m)|Î˜, B) =
L

l=1

(BÎ˜âŠ¤)l,m
 w(n,m)
l
= (BÎ˜âŠ¤)Â´l(w(n,m)),m,
(4.96)
where Î˜ âˆˆ[0, 1]MÃ—H and B âˆˆ[0, 1]LÃ—H are parameter matrices to be estimated.
The rows of Î˜ = (Î¸1,. . . ,Î¸M)âŠ¤and the columns of B = $Î²1,. . . , Î²H
% are
probability mass vectors that sum up to one. That is, Î¸m âˆˆÎ”Hâˆ’1 is the topic
distribution of the mth document, and Î²h âˆˆÎ”Lâˆ’1 is the word distribution of the
hth topic.
Suppose that we observe the data D = {{w(n,m)}N(m)
n=1 }M
m=1. Given the topic
occurrence latent variable z(n,m), the complete likelihood for each word is
written as
p(w(n,m), z(n,m)|Î˜, B) = p(w(n,m)|z(n,m), B)p(z(n,m)|Î˜),
(4.97)
where p(w(n,m)|z(n,m), B) =
L

l=1
H

h=1
(Bl,h)w(n,m)
l
z(n,m)
h , p(z(n,m)|Î˜) =
H

h=1
(Î¸m,h)z(n,m)
h .
We assume the Dirichlet priors on Î˜ and B:
p(Î˜|Î±) =
M

m=1
DirichletH(Î¸m; (Î±1,. . . , Î±H)âŠ¤),
(4.98)
p(B|Î·) =
H

h=1
DirichletL(Î²h; (Î·1,. . . , Î·L)âŠ¤),
(4.99)
where Î± and Î· are hyperparameters that control the prior sparsity.
VB Posterior for LDA
For the set of all hidden variables H = {{z(n,m)}N(m)
n=1 }M
m=1 and the parameter w =
(Î˜, B), we assume that our approximate posterior is factorized as Eq. (4.1).
Thus, the update rule (4.2) yields the further factorization rÎ˜,B(Î˜, B) =
rÎ˜(Î˜)rB(B) and the following update rules:

4.2 Other Latent Variable Models
129
rÎ˜(Î˜) âˆp(Î˜|Î±) log p(D, H|Î˜, B)
rB(B)rH(H) ,
(4.100)
rB(B) âˆp(B|Î·) log p(D, H|Î˜, B)
r(Î˜)rH(H) .
(4.101)
Deï¬ne the expected sufï¬cient statistics as
N
(m)
h
=
N(m)

n=1

z(n,m)
h

rz

{{z(n,m)}N(m)
n=1 }M
m=1
 ,
(4.102)
Wl,h =
M

m=1
N(m)

n=1
w(n,m)
l

z(n,m)
h

rz

{{z(n,m)}N(m)
n=1 }M
m=1
 .
(4.103)
Then, the VB posterior distribution is given by the Dirichlet distributions:
rÎ˜(Î˜) =
M

m=1
Dirichlet
Î¸m;Î±m
 
,
(4.104)
rB(B) =
H

h=1
Dirichlet $Î²h;Î·h
% ,
(4.105)
where the variational parameters satisfy
Î±m,h = (Î±m)h = N
(m)
h
+ Î±h,
(4.106)
Î·l,h = (Î·h)l = Wl,h + Î·l.
(4.107)
From the update rule (4.3), the VB posterior distribution of latent variables
is given by the multinomial distribution:
rz

{{z(n,m)}N(m)
n=1 }M
m=1
 
=
M

m=1
N(m)

n=1
MultinomialH,1

z(n,m);z(n,m) 
,
(4.108)
where the variational parameterz(n,m) âˆˆÎ”Hâˆ’1 is
z(n,m)
h
=
z(n,m)
h
H
hâ€²=1 z(n,m)
hâ€²
(4.109)
for
z(n,m)
h
= exp
â›âœâœâœâœâœâ
log Î˜m,h

rÎ˜(Î˜) +
L

l=1
w(n,m)
l
log Bl,h

rB(B)
ââŸâŸâŸâŸâŸâ .
(4.110)
We also have

z(n,m)
h

rz

{{z(n,m)}N(m)
n=1 }M
m=1
 =z(n,m)
h
,
log Î˜m,h

rÎ˜(Î˜) = Î¨(Î±m,h) âˆ’Î¨
H
hâ€²=1 Î±m,hâ€²
 
,
log Bl,h

rB(B) = Î¨(Î·l,h) âˆ’Î¨
L
lâ€²=1Î·lâ€²,h
 
.

130
4 VB Algorithm for Latent Variable Models
Iterating Eqs. (4.106), (4.107), and (4.110) provides a local minimum of the
free energy, which is given as a function of the variational parameters by
F =
M

m=1
â›âœâœâœâœâlog
â›âœâœâœâœâ
Î“(H
h=1 Î±m,h)
H
h=1 Î“(Î±m,h)
ââŸâŸâŸâŸâ âˆ’log
â›âœâœâœâœâ
Î“(H
h=1 Î±h)
H
h=1 Î“(Î±h)
ââŸâŸâŸâŸâ 
ââŸâŸâŸâŸâ 
+
H

h=1
â›âœâœâœâœâlog
â›âœâœâœâœâ
Î“(L
l=1Î·l,h)
L
l=1 Î“(Î·l,h)
ââŸâŸâŸâŸâ âˆ’log
â›âœâœâœâœâ
Î“(L
l=1 Î·l)
L
l=1 Î“(Î·l)
ââŸâŸâŸâŸâ 
ââŸâŸâŸâŸâ 
+
M

m=1
H

h=1
!
Î±m,h âˆ’(N
(m)
h
+ Î±h)
" 
Î¨(Î±m,h) âˆ’Î¨(H
hâ€²=1 Î±m,hâ€²)
 
+
H

h=1
L

l=1

Î·l,h âˆ’(Wl,h + Î·l)
 
Î¨(Î·l,h) âˆ’Î¨(L
lâ€²=1Î·lâ€²,h)
 
+
M

m=1
N(m)

n=1
H

h=1
z(n,m)
h
logz(n,m)
h
.
(4.111)
The partial derivatives of the free energy with respect to (Î±, Î·) are computed
as follows:
âˆ‚F
âˆ‚Î±h
= M

Î¨(Î±h) âˆ’Î¨(H
hâ€²=1 Î±hâ€²)
 
âˆ’
M

m=1

Î¨(Î±m,h) âˆ’Î¨(H
hâ€²=1 Î±m,hâ€²)
 
,
(4.112)
âˆ‚2F
âˆ‚Î±hâˆ‚Î±hâ€² = M

Î´h,hâ€²Î¨(1)(Î±h) âˆ’Î¨(1)(H
hâ€²â€²=1 Î±hâ€²â€²)
 
,
(4.113)
âˆ‚F
âˆ‚Î·l
= H

Î¨(Î·l) âˆ’Î¨(L
lâ€²=1 Î·lâ€²)
 
âˆ’
H

h=1

Î¨(Î·l,h) âˆ’Î¨(L
lâ€²=1Î·lâ€²,h)
 
,
(4.114)
âˆ‚2F
âˆ‚Î·lâˆ‚Î·lâ€² = H

Î´l,lâ€²Î¨(1)(Î·l) âˆ’Î¨(1)(L
lâ€²â€²=1 Î·lâ€²â€²)
 
,
(4.115)
where Î´n,nâ€² is the Kronecker delta. Thus, we have the following Newtonâ€“
Raphson steps to update the hyperparameters:
Î±new = max
â›âœâœâœâœâœâ0, Î±old âˆ’
 âˆ‚2F
âˆ‚Î±âˆ‚Î±âŠ¤
âˆ’1 âˆ‚F
âˆ‚Î±
ââŸâŸâŸâŸâŸâ ,
(4.116)
Î·new = max
â›âœâœâœâœâœâ0, Î·old âˆ’
 âˆ‚2F
âˆ‚Î·âˆ‚Î·âŠ¤
âˆ’1 âˆ‚F
âˆ‚Î·
ââŸâŸâŸâŸâŸâ ,
(4.117)
where max(Â·) is the max operator applied elementwise.

4.2 Other Latent Variable Models
131
Algorithm 12 EVB learning for latent Dirichlet allocation.
1: Initialize the variational parameters ({{z(n,m)}N(m)
n=1 }M
m=1, {Î±m}M
m=1, {Î·h}H
h=1),
and the hyperparameters (Î±, Î·).
2: Apply (substitute the right-hand side into the left-hand side) Eqs. (4.110),
(4.109), (4.102), (4.103), (4.106), and (4.107) to update {{z(n,m)}N(m)
n=1 }M
m=1,
{Î±m}M
m=1, and {Î·h}H
h=1.
3: Apply Eqs. (4.116) and (4.117) to update Î± and Î·, respectively.
4: Evaluate the free energy (4.111).
5: Iterate Steps 2 through 4 until convergence (until the energy decrease
becomes smaller than a threshold).
The EVB learning for LDA is summarized in Algorithm 12. If the prior
hyperparameters are ï¬xed and Step 3 in the algorithm is omitted, the algorithm
reduces to the (nonempirical) VB learning algorithm.
We can also apply partially Bayesian (PB) learning by approximating the
posterior of Î˜ or B by the delta function (see Section 2.2.2). We call it PB-A
learning if Î˜ is marginalized and B is point-estimated, and PB-B learning
if B is marginalized and Î˜ is point-estimated. Note that the original VB
algorithm for LDA proposed by Blei et al. (2003) corresponds to PB-A learing
in our terminology. MAP learning, where both of Î˜ and B are point-estimated,
corresponds to the probabilistic latent semantic analysis (pLSA) (Hofmann,
2001), if we assume the ï¬‚at prior Î±h = Î·l = 1 (Girolami and Kaban, 2003).

5
VB Algorithm under No Conjugacy
As discussed in Section 2.1.7, there are practical combinations of a model
and a prior where conjugacy is no longer available. In this chapter, as
a method for addressing nonconjugacy, we demonstrate local variational
approximation (LVA), also known as direct site bounding, for logistic regres-
sion and a sparsity-inducing prior (Jaakkola and Jordan, 2000; Girolami, 2001;
Bishop, 2006; Seeger, 2008, 2009). Then we describe a general framework
of LVA based on convex functions by using the associated Bregman diver-
gence (Watanabe et al., 2011).
5.1 Logistic Regression
Let D = {(x(1), y(1)), (x(2), y(2)),. . . , (x(N), y(N))} be the N observations of the
binary response variable y(n) âˆˆ{0, 1} and the input vector x(n) âˆˆRM. The
logistic regression model assumes the following Bernoulli model over y =
(y(1), y(2),. . . , y(N))âŠ¤given X = {x(1), x(2),. . . , x(N)}:
p(y|X, w) =
N

n=1
exp

y(n)(wâŠ¤x(n)) âˆ’log

1 + ewâŠ¤x(n)  
.
(5.1)
Let us consider the Bayesian learning of the parameter w assuming the
Gaussian prior distribution:
p(w) = GaussM(w; w0, Sâˆ’1
0 ),
where S0 and w0 are the hyperparameters.
Gaussian approximations for the posterior distribution p(w|D) âˆp(w, y|X)
are obtained by LVA based on the facts that âˆ’log(e
âˆš
h/2 + eâˆ’
âˆš
h/2) is a convex
function of h and that log(1 + eg) is a convex function of g. More speciï¬cally,
132

5.1 Logistic Regression
133
because Ï†(h(w)) = âˆ’log(e
âˆš
w2/2 + eâˆ’
âˆš
w2/2) is a convex function of h(w) = w2
and Ïˆ(g(w)) = log(1 + ew) is a convex function of g(w) = w, they are bounded
from below by their tangents at h(Î¾) = Î¾2 and g(Î·) = Î·, respectively:
âˆ’log
!
e
âˆš
w2/2 + eâˆ’
âˆš
w2/2"
â‰¥âˆ’log
!
e
âˆš
Î¾2/2 + eâˆ’âˆš
Î¾2/2"
âˆ’(w2 âˆ’Î¾2)tanh (Î¾/2)
4Î¾
,
log(1 + ew) â‰¥log(1 + eÎ·) + (w âˆ’Î·)
eÎ·
1 + eÎ· .
By substituting these bounds into the likelihood (5.1), we obtain the following
bounds on p(w, y|X):
p(w; Î¾) â‰¤p(w, y|X) â‰¤p(w; Î·),
where
p(w; Î¾) â‰¡p(w)
N

n=1
exp

y(n) âˆ’1
2

wâŠ¤x(n)
âˆ’Î¸n
,
(wâŠ¤x(n))2 âˆ’hn
-
âˆ’log
!
e
âˆšhn
2 + eâˆ’
âˆšhn
2
""
,
p(w; Î·) â‰¡p(w)
N

n=1
exp

(y(n) âˆ’Îºn)wâŠ¤x(n) âˆ’b(Îºn)
 
.
Here we have put
Î¸n = tanh( âˆšhn/2)
4 âˆšhn
,
(5.2)
Îºn =
egn
1 + egn ,
(5.3)
and {hn}N
n=1 and {gn}N
n=1 are the sets of variational parameters deï¬ned from
Î¾ = (Î¾1, Î¾2,. . . , Î¾M)âŠ¤and Î· = (Î·1, Î·2,. . . , Î·M)âŠ¤by the transformations hn =
(Î¾âŠ¤x(n))2 and gn = Î·âŠ¤x(n), respectively. We also deï¬ned the binary entropy
function by b(Îº) = âˆ’Îº log Îº âˆ’(1 âˆ’Îº) log(1 âˆ’Îº) for Îº âˆˆ[0, 1].
Normalizing these bounds with respect to w, we approximate the posterior
by the Gaussian distributions as
qÎ¾(w; Î¾) = GaussM(m, Sâˆ’1),
qÎ·(w; Î·) = GaussM(m, S
âˆ’1),
whose mean and precision (inverse-covariance) matrix are respectively
given by
m = Sâˆ’1 ,
S0w0 + N
n=1(y(n) âˆ’1/2)x(n)-
,
S = S0 + 2 N
n=1 Î¸nx(n)x(n)âŠ¤,
(5.4)

134
5 VB Algorithm under No Conjugacy
and
m = w0 + Sâˆ’1
0
N
n=1(y(n) âˆ’Îºn)x(n),
S = S0.
(5.5)
We also obtain the bounds for the marginal likelihood, Z(Î¾) â‰¡

p(w; Î¾)dw
and Z(Î·) â‰¡

p(w; Î·)dw. These are respectively given in the forms of free
energy bounds as follows:
F(Î¾) â‰¡âˆ’log Z(Î¾)
= 1
2 log |S| âˆ’1
2 log |S0| + wâŠ¤
0 S0w0
2
âˆ’mâŠ¤(S)m
2
âˆ’
N

n=1
)
hnÎ¸n âˆ’log

2 cosh
 âˆšhn
2
1
,
(5.6)
and
F(Î·) â‰¡âˆ’log Z(Î·)
= wâŠ¤
0 S0w0
2
âˆ’mâŠ¤S0m
2
+
N

n=1
b(Îºn).
We optimize the free energy bounds to determine the variational parameters.
As will be discussed generally in Section 5.3.2, to decrease the upper-bound
F(Î¾), the EM algorithm is available, which instead maximizes

log p(w; Î¾)

qÎ¾(w;Î¾o) ,
where the expectation is taken with respect to the approximate posterior before
updating with the variational parameters given by Î¾o. The update rule of the
variational parameters is speciï¬cally given by
hn =

(wâŠ¤x(n))2
qÎ¾(w;Î¾o)
= x(n)âŠ¤(Sâˆ’1 + mmâŠ¤)x(n),
(5.7)
where m and Sâˆ’1 are the mean and covariance matrix of qÎ¾(w; Î¾o).
We can use the following gradient for the maximization of the lower-bound
F(Î·):
âˆ‚F(Î·)
âˆ‚Îºn
=

wâŠ¤x(n)
qÎ·(w;Î·) âˆ’Î·âŠ¤x(n)
= mâŠ¤x(n) âˆ’gn.
(5.8)
The Newtonâ€“Raphson step to update Îº = (Îº1,. . . , ÎºN)âŠ¤is given by
Îºnew = Îºold âˆ’
 âˆ‚2F
âˆ‚Îºâˆ‚ÎºâŠ¤
âˆ’1 âˆ‚F
âˆ‚Îº ,
(5.9)

5.2 Sparsity-Inducing Prior
135
Algorithm 13 LVA algorithm for logistic regression.
1: Initialize the variational parameters {hn}N
n=1 and transform them to {Î¸n}N
n=1
by Eq. (5.2).
2: Compute the approximate posterior mean and covariance matrix (m, Sâˆ’1)
by Eq. (5.4).
3: Apply Eq. (5.7) to update {hn}N
n=1 and transform them to {Î¸n}N
n=1 by
Eq. (5.2).
4: Evaluate the free energy bound (5.6).
5: Iterate Steps 2 through 4 until convergence (until the decrease of the bound
becomes smaller than a threshold).
where the (n, nâ€²)th entry of the Hessian matrix is given as follows:
âˆ‚2F(Î·)
âˆ‚Îºnâˆ‚Îºnâ€² = âˆ’x(n)âŠ¤Sâˆ’1
0 x(nâ€²) âˆ’Î´n,nâ€²
 1
Îºn
+
1
1 âˆ’Îºn

.
The learning algorithm for logistic regression with LVA is summarized in
Algorithm 13 in the case of F(Î¾) minimization. To obtain the algorithm for
F(Î·) maximization, the updated variables are replaced with {gn}N
n=1, {Îºn}N
n=1,
and (m, S
âˆ’1), and the update rule (5.7) in Step 3 is replaced with the Newtonâ€“
Raphson step (5.9).
Recall the arguments in Section 2.1.7 that the VB posterior r(w;Î») = q(w; Î¾)
in Eq. (2.32) minimizes the upper-bound of the free energy (2.25) and the
approximate posterior r(w;Î½) = q(w; Î·) in Eq. (2.58) maximizes the lower-
bound of the objective function of EP (2.50). This means that the variational
parameters are given byÎ» = (m, Sâˆ’1) andÎ½ = (m, S
âˆ’1) in the LVAs for VB and
EP, respectively.
5.2 Sparsity-Inducing Prior
Another representative example where a nonconjugate prior is used is the linear
regression model with a sparsity-inducing prior distribution (Girolami, 2001;
Seeger, 2008, 2009). We discuss the linear regression model for i.i.d. data D =
{(x(1), y(1)), (x(2), y(2)),. . . , (x(N), y(N))}, where for each observation, x(n) âˆˆRM
is the input vector and y(n) âˆˆR is the response. Denoting y = (y(1),. . . , y(N))âŠ¤
and X = (x(1),. . . , x(N))âŠ¤, we assume the model,
p(y|X, w) = GaussN(y; Xw, Ïƒ2IN),

136
5 VB Algorithm under No Conjugacy
and the following sparsity-inducing prior with the LÎ²-norm:
p(w) =
M

m=1
1
CÎ²,Î³
exp

âˆ’Î³|wm|Î² 
,
(5.10)
where Î³ > 0 and 0 < Î² â‰¤2 are the hyperparameters, and CÎ²,Î³ = 2
Î²Î³1âˆ’1/Î²Î“(1/Î²)
is the normalizing constant. For 0 < Î² < 2, the prior has a heavier tail than the
Gaussian distribution (Î² = 2) and induces sparsity of the coefï¬cients w.
We apply the following inequality for w, Î¾ âˆˆR:

w2 Î²/2 â‰¤Î²
2

Î¾2 Î²
2 âˆ’1 
w2 âˆ’Î¾2 
,
which is obtained from the concavity of the function f(x) = xÎ²/2 for x > 0
and 0 < Î² < 2. Introducing the variational parameter to each dimension,
Î¾
=
(Î¾1,. . . , Î¾M)âŠ¤and bounding the nonconjugate prior (5.10) by this
inequality, we have
p(y, w|X) = p(y|X, w)p(w)
â‰¥
1
(2Ï€Ïƒ2)N/2CM
Î²,Î³
Â· exp
â›âœâœâœâœâœââˆ’1
2Ïƒ2
N

n=1
(y(n) âˆ’wâŠ¤x(n))2 âˆ’Î²Î³
2
M

m=1

Î¾2
m
 Î²
2 âˆ’1 
w2
m âˆ’Î¾2
m
 ââŸâŸâŸâŸâŸâ 
â‰¡p(w; Î¾).
Normalizing the lower-bound p(w; Î¾), we obtain a Gaussian approximation to
the posterior. This is in effect equivalent to assuming the Gaussian prior for w:
GaussM(w; 0, Sâˆ’1
Î¾ ),
where SÎ¾ = Î³Î²Diag(Î¾Î²âˆ’1/2) for Î¾Î²âˆ’1/2 â‰¡

Î¾2
1
 Î²
2 âˆ’1 ,. . . ,

Î¾2
M
 Î²
2 âˆ’1âŠ¤
.
The resulting Gaussian approximation to the posterior is
qÎ¾(w; Î¾) = GaussM(w; m, Sâˆ’1),
where
S = SÎ¾ + 1
Ïƒ2 XâŠ¤X,
(5.11)
m = 1
Ïƒ2 Sâˆ’1XâŠ¤y.
(5.12)

5.3 Uniï¬ed Approach by Local VB Bounds
137
Algorithm 14 LVA algorithm for sparse linear regression.
1: Initialize the variational parameters {Î¾2
m}M
m=1.
2: Compute the approximate posterior mean and covariance matrix (m, Sâˆ’1)
by Eqs. (5.11) and (5.12).
3: Apply Eq. (5.14) to update {Î¾2
m}M
m=1.
4: Evaluate the free energy bound (5.13).
5: Iterate Steps 2 through 4 until convergence (until the decrease of the bound
becomes smaller than a threshold).
We also obtain the upper bound for the free energy:
F(Î¾) = âˆ’log

p(w; Î¾)dw
= N âˆ’M
2
log(2Ï€) + log |S|
2
+ M logCÎ²,Î³ âˆ’
N

n=1
(y(n))2
2Ïƒ2 + Î³Î²
2
M

m=1

Î¾2
m
 Î²/2 ,
(5.13)
which is optimized with respect to the variational parameter. The general
framework in Section 5.3.2, which corresponds to the EM algorithm, provides
the following update rule:
Î¾2
m =

w2
m

qÎ¾(w;Î¾o)
= (S âˆ’1)mm + m2
m,
(5.14)
where m and Sâˆ’1 are the mean and covariance matrix of qÎ¾(w; Î¾o).
The learning algorithm for sparse linear regression with this approximation
is summarized in Algorithm 14.
This approximation has been applied to the Laplace prior (Î² = 1) in Seeger
(2008, 2009). LVA for another heavy-tailed distribution, p(w) âˆcoshâˆ’1/Î²(Î²w),
is discussed in Girolami (2001), which also bridges the Gaussian distribution
(Î² â†’0) and the Laplace distribution (Î² â†’âˆ).
5.3 Uniï¬ed Approach by Local VB Bounds
As discussed in Section 2.1.7, LVA for VB and LVA for EP form lower- and
upper-bounds of the joint distribution p(w, D), denoted by p(w; Î¾) and p(w; Î·),
respectively. If the bounds satisfying

138
5 VB Algorithm under No Conjugacy
p(w; Î¾) â‰¤p(w, D),
(5.15)
p(w; Î·) â‰¥p(w, D),
(5.16)
for all w and D are analytically integrable, then by normalizing the bounds
instead of p(w, D), LVAs approximate the posterior distribution by
qÎ¾(w; Î¾) =
p(w; Î¾)
Z(Î¾) ,
(5.17)
qÎ·(w; Î·) = p(w; Î·)
Z(Î·)
,
(5.18)
respectively, where Z(Î¾) and Z(Î·) are the normalization constants deï¬ned by
Z(Î¾) =

p(w; Î¾)dw,
Z(Î·) =

p(w; Î·)dw.
Here Î¾ and Î· are called the variational parameters.
The respective approximations are optimized by estimating the variational
parameters, Î¾ and Î· so that Z(Î¾) is maximized and Z(Î·) is minimized since the
inequalities
Z(Î¾) â‰¤Z â‰¤Z(Î·)
(5.19)
hold by deï¬nition, where Z = p(D) is the marginal likelihood.
To consider the respective LVAs in terms of information divergences in later
sections, let us introduce the Bayes free energy,
FBayes â‰¡âˆ’log Z,
and its lower- and upper-bounds, F(Î·) = âˆ’log Z(Î·) and F(Î¾) = âˆ’log Z(Î¾). By
taking the negative logarithms on both sides of Eq. (5.19), we have
F(Î·) â‰¤FBayes â‰¤F(Î¾).
(5.20)
Hereafter, we follow the measure of the free energy and adopt the following
terminology to refer to respective LVAs (5.18) and (5.17): the lower-bound
maximization (F(Î·) maximization) and the upper-bound minimization (F(Î¾)
minimization).
5.3.1 Divergence Measures in LVA
Most of the existing LVA techniques are based on the convexity of the
log-likelihood function or the log-prior (Jaakkola and Jordan, 2000; Bishop,

5.3 Uniï¬ed Approach by Local VB Bounds
139
2006; Seeger, 2008, 2009). We describe these cases by using general convex
functions, Ï† and Ïˆ, and show that the objective functions,
F(Î¾) âˆ’FBayes = log
Z
Z(Î¾) â‰¥0,
FBayes âˆ’F(Î·) = log Z(Î·)
Z
â‰¥0,
to be minimized in the approximations (5.17) and (5.18), are decomposable
into the sum of the KL divergence and the expected Bregman divergence.
Let Ï† and Ïˆ be twice differentiable real-valued strictly convex functions and
denote by dÏ† the Bregman divergence associated with the function Ï† (Banerjee
et al., 2005):
dÏ†(v1, v2) = Ï†(v1) âˆ’Ï†(v2) âˆ’(v1 âˆ’v2)âŠ¤âˆ‡Ï†(v2) â‰¥0,
(5.21)
where âˆ‡Ï†(v2) denotes the gradient vector of Ï† at v2.
Let us consider the case where Ï† and Ïˆ are respectively used to form the
following bounds of the joint distribution p(w, D):
p(w; Î¾) = p(w, D) exp{âˆ’dÏ†(h(w), h(Î¾))},
(5.22)
p(w; Î·) = p(w, D) exp{dÏˆ(g(w), g(Î·))},
(5.23)
where h and g are vector-valued functions of w.1
Eq. (5.22) is interpreted as follows. log p(w, D) includes a term that
prevents analytic integration of p(w, D) with respect to w. If such a term
is expressed by the convex function Ï† of some function h transforming w,
it is replaced by the tangent hyperplane, Ï†(h(Î¾)) + (h(w) âˆ’h(Î¾))âŠ¤âˆ‡Ï†(h(Î¾)),
so that log p(w; Î¾) makes a simpler function of w, such as a quadratic
function. Remember that if log p(w; Î¾) is quadratic with respect to w, p(w; Î¾)
is analytically integrable by the Gaussian integral.
Rephrased in terms of the convex duality theory (Jordan et al., 1999;
Bishop, 2006), Ï†(h(w)) is replaced by its lower-bound,
Ï†(h(w)) â‰¥Ï†(h(Î¾)) + (h(w) âˆ’h(Î¾))âŠ¤âˆ‡Ï†(h(Î¾))
(5.24)
= Î¸(Î¾)âŠ¤h(w) âˆ’Ï†(Î¸(Î¾)),
(5.25)
where we have put Î¸(Î¾) = âˆ‡Ï†(h(Î¾)) and
Ï†(Î¸(Î¾)) = Î¸(Î¾)âŠ¤h(Î¾) âˆ’Ï†(h(Î¾))
= max
h {Î¸(Î¾)âŠ¤h âˆ’Ï†(h)}
1 The functions g and h (also Ïˆ and Ï†) can be dependent on D in this discussion. However, we
denote them as if they were independent of D for simplicity. They are actually independent of
D in the examples in Sections 5.1 and 5.2 and in most applications (Bishop, 2006; Seeger,
2008, 2009).

140
5 VB Algorithm under No Conjugacy
Ï†(h(w))
dÏ†(h(w), h(Î¾))
h(w)
h(Î¾)
âˆ’ËœÏ†(Î¸(Î¾))
Î¸(Î¾)h(w) âˆ’ËœÏ†(Î¸(Î¾))
Figure 5.1 Convex function Ï† (solid curve), its tangent (dashed line), and the
Bregman divergence (arrow).
is the conjugate function of Ï†. The inequality (5.24) indicates the fact that
the convex function Ï† is bounded globally by its tangent at h(Î¾), which is
equivalent to the nonnegativity of the Bregman divergence. In Eq. (5.25), the
tangent is reparameterized by Î¸(Î¾), its gradient, instead of the contact point
h(Î¾), and its offset is given by âˆ’Ï†(Î¸(Î¾)). Figure 5.1 illustrates the relationship
among the convex function Ï†, its lower-bound, and the Bregman divergence.
We now describe the free energy bounds F(Î¾) and F(Î·) in terms of infor-
mation divergences. It follows from the deï¬nition (5.17) of the approximate
posterior distribution that
KL(qÎ¾(w; Î¾)||p(w|D)) =

qÎ¾(w; Î¾) log
Zp(w; Î¾)
Z(Î¾)p(w, D)dw
= log
Z
Z(Î¾) âˆ’

dÏ†(h(w), h(Î¾))

qÎ¾(w;Î¾) .
We have a similar decomposition for KL(p(w|D)||qÎ·(w; Î·)) as well. Finally,
we obtain the following expressions:2
F(Î¾) âˆ’FBayes =

dÏ†(h(w), h(Î¾))

qÎ¾ + KL(qÎ¾||p),
(5.26)
FBayes âˆ’F(Î·) =

dÏˆ(g(w), g(Î·))

p + KL(p||qÎ·).
(5.27)
Recall that FBayes + KL(qÎ¾||p) = F is the free energy, which is further bounded
by F(Î», Î¾) in Eq. (2.25). The expression (5.26) shows that the gap between
minÎ» F(Î», Î¾) and F is the expected Bregman divergence

dÏ†(h(w), h(Î¾))

qÎ¾.
Recall also that FBayes âˆ’KL(p||qÎ·) = E is the objective function of the EP
problem (2.46) and that F(Î·) = âˆ’log Z(Î·) is obtained as the maximum of its
lower-bound, maxÎ½ E(Î½, Î·) in Eq. (2.59), under a monotonic transformation.
2 Hereafter in this section, we omit the notation â€œ(w|D)â€ if no confusion is likely.

5.3 Uniï¬ed Approach by Local VB Bounds
141
The expression (5.27) shows that the gap between E and maxÎ½ E(Î½, Î·) is
expressed by the expected Bregman divergence

dÏˆ(g(w), g(Î·))

p while the
expectation is taken with respect to the true posterior.
Similarly, we also have the following decompositions:
F(Î¾) âˆ’FBayes =

dÏ†(h(w), h(Î¾))

p âˆ’KL(p||qÎ¾),
(5.28)
and
FBayes âˆ’F(Î·) =

dÏˆ(g(w), g(Î·))

qÎ· âˆ’KL(qÎ·||p).
Unlike Eqs. (5.26) and (5.27), the KL divergence is subtracted in these
expressions. This again implies the afï¬nities of LVAs by F minimization and
F maximization to VB and EP, respectively.
5.3.2 Optimization of Approximations
In this subsection, we show that the conditions for the optimal variational
parameters are generally given by the moment matching with respect to h(Î¾)
and g(Î·).
Optimal Variational Parameters
From Eqs. (5.22) and (5.23), we can see that the approximate posteriors,
qÎ¾(w; Î¾) âˆp(w, D) exp{h(w)âŠ¤âˆ‡Ï†(h(Î¾)) âˆ’Ï†(h(w))}
(5.29)
qÎ·(w; Î·) âˆp(w, D) exp{âˆ’g(w)âŠ¤âˆ‡Ïˆ(g(Î·)) + Ïˆ(g(w))},
are members of the exponential family with natural parameters âˆ‡Ï†(h(Î¾)) and
âˆ‡Ïˆ(g(Î·)) (Section 1.2.3). Let
Î¸(Î¾) = âˆ‡Ï†(h(Î¾)) and
Îº(Î·) = âˆ‡Ïˆ(g(Î·)).
The variational parameters are optimized so that F(Î¾) is minimized and
F(Î·) is maximized, respectively. In practice, however, they can be optimized
directly with respect to h(Î¾) and g(Î·) instead of Î¾ and Î·. Applications of
LVA, storing h(Î¾) and g(Î·) as parameters, do not require Î¾ and Î· explicitly.
Furthermore, we consider the gradient vectors of the free energy bounds with
respect to Î¸(Î¾) and Îº(Î·), which have one-to-one correspondence with h(Î¾) and
g(Î·), because they provide simple expressions of the gradient vectors. For
notational simplicity, we drop the dependencies on Î¾ and Î· and denote as Î¸
and Îº.

142
5 VB Algorithm under No Conjugacy
The gradient of the upper bound with respect to Î¸ is given by3
âˆ‡Î¸F(Î¾) = âˆ‡Î¸
)
âˆ’log

p(w; Î¾)dw
1
= âˆ’

1
Z(Î¾)
âˆ‚p(w; Î¾)
âˆ‚Î¸
dw
= âˆ’âˆ‚h(Î¾)
âˆ‚Î¸

1
Z(Î¾)
âˆ‚p(w; Î¾)
âˆ‚h(Î¾) dw
= âˆ’âˆ‚h(Î¾)
âˆ‚Î¸
 âˆ‚2Ï†(h(Î¾))
âˆ‚hâˆ‚hâŠ¤(h(w) âˆ’h(Î¾))qÎ¾(w; Î¾)dw
= âˆ’(âŸ¨h(w)âŸ©qÎ¾ âˆ’h(Î¾)),
(5.30)
where we have used Eq. (5.22) and the fact that the matrix âˆ‚h(Î¾)
âˆ‚Î¸ , whose (i, j)th
entry is âˆ‚hi(Î¾)
âˆ‚Î¸j , is the inverse of the Hessian matrix âˆ‚2Ï†(h(Î¾))
âˆ‚hâˆ‚hâŠ¤. Similarly, we obtain
âˆ‡ÎºF(Î·) = âŸ¨g(w)âŸ©qÎ· âˆ’g(Î·).
(5.31)
Hence, we can utilize gradient methods to minimize F(Î¾) and maximize F(Î·).
We can see that when Î¾ and Î· are optimized,
h(Î¾) = âŸ¨h(w)âŸ©qÎ¾
and
g(Î·) = âŸ¨g(w)âŸ©qÎ·
hold.
In practice, the variational parameter h(Î¾) is iteratively updated so that
F(Î¾) is monotonically decreased. Recall the argument in Section 2.1.7 where
LVA for VB was formulated as the joint minimization of F(Î», Î¾) over the
approximate posterior r(w;Î») and Î¾. The free energy bound F(Î¾) = âˆ’log Z(Î¾)
was obtained as the minimum of F(Î», Î¾), which is attained by (see Eq. (2.32))
r(w;Î») = q(w; Î¾).
Let h(Î¾o) be a current estimate of h(Î¾) andÎ»o be the variational parameter such
that r(w;Î»o) = q(w; Î¾o). Then, updating h(Î¾) to argminh(Î¾) F(Î»o, Î¾) decreases
F(Î¾) because
F(Î»o, Î¾) â‰¥F(Î¾)
for all Î¾ and the equality holds when Î¾ = Î¾o. More speciï¬cally, it follows for
h(Î¾) = argminh(Î¾) F(Î»o, Î¾) that
F(Î¾) â‰¤F(Î»o,Î¾) â‰¤F(Î»o, Î¾o) = F(Î¾o),
(5.32)
3 We henceforth use the operator âˆ‡with the subscript expressing for which variable the gradient
is taken. That is, for a function f(Î¸), âˆ‡Î¸ f(Î¸) = âˆ‚f(Î¸)
âˆ‚Î¸
denotes the vector whose ith element is
âˆ‚f(Î¸)
âˆ‚Î¸i .

5.3 Uniï¬ed Approach by Local VB Bounds
143
which means that the bound is improved. This corresponds to the EM algo-
rithm to decrease F(Î¾) and yields the following speciï¬c update rule of h(Î¾):
h(Î¾) = argmin
h(Î¾)
F(Î»o,Î¾)
= argmin
h(Î¾)

âˆ’log p(w; Î¾)

qÎ¾(w;Î¾o)
(5.33)
= argmin
h(Î¾)

dÏ†(h(w), h(Î¾))

qÎ¾(w;Î¾o)
(5.34)
= argmin
h(Î¾)
dÏ†(âŸ¨h(w)âŸ©qqÎ¾(w;Î¾o) , h(Î¾))
(5.35)
= âŸ¨h(w)âŸ©qÎ¾(w;Î¾o) ,
(5.36)
which is summarized as
h(Î¾) = âŸ¨h(w)âŸ©qÎ¾(w;Î¾o) .
(5.37)
The preceding lines of equations are basically derived by focusing on the
terms depending on h(Î¾). Eq. (5.33) follows from the deï¬nition of F(Î»o,Î¾)
by Eq. (2.25). Eq. (5.34) follows from the deï¬nition of p(w; Î¾) by Eq. (5.15).
Eq. (5.35) follows from the deï¬nition of the Bregman divergence (5.21) and
the linearity of expectation. Eq. (5.36) follows from the nonnegativity of the
Bregman divergence. Eqs. (5.34) through (5.36) are equivalent to the fact that
the expected Bregman divergence is minimized by the mean (Banerjee et al.,
2005).
The update rule (5.37) means that h(Î¾) is updated to the expectation of h(w)
with respect to the approximate posterior. Note here again that if we store h(Î¾),
Î¾ is not explicitly required.
The update rule (5.37) is an iterative substitution of h(Î¾). To maximize the
lower-bound F(Î·) in LVA for EP, such a simple update rule is not applicable
in general. Thus, gradient-based optimization methods with the gradient (5.31)
are usually used. The Newtonâ€“Raphson step to update Îº is given by
Îºnew = Îºold âˆ’

âˆ‡2
ÎºF(Î·old)
 âˆ’1 âˆ‡ÎºF(Î·old),
(5.38)
where the Hessian matrix is given as follows:
âˆ‡2
ÎºF(Î·) = âˆ‚2F(Î·)
âˆ‚Îºâˆ‚ÎºâŠ¤
= âˆ’Cov(g(w)) âˆ’âˆ‚g(Î·)
âˆ‚Îº
(5.39)
for the covariance matrix of g(w),
Cov(g(w)) =

g(w)g(w)âŠ¤
qÎ· âˆ’âŸ¨g(w)âŸ©qÎ· âŸ¨g(w)âŸ©âŠ¤
qÎ· ,
and âˆ‚g(Î·)
âˆ‚Îº
=
!
âˆ‚2Ïˆ(g(Î·))
âˆ‚gâˆ‚gâŠ¤
"âˆ’1
holds in Eq. (5.39).

144
5 VB Algorithm under No Conjugacy
5.3.3 An Alternative View of VB for Latent Variable Models
In this subsection, we show that the VB learning for latent variable models can
be viewed as a special case of LVA, where the log-sum-exp function is used to
form the lower-bound of the log-likelihood (Jordan et al., 1999).
Let H be a vector of latent (unobserved) variables and consider the latent
variable model,
p(D, w) =

H
p(D, H, w),
where 
H denotes the summation over all possible realizations of the latent
variables. We have used the notation as if H were discrete in order to include
examples such as GMMs and HMMs, where the likelihood function is given
by p(D|w) = 
H p(D, H|w). In the case of a model with continuous latent
variables, the summation 
H is simply replaced by the integration

dH. This
includes, for example, the hierarchical prior distribution presented in Tipping
(2001), where the prior distribution is deï¬ned by p(w) =

p(w|H)p(H)dH
with the hyperprior p(H).
The Bayesian posterior distribution of the latent variables and the parameter
w is
p(H, w|D) =
p(D, H, w)

H

p(D, H, w)dw
,
which is intractable when Z = 
H

p(D, H, w)dw requires summation
over exponentially many terms as in GMMs and HMMs or the analytically
intractable integration. So is the posterior of the parameter p(w|D).
Let us consider an application of the local variational method for approx-
imating p(w|D). By the convexity of the function log 
H exp(Â·), the log-joint
distribution is lower-bounded as follows:
log p(D, w) = log

H
exp{log p(D, H, w)}
â‰¥log p(D, Î¾) +

H

log p(D, H, w)
p(D, H, Î¾)

p(H|D, Î¾)
= log p(D, w) âˆ’

H
p(H|D, Î¾) log p(H|D, Î¾)
p(H|D, w),
(5.40)
where p(H|D, Î¾) =
p(D,H,Î¾)

H p(D,H,Î¾). This corresponds to the case where Ï†(h) =
log 
n exp(hn) and h(w) is the vector-valued function that consists of the ele-
ments log p(D, H, w) for all possible H. The vector h is inï¬nite dimensional
when H is continuous. Taking exponentials of the most right-hand side and

5.3 Uniï¬ed Approach by Local VB Bounds
145
left-hand side of Inequality (5.40) leads to Eqs. (5.22) and (5.15) with the
Bregman divergence,
dÏ†(h(w), h(Î¾)) =

H
p(H|D, Î¾) log p(H|D, Î¾)
p(H|D, w)
= KL(p(H|D, Î¾)||p(H|D, w)).
From Eq. (5.26), we have
F(Î¾) = FBayes + KL(qÎ¾(w; Î¾)||p(w|D)) + âŸ¨KL(p(H|D, Î¾)||p(H|D, w))âŸ©qÎ¾(w;Î¾)
= FBayes + KL(qÎ¾(w; Î¾)p(H|D, Î¾)||p(w, H|D)),
which is exactly the free energy of the factorized distribution qÎ¾(w; Î¾)p(H|D, Î¾).
In fact, from Eqs. (5.29) and (5.40), the approximating posterior is given by
qÎ¾(w; Î¾) âˆexp
â§âªâªâ¨âªâªâ©

H
log p(D, H, w)p(H|D, Î¾)
â«âªâªâ¬âªâªâ­
= exp log p(D, H, w)
p(H|D,Î¾) .
(5.41)
From Eq. (5.37), the EM update for the variational parameters Î¾ yields
log p(D, H, Î¾) = log p(D, H, w)
qÎ¾(w;Î¾o)
â‡’p(H|D, Î¾) âˆexp log p(D, H, w)
qÎ¾(w;Î¾o) .
(5.42)
Eqs. (5.41) and (5.42) are exactly the same as the VB algorithm for minimizing
the free energy over the factorized distributions, Eqs. (4.2) and (4.3). In this
example, we no longer have Î¾ satisfying Eq. (5.42) in general. However, if the
model p(H, D|w) and the prior p(w) are included in the exponential family,
h(Î¾) as well as p(H|D, Î¾) and qÎ¾(w; Î¾) are expressed by expected sufï¬cient
statistics, the number of which is equal to the dimensionality of w (Beal, 2003).
In that case, it is not necessary to obtain Î¾ explicitly but only to store and update
the expected sufï¬cient statistics instead.


Part III
Nonasymptotic Theory


6
Global VB Solution of Fully Observed
Matrix Factorization
Variational Bayesian (VB) learning has shown good performance in many
applications. However, VB learning sometimes gives a seemingly different
posterior and exhibits different sparsity behavior from full Bayesian learning.
For example, Figure 6.1 compares the Bayes posterior (left) and the VB
posterior (right) of 1Ã—1 matrix factorization. VB posterior tries to approximate
a two-mode Bayes posterior with a single-mode Gaussian, which results in the
zero-mean Gaussian posterior with the VB estimator BA = 0. This behavior
makes the VB estimator exactly sparse as shown in Figure 6.2: thresholding is
observed for the VB estimator, while no thresholding is observed for the full
Bayesian estimator. Mackay (2001) discussed the sparsity of VB learning as
an artifact by showing inappropriate model pruning in mixture models. These
facts might deprive the justiï¬cation of VB learning based solely on the fact
that it is a tractable approximation to Bayesian learning. Can we clarify the
behavior of VB learning, and directly justify its use as an inference method?
The nonasymptotic theory, introduced in Part III, gives some answer to this
question.
In this chapter, we derive an analytic-form of the global VB solution of
fully observed matrix factorization (MF). The analytic-form solution allows
us to make intuitive discussion on the behavior of VB learning (Chapter 7),
and further analysis gives theoretical guarantees of the performance of VB
learning (Chapter 8). The analytic-form global solution naturally leads to
efï¬cient and reliable algorithms (Chapter 9), which are extended to other
similar models (Chapters 10 and 11). Relation to MAP learning and partially
Bayesian learning is also theoretically investigated (Chapter 12).
149

150
6 Global VB Solution of Fully Observed Matrix Factorization
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.3
0.3
0.3
0.3
0.3
A
B
Bayes posterior (V = 1)
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
MAP estimators:
(A, B) â‰ˆ(Â± 1, Â±
0.05
0.05
0.05
0.05
0.05
0.05
0.05
0.1
0.1
0.1
0.1
0.15
0.15
A
B
VB posterior (V = 1)
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
VB estimator : (A, B) = (0, 0)
Figure 6.1
The Bayes posterior (left) and the VB posterior (right) of the 1 Ã— 1
MF model V = BA + E with almost ï¬‚at prior, when V = 1 is observed (E is
the standard Gaussian noise). VB approximates the Bayes posterior having two
modes by an origin-centered Gaussian, which induces sparsity.
0
1
2
3
0
0.5
1
1.5
2
2.5
3
Figure 6.2 Behavior of the estimators of 
U = BA as a function of the observed
value V. The VB estimator is zero when V â‰¤1, which indicates exact sparsity. On
the other hand, the Bayesian estimator shows no sign of sparsity. The maximum
likelihood estimator, i.e., 
U = V, is shown as a reference.
6.1 Problem Description
We ï¬rst summarize the MF model and its VB learning algorithm, which was
derived in Section 3.1. The likelihood and priors are given as
p(V|A, B) âˆexp

âˆ’1
2Ïƒ2
###V âˆ’BAâŠ¤###2
Fro

,
(6.1)
p(A) âˆexp

âˆ’1
2tr

ACâˆ’1
A AâŠ¤ 
,
(6.2)

6.1 Problem Description
151
p(B) âˆexp

âˆ’1
2tr

BCâˆ’1
B BâŠ¤ 
,
(6.3)
where the prior covariances are restricted to be diagonal:
CA = Diag(c2
a1,. . . , c2
aH),
CB = Diag(c2
b1,. . . , c2
bH),
for cah, cbh > 0, h = 1,. . . , H. Without loss of generality, we assume that the
diagonal entries of the product CACB are arranged in nonincreasing order, i.e.,
cahcbh â‰¥cahâ€² cbhâ€² for any pair h < hâ€². We assume that
L â‰¤M.
(6.4)
If L > M, we may simply redeï¬ne the transpose VâŠ¤as V so that L â‰¤M holds.
Therefore, the assumption (6.4) does not impose any restriction.
We solve the following VB learning problem:
r = argmin
r
F(r)
s.t.
r(A, B) = rA(A)rB(B),
(6.5)
where the objective function to be minimized is the free energy:
F =
/
log
rA(A)rB(B)
p(V|A, B)p(A)p(B)
0
rA(A)rB(B)
.
The solution to the problem (6.5) is in the following form:
rA(A) = MGaussM,H(A; A, IM âŠ—Î£A) âˆexp
â›âœâœâœâœâœâœâœâœâœâœâ
âˆ’
tr
!
(A âˆ’A)Î£
âˆ’1
A (A âˆ’A)âŠ¤
"
2
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
,
(6.6)
rB(B) = MGaussL,H(B; B, IL âŠ—Î£B) âˆexp
â›âœâœâœâœâœâœâœâœâœâœâ
âˆ’
tr
!
(B âˆ’B)Î£
âˆ’1
B (B âˆ’B)âŠ¤
"
2
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
.
(6.7)
With the variational parameters A, Î£A, B, Î£B, the free energy can be explicitly
written as
2F = LM log(2Ï€Ïƒ2) +
####V âˆ’BA
âŠ¤####
2
Fro
Ïƒ2
+ M log det (CA)
det
Î£A
 + L log det (CB)
det
Î£B
 
âˆ’(L + M)H + tr
2
Câˆ’1
A
!
A
âŠ¤A + MÎ£A
"3
+ tr
2
Câˆ’1
B
!
B
âŠ¤B + LÎ£B
"3
+ Ïƒâˆ’2tr
2
âˆ’A
âŠ¤AB
âŠ¤B +
!
A
âŠ¤A + MÎ£A
" !
B
âŠ¤B + LÎ£B
"3
.
(6.8)

152
6 Global VB Solution of Fully Observed Matrix Factorization
The stationary conditions for the variational parameters are given by
A = Ïƒâˆ’2VâŠ¤BÎ£A,
(6.9)
Î£A = Ïƒ2 !
B
âŠ¤B + LÎ£B + Ïƒ2Câˆ’1
A
"âˆ’1
,
(6.10)
B = Ïƒâˆ’2VAÎ£B,
(6.11)
Î£B = Ïƒ2 !
A
âŠ¤A + MÎ£A + Ïƒ2Câˆ’1
B
"âˆ’1
.
(6.12)
In the subsequent sections, we derive the global solution to the problem
(6.5) in an analytic form, which was obtained in Nakajima et al. (2013a, 2015).
6.2 Conditions for VB Solutions
With the explicit form (6.8) of the free energy, the VB learning problem (6.5)
can be written as a minimization problem with respect to a ï¬nite number of
variables:
Given
CA, CA âˆˆDH
++,
Ïƒ2 âˆˆR++,
min
A,B,Î£A,Î£B
F
(6.13)
s.t.
A âˆˆRMÃ—H, B âˆˆRLÃ—H,
Î£A, Î£B âˆˆSH
++.
(6.14)
We can easily show that the solution is a stationary point of the free energy.
Lemma 6.1
Any local solution of the problem (6.13) is a stationary point of
the free energy (6.8).
Proof
Since
####V âˆ’BA
âŠ¤####
2
Fro â‰¥0,
and
tr
2
âˆ’A
âŠ¤AB
âŠ¤B +
!
A
âŠ¤A + MÎ£A
" !
B
âŠ¤B + LÎ£B
"3
= tr
2
LA
âŠ¤AÎ£B + MB
âŠ¤BÎ£A + LMÎ£AÎ£B
3
â‰¥0,
the free energy (6.8) is lower-bounded as
2F â‰¥âˆ’M log det
Î£A
 
âˆ’L log det
Î£B
 
+ tr
2
Câˆ’1
A
!
A
âŠ¤A + MÎ£A
"3
+ tr
2
Câˆ’1
B
!
B
âŠ¤B + LÎ£B
"3
+ Ï„,
(6.15)

6.3 Irrelevant Degrees of Freedom
153
where Ï„ is a ï¬nite constant. The right-hand side of Eq. (6.15) diverges to +âˆ
if any entry of A or B goes to +âˆor âˆ’âˆ. Also it diverges if any eigenvalue
of Î£A or Î£B goes to +0 or âˆ. This implies that that no local solution exists
on the boundary of (the closure of) the domain (6.14). Since the free energy is
differentiable in the domain (6.14), any local minimizer is a stationary point.
For any observed matrix V, the free energy (6.8) can be ï¬nite, for example,
at A = 0M,H, B = 0L,H, and Î£A = Î£B = IH, where 0D1,D2 denotes the D1 Ã— D2
matrix with all the entries equal to zero. Therefore, at least one minimizer
always exists, which completes the proof of Lemma 6.1.
â–¡
Lemma 6.1 implies that the stationary conditions (6.9) through (6.12) are
satisï¬ed for any solution. Accordingly, we can obtain the global solution by
ï¬nding all points that satisfy the stationary conditions. However, the condition
involves O(MH) unknowns, and therefore ï¬nding all such candidate points
seems hard. The ï¬rst step to tackle this problem is to ï¬nd hidden separability,
which enables us to decompose the problem so that each problem involves only
O(1) unknowns.
6.3 Irrelevant Degrees of Freedom
The most of the terms in the free energy (6.8) have symmetry, i.e., they are
invariant with respect to the coordinate change shown in Eqs. (6.16) and (6.17).
Assume that (Aâˆ—, Bâˆ—, Î£âˆ—
A, Î£âˆ—
B) is a global solution of the VB problem (6.13),
and let Fâˆ—= F(Aâˆ—, Bâˆ—, Î£âˆ—
A, Î£âˆ—
B) be the minimum free energy. Consider the
following rotation of the coordinate system for an arbitrary orthogonal matrix
Î© âˆˆRHÃ—H:
A = Aâˆ—Î©âŠ¤,
Î£A = Î©Î£âˆ—
AÎ©âŠ¤,
(6.16)
B = Bâˆ—Î©âŠ¤,
Î£B = Î©Î£âˆ—
BÎ©âŠ¤.
(6.17)
We can easily conï¬rm that the terms in Eq. (6.8) except the sixth and the
seventh terms are invariant with respect to the rotation, and the free energy
can be written as a function of Î© as follows:
2F(Î©) = tr
2
Câˆ’1
A Î©
!
A
âŠ¤A + MÎ£A
"
Î©âŠ¤3
+ tr
2
Câˆ’1
B Î©
!
B
âŠ¤B + LÎ£B
"
Î©âŠ¤3
+ const.
To ï¬nd the irrelevant degrees of freedom, we consider skewed rotations that
only affect a single term in Eq. (6.8).
Consider the following transform:
A = Aâˆ—Câˆ’1/2
A
Î©âŠ¤C1/2
A ,
Î£ A = C1/2
A Î©Câˆ’1/2
A
Î£âˆ—
ACâˆ’1/2
A
Î©âŠ¤C1/2
A ,
(6.18)

154
6 Global VB Solution of Fully Observed Matrix Factorization
B = Bâˆ—C1/2
A Î©âŠ¤Câˆ’1/2
A
,
Î£B = Câˆ’1/2
A
Î©C1/2
A Î£âˆ—
BC1/2
A Î©âŠ¤Câˆ’1/2
A
.
(6.19)
Then, the free energy can be written as
2F(Î©) = tr
,
Î“Î©Î¦Î©âŠ¤-
+ const.,
(6.20)
where
Î“ = Câˆ’1
A Câˆ’1
B ,
Î¦ = C1/2
A

Bâˆ—âŠ¤Bâˆ—+ LÎ£âˆ—
B
 
C1/2
A .
By assumption, Î© = IH is a minimizer of Eq. (6.20), i.e., F(IH) = Fâˆ—. Now
we can use the following lemma:
Lemma 6.2
Let Î“, Î©, Î¦ âˆˆRHÃ—H be a nondegenerate diagonal matrix, an
orthogonal matrix, and a symmetric matrix, respectively. Let {Î›(k), Î›â€²(k) âˆˆ
RHÃ—H; k = 1,. . . , K} be arbitrary diagonal matrices. If a function
G(Î©) = tr
â§âªâªâ¨âªâªâ©Î“Î©Î¦Î©âŠ¤+
K

k=1
Î›(k)Î©Î›â€²(k)Î©âŠ¤
â«âªâªâ¬âªâªâ­
(6.21)
is minimized (as a function of Î©, given Î“, Î¦, {Î›(k), Î›â€²(k)}) at Î© = IH, then Î¦ is
diagonal. Here, K can be any natural number including K = 0 (when only the
ï¬rst term exists).
Proof
Let
Î¦ = Î©â€²Î“â€²Î©â€²âŠ¤
(6.22)
be the eigenvalue decomposition of Î¦. Let Î³, Î³â€², {Î»(k)}, {Î»â€²(k)} be the vectors
consist of the diagonal entries of Î“, Î“â€², {Î›(k)}, {Î›â€²(k)}, respectively, i.e.,
Î“ = Diag(Î³),
Î“â€² = Diag(Î³â€²),
Î›(k) = Diag(Î»(k)),
Î›â€²(k) = Diag(Î»â€²(k)).
Then, Eq. (6.21) can be written as
G(Î©) = tr
â§âªâªâ¨âªâªâ©Î“Î©Î¦Î©âŠ¤+
K

k=1
Î›(k)Î©Î›â€²(k)Î©âŠ¤
â«âªâªâ¬âªâªâ­= Î³âŠ¤QÎ³â€² +
K

k=1)
Î»(k)âŠ¤RÎ»â€²(k), (6.23)
where
Q = (Î©Î©â€²) âŠ™(Î©Î©â€²),
R = Î© âŠ™Î©.
Here, âŠ™denotes the Hadamard product. Since Q as well as R is the Hadamard
square of an orthogonal matrix, it is doubly stochastic (i.e., any of the columns
and the rows sums up to one) (Marshall et al., 2009). Therefore, it can be
seen that Q reassigns the components of Î³ to those of Î³â€² when calculating

6.3 Irrelevant Degrees of Freedom
155
the elementwise product in the ï¬rst term of Eq. (6.23). The same applies to
R and {Î»(k), Î»â€²(k)} in the second term. Naturally, rearranging the components
of Î³ in nondecreasing order and the components of Î³â€² in nonincreasing order
minimizes Î³âŠ¤QÎ³â€² (Ruhe, 1970; Marshall et al., 2009).
Using the expression (6.23) with Q and R, we will prove that Î¦ is diagonal
if Î© = IH minimizes Eq. (6.23). Let us consider a bilateral perturbation Î© = Î”
such that the 2 Ã— 2 matrix Î”(h,hâ€²) consisting of the hth and the hâ€²th columns and
rows form an 2 Ã— 2 orthogonal matrix
Î”(h,hâ€²) =
cos Î¸
âˆ’sin Î¸
sin Î¸
cos Î¸

,
and the remaining entries coincide with those of the identity matrix. Then, the
elements of Q become
Qi, j =
â§âªâªâªâªâªâ¨âªâªâªâªâªâ©
(Î©â€²
h, j cos Î¸ âˆ’Î©â€²
hâ€², j sin Î¸)2
if i = h,
(Î©â€²
h, j sin Î¸ + Î©â€²
hâ€², j cos Î¸)2
if i = hâ€²,
Î©â€²2
i, j
otherwise,
and Eq. (6.23) can be written as a function of Î¸:
G(Î¸) =
H

j=1
,
Î³h(Î©â€²
h, j cos Î¸ âˆ’Î©â€²
hâ€², j sin Î¸)2 + Î³hâ€²(Î©â€²
h, j sin Î¸ + Î©â€²
hâ€², j cos Î¸)2-
Î³â€²
j
+
K

k=1

Î»(k)
h
Î»(k)
hâ€²
 cos2 Î¸
sin2 Î¸
sin2 Î¸
cos2 Î¸
 Î»(k)
h
Î»(k)
hâ€²

+ const.
(6.24)
Since Eq. (6.24) is differentiable at Î¸ = 0, our assumption that Eq. (6.23) is
minimized when Î© = IH requires that Î¸ = 0 is a stationary point of Eq. (6.24)
for any h  hâ€². Therefore, it holds that
0 = âˆ‚G
âˆ‚Î¸
Î¸=0
= 2

j
,
Î³h(Î©â€²
h, j cos Î¸ âˆ’Î©â€²
hâ€², j sin Î¸)(âˆ’Î©â€²
h, j sin Î¸ âˆ’Î©â€²
hâ€², j cos Î¸)
+ Î³hâ€²(Î©â€²
h, j sin Î¸ + Î©â€²
hâ€², j cos Î¸)(Î©â€²
h, j cos Î¸ âˆ’Î©â€²
hâ€², j sin Î¸)
-
Î³â€²
j
= 2 (Î³hâ€² âˆ’Î³h)

j
Î©â€²
h, jÎ³â€²
jÎ©â€²
hâ€², j = 2 (Î³hâ€² âˆ’Î³h) Î¦h,hâ€².
(6.25)
In the last equality, we used Eq. (6.22). Since we assumed that Î“ is nonde-
generate (Î³h  Î³hâ€² for h  hâ€²), Eq. (6.25) implies that Î¦ is diagonal, which
completes the proof of Lemma 6.2.
â–¡
Assume for simplicity that Î“ = Câˆ’1
A Câˆ’1
B is nondegenerate, i.e., no pair of
diagonal entries coincide, in Eq. (6.20). Then, since Eq. (6.20) is minimized

156
6 Global VB Solution of Fully Observed Matrix Factorization
at Î© = IH, Lemma 6.2 implies that Î¦ = C1/2
A

Bâˆ—âŠ¤Bâˆ—+ LÎ£âˆ—
B
 
C1/2
A
is diagonal.
This means that Bâˆ—âŠ¤Bâˆ—+LÎ£âˆ—
B is diagonal. Thus, the stationary condition (6.10)
implies that Î£âˆ—
A is diagonal. Similarly, we can ï¬nd that Î£âˆ—
B is diaognal, if Î“ =
Câˆ’1
A Câˆ’1
B is nondegenerate.
To generalize the preceding discussion to degenerate cases, we need to
consider an equivalent solution, deï¬ned as follows:
Deï¬nition 6.3
(Equivalent solutions) We say that two points (A, B, Î£A, Î£B)
and (A
â€², B
â€², Î£
â€²
A, Î£
â€²
B) are equivalent if both give the same free energy and the
same mean prediction, i.e.,
F(A, B, Î£A, Î£B) = F(A
â€², B
â€², Î£
â€²
A, Î£
â€²
B)
and
BA
âŠ¤= B
â€²A
â€²âŠ¤.
With this deï¬nition, we can obtain the following theorem (its proof is given
in the next section):
Theorem 6.4
When CACB is nondegenerate (i.e., cahcbh > cahâ€² cbhâ€² for any
pair h < hâ€²), any solution of the problem (6.13) has diagonal Î£A and Î£B. When
CACB is degenerate, any solution has an equivalent solution with diagonal Î£A
and Î£B.
The result that the solution has diagonal Î£A and Î£B would be natural
because we assumed the independent Gaussian priors on A and B: the fact that
any V can be decomposed into orthogonal singular components may imply that
the observation V cannot convey any preference for singular-componentwise
correlation. Note, however, that Theorem 6.4 does not necessarily hold when
the observed matrix has missing entries.
Obviously, any VB solution (a solution of the problem (6.13)) with diagonal
covariances can be found by solving the following problem:
Given
CA, CA âˆˆDH
++,
Ïƒ2 âˆˆR++,
min
A,B,Î£A,Î£B
F
(6.26)
s.t.
A âˆˆRMÃ—H, B âˆˆRLÃ—H,
Î£A, Î£B âˆˆDH
++,
(6.27)
which is equivalent to solving the SimpleVB learning problem (3.30) with
columnwise independence, introduced in Section 3.1.1. Theorem 6.4 states
that, if CACB is nondegenerate, the set of VB solutions and the set of SimpleVB
solutions are identical. When CACB is degenerate, the set of VB solutions is
the union of the set of SimpleVB solutions and the set of their equivalent
solutions with nondiagonal covariances. Actually, any VB solution can be
obtained by rotating its equivalent SimpleVB solution (VB solution with
diagonal covariances) (see Section 6.4.4). In practice, it is however sufï¬cient

6.4 Proof of Theorem 6.4
157
to focus on the SimpleVB solutions, since equivalent solutions share the same
free energy F and the same mean prediction BA
âŠ¤. In this sense, we can
conclude that the stronger columnwise independence constraint (3.29) does
not degrade approximation accuracy, and the VB solution under the matrixwise
independence (3.4) essentially agrees with the SimpleVB solution.
6.4 Proof of Theorem 6.4
In this section, we prove Theorem 6.4 by considering the following three cases
separately:
Case 1 When no pair of diagonal entries of CACB coincide.
Case 2 When all diagonal entries of CACB coincide.
Case 3 When (not all but) some pairs of diagonal entries of CACB coincide.
We will prove that, in Case 1, Î£A and Î£B are diagonal for any solution
(A, B, Î£A, Î£B), and that, in other cases, any solution has its equivalent solution
with diagonal Î£A and Î£B.
Remember our assumption that the diagonal entries {cahcbh} of CACB are
arranged in nonincreasing order.
6.4.1 Proof for Case 1
Here, we consider the case where cahcbh > cahâ€²cbhâ€² for any pair h < hâ€².
Assume that (Aâˆ—, Bâˆ—, Î£âˆ—
A, Î£âˆ—
B) is a minimizer of the free energy (6.8), and
consider the following variation deï¬ned with an arbitrary H Ã— H orthogonal
matrix Î©:
A = Aâˆ—Câˆ’1/2
A
Î©âŠ¤C1/2
A ,
(6.28)
B = Bâˆ—C1/2
A Î©âŠ¤Câˆ’1/2
A
,
(6.29)
Î£A = C1/2
A Î©Câˆ’1/2
A
Î£âˆ—
ACâˆ’1/2
A
Î©âŠ¤C1/2
A ,
(6.30)
Î£B = Câˆ’1/2
A
Î©C1/2
A Î£âˆ—
BC1/2
A Î©âŠ¤Câˆ’1/2
A
.
(6.31)
Note
that
this
variation
does
not
change
BA
âŠ¤,
and
it
holds
that
(A, B, Î£A, Î£B) = (Aâˆ—, Bâˆ—, Î£âˆ—
A, Î£âˆ—
B) for Î© = IH. Then, the free energy (6.8)

158
6 Global VB Solution of Fully Observed Matrix Factorization
can be written as a function of Î©:
F(Î©) = 1
2tr
,
Câˆ’1
A Câˆ’1
B Î©C1/2
A

Bâˆ—âŠ¤Bâˆ—+ LÎ£âˆ—
B
 
C1/2
A Î©âŠ¤-
+ const.
(6.32)
We deï¬ne
Î¦ = C1/2
A

Bâˆ—âŠ¤Bâˆ—+ LÎ£âˆ—
B
 
C1/2
A ,
and rewrite Eq. (6.32) as
F(Î©) = 1
2tr
,
Câˆ’1
A Câˆ’1
B Î©Î¦Î©âŠ¤-
+ const.
(6.33)
The assumption that (Aâˆ—, Bâˆ—, Î£âˆ—
A, Î£âˆ—
B) is a minimizer requires that Eq. (6.33)
is minimized when Î© = IH. Then, Lemma 6.2 (for K = 0) implies that Î¦ is
diagonal. Therefore,
Câˆ’1/2
A
Î¦Câˆ’1/2
A
(= Î¦Câˆ’1
A ) = Bâˆ—âŠ¤Bâˆ—+ LÎ£âˆ—
B
is also diagonal. Consequently, Eq. (6.10) implies that Î£âˆ—
A is diagonal.
Next, consider the following variation deï¬ned with an arbitrary H Ã— H
orthogonal matrix Î©â€²:
A = Aâˆ—C1/2
B Î©â€²âŠ¤Câˆ’1/2
B
,
B = Bâˆ—Câˆ’1/2
B
Î©â€²âŠ¤C1/2
B ,
Î£A = Câˆ’1/2
B
Î©â€²C1/2
B Î£âˆ—
AC1/2
B Î©â€²âŠ¤Câˆ’1/2
B
,
Î£B = C1/2
B Î©â€²Câˆ’1/2
B
Î£âˆ—
BCâˆ’1/2
B
Î©â€²âŠ¤C1/2
B .
Then, the free energy as a function of Î©â€² is given by
F(Î©â€²) = 1
2tr
,
Câˆ’1
A Câˆ’1
B Î©â€²C1/2
B

Aâˆ—âŠ¤Aâˆ—+ MÎ£âˆ—
A
 
C1/2
B Î©â€²âŠ¤-
+ const.
From this, we can similarly prove that Î£âˆ—
B is also diagonal, which completes
the proof for Case 1.
â–¡
6.4.2 Proof for Case 2
Here, we consider the case where CACB = c2IH for some c2 âˆˆR++. In this case,
there exist solutions with nondiagonal covariances. However, for any (or each)
of those nondiagonal solutions, the equivalent class to which the (nondiagonal)
solution belongs contains a solution with diagonal covariances.
We can easily show that the free energy (6.8) is invariant with respect to
Î© under the transformation (6.28) through (6.31). This arbitrariness forms an
equivalent class of solutions. Since there exists Î© that diagonalizes any given
Î£âˆ—
A through Eq. (6.30), each equivalent class involves a solution with diagonal

6.4 Proof of Theorem 6.4
159
Î£A. In the following, we will prove that any solution with diagonal Î£A has
diagonal Î£B.
Assume that (Aâˆ—, Bâˆ—, Î£âˆ—
A, Î£âˆ—
B) is a solution with diagonal Î£âˆ—
A, and consider
the following variation deï¬ned with an arbitrary H Ã— H orthogonal matrix Î©:
A = Aâˆ—Câˆ’1/2
A
Î“âˆ’1/2Î©âŠ¤Î“1/2C1/2
A ,
B = Bâˆ—C1/2
A Î“1/2Î©âŠ¤Î“âˆ’1/2Câˆ’1/2
A
,
Î£A = C1/2
A Î“1/2Î©Î“âˆ’1/2Câˆ’1/2
A
Î£âˆ—
ACâˆ’1/2
A
Î“âˆ’1/2Î©âŠ¤Î“1/2C1/2
A ,
Î£B = Câˆ’1/2
A
Î“âˆ’1/2Î©Î“1/2C1/2
A Î£âˆ—
BC1/2
A Î“1/2Î©âŠ¤Î“âˆ’1/2Câˆ’1/2
A
.
Here, Î“ = Diag(Î³1,. . . , Î³H) is an arbitrary nondegenerate (Î³h  Î³hâ€² for h  hâ€²)
positive-deï¬nite diagonal matrix. Then, the free energy can be written as a
function of Î©:
F(Î©) = 1
2tr
,
Î“Î©Î“âˆ’1/2Câˆ’1/2
A

Aâˆ—âŠ¤Aâˆ—+ MÎ£âˆ—
A
 
Câˆ’1/2
A
Î“âˆ’1/2Î©âŠ¤
+ câˆ’2Î“âˆ’1Î©Î“1/2C1/2
A

Bâˆ—âŠ¤Bâˆ—+ LÎ£âˆ—
B
 
C1/2
A Î“1/2Î©âŠ¤-
.
(6.34)
We deï¬ne
Î¦A = Î“âˆ’1/2Câˆ’1/2
A

Aâˆ—âŠ¤Aâˆ—+ MÎ£âˆ—
A
 
Câˆ’1/2
A
Î“âˆ’1/2,
Î¦B = câˆ’2Î“1/2C1/2
A

Bâˆ—âŠ¤Bâˆ—+ LÎ£âˆ—
B
 
C1/2
A Î“1/2,
and rewrite Eq. (6.34) as
F(Î©) = 1
2tr
,
Î“Î©Î¦AÎ©âŠ¤+ Î“âˆ’1Î©Î¦BÎ©âŠ¤-
.
(6.35)
Since Î£âˆ—
A is diagonal, Eq. (6.10) implies that Î¦B is diagonal. The assump-
tion that (Aâˆ—, Bâˆ—, Î£âˆ—
A, Î£âˆ—
B) is a solution requires that Eq. (6.35) is minimized
when Î©
=
IH. Accordingly, Lemma 6.2 implies that Î¦A is diagonal.
Consequently, Eq. (6.12) implies that Î£âˆ—
B is diagonal.
Thus, we have proved that any solution has its equivalent solution with
diagonal covariances, which completes the proof for Case 2.
â–¡
6.4.3 Proof for Case 3
Finally, we consider the case where cahcbh = cahâ€² cbhâ€² for (not all but) some
pairs h  hâ€². First, in the same way as Case 1, we can prove that Î£A and Î£B
are block diagonal where the blocks correspond to the groups sharing the same
cahcbh. Next, we can apply the proof for Case 2 to each block, and show that
any solution has its equivalent solution with diagonal Î£A and Î£B. Combining
these results completes the proof of Theorem 6.4.
â–¡

160
6 Global VB Solution of Fully Observed Matrix Factorization
6.4.4 General Expression
In summary, for any minimizer of Eq. (6.8), the covariances can be written in
the following form:
Î£A = C1/2
A Î˜Câˆ’1/2
A
Î“Î£ACâˆ’1/2
A
Î˜âŠ¤C1/2
A (= Câˆ’1/2
B
Î˜C1/2
B Î“Î£AC1/2
B Î˜âŠ¤Câˆ’1/2
B
),
(6.36)
Î£B = Câˆ’1/2
A
Î˜C1/2
A Î“Î£BC1/2
A Î˜âŠ¤Câˆ’1/2
A
(= C1/2
B Î˜Câˆ’1/2
B
Î“Î£BCâˆ’1/2
B
Î˜âŠ¤C1/2
B ).
(6.37)
Here, Î“Î£A and Î“Î£B are positive-deï¬nite diagonal matrices, and Î˜ is a block
diagonal matrix such that the blocks correspond to the groups sharing the same
cahcbh, and each block consists of an orthogonal matrix. Furthermore, if there
exists a solution with (Î£A, Î£B) written in the form of Eqs. (6.36) and (6.37)
with a certain set of (Î“Î£A, Î“Î£B, Î˜), then there also exist its equivalent solutions
with the same (Î“Î£A, Î“Î£B) for any Î˜. Focusing on the solution with Î˜ = IH as
the representative of each equivalent class, we can assume that Î£A and Î£B are
diagonal without loss of generality.
6.5 Problem Decomposition
As discussed in Section 6.3, Theorem 6.4 allows us to focus on the solutions
that have diagonal posterior covariances, i.e., Î£A, Î£B âˆˆDH
++. For any solution
with diagonal covariances, the stationary conditions (6.10) and (6.12) (with
Lemma 6.1) imply that A
âŠ¤A and B
âŠ¤B are also diagonal, which means that the
column vectors of A, as well as B, are orthogonal to each other. In such a case,
the free energy (6.8) depends on the column vectors of A and B only through
the second term
Ïƒâˆ’2 ####V âˆ’BA
âŠ¤####
2
Fro ,
which coincides with the objective function for the singular value decomposi-
tion (SVD). This leads to the following lemma:
Lemma 6.5
Let
V =
L

h=1
Î³hÏ‰bhÏ‰âŠ¤
ah
(6.38)
be the SVD of V, where Î³h (â‰¥0) is the hth largest singular value, and Ï‰ah and
Ï‰bh are the associated right and left singular vectors. Then, any VB solution

6.5 Problem Decomposition
161
(with diagonal posterior covariances) can be written as
BA
âŠ¤=
H

h=1
Î³VB
h Ï‰bhÏ‰âŠ¤
ah
(6.39)
for some {Î³VB
h
â‰¥0}.
Thanks to Theorem 6.4 and Lemma 6.5, the variational parameters
A = (a1,. . . ,aH), B = (b1,. . . ,bH), Î£A, Î£B can be expressed as
ah = ahÏ‰ah,
bh = bhÏ‰bh,
Î£A = Diag

Ïƒ2
a1,. . . , Ïƒ2
aH
 
,
Î£B = Diag

Ïƒ2
b1,. . . , Ïƒ2
bH
 
,
with a new set of unknowns {ah,bh âˆˆR, Ïƒ2
ah, Ïƒ2
bh âˆˆR++}H
h=1. Thus, the
following holds:
Corollary 6.6
The VB posterior can be written as
r(A, B) =
H

h=1
GaussM(ah;ahÏ‰ah, Ïƒ2
ahIM)
H

h=1
GaussL(bh;bhÏ‰bh, Ïƒ2
bh IL),
(6.40)
where {ah,bh, Ïƒ2
ah, Ïƒ2
bh}H
h=1 are the solution of the following minimization
problem:
Given
Ïƒ2 âˆˆR++,
{c2
ah, c2
bh âˆˆR++}H
h=1,
min
{ah,bh,Ïƒ2ah,Ïƒ2
bh}H
h=1
F
(6.41)
s.t.
{ah,bh âˆˆR,
Ïƒ2
ah, Ïƒ2
bh âˆˆR++}H
h=1.
Here, F is the free energy (6.8), which can be written as
2F = LM log(2Ï€Ïƒ2) +
L
h=1 Î³2
h
Ïƒ2
+
H

h=1
2Fh,
(6.42)
where
2Fh = M log
c2
ah
Ïƒ2ah
+ L log
c2
bh
Ïƒ2
bh
+
a2
h + MÏƒ2
ah
c2ah
+
b2
h + LÏƒ2
bh
c2
bh
âˆ’(L + M) +
âˆ’2ahbhÎ³h +

a2
h + MÏƒ2
ah
 b2
h + LÏƒ2
bh
 
Ïƒ2
.
(6.43)

162
6 Global VB Solution of Fully Observed Matrix Factorization
Importantly, the free energy (6.42) depends on the variational parameters
{ah,bh, Ïƒ2
ah, Ïƒ2
bh}H
h=1 only through the third term, and the third term is decom-
posed into H terms, each of which only depends on the variational parameters
(ah,bh, Ïƒ2
ah, Ïƒ2
bh) for the hth singular component. Accordingly, given the noise
variance Ïƒ2, we can separately minimize the free energy (6.43), which involves
only four unknowns, for each singular component.
6.6 Analytic Form of Global VB Solution
The stationary conditions of Eq. (6.43) are given by
ah =
Ïƒ2
ah
Ïƒ2 Î³hbh,
(6.44)
Ïƒ2
ah = Ïƒ2

b2
h + LÏƒ2
bh + Ïƒ2
c2ah
âˆ’1
,
(6.45)
bh =
Ïƒ2
bh
Ïƒ2 Î³hah,
(6.46)
Ïƒ2
bh = Ïƒ2
â›âœâœâœâœâœâa2
h + MÏƒ2
ah + Ïƒ2
c2
bh
ââŸâŸâŸâŸâŸâ 
âˆ’1
,
(6.47)
which form is a polynomial system, a set of polynomial equations, on the four
unknowns (ah,bh, Ïƒ2
ah, Ïƒ2
bh). Since Lemma 6.1 guarantees that any minimizer
is a stationary point, we can obtain the global solution by ï¬nding all points that
satisfy the stationary conditions (6.44) through (6.47) and comparing the free
energy (6.43) at those points.
This leads to the following theorem and corollary:
Theorem 6.7
The VB solution is given by
U
VB =
H

h=1
Î³VB
h Ï‰bhÏ‰âŠ¤
ah,
where
Î³VB
h
=
â§âªâªâ¨âªâªâ©
Ë˜Î³VB
h
if Î³h â‰¥Î³VB
h ,
0
otherwise,
(6.48)
for
Î³VB
h
= Ïƒ
>
?
?
?
@
(L + M)
2
+
Ïƒ2
2c2ahc2
bh
+
>
?
@â›âœâœâœâœâœâ
(L + M)
2
+
Ïƒ2
2c2ahc2
bh
ââŸâŸâŸâŸâŸâ 
2
âˆ’LM,
(6.49)
Ë˜Î³VB
h
= Î³h
â›âœâœâœâœâœâœâœâ1 âˆ’Ïƒ2
2Î³2
h
â›âœâœâœâœâœâœâœâM + L +
>
@
(M âˆ’L)2 +
4Î³2
h
c2ahc2
bh
ââŸâŸâŸâŸâŸâŸâŸâ 
ââŸâŸâŸâŸâŸâŸâŸâ .
(6.50)

6.7 Proofs of Theorem 6.7 and Corollary 6.8
163
Corollary 6.8
The VB posterior is given by Eq. (6.40) with the following
variational parameters: if Î³h > Î³VB
h ,
ah = Â±
.
Ë˜Î³VB
h Î´VB
h ,
bh = Â±
>
@
Ë˜Î³VB
h
Î´VB
h
,
Ïƒ2
ah = Ïƒ2Î´VB
h
Î³h
,
Ïƒ2
bh =
Ïƒ2
Î³hÎ´VB
h
, (6.51)
where
Î´VB
h

â‰¡ah
bh

= cah
Ïƒ2

Î³h âˆ’Ë˜Î³VB
h
âˆ’LÏƒ2
Î³h

,
(6.52)
and otherwise,
ah = 0,
bh = 0,
Ïƒ2
ah = c2
ah
â›âœâœâœâœâœâ1 âˆ’LÎ¶VB
h
Ïƒ2
ââŸâŸâŸâŸâŸâ ,
Ïƒ2
bh = c2
bh
â›âœâœâœâœâœâ1 âˆ’MÎ¶VB
h
Ïƒ2
ââŸâŸâŸâŸâŸâ , (6.53)
where
Î¶VB
h

â‰¡Ïƒ2
ahÏƒ2
bh
 
=
Ïƒ2
2LM
â›âœâœâœâœâœâœâœâœâœâL + M +
Ïƒ2
c2ahc2
bh
âˆ’
>
?
@â›âœâœâœâœâœâL + M +
Ïƒ2
c2ahc2
bh
ââŸâŸâŸâŸâŸâ 
2
âˆ’4LM
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâ .
(6.54)
Theorem 6.7 states that the VB solution for fully observed MF is a truncated
shrinkage SVD with the truncation threshold and the shrinkage estimator given
by Eqs. (6.49) and (6.50), respectively. Corollary 6.8 completely speciï¬es the
VB posterior.1
These results give insights into the behavior of VB learning; for example,
they explain why a sparse solution is obtained, and what are similarities and
differences between the Bayes posterior and the VB posterior, which will be
discussed in Chapter 7. The results also form the basis of further analysis on the
global empirical VB solution (Section 6.8), which will be used for performance
guarantee (Chapter 8), and global (or efï¬cient local) solvers for multilinear
models (Chapters 9, 10, and 11). Before moving on, we give the proofs of the
theorem and the corollary in the next section.
6.7 Proofs of Theorem 6.7 and Corollary 6.8
We will ï¬nd all stationary points that satisfy Eqs. (6.44) through (6.47), and
compare the free energy (6.43).
1 The similarity between (Î³VB
h )2 and LMÎ¶VB
h
comes from the fact that they are the two different
solutions of the same quadratic equations, i.e., Eq. (6.79) with respect to (Î³VB
h )2 and (6.77) with
respect to LMÎ¶VB
h .

164
6 Global VB Solution of Fully Observed Matrix Factorization
By using Eqs. (6.45) and (6.47), the free energy (6.43) can be simpliï¬ed as
Fh = M log
c2
ah
Ïƒ2ah
+ L log
c2
bh
Ïƒ2
bh
+ 1
Ïƒ2
â›âœâœâœâœâœâa2
h + MÏƒ2
ah + Ïƒ2
c2
bh
ââŸâŸâŸâŸâŸâ 

b2
h + LÏƒ2
bh + Ïƒ2
c2ah

âˆ’(L + M) + âˆ’2ahbhÎ³h
Ïƒ2
âˆ’
Ïƒ2
c2ahc2
bh
= M log
c2
ah
Ïƒ2ah
+ L log
c2
bh
Ïƒ2
bh
+
Ïƒ2
Ïƒ2ahÏƒ2
bh
âˆ’2ahbhÎ³h
Ïƒ2
âˆ’
â›âœâœâœâœâœâL + M +
Ïƒ2
c2ahc2
bh
ââŸâŸâŸâŸâŸâ .
(6.55)
The stationary conditions (6.44) through (6.47) imply two possibilities of
stationary points.
6.7.1 Null Stationary Point
If ah = 0 or bh = 0, Eqs. (6.44) and (6.46) require that ah = 0 and bh = 0. In
this case, Eqs. (6.45) and (6.47) lead to
Ïƒ2
ah = c2
ah
â›âœâœâœâœâœâ1 âˆ’
LÏƒ2
ahÏƒ2
bh
Ïƒ2
ââŸâŸâŸâŸâŸâ ,
(6.56)
Ïƒ2
bh = c2
bh
â›âœâœâœâœâœâ1 âˆ’
MÏƒ2
ahÏƒ2
bh
Ïƒ2
ââŸâŸâŸâŸâŸâ .
(6.57)
Multiplying Eqs. (6.56) and (6.57), we have
â›âœâœâœâœâœâ1 âˆ’
LÏƒ2
ahÏƒ2
bh
Ïƒ2
ââŸâŸâŸâŸâŸâ 
â›âœâœâœâœâœâ1 âˆ’
MÏƒ2
ahÏƒ2
bh
Ïƒ2
ââŸâŸâŸâŸâŸâ =
Ïƒ2
ahÏƒ2
bh
c2ahc2
bh
,
(6.58)
and therefore
LM
Ïƒ2 Ïƒ4
ahÏƒ4
bh âˆ’
â›âœâœâœâœâœâL + M +
Ïƒ2
c2ahc2
bh
ââŸâŸâŸâŸâŸâ Ïƒ2
ahÏƒ2
bh + Ïƒ2 = 0.
(6.59)
Solving the quadratic equation (6.59) with respect to Ïƒ2
ahÏƒ2
bh and checking the
signs of Ïƒ2
ah and Ïƒ2
bh, we have the following lemma:
Lemma 6.9
For any Î³h â‰¥0 and c2
ah, c2
bh, Ïƒ2 âˆˆR++, the null stationary point
given by Eq. (6.53) exists with the following free energy:
FVBâˆ’Null
h
= âˆ’M log
!
1 âˆ’L
Ïƒ2Î¶VB
h
"
âˆ’L log
!
1 âˆ’M
Ïƒ2Î¶VB
h
"
âˆ’LM
Ïƒ2 Î¶VB
h ,
(6.60)
where Î¶VB
h
is deï¬ned by Eq. (6.54).

6.7 Proofs of Theorem 6.7 and Corollary 6.8
165
Proof
Eq. (6.59) has two positive real solutions:
Ïƒ2
ahÏƒ2
bh =
Ïƒ2
2LM
â›âœâœâœâœâœâœâœâœâœâL + M +
Ïƒ2
c2ahc2
bh
Â±
>
?
@â›âœâœâœâœâœâL + M +
Ïƒ2
c2ahc2
bh
ââŸâŸâŸâŸâŸâ 
2
âˆ’4LM
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâ .
The larger solution (with the plus sign) is decreasing with respect to c2
ahc2
bh, and
lower-bounded as Ïƒ2
ahÏƒ2
bh > Ïƒ2/L. The smaller solution (with the minus sign)
is increasing with respect to c2
ahc2
bh, and upper-bounded as Ïƒ2
ahÏƒ2
bh < Ïƒ2/M.
For Ïƒ2
ah and Ïƒ2
bh to be positive, Eqs. (6.56) and (6.57) require that
Ïƒ2
ahÏƒ2
bh < Ïƒ2
M ,
(6.61)
which is violated by the larger solution, while satisï¬ed by the smaller solution.
With the smaller solution (6.54), Eqs. (6.56) and (6.57) give the stationary
point given by Eq. (6.53).
Using Eq. (6.59), we can easily derive Eq. (6.60) from Eq. (6.55), which
completes the proof of Lemma 6.9.
â–¡
6.7.2 Positive Stationary Point
Assume that ah,bh  0. In this case, Eqs. (6.44) and (6.46) imply that ah and
bh have the same sign. Deï¬ne
Î³h = ahbh > 0,
(6.62)
Î´h = ah
bh
> 0.
(6.63)
From Eqs. (6.44) and (6.46), we have
Ïƒ2
ah = Ïƒ2
Î³h
Î´h,
(6.64)
Ïƒ2
bh = Ïƒ2
Î³h
Î´âˆ’1
h .
(6.65)
Substituting Eqs. (6.64) and (6.65) into Eqs. (6.45) and (6.47) gives
Î³hÎ´âˆ’1
h =

Î³hÎ´âˆ’1
h + LÏƒ2
Î³h
Î´âˆ’1
h + Ïƒ2
c2ah

,
(6.66)
Î³hÎ´h =
â›âœâœâœâœâœâÎ³hÎ´h + M Ïƒ2
Î³h
Î´h + Ïƒ2
c2
bh
ââŸâŸâŸâŸâŸâ ,
(6.67)

166
6 Global VB Solution of Fully Observed Matrix Factorization
and therefore,
Î´h = cah
Ïƒ2

Î³h âˆ’Î³h âˆ’LÏƒ2
Î³h

,
(6.68)
Î´âˆ’1
h = cbh
Ïƒ2

Î³h âˆ’Î³h âˆ’MÏƒ2
Î³h

.
(6.69)
Multiplying Eqs. (6.68) and (6.69), we have

Î³h âˆ’Î³h âˆ’LÏƒ2
Î³h
 
Î³h âˆ’Î³h âˆ’MÏƒ2
Î³h

=
Ïƒ4
cahcbh
,
(6.70)
and therefore
Î³2
h âˆ’

2Î³h âˆ’(L + M)Ïƒ2
Î³h

Î³h +

Î³h âˆ’LÏƒ2
Î³h
 
Î³h âˆ’MÏƒ2
Î³h

âˆ’
Ïƒ4
cahcbh
= 0. (6.71)
By solving the quadratic equation (6.71) with respect to Î³h, and checking
the signs of Î³h,Î´h, Ïƒ2
ah, and Ïƒ2
bh, we have the following lemma:
Lemma 6.10
If and only if Î³h > Î³VB
h , where Î³VB
h
is deï¬ned by Eq. (6.49),
the positive stationary point given by Eq. (6.51) exists with the following free
energy:
FVBâˆ’Posi
h
= âˆ’M log
â›âœâœâœâœâ1 âˆ’
â›âœâœâœâœâ
Ë˜Î³VB
h
Î³h
+ LÏƒ2
Î³2
h
ââŸâŸâŸâŸâ 
ââŸâŸâŸâŸâ âˆ’L log
â›âœâœâœâœâ1 âˆ’
â›âœâœâœâœâ
Ë˜Î³VB
h
Î³h
+ MÏƒ2
Î³2
h
ââŸâŸâŸâŸâ 
ââŸâŸâŸâŸâ 
âˆ’Î³2
h
Ïƒ2
â›âœâœâœâœâ
Ë˜Î³VB
h
Î³h
+ LÏƒ2
Î³2
h
ââŸâŸâŸâŸâ 
â›âœâœâœâœâ
Ë˜Î³VB
h
Î³h
+ MÏƒ2
Î³2
h
ââŸâŸâŸâŸâ ,
(6.72)
where Ë˜Î³VB
h
is deï¬ned by Eq. (6.50).
Proof
Since Î´h > 0, Eqs. (6.68) and (6.69) require that
Î³h < Î³h âˆ’LÏƒ2
Î³h
,
(6.73)
and therefore, the positive stationary point exists only when
Î³h >
âˆš
MÏƒ.
(6.74)
Let us assume that Eq. (6.74) holds.
Eq. (6.71) has two solutions:
Î³h = 1
2
â›âœâœâœâœâœâœâœâ2Î³h âˆ’(L + M)Ïƒ2
Î³h
Â±
>
@(M âˆ’L)Ïƒ2
Î³h
2
+ 4Ïƒ4
c2ahc2
bh
ââŸâŸâŸâŸâŸâŸâŸâ .

6.7 Proofs of Theorem 6.7 and Corollary 6.8
167
The larger solution with the plus sign is positive, decreasing with respect to
c2
ahc2
bh, and lower-bounded as Î³h > Î³h âˆ’LÏƒ2/Î³h, which violates the condition
(6.73).
The smaller solution, Eq. (6.50), with the minus sign is positive if the
intercept of the left-hand side in Eq. (6.71) is positive, i.e.,

Î³h âˆ’LÏƒ2
Î³h
 
Î³h âˆ’MÏƒ2
Î³h

âˆ’
Ïƒ4
c2ahc2
bh
> 0.
(6.75)
From the condtion (6.75), we obtain the threshold (6.49) for the existence of
the positive stationary point. Note that Î³VB
h
>
âˆš
MÏƒ, and therefore, Eq. (6.74)
holds whenever Î³h > Î³VB
h .
Assume that Î³h > Î³VB
h . Then, with the solution (6.50), Î´h, given by
Eq. (6.68), and Ïƒ2
ah and Ïƒ2
bh, given by Eqs. (6.64) and (6.65), are all positive.
Thus, we obtain the positive stationary point (6.51).
Substituting Eqs. (6.64) and (6.65), and then Eqs. (6.68) and (6.69), into the
free energy (6.55), we have
FVBâˆ’Posi
h
= âˆ’M log
â›âœâœâœâœâ1 âˆ’Ë˜Î³VB
h
Î³h
âˆ’LÏƒ2
Î³2
h
ââŸâŸâŸâŸâ âˆ’L log
â›âœâœâœâœâ1 âˆ’Ë˜Î³VB
h
Î³h
âˆ’MÏƒ2
Î³2
h
ââŸâŸâŸâŸâ 
+ âˆ’2Î³h Ë˜Î³VB
h
Ïƒ2
+ Î³2
h
Ïƒ2 âˆ’
â›âœâœâœâœâœâL + M +
Ïƒ2
c2ahc2
bh
ââŸâŸâŸâŸâŸâ .
(6.76)
Using Eq. (6.70), we can eliminate the direct dependency on c2
ahc2
bh, and express
the free energy (6.76) as a function of Ë˜Î³VB
h . This results in Eq. (6.72), and
completes the proof of Lemma 6.10.
â–¡
6.7.3 Useful Relations
Let us summarize some useful relations between variables, which are used in
the subsequent sections. Î¶VB
h , Ë˜Î³VB
h , and Î³VB
h , derived from Eqs. (6.58), (6.70),
and the constant part of Eq. (6.71), respectively, satisfy the following:
â›âœâœâœâœâœâ1 âˆ’LÎ¶VB
h
Ïƒ2
ââŸâŸâŸâŸâŸâ 
â›âœâœâœâœâœâ1 âˆ’MÎ¶VB
h
Ïƒ2
ââŸâŸâŸâŸâŸâ âˆ’
Î¶VB
h
c2ahc2
bh
= 0,
(6.77)

Î³h âˆ’Ë˜Î³VB
h
âˆ’LÏƒ2
Î³h
 
Î³h âˆ’Ë˜Î³VB
h
âˆ’MÏƒ2
Î³h

âˆ’
Ïƒ4
cahcbh
= 0,
(6.78)
â›âœâœâœâœâœâÎ³VB
h
âˆ’LÏƒ2
Î³VB
h
ââŸâŸâŸâŸâŸâ 
â›âœâœâœâœâœâÎ³VB
h
âˆ’MÏƒ2
Î³VB
h
ââŸâŸâŸâŸâŸâ âˆ’
Ïƒ4
cahcbh
= 0.
(6.79)

168
6 Global VB Solution of Fully Observed Matrix Factorization
From Eqs. (6.54) and (6.49), we ï¬nd that
Î³VB
h
=
>
@â›âœâœâœâœâœâ(L + M)Ïƒ2 +
Ïƒ4
c2ahc2
bh
ââŸâŸâŸâŸâŸâ âˆ’LMÎ¶VB
h ,
(6.80)
which is useful when comparing the free energies of the null and the positive
stationary points.
6.7.4 Free Energy Comparison
Lemmas 6.9 and 6.10 imply that, when Î³h â‰¤Î³VB
h , the null stationary point
is only the stationary point, and therefore the global solution. When Î³h >
Î³VB
h , both of the null and the positive stationary points exist, and therefore
identifying the global solution requires us to compare their free energies, given
by Eqs. (6.60) and (6.72).
Given the observed singular value Î³h â‰¥0, we view the free energy as a
function of c2
ahc2
bh. We also view the threshold Î³VB
h
as a function of c2
ahc2
bh.
We ï¬nd from Eq. (6.49) that Î³VB
h
is decreasing and lower-bounded by Î³VB
h
>
âˆš
MÏƒ. Therefore, when Î³h â‰¤
âˆš
MÏƒ, Î³VB
h
never gets smaller than Î³h for any
c2
ahc2
bh > 0. When Î³h >
âˆš
MÏƒ, on the other hand, there is a threshold cahcbh
such that Î³h > Î³VB
h
if c2
ahc2
bh > cahcbh. Eq. (6.79) implies that the threshold is
given by
cahcbh =
Ïƒ4
Î³2
h
!
1 âˆ’LÏƒ2
Î³2
h
" !
1 âˆ’MÏƒ2
Î³2
h
".
(6.81)
We have the following lemma:
Lemma 6.11
For any Î³h â‰¥0 and c2
ahc2
bh > 0, the derivative of the free energy
(6.60) at the null stationary point with respect to c2
ahc2
bh is given by
âˆ‚FVBâˆ’Null
h
âˆ‚c2ahc2
bh
= LMÎ¶VB
h
Ïƒ2c2ahc2
bh
.
(6.82)
For Î³h > M/Ïƒ2 and c2
ahc2
bh > cahcbh, the derivative of the free energy (6.72) at
the positive stationary point with respect to c2
ahc2
bh is given by

6.7 Proofs of Theorem 6.7 and Corollary 6.8
169
âˆ‚FVBâˆ’Posi
h
âˆ‚c2ahc2
bh
=
Î³2
h
Ïƒ2c2ahc2
bh
â›âœâœâœâœâ
(Ë˜Î³VB
h )2
Î³2
h
âˆ’
â›âœâœâœâœâ1 âˆ’(L + M)Ïƒ2
Î³2
h
ââŸâŸâŸâŸâ 
Ë˜Î³VB
h
Î³h
+ LMÏƒ4
Î³4
h
ââŸâŸâŸâŸâ .
(6.83)
The derivative of the difference is negative, i.e.,
âˆ‚(FVBâˆ’Posi
h
âˆ’FVBâˆ’Null
h
)
âˆ‚c2ahc2
bh
= âˆ’
1
Ïƒ2c2ahc2
bh

Î³h

Î³h âˆ’Ë˜Î³VB
h
 
âˆ’(Î³VB
h )2 
< 0.
(6.84)
Proof
By differentiating Eqs. (6.60), (6.54), (6.72), and (6.50), we have
âˆ‚FVBâˆ’Null
h
âˆ‚Î¶VB
h
=
LM
Ïƒ2 
1 âˆ’L
Ïƒ2Î¶VB
h
 +
LM
Ïƒ2 
1 âˆ’M
Ïƒ2Î¶VB
h
 âˆ’LM
Ïƒ2
=
LMc2
ahc2
bh
!
1 +
âˆš
LM
Ïƒ2 Î¶VB
h
" !
1 âˆ’
âˆš
LM
Ïƒ2 Î¶VB
h
"
Ïƒ2Î¶VB
h
,
(6.85)
âˆ‚Î¶VB
h
âˆ‚c2ahc2
bh
=
Ïƒ2
2LM
â›âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ
âˆ’Ïƒ2
c4ahc4
bh
+
2Ïƒ2

L + M +
Ïƒ2
c2ahc2
bh

2c4ahc4
bh
A
L + M +
Ïƒ2
c2ahc2
bh
2
âˆ’4LM
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
=
1
c4ahc4
bh
â›âœâœâœâœâœâœâœâœâœâœâœâ
(Î¶VB
h )2
!
1 âˆ’
âˆš
LMÎ¶VB
h
Ïƒ2
" !
1 +
âˆš
LMÎ¶VB
h
Ïƒ2
"
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
,
(6.86)
âˆ‚FVBâˆ’Posi
h
âˆ‚Ë˜Î³VB
h
=
M
Î³h
!
1 âˆ’
!
Ë˜Î³VB
h
Î³h + LÏƒ2
Î³2
h
"" +
L
Î³h
!
1 âˆ’
!
Ë˜Î³VB
h
Î³h + MÏƒ2
Î³2
h
""
âˆ’Î³h
Ïƒ2
â›âœâœâœâœâ
2Ë˜Î³VB
h
Î³h
+ (L + M)Ïƒ2
Î³2
h
ââŸâŸâŸâŸâ 
=
2c2
ahc2
bhÎ³3
h
!
1 âˆ’
!
Ë˜Î³VB
h
Î³h + (L+M)Ïƒ2
2Î³2
h
"" !
(Ë˜Î³VB
h )2
Î³2
h
âˆ’
!
1 âˆ’(L+M)Ïƒ2
Î³2
h
"
Ë˜Î³VB
h
Î³h + LMÏƒ4
Î³4
h
"
Ïƒ6
,
(6.87)
âˆ‚Î³h
âˆ‚c2ahc2
bh
=
4Î³2
hÏƒ2
4Î³hc4ahc4
bh
B
(M âˆ’L)2 +
4Î³2
h
c2ahc2
bh
=
Ïƒ4
2Î³hc4ahc4
bh
!
1 âˆ’
!
Ë˜Î³VB
h
Î³h + (L+M)Ïƒ2
2Î³2
h
"".
(6.88)

170
6 Global VB Solution of Fully Observed Matrix Factorization
Here, we used Eqs. (6.54) and (6.77) to obtain Eqs. (6.85) and (6.86), and Eqs.
(6.50) and (6.78) to obtain Eqs. (6.87) and (6.88), respectively. Eq. (6.82) is
obtained by multiplying Eqs. (6.85) and (6.86), while Eq. (6.83) is obtained by
multiplying Eqs. (6.87) and (6.88).
Taking the difference between the derivatives (6.82) and (6.83), and then
using Eqs. (6.78) and (6.80), we have
âˆ‚(FVBâˆ’Posi
h
âˆ’FVBâˆ’Null
h
)
âˆ‚c2ahc2
bh
= âˆ‚FVBâˆ’Posi
h
âˆ‚c2ahc2
bh
âˆ’âˆ‚FVBâˆ’Null
h
âˆ‚c2ahc2
bh
= âˆ’
1
Ïƒ2c2ahc2
bh

Î³h
$Î³h âˆ’Î³h
% âˆ’(Î³VB
h )2 
.
(6.89)
The following can be obtained from Eqs. (6.78) and (6.79), respectively:

Î³h(Î³h âˆ’Ë˜Î³VB
h ) âˆ’(L + M)Ïƒ2
2
2
= (L + M)2Ïƒ4
4
âˆ’LMÏƒ4 +
Ïƒ4
c2ahc2
bh
Î³2
h,
(6.90)

(Î³VB
h )2 âˆ’(L + M)Ïƒ2
2
2
= (L + M)2Ïƒ4
4
âˆ’LMÏƒ4 +
Ïƒ4
c2ahc2
bh
(Î³VB
h )2.
(6.91)
Eqs. (6.90) and (6.91) imply that
Î³h(Î³h âˆ’Ë˜Î³VB
h ) > (Î³VB
h )2
when
Î³h > Î³VB
h .
Therefore, Eq. (6.89) is negative, which completes the proof of Lemma 6.11.
â–¡
It is easy to show that the null stationary point (6.53) and the positive
stationary point (6.51) coincide with each other at c2
ahc2
bh â†’cahcbh + 0. Here,
+0 means that it approaches to zero from the positive side. Therefore,
lim
c2ahc2
bhâ†’cahcbh+0

FVBâˆ’Posi
h
âˆ’FVBâˆ’Null
h
 
= 0.
(6.92)
Eqs. (6.84) and (6.92) together imply that
FVBâˆ’Posi
h
âˆ’FVBâˆ’Null
h
< 0
for
c2
ahc2
bh > cahcbh,
(6.93)
which results in the following lemma:
Lemma 6.12
The positive stationary point is the global solution (the global
minimizer of the free energy (6.43) for ï¬xed cah and cbh) whenever it exists.
Combining Lemmas 6.9, 6.10, and 6.12 completes the proof of Theorem
6.7 and Corollary 6.8.
â–¡
Figure 6.3 illustrates the behavior of the free energies.

6.8 Analytic Form of Global Empirical VB Solution
171
0
0.5
1
1.5
2
2.5
â€“0.5
0
0.5
1
VB
Null
(a) Î³h â‰¤
âˆš
MÏƒ
0
0.5
1
1.5
2
2.5
â€“0.5
0
0.5
1
VB
Null
Posi
(b)
âˆš
MÏƒ < Î³h â‰¤(
âˆš
L +
âˆš
M)Ïƒ
0
0.5
1
1.5
2
2.5
â€“0.5
0
0.5
1
VB
Null
Posi
(c) (
âˆš
L +
âˆš
M)Ïƒ < Î³h < Î³EVB
0
0.5
1
1.5
2
2.5
â€“0.5
0
0.5
1
VB
Null
Posi
(d) Î³h â‰¥Î³EVB
Figure 6.3 Behavior of the free energies (6.60) and (6.72) at the null and the
positive stationary points as functions of cahcbh, when L = M = H = 1
and Ïƒ2 = 1. The curve labeled as â€œVBâ€ shows the VB free energy Fh =
min(FVBâˆ’Null
h
, FVBâˆ’Posi
h
) at the global solution, given cahcbh. If Î³h â‰¤
âˆš
MÏƒ,
only the null stationary point exists for any cahcbh > 0. Otherwise, the positive
stationary point exists for cahcbh > cahcbh, and it is the global minimum whenever
it exists. In empirical VB learning where cahcbh is also optimized, cahcbh â†’0
(indicated by a diamond) is the unique local minimum if Î³h â‰¤(
âˆš
L +
âˆš
M)Ïƒ.
Otherwise, a positive local minimum also exists (indicated by a cross), and it is the
global minimum if and only if Î³h â‰¥Î³EVB (see Section 6.9 for detailed discussion).
6.8 Analytic Form of Global Empirical VB Solution
In this section, we will solve the empirical VB (EVB) problem where the prior
covariances are also estimated from observation, i.e.,
r = argmin
r,CA,CB
F(r)
s.t.
r(A, B) = rA(A)rB(B).
(6.94)
Since the solution of the EVB problem is a VB solution with some values for
the prior covariances CA, CB, the empirical VB posterior is in the same form as
the VB posterior (6.40). Accordingly, the problem (6.94) can be written with
the variational parameters {ah,bh, Ïƒ2
ah, Ïƒ2
bh}H
h=1 as follows:

172
6 Global VB Solution of Fully Observed Matrix Factorization
Given
Ïƒ2 âˆˆR++,
min
{ah,bh,Ïƒ2ah,Ïƒ2
bh,c2ah,c2
bh}H
h=1
F
(6.95)
s.t.
{ah,bh âˆˆR,
Ïƒ2
ah, Ïƒ2
bh, c2
ah, c2
bh âˆˆR++}H
h=1,
(6.96)
where the free energy F is given by Eq. (6.42).
Solving the empirical VB problem (6.95) is not much harder than the
VB problem (6.41) because the objective is still separable into H singular
components when the prior variances {c2
ah, c2
bh} are also optimized. More
speciï¬cally, we can obtain the empirical VB solution by minimizing the
componentwise free energy (6.43) with respect to the only six unknowns
(ah,bh, Ïƒ2
ah, Ïƒ2
bh, c2
ah, c2
bh) for each hth component. On the other hand, analyzing
the VB estimator for the noise variance Ïƒ2 is hard, since Fh for all h = 1,. . . , H
depend on Ïƒ2 and therefore the free energy (6.42) is not separable. We
postpone the analysis of this full empirical VB learning to Chapter 8, where
the theoretical performance guarantee is derived.
For the problem (6.95), the stationary points of the free energy (6.43) satisfy
Eqs. (6.44) through (6.47) along with Eqs. (3.26) and (3.27), which are written
with the new set of variational parameters as
c2
ah = a2
h/M + Ïƒ2
ah,
(6.97)
c2
bh = b2
h/L + Ïƒ2
bh.
(6.98)
However, unlike the VB solution, for which Lemma 6.1 holds, we cannot
assume that the EVB solution is a stationary point, since the free energy Fh
given by Eq. (6.43) does not necessarily diverge to +âˆwhen approaching the
domain boundary (6.96). More speciï¬cally, Fh can converge to a ï¬nite value,
for example, for ah = bh = 0, Ïƒ2
ah, Ïƒ2
bh, c2
ah, c2
bh â†’+0. Taking this into account,
we can obtain the following theorem:
Theorem 6.13
Let
Î± = L
M
(0 < Î± â‰¤1),
(6.99)
and let Ï„ = Ï„(Î±) be the unique zero-cross point of the following decreasing
function:
Î (Ï„; Î±) = Î¦ (Ï„) + Î¦
! Ï„
Î±
"
,
where
Î¦(z) = log(z + 1)
z
âˆ’1
2.
(6.100)
Then, the EVB solution is given by
U
EVB =
H

h=1
Î³EVB
h
Ï‰bhÏ‰âŠ¤
ah,
where
Î³EVB
h
=
â§âªâªâ¨âªâªâ©
Ë˜Î³EVB
h
if Î³h â‰¥Î³EVB,
0
otherwise,
(6.101)

6.9 Proof of Theorem 6.13
173
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
2.5
3
Figure 6.4 Values of Ï„(Î±), âˆšÎ±, and z âˆšÎ±.
for
Î³EVB = Ïƒ
A
M

1 + Ï„
 
1 + Î±
Ï„

,
(6.102)
Ë˜Î³EVB
h
= Î³h
2
â›âœâœâœâœâœâœâœâœâ1 âˆ’(M + L)Ïƒ2
Î³2
h
+
>
@â›âœâœâœâœâ1 âˆ’(M + L)Ïƒ2
Î³2
h
ââŸâŸâŸâŸâ 
2
âˆ’4LMÏƒ4
Î³4
h
ââŸâŸâŸâŸâŸâŸâŸâŸâ .
(6.103)
The EVB threshold (6.102) involves Ï„, which needs to be numerically
computed. However, we can easily prepare a table of the values for 0 < Î± â‰¤1
beforehand, like the cumulative Gaussian probability used in statistical tests
(Pearson, 1914). Alternatively, Ï„ â‰ˆz âˆšÎ± is a good approximation, where
z â‰ˆ2.5129 is the unique zero-cross point of Î¦(z), as seen in Figure 6.4. We
can show that Ï„ lies in the following range (Lemma 6.18 in Section 6.9):
âˆšÎ± < Ï„ < z.
(6.104)
We will see in Chapter 8 that Ï„ is an important quantity in describing the
behavior of the full empirical VB solution where the noise variance Ïƒ2 is also
estimated from observation.
In Section 6.9, we give the proof of Theorem 6.13. Then, in Section 6.10,
some corollaries obtained and variables deï¬ned in the proof are summarized,
which will be used in Chapter 8.
6.9 Proof of Theorem 6.13
In this section, we prove Theorem 6.13, which provides explicit forms, Eqs.
(6.102) and (6.103), of the EVB threshold Î³EVB and the EVB shrinkage
estimator Ë˜Î³EVB
h
. In fact, we can easily obtain Eq. (6.103) in an intuitive way, by

174
6 Global VB Solution of Fully Observed Matrix Factorization
using some of the results obtained in Section 6.7. After that, by expressing the
free energy Fh with normalized versions of the observation and the estimator,
we derive Eq. (6.102).
6.9.1 EVB Shrinkage Estimator
Eqs. (6.60) and (6.72) imply that the free energy does not depend on the
ratio cah/cbh between the hyperparameters. Accordingly, we ï¬x the ratio to
cah/cbh = 1 without loss of generality. Lemma 6.11 allows us to minimize the
free energy with respect to cahcbh in a straightforward way.
Let us regard the free energies (6.60) and (6.72) at the null and the positive
stationary points as functions of cahcbh (see Figure 6.3). Then, we ï¬nd from
Eq. (6.82) that
âˆ‚FVBâˆ’Null
h
âˆ‚c2ahc2
bh
> 0,
which implies that the free energy (6.60) at the null stationary point is
increasing. Using Lemma 6.9, we thus have the following lemma:
Lemma 6.14
For any given Î³h â‰¥0 and Ïƒ2 > 0, the null EVB local solution,
given by
ah = 0,
bh = 0,
Ïƒ2
ah =
.
Î¶EVB,
Ïƒ2
bh =
.
Î¶EVB,
cahcbh =
.
Î¶EVB,
where
Î¶EVB â†’+0,
exists, and its free energy is given by
FEVBâˆ’Null
h
â†’+0.
(6.105)
When Î³h â‰¥(
âˆš
L +
âˆš
M)Ïƒ, the derivative (6.83) of the free energy (6.72) at
the positive stationary point can be further factorized as
âˆ‚FVBâˆ’Posi
h
âˆ‚c2ahc2
bh
=
Î³h
Ïƒ2c2ahc2
bh

Ë˜Î³VB
h
âˆ’Â´Î³h
 
Ë˜Î³VB
h
âˆ’Ë˜Î³EVB
h
 
,
(6.106)
where
Â´Î³h = Î³h
2
â›âœâœâœâœâœâœâœâœâ1 âˆ’(L + M)Ïƒ2
Î³2
h
âˆ’
>
@â›âœâœâœâœâ1 âˆ’(L + M)Ïƒ2
Î³2
h
ââŸâŸâŸâŸâ 
2
âˆ’4LMÏƒ4
Î³4
h
ââŸâŸâŸâŸâŸâŸâŸâŸâ ,
(6.107)

6.9 Proof of Theorem 6.13
175
and Ë˜Î³EVB
h
is given by Eq. (6.103). The VB shrinkage estimator (6.50) is an
increasing function of cahcbh ranging over
0 < Ë˜Î³VB
h
< Î³h âˆ’MÏƒ2
Î³h
,
and both of Eqs. (6.107) and (6.103) are in this range, i.e.,
0 < Â´Î³h â‰¤Ë˜Î³EVB
h
< Î³h âˆ’MÏƒ2
Î³h
.
Therefore Eq. (6.106) leads to the following lemma:
Lemma 6.15
If Î³h â‰¤(
âˆš
L +
âˆš
M)Ïƒ, the free energy FVBâˆ’Posi
h
at the positive
stationary point is monotonically increasing. Otherwise,
FVBâˆ’Posi
h
is
â§âªâªâªâªâªâ¨âªâªâªâªâªâ©
increasing
for
Ë˜Î³VB
h
< Â´Î³h,
decreasing
for
Â´Î³h < Ë˜Î³VB
h
< Ë˜Î³EVB
h
,
increasing
for
Ë˜Î³VB
h
> Ë˜Î³EVB
h
,
and therefore minimized at Ë˜Î³VB
h
= Ë˜Î³EVB
h
.
We can see this behavior of the free energy in Figure 6.3.
The derivative (6.83) is zero when Ë˜Î³VB
h
= Ë˜Î³EVB
h
, which leads to

Ë˜Î³EVB
h
+ LÏƒ2
Î³h
 
Ë˜Î³EVB
h
+ MÏƒ2
Î³h

= Î³hË˜Î³EVB
h
.
(6.108)
Using Eq. (6.108), we obtain the following lemma:
Lemma 6.16
If and only if
Î³h â‰¥Î³localâˆ’EVB â‰¡(
âˆš
L +
âˆš
M)Ïƒ,
(6.109)
the positive EVB local solution given by
ah = Â±
.
Ë˜Î³EVB
h
Î´EVB
h
,
bh = Â±
>
@
Ë˜Î³EVB
h
Î´EVB
h
,
(6.110)
Ïƒ2
ah = Ïƒ2Î´EVB
h
Î³h
,
Ïƒ2
bh =
Ïƒ2
Î³hÎ´EVB
h
,
cahcbh =
A
Î³hË˜Î³EVB
h
LM
,
(6.111)
exists with the following free energy:
FEVBâˆ’Posi
h
= M log
â›âœâœâœâœâ
Î³hË˜Î³EVB
h
MÏƒ2
+ 1
ââŸâŸâŸâŸâ + L log
â›âœâœâœâœâ
Î³hË˜Î³EVB
h
LÏƒ2
+ 1
ââŸâŸâŸâŸâ âˆ’Î³hË˜Î³EVB
h
Ïƒ2
.
(6.112)

176
6 Global VB Solution of Fully Observed Matrix Factorization
Here,
Î´EVB
h
=
A
MË˜Î³EVB
h
LÎ³h
â›âœâœâœâœâ1 +
LÏƒ2
Î³hË˜Î³EVB
h
ââŸâŸâŸâŸâ ,
(6.113)
and Ë˜Î³EVB
h
is given by Eq. (6.103).
Proof
Lemma 6.15 immediately leads to the EVB shrinkage estimator
(6.103). We can ï¬nd the value of cahcbh at the positive EVB local solution
by combining the condition (6.78) for the VB estimator and the condition
(6.108) for the EVB estimator. Speciï¬cally, by using the condition (6.108),
the condition (6.78) for Ë˜Î³VB
h
replaced with Ë˜Î³EVB
h
can be written as
â›âœâœâœâœâœâœâÎ³h âˆ’
Î³hË˜Î³EVB
h
Ë˜Î³EVB
h
+ MÏƒ2
Î³h
ââŸâŸâŸâŸâŸâŸâ 
â›âœâœâœâœâœâœâÎ³h âˆ’
Î³hË˜Î³EVB
h
Ë˜Î³EVB
h
+ LÏƒ2
Î³h
ââŸâŸâŸâŸâŸâŸâ =
Ïƒ4
c2ahc2
bh
,
and therefore
â›âœâœâœâœâœâœâ
MÏƒ2
Ë˜Î³EVB
h
+ MÏƒ2
Î³h
ââŸâŸâŸâŸâŸâŸâ 
â›âœâœâœâœâœâœâ
LÏƒ2
Ë˜Î³EVB
h
+ LÏƒ2
Î³h
ââŸâŸâŸâŸâŸâŸâ =
Ïƒ4
c2ahc2
bh
.
Applying the condition (6.108) again gives
LMÏƒ4
Î³hË˜Î³EVB
h
=
Ïƒ4
c2ahc2
bh
,
which leads to the last equation in Eq. (6.111).
Similarly, using the condition (6.108), Eq. (6.52) for Ë˜Î³VB
h
replaced with Ë˜Î³EVB
h
is written as
Î´h =
c2
ah
Ïƒ2
â›âœâœâœâœâœâœâÎ³h âˆ’
Î³hË˜Î³EVB
h
Ë˜Î³EVB
h
+ MÏƒ2
Î³h
ââŸâŸâŸâŸâŸâŸâ 
=
c2
ah
Ïƒ2
â›âœâœâœâœâœâœâ
MÏƒ2
Ë˜Î³EVB
h
+ MÏƒ2
Î³h
ââŸâŸâŸâŸâŸâŸâ 
=
c2
ah M
Î³h
â›âœâœâœâœâ
Î³hË˜Î³EVB
h
+ LÏƒ2
Î³hË˜Î³EVB
h
ââŸâŸâŸâŸâ 
=
c2
ah M
Î³h
â›âœâœâœâœâ1 +
LÏƒ2
Î³hË˜Î³EVB
h
ââŸâŸâŸâŸâ .
Using the assumption that cah = cbh and therefore c2
ah = cahcbh, we obtain
Eq. (6.113). Eq. (6.110) and the ï¬rst two equations in Eq. (6.111) are simply
obtained from Lemma 6.10.

6.9 Proof of Theorem 6.13
177
Finally, applying Eq. (6.108) to the free energy (6.72), we have
FEVBâˆ’Posi
h
= âˆ’M log
â›âœâœâœâœâ1 âˆ’
Î³hË˜Î³EVB
h
Î³hË˜Î³EVB
h
+ MÏƒ2
ââŸâŸâŸâŸâ âˆ’L log
â›âœâœâœâœâ1 âˆ’
Î³hË˜Î³EVB
h
Î³hË˜Î³EVB
h
+ LÏƒ2
ââŸâŸâŸâŸâ 
âˆ’Î³hË˜Î³EVB
h
Ïƒ2
,
which leads to Eq. (6.112). This completes the proof of Lemma 6.16.
â–¡
In Figure 6.3, the positive EVB local solution at cahcbh =
.
Î³hË˜Î³EVB
h
/(LM)
is indicated by a cross if it exists.
6.9.2 EVB Threshold
Lemmas 6.14 and 6.16 state that, if Î³h â‰¤Î³localâˆ’EVB, only the null EVB local
solution exists, and therefore it is the global EVB solution. In this section,
assuming that Î³h â‰¥Î³localâˆ’EVB, we compare the free energy (6.105) at the
null EVB local solution and the free energy (6.112) at the positive EVB local
solution. Since FEVBâˆ’Null
h
â†’+0, we simply consider the situation where
FEVBâˆ’Posi
h
â‰¤0. Eq. (6.108) gives

Î³hË˜Î³EVB
h
+ LÏƒ2 â›âœâœâœâœâ1 +
MÏƒ2
Î³hË˜Î³EVB
h
ââŸâŸâŸâŸâ = Î³2
h.
(6.114)
By using Eqs. (6.103) and (6.109), we have
Î³hË˜Î³EVB
h
= 1
2

Î³2
h âˆ’

Î³localâˆ’EVB 2 + 2
âˆš
LMÏƒ2
+
B!
Î³2
h âˆ’

Î³localâˆ’EVB 2" !
Î³2
h âˆ’

Î³localâˆ’EVB 2 + 4
âˆš
LMÏƒ2
"
â‰¥
âˆš
LMÏƒ2.
(6.115)
Remember the deï¬nition of Î± (Eq. (6.99))
Î± = L
M
(0 < Î± â‰¤1),
and let
xh =
Î³2
h
MÏƒ2 ,
(6.116)
Ï„h = Î³hË˜Î³EVB
h
MÏƒ2 .
(6.117)

178
6 Global VB Solution of Fully Observed Matrix Factorization
0
5
10
âˆ’0.5
0
0.5
Figure 6.5 Î¦(z) = log(z+1)
z
âˆ’1
2.
Eqs. (6.114) and (6.103) imply the following mutual relations between xh
and Ï„h:
xh â‰¡x(Ï„h; Î±) = (1 + Ï„h)

1 + Î±
Ï„h

,
(6.118)
Ï„h â‰¡Ï„(xh; Î±) = 1
2

xh âˆ’(1 + Î±) +
.
(xh âˆ’(1 + Î±))2 âˆ’4Î±

.
(6.119)
Eqs. (6.109) and (6.115) lead to
xh â‰¥xlocal =
(Î³localâˆ’EVB)2
MÏƒ2
= x( âˆšÎ±; Î±) = (1 + âˆšÎ±)2,
(6.120)
Ï„h â‰¥Ï„local = âˆšÎ±.
(6.121)
Then, using Î (Ï„; Î±) deï¬ned by Eq. (6.100), we can rewrite Eq. (6.112) as
FEVBâˆ’Posi
h
= M log (Ï„h + 1) + L log
!Ï„h
Î± + 1
"
âˆ’MÏ„h
= MÏ„hÎ (Ï„; Î±) .
(6.122)
The following holds for Î¦(z) (which is also deï¬ned in Eq. (6.100)):
Lemma 6.17
Î¦(z) is decreasing for z > 0.
Proof
The derivative is
âˆ‚Î¦
âˆ‚z =
1 âˆ’
1
z+1 âˆ’log(z + 1)
z2
,
which is negative for z > 0 because
1
z + 1 + log(z + 1) > 1.
This completes the proof of Lemma 6.17.
â–¡

6.9 Proof of Theorem 6.13
179
Figure 6.5 shows the proï¬le of Î¦(z). Since Î¦(z) is decreasing, Î(Ï„; Î±) is
also decreasing with respect to Ï„. It holds that, for any 0 < Î± â‰¤1,
lim
Ï„â†’0 Î(Ï„; Î±) = 1,
lim
Ï„â†’âˆÎ(Ï„; Î±) = âˆ’1.
Therefore, Î(Ï„; Î±) has a unique zero-cross point Ï„, such that
Î(Ï„; Î±) â‰¤0
if and only if
Ï„ â‰¥Ï„.
(6.123)
Then, we can prove the following lemma:
Lemma 6.18
The unique zero-cross point Ï„ of Î(Ï„; Î±) lies in the following
range:
âˆšÎ± < Ï„ < z,
where z â‰ˆ2.5129 is the unique zero-cross point of Î¦(z).
Proof
Since Î¦(z) is decreasing, Î (Ï„; Î±) is upper-bounded by
Î (Ï„; Î±) = Î¦ (Ï„) + Î¦
! Ï„
Î±
"
â‰¤2Î¦ (Ï„) = Î (Ï„; 1) .
Therefore, the unique zero-cross point Ï„ of Î (Ï„; Î±) is no greater than the
unique zero-cross point z of Î¦(z), i.e.,
Ï„ â‰¤z.
For obtaining the lower-bound Ï„
>
âˆšÎ±, it sufï¬ces to show that
Î( âˆšÎ±; Î±) > 0. Let us prove that the following function is decreasing and
positive for 0 < Î± â‰¤1:
g(Î±) â‰¡
Î
 âˆšÎ±; Î±
 
âˆšÎ±
.
From the deï¬nition (6.100) of Î (Ï„; Î±), we have
g(Î±) =

1 + 1
Î±

log( âˆšÎ± + 1) âˆ’log âˆšÎ± âˆ’
1
âˆšÎ±.
The derivative is given by
âˆ‚g
âˆ‚âˆšÎ± =

1 + 1
Î±
 
âˆšÎ± + 1 âˆ’
2
Î±3/2 log( âˆšÎ± + 1) âˆ’
1
âˆšÎ± + 1
Î±
= âˆ’2
Î±3/2

log( âˆšÎ± + 1) +
1
âˆšÎ± + 1 âˆ’1

< 0,

180
6 Global VB Solution of Fully Observed Matrix Factorization
which implies that g(Î±) is decreasing. Since
g(1) = 2 log 2 âˆ’1 â‰ˆ0.3863 > 0,
g(Î±) is positive for 0 < Î± â‰¤1, which completes the proof of Lemma 6.18.
â–¡
Since Eq. (6.118) is increasing with respect to Ï„h (> âˆšÎ±), the thresholding
condition Ï„ â‰¥Ï„ in Eq. (6.123) can be expressed in terms of x:
Î(Ï„(x); Î±) â‰¤0
if and only if
x â‰¥x,
(6.124)
where
x â‰¡x(Ï„; Î±) =

1 + Ï„
 
1 + Î±
Ï„

.
(6.125)
Using Eqs. (6.116) and (6.122), we have
FEVBâˆ’Posi
h
â‰¤0
if and only if
Î³h â‰¥Î³EVB,
(6.126)
where Î³EVB is deï¬ned by Eq. (6.102). Thus, we have the following lemma:
Lemma 6.19
The positive EVB local solution is the global EVB solution if
and only if Î³h â‰¥Î³EVB.
Combining Lemmas 6.14, 6.16, and 6.19 completes the proof of
Theorem 6.13.
â–¡
Figure 6.6 shows estimators and thresholds for L = M = H = 1 and Ïƒ2 = 1.
The curves indicate the VB solutionÎ³VB
h
given by Eq. (6.48), the EVB solution
Î³EVB
h
given by Eq. (6.101), the EVB positive local minimizer Ë˜Î³EVB
h
given by
Eq. (6.103), and the EVB positive local maximizer Â´Î³h given by Eq. (6.107),
respectively. The arrows indicate the VB threshold Î³VB
h
given by Eq. (6.49),
the local EVB threshold Î³localâˆ’EVB given by Eq. (6.109), and the EVB threshold
Î³EVB given by Eq. (6.102), respectively.
6.10 Summary of Intermediate Results
In the rest of this section, we summarize some intermediate results obtained in
Section 6.9, which are useful for further analysis (mainly in Chapter 8).
Summarizing Eqs. (6.109), (6.114), and (6.115) leads to the following
corollary:
Corollary 6.20
The EVB shrinkage estimator (6.103) is a stationary point of
the free energy (6.43), which exists if and only if
Î³h â‰¥Î³localâˆ’EVB â‰¡(
âˆš
L +
âˆš
M)Ïƒ,
(6.127)

6.10 Summary of Intermediate Results
181
1
2
3
1
2
3
Figure 6.6 Estimators and thresholds for L = M = H = 1 and Ïƒ2 = 1.
and satisï¬es the following equation:

Î³hË˜Î³EVB
h
+ LÏƒ2 â›âœâœâœâœâ1 +
MÏƒ2
Î³hË˜Î³EVB
h
ââŸâŸâŸâŸâ = Î³2
h.
(6.128)
It holds that
Î³hË˜Î³EVB
h
â‰¥
âˆš
LMÏƒ2.
(6.129)
Combining Lemmas 6.14, 6.16, and 6.19 leads to the following corollary:
Corollary 6.21
The minimum free energy achieved under EVB learning is
given by Eq. (6.42) with
2Fh =
â§âªâªâªâ¨âªâªâªâ©
M log
!
Î³h Ë˜Î³EVB
h
MÏƒ2 + 1
"
+ L log
!
Î³h Ë˜Î³EVB
h
LÏƒ2
+ 1
"
âˆ’
Î³h Ë˜Î³EVB
h
Ïƒ2
if Î³h â‰¥Î³EVB,
+0
otherwise.
(6.130)
Corollary 6.20 together with Theorem 6.13 implies that when
Î³localâˆ’EVB â‰¤Î³h < Î³EVB,
a stationary point (called the positive EVB local solution and speciï¬ed by
Lemma 6.16) exists at Eq. (6.103), but it is not the global minimum. Actually,
a local minimum (called the null EVB local solution and speciï¬ed by Lemma
6.14) with Fh = +0 always exists. The stationary point at Eq. (6.103) is
a nonglobal local minimum when Î³localâˆ’EVB â‰¤Î³h < Î³EVB and the global

182
6 Global VB Solution of Fully Observed Matrix Factorization
minimum when Î³h â‰¥Î³EVB (see Figure 6.3 with its caption). This phase
transition induces the free energy thresholding observed in Corollary 6.21.
We deï¬ne a local-EVB estimator by
U
localâˆ’EVB =
H

h=1
Î³localâˆ’EVB
h
Ï‰bhÏ‰âŠ¤
ah,
where
Î³localâˆ’EVB
h
=
â§âªâªâ¨âªâªâ©
Ë˜Î³EVB
h
if Î³h â‰¥Î³localâˆ’EVB,
0
otherwise,
(6.131)
and call Î³localâˆ’EVB a local-EVB threshold. This estimator gives the positive
EVB local solution, whenever it exists, for each singular component. There
is an interesting relation between the local-EVB solution and an alternative
dimensionality selection method (Hoyle, 2008), which will be discussed in
Section 8.6.
Rescaling the quantities related to the squared singular value by MÏƒ2â€”to
which the contribution from noise (each eigenvalue of EâŠ¤E) scales linearlyâ€”
simpliï¬es expressions. Assume that the condition (6.127) holds, and deï¬ne
xh =
Î³2
h
MÏƒ2 ,
(6.132)
Ï„h = Î³hË˜Î³EVB
h
MÏƒ2 ,
(6.133)
which are used as a rescaled observation and a rescaled EVB estimator,
respectively. Eqs. (6.128) and (6.103) specify the mutual relations between
them:
xh â‰¡x(Ï„h; Î±) = (1 + Ï„h)

1 + Î±
Ï„h

,
(6.134)
Ï„h â‰¡Ï„(xh; Î±) = 1
2

xh âˆ’(1 + Î±) +
.
(xh âˆ’(1 + Î±))2 âˆ’4Î±

.
(6.135)
With these rescaled variables, the condition (6.127), as well as (6.129), for the
existence of the positive local-EVB solution Ë˜Î³EVB
h
is expressed as
xh â‰¥xlocal =
(Î³localâˆ’EVB)2
MÏƒ2
= x( âˆšÎ±; Î±) = (1 + âˆšÎ±)2,
(6.136)
Ï„h â‰¥Ï„local = âˆšÎ±.
(6.137)
The EVB threshold (6.102) is expressed as
x =
(Î³EVB)2
MÏƒ2
= x(Ï„; Î±) =

1 + Ï„
 
1 + Î±
Ï„

,
(6.138)

6.10 Summary of Intermediate Results
183
and the free energy (6.130) is expressed as
Fh = MÏ„h Â· min (0, Î (Ï„h; Î±)) ,
(6.139)
where Î(Ï„; Î±) is deï¬ned by Eq. (6.100).
The preceding rescaled expressions give an intuition of Theorem 6.13: the
EVB solution Î³EVB
h
is positive if and only if the positive local-EVB solution
Ë˜Î³EVB
h
exists (i.e., xh â‰¥xlocal), and the free energy Î (Ï„(xh; Î±); Î±) at the local-
EVB solution is nonpositive (i.e., Ï„(xh; Î±) â‰¥Ï„ or equivalently xh â‰¥x ).

7
Model-Induced Regularization and Sparsity
Inducing Mechanism
Variational Bayesian (VB) learning often shows the automatic relevance
determination (ARD) propertyâ€”the solution is sparse with unnecessary com-
ponents eliminated automatically. In this chapter, we try to elucidate the
sparsity inducing mechanism of VB learning, based on the global analytic
solutions derived in Chapter 6. We argue that the ARD property is induced by
the model-induced regularization (MIR), which all Bayesian learning methods
possess when unidentiï¬able models are involved, and that MIR is enhanced by
the independence constraint (imposed for computational tractability), which
induces phase transitions making the solution (exactly) sparse (Nakajima and
Sugiyama, 2011).
We ï¬rst show the VB solution for special cases where the MIR effect is
visible in the solution form. Then we illustrate the behavior of the posteriors
and estimators in the one-dimensional case, comparing VB learning with
maximum a posteriori (MAP) learning and Bayesian learning. After that, we
explain MIR, and how it is enhanced in VB learning through phase transitions.
7.1 VB Solutions for Special Cases
Here we discuss two special cases of fully observed matrix factorization (MF),
in which the VB solution is simple and intuitive.
Almost Flat Prior
When cahcbh â†’âˆ(i.e., the prior is almost ï¬‚at), the VB solution given by
Theorem 6.7 in Chapter 6 has a simple form.
184

7.1 VB Solutions for Special Cases
185
Corollary 7.1
The VB solution of the fully observed matrix factorization
model (6.1) through (6.3) is given by
U
VB =
H

h=1
Î³VB
h Ï‰bhÏ‰âŠ¤
ah,
(7.1)
where the estimator Î³VB
h
corresponding to the hth largest singular value is
upper-bounded as
Î³VB
h
< max
â§âªâªâ¨âªâªâ©0,
â›âœâœâœâœâ1 âˆ’max(L, M)Ïƒ2
Î³2
h
ââŸâŸâŸâŸâ Î³h
â«âªâªâ¬âªâªâ­.
(7.2)
For the almost ï¬‚at prior (i.e., cahcbh â†’âˆ), the equality holds, i.e.,
lim
cahcbhâ†’âˆÎ³VB
h
= max
â§âªâªâ¨âªâªâ©0,
â›âœâœâœâœâ1 âˆ’max(L, M)Ïƒ2
Î³2
h
ââŸâŸâŸâŸâ Î³h
â«âªâªâ¬âªâªâ­.
(7.3)
Proof
It is clear that the threshold (6.49) is decreasing and the shrinkage
factor (6.50) is increasing with respect to cahcbh. Therefore, Î³VB
h
is largest for
cahcbh â†’âˆ. In this limit, Eqs. (6.49) and (6.50) are reduced to
lim
cahcbhâ†’âˆÎ³VB
h
= Ïƒ
>
?
@
(L + M)
2
+
A(L + M)
2
2
âˆ’LM
= Ïƒ
C
max(L, M),
lim
cahcbhâ†’âˆË˜Î³VB
h
= Î³h
â›âœâœâœâœâ1 âˆ’Ïƒ2
2Î³2
h

M + L +
C
(M âˆ’L)2 ââŸâŸâŸâŸâ 
=
â›âœâœâœâœâ1 âˆ’max(L, M)Ïƒ2
Î³2
h
ââŸâŸâŸâŸâ Î³h,
which prove the corollary.
â–¡
The form of the VB solution (7.3) in the limit is known as the positive-
part Jamesâ€“Stein (PJS) estimator (James and Stein, 1961), operated on each
singular component separately (see Appendix A for its interesting property
and the relation to Bayesian learning). A counterintuitive factâ€”a shrinkage is
observed even in the limit of the ï¬‚at priorâ€”will be explained in terms of MIR
in Section 7.3.
Square Matrix
When L = M (i.e., the observed matrix V is square), the VB solution is
intuitive, so that the shrinkage caused by MIR and the shrinkage caused by
the prior are separately visible in its formula.

186
7 Model-Induced Regularization and Sparsity Inducing Mechanism
Corollary 7.2
When L = M, the VB solution is given by Eq. (7.1) with
Î³VB
h
= max
â§âªâªâ¨âªâªâ©0,
â›âœâœâœâœâ1 âˆ’MÏƒ2
Î³2
h
ââŸâŸâŸâŸâ Î³h âˆ’
Ïƒ2
cahcbh
â«âªâªâ¬âªâªâ­.
(7.4)
Proof
When L = M, Eqs. (6.49) and (6.50) can be written as
Î³VB
h
= Ïƒ
>
?
?
?
@
M +
Ïƒ2
2c2ahc2
bh
+
>
?
@â›âœâœâœâœâœâM +
Ïƒ2
2c2ahc2
bh
ââŸâŸâŸâŸâŸâ 
2
âˆ’M2,
Ë˜Î³VB
h
= Î³h
â›âœâœâœâœâœâœâœâ1 âˆ’Ïƒ2
2Î³2
h
â›âœâœâœâœâœâœâœâ2M +
>
@
4Î³2
h
c2ahc2
bh
ââŸâŸâŸâŸâŸâŸâŸâ 
ââŸâŸâŸâŸâŸâŸâŸâ 
= Î³h
â›âœâœâœâœâ1 âˆ’Ïƒ2
Î³2
h

M +
Î³h
cahcbh
ââŸâŸâŸâŸâ .
We can conï¬rm that Ë˜Î³VB
h
â‰¤0 when Î³h â‰¤Î³VB
h , which proves the corollary.
Actually, we can conï¬rm that Ë˜Î³VB
h
= 0 when Î³h = Î³VB
h , and Ë˜Î³VB
h
< 0 when
Î³h < Î³VB
h
for any L, M, c2
ah, and c2
bh.
â–¡
In the VB solution (7.4), we can identify the PJS shrinkage and a constant
shrinkage. The PJS shrinkage can be considered to be caused by MIR since
it appears even with the ï¬‚at prior, while the constant shrinkage âˆ’Ïƒ2/(cahcbh)
is considered to be caused by the prior since it appears in MAP learning (see
Theorem 12.1 in Chapter 12).
The empirical VB (EVB) solution is also simple for square matrices. The
following corollary is obtained from Theorem 6.13 in Chapter 6:
Corollary 7.3
When L = M, the global EVB solution is given by
Î³EVB
h
=
â§âªâªâ¨âªâªâ©
Ë˜Î³EVB
h
if Î³h > Î³EVB,
0
otherwise,
where
Î³EVB = Ïƒ
A
M

2 + Ï„(1) +
1
Ï„(1)

,
Ë˜Î³EVB
h
= Î³h
2
â›âœâœâœâœâœâœâ1 âˆ’2MÏƒ2
Î³2
h
+
A
1 âˆ’4MÏƒ2
Î³2
h
ââŸâŸâŸâŸâŸâŸâ .

7.2 Posteriors and Estimators in a One-Dimensional Case
187
Proof
When L = M, Eqs. (6.102) and (6.103) can be written as
Î³EVB = Ïƒ
A
M

2 + Ï„(1) +
1
Ï„(1)

,
Ë˜Î³EVB
h
= Î³h
2
â›âœâœâœâœâœâœâœâœâ1 âˆ’2MÏƒ2
Î³2
h
+
>
@â›âœâœâœâœâ1 âˆ’2MÏƒ2
Î³2
h
ââŸâŸâŸâŸâ 
2
âˆ’4M2Ïƒ4
Î³4
h
ââŸâŸâŸâŸâŸâŸâŸâŸâ 
= Î³h
2
â›âœâœâœâœâœâœâ1 âˆ’2MÏƒ2
Î³2
h
+
A
1 âˆ’4MÏƒ2
Î³2
h
ââŸâŸâŸâŸâŸâŸâ ,
which completes the proof.
â–¡
7.2 Posteriors and Estimators in a One-Dimensional Case
In order to illustrate how strongly Bayesian learning and its approximation
methods are regularized, we depict posteriors and estimators in the MF model
for L = M = H = 1 (i.e., U, V, A, and B are merely scalars):
p(V|A, B) =
1
âˆš
2Ï€Ïƒ2 exp

âˆ’(V âˆ’BA)2
2Ïƒ2

.
(7.5)
In this model, we can visualize the unidentiï¬ability of the MF model as
equivalence classesâ€”a set of points (A, B) on which the product is unchanged,
i.e., U = BA, represents the same distribution (see Figure 7.1). When U = 0,
the equivalence class has a â€œcross-shapeâ€ proï¬le on the A- and B-axes;
otherwise, it forms a pair of hyperbolic curves. This redundant structure in the
âˆ’2
âˆ’1
0
1
A
âˆ’3
2
3
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
B
U= 2
U= 1
U= 0
U= âˆ’1
U= âˆ’2
Figure 7.1 Equivalence class structure of the one-dimensional MF model. Any A
and B such that their product is unchanged give the same U.

188
7 Model-Induced Regularization and Sparsity Inducing Mechanism
parameter space is the origin of MIR, and highly inï¬‚uences the phase transition
phenomenon in VB learning, as we will see shortly.
With Gaussian priors,
p(A) =
1
C
2Ï€c2a
exp

âˆ’A2
2c2a

,
(7.6)
p(B) =
1
.
2Ï€c2
b
exp
â›âœâœâœâœââˆ’B2
2c2
b
ââŸâŸâŸâŸâ ,
(7.7)
the Bayes posterior is proportional to
p(A, B|V) âˆp(V|A, B)p(A)p(B)
âˆexp
â›âœâœâœâœââˆ’1
2Ïƒ2 (V âˆ’BA)2 âˆ’A2
2c2a
âˆ’B2
2c2
b
ââŸâŸâŸâŸâ .
(7.8)
Figure 7.2 shows the contour of the unnormalized Bayes posterior (7.8) when
V = 0, 1, 2 are observed, the noise variance is Ïƒ2 = 1, and the prior covariances
are set to ca = cb = 100 (i.e., almost ï¬‚at priors). We can see that the
equivalence class structure is reï¬‚ected in the Bayes posterior: when V = 0,
the surface of the Bayes posterior has a cross-shaped proï¬le and its maximum
is at the origin; when V > 0, the surface is divided into the positive orthant
(i.e., A, B > 0) and the negative orthant (i.e., A, B < 0), and the two â€œmodesâ€
get farther as V increases.
MAP Solution
Let us ï¬rst investigate the behavior of the MAP estimator, which coincides with
the maximum likelihood (ML) estimator when the priors are ï¬‚at. For ï¬nite ca
and cb, the MAP solution can be expressed as
AMAP = Â±
A
ca
cb
max
)
0, |V| âˆ’Ïƒ2
cacb
1
,
BMAP = Â±sign(V)
A
cb
ca
max
)
0, |V| âˆ’Ïƒ2
cacb
1
,
where sign(Â·) denotes the sign of a scalar (see Corollary 12.2 in Chapter 12
for derivation). In Figure 7.2, the asterisks indicate the MAP estimators, and
the dashed curves indicate the ML estimators (the modes of the contour of Eq.
(7.8) when ca = cb â†’âˆ). When V = 0, the Bayes posterior takes the maxi-
mum value on the A- and B-axes, which results in the MAP estimator equal to

UMAP(= BMAPAMAP) = 0. When V = 1, the proï¬le of the Bayes posterior
is hyperbolic and the maximum value is achieved on the hyperbolic curves in
the positive orthant (i.e., A, B > 0) and the negative orthant (i.e., A, B < 0);

7.2 Posteriors and Estimators in a One-Dimensional Case
189
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.3
0.3
0.3
A
B
Bayes posterior (V = 0)
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
MAP estimator:
(A, B) = (0, 0)
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.3
0.3
0.3
0.3
0.3
A
B
Bayes posterior (V = 1)
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
MAP estimators:
(A, B) â‰ˆ(Â± 1, Â± 1)
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.3
0.3
0.3
A
B
Bayes posterior (V = 2)
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
MAP estimators:
(A, B) â‰ˆ(Â±
âˆš
2, Â±
âˆš
2)
Figure 7.2 (Unnormalized) Bayes posteriors for ca = cb = 100 (i.e., almost ï¬‚at
priors). The asterisks are the MAP estimators, and the dashed curves indicate the
ML estimators (the modes of the contour when ca = cb = c â†’âˆ).
in either case, 
UMAP â‰ˆ1 (limca,cbâ†’âˆ
UMAP = 1). When V = 2, a similar
multimodal structure is observed and the MAP estimator is 
UMAP
â‰ˆ2
(limca,cbâ†’âˆ
UMAP = 2). From these plots, we can visually conï¬rm that the
MAP estimator with almost ï¬‚at priors (ca = cb = 100) approximately agrees
with the ML estimator: 
UMAP â‰ˆ
UML = V (limca,cbâ†’âˆ
UMAP = 
UML). We
will use the ML estimator as an unregularized reference in the following
discussion.
Figure 7.3 shows the contour of the Bayes posterior when ca = cb = 2.
The MAP estimators shift from the ML solutions (dashed curves) toward the
origin, and they are more clearly contoured as peaks.
VB Solution
Next we depict the VB posterior, given by Corollary 6.8 in Chapter 6. When
L = M = H = 1, the VB solution is given by

190
7 Model-Induced Regularization and Sparsity Inducing Mechanism
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
A
B
Bayes posterior (V = 0)
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.3
0.3
A
B
Bayes posterior (V = 1)
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
A
B
Bayes posterior (V = 2)
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
Figure 7.3 (Unnormalized) Bayes posteriors for ca = cb = 2. The dashed curves
indicating the ML estimators are identical to those in Figure 7.2.
r(A, B) =
â§âªâªâªâªâªâªâ¨âªâªâªâªâªâªâ©
Gauss1
!
A; Â±
.
Ë˜Î³VB ca
cb , Ïƒ2ca
|V|cb
"
Gauss1
!
B; Â±sign(V)
.
Ë˜Î³VB cb
ca , Ïƒ2cb
|V|ca
"
if |V| â‰¥Î³VB,
Gauss1

A; 0, c2
aÎºVB 
Gauss1

B; 0, c2
bÎºVB 
otherwise,
(7.9)
where
Î³VB = Ïƒ
A
1 +
Ïƒ2
2c2ac2
b +
B!
1 +
Ïƒ2
2c2ac2
b
"2
âˆ’1,
Ë˜Î³VB =

1 âˆ’Ïƒ2
V2
 
|V| âˆ’
Ïƒ2
cacb ,
ÎºVB = âˆ’Ïƒ2
2c2ac2
b +
B!
1 +
Ïƒ2
2c2ac2
b
"2
âˆ’1.
Figure 7.4 shows the contour of the VB posterior (7.9) when V = 0, 1, 2
are observed, the noise variance is Ïƒ2 = 1, and the prior covariances are

7.2 Posteriors and Estimators in a One-Dimensional Case
191
0.05
0.05
0.05
0.05
0.05
0.05
0.05
0.1
0.1
0.1
0.1
0.15
0.15
A
B
VB posterior (V = 0)
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
VB estimator : (A, B) = (0, 0)
0.05
0.05
0.05
0.05
0.05
0.05
0.05
0.1
0.1
0.1
0.1
0.15
0.15
A
B
VB posterior (V = 1)
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
VB estimator : (A, B) = (0, 0)
0.05
0.05
0.05
0.05
0.05
0.05
0.1
0.1
0.1
0.1
0.1
0.15
0.15
0.15
0.15
0.2
0.2
0.2
0.25
0.25
0.3
A
B
VB posterior (V = 2)
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
VB estimator :
(A, B) â‰ˆ(
âˆš
1.5,
âˆš
1.5)
0.05
0.05
0.05
0.05
0.05
0.05
0.1
0.1
0.1
0.1
0.1
0.15
0.15
0.15
0.15
0.2
0.2
0.2
0.25
0.25
0.3
A
B
VB posterior (V = 2)
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
VB estimator :
(A, B) â‰ˆ(âˆ’
âˆš
1.5, âˆ’
âˆš
1.5)
Figure 7.4 VB solutions for ca = cb = 100 (i.e., almost ï¬‚at priors). When V = 2,
VB learning gives either one of the two solutions shown in the bottom row.
set to ca = cb = 100 (i.e., almost ï¬‚at priors). When V = 0, the cross-
shaped contour of the Bayes posterior (see Figure 7.2) is approximated by
a spherical Gaussian distribution located at the origin. Thus, the VB estimator
is 
UVB = 0, which coincides with the MAP estimator. When V = 1, two
hyperbolic â€œmodesâ€ of the Bayes posterior are approximated again by a
spherical Gaussian distribution located at the origin. Thus, the VB estimator
is still 
UVB = 0, which differs from the MAP estimator 
UMAP â‰ˆ1.
V = Î³VB â‰ˆ
âˆš
MÏƒ2 = 1 (limca,cbâ†’âˆÎ³VB =
âˆš
MÏƒ2) is actually a transition
point of the VB solution. When V is not larger than the threshold Î³VB â‰ˆ1,
VB learning tries to approximate the two â€œmodesâ€ of the Bayes posterior by
the origin-centered Gaussian distribution. When V goes beyond the threshold
Î³VB â‰ˆ1, the â€œdistanceâ€ between two hyperbolic modes of the Bayes posterior
becomes so large that VB learning chooses to approximate one of those two
modes in the positive and the negative orthants. As such, the symmetry is

192
7 Model-Induced Regularization and Sparsity Inducing Mechanism
broken spontaneously and the VB estimator is detached from the origin. The
bottom row of Figure 7.4 shows the contour of the two possible VB posteriors
when V = 2. Note that the VB estimator, 
UVB â‰ˆ3/2, is the same for both
cases, and differs from the MAP estimator 
UMAP â‰ˆ2.
In general, the VB estimator is closer to the origin than the MAP estimator,
and the relative difference between them tends to shrink as V increases.
Bayesian Estimator
The full Bayesian estimator is deï¬ned as the mean of the Bayes posterior (see
Eq. (1.7)). In the MF model with L = M = H = 1, the Bayesian estimator is
expressed as

UBayes = âŸ¨BAâŸ©p(V|A,B)p(A)p(B)/p(V) .
(7.10)
If V = 0, 1, 2, 3 are observed, the Bayesian estimator with almost ï¬‚at priors are

UBayes = 0, 0.92, 1.93, 2.95, respectively, which were numerically computed.1
Compared with the MAP estimator (with almost ï¬‚at priors), which gives

UMAP = 0, 1, 2, 3, respectively, the Bayesian estimator is slightly shrunken.
EVB Solution
Next we consider the empirical Bayesian solutions, where the hyperparameters
ca, cb are also estimated from observation (the noise variance Ïƒ2 is still treated
as a given constant). We ï¬x the ratio between the prior variances to ca/cb = 1.
From Corollary 7.3 and Eq. (7.9), we obtain the EVB posterior for L = M =
H = 1 as follows:
r(A, B) =
â§âªâªâªâªâªâ¨âªâªâªâªâªâ©
Gauss1

A; Â±
C
Ë˜Î³EVB, Ïƒ2
|V|
 
Gauss1

B; Â±sign(V)
C
Ë˜Î³EVB, Ïƒ2
|V|
 
if |V| â‰¥Î³EVB,
Gauss1 (A; 0, +0) Gauss1 (B; 0, +0)
otherwise,
(7.11)
where
Î³EVB = Ïƒ
A
2 + Ï„(1) +
1
Ï„(1) â‰ˆÏƒ
B
2 + 2.5129 +
1
2.5129 â‰ˆ2.216Ïƒ,
Ë˜Î³EVB = |V|
2
â›âœâœâœâœâœâ1 âˆ’2Ïƒ2
V2 +
B
1 âˆ’4Ïƒ2
V2
ââŸâŸâŸâŸâŸâ .
1 More precisely, we numerically calculated the Bayesian estimator (7.10) by sampling A and B
from the almost ï¬‚at priors p(A)p(B) for ca = cb = 100 and computing the ratio between the
sample averages of BA Â· p(V|A, B) and p(V|A, B).

7.2 Posteriors and Estimators in a One-Dimensional Case
193
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
A
B
EVB posterior (V = 2)
EVB estimator : (A, B) = (0, 0)
0.1
0.1
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.4
0.4
A
B
EVB posterior (V = 3)
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
EVB estimator :
(A, B) â‰ˆ(
âˆš
2.28,
âˆš
2.28)
0.1
0.1
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.4
0.4
A
B
EVB posterior (V = 3)
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
EVB estimator :
(A, B) â‰ˆ(âˆ’
âˆš
2.28, âˆ’
âˆš
2.28)
Figure 7.5 EVB solutions. Top-left: When V = 2, the EVB posterior is reduced
to the Dirac delta function located at the origin. Top-right and bottom: When
V = 3, the EVB posterior is detached from the origin, and located at (A, B) â‰ˆ
(
âˆš
2.28,
âˆš
2.28) or (A, B) â‰ˆ(âˆ’
âˆš
2.28, âˆ’
âˆš
2.28), both of which yield the same
EVB estimator 
UEVB â‰ˆ2.28.
Figure 7.5 shows the EVB posterior when V = 2, 3 are observed, and the
noise variance is Ïƒ2 = 1. When V = 2 < Î³EVB, the EVB posterior is given by
the Dirac delta function located at the origin, resulting in the EVB estimator
equal to 
UEVB = 0 (top-left graph). On the other hand, when V = 3 > Î³EVB,
the EVB posterior is a Gaussian located in the top-right region or bottom-left
region, and the EVB estimator is 
UEVB â‰ˆ2.28 for both solutions (top-right and
bottom graphs).
Empirical Bayesian Estimator
The empirical Bayesian (EBayes) estimator (introduced in Section 1.2.7) is the
Bayesian estimator,

UEBayes = âŸ¨BAâŸ©p(V|A,B)p(A;ca)p(B;cb)/p(V;ca,cb) ,

194
7 Model-Induced Regularization and Sparsity Inducing Mechanism
with the hyperparameters estimated by minimizing the Bayes free energy
FBayes(V; ca, cb) â‰¡âˆ’log p(V; ca, cb), i.e.,
(ca,cb) = argmin
(ca,cb)
FBayes(V; ca, cb).
When V = 0, 1, 2, 3 are observed, the EBayes estimators are 0.00, 0.00, 1.25,
2.58 (with the prior variance estimators given by ca = cb â‰ˆ0.0, 0.0, 1.4, 2.1),
respectively, which were numerically computed.2
Behavior of Estimators
Figure 7.6 shows the behavior of estimators, including the MAP estimator

UMAP, the VB estimator 
UVB, the Bayesian estimator 
UBayes, the EVB
estimator 
UEVB, and the EBayes estimator 
UEBayes, when the noise variance
is Ïƒ2 = 1. For nonempirical Bayesian estimators, i.e., the MAP, the VB, and
the Bayesian estimators, the hyperparameters are set to ca = cb = 100 (i.e.,
almost ï¬‚at priors). Overall, the solutions satisfy

UEVB < 
UEBayes < 
UVB < 
UBayes < 
UMAP(â‰ˆ
UML),
0
1
2
3
0
0.5
1
1.5
2
2.5
3
Figure 7.6 Behavior of the MAP estimator 
UMAP, the VB estimator 
UVB, the
Bayesian estimator 
UBayes, the EVB estimator 
UEVB, and the EBayes estimator

UEBayes, when the noise variance is Ïƒ2 = 1. For the MAP, the VB, and the
Bayesian estimators, the hyperparameters are set to ca = cb = 100 (i.e., almost
ï¬‚at priors).
2 For cacb = 10âˆ’2.00, 10âˆ’1.99,. . . , 101.00, we numerically computed the Bayes free energy, and
chose its minimizercacb, with which the Bayesian estimator was computed.

7.3 Model-Induced Regularization
195
which shows the strength of the regularization effect of each method. Naturally,
the empirical Bayesian variants are more regularized than their nonempirical
Bayesian counterparts with almost ï¬‚at priors.
With almost ï¬‚at priors, the MAP estimator is almost identical to the ML
estimator, 
UMAP â‰ˆ
UML = V, meaning that it is unregularized. We see in
Figure 7.6 that the Bayesian estimator 
UBayes is regularized even with almost
ï¬‚at priors. Furthermore, the VB estimator 
UVB shows thresholding behavior,
which leads to exact sparsity in multidimensional cases. Exact sparsity also
appears in EVB learning and EBayes learning. In the subsequent sections, we
explain those observations in terms of model-induced regularization and phase
transitions.
7.3 Model-Induced Regularization
In this section, we explain the origin of the shrinkage of the Bayesian estimator,
observed in Section 7.2. The shrinkage is caused by an implicit regularization
effect, called model-induced regularization (MIR), which is strongly related to
unidentiï¬ability of statistical models.
7.3.1 Unidentiï¬able Models
Identiï¬ability is formally deï¬ned as follows:
Deï¬nition 7.4
(Identiï¬ability of statistical models) A statistical model p(Â·|w)
parameterized by w âˆˆW is said to be identiï¬able, if the mapping w â†’p(Â·|w)
is one-to-one, i.e.,
p(Â·|w1) = p(Â·|w2) â‡â‡’w1 = w2
for any w1, w2 âˆˆW.
Otherwise, it is said to be unidentiï¬able.3
Many popular statistical models are unidentiï¬able.
Example 7.5
The MF model (introduced in Section 3.1) is unidentiï¬able,
because the model distribution
p(V|A, B) âˆexp

âˆ’1
2Ïƒ2
###V âˆ’BAâŠ¤###2
Fro

(7.12)
3 Distributions are identiï¬ed in weak topology in distribution, i.e., p(x|w1) is identiï¬ed with
p(x|w2) if

f(x)p(x|w1)dx =

f(x)p(x|w2)dx for any bounded continuous function f(x).

196
7 Model-Induced Regularization and Sparsity Inducing Mechanism
is invariant to the following transformation (A, B) â†’(ATâŠ¤, BTâˆ’1) for any
nonsingular matrix T âˆˆRHÃ—H.
Example 7.6
The multilayer neural network model is unidentiï¬able.
Consider a three-layer neural network with H hidden units:
p(y|x, A, B) âˆexp

âˆ’1
2Ïƒ2 âˆ¥y âˆ’f(x; A, B)âˆ¥2

,
f(x; A, B) =
H

h=1
bh Â· Ïˆ

aâŠ¤
h x
 
,
(7.13)
where x âˆˆRM is an input vector, y âˆˆRL is an output vector, A = (a1,. . . , aH) âˆˆ
RMÃ—H and B = (b1,. . . , bH) âˆˆRLÃ—H are the weight parameters to be estimated,
and Ïˆ(Â·) is an antisymmetric nonlinear activation function such as tanh(Â·). This
model expresses the identical distribution on each of the following sets of
points in the parameter space:
{ah âˆˆRM, bh = 0} âˆª{ah = 0, bh âˆˆRL} for any h,
{ah = ahâ€², bh, bhâ€² âˆˆRL, bh + bhâ€² = const.} for any pair h, hâ€².
In other words, the model is invariant for any ah âˆˆRM if bh = 0, for any
bh âˆˆRL if ah = 0, and for any bh, bhâ€² âˆˆRL as long as bh + bhâ€² is unchanged
and ah = ahâ€².
Example 7.7
(Mixture models) The mixture model (introduced as Example
1.3 in Section 1.1.4) is generally unidentiï¬able. The model distribution is
given as
p(x|Î±, {Ï„k}) =
K

k=1
Î±kp(x|Ï„k),
(7.14)
where x âˆˆX is an observed random variable, and Î± = (Î±1,. . . , Î±K)âŠ¤âˆˆÎ”Kâˆ’1
and {Ï„k âˆˆT }K
k=1 are the parameters to be estimated. This model expresses the
identical distribution on each of the following sets of points in the parameter
space:
{Î±k = 0, Ï„k âˆˆT }
for any k,
{Î±k, Î±kâ€² âˆˆ[0, 1], Î±k + Î±kâ€² = const., Ï„k = Ï„kâ€²} for any pair k, kâ€².
Namely, if the mixing weight Î±k is zero for the kth mixture component, the
corresponding component parameter Ï„k does not affect the model distribution,
and if there are two identical components Ï„k = Ï„kâ€², the balance between the
corresponding mixture weights, Î±k and Î±kâ€², are arbitrary.

7.3 Model-Induced Regularization
197
Readers might have noticed that, in the multilayer neural network (Example
7.6) and the mixture model (Example 7.7), the model expressed by the
unidentiï¬able sets of points corresponds to the model with fewer components
or smaller degrees of freedom. For example, if ah = 0 or bh = 0 in the neural
network with H hidden units, the model is reduced to the neural network
with H âˆ’1 hidden units. If two hidden units receive the identical input, i.e.,
Ïˆ

aâŠ¤
h x
 
= Ïˆ

aâŠ¤
hâ€²x
 
for any x âˆˆRM, they can be combined into a single
unit with its output weight equal to the sum of the original output weights,
i.e., bh + bhâ€² â†’bh. Thus, the model is again reduced to the neural network
with H âˆ’1 hidden units. The same applies to the mixture models and many
other popular statistical models, including Bayesian networks, hidden Markov
models, and latent Dirichlet allocation, which were introduced in Chapter 4. As
will be explained shortly, this nesting structureâ€”simpler models correspond to
unidentiï¬able sets of points in the parameter space of more complex modelsâ€”
is essential for MIR.
7.3.2 Singularities
Continuous points denoting the same distribution are called singularities, on
which the Fisher information,
SD
+ âˆ‹F =
 âˆ‚log p(x|w)
âˆ‚w
âˆ‚log p(x|w)
âˆ‚w
âŠ¤
p(x|w)dx,
(7.15)
is singular, i.e., it has at least one zero eigenvalue. This is a natural conse-
quence from the fact that the Fisher information corresponds to the metric
when the distance between two points in the parameter space is measured by
the KL divergence (Jeffreys, 1946), i.e., it holds that
KL (p(x|w)âˆ¥p(x|w + Î”w)) = 1
2Î”wâŠ¤FÎ”w + O(âˆ¥Î”wâˆ¥3)
for a small change Î”w of the parameter. On the singularities, there is at least
one direction in which the small change Î”w does not affect the distribution,
implying that the Fisher metric F is singular. This means that the volume
element, proportional to the determinant of the Fisher metric, is zero on the
singularities, while it is positive on the regular points (see Appendix B for
more details on the Fisher metric and the volume element in the parameter
space).
This strong nonuniformity of (the density of) the volume element affects the
behavior of Bayesian learning. For this reason, statistical models having singu-
larities in their parameter space are called singular models and distinguished

198
7 Model-Induced Regularization and Sparsity Inducing Mechanism
Figure 7.7 Singularities of a neural network model.
from the regular models in statistical learning theory (Watanabe, 2009). There
are two aspects of how singularities affect the learning properties. In this
chapter, we focus on one aspect that leads to MIR. The other aspect will be
discussed in Chapter 13.
Figure 7.7 illustrates the singularities in the parameter space of the three-
layer neural network (7.13) with H = 1 hidden unit (see Example 7.6).
The horizontal axis corresponds to an arbitrary direction of ah âˆˆRM, while
the vertical axis corresponds to an arbitrary direction of bh
âˆˆRL. The
shadowed locations correspond to the singularities. Importantly, all points on
the singularities express the identical neural network model with no (H = 0)
hidden unit, while each regular point expresses a different neural network
model with H = 1 hidden unit. This illustration gives an intuition that the
neighborhood of the smaller model (H = 0) is broader than the neighborhood
of the larger model (H = 1) in the parameter space.
Consider the Jeffreys prior,
pJef(w) âˆ
C
det (F),
(7.16)
which is the uniform prior in the space of distributions when the distance is
measured by the KL divergence (see Appendix B). As discussed previously,
the Fisher information is singular on the singularities, giving pJef(w) = 0
for the smaller model (with H = 0), while the Fisher information is regular
on the other points, giving pJef(w) > 0 for the larger model (with H = 1). Also
in the neighborhood of the singularities, the Fisher information has similar
values and it holds that pJef(w) â‰ª1. This means that, in comparison with the

7.3 Model-Induced Regularization
199
Jeffreys prior, the ï¬‚at priors on ah and bhâ€”the uniform prior in the parameter
spaceâ€”put much more mass to the smaller model and its neighborhood.
A consequence is that, if we apply Bayesian learning with the ï¬‚at prior,
the overweighted singularities and their neighborhood pull the estimator to
the smaller model through the integral computation, which induces implicit
regularizationâ€”MIR. The same argument holds for mixture models (Example
7.6), and other popular models, including Bayesian networks, hidden Markov
models, and latent Dirichlet allocation.
In summary, MIR occurs in general singular models for the following
reasons:
â€¢ There is strong nonuniformity in (the density of) the volume element
around the singularities.
â€¢ Singularities correspond to the model with fewer degrees of freedom than
the regular points.
This structure in the parameter space makes the ï¬‚at prior favor smaller models
in Bayesian learning, which appears as MIR. Note that MIR does not occur in
point-estimation methods, including ML estimation and MAP learning, since
the nonuniformity of the volume element affects the estimator only through
integral computations.
MIR also occurs in the MF model (Example 7.6), which will be investigated
in the next subsection with a generalization of the Jeffreys prior.
7.3.3 MIR in one-Dimensional Matrix Factorization
In Section 7.2, we numerically observed MIRâ€”the Bayesian estimator is
shrunken even with the almost ï¬‚at prior in the one-dimensional MF model.
However, in the MF model, the original deï¬nition (7.16) of the Jeffreys
prior is zero everywhere in the parameter space because of the equivalence
class structure (Figure 7.1), and therefore, it provides no information on
MIR. To evaluate the nonuniformity of the volume element, we redeï¬ne the
(generalized) Jeffreys prior by ignoring the zero common eigenvalues, i.e.,
pJef(w) âˆ
.D
d=1 Î»d,
(7.17)
where Î»d is the dth largest eigenvalue of the Fisher metric F, and D is the
maximum number of positive eigenvalues over the whole parameter space.
Let us consider the nonfactorizing model,
p(V|U) = Gauss1(V; U, Ïƒ2) âˆexp

âˆ’1
2Ïƒ2 (V âˆ’U)2

,
(7.18)

200
7 Model-Induced Regularization and Sparsity Inducing Mechanism
0.1
0.1
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.3
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.5
0.5
0.5
0.5
A
B
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
Figure 7.8 The (unnormalized) Jeffreys noninformative prior (7.20) of the one-
dimensional MF model (7.5).
where U itself is the parameter to be estimated. The Jeffreys prior for this
model is uniform (see Example B.1 in Appendix B for derivation):
pJef(U) âˆ1.
(7.19)
On the other hand, the Jeffreys prior for the MF model (7.5) is given as follows
(see Example B.2 in Appendix B for derivation):
pJef(A, B) âˆ
âˆš
A2 + B2,
(7.20)
which is illustrated in Figure 7.8. Note that the Jeffreys priors (7.19) and (7.20)
for both cases are improper, meaning that they cannot be normalized since their
integrals diverge.
Jeffreys (1946) stated that the both combinations, the nonfactorizing model
(7.18) with its Jeffreys prior (7.19) and the MF model (7.5) with its Jeffreys
prior (7.20) give the equivalent Bayesian estimator. We can easily show that the
former combination, Eqs. (7.18) and (7.19), gives an unregularized solution.
Thus, the Bayesian estimator in the MF model (7.5) with its Jeffreys prior
(7.20) is also unregularized. Since the ï¬‚at prior on (A, B) has more probability
mass around the origin than the Jeffreys prior (7.20) (see Figure 7.8), it favors
smaller |U| and regularizes the Bayesian estimator.
Although MIR appears also in regular models unless the Jeffreys prior is
ï¬‚at in the parameter space, its effect is prominent in singular models with
unidentiï¬ability, since the difference between the ï¬‚at prior and the Jeffreys
prior is large.

7.3 Model-Induced Regularization
201
7.3.4 Evidence View of Unidentiï¬able Models
MIR works as Occamâ€™s razor in general. MacKay (1992) explained, with the
illustration shown in the left panel of Figure 7.9, that evidence-based (i.e.,
free-energy-minimization-based) model selection is naturally equipped with
Occamâ€™s razor. In the ï¬gure, the horizontal axis denotes the space of the
observed data set D. H1 and H2 denote a simple hypothesis and a more
complex hypothesis, respectively. For example, in the MF model, the observed
data set corresponds to the observed matrix, i.e., D = V, H1 corresponds to
a lower-rank model, and H2 corresponds to a higher-rank model. The vertical
axis indicates the evidence or marginal likelihood,
p(D|Ht) = p(D|wHt, Ht)
p(wHt )
for
t = 1, 2,
(7.21)
where Î¸Ht denotes the unknown parameters that the hypothesis Ht has.
Since H1 is simple, it covers a limited area of the space of D (meaning that
it can explain only a simple phenomenon), while H2 covers a broader area.
The illustration implies that, because of the normalization, it holds that
p(D|H1) > p(D|H2)
for
D âˆˆC1,
where C1 denotes the observed data region where H1 can explain the data well.
This view gives an intuition on why evidence-based model selection prefers
simpler models when the observed data can be well explained by them.
However, this view does not explain MIR, which is observed even without
any model selection procedure. In fact, the illustration in the left panel of
Figure 7.9 is not accurate for unidentiï¬able models unless the Jeffreys prior
is adopted (note that a hypothesis consists of a model and a prior). The right
illustration of Figure 7.9 is a more accurate view for unidentiï¬able models.
When H2 is a complex unidentiï¬able model nesting H1 as a simpler model in
Figure 7.9 Left: The evidence view by MacKay (1992), which gives an intuition
on why evidence-based model selection prefers simpler models. Right: A more
accurate view for unidentiï¬able models. Simpler models are preferred even
without explicit model selection.

202
7 Model-Induced Regularization and Sparsity Inducing Mechanism
its parameter space, its evidence p(D|H2) has a bump covering the region C1
if the ï¬‚at prior is adopted. This is because the ï¬‚at prior typically places large
weights on the singularities representing the simpler model H1.
7.4 Phase Transition in VB Learning
In Section 7.3, we explained MIR, which shrinks the Bayesian estimator. We
can expect that VB learning, which involves integral computations, inherits
this property. However, we observe in Figure 7.6 that VB learning behaves
differently from Bayesian learning. Actually, the Bayesian estimator behaves
more similarly to the ML estimator (the MAP estimator with almost ï¬‚at priors),
rather than the VB estimator. A remarkable difference is that the VB estimator,
which is upper-bounded by the PJS estimator (7.2), shows exact sparsity,
i.e., the estimator can be zero for nonzero observation |V|. In this section,
we explain that this gap is caused by a phase transition phenomenon in VB
learning.
The middle graph in Figure 7.2 shows the Bayes posterior when V = 1.
The probability mass in the ï¬rst and the third quadrants pulls the product
U = BA toward the positive direction, and the mass in the second and the fourth
quadrants toward the negative direction. Since the Bayes posterior is skewed
and more mass is placed in the ï¬rst and the third quadrants, the Bayesian
estimator 
UBayes = âŸ¨BAâŸ©p(A,B|V) is positive. This is true even if V > 0 is
very small, and therefore, no thresholding occurs in Bayesian learningâ€”the
Bayesian estimator is not sparse.
On the other hand, the VB posterior (the top-right graph of Figure 7.4) is
prohibited to be skewed because of the independent constraint, which causes
the following phase transition phenomenon. As seen in Figure 7.2, the Bayes
posterior has two modes unless V = 0, and the distance between the two
modes increases as |V| increases. Since the VB posterior tries to approximate
the Bayes posterior with a single uncorrelated distribution, it stays at the origin
if the two modes are close to each other so that covering both modes minimizes
the free energy. The VB posterior detaches from the origin if the two modes
get far apart so that approximating either one of the modes minimizes the
free energy. This phase transition mechanism makes the VB estimator exactly
sparse. The proï¬le of the Bayes posterior (the middle graph of Figure 7.2)
implies that, if we restrict the posterior to be Gaussian, but allow it to have
correlation between A and B, exact sparsity will not appear. In this sense,
we can say that MIR is enhanced by the independence constraint, which was
imposed for computational tractability.

7.4 Phase Transition in VB Learning
203
Mackay (2001) pointed out that there are cases where VB learning prunes
model components inappropriately, by giving a toy example of a mixture of
Gaussians. Note that appropriateness was measured in terms of the similarity
to full Bayesian learning. He plotted the free energy of the mixture of
Gaussians as a function of hidden responsibility variablesâ€”the probabilities
that each sample belongs to each Gaussian componentâ€”and argued that VB
learning sometimes favors simpler models too much. In this case, degrees of
freedom are pruned when spontaneous symmetry breaking (a phase transition)
occurs. Interestingly, in the MF model, degrees of freedom are pruned when
spontaneous symmetry breaking does not occur, as explained earlier.
Eq. (7.3) implies that the symmetry breaking occurs when V > Î³VB
h
â‰ˆ
âˆš
MÏƒ2 = 1, which coincides with the average contribution of noise to the
observed singular values over all singular componentsâ€”more accurately,
âˆš
MÏƒ2 is the square root of the average eigenvalues of the Wishart matrix
EEâŠ¤
âˆ¼WishartL(Ïƒ2IL, M).4 In this way, VB learning discards singular
components dominated by noise.
Given that the full Bayesian estimator in MF is not sparse (see Figure 7.6),
one might argue that the sparsity of VB learning is an inappropriate artifact. On
the other hand, given that automatic model pruning by VB learning has been
acknowledged as a practically useful property (Bishop, 1999b; Bishop and
Tipping, 2000; Sato et al., 2004; Babacan et al., 2012b), one might also argue
that appropriateness should be measured in terms of performance. Motivated
by the latter idea, performance analysis has been carried out (Nakajima et al.,
2015), which will be detailed in Chapter 8.
In the empirical Bayesian scenario, where the prior variances ca, cb are also
estimated from observation, Bayesian learning also gives a sparse solution,
which is shown as diamonds (labeled as â€œEBayesâ€) in Figure 7.6. This
is somewhat natural since, in empirical Bayesian learning, the dependency
between A and câˆ’2
a (as well as B and câˆ’2
b ) in the prior (7.6) (in the prior (7.7))
and hence in the Bayes posterior is brokenâ€”the point-estimation of c2
a (as
well as c2
a) forces it to be independent of all other parameters. This forced
independence causes a similar phase transition phenomenon to the one caused
by the independence constraint between A and B in the (nonempirical) VB
learning, and results in exact sparsity of the EBayes estimator.
EVB learning has a different transition point, and tends to give a sparser
solution than VB learning. A notable difference from the VB estimator is that
the EVB estimator is no longer continuous as a function of the observation V.
This comes from the fact that, when |V| > Î³localâˆ’EVB, there exist two local
4 It holds that EEâŠ¤âˆ¼WishartL(Ïƒ2IL, M) if E âˆ¼GaussL(0, Ïƒ2IL).

204
7 Model-Induced Regularization and Sparsity Inducing Mechanism
solutions (see Figure 6.3), but the global solution is 
UEVB = 0 until the
observed amplitude |V| exceeds Î³EVB(> Î³localâˆ’EVB). When the positive local
solution Ë˜Î³EVB becomes the global solution, it is already distant from the origin,
which makes the estimator noncontinuous at the thresholding point (see the
dashed curve labeled as â€œEVBâ€ in Figure 7.6).
7.5 Factorization as ARD Model
As shown in Section 7.1, MIR in VB learning for the MF model appears as
PJS shrinkage. We can see this as a natural consequence from the equivalence
between the MF model and the ARD model (Neal, 1996).
Assume that CA = IH in the MF model (6.1) through (6.3), and consider the
following transformation: BAâŠ¤â†’U âˆˆRLÃ—M. Then, the likelihood (6.1) and
the prior (6.2) on A become
p(V|U) âˆexp

âˆ’1
2Ïƒ2 âˆ¥V âˆ’Uâˆ¥2
Fro

,
(7.22)
p(U|B) âˆexp

âˆ’1
2tr

UâŠ¤(BBâŠ¤)â€ U
 
,
(7.23)
where â€  denotes the Mooreâ€“Penrose generalized inverse of a matrix. The
prior (6.3) on B is kept unchanged. p(U|B) in Eq. (7.23) is so-called the
ARD prior with the covariance hyperparameter BBâŠ¤âˆˆRLÃ—L. It is known
that this prior induces the ARD propertyâ€”empirical Bayesian learning, where
the prior covariance hyperparameter BBâŠ¤is estimated from observation by
maximizing the marginal likelihood (or minimizing the free energy), induces
strong regularization and sparsity (Neal, 1996). Efron and Morris (1973)
showed that this particular model gives the JS shrinkage estimator as an
empirical Bayesian estimator (see Appendix A).
This equivalence can explain the sparsity-inducing terms (3.113) through
(3.116), introduced for sparse additive matrix factorization (SAMF) in Section
3.5. The ARD prior (7.23) induces low-rankness on U if no restriction on
BBâŠ¤is imposed. We can similarly show that, (Î³e
l )2 in Eq. (3.114) corresponds
to the prior variance shared by the entries in ul
â‰¡
Î³e
ldl
âˆˆ
RM, that
(Î³d
m)2 in Eq. (3.115) corresponds to the prior variance shared by the entries
in um
â‰¡Î³d
mem
âˆˆRL, and that E2
l,m in Eq. (3.116) corresponds to the
prior variance on Ul,m â‰¡El,mDl,m âˆˆR, respectively. This explains why the
factorization forms in Eqs. (3.113) through (3.116) induce low-rank, rowwise,
columnwise, and elementwise sparsity, respectively. If we employ the sparse
matrix factorization (SMF) term (3.117), ARD occurs in each partition, which
induces partitionwise sparsity and low-rank sparsity within each partition.

8
Performance Analysis of VB Matrix
Factorization
In this chapter, we further analyze the behavior of VB learning in the fully
observed MF model, introduced in Section 3.1. Then, we derive a theoretical
guarantee for rank estimation (Nakajima et al., 2015), which corresponds to
the hidden dimensionality selection in principal component analysis (PCA).
In Chapter 6, we derived an analytic-form solution (Theorem 6.13) of
EVB learning, where the prior variances are also estimated from observation.
When discussing the dimensionality selection performance in PCA, it is
more practical to assume that the noise variance Ïƒ2 is estimated, since it is
unknown in many situations. To this end, we ï¬rst analyze the behavior of the
noise variance estimator. After that, based on the random matrix theory, we
derive a theoretical guarantee of dimensionality selection performance, and
show numerical results validating the theory. We also discuss the relation to
an alternative dimensionality selection method (Hoyle, 2008) based on the
Laplace approximation.
In the following analysis, we use some results in Chapter 6. Speciï¬cally,
we mostly rely on Theorem 6.13 along with the corollaries and the equations
summarized in Section 6.10.
8.1 Objective Function for Noise Variance Estimation
Let us consider the complete empirical VB problem, where all the variational
parameters and the hyperparameters are estimated in the free energy minimiza-
tion framework:
min
{ah,bh,Ïƒ2ah,Ïƒ2
bh,c2ah,c2
bh}H
h=1,Ïƒ2 F
(8.1)
s.t.
{ah,bh âˆˆR,
Ïƒ2
ah, Ïƒ2
bh, c2
ah, c2
bh âˆˆR++}H
h=1, Ïƒ2 âˆˆR++.
205

206
8 Performance Analysis of VB Matrix Factorization
Here, the free energy is given by
2F = LM log(2Ï€Ïƒ2) +
L
h=1 Î³2
h
Ïƒ2
+
H

h=1
2Fh,
(8.2)
where
2Fh = M log
c2
ah
Ïƒ2ah
+ L log
c2
bh
Ïƒ2
bh
+
a2
h + MÏƒ2
ah
c2ah
+
b2
h + LÏƒ2
bh
c2
bh
âˆ’(L + M) +
âˆ’2ahbhÎ³h +

a2
h + MÏƒ2
ah
 b2
h + LÏƒ2
bh
 
Ïƒ2
.
(8.3)
Note that we are focusing on the solution with diagonal posterior covariances
without loss of generality (see Theorem 6.4).
We have already obtained the empirical VB estimator (Theorem 6.13) and
the minimum free energy (Corollary 6.21) for given Ïƒ2. By using those results,
we can express the free energy (8.2) as a function of Ïƒ2. With the rescaled
expressions (6.132) through (6.138), the free energy can be written in a simple
form, which leads to the following theorem:
Theorem 8.1
The noise variance estimator, denoted by Ïƒ2 EVB, is the global
minimizer of
Î©(Ïƒâˆ’2)

â‰¡2F(Ïƒâˆ’2)
LM
+ const.

= 1
L
â›âœâœâœâœâœâ
H

h=1
Ïˆ
â›âœâœâœâœâ
Î³2
h
MÏƒ2
ââŸâŸâŸâŸâ +
L

h=H+1
Ïˆ0
â›âœâœâœâœâ
Î³2
h
MÏƒ2
ââŸâŸâŸâŸâ 
ââŸâŸâŸâŸâŸâ , (8.4)
where
Ïˆ (x) = Ïˆ0 (x) + Î¸

x > x
 
Ïˆ1 (x) ,
(8.5)
Ïˆ0 (x) = x âˆ’log x,
(8.6)
Ïˆ1 (x) = log (Ï„(x; Î±) + 1) + Î± log
Ï„(x; Î±)
Î±
+ 1

âˆ’Ï„(x; Î±).
(8.7)
Here, x is given by
x =

1 + Ï„
 
1 + Î±
Ï„

,
(8.8)
where Ï„ is deï¬ned in Theorem 6.13, Ï„(x; Î±) is a function of x (> x) deï¬ned by
Ï„(x; Î±) = 1
2

x âˆ’(1 + Î±) +
.
(x âˆ’(1 + Î±))2 âˆ’4Î±

,
(8.9)
and Î¸(Â·) denotes the indicator function such that Î¸(condition) = 1 if the
condition is true and Î¸(condition) = 0 otherwise.

8.2 Bounds of Noise Variance Estimator
207
Proof
By using Lemma 6.14 and Lemma 6.16, the free energy (8.2) can be
written as a function of Ïƒ2 as follows:
2F = LM log(2Ï€Ïƒ2) +
L
h=1 Î³2
h
Ïƒ2
+
H

h=1
Î¸

Î³h > Î³EVB 
FEVBâˆ’Posi
h
,
(8.10)
where FEVBâˆ’Posi
h
is given by Eq. (6.112). By using Eqs. (6.133) and (6.135),
Eq. (6.112) can be written as
FEVBâˆ’Posi
h
= M log (Ï„h + 1) + L log
!Ï„h
Î± + 1
"
âˆ’MÏ„h
= MÏˆ1(xh).
(8.11)
Therefore, Eq. (8.10) is written as
2F = M
)
L

h=1
log
â›âœâœâœâœâ
2Ï€Î³2
h
M
ââŸâŸâŸâŸâ +
L

h=1
â›âœâœâœâœâlog
â›âœâœâœâœâ
MÏƒ2
Î³2
h
ââŸâŸâŸâŸâ +
Î³2
h
MÏƒ2
ââŸâŸâŸâŸâ 
+
H

h=1
Î¸

Î³h > Î³EVB FEVBâˆ’Posi
h
M
1
= M
)
L

h=1
log
â›âœâœâœâœâ
2Ï€Î³2
h
M
ââŸâŸâŸâŸâ +
L

h=1
Ïˆ0(xh) +
H

h=1
Î¸

xh > x
 
Ïˆ1(xh)
1
.
Note that the ï¬rst term in the curly braces is constant with respect to Ïƒ2. By
deï¬ning
Î© = 2F
LM âˆ’1
L
L

h=1
log
â›âœâœâœâœâ
2Ï€Î³2
h
M
ââŸâŸâŸâŸâ ,
we obtain Eq. (8.4), which completes the proof of Theorem 8.1.
â–¡
The functions Ïˆ0 (x) and Ïˆ (x) are depicted in Figure 8.1. We can conï¬rm
the convexity of Ïˆ0 (x) and the quasiconvexity of Ïˆ (x) (Lemma 8.4 in
Section 8.3), which are useful properties in the subsequent analysis.1
8.2 Bounds of Noise Variance Estimator
Let 
HEVB be the estimated rank by EVB learning, i.e., the rank of the EVB
estimator U
EVB, such that Î³EVB
h
> 0 for h = 1,. . . , 
HEVB, and Î³EVB
h
= 0 for
1 A function f : X â†’R on the domain X being a convex subset of a real vector space is said to
be quasiconvex if f(Î»x1 + (1 âˆ’Î»)x2) â‰¤max( f(x1), f(x2)) for all x1, x2 âˆˆX and Î» âˆˆ[0, 1]. It is
furthermore said to be strictly quasiconxex if f(Î»x1 + (1 âˆ’Î»)x2) < max( f(x1), f(x2)) for all
x1  x2 and Î» âˆˆ(0, 1). Intuitively, a strictly quasiconvex function does not have more than one
local minima.

208
8 Performance Analysis of VB Matrix Factorization
0
2
4
6
8
0
2
4
6
Figure 8.1 Ïˆ0(x) and Ïˆ(x).
h = 
HEVB + 1,. . . , H. By further analyzing the objective (8.4), we can derive
bounds of the estimated rank and the noise variance estimator:
Theorem 8.2

HEVB is upper-bounded as

HEVB â‰¤H = min
!D
L
1 + Î±
E
âˆ’1, H
"
,
(8.12)
and the noise variance estimator Ïƒ2 EVB is bounded as follows:
max
â›âœâœâœâœâœâœâÏƒ2
H+1,
L
h=H+1 Î³2
h
M

L âˆ’H
 
ââŸâŸâŸâŸâŸâŸâ â‰¤Ïƒ2 EVB â‰¤
1
LM
L

h=1
Î³2
h,
(8.13)
where
Ïƒ2
h =
â§âªâªâªâªâªâªâ¨âªâªâªâªâªâªâ©
âˆ
for h = 0,
Î³2
h
Mx
for h = 1,. . . , L,
0
for h = L + 1.
(8.14)
Theorem 8.2 states that EVB learning discards the (L âˆ’âŒˆL/(1 + Î±)âŒ‰+ 1)
smallest components, regardless of the observed singular values {Î³h}L
h=1. For
example, half of the components are always discarded when the matrix is
square (i.e., Î± = L/M = 1). The smallest singular value Î³L is always discarded,
and Ïƒ2 EVB â‰¥Î³2
L/M always holds.
Given the EVB estimators {Î³EVB
h
}H
h=1 for the singular values, the noise
variance estimator Ïƒ2 EVB is speciï¬ed by the following corollary:
Corollary 8.3
The EVB estimator for the noise variance satisï¬es the
following equality:
Ïƒ2 EVB =
1
LM
â›âœâœâœâœâœâ
L

l=1
Î³2
l âˆ’
H

h=1
Î³hÎ³EVB
h
ââŸâŸâŸâŸâŸâ .
(8.15)

8.3 Proofs of Theorem 8.2 and Corollary 8.3
209
This corollary can be used for implementing a global EVB solver (see
Chapter 9). In the next section we give the proofs of the theorem and the
corollary.
8.3 Proofs of Theorem 8.2 and Corollary 8.3
First, we show nice properties of the functions, Ïˆ (x) and Ïˆ0 (x), which are
deï¬ned by Eqs. (8.5) and (8.6), respectively, and depicted in Figure 8.1:
Lemma 8.4
The following hold for x > 0: Ïˆ0 (x) is differentiable and strictly
convex; Ïˆ (x) is continuous and strictly quasiconvex; Ïˆ (x) is differentiable
except x = x, at which Ïˆ (x) has a discontinuously decreasing derivative, i.e.,
limxâ†’xâˆ’0 âˆ‚Ïˆ/âˆ‚x > limxâ†’x+0 âˆ‚Ïˆ/âˆ‚x; both of Ïˆ0 (x) and Ïˆ (x) are minimized at
x = 1. For x > x, Ïˆ1 (x) is negative and decreasing.
Proof
Since
âˆ‚Ïˆ0
âˆ‚x = 1 âˆ’1
x,
(8.16)
âˆ‚2Ïˆ0
âˆ‚x2 = 1
x2 > 0,
Ïˆ0(x) is differentiable and strictly convex for x > 0 with its minimizer at x = 1.
Ïˆ1(x) is continuous for x â‰¥x, and Eq. (8.11) implies that Ïˆ1(xh) âˆFEVBâˆ’Posi
h
.
Accordingly, Ïˆ1(x) â‰¤0 for x â‰¥x, where the equality holds when x = x. This
equality implies that Ïˆ(x) is continuous. Since x > 1, Ïˆ(x) shares the same
minimizer as Ïˆ0(x) at x = 1 (see Figure 8.1).
Hereafter, we investigate Ïˆ1(x) and Ïˆ(x) for x â‰¥x. By differentiating
Eqs. (8.7) and (6.135), respectively, we have
âˆ‚Ïˆ1
âˆ‚Ï„ = âˆ’
â›âœâœâœâœâœâœâ
Ï„2
Î± âˆ’1
(Ï„ + 1)
 Ï„
Î± + 1
 
ââŸâŸâŸâŸâŸâŸâ < 0,
(8.17)
âˆ‚Ï„
âˆ‚x = 1
2
â›âœâœâœâœâœâœâœâœâœâœâ
1 +
x âˆ’(1 + Î±)
.
(x âˆ’(1 + Î±))2 âˆ’4Î±
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
> 0.
(8.18)
Substituting Eq. (6.134) into Eq. (8.18), we have
âˆ‚Ï„
âˆ‚x =
Ï„2
Î±
 Ï„2
Î± âˆ’1
 .
(8.19)

210
8 Performance Analysis of VB Matrix Factorization
Multiplying Eqs. (8.17) and (8.19) gives
âˆ‚Ïˆ1
âˆ‚x = âˆ‚Ïˆ1
âˆ‚Ï„
âˆ‚Ï„
âˆ‚x = âˆ’
â›âœâœâœâœâœâœâ
Ï„2
Î± (Ï„ + 1)
 Ï„
Î± + 1
 
ââŸâŸâŸâŸâŸâŸâ = âˆ’Ï„
x < 0,
(8.20)
which implies that Ïˆ1(x) is decreasing for x > x.
Let us focus on the thresholding point of Ïˆ(x) at x = x. Eq. (8.20) does not
converge to zero for x â†’x + 0 but stay negative. On the other hand, Ïˆ0(x) is
differentiable at x = x. Consequently, Ïˆ (x) has a discontinuously decreasing
derivative, i.e., limxâ†’xâˆ’0 âˆ‚Ïˆ/âˆ‚x > limxâ†’x+0 âˆ‚Ïˆ/âˆ‚x, at x = x.
Finally, we prove the strict quasiconvexity of Ïˆ(x). Taking the sum of
Eqs. (8.16) and (8.20) gives
âˆ‚Ïˆ
âˆ‚x = âˆ‚Ïˆ0
âˆ‚x + âˆ‚Ïˆ1
âˆ‚x = 1 âˆ’1 + Ï„
x
= 1 âˆ’
1 + Ï„
1 + Ï„ + Î± + Î±Ï„âˆ’1 > 0.
This means that Ïˆ(x) is increasing for x > x. Since Ïˆ0(x) is strictly convex and
increasing at x = x, and Ïˆ(x) is continuous, Ïˆ(x) is strictly quasiconvex. This
completes the proof of Lemma 8.4.
â–¡
Lemma 8.4 implies that our objective (8.4) is a sum of quasiconvex
functions with respect to Ïƒâˆ’2. Therefore, its minimizer can be bounded by
the smallest one and the largest one among the set collecting the minimizer
from each quasiconvex function:
Lemma 8.5
Î©(Ïƒâˆ’2) has at least one global minimizer, and any of its local
minimizers is bounded as
M
Î³2
1
â‰¤Ïƒâˆ’2 â‰¤M
Î³2
L
.
(8.21)
Proof
The strict convexity of Ïˆ0(x) and the strict quasiconvexity of Ïˆ(x) also
hold for Ïˆ0(Î³2
hÏƒâˆ’2/M) and Ïˆ(Î³2
hÏƒâˆ’2/M) as functions of Ïƒâˆ’2 (for Î³h > 0).
Because of the different scale factor Î³2
h/M for each h = 1. . . , L, each of
Ïˆ0(Î³2
hÏƒâˆ’2/M) and Ïˆ(Î³2
hÏƒâˆ’2/M) has a minimizer at a different position:
Ïƒâˆ’2 = M
Î³2
h
.
The strict quasiconvexity of Ïˆ0 and Ïˆ guarantees that Î©(Ïƒâˆ’2) is decreasing for
0 < Ïƒâˆ’2 < M
Î³2
1
,
(8.22)
and increasing for
M
Î³2
L
< Ïƒâˆ’2 < âˆ.
(8.23)
This proves Lemma 8.5.
â–¡

8.3 Proofs of Theorem 8.2 and Corollary 8.3
211
Î©(Ïƒâˆ’2) has at most H nondifferentiable points, which come from the
nondifferentiable point x = x of Ïˆ(x). The values
Ïƒâˆ’2
h =
â§âªâªâªâªâªâªâ¨âªâªâªâªâªâªâ©
0
for h = 0,
Mx
Î³2
h
for h = 1,. . . , L,
âˆ
for h = L + 1,
(8.24)
deï¬ned in Eq. (8.14) for h = 1,. . . , H actually correspond to those points.
Lemma 8.4 states that, at x = x, Ïˆ(x) has a discontinuously decreasing
derivative and neither Ïˆ0(x) nor Ïˆ(x) has a discontinuously increasing deriva-
tive at any point. Therefore, none of those nondifferentiable points can be a
local minimum. Consequently, we have the following lemma:
Lemma 8.6
Î©(Ïƒâˆ’2) has no local minimizer at Ïƒâˆ’2 = Ïƒâˆ’2
h for h = 1,. . . , H,
and therefore any of its local minimizers is a stationary point.
Then, Theorem 6.13 leads to the following lemma:
Lemma 8.7
The estimated rank is 
H = h if and only if the inverse noise
variance estimator lies in the range
Ïƒâˆ’2 âˆˆBh â‰¡
,
Ïƒâˆ’2; Ïƒâˆ’2
h < Ïƒâˆ’2 < Ïƒâˆ’2
h+1
-
.
(8.25)
Figure 8.2 shows quasiconvex functions {Ïˆ(Î³2
hÏƒâˆ’2/M)}H
h=1 and their average
Î©(Ïƒâˆ’2) in two exemplary cases for H = L. In the left case, the inverse noise
variance estimator Ïƒâˆ’2 is smaller than the inverse threshold Ïƒâˆ’2
1 for the largest
singular value, and therefore no EVB estimator Î³h is positive, i.e., 
H = 0. In
the right case, it holds that Ïƒâˆ’2
1
< Ïƒâˆ’2 < Ïƒâˆ’2
2 , and therefore Î³1 is positive and
the others are zero, i.e., 
H = 1.
We have the following lemma:
Figure 8.2 {Ïˆ(Î³2
hÏƒâˆ’2/M)}H
h=1 and Î©(Ïƒâˆ’2) in two examplary cases for H = L. Left:
the case where Î³2
h/M = 4, 3, 2 for h = 1, 2, 3. Right: the case where Î³2
1/M = 30,
Î³2
h/M = 6, 5.75, 5.5,. . . , 2.0 for h = 2,. . . , 18.

212
8 Performance Analysis of VB Matrix Factorization
Lemma 8.8
The derivative of Î©(Ïƒâˆ’2) is given by
Î˜ â‰¡âˆ‚Î©
âˆ‚Ïƒâˆ’2 = âˆ’Ïƒ2 +

H
h=1 Î³h

Î³h âˆ’Ë˜Î³EVB
h
 
+ L
h=
H+1 Î³2
h
LM
,
(8.26)
where 
H is a function of Ïƒâˆ’2 deï¬ned by

H = 
H(Ïƒâˆ’2) = h
if
Ïƒâˆ’2 âˆˆBh.
(8.27)
Proof
The derivative of Eq. (8.4) with respect to Ïƒâˆ’2 is given by
âˆ‚Î©
âˆ‚Ïƒâˆ’2 = 1
L
â›âœâœâœâœâœâ
H

h=1
Î³2
h
M
âˆ‚Ïˆ
âˆ‚x +
L

h=H+1
Î³2
h
M
âˆ‚Ïˆ0
âˆ‚x
ââŸâŸâŸâŸâŸâ .
(8.28)
By using Eqs. (8.16) and (8.20), Eq. (8.28) can be written as
âˆ‚Î©
âˆ‚Ïƒâˆ’2 = 1
L
â›âœâœâœâœâœâ
L

h=1
Î³2
h
M
âˆ‚Ïˆ0
âˆ‚x +
H

h=1
Î¸

xh â‰¥x
 Î³2
h
M
âˆ‚Ïˆ1
âˆ‚x
ââŸâŸâŸâŸâŸâ 
= 1
L
â›âœâœâœâœâœâ
L

h=1
Î³2
h
M

1 âˆ’1
xh

âˆ’
H

h=1
Î¸

xh â‰¥x
 Î³2
hÏ„h
Mxh
ââŸâŸâŸâŸâŸâ 
=
L
h=1 Î³2
h
LM
âˆ’Ïƒ2 âˆ’1
L
H

h=1
Î¸ (Ï„h â‰¥Ï„) Ïƒ2Ï„h.
(8.29)
Here we also used the deï¬nition (6.132) of xh. Using Eq. (6.133), Eq. (8.29)
can be written as
âˆ‚Î©
âˆ‚Ïƒâˆ’2 =
L
h=1 Î³2
h
LM
âˆ’Ïƒ2 âˆ’
H

h=1
Î¸

Î³h â‰¥Î³EVB Î³hË˜Î³EVB
h
LM
= âˆ’Ïƒ2 +
H
h=1 Î³h

Î³h âˆ’Î³EVB
h
 
+ L
h=H+1 Î³2
h
LM
.
(8.30)
Here we also used the deï¬nition (6.101) of Î³EVB
h
. Using the deï¬nition (8.27)
and Lemma 8.7, we can replace Î³EVB
h
and H with Ë˜Î³EVB
h
and 
H, respectively,
which completes the proof of Lemma 8.8.
â–¡
Note that Eq. (8.26) involves the shrinkage estimator Ë˜Î³EVB
h
, which is a
function of Ïƒâˆ’2 (see Eq. (6.103)). For each hypothetical 
H, the solutions of
the equation
Î˜ = 0
(8.31)
lying in Ïƒâˆ’2
âˆˆB
H are stationary points, and hence candidates for the
global minimum. If we can solve Eq. (8.31) for all 
H = 1,. . . , H, we can
obtain the global solution by evaluating the objective (8.4) at each obtained

8.3 Proofs of Theorem 8.2 and Corollary 8.3
213
stationary point. However, solving Eq. (8.31) is computationally hard unless

H is small.2 Based on Lemma 8.8, we will obtain tighter bounds than
Lemma 8.5.
Since
Î³h âˆ’Ë˜Î³EVB
h
> 0,
Eq. (8.26) is upper-bounded by
Î˜ â‰¤âˆ’Ïƒ2 +
L

h=1
Î³2
h
LM ,
which leads to the upper-bound given in Eq. (8.13). Actually, if
â›âœâœâœâœâœâ
L

h=1
Î³2
h
LM
ââŸâŸâŸâŸâŸâ 
âˆ’1
âˆˆB0,
then

H = 0,
Ïƒ2 =
L

h=1
Î³2
h
LM ,
is a local minimum.
The following lemma is easily obtained from Eq. (6.103) by using the
inequalities z1 <
.
z2
1 âˆ’z2
2 < z1 âˆ’z2 for z1 > z2 > 0:
Lemma 8.9
For Î³h â‰¥Î³EVB, the EVB shrinkage estimator (6.103) can be
bounded as follows:
Î³h âˆ’(
âˆš
M +
âˆš
L)2Ïƒ2
Î³h
< Ë˜Î³EVB
h
< Î³h âˆ’(M + L)Ïƒ2
Î³h
.
(8.32)
This lemma is important for our analysis, because it allows us to bound the
most complicated part of Eq. (8.26) by quantities independent of Î³h, i.e.,
(M + L)Ïƒ2 < Î³h

Î³h âˆ’Ë˜Î³EVB
h
 
< (
âˆš
M +
âˆš
L)2Ïƒ2.
(8.33)
Using Eq. (8.33), we obtain the following lemma:
Lemma 8.10
Any local minimizer exists in Ïƒâˆ’2 âˆˆB
H such that

H <
L
1 + Î±,
2 It is easy to derive a closed-form solution for 
H = 0, 1.

214
8 Performance Analysis of VB Matrix Factorization
and the following holds for any local minimizer lying in Ïƒâˆ’2 âˆˆB
H:
Ïƒ2 â‰¥
L
h=
H+1 Î³2
h
LM âˆ’
H(M + L)
.
Proof
By substituting the lower-bound in Eq. (8.33) into Eq. (8.26), we obtain
Î˜ â‰¥âˆ’Ïƒ2 +

H(L + M)Ïƒ2 + L
h=
H+1 Î³2
h
LM
.
This implies that Î˜ > 0 unless the following hold:

H <
LM
L + M =
L
1 + Î±,
Ïƒ2 â‰¥
L
h=
H+1 Î³2
h
LM âˆ’
H(L + M)
.
Therefore, no local minimum exists if either of these conditions is violated.
This completes the proof of Lemma 8.10.
â–¡
It holds that
L
h=
H+1 Î³2
h
LM âˆ’
H(M + L)
â‰¥
L
h=
H+1 Î³2
h
M(L âˆ’
H)
,
(8.34)
of which the right-hand side is decreasing with respect to 
H. Combining
Lemmas 8.5, 8.6, 8.7, and 8.10 and Eq. (8.34) completes the proof of Theorem
8.2. Corollary 8.3 is easily obtained from Lemmas 8.6 and 8.8.
8.4 Performance Analysis
To analyze the behavior of the EVB solution in the fully observed MF model,
we rely on the random matrix theory (MarË‡cenko and Pastur, 1967; Wachter,
1978; Johnstone, 2001; Bouchaud and Potters, 2003; Hoyle and Rattray, 2004;
Baik and Silverstein, 2006), which describes the distribution of the singular
values of random matrices in the limit when the matrix size goes to inï¬nity.
We ï¬rst introduce some results obtained in the random matrix theory and then
apply them to our analysis.
8.4.1 Random Matrix Theory
Assume that the observed matrix V is generated from the spiked covariance
model (Johnstone, 2001):
V = Uâˆ—+ E,
(8.35)

8.4 Performance Analysis
215
where Uâˆ—âˆˆRLÃ—M is a true signal matrix with rank Hâˆ—and singular values
{Î³âˆ—
h}Hâˆ—
h=1, and E
âˆˆ
RLÃ—M is a random matrix such that each element is
independently drawn from a distribution with mean zero and variance Ïƒâˆ—2 (not
necessarily Gaussian). As the observed singular values {Î³h}L
h=1 of V, the true
singular values {Î³âˆ—
h}Hâˆ—
h=1 are also assumed to be arranged in the nonincreasing
order.
We deï¬ne normalized versions of the observed and the true singular values:
yh =
Î³2
h
MÏƒâˆ—2
for
h = 1,. . . , L,
(8.36)
Î½âˆ—
h =
Î³âˆ—2
h
MÏƒâˆ—2
for
h = 1,. . . , Hâˆ—.
(8.37)
In other words, {yh}L
h=1 are the eigenvalues of VVâŠ¤/(MÏƒâˆ—2), and {Î½âˆ—
h}Hâˆ—
h=1 are
the eigenvalues of Uâˆ—Uâˆ—âŠ¤/(MÏƒâˆ—2). Note the difference between xh, deï¬ned by
Eq. (6.132), and yh: xh is the squared observed singular value normalized with
the model noise variance Ïƒ2, which is to be estimated, while yh is the one
normalized with the true noise variance Ïƒâˆ—2.
Deï¬ne the empirical distribution of the observed eigenvalues {yh}L
h=1 by
p(y) = 1
L
L

h=1
Î´(y âˆ’yh),
(8.38)
where Î´(y) denotes the Dirac delta function. When Hâˆ—= 0, the observed matrix
V = E consists only of noise, and its singular value distribution in the large-
scale limit is speciï¬ed by the following proposition:
Proposition 8.11
(MarË‡cenko and Pastur, 1967; Wachter, 1978) In the large-
scale limit when L and M go to inï¬nity with its ratio Î± = L/M ï¬xed,
the empirical distribution of the eigenvalue y of EEâŠ¤/(MÏƒâˆ—2) almost surely
converges to
p(y) â†’pMP(y) â‰¡
.
(y âˆ’y)(y âˆ’y)
2Ï€Î±y
Î¸(y < y < y),
(8.39)
where
y = (1 + âˆšÎ±)2,
y = (1 âˆ’âˆšÎ±)2,
(8.40)
and Î¸(Â·) is the indicator function, deï¬ned in Theorem 8.1.3
Figure 8.3 shows Eq. (8.39), which we call the MarË‡cenkoâ€“Pastur (MP)
distribution, for Î± = 0.1, 1. The mean âŸ¨yâŸ©pMP(y) = 1 (which is constant for
3 Convergence is in weak topology in distribution, i.e., p(y) almost surely converges to pMP(y) so
that

f(y)p(y)dy =

f(y)pMP(y)dy for any bounded continuous function f(y).

216
8 Performance Analysis of VB Matrix Factorization
Figure 8.3 MarË‡cenkoâ€“Pastur distirbution.
any 0 < Î± â‰¤1) and the upper-limits y = y(Î±) of the support for Î± = 0.1, 1
are indicated by arrows. Proposition 8.11 states that the probability mass is
concentrated in the range between y â‰¤y â‰¤y. Note that the MP distribution
appears for a single sample matrix; differently from standard â€œlarge-sampleâ€
theories, Proposition 8.11 does not require one to average over many sample
matrices.4 This single-sample property of the MP distribution is highly useful
in our analysis because the MF model usually assumes a single observed
matrix V. We call the (unnormalized) singular value corresponding to the
upper-limit y, i.e.,
Î³MPUL =
.
MÏƒâˆ—2 Â· y = (
âˆš
L +
âˆš
M)Ïƒâˆ—,
(8.41)
the MarË‡cenkoâ€“Pastur upper limit (MPUL).
When Hâˆ—
>
0, the true signal matrix Uâˆ—affects the singular value
distribution of V. However, if Hâˆ—â‰ªL, the distribution can be approximated
by a mixture of spikes (delta functions) and the MP distribution pMP(y). Let
Hâˆ—âˆ—(â‰¤Hâˆ—) be the number of singular values of Uâˆ—greater than Î³âˆ—
h >
Î±1/4 âˆš
MÏƒâˆ—, i.e.,
Î½âˆ—
Hâˆ—âˆ—> âˆšÎ±
and
Î½âˆ—
Hâˆ—âˆ—+1 â‰¤âˆšÎ±.
(8.42)
Then, the following proposition holds:
Proposition 8.12
(Baik and Silverstein, 2006) In the large-scale limit when
L and M go to inï¬nity with ï¬nite Î± and Hâˆ—, it almost surely holds that
yh = ySig
h
â‰¡

1 + Î½âˆ—
h
 
1 + Î±
Î½âˆ—
h

for
h = 1,. . . , Hâˆ—âˆ—,
(8.43)
yHâˆ—âˆ—+1 = y,
and
yL = y.
4 This property is called self-averaging (Bouchaud and Potters, 2003).

8.4 Performance Analysis
217
Figure 8.4 Spiked covariance distribution when {Î½âˆ—
h}Hâˆ—âˆ—
h=1 = {1.5, 1.0, 0.5}.
Furthermore, Hoyle and Rattray (2004) argued that, when L and M are
large (but ï¬nite) and Hâˆ—â‰ªL, the empirical distribution of the eigenvalue y
of VVâŠ¤/(MÏƒâˆ—2), is accurately approximated by
p(y) â‰ˆpSC(y) â‰¡1
L
Hâˆ—âˆ—

h=1
Î´

y âˆ’ySig
h
 
+ L âˆ’Hâˆ—âˆ—
L
pMP(y).
(8.44)
Figure 8.4 shows Eq. (8.44), which we call the spiked covariance (SC)
distribution, for Î± = 0.1, Hâˆ—âˆ—= 3, and {Î½âˆ—
h}Hâˆ—âˆ—
h=1 = {1.5, 1.0, 0.5}. The SC
distribution is irrespective of {Î½âˆ—
h}Hâˆ—
h=Hâˆ—âˆ—+1, which satisfy 0 < Î½âˆ—
h â‰¤âˆšÎ± (see
the deï¬nition (8.42) of Hâˆ—âˆ—).
Proposition 8.12 states that in the large-scale limit, the large signal compo-
nents such that Î½âˆ—
h > âˆšÎ± appear outside the support of the MP distribution
as spikes, while the other small signals are indistinguishable from the MP
distribution (note that Eq. (8.43) implies that ySig
h
> y for Î½âˆ—
h > âˆšÎ±). This
implies that any PCA method fails to recover the true dimensionality, unless
Î½âˆ—
Hâˆ—> âˆšÎ±.
(8.45)
The condition (8.45) requires that Uâˆ—has no small positive singular value such
that 0 < Î½âˆ—
h â‰¤âˆšÎ±, and therefore Hâˆ—âˆ—= Hâˆ—.
The approximation (8.44) allows us to investigate more practical situations
where the matrix size is ï¬nite. In Sections 8.4.2 and 8.4.4, respectively, we
provide two theorems: one is based on Proposition 8.12 and guarantees perfect
rank (PCA dimensionality) recovery of EVB learning in the large-scale limit,
and the other one assumes that the approximation (8.44) exactly holds and
provides a more realistic condition for perfect recovery.

218
8 Performance Analysis of VB Matrix Factorization
8.4.2 Perfect Rank Recovery Condition in Large-Scale Limit
Now, we are almost ready for clarifying the behavior of the EVB solution. We
assume that the model rank is set to be large enough, i.e., Hâˆ—â‰¤H â‰¤L, and all
model parameters including the noise variance are estimated from observation
(i.e., complete EVB learning). The last proposition on which our analysis relies
is related to the property, called the strong unimodality, of the log-concave
distributions:
Proposition 8.13
(Ibragimov, 1956; Dharmadhikari and Joag-Dev, 1988)
The convolution
g(s) = âŸ¨f(s + t)âŸ©p(t) =

f(s + t)p(t)dt
is quasiconvex, if p(t) is a log-concave distribution, and f(t) is a quasiconvex
function.
In the large-scale limit, the summation over h = 1,. . . , L in the objective
Î©(Ïƒâˆ’2), given by Eq. (8.4), for noise variance estimation can be replaced
with the expectation over the MP distribution pMP(y). By scaling variables,
the objective can be written as a convolution with a scaled version of the
MP distribution, which turns out to be log-concave. Accordingly, we can use
Proposition 8.13 to show that Î©(Ïƒâˆ’2) is quasiconvex, which means that the
noise variance estimation by EVB learning can be accurately performed by a
local search algorithm. Combining this result with Proposition 8.12, we obtain
the following theorem:
Theorem 8.14
In the large-scale limit when L and M go to inï¬nity with ï¬nite
Î± and Hâˆ—, EVB learning almost surely recovers the true rank, i.e., 
HEVB = Hâˆ—,
if and only if
Î½âˆ—
Hâˆ—â‰¥Ï„,
(8.46)
where Ï„ is deï¬ned in Theorem 6.13.
Furthermore, the following corollary completely describes the behavior of
the EVB solution in the large-scale limit:
Corollary 8.15
In the large-scale limit, the objective Î©(Ïƒâˆ’2), deï¬ned by
Eq. (8.4), for the noise variance estimation converges to a quasiconvex
function, and it almost surely holds that
Ï„EVB
h
â›âœâœâœâœââ‰¡Î³hÎ³EVB
h
MÏƒ2 EVB
ââŸâŸâŸâŸâ =
â§âªâªâ¨âªâªâ©
Î½âˆ—
h
if Î½âˆ—
h â‰¥Ï„,
0
otherwise,
(8.47)
Ïƒ2 EVB = Ïƒâˆ—2.

8.4 Performance Analysis
219
One may get intuition of Eqs. (8.46) and (8.47) by comparing Eqs. (8.8)
and (6.134) with Eq. (8.43): The estimator Ï„h has the same relation to the
observation xh as the true signal Î½âˆ—
h, and hence is an unbiased estimator of
the signal. However, Theorem 8.14 does not even approximately hold in
practical situations with moderate-sized matrices (see the numerical validation
in Section 8.5). After proving Theorem 8.14 and Corollary 8.15, we will derive
a more practical condition for perfect recovery in Section 8.4.4.
8.4.3 Proofs of Theorem 8.14 and Corollary 8.15
In the large-scale limit, we can substitute the expectation âŸ¨f(y)âŸ©p(y) for the
summation Lâˆ’1 L
h=1 f (yh). We can also substitute the MP distribution pMP(y)
for p(y) for the expectation, since the contribution from the Hâˆ—signal
components converges to zero. Accordingly, our objective (8.4) converges to
Î©(Ïƒâˆ’2) â†’Î©LSL(Ïƒâˆ’2) â‰¡
 y
Îº
Ïˆ

Ïƒâˆ—2Ïƒâˆ’2y
 
pMP(y)dy +
 Îº
y
Ïˆ0

Ïƒâˆ—2Ïƒâˆ’2y
 
pMP(y)dy
= Î©LSLâˆ’Full(Ïƒâˆ’2) âˆ’
 Îº
max(xÏƒ2/Ïƒâˆ—2,y)
Ïˆ1

Ïƒâˆ—2Ïƒâˆ’2y
 
pMP(y)dy,
(8.48)
where
Î©LSLâˆ’Full(Ïƒâˆ’2) â‰¡
 y
y
Ïˆ

Ïƒâˆ—2Ïƒâˆ’2y
 
pMP(y)dy,
(8.49)
and Îº is a constant satisfying
H
L =
 y
Îº
pMP(y)dy
(y â‰¤Îº â‰¤y).
(8.50)
Note that x, y, and y are deï¬ned by Eqs. (8.8) and (8.40), and it holds that
x > y.
(8.51)
We ï¬rst investigate Eq. (8.49), which corresponds to the objective for the
full-rank model (i.e., H = L). Let
s = log(Ïƒâˆ’2),
t = log y

dt = 1
ydy
 
.
Then Eq. (8.49) is written as a convolution:
Î©LSLâˆ’Full(s) â‰¡Î©LSLâˆ’Full(es) =

Ïˆ

Ïƒâˆ—2es+t 
etpMP(et)dt
=

Ïˆ(s + t)pLSMP(t)dt,
(8.52)

220
8 Performance Analysis of VB Matrix Factorization
where
Ïˆ(s) = Ïˆ(Ïƒâˆ—2es),
pLSMP(t) = etpMP(et)
=
.
(et âˆ’y)(y âˆ’et)
2Ï€Î±
Î¸(y < et < y).
(8.53)
Since Lemma 8.4 states that Ïˆ(x) is quasiconvex, its composition Ïˆ(s) with the
nondecreasing function Ïƒâˆ—2es is also quasiconvex.
The following holds for pLSMP(t), which we call a log-scaled MP (LSMP)
distribution:
Lemma 8.16
The LSMP distribution (8.53) is log-concave.
Proof
Focusing on the support,
log y < t < log y,
of the LSMP distribution (8.53), we deï¬ne
f(t) â‰¡2 log pLSMP(t) = 2 log
.
(et âˆ’y)(y âˆ’et)
2Ï€Î±
= log(âˆ’e2t + (y + y)et âˆ’yy) + const.
Let
u(t) â‰¡(et âˆ’y)(y âˆ’et) = âˆ’e2t + (y + y)et âˆ’yy > 0,
(8.54)
and let
v(t) â‰¡âˆ‚u
âˆ‚t = âˆ’2e2t + (y + y)et = u âˆ’e2t + yy,
w(t) â‰¡âˆ‚2u
âˆ‚t2 = âˆ’4e2t + (y + y)et = v âˆ’2e2t,
be the ï¬rst and the second derivatives of u. Then, the ï¬rst and the second
derivatives of f(t) are given by
âˆ‚f
âˆ‚t = v
u,
âˆ‚2 f
âˆ‚t2 = uw âˆ’v2
u2
= âˆ’
et 
(y + y)e2t âˆ’4yyet + (y + y)yy
 
u2

8.4 Performance Analysis
221
= âˆ’
et(y + y)
u2
â›âœâœâœâœâœâœâœâœâ
â›âœâœâœâœâœâet âˆ’
2yy
(y + y)
ââŸâŸâŸâŸâŸâ 
2
+
yy

y âˆ’y
 2
(y + y)2
ââŸâŸâŸâŸâŸâŸâŸâŸâ 
â‰¤0.
This proves the log-concavity of the LSMP dsitribution pLSMP(t), and com-
pletes the proof of Lemma 8.16.
â–¡
Lemma 8.16 and Proposition 8.13 imply that Î©LSLâˆ’Full(s) is quasiconvex,
and therefore its composition Î©LSLâˆ’Full(Ïƒâˆ’2) with the nondecreasing function
log(Ïƒâˆ’2) is quasiconvex. The minimizer of Î©LSLâˆ’Full(Ïƒâˆ’2) can be found by
evaluating the derivative Î˜, given by Eq. (8.26), in the large-scale limit:
Î˜Full â†’Î˜LSLâˆ’Full = âˆ’Ïƒ2 + Ïƒâˆ—2
 y
y
y Â· pMP(y)dy
âˆ’
 y
xÏƒ2/Ïƒâˆ—2 Ï„(Ïƒâˆ—2Ïƒâˆ’2y; Î±)pMP(y)dy.
(8.55)
Here, we used Eqs. (6.133) and (8.9). In the range
0 < Ïƒâˆ’2 < xÏƒâˆ—âˆ’2
y

i.e.,
xÏƒ2
Ïƒâˆ—2 > y

,
(8.56)
the third term in Eq. (8.55) is zero. Therefore, Eq. (8.55) is increasing with
respect to Ïƒâˆ’2, and zero when
Ïƒ2 = Ïƒâˆ—2
 y
y
y Â· pMP(y)dy = Ïƒâˆ—2.
Accordingly, Î©LSLâˆ’Full(Ïƒâˆ’2) is strictly convex in the range (8.56). Eq. (8.51)
implies that the point Ïƒâˆ’2 = Ïƒâˆ—âˆ’2 is contained in the region (8.56), and
therefore it is a local minimum of Î©LSLâˆ’Full(Ïƒâˆ’2). Combined with the quasi-
convexity of Î©LSLâˆ’Full(Ïƒâˆ’2), we have the following lemma:
Lemma 8.17
The objective Î©LSLâˆ’Full(Ïƒâˆ’2) for the full-rank model H = L
in the large-scale limit is quasiconvex with its minimizer at Ïƒâˆ’2 = Ïƒâˆ—âˆ’2. It is
strictly convex in the range (8.56).
For any Îº (y < Îº < y), the second term in Eq. (8.48) is zero in the
range (8.56), which includes its minimizer at Ïƒâˆ’2 = Ïƒâˆ—âˆ’2. Since Lemma 8.4
states that Ïˆ1(x) is decreasing for x > x, the second term in Eq. (8.48) is
nondecreasing in the region where

Ïƒâˆ—âˆ’2 <
 xÏƒâˆ—âˆ’2
y
â‰¤Ïƒâˆ’2 < âˆ.

222
8 Performance Analysis of VB Matrix Factorization
Therefore, the quasi-convexity of Î©LSLâˆ’Full is inherited to Î©LSL:
Lemma 8.18
The objective Î©LSL(Ïƒâˆ’2) for noise variance estimation in the
large-scale limit is quasiconvex with its minimizer at Ïƒâˆ’2 = Ïƒâˆ—âˆ’2. Î©LSL(Ïƒâˆ’2)
is strictly convex in the range (8.56).
Thus we have proved that EVB learning accurately estimates the noise
variance in the large-scale limit:
Ïƒ2 EVB = Ïƒâˆ—2.
(8.57)
Assume that Eq. (8.45) holds. Then Proposition 8.12 guarantees that, in the
large-scale limit, the following hold:
Î³2
Hâˆ—
MÏƒâˆ—2 â‰¡yHâˆ—= $1 + Î½âˆ—
Hâˆ—% 
1 + Î±
Î½âˆ—
Hâˆ—

,
(8.58)
Î³2
Hâˆ—+1
MÏƒâˆ—2 â‰¡yHâˆ—+1 = y = (1 + âˆšÎ±)2.
(8.59)
Remember that the EVB threshold is given by Eq. (8.8), i.e.,
(Î³EVB)2
MÏƒ2 EVB â‰¡x =

1 + Ï„
 
1 + Î±
Ï„

.
(8.60)
Since Lemma 8.18 states that Ïƒ2 EVB = Ïƒâˆ—2, comparing Eqs. (8.58) and (8.59)
with Eq. (8.60) results in the following lemma:
Lemma 8.19
It almost surely holds that
Î³Hâˆ—â‰¥Î³EVB
if and only if
Î½âˆ—
Hâˆ—â‰¥Ï„,
(8.61)
Î³Hâˆ—+1 < Î³EVB
for any
{Î½âˆ—
h}.
This completes the proof of Theorem 8.14. Comparing Eqs. (6.134) and
(8.43) under Lemmas 8.18 and 8.19 proves Corollary 8.15.
â–¡
8.4.4 Practical Condition for Perfect Rank Recovery
Theorem 8.14 rigorously holds in the large-scale limit. However, it does
not describe the behavior of the EVB solution very accurately in practical
ï¬nite matrix-size cases. We can obtain a more practical condition for perfect
recovery by relying on the approximation (8.44). We can prove the following
theorem:
Theorem 8.20
Let
Î¾ = Hâˆ—
L

8.4 Performance Analysis
223
be the relevant rank ratio, and assume that
p(y) = pSC(y).
(8.62)
Then, EVB learning recovers the true rank, i.e., 
HEVB = Hâˆ—, if the following
two inequalities hold:
Î¾ < 1
x,
(8.63)
Î½âˆ—
Hâˆ—>
 xâˆ’1
1âˆ’xÎ¾ âˆ’Î±
 
+
B xâˆ’1
1âˆ’xÎ¾ âˆ’Î±
 2 âˆ’4Î±
2
,
(8.64)
where x is deï¬ned by Eq. (8.8).
Note that, in the large-scale limit, Î¾ converges to zero, and the sufï¬cient
condition, Eqs. (8.63) and (8.64), in Theorem 8.20 is equivalent to the
necessary and sufï¬cient condition (8.46) in Theorem 8.14.
Theorem 8.20 only requires that the SC distribution (8.44) well approxi-
mates the observed singular value distribution. Accordingly, it well describes
the dependency of the EVB solution on Î¾, which will be shown in numerical
validation in Section 8.5. Theorem 8.20 states that, if the true rank Hâˆ—is small
enough compared with L and the smallest signal Î½âˆ—
Hâˆ—is large enough, EVB
learning perfectly recovers the true rank.
The following corollary also supports EVB learning:
Corollary 8.21
Under the assumption (8.62) and the conditions (8.63) and
(8.64), the objective Î©(Ïƒâˆ’2) for the noise variance estimation has no local
minimum (no stationary point if Î¾ > 0) that results in a wrong estimated rank

HEVB  Hâˆ—.
This corollary states that, although the objective function (8.4) is nonconvex
and possibly multimodal in general, any local minimum leads to the correct
estimated rank. Therefore, perfect recovery does not require global search, but
only local search, for noise variance estimation, if L and M are sufï¬ciently
large so that we can warrant Eq. (8.62).
In the next section, we give the proofs of Theorem 8.20 and Corollary 8.21,
and then show numerical experiments that support the theory.
8.4.5 Proofs of Theorem 8.20 and Corollary 8.21
We regroup the terms in Eq. (8.4) as follows:
Î©(Ïƒâˆ’2) = Î©1(Ïƒâˆ’2) + Î©0(Ïƒâˆ’2),
(8.65)

224
8 Performance Analysis of VB Matrix Factorization
where
Î©1(Ïƒâˆ’2) = 1
Hâˆ—
Hâˆ—

h=1
Ïˆ
â›âœâœâœâœâ
Î³2
h
M Ïƒâˆ’2
ââŸâŸâŸâŸâ ,
(8.66)
Î©0(Ïƒâˆ’2) =
1
L âˆ’Hâˆ—
â›âœâœâœâœâœâ
H

h=Hâˆ—+1
Ïˆ
â›âœâœâœâœâ
Î³2
h
M Ïƒâˆ’2
ââŸâŸâŸâŸâ +
L

h=H+1
Ïˆ0
â›âœâœâœâœâ
Î³2
h
M Ïƒâˆ’2
ââŸâŸâŸâŸâ 
ââŸâŸâŸâŸâŸâ .
(8.67)
In the following, assuming that Eq. (8.62) holds and
yHâˆ—> y,
(8.68)
we derive a sufï¬cient condition for any local minimizer to lie only in Ïƒâˆ’2 âˆˆ
BHâˆ—, with which Lemma 8.7 proves Theorem 8.20.
Under the assumption (8.62) and the condition (8.68), Î©0(Ïƒâˆ’2), deï¬ned by
Eq. (8.67), is equivalent to the objective Î©LSL(Ïƒâˆ’2) in the large-scale limit.
Using Lemma 8.18, and noting that
Ïƒâˆ’2
Hâˆ—+1 =
Mx
Î³Hâˆ—+1
2
= xÏƒâˆ—âˆ’2
y
> Ïƒâˆ—âˆ’2,
(8.69)
we have the following lemma:
Lemma 8.22
Î©0(Ïƒâˆ’2) is quasiconvex with its minimizer at
Ïƒâˆ’2 = Ïƒâˆ—âˆ’2.
Î©0(Ïƒâˆ’2) is strictly convex in the range
0 < Ïƒâˆ’2 < Ïƒâˆ’2
Hâˆ—+1.
Using Lemma 8.22 and the strict quasiconvexity of Ïˆ(x), we can deduce the
following lemma:
Lemma 8.23
Î©(Ïƒâˆ’2) is nondecreasing (increasing if Î¾ > 0) in the range
Ïƒ2
Hâˆ—+1 < Ïƒâˆ’2 < âˆ.
Proof
Lemma 8.22 states that Î©0(Ïƒâˆ’2), deï¬ned by Eq. (8.67), is quasiconvex
with its minimizer at
Ïƒâˆ’2 =
â›âœâœâœâœâœâ
L
h=Hâˆ—+1 Î³2
h
(L âˆ’Hâˆ—)M
ââŸâŸâŸâŸâŸâ 
âˆ’1
= Ïƒâˆ—âˆ’2.
Since Î©1(Ïƒâˆ’2), deï¬ned by Eq. (8.66), is the sum of strictly quasiconvex
functions with their minimizers at Ïƒâˆ’2 = M/Î³2
h < Ïƒâˆ—âˆ’2 for h = 1,. . . , Hâˆ—,
our objective Î©(Ïƒâˆ’2), given by Eq. (8.65), is nondecreasing (increasing if
Hâˆ—> 0) for
Ïƒâˆ’2 â‰¥Ïƒâˆ—âˆ’2.

8.4 Performance Analysis
225
Since Eq. (8.69) implies that Ïƒâˆ’2
Hâˆ—+1 > Ïƒâˆ—âˆ’2, Î©(Ïƒâˆ’2) is nondecreasing
(increasing if Î¾ > 0) for Ïƒâˆ’2
> Ïƒâˆ’2
Hâˆ—+1, which completes the proof of
Lemma 8.23.
â–¡
Using the bounds given by Eq. (8.33) and Lemma 8.22, we also obtain the
following lemma:
Lemma 8.24
Î©(Ïƒâˆ’2) is increasing at Ïƒâˆ’2 = Ïƒ2
Hâˆ—+1 âˆ’0.5 It is decreasing at
Ïƒâˆ’2 = Ïƒ2
Hâˆ—+ 0 if the following hold:
Î¾ <
1
(1 + âˆšÎ±)2 ,
(8.70)
yHâˆ—>
x(1 âˆ’Î¾)
1 âˆ’Î¾(1 + âˆšÎ±)2 .
(8.71)
Proof
Lemma 8.22 states that Î©0(Ïƒâˆ’2) is strictly convex in the range 0 <
Ïƒâˆ’2 < Ïƒ2
Hâˆ—+1, and minimized at Ïƒâˆ’2 = Ïƒâˆ—âˆ’2. Since Eq. (8.69) implies
that Ïƒâˆ—âˆ’2
< Ïƒ2
Hâˆ—+1, Î©0(Ïƒâˆ’2) is increasing at Ïƒâˆ’2
= Ïƒ2
Hâˆ—+1 âˆ’0. Since
Î©1(Ïƒâˆ’2) is the sum of strictly quasiconvex functions with their minimizers
at Ïƒâˆ’2 = M/Î³2
h < Ïƒâˆ—âˆ’2 for h = 1,. . . , Hâˆ—, Î©(Ïƒâˆ’2) is also increasing at
Ïƒâˆ’2 = Ïƒ2
Hâˆ—+1 âˆ’0.
Let us investigate the sign of the derivative Î˜ of Î©(Ïƒâˆ’2) at Ïƒâˆ’2 = Ïƒ2
Hâˆ—+ 0 âˆˆ
BHâˆ—. Substituting the upper-bound in Eq. (8.33) into Eq. (8.26), we have
Î˜ < âˆ’Ïƒ2 + Hâˆ—(
âˆš
L +
âˆš
M)2Ïƒ2 + L
h=Hâˆ—+1 Î³2
h
LM
= âˆ’Ïƒ2 + Hâˆ—(
âˆš
L +
âˆš
M)2Ïƒ2 + (L âˆ’Hâˆ—)MÏƒâˆ—2
LM
.
(8.72)
The right-hand side of Eq. (8.72) is negative if the following hold:
Î¾ = Hâˆ—
L <
M
(
âˆš
L +
âˆš
M)2 =
1
(1 + âˆšÎ±)2 ,
(8.73)
Ïƒ2 >
(L âˆ’Hâˆ—)MÏƒâˆ—2
LM âˆ’Hâˆ—(
âˆš
L +
âˆš
M)2 =
(1 âˆ’Î¾)Ïƒâˆ—2
1 âˆ’Î¾(1 + âˆšÎ±)2 .
(8.74)
Assume that the ï¬rst condition (8.73) holds. Then the second condition
(8.74) holds at Ïƒâˆ’2 = Ïƒ2
Hâˆ—+ 0, if
Ïƒâˆ’2
Hâˆ—< 1 âˆ’Î¾(1 + âˆšÎ±)2
(1 âˆ’Î¾)
Ïƒâˆ—âˆ’2,
5 By â€œâˆ’0â€ we denote an arbitrarily large negative value.

226
8 Performance Analysis of VB Matrix Factorization
or equivalently,
yHâˆ—=
Î³2
Hâˆ—
MÏƒâˆ—2 = x Â· Ïƒ2
Hâˆ—
Ïƒâˆ—2 >
x(1 âˆ’Î¾)
1 âˆ’Î¾(1 + âˆšÎ±)2 ,
which completes the proof of Lemma 8.24.
â–¡
Finally, we obtain the following lemma:
Lemma 8.25
Î©(Ïƒâˆ’2) is decreasing in the range 0 < Ïƒâˆ’2 < Ïƒ2
Hâˆ—if the
following hold:
Î¾ < 1
x,
(8.75)
yHâˆ—> x(1 âˆ’Î¾)
1 âˆ’xÎ¾ .
(8.76)
Proof
In the range 0 < Ïƒâˆ’2 < Ïƒ2
Hâˆ—, the estimated rank (8.27) is bounded as
0 â‰¤
H â‰¤Hâˆ—âˆ’1.
(8.77)
Substituting the upper-bound in Eq. (8.33) into Eq. (8.26), we have
Î˜ < âˆ’Ïƒ2 +

H(
âˆš
L +
âˆš
M)2Ïƒ2 + Hâˆ—
h=
H+1 Î³2
h + L
h=Hâˆ—+1 Î³2
h
LM
= âˆ’Ïƒ2 +

H(
âˆš
L +
âˆš
M)2Ïƒ2 + Hâˆ—
h=
H+1 Î³2
h + (L âˆ’Hâˆ—)MÏƒâˆ—2
LM
.
(8.78)
The right-hand side of Eq. (8.78) is negative, if the following hold:

H
L <
M
(
âˆš
L +
âˆš
M)2 =
1
(1 + âˆšÎ±)2 ,
(8.79)
Ïƒ2 >
Hâˆ—
h=
H+1 Î³2
h + (L âˆ’Hâˆ—)MÏƒâˆ—2
LM âˆ’
H(
âˆš
L +
âˆš
M)2
.
(8.80)
Assume that
Î¾ = Hâˆ—
L <
1
(1 + âˆšÎ±)2 .
Then both of the conditions (8.79) and (8.80) hold for Ïƒâˆ’2 âˆˆ(0, Ïƒ2
Hâˆ—), if the
following holds:
Ïƒâˆ’2

H+1 <
LM âˆ’
H(
âˆš
L +
âˆš
M)2
Hâˆ—
h=
H+1 Î³2
h + (L âˆ’Hâˆ—)MÏƒâˆ—2
for

H = 0,. . . , Hâˆ—âˆ’1. (8.81)

8.4 Performance Analysis
227
Since the sum Hâˆ—
h=
H+1 Î³2
h in the right-hand side of Eq. (8.81) is upper-
bounded as
Hâˆ—

h=
H+1
Î³2
h â‰¤(Hâˆ—âˆ’
H)Î³2

H+1,
Eq. (8.81) holds if
Ïƒâˆ’2

H+1 <
LM âˆ’
H(
âˆš
L +
âˆš
M)2
(Hâˆ—âˆ’
H)Î³2

H+1 + (L âˆ’Hâˆ—)LÏƒâˆ—2
=
1 âˆ’
H
L (1 + âˆšÎ±)2
(Î¾ âˆ’
H
L )
Î³2

H+1
M + (1 âˆ’Î¾)Ïƒâˆ—2
for

H = 0,. . . , Hâˆ—âˆ’1.
(8.82)
Using Eq. (8.24), the condition (8.82) is rewritten as
Î³2

H+1
Mx > (Î¾ âˆ’
H
L )
Î³2

H+1
M + (1 âˆ’Î¾)Ïƒâˆ—2
1 âˆ’
H
L (1 + âˆšÎ±)2
â›âœâœâœâœâ1 âˆ’

H
L (1 + âˆšÎ±)2
ââŸâŸâŸâŸâ 
Î³2

H+1
MÏƒâˆ—2 >
â›âœâœâœâœâÎ¾x âˆ’

H
L x
ââŸâŸâŸâŸâ 
Î³2

H+1
MÏƒâˆ—2 + (1 âˆ’Î¾)x,
or equivalently
y
H+1 =
Î³2

H+1
MÏƒâˆ—2 >
(1 âˆ’Î¾) x

1 âˆ’Î¾x + 
H
L

x âˆ’(1 + âˆšÎ±)2  
for

H = 0,. . . , Hâˆ—âˆ’1.
(8.83)
Note that x > y = (1 + âˆšÎ±)2. Further bounding both sides, we have the
following sufï¬cient condition for Eq. (8.83) to hold:
yHâˆ—>
(1 âˆ’Î¾)x
max

0, 1 âˆ’Î¾x
 .
(8.84)
Thus we obtain the conditions (8.75) and (8.76) for Î˜ to be negative for Ïƒâˆ’2 âˆˆ
(0, Ïƒ2
Hâˆ—), which completes the proof of Lemma 8.25.
â–¡
Lemmas 8.23, 8.24, and 8.25 together state that, if all the conditions (8.68)
and (8.70) through (8.76) hold, at least one local minimum exists in the correct
range Ïƒâˆ’2 âˆˆBHâˆ—, and no local minimum (no stationary point if Î¾ > 0) exists
outside the correct range. Therefore, we can estimate the correct rank 
HEVB =
Hâˆ—by using a local search algorithm for noise variance estimation. Choosing
the tightest conditions, we have the following lemma:

228
8 Performance Analysis of VB Matrix Factorization
Lemma 8.26
Î©(Ïƒâˆ’2) has a global minimum in Ïƒâˆ’2 âˆˆBHâˆ—, and no local
minimum (no stationary point if Î¾ > 0) outside BHâˆ—, if the following hold:
Î¾ < 1
x,
yHâˆ—=
Î³2
Hâˆ—
MÏƒâˆ—2 > x(1 âˆ’Î¾)
1 âˆ’xÎ¾ .
(8.85)
Using Eq. (8.43), Eq. (8.85) can be written with the true signal amplitude
as follows:
$1 + Î½âˆ—
Hâˆ—% 
1 + Î±
Î½âˆ—
Hâˆ—

âˆ’x(1 âˆ’Î¾)
1 âˆ’xÎ¾ > 0.
(8.86)
The left-hand side of Eq. (8.86) can be factorized as follows:
1
Î½âˆ—
Hâˆ—
â›âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ
Î½âˆ—
Hâˆ—âˆ’
 x(1âˆ’Î¾)
1âˆ’xÎ¾ âˆ’(1 + Î±)
 
+
B x(1âˆ’Î¾)
1âˆ’xÎ¾ âˆ’(1 + Î±)
 2 âˆ’4Î±
2
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
Â·
â›âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ
Î½âˆ—
Hâˆ—âˆ’
 x(1âˆ’Î¾)
1âˆ’xÎ¾ âˆ’(1 + Î±)
 
âˆ’
B x(1âˆ’Î¾)
1âˆ’xÎ¾ âˆ’(1 + Î±)
 2 âˆ’4Î±
2
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
> 0. (8.87)
When Eq. (8.45) holds, the last factor in the left-hand side in Eq. (8.87) is
positive. Therefore, we have the following condition:
Î½âˆ—
Hâˆ—>
 x(1âˆ’Î¾)
1âˆ’xÎ¾ âˆ’(1 + Î±)
 
+
B x(1âˆ’Î¾)
1âˆ’xÎ¾ âˆ’(1 + Î±)
 2 âˆ’4Î±
2
=
 xâˆ’1
1âˆ’xÎ¾ âˆ’Î±
 
+
B xâˆ’1
1âˆ’xÎ¾ âˆ’Î±
 2 âˆ’4Î±
2
.
(8.88)
Lemma 8.26 with the condition (8.85) replaced with the condition (8.88) leads
to Theorem 8.20 and Corollary 8.21.
8.5 Numerical Veriï¬cation
Figure 8.5 shows numerical simulation results for M
= 200 and L =
20, 100, 200. E was drawn from the independent Gaussian distribution with
mean 0 and variance Ïƒâˆ—2 = 1, and true signal singular values {Î³âˆ—
h}Hâˆ—
h=1 were

8.5 Numerical Veriï¬cation
229
0
1
2
3
4
5
0
0.5
1
Success rate
(a) L = 20
0
1
2
3
4
5
0
0.5
1
Success rate
(b) L = 100
0
1
2
3
4
5
0
0.5
1
Success rate
(c) L = 200
Figure 8.5 Success rate of rank recovery in numerical simulation for M = 200.
The horizontal axis indicates the lower limit of the support of the simulated
true signal distribution, i.e., z â‰ˆCÎ½âˆ—
Hâˆ—. The recovery condition (8.64) for ï¬nite-
sized matrices is indicated by a vertical bar with the same line style for each Î¾.
The leftmost vertical bar, which corresponds to the condition (8.64) for Î¾ = 0,
coincides with the recovery condition (8.46) for inï¬nite-sized matrices.
drawn from the uniform distribution on [z
âˆš
MÏƒâˆ—, 10
âˆš
MÏƒâˆ—] for different z,
which is indicated by the horizontal axis. We used Algorithm 16, which will
be introduced in Chapter 9, to compute the global EVB solution.
The vertical axis indicates the success rate of rank recovery over 100 trials,
i.e., the proportion of the trials giving 
HEVB = Hâˆ—. If the condition (8.63) on
Î¾ is violated, the corresponding curve is depicted with markers. Otherwise, the
condition (8.64) on Î½âˆ—
Hâˆ—(= Î³âˆ—2
Hâˆ—/(MÏƒâˆ—2)) is indicated by a vertical bar with the
same line style for each Î¾. In other words, Theorem 8.20 states that the success
rate should be equal to one if z (> Î³âˆ—
Hâˆ—/(
âˆš
MÏƒâˆ—2)) is larger than the value
indicated by the vertical bar. The leftmost vertical bar, which corresponds to
the condition (8.64) for Î¾ = 0, coincides with the recovery condition (8.46),
given by Theorem 8.14, for inï¬nite-sized matrices.
We see that Theorem 8.20 with the condition (8.64) approximately holds for
these moderate-sized matrices, while Theorem 8.14 with the condition (8.46),
which does not depend on the relevant rank ratio Î¾, immediately breaks for
positive Î¾.

230
8 Performance Analysis of VB Matrix Factorization
8.6 Comparison with Laplace Approximation
Here, we compare EVB learning with an alternative dimensionality selection
method (Hoyle, 2008) based on the Laplace approximation (LA). Consider the
PCA application, where D denotes the dimensionality of the observation space,
and N denotes the number of samples, i.e., in our MF notation to keep L â‰¤M,
L = D, M = N
if
D â‰¤N,
L = N, M = D
if
D > N.
(8.89)
Right after Tipping and Bishop (1999) proposed the probabilistic PCA,
Bishop (1999a) proposed to select the PCA dimensionality by maximizing the
marginal likelihood:
p(V) = âŸ¨p(V|A, B)âŸ©p(A)p(B) .
(8.90)
Since the marginal likelihood (8.90) is computationally intractable, he approxi-
mated it by LA, and suggested Gibbs sampling and VB learning as alternatives.
The VB variant, of which the model is almost the same as the MF deï¬ned by
Eqs. (6.1) through (6.3), was also proposed by himself (Bishop, 1999b) along
with a standard local solver similar to Algorithm 1 in Chapter 3.
The LA-based approach was polished in Minka (2001a), by introducing a
conjugate prior6 on B to p(V|B) = âŸ¨p(V|A, B)âŸ©p(A), and ignoring the non-
leading terms that do not grow fast as the number N of samples goes to inï¬nity.
Hoyle (2008) pointed out that Minkaâ€™s method is inaccurate when D â‰«N, and
proposed the overlap (OL) method, a further polished variant of the LA-based
approach. A notable difference of the OL method from most of the LA-based
methods is that the OL method applies LA around a more accurate estimator
than the MAP estimator.7 Thanks to the use of the accurate estimator, the OL
method behaves optimally in the large-scale limit when D and N go to inï¬nity,
while Minkaâ€™s method does not. We will clarify the meaning of the optimality
and discuss it in more detail in Section 8.7.
The OL method minimizes an approximation to the negative logarithm of
the marginal likelihood (8.90), which depends on estimators for Î»h = b2
h + Ïƒ2
and Ïƒ2 computed by an iterative algorithm, over the hypothetical model rank
H = 0,. . . , L (see Appendix C for the detailed computational procedure).
Figure 8.6 shows numerical simulation results that compare EVB learning and
the OL method: Figure 8.6(a) shows the success rate for the no-signal case
Î¾ = 0 (Hâˆ—= 0), while Figures 8.6(b) through 8.6(f) show the success rate for
Î¾ = 0.05 and D = 20, 100, 200, 400, and 1, 000, respectively. We also show
6 This conjugate prior does not satisfy the implicit requirement, footnoted in Section 1.2.4, that
the moments of the family member can be computed analytically.
7 As explained in Section 2.2.1, LA is usually applied around the MAP estimator.

8.6 Comparison with Laplace Approximation
231
0
0.2
0.4
0.6
0.8
1
1.2
EVB
OL
Local-EVB
Success rate
(a) Î¾ = 0
0
1
2
3
4
5
0
0.5
1
Success rate
Local-EVB
(b) Î¾ = 0.05, D = 20
0
3
4
5
0
0.5
1
2
1
Success rate
Local-EVB
(c) Î¾ = 0.05, D = 100
0
1
2
3
4
5
0
0.5
1
Success rate
Local-EVB
(d) Î¾ = 0.05, D = 200
0
1
2
3
4
5
0
0.5
1
Success rate
Local-EVB
(e) Î¾ = 0.05, D = 400
0
0.5
1
0
1
2
3
4
5
Success rate
Local-EVB
(f) Î¾ = 0.05, D = 1, 000
Figure 8.6 Success rate of PCA dimensionality recovery by (global) EVB
learning, the OL method, and the local-EVB estimator for N = 200. Vertical bars
indicate the recovery conditions, Eq. (8.46) for EVB learning, and Eq. (8.95) for
the OL method and the local-EVB estimator, in the large-scale limit.
the performance of the local-EVB estimator (6.131), which was computed by
a local solver (Algorithm 18 introduced in Chapter 9). For the OL method
and the local-EVB estimator, we initialized the noise variance estimator to
10âˆ’4 Â· L
h=1 Î³2
h/(LM).
In comparison with the OL method, EVB learning shows its conservative
nature: It exhibits almost zero false positive rate (Figure 8.6(a)) at the expense
of low sensitivity (Figures 8.6(c) through 8.6(f)). Actually, because of its low
sensitivity, EVB learning does not behave optimally in the large-scale limit.
The local-EVB estimator, on the other hand, behaves similarly to the OL
method, for which the reason will be elucidated in the next section.

232
8 Performance Analysis of VB Matrix Factorization
8.7 Optimality in Large-Scale Limit
Consider the large-scale limit, and assume that the model rank H is set to be
large enough but ï¬nite so that H â‰¥Hâˆ—and H/L â†’0. Then the rank estimation
procedure, detailed in Appendix C, by the OL method is reduced to counting
the number of components such that Î»OLâˆ’LSL
h
> Ïƒ2 OLâˆ’LSL, i.e.,

HOLâˆ’LSL =
L

h=1
Î¸
Î»OLâˆ’LSL
h
> Ïƒ2 OLâˆ’LSL 
,
(8.91)
where Î¸(Â·) is the indicator function deï¬ned in Theorem 8.1. Here Î»OLâˆ’LSL
h
and
Ïƒ2 OLâˆ’LSL are computed by iterating the following updates until convergence:
Î»OLâˆ’LSL
h
=
â§âªâªâ¨âªâªâ©
Ë˜Î»OLâˆ’LSL
h
if Î³h â‰¥Î³localâˆ’EVB,
Ïƒ2 OLâˆ’LSL
otherwise,
(8.92)
Ïƒ2 OLâˆ’LSL =
1
(M âˆ’H)
â›âœâœâœâœâœâ
L

l=1
Î³2
l
L âˆ’
H

h=1
Î»OLâˆ’LSL
h
ââŸâŸâŸâŸâŸâ ,
(8.93)
where
Ë˜Î»OLâˆ’LSL
h
= Î³2
h
2L

1 âˆ’(M âˆ’L)Ïƒ2 OLâˆ’LSL
Î³2
h
+
>
@â›âœâœâœâœâ1 âˆ’(M âˆ’L)Ïƒ2 OLâˆ’LSL
Î³2
h
ââŸâŸâŸâŸâ 
2
âˆ’4LÏƒ2 OLâˆ’LSL
Î³2
h

.
(8.94)
The OL method evaluates its objective, which approximates the negative
logarithm of the marginal likelihood (8.90), after the updates (8.92) and (8.93)
converge for each hypothetical H, and adopts the minimizer 
HOLâˆ’LSL as the
rank estimator. However, Hoyle (2008) proved that, in the large-scale limit, the
objective decreases as H increases, as long as Eq. (8.94) is a real number (or
equivalently Î³h â‰¥Î³localâˆ’EVB holds) for all h = 1,. . . , H at the convergence.
Accordingly, Eq. (8.91) holds.
Interestingly, the threshold in Eq. (8.92) coincides with the local-EVB
threshold (6.127). Moreover, the updates (8.92) and (8.93) for the OL method
are equivalent to the updates (9.29) and (9.30) for the local-EVB estimator
(Algorithm 18) with the following correspondence:
Î»OLâˆ’LSL
h
= Î³hÎ³localâˆ’EVB
h
L
+ Ïƒ2 localâˆ’EVB,
Ïƒ2 OLâˆ’LSL = Ïƒ2 localâˆ’EVB.
Thus, the rank estimation procedure by the OL method and that by the local-
EVB estimator are equivalent, and therefore 
HOLâˆ’LSL = 
Hlocalâˆ’EVB in the large-
scale limit.

8.7 Optimality in Large-Scale Limit
233
If the noise variance is accurately estimated, i.e., Ïƒ2 = Ïƒâˆ—2, the threshold
Î³localâˆ’EVB both for the OL method and the local-EVB estimator coincides with
the MPUL (8.41), which corresponds to the minimum detectable observed
singular value. By using this fact, the optimality of the OL method in the large-
scale limit was shown:
Proposition 8.27
(Hoyle, 2008) In the large-scale limit, when L and M go to
inï¬nity with ï¬nite Î±, Hâˆ—, and H (â‰¥Hâˆ—)8, the OL method almost surely recovers
the true rank, i.e., 
HOLâˆ’LSL = Hâˆ—, if and only if
Î½âˆ—
Hâˆ—> âˆšÎ±.
(8.95)
It almost surely holds that
Î»OLâˆ’LSL
h
Ïƒ2 OLâˆ’LSL âˆ’1 = Î½âˆ—
h,
Ïƒ2 OLâˆ’LSL = Ïƒâˆ—2.
The condition (8.95) coincides with the condition (8.45), which any PCA
method requires for perfect dimensionality recovery. In this sense, the OL
method, as well as the local-EVB estimator, is optimal in the large-scale limit.
On the other hand, Theorem 8.14 implies that (global) EVB learning is
not optimal in the large-scale limit but more conservative (see the difference
between Ï„ and âˆšÎ± in Figure 6.4). In Figure 8.6, the conditions for perfect
dimensionality recovery in the large-scale limit are indicated by vertical bars:
z = âˆšÏ„ for EVB, and z =
.
Ï„local = Î±1/4 for OL and local-EVB.
All methods accurately estimate the noise variance in the large-scale
limit, i.e.,
Ïƒ2 EVB = Ïƒ2 OLâˆ’LSL = Ïƒ2 localâˆ’EVB = Ïƒâˆ—2.
Taking this into account, we indicate the recovery conditions in Figure 8.4 by
arrows at
y = x for EVB, and y = xlocal(= y) for OL and local-EVB,
respectively. Figure 8.4 implies that, in this particular case, EVB learning
discards the third spike coming from the third true signal Î½âˆ—
3 = 0.5, while the
OL method and the local-EVB estimator successfully capture it as a signal.
When the matrix size is ï¬nite, the conservative nature of EVB learning is
not always bad, since it offers almost zero false positive rate, which makes
8 Unlike our analysis in Section 8.4, Hoyle (2008) assumed H/L â†’0 to prove that the OL
method accurately estimates the noise variance.

234
8 Performance Analysis of VB Matrix Factorization
(a) V = 1.0
(b) V = 1.05
(c) V = 1.5
(d) V = 2.1
Figure 8.7 The VB free energy contribution (6.55) from the (ï¬rst) component
and its counterpart (8.96) of Bayesian learning for L = M = H = 1 and Ïƒ2 = 1.
Markers indicate the local minima.
Theorem 8.20 approximately hold for ï¬nite cases, as seen in Figures 8.5
and 8.6. However, the fact that not (global) EVB learning but the local-
EVB estimator is optimal in the large-scale limit might come from inaccurate
approximation to the Bayes posterior by the VB posterior. Having this in mind,
we discuss the difference between VB learning and full Bayesian learning in
the remainder of this section.
Figure 8.7 shows the VB free energy contribution (6.55) from the (ï¬rst)
component as a function of cacb, and its counterpart of Bayesian learning:
2FBayes
1
= âˆ’2 log âŸ¨p(V|A, B)âŸ©p(A)p(B) âˆ’

log(2Ï€Ïƒ2) + V2
Ïƒ2

,
(8.96)
which was numerically computed. We see that the minimizer (shown as a
diamond) of the Bayes free energy is at cacb â†’+0 until V exceeds 1.
The difference in behavior between EVB learning and the local-EVB
estimator appears in the nonempty range of the observed value V where the
positive local solution exists but gives positive free energy. Figure 8.7(d)
shows this case, where a bump exists between two local minima (indicated by
crosses). On the other hand, such multimodality is not observed in empirical

8.7 Optimality in Large-Scale Limit
235
full Bayesian learning (see the dashed curves in Figures 8.7(a) through 8.7(d)).
We can say that this multimodality in EVB learning with a bump between
two local minima is induced by the independence constraint for VB learning.
We further guess that it is this bump that pushes the EVB threshold from
the optimal point (at the local-EVB threshold) to a larger value. Further
investigation is necessary to fully understand this phenomenon.

9
Global Solver for Matrix Factorization
The analytic-form solutions, derived in Chapter 6, for VB learning and EVB
learning in fully observed MF can naturally be used to develop efï¬cient
and reliable VB solvers. Some properties, shown in Chapter 8, can also be
incorporated when the noise variance is unknown and to be estimated.
In this chapter, we introduce global solvers for VB learning and EVB
learning (Nakajima et al., 2013a, 2015), and how to extend them to more
general cases with missing entries and nonconjugate likelihoods (Seeger and
Bouchard, 2012).
9.1 Global VB Solver for Fully Observed MF
We consider the MF model, introduced in Section 3.1:
p(V|A, B) âˆexp

âˆ’1
2Ïƒ2
###V âˆ’BAâŠ¤###2
Fro

,
(9.1)
p(A) âˆexp

âˆ’1
2tr

ACâˆ’1
A AâŠ¤ 
,
(9.2)
p(B) âˆexp

âˆ’1
2tr

BCâˆ’1
B BâŠ¤ 
,
(9.3)
where V âˆˆRLÃ—M is an observed matrix;
A = (a1,. . . , aH) = $a1,. . . ,aM
%âŠ¤âˆˆRMÃ—H,
B = (b1,. . . , bH) =
b1,. . . ,bL
 âŠ¤âˆˆRLÃ—H,
236

9.1 Global VB Solver for Fully Observed MF
237
are parameter matrices; and CA, CB, and Ïƒ2 are hyperparameters. The prior
covariance hyperparameters are restricted to be diagonal:
CA = Diag(c2
a1,. . . , c2
aH),
CB = Diag(c2
b1,. . . , c2
bH).
Our VB solver gives the global solution to the following minimization
problem,
r = argmin
r
F(r)
s.t.
r(A, B) = rA(A)rB(B),
(9.4)
of the free energy
F(r) =
/
log
rA(A)rB(B)
p(V|A, B)p(A)p(B)
0
rA(A)rB(B)
.
(9.5)
Assume that L â‰¤M without loss of generality, and let
V =
L

h=1
Î³hÏ‰bhÏ‰âŠ¤
ah
(9.6)
be the singular value decomposition (SVD) of the observed matrix V âˆˆRLÃ—M.
According to Theorem 6.7, the VB solution is given by
U
VB = BA =
H

h=1
Î³VB
h Ï‰bhÏ‰âŠ¤
ah,
where
Î³VB
h
=
â§âªâªâ¨âªâªâ©
Ë˜Î³VB
h
if Î³h â‰¥Î³VB
h ,
0
otherwise,
(9.7)
for
Î³VB
h
= Ïƒ
>
?
?
?
@
(L + M)
2
+
Ïƒ2
2c2ahc2
bh
+
>
?
@â›âœâœâœâœâœâ
(L + M)
2
+
Ïƒ2
2c2ahc2
bh
ââŸâŸâŸâŸâŸâ 
2
âˆ’LM,
(9.8)
Ë˜Î³VB
h
= Î³h
â›âœâœâœâœâœâœâœâ1 âˆ’Ïƒ2
2Î³2
h
â›âœâœâœâœâœâœâœâM + L +
>
@
(M âˆ’L)2 +
4Î³2
h
c2ahc2
bh
ââŸâŸâŸâŸâŸâŸâŸâ 
ââŸâŸâŸâŸâŸâŸâŸâ .
(9.9)
Corollary 6.8 completely speciï¬es the VB posterior, which is written as
r(A, B) =
H

h=1
GaussM(ah;ahÏ‰ah, Ïƒ2
ah IM)
H

h=1
GaussL(bh;bhÏ‰bh, Ïƒ2
bhIL)
(9.10)
with the following variational parameters: if Î³h > Î³VB
h ,
ah = Â±
.
Ë˜Î³VB
h Î´VB
h ,
bh = Â±
>
@
Ë˜Î³VB
h
Î´VB
h
,
Ïƒ2
ah = Ïƒ2Î´VB
h
Î³h
,
Ïƒ2
bh =
Ïƒ2
Î³hÎ´VB
h
, (9.11)

238
9 Global Solver for Matrix Factorization
Algorithm 15 Global VB solver for fully observed matrix factorization.
1: Transpose V â†’VâŠ¤if L > M, and set H (â‰¤L) to a sufï¬ciently large value.
2: Compute the SVD (9.6) of V.
3: Apply Eqs. (9.7) through (9.9) to get the VB estimator.
4: If necessary, compute the variational parameters by using Eqs. (9.11)
through (9.14), which specify the VB posterior (9.10). We can also
evaluate the free energy by using Eqs. (9.15) and (9.16).
where
Î´VB
h

â‰¡ah
bh

= cah
Ïƒ2

Î³h âˆ’Ë˜Î³VB
h
âˆ’LÏƒ2
Î³h

,
(9.12)
and otherwise,
ah = 0,
bh = 0,
Ïƒ2
ah = c2
ah
â›âœâœâœâœâœâ1 âˆ’LÎ¶VB
h
Ïƒ2
ââŸâŸâŸâŸâŸâ ,
Ïƒ2
bh = c2
bh
â›âœâœâœâœâœâ1 âˆ’MÎ¶VB
h
Ïƒ2
ââŸâŸâŸâŸâŸâ , (9.13)
where
Î¶VB
h

â‰¡Ïƒ2
ahÏƒ2
bh
 
=
Ïƒ2
2LM
â›âœâœâœâœâœâœâœâœâœâL + M +
Ïƒ2
c2ahc2
bh
âˆ’
>
?
@â›âœâœâœâœâœâL + M +
Ïƒ2
c2ahc2
bh
ââŸâŸâŸâŸâŸâ 
2
âˆ’4LM
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâ .
(9.14)
The free energy can be written as
2F = LM log(2Ï€Ïƒ2) +
L
h=1 Î³2
h
Ïƒ2
+
H

h=1
2Fh,
(9.15)
where
2Fh = M log
c2
ah
Ïƒ2ah
+ L log
c2
bh
Ïƒ2
bh
+
a2
h + MÏƒ2
ah
c2ah
+
b2
h + LÏƒ2
bh
c2
bh
âˆ’(L + M) +
âˆ’2ahbhÎ³h +

a2
h + MÏƒ2
ah
 b2
h + LÏƒ2
bh
 
Ïƒ2
.
(9.16)
Based on these results, we can straightforwardly construct a global solver
for VB learning, which is given in Algorithm 15.
9.2 Global EVB Solver for Fully Observed MF
EVB learning, where the hyperparameters CA, CA, and Ïƒ2 are also estimated
from observation, solves the following minimization problem,
r = argmin
r,CA,CA,Ïƒ2F
s.t.
r(A, B) = rA(A)rB(B),
(9.17)
of the free energy (9.5).

9.2 Global EVB Solver for Fully Observed MF
239
According to Theorem 6.13, given the noise variance Ïƒ2, the EVB solution
can be written as
U
EVB =
H

h=1
Î³EVB
h
Ï‰bhÏ‰âŠ¤
ah,
where
Î³EVB
h
=
â§âªâªâ¨âªâªâ©
Ë˜Î³EVB
h
if Î³h â‰¥Î³EVB,
0
otherwise,
(9.18)
for
Î³EVB = Ïƒ
A
M

1 + Ï„
 
1 + Î±
Ï„

,
Ë˜Î³EVB
h
= Î³h
2
â›âœâœâœâœâœâœâœâœâ1 âˆ’(M + L)Ïƒ2
Î³2
h
+
>
@â›âœâœâœâœâ1 âˆ’(M + L)Ïƒ2
Î³2
h
ââŸâŸâŸâŸâ 
2
âˆ’4LMÏƒ4
Î³4
h
ââŸâŸâŸâŸâŸâŸâŸâŸâ .
Here
Î± = L
M
(0 < Î± â‰¤1),
is the â€œsquarednessâ€ of the observed matrix V, and Ï„ = Ï„(Î±) is the unique
zero-cross point of the following function:
Î (Ï„; Î±) = Î¦ (Ï„) + Î¦
! Ï„
Î±
"
,
where
Î¦(z) = log(z + 1)
z
âˆ’1
2.
(9.19)
Summarizing Lemmas 6.14, 6.16, and 6.19, the EVB posterior is com-
pletely speciï¬ed by Eq. (9.10) with the variational parameters given as follows:
If Î³h â‰¥Î³EVB,
ah = Â±
.
Ë˜Î³EVB
h
Î´EVB
h
,
bh = Â±
>
@
Ë˜Î³EVB
h
Î´EVB
h
,
(9.20)
Ïƒ2
ah = Ïƒ2Î´EVB
h
Î³h
,
Ïƒ2
bh =
Ïƒ2
Î³hÎ´EVB
h
,
cahcbh =
A
Î³hË˜Î³EVB
h
LM
,
(9.21)
where
Î´EVB
h
=
A
MË˜Î³EVB
h
LÎ³h
â›âœâœâœâœâ1 +
LÏƒ2
Î³hË˜Î³EVB
h
ââŸâŸâŸâŸâ ,
(9.22)
and otherwise
ah = 0,
bh = 0,
Ïƒ2
ah =
.
Î¶EVB,
Ïƒ2
bh =
.
Î¶EVB,
cahcbh =
.
Î¶EVB,
(9.23)
where
Î¶EVB â†’+0.
(9.24)
To use the preceding result, we need to prepare a table of Ï„ by computing
the zero-cross point of Eq. (9.19) as a function of Î±. A simple approximation
Ï„ â‰ˆz âˆšÎ± â‰ˆ2.5129 âˆšÎ± is a reasonable alternative (see Figure 6.4).

240
9 Global Solver for Matrix Factorization
For noise variance estimation, we can use Theorems 8.1 and 8.2, derived in
Chapter 8. Speciï¬cally, after performing the SVD (9.6), we ï¬rst estimate the
noise variance by solving the following problem:
Ïƒ2 EVB = argmin
Ïƒ2
Î©(Ïƒâˆ’2),
(9.25)
s.t.
max
â›âœâœâœâœâœâœâÏƒ2
H+1,
L
h=H+1 Î³2
h
M

L âˆ’H
 
ââŸâŸâŸâŸâŸâŸâ â‰¤Ïƒ2 â‰¤
1
LM
L

h=1
Î³2
h,
(9.26)
where
Î©(Ïƒâˆ’2) = 1
L
â›âœâœâœâœâœâ
H

h=1
Ïˆ
â›âœâœâœâœâ
Î³2
h
MÏƒ2
ââŸâŸâŸâŸâ +
L

h=H+1
Ïˆ0
â›âœâœâœâœâ
Î³2
h
MÏƒ2
ââŸâŸâŸâŸâ 
ââŸâŸâŸâŸâŸâ ,
(9.27)
Ïˆ (x) = Ïˆ0 (x) + Î¸

x > x
 
Ïˆ1 (x) ,
Ïˆ0 (x) = x âˆ’log x,
Ïˆ1 (x) = log (Ï„(x; Î±) + 1) + Î± log
Ï„(x; Î±)
Î±
+ 1

âˆ’Ï„(x; Î±),
x =

1 + Ï„
 
1 + Î±
Ï„

,
Ï„(x; Î±) = 1
2

x âˆ’(1 + Î±) +
.
(x âˆ’(1 + Î±))2 âˆ’4Î±

,
Ïƒ2
h =
â§âªâªâªâªâªâªâ¨âªâªâªâªâªâªâ©
âˆ
for h = 0,
Î³2
h
Mx
for h = 1,. . . , L,
0
for h = L + 1,
H = min
!D
L
1 + Î±
E
âˆ’1, H
"
.
Problem (9.25) is simply a one-dimensional search for the minimizer of the
function Î©(Ïƒâˆ’2), which is typically smooth. Note also that, if the matrix size is
large enough, Corollary 8.21 states that any local minimizer is accurate enough
to estimate the correct rank. Given the estimated noise variance Ïƒ2 = Ïƒ2 EVB,
Eq. (9.18) gives the EVB solution.
Algorithm 16 summarizes the procedure explained in the preceding discus-
sion. This algorithm gives the global solution, provided that the global solution
to the one-dimensional search problem (9.25) is attained. If the noise variance
Ïƒ2 is known, we should simply skip Step 4.

9.2 Global EVB Solver for Fully Observed MF
241
Algorithm 16 Global EVB solver for fully observed matrix facrtorization.
1: Transpose V â†’VâŠ¤if L > M, and set H (â‰¤L) to a sufï¬ciently large value.
2: Refer to the table of Ï„(Î±) at Î± = L/M (or use a simple approximation
Ï„ â‰ˆ2.5129 âˆšÎ±).
3: Compute the SVD (9.6) of V.
4: Solve the one-dimensional search problem (9.25) to get Ïƒ2 EVB.
5: Apply Eq. (9.18) to get the EVB estimator {Î³EVB
h
}H
h=1 for Ïƒ2 = Ïƒ2 EVB.
6: If necessary, compute the variational parameters and the hyperparameters
by using Eqs. (9.20) through (9.24), which specify the EVB posterior
(9.10). We can also evaluate the free energy by using Eqs. (9.15) and
(9.16), noting that Fh â†’+0 for h such that Î³h < Î³EVB.
Algorithm 17 Iterative EVB solver for fully observed matrix factorization.
1: Transpose V â†’VâŠ¤if L > M, and set H (â‰¤L) to a sufï¬ciently large value.
2: Refer to the table of Ï„(Î±) at Î± = L/M (or use a simple approximation
Ï„ â‰ˆ2.5129 âˆšÎ±).
3: Compute the SVD (9.6) of V.
4: Initialize the noise variance Ïƒ2 EVB to the lower bound in Eq. (9.26).
5: Apply Eq. (9.18) to update the EVB estimator {Î³EVB
h
}H
h=1.
6: Apply Eq. (9.28) to update the noise variance estimator Ïƒ2 EVB.
7: Compute the variational parameters and the hyperparameters by using Eqs.
(9.20) through (9.24).
8: Evaluate the free energy (9.15), noting that Fh â†’+0 for h such that Î³h <
Î³EVB.
9: Iterate Steps 5 through 8 until convergence (until the energy decrease
becomes smaller than a threshold).
Another implementation is to iterate Eq. (9.18) and
Ïƒ2 EVB =
1
LM
â›âœâœâœâœâœâ
L

l=1
Î³2
l âˆ’
H

h=1
Î³hÎ³EVB
h
ââŸâŸâŸâŸâŸâ 
(9.28)
in turn. Note that Eq. (9.28) was derived in Corollary 8.3 and can be used
as an update rule for the noise variance estimator, given the current EVB
estimators {Î³EVB
h
}H
h=1. Although it is not guaranteed, this iterative algorithm
(Algorithm 17) tends to converge to the global solution if we initialize the
noise variance Ïƒ2 EVB to be sufï¬ciently small (Nakajima et al., 2015). We
recommend to initialize it to the lower-bound given in Eq. (9.26).

242
9 Global Solver for Matrix Factorization
Algorithm 18 Local-EVB solver for fully observed matrix factorization.
1: Transpose V â†’VâŠ¤if L > M, and set H (â‰¤L) to a sufï¬ciently large value.
2: Refer to the table of Ï„(Î±) at Î± = L/M (or use a simple approximation
Ï„ â‰ˆ2.5129 âˆšÎ±).
3: Compute the SVD (9.6) of V.
4: Initialize the noise variance Ïƒ2 localâˆ’EVB to the lower-bound in Eq. (9.26).
5: Apply Eq. (9.29) to update the local-EVB estimator {Î³localâˆ’EVB
h
}H
h=1.
6: Apply Eq. (9.30) to update the noise variance estimator Ïƒ2 localâˆ’EVB.
7: Compute the variational parameters and the hyperparameters by using
Eqs. (9.20) through (9.24).
8: Evaluate the free energy (9.15), noting that Fh â†’+0 for h such that
Î³h < Î³localâˆ’EVB.
9: Iterate Steps 5 through 8 until convergence (until the energy decrease
becomes smaller than a threshold).
Finally, we introduce an iterative solver, in Algorithm 18, for the local-EVB
estimator (6.131), which iterates the following updates:
Î³localâˆ’EVB
h
=
â§âªâªâ¨âªâªâ©
Ë˜Î³EVB
h
if Î³h â‰¥Î³localâˆ’EVB,
0
otherwise,
(9.29)
Ïƒ2 localâˆ’EVB =
1
LM
â›âœâœâœâœâœâ
L

l=1
Î³2
l âˆ’
H

h=1
Î³hÎ³localâˆ’EVB
h
ââŸâŸâŸâŸâŸâ ,
(9.30)
where
Î³localâˆ’EVB â‰¡
 âˆš
L +
âˆš
M
 
Ïƒ
(9.31)
is the local-EVB threshold, deï¬ned by Eq. (6.127). If we initialize the
noise variance Ïƒ2 localâˆ’EVB to be sufï¬ciently small, this algorithm tends to
retain the positive local-EVB solution for each h if it exists, and therefore
does not necessarily converge to the global EVB solution. The interesting
relation between the local-EVB estimator and the overlap (OL) method (Hoyle,
2008), an alternative dimensionality selection method based on the Laplace
approximation, was discussed in Section 8.7.
9.3 Empirical Comparison with the Standard VB Algorithm
Here we see how efï¬cient the global solver (Algorithm 16) is in comparison
with the standard VB algorithm (Algorithm 1 in Section 3.1) on artiï¬cial and
benchmark data.

9.3 Empirical Comparison with the Standard VB Algorithm
243
9.3.1 Experiment on Artiï¬cial Data
We ï¬rst created an artiï¬cial data set (Artiï¬cial1) with the data matrix size
L = 100 and M = 300, and the true rank Hâˆ—= 20. We randomly drew
true matrices Aâˆ—âˆˆRMÃ—Hâˆ—and Bâˆ—âˆˆRLÃ—Hâˆ—so that each entry of Aâˆ—and
Bâˆ—follows Gauss1(0, 1), where Gauss1(Î¼, Ïƒ2) denotes the one-dimensional
Gaussian distribution with mean Î¼ and variance Ïƒ2. An observed matrix V
was created by adding noise subject to Gauss1(0, 1) to each entry of Bâˆ—Aâˆ—âŠ¤.
We evaluated the performance under the complete empirical Bayesian
scenario, where all variational parameters and hyperparameters are estimated
from observation. We used the full-rank model (i.e., H
=
min(L, M)),
expecting that irrelevant H âˆ’Hâˆ—components will be automatically trimmed
out by the automatic relevance determination (ARD) effect (see Chapters 7
and 8).
We compare the global solver (Algorithm 16) and the standard VB algo-
rithm (Algorithm 1 in Section 3.1), and show the free energy, the computation
time, and the estimated rank over iterations in Figure 9.1. For the standard VB
algorithm, initial values were set in the following way: A and B are randomly
created so that each entry follows Gauss1(0, 1). Other variables are set to
Î£A = Î£B = CA = CB = IH and Ïƒ2 = 1. Note that we rescale V so that
âˆ¥Vâˆ¥2
Fro /(LM) = 1, before starting iterations. We ran the standard algorithm 10
0
50
100
150
200
250
Iteration
1.8
1.85
1.9
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(a) Free energy
0
50
100
150
200
250
Iteration
0
20
40
60
80
Time(sec)
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(b) Computation time
0
50
100
150
200
250
Iteration
0
20
40
60
80
100
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(c) Estimated rank
Figure 9.1 Experimental results on the Artiï¬cial1 data, where the data matrix size
is L = 100 and M = 300, and the true rank is Hâˆ—= 20.

244
9 Global Solver for Matrix Factorization
0
50
100
150
200
250
Iteration
2.4
2.6
2.8
3
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(a) Free energy
0
50
100
150
200
250
Iteration
0
10
20
30
40
Time(sec)
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(b) Computation time
0
50
100
150
200
250
Iteration
0
20
40
60
80
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(c) Estimated rank
Figure 9.2 Experimental results on the Artiï¬cial2 data set (L = 70, M = 300, and
Hâˆ—= 40).
times, starting from different initial points, and each trial is plotted by a solid
curve labeled as â€œStandard(iniRan)â€ in Figure 9.1.
The global solver has no iteration loop, and therefore the corresponding
dashed line labeled as â€œGlobalâ€ is constant over iterations. We see that the
global solver ï¬nds the true rank 
H = Hâˆ—= 20 immediately (âˆ¼0.1 sec on
average over 10 trials), while the standard iterative algorithm does not converge
in 60 sec.
Figure 9.2 shows experimental results on another artiï¬cial data set (Artiï¬-
cial2) where L = 70, M = 300, and Hâˆ—= 40. In this case, all the 10 trials of the
standard algorithm are trapped at local minima. We empirically observed that
the local minimum problem tends to be more critical when Hâˆ—is large (close
to H).
We also evaluated the standard algorithm with different initialization
schemes. The curve labeled as â€œStandard(iniML)â€ indicates the standard
algorithm starting from the maximum likelihood (ML) solution: (ah,bh) =
( âˆšÎ³hÏ‰ah, âˆšÎ³hÏ‰bh). The initial values for other variables are the same as the
random initialization. Figures 9.1 and 9.2 show that the ML initialization
generally makes convergence faster than the random initialization, but suffers
from the local minimum problem more severelyâ€”it tends to converge to a
worse local minimum.

9.3 Empirical Comparison with the Standard VB Algorithm
245
0
50
100
150
200
250
Iteration
0
0.5
1
1.5
2
2.5
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(a) Free energy
0
50
100
150
200
250
Iteration
0
1
2
3
Time(sec)
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(b) Computation time
0
50
100
150
200
250
Iteration
0
5
10
15
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(c) Estimated rank
Figure 9.3 Experimental results on the Glass data set (L = 9, M = 214).
We observed that starting from a small noise variance tends to alleviate
the local minimum problem at the expense of slightly slower convergence.
The curve labeled as â€œStandard(iniMLSS)â€ indicates the standard algorithm
starting from the ML solution with a small noise variance Ïƒ2 = 0.0001. We see
in Figures 9.1 and 9.2 that this initialization improves the quality of solutions,
and successfully ï¬nds the true rank for these artiï¬cial data sets. However, we
will show in Section 9.3.2 that this scheme still suffers from the local minimum
problem on benchmark datasets.
9.3.2 Experiment on Benchmark Data
Figures 9.3 through 9.5 show the experimental results on the Glass, the
Satimage, and the Spectf data sets available from the University of California,
Irvine (UCI) repository (Asuncion and Newman, 2007). A similar tendency
to the artiï¬cial data experiment (Figures 9.1 and 9.2) is observed: â€œStan-
dard(iniRan)â€ converges slowly, and is often trapped at a local minimum
with a wrong estimated rank;1 â€œStandard(iniML)â€ converges slightly faster but
to a worse local minimum; and â€œStandard(iniMLSS)â€ tends to give a better
solution. Unlike the artiï¬cial data experiment, â€œStandard(iniMLSS)â€ fails to
1 Since the true ranks of the benchmark data sets are unknown, we mean by a wrong rank a rank
different from the one giving the lowest free energy.

246
9 Global Solver for Matrix Factorization
0
50
100
150
200
250
Iteration
2.4
2.6
2.8
3
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(a) Free energy
0
50
100
150
200
250
Iteration
0
2000
4000
6000
8000
Time(sec)
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(b) Computation time
0
50
100
150
200
250
Iteration
0
10
20
30
40
50
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(c) Estimated rank
Figure 9.4 Experimental results on the Satimage data set (L = 36, M = 6435).
0
50
100
150
200
250
Iteration
3
3.1
3.2
3.3
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(a) Free energy
0
50
100
150
200
250
Iteration
0
5
10
15
20
25
Time(sec)
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(b) Computation time
0
50
100
150
200
250
Iteration
0
10
20
30
40
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(c) Estimated rank
Figure 9.5 Experimental results on the Spectf data set (L = 44, M = 267).
ï¬nd the correct rank in these benchmark data sets. We also conducted experi-
ments on other benchmark data sets and found that the standard VB algorithm
generally converges slowly, and sometimes suffers from the local minimum
problem, while the global solver gives the global solution immediately.

9.4 Extension to Nonconjugate MF with Missing Entries
247
0
50
100
150
200
250
Iteration
â€“2.0435
â€“2.0434
â€“2.0433
â€“2.0432
â€“2.0431
â€“2.043
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(a) Free energy
0
50
100
150
200
250
Iteration
0
0.02
0.04
0.06
0.08
Time(sec)
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(b) Computation time
0
50
100
150
200
250
Iteration
0
1
2
3
4
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(c) Estimated rank
Figure 9.6 Experimental results on the Concrete Slump Test data set (an RRR task
with L = 3, M = 7).
Finally, we applied EVB learning to the reduced rank regression (RRR)
model (see Section 3.1.2), of which the model likelihood is given by
Eq. (3.36). Figure 9.6 shows the results on the Concrete Slump Test data set,
where we centered the L = 3-dimensional outputs and prewhitened the M = 7-
dimensional inputs. We also standardized the outputs so that the variance of
each element is equal to one. Note that we cannot directly apply Algorithm
16 for the RRR model. Instead, we use Algorithm 16 with a ï¬xed noise
variance (skipping Step 4) and apply one-dimensional search to minimize the
free energy (3.42), in order to estimate the rescaled noise variance Ïƒ2. For
the standard VB algorithm, the rescaled noise variance should be updated by
Eq. (3.43), instead of Eq. (3.28). The original noise variance Ïƒâ€²2 is recovered
by Eq. (3.40) for both cases.
Overall, the global solver showed excellent performance over the standard
VB algorithm.
9.4 Extension to Nonconjugate MF with Missing Entries
The global solvers introduced in Section 9.1 can be directly applied only for
the fully observed isotropic Gaussian likelihood (9.1). However, the global
solver can be used as a subroutine to develop efï¬cient algorithms for more

248
9 Global Solver for Matrix Factorization
general cases. In this section, we introduce the approach by Seeger and
Bouchard (2012), where an iterative singular value shrinkage algorithm was
proposed, based on the global VB solver (Algorithm 15) and local variational
approximation (Section 2.1.7).
9.4.1 Nonconjugate MF Model
Consider the following model:
p(V|A, B) =
L

l=1
M

m=1
Ï†l,m(Vl,m,b
âŠ¤
l am),
(9.32)
p(A) âˆexp

âˆ’1
2tr

ACâˆ’1
A AâŠ¤ 
,
(9.33)
p(B) âˆexp

âˆ’1
2tr

BCâˆ’1
B BâŠ¤ 
,
(9.34)
where Ï†l,m(v|u) is a function of v and u, and satisfy
âˆ’âˆ‚2 log Ï†l,m(v, u)
âˆ‚u2
â‰¤1
Ïƒ2
(9.35)
for any l, m, v, and u.
The function Ï†l,m(v, u) corresponds to the model distribution of the (l, m)th
entry v of V parameterized by u. If
Ï†l,m(v, u) = Gauss1(v; u, Ïƒ2)
(9.36)
for all l and m, the model (9.32) through (9.34) is reduced to the fully observed
isotropic Gaussian MF model (9.1)â€“(9.3), and the Hessian,
âˆ’âˆ‚2 log Ï†l,m(v, u)
âˆ‚u2
= 1
Ïƒ2 ,
of the negative log-likelihood is a constant with respect to v and u.
The model (9.32) through (9.34) can cover the case with missing entries by
setting the noise variance in Eq. (9.36) to Ïƒ2 â†’âˆfor the unobserved entries
(the condition (9.35) is tight for the smallest Ïƒ2). Other one-dimensional dis-
tributions, including the Bernoulli distribution with sigmoid parameterization
and the Poisson distribution, satisfy the condition (9.35) for a certain Ïƒ2, which
will be introduced in Section 9.4.3.

9.4 Extension to Nonconjugate MF with Missing Entries
249
9.4.2 Local Variational Approximation for Non-conjugate MF
The VB learning problem (9.4) minimizes the free energy, which can be
written as
F(r) =
/
log
rA(A)rB(B)
L
l=1
M
m=1 Ï†l,m(Vl,m,b
âŠ¤
l am)p(A)p(B)
0
rA(A)rB(B)
.
(9.37)
In order to make the global VB solver applicable as a subroutine, we instead
solve the following joint minimization problem,
r = argmin
r,Î
F(r, Î)
s.t.
r(A, B) = rA(A)rB(B),
(9.38)
of an upper-bound of the free energy,
F â‰¤F(r, Î) â‰¡
/
log
rA(A)rB(B)
L
l=1
M
m=1 Ï†l,m(Vl,m,b
âŠ¤
l am, Îl,m)p(A)p(B)
0
rA(A)rB(B)
,
(9.39)
where
Ï†l,m(v, u, Î¾) â‰¤Ï†l,m(v, u)
(9.40)
is a lower-bound of the likelihood parameterized with variational parameters
Î âˆˆRLÃ—M.
The condition (9.35) allows us to form a parametric lower-bound in the
(unnormalized) isotropic Gaussian form, which we derive as follows. Any
function f(x) with bounded curvature âˆ‚2 f
âˆ‚x2 â‰¤Îº can be upper-bounded by the
following quadratic function:
f(x) â‰¤Îº(x âˆ’Î¾)2 + âˆ‚f
âˆ‚x
x=Î¾
(x âˆ’Î¾) + f(Î¾)
for any Î¾ âˆˆR.
(9.41)
Therefore, it holds that, for any Î¾ âˆˆR,
âˆ’log Ï†l,m(v, u) â‰¤(u âˆ’Î¾)2
2Ïƒ2
+ g(v, Î¾)(u âˆ’Î¾) âˆ’log Ï†l,m(v, Î¾),
(9.42)
where
g(v, Î¾) = âˆ’âˆ‚log Ï†l,m(v, u)
âˆ‚u
u=Î¾
.
The left graph in Figure 9.7 shows the parametric quadratic upper-bounds
(9.42) for the Bernoulli likelihood with sigmoid parameterization.

250
9 Global Solver for Matrix Factorization
â€“5
5
0
0
2
4
6
8
10
â€“5
5
0
0
0.2
0.4
0.6
0.8
1
Figure 9.7 Parametric quadratic (Gaussian-form) bounds for the Bernoulli like-
lihood with sigmoid parameterization, Ï†(v, u) = evu/(1 + eu), for v = 1. Left:
the negative log-likelihood (the left-hand side of Eq. (9.42)) and its quadratic
upper-bounds (the right-hand side of Eq. (9.42)) for Î¾ = âˆ’2.5, 0.0, 2.5. Right:
the likelihood function Ï†(1, Î¾) and its Gaussian-form lower-bounds (9.43) for
Î¾ = âˆ’2.5, 0.0, 2.5.
Since log(Â·) is a monotonic function, we can adopt the following parametric
lower-bound of Ï†l,m(v, u):
Ï†l,m(u, Î¾) = exp

âˆ’
(u âˆ’Î¾)2
2Ïƒ2
+ g(v, Î¾)(u âˆ’Î¾) âˆ’log Ï†l,m(v, Î¾)

= Ï†l,m(v, Î¾) exp

âˆ’1
2Ïƒ2

(u âˆ’Î¾)2 + 2Ïƒ2g(v, Î¾)(u âˆ’Î¾)
  
= Ï†l,m(v, Î¾) exp
!
âˆ’1
2Ïƒ2
!
u âˆ’Î¾ + Ïƒ2g(v, Î¾)
 2 âˆ’

Ïƒ2g(v, Î¾)
 2""
= Ï†l,m(v, Î¾) exp
 Ïƒ2
2 g2(v, Î¾)
 
exp
!
âˆ’1
2Ïƒ2

(Î¾ âˆ’Ïƒ2g(v, Î¾)) âˆ’u
 2"
=
âˆš
2Ï€Ïƒ2Ï†l,m(v, Î¾) exp
 Ïƒ2
2 g2(v, Î¾)
 
Gauss1

Î¾ âˆ’Ïƒ2g(v, Î¾); u, Ïƒ2 
.
(9.43)
The right graph in Figure 9.7 shows the parametric Gaussian-form lower-
bounds (9.43) for the Bernoulli likelihood with sigmoid parameterization.
Substituting Eq. (9.43) into Eq. (9.39) gives
F(r, Î) = âˆ’
L

l=1
M

m=1
1
2 log(2Ï€Ïƒ2) + log Ï†l,m(Vl,m, Îl,m) + Ïƒ2
2 g2(Vl,m, Îl,m)

+
*
log
rA(A)rB(B)
L
l=1
M
m=1 Gauss1(Îl,mâˆ’Ïƒ2g(Vl,m,Îl,m);b
âŠ¤
l am,Ïƒ2)p(A)p(B)
+
rA(A)rB(B)
= âˆ’
L

l=1
M

m=1
1
2 log(2Ï€Ïƒ2) + log Ï†l,m(Vl,m, Îl,m) + Ïƒ2
2 g2(Vl,m, Îl,m)

+
/
log
rA(A)rB(B)
(2Ï€Ïƒ2)âˆ’LM/2 exp

âˆ’
1
2Ïƒ2 âˆ¥Ë˜Vâˆ’BAâŠ¤âˆ¥2
Fro
 
p(A)p(B)
0
rA(A)rB(B)
,
(9.44)
where Ë˜V âˆˆRLÃ—M is a matrix such that
Ë˜Vl,m = Îl,m âˆ’Ïƒ2g(Vl,m, Îl,m).
(9.45)

9.4 Extension to Nonconjugate MF with Missing Entries
251
The ï¬rst term in Eq. (9.44) does not depend on r and the second term is equal
to the free energy of the fully observed isotropic Gaussian MF model with the
observed matrix V replaced with Ë˜V. Therefore, given the variational parameter
Î, we can partially solve the minimization problem (9.38) with respect to r by
applying the global VB solver (Algorithm 15). The solution is Gaussian in the
following form:
r(A, B) = rA(A)rB(B),
where
(9.46)
rA(A) = MGaussM,H(A; A, IM âŠ—Î£A) âˆexp
â›âœâœâœâœâœââˆ’
tr
!
(Aâˆ’A)Î£
âˆ’1
A (Aâˆ’A)âŠ¤
"
2
ââŸâŸâŸâŸâŸâ ,
(9.47)
rB(B) = MGaussL,H(B; B, IL âŠ—Î£B) âˆexp
â›âœâœâœâœâœââˆ’
tr
!
(Bâˆ’B)Î£
âˆ’1
B (Bâˆ’B)âŠ¤
"
2
ââŸâŸâŸâŸâŸâ .
(9.48)
Here the mean and the covariance parameters A, Î£A, B, Î£B are another set of
variational parameters.
Given the optimal r speciï¬ed by Eq. (9.46), the free energy bound (9.44) is
written (as a function of Î) as follows:
min
r
F(r, Î) = âˆ’
L

l=1
M

m=1

log Ï†l,m(Vl,m, Îl,m) + Ïƒ2
2 g2(Vl,m, Îl,m)

âˆ’
1
2Ïƒ2

âˆ¥Ë˜V âˆ’BAâŠ¤âˆ¥2
Fro

rA(A)rB(B) + const.
= âˆ’
L

l=1
M

m=1

log Ï†l,m(Vl,m, Îl,m) + Ïƒ2
2 g2(Vl,m, Îl,m)

âˆ’
1
2Ïƒ2 âˆ¥Ë˜V âˆ’BA
âŠ¤âˆ¥2
Fro + const.
= âˆ’
L

l=1
M

m=1

log Ï†l,m(Vl,m, Îl,m) + Ïƒ2
2 g2(Vl,m, Îl,m)

âˆ’
1
2Ïƒ2
L
l=1
M
m=1
 Ë˜Vl,m âˆ’
Ul,m
 2 + const.,
(9.49)
where
U = BA
âŠ¤.
The second-to-last equation in Eq. (9.43), together with Eq. (9.45), implies
that
log Ï†l,m(
Ul,m, Îl,m) = log Ï†l,m(Vl,m, Îl,m) + Ïƒ2
2 g2(Vl,m, Îl,m) âˆ’
1
2Ïƒ2
 Ë˜Vl,m âˆ’
Ul,m
 2 ,
with which Eq. (9.49) is written as
min
r
F(r, Î) = âˆ’L
l=1
M
m=1 log Ï†l,m(
Ul,m, Îl,m) + const.
(9.50)

252
9 Global Solver for Matrix Factorization
Algorithm 19 Iterative singular value shrinkage algorithm for nonconjugate
MF (with missing entries).
1: Set the noise variance Ïƒ2 with which the condition (9.35) tightly holds,
and initialize the variational parameters to Î = 0(L,M).
2: Compute Ë˜V by Eq. (9.45).
3: Compute the VB posterior (9.46) by applying the global solver (Algorithm
15) with Ë˜V substituted for V.
4: Update Î by Eq. (9.51).
5: Iterate Steps 2 through 4 until convergence.
Since log Ï†l,m(u, Î¾) is the quadratic upper-bound (the right-hand side in
Eq. (9.42)) of âˆ’log Ï†l,m(v, u), which is tight at u = 
Ul,m when Î¾ = 
Ul,m, the
minimizer of Eq. (9.50) with respect to Î is given by
Î â‰¡argmin
Î
min
r
F(r, Î) = U.
(9.51)
In summary, to solve the joint minimization problem (9.38), we can
iteratively update r and Î. The update of r can be performed by the global
solver (Algorithm 15) with the observed matrix V replaced with Ë˜V, deï¬ned
by Eq. (9.45). The update of Î is simply performed by Eq. (9.51). Algorithm
19 summarizes this procedure, where 0(d1,d2) denotes the d1 Ã— d2 matrix with
all entries equal to zero. Seeger and Bouchard (2012) empirically showed
that this iterative singular value shrinkage algorithm signiï¬cantly outperforms
the MAP solution at comparable computational costs. They also proposed an
efï¬cient way to perform SVD when V is huge but sparsely observed, based on
the techniques proposed by Tomioka et al. (2010).
9.4.3 Examples of Nonconjugate MF
In this subsection, we introduce a few examples of model likelihood Ï†l,m(v, u),
which satisfy Condition (9.35), and give the corresponding derivatives of the
negative log likelihood.
Isotropic Gaussian MF with Missing Entries
If we let
Ï†l,m(v, u) =
â§âªâªâ¨âªâªâ©
Gauss1(v; u, Ïƒ2)
if (l, m) âˆˆÎ›,
1
otherwise,
(9.52)

9.4 Extension to Nonconjugate MF with Missing Entries
253
where Î› denotes the set of observed entries, the model distribution (9.32)
corresponds to the model distribution (3.44) of MF with missing entries. The
ï¬rst and the second derivatives of the negative log likelihood are given as
follows:
âˆ’âˆ‚log Ï†l,m(v, u)
âˆ‚u
=
â§âªâªâ¨âªâªâ©
1
Ïƒ2 (u âˆ’v)
if (l, m) âˆˆÎ›,
0
otherwise,
âˆ’âˆ‚2 log Ï†l,m(v, u)
âˆ‚u2
=
â§âªâªâ¨âªâªâ©
1
Ïƒ2
if (l, m) âˆˆÎ›,
0
otherwise.
(9.53)
Bernoulli MF with Sigmoid Parameterization
The Bernoulli distribution with sigmoid parameterization is suitable for binary
observations, i.e., V âˆˆ{0, 1}LÃ—M:
Ï†l,m(v, u) =
â§âªâªâªâ¨âªâªâªâ©
evu
1 + eu
if (l, m) âˆˆÎ›,
1
otherwise.
(9.54)
The ï¬rst and the second derivatives are given as follows:
âˆ’âˆ‚log Ï†l,m(v, u)
âˆ‚u
=
â§âªâªâ¨âªâªâ©
1
1+eâˆ’u âˆ’v
if (l, m) âˆˆÎ›,
0
otherwise,
âˆ’âˆ‚2 log Ï†l,m(v, u)
âˆ‚u2
=
â§âªâªâ¨âªâªâ©
1
(1+eâˆ’u)(1+eu)
if (l, m) âˆˆÎ›,
0
otherwise.
(9.55)
It holds that
âˆ’âˆ‚2 log Ï†l,m(v, u)
âˆ‚u2
â‰¤1
4,
and therefore, the noise variance should be set to Ïƒ2 = 4, which satisï¬es the
condition (9.35). Figure 9.7 was depicted for this model.
Poisson MF
The Poisson distribution is suitable for count data, i.e., V âˆˆ{0, 1, 2,. . .}LÃ—M:
Ï†l,m(v, u) =
â§âªâªâ¨âªâªâ©
Î»v(u)eâˆ’Î»(u)
if (l, m) âˆˆÎ›,
1
otherwise,
(9.56)
where Î»(u) is the link function. Since a common choice Î»(u) = eu for the link
function gives unbounded curvature for large u, Seeger and Bouchard (2012)

254
9 Global Solver for Matrix Factorization
proposed to use another link function Î»(u) = log(1 + eu). The ï¬rst derivative is
given as follows:
âˆ’âˆ‚log Ï†l,m(v, u)
âˆ‚u
=
â§âªâªâ¨âªâªâ©
1
1+eâˆ’u

1 âˆ’
v
Î»(u)
 
if (l, m) âˆˆÎ›,
0
otherwise.
(9.57)
It was conï¬rmed that the second derivative is upper-bounded as
âˆ’âˆ‚2 log Ï†l,m(v, u)
âˆ‚u2
â‰¤1
4 + 0.17v,
and therefore, the noise variance should be set to
Ïƒ2 =
1
1/4 + 0.17 maxl,m Vl,m
.
Since the bound can be loose if some of the entries Vl,m of the observed matrix
are huge compared to the others, overly large counts should be clipped.

10
Global Solver for Low-Rank
Subspace Clustering
The nonasymptotic theory, described in Chapter 6, for fully observed matrix
factorization (MF) has been extended to other bilinear models. In this chapter,
we introduce exact and approximate global variational Bayesian (VB) solvers
(Nakajima et al., 2013c) for low-rank subspace clustering (LRSC).
10.1 Problem Description
The LRSC model, introduced in Section 3.4, is deï¬ned as
p(V|Aâ€², Bâ€²) âˆexp

âˆ’1
2Ïƒ2
###V âˆ’VBâ€² Aâ€²âŠ¤###2
Fro

,
(10.1)
p(Aâ€²) âˆexp

âˆ’1
2tr(Aâ€²Câˆ’1
A Aâ€²âŠ¤)

,
(10.2)
p(Bâ€²) âˆexp

âˆ’1
2tr(Bâ€²Câˆ’1
B Bâ€²âŠ¤)

,
(10.3)
where V âˆˆRLÃ—M is an observation matrix, and Aâ€² âˆˆRMÃ—H and Bâ€² âˆˆRMÃ—H for
H â‰¤min(L, M) are the parameters to be estimated. Note that in this chapter
we denote the original parameters Aâ€² and Bâ€² with primes for convenience. We
assume that hyperparameters
CA = Diag(c2
a1,. . . , c2
aH),
CB = Diag(c2
b1,. . . , c2
bH),
are diagonal and positive deï¬nite. The LRSC model is similar to MF. The only
difference is that the product Bâ€² Aâ€²âŠ¤of the parameters is further multiplied
by V in Eq. (10.1). Accordingly, we can hope that similar analysis could be
applied to LRSC, providing a global solver for LRSC.
255

256
10 Global Solver for Low-Rank Subspace Clustering
We ï¬rst transform the parameters as
A â†Î©rightâŠ¤
V
Aâ€²,
B â†Î©rightâŠ¤
V
Bâ€²,
where
V = Î©left
V Î“VÎ©rightâŠ¤
V
(10.4)
is the singular value decomposition (SVD) of V. Here, Î©left
V
âˆˆRLÃ—L and Î©right
V
âˆˆ
RMÃ—M are orthogonal matrices, and Î“V âˆˆRLÃ—M is a (possibly nonsquare)
diagonal matrix with nonnegative diagonal entries aligned in nonincreasing
order, i.e., Î³1 â‰¥Î³2 â‰¥Â· Â· Â· â‰¥Î³min(L,M). After this transformation, the LRSC
model (10.1) through (10.3) is rewritten as
p(Î“V|A, B) âˆexp

âˆ’1
2Ïƒ2
###Î“V âˆ’Î“V BAâŠ¤)
###2
Fro

,
(10.5)
p(A) âˆexp

âˆ’1
2tr(ACâˆ’1
A AâŠ¤)

,
(10.6)
p(B) âˆexp

âˆ’1
2tr(BCâˆ’1
B BâŠ¤)

.
(10.7)
The transformation (10.4) does not affect much the derivation of the VB
learning algorithm. The following summarizes the result obtained in Section
3.4 with the transformed parameters A and B. The solution of the VB learning
problem,
r = argmin
r
F(r)
s.t.
r(A, B) = rA(A)rB(B),
where
(10.8)
F =
/
log
rA(A)rB(B)
p(Î“V|A, B)p(A)p(B)
0
rA(A)rB(B)
,
has the following form:
r(A) âˆexp
â›âœâœâœâœâœâœâœâœâœâœâ
âˆ’
tr
!
(A âˆ’A)Î£
âˆ’1
A (A âˆ’A)âŠ¤
"
2
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
,
r(B) âˆexp
â›âœâœâœâœâœâœâœâœââˆ’(Ë˜b âˆ’Ë˜b)âŠ¤Ë˜Î£
âˆ’1
B (Ë˜b âˆ’Ë˜b)
2
ââŸâŸâŸâŸâŸâŸâŸâŸâ ,
(10.9)
for Ë˜b = vec(B) âˆˆRMH, and the free energy can be explicitly written as
2F = LM log(2Ï€Ïƒ2) +
####Î“Vâˆ’Î“VBA
âŠ¤####
2
Fro
Ïƒ2
+ M log det(CA)
det
Î£ A
 + log det(CBâŠ—IM)
det
!Ë˜Î£B
"
âˆ’2MH + tr
2
Câˆ’1
A
!
A
âŠ¤A + MÎ£ A
"3
+ tr
2
Câˆ’1
B B
âŠ¤B
3
+ tr
2
(Câˆ’1
B âŠ—IM)Ë˜Î£B
3
+ tr
)
Ïƒâˆ’2Î“âŠ¤
VÎ“V

âˆ’BA
âŠ¤AB
âŠ¤+
*
B(A
âŠ¤A + MÎ£ A)BâŠ¤+
r(B)
1
.
(10.10)

10.1 Problem Description
257
Therefore, the variational parameters (A, B, Î£A, Ë˜Î£B) can be obtained by solv-
ing the following problem:
Given CA, CB âˆˆDH
++, Ïƒ2 âˆˆR++,
min
(A,B,Î£A,Ë˜Î£B)
F,
(10.11)
s.t.
A, B âˆˆRMÃ—H, Î£A âˆˆSH
++, Ë˜Î£B âˆˆSMH
++ .
(10.12)
The stationary conditions with respect to the variational parameters are
given by
A = 1
Ïƒ2 Î“âŠ¤
VÎ“VBÎ£ A,
(10.13)
Î£ A = Ïƒ2 !
BâŠ¤Î“âŠ¤
VÎ“V B

r(B) + Ïƒ2Câˆ’1
A
"âˆ’1
,
(10.14)
Ë˜b =
Ë˜Î£B
Ïƒ2 vec

Î“âŠ¤
VÎ“VA
 
,
(10.15)
Ë˜Î£B = Ïƒ2 !
(A
âŠ¤A + MÎ£ A) âŠ—Î“âŠ¤
VÎ“V + Ïƒ2(Câˆ’1
B âŠ—IM)
"âˆ’1
.
(10.16)
For empirical VB (EVB) learning, we solve the problem,
Given Ïƒ2 âˆˆR++,
min
(A,B,Î£A,Ë˜Î£B,CA,CB)
F
(10.17)
subject to
A, B âˆˆRMÃ—H, Î£A âˆˆSH
++, Ë˜Î£B âˆˆSMH
++ , CA, CB âˆˆDH
++,
(10.18)
for which the stationary conditions with respect to the hyperparameters are
given by
c2
ah =
###ah
###2 /M +
Î£ A
 
h,h ,
(10.19)
c2
bh =
!####bh
####
2
+ tr
!
Î£
(h,h)
B
""
/M,
(10.20)
Ïƒ2 =
tr

Î“âŠ¤
VÎ“V

IM âˆ’2BA
âŠ¤+
*
B(A
âŠ¤A + MÎ£ A)BâŠ¤+
r(B)

LM
.
(10.21)
In deriving the global VB solution of fully observed MF in Chapter 6, the
following two facts were essential. First, a large portion of the degrees of
freedom of the original variational parameters are irrelevant (see Section 6.3),
and the optimization problem can be decomposed into subproblems, each of
which has only a small number of unknown variables. Second, the stationary
conditions of each subproblem is written as a polynomial system (a set of

258
10 Global Solver for Low-Rank Subspace Clustering
polynomial equations). These two facts also apply to the LRSC model, which
allows us to derive an exact global VB solver (EGVBS). However, each of
the decomposed subproblems still has too many unknowns whose number is
proportional to the problem size, and therefore EGVBS is still computationally
demanding for typical problem sizes. As an alternative, we also derive an
approximate global VB solver (AGVBS) by imposing an additional constraint,
which allows further decomposition of the problem into subproblems with a
constant number of unknowns.
In this chapter, we ï¬rst ï¬nd irrelevant degrees of freedom of the variational
parameters and decompose the VB learning problem. Then we derive EGVBS
and AGVBS and empirically show their usefulness.
10.2 Conditions for VB Solutions
Let J (â‰¤min(L, M)) be the rank of the observed matrix V. For simplicity,
we assume that no pair of positive singular values of V coincide with each
other, i.e.,
Î³1 > Î³2 > Â· Â· Â· > Î³J > 0.
This holds with probability 1 if V is contaminated with Gaussian noise, as the
LRSC model (10.1) assumes. Since (Î“âŠ¤
VÎ“V)m,mâ€² is zero for m > J or mâ€² > J,
Eqs. (10.13) and (10.15) imply that
Am,h = Bm,h = 0
for
m > J.
(10.22)
Similarly to Lemma 6.1 for the fully observed MF, we can prove the
following lemma:
Lemma 10.1
Any local solution of the problem (10.11) is a stationary point
of the free energy (10.10).
Proof
Since
####Î“V âˆ’Î“VBA
âŠ¤####
2
Fro â‰¥0,
and
tr
)
Î“âŠ¤
VÎ“V

âˆ’BA
âŠ¤AB
âŠ¤+
*
B(A
âŠ¤A + MÎ£ A)BâŠ¤+
r(B)
1
= M Â· tr
2
Î“âŠ¤
VÎ“V

BÎ£ ABâŠ¤
r(B)
3
â‰¥0,

10.4 Proof of Theorem 10.2
259
the free energy (10.10) is lower-bounded as
2F â‰¥âˆ’M log det
Î£A
 
âˆ’log det
!Ë˜Î£B
"
+ tr
2
Câˆ’1
A
!
A
âŠ¤A + MÎ£A
"3
+ tr
2
Câˆ’1
B B
âŠ¤B
3
+ tr
2
(Câˆ’1
B âŠ—IM)Ë˜Î£B
3
+ Ï„,
(10.23)
where Ï„ is a ï¬nite constant. The right-hand side of Eq. (10.23) diverges to +âˆ
if any entry of A or B goes to +âˆor âˆ’âˆ. Also it diverges if any eigenvalue
of Î£A or Ë˜Î£B goes to +0 or âˆ. This implies that no local solution exists on
the boundary of (the closure of) the domain (10.12). Since the free energy is
differentiable in the domain (10.12), any local minimizer is a stationary point.
For any (diagonalized) observed matrix Î“V, the free energy (10.10) can
be ï¬nite, for example, at A = 0M,H, B = 0M,H, Î£A = IH, and Ë˜Î£B = IMH.
Therefore, at least one minimizer always exists, which completes the proof of
Lemma 10.1.
â–¡
Lemma 10.1 implies that Eqs. (10.13) through (10.16) hold at any local
solution.
10.3 Irrelevant Degrees of Freedom
Also similarly to Theorem 6.4, we have the following theorem:
Theorem 10.2
When CACB is nondegenerate (i.e., cahcbh > cahâ€² cbhâ€² for any
pair h < hâ€²), (A, B, Î£A, Ë˜Î£B) are diagonal for any solution of the problem
(10.11). When CACB is degenerate, any solution has an equivalent solution
with diagonal (A, B, Î£A, Ë˜Î£B).
Theorem 10.2 signiï¬cantly reduces the complexity of the optimization
problem, and furthermore makes the problem separable, as seen in
Section 10.5.
10.4 Proof of Theorem 10.2
Similarly to Section 6.4, we separately consider the following three cases:
Case 1 When no pair of diagonal entries of CACB coincide.
Case 2 When all diagonal entries of CACB coincide.
Case 3 When (not all but) some pairs of diagonal entries of CACB coincide.

260
10 Global Solver for Low-Rank Subspace Clustering
10.4.1 Diagonality Implied by Optimality
We can prove the following lemma, which is an extension of Lemma 6.2.
Lemma 10.3
Let Î“, Î©, Î¦ âˆˆRHÃ—H be a nondegenerate diagonal matrix,
an orthogonal matrix, and a symmetric matrix, respectively. Let {Î›(k), Î›â€²(k) âˆˆ
RHÃ—H; k = 1,. . . , K} be arbitrary diagonal matrices, and {Î¨(kâ€²) âˆˆRHÃ—H; kâ€² =
1,. . . , Kâ€²} be arbitrary symmetric matrices. If
G(Î©) = tr
â§âªâªâ¨âªâªâ©Î“Î©Î¦Î©âŠ¤+
K

k=1
Î›(k)Î©Î›â€²(k)Î©âŠ¤+
Kâ€²

kâ€²=1
Î©Î¨(kâ€²)
â«âªâªâ¬âªâªâ­
(10.24)
is minimized or maximized (as a function of Î©, given Î“, Î¦, {Î›(k), Î›â€²(k)}, {Î¨(kâ€²)})
when Î© = IH, then Î¦ is diagonal. Here, K and Kâ€² can be any natural numbers
including K = 0 and Kâ€² = 0 (when the second and the third terms, respectively,
do not exist).
Proof
Let
Î¦ = Î©â€²Î“â€²Î©â€²âŠ¤
(10.25)
be the eigenvalue decomposition of Î¦. Let Î³, Î³â€², {Î»(k)}, {Î»â€²(k)} be the vectors
consisting of the diagonal entries of Î“, Î“â€², {Î›(k)}, {Î›â€²(k)}, respectively, i.e.,
Î“ = Diag(Î³),
Î“â€² = Diag(Î³â€²),
Î›(k) = Diag(Î»(k)),
Î›â€²(k) = Diag(Î»â€²(k)).
Then, Eq. (10.24) can be written as
G(Î©) = tr
â§âªâªâ¨âªâªâ©Î“Î©Î¦Î©âŠ¤+
K

k=1
Î›(k)Î©Î›â€²(k)Î©âŠ¤+
Kâ€²

kâ€²=1
Î©Î¨(kâ€²)
â«âªâªâ¬âªâªâ­
= Î³âŠ¤QÎ³â€² +
K

k=1
Î»(k)âŠ¤RÎ»â€²(k) +
Kâ€²

kâ€²=1
tr
,
Î©Î¨(kâ€²)-
,
(10.26)
where
Q = (Î©Î©â€²) âŠ™(Î©Î©â€²),
R = Î© âŠ™Î©.
Here, âŠ™denotes the Hadamard product.
Using this expression, we will prove that Î¦ is diagonal if Î© = IH minimizes
or maximizes Eq. (10.26). Let us consider a bilateral perturbation Î© = Î” such
that the 2Ã—2 matrix Î”(h,hâ€²) for h  hâ€² consisting of the hth and the hâ€²th columns
and rows form a 2 Ã— 2 orthogonal matrix,
Î”(h,hâ€²) =
cos Î¸
âˆ’sin Î¸
sin Î¸
cos Î¸

,

10.4 Proof of Theorem 10.2
261
and the remaining entries coincide with those of the identity matrix. Then, the
elements of Q become
Qi, j =
â§âªâªâªâªâªâ¨âªâªâªâªâªâ©
(Î©â€²
h, j cos Î¸ âˆ’Î©â€²
hâ€², j sin Î¸)2
if i = h,
(Î©â€²
h, j sin Î¸ + Î©â€²
hâ€², j cos Î¸)2
if i = hâ€²,
Î©â€²2
i, j
otherwise,
and Eq. (10.26) can be written as a function of Î¸ as follows:
G(Î¸) =
H

j=1
,
Î³h(Î©â€²
h, j cos Î¸ âˆ’Î©â€²
hâ€², j sin Î¸)2 + Î³hâ€²(Î©â€²
h, j sin Î¸ + Î©â€²
hâ€², j cos Î¸)2-
Î³â€²
j
+
K

k=1

Î»(kâ€²)
h
Î»(kâ€²)
hâ€²
 cos2 Î¸
sin2 Î¸
sin2 Î¸
cos2 Î¸
 â›âœâœâœâœâÎ»(kâ€²)
h
Î»(kâ€²)
hâ€²
ââŸâŸâŸâŸâ 
+
Kâ€²

kâ€²=1

Î¨(kâ€²)
h,h cos Î¸ âˆ’Î¨(kâ€²)
hâ€²,h sin Î¸ + Î¨(kâ€²)
h,hâ€² sin Î¸ + Î¨(kâ€²)
hâ€²,hâ€² cos Î¸
 
+ const.
(10.27)
Since Eq. (10.27) is differentiable at Î¸ = 0, our assumption that Eq. (10.26)
is minimized or maximized when Î© = IH requires that Î¸ = 0 is a stationary
point of Eq. (10.27) for any h  hâ€². Therefore, it holds that
0 = âˆ‚G
âˆ‚Î¸
Î¸=0
=
F
2

j
,
Î³h(Î©â€²
h, j cos Î¸ âˆ’Î©â€²
hâ€², j sin Î¸)(âˆ’Î©â€²
h, j sin Î¸ âˆ’Î©â€²
hâ€², j cos Î¸)
+ Î³hâ€²(Î©â€²
h, j sin Î¸ + Î©â€²
hâ€², j cos Î¸)(Î©â€²
h, j cos Î¸ âˆ’Î©â€²
hâ€², j sin Î¸)
-
Î³â€²
j
+
Kâ€²

kâ€²=1

âˆ’Î¨(kâ€²)
h,h sin Î¸ âˆ’Î¨(kâ€²)
hâ€²,h cos Î¸ + Î¨(kâ€²)
h,hâ€² cos Î¸ âˆ’Î¨(kâ€²)
hâ€²,hâ€² sin Î¸
 GÎ¸=0
= 2 (Î³hâ€² âˆ’Î³h)

j
Î©â€²
h, jÎ³â€²
jÎ©â€²
hâ€², j +
Kâ€²

kâ€²=1

Î¨(kâ€²)
h,hâ€² âˆ’Î¨(kâ€²)
hâ€²,h
 
= 2 (Î³hâ€² âˆ’Î³h) Î¦h,hâ€².
(10.28)
In the last equation, we used Eq. (10.25) and the assumption that {Î¨(kâ€²)}
are symmetric. Since we assume that Î“ is nondegenerate (Î³h  Î³hâ€² for
h  hâ€²), Eq. (10.28) implies that Î¦ is diagonal, which completes the proof
of Lemma 10.3.
â–¡
10.4.2 Proof for Case 1
Assume that (Aâˆ—, Bâˆ—, Î£âˆ—
A, Ë˜Î£
âˆ—
B) is a minimizer, and consider the following
variation deï¬ned with an arbitrary H Ã— H orthogonal matrix Î©1:
A = Aâˆ—C1/2
B Î©âŠ¤
1 Câˆ’1/2
B
,
(10.29)

262
10 Global Solver for Low-Rank Subspace Clustering
B = Bâˆ—Câˆ’1/2
B
Î©âŠ¤
1 C1/2
B ,
(10.30)
Î£A = Câˆ’1/2
B
Î©1C1/2
B Î£âˆ—
AC1/2
B Î©âŠ¤
1 Câˆ’1/2
B
,
(10.31)
Ë˜Î£B = (C1/2
B Î©1Câˆ’1/2
B
âŠ—IM) Ë˜Î£
âˆ—
B(Câˆ’1/2
B
Î©âŠ¤
1 C1/2
B
âŠ—IM).
(10.32)
Then the free energy (10.10) can be written as a function of Î©1:
2F(Î©1) = tr
,
(Câˆ’1
A Câˆ’1
B Î©1C1/2
B

Aâˆ—âŠ¤Aâˆ—+ MÎ£âˆ—
A
 
C1/2
B Î©âŠ¤
1
-
+ const.
(10.33)
Since Eq. (10.33) is minimized when Î©1 = IH by assumption, Lemma 10.3
implies that
C1/2
B

Aâˆ—âŠ¤Aâˆ—+ MÎ£âˆ—
A
 
C1/2
B
is diagonal. Therefore,
Î¦1 = Aâˆ—âŠ¤Aâˆ—+ MÎ£âˆ—
A
(10.34)
is diagonal, with which Eq. (10.16) implies that Ë˜Î£
âˆ—
B is diagonal.
Since we have proved the diagonality of Ë˜Î£
âˆ—
B, the expectations in Eqs. (10.10)
and (10.14), respectively, can be expressed in the following simple forms at the
solution (A, B, Î£A, Ë˜Î£B) = (Aâˆ—, Bâˆ—, Î£âˆ—
A, Ë˜Î£
âˆ—
B):
*
B
!
A
âŠ¤A + MÎ£A
"
BâŠ¤+
rB(B) = B
!
A
âŠ¤A + MÎ£A
"
B
âŠ¤+ ÎÎ¦1,
(10.35)

BâŠ¤Î“âŠ¤
VÎ“V B

rB(B) = B
âŠ¤Î“âŠ¤
VÎ“VB + ÎÎ“V,
(10.36)
where ÎÎ“V âˆˆRHÃ—H and ÎÎ¦1 âˆˆRMÃ—M are diagonal matrices with their entries
given by
(ÎÎ¦1)m,m =
H

h=1
!
A
âŠ¤A + MÎ£A
"
h,h Ïƒ2
Bm,h,
(ÎÎ“V)h,h =
M

m=1
Î³2
mÏƒ2
Bm,h.
Here {Ïƒ2
Bm,h} are the diagonal entries of Ë˜Î£B such that
Ë˜Î£B = Diag$(Ïƒ2
B1,1,. . . , Ïƒ2
BM,1), (Ïƒ2
B1,2,. . . , Ïƒ2
BM,2),. . .. . . , (Ïƒ2
B1,H,. . . , Ïƒ2
BM,H)%.
Next consider the following variation deï¬ned with an M Ã— M matrix Î©2
such that the upper-left J Ã— J submatrix is an arbitrary orthogonal matrix and
the other entries are zero:
A = Î©âŠ¤
2 Aâˆ—,
B = Î©âŠ¤
2 Bâˆ—.

10.4 Proof of Theorem 10.2
263
Then, by using Eq. (10.35), the free energy (10.10) is written as
2F(Î©2) = 1
Ïƒ2 tr
,
Î“âŠ¤
VÎ“VÎ©âŠ¤
2

âˆ’2Bâˆ—Aâˆ—âŠ¤+ Bâˆ—
Aâˆ—âŠ¤Aâˆ—+ MÎ£A
 
Bâˆ—âŠ¤ 
Î©2
-
+ const.
(10.37)
Applying Lemma 10.3 to the upper-left J Ã— J submatrix in the trace, and then
using Eq. (10.22), we ï¬nd that
Î¦2 = âˆ’2Bâˆ—Aâˆ—âŠ¤+ Bâˆ—
Aâˆ—âŠ¤Aâˆ—+ MÎ£A
 
Bâˆ—âŠ¤
(10.38)
is diagonal. Eq. (10.38) also implies that Bâˆ—Aâˆ—âŠ¤is symmetric.
Consider the following variation deï¬ned with an MÃ—M matrix Î©3 such that
the upper-left J Ã— J submatrix is an arbitrary orthogonal matrix and the other
entries are zero:
B = Î©âŠ¤
3 Bâˆ—.
Then the free energy is written as
2F(Î©3) = 1
Ïƒ2 tr
,
Î“âŠ¤
VÎ“VÎ©âŠ¤
3

âˆ’2Bâˆ—Aâˆ—âŠ¤ 
+Î“âŠ¤
VÎ“VÎ©âŠ¤
3

Bâˆ—
Aâˆ—âŠ¤Aâˆ—+ MÎ£A
 
Bâˆ—âŠ¤ 
Î©3
-
+ const.
(10.39)
Applying Lemma 10.3 to the upper-left J Ã— J submatrix in the trace, we ï¬nd
that
Î¦3 = Bâˆ—
Aâˆ—âŠ¤Aâˆ—+ MÎ£A
 
Bâˆ—âŠ¤
(10.40)
is diagonal. Since Eqs. (10.34) and (10.40) are diagonal, Bâˆ—is diagonal.
Consequently, Eq. (10.14) combined with Eq. (10.36) implies that Aâˆ—and Î£âˆ—
A
are diagonal.
Thus we proved that the solution for (A, B, Î£A, Ë˜Î£B) are diagonal, provided
that CACB is nondegenerate.
10.4.3 Proof for Case 2
When CACB is degenerate, there are multiple equivalent solutions giving the
same free energy (10.10) and the output BA
âŠ¤. In the following, we show that
one of the equivalent solutions has diagonal (Aâˆ—, Bâˆ—, Î£âˆ—
A, Ë˜Î£
âˆ—
B).
Assume that CACB = c2IH for some c2 âˆˆR++. In this case, the free
energy (10.10) is invariant with respect to Î©1 under the transformation (10.29)
through (10.32). Let us focus on the solution with diagonal Ë˜Î£B, which can be
obtained by the transform (10.29) through (10.32) with a certain Î©1 from any
solution satisfying Eq. (10.16). Then we can show, in the same way as in the

264
10 Global Solver for Low-Rank Subspace Clustering
nondegenerate case, that Eqs. (10.34), (10.38), and (10.40) are diagonal. This
proves the existence of a solution such that (Aâˆ—, Bâˆ—, Î£âˆ—
A, Ë˜Î£
âˆ—
B) are diagonal.
10.4.4 Proof for Case 3
When cahcbh = cahâ€² cbhâ€² for (not all but) some pairs h  hâ€², we can show that Î£A
and Ë˜Î£B are block diagonal where the blocks correspond to the groups sharing
the same cahcbh. In each block, multiple equivalent solutions exist, one of which
is a solution such that (Aâˆ—, Bâˆ—, Î£âˆ—
A, Ë˜Î£
âˆ—
B) are diagonal.
This completes the proof of Theorem 10.2.
â–¡
10.5 Exact Global VB Solver (EGVBS)
Theorem 10.2 allows us to focus on the solutions such that (A, B, Î£A, Î£B) are
diagonal. Accordingly, we express the solution of the VB learning problem
(10.11) with diagonal entries, i.e.,
A = DiagM,H(a1,. . . ,aH),
(10.41)
B = DiagM,H(b1,. . . ,bH),
(10.42)
Î£A = Diag(Ïƒ2
a1,. . . , Ïƒ2
aH),
(10.43)
Ë˜Î£B = Diag$(Ïƒ2
B1,1,. . . , Ïƒ2
BM,1), (Ïƒ2
B1,2,. . . , Ïƒ2
BM,2),. . .. . . , (Ïƒ2
B1,H,. . . , Ïƒ2
BM,H)%,
(10.44)
where DiagD1,D2(Â·) denotes the D1 Ã— D2 diagonal matrix with the speciï¬ed
diagonal entries. Remember that J (â‰¤min(L, M)) is the rank of the observed
matrix V, and {Î³m} are the singular values arranged in nonincreasing order.
Without loss of generality, we assume thatah,bh âˆˆR+ for all h = 1,. . . , H.
We can easily obtain the following theorem:
Theorem 10.4
Any local solution of the VB learning problem (10.11)
satisï¬es, for all h = 1,. . . , H,
ah = Î³2
h
Ïƒ2bhÏƒ2
ah,
(10.45)
Ïƒ2
ah = Ïƒ2
â›âœâœâœâœâœâÎ³2
hb2
h +
J

m=1
Î³2
mÏƒ2
Bm,h + Ïƒ2
c2ah
ââŸâŸâŸâŸâŸâ 
âˆ’1
,
(10.46)
bh = Î³2
h
Ïƒ2ahÏƒ2
Bh,h,
(10.47)

10.5 Exact Global VB Solver (EGVBS)
265
Ïƒ2
Bm,h =
â§âªâªâªâªâ¨âªâªâªâªâ©
Ïƒ2

Î³2
m

a2
h + MÏƒ2
ah
 
+ Ïƒ2
c2
bh
âˆ’1
(for m = 1,. . . , J),
c2
bh
(for m = J + 1,. . . , M),
(10.48)
and has the free energy given by
2F = LM log(2Ï€Ïƒ2) +
J
h=1 Î³2
h
Ïƒ2
+
H

h=1
2Fh,
where
(10.49)
2Fh = M log
c2
ah
Ïƒ2ah
+
J

m=1
log
c2
bh
Ïƒ2
Bm,h
âˆ’(M + J) +
a2
h + MÏƒ2
ah
c2ah
+
b2
h + J
m=1 Ïƒ2
Bm,h
c2
bh
+ 1
Ïƒ2
â§âªâªâ¨âªâªâ©Î³2
h

âˆ’2ahbh +b2
h(a2
h + MÏƒ2
ah)
 
+
J

m=1
Î³2
mÏƒ2
Bm,h(a2
h + MÏƒ2
ah)
â«âªâªâ¬âªâªâ­.
(10.50)
Proof
By substituting the diagonal expression, Eqs. (10.41) through (10.44),
into the free energy (10.10), we have
2F = LM log(2Ï€Ïƒ2) + M
H

h=1
log
c2
ah
Ïƒ2ah
+
M

m=1
H

h=1
log
c2
bh
Ïƒ2
Bm,h
+
M
h=1 Î³2
h
Ïƒ2
âˆ’2MH
+
H

h=1
â§âªâªâ¨âªâªâ©
1
c2ah

a2
h + MÏƒ2
ah
 
+ 1
c2
bh
â›âœâœâœâœâœâb2
h +
M

m=1
Ïƒ2
Bm,h
ââŸâŸâŸâŸâŸâ 
â«âªâªâ¬âªâªâ­
+ 1
Ïƒ2
H

h=1
â§âªâªâ¨âªâªâ©Î³2
h

âˆ’2ahbh +b2
h(a2
h + MÏƒ2
ah)
 
+
J

m=1
Î³2
mÏƒ2
Bm,h(a2
h + MÏƒ2
ah)
â«âªâªâ¬âªâªâ­.
(10.51)
Eqs. (10.45) through (10.48) are obtained as the stationary conditions of
Eq. (10.51) that any solution satisï¬es, according to Lemma 10.1. By substitut-
ing Eq. (10.48) for m = J +1,. . . , M into Eq. (10.51), we obtain Eq. (10.49). â–¡
For EVB learning, where the prior covariances CA, CB are also estimated,
we have the following theorem:
Theorem 10.5
Any local solution of the EVB learning problem (10.17)
satisï¬es the following. For each h = 1,. . . , H, (ah,bh, Ïƒ2
ah, {Ïƒ2
Bm,h}M
m=1, c2
ah, c2
bh)
is either a (positive) stationary point that satisï¬es Eqs. (10.45) through (10.48)
and
c2
ah = a2
h/M + Ïƒ2
ah,
(10.52)
c2
bh =
â›âœâœâœâœâœâb2
h +
J

m=1
Ïƒ2
Bm,h
ââŸâŸâŸâŸâŸâ /J,
(10.53)

266
10 Global Solver for Low-Rank Subspace Clustering
or the null local solution deï¬ned by
ah = bh = 0,
Ïƒ2
ah = c2
ah â†’+0,
Ïƒ2
Bm,h = c2
bh â†’+0
(for m = 1,. . . , M),
(10.54)
of which the contribution (10.50) to the free energy is
Fh â†’+0.
(10.55)
The total free energy is given by Eq. (10.49).
Proof
Considering the derivatives of Eq. (10.51) with respect to c2
ah and c2
bh,
we have
2Mc2
ah = a2
h + MÏƒ2
ah,
(10.56)
2Mc2
bh = b2
h +
M

m=1
Ïƒ2
Bm,h,
(10.57)
as stationary conditions. By using Eq. (10.48), we can easily obtain Eqs.
(10.52) and (10.53).
Unlike in VB learning, where Lemma 10.1 guarantees that any local
solution is a stationary point, there exist nonstationary local solutions in EVB
learning. We can conï¬rm that, along any path such that
ah,bh = 0,
Ïƒ2
ah, Ïƒ2
Bm,h, c2
ah, c2
bh â†’+0
with Î²a =
Ïƒ2
ah
c2ah
and Î²b =
Ïƒ2
Bm,h
c2
bh
kept constant,
(10.58)
the free energy contribution (10.50) from the hth component decreases mono-
tonically. Among the possible paths, Î²a = Î²b = 1 gives the lowest free energy
(10.55).
â–¡
Based on Theorem 10.5, we can obtain the following corollary for the global
solution.
Corollary 10.6
The global solution of the EVB learning problem (10.17) can
be found in the following way. For each h = 1,. . . , H, ï¬nd all stationary points
that satisfy Eqs. (10.45) through (10.48), (10.52), and (10.53), and choose the
one giving the minimum free energy contribution Fh. The chosen stationary
point is the global solution if Fh < 0. Otherwise (including the case where no
stationary point exists), the null local solution (10.54) is global.
Proof
For each h = 1,. . . , H, any candidate for a local solution is a stationary
point or the null local solution. Therefore, if the minimum free energy con-
tribution over all stationary points is negative, i.e., Fh < 0, the corresponding

10.6 Approximate Global VB Solver (AGVBS)
267
stationary point is the global minimizer. With this fact, Corollary 10.6 is a
straightforward deduction from Theorem 10.5.
â–¡
Taking account of the trivial relations c2
bh = Ïƒ2
Bm,h for m > J, the stationary
conditions consisting of Eqs. (10.45) through (10.48), (10.52), and (10.53) for
each h can be seen as a polynomial system, a set of polynomial equations,
with 5 + J unknown variables,

ah,bh, Ïƒ2
ah, {Ïƒ2
Bm,h}J
m=1, c2
ah, c2
bh
 
. Thus, Theorem
10.5 has decomposed the original problem with O(M2H2) unknown variables,
for which the stationary conditions are given by Eqs. (10.13) through (10.16),
(10.19), and (10.20), into H subproblems with O(J) unknown variables each.
Fortunately, there is a reliable numerical method to solve a polynomial
system, called the homotopy method or continuation method (Drexler, 1978;
Garcia and Zangwill, 1979; Gunji et al., 2004; Lee et al., 2008). It provides all
isolated solutions to a system of n polynomials f(x) â‰¡( f1(x),. . . , fn(x)) = 0
by deï¬ning a smooth set of homotopy systems with a parameter t âˆˆ[0, 1],
i.e., g(x, t) â‰¡(g1(x, t), g2(x, t),. . . , gn(x, t)) = 0 such that one can continuously
trace the solution path from the easiest (t = 0) to the target (t = 1). For
empirical evaluation, which will be given in Section 10.8, we use HOM4PS-
2.0 (Lee et al., 2008), one of the most successful polynomial system solvers.
With the homotopy method in hand, Corollary 10.6 allows us to solve the
EVB learning problem (10.17) in the following way, which we call the exact
global VB solver (EGVBS). For each h = 1,. . . , H, we ï¬rst ï¬nd all stationary
points that satisfy the polynomial system, Eqs. (10.45) through (10.48),
(10.52), and (10.53). After that, we discard the prohibitive solutions with
complex numbers or negative variances, and then select the stationary point
giving the minimum free energy contribution Fh, deï¬ned by Eq. (10.50). The
global solution is the selected stationary point if it satisï¬es Fh < 0; otherwise,
the null local solution (10.54) is the global solution. Algorithm 20 summarizes
the procedure of EGVBS. When the noise variance Ïƒ2 is unknown, we conduct
a naive one-dimensional search to minimize the total free energy (10.49), with
EGVBS applied for every candidate value of Ïƒ2.
It is straightforward to modify Algorithm 20 to solve the VB learning
problem (10.11), where the prior covariances CA, CB are given. In this case,
we should solve the polynomial system (10.45) through (10.48) in Step 3, and
skip Step 6 since all local solutions are stationary points.
10.6 Approximate Global VB Solver (AGVBS)
Theorems 10.4 and 10.5 signiï¬cantly reduced the complexity of the optimiza-
tion problem. However, EGVBS is still not applicable to data with typical

268
10 Global Solver for Low-Rank Subspace Clustering
Algorithm 20 Exact global VB solver (EGVBS) for LRSC.
1: Compute the SVD of V = Î©left
V Î“VÎ©rightâŠ¤
V
.
2: for h = 1 to H do
3:
Find all solutions of the polynomial system, Eqs. (10.45) through
(10.48), (10.52), and (10.53) by the homotopy method.
4:
Discard prohibitive solutions with complex numbers or negative vari-
ances.
5:
Select the stationary point giving the smallest Fh (deï¬ned by Eq.
(10.50)).
6:
The global solution for the hth component is the selected stationary
point if it satisï¬es Fh < 0; otherwise, the null local solution (10.54)
is the global solution.
7: end for
8: Compute U = Î©right
V
BA
âŠ¤Î©rightâŠ¤
V
.
9: Apply spectral clustering with the afï¬nity matrix equal to abs(U) +
abs(U
âŠ¤).
problem sizes. This is because the homotopy method is not guaranteed to ï¬nd
all solutions in polynomial time in J, when the polynomial system involves
O(J) unknown variables.
The following simple trick further reduces the complexity and leads to
an efï¬cient approximate solver. Let us impose an additional constraint that
Î³2
mÏƒ2
Bm,h are constant over m = 1,. . . , J, i.e.,
Î³2
mÏƒ2
Bm,h = Ïƒ
2
bh
for
m = 1,. . . , J.
(10.59)
Under this constraint, the stationary conditions for the six unknowns
(ah,bh, Ïƒ2
ah, Ïƒ
2
bh, c2
ah, c2
bh) (for each h) become similar to the stationary
conditions for fully observed MF, which allows us to obtain the following
theorem:
Theorem 10.7
Under the constraint (10.59), any stationary point of the free
energy (10.50) for each h satisï¬es the following polynomial equation for a
single variable Î³h âˆˆR:
Î¾6Î³
6
h + Î¾5Î³
5
h + Î¾4Î³
4
h + Î¾3Î³
3
h + Î¾2Î³
2
h + Î¾1Î³h + Î¾0 = 0,
(10.60)
where
Î¾6 =
Ï†2
h
Î³2
h ,
(10.61)
Î¾5 = âˆ’2
Ï†2
hMÏƒ2
Î³3
h
+ 2Ï†h
Î³h ,
(10.62)

10.6 Approximate Global VB Solver (AGVBS)
269
Î¾4 =
Ï†2
hM2Ïƒ4
Î³4
h
âˆ’2Ï†h(2Mâˆ’J)Ïƒ2
Î³2
h
+ 1 +
Ï†2
h(MÏƒ2âˆ’Î³2
h)
Î³2
h
,
(10.63)
Î¾3 = 2Ï†hM(Mâˆ’J)Ïƒ4
Î³3
h
âˆ’2(Mâˆ’J)Ïƒ2
Î³h
+
Ï†h((M+J)Ïƒ2âˆ’Î³2
h)
Î³h
âˆ’
Ï†2
hMÏƒ2(MÏƒ2âˆ’Î³2
h)
Î³3
h
+
Ï†h(MÏƒ2âˆ’Î³2
h)
Î³h
,
(10.64)
Î¾2 = (Mâˆ’J)2Ïƒ4
Î³2
h
âˆ’
Ï†hMÏƒ2((M+J)Ïƒ2âˆ’Î³2
h)
Î³2
h
+ ((M + J)Ïƒ2 âˆ’Î³2
h) âˆ’
Ï†h(Mâˆ’J)Ïƒ2(MÏƒ2âˆ’Î³2
h)
Î³2
h
,
(10.65)
Î¾1 = âˆ’
(Mâˆ’J)Ïƒ2((M+J)Ïƒ2âˆ’Î³2
h)
Î³h
+ Ï†hMJÏƒ4
Î³h
,
(10.66)
Î¾0 = MJÏƒ4.
(10.67)
Here Ï†h = 1 âˆ’
Î³2
h
Î³2 for Î³2 = (J
m=1 Î³âˆ’2
m /J)âˆ’1. For each real solution Î³h such that
Î³h = Î³h + Î³h âˆ’MÏƒ2
Î³h ,
(10.68)
Îºh = Î³2
h âˆ’(M + J)Ïƒ2 âˆ’

MÏƒ2 âˆ’Î³2
h
 
Ï†h
Î³h
Î³h
,
(10.69)
Ï„h =
1
2MJ

Îºh +
B
Îº2
h âˆ’4MJÏƒ4
!
1 + Ï†h
Î³h
Î³h
"
,
(10.70)
Î´h =
Ïƒ2
âˆš
Ï„h

Î³h âˆ’MÏƒ2
Î³h âˆ’Î³h
 âˆ’1 ,
(10.71)
are real and positive, there exists the corresponding stationary point given by
!
ah,bh, Ïƒ2
ah, Ïƒ
2
bh, c2
ah, c2
bh
"
=
â›âœâœâœâœâœâ
.
Î³hÎ´h,
âˆš
Î³h/Î´h
Î³h
, Ïƒ2Î´h
Î³h ,
Ïƒ2
Î³hÎ´hâˆ’Ï†h
Ïƒ2
âˆš
Ï„h
,
C
Ï„h,
âˆš
Ï„h
Î³2
h
ââŸâŸâŸâŸâŸâ .
(10.72)
Given the noise variance Ïƒ2, computing the coefï¬cients (10.61) through
(10.67) is straightforward. Theorem 10.7 implies that the following algorithm,
which we call the AGVBS, provides the global solution of the EVB learning
problem (10.17) under the additional constraint (10.59). After computing the
SVD of the observed matrix V, AGVBS ï¬rst ï¬nds all real solutions of the
sixth-order polynomial equation (10.60) by using, e.g., the â€œrootsâ€ command
in MATLAB R
âƒ, for each h. Then, it discards the prohibitive solutions such
that any of Eqs. (10.68) through (10.71) gives a complex or negative number.
For each of the retained solutions, AGVBS computes the corresponding
stationary point by Eq. (10.72), along with the free energy contribution Fh
by Eq. (10.50). Here, Eq. (10.59) is used for retrieving the original posterior
variances {Ïƒ2
Bm,h}J
m=1 for B. Finally, AGVBS selects the stationary point giving
the minimum free energy contribution Fh. The global solution is the selected
stationary point if it satisï¬es Fh < 0; otherwise, the null local solution (10.54)
is the global solution. Algorithm 21 summarizes the procedure of AGVBS.

270
10 Global Solver for Low-Rank Subspace Clustering
Algorithm 21 Approximate global VB solver (AGVBS) for LRSC.
1: Compute the SVD of V = Î©left
V Î“VÎ©rightâŠ¤
V
.
2: for h = 1 to H do
3:
Find all real solutions of the sixth-order polynomial equation (10.60).
4:
Discard prohibitive solutions such that any of Eqs. (10.68) through
(10.71) gives a complex or negative number.
5:
Compute the corresponding stationary point by Eq. (10.72) and its free
energy contribution Fh by Eq. (10.50) for each of the retained solutions.
6:
Select the stationary point giving the minimum free energy contribution
Fh.
7:
The global solution for the hth component is the selected stationary
point if it satisï¬es Fh < 0; otherwise, the null local solution (10.54) is
the global solution.
8: end for
9: Compute U = Î©right
V
BA
âŠ¤Î©rightâŠ¤
V
.
10: Apply spectral clustering with the afï¬nity matrix equal to abs(U) +
abs(U
âŠ¤).
As in EGVBS, a naive one-dimensional search is conducted when the noise
variance Ïƒ2 is unknown.
In Section 10.8, we show that AGVBS is practically a good alternative to the
Kronecker product covariance approximation (KPCA), an approximate EVB
algorithm for LRSC under the Kronecker product covariance constraint (see
Section 3.4.2), in terms of accuracy and computation time.
10.7 Proof of Theorem 10.7
Let us rescalebh and c2
bh as follows:
bh = Î³hbh,
c2
bh = Î³2
hc2
bh.
(10.73)
By substituting Eqs. (10.59) and (10.73) into Eq. (10.50), we have
2Fh = M log
c2
ah
Ïƒ2ah
+ J log
c2
bh
Ïƒ
2
bh
+ 1
c2ah

a2
h + MÏƒ2
ah
 
+ 1
c2
bh
â›âœâœâœâœâœâb
2
h + J Î³2
h
Î³2 Ïƒ
2
bh
ââŸâŸâŸâŸâŸâ 
+ 1
Ïƒ2

âˆ’2Î³hahbh + (a2
h + MÏƒ2
ah)(b
2
h + JÏƒ
2
bh)

âˆ’(M + J) +
J

m=1
log Î³2
m
Î³2
h
,
(10.74)

10.7 Proof of Theorem 10.7
271
where
Î³2 =
â›âœâœâœâœâœâ
J

m=1
Î³âˆ’2
m /J
ââŸâŸâŸâŸâŸâ 
âˆ’1
.
Ignoring the last two constant terms, we ï¬nd that Eq. (10.74) is in almost the
same form as the free energy of fully observed MF for a JÃ—M observed matrix
(see Eq. (6.43)). Only the difference is in the fourth term: JÏƒ
2
bh is multiplied by
Î³2
h
Î³2 . Note that, as in MF, the free energy (10.74) is invariant under the following
transformation:
2
(ah,bh, Ïƒ2
ah, Ïƒ
2
bh, c2
ah, c2
bh)
3
â†’
2
(shah, sâˆ’1
h bh, s2
hÏƒ2
ah, sâˆ’2
h Ïƒ
2
bh, s2
hc2
ah, sâˆ’2
h c2
bh)
3
for any {sh  0; h = 1,. . . , H}. Accordingly, we ï¬x the ratio between cah and
cbh to cah/cbh = 1 without loss of generality.
By differentiating the free energy (10.74) with respect toah, Ïƒ2
ah,bh, Ïƒ2
bh, c2
ah,
and c2
bh, respectively, we obtain the following stationary conditions:
ah = 1
Ïƒ2 Î³hbhÏƒ2
ah,
(10.75)
Ïƒ2
ah = Ïƒ2

b
2
h + JÏƒ
2
bh + Ïƒ2
c2ah
âˆ’1
,
(10.76)
bh = Î³hah
â›âœâœâœâœâœâa2
h + MÏƒ2
ah + Ïƒ2
c2
bh
ââŸâŸâŸâŸâŸâ 
âˆ’1
,
(10.77)
Ïƒ2
bh = Ïƒ2
â›âœâœâœâœâœâœâa2
h + MÏƒ2
ah + Ïƒ2Î³2
h
c2
bhÎ³2
ââŸâŸâŸâŸâŸâŸâ 
âˆ’1
,
(10.78)
c2
ah = a2
h/M + Ïƒ2
ah,
(10.79)
c2
bh = b
2
h/J + Î³2
h
Î³2 Ïƒ
2
bh.
(10.80)
Note that, unlike the case of fully observed MF, A and B are not symmetric,
which makes analysis more involved. Apparently, if ah = 0 or bh = 0, the null
solution (10.54) gives the minimum Fh â†’+0 of the free energy (10.74). In
the following, we identify the positive stationary points such that ah,bh > 0.
To this end, we derive a polynomial equation with a single unknown variable
from the stationary conditions (10.75) through (10.80). Let
Î³h = ahbh,
(10.81)
Î´h = ah/bh.
(10.82)

272
10 Global Solver for Low-Rank Subspace Clustering
From Eqs. (10.75) through (10.78), we obtain
Î³2
h =
â›âœâœâœâœâœâa2
h + MÏƒ2
ah + Ïƒ2
c2
bh
ââŸâŸâŸâŸâŸâ 

b
2
h + JÏƒ
2
bh + Ïƒ2
c2ah

,
(10.83)
Î³hÎ´âˆ’1
h =

b
2
h + JÏƒ
2
bh + Ïƒ2
c2ah

,
(10.84)
Î³hÎ´h =
â›âœâœâœâœâœâa2
h + MÏƒ2
ah + Ïƒ2
c2
bh
ââŸâŸâŸâŸâŸâ .
(10.85)
Substituting Eq. (10.84) into Eq. (10.76) gives
Ïƒ2
ah = Ïƒ2Î´h
Î³h
.
(10.86)
Substituting Eq. (10.85) into Eq. (10.78) gives
Ïƒ
2
bh =
Ïƒ2
Î³hÎ´h âˆ’Ï†h Ïƒ2
c2
bh
,
(10.87)
where
Ï†h = 1 âˆ’Î³2
h
Î³2 .
Thus, the variances Ïƒ2
ah and Ïƒ
2
bh have been written as functions of Î´h and c2
bh.
Substituting Eqs. (10.86) and (10.87) into Eq. (10.78) gives
Ïƒ2
Î³hÎ´h âˆ’Ï†h Ïƒ2
c2
bh
â›âœâœâœâœâœâœâa2
h + M Ïƒ2Î´h
Î³h
+ Ïƒ2Î³2
h
c2
bhÎ³2
ââŸâŸâŸâŸâŸâŸâ = Ïƒ2,
and therefore
Î³h + MÏƒ2
Î³h
âˆ’Î³h + Ïƒ2
c2
bh
Î´âˆ’1
h = 0.
Solving the preceding equation with respect to Î´âˆ’1
h gives
Î´âˆ’1
h =
c2
bh
Ïƒ2

Î³h âˆ’MÏƒ2
Î³h
âˆ’Î³h

.
(10.88)
Thus, we have obtained an expression of Î´h as a function of Î³h and c2
bh.
Substituting Eqs. (10.86) and (10.87) into Eq. (10.76) gives
Ïƒ2Î´h
Î³h
â›âœâœâœâœâœâœâœâœâœâ
b
2
h + J
Ïƒ2
Î³hÎ´h âˆ’Ï†h Ïƒ2
c2
bh
+ Ïƒ2
c2ah
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâ = Ïƒ2.

10.7 Proof of Theorem 10.7
273
Rearranging the previous equation with respect toÎ´âˆ’1
h gives
$Î³h âˆ’Î³h
% Ï†hÏƒ2
c2
bhÎ³h
Î´âˆ’2
h +
â›âœâœâœâœâœâÎ³h + JÏƒ2
Î³h
âˆ’Î³h âˆ’
Ï†hÏƒ4
c2ahc2
bhÎ³h
ââŸâŸâŸâŸâŸâ Î´âˆ’1
h + Ïƒ2
c2ah
= 0.
(10.89)
Substituting Eq. (10.88) into Eq. (10.89), we have
Ï†h
Î³h
$Î³h âˆ’Î³h
% 
Î³h âˆ’

Î³h âˆ’MÏƒ2
Î³h
2
âˆ’
Ïƒ4
c2ahc2
bh
+
â›âœâœâœâœâœâÎ³h âˆ’
â›âœâœâœâœâœâÎ³h âˆ’JÏƒ2
Î³h
+
Ï†hÏƒ4
c2ahc2
bhÎ³h
ââŸâŸâŸâŸâŸâ 
ââŸâŸâŸâŸâŸâ 

Î³h âˆ’

Î³h âˆ’MÏƒ2
Î³h

= 0.
(10.90)
Thus we have derived an equation that includes only two unknown variables,
Î³h and c2
ahc2
bh.
Next we will obtain another equation that includes only Î³h and c2
ahc2
bh.
Substituting Eqs. (10.79) and (10.80) into Eq. (10.83), we have
Î³2
h =
â›âœâœâœâœâœâMc2
ah + Ïƒ2
c2
bh
ââŸâŸâŸâŸâŸâ 

Jc2
bh + JÏ†hÏƒ
2
bh + Ïƒ2
c2ah

.
(10.91)
Substituting Eq. (10.88) into Eq. (10.87) gives
Ïƒ
2
bh =
c2
bh

Î³h âˆ’MÏƒ2
Î³h âˆ’Î³h
 
Î³h âˆ’Ï†h

Î³h âˆ’MÏƒ2
Î³h âˆ’Î³h
 .
(10.92)
Substituting Eq. (10.92) into Eq. (10.91) gives
Î³2
h = MJc2
ahc2
bh + (M + J)Ïƒ2 +
Ïƒ4
c2ahc2
bh
+ JÏ†h

Mc2
ahc2
bh + Ïƒ2 
Î³h âˆ’MÏƒ2
Î³h âˆ’Î³h
 
Î³h âˆ’Ï†h

Î³h âˆ’MÏƒ2
Î³h âˆ’Î³h
 
.
Rearranging the preceding equation with respect to c2
ahc2
bh, we have
MJc4
ahc4
bh +
â›âœâœâœâœâœâ(M + J)Ïƒ2 âˆ’Î³2
h +

MÏƒ2 âˆ’Î³2
h
 
Ï†h
Î³h
Î³h
ââŸâŸâŸâŸâŸâ c2
ahc2
bh + Ïƒ4
â›âœâœâœâœâœâ1 + Ï†h
Î³h
Î³h
ââŸâŸâŸâŸâŸâ = 0,
(10.93)
where
Î³h = Î³h âˆ’

Î³h âˆ’MÏƒ2
Î³h

.
(10.94)
The solution of Eq. (10.93) with respect to c2
ahc2
bh is given by
c2
ahc2
bh =
Îºh +
B
Îº2
h âˆ’4MJÏƒ4
!
1 + Ï†h
Î³h
Î³h
"
2MJ
,
(10.95)

274
10 Global Solver for Low-Rank Subspace Clustering
where
Îºh = Î³2
h âˆ’(M + J)Ïƒ2 âˆ’

MÏƒ2 âˆ’Î³2
h
 
Ï†h
Î³h
Î³h
.
(10.96)
By using Eq. (10.94), Eq. (10.90) can be rewritten as
1
Î³h
Ï†h

Î³h âˆ’MÏƒ2
Î³h

Î³
2
h +

Î³h âˆ’(M âˆ’J)Ïƒ2
Î³h

Î³h âˆ’
 1
Î³h
Ï†hÎ³h + 1

Ïƒ4
c2ahc2
bh
= 0.
(10.97)
Thus, we have obtained two equations, Eqs. (10.95) and (10.97), that relate
two unknown variables, Î³h (or Î³h) and c2
ahc2
bh. Substituting Eq. (10.95) into Eq.
(10.97) gives a polynomial equation involving only a single unknown variable
Î³h. With some algebra, we obtain Eq. (10.60).
Let
Ï„h = c2
ahc2
bh.
(10.98)
Since we ï¬xed the arbitrary ratio to c2
ah/c2
bh = 1, we have
c2
ah =
C
Ï„h,
(10.99)
c2
bh =
C
Ï„h.
(10.100)
Some solutions of Eq. (10.60) have no corresponding points in the problem
domain (10.18). Assume that a solution Î³h is associated with a point in
the domain. Then Î³h is given by Eq. (10.94), which is real and positive by
its deï¬nition (10.81). Ï„h is deï¬ned and given, respectively, by Eqs. (10.98)
and (10.95), which is real and positive. Îºh, deï¬ned by Eq. (10.96), is also
real and positive, since Ï„h cannot be real and positive otherwise. Î´h is given
by Eq. (10.88), which is real and positive by its deï¬nition (10.82). Finally,
remembering the variable change (10.73), we can obtain Eq. (10.72) from Eqs.
(10.81), (10.82), (10.86), (10.87), (10.99), and (10.100), which completes the
proof of Theorem 10.7.
â–¡
10.8 Empirical Evaluation
In this section, we empirically compare the global solvers, EGVBS (Algorithm
20) and AGVBS (Algorithm 21), with the standard iterative algorithm (Algo-
rithm 4 in Section 3.4.2) and its approximation (Algorithm 5 in Section 3.4.2),
which we here call the standard VB (SVB) iteration and the KPCA iteration,
respectively. We assume that the prior covariances (CA, CB) and the noise

10.8 Empirical Evaluation
275
variance Ïƒ2 are unknown and estimated from observation. We use the full-
rank model (i.e., H = min(L, M)), and expect EVB learning to automatically
ï¬nd the true rank without any parameter tuning.
Artiï¬cial Data Experiment
We ï¬rst conducted an experiment with a small artiï¬cial data set (â€œartiï¬cial
smallâ€), on which the exact algorithms, i.e., EGVBS and the SVB iteration,
are computationally tractable. Through this experiment, we can assess the
accuracy of the efï¬cient approximate solvers, i.e., AGVBS and the KPCA
iteration. We randomly created M = 75 samples in the L = 10 dimensional
space. We assumed K = 2 clusters: M(1)âˆ—= 50 samples lie in a H(1)âˆ—= 3-
dimensional subspace, and the other M(2)âˆ—= 25 samples lie in a H(2)âˆ—=
1-dimensional subspace. For each cluster k, we independently drew M(k)âˆ—
samples from GaussH(k)âˆ—(0, 10 Â· IH(k)âˆ—), and projected them onto the observed
L-dimensional space by R(k) âˆˆRLÃ—H(k)âˆ—, each entry of which follows R(k)
l,h âˆ¼
Gauss1(0, 1). Thus, we obtained a noiseless matrix V(k)âˆ—âˆˆRLÃ—M(k)âˆ—for the kth
cluster. Concatenating all clusters, Vâˆ—= (V(1)âˆ—,. . . , V(K)âˆ—), and adding random
noise subject to Gauss1(0, 1) to each entry gave an artiï¬cial observed matrix
V âˆˆRLÃ—M, where M = K
k=1 M(k)âˆ—= 75. The true rank of Vâˆ—is given by
Hâˆ—= min(K
k=1 H(k)âˆ—, L, M) = 4. Note that Hâˆ—is different from the rank of
the observed matrix V, which is almost surely equal to J = min(L, M) (= 10)
under the Gaussian noise.
Figure 10.1 shows the free energy, the computation time, and the estimated
rank of U = B
â€²A
â€²âŠ¤over iterations. For the iterative methods, we show the
results of 10 trials starting from different random initializations. We can see
that AGVBS gives almost the same free energy as the exact methods (EGVBS
and the SVB iteration). The exact methods require large computation costs:
EGVBS took 621 sec to obtain the global solution, and the SVB iteration
took âˆ¼100 sec to achieve almost the same free energy. On the other hand,
the approximate methods are much faster: AGVBS took less than 1 sec, and
the KPCA iteration took âˆ¼10 sec. Since the KPCA iteration had not converged
after 250 iterations, we continued its computation until 2,500 iterations, and
found that it sometimes converges to a local solution with a signiï¬cantly higher
free energy than the other methods. EGVBS, AGVBS, and the SVB iteration
successfully found the true rank Hâˆ—= 4, while the KPCA iteration sometimes
failed to ï¬nd it. This difference is actually reï¬‚ected to the clustering error, i.e.,
the misclassiï¬cation rate with all possible cluster correspondences taken into
account, after spectral clustering (Shi and Malik, 2000) is performed: 1.3% for
EGVBS, AGVBS, and the SVB iteration, and 2.4% for the KPCA iteration.

276
10 Global Solver for Low-Rank Subspace Clustering
0
50
100
150
200
250
Iteration
1.8
1.9
2
2.1
2.2
2.3
EGVBS
AGVBS
SVB iteration
KPCA iteration
(a) Free energy
0
50
100
150
200
250
Iteration
100
102
104
Time(sec)
EGVBS
AGVBS
SVB iteration
KPCA iteration
(b) Computation time
0
50
100
150
200
250
Iteration
0
2
4
6
8
10
EGVBS
AGVBS
SVB iteration
KPCA iteration
(c) Estimated rank
Figure 10.1 Results on the â€œartiï¬cial smallâ€ data set (L = 10, M = 75, Hâˆ—= 4).
The clustering errors were 1.3% for EGVBS, AGVBS, and the SVB iteration, and
2.4% for the KPCA iteration.
Next we conducted the same experiment with a larger artiï¬cial data set
(â€œartiï¬cial largeâ€) (L = 50, K = 4, (M(1)âˆ—,. . . , M(K)âˆ—) = (100, 50, 50, 25),
(H(1)âˆ—,. . . , H(K)âˆ—) = (2, 1, 1, 1)), on which EGVBS and the SVB iteration are
computationally intractable. Figure 10.2 shows the results with AGVBS and
the KPCA iteration. The advantage in computation time is clear: AGVBS only
took âˆ¼0.1 sec, while the KPCA iteration took more than 100 sec. The clustering
errors were 4.0% for AGVBS and 11.2% for the KPCA iteration.
Benchmark Data Experiment
Finally, we applied AGVBS and the KPCA iteration to the Hopkins 155 motion
database (Tron and Vidal, 2007). In this data set, each sample corresponds to
the trajectory of a point in a video, and clustering the trajectories amounts to
ï¬nding a set of rigid bodies. Figure 10.3 shows the results on the â€œ1R2RCâ€
(L = 59, M = 459) sequence.1 We see that AGVBS gave a lower free energy
with much less computation time than the KPCA iteration. Figure 10.4 shows
the clustering errors on the ï¬rst 20 sequences, which implies that AGVBS
generally outperforms the KPCA iteration. Figure 10.4 also shows the results
1 Peaks in the free energy curves are due to pruning. As noted in Section 3.1.1, the free energy
can increase right after pruning happens, but immediately gets lower than the free energy before
pruning.

10.8 Empirical Evaluation
277
0
500
1,000
1,500
2,000 2,500
Iteration
1.61
1.615
1.62
1.625
1.63
1.635
1.64
AGVBS
KPC alteration
(a) Free energy
0
500
1,000
1,500
2,000
2,500
Iteration
100
102
104
Time(sec)
AGVBS
KPC alteration
(b) Computation time
0
500
1,000
1,500
2,000
2,500
Iteration
0
5
10
15
AGVBS
KPC alteration
(c) Estimated rank
Figure 10.2 Results on the â€œartiï¬cial largeâ€ data set (L = 50, M = 225, Hâˆ—= 5).
The clustering errors were 4.0% for AGVBS and 11.2% for the KPCA iteration.
0
500
1,000
1,500
2,000
2,500
Iteration
2
3
4
5
6
7
AGVBS
KPC alteration
(a) Free energy
0
500
1,000
1,500
2,000
2,500
Iteration
100
102
104
Time(sec)
AGVBS
KPC aIteration
(b) Computation time
0
500
1,000
1,500
2,000
2,500
Iteration
0
10
20
30
40
50
AGVBS
KPC alteration
(c) Estimated rank
Figure 10.3 Results on the â€œ1R2RCâ€ sequence (L = 59, M = 459) of the Hopkins
155 motion database. Peaks in the free energy curves are due to pruning. The
clustering errors are shown in Figure 10.4.

278
10 Global Solver for Low-Rank Subspace Clustering
0
0.005
0.01
0.015
0.02
0.025
0.03
1R2RC 
1R2RCR 
1R2RCR_g12 
1R2RCR_g13 
1R2RCR_g23 
1R2RCT_A 
R2RCT_A_g12 
R2RCT_A_g13 
R2RCT_A_g23 
1R2RCT_B 
R2RCT_B_g12 
R2RCT_B_g13 
R2RCT_B_g23 
1R2RC_g12 
1R2RC_g13 
1R2RC_g23 
1R2TCR 
1R2TCRT 
1R2TCRT_g12 
1R2TCRT_g13 
MAP (with optimized lambda)
AGVBS
KPC aIteration
Figure 10.4 Clustering errors on the ï¬rst 20 sequences of the Hopkins 155
data set.
by MAP learning (Eq. (3.87) in Section 3.4) with the tuning parameter Î»
optimized over the 20 sequences (i.e., we performed MAP learning with
different values for Î», and selected the one giving the lowest average clustering
error). We see that AGVBS performs comparably to MAP learning with
optimized Î», which implies that EVB learning estimates the hyperparameters
and the noise variance reasonably well.

11
Efï¬cient Solver for Sparse Additive
Matrix Factorization
In this chapter, we introduce an efï¬cient variational Bayesian (VB) solver
(Nakajima et al., 2013b) for sparse additive matrix factorization (SAMF),
where the global VB solver, derived in Chapter 9, for fully observed MF is
used as a subroutine.
11.1 Problem Description
The SAMF model, introduced in Section 3.5, is deï¬ned as
p(V|Î˜) âˆexp
â›âœâœâœâœâœâœâœââˆ’1
2Ïƒ2
#######V âˆ’
S
s=1
U(s)
#######
2
Fro
ââŸâŸâŸâŸâŸâŸâŸâ ,
(11.1)
p({Î˜(s)
A }S
s=1) âˆexp

âˆ’1
2
S
s=1
K(s)

k=1
tr

A(k,s)C(k,s)âˆ’1
A
A(k,s)âŠ¤ 
,
(11.2)
p({Î˜(s)
B }S
s=1) âˆexp

âˆ’1
2
S
s=1
K(s)

k=1
tr

B(k,s)C(k,s)âˆ’1
B
B(k,s)âŠ¤ 
,
(11.3)
where
U(s) = G({B(k,s)A(k,s)âŠ¤}K(s)
k=1; X(s))
(11.4)
is the
sth sparse matrix factorization (SMF) term. Here G(Â·; X)
:
R
K
k=1(Lâ€²(k)Ã—Mâ€²(k)) â†’RLÃ—M maps the partitioned-and-rearranged (PR) matrices
{Uâ€²(k)}K
k=1 to the target matrix U âˆˆRLÃ—M, based on the one-to-one map
X : (k, lâ€², mâ€²) â†’(l, m) from the indices of the entries in {Uâ€²(k)}K
k=1 to the
indices of the entries in U such that

G({Uâ€²(k)}K
k=1; X)
 
l,m = Ul,m = UX(k,lâ€²,mâ€²) = Uâ€²(k)
lâ€²,mâ€².
(11.5)
279

280
11 Efï¬cient Solver for Sparse Additive Matrix Factorization
The prior covariances of A(k,s) and B(k,s) are assumed to be diagonal and
positive-deï¬nite:
C(k,s)
A
= Diag(c(k,s)2
a1
,. . . , c(k,s)2
aH
),
C(k,s)
B
= Diag(c(k,s)2
b1
,. . . , c(k,s)2
bH
),
and Î˜ summarizes the parameters as follows:
Î˜ = {Î˜(s)
A , Î˜(s)
B }S
s=1, where Î˜(s)
A = {A(k,s)}K(s)
k=1, Î˜(s)
B = {B(k,s)}K(s)
k=1.
Under the independence constraint,
r(Î˜) =
S
s=1
r(s)
A (Î˜(s)
A )r(s)
B (Î˜(s)
B ),
(11.6)
the VB posterior minimizing the free energy can be written as
r(Î˜) =
S
s=1
K(s)

k=1

MGaussMâ€²(k,s),Hâ€²(k,s)(A(k,s); A
(k,s), Î£
(k,s)
A )
Â· MGaussLâ€²(k,s),Hâ€²(k,s)(B(k,s); B
(k,s), Î£
(k,s)
B
)

=
S
s=1
K(s)

k=1
â›âœâœâœâœâœâœâ
Mâ€²(k,s)

mâ€²=1
GaussHâ€²(k,s)(a(k,s)
mâ€² ;a
(k,s)
mâ€² , Î£
(k,s)
A )
Â·
Lâ€²(k,s)

lâ€²=1
GaussHâ€²(k,s)(b
(k,s)
lâ€²
;b
(k,s)
lâ€²
, Î£
(k,s)
B
)
ââŸâŸâŸâŸâŸâŸâ .
(11.7)
The free energy can be explicitly written as
2F = LM log(2Ï€Ïƒ2) + âˆ¥Vâˆ¥2
Fro
Ïƒ2
+
S
s=1
K(s)

k=1
â›âœâœâœâœâœâMâ€²(k,s) log
det

C(k,s)
A
 
det
!
Î£
(k,s)
A
" + Lâ€²(k,s) log
det

C(k,s)
B
 
det
!
Î£
(k,s)
B
"
ââŸâŸâŸâŸâŸâ 
+
S
s=1
K(S )

k=1
tr
2
C(k,s)âˆ’1
A
(A
(k,s)âŠ¤A
(k,s) + Mâ€²(k,s)Î£
(k,s)
A
)
+ C(k,s)âˆ’1
B
(B
(k,s)âŠ¤B
(k,s) + Lâ€²(k,s)Î£
(k,s)
B
)
3
+ 1
Ïƒ2 tr
â§âªâªâ¨âªâªâ©âˆ’2VâŠ¤
â›âœâœâœâœâœâ
S
s=1
G({B
(k,s)A
(k,s)âŠ¤}K(s)
k=1; X(s))
ââŸâŸâŸâŸâŸâ 

11.1 Problem Description
281
+ 2
S
s=1
S
sâ€²=s+1
GâŠ¤({B
(k,s)A
(k,s)âŠ¤}K(s)
k=1; X(s))G({B
(k,sâ€²)A
(k,sâ€²)âŠ¤}K(sâ€²)
k=1 ; X(sâ€²))
â«âªâªâ¬âªâªâ­
+ 1
Ïƒ2
S
s=1
K(S )

k=1
tr
2
(A
(k,s)âŠ¤A
(k,s) + Mâ€²(k,s)Î£
(k,s)
A
)(B
(k,s)âŠ¤B
(k,s) + Lâ€²(k,s)Î£
(k,s)
B
)
3
âˆ’
S
s=1
K(S )

k=1
(Lâ€²(k,s) + Mâ€²(k,s))Hâ€²(k,s),
(11.8)
of which the stationary conditions are given by
A
(k,s) = Ïƒâˆ’2Zâ€²(k,s)âŠ¤B
(k,s)Î£
(k,s)
A ,
(11.9)
Î£
(k,s)
A
= Ïƒ2 !
B
(k,s)âŠ¤B
(k,s) + Lâ€²(k,s)Î£
(k,s)
B
+ Ïƒ2C(k,s)âˆ’1
A
"âˆ’1
,
(11.10)
B
(k,s) = Ïƒâˆ’2Zâ€²(k,s)A
(k,s)Î£
(k,s)
B
,
(11.11)
Î£
(k,s)
B
= Ïƒ2 !
A
(k,s)âŠ¤A
(k,s) + Mâ€²(k,s)Î£
(k,s)
A
+ Ïƒ2C(k,s)âˆ’1
B
"âˆ’1
.
(11.12)
Here Zâ€²(k,s) âˆˆRLâ€²(k,s)Ã—Mâ€²(k,s) is deï¬ned as
Zâ€²(k,s)
lâ€²,mâ€² = Z(s)
X(s)(k,lâ€²,mâ€²),
where
Z(s) = V âˆ’

sâ€²s
U
(s).
(11.13)
The stationary conditions for the hyperparameters {C(k,s)
A
, C(k,s)
B
}K(s)
k=1,
S
s=1, Ïƒ2 are
given as
c(k,s)2
ah
=
####a(k,s)
h
####
2
/Mâ€²(k,s) + (Î£
(k,s)
A )hh,
(11.14)
c(k,s)2
bh
=
#####b
(k,s)
h
#####
2
/Lâ€²(k,s) + (Î£
(k,s)
B
)hh,
(11.15)
Ïƒ2 =
1
LM
â§âªâªâ¨âªâªâ©âˆ¥Vâˆ¥2
Fro âˆ’2
S
s=1
tr
â›âœâœâœâœâœâU
(s)âŠ¤
â›âœâœâœâœâœâV âˆ’
S
sâ€²=s+1
U
(sâ€²)
ââŸâŸâŸâŸâŸâ 
ââŸâŸâŸâŸâŸâ 
+
S
s=1
K(s)

k=1
tr

(A
(k,s)âŠ¤A
(k,s) + Mâ€²(k,s)Î£
(k,s)
A
) Â· (B
(k,s)âŠ¤B
(k,s) + Lâ€²(k,s)Î£
(k,s)
B
)
 
â«âªâªâªâ¬âªâªâªâ­.
(11.16)
The standard VB algorithm (Algorithm 6 in Section 3.5) iteratively applies
Eqs. (11.9) through (11.12) and (11.14) through (11.16) until convergence.

282
11 Efï¬cient Solver for Sparse Additive Matrix Factorization
11.2 Efï¬cient Algorithm for SAMF
In this section, we derive a more efï¬cient algorithm than the standard VB
algorithm. We ï¬rst present a theorem that reduces a partial SAMF problem
to the (fully observed) MF problem, which can be solved analytically. Then
we describe the algorithm that solves the entire SAMF problem.
11.2.1 Reduction of the Partial SAMF Problem to the MF Problem
Let us denote the mean of U(s), deï¬ned in Eq. (11.4), over the VB posterior by
U
(s) =

U(s)
r(s)
A (Î˜(s)
A )r(s)
B (Î˜(s)
B )
= G
2
B
(k,s)A
(k,s)âŠ¤3K(s)
k=1 ; X(s)

.
(11.17)
Then we obtain the following theorem:
Theorem 11.1
Given {U
(sâ€²)}sâ€²s and the noise variance Ïƒ2, the VB posterior
of (Î˜(s)
A , Î˜(s)
B ) = {A(k,s), B(k,s)}K(s)
k=1 coincides with the VB posterior of the
following MF model:
p(Zâ€²(k,s)|A(k,s), B(k,s)) âˆexp

âˆ’1
2Ïƒ2
###Zâ€²(k,s) âˆ’B(k,s)A(k,s)âŠ¤###2
Fro

,
(11.18)
p(A(k,s)) âˆexp

âˆ’1
2tr

A(k,s)C(k,s)âˆ’1
A
A(k,s)âŠ¤ 
,
(11.19)
p(B(k,s)) âˆexp

âˆ’1
2tr

B(k,s)C(k,s)âˆ’1
B
B(k,s)âŠ¤ 
,
(11.20)
for each k = 1,. . . , K(s). Here, Zâ€²(k,s) âˆˆRLâ€²(k,s)Ã—Mâ€²(k,s) is deï¬ned by Eq. (11.13).
Proof
Given {U
(s)}sâ€²s = {{B
(k,sâ€²)A
(k,sâ€²)âŠ¤}K(sâ€²)
k=1 }sâ€²s and Ïƒ2 as ï¬xed constants, the
free energy (11.8) can be written as a function of {A
(k,s), B
(k,s), Î£
(k,s)
A
, Î£
(k,s)
B
, C(k,s)
A
,
C(k,s)
B
}K(s)
k=1 as follows:
2F(s) !
{A
(k,s), B
(k,s), Î£
(k,s)
A
, Î£
(k,s)
B
, C(k,s)
A
, C(k,s)
B
}K(s)
k=1
"
=
K(s)

k=1
2F(k,s) + const.,
(11.21)
where
2F(k,s) = Mâ€²(k,s) log
det

C(k,s)
A
 
det
!
Î£
(k,s)
A
" + Lâ€²(k,s) log
det

C(k,s)
B
 
det
!
Î£
(k,s)
B
"
+ tr
2
C(k,s)âˆ’1
A
(A
(k,s)âŠ¤A
(k,s)+Mâ€²(k,s)Î£
(k,s)
A
) + C(k,s)âˆ’1
B
(B
(k,s)âŠ¤B
(k,s)+Lâ€²(k,s)Î£
(k,s)
B
)
3

11.2 Efï¬cient Algorithm for SAMF
283
+ 1
Ïƒ2 tr
2
âˆ’2A
(k,s)âŠ¤Zâ€²(k,s)âŠ¤B
(k,s)
+ (A
(k,s)âŠ¤A
(k,s) + Mâ€²(k,s)Î£
(k,s)
A
)(B
(k,s)âŠ¤B
(k,s) + Lâ€²(k,s)Î£
(k,s)
B
)
3
.
(11.22)
Eq. (11.22) coincides with the free energy of the fully observed matrix
factorization model (11.18) through (11.20) up to a constant (see Eq. (3.23)
with Zâ€² substituted for V). Therefore, the VB solution is the same.
â–¡
Eq. (11.13) relates the entries of Z(s) âˆˆRLÃ—M to the entries of {Zâ€²(k,s) âˆˆ
RLâ€²(k,s)Ã—Mâ€²(k,s)}K(s)
k=1 by using the map X(s) : (k, lâ€², mâ€²) â†’(l, m) (see Eq. (11.5) and
Figure 3.3).
11.2.2 Mean Update Algorithm
Theorem 11.1 states that a partial problem of SAMFâ€”ï¬nding the posterior of
(A(k,s), B(k,s)) for each k = 1,. . . , K(s) given {U
(sâ€²)}sâ€²s and Ïƒ2â€”can be solved
by the global solver for the fully observed MF model. Speciï¬cally, we use
Algorithm 16, introduced in Chapter 9, for estimating each SMF term U
(s)
in turn. We use Eq. (11.16) for updating the noise variance Ïƒ2. The whole
procedure, called the mean update (MU) algorithm (Nakajima et al., 2013b),
is summarized in Algorithm 22, where 0(d1,d2) denotes the d1 Ã— d2 matrix with
all entries equal to zero.
The MU algorithm is similar in spirit to the backï¬tting algorithm (Hastie
and Tibshirani, 1986; Dâ€™Souza et al., 2004), where each additive term is
updated to ï¬t a dummy target. In the MU algorithm, Z(s) deï¬ned in Eq. (11.13)
corresponds to the dummy target. Although the MU algorithm globally solves
a partial problem in each step, its joint global optimality over the entire
Algorithm 22 Mean update (MU) algorithm for VB SAMF.
1: Initialize: U
(s) â†0(L,M) for s = 1,. . . , S , Ïƒ2 â†âˆ¥Vâˆ¥2
Fro /(LM).
2: for s = 1 to S do
3:
Compute Zâ€²(k,s) âˆˆRLâ€²(k,s)Ã—Mâ€²(k,s) by Eq. (11.13).
4:
For each partition k = 1,. . . , K(s), compute the solution Uâ€²(k,s) =
B(k,s) A(k,s)âŠ¤for the fully observed MF by Algorithm 16 with Zâ€²(k,s) as
the observed matrix.
5:
U
(s) â†G({B
(k,s)A
(k,s)âŠ¤}K(s)
k=1; X(s)).
6: end for
7: Update Ïƒ2 by Eq. (11.16).
8: Repeat 2 to 7 until convergence.

284
11 Efï¬cient Solver for Sparse Additive Matrix Factorization
parameter space is not guaranteed. Nevertheless, experimental results in
Section 11.3 show that the MU algorithm performs well in practice.
When Algorithm 16 is applied to the dummy target matrix Zâ€²(k,s)
âˆˆ
RLâ€²(k,s)Ã—Mâ€²(k,s) in Step 4, singular value decomposition is required, which domi-
nates the computation time. However, for many practical SMF terms, including
the rowwise (3.114), the columnwise (3.115), and the elementwise (3.116)
terms (as well as the segmentwise term, which will be deï¬ned for a video
application in Section 11.3.2), Zâ€²(k,s) is a vector or scalar, i.e., Lâ€²(k,s) = 1 or
Mâ€²(k,s) = 1 holds. In such cases, the singular value and the singular vectors are
given simply by
Î³(k,s)
1
= âˆ¥Zâ€²(k,s)âˆ¥,
Ï‰(k,s)
a1
= Zâ€²(k,s)/âˆ¥Zâ€²(k,s)âˆ¥,
Ï‰(k,s)
b1
= 1
if Lâ€²(k,s) = 1,
Î³(k,s)
1
= âˆ¥Zâ€²(k,s)âˆ¥,
Ï‰(k,s)
a1
= 1,
Ï‰(k,s)
b1
= Zâ€²(k,s)/âˆ¥Zâ€²(k,s)âˆ¥
if Mâ€²(k,s) = 1.
11.3 Experimental Results
In this section, we experimentally show good performance of the MU
algorithm (Algorithm 22) over the standard VB algorithm (Algorithm 6 in
Section 3.5). We also demonstrate advantages of SAMF in its ï¬‚exibility in a
real-world application.
11.3.1 Mean Update vs. Standard VB
We compare the algorithms under the following model:
V = ULRCE + E,
where
ULRCE =
4

s=1
U(s) = Ulowâˆ’rank + Urow + Ucolumn + Uelement.
(11.23)
Here, â€œLRCEâ€ stands for the sum of the low-rank, rowwise, columnwise, and
elementwise terms, each of which is deï¬ned in Eqs. (3.113) through (3.116).
We call this model â€œLRCEâ€-SAMF. As explained in Section 3.5, â€œLRCEâ€-
SAMF may be used to separate the clean signal Ulowâˆ’rank from a possible
rowwise sparse component (constantly broken sensors), a columnwise sparse
component (accidental disturbances affecting all sensors), and an element-
wise sparse component (randomly distributed spiky noise). We also evaluate
â€œLCEâ€-SAMF, â€œLREâ€-SAMF, and â€œLEâ€-SAMF, which can be regarded as
generalizations of robust PCA (Cand`es et al., 2011; Ding et al., 2011; Babacan

11.3 Experimental Results
285
et al., 2012b). Note that â€œLEâ€-SAMF corresponds to an SAMF counterpart of
robust PCA.
First, we conducted an experiment with artiï¬cial data. We assume the
empirical VB scenario with unknown noise variance, i.e., all hyperparameters,
{C(k,s)
A
, C(k,s)
B
}K(s)
k=1,
S
s=1, and Ïƒ2, are estimated from observations. We use the full-
rank model (H = min(L, M)) for the low-rank term Ulowâˆ’rank, and expect the
model-induced regularization (MIR) effect (see Chapter 7) to ï¬nd the true rank
of Ulowâˆ’rank, as well as the nonzero entries in Urow, Ucolumn, and Uelement.
We created an artiï¬cial data set with the data matrix size L = 40 and
M = 100, and the rank Hâˆ—= 10 for a true low-rank matrix Ulowâˆ’rankâˆ—= Bâˆ—Aâˆ—âŠ¤.
Each entry in Aâˆ—âˆˆRLÃ—Hâˆ—and Bâˆ—âˆˆRLÃ—Hâˆ—was drawn from Gauss1(0, 1). A true
rowwise (columnwise) part Urowâˆ—(Ucolumnâˆ—) was created by ï¬rst randomly
selecting ÏL rows (ÏM columns) for Ï = 0.05, and then adding a noise
subject to GaussM(0, Î¶IM) (GaussL(0, Î¶IL)) for Î¶ = 100 to each of the selected
rows (columns). A true elementwise part Uelementâˆ—was similarly created by
ï¬rst selecting ÏLM entries and then adding a noise subject to Gauss1(0, Î¶)
to each of the selected entries. Finally, an observed matrix V was created by
adding a noise subject to Gauss1(0, 1) to each entry of the sum ULRCEâˆ—of the
aforementioned four true matrices.
For the standard VB algorithm, we initialized the variational parameters
and the hyperparameters in the following way: the mean parameters,
{A
(k,s), B
(k,s)}K(s)
k=1,
S
s=1, were randomly created so that each entry follows
Gauss1(0, 1); the covariances, {Î£
(k,s)
A
, Î£
(k,s)
B
}K(s)
k=1,
S
s=1 and {C(k,s)
A
, C(k,s)
B
}K(s)
k=1,
S
s=1,
were set to be identity; and the noise variance was set to Ïƒ2 = 1. Note that
we rescaled V so that âˆ¥Vâˆ¥2
Fro /(LM) = 1, before starting iteration. We ran the
standard VB algorithm 10 times, starting from different initial points, and each
trial is plotted by a solid line (labeled as â€œStandard(iniRan)â€) in Figure 11.1.
Initialization for the MU algorithm is simple: we simply set U
(s) = 0(L,M)
for s = 1,. . . , S , and Ïƒ2 = 1. Initialization of all other variables is not needed.
Furthermore, we empirically observed that the initial value for Ïƒ2 does not
affect the result much, unless it is too small. Actually, initializing Ïƒ2 to a large
value is not harmful in the MU algorithm, because it is set to an adequate
value after the ï¬rst iteration with the mean parameters kept U
(s) = 0(L,M). The
performance of the MU algorithm is plotted by the dashed line in Figure 11.1.
Figures 11.1(a) through 11.1(c) show the free energy, the computation time,
and the estimated rank, respectively, over iterations, and Figure 11.1(d) shows
the reconstruction errors after 250 iterations. The reconstruction errors consist
of the overall error
####U
LRCE âˆ’ULRCEâˆ—####
2
Fro /(LM), and the four componentwise

286
11 Efï¬cient Solver for Sparse Additive Matrix Factorization
0
50
100
150
200
250
4.1
4.2
4.3
4.4
4.5
4.6
4.7
Iteration
MeanUpdate
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(a) Free energy
0
50
100
150
200
250
0
2
4
6
8
10
Iteration
Time(sec)
MeanUpdate
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(b) Computation time
0
50
100
150
200
250
0
5
10
15
20
25
30
Iteration
MeanUpdate
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(c) Estimated rank
0
1
2
3
4
5
Overall
Lowâˆ’rank
Row
Column
Element
MeanUpdate
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(d) Reconstruction error
Figure 11.1 Experimental results of â€œLRCEâ€-SAMF on an artiï¬cial data set (L =
40, M = 100, Hâˆ—= 10, Ï = 0.05).
errors
####U
(s) âˆ’U(s)âˆ—####
2
Fro /(LM). The graphs show that the MU algorithm, whose
iteration is computationally slightly more expensive than the standard VB
algorithm, immediately converges to a local minimum with the free energy
substantially lower than the standard VB algorithm. The estimated rank agrees
with the true rank 
H = Hâˆ—= 10, while all 10 trials of the standard VB
algorithm failed to estimate the true rank. It is also observed that the MU
algorithm well reconstructs each of the four terms.
We can slightly improve the performance of the standard VB algo-
rithm by adopting different initialization schemes. The line labeled as
â€œStandard(iniML)â€ in Figure 11.1 indicates the maximum likelihood (ML)
initialization, i.e, (a(k,s)
h
,b
(k,s)
h
) = (Î³(k,s)1/2
h
Ï‰(k,s)
ah , Î³(k,s)1/2
h
Ï‰(k,s)
bh ). Here, Î³(k,s)
h
is the
hth largest singular value of the (k, s)th PR matrix Vâ€²(k,s) of V (such that
Vâ€²(k,s)
lâ€²,mâ€²
=
VX(s)(k,lâ€²,mâ€²)), and Ï‰(k,s)
ah
and Ï‰(k,s)
bh
are the associated right and
left singular vectors. Also, we empirically found that starting from small
Ïƒ2 alleviates the local minimum problem. The line labeled as â€œStandard
(iniMLSS)â€ indicates the ML initialization with Ïƒ2 = 0.0001. We can see that
this scheme successfully recovered the true rank. However, it still performs
substantially worse than the MU algorithm in terms of the free energy and the
reconstruction error.

11.3 Experimental Results
287
0
50
100
150
200
250
3
3.2
3.4
3.6
3.8
4
4.2
Iteration
MeanUpdate
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(a) Free energy
0
50
100
150
200
250
0
20
40
60
80
Iteration
Time(sec)
MeanUpdate
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(b) Computation time
0
50
100
150
200
250
0
10
20
30
40
50
60
Iteration
MeanUpdate
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(c) Estimated rank
0
1
2
3
4
Overall
Lowâˆ’rank
Row
Column
Element
MeanUpdate
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(d) Reconstruction error
Figure 11.2 Experimental results of â€œLEâ€-SAMF on an artiï¬cial data set (L =
100, M = 300, Hâˆ—= 20, Ï = 0.1).
Figure 11.2 shows results of â€œLEâ€-SAMF on another artiï¬cial data set
with L = 100, M = 300, Hâˆ—= 20, and Ï = 0.1. We see that the MU
algorithm performs better than the standard VB algorithm. We also tested
various SAMF models including â€œLCEâ€-SAMF, â€œLREâ€-SAMF, and â€œLEâ€-
SAMF under different settings for M, L, Hâˆ—, and Ï, and empirically found
that the MU algorithm generally gives a better solution with lower free energy
and smaller reconstruction errors than the standard VB algorithm.
Next, we conducted experiments on several data sets from the UCI reposi-
tory (Asuncion and Newman, 2007). Since we do not know the true model of
those data sets, we only focus on the achieved free energy. Figure 11.3 shows
the free energy after convergence in â€œLRCEâ€-SAMF, â€œLCEâ€-SAMF, â€œLREâ€-
SAMF, and â€œLEâ€-SAMF. For better comparison, a constant is added so that
the free energy achieved by the MU algorithm is zero. We can see a clear
advantage of the MU algorithm over the standard VB algorithm.
11.3.2 Real-World Application
Finally, we demonstrate the usefulness of the ï¬‚exibility of SAMF in a
foreground (FG)/background (BG) video separation problem (Figure 3.5 in

288
11 Efï¬cient Solver for Sparse Additive Matrix Factorization
âˆ’0.5
0
0.5
chart
forestfires
glass
iris
spectf
wine
MeanUpdate
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(a) â€œLRCEâ€-SAMF
âˆ’0.5
0
0.5
chart
forestfires
glass
iris
spectf
wine
MeanUpdate
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(b) â€œLCEâ€-SAMF
(c) â€œLREâ€-SAMF
âˆ’0.5
0
0.5
chart
forestfires
glass
iris
spectf
wine
MeanUpdate
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(d) â€œLEâ€-SAMF
Figure 11.3 Experimental results on benchmark data sets. For better comparison,
a constant is added so that the free energy achieved by the MU algorithm is zero
(therefore, the bar for â€œMeanUpdateâ€ is invisible).
Section 3.5). Cand`es et al. (2011) formed the observed matrix V by stacking
all pixels in each frame into each column (Figure 3.6), and applied the robust
PCA (with â€œLEâ€-terms)â€”the low-rank term captures the static BG and the
elementwise (or pixelwise) term captures the moving FG, e.g., people walking
through. SAMF can be seen as an extension of the VB variant of robust PCA
(Babacan et al., 2012b). Accordingly, we use â€œLEâ€-SAMF,
V = Ulowâˆ’rank + Uelement + E,
as a baseline method for comparison.
The SAMF framework enables a ï¬ne-tuned design for the FG term.
Assuming that pixels in an image segment with similar intensity values tend
to share the same label (i.e., FG or BG), we formed a segmentwise sparse
SMF term: Uâ€²(k) for each k is a column vector consisting of all pixels in each
segment. We produced an oversegmented image from each frame by using
the efï¬cient graph-based segmentation (EGS) algorithm (Felzenszwalb and
Huttenlocher, 2004), and substituted the segmentwise sparse term for the FG
term (see Figure 3.7):
V = Ulowâˆ’rank + Usegment + E.

11.3 Experimental Results
289
We call this model segmentation-based SAMF (sSAMF). Note that EGS is
computationally very efï¬cient: it takes less than 0.05 sec on a usual laptop to
segment a 192 Ã— 144 gray image. EGS has several tuning parameters, and the
obtained segmentation is sensitive to some of them. However, we conï¬rmed
that sSAMF performs similarly with visually different segmentations obtained
over a wide range of tuning parameters (see the detailed information in the
section â€œSegmentation Algorithmâ€). Therefore, careful parameter tuning of
EGS is not necessary for our purpose.
We compared sSAMF with â€œLEâ€-SAMF on the â€œWalkByShop1frontâ€ video
from the Caviar data set.1 Thanks to the Bayesian framework, all unknown
parameters (except the ones for segmentation) are estimated from the data,
and therefore no manual parameter tuning is required. For both models (â€œLEâ€-
SAMF and sSAMF), we used the MU algorithm, which was shown in Section
11.3.1 to be practically more reliable than the standard VB algorithm. The
original video consists of 2,360 frames, each of which is a color image with
384 Ã— 288 pixels. We resized each image into 192 Ã— 144 pixels, averaged
over the color channels, and subsampled every 15 frames (the frame IDs are
0, 15, 30,. . . , 2355). Thus, V is of the size of 27,684 [pixels] Ã— 158 [frames].
We evaluated â€œLEâ€-SAMF and sSAMF on this video, and found that both
models perform well (although â€œLEâ€-SAMF failed in a few frames).
In order to contrast between the two models more clearly, we created a more
difï¬cult video by subsampling every ï¬ve frames from 1,501 to 2,000 (the frame
IDs are 1501, 1506,. . . , 1996 and V is of the size of 27,684 [pixels] Ã— 100
[frames]). Since more people walked through in this period, FG/BG separation
is more challenging.
Figure 11.4(a) shows one of the original frames. This is a difï¬cult snap
shot, because a person stayed at the same position for a while, which confuses
separationâ€”any object in the FG pixels is assumed to be moving. Figures
11.4(c) and 11.4(d) show the BG and the FG terms, respectively, obtained by
â€œLEâ€-SAMF. We can see that â€œLEâ€-SAMF failed to separate the person from
BG (the person is partly captured in the BG term). On the other hand, Figures
11.4(e) and 11.4(f) show the BG and the FG terms obtained by sSAMF based
on the segmented image shown in Figure 11.4(b). We can see that sSAMF
successfully separated the person from BG in this difï¬cult frame. A careful
look at the legs of the person reveals how segmentation helps separationâ€”
the legs form a single segment in Figure 11.4(b), and the segmentwise sparse
1 The EC Funded CAVIAR project/IST 2001 37540, found at URL: http://homepages.inf.ed.ac
.uk/rbf/CAVIAR/.

290
11 Efï¬cient Solver for Sparse Additive Matrix Factorization
(a) Original
(b) Segmented
(c) BG (â€˜LEâ€™-SAMF)
(d) FG (â€˜LEâ€™-SAMF)
(e) BG (sSAMF)
(f) FG (sSAMF)
Figure 11.4 â€œLEâ€-SAMF vs. segmentation-based SAMF in FG/BG video
separation.
term (Figure 11.4(f)) captured all pixels on the legs, while the pixelwise sparse
term (Figure 11.4(d)) captured only a part of those pixels. We observed that, in
all frames of the difï¬cult video, as well as the easier one, sSAMF gave good
separation, while â€œLEâ€-SAMF failed in several frames.
For reference, we applied the convex formulation of robust PCA (Cand`es
et al., 2011), which solves the following minimization problem by the inexact
augmented Lagrange multiplier (ALM) algorithm (Lin et al., 2009):
min
UBG,UFG âˆ¥UBGâˆ¥tr + Î»âˆ¥UFGâˆ¥1
s.t.
V = UBG + UFG,
(11.24)

11.3 Experimental Results
291
(a) BG (ALM Î» = 0.001)
(b) FG (ALM Î» = 0.001)
(c) BG (ALM Î» = 0.005)
(d) FG (ALM Î» = 0.05)
(e) BG (ALM Î» = 0.025)
(f) FG (ALM Î» = 0.025)
Figure 11.5 FG/BG video separation by the convex formulation of robust PCA
(11.24) for Î» = 0.001 (top row), Î» = 0.005 (middle row), and Î» = 0.025 (bottom
row).
where âˆ¥Â· âˆ¥tr and âˆ¥Â· âˆ¥1 denote the trace norm and the â„“1-norm of a matrix,
respectively. Figure 11.5 shows the obtained BG and FG terms of the same
frame as that in Figure 11.4 with Î» = 0.001, 0.005, 0.025. We see that the
performance strongly depends on the parameter value of Î», and that sSAMF
gives an almost identical result (bottom row in Figure 11.4) to the best ALM
result with Î» = 0.005 (middle row in Figure 11.5) without any manual
parameter tuning.
In the following subsections, we give detailed information on the segmen-
tation algorithm and the computation time.

292
11 Efï¬cient Solver for Sparse Additive Matrix Factorization
(a) Original image
(b) Segmented (k = 1)
(c) Segmented (k = 10)
(d) Segmented (k = 100)
Figure 11.6 Segmented images by the efï¬cient graph-based segmentation (EGS)
algorithm with different k values. They are visually different, but with all these
segmentations, sSAMF gave almost identical FB/BG separations. The original
image (a) is the same frame as the one in Figure 11.4.
Segmentation Algorithm
For the EGS algorithm (Felzenszwalb and Huttenlocher, 2004), we used
the code publicly available from the authorsâ€™ homepage.2 EGS has three
tuning parameters: sigma, the smoothing parameter; k, the threshold param-
eter; and minc, the minimum segment size. Among them, k dominantly
determines the typical size of segments (larger k leads to larger segments).
To obtain oversegmented images for sSAMF in our experiment, we chose
k = 50, and the other parameters are set to sigma = 0.5 and minc = 20
as recommended by the authors. We also tested other parameter setting, and
observed that FG/BG separation by sSAMF performed almost equally for
1 â‰¤k â‰¤100, despite the visual variation of segmented images (see Figure
11.6). Overall, we empirically observed that the performance of sSAMF is
not very sensitive to the selection of segmented images, unless it is highly
undersegmented.
2 www.cs.brown.edu/âˆ¼pff/

11.3 Experimental Results
293
Computation Time
The computation time for segmentation by EGS was less than 10 sec (for
100 frames). Forming the one-to-one map X took more than 80 sec (which
is expected to be improved). In total, sSAMF took 600 sec on a Linux machine
with Xeon X5570 (2.93GHz), while â€œLEâ€-SAMF took 700 sec. This slight
reduction in computation time comes from the reduction in the number K of
partitions for the FG term, and hence the number of calculations of partial
analytic solutions.

12
MAP and Partially Bayesian Learning
Variational Bayesian (VB) learning generally offers a tractable approximation
of Bayesian learning, and efï¬cient iterative local search algorithms were
derived for many practical models based on the conditional conjugacy (see
Part II). However, in some applications, VB learning is still computationally
too costly. In such cases, cruder approximation methods, where all or some
of the parameters are point-estimated, with potentially less computation cost,
are attractive alternatives. For example, Chu and Ghahramani (2009) applied
partially Bayesian (PB) learning (introduced in Section 2.2.2), where the core
tensor is integrated out and the factor matrices are point-estimated, to Tucker
factorization (TF) (Carroll and Chang, 1970; Harshman, 1970; Tucker, 1996;
Kolda and Bader, 2009). MÃ¸rup and Hansen (2009) applied the maximum
a posteriori (MAP) learning to TF with the empirical Bayesian procedure,
i.e., the hyperparameters are also estimated from observations. Their proposed
empirical MAP learning, which only requires the same order of computation
costs as the plain alternating least squares algorithm (Kolda and Bader,
2009), showed its model selection capability through the automatic relevance
determination (ARD) property.
Motivated by the empirical success, we have analyzed PB learning and
MAP learning and their empirical Bayesian variants (Nakajima et al., 2011;
Nakajima and Sugiyama, 2014), which this chapter introduces. Focusing on
fully observed matrix factorization (MF), we ï¬rst analyze the global and local
solutions of MAP learning and PB learning and their empirical Bayesian
variants. This analysis theoretically reveals similarities and dissimilarities to
VB learning. After that, we discuss more general cases, including MF with
missing entries and TF.
294

12.1 Theoretical Analysis in Fully Observed MF
295
12.1 Theoretical Analysis in Fully Observed MF
In this section, we formulate MAP learning and PB learning in the free energy
minimization framework (Section 2.1.1) and derive analytic-form solutions.
12.1.1 Problem Description
The model likelihood and the prior of the MF model are given by
p(V|A, B) âˆexp

âˆ’1
2Ïƒ2
###V âˆ’BAâŠ¤###2
Fro

,
(12.1)
p(A) âˆexp

âˆ’1
2tr

ACâˆ’1
A AâŠ¤ 
,
(12.2)
p(B) âˆexp

âˆ’1
2tr

BCâˆ’1
B BâŠ¤ 
,
(12.3)
where the prior covariance matrices are restricted to be diagonal:
CA = Diag(c2
a1,. . . , c2
aH),
CB = Diag(c2
b1,. . . , c2
bH),
for cah, cbh > 0, h = 1,. . . , H. Without loss of generality, we assume that the
diagonal entries of the product CACB are arranged in nonincreasing order, i.e.,
cahcbh â‰¥cahâ€² cbhâ€² for any pair h < hâ€².
As in Section 2.2.2, we treat MAP learning and PB learning as special
cases of VB learning in the free energy minimization framework. The Bayes
posterior is given by
p(A, B|V) = p(V|A, B)p(A)p(B)
p(V)
,
(12.4)
which is intractable for the MF model (12.1) through (12.3). Accordingly, we
approximate it by
r = argmin
r
F(r)
s.t.
r(A, B) âˆˆG,
(12.5)
where G speciï¬es the constraint on the approximate posterior. F(r) is the free
energy, deï¬ned as
F(r) =
/
log
r(A, B)
p(V|A, B)p(A)p(B)
0
r(A,B)
(12.6)
=
/
log
r(A, B)
p(A, B|V)
0
r(A,B)
âˆ’log p(V),

296
12 MAP and Partially Bayesian Learning
which is a monotonic function of the KL divergence

log
r(A,B)
p(A,B|V)

r(A,B) to the
Bayes posterior.
Constraints for MAP Learning and PB Learning
MAP learning ï¬nds the mode of the posterior distribution, which amounts to
approximating the posterior with the Dirac delta function. Accordingly, solving
the problem (12.5) with the following constraint gives the MAP solution:
rMAP(A, B) = Î´(A; A)Î´(B; B),
(12.7)
where Î´(Î¼;Î¼) denotes the (pseudo-)Dirac delta function located at Î¼.1
Under the MAP constraint (12.7), the free energy (12.6) is written as
FMAP(A, B) =
/
log
Î´(A; A)Î´(B; B)
p(V|A, B)p(A)p(B)
0
Î´(A;A)Î´(B;B)
= âˆ’log p(V|A, B)p(A)p(B) + Ï‡A + Ï‡B,
(12.8)
where
Ï‡A =

log Î´(A; A)

Î´(A;A) ,
Ï‡B =

log Î´(B; B)

Î´(B;B)
(12.9)
are the negative entropies of the pseudo-Dirac delta functions.
PB learning is a strategy to analytically integrate out as many parameters as
possible, and the rest are point-estimated. In the MF model, a natural choice
is to integrate A out and point-estimate B, which we call PB-A learning, or
to integrate B out and point-estimate A, which we call PB-B learning. Their
solutions can be obtained by solving the problem (12.5) with the following
constraints, respectively:
rPBâˆ’A(A, B) = rPB
A (A)Î´(B; B),
(12.10)
rPBâˆ’B(A, B) = Î´(A; A)rPB
B (B).
(12.11)
Under the PB-A constraint (12.10), the free energy (12.6) is written as
FPBâˆ’A(rPB
A , B) =
/
log
rPB
A (A)
p(V|A, B)p(A)p(B)
0
rPB
A (A)
+ Ï‡B
=
/
log
rPB
A (A)
p(A|V, B)
0
rPB
A (A)
âˆ’log p(V|B)p(B) + Ï‡B,
(12.12)
1 By the pseudo-Dirac delta function, we mean an extremely localized density function, e.g.,
Î´(A; A) âˆexp

âˆ’âˆ¥A âˆ’Aâˆ¥2
Fro/(2Îµ2)
 
with a very small but strictly positive variance Îµ2 > 0, such
that its tail effect can be ignored, while its negative entropy Ï‡A = âŸ¨log Î´(A; A)âŸ©Î´(A;A) remains
ï¬nite.

12.1 Theoretical Analysis in Fully Observed MF
297
where
p(A|V, B) = p(V|A, B)p(A)
p(V|B)
,
(12.13)
and
p(V|B) =

p(V|A, B)

p(A)
(12.14)
are the posterior distribution with respect to A (given B) and the marginal
distribution, respectively. Note that Eq. (12.12) is a functional of rPB
A
and B,
and Ï‡B is a constant with respect to them.
Since only the ï¬rst term depends on rPB
A , on which no restriction is imposed,
Eq. (12.12) is minimized when
rPB
A (A) = p(A|V, B)
(12.15)
for any B. With Eq. (12.15), the ï¬rst term in Eq. (12.12) vanishes, and thus the
estimator for B is given by
B
PBâˆ’A = argmin
B
Â´FPBâˆ’A(B),
(12.16)
where
Â´FPBâˆ’A(B) â‰¡min
rPB
A
FPBâˆ’A(rPB
A , B) = âˆ’log p(V|B)p(B) + Ï‡B.
(12.17)
The process to compute Â´FPBâˆ’A(B) in Eq. (12.17) corresponds to integrating
A out based on the conditional conjugacy. The probabilistic PCA, introduced
in Section 3.1.2, was originally proposed with PB-A learning (Tipping and
Bishop, 1999).
In the same way, we can obtain the approximate posterior under the PB-B
constraint (12.11) as follows:
rPB
B (B) = p(B|V, A),
(12.18)
A
PBâˆ’B = argmin
A
Â´FPBâˆ’B(A),
(12.19)
where
p(B|V, A) = p(V|A, B)p(B)
p(V|A)
,
(12.20)
p(V|A) =

p(V|A, B)

p(B) ,
(12.21)
Â´FPBâˆ’B(A) â‰¡min
rPB
B
FPBâˆ’B(rPB
B , A) = âˆ’log p(V|A)p(A) + Ï‡A.
(12.22)

298
12 MAP and Partially Bayesian Learning
We deï¬ne PB learning as one of PB-A learning and PB-B learning giving a
lower free energy. Namely,
rPB(A, B) =
â§âªâªâ¨âªâªâ©
rPBâˆ’A(A, B)
if min FPBâˆ’A(rPB
A , B) â‰¤min FPBâˆ’B(rPB
B , A),
rPBâˆ’B(A, B)
otherwise.
Free Energies for MAP Learning and PB Learning
Apparently, the constraint (12.7) for MAP learning and the constraints (12.10)
and (12.11) for PB learning forces independence between A and B, and
therefore, they are stronger than the independence constraint
rVB(A, B) = rVB
A (A)rVB
B (B)
(12.23)
for VB learning. In Chapter 6, we showed that the VB posterior under the
independence constraint (12.23) is in the following Gaussian form:
rA(A) = MGaussM,H(A; A, IM âŠ—Î£A) âˆexp
â›âœâœâœâœâœââˆ’
tr
!
(Aâˆ’A)Î£
âˆ’1
A (Aâˆ’A)âŠ¤
"
2
ââŸâŸâŸâŸâŸâ ,
(12.24)
rB(B) = MGaussL,H(B; B, IL âŠ—Î£B) âˆexp
â›âœâœâœâœâœââˆ’
tr
!
(Bâˆ’B)Î£
âˆ’1
B (Bâˆ’B)âŠ¤
"
2
ââŸâŸâŸâŸâŸâ ,
(12.25)
where the posterior covariances, Î£A and Î£B, are diagonal. Furthermore, Eqs.
(12.24) and (12.25) can be the pseudo-Dirac delta functions by setting the
posterior covariances to Î£A = Îµ2IH and Î£B = Îµ2IH, respectively, for a very
small Îµ2 > 0.
Consequently, the MAP and the PB solutions can be obtained by mini-
mizing the free energy for VB learning with posterior covariances clipped to
Îµ2IH, according to the corresponding constraint. Namely, we start from the
free energy expression (6.42) for VB learning, i.e.,
2F = LM log(2Ï€Ïƒ2) +
L
h=1 Î³2
h
Ïƒ2
+
H

h=1
2Fh,
(12.26)
where
2Fh = M log
c2
ah
Ïƒ2ah
+ L log
c2
bh
Ïƒ2
bh
+
a2
h + MÏƒ2
ah
c2ah
+
b2
h + LÏƒ2
bh
c2
bh
âˆ’(L + M) +
âˆ’2ahbhÎ³h +

a2
h + MÏƒ2
ah
 b2
h + LÏƒ2
bh
 
Ïƒ2
,
(12.27)
and set
Ïƒ2
ah = Îµ2
h = 1,. . . , H,
(12.28)

12.1 Theoretical Analysis in Fully Observed MF
299
for MAP learning and PB-B learning, and
Ïƒ2
bh = Îµ2
h = 1,. . . , H,
(12.29)
for MAP learning and PB-A learning. Here,
A = $a1Ï‰a1,. . . ,aHÏ‰aH
% ,
B =
b1Ï‰b1,. . . ,bHÏ‰bH
 
,
Î£A = Diag

Ïƒ2
a1,. . . , Ïƒ2
aH
 
,
Î£B = Diag

Ïƒ2
b1,. . . , Ïƒ2
bH
 
,
and
V =
L

h=1
Î³hÏ‰bhÏ‰âŠ¤
ah
is the singular value decomposition (SVD) of V.
Thus, the free energies for MAP learning, PB-A learning, and PB-B
learning are given by Eq. (12.26) for
2FMAP
h
= M log c2
ah + L log c2
bh +
a2
h
c2ah +
b2
h
c2
bh
+
âˆ’2ahbhÎ³h+a2
hb2
h
Ïƒ2
âˆ’(L + M) + (L + M)Ï‡,
(12.30)
2FPBâˆ’A
h
= M log
c2
ah
Ïƒ2ah + L log c2
bh +
a2
h+MÏƒ2
ah
c2ah
+
b2
h
c2
bh
+
âˆ’2ahbhÎ³h+

a2
h+MÏƒ2
ah
 b2
h
Ïƒ2
âˆ’(L + M) + LÏ‡,
(12.31)
2FPBâˆ’B
h
= M log c2
ah + L log
c2
bh
Ïƒ2
bh
+
a2
h
c2ah +
b2
h+LÏƒ2
bh
c2
bh
+
âˆ’2ahbhÎ³h+a2
h
!
b2
h+LÏƒ2
bh
"
Ïƒ2
âˆ’(L + M) + MÏ‡,
(12.32)
respectively, where
Ï‡ = âˆ’log Îµ2
(12.33)
is a large positive constant corresponding to the negative entropy of the one-
dimensional pseudo-Dirac delta function.
As in VB learning, the free energy (12.26) is separable for each singular
component as long as the noise variance Ïƒ2 is treated as a constant. Therefore,
the variational parameters (ah,bh, Ïƒ2
ah, Ïƒ2
bh) and the prior covariances (c2
ah, c2
bh)
for the hth component can be estimated by minimizing Fh.

300
12 MAP and Partially Bayesian Learning
12.1.2 Global Solutions
In this section, we derive the global minimizers of the free energies, (12.30)
through (12.32), and analyze their behavior.
Global MAP and PB Solutions
By minimizing the MAP free energy (12.30), we can obtain the global solution
for MAP learning, given the hyperparameters CA, CB, Ïƒ2 treated as ï¬xed
constants. Let
U = BAâŠ¤
be the target low-rank matrix.
Theorem 12.1
Given CA, CB âˆˆDH
++, and Ïƒ2 âˆˆR++, the MAP solution of the
MF model (12.1) through (12.3) is given by
U
MAP =
H

h=1
Î³MAP
h
Ï‰bhÏ‰âŠ¤
ah,
where
Î³MAP
h
=
â§âªâªâ¨âªâªâ©
Ë˜Î³MAP
h
if Î³h â‰¥Î³MAP
h
,
0
otherwise,
(12.34)
where
Î³MAP
h
=
Ïƒ2
cahcbh
,
(12.35)
Ë˜Î³MAP
h
= Î³h âˆ’
Ïƒ2
cahcbh
.
(12.36)
Proof
Eq. (12.30) can be written as a function of ah andbh as
2FMAP
h
= a2
h
c2ah
+
b2
h
c2
bh
+ âˆ’2ahbhÎ³h +a2
hb2
h
Ïƒ2
+ const.
=
â›âœâœâœâœâ
ah
cah
âˆ’
bh
cbh
ââŸâŸâŸâŸâ 
2
+
!
ahbh âˆ’
!
Î³h âˆ’
Ïƒ2
cahcbh
""2
Ïƒ2
+ const.
(12.37)
Noting that the ï¬rst two terms are nonnegative, we ï¬nd that, if Î³h > Î³MAP
h
,
Eq. (12.37) is minimized when
Ë˜Î³MAP
h
â‰¡ahbh = Î³h âˆ’
Ïƒ2
cahcbh
,
(12.38)
Î´MAP
h
â‰¡ah
bh
= cah
cbh
.
(12.39)

12.1 Theoretical Analysis in Fully Observed MF
301
Otherwise, it is minimized when
ah = 0,
bh = 0,
which completes the proof.
â–¡
Eqs. (12.38) and (12.39) immediately lead to the following corollary:
Corollary 12.2
The MAP posterior is given by
rMAP(A, B) =
H

h=1
Î´(ah;ahÏ‰ah)
H

h=1
Î´(bh;bhÏ‰bh),
(12.40)
with the following estimators: if Î³h > Î³MAP
h
,
ah = Â±
.
Ë˜Î³MAP
h
Î´MAP
h
,
bh = Â±
>
@
Ë˜Î³MAP
h
Î´MAP
h
,
(12.41)
where
Î´MAP
h

â‰¡ah
bh

= cah
cbh
,
(12.42)
and otherwise
ah = 0,
bh = 0.
(12.43)
Similarly, by minimizing the PB-A free energy (12.31) and the PB-B free
energy (12.32) and comparing them, we can obtain the global solution for PB
learning:
Theorem 12.3
Given CA, CB âˆˆDH
++, and Ïƒ2 âˆˆR++, the PB solution of the
MF model (12.1) through (12.3) is given by
U
PB =
H

h=1
Î³PB
h Ï‰bhÏ‰âŠ¤
ah,
where
Î³PB
h
=
â§âªâªâ¨âªâªâ©
Ë˜Î³PB
h
if Î³h â‰¥Î³PB
h ,
0
otherwise,
(12.44)
where
Î³PB
h
= Ïƒ
A
max(L, M) +
Ïƒ2
c2ahc2
bh
,
(12.45)
Ë˜Î³PB
h
=
â›âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ
1 âˆ’
Ïƒ2

max(L, M) +
B
max(L, M)2 + 4
Î³2
h
c2ahc2
bh

2Î³2
h
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
Î³h.
(12.46)

302
12 MAP and Partially Bayesian Learning
Corollary 12.4
The PB posterior is given by
rPB(A, B) =
â§âªâªâªâªâªâ¨âªâªâªâªâªâ©
rPBâˆ’A(A, B)
if L < M,
rPBâˆ’A(A, B) or rPBâˆ’B(A, B)
if L = M,
rPBâˆ’B(A, B)
if L > M,
(12.47)
where rPBâˆ’A(A, B) and rPBâˆ’B(A, B) are the PB-A posterior and the PB-B
posterior, respectively, given as follows. The PB-A posterior is given by
rPBâˆ’A(A, B) =
H

h=1
GaussM(ah;ahÏ‰ah, Ïƒ2
ah IM)
H

h=1
Î´(bh;bhÏ‰bh),
(12.48)
with the following estimators: if
Î³h > Î³PBâˆ’A
h
â‰¡Ïƒ
A
M +
Ïƒ2
c2ahc2
bh
,
(12.49)
then
ah = Â±
.
Ë˜Î³PBâˆ’A
h
Î´PBâˆ’A
h
,
bh = Â±
>
@
Ë˜Î³PBâˆ’A
h
Î´PBâˆ’A
h
,
Ïƒ2
ah =
Ïƒ2
Ë˜Î³PBâˆ’A
h
/Î´PBâˆ’A
h
+ Ïƒ2/c2ah
,
(12.50)
where
Ë˜Î³PBâˆ’A
h

â‰¡ahbh
 
=
â›âœâœâœâœâœâœâœâœâœâœâœâ
1 âˆ’
Ïƒ2
â›âœâœâœâœâœâœâM+
A
M2+4
Î³2
h
c2ah c2
bh
ââŸâŸâŸâŸâŸâŸâ 
2Î³2
h
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
Î³h,
(12.51)
Î´PBâˆ’A
h

â‰¡ah
bh

=
c2
ah
â›âœâœâœâœâœâœâM+
A
M2+4
Î³2
h
c2ah c2
bh
ââŸâŸâŸâŸâŸâŸâ 
2Î³h
,
(12.52)
and otherwise
ah = 0,
bh = 0,
Ïƒ2
ah = c2
ah.
(12.53)
The PB-B posterior is given by
rPBâˆ’B(A, B) =
H

h=1
Î´(ah;ahÏ‰ah)
H

h=1
GaussL(bh;bhÏ‰bh, Ïƒ2
bh IL),
(12.54)
with the following estimators: if
Î³h > Î³PBâˆ’B
h
â‰¡Ïƒ
A
L +
Ïƒ2
c2ahc2
bh
,
(12.55)

12.1 Theoretical Analysis in Fully Observed MF
303
then
ah = Â±
.
Ë˜Î³PBâˆ’B
h
Î´PBâˆ’B
h
,
bh = Â±
>
@
Ë˜Î³PBâˆ’B
h
Î´PBâˆ’B
h
,
Ïƒ2
bh =
Ïƒ2
Ë˜Î³PBâˆ’B
h
Î´PBâˆ’B
h
+ Ïƒ2/c2
bh
,
(12.56)
where
Ë˜Î³PBâˆ’B
h

â‰¡ahbh
 
=
â›âœâœâœâœâœâœâœâœâœâœâœâ
1 âˆ’
Ïƒ2
â›âœâœâœâœâœâœâL+
A
L2+4
Î³2
h
c2ah c2
bh
ââŸâŸâŸâŸâŸâŸâ 
2Î³2
h
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
Î³h,
(12.57)
Î´PBâˆ’B
h

â‰¡ah
bh

=
â›âœâœâœâœâœâœâœâœâœâœâœâ
c2
bh
â›âœâœâœâœâœâœâL+
A
L2+4
Î³2
h
c2ah c2
bh
ââŸâŸâŸâŸâŸâŸâ 
2Î³h
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
âˆ’1
,
(12.58)
and otherwise
ah = 0,
bh = 0,
Ïƒ2
bh = c2
bh.
(12.59)
Note that, when L = M, the choice from the PB-A and the PB-B posteriors
depends on the prior covariances CA and CB. However, as long as the estimator
for the target low-rank matrix U is concerned, the choice does not matter, as
Theorem 12.3 states. This is because
Î³PBâˆ’A
h
= Î³PBâˆ’B
h
and
Ë˜Î³PBâˆ’A
h
= Ë˜Î³PBâˆ’B
h
when
L = M,
as Corollary 12.4 implies.
Proofs of Theorem 12.3 and Corollary 12.4
We ï¬rst derive the PB-A solution by minimizing the corresponding free energy
(12.31):
2FPBâˆ’A
h
= M log
c2
ah
Ïƒ2ah + L log c2
bh +
a2
h+MÏƒ2
ah
c2ah
+
b2
h
c2
bh
+
âˆ’2ahbhÎ³h+

a2
h+MÏƒ2
ah
 b2
h
Ïƒ2
âˆ’(L + M) + LÏ‡.
(12.60)
As a function ofah (treatingbh and Ïƒ2
ah as ï¬xed constants), Eq. (12.60) can
be written as
2FPBâˆ’A
h
(ah) = a2
h
c2ah
+ âˆ’2ahbhÎ³h +a2
hb2
h
Ïƒ2
+ const.

304
12 MAP and Partially Bayesian Learning
=
b2
h + Ïƒ2/c2
ah
Ïƒ2
â›âœâœâœâœâœâa2
h âˆ’2
bhÎ³h
b2
h + Ïƒ2/c2ah
ah
ââŸâŸâŸâŸâŸâ + const.
=
b2
h + Ïƒ2/c2
ah
Ïƒ2
â›âœâœâœâœâœâah âˆ’
bhÎ³h
b2
h + Ïƒ2/c2ah
ââŸâŸâŸâŸâŸâ 
2
+ const.,
which is minimized when
ah =
bhÎ³h
b2
h + Ïƒ2/c2ah
.
(12.61)
As a function of Ïƒ2
ah (treatingah andbh as ï¬xed constants), Eq. (12.60) can be
written as
2FPBâˆ’A
h
(Ïƒ2
ah) = M
!
âˆ’log Ïƒ2
ah +
!
1
c2ah +
b2
h
Ïƒ2
"
Ïƒ2
ah
"
+ const.
= M
!
âˆ’log
!
1
c2ah +
b2
h
Ïƒ2
"
Ïƒ2
ah +
!
1
c2ah +
b2
h
Ïƒ2
"
Ïƒ2
ah
"
+ const.,
which is minimized when
Ïƒ2
ah =
â›âœâœâœâœâœâ
1
c2ah
+
b2
h
Ïƒ2
ââŸâŸâŸâŸâŸâ 
âˆ’1
=
Ïƒ2
b2
h + Ïƒ2/c2ah
.
(12.62)
Therefore, substituting Eqs. (12.61) and (12.62) into Eq. (12.60) gives the free
energy withah and Ïƒ2
ah already optimized:
2 Â´FPBâˆ’A
h
= min
ah,Ïƒ2ah
2FPBâˆ’A
h
= âˆ’M log Ïƒ2
ah +
a2
h+MÏƒ2
ah
Ïƒ2ah
+
b2
h
c2
bh
âˆ’2ahbhÎ³h
Ïƒ2
+ const.
= M log(b2
h + Ïƒ2/c2
ah) âˆ’
b2
hÎ³2
h
Ïƒ2(b2
h+Ïƒ2/c2ah) +
b2
h
c2
bh
+ const.
= M log(b2
h + Ïƒ2/c2
ah) +
!
Î³2
h
Ïƒ2 âˆ’
b2
hÎ³2
h
Ïƒ2(b2
h+Ïƒ2/c2ah)
"
+

b2
h
c2
bh
+
Ïƒ2
c2ahc2
bh

+ const.
= M log(b2
h + Ïƒ2/c2
ah) +
Î³2
h
c2ah (b2
h + Ïƒ2/c2
ah)âˆ’1 +
1
c2
bh
(b2
h + Ïƒ2/c2
ah) + const.
(12.63)
In the second last equation, we added some constants so that the minimizer can
be found by the following lemma:
Lemma 12.5
The function
f(x) = Î¾log log x + Î¾âˆ’1xâˆ’1 + Î¾1x

12.1 Theoretical Analysis in Fully Observed MF
305
of x > 0 for positive coefï¬cients Î¾log, Î¾âˆ’1, Î¾1 > 0 is strictly quasiconvex,2 and
minimized at
x =
âˆ’Î¾log +
.
Î¾2
log + 4Î¾1Î¾âˆ’1
2Î¾1
.
Proof
f(x) is differentiable in x > 0, and its ï¬rst derivative is
âˆ‚f
âˆ‚x = Î¾logxâˆ’1 âˆ’Î¾âˆ’1xâˆ’2 + Î¾1
= xâˆ’2 
Î¾1x2 + Î¾logx âˆ’Î¾âˆ’1
 
= xâˆ’2
â›âœâœâœâœâx +
Î¾log+
.
Î¾2
log+4Î¾1Î¾âˆ’1
2Î¾1
ââŸâŸâŸâŸâ 
â›âœâœâœâœâx âˆ’
âˆ’Î¾log+
.
Î¾2
log+4Î¾1Î¾âˆ’1
2Î¾1
ââŸâŸâŸâŸâ .
Since the ï¬rst two factors are positive, we ï¬nd that f(x) is strictly decreasing
for 0 < x < x, and strictly increasing for x > x, which proves the lemma.
â–¡
By applying Lemma 12.5 to Eq. (12.63) with x = b2
h + Ïƒ2/c2
ah, we ï¬nd that
Â´FPBâˆ’A
h
is strictly quasiconvex and minimized when
b2
h + Ïƒ2/c2
ah =
c2
bh

âˆ’M +
B
M2 +
4Î³2
h
c2ahc2
bh

2
.
(12.64)
Sinceb2
h is nonnegative, the minimizer of the free energy (12.63) is given by
b2
h = max
â§âªâªâªâªâªâªâªâ¨âªâªâªâªâªâªâªâ©
0,
c2
bh

âˆ’

M +
2Ïƒ2
c2ahc2
bh

+
B
M2 +
4Î³2
h
c2ahc2
bh

2
â«âªâªâªâªâªâªâªâ¬âªâªâªâªâªâªâªâ­
.
(12.65)
Apparently, Eq. (12.65) is positive when
â›âœâœâœâœâœâM + 2Ïƒ2
c2ahc2
bh
ââŸâŸâŸâŸâŸâ 
2
< M2 + 4Î³2
h
c2ahc2
bh
,
which leads to the thresholding condition:
Î³h > Î³PBâˆ’A
h
.
By using Eqs. (12.61), (12.62), (12.65), and
b2
h + Ïƒ2/c2
ah
 âˆ’1 =
â§âªâªâªâªâªâªâ¨âªâªâªâªâªâªâ©
c2
ah
â›âœâœâœâœâœâœâM+
A
M2+
4Î³2
h
c2ah c2
bh
ââŸâŸâŸâŸâŸâŸâ 
2Î³2
h
if Î³h > Î³PBâˆ’A
h
,
c2
ah
Ïƒ2
otherwise,
(12.66)
2 The deï¬nition of quasiconvexity is given in footnote 1 in Section 8.1.

306
12 MAP and Partially Bayesian Learning
derived from Eqs. (12.64) and (12.65), we obtain
Î³PBâˆ’A
h
â‰¡ahbh =
b2
hÎ³h
b2
h + Ïƒ2/c2ah
=
â›âœâœâœâœâœâ1 âˆ’
Ïƒ2/c2
ah
b2
h + Ïƒ2/c2ah
ââŸâŸâŸâŸâŸâ Î³h
=
â§âªâªâªâªâªâªâªâªâ¨âªâªâªâªâªâªâªâªâ©
â›âœâœâœâœâœâœâœâœâœâœâœâ
1 âˆ’
Ïƒ2
â›âœâœâœâœâœâœâM+
A
M2+4
Î³2
h
c2ah c2
bh
ââŸâŸâŸâŸâŸâŸâ 
2Î³2
h
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
Î³h
if Î³h > Î³PBâˆ’A
h
,
0
otherwise,
Î´PBâˆ’A
h
â‰¡ah
bh
=
Î³h
b2
h + Ïƒ2/c2ah
=
c2
ah
â›âœâœâœâœâœâœâM+
A
M2+4
Î³2
h
c2ah c2
bh
ââŸâŸâŸâŸâŸâŸâ 
2Î³h
for Î³h > Î³PBâˆ’A
h
,
Ïƒ2
ah =
Ïƒ2
b2
h + Ïƒ2/c2ah
=
â§âªâªâªâ¨âªâªâªâ©
Ïƒ2
Î³PBâˆ’A
h
/Î´PBâˆ’A
h
+Ïƒ2/c2ah
if Î³h > Î³PBâˆ’A
h
,
c2
ah
otherwise.
Thus, we have obtained the PB-A posterior (12.48) speciï¬ed by Eqs. (12.49)
through (12.53).
In exactly the same way, we can obtain the PB-B posterior (12.54) speciï¬ed
by Eqs. (12.55) through (12.59), by minimizing the free energy (12.32) for
PB-B learning.
Finally, for the choice from the PB-A posterior and the PB-B posterior, we
can easily prove the following lemma:
Lemma 12.6
It holds that
min
ah,bh,Ïƒ2ah
FPBâˆ’A
h
<
min
ah,bh,Ïƒ2
bh
FPBâˆ’B
h
if
L < M,
min
ah,bh,Ïƒ2ah
FPBâˆ’A
h
>
min
ah,bh,Ïƒ2
bh
FPBâˆ’B
h
if
L > M.
Proof
When comparing the PB-A free energy (12.31) and the PB-B free
energy (12.32), the last terms are dominant since we assume that the negative
entropy Ï‡, deï¬ned by Eq. (12.33), of the one-dimensional pseudo-Dirac delta
function is ï¬nite but arbitrarily large. Then, comparing the last terms, each

12.1 Theoretical Analysis in Fully Observed MF
307
of which is proportional to the number of parameters point-estimated, of
Eqs. (12.31) and (12.32) proves Lemma 12.6.
â–¡
Combining Lemma 12.6 with the PB-A posterior and the PB-B posterior
obtained before, we have Corollary 12.4. Theorem 12.3 is a direct consequence
of Corollary 12.4.
â–¡
Comparison between MAP, PB, and VB Solutions
Here we compare the MAP solution (Theorem 12.1), the PB solution (Theorem
12.3), and the VB solution (Theorem 6.7 in Chapter 6). For all methods, the
solution is a shrinkage estimator applied to each singular value, i.e., in the
following form for Î³h â‰¤Î³h:
U =
H

h=1
Î³hÏ‰bhÏ‰âŠ¤
ah,
where
Î³h =
â§âªâªâ¨âªâªâ©
Î³h
if Î³h â‰¥Î³h,
0
otherwise.
(12.67)
When the prior is ï¬‚at, i.e., cahcbh â†’âˆ, the truncation threshold Î³h and the
shrinkage factor Ë˜Î³h are simpliï¬ed as
lim
cahcbhâ†’âˆÎ³MAP
h
= 0,
lim
cahcbhâ†’âˆË˜Î³MAP
h
= Î³h,
(12.68)
lim
cahcbhâ†’âˆÎ³PB
h
= Ïƒ âˆšmax(L, M),
lim
cahcbhâ†’âˆË˜Î³PB
h
=

1 âˆ’max(L,M)Ïƒ2
Î³h
 
Î³h,
(12.69)
lim
cahcbhâ†’âˆÎ³VB
h
= Ïƒ âˆšmax(L, M),
lim
cahcbhâ†’âˆË˜Î³VB
h
=

1 âˆ’max(L,M)Ïƒ2
Î³h
 
Î³h,
(12.70)
and therefore the estimators Î³h can be written as
Î³MAP
h
= Î³h,
(12.71)
Î³PB
h
= max

0,

1 âˆ’max(L,M)Ïƒ2
Î³h
 
Î³h
 
,
(12.72)
Î³VB
h
= max

0,

1 âˆ’max(L,M)Ïƒ2
Î³h
 
Î³h
 
.
(12.73)
As expected, the MAP estimator (12.71) coincides with the maximum
likelihood (ML) estimator when the prior is ï¬‚at. On the other hand, the PB
estimator (12.72) and the VB estimator (12.73) do not converge to the ML
estimator. Interestingly, the PB estimator and the VB estimator coincide with
each other, and they are in the form of the positive-part Jamesâ€“Stein (PJS)
estimator (James and Stein, 1961; Efron and Morris, 1973) applied to each
singular component (see Appendix A for a short introduction to the Jamesâ€“
Stein estimator). The reason why the VB estimator is shrunken even with the
ï¬‚at prior was explained in Chapter 7 in terms of model-induced regularization

308
12 MAP and Partially Bayesian Learning
0
0.5
1
1.5
2
0
5
10
15
(a) L = 20, M = 50
0
0.5
1
1.5
2
0
5
10
15
(b) L = 20, M = 10
Figure 12.1 Truncation thresholds, Î³VB
h , Î³PB
h , Î³PBâˆ’A
h
, and Î³MAP
h
as functions of the
product cahcbh of the prior covariances. The noise variance is set to Ïƒ2 = 1.
0
10
20
30
40
0
2
4
6
8
10
(a) L = 20, cahcbh â†’âˆ
0
10
20
30
40
0
2
4
6
8
10
(b) L = 20, cahcbh = 1
Figure 12.2 Truncation thresholds as functions of M.
(MIR) enhanced by phase transitions. Eq. (12.72) implies that PB learningâ€”
a cruder approximation to Bayesian learningâ€”shares the same property as
VB learning.
To investigate the dimensionality selection behavior, we depict the trunca-
tion thresholds of VB learning, PB learning, PB-A learning, and MAP learning
as functions of the product cahcbh of the prior covariances in Figure 12.1.
The left panel is for the case with L = 20, M = 50, and the right panel
is for the case with L = 20, M = 10. PB-A learning corresponds to PB
learning with the predetermined marginalized and the point-estimated spaces
as in Tipping and Bishop (1999) and Chu and Ghahramani (2009), i.e., the
matrix A is always marginalized out and B is point-estimated regardless of
the dimensionality. We see in Figure 12.1 that PB learning and VB learning
show similar dimensionality selection behaviors, while PB-A learning behaves
differently when L > M.
Figure 12.2 shows the truncation thresholds as functions of M for L = 20.
With the ï¬‚at prior cahcbh â†’âˆ(left panel), the PB and the VB solutions agree

12.1 Theoretical Analysis in Fully Observed MF
309
with each other, as Eqs. (12.69) and (12.70) imply. The PB-A solution is also
identical to them when M â‰¥L. However, its behavior changes at M = L: the
truncation threshold of PB-A learning smoothly goes down as M decreases,
while those of PB learning and VB learning make a sudden turn and become
constant. The right panel is for the case with a nonï¬‚at prior (cahcbh = 1), which
shows similar tendency to the case with the ï¬‚at prior.
A question is which behavior is more desirable, a sudden turn in the
threshold curve in VB/PB learning, or the smooth behavior in PB-A learning?
We argue that the behavior of VB/PB learning is more desirable for the
following reason. Let us consider the case where no true signal exists, i.e.,
the true rank is Hâˆ—= 0. In this case, we merely observe pure noise, V = E, and
the average of the squared singular values of V over all components is given by

tr(EEâŠ¤)

MGaussL,M(E;0L,M,Ïƒ2ILâŠ—IM)
min(L, M)
= Ïƒ2 max(L, M).
(12.74)
Comparing Eq. (12.74) with Eqs. (12.70) and (12.69), we ï¬nd that VB learning
and PB learning always discard the components with singular values no greater
than the average noise contribution (note here that Eqs. (12.70) and (12.69)
give the thresholds for the ï¬‚at prior cahcbh â†’âˆ, and the thresholds increase
as cahcbh decreases). The sudden turn in the threshold curve actually follows
the behavior of the average noise contribution (12.74) to the singular values.
On the other hand, PB-A learning does not necessarily discard such noise-
dominant components, and can strongly overï¬t the noise when L â‰«M.
Figure 12.3 shows the estimators Î³h by VB learning, PB learning, PB-A
learning, and MAP learning for cahcbh = 1, as functions of the observed
0
5
10
15
20
0
5
10
15
20
(a) L = 20, M = 50
0
2
4
6
8
0
2
4
6
8
(b) L = 20, M = 10
Figure 12.3 Behavior of VB, PB, PB-A, and MAP estimators (the vertical axis)
for cahcbh = 1, when the singular value Î³h (the horizontal axis) is observed. The
noise variance is set to Ïƒ2 = 1.

310
12 MAP and Partially Bayesian Learning
singular value Î³h. We can see that the PB estimator behaves similarly to the VB
estimator, while the MAP estimator behaves signiï¬cantly differently. The right
panel shows that PB-A learning also behaves differently from VB learning
when L > M, which implies that the choice between the PB-A posterior and the
PB-B posterior based on the free energy is essential to accurately approximate
VB learning.
Actually, the coincidence between the VB solution (12.70) and the PB
solution (12.69) with the ï¬‚at prior can be seen as a natural consequence from
the similarity in the posterior shape. From Theorem 6.7 and Corollary 6.8, we
can derive the following corollary:
Corollary 12.7
Assume that, when we make the prior ï¬‚at cahcbh â†’âˆ, cah and
cbh go to inï¬nity in the same order, i.e., cah/cbh = Î˜(1).3 Then, the following
hold for the variances of the VB posterior (6.40): when L < M,
lim
cahcbhâ†’âˆÏƒ2
ah = âˆ,
lim
cahcbhâ†’âˆÏƒ2
bh = 0,
(12.75)
and when L > M,
lim
cahcbhâ†’âˆÏƒ2
ah = 0,
lim
cahcbhâ†’âˆÏƒ2
bh = âˆ.
(12.76)
Proof
Assume ï¬rst that L
<
M. When Î³h
>
Î³VB
h , Eq. (6.52) gives
limcahcbhâ†’âˆÎ´VB
h
= âˆ, and therefore Eq. (6.51) gives Eq. (12.75). When
Î³h â‰¤Î³VB
h , Eq. (6.54) gives Î¶VB
h
= Ïƒ2/M âˆ’Î˜(câˆ’2
ah câˆ’2
bh ) as cahcbh â†’âˆ, and
therefore Eq. (6.53) gives Eq. (12.75).
When L > M, Theorem 6.7 and Corollary 6.8 hold for V â†VâŠ¤, meaning
that the VB posterior is obtained by exchanging the variational parameters for
A and those for B. Thus, we obtain Eq. (12.76), and complete the proof.
â–¡
Corollary 12.7 implies that, with the ï¬‚at prior cahcbh â†’âˆ, the shape of
the VB posterior is similar to the shape of the PB posterior: they extend in the
space of A when M > L, and extend in the space of B when M < L. Therefore,
it is no wonder that the solutions coincide with each other.
Global Empirical MAP and Empirical PB Solutions
Next, we investigate the empirical Bayesian variants of MAP learning and
PB learning, where the hyperparameters CA and CB are also estimated from
observations. Note that the noise variance Ïƒ2 is still considered as a ï¬xed
constant (noise variance estimation will be discussed in Section 12.1.6).
3 Î˜( f(x)) is a positive function such that lim supxâ†’âˆ|Î˜( f(x))/ f(x)| < âˆand
lim infxâ†’âˆ|Î˜( f(x))/ f(x)| > 0.

12.1 Theoretical Analysis in Fully Observed MF
311
Let us ï¬rst consider the MAP free energy (12.30):
2FMAP
h
= M log c2
ah + L log c2
bh +
a2
h
c2ah +
b2
h
c2
bh
+
âˆ’2ahbhÎ³h+a2
hb2
h
Ïƒ2
âˆ’(L + M) + (L + M)Ï‡.
We can make the MAP free energy arbitrarily small, i.e., FMAP
h
â†’âˆ’âˆby
setting c2
ah, c2
bh â†’+0 with the variational parameters set to the corresponding
solution, i.e., ah = bh = 0 (see Corollary 12.2). Therefore, the global solution
of empirical MAP (EMAP) learning is given by
ah = 0,
bh = 0,
c2
ah â†’+0,
c2
bh â†’+0,
for h = 1,. . . , H,
which results in the following theorem:
Theorem 12.8
The global solution of EMAP learning is Î³EMAP
h

â‰¡ahbh
 
= 0
for all h = 1,. . . , H, regardless of observations.
The same happens in empirical PB (EPB) learning. The PB-A free energy
(12.31),
2FPBâˆ’A
h
= M log
c2
ah
Ïƒ2ah + L log c2
bh +
a2
h+MÏƒ2
ah
c2ah
+
b2
h
c2
bh
+
âˆ’2ahbhÎ³h+

a2
h+MÏƒ2
ah
 b2
h
Ïƒ2
âˆ’(L + M) + LÏ‡,
can be arbitrarily small, i.e., FPBâˆ’A
h
â†’âˆ’âˆby setting c2
ah, c2
bh â†’+0 with the
variational parameters set to the corresponding solutionah = bh = 0, Ïƒ2
ah = c2
ah
(see Corollary 12.4). Also, the PB-B free energy (12.32),
2FPBâˆ’B
h
= M log c2
ah + L log
c2
bh
Ïƒ2
bh
+
a2
h
c2ah +
b2
h+LÏƒ2
bh
c2
bh
+
âˆ’2ahbhÎ³h+a2
h
!
b2
h+LÏƒ2
bh
"
Ïƒ2
âˆ’(L + M) + MÏ‡,
can be arbitrarily small, i.e., FPBâˆ’B
h
â†’âˆ’âˆby setting c2
ah, c2
bh â†’+0 with the
variational parameters set to the corresponding solutionah = bh = 0, Ïƒ2
bh = c2
bh.
Thus, we have the following theorem:
Theorem 12.9
The global solution of EPB learning is Î³EPB
h

â‰¡ahbh
 
= 0 for
all h = 1,. . . , H, regardless of observations.
Theorems 12.8 and 12.9 imply that empirical Bayesian variants of MAP
learning and PB learning give useless trivial estimators. This happens because
the posterior variances of the parameters to be point-estimated are ï¬xed to a
small value, so that the posteriors form the pseudo-Dirac delta functions. In VB
learning, if we set cahcbh to a small value, the posterior variances, Ïƒ2
ah and Ïƒ2
bh,

312
12 MAP and Partially Bayesian Learning
get small accordingly, so that the third and the fourth terms in Eq. (12.27) do
not diverge to +âˆ. As a result, the ï¬rst and the second terms in Eq. (12.27)
remain ï¬nite. On the other hand, in MAP learning and PB learning, at least
one of the posterior variances, Ïƒ2
ah and Ïƒ2
bh, is treated as a constant and cannot
be adjusted to the corresponding prior covariance when it is set to be small.
This makes the free energy lower-unbounded. Actually, if we lower-bound the
prior covariances as c2
ah, c2
bh â‰¥Îµ2 with the same Îµ2 as the one we used for
deï¬ning the variances (12.28) and (12.29) of the pseudo-Dirac delta functions
and their entropy (12.33), the MAP and the PB free energies, FMAP
h
, FPBâˆ’A
h
,
and FPBâˆ’B
h
, are also lower-bounded by zero, as the VB free energy, FVB
h .
12.1.3 Local Solutions
The analysis in Section 12.1.2 might seem contradictory with the reported
results in MÃ¸rup and Hansen (2009), where EMAP showed good performance
with the ARD property in TFâ€”since the free energies in MF and TF are similar
to each other, they should share the same issue of the lower-unboundedness.
In the following, we elucidate that this apparent contradiction is because of the
local solutions in EMAP learning and EPB learning that behave similarly to
the nontrivial positive solution of EVB learning. Actually, EMAP learning and
EPB learning can behave similarly to EVB learning when the free energy is
minimized by local search.
Local EMAP and EPB Solutions
Here we conduct more detailed analysis of the free energies for EMAP learning
and EPB learning, and clarify the behavior of their local minima. To make
the free energy always comparable (ï¬nite), we slightly modify the problem.
Speciï¬cally, we solve the following problem:
Given
Ïƒ2 âˆˆR++,
min
r,{c2ah,c2
bh}H
h=1
F,
(12.77)
s.t.
cahcbh â‰¥Îµ2,
cah/cbh = 1
for h = 1,. . . , H,
and
â§âªâªâªâªâªâªâªâªâªâ¨âªâªâªâªâªâªâªâªâªâ©
r(A, B) = Î´(A; A)Î´(B; B)
(for EMAP learning),
r(A, B) = rA(A)Î´(B; B)
(for EPB-A learning),
r(A, B) = Î´(A; A)rB(B)
(for EPB-B learning),
r(A, B) = rA(A)rB(B)
(for EVB learning),

12.1 Theoretical Analysis in Fully Observed MF
313
where the free energy F is deï¬ned by Eq. (12.6), and the pseudo-Dirac delta
function is deï¬ned as Gaussian with an arbitrarily small but positive variance
Îµ2 > 0:
Î´(A; A) = MGaussM,H(A; A, Îµ2IM âŠ—IH) âˆexp
!
âˆ’
âˆ¥Aâˆ’Aâˆ¥2
Fro
2Îµ2
"
,
Î´(B; B) = MGaussL,H(B; B, Îµ2IL âŠ—IH) âˆexp
!
âˆ’
âˆ¥Bâˆ’Bâˆ¥2
Fro
2Îµ2
"
.
Note that, in Eq. (12.77), we lower-bounded the product cahcbh of the prior
covariances and ï¬xed the ratio cah/cbh. We added the constraint for EVB
learning for comparison.
Following the discussion in Section 12.1.1, we can write the posterior as
r(A, B) = rA(A)rB(B),
where
rA(A) = MGaussM,H(A; A, IM âŠ—Î£A) âˆexp
â›âœâœâœâœâœââˆ’
tr
!
(Aâˆ’A)Î£
âˆ’1
A (Aâˆ’A)âŠ¤
"
2
ââŸâŸâŸâŸâŸâ ,
rB(B) = MGaussL,H(B; B, IL âŠ—Î£B) âˆexp
â›âœâœâœâœâœââˆ’
tr
!
(Bâˆ’B)Î£
âˆ’1
B (Bâˆ’B)âŠ¤
"
2
ââŸâŸâŸâŸâŸâ ,
for
A = $a1Ï‰a1,. . . ,aHÏ‰aH
% ,
B =
b1Ï‰b1,. . . ,bHÏ‰bH
 
,
Î£A = Diag

Ïƒ2
a1,. . . , Ïƒ2
aH
 
,
Î£B = Diag

Ïƒ2
b1,. . . , Ïƒ2
bH
 
,
and the variational parameters {ah,bh, Ïƒ2
ah, Ïƒ2
bh}H
h=1are the solution of the
following problem:
Given
Ïƒ2 âˆˆR++,
min
{ah,bh,Ïƒ2ah,Ïƒ2
bh,c2ah,c2
bh}H
h=1
F,
(12.78)
s.t.
ah,bh âˆˆR,
cahcbh â‰¥Îµ2,
cah/cbh = 1,
and
â§âªâªâªâªâªâªâªâªâªâ¨âªâªâªâªâªâªâªâªâªâ©
Ïƒ2
ah = Îµ2,
Ïƒ2
bh = Îµ2
(for EMAP learning),
Ïƒ2
ah â‰¥Îµ2,
Ïƒ2
bh = Îµ2
(for EPB-A learning),
Ïƒ2
ah = Îµ2,
Ïƒ2
bh â‰¥Îµ2
(for EPB-B learning),
Ïƒ2
ah â‰¥Îµ2,
Ïƒ2
bh â‰¥Îµ2
(for EVB learning),
(12.79)
for h = 1,. . . , H,

314
12 MAP and Partially Bayesian Learning
where the free energy F is explicitly written by Eqs. (12.26) and (12.27),
that is,
2F = LM log(2Ï€Ïƒ2) +
min(L,M)
h=1
Î³2
h
Ïƒ2
+
H

h=1
2Fh,
(12.80)
where
2Fh = M log
c2
ah
Ïƒ2ah
+ L log
c2
bh
Ïƒ2
bh
+
a2
h + MÏƒ2
ah
c2ah
+
b2
h + LÏƒ2
bh
c2
bh
âˆ’(L + M) +
âˆ’2ahbhÎ³h +

a2
h + MÏƒ2
ah
 b2
h + LÏƒ2
bh
 
Ïƒ2
.
(12.81)
By substituting the MAP solution (Corollary 12.2) and the PB solution
(Corollary 12.4), respectively, into Eq. (12.81), we can write the free energy as
a function of the product cahcbh of the prior covariances. We have the following
lemmas (the proofs are given in Sections 12.1.4 and 12.1.5, respectively):
Lemma 12.10
In EMAP learning, the free energy (12.81) can be written as
a function of cahcbh as follows:
2 Â´FMAP
h
=
min
ah,bhâˆˆR, Ïƒ2ah=Ïƒ2
bh=Îµ2 2Fh
=
â§âªâªâªâªâ¨âªâªâªâªâ©
(L + M)
!
log cahcbh +
Îµ2
cahcbh âˆ’1 + Ï‡
"
for Îµ2 â‰¤cahcbh â‰¤Ïƒ2
Î³h ,
(L + M) $log cahcbh âˆ’1 + Ï‡% âˆ’Ïƒâˆ’2
!
Î³h âˆ’
Ïƒ2
cahcbh
"2
for cahcbh > Ïƒ2
Î³h .
(12.82)
Lemma 12.11
In EPB learning, the free energy (12.81) can be written as a
function of cahcbh as follows: if Î³h > Ïƒ âˆšmax(L, M),
2 Â´FPB
h
= min
,
2 Â´FPBâˆ’A
h
, 2 Â´FPBâˆ’B
h
-
= min
2
minah,bhâˆˆR, Ïƒ2ahâ‰¥Îµ2, Ïƒ2
bh=Îµ2 2Fh, minah,bhâˆˆR, Ïƒ2ah=Îµ2, Ïƒ2
bhâ‰¥Îµ2 2Fh
3
=
â§âªâªâªâªâªâªâªâªâªâªâªâªâªâªâªâªâªâªâªâªâªâªâ¨âªâªâªâªâªâªâªâªâªâªâªâªâªâªâªâªâªâªâªâªâªâªâ©
min(L, M)
!
log cahcbh +
Îµ2
cahcbh âˆ’1 + Ï‡
"
for Îµ2 â‰¤cahcbh â‰¤
Ïƒ2
âˆš
Î³2
hâˆ’max(L,M)Ïƒ2 ,
min(L,M)+2 max(L,M)
2
log c2
ahc2
bh +
B
max(L, M)2 +
4Î³2
h
c2ahc2
bh
âˆ’
Ïƒ2
c2ahc2
bh
+ max(L, M) log

âˆ’max(L, M) +
B
max(L, M)2 +
4Î³2
h
c2ahc2
bh

âˆ’
Î³2
h
Ïƒ2 âˆ’max(L, M) log(2Ïƒ2) + min(L, M)(Ï‡ âˆ’1)
for cahcbh >
Ïƒ2
âˆš
Î³2
hâˆ’max(L,M)Ïƒ2 ,
(12.83)

12.1 Theoretical Analysis in Fully Observed MF
315
and otherwise,
2 Â´FPB
h
= min(L, M)

log cahcbh +
Îµ2
cahcbh
âˆ’1 + Ï‡

.
(12.84)
By minimizing the EMAP free energy (12.82) and the EPB free energy
(12.83), respectively, with respect to the product cahcbh of the prior covariances,
we obtain the following theorems (the proofs are given also in Sections 12.1.4
and 12.1.5, respectively):
Theorem 12.12
In EMAP learning, the free energy (12.81) has the global
minimum such that
Î³EMAP
h

â‰¡ahbh
 
= 0.
It has a nontrivial local minimum such that
Î³localâˆ’EMAP
h

â‰¡ahbh
 
= Ë˜Î³localâˆ’EMAP
h
if and only if
Î³h > Î³localâˆ’EMAP,
where
Î³localâˆ’EMAP = Ïƒ
C
2(L + M),
(12.85)
Ë˜Î³localâˆ’EMAP
h
= 1
2
!
Î³h +
.
Î³2
h âˆ’2Ïƒ2(L + M)
"
.
(12.86)
Theorem 12.13
In EPB learning, the free energy (12.81) has the global
minimum such that
Î³EPB
h

â‰¡ahbh
 
= 0.
It has a non-trivial local minimum such that
Î³localâˆ’EPB
h

â‰¡ahbh
 
= Ë˜Î³localâˆ’EPB
h
if and only if
Î³h > Î³localâˆ’EPB,
where
Î³localâˆ’EPB = Ïƒ
.
L + M +
C
2LM + min(L, M)2,
(12.87)
Ë˜Î³localâˆ’EPB
h
= Î³h
2

1 +
âˆ’max(L,M)Ïƒ2+âˆš
Î³4
hâˆ’2(L+M)Ïƒ2Î³2
h+min(L,M)2Ïƒ4
Î³2
h

.
(12.88)
Figure 12.4 shows the free energy (normalized by LM) as a function of
cahcbh for EMAP learning (given in Lemma 12.10), EPB learning (given in
Lemma 12.11), and EVB learning, deï¬ned by
2 Â´FVB
h
=
min
ah,bhâˆˆR, Ïƒ2ah,Ïƒ2
bhâ‰¥Îµ2 2Fh.
For EMAP learing and EPB learning, we ignored some constants (e.g., the
entropy terms proportional to Ï‡) to make the shapes of the free energies

316
12 MAP and Partially Bayesian Learning
0
0.2
0.4
0.6
â€“0.2
â€“0.1
0
EVB
EPB
EMAP
(a) Î³h = 10
0
0.2
0.4
0.6
â€“0.2
â€“0.1
0
EVB
EPB
EMAP
(b) Î³h = 12
0
0.2
0.4
0.6
â€“0.2
â€“0.1
0
EVB
EPB
EMAP
(c) Î³h = 14
0
0.2
0.4
0.6
â€“0.2
â€“0.1
0
EVB
EPB
EMAP
(d) Î³h = 16
Figure 12.4 Free energy dependence on cahcbh, where L = 20, M = 50. Crosses
indicate nontrivial local minima.
comparable. We can see deep pits at cahcbh â†’+0 in EMAP and EPB free
energies, which correspond to the global solutions. However, we also see
nontrivial local minima, which behave similarly to the nontrivial local solution
for VB learning. Namely, nontrivial local minima of EMAP, EPB, and EVB
free energies appear at locations similar to each other when the observed
singular value Î³h exceeds the thresholds given by Eqs. (12.85), (12.87), and
(6.127), respectively.
The deep pit at cahcbh â†’+0 is essential when we stick to the global
solution. The VB free energy does not have such a pit, which enables consistent
inference based on the free energy minimization principle. However, as long as
we rely on local search, the pit at the origin is not essential in practice. Assume
that a nontrivial local minimum exists, and we perform local search only once.
Then, whether local search for EMAP learning or EPB learning converges to
the trivial global solution or the nontrivial local solution simply depends on the
initialization. Note that the same applies also to EVB learning, for which local
search is not guaranteed to converge to the global solution. This is because of
the multimodality of the VB free energy, which can be seen in Figure 12.4.
One might wonder if some hyperpriors on c2
ah and c2
bh could ï¬ll the deep pits
at cahcbh â†’+0 in the EMAP and the EPB free energies, so that the nontrivial

12.1 Theoretical Analysis in Fully Observed MF
317
local solutions are global when some reasonable conditions hold. However,
when we rely on the ARD property for model selection, hyperpriors should be
almost noninformative. With such an almost noninformative hyperprior, e.g.,
the inverge-Gamma, p(c2
ah, c2
bh) âˆ(c2
ahc2
bh)1.001+0.001/(c2
ahc2
bh), which was used
in Bishop (1999b), deep pits still exist very close to the origin, which keep the
global EMAP and EPB estimators useless.
Comparison between Local-EMAP, Local-EPB, and EVB Solutions
Let us observe the behavior of local solutions. We deï¬ne the local-EMAP
estimator and the local-EPB estimator, respectively, by
U
localâˆ’EMAP =
H

h=1
Î³localâˆ’EMAP
h
Ï‰bhÏ‰âŠ¤
ah,
where
Î³localâˆ’EMAP
h
=
â§âªâªâ¨âªâªâ©
Ë˜Î³localâˆ’EMAP
h
if Î³h â‰¥Î³localâˆ’EMAP,
0
otherwise,
(12.89)
U
localâˆ’EPB =
H

h=1
Î³localâˆ’EPB
h
Ï‰bhÏ‰âŠ¤
ah,
where
Î³localâˆ’EPB
h
=
â§âªâªâ¨âªâªâ©
Ë˜Î³localâˆ’EPB
h
if Î³h â‰¥Î³localâˆ’EPB,
0
otherwise,
(12.90)
following the deï¬nition of the local-EVB estimator (6.131) in Chapter 6. In
the following, we assume that local search algorithms for EMAP learning and
EPB learning ï¬nd these solutions.
Deï¬ne the normalized (by the average noise contribution (12.74)) singular
values:
Î³â€²
h =
Î³h
C
max(L, M)Ïƒ2 .
We also deï¬ne normalized versions of the estimator, the truncation threshold,
and the shrinkage factor as
Î³â€²
h =
Î³h
C
max(L, M)Ïƒ2 , Î³â€²
h =
Î³h
C
max(L, M)Ïƒ2 , Ë˜Î³â€²
h =
Ë˜Î³h
C
max(L, M)Ïƒ2 . (12.91)
Then the normalized truncation thresholds and the normalized shrinkage
factors can be written as functions of Î± = min(L, M)/ max(L, M) as follows:
Î³â€²EVB = Ïƒ
.
1 + Î± + âˆšÎ±

Îº + 1
Îº
 
,
(12.92)
Ë˜Î³â€²EVB
h
=
Î³â€²
h
2
â›âœâœâœâœâœâ1 âˆ’(1+Î±)Ïƒ2
Î³â€²2
h
+
B!
1 âˆ’(1+Î±)Ïƒ2
Î³â€²2
h
"2
âˆ’4Î±Ïƒ4
Î³â€²4
h
ââŸâŸâŸâŸâŸâ ,
(12.93)

318
12 MAP and Partially Bayesian Learning
Î³â€²localâˆ’EPB = Ïƒ
.
1 + Î± +
âˆš
2Î± + Î±2,
(12.94)
Ë˜Î³â€²localâˆ’EPB
h
=
Î³â€²
h
2

1 +
âˆ’Ïƒ2+âˆš
Î³â€²4
h âˆ’2(1+Î±)Ïƒ2Î³â€²2
h +Ïƒ4
Î³â€²2
h

,
(12.95)
Î³â€²localâˆ’EMAP = Ïƒ âˆš2(1 + Î±),
(12.96)
Ë˜Î³â€²localâˆ’EMAP
h
= 1
2
!
Î³â€²
h +
.
Î³â€²2
h âˆ’2Ïƒ2(1 + Î±)
"
.
(12.97)
Note that Îº is also a function of Î±.
Figure 12.5 compares the normalized versions of the (global) EVB
estimator Î³â€²EVB
h
, the local-EPB estimator Î³â€²localâˆ’EPB
h
, and the local-EMAP
estimator Î³â€²localâˆ’EMAP
h
. We can observe similar behaviors of those three
empirical Bayesian estimators. This is in contrast to the nonempirical Bayesian
estimators shown in Figure 12.3, where the PB estimator behaves similarly to
the VB estimator, while the MAP estimator behaves differently.
Figure 12.6 compares the normalized versions of the EVB truncation
threshold (12.92), the local-EPB truncation threshold (12.94), and the local-
EMAP truncation threshold (12.96). We can see that those thresholds behave
similarly. However, we can ï¬nd an essential difference of the local-EPB
threshold from the EVB and the local-EMAP thresholds: it holds that, for
any Î±,
Î³â€²localâˆ’EPB < Î³â€²MPUL â‰¤Î³â€²EVB, Î³â€²localâˆ’EMAP,
(12.98)
where
Î³â€²MPUL =
Î³MPUL
C
max(L, M)Ïƒ2 = (1 + âˆšÎ±)
0
0.5
1
1.5
2
0
0.5
1
1.5
2
Loca
Local
Figure 12.5 Behavior of (global) EVB, the local-EPB, and the local-EMAP
estimators for Î± = min(L, M)/ max(L, M) = 1/3.

12.1 Theoretical Analysis in Fully Observed MF
319
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
Local-EPB
Local-EMAP
Figure 12.6 Truncation thresholds.
is the normalized version of the MarË‡cenkoâ€“Pastur upper limit (MPUL)
(Eq. (8.41) in Chapter 8), which is also shown in Figure 12.6.
As discussed in Section 8.4.1, the MPUL is the largest singular value of an
LÃ—M zero-mean independent random matrix in the large-scale limit where the
matrix size (L, M) goes to inï¬nity with ï¬xed ratio Î± = min(L, M)/ max(L, M).
In other words, the MPUL corresponds to the minimum observed singu-
lar value detectable (or distinguishable from noise) by any dimensionality
reduction method. The inequalities (12.98) say that local-EPB threshold is
always smaller than the MPUL, while the EVB threshold and the local-EMAP
threshold are never smaller than the MPUL. This implies that, for a large-scale
observed matrix, the EVB estimator and the local-EMAP estimator discard
the singular components dominated by noise, while the local-EPB estimator
retains some of them.
12.1.4 Proofs of Lemma 12.10 and Theorem 12.12
By substituting the MAP solution, given by Corollary 12.2, we can write the
free energy (12.81) as follows: for Îµ2 â‰¤cahcbh â‰¤Ïƒ2
Î³h ,
2 Â´FMAP
h
=
min
ah,bhâˆˆR, Ïƒ2ah=Ïƒ2
bh=Îµ2 2Fh
= M log c2
ah + L log c2
bh
+

M
c2ah +
L
c2
bh

Îµ2 âˆ’(L + M) + (L + M)Ï‡
= M log c2
ah + L log c2
bh +

M
c2ah +
L
c2
bh

Îµ2
âˆ’(L + M) + (L + M)Ï‡,
(12.99)

320
12 MAP and Partially Bayesian Learning
and for cahcbh > Ïƒ2
Î³h ,
2 Â´FMAP
h
=
min
ah,bhâˆˆR, Ïƒ2ah=Ïƒ2
bh=Îµ2 2Fh
= M log c2
ah + L log c2
bh +
!
Î³h âˆ’
Ïƒ2
cahcbh
" â›âœâœâœâœâœâ
2
cahcbh +
âˆ’2Î³h+Î³hâˆ’
Ïƒ2
cah cbh
Ïƒ2
ââŸâŸâŸâŸâŸâ 
+

M
c2ah +
L
c2
bh

Îµ2 âˆ’(L + M) + (L + M)Ï‡
= M log c2
ah + L log c2
bh âˆ’Ïƒâˆ’2

Î³h âˆ’
Ïƒ2
cahcbh
2
âˆ’(L + M) + (L + M)Ï‡.
(12.100)
In the second-to-last equation in Eq. (12.100), we ignored the fourth term
because cahcbh > Ïƒ2
Î³h implies c2
ah, c2
bh â‰«Îµ2 (with an arbitrarily high probability
depending on Îµ2). By ï¬xing the ratio to cah/cbh = 1, we obtain Eq. (12.82),
which proves Lemma 12.10.
Now we minimize the free energy (12.82) with respect to cahcbh, and ï¬nd
nontrivial local solutions. The free energy is continuous in the domain Îµ2 â‰¤
cahcbh < âˆ, and differentiable except at cahcbh = Ïƒ2
Î³h . The derivative is given by
âˆ‚2 Â´FMAP
h
âˆ‚(cahcbh) =
â§âªâªâªâªâªâªâ¨âªâªâªâªâªâªâ©
(L + M)

1
cahcbh âˆ’
Îµ2
c2ahc2
bh

for Îµ2 â‰¤cahcbh â‰¤Ïƒ2
Î³h

L+M
cahcbh + 2Ïƒâˆ’2
!
Î³h âˆ’
Ïƒ2
cahcbh
"
Ïƒ2
c2ahc2
bh

for cahcbh > Ïƒ2
Î³h
=
â§âªâªâªâªâ¨âªâªâªâªâ©
L+M
c2ahc2
bh

cahcbh âˆ’Îµ2 
for Îµ2 â‰¤cahcbh â‰¤Ïƒ2
Î³h ,
1
c3ahc3
bh

(L + M)c2
ahc2
bh + 2Î³hcahcbh âˆ’2Ïƒ2 
for cahcbh > Ïƒ2
Î³h .
(12.101)
Eq. (12.101) implies that the free energy Â´FMAP
h
is increasing for Îµ2 â‰¤cahcbh â‰¤
Ïƒ2
Î³h , and that it is increasing at cahcbh = Ïƒ2
Î³h and at cahcbh â†’+âˆ.
In the region of Ïƒ2
Î³h < cahcbh < +âˆ, the free energy has stationary points if
and only if
Î³h â‰¥Ïƒ
C
2(L + M)

â‰¡Î³localâˆ’EMAP 
,
(12.102)
because the derivative can be factorized (with real factors if and only if the
condition (12.102) holds) as
âˆ‚2 Â´FMAP
h
âˆ‚(cahcbh) = L + M
c3ahc3
bh
$cahcbh âˆ’Â´cah Â´cbh
% $cahcbh âˆ’Ë˜cah Ë˜cbh
% ,

12.1 Theoretical Analysis in Fully Observed MF
321
where
Â´cah Â´cbh =
Î³h âˆ’
.
Î³2
h âˆ’2Ïƒ2(L + M)
L + M
,
(12.103)
Ë˜cah Ë˜cbh =
Î³h +
.
Î³2
h âˆ’2Ïƒ2(L + M)
L + M
.
(12.104)
Summarizing the preceding discussion, we have the following lemma:
Lemma 12.14
If Î³h â‰¤Î³localâˆ’EMAP, the EMAP free energy Â´FMAP
h
, deï¬ned
by Eq. (12.82), is increasing for cahcbh > Îµ2 , and therefore minimized at
cahcbh = Îµ2. If Î³h > Î³localâˆ’EMAP,
Â´FMAP
h
is
â§âªâªâªâªâªâ¨âªâªâªâªâªâ©
increasing
for
Îµ2 < cahcbh < Â´cah Â´cbh,
decreasing
for
Â´cah Â´cbh < cahcbh < Ë˜cah Ë˜cbh,
increasing
for
Ë˜cah Ë˜cbh < cahcbh < +âˆ,
and therefore has two (local) minima at cahcbh = Îµ2 and at cahcbh = Ë˜cah Ë˜cbh.
Here Â´cah Â´cbh and Ë˜cah Ë˜cbh are deï¬ned by Eqs. (12.103) and (12.104), respectively.
When Î³h > Î³localâˆ’EMAP, the EMAP free energy (12.82) at the local minima is
2 Â´FMAP
h
=
â§âªâªâªâ¨âªâªâªâ©
0
at cahcbh = Îµ2,
(L + M) $log Ë˜cah Ë˜cbh âˆ’1 + Ï‡% âˆ’Ïƒâˆ’2
!
Î³h âˆ’
Ïƒ2
Ë˜cah Ë˜cbh
"2
at cahcbh = Ë˜cah Ë˜cbh,
respectively. Since we assume that Ï‡ = âˆ’log Îµ2 is an arbitrarily large constant
(Îµ2 > 0 is arbitrarily small), cahcbh = Îµ2 is always the global minimum.
Substituting Eq. (12.104) into Eq. (12.36) gives Eq. (12.86), which com-
pletes the proof of Theorem 12.12.
â–¡
12.1.5 Proofs of Lemma 12.11 and Theorem 12.13
We ï¬rst analyze the free energy for EPB-A learning. From Eq. (12.49), we
have
cahcbh =
Ïƒ2
.
(Î³PBâˆ’A
h
)2 âˆ’MÏƒ2 .
Therefore, if
Î³h â‰¤Ïƒ
âˆš
M,

322
12 MAP and Partially Bayesian Learning
there exists only the null solution (12.53) for any cahcbh > 0, and therefore the
free energy (12.81) is given by
2 Â´FPBâˆ’A
h
=
min
ah,bhâˆˆR, Ïƒ2ahâ‰¥Îµ2, Ïƒ2
bh=Îµ2 2Fh
= L

log c2
bh + Îµ2
c2
bh
âˆ’1 + Ï‡

.
(12.105)
In the following, we consider the case where
Î³h > Ïƒ
âˆš
M.
(12.106)
For Îµ2 â‰¤cahcbh â‰¤
Ïƒ2
âˆš
Î³2
hâˆ’MÏƒ2 , there still exists only the null solution (12.53) with
the free energy given by Eq. (12.105). The positive solution (12.50) appears for
cahcbh >
Ïƒ2
âˆš
Î³2
hâˆ’MÏƒ2 with the free energy given by
2 Â´FPBâˆ’A
h
=
min
ah,bhâˆˆR, Ïƒ2ahâ‰¥Îµ2, Ïƒ2
bh=Îµ2 2Fh
=
min
ah,bhâˆˆR, Ïƒ2ahâ‰¥Îµ2
)
M log
c2
ah
Ïƒ2ah + L log c2
bh +
b2
h
c2
bh
âˆ’2ahbhÎ³h
Ïƒ2
+

a2
h + MÏƒ2
ah
 ! b2
h
Ïƒ2 +
1
c2ah
" 1
âˆ’(L + M) + LÏ‡
=
min
ah,bhâˆˆR, Ïƒ2ahâ‰¥Îµ2
)
M log
b2
h+Ïƒ2/c2
ah
Ïƒ2
+
b2
h
c2
bh
âˆ’
b2
hÎ³h
b2
h+Ïƒ2/c2ah
2Î³h
Ïƒ2 +
a2
h+MÏƒ2
ah
Ïƒ2ah
1
+ M log c2
ah + L log c2
bh âˆ’(L + M) + LÏ‡
= min
bhâˆˆR
)
M log
b2
h + Ïƒ2/c2
ah
 
+
b2
h+Ïƒ2/c2
ah
c2
bh
âˆ’
b2
hÎ³2
h
Ïƒ2(b2
h+Ïƒ2/c2ah)
1
âˆ’
Ïƒ2
c2ahc2
bh
+ M log c2
ah + L log c2
bh âˆ’M log Ïƒ2 âˆ’L + LÏ‡
= min
bhâˆˆR
)
M log
b2
h + Ïƒ2/c2
ah
 
+
b2
h+Ïƒ2/c2
ah
c2
bh
+
Î³2
h
c2ah(b2
h+Ïƒ2/c2ah)
1
âˆ’
Î³2
h
Ïƒ2 âˆ’
Ïƒ2
c2ahc2
bh
+ M log c2
ah + L log c2
bh âˆ’M log Ïƒ2 âˆ’L + LÏ‡.
(12.107)
Here we used the conditions (12.61) and (12.62) for the PB-A solution. By
substituting the other conditions (12.64) and (12.66) into Eq. (12.107), we have

12.1 Theoretical Analysis in Fully Observed MF
323
2 Â´FPBâˆ’A
h
= M log

âˆ’M +
B
M2 +
4Î³2
h
c2ahc2
bh

+
âˆ’M+
A
M2+
4Î³2
h
c2ah c2
bh
2
+
M+
A
M2+
4Î³2
h
c2ah c2
bh
2
âˆ’
Î³2
h
Ïƒ2 âˆ’
Ïƒ2
c2ahc2
bh
+ M log c2
ah + (L + M) log c2
bh âˆ’M log(2Ïƒ2) âˆ’L + LÏ‡
= M log

âˆ’M +
B
M2 +
4Î³2
h
c2ahc2
bh

+
B
M2 +
4Î³2
h
c2ahc2
bh
âˆ’
Ïƒ2
c2ahc2
bh
âˆ’
Î³2
h
Ïƒ2 + M log c2
ah + (L + M) log c2
bh âˆ’M log(2Ïƒ2) âˆ’L + LÏ‡.
(12.108)
The PB-B free energy can be derived in exactly the same way, and the result
is symmetric to the PB-A free energy. Namely, if
Î³h â‰¤Ïƒ
âˆš
L,
there exists only the null solution (12.59) for any cahcbh > 0 with the free
energy given by
2 Â´FPBâˆ’B
h
=
min
ah,bhâˆˆR, Ïƒ2ah=Îµ2, Ïƒ2
bhâ‰¥Îµ2 2Fh
= M
!
log c2
ah + Îµ2
c2ah âˆ’1 + Ï‡
"
.
(12.109)
Assume that
Î³h > Ïƒ
âˆš
L.
For Îµ2 â‰¤cahcbh â‰¤
Ïƒ2
âˆš
Î³2
hâˆ’LÏƒ2 , there still exists only the null solution (12.59) with
the free energy given by Eq. (12.109). The positive solution (12.56) appears
for cahcbh >
Ïƒ2
âˆš
Î³2
hâˆ’LÏƒ2 with the free energy given by
2 Â´FPBâˆ’B
h
=
min
ah,bhâˆˆR, Ïƒ2ah=Îµ2, Ïƒ2
bhâ‰¥Îµ2 2Fh
= L log

âˆ’L +
B
L2 +
4Î³2
h
c2ahc2
bh

+
B
L2 +
4Î³2
h
c2ahc2
bh
âˆ’
Ïƒ2
c2ahc2
bh
âˆ’
Î³2
h
Ïƒ2 + (L + M) log c2
ah + L log c2
bh âˆ’L log(2Ïƒ2) âˆ’M + MÏ‡.
(12.110)
By ï¬xing the ratio between the prior covariances to cah/cbh = 1 in Eqs.
(12.105) and (12.108) through (12.110), and taking the posterior choice in Eq.
(12.47) into account, we obtain Eqs. (12.83) and (12.84), which prove Lemma
12.11.

324
12 MAP and Partially Bayesian Learning
Let us minimize the free energy with respect to cahcbh. When
Î³h â‰¤Ïƒ
C
max(L, M),
the free energy is given by Eq. (12.84), and its derivative is given by
âˆ‚2 Â´FPB
h
âˆ‚(cahcbh) = min(L, M)
c2ahc2
bh

cahcbh âˆ’Îµ2 
.
This implies that the free energy Â´FPB
h
is increasing for Îµ2 < cahcbh < âˆ, and
therefore minimized at cahcbh = Îµ2.
Assume that
Î³h > Ïƒ
C
max(L, M).
In this case, the free energy is given by Eq. (12.83), which is continuous in the
domain Îµ2 â‰¤cahcbh < âˆ, and differentiable except at cahcbh =
Ïƒ2
âˆš
Î³2
hâˆ’max(L,M)Ïƒ2 .
Although the continuity is not very obvious, one can verify it by checking the
value at cahcbh =
Ïƒ2
âˆš
Î³2
hâˆ’max(L,M)Ïƒ2 for each case in Eq. (12.83). The continuity is
also expected from the fact that the PB solution is continuous at the threshold
Î³h = Î³PB
h , i.e., the positive solution (Eq. (12.50) for PB-A and Eq. (12.56) for
PB-B) converges to the null solution (Eq. (12.53) for PB-A and Eq. (12.59) for
PB-B) when Î³h â†’Î³PBâˆ’A
h
+ 0.
The free energy (12.83) is the same as Eq. (12.84) for Îµ2 â‰¤cahcbh â‰¤
Ïƒ2
âˆš
Î³2
hâˆ’max(L,M)Ïƒ2 , and therefore increasing in Îµ2 < cahcbh â‰¤
Ïƒ2
âˆš
Î³2
hâˆ’max(L,M)Ïƒ2 . For
cahcbh >
Ïƒ2
âˆš
Î³2
hâˆ’max(L,M)Ïƒ2 , the derivative of the free energy with respect to c2
ahc2
bh
is given by
âˆ‚2 Â´FPB
h
âˆ‚(c2ahc2
bh) = min(L,M)+2 max(L,M)
2c2ahc2
bh
âˆ’
4Î³2
h
2c4ahc4
bh
A
max(L,M)2+
4Î³2
h
c2ah c2
bh
+
Ïƒ2
c4ahc4
bh
âˆ’
4 max(L,M)Î³2
h
2c4ahc4
bh
A
max(L,M)2+
4Î³2
h
c2ah c2
bh
â›âœâœâœâœâœâœââˆ’max(L,M)+
A
max(L,M)2+
4Î³2
h
c2ah c2
bh
ââŸâŸâŸâŸâŸâŸâ 
= min(L,M)+2 max(L,M)
2c2ahc2
bh
+
Ïƒ2
c4ahc4
bh
âˆ’
4Î³2
h
2c4ahc4
bh
â›âœâœâœâœâœâœââˆ’max(L,M)+
A
max(L,M)2+
4Î³2
h
c2ah c2
bh
ââŸâŸâŸâŸâŸâŸâ 
=
1
2c2ahc2
bh

L + M + 2
Ïƒ2
c2ahc2
bh
âˆ’
B
max(L, M)2 +
4Î³2
h
c2ahc2
bh

=
1
2c4ahc4
bh
!
(L + M)c2
ahc2
bh + 2Ïƒ2 âˆ’cahcbh
.
max(L, M)2c2ahc2
bh + 4Î³2
h
"
,
(12.111)

12.1 Theoretical Analysis in Fully Observed MF
325
which has the same sign as
Ï„(c2
ahc2
bh) =
,
(L + M)c2
ahc2
bh + 2Ïƒ2-2 âˆ’
2
cahcbh
.
max(L, M)2c2ahc2
bh + 4Î³2
h
32
=

2LM + min(L, M)2 
c4
ahc4
bh âˆ’4

Î³2
h âˆ’Ïƒ2(L + M)
 
c2
ahc2
bh + 4Ïƒ4.
(12.112)
Eq. (12.112) is a quadratic function of c2
ahc2
bh, being positive at c2
ahc2
bh â†’+0
and at c2
ahc2
bh â†’+âˆ. The free energy has stationary points if and only if
Î³h â‰¥Ïƒ
.
L + M +
C
2LM + min(L, M)2 
â‰¡Î³localâˆ’EPB 
,
(12.113)
because Ï„(c2
ahc2
bh), which has the same sign as the derivative of the free energy,
can be factorized (with real factors if and only if the condition (12.113) holds)
as
Ï„(c2
ahc2
bh) =

2LM + min(L, M)2 
c2
ahc2
bh âˆ’Â´c2
ah Â´c2
bh
 
c2
ahc2
bh âˆ’Ë˜c2
ah Ë˜c2
bh
 
,
where
Â´c2
ah Â´c2
bh = 2 Â· (Î³2
hâˆ’Ïƒ2(L+M))âˆ’
.
(Î³2
hâˆ’Ïƒ2(L+M))
2âˆ’(2LM+min(L,M)2)Ïƒ4
2LM+min(L,M)2
,
(12.114)
Ë˜c2
ah Ë˜c2
bh = 2 Â· (Î³2
hâˆ’Ïƒ2(L+M))+
.
(Î³2
hâˆ’Ïƒ2(L+M))
2âˆ’(2LM+min(L,M)2)Ïƒ4
2LM+min(L,M)2
.
(12.115)
Summarizing the preceding discussion, we have the following lemma:
Lemma 12.15
If Î³h â‰¤Î³localâˆ’EPB, the EPB free energy Â´FPB
h , deï¬ned by Eqs.
(12.83) and (12.84), is increasing for cahcbh > Îµ2, and therefore minimized at
cahcbh = Îµ2. If Î³h > Î³localâˆ’EPB,
Â´FPB
h
is
â§âªâªâªâªâªâ¨âªâªâªâªâªâ©
increasing
for
Îµ2 < cahcbh < Â´cah Â´cbh,
decreasing
for
Â´cah Â´cbh < cahcbh < Ë˜cah Ë˜cbh,
increasing
for
Ë˜cah Ë˜cbh < cahcbh < +âˆ,
and therefore has two (local) minima at cahcbh = Îµ2 and at cahcbh = Ë˜cah Ë˜cbh.
Here, Â´cah Â´cbh and Ë˜cah Ë˜cbh are deï¬ned by Eqs. (12.114) and (12.115), respectively.
When Î³h > Î³localâˆ’EPB, the EPB free energy (12.83) at the null local solution
cahcbh = Îµ2 is 2 Â´FPB
h
= 0, while the EPB free energy at the positive local solution
cahcbh = Ë˜cah Ë˜cbh contains the term min(L, M)Ï‡ with Ï‡ = âˆ’log Îµ2 assumed to be
arbitrarily large. Therefore, the null solution is always the global minimum.
Substituting Eq. (12.115) into Eq. (12.46) gives Eq. (12.88), which com-
pletes the proof of Theorem 12.13.
â–¡

326
12 MAP and Partially Bayesian Learning
12.1.6 Noise Variance Estimation
The noise variance Ïƒ2 is unknown in many practical applications. In VB
learning, minimizing the free energy (12.26) with respect also to Ïƒ2 gives a
reasonable estimator, with which perfect dimensionality recovery was proven
in Chapter 8. Here, we investigate whether MAP learning and PB learning offer
good noise variance estimators.
We ï¬rst consider the nonempirical Bayesian variants where the prior
covariances CA, CB are treated as given constants. By using Lemma 12.10,
we can write the MAP free energy with the variational parameters optimized,
as a function of Ïƒ2, as follows:
2 Â´FMAP = LM log(2Ï€Ïƒ2) +
min(L,M)
h=1
Î³2
h
Ïƒ2
+ H
h=1 2 Â´FMAP
h
= LM log(2Ï€Ïƒ2) +
min(L,M)
h=1
Î³2
h
Ïƒ2
+ min(H,H)
h=1
)
(L + M) $log cahcbh âˆ’1 + Ï‡% âˆ’Ïƒâˆ’2
!
Î³h âˆ’
Ïƒ2
cahcbh
"21
+ min(L,M)
h=min(H,H)+1(L + M)
!
log cahcbh +
Îµ2
cahcbh âˆ’1 + Ï‡
"
= LM log Ïƒ2 +
min(L,M)
h=min(H,H)+1 Î³2
h
Ïƒ2
+ min(H,H)
h=1

2Î³h
cahcbh âˆ’
Ïƒ2
c2ahc2
bh

+ LM log(2Ï€) + min(L,M)
h=1
(L + M)
!
log cahcbh +
Îµ2
cahcbh âˆ’1 + Ï‡
"
= LM log Ïƒ2 +
min(L,M)
h=min(H,H)+1 Î³2
h
Ïƒ2
+ min(H,H)
h=1

2Î³h
cahcbh âˆ’
Ïƒ2
c2ahc2
bh

+ const.
(12.116)
for
Ïƒ2 MAP
H+1
â‰¤Ïƒ2 â‰¤Ïƒ2 MAP
H
,
(12.117)
where
Ïƒ2 MAP
h
=
â§âªâªâªâªâªâ¨âªâªâªâªâªâ©
âˆ
for h = 0,
cahcbhÎ³h
for h = 1,. . . , min(L, M),
0
for h = min(L, M) + 1.
(12.118)
Assume that we use the full-rank model H = min(L, M), and expect the
ARD property to ï¬nd the correct rank. Under this setting, the free energy
(12.116) can be arbitrarily small for Ïƒ2
â†’+0, because the ï¬rst term
diverges to âˆ’âˆ, and the second term is equal to zero for 0 (= Ïƒ2 MAP
min(L,M)+1) â‰¤
Ïƒ2 â‰¤camin(L,M)cbmin(L,M)Î³min(L,M)(= Ïƒ2 MAP
min(L,M)) (note that Î³min(L,M) > 0 with
probability 1). This leads to the following lemma:

12.1 Theoretical Analysis in Fully Observed MF
327
Lemma 12.16
Assume that H
= min(L, M) and CA,CB are given as
constants. Then the MAP free energy with respect to Ïƒ2 is (globally) minimized
at
Ïƒ2 MAP â†’+0.
The PB free energy behaves differently. By using Lemma 12.11, we can
write the PB free energy with the variational parameters optimized, as a
function of Ïƒ2, as follows:
2 Â´FPB = LM log(2Ï€Ïƒ2) +
min(L,M)
h=1
Î³2
h
Ïƒ2
+ H
h=1 2 Â´FPB
h
= LM log(2Ï€Ïƒ2) +
min(L,M)
h=1
Î³2
h
Ïƒ2
+ min(H,H)
h=1
)
min(L,M)+2 max(L,M)
2
log c2
ahc2
bh +
B
max(L, M)2 +
4Î³2
h
c2ahc2
bh
âˆ’
Ïƒ2
c2ahc2
bh
+ max(L, M) log

âˆ’max(L, M) +
B
max(L, M)2 +
4Î³2
h
c2ahc2
bh

âˆ’
Î³2
h
Ïƒ2 âˆ’max(L, M) log(2Ïƒ2) + min(L, M)(Ï‡ âˆ’1)
1
+
min(L,M)

h=min(H,H)+1
min(L, M)

log cahcbh +
Îµ2
cahcbh
âˆ’1 + Ï‡

= (min(L, M) âˆ’min(H, H)) max(L, M) log(2Ïƒ2) +
min(L,M)
h=min(H,H)+1 Î³2
h
Ïƒ2
+ min(H,H)
h=1
)
max(L, M) log c2
ahc2
bh +
B
max(L, M)2 +
4Î³2
h
c2ahc2
bh
âˆ’
Ïƒ2
c2ahc2
bh
+ max(L, M) log

âˆ’max(L, M) +
B
max(L, M)2 +
4Î³2
h
c2ahc2
bh
 1
+ LM log(Ï€) + min(L,M)
h=1
min(L, M) $log cahcbh âˆ’1 + Ï‡%
= (min(L, M) âˆ’min(H, H)) max(L, M) log(2Ïƒ2) +
min(L,M)
h=min(H,H)+1 Î³2
h
Ïƒ2
+ min(H,H)
h=1
)
max(L, M) log c2
ahc2
bh +
B
max(L, M)2 +
4Î³2
h
c2ahc2
bh
âˆ’
Ïƒ2
c2ahc2
bh
+ max(L, M) log

âˆ’max(L, M) +
B
max(L, M)2 +
4Î³2
h
c2ahc2
bh
 1
+ const.
(12.119)
for
Ïƒ2 PB
H+1 â‰¤Ïƒ2 â‰¤Ïƒ2 PB
H
,
(12.120)

328
12 MAP and Partially Bayesian Learning
where
Ïƒ2 PB
h
=
â§âªâªâªâªâªâªâªâªâªâªâªâ¨âªâªâªâªâªâªâªâªâªâªâªâ©
âˆ
for h = 0,
c2
ahc2
bh
2

âˆ’max(L, M) +
.
max(L, M)2 + 4
Î³h
c2ahc2
bh

for h = 1,. . . , min(L, M),
0
for h = min(L, M) + 1.
(12.121)
We ï¬nd a remarkable difference between the MAP free energy (12.116)
and the PB free energy (12.119): unlike in the MAP free energy, the ï¬rst log
term in the PB free energy disappears for 0 = Ïƒ2 PB
min(L,M)+1 < Ïƒ2 < Ïƒ2 PB
min(L,M),
and therefore, the PB free energy does not diverge to âˆ’âˆat Ïƒ2 â†’+0. We
can actually prove that the noise variance estimator is lower-bounded by a
positive value as follows. The PB free energy (12.121) is continuous, and, for
0 = Ïƒ2 PB
min(L,M)+1 < Ïƒ2 < Ïƒ2 PB
min(L,M), it can be written as
2 Â´FPB = âˆ’Ïƒ2
c2ahc2
bh
+ const.,
which is monotonically decreasing. This leads to the following lemma:
Lemma 12.17
Assume that H = min(L, M) and CA,CB are given as con-
stants. Then the noise variance estimator in PB learning is lower-bounded by
Ïƒ2 MAP â‰¥Ïƒ2 PB
min(L,M)
=
c2
amin(L,M)c2
bmin(L,M)
2
â›âœâœâœâœâœââˆ’max(L, M) +
B
max(L, M)2 + 4
Î³min(L,M)
c2amin(L,M)c2
bmin(L,M)
ââŸâŸâŸâŸâŸâ .
(12.122)
We numerically investigated the behavior of the noise variance estimator
by creating random observed matrices V = Bâˆ—Aâˆ—âŠ¤+ E âˆˆRLÃ—M, and depicting
the VB, PB, and MAP free energies as functions of Ïƒ2 with the variational
parameters optimized. Figure 12.7 shows a typical case for L = 20, M = 50,
Hâˆ—= 2 with the entries of Aâˆ—âˆˆRMÃ—Hâˆ—and Bâˆ—âˆˆRLÃ—Hâˆ—independently drawn
from Gauss1(0, 12), and the entries of E âˆˆRLÃ—M independently drawn from
Gauss1(0, 0.32). We set the prior covariances to cahcbh = 1. As Lemma 12.16
states, the global minimum of the MAP free energy is at Ïƒ2 â†’+0. Since
no nontrivial local minimum is observed, local search gives the same trivial
solution. On the other hand, the PB free energy has a minimum in the
positive region Ïƒ2 > 0 with probability 1, as Lemma 12.17 states. However,
we empirically observed that PB learning tends to underestimate the noise

12.2 More General Cases
329
0.1
0.2
0.3
0.4
-1
0
1
2
VB
PB
MAP
Figure 12.7 Free energy dependence on Ïƒ2. Crosses indicate nontrivial minima.
variance, as in Figure 12.7. Therefore, we cannot expect that the noise variance
estimation works well in PB learning, either.
The situation is more complicated in the empirical Bayesian variants. Since
the global EMAP estimator and the global EPB estimator, given any Ïƒ2 > 0,
are the null solution, the joint global optimization over all variational param-
eters and the hyperparameters results in Ïƒ2 = min(L,M)
h=1
Î³2
h/(LM), regardless
of observationsâ€”all observed signals are considered to be noise. If we adopt
nontrivial local solutions as estimators, i.e., the local-EMAP estimator and the
local-EPB estimator, the free energies are not continuous anymore as functions
of Ïƒ2, because of the energy jump by the entropy factor Ï‡ of the pseudo-
Dirac delta function. Even in that case, if we globally minimize the free
energies with respect to Ïƒ2, the estimator contains no nontrivial local solution,
because the null solutions cancel all entropy factors. As such, no reasonable
way to estimate the noise variance has been found in EMAP learning and in
EPB learning.
In the previous work on the TF model with PB learning (Chu and Ghahra-
mani, 2009) and with EMAP learning (MÃ¸rup and Hansen, 2009), the noise
variance was treated as a given constant. This was perhaps because the noise
variance estimation failed, which is consistent with the preceding discussion.
12.2 More General Cases
Although extending the analysis for fully observed MF to more general cases
is not easy in general, some basic properties can be shown. Speciï¬cally, this
section shows that the global solutions for EMAP learning and EPB learning
are also trivial and useless in the MF model with missing entries and in the TF
model. Nevertheless, we experimentally show in Section 12.3 that local search
for EMAP learning and EPB learning provides estimators that behave similarly
to the EVB estimator.

330
12 MAP and Partially Bayesian Learning
12.2.1 Matrix Factorization with Missing Entries
The MF model with missing entries was introduced in Section 3.2. There, the
likelihood (12.1) is replaced with
p(V|A, B) âˆexp

âˆ’1
2Ïƒ2
####PÎ› (V) âˆ’PÎ›

BAâŠ¤ ####
2
Fro

,
(12.123)
where Î› denotes the set of observed indices, and
(PÎ› (V))l,m =
â§âªâªâ¨âªâªâ©
Vl,m
if (l, m) âˆˆÎ›,
0
otherwise.
The VB free energy is explicitly written as
2F = # (Î›) Â· log(2Ï€Ïƒ2) + M log det (CA) + L log det (CB)
âˆ’
M

m=1
log det
Î£A,m
 
âˆ’
L

l=1
log det
Î£B,l
 
âˆ’(L + M)H
+ tr
â§âªâªâ¨âªâªâ©Câˆ’1
A
â›âœâœâœâœâœâA
âŠ¤A +
M

m=1
Î£A,m
ââŸâŸâŸâŸâŸâ + Câˆ’1
B
â›âœâœâœâœâœâB
âŠ¤B +
L

l=1
Î£B,l
ââŸâŸâŸâŸâŸâ 
â«âªâªâ¬âªâªâ­
+ Ïƒâˆ’2 
(l,m)âˆˆÎ›

Vl,m âˆ’2Vl,ma
âŠ¤
mbl + tr
)!
ama
âŠ¤
m + Î£A,m
" 
blb
âŠ¤
l + Î£B,l
1
,
(12.124)
where # (Î›) denotes the number of observed entries.
We deï¬ne the EMAP learning problem and the EPB learning problem by
Eq. (12.77) with the free energy given by Eq. (12.124). The following holds:
Lemma 12.18
The global solutions of EMAP learning and EPB learning for
the MF model with missing entries, i.e., Eqs. (12.123), (12.2), and (12.3), are
U
EMAP = U
EPB = BA
âŠ¤= 0(L,M), regardless of observations.
Proof
The posterior covariance for A is clipped to Î£A,m = Îµ2IH in EPB-B
learning, while the posterior covariance for B is clipped to Î£B,m = Îµ2IH in
EPB-A learning. In either case, one can make the second or the third term in the
free energy (12.124) arbitrarily small to cancel the fourth or the ï¬fth term by
setting CA = Îµ2IH or CB = Îµ2IH. Then, because of the terms in the third line of
Eq. (12.124), which come from the prior distributions, it holds that A â†’0(M,H)
or B â†’0(M,H) for Îµ2 â†’+0, which results in U
EPB = BA
âŠ¤â†’0(L,M). In EMAP
learning, both posterior covariances are clipped to Î£A,m = Î£B,m = Îµ2IH. By the
same argument as for EPB learning, we can show that U
EMAP = BA
âŠ¤â†’0(L,M),
which completes the proof.
â–¡

12.2 More General Cases
331
12.2.2 Tucker Factorization
The TF model was introduced in Section 3.3.1. The likelihood and the priors
are given by
p(V|G, {A(n)}) âˆexp
â›âœâœâœâœâœâœââˆ’
###V âˆ’G Ã—1 A(1) Â· Â· Â· Ã—N A(N)###2
2Ïƒ2
ââŸâŸâŸâŸâŸâŸâ ,
(12.125)
p(G) âˆexp

âˆ’vec(G)âŠ¤(CG(N) âŠ—Â· Â· Â· âŠ—CG(1))âˆ’1 vec(G)
2

,
(12.126)
p({A(n)}) âˆexp
â›âœâœâœâœâœââˆ’
N
n=1 tr(A(n)Câˆ’1
A(n) A(n)âŠ¤)
2
ââŸâŸâŸâŸâŸâ ,
(12.127)
where âŠ—and vec(Â·) denote the Kronecker product and the vectorization
operator, respectively. {CG(n)} and {CA(n)} are the prior covariances restricted
to be diagonal, i.e.,
CG(n) = Diag

c2
g(n)
1 ,. . . , c2
g(n)
H(n)

,
CA(n) = Diag

c2
a(n)
1 ,. . . , c2
a(n)
H(n)

.
We denote Ë˜CG = CG(N) âŠ—Â· Â· Â· âŠ—CG(1).
The VB free energy is explicitly written as
2F =
â›âœâœâœâœâœâ
N

n=1
M(n)
ââŸâŸâŸâŸâŸâ log(2Ï€Ïƒ2) + log det
 Ë˜CG
 
+
N

n=1
M(n) log det (CA(n))
âˆ’log det
!Ë˜Î£G
"
âˆ’
N

n=1
M(n) log det
Î£ A(n)
 
+ âˆ¥Vâˆ¥2
Ïƒ2
âˆ’
N

n=1
H(n) âˆ’
N

n=1
(M(n)H(n))
+ tr
!
Ë˜C
âˆ’1
G (Ë˜gË˜g
âŠ¤+ Ë˜Î£G)
"
+
N

n=1
tr
!
Câˆ’1
A(n)(A
(n)âŠ¤A
(n) + M(n)Î£ A(n))
"
âˆ’2
Ïƒ2 Ë˜vâŠ¤(A
(N) âŠ—Â· Â· Â· âŠ—A
(1))Ë˜g
+ 1
Ïƒ2 tr
2!
(A
(N)âŠ¤A
(N) + M(N)Î£ A(N)) âŠ—Â· Â· Â· âŠ—(A
(1)âŠ¤A
(1) + M(1)Î£ A(1))
"
Â·(Ë˜gË˜g
âŠ¤+ Ë˜Î£G)
3
.
(12.128)
In the TF model, we refer as PB-G learning to the approximate Bayesian
method where the posteriors for the factor matrices {A(N)} are approximated

332
12 MAP and Partially Bayesian Learning
by the pseudo-Dirac delta function, and as PB-A learning to the one where
the posterior for the core tensor G is approximated by the pseudo-Dirac delta
function. PB learning chooses the one giving a lower free energy from PB-G
learning and PB-A learning. MAP learning approximates both posteriors
by the pseudo-Dirac delta function. Note that the approach by Chu and
Ghahramani (2009) corresponds to PB-G learning with the prior covariances
ï¬xed to CG(n) = CA(n) = IH(n) for n = 1,. . . , N, while the approach, called ARD
Tucker, by MÃ¸rup and Hansen (2009) corresponds to EMAP learning with the
prior covariances estimated from observations. In both approaches, the noise
variance Ïƒ2 was treated as a given constant.
Again the global solutions of EMAP learning and EPB learning are trivial
and useless.
Lemma 12.19
The global solutions of EMAP learning and EPB learning for
the TF model, i.e., Eqs. (12.125) through (12.127), are 
UEMAP = 
UEPB =

G Ã—1 A
(1) Â· Â· Â· Ã—N A
(N) = 0(M(1), ... ,M(N)), regardless of observations.
Proof
The posterior covariance for G is clipped to Ë˜Î£G = Îµ2IN
n=1 H(n) in EPB-A
learning, while the posterior covariances for {A(n)} are clipped to {Î£ A(n) =
Îµ2IH(n)} in EPB-G learning. In either case, one can make the second or the third
term in the free energy (12.128) arbitrarily small to cancel the fourth or the ï¬fth
term by setting {CG(n) = Îµ2IH(n)} or {CA(n) = Îµ2IH(n)}. Then, because of the terms
in the fourth line of Eq. (12.128), which come from the prior distributions,
it holds that 
G â†’0(H(1),...,H(N)) or {A
(N) â†’0(M(N),H(N))} for Îµ2 â†’+0, which
results in 
UEPB = 0(M(1),...,M(N)). In EMAP learning, both posterior covariances
are clipped to Ë˜Î£G = Îµ2IN
n=1 H(n) and {Î£ A(n) = Îµ2IH(n)}, respectively. By the same
argument as for EPB learning, we can show that 
UEMAP = 0(M(1),...,M(N)), which
completes the proof.
â–¡
12.3 Experimental Results
In this section, we experimentally investigate the behavior of EMAP learning
and EPB learning, in comparison with EVB learning. We start from the fully
observed MF model, where we can assess how often local search ï¬nds the
nontrivial local solution (derived in Section 12.1.3) rather than the global null
solution. After that, we conduct experiments in collaborative ï¬ltering, where
the MF model with missing entries is used, and in TF.
For local search, we adopted the standard iterative algorithm. The standard
iterative algorithm for EVB learning has been derived in Chapter 3. The

12.3 Experimental Results
333
standard iterative algorithms for EPB learning and EMAP learning, which
can be derived simply by setting the derivatives of the corresponding free
energies with respect to the unknown parameters to zero, similarly apply the
stationary conditions in turn to update unknown parameters. For initialization,
the entries of the mean parameters, e.g., A, B, and 
G, were drawn from
Gauss1(0, 12), while the covariance parameters were set to the identity, e.g.,
Ë˜Î£G = IN
n=1 H(n), Î£ A(n) = CG(n) = CA(n) = IH(n). We used this initialization scheme
through all experiments in this section.
12.3.1 Fully Observed MF
We ï¬rst conducted an experiment on an artiï¬cial (Artiï¬cial1) data set, which
was generated as follows. We randomly generated true matrices Aâˆ—âˆˆRMÃ—Hâˆ—
and Bâˆ—âˆˆRLÃ—Hâˆ—such that each entry of Aâˆ—and Bâˆ—follows Gauss1(0, 1).
An observed matrix V âˆˆRLÃ—M was created by adding a noise subject to
Gauss1(0, 1) to each entry of Bâˆ—Aâˆ—âŠ¤. Figures 12.8 through 12.10 show the
free energy and the estimated rank over iterations in EVB learning, local-EPB
0
500
1,000
1,500
2,000
2,500
2.02
2.04
2.06
2.08
2.1
2.12
2.14
Iteration
F /(LM )
 
 
EVB(Analytic)
EVB(Iterative)
(a) Free energy
0
500
1,000
1,500
2,000
2,500
0
20
40
60
80
100
Iteration
H
 
 
EVB(Analytic)
EVB(Iterative)
(b) Estimated rank
Figure 12.8 EVB learning on Artiï¬cial1 (L = 100, M = 300, Hâˆ—= 20).
0
500
1,000
1,500
2,000
2,500
1.85
1.9
1.95
Iteration
Localâˆ’EPB(Analytic)
Localâˆ’EPB(Iterative)
(a) Free energy
0
500
1,000
1,500
2,000
2,500
0
20
40
60
80
100
Iteration
Localâˆ’EPB(Analytic)
Localâˆ’EPB(Iterative)
(b) Estimated rank
Figure 12.9 Local-EPB learning on Artiï¬cial1.

334
12 MAP and Partially Bayesian Learning
Table 12.1 Estimated rank in fully observed MF experiments.

HEVB

Hlocal-EPB

Hlocal-EMAP
Data set
M
L Hâˆ—Analytic Iterative Analytic Iterative Analytic Iterative
Artiï¬cial1
300 100 20
20
20 (100%)
20
20 (100%)
20
20 (100%)
Artiï¬cial2
500 400
5
5
5 (100%)
8
8 (90%)
5
5 (100%)
9 (10%)
Chart
600 60 â€“
2
2 (100%)
2
2 (100%)
2
2 (100%)
Glass
214
9 â€“
1
1 (100%)
1
1 (100%)
1
1 (100%)
Optical
5,620 64 â€“
10
10 (100%)
10
10 (100%)
6
6 (100%)
Digits
Satellite
6,435 36 â€“
2
2 (100%)
2
2 (100%)
1
1 (100%)
0
500
1,000
1,500
2,000
2,500
1.36
1.38
1.4
1.42
1.44
1.46
1.48
Iteration
Localâˆ’EMAP(Analytic)
Localâˆ’EMAP(Iterative)
(a) Free energy
0
500
1,000
1,500
2,000
2,500
0
20
40
60
80
100
Iteration
Localâˆ’EMAP(Analytic)
Localâˆ’EMAP(Iterative)
(b) Estimated rank
Figure 12.10 Local-EMAP learning on Artiï¬cial1.
learning, and local-EMAP learning, respectively, on the Artiï¬cial1 data set
with the data matrix size L = 100 and M = 300, and the true rank Hâˆ—= 20.
The noise variance was assumed to be known, i.e., it was set to Ïƒ2 = 1. We
performed iterative local search 10 times, starting from different initial points,
and each trial is plotted by a solid curve in the ï¬gures. The results computed
by the analytic-form solutions for EVB learning (Theorem 6.13), local-EPB
learning (Theorem 12.13), and local-EMAP learning (Theorem 12.12) were
plotted as dashed lines. We can observe that iterative local search for EPB
learning and EMAP learning tends to successfully ï¬nd the nontrivial local
solutions, although they are not global solutions.
We also conducted experiments on another artiï¬cial data set and benchmark
data sets. The results are summarized in Table 12.1. Artiï¬cial2 was created in
the same way as Artiï¬cial1, but with L = 400, M = 500, and Hâˆ—= 5. The
benchmark data sets were collected from the UCI repository (Asuncion and

12.3 Experimental Results
335
Newman, 2007), on which we set the noise variance under the assumption that
the signal to noise ratio is 0 db, following MÃ¸rup and Hansen (2009).
In the table, the estimated ranks by the analytic-form solution and by
iterative local search are shown. The percentages for iterative local search
indicate the frequencies over 10 trials. We observe the following: ï¬rst, iterative
local search tends to estimate the same rank as the analytic-form (local)
solution; and second, the estimated rank tends to be consistent among EVB
learning, local-EPB learning, and local-EMAP learning. Furthermore, on the
artiï¬cial data sets, where the true rank is known, the rank is correctly estimated
in most of the cases. Exceptions are Artiï¬cial2, where local-EPB learning
overestimates the rank, and Optical Digits and Satellite, where local-EMAP
learning estimates a smaller rank than the others. These phenomena can be
explained by the theoretical implications in Section 12.1.3: in Artiï¬cial2, the
ratio Î¾ = Hâˆ—/ min(L, M) = 5/400 between the true rank and the possible
largest rank is small, which means that most of the singular components consist
of noise. In such a case, local-EPB learning with its truncation threshold
lower than MPUL tends to retain components purely consisting of noise (see
Figure 12.6). In Optical Digits and Satellite, Î± (= 64/5620 for Optical Digits
and = 36/6435 for Satellite) is extremely small, and therefore local-EMAP
learning with its higher truncation threshold tends to discard more components
than the others, as Figure 12.6 implies.
12.3.2 Collaborative Filtering
Next we conducted experiments in the collaborative ï¬ltering (CF) scenario,
where the observed matrix has missing entries to be predicted by the MF
model.
We generated an artiï¬cial (Artiï¬cialCF) data set in the same way as the
fully observed case for L = 2,000, M = 5,000, Hâˆ—= 5, and then masked
99% of the entries as missing values. We applied EVB learning, local-EPB
learning, and local-EMAP learning to the MF model with missing entries, i.e.,
Eqs. (12.123), (12.2), and (12.3).4 Figure 12.11 shows the estimated rank and
the generalization error over iterations for 10 trials, where the generalization
error is deï¬ned as GE = âˆ¥PÎ›â€²(V) âˆ’PÎ›â€²(BA
âŠ¤)âˆ¥2
Fro/(# (Î›â€²) Ïƒ2) for Î›â€² being the
set of test indices.
4 Here we solve the EVB learning problem, the EPB learning problem, and the EMAP learning
problem, respectively, by the standard iterative algorithms. However, we refer to the last two
methods as local-EPB learning and local-EMAP learning, since we expect the local search
algorithm to ï¬nd not the global null solution but the nontrivial local solution.

336
12 MAP and Partially Bayesian Learning
0
500
1,000
1,500
2,000
2,500
Iteration
0
5
10
15
20
EVB
Local-EPB
Local-EMAP
(a) Estimated rank
0
500
1,000
1,500
2,000
2,500
Iteration
4
4.5
5
5.5
6
EVB
Local-EPB
Local-EMAP
(b) Generalization error
Figure 12.11 CF result on Artiï¬cialCF (L = 2,000, M = 5,000, Hâˆ—= 5 with 99%
missing ratio).
0
500
1,000
1,500
2,000
2,500
Iteration
0
5
10
15
20
EVB
Local-EPB
Local-EMAP
(a) Estimated rank
0
500
1,000
1,500
2,000
2,500
Iteration
0.5
0.55
0.6
0.65
EVB
Local-EPB
Local-EMAP
(b) Generalization error
Figure 12.12 CF result on MovieLens (L = 943, M = 1,682 with 99% missing
ratio).
We also conducted an experiment on the MovieLens data sets (with L = 943,
M = 1,682).5 We randomly divided the observed entries into training entries
and test entries, so that 99% of the entries are missing in the training phase. The
test entries are used to evaluate the generalization error. Figure 12.12 shows the
result in the same format as Figure 12.11.
We see that, on both data sets, local-EMAP learning tends to estimate a
similar rank to EVB learning, while local-EPB learning tends to estimate
a larger rankâ€”a similar tendency to the fully observed case. In terms of
the generalization error, local-EPB learning performs comparably to EVB
learning, while local-EMAP learning performs slightly worse.
12.3.3 Tensor Factorization
Finally, we conducted experiments on TF. We created an artiï¬cial (Artiï¬-
cialTF) data set, following MÃ¸rup and Hansen (2009): we drew a three-mode
5 www.grouplens.org/

12.3 Experimental Results
337
Table 12.2 Estimated rank (effective size of core tensor) in TF experiments.
Data set
M
Hâˆ—

HEVB

Hlocal-EPB

Hlocal-EMAP

HARD-Tucker
Artiï¬cialTF
(30, 40, 50)
(3, 4, 5)
(3, 4, 5):
(3, 4, 5):
(3, 4, 5):
(3, 4, 5):
100%
100%
90%
100%
(3, 7, 5):
10%
FIA
(12, 100, 89)
(3, 6, 4)
(3, 5, 3):
(3, 5, 3):
(3, 5, 2):
(3, 4, 2):
100%
100%
50%
70%
(4, 5, 2):
(3, 5, 2):
20%
10%
(5, 4, 2):
(3, 7, 2):
10%
10%
(4, 4, 2):
(10, 4, 3):
10%
10%
(8, 5, 2):
10%
random tensor of the size (M(1), M(1), M(1)) = (30, 40, 50) with the signal
components (H(1)âˆ—, H(2)âˆ—, H(3)âˆ—) = (3, 4, 5). The noise is added so that the
signal-to-noise ratio is 0 db. We also used the Flow Injection Analysis (FIA)
data set.6 Table 12.2 shows the estimated rank with frequencies over 10 trials.
Here we also show the results by ARD Tucker with the ridge prior (MÃ¸rup
and Hansen, 2009), performed with the code provided by the authors. Local-
EMAP learning and ARD Tucker minimize exactly the same objective, and
the slightly different results come from the differences in the local search
algorithm (standard iterative vs. gradient descent) and in the initialization
scheme.
We generally observe that all learning methods provide reasonable results,
although local-EMAP learning, as well as ARD Tucker, is less stable than the
others.
6 www.models.kvl.dk/datasets


Part IV
Asymptotic Theory


13
Asymptotic Learning Theory
Part IV is dedicated to asymptotic theory of variational Bayesian (VB) learn-
ing. In this part, â€œasymptotic limitâ€ always means the limit when the number
N of training samples goes to inï¬nity. The main goal of asymptotic learning
theory is to clarify the behavior of some statistics, e.g., the generalization
error, the training error, and the Bayes free energy, which indicate how fast a
learning machine can be trained as a function of the number of training samples
and how the trained machine is biased to the training samples by overï¬tting.
This provides the mathematical foundation of information criteria for model
selectionâ€”a task to choose the degree of freedom of a statistical model based
on observed training data. We can also evaluate the approximation accuracy of
VB learning to full Bayesian learning in terms of the free energy, i.e., the gap
between the VB free energy and the Bayes free energy, which corresponds to
the tightness of the evidence lower-bound (ELBO) (see Section 2.1.1). In this
ï¬rst chapter of Part IV, we give an overview of asymptotic learning theory as
the background for the subsequent chapters.
13.1 Statistical Learning Machines
A statistical learning machine consists of two fundamental components,
a statistical model and a learning algorithm (Figure 13.1). The statistical
model is denoted by a probabilistic distribution depending on some unknown
parameters, and the learning algorithm estimates the unknown parameters from
observed training samples. Before introducing asymptotic learning theory, we
categorize statistical learning machines based on the model and the learning
algorithm.
341

342
13 Asymptotic Learning Theory
Learning machine
Statistical model
Linear models
Neural networks
Mixture models
Learning algorithm
ML estimation
MAP learning
Bayesian learning
Figure 13.1 A statistical learning machine consists of a statistical model and a
learning algorithm.
Regular
Linear models
Statistical models
Singular
Neural networks
Mixture models
Bayesian networks
Hidden Markov models
Figure 13.2 Statistical models are classiï¬ed into regular models and singular
models.
13.1.1 Statistical Modelsâ€”Regular and Singular
We classify the statistical models into two classes, the regular models and the
singular models (Figure 13.2). The regular models are identiï¬able (Deï¬nition
7.4 in Section 7.3.1), i.e.,
p(x|w1) = p(x|w2) â‡â‡’w1 = w2
for any w1, w2 âˆˆW,
(13.1)
and do not have singularities in the parameter space, i.e., the Fisher
information
SD
+ âˆ‹F(w) =
 âˆ‚log p(x|w)
âˆ‚w
âˆ‚log p(x|w)
âˆ‚w
âŠ¤
p(x|w)dx
(13.2)
is nonsingular (or full-rank) for any w âˆˆW.
With a few additional assumptions, the regular models were analyzed
under the regularity conditions (Section 13.4.1), which lead to the asymptotic
normality of the distribution of the maximum likelihood (ML) estimator,
and the asymptotic normality of the Bayes posterior distribution (Cramer,

13.1 Statistical Learning Machines
343
1949; Sakamoto et al., 1986; van der Vaart, 1998). Based on those asymp-
totic normalities, a uniï¬ed theory was established, clarifying the asymptotic
behavior of generalization properties, which are common over all regular
models, and over all reasonable learning algorithms, including ML learning,
maximum a posteriori (MAP) learning, and Bayesian learning, as will be seen
in Section 13.4.
On the other hand, analyzing singular models requires speciï¬c techniques
for different models and different learning algorithms, and it was revealed that
the asymptotic behavior of generalization properties depends on the model
and the algorithm (Hartigan, 1985; Bickel and Chernoff, 1993; Takemura and
Kuriki, 1997; Kuriki and Takemura, 2001; Amari et al., 2002; Hagiwara, 2002;
Fukumizu, 2003; Watanabe, 2009). This is because the true parameter is at a
singular point when the model size is larger than necessary to express the true
distribution, and, in such cases, singularities affect the distribution of the ML
estimator, as well as the Bayes posterior distribution even in the asymptotic
limit. Consequently, the asymptotic normality, on which the regular learning
theory relies, does not hold in singular models.
13.1.2 Learning Algorithmsâ€”Point Estimation
and Bayesian Learning
When analyzing singular models, we also classify learning algorithms into
two classes, point estimation and Bayesian learning (Figure 13.3). The point
estimation methods, including ML learning and MAP learning, choose a single
model (i.e., a single point in the parameter space) that maximizes a certain
criterion such as the likelihood or the posterior probability, while Bayesian
learning methods use an ensemble of models over the posterior distribution or
its approximation.
Learning algorithms
Point estimation
ML learning
MAP learning
Bayesian
Bayesian learning
VB learning
PB learning
EP
Figure 13.3 Learning algorithms are classiï¬ed into point-estimation and Bayesian
learning.

344
13 Asymptotic Learning Theory
Unlike in the regular models, point estimation and Bayesian learning
show different learning behavior in singular models. This is because how
singularities affect the learning property depends on the learning methods. For
example, as discussed in Chapter 7, strong nonuniformity of the density of
the volume element leads to model-induced regularization (MIR) in Bayesian
learning, while it does not affect point-estimation methods.
13.2 Basic Tools for Asymptotic Analysis
Here we introduce basic tools for asymptotic analysis.
13.2.1 Central Limit Theorem
Asymptotic learning theory heavily relies on the central limit theorem.
Theorem 13.1
(Central limit theorem) (van der Vaart, 1998) Let
{x(1),. . . , x(N)} be N i.i.d. samples from an arbitrary distribution with ï¬nite
mean Î¼ âˆˆRD and ï¬nite covariance Î£ âˆˆSD
++, and let x = Nâˆ’1 N
n=1 x(n) be their
average. Then, the distribution of z =
âˆš
N(x âˆ’Î¼) converges to the Gaussian
distribution with mean zero and covariance Î£,1 i.e.,
p (z) â†’GaussD (z; 0, Î£)
as
N â†’âˆ.
(13.3)
Intuitively, Eq. (13.3) can be interpreted as
p $x% â†’GaussD

x; Î¼, Nâˆ’1Î£
 
as
N â†’âˆ,
(13.4)
implying that the distribution of the average x of i.i.d. random variables
converges to the Gaussian distribution with mean Î¼ and covariance Nâˆ’1Î£.
The central limit theorem implies the (weak) law of large numbers,2 i.e., for
any Îµ > 0,
lim
Nâ†’âˆProb $âˆ¥x âˆ’Î¼âˆ¥> Îµ% = 0.
(13.5)
13.2.2 Asymptotic Notation
We use the following asymptotic notation, a.k.a, Bachmannâ€“Landau notation,
to express the order of functions when the number N of samples goes to
inï¬nity:
1 We consider weak topology in the space of distributions, i.e., p(x) is identiï¬ed with r(x) if
âŸ¨f(x)âŸ©p(x) = âŸ¨f(x)âŸ©r(x) for any bounded continuous function f(x). Convergence (of a random
variable x) in this sense is called convergence in distribution, weak convergence, or convergence
in law, and denoted as p(x) â†’r(x) or x  r(x) (van der Vaart, 1998).
2 Convergence x â†’Î¼ in the sense that limNâ†’âˆProb (âˆ¥x âˆ’Î¼âˆ¥> Îµ) = 0, âˆ€Îµ > 0 is called
convergence in probability.

13.2 Basic Tools for Asymptotic Analysis
345
O( f(N)) : A function such that lim sup
Nâ†’âˆ
|O( f(N))/f(N)| < âˆ,
o( f(N)) : A function such that lim
Nâ†’âˆo( f(N))/f(N) = 0,
Î©( f(N)) : A function such that lim inf
Nâ†’âˆ|Î©( f(N))/f(N)| > 0,
Ï‰( f(N)) : A function such that lim
Nâ†’âˆ|Ï‰( f(N))/f(N)| = âˆ,
Î˜( f(N)) : A function such that lim sup
Nâ†’âˆ
|Î˜( f(N))/f(N)| < âˆ
and lim inf
Nâ†’âˆ|Î˜( f(N))/f(N)| > 0.
Intuitively, as a function of N, O( f(N)) is a function of no greater order than
f(N), o( f(N)) is a function of less order than f(N), Î©( f(N)) is a function
of no less order than f(N), Ï‰( f(N)) is a function of greater order than f(N),
and Î˜( f(N)) is a function of the same order as f(N). One thing we need to
be careful of is that the upper-bounding notations, O and o, preserve after
addition and subtraction, while lower-bounding notations, Î© and Ï‰, as well as
the both-sides-bounding notation Î˜, do not necessarily preserve. For example,
if g1(N) = Î˜( f(N)) and g2(N) = Î˜( f(N)) then g1(N) + g2(N) = O( f(N)),
while it can happen that g1(N) + g2(N)  Î˜( f(N)) since the leading terms
of g1(N) and g2(N) can coincide with each other with opposite signs and be
canceled out.
For random variables, we use their probabilistic versions, Op, op, Î©p, Ï‰p,
and Î˜p, for which the corresponding conditions hold in probability. For
example, for i.i.d. samples {x(n)}N
n=1 from Gauss1(x; 0, 12), we can say that
x(n) = Î˜p(1),
x = 1
N
N

n=1
x(n) = Î˜p(Nâˆ’1/2),
x2 = 1
N
N

n=1

x(n) 2 = 1 + Î˜p(Nâˆ’1/2).
Note that the second and the third equations are consequences from the central
limit theorem (Theorem 13.1) applied to the samples {x(n)} that follow the
Gaussian distribution, and to the samples {(x(n))2} that follow the chi-squared
distribution, respectively.
In this book, we express asymptotic approximation mostly by using asymp-
totic notation. To this end, we sometimes need to translate convergence of
a random variable into an equation with asymptotic notation. Let x be a
random variable depending on N, r(x) be a distribution with ï¬nite mean and

346
13 Asymptotic Learning Theory
covariance, and f(x) be an arbitrary bounded continuous function. Then the
following hold:
â€¢ If p(x) â†’r(x), i.e., the distribution of x converges to r(x), then
x = Op(1)
and
âŸ¨f(x)âŸ©p(x) = âŸ¨f(x)âŸ©r(x) (1 + o(1)) .
â€¢ If limNâ†’âˆProb (âˆ¥x âˆ’yâˆ¥> Îµ) = 0 for any Îµ > 0, then
x = y + op(1).
For example, the central limit theorem (13.3) implies that
x = Î¼ + Op(Nâˆ’1/2),

(x âˆ’Î¼)(x âˆ’Î¼)âŠ¤
p(x) = Nâˆ’1Î£ + o(Nâˆ’1),
while the law of large numbers (13.5) implies that
x = Î¼ + op(1).
13.3 Target Quantities
Here we introduce target quantities to be analyzed in asymptotic learning
theory.
13.3.1 Generalization Error and Training Error
Consider a statistical model p(x|w), where x âˆˆRM is an observed random
variable and w âˆˆRD is a parameter to be estimated. Let X = (x(1),. . . , x(N))âŠ¤âˆˆ
RNÃ—M be N i.i.d. training samples taken from the true distribution q(x). We
assume realizabilityâ€”the true distribution can be exactly expressed by the
statistical model, i.e., âˆƒwâˆ—s.t. q(x) = p(x|wâˆ—), where wâˆ—is called the true
parameter.
Learning algorithms estimate the parameter value w or its posterior distri-
bution given the training data D = X, and provide the predictive distribution
p(x|X) for a new sample x. For example, ML learning provides the predictive
distribution given by
pML(x|X) = p(x|wML),
(13.6)
where
wML = argmax
w
p(X|w) = argmax
w
â›âœâœâœâœâœâ
N

n=1
p(x(n)|w)
ââŸâŸâŸâŸâŸâ 
(13.7)

13.3 Target Quantities
347
is the ML estimator, while Bayesian learning provides the predictive distribu-
tion given by
pBayes(x|X) = âŸ¨p(x|w)âŸ©p(w|X) =

p(x|w)p(w|X)dw,
(13.8)
where
p(w|X) = p(X|w)p(w)
p(X)
=
p(X|w)p(w)

p(X|w)p(w)dw
(13.9)
is the posterior distribution (see Section 1.1).
The generalization error, a criterion of generalization performance, is
deï¬ned as the Kullbackâ€“Leibler (KL) divergence of the predictive distribution
from the true distribution:
GE(X) =

q(x) log q(x)
p(x|X)dx.
(13.10)
Its empirical variant,
TE(X) = 1
N
N

n=1
log q(x(n))
p(x(n)|X),
(13.11)
is called the training error, which is often used as an estimator of the
generalization error. Note that, for the ML predictive distribution (13.6),
âˆ’N Â· TEML(X) =
N

n=1
log p(x(n)|wML)
q(x(n))
(13.12)
corresponds to the log-likelihood ratio, an important statistic for statistical test,
when the null hypothesis is true.
The generalization error (13.10) and the training error (13.11) are random
variables that depend on realization of the training data X. Taking the average
over the distribution of training samples, we deï¬ne deterministic quantities,
GE(N) = âŸ¨GE(X)âŸ©q(X) ,
(13.13)
TE(N) = âŸ¨TE(X)âŸ©q(X) ,
(13.14)
which are called the average generalization error and the average training
error, respectively. Here âŸ¨Â·âŸ©q(X) denotes the expectation value over the distri-
bution of N training samples. The average generalization error and the average
training error are scalar functions of the number N of samples, and represent
generalization performance of a learning machine consisting of a statistical
model and a learning algorithm. The optimality of Bayesian learning is proven
in terms of the average generalization error (see Appendix D).

348
13 Asymptotic Learning Theory
If a learning algorithm can successfully estimate the true parameter wâˆ—
with reasonably small error, the average generalization error and the average
training error converge to zero with the rate Î˜(Nâˆ’1) in the asymptotic limit.3
One of the main goals of asymptotic learning theory is to identify or bound
the coefï¬cients of their leading terms, i.e., Î» and Î½ in the following asymptotic
expansions:
GE(N) = Î»Nâˆ’1 + o(Nâˆ’1),
(13.15)
TE(N) = Î½Nâˆ’1 + o(Nâˆ’1).
(13.16)
We call Î» and Î½ the generalization coefï¬cient and the training coefï¬cient,
respectively.
13.3.2 Bayes Free Energy
The marginal likelihood (deï¬ned by Eq. (1.6) in Chapter 1),
p(X) =

p(X|w)p(w)dw =

p(w)
N

n=1
p(x(n)|w)dw,
(13.17)
is also an important quantity in Bayesian learning. As explained in Section
1.1.3, the marginal likelihood can be regarded as the likelihood of an ensemble
of modelsâ€”the set of model distributions with the parameters subject to the
prior distribution. Following the concept of the â€œlikelihoodâ€ in statistics, we
can say that the ensemble of models giving the highest marginal likelihood
is most likely. Therefore, we can perform model selection by maximizing the
marginal likelihood (Efron and Morris, 1973; Schwarz, 1978; Akaike, 1980;
MacKay, 1992; Watanabe, 2009). Maximizing the marginal likelihood (13.17)
amounts to minimizing the Bayes free energy, deï¬ned by Eq. (1.60):
FBayes(X) = âˆ’log p(X).
(13.18)
The Bayes free energy is a random variable depending on the training
samples X, and is of the order of Î˜p(N). However, the dominating part comes
from the entropy of the true distribution, and does not depend on the statistical
model nor the learning algorithm. In statistical learning theory, we therefore
analyze the behavior of the relative Bayes free energy,
FBayes(X) = log q(X)
p(X) = FBayes(X) âˆ’NS N(X),
(13.19)
3 This holds if the estimator achieves a mean squared error in the same order as the CramÂ´erâ€“Rao
lower-bound, i.e.,

âˆ¥w âˆ’wâˆ—âˆ¥2
q(X) â‰¥Nâˆ’1tr

Fâˆ’1(wâˆ—)
 
, where F is the Fisher information (13.2)
at wâˆ—. The CramÂ´erâ€“Rao lower-bound holds for any unbiased estimator under the regularity
conditions.

13.3 Target Quantities
349
where
S N(X) = âˆ’1
N
N

n=1
log q(x(n))
(13.20)
is the empirical entropy. The negative of the relative Bayes free energy,
âˆ’FBayes(X) = log

p(w) N
n=1 p(x(n)|w)dw
N
n=1 q(x(n))
,
(13.21)
can be seen as an ensemble version of the log-likelihood ratioâ€”the logarithm
of the ratio between the marginal likelihood (alternative hypothesis) and the
true likelihood (null hypothesis).
When the prior p(w) is positive around the true parameter wâˆ—, the relative
Bayes free energy (13.19) is known to be of the order of Î˜(log N) and can be
asymptotically expanded as follows:
FBayes(X) = Î»â€²Bayes log N + op(log N),
(13.22)
where the coefï¬cient of the leading term Î»â€²Bayes is called the Bayes free energy
coefï¬cient. Note that, although the relative Bayes free energy is a random
variable depending on realization of the training data X, the leading term in
Eq. (13.22) is deterministic.
Let us deï¬ne the average relative Bayes free energy over the distribution of
training samples:
F
Bayes(N) =
FBayes(X)

q(X) =
/
log
N
n=1 q(x(n))

p(w) N
n=1 p(x(n)|w)dw
0
q(X)
.
(13.23)
An interesting and useful relation can be found between the average Bayes
generalization error and the average relative Bayes free energy (Levin et al.,
1990):
GE
Bayes(N) =

q(x) log
q(x)
pBayes(x|X)dx

q(X)
=
*
q(x) log
q(x)

p(x|w)p(w|X)dwdx
+
q(X)
=
*
q(x) log
q(x)

p(x|w)p(X|w)p(w)dwdx
+
q(X)
âˆ’
*
q(x) log
1

p(X|w)p(w)dwdx
+
q(X)
=
*
log
q(x)q(X)

p(x|w)p(X|w)p(w)dw
+
q(x)q(X) âˆ’
*
log
q(X)

p(X|w)p(w)dw
+
q(X)
= F
Bayes(N + 1) âˆ’F
Bayes(N).
(13.24)

350
13 Asymptotic Learning Theory
The relation (13.24) combined with the asymptotic expansions, Eqs. (13.15)
and (13.22), implies that the Bayes generalization coefï¬cient and the Bayes
free energy coefï¬cient coincide with each other, i.e.,
Î»â€²Bayes = Î»Bayes.
(13.25)
Importantly, this relation holds for any statistical model, regardless of being
regular or singular.
13.3.3 Target Quantities under Conditional Modeling
Many statistical models are for the regression or classiï¬cation setting, where
the model distribution p(y|x, w) is the distribution of an output y âˆˆRL
conditional on an input x âˆˆRM and an unknown parameter w âˆˆRD. The
input is assumed to be given for all samples including the future test samples.
Let D = {(x(1), y(1)),. . . , (x(N), y(N))} be N i.i.d. training samples drawn from
the true joint distribution q(x, y) = q(y|x)q(x). As noted in Example 1.2 in
Chapter 1, we can proceed with most computations without knowing the input
distribution q(x).
Let X = (x(1),. . . , x(N))âŠ¤âˆˆRNÃ—M and Y = (y(1),. . . , y(N))âŠ¤âˆˆRNÃ—L
separately summarize the inputs and the outputs in the training data. The
predictive distribution, given as a conditional distribution on a new input x
as well as the whole training samples D = (X, Y), can usually be computed
without any information on q(X). For example, the ML predictive distribution
is given as
pML(y|x, D) = p(y|x, wML),
(13.26)
where
wML = argmax
w
p(Y|X, w) Â· q(X) = argmax
w
â›âœâœâœâœâœâ
N

n=1
p(y(n)|x(n), w)
ââŸâŸâŸâŸâŸâ 
(13.27)
is the ML estimator, while the Bayes predictive distribution is given as
pBayes(y|x, D) = âŸ¨p(y|x, w)âŸ©p(w|X,Y) =

p(y|x, w)p(w|X, Y)dw,
(13.28)
where
p(w|X, Y) =
p(Y|X, w)p(w) Â· q(X)

p(Y|X, w)p(w)dw Â· q(X)
=
p(Y|X, w)p(w)

p(Y|X, w)p(w)dw
(13.29)
is the Bayes posterior distribution. Here (x, y) is a new inputâ€“output sample
pair, assumed to be drawn from the true distribution q(y|x)q(x).

13.4 Asymptotic Learning Theory for Regular Models
351
The generalization error (13.10), the training error (13.11), and the relative
Bayes free energy (13.19) can be expressed as follows:
GE(D) =
/
log
q(y|x)q(x)
p(y|x, D)q(x)
0
q(y|x)q(x)
=
/
log
q(y|x)
p(y|x, D)
0
q(y|x)q(x)
, (13.30)
TE(D) = 1
N
N

n=1
log
q(y(n)|x(n))q(x(n))
p(y(n)|x(n), D)q(x(n)) = 1
N
N

n=1
log
q(y(n)|x(n))
p(y(n)|x(n), D),
(13.31)
FBayes(D) = log q(Y|X)q(X)
p(Y|X)q(X) = FBayes(Y|X) âˆ’NS N(Y|X),
(13.32)
where
FBayes(Y|X) = log

p(w)
N

n=1
p(y(n)|x(n), w)dw,
(13.33)
S N(Y|X) = âˆ’1
N
N

n=1
log q(y(n)|x(n)).
(13.34)
We can see that the input distribution q(x) cancels out in most of the preceding
equations, and therefore Eqs. (13.30) through (13.34) can be computed without
considering q(x). Note that in Eq. (13.30), q(x) remains the distribution over
which the expectation is taken. However, it is necessary only formally, and the
expectation value does not depend on q(x) (as long as the regularity conditions
hold). The same applies to the average generalization error (13.13), the average
training error (13.14), and the average relative Bayes free energy (13.23),
where the expectation âŸ¨Â·âŸ©q(Y|X)q(X) over the distribution of the training samples
is taken.
13.4 Asymptotic Learning Theory for Regular Models
In this section, we introduce the regular learning theory, which generally holds
under the regularity conditions.
13.4.1 Regularity Conditions
The regularity conditions are deï¬ned for the statistical model p(x|w) parame-
terized by a ï¬nite-dimensional parameter vector w âˆˆW âŠ†RD, and the true
distribution q(x). We include conditions for the prior distribution p(w), which
are necessary for analyzing MAP learning and Bayesian learning. There are
variations, and we here introduce a (rough) simple set.

352
13 Asymptotic Learning Theory
(i) The statistical model p(x|w) is differentiable (as many times as
necessary) with respect to the parameter w âˆˆW for any x, and the
differential operator and the integral operator are commutable.
(ii) The statistical model p(x|w) is identiï¬able, i.e., Eq. (13.1) holds, and the
Fisher information (13.2) is nonsingular (full-rank) at any w âˆˆW.
(iii) The support of p(x|w), i.e., {x âˆˆX; p(x|w) > 0}, is common for all
w âˆˆW.
(iv) The true distribution is realizable by the statistical model, i.e.,
âˆƒwâˆ—s.t. q(x) = p(x|wâˆ—), and the true parameter wâˆ—is an interior point of
the domain W.
(v) The prior p(w) is twice differentiable and bounded as 0 < p(w) < âˆat
any w âˆˆW.
Note that the ï¬rst three conditions are on the model distribution p(x|w), the
fourth is on the true distribution q(x), and the ï¬fth is on the prior distribution
p(w).
An important consequence of the regularity conditions is that the log-
likelihood can be Taylor-expanded about any w âˆˆW:
log p(x|w) = log p(x|w) + (w âˆ’w)âŠ¤âˆ‚log p(x|w)
âˆ‚w
w=w
+ 1
2(w âˆ’w)âŠ¤âˆ‚2 log p(x|w)
âˆ‚wâˆ‚wâŠ¤
w=w(w âˆ’w) + O(âˆ¥w âˆ’wâˆ¥3). (13.35)
13.4.2 Consistency and Asymptotic Normality
We ï¬rst show consistency and asymptotic normality, which hold in ML
learning, MAP learning, and Bayesian learning.
Consistency of ML Estimator
The ML estimator is deï¬ned by
wML = argmax
w
log
â›âœâœâœâœâœâ
N

n=1
p(x(n)|w)
ââŸâŸâŸâŸâŸâ = argmax
w
LN(w),
(13.36)
where
LN(w) = 1
N
N

n=1
log p(x(n)|w).
(13.37)
By the law of large numbers (13.5), it holds that
LN(w) = Lâˆ—(w) + op(1),
(13.38)

13.4 Asymptotic Learning Theory for Regular Models
353
where
Lâˆ—(w) = log p(x|w)
p(x|wâˆ—) .
(13.39)
Identiï¬ability of the statistical model guarantees that
wâˆ—= argmax
w
Lâˆ—(w)
(13.40)
is the unique maximizer. Eqs. (13.36), (13.38), and (13.40), imply the consis-
tency of the ML estimator, i.e.,
wML = wâˆ—+ op(1).
(13.41)
Asymptotic Normality of the ML Estimator
Since the gradient âˆ‚LN(w)/âˆ‚w is differentiable, the mean value theorem4
guarantees that there exists Â´w âˆˆ[min(wML, wâˆ—), max(wML, wâˆ—)]D (where min(Â·)
and max(Â·) operate elementwise) such that
âˆ‚LN(w)
âˆ‚w
w=wML = âˆ‚LN(w)
âˆ‚w
w=wâˆ—+ âˆ‚2LN(w)
âˆ‚wâˆ‚wâŠ¤
w= Â´w(wML âˆ’wâˆ—).
(13.42)
By the deï¬nition (13.36) of the ML estimator and the differentiability of
LN(w), the left-hand side of Eq. (13.42) is equal to zero, i.e.,
âˆ‚LN(w)
âˆ‚w
w=wML = 0.
(13.43)
The ï¬rst term in the right-hand side of Eq. (13.42) can be written as
âˆ‚LN(w)
âˆ‚w
w=wâˆ—= 1
N
N

n=1
âˆ‚log p(x(n)|w)
âˆ‚w
w=wâˆ—
.
(13.44)
Since Eq. (13.40) and the differentiability of Lâˆ—(w) imply that
âˆ‚Lâˆ—(w)
âˆ‚w
w=wâˆ—=
/âˆ‚log p(x|w)
âˆ‚w

w=wâˆ—
0
p(x|wâˆ—)
= 0,
(13.45)
the right-hand side of Eq. (13.44) is the average over N i.i.d. samples of the
random variable
âˆ‚log p(x(n)|w)
âˆ‚w
w=wâˆ—
,
4 The mean value theorem states that, for a differentiable function f : [a, b] â†’R,
âˆƒc âˆˆ[a, b]
s.t.
d f(x)
dx
x=c = f(b)âˆ’f(a)
bâˆ’a
.

354
13 Asymptotic Learning Theory
which follows a distribution with zero mean (Eq. (13.45)) and the covariance
given by the Fisher information (13.2) at w = wâˆ—, i.e.,
F(wâˆ—) =
/âˆ‚log p(x|w)
âˆ‚w

w=wâˆ—
âˆ‚log p(x|w)
âˆ‚w

âŠ¤
w=wâˆ—
0
p(x|wâˆ—)
.
(13.46)
Therefore, according to the central limit theorem (Theorem 13.1), the distribu-
tion of the ï¬rst term in the right-hand side of Eq. (13.42) converges to
p
âˆ‚LN(w)
âˆ‚w

w=wâˆ—

â†’GaussD
âˆ‚LN(w)
âˆ‚w

w=wâˆ—; 0, Nâˆ’1F(wâˆ—)

.
(13.47)
The coefï¬cient of the second term in the right-hand side of Eq. (13.42)
satisï¬es
âˆ‚2LN(w)
âˆ‚wâˆ‚w
w= Â´w
= âˆ‚2Lâˆ—(w)
âˆ‚wâˆ‚wâŠ¤
w=wâˆ—
+ op(1),
(13.48)
because of the law of large numbers and the consistency of the ML estimator,
i.e., [min(wML, wâˆ—), max(wML, wâˆ—)] âˆ‹Â´w â†’wâˆ—since wML â†’wâˆ—. Furthermore,
the following relation holds under the regularity conditions (see Appendix
B.2):
âˆ‚2Lâˆ—(w)
âˆ‚wâˆ‚wâŠ¤
w=wâˆ—
=
/âˆ‚2 log p(x|w)
âˆ‚wâˆ‚wâŠ¤
w=wâˆ—
0
p(x|wâˆ—)
= âˆ’F(wâˆ—).
(13.49)
Substituting Eqs. (13.43), (13.48), and (13.49) into Eq. (13.42) gives

F(wâˆ—) + op(1)
 
(wML âˆ’wâˆ—) = âˆ‚LN(w)
âˆ‚w
w=wâˆ—.
(13.50)
Since the Fisher information is assumed to be invertible, Eq. (13.47) leads to
the following theorem:
Theorem 13.2
(Asymptotic normality of ML estimator) Under the regularity
conditions, the distribution of vML =
âˆš
N(wML âˆ’wâˆ—) converges to
p

vML 
â†’GaussD

vML; 0, Fâˆ’1(wâˆ—)
 
as
N â†’âˆ.
(13.51)
Theorem 13.2 implies that
wML = wâˆ—+ Op(Nâˆ’1/2).
(13.52)
13.4.3 Asymptotic Normality of the Bayes Posterior
The Bayes posterior can be written as follows:
p(w|X) =
exp $NLN(w) + log p(w)%

exp $NLN(w) + log p(w)% dw
.
(13.53)

13.4 Asymptotic Learning Theory for Regular Models
355
In the asymptotic limit, the factor exp(NLN(w)) dominates the numerator, and
the probability mass concentrates around the peak of LN(w)â€”the ML estimator
wML. The Taylor expansion of LN(w) about wML gives
LN(w) â‰ˆLN(wML) + (w âˆ’wML)âŠ¤âˆ‚LN(w)
âˆ‚w
w=wML
+ 1
2(w âˆ’wML)âŠ¤âˆ‚2LN(w)
âˆ‚wâˆ‚wâŠ¤
w=wML (w âˆ’wML)
â‰ˆLN(wML) âˆ’1
2(w âˆ’wML)âŠ¤F(wâˆ—)(w âˆ’wML),
(13.54)
where we used Eq. (13.43) and
âˆ‚2LN(w)
âˆ‚wâˆ‚wâŠ¤
w=wML = âˆ’F(wâˆ—) + op(1),
(13.55)
which is implied by the law of large numbers and the consistency of the
ML estimator. Eqs. (13.53) and (13.54) imply that the Bayes posterior can
be approximated by Gaussian in the asymptotic limit:
p(w|X) â‰ˆGaussD

w; wML, Nâˆ’1Fâˆ’1(wâˆ—)
 
.
The following theorem was derived with more accurate discussion.
Theorem 13.3
(Asymptotic normality of the Bayes posterior) (van der
Vaart, 1998) Under the regularity conditions, the (rescaled) Bayes posterior
distribution p (v|X) where v =
âˆš
N(w âˆ’wâˆ—) converges to
p (v|X) â†’GaussD

v; vML, Fâˆ’1(wâˆ—)
 
as
N â†’âˆ,
(13.56)
where vML =
âˆš
N(wML âˆ’wâˆ—).
Theorem 13.3 implies that
wMAP = wML + op(Nâˆ’1/2),
(13.57)
wBayes = âŸ¨wâŸ©p(w|X) = wML + op(Nâˆ’1/2),
(13.58)
which prove the consistency of the MAP estimator and the Bayesian estimator.
13.4.4 Generalization Properties
Now we analyze the generalization error and the training error in ML learning,
MAP learning, and Bayesian learning, as well as the Bayes free energy.
After that, we introduce information criteria for model selection, which were
developed based on the asymptotic behavior of those quantities.

356
13 Asymptotic Learning Theory
13.4.5 ML Learning
The generalization error of ML learning can be written as
GEML
Regular(X) =
/
log p(x|wâˆ—)
p(x|wML)
0
p(x|wâˆ—)
= Lâˆ—(wâˆ—) âˆ’Lâˆ—(wML)
(13.59)
with Lâˆ—(w) deï¬ned by Eq. (13.39). The Taylor expansion of the second term of
Eq. (13.59) about the true parameter wâˆ—gives
Lâˆ—(wML) = Lâˆ—(wâˆ—) + (wML âˆ’wâˆ—)âŠ¤âˆ‚Lâˆ—(w)
âˆ‚w
w=wâˆ—
+ 1
2(wML âˆ’wâˆ—)âŠ¤âˆ‚2Lâˆ—(w)
âˆ‚wâˆ‚wâŠ¤
w=wâˆ—
(wML âˆ’wâˆ—) + O(âˆ¥wML âˆ’wâˆ—âˆ¥3)
= Lâˆ—(wâˆ—) âˆ’1
2(wML âˆ’wâˆ—)âŠ¤F(wâˆ—)(wML âˆ’wâˆ—) + O(âˆ¥wML âˆ’wâˆ—âˆ¥3),
(13.60)
where we used Eqs. (13.45) and (13.49) in the last equality. Substituting Eq.
(13.60) into Eq. (13.59) gives
GEML
Regular(X) = 1
2(wML âˆ’wâˆ—)âŠ¤F(wâˆ—)(wML âˆ’wâˆ—) + O(âˆ¥wML âˆ’wâˆ—âˆ¥3). (13.61)
The asymptotic normality (Theorem 13.2) of the ML estimator implies that
âˆš
NF
1
2 (wâˆ—)(wML âˆ’wâˆ—)  GaussD (0, ID) ,
(13.62)
and that
O(âˆ¥wML âˆ’wâˆ—âˆ¥3) = Op(Nâˆ’3/2).
(13.63)
Eq. (13.62) implies that the distribution of s = N(wML âˆ’wâˆ—)âŠ¤F(wâˆ—)(wML âˆ’wâˆ—)
converges to the chi-squared distribution with D degrees of freedom:5
p (s) â†’Ï‡2 (s; D) ,
(13.64)
and therefore,
N

(wML âˆ’wâˆ—)âŠ¤F(wâˆ—)(wML âˆ’wâˆ—)

q(X) = D + o(1).
(13.65)
Eqs. (13.61), (13.63), and (13.65) lead to the following theorem:
5 The chi-squared distribution with D degrees of freedom is the distribution of the sum of the
squares of D i.i.d. samples drawn from Gauss1(0, 12). It is actually a special case of the Gamma
distribution, and it holds that Ï‡2(x; D) = Gamma(x; D/2, 1/2). The mean and the variance are
equal to D and 2D, respectively.

13.4 Asymptotic Learning Theory for Regular Models
357
Theorem 13.4
The average generalization error of ML learning in the
regular models can be asymptotically expanded as
GE
ML
Regular(N) =

GEML
Regular(X)

q(X) = Î»ML
RegularNâˆ’1 + o(Nâˆ’1),
(13.66)
where the generalization coefï¬cient is given by
2Î»ML
Regular = D.
(13.67)
Interestingly, the leading term of the generalization error only depends on
the parameter dimension or the degree of freedom of the statistical model.
The training error of ML learning can be analyzed in a similar fashion. It
can be written as
TEML
Regular(X) = Nâˆ’1
N

n=1
log p(x(n)|wâˆ—)
p(x(n)|wML)
= LN(wâˆ—) âˆ’LN(wML)
(13.68)
with LN(w) deï¬ned by Eq. (13.37). The Taylor expansion of the ï¬rst term of
Eq. (13.68) about the ML estimator wML gives
LN(wâˆ—) = LN(wML) + (wâˆ—âˆ’wML)âŠ¤âˆ‚LN(w)
âˆ‚w
w=wML
+ 1
2(wâˆ—âˆ’wML)âŠ¤âˆ‚2LN(w)
âˆ‚wâˆ‚wâŠ¤
w=wML (wâˆ—âˆ’wML) + O(âˆ¥wâˆ—âˆ’wMLâˆ¥3)
= LN(wML) âˆ’1
2(wâˆ—âˆ’wML)âŠ¤
F(wâˆ—) + op(1)
 
(wâˆ—âˆ’wML)
+ O(âˆ¥wâˆ—âˆ’wMLâˆ¥3),
(13.69)
where we used Eqs. (13.43) and (13.55). Substituting Eq. (13.69) into Eq.
(13.68) and applying Eq. (13.52), we have
TEML
Regular(X) = âˆ’1
2(wML âˆ’wâˆ—)âŠ¤F(wâˆ—)(wML âˆ’wâˆ—) + op(Nâˆ’1).
(13.70)
Thus, Eq. (13.70) together with Eq. (13.65) gives the following theorem:
Theorem 13.5
The average training error of ML learning in the regular
models can be asymptotically expanded as
TE
ML
Regular(N) =

TEML
Regular(X)

q(X) = Î½ML
RegularNâˆ’1 + o(Nâˆ’1),
(13.71)
where the training coefï¬cient is given by
2Î½ML
Regular = âˆ’D.
(13.72)

358
13 Asymptotic Learning Theory
Comparing Theorems 13.4 and 13.5, we see that the generalization coefï¬-
cient and the training coefï¬cient are antisymmetric with each other:
Î»ML
Regular = âˆ’Î½ML
Regular.
13.4.6 MAP Learning
We ï¬rst prove the following theorem:
Theorem 13.6
For any (point-) estimator such that
w = wML + op(Nâˆ’1/2),
(13.73)
it holds that
GEw
Regular(X) =
/
log p(x|wâˆ—)
p(x|w)
0
p(x|wâˆ—)
= GEML
Regular(X) + op(Nâˆ’1),
(13.74)
TEw
Regular(X) = Nâˆ’1
N

n=1
log p(x(n)|wâˆ—)
p(x(n)|w) = TEML
Regular(X) + op(Nâˆ’1).
(13.75)
Proof
The generalization error of the estimator w can be written as
GEw
Regular(X) =
/
log p(x|wâˆ—)
p(x|w)
0
p(x|wâˆ—)
= Lâˆ—(wâˆ—) âˆ’Lâˆ—(w)
= GEML
Regular(X) +

Lâˆ—(wML) âˆ’Lâˆ—(w)
 
,
(13.76)
where the second term can be expanded as
Lâˆ—(wML) âˆ’Lâˆ—(w) = âˆ’(w âˆ’wML)âŠ¤âˆ‚Lâˆ—(w)
âˆ‚w
w=wML
âˆ’1
2(w âˆ’wML)âŠ¤âˆ‚2Lâˆ—(w)
âˆ‚wâˆ‚wâŠ¤
w=wML(w âˆ’wML) +O(âˆ¥w âˆ’wMLâˆ¥3).
(13.77)
Eqs. (13.45) and (13.52) (with the differentiability of âˆ‚Lâˆ—(w)/âˆ‚w) imply that
âˆ‚Lâˆ—(w)
âˆ‚w
w=wML = âˆ‚Lâˆ—(w)
âˆ‚w
w=wâˆ—+Op(Nâˆ’1/2)
= Op(Nâˆ’1/2),
with which Eqs. (13.73) and (13.77) lead to
Lâˆ—(wML) âˆ’Lâˆ—(w) = op(Nâˆ’1).
Substituting the preceding into Eq. (13.76) gives Eq. (13.74).

13.4 Asymptotic Learning Theory for Regular Models
359
Similarly, the training error of the estimator w can be written as
TEw
Regular(X) = Nâˆ’1
N

n=1
log p(x(n)|wâˆ—)
p(x(n)|w)
= LN(wâˆ—) âˆ’LN(w)
= TEML
Regular(X) +

LN(wML) âˆ’LN(w)
 
,
(13.78)
where the second term can be expanded as
LN(wML) âˆ’LN(w) = âˆ’(w âˆ’wML)âŠ¤âˆ‚LN(w)
âˆ‚w
w=wML
âˆ’1
2(w âˆ’wML)âŠ¤âˆ‚2LN(w)
âˆ‚wâˆ‚wâŠ¤
w=wML(w âˆ’wML) +O(âˆ¥w âˆ’wMLâˆ¥3).
(13.79)
Eqs. (13.43), (13.73), and (13.79) lead to
LN(wML) âˆ’LN(w) = op(Nâˆ’1).
Substituting the preceding into Eq. (13.78) gives Eq. (13.75), which completes
the proof.
â–¡
Since the MAP estimator satisï¬es the condition (13.73) of Theorem 13.6
(see Eq. (13.57)), we obtain the following corollaries:
Corollary 13.7
The average generalization error of MAP learning in the
regular models can be asymptotically expanded as
GE
MAP
Regular(N) =

GEMAP
Regular(X)

q(X) = Î»MAP
RegularNâˆ’1 + o(Nâˆ’1),
(13.80)
where the generalization coefï¬cient is given by
2Î»MAP
Regular = D.
(13.81)
Corollary 13.8
The average training error of MAP learning in the regular
models can be asymptotically expanded as
TE
MAP
Regular(N) =

TEMAP
Regular(X)

q(X) = Î½MAP
RegularNâˆ’1 + o(Nâˆ’1),
(13.82)
where the training coefï¬cient is given by
2Î½MAP
Regular = âˆ’D.
(13.83)

360
13 Asymptotic Learning Theory
13.4.7 Bayesian Learning
Eq. (13.58) and Theorem 13.6 imply that the Bayesian estimator also gives
the same generalization and training coefï¬cients as ML learning, if the plug-
in predictive distribution p(x|wBayes), i.e., the model distribution with the
Bayesian parameter plugged-in, is used for prediction. We can show that
the proper Bayesian procedure with the predictive distribution p(x|X) =
âŸ¨p(x|w)âŸ©p(w|X) also gives the same generalization and training coefï¬cients.
We ï¬rst prove the following theorem:
Theorem 13.9
Let r(w) be a (possibly approximate posterior) distribution of
the parameter, of which the mean and the covariance satisfy the following:
w = âŸ¨wâŸ©r(w) = wâˆ—+ Op(Nâˆ’1/2),
(13.84)
Î£w =
*
w âˆ’âŸ¨wâŸ©r(w)
 
w âˆ’âŸ¨wâŸ©r(w)
 âŠ¤+
r(w) = Op(Nâˆ’1).
(13.85)
Then the generalization error and the training error of the predictive distribu-
tion p(x|X) = âŸ¨p(x|w)âŸ©r(w) satisfy
GEr
Regular(X) =
/
log
p(x|wâˆ—)
âŸ¨p(x|w)âŸ©r(w)
0
p(x|wâˆ—)
= GEw
Regular(X) + op(Nâˆ’1),
(13.86)
TEr
Regular(X) = Nâˆ’1
N

n=1
log
p(x(n)|wâˆ—)
p(x(n)|w)
r(w)
=TEw
Regular(X) + op(Nâˆ’1), (13.87)
where GEw
Regular(X) and TEw
Regular(X) are, respectively, the generalization error
and the training error of the point estimator w (deï¬ned in Theorem 13.6).
Proof
The predictive distribution can be expressed as
âŸ¨p(x|w)âŸ©r(w) = exp $log p(x|w)%
r(w)
=
*
exp

log p(x|w) + (w âˆ’w)âŠ¤âˆ‚log p(x|wâ€²)
âˆ‚wâ€²
wâ€²=w
+ 1
2(w âˆ’w)âŠ¤âˆ‚2 log p(x|wâ€²)
âˆ‚wâ€²âˆ‚wâ€²âŠ¤
wâ€²=w (w âˆ’w) + O

âˆ¥w âˆ’wâˆ¥3 "+
r(w)
= p(x|w) Â·
*!
1 + (w âˆ’w)âŠ¤âˆ‚log p(x|wâ€²)
âˆ‚wâ€²
wâ€²=w
+ 1
2(w âˆ’w)âŠ¤âˆ‚log p(x|wâ€²)
âˆ‚wâ€²
wâ€²=w
âˆ‚log p(x|wâ€²)
âˆ‚wâ€²

âŠ¤
wâ€²=w (w âˆ’w)
+ 1
2(w âˆ’w)âŠ¤âˆ‚2 log p(x|wâ€²)
âˆ‚wâ€²âˆ‚wâ€²âŠ¤
wâ€²=w (w âˆ’w) + O

âˆ¥w âˆ’wâˆ¥3 "+
r(w) .
Here we ï¬rst expanded log p(x|w) about w, and then expanded the exponential
function (with exp(z) = 1 + z + z2/2 + O(z3)).

13.4 Asymptotic Learning Theory for Regular Models
361
Using the conditions (13.84) and (13.85) on r(w), we have
âŸ¨p(x|w)âŸ©r(w) = p(x|w) Â·

1 + 1
2tr
Î£wÎ¦(x; w)
 
+ Op

Nâˆ’3/2  
,
(13.88)
where
Î¦(x; w) = âˆ‚log p(x|w)
âˆ‚w
w=w
âˆ‚log p(x|w)
âˆ‚w

âŠ¤
w=w + âˆ‚2 log p(x|w)
âˆ‚wâ€²âˆ‚wâ€²âŠ¤
w=w.
(13.89)
Therefore,
*
log
âŸ¨p(x|w)âŸ©r(w)
p(x|w)
+
p(x|wâˆ—) =

log

1 + 1
2tr
Î£wÎ¦(x; w)
 
+ Op

Nâˆ’3/2  
p(x|wâˆ—)
= 1
2

tr
Î£wÎ¦(x; w)
 
p(x|wâˆ—) + Op

Nâˆ’3/2 
.
(13.90)
Here we expanded the logarithm function (with log(1 + z) = z + O(z2)), using
the condition (13.85) on the covariance, i.e., Î£w = Op(Nâˆ’1).
The condition (13.84) on the mean, i.e., w = wâˆ—+ Op(Nâˆ’1/2), implies that
Î¦(x; w)
p(x|wâˆ—) = âŸ¨Î¦(x; wâˆ—)âŸ©p(x|wâˆ—) + Op(Nâˆ’1/2)
= F(wâˆ—) âˆ’F(wâˆ—) + Op(Nâˆ’1/2)
= Op(Nâˆ’1/2),
(13.91)
where we used the deï¬nition of the Fisher information (13.46) and its
equivalent expression (13.49) (under the regularity conditions). Eqs. (13.90)
and (13.91) together with the condition (13.85) give
*
log
âŸ¨p(x|w)âŸ©r(w)
p(x|w)
+
p(x|wâˆ—) = Op(Nâˆ’3/2),
which results in Eq. (13.86).
Similarly, by using the expression (13.88) of the predictive distribution, we
have
Nâˆ’1
N

n=1
log âŸ¨p(x(n)|w)âŸ©r(w)
p(x(n)|w)
= Nâˆ’1
N

n=1
log

1 + 1
2tr
Î£wÎ¦(x(n); w)
 
+ Op

Nâˆ’3/2  
= 1
2Nâˆ’1
N

n=1
tr
Î£wÎ¦(x(n); w)
 
+ Op

Nâˆ’3/2 
. (13.92)
The law of large numbers (13.5) and Eq. (13.91) lead to
Nâˆ’1
N

n=1
Î¦(x(n); w) = Î¦(x; w)
p(x|wâˆ—) + op(1)
= op(1).

362
13 Asymptotic Learning Theory
Substituting the preceding and the condition (13.85) into Eq. (13.92) gives
Nâˆ’1
N

n=1
log âŸ¨p(x(n)|w)âŸ©r(w)
p(x(n)|w)
= op

Nâˆ’1 
,
which results in Eq. (13.87). This completes the proof.
â–¡
The asymptotic normality of the Bayes posterior (Theorem 13.3), combined
with the asymptotic normality of the ML estimator (Theorem 13.2), guarantees
that the conditions (13.84) and (13.85) of Theorem 13.9 hold in Bayesian
learning, which leads to the following corollaries:
Corollary 13.10
The average generalization error of Bayesian learning in
the regular models can be asymptotically expanded as
GE
Bayes
Regular(N) =

GEBayes
Regular(X)

q(X) = Î»Bayes
RegularNâˆ’1 + o(Nâˆ’1),
(13.93)
where the generalization coefï¬cient is given by
2Î»Bayes
Regular = D.
(13.94)
Corollary 13.11
The average training error of Bayesian learning in the
regular models can be asymptotically expanded as
TE
Bayes
Regular(N) =

TEBayes
Regular(X)

q(X) = Î½Bayes
RegularNâˆ’1 + o(Nâˆ’1),
(13.95)
where the training coefï¬cient is given by
2Î½Bayes
Regular = âˆ’D.
(13.96)
Asymptotic behavior of the Bayes free energy (13.18) was also analyzed
(Schwarz, 1978; Watanabe, 2009). The Bayes free energy can be written as
FBayes(X) = âˆ’log p(X)
= âˆ’log

p(w)
N

n=1
p(x(n)|w)dw
= âˆ’log

exp $NLN(w) + log p(w)% dw,
where the factor exp (NLN(w)) dominates in the asymptotic limit. By using the
Taylor expansion
LN(w) = LN(wML) + (w âˆ’wML)âŠ¤âˆ‚LN(w)
âˆ‚w
w=wML

13.4 Asymptotic Learning Theory for Regular Models
363
+1
2(w âˆ’wML)âŠ¤âˆ‚2LN(w)
âˆ‚wâˆ‚wâŠ¤
w=wML (w âˆ’wML) + O

âˆ¥w âˆ’wMLâˆ¥3 
= LN(wML)âˆ’1
2(w âˆ’wML)âŠ¤
F(wâˆ—) + op(1)
 
(w âˆ’wML)+O

âˆ¥w âˆ’wMLâˆ¥3 
,
we can approximate the Bayes free energy as follows:
FBayes(X) â‰ˆâˆ’NLN(wML) âˆ’log

exp

âˆ’N
2 (w âˆ’wML)âŠ¤F(wâˆ—)(w âˆ’wML)
+ log p(w)

dw
= âˆ’NLN(wML) âˆ’log

exp

âˆ’1
2vâŠ¤F(wâˆ—)v
+ log p(wML + Nâˆ’1/2v)
 dv
ND/2
= âˆ’NLN(wML) + D
2 log N + Op(1).
(13.97)
where v =
âˆš
N(w âˆ’wML) is a rescaled parameter, on which the integration was
performed with dv = ND/2dw.
Therefore, the relative Bayes free energy (13.19) can be written as
FBayes(X) = FBayes(X) + NLN(wâˆ—)
â‰ˆD
2 log N + N

LN(wâˆ—) âˆ’LN(wML)
 
+ Op(1).
(13.98)
Here we used S N(X) = âˆ’LN(wâˆ—), which can be conï¬rmed by their deï¬nitions
(13.20) and (13.37). The second term in Eq. (13.98) is of the order of Op(1),
because Eqs. (13.68), (13.70), and (13.52) imply that
LN(wâˆ—) âˆ’LN(wML) = TEML
Regular(X) = Op(Nâˆ’1).
The following theorem was obtained with more rigorous discussion.
Theorem 13.12
(Watanabe, 2009) The relative Bayes free energy for the
regular models can be asymptotically expanded as
FBayes
Regular(X) = FBayes(X) âˆ’NS N(X) = Î»â€²Bayes
Regular log N + Op(1),
(13.99)
where the Bayes free energy coefï¬cient is given by
2Î»â€²Bayes
Regular = D.
(13.100)
Note that Corollary 13.10 and Theorem 13.12 are consistent with Eq.
(13.25), which holds for any statistical model.

364
13 Asymptotic Learning Theory
13.4.8 Information Criteria
We have seen that the leading terms of the generalization error, the training
error, and the relative Bayes free energy are proportional to the parameter
dimension. Those results imply that how much a regular statistical model
overï¬ts training data mainly depends on the degrees of freedom of statistical
models. Based on this insight, various information criteria were proposed for
model selection.
Let us ï¬rst recapitulate the model selection problem. Consider a (D âˆ’1)-
degree polynomial regression model for one-dimensional input t and output y:
y =
D

d=1
wdtdâˆ’1 + Îµ,
where Îµ denotes a noise. This model can be written as
y = wâŠ¤x + Îµ,
where w âˆˆRD is a parameter vector, and x = (1, t, t2,. . . , tDâˆ’1)âŠ¤is a
transformed input vector. Suppose that the true distribution can be realized
just with a (Dâˆ—âˆ’1)-degree polynomial:
y =
Dâˆ—

d=1
wâˆ—
dtdâˆ’1 + Îµ = wâˆ—âŠ¤xâ€² + Îµ,
where wâˆ—âˆˆRDâˆ—is the true parameter vector, and xâ€² = (1, t, t2,. . . , tDâˆ—âˆ’1)âŠ¤.6
If we train a (D âˆ’1)-degree polynomial model for D < Dâˆ—, we expect poor
generalization performance because the true distribution is not realizable, i.e.,
the model is too simple to express the true distribution. On the other hand, it
was observed that if we train a model such that D â‰«Dâˆ—, the generalization
performance is also not optimal, because the unnecessarily high degree terms
cause overï¬tting. Accordingly, ï¬nding an appropriate degree D of freedom,
based on the observed data, is an important task, which is known as a model
selection problem.
It would be a good strategy if we could choose D, which minimizes the
generalization error (13.30). Ignoring the terms that do not depend on the
model (or D), the generalization error can be written as
GE(D) = âˆ’

q(x)q(y|x) log p(y|x, D)dxdy + const.
(13.101)
6 By â€œjust,â€ we mean that wâˆ—
Dâˆ— 0, and therefore the true distribution is not realizable with any
(D âˆ’1)-degree polynomial for D < Dâˆ—.

13.4 Asymptotic Learning Theory for Regular Models
365
Unfortunately, we cannot directly evaluate Eq. (13.101), since the true distri-
bution q(y|x) is inaccessible. Instead, the training error (13.31),
TE(D) = âˆ’1
N
N

n=1
log p(y(n)|x(n), D) + const.,
(13.102)
is often used as an estimator for the generalization error. Although Eq.
(13.102) is accessible, the training error is known to be a biased estimator for
the generalization error (13.101). In fact, the training error does not reï¬‚ect
the negative effect of redundancy of the statistical model, and tends to be
monotonically decreasing as the parameter dimension D increases.
Akaikeâ€™s information criterion (AIC) (Akaike, 1974),
AIC = âˆ’2
N

n=1
log p(y(n)|x(n), wML) + 2D,
(13.103)
was proposed as an estimator for the generalization error of ML learning
with bias correction. Theorems 13.4 and 13.5 provide the bias between the
generalization error and the training error as follows:

GEML
Regular(D) âˆ’TEML
Regular(D)

q(D) = GE
ML
Regular(N) âˆ’TE
ML
Regular(N)
=
Î»ML
Regular âˆ’Î½ML
Regular
N
+ o(Nâˆ’1)
= D
N + o(Nâˆ’1).
(13.104)
Therefore, it holds that
TEML
Regular(D) +

GEML
Regular(D) âˆ’TEML
Regular(D)

q(D)
= TEML
Regular(D) + D
N + o(Nâˆ’1)
= AIC
2N âˆ’S N(Y|X) + o(Nâˆ’1),
(13.105)
where S N(Y|X) is the (conditional) empirical entropy (13.34). Since the
empirical entropy S N(Y|X) does not depend on the model, Eq. (13.105) implies
that minimizing AIC amounts to minimizing an asymptotically unbiased
estimator for the generalization error.
Another strategy for model selection is to minimize an approximation to the
Bayes free energy (13.33). Instead of performing integration for computing

366
13 Asymptotic Learning Theory
the Bayes free energy, Schwarz (1978) proposed to minimize the Bayesian
information criterion (BIC):
BIC = MDL = âˆ’2
N

n=1
log p(y(n)|x(n), wML) + D log N.
(13.106)
Interestingly, an equivalent criterion, called the minimum description length
(MDL) (Rissanen, 1986), was derived in the context of information theory in
communication. The relation between BIC and the Bayes free energy can be
directly found from the approximation (13.97), i.e., it holds that
FBayes(Y|X) â‰ˆâˆ’NLN(wML) + D
2 log N + Op(1)
= BIC
2
+ Op(1),
(13.107)
and therefore minimizing BIC amounts to minimizing an approximation to the
Bayes free energy.
The ï¬rst terms of AIC (13.103) and BIC (13.106) are the maximum log-
likelihoodâ€”the log-likelihood at the ML estimatorâ€”multiplied by âˆ’2. The
second terms, called penalty terms, penalize high model complexity, which
explicitly work as Occamâ€™s razor (MacKay, 1992) to prune off irrelevant
degrees of freedom of the statistical model. AIC, BIC, and MDL are easily
computable and have shown their usefulness in many applications. However,
their derivations rely on the fact that the generalization coefï¬cient, the training
coefï¬cient, and the free energy coefï¬cient depend only on the parameter
dimension under the regularity conditions. Actually, it has been revealed
that, in singular models, those coefï¬cients depend not only on the parameter
dimension D but also on the true distribution.
13.5 Asymptotic Learning Theory for Singular Models
Many popular statistical models do not satisfy the regularity conditions.
For example, neural networks, matrix factorization, mixture models, hidden
Markov models, and Bayesian networks are all unidentiï¬able and have
singularities, where the Fisher information is singular, in the parameter space.
As discussed in Chapter 7, the true parameter is on a singular point when the
true distribution is realizable with a model with parameter dimension smaller
than the used model, i.e., when the model has redundant components for
expressing the true distribution. In such cases, the likelihood cannot be Taylor-
expanded about the true parameter, and the asymptotic normality does not hold.

13.5 Asymptotic Learning Theory for Singular Models
367
Consequently, the regular learning theory, described in Section 13.4, cannot be
applied to singular models.
In this section, we ï¬rst give intuition on how singularities affect general-
ization properties, and then introduce asymptotic theoretical results on ML
learning and Bayesian learning. After that, we give an overview of asymptotic
theory of VB learning, which will be described in detail in the subsequent
chapters.
13.5.1 Effect of Singularities
Two types of effects of singularities have been observed, which will be detailed
in the following subsections.
Basis Selection Effect
Consider a regression model for one-dimensional input x âˆˆ[âˆ’10, 10] and
output y âˆˆR with H radial basis function (RBF) units:
p(y|x, a, b, c) =
1
âˆš
2Ï€Ïƒ2 exp

âˆ’1
2Ïƒ2 (y âˆ’f(x; a, b, c))2

,
(13.108)
where
f(x; a, b, c) =
H

h=1
Ïh

x; ah, bh, c2
h
 
.
(13.109)
Each RBF unit in Eq. (13.109) is a weighted Gaussian RBF,
Ïh

x; ah, bh, c2
h
 
= ah Â· Gauss1

x; bh, c2
h
 
=
ah
.
2Ï€c2
h
exp
â›âœâœâœâœââˆ’(x âˆ’bh)2
2c2
h
ââŸâŸâŸâŸâ ,
controlled by a weight parameter ah âˆˆR, a mean parameter bh âˆˆR, and
a scale parameter c2
h âˆˆR++. Treating the noise variance Ïƒ2 in Eq. (13.108)
as a known constant, the parameters to be estimated are summarized as w =
(aâŠ¤, bâŠ¤, câŠ¤)âŠ¤âˆˆR3H, where a = (a1,. . . , aH)âŠ¤âˆˆRH, b = (b1,. . . , bH)âŠ¤âˆˆRH,
and c = (c2
1,. . . , c2
H)âŠ¤âˆˆRH
++. Figure 13.4(a) shows an example of the RBF
regression function (13.109) for H = 2.
Apparently, the model (13.108) is unidentiï¬able, and has singularitiesâ€”
since Ïh(x; 0, bh, c2
h) = 0 for any bh âˆˆR, c2
h âˆˆR++, the (bh, c2
h) half-space at
ah = 0 is an unidentiï¬able set, on which the Fisher information is singular
(see Figure 13.5).7 Accordingly, we call the model (13.108) a singular RBF
regression model, of which the parameter dimension is equal to Dsinâˆ’RBF = 3H.
7 More unidentiï¬able sets can exist, depending on the other RBF units. See Section 7.3.1 for
details on identiï¬ability.

368
13 Asymptotic Learning Theory
â€“10
â€“5
0
5
10
â€“1.5
â€“1
â€“0.5
0
0.5
1
1.5
(a) Singular RBF with H = 2 units.
â€“10
â€“5
0
5
10
â€“1.5
â€“1
â€“0.5
0
0.5
1
1.5
(b) Regular RBF with H = 6 units.
Figure 13.4 Examples (solid curves) of the singular RBF regression function
(13.109) and the regular RBF regression function (13.111). Each RBF unit Ïh(x)
is depicted as a dashed curve.
Figure 13.5 Singularities of the RBF regression model (13.108).
Let us consider another RBF regression model
p(y|x, a) =
1
âˆš
2Ï€Ïƒ2 exp

âˆ’1
2Ïƒ2 (y âˆ’f(x; a))2

,
(13.110)
where
f(x; a) =
H

h=1
Ë˜Ïh (x; ah) =
H

h=1
Ïh

x; ah, Ë˜bh, Ë˜c2
h
 
.
(13.111)
Unlike the singular RBF model (13.108), we here treat the mean parameters
Ë˜b = (Ë˜b1,. . . , Ë˜bH)âŠ¤and the scale parameters Ë˜c = (Ë˜c2
1,. . . , Ë˜c2
H)âŠ¤as ï¬xed
constants, and only estimate the weight parameters a = (a1,. . . , aH)âŠ¤âˆˆRH.

13.5 Asymptotic Learning Theory for Singular Models
369
Let us set the means and the scales as follows, so that the model covers the
input domain [âˆ’10, 10]:
Ë˜bh = âˆ’10 + 20 Â· h âˆ’1
H âˆ’1,
(13.112)
Ë˜c2
h = 1.
(13.113)
Figure 13.4(b) shows an example of the RBF regression function (13.111) for
H = 6. Clearly, it holds that Ë˜Ïh(x; ah)  Ë˜Ïh(x; aâ€²
h) if ah  aâ€²
h, and therefore the
model is identiï¬able. The other regularity conditions (summarized in Section
13.4.1) on the model distribution p(y|x, a) are also satisï¬ed. Accordingly,
we call the model (13.110) a regular RBF regression model, of which the
parameter dimension is equal to Dregâˆ’RBF = H.
Now we investigate difference in learning behavior between the singular
RBF model (13.108) and the regular RBF model (13.110). Figure 13.6 shows
trained regression functions (by ML learning) from N = 50 samples (shown as
crosses) generated from the regression model,
q(y|x) =
1
âˆš
2Ï€Ïƒ2 exp

âˆ’1
2Ïƒ2 (y âˆ’f âˆ—(x))2

,
(13.114)
with the following true functions:
(i) poly: Polynomial function f âˆ—(x) = âˆ’0.002x3.
(ii) cos: Cosine function f âˆ—(x) = cos(0.5x).
(iii) tanh: Tangent hyperbolic function f âˆ—(x) = tanh(âˆ’0.5x).
(iv) sin-sig: Sine times sigmoid function f âˆ—(x) = sin(x) Â·
1
1+eâˆ’x .
(v) sin-alg: Sine function aligned for the regular model f âˆ—(x) = sin(2Ï€ 9
70 x).
(vi) rbf: Single RBF function f âˆ—(x) = Ï1 (x; 3, âˆ’10, 1).
The noise variance is set to Ïƒ2 = 0.01, and assumed to be known. We set
the number of RBF units to H = 2 for the singular model, and H = 6 for
the regular model, so that both models have the same degrees of freedom,
Dsinâˆ’RBF = Dregâˆ’RBF = 6.
In Figure 13.6, we can observe the following: the singular RBF model can
ï¬‚exibly ï¬t functions in different shapes (a) through (d), unless the function has
too many peaks (e); the regular RBF model is not as ï¬‚exible as the singular
RBF model (a) through (d), unless the peaks and valleys match the predeï¬ned
means of the RBF units (e). Actually, the frequency of sin-alg is aligned
so that the peaks and the valleys match Eq. (13.112). These observations
are quantitatively supported by the generalization error and the training error
shown in Figure 13.7, leaving us an impression that the singular RBF model

370
13 Asymptotic Learning Theory
â€“10
â€“5
0
5
10
â€“3
â€“2
â€“1
0
1
2
3
True
Singular
Regular
(a) poly
â€“10
â€“5
0
5
10
â€“3
â€“2
â€“1
0
1
2
3
True
Singular
Regular
(b) cos
â€“10
â€“5
0
5
10
â€“1.5
â€“1
â€“0.5
0
0.5
1
1.5
True
Singular
Regular
(c) tanh
â€“10
â€“5
0
5
10
â€“3
â€“2
â€“1
0
1
2
3
True
Singular
Regular
(d) sin-sig
â€“10
â€“5
0
5
10
â€“3
â€“2
â€“1
0
1
2
3
True
Singular
Regular
(e) sin-alg
â€“10
â€“5
0
5
10
â€“3
â€“2
â€“1
0
1
2
3
True
Singular
Regular
(f) rbf
Figure 13.6 Trained regression functions by the singular RBF model (13.108)
with H = 2 RBF units, and the regular RBF model (13.110) with H = 6 RBF
units.
with two modiï¬able basis functions is more ï¬‚exible than the regular RBF
model with six preï¬xed basis functions.
However, ï¬‚exibility is granted at the risk of overï¬tting to noise, which
can be observed in Figure 13.6(f). We can see that the true RBF function at
x = âˆ’10 is captured by both models. However, the singular RBF model shows
a small valley around x = 8, which is a consequence of overï¬tting to sample
noise. Figure 13.7 also shows that, in the rbf case, the singular RBF model
gives lower training error and higher generalization error than the regular RBF
modelâ€”typical behavior when overï¬tting occurs. This overï¬tting tendency is
reï¬‚ected to the generalization and the training coefï¬cients.

13.5 Asymptotic Learning Theory for Singular Models
371
0
0.1
0.2
0.3
poly
cos
tanh
sin-sig
sin-alg
rbf
Singular
Regular
(a) Generalization error
0
0.1
0.2
0.3
poly
cos
tanh
sin-sig
sin-alg
rbf
Singular
Regular
(b) Training error
Figure 13.7 The generalization error and the training error by the singular RBF
model and the regular RBF model.
Apparently, if the true function is realizable, i.e., âˆƒwâˆ—s.t.f âˆ—(x) = f(x; wâˆ—),
the true distribution (13.114) is realizable by the RBF regression model (Eq.
(13.108) or (13.110)). In the examples (a) through (e) in Figure 13.7, the
true function is not realizable by the RBF regression model. In such cases,
the generalization error and the training error do not converge to zero, and it
holds that GE(N) = Î˜(1) and TE(N) = Î˜(1) for the best learning algorithm.
On the other hand, the true function (f) consists of a single RBF unit, and
furthermore its mean and variance match those of the ï¬rst unit of the regular
RBF model (see Eqs. (13.112) and (13.113)). Accordingly, the true function
(f) and therefore the true distribution (13.114) in the example (f) are realizable,
i.e., âˆƒwâˆ—, s.t.q(y|x) = p(y|x, wâˆ—), by both of the singular RBF model (13.108)
and the regular RBF model (13.110).
When the true parameter wâˆ—exists, the generalization error converges to
zero, and, for any reasonable learning algorithm, the average generalization
error and the average training error can be asymptotically expanded as Eqs.
(13.15) and (13.16):
GE(N) = Î»Nâˆ’1 + o(Nâˆ’1),
TE(N) = Î½Nâˆ’1 + o(Nâˆ’1).
Since the regular RBF model (13.110) is regular, its generalization coefï¬cient
and the training coefï¬cient are given by
2Î»regâˆ’RBF = âˆ’2Î½regâˆ’RBF = D = Hregâˆ’RBF
(13.115)

372
13 Asymptotic Learning Theory
for ML learning, MAP learning, and Bayesian learning (under the additional
regularity conditions on the prior). On the other hand, the generalization
coefï¬cient and the training coefï¬cient for the singular RBF model (13.108)
are unknown and can be signiï¬cantly different from the regular models. As
will be introduced in Section 13.5.3, the generalization coefï¬cients of ML
learning and MAP learning for various singular models have been clariï¬ed,
and all results that have been found so far satisfy
2Î»ML
Singuler â‰¥D,
2Î»MAP
Singuler â‰¥D,
(13.116)
where D is the parameter dimensionality. By comparing Eq. (13.116) with
Eq. (13.115) (or Eqs. (13.67) and (13.81)), we ï¬nd that the ML and the MAP
generalization coefï¬cients per single model parameter in singular models are
larger than those in the regular models, which implies that singular models
tend to overï¬t more than the regular models.
We can explain this phenomenon as an effect of the neighborhood structure
around singularities. Recall the example (f), where the singular RBF model and
the regular RBF model learn the true distribution f âˆ—(x) = Ï1 (x; 3, âˆ’10, 1). For
the singular RBF model, wâˆ—
sinâˆ’RBF = (a1, a2, b1, b2, c2
1, c2
2) = (3, 0, âˆ’10, âˆ—, 1, âˆ—),
where âˆ—allows any value in the domain, are possible true parameters, while,
for the regular RBF model, wâˆ—
sinâˆ’RBF = (a1, a2,. . . , a6) = (3, 0,. . . , 0) is the
unique true parameter. Figure 13.8(a) shows the space of the three parameters
(a2, b2, c2
2) of the second RBF unit of the singular RBF model, in which the
true parameter is on the singularities. Since the true parameter extends over
the two-dimensional half-space {(b2, c2
2); b2 âˆˆR, c2
2 âˆˆR++}, the neighborhood
(shown by small arrows) contains any RBF with adjustable mean and variance.
Although the estimated parameter converges to the singularities in the asymp-
totic limit, ML learning on ï¬nite training samples tries to ï¬t the noise, which
contaminates the training samples, by selecting the optimal basis function,
where the optimality is in terms of the training error. On the other hand, Figure
13.8(b) shows the parameter space (ah, Ë˜bh, Ë˜c2
h) for h = 2,. . . , 4. For each h, the
true distribution corresponds to a single point, indicated by a shadowed circle,
and its neighborhood extends only in one direction, i.e., ah = 0 Â± Îµ with a
preï¬xed RBF basis speciï¬ed by the constants (Ë˜bh, Ë˜c2
h). Consequently, with the
same number of redundant parameters as the singular RBF model, ML learning
tries to ï¬t the training noise only with those three basis functions.
Although the probability that the three preï¬xed basis functions can ï¬t
the training noise better than a single ï¬‚exible basis function is not zero, we
would expect that the singular RBF model would likely capture the noise
more ï¬‚exibly than the regular RBF model. This intuition is supported by
previous theoretical work that showed Eq. (13.116) in many singular models,

13.5 Asymptotic Learning Theory for Singular Models
373
(a) Singular RBF
(b) Regular RBF
Figure 13.8 Neighborhood of the true distribution in the rbf example. (a) The
parameter space of the second (h = 2) RBF unit of the singular RBF model.
The true parameter is on the singularities, of which the neighborhood contains
any RBF with adjustable mean and variance. (b) The parameter space of the
second to the fourth (h = 2,. . . , 4) RBF units of the regular RBF model. With
the same degrees of freedom as a single singular RBF unit, the neighborhood of
the true parameter contains only three different RBF bases with preï¬xed means
and variances.
as well as the numerical example in Figure 13.6. We call this phenomenon,
i.e., singular models tending to overï¬t more than regular models, the basis
selection effect. Although Eq. (13.116) was shown for ML learning and MAP
learning, the basis selection effect should occur for any reasonable learning
algorithms, including Bayesian learning. However, in Bayesian learning, this
effect is canceled by the other effect of singularities, which is explained in the
following subsection.
Integration Effect
Assume that, in Bayesian learning with a singular model, we adopt a prior
distribution p(w) bounded as 0 < p(w) < âˆat any w âˆˆW. This assumption
is the same as one of the regularity conditions in Section 13.4.1. However, this
assumption excludes the use of the Jeffreys prior (see Appendix B) and positive
mass is distributed over singularities. As discussed in detail in Chapter 7, this
prior choice leads to nonuniformity of the volume element and favors models
with smaller degrees of freedom, if a learning algorithm involving integral
computations in the parameter space is adopted. As a result, singularities
induce MIR in Bayesian learning and its approximation methods, e.g., VB
learning. Importantly, the integration effect does not occur in point estimation
methods, including ML learning and MAP learning, since the nonuniformity of

374
13 Asymptotic Learning Theory
the volume element affects the estimator only through integral computations.
We call this phenomenon the integration effect of singularities.
The basis selection effect and the integration effect inï¬‚uence the learning
behavior in the opposite way: the former intensiï¬es overï¬tting, while the
latter suppresses it. A question is which is stronger in Bayesian learning.
Singular learning theory, which will be introduced in Section 13.5.4, has
already answered this question. The following has been shown for any singular
models:
2Î»Bayes
Singuler â‰¤D.
(13.117)
Comparing Eq. (13.117) with Eq. (13.115) (or Eq. (13.94)), we ï¬nd that the
Bayes generalization coefï¬cient per single model parameter in the singular
models is smaller than that in the regular models. Note that the conclusion is
opposite to ML learning and MAP learningâ€”singular models overï¬t training
noise more than the regular models in ML learning and MAP learning, while
they less overï¬t in Bayesian learning. Since the basis selection effect should
occur in any reasonable learning algorithm, we can interpret Eq. (13.117) as
evidence that the integration effect is stronger than the basis selection effect in
Bayesian learning.
One might wonder why Bayesian learning is not analyzed with the Jeffreys
priorâ€”the parameterization invariant noninformative prior. Actually, the Jef-
freys prior, or other prior distribution with zero mass at the singularities, is
rarely used in singular models because of the computational reasons: when the
computational tractability relies on the (conditional) conjugacy, the Jefferey
prior is out of choice in singular models; when some sampling method is used
for approximating the Bayes posterior, the diverging outskirts of the Jeffreys
prior prevents the sampling sequence to converge. Note that this excludes the
empirical Bayesian procedure, where the prior can be collapsed after training.
Little is known about the learning behavior of empirical Bayesian learning in
singular models, and the asymptotic learning theory part (Part IV) of this book
also excludes this case.
In the following subsections, we give a brief summary of theoretical results
that revealed learning properties of singular models.
13.5.2 Conditions Assumed in Asymptotic Theory
for Singular Models
Singular models were analyzed under the following conditions on the true
distribution and the prior distribution:

13.5 Asymptotic Learning Theory for Singular Models
375
(i) The true distribution is realizable by the statistical model, i.e.,
âˆƒwâˆ—s.t. q(x) = p(x|wâˆ—).
(ii) The prior p(w) is twice differentiable and bounded as 0 < p(w) < âˆat
any w âˆˆW.
Under the second condition, the prior choice does not affect the generalization
coefï¬cient. Accordingly, the results, introduced in the following subsection,
for ML learning can be directly applied to MAP learning.
13.5.3 ML Learning and MAP Learning
Fukumizu (1999) analyzed the asymptotic behavior of the generalization error
of ML learning for the reduced rank regression (RRR) model (3.36), by
applying the random matrix theory to evaluate the singular value distribution
of the ML estimator. Speciï¬cally, the large-scale limit where the dimensions
of the input and the output are inï¬nitely large was considered, and the exact
generalization coefï¬cient was derived. The training coefï¬cient can be obtained
in the same way (Nakajima and Watanabe, 2007).
The Gaussian mixture model (GMM) (4.6) has been studied as a prototype
of singular models in the case of ML learning. Akaho and Kappen (2000)
showed that the generalization error and the training error behave quite
differently from regular models. As deï¬ned in Eq. (13.12), âˆ’NÂ·TEML(X) is the
log-likelihood ratio, which asymptotically follows the chi-squared distribution
for regular models, while little is known about its behavior for singular models.
In fact, it is conjectured for the spherical GMM (4.6) that the log-likelihood
ratio diverges to inï¬nity in the order of log log N (Hartigan, 1985). For
mixture models with discrete components such as binomial mixture models,
the asymptotic distribution of the log-likelihood ratio was studied through
the distribution of the maximum of the Gaussian random ï¬eld (Bickel and
Chernoff, 1993; Takemura and Kuriki, 1997; Kuriki and Takemura, 2001).
Based on the idea of locally conic parameterization (Dacunha-Castelle and
Gassiat, 1997), the asymptotic behaviors of the log-likelihood ratio in some
singular models were analyzed. For some mixture models with continuous
components, including GMMs, it can be proved that the log-likelihood ratio
diverges to inï¬nity as N â†’âˆ. In neural networks, it is known that the log-
likelihood ratio diverges in the order of log N when there are at least two
redundant hidden units (Fukumizu, 2003; Hagiwara and Fukumizu, 2008).
In all previous works, the obtained generalization coefï¬cient or its equiva-
lent satisï¬es Eq. (13.116).

376
13 Asymptotic Learning Theory
13.5.4 Singular Learning Theory for Bayesian Learning
For analyzing the generalization performance of Bayesian learning, a general
approach, called the singular learning theory (SLT), was established, based on
the mathematical techniques in algebraic geometry (Watanabe, 2001a, 2009).
The average relative Bayes free energy (13.23),
F
Bayes(N) =
/
log
N
n=1 q(x(n))

p(w) N
n=1 p(x(n)|w)dw
0
q(X)
= âˆ’
/
log

exp

âˆ’N log q(x)
p(x|w)

Â· p(w)dw
0
q(x)
,
can be approximated as
F
Bayes(N) â‰ˆâˆ’log

exp (âˆ’NE(w)) Â· p(w)dw,
(13.118)
where
E(w) =
/
log q(x)
p(x|w)
0
q(x)
(13.119)
is the KL divergence between the true distribution q(x) and the model
distribution p(x|w).8
Let us see the KL divergence (13.119) as the energy in physics, and deï¬ne
the state density function for the energy value s > 0:
v(s) =

Î´(s âˆ’E(w)) Â· p(w)dw,
(13.120)
where Î´(Â·) is the Dirac delta function (located at the origin). Note that the
state density (13.120) and the (approximation to the relative) Bayes free energy
(13.118) are connected by the Laplace transform:
F
Bayes(N) = âˆ’log

exp(âˆ’s)v
! s
N
" ds
N .
(13.121)
Deï¬ne furthermore the zeta function as the Mellin transform, an extension of
the Laplace transform, of the state density (13.120):
Î¶(z) =

szv(s)ds =

E(w)zp(w)dw.
(13.122)
The zeta function (13.122) is a function of a complex number z âˆˆC, and it was
proved that all the poles of Î¶(z) are real, negative, and rational numbers.
8 It holds that F
Bayes(N) = âˆ’log

exp (âˆ’NE(w)) Â· p(w)dw + O(1) if the support of the prior is
compact (Watanabe, 2001a, 2009).

13.5 Asymptotic Learning Theory for Singular Models
377
By using the relations through Laplace/Mellin transform among the free
energy (13.118), the state density (13.120), and the zeta function (13.122),
Watanabe (2001a) proved the following theorem:
Theorem 13.13
(Watanabe, 2001a, 2009) Let 0 > âˆ’Î»1 > âˆ’Î»2 > Â· Â· Â· be the
sequence of the poles of the zeta function (13.122) in the decreasing order, and
m1, m2,. . . be the corresponding orders of the poles. Then the average relative
Bayes free energy (13.119) can be asymptotically expanded as
F
Bayes(N) = Î»1 log N âˆ’(m1 âˆ’1) log log N + O(1).
(13.123)
Let c(N) = F
Bayes(N) âˆ’Î»1 log N + (m1 âˆ’1) log log N be the O(1) term in
Eq. (13.123). The relation (13.24) between the generalization error and the free
energy leads to the following corollary:
Corollary 13.14
(Watanabe, 2001a, 2009) If c(N + 1) âˆ’c(N) = o

1
N log N
 
,
the average generalization error (13.13) can be asymptotically expanded as
GE
Bayes(N) = Î»1
N âˆ’m1 âˆ’1
N log N + o

1
N log N

.
(13.124)
To sum up, ï¬nding the maximum pole Î»1 of the zeta function Î¶(z) gives the
Bayes free energy coefï¬cient
Î»â€²Bayes = Î»1,
which is equal to the Bayes generalization coefï¬cient
Î»Bayes = Î»1.
Note that Theorem 13.13 and Corollary 13.14 hold both for regular and
singular models. As discussed in Section 7.3.2, MIR (or the integration effect
of singularities) is caused by strong nonuniformity of the density of the
volume element. Since the state density (13.120) reï¬‚ects the strength of the
nonuniformity, one can see that ï¬nding the maximum pole of Î¶(z) amounts to
ï¬nding the strength of the nonuniformity at the most concentrated point.
Some general inequalities were proven (Watanabe, 2001b, 2009):
â€¢ If the prior is positive at any singular point, i.e.,
p(w) > 0, âˆ€w âˆˆ{w; det (F(w)) = 0}, then
2Î»â€²Bayes = 2Î»Bayes â‰¤D.
(13.125)
â€¢ If the Jeffreys prior (see Appendiex B.4) is adopted, for which p(w) = 0
holds at any singular point, then
2Î»â€²Bayes = 2Î»Bayes â‰¥D.
(13.126)

378
13 Asymptotic Learning Theory
Some cases have been found where 2Î»â€²Bayes = 2Î»Bayes are strictly larger
than D.
These results support the discussion in Section 13.5.1 on the two effects of
singularities: Eq. (13.125) implies that the integration effect dominates the
basis selection effect in Bayesian learning, and Eq. (13.126) implies that the
basis selection effect appears also in Bayesian learning if the integration effect
is suppressed by using the Jeffreys prior.
Theorem 13.13 and Corollary 13.14 hold for general statistical models,
while they do not immediately tell us learning properties of singular models.
This is because ï¬nding the maximum pole of the zeta function Î¶(z) is not an
easy task, and requires a speciï¬c technique in algebraic geometry called the
resolution of singularities. Good news is that, when any pole larger than âˆ’D/2
is found, it provides an upper bound of the generalization coefï¬cient and thus
guarantees the performance with a tighter bound (Theorem 13.13 implies that
the larger the found pole is, the tighter the provided bound is).
For the RRR model (Aoyagi and Watanabe, 2005) and for the GMM
(Aoyagi and Nagata, 2012), the maximum pole was found for general cases,
and therefore the exact value of the free energy coefï¬cient, as well as the
generalization coefï¬cient, was obtained. In other singular models, including
neural networks (Watanabe, 2001a), mixture models (Yamazaki and Watanabe,
2003a), hidden Markov models (Yamazaki and Watanabe, 2005), and Bayesian
networks (Yamazaki and Watanabe, 2003b; Rusakov and Geiger, 2005), upper-
bounds of the free energy coefï¬cient were obtained by ï¬nding some poles
of the zeta function. An effort has been made to perform the resolution
of singularities systematically by using the newton diagram (Yamazaki and
Watanabe, 2004).
13.5.5 Information Criteria for Singular Models
The information criteria introduced in Section 13.4.8 rely on the learn-
ing theory under the regularity conditions. Therefore, although they were
sometimes applied for model selection in singular models, their relations to
generalization properties, e.g., AIC to the ML generalization error, and BIC to
the Bayes free energy, do not generally hold. In the following, we introduce
information criteria applicable for general statistical models including the
regular and the singular models (Watanabe, 2009, 2010, 2013). They also cover
a generalization of Bayesian learning.
Consider a learning method, called generalized Bayesian learning, based
on the generalized posterior distribution,

13.5 Asymptotic Learning Theory for Singular Models
379
p(Î²)(w|X) =
p(w) N
n=1
,
p(x(n)|w)
-Î²

p(w) N
n=1
6p(x(n)|w)7Î² dw
,
(13.127)
where Î², called the inverse temperature parameter, modiï¬es the importance of
the likelihood per training sample. The prediction is made by the generalized
predictive distribution,
p(Î²)(x|X) = âŸ¨p(x|w)âŸ©p(Î²)(w|X) .
(13.128)
Generalized Bayesian learning covers both Bayesian learning and ML learning
as special cases: when Î² = 1, the generalized posterior distribution (13.127) is
reduced to the Bayes posterior distribution (13.9), with which the generalized
predictive distribution (13.128) gives the Bayes predictive distribution (13.8);
As Î² increases, the probability mass of the generalized posterior distribution
concentrates around the ML estimator, and, in the limit when Î² â†’âˆ, the
generalized predictive distribution converges to the ML predictive distribution
(13.6).
Deï¬ne the following quantities:
GL(X) = âˆ’
/
log

p(x|w)p(Î²)(w|X)dw
0
q(x)
,
(13.129)
TL(X) = âˆ’1
N
N

n=1
log

p(x(n)|w)p(Î²)(w|X)dw,
(13.130)
GGL(X) = âˆ’
/ $log p(x|w)% p(Î²)(w|X)dw
0
q(x)
,
(13.131)
GTL(X) = âˆ’1
N
N

n=1
 
log p(x(n)|w)
 
p(Î²)(w|X)dw,
(13.132)
which are called the Bayes generalization loss, the Bayes training loss, the
Gibbs generalization loss, and the Gibbs training loss, respectively. The
generalization error and the training error of generalized Bayesian learning
are, respectively, related to the Bayes generalization loss and the Bayes training
loss as follows (Watanabe, 2009):
GE(Î²)(X) =
/
log
q(x)

p(x|w)p(Î²)(w|X)dw
0
q(x)
= GL(X) âˆ’S ,
(13.133)

380
13 Asymptotic Learning Theory
TE(Î²)(X) = 1
N
N

n=1
log
q(x(n))

p(x(n)|w)p(Î²)(w|X)dw
= TL(X) âˆ’S N(X),
(13.134)
where
S = âˆ’log q(x)
q(x)
and
S N(X) = âˆ’1
N
N

n=1
log q(x(n))
(13.135)
are the entropy of the true distribution and its empirical version, respectively.9
Also, Gibbs counterparts have the following relations:
GGE(Î²)(X) =
/ 
log q(x)
p(x|w)

p(Î²)(w|X)dw
0
q(x)
= GGL(X) âˆ’S ,
(13.136)
GTE(Î²)(X) = 1
N
N

n=1
 
log q(x(n))
p(x(n)|w)

p(Î²)(w|X)dw
= GTL(X) âˆ’S N(X).
(13.137)
Here GGE(Î²)(X) and GTE(Î²)(X) are the generalization error and the training
error, respectively, of Gibbs learning, where prediction is made by p(x|w) with
its parameter w sampled from the generalized posterior distribution (13.127).
The following relations were proven (Watanabe, 2009):
âŸ¨GL(X)âŸ©q(X) = âŸ¨TL(X)âŸ©q(X) + 2Î²

âŸ¨GTL(X)âŸ©q(X) âˆ’âŸ¨TL(X)âŸ©q(X)
 
+ o(Nâˆ’1),
(13.138)
âŸ¨GGL(X)âŸ©q(X) = âŸ¨GTL(X)âŸ©q(X) + 2Î²

âŸ¨GTL(X)âŸ©q(X) âˆ’âŸ¨TL(X)âŸ©q(X)
 
+ o(Nâˆ’1),
(13.139)
which imply that asymptotically unbiased estimators for generalization losses
(the left-hand sides of Eqs. (13.138) and (13.139)) can be constructed from
training losses (the right-hand sides). The aforementioned equations lead to
widely applicable information criteria (WAIC) (Watanabe, 2009), deï¬ned as
WAIC1 = TL(X) + 2Î² (GTL(X) âˆ’TL(X)) ,
(13.140)
WAIC2 = GTL(X) + 2Î² (GTL(X) âˆ’TL(X)) .
(13.141)
9 S N(X) was deï¬ned in Eq. (13.20), and it holds that S = âŸ¨S N(X)âŸ©q(X).

13.5 Asymptotic Learning Theory for Singular Models
381
Clearly, WAIC1 and WAIC2 are asymptotically unbiased estimators for the
Bayes generalization loss GL(X) and the Gibbs generalization loss GGL(X),
respectively, and therefore minimizing them amounts to minimizing the Bayes
generalization error (13.133) and the Gibbs generalization error (13.136),
respectively.
The training losses, TL(X) and GTL(X) , can be computed by, e.g., MCMC
sampling (see Sections 2.2.4 and 2.2.5). Let w(1),. . . , w(T) be samples drawn
from the generalized posterior distribution (13.127). Then we can estimate the
training losses by
TL(X) â‰ˆâˆ’1
N
N

n=1
log
â›âœâœâœâœâœâ
1
T
T

t=1
p(x(n)|w(t))
ââŸâŸâŸâŸâŸâ ,
(13.142)
GTL(X) â‰ˆâˆ’1
N
N

n=1
1
T
T

t=1
log p(x(n)|w(t)).
(13.143)
WAIC can be seen as an extension of AIC, since minimizing it amounts to
minimizing an asymptotically unbiased estimator for the generalization error.
Indeed, under the regularity conditions, it holds that
lim
Î²â†’âˆ2Î² (GTL(X) âˆ’TL(X)) = D
N ,
and therefore
WAIC1, WAIC2 â†’AIC
2N
as
Î² â†’âˆ.
An extension of BIC was also proposed. The widely applicable Bayesian
information criterion (WBIC) (Watanabe, 2013) is deï¬ned as
WBIC = âˆ’
N

n=1

log p(x(n)|w)p(Î²=1/ log N)(w|X)dw,
(13.144)
where p(Î²=1/ log N)(w|X) is the generalized posterior distribution (13.127) with
the inverse temperature parameter set to Î² = 1/ log N. It was shown that
FBayes(X)
â›âœâœâœâœâœââ‰¡âˆ’log

p(w)
N

n=1
p(x(n)|w)dw
ââŸâŸâŸâŸâŸâ = WBIC + Op(
C
log N),
(13.145)
and therefore WBIC can be used as an estimator or approximation for the
Bayes free energy (13.18) when N is large. It was also shown that, under the
regularity conditions, it holds that
WBIC = BIC
2
+ Op(1).

382
13 Asymptotic Learning Theory
WBIC (13.144) can be estimated, similarly to WAIC, from samples
w(1)
Î²=1/ log N,. . . , w(T)
Î²=1/ log N drawn from p(Î²=1/ log N)(w|X) as
WBIC â‰ˆâˆ’
N

n=1
1
T
T

t=1
log p(x(n)|w(t)
Î²=1/ log N).
(13.146)
Note that evaluating the Bayes free energy (13.18) is much more compu-
tationally demanding in general. For example, the all temperatures method
(Watanabe, 2013) requires posterior samples {w(t)
Î² j } for many 0 = Î²1 < Î²2 <
Â· Â· Â· < Î²J = 1, and estimates the Bayes free energy as
FBayes(X) â‰ˆâˆ’
Jâˆ’1

j=1
log 1
T j
T j

t=1
exp
â›âœâœâœâœâœâ(Î²j+1 âˆ’Î²j)
N

n=1
log p(x(n)|w(t)
Î² j )
ââŸâŸâŸâŸâŸâ .
13.6 Asymptotic Learning Theory for VB Learning
In the rest of Part IV, we describe asymptotic learning theory for VB learning
in detail. Here we give an overview of the subsequent chapters.
VB learning is rarely applied to regular models.10 Actually, if the model
(and the prior) satisï¬es the regularity conditions, Laplace approximation
(2.2.1) can give a good approximation to the posterior, because of the
asymptotic normality (Theorem 13.3). Accordingly, we focus on singular
models when analyzing VB learning.
We are interested in generalization properties of the VB posterior, which is
deï¬ned as
r â‰¡argmin
r
F(r),
s.t.
r âˆˆG,
(13.147)
where
F(r) =
/
log
r(w)
p(w) N
n=1 p(x(n)|w)
0
r(w)
= KL (r(w)||p(w|X)) + FBayes(X)
(13.148)
is the free energy and G is the model-speciï¬c constraint, imposed for compu-
tational tractability, on the approximate posterior.
10 VB learning is often applied to a linear model with an ARD prior. In such a model, the model
likelihood satisï¬es the regularity conditions, while the prior does not. Actually, the model
exhibits characteristics of singular models, since it can be translated to a singular model with a
constant prior (see Section 7.5). In Part IV, we only consider the case where the prior is ï¬xed,
without any hyperparameter optimized.

13.6 Asymptotic Learning Theory for VB Learning
383
With the VB predictive distribution
pVB(x|X) = âŸ¨p(x|w)âŸ©r(w) ,
the generalization error (13.10) and the training error (13.11) are deï¬ned and
analyzed.
We also analyze the VB free energy,
FVB(X) = F(r) = min
r
F(r).
(13.149)
Since Eq. (13.148) implies that
FVB(X) âˆ’FBayes(X) = KL $r(w)||p(w|X)% ,
comparing the VB free energy and the Bayes free energy reveals how
accurately VB learning approximates Bayesian learning.
Similarly to the analysis of Bayesian learning, we investigate the asymptotic
behavior of the relative VB free energy,
FVB(X) = FVB(X) âˆ’NS N(X) = Î»â€²VB log N + op(log N),
(13.150)
where S N(X) is the empirical entropy deï¬ned in Eq. (13.20), and Î»â€²VB is called
the VB free energy coefï¬cient.
Chapter 14 introduces asymptotic VB theory for the RRR model. This
model was relatively easily analyzed by using the analytic-form solution for
fully observed matrix factorization (Chapter 6), and the exact values of the VB
generalization coefï¬cient, the VB training coefï¬cient, and the VB free energy
coefï¬cient were derived (Nakajima and Watanabe, 2007). Since generalization
properties of ML learning and Bayesian learning have also been clariï¬ed
(Fukumizu, 1999; Aoyagi and Watanabe, 2005), similarities and dissimilarities
among ML (and MAP) learning, Bayesian learning, and VB learning will be
discussed.
Chapters 15 through 17 are devoted to asymptotic VB theory for latent
variable models. Chapter 15 analyzes the VB free energy of mixture models.
The VB free energy coefï¬cients and their dependencies on prior hyperparam-
eters are revealed. Chapter 16 proceeds to such analyses of the VB free energy
for other latent variable models, namely, Bayesian networks, hidden Markov
models, probabilistic context free grammar, and latent Dirichlet allocation.
Chapter 17 provides a formula for general latent variable models, which
reduces the asymptotic analysis of the VB free energy to that of the Bayes free
energy introduced in Section 13.5.4. Those results will clarify phase transition
phenomena with respect to the hyperparameter settingâ€”the shape of the
posterior distribution in the asymptotic limit drastically changes when some

384
13 Asymptotic Learning Theory
hyparparameter value exceeds a certain threshold. Such implication suggests
to practitioners how to choose hyperparameters.
Note that the relation (13.25) does not necessarily hold for VB learning
and other approximate Bayesian methods, since Eq. (13.24) only holds for
the exact Bayes predictive distribution. Therefore, unlike Bayesian learning,
the asymptotic behavior of the VB free energy does not necessarily inform
us of the asymptotic behavior of the VB generalization error. An effort on
relating the VB free energy and the VB generalization error is introduced in
Chapter 17, although clarifying VB generalization error requires further effort
and techniques.

14
Asymptotic VB Theory of Reduced
Rank Regression
In this chapter, we introduce asymptotic theory of VB learning in the reduced
rank regression (RRR) model (Nakajima and Watanabe, 2007). Among the
singular models, the RRR model is one of the simplest, and many aspects
of its learning behavior have been clariï¬ed. Accordingly, we can discuss
similarities and dissimilarities of ML (and MAP) learning, Bayesian learning,
and VB learning in terms of generalization error, training error, and free energy.
After deï¬ning the problem setting, we show theoretical results and summarize
insights into VB learning that the analysis on the RRR model provides.
14.1 Reduced Rank Regression
RRR (Baldi and Hornik, 1995; Reinsel and Velu, 1998), introduced in Section
3.1.2 as a special case of fully observed matrix factorization, is a regression
model with a rank-H(â‰¤min(L, M)) linear mapping between input x âˆˆRM and
output y âˆˆRL:
y = BAâŠ¤x + Îµ,
(14.1)
where A âˆˆRMÃ—H and B âˆˆRLÃ—H are parameters to be estimated, and Îµ is
observation noise. Assuming Gaussian noise Îµ âˆ¼GaussL(0, Ïƒâ€²2IL), the model
distribution is given as
p(y|x, A, B) =

2Ï€Ïƒâ€²2 âˆ’L/2 exp

âˆ’1
2Ïƒâ€²2
###y âˆ’BAâŠ¤x
###2
.
(14.2)
RRR is also called a linear neural network, since the three-layer neural
network (7.13) is reduced to RRR (14.1) if the activation function Ïˆ(Â·) is linear
385

386
14 Asymptotic VB Theory of Reduced Rank Regression
(see also Figure 3.1). We assume conditionally conjugate Gaussian priors for
the parameters:
p(A) âˆexp

âˆ’1
2tr

ACâˆ’1
A AâŠ¤ 
,
p(B) âˆexp

âˆ’1
2tr

BCâˆ’1
B BâŠ¤ 
,
(14.3)
with diagonal convariances CA and CB:
CA = Diag(c2
a1,. . . , c2
aH),
CB = Diag(c2
b1,. . . , c2
bH),
for cah, cbh > 0, h = 1,. . . , H. In the asymptotic analysis, we assume that
the hyperparameters {c2
ah, c2
bh}, Ïƒâ€²2 are ï¬xed constants of the order of 1, i.e.,
{c2
ah, c2
bh}, Ïƒâ€²2 âˆ¼Î˜(1) when N â†’âˆ.
The degree of freedom of the RRR model is, in general, different from
the apparent number, (M + L)H, of entries of the parameters A and B. This
is because of the trivial redundancy in parameterizationâ€”the transformation
(A, B) â†’(ATâŠ¤, BTâˆ’1) does not change the linear mapping BAâŠ¤for any
nonsingular matrix T âˆˆRHÃ—H. Accordingly, the essential parameter dimen-
sionality is counted as
D = H(M + L) âˆ’H2.
(14.4)
Suppose we are given N training samples:
D =
,
(x(n), y(n)); x(n) âˆˆRM, y(n) âˆˆRL, n = 1,. . . , N
-
,
(14.5)
which are independently drawn from the true distribution q(x, y) = q(y|x)q(x).
We also use the matrix forms that summarize the inputs and the outputs
separately:
X = (x(1),. . . , x(N))âŠ¤âˆˆRNÃ—M,
Y = (y(1),. . . , y(N))âŠ¤âˆˆRNÃ—L.
We suppose that the data are preprocessed so that the input and the output are
centered, i.e.,
1
N
N

n=1
x(n) = 0
and
1
N
N

n=1
y(n) = 0,
(14.6)
and the input is prewhitened (HyvÂ¨arinen et al., 2001), i.e.,
1
N
N

n=1
x(n)x(n)âŠ¤= 1
N XâŠ¤X = IM.
(14.7)
The likelihood of the RRR model (14.1) on the training samples D = (X, Y)
is expressed as
p(Y|X, A, B) âˆexp
â›âœâœâœâœâœââˆ’1
2Ïƒâ€²2
N

n=1
###y(n) âˆ’BAâŠ¤x(n)###2
ââŸâŸâŸâŸâŸâ .
(14.8)

14.1 Reduced Rank Regression
387
As shown in Section 3.1.2, the logarithm of the likelihood (14.8) can be
written, as a function of the parameters, as follows:
log p(Y|X, A, B) = âˆ’N
2Ïƒâ€²2
###V âˆ’BAâŠ¤###2
Fro + const.,
(14.9)
where
V = 1
N
N

n=1
y(n)x(n)âŠ¤= 1
N YâŠ¤X.
(14.10)
Note that, unlike in Section 3.1.2, we here do not use the rescaled noise
variance Ïƒ2 = Ïƒâ€²2/N, in order to make the dependence on the number N
of samples clear for asymptotic analysis. Because the log-likelihood (14.9)
is in the same form as that of the fully observed matrix factorization (MF),
we can use the global VB solution, derived in Chapter 6, of the MF model for
analyzing VB learning in the RRR model.
14.1.1 VB Learning
VB learning solves the following problem:
r = argmin
r
F(r)
s.t.
r(A, B) = rA(A)rB(B),
(14.11)
where
F =
/
log
rA(A)rB(B)
p(Y|X, A, B)p(A)p(B)
0
rA(A)rB(B)
is the free energy. As derived in Section 3.1, the solution to the problem (14.11)
is in the following forms:
rA(A) = MGaussM,H(A; A, IM âŠ—Î£A) âˆexp
â›âœâœâœâœâœâœâœâœâœâœâ
âˆ’
tr
!
(A âˆ’A)Î£
âˆ’1
A (A âˆ’A)âŠ¤
"
2
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
,
(14.12)
rB(B) = MGaussL,H(B; B, IL âŠ—Î£B) âˆexp
â›âœâœâœâœâœâœâœâœâœâœâ
âˆ’
tr
!
(B âˆ’B)Î£
âˆ’1
B (B âˆ’B)âŠ¤
"
2
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
.
(14.13)
With the variational parameters (A, Î£A, B, Î£B), the free energy can be explic-
itly written as

388
14 Asymptotic VB Theory of Reduced Rank Regression
2F = NL log(2Ï€Ïƒâ€²2) +
N
n=1
###y(n)###2 âˆ’N âˆ¥Vâˆ¥2
Fro
Ïƒâ€²2
+
N
####V âˆ’BA
âŠ¤####
2
Fro
Ïƒâ€²2
+ M log det (CA)
det
Î£A
 + L log det (CB)
det
Î£B
 
âˆ’(L + M)H + tr
2
Câˆ’1
A
!
A
âŠ¤A + MÎ£A
"
+ Câˆ’1
B
!
B
âŠ¤B + LÎ£B
"
+ NÏƒâ€²âˆ’2 !
âˆ’A
âŠ¤AB
âŠ¤B +
!
A
âŠ¤A + MÎ£A
" !
B
âŠ¤B + LÎ£B
""3
.
(14.14)
We can further apply Corollary 6.6, which states that the VB learning
problem (14.11) is decomposable in the following way. Let
V =
L

h=1
Î³hÏ‰bhÏ‰âŠ¤
ah
(14.15)
be the singular value decomposition (SVD) of V (deï¬ned in Eq. (14.10)),
where Î³h (â‰¥0) is the hth largest singular value, and Ï‰ah and Ï‰bh are the
associated right and left singular vectors. Then the solution (or its equivalent)
of the variational parameters A = (a1,. . . ,aH), B = (b1,. . . ,bH), Î£A, Î£B,
which minimizes the free energy (14.14), can be expressed as follows:
ah = ahÏ‰ah,
bh = bhÏ‰bh,
Î£A = Diag

Ïƒ2
a1,. . . , Ïƒ2
aH
 
,
Î£B = Diag

Ïƒ2
b1,. . . , Ïƒ2
bH
 
,
where {ah,bh âˆˆR, Ïƒ2
ah, Ïƒ2
bh âˆˆR++}H
h=1 are a new set of variational parameters.
Thus, the VB posteriors (14.12) and (14.13) can be written as
rA(A) =
H

h=1
GaussM(ah;ahÏ‰ah, Ïƒ2
ah IM),
(14.16)
rB(B) =
H

h=1
GaussL(bh;bhÏ‰bh, Ïƒ2
bh IL),
(14.17)
with {ah,bh, Ïƒ2
ah, Ïƒ2
bh}H
h=1 that are the solution of the following minimization
problem:
Given
Ïƒâ€²2 âˆˆR++,
{c2
ah, c2
bh âˆˆR++}H
h=1,
min
{ah,bh,Ïƒ2ah,Ïƒ2
bh}H
h=1
F
(14.18)
s.t.
{ah,bh âˆˆR,
Ïƒ2
ah, Ïƒ2
bh âˆˆR++}H
h=1.

14.1 Reduced Rank Regression
389
Here F is the free energy (14.14), which can be decomposed as
2F = NL log(2Ï€Ïƒâ€²2) +
N
n=1
###y(n)###2
Ïƒâ€²2
+
H

h=1
2Fh,
(14.19)
where
2Fh = M log
c2
ah
Ïƒ2ah
+ L log
c2
bh
Ïƒ2
bh
+
a2
h + MÏƒ2
ah
c2ah
+
b2
h + LÏƒ2
bh
c2
bh
âˆ’(L + M) + N
Ïƒâ€²2

âˆ’2ahbhÎ³h +

a2
h + MÏƒ2
ah
 b2
h + LÏƒ2
bh
  
.
(14.20)
14.1.2 VB Solution
Let us derive an asymptotic-form VB solution from the nonasymptotic global
VB solution, derived in Section 6. Theorem 6.7 leads to the following theorem:
Theorem 14.1
The VB estimator U
VB â‰¡BAâŠ¤
rA(A)rB(B) for the linear
mapping of the RRR model (14.2) and (14.3) can be written as
U
VB = BA
âŠ¤=
H

h=1
Î³VB
h Ï‰bhÏ‰âŠ¤
ah,
where
Î³VB
h
= max

0, Ë˜Î³VB
h
 
(14.21)
for
Ë˜Î³VB
h
= Î³h
â›âœâœâœâœâ1 âˆ’max(L, M)Ïƒâ€²2
NÎ³2
h
ââŸâŸâŸâŸâ + Op(Nâˆ’1).
(14.22)
For each component h, Î³VB
h
> 0 if and only if Î³h > Î³VB
h
for
Î³VB
h
= Ïƒâ€²
B
max(L, M)
N
+ O(Nâˆ’1).
(14.23)
Proof
Noting that Theorem 6.7 gives the VB solution for either V or VâŠ¤âˆˆ
RLÃ—M that satisï¬es L â‰¤M, that the shrinkage estimator Ë˜Î³VB
h
(given by
Eq. (6.50)) is an increasing function of Î³h, and that Ë˜Î³VB
h
= 0 when Î³h is equal
to the threshold Î³VB
h
(given by Eq. (6.49)), we have Eq. (14.21) with
Ë˜Î³VB
h
= Î³h
â›âœâœâœâœâœâœâœâ1 âˆ’Ïƒâ€²2
2NÎ³2
h
â›âœâœâœâœâœâœâœâL + M +
>
@
(M âˆ’L)2 +
4Î³2
h
c2ahc2
bh
ââŸâŸâŸâŸâŸâŸâŸâ 
ââŸâŸâŸâŸâŸâŸâŸâ 
= Î³h
â›âœâœâœâœâ1 âˆ’Ïƒâ€²2
2NÎ³2
h
!
L + M +
.
(M âˆ’L)2 + O(Î³2
h)
"ââŸâŸâŸâŸâ 

390
14 Asymptotic VB Theory of Reduced Rank Regression
=
â§âªâªâªâªâ¨âªâªâªâªâ©
Î³h
!
1 âˆ’
Ïƒâ€²2
2NÎ³2
h

L + M + max(L, M) âˆ’min(L, M) + O(Î³2
h)
 "
(if L  M)
Î³h
!
1 âˆ’
Ïƒâ€²2
2NÎ³2
h (L + M + O(Î³h))
"
(if L = M)
= Î³h
â›âœâœâœâœâ1 âˆ’max(L, M)Ïƒâ€²2
NÎ³2
h

1 + Op(Î³h)
 ââŸâŸâŸâŸâ 
(14.24)
= Î³h
â›âœâœâœâœâ1 âˆ’max(L, M)Ïƒâ€²2
NÎ³2
h
ââŸâŸâŸâŸâ + Op(Nâˆ’1),
and
Î³VB
h
= Ïƒâ€²
âˆš
N
>
?
?
?
@
(L + M)
2
+
Ïƒâ€²2
2Nc2ahc2
bh
+
>
?
@â›âœâœâœâœâœâ
(L + M)
2
+
Ïƒâ€²2
2Nc2ahc2
bh
ââŸâŸâŸâŸâŸâ 
2
âˆ’LM
= Ïƒâ€²
âˆš
N
>
?
@
(L + M)
2
+
Ïƒâ€²2
2Nc2ahc2
bh
+
Amax(L, M) âˆ’min(L, M)
2
2
+ O(Nâˆ’1)
=
â§âªâªâªâªâªâªâªâ¨âªâªâªâªâªâªâªâ©
Ïƒâ€²
âˆš
N
B
(L+M)
2
+
Ïƒâ€²2
2Nc2ahc2
bh
+ max(L,M)âˆ’min(L,M)
2
+ O(Nâˆ’1)
(if L  M)
Ïƒâ€²
âˆš
N
B
(L+M)
2
+
Ïƒâ€²2
2Nc2ahc2
bh
+ O(Nâˆ’1/2)
(if L = M)
= Ïƒâ€²
âˆš
N
C
max(L, M) + O(Nâˆ’1),
which completes the proof. Note that we used Î³h = Op(1) to get Eq. (14.24). â–¡
Theorem 14.1 states that the VB estimator converges to the positive-part
Jamesâ€“Stein (PJS) estimator (see Appendix A)â€”the same solution (Corollary
7.1) as the nonasymptotic MF solution with the ï¬‚at prior. This is natural
because the inï¬‚uence from the constant prior disappears in the asymptotic
limit, making MAP learning converge to ML learning.
Corollary 6.8 leads to the following corollary:
Corollary 14.2
The VB posterior of the RRR model (14.2) and (14.3) is given
by Eqs. (14.16) and (14.17) with the variational parameters given as follows:
if Î³h > Î³VB
h ,
ah = Â±
.
Ë˜Î³VB
h Î´VB
h ,
bh = Â±
>
@
Ë˜Î³VB
h
Î´VB
h
,
Ïƒ2
ah = Ïƒâ€²2Î´VB
h
NÎ³h
,
Ïƒ2
bh =
Ïƒâ€²2
NÎ³hÎ´VB
h
,
(14.25)

14.1 Reduced Rank Regression
391
where
Î´VB
h

â‰¡ah
bh

=
â§âªâªâªâ¨âªâªâªâ©
(max(L,M)âˆ’min(L,M))cah
Î³h
+ Op(1)
(if L â‰¤M),
!
(max(L,M)âˆ’min(L,M))cbh
Î³h
+ Op(1)
"âˆ’1
(if L > M),
(14.26)
and otherwise,
ah = 0,
bh = 0,
Ïƒ2
ah = c2
ah
â›âœâœâœâœâœâ1 âˆ’NLÎ¶VB
h
Ïƒâ€²2
ââŸâŸâŸâŸâŸâ ,
Ïƒ2
bh = c2
bh
â›âœâœâœâœâœâ1 âˆ’NMÎ¶VB
h
Ïƒâ€²2
ââŸâŸâŸâŸâŸâ ,
(14.27)
where
Î¶VB
h

â‰¡Ïƒ2
ahÏƒ2
bh
 
=
â§âªâªâªâ¨âªâªâªâ©
min(L,M)Ïƒâ€²2
NLM
+ Î˜(Nâˆ’2),
(if L  M),
min(L,M)Ïƒâ€²2
NLM
+ Î˜(Nâˆ’3/2),
(if L = M).
(14.28)
Proof
Noting that Corollary 6.8 gives the VB posterior for either V or VâŠ¤âˆˆ
RLÃ—M that satisï¬es L â‰¤M, we have Eq. (14.25) with
Î´VB
h
=
â§âªâªâªâªâ¨âªâªâªâªâ©
Ncah
Ïƒâ€²2

Î³h âˆ’Ë˜Î³VB
h
âˆ’LÏƒâ€²2
NÎ³h
 
(if L â‰¤M)
!
Ncbh
Ïƒâ€²2

Î³h âˆ’Ë˜Î³VB
h
âˆ’MÏƒâ€²2
NÎ³h
 "âˆ’1
(if L > M)
=
â§âªâªâªâªâ¨âªâªâªâªâ©
(max(L,M)âˆ’min(L,M))cah
Î³h
+ Op(1)
(if L â‰¤M),
!
(max(L,M)âˆ’min(L,M))cbh
Î³h
+ Op(1)
"âˆ’1
(if L > M),
when Î³h > Î³VB
h
, and Eq. (14.27) with
Î¶VB
h
=
Ïƒâ€²2
2NLM
â§âªâªâ¨âªâªâ©L + M +
Ïƒâ€²2
Nc2ahc2
bh
âˆ’
B!
L + M +
Ïƒâ€²2
Nc2ahc2
bh
"2
âˆ’4LM
â«âªâªâ¬âªâªâ­
=
Ïƒâ€²2
2NLM
)
L + M +
Ïƒâ€²2
Nc2ahc2
bh
âˆ’
B
(L + M)2 + 2 (L + M)
Ïƒâ€²2
Nc2ahc2
bh
+
!
Ïƒâ€²2
Nc2ahc2
bh
"2
âˆ’4LM
â«âªâªâ¬âªâªâ­
=
Ïƒâ€²2
2NLM
)
L + M +
Ïƒâ€²2
Nc2ahc2
bh
âˆ’
B
(max(L, M) âˆ’min(L, M))2 + 2 (L + M)
Ïƒâ€²2
Nc2ahc2
bh
+
!
Ïƒâ€²2
Nc2ahc2
bh
"2 â«âªâªâ¬âªâªâ­

392
14 Asymptotic VB Theory of Reduced Rank Regression
=
â§âªâªâªâªâªâªâªâªâªâªâªâªâªâªâªâªâªâ¨âªâªâªâªâªâªâªâªâªâªâªâªâªâªâªâªâªâ©
Ïƒâ€²2
2NLM
)
L + M +
Ïƒâ€²2
Nc2ahc2
bh
âˆ’(max(L, M) âˆ’min(L, M))
Â·
!
1 +
L+M
(max(L,M)âˆ’min(L,M))2
Ïƒâ€²2
Nc2ahc2
bh
"
+ O(Nâˆ’2)
1
(if L  M)
Ïƒâ€²2
2NLM
)
L + M +
Ïƒâ€²2
Nc2ahc2
bh
âˆ’
B
2 (L + M)
!
Ïƒâ€²2
Nc2ahc2
bh
"
+
!
Ïƒâ€²2
Nc2ahc2
bh
"2 â«âªâªâ¬âªâªâ­
(if L = M)
=
â§âªâªâªâ¨âªâªâªâ©
Ïƒâ€²2
2NLM

2 min(L, M) + Î˜(Nâˆ’1)
 
(if L  M)
Ïƒâ€²2
2NLM

2 min(L, M) + Î˜(Nâˆ’1/2)
 
(if L = M)
=
â§âªâªâªâ¨âªâªâªâ©
min(L,M)Ïƒâ€²2
NLM
+ Î˜(Nâˆ’2)
(if L  M),
min(L,M)Ïƒâ€²2
NLM
+ Î˜(Nâˆ’3/2)
(if L = M),
when Î³h â‰¤Î³VB
h . This completes the proof.
â–¡
From Corollary 14.2, we can evaluate the orders of the optimal variational
parameters in the asymptotic limit, which will be used when the VB free energy
is analyzed.
Corollary 14.3
The orders of the optimal variational parameters, given by
Eq. (14.25) or Eq. (14.27), are as follows: if Î³h > Î³VB
h (= Î˜(Nâˆ’1/2)),
ah =Î˜p(1),
bh =Î˜p(Î³h),
Ïƒ2
ah =Î˜p(Nâˆ’1Î³âˆ’2
h ), Ïƒ2
bh =Î˜p(Nâˆ’1)
(if L< M),
ah =Î˜p(Î³1/2
h ), bh =Î˜p(Î³1/2
h ), Ïƒ2
ah =Î˜p(Nâˆ’1Î³âˆ’1
h ), Ïƒ2
bh =Î˜p(Nâˆ’1Î³âˆ’1
h ) (if L= M),
ah =Î˜p(Î³h),
bh =Î˜p(1),
Ïƒ2
ah =Î˜p(Nâˆ’1),
Ïƒ2
bh =Î˜p(Nâˆ’1Î³âˆ’2
h ) (if L> M),
(14.29)
and otherwise,
ah = 0,
bh = 0,
Ïƒ2
ah = Î˜(1),
Ïƒ2
bh = Î˜(Nâˆ’1)
(if L < M),
ah = 0,
bh = 0,
Ïƒ2
ah = Î˜(Nâˆ’1/2),
Ïƒ2
bh = Î˜(Nâˆ’1/2)
(if L = M),
ah = 0,
bh = 0,
Ïƒ2
ah = Î˜(Nâˆ’1),
Ïƒ2
bh = Î˜(1)
(if L > M). (14.30)
Proof
Eqs. (14.22) and (14.23) give
Î³VB
h
= Î˜(Nâˆ’1/2),
Ë˜Î³VB
h
= Î˜(Î³h),

14.1 Reduced Rank Regression
393
and Eq. (14.26) gives
Î´VB
h
=
â§âªâªâªâªâªâ¨âªâªâªâªâªâ©
Î˜p(Î³âˆ’1
h )
(if L < M),
Î˜p(1)
(if L = M),
Î˜p(Î³h)
(if L > M).
Substituting the preceding into Eq. (14.25) gives Eq. (14.29), and substituting
Eq. (14.28) into Eq. (14.27) gives Eq. (14.30), which complete the proof.
â–¡
Corollary 14.3 implies that the posterior probability mass does not neces-
sarily converge to a single point, for example, Ïƒ2
ah = Î˜(1) if Î³h < Î³VB
h
and
L < M. This is typical behavior of singular models with nonidentiï¬ability. On
the other hand, the probability mass of the linear mapping U = BAâŠ¤converges
to a single point.
Corollary 14.4
It holds that
*####BAâŠ¤âˆ’BA
âŠ¤####
2
Fro
+
rA(A)rB(B) = Op(Nâˆ’1).
Proof
We have
*####BAâŠ¤âˆ’BA
âŠ¤####
2
Fro
+
rA(A)rB(B) = tr
/!
BAâŠ¤âˆ’BA
âŠ¤"âŠ¤!
BAâŠ¤âˆ’BA
âŠ¤"0
rA(A)rB(B)
= tr
*
ABâŠ¤BAâŠ¤âˆ’2ABâŠ¤BA
âŠ¤+ AB
âŠ¤BA
âŠ¤+
rA(A)rB(B)
= tr
!
AâŠ¤ABâŠ¤B

rA(A)rB(B) âˆ’A
âŠ¤AB
âŠ¤B
"
= tr
!!
A
âŠ¤A + MÎ£A
" !
B
âŠ¤B + LÎ£B
"
âˆ’A
âŠ¤AB
âŠ¤B
"
=
H

h=1

a2
h + MÏƒ2
ah
 b2
h + LÏƒ2
bh
 
âˆ’a2
hb2
h
 
=
H

h=1

La2
hÏƒ2
bh + Mb2
hÏƒ2
ah + LMÏƒ2
ahÏƒ2
bh
 
.
(14.31)
Corollary 14.3 guarantees that all terms in Eq. (14.31) are of the order of
Î˜p(Nâˆ’1) for any L, M, and {Î³h}, which completes the proof.
â–¡
Now we derive an asymptotic form of the VB predictive distribution,
p (y|x, X, Y) = âŸ¨p(y|x, A, B)âŸ©rA(A)rB(B) .
(14.32)

394
14 Asymptotic VB Theory of Reduced Rank Regression
From Corollary 14.4, we expect that the predictive distribution is not very far
from the plug-in VB predictive distribution (see Section 1.1.3):
p(y|x, A, B) = GaussL(y; BA
âŠ¤x, Ïƒâ€²2IL).
(14.33)
Indeed, we will show in the next section that both predictive distributions
(14.32) and (14.33) give the same generalization and training coefï¬cients. This
justiï¬es the use of the plug-in VB predictive distribution, which is easy to
compute from the optimal variational parameters.
By expanding the VB predictive distribution around the plug-in VB predic-
tive distribution, we have the following theorem:
Theorem 14.5
The VB predictive distribution (14.32) of the RRR model
(14.2) and (14.3) can be written as
p(y|x, X, Y) = GaussL(y; Î¨BA
âŠ¤x, Ïƒâ€²2Î¨) + Op(Nâˆ’3/2)
(14.34)
for Î¨ = IL + Op(Nâˆ’1).
Proof
The VB predictive distribution can be written as follows:
p (y|x, X, Y) = âŸ¨p(y|x, A, B)âŸ©rA(A)rB(B)
= p(y|x, A, B)
/ p(y|x, A, B)
p(y|x, A, B)
0
rA(A)rB(B)
= p(y|x, A, B)
/
exp
â›âœâœâœâœââˆ’âˆ¥yâˆ’BAâŠ¤xâˆ¥
2âˆ’
####yâˆ’BA
âŠ¤x
####
2
2Ïƒâ€²2
ââŸâŸâŸâŸâ 
0
rA(A)rB(B)
= p(y|x, A, B)
/
exp
â›âœâœâœâœâœââˆ’
!
yâˆ’BAâŠ¤x+(yâˆ’BA
âŠ¤x)
"âŠ¤!
yâˆ’BAâŠ¤xâˆ’(yâˆ’BA
âŠ¤x)
"
2Ïƒâ€²2
ââŸâŸâŸâŸâŸâ 
0
rA(A)rB(B)
= p(y|x, A, B)
/
exp
â›âœâœâœâœâœâ
!
yâˆ’(BAâŠ¤âˆ’BA
âŠ¤)x
"âŠ¤
(BAâŠ¤âˆ’BA
âŠ¤)x
Ïƒâ€²2
ââŸâŸâŸâŸâŸâ 
0
rA(A)rB(B)
.
(14.35)
Corollary 14.4 implies that the exponent in Eq. (14.35) is of the order of Nâˆ’1/2,
i.e.,
Ï† â‰¡
!
y âˆ’(BAâŠ¤âˆ’BA
âŠ¤)x
"âŠ¤
(BAâŠ¤âˆ’BA
âŠ¤)x
Ïƒâ€²2
= Op(Nâˆ’1/2).
(14.36)
By applying the Taylor expansion of the exponential function to Eq. (14.35),
we obtain an asymptotic expansion of the predictive distribution around the
plug-in predictive distribution:
p (y|x, X, Y) = p(y|x, A, B)

1 + âŸ¨Ï†âŸ©rA(A)rB(B) + 1
2

Ï†2
rA(A)rB(B) + Op(Nâˆ’3/2)

.

14.1 Reduced Rank Regression
395
Focusing on the dependence on the random variable y, we can identify the
function form of the predictive distribution as follows:
p (y|x, X, Y) âˆexp

âˆ’
####yâˆ’BA
âŠ¤x
####
2
2Ïƒâ€²2
+ log
!
1 + âŸ¨Ï†âŸ©rA(A)rB(B) + 1
2

Ï†2
rA(A)rB(B) + Op(Nâˆ’3/2)
" 
= exp

âˆ’
####yâˆ’BA
âŠ¤x
####
2
2Ïƒâ€²2
+ âŸ¨Ï†âŸ©rA(A)rB(B) + 1
2

Ï†2
rA(A)rB(B)
âˆ’1
2 âŸ¨Ï†âŸ©2
rA(A)rB(B) + Op(Nâˆ’3/2)

âˆexp
â›âœâœâœâœââˆ’
####yâˆ’BA
âŠ¤x
####
2
2Ïƒâ€²2
+ 1
2

Ï†2
rA(A)rB(B) + Op(Nâˆ’3/2)
ââŸâŸâŸâŸâ 
âˆexp
â›âœâœâœâœââˆ’
####yâˆ’BA
âŠ¤x
####
2
âˆ’yâŠ¤Î¨1y
2Ïƒâ€²2
+ Op(Nâˆ’3/2)
ââŸâŸâŸâŸâ 
âˆexp
!
âˆ’âˆ¥yâˆ¥2âˆ’2yâŠ¤BA
âŠ¤xâˆ’yâŠ¤Î¨1y
2Ïƒâ€²2
+ Op(Nâˆ’3/2)
"
âˆexp
â›âœâœâœâœâœââˆ’
!
yâˆ’Î¨BA
âŠ¤x
"âŠ¤
Î¨âˆ’1!
yâˆ’Î¨BA
âŠ¤x
"
2Ïƒâ€²2
+ Op(Nâˆ’3/2)
ââŸâŸâŸâŸâŸâ ,
(14.37)
where
Î¨ = (IL âˆ’Î¨1)âˆ’1 ,
(14.38)
Î¨1 =
/(BAâŠ¤âˆ’BA
âŠ¤)xxâŠ¤(BAâŠ¤âˆ’BA
âŠ¤)âŠ¤
Ïƒâ€²2
0
rA(A)rB(B)
.
(14.39)
Here we used
âŸ¨Ï†âŸ©rA(A)rB(B) =
/ !
yâˆ’(BAâŠ¤âˆ’BA
âŠ¤)x
"âŠ¤
(BAâŠ¤âˆ’BA
âŠ¤)x
Ïƒâ€²2
0
rA(A)rB(B)
=
/ ####(BAâŠ¤âˆ’BA
âŠ¤)x
####
2
Ïƒâ€²2
0
rA(A)rB(B)
= const.,

Ï†2
rA(A)rB(B) =
/ !
yâˆ’(BAâŠ¤âˆ’BA
âŠ¤)x
"âŠ¤
(BAâŠ¤âˆ’BA
âŠ¤)xxâŠ¤(BAâŠ¤âˆ’BA
âŠ¤)âŠ¤
!
yâˆ’(BAâŠ¤âˆ’BA
âŠ¤)x
"
Ïƒâ€²4
0
rA(A)rB(B)
= yâŠ¤Î¨1y
Ïƒâ€²2
+ Op(Nâˆ’3/2).
Eq. (14.39) implies that Î¨1 is symmetric and Î¨1 = Op(Nâˆ’1). Therefore,
Î¨, deï¬ned by Eq. (14.38), is symmetric, positive deï¬nite, and can be written

396
14 Asymptotic VB Theory of Reduced Rank Regression
as Î¨ = IL + Op(Nâˆ’1). The function form of Eq. (14.37) implies that the VB
predictive distribution converges to the Gaussian distribution in the asymptotic
limit, and we thus have
p (y|x, X, Y) =
exp
â›âœâœâœâœâœââˆ’(yâˆ’Î¨BAâŠ¤x)
âŠ¤Î¨âˆ’1(yâˆ’Î¨BAâŠ¤x)
2Ïƒâ€²2
+Op(Nâˆ’3/2)
ââŸâŸâŸâŸâŸâ 

exp
â›âœâœâœâœâœââˆ’(yâˆ’Î¨BAâŠ¤x)
âŠ¤Î¨âˆ’1(yâˆ’Î¨BAâŠ¤x)
2Ïƒâ€²2
+Op(Nâˆ’3/2)
ââŸâŸâŸâŸâŸâ dy
=
exp
â›âœâœâœâœâœââˆ’(yâˆ’Î¨BAâŠ¤x)
âŠ¤Î¨âˆ’1(yâˆ’Î¨BAâŠ¤x)
2Ïƒâ€²2
ââŸâŸâŸâŸâŸâ (1+Op(Nâˆ’3/2))

exp
â›âœâœâœâœâœââˆ’(yâˆ’Î¨BAâŠ¤x)
âŠ¤Î¨âˆ’1(yâˆ’Î¨BAâŠ¤x)
2Ïƒâ€²2
ââŸâŸâŸâŸâŸâ (1+Op(Nâˆ’3/2))dy
=
1
(2Ï€Ïƒâ€²2)
L/2 det(Î¨)1/2 exp
â›âœâœâœâœâœââˆ’
!
yâˆ’Î¨BA
âŠ¤x
"âŠ¤
Î¨âˆ’1!
yâˆ’Î¨BA
âŠ¤x
"
2Ïƒâ€²2
ââŸâŸâŸâŸâŸâ + Op(Nâˆ’3/2),
which completes the proof.
â–¡
14.2 Generalization Properties
Let us analyze generalization properties of VB learning based on the posterior
distribution and the predictive distribution, derived in Section 14.1.2.
14.2.1 Assumption on True Distribution
We assume that the true distribution can be expressed by the model distribution
with the true parameter Aâˆ—and Bâˆ—with their rank Hâˆ—:
q(y|x) = GaussL

y, Bâˆ—Aâˆ—âŠ¤x, Ïƒâ€²2IL
 
=

2Ï€Ïƒâ€²2 âˆ’L/2 exp
â›âœâœâœâœâœâœââˆ’
###y âˆ’Bâˆ—Aâˆ—âŠ¤x
###2
2Ïƒâ€²2
ââŸâŸâŸâŸâŸâŸâ .
(14.40)
Let
Uâˆ—â‰¡Bâˆ—Aâˆ—âŠ¤=
min(L,M)

h=1
Î³âˆ—
hÏ‰âˆ—
bhÏ‰âˆ—âŠ¤
ah
(14.41)
be the SVD of the true linear mapping Bâˆ—Aâˆ—âŠ¤, where Î³âˆ—
h (â‰¥0) is the hth
largest singular value, and Ï‰âˆ—
ah and Ï‰âˆ—
bh are the associated right and left singular
vectors. The assumption that the true linear mapping has rank Hâˆ—amounts to
Î³âˆ—
h =
â§âªâªâ¨âªâªâ©
Î˜(1)
for
h = 1,. . . , Hâˆ—,
0
for
h = Hâˆ—+ 1,. . . , min(L, M).
(14.42)

14.2 Generalization Properties
397
14.2.2 Consistency of VB Estimator
Since the training samples are drawn from the true distribution (14.40), the
central limit theorem (Theorem 13.1) guarantees the following:
V
â›âœâœâœâœâœââ‰¡1
N
N

n=1
y(n)x(n)âŠ¤
ââŸâŸâŸâŸâŸâ = 1
N
N

n=1

Bâˆ—Aâˆ—âŠ¤x(n) + Îµ(n) 
x(n)âŠ¤
= Bâˆ—Aâˆ—âŠ¤+ Op(Nâˆ’1/2),
(14.43)

xxâŠ¤
q(x) = 1
N
N

n=1
x(n)x(n)âŠ¤+ Op(Nâˆ’1/2)
= IM + Op(Nâˆ’1/2).
(14.44)
Here we used the assumption (14.7) that the input is prewhitened. Eq. (14.43)
is consistent with Eq. (14.9), which implies that the distribution of V is
given by
q(V) = MGaussL,M

V; Bâˆ—Aâˆ—âŠ¤, Ïƒâ€²2
N IL âŠ—IM

,
(14.45)
and therefore
âŸ¨VâŸ©q(X,Y) = âŸ¨VâŸ©q(V) = Bâˆ—Aâˆ—âŠ¤,
(14.46)
and for each (l, m),
*####Vl,m âˆ’

Bâˆ—Aâˆ—âŠ¤ 
l,m
####
2
Fro
+
q(X,Y) = Ïƒâ€²2
N .
(14.47)
Eq. (14.43) implies that
Î³h = Î³âˆ—
h + Op(Nâˆ’1/2),
(14.48)
where Î³h is the hth largest singular value of V (see Eq. (14.15)). Eq. (14.45)
also implies that, for any h,

hâ€²:Î³âˆ—
hâ€²=Î³âˆ—
h

Î³hâ€²Ï‰bhâ€²Ï‰âŠ¤
ahâ€²

q(X,Y) =

hâ€²:Î³âˆ—
hâ€²=Î³âˆ—
h
Î³âˆ—
hâ€²Ï‰âˆ—
bhâ€² Ï‰âˆ—âŠ¤
ahâ€²,
(14.49)

hâ€²:Î³âˆ—
hâ€²=Î³âˆ—
h
Î³hâ€²Ï‰bhâ€²Ï‰âŠ¤
ahâ€² =

hâ€²:Î³âˆ—
hâ€²=Î³âˆ—
h
Î³âˆ—
hâ€²Ï‰âˆ—
bhâ€² Ï‰âˆ—âŠ¤
ahâ€² + Op(Nâˆ’1/2),
(14.50)
where 
hâ€²:Î³âˆ—
hâ€²=Î³âˆ—
h denotes the sum over all hâ€² such that Î³âˆ—
hâ€² = Î³âˆ—
h. Eq. (14.50)
implies that for any nonzero and nondegenerate singular component h (i.e.,
Î³âˆ—
h > 0 and Î³âˆ—
h  Î³âˆ—
hâ€²âˆ€hâ€²  h), it holds that

398
14 Asymptotic VB Theory of Reduced Rank Regression
Ï‰ah = Ï‰âˆ—
ah + Op(Nâˆ’1/2),
Ï‰bh = Ï‰âˆ—
bh + Op(Nâˆ’1/2).
Eq. (14.9) implies that the ML estimator is given by
!
BA
âŠ¤"ML
=
H

h=1
Î³hÏ‰bhÏ‰âŠ¤
ah.
(14.51)
Therefore, Eq. (14.43) guarantees the convergence of the ML estimator to the
true linear mapping Bâˆ—Aâˆ—âŠ¤when H â‰¥Hâˆ—.
Lemma 14.6
(Consistency of ML estimator in RRR) It holds that
!
BA
âŠ¤"ML
âˆ’Bâˆ—Aâˆ—âŠ¤=
â§âªâªâ¨âªâªâ©
Î˜(1)
if
H < Hâˆ—,
Op(Nâˆ’1/2)
if
H â‰¥Hâˆ—.
We can also show the convergence of the VB estimator:
Lemma 14.7
(Consistency of VB estimator in RRR) It holds that
BA
âŠ¤âˆ’Bâˆ—Aâˆ—âŠ¤=
â§âªâªâ¨âªâªâ©
Î˜(1)
if
H < Hâˆ—,
Op(Nâˆ’1/2)
if
H â‰¥Hâˆ—.
Proof
The case where H < Hâˆ—is trivial because the rank H matrix BA
âŠ¤can
never converge to the rank Hâˆ—matrix Bâˆ—Aâˆ—âŠ¤. Assume that H â‰¥Hâˆ—. Theorem
14.1 implies that, when Î³h > Î³VB
h (= Î˜(Nâˆ’1/2)),
Î³VB
h
= Ë˜Î³VB
h
= Î³h
â›âœâœâœâœâ1 âˆ’max(L, M)Ïƒâ€²2
NÎ³2
h
ââŸâŸâŸâŸâ + Op(Nâˆ’1) = Î³h + Op(Nâˆ’1/2),
and otherwise
Î³VB
h
= 0.
Since Î³h = Op(Nâˆ’1/2) for h = Hâˆ—+ 1,. . . , min(L, M), the preceding two
equations lead to
BA
âŠ¤=
H

h=1
Î³VB
h Ï‰bhÏ‰âŠ¤
ah =
min(L,M)

h=1
Î³hÏ‰bhÏ‰âŠ¤
ah + Op(Nâˆ’1/2) = V + Op(Nâˆ’1/2).
(14.52)
Substituting Eq. (14.43) into Eq. (14.52) completes the proof.
â–¡
14.2.3 Generalization Error
Now we analyze the asymptotic behavior of the generalization error. We
ï¬rst show the asymptotic equivalence between the VB predictive distribution,

14.2 Generalization Properties
399
given by Theorem 14.5, and the plug-in VB predictive distribution (14.33)â€”
both give the same leading term of the generalization error with Op(Nâˆ’3/2)
difference. To this end, we use the following lemma:
Lemma 14.8
For any three sets of Gaussian parameters (Î¼âˆ—, Î£âˆ—), (Î¼, Î£), (Â´Î¼, Â´Î£)
such that
Î¼ = Î¼âˆ—+ Op(Nâˆ’1/2),
Î£ = Î£âˆ—+ Op(Nâˆ’1/2),
(14.53)
Â´Î¼ = Î¼ + Op(Nâˆ’1),
Â´Î£ = Î£ + Op(Nâˆ’1),
(14.54)
it holds that
/
log
GaussL

y; Â´Î¼, Â´Î£
 
+ Op(Nâˆ’3/2)
GaussL

y;Î¼, Î£
 
0
GaussL(y;Î¼âˆ—,Î£âˆ—)
= Op(Nâˆ’3/2).
(14.55)
Proof
The (twice of the) left-hand side of Eq. (14.55) can be written as
Ïˆ1 â‰¡2
/
log
GaussL

y; Â´Î¼, Â´Î£
 
+ Op(Nâˆ’3/2)
GaussL

y;Î¼, Î£
 
0
GaussL(y;Î¼âˆ—,Î£âˆ—)
=
/
log
det
Î£
 
det( Â´Î£) âˆ’(y âˆ’Â´Î¼)âŠ¤Â´Î£
âˆ’1(y âˆ’Â´Î¼) + (y âˆ’Î¼)âŠ¤Î£
âˆ’1(y âˆ’Î¼)
0
GaussL(y;Î¼âˆ—,Î£âˆ—)
+ Op(Nâˆ’3/2)
= âˆ’log det
!
Î£âˆ—Î£
âˆ’1Î£âˆ—âˆ’1 Â´Î£
"
âˆ’
*
(y âˆ’Î¼âˆ—âˆ’(Â´Î¼ âˆ’Î¼âˆ—))âŠ¤Â´Î£
âˆ’1 (y âˆ’Î¼âˆ—âˆ’(Â´Î¼ âˆ’Î¼âˆ—))
+
GaussL(y;Î¼âˆ—,Î£âˆ—)
+
*$y âˆ’Î¼âˆ—âˆ’(Î¼ âˆ’Î¼âˆ—)%âŠ¤Î£
âˆ’1 $y âˆ’Î¼âˆ—âˆ’(Î¼ âˆ’Î¼âˆ—)%+
GaussL(y;Î¼âˆ—,Î£âˆ—) + Op(Nâˆ’3/2)
= tr
!
log
!
Î£âˆ—Â´Î£
âˆ’1"
âˆ’log
!
Î£âˆ—Î£
âˆ’1""
âˆ’tr
!
Î£âˆ—Â´Î£
âˆ’1"
âˆ’(Â´Î¼ âˆ’Î¼âˆ—)âŠ¤Â´Î£
âˆ’1 (Â´Î¼ âˆ’Î¼âˆ—)
+ tr
!
Î£âˆ—Î£
âˆ’1"
+ $Î¼ âˆ’Î¼âˆ—%âŠ¤Î£
âˆ’1 $Î¼ âˆ’Î¼âˆ—% + Op(Nâˆ’3/2).
By using Eqs. (14.53) and (14.54) and the Taylor expansion of the logarithmic
function, we have
Ïˆ1 = tr
 !
Î£âˆ—Â´Î£
âˆ’1 âˆ’IL
"
âˆ’
!
Î£âˆ—Â´Î£
âˆ’1âˆ’IL
"âŠ¤!
Î£âˆ—Â´Î£
âˆ’1âˆ’IL
"
2
âˆ’
!
Î£âˆ—Î£
âˆ’1 âˆ’IL
"
+
!
Î£âˆ—Î£
âˆ’1âˆ’IL
"âŠ¤!
Î£âˆ—Î£
âˆ’1âˆ’IL
"
2

âˆ’tr
!
Î£âˆ—Â´Î£
âˆ’1"
âˆ’$Î¼ âˆ’Î¼âˆ—%âŠ¤Î£
âˆ’1 $Î¼ âˆ’Î¼âˆ—%

400
14 Asymptotic VB Theory of Reduced Rank Regression
+ tr
!
Î£âˆ—Î£
âˆ’1"
+ $Î¼ âˆ’Î¼âˆ—%âŠ¤Î£
âˆ’1 $Î¼ âˆ’Î¼âˆ—% + Op(Nâˆ’3/2)
= Op(Nâˆ’3/2),
which completes the proof.
â–¡
Given a test input x, Lemma 14.8 can be applied to the true distribution
(14.40), the plug-in VB predictive distribution (14.33), and the predictive
distribution (14.34) when H â‰¥Hâˆ—, where
Î¼âˆ—= Bâˆ—Aâˆ—âŠ¤x,
Î£âˆ—= Ïƒâ€²2IL,
Î¼ = BA
âŠ¤x = Î¼âˆ—+ Op(Nâˆ’1/2),
Î£ = Ïƒâ€²2IL = Î£âˆ—,
Â´Î¼ = Î¨BA
âŠ¤x = Î¼ + Op(Nâˆ’1),
Â´Î£ = Ïƒâ€²2Î¨ = Î£ + Op(Nâˆ’1),
for Î¨ = IL+Op(Nâˆ’1). Here, Lemma 14.7 was used in the equation forÎ¼. Thus,
we have the following corollary:
Corollary 14.9
When H â‰¥Hâˆ—, it holds that
/
log p (y|x, X, Y)
p(y|x, A, B)
0
q(y|x)
= Op(Nâˆ’3/2),
and therefore the difference between the generalization error (13.30) of the VB
predictive distribution (14.34) and the generalization error of the plug-in VB
predictive distribution (14.33) is of the order of Nâˆ’3/2, i.e.,
GE(D) =
/
log
q(y|x)
p(y|x, X, Y)
0
q(y|x)q(x)
=
/
log
q(y|x)
p(y|x, A, B)
0
q(y|x)q(x)
+ Op(Nâˆ’3/2).
Corollary 14.9 leads to the following theorem:
Theorem 14.10
The generalization error of the RRR model is written as
GE(D) =
â§âªâªâªâ¨âªâªâªâ©
Î˜(1)
if
H < Hâˆ—,
####BA
âŠ¤âˆ’Bâˆ—Aâˆ—âŠ¤####
2
Fro
2Ïƒâ€²2
+ Op(Nâˆ’3/2)
if
H â‰¥Hâˆ—.
(14.56)
Proof
When H < Hâˆ—, Theorem 14.5 implies that
GE(D) =
/
log
q(y|x)
p(y|x, A, B)
0
q(y|x)q(x)
+ Op(Nâˆ’1)
=
####BA
âŠ¤âˆ’Bâˆ—Aâˆ—âŠ¤####
2
Fro
2Ïƒâ€²2
+ Op(Nâˆ’1).

14.2 Generalization Properties
401
With Lemma 14.7, we have GE(D) = Î˜(1). When H â‰¥Hâˆ—, we have
GE(D) =
/
log
q(y|x)
p(y|x, A, B)
0
q(y|x)q(x)
+ Op(Nâˆ’3/2)
=
/
âˆ’âˆ¥yâˆ’Bâˆ—Aâˆ—âŠ¤xâˆ¥
2âˆ’
####yâˆ’BA
âŠ¤x
####
2
2Ïƒâ€²2
0
q(y|x)q(x)
+ Op(Nâˆ’3/2)
=
/
âˆ’
âˆ¥yâˆ’Bâˆ—Aâˆ—âŠ¤xâˆ¥
2âˆ’
#####yâˆ’Bâˆ—Aâˆ—âŠ¤xâˆ’
!
BA
âŠ¤âˆ’Bâˆ—Aâˆ—âŠ¤"
x
#####
2
2Ïƒâ€²2
0
q(y|x)q(x)
+ Op(Nâˆ’3/2)
=
/ #####
!
BA
âŠ¤âˆ’Bâˆ—Aâˆ—âŠ¤"
x
#####
2
2Ïƒâ€²2
0
q(x)
+ Op(Nâˆ’3/2)
=
/ tr
)!
BA
âŠ¤âˆ’Bâˆ—Aâˆ—âŠ¤"
xxâŠ¤
!
BA
âŠ¤âˆ’Bâˆ—Aâˆ—âŠ¤"âŠ¤1
2Ïƒâ€²2
0
q(x)
+ Op(Nâˆ’3/2).
By using Eq. (14.44) and Lemma 14.7, we obtain Eq. (14.56), which completes
the proof.
â–¡
Next we compute the average generalization error (13.13) over the distri-
bution of training samples. As Theorem 14.10 states, the generalization error
never converges to zero if H < Hâˆ—, since a rank Hâˆ—matrix cannot be well
approximated by a rank H matrix. Accordingly, we hereafter focus on the
case where H â‰¥Hâˆ—. By WishartD(V, Î½) we denote the D-dimensional Wishart
distribution with scale matrix V and degree of freedom Î½. Then we have the
following theorem:
Theorem 14.11
The average generalization error of the RRR model for
H â‰¥Hâˆ—is asymptotically expanded as
GE(N) = âŸ¨GE(D)âŸ©q(D) = Î»VBNâˆ’1 + O(Nâˆ’3/2),
where the generalization coefï¬cient is given by
2Î»VB = (Hâˆ—(L + M) âˆ’Hâˆ—2)
+
/Hâˆ’Hâˆ—

h=1
Î¸

Î³â€²2
h > max(L, M)
 !
1 âˆ’max(L,M)
Î³â€²2
h
"2
Î³â€²2
h
0
q(W)
.
(14.57)
Here Î³â€²2
h is the hth largest eigenvalue of a random matrix W âˆˆSmin(L,M)
+
subject
to Wishartmin(L,M)âˆ’Hâˆ—(Imin(L,M)âˆ’Hâˆ—, max(L, M) âˆ’Hâˆ—), and Î¸(Â·) is the indicator
function such that Î¸(condition) = 1 if the condition is true and Î¸(condition) = 0
otherwise.

402
14 Asymptotic VB Theory of Reduced Rank Regression
Proof
Theorem 14.1 and Eqs. (14.42) and (14.48) imply that
Î³VB
h
=
â§âªâªâªâªâªâªâªâ¨âªâªâªâªâªâªâªâ©
Î³h + Op(Nâˆ’1) = Op(1)
for h = 1,. . . , Hâˆ—,
max
!
0, Î³h
!
1 âˆ’max(L,M)Ïƒâ€²2
NÎ³2
h
""
+ Op(Nâˆ’1) = Op(Nâˆ’1/2)
for h = Hâˆ—+ 1,. . . , H.
(14.58)
Therefore, we have
####BA
âŠ¤âˆ’Bâˆ—Aâˆ—âŠ¤####
2
Fro =
####
H
h=1Î³VB
h Ï‰bhÏ‰âŠ¤
ah âˆ’min(L,M)
h=1
Î³âˆ—
hÏ‰âˆ—
bhÏ‰âˆ—âŠ¤
ah
####
2
Fro
=
####
Hâˆ—
h=1

Î³hÏ‰bhÏ‰âŠ¤
ah âˆ’Î³âˆ—
hÏ‰âˆ—
bhÏ‰âˆ—âŠ¤
ah
 
+ H
h=Hâˆ—+1Î³VB
h Ï‰bhÏ‰âŠ¤
ah + Op(Nâˆ’1)
####
2
Fro
=
####
Hâˆ—
h=1

Î³hÏ‰bhÏ‰âŠ¤
ah âˆ’Î³âˆ—
hÏ‰âˆ—
bhÏ‰âˆ—âŠ¤
ah
 
+ H
h=Hâˆ—+1Î³VB
h Ï‰bhÏ‰âŠ¤
ah
####
2
Fro + Op(Nâˆ’3/2)
=
####V âˆ’Bâˆ—Aâˆ—+ H
h=Hâˆ—+1Î³VB
h Ï‰bhÏ‰âŠ¤
ah âˆ’min(L,M)
h=Hâˆ—+1 Î³hÏ‰bhÏ‰âŠ¤
ah
####
2
Fro
+ Op(Nâˆ’3/2).
Here, in order to get the third equation, we used the fact that the ï¬rst two
terms in the norm in the second equation are of the order of Op(Nâˆ’1/2). The
expectation over the distribution of training samples is given by
*####BA
âŠ¤âˆ’Bâˆ—Aâˆ—âŠ¤####
2
Fro
+
q(D)
=
*####V âˆ’Bâˆ—Aâˆ—+ H
h=Hâˆ—+1Î³VB
h Ï‰bhÏ‰âŠ¤
ah âˆ’min(L,M)
h=Hâˆ—+1 Î³hÏ‰bhÏ‰âŠ¤
ah
####
2
Fro
+
q(D)
+ O(Nâˆ’3/2)
=

âˆ¥V âˆ’Bâˆ—Aâˆ—âˆ¥2
Fro

q(D)
+ 2

(V âˆ’Bâˆ—Aâˆ—)âŠ¤H
h=Hâˆ—+1Î³VB
h Ï‰bhÏ‰âŠ¤
ah âˆ’min(L,M)
h=Hâˆ—+1 Î³hÏ‰bhÏ‰âŠ¤
ah
 
q(D)
+
H
h=Hâˆ—+1(Î³VB
h )2 âˆ’2 H
h=Hâˆ—+1 Î³hÎ³VB
h
+ min(L,M)
h=Hâˆ—+1 Î³2
h

q(D)
+ O(Nâˆ’3/2)
=

âˆ¥V âˆ’Bâˆ—Aâˆ—âˆ¥2
Fro

q(D) + 2
H
h=Hâˆ—+1 Î³hÎ³VB
h
âˆ’min(L,M)
h=Hâˆ—+1 Î³2
h

q(D)
+
H
h=Hâˆ—+1(Î³VB
h )2 âˆ’2 H
h=Hâˆ—+1 Î³hÎ³VB
h
+ min(L,M)
h=Hâˆ—+1 Î³2
h

q(D) + O(Nâˆ’3/2)
=

âˆ¥V âˆ’Bâˆ—Aâˆ—âˆ¥2
Fro

q(D) âˆ’
min(L,M)
h=Hâˆ—+1 Î³2
h

q(D)
+
H
h=Hâˆ—+1(Î³VB
h )2
q(D) + O(Nâˆ’3/2).
(14.59)
Here we used Eq. (14.49) and the orthonormality of the singular vectors.

14.2 Generalization Properties
403
Eq. (14.45) implies that the ï¬rst term in Eq. (14.59) is equal to

âˆ¥V âˆ’Bâˆ—Aâˆ—âˆ¥2
Fro

q(D) = LM Ïƒâ€²2
N .
(14.60)
The redundant components {Î³hÏ‰bhÏ‰âŠ¤
ah}min(L,M)
h=Hâˆ—+1 are zero-mean (see Eq. (14.49))
Gaussian matrices capturing the Gaussian noise in the orthogonal space to
the necessary components {Î³hÏ‰bhÏ‰âŠ¤
ah}Hâˆ—
h=1. Therefore, the distribution of the
corresponding singular values {Î³h}min(L,M)
h=Hâˆ—+1 coincides with the distribution of
the singular values of Vâ€² âˆˆR(min(L,M)âˆ’Hâˆ—)Ã—(max(L,M)âˆ’Hâˆ—) subject to
q(Vâ€²) = MGaussmin(L,M)âˆ’Hâˆ—,max(L,M)âˆ’Hâˆ—

Vâ€²; 0min(L,M)âˆ’Hâˆ—,max(L,M)âˆ’Hâˆ—, Ïƒâ€²2
N Imin(L,M)âˆ’Hâˆ—âŠ—Imax(L,M)âˆ’Hâˆ—

.
(14.61)
This leads to
/min(L,M)

h=Hâˆ—+1
Î³2
h
0
q(D)
= (L âˆ’Hâˆ—)(M âˆ’Hâˆ—)Ïƒâ€²2
N .
(14.62)
Let {Î³â€²
h}min(L,M)âˆ’Hâˆ—
h=1
be the singular values of
âˆš
N
Ïƒâ€² Vâ€². Then, {Î³â€²2
h }min(L,M)âˆ’Hâˆ—
h=1
are the eigenvalues of W =
N
Ïƒâ€²2 Vâ€²Vâ€²âŠ¤, which is subject to Wishartmin(L,M)âˆ’Hâˆ—
(Imin(L,M)âˆ’Hâˆ—, max(L, M) âˆ’Hâˆ—). By substituting Eqs. (14.60), (14.62), and
(14.58) into Eq. (14.59), we have
*####BA
âŠ¤âˆ’Bâˆ—Aâˆ—âŠ¤####
2
Fro
+
q(D)
= Ïƒâ€²2
N {LM âˆ’(L âˆ’Hâˆ—)(M âˆ’Hâˆ—)}
+
/H
h=Hâˆ—+1
2
max
!
0, Î³h
!
1 âˆ’max(L,M)Ïƒâ€²2
NÎ³2
h
""320
q(D)
+ O(Nâˆ’3/2)
= Ïƒâ€²2
N
â§âªâªâ¨âªâªâ©(Hâˆ—(L + M) âˆ’Hâˆ—2) +
/Hâˆ’Hâˆ—
h=1
2
max
!
0, 1 âˆ’max(L,M)
Î³â€²2
h
"32
Î³â€²2
h
0
q(W)
â«âªâªâ¬âªâªâ­
+ O(Nâˆ’3/2).
Substituting the preceding into Eq. (14.56) completes the proof.
â–¡
The ï¬rst and the second terms in Eq. (14.57) correspond to the contribution
from the necessary components h = 1,. . . , Hâˆ—and the contribution from the
redundant components h = Hâˆ—+ 1,. . . , H, respectively. If we focus on the
parameter space of the ï¬rst Hâˆ—components, i.e., {ah, bh}Hâˆ—
h=1, the true linear
mapping {aâˆ—
h, bâˆ—
h}Hâˆ—
h=1 lies at an (essentially) nonsingular point (after removing

404
14 Asymptotic VB Theory of Reduced Rank Regression
the trivial Hâˆ—2 redundancy). Therefore, as the regular learning theory states,
the contribution from the necessary components is equal to the (essential)
degree of freedom (see Eq. (14.4)) of the RRR model for H = Hâˆ—. On the
other hand, the regular learning theory cannot be applied to the redundant
components {ah, bh}H
h=Hâˆ—+1 since the true parameter is on the singularities
{ah = 0} âˆª{bh = 0}, making the second term different from the degree of
freedom of the redundant parameters.
Assuming that L and M are large, we can approximate the second term in
Eq. (14.57) by using the random matrix theory (see Section 8.4.1). Consider
the large-scale limit when L, M, H, Hâˆ—go to inï¬nity with the same ratio, so that
Î± = min(L, M) âˆ’Hâˆ—
max(L, M) âˆ’Hâˆ—,
(14.63)
Î² =
H âˆ’Hâˆ—
min(L, M) âˆ’Hâˆ—,
(14.64)
Îº =
max(L, M)
max(L, M) âˆ’Hâˆ—
(14.65)
are constant. Then MarË‡cenkoâ€“Pastur law (Proposition 8.11) states that the
empirical distribution of the eigenvalues {y1,. . . , ymin(L,M)âˆ’Hâˆ—} of the random
matrix
NVâ€²Vâ€²âŠ¤
(max(L,M)âˆ’Hâˆ—)Ïƒâ€²2
âˆ¼
Wishartmin(L,M)âˆ’Hâˆ—(Imin(L,M)âˆ’Hâˆ—, 1) almost surely
converges to
p(y) â†’pMP(y) â‰¡
.
(y âˆ’y)(y âˆ’y)
2Ï€Î±y
Î¸

y < y < y
 
,
(14.66)
where
y = (1 + âˆšÎ±)2,
y = (1 âˆ’âˆšÎ±)2.
(14.67)
Let
(2Ï€Î±)âˆ’1Jâ€²
s(u) =
 âˆ
u
ysp(y)dy
(14.68)
be the sth order (incomplete) moment of the MarË‡cenkoâ€“Pastur distribution
(14.66) with the lower bound u of the integration range. Then, the second term
of Eq. (14.57) can be written as
/Hâˆ’Hâˆ—

h=1
Î¸(Î³â€²2
h > max(L, M))
!
1 âˆ’max(L,M)
Î³â€²2
h
"2
Î³â€²2
h
0
q(W)
â†’(min(L, M) âˆ’Hâˆ—)(max(L, M) âˆ’Hâˆ—)
 âˆ
uÎ²
Î¸ (y > Îº)

1 âˆ’Îº
y
 2 yp(y)dy
= (min(L, M) âˆ’Hâˆ—)(max(L, M) âˆ’Hâˆ—)
 âˆ
max(Îº,uÎ²)

y âˆ’2Îº + Îº2yâˆ’1 
p(y)dy
= (min(L, M) âˆ’Hâˆ—)(max(L, M) âˆ’Hâˆ—)
2Ï€Î±

Jâ€²
1(Â´u) âˆ’2ÎºJâ€²
0(Â´u) + Îº2Jâ€²
âˆ’1(Â´u)
 
,

14.2 Generalization Properties
405
where uÎ² is the Î²-percentile point of p(y), i.e.,
Î² =
 âˆ
uÎ²
p(y)dy = (2Ï€Î±)âˆ’1Jâ€²
0(uÎ²),
(14.69)
and
Â´u = max(Îº, uÎ²).
(14.70)
Using the transformation z =

y âˆ’(y + y)/2
 
/(2 âˆšÎ±), we can derive analytic
forms of the moments (14.68) and thus obtain the following theorem:
Theorem 14.12
The VB generalization coefï¬cient of the RRR model in the
large-scale limit is given by
2Î»VB â†’(Hâˆ—(L + M) âˆ’Hâˆ—2)
+ (min(L, M) âˆ’Hâˆ—)(max(L, M) âˆ’Hâˆ—)
2Ï€Î±
,
J1(Â´z) âˆ’2ÎºJ0(Â´z) + Îº2Jâˆ’1(Â´z)
-
,
(14.71)
where
J1(z) = 2Î±(âˆ’z
C
1 âˆ’z2 + cosâˆ’1 z),
J0(z) = âˆ’2 âˆšÎ±
C
1 âˆ’z2 + (1 + Î±) cosâˆ’1 z âˆ’(1 âˆ’Î±) cosâˆ’1
âˆšÎ±(1 + Î±)z + 2Î±
2Î±z + âˆšÎ±(1 + Î±),
Jâˆ’1(z) =
â§âªâªâªâ¨âªâªâªâ©
2 âˆšÎ±
âˆš
1âˆ’z2
2 âˆšÎ±z+1+Î± âˆ’cosâˆ’1z+ 1+Î±
1âˆ’Î± cosâˆ’1
âˆšÎ±(1+Î±)z+2Î±
2Î±z+ âˆšÎ±(1+Î±)
(0 < Î± < 1),
2
.
1âˆ’z
1+z âˆ’cosâˆ’1 z
(Î± = 1),
and Â´z = max

(Îº âˆ’(1 + Î±))/2 âˆšÎ±, Jâˆ’1
0 (2Ï€Î±Î²)
 
. Here Jâˆ’1
s (Â·) denotes the inverse
function of Js(z).
Theorem 14.12 allows us to compare the generalization error of VB learning
with those of ML (MAP) learning and Bayesian learning in Section 14.2.6.
14.2.4 Training Error
The training error can be analyzed in a similar way to the generalization error.
We ï¬rst prove the following lemma:
Lemma 14.13
Let U âˆˆRL,M and Î£ âˆˆSL
+ be the ML estimators of the linear
regression model y = Ux + Îµ with Gaussian noise Îµ âˆ¼Gauss(0, Î£). For any
two sets of parameters (U, Î£), ( Â´U, Â´Î£) such that
U = U + Op(Nâˆ’1/2),
Î£ = Î£ + Op(Nâˆ’1/2),
(14.72)
Â´U = U + Op(Nâˆ’1),
Â´Î£ = Î£ + Op(Nâˆ’1),
(14.73)

406
14 Asymptotic VB Theory of Reduced Rank Regression
it holds that
1
N
N

n=1
log
GaussL

y(n); Â´Ux(n), Â´Î£
 
+ Op(Nâˆ’3/2)
GaussL

y(n); Ux(n), Î£
 
= Op(Nâˆ’3/2).
(14.74)
Proof
The (twice of the) left-hand side of Eq. (14.74) can be written as
Ïˆ2 â‰¡2
N
N

n=1
log
GaussL

y(n); Â´Ux(n), Â´Î£
 
+ Op(Nâˆ’3/2)
GaussL

y(n); Ux(n), Î£
 
= log
det
Î£
 
det
 Â´Î£
 + 1
N
N

n=1
!
âˆ’(y(n) âˆ’Â´Ux(n))âŠ¤Â´Î£
âˆ’1(y(n) âˆ’Â´Ux(n))
+ (y(n) âˆ’Ux(n))âŠ¤Î£
âˆ’1(y(n) âˆ’Ux(n))
"
+ Op(Nâˆ’3/2)
= âˆ’log det
!
Î£Î£
âˆ’1Î£
âˆ’1 Â´Î£
"
+ Op(Nâˆ’3/2)
âˆ’1
N
N

n=1

y(n) âˆ’Ux(n) âˆ’( Â´U âˆ’U)x(n) âŠ¤Â´Î£
âˆ’1 
y(n) âˆ’Ux(n) âˆ’( Â´U âˆ’U)x(n) 
+ 1
N
N

n=1

y(n) âˆ’Ux(n) âˆ’(U âˆ’U)x(n) âŠ¤Î£
âˆ’1 
y(n) âˆ’Ux(n) âˆ’(U âˆ’U)x(n) 
= tr
!
log
!
Î£ Â´Î£
âˆ’1"
âˆ’log
!
Î£Î£
âˆ’1""
+ Op(Nâˆ’3/2)
âˆ’tr
!
Î£ Â´Î£
âˆ’1"
âˆ’1
N
N

n=1

( Â´U âˆ’U)x(n) âŠ¤Â´Î£
âˆ’1 
( Â´U âˆ’U)x(n) 
+ tr
!
Î£Î£
âˆ’1"
+ 1
N
N

n=1

(U âˆ’U)x(n) âŠ¤Î£
âˆ’1 
(U âˆ’U)x(n) 
.
By using Eqs. (14.72) and (14.73) and the Taylor expansion of the logarithmic
function, we have
Ïˆ2 = tr
â›âœâœâœâœâœâ
!
Î£ Â´Î£
âˆ’1 âˆ’IL
"
âˆ’
!
Î£ Â´Î£
âˆ’1âˆ’IL
"âŠ¤!
Î£ Â´Î£
âˆ’1âˆ’IL
"
2
âˆ’
!
Î£Î£
âˆ’1 âˆ’IL
"
+
!
Î£Î£
âˆ’1âˆ’IL
"âŠ¤!
Î£Î£
âˆ’1âˆ’IL
"
2
ââŸâŸâŸâŸâŸâ 
âˆ’tr
!
Î£ Â´Î£
âˆ’1"
âˆ’1
N
N

n=1

(U âˆ’U)x(n) âŠ¤Â´Î£
âˆ’1 
(U âˆ’U)x(n) 
+ tr
!
Î£Î£
âˆ’1"
+ 1
N
N

n=1

(U âˆ’U)x(n) âŠ¤Î£
âˆ’1 
(U âˆ’U)x(n) 
+ Op(Nâˆ’3/2)
= Op(Nâˆ’3/2),
which completes the proof.
â–¡

14.2 Generalization Properties
407
When H â‰¥Hâˆ—, Lemma 14.13 can be applied to the plug-in VB predictive
distribution (14.33) and the VB predictive distribution (14.34), where
U = V,
Î£ = Ïƒâ€²2
N
N

n=1

y(n) âˆ’Vx(n) 
y(n) âˆ’Vx(n) âŠ¤
= Ïƒâ€²2IL + Op(Nâˆ’1/2),
U = BA
âŠ¤= U + Op(Nâˆ’1/2),
Î£ = Ïƒâ€²2IL = Î£ + Op(Nâˆ’1/2),
Â´U = Î¨BA
âŠ¤= U + Op(Nâˆ’1),
Â´Î£ = Ïƒâ€²2Î¨ = Î£ + Op(Nâˆ’1),
for Î¨ = IL + Op(Nâˆ’1). Here Eq. (14.43) and Lemma 14.7 were used in the
equation for U. Thus, we have the following corollary:
Corollary 14.14
When H â‰¥Hâˆ—, it holds that
1
N
N

n=1
log
p

y(n)|x(n), X, Y
 
p

y(n)|x(n), A, B
 = Op(Nâˆ’3/2),
and therefore the difference between the training error (13.31) of the VB
predictive distribution (14.34) and the training error of the plug-in VB
predictive distribution (14.33) is of the order of Nâˆ’3/2, i.e.,
TE(D) = 1
N
N

n=1
log
q(y(n)|x(n))
p(y(n)|x(n), X, Y)
= 1
N
N

n=1
log
q(y(n)|x(n))
p(y(n)|x(n), A, B)
+ Op(Nâˆ’3/2).
Corollary 14.14 leads to the following theorem:
Theorem 14.15
The training error of the RRR model is written as
TE(D) =
â§âªâªâªâ¨âªâªâªâ©
Î˜(1)
if
H < Hâˆ—,
####Vâˆ’BA
âŠ¤####
2
Froâˆ’âˆ¥Vâˆ’Bâˆ—Aâˆ—âŠ¤âˆ¥
2
Fro
2Ïƒâ€²2
+ Op(Nâˆ’3/2)
if
H â‰¥Hâˆ—.
(14.75)
Proof
When H < Hâˆ—, Theorem 14.5 implies that
TE(D) = 1
N
N

n=1
log
q(y(n)|x(n))
p(y(n)|x(n), A, B)
+ Op(Nâˆ’1)
=
####V âˆ’BA
âŠ¤####
2
Fro âˆ’
###V âˆ’Bâˆ—Aâˆ—âŠ¤###2
Fro
2Ïƒâ€²2
+ Op(Nâˆ’1).

408
14 Asymptotic VB Theory of Reduced Rank Regression
With Lemma 14.7, we have TE(D) = Î˜(1). When H â‰¥Hâˆ—, we have
TE(D) = âˆ’1
N
N
n=1
âˆ¥y(n)âˆ’Bâˆ—Aâˆ—âŠ¤x(n)âˆ¥
2âˆ’
####y(n)âˆ’BA
âŠ¤x(n)####
2
2Ïƒâ€²2
+ Op(Nâˆ’3/2)
= âˆ’1
N
N
n=1
âˆ¥y(n)âˆ’Vx(n)âˆ’(Bâˆ—Aâˆ—âŠ¤âˆ’V)x(n)âˆ¥
2âˆ’
#####y(n)âˆ’Vx(n)âˆ’
!
BA
âŠ¤âˆ’V
"
x(n)
#####
2
2Ïƒâ€²2
+ Op(Nâˆ’3/2)
= âˆ’1
N
N
n=1
âˆ¥(Bâˆ—Aâˆ—âŠ¤âˆ’V)x(n)âˆ¥
2âˆ’
#####
!
BA
âŠ¤âˆ’V
"
x(n)
#####
2
2Ïƒâ€²2
+ Op(Nâˆ’3/2)
= âˆ’1
N
N
n=1
tr
)
(Bâˆ—Aâˆ—âŠ¤âˆ’V)x(n)x(n)âŠ¤(Bâˆ—Aâˆ—âŠ¤âˆ’V)
âŠ¤âˆ’
!
BA
âŠ¤âˆ’V
"
x(n)x(n)âŠ¤
!
BA
âŠ¤âˆ’V
"âŠ¤1
2Ïƒâ€²2
+ Op(Nâˆ’3/2).
By using the prewhitening condition (14.7) and Lemma 14.7, we obtain
Eq. (14.75), which completes the proof.
â–¡
Now we can derive an asymptotic form of the average training error:
Theorem 14.16
The average training error of the RRR model for H â‰¥Hâˆ—is
asymptotically expanded as
TE(N) = âŸ¨TE(D)âŸ©q(D) = Î½VBNâˆ’1 + O(Nâˆ’3/2),
where the training coefï¬cient is given by
2Î½VB = âˆ’(Hâˆ—(L + M) âˆ’Hâˆ—2)
âˆ’
/Hâˆ’Hâˆ—

h=1
Î¸

Î³â€²2
h > max(L, M)
 !
1 âˆ’max(L,M)
Î³â€²2
h
" !
1 + max(L,M)
Î³â€²2
h
"
Î³â€²2
h
0
q(W)
.
(14.76)
Here Î³â€²2
h is the hth largest eigenvalue of a random matrix W âˆˆSmin(L,M)
+
subject
to Wishartmin(L,M)âˆ’Hâˆ—(Imin(L,M)âˆ’Hâˆ—, max(L, M) âˆ’Hâˆ—).
Proof
From Eq. (14.58), we have
####BA
âŠ¤âˆ’V
####
2
Fro =
####
H
h=1Î³VB
h Ï‰bhÏ‰âŠ¤
ah âˆ’min(L,M)
h=1
Î³hÏ‰bhÏ‰âŠ¤
ah
####
2
Fro
=
####
H
h=Hâˆ—+1(Î³VB
h
âˆ’Î³h)Ï‰bhÏ‰âŠ¤
ah âˆ’min(L,M)
h=H+1
Î³hÏ‰bhÏ‰âŠ¤
ah + Op(Nâˆ’1)
####
2
Fro
=
####
H
h=Hâˆ—+1(Î³VB
h
âˆ’Î³h)Ï‰bhÏ‰âŠ¤
ah âˆ’min(L,M)
h=H+1
Î³hÏ‰bhÏ‰âŠ¤
ah
####
2
Fro + Op(Nâˆ’3/2)
= H
h=Hâˆ—+1(Î³VB
h
âˆ’Î³h)2 + min(L,M)
h=H+1
Î³2
h + Op(Nâˆ’3/2)
= H
h=Hâˆ—+1
!
max
!
0, Î³h
!
1 âˆ’max(L,M)Ïƒâ€²2
NÎ³2
h
""
âˆ’Î³h
"2
+ min(L,M)
h=H+1
Î³2
h + Op(Nâˆ’3/2)

14.2 Generalization Properties
409
= âˆ’H
h=Hâˆ—+1 Î¸

Î³2
h > max(L,M)Ïƒâ€²2
N
 
Â·

Î³h âˆ’max(L,M)Ïƒâ€²2
NÎ³h
 
Î³h + max(L,M)Ïƒâ€²2
NÎ³h
 
+ min(L,M)
h=Hâˆ—+1 Î³2
h + Op(Nâˆ’3/2).
(14.77)
By using Eqs. (14.60), (14.62) and (14.77), we have
*####V âˆ’BA
âŠ¤####
2
Fro âˆ’
###V âˆ’Bâˆ—Aâˆ—âŠ¤###2
Fro
+
q(D)
= âˆ’Ïƒâ€²2
N
,
(Hâˆ—(L + M) âˆ’Hâˆ—2)
+ H
h=Hâˆ—+1 Î¸

Î³2
h > max(L,M)Ïƒâ€²2
N
 
Â·

Î³h âˆ’max(L,M)Ïƒâ€²2
NÎ³h
 
Î³h + max(L,M)Ïƒâ€²2
NÎ³h
 -
+ Op(Nâˆ’3/2).
Thus, by introducing the singular values {Î³â€²
h}min(L,M)âˆ’Hâˆ—
h=1
of
âˆš
N
Ïƒâ€² Vâ€², where Vâ€² is
a random matrix subject to Eq. (14.61), and using Theorem 14.15, we obtain
Eq. (14.76), which completes the proof.
â–¡
Finally, we apply the MarË‡cenkoâ€“Pastur law (Proposition 8.11) for evaluat-
ing the second term in Eq. (14.76). In the large-scale limit when L, M, H, Hâˆ—
go to inï¬nity with the same ratio, so that Eqs. (14.63) through (14.65) are
constant, we have
/Hâˆ’Hâˆ—

h=1
Î¸

Î³â€²2
h > max(L, M)
 !
1 âˆ’max(L,M)
Î³â€²2
h
" !
1 + max(L,M)
Î³â€²2
h
"
Î³â€²2
h
0
q(W)
â†’(min(L, M) âˆ’Hâˆ—)(max(L, M) âˆ’Hâˆ—)
 âˆ
uÎ²
Î¸ (y > Îº)

1 âˆ’Îº
y
 
1 + Îº
y
 
yp(y)dy
= (min(L, M) âˆ’Hâˆ—)(max(L, M) âˆ’Hâˆ—)
 âˆ
max(Îº,uÎ²)

y âˆ’Îº2yâˆ’1 
p(y)dy
= (min(L, M) âˆ’Hâˆ—)(max(L, M) âˆ’Hâˆ—)
2Ï€Î±

Jâ€²
1(Â´u) âˆ’Îº2Jâ€²
âˆ’1(Â´u)
 
,
where Jâ€²
s(u), Î², and Â´u are deï¬ned in Eqs. (14.68), (14.69), and (14.70),
respectively. Thus, the transformation z =

y âˆ’(y + y)/2
 
/(2 âˆšÎ±) gives the
following theorem:
Theorem 14.17
The VB training coefï¬cient of the RRR model in the large
scale limit is given by
2Î½VB â†’âˆ’(Hâˆ—(L + M) âˆ’Hâˆ—2)
âˆ’(min(L, M) âˆ’Hâˆ—)(max(L, M) âˆ’Hâˆ—)
2Ï€Î±
,
J1(Â´z) âˆ’Îº2Jâˆ’1(Â´z)
-
,
(14.78)
where J1(z), Jâˆ’1(z), and Â´z are deï¬ned in Theorem 14.12.

410
14 Asymptotic VB Theory of Reduced Rank Regression
14.2.5 Free Energy
The VB free energy can be analyzed relatively easily based on the orders of
the variational parameters, given by Corollary 14.3:
Theorem 14.18
The relative VB free energy (13.150) of the RRR model for
H â‰¥Hâˆ—is asymptotically expanded as
FVB(D) = FVB(Y|X) âˆ’NS N(Y|X) = Î»â€²VB log N + Op(1),
(14.79)
where the free energy coefï¬cient is given by
2Î»â€²VB = Hâˆ—(L + M) + (H âˆ’Hâˆ—) min(L, M).
(14.80)
Proof
The VB free energy for the RRR model is given by Eq. (14.14), and
the empirical entropy is given by
2S N(Y|X) = âˆ’2
N
N

n=1
log q(y(n)|x(n))
= L log(2Ï€Ïƒâ€²2) +
N
n=1
###y(n) âˆ’Bâˆ—Aâˆ—âŠ¤x(n)###2
NÏƒâ€²2
= L log(2Ï€Ïƒâ€²2) +
1
N
N
n=1
###y(n)###2 âˆ’2tr(VâŠ¤Bâˆ—Aâˆ—âŠ¤) +
###Bâˆ—Aâˆ—âŠ¤###2
Fro
Ïƒâ€²2
= L log(2Ï€Ïƒâ€²2) +
1
N
N
n=1
###y(n)###2 +
###V âˆ’Bâˆ—Aâˆ—âŠ¤###2
Fro âˆ’âˆ¥Vâˆ¥2
Fro
Ïƒâ€²2
.
Therefore, the relative VB free energy (14.79) is given as
2FVB(D) = N Â·
####Vâˆ’BA
âŠ¤####
2
Froâˆ’âˆ¥Vâˆ’Bâˆ—Aâˆ—âŠ¤âˆ¥
2
Fro
Ïƒâ€²2
+ M log det(CA)
det
Î£A
 + L log det(CB)
det
Î£B
 
âˆ’(L + M)H + tr
2
Câˆ’1
A
!
A
âŠ¤A + MÎ£A
"
+ Câˆ’1
B
!
B
âŠ¤B + LÎ£B
"
+ NÏƒâ€²âˆ’2 !
âˆ’A
âŠ¤AB
âŠ¤B +
!
A
âŠ¤A + MÎ£A
" !
B
âŠ¤B + LÎ£B
""3
.
(14.81)
Eqs. (14.52) and (14.43) imply that the ï¬rst term in Eq. (14.81) is Op(1).
Corollary 14.3 with Eqs. (14.42) and (14.48) implies that, for h = 1,. . . , Hâˆ—,
ah =Î˜p(1),
bh =Î˜p(1),
Ïƒ2
ah =Î˜p(Nâˆ’1),
Ïƒ2
bh =Î˜p(Nâˆ’1),
and, for h = Hâˆ—+ 1,. . . , H,
ah =Op(1),
bh =Op(Nâˆ’1/2), Ïƒ2
ah =Î˜p(1),
Ïƒ2
bh =Î˜p(Nâˆ’1),
(if L< M),
ah =Op(Nâˆ’1/4), bh =Op(Nâˆ’1/4), Ïƒ2
ah =Î˜p(Nâˆ’1/2), Ïƒ2
bh =Î˜p(Nâˆ’1/2), (if L= M),
ah =Op(Nâˆ’1/2), bh =Op(1),
Ïƒ2
ah =Î˜p(Nâˆ’1),
Ïƒ2
bh =Î˜p(1),
(if L> M).

14.2 Generalization Properties
411
These results imply that the most terms in Eq. (14.81) are Op(1), and we thus
have
2FVB(D) = M log det (CA)
det
Î£A
 + L log det (CB)
det
Î£B
 + Op(1)
= M log
H

h=1
Ïƒâˆ’2
ah + L log
H

h=1
Ïƒâˆ’2
bh + Op(1)
= {Hâˆ—(L + M) + (H âˆ’Hâˆ—) min(L, M)} log N + Op(1),
which completes the proof.
â–¡
Clearly from the proof, the ï¬rst term and the second term in Eq. (14.80) cor-
respond to the contribution from the necessary components, h = 1,. . . , Hâˆ—, and
the contribution from the redundant components, h = Hâˆ—,. . . , H, respectively.
A remark is that the contribution from the necessary components contains the
trivial redundancy, i.e., it is Hâˆ—(L + M) instead of Hâˆ—(L + M) âˆ’Hâˆ—2. This
is because the independence between A and B prevents the VB posterior
distribution from extending along the trivial redundancy.
14.2.6 Comparison with Other Learning Algorithms
Theorems 14.12, 14.17, and 14.18 allow us to compute the generalization, the
training, and the free energy coefï¬cients of VB learning. We can now compare
those properties with those of ML learning and Bayesian learning, which have
been clariï¬ed for the RRR model. Note that MAP learning with a smooth
and ï¬nite prior (e.g., the Gaussian prior (14.3)) with ï¬xed hyperparameters
is asymptotically equivalent to ML learning, and has the same generalization
and training coefï¬cients.
ML Learning
The generalization error of ML learning in the RRR model was analyzed
(Fukumizu, 1999), based on the MarË‡cenkoâ€“Pastur law (Proposition 8.11). Let
Î³â€²2
h be the hth largest eigenvalue of a random matrix W âˆˆSmin(L,M)
+
subject to
Wishartmin(L,M)âˆ’Hâˆ—(Imin(L,M)âˆ’Hâˆ—, max(L, M) âˆ’Hâˆ—).
Theorem 14.19
(Fukumizu, 1999) The average ML generalization error of
the RRR model for H â‰¥Hâˆ—is asymptotically expanded as
GE
ML(N) = Î»MLNâˆ’1 + O(Nâˆ’3/2),
where the generalization coefï¬cient is given by
2Î»ML = (Hâˆ—(L + M) âˆ’Hâˆ—2) +
/Hâˆ’Hâˆ—

h=1
Î³â€²2
h
0
q(W)
.
(14.82)

412
14 Asymptotic VB Theory of Reduced Rank Regression
Theorem 14.20
(Fukumizu, 1999) The ML generalization coefï¬cient of the
RRR model in the large-scale limit is given by
2Î»ML â†’(Hâˆ—(L + M) âˆ’Hâˆ—2)
+ (min(L, M) âˆ’Hâˆ—)(max(L, M) âˆ’Hâˆ—)
2Ï€Î±
J1(Â´z),
(14.83)
where J1(Â·) and Â´z are deï¬ned in Theorem 14.12.
Actually, Theorems 14.11 and 14.12 were derived by extending Theorems
14.19 and 14.20 to VB learning. We can derive Theorems 14.19 and 14.20 in
the same way as VB learning by replacing the VB estimator (14.58) with the
ML estimator Î³ML
h
= Î³h.
The training error can be similarly analyzed.
Theorem 14.21
The average ML training error of the RRR model for H â‰¥Hâˆ—
is asymptotically expanded as
TE
ML(N) = Î½MLNâˆ’1 + O(Nâˆ’3/2),
where the training coefï¬cient is given by
2Î½ML = âˆ’(Hâˆ—(L + M) âˆ’Hâˆ—2) âˆ’
/Hâˆ’Hâˆ—

h=1
Î³â€²2
h
0
q(W)
.
(14.84)
Theorem 14.22
The ML training coefï¬cient of the RRR model in the large-
scale limit is given by
2Î½ML â†’âˆ’(Hâˆ—(L + M) âˆ’Hâˆ—2)
âˆ’(min(L, M) âˆ’Hâˆ—)(max(L, M) âˆ’Hâˆ—)
2Ï€Î±
J1(Â´z),
(14.85)
where J1(Â·) and Â´z are deï¬ned in Theorem 14.12.
A note is that Theorems 14.19 and 14.21 imply that the generalization
coefï¬cient and the training coefï¬cient are antisymmetric in ML learning,
i.e., Î»ML = âˆ’Î½ML, while they are not antisymmetric in VB learning, i.e.,
Î»VB  âˆ’Î½VB (see Theorems 14.11 and 14.16).
Bayesian Learning
The Bayes free energy in the RRR model was clariï¬ed based on the singular
learning theory (see Section 13.5.4).

14.2 Generalization Properties
413
Theorem 14.23
(Aoyagi and Watanabe, 2005) The relative Bayes free energy
(13.32) in the RRR model is asymptotically expanded as
FBayes(D) = FBayes(Y|X) âˆ’NS N(Y|X)
= Î»â€²Bayes log N âˆ’(m âˆ’1) log log N + Op(1),
where the free energy coefï¬cient, as well as the coefï¬cient of the second leading
term, is given as follows:
(i) When L + Hâˆ—â‰¤M + H, M + Hâˆ—â‰¤L + H, and Hâˆ—+ H â‰¤L + M:
(a) If L + M + H + Hâˆ—is even, then m = 1 and
2Î»â€²Bayes = âˆ’(Hâˆ—+ H)2 âˆ’(L âˆ’M)2 + 2(Hâˆ—+ H)(L + M)
4
.
(b) If L + M + H + Hâˆ—is odd, then m = 2 and
2Î»â€²Bayes = âˆ’(Hâˆ—+ H)2 âˆ’(L âˆ’M)2 + 2(Hâˆ—+ H)(L + M) + 1
4
.
(ii) When M + H < L + Hâˆ—, then m = 1 and
2Î»â€²Bayes = HM âˆ’HHâˆ—+ LHâˆ—.
(iii) When L + H < M + Hâˆ—, then m = 1 and
2Î»â€²Bayes = HL âˆ’HHâˆ—+ MHâˆ—.
(iv) When L + M < H + Hâˆ—, then m = 1 and
2Î»â€²Bayes = LM.
Theorem 14.23 immediately informs us of the asymptotic behavior of the
Bayes generalization error, based on Corollary 13.14.
Theorem 14.24
(Aoyagi and Watanabe, 2005) The Bayes generalization
error of the RRR model for H â‰¥Hâˆ—is asymptotically expanded as
GE
Bayes(N) = Î»BayesNâˆ’1 âˆ’(m âˆ’1)(N log N)âˆ’1 + o

(N log N)âˆ’1 
,
where Î»Bayes = Î»â€²Bayes and m are given in Theorem 14.23.
Unfortunately, the Bayes training error has not been clariï¬ed yet.
Numerical Comparison
Let us visually compare the theoretically clariï¬ed generalization properties.
Figures 14.1 through 14.4 show the generalization coefï¬cients and the training
coefï¬cients of the RRR model under the following settings:

414
14 Asymptotic VB Theory of Reduced Rank Regression
â€“2
â€“1.5
â€“1
â€“0.5
0
0.5
1
1.5
2
0
5
10
15
20
25
30
35
40
VB
ML
Bayes
Regular
Figure 14.1 The generalization coefï¬cients (in the positive vertical region) and
the training coefï¬cients (in the negative vertical region) of VB learning, ML
learning, and Bayesian learning in the RRR model with max(L, M) = 50,
min(L, M) = 30, H = 1,. . . , 30, and Hâˆ—= 0.
â€“2
â€“1.5
â€“1
â€“0.5
0
0.5
1
1.5
2
0
20
40
60
80
100
VB
ML
Bayes
Regular
Figure 14.2 The generalization coefï¬cients and the training coefï¬cients
(max(L, M) = 80, min(L, M) = 1,. . . , 80, H = 1, and Hâˆ—= 0).
(i) max(L, M) = 50, min(L, M) = 30, H = 1,. . . , 30 (horizontal axis),
Hâˆ—= 0,
(ii) max(L, M) = 80, min(L, M) = 1,. . . , 80 (horizontal axis), H = 1, Hâˆ—= 0,
(iii) L = M = 80, H = 1,. . . , 80 (horizontal axis), Hâˆ—= 0,
(iv) max(L, M) = 50, min(L, M) = 30, H = 20, Hâˆ—= 1,. . . , 20
(horizontal axis).
The vertical axis indicates the coefï¬cient normalized by the half of the
essential parameter dimension D, given by Eq. (14.4). The curves in the
positive vertical region correspond to the generalization coefï¬cients of VB
learning, ML learning, and Bayesian learning, while the curves in the negative
vertical region correspond to the training coefï¬cients. As a guide, we depicted
the lines 2Î»/D = 1 and 2Î½/D = âˆ’1, which correspond to the generalization and
the training coefï¬cients (by ML learning and Bayesian learning) of the regular

14.2 Generalization Properties
415
â€“2
â€“1.5
â€“1
â€“0.5
0
0.5
1
1.5
2
0
20
40
60
80
100
VB
ML
Bayes
Regular
Figure 14.3 The generalization coefï¬cients and the training coefï¬cients
(L = M = 80, H = 1,. . . , 80, and Hâˆ—= 0).
â€“2
â€“1.5
â€“1
â€“0.5
0
0.5
1
1.5
2
0
5
10
15
20
25
30
VB
ML
Bayes
Regular
Figure 14.4 The generalization coefï¬cients and the training coefï¬cients
(max(L, M) = 50, min(L, M) = 30, H = 20, and Hâˆ—= 1,. . . , 20).
models with the same parameter dimensionality. The curves for ML learning
and VB learning were computed under the large-scale approximation, i.e., by
using Theorems 14.12, 14.17, 14.20, and 14.22.1
We see in Figures 14.1 through 14.4 that VB learning generally pro-
vides comparable generalization performance to Bayesian learning. However,
signiï¬cant differences are also observed. For example, we see in Figure
14.1 that VB learning provides much worse generalization performance than
Bayesian learning when H â‰ªmin(L, M), and much better performance when
H âˆ¼min(L, M).
Another ï¬nding is that, in Figures 14.1 and 14.3, the VB generalization
coefï¬cient depends on H similarly to the ML generalization coefï¬cient.
Moreover, we see that, when min(L, M) = 80 in Figure 14.2 and when H = 1
in Figure 14.3, the VB generalization coefï¬cient slightly exceeds the line
1 We conï¬rmed that numerical computation with Theorems 14.11, 14.16, 14.19, and 14.21 gives
visually indistinguishable results.

416
14 Asymptotic VB Theory of Reduced Rank Regression
2Î»/D = 1â€”the VB generalization coefï¬cient per parameter dimension can
be larger than that in the regular models, which never happens for the Bayes
generalization coefï¬cient (see Eq. (13.125)).
Finally, Figure 14.4 shows that, for this particular RRR model with
max(L, M) = 50, min(L, M) = 30, and H = 20, VB learning always gives
smaller generalization error than Bayesian learning in the asymptotic limit,
regardless of the true rank Hâˆ—. This might be seen contradictory with the
proven optimality of Bayesian learningâ€”Bayesian learning is never dominated
by any other method (see Appendix D for the optimality of Bayesin learning
and Appendix A for the deï¬nition of the term â€œdominationâ€). We further
discuss this issue by considering subtle true singular values in Section 14.2.7.
Next we compare the VB free energy with the Bayes free energy, by using
Theorems 14.18 and 14.23. Figures 14.5 through 14.8 show the free energy
0
0.5
1
1.5
2
0
5
10
15
20
25
30
35
40
VB
Bayes
Regular
Figure 14.5 Free energy coefï¬cients (max(L, M)
=
50, min(L, M)
=
30,
H = 1,. . . , 30, and Hâˆ—= 0). The VB and the Bayes free energy coefï¬cients
are almost overlapped.
0
0.5
1
1.5
2
0
20
40
60
80
100
VB
Bayes
Regular
Figure 14.6 Free energy coefï¬cients (max(L, M) = 80, min(L, M) = 1,. . . , 80,
H = 1, and Hâˆ—= 0). The VB and the Bayes free energy coefï¬cients are almost
overlapped.

14.2 Generalization Properties
417
0
0.5
1
1.5
2
0
20
40
60
80
100
VB
Bayes
Regular
Figure 14.7 Free energy coefï¬cients (L = M = 80, H = 1,. . . , 80, and Hâˆ—= 0).
0
0.5
1
1.5
2
0
5
10
15
20
25
30
VB
Bayes
Regular
Figure 14.8 Free energy coefï¬cients (max(L, M) = 50, min(L, M) = 30, H = 20,
and Hâˆ—= 1,. . . , 20).
coefï¬cients of the RRR model with the same setting as Figures 14.1 through
14.4, respectively. As for the generalization and the training coefï¬cients, the
vertical axis indicates the free energy coefï¬cient normalized by the half of
the essential parameter dimensionality D, given by Eq. (14.4). The curves
correspond to the VB free energy coefï¬cient (Theorem 14.18), the Bayes
free energy coefï¬cient (Theorem 14.23), and the Bayes free energy coefï¬cient
2Î»â€²Bayes
Regular = D of the regular models with the same parameter dimensionality.
We ï¬nd that the VB free energy almost coincides with the Bayes free energy in
Figures 14.5 and 14.6, while the VB free energy is much larger than the Bayes
free energy in Figures 14.7 and 14.8.
Since the gap between the VB free energy and the Bayes free energy
indicates how well the VB posterior approximates the Bayes posterior in terms
of the KL divergence (see Section 13.6), our observation is not exactly what
we would expect. For example, we see in Figure 14.1 that the generalization
performance of VB learning is signiï¬cantly different from Bayesian learning
(when H â‰ªmin(L, M) and when H âˆ¼min(L, M)), while the free energies in

418
14 Asymptotic VB Theory of Reduced Rank Regression
0
0.5
1
1.5
2
0
20
40
60
80
100
VB
Bayes
Regular
Figure 14.9 Free energy coefï¬cients (max(L, M) = 80, min(L, M) = 10,. . . , 80,
H = 10, and Hâˆ—= 0).
0
0.5
1
1.5
2
0
20
40
60
80
100
VB
Bayes
Regular
Figure 14.10 Free energy coefï¬cients (max(L, M) = 80, min(L, M) = 20,. . . , 80,
H = 20, and Hâˆ—= 0).
Figure 14.5 imply that the VB posterior well approximates the Bayes posterior.
Also, by comparing Figures 14.3 and 14.7, we observe that, when H â‰ª
min(L, M), VB learning provides much worse generalization performance than
Bayesian learning, while the VB free energy well approximates the Bayes free
energy; and that, when H âˆ¼min(L, M), VB learning provides much better
generalization performance, while the VB free energy is signiï¬cantly larger
than the Bayes free energy. Further investigation is required to understand the
relation between the generalization performance and the gap between the VB
and the Bayes free energies.
Figures 14.9 through 14.11 show similar cases to Figure 14.6 but for
different ranks H = 10, 20, 40, respectively. From Figures 14.5 through 14.11,
we conclude that, in general, the VB free energy behaves similarly to the
Bayes free energy when L and M are signiï¬cantly different from each other
or H â‰ªmin(L, M). In Figure 14.8, the VB free energy behaves strangely and
poorly approximates the Bayes free energy when Hâˆ—is large. This is because

14.2 Generalization Properties
419
0
0.5
1
1.5
2
0
20
40
60
80
100
VB
Bayes
Regular
Figure 14.11 Free energy coefï¬cients (max(L, M) = 80, min(L, M) = 40,. . . , 80,
H = 40, and Hâˆ—= 0).
of the trivial redundancy of the RRR model, of which VB learning with the
independence constraint cannot make use to reduce the free energy (see the
remark in the last paragraph of Section 14.2.5).
14.2.7 Analysis with Subtle True Singular Values
Here we conduct an additional analysis to explain the seemingly contradictory
observation in Figure 14.4â€”in the RRR model with max(L, M)
=
50,
min(L, M) = 30, H = 20, VB learning always gives smaller generalization
error than Bayesian learning, regardless of the true rank Hâˆ—. We show that this
does not mean the domination by VB learning over Bayesian learning, which
was proven to be never dominated by any other method (see Appendix D).
Distinct and Subtle Signal Assumptions
The contradictory observation was due to the assumption (14.42) on the true
singular values:
Î³âˆ—
h =
â§âªâªâ¨âªâªâ©
Î˜(1)
for
h = 1,. . . , Hâˆ—,
0
for
h = Hâˆ—+ 1,. . . , min(L, M),
(14.86)
which we call the distinct signal assumption. This assumption seems to cover
any true linear mapping Bâˆ—Aâˆ—âŠ¤= H
h=1 Î³âˆ—
hÏ‰âˆ—
bhÏ‰âˆ—
ah by classifying all singular
components such that Î³âˆ—
h > 0 to the necessary components h = 1,. . . , Hâˆ—, and
the other components such that Î³âˆ—
h = 0 to the redundant components h = Hâˆ—+
1,. . . , min(L, M). However, in the asymptotic limit, the assumption (14.86)
implicitly prohibits the existence of true singular values in the same order as
the noise contribution, i.e., Î³âˆ—
h = Î˜p(Nâˆ’1/2). In other words, the distinct signal
assumption (14.86) considers all true singular values to be either inï¬nitely

420
14 Asymptotic VB Theory of Reduced Rank Regression
larger than the noise or exactly equal to zero. As a result, asymptotic analysis
under the distinct signal assumption reï¬‚ects only the overï¬tting tendency of
a learning machine, and ignores the underï¬tting tendency, which happens
when the signal is not clearly separable from the noise. Since overï¬tting and
underï¬tting are in the trade-off relation, it is important to investigate both
tendencies when generalization performance is analyzed.
To relax the restriction discussed previously, we replace the assumption
(14.42) with
Î³âˆ—
h =
â§âªâªâ¨âªâªâ©
Î˜(1)
for
h = 1,. . . , Hâˆ—,
O(Nâˆ’1/2)
for
h = Hâˆ—+ 1,. . . , min(L, M),
(14.87)
which we call the subtle signal assumption, in the following analysis (Watan-
abe and Amari, 2003; Nakajima and Watanabe, 2007). Note that, with the
assumption (14.87), we do not intend to analyze the case where the true
singular values depend on N. Rather, we assume realistic situations where the
number of necessary components Hâˆ—depends on N. Let us keep in mind the
following two points, which are usually true when we analyze real-world data:
â€¢ The number N of samples is always ï¬nite.
Asymptotic theory is not to investigate what happens when N â†’âˆ, but
to approximate the situation where N is ï¬nite but large.
â€¢ It rarely happens that real-world data can be exactly expressed by a
low-rank model.
Statistical models are supposed to be simpler than the real-world data
generation process, but expected to approximate it with certain accuracy,
and the accuracy depends on the noise level and the number of samples.
Then we expect that, for most real-world data, it holds that Î³âˆ—
h > 0 for all
h = 1,. . . , min(L, M), but, given ï¬nite N, some of the true singular values
are comparable to the noise contribution Î³âˆ—
h = Î˜(Nâˆ’1/2), and some others are
negligible Î³âˆ—
h = o(Nâˆ’1/2). The subtle signal assumption (14.87) covers such
realistic situations.
Generalization Error under Subtle Signal Assumption
Replacing the distinct signal assumption (14.86) with the subtle signal assump-
tion (14.87) does not affect the discussion up to Theorem 14.10, i.e., Theorems
14.1, 14.5, and 14.10, Lemmas 14.6 through 14.8, and their corollaries still
hold. Instead of Theorem 14.11, we have the following theorem:
Theorem 14.25
Under the subtle signal assumption (14.87), the average gen-
eralization error of the RRR model for H â‰¥Hâˆ—is asymptotically expanded as

14.2 Generalization Properties
421
GE(N) = Î»VBNâˆ’1 + O(Nâˆ’3/2),
where the generalization coefï¬cient is given by
2Î»VB = (Hâˆ—(L + M) âˆ’Hâˆ—2) +
N
Ïƒâ€²2
min(L,M)
h=Hâˆ—+1 Î³âˆ—2
h
+
/ Hâˆ’Hâˆ—
h=1
Î¸

Î³â€²â€²2
h
> max(L, M)
 
Â·
)!
1 âˆ’max(L,M)
Î³â€²â€²2
h
"2
Î³â€²â€²2
h âˆ’2
!
1 âˆ’max(L,M)
Î³â€²â€²2
h
"
Î³â€²â€²
h Ï‰â€²â€²âŠ¤
bh Vâ€²â€²âˆ—Ï‰â€²â€²
ah
1 0
q(Vâ€²â€²)
.
(14.88)
Here,
Vâ€²â€² =
min(L,M)âˆ’Hâˆ—

h=1
Î³â€²â€²
h Ï‰â€²â€²
bhÏ‰â€²â€²âŠ¤
ah
(14.89)
is the SVD of a random matrix Vâ€²â€² âˆˆR(min(L,M)âˆ’Hâˆ—)Ã—(max(L,M)âˆ’Hâˆ—) subject to
q(Vâ€²â€²) = MGaussmin(L,M)âˆ’Hâˆ—,max(L,M)âˆ’Hâˆ—$Vâ€²â€²; Vâ€²â€²âˆ—, Imin(L,M)âˆ’Hâˆ—âŠ—Imax(L,M)âˆ’Hâˆ—% ,
(14.90)
and Vâ€²â€²âˆ—âˆˆR(min(L,M)âˆ’Hâˆ—)Ã—(max(L,M)âˆ’Hâˆ—) is a (nonsquare) diagonal matrix with
the diagonal entries given by Vâ€²â€²
h,h =
âˆš
N
Ïƒâ€² Î³âˆ—
Hâˆ—+h for h = 1,. . . , min(L, M) âˆ’Hâˆ—.
Proof
From Eq. (14.58), we have
####BA
âŠ¤âˆ’Bâˆ—Aâˆ—âŠ¤####
2
Fro =
###H
h=1Î³VB
h Ï‰bhÏ‰âŠ¤
ah âˆ’Bâˆ—Aâˆ—âŠ¤###2
Fro
=
###Hâˆ—
h=1 Î³hÏ‰bhÏ‰âŠ¤
ah âˆ’Bâˆ—Aâˆ—âŠ¤+ H
h=Hâˆ—+1Î³VB
h Ï‰bhÏ‰âŠ¤
ah
###2
Fro + Op(Nâˆ’3/2)
=
####V âˆ’Bâˆ—Aâˆ—+ H
h=Hâˆ—+1Î³VB
h Ï‰bhÏ‰âŠ¤
ah âˆ’min(L,M)
h=Hâˆ—+1 Î³hÏ‰bhÏ‰âŠ¤
ah
####
2
Fro
+ Op(Nâˆ’3/2),
and therefore,
*####BA
âŠ¤âˆ’Bâˆ—Aâˆ—âŠ¤####
2
Fro
+
q(D)
=
*####V âˆ’Bâˆ—Aâˆ—+ H
h=Hâˆ—+1Î³VB
h Ï‰bhÏ‰âŠ¤
ah âˆ’min(L,M)
h=Hâˆ—+1 Î³hÏ‰bhÏ‰âŠ¤
ah
####
2
Fro
+
q(D)
+ O(Nâˆ’3/2)
=

âˆ¥V âˆ’Bâˆ—Aâˆ—âˆ¥2
Fro

q(D)
+ 2

(V âˆ’Bâˆ—Aâˆ—)âŠ¤H
h=Hâˆ—+1Î³VB
h Ï‰bhÏ‰âŠ¤
ah âˆ’min(L,M)
h=Hâˆ—+1 Î³hÏ‰bhÏ‰âŠ¤
ah
 
q(D)

422
14 Asymptotic VB Theory of Reduced Rank Regression
+
H
h=Hâˆ—+1(Î³VB
h )2 âˆ’2 H
h=Hâˆ—+1 Î³hÎ³VB
h
+ min(L,M)
h=Hâˆ—+1 Î³2
h

q(D)
+ O(Nâˆ’3/2)
=

âˆ¥V âˆ’Bâˆ—Aâˆ—âˆ¥2
Fro

q(D)
+ 2
/ H
h=Hâˆ—+1(Î³hÏ‰bhÏ‰âŠ¤
ah âˆ’Î³âˆ—
hÏ‰âˆ—
bhÏ‰âˆ—âŠ¤
ah )âŠ¤Î³VB
h Ï‰bhÏ‰âŠ¤
ah
âˆ’min(L,M)
h=Hâˆ—+1 (Î³hÏ‰bhÏ‰âŠ¤
ah âˆ’Î³âˆ—
hÏ‰âˆ—
bhÏ‰âˆ—âŠ¤
ah )âŠ¤Î³hÏ‰bhÏ‰âŠ¤
ah
0
q(D)
+
H
h=Hâˆ—+1(Î³VB
h )2 âˆ’2 H
h=Hâˆ—+1 Î³hÎ³VB
h
+ min(L,M)
h=Hâˆ—+1 Î³2
h

q(D)
+ O(Nâˆ’3/2)
=

âˆ¥V âˆ’Bâˆ—Aâˆ—âˆ¥2
Fro

q(D) âˆ’2
H
h=Hâˆ—+1(Î³âˆ—
hÏ‰âˆ—
bhÏ‰âˆ—âŠ¤
ah )âŠ¤Î³VB
h Ï‰bhÏ‰âŠ¤
ah

q(D)
+ 2
min(L,M)
h=Hâˆ—+1 (Î³âˆ—
hÏ‰âˆ—
bhÏ‰âˆ—âŠ¤
ah )âŠ¤Î³hÏ‰bhÏ‰âŠ¤
ah

q(D)
+
H
h=Hâˆ—+1(Î³VB
h )2
q(D) âˆ’
min(L,M)
h=Hâˆ—+1 Î³2
h

q(D) + O(Nâˆ’3/2)
=

âˆ¥V âˆ’Bâˆ—Aâˆ—âˆ¥2
Fro

q(D) âˆ’
*min(L,M)
h=Hâˆ—+1
####Î³hÏ‰bhÏ‰âŠ¤
ah âˆ’Î³âˆ—
hÏ‰âˆ—
bhÏ‰âˆ—âŠ¤
ah
####
2
Fro
+
q(D)
âˆ’2
H
h=Hâˆ—+1(Î³âˆ—
hÏ‰âˆ—
bhÏ‰âˆ—âŠ¤
ah )âŠ¤Î³VB
h Ï‰bhÏ‰âŠ¤
ah

q(D)
+
H
h=Hâˆ—+1(Î³VB
h )2
q(D) + min(L,M)
h=Hâˆ—+1 Î³âˆ—2
h + O(Nâˆ’3/2)
= Ïƒâ€²2
N

LM âˆ’(L âˆ’Hâˆ—)(M âˆ’Hâˆ—) +
N
Ïƒâ€²2
min(L,M)
h=Hâˆ—+1 Î³âˆ—2
h
 
+
H
h=Hâˆ—+1(Î³VB
h )2
q(D) âˆ’2
H
h=Hâˆ—+1(Î³âˆ—
hÏ‰âˆ—
bhÏ‰âˆ—âŠ¤
ah )âŠ¤Î³VB
h Ï‰bhÏ‰âŠ¤
ah

q(D)
+ O(Nâˆ’3/2).
(14.91)
In
the
orthogonal
space
to
the
distinctly
necessary
components
{Î³h, Ï‰ah, Ï‰bh}Hâˆ—
h=1, the distribution of {Î³h, Ï‰ah, Ï‰bh}min(L,M)
h=Hâˆ—+1
coincides with
the distribution of { Ïƒâ€²2
âˆš
N Î³â€²â€²
h , Ï‰â€²â€²
ah, Ï‰â€²â€²
bh}min(L,M)âˆ’Hâˆ—
h=1
, deï¬ned in Eq. (14.89),
with Vâ€²â€²âˆ—as the true matrix for subtle or the redundant components,
h = Hâˆ—+ 1,. . . , min(L, M). By using Eq. (14.58), we thus have
*####BA
âŠ¤âˆ’Bâˆ—Aâˆ—âŠ¤####
2
Fro
+
q(D)
= Ïƒâ€²2
N

(Hâˆ—(L + M) âˆ’Hâˆ—2) +
N
Ïƒâ€²2
min(L,M)
h=Hâˆ—+1 Î³âˆ—2
h
+
/ Hâˆ’Hâˆ—
h=1
Î¸

Î³â€²â€²2
h
> max(L, M)
 

14.2 Generalization Properties
423
Â·
)!
1 âˆ’max(L,M)
Î³â€²â€²2
h
"2
Î³â€²â€²2
h âˆ’2
!
1 âˆ’max(L,M)
Î³â€²â€²2
h
"
Î³â€²â€²
h Ï‰â€²â€²âŠ¤
bh Vâ€²â€²âˆ—Ï‰â€²â€²
ah
1 0
q(Vâ€²â€²)

+ O(Nâˆ’3/2),
which completes the proof.
â–¡
Training Error under Subtle Signal Assumption
The training error can be analyzed more easily.
Theorem 14.26
Under the subtle signal assumption (14.87), the average
training error of the RRR model for H â‰¥Hâˆ—is asymptotically expanded as
TE(N) = Î½VBNâˆ’1 + O(Nâˆ’3/2),
where the training coefï¬cient is given by
2Î½VB = âˆ’(Hâˆ—(L + M) âˆ’Hâˆ—2) +
N
Ïƒâ€²2
min(L,M)
h=Hâˆ—+1 Î³âˆ—2
h
+
/ Hâˆ’Hâˆ—
h=1
Î¸

Î³â€²â€²2
h
> max(L, M)
 
Â·
!
1 âˆ’max(L,M)
Î³â€²â€²2
h
" !
1 + max(L,M)
Î³â€²â€²2
h
"
Î³â€²â€²2
h
0
q(Vâ€²â€²)
.
(14.92)
Here Vâ€²â€² and {Î³â€²â€²
h } are deï¬ned in Theorem 14.25.
Proof
Theorem 14.15 and Eq. (14.77) still hold under the assumption (14.87).
Therefore,
####V âˆ’BA
âŠ¤####
2
Fro
= âˆ’H
h=Hâˆ—+1 Î¸

Î³2
h > max(L,M)Ïƒâ€²2
N
 
Â·

Î³h âˆ’max(L,M)Ïƒâ€²2
NÎ³h
 
Î³h + max(L,M)Ïƒâ€²2
NÎ³h
 
+ min(L,M)
h=Hâˆ—+1 (Î³h âˆ’Î³âˆ—
h)2 + min(L,M)
h=Hâˆ—+1 Î³âˆ—2
h + Op(Nâˆ’3/2),
and
*####V âˆ’BA
âŠ¤####
2
Fro âˆ’
###V âˆ’Bâˆ—Aâˆ—âŠ¤###2
Fro
+
q(D)
= âˆ’Ïƒâ€²2
N
)
(Hâˆ—(L + M) âˆ’Hâˆ—2) âˆ’N
Ïƒâ€²2
min(L,M)

h=Hâˆ—+1
Î³âˆ—2
h
+ H
h=Hâˆ—+1 Î¸

Î³2
h > max(L,M)Ïƒâ€²2
N
 
Â·

Î³h âˆ’max(L,M)Ïƒâ€²2
NÎ³h
 
Î³h + max(L,M)Ïƒâ€²2
NÎ³h
 1
+ Op(Nâˆ’3/2).
Substituting the preceding equation into Eq. (14.75) and using the random
matrix Vâ€²â€² and its singular values {Î³â€²â€²
h }, deï¬ned in Theorem 14.25, we obtain
Eq. (14.92).
â–¡

424
14 Asymptotic VB Theory of Reduced Rank Regression
â€“2
â€“1.5
â€“1
â€“0.5
0
0.5
1
1.5
2
0
2
4
6
8
10
12
14
16
18
VB
ML
Regular
Figure 14.12 The generalization coefï¬cients and the training coefï¬cients under
the subtle signal assumption (14.87) in the RRR model with max(L, M) = 50,
min(L, M) = 30, H = 20, and Hâˆ—= 5.
Comparison with Other Learning Algorithms
Figure 14.12 shows the generalization coefï¬cients and the training coefï¬cients,
computed by using Theorems 14.25 and 14.26, respectively, as functions of a
rescaled subtle true singular value
âˆš
NÎ³âˆ—
h/Ïƒâ€². The considered RRR model is
with max(L, M) = 50, min(L, M) = 30, and H = 20, and the true linear
mapping is assumed to consist of Hâˆ—= 5 distinctly necessary components
(Î³âˆ—
h = Î˜(1) for h = 1,. . . , 5), 10 subtle components (Î³âˆ—
h = Î˜(Nâˆ’1/2) for
h = 6,. . . , 15), and the other ï¬ve null components (Î³âˆ—
h = 0 for h = 16,. . . , 20).
The subtle singular values are assumed to be identical, Î³âˆ—
h = Î³âˆ—for h =
6,. . . , 15, and the horizontal axis indicates
âˆš
NÎ³âˆ—/Ïƒâ€². The generalization
coefï¬cients and the training coefï¬cients of ML learning can be derived in the
same way as Theorems 14.25 and 14.26 with the VB estimator Î³VB
h
replaced
with the ML estimator Î³ML
h
= Î³h. Unfortunately, the generalization error nor
the training error of Bayesian learning under the subtle signal assumption for
the general RRR model has not been clariï¬ed.
Only in the case where L = H = 1, the Bayes generalization error under the
subtle signal assumption has been analyzed.
Theorem 14.27
(Watanabe and Amari, 2003) The Bayes generalization error
of the RRR model with M â‰¥2, L = H = 1 under the assumption that the true
mapping is bâˆ—aâˆ—= O(Nâˆ’1/2) is asymptotically expanded as
GE
Bayes(N) = Î»BayesNâˆ’1 + o(Nâˆ’1),
where the generalization coefï¬cient is given by
2Î»Bayes = 1 +
*!####
âˆš
N
Ïƒâ€² bâˆ—aâˆ—####
2
+
âˆš
N
Ïƒâ€² bâˆ—aâˆ—âŠ¤v
"
Î¦M(v)
Î¦Mâˆ’2(v)
+
q(v) .
(14.93)

14.2 Generalization Properties
425
â€“2
â€“1.5
â€“1
â€“0.5
0
0.5
1
1.5
2
0
2
4
6
8
10
12
14
16
18
VB
ML(=Regular)
Bayes
Figure 14.13 The generalization coefï¬cients and the training coefï¬cients under
the subtle signal assumption (14.87) in the RRR model with M = 5, L = H = 1,
and Hâˆ—= 0.
Here,
Î¦M(v) =
 Ï€/2
0
sinM Î¸ exp
!
âˆ’1
2
####
âˆš
N
Ïƒâ€² bâˆ—aâˆ—+ v
####
2
sin2 Î¸
"
dÎ¸,
and v âˆˆRM is a random vector subject to q(v) = GaussM(v; 0, IM).
Figure 14.13 compares the generalization coefï¬cients when M = 5, L =
H = 1, and Hâˆ—= 0, where the horizontal axis corresponds to a rescaled
subtle true singular value
âˆš
NÎ³âˆ—/Ïƒâ€² =
âˆš
N âˆ¥bâˆ—aâˆ—âˆ¥/Ïƒâ€².2 We see that the
generalization error of VB learning is smaller than that of Bayesian learning
when
âˆš
NÎ³âˆ—/Ïƒâ€² = 0, and identical when
âˆš
NÎ³âˆ—/Ïƒâ€² â†’âˆ. This means that,
under the distinct signal assumption (14.86), which considers only the case
where Î³âˆ—= 0 (i.e., Hâˆ—= 0) or Î³âˆ—= Î˜(1) (i.e., Hâˆ—= 1), VB learning
always performs better than Bayesian learning. However, we can see in Figure
14.13 that, when
âˆš
NÎ³âˆ—/Ïƒâ€² â‰ˆ3, Bayesian learning outperforms VB learning.
Figure 14.13 simply implies that VB learning is more strongly regularized
than Bayesian learning, or in other words, VB learning tends to underï¬t subtle
signals such that Î³âˆ—= Î˜(Nâˆ’1/2), while Bayesian learning tends to overï¬t noise.
Knowing the proved optimality of Bayesian learning (Appendix D), we
would expect that the same happens in Figure 14.12, where the limits
âˆš
NÎ³âˆ—/Ïƒâ€² = 0 and
âˆš
NÎ³âˆ—/Ïƒâ€² â†’âˆcorrespond to the cases with Hâˆ—= 5 and
Hâˆ—= 15, respectively, in Figure 14.4 under the distinct signal assumption
2 When L = H = 1, the parameter transformation ba â†’w makes the RRR model identiï¬able, and
therefore the ML generalization coefï¬cient is identical to that of the regular models. This is the
reason why only the integration effect or model induced-regularization was observed in the
one-dimensional matrix factorization model in Section 7.2 and Section 7.3.3. The basis
selection effect appears only when a singular model cannot be equivalently transformed to a
regular model (see the discussion in Section 14.3).

426
14 Asymptotic VB Theory of Reduced Rank Regression
(14.86). Namely, if we could depict the Bayes generalization coefï¬cient
in Figure 14.12, there should be some interval where Bayesian learning
outperforms VB learning.
14.3 Insights into VB Learning
In this chapter, we analyzed the generalization error, the training error, and the
free energy of VB learning in the RRR model, and derived their asymptotic
forms. We also introduced theoretical results providing those properties for
ML learning and Bayesian learning. As mentioned in Section 13.5, the RRR
model is the only singular model of which those three properties have been
theoretically clariï¬ed for ML learning, Bayesian learning, and VB learning.
Accordingly, we here summarize our observations, and discuss effects of
singularities in VB learning and other learning algorithms.
(i) In the RRR model, the basis selection effect, explained in Section 13.5.1,
appears as a selection bias of largest singular values of a zero-mean
random matrix.
Theorem 14.19 gives an asymptotic expansion of the ML
generalization error. The second term in the generalization coefï¬cient
(14.82) is the expectation of the square of the (H âˆ’Hâˆ—) largest singular
values of a random matrix
âˆš
N
Ïƒâ€² Vâ€², where Vâ€² is subject to the zero-mean
Gaussian (14.61). This corresponds to the effect of basis selection: ML
learning chooses the singular components that best ï¬t the observation
noise. With the full-rank model, i.e., H = min(L, M), the second term in
the generalization coefï¬cient (14.82) is equal to
min(L,M)âˆ’Hâˆ—
h=1
Î³â€²2
h

q(W) = (min(L, M) âˆ’Hâˆ—) (max(L, M) âˆ’Hâˆ—)
= (L âˆ’Hâˆ—) (M âˆ’Hâˆ—) ,
and therefore the generalization coefï¬cient becomes
2Î»ML = (Hâˆ—(L + M) âˆ’Hâˆ—2) + (L âˆ’Hâˆ—) (M âˆ’Hâˆ—) = LM
= D,
which is the same as the generalization coefï¬cient of the regular models.
Indeed, the full-rank RRR model is equivalently transformed to a regular
model by BAâŠ¤â†’U, where the domain for U is the whole RLÃ—M space
(no low-rank restriction is imposed to U). In this case, no basis selection
occurs because all possible bases are supposed to be used.

14.3 Insights into VB Learning
427
(ii) In the RRR model, the integration effect, explained in Section 13.5.1,
appears as the Jamesâ€“Stein (JS) type shrinkage.
This was shown in Theorem 14.1 in the asymptotic limit: the VB
estimator converges to the positive-part JS estimator operated on each
singular component separately. By comparing Theorems 14.11 and
14.19, we see that ML learning and VB learning differ from each other
by the factor Î¸

Î³â€²2
h > max(L, M)
 !
1 âˆ’max(L,M)
Î³â€²2
h
"2
, which comes from the
postive-part JS shrinkage. Unlike the basis selection effect, the
integration effect appears even if the model can be equivalently
transformed to a regular modelâ€”the full-rank RRR model (with
H = min(L, M)) is still affected by the singularities. The relation
between VB learning and the JS shrinkage estimator was also observed
in nonasymptotic analysis in Chapter 7, where model-induced
regularization (MIR) was illustrated as a consequence of the integration
effect, by focusing on the one-dimensional matrix factorization model.
Note that basis selection effect does not appear in the one-dimensional
matrix factorization model (where L = M = H), because it can be
equivalently transformed to a regular model.
(iii) VB learning shows similarity both to ML learning and Bayesian
learning.
Figures 14.1 through 14.4 generally show that VB learning is
regularized as much as Bayesian learning, while its dependence on the
model size (H, L, M, etc.) is more like ML learning. Unlike Bayesian
learning, the integration effect does not always dominate the basis
selection effect in VB learningâ€”a good property of Bayesian learning,
2Î»Bayes â‰¤D, does not necessarily hold in VB learning, e.g., we
observe that 2Î»VB > D at min(L, M) = 80 in Figure 14.2, and H = 1 in
Figure 14.3.
(iv) In VB learning, the relation between the generalization error and the free
energy is not as simple as in Bayesian learning.
In Bayesian learning, the generalization coefï¬cient and the free energy
coefï¬cient coincide with each other, i.e., Î»Bayes = Î»â€²Bayes. This property
does not hold in VB learning even approximately, as seen by comparing
Figures 14.1 through 14.4 and Figures 14.5 through 14.8. In many
cases, the VB free energy well approximates the Bayes free energy,
while the VB generalization error signiï¬cantly differs from the
Bayes generalization error. Research on the relation between the free
energy and the generalization error in VB learning is ongoing (see
Section 17.4).

428
14 Asymptotic VB Theory of Reduced Rank Regression
(v) MIR in VB learning can be stronger than that in Bayesian learning.
By deï¬nition, the VB free energy is never less than the Bayes free
energy, and therefore it holds that Î»â€²VB â‰¥Î»â€²Bayes. On the other hand, such
a relation does not hold for the generalization error, i.e., Î»VB can be
larger or less than Î»Bayes. However, even if Î»VB is less than or equal to
Î»Bayes for any true rank Hâˆ—in some RRR model, it does not mean the
domination of VB learning over Bayesian learning. Since the optimality
of Bayesian learning was proved (see Appendix D), Î»VB < Î»Bayes simply
means that VB learning is more strongly regularized than Bayesian
learning, or in other words, VB learning tends to underï¬t small signals
while Bayesian learning tends to overï¬t noise. Extending the analysis
under the subtle signal assumption (14.87) to the general RRR model
would clarify this point.
(vi) The generalization error depends on the dimensionality in an interesting
way.
The shrinkage factor is governed by max(L, M) and independent of
min(L, M) in the asymptotic limit (see Theorem 14.1). This is because
the shrinkage is caused by the VB posterior extending into the parameter
space with larger dimensional space (M-dimensional input space or
L-dimensional output space) for the redundant components, as seen in
Corollary 14.3. This choice was made by maximizing the entropy of the
VB posterior distribution when the free energy is minimized.
Consequently, when L  M, the shape of the VB posterior in the
asymptotic limit is similar to the partially Bayesian learning, where the
posterior of A or B is approximated by the Dirac delta function (see
Chapter 12). On the other hand, increase of the smaller dimensionality
min(L, M) broadens the variety of basis selection: as mentioned in (i),
the basis selection effect in the RRR model occurs by the redundant
components selecting the (H âˆ’Hâˆ—) largest singular components, and
(min(L, M) âˆ’Hâˆ—) corresponds to the dimensionality that the basis
functions can span. This phenomenon can be seen in Figure 8.3â€”the
MarË‡cenkoâ€“Pastur distribution is diverse when Î± = (min(L, M) âˆ’Hâˆ—)/
(max(L, M) âˆ’Hâˆ—) is large. We can conclude that a large max(L, M)
enhances the integration effect, leading to strong regularization, while a
large min(L, M) enhances the basis selection effect, leading to
overï¬tting. As a result, VB learning tends to be strongly regularized
when L â‰ªM or L â‰«M, and tends to overï¬t when L â‰ˆM.

15
Asymptotic VB Theory of Mixture Models
In this chapter, we discuss the asymptotic behavior of the VB free energy of
mixture models, for which VB learning algorithms were introduced in Sections
4.1.1 and 4.1.2. We ï¬rst prepare basic lemmas commonly used in this and the
following chapters.
15.1 Basic Lemmas
Consider the latent variable model expressed as
p(D|w) =

H
p(D, H|w).
In this chapter, we analyze the VB free energy, which is the minimum of the
free energy under the constraint,
r(w, H) = rw(w)rH(H),
(15.1)
i.e.,
FVB(D) =
min
rw(w),rH(H) F(r),
(15.2)
where
F(r) =
/
log rw(w)rH(H)
p(w, H, D)
0
rw(w)rH(H)
= FBayes(D) + KL (rw(w)rH(H)||p(w, H|D)) .
(15.3)
Here,
FBayes(D) = âˆ’log p(D) = âˆ’log

H
p(D, H) = âˆ’log

H

p(D, H, w)dw
429

430
15 Asymptotic VB Theory of Mixture Models
is the Bayes free energy. Recall that the stationary condition of the free energy
yields
rw(w) = 1
Cw
p(w) exp log p(D, H|w)
rH(H),
(15.4)
rH(H) =
1
CH
exp log p(D, H|w)
rw(w).
(15.5)
For the minimizerrw(w) of F(r), let
w = âŸ¨wâŸ©rw(w)
(15.6)
be the VB estimator.
The following lemma shows that the free energy is decomposed into the
sum of two terms.
Lemma 15.1
It holds that
FVB(D) = min
rw(w){R + Q},
(15.7)
where
R = KL(rw(w)||p(w)),
Q = âˆ’logCH,
for CH = 
H exp log p(D, H|w)
rw(w).
Proof
From the restriction of the VB approximation in Eq. (15.1), F(r) can
be divided into two terms,
F(r) =
/
log rw(w)
p(w)
0
rw(w)
+
/
log
rH(H)
p(D, H|w)
0
rw(w)rH(H)
.
Since the optimal VB posteriors satisfy Eqs. (15.4) and (15.5), if the VB
posterior rH(H) is optimized, then
/
log
rH(H)
p(D, H|w)
0
rw(w)rH(H)
= âˆ’logCH
holds. Thus, we obtain Eq. (15.7).
â–¡
The free energies of mixture models and other latent variable models
involve the di-gamma function Î¨(x) and the log-gamma function log Î“(x) (see,
e.g., Eq. (4.22)). To analyze the free energy, we will use the inequalities on
these functions in the following lemma:

15.1 Basic Lemmas
431
Lemma 15.2
(Alzer, 1997) For x > 0,
1
2x < log x âˆ’Î¨(x) < 1
x,
(15.8)
and
0 â‰¤log Î“(x) âˆ’
)
x âˆ’1
2

log x âˆ’x + 1
2 log 2Ï€
1
â‰¤
1
12x.
(15.9)
The inequalities (15.8) ensure that substituting log x for Î¨(x) only con-
tributes at most additive constant terms to the VB free energy. The substitution
for log Î“(x) is given by Eq. (15.9) as well.
For the i.i.d. latent variable models deï¬ned as
p(x|w) =

z
p(x, z|w),
(15.10)
the likelihood for the observed data D = {x(n)}N
n=1 and the complete data
{D, H} = {x(n), z(n)}N
n=1 is given by
p(D|w) =
N

n=1
p(x(n)|w),
p(D, H|w) =
N

n=1
p(x(n), z(n)|w),
respectively. In the asymptotic analysis of the free energy for such a
model, when the free energy is minimized, the second term in Eq. (15.7),
Q = âˆ’logCH, is proved to be close to N times the empirical entropy (13.20),
S N(D) = âˆ’1
N
N

n=1
log p(x(n)|wâˆ—),
(15.11)
where wâˆ—is the true parameter generating the data. Thus, the ï¬rst term in
Eq. (15.7) shows the asymptotic behavior of the VB free energy, which is
analyzed with the inequalities in Lemma 15.2.
Let
Q = Q âˆ’NS N(D).
(15.12)
It follows from Jensenâ€™s inequality that
Q = log p(D|wâˆ—) âˆ’log

H
exp log p(D, H|w)
rw(w)
â‰¥log
p(D|wâˆ—)
âŸ¨p(D|w)âŸ©rw(w)
(15.13)
â‰¥NEN(wML),

432
15 Asymptotic VB Theory of Mixture Models
where wML is the maximum likelihood (ML) estimator, and
EN(w) = LN(wâˆ—) âˆ’LN(w)
= 1
N
N

n=1
log p(x(n)|wâˆ—)
p(x(n)|w)
(15.14)
is the empirical KL divergence. Note here that LN is deï¬ned in Eq. (13.37), and
EN(w) corresponds to the training error of the plug-in predictive distribution
(deï¬ned in Eq. (13.75) for regular models) with an estimator w.
If the domain of data X is discrete and with ï¬nite cardinality, # (X) = M,
Q in Eq. (15.7) can be analyzed in detail. In such a case, we can assume
without loss of generality that x âˆˆ{e1,. . . , eM}, where em is the one-of-M
representation, i.e., only the mth entry is one and the other entries are zeros.
Let Nm be the number of output m in the sequence D, i.e., Nm = N
n=1 x(n)
m , and
deï¬ne the strongly Îµ-typical set T N
Îµ (pâˆ—) with respect to the probability mass
function,
pâˆ—= (pâˆ—
1,. . . , pâˆ—
M)âŠ¤= (p(x1 = 1|wâˆ—),. . . , p(xM = 1|wâˆ—))âŠ¤âˆˆÎ”Mâˆ’1
as follows:
T N
Îµ (pâˆ—) =
â§âªâªâ¨âªâªâ©D âˆˆXN;

Nm
N âˆ’pâˆ—
m
 â‰¤
pâˆ—
m
log M Îµ, m = 1,. . . , M
â«âªâªâ¬âªâªâ­.
(15.15)
It is known that the probability that the observed data sequence is not strongly
Îµ-typical is upper-bounded as follows:
Lemma 15.3
(Han and Kobayashi, 2007) It holds that
Prob(D  T N
Îµ (pâˆ—)) â‰¤ÎºM
NÎµ2 ,
where
Îº = $log M%2 max
m:pâˆ—m0
1 âˆ’pâˆ—
m
pâˆ—m
.
Let
p = (p1,. . . , pM)âŠ¤=

âŸ¨p(x1 = 1|w)âŸ©rw(w) ,. . . , âŸ¨p(xM = 1|w)âŸ©rw(w)
 âŠ¤âˆˆÎ”Mâˆ’1
be the probability mass function deï¬ned by the predictive distribution
âŸ¨p(x|w)âŸ©rw(w) with the VB posterior rw(w). For any ï¬xed Î´ > 0, deï¬ne
Râˆ—
Î´ =
,
p âˆˆÎ”Mâˆ’1; KL(pâˆ—||p) â‰¤Î´
-
,
(15.16)
where KL(pâˆ—||p) = M
m=1 pâˆ—
m log pâˆ—
m
pm . Then the following lemma holds:

15.1 Basic Lemmas
433
Lemma 15.4
Suppose that the domain X is discrete and with ï¬nite cardinal-
ity, # (X) = M. For all Îµ > 0 and D âˆˆT N
Îµ (pâˆ—), there exists a constant C > 0
such that if p  Râˆ—
CÎµ2,
Q = Î©p(N).
(15.17)
Furthermore,
min
rw(w)
Q = Op(1).
(15.18)
Proof
From Eq. (15.13), we have
Q â‰¥log
p(D|wâˆ—)
âŸ¨p(D|w)âŸ©rw(w)
= N
M

m=1
Nm
N log pâˆ—
m
pm
= N
,
KL(pML||p) âˆ’KL(pML||pâˆ—)
-
,
(15.19)
where pML = (pML
1 ,. . . , pML
M )âŠ¤= (N1/N,. . . , NM/N)âŠ¤âˆˆÎ”Mâˆ’1 is the type,
namely the empirical distribution of D. Thus, if KL(pML||p) > KL(pML||pâˆ—),
the right-hand side of Eq. (15.19) grows in the order of N. If D âˆˆT N
Îµ (pâˆ—),
KL(pML||pâˆ—) = Op(Îµ2) since KL(pML||pâˆ—) is well approximated by a quadratic
function ofpML âˆ’pâˆ—. To prove the ï¬rst assertion of the lemma, it sufï¬ces to see
that KL(pâˆ—||p) â‰¤CÎµ2 is equivalent to KL(pML||p) â‰¤Câ€²Îµ2 for a constant Câ€² > 0
if D âˆˆT N
Îµ (pâˆ—). In fact, we have
KL(pâˆ—||p) = KL(pâˆ—||pML) + KL(pML||p) +
M

m=1
(pâˆ—
m âˆ’pML
m )

log pML
m
âˆ’log pm
 
.
(15.20)
It follows from D âˆˆT N
Îµ (pâˆ—) that KL(pâˆ—||pML)/Îµ2 and |pâˆ—
m âˆ’pML
m |/Îµ are bounded
by constants. Then KL(pML||p) â‰¤Câ€²Îµ2 implies that |pML
m âˆ’pm|/Îµ is bounded by
a constant, and hence all the terms in Eq. (15.20) divided by Îµ2 are bounded
by constants.
It follows from Eq. (15.19) that
min
rw(w)
Q â‰¥âˆ’NKL(pML||pâˆ—).
(15.21)
The standard asymptotic theory of the multinomial model implies that twice
the right-hand side of Eq. (15.21), with its sign ï¬‚ipped, asymptotically follows
the chi-squared distribution with degree of freedom M âˆ’1 as discussed in
Section 13.4.5.
â–¡

434
15 Asymptotic VB Theory of Mixture Models
This lemma is used for proving the consistency of the VB posterior and
evaluating lower-bounds of VB free energy for discrete models in Sections
15.3 and 15.4 and Chapter 16.
15.2 Mixture of Gaussians
In this section, we consider the following Gaussian mixture model (GMM)
introduced in Section 4.1.1 and give upper- and lower-bounds for the VB free
energy (Watanabe and Watanabe, 2004, 2006):
p(z|Î±) = MultinomialK,1(z; Î±),
(15.22)
p(x|z, {Î¼k}K
k=1) =
K

k=1
6GaussM(x; Î¼k, IM)7zk ,
(15.23)
p(Î±|Ï†) = DirichletK(Î±; (Ï†,. . . , Ï†)âŠ¤),
(15.24)
p(Î¼k|Î¼0, Î¾) = GaussM(Î¼k|Î¼0, (1/Î¾)IM).
(15.25)
Under the constraint,
r(H, w) = rH(H)rw(w),
the VB posteriors are given as follows:
r({z(n)}N
n=1, Î±, {Î¼k}K
k=1) = rz({z(n)}N
n=1)rÎ±(Î±)rÎ¼({Î¼k}K
k=1),
rz({z(n)}N
n=1) =
N

n=1
MultinomialK,1

z(n);z(n) 
,
rÎ±(Î±) = Dirichlet

Î±; (Ï†1,. . . ,Ï†K)âŠ¤ 
,
rÎ¼({Î¼k}K
k=1) =
K

k=1
GaussM

Î¼k;Î¼k, Ïƒ2
kIM
 
.
The variational parameters {z(n)}N
n=1, {Ï†k}K
k=1, {Î¼k, Ïƒ2
k}K
k=1 minimize the free
energy,
F = log
â›âœâœâœâœâœâ
Î“(K
k=1 Ï†k)
K
k=1 Î“(Ï†k)
ââŸâŸâŸâŸâŸâ âˆ’log
 Î“(KÏ†)
(Î“(Ï†))K

âˆ’M
2
K

k=1
log

Î¾Ïƒ2
k
 
âˆ’KM
2
+
N

n=1
K

k=1
z(n)
k logz(n)
k +
K

k=1
Ï†k âˆ’Ï† âˆ’Nk
 
Î¨(Ï†k) âˆ’Î¨(K
kâ€²=1 Ï†kâ€²)
 

15.2 Mixture of Gaussians
435
+
K

k=1
Î¾

âˆ¥Î¼k âˆ’Î¼0âˆ¥2 + MÏƒ2
k
 
2
+
K

k=1
Nk

M log(2Ï€) + MÏƒ2
k
 
2
+
K

k=1
Nkâˆ¥xk âˆ’Î¼kâˆ¥2 + N
n=1z(n)
k âˆ¥x(n) âˆ’xkâˆ¥2
2
,
(15.26)
where
Nk =
N

n=1
z(n)
k ,
(15.27)
xk = 1
Nk
N

n=1
x(n)z(n)
k .
(15.28)
The stationary condition of the free energy yields
z(n)
k
=
z(n)
k
K
kâ€²=1 z(n)
kâ€²
,
(15.29)
Ï†k = Nk + Ï†,
(15.30)
Î¼k = Nkxk + Î¾Î¼0
Nk + Î¾
,
(15.31)
Ïƒ2
k =
1
Nk + Î¾
,
(15.32)
where
z(n)
k
âˆexp

Î¨(Ï†k) âˆ’1
2âˆ¥x(n) âˆ’Î¼kâˆ¥2 + MÏƒ2
k

.
(15.33)
The following condition is assumed.
Assumption 15.1
The true distribution q(x) is an M-dimensional GMM
p(x|wâˆ—), which has K0 components and parameter wâˆ—= (Î±âˆ—, {Î¼âˆ—
k}K0
k=1):
q(x) = p(x|wâˆ—) =
K0

k=1
Î±âˆ—
kGaussM(x; Î¼âˆ—
k, IM),
(15.34)
where x, Î¼âˆ—
k âˆˆRM. Suppose that the true distribution can be realized by our
model in hand, i.e., K â‰¥K0 holds.
Under this condition, we prove the following theorem, which evaluates the
relative VB free energy,
FVB(D) = FVB(D) âˆ’NS N(D).
(15.35)
The proof will appear in the next section.

436
15 Asymptotic VB Theory of Mixture Models
Theorem 15.5
The relative VB free energy of the GMM satisï¬es
Î»â€²VB
MM log N + NEN(w) + Op(1) â‰¤FVB(D) â‰¤Î»
â€²VB
MM log N + Op(1),
(15.36)
where EN is the empirical KL divergence (15.14), and the coefï¬cients Î»â€²VB
MM,
Î»
â€²VB
MM are given by
Î»â€²VB
MM =
â§âªâªâªâ¨âªâªâªâ©
(K âˆ’1)Ï† + M
2

Ï† < M+1
2
 
,
MK+Kâˆ’1
2

Ï† â‰¥M+1
2
 
,
(15.37)
Î»
â€²VB
MM =
â§âªâªâªâ¨âªâªâªâ©
(K âˆ’K0)Ï† + MK0+K0âˆ’1
2

Ï† < M+1
2
 
,
MK+Kâˆ’1
2

Ï† â‰¥M+1
2
 
.
(15.38)
In this theorem, EN(w) is the training error of the VB estimator. Let wML be
the ML estimator. Then it immediately follows from Eq. (15.14) that
NEN(w) â‰¥NEN(wML),
(15.39)
where NEN(wML) = minw
N
n=1 log p(x(n)|wâˆ—)
p(x(n)|w) is the (maximum) log-likelihood
ratio statistic with sign inversion. As discussed in Section 13.5.3, it is
conjectured for the GMM deï¬ned by Eqs. (15.22) and (15.23) that the log-
likelihood ratio diverges in the order of log log N (Hartigan, 1985). If this
conjecture is proved, the statement of the theorem is simpliï¬ed to
FVB(D) = Î»â€² log N + op(log N),
for Î»â€²VB
MM â‰¤Î»â€² â‰¤Î»
â€²VB
MM. Note, however, that even if NEN(wML) diverges to minus
inï¬nity, Eq. (15.39) does not necessarily mean NEN(w) diverges in the same
order. Also note that NEN(w) does not affect the upper-bound in Eq. (15.36).
Since the dimension of the parameter w is D = MK + K âˆ’1, the relative
Bayes free energy coefï¬cient of regular statistical models, on which the
Bayesian information criterion (BIC) (Schwarz, 1978) and the minimum
description length (MDL) (Rissanen, 1986) are based, is given by D/2. Note
that, unlike regular models, the advantage of Bayesian learning for singular
models is demonstrated by the asymptotic analysis as seen in Eqs. (13.123),
(13.124), and (13.125). Theorem 15.5 claims that the coefï¬cient Î»
â€²VB
MM of log N
is smaller than D/2 when Ï† < (M + 1)/2. This means that the VB free energy
FVB becomes smaller than that of regular models, i.e., 2Î»â€²VB â‰¤D holds.
Theorem 15.5 shows how the hyperparameters affect the learning process.
The coefï¬cients Î»â€²VB
MM and Î»
â€²VB
MM in Eqs. (15.37) and (15.38) are divided into

15.2 Mixture of Gaussians
437
two cases. These cases correspond to whether Ï† < M+1
2
holds, indicating that
the inï¬‚uence of the hyperparameter Ï† in the prior p(Î±|Ï†) appears depending
on the number M of parameters in each component. Let K be the number of
components satisfying Nk = Î˜p(N). Then the following corollary follows from
the proof of Theorem 15.5.
Corollary 15.6
The upper-bound in Eq. (15.36) is attained when K = K0 if
Ï† < M+1
2
and K = K if Ï† â‰¥M+1
2 .
This corollary implies that the phase transition of the VB posterior occurs
at Ï† = M+1
2 , i.e., only when Ï† < M+1
2 , the prior distribution reduces redundant
components; otherwise, it uses all the components. The phase transition of the
posterior occurs also in Bayesian learning while the phase transition point is
different from that of VB learning (Yamazaki and Kaji, 2013).
Theorem 15.5 also implies that the hyperparameter Ï† is the only hyperpa-
rameter on which the leading term of the VB free energy FVB depends. This
is due to the inï¬‚uence of the hyperparameters on the prior probability density
around the true parameters. Consider the case where K0 < K. In this case, for a
parameter that gives the true distribution, either of the followings holds: Î±k = 0
for some k or Î¼i = Î¼j for some pair (i, j). The prior distribution p(Î±|Ï†) given
by Eq. (15.24) can drastically change the probability density around the points
where Î±k = 0 for some k by changing the hyperparameter Ï† while the prior
distribution p(Î¼k|Î¼0, Î¾) given by Eq. (15.25) always takes positive values for
any values of the hyperparameters Î¾ and Î¼0. While the condition for the prior
density p(Î±|Ï†) to diverge at Î±k = 0 is Î±k < 1, and hence is independent of M,
the phase transition point of the VB posterior is Ï† = M+1
2 . As we will see in
Section 15.4 for the Bernoulli mixture model, if some of the components are
located at the boundary of the parameter space, the leading term of the relative
VB free energy depends also on the hyperparameter of the prior for component
parameters.
Theorem 15.5 is also extended to the case of the general Dirichlet prior
p(Î±|Ï†) = DirichletK(Î±; Ï†), where Ï† = (Ï†1,. . . , Ï†K)âŠ¤is the hyperparameter as
follows:
Theorem 15.7
(Nakamura and Watanabe, 2014) The relative VB free energy
of the GMM satisï¬es
K

k=1
Î»â€²VB
k
log N + NEN(w) + Op(1) â‰¤FVB(D) â‰¤
K

k=1
Î»
â€²VB
k
log N + Op(1),

438
15 Asymptotic VB Theory of Mixture Models
where the coefï¬cients Î»â€²VB
k
, Î»
â€²VB
k
are given by
Î»â€²VB
k
=
â§âªâªâªâ¨âªâªâªâ©
Ï†k âˆ’
1
2K

k  1 and Ï†k < M+1
2
 
,
M+1
2
âˆ’
1
2K

k = 1 or Ï†k â‰¥M+1
2
 
,
Î»
â€²VB
k
=
â§âªâªâªâ¨âªâªâªâ©
Ï†k âˆ’
1
2K

k > K0 and Ï†k < M+1
2
 
,
M+1
2
âˆ’
1
2K

k â‰¤K0 or Ï†k â‰¥M+1
2
 
.
The proof of this theorem is omitted. This theorem implies that the phase
transition of the VB posterior of each component occurs at the same transition
point Ï†k = M+1
2
as Theorem 15.5.
Proof of Theorem 15.5
Before proving Theorem 15.5, we show two lemmas where the two terms,
R = KL(rw(w)||p(w)) and Q = âˆ’logCH, in Lemma 15.1 are respectively
evaluated.
Lemma 15.8
It holds that
R âˆ’
â§âªâªâ¨âªâªâ©G(Î±) + Î¾
2
K

k=1
âˆ¥Î¼k âˆ’Î¼0âˆ¥2
â«âªâªâ¬âªâªâ­
 â‰¤C,
where C is a constant, Î¼k = Î¼k

rÎ¼({Î¼k}K
k=1) = Nkxk+Î¾Î¼0
Nk+Î¾
, and the function G(Î±) of
Î± =
2
Î±k = âŸ¨Î±kâŸ©rÎ±(Î±) = Nk+Ï†
N+KÏ†
3K
k=1 is deï¬ned by
G(Î±) = MK + K âˆ’1
2
log N +
) M + 1
2
âˆ’Ï†
1
K

k=1
logÎ±k.
(15.40)
Proof
Calculating the KL divergence between the posterior and the prior, we
obtain
KL(rÎ±(Î±)||p(Î±|Ï†)) =
K

k=1
h(Nk) âˆ’NÎ¨(N + KÏ†) + log Î“(N + KÏ†) + log Î“(Ï†)K
Î“(KÏ†),
(15.41)
where we use the notation h(x) = xÎ¨(x+Ï†)âˆ’log Î“(x+Ï†). Similarly, we obtain
KL(rÎ¼({Î¼k}K
k=1)||p({Î¼k}K
k=1|Î¼0, Î¾))
=
K

k=1
M
2 log Nk + Î¾
Î¾
âˆ’KM
2
+ Î¾
2
K

k=1
)
M
Nk + Î¾
+ âˆ¥Î¼k âˆ’Î¼0âˆ¥2
1
.
(15.42)

15.2 Mixture of Gaussians
439
By using Inequalities (15.8) and (15.9), we obtain
âˆ’1 + 12Ï† âˆ’1
12(x + Ï†) â‰¤h(x) +

Ï† âˆ’1
2

log(x + Ï†) âˆ’x âˆ’Ï† + 1
2 log 2Ï€ â‰¤0. (15.43)
Thus, from Eqs. (15.41), (15.42), (15.43), and
R = KL(rÎ±(Î±)||p(Î±|Ï†)) + KL(rÎ¼({Î¼k}K
k=1)||p({Î¼k}K
k=1|Î¼0, Î¾)),
it follows that
R âˆ’
â§âªâªâ¨âªâªâ©G(Î±) + Î¾
2
K

k=1
âˆ¥Î¼k âˆ’Î¼0âˆ¥2
â«âªâªâ¬âªâªâ­

â‰¤MK + K âˆ’1
2
log
!
1 + KÏ†
N
"
+ (K âˆ’1)
Ï† âˆ’log 2Ï€
2
 + K +
K

k=1
|12Ï† âˆ’1|
12(Nk + Ï†)
+
12N + 1
12(N + KÏ†)
+
log Î“(Ï†)K
Î“(KÏ†)
 +

K

k=1
log Nk + Î¾
Nk + Ï†
âˆ’MK
2 (1 + log Î¾) + Î¾
2
K

k=1
M
Nk + Î¾
 .
The right-hand side of the preceding inequality is bounded by a constant since
1
N + Î¾ <
1
Nk + Î¾
< 1
Î¾ ,
and
1
N + Ï† <
1
Nk + Ï†
< 1
Ï†.
â–¡
Lemma 15.9
It holds that
Q = âˆ’
N

n=1
log
â›âœâœâœâœâœâ
K

k=1
1
âˆš
2Ï€M exp
â›âœâœâœâœâœâÎ¨(Nk + Ï†) âˆ’Î¨(N + KÏ†)
âˆ’âˆ¥x(n) âˆ’Î¼kâˆ¥2
2
âˆ’M
2
1
Nk + Î¾

,
(15.44)
and
NEN(w) âˆ’
N
N + KÏ† â‰¤Q â‰¤NEN(w) âˆ’
N
2(N + KÏ†),
(15.45)
where EN(w) is given by Eq. (15.14) and EN(w) is deï¬ned by
EN(w) = 1
N
N

n=1
log
p(x(n)|wâˆ—)
K
k=1
Î±k
âˆš
2Ï€M exp
!
âˆ’âˆ¥x(n)âˆ’Î¼kâˆ¥2
2
âˆ’
M+2
2(Nk+min{Ï†,Î¾})
".

440
15 Asymptotic VB Theory of Mixture Models
Proof
CH =
N

n=1

z(n)
exp

log p(x(n), z(n)|w)

rw(w)
=
N

n=1
K

k=1
1
âˆš
2Ï€M exp

Î¨(Nk + Ï†) âˆ’Î¨(N + KÏ†)
âˆ’âˆ¥x(n) âˆ’Î¼kâˆ¥2
2
âˆ’M
2
1
Nk + Î¾

.
Thus, we have Eq. (15.44).
Using again Inequality (15.8), we obtain
Q â‰¤âˆ’
N

n=1
log
â›âœâœâœâœâœâ
K

k=1
Î±k
âˆš
2Ï€M exp

âˆ’âˆ¥x(n) âˆ’Î¼kâˆ¥2
2
âˆ’
M + 2
2(Nk + min{Ï†, Î¾})
ââŸâŸâŸâŸâŸâ 
âˆ’
N
2(N + KÏ†),
(15.46)
and
Q â‰¥âˆ’
N

n=1
log
â›âœâœâœâœâœâ
K

k=1
Î±k
âˆš
2Ï€M exp

âˆ’âˆ¥x(n) âˆ’Î¼kâˆ¥2
2
ââŸâŸâŸâŸâŸâ âˆ’
N
N + KÏ†,
which give upper- and lower-bounds in Eq. (15.45), respectively.
â–¡
Now, from the preceding lemmas, we prove Theorem 15.5 by showing
upper- and lower-bounds, respectively. First, we show the upper-bound in
Eq. (15.36).
From Lemma 15.1, Lemma 15.8, and Lemma 15.9, it follows that
F âˆ’NS N(D) â‰¤min
w TN(w) + C,
(15.47)
where
TN(w) = G(Î±) + Î¾
2
K

k=1
âˆ¥Î¼k âˆ’Î¼0âˆ¥2 + NEN(w).
From Eq. (15.47), it is noted that the function values of TN(w) at speciï¬c
points of the variational parameter w give upper-bounds of the VB free energy
FVB(D). Hence, let us consider following two cases.
(I) Consider the case where all components, including redundant ones, are
used to learn K0 true components, i.e.,
Î±k = Î±âˆ—
kN + Ï†
N + KÏ†
(1 â‰¤k â‰¤K0 âˆ’1),

15.2 Mixture of Gaussians
441
Î±k =
Î±âˆ—
K0N/(K âˆ’K0 + 1) + Ï†
N + KÏ†
(K0 â‰¤k â‰¤K),
Î¼k = Î¼âˆ—
k
(1 â‰¤k â‰¤K0 âˆ’1),
Î¼k = Î¼âˆ—
K0
(K0 â‰¤k â‰¤K).
Then we obtain
NEN(w)
<
N

n=1
log p(x(n)|wâˆ—) âˆ’
N

n=1
log N + Ï†
N + KÏ†
âˆ’
N

n=1
log
â›âœâœâœâœâœâœâ
K0âˆ’1

k=1
Î±âˆ—
k
âˆš
2Ï€M exp
â›âœâœâœâœââˆ’âˆ¥x(n) âˆ’Î¼âˆ—
kâˆ¥2
2
âˆ’
M + 2
2(Î±âˆ—
kN + min{Î¾, Ï†})
ââŸâŸâŸâŸâ 
+
Î±âˆ—
K0
âˆš
2Ï€M exp
â›âœâœâœâœâœâœâœâœââˆ’âˆ¥x(n) âˆ’Î¼âˆ—
kâˆ¥2
2
âˆ’
M + 2
2(
Î±âˆ—
K0
Kâˆ’K0+1N + min{Î¾, Ï†})
ââŸâŸâŸâŸâŸâŸâŸâŸâ 
ââŸâŸâŸâŸâŸâŸâŸâŸâ 
<
N

n=1
log
N+KÏ†
N+Ï† p(x(n)|wâˆ—)
p(x(n)|wâˆ—) exp
!
âˆ’
(M+2)(Kâˆ’K0+1)
2(mink{Î±âˆ—
k}N+min{Î¾,Ï†}(Kâˆ’K0+1))
"
< (K âˆ’1)Ï†
N + Ï†
+
(M + 2)(K âˆ’K0 + 1)N
2(mink{Î±âˆ—
k}N + min{Î¾, Ï†}(K âˆ’K0 + 1))
â‰¤(K âˆ’1)Ï† +
 M + 2
2
 K âˆ’K0 + 1
mink{Î±âˆ—
k} ,
where the ï¬rst inequality follows from
Î±âˆ—
kN+Ï†
N+KÏ† > Î±âˆ—
k
N+Ï†
N+KÏ† and the third
inequality follows from log(1 + x) â‰¤x for x > âˆ’1.
It follows that
TN(w) < MK + K âˆ’1
2
log N + Câ€²,
(15.48)
where Câ€² is a constant.
(II) Consider the case where the redundant components are eliminated, i.e.,
Î±k = Î±âˆ—
kN + Ï†
N + KÏ†
(1 â‰¤k â‰¤K0),
Î±k =
Ï†
N + KÏ†
(K0 + 1 â‰¤k â‰¤K),
Î¼k = Î¼âˆ—
k
(1 â‰¤k â‰¤K0),
Î¼k = Î¼0
(K0 + 1 â‰¤k â‰¤K).

442
15 Asymptotic VB Theory of Mixture Models
Then it holds that
NEN(w)
<
N

n=1
log
p(x(n)|wâˆ—)
N+Ï†
N+KÏ†
K0
k=1
Î±âˆ—
k
âˆš
2Ï€M exp
!
âˆ’
âˆ¥x(n)âˆ’Î¼âˆ—
kâˆ¥2
2
âˆ’
M+2
2(Î±âˆ—
kN+min{Î¾,Ï†})
"
< (K âˆ’1)Ï†N
N + Ï†
+
 M + 2
2

N
mink{Î±âˆ—
k}N + min{Î¾, Ï†}
â‰¤(K âˆ’1)Ï† +
 M + 2
2

1
mink{Î±âˆ—
k}.
(15.49)
The ï¬rst inequality follows from
Î±âˆ—
kN+Ï†
N+KÏ† > Î±âˆ—
k
N+Ï†
N+KÏ† and
K

k=K0+1
Î±k
âˆš
2Ï€M exp

âˆ’âˆ¥x(n) âˆ’Î¼kâˆ¥2
2
âˆ’
M + 2
2(Nk + min{Ï†, Î¾})

> 0.
The second inequality follows from log(1 + x) â‰¤x for x > âˆ’1.
It follows that
TN(w) <
)
(K âˆ’K0)Ï† + MK0 + K0 âˆ’1
2
1
log N + Câ€²â€²,
(15.50)
where Câ€²â€² is a constant.
From Eqs. (15.47), (15.48), and (15.50), we obtain the upper-bound in
Eq. (15.36).
Next we show the lower-bound in Eq. (15.36). It follows from Lemma 15.1,
Lemma 15.8, and Lemma 15.9 that
F âˆ’NS N(D) â‰¥min
Î± {G(Î±)} + NEN(w) âˆ’C âˆ’1.
(15.51)
If Ï† â‰¥M+1
2 , then,
G(Î±) â‰¥MK + K âˆ’1
2
log N âˆ’
 M + 1
2
âˆ’Ï†

K log K,
(15.52)
since Jensenâ€™s inequality yields that
K

k=1
logÎ±k â‰¤K log
â›âœâœâœâœâœâ
1
K
K

k=1
Î±k
ââŸâŸâŸâŸâŸâ = K log
 1
K

.

15.3 Mixture of Exponential Family Distributions
443
If Ï† < M+1
2 , then
G(Î±) â‰¥
2
(K âˆ’1)Ï† + M
2
3
log N +
 M + 1
2
âˆ’Ï†

(K âˆ’1) log
Ï†N
N + KÏ† + Câ€²â€²â€²
â‰¥
2
(K âˆ’1)Ï† + M
2
3
log N +
 M + 1
2
âˆ’Ï†

(K âˆ’1) log
Ï†
1 + KÏ† + Câ€²â€²â€²,
(15.53)
where Câ€²â€²â€² is a constant. The ï¬rst inequality follows since
Î±k â‰¥
Ï†
N + KÏ†
holds for every k, and the constraint
K

k=1
Î±k = 1
ensures that | logÎ±k| is bounded by a constant independent of N for at least one
index k. From Eqs. (15.51), (15.52), and (15.53) we obtain the lower-bound in
Eq. (15.36).
15.3 Mixture of Exponential Family Distributions
The previous theorem for the GMM can be generalized to the mixture of
exponential family distributions (Watanabe and Watanabe, 2005, 2007). The
model that we consider is deï¬ned by
p(z|Î±) = MultinomialK,1(z; Î±),
(15.54)
p(t|z, {Î·k}K
k=1) =
K

k=1
,
exp

Î·âŠ¤
k t âˆ’A(Î·k) + B(t)
 -zk ,
(15.55)
p(Î±|Ï†) = DirichletK(Î±; (Ï†,. . . , Ï†)âŠ¤),
(15.56)
p(Î·k|Î½0, Î¾) =
1
C(Î¾, Î½0) exp

Î¾(Î½âŠ¤
0 Î·k âˆ’A(Î·k))
 
.
(15.57)
As demonstrated in Section 4.1.2, under the constraint, r(H, w)
=
rH(H)rw(w), the VB posteriors are given as follows:
r({z(n)}N
n=1, Î±, {Î·k}K
k=1) = rz({z(n)}N
n=1)rÎ±(Î±)rÎ·({Î·k}K
k=1),
rz({z(n)}N
n=1) =
N

n=1
MultinomialK,1

z(n);z(n) 
,

444
15 Asymptotic VB Theory of Mixture Models
rÎ±(Î±) = Dirichlet

Î±; (Ï†1,. . . ,Ï†K)âŠ¤ 
,
rÎ·({Î·k}K
k=1) =
K

k=1
1
C(Î¾k,Î½k)
exp
Î¾k(Î½âŠ¤
k Î·k âˆ’A(Î·k))
 
,
(15.58)
The variational parameters {z(n)}N
n=1, {Ï†k}K
k=1, {Î½k,Î¾k}K
k=1 minimize the free
energy,
F = log
â›âœâœâœâœâœâ
Î“(K
k=1 Ï†k)
K
k=1 Î“(Ï†k)
ââŸâŸâŸâŸâŸâ âˆ’log
 Î“(KÏ†)
(Î“(Ï†))K

âˆ’
K

k=1
logC(Î¾k,Î½k) + K logC(Î¾, Î½0)
+
N

n=1
K

k=1
z (n)
k
logz (n)
k
+
K

k=1
Ï†k âˆ’Ï† âˆ’Nk
 
Î¨(Ï†k) âˆ’Î¨(K
kâ€²=1 Ï†kâ€²)
 
+
K

k=1
â¡â¢â¢â¢â¢â£Î·âŠ¤
k
,
Î¾ $Î½k âˆ’Î½0
% + Nk

Î½k âˆ’tk
 -
+
Î¾k âˆ’Î¾ âˆ’Nk
 âˆ‚logC(Î¾k,Î½k)
âˆ‚Î¾k
â¤â¥â¥â¥â¥â¦
âˆ’
N

n=1
B(t(n)),
(15.59)
where
Nk =
N

n=1
z (n)
k ,
tk = 1
Nk
N

n=1

z(n)
k

rH(H) t(n),
Î·k = 1
Î¾k
âˆ‚logC(Î¾k,Î½k)
âˆ‚Î½k
.
The stationary condition of the free energy yields
z (n)
k
=
z(n)
k
K
kâ€²=1 z(n)
kâ€²
,
(15.60)
Î±k = Nk + Ï†,
(15.61)
Î½k = Nktk + Î¾Î½0
Nk + Î¾
,
(15.62)
Î¾k = Nk + Î¾.
(15.63)
where
z(n)
k
âˆexp

Î¨(Ï†k) +Î·âŠ¤
k t(n) âˆ’A(Î·k)
rÎ·(Î·k)
 
.
(15.64)
We assume the following conditions.

15.3 Mixture of Exponential Family Distributions
445
Assumption 15.2
The true distribution q(t) of sufï¬cient statistics is repre-
sented by a mixture of exponential family distributions p(t|wâˆ—), which has K0
components and the parameter wâˆ—= {Î±âˆ—
k, Î·âˆ—
k}K0
k=1:
q(t) = p(t|wâˆ—) =
K0

k=1
Î±âˆ—
k exp

Î·âˆ—
k
âŠ¤t âˆ’A(Î·âˆ—
k) + B(t)
 
,
where Î·âˆ—
k âˆˆRM and Î·âˆ—
k  Î·âˆ—
kâ€²(k  kâ€²). Also, assume that the true distribution
can be achieved with the model, i.e., K â‰¥K0 holds.
Assumption 15.3
The prior distribution p({Î·k}K
k=1|Î½0, Î¾) deï¬ned by Eq.
(15.57) satisï¬es 0 < p({Î·k}K
k=1|Î½0, Î¾) < âˆ.
Assumption 15.4
Regarding the distribution p(t|Î·) of each component, the
Fisher information matrix
F(Î·) = âˆ‚2A(Î·)
âˆ‚Î·âˆ‚Î·âŠ¤
satisï¬es 0 < det $F(Î·)% < +âˆfor an arbitrary Î· âˆˆH. The function Î½âŠ¤Î· âˆ’A(Î·)
has a stationary point at Î· in the interior of H for each Î½ âˆˆ
, âˆ‚A(Î·)
âˆ‚Î· ; Î· âˆˆH
-
.
The following theorem will be proven under these conditions. The proof
will appear in the next section. Here,
S N(D) = âˆ’1
N
N

n=1
log p(t(n)|wâˆ—)
(15.65)
is the empirical entropy.
Theorem 15.10
The relative VB free energy of the mixture of exponential
family distributions satisï¬es
Î»â€²VB
MM log N + NEN(w) + Op(1) â‰¤FVB(D) = FVB(D) âˆ’NS N(D)
â‰¤Î»
â€²VB
MM log N + Op(1),
(15.66)
where Î»â€²VB
MM and Î»
â€²VB
MM are given by
Î»â€²VB
MM =
) (K âˆ’1)Ï† + M
2
(Ï† < M+1
2 ),
MK+Kâˆ’1
2
(Ï† â‰¥M+1
2 ),
(15.67)
Î»
â€²VB
MM =
) (K âˆ’K0)Ï† + MK0+K0âˆ’1
2
(Ï† < M+1
2 ),
MK+Kâˆ’1
2
(Ï† â‰¥M+1
2 ).
(15.68)

446
15 Asymptotic VB Theory of Mixture Models
Again in this theorem,
EN(w) = 1
N
N

n=1
log p(t(n)|wâˆ—)
p(t(n)|w)
(15.69)
is the training error, and
NEN(w) â‰¥NEN(wML),
(15.70)
holds for the (maximum) log-likelihood ratio statistic. As discussed in Section
13.5.3, the log-likelihood ratio statistics of some singular models diverge to
inï¬nity as N increases. Some known facts about the divergence of the log-
likelihood ratio are described in the following examples. Note again that even
if NEN(wML) diverges to minus inï¬nity, Eq. (15.70) does not necessarily mean
NEN(w) diverges in the same order.
If the domain of the sufï¬cient statistics t of the model p(t|w) is discrete and
ï¬nite, we obtain the following theorem by Lemmas 15.3 and 15.4:
Theorem 15.11
If the domain of the sufï¬cient statistics t is discrete and ï¬nite,
the relative VB free energy of the mixture of exponential family distributions
satisï¬es
FVB(D) = Î»
â€²VB
MM log N + Op(1),
(15.71)
where the coefï¬cient Î»
â€²VB
MM is given by Eq. (15.68).
The proof of this theorem follows the proof of the preceding theorem.
Examples
The following are examples where Theorems 15.10 and 15.11 apply.
Example 1 (Binomial)
Consider a mixture of binomial component distribu-
tions. Each component has a one-dimensional parameter Î½ âˆˆ[0, 1]:
p(x = k|Î½) = BinomialT(k; Î½) =
T
k

Î½k(1 âˆ’Î½)Tâˆ’k,
(15.72)
where T is the number of Bernoulli trials and k = 0, 1, 2,. . . , T. Hence, M = 1
and the natural parameter is given by Î· = log
Î½
1âˆ’Î½. Theorem 15.11 applies with
M = 1.

15.3 Mixture of Exponential Family Distributions
447
Example 2 (Gamma)
Consider the gamma component with shape parameter
Î± > 0 and scale parameter Î² > 0:
p(x|Î±, Î²) = Gamma(x; Î±, Î²) =
Î²Î±
Î“(Î±) xÎ±âˆ’1 exp (âˆ’Î²x) ,
(15.73)
where 0 â‰¤x < âˆ. The natural parameter Î· is given by Î·1 = Î² and Î·2 = Î± âˆ’1.
Hence, Eq. (15.66) holds where Î»â€²VB
MM and Î»
â€²VB
MM are given by Eqs. (15.67) and
(15.68) with M = 2. When shape parameter Î± is known, the likelihood ratio in
ML learning diverges in the order of log log N (Liu et al., 2003). This implies
that NEN(w) = Op(log log N) from Eq. (15.70).
Example 3 (Gaussian)
Consider the L-dimensional Gaussian component
with mean Î¼ and covariance matrix Î£:
p(x|Î¼, Î£) = GaussL(x; Î¼, Î£) =
1
(2Ï€)L/2|Î£|1/2 exp

âˆ’1
2(x âˆ’Î¼)âŠ¤Î£âˆ’1(x âˆ’Î¼)

.
The natural parameter Î· is given by Î¼TÎ£âˆ’1 and Î£âˆ’1. These are functions of the
elements of Î¼ and the upper-right half of Î£âˆ’1. Hence, Eq. (15.66) holds where
Î»â€²VB
MM and Î»
â€²VB
MM are given by Eqs. (15.67) and (15.68) with M = L + L(L + 1)/2.
If the covariance matrix Î£ is known and the parameter is restricted to mean Î¼,
it is conjectured that the likelihood ratio in ML learning diverges in the order of
log log N (Hartigan, 1985). This suggests that the likelihood ratio can diverge
in a higher order than log log N if the covariance matrices are also estimated.
Other than these examples, Theorems 15.10 and 15.11 apply to mixtures of
distributions such as multinomial, Poisson, and Weibull.
Proof of Theorem 15.10
Here Theorem 15.10 is proved in the same way as Theorem 15.5.
Since the VB posterior satisï¬es rw(w) = rÎ±(Î±)rÎ·({Î·k}K
k=1), we have
R = KL(rw(w)||p(w))
= KL(rÎ±(Î±)||p(Î±|Ï†)) +
K

k=1
KL(rÎ·(Î·k)||p(Î·k|Î½0, Î¾)).
(15.74)
The following lemma is used for evaluating KL(rÎ·(Î·k)||p(Î·k|Î½0, Î¾)) in the
case of the mixture of exponential family distributions.
Lemma 15.12
It holds that
KL(rÎ·(Î·k)||p(Î·k|Î½0, Î¾)) = M
2 log(Nk + Î¾) âˆ’log p(Î·k|Î½0, Î¾) + Op(1),

448
15 Asymptotic VB Theory of Mixture Models
where
Î·k = Î·k

rÎ·(Î·k) = 1
Î¾k
âˆ‚logC(Î¾k,Î½k)
âˆ‚Î½k
.
(15.75)
Proof
Using the VB posterior, Eq. (15.58), we obtain
KL(rÎ·(Î·k)||p(Î·k|Î½0, Î¾)) = âˆ’log C(Î¾k,Î½k)
C(Î¾, Î½0) + Nk
,
Î½k
Î·k

rÎ·(Î·k) âˆ’A(Î·k)
rÎ·(Î·k)
-
,
(15.76)
where we used Î¾k = Nk + Î¾. Let us now evaluate the value of C(Î¾k,Î½k)
when Î¾k is sufï¬ciently large. From Assumption 15.4, using the saddle point
approximation, we obtain
C(Î¾k,Î½k) = exp
Î¾k{Î½âŠ¤
k Î·k âˆ’A(Î·k)}
 â›âœâœâœâœâ
2Ï€
Î¾k
ââŸâŸâŸâŸâ 
M/2 .
det $F(Î·k)%âˆ’1
â§âªâ¨âªâ©1 + Op
â›âœâœâœâœâ
1
Î¾k
ââŸâŸâŸâŸâ 
â«âªâ¬âªâ­,
(15.77)
where Î·k is the maximizer of the functionÎ½âŠ¤Î·k âˆ’A(Î·k), that is,
âˆ‚A(Î·k)
âˆ‚Î·k
= Î½k.
Therefore, âˆ’logC(Î¾k,Î½k) is evaluated as
âˆ’logC(Î¾k,Î½k) = M
2 log
Î¾k
2Ï€ + 1
2 log det $F(Î·k)% âˆ’Î¾k
,
Î½âŠ¤
k Î·k âˆ’A(Î·k)
-
+ Op
â›âœâœâœâœâ
1
Î¾k
ââŸâŸâŸâŸâ .
(15.78)
Applying the saddle point approximation to
Î·k âˆ’Î·k =
1
C(Î¾k,Î½k)

(Î·k âˆ’Î·k) exp
Î¾k
,
Î½âŠ¤
k Î·k âˆ’A(Î·k)
- 
dÎ·k,
we obtain
||Î·k âˆ’Î·k|| â‰¤Aâ€²
Î¾k
+ Op
Î¾âˆ’3/2
k
 
,
(15.79)
where Aâ€² is a constant. Since
A(Î·k) âˆ’A(Î·k) = (Î·k âˆ’Î·k)âŠ¤Î½k + 1
2(Î·k âˆ’Î·k)âŠ¤F(Î·k)(Î·k âˆ’Î·k),
(15.80)

15.3 Mixture of Exponential Family Distributions
449
for some point Î·k on the line segment between Î·k and Î·k, we have
A(Î·k) âˆ’A(Î·k) = (Î·k âˆ’Î·k)âŠ¤Î½k + Op(Î¾âˆ’2
k ),
(15.81)
and applying the saddle point approximation, we obtain
A(Î·k)
rÎ·(Î·k) âˆ’A(Î·k) = (Î·k âˆ’Î·k)âŠ¤Î½k + M
2Î¾k
+ Op
Î¾âˆ’3/2
k
 
.
(15.82)
From Eqs. (15.81) and (15.82), we have
A(Î·k)
rÎ·(Î·k) âˆ’A(Â¯Î·k) = M
2Î¾k
+ Op
Î¾âˆ’3/2
k
 
,
(15.83)
Thus, from Eqs. (15.76), (15.78), (15.81), and (15.82), we obtain the
lemma.
â–¡
Lemmas 15.8 and 15.9 are substituted by the following lemmas.
Lemma 15.13
It holds that
R âˆ’G(Î±) +
K

k=1
log p(Î·k|Î½0, Î¾)
 â‰¤C,
(15.84)
where C is a constant and the function G(Î±) is deï¬ned by Eq. (15.40).
Proof
From Eqs. (15.41), (15.43), and (15.74) and Lemma 15.12,
R âˆ’G(Î±) +
K

k=1
log p(Î·k|Î½0, Î¾)

is upper-bounded by a constant since
1
N + Î¾ <
1
Nk + Î¾
< 1
Î¾ .
â–¡
Lemma 15.14
It holds that
NEN(w) + Op(1) â‰¤Q = âˆ’logCH âˆ’NS N(D) â‰¤NEN(w) + Op(1),
(15.85)
where the function EN(w) is deï¬ned by Eq. (15.69) and
EN(w) = 1
N
N

n=1
log
p(t(n)|wâˆ—)
K
k=1 Î±kp(t(n)|Î·k) exp
!
âˆ’
A
Nk+min{Ï†,Î¾}
",
where A is a constant.

450
15 Asymptotic VB Theory of Mixture Models
Proof
CH =
N

n=1
K

k=1
exp

log Î±kp(t(n)|Î·k)

rw(w)
=
N

n=1
K

k=1
exp

Î¨(Nk + Ï†) âˆ’Î¨(N + KÏ†) +Î·âŠ¤
k t(n) âˆ’A(Î·k)
rÎ·(Î·k) + B(t(n))
 
.
Again, using the inequalities in Eqs. (15.8) and (15.83), we obtain
Q â‰¤
N

n=1
log
â›âœâœâœâœâœâ
K

k=1
Î±kp(t(n)|Î·k) exp

âˆ’
M + 2
2(Nk + min{Ï†, Î¾})
+ Op
!
N
âˆ’3
2
k
"ââŸâŸâŸâŸâŸâ + Op(1),
Q â‰¥âˆ’
N

n=1
log
â›âœâœâœâœâœâ
K

k=1
Î±kp(t(n)|Î·k)
ââŸâŸâŸâŸâŸâ + Op(1),
which give the upper- and lower-bounds in Eq. (15.85), respectively.
â–¡
Since the prior distribution p({Î·k}K
k=1|Î½0, Î¾) satisï¬es 0 < p({Î·k}K
k=1|Î½0, Î¾)
< âˆ, from Lemmas 15.13 and 15.14, we complete the proof of Theorem 15.10
in the same way as that of Theorem 15.5.
Proof of Theorem 15.11
The upper-bound follows from Theorem 15.10. From Lemmas 15.1 and 15.13
and the boundedness of the prior, we have the following lower-bound:
F âˆ’NS N(D) â‰¥G(Î±) + Q + Op(1).
Lemma 15.4 implies that for Îµ > 0, if D âˆˆT N
Îµ (pâˆ—) andp  Râˆ—
CÎµ2 for the constant
C in the lemma,
Q = Î©p(N).
Since G(Î±) = Op(log N), this means that if the free energy is minimized,
p âˆˆRâˆ—
CÎµ2 for sufï¬ciently large N, which implies that at least K0 components
are active and
| logÎ±k| = Op(1)
holds for at least K0 components. By minimizing G(Î±) under this constraint
and the second assertion of Lemma 15.4, we have
FVB(D) âˆ’NS N(D) â‰¥Î»
â€²VB
MM log N + Op(1),
for D âˆˆT N
Îµ (pâˆ—). Because the probability that the observed data sequence is
strongly Îµ-typical tends to 1 as N â†’âˆfor any Îµ > 0 by Lemma 15.3, we
obtain the theorem.

15.4 Mixture of Bernoulli with Deterministic Components
451
15.4 Mixture of Bernoulli with Deterministic Components
In the previous sections, we assumed that all true component parameters are in
the interior of the parameter space. In this section, we consider the Bernoulli
mixture model when some components are at the boundary of the parameter
space (Kaji et al., 2010).
For an M-dimensional binary vector, x = (x1,. . . , xM)âŠ¤âˆˆ{0, 1}M, we deï¬ne
the Bernoulli distribution with parameter Î¼ = (Î¼1,. . . , Î¼M)âŠ¤as
BernM(x|Î¼) =
M

m=1
Î¼xm
m (1 âˆ’Î¼m)(1âˆ’xm).
For each element of Î¼, its conjugate prior, the Beta distribution, is given by
Beta(Î¼; a, b) =
1
B(a, b)Î¼aâˆ’1(1 âˆ’Î¼)bâˆ’1,
for a, b > 0.
The Bernoulli mixture model that we consider is given by
p(z|Î±) = MultinomialK,1(z; Î±),
(15.86)
p(x|z, {Î¼k}K
k=1) =
K

k=1
6BernM(x; Î¼k)7zk ,
(15.87)
p(Î±|Ï†) = DirichletK(Î±; (Ï†,. . . , Ï†)âŠ¤),
(15.88)
p(Î¼k|Î¾) =
M

m=1
Beta(Î¼km; Î¾, Î¾),
(15.89)
where Ï† > 0 and Î¾ > 0 are hyperparameters. Under the constraint, r(H, w) =
rH(H)rw(w), the VB posteriors are given as follows:
r({z(n)}N
n=1, Î±, {Î¼k}K
k=1) = rz({z(n)}N
n=1)rÎ±(Î±)rÎ¼({Î¼k}K
k=1),
rz({z(n)}N
n=1) =
N

n=1
MultinomialK,1

z(n);z(n) 
,
rÎ±(Î±) = Dirichlet

Î±; (Ï†1,. . . ,Ï†K)âŠ¤ 
,
rÎ¼({Î¼k}K
k=1) =
K

k=1
M

m=1
Beta

Î¼km;akm,bkm
 
.

452
15 Asymptotic VB Theory of Mixture Models
The variational parameters {z(n)}N
n=1, {Ï†k}K
k=1, {{akm}M
m=1, {bkm}M
m=1}K
k=1 minimize
the free energy,
F =
N

n=1
K

k=1
z (n)
k
logz (n)
k
+ log
â›âœâœâœâœâœâ
Î“(K
k=1 Ï†k)
K
k=1 Î“(Ï†k)
ââŸâŸâŸâŸâŸâ âˆ’log
 Î“(KÏ†)
(Î“(Ï†))K

+
K

k=1
Ï†k âˆ’Ï† âˆ’Nk
 
Î¨(Ï†k) âˆ’Î¨(K
kâ€²=1 Ï†kâ€²)
 
+
K

k=1
M

m=1
â§âªâªâ¨âªâªâ©log
â›âœâœâœâœâ
Î“(akm +bkm)
Î“(akm)Î“(bkm)
ââŸâŸâŸâŸâ âˆ’log
 Î“(2Î¾)
(Î“(Î¾))2

+

akm âˆ’Î¾ âˆ’Nkxkm
 
Î¨(akm) âˆ’Î¨(akm +bkm)
 
+
bkm âˆ’Î¾ âˆ’Nk(1 âˆ’xkm)
 
Î¨(bkm) âˆ’Î¨(akm +bkm)
 -
,
where
Nk =
N

n=1

z(n)
k

rH(H) ,
xkm = 1
Nk
N

n=1

z(n)
k

rH(H) x(n)
m ,
for k = 1,. . . , K and m = 1,. . . , M. The stationary condition of the free energy
yields
z (n)
k
=
z(n)
k
K
kâ€²=1 z(n)
kâ€²
,
(15.90)
Ï†k = Nk + Ï†,
(15.91)
akm = Nkxkm + Î¾,
(15.92)
bkm = Nk(1 âˆ’xkm) + Î¾,
(15.93)
where
z(n)
k
= exp

Î¨(Ï†k) âˆ’Î¨(K
kâ€²=1 Ï†kâ€²) + M
m=1
,
x(n)
m

Î¨(akm) âˆ’Î¨(akm +bkm)
 
+ (1 âˆ’x(n)
m )

Î¨(bkm) âˆ’Î¨(akm +bkm)
 - 
.
(15.94)
We assume the following condition.
Assumption 15.5
For 0 â‰¤Kâˆ—
1 â‰¤Kâˆ—
0 â‰¤K, the true distribution q(x) =
p(x|wâˆ—) is represented by Kâˆ—
0 components and the parameter is given by
wâˆ—= {Î±âˆ—
k, Î¼âˆ—
k}
Kâˆ—
0
k=1:
q(x) = p(x|wâˆ—) =
Kâˆ—
0

k=1
Î±âˆ—
kBernM(x; Î¼âˆ—
k),

15.4 Mixture of Bernoulli with Deterministic Components
453
where
0 < Î¼âˆ—
km < 1 (1 â‰¤k â‰¤Kâˆ—
1),
Î¼âˆ—
km = 0 or 1 (Kâˆ—
1 + 1 â‰¤k â‰¤Kâˆ—
0).
We deï¬ne Î”Kâˆ—= Kâˆ—
0 âˆ’Kâˆ—
1.
Let K0 be the number of components satisfying Nk/N = Î©p(1) and K1
be the number of components satisfying xkm = Î©p(1) and 1 âˆ’xkm = Î©p(1)
for all m = 1, Â· Â· Â· , M. Then, for Î”K â‰¡K0 âˆ’K1 components, it holds that
Nk/N = Î©p(1) and xkm = op(1) or 1 âˆ’xkm = op(1). Hence, the K1 components
with Nk/N = Î©p(1) and xkm = Î©p(1) are said to be â€œnondeterministicâ€ and
the Î”K components are said to be â€œdeterministic,â€ respectively. We have the
following theorem.
Theorem 15.15
The relative free energy of the Bernoulli mixture model
satisï¬es
FVB(D) = FVB(D) âˆ’NS N(D)
=
) M + 1
2
âˆ’Ï†

K1+
1
2 âˆ’Ï† + MÎ¾

Î”K + KÏ† âˆ’1
2
1
log N + Î©p

N J
 
+ Op(1),
where J = 1 if K1 < Kâˆ—
1 or Î”K < Î”Kâˆ—and otherwise J = 0.
The proof of Theorem 15.15 is shown after the next theorem. The following
theorem claims that the numbers of deterministic and nondeterministic com-
ponents are essentially determined by the hyperparameters.
Theorem 15.16
The estimated numbers of components K0 and K1 of the
Bernoulli mixture model are determined as follows:
(1) If M+1
2
âˆ’Ï† > 0 and 1
2 âˆ’Ï† + MÎ¾ > 0, then K1 = Kâˆ—
1 and Î”K = Î”Kâˆ—.
(2) If M+1
2
âˆ’Ï† > 0 and 1
2 âˆ’Ï† + MÎ¾ < 0, then K1 = Kâˆ—
1 and Î”K = K âˆ’Kâˆ—
1.
(3) If M+1
2
âˆ’Ï† < 0 and 1
2 âˆ’Ï† + MÎ¾ > 0, then K1 = K âˆ’Î”Kâˆ—and Î”K = Î”Kâˆ—.
(4) If M+1
2
âˆ’Ï† < 0 and 1
2 âˆ’Ï† + MÎ¾ < 0, and
(a) if Î¾ > 1
2, then K1 = K âˆ’Î”Kâˆ—and Î”K = Î”Kâˆ—.
(b) if Î¾ < 1
2, then K1 = Kâˆ—
1 and Î”K = K âˆ’Kâˆ—
1.
Proof
Minimizing the coefï¬cient of the relative free energy with respect to
K1 and Î”K under the constraint that the true distribution is realizable, i.e.,
K1 â‰¥Kâˆ—
1 and Î”K â‰¥Î”Kâˆ—, we obtain the theorem.
â–¡

454
15 Asymptotic VB Theory of Mixture Models
Proof of Theorem 15.15
From Lemma 15.1, we ï¬rst evaluate R = KL(rw(w)||p(w)). The inequalities of
the di-gamma and log-gamma functions in Eqs. (15.8) and (15.9) yield that
R =
K

k=1
1
2 âˆ’Ï†

log(Nk + Ï†) +

KÏ† âˆ’1
2

log(N + KÏ†)
+
K

k=1
M

m=1
)1
2 âˆ’Î¾

log(Nkxkm + Î¾) +
1
2 âˆ’Î¾

log(Nk(1 âˆ’xkm) + Î¾)
1
+
K

k=1
M

2Î¾ âˆ’1
2

log(Nk + 2Î¾) + Op(1).
We consider variational parameters in which K0 components are active, i.e.,
Nk = Î©p(N). Furthermore, without loss of generality, we can assume that
0 < xkm < 1 (1 â‰¤m â‰¤M) for K1 nondeterministic components and
xkm = Op(1/N) (1 â‰¤m â‰¤M) for Î”K deterministic components. Putting such
variational parameters into the preceding expression, we have the asymptotic
form in the theorem.
Lemmas 15.3 and 15.4 imply that if K1 < Kâˆ—
1 or Î”K < Î”Kâˆ—, Q = âˆ’logCH âˆ’
NS N(D) = Î©p(N), and otherwise Q = Op(1). Thus, we obtain the theorem.

16
Asymptotic VB Theory of Other Latent
Variable Models
In this chapter, we proceed to asymptotic analyses of VB learning in other
latent variable models discussed in Section 4.2, namely, Bayesian networks,
hidden Markov models, probabilistic context-free grammar, and latent
Dirichlet allocation.
16.1 Bayesian Networks
In this section, we analyze the VB free energy of the following Bayesian
network model (Watanabe et al., 2009), introduced in Section 4.2.1:
p(x|w) =

zâˆˆZ
p(x, z|w),
(16.1)
p(x, z|w) = p(x|bz)
K

k=1
Tk

i=1
azk,i
(k,i),
p(x|bz) =
M

j=1
Y j

l=1
b
xj,l
(j,l|z),
p(w) =
â§âªâªâ¨âªâªâ©
K

k=1
p(ak|Ï†)
â«âªâªâ¬âªâªâ­
â§âªâªâªâ¨âªâªâªâ©

zâˆˆZ
M

j=1
p(b j|z|Î¾)
â«âªâªâªâ¬âªâªâªâ­,
p(ak|Ï†) = DirichletTk

ak; (Ï†,. . . , Ï†)âŠ¤ 
,
p(b j|z|Î¾) = DirichletY j

b j|z; (Î¾,. . . , Î¾)âŠ¤ 
,
where Ï† > 0 and Î¾ > 0 are hyperparameters. Here, Z = {(z1,. . . , zK); zk âˆˆ
{ei}Tk
i=1, k = 1,. . . , K}, and zk âˆˆ{ei}Tk
i=1 is the one-of-K representation, i.e.,
zk,i = 1 for some i âˆˆ{1,. . . , Tk} and zk, j = 0 for j  i. Also, x = (x1,. . . , xM)
for x j âˆˆ{el}
Y j
l=1. The number of the parameters of this model is
455

456
16 Asymptotic VB Theory of Other Latent Variable Models
D = Mobs
K

k=1
Tk +
K

k=1
(Tk âˆ’1),
(16.2)
where
Mobs =
M

j=1
(Yj âˆ’1).
Under the constraint, r(H, w) = rH(H)rw(w), the VB posteriors are given by
rw(w) =
â§âªâªâ¨âªâªâ©
K

k=1
ra(ak)
â«âªâªâ¬âªâªâ­
â§âªâªâªâ¨âªâªâªâ©

zâˆˆZ
M

j=1
rb(bj|z)
â«âªâªâªâ¬âªâªâªâ­,
ra(ak) = DirichletTk

ak;Ï†k
 
,
(16.3)
rb(b j|z) = DirichletY j

b j|z;Î¾ j|z
 
,
(16.4)
rH(H) =
N

n=1
rz(z(n)),
where
rz(z(n) = z) âˆexp
â›âœâœâœâœâœâ
K

k=1
2
Î¨(Ï†(k,ik)) âˆ’Î¨
!Tk
iâ€²
k=1 Ï†(k,iâ€²
k)
"3
+
M

j=1
2
Î¨(Î¾(j,l(n)
j |z)) âˆ’Î¨
Y j
lâ€²=1Î¾(j,lâ€²|z)
 3ââŸâŸâŸâŸâŸâŸâ 
(16.5)
for z = (ei1,. . . , eiK) and x(n)
j
= el(n)
j . The free energy is given by
F =
K

k=1
â§âªâªâ¨âªâªâ©log
â›âœâœâœâœâœâ
Î“(Tk
i=1 Ï†(k,i))
Tk
i=1 Î“(Ï†(k,i))
ââŸâŸâŸâŸâŸâ âˆ’log
 Î“(TkÏ†)
(Î“(Ï†))Tk

+
Tk

i=1
Ï†(k,i) âˆ’Ï† âˆ’N
z
(k,i)
 
Î¨(Ï†(k,i)) âˆ’Î¨
Tk
iâ€²=1 Ï†(k,iâ€²)
  â«âªâªâ¬âªâªâ­
+

zâˆˆZ
M

j=1
â§âªâªâ¨âªâªâ©log
â›âœâœâœâœâœâœâ
Î“(Y j
l=1Î¾(j,l|z))
Y j
l=1 Î“(Î¾(j,l|z))
ââŸâŸâŸâŸâŸâŸâ âˆ’log
 Î“(YjÎ¾)
(Î“(Î¾))Y j

+
Y j

l=1
Î¾(j,l|z) âˆ’Î¾ âˆ’N
x
(j,l|z)
 
Î¨(Î¾(j,l|z)) âˆ’Î¨
Y j
lâ€²=1Î¾(j,lâ€²|z)
  
â«âªâªâªâ¬âªâªâªâ­
+
N

n=1

zâˆˆZ
rz(z(n) = z) log rz(z(n) = z),

16.1 Bayesian Networks
457
where
N
z
(k,ik) =
N

n=1

z(n)
k,ik

rH(H) ,
N
x
(j,lj|z) =
N

n=1
x(n)
j,ljrz(z(n) = z),
for
rz(z(n) = (ei1,. . . , eiK)) =
/ K

k=1
z(n)
k,ik
0
rH(H)
.
(16.6)
We assume the following condition:
Assumption 16.1
The true distribution q(x) can be expressed by a Bayesian
network with H hidden nodes, each of which has S k states, for H â‰¤K, i.e.,
q(x) = p(x|wâˆ—) =

zâˆˆZâˆ—
p(x, z|wâˆ—) =

zâˆˆZâˆ—
p(x|bâˆ—
z)
H

k=1
S k

i=1
,
aâˆ—
(k,i)
-zk,i ,
where
p(x|bâˆ—
z) =
M

j=1
Y j

l=1
,
bâˆ—
(j,l|z)
-xj,l
for z âˆˆZâˆ—= {(z1,. . . , zH); zk âˆˆ{ei}S k
i=1, k = 1,. . . , H}. The true parameters
wâˆ—= {{aâˆ—
k}H
k=1, {bâˆ—
z}zâˆˆZâˆ—} are given by
aâˆ—
k = {aâˆ—
(k,i); 1 â‰¤i â‰¤S k}
(k = 1,. . . , H),
bâˆ—
z = {bâˆ—
j|z}M
j=1
(z âˆˆZâˆ—),
bâˆ—
j|z = {bâˆ—
(j,l|z); 1 â‰¤l â‰¤Yj}
( j = 1,. . . , M).
For k > H, we deï¬ne S k = 1.
The true distribution can be realized by the model, i.e., the model is given
by Eq. (16.1), where Tk â‰¥S k holds for k = 1,. . . , H. We assume that the true
distribution is the smallest in the sense that it cannot be realized by any model
with a smaller number of hidden units and with a smaller number of the states
of each hidden unit.
Under this condition, we prove the following theorem, which evaluates the
relative VB free energy. The proof will appear in the next section.

458
16 Asymptotic VB Theory of Other Latent Variable Models
Theorem 16.1
The relative VB free energy of the Bayesian network model
satisï¬es
FVB(D) = FVB(D) âˆ’NS N(D) = Î»â€²VB
BN log N + Op(1),
where
Î»â€²VB
BN = Ï†
K

k=1
Tk âˆ’K
2 + min
{uk}
â§âªâªâ¨âªâªâ©
Mobs
2
K

k=1
uk âˆ’

Ï† âˆ’1
2

K

k=1
uk
â«âªâªâ¬âªâªâ­.
(16.7)
The minimum is taken over the set of positive integers {uk; S k â‰¤uk â‰¤Tk}K
k=1.
If K = 1, this is reduced to the case of the naive Bayesian networks whose
Bayes free energy or stochastic complexity has been evaluated (Yamazaki and
Watanabe, 2003a; Rusakov and Geiger, 2005). Bounds for their VB free energy
have also been obtained (Watanabe and Watanabe, 2004, 2005, 2006).
The coefï¬cient Î»â€²VB
BN is given by the solution of the minimization problem
in Eq. (16.7). We present a few exemplary cases as corollaries in this section.
By taking uk = S k for 1 â‰¤k â‰¤H and uk = 1 for H + 1 â‰¤k â‰¤K, we obtain
the following upper-bound for the VB free energy (Watanabe et al., 2006). This
bound is tight if Ï† â‰¤(1 + Mobs min1â‰¤kâ‰¤K{S k})/2.
Corollary 16.2
It holds that
FVB(D) â‰¤Î»â€²VB
BN log N + Op(1),
(16.8)
where
Î»â€²VB
BN = Ï†
K

k=1
Tk âˆ’Ï†K +

Ï† âˆ’1
2

H +
1
2 âˆ’Ï†

H

k=1
S k + Mobs
2
H

k=1
S k.
(16.9)
If K = H = 2, that is, the true network and the model both have two hidden
nodes, solving the minimization problem in Eq. (16.7) gives the following
corollary. Suppose S 1 â‰¥S 2 and T1 â‰¥T2.
Corollary 16.3
If K = H = 2,
FVB(D) = Î»â€²VB
BN log N + Op(1),
(16.10)
where
Î»â€²VB
BN
=
â§âªâªâªâªâªâªâ¨âªâªâªâªâªâªâ©
(T1 âˆ’S 1 + T2 âˆ’S 2)Ï† + Mobs
2 S 1S 2 + S 1+S 2
2
âˆ’1
(0 < Ï† â‰¤1+S 2Mobs
2
),
(T2 âˆ’S 2)Ï† + Mobs
2 T1S 2 + T1+S 2
2
âˆ’1
( 1+S 2Mobs
2
< Ï† â‰¤1+T1Mobs
2
),
Mobs
2 T1T2 + T1+T2
2
âˆ’1
( 1+T1Mobs
2
< Ï†).
(16.11)

16.1 Bayesian Networks
459
The leading term of the relative Bayes free energy of regular statistical
models is given by (D/2) log N (Schwarz, 1978), where D is the number of
parameters in Eq. (16.2). Corollary 16.3 claims that the coefï¬cient Î»â€²VB
BN of the
leading term is smaller than D/2 when Ï† â‰¤1+T1Mobs
2
.
Proof of Theorem 16.1
From Lemma 15.1, we can rewrite the free energy as follows:
FVB(D) = min
rw(w) [R + Q] ,
(16.12)
where
R = KL(rw(w)||p(w)),
Q = âˆ’logCH = âˆ’log

H
log p(D, H|w)
rw(w) .
From Eqs. (16.3), (16.4), and (16.5), we obtain Q and R in Eq. (16.12) as
follows:
Q = âˆ’
N

n=1
log

z(n)

log p(x(n), z(n)|w)

rw(w)
= âˆ’
N

n=1
log
â›âœâœâœâœâœâœâœâ

z=(ei1,...,eiK )
exp
â›âœâœâœâœâœâ
K

k=1
,
Î¨(N
z
(k,ik) + Ï†) âˆ’Î¨(N + TkÏ†)
-
+
M

j=1
Y j

l=1
x(n)
j,l
,
Î¨(N
x
(j,l|z) + Î¾) âˆ’Î¨(N
x
z + YjÎ¾)
-ââŸâŸâŸâŸâŸâŸâ 
ââŸâŸâŸâŸâŸâŸâ ,
(16.13)
and
R =
K

k=1
KL(ra(ak)||p(ak|Ï†)) +

z
M

j=1
KL(rb(b j|z)||p(b j|z|Î¾))
=
K

k=1
â¡â¢â¢â¢â¢â¢â¢â£
Tk

i=1
,
N
z
(k,i)Î¨(N
z
(k,i) + Ï†) âˆ’log Î“(N
z
(k,i) + Ï†)
-
âˆ’NÎ¨(N + TkÏ†) + log Î“(N + TkÏ†) + log Î“(Ï†)Tk
Î“(TkÏ†)
H
+

z
M

j=1
â¡â¢â¢â¢â¢â¢â¢â£
Y j

l=1
,
N
x
(j,l|z)Î¨(N
x
(j,l|z) + Î¾) âˆ’log Î“(N
x
(j,l|z) + Î¾)
-
âˆ’N
x
zÎ¨(N
x
z + YjÎ¾) + log Î“(N
x
z + YjÎ¾) + log Î“(Î¾)Y j
Î“(YjÎ¾)
H
.
(16.14)

460
16 Asymptotic VB Theory of Other Latent Variable Models
Furthermore, by using the inequalities for the di-gamma and log-gamma
functions in Eqs. (15.8) and (15.9), we can bound Q as follows:
Q â‰¤âˆ’
N

n=1
log
â›âœâœâœâœâœâœâœâ

z=(ei1,...,eiK )
exp
â›âœâœâœâœâœâœâ
K

k=1
â§âªâªâ¨âªâªâ©log
N
z
(k,ik) + Ï†
N + TkÏ† âˆ’
1
N
z
(k,ik) + Ï†
+
1
2(N + TkÏ†)
â«âªâªâ¬âªâªâ­
+
M

j=1
Y j

l=1
x(n)
j,l
â§âªâªâ¨âªâªâ©log
N
x
(j,l|z) + Î¾
N
x
z + YjÎ¾
âˆ’
1
N
x
(j,l|z) + Î¾
+
1
2(N
x
z + YjÎ¾)
â«âªâªâ¬âªâªâ­
ââŸâŸâŸâŸâŸâŸâ 
ââŸâŸâŸâŸâŸâŸâ .
(16.15)
We can also evaluate R in Eq. (16.12) as follows:
R =
K

k=1
)
TkÏ† âˆ’1
2

log (N + TkÏ†)
1
âˆ’
K

k=1
Tk

i=1
)
Ï† âˆ’1
2

log

N
z
(k,i) + Ï†
 1
+

z
M

j=1
â§âªâªâªâ¨âªâªâªâ©

YjÎ¾ âˆ’1
2

log

N
x
z + YjÎ¾
 
âˆ’
Y j

l=1

Î¾ âˆ’1
2

log

N
x
(j,l|z) + Î¾
 
â«âªâªâªâ¬âªâªâªâ­
+ Op(1).
(16.16)
Since FVB(D) is given as the minimum value of the function of {N
x
(j,l|z)}, we
can obtain from Eq. (16.12) an upper-bound for FVB(D) by substituting each
N
x
(j,l|z) by any speciï¬c value. Therefore, let uk be a natural number such that
S k â‰¤uk â‰¤Tk for k = 1,. . . , K and consider the following N
x
(j,l|z) for each j
and l:
N
x
(j,l|z) = Nbâˆ—
(j,l|z)
K

k=1
a(k,ik),
(16.17)
where z = (emin{l1,S 1}, emin{l2,S 2},. . . , emin{lH,S H}) and
a(k,ik) =
â§âªâªâªâªâ¨âªâªâªâªâ©
aâˆ—
(k,ik)
(1 â‰¤ik â‰¤S k âˆ’1),
aâˆ—
(k,S k)/(uk âˆ’S k + 1)
(S k â‰¤ik â‰¤uk),
0
(otherwise).
(16.18)
This corresponds to the case where uk (â‰¥S k) states of the kth hidden node are
active for k = 1,. . . , H. Then we have N
x
z = N K
k=1 a(k,ik) and N
z
(k,i) = Na(k,i).
Substituting them into Eq. (16.16) yields
R =
â§âªâªâ¨âªâªâ©Ï†
K

k=1
Tk âˆ’K
2 + Mobs
2
K

k=1
uk âˆ’

Ï† âˆ’1
2

K

k=1
uk
â«âªâªâ¬âªâªâ­log N + Op(1).
(16.19)

16.2 Hidden Markov Models
461
From Eq. (16.15), we obtain
Q â‰¤âˆ’
N

n=1
log

p(x(n)|wâˆ—) exp

Op
 1
N

= NS N(D) + Op(1).
(16.20)
From Eqs. (16.12), (16.19), and (16.20), we have proved that FVB(D) is upper-
bounded by the right-hand side of Eq. (16.19) for any data set D and {uk; S k â‰¤
uk â‰¤Tk}.
If the number of states such that N
z
(k,i) = Î˜p(N) is less than S k, i.e., uk < S k
for some k, the predictive distribution âŸ¨p(x|w)âŸ©rw(w) cannot approach the true
distribution p(x|wâˆ—). Then Lemma 15.4 implies that Q âˆ’NS N(D) = Î©p(N).
Hence, minimizing the coefï¬cient of the leading term in Eq. (16.19) under the
constraints S k â‰¤uk â‰¤Tk for all k, we complete the proof.
16.2 Hidden Markov Models
Next we analyze the VB free energy of hidden Markov models (HMMs)
(Hosino et al., 2005, 2006b), introduced in Section 4.2.2. Suppose that we
observe N sequences, D = {X(1),. . . , X(N)}, where each sequence X(n) =
(x(n,1),. . . , x(n,T)) has length T. We consider the asymptotic analysis for the
VB free energy as the number of i.i.d. sample sequences tends to inï¬nity, i.e.,
N â†’âˆwhile T is a ï¬xed constant.
The model for observed and hidden sequences X = (x(1),. . . , x(T)) and
Z = (z(1),. . . , z(T)) is given by
p(X|w) =

Z
p(X, Z|w),
(16.21)
p(X, Z|w) =
M

m=1
bx(1)
m
1,m
T

t=2
K

k=1
K

l=1
a
z(t)
l z(tâˆ’1)
k
k,l
M

m=1
b
z(t)
k x(t)
m
k,m ,
p(A|Ï†) =
K

k=1
DirichletK

ak; (Ï†,. . . , Ï†)âŠ¤ 
,
p(B|Î¾) =
K

k=1
DirichletM
bk; (Î¾,. . . , Î¾)âŠ¤ 
,
where Ï† > 0 and Î¾ > 0 are hyperparameters. Let H = {Z(1),. . . , Z(N)} be the
set of hidden sequences. Under the constraint, r(H, w) = rH(H)rw(w), the VB
posteriors are given by

462
16 Asymptotic VB Theory of Other Latent Variable Models
rw(w) = rA(A)rB(B),
rA(A) =
K

k=1
DirichletK

ak; (Ï†k,1,. . . ,Ï†k,K)âŠ¤ 
,
rB(B) =
K

k=1
DirichletM
bk; (Î¾k,1,. . . ,Î¾k,M)âŠ¤ 
,
rH(H) =
N

n=1
rZ(Z(n)),
rZ(Z(n)) =
1
CZ(n) exp
â›âœâœâœâœâœâ
T

t=2
K

k=1
K

l=1
z(n,t)
k
z(n,tâˆ’1)
l
â§âªâªâ¨âªâªâ©Î¨(Ï†k,l) âˆ’Î¨
â›âœâœâœâœâœâ
K

lâ€²=1
Ï†k,lâ€²
ââŸâŸâŸâŸâŸâ 
â«âªâªâ¬âªâªâ­
+
T

t=1
K

k=1
M

m=1
z(n,t)
k
x(t)
m
â§âªâªâ¨âªâªâ©Î¨(Î¾k,m) âˆ’Î¨
â›âœâœâœâœâœâ
M

mâ€²=1
Î¾k,mâ€²
ââŸâŸâŸâŸâŸâ 
â«âªâªâ¬âªâªâ­
ââŸâŸâŸâŸâŸâ ,
where CZ(n) is the normalizing constant. After the substitution of Eq. (15.5),
the free energy is given by
F =
K

k=1
â§âªâªâ¨âªâªâ©log
â›âœâœâœâœâœâ
Î“(K
l=1 Ï†k,l)
K
l=1 Î“(Ï†k,l)
ââŸâŸâŸâŸâŸâ +
K

l=1
Ï†k,l âˆ’Ï†
 
Î¨(Ï†k,l) âˆ’Î¨(K
lâ€²=1 Ï†k,lâ€²)
 
+ log
â›âœâœâœâœâœâ
Î“(M
m=1Î¾k,m)
M
m=1 Î“(Î¾k,m)
ââŸâŸâŸâŸâŸâ +
M

m=1
Î¾k,m âˆ’Î¾
 
Î¨(Î¾k,m) âˆ’Î¨(M
mâ€²=1Î¾k,mâ€²)
 â«âªâªâ¬âªâªâ­
âˆ’K log
 Î“(KÏ†)
(Î“(Ï†))K

âˆ’K log
 Î“(MÎ¾)
(Î“(Î¾))M

âˆ’
N

n=1
logCZ(n).
The variational parameters satisfy
Ï†k,l = N
[z]
k,l + Ï†,
Î¾k,m = N
[x]
k,m + Î¾,
for the expected sufï¬cient statistics deï¬ned by
N
[z]
k,l =
N

n=1
T

t=2

z(n,t)
l
z(n,tâˆ’1)
k

rH(H) ,
N
[x]
k,m =
N

n=1
T

t=1

z(n,t)
k

rH(H) x(n,t)
m .
We assume the following condition.

16.2 Hidden Markov Models
463
Assumption 16.2
The true distribution q(X) has K0 hidden states and emits
M-valued discrete symbols:
q(X) = p(X|wâˆ—) =

Z
M

m=1
(bâˆ—
1m)x(1)
m
T

t=2
K0

k=1
K0

l=1
(aâˆ—
kl)z(t)
l z(tâˆ’1)
k
M

m=1
(bâˆ—
km)z(t)
k x(t)
m ,
(16.22)
where 
Z is taken over all possible values of the hidden variables. Moreover,
the true parameter is deï¬ned by
wâˆ—= (Aâˆ—, Bâˆ—) = ((aâˆ—
kl), (bâˆ—
km)),
where Aâˆ—âˆˆRK0Ã—K0 and Bâˆ—âˆˆRK0Ã—m. The number of hidden states K0 of the
true distribution is the smallest under this parameterization (Ito et al., 1992)
and all parameters {aâˆ—
kl, bâˆ—
km} are strictly positive:
wâˆ—= ((aâˆ—
kl > 0), (bâˆ—
km > 0)) (1 â‰¤k, l â‰¤K0, 1 â‰¤m â‰¤M).
The statistical model given by Eq. (16.21) can attain the true distribution, thus
the model has K (â‰¥K0) hidden states.
Under this assumption, the next theorem evaluates the relative VB free
energy. Here S N(D) = âˆ’1
N
N
n=1 log p(X(n)|wâˆ—) is the empirical entropy of the
true distribution (16.22).
Theorem 16.4
The relative VB free energy of HMMs satisï¬es
FVB(D) = FVB(D) âˆ’NS N(D) = Î»â€²VB
HMM log N + Op(1),
where
Î»â€²VB
HMM =
â§âªâªâªâ¨âªâªâªâ©
K0(K0âˆ’1)+K0(Mâˆ’1)
2
+ K0(K âˆ’K0)Ï†

0 < Ï† â‰¤K0+K+Mâˆ’2
2K0
 
,
K(Kâˆ’1)+K(Mâˆ’1)
2
 K0+K+Mâˆ’2
2K0
< Ï†
 
.
(16.23)
Proof
As in the models discussed in the previous sections, we evaluate
the KL divergence from the posterior distribution to the prior distribution of
parameters:
R = KL(rw(w)||p(w))
=
K

k=1
â¡â¢â¢â¢â¢â¢â£log Î“(Nk + KÏ†) âˆ’NkÎ¨(Nk + KÏ†)
âˆ’
K

l=1
2
log Î“(N
[z]
k,l + Ï†) âˆ’N
[z]
k,lÎ¨(N
[z]
k,l + Ï†)
3

464
16 Asymptotic VB Theory of Other Latent Variable Models
+ log Î“(Nk + MÎ¾) âˆ’NkÎ¨(Nk + MÎ¾)
âˆ’
M

m=1
2
log Î“(N
[x]
k,m + Î¾) âˆ’N
[x]
k,mÎ¨(N
[x]
k,m + Î¾)
3â¤â¥â¥â¥â¥â¥â¦+ Op(1).
(16.24)
Using the inequalities of the di-gamma and the log-gamma functions in
Eqs. (15.8) and (15.9), we have
R =
K

k=1
â¡â¢â¢â¢â¢â¢â£

KÏ† âˆ’1
2

log(Nk + KÏ†) âˆ’
K

l=1

Ï† âˆ’1
2

log(N
[z]
k,l + Ï†)
+

MÎ¾ âˆ’1
2

log(Nk + MÎ¾) âˆ’
M

m=1

Î¾ âˆ’1
2

log(N
[x]
k,m + Î¾)
â¤â¥â¥â¥â¥â¥â¦+ Op(1).
(16.25)
We divide the sum over k and l in Eq. (16.25) to the necessary K0 and redundant
K âˆ’K0 terms. Moreover, we assume that additional l (0 â‰¤l â‰¤K âˆ’K0) hidden
states are used, i.e., having Nk = Î˜p(N).
R
log N =
K0

k=1
â§âªâªâ¨âªâªâ©
!
KÏ† + M
2 âˆ’1
"
âˆ’
K0

l=1

Ï† âˆ’1
2
â«âªâªâ¬âªâªâ­+ g(l) + Op

1
log N

,
(16.26)
where g(l) is given by
g(l) =
!
KÏ† + M
2 âˆ’1
"
l âˆ’

Ï† âˆ’1
2

(2K0l + l2).
If the number of states with Nk = Î˜p(N) is less than K0, Lemma 15.4 implies
that Q = âˆ’logCH âˆ’NS N(D) = Î©p(N) for data sequences in the strongly
Îµ-typical set. Otherwise, we can upper-bound FVB(D) âˆ’NS N(D) so that
Q = Op(1) similarly to the models in the previous sections. Hence, minimizing
the right-hand side of Eq. (16.26) with respect to l, we can evaluate the VB
free energy.
The minimum of g(l) is achieved by
â§âªâªâªâ¨âªâªâªâ©
l = 0

0 < Ï† â‰¤K0+K+Mâˆ’2
2K0
 
,
l = K âˆ’K0
 K0+K+Mâˆ’2
2K0
< Ï†
 
.
Putting this back into Eq. (16.26), we obtain the theorem.
â–¡
Next we consider the simple left-to-right HMMs.
Assumption 16.3
In the simple left-to-right HMMs, transition from each
hidden state is constrained to itself or the next hidden state:
{ak,l = 0, l  {k, k + 1}}.
(16.27)

16.2 Hidden Markov Models
465
Figure 16.1 State transition diagram of a left-to-right HMM.
Thus, only ak,k+1 is a substantial parameter in the transition probability.
Figure 16.1 illustrates the state transition diagram of a left-to-right HMM.
The next theorem evaluates the relative VB free energy of the left-to-right
HMM.1
Theorem 16.5
The relative VB free energy of the left-to-right HMM satisï¬es
FVB(D) = Î»â€²VB
LRâˆ’HMM log N + Op(1),
where
Î»â€²VB
LRâˆ’HMM =
â§âªâªâªâ¨âªâªâªâ©
(K0âˆ’1)+K0(Mâˆ’1)
2
+ Ï†
(Ï† â‰¤M(Kâˆ’K0)
2
),
(Kâˆ’1)+K(Mâˆ’1)
2
(Ï† > M(Kâˆ’K0)
2
).
(16.28)
Proof
From the constraints of the transition probabilities in Eq. (16.27), the
asymptotic form of the KL divergence from the VB posterior to the prior is
given by
R = KL(rw(w)||p(w))
=
Kâˆ’1

k=1
I
2Ï† âˆ’1
2

log(Nk + 2Ï†) âˆ’

Ï† âˆ’1
2
 2
log(N
[z]
k,(k+1) + Ï†) + log(N
[z]
k,k + Ï†)
3H
+
K

k=1
â¡â¢â¢â¢â¢â¢â£

MÎ¾ âˆ’1
2

log(Nk + MÎ¾) âˆ’
M

m=1

Î¾ âˆ’1
2

log(N
[x]
k,m + Î¾)
â¤â¥â¥â¥â¥â¥â¦
+ Op(1).
(16.29)
If K hidden states are used, all the variables, Nk, N
[z]
k,k, N
[z]
k,(k+1), and N
[x]
k,m are
in the order of N, which leads to the asymptotic form in the theorem. If
some states are not used, we assume that the (K0 + l)th state is the last state
that is effectively used. More speciï¬cally, if we consider the case where Nk,
N
[z]
k,k, N
[z]
k,(k+1), and N
[x]
k,m are Î˜p(N) for K0+lâˆ’1 states and N
[z]
(K0+l),(K0+l+1) = Op(1)
and N
[z]
(K0+l),(K0+l) = Î˜p(N) (and hence, NK0+l = Î˜p(N)), we obtain
R
log N = K0 âˆ’1
2
+ Ï† + K0
M âˆ’1
2
+ g(l) + Op

1
log N

,
1 This theorem is not obtained as a special case of Theorem 16.4 since some of the transition
probabilities are ï¬xed to zero and are no longer parameters.

466
16 Asymptotic VB Theory of Other Latent Variable Models
where
g(l) = M
2 l
for 0 â‰¤l â‰¤K âˆ’K0. Since the minimum of g(l) is obviously obtained by l = 0,
we obtain the theorem.
â–¡
16.3 Probabilistic Context-Free Grammar
In this section, we asymptotically analyze the VB free energy of probabilistic
context-free grammar (PCFG), introduced in Section 4.2.3, as the number N
of the sequences in the training corpus D = {X(1),. . . , X(N)} goes to inï¬nity
(Hosino et al., 2006a). The PCFG model is deï¬ned by
p(X|w) =

ZâˆˆT(X)
p(X, Z|w),
(16.30)
p(X, Z|w) =
K

i, j,k=1

aiâ†’jk
 cZ
iâ†’jk
L

l=1
K

i=1
M

m=1
(biâ†’m)z(l)
i x(l)
m ,
w = {{ai}K
i=1, {bi}K
i=1},
ai = {aiâ†’jk}K
j,k=1 (1 â‰¤i â‰¤K),
bi = {biâ†’m}M
m=1 (1 â‰¤i â‰¤K),
p({ai}K
i=1|Ï†) =
K

i=1
DirichletK2

ai; (Ï†,. . . , Ï†)âŠ¤ 
,
p({bi}K
i=1|Î¾) =
K

i=1
DirichletM

bi; (Î¾,. . . , Î¾)âŠ¤ 
,
where Ï† > 0 and Î¾ > 0 are hyperparameters. Here T(X) is the set of derivation
sequences that generate X, cZ
iâ†’jk is the count of the transition rule from the
nonterminal symbol i to the pair of nonterminal symbols (j, k) appearing in
the derivation sequence Z, and z(l) = (z(l)
1 ,. . . ,z(l)
K ) is the indicator of the
(nonterminal) symbol generating the lth output symbol of X.
Under the constraint, r(H, w) = rH(H)rw(w), the VB posteriors are
given by
rw(w) = ra({ai}K
i=1)rb({bi}K
i=1),
ra({ai}K
i=1) =
K

i=1
DirichletK2

ai; (Ï†iâ†’11,. . . ,Ï†iâ†’KK)âŠ¤ 
,

16.3 Probabilistic Context-Free Grammar
467
rb({bi}K
i=1) =
K

i=1
DirichletM

bi; (Î¾iâ†’1,. . . ,Î¾iâ†’M)âŠ¤ 
,
rH(H) =
N

n=1
rz(Z(n)),
rz(Z(n)) =
1
CZ(n) exp $Î³Z(n)% ,
(16.31)
Î³Z(n) =
K

i, j,k=1
cZ(n)
iâ†’jk
,
Î¨
Ï†iâ†’jk
 
âˆ’Î¨
K
jâ€²=1
K
kâ€²=1 Ï†iâ†’jâ€²kâ€²
 -
+
L

l=1
K

i=1
M

m=1
z(n,l)
i
x(n,l)
m
,
Î¨
Î¾iâ†’m
 
âˆ’Î¨
M
mâ€²=1Î¾iâ†’mâ€²
 -
,
where CZ(n) = 
ZâˆˆT(X(n)) exp(Î³Z) is the normalizing constant and T(X(n)) is
the set of derivation sequences that generate X(n). After the substitution of
Eq. (15.5), the free energy is given by
F =
K

i=1
â§âªâªâ¨âªâªâ©log
â›âœâœâœâœâœâœâ
Î“(K
j,k=1 Ï†iâ†’jk)
K
j,k=1 Î“(Ï†iâ†’jk)
ââŸâŸâŸâŸâŸâŸâ 
+
K

j,k=1
Ï†iâ†’jk âˆ’Ï†
 
Î¨
Ï†iâ†’jk
 
âˆ’Î¨
K
jâ€²,kâ€²=1 Ï†iâ†’jâ€²kâ€²)
  
+ log
â›âœâœâœâœâœâœâ
Î“
M
m=1Î¾iâ†’m
 
M
m=1 Î“
Î¾iâ†’m
 
ââŸâŸâŸâŸâŸâŸâ +
M

m=1
Î¾iâ†’m âˆ’Î¾
 
Î¨
Î¾iâ†’m
 
âˆ’Î¨
M
mâ€²=1Î¾iâ†’mâ€²
  
â«âªâªâªâ¬âªâªâªâ­
âˆ’K log
 Î“(K2Ï†)
(Î“(Ï†))K2

âˆ’K log
 Î“(MÎ¾)
(Î“(Î¾))M

âˆ’
N

n=1
logCZ(n).
The variational parameters satisfy
Ï†iâ†’jk = N
z
iâ†’jk + Ï†,
Î¾iâ†’m = N
x
iâ†’m + Î¾,
where
N
z
iâ†’jk =
N

n=1
L

l=1

cZ(n)
iâ†’jk

rz(Z(n)) ,
N
x
iâ†’m =
N

n=1
L

l=1

z(n,l)
i

rz(Z(n)) x(n,l)
m .
We assume the following condition.

468
16 Asymptotic VB Theory of Other Latent Variable Models
Assumption 16.4
The true distribution q(X) has K0 nonterminal symbols
and M terminal symbols with parameter wâˆ—:
q(X) = p(X|wâˆ—) =

ZâˆˆT(X)
p(X, Z|wâˆ—).
(16.32)
The true parameters are
wâˆ—= {{aâˆ—
i }K0
i=1, {bâˆ—
i }K0
i=1},
aâˆ—
i = {aâˆ—
iâ†’jk}K0
j,k=1 (1 â‰¤i â‰¤K0),
bâˆ—
i = {bâˆ—
iâ†’m}M
m=1 (1 â‰¤i â‰¤K0),
which satisfy the constraints
aâˆ—
iâ†’ii = 1 âˆ’

(j,k)(i,i)
aâˆ—
iâ†’jk, bâˆ—
iâ†’M = 1 âˆ’
Mâˆ’1

m=1
bâˆ—
iâ†’m,
respectively. Since PCFG has nontrivial nonidentiï¬ability as in HMM (Ito
et al., 1992), we assume that K0 is the smallest number of nonterminal
symbols under this parameterization. The statistical model given by Eq.
(16.30) includes the true distribution, namely, the number of nonterminal
symbols K satisï¬es the inequality K0 â‰¤K.
Under this assumption, the next theorem evaluates the relative VB free
energy. Here S N(D) = âˆ’1
N
N
n=1 log p(X(n)|wâˆ—) is the empirical entropy of the
true distribution (16.32).
Theorem 16.6
The relative VB free energy of the PCFG model satisï¬es
FVB(D) = FVB(D) âˆ’NS N(D) = Î»â€²VB
PCFG log N + Op(1),
where
Î»â€²VB
PCFG =
â§âªâªâªâªâ¨âªâªâªâªâ©
K0(K2
0âˆ’1)+K0(Mâˆ’1)
2
+ K0(K2 âˆ’K2
0)Ï†
!
0 < Ï† â‰¤
K2
0+KK0+K2+Mâˆ’2
2(K2
0+KK0)
"
,
K(K2âˆ’1)+K(Mâˆ’1)
2
!
K2
0+KK0+K2+Mâˆ’2
2(K2
0+KK0)
< Ï†
"
.
(16.33)
Proof
Based on Lemma 15.1, similarly to the models in the previous sections,
we evaluate R = KL(rw(w)||p(w)). It is expressed by expected sufï¬cient
statistics as
R =
K

i=1
â¡â¢â¢â¢â¢â¢â£log Î“(N
z
i + K2Ï†) âˆ’N
z
iÎ¨(N
z
i + K2Ï†)
âˆ’
K

j,k=1
,
log Î“(N
z
iâ†’jk + Ï†) âˆ’N
z
iâ†’jkÎ¨(N
z
iâ†’jk + Ï†)
-

16.3 Probabilistic Context-Free Grammar
469
+ log Î“(N
x
i + MÎ¾) âˆ’N
x
i Î¨(N
x
i + MÎ¾)
âˆ’
M

m=1
,
log Î“(N
x
iâ†’m + Î¾) âˆ’N
x
iâ†’mÎ¨(N
x
iâ†’m + Î¾)
-â¤â¥â¥â¥â¥â¥â¦+ Op(1).
Using the inequalities of the di-gamma and the log-gamma functions in
Eqs. (15.8) and (15.9), we have
R =
K

i=1
â¡â¢â¢â¢â¢â¢â¢â£

K2Ï† âˆ’1
2

log(N
z
i + K2Ï†) âˆ’
K

j,k=1

Ï† âˆ’1
2

log(N
z
iâ†’jk + Ï†)
+

MÎ¾ âˆ’1
2

log(N
x
i + MÎ¾) âˆ’
M

m=1

Î¾ âˆ’1
2

log(N
x
iâ†’m + Î¾)
â¤â¥â¥â¥â¥â¥â¦+ Op(1).
(16.34)
We divide the sum over i, j, and k in Eq. (16.34) to the necessary K0
and redundant K âˆ’K0 terms. Moreover, we assume the trained model uses
redundant l
(0 â‰¤l â‰¤K âˆ’K0) nonterminal symbols, i.e., it holds that
N
z
i = Î˜p(N):
R
log N =
K0

i=1
â§âªâªâªâ¨âªâªâªâ©
!
K2Ï† + M
2 âˆ’1
"
âˆ’
K0

j,k=1

Ï† âˆ’1
2
â«âªâªâªâ¬âªâªâªâ­
+
K0

i=1
â§âªâªâªâ¨âªâªâªâ©
K0

j,k=1

Ï† âˆ’1
2

âˆ’
K0+l

j,k=1

Ï† âˆ’1
2
â«âªâªâªâ¬âªâªâªâ­
+
K0+l

i=K0+1
â§âªâªâªâ¨âªâªâªâ©
!
K2Ï† + M
2 âˆ’1
"
âˆ’
K0+l

j,k=1

Ï† âˆ’1
2
â«âªâªâªâ¬âªâªâªâ­+ Op

1
log N

=
!
K2Ï† + M
2 âˆ’1
"
âˆ’K2
0

Ï† âˆ’1
2

+ g(l) + Op

1
log N

,
(16.35)
where g(l) is given by
g(l) =
!
K2Ï† + M
2 âˆ’1
"
l âˆ’

Ï† âˆ’1
2
 ,
(K0 + l)3 âˆ’K3
0
-
.
By Lemma 15.4, similarly to the HMM, we can evaluate the VB free energy
by minimizing g(l). The minimum of g(l) is achieved by
â§âªâªâªâªâ¨âªâªâªâªâ©
l = 0
(0 < Ï† â‰¤
K2
0+KK0+K2+Mâˆ’2
2(K2
0+KK0)
),
l = K âˆ’K0
(
K2
0+KK0+K2+Mâˆ’2
2(K2
0+KK0)
< Ï†).
Putting this back into Eq. (16.35), we obtain the theorem.
â–¡

470
16 Asymptotic VB Theory of Other Latent Variable Models
16.4 Latent Dirichlet Allocation
In this section, we investigate the VB free energy of the latent Dirichlet
allocation (LDA) introduced in Section 4.2.4. We also analyze the asymptotic
behavior of MAP learning and partially Bayesian learning, which are often
used alternatively to VB learning, and discuss similarities and dissimilarities
between those learning algorithms.
We consider the following LDA model:
p(w(n,m), z(n,m)|Î˜, B) = p(w(n,m)|z(n,m), B)p(z(n,m)|Î˜),
p(w(n,m)|z(n,m), B) =
L

l=1
H

h=1
(Bl,h)w(n,m)
l
z(n,m)
h
,
p(z(n,m)|Î˜) =
H

h=1
(Î¸m,h)z(n,m)
h
,
p(Î˜|Î±) =
M

m=1
DirichletH(Î¸m; (Î±,. . . , Î±)âŠ¤),
p(B|Î·) =
H

h=1
DirichletL(Î²h; (Î·,. . . , Î·)âŠ¤).
Here we have assumed that the priors are symmetric and have hyperparameters
Î±1 = . . . = Î±H = Î± > 0, Î·1 = . . . = Î·L = Î· > 0, respectively.
Under the constraint, r(w, H) = rÎ˜,B(Î˜, B)rz

{{z(n,m)}N(m)
n=1 }M
m=1
 
, the VB
posteriors are given by
rz

{{z(n,m)}N(m)
n=1 }M
m=1
 
=
M

m=1
N(m)

n=1
MultinomialH,1

z(n,m);z(n,m) 
,
rÎ˜,B(Î˜, B) = rÎ˜(Î˜)rB(B),
rÎ˜(Î˜) =
M

m=1
Dirichlet
Î¸m;Î±m
 
,
rB(B) =
H

h=1
Dirichlet $Î²h;Î·h
% .
The free energy is given by
F =
M

m=1
â›âœâœâœâœâlog
â›âœâœâœâœâ
Î“(H
h=1 Î±m,h)
H
h=1 Î“(Î±m,h)
ââŸâŸâŸâŸâ âˆ’log
Î“(HÎ±)
Î“(Î±)H
ââŸâŸâŸâŸâ 
+
H

h=1
â›âœâœâœâœâlog
â›âœâœâœâœâ
Î“(L
l=1Î·l,h)
L
l=1 Î“(Î·l,h)
ââŸâŸâŸâŸâ âˆ’log
Î“(LÎ·)
Î“(Î·)L
ââŸâŸâŸâŸâ 

16.4 Latent Dirichlet Allocation
471
+
M

m=1
H

h=1
!
Î±m,h âˆ’(N
(m)
h
+ Î±)
" 
Î¨(Î±m,h) âˆ’Î¨(H
hâ€²=1 Î±m,hâ€²)
 
+
H

h=1
L

l=1

Î·l,h âˆ’(Wl,h + Î·)
 
Î¨(Î·l,h) âˆ’Î¨(L
lâ€²=1Î·lâ€²,h)
 
+
M

m=1
N(m)

n=1
H

h=1
z(n,m)
h
logz(n,m)
h
,
where
N
(m)
h
=
N(m)

n=1

z(n,m)
h

rz

{{z(n,m)}N(m)
n=1 }M
m=1
 ,
Wl,h =
M

m=1
N(m)

n=1
w(n,m)
l

z(n,m)
h

rz

{{z(n,m)}N(m)
n=1 }M
m=1
 ,
for the observed data D = {{w(n,m)}N(m)
n=1 }M
m=1. The variational parameters satisfy
Î±m,h = N
(m)
h
+ Î±,
(16.36)
Î·l,h = Wl,h + Î·,
(16.37)
z(n,m)
h
=
z(n,m)
h
H
hâ€²=1 z(n,m)
hâ€²
for
z(n,m)
h
= exp
â›âœâœâœâœâœâ
,
Î¨(Î±m,h) âˆ’Î¨
H
hâ€²=1 Î±m,hâ€²
 -
+
L

l=1
w(n,m)
l
,
Î¨(Î·l,h) âˆ’Î¨
L
lâ€²=1Î·lâ€²,h
 -ââŸâŸâŸâŸâŸâ .
Based on Lemma 15.1, we decompose the free energy as follows:
F = R + Q,
(16.38)
where
R = KL (rÎ˜(Î˜)rB(B)||p(Î˜|Î±)p(B|Î·))
=
M

m=1
â›âœâœâœâœâœâlog Î“(H
h=1 Î±m,h)
H
h=1 Î“(Î±m,h)
Î“(Î±)H
Î“(HÎ±) +
H

h=1
$Î±m,h âˆ’Î±% 
Î¨(Î±m,h) âˆ’Î¨(H
hâ€²=1 Î±m,hâ€²)
 ââŸâŸâŸâŸâŸâ 
+
H

h=1
â›âœâœâœâœâœâlog Î“(L
l=1Î·l,h)
L
l=1 Î“(Î·l,h)
Î“(Î·l)L
Î“(LÎ·l) +
L

l=1
$Î·l,h âˆ’Î·l
% 
Î¨(Î·l,h) âˆ’Î¨(L
lâ€²=1Î·lâ€²,h)
 ââŸâŸâŸâŸâŸâ ,
(16.39)

472
16 Asymptotic VB Theory of Other Latent Variable Models
Q = âˆ’logCH
= âˆ’
M

m=1
N(m)
L

l=1
Vl,m log
â›âœâœâœâœâœâœâ
H

h=1
exp $Î¨(Î±m,h)%
exp

Î¨(H
hâ€²=1 Î±m,hâ€²)
 
exp $Î¨(Î·l,h)%
exp

Î¨(L
lâ€²=1Î·lâ€²,h)
 
ââŸâŸâŸâŸâŸâŸâ .
(16.40)
Here, V âˆˆRLÃ—M is the empirical word distribution matrix with its entries given
by Vl,m =
1
N(m)
N(m)
n=1 w(n,m)
l
.
16.4.1 Asymptotic Analysis of VB Learning
Here we analyze the VB free energy of LDA in the asymptotic limit when
N â‰¡minm N(m) â†’âˆ(Nakajima et al., 2014). Unlike the analyses for the
latent variable models in the previous sections, we do not assume L, M â‰ªN,
but 1 â‰ªL, M, N at this point. This amounts to considering the asymptotic
limit when L, M, N â†’âˆwith a ï¬xed mutual ratio, or equivalently, assuming
L, M âˆ¼O(N).
We assume the following condition on the true distribution.
Assumption 16.5
The word distribution matrix V is a sample from the
multinomial distribution with the true parameter Uâˆ—âˆˆRLÃ—M whose rank is
Hâˆ—âˆ¼O(1), i.e., Uâˆ—= Bâˆ—Î˜âˆ—âŠ¤where Î˜âˆ—âˆˆRMÃ—Hâˆ—and Bâˆ—âˆˆRLÃ—Hâˆ—.2 The
number of topics of the model H is set to H = min(L, M) (i.e., the matrix BÎ˜âŠ¤
can express any multinomial distribution).
The stationary conditions, Eqs. (16.36) and (16.37), lead to the following
lemma:
Lemma 16.7
Let BÎ˜
âŠ¤=

BÎ˜âŠ¤
rÎ˜,B(Î˜,B). Then it holds that
*
(BÎ˜âŠ¤âˆ’BÎ˜
âŠ¤)2
l,m
+
rÎ˜,B(Î˜,B) = Op(Nâˆ’2),
(16.41)
Q = âˆ’
M

m=1
N(m)
L

l=1
Vl,m log(BÎ˜
âŠ¤)l,m + Op(M).
(16.42)
Proof
For the Dirichlet distribution p(a|Ë˜a) âˆH
h=1 aË˜ahâˆ’1
h
, the mean and the
variance are given as follows:
ah = âŸ¨ahâŸ©p(a|Ë˜a) = Ë˜ah
Ë˜a0
,
âŸ¨(ah âˆ’ah)2âŸ©p(a|Ë˜a) = Ë˜ah(Ë˜a0 âˆ’Ë˜ah)
Ë˜a2
0(Ë˜a0 + 1) ,
where Ë˜a0 = H
h=1 Ë˜ah.
2 More precisely, Uâˆ—= Bâˆ—Î˜âˆ—âŠ¤+ O(Nâˆ’1) is sufï¬cient.

16.4 Latent Dirichlet Allocation
473
For ï¬xed N, R, deï¬ned by Eq. (16.39), diverges to +âˆif Î±m,h â†’+0 for
any (m, h) or Î·l,h â†’+0 for any (l, h). Therefore, the global minimizer of the
free energy (16.38) is in the interior of the domain, where the free energy is
differentiable. Consequently, the global minimizer is a stationary point. The
stationary conditions (16.36) and (16.37) imply that
Î±m,h â‰¥Î±,
Î·l,h â‰¥Î·,
(16.43)
H

h=1
Î±m,h =
H

h=1
Î± + N(m),
L

l=1
Î·l,h =
L

l=1
Î· +
M

m=1
(Î±m,h âˆ’Î±).
(16.44)
Therefore, we have

(Î˜m,h âˆ’Î˜m,h)2
rÎ˜(Î˜) = Op(Nâˆ’2)
for all (m, h),
(16.45)
!
max
m
Î˜m,h
"2 
(Bl,h âˆ’Bl,h)2
rB(B) = Op(Nâˆ’2)
for all (l, h),
(16.46)
which leads to Eq. (16.41).
By using Eq. (15.8), Q is bounded as follows:
Q â‰¤Q â‰¤Q,
where
Q = âˆ’
M

m=1
N(m)
L

l=1
Vl,m log
â›âœâœâœâœâœâœâœâœâœâ
H

h=1
Î±m,h
H
hâ€²=1 Î±m,hâ€²
Î·l,h
L
lâ€²=1 Î·lâ€²,h
exp

âˆ’
1
Î±m,h

exp
â›âœâœâœâœâœââˆ’
1
2 H
hâ€²=1 Î±m,hâ€²
ââŸâŸâŸâŸâŸâ 
exp

âˆ’
1
Î·l,h

exp
â›âœâœâœâœâœââˆ’
1
2 L
lâ€²=1 Î·lâ€²,h
ââŸâŸâŸâŸâŸâ 
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâ ,
Q = âˆ’
M

m=1
N(m)
L

l=1
Vl,m log
â›âœâœâœâœâœâœâœâœâœâ
H

h=1
Î±m,h
H
hâ€²=1 Î±m,hâ€²
Î·l,h
L
lâ€²=1 Î·lâ€²,h
exp

âˆ’
1
2Î±m,h

exp
â›âœâœâœâœâœââˆ’
1
H
hâ€²=1 Î±m,hâ€²
ââŸâŸâŸâŸâŸâ 
exp

âˆ’
1
2Î·l,h

exp
â›âœâœâœâœâœââˆ’
1
L
lâ€²=1 Î·lâ€²,h
ââŸâŸâŸâŸâŸâ 
ââŸâŸâŸâŸâŸâŸâŸâŸâŸâ .
Using Eqs. (16.45) and (16.46), we have Eq. (16.42), which completes the
proof of Lemma 16.7.
â–¡
Eq. (16.41) implies the convergence of the posterior. Let uâˆ—
m = Bâˆ—(Î¸
âˆ—
m)âŠ¤be
the true probability mass function for the mth document and um = B(Î¸m)âŠ¤
be its predictive probability. Deï¬ne a measure of how far the predictive
distributions are from the true distributions by
J =
M

m=1
N(m)
N KL(uâˆ—
m||um).
(16.47)
Then, by the same arguments as the proof of Lemma 15.4, Eq. (16.42) leads to
the following lemma:

474
16 Asymptotic VB Theory of Other Latent Variable Models
Lemma 16.8
Q is minimized when J = Op(1/N), and it holds that
Q = NS N(D) + Op(JN + LM),
where
S N(D) = âˆ’1
N log p(D|Î˜âˆ—, Bâˆ—) = âˆ’
M

m=1
N(m)
N
L

l=1
Vl,m log(Bâˆ—Î˜âˆ—)l,m.
Lemma 16.8 simply states that Q/N converges to the empirical entropy
S N(D) of the true distribution if and only if the predictive distribution
converges to the true distribution (i.e., J = Op(1/N)).
Let 
H = H
h=1 Î¸( 1
M
M
m=1 Î˜m,h âˆ¼Op(1)) be the number of topics used in the
whole corpus, 
M(h) = M
m=1 Î¸(Î˜m,h âˆ¼Op(1)) be the number of documents that
contain the hth topic, and L(h) = L
l=1 Î¸(Bl,h âˆ¼Op(1)) be the number of words
of which the hth topic consist. We have the following lemma:
Lemma 16.9
R is written as follows:
R =
â§âªâªâªâ¨âªâªâªâ©M

HÎ± âˆ’1
2

+ 
H

LÎ· âˆ’1
2

âˆ’

H

h=1


M(h)

Î± âˆ’1
2

+ L(h)

Î· âˆ’1
2
â«âªâªâªâ¬âªâªâªâ­log N
+ (H âˆ’
H)

LÎ· âˆ’1
2

log L + Op(H(M + L)).
(16.48)
Proof
By using the bounds (15.8) and (15.9), R can be bounded as
R â‰¤R â‰¤R,
(16.49)
where
R = âˆ’
M

m=1
log
Î“(HÎ±)
Î“(Î±)H

âˆ’
H

h=1
log
Î“(LÎ·)
Î“(Î·)
L
âˆ’M(H âˆ’1) + H(L âˆ’1)
2
log(2Ï€)
+
M

m=1
â§âªâªâ¨âªâªâ©

HÎ± âˆ’1
2

log
H

h=1
Î±m,h âˆ’
H

h=1

Î± âˆ’1
2

logÎ±m,h
â«âªâªâ¬âªâªâ­
+
H

h=1
â§âªâªâ¨âªâªâ©

LÎ· âˆ’1
2

log
L

l=1
Î·l,h âˆ’
L

l=1

Î· âˆ’1
2

logÎ·l,h
â«âªâªâ¬âªâªâ­
+
M

m=1
â§âªâªâ¨âªâªâ©âˆ’
H

h=1
1
12Î±m,h
âˆ’
H

h=1
$Î±m,h âˆ’Î±%  1
Î±m,h
âˆ’
1
2 H
hâ€²=1 Î±m,hâ€²
â«âªâªâ¬âªâªâ­
+
H

h=1
â§âªâªâ¨âªâªâ©âˆ’
L

l=1
1
12Î·l,h
âˆ’
L

l=1
$Î·l,h âˆ’Î·%  1
Î·l,h
âˆ’
1
2 L
lâ€²=1Î·lâ€²,h
â«âªâªâ¬âªâªâ­,
(16.50)

16.4 Latent Dirichlet Allocation
475
R = âˆ’
M

m=1
log
Î“(HÎ±)
Î“(Î±)H

âˆ’
H

h=1
log
Î“(LÎ·)
Î“(Î·)
L
âˆ’M(H âˆ’1) + H(L âˆ’1)
2
log(2Ï€)
+
M

m=1
â§âªâªâ¨âªâªâ©

HÎ± âˆ’1
2

log
H

h=1
Î±m,h âˆ’
H

h=1

Î± âˆ’1
2

logÎ±m,h
â«âªâªâ¬âªâªâ­
+
H

h=1
â§âªâªâ¨âªâªâ©

LÎ· âˆ’1
2

log
L

l=1
Î·l,h âˆ’
L

l=1

Î· âˆ’1
2

logÎ·l,h
â«âªâªâ¬âªâªâ­
+
M

m=1
â§âªâªâ¨âªâªâ©
1
12 H
h=1 Î±m,h
âˆ’
H

h=1
$Î±m,h âˆ’Î±% 
1
2Î±m,h
âˆ’
1
H
hâ€²=1 Î±m,hâ€²
â«âªâªâ¬âªâªâ­
+
H

h=1
â§âªâªâ¨âªâªâ©
1
12 L
l=1Î·l,h
âˆ’
L

l=1
$Î·l,h âˆ’Î·%  1
2Î·l,h
âˆ’
1
L
lâ€²=1Î·lâ€²,h
â«âªâªâ¬âªâªâ­.
(16.51)
Eqs. (16.43) and (16.44) imply that
R =
M

m=1
â§âªâªâ¨âªâªâ©

HÎ± âˆ’1
2

log
H

h=1
Î±m,h âˆ’
H

h=1

Î± âˆ’1
2

logÎ±m,h
â«âªâªâ¬âªâªâ­
+
H

h=1
â§âªâªâ¨âªâªâ©

LÎ· âˆ’1
2

log
L

l=1
Î·l,h âˆ’
L

l=1

Î· âˆ’1
2

logÎ·l,h
â«âªâªâ¬âªâªâ­+ Op(H(M + L)),
which leads to Eq. (16.48). This completes the proof of Lemma 16.9.
â–¡
Since we assumed that the true matrices Î˜âˆ—and Bâˆ—are of the rank of Hâˆ—,

H = Hâˆ—âˆ¼O(1) is sufï¬cient for the VB posterior to converge to the true distri-
bution. However, 
H can be much larger than Hâˆ—with

BÎ˜âŠ¤
rÎ˜,B(Î˜,B) unchanged
because of the nonidentiï¬ability of matrix factorizationâ€”duplicating topics
with divided weights, for example, does not change the distribution.
Let
FVB(D) = FVB(D) âˆ’NS N(D)
(16.52)
be the relative free energy. Based on Lemmas 16.8 and 16.9, we obtain the
following theorem:
Theorem 16.10
In the limit when N â†’âˆwith L, M âˆ¼O(1), it holds that
J = Op(1/N), and
FVB(D) = Î»â€²VB
LDA log N + Op(1),
where
Î»â€²VB
LDA =
â§âªâªâªâ¨âªâªâªâ©M

HÎ± âˆ’1
2

+ 
H

LÎ· âˆ’1
2

âˆ’

H

h=1


M(h)

Î± âˆ’1
2

+ L(h)

Î· âˆ’1
2
â«âªâªâªâ¬âªâªâªâ­.

476
16 Asymptotic VB Theory of Other Latent Variable Models
In the limit when N, M â†’âˆwith M
N , L âˆ¼O(1), it holds that J = op(log N),
and
FVB(D) = Î»â€²VB
LDA log N + op(N log N),
where
Î»â€²VB
LDA =
â§âªâªâªâ¨âªâªâªâ©M

HÎ± âˆ’1
2

âˆ’

H

h=1

M(h)

Î± âˆ’1
2
â«âªâªâªâ¬âªâªâªâ­.
In the limit when N, L â†’âˆwith L
N , M âˆ¼O(1), it holds that J = op(log N),
and
FVB(D) = Î»â€²VB
LDA log N + op(N log N),
where
Î»â€²VB
LDA = HLÎ·.
In the limit when N, L, M â†’âˆwith
L
N , M
N
âˆ¼O(1), it holds that J =
op(N log N), and
FVB(D) = Î»â€²VB
LDA log N + op(N2 log N),
where
Î»â€²VB
LDA = H(MÎ± + LÎ·).
Proof
Lemmas 16.8 and 16.9 imply that the relative free energy can be
written as follows:
FVB(D)
=
â§âªâªâªâ¨âªâªâªâ©M

HÎ± âˆ’1
2

+ 
H

LÎ· âˆ’1
2

âˆ’

H

h=1


M(h)

Î± âˆ’1
2

+ L(h)

Î· âˆ’1
2
â«âªâªâªâ¬âªâªâªâ­log N
+ (H âˆ’
H)

LÎ· âˆ’1
2

log L + Op(JN + LM).
(16.53)
In the following subsection, we investigate the leading term of the relative free
energy (16.53) in different asymptotic limits.
In the Limit When N â†’âˆwith L, M âˆ¼O(1)
In this case, the minimizer should satisfy
J = Op
 1
N

(16.54)

16.4 Latent Dirichlet Allocation
477
and the leading term of the relative free energy (16.52) is of the order of
Op(log N) as follows:
FVB(D)
=
â§âªâªâªâ¨âªâªâªâ©M

HÎ± âˆ’1
2

+ 
H

LÎ· âˆ’1
2

âˆ’

H

h=1


M(h)

Î± âˆ’1
2

+ L(h)

Î· âˆ’1
2
â«âªâªâªâ¬âªâªâªâ­log N
+ Op(1).
Note that Eq. (16.54) implies the consistency of the VB posterior.
In the Limit When N, M â†’âˆwith M
N , L âˆ¼O(1)
In this case,
J = op(log N),
(16.55)
making the leading term of the relative free energy of the order of Op(N log N)
as follows:
FVB(D) =
â§âªâªâªâ¨âªâªâªâ©M

HÎ± âˆ’1
2

âˆ’

H

h=1

M(h)

Î± âˆ’1
2
â«âªâªâªâ¬âªâªâªâ­log N + op(N log N).
Eq. (16.55) implies that the VB posterior is not necessarily consistent.
In the Limit When N, L â†’âˆwith L
N, M âˆ¼O(1)
In this case, Eq. (16.55) holds, and the leading term of the relative free energy
is of the order of Op(N log N) as follows:
FVB(D) = HLÎ· log N + op(N log N).
In the Limit When N, L, M â†’âˆwith L
N, M
N âˆ¼O(1)
In this case,
J = op(N log N),
(16.56)
and the leading term of the relative free energy is of the order of Op(N2 log N)
as follows:
FVB(D) = H (MÎ± + LÎ·) log N + op(N2 log N).
This completes the proof of Theorem 16.10.
â–¡
Since Eq. (16.41) was shown to hold, the predictive distribution converges
to the true distribution if J = Op(1/N). Accordingly, Theorem 16.10 states that
the consistency holds in the limit when N â†’âˆwith L, M âˆ¼O(1).

478
16 Asymptotic VB Theory of Other Latent Variable Models
Theorem 16.10 also implies that, in the asymptotic limits with small
L âˆ¼O(1), the leading term depends on 
H, meaning that it dominates the topic
sparsity of the VB solution. We have the following corollary:
Corollary 16.11
Let Mâˆ—(h) = M
m=1 Î¸(Î˜âˆ—
m,h âˆ¼O(1)) and Lâˆ—(h) = L
l=1 Î¸(Bâˆ—
l,h âˆ¼
O(1)). Consider the limit when N â†’âˆwith L, M âˆ¼O(1). When 0 < Î· â‰¤
1
2L,
the VB solution is sparse (i.e., 
H â‰ªH = min(L, M)) if Î± < 1
2 âˆ’
1
2 âˆ’LÎ·
minh Mâˆ—(h) , and
dense (i.e., 
H â‰ˆH) if Î± > 1
2 âˆ’
1
2 âˆ’LÎ·
minh Mâˆ—(h) . When
1
2L < Î· â‰¤1
2, the VB solution is
sparse if Î± < 1
2 +
LÎ·âˆ’1
2
maxh Mâˆ—(h) , and dense if Î± > 1
2 +
LÎ·âˆ’1
2
maxh Mâˆ—(h) . When Î· > 1
2, the VB
solution is sparse if Î± < 1
2 +
Lâˆ’1
2 maxh Mâˆ—(h) , and dense if Î± > 1
2 +
LÎ·âˆ’1
2
minh Mâˆ—(h) . In the
limit when N, M â†’âˆwith M
N , L âˆ¼O(1), the VB solution is sparse if Î± < 1
2,
and dense if Î± > 1
2.
Proof
From the compact representation when 
H = Hâˆ—, 
M(h) = Mâˆ—(h), and
L(h) = Lâˆ—(h), we can decompose a singular component into two, keeping BÎ˜
âŠ¤
unchanged, so that

H â†’
H + 1,
(16.57)
H

h=1

M(h) â†’
Hâˆ—

h=1

M(h) + Î”M
for
min
h
Mâˆ—(h) â‰¤Î”M â‰¤max
h
Mâˆ—(h),
(16.58)
H

h=1
L(h) â†’
Hâˆ—

h=1
L(h) + Î”L
for
0 â‰¤Î”L â‰¤max
h
Lâˆ—(h).
(16.59)
Here the lower-bound for Î”M in Eq. (16.58) corresponds to the case that
the least frequent topic is chosen to be decomposed, while the upper-bound
to the case that the most frequent topic is chosen. The lower-bound for
Î”L in Eq. (16.59) corresponds to the decomposition such that some of the
word-occurrences are moved to a new topic, while the upper-bound to the
decomposition such that the topic with the widest vocabulary is copied to a
new topic. Note that the bounds both for Î”M and Î”L are not always achievable
simultaneously, when we choose one topic to decompose.
In the following subsection, we investigate the relation between the sparsity
of the solution and the hyperparameter setting in different asymptotic limits.
In the Limit When N â†’âˆwith L, M âˆ¼O(1)
The coefï¬cient of the leading term of the free energy is
Î»â€²VB
LDA = M

HÎ± âˆ’1
2

+

H

h=1

LÎ· âˆ’1
2 âˆ’
M(h)

Î± âˆ’1
2

âˆ’L(h)

Î· âˆ’1
2

. (16.60)

16.4 Latent Dirichlet Allocation
479
Note that the solution is sparse if Eq. (16.60) is increasing with respect to 
H,
and dense if it is decreasing. Eqs. (16.57) through (16.59) imply the following:
(I) When 0 < Î· â‰¤
1
2L and Î± â‰¤1
2, the solution is sparse if
LÎ· âˆ’1
2 âˆ’min
h
Mâˆ—(h)

Î± âˆ’1
2

> 0, or equivalently,
Î± < 1
2 âˆ’
1
minh Mâˆ—(h)
1
2 âˆ’LÎ·

,
and dense if
Î± > 1
2 âˆ’
1
minh Mâˆ—(h)
1
2 âˆ’LÎ·

.
(II) When 0 < Î· â‰¤
1
2L and Î± > 1
2, the solution is sparse if
LÎ· âˆ’1
2 âˆ’max
h
Mâˆ—(h)

Î± âˆ’1
2

> 0, or equivalently,
Î± < 1
2 âˆ’
1
maxh Mâˆ—(h)
1
2 âˆ’LÎ·

,
and dense if
Î± > 1
2 âˆ’
1
maxh Mâˆ—(h)
1
2 âˆ’LÎ·

.
Therefore, the solution is always dense in this case.
(III) When
1
2L < Î· â‰¤1
2 and Î± < 1
2, the solution is sparse if
LÎ· âˆ’1
2 âˆ’min
h
Mâˆ—(h)

Î± âˆ’1
2

> 0, or equivalently,
Î± < 1
2 +
1
minh Mâˆ—(h)

LÎ· âˆ’1
2

,
and dense if
Î± > 1
2 +
1
minh Mâˆ—(h)

LÎ· âˆ’1
2

.
Therefore, the solution is always sparse in this case.
(IV) When
1
2L < Î· â‰¤1
2 and Î± â‰¥1
2, the solution is sparse if
LÎ· âˆ’1
2 âˆ’max
h
Mâˆ—(h)

Î± âˆ’1
2

> 0, or equivalently,
Î± < 1
2 +
1
maxh Mâˆ—(h)

LÎ· âˆ’1
2

,

480
16 Asymptotic VB Theory of Other Latent Variable Models
and dense if
Î± > 1
2 +
1
maxh Mâˆ—(h)

LÎ· âˆ’1
2

.
(V) When Î· > 1
2 and Î± < 1
2, the solution is sparse if
LÎ· âˆ’1
2 âˆ’max
h

Mâˆ—(h)

Î± âˆ’1
2

+ Lâˆ—(h)

Î· âˆ’1
2

> 0,
(16.61)
and dense if
LÎ· âˆ’1
2 âˆ’max
h

Mâˆ—(h)

Î± âˆ’1
2

+ Lâˆ—(h)

Î· âˆ’1
2

< 0.
(16.62)
Therefore, the solution is sparse if
LÎ· âˆ’1
2 âˆ’min
h
Mâˆ—(h)

Î± âˆ’1
2

âˆ’max
h
Lâˆ—(h)

Î· âˆ’1
2

> 0, or equivalently,
Î± < 1
2 +
1
minh Mâˆ—(h)

LÎ· âˆ’1
2 âˆ’max
h
Lâˆ—(h)

Î· âˆ’1
2

,
and dense if
LÎ· âˆ’1
2 âˆ’max
h
Mâˆ—(h)

Î± âˆ’1
2

âˆ’max
h
Lâˆ—(h)

Î· âˆ’1
2

< 0, or equivalently,
Î± > 1
2 +
1
maxh Mâˆ—(h)

LÎ· âˆ’1
2 âˆ’max
h
Lâˆ—(h)

Î· âˆ’1
2

.
Therefore, the solution is always sparse in this case.
(VI) When Î· > 1
2 and Î± â‰¥1
2, the solution is sparse if Eq. (16.61) holds, and
dense if Eq. (16.62) holds. Therefore, the solution is sparse if
LÎ· âˆ’1
2 âˆ’max
h
Mâˆ—(h)

Î± âˆ’1
2

âˆ’max
h
Lâˆ—(h)

Î· âˆ’1
2

> 0, or equivalently,
Î± < 1
2 +
1
maxh Mâˆ—(h)

LÎ· âˆ’1
2 âˆ’max
h
Lâˆ—(h)

Î· âˆ’1
2

,
and dense if
LÎ· âˆ’1
2 âˆ’min
h
Mâˆ—(h)

Î± âˆ’1
2

âˆ’max
h
Lâˆ—(h)

Î· âˆ’1
2

< 0, or equivalently,
Î± > 1
2 +
1
minh Mâˆ—(h)

LÎ· âˆ’1
2 âˆ’max
h
Lâˆ—(h)

Î· âˆ’1
2

.
Thus, we can conclude that, in this case, the solution is sparse if
Î± < 1
2 +
L âˆ’1
2 maxh Mâˆ—(h) ,

16.4 Latent Dirichlet Allocation
481
and dense if
Î± > 1
2 +
LÎ· âˆ’1
2
minh Mâˆ—(h) .
Summarizing the preceding, we have the following lemma:
Lemma 16.12
When 0 < Î· â‰¤
1
2L, the solution is sparse if Î± < 1
2 âˆ’
1
2 âˆ’LÎ·
minh Mâˆ—(h) ,
and dense if Î± >
1
2 âˆ’
1
2 âˆ’LÎ·
minh Mâˆ—(h) . When
1
2L < Î· â‰¤
1
2, the solution is sparse if
Î± < 1
2 +
LÎ·âˆ’1
2
maxh Mâˆ—(h) , and dense if Î± > 1
2 +
LÎ·âˆ’1
2
maxh Mâˆ—(h) . When Î· > 1
2, the solution is
sparse if Î± < 1
2 +
Lâˆ’1
2 maxh Mâˆ—(h) , and dense if Î± > 1
2 +
LÎ·âˆ’1
2
minh Mâˆ—(h) .
In the Limit When N, M â†’âˆwith M
N , L âˆ¼O(1)
The coefï¬cient of the leading term of the free energy is given by
Î»â€²VB
LDA = M

HÎ± âˆ’1
2

âˆ’

H

h=1

M(h)

Î± âˆ’1
2

.
(16.63)
Although the predictive distribution does not necessarily converge to the true
distribution, we can investigate the sparsity of the solution by considering the
duplication rules (16.57) through (16.59) that keep BÎ˜
âŠ¤unchanged. It is clear
that Eq. (16.63) is increasing with respect to 
H if Î± <
1
2, and decreasing
if Î± >
1
2. Combing this result with Lemma 16.12 completes the proof of
Corollary 16.11.
â–¡
In the case when L, M â‰ªN and in the case when L â‰ªM, N, Corollary
16.11 provides information on the sparsity of the VB solution, which will be
compared with other methods in Section 16.4.2. On the other hand, although
we have successfully derived the leading term of the free energy also in the
case when M â‰ªL, N and in the case when 1 â‰ªL, M, N, it unfortunately
provides no information on sparsity of the solution.
16.4.2 Asymptotic Analysis of MAP Learning and
Partially Bayesian Learning
For training the LDA model, MAP learning and partially Bayesian (PB)
learning (see Section 2.2.2), where Î˜ and/or B are point-estimated, are also
popular choices. Although the differences in update equations is small, it
can affect the asymptotic behavior. In this subsection, we aim to clarify the
difference in the asymptotic behavior.

482
16 Asymptotic VB Theory of Other Latent Variable Models
MAP learning, PB-A learning, PB-B learning, and VB learning, respec-
tively, solve the following problem:
min
r
F,
s.t.
â§âªâªâªâªâªâªâªâªâªâ¨âªâªâªâªâªâªâªâªâªâ©
rÎ˜,B(Î˜, B) = Î´(Î˜; Î˜)Î´(B; B)
(for MAP learning),
rÎ˜,B(Î˜, B) = rÎ˜(Î˜)Î´(B; B)
(for PB-A learning),
rÎ˜,B(Î˜, B) = Î´(Î˜; Î˜)rB(B)
(for PB-B learning),
rÎ˜,B(Î˜, B) = rÎ˜(Î˜)rB(B)
(for VB learning),
Similar analysis to Section 16.4.1 leads to the following theorem (the proof
is given in Section 16.4.5):
Theorem 16.13
In the limit when N â†’âˆwith L, M âˆ¼O(1), the solution is
sparse if Î± < Î±sparse, and dense if Î± > Î±dense. In the limit when N, M â†’âˆwith
M
N , L âˆ¼O(1), the solution is sparse if Î± < Î±Mâ†’âˆ, and dense if Î± > Î±Mâ†’âˆ.
Here, Î±sparse, Î±dense, and Î±Mâ†’âˆare given in Table 16.1.
A notable ï¬nding from Table 16.1 is that the threshold that determines the
topic sparsity of PB-B learning is (most of the case exactly) 1
2 larger than the
threshold of VB learning. The same relation is observed between MAP learn-
ing and PB-A learning. From these, we can conclude that point-estimating Î˜,
instead of integrating it out, increases the threshold by 1
2 in the LDA model.
We will validate this observation by numerical experiments in Section 16.4.4.
Table 16.1 Sparsity thresholds of VB, PB-A, PB-B, and MAP methods (see
Theorem 16.13). The ï¬rst four columns show the thresholds (Î±sparse, Î±dense), of
which the function forms depend on the range of Î·, in the limit when N â†’âˆ
with L, M âˆ¼O(1). A single value is shown if Î±sparse = Î±dense. The last column
shows the threshold Î±Mâ†’âˆin the limit when N, M â†’âˆwith M
N , L âˆ¼O(1).

Î±sparse, Î±dense
 
Î±Mâ†’âˆ
Î· range
0 < Î· â‰¤
1
2L
1
2L < Î· â‰¤1
2
1
2 < Î· < 1
1 â‰¤Î· < âˆ
0 < Î· < âˆ
VB
1
2 âˆ’
1
2 âˆ’LÎ·
minh Mâˆ—(h)
1
2 +
LÎ·âˆ’1
2
maxh Mâˆ—(h)
!
1
2 +
Lâˆ’1
2 maxh Mâˆ—(h) , 1
2 +
LÎ·âˆ’1
2
minh Mâˆ—(h)
"
1
2
PB-A
â€”
!
1
2, 1
2 +
L(Î·âˆ’1)
minh Mâˆ—(h)
"
1
2
PB-B
1
1 +
LÎ·âˆ’1
2
maxh Mâˆ—(h)
!
1 +
Lâˆ’1
2 maxh Mâˆ—(h) , 1 +
LÎ·âˆ’1
2
minh Mâˆ—(h)
"
1
MAP
â€”
!
1, 1 +
L(Î·âˆ’1)
minh Mâˆ—(h)
"
1

16.4 Latent Dirichlet Allocation
483
16.4.3 Discussion
The preceding theoretical analysis (Thereom 16.13) showed that VB tends to
induce weaker sparsity than MAP in the LDA model,3 i.e., VB requires sparser
prior (smaller Î±) than MAP to give a sparse solution (mean of the posterior).
This phenomenon is opposite to other models such as mixture models (Chapter
15), Bayesian networks (Section 16.1), hidden Markov models (Section 16.2),
and fully observed matrix factorization (Chapter 7), where VB tends to induce
stronger sparsity than MAP. This phenomenon might be partly explained as
follows: in the case of mixture models, the sparsity threshold depends on the
degree of freedom of a single component (Theorem 15.5). This is reasonable
because adding a single component increases the model complexity by this
amount. Also, in the case of LDA, adding a single topic requires additional
L+1 parameters. However, the added topic is shared over M documents, which
could discount the increased model complexity relative to the increased data
ï¬delity. Corollary 16.11, which implies the dependency of the threshold for Î±
on L and M, might support this conjecture. However, the same applies to the
matrix factorization, where VB was shown to give a sparser solution than MAP
(Chapter 7). Investigation on related models, e.g., Poisson MF (Gopalan et al.,
2013), would help us fully explain this phenomenon.
Unlike for the latent variable models in the previous sections, we derived a
general form of the asymptotic free energy for LDA, which can be applied
to different asymptotic limits and showed that the consistency does not
always hold (see Theorem 16.10). Speciï¬cally, the standard asymptotic theory
requires a large number N of words per document, compared to the number
M of documents and the vocabulary size L. Assuming such a situation
may be reasonable in some collaborative ï¬ltering applications, e.g., in the
Last.FM data which will be used for numerical illustration in Section 16.4.4.
However, L and/or M are comparable to or larger than N in many text analysis
applications.
The general form of the asymptotic free energy also allowed us to elucidate
the behavior of the VB free energy when L and/or M diverges with the same
order as N. This attempt successfully revealed the sparsity of the solution
for the case when M diverges while L âˆ¼O(1). However, when L diverges,
we found that the leading term of the free energy does not contain useful
information on sparsity of the solution. Higher-order asymptotic analysis will
be necessary to further understand the sparsity-inducing mechanism of the
LDA model with large vocabulary.
3 This tendency was pointed out (Asuncion et al., 2009) by using the approximation exp(Î¨(n)) â‰ˆ
n âˆ’1
2 and comparing the stationary condition. The theory here clariï¬ed the sparsity behavior of
the global solution based on the asymptotic free energy analysis.

484
16 Asymptotic VB Theory of Other Latent Variable Models
16.4.4 Numerical Illustration
Here we conduct numerical experiments on artiï¬cial and real data for collabo-
rative ï¬ltering.
The artiï¬cial data were created as follows: we ï¬rst sample the true
document matrix Î˜âˆ—of size MÃ—Hâˆ—and the true topic matrix Bâˆ—of size LÃ—Hâˆ—.
We assume that each row Î¸
âˆ—
m of Î˜âˆ—follows the Dirichlet distribution with
Î±âˆ—= 1/Hâˆ—, while each column Î²âˆ—
h of Bâˆ—follows the Dirichlet distribution with
Î·âˆ—= 1/L. The document length N(m) is sampled from the Poisson distribution
with mean N. The word histogram N(m)vm for each document is sampled from
the multinomial distribution with the parameter speciï¬ed by the mth row vector
of Bâˆ—Î˜âˆ—âŠ¤. Thus, we obtain the L Ã— M matrix V, which corresponds to the
empirical word distribution over M documents.
As a real-world data set, we used the Last.FM data set.4 Last.FM is
a well-known social music web site, and the data set includes the triple
(â€œuser,â€ â€œartist,â€ â€œFreqâ€), which was collected from the playlists of users in
0
0.5
1
0
0.5
1
0
20
40
60
80
100
(a) VB
0
0.5
1
0
0.5
1
0
20
40
60
80
100
(b) PB-A
0
0.5
1
0
0.5
1
0
20
40
60
80
100
(c) PB-B
0
0.5
1
0
0.5
1
0
20
40
60
80
100
(d) MAP
Figure 16.2 Estimated number 
H of topics by (a) VB learning, (b) PB-A learning,
(c) PB-B learning, and (d) MAP learning, on the artiï¬cial data with L = 100, M =
100, Hâˆ—= 20, and N âˆ¼10000.
4 http://mtg.upf.edu/node/1671

16.4 Latent Dirichlet Allocation
485
0
0.5
1
0
0.5
1
0
20
40
60
80
100
(a) VB
0
0.5
1
0
0.5
1
0
20
40
60
80
100
(b) PB-A
0
0.5
1
0
0.5
1
0
20
40
60
80
100
(c) PB-B
0
0.5
1
0
0.5
1
0
20
40
60
80
100
(d) MAP
Figure 16.3 Estimated number 
H of topics on the Last.FM data with L =
100, M = 100, and N âˆ¼700.
the community by using a plug-in in usersâ€™ media players. This triple means
that â€œuserâ€ played â€œartistâ€ music â€œFreqâ€ times, which indicates usersâ€™ preferred
artists. A user and a played artist are analogous to a document and a word,
respectively. We randomly chose L artists from the top 1, 000 frequent artists,
and M users who live in the United States. To ï¬nd a better local solution (which
hopefully is close to the global solution), we adopted a split and merge strategy
(Ueda et al., 2000), and chose the local solution giving the lowest free energy
among different initialization schemes.
Figure 16.2 shows the estimated number

H of topics by different
approximate Bayesian methods, i.e., VB, PB-A, PB-B, and MAP learning,
on the artiï¬cial data with L = 100, M = 100, Hâˆ—= 20, and N âˆ¼10000. We
can clearly see that the sparsity threshold in PB-B and MAP learning, where
Î˜ is point-estimated, is larger than that in VB and PB-A learning, where Î˜
is marginalized. This result supports the statement by Theorem 16.13. Figure
16.3 shows results on the Last.FM data with L = 100, M = 100, and N âˆ¼700.
We see a similar tendency to Figure 16.2 except the region where Î· < 1 for
PB-A learning, in which our theory does not predict the estimated number
of topics.

486
16 Asymptotic VB Theory of Other Latent Variable Models
0
0.5
1
0
0.5
1
0
20
40
60
80
100
(a) L = 100, M = 100
0
0.5
1
0
0.5
1
0
20
40
60
80
100
(b) L = 100, M = 1000
0
0.5
1
0
0.5
1
0
20
40
60
80
100
(c) L = 500, M = 100
0
0.5
1
0
0.5
1
0
20
40
60
80
100
(d) L = 500, M = 1000
Figure 16.4 Estimated number 
H of topics by VB learning on the artiï¬cial data
with Hâˆ—= 20 and N âˆ¼10000. For the case when L = 500, M = 1000, the
maximum estimated rank is limited to 100 for computational reason.
Finally, we investigate how different asymptotic settings affect the topic
sparsity. Figure 16.4 shows the sparsity dependence on L and M on the
artiï¬cial data. The graphs correspond to the four cases mentioned in Theorem
16.10, i.e, (a) L, M â‰ªN, (b) L â‰ªN, M, (c) M â‰ªN, L, and (d) 1 â‰ªN, L, M.
Corollary 16.11 explains the behavior in (a) and (b), and further analysis is
required to explain the behavior in (c) and (d).
16.4.5 Proof of Theorem 16.13
We analyze PB-A learning, PB-B learning, and MAP learning, and then
summarize the results, which proves Theorem 16.13.
PB-A Learning
The free energy for PB-A learning is given as follows:
FPBâˆ’A = Ï‡B + RPBâˆ’A + QPBâˆ’A,
(16.64)

16.4 Latent Dirichlet Allocation
487
where Ï‡B is a large constant corresponding to the negative entropy of the delta
functions (see Section 2.2.2), and
RPBâˆ’A =
/
log rÎ˜(Î˜)rB(B)
p(Î˜|Î±)p(B|Î·)
0
rPBâˆ’A(Î˜,B)
=
M

m=1
â›âœâœâœâœâœâlog
Î“(H
h=1 Î±PBâˆ’A
m,h
)
H
h=1 Î“(Î±PBâˆ’A
m,h
)
Î“(Î±)H
Î“(HÎ±)
+
H

h=1

Î±PBâˆ’A
m,h
âˆ’Î±
 â›âœâœâœâœâœâÎ¨(Î±PBâˆ’A
m,h
) âˆ’Î¨(
H

hâ€²=1
Î±PBâˆ’A
m,hâ€² )
ââŸâŸâŸâŸâŸâ 
ââŸâŸâŸâŸâŸâ 
+
H

h=1
â›âœâœâœâœâœâlog Î“(Î·)L
Î“(LÎ·) +
L

l=1
(1 âˆ’Î·)

log(Î·PBâˆ’A
l,h
) âˆ’log(L
lâ€²=1Î·PBâˆ’A
lâ€²,h
)
 ââŸâŸâŸâŸâŸâ ,
(16.65)
QPBâˆ’A =
/
log
rz

{{z(n,m)}N(m)
n=1 }M
m=1
 
p({w(n,m)}, {z(n,m)}|Î˜, B)
0
rPBâˆ’A(Î˜,B,{z(n,m)})
= âˆ’
M

m=1
N(m)
L

l=1
Vl,m log
â›âœâœâœâœâœâœâ
H

h=1
exp

Î¨(Î±PBâˆ’A
m,h
)
 
exp

Î¨(H
hâ€²=1 Î±PBâˆ’A
m,hâ€² )
 
Î·PBâˆ’A
l,h
L
lâ€²=1Î·PBâˆ’A
lâ€²,h
ââŸâŸâŸâŸâŸâŸâ .
(16.66)
Let us ï¬rst consider the case when Î· < 1. In this case, F diverges to F â†’
âˆ’âˆwith ï¬xed N, when Î·l,h = O(1) for any (l, h) and Î·lâ€²,h â†’+0 for all other
lâ€²  l. Therefore, the solution is useless.
When Î· â‰¥1, the solution satisï¬es the following stationary condition:
Î±PBâˆ’A
m,h
= Î± +
N(m)

n=1
zPBâˆ’A(n,m)
h
,
Î·PBâˆ’A
l,h
= Î· âˆ’1 +
M

m=1
N(m)

n=1
w(n,m)
l
zPBâˆ’A(n,m)
h
,
(16.67)
zPBâˆ’A(n,m)
h
=
exp

Î¨(Î±PBâˆ’A
m,h
)
 L
l=1(Î·PBâˆ’A
l,h
)w(n,m)
l
H
hâ€²=1

exp

Î¨(Î±PBâˆ’A
m,hâ€² )
 L
l=1(Î·PBâˆ’A
l,hâ€²
)w(n,m)
l
 .
(16.68)
In the same way as for VB learning, we can obtain the following lemma:
Lemma 16.14
Let B
PBâˆ’AÎ˜
PBâˆ’AâŠ¤= âŸ¨BÎ˜âŠ¤âŸ©rPBâˆ’A(Î˜,B). Then it holds that
âŸ¨(BÎ˜âŠ¤âˆ’B
PBâˆ’AÎ˜
PBâˆ’AâŠ¤)2
l,mâŸ©rPBâˆ’A(Î˜,B) = Op(Nâˆ’2),
(16.69)
QPBâˆ’A = âˆ’
M

m=1
N(m)
L

l=1
Vl,m log(B
PBâˆ’AÎ˜
PBâˆ’AâŠ¤)l,m + Op(Nâˆ’1).
(16.70)

488
16 Asymptotic VB Theory of Other Latent Variable Models
QPBâˆ’A is minimized when J = Op(Nâˆ’1), and it holds that
QPBâˆ’A = NS N(D) + Op(JN + LM).
RPBâˆ’A is written as follows:
RPBâˆ’A =
â§âªâªâªâ¨âªâªâªâ©M

HÎ±âˆ’1
2

+ 
HL (Î· âˆ’1)âˆ’

H

h=1


M(h)

Î±âˆ’1
2

+ L(h) (Î· âˆ’1)
â«âªâªâªâ¬âªâªâªâ­log N
+ (H âˆ’
H)L (Î· âˆ’1) log L + Op(H(M + L)).
(16.71)
Taking the different asymptotic limits, we obtain the following theorem:
Theorem 16.15
When Î· < 1, each column vector of B
PBâˆ’A has only one
nonzero entry. Assume in the following that Î· â‰¥1. In the limit when N â†’âˆ
with L, M âˆ¼O(1), it holds that J = Op(1/N) and
FPBâˆ’A(D) = Î»â€²PBâˆ’A
LDA
log N + Op(1),
where
Î»â€²PBâˆ’A
LDA
= M

HÎ± âˆ’1
2

+ 
HL (Î· âˆ’1) âˆ’

H

h=1


M(h)

Î± âˆ’1
2

+ L(h) (Î· âˆ’1)

.
In the limit when N, M â†’âˆwith M
N , L âˆ¼O(1), it holds that J = op(log N),
and
FPBâˆ’A(D) = Î»â€²PBâˆ’A
LDA
log N + op(N log N),
where
Î»â€²PBâˆ’A
LDA
= M

HÎ± âˆ’1
2

âˆ’

H

h=1

M(h)

Î± âˆ’1
2

.
In the limit when N, L â†’âˆwith L
N , M âˆ¼O(1), it holds that J = op(log N),
and
FPBâˆ’A(D) = Î»â€²PBâˆ’A
LDA
log N + op(N log N),
where
Î»â€²PBâˆ’A
LDA
= HL(Î· âˆ’1).
In the limit when N, L, M â†’âˆwith
L
N , M
N
âˆ¼O(1), it holds that J =
op(N log N), and
FPBâˆ’A(D) = Î»â€²PBâˆ’A
LDA
log N + op(N2 log N),

16.4 Latent Dirichlet Allocation
489
where
Î»â€²PBâˆ’A
LDA
= H(MÎ± + L(Î· âˆ’1)).
Note that Theorem 16.15 provides no information on the sparsity of the
PB-A solution for Î· < 1. In the following subsection, we investigate the
sparsity of the solution for Î· â‰¥1.
In the Limit When N â†’âˆwith L, M âˆ¼O(1)
The coefï¬cient of the leading term of the free energy is
Î»â€²PBâˆ’A
LDA
= M

HÎ± âˆ’1
2

+

H

h=1

L(Î· âˆ’1) âˆ’
M(h)

Î± âˆ’1
2

âˆ’L(h) (Î· âˆ’1)

.
The solution is sparse if Î»â€²PBâˆ’A
LDA
is increasing with respect to 
H, and dense if it
is decreasing. We focus on the case where Î· â‰¥1. Eqs. (16.57) through (16.59)
imply the following:
(I) When Î± < 1
2, the solution is sparse if
L(Î· âˆ’1) âˆ’max
h

Mâˆ—(h)

Î± âˆ’1
2

+ Lâˆ—(h) (Î· âˆ’1)

> 0,
(16.72)
and dense if
L(Î· âˆ’1) âˆ’max
h

Mâˆ—(h)

Î± âˆ’1
2

+ Lâˆ—(h) (Î· âˆ’1)

< 0.
(16.73)
Therefore, the solution is sparse if
L(Î· âˆ’1) âˆ’min
h
Mâˆ—(h)

Î± âˆ’1
2

âˆ’max
h
Lâˆ—(h) (Î· âˆ’1) > 0, or equivalently,
Î± < 1
2 +

L âˆ’maxh Lâˆ—(h) 
(Î· âˆ’1)
minh Mâˆ—(h)
,
and dense if
L(Î· âˆ’1) âˆ’max
h
Mâˆ—(h)

Î± âˆ’1
2

âˆ’max
h
Lâˆ—(h) (Î· âˆ’1) < 0, or equivalently,
Î± > 1
2 +

L âˆ’maxh Lâˆ—(h) 
(Î· âˆ’1)
maxh Mâˆ—(h)
.
Therefore, the solution is always sparse in this case.

490
16 Asymptotic VB Theory of Other Latent Variable Models
(II) When Î± â‰¥1
2, the solution is sparse if Eq. (16.72) holds, and dense if Eq.
(16.73) holds. Therefore, the solution is sparse if
L(Î· âˆ’1) âˆ’max
h
Mâˆ—(h)

Î± âˆ’1
2

âˆ’max
h
Lâˆ—(h) (Î· âˆ’1) > 0, or equivalently,
Î± < 1
2 +

L âˆ’maxh Lâˆ—(h) 
(Î· âˆ’1)
maxh Mâˆ—(h)
,
and dense if
L(Î· âˆ’1) âˆ’min
h
Mâˆ—(h)

Î± âˆ’1
2

âˆ’max
h
Lâˆ—(h) (Î· âˆ’1) < 0, or equivalently,
Î± > 1
2 +

L âˆ’maxh Lâˆ—(h) 
(Î· âˆ’1)
minh Mâˆ—(h)
.
Thus, we can conclude that, in this case, the solution is sparse if
Î± < 1
2,
and dense if
Î± > 1
2 +
L(Î· âˆ’1)
minh Mâˆ—(h) .
Summarizing the preceding, we have the following lemma:
Lemma 16.16
Assume that Î· â‰¥1. The solution is sparse if Î± < 1
2, and dense
if Î± > 1
2 +
L(Î·âˆ’1)
minh Mâˆ—(h) .
In the Limit When N, M â†’âˆwith M
N , L âˆ¼O(1)
The coefï¬cient of the leading term of the free energy is given by
Î»â€²PBâˆ’A
LDA
= M

HÎ± âˆ’1
2

âˆ’

H

h=1

M(h)

Î± âˆ’1
2

.
(16.74)
Although the predictive distribution does not necessarily converge to the true
distribution, we can investigate the sparsity of the solution by considering the
duplication rules (16.57) through (16.59) that keep BÎ˜
âŠ¤unchanged. It is clear
that Eq. (16.74) is increasing with respect to 
H if Î± <
1
2, and decreasing
if Î± >
1
2. Combing this result with Lemma 16.16, we obtain the following
corollary:
Corollary 16.17
Assume that Î· â‰¥1. In the limit when N â†’âˆwith L,
M âˆ¼O(1), the PB-A solution is sparse if Î± < 1
2, and dense if Î± > 1
2 +
L(Î·âˆ’1)
minh Mâˆ—(h) .

16.4 Latent Dirichlet Allocation
491
In the limit when N, M â†’âˆwith M
N , L âˆ¼O(1), the PB-A solution is sparse if
Î± < 1
2, and dense if Î± > 1
2.
PB-B Learning
The free energy for PB-B learning is given as follows:
FPBâˆ’B = Ï‡Î˜ + RPBâˆ’B + QPBâˆ’B,
(16.75)
where Ï‡Î˜ is a large constant corresponding to the negative entropy of the delta
functions, and
RPBâˆ’B =
/
log rÎ˜(Î˜)rB(B)
p(Î˜|Î±)p(B|Î·)
0
rPBâˆ’B(Î˜,B)
=
M

m=1
â›âœâœâœâœâœâlog Î“(Î±)H
Î“(HÎ±) +
H

h=1
(1 âˆ’Î±)
â›âœâœâœâœâœâlog(Î±PBâˆ’B
m,h ) âˆ’log(
H

hâ€²=1
Î±PBâˆ’B
m,hâ€² )
ââŸâŸâŸâŸâŸâ 
ââŸâŸâŸâŸâŸâ 
+
H

h=1
â›âœâœâœâœâœâlog
Î“(L
l=1Î·PBâˆ’B
l,h
)
L
l=1 Î“(Î·PBâˆ’B
l,h
)
Î“(Î·)L
Î“(LÎ·)
+
L

l=1

Î·PBâˆ’B
l,h
âˆ’Î·
 
Î¨(Î·PBâˆ’B
l,h
) âˆ’Î¨(L
lâ€²=1Î·PBâˆ’B
lâ€²,h
)
 ââŸâŸâŸâŸâŸâ ,
(16.76)
QPBâˆ’B =
/
log
rz

{{z(n,m)}N(m)
n=1 }M
m=1
 
p({w(n,m)}, {z(n,m)}|Î˜, B)
0
rPBâˆ’B(Î˜,B,{z(n,m)})
= âˆ’
M

m=1
N(m)
L

l=1
Vl,m log
â›âœâœâœâœâœâœâ
H

h=1
Î±PBâˆ’B
m,h
H
hâ€²=1 Î±PBâˆ’B
m,hâ€²
exp

Î¨(Î·PBâˆ’B
l,h
)
 
exp

Î¨(L
lâ€²=1Î·PBâˆ’B
lâ€²,h
)
 
ââŸâŸâŸâŸâŸâŸâ .
(16.77)
Let us ï¬rst consider the case when Î± < 1. In this case, F diverges to
F â†’âˆ’âˆwith ï¬xed N, when Î±m,h = O(1) for any (m, h) and Î±m,hâ€² â†’+0 for
all other hâ€²  h. Therefore, the solution is sparse (so sparse that the estimator
is useless).
When Î± â‰¥1, the solution satisï¬es the following stationary condition:
Î±PBâˆ’B
m,h
= Î± âˆ’1 +
N(m)

n=1
zPBâˆ’B(n,m)
h
,
Î·PBâˆ’B
l,h
= Î· +
M

m=1
N(m)

n=1
w(n,m)
l
zPBâˆ’B(n,m)
h
,
(16.78)
zPBâˆ’B(n,m)
h
=
Î±PBâˆ’B
m,h
exp
,L
l=1 w(n,m)
l

Î¨(Î·PBâˆ’B
l,h
) âˆ’Î¨
L
lâ€²=1Î·PBâˆ’B
lâ€²,h
  -
H
hâ€²=1 Î±PBâˆ’B
m,hâ€² exp
,L
l=1 w(n,m)
l

Î¨(Î·PBâˆ’B
l,hâ€²
) âˆ’Î¨
L
lâ€²=1Î·PBâˆ’B
lâ€²,hâ€²
  -.
(16.79)

492
16 Asymptotic VB Theory of Other Latent Variable Models
In the same way as for VB and PB-A learning, we can obtain the following
lemma:
Lemma 16.18
Let B
PBâˆ’BÎ˜
PBâˆ’BâŠ¤= âŸ¨BÎ˜âŠ¤âŸ©rPBâˆ’B(Î˜,B). Then it holds that
âŸ¨(BÎ˜âŠ¤âˆ’B
PBâˆ’BÎ˜
PBâˆ’BâŠ¤)2
l,mâŸ©rPBâˆ’B(Î˜,B) = Op(Nâˆ’2),
(16.80)
QPBâˆ’B = âˆ’
M

m=1
N(m)
L

l=1
Vl,m log(B
PBâˆ’BÎ˜
PBâˆ’BâŠ¤)l,m + Op(Nâˆ’1).
(16.81)
QPBâˆ’B is minimized when J = Op(Nâˆ’1), and it holds that
QPBâˆ’B = NS N(D) + Op(JN + LM).
RPBâˆ’B is written as follows:
RPBâˆ’B =
â§âªâªâªâ¨âªâªâªâ©MH (Î± âˆ’1)+ 
H

LÎ· âˆ’1
2

âˆ’

H

h=1


M(h) (Î± âˆ’1) +L(h)

Î· âˆ’1
2
â«âªâªâªâ¬âªâªâªâ­log N
+ (H âˆ’
H)

LÎ· âˆ’1
2

log L + Op(H(M + L)).
(16.82)
Taking the different asymptotic limits, we obtain the following theorem:
Theorem 16.19
When Î± < 1, each row vector of Î˜
PBâˆ’B has only one nonzero
entry, and the PB-B solution is sparse. Assume in the following that Î± â‰¥1. In
the limit when N â†’âˆwith L, M âˆ¼O(1), it holds that J = Op(1/N) and
FPBâˆ’B(D) = Î»â€²PBâˆ’B
LDA
log N + Op(1),
where
Î»â€²PBâˆ’B
LDA
= MH (Î± âˆ’1) + 
H

LÎ· âˆ’1
2

âˆ’

H

h=1


M(h) (Î± âˆ’1) + L(h)

Î· âˆ’1
2

.
In the limit when N, M â†’âˆwith M
N , L âˆ¼O(1), it holds that J = op(log N),
and
FPBâˆ’B(D) = Î»â€²PBâˆ’B
LDA
log N + op(N log N),
where
Î»â€²PBâˆ’B
LDA
= MH (Î± âˆ’1) âˆ’

H

h=1

M(h) (Î± âˆ’1) .

16.4 Latent Dirichlet Allocation
493
In the limit when N, L â†’âˆwith L
N , M âˆ¼O(1), it holds that J = op(log N),
and
FPBâˆ’B(D) = Î»â€²PBâˆ’B
LDA
log N + op(N log N),
where
Î»â€²PBâˆ’B
LDA
= HLÎ·.
In the limit when N, L, M â†’âˆwith
L
N , M
N
âˆ¼O(1), it holds that J =
op(N log N), and
FPBâˆ’B(D) = Î»â€²PBâˆ’B
LDA
log N + op(N2 log N),
where
Î»â€²PBâˆ’B
LDA
= H(M(Î± âˆ’1) + LÎ·).
Theorem 16.19 states that the PB-B solution is sparse when Î± < 1. In the
following subsection, we investigate the sparsity of the solution for Î± â‰¥1.
In the Limit When N â†’âˆwith L, M âˆ¼O(1)
The coefï¬cient of the leading term of the free energy is
Î»â€²PBâˆ’B
LDA
= MH (Î± âˆ’1) +

H

h=1

LÎ· âˆ’1
2 âˆ’
M(h) (Î± âˆ’1) âˆ’L(h)

Î· âˆ’1
2

.
The solution is sparse if Î»â€²PBâˆ’B
LDA
is increasing with respect to 
H, and dense if it
is decreasing. We focus on the case where Î± â‰¥1. Eqs. (16.57) through (16.59)
imply the following:
(I) When 0 < Î· â‰¤
1
2L, the solution is sparse if
LÎ· âˆ’1
2 âˆ’max
h
Mâˆ—(h) (Î± âˆ’1) > 0, or equivalently,
Î± < 1 âˆ’
1
maxh Mâˆ—(h)
1
2 âˆ’LÎ·

,
and dense if
Î± > 1 âˆ’
1
maxh Mâˆ—(h)
1
2 âˆ’LÎ·

.
Therefore, the solution is always dense in this case.
(II) When
1
2L < Î· â‰¤1
2, the solution is sparse if
LÎ· âˆ’1
2 âˆ’max
h
Mâˆ—(h) (Î± âˆ’1) > 0, or equivalently, Î± < 1 +
LÎ· âˆ’1
2
maxh Mâˆ—(h) ,

494
16 Asymptotic VB Theory of Other Latent Variable Models
and dense if
Î± > 1 +
LÎ· âˆ’1
2
maxh Mâˆ—(h) .
(III) When Î· > 1
2, the solution is sparse if
LÎ· âˆ’1
2 âˆ’max
h

Mâˆ—(h) (Î± âˆ’1) + Lâˆ—(h)

Î· âˆ’1
2

> 0,
(16.83)
and dense if
LÎ· âˆ’1
2 âˆ’max
h

Mâˆ—(h) (Î± âˆ’1) + Lâˆ—(h)

Î· âˆ’1
2

< 0.
(16.84)
Therefore, the solution is sparse if
LÎ· âˆ’1
2 âˆ’max
h
Mâˆ—(h) (Î± âˆ’1) âˆ’max
h
Lâˆ—(h)

Î· âˆ’1
2

> 0, or equivalently,
Î± < 1 +
1
maxh Mâˆ—(h)

LÎ· âˆ’1
2 âˆ’max
h
Lâˆ—(h)

Î· âˆ’1
2

,
and dense if
LÎ· âˆ’1
2 âˆ’min
h
Mâˆ—(h) (Î± âˆ’1) âˆ’max
h
Lâˆ—(h)

Î· âˆ’1
2

< 0, or equivalently,
Î± > 1 +
1
minh Mâˆ—(h)

LÎ· âˆ’1
2 âˆ’max
h
Lâˆ—(h)

Î· âˆ’1
2

.
Thus, we can conclude that, in this case, the solution is sparse if
Î± < 1 +
L âˆ’1
2 maxh Mâˆ—(h) ,
and dense if
Î± > 1 +
LÎ· âˆ’1
2
minh Mâˆ—(h) .
Summarizing the preceding, we have the following lemma:
Lemma 16.20
Assume that Î± â‰¥1. When 0 < Î· â‰¤
1
2L, the solution is always
dense. When
1
2L < Î· â‰¤1
2, the solution is sparse if Î± < 1 +
LÎ·âˆ’1
2
maxh Mâˆ—(h) , and dense
if Î± > 1 +
LÎ·âˆ’1
2
maxh Mâˆ—(h) . When Î· > 1
2, the solution is sparse if Î± < 1 +
Lâˆ’1
2 maxh Mâˆ—(h) ,
and dense if Î± > 1 +
LÎ·âˆ’1
2
minh Mâˆ—(h) .

16.4 Latent Dirichlet Allocation
495
In the Limit When N, M â†’âˆwith M
N , L âˆ¼O(1)
The coefï¬cient of the leading term of the free energy is given by
Î»â€²PBâˆ’B
LDA
= M (HÎ± âˆ’1) âˆ’

H

h=1

M(h) (Î± âˆ’1) .
(16.85)
Although the predictive distribution does not necessarily converge to the true
distribution, we can investigate the sparsity of the solution by considering the
duplication rules (16.57) through (16.59) that keep BÎ˜
âŠ¤unchanged. It is clear
that Eq. (16.85) is decreasing with respect to 
H if Î± > 1. Combing this result
with Theorem 16.19, which states that the PB-B solution is sparse when Î± < 1,
and Lemma 16.20, we obtain the following corollary:
Corollary 16.21
Consider the limit when N â†’âˆwith L, M âˆ¼O(1). When
0 < Î· â‰¤
1
2L, the PB-B solution is sparse if Î± < 1, and dense if Î± > 1. When
1
2L < Î· â‰¤
1
2, the PB-B solution is sparse if Î± < 1 +
LÎ·âˆ’1
2
maxh Mâˆ—(h) , and dense if
Î± > 1+
LÎ·âˆ’1
2
maxh Mâˆ—(h) . When Î· > 1
2, the PB-B solution is sparse if Î± < 1+
Lâˆ’1
2 maxh Mâˆ—(h) ,
and dense if Î± > 1 +
LÎ·âˆ’1
2
minh Mâˆ—(h) . In the limit when N, M â†’âˆwith M
N , L âˆ¼O(1),
the PB-B solution is sparse if Î± < 1, and dense if Î± > 1.
MAP Learning
The free energy for MAP learning is given as follows:
FMAP = Ï‡Î˜ + Ï‡B + RMAP + QMAP,
(16.86)
where Ï‡Î˜ and Ï‡B are large constants corresponding to the negative entropies of
the delta functions, and
RMAP =
/
log rÎ˜(Î˜)rB(B)
p(Î˜|Î±)p(B|Î·)
0
rMAP(Î˜,B)
=
M

m=1
â›âœâœâœâœâœâlog Î“(Î±)H
Î“(HÎ±) +
H

h=1
(1 âˆ’Î±)
â›âœâœâœâœâœâlog(Î±MAP
m,h ) âˆ’log(
H

hâ€²=1
Î±MAP
m,hâ€² )
ââŸâŸâŸâŸâŸâ 
ââŸâŸâŸâŸâŸâ 
+
H

h=1
â›âœâœâœâœâœâlog Î“(Î·)L
Î“(LÎ·) +
L

l=1
(1 âˆ’Î·)

log(Î·MAP
l,h
) âˆ’log(L
lâ€²=1Î·MAP
lâ€²,h )
 ââŸâŸâŸâŸâŸâ ,
(16.87)
QMAP =
/
log
rz

{{z(n,m)}N(m)
n=1 }M
m=1
 
p({w(n,m)}, {z(n,m)}|Î˜, B)
0
rMAP(Î˜,B,{z(n,m)})
= âˆ’
M

m=1
N(m)
L

l=1
Vl,m log
â›âœâœâœâœâœâ
H

h=1
Î±MAP
m,h
H
hâ€²=1 Î±MAP
m,hâ€²
Î·MAP
l,h
L
lâ€²=1Î·MAP
lâ€²,h
ââŸâŸâŸâŸâŸâ .
(16.88)
Let us ï¬rst consider the case when Î± < 1. In this case, F diverges to
F â†’âˆ’âˆwith ï¬xed N, when Î±m,h = O(1) for any (h, m) and Î±m,hâ€² â†’+0 for

496
16 Asymptotic VB Theory of Other Latent Variable Models
all other hâ€²  h. Therefore, the solution is sparse (so sparse that the estimator
is useless). Similarly, assume that Î· < 1. Then F diverges to F â†’âˆ’âˆwith
ï¬xed N, when Î·l,h = O(1) for any (l, h) and Î·lâ€²,h â†’+0 for all other lâ€²  l.
Therefore, the solution is useless.
When Î± â‰¥1 and Î· â‰¥1, the solution satisï¬es the following stationary
condition:
Î±MAP
m,h
= Î± âˆ’1 +
N(m)

n=1
zMAP(n,m)
h
,
Î·MAP
l,h
= Î· âˆ’1 +
M

m=1
N(m)

n=1
w(n,m)
l
zMAP(n,m)
h
,
(16.89)
zMAP(n,m)
h
=
Î±MAP
m,h
L
l=1(Î·MAP
l,h
)w(n,m)
l
H
hâ€²=1

Î±MAP
m,hâ€²
L
l=1(Î·MAP
l,hâ€² )w(n,m)
l
 .
(16.90)
In the same way as for VB, PB-A, and PB-B learning, we can obtain the
following lemma:
Lemma 16.22
Let B
MAPÎ˜
MAPâŠ¤= âŸ¨BÎ˜âŠ¤âŸ©rMAP(Î˜,B). Then QMAP is minimized
when J = Op(Nâˆ’1), and it holds that
QMAP = NS N(D) + Op(JN + LM).
RMAP is written as follows:
RMAP =
â§âªâªâªâ¨âªâªâªâ©MH (Î± âˆ’1) + 
HL (Î· âˆ’1) âˆ’

H

h=1
 
M(h) (Î± âˆ’1) + L(h) (Î· âˆ’1)
 
â«âªâªâªâ¬âªâªâªâ­log N
+ (H âˆ’
H)L (Î· âˆ’1) log L + Op(H(M + L)).
(16.91)
Taking the different asymptotic limits, we obtain the following theorem:
Theorem 16.23
When Î± < 1, each row vector of Î˜
MAP has only one nonzero
entry, and the MAP solution is sparse. When Î· < 1, each column vector of
B
MAP has only one nonzero entry. Assume in the following that Î±, Î· â‰¥1. In the
limit when N â†’âˆwith L, M âˆ¼O(1), it holds that J = Op(1/N) and
FMAP(D) = Î»â€²MAP
LDA log N + Op(1),
where
Î»â€²MAP
LDA = MH (Î± âˆ’1) + 
HL (Î· âˆ’1) âˆ’

H

h=1
 
M(h) (Î± âˆ’1) + L(h) (Î· âˆ’1)
 
.

16.4 Latent Dirichlet Allocation
497
In the limit when N, M â†’âˆwith M
N , L âˆ¼O(1), it holds that J = op(log N),
and
FMAP(D) = Î»â€²MAP
LDA log N + op(N log N),
where
Î»â€²MAP
LDA = MH (Î± âˆ’1) âˆ’

H

h=1

M(h) (Î± âˆ’1) .
In the limit when N, L â†’âˆwith L
N , M âˆ¼O(1), it holds that J = op(log N),
and
FMAP(D) = Î»â€²MAP
LDA log N + op(N log N),
where
Î»â€²MAP
LDA = HL(Î· âˆ’1).
In the limit when N, L, M â†’âˆwith
L
N , M
N
âˆ¼O(1), it holds that J =
op(N log N), and
FMAP(D) = Î»â€²MAP
LDA log N + op(N2 log N),
where
Î»â€²MAP
LDA = H(M(Î± âˆ’1) + L(Î· âˆ’1)).
Theorem 16.23 states that the MAP solution is sparse when Î± < 1. However,
it provides no information on the sparsity of the MAP solution for Î· < 1. In
the following, we investigate the sparsity of the solution for Î±, Î· â‰¥1.
In the Limit When N â†’âˆwith L, M âˆ¼O(1)
The coefï¬cient of the leading term of the free energy is
Î»â€²MAP
LDA = MH (Î± âˆ’1) +

H

h=1

L(Î· âˆ’1) âˆ’
M(h) (Î± âˆ’1) âˆ’L(h) (Î· âˆ’1)
 
.
The solution is sparse if Î»â€²MAP
LDA is increasing with respect to 
H, and dense if it is
decreasing. We focus on the case where Î±, Î· â‰¥1. Eqs. (16.57) through (16.59)
imply the following:

498
16 Asymptotic VB Theory of Other Latent Variable Models
The solution is sparse if
L(Î· âˆ’1) âˆ’max
h

Mâˆ—(h) (Î± âˆ’1) + Lâˆ—(h) (Î· âˆ’1)
 
> 0,
(16.92)
and dense if
L(Î· âˆ’1) âˆ’max
h

Mâˆ—(h) (Î± âˆ’1) + Lâˆ—(h) (Î· âˆ’1)
 
< 0.
(16.93)
Therefore, the solution is sparse if
L(Î· âˆ’1) âˆ’max
h
Mâˆ—(h) (Î± âˆ’1) âˆ’max
h
Lâˆ—(h) (Î· âˆ’1) > 0, or equivalently,
Î± < 1 + (L âˆ’maxh Lâˆ—(h))(Î· âˆ’1)
maxh Mâˆ—(h)
,
and dense if
L(Î· âˆ’1) âˆ’min
h
Mâˆ—(h) (Î± âˆ’1) âˆ’max
h
Lâˆ—(h) (Î· âˆ’1) < 0, or equivalently,
Î± > 1 + (L âˆ’maxh Lâˆ—(h))(Î· âˆ’1)
minh Mâˆ—(h)
.
Thus, we can conclude that the solution is sparse if
Î± < 1,
and dense if
Î± > 1 +
L(Î· âˆ’1)
minh Mâˆ—(h) .
Summarizing the preceding, we have the following lemma:
Lemma 16.24
Assume that Î· â‰¥1. The solution is sparse if Î± < 1, and dense
if Î± > 1 +
L(Î·âˆ’1)
minh Mâˆ—(h) .
In the Limit When N, M â†’âˆwith M
N , L âˆ¼O(1)
The coefï¬cient of the leading term of the free energy is given by
Î»â€²MAP
LDA = MH (Î± âˆ’1) âˆ’

H

h=1

M(h) (Î± âˆ’1) .
(16.94)
Although the predictive distribution does not necessarily converge to the true
distribution, we can investigate the sparsity of the solution by considering the
duplication rules (16.57) through (16.59) that keep BÎ˜
âŠ¤unchanged. It is clear
that Eq. (16.94) is decreasing with respect to 
H if Î± > 1. Combing this result
with Theorem 16.23, which states that the MAP solution is sparse if Î± < 1,
and Lemma 16.24, we obtain the following corollary:

16.4 Latent Dirichlet Allocation
499
Corollary 16.25
Assume that Î· â‰¥1. In the limit when N â†’âˆwith L, M âˆ¼
O(1), the MAP solution is sparse if Î± < 1, and dense if Î± > 1 +
L(Î·âˆ’1)
minh Mâˆ—(h) . In the
limit when N, M â†’âˆwith M
N , L âˆ¼O(1), the MAP solution is sparse if Î± < 1,
and dense if Î± > 1.
Summary of Results
Summarizing Corollaries 16.11, 16.17, 16.21, and 16.25 completes the proof
of Theorem 16.13.
â–¡

17
Uniï¬ed Theory for Latent Variable Models
In this chapter, we present a formula for evaluating an asymptotic form of the
VB free energy of a general class of latent variable models by relating it to
the asymptotic theory of Bayesian learning (Watanabe, 2012). This formula is
applicable to all latent variable models discussed in Chapters 15 and 16.1 It also
explains relationships between these asymptotic analyses of VB free energy
and several previous works where the asymptotic Bayes free energy has been
analyzed for speciï¬c latent variable models. We apply this formula to Gaussian
mixture models (GMMs) as an example and demonstrate another proof of
the upper-bound of the VB free energy given in Section 15.2. Furthermore,
this analysis also provides a quantity that is related to the generalization
performance of VB learning. Analysis of generalization performance of VB
learning has been conducted only for limited cases, as discussed in Chapter
14. We show inequalities that relate the VB free energy to the generalization
errors of an approximate predictive distribution (Watanabe, 2012).
17.1 Local Latent Variable Model
Consider the joint model
p(x, z|w)
(17.1)
on the observed variable x and the local latent variable z with the parameter w.
The marginal distribution of the observed variable is2
1 The reduced rank regression (RRR) model discussed in Chapter 14 is not included in this class
of latent variable models.
2 The model is denoted as if the local latent variable is discrete, it can also be continuous. In this
case, the sum 
z is replaced by the integral

dz. The probabilistic principal component
analysis is an example with a continuous local latent variable.
500

17.1 Local Latent Variable Model
501
p(x|w) =

z
p(x, z|w).
(17.2)
For the complete data set {D, H} = {(x(n), z(1)),. . . , (x(N), z(N))}, we assume
the i.i.d. model
p(D, H|w) =
N

n=1
p(x(n), z(n)|w),
which implies
p(D|w) =
N

n=1
p(x(n)|w),
p(H|D, w) =
N

n=1
p(z(n)|x(n), w).
We assume that
p(x|wâˆ—) =

z
p(x, z|wâˆ—)
with the parameter wâˆ—is the underlying distribution generating data D = {x(1),
. . . , x(N)}. Because of the nonidentiï¬ability of the latent variable model, the set
of true parameters,
Wâˆ—â‰¡
â§âªâªâ¨âªâªâ©wâˆ—;

z
p(x, z|wâˆ—) = p(x|wâˆ—)
â«âªâªâ¬âªâªâ­,
(17.3)
is not generally a point but can be a union of several manifolds with
singularities as demonstrated in Section 13.5.
In the analysis in this chapter, we deï¬ne and analyze quantities related to
generalization performance of a joint model, where the local latent variables
are treated as observed variables. Although we do not consider the case where
the local latent variables are observed, those quantities are useful for relating
generalization properties of VB learning to those of Bayesian learning, with
which we establish a uniï¬ed theory connecting VB learning and Bayesian
learning of latent variable models.
Thus, consider for a moment the Bayesian learning of the joint model
(17.1), where the complete data set {D, H} is observed. For the prior distri-
bution p(w), the posterior distribution is given by
p(w|D, H) = p(D, H|w)p(w)
p(D, H)
.
(17.4)

502
17 Uniï¬ed Theory for Latent Variable Models
The Bayes free energy of the joint model is deï¬ned by
FBayes
Joint (D, H) = âˆ’log p(D, H) = âˆ’log

p(D, H|w)p(w)dw.
If wâˆ—âˆˆWâˆ—is the true parameter, i.e., the complete data set {D, H} is
generated from q(x, z) = p(x, z|wâˆ—) i.i.d., the relative Bayes free energy is
deï¬ned by
FBayes
Joint (D, H) = FBayes
Joint (D, H) âˆ’NS N(D, H),
(17.5)
where
S N(D, H) = âˆ’1
N
N

n=1
log p(x(n), z(n)|wâˆ—)
is the empirical joint entropy. Then the average relative Bayes free energy is
deï¬ned by
F
Bayes
Joint (N) =
FBayes
Joint (D, H)

p(D,H|wâˆ—) ,
and the average Bayes generalization error of the predictive distribution for the
joint model is deï¬ned by
GE
Bayes
Joint (N) = KL(p(x, z|wâˆ—)||p(x, z|D, H))
p(D,H|wâˆ—) ,
where
p(x, z|D, H) =

p(x, z|w)p(w|D, H)dw.
These two quantities are related to each other as Eq. (13.24):
GE
Bayes
Joint (N) = F
Bayes
Joint (N + 1) âˆ’F
Bayes
Joint (N).
(17.6)
Furthermore, the average relative Bayes free energy for the joint model can be
approximated as (see Eq. (13.118))
F
Bayes
Joint (N) â‰ˆâˆ’log

exp

âˆ’NE(w)
 
Â· p(w)dw,
(17.7)
where
E(w) = KL(p(x, z|wâˆ—)||p(x, z|w)) =
/
log p(x, z|wâˆ—)
p(x, z|w)
0
p(x,z|wâˆ—)
.
(17.8)

17.1 Local Latent Variable Model
503
Since the log-sum inequality yields that3

z
p(x, z|wâˆ—) log p(x, z|wâˆ—)
p(x, z|w) â‰¥p(x|wâˆ—) log p(x|wâˆ—)
p(x|w) ,
we have
E(w) â‰¥E(w),
(17.10)
where
E(w) = KL(p(x|wâˆ—)||p(x|w)) =
/
log p(x|wâˆ—)
p(x|w)
0
p(x|wâˆ—)
.
Hence, it follows from Eq. (13.118) that
F
Bayes(N) =
FBayes(D)

q(D)
â‰ˆâˆ’log

exp (âˆ’NE(w)) Â· p(w)dw
â‰¤âˆ’log

exp $ âˆ’NE(w)% Â· p(w)dw â‰ˆF
Bayes
Joint (N),
(17.11)
where
FBayes(D) = FBayes(D) âˆ’NS N(D)
is the relative Bayes free energy deï¬ned by the Bayes free energy of the
original marginal model,
FBayes(D) = âˆ’log p(D) = âˆ’log

p(D|w)p(w)dw
and its empirical entropy,
S N(D) = âˆ’1
N log p(D|wâˆ—) = âˆ’1
N
N

n=1
log p(x(n)|wâˆ—),
(17.12)
as in Section 13.3.2.
The asymptotic theory of Bayesian learning (Theorem 13.13) shows that an
asymptotic form of F
Bayes
Joint (N) is given by
F
Bayes
Joint (N) = Î»â€²Bayes
Joint
log N âˆ’(mâ€²Bayes
Joint
âˆ’1) log log N + O(1),
(17.13)
3 The log-sum inequality is the following inequality satisï¬ed for nonnegative reals ai â‰¥0 and
bi â‰¥0:

i
ai log ai
bi
â‰¥
â›âœâœâœâœâœâ

i
ai
ââŸâŸâŸâŸâŸâ log
$
i ai
%
$
i bi
%.
(17.9)
This can be proved by subtracting the right-hand side from the left-hand side and applying the
nonnegativity of the KL divergence.

504
17 Uniï¬ed Theory for Latent Variable Models
where âˆ’Î»â€²Bayes
Joint
and mâ€²Bayes
Joint
are respectively the largest pole and its order of the
zeta function deï¬ned for a complex number z by
Î¶E(z) =

E(w)zp(w)dw.
(17.14)
This means that the asymptotic behavior of the free energy is characterized by
E(w), while that of the Bayes free energy FBayes is characterized by E(w) =
KL(p(x|wâˆ—)||p(x|w)) and the zeta function Î¶E(z) in Eq. (13.122) as Theorem
13.13. The two functions, E and E, are related by the log-sum inequality
(17.10).
Then Corollary 13.14 implies the following asymptotic expansion of the
average generalization error:
GE
Bayes
Joint (N) =
Î»â€²Bayes
Joint
N
âˆ’
mâ€²Bayes
Joint
âˆ’1
N log N
+ o

1
N log N

.
(17.15)
With the preceding quantities, we ï¬rst provide a general upper-bound for the
VB free energy (Section 17.2), and then show inequalities relating the VB free
energy to the generalization errors of an approximate predictive distribution
for the joint model (Section 17.4).
17.2 Asymptotic Upper-Bound for VB Free Energy
Given the training data D = {x(1),. . . , x(N)}, consider VB learning for the latent
variable model (17.2) with the prior distribution p(w). Under the constraint,
r(w, H) = rw(w)rH(H),
the VB free energy is deï¬ned by
FVB(D) =
min
rw(w),rH(H) F(r),
where
F(r) =
/
log
rw(w)rH(H)
p(D, H|w)p(w)
0
rw(w)rH(H)
(17.16)
= FBayes(D) + KL (rw(w)rH(H)||p(w, H|D)) .
(17.17)
The stationary condition of the free energy yields
rw(w) = 1
Cw
p(w) exp log p(D, H|w)
rH(H),
(17.18)
rH(H) =
1
CH
exp log p(D, H|w)
rw(w).
(17.19)

17.2 Asymptotic Upper-Bound for VB Free Energy
505
Let us deï¬ne the relative VB free energy
FVB(D) = FVB(D) âˆ’NS N(D)
by the VB free energy and the empirical entropy (17.12). For arbitrary wâˆ—âˆˆ
Wâˆ—, substituting Eq. (17.18) into Eq. (17.16), we have
FVB(D) = min
rH(H)
â¡â¢â¢â¢â¢â£âˆ’log

p(w) exp
/
log p(D, H|w)
rH(H)
0
rH(H)
dw
â¤â¥â¥â¥â¥â¦+ log p(D|wâˆ—)
(17.20)
â‰¤âˆ’log

exp
â§âªâªâ¨âªâªâ©

H
p(H|D, wâˆ—) log p(D, H|w)
p(D, H|wâˆ—)
â«âªâªâ¬âªâªâ­p(w)dw (17.21)
â‰¡FVBâˆ—(D).
Here, we have substituted rH(H) â†p(H|D, wâˆ—) =
p(D,H|wâˆ—)

H p(D,H|wâˆ—) to obtain the
upper-bound (17.21). The expression (17.20) of the free energy corresponds to
viewing the VB learning as a local variational approximation (Section 5.3.3),
where the variational parameter h(Î¾) is the vector consisting of the elements
log p(D, H, Î¾) for all possible H.4
By taking the expectation with respect to the distribution of training
samples, we deï¬ne the average relative VB free energy and its upper-bound as
F
VB(N) =
FVB(D)

p(D|wâˆ—) ,
(17.22)
F
VBâˆ—(N) =
FVBâˆ—(D)

p(D|wâˆ—) .
(17.23)
From Eq. (17.7), we have
F
Bayes
Joint (N) â‰ˆâˆ’log

eâˆ’NE(w)p(w)dw â‰¡FBayes
Joint (N),
where E(w) is deï¬ned by Eq. (17.8). Then, the following theorem holds:
Theorem 17.1
It holds that
F
Bayes(N) â‰¤F
VB(N) â‰¤F
VBâˆ—(N) â‰¤FBayes
Joint (N).
(17.24)
Proof
The left inequality follows from Eq. (17.17). Eq. (17.21) gives
F
VB(N) â‰¤F
VBâˆ—(N)
=
FVBâˆ—(D)

p(D|wâˆ—)
4 The variational parameter h(Î¾) has one-to-one correspondence with p(H|D, Î¾), and is
substituted as h(Î¾) â†h(wâˆ—) in Eq. (17.21).

506
17 Uniï¬ed Theory for Latent Variable Models
= âˆ’
/
log

exp
â§âªâªâ¨âªâªâ©

H
p(H|D, wâˆ—) log p(D, H|w)
p(D, H|wâˆ—)
â«âªâªâ¬âªâªâ­p(w)dw
0
p(D|wâˆ—)
â‰¤âˆ’log

exp
â§âªâªâªâ¨âªâªâªâ©
/
H
p(H|D, wâˆ—) log p(D, H|w)
p(D, H|wâˆ—)
0
p(D|wâˆ—)
â«âªâªâªâ¬âªâªâªâ­p(w)dw
= âˆ’log

eâˆ’NE(w)p(w)dw = FBayes
Joint (N).
The ï¬rst and second equalities are deï¬nitions of F
VBâˆ—(N) and FVBâˆ—(D). We
have applied Jensenâ€™s inequality to the convex function log

exp(Â·)p(w)dw to
obtain the last inequality. Finally, the last equality follows from the fact that
p(D|wâˆ—)p(H|D, wâˆ—) = p(D, H|wâˆ—) and the i.i.d. assumption.
â–¡
The following corollary is immediately obtained from Theorems 13.13
and 17.1:
Corollary 17.2
Let 0 > âˆ’Î»1 > âˆ’Î»2 > Â· Â· Â· be the sequence of the poles
of the zeta function (17.14) in the decreasing order, and m1, m2,. . . be the
corresponding orders of the poles. Then the average relative VB free energy
(17.22) can be asymptotically upper-bounded as
F
VB(N) â‰¤Î»1 log N âˆ’(m1 âˆ’1) log log N + O(1).
(17.25)
It holds in Eqs. (17.13) and (17.15) that Î»â€²Bayes
Joint
= Î»1 and mâ€²Bayes
Joint
= m1 for
Î»1 and m1 deï¬ned in Corollary 17.2. Note that E(w) depends on wâˆ—âˆˆWâˆ—.
For different wâˆ—, we have different values of Î»1, which is determined by the
minimum over different wâˆ—âˆˆWâˆ—in Eq. (17.25). Then m1 is determined by
the maximum of the order of the pole for the minimum Î»1. Also note that unlike
for Bayesian learning, even if the largest pole of the zeta function is obtained,
Eq. (17.25) does not necessarily provide a lower-bound of the VB free energy.
If the joint model p(x, z|w), the true distribution p(x, z|wâˆ—), and the prior
p(w) satisfy the regularity conditions (Section 13.4.1), it holds that
2Î»â€²Bayes
Joint
= D,
where D is the number of parameters.
If the joint model p(x, z|w) is identiï¬able, even though the true parameter
is on the boundary of the parameter space or the prior does not satisfy 0 <
p(w) < âˆ, Î»â€²Bayes
Joint
can be analyzed similarly to the case of regular models. The
GMM with redundant components is an example of such a case, as will be
detailed in the next section.

17.3 Example: Average VB Free Energy of Gaussian Mixture Model
507
If the joint model p(x, z|w) is unidentiï¬able, we need the algebraic geomet-
rical technique to analyze Î»â€²Bayes
Joint
as discussed in Section 13.5.4. This technique
is also applicable to identiï¬able cases as will be demonstrated in a part of the
analysis of Î»â€²Bayes
Joint
for the GMM in the last part of the next section.
17.3 Example: Average VB Free Energy of
Gaussian Mixture Model
In this section, we derive an asymptotic upper-bound of the VB free energy
of GMMs. Although this upper-bound is immediately obtained from Theorem
15.5 in Section 15.2, it was derived by direct evaluation and minimization of
the free energy with respect to the expected sufï¬cient statistics. In this section,
we present another derivation through Theorem 17.1 in order to illustrate how
the general theory described in Section 17.2 is applied.
Let
g(x|Î¼) = GaussM(x; Î¼, IM)
be the M-dimensional uncorrelated Gaussian density and consider the GMM
with K components,
p(x|w) =

z
p(x, z|w) =
K

k=1
Î±kg(x|Î¼k),
where x âˆˆRM and w = (Î±, {Î¼k}K
k=1) denote the parameter vector consisting the
mixing weights and the mean vectors, respectively.
Assuming the same prior given by
p(Î±|Ï†) = DirichletK(Î±; (Ï†,. . . , Ï†)âŠ¤),
(17.26)
p(Î¼k|Î¼0, Î¾) = GaussM(Î¼k|Î¼0, (1/Î¾)IM),
(17.27)
and the same true distribution
q(x) = p(x|wâˆ—) =
K0

k=1
Î±âˆ—
kg(x|Î¼âˆ—
k),
(17.28)
as in Sections 4.1.1 and 15.2, we immediately obtain from the upper-bound in
Eq. (15.36) of Theorem 15.5 that
F
VB(N) â‰¤Î»
â€²VB
MM log N + O(1),
(17.29)

508
17 Uniï¬ed Theory for Latent Variable Models
where
Î»
â€²VB
MM =
â§âªâªâ¨âªâªâ©
(K âˆ’K0)Ï† + MK0+K0âˆ’1
2
(Ï† < M+1
2 ),
MK+Kâˆ’1
2
(Ï† â‰¥M+1
2 ).
In this section, we derive this upper-bound by using Theorem 17.1, which
provides an alternative proof to the one presented in Section 15.2. Similar
techniques were used for analyzing the Bayes free energy (13.19) in the
asymptotic limit (Yamazaki and Watanabe, 2003a,b, 2005). Here, we evaluate
the VB free energy and present the details of the proof for the speciï¬c choice
of the prior distribution.
First, in order to deï¬ne p(x, z|wâˆ—) for z with K elements, we extend and
redeï¬ne the true parameter wâˆ—denoting it as wâˆ—= (Î±âˆ—, {Î¼âˆ—
k}K
k=1). Suppose that
the true distribution with parameter wâˆ—has K nonzero mixing weights. For
example, we can assume that
Î±âˆ—
k =
â§âªâªâªâªâ¨âªâªâªâªâ©
Î±âˆ—
k
(1 â‰¤k â‰¤K0 âˆ’1),
Î±âˆ—
K0/(K âˆ’K0 + 1)
(K0 â‰¤k â‰¤K),
0
(K + 1 â‰¤k â‰¤K),
Î¼âˆ—
k =
) Î¼âˆ—
k
(1 â‰¤k â‰¤K0),
Î¼âˆ—
K0
(K0 + 1 â‰¤k â‰¤K).
Note that the marginal distribution of p(x, z|wâˆ—) is reduced to Eq. (17.28). Then
we have
E(w) =
 
z
p(x, z|wâˆ—) log p(x, z|wâˆ—)
p(x, z|w) dx
=

K

k=1
Î±âˆ—
kg(x|Î¼âˆ—
k) log Î±âˆ—
kg(x|Î¼âˆ—
k)
Î±kg(x|Î¼k)dx
=
K

k=1
Î±âˆ—
k
)
log Î±âˆ—
k
Î±k
+

g(x|Î¼âˆ—
k) log g(x|Î¼âˆ—
k)
g(x|Î¼k)dx
1
=
K

k=1
Î±âˆ—
k
)
log Î±âˆ—
k
Î±k
+ ||Î¼k âˆ’Î¼âˆ—
k||2
2
1
.
Second, we divide the parameter w into three parts,
w1 = (Î±2, Î±3,. . . , Î±K),
(17.30)
w2 = (Î±K+1,. . . , Î±K),
(17.31)
w3 = (Î¼1, Î¼2,. . . , Î¼K),
(17.32)

17.3 Example: Average VB Free Energy of Gaussian Mixture Model
509
and deï¬ne
W1 = {w1; |Î±k âˆ’Î±âˆ—
k| â‰¤Ïµ, 2 â‰¤k â‰¤K},
W2 = {w2; |Î±k| â‰¤Ïµ, K â‰¤k â‰¤K},
W3 = {w3; ||Î¼k âˆ’Î¼âˆ—
k|| â‰¤Ïµ, 1 â‰¤k â‰¤K},
for a sufï¬ciently small constant Ïµ. For an arbitrary parameter w âˆˆW1 Ã— W2 Ã—
W3 â‰¡W(Ïµ), we can decompose E(w) as
E(w) = E1(w1) + E2(w2) + E3(w3),
(17.33)
where
E1(w1) =
K

k=2
Î±âˆ—
k log Î±âˆ—
k
Î±k
+
â›âœâœâœâœâœâœâœâ1 âˆ’
K

k=2
Î±âˆ—
k
ââŸâŸâŸâŸâŸâŸâŸâ log 1 âˆ’K
k=2 Î±âˆ—
k
1 âˆ’K
k=2 Î±k
,
E2(w2) =
1
1 âˆ’c
1 âˆ’K0
k=2 Î±âˆ—
k
1 âˆ’K
k=2 Î±k
K

k=K+1
Î±k,
E3(w3) =
K

k=1
Î±âˆ—
k
2 ||Î¼k âˆ’Î¼âˆ—
k||2.
(17.34)
Here we have used the mean value theorem âˆ’log(1 âˆ’t) =
1
1âˆ’ct for some c,
0 â‰¤c â‰¤t with t =
K
k=K+1 Î±k
1âˆ’K
k=2 Î±k . Furthermore, for w âˆˆW(Ïµ), there exist positive
constants C1, C2, C3, and C4 such that
C1
K

k=2
(Î±k âˆ’Î±âˆ—
k)2 â‰¤E1(w1) â‰¤C2
K

k=2
(Î±k âˆ’Î±âˆ—
k)2,
(17.35)
C3
K

k=K+1
Î±k â‰¤E2(w2) â‰¤C4
K

k=K+1
Î±k.
(17.36)
Third, we evaluate the partial free energies deï¬ned for i = 1, 2, 3 by
Fi = âˆ’log

Wi
exp(âˆ’NEi(wi))p(wi)dwi,
(17.37)
where p(wi) is the product of factors of the prior in Eqs. (17.26) and (17.27),
which involve wi deï¬ned in Eqs. (17.30) through (17.32).
It follows from Eqs. (17.24), (17.33), and (17.37) that
F
VB(N) â‰¤F1 + F2 + F3 + O(1).
(17.38)

510
17 Uniï¬ed Theory for Latent Variable Models
From Eqs. (17.35) and (17.34), as for F1 and F3, the Gaussian integration
yields
F1 =
K âˆ’1
2
log N + O(1),
(17.39)
F3 = MK
2
log N + O(1).
(17.40)
Since
NÏ†
 Ïµ
0
eâˆ’nÎ±kÎ±Ï†âˆ’1
k
dÎ±k â†’Î“(Ï†)
(N â†’âˆ),
for k = K + 1,. . . , K, it follows from Eq. (17.36) that
F2 = (K âˆ’K)Ï† log N + O(1).
(17.41)
Finally, combining Eqs. (17.38) through (17.41), we obtain
F
VB(N) â‰¤
â§âªâªâ¨âªâªâ©(K âˆ’K)Ï† + MK + K âˆ’1
2
â«âªâªâ¬âªâªâ­log N + O(1).
Minimizing the right-hand side of the preceding expression over K (K0 â‰¤K â‰¤
K) leads to the upper-bound in Eq. (17.29).
Alternatively, the preceding evaluations of all the partial free energies,
F1, F2, and F3, are obtained by using the algebraic geometrical method based
on Corollary 17.2. For example, as for F2, the zeta function
Î¶E2(z) =

E2(w2)zp(w2)dw2
has a pole z = âˆ’(K âˆ’K)Ï†. This can be observed by the change of variables, the
so-called blow-up,
Î±k = Î±â€²
kÎ±â€²
K
(k = K + 1,. . . , K âˆ’1),
Î±K = Î±â€²
K,
which yields that Î¶E2 has a term

Î±â€²
K
zÎ±â€²
K
(Kâˆ’K)Ï†âˆ’1Î¶E2(wâ€²
2)dÎ±â€²
K =
Î¶E2(wâ€²
2)
z + (K âˆ’K)Ï†
,
where Î¶E2(wâ€²
2) is a function proportional to
 â›âœâœâœâœâœâœâœâ
Kâˆ’1

k=K+1
Î±â€²
k + 1
ââŸâŸâŸâŸâŸâŸâŸâ 
z
Kâˆ’1

k=K+1
Î±â€²
k
Ï†âˆ’1
Kâˆ’1

k=K+1
dÎ±â€²
k.
Hence, we can see that Î¶E2 has a pole at z = âˆ’(K âˆ’K)Ï†.

17.4 Free Energy and Generalization Error
511
17.4 Free Energy and Generalization Error
In this section, we relate the VB free energy to the generalization performance
of VB learning. We denote a training data set by DN = {x(1), x(2),. . . , x(N)}
with the number N of training samples as a superscript in this section.
Let p(x, z|wâˆ—) be the true distribution of the observed variable x and the
latent variable z, which has the marginal distribution p(x|wâˆ—). We deï¬ne the
generalization error of the predictive distribution for the joint distribution,
pVBâˆ—(x, z|DN) = âŸ¨p(x, z|w)âŸ©râˆ—(w;wâˆ—) =

p(x, z|w)râˆ—(w; wâˆ—)dw,
(17.42)
by the Bayes generalization error (13.133)
GEVBâˆ—
Joint(DN) = KL(p(x, z|wâˆ—)||pVBâˆ—(x, z|DN)),
(17.43)
and the Gibbs generalization error (13.136) by
GGEVBâˆ—
Joint(DN) = KL(p(x, z|wâˆ—)||p(x, z|w))
râˆ—(w;wâˆ—) ,
(17.44)
where
râˆ—(w; wâˆ—) âˆp(w)
N

n=1
exp
â›âœâœâœâœâœââˆ’

z
p(z|x(n), wâˆ—) log p(x(n), z|wâˆ—)
p(x(n), z|w)
ââŸâŸâŸâŸâŸâ 
is the approximate posterior distribution (17.18) with p(H|DN, wâˆ—) substituted
for rH(H). We denote their means by
GE
VBâˆ—
Joint(N) =

GEVBâˆ—
Joint(DN)

p(DN|wâˆ—) ,
GGE
VBâˆ—
Joint(N) =

GGEVBâˆ—
Joint(DN)

p(DN|wâˆ—) .
Then the following theorem holds:
Theorem 17.3
It holds that
GE
VBâˆ—
Joint(N) â‰¤F
VBâˆ—(N + 1) âˆ’F
VBâˆ—(N) â‰¤GGE
VBâˆ—
Joint(N),
(17.45)
where F
VBâˆ—(N) is the upper-bound (17.23) of the average relative VB free
energy.
Proof
We have
FVBâˆ—(DN+1) âˆ’FVBâˆ—(DN)
= âˆ’log
 N+1
n=1 exp
!
z p(z|x(n), wâˆ—) log p(x(n),z|w)
p(x(n),z|wâˆ—)
"
p(w)dw
 N
n=1 exp

z p(z|x(n), wâˆ—) log p(x(n),z|w)
p(x(n),z|wâˆ—)
 
p(w)dw

512
17 Uniï¬ed Theory for Latent Variable Models
= âˆ’log

exp
â›âœâœâœâœâœâ

z
p(z|x(N+1), wâˆ—) log p(x(N+1), z|w)
p(x(N+1), z|wâˆ—)
ââŸâŸâŸâŸâŸâ râˆ—(w; wâˆ—)dw
=

z
p(z|x(N+1), wâˆ—) log p(x(N+1), z|wâˆ—)
âˆ’log

exp
!
log p(x(N+1), z|w)

p(z|x(N+1),wâˆ—)
"
râˆ—(w; wâˆ—)dw
(17.46)
â‰¥

z
p(z|x(N+1), wâˆ—) log
p(x(N+1), z|wâˆ—)
p(x(N+1), z|w)
râˆ—(w;wâˆ—)
.
In the last inequality, we have applied Jensenâ€™s inequality to the convex
function
log

exp(Â·)p(w)dw.
Taking
the
expectation
with
respect
to
N+1
n=1 p(x(n)|wâˆ—) in both sides of the preceding inequality yields the left
inequality in Eq. (17.45).
By applying Jensenâ€™s inequality for the exponential function in Eq. (17.46),
and taking the expectation, we have the right inequality in Eq. (17.45).
â–¡
The inequalities in Eq. (17.45) are analogous to Eq. (17.6). Let Î»â€²VBâˆ—be the
free energy coefï¬cient of F
VBâˆ—(N), i.e.,
F
VBâˆ—(N) = Î»â€²VBâˆ—log N + o(log N).
(17.47)
If its difference has the asymptotic form
F
VBâˆ—(N + 1) âˆ’F
VBâˆ—(N) = Î»â€²VBâˆ—
N
+ o
 1
N

,
the left inequality in Eq. (17.45) suggests that
GE
VBâˆ—
Joint(N) â‰¤Î»â€²VBâˆ—
N
+ o
 1
N

.
This means that the free energy coefï¬cient Î»â€²VBâˆ—of F
VBâˆ—(N) is directly related
to the generalization error of VB learning measured by Eq. (17.43). Theorem
17.1 implies that the free energy coefï¬cients satisfy Î»â€²VBâˆ—â‰¤Î»â€²Bayes
Joint , which in
turn implies from Eq. (17.6) that
GE
VBâˆ—
Joint(N) â‰¤GE
Bayes
Joint (N)
(17.48)
holds asymptotically.
Let
rw(w) = argmin
rw(w)
min
rH(H) F(r)

17.5 Relation to Other Analyses
513
be the optimal VB posterior of the parameter that minimizes the free energy
(17.16). The average generalization errors of VB learning are naturally
deï¬ned by
GE
VB
Joint(N) =

KL(p(x, z|wâˆ—)||pVB(x, z|DN))

q(DN)
for the joint predictive distribution pVB(x, z|DN) = âŸ¨p(x, z|w)âŸ©rw(w), and by
GE
VB(N) =

KL(p(x|wâˆ—)||pVB(x|DN))

q(DN)
for the marginal predictive distribution pVB(x|DN) = âŸ¨p(x|w)âŸ©rw(w). It follows
from the log-sum inequality (17.9) that
GE
VB(N) â‰¤GE
VB
Joint(N).
(17.49)
Since the predictive distribution (17.42) is derived from the approximate
posterior distribution râˆ—(w; wâˆ—) consisting of p(H|DN, wâˆ—) instead of the
minimizerrH(H) of the free energy, it is conjectured that GE
VBâˆ—
Joint(N) provides
a lower-bound to GE
VB
Joint(N). At least, the inequalities in Eq. (17.45) imply
the afï¬nity of the VB free energy and the generalization error measured
by the KL divergence of the joint distributions. The generalization error
of the marginal predictive distribution is generally upper-bounded by that
of the joint predictive distribution as in Eq. (17.49). Although Eq. (17.48)
shows that the average generalization error GE
VBâˆ—
Joint(N) of the approximate
predictive distribution of VB learning with râˆ—(w; wâˆ—) is upper-bounded by that
of Bayesian learning in the joint model, the relationship between GE
VBâˆ—
Joint(N)
and GE
VB
Joint(N) is still unknown.
17.5 Relation to Other Analyses
In this section, we discuss the relationships of the asymptotic formulae in
Sections 17.2 and 17.4 to the analyses of the Bayes free energy and the
generalization error.
17.5.1 Asymptotic Analysis of Free Energy Bounds
Asymptotic upper-bounds of the Bayes free energy were obtained for some
statistical models, including the GMM, HMM, and the Bayesian network
(Yamazaki and Watanabe, 2003a,b, 2005). The upper-bounds are given by the
following form:
F
Bayes(N) â‰¤Î»â€²Bayes
Joint
log N + O(1),
(17.50)

514
17 Uniï¬ed Theory for Latent Variable Models
where the coefï¬cient Î»â€²Bayes
Joint
was identiï¬ed for each model by analyzing the
largest pole of the zeta function Î¶E in Eq. (17.14) instead of Î¶E, by using the
log-sum inequality (17.10) (Yamazaki and Watanabe, 2003a,b, 2005). Since
the largest pole of Î¶E provides a lower-bound for that of Î¶E, their analyses
provided upper-bounds of F
Bayes(N) for the aforementioned models.
On the other hand, the asymptotic forms of the VB free energy were
analyzed also for the same models as discussed in Chapters 15 and 16, each of
which has the following form:
F
VB(N) â‰¤Î»â€²VB log N + O(1).
(17.51)
In most cases, asymptotic upper-bounds of F
Bayes(N) and F
VB(N) coincide,
i.e., Î»â€²Bayes
Joint
= Î»â€²VB holds while Theorem 17.1 implies that Î»â€²VB â‰¤Î»â€²Bayes
Joint .
Hence, it is suggested that this upper-bound is tight in some cases. The
zeta function Î¶E was also analyzed by the algebraic geometrical technique
to evaluate the generalization error for estimating local latent variables
(Yamazaki, 2016).
Moreover, the previous analyses of the VB free energy are based on the
direct minimization of the free energy over the variational parameters (Chap-
ters 14 through 16). Hence, the analyses are highly dependent on the concrete
algorithm for the speciï¬c model and the choice of the prior distribution. In
other words, it is required to parameterize the free energy explicitly by a ï¬nite
number of variational parameters in such analyses. Analyzing the right-hand
side of Eq. (17.24) is more general and is independent of the concrete algorithm
for the speciï¬c model. It does not even require that the prior distribution p(w)
be conjugate since Theorem 17.1 holds for any prior. In such a case, the VB
learning algorithm should be implemented with techniques for nonconjugacy
such as the local variational approximation and the black-box variational
inference (Section 2.1.7). In fact, for mixture models, the upper-bound in
Theorem 15.10 averaged over the training samples can be obtained in more
general cases. The mixture component g(x|Î¼) can be generalized to any regular
models, while in Chapter 15 it was generalized only to the exponential family.
17.5.2 Accuracy of Approximation
For several statistical models, tighter bounds or exact evaluations of the
coefï¬cient Î»â€²Bayes of the relative Bayes free energy in Eq. (17.5) have been
obtained (Aoyagi and Watanabe, 2005; Yamazaki et al., 2010). If the relative
Bayes free energy and VB free energy have the asymptotic forms, F
Bayes(N) =
Î»â€²Bayes log N + o(log N) and F
VB(N) = Î»â€²VB log N + o(log N), respectively,

17.5 Relation to Other Analyses
515
Î»â€²Bayes â‰¤Î»â€²VB holds, and the approximation accuracy of VB learning to
Bayesian learning can be evaluated by the gap between them:
F
VB(N) âˆ’F
Bayes(N) = (Î»â€²VB âˆ’Î»â€²Bayes) log N + o(log N).
From Eq. (17.17), this turns out to be the KL divergence from the approximate
posterior to the true posterior. Such a comparison was ï¬rst conducted for
GMMs (Watanabe and Watanabe, 2004, 2006; Aoyagi and Nagata, 2012).
A more detailed comparison was conducted for the Bernoulli mixture model
discussed in Section 15.4 (Yamazaki and Kaji, 2013; Kaji et al., 2010). Accord-
ing to the authorsâ€™ results, Î»â€²VB can be strictly greater than Î»â€²Bayes, while Î»â€²VB
is not so large as D/2, where D is the number of parameters.5 The arguments
in Section 17.2 imply that such a comparison can be extended to general latent
variable models by examining the difference between minwâˆ—âˆˆWâˆ—F
Bayes
Joint (N) and
F
Bayes(N), which is related to the difference between Î»â€²Bayes
Joint
and Î»â€²Bayes, i.e.,
the poles of Î¶E and Î¶E.
17.5.3 Average Generalization Error
Although the generalization performance of the VB learning was fully
analyzed in the RRR model as discussed in Chapter 14, little has been known
in other models. In Section 17.4, we derived an inequality that implies the
relationship between the generalization error and the VB free energy for
general latent variable models.
In the exact Bayesian learning, the universal relations (13.138) and (13.139)
among the quartet, Bayes and Gibbs generalization losses and Bayes and Gibbs
training losses, were proved as discussed in Section 13.5.5 (Watanabe, 2009). It
is an important future work to explore such relationships among the quantities
introduced in Section 17.4 for VB learning.
5 For the local latent variable model deï¬ned in Section 17.1, Corollary 17.2 combined with
Eq. (13.125) implies that
2Î»â€²VB â‰¤D.
However, this is not true in general as we discussed for the RRR model in Chapter 14 (see
Figure 14.8).

Appendix A
Jamesâ€“Stein Estimator
The Jamesâ€“Stein (JS) estimator (James and Stein, 1961), a shrinkage estimator known
to dominate the maximum likelihood (ML) estimator, has close relation to Bayesian
learning. More speciï¬cally, it can be derived as an empirical Bayesian (EBayes)
estimator (Efron and Morris, 1973).
Consider an M-dimensional Gaussian model with a Gaussian prior for the mean
parameter:
p(x|Î¼) = GaussM(x; Î¼, Ïƒ2IM) =

2Ï€Ïƒ2 âˆ’M/2 exp

âˆ’âˆ¥x âˆ’Î¼âˆ¥2
2Ïƒ2

,
(A.1)
p(Î¼|c2) = GaussM(Î¼; 0, c2IM) =

2Ï€c2 âˆ’M/2 exp

âˆ’âˆ¥Î¼âˆ¥2
2c2

,
(A.2)
where the variance Ïƒ2 of observation noise is assumed to be known. We perform
empirical Bayesian learning to estimate the mean parameter Î¼ and the prior variance
c2 from observed samples D = {x(1),. . . , x(N)}. The joint distribution conditional to the
hyperparameter is
p(D, Î¼|c2) = p(Î¼|c2)
N

n=1
p(x(n)|Î¼)
=
1
(2Ï€c2)M/2(2Ï€Ïƒ2)NM/2 exp
â›âœâœâœâœâœâœââˆ’âˆ¥Î¼âˆ¥2
2c2 âˆ’
N

n=1
###x(n) âˆ’Î¼
###
2
2Ïƒ2
ââŸâŸâŸâŸâŸâŸâ 
=
1
(2Ï€c2)M/2(2Ï€Ïƒ2)NM/2 exp
â›âœâœâœâœâœâœââˆ’1
2Ïƒ2
N

n=1
###x(n)###
2 +
N2 ###x
###
2
2Ïƒ2(N + Ïƒ2/c2)
ââŸâŸâŸâŸâŸâŸâ 
Â· exp

âˆ’N + Ïƒ2/c2
2Ïƒ2
#####Î¼ âˆ’
Nx
N + Ïƒ2/c2
#####
2
,
which implies that the posterior is Gaussian,
p(Î¼|D, c2) âˆp(D, Î¼|c2) âˆexp

âˆ’N + Ïƒ2/c2
2Ïƒ2
#####Î¼ âˆ’
Nx
N + Ïƒ2/c2
#####
2
,
516

A Jamesâ€“Stein Estimator
517
with the mean given by
Î¼ =
Nx
N + Ïƒ2/c2 =

1 âˆ’
Ïƒ2
Nc2 + Ïƒ2

x.
(A.3)
The marginal likelihood is computed as
p(D|c2) =

p(D, Î¼|c2)dÎ¼
=
1
(2Ï€c2)M/2(2Ï€Ïƒ2)NM/2 exp
â›âœâœâœâœâœâœââˆ’1
2Ïƒ2
N

n=1
###x(n)###
2 +
N2 ###x
###
2
2Ïƒ2(N + Ïƒ2/c2)
ââŸâŸâŸâŸâŸâŸâ 
Â·

exp

âˆ’N + Ïƒ2/c2
2Ïƒ2
#####Î¼ âˆ’
Nx
N + Ïƒ2/c2
#####
2
dÎ¼
=
exp

âˆ’1
2Ïƒ2
N
n=1
###x(n)###
2 +
N2âˆ¥xâˆ¥
2
2Ïƒ2(N+Ïƒ2/c2)

(2Ï€c2)M/2(2Ï€Ïƒ2)(Nâˆ’1)M/2(N + Ïƒ2/c2)M/2
=
exp

âˆ’1
2Ïƒ2
N
n=1
###x(n) âˆ’x
###
2 âˆ’
Nâˆ¥xâˆ¥
2
2Ïƒ2
+
N2âˆ¥xâˆ¥
2
2Ïƒ2(N+Ïƒ2/c2)

(2Ï€)M/2(2Ï€Ïƒ2)(Nâˆ’1)M/2(Nc2 + Ïƒ2)M/2
=
exp

âˆ’1
2Ïƒ2
N
n=1
###x(n) âˆ’x
###
2 âˆ’
NÏƒ2/c2âˆ¥xâˆ¥
2
2Ïƒ2(N+Ïƒ2/c2)

(2Ï€)M/2(2Ï€Ïƒ2)(Nâˆ’1)M/2(Nc2 + Ïƒ2)M/2
=
exp

âˆ’1
2Ïƒ2
N
n=1
###x(n) âˆ’x
###
2 âˆ’
Nâˆ¥xâˆ¥
2
2(Nc2+Ïƒ2)

(2Ï€)M/2(2Ï€Ïƒ2)(Nâˆ’1)M/2(Nc2 + Ïƒ2)M/2
=
exp
!
âˆ’1
2Ïƒ2
N
n=1
###x(n) âˆ’x
###
2"
(2Ï€Ïƒ2)(Nâˆ’1)M/2
Â·
exp

âˆ’
Nâˆ¥xâˆ¥
2
2(Nc2+Ïƒ2)

$2Ï€(Nc2 + Ïƒ2)%M/2 .
(A.4)
This implies that v = x
C
N/(Nc2 + Ïƒ2) is a random variable subject to GaussM
(v; 0, IM). Since its (â€“2)nd order moment is equal to

âˆ¥vâˆ¥âˆ’2
GaussM(v;0,IM) = (M âˆ’2)âˆ’1,
we have
/ Nc2 + Ïƒ2
N
###x
###
2
0
p(D|c2)
=
1
M âˆ’2,
and therefore
/ M âˆ’2
N
###x
###
2
0
p(D|c2)
=
1
Nc2 + Ïƒ2 .
Accordingly, (M âˆ’2)/N
###x
###
2 is an unbiased estimator of the factor (Nc2 + Ïƒ2)âˆ’1.

518
A Jamesâ€“Stein Estimator
Replacing the factor (Nc2 + Ïƒ2)âˆ’1 in Eq. (A.3) with its unbiased estimator (M âˆ’2)/
N
###x
###
2, we obtain the JS estimator (with degree M âˆ’2):
Î¼JS =
â›âœâœâœâœâœâœâ1 âˆ’(M âˆ’2)Ïƒ2
N
###x
###
2
ââŸâŸâŸâŸâŸâŸâ x.
(A.5)
If we estimate c2 by maximizing the marginal likelihood (A.4), we obtain the positive-
part JS estimator (with degree M):
Î¼PJS = max
â›âœâœâœâœâœâœâ0, 1 âˆ’MÏƒ2
N
###x
###
2
ââŸâŸâŸâŸâŸâŸâ x.
(A.6)
The JS estimator has an interesting property. Let us ï¬rst introduce terminology.
Assume that we observed data D generated from a distribution p(D|w) with unknown
parameter w. Consider two estimators w1 = w1(D) and w2 = w2(D), and measure some
error criterion E(w, wâˆ—) from the true parameter value wâˆ—.
Deï¬nition A.1
(Domination) We say that the estimator w1 dominates the other
estimator w2 if
E(w1(D), wâˆ—)
p(D|wâˆ—) â‰¤E(w2(D), wâˆ—)
p(D|wâˆ—)
for arbitrary wâˆ—,
and
E(w1(D), wâˆ—)
p(D|wâˆ—) < E(w2(D), wâˆ—)
p(D|wâˆ—)
for a certain wâˆ—.
Deï¬nition A.2
(Efï¬ciency) We say that an estimator is efï¬cient if no unbiased
estimator dominates it.
Deï¬nition A.3
(Admissibility) We say that an estimator is admissible if no estimator
dominates it.
Figure A.1 Generalization error of Jamesâ€“Stein estimator.

A Jamesâ€“Stein Estimator
519
Assume that p(D|w) = GaussM(x; Î¼, Ïƒ2IM). Then the ML estimator,
Î¼ML = x,
(A.7)
is known to be efï¬cient in terms of the mean squared error
E(Î¼, Î¼âˆ—) =
###Î¼ âˆ’Î¼âˆ—###
2 .
However, the ML estimator was proven to be inadmissible when M â‰¥3, i.e., there
exists at least one biased estimator that dominates the ML estimator (Stein, 1956).
Subsequently, the JS estimator (A.5) was introduced as an estimator dominating the
ML estimator (James and Stein, 1961).
Figure A.1 shows the normalized squared loss Nâˆ¥Î¼ âˆ’Î¼âˆ—âˆ¥2/(MÏƒ2) of the ML
estimator (A.7) and the JS estimator (A.5) as a function of a scaled true mean
âˆš
N âˆ¥Î¼âˆ—âˆ¥/Ïƒ. The ML estimator always gives error equal to one, while the JS estimator
gives error dependent on the true value. We can see that the JS estimator dominates the
ML estimator for M â‰¥3. We can easily show that the positive-part JS estimator (A.6)
dominates the JS estimator with the same degree.

Appendix B
Metric in Parameter Space
In this appendix, we give a brief summary of the Kullbackâ€“Leibler (KL) divergence,
the Fisher information, and the Jeffreys prior. The KL divergence is a common (pseudo-
)distance measure between distributions, and the corresponding metric in the parameter
space is given by the Fisher information. The Jeffreys priorâ€”the uniform prior when
the distance between distributions is measured by the KL divergenceâ€”is deï¬ned so as
to reï¬‚ect the nonuniformity of the density of the volume element in the parameter space.
B.1 Kullbackâ€“Leibler (KL) Divergence
The KL divergence between two distributions, q(x) and p(x), is deï¬ned as
KL (q(x)âˆ¥p(x)) =

q(x) log
 q(x)
p(x)

dx
=

q(x) log
1
p(x)dx âˆ’

q(x) log
1
q(x)dx
â‰¥0.
If q(x) is the true distribution, i.e., x âˆ¼q(x), the ï¬rst term is the average information
gain for the one who has (possibly) wrong information (who believes x âˆ¼p(x)), and
the second term is the average information gain, i.e., the entropy, for the one who has
the correct information (who believes x âˆ¼q(x)). The KL divergence is not a proper
distance metric, since it is not symmetric, i.e., for general q(x) and p(x),
KL (q(x)âˆ¥p(x))  KL (p(x)âˆ¥q(x)) .
B.2 Fisher Information
The Fisher information of a parametric distribution p(x|w) with its parameter w âˆˆRD
is deï¬ned as
SD
+ âˆ‹F =
 âˆ‚log p(x|w)
âˆ‚w
âˆ‚log p(x|w)
âˆ‚w
âŠ¤
p(x|w)dx,
(B.1)
520

B.3 Metric and Volume Element
521
where âˆ‚log p(x|w)
âˆ‚w
âˆˆRD is the gradient (column) vector of log p(x|w). Under the regularity
conditions (see Section 13.4.1) on the statistical model p(x|w), the Fisher information
can be written as
F = âˆ’
 âˆ‚2 log p(x|w)
âˆ‚wâˆ‚wâŠ¤
p(x|w)dx,
(B.2)
where
âˆ‚2 log p(x|w)
âˆ‚wâˆ‚wâŠ¤

i, j
= âˆ‚2 log p(x|w)
âˆ‚wiâˆ‚w j
.
This is because
âˆ’
 âˆ‚2 log p(x|w)
âˆ‚wiâˆ‚w j
p(x|w)dx
= âˆ’

âˆ‚
âˆ‚w j
â›âœâœâœâœâœâœâ
âˆ‚p(x|w)
âˆ‚wi
p(x|w)
ââŸâŸâŸâŸâŸâŸâ p(x|w)dx
= âˆ’
 â›âœâœâœâœâœâœâœâœâ
âˆ‚2 p(x|w)
âˆ‚wiâˆ‚wj
p(x|w) âˆ’
âˆ‚p(x|w)
âˆ‚wi
âˆ‚p(x|w)
âˆ‚wj
p2(x|w)
ââŸâŸâŸâŸâŸâŸâŸâŸâ p(x|w)dx
= âˆ’
 âˆ‚2p(x|w)
âˆ‚wiâˆ‚w j
dx +

âˆ‚p(x|w)
âˆ‚wi
âˆ‚p(x|w)
âˆ‚wj
p2(x|w)
p(x|w)dx
= âˆ’
âˆ‚2
âˆ‚wiâˆ‚w j

p(x|w)dx +
 âˆ‚log p(x|w)
âˆ‚wi
âˆ‚log p(x|w)
âˆ‚w j
p(x|w)dx
=
 âˆ‚log p(x|w)
âˆ‚wi
âˆ‚log p(x|w)
âˆ‚w j
p(x|w)dx.
B.3 Metric and Volume Element
For a small perturbation Î”w of the parameter, the KL divergence between p(x|w) and
p(x|w + Î”w) can be written as
KL (p(x|w)âˆ¥p(x|w + Î”w)) =

p(x|w) log

p(x|w)
p(x|w + Î”w)

dx
=

p(x|w) log
â›âœâœâœâœâœâœâ
p(x|w)
p(x|w) + âˆ‚p(x|w)
âˆ‚w
âŠ¤Î”w + 1
2Î”wâŠ¤âˆ‚2 p(x|w)
âˆ‚wâˆ‚w Î”w + O(âˆ¥Î”wâˆ¥3)
ââŸâŸâŸâŸâŸâŸâ dx
= âˆ’

p(x|w) log
â›âœâœâœâœâœâœâ1 +
âˆ‚p(x|w)
âˆ‚w
âŠ¤Î”w
p(x|w)
+ 1
2Î”wâŠ¤
âˆ‚2 p(x|w)
âˆ‚wâˆ‚w
p(x|w) Î”w + O(âˆ¥Î”wâˆ¥3)
ââŸâŸâŸâŸâŸâŸâ dx
= âˆ’

p(x|w)
â›âœâœâœâœâœâœâ
âˆ‚p(x|w)
âˆ‚w
âŠ¤
p(x|w) Î”w + 1
2Î”wâŠ¤
â›âœâœâœâœâœâœâ
âˆ‚2 p(x|w)
âˆ‚wâˆ‚w
p(x|w) âˆ’
âˆ‚p(x|w)
âˆ‚w
âˆ‚p(x|w)
âˆ‚w
âŠ¤
p2(x|w)
ââŸâŸâŸâŸâŸâŸâ Î”w
ââŸâŸâŸâŸâŸâŸâ dx
+ O(âˆ¥Î”wâˆ¥3)

522
B Metric in Parameter Space
= âˆ’
 âˆ‚
âˆ‚w

p(x|w)dx
âŠ¤
Î”w âˆ’1
2Î”wâŠ¤

âˆ‚2
âˆ‚wâˆ‚w

p(x|w)dx

Î”w
+ 1
2Î”wâŠ¤
 âˆ‚log p(x|w)
âˆ‚w
âˆ‚log p(x|w)
âˆ‚w
âŠ¤
p(x|w)dx

Î”w + O(âˆ¥Î”wâˆ¥3)
= 1
2Î”wâŠ¤FÎ”w + O(âˆ¥Î”wâˆ¥3).
Therefore, the Fisher information corresponds to the metric of the space of distributions
when the distance is measured by the KL divergence (Jeffreys, 1946).
When we adopt the Fisher information as the metric, the volume element for
integrating functions is given by
dV =
1âˆš
2
C
det (F)dw,
(B.3)
where
1âˆš
2
âˆšdet (F) corresponds to the density.
B.4 Jeffreys Prior
The prior,
p(w) âˆ
C
det (F),
(B.4)
proportional to the density of the volume element (B.3), is called the Jeffreys prior
(Jeffreys, 1946). The Jeffreys prior assigns the equal probability to the unit volume
element at any point in the parameter space, i.e., it is the uniform prior in the distribution
space. Since the uniformity is deï¬ned not in the parameter space but in the distribution
space, the Jeffreys prior is invariant under parameter transformation. Accordingly, the
Jeffreys prior is said to be the parameterization invariant noninformative prior.
For singular models, the Fisher information can have zero eigenvalues, which makes
the Jeffreys prior zero. In some models, including the matrix factorization model, zero
eigenvalues appear everywhere in the parameter space (see Example B.2). In such cases,
we ignore the common zero eigenvalues and redeï¬ne the (generalized) Jeffrey prior by
p(w) âˆ
.D
d=1 Î»d,
(B.5)
where Î»d is the dth largest eigenvalue of the Fisher information F, and D is the
maximum number of positive eigenvalues over the whole parameter space.
Example B.1
(Jeffreys prior for one-dimensional Gaussian distribution) The Fisher
information of the Gaussian distribution,
p(x|Î¼, Ïƒ2) = Gauss1(x; Î¼, Ïƒ2) =

2Ï€Ïƒ2 âˆ’1/2 exp

âˆ’(x âˆ’Î¼)2
2Ïƒ2

,
is calculated as follows. The derivatives of the log likelihood are
âˆ‚log p(x|Î¼, Ïƒ2)
âˆ‚Î¼
= âˆ‚
âˆ‚Î¼

âˆ’(x âˆ’Î¼)2
2Ïƒ2

= x âˆ’Î¼
Ïƒ2 ,

B.4 Jeffreys Prior
523
âˆ‚log p(x|Î¼, Ïƒ2)
âˆ‚Ïƒ2
=
âˆ‚
âˆ‚Ïƒ2

âˆ’1
2 log Ïƒ2 âˆ’(x âˆ’Î¼)2
2Ïƒ2

= âˆ’1
2Ïƒ2 + (x âˆ’Î¼)2
2Ïƒ4
,
âˆ‚2 log p(x|Î¼, Ïƒ2)
âˆ‚Î¼2
= âˆ’1
Ïƒ2 ,
âˆ‚2 log p(x|Î¼, Ïƒ2)
âˆ‚Î¼âˆ‚Ïƒ2
= âˆ’x âˆ’Î¼
Ïƒ4 ,
âˆ‚2 log p(x|Î¼, Ïƒ2)
âˆ‚(Ïƒ2)2
=
1
2Ïƒ4 âˆ’(x âˆ’Î¼)2
Ïƒ6
,
and therefore
F =
/ â›âœâœâœâœâœâ
1
Ïƒ2
xâˆ’Î¼
Ïƒ4
xâˆ’Î¼
Ïƒ4
(xâˆ’Î¼)2
Ïƒ6
âˆ’
1
2Ïƒ4
ââŸâŸâŸâŸâŸâ 
0
p(x|Î¼,Ïƒ2)
=
 1
Ïƒ2
0
0
1
Ïƒ4 âˆ’
1
2Ïƒ4

=
 1
Ïƒ2
0
0
1
2Ïƒ4

.
Thus, the Jeffreys priors for p(x|Î¼), p(x|Ïƒ2), and p(x|Î¼, Ïƒ2) are
p(Î¼) âˆ
C
FÎ¼,Î¼ âˆ1,
p(Ïƒ2) âˆ
.
FÏƒ2,Ïƒ2 âˆ1
Ïƒ2 ,
p(Î¼, Ïƒ2) âˆ
C
det (F) âˆ1
Ïƒ3 ,
respectively.
Example B.2
(Jeffreys prior for one-dimensional matrix factorization model) The
Fisher information of the one-dimensional matrix factorization (MF) model,
p(V|A, B) = Gauss1(V; BA, Ïƒ2) =

2Ï€Ïƒ2 âˆ’1/2 exp

âˆ’(V âˆ’BA)2
2Ïƒ2

,
(B.6)
is calculated as follows. The derivatives of the log likelihood are
âˆ‚log p(V|A, B)
âˆ‚A
= Ïƒâˆ’2(V âˆ’BA)B,
âˆ‚log p(V|A, B)
âˆ‚B
= Ïƒâˆ’2(V âˆ’BA)A,
and therefore
F = 1
Ïƒ4
/(V âˆ’BA)2B2
(V âˆ’BA)2BA
(V âˆ’BA)2BA
(V âˆ’BA)2A2
0
Gauss1(V;BA,Ïƒ2)
= 1
Ïƒ2
 B2
BA
BA
A2

.

524
B Metric in Parameter Space
The Fisher information F has eigenvalues Î»1 = Ïƒâˆ’2(A2 + B2) and Î»2 = 0, since
det

Ïƒ2F âˆ’Î»I2
 
= det
B2 âˆ’Î»
BA
BA
A2 âˆ’Î»

= (B2 âˆ’Î»)(A2 âˆ’Î») âˆ’B2A2
= Î»2 âˆ’(A2 + B2)Î»
=

Î» âˆ’(A2 + B2)
 
Î».
The common (over the whole parameter space) zero eigenvalue comes from the
invariance of the MF model under the transformation (A, B) â†’(sA, sâˆ’1B) for any
s  0. By adopting the generalized deï¬nition (B.5) of the Jeffreys prior, the distribution
proportional to
p(A, B) âˆ
âˆš
A2 + B2
(B.7)
is the parameterization invariant noninformative prior.
The Jeffreys prior is often improper, i.e., the integral of the unnormalized prior
over the parameter domain diverges, and therefore the normalization factor cannot be
computed, as in Examples B.1 and B.2.

Appendix C
Detailed Description
of Overlap Method
Let V âˆˆRLÃ—M be the observed matrix, where L and M correspond to the dimensionality
D of the observation space and the number N of samples as follows:
L = D, M = N
if
D â‰¤N,
L = N, M = D
if
D > N.
(C.1)
Let
V =
L

h=1
Î³hÏ‰bhÏ‰âŠ¤
ah
(C.2)
be the singular value decomposition (SVD) of V. The overlap (OL) method (Hoyle,
2008) computes the following approximation to the negative logarithm of the marginal
likelihood (8.90) over the hypothetical model rank H = 1,. . . , L:1
2FOL(H) â‰ˆâˆ’2 log p(V)
= (LM âˆ’H(L âˆ’H âˆ’2)) log(2Ï€) + L log Ï€ âˆ’2 H
h=1 log
 Î“((Mâˆ’h+1)/2)
Î“(Mâˆ’Lâˆ’h+1)/2)
 
+ H(M âˆ’L) $1 âˆ’log (M âˆ’L)% + H
h=1
L
l=H+1 log

Î³2
h âˆ’Î³2
l
 
+ (M âˆ’L) H
h=1 log Î³2
h + (M âˆ’H) H
h=1 log
!
1
Ïƒ2 OL âˆ’
1
Î»OL
h
"
âˆ’H
h=1
!
1
Ïƒ2 OL âˆ’
1
Î»OL
h
"
Î³2
h + (L + 2)
H
h=1 logÎ»OL
h
+ (M âˆ’H) log Ïƒ2 OL 
+ L
l=1
Î³2
l
Ïƒ2 OL ,
(C.3)
where Î“(Â·) denotes the Gamma function, and {Î»OL
h }H
h=1 and Ïƒ2 OL are estimators for
{Î»h = b2
h + Ïƒ2}H
h=1 and Ïƒ2, respectively, computed by iterating the following updates
until convergence:
1 Our description is slightly different from Hoyle (2008), because the MF model (6.1) does not
have the mean parameter shared over the samples.
525

526
C Detailed Description of Overlap Method
Algorithm 23 Overlap method.
1: Prepare the observed matrix V âˆˆRLÃ—M, following the rule (C.1).
2: Compute the SVD (C.2) of V.
3: Compute FOL(0) by Eq. (C.6).
4: for H = 1 to L do
5:
Initialize the noise variance to Ïƒ2 OL = 10âˆ’4 Â· L
h=1 Î³2
h/(LM).
6:
Iterate Eq. (C.4) for h = 1,. . . , H, and Eq. (C.5) until convergence or any Î»OL
h
becomes a complex number.
7:
Compute FOL(H) by Eq. (C.3) if all {Î»OL
h }H
h=1 are real numbers. Otherwise, set
FOL(H) = âˆ.
8: end for
9: Estimate the rank by 
HOL = minHâˆˆ{0,...,L} FOL(H).
Î»OL
h
=
Î³2
h
2(L+2)

1 âˆ’(Mâˆ’Hâˆ’(L+2))Ïƒ2 OL
Î³2
h
+
B!
1 âˆ’(Mâˆ’Hâˆ’(L+2))Ïƒ2 OL
Î³2
h
"2
âˆ’4(L+2)Ïƒ2 OL
Î³2
h

,
(C.4)
Ïƒ2 OL =
1
(Mâˆ’H)
!L
l=1
Î³2
l
L âˆ’H
h=1Î»OL
h
"
.
(C.5)
When iterating Eqs. (C.4) and (C.5), Î»OL
h
can become a complex number. In such a
case, the hypothetical H is rejected. Otherwise, Eq. (C.3) is evaluated after convergence.
For the null hypothesis, i.e., H = 0, the negative log likelihood is given by
2FOL(0) = âˆ’2 log p(V) = LM

log

2Ï€
LM
L
l=1 Î³2
l
 
+ 1
 
.
(C.6)
The estimated rank 
HOL is the minimizer of FOL(H) over H = 0,. . . , L.
Algorithm 23 summarizes the procedure.

Appendix D
Optimality of Bayesian Learning
Bayesian learning is deduced from the basic probability theory, and therefore it is
optimal in terms of generalization performance under the assumption that the model
and the prior are set reasonably.
Consider a distribution of problems where the true distribution is written as q(x) =
p(x|wâˆ—) with the true parameter wâˆ—subject to q(wâˆ—). Although we usually omit the
dependency description on the true distribution or the true parameter, the average
generalization error, Eq. (13.13), naturally depends on the true distribution, so we here
denote the dependence explicitly as GE(N; wâˆ—). Let
GE(N) =

GE(N; wâˆ—)

q(wâˆ—)
(D.1)
be the average of the average generalization error over the distribution q(wâˆ—) of the true
parameter.
Theorem D.1
If we know the distribution q(wâˆ—) of the true parameter and use it as
the prior distribution, i.e., p(w) = q(w), then Bayesian learning minimizes the average
generalization error (D.1) over q(wâˆ—), i.e.,
GE
Bayes
(N) â‰¤GE
Other
(N),
(D.2)
where GE
Other
(N) denotes the average generalization error of any (other) learning
algorithm.
Proof
Let XN = (x(1),. . . , x(N)) be the N training samples. Regarding the new test
sample as the (N + 1)th sample, we can write the Bayes predictive distribution as
follows:
pBayes(x(N+1)|XN) =

p(x(N+1)|w)p(w|XN)dw
=

p(w) N+1
n=1 p(x(n)|w)dw

p(wâ€²) N
n=1 p(x(n)|wâ€²)dwâ€²
= p(XN+1)
p(XN) ,
(D.3)
527

528
D Optimality of Bayesian Learning
where p(XN) =

p(w)p(XN|w)dw is the marginal likelihood. The average generaliza-
tion error (D.1) of a learning algorithm with its predictive distribution r(x) is given by
GE(N) =
/
log p(x(N+1)|wâˆ—)
r(x(N+1))
0
p(XN+1|wâˆ—)q(wâˆ—)
= âˆ’
 
p(XN+1|wâˆ—)q(wâˆ—)dwâˆ—

log r(x(N+1))dXN+1 âˆ’(N + 1)S ,
= âˆ’

q(XN+1) log r(x(N+1))dXN+1 âˆ’(N + 1)S ,
(D.4)
where
q(XN) =

p(XN|wâˆ—)q(wâˆ—)dwâˆ—
is the marginal likelihood with the true prior distribution q(wâˆ—), and
S = âˆ’log p(x|wâˆ—)
p(x|wâˆ—)q(wâˆ—)
is the entropy, which does not depend on the predictive distribution r(x). Eq. (D.4) can
be written as
GE(N) = âˆ’

q(XN)q(XN+1)
q(XN) log r(x(N+1))dXN+1 âˆ’(N + 1)S
= âˆ’
/ q(XN+1)
q(XN) log r(x(N+1))dx(N+1)
0
q(XN)
âˆ’(N + 1)S
=
/ q(XN+1)
q(XN) log
q(XN+1)
q(XN)
r(x(N+1))dx(N+1)
0
q(XN)
+ const.
(D.5)
Since the ï¬rst term is the KL divergence between q(XN+1)/q(XN) and r(x(N+1)),
Eq. (D.5) is minimized when
r(x(N+1)) = q(XN+1)
q(XN) = qBayes(x(N+1)|XN),
(D.6)
where qBayes(x(N+1)|XN) is the Bayes predictive distribution (D.3) with the prior
distribution set to the distribution of the true parameter, i.e., p(w) = q(w). Thus, we
have proved that no other learning method can give better generalization error than
Bayesian learning with the true prior.
â–¡
A remark is that, since we usually do not know the true distribution and the true
prior (the distribution of the true parameter), it is not surprising that an approximation
method, e.g., variational Bayesian learning, to Bayesian learning provides better
generalization performance than Bayesian learning with a nontrue prior in some
situations.

Bibliography
Akaho, S., and Kappen, H. J. 2000. Nonmonotonic Generalization Bias of Gaussian
Mixture Models. Neural Computation, 12, 1411â€“1427.
Akaike, H. 1974. A New Look at Statistical Model. IEEE Transactions on Automatic
Control, 19(6), 716â€“723.
Akaike, H. 1980. Likelihood and Bayes Procedure. Pages 143â€“166 of: Bernald, J. M.
(ed.), Bayesian Statistics. Valencia, Italy: University Press.
Alzer, H. 1997. On Some Inequalities for the Gamma and Psi Functions. Mathematics
of Computation, 66(217), 373â€“389.
Amari, S., Park, H., and Ozeki, T. 2002. Geometrical Singularities in the Neuroman-
ifold of Multilayer Perceptrons. Pages 343â€“350 of: Advances in NIPS, vol. 14.
Cambridge, MA: MIT Press.
Aoyagi, M., and Nagata, K. 2012. Learning Coefï¬cient of Generalization Error in
Bayesian Estimation and Vandermonde Matrix-Type Singularity. Neural Compu-
tation, 24(6), 1569â€“1610.
Aoyagi, M., and Watanabe, S. 2005. Stochastic Complexities of Reduced Rank
Regression in Bayesian Estimation. Neural Networks, 18(7), 924â€“933.
Asuncion, A., and Newman, D.J. 2007. UCI Machine Learning Repository.
www.ics.uci.edu/âˆ¼mlearn/MLRepository.html
Asuncion, A., Welling, M., Smyth, P., and Teh, Y. W. 2009. On Smoothing and
Inference for Topic Models. Pages 27â€“34 of: Proceedings of UAI. Stockholm,
Sweden: Morgan Kaufmann Publishers Inc.
Attias, H. 1999. Inferring Parameters and Structure of Latent Variable Models by
Variational Bayes. Pages 21â€“30 of: Proceedings of UAI. Stockholm, Sweden:
Morgan Kaufmann Publishers Inc.
Babacan, S. D., Nakajima, S., and Do, M. N. 2012a. Probabilistic Low-Rank Subspace
Clustering. Pages 2753â€“2761 of: Advances in Neural Information Processing
Systems 25. Lake Tahoe, NV: NIPS Foundation.
Babacan, S. D., Luessi, M., Molina, R., and Katsaggelos, A. K. 2012b. Sparse
Bayesian Methods for Low-Rank Matrix Estimation. IEEE Transactions on Signal
Processing, 60(8), 3964â€“3977.
Baik, J., and Silverstein, J. W. 2006. Eigenvalues of Large Sample Covariance Matrices
of Spiked Population Models. Journal of Multivariate Analysis, 97(6), 1382â€“1408.
529

530
Bibliography
Baldi, P. F., and Hornik, K. 1995. Learning in Linear Neural Networks: A Survey. IEEE
Transactions on Neural Networks, 6(4), 837â€“858.
Banerjee, A., Merugu, S., Dhillon, I. S., and Ghosh, J. 2005. Clustering with Bregman
Divergences. Journal of Machine Learning Research, 6, 1705â€“1749.
Beal, M. J. 2003. Variational Algorithms for Approximate Bayesian Inference. PhD
thesis, University College London.
Bicego, M., Lovato, P., Ferrarini, A., and Delledonne, M. 2010. Biclustering of Expres-
sion Microarray Data with Topic Models. Pages 2728â€“2731 of: Proceedings of
ICPR. Istanbul, Turkey: ICPR.
Bickel, P., and Chernoff, H. 1993. Asymptotic Distribution of the Likelihood Ratio
Statistic in a Prototypical Non Regular Problem. New Delhi, India: Wiley Eastern
Limited.
Bishop, C. M. 1999a. Bayesian Principal Components. Pages 382â€“388 of: Advances in
NIPS, vol. 11. Denver, CO: NIPS Foundation.
Bishop, C. M. 1999b. Variational Principal Components. Pages 514â€“509 of: Proceed-
ings of International Conference on Artiï¬cial Neural Networks, vol. 1. Edinburgh,
UK: Computing and Control Engineering Journal.
Bishop, C. M. 2006. Pattern Recognition and Machine Learning. New York: Springer.
Bishop, C. M., and Tipping, M. E. 2000. Variational Relevance Vector Machines.
Pages 46â€“53 of: Proceedings of the Sixteenth Conference Annual Conference
on Uncertainty in Artiï¬cial Intelligence. Stanford, CA: Morgan Kaufmann
Publishers Inc.
Blei, D. M., and Jordan, M. I. 2005. Variational Inference for Dirichlet Process
Mixtures. Bayesian Analysis, 1, 121â€“144.
Blei, D. M., Ng, A. Y., and Jordan, M. I. 2003. Latent Dirichlet Allocation. Journal of
Machine Learning Research, 3, 993â€“1022.
Bouchaud, J. P., and Potters, M. 2003. Theory of Financial Risk and Derivative
Pricingâ€”From Statistical Physics to Risk Management, 2nd edn. Cambridge, UK:
University Press.
Brown, L. D. 1986. Fundamentals of Statistical Exponential Families. IMS Lecture
Notesâ€“Monograph Series 9. Beachwood, OH: Institute of Mathematical Statistics.
Cand`es, E. J., Li, X., Ma, Y., and Wright, J. 2011. Robust Principal Component
Analysis? Journal of the ACM, 58(3), 1â€“37.
Carroll, J. D., and Chang, J. J. 1970. Analysis of Individual Differences in Multidimen-
sional Scaling via an N-way Generalization of â€œEckartâ€“Youngâ€ Decomposition.
Psychometrika, 35, 283â€“319.
Chen, X., Hu, X., Shen, X., and Rosen, G. 2010. Probabilistic Topic Modeling
for Genomic Data Interpretation. Pages 149â€“152 of: 2010 IEEE International
Conference on Bioinformatics and Biomedicine (BIBM).
Chib, S. 1995. Marginal Likelihood from the Gibbs Output. Journal of the American
Statistical Association, 90(432), 1313â€“1321.
Chu, W., and Ghahramani, Z. 2009. Probabilistic Models for Incomplete Multi-
dimensional Arrays. Pages 89â€“96. In: Proceedings of International Conference
on Artiï¬cial Intelligence and Statistics. Clearwater Beach, FL: Proceedings of
Machine Learning Research.

Bibliography
531
Courant, R., and Hilbert, D. 1953. Methods of Mathematical Physics, Volume 1. New
York: Wiley.
Cramer, H. 1949. Mathematical Methods of Statistics. Princeton, NJ: University Press.
Dacunha-Castelle, D., and Gassiat, E. 1997. Testing in Locally Conic Models, and
Application to Mixture Models. Probability and Statistics, 1, 285â€“317.
Dempster, A. P., Laird, N. M., and Rubin, D. B. 1977. Maximum Likelihood for
Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society,
39-B, 1â€“38.
Dharmadhikari, S., and Joag-Dev, K. 1988. Unimodality, Convexity, and Applications.
Cambridge, MA: Academic Press.
Ding, X., He, L., and Carin, L. 2011. Bayesian Robust Principal Component Analysis.
IEEE Transactions on Image Processing, 20(12), 3419â€“3430.
Drexler, F. J. 1978. A Homotopy Method for the Calculation of All Zeros of Zero-
Dimensional Polynomial Ideals. Pages 69â€“93 of: Wacker, H. J. (ed.), Continuation
Methods. New York: Academic Press.
Dâ€™Souza, A., Vijayakumar, S., and Schaal, S. 2004. The Bayesian Backï¬tting Rele-
vance Vector Machine. In: Proceedings of the 21st International Conference on
Machine Learning. Banff, AB: Association for Computing Machinery.
Durbin, R., Eddy, S., Krogh, A., and Mitchison, G. 1998. Biological Sequence Analysis:
Probabilistic Models of Proteins and Nucleic Acids. Cambridge: Cambridge
University Press.
Efron, B., and Morris, C. 1973. Steinâ€™s Estimation Rule and its Competitorsâ€”An
Empirical Bayes Approach. Journal of the American Statistical Association, 68,
117â€“130.
Elhamifar, E., and Vidal, R. 2013. Sparse Subspace Clustering: Algorithm, Theory, and
Applications. IEEE Transactions on Pattern Analysis and Machine Intelligence,
35(11), 2765â€“2781.
Felzenszwalb, P. F., and Huttenlocher, D. P. 2004. Efï¬cient Graph-Based Image
Segmentation. International Journal of Computer Vision, 59(2), 167â€“181.
Fukumizu, K. 1999. Generalization Error of Linear Neural Networks in Unidentiï¬able
Cases. Pages 51â€“62 of: Proceedings of International Conference on Algorithmic
Learning Theory. Tokyo, Japan: Springer.
Fukumizu, K. 2003. Likelihood Ratio of Unidentiï¬able Models and Multilayer Neural
Networks. Annals of Statistics, 31(3), 833â€“851.
Garcia, C. B., and Zangwill, W. I. 1979. Determining All Solutions to Certain Systems
of Nonlinear Equations. Mathematics of Operations Research, 4, 1â€“14.
Gershman, S. J., and Blei, D. M. 2012. A Tutorial on Bayesian Nonparametric Models.
Journal of Mathematical Psychology, 56(1), 1â€“12.
Ghahramani, Z., and Beal, M. J. 2001. Graphical Models and Variational Methods.
Pages 161â€“177 of: Advanced Mean Field Methods. Cambridge, MA: MIT Press.
Girolami, M. 2001. A Variational Method for Learning Sparse and Overcomplete
Representations. Neural Computation, 13(11), 2517â€“2532.
Girolami, M., and Kaban, A. 2003. On an Equivalence between PLSI and LDA. Pages
433â€“434 of: Proceedings of SIGIR, New York and Toronto, ON: Association for
Computing Machinery.

532
Bibliography
Gopalan, P., Hofman, J. M., and Blei, D. M. 2013. Scalable Recommendation with
Poisson Factorization. arXiv:1311.1704 [cs.IR].
Grifï¬ths, T. L., and Steyvers, M. 2004. Finding Scientiï¬c Topics. PNAS, 101, 5228â€“
5235.
Gunji, T., Kim, S., Kojima, M., Takeda, A., Fujisawa, K., and Mizutani, T. 2004.
PHoMâ€”A Polyhedral Homotopy Continuation Method. Computing, 73, 57â€“77.
Gupta, A. K., and Nagar, D. K. 1999. Matrix Variate Distributions. London, UK:
Chapman and Hall/CRC.
Hagiwara, K. 2002. On the Problem in Model Selection of Neural Network Regression
in Overrealizable Scenario. Neural Computation, 14, 1979â€“2002.
Hagiwara, K., and Fukumizu, K. 2008. Relation between Weight Size and Degree of
Over-Fitting in Neural Network Regression. Neural Networks, 21(1), 48â€“58.
Han, T. S., and Kobayashi, K. 2007. Mathematics of Information and Coding. Provi-
dence, RI: American Mathematical Society.
Harshman, R. A. 1970. Foundations of the PARAFAC Procedure: Models and Condi-
tions for an â€œExplanatoryâ€ Multimodal Factor Analysis. UCLA Working Papers in
Phonetics, 16, 1â€“84.
Hartigan, J. A. 1985. A Failure of Likelihood Ratio Asymptotics for Normal Mixtures.
Pages 807â€“810 of: Proceedings of the Berkeley Conference in Honor of J. Neyman
and J. Kiefer. Berkeley, CA: Springer.
Hastie, T., and Tibshirani, R. 1986. Generalized Additive Models. Statistical Science,
1(3), 297â€“318.
Hinton, G. E., and van Camp, D. 1993. Keeping Neural Networks Simple by Minimiz-
ing the Description Length of the Weights. Pages 5â€“13 of: Proceedings of COLT.
Santa Cruz, CA.
Hoffman, M. D., Blei, D. M., Wang, C., and Paisley, J. 2013. Stochastic Variational
Inference. Journal of Machine Learning Research, 14, 1303â€“1347.
Hofmann, T. 2001. Unsupervised Learning by Probabilistic Latent Semantic Analysis.
Machine Learning, 42, 177â€“196.
Hosino, T., Watanabe, K., and Watanabe, S. 2005. Stochastic Complexity of Variational
Bayesian Hidden Markov Models. In: Proceedings of IJCNN. Montreal, QC.
Hosino, T., Watanabe, K., and Watanabe, S. 2006a. Free Energy of Stochastic Context
Free Grammar on Variational Bayes. Pages 407â€“416 of: Proceedings of ICONIP.
Hong Kong, China: Springer.
Hosino, T., Watanabe, K., and Watanabe, S. 2006b. Stochastic Complexity of Hidden
Markov Models on the Variational Bayesian Learning (in Japanese). IEICE
Transactions on Information and Systems, J89-D(6), 1279â€“1287.
Hotelling, H. 1933. Analysis of a Complex of Statistical Variables into Principal
Components. Journal of Educational Psychology, 24, 417â€“441.
Hoyle, D. C. 2008. Automatic PCA Dimension Selection for High Dimensional Data
and Small Sample Sizes. Journal of Machine Learning Research, 9, 2733â€“2759.
Hoyle, D. C., and Rattray, M. 2004. Principal-Component-Analysis Eigenvalue Spectra
from Data with Symmetry-Breaking Structure. Physical Review E, 69(026124).
Huynh, T., Mario, F., and Schiele, B. 2008. Discovery of Activity Patterns Using Topic
Models. Pages 9â€“10. In: International Conference on Ubiquitous Computing (Ubi-
Comp). New York and Seoul, South Korea: Association for Computer Machinery.

Bibliography
533
HyvÂ¨arinen, A., Karhunen, J., and Oja, E. 2001. Independent Component Analysis. New
York: Wiley.
Ibragimov, I. A. 1956. On the Composition of Unimodal Distributions. Theory of
Probability and Its Applications, 1(2), 255â€“260.
Ilin, A., and Raiko, T. 2010. Practical Approaches to Principal Component Analysis
in the Presence of Missing Values. Journal of Machine Learning Research, 11,
1957â€“2000.
Ito, H., Amari, S., and Kobayashi, K. 1992. Identiï¬ability of Hidden Markov Infor-
mation Sources and Their Minimum Degrees of Freedom. IEEE Transactions on
Information Theory, 38(2), 324â€“333.
Jaakkola, T. S., and Jordan, M. I. 2000. Bayesian Parameter Estimation via Variational
Methods. Statistics and Computing, 10, 25â€“37.
James, W., and Stein, C. 1961. Estimation with Quadratic Loss. Pages 361â€“379
of: Proceedings of the 4th Berkeley Symposium on Mathematical Statistics and
Probability, vol. 1. Berkeley: University of California Press.
Jeffreys, H. 1946. An Invariant Form for the Prior Probability in Estimation Problems.
Pages 453â€“461 of: Proceedings of the Royal Society of London. Series A,
Mathematical and Physical Sciences, vol. 186. London, UK: Royal Society.
Jensen, F. V. 2001. Bayesian Networks and Decision Graphs. Springer.
Johnstone, I. M. 2001. On the Distribution of the Largest Eigenvalue in Principal
Components Analysis. Annals of Statistics, 29, 295â€“327.
Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., and Saul, L. 1999. Introduction to
Variational Methods for Graphical Models. Machine Learning, 37, 183â€“233.
Kaji, D., Watanabe, K., and Watanabe, S. 2010. Phase Transition of Variational Bayes
Learning in Bernoulli Mixture. Australian Journal of Intelligent Information
Processing Systems, 11(4), 35â€“40.
Khan, M. E., Babanezhad, R., Lin, W., Schmidt, M., and Sugiyama, M. 2016. Faster
Stochastic Variational Inference Using Proximal-Gradient Methods with General
Divergence Functions. Pages 309â€“318. In: Proceedings of UAI. New York: AUAI
Press.
Kim, Y. D., and Choi, S. 2014. Scalable Variational Bayesian Matrix Factorization with
Side Information. Pages 493â€“502 of: Proceedings of AISTATS. Reykjavik, Iceland:
Proceedings of Machine Learning Research.
Kingma, D. P., and Welling, M. 2014. Auto-Encoding Variational Bayes. In: Interna-
tional Conference on Learning Representations (ICLR). arXiv:1412.6980
Kolda, T. G., and Bader, B. W. 2009. Tensor Decompositions and Applications. SIAM
Review, 51(3), 455â€“500.
Krestel, R., Fankhauser, P., and Nejdl, W. 2009. Latent Dirichlet Allocation for Tag
Recommendation. Pages 61â€“68 of: Proceedings of the Third ACM Conference on
Recommender Systems. New York: Association for Computing Machinery.
Krizhevsky, A., Sutskever, I., and Hinton, G. E. 2012. ImageNet Classiï¬cation with
Deep Convolutional Neural Networks. Pages 1097â€“1105 of: Advances in NIPS.
Lake Tahoe, NV: NIPS Foundation.
Kurihara, K., and Sato, T. 2004. An application of the variational Bayesian Approach
to Probabilistic Context-Free Grammars. In: Proceedings of IJCNLP. Banff, AB.

534
Bibliography
Kurihara, K., Welling, M., and Teh, M. Y. W. 2007. Collapsed Variational Dirichlet
Process Mixture Models. In: Proceedings of IJCAI. Hyderabad, India.
Kuriki, S., and Takemura, A. 2001. Tail Probabilities of the Maxima of Multilinear
Forms and Their Applications. Annals of Statistics, 29(2), 328â€“371.
Lee, T. L., Li, T. Y., and Tsai, C. H. 2008. HOM4PS-2.0: A Software Package for
Solving Polynomial Systems by the Polyhedral Homotopy Continuation Method.
Computing, 83, 109â€“133.
Levin, E., Tishby, N., and Solla, S. A. 1990. A Statistical Approaches to Learning and
Generalization in Layered Neural Networks. Pages 1568â€“1674 of: Proceedings of
IEEE, vol. 78.
Li, F.-F., and Perona, P. 2005. A Bayesian Hierarchical Model for Learning Natural
Scene Categories. Pages 524â€“531 of: Proceedings of CVPR. San Diego, CA.
Lim, Y. J., and Teh, Y. W. 2007. Variational Bayesian Approach to Movie Rating
Prediction. In: Proceedings of KDD Cup and Workshop. New York and San Jose,
CA: Association for Computing Machinery.
Lin, Z., Chen, M., and Ma, Y. 2009. The Augmented Lagrange Multiplier Method for
Exact Recovery of Corrupted Low-Rank Matrices. UIUC Technical Report UILU-
ENG-09-2215.
Liu, G., and Yan, S. 2011. Latent Low-Rank Representation for Subspace Segmentation
and Feature Extraction. In: Proceedings of ICCV. Barcelona, Spain.
Liu, G., Lin, Z., and Yu, Y. 2010. Robust Subspace Segmentation by Low-Rank Repre-
sentation. Pages 663â€“670 of: Proceedings of ICML. Haifa, Israel: Omnipress.
Liu, G., Xu, H., and Yan, S. 2012. Exact Subspace Segmentation and Outlier Detection
by Low-Rank Representation. In: Proceedings of AISTATS. La Palma, Canary
Islands: Proceedings of Machine Learning Research.
Liu, X., Pasarica, C., and Shao, Y. 2003. Testing Homogeneity in Gamma Mixture
Models. Scandinavian Journal of Statistics, 30, 227â€“239.
Lloyd, S. P. 1982. Least Square Quantization in PCM. IEEE Transactions on Informa-
tion Theory, 28(2), 129â€“137.
MacKay, D. J. C. 1992. Bayesian Interpolation. Neural Computation, 4(2), 415â€“447.
MacKay, D. J. C. 1995. Developments in Probabilistic Modeling with Neural
Networksâ€”Ensemble Learning. Pages 191â€“198 of: Proceedings of the 3rd Annual
Symposium on Neural Networks.
Mackay, D. J. C. 2001. Local Minima, Symmetry-Breaking, and Model Pruning in
Variational Free Energy Minimization. Available from www.inference.phy.cam
.ac.uk/mackay/minima.pdf.
MacKay, D. J. C. 2003. Information Theory, Inference, and Learning Algorithms.
Cambridge: Cambridge University Press. Available from www.inference.phy.cam
.ac.uk/mackay/itila/.
MacQueen, J. B. 1967. Some Methods for Classiï¬cation and Analysis of Multivariate
Observations. Pages 281â€“297 of: Proceedings of 5th Berkeley Symposium on
Mathematical Statistics and Probability, vol. 1. Berkeley: University of California
Press.
MarË‡cenko, V. A., and Pastur, L. A. 1967. Distribution of Eigenvalues for Some Sets of
Random Matrices. Mathematics of the USSR-Sbornik, 1(4), 457â€“483.

Bibliography
535
Marshall, A. W., Olkin, I., and Arnold, B. C. 2009. Inequalities: Theory of Majorization
and Its Applications, 2d ed. Springer.
Minka, T. P. 2001a. Automatic Choice of Dimensionality for PCA. Pages 598â€“604 of:
Advances in NIPS, vol. 13. Cambridge, MA: MIT Press.
Minka, T. P. 2001b. Expectation Propagation for Approximate Bayesian Inference.
Pages 362â€“369 of: Proceedings of UAI. Seattle, WA: Morgan Kaufmann Publish-
ers Inc.
MÃ¸rup, M., and Hansen, L. R. 2009. Automatic Relevance Determination for Multi-
Way Models. Journal of Chemometrics, 23, 352â€“363.
Nakajima, S., and Sugiyama, M. 2011. Theoretical Analysis of Bayesian Matrix
Factorization. Journal of Machine Learning Research, 12, 2579â€“2644.
Nakajima, S., and Sugiyama, M. 2014. Analysis of Empirical MAP and Empirical
Partially Bayes: Can They Be Alternatives to Variational Bayes? Pages 20â€“28 of:
Proceedings of International Conference on Artiï¬cial Intelligence and Statistics,
vol. 33. Reykjavik, Iceland: Proceedings of Machine Learning Research.
Nakajima, S., and Watanabe, S. 2007. Variational Bayes Solution of Linear Neural
Networks and Its Generalization Performance. Neural Computation, 19(4), 1112â€“
1153.
Nakajima, S., Sugiyama, M., and Babacan, S. D. 2011 (June 28â€“July 2). On Bayesian
PCA: Automatic Dimensionality Selection and Analytic Solution. Pages 497â€“
504 of: Proceedings of 28th International Conference on Machine Learning
(ICML2011). Bellevue, WA: Omnipress.
Nakajima, S., Sugiyama, M., Babacan, S. D., and Tomioka, R. 2013a. Global Analytic
Solution of Fully-Observed Variational Bayesian Matrix Factorization. Journal of
Machine Learning Research, 14, 1â€“37.
Nakajima, S., Sugiyama, M., and Babacan, S. D. 2013b. Variational Bayesian Sparse
Additive Matrix Factorization. Machine Learning, 92, 319â€“1347.
Nakajima, S., Takeda, A., Babacan, S. D., Sugiyama, M., and Takeuchi, I. 2013c.
Global Solver and Its Efï¬cient Approximation for Variational Bayesian Low-Rank
Subspace Clustering. In: Advances in Neural Information Processing Systems 26.
Lake Tahoe, NV: NIPS Foundation.
Nakajima, S., Sato, I., Sugiyama, M., Watanabe, K., and Kobayashi, H. 2014. Analysis
of Variational Bayesian Latent Dirichlet Allocation: Weaker Sparsity Than MAP.
Pages 1224â€“1232 of: Advances in NIPS, vol. 27. Montreal, Quebec:: NIPS
Foundation.
Nakajima, S., Tomioka, R., Sugiyama, M., and Babacan, S. D. 2015. Condition
for Perfect Dimensionality Recovery by Variational Bayesian PCA. Journal of
Machine Learning Research, 16, 3757â€“3811.
Nakamura, F., and Watanabe, S. 2014. Asymptotic Behavior of Variational Free Energy
for Normal Mixtures Using General Dirichlet Distribution (in Japanese). IEICE
Transactions on Information and Systems, J97-D(5), 1001â€“1013.
Neal, R. M. 1996. Bayesian Learning for Neural Networks. New York: Springer.
Opper, M., and Winther, O. 1996. A Mean Field Algorithm for Bayes Learning in Large
Feed-Forward Neural Networks. Pages 225â€“231 of: Advances in NIPS. Denver,
CO: NIPS Foundation.

536
Bibliography
Pearson, K. 1914. Tables for Statisticians and Biometricians. Cambridge: Cambridge
University Press.
Purushotham, S., Liu, Y., and Kuo, C. C. J. 2012. Collaborative Topic Regression
with Social Matrix Factorization for Recommendation Systems. In: Proceedings
of ICML. Edinburgh, UK: Omnipress.
Rabiner, L. R. 1989. A Tutorial on Hidden Markov Models and Selected Applications
in Speech Recognition. Pages 257â€“286 of: Proceedings of the IEEE. Piscataway,
NJ: IEEE.
Ranganath, R., Gerrish, S., and Blei, D. M. 2013. Black Box Variational Inference.
In: Proceedings of AISTATS. Scottsdale, AZ: Proceedings of Machine Learning
Research.
Reinsel, G. R., and Velu, R. P. 1998. Multivariate Reduced-Rank Regression: Theory
and Applications. New York: Springer.
Rissanen, J. 1986. Stochastic Complexity and Modeling. Annals of Statistics, 14(3),
1080â€“1100.
Robbins, H., and Monro, S. 1951. A Stochastic Approximation Method. Annals of
Mathematical Statistics, 22(3), 400â€“407.
Ruhe, A. 1970. Perturbation Bounds for Means of Eigenvalues and Invariant Subspaces.
BIT Numerical Mathematics, 10, 343â€“354.
Rusakov, D., and Geiger, D. 2005. Asymptotic Model Selection for Naive Bayesian
Networks. Journal of Machine Learning Research, 6, 1â€“35.
Sakamoto, T., Ishiguro, M., and Kitagawa, G. 1986. Akaike Information Criterion
Statistics. Dordrecht: D. Reidel Publishing Company.
Salakhutdinov, R., and Mnih, A. 2008. Probabilistic Matrix Factorization. Pages 1257â€“
1264 of: Platt, J. C., Koller, D., Singer, Y., and Roweis, S. (eds), Advances in
Neural Information Processing Systems 20. Cambridge, MA: MIT Press.
Sato, I., Kurihara, K., and Nakagawa, H. 2012. Practical Collapsed Variational
Bayes Inference for Hierarchical Dirichlet Process. Pages 105â€“113. In: Pro-
ceedings of KDD. New York and Beijing, China: Association for Computing
Machinery.
Sato, M., Yoshioka, T., Kajihara, S., et al. 2004. Hierarchical Bayesian Estimation for
MEG Inverse Problem. NeuroImage, 23, 806â€“826.
Schwarz, G. 1978. Estimating the Dimension of a Model. Annals of Statistics, 6(2),
461â€“464.
Seeger, M. 2008. Bayesian Inference and Optimal Design for the Sparse Linear Model.
Journal of Machine Learning Research, 9, 759â€“813.
Seeger, M. 2009. Sparse Linear Models: Variational Approximate Inference and
Bayesian Experimental Design. In: Journal of Physics: Conference Series, vol.
197. Bristol, UK: IOP Publishing.
Seeger, M., and Bouchard, G. 2012. Fast Variational Bayesian Inference for Non-
Conjugate Matrix Factorization Models. Pages 1012â€“1018. In: Proceedings of
International Conference on Artiï¬cial Intelligence and Statistics. La Palma,
Canary Islands: Proceedings of Machine Learning Research.
Shi, J., and Malik, J. 2000. Normalized Cuts and Image Segmentation. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, 22(8), 888â€“905.

Bibliography
537
Soltanolkotabi, M., and Cand`es, E. J. 2011. A Geometric Analysis of Subspace
Clustering with Outliers. CoRR. arXiv:1112.4258 [cs.IT].
Spall, J. 2003. Introduction to Stochastic Search and Optimization: Estimation, Simu-
lation, and Control. New York: John Wiley and Sons.
Srebro, N., and Jaakkola, T. 2003. Weighted Low Rank Approximation. In: Fawcett, T.,
and Mishra, N. (eds), Proceedings of the Twentieth International Conference on
Machine Learning. Washington, DC: AAAI Press.
Srebro, N., Rennie, J., and Jaakkola, T. 2005. Maximum Margin Matrix Factorization.
In: Advances in Neural Information Processing Systems 17. Vancouver, BC: NIPS
Foundation.
Stein, C. 1956. Inadmissibility of the Usual Estimator for the Mean of a Multivariate
Normal Distribution. Pages 197â€“206 of: Proceedings of the 3rd Berkeley Symposi-
ium on Mathematics Statistics and Probability. Berkeley: University of California
Press.
Takemura, A., and Kuriki, S. 1997. Weights of Chi-Squared Distribution for Smooth or
Piecewise Smooth Cone Alternatives. Annals of Statistics, 25(6), 2368â€“2387.
Teh, Y. W., Newman, D., and Welling, M. 2007. A Collapsed Variational Bayesian
Inference Algorithm for Latent Dirichlet Allocation. In: Advances in NIPS.
Vancouver, BC: NIPS Foundation.
Tipping, M. E. 2001. Sparse Bayesian Learning and the Relevance Vector Machine.
Journal of Machine Learning Research, 1, 211â€“244.
Tipping, M. E., and Bishop, C. M. 1999. Probabilistic Principal Component Analysis.
Journal of the Royal Statistical Society, 61, 611â€“622.
Tomioka, R., Suzuki, T., Sugiyama, M., and Kashima, H. 2010. An Efï¬cient and
General Augmented Lagrangian Algorithm for Learning Low-Rank Matrices. In:
Proceedings of International Conference on Machine Learning. Haifa, Israel:
Omnipress.
Tron, R., and Vidal, R. 2007. A Benchmark for the Comparison of 3-D Motion
Segmentation Algorithms. Pages 1â€“8. In: Proceedings of CVPR. Minneapolis,
MN.
Tucker, L. R. 1996. Some Mathematical Notes on Three-Mode Factor Analysis.
Psychometrika, 31, 279â€“311.
Ueda, N., Nakano, R., Ghahramani, Z., and Hinton, G. E. 2000. SMEM Algorithm for
Mixture Models. Neural Computation, 12(9), 2109â€“2128.
van der Vaart, A. W. 1998. Asymptotic Statistics. Cambridge Series in Statistical
and Probabilistic Mathematics. Cambridge and New York: Cambridge University
Press.
Vidal, R., and Favaro, P. 2014. Low Rank Subspace Clustering. Pattern Recognition
Letters, 43(1), 47â€“61.
Wachter, K. W. 1978. The Strong Limits of Random Matrix Spectra for Sample
Matrices of Independent Elements. Annals of Probability, 6, 1â€“18.
Wainwright, M. J., and Jordan, M. I. 2008. Graphical Models, Exponential Families,
and Variational Inference. Foundations and Trends in Machine Learning, 1,
1â€“305.
Watanabe, K. 2012. An Alternative View of Variational Bayes and Asymptotic Approx-
imations of Free Energy. Machine Learning, 86(2), 273â€“293.

538
Bibliography
Watanabe, K., and Watanabe, S. 2004. Lower Bounds of Stochastic Complexities
in Variational Bayes Learning of Gaussian Mixture Models. Pages 99â€“104 of:
Proceedings of IEEE on CIS.
Watanabe, K., and Watanabe, S. 2005. Variational Bayesian Stochastic Complexity of
Mixture Models. Pages 99â€“104. In: Advances in NIPS, vol. 18. Vancouver, BC:
NIPS Foundation.
Watanabe, K., and Watanabe, S. 2006. Stochastic Complexities of Gaussian Mixtures
in Variational Bayesian Approximation. Journal of Machine Learning Research,
7, 625â€“644.
Watanabe, K., and Watanabe, S. 2007. Stochastic Complexities of General Mixture
Models in Variational Bayesian Learning. Neural Networks, 20(2), 210â€“219.
Watanabe, K., Shiga, M., and Watanabe, S. 2006. Upper Bounds for Variational
Stochastic Complexities of Bayesian Networks. Pages 139â€“146 of: Proceedings
of IDEAL. Burgos, Spain: Springer.
Watanabe, K., Shiga, M., and Watanabe, S. 2009. Upper Bound for Variational Free
Energy of Bayesian Networks. Machine Learning, 75(2), 199â€“215.
Watanabe, K., Okada, M., and Ikeda, K. 2011. Divergence Measures and a General
Framework for Local Variational Approximation. Neural Networks, 24(10), 1102â€“
1109.
Watanabe, S. 2001a. Algebraic Analysis for Nonidentiï¬able Learning Machines.
Neural Computation, 13(4), 899â€“933.
Watanabe, S. 2001b. Algebraic Information Geometry for Learning Machines with
Singularities. Pages 329â€“336 of: Advances in NIPS, vol. 13. Vancouver, BC: NIPS
Foundation.
Watanabe, S. 2009. Algebraic Geometry and Statistical Learning Theory. Cambridge:
Cambridge University Press.
Watanabe, S. 2010. Asymptotic Equivalence of Bayes Cross Validation and Widely
Applicable Information Criterion in Singular Learning Theory. Journal of
Machine Learning Research, 11, 3571â€“3594.
Watanabe, S. 2013. A Widely Applicable Bayesian Information Criterion. Journal of
Machine Learning Research, 14, 867â€“897.
Watanabe, S., and Amari, S. 2003. Learning Coefï¬cients of Layered Models When the
True Distribution Mismatches the Singularities. Neural Computation, 15, 1013â€“
1033.
Wei, X., and Croft, W. B. 2006. LDA-Based Document Models for Ad-Hoc Retrieval.
Pages 178â€“185 of: Proceedings of SIGIR. Seattle, WA: Association for Computing
Machinery New York.
Wingate, D., and Weber, T. 2013. Automated Variational Inference in Probabilistic
Programming. arXiv:1301.1299.
Yamazaki, K. 2016. Asymptotic Accuracy of Bayes Estimation for Latent Variables
with Redundancy. Machine Learning, 102(1), 1â€“28.
Yamazaki, K., and Kaji, D. 2013. Comparing Two Bayes Methods Based on the Free
Energy Functions in Bernoulli Mixtures. Neural Networks, 44, 36â€“43.
Yamazaki, K., and Watanabe, S. 2003a. Singularities in Mixture Models and Upper
Bounds Pages 1â€“8. of Stochastic Complexity. Neural Networks, 16(7), 1029â€“1038.

Bibliography
539
Yamazaki, K., and Watanabe, S. 2003b. Stochastic Complexity of Bayesian Networks.
Pages 592â€“599 of: Proceedings of the Nineteenth Conference on Uncertainty in
Artiï¬cial Intelligence. Acapulco, Mexico: Morgan Kaufmann.
Yamazaki, K., and Watanabe, S. 2004. Newton Diagram and Stochastic Complexity
in Mixture of Binomial Distributions. Pages 350â€“364. In: Proceedings of ALT.
Padova, Italy: Springer.
Yamazaki, K., and Watanabe, S. 2005. Algebraic Geometry and Stochastic Complexity
of Hidden Markov Models. Neurocomputing, 69, 62â€“84.
Yamazaki, K., Aoyagi, M., and Watanabe, S. 2010. Asymptotic Analysis of Bayesian
Generalization Error with Newton Diagram. Neural Networks, 23(1), 35â€“43.

Subject Index
N-mode tensor, 80
â„“1-norm, 94, 291
â„“1-regularizer, 88
n-mode tensor product, 80
activation function, 47
admissibility, 518
Akaikeâ€™s information criterion (AIC), 365
all temperatures method, 382
approximate global variational Bayesian
solver (AGVBS), 269
ARD Tucker, 332
asymptotic normality, 342, 352
asymptotic notation, 344
automatic relevance determination (ARD), 10,
36, 72, 204, 243, 294
average generalization error, 347
average training error, 347
Bachmannâ€“Landau notation, 344
backï¬tting algorithm, 283
basis selection effect, 373, 426
Bayes free energy, 36, 194, 348
Bayes generalization loss, 379
Bayes posterior, 4
Bayes theorem, 4
Bayes training loss, 379
Bayesian estimator, 5
Bayesian information criterion (BIC), 366
Bayesian learning, 3, 5
Bayesian network, 115, 455
Bernoulli distribution, 11, 27, 248, 253
Bernoulli mixture model, 451
Beta distribution, 11, 27
Beta function, 27
binomial distribution, 11, 27
black-box variational inference, 50
burn-in, 59
calculus of variations, 44
centering, 73
central limit theorem, 344
chi-squared distribution, 356
classiï¬cation, 47
collaborative ï¬ltering (CF), 74, 335
collapsed Gibbs sampling, 53
collapsed MAP learning, 53
collapsed variational Bayesian learning,
53
complete likelihood, 8
conditional conjugacy, 39, 42
conditional distribution, 3
conditionally conjugate prior, 42
conjugacy, 10, 12
consistency, 352
continuation method, 267
convergence in distribution, 344
convergence in law, 344
convergence in probability, 344
coordinate descent, 66
core tensor, 80
CramÂ´erâ€“Rao lower-bound, 348
credible interval, 32
cross-validation, 9
cross-covariance, 73
density, 522
Dirac delta function, 37, 52
direct site bounding, 49, 132
Dirichlet distribution, 11, 26
Dirichlet process prior, 112
distinct signal assumption, 419
540

Subject Index
541
domination, 38, 518
doubly stochastic, 154
efï¬ciency, 518
empirical Bayesian (EBayes) estimator, 38,
193, 516
empirical Bayesian (EBayes) learning, 9, 35
empirical entropy, 349
empirical MAP (EMAP) learning, 311
empirical PB (EPB) learning, 311
empirical variational Bayesian (EVB)
learning, 47
entropy, 380, 520
equivalence class, 187
error function, 47
Eulerâ€“Lagrange equation, 44
evidence, 36
evidence lower-bound (ELBO), 40, 341
exact global variational Bayesian solver
(EGVBS), 267
expectation propagation (EP), 53
expectation-maximization (EM) algorithm, 9,
53, 105
exponential family, 15, 108, 443
factor matrix, 80
ï¬nite mixture model, 103
Fisher information, 51, 197, 342, 520
foreground/background video separation,
97
forwardâ€“backward algorithm, 122
free energy, 40
free energy coefï¬cient, 349
Gamma distribution, 11, 16
Gamma function, 525
Gaussâ€“Wishart distribution, 21
Gaussian distribution, 11
Gaussian mixture model, 104, 434
generalization coefï¬cient, 348
generalization error, 335, 347
generalized Bayesian learning, 378
generalized posterior distribution, 378
generalized predictive distribution, 379
Gibbs generalization loss, 379
Gibbs learning, 380
Gibbs sampling, 59
Gibbs training loss, 379
global latent variable, 7, 103
Hadamard product, 154
hard assignment, 9
hidden Markov model, 119, 461
hidden variable, 6
hierarchical model, 17
histogram, 26
homotopy method, 267
hyperparameter, 9
hyperprior, 9
identiï¬ability, 342, 352
improper prior, 200
independent and identically distributed
(i.i.d.), 4
information criterion, 364
insideâ€“outside algorithm, 126
integration effect, 374, 427
inverse temperature parameter, 379
isotropic Gauss-Gamma distribution, 17
isotropic Gaussian distribution, 11
iterative singular value shrinkage, 248, 252
Jamesâ€“Stein (JS) estimator, 38, 516
Jeffreys prior, 198, 522
joint distribution, 3
Kronecker delta, 130
Kronecker product, 66, 81, 331
Kronecker product covariance approximation
(KPCA), 93, 274
Kullbackâ€“Leibler (KL) divergence, 39, 197,
347, 520
Laplace approximation (LA), 51, 230
large-scale limit, 215, 319
latent Dirichlet allocation, 26, 127, 470
latent variable, 6
latent variable model, 103, 429
law of large numbers, 344
likelihood ratio, 347
linear neural network, 385
linear regression model, 22
link function, 253
local latent variable, 7, 103
local variational approximation, 49, 132
local-EMAP estimator, 317
local-EPB estimator, 317
local-EVB estimator, 182, 231, 242
log marginal likelihood, 36
log-concave distribution, 218
logistic regression, 132

542
Subject Index
low-rank representation, 88
low-rank subspace clustering (LRSC), 88, 255
MarË‡cenkoâ€“Pastur (MP) distribution, 215
MarË‡cenkoâ€“Pastur upper limit (MPUL), 216,
319
marginal likelihood, 5
Markov chain Monte Carlo (MCMC), 58
matrix factorization (MF), 63, 195
matrix variate Gaussian, 66
maximum a posteriori (MAP) estimator, 188
maximum a posteriori (MAP) learning, 5, 294
maximum likelihood (ML) estimator, 188, 432
maximum likelihood (ML) learning, 5, 105
maximum log-likelihood, 366
mean update (MU) algorithm, 283
mean value theorem, 353
metric, 522
Metropolisâ€“Hastings sampling, 58
minimum description length (MDL), 366
mixing weight, 7
mixture coefï¬cient, 7
mixture model, 26, 196
mixture of Gaussians, 104
model distribution, 3
model likelihood, 3
model parameter, 3
model selection, 364
model-induced regularization (MIR), 72, 89,
94, 184, 195, 285, 308, 344, 373, 427
moment matching, 54
multilayer neural network, 196
multinomial distribution, 11, 26
multinomial parameter, 26
natural parameter, 15, 108
neural network, 47
Newtonâ€“Raphson method, 108
noise variance parameter, 22
noninformative prior, 522
nonsingular, 23
normalization constant, 10
normalized cuts, 88
Occamâ€™s razor, 201, 366
one-of-K representation, 8, 104, 455
overï¬tting, 420
overlap (OL) method, 230, 242
Parafac, 80
partially Bayesian (PB) learning, 51, 131, 294
partitioned-and-rearranged (PR) matrix, 95,
279
plug-in predictive distribution, 6, 46
Poisson distribution, 248, 253
polygamma function, 108
polynomial system, 162, 257, 267
positive-part Jamesâ€“Stein (PJS) estimator,
185, 307, 390, 518
posterior covariance, 5
posterior distribution, 4
posterior mean, 5
predictive distribution, 6
prior distribution, 3
probabilistic context-free grammar, 123, 466
probabilistic latent semantic analysis (pLSA),
131
probabilistic principal component analysis
(probabilistic PCA), 63, 71, 230
quasiconvexity, 207, 305
radial basis function (RBF), 367
random matrix theory, 214, 375, 404
realizability, 346, 352
realizable, 375
rectiï¬ed linear unit (ReLU), 47
reduced rank regression (RRR), 72, 385
regression parameter, 22
regular learning theory, 351
regular model, 198, 342
regularity condition, 342, 351
relative Bayes free energy, 348
relative variational Bayesian (VB) free energy,
383
resolution of singularities, 378
robust principal component analysis (robust
PCA), 93, 288
sample mean, 13
score function, 50
segmentation-based SAMF (sSAMF), 289
selecting the optimal basis function, 372
self-averaging, 216
sigmoid function, 47, 248, 253
simple variational Bayesian (SimpleVB)
learning, 71
singular learning theory (SLT), 376
singular model, 197, 342, 522
singularities, 197, 342
soft assignment, 9

Subject Index
543
sparse additive matrix factorization (SAMF),
94, 96, 204, 279
sparse matrix factorization (SMF) term, 94,
204, 279
sparse subspace clustering, 88
sparsity-inducing prior, 135
spectral clustering algorithm, 88
spiked covariance (SC) distribution, 217
spiked covariance model, 214
standard (K âˆ’1)-simplex, 7
state density, 376
stick-breaking process, 112
stochastic complexity, 36
stochastic gradient descent, 50
strictly quasiconxex, 207
strong unimodality, 218
subspace clustering, 87
subtle signal assumption, 420
sufï¬cient statistics, 15
superdiagonal, 80
Taylor approximation, 51
tensor, 80
tensor mode, 80
tensor rank, 80
trace norm, 88, 94, 291
training coefï¬cient, 348
training error, 347
trial distribution, 39
Tucker factorization (TF), 80, 294, 331, 336
underï¬tting, 420
unidentiï¬ability, 184, 187, 195
uniform prior, 198, 522
unnormalized posterior distribution, 4
variation, 44
variational Bayesian (VB) estimator, 46
variational Bayesian (VB) free energy, 383
variational Bayesian (VB) learning, 39
variational Bayesian (VB) posterior, 43, 46
variational parameter, 46, 66
vectorization operator, 66, 81, 331
volume element, 197, 522
weak convergence, 344
whitening, 73, 386
widely applicable Bayesian information
criterion (WBIC), 60, 381
widely applicable information criterion
(WAIC), 380
Wishart distribution, 11, 20
zeta function, 376




Variational Bayesian Learning Theory
Variational Bayesian learning is one of the most popular methods in machine learning.
Designed for researchers and graduate students in machine learning, this book summa-
rizes recent developments in the nonasymptotic and asymptotic theory of variational
Bayesian learning and suggests how this theory can be applied in practice.
The authors begin by developing a basic framework with a focus on conjugacy, which
enables the reader to derive tractable algorithms. Next, it summarizes nonasymptotic
theory, which, although limited in application to bilinear models, precisely describes
the behavior of the variational Bayesian solution and reveals its sparsity-inducing
mechanism. Finally, the text summarizes asymptotic theory, which reveals phase
transition phenomena depending on the prior setting, thus providing suggestions on
how to set hyperparameters for particular purposes. Detailed derivations allow readers
to follow along without prior knowledge of the mathematical techniques speciﬁc to
Bayesian learning.
Shinichi Nakajima is a senior researcher at Technische Universit¨at Berlin. His
research interests include the theory and applications of machine learning, and he
has published papers at numerous conferences and in journals such as The Journal
of Machine Learning Research, The Machine Learning Journal, Neural Computation,
and IEEE Transactions on Signal Processing. He currently serves as an area chair for
Neural Information Processing Systems (NIPS) and an action editor for Digital Signal
Processing.
Kazuho Watanabe is an associate professor at Toyohashi University of Tech-
nology. His research interests include statistical machine learning and information
theory, and he has published papers at numerous conferences and in journals such
as The Journal of Machine Learning Research, The Machine Learning Journal, IEEE
Transactions on Information Theory, and IEEE Transactions on Neural Networks and
Learning Systems.
Masashi Sugiyama is the director of the RIKEN Center for Advanced Intelligence
Project and professor of Complexity Science and Engineering at the University of
Tokyo. His research interests include the theory, algorithms, and applications of
machine learning. He has written several books on machine learning, including Density
Ratio Estimation in Machine Learning. He served as program cochair and general
cochair of the NIPS conference in 2015 and 2016, respectively, and received the Japan
Academy Medal in 2017.


Variational Bayesian Learning Theory
SHINICHI NAKAJIMA
Technische Universit¨at Berlin
KAZUHO WATANABE
Toyohashi University of Technology
MASASHI SUGIYAMA
University of Tokyo

University Printing House, Cambridge CB2 8BS, United Kingdom
One Liberty Plaza, 20th Floor, New York, NY 10006, USA
477 Williamstown Road, Port Melbourne, VIC 3207, Australia
314–321, 3rd Floor, Plot 3, Splendor Forum, Jasola District Centre,
New Delhi – 110025, India
79 Anson Road, #06–04/06, Singapore 079906
Cambridge University Press is part of the University of Cambridge.
It furthers the University’s mission by disseminating knowledge in the pursuit of
education, learning, and research at the highest international levels of excellence.
www.cambridge.org
Information on this title: www.cambridge.org/9781107076150
DOI: 10.1017/9781139879354
© Shinichi Nakajima, Kazuho Watanabe, and Masashi Sugiyama 2019
This publication is in copyright. Subject to statutory exception
and to the provisions of relevant collective licensing agreements,
no reproduction of any part may take place without the written
permission of Cambridge University Press.
First published 2019
Printed and bound in Great Britain by Clays Ltd, Elcograf S.p.A.
A catalogue record for this publication is available from the British Library.
Library of Congress Cataloging-in-Publication Data
Names: Nakajima, Shinichi, author. | Watanabe, Kazuho, author. | Sugiyama,
Masashi, 1974- author.
Title: Variational Bayesian learning theory / Shinichi Nakajima (Technische
Universit¨at Berlin), Kazuho Watanabe (Toyohashi University of
Technology), Masashi Sugiyama (University of Tokyo).
Description: Cambridge ; New York, NY : Cambridge University Press, 2019. |
Includes bibliographical references and index.
Identiﬁers: LCCN 2019005983| ISBN 9781107076150 (hardback : alk. paper) |
ISBN 9781107430761 (pbk. : alk. paper)
Subjects: LCSH: Bayesian ﬁeld theory. | Probabilities.
Classiﬁcation: LCC QC174.85.B38 N35 2019 | DDC 519.2/33–dc23
LC record available at https://lccn.loc.gov/2019005983
ISBN 978-1-107-07615-0 Hardback
Cambridge University Press has no responsibility for the persistence or accuracy
of URLs for external or third-party internet websites referred to in this publication
and does not guarantee that any content on such websites is, or will remain,
accurate or appropriate.

Contents
Preface
page ix
Nomenclature
xii
Part I
Formulation
1
1
Bayesian Learning
3
1.1
Framework
3
1.2
Computation
10
2
Variational Bayesian Learning
39
2.1
Framework
39
2.2
Other Approximation Methods
51
Part II
Algorithm
61
3
VB Algorithm for Multilinear Models
63
3.1
Matrix Factorization
63
3.2
Matrix Factorization with Missing Entries
74
3.3
Tensor Factorization
80
3.4
Low-Rank Subspace Clustering
87
3.5
Sparse Additive Matrix Factorization
93
4
VB Algorithm for Latent Variable Models
103
4.1
Finite Mixture Models
103
4.2
Other Latent Variable Models
115
5
VB Algorithm under No Conjugacy
132
5.1
Logistic Regression
132
5.2
Sparsity-Inducing Prior
135
5.3
Uniﬁed Approach by Local VB Bounds
137
v

vi
Contents
Part III
Nonasymptotic Theory
147
6
Global VB Solution of Fully Observed Matrix Factorization
149
6.1
Problem Description
150
6.2
Conditions for VB Solutions
152
6.3
Irrelevant Degrees of Freedom
153
6.4
Proof of Theorem 6.4
157
6.5
Problem Decomposition
160
6.6
Analytic Form of Global VB Solution
162
6.7
Proofs of Theorem 6.7 and Corollary 6.8
163
6.8
Analytic Form of Global Empirical VB Solution
171
6.9
Proof of Theorem 6.13
173
6.10 Summary of Intermediate Results
180
7
Model-Induced Regularization and Sparsity Inducing
Mechanism
184
7.1
VB Solutions for Special Cases
184
7.2
Posteriors and Estimators in a One-Dimensional Case
187
7.3
Model-Induced Regularization
195
7.4
Phase Transition in VB Learning
202
7.5
Factorization as ARD Model
204
8
Performance Analysis of VB Matrix Factorization
205
8.1
Objective Function for Noise Variance Estimation
205
8.2
Bounds of Noise Variance Estimator
207
8.3
Proofs of Theorem 8.2 and Corollary 8.3
209
8.4
Performance Analysis
214
8.5
Numerical Veriﬁcation
228
8.6
Comparison with Laplace Approximation
230
8.7
Optimality in Large-Scale Limit
232
9
Global Solver for Matrix Factorization
236
9.1
Global VB Solver for Fully Observed MF
236
9.2
Global EVB Solver for Fully Observed MF
238
9.3
Empirical Comparison with the Standard VB Algorithm
242
9.4
Extension to Nonconjugate MF with Missing Entries
247
10
Global Solver for Low-Rank Subspace Clustering
255
10.1 Problem Description
255
10.2 Conditions for VB Solutions
258
10.3 Irrelevant Degrees of Freedom
259
10.4 Proof of Theorem 10.2
259

Contents
vii
10.5 Exact Global VB Solver (EGVBS)
264
10.6 Approximate Global VB Solver (AGVBS)
267
10.7 Proof of Theorem 10.7
270
10.8 Empirical Evaluation
274
11
Efﬁcient Solver for Sparse Additive Matrix Factorization
279
11.1 Problem Description
279
11.2 Efﬁcient Algorithm for SAMF
282
11.3 Experimental Results
284
12
MAP and Partially Bayesian Learning
294
12.1 Theoretical Analysis in Fully Observed MF
295
12.2 More General Cases
329
12.3 Experimental Results
332
Part IV
Asymptotic Theory
339
13
Asymptotic Learning Theory
341
13.1 Statistical Learning Machines
341
13.2 Basic Tools for Asymptotic Analysis
344
13.3 Target Quantities
346
13.4 Asymptotic Learning Theory for Regular Models
351
13.5 Asymptotic Learning Theory for Singular Models
366
13.6 Asymptotic Learning Theory for VB Learning
382
14
Asymptotic VB Theory of Reduced Rank Regression
385
14.1 Reduced Rank Regression
385
14.2 Generalization Properties
396
14.3 Insights into VB Learning
426
15
Asymptotic VB Theory of Mixture Models
429
15.1 Basic Lemmas
429
15.2 Mixture of Gaussians
434
15.3 Mixture of Exponential Family Distributions
443
15.4 Mixture of Bernoulli with Deterministic Components
451
16
Asymptotic VB Theory of Other Latent Variable Models
455
16.1 Bayesian Networks
455
16.2 Hidden Markov Models
461
16.3 Probabilistic Context-Free Grammar
466
16.4 Latent Dirichlet Allocation
470

viii
Contents
17
Uniﬁed Theory for Latent Variable Models
500
17.1 Local Latent Variable Model
500
17.2 Asymptotic Upper-Bound for VB Free Energy
504
17.3 Example: Average VB Free Energy of Gaussian Mixture Model
507
17.4 Free Energy and Generalization Error
511
17.5 Relation to Other Analyses
513
Appendix A
James–Stein Estimator
516
Appendix B
Metric in Parameter Space
520
Appendix C
Detailed Description of Overlap Method
525
Appendix D
Optimality of Bayesian Learning
527
Bibliography
529
Subject Index
540

Preface
Bayesian learning is a statistical inference method that provides estimators
and other quantities computed from the posterior distribution—the conditional
distribution of unknown variables given observed variables. Compared with
point estimation methods such as maximum likelihood (ML) estimation and
maximum a posteriori (MAP) learning, Bayesian learning has the following
advantages:
• Theoretically optimal.
The posterior distribution is what we can obtain best about the unknown
variables from observation. Therefore, Bayesian learning provides most
accurate predictions, provided that the assumed model is appropriate.
• Uncertainty information is available.
Sharpness of the posterior distribution indicates the reliability of
estimators. The credible interval, which can be computed from the posterior
distribution, provides probabilistic bounds of unknown variables.
• Model selection and hyperparameter estimation can be performed in a
single framework.
The marginal likelihood can be used as a criterion to evaluate how well a
statistical model (which is typically a combination of model and prior
distributions) ﬁts the observed data, taking account of the ﬂexibility of the
model as a penalty.
• Less prone to overﬁtting.
It was theoretically proven that Bayesian learning overﬁts the observation
noise less than MAP learning.
On the other hand, Bayesian learning has a critical drawback—computing
the posterior distribution is computationally hard in many practical models.
This is because Bayesian learning requires expectation operations or integral
computations, which cannot be analytically performed except for simple cases.
ix

x
Preface
Accordingly, various approximation methods, including deterministic and
sampling methods, have been proposed.
Variational Bayesian (VB) learning is one of the most popular deterministic
approximation methods to Bayesian learning. VB learning aims to ﬁnd the
closest distribution to the Bayes posterior under some constraints, which are
designed so that the expectation operation is tractable. The simplest and most
popular approach is the mean ﬁeld approximation where the approximate
posterior is sought in the space of decomposable distributions, i.e., groups
of unknown variables are forced to be independent of each other. In many
practical models, Bayesian learning is intractable jointly for all unknown
parameters, while it is tractable if the dependence between groups of parame-
ters is ignored. Such a case often happens because many practical models have
been constructed by combining simple models in which Bayesian learning is
analytically tractable. This property is called conditional conjugacy, and makes
VB learning computationally tractable.
Since its development, VB learning has shown good performance in many
applications. Its good aspects and downsides have been empirically observed
and qualitatively discussed. Some of those aspects seem inherited from full
Bayesian learning, while some others seem to be artifacts by forced indepen-
dence constraints. We have dedicated ourselves to theoretically clarifying the
behavior of VB learning quantitatively, which is the main topic of this book.
This book starts from the formulation of Bayesian learning methods. In
Part I, we introduce Bayesian learning and VB learning, emphasizing how
conjugacy and conditional conjugacy make the computation tractable. We also
brieﬂy introduce other approximation methods and relate them to VB learning.
In Part II, we derive algorithms of VB learning for popular statistical models,
on which theoretical analysis will be conducted in the subsequent parts.
We categorize the theory of VB learning into two parts, and exhibit them
separately. Part III focuses on nonasymptotic theory, where we do not assume
the availability of a large number of samples. This analysis so far has been
applied only to a class of bilinear models, but we can make detailed discus-
sions including analytic forms of global solutions and theoretical performance
guarantees. On the other hand, Part IV focuses on asymptotic theory, where
the number of observed samples is assumed to be large. This approach has
been applied to a broad range of statistical models, and successfully elucidated
the phase transition phenomenon of VB learning. As a practical outcome,
this analysis provides a guideline on how to set hyperparameters for different
purposes.

Preface
xi
Recently, a lot of variations of VB learning have been proposed, e.g., more
accurate inference methods beyond the mean ﬁeld approximation, stochastic
gradient optimization for big data analysis, and sampling based update rules for
automatic (black-box) inference to cope with general nonconjugate likelihoods
including deep neural networks. Although we brieﬂy introduce some of those
recent works in Part I, they are not in the central scope of this book. We rather
focus on the simplest mean ﬁeld approximation, of which the behavior has
been clariﬁed quantitatively by theory.
This book was completed under the support by many people. Shinichi
Nakajima deeply thanks Professor Klaus-Robert M¨uller and the members in
Machine Learning Group in Technische Universit¨at Berlin for their direct and
indirect support during the period of book writing. Special thanks go to Sergej
Dogadov, Hannah Marienwald, Ludwig Winkler, Dr. Nico G¨onitz, and Dr. Pan
Kessel, who reviewed chapters of earlier versions, found errors and typos,
provided suggestions to improve the presentation, and kept encouraging him
in proceeding book writing. The authors also thank Lauren Cowles and her
team in Cambridge University Press, as well as all other staff members who
contributed to the book production process, for their help, as well as their
patience on the delays in our manuscript preparation. Lauren Cowles, Clare
Dennison, Adam Kratoska, and Amy He have coordinated the project since its
proposal, and Harsha Vardhanan in SPi Global has managed the copy-editing
process with Andy Saff.
The book writing project was partially supported by the following orga-
nizations: the German Research Foundation (GRK 1589/1) by the Federal
Ministry of Education and Research (BMBF) under the Berlin Big Data Center
project (Phase 1: FKZ 01IS14013A and Phase 2: FKz 01IS18025A), the
Japan Society for the Promotion of Science (15K16050), and the International
Research Center for Neurointelligence (WPI-IRCN) at The University of
Tokyo Institutes for Advanced Study.

Nomenclature
a, b, c,. . . , A, B,C,. . .
: Scalars.
a, b, c, . . . (bold-faced small letters)
: Vectors.
A, B, C, . . . (bold-faced capital letters)
: Matrices.
A, B, C, . . . (calligraphic capital letters)
: Tensors or sets.
(·)l,m
: (l, m)th element of a matrix.
⊤
: Transpose of a matrix or vector.
tr(·)
: Trace of a matrix.
det (·)
: Determinant of a matrix.
⊙
: Hadamard (elementwise) product.
⊗
: Kronecker product.
×n
: n-mode tensor product.
|·|
: Absolute value of a scalar. It applies element-wise for a vector
or matrix.
sign(·)
: Sign operator such that sign(x) =
⎧⎪⎪⎨⎪⎪⎩
1
if x ≥0,
−1
otherwise.
It applies
elementwise for a vector or matrix.
{· · · }
: Set consisting of speciﬁed entities.
{· · · }D
: Dfold Cartesian product, i.e.,
XD ≡{(x1,. . . , xD)⊤; xd ∈X for d = 1,. . . , D}.
# (·)
: Cardinality (the number of entities) of a set.
R
: The set of all real numbers.
R+
: The set of all nonnegative real numbers.
R++
: The set of all positive real numbers.
RD
: The set of all D-dimensional real (column) vectors.
xii

Nomenclature
xiii
[·, ·]
: The set of real numbers in a range, i.e.,
[l, u] = {x ∈R; l ≤x ≤u}.
[·, ·]D
: The set of D-dimensional real vectors whose entries are in a
range, i.e., [l, u]D ≡{x ∈RD; l ≤xd ≤u for d = 1,. . . , D}.
RL×M
: The set of all L × M real matrices.
RM1×M2×···×MN
: The set of all M1 × M2 × · · · × MN real tensors.
I
: The set of all integers.
I++
: The set of all positive integers.
C
: The set of all complex numbers.
SD
: The set of all D × D symmetric matrices.
SD
+
: The set of all D × D positive semideﬁnite matrices.
SD
++
: The set of all D × D positive deﬁnite matrices.
DD
: The set of all D × D diagonal matrices.
DD
+
: The set of all D × D positive semideﬁnite diagonal matrices.
DD
++
: The set of all D × D positive deﬁnite diagonal matrices.
HK−1
N
: The set of all possible histograms for N samples and
K categories, i.e., HK−1
N
≡{x ∈{0,. . . , N}K; K
k=1 xk = N}.
ΔK−1
: The standard (K −1)-simplex, i.e.,
ΔK−1 ≡{θ ∈[0, 1]K; K
k=1 θk = 1}).
(a1,. . . , aM)
: Column vectors of A, i.e., A = (a1,. . . , aM) ∈RL×M.
(a1,. . . ,aL)
: Row vectors of A, i.e., A = (a1,. . . ,aL)⊤∈RL×M.
Diag(·)
: Diagonal matrix with speciﬁed diagonal elements, i.e.,
(Diag(x))l,m =
⎧⎪⎪⎨⎪⎪⎩
xl
if l = m,
0
otherwise.
diag(·)
: Column vector consisting of the diagonal entries of a matrix, i.e.,
(diag(X))l = Xl,l.
vec(·)
: Vectorization operator concatenating all column vectors of a matrix
into a long column vector, i.e., vec(A) = (a⊤
1 ,. . . , a⊤
M)⊤∈RLM
for a matrix A = (a1,. . . , aM) ∈RL×M.
ID
: D-dimensional (D × D) identity matrix.
Γ
: A diagonal matrix.
Ω
: An orthogonal matrix.
ek
: One of K expression, i.e., ek = (0,. . . , 0,
kth
	

1
, 0,. . . , 0

	
K
)⊤∈{0, 1}K.
1K
: K-dimensional vector with all elements equal to one, i.e.,
ek = (1,. . . , 1

	
K
)⊤.

xiv
Nomenclature
GaussD(μ, Σ)
: D-dimensional Gaussian distribution with mean
μ and covariance Σ.
MGaussD1,D2(M, Σ ⊗Ψ)
: D1 × D2 dimensional matrix variate Gaussian
distribution with mean M and covariance Σ ⊗Ψ.
Gamma(α, β)
: Gamma distribution with shape parameter α
and scale parameter β.
InvGamma(α, β)
: Inverse-Gamma distribution with shape parameter
α and scale parameter β.
WishartD(V, ν)
: D-dimensional Wishart distribution with scale
matrix V and degree of freedom ν.
InvWishartD(V, ν)
: D-dimensional inverse-Wishart distribution with
scale matrix V and degree of freedom ν.
Multinomial(θ, N)
: Multinomial distribution with event probabilities
θ and number of trials N.
Dirichlet(φ)
: Dirichlet distribution with concentration
parameters φ.
Prob(·)
: Probability of an event.
p(·), q(·)
: Probability distribution (probability mass function for discrete
random variables, and probability density function for
continuous random variables). Typically p is used for a
model distribution and q is used for the true distribution.
r(·)
: A trial distribution (a variable of a functional) for approximation.
⟨f(x)⟩p(x)
: Expectation value of f(x) over distribution p(x), i.e.,
⟨f(x)⟩p(x) ≡

f(x)p(x)dx.
·
: Estimator for an unknown variable, e.g., x and A are estimators
for a vector x and a matrix A, respectively.
Mean(·)
: Mean of a random variable.
Var(·)
: Variance of a random variable.
Cov(·)
: Covariance of a random variable.
KL(·||·)
: Kullbuck–Leibler divergence between distributions, i.e.,
KL(p||q) ≡

log p(x)
q(x)

p(x).
δ(μ;μ)
: Dirac delta function located at μ. It also denotes its
approximation (called Pseudo-delta function) with its
entropy ﬁnite.
GE
: Generalization error.
TE
: Training error.
F
: Free energy.

Nomenclature
xv
O( f(N))
: A function such that lim supN→∞|O( f(N))/f(N)| < ∞.
o( f(N))
: A function such that limN→∞o( f(N))/f(N) = 0.
Ω( f(N))
: A function such that lim infN→∞|Ω( f(N))/f(N)| > 0
ω( f(N))
: A function such that limN→∞|ω( f(N))/f(N)| = ∞.
Θ( f(N))
: A function such that lim supN→∞|Θ( f(N))/f(N)| < ∞
and lim infN→∞|Θ( f(N))/f(N)| > 0.
Op( f(N))
: A function such that lim supN→∞
Op( f(N))/f(N)
 < ∞
in probability.
op( f(N))
: A function such that limN→∞op( f(N))/f(N) = 0 in probability.
Ωp( f(N))
: A function such that lim infN→∞
Ωp( f(N))/f(N)
 > 0
in probability
ωp( f(N))
: A function such that limN→∞
ωp( f(N))/f(N)
 = ∞
in probability.
Θp( f(N))
: A function such that lim supN→∞
Θp( f(N))/f(N)
 < ∞
and lim infN→∞
Θp( f(N))/f(N)
 > 0 in probability.


Part I
Formulation


1
Bayesian Learning
Bayesian learning is an inference method based on the fundamental law
of probability, called the Bayes theorem. In this ﬁrst chapter, we introduce
the framework of Bayesian learning with simple examples where Bayesian
learning can be performed analytically.
1.1 Framework
Bayesian learning considers the following situation. We have observed a set
D of data, which are subject to a conditional distribution p(D|w), called the
model distribution, of the data given unknown model parameter w. Although
the value of w is unknown, vague information on w is provided as a prior
distribution p(w). The conditional distribution p(D|w) is also called the model
likelihood when it is seen as a function of the unknown parameter w.
1.1.1 Bayes Theorem and Bayes Posterior
Bayesian learning is based on the following basic factorization property of the
joint distribution p(D, w):
p(w|D)

	
posterior
p(D)

	
marginal
= p(D, w)

	
joint
= p(D|w)

	
likelihood
p(w)

	
prior
,
(1.1)
where the marginal distribution is given by
p(D) =

W
p(D, w)dw =

W
p(D|w)p(w)dw.
(1.2)
Here, the integration is performed in the domain W of the parameter w.
Note that, if the domain W is discrete, integration should be replaced with
3

4
1 Bayesian Learning
summation, i.e., for any function f(w),

W
f(w)dw →

w′∈W
f(w′).
The posterior distribution, the distribution of the unknown parameter w
given the observed data set D, is derived by dividing both sides of Eq. (1.1) by
the marginal distribution p(D):
p(w|D) = p(D, w)
p(D)
∝p(D, w).
(1.3)
Here, we emphasized that the posterior distribution is proportional to the joint
distribution p(D, w) because the marginal distribution p(D) is a constant (as
a function of w). In other words, the joint distribution is an unnormalized
posterior distribution. Eq. (1.3) is called the Bayes theorem, and the posterior
distribution computed exactly by Eq. (1.3) is called the Bayes posterior when
we distinguish it from its approximations.
Example 1.1
(Parametric density estimation) Assume that the observed data
D = {x(1),. . . , x(N)} consist of N independent and identically distributed (i.i.d.)
samples from the model distribution p(x|w). Then, the model likelihood is
given by p(D|w) = N
n=1 p(x(n)|w), and therefore, the posterior distribution
is given by
p(w|D) =
N
n=1 p(x(n)|w)p(w)
 N
n=1 p(x(n)|w)p(w)dw
∝
N

n=1
p(x(n)|w)p(w).
Example 1.2
(Parametric regression) Assume that the observed data D =
{(x(1), y(1)),. . . , (x(N), y(N))} consist of N i.i.d. input–output pairs from the
model distribution p(x, y|w) = p(y|x, w)p(x). Then, the likelihood function
is given by p(D|w) = N
n=1 p(y(n)|x(n), w)p(x(n)), and therefore, the posterior
distribution is given by
p(w|D) =
N
n=1 p(y(n)|x(n), w)p(w)
 N
n=1 p(y(n)|x(n), w)p(w)dw
∝
N

n=1
p(y(n)|x(n), w)p(w).
Note that the input distribution p(x) does not affect the posterior, and accord-
ingly is often ignored in practice.
1.1.2 Maximum A Posteriori Learning
Since the joint distribution p(D, w) is just the product of the likelihood
function and the prior distribution (see Eq. (1.1)), it is usually easy to

1.1 Framework
5
compute. Therefore, it is relatively easy to perform maximum a posteriori
(MAP) learning, where the parameters are point-estimated so that the posterior
probability is maximized, i.e.,
wMAP = argmax
w
p(w|D) = argmax
w
p(D, w).
(1.4)
MAP learning includes maximum likelihood (ML) learning,
wML = argmax
w
p(D|w),
(1.5)
as a special case with the ﬂat prior p(w) ∝1.
1.1.3 Bayesian Learning
On the other hand, Bayesian learning requires integration of the joint distri-
bution with respect to the parameter w, which is often computationally hard.
More speciﬁcally, performing Bayesian learning means computing at least one
of the following quantities:
Marginal likelihood (zeroth moment)
p(D) =

p(D, w)dw.
(1.6)
This quantity has been already introduced in Eq. (1.2) as the normalization
factor of the posterior distribution. As seen in Section 1.1.5 and subsequent
sections, the marginal likelihood plays an important role in model selection
and hyperparameter estimation.
Posterior mean (ﬁrst moment)
w = ⟨w⟩p(w|D) =
1
p(D)

w · p(D, w)dw,
(1.7)
where ⟨·⟩p denotes the expectation value over the distribution p, i.e., ⟨·⟩p(w) =

·p(w)dw. This quantity is also called the Bayesian estimator. The Bayesian
estimator or the model distribution with the Bayesian estimator plugged in (see
the plug-in predictive distribution (1.10)) can be the ﬁnal output of Bayesian
learning.
Posterior covariance (second moment)
Σw =

(w −w)(w −w)⊤
p(w|D) =
1
p(D)

(w −w)(w −w)⊤p(D, w)dw, (1.8)

6
1 Bayesian Learning
where ⊤denotes the transpose of a matrix or vector. This quantity provides
the credibility information, and is used to assess the conﬁdence level of the
Bayesian estimator.
Predictive distribution (expectation of model distribution)
p(Dnew|D) = p(Dnew|w)
p(w|D) =
1
p(D)

p(Dnew|w)p(D, w)dw,
(1.9)
where p(Dnew|w) denotes the model distribution on unobserved new data Dnew.
In the i.i.d. case such as Examples 1.1 and 1.2, it is sufﬁcient to compute the
predictive distribution for a single new sample Dnew = {x}.
Note that each of the four quantities (1.6) through (1.9) requires to compute the
expectation of some function f(w) over the unnormalized posterior distribution
p(D, w) on w, i.e.,

f(w)p(D, w)dw. Speciﬁcally, the marginal likelihood,
the posterior mean, and the posterior covariance are the zeroth, the ﬁrst, and
the second moments of the unnormalized posterior distribution, respectively.
The expectation is analytically intractable except for some simple cases, and
numerical computation is also hard when the dimensionality of the unknown
parameter w is high. This is the main bottleneck of Bayesian learning, with
which many approximation methods have been developed to cope.
It hardly happens that the ﬁrst moment (1.7) or the second moment (1.8)
are computationally tractable but the zeroth moment (1.6) is not. Accordingly,
we can say that performing Bayesian learning on the parameter w amounts to
obtaining the normalized posterior distribution p(w|D). It sometimes happens
that computing the predictive distribution (1.9) is still intractable even if the
zeroth, the ﬁrst, and the second moments can be computed based on some
approximation. In such a case, the model distribution with the Bayesian
estimator plugged in, called the plug-in predictive distribution,
p(Dnew|w),
(1.10)
is used for prediction in practice.
1.1.4 Latent Variables
So far, we introduced the observed data set D as a known variable, and the
model parameter w as an unknown variable. In practice, more varieties of
known and unknown variables can be involved.
Some probabilistic models have latent variables (or hidden variables) z,
which can be involved in the original model, or additionally introduced for

1.1 Framework
7
computational reasons. They are typically attributed to each of the observed
samples, and therefore have large degrees of freedom. However, they are just
additional unknown variables, and there is no reason in inference to distinguish
them from the model parameters w.1 The joint posterior over the parameters
and the latent variables is given by Eq. (1.3) with w and p(w) replaced with
w = (w, z) and p(w) = p(z|w)p(w), respectively.
Example 1.3
(Mixture models) A mixture model is often used for parametric
density estimation (Example 1.1). The model distribution is given by
p(x|w) =
K

k=1
αkp(x|τk),
(1.11)
where w = {αk, τk; αk ≥0, K
k=1 αk = 1}K
k=1 is the unknown parameters. The
mixture model (1.11) is the weighted sum of K distributions, each of which
is parameterized by the component parameter τk. The domain of the mixing
weights α = (α1,. . . , αK)⊤, also called as the mixture coefﬁcients, forms the
standard (K −1)-simplex, denoted by ΔK−1 ≡{α ∈RK
+; K
k=1 αk = 1} (see
Figure 1.1). Figure 1.2 shows an example of the mixture model with three
one-dimensional Gaussian components.
The likelihood,
p(D|w) =
N

n=1
p(x(n)|w),
=
N

n=1
⎛⎜⎜⎜⎜⎜⎝
K

k=1
αkp(x|τk)
⎞⎟⎟⎟⎟⎟⎠,
(1.12)
α1
α2
α3
α1 + α2 + α3 = 1
Figure 1.1 (K −1)-simplex, ΔK−1, for K = 3.
1 For this reason, the latent variables z and the model parameters w are also called local latent
variables and global latent variables, respectively.

8
1 Bayesian Learning
–2
–1
0
1
2
0
0.2
0.4
0.6
0.8
1
Figure 1.2 Gaussian mixture.
for N observed i.i.d. samples D = {x(1),. . . , x(N)} has O(KN) terms, which
makes even ML learning intractable. This intractability arises from the summa-
tion inside the multiplication in Eq. (1.12). By introducing latent variables, we
can turn this summation into a multiplication, and make Eq. (1.12) tractable.
Assume that each sample x belongs to a single component k, and is drawn
from p(x|τk). To describe the assignment, we introduce a latent variable
z ∈Z ≡{ek}K
k=1 associated with each observed sample x, where ek ∈{0, 1}K is
the K-dimensional binary vector, called the one-of-K representation, with one
at the kth entry and zeros at the other entries:
ek = (0,. . . , 0,
kth
	

1
, 0,. . . , 0

	
K
)⊤.
Then, we have the following model:
p(x, z|w) = p(x|z, w)p(z|w),
(1.13)
where
p(x|z, w) =
K

k=1
{p(x|τk)}zk ,
p(z|w) =
K

k=1
αzk
k .
The conditional distribution (1.13) on the observed variable x and the latent
variable z given the parameter w is called the complete likelihood.
Note that marginalizing the complete likelihood over the latent variable
recovers the original mixture model:
p(x|w) =

Z
p(x, z|w)dz =

z∈{ek}K
k=1
K

k=1
{αkp(x|τk)}zk =
K

k=1
αkp(x|τk).
This means that, if samples are generated from the model distribution (1.13),
and only x is recorded, the observed data follow the original mixture model
(1.11).

1.1 Framework
9
In the literature, latent variables tend to be marginalized out even in
MAP learning. For example, the expectation-maximization (EM) algorithm
(Dempster et al., 1977), a popular MAP solver for latent variable models,
seeks a (local) maximizer of the posterior distribution with the latent variables
marginalized out, i.e.,
wEM = argmax
w
p(w|D) = argmax
w

Z
p(D, w, z)dz.
(1.14)
However, we can also maximize the posterior jointly over the parameters and
the latent variables, i.e.,
(wMAP−hard,zMAP−hard) = argmax
w,z
p(w, z|D) = argmax
w,z
p(D, w, z).
(1.15)
For clustering based on the mixture model in Example 1.3, the EM algorithm
(1.14) gives a soft assignment, where the expectation value zEM ∈ΔK−1 ⊂
[0, 1]K is substituted into the joint distribution p(D, w, z), while the joint
maximization (1.15) gives the hard assignment, where the optimal assignment
zMAP−hard ∈{ek}K
k=1 ⊂{0, 1}K is looked for in the binary domain.
1.1.5 Empirical Bayesian Learning
In many practical cases, it is reasonable to use a prior distribution parame-
terized by hyperparameters κ. The hyperparameters can be tuned by hand or
based on some criterion outside the Bayesian framework. A popular method of
the latter is the cross validation, where the hyperparameters are tuned so that
an (preferably unbiased) estimator of the performance criterion is optimized.
In such cases, the hyperparameters should be treated as known variables when
Bayesian learning is performed.
On the other hand, the hyperparameters can be estimated within the
Bayesian framework. In this case, there is again no reason to distinguish the
hyperparameters from the other unknown variables (w, z). The joint posterior
over all unknown variables is given by Eq. (1.3) with w and p(w) replaced
with w = (w, κ, z) and p(w) = p(z|w)p(w|κ)p(κ), respectively, where p(κ) is
called a hyperprior. A popular approach, called empirical Bayesian (EBayes)
learning (Efron and Morris, 1973), applies Bayesian learning on w (and z) and
point-estimate κ, i.e.,
κEBayes = argmax
κ
p(D, κ) = argmax
κ
p(D|κ)p(κ),
where
p(D|κ) =

p(D, w, z|κ)dwdz.

10
1 Bayesian Learning
Here the marginal likelihood p(D|κ) is seen as the likelihood of the hyperpa-
rameter κ, and MAP learning is performed by maximizing the joint distribution
p(D, κ) of the observed data D and the hyperparameter κ, which can be seen as
an unnormalized posterior distribution of the hyperparameter. The hyperprior
is often assumed to be ﬂat: p(κ) ∝1.
With an appropriate design of priors, empirical Bayesian learning combined
with approximate Bayesian learning is often used for automatic relevance
determination (ARD), where irrelevant degrees of freedom of the statistical
model are automatically pruned out. Explaining the ARD property of approxi-
mate Bayesian learning is one of the main topics of theoretical analysis in Parts
III and IV.
1.2 Computation
Now, let us explain how Bayesian learning is performed in simple cases. We
start from introducing conjugacy, an important notion in performing Bayesian
learning.
1.2.1 Popular Distributions
Table 1.1 summarizes several distributions that are frequently used as a model
distribution (or likelihood function) p(D|w) or a prior distribution p(w) in
Bayesian learning. The domain X of the random variable x and the domain
W of the parameters w are shown in the table.
Some of the distributions in Table 1.1 have complicated function forms,
involving Beta or Gamma functions. However, such complications are mostly
in the normalization constant, and can often be ignored when it is sufﬁcient
to ﬁnd the shape of a function. In Table 1.1, the normalization constant is
separated by a dot, so that one can ﬁnd the simple main part. As will be
seen shortly, we often refer to the normalization constant when we need to
perform integration of a function, which is in the same form as the main part
of a popular distribution.
Below we summarize abbreviations of distributions:
GaussM(x; μ, Σ) ≡
1
(2π)M/2 det (Σ)1/2 · exp

−1
2 (x −μ)⊤Σ−1(x −μ)

,
(1.16)
Gamma(x; α, β) ≡
βα
Γ(α) · xα−1 exp(−βx),
(1.17)

Table 1.1 Popular distributions. The following notation is used: R : The set of all real numbers, R++ : The set of all
positive real numbers, I++ : The set of all positive integers, SM
++ : The set of all M × M positive deﬁnite matrices,
HK−1
N
≡{x ∈{0,. . . , N}K; K
k=1 xk = N} : The set of all possible histograms for N samples and K categories,
ΔK−1 ≡{θ ∈[0, 1]K; K
k=1 θk = 1} : The standard (K −1)-simplex, det (·) :Determinant of matrix, B(y, z) ≡
 1
0 ty−1(1 −t)z−1dt : Beta function, Γ(y) ≡
 ∞
0 ty−1 exp(−t)dt : Gamma function, and ΓM(y) ≡

T∈SM
++ det (T)y−(M+1)/2 exp(−tr(T))dT : Multivariate Gamma function.
Probability distribution
p(x|w)
x ∈X
w ∈W
Isotropic Gaussian
GaussM(x; μ, σ2IM) ≡
1
(2πσ2)M/2 · exp

−1
2σ2 ∥x −μ∥2 
x ∈RM
μ ∈RM, σ2 > 0
Gaussian
GaussM(x; μ, Σ) ≡
1
(2π)M/2 det(Σ)1/2 · exp

−1
2 (x −μ)⊤Σ−1 (x −μ)
 
x ∈RM
μ ∈RM, Σ ∈SM
++
Gamma
Gamma(x; α, β) ≡
βα
Γ(α) · xα−1 exp(−βx)
x > 0
α > 0, β > 0
Wishart
WishartM(X; V, ν) ≡
1
(2ν|V|)M/2ΓM( ν
2 ) · det (X)
ν−M−1
2
exp
!
−tr(V−1X)
2
"
X ∈SM
++
V ∈SM
++, ν > M −1
Bernoulli
Binomial1(x; θ) ≡θx(1 −θ)1−x
x ∈{0, 1}
θ ∈[0, 1]
Binomial
BinomialN(x; θ) ≡

N
x

θx(1 −θ)N−x
x ∈{0,. . . , N}
θ ∈[0, 1]
Multinomial
MultinomialK,N(x; θ) ≡N! · K
k=1(xk! )−1θxk
k
x ∈HK−1
N
θ ∈ΔK−1
Beta
Beta(x; a, b) ≡
1
B(a,b) · xa−1(1 −x)b−1
x ∈[0, 1]
a > 0, b > 0
Dirichlet
DirichletK(x; φ) ≡
Γ(K
k=1 φk)
K
k=1 Γ(φk) · K
k=1 xφk−1
k
x ∈ΔK−1
φ ∈RK
++

12
1 Bayesian Learning
WishartM(X; V, ν) ≡
1
(2ν|V|)M/2ΓM
 ν
2
 · det (X)
ν−M−1
2
exp

−tr(V−1X)
2

,
(1.18)
BinomialN(x; θ) ≡
N
x

· θx(1 −θ)N−x,
(1.19)
MultinomialK,N(x; θ) ≡N! ·
K

k=1
(xk! )−1θxk
k ,
(1.20)
Beta(x; a, b) ≡
1
B(a, b) · xa−1(1 −x)b−1,
(1.21)
DirichletK(x; φ) ≡Γ(K
k=1 φk)
K
k=1 Γ(φk)
·
K

k=1
xφk−1
k
.
(1.22)
The distributions in Table 1.1 are categorized into four groups, which are
separated by dashed lines. In each group, an upper distribution family is a
special case of a lower distribution family. Note that the following hold:
Gamma(x; α, β) = Wishart1

x; 1
2β, 2α

,
BinomialN(x; θ) = Multinomial2,N

(x, N −x)⊤; (θ, 1 −θ)⊤ 
,
Beta(x; a, b) = Dirichlet2

(x, 1 −x)⊤; (a, b)⊤ 
.
1.2.2 Conjugacy
Let us think about the function form of the posterior (1.3):
p(w|D) = p(D|w)p(w)
p(D)
∝p(D|w)p(w),
which is determined by the function form of the product of the model likeli-
hood p(D|w) and the prior p(w). Note that we here call the conditional p(D|w)
NOT the model distribution but the model likelihood, since we are interested
in the function form of the posterior, a distribution of the parameter w.
Conjugacy is deﬁned as the relation between the likelihood p(D|w) and the
prior p(w).
Deﬁnition 1.4
(Conjugate prior) A prior p(w) is called conjugate with a
likelihood p(D|w), if the posterior p(w|D) is in the same distribution family
as the prior.

1.2 Computation
13
1.2.3 Posterior Distribution
Here, we introduce computation of the posterior distribution in simple cases
where a conjugate prior exists and is adopted.
Isotropic Gaussian Model
Let us compute the posterior distribution for the isotropic Gaussian model:
p(x|w) = GaussM(x; μ, σ2IM) =
1
(2πσ2)M/2 · exp

−1
2σ2 ∥x −μ∥2

.
(1.23)
The likelihood for N i.i.d. samples D = {x(1),. . . , x(N)} is written as
p(D|w) =
N

n=1
p(x(n)|w) =
exp

−1
2σ2
N
n=1 ∥x(n) −μ∥2 
(2πσ2)MN/2
.
(1.24)
Gaussian Likelihood As noted in Section 1.2.2, we should see Eq. (1.24),
which is the distribution of observed data D, as a function of the parameter
w. Naturally, the function form depends on which parameters are estimated in
the Bayesian way. The isotropic Gaussian has two parameters w = (μ, σ2),
and we ﬁrst consider the case where the variance parameter σ2 is known,
and the posterior of the mean parameter μ is estimated, i.e., we set w = μ.
This case contains the case where σ2 is unknown but point-estimated in the
empirical Bayesian procedure or tuned outside the Bayesian framework, e.g.,
by performing cross-validation (we set w = μ, κ = σ2 in the latter case).
Omitting the constant (with respect to μ), the likelihood (1.24) can be
written as
p(D|μ) ∝exp
⎛⎜⎜⎜⎜⎜⎝−1
2σ2
N

n=1
∥x(n) −μ∥2
⎞⎟⎟⎟⎟⎟⎠
∝exp
⎛⎜⎜⎜⎜⎜⎝−1
2σ2
N

n=1
∥(x(n) −x) + (x −μ)∥2
⎞⎟⎟⎟⎟⎟⎠
= exp
⎛⎜⎜⎜⎜⎜⎝−1
2σ2
⎛⎜⎜⎜⎜⎜⎝
N

n=1
∥x(n) −x∥2 + N∥x −μ∥2
⎞⎟⎟⎟⎟⎟⎠
⎞⎟⎟⎟⎟⎟⎠
∝exp
!
−N
2σ2
###μ −x
###2"
∝GaussM

μ; x, σ2
N IM

,
(1.25)
where x =
1
N
N
n=1 x(n) is the sample mean. Note that we omitted the factor
exp $ −
1
2σ2
N
n=1 ∥x(n) −x∥2% as a constant in the fourth equation.

14
1 Bayesian Learning
The last equation (1.25) implies that, as a function of the mean parameter μ,
the model likelihood p(D|μ) has the same form as the isotropic Gaussian with
mean x and variance σ2
N . Eq. (1.25) also implies that the ML estimator for the
mean parameter is given by
μML = x.
Thus, we found that the likelihood function for the mean parameter of the
isotropic Gaussian is in the Gaussian form. This comes from the following
facts:
• The isotropic Gaussian model for a single sample x is in the Gaussian form
also as a function of the mean parameter, i.e., GaussM(x; μ, σ2IM) ∝
GaussM(μ; x, σ2IM).
• The isotropic Gaussians are multiplicatively closed, i.e., the product of
isotropic Gaussians with different means is a Gaussian: p(D|μ) ∝
N
n=1 GaussM(μ; x(n), σ2IM) ∝GaussM

μ; x, σ2
N IM
 
.
Since the isotropic Gaussian is multiplicatively closed and the likelihood
(1.25) is in the Gaussian form, the isotropic Gaussian prior must be conjugate.
Let us choose the isotropic Gaussian prior,
p(μ|μ0, σ2
0) = GaussM(μ; μ0, σ2
0IM) ∝exp
⎛⎜⎜⎜⎜⎝−1
2σ2
0
∥μ −μ0∥2
⎞⎟⎟⎟⎟⎠,
for hyperparameters κ = (μ0, σ2
0). Then, the function form of the posterior is
given by
p(μ|D, μ0, σ2
0) ∝p(D|μ)p(μ|μ0, σ2
0)
∝GaussM

μ; x, σ2
N

GaussM(μ; μ0, σ2
0)
∝exp
⎛⎜⎜⎜⎜⎝−N
2σ2
###μ −x
###2 −
1
2σ2
0
###μ −μ0
###2
⎞⎟⎟⎟⎟⎠
∝exp
⎛⎜⎜⎜⎜⎜⎜⎝−Nσ−2 + σ−2
0
2
######μ −Nσ−2x + σ−2
0 μ0
Nσ−2 + σ−2
0
######
2⎞⎟⎟⎟⎟⎟⎟⎠
∝GaussM
⎛⎜⎜⎜⎜⎝μ; Nσ−2x + σ−2
0 μ0
Nσ−2 + σ−2
0
,
1
Nσ−2 + σ−2
0
⎞⎟⎟⎟⎟⎠.
Therefore, the posterior is
p(μ|D, μ0, σ2
0) = GaussM
⎛⎜⎜⎜⎜⎝μ; Nσ−2x + σ−2
0 μ0
Nσ−2 + σ−2
0
,
1
Nσ−2 + σ−2
0
⎞⎟⎟⎟⎟⎠.
(1.26)

1.2 Computation
15
Note that the equality holds in Eq. (1.26). We omitted constant factors in the
preceding derivation. But once the function form of the posterior is found,
the normalization factor is unique. If the function form coincides with one of
the well-known distributions (e.g., ones given in Table 1.1), one can ﬁnd the
normalization constant (from the table) without any further computation.
Multiplicative closedness of a function family of the model likelihood
is essential in performing Bayesian learning. Such families are called the
exponential family:
Deﬁnition 1.5
(Exponential families) A family of distributions is called the
exponential family if it is written as
p(x|w) = p(t|η) = exp

η⊤t −A(η) + B(t)
 
,
(1.27)
where t = t(x) is a function, called sufﬁcient statistics, of the random variable
x, and η = η(w) is a function, called natural parameters, of the parameter w.
The essential property of the exponential family is that the interaction
between the random variable and the parameter occurs only in the log linear
form, i.e., exp $η⊤t%. Note that, although A(·) and B(·) are arbitrary functions,
A(·) does not depend on t, and B(·) does not depend on η.
Assume that N observed samples D = (t(1),. . . , t(N)) = (t(x(1)),. . . , t(x(N)))
are drawn from the exponential family distribution (1.27). If we use the
exponential family prior p(η)
=
exp

η⊤t(0) −A0(η) + B0(t(0))
 
, then the
posterior is given as an exponential family distribution with the same set of
natural parameters η:
p(η|D) = exp
⎛⎜⎜⎜⎜⎜⎝η⊤
N

n=0
t(n) −A′(η) + B′(D)
⎞⎟⎟⎟⎟⎟⎠,
where A′(η) and B′(D) are a function of η and a function of D, respectively.
Therefore, the conjugate prior for the exponential family distribution is the
exponential family with the same natural parameters η.
All distributions given in Table 1.1 are exponential families. For example,
the sufﬁcient statistics and the natural parameters for the univariate Gaussian
are given by η = ( μ
σ2 , −1
2σ2 )⊤and t = (x, x2)⊤, respectively. The mixture model
(1.11) is a common nonexponential family distribution.
Gamma Likelihood Next we consider the posterior distribution of the vari-
ance parameter σ2 with the mean parameter regarded as a constant, i.e.,
w = σ2.

16
1 Bayesian Learning
Omitting the constants (with respect to σ2) of the model likelihood (1.24),
we have
p(D|σ2) ∝(σ2)−MN/2 exp
⎛⎜⎜⎜⎜⎜⎝−1
2σ2
N

n=1
∥x(n) −μ∥2
⎞⎟⎟⎟⎟⎟⎠.
If we see the likelihood as a function of the inverse of σ2, we ﬁnd that it is
proportional to the Gamma distribution:
p(D|σ−2) ∝(σ−2)MN/2 exp
⎛⎜⎜⎜⎜⎜⎝−
⎛⎜⎜⎜⎜⎜⎝
1
2
N

n=1
∥x(n) −μ∥2
⎞⎟⎟⎟⎟⎟⎠σ−2
⎞⎟⎟⎟⎟⎟⎠
∝Gamma
⎛⎜⎜⎜⎜⎜⎝σ−2; MN
2
+ 1, 1
2
N

n=1
∥x(n) −μ∥2
⎞⎟⎟⎟⎟⎟⎠.
(1.28)
Since the mode of the Gamma distribution is known as argmaxx
Gamma (x; α, β) =
α−1
β , Eq. (1.28) implies that the ML estimator for the
variance parameter is given by
σ2 ML =
1
σ−2 ML =
1
2
N
n=1 ∥x(n) −μ∥2
MN
2 + 1 −1
=
1
MN
N

n=1
∥x(n) −μ∥2.
Now we found that the model likelihood of the isotropic Gaussian is
in the Gamma form as a function of the inverse variance σ−2. Since the
Gamma distribution is in the exponential family and multiplicatively closed,
the Gamma prior is conjugate.
If we use the Gamma prior
p(σ−2|α0, β0) = Gamma(σ−2; α0, β0) ∝(σ−2)α0−1 exp(−β0σ−2)
with hyperparameters κ = (α0, β0), the posterior can be written as
p(σ−2|D, α0, β0) ∝p(D|σ−2)p(σ−2|α0, β0)
∝Gamma
⎛⎜⎜⎜⎜⎜⎝σ−2; MN
2
+ 1, 1
2
N

n=1
∥x(n) −μ∥2
⎞⎟⎟⎟⎟⎟⎠Gamma(σ−2; α0, β0)
∝(σ−2)MN/2+α0−1 exp
⎛⎜⎜⎜⎜⎜⎝−
⎛⎜⎜⎜⎜⎜⎝
1
2
N

n=1
∥x(n) −μ∥2 + β0
⎞⎟⎟⎟⎟⎟⎠σ−2
⎞⎟⎟⎟⎟⎟⎠,
and therefore
p(σ−2|D, α0, β0) = Gamma
⎛⎜⎜⎜⎜⎜⎝σ−2; MN
2
+ α0, 1
2
N

n=1
∥x(n) −μ∥2 + β0
⎞⎟⎟⎟⎟⎟⎠. (1.29)

1.2 Computation
17
Isotropic Gauss-Gamma Likelihood Finally, we consider the general case
where both the mean and variance parameters are unknown, i.e., w = (μ, σ−2).
The likelihood is written as
p(D|μ, σ−2) ∝(σ−2)MN/2 exp
⎛⎜⎜⎜⎜⎜⎝−
⎛⎜⎜⎜⎜⎜⎝
1
2
N

n=1
∥x(n) −μ∥2
⎞⎟⎟⎟⎟⎟⎠σ−2
⎞⎟⎟⎟⎟⎟⎠
= (σ−2)MN/2 exp
⎛⎜⎜⎜⎜⎝−
⎛⎜⎜⎜⎜⎝
N∥μ −x∥2
2
+
N
n=1 ∥x(n) −x∥2
2
⎞⎟⎟⎟⎟⎠σ−2
⎞⎟⎟⎟⎟⎠
∝GaussGammaM
⎛⎜⎜⎜⎜⎝μ, σ−2
x, NIM, M(N −1)
2
+ 1,
N
n=1 ∥x(n) −x∥2
2
⎞⎟⎟⎟⎟⎠,
where
GaussGammaM(x, τ|μ, λIM, α, β)
≡GaussM(x|μ, (τλ)−1IM) · Gamma(τ|α, β)
=
exp

−τλ
2 ∥x −μ∥2 
(2π(τλ)−1)M/2
·
βα
Γ(α) τα−1 exp(−βτ)
=
βα
(2π/λ)M/2Γ(α) τα+ M
2 −1 exp

−
λ∥x −μ∥2
2
+ β

τ

is the isotropic Gauss-Gamma distribution on the random variable x ∈RM,
τ > 0 with parameters μ ∈RM, λ > 0, α > 0, β > 0.
Note that, although the isotropic Gauss-Gamma distribution is the product
of an isotropic Gaussian distribution and a Gamma distribution, the random
variables x and τ are not independent of each other. This is because the
isotropic Gauss-Gamma distribution is a hierarchical model p(x|τ)p(τ), where
the variance parameter σ2 = (τλ)−1 for the isotropic Gaussian depends on the
random variable τ of the Gamma distribution.
Since the isotropic Gauss-Gamma distribution is multiplicatively closed, it
is a conjugate prior. Choosing the isotropic Gauss-Gamma prior
p(μ, σ−2|μ0, λ0, α0, β0) = GaussGammaM(μ, σ−2|μ0, λ0IM, α0, β)
∝(σ−2)α0+ M
2 −1 exp

−
λ0∥μ −μ0∥2
2
+ β0

σ−2

with hyperparameters κ = (μ0, λ0, α0, β0), the posterior is given by
p(μ, σ−2|D, κ) ∝p(D|μ, σ−2)p(μ, σ−2|κ)
∝GaussGammaM
⎛⎜⎜⎜⎜⎝μ, σ−2
x, NIM, M(N −1)
2
+ 1,
N
n=1 ∥x(n) −x∥2
2
⎞⎟⎟⎟⎟⎠
· GaussGammaM(μ, σ−2|μ0, λ0IM, α0, β)

18
1 Bayesian Learning
∝(σ−2)MN/2 exp
⎛⎜⎜⎜⎜⎝−
⎛⎜⎜⎜⎜⎝
N∥μ −x∥2
2
+
N
n=1 ∥x(n) −x∥2
2
⎞⎟⎟⎟⎟⎠σ−2
⎞⎟⎟⎟⎟⎠
· (σ−2)α0+ M
2 −1 exp

−
λ0∥μ −μ0∥2
2
+ β0

σ−2

∝(σ−2)M(N+1)/2+α0−1
· exp
!
−
!
N∥μ−x∥2+λ0∥μ−μ0∥2
2
+
N
n=1 ∥x(n)−x∥2
2
+ β0
"
σ−2
"
∝(σ−2)α+ M
2 −1 exp
⎛⎜⎜⎜⎜⎜⎜⎝−
⎛⎜⎜⎜⎜⎜⎜⎝
λ
###μ −μ
###2
2
+ β
⎞⎟⎟⎟⎟⎟⎟⎠σ−2
⎞⎟⎟⎟⎟⎟⎟⎠,
where
μ = Nx + λ0μ0
N + λ0
,
λ = N + λ0,
α = MN
2
+ α0,
β =
N
n=1 ∥x(n) −x∥2
2
+ Nλ0∥x −μ0∥2
2(N + λ0)
+ β0.
Thus, the posterior is obtained as
p(μ, σ−2|D, κ) = GaussGammaM(μ, σ−2|μ,λIM,α,β).
(1.30)
Although the Gauss-Gamma distribution seems a bit more complicated
than the ones in Table 1.1, its moments are known. Therefore, Bayesian
learning with a conjugate prior can be analytically performed also when both
parameters w = (μ, σ−2) are estimated.
Gaussian Model
Bayesian learning can be performed for a general Gaussian model in a
similar fashion to the isotropic case. Consider the M-dimensional Gaussian
distribution,
p(x|w) = GaussM(x; μ, Σ) ≡
1
(2π)M/2 det (Σ)1/2 · exp

−1
2(x −μ)⊤Σ−1(x −μ)

(1.31)
with mean and covariance parameters w = (μ, Σ). The likelihood for N i.i.d.
samples D = {x(1),. . . , x(N)} is written as
p(D|w) =
N

n=1
p(x(n)|w) =
exp

−1
2
N
n=1(x(n) −μ)⊤Σ−1(x(n) −μ)
 
(2π)NM/2 det (Σ)N/2
.
(1.32)

1.2 Computation
19
Gaussian Likelihood Let us ﬁrst compute the posterior distribution on the
mean parameter μ, with the covariance parameter regarded as a known
constant. In this case, the likelihood can be written as
p(D|μ) ∝exp
⎛⎜⎜⎜⎜⎜⎝−1
2
N

n=1
(x(n) −μ)⊤Σ−1(x(n) −μ)
⎞⎟⎟⎟⎟⎟⎠
∝exp

−1
2
N

n=1

(x(n) −x) + (x −μ)
 ⊤· Σ−1 
(x(n) −x) + (x −μ)
 
= exp

−
1
2σ2

N

n=1
(x(n) −x)⊤Σ−1(x(n) −x) + N(x −μ)⊤Σ−1(x −μ)

∝exp
!
−N
2 (μ −x)⊤Σ−1(μ −x)
"
∝GaussM

μ; x, 1
N Σ

.
(1.33)
Therefore, with the conjugate Gaussian prior
p(μ|μ0, Σ0) = GaussM(μ; μ0, Σ0) ∝exp

−1
2(μ −μ0)⊤Σ−1
0 (μ −μ0)

,
with hyperparameters κ = (μ0, Σ0), the posterior is written as
p(μ|D, μ0, Σ0) ∝p(D|μ)p(μ|μ0, Σ0)
∝GaussM

μ; x, 1
N Σ

GaussM(μ; μ0, Σ0)
∝exp
⎛⎜⎜⎜⎜⎝−N(μ −x)⊤Σ−1(μ −x) + (μ −μ0)⊤Σ−1
0 (μ −μ0)
2
⎞⎟⎟⎟⎟⎠
∝exp
⎛⎜⎜⎜⎜⎜⎜⎝−
$μ −μ%⊤Σ
−1 $μ −μ%
2
⎞⎟⎟⎟⎟⎟⎟⎠,
where
μ =

NΣ−1 + Σ−1
0
 −1 
NΣ−1x + Σ−1
0 μ0
 
,
Σ =

NΣ−1 + Σ−1
0
 −1 .
Thus, we have
p(μ|D, μ0, Σ0) = GaussM

μ;μ, Σ
 
.
(1.34)

20
1 Bayesian Learning
Wishart Likelihood If we see the mean parameter μ as a given constant,
the model likelihood (1.32) can be written as follows, as a function of the
covariance parameter Σ:
p(D|Σ−1) ∝det

Σ−1 N/2 exp
⎛⎜⎜⎜⎜⎝−
N
n=1(x(n) −μ)⊤Σ−1(x(n) −μ)
2
⎞⎟⎟⎟⎟⎠
∝det

Σ−1 N/2 exp
⎛⎜⎜⎜⎜⎜⎜⎝−
tr
N
n=1(x(n) −μ)(x(n) −μ)⊤Σ−1 
2
⎞⎟⎟⎟⎟⎟⎟⎠
∝WishartM
⎛⎜⎜⎜⎜⎜⎜⎜⎝Σ−1;
⎛⎜⎜⎜⎜⎜⎝
N

n=1
(x(n) −μ)(x(n) −μ)⊤
⎞⎟⎟⎟⎟⎟⎠
−1
, M + N + 1
⎞⎟⎟⎟⎟⎟⎟⎟⎠.
Here, as in the isotropic Gaussian case, we computed the distribution on the
inverse Σ−1 of the covariance parameter. With the Wishart distribution
p(Σ−1|V0, ν0) = WishartM(Σ−1; V0, ν0)
=
1
(2ν0 det (V0))M/2 ΓM
 ν0
2
 · det

Σ−1 ν0−M−1
2
exp
⎛⎜⎜⎜⎜⎝−tr(V−1
0 Σ−1)
2
⎞⎟⎟⎟⎟⎠
for hyperparameters κ
=
(V0, ν0) as a conjugate prior, the posterior is
computed as
p(Σ−1|D, V0, ν0) ∝p(D|Σ−1)p(Σ−1|V0, ν0)
∝WishartM
⎛⎜⎜⎜⎜⎜⎜⎜⎝Σ−1;
⎛⎜⎜⎜⎜⎜⎝
N

n=1
(x(n) −μ)(x(n) −μ)⊤
⎞⎟⎟⎟⎟⎟⎠
−1
, M + N + 1
⎞⎟⎟⎟⎟⎟⎟⎟⎠
· WishartM(Σ−1; V0, ν0)
∝det

Σ−1 N
2 exp
⎛⎜⎜⎜⎜⎜⎜⎝−
tr
N
n=1(x(n) −μ)(x(n) −μ)⊤ 
Σ−1 
2
⎞⎟⎟⎟⎟⎟⎟⎠
· det

Σ−1 ν0−M−1
2
exp
⎛⎜⎜⎜⎜⎜⎜⎝−
tr

V−1
0 Σ−1 
2
⎞⎟⎟⎟⎟⎟⎟⎠
∝det

Σ−1 ν0−M+N−1
2
exp
!
−
tr((N
n=1(x(n)−μ)(x(n)−μ)⊤+V−1
0 )Σ−1)
2
"
.
Thus we have
p(Σ−1|D, V0, ν0)
= WishartM
⎛⎜⎜⎜⎜⎜⎜⎜⎝Σ−1;
⎛⎜⎜⎜⎜⎜⎝
N

n=1
(x(n) −μ)(x(n) −μ)⊤+ V−1
0
⎞⎟⎟⎟⎟⎟⎠
−1
, N + ν0
⎞⎟⎟⎟⎟⎟⎟⎟⎠. (1.35)

1.2 Computation
21
Note that the Wishart distribution can be seen as a multivariate extension of
the Gamma distribution and is reduced to the Gamma distribution for M = 1:
Wishart1 (x; V, ν) = Gamma (x; ν/2, 1/(2V)) .
Gauss-Wishart Likelihood When both parameters w
=
(μ, Σ−1) are
unknown, the model likelihood (1.32) is seen as
p(D|μ, Σ−1) ∝det

Σ−1 N/2 exp
!
−
N
n=1(x(n)−μ)⊤Σ−1(x(n)−μ)
2
"
∝det

Σ−1 N/2 exp
!
−
tr(N
n=1(x(n)−μ)(x(n)−μ)⊤Σ−1)
2
"
∝det

Σ−1 N/2 exp

−
tr
N
n=1((x(n)−x)+(x−μ))((x(n)−x)+(x−μ))
⊤Σ−1 
2

∝det

Σ−1 N/2 exp
!
−
tr(N(μ−x)(μ−x)⊤+N
n=1(x(n)−x)(x(n)−x)⊤)Σ−1)
2
"
∝GaussWishartM

μ, Σ−1; x, N,
N
n=1(x(n) −x)(x(n) −x)⊤ −1 , M + N
 
,
where
GaussWishartM(x, Λ|μ, λ, V, ν)
≡GaussM(x|μ, (λΛ)−1)WishartM(Λ|V, ν)
=
exp

−λ
2 (x −μ)⊤Λ (x −μ)
 
(2π)M/2det(λΛ)−1/2
·
det (Λ)
ν−M−1
2
exp
!
−tr(V−1Λ)
2
"
(2ν det (V))M/2ΓM
 ν
2
 
=
λM/2
(2ν+1π det(V))M/2ΓM( ν
2) det (Λ)
ν−M
2 exp
!
−
tr((λ(x−μ)(x−μ)⊤+V−1)Λ)
2
"
is the Gauss–Wishart distribution on the random variables x ∈RM, Λ ∈SM
++
with parameters μ ∈RM, λ > 0, V ∈SM
++, ν > M −1.
With the conjugate Gauss–Wishart prior,
p(μ, Σ−1|μ0, λ0, α0, β0) = GaussWishartM(μ, Σ−1|μ0, λ0, V0, ν0)
∝det

Σ−1 ν−M
2 exp

−
tr

(λ0(μ−μ0)(μ−μ0)
⊤+V−1
0 )Σ−1 
2

with hyperparameters κ = (μ0, λ0, V0, ν0), the posterior is written as
p(μ, Σ−1|D, κ) ∝p(D|μ, Σ−1)p(μ, Σ−1|κ)
∝GaussWishartM

μ, Σ−1; x, N,
N
n=1(x(n) −x)(x(n) −x)⊤ −1 , M + N
 
· GaussWishartM(μ, Σ−1|μ0, λ0, V0, ν0)

22
1 Bayesian Learning
∝det

Σ−1 N/2 exp
!
−
tr(N(μ−x)(μ−x)⊤+N
n=1(x(n)−x)(x(n)−x)⊤)Σ−1)
2
"
· det

Σ−1 ν0−M
2
exp

−
tr

(λ0(μ−μ0)(μ−μ0)
⊤+V−1
0 )Σ−1 
2

∝det

Σ−1 ν−M
2 exp
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
−tr
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
!
λ $μ −μ% $μ −μ%⊤V
−1"
Σ−1
2
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
,
where
μ = Nx + λ0μ0
N + λ0
,
λ = N + λ0,
V =
 N
n=1(x(n) −x)(x(n) −x)⊤+
Nλ0
N+λ0 (x −μ0)(x −μ0)⊤+ V−1
0
 −1,
ν = N + ν0.
Thus, we have the posterior distribution as the Gauss–Wishart distribution:
p(μ, Σ−1|D, κ) = GaussWishartM

μ, Σ−1|μ,λ, V,ν
 
.
(1.36)
Linear Regression Model
Consider the linear regression model, where an input variable x ∈RM and
an output variable y ∈R are assumed to satisfy the following probabilistic
relation:
y = a⊤x + ε,
(1.37)
p(ε|σ2) = Gauss1(ε; 0, σ2) =
1
√
2πσ2 · exp

−ε2
2σ2

.
(1.38)
Here a and σ2 are called the regression parameter and the noise variance
parameter, respectively. By substituting ε = y −a⊤x, which is obtained from
Eq. (1.37), into Eq. (1.38), we have
p(y|x, w) = Gauss1(y; a⊤x, σ2) =
1
√
2πσ2 · exp

−(y −a⊤x)2
2σ2

.
The likelihood function for N observed i.i.d.2 samples,
D = (y, X),
2 In the context of regression, i.i.d. usually means that the observation noise ε(n) = y(n) −a⊤x(n) is
independent for different samples, i.e., p({ε(n)}N
n=1) = N
n=1 p(ε(n)), and the independence
between the input (x(1),. . . , x(N)), i.e., p({x(n)}N
n=1) = N
n=1 p(x(n)), is not required.

1.2 Computation
23
is given by
p(D|w) =
1
(2πσ2)N/2 · exp

−∥y −Xa∥2
2σ2

,
(1.39)
where we deﬁned
y = (y(1),. . . , y(N))⊤∈RN,
X = (x(1),. . . , x(N))⊤∈RN×M.
Gaussian Likelihood The computation of the posterior is similar to the
isotropic Gaussian case. As in Section 1.2.3, we ﬁrst consider the case where
only the regression parameter a is estimated, with the noise variance parameter
σ2 regarded as a known constant.
One can guess that the likelihood (1.39) is Gaussian as a function of a, since
it is an exponential of a concave quadratic function. Indeed, by expanding the
exponent and completing the square for a, we obtain
p(D|a) ∝exp

−∥y −Xa∥2
2σ2

∝exp
!
−(a−(X⊤X)−1X⊤y)
⊤X⊤X(a−(X⊤X)−1X⊤y)
2σ2
"
∝GaussM

a; (X⊤X)−1X⊤y, σ2(X⊤X)−1 
.
(1.40)
Eq. (1.40) implies that, when X⊤X is nonsingular (i.e., its inverse exists), the
ML estimator for a is given by
aML = (X⊤X)−1X⊤y.
(1.41)
Therefore, with the conjugate Gaussian prior
p(a|a0, Σ0) = GaussM(a; a0, Σ0) ∝exp

−1
2(a −a0)⊤Σ−1
0 (a −a0)

for hyperparameters κ = (a0, Σ0), the posterior is Gaussian:
p(a|D, a0, Σ0) ∝p(D|a)p(a|a0, Σ0)
∝GaussM

a; a0, 1
N σ2(X⊤X)−1 
GaussM(a; a0, Σ0)
∝exp
⎛⎜⎜⎜⎜⎜⎜⎝−
(a−(X⊤X)−1 X⊤y)
⊤X⊤X(a−(X⊤X)−1 X⊤y)
σ2
+(a−a0)⊤Σ−1
0 (a−a0)
2
⎞⎟⎟⎟⎟⎟⎟⎠
∝exp
⎛⎜⎜⎜⎜⎜⎜⎝−
$a −a%⊤Σ
−1
a
$a −a%
2
⎞⎟⎟⎟⎟⎟⎟⎠,

24
1 Bayesian Learning
where
a =
 X⊤X
σ2
+ Σ−1
0
−1  X⊤y
σ2 + Σ−1
0 a0

,
Σa =
 X⊤X
σ2
+ Σ−1
0
−1
.
Thus we have
p(a|D, a0, Σ0) = GaussM

a;a, Σa
 
.
(1.42)
Gamma Likelihood When only the noise variance parameter σ2 is unknown,
the model likelihood (1.39) is in the Gamma form, as a function of the
inverse σ−2:
p(D|σ−2) ∝(σ−2)NM/2 exp

−∥y −Xa∥2
2
σ−2

∝Gamma

σ−2; NM
2
+ 1, ∥y −Xa∥2
2

,
(1.43)
which implies that the ML estimator is
σ2 ML =
1
σ−2 ML =
1
MN
N

n=1
∥y −Xa∥2 .
With the conjugate Gamma prior
p(σ−2|α0, β0) = Gamma(σ−2; α0, β0) ∝(σ−2)α0−1 exp(−β0σ−2)
with hyperparameters κ = (α0, β0), the posterior is computed as
p(σ−2|D, α0, β0) ∝p(D|σ−2)p(σ−2|α0, β0)
∝Gamma

σ−2; MN
2
+ 1, 1
2 ∥y −Xa∥2

Gamma(σ−2; α0, β0)
∝(σ−2)MN/2+α0−1 exp

−
1
2 ∥y −Xa∥2 + β0

σ−2

.
Therefore,
p(σ−2|D, α0, β0) = Gamma

σ−2; MN
2
+ α0, 1
2 ∥y −Xa∥2 + β0

.
(1.44)
Gauss-Gamma Likelihood When we estimate both parameters w = (a, σ−2),
the likelihood (1.39) is written as

1.2 Computation
25
p(D|a, σ−2) ∝(σ−2)NM/2 exp
!
−∥y−Xa∥2
2
σ−2
"
∝(σ−2)NM/2 exp

−

a−aML ⊤X⊤X

a−aML 
+∥y−XaML∥2
2
σ−2

∝GaussGammaM
!
a, σ−2;aML, X⊤X, M(N−1)
2
+ 1, ∥y−XaML∥2
2
"
,
where aML is the ML estimator, given by Eq. (1.41), for the regression
parameter, and
GaussGammaM(x, τ|μ, Λ, α, β)
≡GaussM(x|μ, (τΛ)−1) · Gamma(τ|α, β)
=
exp(−τ
2 (x−μ)⊤Λ(x−μ))
(2πτ−1)M/2 det(Λ)−1/2 ·
βα
Γ(α) τα−1 exp(−βτ)
=
βα
(2π)M/2 det(Λ)−1/2Γ(α) τα+ M
2 −1 exp

−
 (x−μ)⊤Λ(x−μ)
2
+ β
 
τ
 
is the (general) Gauss-Gamma distribution on the random variable x ∈RM,
τ > 0 with parameters μ ∈RM, Λ ∈SM
++, α > 0, β > 0. With the conjugate
Gauss-Gamma prior
p(a, σ−2|κ) = GaussGammaM(a, σ−2|μ0, Λ0, α0, β0)
∝(σ−2)α0+ M
2 −1 exp

−
 (a−μ0)⊤Λ0(a−μ0)
2
+ β0
 
σ−2 
for hyperparameters κ = (μ0, Λ0, α0, β0), the posterior is computed as
p(a, σ−2|D, κ) ∝p(D|a, σ−2)p(a, σ−2|κ)
∝GaussGammaM
!
a, σ−2;aML, X⊤X, M(N−1)
2
+ 1, ∥y−XaML∥2
2
"
· GaussGammaM(a, σ−2|μ0, Λ0, α0, β0)
∝(σ−2)NM/2 exp

−

a−aML ⊤X⊤X

a−aML 
+∥y−XaML∥2
2
σ−2

· (σ−2)α0+ M
2 −1 exp

−
 (a−μ0)⊤Λ0(a−μ0)
2
+ β0
 
σ−2 
∝(σ−2)α+ M
2 −1 exp
⎛⎜⎜⎜⎜⎝−
⎛⎜⎜⎜⎜⎝
$a −μ%⊤Λ $a −μ%
2
+ β
⎞⎟⎟⎟⎟⎠σ−2
⎞⎟⎟⎟⎟⎠,
where
μ = (X⊤X + Λ0)−1 
X⊤XaML + Λ0μ0
 
,
Λ = X⊤X + Λ0,
α = NM
2 + α0,
β = ∥y−XaML∥2
2
+ (aML−μ0)⊤Λ0(X⊤X+Λ0)−1X⊤X(aML−μ0)
2
+ β0.

26
1 Bayesian Learning
Thus, we obtain
p(a, σ−2|D, κ) = GaussGammaM(a, σ−2|μ,Λ,α,β).
(1.45)
Multinomial Model
The multinomial distribution, which expresses a distribution over the his-
tograms of independent events, is another frequently used basic component
in Bayesian modeling. For example, it appears in mixture models and latent
Dirichlet allocation.
Assume that exclusive K events occur with the probability
θ = (θ1,. . . , θK) ∈ΔK−1 ≡
⎧⎪⎪⎨⎪⎪⎩θ ∈RK; 0 ≤θk ≤1,
K

k=1
θk = 1
⎫⎪⎪⎬⎪⎪⎭.
Then, the histogram
x = (x1,. . . , xK) ∈HK−1
N
≡
⎧⎪⎪⎨⎪⎪⎩x ∈IK; 0 ≤xk ≤N;
K

k=1
xk = N
⎫⎪⎪⎬⎪⎪⎭
of events after N iterations follows the multinomial distribution, deﬁned as
p(x|θ) = MultinomialK,N(x; θ) ≡N! ·
K

k=1
θxk
k
xk!.
(1.46)
θ is called the multinomial parameter.
As seen shortly, calculation of the posterior with its conjugate prior is
surprisingly easy.
Dirichlet Likelihood As a function of the multinomial parameter w = θ, it is
easy to ﬁnd that the likelihood (1.46) is in the form of the Dirichlet distribution:
p(x|θ) ∝DirichletK(θ; x + 1K),
where 1K is the K-dimensional vector with all elements equal to 1. Since
the Dirichlet distribution is an exponential family and hence multiplicatively
closed, it is conjugate for the multinomial parameter. With the conjugate
Dirichlet prior
p(θ|φ) = DirichletK(θ; φ) ∝
K

k=1
θφ−1
k
with hyperparameters κ = φ, the posterior is computed as

1.2 Computation
27
p(θ|x, φ) ∝p(x|θ)p(θ|φ)
∝DirichletK(θ; x + 1K) · DirichletK(θ; φ)
∝
K

k=1
θxk
k · θφk−1
k
∝
K

k=1
θxk+φk−1
k
.
Thus we have
p(θ|x, φ) = DirichletK(θ; x + φ).
(1.47)
Special Cases For K = 2, the multinomial distribution is reduced to the
binomial distribution:
p(x1|θ1) = Multinomial2,N

(x1, N −x1)⊤; (θ1, 1 −θ1)⊤ 
= BinomialN(x1; θ1)
=
!N
x1
"
· θx1
1 (1 −θ1)N−x1.
Furthermore, it is reduced to the Bernoulli distribution for K = 2 and N = 1:
p(x1|θ1) = Binomial1(x1; θ1)
= θx1
1 (1 −θ1)1−x1.
Similarly, its conjugate Dirichlet distribution for K = 2 is reduced to the
Beta distribution:
p(θ1|φ1, φ2) = Dirichlet2

(θ1, 1 −θ1)⊤; (φ1, φ2)⊤ 
= Beta(θ1; φ1, φ2)
=
1
B(φ1, φ2) · θφ1−1
1
(1 −θ1)φ2−1,
where B(φ1, φ2) =
Γ(φ1)Γ(φ2)
Γ(φ1+φ2) is the Beta function. Naturally, the Beta distri-
bution is conjugate to the binomial and the Bernoulli distributions, and the
posterior can be computed as easily as for the multinomial case.
With a conjugate prior in the form of a popular distribution, the four quan-
tities introduced in Section 1.1.3, i.e., the marginal likelihood, the posterior
mean, the posterior covariance, and the predictive distribution, can be obtained
analytically. In the following subsections, we show how they are obtained.

28
1 Bayesian Learning
Table 1.2 First and second moments of common distributions.
Mean(x) = ⟨x⟩p(x|w), Var(x) =

(x −Mean(x))2
p(x|w),
Cov(x) = (x −Mean(x))(x −Mean(x))⊤
p(x|w), Ψ(z) ≡d
dzlog Γ(z) :
Digamma function, and Ψm(z) ≡dm
dzm Ψ(z): Polygamma function of order m.
p(x|w)
First moment
Second moment
GaussM(x; μ, Σ)
Mean(x) = μ
Cov(x) = Σ
Gamma(x; α, β)
Mean(x) = α
β
Var(x) =
α
β2
Mean(log x)
Var(log x) = Ψ1(α)
= Ψ(α) −log β
WishartM(X; V, ν)
Mean(X) = νV
Var(xm,m′) = ν(V2
m,m′ + Vm,mVm′,m′)
MultinomialK,N(x; θ)
Mean(x) = Nθ
(Cov(x))k,k′ =
)Nθk(1 −θk)
(k = k′)
−Nθkθk′
(k  k′)
DirichletK(x; φ)
Mean(x) =
1
K
k=1 φk φ
(Cov(x))k,k′ =
⎧⎪⎪⎨⎪⎪⎩
φk(τ−φk)
τ2(τ+1)
(k = k′)
−φkφk′
τ2(τ+1)
(k  k′)
Mean(log xk)
where
τ = K
k=1 φk
= Ψ(φk) −Ψ(K
k′=1 φk′)
1.2.4 Posterior Mean and Covariance
As seen in Section 1.2.3, by adopting a conjugate prior having a form of one
of the common family distributions, such as the one in Table 1.1, we can have
the posterior distribution in the same common family.3 In such cases, we can
simply use the known form of moments, which are summarized in Table 1.2.
For example, the posterior (1.42) for the regression parameter a (when the
noise variance σ2 is treated as a known constant) is the following Gaussian
distribution:
p(a|D, a0, Σ0) = GaussM

a;a, Σa
 
,
where
a =
 X⊤X
σ2
+ Σ−1
0
−1  X⊤y
σ2 + Σ−1
0 a0

,
Σa =
 X⊤X
σ2
+ Σ−1
0
−1
.
3 If we would say that the prior is in the family that contains all possible distributions, this family
would be the conjugate prior for any likelihood function, which is however useless. Usually, the
notion of the conjugate prior implicitly requires that moments (at least the normalization
constant and the ﬁrst moment) of any family member can be computed analytically.

1.2 Computation
29
Therefore, the posterior mean and the posterior covariance are simply given by
⟨a⟩p(a|D,a0,Σ0) = a,

(a −⟨a⟩)(a −⟨a⟩)⊤
p(a|D,a0,Σ0) = Σa,
respectively. The posterior (1.29) of the (inverse) variance parameter σ−2 of
the isotropic Gaussian distribution (when the mean parameter μ is treated as a
known constant) is the following Gamma distribution:
p(σ−2|D, α0, β0) = Gamma
⎛⎜⎜⎜⎜⎜⎝σ−2; MN
2
+ α0, 1
2
N

n=1
∥x(n) −μ∥2 + β0
⎞⎟⎟⎟⎟⎟⎠.
Therefore, the posterior mean and the posterior variance are given by

σ−2
p(σ−2|D,α0,β0) =
MN
2 + α0
1
2
N
n=1 ∥x(n) −μ∥2 + β0
,
*
σ−2 −

σ−2 2+
p(σ−2|D,α0,β0) =
MN
2 + α0
( 1
2
N
n=1 ∥x(n) −μ∥2 + β0)2 ,
respectively.
Also in other cases, the posterior mean and the posterior covariances can be
easily computed by using Table 1.2, if the form of the posterior distribution is
in the table.
1.2.5 Predictive Distribution
The predictive distribution (1.9) for a new data set Dnew can be computed
analytically, if the posterior distribution is in the exponential family, and hence
multiplicatively closed. In this section, we show two examplary cases, the
linear regression model and the multinomial model.
Linear Regression Model
Consider the linear regression model:
p(y|x, a) = Gauss1(y; a⊤x, σ2) =
1
√
2πσ2 · exp

−(y −a⊤x)2
2σ2

,
(1.48)
where only the regression parameter is unknown, i.e., w = a ∈RM, and the
noise variance parameter σ2 is treated as a known constant. We choose the
zero-mean Gaussian as a conjugate prior:
p(a|C) = GaussM(a; 0, C) =
exp

−1
2 a⊤C−1a
 
(2π)M/2 det (C)1/2 ,
(1.49)
where C is the prior covariance.

30
1 Bayesian Learning
When N i.i.d. samples D = (X, y), where
y = (y(1),. . . , y(N))⊤∈RN,
X = (x(1),. . . , x(N))⊤∈RN×M,
are observed, the posterior is given by
p(a|y, X, C) = GaussM

a;a, Σa
 
=
1
(2π)M/2det
Σa
 1/2 · exp
⎛⎜⎜⎜⎜⎜⎜⎝−
$a −a%⊤Σ
−1
a
$a −a%
2
⎞⎟⎟⎟⎟⎟⎟⎠,
(1.50)
where
a =
 X⊤X
σ2
+ C−1
−1 X⊤y
σ2
= Σa
X⊤y
σ2 ,
(1.51)
Σa =
 X⊤X
σ2
+ C−1
−1
.
(1.52)
This is just a special case of the posterior (1.42) for the linear regression model
with the most general Gaussian prior.
Now, let us compute the predictive distribution on the output y∗for a
new given input x∗. As deﬁned in Eq. (1.9), the predictive distribution is the
expectation value of the model distribution (1.48) (for a new input–output pair)
over the posterior distribution (1.50):
p(y∗|x∗, y, X, C) = ⟨p(y∗|x∗, a)⟩p(a|y,X,C)
=

p(y∗|x∗, a)p(a|y, X, C)da
=

Gauss1(y∗; a⊤x∗, σ2)GaussM

a;a, Σa
 
da
∝

exp

−(y∗−a⊤x∗)2
2σ2
−(a−a)
⊤Σ
−1
a (a−a)
2

da
∝exp
!
−y∗2
2σ2
" 
exp
⎛⎜⎜⎜⎜⎜⎝−
a⊤
!
Σ
−1
a + x∗x∗⊤
σ2
"
a−2a⊤
!
Σ
−1
a a+ x∗y∗
σ2
"
2
⎞⎟⎟⎟⎟⎟⎠da
∝exp
⎛⎜⎜⎜⎜⎜⎜⎝−
σ−2y∗2−
!
Σ
−1
a a+ x∗y∗
σ2
"⊤!
Σ
−1
a + x∗x∗⊤
σ2
"−1!
Σ
−1
a a+ x∗y∗
σ2
"
2
⎞⎟⎟⎟⎟⎟⎟⎠
·

exp
⎛⎜⎜⎜⎜⎜⎝−
(a−˘a)⊤!
Σ
−1
a + x∗x∗⊤
σ2
"
(a−˘a)
2
⎞⎟⎟⎟⎟⎟⎠da,
(1.53)
where
˘a =

Σ
−1
a + x∗x∗⊤
σ2
−1 
Σ
−1
a a + x∗y∗
σ2

.

1.2 Computation
31
Note that, although the preceding computation is similar to the one for the
posterior distribution in Section 1.2.3, any factor that depends on y∗cannot
be ignored even if it does not depend on a, since the goal is to obtain the
distribution on y∗.
The integrand in Eq. (1.53) coincides with the main part of
GaussM
⎛⎜⎜⎜⎜⎜⎝a; ˘a,

Σ
−1
a + x∗x∗⊤
σ2
−1⎞⎟⎟⎟⎟⎟⎠
without the normalization factor. Therefore, the integral is the inverse of the
normalization factor, i.e.,

exp
⎛⎜⎜⎜⎜⎜⎝−
(a−˘a)⊤!
Σ
−1
a + x∗x∗⊤
σ2
"
(a−˘a)
2
⎞⎟⎟⎟⎟⎟⎠da = (2π)M/2det
!
Σ
−1
a + x∗x∗⊤
σ2
"−1/2
,
which is a constant with respect to y∗. Therefore, by using Eqs. (1.51) and
(1.52), we have
p(y∗|x∗, y, X, C)
∝exp
⎛⎜⎜⎜⎜⎜⎜⎝−
σ−2y∗2−
!
Σ
−1
a a+ x∗y∗
σ2
"⊤!
Σ
−1
a + x∗x∗⊤
σ2
"−1!
Σ
−1
a a+ x∗y∗
σ2
"
2
⎞⎟⎟⎟⎟⎟⎟⎠
∝exp

−
y∗2−(X⊤y+x∗y∗)
⊤(X⊤X+x∗x∗⊤+σ2C−1)
−1(X⊤y+x∗y∗)
2σ2

∝exp

−
1
2σ2
,
y∗2
1 −x∗⊤
X⊤X + x∗x∗⊤+ σ2C−1 −1 x∗ 
−2y∗x∗⊤
X⊤X + x∗x∗⊤+ σ2C−1 −1 X⊤y
-
∝exp

−
1−x∗⊤(X⊤X+x∗x∗⊤+σ2C−1)
−1x∗
2σ2
·

y∗−
x∗⊤(X⊤X+x∗x∗⊤+σ2C−1)
−1X⊤y
1−x∗⊤(X⊤X+x∗x∗⊤+σ2C−1)
−1x∗
2 
∝exp
⎛⎜⎜⎜⎜⎝−(y∗−y)2
2σ2y
⎞⎟⎟⎟⎟⎠,
where
y =
x∗⊤
X⊤X + x∗x∗⊤+ σ2C−1 −1 X⊤y
1 −x∗⊤
X⊤X + x∗x∗⊤+ σ2C−1 −1 x∗
,
σ2
y =
σ2
1 −x∗⊤
X⊤X + x∗x∗⊤+ σ2C−1 −1 x∗
.

32
1 Bayesian Learning
–4
–2
0
2
4
–4
–2
0
2
4
True
Estimated
Credible interval
Figure 1.3 Predictive distribution of the linear regression model.
Thus, the predictive distribution has been analytically obtained:
p(y∗|x∗, y, X, C) = Gauss1

y∗;y, σ2
y
 
.
(1.54)
Figure 1.3 shows an example of the predictive distribution of the linear
regression model. The curve labeled as “True” indicates the mean y = a∗x
of the true regression model y = a∗x + ε, where a∗= (−2, 0.4, 0.3, −0.1)⊤,
x = (1, t, t2, t3)⊤, and ε ∼Gauss1(0, 12). The crosses are N = 30 i.i.d.
observed samples generated from the true regression model and the input
distribution t ∼Uniform(−2.4, 1.6), where Uniform(l, u) denotes the uniform
distribution on [l, u]. The regression model (1.48) with the prior (1.49) for
the hyperparameters C = 10000 · IM, σ2 = 1 was trained with the observed
samples. The curve labeled as “Estimated” and the pair of curves labeled as
“Credible interval” show the mean y and the credible interval y ± σy of the
predictive distribution (1.54), respectively.
Reﬂecting the fact that the samples are observed only in the middle region
(t ∈[−2.4, 1.6]), the credible interval is large in outer regions. The larger
interval implies that the “Estimated” function is less reliable, and we see that
the gap from the “True” function is indeed large. Since the true function is
unknown in practical situations, the variance of the predictive distribution is
important information on the reliability of the estimated result.
Multinomial Model
Let us compute the predictive distribution of the multinomial model:
p(x|θ) = MultinomialK,N(x; θ) ∝
K

k=1
θxk
k
xk!,

1.2 Computation
33
p(θ|φ) = DirichletK(θ; φ) ∝
K

k=1
θφk−1
k
,
with the observed data D = x = (x1,. . . , xK) ∈HK−1
N
and the unknown
parameter w = θ = (θ1,. . . , θK) ∈ΔK−1.
The posterior was derived in Eq. (1.47):
p(θ|x, φ) = DirichletK(θ; x + φ) ∝
K

k=1
θxk+φk−1
k
.
Therefore, the predictive distribution for a new single sample x∗∈HK−1
1
is
given by
p(x∗|x, φ) = ⟨p(x∗|θ)⟩p(θ|x,φ)
=

p(x∗|θ)p(θ|x, φ)dθ
=

MultinomialK,1(x∗; θ)DirichletK(θ; x + φ)dθ
∝

K

k=1
θ
x∗
k
k · θxk+φk−1
k
dθ
=

K

k=1
θ
x∗
k+xk+φk−1
k
dθ.
(1.55)
In the fourth equation, we ignored the factors that depend neither on x∗
nor on θ.
The integrand in Eq. (1.55) is the main part of DirichletK(θ; x∗+ x+φ), and
therefore, the integral is equal to the inverse of its normalization factor:

K

k=1
θ
x∗
k+xk+φk−1
k
dθ =
K
k=1 Γ(x∗
k + xk + φk)
Γ(K
k=1 x∗
k + xk + φk)
=
K
k=1 Γ(x∗
k + xk + φk)
Γ(N + K
k=1 φk + 1)
.
Thus, by using the identity Γ(x + 1) = xΓ(x) for the Gamma function, we have
p(x∗|x, φ) ∝
K

k=1
Γ(x∗
k + xk + φk)
∝
K

k=1
(xk + φk)x∗
kΓ(xk + φk)

34
1 Bayesian Learning
∝
K

k=1
(xk + φk)x∗
k
∝
K

k=1
⎛⎜⎜⎜⎜⎝
xk + φk
K
k′=1 xk′ + φ k′
⎞⎟⎟⎟⎟⎠
x∗
k
= MultinomialK,1(x∗;θ),
(1.56)
where
θk =
xk + φk
K
k′=1 xk′ + φk′ .
(1.57)
From Eq. (1.47) and Table 1.2, we can easily see that the predictive mean
θ, speciﬁed by Eq. (1.57), coincides with the posterior mean, i.e., the Bayesian
estimator:
θ = ⟨θ⟩DirichletK(θ;x+φ) .
Therefore, in the multinomial model, the predictive distribution coincides with
the model distribution with the Bayesian estimator plugged in.
In the preceding derivation, we performed the integral computation and
derived the form of the predictive distribution. However, the necessary infor-
mation to determine the predictive distribution is the probability table on the
events x∗∈HK−1
1
= {ek}K
k=1, of which the degree of freedom is only K.
Therefore, the following simple calculation gives the same result:
Prob(x∗= ek|x, φ) = MultinomialK,1(ek; θ)
DirichletK(θ;x+φ)
= ⟨θk⟩DirichletK(θ;x+φ)
= θk,
which speciﬁes the function form of the predictive distribution, given by
Eq. (1.56).
1.2.6 Marginal Likelihood
Let us compute the marginal likelihood of the linear regression model, deﬁned
by Eqs. (1.48) and (1.49):
p(D|C) = p(y|X, C)
= ⟨p(y|X, a)⟩p(a|C)
=

p(y|X, a)p(a|C)da

1.2 Computation
35
=

GaussN(y; Xa, σ2IN)GaussM(a; 0, C)da
=
 exp
!
−∥y−Xa∥2
2σ2
"
(2πσ2)N/2
·
exp

−1
2 a⊤C−1a
 
(2π)M/2 det (C)1/2 da
=
exp
!
−∥y∥2
2σ2
"
(2πσ2)N/2(2π)M/2 det (C)1/2
·

exp
⎛⎜⎜⎜⎜⎜⎜⎜⎝−
−2a⊤X⊤y
σ2 + a⊤ X⊤X
σ2 + C−1 
a
2
⎞⎟⎟⎟⎟⎟⎟⎟⎠da
=
exp
!
−1
2
!
∥y∥2
σ2 −a⊤Σ
−1
a a
""
(2πσ2)N/2(2π)M/2 det (C)1/2
·

exp
⎛⎜⎜⎜⎜⎜⎜⎝−
$a −a%⊤Σ
−1
a
$a −a%
2
⎞⎟⎟⎟⎟⎟⎟⎠da,
(1.58)
where a and Σa are, respectively, the posterior mean and the posterior
covariance, given by Eqs. (1.51) and (1.52).
By using

exp
⎛⎜⎜⎜⎜⎜⎜⎝−
$a −a%⊤Σ
−1
a
$a −a%
2
⎞⎟⎟⎟⎟⎟⎟⎠da =
.
(2π)Mdet
Σa
 
,
and Eq. (1.58), we have
p(y|X, C) =
exp
!
−1
2
!
∥y∥2
σ2 −y⊤XΣaX⊤y
σ4
""
(2πσ2)N/2(2π)M/2 det (C)1/2
.
(2π)Mdet
Σa
 
=
exp

−
∥y∥2−y⊤X(X⊤X+σ2C−1)
−1X⊤y
2σ2

(2πσ2)N/2det(CX⊤X + σ2IM)1/2 ,
(1.59)
where we also used Eqs. (1.51) and (1.52).
Eq. (1.59) is an explicit expression of the marginal likelihood as a function
of the hyperparameter κ = C. Based on it, we perform EBayes learning in
Section 1.2.7.
1.2.7 Empirical Bayesian Learning
In empirical Bayesian (EBayes) learning, the hyperparameter κ is estimated
by maximizing the marginal likelihood p(D|κ). The negative logarithm of the
marginal likelihood,

36
1 Bayesian Learning
FBayes = −log p(D|κ),
(1.60)
is called the Bayes free energy or stochastic complexity.4 Since log(·) is
a monotonic function, maximizing the marginal likelihood is equivalent to
minimizing the Bayes free energy.
Eq. (1.59) implies that the Bayes free energy of the linear regression model
is given by
2FBayes = −2 log p(y|X, C)
= N log(2πσ2) + log det(CX⊤X + σ2IM)
+
∥y∥2 −y⊤X

X⊤X + σ2C−1 −1 X⊤y
σ2
.
(1.61)
Let us restrict the prior covariance to be diagonal:
C = Diag(c2
1,. . . , c2
M) ∈DM.
(1.62)
The prior (1.49) with diagonal covariance (1.62) is called the automatic
relevance determination (ARD) prior, which is known to make the EBayes
estimator sparse (Neal, 1996). In the following example, we see this effect by
setting the design matrix to identity, X = IM, which enables us to derive the
EBayes solution analytically.
Under the identity design matrix, the Bayes free energy (1.61) can be
decomposed as
2FBayes = N log(2πσ2) + log det(C + σ2IM) +
∥y∥2 −y⊤
IM + σ2C−1 −1 y
σ2
= N log(2πσ2) + ∥y∥2
σ2 +
M

m=1

log(c2
m + σ2) −
y2
m
σ2 $1 + σ2c−2
m
%

=
M

m=1
2F∗
m + const.,
(1.63)
where
2F∗
m = log

1 + c2
m
σ2

−y2
m
σ2

1 + σ2
c2m
−1
.
(1.64)
In Eq. (1.63), we omitted the constant factors with respect to the hyperpa-
rameter C. As the remaining terms are decomposed into each component m,
we can independently minimize F∗
m with respect to c2
m.
4 The logarithm of the marginal likelihood log p(D|κ) is called the log marginal likelihood or
evidence.

1.2 Computation
37
0
1
2
3
–0.5
0
0.5
1
Figure 1.4 The (componentwise) Bayes free energy (1.64) of linear regression
model with the ARD prior. The minimizer is shown as a cross if it lies in the
positive region of c2
m/σ2.
The derivative of Eq. (1.64) with respect to c2
m is
2∂F∗
m
∂c2m
=
1
c2m + σ2 −
y2
m
$1 + σ2c−2
m
%2 c4m
=
1
c2m + σ2 −
y2
m
$c2m + σ2%2
= c2
m −(y2
m −σ2)
(c2m + σ2)2
.
(1.65)
Eq. (1.65) implies that F∗
m is monotonically increasing over all domain c2
m > 0
when y2
m ≤σ2, and has the unique minimizer in the region c2
m > 0 when
y2
m > σ2. Speciﬁcally, the minimizer is given by
c2
m =
⎧⎪⎪⎨⎪⎪⎩
y2
m −σ2
if y2
m > σ2,
+0
otherwise.
(1.66)
Figure 1.4 shows the (componentwise) Bayes free energy (1.64) for dif-
ferent observations, y2
m = 0, σ2, 1.5σ2, 2σ2. The minimizer is in the positive
region of c2
m if and only if y2
m > σ2.
If the EBayes estimator is given by c2
m →+0, it means that the prior
distribution for the mth component am of the regression parameter is the Dirac
delta function located at the origin.5 This formally means that we a priori
5 When y2
m ≤σ2, the Bayes free energy (1.64) decreases as c2
m approaches to 0. However, the
domain of c2
m is restricted to be positive, and therefore,c2
m = 0 is not the solution. We express
this solution asc2
m →+0.

38
1 Bayesian Learning
knew that am = 0, i.e., we choose a model that does not contain the mth
component.
By substituting Eq. (1.66) into the Bayes posterior mean (1.51), we obtain
the EBayes estimator:
aEBayes
m
= c2
m

c2
m + σ2 −1 ym
=
⎧⎪⎪⎪⎨⎪⎪⎪⎩
!
1 −σ2
y2m
"
ym
if y2
m > σ2,
0
otherwise.
(1.67)
The form of the estimator (1.67) is called the James–Stein (JS) estimator
having interesting properties including the domination over the ML esti-
mator (Stein, 1956; James and Stein, 1961; Efron and Morris, 1973) (see
Appendix A).
Note that the assumption that X = IM is not practical. For a general design
matrix X, the Bayes free energy is not decomposable into each component.
Consequently, the prior variances {c2
m}M
m=1 that minimize the Bayes free energy
(1.61) interact with each other. Therefore, the preceding simple mechanism is
not applied. However, it is empirically observed that many prior variances tend
to go toc2
m →+0, so that the EBayes estimator aEBayes is sparse.

2
Variational Bayesian Learning
In Chapter 1, we saw examples where the model likelihood has a conjugate
prior, with which Bayesian learning can be performed analytically. However,
many practical models do not have conjugate priors. Even in such cases,
the notion of conjugacy is still useful. Speciﬁcally, we can make use of
the conditional conjugacy, which comes from the fact that many practical
models are built by combining basic distributions. In this chapter, we introduce
variational Bayesian (VB) learning, which makes use of the conditional
conjugacy, and approximates the Bayes posterior by solving a constrained
minimization problem.
2.1 Framework
VB learning is derived by casting Bayesian learning as an optimization
problem with respect to the posterior distribution (Hinton and van Camp,
1993; MacKay, 1995; Opper and Winther, 1996; Attias, 1999; Jordan et al.,
1999; Jaakkola and Jordan, 2000; Ghahramani and Beal, 2001; Bishop, 2006;
Wainwright and Jordan, 2008).
2.1.1 Free Energy Minimization
Let r(w), or r for short, be an arbitrary distribution, which we call a trial
distribution, on the parameter w, and consider the Kullback–Leibler (KL)
divergence from the trial distribution r(w) to the Bayes posterior p(w|D):
KL (r(w)∥p(w|D)) =

r(w) log
r(w)
p(w|D)dw =
/
log
r(w)
p(w|D)
0
r(w)
.
(2.1)
39

40
2 Variational Bayesian Learning
Since the KL divergence is equal to zero if and only if the two distributions
coincide with each other, the minimizer of Eq. (2.1) is the Bayes posterior, i.e.,
p(w|D) = argmin
r
KL (r(w)∥p(w|D)) .
(2.2)
The problem (2.2) is equivalent to the following problem:
p(w|D) = argmin
r
F(r),
(2.3)
where the functional of r,
F(r) =

r(w) log
r(w)
p(w, D)dw =
/
log
r(w)
p(w, D)
0
r(w)
(2.4)
= KL (r(w)∥p(w|D)) −log p(D),
(2.5)
is called the free energy. Intuitively, we replaced the posterior distribution
p(w|D) in the KL divergence (2.1) with its unnormalized version—the joint
distribution p(D, w) = p(w|D)p(D)—in the free energy (2.4). The equivalence
holds because the normalization factor p(D) does not depend on w, and
therefore log p(D)
r(w) = log p(D) does not depend on r. Note that the free
energy (2.4) is a generalization of the Bayes free energy, deﬁned by Eq. (1.60):
The free energy (2.4) is a functional of an arbitrary distribution r, and equal
to the Bayes free energy (1.60) for the Bayes poterior r(w) = p(w|D). Since
the KL divergence is nonnegative, Eq. (2.5) implies that the free energy F(r)
is an upper-bound of the Bayes free energy −log p(D) for any distribution r.
Since the log marginal likelihood log p(D) is called the evidence, −F(r) is also
called the evidence lower-bound (ELBO).
As mentioned in Section 1.1.2, the joint distribution is easy to compute
in general. However, the minimization problem in Eq. (2.3) can still be
computationally intractable, because the objective functional (2.4) involves the
expectation over the distribution r(w). Actually, it can be hard to even evaluate
the objective functional for most of the possible distributions. To make the
evaluation of the objective functional tractable for optimal r(w), we restrict the
search space to G. Namely, we solve the following problem:
min
r
F(r)
s.t.
r ∈G,
(2.6)
where s.t. is an abbreviation for “subject to.”
We can choose a tractable distribution class directly for G, e.g., Gaussian,
such that the expectation for evaluating the free energy is tractable for any
r ∈G. However, in many practical models, a weaker constraint restricts the
optimal distribution to be in a tractable class, thanks to conditional conjugacy.

2.1 Framework
41
2.1.2 Conditional Conjugacy
Let us consider a few examples where the model likelihood has no conjugate
prior. The likelihood of the matrix factorization model (which will be discussed
in detail in Section 3.1) is given by
p(V|A, B) =
exp
!
−1
2σ2
###V −BA⊤###2
Fro
"
(2πσ2)LM/2
,
(2.7)
where V ∈RL×M is an observed random variable, and A ∈RM×H and B ∈RL×H
are the parameters to be estimated. Although σ2 ∈R++ can also be unknown,
let us treat it as a hyperparameter, i.e., a constant when computing the posterior
distribution.
If we see Eq. (2.7) as a function of the parameters w = (A, B), its
function form is the exponential of a polynomial including a fourth-order
term
###BA⊤###2
Fro = tr(BA⊤AB⊤). Therefore, no conjugate prior exists for this
likelihood with respect to the parameters w = (A, B).1
The next example is a mixture of Gaussians (which will be discussed in
detail in Section 4.1.1):
p(D, H|w) =
N

n=1
K

k=1
⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩
αk
exp
!
−∥x(n)−μk∥
2
2σ2
"
(2πσ2)M/2
⎫⎪⎪⎪⎪⎪⎬⎪⎪⎪⎪⎪⎭
z(n)
k
,
(2.8)
where D = {x(n)}N
n=1 are observed data, H = {z(n)}N
n=1 are hidden variables, and
w = (α, {μk}K
k=1) are parameters. For simplicity, we here assume that all Gaus-
sian components have the same variance σ2, which is treated as a hyperparam-
eter, i.e., we compute the joint posterior distribution of the hidden variables
{z(n)}N
n=1 and the parameters w, regarding the hyperparameter σ2 as a constant.
If we see Eq. (2.8) as a function of ({z(n)}N
n=1, α, {μk}K
k=1), no conjugate
prior exists. More speciﬁcally, it has a factor N
n=1
K
k=1 αkz(n)
k , and we cannot
compute

z(n)
k ∈{ek}K
k=1

N

n=1
K

k=1
αk
z(n)
k dαk
analytically for general N, which is required when evaluating moments.
1 Here, “no conjugate prior” means that there is no useful and nonconditional conjugate prior,
such that the posterior is in the same distribution family with computable moments. We might
say that the exponential function of fourth-order polynomials is conjugate to the likelihood
(2.7), since the posterior is within the same family. However, this statement is useless in
practice because we cannot compute moments of the distribution analytically.

42
2 Variational Bayesian Learning
The same difﬁculty happens in the latent Dirichlet allocation model (which
will be discussed in detail in Section 4.2.4). The likelihood is written as
p(D, H|w) =
M

m=1
N(m)

n=1
H

h=1
⎧⎪⎪⎨⎪⎪⎩Θm,h
L

l=1
Bl,h
w(n,m)
l
⎫⎪⎪⎬⎪⎪⎭
z(n,m)
h
,
(2.9)
where D = {{w(n,m)}N(m)
n=1 }M
m=1 are observed data, H = {{z(n,m)}N(m)
n=1 }M
m=1 are hidden
variables, and w = (Θ, B) are parameters to be estimated. Computing the
sum (over the hidden variables H) of the integral (over the parameters w) is
intractable for practical problem sizes.
Readers might ﬁnd that Eqs. (2.7), (2.8), and (2.9) are not much more
complicated than the conjugate cases: Eq. (2.7) is similar to the Gaussian
form, and Eqs. (2.8) and (2.9) are in the form of the multinomial or Dirichlet
distribution, where we have unknowns both in the base and in the exponent.
Indeed, they are in a known form if we regard a part of unknowns as ﬁxed
constants.
The likelihood (2.7) of the matrix factorization model is in the Gaussian
form of A if we see B as a constant, or vice versa. The likelihood (2.8) of a
mixture of Gaussians is in the multinomial form of the hidden variables H =
{z(n)}N
n=1 if we see the parameters w = (α, {μk}K
k=1) as constants, and it is the
(independent) product of the Dirichlet form of α and the Gaussian form of
{μk}K
k=1 if we see the hidden variables H = {z(n)}N
n=1 as constants. Similarly, the
likelihood (2.9) of the latent Dirichlet allocation model is in the multinomial
form of the hidden variables H = {{z(n,m)}N(m)
n=1 }M
m=1 if we see the parameters
w = (Θ, B) as constants, and it is the product of the Dirichlet form of the row
vectors {θm}M
m=1 of Θ and the Dirichlet form of the column vectors {βh}H
h=1 of B
if we see the hidden variables H = {{z(n,m)}N(m)
n=1 }M
m=1 as constants.
Since the likelihoods in the Gaussian, multinomial, and Dirichlet forms
have conjugate priors, the aforementioned properties can be described with
the notion of conditional conjugacy, which is deﬁned as follows:
Deﬁnition 2.1
(Conditionally conjugate prior) Let us divide the unknown
parameters w (or more generally all unknown variables including hidden
variables) into two parts w = (w1, w2). If the posterior of w1,
p(w1|w2, D) ∝p(D|w1, w2)p(w1),
(2.10)
is in the same distribution family as the prior p(w1) (where w2 is regarded as a
given constant or condition), the prior p(w1) is called a conditionally conjugate
prior of the model likelihood p(D|w) with respect to the parameter w1, given
the ﬁxed parameter w2.

2.1 Framework
43
2.1.3 Constraint Design
Once conditional conjugacy for all unknowns is found, designing tractable VB
learning is straightforward.
Let us divide the unknown parameters w into S groups, i.e., w
=
(w1,. . . , wS ), such that, for each s
=
1,. . . , S , the model likelihood
p(D|w) = p(D|ws, {ws′}s′s) has a conditionally conjugate prior p(ws) with
respect to ws, given {ws′}s′s as ﬁxed constants. Then, if we use the prior
p(w) =
S
s=1
p(ws),
(2.11)
the posterior distribution
p(w|D) ∝p(D|w)p(w)
is, as a function of ws, in the same distribution family as the prior p(ws).
Therefore, moments of the posterior distribution are tractable, if the other
parameters {ws′}s′s are given.
To make use of this property, we impose on the approximate posterior the
independence constraint between the parameter groups,
r(w) =
S
s=1
rs(ws),
(2.12)
which allows us to compute moments with respect to ws independently from
the other parameters {ws′}s′s. In VB learning, we solve the minimization
problem (2.6) under the constraint (2.12). This makes the expectation com-
putation, which is required in evaluating the free energy (2.4), tractable (on
any stationary points for r). Namely, we deﬁne the VB posterior as
r = argmin
r
F(r)
s.t.
r(w) =
S
s=1
rs(ws).
(2.13)
Note that it is not guaranteed that the free energy F(r) =

log
r(w)
p(D|w)p(w)

r(w)
is tractable for any r satisfying the constraint (2.12). However, the constraint
allows us to optimize each factor {rs}S
s=1 separately. To optimize each factor,
we rely on calculus of variations, which will be explained in Section 2.1.4.
By applying calculus of variations, the free energy is expressed as an explicit
function with a ﬁnite number of unknown parameters.

44
2 Variational Bayesian Learning
2.1.4 Calculus of Variations
Calculus of variations is a method, developed in physics, to derive conditions
that any optimal function minimizing a (smooth) functional should satisfy
(Courant and Hilbert, 1953). Speciﬁcally, it gives (inﬁnitely many) stationary
conditions of the functional with respect to the variable.
The change of the functional F(r) with respect to an inﬁnitesimal change of
the variable r (which is a function of w) is called a variation and written as δI.
For r to be a stationary point of the functional, the variation must be equal to
zero for all possible values of w. Since the free energy (2.4) does not depend
on the derivatives of r(w), the variation δI is simply the derivative with respect
to r. Therefore, the stationary conditions are given by
δI = ∂F
∂r = 0,
∀w ∈W,
(2.14)
which is a special case of the Euler–Lagrange equation. If we see the function
r(w) as a (possibly) inﬁnite-dimensional vector with the parameter value w as
its index, the variation δI = δI(w) can be interpreted as the gradient of the
functional F(r) in the |W|-dimensional space. As the stationary conditions in
a ﬁnite-dimensional space require that all entries of the gradient equal to zero,
the optimal function r(w) should satisfy Eq. (2.14) for any parameter values
w ∈W.
In Section 2.1.5, we see that, by applying the stationary conditions (2.14) to
the free energy minimization problem (2.13) with the independence constraint
taken into account, we can ﬁnd that each factor rs(ws) of the approximate
posterior is in the same distribution family as the corresponding prior ps(ws),
thanks to the conditional conjugacy.
2.1.5 Variational Bayesian Learning
Let us solve the problem (2.13) to get the VB posterior
r = argmin
r
F(r)
s.t.
r(w) =
S
s=1
rs(ws).
We use the decomposable conditionally conjugate prior (2.11):
p(w) =
S
s=1
p(ws),
which means that, for each s = 1,. . . , S , the posterior p(ws|{ws′}s′s, D) for ws
is in the same form as the corresponding prior p(ws), given {ws′}s′s as ﬁxed
constants.

2.1 Framework
45
Now we apply the calculus of variations, and compute the stationary
conditions (2.14). The free energy can be written as
F(r) =
 ⎛⎜⎜⎜⎜⎜⎝
S
s=1
rs(ws)
⎞⎟⎟⎟⎟⎟⎠
⎛⎜⎜⎜⎜⎝log
S
s=1 rs(ws)
p(D|w) S
s=1 p(ws)
⎞⎟⎟⎟⎟⎠dw.
(2.15)
Taking the derivative of Eq. (2.15) with respect to rs(ws) for any s = 1,. . . , S
and ws ∈W, we obtain the following stationary conditions:
0 = ∂F
∂rs
=
 ⎛⎜⎜⎜⎜⎜⎝

s′s
rs′(ws′)
⎞⎟⎟⎟⎟⎟⎠
⎛⎜⎜⎜⎜⎝log
S
s′=1 rs′(ws′)
p(D|w) S
s′=1 p(ws′)
+ 1
⎞⎟⎟⎟⎟⎠dw
=
/
log
S
s′=1 rs′(ws′)
p(D|w) S
s′=1 p(ws′)
0

s′s rs′(ws′)
+ 1
=
/
log

s′s rs′(ws′)
p(D|w) 
s′s p(ws′)
0

s′s rs′(ws′)
+ log rs(ws)
p(ws) + 1
=
/
log
1
p(D|w)
0

s′s rs′(ws′)
+ log rs(ws)
p(ws) + const.
(2.16)
Note the following on Eq. (2.16):
• The right-hand side is a function of ws (ws′ for s′  s are integrated out).
• For each s, Eq. (2.16) must hold for any possible value of ws, which can
fully specify the function form of the posterior rs(ws).
• To make Eq. (2.16) satisﬁed for any ws, it is necessary that
−log p(D|w)
s′s rs′(ws′) + log rs(ws)
p(ws)
is a constant.
The last note leads to the following relation:
rs(ws) ∝p(ws) exp log p(D|w)
s′s rs′(ws′) .
(2.17)
As a function of ws, Eq. (2.17) can be written as
rs(ws) ∝exp log p(D|w)p(ws)
s′s rs′(ws′)
∝exp log p(ws|{ws′}s′s, D)
s′s rs′(ws′)
∝exp

log p(ws|{ws′}s′s, D)

s′s
rs′(ws′)dws′.
(2.18)
Due to the conditional conjugacy, p(ws|{ws′}s′s, D) is in the same form as the
prior p(ws). As the intergral operator g(x) =

f(x; α)dα can be interpreted as
an inﬁnite number of additions of parametric functions f(x; α) over all possible
values of α, the operator h(x) = exp

log f(x; α)dα corresponds to an inﬁnite

46
2 Variational Bayesian Learning
number of multiplications of f(x; α) over all possible values of α. Therefore,
Eq. (2.18) implies that the VB posterior rs(ws) is in the same form as the prior
p(ws), if the distribution family is multiplicatively closed.
Assume that the prior p(ws) for each group of parameters is in a multi-
plicatively closed distribution family. Then, we may express the corresponding
VB posterior rs(ws) in a parametric form, of which the parameters are called
variational parameters, without any loss of accuracy or optimality. The last
question is whether we can compute the expectation value of the log-likelihood
log p(D|w) for each factor rs(ws) of the approximate posterior. In many cases,
this expectation can be computed analytically, which allows us to express the
stationary conditions (2.17) as a ﬁnite number of equations in explicit forms of
the variational parameters.
Typically, the obtained stationary conditions are used to update the varia-
tional parameters in an iterative algorithm, which gives a local minimizerr of
the free energy (2.4). We call the minimizerr the VB posterior, and its mean
w = ⟨w⟩r(w)
(2.19)
the VB estimator.
The computation of predictive distribution
p(Dnew|D) = p(Dnew|w)
r(w)
can be hard even after ﬁnding the VB posteriorr(w). This is natural because we
need approximation for the function form of the likelihood p(D|w), and now
we need to compute the integral with the integrand involving the same function
form. In many practical cases, the plug-in predictive distribution p(Dnew|w),
i.e., the model distribution with the VB estimator plugged in, is substituted for
the predictive distribution.
2.1.6 Empirical Variational Bayesian Learning
When the model involves hyperparameters κ in the likelihood and/or in the
prior, the joint distribution is dependent on κ, i.e.,
p(D, w|κ) = p(w|κ)p(D|w, κ),
and so is the free energy:
F(r, κ) =

r(w) log
r(w)
p(D, w|κ)dw
=
/
log
r(w)
p(w, D|κ)
0
r(w)
(2.20)
= KL (r(w)∥p(w|D, κ)) −log p(D|κ).
(2.21)

2.1 Framework
47
Similarly to the empirical Bayesian learning, the hyperparameters can be
estimated from observation by minimizing the free energy simultaneously with
respect to r and κ:
(r,κ) = argmin
r,κ
F(r, κ).
This approach is called the empirical VB (EVB) learning.
EVB learning amounts to minimizing the sum of the KL divergence to the
Bayes posterior and the marginal likelihood (see Eq. (2.21)). Conceptually,
minimizing any weighted sum of those two terms is reasonable to ﬁnd the VB
posterior and the hyperparameters at the same time. But only the unweighted
sum makes the objective tractable—under this choice, the objective is written
with the joint distribution as in Eq. (2.20), while any other choice requires
explicitly accessing the Bayes posterior and the marginal likelihood separately.
2.1.7 Techniques for Nonconjugate Models
In Sections 2.1.2 through 2.1.5, we saw how to design tractable VB learning by
making use of the conditional conjugacy. However, there are also many cases
where a reasonable model does not have a conditionally conjugate prior. A
frequent and important example is the case where the likelihood involves the
sigmoid function,
σ(x; w) =
1
1 + e−w⊤x ,
(2.22)
or a function with a similar shape, e.g., the error function, the hyperbolic
tangent, and the rectiﬁed linear unit (ReLU). We face such cases, for example,
in solving classiﬁcation problems and in adopting neural network structure
with a nonlinear activation function.
To maintain the tractability in such cases, we need to explicitly restrict the
function form of the approximate posterior r(w;λ), and optimize its variational
parameters λ by free energy minimization. Namely, we solve the VB learning
problem (2.6) with the search space G set to the function space of a simple
distribution family, e.g., the Gaussian distribution r(w;λ) = GaussD(w; w, Σ)
parameterized with the variational parameters λ = (w, Σ) consisting of the
mean and the covariance parameters. Then, the VB learning problem (2.6) is
reduced to the following unconstrained minimization problem,
min
λ
F(λ),
(2.23)

48
2 Variational Bayesian Learning
of the free energy
F(λ) =

r(w;λ) log r(w;λ)
p(w, D)dw =
/
log r(w;λ)
p(w, D)
0
r(w;λ)
,
(2.24)
which is a function of the variational parameters λ.
It is often the case that the free energy (2.24) is still intractable in
computing the expectation value of the log joint probability, log p(w, D) =
log p(D|w)p(w), over the approximate posterior r(w;λ) (because of the
intractable function form of the likelihood p(D|w) or the prior p(w)). In
this section, we introduce a few techniques developed for coping with such
intractable functions.
Local Variational Approximation
The ﬁrst method is to bound the joint distribution p(w, D) with a simple
function, of which the expectation value over the approximate distribution
r(w;λ) is tractable.
As seen in Section 2.1.1, the free energy (2.24) is an upper-bound of the
Bayes free energy, FBayes ≡−log p(D), for any λ. Consider further upper-
bounding the free energy as
F(λ) ≤F(λ, ξ) ≡

r(w;λ) log r(w;λ)
p(w; ξ)dw
(2.25)
by replacing the joint distribution p(w, D) with its parametric lower-bound
p(w; ξ) such that
0 ≤p(w; ξ) ≤p(w, D)
(2.26)
for any w ∈W and ξ ∈Ξ. Here, we introduced another set of variational
parameters ξ with its domain Ξ. Let us choose a lower-bound p(w; ξ) such that
its function form with respect to w is the same as the approximate posterior
r(w;λ). More speciﬁcally, we assume that, for any given ξ, there exists λ
such that
p(w; ξ) ∝r(w;λ)
(2.27)
as a function of w.2 Since the direct minimization of F(λ) is intractable, we
instead minimize its upper-bound F(λ, ξ) jointly over λ and ξ. Namely, we
solve the problem
min
λ,ξ
F(λ, ξ),
(2.28)
2 The parameterization, i.e., the function form with respect to the variational parameters, can be
different between p(w; ξ) and r(w;λ).

2.1 Framework
49
to ﬁnd the approximate posterior r(w;λ) such that F(λ, ξ) (≥F(λ)) is closest
to the Bayes free energy FBayes (when ξ is also optimized).
Let
q(w; ξ) =
p(w; ξ)
Z(ξ)
(2.29)
be the distribution created by normalizing the lower-bound with its normaliza-
tion factor
Z(ξ) =

p(w; ξ)dw.
(2.30)
Note that the normalization factor (2.30) is trivially a lower-bound of the
marginal likelihood, i.e.,
Z(ξ) ≤

p(w, D)dw = p(D),
and is tractable because of the assumption (2.27) that p is in the same simple
function form as r.
With Eq. (2.29), the upper-bound (2.25) is expressed as
F(λ, ξ) =

r(w;λ) log r(w;λ)
q(w; ξ)dw −log Z(ξ)
= KL

r(w;λ)∥q(w; ξ)
 
−log Z(ξ),
(2.31)
which implies that the optimal λ is attained when
r(w;λ) = q(w; ξ)
(2.32)
for any ξ ∈Ξ (the assumption (2.27) guarantees the attainability). Thus, by
putting this back into Eq. (2.31), the problem (2.28) is reduced to
max
ξ
Z(ξ),
(2.33)
which amounts to maximizing the lower-bound (2.30) of the marginal likeli-
hood p(D). Once the maximizer ξ is obtained, Eq. (2.32) gives the optimal
approximate posterior.
Such an approximation scheme for nonconjugate models is called local
variational approximation or direct site bounding (Jaakkola and Jordan, 2000;
Girolami, 2001; Bishop, 2006; Seeger, 2008, 2009), which will be discussed
further with concrete examples in Chapter 5. Existing nonconjugate models
applied with the local variational approximation form the bound in Eq. (2.26)
based on the convexity of a function. In such a case, the gap between
F(ξ) and F turns out to be the expected Bregman divergence associated
with the convex function (see Section 5.3.1). A similar approach can be

50
2 Variational Bayesian Learning
applied to expectation propagation, another approximation method introduced
in Section 2.2.3. There, by upper-bounding the joint probability p(w, D), we
minimize an upper-bound of KL

p(w|D)∥r(w;λ)
 
(see Section 2.2.3).
Black Box Variational Inference
As the available data size increases, and the beneﬁt of using big data has
been proven, for example, by the breakthrough in deep learning (Krizhevsky
et al., 2012), scalable training algorithms have been intensively developed, to
enable big data analysis on billions of data samples. The stochastic gradient
descent (Robbins and Monro, 1951; Spall, 2003), where a noisy gradient of the
objective function is cheaply computed from a subset of the whole data in each
iteration, has become popular, and has been adopted for VB learning (Hoffman
et al., 2013; Khan et al., 2016).
The black-box variational inference was proposed as a general method
to compute a noisy gradient of the free energy in nonconjugate models
(Ranganath et al., 2013; Wingate and Weber, 2013; Kingma and Welling,
2014). As a function of the variational parameters λ, the gradient of the free
energy (2.24) can be evaluated by
∂F
∂λ
= ∂
∂λ

r(w;λ) log r(w;λ)
p(D, w)dw
=
 ∂r(w;λ)
∂λ
log r(w;λ)
p(D, w)dw +

r(w;λ) ∂
∂λ

log r(w;λ)
 
dw
=

r(w;λ)∂log r(w;λ)
∂λ
log r(w;λ)
p(D, w)dw + ∂
∂λ

r(w;λ)dw
=
/∂log r(w;λ)
∂λ
log r(w;λ)
p(D, w)
0
r(w;λ)
.
(2.34)
Assume that we restrict the approximate posterior r(w;λ) to be in a
simple distribution family, from which samples can be easily drawn, and
its score function, the gradient of the log probability, is easily computed,
e.g., an analytic form is available. Then, Eq. (2.34) can be easily computed
by drawing samples from r(w;λ), and computing the sample average. With
some variance reduction techniques, the stochastic gradient with the black box
gradient estimator (2.34) has shown to be useful for VB learning in general
nonconjugate models. A notable advantage is that it does not require any model
speciﬁc analysis to implement the gradient estimation, since Eq. (2.34) can be
evaluated as long as the log joint probability p(D, w) = p(D|w)p(w) of the
model can be evaluated for drawn samples of w.

2.2 Other Approximation Methods
51
2.2 Other Approximation Methods
There are several other methods for approximate Bayesian learning, which are
brieﬂy introduced in this section.
2.2.1 Laplace Approximation
In the Laplace approximation, the posterior is approximated by a Gaussian:
r(w) = GaussD(w; w, Σ).
VB learning ﬁnds the variational parametersλ = (w, Σ) by minimizing the free
energy (2.4), i.e., solving the problem (2.6) with the search space G restricted
to the Gaussian distributions. Instead, the Laplace approximation estimates the
mean and the covariance by
wLA(= wMAP) = argmax
w
p(D|w)p(w),
(2.35)
Σ
LA = F
−1,
(2.36)
where the entries of F ∈SD
++ are given by
Fi, j = −∂2 log p(D|w)p(w)
∂wi∂wj
w=wLA.
(2.37)
Namely, the Laplace approximation ﬁrst ﬁnds the MAP estimator for the
mean, and then computes Eq.(2.37) at w = wLA to estimate the inverse
covariance, which corresponds to the second-order Taylor approximation to
log p(D|w)p(w). Note that, for the ﬂat prior p(w) ∝1, Eq. (2.37) is reduced to
the Fisher information:
Fi, j =
/∂log p(D|w)
∂wi
∂log p(D|w)
∂wj
0
p(D|w)
= −
/∂2 log p(D|w)
∂wi∂wj
0
p(D|w)
.
In general, the Laplace approximation is computationally less demanding
than VB learning, since no integral computation is involved, and the inverse
covariance estimation (2.36) is performed only once after the MAP mean
estimator (2.35) is found.
2.2.2 Partially Bayesian Learning
Partially Bayesian (PB) learning is MAP learning after some of the unknown
parameters are integrated out. This approach can be described in the free
energy minimization framework (2.6) with a strnger constraint than VB
learning.

52
2 Variational Bayesian Learning
Let us split the unknown parameters w into two parts w = (w1, w2), and
assume that we integrate w1 out and point-estimate w2. Integrating w1 out
means that we consider the exact posterior on w1, and MAP estimating w2
means that we approximate the posterior w2 with the delta function. Namely,
PB learning solves the following problem:
min
r
F(r)
s.t.
r(w) = r1(w1) · δ(w2; w2),
(2.38)
where the free energy F(r) is deﬁned by Eq. (2.4), and δ(w; w) is the Dirac
delta function located at w.
Using the constraint in Eq. (2.38), under which the variables to be optimized
are r1 and w2, we can express the free energy as
F(r1, w2) =
/
log
r1(w1) · δ(w2; w2)
p(D|w1, w2)p(w1)p(w2)
0
r1(w1)·δ(w2;w2)
=
/
log
r1(w1)
p(D|w1, w2)p(w1)p(w2)
0
r1(w1)
+ log δ(w2; w2)
δ(w2;w2)
=
/
log
r1(w1)
p(w1|w2, D)
0
r1(w1)
−log p(D|w2)p(w2) + log δ(w2; w2)
δ(w2;w2) ,
(2.39)
where
p(D|w2) = p(D|w1, w2)
p(w1) =

p(D|w1, w2)p(w1)dw1.
(2.40)
The free energy (2.39) depends on r1 only through the ﬁrst term, which is
the KL divergence, KL $r1(w1)∥p(w1|w2, D)%, from the trial distribution to the
Bayes posterior (conditioned on w2). Therefore, the minimizer for r1 is trivially
the conditional Bayes posterior
r1(w1) = p(w1|w2, D),
(2.41)
with which the ﬁrst term in Eq. (2.39) vanishes. The third term in Eq. (2.39) is
the entropy of the delta function, which diverges to inﬁnity but is independent
of w2. By regarding the delta function as a distribution with its width narrow
enough to express a point estimate, while its entropy is ﬁnite (although it is
very large), we can ignore the third term. Thus, the free energy minimization
problem (2.38) can be written as
min
w2
−log p(D|w2)p(w2),
(2.42)
which amounts to MAP learning for w2 after w1 is marginalized out.

2.2 Other Approximation Methods
53
This method is computationally beneﬁcial when the likelihood p(D|w) =
p(D|w1, w2) is conditionally conjugate to the prior p(w1) with respect to w1,
given w2. Thanks to the conditional conjugacy, the posterior (2.41) of w1 is
in a known form, and its normalization factor (2.40), which is required when
evaluating the objective in Eq. (2.42), can be obtained analytically.
PB learning was applied in many previous works. For example, in the
expectation-maximization (EM) algorithm (Dempster et al., 1977), latent
variables are integrated out and parameters are point-estimated. In the ﬁrst
probabilisitic interpretation of principal component analysis (PCA) (Tipping
and Bishop, 1999), one factor of the matrix factorization was called a latent
variable and integrated out, while the other factor was called a parameter and
point-estimated.
The same idea has been adopted for Gibbs sampling and VB learning, where
some of the unknown parameters are integrated out based on the conditional
conjugacy, and the other parameters are estimated by the corresponding learn-
ing method. Those methods are called collapsed Gibbs sampling (Grifﬁths and
Steyvers, 2004) and collapsed VB learning (Kurihara et al., 2007; Teh et al.,
2007; Sato et al., 2012), respectively. Following this terminology, PB learning
may be also called collapsed MAP learning. The collapsed version is in
general more accurate and more computationally efﬁcient than the uncollapsed
counterpart, since it imposes a weaker constraint and applies a nonexact
numerical estimation to a smaller number of unknowns.
2.2.3 Expectation Propagation
As explained in Section 2.1.1, VB learning amounts to minimizing the KL
divergence KL (r(w)∥p(w|D)) from the approximate posterior to the Bayes
posterior. Expectation propagation (EP) is an alternative deterministic approx-
imation scheme, which minimizes the KL divergence from the Bayes posterior
to the approximate posterior (Minka, 2001b), i.e.,
min
r
KL (p(w|D)∥r(w))
s.t.
r ∈G.
(2.43)
Clearly from its deﬁnition, the KL divergence,
KL (q(x)∥p(x)) =

q(x) log q(x)
p(x)dx,
diverges to +∞if the support of q(x) is not covered by the support of p(x),
while it remains ﬁnite if the support of p(x) is not covered by the support
of q(x). Due to this asymmetric property of the KL divergence, VB learning
and EP can provide drastically different approximate posteriors—VB learning,

54
2 Variational Bayesian Learning
–2
–1
0
1
2
0
0.2
0.4
0.6
0.8
1
Bayes posterior
VB posterior
EP posterior
Figure 2.1 Bayes posterior, VB posterior, and EP posterior.
minimizing KL (r(w)∥p(w|D)), tends to provide a posterior that approximates
a single mode of the Bayes posterior, while EP, minimizing KL (p(w|D)∥r(w)),
tends to provide a posterior with a broad support covering all modes of the
Bayes posterior (see the illustration in Figure 2.1).
Moment Matching Algorithm
The EP problem (2.43) is typically solved by moment matching. It starts with
expressing the posterior distribution by the product of factors,
p(w|D) = 1
Z

n
tn(w),
where Z = p(D) is the marginal likelihood. For example, in the parametric
density estimation (Example 1.1) with i.i.d. samples D = {x(1),. . . , x(N)},
the factor can be set to tn(w) = p(x(n)|w) and t0(w) = p(w). In EP, the
approximating posterior is also assumed to have the same form,
r(w) = 1
Z

n
tn(w),
(2.44)
where Z is the normalization constant and becomes an approximation of the
marginal likelihood Z. Note that the factorization is not over the elements of w.
EP tries to minimize the KL divergence,
KL(p∥r) = KL
⎛⎜⎜⎜⎜⎜⎝
1
Z

n
tn(w)
####
1
Z

n
tn(w)
⎞⎟⎟⎟⎟⎟⎠,
which is approximately carried out by reﬁning each factor while the other
factors are ﬁxed, and cycling through all the factors. To reﬁne the factor tn(w),
we deﬁne the unnormalized distribution,
r¬n(w) = r(w)
tn(w),

2.2 Other Approximation Methods
55
and the following distribution is used as an estimator of the true posterior:
pn(w) = tn(w)r¬n(w)
Zn
,
where Zn =

tn(w)r¬n(w)dw is the normalization constant. That is, the new
approximating posterior rnew(w) is computed so that it minimizes KL(pn∥rnew).
Usually, the approximating posterior is assumed to be a member of the
exponential family. In that case, the minimization of KL(pn∥rnew) is reduced
to the moment matching between pn and rnew. Namely, the parameter of rnew
is determined so that its moments are matched with those of pn.
The new approximating posterior rnew yields the reﬁnement of the factor
tn(w),
tn(w) = Zn
rnew(w)
r¬n(w) ,
where the multiplication of Zn is derived from the zeroth-order moment
matching between pn and rnew,

tn(w)r¬n(w)dw =
 tn(w)r¬n(w)dw.
After several passes through all the factors, if the factors converge, then
the posterior is approximated by Eq. (2.44), and the marginal likelihood is
approximated by Z =
 
ntn(w)dw or alternatively by updating it as Z ←ZZn
whenever the factor tn(w) is reﬁned. Although the convergence of EP is not
guaranteed, it is known that if EP converges, the resulting approximating
posterior is a stationary point of a certain energy function (Minka, 2001b).
Local Variational Approximation for EP
In Section 2.1.1, we saw that VB learning minimizes an upper-bound (the free
energy (2.4)) of the Bayes free energy FBayes ≡−log p(D) (or equivalently
maximizing the ELBO). We can say that EP does the opposite. Namely, the EP
problem (2.43) maximizes a lowerbound of the Bayes free energy:
max
r E(r)
s.t.
r ∈G,
(2.45)
where
E(r) = −

p(w, D)
p(D)
log p(w, D)
r(w)
dw
(2.46)
= −

p(w|D) log p(w|D)
r(w) dw −log p(D)
= −KL (p(w|D)∥r(w)) −log p(D).
(2.47)
The maximization form (2.45) of the EP problem can be solved by local
variational approximation, which is akin to the local variational approximation
for VB learning (Section 2.1.7). Let us restrict the search space G for the
approximate posterior r(w;ν) to the function space of a simple distribution

56
2 Variational Bayesian Learning
family, e.g., Gaussian, parameterized with variational parameters ν. Then, the
EP problem (2.45) is reduced to the following unconstrained maximization
problem,
max
ν
E(ν),
(2.48)
of the objective function written as
E(ν) = −

p(w, D)
p(D)
log
p(w, D)
p(D)r(w;ν)dw −log p(D).
(2.49)
Consider lower-bounding the objective (2.49) as
E(ν) ≥E(ν, η) ≡−

p(w; η)
p(D) max
)
0, log
p(w; η)
p(D)r(w;ν)
1
dw −log p(D)
(2.50)
by using a parametric upper-bound p(w; η) of the joint distribution such that
p(w; η) ≥p(w, D)
(2.51)
for any w ∈W and η ∈H, where η is another set of variational parameters
with its domain H.3 Let us choose an upper-bound p(w; η) such that its
function form with respect to w is the same as the approximate posterior
r(w;ν). More speciﬁcally, we assume that, for any given η, there exists ν
such that
p(w; η) ∝r(w;ν)
(2.52)
as a function of w.
Since the direct maximization of E(ν) is intractable, we instead maximize
its lower-bound E(ν, η) jointly overν and η. Namely, we solve the problem,
max
ν,η E(ν, η),
(2.53)
to ﬁnd the approximate posterior r(w;ν) such that E(ν, η) (≤E(ν)) is closest to
the Bayes free energy FBayes (when η is also optimized).
Let
q(w; η) = p(w; η)
Z(η)
(2.54)
be the distribution created by normalizing the upper-bound with its normaliza-
tion factor
Z(η) =

p(w; η)dw.
(2.55)
3 The two sets,ν and η, of variational parameters play the same roles as λ and ξ, respectively, in
the local variational approximation for VB learning.

2.2 Other Approximation Methods
57
Note that the normalization factor (2.55) is trivially an upper-bound of the
marginal likelihood, i.e.,
Z(η) ≥

p(w, D)dw = p(D),
and is tractable because of the assumption (2.52) that p is in the same simple
function form as r.
With Eq. (2.54), the lower-bound (2.50) is expressed as
E(ν, η) = −
 Z(η)q(w; η)
p(D)
max
⎧⎪⎨⎪⎩0, log Z(η)q(w; η)
p(D)r(w;ν)
⎫⎪⎬⎪⎭dw −log p(D)
= −Z(η)
p(D)

q(w; η) max
⎧⎪⎨⎪⎩0, log q(w; η)
r(w;ν) + log Z(η)
p(D)
⎫⎪⎬⎪⎭dw −log p(D).
(2.56)
Eq. (2.56) is upper-bounded by
−Z(η)
p(D)

q(w; η)
⎛⎜⎜⎜⎜⎝log q(w; η)
r(w;ν) + log Z(η)
p(D)
⎞⎟⎟⎟⎟⎠dw −log p(D)
= −Z(η)
p(D)KL $q(w; η)∥r(w;ν)% −Z(η)
p(D) log Z(η)
p(D) −log p(D),
(2.57)
which, for any η ∈H, is maximized whenν is such that
r(w;ν) = q(w; η)
(2.58)
(the assumption (2.52) guarantees the attainability). With this optimal ν,
Eq. (2.57) coincides with Eq. (2.56). Thus, after optimization with respect to
ν, the lower-bound (2.56) is given as
max
ν
E(ν, η) = −Z(η)
p(D) log Z(η)
p(D) −log p(D).
(2.59)
Since x log x for x ≥1 is monotonically increasing, maximizing the lower-
bound (2.59) is achieved by solving
min
η Z(η).
(2.60)
Once the minimizer η is obtained, Eq. (2.58) gives the optimal approximate
posterior.
The problem (2.60) amounts to minimizing an upper-bound of the marginal
likelihood. This is in contrast to the local variational approximation for VB
learning, where a lower-bound of the marginal likelihood is maximized in the
end (compare Eq. (2.33) and Eq. (2.60)).

58
2 Variational Bayesian Learning
–2
–1
0
1
2
0
0.2
0.4
0.6
0.8
1
Bayes posterior
Lower-bound
Upper-bound
Figure 2.2 Bayes posterior and its tightest lower- and upper-bounds, formed by a
Gaussian.
Remembering that the joint distribution is proportional to the Bayes
posterior, i.e., p(w|D) = p(w, D)/p(D), we can say that the VB posterior
is the normalized version of the tightest (in terms of the total mass) lower-
bound of the Bayes posterior, while the EP posterior is the normalized version
of the tightest upper-bound of the Bayes posterior. Figure 2.2 illustrates the
tightest upper-bound and the tightest lower-bound of the Bayes posterior,
which correspond to unnormalized versions of the VB posterior and the EP
posterior, respectively (compare Figures 2.1 and 2.2). This view also explains
the tendency of VB learning and EP—a lower-bound (the VB posterior) must
be zero wherever the Bayes posterior is zero, while an upper-bound (the EP
posterior) must be positive wherever the Bayes posterior is positive.
2.2.4 Metropolis–Hastings Sampling
If a sufﬁcient number of samples {w(1),. . . , w(L)} from the posterior distribution
(1.3) are obtained, the expectation

f(w)p(w|D)dw required for computing
the quantities such as Eqs. (1.6) through (1.9) can be approximated by
1
L
L

l=1
f(w(l)).
The Metropolis–Hastings sampling and the Gibbs sampling are most popular
methods to sample from the (unnormalized) posterior distribution in the
framework of Markov chain Monte Carlo (MCMC).
In the Metropolis–Hastings sampling, we draw samples from a simple
distribution q(w|w(t)) called a proposal distribution, which is conditioned on
the current state w(t) of the parameter (or latent variables) w. The proposal
distribution is chosen to be a simple distribution such as a Gaussian centered
at w(t) if w is continuous or the uniform distribution in a certain neighborhood

2.2 Other Approximation Methods
59
of w(t) if w is discrete. At each cycle of the algorithm, we draw a candidate
sample w∗from the proposal distribution q(w|w(t)), and we accept it with
probability
min

1, p(w∗, D)
p(w(t), D)
q(w(t)|w∗)
q(w∗|w(t))

.
If w∗is accepted, then the next state w(t+1) is moved to w∗, w(t+1) = w∗;
otherwise, it stays at the current state, w(t+1) = w(t). We repeat this procedure
until a sufﬁciently long sequence of states is obtained. Note that if the proposal
distribution is symmetric, i.e., q(w|w′) = q(w′|w) for any w and w′, in which
case the algorithm is called the Metropolis algorithm, the probability of
acceptance depends on the ratio of the posteriors,
p(w∗, D)
p(w(t), D) = p(w∗, D)/Z
p(w(t), D)/Z = p(w∗|D)
p(w(t)|D),
and if w∗has higher posterior probability (density) than w(t), it is accepted with
probability 1.
To guarantee that the distribution of the sampled sequence converges to the
posterior distribution, we discard a ﬁrst part of the sequence, which is called
burn-in. Usually, after the burn-in period, we retain only every Mth sample
and discard the other samples so that the retained samples can be considered
as independent if M is sufﬁciently large.
2.2.5 Gibbs Sampling
Another popular MCMC method is Gibbs sampling, which makes use of the
conditional conjugacy. More speciﬁcally, it is applicable when we can compute
and draw samples from the conditional distribution of a variable of w ∈RJ,
p(wj|w1,. . . , wj−1, wj+1,. . . , wJ, D) ≡p(wj|w¬j, D),
conditioned on the rest of the variables of w.
Assuming that w(t) is obtained at the tth cycle of the Gibbs sampling
algorithm, the next sample of each variable is drawn from the conditional
distribution,
p(w(t+1)
j
|w(t)
¬j, D),
where
w(t)
¬j = (w(t+1)
1
,. . . , w(t+1)
j−1 , w(t)
j+1,. . . , w(t)
J )
from j = 1 to J in turn.
This sampling procedure can be viewed as a special case of the Metropolis–
Hastings algorithm. If the proposal distribution q(w|w(t)) is chosen to be

60
2 Variational Bayesian Learning
p(wj|w(t)
¬j, D)δ(w¬j −w(t)
¬j),
then the probability that the candidate w∗is accepted is 1 since w∗
¬j = w(t)
¬j
implies that
p(w∗, D)
p(w(t), D)
q(w(t)|w∗)
q(w∗|w(t)) = p(w∗|D)
p(w(t)|D)
p(w(t)
j |w(t)
¬j, D)
p(w∗
j|w(t)
¬j, D)
=
p(w∗
¬j|D)p(w∗
j|w∗
¬j, D)
p(w(t)
¬j|D)p(w(t)
j |w(t)
¬j, D)
p(w(t)
j |w(t)
¬j, D)
p(w∗
j|w(t)
¬j, D)
= 1.
As we have seen in the Metropolis–Hastings and Gibbs sampling algo-
rithms, MCMC methods do not require the knowledge of the normalization
constant Z =

p(w, D)dw. Note that, however, even if we have samples
from the posterior, we need additional steps to compute Z with the samples.
A simple way is to calculate the expectation of the inverse of the likelihood by
the sample average,
/
1
p(D|w)
0
p(w|D)
≃1
L
L

l=1
1
p(D|w(l)).
It provides an estimate of the inverse of Z because
/
1
p(D|w)
0
p(w|D)
=

1
p(D|w)
p(D|w)p(w)
Z
dw = 1
Z

p(w)dw = 1
Z .
However, this estimator is known to have high variance. A more sophisticated
sampling method to compute Z was developed by Chib (1995), while it
requires multiple runs of MCMC sampling.
A new efﬁcient method to compute the marginal likelihood was recently
proposed and named a widely applicable Bayesian information criterion
(WBIC), which requires only a single run of MCMC sampling from a
generalized posterior distribution (Watanabe, 2013). This method computes
the expectation of the negative log-likelihood,
−log p(D|w)
p(β)(w|D) ,
over the β-generalized posterior distribution deﬁned as
p(β)(w|D) ∝p(D|w)βp(w),
with β = 1/ log N, where N is the number of i.i.d. samples. The computed
(approximated) expectation is proved to have the same leading terms as those
of the asymptotic expansion of −log Z as N →∞.

Part II
Algorithm


3
VB Algorithm for Multilinear Models
In this chapter, we derive iterative VB algorithms for multilinear models with
Gaussian noise, where we can rely on the conditional conjugacy with respect
to each linear factor. The models introduced in this chapter will be further ana-
lyzed in Part III, where the global solution or its approximation is analytically
derived, and the behavior of the VB solution is investigated in detail.
3.1 Matrix Factorization
Assume that we observe a matrix V ∈RL×M, which is the sum of a target
matrix U ∈RL×M and a noise matrix E ∈RL×M:
V = U + E.
In the matrix factorization (MF) model (Srebro and Jaakkola, 2003; Srebro
et al., 2005; Lim and Teh, 2007; Salakhutdinov and Mnih, 2008; Ilin and Raiko,
2010) or the probabilistic principal component analysis (probabilistic PCA)
(Tipping and Bishop, 1999; Bishop, 1999b), the target matrix is assumed to be
low rank, and therefore can be factorized as
U = BA⊤,
where A ∈RM×H, B ∈RL×H for H ≤min(L, M) are unknown parameters
to be estimated, and ⊤denotes the transpose of a matrix or vector. Here, the
rank of U is upper-bounded by H. We denote a column vector of a matrix by a
bold lowercase letter, and a row vector by a bold lowercase letter with a tilde,
namely,
A = (a1,. . . , aH) = $a1,. . . ,aM
%⊤∈RM×H,
B = (b1,. . . , bH) =
b1,. . . ,bL
 ⊤∈RL×H.
63

64
3 VB Algorithm for Multilinear Models
3.1.1 VB Learning for MF
Assume that the observation noise E is independent Gaussian:
p(V|A, B) ∝exp

−1
2σ2
###V −BA⊤###2
Fro

,
(3.1)
where ∥·∥Fro denotes the Frobenius norm.
Conditional Conjugacy
If we treat B as a constant, the likelihood (3.1) is in the Gaussian form of A.
Similarly, if we treat A as a constant, the likelihood (3.1) is in the Gaussian
form of B. Therefore, conditional conjugacy with respect to A given B, as
well as with respect to B given A, holds if we adopt Gaussian priors:
p(A) ∝exp

−1
2tr

AC−1
A A⊤ 
,
(3.2)
p(B) ∝exp

−1
2tr

BC−1
B B⊤ 
,
(3.3)
where tr(·) denotes the trace of a matrix.
Typically, the prior covariance matrices CA and CB are restricted to be diag-
onal, which induces low-rankness (we discuss this mechanism in Chapter 7):
CA = Diag(c2
a1,. . . , c2
aH),
CB = Diag(c2
b1,. . . , c2
bH),
for cah, cbh > 0, h = 1,. . . , H.
Variational Bayesian Algorithm
Thanks to the conditional conjugacy, the following independence constraint
makes the approximate posterior Gaussian:
r(A, B) = rA(A)rB(B).
(3.4)
The VB learning problem (2.13) is then reduced to
r = argmin
r
F(r)
s.t.
r(A, B) = rA(A)rB(B).
(3.5)
Under the constraint (3.4), the free energy is written as
F(r) =
/
log
rA(A)rB(B)
p(V|A, B)p(A)p(B)
0
rA(A)rB(B)
=

rA(A)rB(B) log
rA(A)rB(B)
p(V|A, B)p(A)p(B)dAdB.
(3.6)

3.1 Matrix Factorization
65
Following the recipe described in Section 2.1.5, we take the derivatives of
the free energy (3.6) with respect to rA(A) and rB(B), respectively. Thus, we
obtain the following stationary conditions:
rA(A) ∝p(A) exp log p(V|A, B)
rB(B) ,
(3.7)
rB(B) ∝p(B) exp log p(V|A, B)
rA(A) .
(3.8)
By substituting the likelihood (3.1) and the prior (3.2) into Eq. (3.7), we
obtain
rA(A) ∝exp

−1
2tr

AC−1
A A⊤ 
−
1
2σ2
*###V −BA⊤###2
Fro
+
rB(B)

∝exp

−1
2tr
!
AC−1
A A⊤+ σ−2 
−2V⊤BA⊤+ AB⊤BA⊤
rB(B)
"
∝exp
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
−
tr
!
(A −A)Σ
−1
A (A −A)⊤
"
2
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
,
(3.9)
where
A = σ−2V⊤⟨B⟩rB(B) ΣA,
(3.10)
ΣA = σ2 !
B⊤B

rB(B) + σ2C−1
A
"−1
.
(3.11)
Similarly, by substituting the likelihood (3.1) and the prior (3.3) into Eq. (3.8),
we obtain
rB(B) ∝exp

−1
2tr

BC−1
B B⊤ 
−
1
2σ2
*###V −BA⊤###2
Fro
+
rA(A)

∝exp

−1
2tr
!
BC−1
B B⊤+ σ−2 
−2VAB⊤+ BA⊤AB⊤
rA(A)
"
∝exp
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
−
tr
!
(B −B)Σ
−1
B (B −B)⊤
"
2
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
,
(3.12)
where
B = σ−2V ⟨A⟩rA(A) ΣB,
(3.13)
ΣB = σ2 !
A⊤A

rA(A) + σ2C−1
B
"−1
.
(3.14)
Eqs. (3.9) and (3.12) imply that the posteriors are Gaussian. More speciﬁ-
cally, they can be written as
rA(A) = MGaussM,H(A; A, IM ⊗ΣA),
(3.15)
rB(B) = MGaussL,H(B; B, IL ⊗ΣB),
(3.16)

66
3 VB Algorithm for Multilinear Models
where ⊗denotes the Kronecker product, and
MGaussD1,D2(X; M, ˘Σ) ≡GaussD1·D2(vec(X⊤); vec(M⊤), ˘Σ)
(3.17)
denotes the matrix variate Gaussian distribution (Gupta and Nagar, 1999).
Here, vec : RD2×D1 →RD2D1 is the vectorization operator, which concatenates
all column vectors of a matrix into a long column vector. Note that, if the
covariance has a speciﬁc structure expressed as ˘Σ = Σ ⊗Ψ ∈RD2D1×D2D1,
such as Eqs. (3.15) and (3.16), the matrix variate Gaussian distribution can be
written as
MGaussD1,D2(X; M, Σ ⊗Ψ) ≡
1
(2π)D1D2/2 det (Σ)D2/2 det (Ψ)D1/2
· exp

−1
2tr

Σ−1 (X −M) Ψ−1 (X −M)⊤ 
.
(3.18)
The fact that the posterior is Gaussian is a consequence of the forced
independence between A and B and conditional conjugacy. The parameters,
,A, B, ΣA, ΣB
-
, deﬁning the VB posterior (3.15) and (3.16), are the variational
parameters.
Since rA(A) and rB(B) are Gaussian, the ﬁrst and the (noncenterized) second
moments can be expressed with variational parameters as follows:
⟨A⟩rA(A) = A,

A⊤A

rA(A) = A
⊤A + MΣA,
⟨B⟩rB(B) = B,

B⊤B

rB(B) = B
⊤B + LΣB.
By substituting the preceding into Eqs. (3.10), (3.11), (3.13), and (3.14), we
have the following relations among the variational parameters:
A = σ−2V⊤BΣA,
(3.19)
ΣA = σ2 !
B
⊤B + LΣB + σ2C−1
A
"−1
,
(3.20)
B = σ−2VAΣB,
(3.21)
ΣB = σ2 !
A
⊤A + MΣA + σ2C−1
B
"−1
.
(3.22)
As we see shortly, Eqs. (3.19) through (3.22) are stationary conditions for
variational parameters, which can be used as update rules for coordinate
descent local search (Bishop, 1999b).

3.1 Matrix Factorization
67
Free Energy as a Function of Variational Parameters
By substituting Eqs. (3.15) and (3.16) into Eq. (3.6), we can explicitly write
down the free energy as (not a functional but) a function of the unknown
variational parameters
,A, B, ΣA, ΣB
-
:
2F = 2
/
log
rA(A)rB(B)
p(V|A, B)p(A)p(B)
0
rA(A)rB(B)
= 2
/
log rA(A)rB(B)
p(A)p(B)
0
rA(A)rB(B)
−2 log p(V|A, B)
rA(A)rB(B)
=
/
M log det (CA)
det
ΣA
 + L log det (CB)
det
ΣB
 + tr

C−1
A A⊤A + C−1
B B⊤B
 
−tr
!
Σ
−1
A (A −A)⊤(A −A) + Σ
−1
B (B −B)⊤(B −B)
"
+ LM log(2πσ2) + ∥V −BA⊤∥2
Fro
σ2
0
rA(A)rB(B)
= M log det (CA)
det
ΣA
 + L log det (CB)
det
ΣB
 −tr
!
MΣ
−1
A ΣA + LΣ
−1
B ΣB
"
+ tr
!
C−1
A
!
A
⊤A + MΣA
"
+ C−1
B
!
B
⊤B + LΣB
""
+ LM log(2πσ2) +
/
∥(V−BA
⊤)+(BA
⊤−BA⊤)∥2
Fro
σ2
0
rA(A)rB(B)
= M log det (CA)
det
ΣA
 + L log det (CB)
det
ΣB
 −(L + M)H
+ tr
!
C−1
A
!
A
⊤A + MΣA
"
+ C−1
B
!
B
⊤B + LΣB
""
+ LM log(2πσ2) +
∥V−BA
⊤∥2
Fro
σ2
+
/
∥BA
⊤−BA⊤∥2
Fro
σ2
0
rA(A)rB(B)
= LM log(2πσ2) +
####V −BA
⊤####
2
Fro
σ2
+ M log det (CA)
det
ΣA
 + L log det (CB)
det
ΣB
 
−(L + M)H + tr
2
C−1
A
!
A
⊤A + MΣA
"
+ C−1
B
!
B
⊤B + LΣB
"
+ σ−2 !
−A
⊤AB
⊤B +
!
A
⊤A + MΣA
" !
B
⊤B + LΣB
""3
.
(3.23)
Now, the VB learning problem is reduced from the function optimization
(3.5) to the following variable optimization:

68
3 VB Algorithm for Multilinear Models
Given
CA, CA ∈DH
++,
σ2 ∈R++,
min
A,B,ΣA,ΣB
F,
(3.24)
s.t.
A ∈RM×H, B ∈RL×H,
ΣA, ΣB ∈SH
++,
where R++ is the set of positive real numbers, SD
++ is the set of D × D
(symmetric) positive deﬁnite matrices, and DD
++ is the set of D × D positive
deﬁnite diagonal matrices.
We note the following:
• Once the solution
,A, B, ΣA, ΣB
-
of the problem (3.24) is obtained, Eqs.
(3.15) and (3.16) specify the VB posteriorr(A, B) = rA(A)rB(B).
• We treated the prior covariances CA and CB and the noise variance σ2 as
hyperparameters, and therefore assumed to be given when the VB problem
was solved. However, they can be estimated through the empirical Bayesian
procedure, which is explained shortly. They can also be treated as random
variables, and their VB posterior can be computed by adopting conjugate
Gamma priors and minimizing the free energy under an appropriate
independence constraint.
• Eqs. (3.19) through (3.22) coincide with the stationary conditions of the
free energy (3.23), which are derived from the derivatives with respect to
A, ΣA, B, and ΣB, respectively. Therefore, iterating Eqs. (3.19) through
(3.22) gives a local solution to the problem (3.24).
Empirical Variational Bayesian Algorithm
The empirical variational Bayesian (EVB) procedure can be performed by
minimizing the free energy also with respect to the hyperparameters:
min
A,B,ΣA,ΣB,CA,CA,σ2
F,
(3.25)
s.t.
A ∈RM×H, B ∈RL×H,
ΣA, ΣB ∈SH
++,
CA, CA ∈DH
++,
σ2 ∈R++.
By differentiating the free energy (3.23) with respect to each entry of CA
and CB, we have, for h = 1,. . . , H,
c2
ah =
###ah
###2/M +
ΣA
 
h,h ,
(3.26)
c2
bh =
####bh
####
2
/L +
ΣB
 
h,h .
(3.27)

3.1 Matrix Factorization
69
Algorithm 1 EVB learning for matrix factorization.
1: Initialize the variational parameters (A, ΣA, B, ΣB), and the hyperparame-
ters (CA, CB, σ2), for example, Am,h, Bl,h ∼Gauss1(0, τ), ΣA = ΣB = CA =
CB = τIH, and σ2 = τ2 for τ2 = ∥V∥2
Fro/(LM).
2: Apply (substitute the right-hand side into the left-hand side) Eqs. (3.20),
(3.19), (3.22), and (3.21) to update ΣA, A, ΣB, and B, respectively.
3: Apply Eqs. (3.26) and (3.27) for all h = 1,. . . , H, and Eq. (3.28) to update
CA, CB, and σ2, respectively.
4: Prune the hth component if c2
ahc2
bh < ε, where ε > 0 is a small threshold,
e.g., set to ε = 10−4.
5: Evaluate the free energy (3.23).
6: Iterate Steps 2 through 5 until convergence (until the energy decrease
becomes smaller than a threshold).
Similarly, by differentiating the free energy (3.23) with respect to σ2, we have
σ2 =
∥V∥2
Fro −tr
!
2V⊤BA
⊤"
+ tr
!
(A
⊤A + MΣA)(B
⊤B + LΣB)
"
LM
.
(3.28)
Eqs. (3.26)–(3.28) are used as update rules for the prior covariances CA, CB,
and the noise variance σ2, respectively.
Starting from some initial value, iterating Eqs. (3.19) through (3.22) and
Eqs. (3.26) through (3.28) gives a local solution for EVB learning. Algorithm 1
summarizes this iterative procedure. If we appropriately set the hyperpara-
meters (CA, CB, σ2) in Step 1 and skip Steps 3 and 4, Algorithm 1 is reduced
to (nonempirical) VB learning.
We note the following for implementation:
• Due to the automatic relevance determination (ARD) effect in EVB
learning (see Chapter 7), c2
ahc2
bh converges to zero for some h. For this
reason, “pruning” in Step 4 is necessary for numerical stability (log det (C)
diverges if C is singular). If the hth component is pruned, the corresponding
hth column of A and B and the hth column and row of ΣA, ΣB, CA, CB
should be removed, and the rank H should be reduced accordingly.
• In principle, the update rules never increase the free energy. However,
pruning can slightly increase it.
• When computing the free energy by Eq. (3.23), log det (·) should be
computed as twice the sum of the log of the diagonals of the Cholesky
decomposition, i.e.,

70
3 VB Algorithm for Multilinear Models
log det (C) = 2
H

h=1
$log(Chol(C))h,h
% .
Otherwise, det (·) can be huge for practical size of H, causing numerical
instability.
Simple Variational Bayesian Learning (with Columnwise Independence)
The updates (3.19) through (3.22) require inversion of an H × H matrix. One
can derive a faster VB learning algorithm by using a stronger constraint for the
VB learning. More speciﬁcally, instead of the matrixwise independence (3.4),
we assume the independence between the column vectors of A = (a1,. . . , aH)
and B = (b1,. . . , bH) (Ilin and Raiko, 2010; Nakajima and Sugiyama, 2011;
Kim and Choi, 2014):
r(A, B) =
H

h=1
rah(ah)
H

h=1
rbh(bh).
(3.29)
By applying the same procedure as that with the matrixwise independence
constraint, we can derive the solution to
r = argmin
r
F(r)
s.t.
r(A, B) =
H

h=1
rah(ah)
H

h=1
rbh(bh),
(3.30)
which is in the form of the matrix variate Gaussian:
rA(A) = MGaussM,H(A; A, IM ⊗ΣA) =
H

h=1
GaussM(ah;ah, σ2
ah IM),
rB(B) = MGaussL,H(B; B, IL ⊗ΣB) =
H

h=1
GaussL(bh;bh, σ2
bhIL),
with the variational parameters,
A = (a1,. . . ,aH),
B = (b1,. . . ,bH),
ΣA = Diag(σ2
a1,. . . , σ2
aH),
ΣB = Diag(σ2
b1,. . . , σ2
bH).
Here Diag(· · · ) denotes the diagonal matrix with the speciﬁed diagonal entries.
The stationary conditions are given as follows: for all h = 1,. . . , H,
ah =
σ2
ah
σ2
⎛⎜⎜⎜⎜⎜⎜⎝V −

h′h
bh′a⊤
h′
⎞⎟⎟⎟⎟⎟⎟⎠
⊤
bh,
(3.31)

3.1 Matrix Factorization
71
σ2
ah = σ2
####bh
####
2
+ Lσ2
bh + σ2
c2ah
−1
,
(3.32)
bh =
σ2
bh
σ2
⎛⎜⎜⎜⎜⎜⎜⎝V −

h′h
bh′a⊤
h′
⎞⎟⎟⎟⎟⎟⎟⎠ah,
(3.33)
σ2
bh = σ2
⎛⎜⎜⎜⎜⎜⎝
###ah
###2 + Mσ2
ah + σ2
c2
bh
⎞⎟⎟⎟⎟⎟⎠
−1
.
(3.34)
The free energy is given by Eq. (3.23) with the posterior covariances ΣA and
ΣB restricted to be diagonal. The stationary conditions for the hyperparameters
are unchanged, and given by Eqs. (3.26) through (3.28). Therefore, Algorithm
1 with Eqs. (3.31) through (3.34), substituted for Eqs. (3.19) through (3.22),
gives a local solution to the VB problem (3.30) with the columnwise indepen-
dence constraint.
We call this variant simple VB (SimpleVB) learning. In Chapter 6, it will
be shown that, in the fully observed MF model, the SimpleVB problem
(3.30) with columnwise independence and the original VB problem (3.5) with
matrixwise independence actually give the equivalent solution.
3.1.2 Special Cases
Probabilistic principal component analysis and reduced rank regression are
special cases of matrix factorization. Therefore, they can be trained by
Algorithm 1 with or without small modiﬁcations.
Probabilistic Principal Component Analysis Probabilistic principal com-
ponent analysis (Tipping and Bishop, 1999; Bishop, 1999b) is a probabilisitic
model of which the ML estimation corresponds to the classical principal
component analysis (PCA) (Hotelling, 1933). The observation v ∈RL is
assumed to be driven by a latent vector a ∈RH in the following form:
v = Ba + ε.
Here, B ∈RL×H speciﬁes the linear relationship between a and v, and ε ∈RL
is a Gaussian noise subject to GaussL(0, σ2IL).
Suppose that we are given M observed samples V = (v1,. . . , vM) generated
from the latent vectors A⊤= (a1,. . . ,aM), and each latent vector is subject to
a ∼GaussH(0, IH). Then, the probabilistic PCA model is written as Eqs. (3.1)
and (3.2) with CA = IH. Having the prior (3.2) on B, it is equivalent to the MF
model.

72
3 VB Algorithm for Multilinear Models
Figure 3.1 Reduced rank regression model.
If we apply VB or EVB learning, the intrinsic dimension H is automatically
selected without additional procedure (Bishop, 1999b). This useful property is
caused by the ARD (Neal, 1996), which makes the estimators for the irrelevant
column vectors of A and B zero. In Chapter 7, this phenomenon is explained in
terms of model-induced regularization (MIR), while in Chapter 8, a theoretical
guarantee of the dimensionality estimation is given.
Reduced Rank Regression Reduced rank regression (RRR) (Baldi and
Hornik, 1995; Reinsel and Velu, 1998) is aimed at learning a relation between
an input vector x ∈RM and an output vector y ∈RL by using the following
linear model:
y = BA⊤x + ε,
(3.35)
where A ∈RM×H and B ∈RL×H are parameters to be estimated, and
ε ∼GaussL(0, σ′2IL) is a Gaussian noise. RRR can be seen as a linear neural
network (Figure 3.1), of which the model distribution is given by
p(y|x, A, B) =

2πσ′2 −L/2 exp

−1
2σ′2
###y −BA⊤x
###2
.
(3.36)
Thus, we can interpret this model as ﬁrst projecting the input vector x
onto a lower-dimensional latent subspace by A⊤and then performing linear
prediction by B.
Suppose we are given N pairs of input and output vectors:
D =
,
(x(n), y(n))|x(n) ∈RM, y(n) ∈RL, n = 1,. . . , N
-
.
(3.37)
Then, the likelihood of the RRR model (3.36) is expressed as
p(D|A, B) =
N

n=1
p(y(n)|x(n), A, B)p(x(n))
∝exp
⎛⎜⎜⎜⎜⎜⎝−1
2σ′2
N

n=1
###y(n) −BA⊤x(n)###2
⎞⎟⎟⎟⎟⎟⎠.
(3.38)

3.1 Matrix Factorization
73
Note that we here ignored the input distributions N
n=1 p(x(n)) as constants (see
Example 1.2 in Section 1.1.1). Let us assume that the samples are centered:
1
N
N

n=1
x(n) = 0
and
1
N
N

n=1
y(n) = 0.
Furthermore, let us assume that the input samples are prewhitened (Hyv¨arinen
et al., 2001), i.e., they satisfy
1
N
N

n=1
x(n)x(n)⊤= IM.
Let
V = ΣXY = 1
N
N

n=1
y(n)x(n)⊤
(3.39)
be the sample cross-covariance matrix, and
σ2 = σ′2
N
(3.40)
be a rescaled noise variance. Then the exponent of the likelihood (3.38) can be
written as
−1
2σ′2
N

n=1
###y(n) −BA⊤x(n)###2
= −1
2σ′2
N

n=1
2###y(n)###2 −2tr

y(n)x(n)⊤AB⊤ 
+ tr

BA⊤x(n)x(n)⊤AB⊤ 3
= −1
2σ′2
⎧⎪⎪⎨⎪⎪⎩
N

n=1
###y(n)###2 −2Ntr

VAB⊤ 
+ Ntr

AB⊤BA⊤ ⎫⎪⎪⎬⎪⎪⎭
= −1
2σ′2
⎛⎜⎜⎜⎜⎜⎝N
###V −BA⊤###2
Fro +
N

n=1
###y(n)###2 −N ∥V∥2
Fro
⎞⎟⎟⎟⎟⎟⎠
= −1
2σ2
###V −BA⊤###2
Fro −
1
2σ2
⎛⎜⎜⎜⎜⎜⎝
1
N
N

n=1
###y(n)###2 −∥V∥2
Fro
⎞⎟⎟⎟⎟⎟⎠.
(3.41)
The ﬁrst term in Eq. (3.41) coincides with the log-likelihood of the MF model
(3.1), and the second term is constant with respect to A and B. Thus, RRR is
reduced to MF, as far as the posteriors for A and B are concerned.
However, the second term depends on the rescaled noise variance σ2,
and therefore, should be considered when σ2 is estimated based on the free
energy minimization principle. Furthermore, the normalization constant of the

74
3 VB Algorithm for Multilinear Models
likelihood (3.38) differs from that of the MF model. Taking these differences
into account, the VB free energy of the RRR model (3.38) with the priors (3.2)
and (3.3) is given by
2FRRR = NL log(2πNσ2) +
1
N
N
n=1
###y(n)###2 −∥V∥2
Fro
σ2
+
####V −BA
⊤####
2
Fro
σ2
+ M log det (CA)
det
ΣA
 + L log det (CB)
det
ΣB
 
−(L + M)H + tr
2
C−1
A
!
A
⊤A + MΣA
"
+ C−1
B
!
B
⊤B + LΣB
"
+σ−2 !
−A
⊤AB
⊤B +
!
A
⊤A + MΣA
" !
B
⊤B + LΣB
""3
.
(3.42)
Note that the difference from Eq. (3.23) is only in the ﬁrst two terms.
Accordingly, the stationary conditions for the variational parameters A, B, ΣA,
and ΣB, and those for the prior covariances CA and CB (in EVB learning) are
the same, i.e., the update rules given by Eqs. (3.19) through (3.22), (3.26), and
(3.27) are valid for the RRR model. The update rule for the rescaled noise
variance is different from Eq. (3.28), and given by
(σ2)RRR =
1
N
N
n=1
###y(n)###2 −tr
!
2V⊤BA
⊤"
+ tr
!
(A
⊤A + MΣA)(B
⊤B + LΣB)
"
NL
,
(3.43)
which was obtained from the derivative of Eq. (3.42), instead of Eq. (3.23),
with respect to σ2.
Once the rescaled noise variance σ2 is estimated, Eq. (3.40) gives the
original noise variance σ′2 of the RRR model (3.38).
3.2 Matrix Factorization with Missing Entries
One of the major applications of MF is collaborative ﬁltering (CF), where
only a part of the entries in V are observed, and the task is to predict missing
entries. We can derive a VB algorithm for this scenario, similarly to the fully
observed case.
3.2.1 VB Learning for MF with Missing Entries
To express missing entris, the likelihood (3.1) should be replaced with
p(V|A, B) ∝exp

−1
2σ2
####PΛ (V) −PΛ

BA⊤ ####
2
Fro

,
(3.44)

3.2 Matrix Factorization with Missing Entries
75
where Λ denotes the set of observed indices, and PΛ (V) denotes the matrix of
the same size as V with its entries given by
(PΛ (V))l,m =
⎧⎪⎪⎨⎪⎪⎩
Vl,m
if (l, m) ∈Λ,
0
otherwise.
Conditional Conjugacy
Since the likelihood (3.44) is still in a Gaussian form of A if B is regarded as
a constant, or vise versa, the conditional conjugacy with respect to A and B,
respectively, still holds if we adopt the Gaussian priors (3.2) and (3.3):
p(A) ∝exp

−1
2tr

AC−1
A A⊤ 
,
p(B) ∝exp

−1
2tr

BC−1
B B⊤ 
.
The posterior will be still Gaussian, but in a broader class than the fully
observed case, as will be seen shortly.
Variational Bayesian Algorithm
With the missing entries, the stationary condition (3.7) becomes
rA(A) ∝exp

−1
2tr

AC−1
A A⊤ 
−
1
2σ2
*####PΛ (V) −PΛ

BA⊤ ####
2
Fro
+
rB(B)

∝exp

−1
2tr

AC−1
A A⊤ 
+ σ−2 
(l,m)∈Λ
/
−2Vl,m
H

h=1
Bl,hAm,h +
H

h=1
H

h′=1
Bl,hBl,h′Am,hAm,h′
0
rB(B)
⎞⎟⎟⎟⎟⎟⎟⎟⎠
∝exp
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
−
M
m=1
!
(am −am)⊤Σ
−1
A,m(am −am)
"
2
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
,
(3.45)
where
am = σ−2ΣA,m

l:(l,m)∈Λ
Vl,m
bl

rB(B) ,
(3.46)
ΣA,m = σ2
⎛⎜⎜⎜⎜⎜⎜⎝

l:(l,m)∈Λ
*
blb
⊤
l
+
rB(B) + σ2C−1
A
⎞⎟⎟⎟⎟⎟⎟⎠
−1
.
(3.47)
Here, 
(l,m)∈Λ denotes the sum over l and m such that (l, m) ∈Λ, and 
l:(l,m)∈Λ
denotes the sum over l such that (l, m) ∈Λ for given m.

76
3 VB Algorithm for Multilinear Models
Similarly, we have
rB(B) ∝exp

−1
2tr

BC−1
B B⊤ 
−
1
2σ2
*####PΛ (V) −PΛ

BA⊤ ####
2
Fro
+
rA(A)

∝exp
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
−
L
l=1
!
(bm −bl)⊤Σ
−1
B,l(bl −bl)
"
2
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
,
(3.48)
where
bl = σ−2ΣB,l

m:(l,m)∈Λ
Vl,m
am

rA(A) ,
(3.49)
ΣB,l = σ2
⎛⎜⎜⎜⎜⎜⎜⎝

m:(l,m)∈Λ

ama⊤
m

rA(A) + σ2C−1
B
⎞⎟⎟⎟⎟⎟⎟⎠
−1
.
(3.50)
Eqs. (3.45) and (3.48) imply that A and B are Gaussian in the following
form:
rA(A) = MGaussM,H(A; A, ˘ΣA) =
M

m=1
GaussH(am;am, ΣA,m),
rB(B) = MGaussL,H(B; B, ˘ΣB) =
L

l=1
GaussH(bm;bl, ΣB,l),
where
˘ΣA =
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
ΣA,1
0
· · ·
0
0
ΣA,2
...
...
...
0
0
· · ·
0
ΣA,M
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
,
˘ΣB =
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
ΣB,1
0
· · ·
0
0
ΣB,2
...
...
...
0
0
· · ·
0
ΣB,L
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
.
Note that the posterior covariances cannot be expressed with a Kronecker
product, unlike the fully observed case. However, the posteriors are Gaussian,
and moments are given by
am

rA(A) = am,

ama⊤
m

rA(A) = ama
⊤
m + ΣA,m,
bl

rB(B) = bl,
*
blb
⊤
l
+
rB(B) = blb
⊤
l + ΣB,l.

3.2 Matrix Factorization with Missing Entries
77
Thus, Eqs. (3.46), (3.47), (3.49), and (3.50) lead to
am = σ−2ΣA,m

l:(l,m)∈Λ
Vl,mbl,
(3.51)
ΣA,m = σ2
⎛⎜⎜⎜⎜⎜⎜⎝

l:(l,m)∈Λ

blb
⊤
l + ΣB,l

+ σ2C−1
A
⎞⎟⎟⎟⎟⎟⎟⎠
−1
,
(3.52)
bl = σ−2ΣB,l

m:(l,m)∈Λ
Vl,mam,
(3.53)
ΣB,l = σ2
⎛⎜⎜⎜⎜⎜⎜⎝

m:(l,m)∈Λ
!
ama
⊤
m + ΣA,m
"
+ σ2C−1
B
⎞⎟⎟⎟⎟⎟⎟⎠
−1
,
(3.54)
which are used as update rules for local search (Lim and Teh, 2007).
Free Energy as a Function of Variational Parameters
An explicit form of the free energy can be obtained in a similar fashion to the
fully observed case:
2F = # (Λ) · log(2πσ2) + M log det (CA) + L log det (CB)
−
M

m=1
log det
ΣA,m
 
−
L

l=1
log det
ΣB,l
 
−(L + M)H
+ tr
⎧⎪⎪⎨⎪⎪⎩C−1
A
⎛⎜⎜⎜⎜⎜⎝A
⊤A +
M

m=1
ΣA,m
⎞⎟⎟⎟⎟⎟⎠+ C−1
B
⎛⎜⎜⎜⎜⎜⎝B
⊤B +
L

l=1
ΣB,l
⎞⎟⎟⎟⎟⎟⎠
⎫⎪⎪⎬⎪⎪⎭
+ σ−2 
(l,m)∈Λ

Vl,m −2Vl,ma
⊤
mbl + tr
)!
ama
⊤
m + ΣA,m
" 
blb
⊤
l + ΣB,l
1
,
(3.55)
where # (Λ) denotes the number of observed entries.
Empirical Variational Bayesian Algorithm
By taking derivatives of the free energy (3.55), we can derive update rules for
the hyperparameters:
c2
ah =
###ah
###2 +
M
m=1 ΣA,m
 
h,h
M
,
(3.56)
c2
bh =
####bh
####
2
+
L
l=1 ΣB,l
 
h,h
L
,
(3.57)

78
3 VB Algorithm for Multilinear Models
Algorithm 2 EVB learning for matrix factorization with missing entries.
1: Initialize the variational parameters (A, {ΣA,m}M
m=1, B, {ΣB,l}L
l=1), and the
hyperparameters (CA, CB, σ2), for example, Am,h, Bl,h ∼Gauss1 (0, τ),
ΣA,m = ΣB,l = CA = CB = τIH, and σ2 = τ2 for τ2 = 
(l,m)∈Λ V2
l,m/# (Λ).
2: Apply (substitute the right-hand side into the left-hand side) Eqs. (3.52),
(3.51), (3.54), and (3.53) to update {ΣA,m}M
m=1, A, {ΣB,l}L
l=1, and B, respec-
tively.
3: Apply Eqs. (3.56) and (3.57) for all h = 1,. . . , H, and Eq. (3.58) to update
CA, CB, and σ2, respectively.
4: Prune the hth component if c2
ahc2
bh < ε, where ε > 0 is a threshold, e.g., set
to ε = 10−4.
5: Evaluate the free energy (3.55).
6: Iterate Steps 2 through 5 until convergence (until the energy decrease
becomes smaller than a threshold).
σ2 =

(l,m)∈Λ

Vl,m −2Vl,ma
⊤
mbl + tr
)!
ama
⊤
m + ΣA,m
" 
blb
⊤
l + ΣB,l
1
# (Λ)
. (3.58)
Algorithm 2 summarizes the EVB algorithm for MF with missing entries.
Again, if we appropriately set the hyperparameters (CA, CB, σ2) in Step 1 and
skip Steps 3 and 4, Algorithm 2 is reduced to (nonempirical) VB learning.
Simple Variational Bayesian Learning (with Columnwise Independence)
Similarly to the fully observed case, we can reduce the computational burden
and the memory requirement of VB learning by adopting the columnwise
independence (Ilin and Raiko, 2010):
r(A, B) =
H

h=1
rah(ah)
H

h=1
rbh(bh).
(3.59)
By applying the same procedure as the matrixwise independence case, we can
derive the solution to
r = argmin
r
F(r)
s.t.
r(A, B) =
H

h=1
rah(ah)
H

h=1
rbh(bh),
(3.60)
which is in the form of the matrix variate Gaussian,

3.2 Matrix Factorization with Missing Entries
79
rA(A) = MGaussM,H(A; A, ˘ΣA) =
M

m=1
H

h=1
Gauss1(Am,h; Am,h, σ2
Am,h),
rB(B) = MGaussL,H(B; B, ˘ΣB) =
L

l=1
H

h=1
Gauss1(Bl,h; Bl,h, σ2
Bl,h),
with diagonal posterior covariances, i.e.,
˘ΣA =
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
ΣA,1
0
· · ·
0
0
ΣA,2
...
...
...
0
0
· · ·
0
ΣA,M
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
,
˘ΣB =
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
ΣB,1
0
· · ·
0
0
ΣB,2
...
...
...
0
0
· · ·
0
ΣB,L
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
,
for
ΣA,m = Diag(σ2
Am,1,. . . , σ2
Am,H),
ΣB,l = Diag(σ2
Bl,1,. . . , σ2
Bl,H).
The stationary conditions are given as follows: for all l = 1,. . . , L,
m = 1,. . . , M, and h = 1,. . . , H,
Am,h =
σ2
Am,h
σ2

l;(l,m)∈Λ
⎛⎜⎜⎜⎜⎜⎜⎝Vl,m −

h′h
Bl,h′ Am,h′
⎞⎟⎟⎟⎟⎟⎟⎠Bl,h,
(3.61)
σ2
Am,h = σ2
⎛⎜⎜⎜⎜⎜⎜⎝

l;(l,m)∈Λ
B2
l,h + σ2
Bl,h
 
+ σ2
c2ah
⎞⎟⎟⎟⎟⎟⎟⎠
−1
,
(3.62)
Bl,h =
σ2
Bl,h
σ2

m;(l,m)∈Λ
⎛⎜⎜⎜⎜⎜⎜⎝Vl,m −

h′h
Am,h′Bl,h′
⎞⎟⎟⎟⎟⎟⎟⎠Am,h,
(3.63)
σ2
Bl,h = σ2
⎛⎜⎜⎜⎜⎜⎜⎝

m;(l,m)∈Λ
A2
m,h + σ2
Am,h
 
+ σ2
c2
bh
⎞⎟⎟⎟⎟⎟⎟⎠
−1
.
(3.64)
The free energy is given by Eq. (3.55) with the posterior covariances
{ΣA,m, ΣB,l} restricted to be diagonal. The stationary conditions for the hyper-
parameters are unchanged, and given by Eqs. (3.56) through (3.58). Therefore,
Algorithm 2 with Eqs. (3.61) through (3.64) substituted for Eqs. (3.51) through
(3.54) gives a local solution to the VB problem (3.60) with the columnwise
independence constraint.
SimpleVB learning is much more practical when missing entries exist. In
the fully observed case, the posterior covariances ΣA and ΣB are common
to all rows of A and to all rows of B, respectively, while in the partially

80
3 VB Algorithm for Multilinear Models
observed case, we need to store the posterior covariances ΣA,m and ΣB,l for
all m = 1,. . . , M and l = 1,. . . , L. Since L and M can be huge, e.g., in
collaborative ﬁltering applications, the required memory size is signiﬁcantly
reduced by restricting the covariances to be diagonal.
3.3 Tensor Factorization
A matrix is a two-dimensional array of numbers. We can extend this notion to
an N-dimensional array, which is called an N-mode tensor. Namely, a tensor
V ∈RM(1)×···×M(N) consists of N
n=1 M(n) entries lying in an N-dimensional
array, where M(n) denotes the length in mode n. In this section, we derive VB
learning for tensor factorization.
3.3.1 Tucker Factorization
Similarly to the rank of a matrix, we can control the degree of freedom of
a tensor by controlling its tensor rank. Although there are a few different
deﬁnitions of the tensor rank and corresponding ways of factorization, we here
focus on Tucker factorization (TF) (Tucker, 1996; Kolda and Bader, 2009)
deﬁned as follows:
V = G ×1 A(1) · · · ×N A(N) + E,
where V ∈RM(1)×···×M(N), G ∈RH(1)×···×H(N), and {A(n) ∈RM(n)×H(n)} are
an observed tensor, a core tensor, and factor matrices, respectively. E ∈
RM(1)×···×M(N) is noise and ×n denotes the n-mode tensor product. Parafac
(Harshman, 1970), another popular way of tensor factorization, can be seen as
a special case of Tucker factorization where the core tensor is superdiagonal,
i.e., only the entries Gh(1),...,h(N) for h(1) = h(2) = · · · = h(N) are nonzero.
3.3.2 VB Learning for TF
The probabilistic model for MF is straightforwardly extended to TF (Chu and
Ghahramani, 2009; Mørup and Hansen, 2009). Assume Gaussian noise and
Gaussian priors:
p(V|G, {A(n)}) ∝exp
⎛⎜⎜⎜⎜⎜⎜⎝−
###V −G ×1 A(1) · · · ×N A(N)###2
2σ2
⎞⎟⎟⎟⎟⎟⎟⎠,
(3.65)

3.3 Tensor Factorization
81
p(G) ∝exp

−vec(G)⊤(CG(N) ⊗· · · ⊗CG(1))−1 vec(G)
2

,
(3.66)
p({A(n)}) ∝exp
⎛⎜⎜⎜⎜⎜⎝−
N
n=1 tr(A(n)C−1
A(n) A(n)⊤)
2
⎞⎟⎟⎟⎟⎟⎠,
(3.67)
where ⊗and vec(·) denote the Kronecker product and the vectorization
operator, respectively. {CG(n)} and {CA(n)} are the prior covariances restricted
to be diagonal, i.e.,
CG(n) = Diag

c2
g(n)
1 ,. . . , c2
g(n)
H(n)

,
CA(n) = Diag

c2
a(n)
1 ,. . . , c2
a(n)
H(n)

.
We denote ˘CG = CG(N) ⊗· · · ⊗CG(1).
Conditional Conjugacy
Since the TF model is multilinear, the likelihood (3.65) is in the Gaussian form
of the core tensor G and of each of the factor matrices {A(n)}, if the others
are ﬁxed as constants. Therefore, the Gaussian priors (3.66) and (3.67) are
conditionally conjugate for each parameter, and the posterior will be Gaussian.
Variational Bayesian Algorithm
Based on the conditional conjugacy, we impose the following constraint on the
VB posterior:
r(G, {A(n)}) = r(G)
N

n=1
r(A(n)).
Then, the free energy can be written as
F(r) =

r(G)
⎛⎜⎜⎜⎜⎜⎝
N

n=1
r(A(n))
⎞⎟⎟⎟⎟⎟⎠
⎛⎜⎜⎜⎜⎜⎝log p(V, G, {A(n)}) −log r(G) −
N

n=1
log r(A(n))
⎞⎟⎟⎟⎟⎟⎠
· dG
⎛⎜⎜⎜⎜⎜⎝
N

n=1
dA(n)
⎞⎟⎟⎟⎟⎟⎠.
(3.68)
Using the variational method, we obtain
0 =
 ⎛⎜⎜⎜⎜⎜⎝
N

n=1
r(A(n))
⎞⎟⎟⎟⎟⎟⎠
⎛⎜⎜⎜⎜⎜⎝log p(V, G, {A(n)}) −log r(G) −
N

n=1
log r(A(n)) −1
⎞⎟⎟⎟⎟⎟⎠
·
⎛⎜⎜⎜⎜⎜⎝
N

n=1
dA(n)
⎞⎟⎟⎟⎟⎟⎠,

82
3 VB Algorithm for Multilinear Models
and therefore
r(G) ∝exp⟨log p(V, G, {A(n)})⟩r({A(n)})
∝p(G) exp⟨log p(V|G, {A(n)})⟩r({A(n)}).
(3.69)
Similarly, we can also obtain
0 =

r(G)
⎛⎜⎜⎜⎜⎜⎝

n′n
r(A(n))
⎞⎟⎟⎟⎟⎟⎠
⎛⎜⎜⎜⎜⎜⎝log p(V, G, {A(n)}) −log r(G) −
N

n=1
log r(A(n)) −1
⎞⎟⎟⎟⎟⎟⎠
·
⎛⎜⎜⎜⎜⎜⎝

n′n
dA(n)
⎞⎟⎟⎟⎟⎟⎠,
and therefore
r(A(n)) ∝exp⟨log p(V, G, {A(n)})⟩r(G)r({A(n′)}n′n)
∝p(A(n)) exp⟨log p(V|G, {A(n)})⟩r(G)r({A(n′)}n′n).
(3.70)
Eqs. (3.69) and (3.70) imply that the VB posteriors are Gaussian. The
expectation in Eq. (3.69) can be calculated as follows:
⟨log p(V|G, {A(n)})⟩r({A(n)})
= −1
2σ2
*###V −G(×1 A(1)) · · · (×N A(N))
###2+
r({A(n)}) + const.
= −1
2σ2

−2vec(V)⊤(A(N) ⊗· · · ⊗A(1))vec(G)
+ vec(G)⊤(A(N)⊤A(N) ⊗· · · ⊗A(1)⊤A(1))vec(G)

r({A(n)}) + const.
Substituting the preceding calculation and the prior (3.66) into Eq. (3.69) gives
log r(G) = log p(G)⟨log p(V|G, {A(n)})⟩r({A(n)}) + const.
= −1
2(˘g −˘g)⊤˘Σ
−1
G (˘g −˘g) + const.,
where
˘g = vec(G),
˘v = vec(V),
˘g =
˘ΣG
σ2
!
A
(N) ⊗· · · ⊗A
(1)"⊤
˘v,
(3.71)
˘ΣG = σ2 !
A(N)⊤A(N) ⊗· · · ⊗A(1)⊤A(1)
r({A(n)}) + σ2 ˘C
−1
G
"−1
.
(3.72)

3.3 Tensor Factorization
83
Similarly, the expectation in Eq. (3.70) can be calculated as follows:
⟨log p(V|G, A(n))⟩r(G)r({A(n′)}n′n)
= −1
2σ2
*###V −G(×1A(1)) · · · (×N A(N))
###2+
r(G)r({A(n′)}n′n) + const.
= −1
2σ2
)
tr
!
−2V⊤
(n) A(n)G(n)(A
(N) ⊗· · · ⊗A
(n+1) ⊗A
(n−1) · · · ⊗A
(1))⊤"
+ tr

A(n) 
G(n)(A(N)⊤A(N) ⊗· · · ⊗A(n+1)⊤A(n+1)
⊗A(n−1)⊤A(n−1) · · · ⊗A(1)⊤A(1))G⊤
(n)

r(G)r({A(n′)}n′n) A(n)⊤" 1
.
Substituting the preceding calculation and the prior (3.67) into Eq. (3.70) gives
log r(A(n)) = log p(A(n)) exp⟨log p(V|G, {A(n)})⟩r(G)r({A(n′)}n′n) + const.
= −1
2tr
!
(A(n) −A
(n))Σ
−1
A(n)(A(n) −A
(n))⊤"
,
where
A
(n) = 1
σ2 V(n)(A
(N) ⊗· · · ⊗A
(n+1) ⊗A
(n−1) · · · ⊗A
(1))G
⊤
(n)Σ A(n),
(3.73)
Σ A(n) = σ2 
G(n)(A(N)⊤A(N) ⊗· · · ⊗A(n+1)⊤A(n+1)
⊗A(n−1)⊤A(n−1) ⊗· · · ⊗A(1)⊤A(1))G⊤
(n)

r(G)r({A(n′)}n′n) + σ2C−1
A(n)
"−1
.
(3.74)
Thus, the VB posterior is given by
r(G, {A(n)}) = GaussN
n=1 H(n)
!
˘g;˘g, ˘ΣG
"
·
N

n=1
MGaussM(n),H(n)
!
A(n); A
(n), IM(n) ⊗Σ A(n)
"
,
(3.75)
where the means and the covariances satisfy
˘g =
˘ΣG
σ2
!
A
(N) ⊗· · · ⊗A
(1)"⊤
˘v,
(3.76)
˘ΣG = σ2
 !
A
(N)⊤A
(N) + M(N)Σ A(N)
"
⊗· · · ⊗
!
A
(1)⊤A
(1) + M(1)ΣA(1)
"
+ σ2 ˘C
−1
G
−1
,
(3.77)

84
3 VB Algorithm for Multilinear Models
A
(n) = 1
σ2 V(n)(A
(N) ⊗· · · ⊗A
(n+1) ⊗A
(n−1) · · · ⊗A
(1))G
⊤
(n)Σ A(n),
(3.78)
Σ A(n) = σ2
*
G(n)
!
(A
(N)⊤A
(N)+M(N)Σ A(N)) ⊗· · · ⊗(A
(n+1)⊤A
(n+1)+M(n+1)Σ A(n+1))
⊗(A
(n−1)⊤A
(n−1)+M(n−1)Σ A(n−1)) ⊗· · · ⊗(A
(1)⊤A
(1)+M(1)Σ A(1))
"
G⊤
(n)
+
r(G)
+ σ2C−1
A(n)
−1
.
(3.79)
The expectation in Eqs. (3.79) is explicitly given by
*
G(n)
!
(A
(N)⊤A
(N)+M(N)Σ A(N)) ⊗· · · ⊗(A
(n+1)⊤A
(n+1)+M(n+1)Σ A(n+1))
⊗(A
(n−1)⊤A
(n−1)+M(n−1)Σ A(n−1)) ⊗· · · ⊗(A
(1)⊤A
(1)+M(1)Σ A(1))
"
G⊤
(n)
+
r(G)
= G(n)
!
(A
(N)⊤A
(N)+M(N)Σ A(N)) ⊗· · · ⊗(A
(n+1)⊤A
(n+1)+M(n+1)Σ A(n+1))
⊗(A
(n−1)⊤A
(n−1)+M(n−1)Σ A(n−1)) ⊗· · · ⊗(A
(1)⊤A
(1)+M(1)Σ A(1))
"
G
⊤
(n) + Ξ(n),
(3.80)
where the entries of Ξ(n) ∈RH(n)×H(n) are speciﬁed as
Ξ(n)
h(n),h′(n) =

(h(1),h′(1)),...,(h(n−1),h′(n−1)),(h(n+1),h′(n+1)),...,(h(N),h′(N))
(A
(N)⊤A
(N)+M(N)Σ A(N))h(N),h′(N)
· · · (A
(n+1)⊤A
(n+1)+M(n+1)Σ A(n+1))h(n+1),h′(n+1)(A
(n−1)⊤A
(n−1)+M(n−1)Σ A(n−1))h(n−1),h′(n−1)
· · · (A
(1)⊤A
(1) + M(1)Σ A(1))h(1),h′(1)(ΣG)(h(1),h′(1)),...,(h(N),h′(N)).
Here, we used the tensor expression of ΣG ∈R
N
n=1 2H(n) for the core posterior
covariance ˘ΣG.
Free Energy as a Function of Variational Parameters
Substituting Eq. (3.75) into Eq. (3.68), we have
2F = 2
/
log r(G) +
N

n=1
log r(A(n))
−log p(V|G, {A(n)})p(G)
N

n=1
p(A(n))
0
r(G)(N
n=1 r(A(n)))

3.3 Tensor Factorization
85
=
⎛⎜⎜⎜⎜⎜⎝
N

n=1
M(n)
⎞⎟⎟⎟⎟⎟⎠log(2πσ2) + log
det
 ˘CG
 
det
!˘ΣG
" +
N

n=1
M(n) log det (CA(n))
det
Σ A(n)
 
+ ∥V∥2
σ2
−
N

n=1
H(n) −
N

n=1
(M(n)H(n))
+ tr
!
˘C
−1
G (˘g˘g
⊤+ ˘ΣG)
"
+
N

n=1
tr
!
C−1
A(n)(A
(n)⊤A
(n) + M(n)Σ A(n))
"
−2
σ2 ˘v⊤(A
(N) ⊗· · · ⊗A
(1))˘g
+ 1
σ2 tr
2!
(A
(N)⊤A
(N) + M(N)Σ A(N)) ⊗· · · ⊗(A
(1)⊤A
(1) + M(1)Σ A(1))
"
·(˘g˘g
⊤+ ˘ΣG)
3
.
(3.81)
Empirical Variational Bayesian Algorithm
The derivative of the free energy (3.81) with respect to ˘CG gives
2 ∂F
∂˘CG
= M(n) !
˘C
−1
G −˘C
−2
G
!
˘g˘g⊤+ ˘ΣG
""
.
Since it holds that
∂˘CG
∂(CG(n))h,h
= CG(N) ⊗· · · ⊗CG(n+1) ⊗E(H(n),h,h) ⊗CG(n−1) ⊗· · · ⊗CG(1),
where E(H,h,h′) ∈RH×H is the matrix with the (h, h′)th entry equal to one and
the others equal to zero, we have
2
∂F
∂(CG(n))h,h
= 2tr
 ∂F
∂˘CG
∂˘CG
∂(CG(n))h,h

= M(n)
######vec

IH(N) ⊗· · · ⊗IH(n+1) ⊗(CG(n))−1
h,hE(H(n),h,h) ⊗IH(n−1) ⊗· · · ⊗IH(1)
−C−1
G(N) ⊗· · · ⊗C−1
G(n+1) ⊗(CG(n))−2
h,hE(H(n),h,h) ⊗C−1
G(n−1) ⊗· · · ⊗C−1
G(1)
!
˘g˘g⊤+˘ΣG
"" ######1
= M(n)(CG(n))−2
h,h
######vec

IH(N) ⊗· · · ⊗IH(n+1) ⊗(CG(n))h,hE(H(n),h,h)⊗IH(n−1) ⊗· · · ⊗IH(1)
−C−1
G(N) ⊗· · · ⊗C−1
G(n+1) ⊗E(H(n),h,h) ⊗C−1
G(n−1) ⊗· · · ⊗C−1
G(1)
!
˘g˘g⊤+ ˘ΣG
"" ######1
= M(n)(CG(n))−2
h,h
 
n′n H(n′) 
(CG(n))h,h −diag
!
˘g˘g⊤+ ˘ΣG
"⊤
diag

C−1
G(N) ⊗· · · ⊗C−1
G(n+1) ⊗E(H(n),h,h) ⊗C−1
G(n−1) ⊗· · · ⊗C−1
G(1)
 
,
(3.82)

86
3 VB Algorithm for Multilinear Models
Algorithm 3 EVB learning for Tucker factorization.
1: Initialize the variational parameters (˘g, ˘ΣG, {A
(n)}, {Σ A(n)}), and the hyper-
parameters ({CG(n)}, {CA(n)}, σ2), for example, ˘gh ∼Gauss1(0, τ), A
(n)
m,h ∼
Gauss1

0, τ1/N 
, ˘ΣG = τIN
n=1 H(n), CG(n) = τIH(n), Σ A(n) = CA(n) = τ1/NIH(n),
and σ2 = τ2 for τ2 = ∥V∥2/N
n=1 M(n).
2: Apply (substitute the right-hand side into the left-hand side) Eqs. (3.77),
(3.76), (3.79), and (3.78) to update ˘ΣG,˘g, {Σ A(n)}, and {A
(n)}, respectively.
3: Apply Eqs. (3.83) through (3.85) to update {CG(n)}, {CA(n)}, and σ2, respec-
tively.
4: Prune the hth component if c2
g(n)
h c2
a(n)
h < ε, where ε > 0 is a threshold, e.g.,
set to ε = 10−4.
5: Evaluate the free energy (3.81).
6: Iterate Steps 2 through 5 until convergence (until the energy decrease
becomes smaller than a threshold).
where ∥· ∥1 denotes the ℓ1-norm, and diag(·) denotes the column vector
consisting of the diagonal entries of a matrix. Thus, the prior covariance for
the core tensor can be updated by
c2
g(n)
h =
diag
!
˘g˘g⊤+˘ΣG
"⊤
diag

C−1
G(N)⊗···⊗C−1
G(n+1)⊗E(H(n),h,h)⊗C−1
G(n−1)⊗···⊗C−1
G(1)
 

n′n H(n′)
.
(3.83)
The derivative of the free energy (3.81) with respect to CA(n) gives
2 ∂F
∂c2
a(n)
h
= M(n)
⎛⎜⎜⎜⎜⎜⎝c−2
a(n)
h −c−4
a(n)
h
⎛⎜⎜⎜⎜⎜⎝
∥a(n)
h ∥2
M(n)
+ (Σ A(n))h,h
⎞⎟⎟⎟⎟⎟⎠
⎞⎟⎟⎟⎟⎟⎠,
which leads to the following update rule for the prior covariance for the factor
matrices:
c2
a(n)
h = ∥a(n)
h ∥2
M(n)
+ (Σ A(n))h,h.
(3.84)
Finally, the derivative of the free energy (3.81) with respect to σ2 gives
2 ∂F
∂σ2 =
N
n=1 M(n)
σ2
−1
σ4

∥V∥2 −2˘v⊤(A
(N) ⊗· · · ⊗A
(1))˘g
+ tr
2!
(A
(N)⊤A
(N)+M(N)Σ A(N)) ⊗· · · ⊗(A
(1)⊤A
(1)+M(1)Σ A(1))
"
(˘g˘g
⊤+˘ΣG)
3
,

3.4 Low-Rank Subspace Clustering
87
which leads to the update rule for the noise variance as follows:
σ2 =
1
N
n=1 M(n)

∥V∥2 −2˘v⊤(A
(N) ⊗· · · ⊗A
(1))˘g
+ tr
2!
(A
(N)⊤A
(N)+M(N)Σ A(N)) ⊗· · · ⊗(A
(1)⊤A
(1)+M(1)Σ A(1))
"
(˘g˘g
⊤+˘ΣG)
3 
.
(3.85)
Algorithm 3 summarizes the EVB algorithm for Tucker factorization. If we
appropriately set the hyperparameters ({CG(n)}, {CA(n)}, σ2) in Step 1 and skip
Steps 3 and 4, Algorithm 3 is reduced to (nonempirical) VB learning.
3.4 Low-Rank Subspace Clustering
PCA globally embeds data points into a low-dimensional subspace. As more
ﬂexible tools, subspace clustering methods, which locally embed the data into
the union of subspaces, have been developed. In this section, we derive VB
learning for subspace clustering.
3.4.1 Subspace Clustering Methods
Most clustering methods, such as k-means (MacQueen, 1967; Lloyd, 1982)
and spectral clustering (Shi and Malik, 2000), (explicitly or implicitly) assume
that there are sparse areas between dense areas, and separate the dense areas as
clusters (Figure 3.2 left). On the other hand, there are some data, e.g., projected
trajectories of points on a rigid body in 3D space, where data points can be
assumed to lie in a union of small dimensional subspaces (Figure 3.2 right).
Note that a point lying in a subspace is not necessarily far from a point lying in
another subspace if those subspaces intersect each other. Subspace clustering
methods have been developed to analyze this kind of data.
Figure 3.2 Clustering (left) and subspace clustering (right).

88
3 VB Algorithm for Multilinear Models
Let V = (v1,. . . , vM) ∈RL×M be L-dimensional observed samples of size
M. We assume that each vm is approximately expressed as a linear combination
of M′ words in a dictionary, D = (d1,. . . , dM′) ∈RL×M′, i.e.,
V = DU + E,
where U ∈RM′×M is unknown coefﬁcients, and E ∈RL×M is noise. In subspace
clustering, the observed matrix V itself is often used as a dictionary D. Then,
a convex formulation of the sparse subspace clustering (SSC) (Soltanolkotabi
and Cand`es, 2011; Elhamifar and Vidal, 2013) is given by
min
U ∥V −VU∥2
Fro + λ ∥U∥1 , s.t. diag(U) = 0,
(3.86)
where U ∈RM×M is a parameter to be estimated, λ > 0 is a regularization
coefﬁcient. ∥·∥1 is the ℓ1-norm of a matrix. The ﬁrst term in Eq. (3.86)
together with the constraint requires that each data point vm is accurately
expressed as a linear combination of other data points, {vm′} for m′  m. The
second term, which is the ℓ1-regularizer, enforces that the number of samples
contributing to the linear combination should be small, which leads to low-
dimensionality of each obtained subspace. After the solution U to the problem
(3.86) is obtained, the matrix abs(U)+abs(U
⊤), where abs(·) takes the absolute
value elementwise, is regarded as an afﬁnity matrix, and a spectral clustering
algorithm, such as the normalized cuts (Shi and Malik, 2000), is applied to
obtain clusters.
Another popular method for subspace clustering is low-rank subspace
clustering (LRSC) or low-rank representation (Liu et al., 2010; Liu and Yan,
2011; Liu et al., 2012; Vidal and Favaro, 2014), where low-dimensional
subspaces are sought by enforcing the low-rankness of U with the trace norm:
min
U ∥V −VU∥2
Fro + λ ∥U∥tr .
(3.87)
Since LRSC enforces the low-rankness of U, the constraint diag(U) = 0 is not
necessary, which makes its optimization problem (3.87) signiﬁcantly simpler
than the optimization problem (3.86) for SSC. Thanks to this simplicity, the
global solution of Eq. (3.87) has been analytically obtained (Vidal and Favaro,
2014).
Good properties of SSC and LRSC have been theoretically shown (Liu
et al., 2010, 2012; Soltanolkotabi and Cand`es, 2011; Elhamifar and Vidal,
2013; Vidal and Favaro, 2014). It is observed that they behave differently
in different situations, and each of SSC and LRSC shows advantages and
disadvantages over the other, i.e., neither SSC nor LRSC is known to dominate
the other in the general situations. In the rest of this section, we focus on LRSC
and derive its VB learning algorithm.

3.4 Low-Rank Subspace Clustering
89
3.4.2 VB Learning for LRSC
We start with the following probabilistic model, of which the maximum
a posteriori (MAP) estimator coincides with the solution to the convex
formulation (3.87) under a certain hyperparameter setting:
p(V|A, B) ∝exp

−1
2σ2
###V −VBA⊤###2
Fro

,
(3.88)
p(A) ∝exp

−1
2tr(AC−1
A A⊤)

,
(3.89)
p(B) ∝exp

−1
2tr(BC−1
B B⊤)

.
(3.90)
Here, we factorized U as U = BA⊤, where A ∈RM×H and B ∈RM×H for
H ≤min(L, M) are the parameters to be estimated (Babacan et al., 2012a). This
factorization is known to induce low-rankness through the MIR mechanism,
which will be discussed in Chapter 7. We assume that the prior covariances are
diagonal:
CA = Diag(c2
a1,. . . , c2
aH),
CB = Diag(c2
b1,. . . , c2
bH).
Conditional Conjugacy
The model likelihood (3.88) of LRSC is similar to the model likelihood (3.1)
of MF, and it is in the Gaussian form with respect to A if B is regarded as a
constant, or vice versa. Therefore, the priors (3.89) and (3.90) are conditionally
conjugate for A and B, respectively.
Variational Bayesian Algorithm
The conditional conjugacy implies that the following independence constraint
on the approximate posterior leads to a tractable algorithm:
r(A, B) = r(A)r(B).
In the same way as MF, we can show that the VB posterior is Gaussian in the
following form:
r(A) ∝exp
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
−
tr
!
(A −A)Σ
−1
A (A −A)⊤
"
2
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
,
r(B) ∝exp
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎝−(˘b −˘b)⊤˘Σ
−1
B (˘b −˘b)
2
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎠,
(3.91)

90
3 VB Algorithm for Multilinear Models
where ˘b = vec(B) ∈RMH. The variational parameters satisfy the following
stationary conditions:
A = 1
σ2 V⊤VBΣ A,
(3.92)
Σ A = σ2 !
B⊤V⊤VB

r(B) + σ2C−1
A
"−1
,
(3.93)
˘b =
˘ΣB
σ2 vec

V⊤VA
 
,
(3.94)
˘ΣB = σ2 !
(A
⊤A + MΣ A) ⊗V⊤V + σ2(C−1
B ⊗IM)
"−1
,
(3.95)
where the entries of B⊤V⊤VB
r(B) in Eq. (3.93) are explicitly given by
!
B⊤V⊤VB

r(B)
"
h,h′ =
!
B
⊤V⊤VB
"
h,h′ + tr
!
V⊤VΣ
(h,h′)
B
"
.
(3.96)
Here Σ
(h,h′)
B
∈RM×M is the (h, h′)th block matrix of ˘ΣB ∈RMH×MH, i.e.,
˘ΣB =
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
Σ
(1,1)
B
· · ·
Σ
(1,H)
B
...
...
...
Σ
(H,1)
B
· · ·
Σ
(H,H)
B
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
.
Free Energy as a Function of Variational Parameters
The free energy can be explicitly written as
2F = LM log(2πσ2) +
####V−VBA
⊤####
2
Fro
σ2
+ M log det(CA)
det
Σ A
 + log det(CB⊗IM)
det
!˘ΣB
"
−2MH + tr
2
C−1
A
!
A
⊤A + MΣ A
"3
+ tr
2
C−1
B B
⊤B
3
+ tr
2
(C−1
B ⊗IM)˘ΣB
3
+ tr
)
σ−2V⊤V

−BA
⊤AB
⊤+
*
B(A
⊤A + MΣ A)B⊤+
r(B)
1
,
(3.97)
where the expectation in the last term is given by
*
B(A
⊤A + MΣ A)B
+⊤
r(B)

m,m′ =
!
B(A
⊤A + MΣ A)B
⊤"
m,m′
+ tr

(A
⊤A + MΣ A)´Σ
(m,m′)
B

.
(3.98)

3.4 Low-Rank Subspace Clustering
91
Algorithm 4 EVB learning for low-rank subspace clustering.
1: Initialize the variational parameters (A, Σ A, B, ˘ΣB), and the hyperpara-
meters (CA, CB, σ2), for example, Am,h, Bm,h ∼Gauss1(0, 12), Σ A = CA =
CB = IH, ˘ΣB = IMH, and σ2 = ∥V∥2/(LM).
2: Apply (substitute the right-hand side into the left-hand side) Eqs. (3.93),
(3.92), (3.95), and (3.94) to update Σ A, A, ˘ΣB, and B, respectively.
3: Apply Eqs. (3.99) through (3.101) to update CA, CB, and σ2, respectively.
4: Prune the hth component if c2
ahc2
bh < ε, where ε > 0 is a threshold, e.g., set
to ε = 10−4.
5: Evaluate the free energy (3.97).
6: Iterate Steps 2 through 5 until convergence (until the energy decrease
becomes smaller than a threshold).
Here, ´Σ
(m,m′)
B
∈RH×H is deﬁned as
´Σ
(m,m′)
B
=
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
!
Σ
(1,1)
B
"
m,m′
· · ·
!
Σ
(1,H)
B
"
m,m′
...
...
...
!
Σ
(H,1)
B
"
m,m′
· · ·
!
Σ
(H,H)
B
"
m,m′
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
.
Empirical Variational Bayesian Algorithm
By differentiating the free energy (3.97) with respect to the hyperparameters,
we can obtain the stationary conditions for the hyperparameters:
c2
ah =
###ah
###2 /M +
Σ A
 
h,h ,
(3.99)
c2
bh =
!####bh
####
2
+ tr
!
Σ
(h,h)
B
""
/M,
(3.100)
σ2 =
tr

V⊤V

IM −2BA
⊤+
*
B(A
⊤A + MΣ A)B⊤+
r(B)

LM
.
(3.101)
Algorithm 4 summarizes the EVB algorithm. If we appropriately set the
hyperparameters (CA, CB, σ2) in Step 1 and skip Steps 3 and 4, Algorithm 4 is
reduced to (nonempirical) VB learning.
Variational Bayesian Algorithm under the Kronecker
Product Covariance Constraint
The standard VB algorithm, given in Algorithm 4, for LRSC requires the
inversion of an MH × MH matrix, which is prohibitively huge in practical

92
3 VB Algorithm for Multilinear Models
applications. As a remedy, Babacan et al. (2012a) proposed to restrict the
posterior r(B) for B to be the matrix variate Gaussian with the Kronecker
product covariance (KPC) structure, as Eq. (3.18). Namely, we restrict the
approximate posterior to be in the following form:
r(B) = MGaussM,H(B; B, Ψ B ⊗ΣB)
∝exp

−1
2tr
!
Ψ
−1
B (B −B)Σ
−1
B (B −B)⊤"
.
(3.102)
Under this additional constraint, the free energy is written as
2FKPC = LM log(2πσ2) +
####V −VBA
⊤####
2
σ2
+ M log det (CA)
det
Σ A
 + M log det (CB)
det
ΣB
 
+ H log
1
det
Ψ B
 −2MH
+ tr
2
C−1
A
!
A
⊤A + MΣ A
"3
+ tr
2
C−1
B
!
B
⊤B + tr(Ψ B)ΣB
"3
+ tr
2
σ−2V⊤V
!
MBΣ AB
⊤+ tr
!
(A
⊤A + MΣ A)ΣB
"
Ψ B
"3
. (3.103)
By differentiating the free energy (3.103) with respect to each variational
parameter, we obtain the following update rules:
A = 1
σ2 V⊤VBΣ A,
(3.104)
Σ A = σ2 !
B
⊤V⊤VB + tr

V⊤VΨ B
 ΣB + σ2C−1
A
"−1
,
(3.105)
B
new = B
old −α

B
oldC−1
B + 1
σ2 V⊤V
!
−A + B
old(A
⊤A + MΣ A)
"
,
(3.106)
ΣB = σ2
⎛⎜⎜⎜⎜⎜⎜⎝
tr

V⊤VΨ B
 
M
(A
⊤A + MΣ A) + σ2tr(Ψ B)
M
C−1
B
⎞⎟⎟⎟⎟⎟⎟⎠
−1
,
(3.107)
Ψ B = σ2
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
tr
!
(A
⊤A + MΣ A)ΣB
"
H
V⊤V + σ2tr(C−1
B ΣB)
H
IM
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
−1
,
(3.108)
c2
ah = (A
⊤A + MΣ A)h,h/M,
(3.109)
c2
bh =
!
B
⊤B + tr(Ψ B)ΣB
"
h,h /M,
(3.110)
σ2 =
1
LM tr
!
V⊤V
!
IM −2BA
⊤+ tr
!
(A
⊤A + MΣ A)ΣB
"
Ψ B
+ B(A
⊤A + MΣ A)B
⊤""
.
(3.111)

3.5 Sparse Additive Matrix Factorization
93
Algorithm 5 EVB learning for low-rank subspace clustering under the Kro-
necker product covariance constraint (3.102).
1: Initialize the variational parameters (A, Σ A, B, ΣB, Ψ B), and the hyper-
parameters (CA, CB, σ2), for example, Am,h, Bm,h ∼Gauss1(0, 12), Σ A =
ΣB = CA = CB = IH, Ψ B = IM, and σ2 = ∥V∥2/(LM).
2: Apply (substitute the right-hand side into the left-hand side) Eqs. (3.105),
(3.104), (3.107), and (3.108) to update Σ A, A, ΣB, and Ψ B, respectively.
3: Apply Eq. (3.106) T times (e.g., T = 20) to update B.
4: Apply Eqs. (3.109) through (3.111) to update CA, CB, and σ2, respectively.
5: Prune the hth component if c2
ahc2
bh < ε, where ε > 0 is a threshold, e.g., set
to ε = 10−4.
6: Evaluate the free energy (3.103).
7: Iterate Steps 2 through 6 until convergence (until the energy decrease
becomes smaller than a threshold).
Note that Eq. (3.106) is the gradient descent algorithm for B with the step size
α > 0.
Algorithm 5 summarizes the EVB algorithm under the KPC constraint,
which we call KPC approximation (KPCA). If we appropriately set the
hyperparameters (CA, CB, σ2) in Step 1 and skip Steps 4 and 5, Algorithm 5
is reduced to (nonempirical) VB learning.
3.5 Sparse Additive Matrix Factorization
PCA is known to be sensitive to outliers in data and generally fails in their
presence. To cope with outliers, robust PCA, where spiky noise is captured by
an elementwise sparse term, was proposed (Cand`es et al., 2011). In this section,
we introduce a generalization of robust PCA, called sparse additive matrix
factorization (SAMF) (Nakajima et al., 2013b) and derive its VB learning
algorithm.
3.5.1 Robust PCA and Matrix Factorization
In robust PCA, the observed matrix V ∈RL×M is modeled as follows:
V = Ulow-rank + Uelement + E,
(3.112)
where Ulow-rank
∈
RL×M is a low-rank matrix, Uelement
∈
RL×M is an
elementwise sparse matrix, and E ∈RL×M is a (typically dense) noise matrix.

94
3 VB Algorithm for Multilinear Models
Given the observed matrix V, one can infer each term in the right-hand side of
Eq. (3.112) by solving the following convex problem (Cand`es et al., 2011):
min
Ulow-rank,Uelement
###V −Ulow-rank −Uelement###2
Fro + λ1
###Ulow-rank###tr + λ2
###Uelement###1 ,
where the trace norm ∥·∥tr induces low-rank sparsity, and the ℓ1-norm ∥·∥1
induces elementwise sparsity. The regularization coefﬁcients λ1 and λ2 control
the strength of sparsity.
In Bayesian modeling, the low-rank matrix is commonly expressed as the
product of two matrices, A ∈RM×H and B ∈RL×H:
Ulow-rank = BA⊤=
H

h=1
bha⊤
h .
(3.113)
Trivially, low-rankness is forced if H is set to a small value. However, when
VB learning is applied, the estimator can be low-rank even if we adopt the full-
rank model, i.e., H = min(L, M). This phenomenon is caused by MIR, which
will be discussed in Chapter 7.
3.5.2 Sparse Matrix Factorization Terms
SAMF (Nakajima et al., 2013b) was proposed as a generalization of robust
PCA, where various types of sparsity are induced by combining different types
of factorization. For example, the following factorization implicitly induces
rowwise, columnwise, and elementwise sparsity, respectively:
Urow = ΓE D = (γe
1d1,. . . , γe
LdL)⊤,
(3.114)
Ucolumn = EΓD = (γd
1e1,. . . , γd
MeM),
(3.115)
Uelement = E ⊙D,
(3.116)
where ΓD = Diag(γd
1,. . . , γd
M) ∈RM×M and ΓE = Diag(γe
1,. . . , γe
L) ∈RL×L
are diagonal matrices, and D, E ∈RL×M. ⊙denotes the Hadamard product,
i.e., (E ⊙D)l,m = El,mDl,m. The reason why the factorizations (3.114) through
(3.116) induce the corresponding types of sparsity is explained in Section 7.5.
As a general expression of sparsity inducing factorizations, we deﬁne
a sparse matrix factorization (SMF) term with a mapping G consisting of
partitioning, rearrangement, and factorization:
U = G({U′(k)}K
k=1; X), where U′(k) = B(k) A(k)⊤.
(3.117)
Here, {A(k), B(k)}K
k=1
are parameters to be estimated, and G(·; X)
:
R
K
k=1(L′(k)×M′(k)) →RL×M is a designed function associated with an index
mapping X (explained shortly).

3.5 Sparse Additive Matrix Factorization
95
U
A
A
A
A
A
B
B
B
B
B
U
U
U
U
U
Figure 3.3 An example of SMF-term construction. G(·; X) with X : (k, l′, m′) →
(l, m) maps the set {U′(k)}K
k=1 of the PR matrices to the target matrix U, so that
U′(k)
l′,m′ = UX(k,l′,m′) = Ul,m.
Figure 3.3 illustrates how to construct an SMF term. First, we partition the
entries of U into K parts. Then, by rearranging the entries in each part, we form
partitioned-and-rearranged (PR) matrices U′(k) ∈RL′(k)×M′(k) for k = 1,. . . , K.
Finally, each of U′(k) is decomposed into the product of A(k) ∈RM′(k)×H′(k) and
B(k) ∈RL′(k)×H′(k), where H′(k) ≤min(L′(k), M′(k)).
In Eq. (3.117), the function G(·; X) is responsible for partitioning and
rearrangement: it maps the set {U′(k)}K
k=1 of the PR matrices to the target matrix
U ∈RL×M, based on the one-to-one map X : (k, l′, m′) →(l, m) from the
indices of the entries in {U′(k)}K
k=1 to the indices of the entries in U such that

G({U′(k)}K
k=1; X)
 
l,m = Ul,m = UX(k,l′,m′) = U′(k)
l′,m′.
(3.118)
When VB learning is applied, the SMF-term expression (3.117) induces
partitionwise sparsity and low-rank sparsity in each partition. Accordingly,
partitioning, rearrangement, and factorization should be designed in the fol-
lowing way. Suppose that we are given a required sparsity structure on a
matrix (examples of possible side information that suggests particular sparsity
structures are given in Section 3.5.3). We ﬁrst partition the matrix, according to
the required sparsity. Some partitions can be submatrices. We rearrange each
of the submatrices on which we do not want to impose low-rank sparsity into a
long vector (U′(3) in the example in Figure 3.3). We leave the other submatrices
which we want to be low-rank (U′(2)) and the original vectors (U′(1) and U′(4))
and scalars (U′(5)) as they are. Finally, we factorize each of the PR matrices to
induce sparsity.
Let us, for example, assume that rowwise sparsity is required. We ﬁrst make
the rowwise partition, i.e., separate U ∈RL×M into L pieces of M-dimensional
row vectors U′(l) = u⊤
l ∈R1×M. Then, we factorize each partition as U′(l) =
B(l) A(l)⊤(see the top illustration in Figure 3.4). Thus, we obtain the rowwise
sparse term (3.114). Here, X(k, 1, m′) = (k, m′) makes the following connection
between Eqs. (3.114) and (3.117): γe
l = B(k) ∈R,dl = A(k) ∈RM×1 for
k = l. Similarly, requiring columnwise and elementwise sparsity leads to

96
3 VB Algorithm for Multilinear Models
Table 3.1 Examples of SMF terms.
Factorization
Induced sparsity
K
(L′(k), M′(k))
X : (k, l′, m′) →(l, m)
U = BA⊤
low-rank
1
(L, M)
X(1, l′, m′) = (l′, m′)
U = ΓE D
rowwise
L
(1, M)
X(k, 1, m′) = (k, m′)
U = EΓD
columnwise
M
(L, 1)
X(k, l′, 1) = (l′, k)
U = E ⊙D
elementwise
L × M
(1, 1)
X(k, 1, 1) = vec-order(k)
G
=
U1,1
U1,2
U1,3
U2,1
U2,2
U2,3
(1) = U1,1
U1,2
U1,3 = B (1)A(1)
(2) = U2,1
U2,2
U2,3 = B (2)A(2)
G
=
U1,1
U1,2
U1,3
U2,1
U2,2
U2,3
(1) =
U1,1
U2,1
= B (1)A(1)
(2) =
U1,2
U2,2
= B (2)A(2)
(3) =
U1,3
U2,3
= B (3)A(3)
G
=
U1,1
U1,2
U1,3
U2,1
U2,2
U2,3
(1) = U1,1 = B (1)A(1)
(6) = U2,3 = B (6)A(6)
(2) = U2,1 = B (2)A(2)
(3) = U1,2 = B (3)A(3)
(4) = U2,2 = B (4)A(4)
(5) = U1,3 = B (5)A(5)
U
U
U
U
U
U
U
U
U
U
U
U
U
U
Figure 3.4 SMF-term construction for the rowwise (top), the columnwise
(middle), and the elementwise (bottom) sparse terms.
Eqs. (3.115) and (3.116), respectively (see the bottom two illustrations in
Figure 3.4). Table 3.1 summarizes how to design these SMF terms, where
vec-order(k) = (1 + ((k −1) mod L), ⌈k/L⌉) goes along the columns one after
another in the same way as the vec operator forming a vector by stacking the
columns of a matrix (in other words, (U′(1),. . . , U′(K))⊤= vec(U)).
Now we deﬁne the SAMF model as the sum of SMF terms (3.117):
V =
S
s=1
U(s) + E,
(3.119)
where
U(s) = G({B(k,s) A(k,s)⊤}K(s)
k=1; X(s)).
(3.120)
3.5.3 Examples of SMF Terms
In practice, SMF terms should be designed based on side information. Suppose
that V ∈RL×M consists of M samples of L-dimensional sensor outputs.
In robust PCA (3.112), we add an elementwise sparse term (3.116) to the
low-rank term (3.113), assuming that the low-rank signal is expected to be

3.5 Sparse Additive Matrix Factorization
97
B
F
Figure 3.5 Foreground/background video separation task.
contaminated with spiky noise when observed. Here, we can say that the
existence of spiky noise is used as side information.
Similarly, if we expect that a small number of sensors can be broken, and
their outputs are unreliable over all M samples, we should add the rowwise
sparse term (3.114) to separate the low-rank signal from rowwise noise:
V = Ulow-rank + Urow + E.
If we expect some accidental disturbances occurred during the observation,
but do not know their exact locations (i.e., which samples are affected), the
columnwise sparse term (3.115) can effectively capture such disturbances.
The SMF expression (3.117) enables us to use side information in a more
ﬂexible way, and its advantage has been shown in a foreground/background
video separation problem (Nakajima et al., 2013b). The top image in
Figure 3.5 is a frame of a video available from the Caviar Project website,1
and the task is to separate moving objects (bottom-right) from the background
(bottom-left). Previous approaches (Cand`es et al., 2011; Ding et al., 2011;
Babacan et al., 2012b) ﬁrst constructed the observation matrix V by stacking
all pixels in each frame into each column (Figure 3.6), and then ﬁtted it by
the robust PCA model (3.112). Here, the low-rank term and the elementwise
1 The European Commission (EC)-funded CAVIAR project/IST 2001 37540, found at URL:
http://homepages.inf.ed.ac.uk/rbf/CAVIAR/.

98
3 VB Algorithm for Multilinear Models
V
T
P
Figure 3.6 The observation matrix V is constructed by stacking all pixels in each
frame into each column.
sparse term are expected to capture the static background and the moving
foreground, respectively. However, we can also rely on the natural assumption
that the pixels in a segment sharing similar intensities tend to belong to the
same object. Under this assumption as side information, we can adopt a
segmentwise sparse term, for which the PR matrix is constructed based on
a precomputed oversegmented image (Figure 3.7). The segmentwise sparse
term has been shown to capture the foreground more accurately than the
elementwise sparse term in this application. Details will be discussed in
Chapter 11.
3.5.4 VB Learning for SAMF
Let us summarize the parameters of the SAMF model (3.119) as follows:
Θ = {Θ(s)
A , Θ(s)
B }S
s=1,
where
Θ(s)
A = {A(k,s)}K(s)
k=1,
Θ(s)
B = {B(k,s)}K(s)
k=1.
As in the MF model, we assume independent Gaussian noise and priors. Then,
the likelihood and the priors are given by
p(V|Θ) ∝exp
⎛⎜⎜⎜⎜⎜⎜⎜⎝−1
2σ2
#######V −
S
s=1
U(s)
#######
2
Fro
⎞⎟⎟⎟⎟⎟⎟⎟⎠,
(3.121)

3.5 Sparse Additive Matrix Factorization
99
P
P
T
U
Figure 3.7 Construction of a segmentwise sparse term. The original frame is
presegmented, based on which the segmentwise sparse term is constructed as an
SMF term.
p({Θ(s)
A }S
s=1) ∝exp

−1
2
S
s=1
K(s)

k=1
tr

A(k,s)C(k,s)−1
A
A(k,s)⊤ 
,
(3.122)
p({Θ(s)
B }S
s=1) ∝exp

−1
2
S
s=1
K(s)

k=1
tr

B(k,s)C(k,s)−1
B
B(k,s)⊤ 
.
(3.123)
We assume that the prior covariances of A(k,s) and B(k,s) are diagonal:
C(k,s)
A
= Diag(c(k,s)2
a1
,. . . , c(k,s)2
aH
),
C(k,s)
B
= Diag(c(k,s)2
b1
,. . . , c(k,s)2
bH
).
Conditional Conjugacy
As seen in Eq. (3.121), the SAMF model is the MF model for the parameters
(Θ(s)
A , Θ(s)
B ) in the sth SMF term, if the other parameters {Θ(s′)
A , Θ(s′)
B }s′s are
regarded as constants. Therefore, the Gaussian priors (3.122) and (3.123) are
conditionally conjugate for each of Θ(s)
A and Θ(s)
B in each of the SMF terms.
Variational Bayesian Algorithm
Based on the conditional conjugacy, we solve the VB learning problem under
the following independence constraint (Babacan et al., 2012b):

100
3 VB Algorithm for Multilinear Models
r(Θ) =
S
s=1
r(s)
A (Θ(s)
A )r(s)
B (Θ(s)
B ).
(3.124)
Following the standard procedure described in Section 2.1.5, we can ﬁnd that
the VB posterior, which minimizes the free energy (2.15), is in the following
form:
r(Θ) =
S
s=1
K(s)

k=1

MGaussM′(k,s),H′(k,s)(A(k,s); A
(k,s), Σ
(k,s)
A )
· MGaussL′(k,s),H′(k,s)(B(k,s); B
(k,s), Σ
(k,s)
B
)

=
S
s=1
K(s)

k=1
 M′(k,s)

m′=1
GaussH′(k,s)(a(k,s)
m′ ;a
(k,s)
m′ , Σ
(k,s)
A )
·
L′(k,s)

l′=1
GaussH′(k,s)(b
(k,s)
l′
;b
(k,s)
l′
, Σ
(k,s)
B
)

(3.125)
with the variational parameters satisfying the stationary conditions given by
A
(k,s) = σ−2Z′(k,s)⊤B
(k,s)Σ
(k,s)
A ,
(3.126)
Σ
(k,s)
A
= σ2 !
B
(k,s)⊤B
(k,s) + L′(k,s)Σ
(k,s)
B
+ σ2C(k,s)−1
A
"−1
,
(3.127)
B
(k,s) = σ−2Z′(k,s)A
(k,s)Σ
(k,s)
B
,
(3.128)
Σ
(k,s)
B
= σ2 !
A
(k,s)⊤A
(k,s) + M′(k,s)Σ
(k,s)
A
+ σ2C(k,s)−1
B
"−1
.
(3.129)
Here, Z′(k,s) ∈RL′(k,s)×M′(k,s) is deﬁned as
Z′(k,s)
l′,m′ = Z(s)
X(s)(k,l′,m′),
where
Z(s) = V −

s′s
U
(s).
(3.130)
Free Energy as a Function of Variational Parameters
The free energy can be explicitly written as
2F = LM log(2πσ2) + ∥V∥2
Fro
σ2
+
S
s=1
K(s)

k=1

M′(k,s) log
det

C(k,s)
A
 
det
!
Σ
(k,s)
A
" + L′(k,s) log
det

C(k,s)
B
 
det
!
Σ
(k,s)
B
"

+
S
s=1
K(S )

k=1
tr
2
C(k,s)−1
A
(A
(k,s)⊤A
(k,s) + M′(k,s)Σ
(k,s)
A
)
+ C(k,s)−1
B
(B
(k,s)⊤B
(k,s) + L′(k,s)Σ
(k,s)
B
)
3

3.5 Sparse Additive Matrix Factorization
101
+ 1
σ2 tr
⎧⎪⎪⎨⎪⎪⎩−2V⊤

S
s=1
G({B
(k,s)A
(k,s)⊤}K(s)
k=1; X(s))

+ 2
S
s=1
S
s′=s+1
G⊤({B
(k,s)A
(k,s)⊤}K(s)
k=1; X(s))G({B
(k,s′)A
(k,s′)⊤}K(s′)
k=1 ; X(s′))
⎫⎪⎪⎬⎪⎪⎭
+ 1
σ2
S
s=1
K(S )

k=1
tr
2
(A
(k,s)⊤A
(k,s) + M′(k,s)Σ
(k,s)
A
)(B
(k,s)⊤B
(k,s) + L′(k,s)Σ
(k,s)
B
)
3
−
S
s=1
K(S )

k=1
(L′(k,s) + M′(k,s))H′(k,s).
(3.131)
Empirical Variational Bayesian Algorithm
The following stationary conditions for the hyperparameters can be obtained
from the derivatives of the free energy (3.131):
c(k,s)2
ah
=
####a(k,s)
h
####
2
/M′(k,s) + (Σ
(k,s)
A )hh,
(3.132)
c(k,s)2
bh
=
#####b
(k,s)
h
#####
2
/L′(k,s) + (Σ
(k,s)
B
)hh,
(3.133)
Algorithm 6 EVB learning for sparse additive matrix factorization.
1: Initialize the variational parameters {A
(k,s), Σ
(k,s)
A , B
(k,s), Σ
(k,s)
B
}K(s)
k=1,
S
s=1, and
the hyperparameters {C(k,s)
A
, C(k,s)
B
}K(s)
k=1,
S
s=1, σ2, for example, A(k,s)
m,h , B(k,s)
l,h
∼
Gauss1(0, τ), Σ
(k,s)
A
= Σ
(k,s)
B
= C(k,s)
A
= C(k,s)
B
= τIH′(k,s), and σ2 = τ2 for τ2 =
∥V∥2
Fro/(LM).
2: Apply (substitute the right-hand side into the left-hand side) Eqs. (3.127),
(3.126), (3.129), and (3.128) for each k and s to update Σ
(k,s)
A
, A
(k,s), Σ
(k,s)
B
,
and B
(k,s), respectively.
3: Apply Eqs. (3.132) and (3.133) for all h′ = 1,. . . , H′(k,s), k and s, and
Eq. (3.134) to update C(k,s)
A
, C(k,s)
B
, and σ2, respectively.
4: Prune the hth component if c(k,s)2
ah
c(k,s)2
bh
< ε, where ε > 0 is a threshold,
e.g., set to ε = 10−4.
5: Evaluate the free energy (3.131).
6: Iterate Steps 2 through 5 until convergence (until the energy decrease
becomes smaller than a threshold).

102
3 VB Algorithm for Multilinear Models
σ2 =
1
LM
)
∥V∥2
Fro −2
S
s=1
tr
⎛⎜⎜⎜⎜⎜⎝U
(s)⊤
⎛⎜⎜⎜⎜⎜⎝V −
S
s′=s+1
U
(s′)
⎞⎟⎟⎟⎟⎟⎠
⎞⎟⎟⎟⎟⎟⎠
+
S
s=1
K(s)

k=1
tr

(B
(k,s)⊤B
(k,s) + L′(k,s)Σ
(k,s)
B
) · (A
(k,s)⊤A
(k,s) + M′(k,s)Σ
(k,s)
A
)
 1
.
(3.134)
Algorithm 6 summarizes the EVB algorithm for SAMF. If we appropriately
set the hyperparameters {C(k,s)
A
, C(k,s)
B
}K(s)
k=1,
S
s=1, σ2 in Step 1 and skip Steps 3 and
4, Algorithm 6 is reduced to (nonempirical) VB learning.

4
VB Algorithm for Latent Variable Models
In this chapter, we discuss VB learning for latent variable models. Starting
with ﬁnite mixture models as the simplest example, we overview the VB
learning algorithms for more complex latent variable models such as Bayesian
networks and hidden Markov models.
Let H denote the set of (local) latent variables and w denote a model
parameter vector (or the set of global latent variables). In this chapter, we
consider the latent variable model for training data D:
p(D|w) =

H
p(D, H|w).
Let us employ the following factorized model to approximate the posterior
distribution for w and H:
r(w, H) = rw(w)rH(H).
(4.1)
Applying the general VB framework explained in Section 2.1.5 to the preced-
ing model leads to the following update rules for w and H:
rw(w) = 1
Cw
p(w) exp log p(D, H|w)
rH(H),
(4.2)
rH(H) =
1
CH
exp log p(D, H|w)
rw(w).
(4.3)
In the following sections, we discuss these update rules for some speciﬁc
examples of latent variable models.
4.1 Finite Mixture Models
A ﬁnite mixture model p(x|w) of an L-dimensional input x ∈RL with a
parameter vector w ∈RM is deﬁned by
103

104
4 VB Algorithm for Latent Variable Models
p(x|w) =
K

k=1
αkp(x|τk),
(4.4)
where integer K is the number of components and α = (α1,. . . , αK)⊤∈ΔK−1
is the set of mixing weights (Example 1.3). The parameter w of the model is
w = {αk, τk}K
k=1.
The ﬁnite mixture model can be rewritten as follows by using a hidden
variable z = (z1,. . . , zK)⊤∈{e1,. . . , eK},
p(x, z|w) =
K

k=1
4αkp(x|τk)5zk .
(4.5)
Here ek ∈{0, 1}K is the K-dimensional binary vector, called the one-of-K
representation, with one at the kth entry and zeros at the other entries:
ek = (0,. . . , 0,
k-th
	

1
, 0,. . . , 0

	
K
)⊤.
The hidden variable z is not observed and is representing the component from
which the data sample x is generated. If the data sample x is from the kth
component, then zk = 1, otherwise, zk = 0. Then

z
p(x, z|w) = p(x|w)
holds where the sum over z goes through all possible values of the hidden
variable.
4.1.1 Mixture of Gaussians
If the component distribution in Eq. (4.4) is chosen to be a Gaussian distribu-
tion,
p(x|τ) = GaussL(x; μ, Σ),
the ﬁnite mixture model is called the mixture of Gaussians or the Gaussian
mixture model (GMM).
In some applications, the parameters are restricted to the means of each
component, and it is assumed that there is no correlation between each input
dimension. In this case, since L = M, the model is written by
p(x|w) =
K

k=1
αk
(2πσ2)M/2 exp

−∥x −μk∥2
2σ2

,
(4.6)
where σ > 0 is a constant.

4.1 Finite Mixture Models
105
In this subsection, the uncorrelated GMM (4.6) is considered in the VB
framework by further assuming that σ2 = 1 for simplicity. The joint model for
the observed and hidden variables (4.5) is given by the product of the following
two distributions:
p(z|α) = MultinomialK,1(z; α),
(4.7)
p(x|z, {μk}K
k=1) =
K

k=1
6GaussM(x; μk, IM)7zk .
(4.8)
Thus, for the set of hidden variables H = {z(n)}N
n=1 and the complete data set
{D, H} = {x(n), z(n)}N
n=1, the complete likelihood is given by
p(D, H|α, {μk}K
k=1) =
N

n=1
K

k=1
,
αkGaussM(x(n); μk, IM)
-z(n)
k .
(4.9)
ML learning of the GMM is carried out by the expectation-maximization
(EM) algorithm (Dempster et al., 1977), which corresponds to a clustering
algorithm called the soft K-means (MacKay, 2003, ch. 22).
Because of the conditional conjugacy (Section 2.1.2) of the parameters α =
(α1,. . . , αK)⊤∈ΔK−1 and {μk}K
k=1, we assume that the prior of the parameters
is the product of the following two distributions:
p(α|φ) = DirichletK(α; (φ,. . . , φ)⊤),
(4.10)
p({μk}K
k=1|μ0, ξ) =
K

k=1
GaussM(μk|μ0, (1/ξ)IM),
(4.11)
where ξ > 0, μ0 ∈RM and φ > 0 are the hyperparameters.
VB Posterior for the Gaussian Mixture Model
Let
Nk =
N

n=1

z(n)
k

rH(H)
(4.12)
and
xk = 1
Nk
N

n=1

z(n)
k

rH(H) x(n),
(4.13)
where z(n)
k
= 1 if the nth data sample x(n) is from the kth component; otherwise,
z(n)
k
= 0. The variable Nk is the expected number of times data come from
the kth component, and xk is the mean of them. Note that the variables Nk
and xk satisfy the constraints K
k=1 Nk = N and K
k=1 Nkxk = N
n=1 x(n). From

106
4 VB Algorithm for Latent Variable Models
(4.2) and the respective priors (4.10) and (4.11), the VB posterior rw(w) =
rα(α)rμ({μk}K
k=1) is obtained as the product of the following two distributions:
rα(α) = DirichletK

α; (φ1,. . . ,φK)⊤ 
,
(4.14)
rμ({μk}K
k=1) =
K

k=1
GaussM

μk;μk, σ2
kIM
 
,
(4.15)
where
φk = Nk + φ,
(4.16)
σ2
k =
1
Nk + ξ
,
(4.17)
μk = Nkxk + ξμ0
Nk + ξ
.
(4.18)
From Eq. (4.3), the VB posterior rH(H) is given by
rH(H) =
1
CH
N

n=1
exp
⎛⎜⎜⎜⎜⎜⎝z(n)
k
⎧⎪⎪⎨⎪⎪⎩Ψ(φk) −Ψ
⎛⎜⎜⎜⎜⎜⎝
K

k′=1
φk′
⎞⎟⎟⎟⎟⎟⎠
−∥x(n) −μk∥2
2
−M
2

log 2π +
1
Nk + ξ
1
,
where Ψ is the di-gamma (psi) function, and we used
log αk

rα(α) = Ψ(φk) −Ψ
⎛⎜⎜⎜⎜⎜⎝
K

k′=1
φk′
⎞⎟⎟⎟⎟⎟⎠.
(4.19)
That is, rH(H) = rz({z(n)}N
n=1) is the multinomial distribution:
rz({z(n)}N
n=1) =
N

n=1
rz(z(n))
=
N

n=1
MultinomialK,1

z(n);z(n) 
,
wherez(n) ∈ΔK−1 is
z(n)
k
=

z(n)
k

rH(H) =
z(n)
k
K
k′=1 z(n)
k′
,
(4.20)
for
z(n)
k
= exp
⎛⎜⎜⎜⎜⎜⎝Ψ(φk) −Ψ
⎛⎜⎜⎜⎜⎜⎝
K

k′=1
φk′
⎞⎟⎟⎟⎟⎟⎠−1
2
###x(n) −μk
###2 + Mσ2
k
⎞⎟⎟⎟⎟⎟⎠.
(4.21)

4.1 Finite Mixture Models
107
The free energy as a function of variational parameters is expressed as
follows:
F =
/
log rH(H)rw(w)
p(w)
0
rH(H)rw(w)
−log p(D, H|w)
rH(H)rw(w)
=
/
log
(z(n)
k )z(n)
k
Γ(K
k=1 φk)
K
k=1 Γ(φk)
K
k=1 α
φk−1
k
exp

−
∥μk−μk∥2
2σ2
k

(2πσ2
k)M/2
Γ(Kφ)
(Γ(φ))K
K
k=1 αφ−1
k
 ξ
2π
 M/2 exp

−ξ∥μk−μ0∥2
2
 
0
rH(H)rw(w)
−
/
log
N

n=1
K

k=1
⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩
αk
exp
!
−∥x(n)−μk∥2
2
"
(2π)M/2
⎫⎪⎪⎪⎪⎪⎬⎪⎪⎪⎪⎪⎭
z(n)
k 0
rH(H)rw(w)
= log
⎛⎜⎜⎜⎜⎜⎝
Γ(K
k=1 φk)
K
k=1 Γ(φk)
⎞⎟⎟⎟⎟⎟⎠−log
 Γ(Kφ)
(Γ(φ))K

−M
2
K

k=1
log

ξσ2
k
 
−KM
2
+
N

n=1
K

k=1
z(n)
k logz(n)
k +
K

k=1
φk −φ −Nk
 
Ψ(φk) −Ψ(K
k′=1 φk′)
 
+
K

k=1
ξ

∥μk −μ0∥2 + Mσ2
k
 
2
+
K

k=1
Nk

M log(2π) + Mσ2
k
 
2
+
K

k=1
Nk∥xk −μk∥2 + N
n=1z(n)
k ∥x(n) −xk∥2
2
.
(4.22)
The prior hyperparameters, (φ, μ0, ξ), can be estimated by the EVB learning
(Section 2.1.6). Computing the partial derivatives, we have
∂F
∂φ = K (Ψ(φ) −Ψ(Kφ)) −
K

k=1

Ψ(φk) −Ψ
K
k′=1 φk′
  
,
(4.23)
∂F
∂μ0
= ξ
K

k=1
$μ0 −μk
% ,
(4.24)
∂F
∂ξ = −M
2
⎛⎜⎜⎜⎜⎜⎝
K
ξ −
K

k=1
∥μk −μ0∥2
M
+ σ2
k
⎞⎟⎟⎟⎟⎟⎠.
(4.25)
The stationary conditions ∂F
∂μ0 = 0 and ∂F
∂ξ = 0 yield the following update rules:
μ0 = 1
K
K

k=1
μk,
(4.26)

108
4 VB Algorithm for Latent Variable Models
Algorithm 7 EVB learning for the Gaussian mixture model.
1: Initialize the variational parameters ({z(n)}N
n=1, {φk}K
k=1, {μk, σ2
k}K
k=1), and the
hyperparameters (φ, μ0, ξ).
2: Apply (substitute the right-hand side into the left-hand side) Eqs. (4.21),
(4.20), (4.12), (4.13), (4.16), (4.17), and (4.18) to update {z(n)}N
n=1, {φk}K
k=1,
and {μk, σ2
k}K
k=1.
3: Apply Eqs. (4.29), (4.26), and (4.27) to update φ, μ0 and ξ, respectively.
4: Evaluate the free energy (4.22).
5: Iterate Steps 2 through 4 until convergence (until the energy decrease
becomes smaller than a threshold).
ξ =
⎧⎪⎪⎨⎪⎪⎩
1
K
K

k=1
∥μk −μ0∥2
M
+ σ2
k
⎫⎪⎪⎬⎪⎪⎭
−1
.
(4.27)
Since the stationary condition
∂F
∂φ
= 0 is not explicitly solved for φ, the
Newton–Raphson step is usually used for updating φ. With the second deriva-
tive,
∂2F
∂φ2 = K

Ψ(1)(φ) −KΨ(1)(Kφ)
 
,
(4.28)
the update rule is obtained as follows:
φnew = max
⎛⎜⎜⎜⎜⎜⎝0, φold −
∂2F
∂φ2
−1 ∂F
∂φ
⎞⎟⎟⎟⎟⎟⎠
= max

0, φold −
K(Ψ(φ)−Ψ(Kφ))−K
k=1

Ψ(φk)−Ψ
K
k′=1 φk′
  
K(Ψ(1)(φ)−KΨ(1)(Kφ))

,
(4.29)
where Ψm(z) ≡dm
dzm Ψ(z) is the polygamma function of order m.
The EVB learning for the GMM is summarized in Algorithm 7. If the prior
hyperparameters are ﬁxed and Step 3 in the algorithm is omitted, the algorithm
reduces to the (nonempirical) VB learning algorithm.
4.1.2 Mixture of Exponential Families
It is well known that the Gaussian distribution is an example of the exponential
family distribution:
p(x|τ) = p(t|η) = exp

η⊤t −A(η) + B(t)
 
,
(4.30)
where η ∈H is the natural parameter, η⊤t is its inner product with the vector
t = t(x) = (t1(x),. . . , tM(x))⊤, and A(η) and B(t) are real-valued functions

4.1 Finite Mixture Models
109
of the parameter η and the sufﬁcient statistics t, respectively (Brown, 1986)
(see Eq. (1.27) in Section 1.2.3). Suppose functions t1,. . . , tM and the constant
function, 1, are linearly independent and the number of parameters in a single
component distribution, p(t|η), is M.
The VB framework for GMMs in Section 4.1.1 is generalized to a mixture
of exponential family distributions as follows. The conditional conjugate prior
distributions of α ∈ΔK−1 and {ηk}K
k=1 are given by
p(α|φ) = DirichletK(α; (φ,. . . , φ)⊤),
(4.31)
p({ηk}K
k=1|ν0, ξ) =
K

k=1
1
C(ξ, ν0) exp

ξ(ν⊤
0 ηk −A(ηk))
 
,
(4.32)
where the function C(ξ, ν) of ξ ∈R and ν ∈RM is deﬁned by
C(ξ, ν) =

H
exp

ξ(ν⊤η −A(η))
 
dη.
(4.33)
Constants ξ > 0, ν0 ∈RM, and φ > 0 are the hyperparameters.
VB Posterior for Mixture-of-Exponential-Family Models
Here, we derive the VB posterior rw(w) for the mixture-of-exponential-family
model using Eq. (4.2).
Using the complete data {x(n), z(n)}N
n=1, we put
Nk =
N

n=1

z(n)
k

rH(H) ,
(4.34)
tk = 1
Nk
N

n=1

z(n)
k

rH(H) t(n),
(4.35)
where t(n) = t(x(n)). Note that the variables Nk and tk satisfy the constraints
K
k=1 Nk = N and K
k=1 Nktk = N
n=1 t(n). From Eq. (4.2) and the respec-
tive prior distributions, Eqs. (4.10) and (4.32), the VB posterior rw(w) =
rα(α)rη({ηk}K
k=1) is obtained as the product of the following two distributions:
rα(α) = DirichletK

α; (φ1,. . . ,φK)⊤ 
,
rη({ηk}K
k=1) =
K

k=1
1
C(ξk,νk)
exp
ξk(ν⊤
k ηk −A(ηk))
 
,
(4.36)
where
φk = Nk + φ,
(4.37)

110
4 VB Algorithm for Latent Variable Models
νk = Nktk + ξν0
Nk + ξ
,
(4.38)
ξk = Nk + ξ.
(4.39)
Let
ηk = ηk

rη(ηk) = 1
ξk
∂logC(ξk,νk)
∂νk
.
(4.40)
It follows that
A(ηk)
rη(ηk) = η⊤
kνk −∂logC(ξk,νk)
∂ξk
.
(4.41)
From Eq. (4.3), the VB posterior rH(H) is given by
rH(H) =
N

n=1
rz(z(n))
=
N

n=1
MultinomialK,1

z(n);z(n) 
,
wherez(n) ∈ΔK−1 is
z(n)
k
=

z(n)
k

rH(H) =
z(n)
k
K
k′=1 z(n)
k′
,
(4.42)
for
z(n)
k
= exp
⎛⎜⎜⎜⎜⎜⎝Ψ(φk) −Ψ
⎛⎜⎜⎜⎜⎜⎝
K

k′=1
φk′
⎞⎟⎟⎟⎟⎟⎠+η⊤
k t(n) −A(ηk)
rη(ηk) + B(t(n))
⎞⎟⎟⎟⎟⎟⎠.
(4.43)
To obtain the preceding expression of z(n)
k , we used Eq. (4.19).
The free energy as a function of variational parameters is expressed as
F = log
⎛⎜⎜⎜⎜⎜⎝
Γ(K
k=1 φk)
K
k=1 Γ(φk)
⎞⎟⎟⎟⎟⎟⎠−log
 Γ(Kφ)
(Γ(φ))K

−
K

k=1
logC(ξk,νk) + K logC(ξ, ν0)
+
N

n=1
K

k=1
z(n)
k logz(n)
k +
K

k=1
φk −φ −Nk
 
Ψ(φk) −Ψ(K
k′=1 φk′)
 
+
K

k=1
⎡⎢⎢⎢⎢⎣η⊤
k
,
ξ $νk −ν0
% + Nk

νk −tk
 -
+
ξk −ξ −Nk
 ∂logC(ξk,νk)
∂ξk
⎤⎥⎥⎥⎥⎦
−
N

n=1
B(t(n)).
(4.44)

4.1 Finite Mixture Models
111
The update rule of φ for the EVB learning is obtained by Eq. (4.29) as in
the GMM. The partial derivatives of F with respect to the hyperparameters
(ν0, ξ) are
∂F
∂ν0
= K ∂logC(ξ, ν0)
∂ν0
−ξ
K

k=1
ηk,
(4.45)
∂F
∂ξ =
K

k=1
⎧⎪⎪⎨⎪⎪⎩η⊤
k
$νk −ν0
% −∂logC(ξk,νk)
∂ξk
⎫⎪⎪⎬⎪⎪⎭+ K ∂logC(ξ, ν0)
∂ξ
.
(4.46)
Equating these derivatives to zeros, we have the following stationary condi-
tions:
1
ξ
∂logC(ξ, ν0)
∂ν0
= 1
K
K

k=1
ηk,
(4.47)
∂logC(ξ, ν0)
∂ξ
=
⎛⎜⎜⎜⎜⎜⎝
1
K
K

k=1
ηk
⎞⎟⎟⎟⎟⎟⎠
⊤
ν0 −1
K
K

k=1
A(ηk)
rη(ηk) ,
(4.48)
where we have used Eq. (4.41). If these equations are solved for ν0 and ξ,
respectively, we obtain their update rules as in the case of the GMM.
Otherwise, we need the Newton–Raphson steps to update them.
The EVB learning for the mixture of exponential families is summarized in
Algorithm 8. If the prior hyperparameters are ﬁxed and Step 3 in the algorithm
is omitted, the algorithm reduces to the (nonempirical) VB learning algorithm.
Algorithm 8 EVB learning for the mixture-of-exponential-family model.
1: Initialize the variational parameters ({z(n)}N
n=1, {φk}K
k=1, {νk,ξk}K
k=1), and the
hyperparameters (φ, ν0, ξ).
2: Apply (substitute the right-hand side into the left-hand side) Eqs. (4.43),
(4.42), (4.34), (4.35), (4.37), (4.38), and (4.39) to update {z(n)}N
n=1, {φk}K
k=1,
and {νk,ξk}K
k=1. Transform {νk,ξk}K
k=1 to {ηk, A(ηk)
rη(ηk)}K
k=1 by Eqs. (4.40)
and (4.41).
3: Apply Eqs. (4.29), (4.47), and (4.48) to update φ, ν0 and ξ, respectively.
4: Evaluate the free energy (4.44).
5: Iterate Steps 2 through 4 until convergence (until the energy decrease
becomes smaller than a threshold).

112
4 VB Algorithm for Latent Variable Models
4.1.3 Inﬁnite Mixture Models
In 2000s, there was a revival of Bayesian nonparametric models to estimate the
model complexity, e.g., the number of components in mixture models, by using
a prior distribution for probability measures such as the Dirichlet process (DP)
prior. The Bayesian nonparametric approach ﬁts a single model adapting its
complexity to the data. The VB framework plays an important role in achieving
tractable inference for Bayesian nonparametric models. Here, we introduce the
VB learning for the stick-breaking construction of the DP prior by instantiating
the estimation of the number of components of the mixture model.
For the ﬁnite mixture model with K components, we had the discrete latent
variable,
z ∈{e1, e2,. . . , eK},
indicating the label of the component. We also assumed the multinomial
distribution,
p(z|α) = MultinomialK,1(z; α) =
K

k=1
αzk
k .
In the nonparametric Bayesian approach, we consider possibly an inﬁnite
number of components,
p(z|α) = lim
K→∞MultinomialK,1(z; α),
and the following generation process of αk, called the stick-breaking pro-
cess (Blei and Jordan, 2005; Gershman and Blei, 2012):
αk = vk
k−1

l=1
(1 −vl),
vk ∼Beta(1, γ),
where Beta(α, β) denotes the beta distribution with parameters α and β, and γ
is the scaling parameter.
To derive a tractable VB learning algorithm, the truncation level T is usually
introduced to the preceding process, which enforces vT = 1. If the truncation
level T is sufﬁciently large, some components are left unused, and hence T
does not directly specify the number of components.
Then, the VB posterior r(H, v) for the latent variables and v = {vk}T−1
k=1 is
assumed to be factorized, rH(H)rv(v), for which the free energy minimization
implies further factorization:
r(H, v) =
N

n=1
rz(z(n))
T−1

k=1
rv(vk),

4.1 Finite Mixture Models
113
where rz(z(n)) is the multinomial distribution as in the case of the ﬁnite mixture
model, and rv(vk) is the beta distribution because of the conditional conjugacy.
To see this and how the VB learning algorithm is derived, we instantiate the
GMM discussed in Section 4.1.1.
The free energy is decomposed as
F =
/
log rz({z(n)}N
n=1)rv(v)rμ({μk}K
k=1)
p(v)p({μk}T
k=1)
0
rz({z(n)}N
n=1)rv(v)rμ({μk}K
k=1)
−

log p(D|{z(n)}N
n=1, w)

rz({z(n)}N
n=1)rv(v)rμ({μk}K
k=1)
−

log p({z(n)}N
n=1|v)

rz({z(n)}N
n=1)rv(v) .
These terms are computed in the same way as in Section 4.1.1 except for the
last term,

log p({z(n)}N
n=1|v)

rz({z(n)}N
n=1)rv(v) = N
n=1

log p(z(n)|v)

rz(z(n))rv(v).
Let c(n) be the index k such that z(n)
k
= 1 and θ be the indicator function.
Then, we have

log p(z(n)|v)

rz(z(n))rv(v)
=
/
log
∞

k=1
(1 −vk)θ(c(n)>k)vθ(c(n)=k)
k
0
rz(z(n))rv(v)
=
∞

k=1
,
rz(c(n) > k) log(1 −vk)
rv(v) + rz(c(n) = k) log vk

rv(v)
-
=
T−1

k=1
,
rz(c(n) > k) log(1 −vk)
rv(v) + rz(c(n) = k) log vk

rv(v)
-
,
where we have used log vT = 0 and rz(c(n) > T) = 0.
Since the probabilities rz(c(n) = k) and rz(c(n) > k) are given by
rz(c(n) = k) =z(n)
k ,
rz(c(n) > k) =
T

l=k+1
z(n)
l ,
it follows from Eq. (4.12) that
N

n=1
rz(c(n) = k) = Nk,
N

n=1
rz(c(n) > k) =
T

l=k+1
Nl = N −
k

l=1
Nl.

114
4 VB Algorithm for Latent Variable Models
Now Eq. (4.3) in this case yields that
rv(v) ∝
N

n=1

log p(z(n)|v)

rz(z(n)) p(v).
It follows from similar manipulations to those just mentioned and the condi-
tional conjugacy that
rv(v) =
T−1

k=1
Beta(vk;κk,λk),
(4.49)
i.e., for a ﬁxed rH(H), the optimal rv(v) is the beta distribution with the
parameters,
κk = 1 + Nk,
(4.50)
λk = γ + N −
k

l=1
Nl.
(4.51)
The VB posterior rH(H) is computed similarly except that the expectation
log αk

rα(α) = Ψ(Nk + φ) −Ψ(N + Kφ)
in Eq. (4.19) for the ﬁnite mixture model is replaced by
log vk

rv(vk)+
k−1

l=1
log(1 −vl)
rv(vl) = Ψ(κk)−Ψ(κk+λk)+
k−1

l=1
{Ψ(λl)−Ψ(κl+λl)},
since
log vk

rv(vk) = Ψ(κk) −Ψ(κk +λk),
log(1 −vk)
rv(vk) = Ψ(λk) −Ψ(κk +λk),
for rv(vk) = Beta(vk;κk,λk). In the case of the GMM, z(n)
k
in Eq. (4.21) is
replaced with
z(n)
k
= exp
⎛⎜⎜⎜⎜⎜⎜⎝Ψ(κk) −Ψ(κk +λk) +
k−1

l=1
{Ψ(λl) −Ψ(κl +λl)}
−1
2
###x(n) −μk
###2 + Mσ2
k

.
(4.52)
The free energy is given by
F =
T−1

k=1
log
⎛⎜⎜⎜⎜⎝
Γ(κk +λk)
Γ(κk)Γ(λk)
⎞⎟⎟⎟⎟⎠−(T −1) log γ −M
2
T

k=1
log

ξσ2
k
 
−T M
2
+
N

n=1
T

k=1
z(n)
k logz(n)
k +
T−1

k=1

κk −1 −Nk
 ,
Ψ(κk) −Ψ(κk +λk)
-

4.2 Other Latent Variable Models
115
+
T−1

k=1
⎧⎪⎪⎨⎪⎪⎩λk −γ −
⎛⎜⎜⎜⎜⎜⎜⎝N −
k

l=1
Nl
⎞⎟⎟⎟⎟⎟⎟⎠
⎫⎪⎪⎬⎪⎪⎭
,
Ψ(λk) −Ψ(κk +λk)
-
+
T

k=1
ξ

∥μk −μ0∥2 + Mσ2
k
 
2
+
T

k=1
Nk

M log(2π) + Mσ2
k
 
2
+
T

k=1
Nk∥xk −μk∥2 + N
n=1z(n)
k ∥x(n) −xk∥2
2
.
(4.53)
The VB learning algorithm is similar to Algorithm 7 for the ﬁnite GMM
while the number of components K is replaced with the truncation level T
throughout, the update rule (4.21) is replaced with Eq. (4.52), and {κk,λk}T−1
k=1
are updated by Eqs. (4.50) and (4.51) instead of {φk}K
k=1.
By computing
∂F
∂γ and equating it to zero, the EVB learning for the
hyperparameter γ updates it as follows:
γ =
⎡⎢⎢⎢⎢⎢⎢⎣
−1
T −1
T−1

k=1
,
Ψ(λk) −Ψ(κk +λk)
-⎤⎥⎥⎥⎥⎥⎥⎦
−1
,
(4.54)
which can replace the update rule of φ in Step 3 of Algorithm 7.
4.2 Other Latent Variable Models
In this section, we discuss more complex latent variable models than mixture
models and derive VB learning algorithms for them. Although we focus on the
models where the multinomial distribution is assumed on the observed data
given latent variables, it is straightforward to replace it with other members of
the exponential family.
4.2.1 Bayesian Networks
A Bayesian network is a probabilistic model deﬁned by a graphical model
expressing the relations among random variables by a graph and the condi-
tional probabilities associated with them (Jensen, 2001). In this subsection,
we focus on a Bayesian network whose states of all hidden nodes inﬂuence
those of all observation nodes, and assume that it has M observation nodes
and K hidden nodes. The graphical structure of this Bayesian network is called
bipartite and presented in Figure 4.1.
The observation nodes are denoted by x = (x1,. . . , xM), and the set of states
of observation node x j = (xj,1,. . . , xj,Y j)⊤∈{el}
Y j
l=1 is {1,. . . , Yj}. The hidden

116
4 VB Algorithm for Latent Variable Models
. . .
.  .  .
 .  .  .
x1
x2
xM
z1
z2
zK
Figure 4.1 Graphical structure of the Bayesian network.
nodes are denoted by z = (z1,. . . , zK), and the set of states of hidden node
zk = (zk,1,. . . , zk,Tk)⊤∈{ei}Tk
i=1 is {1,. . . , Tk}.
The probability that the state of the hidden node zk is i (1 ≤i ≤Tk) is
expressed as
a(k,i) = Prob(zk = ei).
Then, ak = (a(k,1),. . . , a(k,Tk))⊤∈ΔTk−1 for k = 1,. . . , K.
The conditional probability that the jth observation node xj is l (1 ≤l ≤Yj),
given the condition that the states of hidden nodes are z = (z1,. . . , zK), is
denoted by
b(j,l|z) = Prob(xj = el|z).
Then, b j|z = (b(j,1|z),. . . , b(j,Y j|z))⊤∈ΔY j−1 for j = 1,. . . , M. Deﬁne bz =
{b j|z}M
j=1 for z ∈Z = {z; zk ∈{ei}Tk
i=1, k = 1,. . . , K}. Let w = {{ak}K
k=1, {bz}z∈Z}
be the set of all parameters. Then, the joint probability that the states of
observation nodes are x = (x1,. . . , xM) and the states of hidden nodes are
z = (z1,. . . , zK) is
p(x, z|w) = p(x|bz)
K

k=1
Tk

i=1
azk,i
(k,i),
where
p(x|bz) =
M

j=1
Y j

l=1
b
xj,l
(j,l|z).
Therefore, the marginal probability that the states of observation nodes are
x is
p(x|w) =

z∈Z
p(x, z|w)
=

z∈Z
p(x|bz)
K

k=1
Tk

i=1
azk,i
(k,i),
(4.55)

4.2 Other Latent Variable Models
117
where we used the notation 
z∈Z for the summation over all states of hidden
nodes. Let
Mobs =
M

j=1
(Yj −1),
which is the number of parameters to specify the conditional probability
p(x|bz) of the states of all the observation nodes given the states of the hidden
nodes. Then, the number of the parameters of the model, D, is
D = Mobs
K

k=1
Tk +
K

k=1
(Tk −1).
(4.56)
We assume that the prior distribution p(w) of the parameters w
=
{{ak}K
k=1, {bz}z∈Z} is the conditional conjugate prior distribution. Then, p(w) is
given by
,K
k=1 p(ak|φ)
- ,
z∈Z
M
j=1 p(b j|z|ξ)
-
, where
p(ak|φ) = DirichletTk

ak; (φ,. . . , φ)⊤ 
,
(4.57)
p(b j|z|ξ) = DirichletY j

b j|z; (ξ,. . . , ξ)⊤ 
,
(4.58)
are the Dirichlet distributions with hyperparameters φ > 0 and ξ > 0.
VB Posterior for Bayesian Networks
Let {D, H} be the complete data with the observed data set D = {x(n)}N
n=1
and the corresponding hidden variables H = {z(n)}N
n=1. Deﬁne the expected
sufﬁcient statistics:
N
z
(k,ik) =
N

n=1

z(n)
k,ik

rH(H) ,
N
x
(j,lj|z) =
N

n=1
x(n)
j,ljrz(z(n) = z),
(4.59)
where
rz(z(n) = z) =
/ K

k=1
z(n)
k,ik
0
rH(H)
(4.60)
is the estimated probability that z(n) = z = (ei1,. . . , eiK). Here x(n)
j
indicates
the state of the jth observation node and z(n)
k
indicates the state of the kth
hidden node when the nth training sample is observed. From Eq. (4.2), the
VB posterior distribution of parameters w = {{ak}K
k=1, {bz}z∈Z} is given by
rw(w) =
⎧⎪⎪⎨⎪⎪⎩
K

k=1
ra(ak)
⎫⎪⎪⎬⎪⎪⎭
⎧⎪⎪⎪⎨⎪⎪⎪⎩

z∈Z
M

j=1
rb(b j|z)
⎫⎪⎪⎪⎬⎪⎪⎪⎭,

118
4 VB Algorithm for Latent Variable Models
ra(ak) = DirichletTk

ak;φk
 
,
(4.61)
rb(b j|z) = DirichletY j

b j|z;ξ j|z
 
,
(4.62)
where
φk = (φ(k,1),. . . ,φ(k,Tk))⊤
(k = 1,. . . , K),
φ(k,i) = N
z
(k,i) + φ
(i = 1,. . . , Tk),
(4.63)
ξ j|z = (ξ(j,1|z),. . . ,ξ(j,Y j|z))⊤
( j = 1,. . . , M, z ∈Z),
ξ(j,l|z) = N
x
(j,l|z) + ξ
(l = 1,. . . , Yj).
(4.64)
Note that if we deﬁne
N
x
z =
N

n=1
rz(z(n) = z) =
N

n=1
/ K

k=1
z(n)
k,ik
0
rH(H)
,
for z = (ei1,. . . , eiK) ∈Z, we have
N
x
z =
Y j

l=1
N
x
(j,l|z),
(4.65)
for j = 1,. . . , M, and
N
z
(k,i) =

z−k
N
x
z,
(4.66)
where 
z−k denotes the summation over zk′ (k′  k) other than zk = ei.
It follows from Eqs. (4.61) and (4.62) that
log a(k,i)

ra(ak) = Ψ(φ(k,i)) −Ψ
Tk
i′=1 φ(k,i′)
 
(i = 1,. . . , Tk),
for k = 1,. . . , K and

log b(j,l|z)

rb(bj|z) = Ψ(ξ(j,l|z)) −Ψ
Y j
l′=1ξ(j,l′|z)
 
(l = 1,. . . , Yj),
for j = 1,. . . , M. From Eq. (4.3), the VB posterior distribution of the hidden
variables is given by rH(H) = N
n=1 rz(z(n)), where for z = (ei1,. . . , eiK),
rz(z(n) = z) =

z(n)∈Z
rz(z(n))
K

k=1
z(n)
k,ik
∝exp
⎛⎜⎜⎜⎜⎜⎝
K

k=1
2
Ψ(φ(k,ik)) −Ψ
!Tk
i′
k=1 φ(k,i′
k)
"3
+
M

j=1
2
Ψ(ξ(j,l(n)
j |z)) −Ψ
Y j
l′=1ξ(j,l′|z)
 3⎞⎟⎟⎟⎟⎟⎟⎠,
(4.67)
if x(n)
j
= el(n)
j .

4.2 Other Latent Variable Models
119
The VB algorithm updates {N
x
(j,lj|z)} using Eqs. (4.59) and (4.67) iteratively.
The other expected sufﬁcient statistics and variational parameters are com-
puted by Eqs. (4.65), (4.66) and Eqs. (4.63), (4.64), respectively. The free
energy as a function of the variational parameters is given by
F =
K

k=1
⎧⎪⎪⎨⎪⎪⎩log
⎛⎜⎜⎜⎜⎜⎝
Γ(Tk
i=1 φ(k,i))
Tk
i=1 Γ(φ(k,i))
⎞⎟⎟⎟⎟⎟⎠−log
 Γ(Tkφ)
(Γ(φ))Tk

+
Tk

i=1
φ(k,i) −φ −N
z
(k,i)
 
Ψ(φ(k,i)) −Ψ
Tk
i′=1 φ(k,i′)
  ⎫⎪⎪⎬⎪⎪⎭
+

z∈Z
M

j=1
⎧⎪⎪⎨⎪⎪⎩log
⎛⎜⎜⎜⎜⎜⎜⎝
Γ(Y j
l=1ξ(j,l|z))
Y j
l=1 Γ(ξ(j,l|z))
⎞⎟⎟⎟⎟⎟⎟⎠−log
 Γ(Yjξ)
(Γ(ξ))Y j

+
Y j

l=1
ξ(j,l|z) −ξ −N
x
(j,l|z)
 
Ψ(ξ(j,l|z)) −Ψ
Y j
l′=1ξ(j,l′|z)
  
⎫⎪⎪⎪⎬⎪⎪⎪⎭
+
N

n=1

z∈Z
rz(z(n) = z) log rz(z(n) = z).
(4.68)
The following update rule for the EVB learning of the hyperparameter φ is
obtained in the same way as the update rule (4.29) for the GMM:
φnew = max

0, φold −
K
k=1
,
Tk(Ψ(φ)−Ψ(Tkφ))−Tk
i=1

Ψ(φ(k,i))−Ψ
Tk
i′=1 φ(k,i′)
  -
K
k=1 Tk(Ψ(1)(φ)−TkΨ(1)(Tkφ))

.
(4.69)
Similarly, we obtain the following update rule of the hyperparameter ξ:
ξnew = max
⎛⎜⎜⎜⎜⎜⎝0, ξold −

z∈Z
M
j=1
2
Y j(Ψ(ξ)−Ψ(Yjξ))−Y j
l=1

Ψ(ξ(j,l|z))−Ψ
Yk
l′=1 ξ(j,l′|z)
  3
(K
k=1 Tk) M
j=1 Y j(Ψ(1)(ξ)−Y jΨ(1)(Y jξ))
⎞⎟⎟⎟⎟⎟⎠.
(4.70)
Let 
S = {rz(z(n) = z)}N
n=1,z∈Z =
2K
k=1 z(n)
k,ik

rH(H)
3N
n=1,z∈Z, Φ = {φk}K
k=1, and
Ξ = {ξ j|z}M
j=1,z∈Z be the sets of variational parameters. The EVB learning for the
Bayesian network is summarized in Algorithm 9. If the prior hyperparameters
are ﬁxed and Step 3 in the algorithm is omitted, the algorithm reduces to the
(nonempirical) VB learning algorithm.
4.2.2 Hidden Markov Models
Hidden Markov models (HMMs) have been widely used for sequence modeling
in speech recognition, natural language processing, and so on (Rabiner,

120
4 VB Algorithm for Latent Variable Models
Algorithm 9 EVB learning for the Bayesian network.
1: Initialize the variational parameters (
S, Φ, Ξ) and the hyperparameters
(φ, ξ).
2: Apply (substitute the right-hand side into the left-hand side) Eqs. (4.67),
(4.59), (4.65), (4.66), (4.63), and (4.64) to update 
S, Φ, and Ξ.
3: Apply Eqs. (4.69) and (4.70) to update φ and ξ, respectively.
4: Evaluate the free energy (4.68).
5: Iterate Steps 2 through 4 until convergence (until the energy decrease
becomes smaller than a threshold).
1989). In this subsection, we consider discrete HMMs. Suppose a sequence
D = (x(1),. . . , x(T)) was observed. Each x(t) is an M-dimensional binary vector
(M-valued ﬁnite alphabet):
x(t) = (x(t)
1 ,. . . , x(t)
M) ∈{e1,. . . , eM},
where if the output symbol at time t is m, then x(t)
m = 1, and otherwise 0.
Moreover, x(t) is produced in K-valued discrete hidden state z(t). The sequence
of hidden states H = (z(1),. . . , z(T)) is generated by a ﬁrst-order Markov
process. Similarly, z(t) is represented by a K-dimensional binary vector
z(t) = (z(t)
1 ,. . . , z(t)
K ) ∈{e1,. . . , eK},
where if the hidden state at time t is k, then z(t)
k = 1, and otherwise 0.
Without loss of generality, we assume that the initial state (t = 1) is the ﬁrst
one, namely z(1)
1 = 1. Then, the probability of a sequence is given by
p(D|w) =

H
M

m=1
bx(1)
m
1,m
T

t=2
K

k=1
K

l=1
a
z(t)
l z(t−1)
k
k,l
M

m=1
b
z(t)
k x(t)
m
k,m ,
(4.71)
where 
H is taken all over possible values of hidden variables, and the model
parameters, w = (A, B), consist of the state transition probability matrix A =
(a1,. . . ,aK)⊤and the emission probability matrix B = (b1,. . . ,bK)T satisfying
ak = (ak,1,. . . , ak,K)⊤∈ΔK−1 and bk = (bk,1,. . . , bk,K)T ∈ΔM−1 for 1 ≤k ≤K,
respectively. ak,l represents the transition probability from the kth hidden state
to the lth hidden state and bk,m is the emission probability that alphabet m is
produced in the kth hidden state. Figure 4.2 illustrates an example of the state
transition diagram of an HMM.

4.2 Other Latent Variable Models
121
Figure 4.2 State transition diagram of an HMM.
The log-likelihood of the HMM for a sequence of complete data {D, H} is
deﬁned by
log p(D, H|w) =
T

t=2
K

k=1
K

l=1
z(t)
k z(t−1)
l
log ak,l +
T

t=1
K

k=1
M

m=1
z(t)
k x(t)
m log bk,m.
We assume that the prior distributions of the transition probability matrix
A and the emission probability matrix B are the Dirichlet distributions with
hyperparameters φ > 0 and ξ > 0:
p(A|φ) =
K

k=1
DirichletK

ak; (φ,. . . , φ)⊤ 
,
(4.72)
p(B|ξ) =
K

k=1
DirichletM
bk; (ξ,. . . , ξ)⊤ 
.
(4.73)
VB Posterior for HMMs
We deﬁne the expected sufﬁcient statistics by
Nk =
T

t=1

z(t)
k

rH(H) ,
(4.74)
N
[z]
k,l =
T

t=2

z(t)
l z(t−1)
k

rH(H) ,
(4.75)
N
[x]
k,m =
T

t=1

z(t)
k

rH(H) x(t)
m ,
(4.76)
where the expected count Nk is constrained by Nk = 
l N
[z]
k,l. Then, the VB
posterior distribution of parameters rw(w) is given by

122
4 VB Algorithm for Latent Variable Models
rA(A) =
K

k=1
DirichletK

ak; (φk,1,. . . ,φk,K)⊤ 
,
rB(B) =
K

k=1
DirichletM
bk; (ξk,1,. . . ,ξk,M)⊤ 
,
where
φk,l = N
[z]
k,l + φ,
(4.77)
ξk,m = N
[x]
k,m + ξ.
(4.78)
The posterior distribution of hidden variables rH(H) is given by
rH(H) =
1
CH
exp
⎛⎜⎜⎜⎜⎜⎝
T

t=2
K

k=1
K

l=1
z(t)
k z(t−1)
l
log ak,l

rA(A)
+
T

t=1
K

k=1
M

m=1
z(t)
k x(t)
m
log bk,m

rB(B)
⎞⎟⎟⎟⎟⎟⎠,
(4.79)
where CH is the normalizing constant and
log ak,l

rA(A) = Ψ(φk,l) −Ψ
⎛⎜⎜⎜⎜⎜⎝
K

l′=1
φk,l′
⎞⎟⎟⎟⎟⎟⎠,
log bk,m

rB(B) = Ψ(ξk,m) −Ψ
⎛⎜⎜⎜⎜⎜⎝
M

m′=1
ξk,m′
⎞⎟⎟⎟⎟⎟⎠.
The expected sufﬁcient statistics

z(t)
k

rH(H) and

z(t)
l z(t−1)
k

rH(H) in Eqs.
(4.74) through (4.76) can be efﬁciently computed in the order of O(T) by the
forward–backward algorithm (Beal, 2003). This algorithm can also compute
CH. Thus, after the substitution of Eq. (4.3), the free energy is given by
F =
K

k=1
⎧⎪⎪⎨⎪⎪⎩log
⎛⎜⎜⎜⎜⎜⎝
Γ(K
l=1 φk,l)
K
l=1 Γ(φk,l)
⎞⎟⎟⎟⎟⎟⎠+
K

l=1
φk,l −φ
 
Ψ(φk,l) −Ψ(K
l′=1 φk,l′)
 
+ log
⎛⎜⎜⎜⎜⎜⎝
Γ(M
m=1ξk,m)
M
m=1 Γ(ξk,m)
⎞⎟⎟⎟⎟⎟⎠+
M

m=1
ξk,m −ξ
 
Ψ(ξk,m) −Ψ(M
m′=1ξk,m′)
 ⎫⎪⎪⎬⎪⎪⎭
−K log
 Γ(Kφ)
(Γ(φ))K

−K log
 Γ(Mξ)
(Γ(ξ))M

−logCH.
(4.80)
The following update rule for the EVB learning of the hyperparameter φ is
obtained in the same way as the update rule (4.29) for the GMM:
φnew = max

0, φold −
K2(Ψ(φ)−Ψ(Kφ))−K
k=1
K
l=1

Ψ(φk,l)−Ψ
K
l′=1 φk,l′
  
K2(Ψ(1)(φ)−KΨ(1)(Kφ))

.
(4.81)

4.2 Other Latent Variable Models
123
Algorithm 10 EVB learning for the hidden Markov model.
1: Initialize the variational parameters (
S, Φ, Ξ), and the hyperparameters
(φ, ξ).
2: Apply the forward–backward algorithm to rH(H) in Eq. (4.79) and
compute CH.
3: Apply (substitute the right-hand side into the left-hand side) Eqs. (4.74),
(4.75), (4.76), (4.77), and (4.78) to update Φ, and Ξ.
4: Apply Eqs.(4.81) and (4.82) to update φ and ξ, respectively.
5: Evaluate the free energy (4.80).
6: Iterate Steps 2 through 5 until convergence (until the energy decrease
becomes smaller than a threshold).
Similarly, we obtain the following update rule of the hyperparameter ξ:
ξnew = max

0, ξold −
KM(Ψ(ξ)−Ψ(Mξ))−K
k=1
M
m=1

Ψ(ξk,m)−Ψ
M
m′=1 ξk,m′
  
KM(Ψ(1)(ξ)−MΨ(1)(Mξ))

.
(4.82)
Let

S =
)2
z(t)
k

rH(H)
3K
k=1 ,
2
z(t)
l z(t−1)
k

rH(H)
3K
k,l=1
1T
t=1
,
Φ = {φk,l}K
k,l=1, and Ξ = {ξk,m}K,M
k,m=1 be the sets of variational parameters.
The EVB learning for the HMM is summarized in Algorithm 10. If the prior
hyperparameters are ﬁxed, and Step 4 in the algorithm is omitted, the algorithm
reduces to the (nonempirical) VB learning algorithm.
4.2.3 Probabilistic Context-Free Grammars
In this subsection, we discuss probabilistic context-free grammars (PCFGs),
which have been used for more complex sequence modeling applications
than those with the Markov assumption in natural language processing,
bioinformatics, and so on (Durbin et al., 1998). Without loss of generality, we
can assume that the grammar is written by the Chomsky normal form. Let the
model have K nonterminal symbols and M terminal symbols. The observation
sequence of length L is written by X = (x(1),. . . , x(L)) ∈{e1,. . . , eM}L. Then,
the statistical model is deﬁned by
p(X|w) =

Z∈T(X)
p(X, Z|w),
(4.83)

124
4 VB Algorithm for Latent Variable Models
Figure 4.3 Derivation tree of PCFG. A and B are nonterminal symbols and a and
b are terminal symbols.
p(X, Z|w) =
K

i, j,k=1

ai→jk
 cZ
i→jk
L

l=1
K

i=1
M

m=1
(bi→m)z(l)
i x(l)
m ,
w = {{ai}K
i=1, {bi}K
i=1},
ai = {ai→jk}K
j,k=1 (1 ≤i ≤K),
bi = {bi→m}M
m=1 (1 ≤i ≤K),
where T(X) is the set of derivation sequences that generate X, and Z
corresponds to a tree structure representing a derivation sequence. Figure 4.3
illustrates an example of the derivation tree of a PCFG model. The derivation
sequence is summarized by {cZ
i→jk}K
i, j,k=1 and {z(l)
i }L
l=1, where cZ
i→jk denotes the
count of the transition rule from the nonterminal symbol i to the pair of
nonterminal symbols ( j, k) appearing in the derivation sequence Z and z(l) =
(z(l)
1 ,. . . ,z(l)
K ) is the indicator of the (nonterminal) symbol generating the lth
output symbol of X. Moreover the parameter ai→jk represents the probability
that the nonterminal symbol i emits the pair of nonterminal symbols (j, k) and
bi→m represents the probability that the nonterminal symbol i emits the terminal
symbol m. The parameters, {{ai}K
i=1, {bi}K
i=1}, have constraints
ai→ii = 1 −

(j,k)(i,i)
ai→jk, bi→M = 1 −
M−1

m=1
bi→m,
i.e., ai ∈ΔK2−1 and bi ∈ΔM−1, respectively.
Let D
=
{X(1),. . . , X(N)} be a given training corpus and H
=
{Z(1),. . . , Z(N)} be the corresponding hidden derivation sequences. The log-
likelihood for the complete sample {D, H} is given by

4.2 Other Latent Variable Models
125
log p(D, H|w) =
N

n=1
⎡⎢⎢⎢⎢⎢⎢⎣
K

i, j,k=1
cZ(n)
i→jk log ai→jk +
L

l=1
K

i=1
M

m=1
z(n,l)
i
x(n,l)
m
log bi→m
⎤⎥⎥⎥⎥⎥⎥⎦,
where x(n,l) and z(n,l) are the indicators of the lth output symbol and the
nonterminal symbol generating the lth output in the nth sequences, X(n) and
Z(n), respectively.
We now turn to the VB learning for PCFGs (Kurihara and Sato, 2004).
We assume that the prior distributions of parameters {ai}K
i=1 and {bi}K
i=1 are the
Dirichlet distributions with hyperparameters φ and ξ:
p({ai}K
i=1|φ) =
K

i=1
DirichletK2

ai; (φ,. . . , φ)⊤ 
,
(4.84)
p({bi}K
i=1|ξ) =
K

i=1
DirichletM

bi; (ξ,. . . , ξ)⊤ 
.
(4.85)
VB Posterior for PCFGs
We deﬁne the expected sufﬁcient statistics as follows:
N
z
i→jk =
N

n=1
L

l=1

cZ(n)
i→jk

rz(Z(n)) ,
(4.86)
N
z
i =
K

j,k=1
N
z
i→jk,
N
x
i→m =
N

n=1
L

l=1

z(n,l)
i

rz(Z(n)) x(n,l)
m ,
(4.87)
N
x
i =
M

m=1
N
x
i→m.
Then the VB posteriors of the parameters are given by
rw(w) = ra({ai}K
i=1)rb({bi}K
i=1),
ra({ai}K
i=1) =
K

i=1
DirichletK2

ai; (φi→11,. . . ,φi→KK)⊤ 
,
(4.88)
rb({bi}K
i=1) =
K

i=1
DirichletM

bi; (ξi→1,. . . ,ξi→M)⊤ 
,
(4.89)
where
φi→jk = N
z
i→jk + φ,
(4.90)
ξi→m = N
x
i→m + ξ.
(4.91)

126
4 VB Algorithm for Latent Variable Models
The VB posteriors of the hidden variables are given by
rH(H) =
N

n=1
rz(Z(n)),
rz(Z(n)) =
1
CZ(n) exp $γZ(n)% ,
(4.92)
γZ(n) =
K

i, j,k=1
cZ(n)
i→jk

log ai→jk

ra({ai}K
i=1)
+
L

l=1
K

i=1
M

m=1
z(n,l)
i
x(n,l)
m
log bi→m

rb({bi}K
i=1) ,
where CZ(n) = 
Z∈T(X(n)) exp(γZ) is the normalizing constant and

log ai→jk

ra({ai}K
i=1) = Ψ
φi→jk
 
−Ψ
K
j′=1
K
k′=1 φi→j′k′
 
,
log bi→m

rb({bi}K
i=1) = Ψ
ξi→m
 
−Ψ
M
m′=1ξi→m′
 
.
All the expected sufﬁcient statistics and CZ(n) can be efﬁciently computed
by the inside–outside algorithm (Kurihara and Sato, 2004). The free energy,
after the substitution of Eq. (4.3), is given by
F =
K

i=1
⎧⎪⎪⎨⎪⎪⎩log
⎛⎜⎜⎜⎜⎜⎜⎝
Γ(K
j,k=1 φi→jk)
K
j,k=1 Γ(φi→jk)
⎞⎟⎟⎟⎟⎟⎟⎠
+
K

j,k=1
φi→jk −φ
 
Ψ
φi→jk
 
−Ψ
K
j′,k′=1 φi→j′k′)
  
+ log
⎛⎜⎜⎜⎜⎜⎜⎝
Γ
M
m=1ξi→m
 
M
m=1 Γ
ξi→m
 
⎞⎟⎟⎟⎟⎟⎟⎠+
M

m=1
ξi→m −ξ
 
Ψ
ξi→m
 
−Ψ
M
m′=1ξi→m′
  
⎫⎪⎪⎪⎬⎪⎪⎪⎭
−K log
 Γ(K2φ)
(Γ(φ))K2

−K log
 Γ(Mξ)
(Γ(ξ))M

−
N

n=1
logCZ(n).
(4.93)
The following update rules for the EVB learning of the hyperparameters φ
and ξ are obtained similarly to the HMM in Eqs. (4.81) and (4.82):
φnew = max

0, φold −
K3(Ψ(φ)−Ψ(K2φ))−K
i=1
K
j,k=1

Ψ
φi→jk
 
−Ψ
K
j′,k′=1 φi→j′k′
  
K3(Ψ(1)(φ)−K2Ψ(1)(K2φ))

,
(4.94)
ξnew = max

0, ξold −
KM(Ψ(ξ)−Ψ(Mξ))−K
i=1
M
m=1

Ψ
ξi→m
 
−Ψ
M
m′=1 ξi→m′
  
KM(Ψ(1)(ξ)−MΨ(1)(Mξ))

.
(4.95)

4.2 Other Latent Variable Models
127
Algorithm 11 EVB learning for probabilistic context-free grammar.
1: Initialize the variational parameters (
S, Φ, Ξ) and the hyperparameters
(φ, ξ).
2: Apply the inside–outside algorithm to rz(Z(n)) in Eq. (4.92) and compute
CZ(n) for n = 1,. . . , N.
3: Apply (substitute the right-hand side into the left-hand side) Eqs. (4.86),
(4.87), (4.90), and (4.91) to update Φ, and Ξ.
4: Apply Eqs. (4.94) and (4.95) to update φ and ξ, respectively.
5: Evaluate the free energy (4.93).
6: Iterate Steps 2 through 5 until convergence (until the energy decrease
becomes smaller than a threshold).
Let

S =
)2
cZ(n)
i→jk

rz(Z(n))
3K
i, j,k=1 ,
2
z(n,l)
i

rz(Z(n))
3L
l=1
1N
n=1
,
Φ = {φi→jk}K
i, j,k=1, and Ξ = {ξi→m}K,M
i,m=1 be the sets of variational parameters. The
EVB learning for the PCFG model is summarized in Algorithm 11. If the prior
hyperparameters are ﬁxed and Step 4 in the algorithm is omitted, the algorithm
reduces to the (nonempirical) VB learning algorithm.
4.2.4 Latent Dirichlet Allocation
Latent Dirichlet allocation (LDA) (Blei et al., 2003) is a generative model
successfully used in various applications such as text analysis (Blei et al.,
2003), image analysis (Li and Perona, 2005), genomics (Bicego et al., 2010;
Chen et al., 2010), human activity analysis (Huynh et al., 2008), and collab-
orative ﬁltering (Krestel et al., 2009; Purushotham et al., 2012). Given word
occurrences of documents in a corpora, LDA expresses each document as a
mixture of multinomial distributions, each of which is expected to capture a
topic. The extracted topics provide bases in a low-dimensional feature space,
in which each document is compactly represented. This topic expression was
shown to be useful for solving various tasks, including classiﬁcation (Li and
Perona, 2005), retrieval (Wei and Croft, 2006), and recommendation (Krestel
et al., 2009).
In this subsection, we introduce the VB learning for tractable inference in
the LDA model. Suppose that we observe M documents, each of which consists
of N(m) words. Each word is included in a vocabulary with size L. We assume
that each word is associated with one of the H topics, which is not observed.

128
4 VB Algorithm for Latent Variable Models
We express the word occurrence by an L-dimensional indicator vector w, where
one of the entries is equal to one and the others are equal to zero. Similarly,
we express the topic occurrence as an H-dimensional indicator vector z. We
deﬁne the following functions that give the item numbers chosen by w and z,
respectively:
´l(w) = l if wl = 1 and wl′ = 0 for l′  l,
´h(z) = h if zh = 1 and zh′ = 0 for h′  h.
In the LDA model (Blei et al., 2003), the word occurrence w(n,m) of the
nth position in the mth document is assumed to follow the multinomial
distribution:
p(w(n,m)|Θ, B) =
L

l=1

(BΘ⊤)l,m
 w(n,m)
l
= (BΘ⊤)´l(w(n,m)),m,
(4.96)
where Θ ∈[0, 1]M×H and B ∈[0, 1]L×H are parameter matrices to be estimated.
The rows of Θ = (θ1,. . . ,θM)⊤and the columns of B = $β1,. . . , βH
% are
probability mass vectors that sum up to one. That is, θm ∈ΔH−1 is the topic
distribution of the mth document, and βh ∈ΔL−1 is the word distribution of the
hth topic.
Suppose that we observe the data D = {{w(n,m)}N(m)
n=1 }M
m=1. Given the topic
occurrence latent variable z(n,m), the complete likelihood for each word is
written as
p(w(n,m), z(n,m)|Θ, B) = p(w(n,m)|z(n,m), B)p(z(n,m)|Θ),
(4.97)
where p(w(n,m)|z(n,m), B) =
L

l=1
H

h=1
(Bl,h)w(n,m)
l
z(n,m)
h , p(z(n,m)|Θ) =
H

h=1
(θm,h)z(n,m)
h .
We assume the Dirichlet priors on Θ and B:
p(Θ|α) =
M

m=1
DirichletH(θm; (α1,. . . , αH)⊤),
(4.98)
p(B|η) =
H

h=1
DirichletL(βh; (η1,. . . , ηL)⊤),
(4.99)
where α and η are hyperparameters that control the prior sparsity.
VB Posterior for LDA
For the set of all hidden variables H = {{z(n,m)}N(m)
n=1 }M
m=1 and the parameter w =
(Θ, B), we assume that our approximate posterior is factorized as Eq. (4.1).
Thus, the update rule (4.2) yields the further factorization rΘ,B(Θ, B) =
rΘ(Θ)rB(B) and the following update rules:

4.2 Other Latent Variable Models
129
rΘ(Θ) ∝p(Θ|α) log p(D, H|Θ, B)
rB(B)rH(H) ,
(4.100)
rB(B) ∝p(B|η) log p(D, H|Θ, B)
r(Θ)rH(H) .
(4.101)
Deﬁne the expected sufﬁcient statistics as
N
(m)
h
=
N(m)

n=1

z(n,m)
h

rz

{{z(n,m)}N(m)
n=1 }M
m=1
 ,
(4.102)
Wl,h =
M

m=1
N(m)

n=1
w(n,m)
l

z(n,m)
h

rz

{{z(n,m)}N(m)
n=1 }M
m=1
 .
(4.103)
Then, the VB posterior distribution is given by the Dirichlet distributions:
rΘ(Θ) =
M

m=1
Dirichlet
θm;αm
 
,
(4.104)
rB(B) =
H

h=1
Dirichlet $βh;ηh
% ,
(4.105)
where the variational parameters satisfy
αm,h = (αm)h = N
(m)
h
+ αh,
(4.106)
ηl,h = (ηh)l = Wl,h + ηl.
(4.107)
From the update rule (4.3), the VB posterior distribution of latent variables
is given by the multinomial distribution:
rz

{{z(n,m)}N(m)
n=1 }M
m=1
 
=
M

m=1
N(m)

n=1
MultinomialH,1

z(n,m);z(n,m) 
,
(4.108)
where the variational parameterz(n,m) ∈ΔH−1 is
z(n,m)
h
=
z(n,m)
h
H
h′=1 z(n,m)
h′
(4.109)
for
z(n,m)
h
= exp
⎛⎜⎜⎜⎜⎜⎝
log Θm,h

rΘ(Θ) +
L

l=1
w(n,m)
l
log Bl,h

rB(B)
⎞⎟⎟⎟⎟⎟⎠.
(4.110)
We also have

z(n,m)
h

rz

{{z(n,m)}N(m)
n=1 }M
m=1
 =z(n,m)
h
,
log Θm,h

rΘ(Θ) = Ψ(αm,h) −Ψ
H
h′=1 αm,h′
 
,
log Bl,h

rB(B) = Ψ(ηl,h) −Ψ
L
l′=1ηl′,h
 
.

130
4 VB Algorithm for Latent Variable Models
Iterating Eqs. (4.106), (4.107), and (4.110) provides a local minimum of the
free energy, which is given as a function of the variational parameters by
F =
M

m=1
⎛⎜⎜⎜⎜⎝log
⎛⎜⎜⎜⎜⎝
Γ(H
h=1 αm,h)
H
h=1 Γ(αm,h)
⎞⎟⎟⎟⎟⎠−log
⎛⎜⎜⎜⎜⎝
Γ(H
h=1 αh)
H
h=1 Γ(αh)
⎞⎟⎟⎟⎟⎠
⎞⎟⎟⎟⎟⎠
+
H

h=1
⎛⎜⎜⎜⎜⎝log
⎛⎜⎜⎜⎜⎝
Γ(L
l=1ηl,h)
L
l=1 Γ(ηl,h)
⎞⎟⎟⎟⎟⎠−log
⎛⎜⎜⎜⎜⎝
Γ(L
l=1 ηl)
L
l=1 Γ(ηl)
⎞⎟⎟⎟⎟⎠
⎞⎟⎟⎟⎟⎠
+
M

m=1
H

h=1
!
αm,h −(N
(m)
h
+ αh)
" 
Ψ(αm,h) −Ψ(H
h′=1 αm,h′)
 
+
H

h=1
L

l=1

ηl,h −(Wl,h + ηl)
 
Ψ(ηl,h) −Ψ(L
l′=1ηl′,h)
 
+
M

m=1
N(m)

n=1
H

h=1
z(n,m)
h
logz(n,m)
h
.
(4.111)
The partial derivatives of the free energy with respect to (α, η) are computed
as follows:
∂F
∂αh
= M

Ψ(αh) −Ψ(H
h′=1 αh′)
 
−
M

m=1

Ψ(αm,h) −Ψ(H
h′=1 αm,h′)
 
,
(4.112)
∂2F
∂αh∂αh′ = M

δh,h′Ψ(1)(αh) −Ψ(1)(H
h′′=1 αh′′)
 
,
(4.113)
∂F
∂ηl
= H

Ψ(ηl) −Ψ(L
l′=1 ηl′)
 
−
H

h=1

Ψ(ηl,h) −Ψ(L
l′=1ηl′,h)
 
,
(4.114)
∂2F
∂ηl∂ηl′ = H

δl,l′Ψ(1)(ηl) −Ψ(1)(L
l′′=1 ηl′′)
 
,
(4.115)
where δn,n′ is the Kronecker delta. Thus, we have the following Newton–
Raphson steps to update the hyperparameters:
αnew = max
⎛⎜⎜⎜⎜⎜⎝0, αold −
 ∂2F
∂α∂α⊤
−1 ∂F
∂α
⎞⎟⎟⎟⎟⎟⎠,
(4.116)
ηnew = max
⎛⎜⎜⎜⎜⎜⎝0, ηold −
 ∂2F
∂η∂η⊤
−1 ∂F
∂η
⎞⎟⎟⎟⎟⎟⎠,
(4.117)
where max(·) is the max operator applied elementwise.

4.2 Other Latent Variable Models
131
Algorithm 12 EVB learning for latent Dirichlet allocation.
1: Initialize the variational parameters ({{z(n,m)}N(m)
n=1 }M
m=1, {αm}M
m=1, {ηh}H
h=1),
and the hyperparameters (α, η).
2: Apply (substitute the right-hand side into the left-hand side) Eqs. (4.110),
(4.109), (4.102), (4.103), (4.106), and (4.107) to update {{z(n,m)}N(m)
n=1 }M
m=1,
{αm}M
m=1, and {ηh}H
h=1.
3: Apply Eqs. (4.116) and (4.117) to update α and η, respectively.
4: Evaluate the free energy (4.111).
5: Iterate Steps 2 through 4 until convergence (until the energy decrease
becomes smaller than a threshold).
The EVB learning for LDA is summarized in Algorithm 12. If the prior
hyperparameters are ﬁxed and Step 3 in the algorithm is omitted, the algorithm
reduces to the (nonempirical) VB learning algorithm.
We can also apply partially Bayesian (PB) learning by approximating the
posterior of Θ or B by the delta function (see Section 2.2.2). We call it PB-A
learning if Θ is marginalized and B is point-estimated, and PB-B learning
if B is marginalized and Θ is point-estimated. Note that the original VB
algorithm for LDA proposed by Blei et al. (2003) corresponds to PB-A learing
in our terminology. MAP learning, where both of Θ and B are point-estimated,
corresponds to the probabilistic latent semantic analysis (pLSA) (Hofmann,
2001), if we assume the ﬂat prior αh = ηl = 1 (Girolami and Kaban, 2003).

5
VB Algorithm under No Conjugacy
As discussed in Section 2.1.7, there are practical combinations of a model
and a prior where conjugacy is no longer available. In this chapter, as
a method for addressing nonconjugacy, we demonstrate local variational
approximation (LVA), also known as direct site bounding, for logistic regres-
sion and a sparsity-inducing prior (Jaakkola and Jordan, 2000; Girolami, 2001;
Bishop, 2006; Seeger, 2008, 2009). Then we describe a general framework
of LVA based on convex functions by using the associated Bregman diver-
gence (Watanabe et al., 2011).
5.1 Logistic Regression
Let D = {(x(1), y(1)), (x(2), y(2)),. . . , (x(N), y(N))} be the N observations of the
binary response variable y(n) ∈{0, 1} and the input vector x(n) ∈RM. The
logistic regression model assumes the following Bernoulli model over y =
(y(1), y(2),. . . , y(N))⊤given X = {x(1), x(2),. . . , x(N)}:
p(y|X, w) =
N

n=1
exp

y(n)(w⊤x(n)) −log

1 + ew⊤x(n)  
.
(5.1)
Let us consider the Bayesian learning of the parameter w assuming the
Gaussian prior distribution:
p(w) = GaussM(w; w0, S−1
0 ),
where S0 and w0 are the hyperparameters.
Gaussian approximations for the posterior distribution p(w|D) ∝p(w, y|X)
are obtained by LVA based on the facts that −log(e
√
h/2 + e−
√
h/2) is a convex
function of h and that log(1 + eg) is a convex function of g. More speciﬁcally,
132

5.1 Logistic Regression
133
because φ(h(w)) = −log(e
√
w2/2 + e−
√
w2/2) is a convex function of h(w) = w2
and ψ(g(w)) = log(1 + ew) is a convex function of g(w) = w, they are bounded
from below by their tangents at h(ξ) = ξ2 and g(η) = η, respectively:
−log
!
e
√
w2/2 + e−
√
w2/2"
≥−log
!
e
√
ξ2/2 + e−√
ξ2/2"
−(w2 −ξ2)tanh (ξ/2)
4ξ
,
log(1 + ew) ≥log(1 + eη) + (w −η)
eη
1 + eη .
By substituting these bounds into the likelihood (5.1), we obtain the following
bounds on p(w, y|X):
p(w; ξ) ≤p(w, y|X) ≤p(w; η),
where
p(w; ξ) ≡p(w)
N

n=1
exp

y(n) −1
2

w⊤x(n)
−θn
,
(w⊤x(n))2 −hn
-
−log
!
e
√hn
2 + e−
√hn
2
""
,
p(w; η) ≡p(w)
N

n=1
exp

(y(n) −κn)w⊤x(n) −b(κn)
 
.
Here we have put
θn = tanh( √hn/2)
4 √hn
,
(5.2)
κn =
egn
1 + egn ,
(5.3)
and {hn}N
n=1 and {gn}N
n=1 are the sets of variational parameters deﬁned from
ξ = (ξ1, ξ2,. . . , ξM)⊤and η = (η1, η2,. . . , ηM)⊤by the transformations hn =
(ξ⊤x(n))2 and gn = η⊤x(n), respectively. We also deﬁned the binary entropy
function by b(κ) = −κ log κ −(1 −κ) log(1 −κ) for κ ∈[0, 1].
Normalizing these bounds with respect to w, we approximate the posterior
by the Gaussian distributions as
qξ(w; ξ) = GaussM(m, S−1),
qη(w; η) = GaussM(m, S
−1),
whose mean and precision (inverse-covariance) matrix are respectively
given by
m = S−1 ,
S0w0 + N
n=1(y(n) −1/2)x(n)-
,
S = S0 + 2 N
n=1 θnx(n)x(n)⊤,
(5.4)

134
5 VB Algorithm under No Conjugacy
and
m = w0 + S−1
0
N
n=1(y(n) −κn)x(n),
S = S0.
(5.5)
We also obtain the bounds for the marginal likelihood, Z(ξ) ≡

p(w; ξ)dw
and Z(η) ≡

p(w; η)dw. These are respectively given in the forms of free
energy bounds as follows:
F(ξ) ≡−log Z(ξ)
= 1
2 log |S| −1
2 log |S0| + w⊤
0 S0w0
2
−m⊤(S)m
2
−
N

n=1
)
hnθn −log

2 cosh
 √hn
2
1
,
(5.6)
and
F(η) ≡−log Z(η)
= w⊤
0 S0w0
2
−m⊤S0m
2
+
N

n=1
b(κn).
We optimize the free energy bounds to determine the variational parameters.
As will be discussed generally in Section 5.3.2, to decrease the upper-bound
F(ξ), the EM algorithm is available, which instead maximizes

log p(w; ξ)

qξ(w;ξo) ,
where the expectation is taken with respect to the approximate posterior before
updating with the variational parameters given by ξo. The update rule of the
variational parameters is speciﬁcally given by
hn =

(w⊤x(n))2
qξ(w;ξo)
= x(n)⊤(S−1 + mm⊤)x(n),
(5.7)
where m and S−1 are the mean and covariance matrix of qξ(w; ξo).
We can use the following gradient for the maximization of the lower-bound
F(η):
∂F(η)
∂κn
=

w⊤x(n)
qη(w;η) −η⊤x(n)
= m⊤x(n) −gn.
(5.8)
The Newton–Raphson step to update κ = (κ1,. . . , κN)⊤is given by
κnew = κold −
 ∂2F
∂κ∂κ⊤
−1 ∂F
∂κ ,
(5.9)

5.2 Sparsity-Inducing Prior
135
Algorithm 13 LVA algorithm for logistic regression.
1: Initialize the variational parameters {hn}N
n=1 and transform them to {θn}N
n=1
by Eq. (5.2).
2: Compute the approximate posterior mean and covariance matrix (m, S−1)
by Eq. (5.4).
3: Apply Eq. (5.7) to update {hn}N
n=1 and transform them to {θn}N
n=1 by
Eq. (5.2).
4: Evaluate the free energy bound (5.6).
5: Iterate Steps 2 through 4 until convergence (until the decrease of the bound
becomes smaller than a threshold).
where the (n, n′)th entry of the Hessian matrix is given as follows:
∂2F(η)
∂κn∂κn′ = −x(n)⊤S−1
0 x(n′) −δn,n′
 1
κn
+
1
1 −κn

.
The learning algorithm for logistic regression with LVA is summarized in
Algorithm 13 in the case of F(ξ) minimization. To obtain the algorithm for
F(η) maximization, the updated variables are replaced with {gn}N
n=1, {κn}N
n=1,
and (m, S
−1), and the update rule (5.7) in Step 3 is replaced with the Newton–
Raphson step (5.9).
Recall the arguments in Section 2.1.7 that the VB posterior r(w;λ) = q(w; ξ)
in Eq. (2.32) minimizes the upper-bound of the free energy (2.25) and the
approximate posterior r(w;ν) = q(w; η) in Eq. (2.58) maximizes the lower-
bound of the objective function of EP (2.50). This means that the variational
parameters are given byλ = (m, S−1) andν = (m, S
−1) in the LVAs for VB and
EP, respectively.
5.2 Sparsity-Inducing Prior
Another representative example where a nonconjugate prior is used is the linear
regression model with a sparsity-inducing prior distribution (Girolami, 2001;
Seeger, 2008, 2009). We discuss the linear regression model for i.i.d. data D =
{(x(1), y(1)), (x(2), y(2)),. . . , (x(N), y(N))}, where for each observation, x(n) ∈RM
is the input vector and y(n) ∈R is the response. Denoting y = (y(1),. . . , y(N))⊤
and X = (x(1),. . . , x(N))⊤, we assume the model,
p(y|X, w) = GaussN(y; Xw, σ2IN),

136
5 VB Algorithm under No Conjugacy
and the following sparsity-inducing prior with the Lβ-norm:
p(w) =
M

m=1
1
Cβ,γ
exp

−γ|wm|β 
,
(5.10)
where γ > 0 and 0 < β ≤2 are the hyperparameters, and Cβ,γ = 2
βγ1−1/βΓ(1/β)
is the normalizing constant. For 0 < β < 2, the prior has a heavier tail than the
Gaussian distribution (β = 2) and induces sparsity of the coefﬁcients w.
We apply the following inequality for w, ξ ∈R:

w2 β/2 ≤β
2

ξ2 β
2 −1 
w2 −ξ2 
,
which is obtained from the concavity of the function f(x) = xβ/2 for x > 0
and 0 < β < 2. Introducing the variational parameter to each dimension,
ξ
=
(ξ1,. . . , ξM)⊤and bounding the nonconjugate prior (5.10) by this
inequality, we have
p(y, w|X) = p(y|X, w)p(w)
≥
1
(2πσ2)N/2CM
β,γ
· exp
⎛⎜⎜⎜⎜⎜⎝−1
2σ2
N

n=1
(y(n) −w⊤x(n))2 −βγ
2
M

m=1

ξ2
m
 β
2 −1 
w2
m −ξ2
m
 ⎞⎟⎟⎟⎟⎟⎠
≡p(w; ξ).
Normalizing the lower-bound p(w; ξ), we obtain a Gaussian approximation to
the posterior. This is in effect equivalent to assuming the Gaussian prior for w:
GaussM(w; 0, S−1
ξ ),
where Sξ = γβDiag(ξβ−1/2) for ξβ−1/2 ≡

ξ2
1
 β
2 −1 ,. . . ,

ξ2
M
 β
2 −1⊤
.
The resulting Gaussian approximation to the posterior is
qξ(w; ξ) = GaussM(w; m, S−1),
where
S = Sξ + 1
σ2 X⊤X,
(5.11)
m = 1
σ2 S−1X⊤y.
(5.12)

5.3 Uniﬁed Approach by Local VB Bounds
137
Algorithm 14 LVA algorithm for sparse linear regression.
1: Initialize the variational parameters {ξ2
m}M
m=1.
2: Compute the approximate posterior mean and covariance matrix (m, S−1)
by Eqs. (5.11) and (5.12).
3: Apply Eq. (5.14) to update {ξ2
m}M
m=1.
4: Evaluate the free energy bound (5.13).
5: Iterate Steps 2 through 4 until convergence (until the decrease of the bound
becomes smaller than a threshold).
We also obtain the upper bound for the free energy:
F(ξ) = −log

p(w; ξ)dw
= N −M
2
log(2π) + log |S|
2
+ M logCβ,γ −
N

n=1
(y(n))2
2σ2 + γβ
2
M

m=1

ξ2
m
 β/2 ,
(5.13)
which is optimized with respect to the variational parameter. The general
framework in Section 5.3.2, which corresponds to the EM algorithm, provides
the following update rule:
ξ2
m =

w2
m

qξ(w;ξo)
= (S −1)mm + m2
m,
(5.14)
where m and S−1 are the mean and covariance matrix of qξ(w; ξo).
The learning algorithm for sparse linear regression with this approximation
is summarized in Algorithm 14.
This approximation has been applied to the Laplace prior (β = 1) in Seeger
(2008, 2009). LVA for another heavy-tailed distribution, p(w) ∝cosh−1/β(βw),
is discussed in Girolami (2001), which also bridges the Gaussian distribution
(β →0) and the Laplace distribution (β →∞).
5.3 Uniﬁed Approach by Local VB Bounds
As discussed in Section 2.1.7, LVA for VB and LVA for EP form lower- and
upper-bounds of the joint distribution p(w, D), denoted by p(w; ξ) and p(w; η),
respectively. If the bounds satisfying

138
5 VB Algorithm under No Conjugacy
p(w; ξ) ≤p(w, D),
(5.15)
p(w; η) ≥p(w, D),
(5.16)
for all w and D are analytically integrable, then by normalizing the bounds
instead of p(w, D), LVAs approximate the posterior distribution by
qξ(w; ξ) =
p(w; ξ)
Z(ξ) ,
(5.17)
qη(w; η) = p(w; η)
Z(η)
,
(5.18)
respectively, where Z(ξ) and Z(η) are the normalization constants deﬁned by
Z(ξ) =

p(w; ξ)dw,
Z(η) =

p(w; η)dw.
Here ξ and η are called the variational parameters.
The respective approximations are optimized by estimating the variational
parameters, ξ and η so that Z(ξ) is maximized and Z(η) is minimized since the
inequalities
Z(ξ) ≤Z ≤Z(η)
(5.19)
hold by deﬁnition, where Z = p(D) is the marginal likelihood.
To consider the respective LVAs in terms of information divergences in later
sections, let us introduce the Bayes free energy,
FBayes ≡−log Z,
and its lower- and upper-bounds, F(η) = −log Z(η) and F(ξ) = −log Z(ξ). By
taking the negative logarithms on both sides of Eq. (5.19), we have
F(η) ≤FBayes ≤F(ξ).
(5.20)
Hereafter, we follow the measure of the free energy and adopt the following
terminology to refer to respective LVAs (5.18) and (5.17): the lower-bound
maximization (F(η) maximization) and the upper-bound minimization (F(ξ)
minimization).
5.3.1 Divergence Measures in LVA
Most of the existing LVA techniques are based on the convexity of the
log-likelihood function or the log-prior (Jaakkola and Jordan, 2000; Bishop,

5.3 Uniﬁed Approach by Local VB Bounds
139
2006; Seeger, 2008, 2009). We describe these cases by using general convex
functions, φ and ψ, and show that the objective functions,
F(ξ) −FBayes = log
Z
Z(ξ) ≥0,
FBayes −F(η) = log Z(η)
Z
≥0,
to be minimized in the approximations (5.17) and (5.18), are decomposable
into the sum of the KL divergence and the expected Bregman divergence.
Let φ and ψ be twice differentiable real-valued strictly convex functions and
denote by dφ the Bregman divergence associated with the function φ (Banerjee
et al., 2005):
dφ(v1, v2) = φ(v1) −φ(v2) −(v1 −v2)⊤∇φ(v2) ≥0,
(5.21)
where ∇φ(v2) denotes the gradient vector of φ at v2.
Let us consider the case where φ and ψ are respectively used to form the
following bounds of the joint distribution p(w, D):
p(w; ξ) = p(w, D) exp{−dφ(h(w), h(ξ))},
(5.22)
p(w; η) = p(w, D) exp{dψ(g(w), g(η))},
(5.23)
where h and g are vector-valued functions of w.1
Eq. (5.22) is interpreted as follows. log p(w, D) includes a term that
prevents analytic integration of p(w, D) with respect to w. If such a term
is expressed by the convex function φ of some function h transforming w,
it is replaced by the tangent hyperplane, φ(h(ξ)) + (h(w) −h(ξ))⊤∇φ(h(ξ)),
so that log p(w; ξ) makes a simpler function of w, such as a quadratic
function. Remember that if log p(w; ξ) is quadratic with respect to w, p(w; ξ)
is analytically integrable by the Gaussian integral.
Rephrased in terms of the convex duality theory (Jordan et al., 1999;
Bishop, 2006), φ(h(w)) is replaced by its lower-bound,
φ(h(w)) ≥φ(h(ξ)) + (h(w) −h(ξ))⊤∇φ(h(ξ))
(5.24)
= θ(ξ)⊤h(w) −φ(θ(ξ)),
(5.25)
where we have put θ(ξ) = ∇φ(h(ξ)) and
φ(θ(ξ)) = θ(ξ)⊤h(ξ) −φ(h(ξ))
= max
h {θ(ξ)⊤h −φ(h)}
1 The functions g and h (also ψ and φ) can be dependent on D in this discussion. However, we
denote them as if they were independent of D for simplicity. They are actually independent of
D in the examples in Sections 5.1 and 5.2 and in most applications (Bishop, 2006; Seeger,
2008, 2009).

140
5 VB Algorithm under No Conjugacy
φ(h(w))
dφ(h(w), h(ξ))
h(w)
h(ξ)
−˜φ(θ(ξ))
θ(ξ)h(w) −˜φ(θ(ξ))
Figure 5.1 Convex function φ (solid curve), its tangent (dashed line), and the
Bregman divergence (arrow).
is the conjugate function of φ. The inequality (5.24) indicates the fact that
the convex function φ is bounded globally by its tangent at h(ξ), which is
equivalent to the nonnegativity of the Bregman divergence. In Eq. (5.25), the
tangent is reparameterized by θ(ξ), its gradient, instead of the contact point
h(ξ), and its offset is given by −φ(θ(ξ)). Figure 5.1 illustrates the relationship
among the convex function φ, its lower-bound, and the Bregman divergence.
We now describe the free energy bounds F(ξ) and F(η) in terms of infor-
mation divergences. It follows from the deﬁnition (5.17) of the approximate
posterior distribution that
KL(qξ(w; ξ)||p(w|D)) =

qξ(w; ξ) log
Zp(w; ξ)
Z(ξ)p(w, D)dw
= log
Z
Z(ξ) −

dφ(h(w), h(ξ))

qξ(w;ξ) .
We have a similar decomposition for KL(p(w|D)||qη(w; η)) as well. Finally,
we obtain the following expressions:2
F(ξ) −FBayes =

dφ(h(w), h(ξ))

qξ + KL(qξ||p),
(5.26)
FBayes −F(η) =

dψ(g(w), g(η))

p + KL(p||qη).
(5.27)
Recall that FBayes + KL(qξ||p) = F is the free energy, which is further bounded
by F(λ, ξ) in Eq. (2.25). The expression (5.26) shows that the gap between
minλ F(λ, ξ) and F is the expected Bregman divergence

dφ(h(w), h(ξ))

qξ.
Recall also that FBayes −KL(p||qη) = E is the objective function of the EP
problem (2.46) and that F(η) = −log Z(η) is obtained as the maximum of its
lower-bound, maxν E(ν, η) in Eq. (2.59), under a monotonic transformation.
2 Hereafter in this section, we omit the notation “(w|D)” if no confusion is likely.

5.3 Uniﬁed Approach by Local VB Bounds
141
The expression (5.27) shows that the gap between E and maxν E(ν, η) is
expressed by the expected Bregman divergence

dψ(g(w), g(η))

p while the
expectation is taken with respect to the true posterior.
Similarly, we also have the following decompositions:
F(ξ) −FBayes =

dφ(h(w), h(ξ))

p −KL(p||qξ),
(5.28)
and
FBayes −F(η) =

dψ(g(w), g(η))

qη −KL(qη||p).
Unlike Eqs. (5.26) and (5.27), the KL divergence is subtracted in these
expressions. This again implies the afﬁnities of LVAs by F minimization and
F maximization to VB and EP, respectively.
5.3.2 Optimization of Approximations
In this subsection, we show that the conditions for the optimal variational
parameters are generally given by the moment matching with respect to h(ξ)
and g(η).
Optimal Variational Parameters
From Eqs. (5.22) and (5.23), we can see that the approximate posteriors,
qξ(w; ξ) ∝p(w, D) exp{h(w)⊤∇φ(h(ξ)) −φ(h(w))}
(5.29)
qη(w; η) ∝p(w, D) exp{−g(w)⊤∇ψ(g(η)) + ψ(g(w))},
are members of the exponential family with natural parameters ∇φ(h(ξ)) and
∇ψ(g(η)) (Section 1.2.3). Let
θ(ξ) = ∇φ(h(ξ)) and
κ(η) = ∇ψ(g(η)).
The variational parameters are optimized so that F(ξ) is minimized and
F(η) is maximized, respectively. In practice, however, they can be optimized
directly with respect to h(ξ) and g(η) instead of ξ and η. Applications of
LVA, storing h(ξ) and g(η) as parameters, do not require ξ and η explicitly.
Furthermore, we consider the gradient vectors of the free energy bounds with
respect to θ(ξ) and κ(η), which have one-to-one correspondence with h(ξ) and
g(η), because they provide simple expressions of the gradient vectors. For
notational simplicity, we drop the dependencies on ξ and η and denote as θ
and κ.

142
5 VB Algorithm under No Conjugacy
The gradient of the upper bound with respect to θ is given by3
∇θF(ξ) = ∇θ
)
−log

p(w; ξ)dw
1
= −

1
Z(ξ)
∂p(w; ξ)
∂θ
dw
= −∂h(ξ)
∂θ

1
Z(ξ)
∂p(w; ξ)
∂h(ξ) dw
= −∂h(ξ)
∂θ
 ∂2φ(h(ξ))
∂h∂h⊤(h(w) −h(ξ))qξ(w; ξ)dw
= −(⟨h(w)⟩qξ −h(ξ)),
(5.30)
where we have used Eq. (5.22) and the fact that the matrix ∂h(ξ)
∂θ , whose (i, j)th
entry is ∂hi(ξ)
∂θj , is the inverse of the Hessian matrix ∂2φ(h(ξ))
∂h∂h⊤. Similarly, we obtain
∇κF(η) = ⟨g(w)⟩qη −g(η).
(5.31)
Hence, we can utilize gradient methods to minimize F(ξ) and maximize F(η).
We can see that when ξ and η are optimized,
h(ξ) = ⟨h(w)⟩qξ
and
g(η) = ⟨g(w)⟩qη
hold.
In practice, the variational parameter h(ξ) is iteratively updated so that
F(ξ) is monotonically decreased. Recall the argument in Section 2.1.7 where
LVA for VB was formulated as the joint minimization of F(λ, ξ) over the
approximate posterior r(w;λ) and ξ. The free energy bound F(ξ) = −log Z(ξ)
was obtained as the minimum of F(λ, ξ), which is attained by (see Eq. (2.32))
r(w;λ) = q(w; ξ).
Let h(ξo) be a current estimate of h(ξ) andλo be the variational parameter such
that r(w;λo) = q(w; ξo). Then, updating h(ξ) to argminh(ξ) F(λo, ξ) decreases
F(ξ) because
F(λo, ξ) ≥F(ξ)
for all ξ and the equality holds when ξ = ξo. More speciﬁcally, it follows for
h(ξ) = argminh(ξ) F(λo, ξ) that
F(ξ) ≤F(λo,ξ) ≤F(λo, ξo) = F(ξo),
(5.32)
3 We henceforth use the operator ∇with the subscript expressing for which variable the gradient
is taken. That is, for a function f(θ), ∇θ f(θ) = ∂f(θ)
∂θ
denotes the vector whose ith element is
∂f(θ)
∂θi .

5.3 Uniﬁed Approach by Local VB Bounds
143
which means that the bound is improved. This corresponds to the EM algo-
rithm to decrease F(ξ) and yields the following speciﬁc update rule of h(ξ):
h(ξ) = argmin
h(ξ)
F(λo,ξ)
= argmin
h(ξ)

−log p(w; ξ)

qξ(w;ξo)
(5.33)
= argmin
h(ξ)

dφ(h(w), h(ξ))

qξ(w;ξo)
(5.34)
= argmin
h(ξ)
dφ(⟨h(w)⟩qqξ(w;ξo) , h(ξ))
(5.35)
= ⟨h(w)⟩qξ(w;ξo) ,
(5.36)
which is summarized as
h(ξ) = ⟨h(w)⟩qξ(w;ξo) .
(5.37)
The preceding lines of equations are basically derived by focusing on the
terms depending on h(ξ). Eq. (5.33) follows from the deﬁnition of F(λo,ξ)
by Eq. (2.25). Eq. (5.34) follows from the deﬁnition of p(w; ξ) by Eq. (5.15).
Eq. (5.35) follows from the deﬁnition of the Bregman divergence (5.21) and
the linearity of expectation. Eq. (5.36) follows from the nonnegativity of the
Bregman divergence. Eqs. (5.34) through (5.36) are equivalent to the fact that
the expected Bregman divergence is minimized by the mean (Banerjee et al.,
2005).
The update rule (5.37) means that h(ξ) is updated to the expectation of h(w)
with respect to the approximate posterior. Note here again that if we store h(ξ),
ξ is not explicitly required.
The update rule (5.37) is an iterative substitution of h(ξ). To maximize the
lower-bound F(η) in LVA for EP, such a simple update rule is not applicable
in general. Thus, gradient-based optimization methods with the gradient (5.31)
are usually used. The Newton–Raphson step to update κ is given by
κnew = κold −

∇2
κF(ηold)
 −1 ∇κF(ηold),
(5.38)
where the Hessian matrix is given as follows:
∇2
κF(η) = ∂2F(η)
∂κ∂κ⊤
= −Cov(g(w)) −∂g(η)
∂κ
(5.39)
for the covariance matrix of g(w),
Cov(g(w)) =

g(w)g(w)⊤
qη −⟨g(w)⟩qη ⟨g(w)⟩⊤
qη ,
and ∂g(η)
∂κ
=
!
∂2ψ(g(η))
∂g∂g⊤
"−1
holds in Eq. (5.39).

144
5 VB Algorithm under No Conjugacy
5.3.3 An Alternative View of VB for Latent Variable Models
In this subsection, we show that the VB learning for latent variable models can
be viewed as a special case of LVA, where the log-sum-exp function is used to
form the lower-bound of the log-likelihood (Jordan et al., 1999).
Let H be a vector of latent (unobserved) variables and consider the latent
variable model,
p(D, w) =

H
p(D, H, w),
where 
H denotes the summation over all possible realizations of the latent
variables. We have used the notation as if H were discrete in order to include
examples such as GMMs and HMMs, where the likelihood function is given
by p(D|w) = 
H p(D, H|w). In the case of a model with continuous latent
variables, the summation 
H is simply replaced by the integration

dH. This
includes, for example, the hierarchical prior distribution presented in Tipping
(2001), where the prior distribution is deﬁned by p(w) =

p(w|H)p(H)dH
with the hyperprior p(H).
The Bayesian posterior distribution of the latent variables and the parameter
w is
p(H, w|D) =
p(D, H, w)

H

p(D, H, w)dw
,
which is intractable when Z = 
H

p(D, H, w)dw requires summation
over exponentially many terms as in GMMs and HMMs or the analytically
intractable integration. So is the posterior of the parameter p(w|D).
Let us consider an application of the local variational method for approx-
imating p(w|D). By the convexity of the function log 
H exp(·), the log-joint
distribution is lower-bounded as follows:
log p(D, w) = log

H
exp{log p(D, H, w)}
≥log p(D, ξ) +

H

log p(D, H, w)
p(D, H, ξ)

p(H|D, ξ)
= log p(D, w) −

H
p(H|D, ξ) log p(H|D, ξ)
p(H|D, w),
(5.40)
where p(H|D, ξ) =
p(D,H,ξ)

H p(D,H,ξ). This corresponds to the case where φ(h) =
log 
n exp(hn) and h(w) is the vector-valued function that consists of the ele-
ments log p(D, H, w) for all possible H. The vector h is inﬁnite dimensional
when H is continuous. Taking exponentials of the most right-hand side and

5.3 Uniﬁed Approach by Local VB Bounds
145
left-hand side of Inequality (5.40) leads to Eqs. (5.22) and (5.15) with the
Bregman divergence,
dφ(h(w), h(ξ)) =

H
p(H|D, ξ) log p(H|D, ξ)
p(H|D, w)
= KL(p(H|D, ξ)||p(H|D, w)).
From Eq. (5.26), we have
F(ξ) = FBayes + KL(qξ(w; ξ)||p(w|D)) + ⟨KL(p(H|D, ξ)||p(H|D, w))⟩qξ(w;ξ)
= FBayes + KL(qξ(w; ξ)p(H|D, ξ)||p(w, H|D)),
which is exactly the free energy of the factorized distribution qξ(w; ξ)p(H|D, ξ).
In fact, from Eqs. (5.29) and (5.40), the approximating posterior is given by
qξ(w; ξ) ∝exp
⎧⎪⎪⎨⎪⎪⎩

H
log p(D, H, w)p(H|D, ξ)
⎫⎪⎪⎬⎪⎪⎭
= exp log p(D, H, w)
p(H|D,ξ) .
(5.41)
From Eq. (5.37), the EM update for the variational parameters ξ yields
log p(D, H, ξ) = log p(D, H, w)
qξ(w;ξo)
⇒p(H|D, ξ) ∝exp log p(D, H, w)
qξ(w;ξo) .
(5.42)
Eqs. (5.41) and (5.42) are exactly the same as the VB algorithm for minimizing
the free energy over the factorized distributions, Eqs. (4.2) and (4.3). In this
example, we no longer have ξ satisfying Eq. (5.42) in general. However, if the
model p(H, D|w) and the prior p(w) are included in the exponential family,
h(ξ) as well as p(H|D, ξ) and qξ(w; ξ) are expressed by expected sufﬁcient
statistics, the number of which is equal to the dimensionality of w (Beal, 2003).
In that case, it is not necessary to obtain ξ explicitly but only to store and update
the expected sufﬁcient statistics instead.


Part III
Nonasymptotic Theory


6
Global VB Solution of Fully Observed
Matrix Factorization
Variational Bayesian (VB) learning has shown good performance in many
applications. However, VB learning sometimes gives a seemingly different
posterior and exhibits different sparsity behavior from full Bayesian learning.
For example, Figure 6.1 compares the Bayes posterior (left) and the VB
posterior (right) of 1×1 matrix factorization. VB posterior tries to approximate
a two-mode Bayes posterior with a single-mode Gaussian, which results in the
zero-mean Gaussian posterior with the VB estimator BA = 0. This behavior
makes the VB estimator exactly sparse as shown in Figure 6.2: thresholding is
observed for the VB estimator, while no thresholding is observed for the full
Bayesian estimator. Mackay (2001) discussed the sparsity of VB learning as
an artifact by showing inappropriate model pruning in mixture models. These
facts might deprive the justiﬁcation of VB learning based solely on the fact
that it is a tractable approximation to Bayesian learning. Can we clarify the
behavior of VB learning, and directly justify its use as an inference method?
The nonasymptotic theory, introduced in Part III, gives some answer to this
question.
In this chapter, we derive an analytic-form of the global VB solution of
fully observed matrix factorization (MF). The analytic-form solution allows
us to make intuitive discussion on the behavior of VB learning (Chapter 7),
and further analysis gives theoretical guarantees of the performance of VB
learning (Chapter 8). The analytic-form global solution naturally leads to
efﬁcient and reliable algorithms (Chapter 9), which are extended to other
similar models (Chapters 10 and 11). Relation to MAP learning and partially
Bayesian learning is also theoretically investigated (Chapter 12).
149

150
6 Global VB Solution of Fully Observed Matrix Factorization
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.3
0.3
0.3
0.3
0.3
A
B
Bayes posterior (V = 1)
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
MAP estimators:
(A, B) ≈(± 1, ±
0.05
0.05
0.05
0.05
0.05
0.05
0.05
0.1
0.1
0.1
0.1
0.15
0.15
A
B
VB posterior (V = 1)
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
VB estimator : (A, B) = (0, 0)
Figure 6.1
The Bayes posterior (left) and the VB posterior (right) of the 1 × 1
MF model V = BA + E with almost ﬂat prior, when V = 1 is observed (E is
the standard Gaussian noise). VB approximates the Bayes posterior having two
modes by an origin-centered Gaussian, which induces sparsity.
0
1
2
3
0
0.5
1
1.5
2
2.5
3
Figure 6.2 Behavior of the estimators of 
U = BA as a function of the observed
value V. The VB estimator is zero when V ≤1, which indicates exact sparsity. On
the other hand, the Bayesian estimator shows no sign of sparsity. The maximum
likelihood estimator, i.e., 
U = V, is shown as a reference.
6.1 Problem Description
We ﬁrst summarize the MF model and its VB learning algorithm, which was
derived in Section 3.1. The likelihood and priors are given as
p(V|A, B) ∝exp

−1
2σ2
###V −BA⊤###2
Fro

,
(6.1)
p(A) ∝exp

−1
2tr

AC−1
A A⊤ 
,
(6.2)

6.1 Problem Description
151
p(B) ∝exp

−1
2tr

BC−1
B B⊤ 
,
(6.3)
where the prior covariances are restricted to be diagonal:
CA = Diag(c2
a1,. . . , c2
aH),
CB = Diag(c2
b1,. . . , c2
bH),
for cah, cbh > 0, h = 1,. . . , H. Without loss of generality, we assume that the
diagonal entries of the product CACB are arranged in nonincreasing order, i.e.,
cahcbh ≥cah′ cbh′ for any pair h < h′. We assume that
L ≤M.
(6.4)
If L > M, we may simply redeﬁne the transpose V⊤as V so that L ≤M holds.
Therefore, the assumption (6.4) does not impose any restriction.
We solve the following VB learning problem:
r = argmin
r
F(r)
s.t.
r(A, B) = rA(A)rB(B),
(6.5)
where the objective function to be minimized is the free energy:
F =
/
log
rA(A)rB(B)
p(V|A, B)p(A)p(B)
0
rA(A)rB(B)
.
The solution to the problem (6.5) is in the following form:
rA(A) = MGaussM,H(A; A, IM ⊗ΣA) ∝exp
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
−
tr
!
(A −A)Σ
−1
A (A −A)⊤
"
2
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
,
(6.6)
rB(B) = MGaussL,H(B; B, IL ⊗ΣB) ∝exp
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
−
tr
!
(B −B)Σ
−1
B (B −B)⊤
"
2
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
.
(6.7)
With the variational parameters A, ΣA, B, ΣB, the free energy can be explicitly
written as
2F = LM log(2πσ2) +
####V −BA
⊤####
2
Fro
σ2
+ M log det (CA)
det
ΣA
 + L log det (CB)
det
ΣB
 
−(L + M)H + tr
2
C−1
A
!
A
⊤A + MΣA
"3
+ tr
2
C−1
B
!
B
⊤B + LΣB
"3
+ σ−2tr
2
−A
⊤AB
⊤B +
!
A
⊤A + MΣA
" !
B
⊤B + LΣB
"3
.
(6.8)

152
6 Global VB Solution of Fully Observed Matrix Factorization
The stationary conditions for the variational parameters are given by
A = σ−2V⊤BΣA,
(6.9)
ΣA = σ2 !
B
⊤B + LΣB + σ2C−1
A
"−1
,
(6.10)
B = σ−2VAΣB,
(6.11)
ΣB = σ2 !
A
⊤A + MΣA + σ2C−1
B
"−1
.
(6.12)
In the subsequent sections, we derive the global solution to the problem
(6.5) in an analytic form, which was obtained in Nakajima et al. (2013a, 2015).
6.2 Conditions for VB Solutions
With the explicit form (6.8) of the free energy, the VB learning problem (6.5)
can be written as a minimization problem with respect to a ﬁnite number of
variables:
Given
CA, CA ∈DH
++,
σ2 ∈R++,
min
A,B,ΣA,ΣB
F
(6.13)
s.t.
A ∈RM×H, B ∈RL×H,
ΣA, ΣB ∈SH
++.
(6.14)
We can easily show that the solution is a stationary point of the free energy.
Lemma 6.1
Any local solution of the problem (6.13) is a stationary point of
the free energy (6.8).
Proof
Since
####V −BA
⊤####
2
Fro ≥0,
and
tr
2
−A
⊤AB
⊤B +
!
A
⊤A + MΣA
" !
B
⊤B + LΣB
"3
= tr
2
LA
⊤AΣB + MB
⊤BΣA + LMΣAΣB
3
≥0,
the free energy (6.8) is lower-bounded as
2F ≥−M log det
ΣA
 
−L log det
ΣB
 
+ tr
2
C−1
A
!
A
⊤A + MΣA
"3
+ tr
2
C−1
B
!
B
⊤B + LΣB
"3
+ τ,
(6.15)

6.3 Irrelevant Degrees of Freedom
153
where τ is a ﬁnite constant. The right-hand side of Eq. (6.15) diverges to +∞
if any entry of A or B goes to +∞or −∞. Also it diverges if any eigenvalue
of ΣA or ΣB goes to +0 or ∞. This implies that that no local solution exists
on the boundary of (the closure of) the domain (6.14). Since the free energy is
differentiable in the domain (6.14), any local minimizer is a stationary point.
For any observed matrix V, the free energy (6.8) can be ﬁnite, for example,
at A = 0M,H, B = 0L,H, and ΣA = ΣB = IH, where 0D1,D2 denotes the D1 × D2
matrix with all the entries equal to zero. Therefore, at least one minimizer
always exists, which completes the proof of Lemma 6.1.
□
Lemma 6.1 implies that the stationary conditions (6.9) through (6.12) are
satisﬁed for any solution. Accordingly, we can obtain the global solution by
ﬁnding all points that satisfy the stationary conditions. However, the condition
involves O(MH) unknowns, and therefore ﬁnding all such candidate points
seems hard. The ﬁrst step to tackle this problem is to ﬁnd hidden separability,
which enables us to decompose the problem so that each problem involves only
O(1) unknowns.
6.3 Irrelevant Degrees of Freedom
The most of the terms in the free energy (6.8) have symmetry, i.e., they are
invariant with respect to the coordinate change shown in Eqs. (6.16) and (6.17).
Assume that (A∗, B∗, Σ∗
A, Σ∗
B) is a global solution of the VB problem (6.13),
and let F∗= F(A∗, B∗, Σ∗
A, Σ∗
B) be the minimum free energy. Consider the
following rotation of the coordinate system for an arbitrary orthogonal matrix
Ω ∈RH×H:
A = A∗Ω⊤,
ΣA = ΩΣ∗
AΩ⊤,
(6.16)
B = B∗Ω⊤,
ΣB = ΩΣ∗
BΩ⊤.
(6.17)
We can easily conﬁrm that the terms in Eq. (6.8) except the sixth and the
seventh terms are invariant with respect to the rotation, and the free energy
can be written as a function of Ω as follows:
2F(Ω) = tr
2
C−1
A Ω
!
A
⊤A + MΣA
"
Ω⊤3
+ tr
2
C−1
B Ω
!
B
⊤B + LΣB
"
Ω⊤3
+ const.
To ﬁnd the irrelevant degrees of freedom, we consider skewed rotations that
only affect a single term in Eq. (6.8).
Consider the following transform:
A = A∗C−1/2
A
Ω⊤C1/2
A ,
Σ A = C1/2
A ΩC−1/2
A
Σ∗
AC−1/2
A
Ω⊤C1/2
A ,
(6.18)

154
6 Global VB Solution of Fully Observed Matrix Factorization
B = B∗C1/2
A Ω⊤C−1/2
A
,
ΣB = C−1/2
A
ΩC1/2
A Σ∗
BC1/2
A Ω⊤C−1/2
A
.
(6.19)
Then, the free energy can be written as
2F(Ω) = tr
,
ΓΩΦΩ⊤-
+ const.,
(6.20)
where
Γ = C−1
A C−1
B ,
Φ = C1/2
A

B∗⊤B∗+ LΣ∗
B
 
C1/2
A .
By assumption, Ω = IH is a minimizer of Eq. (6.20), i.e., F(IH) = F∗. Now
we can use the following lemma:
Lemma 6.2
Let Γ, Ω, Φ ∈RH×H be a nondegenerate diagonal matrix, an
orthogonal matrix, and a symmetric matrix, respectively. Let {Λ(k), Λ′(k) ∈
RH×H; k = 1,. . . , K} be arbitrary diagonal matrices. If a function
G(Ω) = tr
⎧⎪⎪⎨⎪⎪⎩ΓΩΦΩ⊤+
K

k=1
Λ(k)ΩΛ′(k)Ω⊤
⎫⎪⎪⎬⎪⎪⎭
(6.21)
is minimized (as a function of Ω, given Γ, Φ, {Λ(k), Λ′(k)}) at Ω = IH, then Φ is
diagonal. Here, K can be any natural number including K = 0 (when only the
ﬁrst term exists).
Proof
Let
Φ = Ω′Γ′Ω′⊤
(6.22)
be the eigenvalue decomposition of Φ. Let γ, γ′, {λ(k)}, {λ′(k)} be the vectors
consist of the diagonal entries of Γ, Γ′, {Λ(k)}, {Λ′(k)}, respectively, i.e.,
Γ = Diag(γ),
Γ′ = Diag(γ′),
Λ(k) = Diag(λ(k)),
Λ′(k) = Diag(λ′(k)).
Then, Eq. (6.21) can be written as
G(Ω) = tr
⎧⎪⎪⎨⎪⎪⎩ΓΩΦΩ⊤+
K

k=1
Λ(k)ΩΛ′(k)Ω⊤
⎫⎪⎪⎬⎪⎪⎭= γ⊤Qγ′ +
K

k=1)
λ(k)⊤Rλ′(k), (6.23)
where
Q = (ΩΩ′) ⊙(ΩΩ′),
R = Ω ⊙Ω.
Here, ⊙denotes the Hadamard product. Since Q as well as R is the Hadamard
square of an orthogonal matrix, it is doubly stochastic (i.e., any of the columns
and the rows sums up to one) (Marshall et al., 2009). Therefore, it can be
seen that Q reassigns the components of γ to those of γ′ when calculating

6.3 Irrelevant Degrees of Freedom
155
the elementwise product in the ﬁrst term of Eq. (6.23). The same applies to
R and {λ(k), λ′(k)} in the second term. Naturally, rearranging the components
of γ in nondecreasing order and the components of γ′ in nonincreasing order
minimizes γ⊤Qγ′ (Ruhe, 1970; Marshall et al., 2009).
Using the expression (6.23) with Q and R, we will prove that Φ is diagonal
if Ω = IH minimizes Eq. (6.23). Let us consider a bilateral perturbation Ω = Δ
such that the 2 × 2 matrix Δ(h,h′) consisting of the hth and the h′th columns and
rows form an 2 × 2 orthogonal matrix
Δ(h,h′) =
cos θ
−sin θ
sin θ
cos θ

,
and the remaining entries coincide with those of the identity matrix. Then, the
elements of Q become
Qi, j =
⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩
(Ω′
h, j cos θ −Ω′
h′, j sin θ)2
if i = h,
(Ω′
h, j sin θ + Ω′
h′, j cos θ)2
if i = h′,
Ω′2
i, j
otherwise,
and Eq. (6.23) can be written as a function of θ:
G(θ) =
H

j=1
,
γh(Ω′
h, j cos θ −Ω′
h′, j sin θ)2 + γh′(Ω′
h, j sin θ + Ω′
h′, j cos θ)2-
γ′
j
+
K

k=1

λ(k)
h
λ(k)
h′
 cos2 θ
sin2 θ
sin2 θ
cos2 θ
 λ(k)
h
λ(k)
h′

+ const.
(6.24)
Since Eq. (6.24) is differentiable at θ = 0, our assumption that Eq. (6.23) is
minimized when Ω = IH requires that θ = 0 is a stationary point of Eq. (6.24)
for any h  h′. Therefore, it holds that
0 = ∂G
∂θ
θ=0
= 2

j
,
γh(Ω′
h, j cos θ −Ω′
h′, j sin θ)(−Ω′
h, j sin θ −Ω′
h′, j cos θ)
+ γh′(Ω′
h, j sin θ + Ω′
h′, j cos θ)(Ω′
h, j cos θ −Ω′
h′, j sin θ)
-
γ′
j
= 2 (γh′ −γh)

j
Ω′
h, jγ′
jΩ′
h′, j = 2 (γh′ −γh) Φh,h′.
(6.25)
In the last equality, we used Eq. (6.22). Since we assumed that Γ is nonde-
generate (γh  γh′ for h  h′), Eq. (6.25) implies that Φ is diagonal, which
completes the proof of Lemma 6.2.
□
Assume for simplicity that Γ = C−1
A C−1
B is nondegenerate, i.e., no pair of
diagonal entries coincide, in Eq. (6.20). Then, since Eq. (6.20) is minimized

156
6 Global VB Solution of Fully Observed Matrix Factorization
at Ω = IH, Lemma 6.2 implies that Φ = C1/2
A

B∗⊤B∗+ LΣ∗
B
 
C1/2
A
is diagonal.
This means that B∗⊤B∗+LΣ∗
B is diagonal. Thus, the stationary condition (6.10)
implies that Σ∗
A is diagonal. Similarly, we can ﬁnd that Σ∗
B is diaognal, if Γ =
C−1
A C−1
B is nondegenerate.
To generalize the preceding discussion to degenerate cases, we need to
consider an equivalent solution, deﬁned as follows:
Deﬁnition 6.3
(Equivalent solutions) We say that two points (A, B, ΣA, ΣB)
and (A
′, B
′, Σ
′
A, Σ
′
B) are equivalent if both give the same free energy and the
same mean prediction, i.e.,
F(A, B, ΣA, ΣB) = F(A
′, B
′, Σ
′
A, Σ
′
B)
and
BA
⊤= B
′A
′⊤.
With this deﬁnition, we can obtain the following theorem (its proof is given
in the next section):
Theorem 6.4
When CACB is nondegenerate (i.e., cahcbh > cah′ cbh′ for any
pair h < h′), any solution of the problem (6.13) has diagonal ΣA and ΣB. When
CACB is degenerate, any solution has an equivalent solution with diagonal ΣA
and ΣB.
The result that the solution has diagonal ΣA and ΣB would be natural
because we assumed the independent Gaussian priors on A and B: the fact that
any V can be decomposed into orthogonal singular components may imply that
the observation V cannot convey any preference for singular-componentwise
correlation. Note, however, that Theorem 6.4 does not necessarily hold when
the observed matrix has missing entries.
Obviously, any VB solution (a solution of the problem (6.13)) with diagonal
covariances can be found by solving the following problem:
Given
CA, CA ∈DH
++,
σ2 ∈R++,
min
A,B,ΣA,ΣB
F
(6.26)
s.t.
A ∈RM×H, B ∈RL×H,
ΣA, ΣB ∈DH
++,
(6.27)
which is equivalent to solving the SimpleVB learning problem (3.30) with
columnwise independence, introduced in Section 3.1.1. Theorem 6.4 states
that, if CACB is nondegenerate, the set of VB solutions and the set of SimpleVB
solutions are identical. When CACB is degenerate, the set of VB solutions is
the union of the set of SimpleVB solutions and the set of their equivalent
solutions with nondiagonal covariances. Actually, any VB solution can be
obtained by rotating its equivalent SimpleVB solution (VB solution with
diagonal covariances) (see Section 6.4.4). In practice, it is however sufﬁcient

6.4 Proof of Theorem 6.4
157
to focus on the SimpleVB solutions, since equivalent solutions share the same
free energy F and the same mean prediction BA
⊤. In this sense, we can
conclude that the stronger columnwise independence constraint (3.29) does
not degrade approximation accuracy, and the VB solution under the matrixwise
independence (3.4) essentially agrees with the SimpleVB solution.
6.4 Proof of Theorem 6.4
In this section, we prove Theorem 6.4 by considering the following three cases
separately:
Case 1 When no pair of diagonal entries of CACB coincide.
Case 2 When all diagonal entries of CACB coincide.
Case 3 When (not all but) some pairs of diagonal entries of CACB coincide.
We will prove that, in Case 1, ΣA and ΣB are diagonal for any solution
(A, B, ΣA, ΣB), and that, in other cases, any solution has its equivalent solution
with diagonal ΣA and ΣB.
Remember our assumption that the diagonal entries {cahcbh} of CACB are
arranged in nonincreasing order.
6.4.1 Proof for Case 1
Here, we consider the case where cahcbh > cah′cbh′ for any pair h < h′.
Assume that (A∗, B∗, Σ∗
A, Σ∗
B) is a minimizer of the free energy (6.8), and
consider the following variation deﬁned with an arbitrary H × H orthogonal
matrix Ω:
A = A∗C−1/2
A
Ω⊤C1/2
A ,
(6.28)
B = B∗C1/2
A Ω⊤C−1/2
A
,
(6.29)
ΣA = C1/2
A ΩC−1/2
A
Σ∗
AC−1/2
A
Ω⊤C1/2
A ,
(6.30)
ΣB = C−1/2
A
ΩC1/2
A Σ∗
BC1/2
A Ω⊤C−1/2
A
.
(6.31)
Note
that
this
variation
does
not
change
BA
⊤,
and
it
holds
that
(A, B, ΣA, ΣB) = (A∗, B∗, Σ∗
A, Σ∗
B) for Ω = IH. Then, the free energy (6.8)

158
6 Global VB Solution of Fully Observed Matrix Factorization
can be written as a function of Ω:
F(Ω) = 1
2tr
,
C−1
A C−1
B ΩC1/2
A

B∗⊤B∗+ LΣ∗
B
 
C1/2
A Ω⊤-
+ const.
(6.32)
We deﬁne
Φ = C1/2
A

B∗⊤B∗+ LΣ∗
B
 
C1/2
A ,
and rewrite Eq. (6.32) as
F(Ω) = 1
2tr
,
C−1
A C−1
B ΩΦΩ⊤-
+ const.
(6.33)
The assumption that (A∗, B∗, Σ∗
A, Σ∗
B) is a minimizer requires that Eq. (6.33)
is minimized when Ω = IH. Then, Lemma 6.2 (for K = 0) implies that Φ is
diagonal. Therefore,
C−1/2
A
ΦC−1/2
A
(= ΦC−1
A ) = B∗⊤B∗+ LΣ∗
B
is also diagonal. Consequently, Eq. (6.10) implies that Σ∗
A is diagonal.
Next, consider the following variation deﬁned with an arbitrary H × H
orthogonal matrix Ω′:
A = A∗C1/2
B Ω′⊤C−1/2
B
,
B = B∗C−1/2
B
Ω′⊤C1/2
B ,
ΣA = C−1/2
B
Ω′C1/2
B Σ∗
AC1/2
B Ω′⊤C−1/2
B
,
ΣB = C1/2
B Ω′C−1/2
B
Σ∗
BC−1/2
B
Ω′⊤C1/2
B .
Then, the free energy as a function of Ω′ is given by
F(Ω′) = 1
2tr
,
C−1
A C−1
B Ω′C1/2
B

A∗⊤A∗+ MΣ∗
A
 
C1/2
B Ω′⊤-
+ const.
From this, we can similarly prove that Σ∗
B is also diagonal, which completes
the proof for Case 1.
□
6.4.2 Proof for Case 2
Here, we consider the case where CACB = c2IH for some c2 ∈R++. In this case,
there exist solutions with nondiagonal covariances. However, for any (or each)
of those nondiagonal solutions, the equivalent class to which the (nondiagonal)
solution belongs contains a solution with diagonal covariances.
We can easily show that the free energy (6.8) is invariant with respect to
Ω under the transformation (6.28) through (6.31). This arbitrariness forms an
equivalent class of solutions. Since there exists Ω that diagonalizes any given
Σ∗
A through Eq. (6.30), each equivalent class involves a solution with diagonal

6.4 Proof of Theorem 6.4
159
ΣA. In the following, we will prove that any solution with diagonal ΣA has
diagonal ΣB.
Assume that (A∗, B∗, Σ∗
A, Σ∗
B) is a solution with diagonal Σ∗
A, and consider
the following variation deﬁned with an arbitrary H × H orthogonal matrix Ω:
A = A∗C−1/2
A
Γ−1/2Ω⊤Γ1/2C1/2
A ,
B = B∗C1/2
A Γ1/2Ω⊤Γ−1/2C−1/2
A
,
ΣA = C1/2
A Γ1/2ΩΓ−1/2C−1/2
A
Σ∗
AC−1/2
A
Γ−1/2Ω⊤Γ1/2C1/2
A ,
ΣB = C−1/2
A
Γ−1/2ΩΓ1/2C1/2
A Σ∗
BC1/2
A Γ1/2Ω⊤Γ−1/2C−1/2
A
.
Here, Γ = Diag(γ1,. . . , γH) is an arbitrary nondegenerate (γh  γh′ for h  h′)
positive-deﬁnite diagonal matrix. Then, the free energy can be written as a
function of Ω:
F(Ω) = 1
2tr
,
ΓΩΓ−1/2C−1/2
A

A∗⊤A∗+ MΣ∗
A
 
C−1/2
A
Γ−1/2Ω⊤
+ c−2Γ−1ΩΓ1/2C1/2
A

B∗⊤B∗+ LΣ∗
B
 
C1/2
A Γ1/2Ω⊤-
.
(6.34)
We deﬁne
ΦA = Γ−1/2C−1/2
A

A∗⊤A∗+ MΣ∗
A
 
C−1/2
A
Γ−1/2,
ΦB = c−2Γ1/2C1/2
A

B∗⊤B∗+ LΣ∗
B
 
C1/2
A Γ1/2,
and rewrite Eq. (6.34) as
F(Ω) = 1
2tr
,
ΓΩΦAΩ⊤+ Γ−1ΩΦBΩ⊤-
.
(6.35)
Since Σ∗
A is diagonal, Eq. (6.10) implies that ΦB is diagonal. The assump-
tion that (A∗, B∗, Σ∗
A, Σ∗
B) is a solution requires that Eq. (6.35) is minimized
when Ω
=
IH. Accordingly, Lemma 6.2 implies that ΦA is diagonal.
Consequently, Eq. (6.12) implies that Σ∗
B is diagonal.
Thus, we have proved that any solution has its equivalent solution with
diagonal covariances, which completes the proof for Case 2.
□
6.4.3 Proof for Case 3
Finally, we consider the case where cahcbh = cah′ cbh′ for (not all but) some
pairs h  h′. First, in the same way as Case 1, we can prove that ΣA and ΣB
are block diagonal where the blocks correspond to the groups sharing the same
cahcbh. Next, we can apply the proof for Case 2 to each block, and show that
any solution has its equivalent solution with diagonal ΣA and ΣB. Combining
these results completes the proof of Theorem 6.4.
□

160
6 Global VB Solution of Fully Observed Matrix Factorization
6.4.4 General Expression
In summary, for any minimizer of Eq. (6.8), the covariances can be written in
the following form:
ΣA = C1/2
A ΘC−1/2
A
ΓΣAC−1/2
A
Θ⊤C1/2
A (= C−1/2
B
ΘC1/2
B ΓΣAC1/2
B Θ⊤C−1/2
B
),
(6.36)
ΣB = C−1/2
A
ΘC1/2
A ΓΣBC1/2
A Θ⊤C−1/2
A
(= C1/2
B ΘC−1/2
B
ΓΣBC−1/2
B
Θ⊤C1/2
B ).
(6.37)
Here, ΓΣA and ΓΣB are positive-deﬁnite diagonal matrices, and Θ is a block
diagonal matrix such that the blocks correspond to the groups sharing the same
cahcbh, and each block consists of an orthogonal matrix. Furthermore, if there
exists a solution with (ΣA, ΣB) written in the form of Eqs. (6.36) and (6.37)
with a certain set of (ΓΣA, ΓΣB, Θ), then there also exist its equivalent solutions
with the same (ΓΣA, ΓΣB) for any Θ. Focusing on the solution with Θ = IH as
the representative of each equivalent class, we can assume that ΣA and ΣB are
diagonal without loss of generality.
6.5 Problem Decomposition
As discussed in Section 6.3, Theorem 6.4 allows us to focus on the solutions
that have diagonal posterior covariances, i.e., ΣA, ΣB ∈DH
++. For any solution
with diagonal covariances, the stationary conditions (6.10) and (6.12) (with
Lemma 6.1) imply that A
⊤A and B
⊤B are also diagonal, which means that the
column vectors of A, as well as B, are orthogonal to each other. In such a case,
the free energy (6.8) depends on the column vectors of A and B only through
the second term
σ−2 ####V −BA
⊤####
2
Fro ,
which coincides with the objective function for the singular value decomposi-
tion (SVD). This leads to the following lemma:
Lemma 6.5
Let
V =
L

h=1
γhωbhω⊤
ah
(6.38)
be the SVD of V, where γh (≥0) is the hth largest singular value, and ωah and
ωbh are the associated right and left singular vectors. Then, any VB solution

6.5 Problem Decomposition
161
(with diagonal posterior covariances) can be written as
BA
⊤=
H

h=1
γVB
h ωbhω⊤
ah
(6.39)
for some {γVB
h
≥0}.
Thanks to Theorem 6.4 and Lemma 6.5, the variational parameters
A = (a1,. . . ,aH), B = (b1,. . . ,bH), ΣA, ΣB can be expressed as
ah = ahωah,
bh = bhωbh,
ΣA = Diag

σ2
a1,. . . , σ2
aH
 
,
ΣB = Diag

σ2
b1,. . . , σ2
bH
 
,
with a new set of unknowns {ah,bh ∈R, σ2
ah, σ2
bh ∈R++}H
h=1. Thus, the
following holds:
Corollary 6.6
The VB posterior can be written as
r(A, B) =
H

h=1
GaussM(ah;ahωah, σ2
ahIM)
H

h=1
GaussL(bh;bhωbh, σ2
bh IL),
(6.40)
where {ah,bh, σ2
ah, σ2
bh}H
h=1 are the solution of the following minimization
problem:
Given
σ2 ∈R++,
{c2
ah, c2
bh ∈R++}H
h=1,
min
{ah,bh,σ2ah,σ2
bh}H
h=1
F
(6.41)
s.t.
{ah,bh ∈R,
σ2
ah, σ2
bh ∈R++}H
h=1.
Here, F is the free energy (6.8), which can be written as
2F = LM log(2πσ2) +
L
h=1 γ2
h
σ2
+
H

h=1
2Fh,
(6.42)
where
2Fh = M log
c2
ah
σ2ah
+ L log
c2
bh
σ2
bh
+
a2
h + Mσ2
ah
c2ah
+
b2
h + Lσ2
bh
c2
bh
−(L + M) +
−2ahbhγh +

a2
h + Mσ2
ah
 b2
h + Lσ2
bh
 
σ2
.
(6.43)

162
6 Global VB Solution of Fully Observed Matrix Factorization
Importantly, the free energy (6.42) depends on the variational parameters
{ah,bh, σ2
ah, σ2
bh}H
h=1 only through the third term, and the third term is decom-
posed into H terms, each of which only depends on the variational parameters
(ah,bh, σ2
ah, σ2
bh) for the hth singular component. Accordingly, given the noise
variance σ2, we can separately minimize the free energy (6.43), which involves
only four unknowns, for each singular component.
6.6 Analytic Form of Global VB Solution
The stationary conditions of Eq. (6.43) are given by
ah =
σ2
ah
σ2 γhbh,
(6.44)
σ2
ah = σ2

b2
h + Lσ2
bh + σ2
c2ah
−1
,
(6.45)
bh =
σ2
bh
σ2 γhah,
(6.46)
σ2
bh = σ2
⎛⎜⎜⎜⎜⎜⎝a2
h + Mσ2
ah + σ2
c2
bh
⎞⎟⎟⎟⎟⎟⎠
−1
,
(6.47)
which form is a polynomial system, a set of polynomial equations, on the four
unknowns (ah,bh, σ2
ah, σ2
bh). Since Lemma 6.1 guarantees that any minimizer
is a stationary point, we can obtain the global solution by ﬁnding all points that
satisfy the stationary conditions (6.44) through (6.47) and comparing the free
energy (6.43) at those points.
This leads to the following theorem and corollary:
Theorem 6.7
The VB solution is given by
U
VB =
H

h=1
γVB
h ωbhω⊤
ah,
where
γVB
h
=
⎧⎪⎪⎨⎪⎪⎩
˘γVB
h
if γh ≥γVB
h ,
0
otherwise,
(6.48)
for
γVB
h
= σ
>
?
?
?
@
(L + M)
2
+
σ2
2c2ahc2
bh
+
>
?
@⎛⎜⎜⎜⎜⎜⎝
(L + M)
2
+
σ2
2c2ahc2
bh
⎞⎟⎟⎟⎟⎟⎠
2
−LM,
(6.49)
˘γVB
h
= γh
⎛⎜⎜⎜⎜⎜⎜⎜⎝1 −σ2
2γ2
h
⎛⎜⎜⎜⎜⎜⎜⎜⎝M + L +
>
@
(M −L)2 +
4γ2
h
c2ahc2
bh
⎞⎟⎟⎟⎟⎟⎟⎟⎠
⎞⎟⎟⎟⎟⎟⎟⎟⎠.
(6.50)

6.7 Proofs of Theorem 6.7 and Corollary 6.8
163
Corollary 6.8
The VB posterior is given by Eq. (6.40) with the following
variational parameters: if γh > γVB
h ,
ah = ±
.
˘γVB
h δVB
h ,
bh = ±
>
@
˘γVB
h
δVB
h
,
σ2
ah = σ2δVB
h
γh
,
σ2
bh =
σ2
γhδVB
h
, (6.51)
where
δVB
h

≡ah
bh

= cah
σ2

γh −˘γVB
h
−Lσ2
γh

,
(6.52)
and otherwise,
ah = 0,
bh = 0,
σ2
ah = c2
ah
⎛⎜⎜⎜⎜⎜⎝1 −LζVB
h
σ2
⎞⎟⎟⎟⎟⎟⎠,
σ2
bh = c2
bh
⎛⎜⎜⎜⎜⎜⎝1 −MζVB
h
σ2
⎞⎟⎟⎟⎟⎟⎠, (6.53)
where
ζVB
h

≡σ2
ahσ2
bh
 
=
σ2
2LM
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝L + M +
σ2
c2ahc2
bh
−
>
?
@⎛⎜⎜⎜⎜⎜⎝L + M +
σ2
c2ahc2
bh
⎞⎟⎟⎟⎟⎟⎠
2
−4LM
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠.
(6.54)
Theorem 6.7 states that the VB solution for fully observed MF is a truncated
shrinkage SVD with the truncation threshold and the shrinkage estimator given
by Eqs. (6.49) and (6.50), respectively. Corollary 6.8 completely speciﬁes the
VB posterior.1
These results give insights into the behavior of VB learning; for example,
they explain why a sparse solution is obtained, and what are similarities and
differences between the Bayes posterior and the VB posterior, which will be
discussed in Chapter 7. The results also form the basis of further analysis on the
global empirical VB solution (Section 6.8), which will be used for performance
guarantee (Chapter 8), and global (or efﬁcient local) solvers for multilinear
models (Chapters 9, 10, and 11). Before moving on, we give the proofs of the
theorem and the corollary in the next section.
6.7 Proofs of Theorem 6.7 and Corollary 6.8
We will ﬁnd all stationary points that satisfy Eqs. (6.44) through (6.47), and
compare the free energy (6.43).
1 The similarity between (γVB
h )2 and LMζVB
h
comes from the fact that they are the two different
solutions of the same quadratic equations, i.e., Eq. (6.79) with respect to (γVB
h )2 and (6.77) with
respect to LMζVB
h .

164
6 Global VB Solution of Fully Observed Matrix Factorization
By using Eqs. (6.45) and (6.47), the free energy (6.43) can be simpliﬁed as
Fh = M log
c2
ah
σ2ah
+ L log
c2
bh
σ2
bh
+ 1
σ2
⎛⎜⎜⎜⎜⎜⎝a2
h + Mσ2
ah + σ2
c2
bh
⎞⎟⎟⎟⎟⎟⎠

b2
h + Lσ2
bh + σ2
c2ah

−(L + M) + −2ahbhγh
σ2
−
σ2
c2ahc2
bh
= M log
c2
ah
σ2ah
+ L log
c2
bh
σ2
bh
+
σ2
σ2ahσ2
bh
−2ahbhγh
σ2
−
⎛⎜⎜⎜⎜⎜⎝L + M +
σ2
c2ahc2
bh
⎞⎟⎟⎟⎟⎟⎠.
(6.55)
The stationary conditions (6.44) through (6.47) imply two possibilities of
stationary points.
6.7.1 Null Stationary Point
If ah = 0 or bh = 0, Eqs. (6.44) and (6.46) require that ah = 0 and bh = 0. In
this case, Eqs. (6.45) and (6.47) lead to
σ2
ah = c2
ah
⎛⎜⎜⎜⎜⎜⎝1 −
Lσ2
ahσ2
bh
σ2
⎞⎟⎟⎟⎟⎟⎠,
(6.56)
σ2
bh = c2
bh
⎛⎜⎜⎜⎜⎜⎝1 −
Mσ2
ahσ2
bh
σ2
⎞⎟⎟⎟⎟⎟⎠.
(6.57)
Multiplying Eqs. (6.56) and (6.57), we have
⎛⎜⎜⎜⎜⎜⎝1 −
Lσ2
ahσ2
bh
σ2
⎞⎟⎟⎟⎟⎟⎠
⎛⎜⎜⎜⎜⎜⎝1 −
Mσ2
ahσ2
bh
σ2
⎞⎟⎟⎟⎟⎟⎠=
σ2
ahσ2
bh
c2ahc2
bh
,
(6.58)
and therefore
LM
σ2 σ4
ahσ4
bh −
⎛⎜⎜⎜⎜⎜⎝L + M +
σ2
c2ahc2
bh
⎞⎟⎟⎟⎟⎟⎠σ2
ahσ2
bh + σ2 = 0.
(6.59)
Solving the quadratic equation (6.59) with respect to σ2
ahσ2
bh and checking the
signs of σ2
ah and σ2
bh, we have the following lemma:
Lemma 6.9
For any γh ≥0 and c2
ah, c2
bh, σ2 ∈R++, the null stationary point
given by Eq. (6.53) exists with the following free energy:
FVB−Null
h
= −M log
!
1 −L
σ2ζVB
h
"
−L log
!
1 −M
σ2ζVB
h
"
−LM
σ2 ζVB
h ,
(6.60)
where ζVB
h
is deﬁned by Eq. (6.54).

6.7 Proofs of Theorem 6.7 and Corollary 6.8
165
Proof
Eq. (6.59) has two positive real solutions:
σ2
ahσ2
bh =
σ2
2LM
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝L + M +
σ2
c2ahc2
bh
±
>
?
@⎛⎜⎜⎜⎜⎜⎝L + M +
σ2
c2ahc2
bh
⎞⎟⎟⎟⎟⎟⎠
2
−4LM
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠.
The larger solution (with the plus sign) is decreasing with respect to c2
ahc2
bh, and
lower-bounded as σ2
ahσ2
bh > σ2/L. The smaller solution (with the minus sign)
is increasing with respect to c2
ahc2
bh, and upper-bounded as σ2
ahσ2
bh < σ2/M.
For σ2
ah and σ2
bh to be positive, Eqs. (6.56) and (6.57) require that
σ2
ahσ2
bh < σ2
M ,
(6.61)
which is violated by the larger solution, while satisﬁed by the smaller solution.
With the smaller solution (6.54), Eqs. (6.56) and (6.57) give the stationary
point given by Eq. (6.53).
Using Eq. (6.59), we can easily derive Eq. (6.60) from Eq. (6.55), which
completes the proof of Lemma 6.9.
□
6.7.2 Positive Stationary Point
Assume that ah,bh  0. In this case, Eqs. (6.44) and (6.46) imply that ah and
bh have the same sign. Deﬁne
γh = ahbh > 0,
(6.62)
δh = ah
bh
> 0.
(6.63)
From Eqs. (6.44) and (6.46), we have
σ2
ah = σ2
γh
δh,
(6.64)
σ2
bh = σ2
γh
δ−1
h .
(6.65)
Substituting Eqs. (6.64) and (6.65) into Eqs. (6.45) and (6.47) gives
γhδ−1
h =

γhδ−1
h + Lσ2
γh
δ−1
h + σ2
c2ah

,
(6.66)
γhδh =
⎛⎜⎜⎜⎜⎜⎝γhδh + M σ2
γh
δh + σ2
c2
bh
⎞⎟⎟⎟⎟⎟⎠,
(6.67)

166
6 Global VB Solution of Fully Observed Matrix Factorization
and therefore,
δh = cah
σ2

γh −γh −Lσ2
γh

,
(6.68)
δ−1
h = cbh
σ2

γh −γh −Mσ2
γh

.
(6.69)
Multiplying Eqs. (6.68) and (6.69), we have

γh −γh −Lσ2
γh
 
γh −γh −Mσ2
γh

=
σ4
cahcbh
,
(6.70)
and therefore
γ2
h −

2γh −(L + M)σ2
γh

γh +

γh −Lσ2
γh
 
γh −Mσ2
γh

−
σ4
cahcbh
= 0. (6.71)
By solving the quadratic equation (6.71) with respect to γh, and checking
the signs of γh,δh, σ2
ah, and σ2
bh, we have the following lemma:
Lemma 6.10
If and only if γh > γVB
h , where γVB
h
is deﬁned by Eq. (6.49),
the positive stationary point given by Eq. (6.51) exists with the following free
energy:
FVB−Posi
h
= −M log
⎛⎜⎜⎜⎜⎝1 −
⎛⎜⎜⎜⎜⎝
˘γVB
h
γh
+ Lσ2
γ2
h
⎞⎟⎟⎟⎟⎠
⎞⎟⎟⎟⎟⎠−L log
⎛⎜⎜⎜⎜⎝1 −
⎛⎜⎜⎜⎜⎝
˘γVB
h
γh
+ Mσ2
γ2
h
⎞⎟⎟⎟⎟⎠
⎞⎟⎟⎟⎟⎠
−γ2
h
σ2
⎛⎜⎜⎜⎜⎝
˘γVB
h
γh
+ Lσ2
γ2
h
⎞⎟⎟⎟⎟⎠
⎛⎜⎜⎜⎜⎝
˘γVB
h
γh
+ Mσ2
γ2
h
⎞⎟⎟⎟⎟⎠,
(6.72)
where ˘γVB
h
is deﬁned by Eq. (6.50).
Proof
Since δh > 0, Eqs. (6.68) and (6.69) require that
γh < γh −Lσ2
γh
,
(6.73)
and therefore, the positive stationary point exists only when
γh >
√
Mσ.
(6.74)
Let us assume that Eq. (6.74) holds.
Eq. (6.71) has two solutions:
γh = 1
2
⎛⎜⎜⎜⎜⎜⎜⎜⎝2γh −(L + M)σ2
γh
±
>
@(M −L)σ2
γh
2
+ 4σ4
c2ahc2
bh
⎞⎟⎟⎟⎟⎟⎟⎟⎠.

6.7 Proofs of Theorem 6.7 and Corollary 6.8
167
The larger solution with the plus sign is positive, decreasing with respect to
c2
ahc2
bh, and lower-bounded as γh > γh −Lσ2/γh, which violates the condition
(6.73).
The smaller solution, Eq. (6.50), with the minus sign is positive if the
intercept of the left-hand side in Eq. (6.71) is positive, i.e.,

γh −Lσ2
γh
 
γh −Mσ2
γh

−
σ4
c2ahc2
bh
> 0.
(6.75)
From the condtion (6.75), we obtain the threshold (6.49) for the existence of
the positive stationary point. Note that γVB
h
>
√
Mσ, and therefore, Eq. (6.74)
holds whenever γh > γVB
h .
Assume that γh > γVB
h . Then, with the solution (6.50), δh, given by
Eq. (6.68), and σ2
ah and σ2
bh, given by Eqs. (6.64) and (6.65), are all positive.
Thus, we obtain the positive stationary point (6.51).
Substituting Eqs. (6.64) and (6.65), and then Eqs. (6.68) and (6.69), into the
free energy (6.55), we have
FVB−Posi
h
= −M log
⎛⎜⎜⎜⎜⎝1 −˘γVB
h
γh
−Lσ2
γ2
h
⎞⎟⎟⎟⎟⎠−L log
⎛⎜⎜⎜⎜⎝1 −˘γVB
h
γh
−Mσ2
γ2
h
⎞⎟⎟⎟⎟⎠
+ −2γh ˘γVB
h
σ2
+ γ2
h
σ2 −
⎛⎜⎜⎜⎜⎜⎝L + M +
σ2
c2ahc2
bh
⎞⎟⎟⎟⎟⎟⎠.
(6.76)
Using Eq. (6.70), we can eliminate the direct dependency on c2
ahc2
bh, and express
the free energy (6.76) as a function of ˘γVB
h . This results in Eq. (6.72), and
completes the proof of Lemma 6.10.
□
6.7.3 Useful Relations
Let us summarize some useful relations between variables, which are used in
the subsequent sections. ζVB
h , ˘γVB
h , and γVB
h , derived from Eqs. (6.58), (6.70),
and the constant part of Eq. (6.71), respectively, satisfy the following:
⎛⎜⎜⎜⎜⎜⎝1 −LζVB
h
σ2
⎞⎟⎟⎟⎟⎟⎠
⎛⎜⎜⎜⎜⎜⎝1 −MζVB
h
σ2
⎞⎟⎟⎟⎟⎟⎠−
ζVB
h
c2ahc2
bh
= 0,
(6.77)

γh −˘γVB
h
−Lσ2
γh
 
γh −˘γVB
h
−Mσ2
γh

−
σ4
cahcbh
= 0,
(6.78)
⎛⎜⎜⎜⎜⎜⎝γVB
h
−Lσ2
γVB
h
⎞⎟⎟⎟⎟⎟⎠
⎛⎜⎜⎜⎜⎜⎝γVB
h
−Mσ2
γVB
h
⎞⎟⎟⎟⎟⎟⎠−
σ4
cahcbh
= 0.
(6.79)

168
6 Global VB Solution of Fully Observed Matrix Factorization
From Eqs. (6.54) and (6.49), we ﬁnd that
γVB
h
=
>
@⎛⎜⎜⎜⎜⎜⎝(L + M)σ2 +
σ4
c2ahc2
bh
⎞⎟⎟⎟⎟⎟⎠−LMζVB
h ,
(6.80)
which is useful when comparing the free energies of the null and the positive
stationary points.
6.7.4 Free Energy Comparison
Lemmas 6.9 and 6.10 imply that, when γh ≤γVB
h , the null stationary point
is only the stationary point, and therefore the global solution. When γh >
γVB
h , both of the null and the positive stationary points exist, and therefore
identifying the global solution requires us to compare their free energies, given
by Eqs. (6.60) and (6.72).
Given the observed singular value γh ≥0, we view the free energy as a
function of c2
ahc2
bh. We also view the threshold γVB
h
as a function of c2
ahc2
bh.
We ﬁnd from Eq. (6.49) that γVB
h
is decreasing and lower-bounded by γVB
h
>
√
Mσ. Therefore, when γh ≤
√
Mσ, γVB
h
never gets smaller than γh for any
c2
ahc2
bh > 0. When γh >
√
Mσ, on the other hand, there is a threshold cahcbh
such that γh > γVB
h
if c2
ahc2
bh > cahcbh. Eq. (6.79) implies that the threshold is
given by
cahcbh =
σ4
γ2
h
!
1 −Lσ2
γ2
h
" !
1 −Mσ2
γ2
h
".
(6.81)
We have the following lemma:
Lemma 6.11
For any γh ≥0 and c2
ahc2
bh > 0, the derivative of the free energy
(6.60) at the null stationary point with respect to c2
ahc2
bh is given by
∂FVB−Null
h
∂c2ahc2
bh
= LMζVB
h
σ2c2ahc2
bh
.
(6.82)
For γh > M/σ2 and c2
ahc2
bh > cahcbh, the derivative of the free energy (6.72) at
the positive stationary point with respect to c2
ahc2
bh is given by

6.7 Proofs of Theorem 6.7 and Corollary 6.8
169
∂FVB−Posi
h
∂c2ahc2
bh
=
γ2
h
σ2c2ahc2
bh
⎛⎜⎜⎜⎜⎝
(˘γVB
h )2
γ2
h
−
⎛⎜⎜⎜⎜⎝1 −(L + M)σ2
γ2
h
⎞⎟⎟⎟⎟⎠
˘γVB
h
γh
+ LMσ4
γ4
h
⎞⎟⎟⎟⎟⎠.
(6.83)
The derivative of the difference is negative, i.e.,
∂(FVB−Posi
h
−FVB−Null
h
)
∂c2ahc2
bh
= −
1
σ2c2ahc2
bh

γh

γh −˘γVB
h
 
−(γVB
h )2 
< 0.
(6.84)
Proof
By differentiating Eqs. (6.60), (6.54), (6.72), and (6.50), we have
∂FVB−Null
h
∂ζVB
h
=
LM
σ2 
1 −L
σ2ζVB
h
 +
LM
σ2 
1 −M
σ2ζVB
h
 −LM
σ2
=
LMc2
ahc2
bh
!
1 +
√
LM
σ2 ζVB
h
" !
1 −
√
LM
σ2 ζVB
h
"
σ2ζVB
h
,
(6.85)
∂ζVB
h
∂c2ahc2
bh
=
σ2
2LM
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
−σ2
c4ahc4
bh
+
2σ2

L + M +
σ2
c2ahc2
bh

2c4ahc4
bh
A
L + M +
σ2
c2ahc2
bh
2
−4LM
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
=
1
c4ahc4
bh
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
(ζVB
h )2
!
1 −
√
LMζVB
h
σ2
" !
1 +
√
LMζVB
h
σ2
"
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
,
(6.86)
∂FVB−Posi
h
∂˘γVB
h
=
M
γh
!
1 −
!
˘γVB
h
γh + Lσ2
γ2
h
"" +
L
γh
!
1 −
!
˘γVB
h
γh + Mσ2
γ2
h
""
−γh
σ2
⎛⎜⎜⎜⎜⎝
2˘γVB
h
γh
+ (L + M)σ2
γ2
h
⎞⎟⎟⎟⎟⎠
=
2c2
ahc2
bhγ3
h
!
1 −
!
˘γVB
h
γh + (L+M)σ2
2γ2
h
"" !
(˘γVB
h )2
γ2
h
−
!
1 −(L+M)σ2
γ2
h
"
˘γVB
h
γh + LMσ4
γ4
h
"
σ6
,
(6.87)
∂γh
∂c2ahc2
bh
=
4γ2
hσ2
4γhc4ahc4
bh
B
(M −L)2 +
4γ2
h
c2ahc2
bh
=
σ4
2γhc4ahc4
bh
!
1 −
!
˘γVB
h
γh + (L+M)σ2
2γ2
h
"".
(6.88)

170
6 Global VB Solution of Fully Observed Matrix Factorization
Here, we used Eqs. (6.54) and (6.77) to obtain Eqs. (6.85) and (6.86), and Eqs.
(6.50) and (6.78) to obtain Eqs. (6.87) and (6.88), respectively. Eq. (6.82) is
obtained by multiplying Eqs. (6.85) and (6.86), while Eq. (6.83) is obtained by
multiplying Eqs. (6.87) and (6.88).
Taking the difference between the derivatives (6.82) and (6.83), and then
using Eqs. (6.78) and (6.80), we have
∂(FVB−Posi
h
−FVB−Null
h
)
∂c2ahc2
bh
= ∂FVB−Posi
h
∂c2ahc2
bh
−∂FVB−Null
h
∂c2ahc2
bh
= −
1
σ2c2ahc2
bh

γh
$γh −γh
% −(γVB
h )2 
.
(6.89)
The following can be obtained from Eqs. (6.78) and (6.79), respectively:

γh(γh −˘γVB
h ) −(L + M)σ2
2
2
= (L + M)2σ4
4
−LMσ4 +
σ4
c2ahc2
bh
γ2
h,
(6.90)

(γVB
h )2 −(L + M)σ2
2
2
= (L + M)2σ4
4
−LMσ4 +
σ4
c2ahc2
bh
(γVB
h )2.
(6.91)
Eqs. (6.90) and (6.91) imply that
γh(γh −˘γVB
h ) > (γVB
h )2
when
γh > γVB
h .
Therefore, Eq. (6.89) is negative, which completes the proof of Lemma 6.11.
□
It is easy to show that the null stationary point (6.53) and the positive
stationary point (6.51) coincide with each other at c2
ahc2
bh →cahcbh + 0. Here,
+0 means that it approaches to zero from the positive side. Therefore,
lim
c2ahc2
bh→cahcbh+0

FVB−Posi
h
−FVB−Null
h
 
= 0.
(6.92)
Eqs. (6.84) and (6.92) together imply that
FVB−Posi
h
−FVB−Null
h
< 0
for
c2
ahc2
bh > cahcbh,
(6.93)
which results in the following lemma:
Lemma 6.12
The positive stationary point is the global solution (the global
minimizer of the free energy (6.43) for ﬁxed cah and cbh) whenever it exists.
Combining Lemmas 6.9, 6.10, and 6.12 completes the proof of Theorem
6.7 and Corollary 6.8.
□
Figure 6.3 illustrates the behavior of the free energies.

6.8 Analytic Form of Global Empirical VB Solution
171
0
0.5
1
1.5
2
2.5
–0.5
0
0.5
1
VB
Null
(a) γh ≤
√
Mσ
0
0.5
1
1.5
2
2.5
–0.5
0
0.5
1
VB
Null
Posi
(b)
√
Mσ < γh ≤(
√
L +
√
M)σ
0
0.5
1
1.5
2
2.5
–0.5
0
0.5
1
VB
Null
Posi
(c) (
√
L +
√
M)σ < γh < γEVB
0
0.5
1
1.5
2
2.5
–0.5
0
0.5
1
VB
Null
Posi
(d) γh ≥γEVB
Figure 6.3 Behavior of the free energies (6.60) and (6.72) at the null and the
positive stationary points as functions of cahcbh, when L = M = H = 1
and σ2 = 1. The curve labeled as “VB” shows the VB free energy Fh =
min(FVB−Null
h
, FVB−Posi
h
) at the global solution, given cahcbh. If γh ≤
√
Mσ,
only the null stationary point exists for any cahcbh > 0. Otherwise, the positive
stationary point exists for cahcbh > cahcbh, and it is the global minimum whenever
it exists. In empirical VB learning where cahcbh is also optimized, cahcbh →0
(indicated by a diamond) is the unique local minimum if γh ≤(
√
L +
√
M)σ.
Otherwise, a positive local minimum also exists (indicated by a cross), and it is the
global minimum if and only if γh ≥γEVB (see Section 6.9 for detailed discussion).
6.8 Analytic Form of Global Empirical VB Solution
In this section, we will solve the empirical VB (EVB) problem where the prior
covariances are also estimated from observation, i.e.,
r = argmin
r,CA,CB
F(r)
s.t.
r(A, B) = rA(A)rB(B).
(6.94)
Since the solution of the EVB problem is a VB solution with some values for
the prior covariances CA, CB, the empirical VB posterior is in the same form as
the VB posterior (6.40). Accordingly, the problem (6.94) can be written with
the variational parameters {ah,bh, σ2
ah, σ2
bh}H
h=1 as follows:

172
6 Global VB Solution of Fully Observed Matrix Factorization
Given
σ2 ∈R++,
min
{ah,bh,σ2ah,σ2
bh,c2ah,c2
bh}H
h=1
F
(6.95)
s.t.
{ah,bh ∈R,
σ2
ah, σ2
bh, c2
ah, c2
bh ∈R++}H
h=1,
(6.96)
where the free energy F is given by Eq. (6.42).
Solving the empirical VB problem (6.95) is not much harder than the
VB problem (6.41) because the objective is still separable into H singular
components when the prior variances {c2
ah, c2
bh} are also optimized. More
speciﬁcally, we can obtain the empirical VB solution by minimizing the
componentwise free energy (6.43) with respect to the only six unknowns
(ah,bh, σ2
ah, σ2
bh, c2
ah, c2
bh) for each hth component. On the other hand, analyzing
the VB estimator for the noise variance σ2 is hard, since Fh for all h = 1,. . . , H
depend on σ2 and therefore the free energy (6.42) is not separable. We
postpone the analysis of this full empirical VB learning to Chapter 8, where
the theoretical performance guarantee is derived.
For the problem (6.95), the stationary points of the free energy (6.43) satisfy
Eqs. (6.44) through (6.47) along with Eqs. (3.26) and (3.27), which are written
with the new set of variational parameters as
c2
ah = a2
h/M + σ2
ah,
(6.97)
c2
bh = b2
h/L + σ2
bh.
(6.98)
However, unlike the VB solution, for which Lemma 6.1 holds, we cannot
assume that the EVB solution is a stationary point, since the free energy Fh
given by Eq. (6.43) does not necessarily diverge to +∞when approaching the
domain boundary (6.96). More speciﬁcally, Fh can converge to a ﬁnite value,
for example, for ah = bh = 0, σ2
ah, σ2
bh, c2
ah, c2
bh →+0. Taking this into account,
we can obtain the following theorem:
Theorem 6.13
Let
α = L
M
(0 < α ≤1),
(6.99)
and let τ = τ(α) be the unique zero-cross point of the following decreasing
function:
Ξ (τ; α) = Φ (τ) + Φ
! τ
α
"
,
where
Φ(z) = log(z + 1)
z
−1
2.
(6.100)
Then, the EVB solution is given by
U
EVB =
H

h=1
γEVB
h
ωbhω⊤
ah,
where
γEVB
h
=
⎧⎪⎪⎨⎪⎪⎩
˘γEVB
h
if γh ≥γEVB,
0
otherwise,
(6.101)

6.9 Proof of Theorem 6.13
173
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
2.5
3
Figure 6.4 Values of τ(α), √α, and z √α.
for
γEVB = σ
A
M

1 + τ
 
1 + α
τ

,
(6.102)
˘γEVB
h
= γh
2
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎝1 −(M + L)σ2
γ2
h
+
>
@⎛⎜⎜⎜⎜⎝1 −(M + L)σ2
γ2
h
⎞⎟⎟⎟⎟⎠
2
−4LMσ4
γ4
h
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎠.
(6.103)
The EVB threshold (6.102) involves τ, which needs to be numerically
computed. However, we can easily prepare a table of the values for 0 < α ≤1
beforehand, like the cumulative Gaussian probability used in statistical tests
(Pearson, 1914). Alternatively, τ ≈z √α is a good approximation, where
z ≈2.5129 is the unique zero-cross point of Φ(z), as seen in Figure 6.4. We
can show that τ lies in the following range (Lemma 6.18 in Section 6.9):
√α < τ < z.
(6.104)
We will see in Chapter 8 that τ is an important quantity in describing the
behavior of the full empirical VB solution where the noise variance σ2 is also
estimated from observation.
In Section 6.9, we give the proof of Theorem 6.13. Then, in Section 6.10,
some corollaries obtained and variables deﬁned in the proof are summarized,
which will be used in Chapter 8.
6.9 Proof of Theorem 6.13
In this section, we prove Theorem 6.13, which provides explicit forms, Eqs.
(6.102) and (6.103), of the EVB threshold γEVB and the EVB shrinkage
estimator ˘γEVB
h
. In fact, we can easily obtain Eq. (6.103) in an intuitive way, by

174
6 Global VB Solution of Fully Observed Matrix Factorization
using some of the results obtained in Section 6.7. After that, by expressing the
free energy Fh with normalized versions of the observation and the estimator,
we derive Eq. (6.102).
6.9.1 EVB Shrinkage Estimator
Eqs. (6.60) and (6.72) imply that the free energy does not depend on the
ratio cah/cbh between the hyperparameters. Accordingly, we ﬁx the ratio to
cah/cbh = 1 without loss of generality. Lemma 6.11 allows us to minimize the
free energy with respect to cahcbh in a straightforward way.
Let us regard the free energies (6.60) and (6.72) at the null and the positive
stationary points as functions of cahcbh (see Figure 6.3). Then, we ﬁnd from
Eq. (6.82) that
∂FVB−Null
h
∂c2ahc2
bh
> 0,
which implies that the free energy (6.60) at the null stationary point is
increasing. Using Lemma 6.9, we thus have the following lemma:
Lemma 6.14
For any given γh ≥0 and σ2 > 0, the null EVB local solution,
given by
ah = 0,
bh = 0,
σ2
ah =
.
ζEVB,
σ2
bh =
.
ζEVB,
cahcbh =
.
ζEVB,
where
ζEVB →+0,
exists, and its free energy is given by
FEVB−Null
h
→+0.
(6.105)
When γh ≥(
√
L +
√
M)σ, the derivative (6.83) of the free energy (6.72) at
the positive stationary point can be further factorized as
∂FVB−Posi
h
∂c2ahc2
bh
=
γh
σ2c2ahc2
bh

˘γVB
h
−´γh
 
˘γVB
h
−˘γEVB
h
 
,
(6.106)
where
´γh = γh
2
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎝1 −(L + M)σ2
γ2
h
−
>
@⎛⎜⎜⎜⎜⎝1 −(L + M)σ2
γ2
h
⎞⎟⎟⎟⎟⎠
2
−4LMσ4
γ4
h
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎠,
(6.107)

6.9 Proof of Theorem 6.13
175
and ˘γEVB
h
is given by Eq. (6.103). The VB shrinkage estimator (6.50) is an
increasing function of cahcbh ranging over
0 < ˘γVB
h
< γh −Mσ2
γh
,
and both of Eqs. (6.107) and (6.103) are in this range, i.e.,
0 < ´γh ≤˘γEVB
h
< γh −Mσ2
γh
.
Therefore Eq. (6.106) leads to the following lemma:
Lemma 6.15
If γh ≤(
√
L +
√
M)σ, the free energy FVB−Posi
h
at the positive
stationary point is monotonically increasing. Otherwise,
FVB−Posi
h
is
⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩
increasing
for
˘γVB
h
< ´γh,
decreasing
for
´γh < ˘γVB
h
< ˘γEVB
h
,
increasing
for
˘γVB
h
> ˘γEVB
h
,
and therefore minimized at ˘γVB
h
= ˘γEVB
h
.
We can see this behavior of the free energy in Figure 6.3.
The derivative (6.83) is zero when ˘γVB
h
= ˘γEVB
h
, which leads to

˘γEVB
h
+ Lσ2
γh
 
˘γEVB
h
+ Mσ2
γh

= γh˘γEVB
h
.
(6.108)
Using Eq. (6.108), we obtain the following lemma:
Lemma 6.16
If and only if
γh ≥γlocal−EVB ≡(
√
L +
√
M)σ,
(6.109)
the positive EVB local solution given by
ah = ±
.
˘γEVB
h
δEVB
h
,
bh = ±
>
@
˘γEVB
h
δEVB
h
,
(6.110)
σ2
ah = σ2δEVB
h
γh
,
σ2
bh =
σ2
γhδEVB
h
,
cahcbh =
A
γh˘γEVB
h
LM
,
(6.111)
exists with the following free energy:
FEVB−Posi
h
= M log
⎛⎜⎜⎜⎜⎝
γh˘γEVB
h
Mσ2
+ 1
⎞⎟⎟⎟⎟⎠+ L log
⎛⎜⎜⎜⎜⎝
γh˘γEVB
h
Lσ2
+ 1
⎞⎟⎟⎟⎟⎠−γh˘γEVB
h
σ2
.
(6.112)

176
6 Global VB Solution of Fully Observed Matrix Factorization
Here,
δEVB
h
=
A
M˘γEVB
h
Lγh
⎛⎜⎜⎜⎜⎝1 +
Lσ2
γh˘γEVB
h
⎞⎟⎟⎟⎟⎠,
(6.113)
and ˘γEVB
h
is given by Eq. (6.103).
Proof
Lemma 6.15 immediately leads to the EVB shrinkage estimator
(6.103). We can ﬁnd the value of cahcbh at the positive EVB local solution
by combining the condition (6.78) for the VB estimator and the condition
(6.108) for the EVB estimator. Speciﬁcally, by using the condition (6.108),
the condition (6.78) for ˘γVB
h
replaced with ˘γEVB
h
can be written as
⎛⎜⎜⎜⎜⎜⎜⎝γh −
γh˘γEVB
h
˘γEVB
h
+ Mσ2
γh
⎞⎟⎟⎟⎟⎟⎟⎠
⎛⎜⎜⎜⎜⎜⎜⎝γh −
γh˘γEVB
h
˘γEVB
h
+ Lσ2
γh
⎞⎟⎟⎟⎟⎟⎟⎠=
σ4
c2ahc2
bh
,
and therefore
⎛⎜⎜⎜⎜⎜⎜⎝
Mσ2
˘γEVB
h
+ Mσ2
γh
⎞⎟⎟⎟⎟⎟⎟⎠
⎛⎜⎜⎜⎜⎜⎜⎝
Lσ2
˘γEVB
h
+ Lσ2
γh
⎞⎟⎟⎟⎟⎟⎟⎠=
σ4
c2ahc2
bh
.
Applying the condition (6.108) again gives
LMσ4
γh˘γEVB
h
=
σ4
c2ahc2
bh
,
which leads to the last equation in Eq. (6.111).
Similarly, using the condition (6.108), Eq. (6.52) for ˘γVB
h
replaced with ˘γEVB
h
is written as
δh =
c2
ah
σ2
⎛⎜⎜⎜⎜⎜⎜⎝γh −
γh˘γEVB
h
˘γEVB
h
+ Mσ2
γh
⎞⎟⎟⎟⎟⎟⎟⎠
=
c2
ah
σ2
⎛⎜⎜⎜⎜⎜⎜⎝
Mσ2
˘γEVB
h
+ Mσ2
γh
⎞⎟⎟⎟⎟⎟⎟⎠
=
c2
ah M
γh
⎛⎜⎜⎜⎜⎝
γh˘γEVB
h
+ Lσ2
γh˘γEVB
h
⎞⎟⎟⎟⎟⎠
=
c2
ah M
γh
⎛⎜⎜⎜⎜⎝1 +
Lσ2
γh˘γEVB
h
⎞⎟⎟⎟⎟⎠.
Using the assumption that cah = cbh and therefore c2
ah = cahcbh, we obtain
Eq. (6.113). Eq. (6.110) and the ﬁrst two equations in Eq. (6.111) are simply
obtained from Lemma 6.10.

6.9 Proof of Theorem 6.13
177
Finally, applying Eq. (6.108) to the free energy (6.72), we have
FEVB−Posi
h
= −M log
⎛⎜⎜⎜⎜⎝1 −
γh˘γEVB
h
γh˘γEVB
h
+ Mσ2
⎞⎟⎟⎟⎟⎠−L log
⎛⎜⎜⎜⎜⎝1 −
γh˘γEVB
h
γh˘γEVB
h
+ Lσ2
⎞⎟⎟⎟⎟⎠
−γh˘γEVB
h
σ2
,
which leads to Eq. (6.112). This completes the proof of Lemma 6.16.
□
In Figure 6.3, the positive EVB local solution at cahcbh =
.
γh˘γEVB
h
/(LM)
is indicated by a cross if it exists.
6.9.2 EVB Threshold
Lemmas 6.14 and 6.16 state that, if γh ≤γlocal−EVB, only the null EVB local
solution exists, and therefore it is the global EVB solution. In this section,
assuming that γh ≥γlocal−EVB, we compare the free energy (6.105) at the
null EVB local solution and the free energy (6.112) at the positive EVB local
solution. Since FEVB−Null
h
→+0, we simply consider the situation where
FEVB−Posi
h
≤0. Eq. (6.108) gives

γh˘γEVB
h
+ Lσ2 ⎛⎜⎜⎜⎜⎝1 +
Mσ2
γh˘γEVB
h
⎞⎟⎟⎟⎟⎠= γ2
h.
(6.114)
By using Eqs. (6.103) and (6.109), we have
γh˘γEVB
h
= 1
2

γ2
h −

γlocal−EVB 2 + 2
√
LMσ2
+
B!
γ2
h −

γlocal−EVB 2" !
γ2
h −

γlocal−EVB 2 + 4
√
LMσ2
"
≥
√
LMσ2.
(6.115)
Remember the deﬁnition of α (Eq. (6.99))
α = L
M
(0 < α ≤1),
and let
xh =
γ2
h
Mσ2 ,
(6.116)
τh = γh˘γEVB
h
Mσ2 .
(6.117)

178
6 Global VB Solution of Fully Observed Matrix Factorization
0
5
10
−0.5
0
0.5
Figure 6.5 Φ(z) = log(z+1)
z
−1
2.
Eqs. (6.114) and (6.103) imply the following mutual relations between xh
and τh:
xh ≡x(τh; α) = (1 + τh)

1 + α
τh

,
(6.118)
τh ≡τ(xh; α) = 1
2

xh −(1 + α) +
.
(xh −(1 + α))2 −4α

.
(6.119)
Eqs. (6.109) and (6.115) lead to
xh ≥xlocal =
(γlocal−EVB)2
Mσ2
= x( √α; α) = (1 + √α)2,
(6.120)
τh ≥τlocal = √α.
(6.121)
Then, using Ξ (τ; α) deﬁned by Eq. (6.100), we can rewrite Eq. (6.112) as
FEVB−Posi
h
= M log (τh + 1) + L log
!τh
α + 1
"
−Mτh
= MτhΞ (τ; α) .
(6.122)
The following holds for Φ(z) (which is also deﬁned in Eq. (6.100)):
Lemma 6.17
Φ(z) is decreasing for z > 0.
Proof
The derivative is
∂Φ
∂z =
1 −
1
z+1 −log(z + 1)
z2
,
which is negative for z > 0 because
1
z + 1 + log(z + 1) > 1.
This completes the proof of Lemma 6.17.
□

6.9 Proof of Theorem 6.13
179
Figure 6.5 shows the proﬁle of Φ(z). Since Φ(z) is decreasing, Ξ(τ; α) is
also decreasing with respect to τ. It holds that, for any 0 < α ≤1,
lim
τ→0 Ξ(τ; α) = 1,
lim
τ→∞Ξ(τ; α) = −1.
Therefore, Ξ(τ; α) has a unique zero-cross point τ, such that
Ξ(τ; α) ≤0
if and only if
τ ≥τ.
(6.123)
Then, we can prove the following lemma:
Lemma 6.18
The unique zero-cross point τ of Ξ(τ; α) lies in the following
range:
√α < τ < z,
where z ≈2.5129 is the unique zero-cross point of Φ(z).
Proof
Since Φ(z) is decreasing, Ξ (τ; α) is upper-bounded by
Ξ (τ; α) = Φ (τ) + Φ
! τ
α
"
≤2Φ (τ) = Ξ (τ; 1) .
Therefore, the unique zero-cross point τ of Ξ (τ; α) is no greater than the
unique zero-cross point z of Φ(z), i.e.,
τ ≤z.
For obtaining the lower-bound τ
>
√α, it sufﬁces to show that
Ξ( √α; α) > 0. Let us prove that the following function is decreasing and
positive for 0 < α ≤1:
g(α) ≡
Ξ
 √α; α
 
√α
.
From the deﬁnition (6.100) of Ξ (τ; α), we have
g(α) =

1 + 1
α

log( √α + 1) −log √α −
1
√α.
The derivative is given by
∂g
∂√α =

1 + 1
α
 
√α + 1 −
2
α3/2 log( √α + 1) −
1
√α + 1
α
= −2
α3/2

log( √α + 1) +
1
√α + 1 −1

< 0,

180
6 Global VB Solution of Fully Observed Matrix Factorization
which implies that g(α) is decreasing. Since
g(1) = 2 log 2 −1 ≈0.3863 > 0,
g(α) is positive for 0 < α ≤1, which completes the proof of Lemma 6.18.
□
Since Eq. (6.118) is increasing with respect to τh (> √α), the thresholding
condition τ ≥τ in Eq. (6.123) can be expressed in terms of x:
Ξ(τ(x); α) ≤0
if and only if
x ≥x,
(6.124)
where
x ≡x(τ; α) =

1 + τ
 
1 + α
τ

.
(6.125)
Using Eqs. (6.116) and (6.122), we have
FEVB−Posi
h
≤0
if and only if
γh ≥γEVB,
(6.126)
where γEVB is deﬁned by Eq. (6.102). Thus, we have the following lemma:
Lemma 6.19
The positive EVB local solution is the global EVB solution if
and only if γh ≥γEVB.
Combining Lemmas 6.14, 6.16, and 6.19 completes the proof of
Theorem 6.13.
□
Figure 6.6 shows estimators and thresholds for L = M = H = 1 and σ2 = 1.
The curves indicate the VB solutionγVB
h
given by Eq. (6.48), the EVB solution
γEVB
h
given by Eq. (6.101), the EVB positive local minimizer ˘γEVB
h
given by
Eq. (6.103), and the EVB positive local maximizer ´γh given by Eq. (6.107),
respectively. The arrows indicate the VB threshold γVB
h
given by Eq. (6.49),
the local EVB threshold γlocal−EVB given by Eq. (6.109), and the EVB threshold
γEVB given by Eq. (6.102), respectively.
6.10 Summary of Intermediate Results
In the rest of this section, we summarize some intermediate results obtained in
Section 6.9, which are useful for further analysis (mainly in Chapter 8).
Summarizing Eqs. (6.109), (6.114), and (6.115) leads to the following
corollary:
Corollary 6.20
The EVB shrinkage estimator (6.103) is a stationary point of
the free energy (6.43), which exists if and only if
γh ≥γlocal−EVB ≡(
√
L +
√
M)σ,
(6.127)

6.10 Summary of Intermediate Results
181
1
2
3
1
2
3
Figure 6.6 Estimators and thresholds for L = M = H = 1 and σ2 = 1.
and satisﬁes the following equation:

γh˘γEVB
h
+ Lσ2 ⎛⎜⎜⎜⎜⎝1 +
Mσ2
γh˘γEVB
h
⎞⎟⎟⎟⎟⎠= γ2
h.
(6.128)
It holds that
γh˘γEVB
h
≥
√
LMσ2.
(6.129)
Combining Lemmas 6.14, 6.16, and 6.19 leads to the following corollary:
Corollary 6.21
The minimum free energy achieved under EVB learning is
given by Eq. (6.42) with
2Fh =
⎧⎪⎪⎪⎨⎪⎪⎪⎩
M log
!
γh ˘γEVB
h
Mσ2 + 1
"
+ L log
!
γh ˘γEVB
h
Lσ2
+ 1
"
−
γh ˘γEVB
h
σ2
if γh ≥γEVB,
+0
otherwise.
(6.130)
Corollary 6.20 together with Theorem 6.13 implies that when
γlocal−EVB ≤γh < γEVB,
a stationary point (called the positive EVB local solution and speciﬁed by
Lemma 6.16) exists at Eq. (6.103), but it is not the global minimum. Actually,
a local minimum (called the null EVB local solution and speciﬁed by Lemma
6.14) with Fh = +0 always exists. The stationary point at Eq. (6.103) is
a nonglobal local minimum when γlocal−EVB ≤γh < γEVB and the global

182
6 Global VB Solution of Fully Observed Matrix Factorization
minimum when γh ≥γEVB (see Figure 6.3 with its caption). This phase
transition induces the free energy thresholding observed in Corollary 6.21.
We deﬁne a local-EVB estimator by
U
local−EVB =
H

h=1
γlocal−EVB
h
ωbhω⊤
ah,
where
γlocal−EVB
h
=
⎧⎪⎪⎨⎪⎪⎩
˘γEVB
h
if γh ≥γlocal−EVB,
0
otherwise,
(6.131)
and call γlocal−EVB a local-EVB threshold. This estimator gives the positive
EVB local solution, whenever it exists, for each singular component. There
is an interesting relation between the local-EVB solution and an alternative
dimensionality selection method (Hoyle, 2008), which will be discussed in
Section 8.6.
Rescaling the quantities related to the squared singular value by Mσ2—to
which the contribution from noise (each eigenvalue of E⊤E) scales linearly—
simpliﬁes expressions. Assume that the condition (6.127) holds, and deﬁne
xh =
γ2
h
Mσ2 ,
(6.132)
τh = γh˘γEVB
h
Mσ2 ,
(6.133)
which are used as a rescaled observation and a rescaled EVB estimator,
respectively. Eqs. (6.128) and (6.103) specify the mutual relations between
them:
xh ≡x(τh; α) = (1 + τh)

1 + α
τh

,
(6.134)
τh ≡τ(xh; α) = 1
2

xh −(1 + α) +
.
(xh −(1 + α))2 −4α

.
(6.135)
With these rescaled variables, the condition (6.127), as well as (6.129), for the
existence of the positive local-EVB solution ˘γEVB
h
is expressed as
xh ≥xlocal =
(γlocal−EVB)2
Mσ2
= x( √α; α) = (1 + √α)2,
(6.136)
τh ≥τlocal = √α.
(6.137)
The EVB threshold (6.102) is expressed as
x =
(γEVB)2
Mσ2
= x(τ; α) =

1 + τ
 
1 + α
τ

,
(6.138)

6.10 Summary of Intermediate Results
183
and the free energy (6.130) is expressed as
Fh = Mτh · min (0, Ξ (τh; α)) ,
(6.139)
where Ξ(τ; α) is deﬁned by Eq. (6.100).
The preceding rescaled expressions give an intuition of Theorem 6.13: the
EVB solution γEVB
h
is positive if and only if the positive local-EVB solution
˘γEVB
h
exists (i.e., xh ≥xlocal), and the free energy Ξ (τ(xh; α); α) at the local-
EVB solution is nonpositive (i.e., τ(xh; α) ≥τ or equivalently xh ≥x ).

7
Model-Induced Regularization and Sparsity
Inducing Mechanism
Variational Bayesian (VB) learning often shows the automatic relevance
determination (ARD) property—the solution is sparse with unnecessary com-
ponents eliminated automatically. In this chapter, we try to elucidate the
sparsity inducing mechanism of VB learning, based on the global analytic
solutions derived in Chapter 6. We argue that the ARD property is induced by
the model-induced regularization (MIR), which all Bayesian learning methods
possess when unidentiﬁable models are involved, and that MIR is enhanced by
the independence constraint (imposed for computational tractability), which
induces phase transitions making the solution (exactly) sparse (Nakajima and
Sugiyama, 2011).
We ﬁrst show the VB solution for special cases where the MIR effect is
visible in the solution form. Then we illustrate the behavior of the posteriors
and estimators in the one-dimensional case, comparing VB learning with
maximum a posteriori (MAP) learning and Bayesian learning. After that, we
explain MIR, and how it is enhanced in VB learning through phase transitions.
7.1 VB Solutions for Special Cases
Here we discuss two special cases of fully observed matrix factorization (MF),
in which the VB solution is simple and intuitive.
Almost Flat Prior
When cahcbh →∞(i.e., the prior is almost ﬂat), the VB solution given by
Theorem 6.7 in Chapter 6 has a simple form.
184

7.1 VB Solutions for Special Cases
185
Corollary 7.1
The VB solution of the fully observed matrix factorization
model (6.1) through (6.3) is given by
U
VB =
H

h=1
γVB
h ωbhω⊤
ah,
(7.1)
where the estimator γVB
h
corresponding to the hth largest singular value is
upper-bounded as
γVB
h
< max
⎧⎪⎪⎨⎪⎪⎩0,
⎛⎜⎜⎜⎜⎝1 −max(L, M)σ2
γ2
h
⎞⎟⎟⎟⎟⎠γh
⎫⎪⎪⎬⎪⎪⎭.
(7.2)
For the almost ﬂat prior (i.e., cahcbh →∞), the equality holds, i.e.,
lim
cahcbh→∞γVB
h
= max
⎧⎪⎪⎨⎪⎪⎩0,
⎛⎜⎜⎜⎜⎝1 −max(L, M)σ2
γ2
h
⎞⎟⎟⎟⎟⎠γh
⎫⎪⎪⎬⎪⎪⎭.
(7.3)
Proof
It is clear that the threshold (6.49) is decreasing and the shrinkage
factor (6.50) is increasing with respect to cahcbh. Therefore, γVB
h
is largest for
cahcbh →∞. In this limit, Eqs. (6.49) and (6.50) are reduced to
lim
cahcbh→∞γVB
h
= σ
>
?
@
(L + M)
2
+
A(L + M)
2
2
−LM
= σ
C
max(L, M),
lim
cahcbh→∞˘γVB
h
= γh
⎛⎜⎜⎜⎜⎝1 −σ2
2γ2
h

M + L +
C
(M −L)2 ⎞⎟⎟⎟⎟⎠
=
⎛⎜⎜⎜⎜⎝1 −max(L, M)σ2
γ2
h
⎞⎟⎟⎟⎟⎠γh,
which prove the corollary.
□
The form of the VB solution (7.3) in the limit is known as the positive-
part James–Stein (PJS) estimator (James and Stein, 1961), operated on each
singular component separately (see Appendix A for its interesting property
and the relation to Bayesian learning). A counterintuitive fact—a shrinkage is
observed even in the limit of the ﬂat prior—will be explained in terms of MIR
in Section 7.3.
Square Matrix
When L = M (i.e., the observed matrix V is square), the VB solution is
intuitive, so that the shrinkage caused by MIR and the shrinkage caused by
the prior are separately visible in its formula.

186
7 Model-Induced Regularization and Sparsity Inducing Mechanism
Corollary 7.2
When L = M, the VB solution is given by Eq. (7.1) with
γVB
h
= max
⎧⎪⎪⎨⎪⎪⎩0,
⎛⎜⎜⎜⎜⎝1 −Mσ2
γ2
h
⎞⎟⎟⎟⎟⎠γh −
σ2
cahcbh
⎫⎪⎪⎬⎪⎪⎭.
(7.4)
Proof
When L = M, Eqs. (6.49) and (6.50) can be written as
γVB
h
= σ
>
?
?
?
@
M +
σ2
2c2ahc2
bh
+
>
?
@⎛⎜⎜⎜⎜⎜⎝M +
σ2
2c2ahc2
bh
⎞⎟⎟⎟⎟⎟⎠
2
−M2,
˘γVB
h
= γh
⎛⎜⎜⎜⎜⎜⎜⎜⎝1 −σ2
2γ2
h
⎛⎜⎜⎜⎜⎜⎜⎜⎝2M +
>
@
4γ2
h
c2ahc2
bh
⎞⎟⎟⎟⎟⎟⎟⎟⎠
⎞⎟⎟⎟⎟⎟⎟⎟⎠
= γh
⎛⎜⎜⎜⎜⎝1 −σ2
γ2
h

M +
γh
cahcbh
⎞⎟⎟⎟⎟⎠.
We can conﬁrm that ˘γVB
h
≤0 when γh ≤γVB
h , which proves the corollary.
Actually, we can conﬁrm that ˘γVB
h
= 0 when γh = γVB
h , and ˘γVB
h
< 0 when
γh < γVB
h
for any L, M, c2
ah, and c2
bh.
□
In the VB solution (7.4), we can identify the PJS shrinkage and a constant
shrinkage. The PJS shrinkage can be considered to be caused by MIR since
it appears even with the ﬂat prior, while the constant shrinkage −σ2/(cahcbh)
is considered to be caused by the prior since it appears in MAP learning (see
Theorem 12.1 in Chapter 12).
The empirical VB (EVB) solution is also simple for square matrices. The
following corollary is obtained from Theorem 6.13 in Chapter 6:
Corollary 7.3
When L = M, the global EVB solution is given by
γEVB
h
=
⎧⎪⎪⎨⎪⎪⎩
˘γEVB
h
if γh > γEVB,
0
otherwise,
where
γEVB = σ
A
M

2 + τ(1) +
1
τ(1)

,
˘γEVB
h
= γh
2
⎛⎜⎜⎜⎜⎜⎜⎝1 −2Mσ2
γ2
h
+
A
1 −4Mσ2
γ2
h
⎞⎟⎟⎟⎟⎟⎟⎠.

7.2 Posteriors and Estimators in a One-Dimensional Case
187
Proof
When L = M, Eqs. (6.102) and (6.103) can be written as
γEVB = σ
A
M

2 + τ(1) +
1
τ(1)

,
˘γEVB
h
= γh
2
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎝1 −2Mσ2
γ2
h
+
>
@⎛⎜⎜⎜⎜⎝1 −2Mσ2
γ2
h
⎞⎟⎟⎟⎟⎠
2
−4M2σ4
γ4
h
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎠
= γh
2
⎛⎜⎜⎜⎜⎜⎜⎝1 −2Mσ2
γ2
h
+
A
1 −4Mσ2
γ2
h
⎞⎟⎟⎟⎟⎟⎟⎠,
which completes the proof.
□
7.2 Posteriors and Estimators in a One-Dimensional Case
In order to illustrate how strongly Bayesian learning and its approximation
methods are regularized, we depict posteriors and estimators in the MF model
for L = M = H = 1 (i.e., U, V, A, and B are merely scalars):
p(V|A, B) =
1
√
2πσ2 exp

−(V −BA)2
2σ2

.
(7.5)
In this model, we can visualize the unidentiﬁability of the MF model as
equivalence classes—a set of points (A, B) on which the product is unchanged,
i.e., U = BA, represents the same distribution (see Figure 7.1). When U = 0,
the equivalence class has a “cross-shape” proﬁle on the A- and B-axes;
otherwise, it forms a pair of hyperbolic curves. This redundant structure in the
−2
−1
0
1
A
−3
2
3
−3
−2
−1
0
1
2
3
B
U= 2
U= 1
U= 0
U= −1
U= −2
Figure 7.1 Equivalence class structure of the one-dimensional MF model. Any A
and B such that their product is unchanged give the same U.

188
7 Model-Induced Regularization and Sparsity Inducing Mechanism
parameter space is the origin of MIR, and highly inﬂuences the phase transition
phenomenon in VB learning, as we will see shortly.
With Gaussian priors,
p(A) =
1
C
2πc2a
exp

−A2
2c2a

,
(7.6)
p(B) =
1
.
2πc2
b
exp
⎛⎜⎜⎜⎜⎝−B2
2c2
b
⎞⎟⎟⎟⎟⎠,
(7.7)
the Bayes posterior is proportional to
p(A, B|V) ∝p(V|A, B)p(A)p(B)
∝exp
⎛⎜⎜⎜⎜⎝−1
2σ2 (V −BA)2 −A2
2c2a
−B2
2c2
b
⎞⎟⎟⎟⎟⎠.
(7.8)
Figure 7.2 shows the contour of the unnormalized Bayes posterior (7.8) when
V = 0, 1, 2 are observed, the noise variance is σ2 = 1, and the prior covariances
are set to ca = cb = 100 (i.e., almost ﬂat priors). We can see that the
equivalence class structure is reﬂected in the Bayes posterior: when V = 0,
the surface of the Bayes posterior has a cross-shaped proﬁle and its maximum
is at the origin; when V > 0, the surface is divided into the positive orthant
(i.e., A, B > 0) and the negative orthant (i.e., A, B < 0), and the two “modes”
get farther as V increases.
MAP Solution
Let us ﬁrst investigate the behavior of the MAP estimator, which coincides with
the maximum likelihood (ML) estimator when the priors are ﬂat. For ﬁnite ca
and cb, the MAP solution can be expressed as
AMAP = ±
A
ca
cb
max
)
0, |V| −σ2
cacb
1
,
BMAP = ±sign(V)
A
cb
ca
max
)
0, |V| −σ2
cacb
1
,
where sign(·) denotes the sign of a scalar (see Corollary 12.2 in Chapter 12
for derivation). In Figure 7.2, the asterisks indicate the MAP estimators, and
the dashed curves indicate the ML estimators (the modes of the contour of Eq.
(7.8) when ca = cb →∞). When V = 0, the Bayes posterior takes the maxi-
mum value on the A- and B-axes, which results in the MAP estimator equal to

UMAP(= BMAPAMAP) = 0. When V = 1, the proﬁle of the Bayes posterior
is hyperbolic and the maximum value is achieved on the hyperbolic curves in
the positive orthant (i.e., A, B > 0) and the negative orthant (i.e., A, B < 0);

7.2 Posteriors and Estimators in a One-Dimensional Case
189
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.3
0.3
0.3
A
B
Bayes posterior (V = 0)
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
MAP estimator:
(A, B) = (0, 0)
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.3
0.3
0.3
0.3
0.3
A
B
Bayes posterior (V = 1)
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
MAP estimators:
(A, B) ≈(± 1, ± 1)
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.3
0.3
0.3
A
B
Bayes posterior (V = 2)
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
MAP estimators:
(A, B) ≈(±
√
2, ±
√
2)
Figure 7.2 (Unnormalized) Bayes posteriors for ca = cb = 100 (i.e., almost ﬂat
priors). The asterisks are the MAP estimators, and the dashed curves indicate the
ML estimators (the modes of the contour when ca = cb = c →∞).
in either case, 
UMAP ≈1 (limca,cb→∞
UMAP = 1). When V = 2, a similar
multimodal structure is observed and the MAP estimator is 
UMAP
≈2
(limca,cb→∞
UMAP = 2). From these plots, we can visually conﬁrm that the
MAP estimator with almost ﬂat priors (ca = cb = 100) approximately agrees
with the ML estimator: 
UMAP ≈
UML = V (limca,cb→∞
UMAP = 
UML). We
will use the ML estimator as an unregularized reference in the following
discussion.
Figure 7.3 shows the contour of the Bayes posterior when ca = cb = 2.
The MAP estimators shift from the ML solutions (dashed curves) toward the
origin, and they are more clearly contoured as peaks.
VB Solution
Next we depict the VB posterior, given by Corollary 6.8 in Chapter 6. When
L = M = H = 1, the VB solution is given by

190
7 Model-Induced Regularization and Sparsity Inducing Mechanism
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
A
B
Bayes posterior (V = 0)
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.3
0.3
A
B
Bayes posterior (V = 1)
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
A
B
Bayes posterior (V = 2)
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
Figure 7.3 (Unnormalized) Bayes posteriors for ca = cb = 2. The dashed curves
indicating the ML estimators are identical to those in Figure 7.2.
r(A, B) =
⎧⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎩
Gauss1
!
A; ±
.
˘γVB ca
cb , σ2ca
|V|cb
"
Gauss1
!
B; ±sign(V)
.
˘γVB cb
ca , σ2cb
|V|ca
"
if |V| ≥γVB,
Gauss1

A; 0, c2
aκVB 
Gauss1

B; 0, c2
bκVB 
otherwise,
(7.9)
where
γVB = σ
A
1 +
σ2
2c2ac2
b +
B!
1 +
σ2
2c2ac2
b
"2
−1,
˘γVB =

1 −σ2
V2
 
|V| −
σ2
cacb ,
κVB = −σ2
2c2ac2
b +
B!
1 +
σ2
2c2ac2
b
"2
−1.
Figure 7.4 shows the contour of the VB posterior (7.9) when V = 0, 1, 2
are observed, the noise variance is σ2 = 1, and the prior covariances are

7.2 Posteriors and Estimators in a One-Dimensional Case
191
0.05
0.05
0.05
0.05
0.05
0.05
0.05
0.1
0.1
0.1
0.1
0.15
0.15
A
B
VB posterior (V = 0)
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
VB estimator : (A, B) = (0, 0)
0.05
0.05
0.05
0.05
0.05
0.05
0.05
0.1
0.1
0.1
0.1
0.15
0.15
A
B
VB posterior (V = 1)
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
VB estimator : (A, B) = (0, 0)
0.05
0.05
0.05
0.05
0.05
0.05
0.1
0.1
0.1
0.1
0.1
0.15
0.15
0.15
0.15
0.2
0.2
0.2
0.25
0.25
0.3
A
B
VB posterior (V = 2)
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
VB estimator :
(A, B) ≈(
√
1.5,
√
1.5)
0.05
0.05
0.05
0.05
0.05
0.05
0.1
0.1
0.1
0.1
0.1
0.15
0.15
0.15
0.15
0.2
0.2
0.2
0.25
0.25
0.3
A
B
VB posterior (V = 2)
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
VB estimator :
(A, B) ≈(−
√
1.5, −
√
1.5)
Figure 7.4 VB solutions for ca = cb = 100 (i.e., almost ﬂat priors). When V = 2,
VB learning gives either one of the two solutions shown in the bottom row.
set to ca = cb = 100 (i.e., almost ﬂat priors). When V = 0, the cross-
shaped contour of the Bayes posterior (see Figure 7.2) is approximated by
a spherical Gaussian distribution located at the origin. Thus, the VB estimator
is 
UVB = 0, which coincides with the MAP estimator. When V = 1, two
hyperbolic “modes” of the Bayes posterior are approximated again by a
spherical Gaussian distribution located at the origin. Thus, the VB estimator
is still 
UVB = 0, which differs from the MAP estimator 
UMAP ≈1.
V = γVB ≈
√
Mσ2 = 1 (limca,cb→∞γVB =
√
Mσ2) is actually a transition
point of the VB solution. When V is not larger than the threshold γVB ≈1,
VB learning tries to approximate the two “modes” of the Bayes posterior by
the origin-centered Gaussian distribution. When V goes beyond the threshold
γVB ≈1, the “distance” between two hyperbolic modes of the Bayes posterior
becomes so large that VB learning chooses to approximate one of those two
modes in the positive and the negative orthants. As such, the symmetry is

192
7 Model-Induced Regularization and Sparsity Inducing Mechanism
broken spontaneously and the VB estimator is detached from the origin. The
bottom row of Figure 7.4 shows the contour of the two possible VB posteriors
when V = 2. Note that the VB estimator, 
UVB ≈3/2, is the same for both
cases, and differs from the MAP estimator 
UMAP ≈2.
In general, the VB estimator is closer to the origin than the MAP estimator,
and the relative difference between them tends to shrink as V increases.
Bayesian Estimator
The full Bayesian estimator is deﬁned as the mean of the Bayes posterior (see
Eq. (1.7)). In the MF model with L = M = H = 1, the Bayesian estimator is
expressed as

UBayes = ⟨BA⟩p(V|A,B)p(A)p(B)/p(V) .
(7.10)
If V = 0, 1, 2, 3 are observed, the Bayesian estimator with almost ﬂat priors are

UBayes = 0, 0.92, 1.93, 2.95, respectively, which were numerically computed.1
Compared with the MAP estimator (with almost ﬂat priors), which gives

UMAP = 0, 1, 2, 3, respectively, the Bayesian estimator is slightly shrunken.
EVB Solution
Next we consider the empirical Bayesian solutions, where the hyperparameters
ca, cb are also estimated from observation (the noise variance σ2 is still treated
as a given constant). We ﬁx the ratio between the prior variances to ca/cb = 1.
From Corollary 7.3 and Eq. (7.9), we obtain the EVB posterior for L = M =
H = 1 as follows:
r(A, B) =
⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩
Gauss1

A; ±
C
˘γEVB, σ2
|V|
 
Gauss1

B; ±sign(V)
C
˘γEVB, σ2
|V|
 
if |V| ≥γEVB,
Gauss1 (A; 0, +0) Gauss1 (B; 0, +0)
otherwise,
(7.11)
where
γEVB = σ
A
2 + τ(1) +
1
τ(1) ≈σ
B
2 + 2.5129 +
1
2.5129 ≈2.216σ,
˘γEVB = |V|
2
⎛⎜⎜⎜⎜⎜⎝1 −2σ2
V2 +
B
1 −4σ2
V2
⎞⎟⎟⎟⎟⎟⎠.
1 More precisely, we numerically calculated the Bayesian estimator (7.10) by sampling A and B
from the almost ﬂat priors p(A)p(B) for ca = cb = 100 and computing the ratio between the
sample averages of BA · p(V|A, B) and p(V|A, B).

7.2 Posteriors and Estimators in a One-Dimensional Case
193
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
A
B
EVB posterior (V = 2)
EVB estimator : (A, B) = (0, 0)
0.1
0.1
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.4
0.4
A
B
EVB posterior (V = 3)
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
EVB estimator :
(A, B) ≈(
√
2.28,
√
2.28)
0.1
0.1
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.4
0.4
A
B
EVB posterior (V = 3)
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
EVB estimator :
(A, B) ≈(−
√
2.28, −
√
2.28)
Figure 7.5 EVB solutions. Top-left: When V = 2, the EVB posterior is reduced
to the Dirac delta function located at the origin. Top-right and bottom: When
V = 3, the EVB posterior is detached from the origin, and located at (A, B) ≈
(
√
2.28,
√
2.28) or (A, B) ≈(−
√
2.28, −
√
2.28), both of which yield the same
EVB estimator 
UEVB ≈2.28.
Figure 7.5 shows the EVB posterior when V = 2, 3 are observed, and the
noise variance is σ2 = 1. When V = 2 < γEVB, the EVB posterior is given by
the Dirac delta function located at the origin, resulting in the EVB estimator
equal to 
UEVB = 0 (top-left graph). On the other hand, when V = 3 > γEVB,
the EVB posterior is a Gaussian located in the top-right region or bottom-left
region, and the EVB estimator is 
UEVB ≈2.28 for both solutions (top-right and
bottom graphs).
Empirical Bayesian Estimator
The empirical Bayesian (EBayes) estimator (introduced in Section 1.2.7) is the
Bayesian estimator,

UEBayes = ⟨BA⟩p(V|A,B)p(A;ca)p(B;cb)/p(V;ca,cb) ,

194
7 Model-Induced Regularization and Sparsity Inducing Mechanism
with the hyperparameters estimated by minimizing the Bayes free energy
FBayes(V; ca, cb) ≡−log p(V; ca, cb), i.e.,
(ca,cb) = argmin
(ca,cb)
FBayes(V; ca, cb).
When V = 0, 1, 2, 3 are observed, the EBayes estimators are 0.00, 0.00, 1.25,
2.58 (with the prior variance estimators given by ca = cb ≈0.0, 0.0, 1.4, 2.1),
respectively, which were numerically computed.2
Behavior of Estimators
Figure 7.6 shows the behavior of estimators, including the MAP estimator

UMAP, the VB estimator 
UVB, the Bayesian estimator 
UBayes, the EVB
estimator 
UEVB, and the EBayes estimator 
UEBayes, when the noise variance
is σ2 = 1. For nonempirical Bayesian estimators, i.e., the MAP, the VB, and
the Bayesian estimators, the hyperparameters are set to ca = cb = 100 (i.e.,
almost ﬂat priors). Overall, the solutions satisfy

UEVB < 
UEBayes < 
UVB < 
UBayes < 
UMAP(≈
UML),
0
1
2
3
0
0.5
1
1.5
2
2.5
3
Figure 7.6 Behavior of the MAP estimator 
UMAP, the VB estimator 
UVB, the
Bayesian estimator 
UBayes, the EVB estimator 
UEVB, and the EBayes estimator

UEBayes, when the noise variance is σ2 = 1. For the MAP, the VB, and the
Bayesian estimators, the hyperparameters are set to ca = cb = 100 (i.e., almost
ﬂat priors).
2 For cacb = 10−2.00, 10−1.99,. . . , 101.00, we numerically computed the Bayes free energy, and
chose its minimizercacb, with which the Bayesian estimator was computed.

7.3 Model-Induced Regularization
195
which shows the strength of the regularization effect of each method. Naturally,
the empirical Bayesian variants are more regularized than their nonempirical
Bayesian counterparts with almost ﬂat priors.
With almost ﬂat priors, the MAP estimator is almost identical to the ML
estimator, 
UMAP ≈
UML = V, meaning that it is unregularized. We see in
Figure 7.6 that the Bayesian estimator 
UBayes is regularized even with almost
ﬂat priors. Furthermore, the VB estimator 
UVB shows thresholding behavior,
which leads to exact sparsity in multidimensional cases. Exact sparsity also
appears in EVB learning and EBayes learning. In the subsequent sections, we
explain those observations in terms of model-induced regularization and phase
transitions.
7.3 Model-Induced Regularization
In this section, we explain the origin of the shrinkage of the Bayesian estimator,
observed in Section 7.2. The shrinkage is caused by an implicit regularization
effect, called model-induced regularization (MIR), which is strongly related to
unidentiﬁability of statistical models.
7.3.1 Unidentiﬁable Models
Identiﬁability is formally deﬁned as follows:
Deﬁnition 7.4
(Identiﬁability of statistical models) A statistical model p(·|w)
parameterized by w ∈W is said to be identiﬁable, if the mapping w →p(·|w)
is one-to-one, i.e.,
p(·|w1) = p(·|w2) ⇐⇒w1 = w2
for any w1, w2 ∈W.
Otherwise, it is said to be unidentiﬁable.3
Many popular statistical models are unidentiﬁable.
Example 7.5
The MF model (introduced in Section 3.1) is unidentiﬁable,
because the model distribution
p(V|A, B) ∝exp

−1
2σ2
###V −BA⊤###2
Fro

(7.12)
3 Distributions are identiﬁed in weak topology in distribution, i.e., p(x|w1) is identiﬁed with
p(x|w2) if

f(x)p(x|w1)dx =

f(x)p(x|w2)dx for any bounded continuous function f(x).

196
7 Model-Induced Regularization and Sparsity Inducing Mechanism
is invariant to the following transformation (A, B) →(AT⊤, BT−1) for any
nonsingular matrix T ∈RH×H.
Example 7.6
The multilayer neural network model is unidentiﬁable.
Consider a three-layer neural network with H hidden units:
p(y|x, A, B) ∝exp

−1
2σ2 ∥y −f(x; A, B)∥2

,
f(x; A, B) =
H

h=1
bh · ψ

a⊤
h x
 
,
(7.13)
where x ∈RM is an input vector, y ∈RL is an output vector, A = (a1,. . . , aH) ∈
RM×H and B = (b1,. . . , bH) ∈RL×H are the weight parameters to be estimated,
and ψ(·) is an antisymmetric nonlinear activation function such as tanh(·). This
model expresses the identical distribution on each of the following sets of
points in the parameter space:
{ah ∈RM, bh = 0} ∪{ah = 0, bh ∈RL} for any h,
{ah = ah′, bh, bh′ ∈RL, bh + bh′ = const.} for any pair h, h′.
In other words, the model is invariant for any ah ∈RM if bh = 0, for any
bh ∈RL if ah = 0, and for any bh, bh′ ∈RL as long as bh + bh′ is unchanged
and ah = ah′.
Example 7.7
(Mixture models) The mixture model (introduced as Example
1.3 in Section 1.1.4) is generally unidentiﬁable. The model distribution is
given as
p(x|α, {τk}) =
K

k=1
αkp(x|τk),
(7.14)
where x ∈X is an observed random variable, and α = (α1,. . . , αK)⊤∈ΔK−1
and {τk ∈T }K
k=1 are the parameters to be estimated. This model expresses the
identical distribution on each of the following sets of points in the parameter
space:
{αk = 0, τk ∈T }
for any k,
{αk, αk′ ∈[0, 1], αk + αk′ = const., τk = τk′} for any pair k, k′.
Namely, if the mixing weight αk is zero for the kth mixture component, the
corresponding component parameter τk does not affect the model distribution,
and if there are two identical components τk = τk′, the balance between the
corresponding mixture weights, αk and αk′, are arbitrary.

7.3 Model-Induced Regularization
197
Readers might have noticed that, in the multilayer neural network (Example
7.6) and the mixture model (Example 7.7), the model expressed by the
unidentiﬁable sets of points corresponds to the model with fewer components
or smaller degrees of freedom. For example, if ah = 0 or bh = 0 in the neural
network with H hidden units, the model is reduced to the neural network
with H −1 hidden units. If two hidden units receive the identical input, i.e.,
ψ

a⊤
h x
 
= ψ

a⊤
h′x
 
for any x ∈RM, they can be combined into a single
unit with its output weight equal to the sum of the original output weights,
i.e., bh + bh′ →bh. Thus, the model is again reduced to the neural network
with H −1 hidden units. The same applies to the mixture models and many
other popular statistical models, including Bayesian networks, hidden Markov
models, and latent Dirichlet allocation, which were introduced in Chapter 4. As
will be explained shortly, this nesting structure—simpler models correspond to
unidentiﬁable sets of points in the parameter space of more complex models—
is essential for MIR.
7.3.2 Singularities
Continuous points denoting the same distribution are called singularities, on
which the Fisher information,
SD
+ ∋F =
 ∂log p(x|w)
∂w
∂log p(x|w)
∂w
⊤
p(x|w)dx,
(7.15)
is singular, i.e., it has at least one zero eigenvalue. This is a natural conse-
quence from the fact that the Fisher information corresponds to the metric
when the distance between two points in the parameter space is measured by
the KL divergence (Jeffreys, 1946), i.e., it holds that
KL (p(x|w)∥p(x|w + Δw)) = 1
2Δw⊤FΔw + O(∥Δw∥3)
for a small change Δw of the parameter. On the singularities, there is at least
one direction in which the small change Δw does not affect the distribution,
implying that the Fisher metric F is singular. This means that the volume
element, proportional to the determinant of the Fisher metric, is zero on the
singularities, while it is positive on the regular points (see Appendix B for
more details on the Fisher metric and the volume element in the parameter
space).
This strong nonuniformity of (the density of) the volume element affects the
behavior of Bayesian learning. For this reason, statistical models having singu-
larities in their parameter space are called singular models and distinguished

198
7 Model-Induced Regularization and Sparsity Inducing Mechanism
Figure 7.7 Singularities of a neural network model.
from the regular models in statistical learning theory (Watanabe, 2009). There
are two aspects of how singularities affect the learning properties. In this
chapter, we focus on one aspect that leads to MIR. The other aspect will be
discussed in Chapter 13.
Figure 7.7 illustrates the singularities in the parameter space of the three-
layer neural network (7.13) with H = 1 hidden unit (see Example 7.6).
The horizontal axis corresponds to an arbitrary direction of ah ∈RM, while
the vertical axis corresponds to an arbitrary direction of bh
∈RL. The
shadowed locations correspond to the singularities. Importantly, all points on
the singularities express the identical neural network model with no (H = 0)
hidden unit, while each regular point expresses a different neural network
model with H = 1 hidden unit. This illustration gives an intuition that the
neighborhood of the smaller model (H = 0) is broader than the neighborhood
of the larger model (H = 1) in the parameter space.
Consider the Jeffreys prior,
pJef(w) ∝
C
det (F),
(7.16)
which is the uniform prior in the space of distributions when the distance is
measured by the KL divergence (see Appendix B). As discussed previously,
the Fisher information is singular on the singularities, giving pJef(w) = 0
for the smaller model (with H = 0), while the Fisher information is regular
on the other points, giving pJef(w) > 0 for the larger model (with H = 1). Also
in the neighborhood of the singularities, the Fisher information has similar
values and it holds that pJef(w) ≪1. This means that, in comparison with the

7.3 Model-Induced Regularization
199
Jeffreys prior, the ﬂat priors on ah and bh—the uniform prior in the parameter
space—put much more mass to the smaller model and its neighborhood.
A consequence is that, if we apply Bayesian learning with the ﬂat prior,
the overweighted singularities and their neighborhood pull the estimator to
the smaller model through the integral computation, which induces implicit
regularization—MIR. The same argument holds for mixture models (Example
7.6), and other popular models, including Bayesian networks, hidden Markov
models, and latent Dirichlet allocation.
In summary, MIR occurs in general singular models for the following
reasons:
• There is strong nonuniformity in (the density of) the volume element
around the singularities.
• Singularities correspond to the model with fewer degrees of freedom than
the regular points.
This structure in the parameter space makes the ﬂat prior favor smaller models
in Bayesian learning, which appears as MIR. Note that MIR does not occur in
point-estimation methods, including ML estimation and MAP learning, since
the nonuniformity of the volume element affects the estimator only through
integral computations.
MIR also occurs in the MF model (Example 7.6), which will be investigated
in the next subsection with a generalization of the Jeffreys prior.
7.3.3 MIR in one-Dimensional Matrix Factorization
In Section 7.2, we numerically observed MIR—the Bayesian estimator is
shrunken even with the almost ﬂat prior in the one-dimensional MF model.
However, in the MF model, the original deﬁnition (7.16) of the Jeffreys
prior is zero everywhere in the parameter space because of the equivalence
class structure (Figure 7.1), and therefore, it provides no information on
MIR. To evaluate the nonuniformity of the volume element, we redeﬁne the
(generalized) Jeffreys prior by ignoring the zero common eigenvalues, i.e.,
pJef(w) ∝
.D
d=1 λd,
(7.17)
where λd is the dth largest eigenvalue of the Fisher metric F, and D is the
maximum number of positive eigenvalues over the whole parameter space.
Let us consider the nonfactorizing model,
p(V|U) = Gauss1(V; U, σ2) ∝exp

−1
2σ2 (V −U)2

,
(7.18)

200
7 Model-Induced Regularization and Sparsity Inducing Mechanism
0.1
0.1
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.3
0.4
0.4
0.4
0.4
0.4
0.4
0.4
0.5
0.5
0.5
0.5
A
B
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
Figure 7.8 The (unnormalized) Jeffreys noninformative prior (7.20) of the one-
dimensional MF model (7.5).
where U itself is the parameter to be estimated. The Jeffreys prior for this
model is uniform (see Example B.1 in Appendix B for derivation):
pJef(U) ∝1.
(7.19)
On the other hand, the Jeffreys prior for the MF model (7.5) is given as follows
(see Example B.2 in Appendix B for derivation):
pJef(A, B) ∝
√
A2 + B2,
(7.20)
which is illustrated in Figure 7.8. Note that the Jeffreys priors (7.19) and (7.20)
for both cases are improper, meaning that they cannot be normalized since their
integrals diverge.
Jeffreys (1946) stated that the both combinations, the nonfactorizing model
(7.18) with its Jeffreys prior (7.19) and the MF model (7.5) with its Jeffreys
prior (7.20) give the equivalent Bayesian estimator. We can easily show that the
former combination, Eqs. (7.18) and (7.19), gives an unregularized solution.
Thus, the Bayesian estimator in the MF model (7.5) with its Jeffreys prior
(7.20) is also unregularized. Since the ﬂat prior on (A, B) has more probability
mass around the origin than the Jeffreys prior (7.20) (see Figure 7.8), it favors
smaller |U| and regularizes the Bayesian estimator.
Although MIR appears also in regular models unless the Jeffreys prior is
ﬂat in the parameter space, its effect is prominent in singular models with
unidentiﬁability, since the difference between the ﬂat prior and the Jeffreys
prior is large.

7.3 Model-Induced Regularization
201
7.3.4 Evidence View of Unidentiﬁable Models
MIR works as Occam’s razor in general. MacKay (1992) explained, with the
illustration shown in the left panel of Figure 7.9, that evidence-based (i.e.,
free-energy-minimization-based) model selection is naturally equipped with
Occam’s razor. In the ﬁgure, the horizontal axis denotes the space of the
observed data set D. H1 and H2 denote a simple hypothesis and a more
complex hypothesis, respectively. For example, in the MF model, the observed
data set corresponds to the observed matrix, i.e., D = V, H1 corresponds to
a lower-rank model, and H2 corresponds to a higher-rank model. The vertical
axis indicates the evidence or marginal likelihood,
p(D|Ht) = p(D|wHt, Ht)
p(wHt )
for
t = 1, 2,
(7.21)
where θHt denotes the unknown parameters that the hypothesis Ht has.
Since H1 is simple, it covers a limited area of the space of D (meaning that
it can explain only a simple phenomenon), while H2 covers a broader area.
The illustration implies that, because of the normalization, it holds that
p(D|H1) > p(D|H2)
for
D ∈C1,
where C1 denotes the observed data region where H1 can explain the data well.
This view gives an intuition on why evidence-based model selection prefers
simpler models when the observed data can be well explained by them.
However, this view does not explain MIR, which is observed even without
any model selection procedure. In fact, the illustration in the left panel of
Figure 7.9 is not accurate for unidentiﬁable models unless the Jeffreys prior
is adopted (note that a hypothesis consists of a model and a prior). The right
illustration of Figure 7.9 is a more accurate view for unidentiﬁable models.
When H2 is a complex unidentiﬁable model nesting H1 as a simpler model in
Figure 7.9 Left: The evidence view by MacKay (1992), which gives an intuition
on why evidence-based model selection prefers simpler models. Right: A more
accurate view for unidentiﬁable models. Simpler models are preferred even
without explicit model selection.

202
7 Model-Induced Regularization and Sparsity Inducing Mechanism
its parameter space, its evidence p(D|H2) has a bump covering the region C1
if the ﬂat prior is adopted. This is because the ﬂat prior typically places large
weights on the singularities representing the simpler model H1.
7.4 Phase Transition in VB Learning
In Section 7.3, we explained MIR, which shrinks the Bayesian estimator. We
can expect that VB learning, which involves integral computations, inherits
this property. However, we observe in Figure 7.6 that VB learning behaves
differently from Bayesian learning. Actually, the Bayesian estimator behaves
more similarly to the ML estimator (the MAP estimator with almost ﬂat priors),
rather than the VB estimator. A remarkable difference is that the VB estimator,
which is upper-bounded by the PJS estimator (7.2), shows exact sparsity,
i.e., the estimator can be zero for nonzero observation |V|. In this section,
we explain that this gap is caused by a phase transition phenomenon in VB
learning.
The middle graph in Figure 7.2 shows the Bayes posterior when V = 1.
The probability mass in the ﬁrst and the third quadrants pulls the product
U = BA toward the positive direction, and the mass in the second and the fourth
quadrants toward the negative direction. Since the Bayes posterior is skewed
and more mass is placed in the ﬁrst and the third quadrants, the Bayesian
estimator 
UBayes = ⟨BA⟩p(A,B|V) is positive. This is true even if V > 0 is
very small, and therefore, no thresholding occurs in Bayesian learning—the
Bayesian estimator is not sparse.
On the other hand, the VB posterior (the top-right graph of Figure 7.4) is
prohibited to be skewed because of the independent constraint, which causes
the following phase transition phenomenon. As seen in Figure 7.2, the Bayes
posterior has two modes unless V = 0, and the distance between the two
modes increases as |V| increases. Since the VB posterior tries to approximate
the Bayes posterior with a single uncorrelated distribution, it stays at the origin
if the two modes are close to each other so that covering both modes minimizes
the free energy. The VB posterior detaches from the origin if the two modes
get far apart so that approximating either one of the modes minimizes the
free energy. This phase transition mechanism makes the VB estimator exactly
sparse. The proﬁle of the Bayes posterior (the middle graph of Figure 7.2)
implies that, if we restrict the posterior to be Gaussian, but allow it to have
correlation between A and B, exact sparsity will not appear. In this sense,
we can say that MIR is enhanced by the independence constraint, which was
imposed for computational tractability.

7.4 Phase Transition in VB Learning
203
Mackay (2001) pointed out that there are cases where VB learning prunes
model components inappropriately, by giving a toy example of a mixture of
Gaussians. Note that appropriateness was measured in terms of the similarity
to full Bayesian learning. He plotted the free energy of the mixture of
Gaussians as a function of hidden responsibility variables—the probabilities
that each sample belongs to each Gaussian component—and argued that VB
learning sometimes favors simpler models too much. In this case, degrees of
freedom are pruned when spontaneous symmetry breaking (a phase transition)
occurs. Interestingly, in the MF model, degrees of freedom are pruned when
spontaneous symmetry breaking does not occur, as explained earlier.
Eq. (7.3) implies that the symmetry breaking occurs when V > γVB
h
≈
√
Mσ2 = 1, which coincides with the average contribution of noise to the
observed singular values over all singular components—more accurately,
√
Mσ2 is the square root of the average eigenvalues of the Wishart matrix
EE⊤
∼WishartL(σ2IL, M).4 In this way, VB learning discards singular
components dominated by noise.
Given that the full Bayesian estimator in MF is not sparse (see Figure 7.6),
one might argue that the sparsity of VB learning is an inappropriate artifact. On
the other hand, given that automatic model pruning by VB learning has been
acknowledged as a practically useful property (Bishop, 1999b; Bishop and
Tipping, 2000; Sato et al., 2004; Babacan et al., 2012b), one might also argue
that appropriateness should be measured in terms of performance. Motivated
by the latter idea, performance analysis has been carried out (Nakajima et al.,
2015), which will be detailed in Chapter 8.
In the empirical Bayesian scenario, where the prior variances ca, cb are also
estimated from observation, Bayesian learning also gives a sparse solution,
which is shown as diamonds (labeled as “EBayes”) in Figure 7.6. This
is somewhat natural since, in empirical Bayesian learning, the dependency
between A and c−2
a (as well as B and c−2
b ) in the prior (7.6) (in the prior (7.7))
and hence in the Bayes posterior is broken—the point-estimation of c2
a (as
well as c2
a) forces it to be independent of all other parameters. This forced
independence causes a similar phase transition phenomenon to the one caused
by the independence constraint between A and B in the (nonempirical) VB
learning, and results in exact sparsity of the EBayes estimator.
EVB learning has a different transition point, and tends to give a sparser
solution than VB learning. A notable difference from the VB estimator is that
the EVB estimator is no longer continuous as a function of the observation V.
This comes from the fact that, when |V| > γlocal−EVB, there exist two local
4 It holds that EE⊤∼WishartL(σ2IL, M) if E ∼GaussL(0, σ2IL).

204
7 Model-Induced Regularization and Sparsity Inducing Mechanism
solutions (see Figure 6.3), but the global solution is 
UEVB = 0 until the
observed amplitude |V| exceeds γEVB(> γlocal−EVB). When the positive local
solution ˘γEVB becomes the global solution, it is already distant from the origin,
which makes the estimator noncontinuous at the thresholding point (see the
dashed curve labeled as “EVB” in Figure 7.6).
7.5 Factorization as ARD Model
As shown in Section 7.1, MIR in VB learning for the MF model appears as
PJS shrinkage. We can see this as a natural consequence from the equivalence
between the MF model and the ARD model (Neal, 1996).
Assume that CA = IH in the MF model (6.1) through (6.3), and consider the
following transformation: BA⊤→U ∈RL×M. Then, the likelihood (6.1) and
the prior (6.2) on A become
p(V|U) ∝exp

−1
2σ2 ∥V −U∥2
Fro

,
(7.22)
p(U|B) ∝exp

−1
2tr

U⊤(BB⊤)†U
 
,
(7.23)
where † denotes the Moore–Penrose generalized inverse of a matrix. The
prior (6.3) on B is kept unchanged. p(U|B) in Eq. (7.23) is so-called the
ARD prior with the covariance hyperparameter BB⊤∈RL×L. It is known
that this prior induces the ARD property—empirical Bayesian learning, where
the prior covariance hyperparameter BB⊤is estimated from observation by
maximizing the marginal likelihood (or minimizing the free energy), induces
strong regularization and sparsity (Neal, 1996). Efron and Morris (1973)
showed that this particular model gives the JS shrinkage estimator as an
empirical Bayesian estimator (see Appendix A).
This equivalence can explain the sparsity-inducing terms (3.113) through
(3.116), introduced for sparse additive matrix factorization (SAMF) in Section
3.5. The ARD prior (7.23) induces low-rankness on U if no restriction on
BB⊤is imposed. We can similarly show that, (γe
l )2 in Eq. (3.114) corresponds
to the prior variance shared by the entries in ul
≡
γe
ldl
∈
RM, that
(γd
m)2 in Eq. (3.115) corresponds to the prior variance shared by the entries
in um
≡γd
mem
∈RL, and that E2
l,m in Eq. (3.116) corresponds to the
prior variance on Ul,m ≡El,mDl,m ∈R, respectively. This explains why the
factorization forms in Eqs. (3.113) through (3.116) induce low-rank, rowwise,
columnwise, and elementwise sparsity, respectively. If we employ the sparse
matrix factorization (SMF) term (3.117), ARD occurs in each partition, which
induces partitionwise sparsity and low-rank sparsity within each partition.

8
Performance Analysis of VB Matrix
Factorization
In this chapter, we further analyze the behavior of VB learning in the fully
observed MF model, introduced in Section 3.1. Then, we derive a theoretical
guarantee for rank estimation (Nakajima et al., 2015), which corresponds to
the hidden dimensionality selection in principal component analysis (PCA).
In Chapter 6, we derived an analytic-form solution (Theorem 6.13) of
EVB learning, where the prior variances are also estimated from observation.
When discussing the dimensionality selection performance in PCA, it is
more practical to assume that the noise variance σ2 is estimated, since it is
unknown in many situations. To this end, we ﬁrst analyze the behavior of the
noise variance estimator. After that, based on the random matrix theory, we
derive a theoretical guarantee of dimensionality selection performance, and
show numerical results validating the theory. We also discuss the relation to
an alternative dimensionality selection method (Hoyle, 2008) based on the
Laplace approximation.
In the following analysis, we use some results in Chapter 6. Speciﬁcally,
we mostly rely on Theorem 6.13 along with the corollaries and the equations
summarized in Section 6.10.
8.1 Objective Function for Noise Variance Estimation
Let us consider the complete empirical VB problem, where all the variational
parameters and the hyperparameters are estimated in the free energy minimiza-
tion framework:
min
{ah,bh,σ2ah,σ2
bh,c2ah,c2
bh}H
h=1,σ2 F
(8.1)
s.t.
{ah,bh ∈R,
σ2
ah, σ2
bh, c2
ah, c2
bh ∈R++}H
h=1, σ2 ∈R++.
205

206
8 Performance Analysis of VB Matrix Factorization
Here, the free energy is given by
2F = LM log(2πσ2) +
L
h=1 γ2
h
σ2
+
H

h=1
2Fh,
(8.2)
where
2Fh = M log
c2
ah
σ2ah
+ L log
c2
bh
σ2
bh
+
a2
h + Mσ2
ah
c2ah
+
b2
h + Lσ2
bh
c2
bh
−(L + M) +
−2ahbhγh +

a2
h + Mσ2
ah
 b2
h + Lσ2
bh
 
σ2
.
(8.3)
Note that we are focusing on the solution with diagonal posterior covariances
without loss of generality (see Theorem 6.4).
We have already obtained the empirical VB estimator (Theorem 6.13) and
the minimum free energy (Corollary 6.21) for given σ2. By using those results,
we can express the free energy (8.2) as a function of σ2. With the rescaled
expressions (6.132) through (6.138), the free energy can be written in a simple
form, which leads to the following theorem:
Theorem 8.1
The noise variance estimator, denoted by σ2 EVB, is the global
minimizer of
Ω(σ−2)

≡2F(σ−2)
LM
+ const.

= 1
L
⎛⎜⎜⎜⎜⎜⎝
H

h=1
ψ
⎛⎜⎜⎜⎜⎝
γ2
h
Mσ2
⎞⎟⎟⎟⎟⎠+
L

h=H+1
ψ0
⎛⎜⎜⎜⎜⎝
γ2
h
Mσ2
⎞⎟⎟⎟⎟⎠
⎞⎟⎟⎟⎟⎟⎠, (8.4)
where
ψ (x) = ψ0 (x) + θ

x > x
 
ψ1 (x) ,
(8.5)
ψ0 (x) = x −log x,
(8.6)
ψ1 (x) = log (τ(x; α) + 1) + α log
τ(x; α)
α
+ 1

−τ(x; α).
(8.7)
Here, x is given by
x =

1 + τ
 
1 + α
τ

,
(8.8)
where τ is deﬁned in Theorem 6.13, τ(x; α) is a function of x (> x) deﬁned by
τ(x; α) = 1
2

x −(1 + α) +
.
(x −(1 + α))2 −4α

,
(8.9)
and θ(·) denotes the indicator function such that θ(condition) = 1 if the
condition is true and θ(condition) = 0 otherwise.

8.2 Bounds of Noise Variance Estimator
207
Proof
By using Lemma 6.14 and Lemma 6.16, the free energy (8.2) can be
written as a function of σ2 as follows:
2F = LM log(2πσ2) +
L
h=1 γ2
h
σ2
+
H

h=1
θ

γh > γEVB 
FEVB−Posi
h
,
(8.10)
where FEVB−Posi
h
is given by Eq. (6.112). By using Eqs. (6.133) and (6.135),
Eq. (6.112) can be written as
FEVB−Posi
h
= M log (τh + 1) + L log
!τh
α + 1
"
−Mτh
= Mψ1(xh).
(8.11)
Therefore, Eq. (8.10) is written as
2F = M
)
L

h=1
log
⎛⎜⎜⎜⎜⎝
2πγ2
h
M
⎞⎟⎟⎟⎟⎠+
L

h=1
⎛⎜⎜⎜⎜⎝log
⎛⎜⎜⎜⎜⎝
Mσ2
γ2
h
⎞⎟⎟⎟⎟⎠+
γ2
h
Mσ2
⎞⎟⎟⎟⎟⎠
+
H

h=1
θ

γh > γEVB FEVB−Posi
h
M
1
= M
)
L

h=1
log
⎛⎜⎜⎜⎜⎝
2πγ2
h
M
⎞⎟⎟⎟⎟⎠+
L

h=1
ψ0(xh) +
H

h=1
θ

xh > x
 
ψ1(xh)
1
.
Note that the ﬁrst term in the curly braces is constant with respect to σ2. By
deﬁning
Ω = 2F
LM −1
L
L

h=1
log
⎛⎜⎜⎜⎜⎝
2πγ2
h
M
⎞⎟⎟⎟⎟⎠,
we obtain Eq. (8.4), which completes the proof of Theorem 8.1.
□
The functions ψ0 (x) and ψ (x) are depicted in Figure 8.1. We can conﬁrm
the convexity of ψ0 (x) and the quasiconvexity of ψ (x) (Lemma 8.4 in
Section 8.3), which are useful properties in the subsequent analysis.1
8.2 Bounds of Noise Variance Estimator
Let 
HEVB be the estimated rank by EVB learning, i.e., the rank of the EVB
estimator U
EVB, such that γEVB
h
> 0 for h = 1,. . . , 
HEVB, and γEVB
h
= 0 for
1 A function f : X →R on the domain X being a convex subset of a real vector space is said to
be quasiconvex if f(λx1 + (1 −λ)x2) ≤max( f(x1), f(x2)) for all x1, x2 ∈X and λ ∈[0, 1]. It is
furthermore said to be strictly quasiconxex if f(λx1 + (1 −λ)x2) < max( f(x1), f(x2)) for all
x1  x2 and λ ∈(0, 1). Intuitively, a strictly quasiconvex function does not have more than one
local minima.

208
8 Performance Analysis of VB Matrix Factorization
0
2
4
6
8
0
2
4
6
Figure 8.1 ψ0(x) and ψ(x).
h = 
HEVB + 1,. . . , H. By further analyzing the objective (8.4), we can derive
bounds of the estimated rank and the noise variance estimator:
Theorem 8.2

HEVB is upper-bounded as

HEVB ≤H = min
!D
L
1 + α
E
−1, H
"
,
(8.12)
and the noise variance estimator σ2 EVB is bounded as follows:
max
⎛⎜⎜⎜⎜⎜⎜⎝σ2
H+1,
L
h=H+1 γ2
h
M

L −H
 
⎞⎟⎟⎟⎟⎟⎟⎠≤σ2 EVB ≤
1
LM
L

h=1
γ2
h,
(8.13)
where
σ2
h =
⎧⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎩
∞
for h = 0,
γ2
h
Mx
for h = 1,. . . , L,
0
for h = L + 1.
(8.14)
Theorem 8.2 states that EVB learning discards the (L −⌈L/(1 + α)⌉+ 1)
smallest components, regardless of the observed singular values {γh}L
h=1. For
example, half of the components are always discarded when the matrix is
square (i.e., α = L/M = 1). The smallest singular value γL is always discarded,
and σ2 EVB ≥γ2
L/M always holds.
Given the EVB estimators {γEVB
h
}H
h=1 for the singular values, the noise
variance estimator σ2 EVB is speciﬁed by the following corollary:
Corollary 8.3
The EVB estimator for the noise variance satisﬁes the
following equality:
σ2 EVB =
1
LM
⎛⎜⎜⎜⎜⎜⎝
L

l=1
γ2
l −
H

h=1
γhγEVB
h
⎞⎟⎟⎟⎟⎟⎠.
(8.15)

8.3 Proofs of Theorem 8.2 and Corollary 8.3
209
This corollary can be used for implementing a global EVB solver (see
Chapter 9). In the next section we give the proofs of the theorem and the
corollary.
8.3 Proofs of Theorem 8.2 and Corollary 8.3
First, we show nice properties of the functions, ψ (x) and ψ0 (x), which are
deﬁned by Eqs. (8.5) and (8.6), respectively, and depicted in Figure 8.1:
Lemma 8.4
The following hold for x > 0: ψ0 (x) is differentiable and strictly
convex; ψ (x) is continuous and strictly quasiconvex; ψ (x) is differentiable
except x = x, at which ψ (x) has a discontinuously decreasing derivative, i.e.,
limx→x−0 ∂ψ/∂x > limx→x+0 ∂ψ/∂x; both of ψ0 (x) and ψ (x) are minimized at
x = 1. For x > x, ψ1 (x) is negative and decreasing.
Proof
Since
∂ψ0
∂x = 1 −1
x,
(8.16)
∂2ψ0
∂x2 = 1
x2 > 0,
ψ0(x) is differentiable and strictly convex for x > 0 with its minimizer at x = 1.
ψ1(x) is continuous for x ≥x, and Eq. (8.11) implies that ψ1(xh) ∝FEVB−Posi
h
.
Accordingly, ψ1(x) ≤0 for x ≥x, where the equality holds when x = x. This
equality implies that ψ(x) is continuous. Since x > 1, ψ(x) shares the same
minimizer as ψ0(x) at x = 1 (see Figure 8.1).
Hereafter, we investigate ψ1(x) and ψ(x) for x ≥x. By differentiating
Eqs. (8.7) and (6.135), respectively, we have
∂ψ1
∂τ = −
⎛⎜⎜⎜⎜⎜⎜⎝
τ2
α −1
(τ + 1)
 τ
α + 1
 
⎞⎟⎟⎟⎟⎟⎟⎠< 0,
(8.17)
∂τ
∂x = 1
2
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
1 +
x −(1 + α)
.
(x −(1 + α))2 −4α
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
> 0.
(8.18)
Substituting Eq. (6.134) into Eq. (8.18), we have
∂τ
∂x =
τ2
α
 τ2
α −1
 .
(8.19)

210
8 Performance Analysis of VB Matrix Factorization
Multiplying Eqs. (8.17) and (8.19) gives
∂ψ1
∂x = ∂ψ1
∂τ
∂τ
∂x = −
⎛⎜⎜⎜⎜⎜⎜⎝
τ2
α (τ + 1)
 τ
α + 1
 
⎞⎟⎟⎟⎟⎟⎟⎠= −τ
x < 0,
(8.20)
which implies that ψ1(x) is decreasing for x > x.
Let us focus on the thresholding point of ψ(x) at x = x. Eq. (8.20) does not
converge to zero for x →x + 0 but stay negative. On the other hand, ψ0(x) is
differentiable at x = x. Consequently, ψ (x) has a discontinuously decreasing
derivative, i.e., limx→x−0 ∂ψ/∂x > limx→x+0 ∂ψ/∂x, at x = x.
Finally, we prove the strict quasiconvexity of ψ(x). Taking the sum of
Eqs. (8.16) and (8.20) gives
∂ψ
∂x = ∂ψ0
∂x + ∂ψ1
∂x = 1 −1 + τ
x
= 1 −
1 + τ
1 + τ + α + ατ−1 > 0.
This means that ψ(x) is increasing for x > x. Since ψ0(x) is strictly convex and
increasing at x = x, and ψ(x) is continuous, ψ(x) is strictly quasiconvex. This
completes the proof of Lemma 8.4.
□
Lemma 8.4 implies that our objective (8.4) is a sum of quasiconvex
functions with respect to σ−2. Therefore, its minimizer can be bounded by
the smallest one and the largest one among the set collecting the minimizer
from each quasiconvex function:
Lemma 8.5
Ω(σ−2) has at least one global minimizer, and any of its local
minimizers is bounded as
M
γ2
1
≤σ−2 ≤M
γ2
L
.
(8.21)
Proof
The strict convexity of ψ0(x) and the strict quasiconvexity of ψ(x) also
hold for ψ0(γ2
hσ−2/M) and ψ(γ2
hσ−2/M) as functions of σ−2 (for γh > 0).
Because of the different scale factor γ2
h/M for each h = 1. . . , L, each of
ψ0(γ2
hσ−2/M) and ψ(γ2
hσ−2/M) has a minimizer at a different position:
σ−2 = M
γ2
h
.
The strict quasiconvexity of ψ0 and ψ guarantees that Ω(σ−2) is decreasing for
0 < σ−2 < M
γ2
1
,
(8.22)
and increasing for
M
γ2
L
< σ−2 < ∞.
(8.23)
This proves Lemma 8.5.
□

8.3 Proofs of Theorem 8.2 and Corollary 8.3
211
Ω(σ−2) has at most H nondifferentiable points, which come from the
nondifferentiable point x = x of ψ(x). The values
σ−2
h =
⎧⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎩
0
for h = 0,
Mx
γ2
h
for h = 1,. . . , L,
∞
for h = L + 1,
(8.24)
deﬁned in Eq. (8.14) for h = 1,. . . , H actually correspond to those points.
Lemma 8.4 states that, at x = x, ψ(x) has a discontinuously decreasing
derivative and neither ψ0(x) nor ψ(x) has a discontinuously increasing deriva-
tive at any point. Therefore, none of those nondifferentiable points can be a
local minimum. Consequently, we have the following lemma:
Lemma 8.6
Ω(σ−2) has no local minimizer at σ−2 = σ−2
h for h = 1,. . . , H,
and therefore any of its local minimizers is a stationary point.
Then, Theorem 6.13 leads to the following lemma:
Lemma 8.7
The estimated rank is 
H = h if and only if the inverse noise
variance estimator lies in the range
σ−2 ∈Bh ≡
,
σ−2; σ−2
h < σ−2 < σ−2
h+1
-
.
(8.25)
Figure 8.2 shows quasiconvex functions {ψ(γ2
hσ−2/M)}H
h=1 and their average
Ω(σ−2) in two exemplary cases for H = L. In the left case, the inverse noise
variance estimator σ−2 is smaller than the inverse threshold σ−2
1 for the largest
singular value, and therefore no EVB estimator γh is positive, i.e., 
H = 0. In
the right case, it holds that σ−2
1
< σ−2 < σ−2
2 , and therefore γ1 is positive and
the others are zero, i.e., 
H = 1.
We have the following lemma:
Figure 8.2 {ψ(γ2
hσ−2/M)}H
h=1 and Ω(σ−2) in two examplary cases for H = L. Left:
the case where γ2
h/M = 4, 3, 2 for h = 1, 2, 3. Right: the case where γ2
1/M = 30,
γ2
h/M = 6, 5.75, 5.5,. . . , 2.0 for h = 2,. . . , 18.

212
8 Performance Analysis of VB Matrix Factorization
Lemma 8.8
The derivative of Ω(σ−2) is given by
Θ ≡∂Ω
∂σ−2 = −σ2 +

H
h=1 γh

γh −˘γEVB
h
 
+ L
h=
H+1 γ2
h
LM
,
(8.26)
where 
H is a function of σ−2 deﬁned by

H = 
H(σ−2) = h
if
σ−2 ∈Bh.
(8.27)
Proof
The derivative of Eq. (8.4) with respect to σ−2 is given by
∂Ω
∂σ−2 = 1
L
⎛⎜⎜⎜⎜⎜⎝
H

h=1
γ2
h
M
∂ψ
∂x +
L

h=H+1
γ2
h
M
∂ψ0
∂x
⎞⎟⎟⎟⎟⎟⎠.
(8.28)
By using Eqs. (8.16) and (8.20), Eq. (8.28) can be written as
∂Ω
∂σ−2 = 1
L
⎛⎜⎜⎜⎜⎜⎝
L

h=1
γ2
h
M
∂ψ0
∂x +
H

h=1
θ

xh ≥x
 γ2
h
M
∂ψ1
∂x
⎞⎟⎟⎟⎟⎟⎠
= 1
L
⎛⎜⎜⎜⎜⎜⎝
L

h=1
γ2
h
M

1 −1
xh

−
H

h=1
θ

xh ≥x
 γ2
hτh
Mxh
⎞⎟⎟⎟⎟⎟⎠
=
L
h=1 γ2
h
LM
−σ2 −1
L
H

h=1
θ (τh ≥τ) σ2τh.
(8.29)
Here we also used the deﬁnition (6.132) of xh. Using Eq. (6.133), Eq. (8.29)
can be written as
∂Ω
∂σ−2 =
L
h=1 γ2
h
LM
−σ2 −
H

h=1
θ

γh ≥γEVB γh˘γEVB
h
LM
= −σ2 +
H
h=1 γh

γh −γEVB
h
 
+ L
h=H+1 γ2
h
LM
.
(8.30)
Here we also used the deﬁnition (6.101) of γEVB
h
. Using the deﬁnition (8.27)
and Lemma 8.7, we can replace γEVB
h
and H with ˘γEVB
h
and 
H, respectively,
which completes the proof of Lemma 8.8.
□
Note that Eq. (8.26) involves the shrinkage estimator ˘γEVB
h
, which is a
function of σ−2 (see Eq. (6.103)). For each hypothetical 
H, the solutions of
the equation
Θ = 0
(8.31)
lying in σ−2
∈B
H are stationary points, and hence candidates for the
global minimum. If we can solve Eq. (8.31) for all 
H = 1,. . . , H, we can
obtain the global solution by evaluating the objective (8.4) at each obtained

8.3 Proofs of Theorem 8.2 and Corollary 8.3
213
stationary point. However, solving Eq. (8.31) is computationally hard unless

H is small.2 Based on Lemma 8.8, we will obtain tighter bounds than
Lemma 8.5.
Since
γh −˘γEVB
h
> 0,
Eq. (8.26) is upper-bounded by
Θ ≤−σ2 +
L

h=1
γ2
h
LM ,
which leads to the upper-bound given in Eq. (8.13). Actually, if
⎛⎜⎜⎜⎜⎜⎝
L

h=1
γ2
h
LM
⎞⎟⎟⎟⎟⎟⎠
−1
∈B0,
then

H = 0,
σ2 =
L

h=1
γ2
h
LM ,
is a local minimum.
The following lemma is easily obtained from Eq. (6.103) by using the
inequalities z1 <
.
z2
1 −z2
2 < z1 −z2 for z1 > z2 > 0:
Lemma 8.9
For γh ≥γEVB, the EVB shrinkage estimator (6.103) can be
bounded as follows:
γh −(
√
M +
√
L)2σ2
γh
< ˘γEVB
h
< γh −(M + L)σ2
γh
.
(8.32)
This lemma is important for our analysis, because it allows us to bound the
most complicated part of Eq. (8.26) by quantities independent of γh, i.e.,
(M + L)σ2 < γh

γh −˘γEVB
h
 
< (
√
M +
√
L)2σ2.
(8.33)
Using Eq. (8.33), we obtain the following lemma:
Lemma 8.10
Any local minimizer exists in σ−2 ∈B
H such that

H <
L
1 + α,
2 It is easy to derive a closed-form solution for 
H = 0, 1.

214
8 Performance Analysis of VB Matrix Factorization
and the following holds for any local minimizer lying in σ−2 ∈B
H:
σ2 ≥
L
h=
H+1 γ2
h
LM −
H(M + L)
.
Proof
By substituting the lower-bound in Eq. (8.33) into Eq. (8.26), we obtain
Θ ≥−σ2 +

H(L + M)σ2 + L
h=
H+1 γ2
h
LM
.
This implies that Θ > 0 unless the following hold:

H <
LM
L + M =
L
1 + α,
σ2 ≥
L
h=
H+1 γ2
h
LM −
H(L + M)
.
Therefore, no local minimum exists if either of these conditions is violated.
This completes the proof of Lemma 8.10.
□
It holds that
L
h=
H+1 γ2
h
LM −
H(M + L)
≥
L
h=
H+1 γ2
h
M(L −
H)
,
(8.34)
of which the right-hand side is decreasing with respect to 
H. Combining
Lemmas 8.5, 8.6, 8.7, and 8.10 and Eq. (8.34) completes the proof of Theorem
8.2. Corollary 8.3 is easily obtained from Lemmas 8.6 and 8.8.
8.4 Performance Analysis
To analyze the behavior of the EVB solution in the fully observed MF model,
we rely on the random matrix theory (Marˇcenko and Pastur, 1967; Wachter,
1978; Johnstone, 2001; Bouchaud and Potters, 2003; Hoyle and Rattray, 2004;
Baik and Silverstein, 2006), which describes the distribution of the singular
values of random matrices in the limit when the matrix size goes to inﬁnity.
We ﬁrst introduce some results obtained in the random matrix theory and then
apply them to our analysis.
8.4.1 Random Matrix Theory
Assume that the observed matrix V is generated from the spiked covariance
model (Johnstone, 2001):
V = U∗+ E,
(8.35)

8.4 Performance Analysis
215
where U∗∈RL×M is a true signal matrix with rank H∗and singular values
{γ∗
h}H∗
h=1, and E
∈
RL×M is a random matrix such that each element is
independently drawn from a distribution with mean zero and variance σ∗2 (not
necessarily Gaussian). As the observed singular values {γh}L
h=1 of V, the true
singular values {γ∗
h}H∗
h=1 are also assumed to be arranged in the nonincreasing
order.
We deﬁne normalized versions of the observed and the true singular values:
yh =
γ2
h
Mσ∗2
for
h = 1,. . . , L,
(8.36)
ν∗
h =
γ∗2
h
Mσ∗2
for
h = 1,. . . , H∗.
(8.37)
In other words, {yh}L
h=1 are the eigenvalues of VV⊤/(Mσ∗2), and {ν∗
h}H∗
h=1 are
the eigenvalues of U∗U∗⊤/(Mσ∗2). Note the difference between xh, deﬁned by
Eq. (6.132), and yh: xh is the squared observed singular value normalized with
the model noise variance σ2, which is to be estimated, while yh is the one
normalized with the true noise variance σ∗2.
Deﬁne the empirical distribution of the observed eigenvalues {yh}L
h=1 by
p(y) = 1
L
L

h=1
δ(y −yh),
(8.38)
where δ(y) denotes the Dirac delta function. When H∗= 0, the observed matrix
V = E consists only of noise, and its singular value distribution in the large-
scale limit is speciﬁed by the following proposition:
Proposition 8.11
(Marˇcenko and Pastur, 1967; Wachter, 1978) In the large-
scale limit when L and M go to inﬁnity with its ratio α = L/M ﬁxed,
the empirical distribution of the eigenvalue y of EE⊤/(Mσ∗2) almost surely
converges to
p(y) →pMP(y) ≡
.
(y −y)(y −y)
2παy
θ(y < y < y),
(8.39)
where
y = (1 + √α)2,
y = (1 −√α)2,
(8.40)
and θ(·) is the indicator function, deﬁned in Theorem 8.1.3
Figure 8.3 shows Eq. (8.39), which we call the Marˇcenko–Pastur (MP)
distribution, for α = 0.1, 1. The mean ⟨y⟩pMP(y) = 1 (which is constant for
3 Convergence is in weak topology in distribution, i.e., p(y) almost surely converges to pMP(y) so
that

f(y)p(y)dy =

f(y)pMP(y)dy for any bounded continuous function f(y).

216
8 Performance Analysis of VB Matrix Factorization
Figure 8.3 Marˇcenko–Pastur distirbution.
any 0 < α ≤1) and the upper-limits y = y(α) of the support for α = 0.1, 1
are indicated by arrows. Proposition 8.11 states that the probability mass is
concentrated in the range between y ≤y ≤y. Note that the MP distribution
appears for a single sample matrix; differently from standard “large-sample”
theories, Proposition 8.11 does not require one to average over many sample
matrices.4 This single-sample property of the MP distribution is highly useful
in our analysis because the MF model usually assumes a single observed
matrix V. We call the (unnormalized) singular value corresponding to the
upper-limit y, i.e.,
γMPUL =
.
Mσ∗2 · y = (
√
L +
√
M)σ∗,
(8.41)
the Marˇcenko–Pastur upper limit (MPUL).
When H∗
>
0, the true signal matrix U∗affects the singular value
distribution of V. However, if H∗≪L, the distribution can be approximated
by a mixture of spikes (delta functions) and the MP distribution pMP(y). Let
H∗∗(≤H∗) be the number of singular values of U∗greater than γ∗
h >
α1/4 √
Mσ∗, i.e.,
ν∗
H∗∗> √α
and
ν∗
H∗∗+1 ≤√α.
(8.42)
Then, the following proposition holds:
Proposition 8.12
(Baik and Silverstein, 2006) In the large-scale limit when
L and M go to inﬁnity with ﬁnite α and H∗, it almost surely holds that
yh = ySig
h
≡

1 + ν∗
h
 
1 + α
ν∗
h

for
h = 1,. . . , H∗∗,
(8.43)
yH∗∗+1 = y,
and
yL = y.
4 This property is called self-averaging (Bouchaud and Potters, 2003).

8.4 Performance Analysis
217
Figure 8.4 Spiked covariance distribution when {ν∗
h}H∗∗
h=1 = {1.5, 1.0, 0.5}.
Furthermore, Hoyle and Rattray (2004) argued that, when L and M are
large (but ﬁnite) and H∗≪L, the empirical distribution of the eigenvalue y
of VV⊤/(Mσ∗2), is accurately approximated by
p(y) ≈pSC(y) ≡1
L
H∗∗

h=1
δ

y −ySig
h
 
+ L −H∗∗
L
pMP(y).
(8.44)
Figure 8.4 shows Eq. (8.44), which we call the spiked covariance (SC)
distribution, for α = 0.1, H∗∗= 3, and {ν∗
h}H∗∗
h=1 = {1.5, 1.0, 0.5}. The SC
distribution is irrespective of {ν∗
h}H∗
h=H∗∗+1, which satisfy 0 < ν∗
h ≤√α (see
the deﬁnition (8.42) of H∗∗).
Proposition 8.12 states that in the large-scale limit, the large signal compo-
nents such that ν∗
h > √α appear outside the support of the MP distribution
as spikes, while the other small signals are indistinguishable from the MP
distribution (note that Eq. (8.43) implies that ySig
h
> y for ν∗
h > √α). This
implies that any PCA method fails to recover the true dimensionality, unless
ν∗
H∗> √α.
(8.45)
The condition (8.45) requires that U∗has no small positive singular value such
that 0 < ν∗
h ≤√α, and therefore H∗∗= H∗.
The approximation (8.44) allows us to investigate more practical situations
where the matrix size is ﬁnite. In Sections 8.4.2 and 8.4.4, respectively, we
provide two theorems: one is based on Proposition 8.12 and guarantees perfect
rank (PCA dimensionality) recovery of EVB learning in the large-scale limit,
and the other one assumes that the approximation (8.44) exactly holds and
provides a more realistic condition for perfect recovery.

218
8 Performance Analysis of VB Matrix Factorization
8.4.2 Perfect Rank Recovery Condition in Large-Scale Limit
Now, we are almost ready for clarifying the behavior of the EVB solution. We
assume that the model rank is set to be large enough, i.e., H∗≤H ≤L, and all
model parameters including the noise variance are estimated from observation
(i.e., complete EVB learning). The last proposition on which our analysis relies
is related to the property, called the strong unimodality, of the log-concave
distributions:
Proposition 8.13
(Ibragimov, 1956; Dharmadhikari and Joag-Dev, 1988)
The convolution
g(s) = ⟨f(s + t)⟩p(t) =

f(s + t)p(t)dt
is quasiconvex, if p(t) is a log-concave distribution, and f(t) is a quasiconvex
function.
In the large-scale limit, the summation over h = 1,. . . , L in the objective
Ω(σ−2), given by Eq. (8.4), for noise variance estimation can be replaced
with the expectation over the MP distribution pMP(y). By scaling variables,
the objective can be written as a convolution with a scaled version of the
MP distribution, which turns out to be log-concave. Accordingly, we can use
Proposition 8.13 to show that Ω(σ−2) is quasiconvex, which means that the
noise variance estimation by EVB learning can be accurately performed by a
local search algorithm. Combining this result with Proposition 8.12, we obtain
the following theorem:
Theorem 8.14
In the large-scale limit when L and M go to inﬁnity with ﬁnite
α and H∗, EVB learning almost surely recovers the true rank, i.e., 
HEVB = H∗,
if and only if
ν∗
H∗≥τ,
(8.46)
where τ is deﬁned in Theorem 6.13.
Furthermore, the following corollary completely describes the behavior of
the EVB solution in the large-scale limit:
Corollary 8.15
In the large-scale limit, the objective Ω(σ−2), deﬁned by
Eq. (8.4), for the noise variance estimation converges to a quasiconvex
function, and it almost surely holds that
τEVB
h
⎛⎜⎜⎜⎜⎝≡γhγEVB
h
Mσ2 EVB
⎞⎟⎟⎟⎟⎠=
⎧⎪⎪⎨⎪⎪⎩
ν∗
h
if ν∗
h ≥τ,
0
otherwise,
(8.47)
σ2 EVB = σ∗2.

8.4 Performance Analysis
219
One may get intuition of Eqs. (8.46) and (8.47) by comparing Eqs. (8.8)
and (6.134) with Eq. (8.43): The estimator τh has the same relation to the
observation xh as the true signal ν∗
h, and hence is an unbiased estimator of
the signal. However, Theorem 8.14 does not even approximately hold in
practical situations with moderate-sized matrices (see the numerical validation
in Section 8.5). After proving Theorem 8.14 and Corollary 8.15, we will derive
a more practical condition for perfect recovery in Section 8.4.4.
8.4.3 Proofs of Theorem 8.14 and Corollary 8.15
In the large-scale limit, we can substitute the expectation ⟨f(y)⟩p(y) for the
summation L−1 L
h=1 f (yh). We can also substitute the MP distribution pMP(y)
for p(y) for the expectation, since the contribution from the H∗signal
components converges to zero. Accordingly, our objective (8.4) converges to
Ω(σ−2) →ΩLSL(σ−2) ≡
 y
κ
ψ

σ∗2σ−2y
 
pMP(y)dy +
 κ
y
ψ0

σ∗2σ−2y
 
pMP(y)dy
= ΩLSL−Full(σ−2) −
 κ
max(xσ2/σ∗2,y)
ψ1

σ∗2σ−2y
 
pMP(y)dy,
(8.48)
where
ΩLSL−Full(σ−2) ≡
 y
y
ψ

σ∗2σ−2y
 
pMP(y)dy,
(8.49)
and κ is a constant satisfying
H
L =
 y
κ
pMP(y)dy
(y ≤κ ≤y).
(8.50)
Note that x, y, and y are deﬁned by Eqs. (8.8) and (8.40), and it holds that
x > y.
(8.51)
We ﬁrst investigate Eq. (8.49), which corresponds to the objective for the
full-rank model (i.e., H = L). Let
s = log(σ−2),
t = log y

dt = 1
ydy
 
.
Then Eq. (8.49) is written as a convolution:
ΩLSL−Full(s) ≡ΩLSL−Full(es) =

ψ

σ∗2es+t 
etpMP(et)dt
=

ψ(s + t)pLSMP(t)dt,
(8.52)

220
8 Performance Analysis of VB Matrix Factorization
where
ψ(s) = ψ(σ∗2es),
pLSMP(t) = etpMP(et)
=
.
(et −y)(y −et)
2πα
θ(y < et < y).
(8.53)
Since Lemma 8.4 states that ψ(x) is quasiconvex, its composition ψ(s) with the
nondecreasing function σ∗2es is also quasiconvex.
The following holds for pLSMP(t), which we call a log-scaled MP (LSMP)
distribution:
Lemma 8.16
The LSMP distribution (8.53) is log-concave.
Proof
Focusing on the support,
log y < t < log y,
of the LSMP distribution (8.53), we deﬁne
f(t) ≡2 log pLSMP(t) = 2 log
.
(et −y)(y −et)
2πα
= log(−e2t + (y + y)et −yy) + const.
Let
u(t) ≡(et −y)(y −et) = −e2t + (y + y)et −yy > 0,
(8.54)
and let
v(t) ≡∂u
∂t = −2e2t + (y + y)et = u −e2t + yy,
w(t) ≡∂2u
∂t2 = −4e2t + (y + y)et = v −2e2t,
be the ﬁrst and the second derivatives of u. Then, the ﬁrst and the second
derivatives of f(t) are given by
∂f
∂t = v
u,
∂2 f
∂t2 = uw −v2
u2
= −
et 
(y + y)e2t −4yyet + (y + y)yy
 
u2

8.4 Performance Analysis
221
= −
et(y + y)
u2
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎝
⎛⎜⎜⎜⎜⎜⎝et −
2yy
(y + y)
⎞⎟⎟⎟⎟⎟⎠
2
+
yy

y −y
 2
(y + y)2
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎠
≤0.
This proves the log-concavity of the LSMP dsitribution pLSMP(t), and com-
pletes the proof of Lemma 8.16.
□
Lemma 8.16 and Proposition 8.13 imply that ΩLSL−Full(s) is quasiconvex,
and therefore its composition ΩLSL−Full(σ−2) with the nondecreasing function
log(σ−2) is quasiconvex. The minimizer of ΩLSL−Full(σ−2) can be found by
evaluating the derivative Θ, given by Eq. (8.26), in the large-scale limit:
ΘFull →ΘLSL−Full = −σ2 + σ∗2
 y
y
y · pMP(y)dy
−
 y
xσ2/σ∗2 τ(σ∗2σ−2y; α)pMP(y)dy.
(8.55)
Here, we used Eqs. (6.133) and (8.9). In the range
0 < σ−2 < xσ∗−2
y

i.e.,
xσ2
σ∗2 > y

,
(8.56)
the third term in Eq. (8.55) is zero. Therefore, Eq. (8.55) is increasing with
respect to σ−2, and zero when
σ2 = σ∗2
 y
y
y · pMP(y)dy = σ∗2.
Accordingly, ΩLSL−Full(σ−2) is strictly convex in the range (8.56). Eq. (8.51)
implies that the point σ−2 = σ∗−2 is contained in the region (8.56), and
therefore it is a local minimum of ΩLSL−Full(σ−2). Combined with the quasi-
convexity of ΩLSL−Full(σ−2), we have the following lemma:
Lemma 8.17
The objective ΩLSL−Full(σ−2) for the full-rank model H = L
in the large-scale limit is quasiconvex with its minimizer at σ−2 = σ∗−2. It is
strictly convex in the range (8.56).
For any κ (y < κ < y), the second term in Eq. (8.48) is zero in the
range (8.56), which includes its minimizer at σ−2 = σ∗−2. Since Lemma 8.4
states that ψ1(x) is decreasing for x > x, the second term in Eq. (8.48) is
nondecreasing in the region where

σ∗−2 <
 xσ∗−2
y
≤σ−2 < ∞.

222
8 Performance Analysis of VB Matrix Factorization
Therefore, the quasi-convexity of ΩLSL−Full is inherited to ΩLSL:
Lemma 8.18
The objective ΩLSL(σ−2) for noise variance estimation in the
large-scale limit is quasiconvex with its minimizer at σ−2 = σ∗−2. ΩLSL(σ−2)
is strictly convex in the range (8.56).
Thus we have proved that EVB learning accurately estimates the noise
variance in the large-scale limit:
σ2 EVB = σ∗2.
(8.57)
Assume that Eq. (8.45) holds. Then Proposition 8.12 guarantees that, in the
large-scale limit, the following hold:
γ2
H∗
Mσ∗2 ≡yH∗= $1 + ν∗
H∗% 
1 + α
ν∗
H∗

,
(8.58)
γ2
H∗+1
Mσ∗2 ≡yH∗+1 = y = (1 + √α)2.
(8.59)
Remember that the EVB threshold is given by Eq. (8.8), i.e.,
(γEVB)2
Mσ2 EVB ≡x =

1 + τ
 
1 + α
τ

.
(8.60)
Since Lemma 8.18 states that σ2 EVB = σ∗2, comparing Eqs. (8.58) and (8.59)
with Eq. (8.60) results in the following lemma:
Lemma 8.19
It almost surely holds that
γH∗≥γEVB
if and only if
ν∗
H∗≥τ,
(8.61)
γH∗+1 < γEVB
for any
{ν∗
h}.
This completes the proof of Theorem 8.14. Comparing Eqs. (6.134) and
(8.43) under Lemmas 8.18 and 8.19 proves Corollary 8.15.
□
8.4.4 Practical Condition for Perfect Rank Recovery
Theorem 8.14 rigorously holds in the large-scale limit. However, it does
not describe the behavior of the EVB solution very accurately in practical
ﬁnite matrix-size cases. We can obtain a more practical condition for perfect
recovery by relying on the approximation (8.44). We can prove the following
theorem:
Theorem 8.20
Let
ξ = H∗
L

8.4 Performance Analysis
223
be the relevant rank ratio, and assume that
p(y) = pSC(y).
(8.62)
Then, EVB learning recovers the true rank, i.e., 
HEVB = H∗, if the following
two inequalities hold:
ξ < 1
x,
(8.63)
ν∗
H∗>
 x−1
1−xξ −α
 
+
B x−1
1−xξ −α
 2 −4α
2
,
(8.64)
where x is deﬁned by Eq. (8.8).
Note that, in the large-scale limit, ξ converges to zero, and the sufﬁcient
condition, Eqs. (8.63) and (8.64), in Theorem 8.20 is equivalent to the
necessary and sufﬁcient condition (8.46) in Theorem 8.14.
Theorem 8.20 only requires that the SC distribution (8.44) well approxi-
mates the observed singular value distribution. Accordingly, it well describes
the dependency of the EVB solution on ξ, which will be shown in numerical
validation in Section 8.5. Theorem 8.20 states that, if the true rank H∗is small
enough compared with L and the smallest signal ν∗
H∗is large enough, EVB
learning perfectly recovers the true rank.
The following corollary also supports EVB learning:
Corollary 8.21
Under the assumption (8.62) and the conditions (8.63) and
(8.64), the objective Ω(σ−2) for the noise variance estimation has no local
minimum (no stationary point if ξ > 0) that results in a wrong estimated rank

HEVB  H∗.
This corollary states that, although the objective function (8.4) is nonconvex
and possibly multimodal in general, any local minimum leads to the correct
estimated rank. Therefore, perfect recovery does not require global search, but
only local search, for noise variance estimation, if L and M are sufﬁciently
large so that we can warrant Eq. (8.62).
In the next section, we give the proofs of Theorem 8.20 and Corollary 8.21,
and then show numerical experiments that support the theory.
8.4.5 Proofs of Theorem 8.20 and Corollary 8.21
We regroup the terms in Eq. (8.4) as follows:
Ω(σ−2) = Ω1(σ−2) + Ω0(σ−2),
(8.65)

224
8 Performance Analysis of VB Matrix Factorization
where
Ω1(σ−2) = 1
H∗
H∗

h=1
ψ
⎛⎜⎜⎜⎜⎝
γ2
h
M σ−2
⎞⎟⎟⎟⎟⎠,
(8.66)
Ω0(σ−2) =
1
L −H∗
⎛⎜⎜⎜⎜⎜⎝
H

h=H∗+1
ψ
⎛⎜⎜⎜⎜⎝
γ2
h
M σ−2
⎞⎟⎟⎟⎟⎠+
L

h=H+1
ψ0
⎛⎜⎜⎜⎜⎝
γ2
h
M σ−2
⎞⎟⎟⎟⎟⎠
⎞⎟⎟⎟⎟⎟⎠.
(8.67)
In the following, assuming that Eq. (8.62) holds and
yH∗> y,
(8.68)
we derive a sufﬁcient condition for any local minimizer to lie only in σ−2 ∈
BH∗, with which Lemma 8.7 proves Theorem 8.20.
Under the assumption (8.62) and the condition (8.68), Ω0(σ−2), deﬁned by
Eq. (8.67), is equivalent to the objective ΩLSL(σ−2) in the large-scale limit.
Using Lemma 8.18, and noting that
σ−2
H∗+1 =
Mx
γH∗+1
2
= xσ∗−2
y
> σ∗−2,
(8.69)
we have the following lemma:
Lemma 8.22
Ω0(σ−2) is quasiconvex with its minimizer at
σ−2 = σ∗−2.
Ω0(σ−2) is strictly convex in the range
0 < σ−2 < σ−2
H∗+1.
Using Lemma 8.22 and the strict quasiconvexity of ψ(x), we can deduce the
following lemma:
Lemma 8.23
Ω(σ−2) is nondecreasing (increasing if ξ > 0) in the range
σ2
H∗+1 < σ−2 < ∞.
Proof
Lemma 8.22 states that Ω0(σ−2), deﬁned by Eq. (8.67), is quasiconvex
with its minimizer at
σ−2 =
⎛⎜⎜⎜⎜⎜⎝
L
h=H∗+1 γ2
h
(L −H∗)M
⎞⎟⎟⎟⎟⎟⎠
−1
= σ∗−2.
Since Ω1(σ−2), deﬁned by Eq. (8.66), is the sum of strictly quasiconvex
functions with their minimizers at σ−2 = M/γ2
h < σ∗−2 for h = 1,. . . , H∗,
our objective Ω(σ−2), given by Eq. (8.65), is nondecreasing (increasing if
H∗> 0) for
σ−2 ≥σ∗−2.

8.4 Performance Analysis
225
Since Eq. (8.69) implies that σ−2
H∗+1 > σ∗−2, Ω(σ−2) is nondecreasing
(increasing if ξ > 0) for σ−2
> σ−2
H∗+1, which completes the proof of
Lemma 8.23.
□
Using the bounds given by Eq. (8.33) and Lemma 8.22, we also obtain the
following lemma:
Lemma 8.24
Ω(σ−2) is increasing at σ−2 = σ2
H∗+1 −0.5 It is decreasing at
σ−2 = σ2
H∗+ 0 if the following hold:
ξ <
1
(1 + √α)2 ,
(8.70)
yH∗>
x(1 −ξ)
1 −ξ(1 + √α)2 .
(8.71)
Proof
Lemma 8.22 states that Ω0(σ−2) is strictly convex in the range 0 <
σ−2 < σ2
H∗+1, and minimized at σ−2 = σ∗−2. Since Eq. (8.69) implies
that σ∗−2
< σ2
H∗+1, Ω0(σ−2) is increasing at σ−2
= σ2
H∗+1 −0. Since
Ω1(σ−2) is the sum of strictly quasiconvex functions with their minimizers
at σ−2 = M/γ2
h < σ∗−2 for h = 1,. . . , H∗, Ω(σ−2) is also increasing at
σ−2 = σ2
H∗+1 −0.
Let us investigate the sign of the derivative Θ of Ω(σ−2) at σ−2 = σ2
H∗+ 0 ∈
BH∗. Substituting the upper-bound in Eq. (8.33) into Eq. (8.26), we have
Θ < −σ2 + H∗(
√
L +
√
M)2σ2 + L
h=H∗+1 γ2
h
LM
= −σ2 + H∗(
√
L +
√
M)2σ2 + (L −H∗)Mσ∗2
LM
.
(8.72)
The right-hand side of Eq. (8.72) is negative if the following hold:
ξ = H∗
L <
M
(
√
L +
√
M)2 =
1
(1 + √α)2 ,
(8.73)
σ2 >
(L −H∗)Mσ∗2
LM −H∗(
√
L +
√
M)2 =
(1 −ξ)σ∗2
1 −ξ(1 + √α)2 .
(8.74)
Assume that the ﬁrst condition (8.73) holds. Then the second condition
(8.74) holds at σ−2 = σ2
H∗+ 0, if
σ−2
H∗< 1 −ξ(1 + √α)2
(1 −ξ)
σ∗−2,
5 By “−0” we denote an arbitrarily large negative value.

226
8 Performance Analysis of VB Matrix Factorization
or equivalently,
yH∗=
γ2
H∗
Mσ∗2 = x · σ2
H∗
σ∗2 >
x(1 −ξ)
1 −ξ(1 + √α)2 ,
which completes the proof of Lemma 8.24.
□
Finally, we obtain the following lemma:
Lemma 8.25
Ω(σ−2) is decreasing in the range 0 < σ−2 < σ2
H∗if the
following hold:
ξ < 1
x,
(8.75)
yH∗> x(1 −ξ)
1 −xξ .
(8.76)
Proof
In the range 0 < σ−2 < σ2
H∗, the estimated rank (8.27) is bounded as
0 ≤
H ≤H∗−1.
(8.77)
Substituting the upper-bound in Eq. (8.33) into Eq. (8.26), we have
Θ < −σ2 +

H(
√
L +
√
M)2σ2 + H∗
h=
H+1 γ2
h + L
h=H∗+1 γ2
h
LM
= −σ2 +

H(
√
L +
√
M)2σ2 + H∗
h=
H+1 γ2
h + (L −H∗)Mσ∗2
LM
.
(8.78)
The right-hand side of Eq. (8.78) is negative, if the following hold:

H
L <
M
(
√
L +
√
M)2 =
1
(1 + √α)2 ,
(8.79)
σ2 >
H∗
h=
H+1 γ2
h + (L −H∗)Mσ∗2
LM −
H(
√
L +
√
M)2
.
(8.80)
Assume that
ξ = H∗
L <
1
(1 + √α)2 .
Then both of the conditions (8.79) and (8.80) hold for σ−2 ∈(0, σ2
H∗), if the
following holds:
σ−2

H+1 <
LM −
H(
√
L +
√
M)2
H∗
h=
H+1 γ2
h + (L −H∗)Mσ∗2
for

H = 0,. . . , H∗−1. (8.81)

8.4 Performance Analysis
227
Since the sum H∗
h=
H+1 γ2
h in the right-hand side of Eq. (8.81) is upper-
bounded as
H∗

h=
H+1
γ2
h ≤(H∗−
H)γ2

H+1,
Eq. (8.81) holds if
σ−2

H+1 <
LM −
H(
√
L +
√
M)2
(H∗−
H)γ2

H+1 + (L −H∗)Lσ∗2
=
1 −
H
L (1 + √α)2
(ξ −
H
L )
γ2

H+1
M + (1 −ξ)σ∗2
for

H = 0,. . . , H∗−1.
(8.82)
Using Eq. (8.24), the condition (8.82) is rewritten as
γ2

H+1
Mx > (ξ −
H
L )
γ2

H+1
M + (1 −ξ)σ∗2
1 −
H
L (1 + √α)2
⎛⎜⎜⎜⎜⎝1 −

H
L (1 + √α)2
⎞⎟⎟⎟⎟⎠
γ2

H+1
Mσ∗2 >
⎛⎜⎜⎜⎜⎝ξx −

H
L x
⎞⎟⎟⎟⎟⎠
γ2

H+1
Mσ∗2 + (1 −ξ)x,
or equivalently
y
H+1 =
γ2

H+1
Mσ∗2 >
(1 −ξ) x

1 −ξx + 
H
L

x −(1 + √α)2  
for

H = 0,. . . , H∗−1.
(8.83)
Note that x > y = (1 + √α)2. Further bounding both sides, we have the
following sufﬁcient condition for Eq. (8.83) to hold:
yH∗>
(1 −ξ)x
max

0, 1 −ξx
 .
(8.84)
Thus we obtain the conditions (8.75) and (8.76) for Θ to be negative for σ−2 ∈
(0, σ2
H∗), which completes the proof of Lemma 8.25.
□
Lemmas 8.23, 8.24, and 8.25 together state that, if all the conditions (8.68)
and (8.70) through (8.76) hold, at least one local minimum exists in the correct
range σ−2 ∈BH∗, and no local minimum (no stationary point if ξ > 0) exists
outside the correct range. Therefore, we can estimate the correct rank 
HEVB =
H∗by using a local search algorithm for noise variance estimation. Choosing
the tightest conditions, we have the following lemma:

228
8 Performance Analysis of VB Matrix Factorization
Lemma 8.26
Ω(σ−2) has a global minimum in σ−2 ∈BH∗, and no local
minimum (no stationary point if ξ > 0) outside BH∗, if the following hold:
ξ < 1
x,
yH∗=
γ2
H∗
Mσ∗2 > x(1 −ξ)
1 −xξ .
(8.85)
Using Eq. (8.43), Eq. (8.85) can be written with the true signal amplitude
as follows:
$1 + ν∗
H∗% 
1 + α
ν∗
H∗

−x(1 −ξ)
1 −xξ > 0.
(8.86)
The left-hand side of Eq. (8.86) can be factorized as follows:
1
ν∗
H∗
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
ν∗
H∗−
 x(1−ξ)
1−xξ −(1 + α)
 
+
B x(1−ξ)
1−xξ −(1 + α)
 2 −4α
2
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
·
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
ν∗
H∗−
 x(1−ξ)
1−xξ −(1 + α)
 
−
B x(1−ξ)
1−xξ −(1 + α)
 2 −4α
2
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
> 0. (8.87)
When Eq. (8.45) holds, the last factor in the left-hand side in Eq. (8.87) is
positive. Therefore, we have the following condition:
ν∗
H∗>
 x(1−ξ)
1−xξ −(1 + α)
 
+
B x(1−ξ)
1−xξ −(1 + α)
 2 −4α
2
=
 x−1
1−xξ −α
 
+
B x−1
1−xξ −α
 2 −4α
2
.
(8.88)
Lemma 8.26 with the condition (8.85) replaced with the condition (8.88) leads
to Theorem 8.20 and Corollary 8.21.
8.5 Numerical Veriﬁcation
Figure 8.5 shows numerical simulation results for M
= 200 and L =
20, 100, 200. E was drawn from the independent Gaussian distribution with
mean 0 and variance σ∗2 = 1, and true signal singular values {γ∗
h}H∗
h=1 were

8.5 Numerical Veriﬁcation
229
0
1
2
3
4
5
0
0.5
1
Success rate
(a) L = 20
0
1
2
3
4
5
0
0.5
1
Success rate
(b) L = 100
0
1
2
3
4
5
0
0.5
1
Success rate
(c) L = 200
Figure 8.5 Success rate of rank recovery in numerical simulation for M = 200.
The horizontal axis indicates the lower limit of the support of the simulated
true signal distribution, i.e., z ≈Cν∗
H∗. The recovery condition (8.64) for ﬁnite-
sized matrices is indicated by a vertical bar with the same line style for each ξ.
The leftmost vertical bar, which corresponds to the condition (8.64) for ξ = 0,
coincides with the recovery condition (8.46) for inﬁnite-sized matrices.
drawn from the uniform distribution on [z
√
Mσ∗, 10
√
Mσ∗] for different z,
which is indicated by the horizontal axis. We used Algorithm 16, which will
be introduced in Chapter 9, to compute the global EVB solution.
The vertical axis indicates the success rate of rank recovery over 100 trials,
i.e., the proportion of the trials giving 
HEVB = H∗. If the condition (8.63) on
ξ is violated, the corresponding curve is depicted with markers. Otherwise, the
condition (8.64) on ν∗
H∗(= γ∗2
H∗/(Mσ∗2)) is indicated by a vertical bar with the
same line style for each ξ. In other words, Theorem 8.20 states that the success
rate should be equal to one if z (> γ∗
H∗/(
√
Mσ∗2)) is larger than the value
indicated by the vertical bar. The leftmost vertical bar, which corresponds to
the condition (8.64) for ξ = 0, coincides with the recovery condition (8.46),
given by Theorem 8.14, for inﬁnite-sized matrices.
We see that Theorem 8.20 with the condition (8.64) approximately holds for
these moderate-sized matrices, while Theorem 8.14 with the condition (8.46),
which does not depend on the relevant rank ratio ξ, immediately breaks for
positive ξ.

230
8 Performance Analysis of VB Matrix Factorization
8.6 Comparison with Laplace Approximation
Here, we compare EVB learning with an alternative dimensionality selection
method (Hoyle, 2008) based on the Laplace approximation (LA). Consider the
PCA application, where D denotes the dimensionality of the observation space,
and N denotes the number of samples, i.e., in our MF notation to keep L ≤M,
L = D, M = N
if
D ≤N,
L = N, M = D
if
D > N.
(8.89)
Right after Tipping and Bishop (1999) proposed the probabilistic PCA,
Bishop (1999a) proposed to select the PCA dimensionality by maximizing the
marginal likelihood:
p(V) = ⟨p(V|A, B)⟩p(A)p(B) .
(8.90)
Since the marginal likelihood (8.90) is computationally intractable, he approxi-
mated it by LA, and suggested Gibbs sampling and VB learning as alternatives.
The VB variant, of which the model is almost the same as the MF deﬁned by
Eqs. (6.1) through (6.3), was also proposed by himself (Bishop, 1999b) along
with a standard local solver similar to Algorithm 1 in Chapter 3.
The LA-based approach was polished in Minka (2001a), by introducing a
conjugate prior6 on B to p(V|B) = ⟨p(V|A, B)⟩p(A), and ignoring the non-
leading terms that do not grow fast as the number N of samples goes to inﬁnity.
Hoyle (2008) pointed out that Minka’s method is inaccurate when D ≫N, and
proposed the overlap (OL) method, a further polished variant of the LA-based
approach. A notable difference of the OL method from most of the LA-based
methods is that the OL method applies LA around a more accurate estimator
than the MAP estimator.7 Thanks to the use of the accurate estimator, the OL
method behaves optimally in the large-scale limit when D and N go to inﬁnity,
while Minka’s method does not. We will clarify the meaning of the optimality
and discuss it in more detail in Section 8.7.
The OL method minimizes an approximation to the negative logarithm of
the marginal likelihood (8.90), which depends on estimators for λh = b2
h + σ2
and σ2 computed by an iterative algorithm, over the hypothetical model rank
H = 0,. . . , L (see Appendix C for the detailed computational procedure).
Figure 8.6 shows numerical simulation results that compare EVB learning and
the OL method: Figure 8.6(a) shows the success rate for the no-signal case
ξ = 0 (H∗= 0), while Figures 8.6(b) through 8.6(f) show the success rate for
ξ = 0.05 and D = 20, 100, 200, 400, and 1, 000, respectively. We also show
6 This conjugate prior does not satisfy the implicit requirement, footnoted in Section 1.2.4, that
the moments of the family member can be computed analytically.
7 As explained in Section 2.2.1, LA is usually applied around the MAP estimator.

8.6 Comparison with Laplace Approximation
231
0
0.2
0.4
0.6
0.8
1
1.2
EVB
OL
Local-EVB
Success rate
(a) ξ = 0
0
1
2
3
4
5
0
0.5
1
Success rate
Local-EVB
(b) ξ = 0.05, D = 20
0
3
4
5
0
0.5
1
2
1
Success rate
Local-EVB
(c) ξ = 0.05, D = 100
0
1
2
3
4
5
0
0.5
1
Success rate
Local-EVB
(d) ξ = 0.05, D = 200
0
1
2
3
4
5
0
0.5
1
Success rate
Local-EVB
(e) ξ = 0.05, D = 400
0
0.5
1
0
1
2
3
4
5
Success rate
Local-EVB
(f) ξ = 0.05, D = 1, 000
Figure 8.6 Success rate of PCA dimensionality recovery by (global) EVB
learning, the OL method, and the local-EVB estimator for N = 200. Vertical bars
indicate the recovery conditions, Eq. (8.46) for EVB learning, and Eq. (8.95) for
the OL method and the local-EVB estimator, in the large-scale limit.
the performance of the local-EVB estimator (6.131), which was computed by
a local solver (Algorithm 18 introduced in Chapter 9). For the OL method
and the local-EVB estimator, we initialized the noise variance estimator to
10−4 · L
h=1 γ2
h/(LM).
In comparison with the OL method, EVB learning shows its conservative
nature: It exhibits almost zero false positive rate (Figure 8.6(a)) at the expense
of low sensitivity (Figures 8.6(c) through 8.6(f)). Actually, because of its low
sensitivity, EVB learning does not behave optimally in the large-scale limit.
The local-EVB estimator, on the other hand, behaves similarly to the OL
method, for which the reason will be elucidated in the next section.

232
8 Performance Analysis of VB Matrix Factorization
8.7 Optimality in Large-Scale Limit
Consider the large-scale limit, and assume that the model rank H is set to be
large enough but ﬁnite so that H ≥H∗and H/L →0. Then the rank estimation
procedure, detailed in Appendix C, by the OL method is reduced to counting
the number of components such that λOL−LSL
h
> σ2 OL−LSL, i.e.,

HOL−LSL =
L

h=1
θ
λOL−LSL
h
> σ2 OL−LSL 
,
(8.91)
where θ(·) is the indicator function deﬁned in Theorem 8.1. Here λOL−LSL
h
and
σ2 OL−LSL are computed by iterating the following updates until convergence:
λOL−LSL
h
=
⎧⎪⎪⎨⎪⎪⎩
˘λOL−LSL
h
if γh ≥γlocal−EVB,
σ2 OL−LSL
otherwise,
(8.92)
σ2 OL−LSL =
1
(M −H)
⎛⎜⎜⎜⎜⎜⎝
L

l=1
γ2
l
L −
H

h=1
λOL−LSL
h
⎞⎟⎟⎟⎟⎟⎠,
(8.93)
where
˘λOL−LSL
h
= γ2
h
2L

1 −(M −L)σ2 OL−LSL
γ2
h
+
>
@⎛⎜⎜⎜⎜⎝1 −(M −L)σ2 OL−LSL
γ2
h
⎞⎟⎟⎟⎟⎠
2
−4Lσ2 OL−LSL
γ2
h

.
(8.94)
The OL method evaluates its objective, which approximates the negative
logarithm of the marginal likelihood (8.90), after the updates (8.92) and (8.93)
converge for each hypothetical H, and adopts the minimizer 
HOL−LSL as the
rank estimator. However, Hoyle (2008) proved that, in the large-scale limit, the
objective decreases as H increases, as long as Eq. (8.94) is a real number (or
equivalently γh ≥γlocal−EVB holds) for all h = 1,. . . , H at the convergence.
Accordingly, Eq. (8.91) holds.
Interestingly, the threshold in Eq. (8.92) coincides with the local-EVB
threshold (6.127). Moreover, the updates (8.92) and (8.93) for the OL method
are equivalent to the updates (9.29) and (9.30) for the local-EVB estimator
(Algorithm 18) with the following correspondence:
λOL−LSL
h
= γhγlocal−EVB
h
L
+ σ2 local−EVB,
σ2 OL−LSL = σ2 local−EVB.
Thus, the rank estimation procedure by the OL method and that by the local-
EVB estimator are equivalent, and therefore 
HOL−LSL = 
Hlocal−EVB in the large-
scale limit.

8.7 Optimality in Large-Scale Limit
233
If the noise variance is accurately estimated, i.e., σ2 = σ∗2, the threshold
γlocal−EVB both for the OL method and the local-EVB estimator coincides with
the MPUL (8.41), which corresponds to the minimum detectable observed
singular value. By using this fact, the optimality of the OL method in the large-
scale limit was shown:
Proposition 8.27
(Hoyle, 2008) In the large-scale limit, when L and M go to
inﬁnity with ﬁnite α, H∗, and H (≥H∗)8, the OL method almost surely recovers
the true rank, i.e., 
HOL−LSL = H∗, if and only if
ν∗
H∗> √α.
(8.95)
It almost surely holds that
λOL−LSL
h
σ2 OL−LSL −1 = ν∗
h,
σ2 OL−LSL = σ∗2.
The condition (8.95) coincides with the condition (8.45), which any PCA
method requires for perfect dimensionality recovery. In this sense, the OL
method, as well as the local-EVB estimator, is optimal in the large-scale limit.
On the other hand, Theorem 8.14 implies that (global) EVB learning is
not optimal in the large-scale limit but more conservative (see the difference
between τ and √α in Figure 6.4). In Figure 8.6, the conditions for perfect
dimensionality recovery in the large-scale limit are indicated by vertical bars:
z = √τ for EVB, and z =
.
τlocal = α1/4 for OL and local-EVB.
All methods accurately estimate the noise variance in the large-scale
limit, i.e.,
σ2 EVB = σ2 OL−LSL = σ2 local−EVB = σ∗2.
Taking this into account, we indicate the recovery conditions in Figure 8.4 by
arrows at
y = x for EVB, and y = xlocal(= y) for OL and local-EVB,
respectively. Figure 8.4 implies that, in this particular case, EVB learning
discards the third spike coming from the third true signal ν∗
3 = 0.5, while the
OL method and the local-EVB estimator successfully capture it as a signal.
When the matrix size is ﬁnite, the conservative nature of EVB learning is
not always bad, since it offers almost zero false positive rate, which makes
8 Unlike our analysis in Section 8.4, Hoyle (2008) assumed H/L →0 to prove that the OL
method accurately estimates the noise variance.

234
8 Performance Analysis of VB Matrix Factorization
(a) V = 1.0
(b) V = 1.05
(c) V = 1.5
(d) V = 2.1
Figure 8.7 The VB free energy contribution (6.55) from the (ﬁrst) component
and its counterpart (8.96) of Bayesian learning for L = M = H = 1 and σ2 = 1.
Markers indicate the local minima.
Theorem 8.20 approximately hold for ﬁnite cases, as seen in Figures 8.5
and 8.6. However, the fact that not (global) EVB learning but the local-
EVB estimator is optimal in the large-scale limit might come from inaccurate
approximation to the Bayes posterior by the VB posterior. Having this in mind,
we discuss the difference between VB learning and full Bayesian learning in
the remainder of this section.
Figure 8.7 shows the VB free energy contribution (6.55) from the (ﬁrst)
component as a function of cacb, and its counterpart of Bayesian learning:
2FBayes
1
= −2 log ⟨p(V|A, B)⟩p(A)p(B) −

log(2πσ2) + V2
σ2

,
(8.96)
which was numerically computed. We see that the minimizer (shown as a
diamond) of the Bayes free energy is at cacb →+0 until V exceeds 1.
The difference in behavior between EVB learning and the local-EVB
estimator appears in the nonempty range of the observed value V where the
positive local solution exists but gives positive free energy. Figure 8.7(d)
shows this case, where a bump exists between two local minima (indicated by
crosses). On the other hand, such multimodality is not observed in empirical

8.7 Optimality in Large-Scale Limit
235
full Bayesian learning (see the dashed curves in Figures 8.7(a) through 8.7(d)).
We can say that this multimodality in EVB learning with a bump between
two local minima is induced by the independence constraint for VB learning.
We further guess that it is this bump that pushes the EVB threshold from
the optimal point (at the local-EVB threshold) to a larger value. Further
investigation is necessary to fully understand this phenomenon.

9
Global Solver for Matrix Factorization
The analytic-form solutions, derived in Chapter 6, for VB learning and EVB
learning in fully observed MF can naturally be used to develop efﬁcient
and reliable VB solvers. Some properties, shown in Chapter 8, can also be
incorporated when the noise variance is unknown and to be estimated.
In this chapter, we introduce global solvers for VB learning and EVB
learning (Nakajima et al., 2013a, 2015), and how to extend them to more
general cases with missing entries and nonconjugate likelihoods (Seeger and
Bouchard, 2012).
9.1 Global VB Solver for Fully Observed MF
We consider the MF model, introduced in Section 3.1:
p(V|A, B) ∝exp

−1
2σ2
###V −BA⊤###2
Fro

,
(9.1)
p(A) ∝exp

−1
2tr

AC−1
A A⊤ 
,
(9.2)
p(B) ∝exp

−1
2tr

BC−1
B B⊤ 
,
(9.3)
where V ∈RL×M is an observed matrix;
A = (a1,. . . , aH) = $a1,. . . ,aM
%⊤∈RM×H,
B = (b1,. . . , bH) =
b1,. . . ,bL
 ⊤∈RL×H,
236

9.1 Global VB Solver for Fully Observed MF
237
are parameter matrices; and CA, CB, and σ2 are hyperparameters. The prior
covariance hyperparameters are restricted to be diagonal:
CA = Diag(c2
a1,. . . , c2
aH),
CB = Diag(c2
b1,. . . , c2
bH).
Our VB solver gives the global solution to the following minimization
problem,
r = argmin
r
F(r)
s.t.
r(A, B) = rA(A)rB(B),
(9.4)
of the free energy
F(r) =
/
log
rA(A)rB(B)
p(V|A, B)p(A)p(B)
0
rA(A)rB(B)
.
(9.5)
Assume that L ≤M without loss of generality, and let
V =
L

h=1
γhωbhω⊤
ah
(9.6)
be the singular value decomposition (SVD) of the observed matrix V ∈RL×M.
According to Theorem 6.7, the VB solution is given by
U
VB = BA =
H

h=1
γVB
h ωbhω⊤
ah,
where
γVB
h
=
⎧⎪⎪⎨⎪⎪⎩
˘γVB
h
if γh ≥γVB
h ,
0
otherwise,
(9.7)
for
γVB
h
= σ
>
?
?
?
@
(L + M)
2
+
σ2
2c2ahc2
bh
+
>
?
@⎛⎜⎜⎜⎜⎜⎝
(L + M)
2
+
σ2
2c2ahc2
bh
⎞⎟⎟⎟⎟⎟⎠
2
−LM,
(9.8)
˘γVB
h
= γh
⎛⎜⎜⎜⎜⎜⎜⎜⎝1 −σ2
2γ2
h
⎛⎜⎜⎜⎜⎜⎜⎜⎝M + L +
>
@
(M −L)2 +
4γ2
h
c2ahc2
bh
⎞⎟⎟⎟⎟⎟⎟⎟⎠
⎞⎟⎟⎟⎟⎟⎟⎟⎠.
(9.9)
Corollary 6.8 completely speciﬁes the VB posterior, which is written as
r(A, B) =
H

h=1
GaussM(ah;ahωah, σ2
ah IM)
H

h=1
GaussL(bh;bhωbh, σ2
bhIL)
(9.10)
with the following variational parameters: if γh > γVB
h ,
ah = ±
.
˘γVB
h δVB
h ,
bh = ±
>
@
˘γVB
h
δVB
h
,
σ2
ah = σ2δVB
h
γh
,
σ2
bh =
σ2
γhδVB
h
, (9.11)

238
9 Global Solver for Matrix Factorization
Algorithm 15 Global VB solver for fully observed matrix factorization.
1: Transpose V →V⊤if L > M, and set H (≤L) to a sufﬁciently large value.
2: Compute the SVD (9.6) of V.
3: Apply Eqs. (9.7) through (9.9) to get the VB estimator.
4: If necessary, compute the variational parameters by using Eqs. (9.11)
through (9.14), which specify the VB posterior (9.10). We can also
evaluate the free energy by using Eqs. (9.15) and (9.16).
where
δVB
h

≡ah
bh

= cah
σ2

γh −˘γVB
h
−Lσ2
γh

,
(9.12)
and otherwise,
ah = 0,
bh = 0,
σ2
ah = c2
ah
⎛⎜⎜⎜⎜⎜⎝1 −LζVB
h
σ2
⎞⎟⎟⎟⎟⎟⎠,
σ2
bh = c2
bh
⎛⎜⎜⎜⎜⎜⎝1 −MζVB
h
σ2
⎞⎟⎟⎟⎟⎟⎠, (9.13)
where
ζVB
h

≡σ2
ahσ2
bh
 
=
σ2
2LM
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝L + M +
σ2
c2ahc2
bh
−
>
?
@⎛⎜⎜⎜⎜⎜⎝L + M +
σ2
c2ahc2
bh
⎞⎟⎟⎟⎟⎟⎠
2
−4LM
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠.
(9.14)
The free energy can be written as
2F = LM log(2πσ2) +
L
h=1 γ2
h
σ2
+
H

h=1
2Fh,
(9.15)
where
2Fh = M log
c2
ah
σ2ah
+ L log
c2
bh
σ2
bh
+
a2
h + Mσ2
ah
c2ah
+
b2
h + Lσ2
bh
c2
bh
−(L + M) +
−2ahbhγh +

a2
h + Mσ2
ah
 b2
h + Lσ2
bh
 
σ2
.
(9.16)
Based on these results, we can straightforwardly construct a global solver
for VB learning, which is given in Algorithm 15.
9.2 Global EVB Solver for Fully Observed MF
EVB learning, where the hyperparameters CA, CA, and σ2 are also estimated
from observation, solves the following minimization problem,
r = argmin
r,CA,CA,σ2F
s.t.
r(A, B) = rA(A)rB(B),
(9.17)
of the free energy (9.5).

9.2 Global EVB Solver for Fully Observed MF
239
According to Theorem 6.13, given the noise variance σ2, the EVB solution
can be written as
U
EVB =
H

h=1
γEVB
h
ωbhω⊤
ah,
where
γEVB
h
=
⎧⎪⎪⎨⎪⎪⎩
˘γEVB
h
if γh ≥γEVB,
0
otherwise,
(9.18)
for
γEVB = σ
A
M

1 + τ
 
1 + α
τ

,
˘γEVB
h
= γh
2
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎝1 −(M + L)σ2
γ2
h
+
>
@⎛⎜⎜⎜⎜⎝1 −(M + L)σ2
γ2
h
⎞⎟⎟⎟⎟⎠
2
−4LMσ4
γ4
h
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎠.
Here
α = L
M
(0 < α ≤1),
is the “squaredness” of the observed matrix V, and τ = τ(α) is the unique
zero-cross point of the following function:
Ξ (τ; α) = Φ (τ) + Φ
! τ
α
"
,
where
Φ(z) = log(z + 1)
z
−1
2.
(9.19)
Summarizing Lemmas 6.14, 6.16, and 6.19, the EVB posterior is com-
pletely speciﬁed by Eq. (9.10) with the variational parameters given as follows:
If γh ≥γEVB,
ah = ±
.
˘γEVB
h
δEVB
h
,
bh = ±
>
@
˘γEVB
h
δEVB
h
,
(9.20)
σ2
ah = σ2δEVB
h
γh
,
σ2
bh =
σ2
γhδEVB
h
,
cahcbh =
A
γh˘γEVB
h
LM
,
(9.21)
where
δEVB
h
=
A
M˘γEVB
h
Lγh
⎛⎜⎜⎜⎜⎝1 +
Lσ2
γh˘γEVB
h
⎞⎟⎟⎟⎟⎠,
(9.22)
and otherwise
ah = 0,
bh = 0,
σ2
ah =
.
ζEVB,
σ2
bh =
.
ζEVB,
cahcbh =
.
ζEVB,
(9.23)
where
ζEVB →+0.
(9.24)
To use the preceding result, we need to prepare a table of τ by computing
the zero-cross point of Eq. (9.19) as a function of α. A simple approximation
τ ≈z √α ≈2.5129 √α is a reasonable alternative (see Figure 6.4).

240
9 Global Solver for Matrix Factorization
For noise variance estimation, we can use Theorems 8.1 and 8.2, derived in
Chapter 8. Speciﬁcally, after performing the SVD (9.6), we ﬁrst estimate the
noise variance by solving the following problem:
σ2 EVB = argmin
σ2
Ω(σ−2),
(9.25)
s.t.
max
⎛⎜⎜⎜⎜⎜⎜⎝σ2
H+1,
L
h=H+1 γ2
h
M

L −H
 
⎞⎟⎟⎟⎟⎟⎟⎠≤σ2 ≤
1
LM
L

h=1
γ2
h,
(9.26)
where
Ω(σ−2) = 1
L
⎛⎜⎜⎜⎜⎜⎝
H

h=1
ψ
⎛⎜⎜⎜⎜⎝
γ2
h
Mσ2
⎞⎟⎟⎟⎟⎠+
L

h=H+1
ψ0
⎛⎜⎜⎜⎜⎝
γ2
h
Mσ2
⎞⎟⎟⎟⎟⎠
⎞⎟⎟⎟⎟⎟⎠,
(9.27)
ψ (x) = ψ0 (x) + θ

x > x
 
ψ1 (x) ,
ψ0 (x) = x −log x,
ψ1 (x) = log (τ(x; α) + 1) + α log
τ(x; α)
α
+ 1

−τ(x; α),
x =

1 + τ
 
1 + α
τ

,
τ(x; α) = 1
2

x −(1 + α) +
.
(x −(1 + α))2 −4α

,
σ2
h =
⎧⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎩
∞
for h = 0,
γ2
h
Mx
for h = 1,. . . , L,
0
for h = L + 1,
H = min
!D
L
1 + α
E
−1, H
"
.
Problem (9.25) is simply a one-dimensional search for the minimizer of the
function Ω(σ−2), which is typically smooth. Note also that, if the matrix size is
large enough, Corollary 8.21 states that any local minimizer is accurate enough
to estimate the correct rank. Given the estimated noise variance σ2 = σ2 EVB,
Eq. (9.18) gives the EVB solution.
Algorithm 16 summarizes the procedure explained in the preceding discus-
sion. This algorithm gives the global solution, provided that the global solution
to the one-dimensional search problem (9.25) is attained. If the noise variance
σ2 is known, we should simply skip Step 4.

9.2 Global EVB Solver for Fully Observed MF
241
Algorithm 16 Global EVB solver for fully observed matrix facrtorization.
1: Transpose V →V⊤if L > M, and set H (≤L) to a sufﬁciently large value.
2: Refer to the table of τ(α) at α = L/M (or use a simple approximation
τ ≈2.5129 √α).
3: Compute the SVD (9.6) of V.
4: Solve the one-dimensional search problem (9.25) to get σ2 EVB.
5: Apply Eq. (9.18) to get the EVB estimator {γEVB
h
}H
h=1 for σ2 = σ2 EVB.
6: If necessary, compute the variational parameters and the hyperparameters
by using Eqs. (9.20) through (9.24), which specify the EVB posterior
(9.10). We can also evaluate the free energy by using Eqs. (9.15) and
(9.16), noting that Fh →+0 for h such that γh < γEVB.
Algorithm 17 Iterative EVB solver for fully observed matrix factorization.
1: Transpose V →V⊤if L > M, and set H (≤L) to a sufﬁciently large value.
2: Refer to the table of τ(α) at α = L/M (or use a simple approximation
τ ≈2.5129 √α).
3: Compute the SVD (9.6) of V.
4: Initialize the noise variance σ2 EVB to the lower bound in Eq. (9.26).
5: Apply Eq. (9.18) to update the EVB estimator {γEVB
h
}H
h=1.
6: Apply Eq. (9.28) to update the noise variance estimator σ2 EVB.
7: Compute the variational parameters and the hyperparameters by using Eqs.
(9.20) through (9.24).
8: Evaluate the free energy (9.15), noting that Fh →+0 for h such that γh <
γEVB.
9: Iterate Steps 5 through 8 until convergence (until the energy decrease
becomes smaller than a threshold).
Another implementation is to iterate Eq. (9.18) and
σ2 EVB =
1
LM
⎛⎜⎜⎜⎜⎜⎝
L

l=1
γ2
l −
H

h=1
γhγEVB
h
⎞⎟⎟⎟⎟⎟⎠
(9.28)
in turn. Note that Eq. (9.28) was derived in Corollary 8.3 and can be used
as an update rule for the noise variance estimator, given the current EVB
estimators {γEVB
h
}H
h=1. Although it is not guaranteed, this iterative algorithm
(Algorithm 17) tends to converge to the global solution if we initialize the
noise variance σ2 EVB to be sufﬁciently small (Nakajima et al., 2015). We
recommend to initialize it to the lower-bound given in Eq. (9.26).

242
9 Global Solver for Matrix Factorization
Algorithm 18 Local-EVB solver for fully observed matrix factorization.
1: Transpose V →V⊤if L > M, and set H (≤L) to a sufﬁciently large value.
2: Refer to the table of τ(α) at α = L/M (or use a simple approximation
τ ≈2.5129 √α).
3: Compute the SVD (9.6) of V.
4: Initialize the noise variance σ2 local−EVB to the lower-bound in Eq. (9.26).
5: Apply Eq. (9.29) to update the local-EVB estimator {γlocal−EVB
h
}H
h=1.
6: Apply Eq. (9.30) to update the noise variance estimator σ2 local−EVB.
7: Compute the variational parameters and the hyperparameters by using
Eqs. (9.20) through (9.24).
8: Evaluate the free energy (9.15), noting that Fh →+0 for h such that
γh < γlocal−EVB.
9: Iterate Steps 5 through 8 until convergence (until the energy decrease
becomes smaller than a threshold).
Finally, we introduce an iterative solver, in Algorithm 18, for the local-EVB
estimator (6.131), which iterates the following updates:
γlocal−EVB
h
=
⎧⎪⎪⎨⎪⎪⎩
˘γEVB
h
if γh ≥γlocal−EVB,
0
otherwise,
(9.29)
σ2 local−EVB =
1
LM
⎛⎜⎜⎜⎜⎜⎝
L

l=1
γ2
l −
H

h=1
γhγlocal−EVB
h
⎞⎟⎟⎟⎟⎟⎠,
(9.30)
where
γlocal−EVB ≡
 √
L +
√
M
 
σ
(9.31)
is the local-EVB threshold, deﬁned by Eq. (6.127). If we initialize the
noise variance σ2 local−EVB to be sufﬁciently small, this algorithm tends to
retain the positive local-EVB solution for each h if it exists, and therefore
does not necessarily converge to the global EVB solution. The interesting
relation between the local-EVB estimator and the overlap (OL) method (Hoyle,
2008), an alternative dimensionality selection method based on the Laplace
approximation, was discussed in Section 8.7.
9.3 Empirical Comparison with the Standard VB Algorithm
Here we see how efﬁcient the global solver (Algorithm 16) is in comparison
with the standard VB algorithm (Algorithm 1 in Section 3.1) on artiﬁcial and
benchmark data.

9.3 Empirical Comparison with the Standard VB Algorithm
243
9.3.1 Experiment on Artiﬁcial Data
We ﬁrst created an artiﬁcial data set (Artiﬁcial1) with the data matrix size
L = 100 and M = 300, and the true rank H∗= 20. We randomly drew
true matrices A∗∈RM×H∗and B∗∈RL×H∗so that each entry of A∗and
B∗follows Gauss1(0, 1), where Gauss1(μ, σ2) denotes the one-dimensional
Gaussian distribution with mean μ and variance σ2. An observed matrix V
was created by adding noise subject to Gauss1(0, 1) to each entry of B∗A∗⊤.
We evaluated the performance under the complete empirical Bayesian
scenario, where all variational parameters and hyperparameters are estimated
from observation. We used the full-rank model (i.e., H
=
min(L, M)),
expecting that irrelevant H −H∗components will be automatically trimmed
out by the automatic relevance determination (ARD) effect (see Chapters 7
and 8).
We compare the global solver (Algorithm 16) and the standard VB algo-
rithm (Algorithm 1 in Section 3.1), and show the free energy, the computation
time, and the estimated rank over iterations in Figure 9.1. For the standard VB
algorithm, initial values were set in the following way: A and B are randomly
created so that each entry follows Gauss1(0, 1). Other variables are set to
ΣA = ΣB = CA = CB = IH and σ2 = 1. Note that we rescale V so that
∥V∥2
Fro /(LM) = 1, before starting iterations. We ran the standard algorithm 10
0
50
100
150
200
250
Iteration
1.8
1.85
1.9
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(a) Free energy
0
50
100
150
200
250
Iteration
0
20
40
60
80
Time(sec)
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(b) Computation time
0
50
100
150
200
250
Iteration
0
20
40
60
80
100
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(c) Estimated rank
Figure 9.1 Experimental results on the Artiﬁcial1 data, where the data matrix size
is L = 100 and M = 300, and the true rank is H∗= 20.

244
9 Global Solver for Matrix Factorization
0
50
100
150
200
250
Iteration
2.4
2.6
2.8
3
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(a) Free energy
0
50
100
150
200
250
Iteration
0
10
20
30
40
Time(sec)
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(b) Computation time
0
50
100
150
200
250
Iteration
0
20
40
60
80
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(c) Estimated rank
Figure 9.2 Experimental results on the Artiﬁcial2 data set (L = 70, M = 300, and
H∗= 40).
times, starting from different initial points, and each trial is plotted by a solid
curve labeled as “Standard(iniRan)” in Figure 9.1.
The global solver has no iteration loop, and therefore the corresponding
dashed line labeled as “Global” is constant over iterations. We see that the
global solver ﬁnds the true rank 
H = H∗= 20 immediately (∼0.1 sec on
average over 10 trials), while the standard iterative algorithm does not converge
in 60 sec.
Figure 9.2 shows experimental results on another artiﬁcial data set (Artiﬁ-
cial2) where L = 70, M = 300, and H∗= 40. In this case, all the 10 trials of the
standard algorithm are trapped at local minima. We empirically observed that
the local minimum problem tends to be more critical when H∗is large (close
to H).
We also evaluated the standard algorithm with different initialization
schemes. The curve labeled as “Standard(iniML)” indicates the standard
algorithm starting from the maximum likelihood (ML) solution: (ah,bh) =
( √γhωah, √γhωbh). The initial values for other variables are the same as the
random initialization. Figures 9.1 and 9.2 show that the ML initialization
generally makes convergence faster than the random initialization, but suffers
from the local minimum problem more severely—it tends to converge to a
worse local minimum.

9.3 Empirical Comparison with the Standard VB Algorithm
245
0
50
100
150
200
250
Iteration
0
0.5
1
1.5
2
2.5
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(a) Free energy
0
50
100
150
200
250
Iteration
0
1
2
3
Time(sec)
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(b) Computation time
0
50
100
150
200
250
Iteration
0
5
10
15
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(c) Estimated rank
Figure 9.3 Experimental results on the Glass data set (L = 9, M = 214).
We observed that starting from a small noise variance tends to alleviate
the local minimum problem at the expense of slightly slower convergence.
The curve labeled as “Standard(iniMLSS)” indicates the standard algorithm
starting from the ML solution with a small noise variance σ2 = 0.0001. We see
in Figures 9.1 and 9.2 that this initialization improves the quality of solutions,
and successfully ﬁnds the true rank for these artiﬁcial data sets. However, we
will show in Section 9.3.2 that this scheme still suffers from the local minimum
problem on benchmark datasets.
9.3.2 Experiment on Benchmark Data
Figures 9.3 through 9.5 show the experimental results on the Glass, the
Satimage, and the Spectf data sets available from the University of California,
Irvine (UCI) repository (Asuncion and Newman, 2007). A similar tendency
to the artiﬁcial data experiment (Figures 9.1 and 9.2) is observed: “Stan-
dard(iniRan)” converges slowly, and is often trapped at a local minimum
with a wrong estimated rank;1 “Standard(iniML)” converges slightly faster but
to a worse local minimum; and “Standard(iniMLSS)” tends to give a better
solution. Unlike the artiﬁcial data experiment, “Standard(iniMLSS)” fails to
1 Since the true ranks of the benchmark data sets are unknown, we mean by a wrong rank a rank
different from the one giving the lowest free energy.

246
9 Global Solver for Matrix Factorization
0
50
100
150
200
250
Iteration
2.4
2.6
2.8
3
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(a) Free energy
0
50
100
150
200
250
Iteration
0
2000
4000
6000
8000
Time(sec)
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(b) Computation time
0
50
100
150
200
250
Iteration
0
10
20
30
40
50
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(c) Estimated rank
Figure 9.4 Experimental results on the Satimage data set (L = 36, M = 6435).
0
50
100
150
200
250
Iteration
3
3.1
3.2
3.3
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(a) Free energy
0
50
100
150
200
250
Iteration
0
5
10
15
20
25
Time(sec)
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(b) Computation time
0
50
100
150
200
250
Iteration
0
10
20
30
40
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(c) Estimated rank
Figure 9.5 Experimental results on the Spectf data set (L = 44, M = 267).
ﬁnd the correct rank in these benchmark data sets. We also conducted experi-
ments on other benchmark data sets and found that the standard VB algorithm
generally converges slowly, and sometimes suffers from the local minimum
problem, while the global solver gives the global solution immediately.

9.4 Extension to Nonconjugate MF with Missing Entries
247
0
50
100
150
200
250
Iteration
–2.0435
–2.0434
–2.0433
–2.0432
–2.0431
–2.043
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(a) Free energy
0
50
100
150
200
250
Iteration
0
0.02
0.04
0.06
0.08
Time(sec)
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(b) Computation time
0
50
100
150
200
250
Iteration
0
1
2
3
4
Global
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(c) Estimated rank
Figure 9.6 Experimental results on the Concrete Slump Test data set (an RRR task
with L = 3, M = 7).
Finally, we applied EVB learning to the reduced rank regression (RRR)
model (see Section 3.1.2), of which the model likelihood is given by
Eq. (3.36). Figure 9.6 shows the results on the Concrete Slump Test data set,
where we centered the L = 3-dimensional outputs and prewhitened the M = 7-
dimensional inputs. We also standardized the outputs so that the variance of
each element is equal to one. Note that we cannot directly apply Algorithm
16 for the RRR model. Instead, we use Algorithm 16 with a ﬁxed noise
variance (skipping Step 4) and apply one-dimensional search to minimize the
free energy (3.42), in order to estimate the rescaled noise variance σ2. For
the standard VB algorithm, the rescaled noise variance should be updated by
Eq. (3.43), instead of Eq. (3.28). The original noise variance σ′2 is recovered
by Eq. (3.40) for both cases.
Overall, the global solver showed excellent performance over the standard
VB algorithm.
9.4 Extension to Nonconjugate MF with Missing Entries
The global solvers introduced in Section 9.1 can be directly applied only for
the fully observed isotropic Gaussian likelihood (9.1). However, the global
solver can be used as a subroutine to develop efﬁcient algorithms for more

248
9 Global Solver for Matrix Factorization
general cases. In this section, we introduce the approach by Seeger and
Bouchard (2012), where an iterative singular value shrinkage algorithm was
proposed, based on the global VB solver (Algorithm 15) and local variational
approximation (Section 2.1.7).
9.4.1 Nonconjugate MF Model
Consider the following model:
p(V|A, B) =
L

l=1
M

m=1
φl,m(Vl,m,b
⊤
l am),
(9.32)
p(A) ∝exp

−1
2tr

AC−1
A A⊤ 
,
(9.33)
p(B) ∝exp

−1
2tr

BC−1
B B⊤ 
,
(9.34)
where φl,m(v|u) is a function of v and u, and satisfy
−∂2 log φl,m(v, u)
∂u2
≤1
σ2
(9.35)
for any l, m, v, and u.
The function φl,m(v, u) corresponds to the model distribution of the (l, m)th
entry v of V parameterized by u. If
φl,m(v, u) = Gauss1(v; u, σ2)
(9.36)
for all l and m, the model (9.32) through (9.34) is reduced to the fully observed
isotropic Gaussian MF model (9.1)–(9.3), and the Hessian,
−∂2 log φl,m(v, u)
∂u2
= 1
σ2 ,
of the negative log-likelihood is a constant with respect to v and u.
The model (9.32) through (9.34) can cover the case with missing entries by
setting the noise variance in Eq. (9.36) to σ2 →∞for the unobserved entries
(the condition (9.35) is tight for the smallest σ2). Other one-dimensional dis-
tributions, including the Bernoulli distribution with sigmoid parameterization
and the Poisson distribution, satisfy the condition (9.35) for a certain σ2, which
will be introduced in Section 9.4.3.

9.4 Extension to Nonconjugate MF with Missing Entries
249
9.4.2 Local Variational Approximation for Non-conjugate MF
The VB learning problem (9.4) minimizes the free energy, which can be
written as
F(r) =
/
log
rA(A)rB(B)
L
l=1
M
m=1 φl,m(Vl,m,b
⊤
l am)p(A)p(B)
0
rA(A)rB(B)
.
(9.37)
In order to make the global VB solver applicable as a subroutine, we instead
solve the following joint minimization problem,
r = argmin
r,Ξ
F(r, Ξ)
s.t.
r(A, B) = rA(A)rB(B),
(9.38)
of an upper-bound of the free energy,
F ≤F(r, Ξ) ≡
/
log
rA(A)rB(B)
L
l=1
M
m=1 φl,m(Vl,m,b
⊤
l am, Ξl,m)p(A)p(B)
0
rA(A)rB(B)
,
(9.39)
where
φl,m(v, u, ξ) ≤φl,m(v, u)
(9.40)
is a lower-bound of the likelihood parameterized with variational parameters
Ξ ∈RL×M.
The condition (9.35) allows us to form a parametric lower-bound in the
(unnormalized) isotropic Gaussian form, which we derive as follows. Any
function f(x) with bounded curvature ∂2 f
∂x2 ≤κ can be upper-bounded by the
following quadratic function:
f(x) ≤κ(x −ξ)2 + ∂f
∂x
x=ξ
(x −ξ) + f(ξ)
for any ξ ∈R.
(9.41)
Therefore, it holds that, for any ξ ∈R,
−log φl,m(v, u) ≤(u −ξ)2
2σ2
+ g(v, ξ)(u −ξ) −log φl,m(v, ξ),
(9.42)
where
g(v, ξ) = −∂log φl,m(v, u)
∂u
u=ξ
.
The left graph in Figure 9.7 shows the parametric quadratic upper-bounds
(9.42) for the Bernoulli likelihood with sigmoid parameterization.

250
9 Global Solver for Matrix Factorization
–5
5
0
0
2
4
6
8
10
–5
5
0
0
0.2
0.4
0.6
0.8
1
Figure 9.7 Parametric quadratic (Gaussian-form) bounds for the Bernoulli like-
lihood with sigmoid parameterization, φ(v, u) = evu/(1 + eu), for v = 1. Left:
the negative log-likelihood (the left-hand side of Eq. (9.42)) and its quadratic
upper-bounds (the right-hand side of Eq. (9.42)) for ξ = −2.5, 0.0, 2.5. Right:
the likelihood function φ(1, ξ) and its Gaussian-form lower-bounds (9.43) for
ξ = −2.5, 0.0, 2.5.
Since log(·) is a monotonic function, we can adopt the following parametric
lower-bound of φl,m(v, u):
φl,m(u, ξ) = exp

−
(u −ξ)2
2σ2
+ g(v, ξ)(u −ξ) −log φl,m(v, ξ)

= φl,m(v, ξ) exp

−1
2σ2

(u −ξ)2 + 2σ2g(v, ξ)(u −ξ)
  
= φl,m(v, ξ) exp
!
−1
2σ2
!
u −ξ + σ2g(v, ξ)
 2 −

σ2g(v, ξ)
 2""
= φl,m(v, ξ) exp
 σ2
2 g2(v, ξ)
 
exp
!
−1
2σ2

(ξ −σ2g(v, ξ)) −u
 2"
=
√
2πσ2φl,m(v, ξ) exp
 σ2
2 g2(v, ξ)
 
Gauss1

ξ −σ2g(v, ξ); u, σ2 
.
(9.43)
The right graph in Figure 9.7 shows the parametric Gaussian-form lower-
bounds (9.43) for the Bernoulli likelihood with sigmoid parameterization.
Substituting Eq. (9.43) into Eq. (9.39) gives
F(r, Ξ) = −
L

l=1
M

m=1
1
2 log(2πσ2) + log φl,m(Vl,m, Ξl,m) + σ2
2 g2(Vl,m, Ξl,m)

+
*
log
rA(A)rB(B)
L
l=1
M
m=1 Gauss1(Ξl,m−σ2g(Vl,m,Ξl,m);b
⊤
l am,σ2)p(A)p(B)
+
rA(A)rB(B)
= −
L

l=1
M

m=1
1
2 log(2πσ2) + log φl,m(Vl,m, Ξl,m) + σ2
2 g2(Vl,m, Ξl,m)

+
/
log
rA(A)rB(B)
(2πσ2)−LM/2 exp

−
1
2σ2 ∥˘V−BA⊤∥2
Fro
 
p(A)p(B)
0
rA(A)rB(B)
,
(9.44)
where ˘V ∈RL×M is a matrix such that
˘Vl,m = Ξl,m −σ2g(Vl,m, Ξl,m).
(9.45)

9.4 Extension to Nonconjugate MF with Missing Entries
251
The ﬁrst term in Eq. (9.44) does not depend on r and the second term is equal
to the free energy of the fully observed isotropic Gaussian MF model with the
observed matrix V replaced with ˘V. Therefore, given the variational parameter
Ξ, we can partially solve the minimization problem (9.38) with respect to r by
applying the global VB solver (Algorithm 15). The solution is Gaussian in the
following form:
r(A, B) = rA(A)rB(B),
where
(9.46)
rA(A) = MGaussM,H(A; A, IM ⊗ΣA) ∝exp
⎛⎜⎜⎜⎜⎜⎝−
tr
!
(A−A)Σ
−1
A (A−A)⊤
"
2
⎞⎟⎟⎟⎟⎟⎠,
(9.47)
rB(B) = MGaussL,H(B; B, IL ⊗ΣB) ∝exp
⎛⎜⎜⎜⎜⎜⎝−
tr
!
(B−B)Σ
−1
B (B−B)⊤
"
2
⎞⎟⎟⎟⎟⎟⎠.
(9.48)
Here the mean and the covariance parameters A, ΣA, B, ΣB are another set of
variational parameters.
Given the optimal r speciﬁed by Eq. (9.46), the free energy bound (9.44) is
written (as a function of Ξ) as follows:
min
r
F(r, Ξ) = −
L

l=1
M

m=1

log φl,m(Vl,m, Ξl,m) + σ2
2 g2(Vl,m, Ξl,m)

−
1
2σ2

∥˘V −BA⊤∥2
Fro

rA(A)rB(B) + const.
= −
L

l=1
M

m=1

log φl,m(Vl,m, Ξl,m) + σ2
2 g2(Vl,m, Ξl,m)

−
1
2σ2 ∥˘V −BA
⊤∥2
Fro + const.
= −
L

l=1
M

m=1

log φl,m(Vl,m, Ξl,m) + σ2
2 g2(Vl,m, Ξl,m)

−
1
2σ2
L
l=1
M
m=1
 ˘Vl,m −
Ul,m
 2 + const.,
(9.49)
where
U = BA
⊤.
The second-to-last equation in Eq. (9.43), together with Eq. (9.45), implies
that
log φl,m(
Ul,m, Ξl,m) = log φl,m(Vl,m, Ξl,m) + σ2
2 g2(Vl,m, Ξl,m) −
1
2σ2
 ˘Vl,m −
Ul,m
 2 ,
with which Eq. (9.49) is written as
min
r
F(r, Ξ) = −L
l=1
M
m=1 log φl,m(
Ul,m, Ξl,m) + const.
(9.50)

252
9 Global Solver for Matrix Factorization
Algorithm 19 Iterative singular value shrinkage algorithm for nonconjugate
MF (with missing entries).
1: Set the noise variance σ2 with which the condition (9.35) tightly holds,
and initialize the variational parameters to Ξ = 0(L,M).
2: Compute ˘V by Eq. (9.45).
3: Compute the VB posterior (9.46) by applying the global solver (Algorithm
15) with ˘V substituted for V.
4: Update Ξ by Eq. (9.51).
5: Iterate Steps 2 through 4 until convergence.
Since log φl,m(u, ξ) is the quadratic upper-bound (the right-hand side in
Eq. (9.42)) of −log φl,m(v, u), which is tight at u = 
Ul,m when ξ = 
Ul,m, the
minimizer of Eq. (9.50) with respect to Ξ is given by
Ξ ≡argmin
Ξ
min
r
F(r, Ξ) = U.
(9.51)
In summary, to solve the joint minimization problem (9.38), we can
iteratively update r and Ξ. The update of r can be performed by the global
solver (Algorithm 15) with the observed matrix V replaced with ˘V, deﬁned
by Eq. (9.45). The update of Ξ is simply performed by Eq. (9.51). Algorithm
19 summarizes this procedure, where 0(d1,d2) denotes the d1 × d2 matrix with
all entries equal to zero. Seeger and Bouchard (2012) empirically showed
that this iterative singular value shrinkage algorithm signiﬁcantly outperforms
the MAP solution at comparable computational costs. They also proposed an
efﬁcient way to perform SVD when V is huge but sparsely observed, based on
the techniques proposed by Tomioka et al. (2010).
9.4.3 Examples of Nonconjugate MF
In this subsection, we introduce a few examples of model likelihood φl,m(v, u),
which satisfy Condition (9.35), and give the corresponding derivatives of the
negative log likelihood.
Isotropic Gaussian MF with Missing Entries
If we let
φl,m(v, u) =
⎧⎪⎪⎨⎪⎪⎩
Gauss1(v; u, σ2)
if (l, m) ∈Λ,
1
otherwise,
(9.52)

9.4 Extension to Nonconjugate MF with Missing Entries
253
where Λ denotes the set of observed entries, the model distribution (9.32)
corresponds to the model distribution (3.44) of MF with missing entries. The
ﬁrst and the second derivatives of the negative log likelihood are given as
follows:
−∂log φl,m(v, u)
∂u
=
⎧⎪⎪⎨⎪⎪⎩
1
σ2 (u −v)
if (l, m) ∈Λ,
0
otherwise,
−∂2 log φl,m(v, u)
∂u2
=
⎧⎪⎪⎨⎪⎪⎩
1
σ2
if (l, m) ∈Λ,
0
otherwise.
(9.53)
Bernoulli MF with Sigmoid Parameterization
The Bernoulli distribution with sigmoid parameterization is suitable for binary
observations, i.e., V ∈{0, 1}L×M:
φl,m(v, u) =
⎧⎪⎪⎪⎨⎪⎪⎪⎩
evu
1 + eu
if (l, m) ∈Λ,
1
otherwise.
(9.54)
The ﬁrst and the second derivatives are given as follows:
−∂log φl,m(v, u)
∂u
=
⎧⎪⎪⎨⎪⎪⎩
1
1+e−u −v
if (l, m) ∈Λ,
0
otherwise,
−∂2 log φl,m(v, u)
∂u2
=
⎧⎪⎪⎨⎪⎪⎩
1
(1+e−u)(1+eu)
if (l, m) ∈Λ,
0
otherwise.
(9.55)
It holds that
−∂2 log φl,m(v, u)
∂u2
≤1
4,
and therefore, the noise variance should be set to σ2 = 4, which satisﬁes the
condition (9.35). Figure 9.7 was depicted for this model.
Poisson MF
The Poisson distribution is suitable for count data, i.e., V ∈{0, 1, 2,. . .}L×M:
φl,m(v, u) =
⎧⎪⎪⎨⎪⎪⎩
λv(u)e−λ(u)
if (l, m) ∈Λ,
1
otherwise,
(9.56)
where λ(u) is the link function. Since a common choice λ(u) = eu for the link
function gives unbounded curvature for large u, Seeger and Bouchard (2012)

254
9 Global Solver for Matrix Factorization
proposed to use another link function λ(u) = log(1 + eu). The ﬁrst derivative is
given as follows:
−∂log φl,m(v, u)
∂u
=
⎧⎪⎪⎨⎪⎪⎩
1
1+e−u

1 −
v
λ(u)
 
if (l, m) ∈Λ,
0
otherwise.
(9.57)
It was conﬁrmed that the second derivative is upper-bounded as
−∂2 log φl,m(v, u)
∂u2
≤1
4 + 0.17v,
and therefore, the noise variance should be set to
σ2 =
1
1/4 + 0.17 maxl,m Vl,m
.
Since the bound can be loose if some of the entries Vl,m of the observed matrix
are huge compared to the others, overly large counts should be clipped.

10
Global Solver for Low-Rank
Subspace Clustering
The nonasymptotic theory, described in Chapter 6, for fully observed matrix
factorization (MF) has been extended to other bilinear models. In this chapter,
we introduce exact and approximate global variational Bayesian (VB) solvers
(Nakajima et al., 2013c) for low-rank subspace clustering (LRSC).
10.1 Problem Description
The LRSC model, introduced in Section 3.4, is deﬁned as
p(V|A′, B′) ∝exp

−1
2σ2
###V −VB′ A′⊤###2
Fro

,
(10.1)
p(A′) ∝exp

−1
2tr(A′C−1
A A′⊤)

,
(10.2)
p(B′) ∝exp

−1
2tr(B′C−1
B B′⊤)

,
(10.3)
where V ∈RL×M is an observation matrix, and A′ ∈RM×H and B′ ∈RM×H for
H ≤min(L, M) are the parameters to be estimated. Note that in this chapter
we denote the original parameters A′ and B′ with primes for convenience. We
assume that hyperparameters
CA = Diag(c2
a1,. . . , c2
aH),
CB = Diag(c2
b1,. . . , c2
bH),
are diagonal and positive deﬁnite. The LRSC model is similar to MF. The only
difference is that the product B′ A′⊤of the parameters is further multiplied
by V in Eq. (10.1). Accordingly, we can hope that similar analysis could be
applied to LRSC, providing a global solver for LRSC.
255

256
10 Global Solver for Low-Rank Subspace Clustering
We ﬁrst transform the parameters as
A ←Ωright⊤
V
A′,
B ←Ωright⊤
V
B′,
where
V = Ωleft
V ΓVΩright⊤
V
(10.4)
is the singular value decomposition (SVD) of V. Here, Ωleft
V
∈RL×L and Ωright
V
∈
RM×M are orthogonal matrices, and ΓV ∈RL×M is a (possibly nonsquare)
diagonal matrix with nonnegative diagonal entries aligned in nonincreasing
order, i.e., γ1 ≥γ2 ≥· · · ≥γmin(L,M). After this transformation, the LRSC
model (10.1) through (10.3) is rewritten as
p(ΓV|A, B) ∝exp

−1
2σ2
###ΓV −ΓV BA⊤)
###2
Fro

,
(10.5)
p(A) ∝exp

−1
2tr(AC−1
A A⊤)

,
(10.6)
p(B) ∝exp

−1
2tr(BC−1
B B⊤)

.
(10.7)
The transformation (10.4) does not affect much the derivation of the VB
learning algorithm. The following summarizes the result obtained in Section
3.4 with the transformed parameters A and B. The solution of the VB learning
problem,
r = argmin
r
F(r)
s.t.
r(A, B) = rA(A)rB(B),
where
(10.8)
F =
/
log
rA(A)rB(B)
p(ΓV|A, B)p(A)p(B)
0
rA(A)rB(B)
,
has the following form:
r(A) ∝exp
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
−
tr
!
(A −A)Σ
−1
A (A −A)⊤
"
2
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
,
r(B) ∝exp
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎝−(˘b −˘b)⊤˘Σ
−1
B (˘b −˘b)
2
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎠,
(10.9)
for ˘b = vec(B) ∈RMH, and the free energy can be explicitly written as
2F = LM log(2πσ2) +
####ΓV−ΓVBA
⊤####
2
Fro
σ2
+ M log det(CA)
det
Σ A
 + log det(CB⊗IM)
det
!˘ΣB
"
−2MH + tr
2
C−1
A
!
A
⊤A + MΣ A
"3
+ tr
2
C−1
B B
⊤B
3
+ tr
2
(C−1
B ⊗IM)˘ΣB
3
+ tr
)
σ−2Γ⊤
VΓV

−BA
⊤AB
⊤+
*
B(A
⊤A + MΣ A)B⊤+
r(B)
1
.
(10.10)

10.1 Problem Description
257
Therefore, the variational parameters (A, B, ΣA, ˘ΣB) can be obtained by solv-
ing the following problem:
Given CA, CB ∈DH
++, σ2 ∈R++,
min
(A,B,ΣA,˘ΣB)
F,
(10.11)
s.t.
A, B ∈RM×H, ΣA ∈SH
++, ˘ΣB ∈SMH
++ .
(10.12)
The stationary conditions with respect to the variational parameters are
given by
A = 1
σ2 Γ⊤
VΓVBΣ A,
(10.13)
Σ A = σ2 !
B⊤Γ⊤
VΓV B

r(B) + σ2C−1
A
"−1
,
(10.14)
˘b =
˘ΣB
σ2 vec

Γ⊤
VΓVA
 
,
(10.15)
˘ΣB = σ2 !
(A
⊤A + MΣ A) ⊗Γ⊤
VΓV + σ2(C−1
B ⊗IM)
"−1
.
(10.16)
For empirical VB (EVB) learning, we solve the problem,
Given σ2 ∈R++,
min
(A,B,ΣA,˘ΣB,CA,CB)
F
(10.17)
subject to
A, B ∈RM×H, ΣA ∈SH
++, ˘ΣB ∈SMH
++ , CA, CB ∈DH
++,
(10.18)
for which the stationary conditions with respect to the hyperparameters are
given by
c2
ah =
###ah
###2 /M +
Σ A
 
h,h ,
(10.19)
c2
bh =
!####bh
####
2
+ tr
!
Σ
(h,h)
B
""
/M,
(10.20)
σ2 =
tr

Γ⊤
VΓV

IM −2BA
⊤+
*
B(A
⊤A + MΣ A)B⊤+
r(B)

LM
.
(10.21)
In deriving the global VB solution of fully observed MF in Chapter 6, the
following two facts were essential. First, a large portion of the degrees of
freedom of the original variational parameters are irrelevant (see Section 6.3),
and the optimization problem can be decomposed into subproblems, each of
which has only a small number of unknown variables. Second, the stationary
conditions of each subproblem is written as a polynomial system (a set of

258
10 Global Solver for Low-Rank Subspace Clustering
polynomial equations). These two facts also apply to the LRSC model, which
allows us to derive an exact global VB solver (EGVBS). However, each of
the decomposed subproblems still has too many unknowns whose number is
proportional to the problem size, and therefore EGVBS is still computationally
demanding for typical problem sizes. As an alternative, we also derive an
approximate global VB solver (AGVBS) by imposing an additional constraint,
which allows further decomposition of the problem into subproblems with a
constant number of unknowns.
In this chapter, we ﬁrst ﬁnd irrelevant degrees of freedom of the variational
parameters and decompose the VB learning problem. Then we derive EGVBS
and AGVBS and empirically show their usefulness.
10.2 Conditions for VB Solutions
Let J (≤min(L, M)) be the rank of the observed matrix V. For simplicity,
we assume that no pair of positive singular values of V coincide with each
other, i.e.,
γ1 > γ2 > · · · > γJ > 0.
This holds with probability 1 if V is contaminated with Gaussian noise, as the
LRSC model (10.1) assumes. Since (Γ⊤
VΓV)m,m′ is zero for m > J or m′ > J,
Eqs. (10.13) and (10.15) imply that
Am,h = Bm,h = 0
for
m > J.
(10.22)
Similarly to Lemma 6.1 for the fully observed MF, we can prove the
following lemma:
Lemma 10.1
Any local solution of the problem (10.11) is a stationary point
of the free energy (10.10).
Proof
Since
####ΓV −ΓVBA
⊤####
2
Fro ≥0,
and
tr
)
Γ⊤
VΓV

−BA
⊤AB
⊤+
*
B(A
⊤A + MΣ A)B⊤+
r(B)
1
= M · tr
2
Γ⊤
VΓV

BΣ AB⊤
r(B)
3
≥0,

10.4 Proof of Theorem 10.2
259
the free energy (10.10) is lower-bounded as
2F ≥−M log det
ΣA
 
−log det
!˘ΣB
"
+ tr
2
C−1
A
!
A
⊤A + MΣA
"3
+ tr
2
C−1
B B
⊤B
3
+ tr
2
(C−1
B ⊗IM)˘ΣB
3
+ τ,
(10.23)
where τ is a ﬁnite constant. The right-hand side of Eq. (10.23) diverges to +∞
if any entry of A or B goes to +∞or −∞. Also it diverges if any eigenvalue
of ΣA or ˘ΣB goes to +0 or ∞. This implies that no local solution exists on
the boundary of (the closure of) the domain (10.12). Since the free energy is
differentiable in the domain (10.12), any local minimizer is a stationary point.
For any (diagonalized) observed matrix ΓV, the free energy (10.10) can
be ﬁnite, for example, at A = 0M,H, B = 0M,H, ΣA = IH, and ˘ΣB = IMH.
Therefore, at least one minimizer always exists, which completes the proof of
Lemma 10.1.
□
Lemma 10.1 implies that Eqs. (10.13) through (10.16) hold at any local
solution.
10.3 Irrelevant Degrees of Freedom
Also similarly to Theorem 6.4, we have the following theorem:
Theorem 10.2
When CACB is nondegenerate (i.e., cahcbh > cah′ cbh′ for any
pair h < h′), (A, B, ΣA, ˘ΣB) are diagonal for any solution of the problem
(10.11). When CACB is degenerate, any solution has an equivalent solution
with diagonal (A, B, ΣA, ˘ΣB).
Theorem 10.2 signiﬁcantly reduces the complexity of the optimization
problem, and furthermore makes the problem separable, as seen in
Section 10.5.
10.4 Proof of Theorem 10.2
Similarly to Section 6.4, we separately consider the following three cases:
Case 1 When no pair of diagonal entries of CACB coincide.
Case 2 When all diagonal entries of CACB coincide.
Case 3 When (not all but) some pairs of diagonal entries of CACB coincide.

260
10 Global Solver for Low-Rank Subspace Clustering
10.4.1 Diagonality Implied by Optimality
We can prove the following lemma, which is an extension of Lemma 6.2.
Lemma 10.3
Let Γ, Ω, Φ ∈RH×H be a nondegenerate diagonal matrix,
an orthogonal matrix, and a symmetric matrix, respectively. Let {Λ(k), Λ′(k) ∈
RH×H; k = 1,. . . , K} be arbitrary diagonal matrices, and {Ψ(k′) ∈RH×H; k′ =
1,. . . , K′} be arbitrary symmetric matrices. If
G(Ω) = tr
⎧⎪⎪⎨⎪⎪⎩ΓΩΦΩ⊤+
K

k=1
Λ(k)ΩΛ′(k)Ω⊤+
K′

k′=1
ΩΨ(k′)
⎫⎪⎪⎬⎪⎪⎭
(10.24)
is minimized or maximized (as a function of Ω, given Γ, Φ, {Λ(k), Λ′(k)}, {Ψ(k′)})
when Ω = IH, then Φ is diagonal. Here, K and K′ can be any natural numbers
including K = 0 and K′ = 0 (when the second and the third terms, respectively,
do not exist).
Proof
Let
Φ = Ω′Γ′Ω′⊤
(10.25)
be the eigenvalue decomposition of Φ. Let γ, γ′, {λ(k)}, {λ′(k)} be the vectors
consisting of the diagonal entries of Γ, Γ′, {Λ(k)}, {Λ′(k)}, respectively, i.e.,
Γ = Diag(γ),
Γ′ = Diag(γ′),
Λ(k) = Diag(λ(k)),
Λ′(k) = Diag(λ′(k)).
Then, Eq. (10.24) can be written as
G(Ω) = tr
⎧⎪⎪⎨⎪⎪⎩ΓΩΦΩ⊤+
K

k=1
Λ(k)ΩΛ′(k)Ω⊤+
K′

k′=1
ΩΨ(k′)
⎫⎪⎪⎬⎪⎪⎭
= γ⊤Qγ′ +
K

k=1
λ(k)⊤Rλ′(k) +
K′

k′=1
tr
,
ΩΨ(k′)-
,
(10.26)
where
Q = (ΩΩ′) ⊙(ΩΩ′),
R = Ω ⊙Ω.
Here, ⊙denotes the Hadamard product.
Using this expression, we will prove that Φ is diagonal if Ω = IH minimizes
or maximizes Eq. (10.26). Let us consider a bilateral perturbation Ω = Δ such
that the 2×2 matrix Δ(h,h′) for h  h′ consisting of the hth and the h′th columns
and rows form a 2 × 2 orthogonal matrix,
Δ(h,h′) =
cos θ
−sin θ
sin θ
cos θ

,

10.4 Proof of Theorem 10.2
261
and the remaining entries coincide with those of the identity matrix. Then, the
elements of Q become
Qi, j =
⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩
(Ω′
h, j cos θ −Ω′
h′, j sin θ)2
if i = h,
(Ω′
h, j sin θ + Ω′
h′, j cos θ)2
if i = h′,
Ω′2
i, j
otherwise,
and Eq. (10.26) can be written as a function of θ as follows:
G(θ) =
H

j=1
,
γh(Ω′
h, j cos θ −Ω′
h′, j sin θ)2 + γh′(Ω′
h, j sin θ + Ω′
h′, j cos θ)2-
γ′
j
+
K

k=1

λ(k′)
h
λ(k′)
h′
 cos2 θ
sin2 θ
sin2 θ
cos2 θ
 ⎛⎜⎜⎜⎜⎝λ(k′)
h
λ(k′)
h′
⎞⎟⎟⎟⎟⎠
+
K′

k′=1

Ψ(k′)
h,h cos θ −Ψ(k′)
h′,h sin θ + Ψ(k′)
h,h′ sin θ + Ψ(k′)
h′,h′ cos θ
 
+ const.
(10.27)
Since Eq. (10.27) is differentiable at θ = 0, our assumption that Eq. (10.26)
is minimized or maximized when Ω = IH requires that θ = 0 is a stationary
point of Eq. (10.27) for any h  h′. Therefore, it holds that
0 = ∂G
∂θ
θ=0
=
F
2

j
,
γh(Ω′
h, j cos θ −Ω′
h′, j sin θ)(−Ω′
h, j sin θ −Ω′
h′, j cos θ)
+ γh′(Ω′
h, j sin θ + Ω′
h′, j cos θ)(Ω′
h, j cos θ −Ω′
h′, j sin θ)
-
γ′
j
+
K′

k′=1

−Ψ(k′)
h,h sin θ −Ψ(k′)
h′,h cos θ + Ψ(k′)
h,h′ cos θ −Ψ(k′)
h′,h′ sin θ
 Gθ=0
= 2 (γh′ −γh)

j
Ω′
h, jγ′
jΩ′
h′, j +
K′

k′=1

Ψ(k′)
h,h′ −Ψ(k′)
h′,h
 
= 2 (γh′ −γh) Φh,h′.
(10.28)
In the last equation, we used Eq. (10.25) and the assumption that {Ψ(k′)}
are symmetric. Since we assume that Γ is nondegenerate (γh  γh′ for
h  h′), Eq. (10.28) implies that Φ is diagonal, which completes the proof
of Lemma 10.3.
□
10.4.2 Proof for Case 1
Assume that (A∗, B∗, Σ∗
A, ˘Σ
∗
B) is a minimizer, and consider the following
variation deﬁned with an arbitrary H × H orthogonal matrix Ω1:
A = A∗C1/2
B Ω⊤
1 C−1/2
B
,
(10.29)

262
10 Global Solver for Low-Rank Subspace Clustering
B = B∗C−1/2
B
Ω⊤
1 C1/2
B ,
(10.30)
ΣA = C−1/2
B
Ω1C1/2
B Σ∗
AC1/2
B Ω⊤
1 C−1/2
B
,
(10.31)
˘ΣB = (C1/2
B Ω1C−1/2
B
⊗IM) ˘Σ
∗
B(C−1/2
B
Ω⊤
1 C1/2
B
⊗IM).
(10.32)
Then the free energy (10.10) can be written as a function of Ω1:
2F(Ω1) = tr
,
(C−1
A C−1
B Ω1C1/2
B

A∗⊤A∗+ MΣ∗
A
 
C1/2
B Ω⊤
1
-
+ const.
(10.33)
Since Eq. (10.33) is minimized when Ω1 = IH by assumption, Lemma 10.3
implies that
C1/2
B

A∗⊤A∗+ MΣ∗
A
 
C1/2
B
is diagonal. Therefore,
Φ1 = A∗⊤A∗+ MΣ∗
A
(10.34)
is diagonal, with which Eq. (10.16) implies that ˘Σ
∗
B is diagonal.
Since we have proved the diagonality of ˘Σ
∗
B, the expectations in Eqs. (10.10)
and (10.14), respectively, can be expressed in the following simple forms at the
solution (A, B, ΣA, ˘ΣB) = (A∗, B∗, Σ∗
A, ˘Σ
∗
B):
*
B
!
A
⊤A + MΣA
"
B⊤+
rB(B) = B
!
A
⊤A + MΣA
"
B
⊤+ ΞΦ1,
(10.35)

B⊤Γ⊤
VΓV B

rB(B) = B
⊤Γ⊤
VΓVB + ΞΓV,
(10.36)
where ΞΓV ∈RH×H and ΞΦ1 ∈RM×M are diagonal matrices with their entries
given by
(ΞΦ1)m,m =
H

h=1
!
A
⊤A + MΣA
"
h,h σ2
Bm,h,
(ΞΓV)h,h =
M

m=1
γ2
mσ2
Bm,h.
Here {σ2
Bm,h} are the diagonal entries of ˘ΣB such that
˘ΣB = Diag$(σ2
B1,1,. . . , σ2
BM,1), (σ2
B1,2,. . . , σ2
BM,2),. . .. . . , (σ2
B1,H,. . . , σ2
BM,H)%.
Next consider the following variation deﬁned with an M × M matrix Ω2
such that the upper-left J × J submatrix is an arbitrary orthogonal matrix and
the other entries are zero:
A = Ω⊤
2 A∗,
B = Ω⊤
2 B∗.

10.4 Proof of Theorem 10.2
263
Then, by using Eq. (10.35), the free energy (10.10) is written as
2F(Ω2) = 1
σ2 tr
,
Γ⊤
VΓVΩ⊤
2

−2B∗A∗⊤+ B∗
A∗⊤A∗+ MΣA
 
B∗⊤ 
Ω2
-
+ const.
(10.37)
Applying Lemma 10.3 to the upper-left J × J submatrix in the trace, and then
using Eq. (10.22), we ﬁnd that
Φ2 = −2B∗A∗⊤+ B∗
A∗⊤A∗+ MΣA
 
B∗⊤
(10.38)
is diagonal. Eq. (10.38) also implies that B∗A∗⊤is symmetric.
Consider the following variation deﬁned with an M×M matrix Ω3 such that
the upper-left J × J submatrix is an arbitrary orthogonal matrix and the other
entries are zero:
B = Ω⊤
3 B∗.
Then the free energy is written as
2F(Ω3) = 1
σ2 tr
,
Γ⊤
VΓVΩ⊤
3

−2B∗A∗⊤ 
+Γ⊤
VΓVΩ⊤
3

B∗
A∗⊤A∗+ MΣA
 
B∗⊤ 
Ω3
-
+ const.
(10.39)
Applying Lemma 10.3 to the upper-left J × J submatrix in the trace, we ﬁnd
that
Φ3 = B∗
A∗⊤A∗+ MΣA
 
B∗⊤
(10.40)
is diagonal. Since Eqs. (10.34) and (10.40) are diagonal, B∗is diagonal.
Consequently, Eq. (10.14) combined with Eq. (10.36) implies that A∗and Σ∗
A
are diagonal.
Thus we proved that the solution for (A, B, ΣA, ˘ΣB) are diagonal, provided
that CACB is nondegenerate.
10.4.3 Proof for Case 2
When CACB is degenerate, there are multiple equivalent solutions giving the
same free energy (10.10) and the output BA
⊤. In the following, we show that
one of the equivalent solutions has diagonal (A∗, B∗, Σ∗
A, ˘Σ
∗
B).
Assume that CACB = c2IH for some c2 ∈R++. In this case, the free
energy (10.10) is invariant with respect to Ω1 under the transformation (10.29)
through (10.32). Let us focus on the solution with diagonal ˘ΣB, which can be
obtained by the transform (10.29) through (10.32) with a certain Ω1 from any
solution satisfying Eq. (10.16). Then we can show, in the same way as in the

264
10 Global Solver for Low-Rank Subspace Clustering
nondegenerate case, that Eqs. (10.34), (10.38), and (10.40) are diagonal. This
proves the existence of a solution such that (A∗, B∗, Σ∗
A, ˘Σ
∗
B) are diagonal.
10.4.4 Proof for Case 3
When cahcbh = cah′ cbh′ for (not all but) some pairs h  h′, we can show that ΣA
and ˘ΣB are block diagonal where the blocks correspond to the groups sharing
the same cahcbh. In each block, multiple equivalent solutions exist, one of which
is a solution such that (A∗, B∗, Σ∗
A, ˘Σ
∗
B) are diagonal.
This completes the proof of Theorem 10.2.
□
10.5 Exact Global VB Solver (EGVBS)
Theorem 10.2 allows us to focus on the solutions such that (A, B, ΣA, ΣB) are
diagonal. Accordingly, we express the solution of the VB learning problem
(10.11) with diagonal entries, i.e.,
A = DiagM,H(a1,. . . ,aH),
(10.41)
B = DiagM,H(b1,. . . ,bH),
(10.42)
ΣA = Diag(σ2
a1,. . . , σ2
aH),
(10.43)
˘ΣB = Diag$(σ2
B1,1,. . . , σ2
BM,1), (σ2
B1,2,. . . , σ2
BM,2),. . .. . . , (σ2
B1,H,. . . , σ2
BM,H)%,
(10.44)
where DiagD1,D2(·) denotes the D1 × D2 diagonal matrix with the speciﬁed
diagonal entries. Remember that J (≤min(L, M)) is the rank of the observed
matrix V, and {γm} are the singular values arranged in nonincreasing order.
Without loss of generality, we assume thatah,bh ∈R+ for all h = 1,. . . , H.
We can easily obtain the following theorem:
Theorem 10.4
Any local solution of the VB learning problem (10.11)
satisﬁes, for all h = 1,. . . , H,
ah = γ2
h
σ2bhσ2
ah,
(10.45)
σ2
ah = σ2
⎛⎜⎜⎜⎜⎜⎝γ2
hb2
h +
J

m=1
γ2
mσ2
Bm,h + σ2
c2ah
⎞⎟⎟⎟⎟⎟⎠
−1
,
(10.46)
bh = γ2
h
σ2ahσ2
Bh,h,
(10.47)

10.5 Exact Global VB Solver (EGVBS)
265
σ2
Bm,h =
⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩
σ2

γ2
m

a2
h + Mσ2
ah
 
+ σ2
c2
bh
−1
(for m = 1,. . . , J),
c2
bh
(for m = J + 1,. . . , M),
(10.48)
and has the free energy given by
2F = LM log(2πσ2) +
J
h=1 γ2
h
σ2
+
H

h=1
2Fh,
where
(10.49)
2Fh = M log
c2
ah
σ2ah
+
J

m=1
log
c2
bh
σ2
Bm,h
−(M + J) +
a2
h + Mσ2
ah
c2ah
+
b2
h + J
m=1 σ2
Bm,h
c2
bh
+ 1
σ2
⎧⎪⎪⎨⎪⎪⎩γ2
h

−2ahbh +b2
h(a2
h + Mσ2
ah)
 
+
J

m=1
γ2
mσ2
Bm,h(a2
h + Mσ2
ah)
⎫⎪⎪⎬⎪⎪⎭.
(10.50)
Proof
By substituting the diagonal expression, Eqs. (10.41) through (10.44),
into the free energy (10.10), we have
2F = LM log(2πσ2) + M
H

h=1
log
c2
ah
σ2ah
+
M

m=1
H

h=1
log
c2
bh
σ2
Bm,h
+
M
h=1 γ2
h
σ2
−2MH
+
H

h=1
⎧⎪⎪⎨⎪⎪⎩
1
c2ah

a2
h + Mσ2
ah
 
+ 1
c2
bh
⎛⎜⎜⎜⎜⎜⎝b2
h +
M

m=1
σ2
Bm,h
⎞⎟⎟⎟⎟⎟⎠
⎫⎪⎪⎬⎪⎪⎭
+ 1
σ2
H

h=1
⎧⎪⎪⎨⎪⎪⎩γ2
h

−2ahbh +b2
h(a2
h + Mσ2
ah)
 
+
J

m=1
γ2
mσ2
Bm,h(a2
h + Mσ2
ah)
⎫⎪⎪⎬⎪⎪⎭.
(10.51)
Eqs. (10.45) through (10.48) are obtained as the stationary conditions of
Eq. (10.51) that any solution satisﬁes, according to Lemma 10.1. By substitut-
ing Eq. (10.48) for m = J +1,. . . , M into Eq. (10.51), we obtain Eq. (10.49). □
For EVB learning, where the prior covariances CA, CB are also estimated,
we have the following theorem:
Theorem 10.5
Any local solution of the EVB learning problem (10.17)
satisﬁes the following. For each h = 1,. . . , H, (ah,bh, σ2
ah, {σ2
Bm,h}M
m=1, c2
ah, c2
bh)
is either a (positive) stationary point that satisﬁes Eqs. (10.45) through (10.48)
and
c2
ah = a2
h/M + σ2
ah,
(10.52)
c2
bh =
⎛⎜⎜⎜⎜⎜⎝b2
h +
J

m=1
σ2
Bm,h
⎞⎟⎟⎟⎟⎟⎠/J,
(10.53)

266
10 Global Solver for Low-Rank Subspace Clustering
or the null local solution deﬁned by
ah = bh = 0,
σ2
ah = c2
ah →+0,
σ2
Bm,h = c2
bh →+0
(for m = 1,. . . , M),
(10.54)
of which the contribution (10.50) to the free energy is
Fh →+0.
(10.55)
The total free energy is given by Eq. (10.49).
Proof
Considering the derivatives of Eq. (10.51) with respect to c2
ah and c2
bh,
we have
2Mc2
ah = a2
h + Mσ2
ah,
(10.56)
2Mc2
bh = b2
h +
M

m=1
σ2
Bm,h,
(10.57)
as stationary conditions. By using Eq. (10.48), we can easily obtain Eqs.
(10.52) and (10.53).
Unlike in VB learning, where Lemma 10.1 guarantees that any local
solution is a stationary point, there exist nonstationary local solutions in EVB
learning. We can conﬁrm that, along any path such that
ah,bh = 0,
σ2
ah, σ2
Bm,h, c2
ah, c2
bh →+0
with βa =
σ2
ah
c2ah
and βb =
σ2
Bm,h
c2
bh
kept constant,
(10.58)
the free energy contribution (10.50) from the hth component decreases mono-
tonically. Among the possible paths, βa = βb = 1 gives the lowest free energy
(10.55).
□
Based on Theorem 10.5, we can obtain the following corollary for the global
solution.
Corollary 10.6
The global solution of the EVB learning problem (10.17) can
be found in the following way. For each h = 1,. . . , H, ﬁnd all stationary points
that satisfy Eqs. (10.45) through (10.48), (10.52), and (10.53), and choose the
one giving the minimum free energy contribution Fh. The chosen stationary
point is the global solution if Fh < 0. Otherwise (including the case where no
stationary point exists), the null local solution (10.54) is global.
Proof
For each h = 1,. . . , H, any candidate for a local solution is a stationary
point or the null local solution. Therefore, if the minimum free energy con-
tribution over all stationary points is negative, i.e., Fh < 0, the corresponding

10.6 Approximate Global VB Solver (AGVBS)
267
stationary point is the global minimizer. With this fact, Corollary 10.6 is a
straightforward deduction from Theorem 10.5.
□
Taking account of the trivial relations c2
bh = σ2
Bm,h for m > J, the stationary
conditions consisting of Eqs. (10.45) through (10.48), (10.52), and (10.53) for
each h can be seen as a polynomial system, a set of polynomial equations,
with 5 + J unknown variables,

ah,bh, σ2
ah, {σ2
Bm,h}J
m=1, c2
ah, c2
bh
 
. Thus, Theorem
10.5 has decomposed the original problem with O(M2H2) unknown variables,
for which the stationary conditions are given by Eqs. (10.13) through (10.16),
(10.19), and (10.20), into H subproblems with O(J) unknown variables each.
Fortunately, there is a reliable numerical method to solve a polynomial
system, called the homotopy method or continuation method (Drexler, 1978;
Garcia and Zangwill, 1979; Gunji et al., 2004; Lee et al., 2008). It provides all
isolated solutions to a system of n polynomials f(x) ≡( f1(x),. . . , fn(x)) = 0
by deﬁning a smooth set of homotopy systems with a parameter t ∈[0, 1],
i.e., g(x, t) ≡(g1(x, t), g2(x, t),. . . , gn(x, t)) = 0 such that one can continuously
trace the solution path from the easiest (t = 0) to the target (t = 1). For
empirical evaluation, which will be given in Section 10.8, we use HOM4PS-
2.0 (Lee et al., 2008), one of the most successful polynomial system solvers.
With the homotopy method in hand, Corollary 10.6 allows us to solve the
EVB learning problem (10.17) in the following way, which we call the exact
global VB solver (EGVBS). For each h = 1,. . . , H, we ﬁrst ﬁnd all stationary
points that satisfy the polynomial system, Eqs. (10.45) through (10.48),
(10.52), and (10.53). After that, we discard the prohibitive solutions with
complex numbers or negative variances, and then select the stationary point
giving the minimum free energy contribution Fh, deﬁned by Eq. (10.50). The
global solution is the selected stationary point if it satisﬁes Fh < 0; otherwise,
the null local solution (10.54) is the global solution. Algorithm 20 summarizes
the procedure of EGVBS. When the noise variance σ2 is unknown, we conduct
a naive one-dimensional search to minimize the total free energy (10.49), with
EGVBS applied for every candidate value of σ2.
It is straightforward to modify Algorithm 20 to solve the VB learning
problem (10.11), where the prior covariances CA, CB are given. In this case,
we should solve the polynomial system (10.45) through (10.48) in Step 3, and
skip Step 6 since all local solutions are stationary points.
10.6 Approximate Global VB Solver (AGVBS)
Theorems 10.4 and 10.5 signiﬁcantly reduced the complexity of the optimiza-
tion problem. However, EGVBS is still not applicable to data with typical

268
10 Global Solver for Low-Rank Subspace Clustering
Algorithm 20 Exact global VB solver (EGVBS) for LRSC.
1: Compute the SVD of V = Ωleft
V ΓVΩright⊤
V
.
2: for h = 1 to H do
3:
Find all solutions of the polynomial system, Eqs. (10.45) through
(10.48), (10.52), and (10.53) by the homotopy method.
4:
Discard prohibitive solutions with complex numbers or negative vari-
ances.
5:
Select the stationary point giving the smallest Fh (deﬁned by Eq.
(10.50)).
6:
The global solution for the hth component is the selected stationary
point if it satisﬁes Fh < 0; otherwise, the null local solution (10.54)
is the global solution.
7: end for
8: Compute U = Ωright
V
BA
⊤Ωright⊤
V
.
9: Apply spectral clustering with the afﬁnity matrix equal to abs(U) +
abs(U
⊤).
problem sizes. This is because the homotopy method is not guaranteed to ﬁnd
all solutions in polynomial time in J, when the polynomial system involves
O(J) unknown variables.
The following simple trick further reduces the complexity and leads to
an efﬁcient approximate solver. Let us impose an additional constraint that
γ2
mσ2
Bm,h are constant over m = 1,. . . , J, i.e.,
γ2
mσ2
Bm,h = σ
2
bh
for
m = 1,. . . , J.
(10.59)
Under this constraint, the stationary conditions for the six unknowns
(ah,bh, σ2
ah, σ
2
bh, c2
ah, c2
bh) (for each h) become similar to the stationary
conditions for fully observed MF, which allows us to obtain the following
theorem:
Theorem 10.7
Under the constraint (10.59), any stationary point of the free
energy (10.50) for each h satisﬁes the following polynomial equation for a
single variable γh ∈R:
ξ6γ
6
h + ξ5γ
5
h + ξ4γ
4
h + ξ3γ
3
h + ξ2γ
2
h + ξ1γh + ξ0 = 0,
(10.60)
where
ξ6 =
φ2
h
γ2
h ,
(10.61)
ξ5 = −2
φ2
hMσ2
γ3
h
+ 2φh
γh ,
(10.62)

10.6 Approximate Global VB Solver (AGVBS)
269
ξ4 =
φ2
hM2σ4
γ4
h
−2φh(2M−J)σ2
γ2
h
+ 1 +
φ2
h(Mσ2−γ2
h)
γ2
h
,
(10.63)
ξ3 = 2φhM(M−J)σ4
γ3
h
−2(M−J)σ2
γh
+
φh((M+J)σ2−γ2
h)
γh
−
φ2
hMσ2(Mσ2−γ2
h)
γ3
h
+
φh(Mσ2−γ2
h)
γh
,
(10.64)
ξ2 = (M−J)2σ4
γ2
h
−
φhMσ2((M+J)σ2−γ2
h)
γ2
h
+ ((M + J)σ2 −γ2
h) −
φh(M−J)σ2(Mσ2−γ2
h)
γ2
h
,
(10.65)
ξ1 = −
(M−J)σ2((M+J)σ2−γ2
h)
γh
+ φhMJσ4
γh
,
(10.66)
ξ0 = MJσ4.
(10.67)
Here φh = 1 −
γ2
h
γ2 for γ2 = (J
m=1 γ−2
m /J)−1. For each real solution γh such that
γh = γh + γh −Mσ2
γh ,
(10.68)
κh = γ2
h −(M + J)σ2 −

Mσ2 −γ2
h
 
φh
γh
γh
,
(10.69)
τh =
1
2MJ

κh +
B
κ2
h −4MJσ4
!
1 + φh
γh
γh
"
,
(10.70)
δh =
σ2
√
τh

γh −Mσ2
γh −γh
 −1 ,
(10.71)
are real and positive, there exists the corresponding stationary point given by
!
ah,bh, σ2
ah, σ
2
bh, c2
ah, c2
bh
"
=
⎛⎜⎜⎜⎜⎜⎝
.
γhδh,
√
γh/δh
γh
, σ2δh
γh ,
σ2
γhδh−φh
σ2
√
τh
,
C
τh,
√
τh
γ2
h
⎞⎟⎟⎟⎟⎟⎠.
(10.72)
Given the noise variance σ2, computing the coefﬁcients (10.61) through
(10.67) is straightforward. Theorem 10.7 implies that the following algorithm,
which we call the AGVBS, provides the global solution of the EVB learning
problem (10.17) under the additional constraint (10.59). After computing the
SVD of the observed matrix V, AGVBS ﬁrst ﬁnds all real solutions of the
sixth-order polynomial equation (10.60) by using, e.g., the “roots” command
in MATLAB R
⃝, for each h. Then, it discards the prohibitive solutions such
that any of Eqs. (10.68) through (10.71) gives a complex or negative number.
For each of the retained solutions, AGVBS computes the corresponding
stationary point by Eq. (10.72), along with the free energy contribution Fh
by Eq. (10.50). Here, Eq. (10.59) is used for retrieving the original posterior
variances {σ2
Bm,h}J
m=1 for B. Finally, AGVBS selects the stationary point giving
the minimum free energy contribution Fh. The global solution is the selected
stationary point if it satisﬁes Fh < 0; otherwise, the null local solution (10.54)
is the global solution. Algorithm 21 summarizes the procedure of AGVBS.

270
10 Global Solver for Low-Rank Subspace Clustering
Algorithm 21 Approximate global VB solver (AGVBS) for LRSC.
1: Compute the SVD of V = Ωleft
V ΓVΩright⊤
V
.
2: for h = 1 to H do
3:
Find all real solutions of the sixth-order polynomial equation (10.60).
4:
Discard prohibitive solutions such that any of Eqs. (10.68) through
(10.71) gives a complex or negative number.
5:
Compute the corresponding stationary point by Eq. (10.72) and its free
energy contribution Fh by Eq. (10.50) for each of the retained solutions.
6:
Select the stationary point giving the minimum free energy contribution
Fh.
7:
The global solution for the hth component is the selected stationary
point if it satisﬁes Fh < 0; otherwise, the null local solution (10.54) is
the global solution.
8: end for
9: Compute U = Ωright
V
BA
⊤Ωright⊤
V
.
10: Apply spectral clustering with the afﬁnity matrix equal to abs(U) +
abs(U
⊤).
As in EGVBS, a naive one-dimensional search is conducted when the noise
variance σ2 is unknown.
In Section 10.8, we show that AGVBS is practically a good alternative to the
Kronecker product covariance approximation (KPCA), an approximate EVB
algorithm for LRSC under the Kronecker product covariance constraint (see
Section 3.4.2), in terms of accuracy and computation time.
10.7 Proof of Theorem 10.7
Let us rescalebh and c2
bh as follows:
bh = γhbh,
c2
bh = γ2
hc2
bh.
(10.73)
By substituting Eqs. (10.59) and (10.73) into Eq. (10.50), we have
2Fh = M log
c2
ah
σ2ah
+ J log
c2
bh
σ
2
bh
+ 1
c2ah

a2
h + Mσ2
ah
 
+ 1
c2
bh
⎛⎜⎜⎜⎜⎜⎝b
2
h + J γ2
h
γ2 σ
2
bh
⎞⎟⎟⎟⎟⎟⎠
+ 1
σ2

−2γhahbh + (a2
h + Mσ2
ah)(b
2
h + Jσ
2
bh)

−(M + J) +
J

m=1
log γ2
m
γ2
h
,
(10.74)

10.7 Proof of Theorem 10.7
271
where
γ2 =
⎛⎜⎜⎜⎜⎜⎝
J

m=1
γ−2
m /J
⎞⎟⎟⎟⎟⎟⎠
−1
.
Ignoring the last two constant terms, we ﬁnd that Eq. (10.74) is in almost the
same form as the free energy of fully observed MF for a J×M observed matrix
(see Eq. (6.43)). Only the difference is in the fourth term: Jσ
2
bh is multiplied by
γ2
h
γ2 . Note that, as in MF, the free energy (10.74) is invariant under the following
transformation:
2
(ah,bh, σ2
ah, σ
2
bh, c2
ah, c2
bh)
3
→
2
(shah, s−1
h bh, s2
hσ2
ah, s−2
h σ
2
bh, s2
hc2
ah, s−2
h c2
bh)
3
for any {sh  0; h = 1,. . . , H}. Accordingly, we ﬁx the ratio between cah and
cbh to cah/cbh = 1 without loss of generality.
By differentiating the free energy (10.74) with respect toah, σ2
ah,bh, σ2
bh, c2
ah,
and c2
bh, respectively, we obtain the following stationary conditions:
ah = 1
σ2 γhbhσ2
ah,
(10.75)
σ2
ah = σ2

b
2
h + Jσ
2
bh + σ2
c2ah
−1
,
(10.76)
bh = γhah
⎛⎜⎜⎜⎜⎜⎝a2
h + Mσ2
ah + σ2
c2
bh
⎞⎟⎟⎟⎟⎟⎠
−1
,
(10.77)
σ2
bh = σ2
⎛⎜⎜⎜⎜⎜⎜⎝a2
h + Mσ2
ah + σ2γ2
h
c2
bhγ2
⎞⎟⎟⎟⎟⎟⎟⎠
−1
,
(10.78)
c2
ah = a2
h/M + σ2
ah,
(10.79)
c2
bh = b
2
h/J + γ2
h
γ2 σ
2
bh.
(10.80)
Note that, unlike the case of fully observed MF, A and B are not symmetric,
which makes analysis more involved. Apparently, if ah = 0 or bh = 0, the null
solution (10.54) gives the minimum Fh →+0 of the free energy (10.74). In
the following, we identify the positive stationary points such that ah,bh > 0.
To this end, we derive a polynomial equation with a single unknown variable
from the stationary conditions (10.75) through (10.80). Let
γh = ahbh,
(10.81)
δh = ah/bh.
(10.82)

272
10 Global Solver for Low-Rank Subspace Clustering
From Eqs. (10.75) through (10.78), we obtain
γ2
h =
⎛⎜⎜⎜⎜⎜⎝a2
h + Mσ2
ah + σ2
c2
bh
⎞⎟⎟⎟⎟⎟⎠

b
2
h + Jσ
2
bh + σ2
c2ah

,
(10.83)
γhδ−1
h =

b
2
h + Jσ
2
bh + σ2
c2ah

,
(10.84)
γhδh =
⎛⎜⎜⎜⎜⎜⎝a2
h + Mσ2
ah + σ2
c2
bh
⎞⎟⎟⎟⎟⎟⎠.
(10.85)
Substituting Eq. (10.84) into Eq. (10.76) gives
σ2
ah = σ2δh
γh
.
(10.86)
Substituting Eq. (10.85) into Eq. (10.78) gives
σ
2
bh =
σ2
γhδh −φh σ2
c2
bh
,
(10.87)
where
φh = 1 −γ2
h
γ2 .
Thus, the variances σ2
ah and σ
2
bh have been written as functions of δh and c2
bh.
Substituting Eqs. (10.86) and (10.87) into Eq. (10.78) gives
σ2
γhδh −φh σ2
c2
bh
⎛⎜⎜⎜⎜⎜⎜⎝a2
h + M σ2δh
γh
+ σ2γ2
h
c2
bhγ2
⎞⎟⎟⎟⎟⎟⎟⎠= σ2,
and therefore
γh + Mσ2
γh
−γh + σ2
c2
bh
δ−1
h = 0.
Solving the preceding equation with respect to δ−1
h gives
δ−1
h =
c2
bh
σ2

γh −Mσ2
γh
−γh

.
(10.88)
Thus, we have obtained an expression of δh as a function of γh and c2
bh.
Substituting Eqs. (10.86) and (10.87) into Eq. (10.76) gives
σ2δh
γh
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
b
2
h + J
σ2
γhδh −φh σ2
c2
bh
+ σ2
c2ah
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠= σ2.

10.7 Proof of Theorem 10.7
273
Rearranging the previous equation with respect toδ−1
h gives
$γh −γh
% φhσ2
c2
bhγh
δ−2
h +
⎛⎜⎜⎜⎜⎜⎝γh + Jσ2
γh
−γh −
φhσ4
c2ahc2
bhγh
⎞⎟⎟⎟⎟⎟⎠δ−1
h + σ2
c2ah
= 0.
(10.89)
Substituting Eq. (10.88) into Eq. (10.89), we have
φh
γh
$γh −γh
% 
γh −

γh −Mσ2
γh
2
−
σ4
c2ahc2
bh
+
⎛⎜⎜⎜⎜⎜⎝γh −
⎛⎜⎜⎜⎜⎜⎝γh −Jσ2
γh
+
φhσ4
c2ahc2
bhγh
⎞⎟⎟⎟⎟⎟⎠
⎞⎟⎟⎟⎟⎟⎠

γh −

γh −Mσ2
γh

= 0.
(10.90)
Thus we have derived an equation that includes only two unknown variables,
γh and c2
ahc2
bh.
Next we will obtain another equation that includes only γh and c2
ahc2
bh.
Substituting Eqs. (10.79) and (10.80) into Eq. (10.83), we have
γ2
h =
⎛⎜⎜⎜⎜⎜⎝Mc2
ah + σ2
c2
bh
⎞⎟⎟⎟⎟⎟⎠

Jc2
bh + Jφhσ
2
bh + σ2
c2ah

.
(10.91)
Substituting Eq. (10.88) into Eq. (10.87) gives
σ
2
bh =
c2
bh

γh −Mσ2
γh −γh
 
γh −φh

γh −Mσ2
γh −γh
 .
(10.92)
Substituting Eq. (10.92) into Eq. (10.91) gives
γ2
h = MJc2
ahc2
bh + (M + J)σ2 +
σ4
c2ahc2
bh
+ Jφh

Mc2
ahc2
bh + σ2 
γh −Mσ2
γh −γh
 
γh −φh

γh −Mσ2
γh −γh
 
.
Rearranging the preceding equation with respect to c2
ahc2
bh, we have
MJc4
ahc4
bh +
⎛⎜⎜⎜⎜⎜⎝(M + J)σ2 −γ2
h +

Mσ2 −γ2
h
 
φh
γh
γh
⎞⎟⎟⎟⎟⎟⎠c2
ahc2
bh + σ4
⎛⎜⎜⎜⎜⎜⎝1 + φh
γh
γh
⎞⎟⎟⎟⎟⎟⎠= 0,
(10.93)
where
γh = γh −

γh −Mσ2
γh

.
(10.94)
The solution of Eq. (10.93) with respect to c2
ahc2
bh is given by
c2
ahc2
bh =
κh +
B
κ2
h −4MJσ4
!
1 + φh
γh
γh
"
2MJ
,
(10.95)

274
10 Global Solver for Low-Rank Subspace Clustering
where
κh = γ2
h −(M + J)σ2 −

Mσ2 −γ2
h
 
φh
γh
γh
.
(10.96)
By using Eq. (10.94), Eq. (10.90) can be rewritten as
1
γh
φh

γh −Mσ2
γh

γ
2
h +

γh −(M −J)σ2
γh

γh −
 1
γh
φhγh + 1

σ4
c2ahc2
bh
= 0.
(10.97)
Thus, we have obtained two equations, Eqs. (10.95) and (10.97), that relate
two unknown variables, γh (or γh) and c2
ahc2
bh. Substituting Eq. (10.95) into Eq.
(10.97) gives a polynomial equation involving only a single unknown variable
γh. With some algebra, we obtain Eq. (10.60).
Let
τh = c2
ahc2
bh.
(10.98)
Since we ﬁxed the arbitrary ratio to c2
ah/c2
bh = 1, we have
c2
ah =
C
τh,
(10.99)
c2
bh =
C
τh.
(10.100)
Some solutions of Eq. (10.60) have no corresponding points in the problem
domain (10.18). Assume that a solution γh is associated with a point in
the domain. Then γh is given by Eq. (10.94), which is real and positive by
its deﬁnition (10.81). τh is deﬁned and given, respectively, by Eqs. (10.98)
and (10.95), which is real and positive. κh, deﬁned by Eq. (10.96), is also
real and positive, since τh cannot be real and positive otherwise. δh is given
by Eq. (10.88), which is real and positive by its deﬁnition (10.82). Finally,
remembering the variable change (10.73), we can obtain Eq. (10.72) from Eqs.
(10.81), (10.82), (10.86), (10.87), (10.99), and (10.100), which completes the
proof of Theorem 10.7.
□
10.8 Empirical Evaluation
In this section, we empirically compare the global solvers, EGVBS (Algorithm
20) and AGVBS (Algorithm 21), with the standard iterative algorithm (Algo-
rithm 4 in Section 3.4.2) and its approximation (Algorithm 5 in Section 3.4.2),
which we here call the standard VB (SVB) iteration and the KPCA iteration,
respectively. We assume that the prior covariances (CA, CB) and the noise

10.8 Empirical Evaluation
275
variance σ2 are unknown and estimated from observation. We use the full-
rank model (i.e., H = min(L, M)), and expect EVB learning to automatically
ﬁnd the true rank without any parameter tuning.
Artiﬁcial Data Experiment
We ﬁrst conducted an experiment with a small artiﬁcial data set (“artiﬁcial
small”), on which the exact algorithms, i.e., EGVBS and the SVB iteration,
are computationally tractable. Through this experiment, we can assess the
accuracy of the efﬁcient approximate solvers, i.e., AGVBS and the KPCA
iteration. We randomly created M = 75 samples in the L = 10 dimensional
space. We assumed K = 2 clusters: M(1)∗= 50 samples lie in a H(1)∗= 3-
dimensional subspace, and the other M(2)∗= 25 samples lie in a H(2)∗=
1-dimensional subspace. For each cluster k, we independently drew M(k)∗
samples from GaussH(k)∗(0, 10 · IH(k)∗), and projected them onto the observed
L-dimensional space by R(k) ∈RL×H(k)∗, each entry of which follows R(k)
l,h ∼
Gauss1(0, 1). Thus, we obtained a noiseless matrix V(k)∗∈RL×M(k)∗for the kth
cluster. Concatenating all clusters, V∗= (V(1)∗,. . . , V(K)∗), and adding random
noise subject to Gauss1(0, 1) to each entry gave an artiﬁcial observed matrix
V ∈RL×M, where M = K
k=1 M(k)∗= 75. The true rank of V∗is given by
H∗= min(K
k=1 H(k)∗, L, M) = 4. Note that H∗is different from the rank of
the observed matrix V, which is almost surely equal to J = min(L, M) (= 10)
under the Gaussian noise.
Figure 10.1 shows the free energy, the computation time, and the estimated
rank of U = B
′A
′⊤over iterations. For the iterative methods, we show the
results of 10 trials starting from different random initializations. We can see
that AGVBS gives almost the same free energy as the exact methods (EGVBS
and the SVB iteration). The exact methods require large computation costs:
EGVBS took 621 sec to obtain the global solution, and the SVB iteration
took ∼100 sec to achieve almost the same free energy. On the other hand,
the approximate methods are much faster: AGVBS took less than 1 sec, and
the KPCA iteration took ∼10 sec. Since the KPCA iteration had not converged
after 250 iterations, we continued its computation until 2,500 iterations, and
found that it sometimes converges to a local solution with a signiﬁcantly higher
free energy than the other methods. EGVBS, AGVBS, and the SVB iteration
successfully found the true rank H∗= 4, while the KPCA iteration sometimes
failed to ﬁnd it. This difference is actually reﬂected to the clustering error, i.e.,
the misclassiﬁcation rate with all possible cluster correspondences taken into
account, after spectral clustering (Shi and Malik, 2000) is performed: 1.3% for
EGVBS, AGVBS, and the SVB iteration, and 2.4% for the KPCA iteration.

276
10 Global Solver for Low-Rank Subspace Clustering
0
50
100
150
200
250
Iteration
1.8
1.9
2
2.1
2.2
2.3
EGVBS
AGVBS
SVB iteration
KPCA iteration
(a) Free energy
0
50
100
150
200
250
Iteration
100
102
104
Time(sec)
EGVBS
AGVBS
SVB iteration
KPCA iteration
(b) Computation time
0
50
100
150
200
250
Iteration
0
2
4
6
8
10
EGVBS
AGVBS
SVB iteration
KPCA iteration
(c) Estimated rank
Figure 10.1 Results on the “artiﬁcial small” data set (L = 10, M = 75, H∗= 4).
The clustering errors were 1.3% for EGVBS, AGVBS, and the SVB iteration, and
2.4% for the KPCA iteration.
Next we conducted the same experiment with a larger artiﬁcial data set
(“artiﬁcial large”) (L = 50, K = 4, (M(1)∗,. . . , M(K)∗) = (100, 50, 50, 25),
(H(1)∗,. . . , H(K)∗) = (2, 1, 1, 1)), on which EGVBS and the SVB iteration are
computationally intractable. Figure 10.2 shows the results with AGVBS and
the KPCA iteration. The advantage in computation time is clear: AGVBS only
took ∼0.1 sec, while the KPCA iteration took more than 100 sec. The clustering
errors were 4.0% for AGVBS and 11.2% for the KPCA iteration.
Benchmark Data Experiment
Finally, we applied AGVBS and the KPCA iteration to the Hopkins 155 motion
database (Tron and Vidal, 2007). In this data set, each sample corresponds to
the trajectory of a point in a video, and clustering the trajectories amounts to
ﬁnding a set of rigid bodies. Figure 10.3 shows the results on the “1R2RC”
(L = 59, M = 459) sequence.1 We see that AGVBS gave a lower free energy
with much less computation time than the KPCA iteration. Figure 10.4 shows
the clustering errors on the ﬁrst 20 sequences, which implies that AGVBS
generally outperforms the KPCA iteration. Figure 10.4 also shows the results
1 Peaks in the free energy curves are due to pruning. As noted in Section 3.1.1, the free energy
can increase right after pruning happens, but immediately gets lower than the free energy before
pruning.

10.8 Empirical Evaluation
277
0
500
1,000
1,500
2,000 2,500
Iteration
1.61
1.615
1.62
1.625
1.63
1.635
1.64
AGVBS
KPC alteration
(a) Free energy
0
500
1,000
1,500
2,000
2,500
Iteration
100
102
104
Time(sec)
AGVBS
KPC alteration
(b) Computation time
0
500
1,000
1,500
2,000
2,500
Iteration
0
5
10
15
AGVBS
KPC alteration
(c) Estimated rank
Figure 10.2 Results on the “artiﬁcial large” data set (L = 50, M = 225, H∗= 5).
The clustering errors were 4.0% for AGVBS and 11.2% for the KPCA iteration.
0
500
1,000
1,500
2,000
2,500
Iteration
2
3
4
5
6
7
AGVBS
KPC alteration
(a) Free energy
0
500
1,000
1,500
2,000
2,500
Iteration
100
102
104
Time(sec)
AGVBS
KPC aIteration
(b) Computation time
0
500
1,000
1,500
2,000
2,500
Iteration
0
10
20
30
40
50
AGVBS
KPC alteration
(c) Estimated rank
Figure 10.3 Results on the “1R2RC” sequence (L = 59, M = 459) of the Hopkins
155 motion database. Peaks in the free energy curves are due to pruning. The
clustering errors are shown in Figure 10.4.

278
10 Global Solver for Low-Rank Subspace Clustering
0
0.005
0.01
0.015
0.02
0.025
0.03
1R2RC 
1R2RCR 
1R2RCR_g12 
1R2RCR_g13 
1R2RCR_g23 
1R2RCT_A 
R2RCT_A_g12 
R2RCT_A_g13 
R2RCT_A_g23 
1R2RCT_B 
R2RCT_B_g12 
R2RCT_B_g13 
R2RCT_B_g23 
1R2RC_g12 
1R2RC_g13 
1R2RC_g23 
1R2TCR 
1R2TCRT 
1R2TCRT_g12 
1R2TCRT_g13 
MAP (with optimized lambda)
AGVBS
KPC aIteration
Figure 10.4 Clustering errors on the ﬁrst 20 sequences of the Hopkins 155
data set.
by MAP learning (Eq. (3.87) in Section 3.4) with the tuning parameter λ
optimized over the 20 sequences (i.e., we performed MAP learning with
different values for λ, and selected the one giving the lowest average clustering
error). We see that AGVBS performs comparably to MAP learning with
optimized λ, which implies that EVB learning estimates the hyperparameters
and the noise variance reasonably well.

11
Efﬁcient Solver for Sparse Additive
Matrix Factorization
In this chapter, we introduce an efﬁcient variational Bayesian (VB) solver
(Nakajima et al., 2013b) for sparse additive matrix factorization (SAMF),
where the global VB solver, derived in Chapter 9, for fully observed MF is
used as a subroutine.
11.1 Problem Description
The SAMF model, introduced in Section 3.5, is deﬁned as
p(V|Θ) ∝exp
⎛⎜⎜⎜⎜⎜⎜⎜⎝−1
2σ2
#######V −
S
s=1
U(s)
#######
2
Fro
⎞⎟⎟⎟⎟⎟⎟⎟⎠,
(11.1)
p({Θ(s)
A }S
s=1) ∝exp

−1
2
S
s=1
K(s)

k=1
tr

A(k,s)C(k,s)−1
A
A(k,s)⊤ 
,
(11.2)
p({Θ(s)
B }S
s=1) ∝exp

−1
2
S
s=1
K(s)

k=1
tr

B(k,s)C(k,s)−1
B
B(k,s)⊤ 
,
(11.3)
where
U(s) = G({B(k,s)A(k,s)⊤}K(s)
k=1; X(s))
(11.4)
is the
sth sparse matrix factorization (SMF) term. Here G(·; X)
:
R
K
k=1(L′(k)×M′(k)) →RL×M maps the partitioned-and-rearranged (PR) matrices
{U′(k)}K
k=1 to the target matrix U ∈RL×M, based on the one-to-one map
X : (k, l′, m′) →(l, m) from the indices of the entries in {U′(k)}K
k=1 to the
indices of the entries in U such that

G({U′(k)}K
k=1; X)
 
l,m = Ul,m = UX(k,l′,m′) = U′(k)
l′,m′.
(11.5)
279

280
11 Efﬁcient Solver for Sparse Additive Matrix Factorization
The prior covariances of A(k,s) and B(k,s) are assumed to be diagonal and
positive-deﬁnite:
C(k,s)
A
= Diag(c(k,s)2
a1
,. . . , c(k,s)2
aH
),
C(k,s)
B
= Diag(c(k,s)2
b1
,. . . , c(k,s)2
bH
),
and Θ summarizes the parameters as follows:
Θ = {Θ(s)
A , Θ(s)
B }S
s=1, where Θ(s)
A = {A(k,s)}K(s)
k=1, Θ(s)
B = {B(k,s)}K(s)
k=1.
Under the independence constraint,
r(Θ) =
S
s=1
r(s)
A (Θ(s)
A )r(s)
B (Θ(s)
B ),
(11.6)
the VB posterior minimizing the free energy can be written as
r(Θ) =
S
s=1
K(s)

k=1

MGaussM′(k,s),H′(k,s)(A(k,s); A
(k,s), Σ
(k,s)
A )
· MGaussL′(k,s),H′(k,s)(B(k,s); B
(k,s), Σ
(k,s)
B
)

=
S
s=1
K(s)

k=1
⎛⎜⎜⎜⎜⎜⎜⎝
M′(k,s)

m′=1
GaussH′(k,s)(a(k,s)
m′ ;a
(k,s)
m′ , Σ
(k,s)
A )
·
L′(k,s)

l′=1
GaussH′(k,s)(b
(k,s)
l′
;b
(k,s)
l′
, Σ
(k,s)
B
)
⎞⎟⎟⎟⎟⎟⎟⎠.
(11.7)
The free energy can be explicitly written as
2F = LM log(2πσ2) + ∥V∥2
Fro
σ2
+
S
s=1
K(s)

k=1
⎛⎜⎜⎜⎜⎜⎝M′(k,s) log
det

C(k,s)
A
 
det
!
Σ
(k,s)
A
" + L′(k,s) log
det

C(k,s)
B
 
det
!
Σ
(k,s)
B
"
⎞⎟⎟⎟⎟⎟⎠
+
S
s=1
K(S )

k=1
tr
2
C(k,s)−1
A
(A
(k,s)⊤A
(k,s) + M′(k,s)Σ
(k,s)
A
)
+ C(k,s)−1
B
(B
(k,s)⊤B
(k,s) + L′(k,s)Σ
(k,s)
B
)
3
+ 1
σ2 tr
⎧⎪⎪⎨⎪⎪⎩−2V⊤
⎛⎜⎜⎜⎜⎜⎝
S
s=1
G({B
(k,s)A
(k,s)⊤}K(s)
k=1; X(s))
⎞⎟⎟⎟⎟⎟⎠

11.1 Problem Description
281
+ 2
S
s=1
S
s′=s+1
G⊤({B
(k,s)A
(k,s)⊤}K(s)
k=1; X(s))G({B
(k,s′)A
(k,s′)⊤}K(s′)
k=1 ; X(s′))
⎫⎪⎪⎬⎪⎪⎭
+ 1
σ2
S
s=1
K(S )

k=1
tr
2
(A
(k,s)⊤A
(k,s) + M′(k,s)Σ
(k,s)
A
)(B
(k,s)⊤B
(k,s) + L′(k,s)Σ
(k,s)
B
)
3
−
S
s=1
K(S )

k=1
(L′(k,s) + M′(k,s))H′(k,s),
(11.8)
of which the stationary conditions are given by
A
(k,s) = σ−2Z′(k,s)⊤B
(k,s)Σ
(k,s)
A ,
(11.9)
Σ
(k,s)
A
= σ2 !
B
(k,s)⊤B
(k,s) + L′(k,s)Σ
(k,s)
B
+ σ2C(k,s)−1
A
"−1
,
(11.10)
B
(k,s) = σ−2Z′(k,s)A
(k,s)Σ
(k,s)
B
,
(11.11)
Σ
(k,s)
B
= σ2 !
A
(k,s)⊤A
(k,s) + M′(k,s)Σ
(k,s)
A
+ σ2C(k,s)−1
B
"−1
.
(11.12)
Here Z′(k,s) ∈RL′(k,s)×M′(k,s) is deﬁned as
Z′(k,s)
l′,m′ = Z(s)
X(s)(k,l′,m′),
where
Z(s) = V −

s′s
U
(s).
(11.13)
The stationary conditions for the hyperparameters {C(k,s)
A
, C(k,s)
B
}K(s)
k=1,
S
s=1, σ2 are
given as
c(k,s)2
ah
=
####a(k,s)
h
####
2
/M′(k,s) + (Σ
(k,s)
A )hh,
(11.14)
c(k,s)2
bh
=
#####b
(k,s)
h
#####
2
/L′(k,s) + (Σ
(k,s)
B
)hh,
(11.15)
σ2 =
1
LM
⎧⎪⎪⎨⎪⎪⎩∥V∥2
Fro −2
S
s=1
tr
⎛⎜⎜⎜⎜⎜⎝U
(s)⊤
⎛⎜⎜⎜⎜⎜⎝V −
S
s′=s+1
U
(s′)
⎞⎟⎟⎟⎟⎟⎠
⎞⎟⎟⎟⎟⎟⎠
+
S
s=1
K(s)

k=1
tr

(A
(k,s)⊤A
(k,s) + M′(k,s)Σ
(k,s)
A
) · (B
(k,s)⊤B
(k,s) + L′(k,s)Σ
(k,s)
B
)
 
⎫⎪⎪⎪⎬⎪⎪⎪⎭.
(11.16)
The standard VB algorithm (Algorithm 6 in Section 3.5) iteratively applies
Eqs. (11.9) through (11.12) and (11.14) through (11.16) until convergence.

282
11 Efﬁcient Solver for Sparse Additive Matrix Factorization
11.2 Efﬁcient Algorithm for SAMF
In this section, we derive a more efﬁcient algorithm than the standard VB
algorithm. We ﬁrst present a theorem that reduces a partial SAMF problem
to the (fully observed) MF problem, which can be solved analytically. Then
we describe the algorithm that solves the entire SAMF problem.
11.2.1 Reduction of the Partial SAMF Problem to the MF Problem
Let us denote the mean of U(s), deﬁned in Eq. (11.4), over the VB posterior by
U
(s) =

U(s)
r(s)
A (Θ(s)
A )r(s)
B (Θ(s)
B )
= G
2
B
(k,s)A
(k,s)⊤3K(s)
k=1 ; X(s)

.
(11.17)
Then we obtain the following theorem:
Theorem 11.1
Given {U
(s′)}s′s and the noise variance σ2, the VB posterior
of (Θ(s)
A , Θ(s)
B ) = {A(k,s), B(k,s)}K(s)
k=1 coincides with the VB posterior of the
following MF model:
p(Z′(k,s)|A(k,s), B(k,s)) ∝exp

−1
2σ2
###Z′(k,s) −B(k,s)A(k,s)⊤###2
Fro

,
(11.18)
p(A(k,s)) ∝exp

−1
2tr

A(k,s)C(k,s)−1
A
A(k,s)⊤ 
,
(11.19)
p(B(k,s)) ∝exp

−1
2tr

B(k,s)C(k,s)−1
B
B(k,s)⊤ 
,
(11.20)
for each k = 1,. . . , K(s). Here, Z′(k,s) ∈RL′(k,s)×M′(k,s) is deﬁned by Eq. (11.13).
Proof
Given {U
(s)}s′s = {{B
(k,s′)A
(k,s′)⊤}K(s′)
k=1 }s′s and σ2 as ﬁxed constants, the
free energy (11.8) can be written as a function of {A
(k,s), B
(k,s), Σ
(k,s)
A
, Σ
(k,s)
B
, C(k,s)
A
,
C(k,s)
B
}K(s)
k=1 as follows:
2F(s) !
{A
(k,s), B
(k,s), Σ
(k,s)
A
, Σ
(k,s)
B
, C(k,s)
A
, C(k,s)
B
}K(s)
k=1
"
=
K(s)

k=1
2F(k,s) + const.,
(11.21)
where
2F(k,s) = M′(k,s) log
det

C(k,s)
A
 
det
!
Σ
(k,s)
A
" + L′(k,s) log
det

C(k,s)
B
 
det
!
Σ
(k,s)
B
"
+ tr
2
C(k,s)−1
A
(A
(k,s)⊤A
(k,s)+M′(k,s)Σ
(k,s)
A
) + C(k,s)−1
B
(B
(k,s)⊤B
(k,s)+L′(k,s)Σ
(k,s)
B
)
3

11.2 Efﬁcient Algorithm for SAMF
283
+ 1
σ2 tr
2
−2A
(k,s)⊤Z′(k,s)⊤B
(k,s)
+ (A
(k,s)⊤A
(k,s) + M′(k,s)Σ
(k,s)
A
)(B
(k,s)⊤B
(k,s) + L′(k,s)Σ
(k,s)
B
)
3
.
(11.22)
Eq. (11.22) coincides with the free energy of the fully observed matrix
factorization model (11.18) through (11.20) up to a constant (see Eq. (3.23)
with Z′ substituted for V). Therefore, the VB solution is the same.
□
Eq. (11.13) relates the entries of Z(s) ∈RL×M to the entries of {Z′(k,s) ∈
RL′(k,s)×M′(k,s)}K(s)
k=1 by using the map X(s) : (k, l′, m′) →(l, m) (see Eq. (11.5) and
Figure 3.3).
11.2.2 Mean Update Algorithm
Theorem 11.1 states that a partial problem of SAMF—ﬁnding the posterior of
(A(k,s), B(k,s)) for each k = 1,. . . , K(s) given {U
(s′)}s′s and σ2—can be solved
by the global solver for the fully observed MF model. Speciﬁcally, we use
Algorithm 16, introduced in Chapter 9, for estimating each SMF term U
(s)
in turn. We use Eq. (11.16) for updating the noise variance σ2. The whole
procedure, called the mean update (MU) algorithm (Nakajima et al., 2013b),
is summarized in Algorithm 22, where 0(d1,d2) denotes the d1 × d2 matrix with
all entries equal to zero.
The MU algorithm is similar in spirit to the backﬁtting algorithm (Hastie
and Tibshirani, 1986; D’Souza et al., 2004), where each additive term is
updated to ﬁt a dummy target. In the MU algorithm, Z(s) deﬁned in Eq. (11.13)
corresponds to the dummy target. Although the MU algorithm globally solves
a partial problem in each step, its joint global optimality over the entire
Algorithm 22 Mean update (MU) algorithm for VB SAMF.
1: Initialize: U
(s) ←0(L,M) for s = 1,. . . , S , σ2 ←∥V∥2
Fro /(LM).
2: for s = 1 to S do
3:
Compute Z′(k,s) ∈RL′(k,s)×M′(k,s) by Eq. (11.13).
4:
For each partition k = 1,. . . , K(s), compute the solution U′(k,s) =
B(k,s) A(k,s)⊤for the fully observed MF by Algorithm 16 with Z′(k,s) as
the observed matrix.
5:
U
(s) ←G({B
(k,s)A
(k,s)⊤}K(s)
k=1; X(s)).
6: end for
7: Update σ2 by Eq. (11.16).
8: Repeat 2 to 7 until convergence.

284
11 Efﬁcient Solver for Sparse Additive Matrix Factorization
parameter space is not guaranteed. Nevertheless, experimental results in
Section 11.3 show that the MU algorithm performs well in practice.
When Algorithm 16 is applied to the dummy target matrix Z′(k,s)
∈
RL′(k,s)×M′(k,s) in Step 4, singular value decomposition is required, which domi-
nates the computation time. However, for many practical SMF terms, including
the rowwise (3.114), the columnwise (3.115), and the elementwise (3.116)
terms (as well as the segmentwise term, which will be deﬁned for a video
application in Section 11.3.2), Z′(k,s) is a vector or scalar, i.e., L′(k,s) = 1 or
M′(k,s) = 1 holds. In such cases, the singular value and the singular vectors are
given simply by
γ(k,s)
1
= ∥Z′(k,s)∥,
ω(k,s)
a1
= Z′(k,s)/∥Z′(k,s)∥,
ω(k,s)
b1
= 1
if L′(k,s) = 1,
γ(k,s)
1
= ∥Z′(k,s)∥,
ω(k,s)
a1
= 1,
ω(k,s)
b1
= Z′(k,s)/∥Z′(k,s)∥
if M′(k,s) = 1.
11.3 Experimental Results
In this section, we experimentally show good performance of the MU
algorithm (Algorithm 22) over the standard VB algorithm (Algorithm 6 in
Section 3.5). We also demonstrate advantages of SAMF in its ﬂexibility in a
real-world application.
11.3.1 Mean Update vs. Standard VB
We compare the algorithms under the following model:
V = ULRCE + E,
where
ULRCE =
4

s=1
U(s) = Ulow−rank + Urow + Ucolumn + Uelement.
(11.23)
Here, “LRCE” stands for the sum of the low-rank, rowwise, columnwise, and
elementwise terms, each of which is deﬁned in Eqs. (3.113) through (3.116).
We call this model “LRCE”-SAMF. As explained in Section 3.5, “LRCE”-
SAMF may be used to separate the clean signal Ulow−rank from a possible
rowwise sparse component (constantly broken sensors), a columnwise sparse
component (accidental disturbances affecting all sensors), and an element-
wise sparse component (randomly distributed spiky noise). We also evaluate
“LCE”-SAMF, “LRE”-SAMF, and “LE”-SAMF, which can be regarded as
generalizations of robust PCA (Cand`es et al., 2011; Ding et al., 2011; Babacan

11.3 Experimental Results
285
et al., 2012b). Note that “LE”-SAMF corresponds to an SAMF counterpart of
robust PCA.
First, we conducted an experiment with artiﬁcial data. We assume the
empirical VB scenario with unknown noise variance, i.e., all hyperparameters,
{C(k,s)
A
, C(k,s)
B
}K(s)
k=1,
S
s=1, and σ2, are estimated from observations. We use the full-
rank model (H = min(L, M)) for the low-rank term Ulow−rank, and expect the
model-induced regularization (MIR) effect (see Chapter 7) to ﬁnd the true rank
of Ulow−rank, as well as the nonzero entries in Urow, Ucolumn, and Uelement.
We created an artiﬁcial data set with the data matrix size L = 40 and
M = 100, and the rank H∗= 10 for a true low-rank matrix Ulow−rank∗= B∗A∗⊤.
Each entry in A∗∈RL×H∗and B∗∈RL×H∗was drawn from Gauss1(0, 1). A true
rowwise (columnwise) part Urow∗(Ucolumn∗) was created by ﬁrst randomly
selecting ρL rows (ρM columns) for ρ = 0.05, and then adding a noise
subject to GaussM(0, ζIM) (GaussL(0, ζIL)) for ζ = 100 to each of the selected
rows (columns). A true elementwise part Uelement∗was similarly created by
ﬁrst selecting ρLM entries and then adding a noise subject to Gauss1(0, ζ)
to each of the selected entries. Finally, an observed matrix V was created by
adding a noise subject to Gauss1(0, 1) to each entry of the sum ULRCE∗of the
aforementioned four true matrices.
For the standard VB algorithm, we initialized the variational parameters
and the hyperparameters in the following way: the mean parameters,
{A
(k,s), B
(k,s)}K(s)
k=1,
S
s=1, were randomly created so that each entry follows
Gauss1(0, 1); the covariances, {Σ
(k,s)
A
, Σ
(k,s)
B
}K(s)
k=1,
S
s=1 and {C(k,s)
A
, C(k,s)
B
}K(s)
k=1,
S
s=1,
were set to be identity; and the noise variance was set to σ2 = 1. Note that
we rescaled V so that ∥V∥2
Fro /(LM) = 1, before starting iteration. We ran the
standard VB algorithm 10 times, starting from different initial points, and each
trial is plotted by a solid line (labeled as “Standard(iniRan)”) in Figure 11.1.
Initialization for the MU algorithm is simple: we simply set U
(s) = 0(L,M)
for s = 1,. . . , S , and σ2 = 1. Initialization of all other variables is not needed.
Furthermore, we empirically observed that the initial value for σ2 does not
affect the result much, unless it is too small. Actually, initializing σ2 to a large
value is not harmful in the MU algorithm, because it is set to an adequate
value after the ﬁrst iteration with the mean parameters kept U
(s) = 0(L,M). The
performance of the MU algorithm is plotted by the dashed line in Figure 11.1.
Figures 11.1(a) through 11.1(c) show the free energy, the computation time,
and the estimated rank, respectively, over iterations, and Figure 11.1(d) shows
the reconstruction errors after 250 iterations. The reconstruction errors consist
of the overall error
####U
LRCE −ULRCE∗####
2
Fro /(LM), and the four componentwise

286
11 Efﬁcient Solver for Sparse Additive Matrix Factorization
0
50
100
150
200
250
4.1
4.2
4.3
4.4
4.5
4.6
4.7
Iteration
MeanUpdate
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(a) Free energy
0
50
100
150
200
250
0
2
4
6
8
10
Iteration
Time(sec)
MeanUpdate
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(b) Computation time
0
50
100
150
200
250
0
5
10
15
20
25
30
Iteration
MeanUpdate
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(c) Estimated rank
0
1
2
3
4
5
Overall
Low−rank
Row
Column
Element
MeanUpdate
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(d) Reconstruction error
Figure 11.1 Experimental results of “LRCE”-SAMF on an artiﬁcial data set (L =
40, M = 100, H∗= 10, ρ = 0.05).
errors
####U
(s) −U(s)∗####
2
Fro /(LM). The graphs show that the MU algorithm, whose
iteration is computationally slightly more expensive than the standard VB
algorithm, immediately converges to a local minimum with the free energy
substantially lower than the standard VB algorithm. The estimated rank agrees
with the true rank 
H = H∗= 10, while all 10 trials of the standard VB
algorithm failed to estimate the true rank. It is also observed that the MU
algorithm well reconstructs each of the four terms.
We can slightly improve the performance of the standard VB algo-
rithm by adopting different initialization schemes. The line labeled as
“Standard(iniML)” in Figure 11.1 indicates the maximum likelihood (ML)
initialization, i.e, (a(k,s)
h
,b
(k,s)
h
) = (γ(k,s)1/2
h
ω(k,s)
ah , γ(k,s)1/2
h
ω(k,s)
bh ). Here, γ(k,s)
h
is the
hth largest singular value of the (k, s)th PR matrix V′(k,s) of V (such that
V′(k,s)
l′,m′
=
VX(s)(k,l′,m′)), and ω(k,s)
ah
and ω(k,s)
bh
are the associated right and
left singular vectors. Also, we empirically found that starting from small
σ2 alleviates the local minimum problem. The line labeled as “Standard
(iniMLSS)” indicates the ML initialization with σ2 = 0.0001. We can see that
this scheme successfully recovered the true rank. However, it still performs
substantially worse than the MU algorithm in terms of the free energy and the
reconstruction error.

11.3 Experimental Results
287
0
50
100
150
200
250
3
3.2
3.4
3.6
3.8
4
4.2
Iteration
MeanUpdate
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(a) Free energy
0
50
100
150
200
250
0
20
40
60
80
Iteration
Time(sec)
MeanUpdate
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(b) Computation time
0
50
100
150
200
250
0
10
20
30
40
50
60
Iteration
MeanUpdate
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(c) Estimated rank
0
1
2
3
4
Overall
Low−rank
Row
Column
Element
MeanUpdate
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(d) Reconstruction error
Figure 11.2 Experimental results of “LE”-SAMF on an artiﬁcial data set (L =
100, M = 300, H∗= 20, ρ = 0.1).
Figure 11.2 shows results of “LE”-SAMF on another artiﬁcial data set
with L = 100, M = 300, H∗= 20, and ρ = 0.1. We see that the MU
algorithm performs better than the standard VB algorithm. We also tested
various SAMF models including “LCE”-SAMF, “LRE”-SAMF, and “LE”-
SAMF under different settings for M, L, H∗, and ρ, and empirically found
that the MU algorithm generally gives a better solution with lower free energy
and smaller reconstruction errors than the standard VB algorithm.
Next, we conducted experiments on several data sets from the UCI reposi-
tory (Asuncion and Newman, 2007). Since we do not know the true model of
those data sets, we only focus on the achieved free energy. Figure 11.3 shows
the free energy after convergence in “LRCE”-SAMF, “LCE”-SAMF, “LRE”-
SAMF, and “LE”-SAMF. For better comparison, a constant is added so that
the free energy achieved by the MU algorithm is zero. We can see a clear
advantage of the MU algorithm over the standard VB algorithm.
11.3.2 Real-World Application
Finally, we demonstrate the usefulness of the ﬂexibility of SAMF in a
foreground (FG)/background (BG) video separation problem (Figure 3.5 in

288
11 Efﬁcient Solver for Sparse Additive Matrix Factorization
−0.5
0
0.5
chart
forestfires
glass
iris
spectf
wine
MeanUpdate
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(a) “LRCE”-SAMF
−0.5
0
0.5
chart
forestfires
glass
iris
spectf
wine
MeanUpdate
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(b) “LCE”-SAMF
(c) “LRE”-SAMF
−0.5
0
0.5
chart
forestfires
glass
iris
spectf
wine
MeanUpdate
Standard(iniML)
Standard(iniMLSS)
Standard(iniRan)
(d) “LE”-SAMF
Figure 11.3 Experimental results on benchmark data sets. For better comparison,
a constant is added so that the free energy achieved by the MU algorithm is zero
(therefore, the bar for “MeanUpdate” is invisible).
Section 3.5). Cand`es et al. (2011) formed the observed matrix V by stacking
all pixels in each frame into each column (Figure 3.6), and applied the robust
PCA (with “LE”-terms)—the low-rank term captures the static BG and the
elementwise (or pixelwise) term captures the moving FG, e.g., people walking
through. SAMF can be seen as an extension of the VB variant of robust PCA
(Babacan et al., 2012b). Accordingly, we use “LE”-SAMF,
V = Ulow−rank + Uelement + E,
as a baseline method for comparison.
The SAMF framework enables a ﬁne-tuned design for the FG term.
Assuming that pixels in an image segment with similar intensity values tend
to share the same label (i.e., FG or BG), we formed a segmentwise sparse
SMF term: U′(k) for each k is a column vector consisting of all pixels in each
segment. We produced an oversegmented image from each frame by using
the efﬁcient graph-based segmentation (EGS) algorithm (Felzenszwalb and
Huttenlocher, 2004), and substituted the segmentwise sparse term for the FG
term (see Figure 3.7):
V = Ulow−rank + Usegment + E.

11.3 Experimental Results
289
We call this model segmentation-based SAMF (sSAMF). Note that EGS is
computationally very efﬁcient: it takes less than 0.05 sec on a usual laptop to
segment a 192 × 144 gray image. EGS has several tuning parameters, and the
obtained segmentation is sensitive to some of them. However, we conﬁrmed
that sSAMF performs similarly with visually different segmentations obtained
over a wide range of tuning parameters (see the detailed information in the
section “Segmentation Algorithm”). Therefore, careful parameter tuning of
EGS is not necessary for our purpose.
We compared sSAMF with “LE”-SAMF on the “WalkByShop1front” video
from the Caviar data set.1 Thanks to the Bayesian framework, all unknown
parameters (except the ones for segmentation) are estimated from the data,
and therefore no manual parameter tuning is required. For both models (“LE”-
SAMF and sSAMF), we used the MU algorithm, which was shown in Section
11.3.1 to be practically more reliable than the standard VB algorithm. The
original video consists of 2,360 frames, each of which is a color image with
384 × 288 pixels. We resized each image into 192 × 144 pixels, averaged
over the color channels, and subsampled every 15 frames (the frame IDs are
0, 15, 30,. . . , 2355). Thus, V is of the size of 27,684 [pixels] × 158 [frames].
We evaluated “LE”-SAMF and sSAMF on this video, and found that both
models perform well (although “LE”-SAMF failed in a few frames).
In order to contrast between the two models more clearly, we created a more
difﬁcult video by subsampling every ﬁve frames from 1,501 to 2,000 (the frame
IDs are 1501, 1506,. . . , 1996 and V is of the size of 27,684 [pixels] × 100
[frames]). Since more people walked through in this period, FG/BG separation
is more challenging.
Figure 11.4(a) shows one of the original frames. This is a difﬁcult snap
shot, because a person stayed at the same position for a while, which confuses
separation—any object in the FG pixels is assumed to be moving. Figures
11.4(c) and 11.4(d) show the BG and the FG terms, respectively, obtained by
“LE”-SAMF. We can see that “LE”-SAMF failed to separate the person from
BG (the person is partly captured in the BG term). On the other hand, Figures
11.4(e) and 11.4(f) show the BG and the FG terms obtained by sSAMF based
on the segmented image shown in Figure 11.4(b). We can see that sSAMF
successfully separated the person from BG in this difﬁcult frame. A careful
look at the legs of the person reveals how segmentation helps separation—
the legs form a single segment in Figure 11.4(b), and the segmentwise sparse
1 The EC Funded CAVIAR project/IST 2001 37540, found at URL: http://homepages.inf.ed.ac
.uk/rbf/CAVIAR/.

290
11 Efﬁcient Solver for Sparse Additive Matrix Factorization
(a) Original
(b) Segmented
(c) BG (‘LE’-SAMF)
(d) FG (‘LE’-SAMF)
(e) BG (sSAMF)
(f) FG (sSAMF)
Figure 11.4 “LE”-SAMF vs. segmentation-based SAMF in FG/BG video
separation.
term (Figure 11.4(f)) captured all pixels on the legs, while the pixelwise sparse
term (Figure 11.4(d)) captured only a part of those pixels. We observed that, in
all frames of the difﬁcult video, as well as the easier one, sSAMF gave good
separation, while “LE”-SAMF failed in several frames.
For reference, we applied the convex formulation of robust PCA (Cand`es
et al., 2011), which solves the following minimization problem by the inexact
augmented Lagrange multiplier (ALM) algorithm (Lin et al., 2009):
min
UBG,UFG ∥UBG∥tr + λ∥UFG∥1
s.t.
V = UBG + UFG,
(11.24)

11.3 Experimental Results
291
(a) BG (ALM λ = 0.001)
(b) FG (ALM λ = 0.001)
(c) BG (ALM λ = 0.005)
(d) FG (ALM λ = 0.05)
(e) BG (ALM λ = 0.025)
(f) FG (ALM λ = 0.025)
Figure 11.5 FG/BG video separation by the convex formulation of robust PCA
(11.24) for λ = 0.001 (top row), λ = 0.005 (middle row), and λ = 0.025 (bottom
row).
where ∥· ∥tr and ∥· ∥1 denote the trace norm and the ℓ1-norm of a matrix,
respectively. Figure 11.5 shows the obtained BG and FG terms of the same
frame as that in Figure 11.4 with λ = 0.001, 0.005, 0.025. We see that the
performance strongly depends on the parameter value of λ, and that sSAMF
gives an almost identical result (bottom row in Figure 11.4) to the best ALM
result with λ = 0.005 (middle row in Figure 11.5) without any manual
parameter tuning.
In the following subsections, we give detailed information on the segmen-
tation algorithm and the computation time.

292
11 Efﬁcient Solver for Sparse Additive Matrix Factorization
(a) Original image
(b) Segmented (k = 1)
(c) Segmented (k = 10)
(d) Segmented (k = 100)
Figure 11.6 Segmented images by the efﬁcient graph-based segmentation (EGS)
algorithm with different k values. They are visually different, but with all these
segmentations, sSAMF gave almost identical FB/BG separations. The original
image (a) is the same frame as the one in Figure 11.4.
Segmentation Algorithm
For the EGS algorithm (Felzenszwalb and Huttenlocher, 2004), we used
the code publicly available from the authors’ homepage.2 EGS has three
tuning parameters: sigma, the smoothing parameter; k, the threshold param-
eter; and minc, the minimum segment size. Among them, k dominantly
determines the typical size of segments (larger k leads to larger segments).
To obtain oversegmented images for sSAMF in our experiment, we chose
k = 50, and the other parameters are set to sigma = 0.5 and minc = 20
as recommended by the authors. We also tested other parameter setting, and
observed that FG/BG separation by sSAMF performed almost equally for
1 ≤k ≤100, despite the visual variation of segmented images (see Figure
11.6). Overall, we empirically observed that the performance of sSAMF is
not very sensitive to the selection of segmented images, unless it is highly
undersegmented.
2 www.cs.brown.edu/∼pff/

11.3 Experimental Results
293
Computation Time
The computation time for segmentation by EGS was less than 10 sec (for
100 frames). Forming the one-to-one map X took more than 80 sec (which
is expected to be improved). In total, sSAMF took 600 sec on a Linux machine
with Xeon X5570 (2.93GHz), while “LE”-SAMF took 700 sec. This slight
reduction in computation time comes from the reduction in the number K of
partitions for the FG term, and hence the number of calculations of partial
analytic solutions.

12
MAP and Partially Bayesian Learning
Variational Bayesian (VB) learning generally offers a tractable approximation
of Bayesian learning, and efﬁcient iterative local search algorithms were
derived for many practical models based on the conditional conjugacy (see
Part II). However, in some applications, VB learning is still computationally
too costly. In such cases, cruder approximation methods, where all or some
of the parameters are point-estimated, with potentially less computation cost,
are attractive alternatives. For example, Chu and Ghahramani (2009) applied
partially Bayesian (PB) learning (introduced in Section 2.2.2), where the core
tensor is integrated out and the factor matrices are point-estimated, to Tucker
factorization (TF) (Carroll and Chang, 1970; Harshman, 1970; Tucker, 1996;
Kolda and Bader, 2009). Mørup and Hansen (2009) applied the maximum
a posteriori (MAP) learning to TF with the empirical Bayesian procedure,
i.e., the hyperparameters are also estimated from observations. Their proposed
empirical MAP learning, which only requires the same order of computation
costs as the plain alternating least squares algorithm (Kolda and Bader,
2009), showed its model selection capability through the automatic relevance
determination (ARD) property.
Motivated by the empirical success, we have analyzed PB learning and
MAP learning and their empirical Bayesian variants (Nakajima et al., 2011;
Nakajima and Sugiyama, 2014), which this chapter introduces. Focusing on
fully observed matrix factorization (MF), we ﬁrst analyze the global and local
solutions of MAP learning and PB learning and their empirical Bayesian
variants. This analysis theoretically reveals similarities and dissimilarities to
VB learning. After that, we discuss more general cases, including MF with
missing entries and TF.
294

12.1 Theoretical Analysis in Fully Observed MF
295
12.1 Theoretical Analysis in Fully Observed MF
In this section, we formulate MAP learning and PB learning in the free energy
minimization framework (Section 2.1.1) and derive analytic-form solutions.
12.1.1 Problem Description
The model likelihood and the prior of the MF model are given by
p(V|A, B) ∝exp

−1
2σ2
###V −BA⊤###2
Fro

,
(12.1)
p(A) ∝exp

−1
2tr

AC−1
A A⊤ 
,
(12.2)
p(B) ∝exp

−1
2tr

BC−1
B B⊤ 
,
(12.3)
where the prior covariance matrices are restricted to be diagonal:
CA = Diag(c2
a1,. . . , c2
aH),
CB = Diag(c2
b1,. . . , c2
bH),
for cah, cbh > 0, h = 1,. . . , H. Without loss of generality, we assume that the
diagonal entries of the product CACB are arranged in nonincreasing order, i.e.,
cahcbh ≥cah′ cbh′ for any pair h < h′.
As in Section 2.2.2, we treat MAP learning and PB learning as special
cases of VB learning in the free energy minimization framework. The Bayes
posterior is given by
p(A, B|V) = p(V|A, B)p(A)p(B)
p(V)
,
(12.4)
which is intractable for the MF model (12.1) through (12.3). Accordingly, we
approximate it by
r = argmin
r
F(r)
s.t.
r(A, B) ∈G,
(12.5)
where G speciﬁes the constraint on the approximate posterior. F(r) is the free
energy, deﬁned as
F(r) =
/
log
r(A, B)
p(V|A, B)p(A)p(B)
0
r(A,B)
(12.6)
=
/
log
r(A, B)
p(A, B|V)
0
r(A,B)
−log p(V),

296
12 MAP and Partially Bayesian Learning
which is a monotonic function of the KL divergence

log
r(A,B)
p(A,B|V)

r(A,B) to the
Bayes posterior.
Constraints for MAP Learning and PB Learning
MAP learning ﬁnds the mode of the posterior distribution, which amounts to
approximating the posterior with the Dirac delta function. Accordingly, solving
the problem (12.5) with the following constraint gives the MAP solution:
rMAP(A, B) = δ(A; A)δ(B; B),
(12.7)
where δ(μ;μ) denotes the (pseudo-)Dirac delta function located at μ.1
Under the MAP constraint (12.7), the free energy (12.6) is written as
FMAP(A, B) =
/
log
δ(A; A)δ(B; B)
p(V|A, B)p(A)p(B)
0
δ(A;A)δ(B;B)
= −log p(V|A, B)p(A)p(B) + χA + χB,
(12.8)
where
χA =

log δ(A; A)

δ(A;A) ,
χB =

log δ(B; B)

δ(B;B)
(12.9)
are the negative entropies of the pseudo-Dirac delta functions.
PB learning is a strategy to analytically integrate out as many parameters as
possible, and the rest are point-estimated. In the MF model, a natural choice
is to integrate A out and point-estimate B, which we call PB-A learning, or
to integrate B out and point-estimate A, which we call PB-B learning. Their
solutions can be obtained by solving the problem (12.5) with the following
constraints, respectively:
rPB−A(A, B) = rPB
A (A)δ(B; B),
(12.10)
rPB−B(A, B) = δ(A; A)rPB
B (B).
(12.11)
Under the PB-A constraint (12.10), the free energy (12.6) is written as
FPB−A(rPB
A , B) =
/
log
rPB
A (A)
p(V|A, B)p(A)p(B)
0
rPB
A (A)
+ χB
=
/
log
rPB
A (A)
p(A|V, B)
0
rPB
A (A)
−log p(V|B)p(B) + χB,
(12.12)
1 By the pseudo-Dirac delta function, we mean an extremely localized density function, e.g.,
δ(A; A) ∝exp

−∥A −A∥2
Fro/(2ε2)
 
with a very small but strictly positive variance ε2 > 0, such
that its tail effect can be ignored, while its negative entropy χA = ⟨log δ(A; A)⟩δ(A;A) remains
ﬁnite.

12.1 Theoretical Analysis in Fully Observed MF
297
where
p(A|V, B) = p(V|A, B)p(A)
p(V|B)
,
(12.13)
and
p(V|B) =

p(V|A, B)

p(A)
(12.14)
are the posterior distribution with respect to A (given B) and the marginal
distribution, respectively. Note that Eq. (12.12) is a functional of rPB
A
and B,
and χB is a constant with respect to them.
Since only the ﬁrst term depends on rPB
A , on which no restriction is imposed,
Eq. (12.12) is minimized when
rPB
A (A) = p(A|V, B)
(12.15)
for any B. With Eq. (12.15), the ﬁrst term in Eq. (12.12) vanishes, and thus the
estimator for B is given by
B
PB−A = argmin
B
´FPB−A(B),
(12.16)
where
´FPB−A(B) ≡min
rPB
A
FPB−A(rPB
A , B) = −log p(V|B)p(B) + χB.
(12.17)
The process to compute ´FPB−A(B) in Eq. (12.17) corresponds to integrating
A out based on the conditional conjugacy. The probabilistic PCA, introduced
in Section 3.1.2, was originally proposed with PB-A learning (Tipping and
Bishop, 1999).
In the same way, we can obtain the approximate posterior under the PB-B
constraint (12.11) as follows:
rPB
B (B) = p(B|V, A),
(12.18)
A
PB−B = argmin
A
´FPB−B(A),
(12.19)
where
p(B|V, A) = p(V|A, B)p(B)
p(V|A)
,
(12.20)
p(V|A) =

p(V|A, B)

p(B) ,
(12.21)
´FPB−B(A) ≡min
rPB
B
FPB−B(rPB
B , A) = −log p(V|A)p(A) + χA.
(12.22)

298
12 MAP and Partially Bayesian Learning
We deﬁne PB learning as one of PB-A learning and PB-B learning giving a
lower free energy. Namely,
rPB(A, B) =
⎧⎪⎪⎨⎪⎪⎩
rPB−A(A, B)
if min FPB−A(rPB
A , B) ≤min FPB−B(rPB
B , A),
rPB−B(A, B)
otherwise.
Free Energies for MAP Learning and PB Learning
Apparently, the constraint (12.7) for MAP learning and the constraints (12.10)
and (12.11) for PB learning forces independence between A and B, and
therefore, they are stronger than the independence constraint
rVB(A, B) = rVB
A (A)rVB
B (B)
(12.23)
for VB learning. In Chapter 6, we showed that the VB posterior under the
independence constraint (12.23) is in the following Gaussian form:
rA(A) = MGaussM,H(A; A, IM ⊗ΣA) ∝exp
⎛⎜⎜⎜⎜⎜⎝−
tr
!
(A−A)Σ
−1
A (A−A)⊤
"
2
⎞⎟⎟⎟⎟⎟⎠,
(12.24)
rB(B) = MGaussL,H(B; B, IL ⊗ΣB) ∝exp
⎛⎜⎜⎜⎜⎜⎝−
tr
!
(B−B)Σ
−1
B (B−B)⊤
"
2
⎞⎟⎟⎟⎟⎟⎠,
(12.25)
where the posterior covariances, ΣA and ΣB, are diagonal. Furthermore, Eqs.
(12.24) and (12.25) can be the pseudo-Dirac delta functions by setting the
posterior covariances to ΣA = ε2IH and ΣB = ε2IH, respectively, for a very
small ε2 > 0.
Consequently, the MAP and the PB solutions can be obtained by mini-
mizing the free energy for VB learning with posterior covariances clipped to
ε2IH, according to the corresponding constraint. Namely, we start from the
free energy expression (6.42) for VB learning, i.e.,
2F = LM log(2πσ2) +
L
h=1 γ2
h
σ2
+
H

h=1
2Fh,
(12.26)
where
2Fh = M log
c2
ah
σ2ah
+ L log
c2
bh
σ2
bh
+
a2
h + Mσ2
ah
c2ah
+
b2
h + Lσ2
bh
c2
bh
−(L + M) +
−2ahbhγh +

a2
h + Mσ2
ah
 b2
h + Lσ2
bh
 
σ2
,
(12.27)
and set
σ2
ah = ε2
h = 1,. . . , H,
(12.28)

12.1 Theoretical Analysis in Fully Observed MF
299
for MAP learning and PB-B learning, and
σ2
bh = ε2
h = 1,. . . , H,
(12.29)
for MAP learning and PB-A learning. Here,
A = $a1ωa1,. . . ,aHωaH
% ,
B =
b1ωb1,. . . ,bHωbH
 
,
ΣA = Diag

σ2
a1,. . . , σ2
aH
 
,
ΣB = Diag

σ2
b1,. . . , σ2
bH
 
,
and
V =
L

h=1
γhωbhω⊤
ah
is the singular value decomposition (SVD) of V.
Thus, the free energies for MAP learning, PB-A learning, and PB-B
learning are given by Eq. (12.26) for
2FMAP
h
= M log c2
ah + L log c2
bh +
a2
h
c2ah +
b2
h
c2
bh
+
−2ahbhγh+a2
hb2
h
σ2
−(L + M) + (L + M)χ,
(12.30)
2FPB−A
h
= M log
c2
ah
σ2ah + L log c2
bh +
a2
h+Mσ2
ah
c2ah
+
b2
h
c2
bh
+
−2ahbhγh+

a2
h+Mσ2
ah
 b2
h
σ2
−(L + M) + Lχ,
(12.31)
2FPB−B
h
= M log c2
ah + L log
c2
bh
σ2
bh
+
a2
h
c2ah +
b2
h+Lσ2
bh
c2
bh
+
−2ahbhγh+a2
h
!
b2
h+Lσ2
bh
"
σ2
−(L + M) + Mχ,
(12.32)
respectively, where
χ = −log ε2
(12.33)
is a large positive constant corresponding to the negative entropy of the one-
dimensional pseudo-Dirac delta function.
As in VB learning, the free energy (12.26) is separable for each singular
component as long as the noise variance σ2 is treated as a constant. Therefore,
the variational parameters (ah,bh, σ2
ah, σ2
bh) and the prior covariances (c2
ah, c2
bh)
for the hth component can be estimated by minimizing Fh.

300
12 MAP and Partially Bayesian Learning
12.1.2 Global Solutions
In this section, we derive the global minimizers of the free energies, (12.30)
through (12.32), and analyze their behavior.
Global MAP and PB Solutions
By minimizing the MAP free energy (12.30), we can obtain the global solution
for MAP learning, given the hyperparameters CA, CB, σ2 treated as ﬁxed
constants. Let
U = BA⊤
be the target low-rank matrix.
Theorem 12.1
Given CA, CB ∈DH
++, and σ2 ∈R++, the MAP solution of the
MF model (12.1) through (12.3) is given by
U
MAP =
H

h=1
γMAP
h
ωbhω⊤
ah,
where
γMAP
h
=
⎧⎪⎪⎨⎪⎪⎩
˘γMAP
h
if γh ≥γMAP
h
,
0
otherwise,
(12.34)
where
γMAP
h
=
σ2
cahcbh
,
(12.35)
˘γMAP
h
= γh −
σ2
cahcbh
.
(12.36)
Proof
Eq. (12.30) can be written as a function of ah andbh as
2FMAP
h
= a2
h
c2ah
+
b2
h
c2
bh
+ −2ahbhγh +a2
hb2
h
σ2
+ const.
=
⎛⎜⎜⎜⎜⎝
ah
cah
−
bh
cbh
⎞⎟⎟⎟⎟⎠
2
+
!
ahbh −
!
γh −
σ2
cahcbh
""2
σ2
+ const.
(12.37)
Noting that the ﬁrst two terms are nonnegative, we ﬁnd that, if γh > γMAP
h
,
Eq. (12.37) is minimized when
˘γMAP
h
≡ahbh = γh −
σ2
cahcbh
,
(12.38)
δMAP
h
≡ah
bh
= cah
cbh
.
(12.39)

12.1 Theoretical Analysis in Fully Observed MF
301
Otherwise, it is minimized when
ah = 0,
bh = 0,
which completes the proof.
□
Eqs. (12.38) and (12.39) immediately lead to the following corollary:
Corollary 12.2
The MAP posterior is given by
rMAP(A, B) =
H

h=1
δ(ah;ahωah)
H

h=1
δ(bh;bhωbh),
(12.40)
with the following estimators: if γh > γMAP
h
,
ah = ±
.
˘γMAP
h
δMAP
h
,
bh = ±
>
@
˘γMAP
h
δMAP
h
,
(12.41)
where
δMAP
h

≡ah
bh

= cah
cbh
,
(12.42)
and otherwise
ah = 0,
bh = 0.
(12.43)
Similarly, by minimizing the PB-A free energy (12.31) and the PB-B free
energy (12.32) and comparing them, we can obtain the global solution for PB
learning:
Theorem 12.3
Given CA, CB ∈DH
++, and σ2 ∈R++, the PB solution of the
MF model (12.1) through (12.3) is given by
U
PB =
H

h=1
γPB
h ωbhω⊤
ah,
where
γPB
h
=
⎧⎪⎪⎨⎪⎪⎩
˘γPB
h
if γh ≥γPB
h ,
0
otherwise,
(12.44)
where
γPB
h
= σ
A
max(L, M) +
σ2
c2ahc2
bh
,
(12.45)
˘γPB
h
=
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
1 −
σ2

max(L, M) +
B
max(L, M)2 + 4
γ2
h
c2ahc2
bh

2γ2
h
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
γh.
(12.46)

302
12 MAP and Partially Bayesian Learning
Corollary 12.4
The PB posterior is given by
rPB(A, B) =
⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩
rPB−A(A, B)
if L < M,
rPB−A(A, B) or rPB−B(A, B)
if L = M,
rPB−B(A, B)
if L > M,
(12.47)
where rPB−A(A, B) and rPB−B(A, B) are the PB-A posterior and the PB-B
posterior, respectively, given as follows. The PB-A posterior is given by
rPB−A(A, B) =
H

h=1
GaussM(ah;ahωah, σ2
ah IM)
H

h=1
δ(bh;bhωbh),
(12.48)
with the following estimators: if
γh > γPB−A
h
≡σ
A
M +
σ2
c2ahc2
bh
,
(12.49)
then
ah = ±
.
˘γPB−A
h
δPB−A
h
,
bh = ±
>
@
˘γPB−A
h
δPB−A
h
,
σ2
ah =
σ2
˘γPB−A
h
/δPB−A
h
+ σ2/c2ah
,
(12.50)
where
˘γPB−A
h

≡ahbh
 
=
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
1 −
σ2
⎛⎜⎜⎜⎜⎜⎜⎝M+
A
M2+4
γ2
h
c2ah c2
bh
⎞⎟⎟⎟⎟⎟⎟⎠
2γ2
h
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
γh,
(12.51)
δPB−A
h

≡ah
bh

=
c2
ah
⎛⎜⎜⎜⎜⎜⎜⎝M+
A
M2+4
γ2
h
c2ah c2
bh
⎞⎟⎟⎟⎟⎟⎟⎠
2γh
,
(12.52)
and otherwise
ah = 0,
bh = 0,
σ2
ah = c2
ah.
(12.53)
The PB-B posterior is given by
rPB−B(A, B) =
H

h=1
δ(ah;ahωah)
H

h=1
GaussL(bh;bhωbh, σ2
bh IL),
(12.54)
with the following estimators: if
γh > γPB−B
h
≡σ
A
L +
σ2
c2ahc2
bh
,
(12.55)

12.1 Theoretical Analysis in Fully Observed MF
303
then
ah = ±
.
˘γPB−B
h
δPB−B
h
,
bh = ±
>
@
˘γPB−B
h
δPB−B
h
,
σ2
bh =
σ2
˘γPB−B
h
δPB−B
h
+ σ2/c2
bh
,
(12.56)
where
˘γPB−B
h

≡ahbh
 
=
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
1 −
σ2
⎛⎜⎜⎜⎜⎜⎜⎝L+
A
L2+4
γ2
h
c2ah c2
bh
⎞⎟⎟⎟⎟⎟⎟⎠
2γ2
h
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
γh,
(12.57)
δPB−B
h

≡ah
bh

=
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
c2
bh
⎛⎜⎜⎜⎜⎜⎜⎝L+
A
L2+4
γ2
h
c2ah c2
bh
⎞⎟⎟⎟⎟⎟⎟⎠
2γh
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
−1
,
(12.58)
and otherwise
ah = 0,
bh = 0,
σ2
bh = c2
bh.
(12.59)
Note that, when L = M, the choice from the PB-A and the PB-B posteriors
depends on the prior covariances CA and CB. However, as long as the estimator
for the target low-rank matrix U is concerned, the choice does not matter, as
Theorem 12.3 states. This is because
γPB−A
h
= γPB−B
h
and
˘γPB−A
h
= ˘γPB−B
h
when
L = M,
as Corollary 12.4 implies.
Proofs of Theorem 12.3 and Corollary 12.4
We ﬁrst derive the PB-A solution by minimizing the corresponding free energy
(12.31):
2FPB−A
h
= M log
c2
ah
σ2ah + L log c2
bh +
a2
h+Mσ2
ah
c2ah
+
b2
h
c2
bh
+
−2ahbhγh+

a2
h+Mσ2
ah
 b2
h
σ2
−(L + M) + Lχ.
(12.60)
As a function ofah (treatingbh and σ2
ah as ﬁxed constants), Eq. (12.60) can
be written as
2FPB−A
h
(ah) = a2
h
c2ah
+ −2ahbhγh +a2
hb2
h
σ2
+ const.

304
12 MAP and Partially Bayesian Learning
=
b2
h + σ2/c2
ah
σ2
⎛⎜⎜⎜⎜⎜⎝a2
h −2
bhγh
b2
h + σ2/c2ah
ah
⎞⎟⎟⎟⎟⎟⎠+ const.
=
b2
h + σ2/c2
ah
σ2
⎛⎜⎜⎜⎜⎜⎝ah −
bhγh
b2
h + σ2/c2ah
⎞⎟⎟⎟⎟⎟⎠
2
+ const.,
which is minimized when
ah =
bhγh
b2
h + σ2/c2ah
.
(12.61)
As a function of σ2
ah (treatingah andbh as ﬁxed constants), Eq. (12.60) can be
written as
2FPB−A
h
(σ2
ah) = M
!
−log σ2
ah +
!
1
c2ah +
b2
h
σ2
"
σ2
ah
"
+ const.
= M
!
−log
!
1
c2ah +
b2
h
σ2
"
σ2
ah +
!
1
c2ah +
b2
h
σ2
"
σ2
ah
"
+ const.,
which is minimized when
σ2
ah =
⎛⎜⎜⎜⎜⎜⎝
1
c2ah
+
b2
h
σ2
⎞⎟⎟⎟⎟⎟⎠
−1
=
σ2
b2
h + σ2/c2ah
.
(12.62)
Therefore, substituting Eqs. (12.61) and (12.62) into Eq. (12.60) gives the free
energy withah and σ2
ah already optimized:
2 ´FPB−A
h
= min
ah,σ2ah
2FPB−A
h
= −M log σ2
ah +
a2
h+Mσ2
ah
σ2ah
+
b2
h
c2
bh
−2ahbhγh
σ2
+ const.
= M log(b2
h + σ2/c2
ah) −
b2
hγ2
h
σ2(b2
h+σ2/c2ah) +
b2
h
c2
bh
+ const.
= M log(b2
h + σ2/c2
ah) +
!
γ2
h
σ2 −
b2
hγ2
h
σ2(b2
h+σ2/c2ah)
"
+

b2
h
c2
bh
+
σ2
c2ahc2
bh

+ const.
= M log(b2
h + σ2/c2
ah) +
γ2
h
c2ah (b2
h + σ2/c2
ah)−1 +
1
c2
bh
(b2
h + σ2/c2
ah) + const.
(12.63)
In the second last equation, we added some constants so that the minimizer can
be found by the following lemma:
Lemma 12.5
The function
f(x) = ξlog log x + ξ−1x−1 + ξ1x

12.1 Theoretical Analysis in Fully Observed MF
305
of x > 0 for positive coefﬁcients ξlog, ξ−1, ξ1 > 0 is strictly quasiconvex,2 and
minimized at
x =
−ξlog +
.
ξ2
log + 4ξ1ξ−1
2ξ1
.
Proof
f(x) is differentiable in x > 0, and its ﬁrst derivative is
∂f
∂x = ξlogx−1 −ξ−1x−2 + ξ1
= x−2 
ξ1x2 + ξlogx −ξ−1
 
= x−2
⎛⎜⎜⎜⎜⎝x +
ξlog+
.
ξ2
log+4ξ1ξ−1
2ξ1
⎞⎟⎟⎟⎟⎠
⎛⎜⎜⎜⎜⎝x −
−ξlog+
.
ξ2
log+4ξ1ξ−1
2ξ1
⎞⎟⎟⎟⎟⎠.
Since the ﬁrst two factors are positive, we ﬁnd that f(x) is strictly decreasing
for 0 < x < x, and strictly increasing for x > x, which proves the lemma.
□
By applying Lemma 12.5 to Eq. (12.63) with x = b2
h + σ2/c2
ah, we ﬁnd that
´FPB−A
h
is strictly quasiconvex and minimized when
b2
h + σ2/c2
ah =
c2
bh

−M +
B
M2 +
4γ2
h
c2ahc2
bh

2
.
(12.64)
Sinceb2
h is nonnegative, the minimizer of the free energy (12.63) is given by
b2
h = max
⎧⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎩
0,
c2
bh

−

M +
2σ2
c2ahc2
bh

+
B
M2 +
4γ2
h
c2ahc2
bh

2
⎫⎪⎪⎪⎪⎪⎪⎪⎬⎪⎪⎪⎪⎪⎪⎪⎭
.
(12.65)
Apparently, Eq. (12.65) is positive when
⎛⎜⎜⎜⎜⎜⎝M + 2σ2
c2ahc2
bh
⎞⎟⎟⎟⎟⎟⎠
2
< M2 + 4γ2
h
c2ahc2
bh
,
which leads to the thresholding condition:
γh > γPB−A
h
.
By using Eqs. (12.61), (12.62), (12.65), and
b2
h + σ2/c2
ah
 −1 =
⎧⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎩
c2
ah
⎛⎜⎜⎜⎜⎜⎜⎝M+
A
M2+
4γ2
h
c2ah c2
bh
⎞⎟⎟⎟⎟⎟⎟⎠
2γ2
h
if γh > γPB−A
h
,
c2
ah
σ2
otherwise,
(12.66)
2 The deﬁnition of quasiconvexity is given in footnote 1 in Section 8.1.

306
12 MAP and Partially Bayesian Learning
derived from Eqs. (12.64) and (12.65), we obtain
γPB−A
h
≡ahbh =
b2
hγh
b2
h + σ2/c2ah
=
⎛⎜⎜⎜⎜⎜⎝1 −
σ2/c2
ah
b2
h + σ2/c2ah
⎞⎟⎟⎟⎟⎟⎠γh
=
⎧⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎩
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
1 −
σ2
⎛⎜⎜⎜⎜⎜⎜⎝M+
A
M2+4
γ2
h
c2ah c2
bh
⎞⎟⎟⎟⎟⎟⎟⎠
2γ2
h
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
γh
if γh > γPB−A
h
,
0
otherwise,
δPB−A
h
≡ah
bh
=
γh
b2
h + σ2/c2ah
=
c2
ah
⎛⎜⎜⎜⎜⎜⎜⎝M+
A
M2+4
γ2
h
c2ah c2
bh
⎞⎟⎟⎟⎟⎟⎟⎠
2γh
for γh > γPB−A
h
,
σ2
ah =
σ2
b2
h + σ2/c2ah
=
⎧⎪⎪⎪⎨⎪⎪⎪⎩
σ2
γPB−A
h
/δPB−A
h
+σ2/c2ah
if γh > γPB−A
h
,
c2
ah
otherwise.
Thus, we have obtained the PB-A posterior (12.48) speciﬁed by Eqs. (12.49)
through (12.53).
In exactly the same way, we can obtain the PB-B posterior (12.54) speciﬁed
by Eqs. (12.55) through (12.59), by minimizing the free energy (12.32) for
PB-B learning.
Finally, for the choice from the PB-A posterior and the PB-B posterior, we
can easily prove the following lemma:
Lemma 12.6
It holds that
min
ah,bh,σ2ah
FPB−A
h
<
min
ah,bh,σ2
bh
FPB−B
h
if
L < M,
min
ah,bh,σ2ah
FPB−A
h
>
min
ah,bh,σ2
bh
FPB−B
h
if
L > M.
Proof
When comparing the PB-A free energy (12.31) and the PB-B free
energy (12.32), the last terms are dominant since we assume that the negative
entropy χ, deﬁned by Eq. (12.33), of the one-dimensional pseudo-Dirac delta
function is ﬁnite but arbitrarily large. Then, comparing the last terms, each

12.1 Theoretical Analysis in Fully Observed MF
307
of which is proportional to the number of parameters point-estimated, of
Eqs. (12.31) and (12.32) proves Lemma 12.6.
□
Combining Lemma 12.6 with the PB-A posterior and the PB-B posterior
obtained before, we have Corollary 12.4. Theorem 12.3 is a direct consequence
of Corollary 12.4.
□
Comparison between MAP, PB, and VB Solutions
Here we compare the MAP solution (Theorem 12.1), the PB solution (Theorem
12.3), and the VB solution (Theorem 6.7 in Chapter 6). For all methods, the
solution is a shrinkage estimator applied to each singular value, i.e., in the
following form for γh ≤γh:
U =
H

h=1
γhωbhω⊤
ah,
where
γh =
⎧⎪⎪⎨⎪⎪⎩
γh
if γh ≥γh,
0
otherwise.
(12.67)
When the prior is ﬂat, i.e., cahcbh →∞, the truncation threshold γh and the
shrinkage factor ˘γh are simpliﬁed as
lim
cahcbh→∞γMAP
h
= 0,
lim
cahcbh→∞˘γMAP
h
= γh,
(12.68)
lim
cahcbh→∞γPB
h
= σ √max(L, M),
lim
cahcbh→∞˘γPB
h
=

1 −max(L,M)σ2
γh
 
γh,
(12.69)
lim
cahcbh→∞γVB
h
= σ √max(L, M),
lim
cahcbh→∞˘γVB
h
=

1 −max(L,M)σ2
γh
 
γh,
(12.70)
and therefore the estimators γh can be written as
γMAP
h
= γh,
(12.71)
γPB
h
= max

0,

1 −max(L,M)σ2
γh
 
γh
 
,
(12.72)
γVB
h
= max

0,

1 −max(L,M)σ2
γh
 
γh
 
.
(12.73)
As expected, the MAP estimator (12.71) coincides with the maximum
likelihood (ML) estimator when the prior is ﬂat. On the other hand, the PB
estimator (12.72) and the VB estimator (12.73) do not converge to the ML
estimator. Interestingly, the PB estimator and the VB estimator coincide with
each other, and they are in the form of the positive-part James–Stein (PJS)
estimator (James and Stein, 1961; Efron and Morris, 1973) applied to each
singular component (see Appendix A for a short introduction to the James–
Stein estimator). The reason why the VB estimator is shrunken even with the
ﬂat prior was explained in Chapter 7 in terms of model-induced regularization

308
12 MAP and Partially Bayesian Learning
0
0.5
1
1.5
2
0
5
10
15
(a) L = 20, M = 50
0
0.5
1
1.5
2
0
5
10
15
(b) L = 20, M = 10
Figure 12.1 Truncation thresholds, γVB
h , γPB
h , γPB−A
h
, and γMAP
h
as functions of the
product cahcbh of the prior covariances. The noise variance is set to σ2 = 1.
0
10
20
30
40
0
2
4
6
8
10
(a) L = 20, cahcbh →∞
0
10
20
30
40
0
2
4
6
8
10
(b) L = 20, cahcbh = 1
Figure 12.2 Truncation thresholds as functions of M.
(MIR) enhanced by phase transitions. Eq. (12.72) implies that PB learning—
a cruder approximation to Bayesian learning—shares the same property as
VB learning.
To investigate the dimensionality selection behavior, we depict the trunca-
tion thresholds of VB learning, PB learning, PB-A learning, and MAP learning
as functions of the product cahcbh of the prior covariances in Figure 12.1.
The left panel is for the case with L = 20, M = 50, and the right panel
is for the case with L = 20, M = 10. PB-A learning corresponds to PB
learning with the predetermined marginalized and the point-estimated spaces
as in Tipping and Bishop (1999) and Chu and Ghahramani (2009), i.e., the
matrix A is always marginalized out and B is point-estimated regardless of
the dimensionality. We see in Figure 12.1 that PB learning and VB learning
show similar dimensionality selection behaviors, while PB-A learning behaves
differently when L > M.
Figure 12.2 shows the truncation thresholds as functions of M for L = 20.
With the ﬂat prior cahcbh →∞(left panel), the PB and the VB solutions agree

12.1 Theoretical Analysis in Fully Observed MF
309
with each other, as Eqs. (12.69) and (12.70) imply. The PB-A solution is also
identical to them when M ≥L. However, its behavior changes at M = L: the
truncation threshold of PB-A learning smoothly goes down as M decreases,
while those of PB learning and VB learning make a sudden turn and become
constant. The right panel is for the case with a nonﬂat prior (cahcbh = 1), which
shows similar tendency to the case with the ﬂat prior.
A question is which behavior is more desirable, a sudden turn in the
threshold curve in VB/PB learning, or the smooth behavior in PB-A learning?
We argue that the behavior of VB/PB learning is more desirable for the
following reason. Let us consider the case where no true signal exists, i.e.,
the true rank is H∗= 0. In this case, we merely observe pure noise, V = E, and
the average of the squared singular values of V over all components is given by

tr(EE⊤)

MGaussL,M(E;0L,M,σ2IL⊗IM)
min(L, M)
= σ2 max(L, M).
(12.74)
Comparing Eq. (12.74) with Eqs. (12.70) and (12.69), we ﬁnd that VB learning
and PB learning always discard the components with singular values no greater
than the average noise contribution (note here that Eqs. (12.70) and (12.69)
give the thresholds for the ﬂat prior cahcbh →∞, and the thresholds increase
as cahcbh decreases). The sudden turn in the threshold curve actually follows
the behavior of the average noise contribution (12.74) to the singular values.
On the other hand, PB-A learning does not necessarily discard such noise-
dominant components, and can strongly overﬁt the noise when L ≫M.
Figure 12.3 shows the estimators γh by VB learning, PB learning, PB-A
learning, and MAP learning for cahcbh = 1, as functions of the observed
0
5
10
15
20
0
5
10
15
20
(a) L = 20, M = 50
0
2
4
6
8
0
2
4
6
8
(b) L = 20, M = 10
Figure 12.3 Behavior of VB, PB, PB-A, and MAP estimators (the vertical axis)
for cahcbh = 1, when the singular value γh (the horizontal axis) is observed. The
noise variance is set to σ2 = 1.

310
12 MAP and Partially Bayesian Learning
singular value γh. We can see that the PB estimator behaves similarly to the VB
estimator, while the MAP estimator behaves signiﬁcantly differently. The right
panel shows that PB-A learning also behaves differently from VB learning
when L > M, which implies that the choice between the PB-A posterior and the
PB-B posterior based on the free energy is essential to accurately approximate
VB learning.
Actually, the coincidence between the VB solution (12.70) and the PB
solution (12.69) with the ﬂat prior can be seen as a natural consequence from
the similarity in the posterior shape. From Theorem 6.7 and Corollary 6.8, we
can derive the following corollary:
Corollary 12.7
Assume that, when we make the prior ﬂat cahcbh →∞, cah and
cbh go to inﬁnity in the same order, i.e., cah/cbh = Θ(1).3 Then, the following
hold for the variances of the VB posterior (6.40): when L < M,
lim
cahcbh→∞σ2
ah = ∞,
lim
cahcbh→∞σ2
bh = 0,
(12.75)
and when L > M,
lim
cahcbh→∞σ2
ah = 0,
lim
cahcbh→∞σ2
bh = ∞.
(12.76)
Proof
Assume ﬁrst that L
<
M. When γh
>
γVB
h , Eq. (6.52) gives
limcahcbh→∞δVB
h
= ∞, and therefore Eq. (6.51) gives Eq. (12.75). When
γh ≤γVB
h , Eq. (6.54) gives ζVB
h
= σ2/M −Θ(c−2
ah c−2
bh ) as cahcbh →∞, and
therefore Eq. (6.53) gives Eq. (12.75).
When L > M, Theorem 6.7 and Corollary 6.8 hold for V ←V⊤, meaning
that the VB posterior is obtained by exchanging the variational parameters for
A and those for B. Thus, we obtain Eq. (12.76), and complete the proof.
□
Corollary 12.7 implies that, with the ﬂat prior cahcbh →∞, the shape of
the VB posterior is similar to the shape of the PB posterior: they extend in the
space of A when M > L, and extend in the space of B when M < L. Therefore,
it is no wonder that the solutions coincide with each other.
Global Empirical MAP and Empirical PB Solutions
Next, we investigate the empirical Bayesian variants of MAP learning and
PB learning, where the hyperparameters CA and CB are also estimated from
observations. Note that the noise variance σ2 is still considered as a ﬁxed
constant (noise variance estimation will be discussed in Section 12.1.6).
3 Θ( f(x)) is a positive function such that lim supx→∞|Θ( f(x))/ f(x)| < ∞and
lim infx→∞|Θ( f(x))/ f(x)| > 0.

12.1 Theoretical Analysis in Fully Observed MF
311
Let us ﬁrst consider the MAP free energy (12.30):
2FMAP
h
= M log c2
ah + L log c2
bh +
a2
h
c2ah +
b2
h
c2
bh
+
−2ahbhγh+a2
hb2
h
σ2
−(L + M) + (L + M)χ.
We can make the MAP free energy arbitrarily small, i.e., FMAP
h
→−∞by
setting c2
ah, c2
bh →+0 with the variational parameters set to the corresponding
solution, i.e., ah = bh = 0 (see Corollary 12.2). Therefore, the global solution
of empirical MAP (EMAP) learning is given by
ah = 0,
bh = 0,
c2
ah →+0,
c2
bh →+0,
for h = 1,. . . , H,
which results in the following theorem:
Theorem 12.8
The global solution of EMAP learning is γEMAP
h

≡ahbh
 
= 0
for all h = 1,. . . , H, regardless of observations.
The same happens in empirical PB (EPB) learning. The PB-A free energy
(12.31),
2FPB−A
h
= M log
c2
ah
σ2ah + L log c2
bh +
a2
h+Mσ2
ah
c2ah
+
b2
h
c2
bh
+
−2ahbhγh+

a2
h+Mσ2
ah
 b2
h
σ2
−(L + M) + Lχ,
can be arbitrarily small, i.e., FPB−A
h
→−∞by setting c2
ah, c2
bh →+0 with the
variational parameters set to the corresponding solutionah = bh = 0, σ2
ah = c2
ah
(see Corollary 12.4). Also, the PB-B free energy (12.32),
2FPB−B
h
= M log c2
ah + L log
c2
bh
σ2
bh
+
a2
h
c2ah +
b2
h+Lσ2
bh
c2
bh
+
−2ahbhγh+a2
h
!
b2
h+Lσ2
bh
"
σ2
−(L + M) + Mχ,
can be arbitrarily small, i.e., FPB−B
h
→−∞by setting c2
ah, c2
bh →+0 with the
variational parameters set to the corresponding solutionah = bh = 0, σ2
bh = c2
bh.
Thus, we have the following theorem:
Theorem 12.9
The global solution of EPB learning is γEPB
h

≡ahbh
 
= 0 for
all h = 1,. . . , H, regardless of observations.
Theorems 12.8 and 12.9 imply that empirical Bayesian variants of MAP
learning and PB learning give useless trivial estimators. This happens because
the posterior variances of the parameters to be point-estimated are ﬁxed to a
small value, so that the posteriors form the pseudo-Dirac delta functions. In VB
learning, if we set cahcbh to a small value, the posterior variances, σ2
ah and σ2
bh,

312
12 MAP and Partially Bayesian Learning
get small accordingly, so that the third and the fourth terms in Eq. (12.27) do
not diverge to +∞. As a result, the ﬁrst and the second terms in Eq. (12.27)
remain ﬁnite. On the other hand, in MAP learning and PB learning, at least
one of the posterior variances, σ2
ah and σ2
bh, is treated as a constant and cannot
be adjusted to the corresponding prior covariance when it is set to be small.
This makes the free energy lower-unbounded. Actually, if we lower-bound the
prior covariances as c2
ah, c2
bh ≥ε2 with the same ε2 as the one we used for
deﬁning the variances (12.28) and (12.29) of the pseudo-Dirac delta functions
and their entropy (12.33), the MAP and the PB free energies, FMAP
h
, FPB−A
h
,
and FPB−B
h
, are also lower-bounded by zero, as the VB free energy, FVB
h .
12.1.3 Local Solutions
The analysis in Section 12.1.2 might seem contradictory with the reported
results in Mørup and Hansen (2009), where EMAP showed good performance
with the ARD property in TF—since the free energies in MF and TF are similar
to each other, they should share the same issue of the lower-unboundedness.
In the following, we elucidate that this apparent contradiction is because of the
local solutions in EMAP learning and EPB learning that behave similarly to
the nontrivial positive solution of EVB learning. Actually, EMAP learning and
EPB learning can behave similarly to EVB learning when the free energy is
minimized by local search.
Local EMAP and EPB Solutions
Here we conduct more detailed analysis of the free energies for EMAP learning
and EPB learning, and clarify the behavior of their local minima. To make
the free energy always comparable (ﬁnite), we slightly modify the problem.
Speciﬁcally, we solve the following problem:
Given
σ2 ∈R++,
min
r,{c2ah,c2
bh}H
h=1
F,
(12.77)
s.t.
cahcbh ≥ε2,
cah/cbh = 1
for h = 1,. . . , H,
and
⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
r(A, B) = δ(A; A)δ(B; B)
(for EMAP learning),
r(A, B) = rA(A)δ(B; B)
(for EPB-A learning),
r(A, B) = δ(A; A)rB(B)
(for EPB-B learning),
r(A, B) = rA(A)rB(B)
(for EVB learning),

12.1 Theoretical Analysis in Fully Observed MF
313
where the free energy F is deﬁned by Eq. (12.6), and the pseudo-Dirac delta
function is deﬁned as Gaussian with an arbitrarily small but positive variance
ε2 > 0:
δ(A; A) = MGaussM,H(A; A, ε2IM ⊗IH) ∝exp
!
−
∥A−A∥2
Fro
2ε2
"
,
δ(B; B) = MGaussL,H(B; B, ε2IL ⊗IH) ∝exp
!
−
∥B−B∥2
Fro
2ε2
"
.
Note that, in Eq. (12.77), we lower-bounded the product cahcbh of the prior
covariances and ﬁxed the ratio cah/cbh. We added the constraint for EVB
learning for comparison.
Following the discussion in Section 12.1.1, we can write the posterior as
r(A, B) = rA(A)rB(B),
where
rA(A) = MGaussM,H(A; A, IM ⊗ΣA) ∝exp
⎛⎜⎜⎜⎜⎜⎝−
tr
!
(A−A)Σ
−1
A (A−A)⊤
"
2
⎞⎟⎟⎟⎟⎟⎠,
rB(B) = MGaussL,H(B; B, IL ⊗ΣB) ∝exp
⎛⎜⎜⎜⎜⎜⎝−
tr
!
(B−B)Σ
−1
B (B−B)⊤
"
2
⎞⎟⎟⎟⎟⎟⎠,
for
A = $a1ωa1,. . . ,aHωaH
% ,
B =
b1ωb1,. . . ,bHωbH
 
,
ΣA = Diag

σ2
a1,. . . , σ2
aH
 
,
ΣB = Diag

σ2
b1,. . . , σ2
bH
 
,
and the variational parameters {ah,bh, σ2
ah, σ2
bh}H
h=1are the solution of the
following problem:
Given
σ2 ∈R++,
min
{ah,bh,σ2ah,σ2
bh,c2ah,c2
bh}H
h=1
F,
(12.78)
s.t.
ah,bh ∈R,
cahcbh ≥ε2,
cah/cbh = 1,
and
⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
σ2
ah = ε2,
σ2
bh = ε2
(for EMAP learning),
σ2
ah ≥ε2,
σ2
bh = ε2
(for EPB-A learning),
σ2
ah = ε2,
σ2
bh ≥ε2
(for EPB-B learning),
σ2
ah ≥ε2,
σ2
bh ≥ε2
(for EVB learning),
(12.79)
for h = 1,. . . , H,

314
12 MAP and Partially Bayesian Learning
where the free energy F is explicitly written by Eqs. (12.26) and (12.27),
that is,
2F = LM log(2πσ2) +
min(L,M)
h=1
γ2
h
σ2
+
H

h=1
2Fh,
(12.80)
where
2Fh = M log
c2
ah
σ2ah
+ L log
c2
bh
σ2
bh
+
a2
h + Mσ2
ah
c2ah
+
b2
h + Lσ2
bh
c2
bh
−(L + M) +
−2ahbhγh +

a2
h + Mσ2
ah
 b2
h + Lσ2
bh
 
σ2
.
(12.81)
By substituting the MAP solution (Corollary 12.2) and the PB solution
(Corollary 12.4), respectively, into Eq. (12.81), we can write the free energy as
a function of the product cahcbh of the prior covariances. We have the following
lemmas (the proofs are given in Sections 12.1.4 and 12.1.5, respectively):
Lemma 12.10
In EMAP learning, the free energy (12.81) can be written as
a function of cahcbh as follows:
2 ´FMAP
h
=
min
ah,bh∈R, σ2ah=σ2
bh=ε2 2Fh
=
⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩
(L + M)
!
log cahcbh +
ε2
cahcbh −1 + χ
"
for ε2 ≤cahcbh ≤σ2
γh ,
(L + M) $log cahcbh −1 + χ% −σ−2
!
γh −
σ2
cahcbh
"2
for cahcbh > σ2
γh .
(12.82)
Lemma 12.11
In EPB learning, the free energy (12.81) can be written as a
function of cahcbh as follows: if γh > σ √max(L, M),
2 ´FPB
h
= min
,
2 ´FPB−A
h
, 2 ´FPB−B
h
-
= min
2
minah,bh∈R, σ2ah≥ε2, σ2
bh=ε2 2Fh, minah,bh∈R, σ2ah=ε2, σ2
bh≥ε2 2Fh
3
=
⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
min(L, M)
!
log cahcbh +
ε2
cahcbh −1 + χ
"
for ε2 ≤cahcbh ≤
σ2
√
γ2
h−max(L,M)σ2 ,
min(L,M)+2 max(L,M)
2
log c2
ahc2
bh +
B
max(L, M)2 +
4γ2
h
c2ahc2
bh
−
σ2
c2ahc2
bh
+ max(L, M) log

−max(L, M) +
B
max(L, M)2 +
4γ2
h
c2ahc2
bh

−
γ2
h
σ2 −max(L, M) log(2σ2) + min(L, M)(χ −1)
for cahcbh >
σ2
√
γ2
h−max(L,M)σ2 ,
(12.83)

12.1 Theoretical Analysis in Fully Observed MF
315
and otherwise,
2 ´FPB
h
= min(L, M)

log cahcbh +
ε2
cahcbh
−1 + χ

.
(12.84)
By minimizing the EMAP free energy (12.82) and the EPB free energy
(12.83), respectively, with respect to the product cahcbh of the prior covariances,
we obtain the following theorems (the proofs are given also in Sections 12.1.4
and 12.1.5, respectively):
Theorem 12.12
In EMAP learning, the free energy (12.81) has the global
minimum such that
γEMAP
h

≡ahbh
 
= 0.
It has a nontrivial local minimum such that
γlocal−EMAP
h

≡ahbh
 
= ˘γlocal−EMAP
h
if and only if
γh > γlocal−EMAP,
where
γlocal−EMAP = σ
C
2(L + M),
(12.85)
˘γlocal−EMAP
h
= 1
2
!
γh +
.
γ2
h −2σ2(L + M)
"
.
(12.86)
Theorem 12.13
In EPB learning, the free energy (12.81) has the global
minimum such that
γEPB
h

≡ahbh
 
= 0.
It has a non-trivial local minimum such that
γlocal−EPB
h

≡ahbh
 
= ˘γlocal−EPB
h
if and only if
γh > γlocal−EPB,
where
γlocal−EPB = σ
.
L + M +
C
2LM + min(L, M)2,
(12.87)
˘γlocal−EPB
h
= γh
2

1 +
−max(L,M)σ2+√
γ4
h−2(L+M)σ2γ2
h+min(L,M)2σ4
γ2
h

.
(12.88)
Figure 12.4 shows the free energy (normalized by LM) as a function of
cahcbh for EMAP learning (given in Lemma 12.10), EPB learning (given in
Lemma 12.11), and EVB learning, deﬁned by
2 ´FVB
h
=
min
ah,bh∈R, σ2ah,σ2
bh≥ε2 2Fh.
For EMAP learing and EPB learning, we ignored some constants (e.g., the
entropy terms proportional to χ) to make the shapes of the free energies

316
12 MAP and Partially Bayesian Learning
0
0.2
0.4
0.6
–0.2
–0.1
0
EVB
EPB
EMAP
(a) γh = 10
0
0.2
0.4
0.6
–0.2
–0.1
0
EVB
EPB
EMAP
(b) γh = 12
0
0.2
0.4
0.6
–0.2
–0.1
0
EVB
EPB
EMAP
(c) γh = 14
0
0.2
0.4
0.6
–0.2
–0.1
0
EVB
EPB
EMAP
(d) γh = 16
Figure 12.4 Free energy dependence on cahcbh, where L = 20, M = 50. Crosses
indicate nontrivial local minima.
comparable. We can see deep pits at cahcbh →+0 in EMAP and EPB free
energies, which correspond to the global solutions. However, we also see
nontrivial local minima, which behave similarly to the nontrivial local solution
for VB learning. Namely, nontrivial local minima of EMAP, EPB, and EVB
free energies appear at locations similar to each other when the observed
singular value γh exceeds the thresholds given by Eqs. (12.85), (12.87), and
(6.127), respectively.
The deep pit at cahcbh →+0 is essential when we stick to the global
solution. The VB free energy does not have such a pit, which enables consistent
inference based on the free energy minimization principle. However, as long as
we rely on local search, the pit at the origin is not essential in practice. Assume
that a nontrivial local minimum exists, and we perform local search only once.
Then, whether local search for EMAP learning or EPB learning converges to
the trivial global solution or the nontrivial local solution simply depends on the
initialization. Note that the same applies also to EVB learning, for which local
search is not guaranteed to converge to the global solution. This is because of
the multimodality of the VB free energy, which can be seen in Figure 12.4.
One might wonder if some hyperpriors on c2
ah and c2
bh could ﬁll the deep pits
at cahcbh →+0 in the EMAP and the EPB free energies, so that the nontrivial

12.1 Theoretical Analysis in Fully Observed MF
317
local solutions are global when some reasonable conditions hold. However,
when we rely on the ARD property for model selection, hyperpriors should be
almost noninformative. With such an almost noninformative hyperprior, e.g.,
the inverge-Gamma, p(c2
ah, c2
bh) ∝(c2
ahc2
bh)1.001+0.001/(c2
ahc2
bh), which was used
in Bishop (1999b), deep pits still exist very close to the origin, which keep the
global EMAP and EPB estimators useless.
Comparison between Local-EMAP, Local-EPB, and EVB Solutions
Let us observe the behavior of local solutions. We deﬁne the local-EMAP
estimator and the local-EPB estimator, respectively, by
U
local−EMAP =
H

h=1
γlocal−EMAP
h
ωbhω⊤
ah,
where
γlocal−EMAP
h
=
⎧⎪⎪⎨⎪⎪⎩
˘γlocal−EMAP
h
if γh ≥γlocal−EMAP,
0
otherwise,
(12.89)
U
local−EPB =
H

h=1
γlocal−EPB
h
ωbhω⊤
ah,
where
γlocal−EPB
h
=
⎧⎪⎪⎨⎪⎪⎩
˘γlocal−EPB
h
if γh ≥γlocal−EPB,
0
otherwise,
(12.90)
following the deﬁnition of the local-EVB estimator (6.131) in Chapter 6. In
the following, we assume that local search algorithms for EMAP learning and
EPB learning ﬁnd these solutions.
Deﬁne the normalized (by the average noise contribution (12.74)) singular
values:
γ′
h =
γh
C
max(L, M)σ2 .
We also deﬁne normalized versions of the estimator, the truncation threshold,
and the shrinkage factor as
γ′
h =
γh
C
max(L, M)σ2 , γ′
h =
γh
C
max(L, M)σ2 , ˘γ′
h =
˘γh
C
max(L, M)σ2 . (12.91)
Then the normalized truncation thresholds and the normalized shrinkage
factors can be written as functions of α = min(L, M)/ max(L, M) as follows:
γ′EVB = σ
.
1 + α + √α

κ + 1
κ
 
,
(12.92)
˘γ′EVB
h
=
γ′
h
2
⎛⎜⎜⎜⎜⎜⎝1 −(1+α)σ2
γ′2
h
+
B!
1 −(1+α)σ2
γ′2
h
"2
−4ασ4
γ′4
h
⎞⎟⎟⎟⎟⎟⎠,
(12.93)

318
12 MAP and Partially Bayesian Learning
γ′local−EPB = σ
.
1 + α +
√
2α + α2,
(12.94)
˘γ′local−EPB
h
=
γ′
h
2

1 +
−σ2+√
γ′4
h −2(1+α)σ2γ′2
h +σ4
γ′2
h

,
(12.95)
γ′local−EMAP = σ √2(1 + α),
(12.96)
˘γ′local−EMAP
h
= 1
2
!
γ′
h +
.
γ′2
h −2σ2(1 + α)
"
.
(12.97)
Note that κ is also a function of α.
Figure 12.5 compares the normalized versions of the (global) EVB
estimator γ′EVB
h
, the local-EPB estimator γ′local−EPB
h
, and the local-EMAP
estimator γ′local−EMAP
h
. We can observe similar behaviors of those three
empirical Bayesian estimators. This is in contrast to the nonempirical Bayesian
estimators shown in Figure 12.3, where the PB estimator behaves similarly to
the VB estimator, while the MAP estimator behaves differently.
Figure 12.6 compares the normalized versions of the EVB truncation
threshold (12.92), the local-EPB truncation threshold (12.94), and the local-
EMAP truncation threshold (12.96). We can see that those thresholds behave
similarly. However, we can ﬁnd an essential difference of the local-EPB
threshold from the EVB and the local-EMAP thresholds: it holds that, for
any α,
γ′local−EPB < γ′MPUL ≤γ′EVB, γ′local−EMAP,
(12.98)
where
γ′MPUL =
γMPUL
C
max(L, M)σ2 = (1 + √α)
0
0.5
1
1.5
2
0
0.5
1
1.5
2
Loca
Local
Figure 12.5 Behavior of (global) EVB, the local-EPB, and the local-EMAP
estimators for α = min(L, M)/ max(L, M) = 1/3.

12.1 Theoretical Analysis in Fully Observed MF
319
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
Local-EPB
Local-EMAP
Figure 12.6 Truncation thresholds.
is the normalized version of the Marˇcenko–Pastur upper limit (MPUL)
(Eq. (8.41) in Chapter 8), which is also shown in Figure 12.6.
As discussed in Section 8.4.1, the MPUL is the largest singular value of an
L×M zero-mean independent random matrix in the large-scale limit where the
matrix size (L, M) goes to inﬁnity with ﬁxed ratio α = min(L, M)/ max(L, M).
In other words, the MPUL corresponds to the minimum observed singu-
lar value detectable (or distinguishable from noise) by any dimensionality
reduction method. The inequalities (12.98) say that local-EPB threshold is
always smaller than the MPUL, while the EVB threshold and the local-EMAP
threshold are never smaller than the MPUL. This implies that, for a large-scale
observed matrix, the EVB estimator and the local-EMAP estimator discard
the singular components dominated by noise, while the local-EPB estimator
retains some of them.
12.1.4 Proofs of Lemma 12.10 and Theorem 12.12
By substituting the MAP solution, given by Corollary 12.2, we can write the
free energy (12.81) as follows: for ε2 ≤cahcbh ≤σ2
γh ,
2 ´FMAP
h
=
min
ah,bh∈R, σ2ah=σ2
bh=ε2 2Fh
= M log c2
ah + L log c2
bh
+

M
c2ah +
L
c2
bh

ε2 −(L + M) + (L + M)χ
= M log c2
ah + L log c2
bh +

M
c2ah +
L
c2
bh

ε2
−(L + M) + (L + M)χ,
(12.99)

320
12 MAP and Partially Bayesian Learning
and for cahcbh > σ2
γh ,
2 ´FMAP
h
=
min
ah,bh∈R, σ2ah=σ2
bh=ε2 2Fh
= M log c2
ah + L log c2
bh +
!
γh −
σ2
cahcbh
" ⎛⎜⎜⎜⎜⎜⎝
2
cahcbh +
−2γh+γh−
σ2
cah cbh
σ2
⎞⎟⎟⎟⎟⎟⎠
+

M
c2ah +
L
c2
bh

ε2 −(L + M) + (L + M)χ
= M log c2
ah + L log c2
bh −σ−2

γh −
σ2
cahcbh
2
−(L + M) + (L + M)χ.
(12.100)
In the second-to-last equation in Eq. (12.100), we ignored the fourth term
because cahcbh > σ2
γh implies c2
ah, c2
bh ≫ε2 (with an arbitrarily high probability
depending on ε2). By ﬁxing the ratio to cah/cbh = 1, we obtain Eq. (12.82),
which proves Lemma 12.10.
Now we minimize the free energy (12.82) with respect to cahcbh, and ﬁnd
nontrivial local solutions. The free energy is continuous in the domain ε2 ≤
cahcbh < ∞, and differentiable except at cahcbh = σ2
γh . The derivative is given by
∂2 ´FMAP
h
∂(cahcbh) =
⎧⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎩
(L + M)

1
cahcbh −
ε2
c2ahc2
bh

for ε2 ≤cahcbh ≤σ2
γh

L+M
cahcbh + 2σ−2
!
γh −
σ2
cahcbh
"
σ2
c2ahc2
bh

for cahcbh > σ2
γh
=
⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩
L+M
c2ahc2
bh

cahcbh −ε2 
for ε2 ≤cahcbh ≤σ2
γh ,
1
c3ahc3
bh

(L + M)c2
ahc2
bh + 2γhcahcbh −2σ2 
for cahcbh > σ2
γh .
(12.101)
Eq. (12.101) implies that the free energy ´FMAP
h
is increasing for ε2 ≤cahcbh ≤
σ2
γh , and that it is increasing at cahcbh = σ2
γh and at cahcbh →+∞.
In the region of σ2
γh < cahcbh < +∞, the free energy has stationary points if
and only if
γh ≥σ
C
2(L + M)

≡γlocal−EMAP 
,
(12.102)
because the derivative can be factorized (with real factors if and only if the
condition (12.102) holds) as
∂2 ´FMAP
h
∂(cahcbh) = L + M
c3ahc3
bh
$cahcbh −´cah ´cbh
% $cahcbh −˘cah ˘cbh
% ,

12.1 Theoretical Analysis in Fully Observed MF
321
where
´cah ´cbh =
γh −
.
γ2
h −2σ2(L + M)
L + M
,
(12.103)
˘cah ˘cbh =
γh +
.
γ2
h −2σ2(L + M)
L + M
.
(12.104)
Summarizing the preceding discussion, we have the following lemma:
Lemma 12.14
If γh ≤γlocal−EMAP, the EMAP free energy ´FMAP
h
, deﬁned
by Eq. (12.82), is increasing for cahcbh > ε2 , and therefore minimized at
cahcbh = ε2. If γh > γlocal−EMAP,
´FMAP
h
is
⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩
increasing
for
ε2 < cahcbh < ´cah ´cbh,
decreasing
for
´cah ´cbh < cahcbh < ˘cah ˘cbh,
increasing
for
˘cah ˘cbh < cahcbh < +∞,
and therefore has two (local) minima at cahcbh = ε2 and at cahcbh = ˘cah ˘cbh.
Here ´cah ´cbh and ˘cah ˘cbh are deﬁned by Eqs. (12.103) and (12.104), respectively.
When γh > γlocal−EMAP, the EMAP free energy (12.82) at the local minima is
2 ´FMAP
h
=
⎧⎪⎪⎪⎨⎪⎪⎪⎩
0
at cahcbh = ε2,
(L + M) $log ˘cah ˘cbh −1 + χ% −σ−2
!
γh −
σ2
˘cah ˘cbh
"2
at cahcbh = ˘cah ˘cbh,
respectively. Since we assume that χ = −log ε2 is an arbitrarily large constant
(ε2 > 0 is arbitrarily small), cahcbh = ε2 is always the global minimum.
Substituting Eq. (12.104) into Eq. (12.36) gives Eq. (12.86), which com-
pletes the proof of Theorem 12.12.
□
12.1.5 Proofs of Lemma 12.11 and Theorem 12.13
We ﬁrst analyze the free energy for EPB-A learning. From Eq. (12.49), we
have
cahcbh =
σ2
.
(γPB−A
h
)2 −Mσ2 .
Therefore, if
γh ≤σ
√
M,

322
12 MAP and Partially Bayesian Learning
there exists only the null solution (12.53) for any cahcbh > 0, and therefore the
free energy (12.81) is given by
2 ´FPB−A
h
=
min
ah,bh∈R, σ2ah≥ε2, σ2
bh=ε2 2Fh
= L

log c2
bh + ε2
c2
bh
−1 + χ

.
(12.105)
In the following, we consider the case where
γh > σ
√
M.
(12.106)
For ε2 ≤cahcbh ≤
σ2
√
γ2
h−Mσ2 , there still exists only the null solution (12.53) with
the free energy given by Eq. (12.105). The positive solution (12.50) appears for
cahcbh >
σ2
√
γ2
h−Mσ2 with the free energy given by
2 ´FPB−A
h
=
min
ah,bh∈R, σ2ah≥ε2, σ2
bh=ε2 2Fh
=
min
ah,bh∈R, σ2ah≥ε2
)
M log
c2
ah
σ2ah + L log c2
bh +
b2
h
c2
bh
−2ahbhγh
σ2
+

a2
h + Mσ2
ah
 ! b2
h
σ2 +
1
c2ah
" 1
−(L + M) + Lχ
=
min
ah,bh∈R, σ2ah≥ε2
)
M log
b2
h+σ2/c2
ah
σ2
+
b2
h
c2
bh
−
b2
hγh
b2
h+σ2/c2ah
2γh
σ2 +
a2
h+Mσ2
ah
σ2ah
1
+ M log c2
ah + L log c2
bh −(L + M) + Lχ
= min
bh∈R
)
M log
b2
h + σ2/c2
ah
 
+
b2
h+σ2/c2
ah
c2
bh
−
b2
hγ2
h
σ2(b2
h+σ2/c2ah)
1
−
σ2
c2ahc2
bh
+ M log c2
ah + L log c2
bh −M log σ2 −L + Lχ
= min
bh∈R
)
M log
b2
h + σ2/c2
ah
 
+
b2
h+σ2/c2
ah
c2
bh
+
γ2
h
c2ah(b2
h+σ2/c2ah)
1
−
γ2
h
σ2 −
σ2
c2ahc2
bh
+ M log c2
ah + L log c2
bh −M log σ2 −L + Lχ.
(12.107)
Here we used the conditions (12.61) and (12.62) for the PB-A solution. By
substituting the other conditions (12.64) and (12.66) into Eq. (12.107), we have

12.1 Theoretical Analysis in Fully Observed MF
323
2 ´FPB−A
h
= M log

−M +
B
M2 +
4γ2
h
c2ahc2
bh

+
−M+
A
M2+
4γ2
h
c2ah c2
bh
2
+
M+
A
M2+
4γ2
h
c2ah c2
bh
2
−
γ2
h
σ2 −
σ2
c2ahc2
bh
+ M log c2
ah + (L + M) log c2
bh −M log(2σ2) −L + Lχ
= M log

−M +
B
M2 +
4γ2
h
c2ahc2
bh

+
B
M2 +
4γ2
h
c2ahc2
bh
−
σ2
c2ahc2
bh
−
γ2
h
σ2 + M log c2
ah + (L + M) log c2
bh −M log(2σ2) −L + Lχ.
(12.108)
The PB-B free energy can be derived in exactly the same way, and the result
is symmetric to the PB-A free energy. Namely, if
γh ≤σ
√
L,
there exists only the null solution (12.59) for any cahcbh > 0 with the free
energy given by
2 ´FPB−B
h
=
min
ah,bh∈R, σ2ah=ε2, σ2
bh≥ε2 2Fh
= M
!
log c2
ah + ε2
c2ah −1 + χ
"
.
(12.109)
Assume that
γh > σ
√
L.
For ε2 ≤cahcbh ≤
σ2
√
γ2
h−Lσ2 , there still exists only the null solution (12.59) with
the free energy given by Eq. (12.109). The positive solution (12.56) appears
for cahcbh >
σ2
√
γ2
h−Lσ2 with the free energy given by
2 ´FPB−B
h
=
min
ah,bh∈R, σ2ah=ε2, σ2
bh≥ε2 2Fh
= L log

−L +
B
L2 +
4γ2
h
c2ahc2
bh

+
B
L2 +
4γ2
h
c2ahc2
bh
−
σ2
c2ahc2
bh
−
γ2
h
σ2 + (L + M) log c2
ah + L log c2
bh −L log(2σ2) −M + Mχ.
(12.110)
By ﬁxing the ratio between the prior covariances to cah/cbh = 1 in Eqs.
(12.105) and (12.108) through (12.110), and taking the posterior choice in Eq.
(12.47) into account, we obtain Eqs. (12.83) and (12.84), which prove Lemma
12.11.

324
12 MAP and Partially Bayesian Learning
Let us minimize the free energy with respect to cahcbh. When
γh ≤σ
C
max(L, M),
the free energy is given by Eq. (12.84), and its derivative is given by
∂2 ´FPB
h
∂(cahcbh) = min(L, M)
c2ahc2
bh

cahcbh −ε2 
.
This implies that the free energy ´FPB
h
is increasing for ε2 < cahcbh < ∞, and
therefore minimized at cahcbh = ε2.
Assume that
γh > σ
C
max(L, M).
In this case, the free energy is given by Eq. (12.83), which is continuous in the
domain ε2 ≤cahcbh < ∞, and differentiable except at cahcbh =
σ2
√
γ2
h−max(L,M)σ2 .
Although the continuity is not very obvious, one can verify it by checking the
value at cahcbh =
σ2
√
γ2
h−max(L,M)σ2 for each case in Eq. (12.83). The continuity is
also expected from the fact that the PB solution is continuous at the threshold
γh = γPB
h , i.e., the positive solution (Eq. (12.50) for PB-A and Eq. (12.56) for
PB-B) converges to the null solution (Eq. (12.53) for PB-A and Eq. (12.59) for
PB-B) when γh →γPB−A
h
+ 0.
The free energy (12.83) is the same as Eq. (12.84) for ε2 ≤cahcbh ≤
σ2
√
γ2
h−max(L,M)σ2 , and therefore increasing in ε2 < cahcbh ≤
σ2
√
γ2
h−max(L,M)σ2 . For
cahcbh >
σ2
√
γ2
h−max(L,M)σ2 , the derivative of the free energy with respect to c2
ahc2
bh
is given by
∂2 ´FPB
h
∂(c2ahc2
bh) = min(L,M)+2 max(L,M)
2c2ahc2
bh
−
4γ2
h
2c4ahc4
bh
A
max(L,M)2+
4γ2
h
c2ah c2
bh
+
σ2
c4ahc4
bh
−
4 max(L,M)γ2
h
2c4ahc4
bh
A
max(L,M)2+
4γ2
h
c2ah c2
bh
⎛⎜⎜⎜⎜⎜⎜⎝−max(L,M)+
A
max(L,M)2+
4γ2
h
c2ah c2
bh
⎞⎟⎟⎟⎟⎟⎟⎠
= min(L,M)+2 max(L,M)
2c2ahc2
bh
+
σ2
c4ahc4
bh
−
4γ2
h
2c4ahc4
bh
⎛⎜⎜⎜⎜⎜⎜⎝−max(L,M)+
A
max(L,M)2+
4γ2
h
c2ah c2
bh
⎞⎟⎟⎟⎟⎟⎟⎠
=
1
2c2ahc2
bh

L + M + 2
σ2
c2ahc2
bh
−
B
max(L, M)2 +
4γ2
h
c2ahc2
bh

=
1
2c4ahc4
bh
!
(L + M)c2
ahc2
bh + 2σ2 −cahcbh
.
max(L, M)2c2ahc2
bh + 4γ2
h
"
,
(12.111)

12.1 Theoretical Analysis in Fully Observed MF
325
which has the same sign as
τ(c2
ahc2
bh) =
,
(L + M)c2
ahc2
bh + 2σ2-2 −
2
cahcbh
.
max(L, M)2c2ahc2
bh + 4γ2
h
32
=

2LM + min(L, M)2 
c4
ahc4
bh −4

γ2
h −σ2(L + M)
 
c2
ahc2
bh + 4σ4.
(12.112)
Eq. (12.112) is a quadratic function of c2
ahc2
bh, being positive at c2
ahc2
bh →+0
and at c2
ahc2
bh →+∞. The free energy has stationary points if and only if
γh ≥σ
.
L + M +
C
2LM + min(L, M)2 
≡γlocal−EPB 
,
(12.113)
because τ(c2
ahc2
bh), which has the same sign as the derivative of the free energy,
can be factorized (with real factors if and only if the condition (12.113) holds)
as
τ(c2
ahc2
bh) =

2LM + min(L, M)2 
c2
ahc2
bh −´c2
ah ´c2
bh
 
c2
ahc2
bh −˘c2
ah ˘c2
bh
 
,
where
´c2
ah ´c2
bh = 2 · (γ2
h−σ2(L+M))−
.
(γ2
h−σ2(L+M))
2−(2LM+min(L,M)2)σ4
2LM+min(L,M)2
,
(12.114)
˘c2
ah ˘c2
bh = 2 · (γ2
h−σ2(L+M))+
.
(γ2
h−σ2(L+M))
2−(2LM+min(L,M)2)σ4
2LM+min(L,M)2
.
(12.115)
Summarizing the preceding discussion, we have the following lemma:
Lemma 12.15
If γh ≤γlocal−EPB, the EPB free energy ´FPB
h , deﬁned by Eqs.
(12.83) and (12.84), is increasing for cahcbh > ε2, and therefore minimized at
cahcbh = ε2. If γh > γlocal−EPB,
´FPB
h
is
⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩
increasing
for
ε2 < cahcbh < ´cah ´cbh,
decreasing
for
´cah ´cbh < cahcbh < ˘cah ˘cbh,
increasing
for
˘cah ˘cbh < cahcbh < +∞,
and therefore has two (local) minima at cahcbh = ε2 and at cahcbh = ˘cah ˘cbh.
Here, ´cah ´cbh and ˘cah ˘cbh are deﬁned by Eqs. (12.114) and (12.115), respectively.
When γh > γlocal−EPB, the EPB free energy (12.83) at the null local solution
cahcbh = ε2 is 2 ´FPB
h
= 0, while the EPB free energy at the positive local solution
cahcbh = ˘cah ˘cbh contains the term min(L, M)χ with χ = −log ε2 assumed to be
arbitrarily large. Therefore, the null solution is always the global minimum.
Substituting Eq. (12.115) into Eq. (12.46) gives Eq. (12.88), which com-
pletes the proof of Theorem 12.13.
□

326
12 MAP and Partially Bayesian Learning
12.1.6 Noise Variance Estimation
The noise variance σ2 is unknown in many practical applications. In VB
learning, minimizing the free energy (12.26) with respect also to σ2 gives a
reasonable estimator, with which perfect dimensionality recovery was proven
in Chapter 8. Here, we investigate whether MAP learning and PB learning offer
good noise variance estimators.
We ﬁrst consider the nonempirical Bayesian variants where the prior
covariances CA, CB are treated as given constants. By using Lemma 12.10,
we can write the MAP free energy with the variational parameters optimized,
as a function of σ2, as follows:
2 ´FMAP = LM log(2πσ2) +
min(L,M)
h=1
γ2
h
σ2
+ H
h=1 2 ´FMAP
h
= LM log(2πσ2) +
min(L,M)
h=1
γ2
h
σ2
+ min(H,H)
h=1
)
(L + M) $log cahcbh −1 + χ% −σ−2
!
γh −
σ2
cahcbh
"21
+ min(L,M)
h=min(H,H)+1(L + M)
!
log cahcbh +
ε2
cahcbh −1 + χ
"
= LM log σ2 +
min(L,M)
h=min(H,H)+1 γ2
h
σ2
+ min(H,H)
h=1

2γh
cahcbh −
σ2
c2ahc2
bh

+ LM log(2π) + min(L,M)
h=1
(L + M)
!
log cahcbh +
ε2
cahcbh −1 + χ
"
= LM log σ2 +
min(L,M)
h=min(H,H)+1 γ2
h
σ2
+ min(H,H)
h=1

2γh
cahcbh −
σ2
c2ahc2
bh

+ const.
(12.116)
for
σ2 MAP
H+1
≤σ2 ≤σ2 MAP
H
,
(12.117)
where
σ2 MAP
h
=
⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩
∞
for h = 0,
cahcbhγh
for h = 1,. . . , min(L, M),
0
for h = min(L, M) + 1.
(12.118)
Assume that we use the full-rank model H = min(L, M), and expect the
ARD property to ﬁnd the correct rank. Under this setting, the free energy
(12.116) can be arbitrarily small for σ2
→+0, because the ﬁrst term
diverges to −∞, and the second term is equal to zero for 0 (= σ2 MAP
min(L,M)+1) ≤
σ2 ≤camin(L,M)cbmin(L,M)γmin(L,M)(= σ2 MAP
min(L,M)) (note that γmin(L,M) > 0 with
probability 1). This leads to the following lemma:

12.1 Theoretical Analysis in Fully Observed MF
327
Lemma 12.16
Assume that H
= min(L, M) and CA,CB are given as
constants. Then the MAP free energy with respect to σ2 is (globally) minimized
at
σ2 MAP →+0.
The PB free energy behaves differently. By using Lemma 12.11, we can
write the PB free energy with the variational parameters optimized, as a
function of σ2, as follows:
2 ´FPB = LM log(2πσ2) +
min(L,M)
h=1
γ2
h
σ2
+ H
h=1 2 ´FPB
h
= LM log(2πσ2) +
min(L,M)
h=1
γ2
h
σ2
+ min(H,H)
h=1
)
min(L,M)+2 max(L,M)
2
log c2
ahc2
bh +
B
max(L, M)2 +
4γ2
h
c2ahc2
bh
−
σ2
c2ahc2
bh
+ max(L, M) log

−max(L, M) +
B
max(L, M)2 +
4γ2
h
c2ahc2
bh

−
γ2
h
σ2 −max(L, M) log(2σ2) + min(L, M)(χ −1)
1
+
min(L,M)

h=min(H,H)+1
min(L, M)

log cahcbh +
ε2
cahcbh
−1 + χ

= (min(L, M) −min(H, H)) max(L, M) log(2σ2) +
min(L,M)
h=min(H,H)+1 γ2
h
σ2
+ min(H,H)
h=1
)
max(L, M) log c2
ahc2
bh +
B
max(L, M)2 +
4γ2
h
c2ahc2
bh
−
σ2
c2ahc2
bh
+ max(L, M) log

−max(L, M) +
B
max(L, M)2 +
4γ2
h
c2ahc2
bh
 1
+ LM log(π) + min(L,M)
h=1
min(L, M) $log cahcbh −1 + χ%
= (min(L, M) −min(H, H)) max(L, M) log(2σ2) +
min(L,M)
h=min(H,H)+1 γ2
h
σ2
+ min(H,H)
h=1
)
max(L, M) log c2
ahc2
bh +
B
max(L, M)2 +
4γ2
h
c2ahc2
bh
−
σ2
c2ahc2
bh
+ max(L, M) log

−max(L, M) +
B
max(L, M)2 +
4γ2
h
c2ahc2
bh
 1
+ const.
(12.119)
for
σ2 PB
H+1 ≤σ2 ≤σ2 PB
H
,
(12.120)

328
12 MAP and Partially Bayesian Learning
where
σ2 PB
h
=
⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
∞
for h = 0,
c2
ahc2
bh
2

−max(L, M) +
.
max(L, M)2 + 4
γh
c2ahc2
bh

for h = 1,. . . , min(L, M),
0
for h = min(L, M) + 1.
(12.121)
We ﬁnd a remarkable difference between the MAP free energy (12.116)
and the PB free energy (12.119): unlike in the MAP free energy, the ﬁrst log
term in the PB free energy disappears for 0 = σ2 PB
min(L,M)+1 < σ2 < σ2 PB
min(L,M),
and therefore, the PB free energy does not diverge to −∞at σ2 →+0. We
can actually prove that the noise variance estimator is lower-bounded by a
positive value as follows. The PB free energy (12.121) is continuous, and, for
0 = σ2 PB
min(L,M)+1 < σ2 < σ2 PB
min(L,M), it can be written as
2 ´FPB = −σ2
c2ahc2
bh
+ const.,
which is monotonically decreasing. This leads to the following lemma:
Lemma 12.17
Assume that H = min(L, M) and CA,CB are given as con-
stants. Then the noise variance estimator in PB learning is lower-bounded by
σ2 MAP ≥σ2 PB
min(L,M)
=
c2
amin(L,M)c2
bmin(L,M)
2
⎛⎜⎜⎜⎜⎜⎝−max(L, M) +
B
max(L, M)2 + 4
γmin(L,M)
c2amin(L,M)c2
bmin(L,M)
⎞⎟⎟⎟⎟⎟⎠.
(12.122)
We numerically investigated the behavior of the noise variance estimator
by creating random observed matrices V = B∗A∗⊤+ E ∈RL×M, and depicting
the VB, PB, and MAP free energies as functions of σ2 with the variational
parameters optimized. Figure 12.7 shows a typical case for L = 20, M = 50,
H∗= 2 with the entries of A∗∈RM×H∗and B∗∈RL×H∗independently drawn
from Gauss1(0, 12), and the entries of E ∈RL×M independently drawn from
Gauss1(0, 0.32). We set the prior covariances to cahcbh = 1. As Lemma 12.16
states, the global minimum of the MAP free energy is at σ2 →+0. Since
no nontrivial local minimum is observed, local search gives the same trivial
solution. On the other hand, the PB free energy has a minimum in the
positive region σ2 > 0 with probability 1, as Lemma 12.17 states. However,
we empirically observed that PB learning tends to underestimate the noise

12.2 More General Cases
329
0.1
0.2
0.3
0.4
-1
0
1
2
VB
PB
MAP
Figure 12.7 Free energy dependence on σ2. Crosses indicate nontrivial minima.
variance, as in Figure 12.7. Therefore, we cannot expect that the noise variance
estimation works well in PB learning, either.
The situation is more complicated in the empirical Bayesian variants. Since
the global EMAP estimator and the global EPB estimator, given any σ2 > 0,
are the null solution, the joint global optimization over all variational param-
eters and the hyperparameters results in σ2 = min(L,M)
h=1
γ2
h/(LM), regardless
of observations—all observed signals are considered to be noise. If we adopt
nontrivial local solutions as estimators, i.e., the local-EMAP estimator and the
local-EPB estimator, the free energies are not continuous anymore as functions
of σ2, because of the energy jump by the entropy factor χ of the pseudo-
Dirac delta function. Even in that case, if we globally minimize the free
energies with respect to σ2, the estimator contains no nontrivial local solution,
because the null solutions cancel all entropy factors. As such, no reasonable
way to estimate the noise variance has been found in EMAP learning and in
EPB learning.
In the previous work on the TF model with PB learning (Chu and Ghahra-
mani, 2009) and with EMAP learning (Mørup and Hansen, 2009), the noise
variance was treated as a given constant. This was perhaps because the noise
variance estimation failed, which is consistent with the preceding discussion.
12.2 More General Cases
Although extending the analysis for fully observed MF to more general cases
is not easy in general, some basic properties can be shown. Speciﬁcally, this
section shows that the global solutions for EMAP learning and EPB learning
are also trivial and useless in the MF model with missing entries and in the TF
model. Nevertheless, we experimentally show in Section 12.3 that local search
for EMAP learning and EPB learning provides estimators that behave similarly
to the EVB estimator.

330
12 MAP and Partially Bayesian Learning
12.2.1 Matrix Factorization with Missing Entries
The MF model with missing entries was introduced in Section 3.2. There, the
likelihood (12.1) is replaced with
p(V|A, B) ∝exp

−1
2σ2
####PΛ (V) −PΛ

BA⊤ ####
2
Fro

,
(12.123)
where Λ denotes the set of observed indices, and
(PΛ (V))l,m =
⎧⎪⎪⎨⎪⎪⎩
Vl,m
if (l, m) ∈Λ,
0
otherwise.
The VB free energy is explicitly written as
2F = # (Λ) · log(2πσ2) + M log det (CA) + L log det (CB)
−
M

m=1
log det
ΣA,m
 
−
L

l=1
log det
ΣB,l
 
−(L + M)H
+ tr
⎧⎪⎪⎨⎪⎪⎩C−1
A
⎛⎜⎜⎜⎜⎜⎝A
⊤A +
M

m=1
ΣA,m
⎞⎟⎟⎟⎟⎟⎠+ C−1
B
⎛⎜⎜⎜⎜⎜⎝B
⊤B +
L

l=1
ΣB,l
⎞⎟⎟⎟⎟⎟⎠
⎫⎪⎪⎬⎪⎪⎭
+ σ−2 
(l,m)∈Λ

Vl,m −2Vl,ma
⊤
mbl + tr
)!
ama
⊤
m + ΣA,m
" 
blb
⊤
l + ΣB,l
1
,
(12.124)
where # (Λ) denotes the number of observed entries.
We deﬁne the EMAP learning problem and the EPB learning problem by
Eq. (12.77) with the free energy given by Eq. (12.124). The following holds:
Lemma 12.18
The global solutions of EMAP learning and EPB learning for
the MF model with missing entries, i.e., Eqs. (12.123), (12.2), and (12.3), are
U
EMAP = U
EPB = BA
⊤= 0(L,M), regardless of observations.
Proof
The posterior covariance for A is clipped to ΣA,m = ε2IH in EPB-B
learning, while the posterior covariance for B is clipped to ΣB,m = ε2IH in
EPB-A learning. In either case, one can make the second or the third term in the
free energy (12.124) arbitrarily small to cancel the fourth or the ﬁfth term by
setting CA = ε2IH or CB = ε2IH. Then, because of the terms in the third line of
Eq. (12.124), which come from the prior distributions, it holds that A →0(M,H)
or B →0(M,H) for ε2 →+0, which results in U
EPB = BA
⊤→0(L,M). In EMAP
learning, both posterior covariances are clipped to ΣA,m = ΣB,m = ε2IH. By the
same argument as for EPB learning, we can show that U
EMAP = BA
⊤→0(L,M),
which completes the proof.
□

12.2 More General Cases
331
12.2.2 Tucker Factorization
The TF model was introduced in Section 3.3.1. The likelihood and the priors
are given by
p(V|G, {A(n)}) ∝exp
⎛⎜⎜⎜⎜⎜⎜⎝−
###V −G ×1 A(1) · · · ×N A(N)###2
2σ2
⎞⎟⎟⎟⎟⎟⎟⎠,
(12.125)
p(G) ∝exp

−vec(G)⊤(CG(N) ⊗· · · ⊗CG(1))−1 vec(G)
2

,
(12.126)
p({A(n)}) ∝exp
⎛⎜⎜⎜⎜⎜⎝−
N
n=1 tr(A(n)C−1
A(n) A(n)⊤)
2
⎞⎟⎟⎟⎟⎟⎠,
(12.127)
where ⊗and vec(·) denote the Kronecker product and the vectorization
operator, respectively. {CG(n)} and {CA(n)} are the prior covariances restricted
to be diagonal, i.e.,
CG(n) = Diag

c2
g(n)
1 ,. . . , c2
g(n)
H(n)

,
CA(n) = Diag

c2
a(n)
1 ,. . . , c2
a(n)
H(n)

.
We denote ˘CG = CG(N) ⊗· · · ⊗CG(1).
The VB free energy is explicitly written as
2F =
⎛⎜⎜⎜⎜⎜⎝
N

n=1
M(n)
⎞⎟⎟⎟⎟⎟⎠log(2πσ2) + log det
 ˘CG
 
+
N

n=1
M(n) log det (CA(n))
−log det
!˘ΣG
"
−
N

n=1
M(n) log det
Σ A(n)
 
+ ∥V∥2
σ2
−
N

n=1
H(n) −
N

n=1
(M(n)H(n))
+ tr
!
˘C
−1
G (˘g˘g
⊤+ ˘ΣG)
"
+
N

n=1
tr
!
C−1
A(n)(A
(n)⊤A
(n) + M(n)Σ A(n))
"
−2
σ2 ˘v⊤(A
(N) ⊗· · · ⊗A
(1))˘g
+ 1
σ2 tr
2!
(A
(N)⊤A
(N) + M(N)Σ A(N)) ⊗· · · ⊗(A
(1)⊤A
(1) + M(1)Σ A(1))
"
·(˘g˘g
⊤+ ˘ΣG)
3
.
(12.128)
In the TF model, we refer as PB-G learning to the approximate Bayesian
method where the posteriors for the factor matrices {A(N)} are approximated

332
12 MAP and Partially Bayesian Learning
by the pseudo-Dirac delta function, and as PB-A learning to the one where
the posterior for the core tensor G is approximated by the pseudo-Dirac delta
function. PB learning chooses the one giving a lower free energy from PB-G
learning and PB-A learning. MAP learning approximates both posteriors
by the pseudo-Dirac delta function. Note that the approach by Chu and
Ghahramani (2009) corresponds to PB-G learning with the prior covariances
ﬁxed to CG(n) = CA(n) = IH(n) for n = 1,. . . , N, while the approach, called ARD
Tucker, by Mørup and Hansen (2009) corresponds to EMAP learning with the
prior covariances estimated from observations. In both approaches, the noise
variance σ2 was treated as a given constant.
Again the global solutions of EMAP learning and EPB learning are trivial
and useless.
Lemma 12.19
The global solutions of EMAP learning and EPB learning for
the TF model, i.e., Eqs. (12.125) through (12.127), are 
UEMAP = 
UEPB =

G ×1 A
(1) · · · ×N A
(N) = 0(M(1), ... ,M(N)), regardless of observations.
Proof
The posterior covariance for G is clipped to ˘ΣG = ε2IN
n=1 H(n) in EPB-A
learning, while the posterior covariances for {A(n)} are clipped to {Σ A(n) =
ε2IH(n)} in EPB-G learning. In either case, one can make the second or the third
term in the free energy (12.128) arbitrarily small to cancel the fourth or the ﬁfth
term by setting {CG(n) = ε2IH(n)} or {CA(n) = ε2IH(n)}. Then, because of the terms
in the fourth line of Eq. (12.128), which come from the prior distributions,
it holds that 
G →0(H(1),...,H(N)) or {A
(N) →0(M(N),H(N))} for ε2 →+0, which
results in 
UEPB = 0(M(1),...,M(N)). In EMAP learning, both posterior covariances
are clipped to ˘ΣG = ε2IN
n=1 H(n) and {Σ A(n) = ε2IH(n)}, respectively. By the same
argument as for EPB learning, we can show that 
UEMAP = 0(M(1),...,M(N)), which
completes the proof.
□
12.3 Experimental Results
In this section, we experimentally investigate the behavior of EMAP learning
and EPB learning, in comparison with EVB learning. We start from the fully
observed MF model, where we can assess how often local search ﬁnds the
nontrivial local solution (derived in Section 12.1.3) rather than the global null
solution. After that, we conduct experiments in collaborative ﬁltering, where
the MF model with missing entries is used, and in TF.
For local search, we adopted the standard iterative algorithm. The standard
iterative algorithm for EVB learning has been derived in Chapter 3. The

12.3 Experimental Results
333
standard iterative algorithms for EPB learning and EMAP learning, which
can be derived simply by setting the derivatives of the corresponding free
energies with respect to the unknown parameters to zero, similarly apply the
stationary conditions in turn to update unknown parameters. For initialization,
the entries of the mean parameters, e.g., A, B, and 
G, were drawn from
Gauss1(0, 12), while the covariance parameters were set to the identity, e.g.,
˘ΣG = IN
n=1 H(n), Σ A(n) = CG(n) = CA(n) = IH(n). We used this initialization scheme
through all experiments in this section.
12.3.1 Fully Observed MF
We ﬁrst conducted an experiment on an artiﬁcial (Artiﬁcial1) data set, which
was generated as follows. We randomly generated true matrices A∗∈RM×H∗
and B∗∈RL×H∗such that each entry of A∗and B∗follows Gauss1(0, 1).
An observed matrix V ∈RL×M was created by adding a noise subject to
Gauss1(0, 1) to each entry of B∗A∗⊤. Figures 12.8 through 12.10 show the
free energy and the estimated rank over iterations in EVB learning, local-EPB
0
500
1,000
1,500
2,000
2,500
2.02
2.04
2.06
2.08
2.1
2.12
2.14
Iteration
F /(LM )
 
 
EVB(Analytic)
EVB(Iterative)
(a) Free energy
0
500
1,000
1,500
2,000
2,500
0
20
40
60
80
100
Iteration
H
 
 
EVB(Analytic)
EVB(Iterative)
(b) Estimated rank
Figure 12.8 EVB learning on Artiﬁcial1 (L = 100, M = 300, H∗= 20).
0
500
1,000
1,500
2,000
2,500
1.85
1.9
1.95
Iteration
Local−EPB(Analytic)
Local−EPB(Iterative)
(a) Free energy
0
500
1,000
1,500
2,000
2,500
0
20
40
60
80
100
Iteration
Local−EPB(Analytic)
Local−EPB(Iterative)
(b) Estimated rank
Figure 12.9 Local-EPB learning on Artiﬁcial1.

334
12 MAP and Partially Bayesian Learning
Table 12.1 Estimated rank in fully observed MF experiments.

HEVB

Hlocal-EPB

Hlocal-EMAP
Data set
M
L H∗Analytic Iterative Analytic Iterative Analytic Iterative
Artiﬁcial1
300 100 20
20
20 (100%)
20
20 (100%)
20
20 (100%)
Artiﬁcial2
500 400
5
5
5 (100%)
8
8 (90%)
5
5 (100%)
9 (10%)
Chart
600 60 –
2
2 (100%)
2
2 (100%)
2
2 (100%)
Glass
214
9 –
1
1 (100%)
1
1 (100%)
1
1 (100%)
Optical
5,620 64 –
10
10 (100%)
10
10 (100%)
6
6 (100%)
Digits
Satellite
6,435 36 –
2
2 (100%)
2
2 (100%)
1
1 (100%)
0
500
1,000
1,500
2,000
2,500
1.36
1.38
1.4
1.42
1.44
1.46
1.48
Iteration
Local−EMAP(Analytic)
Local−EMAP(Iterative)
(a) Free energy
0
500
1,000
1,500
2,000
2,500
0
20
40
60
80
100
Iteration
Local−EMAP(Analytic)
Local−EMAP(Iterative)
(b) Estimated rank
Figure 12.10 Local-EMAP learning on Artiﬁcial1.
learning, and local-EMAP learning, respectively, on the Artiﬁcial1 data set
with the data matrix size L = 100 and M = 300, and the true rank H∗= 20.
The noise variance was assumed to be known, i.e., it was set to σ2 = 1. We
performed iterative local search 10 times, starting from different initial points,
and each trial is plotted by a solid curve in the ﬁgures. The results computed
by the analytic-form solutions for EVB learning (Theorem 6.13), local-EPB
learning (Theorem 12.13), and local-EMAP learning (Theorem 12.12) were
plotted as dashed lines. We can observe that iterative local search for EPB
learning and EMAP learning tends to successfully ﬁnd the nontrivial local
solutions, although they are not global solutions.
We also conducted experiments on another artiﬁcial data set and benchmark
data sets. The results are summarized in Table 12.1. Artiﬁcial2 was created in
the same way as Artiﬁcial1, but with L = 400, M = 500, and H∗= 5. The
benchmark data sets were collected from the UCI repository (Asuncion and

12.3 Experimental Results
335
Newman, 2007), on which we set the noise variance under the assumption that
the signal to noise ratio is 0 db, following Mørup and Hansen (2009).
In the table, the estimated ranks by the analytic-form solution and by
iterative local search are shown. The percentages for iterative local search
indicate the frequencies over 10 trials. We observe the following: ﬁrst, iterative
local search tends to estimate the same rank as the analytic-form (local)
solution; and second, the estimated rank tends to be consistent among EVB
learning, local-EPB learning, and local-EMAP learning. Furthermore, on the
artiﬁcial data sets, where the true rank is known, the rank is correctly estimated
in most of the cases. Exceptions are Artiﬁcial2, where local-EPB learning
overestimates the rank, and Optical Digits and Satellite, where local-EMAP
learning estimates a smaller rank than the others. These phenomena can be
explained by the theoretical implications in Section 12.1.3: in Artiﬁcial2, the
ratio ξ = H∗/ min(L, M) = 5/400 between the true rank and the possible
largest rank is small, which means that most of the singular components consist
of noise. In such a case, local-EPB learning with its truncation threshold
lower than MPUL tends to retain components purely consisting of noise (see
Figure 12.6). In Optical Digits and Satellite, α (= 64/5620 for Optical Digits
and = 36/6435 for Satellite) is extremely small, and therefore local-EMAP
learning with its higher truncation threshold tends to discard more components
than the others, as Figure 12.6 implies.
12.3.2 Collaborative Filtering
Next we conducted experiments in the collaborative ﬁltering (CF) scenario,
where the observed matrix has missing entries to be predicted by the MF
model.
We generated an artiﬁcial (ArtiﬁcialCF) data set in the same way as the
fully observed case for L = 2,000, M = 5,000, H∗= 5, and then masked
99% of the entries as missing values. We applied EVB learning, local-EPB
learning, and local-EMAP learning to the MF model with missing entries, i.e.,
Eqs. (12.123), (12.2), and (12.3).4 Figure 12.11 shows the estimated rank and
the generalization error over iterations for 10 trials, where the generalization
error is deﬁned as GE = ∥PΛ′(V) −PΛ′(BA
⊤)∥2
Fro/(# (Λ′) σ2) for Λ′ being the
set of test indices.
4 Here we solve the EVB learning problem, the EPB learning problem, and the EMAP learning
problem, respectively, by the standard iterative algorithms. However, we refer to the last two
methods as local-EPB learning and local-EMAP learning, since we expect the local search
algorithm to ﬁnd not the global null solution but the nontrivial local solution.

336
12 MAP and Partially Bayesian Learning
0
500
1,000
1,500
2,000
2,500
Iteration
0
5
10
15
20
EVB
Local-EPB
Local-EMAP
(a) Estimated rank
0
500
1,000
1,500
2,000
2,500
Iteration
4
4.5
5
5.5
6
EVB
Local-EPB
Local-EMAP
(b) Generalization error
Figure 12.11 CF result on ArtiﬁcialCF (L = 2,000, M = 5,000, H∗= 5 with 99%
missing ratio).
0
500
1,000
1,500
2,000
2,500
Iteration
0
5
10
15
20
EVB
Local-EPB
Local-EMAP
(a) Estimated rank
0
500
1,000
1,500
2,000
2,500
Iteration
0.5
0.55
0.6
0.65
EVB
Local-EPB
Local-EMAP
(b) Generalization error
Figure 12.12 CF result on MovieLens (L = 943, M = 1,682 with 99% missing
ratio).
We also conducted an experiment on the MovieLens data sets (with L = 943,
M = 1,682).5 We randomly divided the observed entries into training entries
and test entries, so that 99% of the entries are missing in the training phase. The
test entries are used to evaluate the generalization error. Figure 12.12 shows the
result in the same format as Figure 12.11.
We see that, on both data sets, local-EMAP learning tends to estimate a
similar rank to EVB learning, while local-EPB learning tends to estimate
a larger rank—a similar tendency to the fully observed case. In terms of
the generalization error, local-EPB learning performs comparably to EVB
learning, while local-EMAP learning performs slightly worse.
12.3.3 Tensor Factorization
Finally, we conducted experiments on TF. We created an artiﬁcial (Artiﬁ-
cialTF) data set, following Mørup and Hansen (2009): we drew a three-mode
5 www.grouplens.org/

12.3 Experimental Results
337
Table 12.2 Estimated rank (effective size of core tensor) in TF experiments.
Data set
M
H∗

HEVB

Hlocal-EPB

Hlocal-EMAP

HARD-Tucker
ArtiﬁcialTF
(30, 40, 50)
(3, 4, 5)
(3, 4, 5):
(3, 4, 5):
(3, 4, 5):
(3, 4, 5):
100%
100%
90%
100%
(3, 7, 5):
10%
FIA
(12, 100, 89)
(3, 6, 4)
(3, 5, 3):
(3, 5, 3):
(3, 5, 2):
(3, 4, 2):
100%
100%
50%
70%
(4, 5, 2):
(3, 5, 2):
20%
10%
(5, 4, 2):
(3, 7, 2):
10%
10%
(4, 4, 2):
(10, 4, 3):
10%
10%
(8, 5, 2):
10%
random tensor of the size (M(1), M(1), M(1)) = (30, 40, 50) with the signal
components (H(1)∗, H(2)∗, H(3)∗) = (3, 4, 5). The noise is added so that the
signal-to-noise ratio is 0 db. We also used the Flow Injection Analysis (FIA)
data set.6 Table 12.2 shows the estimated rank with frequencies over 10 trials.
Here we also show the results by ARD Tucker with the ridge prior (Mørup
and Hansen, 2009), performed with the code provided by the authors. Local-
EMAP learning and ARD Tucker minimize exactly the same objective, and
the slightly different results come from the differences in the local search
algorithm (standard iterative vs. gradient descent) and in the initialization
scheme.
We generally observe that all learning methods provide reasonable results,
although local-EMAP learning, as well as ARD Tucker, is less stable than the
others.
6 www.models.kvl.dk/datasets


Part IV
Asymptotic Theory


13
Asymptotic Learning Theory
Part IV is dedicated to asymptotic theory of variational Bayesian (VB) learn-
ing. In this part, “asymptotic limit” always means the limit when the number
N of training samples goes to inﬁnity. The main goal of asymptotic learning
theory is to clarify the behavior of some statistics, e.g., the generalization
error, the training error, and the Bayes free energy, which indicate how fast a
learning machine can be trained as a function of the number of training samples
and how the trained machine is biased to the training samples by overﬁtting.
This provides the mathematical foundation of information criteria for model
selection—a task to choose the degree of freedom of a statistical model based
on observed training data. We can also evaluate the approximation accuracy of
VB learning to full Bayesian learning in terms of the free energy, i.e., the gap
between the VB free energy and the Bayes free energy, which corresponds to
the tightness of the evidence lower-bound (ELBO) (see Section 2.1.1). In this
ﬁrst chapter of Part IV, we give an overview of asymptotic learning theory as
the background for the subsequent chapters.
13.1 Statistical Learning Machines
A statistical learning machine consists of two fundamental components,
a statistical model and a learning algorithm (Figure 13.1). The statistical
model is denoted by a probabilistic distribution depending on some unknown
parameters, and the learning algorithm estimates the unknown parameters from
observed training samples. Before introducing asymptotic learning theory, we
categorize statistical learning machines based on the model and the learning
algorithm.
341

342
13 Asymptotic Learning Theory
Learning machine
Statistical model
Linear models
Neural networks
Mixture models
Learning algorithm
ML estimation
MAP learning
Bayesian learning
Figure 13.1 A statistical learning machine consists of a statistical model and a
learning algorithm.
Regular
Linear models
Statistical models
Singular
Neural networks
Mixture models
Bayesian networks
Hidden Markov models
Figure 13.2 Statistical models are classiﬁed into regular models and singular
models.
13.1.1 Statistical Models—Regular and Singular
We classify the statistical models into two classes, the regular models and the
singular models (Figure 13.2). The regular models are identiﬁable (Deﬁnition
7.4 in Section 7.3.1), i.e.,
p(x|w1) = p(x|w2) ⇐⇒w1 = w2
for any w1, w2 ∈W,
(13.1)
and do not have singularities in the parameter space, i.e., the Fisher
information
SD
+ ∋F(w) =
 ∂log p(x|w)
∂w
∂log p(x|w)
∂w
⊤
p(x|w)dx
(13.2)
is nonsingular (or full-rank) for any w ∈W.
With a few additional assumptions, the regular models were analyzed
under the regularity conditions (Section 13.4.1), which lead to the asymptotic
normality of the distribution of the maximum likelihood (ML) estimator,
and the asymptotic normality of the Bayes posterior distribution (Cramer,

13.1 Statistical Learning Machines
343
1949; Sakamoto et al., 1986; van der Vaart, 1998). Based on those asymp-
totic normalities, a uniﬁed theory was established, clarifying the asymptotic
behavior of generalization properties, which are common over all regular
models, and over all reasonable learning algorithms, including ML learning,
maximum a posteriori (MAP) learning, and Bayesian learning, as will be seen
in Section 13.4.
On the other hand, analyzing singular models requires speciﬁc techniques
for different models and different learning algorithms, and it was revealed that
the asymptotic behavior of generalization properties depends on the model
and the algorithm (Hartigan, 1985; Bickel and Chernoff, 1993; Takemura and
Kuriki, 1997; Kuriki and Takemura, 2001; Amari et al., 2002; Hagiwara, 2002;
Fukumizu, 2003; Watanabe, 2009). This is because the true parameter is at a
singular point when the model size is larger than necessary to express the true
distribution, and, in such cases, singularities affect the distribution of the ML
estimator, as well as the Bayes posterior distribution even in the asymptotic
limit. Consequently, the asymptotic normality, on which the regular learning
theory relies, does not hold in singular models.
13.1.2 Learning Algorithms—Point Estimation
and Bayesian Learning
When analyzing singular models, we also classify learning algorithms into
two classes, point estimation and Bayesian learning (Figure 13.3). The point
estimation methods, including ML learning and MAP learning, choose a single
model (i.e., a single point in the parameter space) that maximizes a certain
criterion such as the likelihood or the posterior probability, while Bayesian
learning methods use an ensemble of models over the posterior distribution or
its approximation.
Learning algorithms
Point estimation
ML learning
MAP learning
Bayesian
Bayesian learning
VB learning
PB learning
EP
Figure 13.3 Learning algorithms are classiﬁed into point-estimation and Bayesian
learning.

344
13 Asymptotic Learning Theory
Unlike in the regular models, point estimation and Bayesian learning
show different learning behavior in singular models. This is because how
singularities affect the learning property depends on the learning methods. For
example, as discussed in Chapter 7, strong nonuniformity of the density of
the volume element leads to model-induced regularization (MIR) in Bayesian
learning, while it does not affect point-estimation methods.
13.2 Basic Tools for Asymptotic Analysis
Here we introduce basic tools for asymptotic analysis.
13.2.1 Central Limit Theorem
Asymptotic learning theory heavily relies on the central limit theorem.
Theorem 13.1
(Central limit theorem) (van der Vaart, 1998) Let
{x(1),. . . , x(N)} be N i.i.d. samples from an arbitrary distribution with ﬁnite
mean μ ∈RD and ﬁnite covariance Σ ∈SD
++, and let x = N−1 N
n=1 x(n) be their
average. Then, the distribution of z =
√
N(x −μ) converges to the Gaussian
distribution with mean zero and covariance Σ,1 i.e.,
p (z) →GaussD (z; 0, Σ)
as
N →∞.
(13.3)
Intuitively, Eq. (13.3) can be interpreted as
p $x% →GaussD

x; μ, N−1Σ
 
as
N →∞,
(13.4)
implying that the distribution of the average x of i.i.d. random variables
converges to the Gaussian distribution with mean μ and covariance N−1Σ.
The central limit theorem implies the (weak) law of large numbers,2 i.e., for
any ε > 0,
lim
N→∞Prob $∥x −μ∥> ε% = 0.
(13.5)
13.2.2 Asymptotic Notation
We use the following asymptotic notation, a.k.a, Bachmann–Landau notation,
to express the order of functions when the number N of samples goes to
inﬁnity:
1 We consider weak topology in the space of distributions, i.e., p(x) is identiﬁed with r(x) if
⟨f(x)⟩p(x) = ⟨f(x)⟩r(x) for any bounded continuous function f(x). Convergence (of a random
variable x) in this sense is called convergence in distribution, weak convergence, or convergence
in law, and denoted as p(x) →r(x) or x  r(x) (van der Vaart, 1998).
2 Convergence x →μ in the sense that limN→∞Prob (∥x −μ∥> ε) = 0, ∀ε > 0 is called
convergence in probability.

13.2 Basic Tools for Asymptotic Analysis
345
O( f(N)) : A function such that lim sup
N→∞
|O( f(N))/f(N)| < ∞,
o( f(N)) : A function such that lim
N→∞o( f(N))/f(N) = 0,
Ω( f(N)) : A function such that lim inf
N→∞|Ω( f(N))/f(N)| > 0,
ω( f(N)) : A function such that lim
N→∞|ω( f(N))/f(N)| = ∞,
Θ( f(N)) : A function such that lim sup
N→∞
|Θ( f(N))/f(N)| < ∞
and lim inf
N→∞|Θ( f(N))/f(N)| > 0.
Intuitively, as a function of N, O( f(N)) is a function of no greater order than
f(N), o( f(N)) is a function of less order than f(N), Ω( f(N)) is a function
of no less order than f(N), ω( f(N)) is a function of greater order than f(N),
and Θ( f(N)) is a function of the same order as f(N). One thing we need to
be careful of is that the upper-bounding notations, O and o, preserve after
addition and subtraction, while lower-bounding notations, Ω and ω, as well as
the both-sides-bounding notation Θ, do not necessarily preserve. For example,
if g1(N) = Θ( f(N)) and g2(N) = Θ( f(N)) then g1(N) + g2(N) = O( f(N)),
while it can happen that g1(N) + g2(N)  Θ( f(N)) since the leading terms
of g1(N) and g2(N) can coincide with each other with opposite signs and be
canceled out.
For random variables, we use their probabilistic versions, Op, op, Ωp, ωp,
and Θp, for which the corresponding conditions hold in probability. For
example, for i.i.d. samples {x(n)}N
n=1 from Gauss1(x; 0, 12), we can say that
x(n) = Θp(1),
x = 1
N
N

n=1
x(n) = Θp(N−1/2),
x2 = 1
N
N

n=1

x(n) 2 = 1 + Θp(N−1/2).
Note that the second and the third equations are consequences from the central
limit theorem (Theorem 13.1) applied to the samples {x(n)} that follow the
Gaussian distribution, and to the samples {(x(n))2} that follow the chi-squared
distribution, respectively.
In this book, we express asymptotic approximation mostly by using asymp-
totic notation. To this end, we sometimes need to translate convergence of
a random variable into an equation with asymptotic notation. Let x be a
random variable depending on N, r(x) be a distribution with ﬁnite mean and

346
13 Asymptotic Learning Theory
covariance, and f(x) be an arbitrary bounded continuous function. Then the
following hold:
• If p(x) →r(x), i.e., the distribution of x converges to r(x), then
x = Op(1)
and
⟨f(x)⟩p(x) = ⟨f(x)⟩r(x) (1 + o(1)) .
• If limN→∞Prob (∥x −y∥> ε) = 0 for any ε > 0, then
x = y + op(1).
For example, the central limit theorem (13.3) implies that
x = μ + Op(N−1/2),

(x −μ)(x −μ)⊤
p(x) = N−1Σ + o(N−1),
while the law of large numbers (13.5) implies that
x = μ + op(1).
13.3 Target Quantities
Here we introduce target quantities to be analyzed in asymptotic learning
theory.
13.3.1 Generalization Error and Training Error
Consider a statistical model p(x|w), where x ∈RM is an observed random
variable and w ∈RD is a parameter to be estimated. Let X = (x(1),. . . , x(N))⊤∈
RN×M be N i.i.d. training samples taken from the true distribution q(x). We
assume realizability—the true distribution can be exactly expressed by the
statistical model, i.e., ∃w∗s.t. q(x) = p(x|w∗), where w∗is called the true
parameter.
Learning algorithms estimate the parameter value w or its posterior distri-
bution given the training data D = X, and provide the predictive distribution
p(x|X) for a new sample x. For example, ML learning provides the predictive
distribution given by
pML(x|X) = p(x|wML),
(13.6)
where
wML = argmax
w
p(X|w) = argmax
w
⎛⎜⎜⎜⎜⎜⎝
N

n=1
p(x(n)|w)
⎞⎟⎟⎟⎟⎟⎠
(13.7)

13.3 Target Quantities
347
is the ML estimator, while Bayesian learning provides the predictive distribu-
tion given by
pBayes(x|X) = ⟨p(x|w)⟩p(w|X) =

p(x|w)p(w|X)dw,
(13.8)
where
p(w|X) = p(X|w)p(w)
p(X)
=
p(X|w)p(w)

p(X|w)p(w)dw
(13.9)
is the posterior distribution (see Section 1.1).
The generalization error, a criterion of generalization performance, is
deﬁned as the Kullback–Leibler (KL) divergence of the predictive distribution
from the true distribution:
GE(X) =

q(x) log q(x)
p(x|X)dx.
(13.10)
Its empirical variant,
TE(X) = 1
N
N

n=1
log q(x(n))
p(x(n)|X),
(13.11)
is called the training error, which is often used as an estimator of the
generalization error. Note that, for the ML predictive distribution (13.6),
−N · TEML(X) =
N

n=1
log p(x(n)|wML)
q(x(n))
(13.12)
corresponds to the log-likelihood ratio, an important statistic for statistical test,
when the null hypothesis is true.
The generalization error (13.10) and the training error (13.11) are random
variables that depend on realization of the training data X. Taking the average
over the distribution of training samples, we deﬁne deterministic quantities,
GE(N) = ⟨GE(X)⟩q(X) ,
(13.13)
TE(N) = ⟨TE(X)⟩q(X) ,
(13.14)
which are called the average generalization error and the average training
error, respectively. Here ⟨·⟩q(X) denotes the expectation value over the distri-
bution of N training samples. The average generalization error and the average
training error are scalar functions of the number N of samples, and represent
generalization performance of a learning machine consisting of a statistical
model and a learning algorithm. The optimality of Bayesian learning is proven
in terms of the average generalization error (see Appendix D).

348
13 Asymptotic Learning Theory
If a learning algorithm can successfully estimate the true parameter w∗
with reasonably small error, the average generalization error and the average
training error converge to zero with the rate Θ(N−1) in the asymptotic limit.3
One of the main goals of asymptotic learning theory is to identify or bound
the coefﬁcients of their leading terms, i.e., λ and ν in the following asymptotic
expansions:
GE(N) = λN−1 + o(N−1),
(13.15)
TE(N) = νN−1 + o(N−1).
(13.16)
We call λ and ν the generalization coefﬁcient and the training coefﬁcient,
respectively.
13.3.2 Bayes Free Energy
The marginal likelihood (deﬁned by Eq. (1.6) in Chapter 1),
p(X) =

p(X|w)p(w)dw =

p(w)
N

n=1
p(x(n)|w)dw,
(13.17)
is also an important quantity in Bayesian learning. As explained in Section
1.1.3, the marginal likelihood can be regarded as the likelihood of an ensemble
of models—the set of model distributions with the parameters subject to the
prior distribution. Following the concept of the “likelihood” in statistics, we
can say that the ensemble of models giving the highest marginal likelihood
is most likely. Therefore, we can perform model selection by maximizing the
marginal likelihood (Efron and Morris, 1973; Schwarz, 1978; Akaike, 1980;
MacKay, 1992; Watanabe, 2009). Maximizing the marginal likelihood (13.17)
amounts to minimizing the Bayes free energy, deﬁned by Eq. (1.60):
FBayes(X) = −log p(X).
(13.18)
The Bayes free energy is a random variable depending on the training
samples X, and is of the order of Θp(N). However, the dominating part comes
from the entropy of the true distribution, and does not depend on the statistical
model nor the learning algorithm. In statistical learning theory, we therefore
analyze the behavior of the relative Bayes free energy,
FBayes(X) = log q(X)
p(X) = FBayes(X) −NS N(X),
(13.19)
3 This holds if the estimator achieves a mean squared error in the same order as the Cram´er–Rao
lower-bound, i.e.,

∥w −w∗∥2
q(X) ≥N−1tr

F−1(w∗)
 
, where F is the Fisher information (13.2)
at w∗. The Cram´er–Rao lower-bound holds for any unbiased estimator under the regularity
conditions.

13.3 Target Quantities
349
where
S N(X) = −1
N
N

n=1
log q(x(n))
(13.20)
is the empirical entropy. The negative of the relative Bayes free energy,
−FBayes(X) = log

p(w) N
n=1 p(x(n)|w)dw
N
n=1 q(x(n))
,
(13.21)
can be seen as an ensemble version of the log-likelihood ratio—the logarithm
of the ratio between the marginal likelihood (alternative hypothesis) and the
true likelihood (null hypothesis).
When the prior p(w) is positive around the true parameter w∗, the relative
Bayes free energy (13.19) is known to be of the order of Θ(log N) and can be
asymptotically expanded as follows:
FBayes(X) = λ′Bayes log N + op(log N),
(13.22)
where the coefﬁcient of the leading term λ′Bayes is called the Bayes free energy
coefﬁcient. Note that, although the relative Bayes free energy is a random
variable depending on realization of the training data X, the leading term in
Eq. (13.22) is deterministic.
Let us deﬁne the average relative Bayes free energy over the distribution of
training samples:
F
Bayes(N) =
FBayes(X)

q(X) =
/
log
N
n=1 q(x(n))

p(w) N
n=1 p(x(n)|w)dw
0
q(X)
.
(13.23)
An interesting and useful relation can be found between the average Bayes
generalization error and the average relative Bayes free energy (Levin et al.,
1990):
GE
Bayes(N) =

q(x) log
q(x)
pBayes(x|X)dx

q(X)
=
*
q(x) log
q(x)

p(x|w)p(w|X)dwdx
+
q(X)
=
*
q(x) log
q(x)

p(x|w)p(X|w)p(w)dwdx
+
q(X)
−
*
q(x) log
1

p(X|w)p(w)dwdx
+
q(X)
=
*
log
q(x)q(X)

p(x|w)p(X|w)p(w)dw
+
q(x)q(X) −
*
log
q(X)

p(X|w)p(w)dw
+
q(X)
= F
Bayes(N + 1) −F
Bayes(N).
(13.24)

350
13 Asymptotic Learning Theory
The relation (13.24) combined with the asymptotic expansions, Eqs. (13.15)
and (13.22), implies that the Bayes generalization coefﬁcient and the Bayes
free energy coefﬁcient coincide with each other, i.e.,
λ′Bayes = λBayes.
(13.25)
Importantly, this relation holds for any statistical model, regardless of being
regular or singular.
13.3.3 Target Quantities under Conditional Modeling
Many statistical models are for the regression or classiﬁcation setting, where
the model distribution p(y|x, w) is the distribution of an output y ∈RL
conditional on an input x ∈RM and an unknown parameter w ∈RD. The
input is assumed to be given for all samples including the future test samples.
Let D = {(x(1), y(1)),. . . , (x(N), y(N))} be N i.i.d. training samples drawn from
the true joint distribution q(x, y) = q(y|x)q(x). As noted in Example 1.2 in
Chapter 1, we can proceed with most computations without knowing the input
distribution q(x).
Let X = (x(1),. . . , x(N))⊤∈RN×M and Y = (y(1),. . . , y(N))⊤∈RN×L
separately summarize the inputs and the outputs in the training data. The
predictive distribution, given as a conditional distribution on a new input x
as well as the whole training samples D = (X, Y), can usually be computed
without any information on q(X). For example, the ML predictive distribution
is given as
pML(y|x, D) = p(y|x, wML),
(13.26)
where
wML = argmax
w
p(Y|X, w) · q(X) = argmax
w
⎛⎜⎜⎜⎜⎜⎝
N

n=1
p(y(n)|x(n), w)
⎞⎟⎟⎟⎟⎟⎠
(13.27)
is the ML estimator, while the Bayes predictive distribution is given as
pBayes(y|x, D) = ⟨p(y|x, w)⟩p(w|X,Y) =

p(y|x, w)p(w|X, Y)dw,
(13.28)
where
p(w|X, Y) =
p(Y|X, w)p(w) · q(X)

p(Y|X, w)p(w)dw · q(X)
=
p(Y|X, w)p(w)

p(Y|X, w)p(w)dw
(13.29)
is the Bayes posterior distribution. Here (x, y) is a new input–output sample
pair, assumed to be drawn from the true distribution q(y|x)q(x).

13.4 Asymptotic Learning Theory for Regular Models
351
The generalization error (13.10), the training error (13.11), and the relative
Bayes free energy (13.19) can be expressed as follows:
GE(D) =
/
log
q(y|x)q(x)
p(y|x, D)q(x)
0
q(y|x)q(x)
=
/
log
q(y|x)
p(y|x, D)
0
q(y|x)q(x)
, (13.30)
TE(D) = 1
N
N

n=1
log
q(y(n)|x(n))q(x(n))
p(y(n)|x(n), D)q(x(n)) = 1
N
N

n=1
log
q(y(n)|x(n))
p(y(n)|x(n), D),
(13.31)
FBayes(D) = log q(Y|X)q(X)
p(Y|X)q(X) = FBayes(Y|X) −NS N(Y|X),
(13.32)
where
FBayes(Y|X) = log

p(w)
N

n=1
p(y(n)|x(n), w)dw,
(13.33)
S N(Y|X) = −1
N
N

n=1
log q(y(n)|x(n)).
(13.34)
We can see that the input distribution q(x) cancels out in most of the preceding
equations, and therefore Eqs. (13.30) through (13.34) can be computed without
considering q(x). Note that in Eq. (13.30), q(x) remains the distribution over
which the expectation is taken. However, it is necessary only formally, and the
expectation value does not depend on q(x) (as long as the regularity conditions
hold). The same applies to the average generalization error (13.13), the average
training error (13.14), and the average relative Bayes free energy (13.23),
where the expectation ⟨·⟩q(Y|X)q(X) over the distribution of the training samples
is taken.
13.4 Asymptotic Learning Theory for Regular Models
In this section, we introduce the regular learning theory, which generally holds
under the regularity conditions.
13.4.1 Regularity Conditions
The regularity conditions are deﬁned for the statistical model p(x|w) parame-
terized by a ﬁnite-dimensional parameter vector w ∈W ⊆RD, and the true
distribution q(x). We include conditions for the prior distribution p(w), which
are necessary for analyzing MAP learning and Bayesian learning. There are
variations, and we here introduce a (rough) simple set.

352
13 Asymptotic Learning Theory
(i) The statistical model p(x|w) is differentiable (as many times as
necessary) with respect to the parameter w ∈W for any x, and the
differential operator and the integral operator are commutable.
(ii) The statistical model p(x|w) is identiﬁable, i.e., Eq. (13.1) holds, and the
Fisher information (13.2) is nonsingular (full-rank) at any w ∈W.
(iii) The support of p(x|w), i.e., {x ∈X; p(x|w) > 0}, is common for all
w ∈W.
(iv) The true distribution is realizable by the statistical model, i.e.,
∃w∗s.t. q(x) = p(x|w∗), and the true parameter w∗is an interior point of
the domain W.
(v) The prior p(w) is twice differentiable and bounded as 0 < p(w) < ∞at
any w ∈W.
Note that the ﬁrst three conditions are on the model distribution p(x|w), the
fourth is on the true distribution q(x), and the ﬁfth is on the prior distribution
p(w).
An important consequence of the regularity conditions is that the log-
likelihood can be Taylor-expanded about any w ∈W:
log p(x|w) = log p(x|w) + (w −w)⊤∂log p(x|w)
∂w
w=w
+ 1
2(w −w)⊤∂2 log p(x|w)
∂w∂w⊤
w=w(w −w) + O(∥w −w∥3). (13.35)
13.4.2 Consistency and Asymptotic Normality
We ﬁrst show consistency and asymptotic normality, which hold in ML
learning, MAP learning, and Bayesian learning.
Consistency of ML Estimator
The ML estimator is deﬁned by
wML = argmax
w
log
⎛⎜⎜⎜⎜⎜⎝
N

n=1
p(x(n)|w)
⎞⎟⎟⎟⎟⎟⎠= argmax
w
LN(w),
(13.36)
where
LN(w) = 1
N
N

n=1
log p(x(n)|w).
(13.37)
By the law of large numbers (13.5), it holds that
LN(w) = L∗(w) + op(1),
(13.38)

13.4 Asymptotic Learning Theory for Regular Models
353
where
L∗(w) = log p(x|w)
p(x|w∗) .
(13.39)
Identiﬁability of the statistical model guarantees that
w∗= argmax
w
L∗(w)
(13.40)
is the unique maximizer. Eqs. (13.36), (13.38), and (13.40), imply the consis-
tency of the ML estimator, i.e.,
wML = w∗+ op(1).
(13.41)
Asymptotic Normality of the ML Estimator
Since the gradient ∂LN(w)/∂w is differentiable, the mean value theorem4
guarantees that there exists ´w ∈[min(wML, w∗), max(wML, w∗)]D (where min(·)
and max(·) operate elementwise) such that
∂LN(w)
∂w
w=wML = ∂LN(w)
∂w
w=w∗+ ∂2LN(w)
∂w∂w⊤
w= ´w(wML −w∗).
(13.42)
By the deﬁnition (13.36) of the ML estimator and the differentiability of
LN(w), the left-hand side of Eq. (13.42) is equal to zero, i.e.,
∂LN(w)
∂w
w=wML = 0.
(13.43)
The ﬁrst term in the right-hand side of Eq. (13.42) can be written as
∂LN(w)
∂w
w=w∗= 1
N
N

n=1
∂log p(x(n)|w)
∂w
w=w∗
.
(13.44)
Since Eq. (13.40) and the differentiability of L∗(w) imply that
∂L∗(w)
∂w
w=w∗=
/∂log p(x|w)
∂w

w=w∗
0
p(x|w∗)
= 0,
(13.45)
the right-hand side of Eq. (13.44) is the average over N i.i.d. samples of the
random variable
∂log p(x(n)|w)
∂w
w=w∗
,
4 The mean value theorem states that, for a differentiable function f : [a, b] →R,
∃c ∈[a, b]
s.t.
d f(x)
dx
x=c = f(b)−f(a)
b−a
.

354
13 Asymptotic Learning Theory
which follows a distribution with zero mean (Eq. (13.45)) and the covariance
given by the Fisher information (13.2) at w = w∗, i.e.,
F(w∗) =
/∂log p(x|w)
∂w

w=w∗
∂log p(x|w)
∂w

⊤
w=w∗
0
p(x|w∗)
.
(13.46)
Therefore, according to the central limit theorem (Theorem 13.1), the distribu-
tion of the ﬁrst term in the right-hand side of Eq. (13.42) converges to
p
∂LN(w)
∂w

w=w∗

→GaussD
∂LN(w)
∂w

w=w∗; 0, N−1F(w∗)

.
(13.47)
The coefﬁcient of the second term in the right-hand side of Eq. (13.42)
satisﬁes
∂2LN(w)
∂w∂w
w= ´w
= ∂2L∗(w)
∂w∂w⊤
w=w∗
+ op(1),
(13.48)
because of the law of large numbers and the consistency of the ML estimator,
i.e., [min(wML, w∗), max(wML, w∗)] ∋´w →w∗since wML →w∗. Furthermore,
the following relation holds under the regularity conditions (see Appendix
B.2):
∂2L∗(w)
∂w∂w⊤
w=w∗
=
/∂2 log p(x|w)
∂w∂w⊤
w=w∗
0
p(x|w∗)
= −F(w∗).
(13.49)
Substituting Eqs. (13.43), (13.48), and (13.49) into Eq. (13.42) gives

F(w∗) + op(1)
 
(wML −w∗) = ∂LN(w)
∂w
w=w∗.
(13.50)
Since the Fisher information is assumed to be invertible, Eq. (13.47) leads to
the following theorem:
Theorem 13.2
(Asymptotic normality of ML estimator) Under the regularity
conditions, the distribution of vML =
√
N(wML −w∗) converges to
p

vML 
→GaussD

vML; 0, F−1(w∗)
 
as
N →∞.
(13.51)
Theorem 13.2 implies that
wML = w∗+ Op(N−1/2).
(13.52)
13.4.3 Asymptotic Normality of the Bayes Posterior
The Bayes posterior can be written as follows:
p(w|X) =
exp $NLN(w) + log p(w)%

exp $NLN(w) + log p(w)% dw
.
(13.53)

13.4 Asymptotic Learning Theory for Regular Models
355
In the asymptotic limit, the factor exp(NLN(w)) dominates the numerator, and
the probability mass concentrates around the peak of LN(w)—the ML estimator
wML. The Taylor expansion of LN(w) about wML gives
LN(w) ≈LN(wML) + (w −wML)⊤∂LN(w)
∂w
w=wML
+ 1
2(w −wML)⊤∂2LN(w)
∂w∂w⊤
w=wML (w −wML)
≈LN(wML) −1
2(w −wML)⊤F(w∗)(w −wML),
(13.54)
where we used Eq. (13.43) and
∂2LN(w)
∂w∂w⊤
w=wML = −F(w∗) + op(1),
(13.55)
which is implied by the law of large numbers and the consistency of the
ML estimator. Eqs. (13.53) and (13.54) imply that the Bayes posterior can
be approximated by Gaussian in the asymptotic limit:
p(w|X) ≈GaussD

w; wML, N−1F−1(w∗)
 
.
The following theorem was derived with more accurate discussion.
Theorem 13.3
(Asymptotic normality of the Bayes posterior) (van der
Vaart, 1998) Under the regularity conditions, the (rescaled) Bayes posterior
distribution p (v|X) where v =
√
N(w −w∗) converges to
p (v|X) →GaussD

v; vML, F−1(w∗)
 
as
N →∞,
(13.56)
where vML =
√
N(wML −w∗).
Theorem 13.3 implies that
wMAP = wML + op(N−1/2),
(13.57)
wBayes = ⟨w⟩p(w|X) = wML + op(N−1/2),
(13.58)
which prove the consistency of the MAP estimator and the Bayesian estimator.
13.4.4 Generalization Properties
Now we analyze the generalization error and the training error in ML learning,
MAP learning, and Bayesian learning, as well as the Bayes free energy.
After that, we introduce information criteria for model selection, which were
developed based on the asymptotic behavior of those quantities.

356
13 Asymptotic Learning Theory
13.4.5 ML Learning
The generalization error of ML learning can be written as
GEML
Regular(X) =
/
log p(x|w∗)
p(x|wML)
0
p(x|w∗)
= L∗(w∗) −L∗(wML)
(13.59)
with L∗(w) deﬁned by Eq. (13.39). The Taylor expansion of the second term of
Eq. (13.59) about the true parameter w∗gives
L∗(wML) = L∗(w∗) + (wML −w∗)⊤∂L∗(w)
∂w
w=w∗
+ 1
2(wML −w∗)⊤∂2L∗(w)
∂w∂w⊤
w=w∗
(wML −w∗) + O(∥wML −w∗∥3)
= L∗(w∗) −1
2(wML −w∗)⊤F(w∗)(wML −w∗) + O(∥wML −w∗∥3),
(13.60)
where we used Eqs. (13.45) and (13.49) in the last equality. Substituting Eq.
(13.60) into Eq. (13.59) gives
GEML
Regular(X) = 1
2(wML −w∗)⊤F(w∗)(wML −w∗) + O(∥wML −w∗∥3). (13.61)
The asymptotic normality (Theorem 13.2) of the ML estimator implies that
√
NF
1
2 (w∗)(wML −w∗)  GaussD (0, ID) ,
(13.62)
and that
O(∥wML −w∗∥3) = Op(N−3/2).
(13.63)
Eq. (13.62) implies that the distribution of s = N(wML −w∗)⊤F(w∗)(wML −w∗)
converges to the chi-squared distribution with D degrees of freedom:5
p (s) →χ2 (s; D) ,
(13.64)
and therefore,
N

(wML −w∗)⊤F(w∗)(wML −w∗)

q(X) = D + o(1).
(13.65)
Eqs. (13.61), (13.63), and (13.65) lead to the following theorem:
5 The chi-squared distribution with D degrees of freedom is the distribution of the sum of the
squares of D i.i.d. samples drawn from Gauss1(0, 12). It is actually a special case of the Gamma
distribution, and it holds that χ2(x; D) = Gamma(x; D/2, 1/2). The mean and the variance are
equal to D and 2D, respectively.

13.4 Asymptotic Learning Theory for Regular Models
357
Theorem 13.4
The average generalization error of ML learning in the
regular models can be asymptotically expanded as
GE
ML
Regular(N) =

GEML
Regular(X)

q(X) = λML
RegularN−1 + o(N−1),
(13.66)
where the generalization coefﬁcient is given by
2λML
Regular = D.
(13.67)
Interestingly, the leading term of the generalization error only depends on
the parameter dimension or the degree of freedom of the statistical model.
The training error of ML learning can be analyzed in a similar fashion. It
can be written as
TEML
Regular(X) = N−1
N

n=1
log p(x(n)|w∗)
p(x(n)|wML)
= LN(w∗) −LN(wML)
(13.68)
with LN(w) deﬁned by Eq. (13.37). The Taylor expansion of the ﬁrst term of
Eq. (13.68) about the ML estimator wML gives
LN(w∗) = LN(wML) + (w∗−wML)⊤∂LN(w)
∂w
w=wML
+ 1
2(w∗−wML)⊤∂2LN(w)
∂w∂w⊤
w=wML (w∗−wML) + O(∥w∗−wML∥3)
= LN(wML) −1
2(w∗−wML)⊤
F(w∗) + op(1)
 
(w∗−wML)
+ O(∥w∗−wML∥3),
(13.69)
where we used Eqs. (13.43) and (13.55). Substituting Eq. (13.69) into Eq.
(13.68) and applying Eq. (13.52), we have
TEML
Regular(X) = −1
2(wML −w∗)⊤F(w∗)(wML −w∗) + op(N−1).
(13.70)
Thus, Eq. (13.70) together with Eq. (13.65) gives the following theorem:
Theorem 13.5
The average training error of ML learning in the regular
models can be asymptotically expanded as
TE
ML
Regular(N) =

TEML
Regular(X)

q(X) = νML
RegularN−1 + o(N−1),
(13.71)
where the training coefﬁcient is given by
2νML
Regular = −D.
(13.72)

358
13 Asymptotic Learning Theory
Comparing Theorems 13.4 and 13.5, we see that the generalization coefﬁ-
cient and the training coefﬁcient are antisymmetric with each other:
λML
Regular = −νML
Regular.
13.4.6 MAP Learning
We ﬁrst prove the following theorem:
Theorem 13.6
For any (point-) estimator such that
w = wML + op(N−1/2),
(13.73)
it holds that
GEw
Regular(X) =
/
log p(x|w∗)
p(x|w)
0
p(x|w∗)
= GEML
Regular(X) + op(N−1),
(13.74)
TEw
Regular(X) = N−1
N

n=1
log p(x(n)|w∗)
p(x(n)|w) = TEML
Regular(X) + op(N−1).
(13.75)
Proof
The generalization error of the estimator w can be written as
GEw
Regular(X) =
/
log p(x|w∗)
p(x|w)
0
p(x|w∗)
= L∗(w∗) −L∗(w)
= GEML
Regular(X) +

L∗(wML) −L∗(w)
 
,
(13.76)
where the second term can be expanded as
L∗(wML) −L∗(w) = −(w −wML)⊤∂L∗(w)
∂w
w=wML
−1
2(w −wML)⊤∂2L∗(w)
∂w∂w⊤
w=wML(w −wML) +O(∥w −wML∥3).
(13.77)
Eqs. (13.45) and (13.52) (with the differentiability of ∂L∗(w)/∂w) imply that
∂L∗(w)
∂w
w=wML = ∂L∗(w)
∂w
w=w∗+Op(N−1/2)
= Op(N−1/2),
with which Eqs. (13.73) and (13.77) lead to
L∗(wML) −L∗(w) = op(N−1).
Substituting the preceding into Eq. (13.76) gives Eq. (13.74).

13.4 Asymptotic Learning Theory for Regular Models
359
Similarly, the training error of the estimator w can be written as
TEw
Regular(X) = N−1
N

n=1
log p(x(n)|w∗)
p(x(n)|w)
= LN(w∗) −LN(w)
= TEML
Regular(X) +

LN(wML) −LN(w)
 
,
(13.78)
where the second term can be expanded as
LN(wML) −LN(w) = −(w −wML)⊤∂LN(w)
∂w
w=wML
−1
2(w −wML)⊤∂2LN(w)
∂w∂w⊤
w=wML(w −wML) +O(∥w −wML∥3).
(13.79)
Eqs. (13.43), (13.73), and (13.79) lead to
LN(wML) −LN(w) = op(N−1).
Substituting the preceding into Eq. (13.78) gives Eq. (13.75), which completes
the proof.
□
Since the MAP estimator satisﬁes the condition (13.73) of Theorem 13.6
(see Eq. (13.57)), we obtain the following corollaries:
Corollary 13.7
The average generalization error of MAP learning in the
regular models can be asymptotically expanded as
GE
MAP
Regular(N) =

GEMAP
Regular(X)

q(X) = λMAP
RegularN−1 + o(N−1),
(13.80)
where the generalization coefﬁcient is given by
2λMAP
Regular = D.
(13.81)
Corollary 13.8
The average training error of MAP learning in the regular
models can be asymptotically expanded as
TE
MAP
Regular(N) =

TEMAP
Regular(X)

q(X) = νMAP
RegularN−1 + o(N−1),
(13.82)
where the training coefﬁcient is given by
2νMAP
Regular = −D.
(13.83)

360
13 Asymptotic Learning Theory
13.4.7 Bayesian Learning
Eq. (13.58) and Theorem 13.6 imply that the Bayesian estimator also gives
the same generalization and training coefﬁcients as ML learning, if the plug-
in predictive distribution p(x|wBayes), i.e., the model distribution with the
Bayesian parameter plugged-in, is used for prediction. We can show that
the proper Bayesian procedure with the predictive distribution p(x|X) =
⟨p(x|w)⟩p(w|X) also gives the same generalization and training coefﬁcients.
We ﬁrst prove the following theorem:
Theorem 13.9
Let r(w) be a (possibly approximate posterior) distribution of
the parameter, of which the mean and the covariance satisfy the following:
w = ⟨w⟩r(w) = w∗+ Op(N−1/2),
(13.84)
Σw =
*
w −⟨w⟩r(w)
 
w −⟨w⟩r(w)
 ⊤+
r(w) = Op(N−1).
(13.85)
Then the generalization error and the training error of the predictive distribu-
tion p(x|X) = ⟨p(x|w)⟩r(w) satisfy
GEr
Regular(X) =
/
log
p(x|w∗)
⟨p(x|w)⟩r(w)
0
p(x|w∗)
= GEw
Regular(X) + op(N−1),
(13.86)
TEr
Regular(X) = N−1
N

n=1
log
p(x(n)|w∗)
p(x(n)|w)
r(w)
=TEw
Regular(X) + op(N−1), (13.87)
where GEw
Regular(X) and TEw
Regular(X) are, respectively, the generalization error
and the training error of the point estimator w (deﬁned in Theorem 13.6).
Proof
The predictive distribution can be expressed as
⟨p(x|w)⟩r(w) = exp $log p(x|w)%
r(w)
=
*
exp

log p(x|w) + (w −w)⊤∂log p(x|w′)
∂w′
w′=w
+ 1
2(w −w)⊤∂2 log p(x|w′)
∂w′∂w′⊤
w′=w (w −w) + O

∥w −w∥3 "+
r(w)
= p(x|w) ·
*!
1 + (w −w)⊤∂log p(x|w′)
∂w′
w′=w
+ 1
2(w −w)⊤∂log p(x|w′)
∂w′
w′=w
∂log p(x|w′)
∂w′

⊤
w′=w (w −w)
+ 1
2(w −w)⊤∂2 log p(x|w′)
∂w′∂w′⊤
w′=w (w −w) + O

∥w −w∥3 "+
r(w) .
Here we ﬁrst expanded log p(x|w) about w, and then expanded the exponential
function (with exp(z) = 1 + z + z2/2 + O(z3)).

13.4 Asymptotic Learning Theory for Regular Models
361
Using the conditions (13.84) and (13.85) on r(w), we have
⟨p(x|w)⟩r(w) = p(x|w) ·

1 + 1
2tr
ΣwΦ(x; w)
 
+ Op

N−3/2  
,
(13.88)
where
Φ(x; w) = ∂log p(x|w)
∂w
w=w
∂log p(x|w)
∂w

⊤
w=w + ∂2 log p(x|w)
∂w′∂w′⊤
w=w.
(13.89)
Therefore,
*
log
⟨p(x|w)⟩r(w)
p(x|w)
+
p(x|w∗) =

log

1 + 1
2tr
ΣwΦ(x; w)
 
+ Op

N−3/2  
p(x|w∗)
= 1
2

tr
ΣwΦ(x; w)
 
p(x|w∗) + Op

N−3/2 
.
(13.90)
Here we expanded the logarithm function (with log(1 + z) = z + O(z2)), using
the condition (13.85) on the covariance, i.e., Σw = Op(N−1).
The condition (13.84) on the mean, i.e., w = w∗+ Op(N−1/2), implies that
Φ(x; w)
p(x|w∗) = ⟨Φ(x; w∗)⟩p(x|w∗) + Op(N−1/2)
= F(w∗) −F(w∗) + Op(N−1/2)
= Op(N−1/2),
(13.91)
where we used the deﬁnition of the Fisher information (13.46) and its
equivalent expression (13.49) (under the regularity conditions). Eqs. (13.90)
and (13.91) together with the condition (13.85) give
*
log
⟨p(x|w)⟩r(w)
p(x|w)
+
p(x|w∗) = Op(N−3/2),
which results in Eq. (13.86).
Similarly, by using the expression (13.88) of the predictive distribution, we
have
N−1
N

n=1
log ⟨p(x(n)|w)⟩r(w)
p(x(n)|w)
= N−1
N

n=1
log

1 + 1
2tr
ΣwΦ(x(n); w)
 
+ Op

N−3/2  
= 1
2N−1
N

n=1
tr
ΣwΦ(x(n); w)
 
+ Op

N−3/2 
. (13.92)
The law of large numbers (13.5) and Eq. (13.91) lead to
N−1
N

n=1
Φ(x(n); w) = Φ(x; w)
p(x|w∗) + op(1)
= op(1).

362
13 Asymptotic Learning Theory
Substituting the preceding and the condition (13.85) into Eq. (13.92) gives
N−1
N

n=1
log ⟨p(x(n)|w)⟩r(w)
p(x(n)|w)
= op

N−1 
,
which results in Eq. (13.87). This completes the proof.
□
The asymptotic normality of the Bayes posterior (Theorem 13.3), combined
with the asymptotic normality of the ML estimator (Theorem 13.2), guarantees
that the conditions (13.84) and (13.85) of Theorem 13.9 hold in Bayesian
learning, which leads to the following corollaries:
Corollary 13.10
The average generalization error of Bayesian learning in
the regular models can be asymptotically expanded as
GE
Bayes
Regular(N) =

GEBayes
Regular(X)

q(X) = λBayes
RegularN−1 + o(N−1),
(13.93)
where the generalization coefﬁcient is given by
2λBayes
Regular = D.
(13.94)
Corollary 13.11
The average training error of Bayesian learning in the
regular models can be asymptotically expanded as
TE
Bayes
Regular(N) =

TEBayes
Regular(X)

q(X) = νBayes
RegularN−1 + o(N−1),
(13.95)
where the training coefﬁcient is given by
2νBayes
Regular = −D.
(13.96)
Asymptotic behavior of the Bayes free energy (13.18) was also analyzed
(Schwarz, 1978; Watanabe, 2009). The Bayes free energy can be written as
FBayes(X) = −log p(X)
= −log

p(w)
N

n=1
p(x(n)|w)dw
= −log

exp $NLN(w) + log p(w)% dw,
where the factor exp (NLN(w)) dominates in the asymptotic limit. By using the
Taylor expansion
LN(w) = LN(wML) + (w −wML)⊤∂LN(w)
∂w
w=wML

13.4 Asymptotic Learning Theory for Regular Models
363
+1
2(w −wML)⊤∂2LN(w)
∂w∂w⊤
w=wML (w −wML) + O

∥w −wML∥3 
= LN(wML)−1
2(w −wML)⊤
F(w∗) + op(1)
 
(w −wML)+O

∥w −wML∥3 
,
we can approximate the Bayes free energy as follows:
FBayes(X) ≈−NLN(wML) −log

exp

−N
2 (w −wML)⊤F(w∗)(w −wML)
+ log p(w)

dw
= −NLN(wML) −log

exp

−1
2v⊤F(w∗)v
+ log p(wML + N−1/2v)
 dv
ND/2
= −NLN(wML) + D
2 log N + Op(1).
(13.97)
where v =
√
N(w −wML) is a rescaled parameter, on which the integration was
performed with dv = ND/2dw.
Therefore, the relative Bayes free energy (13.19) can be written as
FBayes(X) = FBayes(X) + NLN(w∗)
≈D
2 log N + N

LN(w∗) −LN(wML)
 
+ Op(1).
(13.98)
Here we used S N(X) = −LN(w∗), which can be conﬁrmed by their deﬁnitions
(13.20) and (13.37). The second term in Eq. (13.98) is of the order of Op(1),
because Eqs. (13.68), (13.70), and (13.52) imply that
LN(w∗) −LN(wML) = TEML
Regular(X) = Op(N−1).
The following theorem was obtained with more rigorous discussion.
Theorem 13.12
(Watanabe, 2009) The relative Bayes free energy for the
regular models can be asymptotically expanded as
FBayes
Regular(X) = FBayes(X) −NS N(X) = λ′Bayes
Regular log N + Op(1),
(13.99)
where the Bayes free energy coefﬁcient is given by
2λ′Bayes
Regular = D.
(13.100)
Note that Corollary 13.10 and Theorem 13.12 are consistent with Eq.
(13.25), which holds for any statistical model.

364
13 Asymptotic Learning Theory
13.4.8 Information Criteria
We have seen that the leading terms of the generalization error, the training
error, and the relative Bayes free energy are proportional to the parameter
dimension. Those results imply that how much a regular statistical model
overﬁts training data mainly depends on the degrees of freedom of statistical
models. Based on this insight, various information criteria were proposed for
model selection.
Let us ﬁrst recapitulate the model selection problem. Consider a (D −1)-
degree polynomial regression model for one-dimensional input t and output y:
y =
D

d=1
wdtd−1 + ε,
where ε denotes a noise. This model can be written as
y = w⊤x + ε,
where w ∈RD is a parameter vector, and x = (1, t, t2,. . . , tD−1)⊤is a
transformed input vector. Suppose that the true distribution can be realized
just with a (D∗−1)-degree polynomial:
y =
D∗

d=1
w∗
dtd−1 + ε = w∗⊤x′ + ε,
where w∗∈RD∗is the true parameter vector, and x′ = (1, t, t2,. . . , tD∗−1)⊤.6
If we train a (D −1)-degree polynomial model for D < D∗, we expect poor
generalization performance because the true distribution is not realizable, i.e.,
the model is too simple to express the true distribution. On the other hand, it
was observed that if we train a model such that D ≫D∗, the generalization
performance is also not optimal, because the unnecessarily high degree terms
cause overﬁtting. Accordingly, ﬁnding an appropriate degree D of freedom,
based on the observed data, is an important task, which is known as a model
selection problem.
It would be a good strategy if we could choose D, which minimizes the
generalization error (13.30). Ignoring the terms that do not depend on the
model (or D), the generalization error can be written as
GE(D) = −

q(x)q(y|x) log p(y|x, D)dxdy + const.
(13.101)
6 By “just,” we mean that w∗
D∗ 0, and therefore the true distribution is not realizable with any
(D −1)-degree polynomial for D < D∗.

13.4 Asymptotic Learning Theory for Regular Models
365
Unfortunately, we cannot directly evaluate Eq. (13.101), since the true distri-
bution q(y|x) is inaccessible. Instead, the training error (13.31),
TE(D) = −1
N
N

n=1
log p(y(n)|x(n), D) + const.,
(13.102)
is often used as an estimator for the generalization error. Although Eq.
(13.102) is accessible, the training error is known to be a biased estimator for
the generalization error (13.101). In fact, the training error does not reﬂect
the negative effect of redundancy of the statistical model, and tends to be
monotonically decreasing as the parameter dimension D increases.
Akaike’s information criterion (AIC) (Akaike, 1974),
AIC = −2
N

n=1
log p(y(n)|x(n), wML) + 2D,
(13.103)
was proposed as an estimator for the generalization error of ML learning
with bias correction. Theorems 13.4 and 13.5 provide the bias between the
generalization error and the training error as follows:

GEML
Regular(D) −TEML
Regular(D)

q(D) = GE
ML
Regular(N) −TE
ML
Regular(N)
=
λML
Regular −νML
Regular
N
+ o(N−1)
= D
N + o(N−1).
(13.104)
Therefore, it holds that
TEML
Regular(D) +

GEML
Regular(D) −TEML
Regular(D)

q(D)
= TEML
Regular(D) + D
N + o(N−1)
= AIC
2N −S N(Y|X) + o(N−1),
(13.105)
where S N(Y|X) is the (conditional) empirical entropy (13.34). Since the
empirical entropy S N(Y|X) does not depend on the model, Eq. (13.105) implies
that minimizing AIC amounts to minimizing an asymptotically unbiased
estimator for the generalization error.
Another strategy for model selection is to minimize an approximation to the
Bayes free energy (13.33). Instead of performing integration for computing

366
13 Asymptotic Learning Theory
the Bayes free energy, Schwarz (1978) proposed to minimize the Bayesian
information criterion (BIC):
BIC = MDL = −2
N

n=1
log p(y(n)|x(n), wML) + D log N.
(13.106)
Interestingly, an equivalent criterion, called the minimum description length
(MDL) (Rissanen, 1986), was derived in the context of information theory in
communication. The relation between BIC and the Bayes free energy can be
directly found from the approximation (13.97), i.e., it holds that
FBayes(Y|X) ≈−NLN(wML) + D
2 log N + Op(1)
= BIC
2
+ Op(1),
(13.107)
and therefore minimizing BIC amounts to minimizing an approximation to the
Bayes free energy.
The ﬁrst terms of AIC (13.103) and BIC (13.106) are the maximum log-
likelihood—the log-likelihood at the ML estimator—multiplied by −2. The
second terms, called penalty terms, penalize high model complexity, which
explicitly work as Occam’s razor (MacKay, 1992) to prune off irrelevant
degrees of freedom of the statistical model. AIC, BIC, and MDL are easily
computable and have shown their usefulness in many applications. However,
their derivations rely on the fact that the generalization coefﬁcient, the training
coefﬁcient, and the free energy coefﬁcient depend only on the parameter
dimension under the regularity conditions. Actually, it has been revealed
that, in singular models, those coefﬁcients depend not only on the parameter
dimension D but also on the true distribution.
13.5 Asymptotic Learning Theory for Singular Models
Many popular statistical models do not satisfy the regularity conditions.
For example, neural networks, matrix factorization, mixture models, hidden
Markov models, and Bayesian networks are all unidentiﬁable and have
singularities, where the Fisher information is singular, in the parameter space.
As discussed in Chapter 7, the true parameter is on a singular point when the
true distribution is realizable with a model with parameter dimension smaller
than the used model, i.e., when the model has redundant components for
expressing the true distribution. In such cases, the likelihood cannot be Taylor-
expanded about the true parameter, and the asymptotic normality does not hold.

13.5 Asymptotic Learning Theory for Singular Models
367
Consequently, the regular learning theory, described in Section 13.4, cannot be
applied to singular models.
In this section, we ﬁrst give intuition on how singularities affect general-
ization properties, and then introduce asymptotic theoretical results on ML
learning and Bayesian learning. After that, we give an overview of asymptotic
theory of VB learning, which will be described in detail in the subsequent
chapters.
13.5.1 Effect of Singularities
Two types of effects of singularities have been observed, which will be detailed
in the following subsections.
Basis Selection Effect
Consider a regression model for one-dimensional input x ∈[−10, 10] and
output y ∈R with H radial basis function (RBF) units:
p(y|x, a, b, c) =
1
√
2πσ2 exp

−1
2σ2 (y −f(x; a, b, c))2

,
(13.108)
where
f(x; a, b, c) =
H

h=1
ρh

x; ah, bh, c2
h
 
.
(13.109)
Each RBF unit in Eq. (13.109) is a weighted Gaussian RBF,
ρh

x; ah, bh, c2
h
 
= ah · Gauss1

x; bh, c2
h
 
=
ah
.
2πc2
h
exp
⎛⎜⎜⎜⎜⎝−(x −bh)2
2c2
h
⎞⎟⎟⎟⎟⎠,
controlled by a weight parameter ah ∈R, a mean parameter bh ∈R, and
a scale parameter c2
h ∈R++. Treating the noise variance σ2 in Eq. (13.108)
as a known constant, the parameters to be estimated are summarized as w =
(a⊤, b⊤, c⊤)⊤∈R3H, where a = (a1,. . . , aH)⊤∈RH, b = (b1,. . . , bH)⊤∈RH,
and c = (c2
1,. . . , c2
H)⊤∈RH
++. Figure 13.4(a) shows an example of the RBF
regression function (13.109) for H = 2.
Apparently, the model (13.108) is unidentiﬁable, and has singularities—
since ρh(x; 0, bh, c2
h) = 0 for any bh ∈R, c2
h ∈R++, the (bh, c2
h) half-space at
ah = 0 is an unidentiﬁable set, on which the Fisher information is singular
(see Figure 13.5).7 Accordingly, we call the model (13.108) a singular RBF
regression model, of which the parameter dimension is equal to Dsin−RBF = 3H.
7 More unidentiﬁable sets can exist, depending on the other RBF units. See Section 7.3.1 for
details on identiﬁability.

368
13 Asymptotic Learning Theory
–10
–5
0
5
10
–1.5
–1
–0.5
0
0.5
1
1.5
(a) Singular RBF with H = 2 units.
–10
–5
0
5
10
–1.5
–1
–0.5
0
0.5
1
1.5
(b) Regular RBF with H = 6 units.
Figure 13.4 Examples (solid curves) of the singular RBF regression function
(13.109) and the regular RBF regression function (13.111). Each RBF unit ρh(x)
is depicted as a dashed curve.
Figure 13.5 Singularities of the RBF regression model (13.108).
Let us consider another RBF regression model
p(y|x, a) =
1
√
2πσ2 exp

−1
2σ2 (y −f(x; a))2

,
(13.110)
where
f(x; a) =
H

h=1
˘ρh (x; ah) =
H

h=1
ρh

x; ah, ˘bh, ˘c2
h
 
.
(13.111)
Unlike the singular RBF model (13.108), we here treat the mean parameters
˘b = (˘b1,. . . , ˘bH)⊤and the scale parameters ˘c = (˘c2
1,. . . , ˘c2
H)⊤as ﬁxed
constants, and only estimate the weight parameters a = (a1,. . . , aH)⊤∈RH.

13.5 Asymptotic Learning Theory for Singular Models
369
Let us set the means and the scales as follows, so that the model covers the
input domain [−10, 10]:
˘bh = −10 + 20 · h −1
H −1,
(13.112)
˘c2
h = 1.
(13.113)
Figure 13.4(b) shows an example of the RBF regression function (13.111) for
H = 6. Clearly, it holds that ˘ρh(x; ah)  ˘ρh(x; a′
h) if ah  a′
h, and therefore the
model is identiﬁable. The other regularity conditions (summarized in Section
13.4.1) on the model distribution p(y|x, a) are also satisﬁed. Accordingly,
we call the model (13.110) a regular RBF regression model, of which the
parameter dimension is equal to Dreg−RBF = H.
Now we investigate difference in learning behavior between the singular
RBF model (13.108) and the regular RBF model (13.110). Figure 13.6 shows
trained regression functions (by ML learning) from N = 50 samples (shown as
crosses) generated from the regression model,
q(y|x) =
1
√
2πσ2 exp

−1
2σ2 (y −f ∗(x))2

,
(13.114)
with the following true functions:
(i) poly: Polynomial function f ∗(x) = −0.002x3.
(ii) cos: Cosine function f ∗(x) = cos(0.5x).
(iii) tanh: Tangent hyperbolic function f ∗(x) = tanh(−0.5x).
(iv) sin-sig: Sine times sigmoid function f ∗(x) = sin(x) ·
1
1+e−x .
(v) sin-alg: Sine function aligned for the regular model f ∗(x) = sin(2π 9
70 x).
(vi) rbf: Single RBF function f ∗(x) = ρ1 (x; 3, −10, 1).
The noise variance is set to σ2 = 0.01, and assumed to be known. We set
the number of RBF units to H = 2 for the singular model, and H = 6 for
the regular model, so that both models have the same degrees of freedom,
Dsin−RBF = Dreg−RBF = 6.
In Figure 13.6, we can observe the following: the singular RBF model can
ﬂexibly ﬁt functions in different shapes (a) through (d), unless the function has
too many peaks (e); the regular RBF model is not as ﬂexible as the singular
RBF model (a) through (d), unless the peaks and valleys match the predeﬁned
means of the RBF units (e). Actually, the frequency of sin-alg is aligned
so that the peaks and the valleys match Eq. (13.112). These observations
are quantitatively supported by the generalization error and the training error
shown in Figure 13.7, leaving us an impression that the singular RBF model

370
13 Asymptotic Learning Theory
–10
–5
0
5
10
–3
–2
–1
0
1
2
3
True
Singular
Regular
(a) poly
–10
–5
0
5
10
–3
–2
–1
0
1
2
3
True
Singular
Regular
(b) cos
–10
–5
0
5
10
–1.5
–1
–0.5
0
0.5
1
1.5
True
Singular
Regular
(c) tanh
–10
–5
0
5
10
–3
–2
–1
0
1
2
3
True
Singular
Regular
(d) sin-sig
–10
–5
0
5
10
–3
–2
–1
0
1
2
3
True
Singular
Regular
(e) sin-alg
–10
–5
0
5
10
–3
–2
–1
0
1
2
3
True
Singular
Regular
(f) rbf
Figure 13.6 Trained regression functions by the singular RBF model (13.108)
with H = 2 RBF units, and the regular RBF model (13.110) with H = 6 RBF
units.
with two modiﬁable basis functions is more ﬂexible than the regular RBF
model with six preﬁxed basis functions.
However, ﬂexibility is granted at the risk of overﬁtting to noise, which
can be observed in Figure 13.6(f). We can see that the true RBF function at
x = −10 is captured by both models. However, the singular RBF model shows
a small valley around x = 8, which is a consequence of overﬁtting to sample
noise. Figure 13.7 also shows that, in the rbf case, the singular RBF model
gives lower training error and higher generalization error than the regular RBF
model—typical behavior when overﬁtting occurs. This overﬁtting tendency is
reﬂected to the generalization and the training coefﬁcients.

13.5 Asymptotic Learning Theory for Singular Models
371
0
0.1
0.2
0.3
poly
cos
tanh
sin-sig
sin-alg
rbf
Singular
Regular
(a) Generalization error
0
0.1
0.2
0.3
poly
cos
tanh
sin-sig
sin-alg
rbf
Singular
Regular
(b) Training error
Figure 13.7 The generalization error and the training error by the singular RBF
model and the regular RBF model.
Apparently, if the true function is realizable, i.e., ∃w∗s.t.f ∗(x) = f(x; w∗),
the true distribution (13.114) is realizable by the RBF regression model (Eq.
(13.108) or (13.110)). In the examples (a) through (e) in Figure 13.7, the
true function is not realizable by the RBF regression model. In such cases,
the generalization error and the training error do not converge to zero, and it
holds that GE(N) = Θ(1) and TE(N) = Θ(1) for the best learning algorithm.
On the other hand, the true function (f) consists of a single RBF unit, and
furthermore its mean and variance match those of the ﬁrst unit of the regular
RBF model (see Eqs. (13.112) and (13.113)). Accordingly, the true function
(f) and therefore the true distribution (13.114) in the example (f) are realizable,
i.e., ∃w∗, s.t.q(y|x) = p(y|x, w∗), by both of the singular RBF model (13.108)
and the regular RBF model (13.110).
When the true parameter w∗exists, the generalization error converges to
zero, and, for any reasonable learning algorithm, the average generalization
error and the average training error can be asymptotically expanded as Eqs.
(13.15) and (13.16):
GE(N) = λN−1 + o(N−1),
TE(N) = νN−1 + o(N−1).
Since the regular RBF model (13.110) is regular, its generalization coefﬁcient
and the training coefﬁcient are given by
2λreg−RBF = −2νreg−RBF = D = Hreg−RBF
(13.115)

372
13 Asymptotic Learning Theory
for ML learning, MAP learning, and Bayesian learning (under the additional
regularity conditions on the prior). On the other hand, the generalization
coefﬁcient and the training coefﬁcient for the singular RBF model (13.108)
are unknown and can be signiﬁcantly different from the regular models. As
will be introduced in Section 13.5.3, the generalization coefﬁcients of ML
learning and MAP learning for various singular models have been clariﬁed,
and all results that have been found so far satisfy
2λML
Singuler ≥D,
2λMAP
Singuler ≥D,
(13.116)
where D is the parameter dimensionality. By comparing Eq. (13.116) with
Eq. (13.115) (or Eqs. (13.67) and (13.81)), we ﬁnd that the ML and the MAP
generalization coefﬁcients per single model parameter in singular models are
larger than those in the regular models, which implies that singular models
tend to overﬁt more than the regular models.
We can explain this phenomenon as an effect of the neighborhood structure
around singularities. Recall the example (f), where the singular RBF model and
the regular RBF model learn the true distribution f ∗(x) = ρ1 (x; 3, −10, 1). For
the singular RBF model, w∗
sin−RBF = (a1, a2, b1, b2, c2
1, c2
2) = (3, 0, −10, ∗, 1, ∗),
where ∗allows any value in the domain, are possible true parameters, while,
for the regular RBF model, w∗
sin−RBF = (a1, a2,. . . , a6) = (3, 0,. . . , 0) is the
unique true parameter. Figure 13.8(a) shows the space of the three parameters
(a2, b2, c2
2) of the second RBF unit of the singular RBF model, in which the
true parameter is on the singularities. Since the true parameter extends over
the two-dimensional half-space {(b2, c2
2); b2 ∈R, c2
2 ∈R++}, the neighborhood
(shown by small arrows) contains any RBF with adjustable mean and variance.
Although the estimated parameter converges to the singularities in the asymp-
totic limit, ML learning on ﬁnite training samples tries to ﬁt the noise, which
contaminates the training samples, by selecting the optimal basis function,
where the optimality is in terms of the training error. On the other hand, Figure
13.8(b) shows the parameter space (ah, ˘bh, ˘c2
h) for h = 2,. . . , 4. For each h, the
true distribution corresponds to a single point, indicated by a shadowed circle,
and its neighborhood extends only in one direction, i.e., ah = 0 ± ε with a
preﬁxed RBF basis speciﬁed by the constants (˘bh, ˘c2
h). Consequently, with the
same number of redundant parameters as the singular RBF model, ML learning
tries to ﬁt the training noise only with those three basis functions.
Although the probability that the three preﬁxed basis functions can ﬁt
the training noise better than a single ﬂexible basis function is not zero, we
would expect that the singular RBF model would likely capture the noise
more ﬂexibly than the regular RBF model. This intuition is supported by
previous theoretical work that showed Eq. (13.116) in many singular models,

13.5 Asymptotic Learning Theory for Singular Models
373
(a) Singular RBF
(b) Regular RBF
Figure 13.8 Neighborhood of the true distribution in the rbf example. (a) The
parameter space of the second (h = 2) RBF unit of the singular RBF model.
The true parameter is on the singularities, of which the neighborhood contains
any RBF with adjustable mean and variance. (b) The parameter space of the
second to the fourth (h = 2,. . . , 4) RBF units of the regular RBF model. With
the same degrees of freedom as a single singular RBF unit, the neighborhood of
the true parameter contains only three different RBF bases with preﬁxed means
and variances.
as well as the numerical example in Figure 13.6. We call this phenomenon,
i.e., singular models tending to overﬁt more than regular models, the basis
selection effect. Although Eq. (13.116) was shown for ML learning and MAP
learning, the basis selection effect should occur for any reasonable learning
algorithms, including Bayesian learning. However, in Bayesian learning, this
effect is canceled by the other effect of singularities, which is explained in the
following subsection.
Integration Effect
Assume that, in Bayesian learning with a singular model, we adopt a prior
distribution p(w) bounded as 0 < p(w) < ∞at any w ∈W. This assumption
is the same as one of the regularity conditions in Section 13.4.1. However, this
assumption excludes the use of the Jeffreys prior (see Appendix B) and positive
mass is distributed over singularities. As discussed in detail in Chapter 7, this
prior choice leads to nonuniformity of the volume element and favors models
with smaller degrees of freedom, if a learning algorithm involving integral
computations in the parameter space is adopted. As a result, singularities
induce MIR in Bayesian learning and its approximation methods, e.g., VB
learning. Importantly, the integration effect does not occur in point estimation
methods, including ML learning and MAP learning, since the nonuniformity of

374
13 Asymptotic Learning Theory
the volume element affects the estimator only through integral computations.
We call this phenomenon the integration effect of singularities.
The basis selection effect and the integration effect inﬂuence the learning
behavior in the opposite way: the former intensiﬁes overﬁtting, while the
latter suppresses it. A question is which is stronger in Bayesian learning.
Singular learning theory, which will be introduced in Section 13.5.4, has
already answered this question. The following has been shown for any singular
models:
2λBayes
Singuler ≤D.
(13.117)
Comparing Eq. (13.117) with Eq. (13.115) (or Eq. (13.94)), we ﬁnd that the
Bayes generalization coefﬁcient per single model parameter in the singular
models is smaller than that in the regular models. Note that the conclusion is
opposite to ML learning and MAP learning—singular models overﬁt training
noise more than the regular models in ML learning and MAP learning, while
they less overﬁt in Bayesian learning. Since the basis selection effect should
occur in any reasonable learning algorithm, we can interpret Eq. (13.117) as
evidence that the integration effect is stronger than the basis selection effect in
Bayesian learning.
One might wonder why Bayesian learning is not analyzed with the Jeffreys
prior—the parameterization invariant noninformative prior. Actually, the Jef-
freys prior, or other prior distribution with zero mass at the singularities, is
rarely used in singular models because of the computational reasons: when the
computational tractability relies on the (conditional) conjugacy, the Jefferey
prior is out of choice in singular models; when some sampling method is used
for approximating the Bayes posterior, the diverging outskirts of the Jeffreys
prior prevents the sampling sequence to converge. Note that this excludes the
empirical Bayesian procedure, where the prior can be collapsed after training.
Little is known about the learning behavior of empirical Bayesian learning in
singular models, and the asymptotic learning theory part (Part IV) of this book
also excludes this case.
In the following subsections, we give a brief summary of theoretical results
that revealed learning properties of singular models.
13.5.2 Conditions Assumed in Asymptotic Theory
for Singular Models
Singular models were analyzed under the following conditions on the true
distribution and the prior distribution:

13.5 Asymptotic Learning Theory for Singular Models
375
(i) The true distribution is realizable by the statistical model, i.e.,
∃w∗s.t. q(x) = p(x|w∗).
(ii) The prior p(w) is twice differentiable and bounded as 0 < p(w) < ∞at
any w ∈W.
Under the second condition, the prior choice does not affect the generalization
coefﬁcient. Accordingly, the results, introduced in the following subsection,
for ML learning can be directly applied to MAP learning.
13.5.3 ML Learning and MAP Learning
Fukumizu (1999) analyzed the asymptotic behavior of the generalization error
of ML learning for the reduced rank regression (RRR) model (3.36), by
applying the random matrix theory to evaluate the singular value distribution
of the ML estimator. Speciﬁcally, the large-scale limit where the dimensions
of the input and the output are inﬁnitely large was considered, and the exact
generalization coefﬁcient was derived. The training coefﬁcient can be obtained
in the same way (Nakajima and Watanabe, 2007).
The Gaussian mixture model (GMM) (4.6) has been studied as a prototype
of singular models in the case of ML learning. Akaho and Kappen (2000)
showed that the generalization error and the training error behave quite
differently from regular models. As deﬁned in Eq. (13.12), −N·TEML(X) is the
log-likelihood ratio, which asymptotically follows the chi-squared distribution
for regular models, while little is known about its behavior for singular models.
In fact, it is conjectured for the spherical GMM (4.6) that the log-likelihood
ratio diverges to inﬁnity in the order of log log N (Hartigan, 1985). For
mixture models with discrete components such as binomial mixture models,
the asymptotic distribution of the log-likelihood ratio was studied through
the distribution of the maximum of the Gaussian random ﬁeld (Bickel and
Chernoff, 1993; Takemura and Kuriki, 1997; Kuriki and Takemura, 2001).
Based on the idea of locally conic parameterization (Dacunha-Castelle and
Gassiat, 1997), the asymptotic behaviors of the log-likelihood ratio in some
singular models were analyzed. For some mixture models with continuous
components, including GMMs, it can be proved that the log-likelihood ratio
diverges to inﬁnity as N →∞. In neural networks, it is known that the log-
likelihood ratio diverges in the order of log N when there are at least two
redundant hidden units (Fukumizu, 2003; Hagiwara and Fukumizu, 2008).
In all previous works, the obtained generalization coefﬁcient or its equiva-
lent satisﬁes Eq. (13.116).

376
13 Asymptotic Learning Theory
13.5.4 Singular Learning Theory for Bayesian Learning
For analyzing the generalization performance of Bayesian learning, a general
approach, called the singular learning theory (SLT), was established, based on
the mathematical techniques in algebraic geometry (Watanabe, 2001a, 2009).
The average relative Bayes free energy (13.23),
F
Bayes(N) =
/
log
N
n=1 q(x(n))

p(w) N
n=1 p(x(n)|w)dw
0
q(X)
= −
/
log

exp

−N log q(x)
p(x|w)

· p(w)dw
0
q(x)
,
can be approximated as
F
Bayes(N) ≈−log

exp (−NE(w)) · p(w)dw,
(13.118)
where
E(w) =
/
log q(x)
p(x|w)
0
q(x)
(13.119)
is the KL divergence between the true distribution q(x) and the model
distribution p(x|w).8
Let us see the KL divergence (13.119) as the energy in physics, and deﬁne
the state density function for the energy value s > 0:
v(s) =

δ(s −E(w)) · p(w)dw,
(13.120)
where δ(·) is the Dirac delta function (located at the origin). Note that the
state density (13.120) and the (approximation to the relative) Bayes free energy
(13.118) are connected by the Laplace transform:
F
Bayes(N) = −log

exp(−s)v
! s
N
" ds
N .
(13.121)
Deﬁne furthermore the zeta function as the Mellin transform, an extension of
the Laplace transform, of the state density (13.120):
ζ(z) =

szv(s)ds =

E(w)zp(w)dw.
(13.122)
The zeta function (13.122) is a function of a complex number z ∈C, and it was
proved that all the poles of ζ(z) are real, negative, and rational numbers.
8 It holds that F
Bayes(N) = −log

exp (−NE(w)) · p(w)dw + O(1) if the support of the prior is
compact (Watanabe, 2001a, 2009).

13.5 Asymptotic Learning Theory for Singular Models
377
By using the relations through Laplace/Mellin transform among the free
energy (13.118), the state density (13.120), and the zeta function (13.122),
Watanabe (2001a) proved the following theorem:
Theorem 13.13
(Watanabe, 2001a, 2009) Let 0 > −λ1 > −λ2 > · · · be the
sequence of the poles of the zeta function (13.122) in the decreasing order, and
m1, m2,. . . be the corresponding orders of the poles. Then the average relative
Bayes free energy (13.119) can be asymptotically expanded as
F
Bayes(N) = λ1 log N −(m1 −1) log log N + O(1).
(13.123)
Let c(N) = F
Bayes(N) −λ1 log N + (m1 −1) log log N be the O(1) term in
Eq. (13.123). The relation (13.24) between the generalization error and the free
energy leads to the following corollary:
Corollary 13.14
(Watanabe, 2001a, 2009) If c(N + 1) −c(N) = o

1
N log N
 
,
the average generalization error (13.13) can be asymptotically expanded as
GE
Bayes(N) = λ1
N −m1 −1
N log N + o

1
N log N

.
(13.124)
To sum up, ﬁnding the maximum pole λ1 of the zeta function ζ(z) gives the
Bayes free energy coefﬁcient
λ′Bayes = λ1,
which is equal to the Bayes generalization coefﬁcient
λBayes = λ1.
Note that Theorem 13.13 and Corollary 13.14 hold both for regular and
singular models. As discussed in Section 7.3.2, MIR (or the integration effect
of singularities) is caused by strong nonuniformity of the density of the
volume element. Since the state density (13.120) reﬂects the strength of the
nonuniformity, one can see that ﬁnding the maximum pole of ζ(z) amounts to
ﬁnding the strength of the nonuniformity at the most concentrated point.
Some general inequalities were proven (Watanabe, 2001b, 2009):
• If the prior is positive at any singular point, i.e.,
p(w) > 0, ∀w ∈{w; det (F(w)) = 0}, then
2λ′Bayes = 2λBayes ≤D.
(13.125)
• If the Jeffreys prior (see Appendiex B.4) is adopted, for which p(w) = 0
holds at any singular point, then
2λ′Bayes = 2λBayes ≥D.
(13.126)

378
13 Asymptotic Learning Theory
Some cases have been found where 2λ′Bayes = 2λBayes are strictly larger
than D.
These results support the discussion in Section 13.5.1 on the two effects of
singularities: Eq. (13.125) implies that the integration effect dominates the
basis selection effect in Bayesian learning, and Eq. (13.126) implies that the
basis selection effect appears also in Bayesian learning if the integration effect
is suppressed by using the Jeffreys prior.
Theorem 13.13 and Corollary 13.14 hold for general statistical models,
while they do not immediately tell us learning properties of singular models.
This is because ﬁnding the maximum pole of the zeta function ζ(z) is not an
easy task, and requires a speciﬁc technique in algebraic geometry called the
resolution of singularities. Good news is that, when any pole larger than −D/2
is found, it provides an upper bound of the generalization coefﬁcient and thus
guarantees the performance with a tighter bound (Theorem 13.13 implies that
the larger the found pole is, the tighter the provided bound is).
For the RRR model (Aoyagi and Watanabe, 2005) and for the GMM
(Aoyagi and Nagata, 2012), the maximum pole was found for general cases,
and therefore the exact value of the free energy coefﬁcient, as well as the
generalization coefﬁcient, was obtained. In other singular models, including
neural networks (Watanabe, 2001a), mixture models (Yamazaki and Watanabe,
2003a), hidden Markov models (Yamazaki and Watanabe, 2005), and Bayesian
networks (Yamazaki and Watanabe, 2003b; Rusakov and Geiger, 2005), upper-
bounds of the free energy coefﬁcient were obtained by ﬁnding some poles
of the zeta function. An effort has been made to perform the resolution
of singularities systematically by using the newton diagram (Yamazaki and
Watanabe, 2004).
13.5.5 Information Criteria for Singular Models
The information criteria introduced in Section 13.4.8 rely on the learn-
ing theory under the regularity conditions. Therefore, although they were
sometimes applied for model selection in singular models, their relations to
generalization properties, e.g., AIC to the ML generalization error, and BIC to
the Bayes free energy, do not generally hold. In the following, we introduce
information criteria applicable for general statistical models including the
regular and the singular models (Watanabe, 2009, 2010, 2013). They also cover
a generalization of Bayesian learning.
Consider a learning method, called generalized Bayesian learning, based
on the generalized posterior distribution,

13.5 Asymptotic Learning Theory for Singular Models
379
p(β)(w|X) =
p(w) N
n=1
,
p(x(n)|w)
-β

p(w) N
n=1
6p(x(n)|w)7β dw
,
(13.127)
where β, called the inverse temperature parameter, modiﬁes the importance of
the likelihood per training sample. The prediction is made by the generalized
predictive distribution,
p(β)(x|X) = ⟨p(x|w)⟩p(β)(w|X) .
(13.128)
Generalized Bayesian learning covers both Bayesian learning and ML learning
as special cases: when β = 1, the generalized posterior distribution (13.127) is
reduced to the Bayes posterior distribution (13.9), with which the generalized
predictive distribution (13.128) gives the Bayes predictive distribution (13.8);
As β increases, the probability mass of the generalized posterior distribution
concentrates around the ML estimator, and, in the limit when β →∞, the
generalized predictive distribution converges to the ML predictive distribution
(13.6).
Deﬁne the following quantities:
GL(X) = −
/
log

p(x|w)p(β)(w|X)dw
0
q(x)
,
(13.129)
TL(X) = −1
N
N

n=1
log

p(x(n)|w)p(β)(w|X)dw,
(13.130)
GGL(X) = −
/ $log p(x|w)% p(β)(w|X)dw
0
q(x)
,
(13.131)
GTL(X) = −1
N
N

n=1
 
log p(x(n)|w)
 
p(β)(w|X)dw,
(13.132)
which are called the Bayes generalization loss, the Bayes training loss, the
Gibbs generalization loss, and the Gibbs training loss, respectively. The
generalization error and the training error of generalized Bayesian learning
are, respectively, related to the Bayes generalization loss and the Bayes training
loss as follows (Watanabe, 2009):
GE(β)(X) =
/
log
q(x)

p(x|w)p(β)(w|X)dw
0
q(x)
= GL(X) −S ,
(13.133)

380
13 Asymptotic Learning Theory
TE(β)(X) = 1
N
N

n=1
log
q(x(n))

p(x(n)|w)p(β)(w|X)dw
= TL(X) −S N(X),
(13.134)
where
S = −log q(x)
q(x)
and
S N(X) = −1
N
N

n=1
log q(x(n))
(13.135)
are the entropy of the true distribution and its empirical version, respectively.9
Also, Gibbs counterparts have the following relations:
GGE(β)(X) =
/ 
log q(x)
p(x|w)

p(β)(w|X)dw
0
q(x)
= GGL(X) −S ,
(13.136)
GTE(β)(X) = 1
N
N

n=1
 
log q(x(n))
p(x(n)|w)

p(β)(w|X)dw
= GTL(X) −S N(X).
(13.137)
Here GGE(β)(X) and GTE(β)(X) are the generalization error and the training
error, respectively, of Gibbs learning, where prediction is made by p(x|w) with
its parameter w sampled from the generalized posterior distribution (13.127).
The following relations were proven (Watanabe, 2009):
⟨GL(X)⟩q(X) = ⟨TL(X)⟩q(X) + 2β

⟨GTL(X)⟩q(X) −⟨TL(X)⟩q(X)
 
+ o(N−1),
(13.138)
⟨GGL(X)⟩q(X) = ⟨GTL(X)⟩q(X) + 2β

⟨GTL(X)⟩q(X) −⟨TL(X)⟩q(X)
 
+ o(N−1),
(13.139)
which imply that asymptotically unbiased estimators for generalization losses
(the left-hand sides of Eqs. (13.138) and (13.139)) can be constructed from
training losses (the right-hand sides). The aforementioned equations lead to
widely applicable information criteria (WAIC) (Watanabe, 2009), deﬁned as
WAIC1 = TL(X) + 2β (GTL(X) −TL(X)) ,
(13.140)
WAIC2 = GTL(X) + 2β (GTL(X) −TL(X)) .
(13.141)
9 S N(X) was deﬁned in Eq. (13.20), and it holds that S = ⟨S N(X)⟩q(X).

13.5 Asymptotic Learning Theory for Singular Models
381
Clearly, WAIC1 and WAIC2 are asymptotically unbiased estimators for the
Bayes generalization loss GL(X) and the Gibbs generalization loss GGL(X),
respectively, and therefore minimizing them amounts to minimizing the Bayes
generalization error (13.133) and the Gibbs generalization error (13.136),
respectively.
The training losses, TL(X) and GTL(X) , can be computed by, e.g., MCMC
sampling (see Sections 2.2.4 and 2.2.5). Let w(1),. . . , w(T) be samples drawn
from the generalized posterior distribution (13.127). Then we can estimate the
training losses by
TL(X) ≈−1
N
N

n=1
log
⎛⎜⎜⎜⎜⎜⎝
1
T
T

t=1
p(x(n)|w(t))
⎞⎟⎟⎟⎟⎟⎠,
(13.142)
GTL(X) ≈−1
N
N

n=1
1
T
T

t=1
log p(x(n)|w(t)).
(13.143)
WAIC can be seen as an extension of AIC, since minimizing it amounts to
minimizing an asymptotically unbiased estimator for the generalization error.
Indeed, under the regularity conditions, it holds that
lim
β→∞2β (GTL(X) −TL(X)) = D
N ,
and therefore
WAIC1, WAIC2 →AIC
2N
as
β →∞.
An extension of BIC was also proposed. The widely applicable Bayesian
information criterion (WBIC) (Watanabe, 2013) is deﬁned as
WBIC = −
N

n=1

log p(x(n)|w)p(β=1/ log N)(w|X)dw,
(13.144)
where p(β=1/ log N)(w|X) is the generalized posterior distribution (13.127) with
the inverse temperature parameter set to β = 1/ log N. It was shown that
FBayes(X)
⎛⎜⎜⎜⎜⎜⎝≡−log

p(w)
N

n=1
p(x(n)|w)dw
⎞⎟⎟⎟⎟⎟⎠= WBIC + Op(
C
log N),
(13.145)
and therefore WBIC can be used as an estimator or approximation for the
Bayes free energy (13.18) when N is large. It was also shown that, under the
regularity conditions, it holds that
WBIC = BIC
2
+ Op(1).

382
13 Asymptotic Learning Theory
WBIC (13.144) can be estimated, similarly to WAIC, from samples
w(1)
β=1/ log N,. . . , w(T)
β=1/ log N drawn from p(β=1/ log N)(w|X) as
WBIC ≈−
N

n=1
1
T
T

t=1
log p(x(n)|w(t)
β=1/ log N).
(13.146)
Note that evaluating the Bayes free energy (13.18) is much more compu-
tationally demanding in general. For example, the all temperatures method
(Watanabe, 2013) requires posterior samples {w(t)
β j } for many 0 = β1 < β2 <
· · · < βJ = 1, and estimates the Bayes free energy as
FBayes(X) ≈−
J−1

j=1
log 1
T j
T j

t=1
exp
⎛⎜⎜⎜⎜⎜⎝(βj+1 −βj)
N

n=1
log p(x(n)|w(t)
β j )
⎞⎟⎟⎟⎟⎟⎠.
13.6 Asymptotic Learning Theory for VB Learning
In the rest of Part IV, we describe asymptotic learning theory for VB learning
in detail. Here we give an overview of the subsequent chapters.
VB learning is rarely applied to regular models.10 Actually, if the model
(and the prior) satisﬁes the regularity conditions, Laplace approximation
(2.2.1) can give a good approximation to the posterior, because of the
asymptotic normality (Theorem 13.3). Accordingly, we focus on singular
models when analyzing VB learning.
We are interested in generalization properties of the VB posterior, which is
deﬁned as
r ≡argmin
r
F(r),
s.t.
r ∈G,
(13.147)
where
F(r) =
/
log
r(w)
p(w) N
n=1 p(x(n)|w)
0
r(w)
= KL (r(w)||p(w|X)) + FBayes(X)
(13.148)
is the free energy and G is the model-speciﬁc constraint, imposed for compu-
tational tractability, on the approximate posterior.
10 VB learning is often applied to a linear model with an ARD prior. In such a model, the model
likelihood satisﬁes the regularity conditions, while the prior does not. Actually, the model
exhibits characteristics of singular models, since it can be translated to a singular model with a
constant prior (see Section 7.5). In Part IV, we only consider the case where the prior is ﬁxed,
without any hyperparameter optimized.

13.6 Asymptotic Learning Theory for VB Learning
383
With the VB predictive distribution
pVB(x|X) = ⟨p(x|w)⟩r(w) ,
the generalization error (13.10) and the training error (13.11) are deﬁned and
analyzed.
We also analyze the VB free energy,
FVB(X) = F(r) = min
r
F(r).
(13.149)
Since Eq. (13.148) implies that
FVB(X) −FBayes(X) = KL $r(w)||p(w|X)% ,
comparing the VB free energy and the Bayes free energy reveals how
accurately VB learning approximates Bayesian learning.
Similarly to the analysis of Bayesian learning, we investigate the asymptotic
behavior of the relative VB free energy,
FVB(X) = FVB(X) −NS N(X) = λ′VB log N + op(log N),
(13.150)
where S N(X) is the empirical entropy deﬁned in Eq. (13.20), and λ′VB is called
the VB free energy coefﬁcient.
Chapter 14 introduces asymptotic VB theory for the RRR model. This
model was relatively easily analyzed by using the analytic-form solution for
fully observed matrix factorization (Chapter 6), and the exact values of the VB
generalization coefﬁcient, the VB training coefﬁcient, and the VB free energy
coefﬁcient were derived (Nakajima and Watanabe, 2007). Since generalization
properties of ML learning and Bayesian learning have also been clariﬁed
(Fukumizu, 1999; Aoyagi and Watanabe, 2005), similarities and dissimilarities
among ML (and MAP) learning, Bayesian learning, and VB learning will be
discussed.
Chapters 15 through 17 are devoted to asymptotic VB theory for latent
variable models. Chapter 15 analyzes the VB free energy of mixture models.
The VB free energy coefﬁcients and their dependencies on prior hyperparam-
eters are revealed. Chapter 16 proceeds to such analyses of the VB free energy
for other latent variable models, namely, Bayesian networks, hidden Markov
models, probabilistic context free grammar, and latent Dirichlet allocation.
Chapter 17 provides a formula for general latent variable models, which
reduces the asymptotic analysis of the VB free energy to that of the Bayes free
energy introduced in Section 13.5.4. Those results will clarify phase transition
phenomena with respect to the hyperparameter setting—the shape of the
posterior distribution in the asymptotic limit drastically changes when some

384
13 Asymptotic Learning Theory
hyparparameter value exceeds a certain threshold. Such implication suggests
to practitioners how to choose hyperparameters.
Note that the relation (13.25) does not necessarily hold for VB learning
and other approximate Bayesian methods, since Eq. (13.24) only holds for
the exact Bayes predictive distribution. Therefore, unlike Bayesian learning,
the asymptotic behavior of the VB free energy does not necessarily inform
us of the asymptotic behavior of the VB generalization error. An effort on
relating the VB free energy and the VB generalization error is introduced in
Chapter 17, although clarifying VB generalization error requires further effort
and techniques.

14
Asymptotic VB Theory of Reduced
Rank Regression
In this chapter, we introduce asymptotic theory of VB learning in the reduced
rank regression (RRR) model (Nakajima and Watanabe, 2007). Among the
singular models, the RRR model is one of the simplest, and many aspects
of its learning behavior have been clariﬁed. Accordingly, we can discuss
similarities and dissimilarities of ML (and MAP) learning, Bayesian learning,
and VB learning in terms of generalization error, training error, and free energy.
After deﬁning the problem setting, we show theoretical results and summarize
insights into VB learning that the analysis on the RRR model provides.
14.1 Reduced Rank Regression
RRR (Baldi and Hornik, 1995; Reinsel and Velu, 1998), introduced in Section
3.1.2 as a special case of fully observed matrix factorization, is a regression
model with a rank-H(≤min(L, M)) linear mapping between input x ∈RM and
output y ∈RL:
y = BA⊤x + ε,
(14.1)
where A ∈RM×H and B ∈RL×H are parameters to be estimated, and ε is
observation noise. Assuming Gaussian noise ε ∼GaussL(0, σ′2IL), the model
distribution is given as
p(y|x, A, B) =

2πσ′2 −L/2 exp

−1
2σ′2
###y −BA⊤x
###2
.
(14.2)
RRR is also called a linear neural network, since the three-layer neural
network (7.13) is reduced to RRR (14.1) if the activation function ψ(·) is linear
385

386
14 Asymptotic VB Theory of Reduced Rank Regression
(see also Figure 3.1). We assume conditionally conjugate Gaussian priors for
the parameters:
p(A) ∝exp

−1
2tr

AC−1
A A⊤ 
,
p(B) ∝exp

−1
2tr

BC−1
B B⊤ 
,
(14.3)
with diagonal convariances CA and CB:
CA = Diag(c2
a1,. . . , c2
aH),
CB = Diag(c2
b1,. . . , c2
bH),
for cah, cbh > 0, h = 1,. . . , H. In the asymptotic analysis, we assume that
the hyperparameters {c2
ah, c2
bh}, σ′2 are ﬁxed constants of the order of 1, i.e.,
{c2
ah, c2
bh}, σ′2 ∼Θ(1) when N →∞.
The degree of freedom of the RRR model is, in general, different from
the apparent number, (M + L)H, of entries of the parameters A and B. This
is because of the trivial redundancy in parameterization—the transformation
(A, B) →(AT⊤, BT−1) does not change the linear mapping BA⊤for any
nonsingular matrix T ∈RH×H. Accordingly, the essential parameter dimen-
sionality is counted as
D = H(M + L) −H2.
(14.4)
Suppose we are given N training samples:
D =
,
(x(n), y(n)); x(n) ∈RM, y(n) ∈RL, n = 1,. . . , N
-
,
(14.5)
which are independently drawn from the true distribution q(x, y) = q(y|x)q(x).
We also use the matrix forms that summarize the inputs and the outputs
separately:
X = (x(1),. . . , x(N))⊤∈RN×M,
Y = (y(1),. . . , y(N))⊤∈RN×L.
We suppose that the data are preprocessed so that the input and the output are
centered, i.e.,
1
N
N

n=1
x(n) = 0
and
1
N
N

n=1
y(n) = 0,
(14.6)
and the input is prewhitened (Hyv¨arinen et al., 2001), i.e.,
1
N
N

n=1
x(n)x(n)⊤= 1
N X⊤X = IM.
(14.7)
The likelihood of the RRR model (14.1) on the training samples D = (X, Y)
is expressed as
p(Y|X, A, B) ∝exp
⎛⎜⎜⎜⎜⎜⎝−1
2σ′2
N

n=1
###y(n) −BA⊤x(n)###2
⎞⎟⎟⎟⎟⎟⎠.
(14.8)

14.1 Reduced Rank Regression
387
As shown in Section 3.1.2, the logarithm of the likelihood (14.8) can be
written, as a function of the parameters, as follows:
log p(Y|X, A, B) = −N
2σ′2
###V −BA⊤###2
Fro + const.,
(14.9)
where
V = 1
N
N

n=1
y(n)x(n)⊤= 1
N Y⊤X.
(14.10)
Note that, unlike in Section 3.1.2, we here do not use the rescaled noise
variance σ2 = σ′2/N, in order to make the dependence on the number N
of samples clear for asymptotic analysis. Because the log-likelihood (14.9)
is in the same form as that of the fully observed matrix factorization (MF),
we can use the global VB solution, derived in Chapter 6, of the MF model for
analyzing VB learning in the RRR model.
14.1.1 VB Learning
VB learning solves the following problem:
r = argmin
r
F(r)
s.t.
r(A, B) = rA(A)rB(B),
(14.11)
where
F =
/
log
rA(A)rB(B)
p(Y|X, A, B)p(A)p(B)
0
rA(A)rB(B)
is the free energy. As derived in Section 3.1, the solution to the problem (14.11)
is in the following forms:
rA(A) = MGaussM,H(A; A, IM ⊗ΣA) ∝exp
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
−
tr
!
(A −A)Σ
−1
A (A −A)⊤
"
2
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
,
(14.12)
rB(B) = MGaussL,H(B; B, IL ⊗ΣB) ∝exp
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
−
tr
!
(B −B)Σ
−1
B (B −B)⊤
"
2
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
.
(14.13)
With the variational parameters (A, ΣA, B, ΣB), the free energy can be explic-
itly written as

388
14 Asymptotic VB Theory of Reduced Rank Regression
2F = NL log(2πσ′2) +
N
n=1
###y(n)###2 −N ∥V∥2
Fro
σ′2
+
N
####V −BA
⊤####
2
Fro
σ′2
+ M log det (CA)
det
ΣA
 + L log det (CB)
det
ΣB
 
−(L + M)H + tr
2
C−1
A
!
A
⊤A + MΣA
"
+ C−1
B
!
B
⊤B + LΣB
"
+ Nσ′−2 !
−A
⊤AB
⊤B +
!
A
⊤A + MΣA
" !
B
⊤B + LΣB
""3
.
(14.14)
We can further apply Corollary 6.6, which states that the VB learning
problem (14.11) is decomposable in the following way. Let
V =
L

h=1
γhωbhω⊤
ah
(14.15)
be the singular value decomposition (SVD) of V (deﬁned in Eq. (14.10)),
where γh (≥0) is the hth largest singular value, and ωah and ωbh are the
associated right and left singular vectors. Then the solution (or its equivalent)
of the variational parameters A = (a1,. . . ,aH), B = (b1,. . . ,bH), ΣA, ΣB,
which minimizes the free energy (14.14), can be expressed as follows:
ah = ahωah,
bh = bhωbh,
ΣA = Diag

σ2
a1,. . . , σ2
aH
 
,
ΣB = Diag

σ2
b1,. . . , σ2
bH
 
,
where {ah,bh ∈R, σ2
ah, σ2
bh ∈R++}H
h=1 are a new set of variational parameters.
Thus, the VB posteriors (14.12) and (14.13) can be written as
rA(A) =
H

h=1
GaussM(ah;ahωah, σ2
ah IM),
(14.16)
rB(B) =
H

h=1
GaussL(bh;bhωbh, σ2
bh IL),
(14.17)
with {ah,bh, σ2
ah, σ2
bh}H
h=1 that are the solution of the following minimization
problem:
Given
σ′2 ∈R++,
{c2
ah, c2
bh ∈R++}H
h=1,
min
{ah,bh,σ2ah,σ2
bh}H
h=1
F
(14.18)
s.t.
{ah,bh ∈R,
σ2
ah, σ2
bh ∈R++}H
h=1.

14.1 Reduced Rank Regression
389
Here F is the free energy (14.14), which can be decomposed as
2F = NL log(2πσ′2) +
N
n=1
###y(n)###2
σ′2
+
H

h=1
2Fh,
(14.19)
where
2Fh = M log
c2
ah
σ2ah
+ L log
c2
bh
σ2
bh
+
a2
h + Mσ2
ah
c2ah
+
b2
h + Lσ2
bh
c2
bh
−(L + M) + N
σ′2

−2ahbhγh +

a2
h + Mσ2
ah
 b2
h + Lσ2
bh
  
.
(14.20)
14.1.2 VB Solution
Let us derive an asymptotic-form VB solution from the nonasymptotic global
VB solution, derived in Section 6. Theorem 6.7 leads to the following theorem:
Theorem 14.1
The VB estimator U
VB ≡BA⊤
rA(A)rB(B) for the linear
mapping of the RRR model (14.2) and (14.3) can be written as
U
VB = BA
⊤=
H

h=1
γVB
h ωbhω⊤
ah,
where
γVB
h
= max

0, ˘γVB
h
 
(14.21)
for
˘γVB
h
= γh
⎛⎜⎜⎜⎜⎝1 −max(L, M)σ′2
Nγ2
h
⎞⎟⎟⎟⎟⎠+ Op(N−1).
(14.22)
For each component h, γVB
h
> 0 if and only if γh > γVB
h
for
γVB
h
= σ′
B
max(L, M)
N
+ O(N−1).
(14.23)
Proof
Noting that Theorem 6.7 gives the VB solution for either V or V⊤∈
RL×M that satisﬁes L ≤M, that the shrinkage estimator ˘γVB
h
(given by
Eq. (6.50)) is an increasing function of γh, and that ˘γVB
h
= 0 when γh is equal
to the threshold γVB
h
(given by Eq. (6.49)), we have Eq. (14.21) with
˘γVB
h
= γh
⎛⎜⎜⎜⎜⎜⎜⎜⎝1 −σ′2
2Nγ2
h
⎛⎜⎜⎜⎜⎜⎜⎜⎝L + M +
>
@
(M −L)2 +
4γ2
h
c2ahc2
bh
⎞⎟⎟⎟⎟⎟⎟⎟⎠
⎞⎟⎟⎟⎟⎟⎟⎟⎠
= γh
⎛⎜⎜⎜⎜⎝1 −σ′2
2Nγ2
h
!
L + M +
.
(M −L)2 + O(γ2
h)
"⎞⎟⎟⎟⎟⎠

390
14 Asymptotic VB Theory of Reduced Rank Regression
=
⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩
γh
!
1 −
σ′2
2Nγ2
h

L + M + max(L, M) −min(L, M) + O(γ2
h)
 "
(if L  M)
γh
!
1 −
σ′2
2Nγ2
h (L + M + O(γh))
"
(if L = M)
= γh
⎛⎜⎜⎜⎜⎝1 −max(L, M)σ′2
Nγ2
h

1 + Op(γh)
 ⎞⎟⎟⎟⎟⎠
(14.24)
= γh
⎛⎜⎜⎜⎜⎝1 −max(L, M)σ′2
Nγ2
h
⎞⎟⎟⎟⎟⎠+ Op(N−1),
and
γVB
h
= σ′
√
N
>
?
?
?
@
(L + M)
2
+
σ′2
2Nc2ahc2
bh
+
>
?
@⎛⎜⎜⎜⎜⎜⎝
(L + M)
2
+
σ′2
2Nc2ahc2
bh
⎞⎟⎟⎟⎟⎟⎠
2
−LM
= σ′
√
N
>
?
@
(L + M)
2
+
σ′2
2Nc2ahc2
bh
+
Amax(L, M) −min(L, M)
2
2
+ O(N−1)
=
⎧⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎩
σ′
√
N
B
(L+M)
2
+
σ′2
2Nc2ahc2
bh
+ max(L,M)−min(L,M)
2
+ O(N−1)
(if L  M)
σ′
√
N
B
(L+M)
2
+
σ′2
2Nc2ahc2
bh
+ O(N−1/2)
(if L = M)
= σ′
√
N
C
max(L, M) + O(N−1),
which completes the proof. Note that we used γh = Op(1) to get Eq. (14.24). □
Theorem 14.1 states that the VB estimator converges to the positive-part
James–Stein (PJS) estimator (see Appendix A)—the same solution (Corollary
7.1) as the nonasymptotic MF solution with the ﬂat prior. This is natural
because the inﬂuence from the constant prior disappears in the asymptotic
limit, making MAP learning converge to ML learning.
Corollary 6.8 leads to the following corollary:
Corollary 14.2
The VB posterior of the RRR model (14.2) and (14.3) is given
by Eqs. (14.16) and (14.17) with the variational parameters given as follows:
if γh > γVB
h ,
ah = ±
.
˘γVB
h δVB
h ,
bh = ±
>
@
˘γVB
h
δVB
h
,
σ2
ah = σ′2δVB
h
Nγh
,
σ2
bh =
σ′2
NγhδVB
h
,
(14.25)

14.1 Reduced Rank Regression
391
where
δVB
h

≡ah
bh

=
⎧⎪⎪⎪⎨⎪⎪⎪⎩
(max(L,M)−min(L,M))cah
γh
+ Op(1)
(if L ≤M),
!
(max(L,M)−min(L,M))cbh
γh
+ Op(1)
"−1
(if L > M),
(14.26)
and otherwise,
ah = 0,
bh = 0,
σ2
ah = c2
ah
⎛⎜⎜⎜⎜⎜⎝1 −NLζVB
h
σ′2
⎞⎟⎟⎟⎟⎟⎠,
σ2
bh = c2
bh
⎛⎜⎜⎜⎜⎜⎝1 −NMζVB
h
σ′2
⎞⎟⎟⎟⎟⎟⎠,
(14.27)
where
ζVB
h

≡σ2
ahσ2
bh
 
=
⎧⎪⎪⎪⎨⎪⎪⎪⎩
min(L,M)σ′2
NLM
+ Θ(N−2),
(if L  M),
min(L,M)σ′2
NLM
+ Θ(N−3/2),
(if L = M).
(14.28)
Proof
Noting that Corollary 6.8 gives the VB posterior for either V or V⊤∈
RL×M that satisﬁes L ≤M, we have Eq. (14.25) with
δVB
h
=
⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩
Ncah
σ′2

γh −˘γVB
h
−Lσ′2
Nγh
 
(if L ≤M)
!
Ncbh
σ′2

γh −˘γVB
h
−Mσ′2
Nγh
 "−1
(if L > M)
=
⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩
(max(L,M)−min(L,M))cah
γh
+ Op(1)
(if L ≤M),
!
(max(L,M)−min(L,M))cbh
γh
+ Op(1)
"−1
(if L > M),
when γh > γVB
h
, and Eq. (14.27) with
ζVB
h
=
σ′2
2NLM
⎧⎪⎪⎨⎪⎪⎩L + M +
σ′2
Nc2ahc2
bh
−
B!
L + M +
σ′2
Nc2ahc2
bh
"2
−4LM
⎫⎪⎪⎬⎪⎪⎭
=
σ′2
2NLM
)
L + M +
σ′2
Nc2ahc2
bh
−
B
(L + M)2 + 2 (L + M)
σ′2
Nc2ahc2
bh
+
!
σ′2
Nc2ahc2
bh
"2
−4LM
⎫⎪⎪⎬⎪⎪⎭
=
σ′2
2NLM
)
L + M +
σ′2
Nc2ahc2
bh
−
B
(max(L, M) −min(L, M))2 + 2 (L + M)
σ′2
Nc2ahc2
bh
+
!
σ′2
Nc2ahc2
bh
"2 ⎫⎪⎪⎬⎪⎪⎭

392
14 Asymptotic VB Theory of Reduced Rank Regression
=
⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
σ′2
2NLM
)
L + M +
σ′2
Nc2ahc2
bh
−(max(L, M) −min(L, M))
·
!
1 +
L+M
(max(L,M)−min(L,M))2
σ′2
Nc2ahc2
bh
"
+ O(N−2)
1
(if L  M)
σ′2
2NLM
)
L + M +
σ′2
Nc2ahc2
bh
−
B
2 (L + M)
!
σ′2
Nc2ahc2
bh
"
+
!
σ′2
Nc2ahc2
bh
"2 ⎫⎪⎪⎬⎪⎪⎭
(if L = M)
=
⎧⎪⎪⎪⎨⎪⎪⎪⎩
σ′2
2NLM

2 min(L, M) + Θ(N−1)
 
(if L  M)
σ′2
2NLM

2 min(L, M) + Θ(N−1/2)
 
(if L = M)
=
⎧⎪⎪⎪⎨⎪⎪⎪⎩
min(L,M)σ′2
NLM
+ Θ(N−2)
(if L  M),
min(L,M)σ′2
NLM
+ Θ(N−3/2)
(if L = M),
when γh ≤γVB
h . This completes the proof.
□
From Corollary 14.2, we can evaluate the orders of the optimal variational
parameters in the asymptotic limit, which will be used when the VB free energy
is analyzed.
Corollary 14.3
The orders of the optimal variational parameters, given by
Eq. (14.25) or Eq. (14.27), are as follows: if γh > γVB
h (= Θ(N−1/2)),
ah =Θp(1),
bh =Θp(γh),
σ2
ah =Θp(N−1γ−2
h ), σ2
bh =Θp(N−1)
(if L< M),
ah =Θp(γ1/2
h ), bh =Θp(γ1/2
h ), σ2
ah =Θp(N−1γ−1
h ), σ2
bh =Θp(N−1γ−1
h ) (if L= M),
ah =Θp(γh),
bh =Θp(1),
σ2
ah =Θp(N−1),
σ2
bh =Θp(N−1γ−2
h ) (if L> M),
(14.29)
and otherwise,
ah = 0,
bh = 0,
σ2
ah = Θ(1),
σ2
bh = Θ(N−1)
(if L < M),
ah = 0,
bh = 0,
σ2
ah = Θ(N−1/2),
σ2
bh = Θ(N−1/2)
(if L = M),
ah = 0,
bh = 0,
σ2
ah = Θ(N−1),
σ2
bh = Θ(1)
(if L > M). (14.30)
Proof
Eqs. (14.22) and (14.23) give
γVB
h
= Θ(N−1/2),
˘γVB
h
= Θ(γh),

14.1 Reduced Rank Regression
393
and Eq. (14.26) gives
δVB
h
=
⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩
Θp(γ−1
h )
(if L < M),
Θp(1)
(if L = M),
Θp(γh)
(if L > M).
Substituting the preceding into Eq. (14.25) gives Eq. (14.29), and substituting
Eq. (14.28) into Eq. (14.27) gives Eq. (14.30), which complete the proof.
□
Corollary 14.3 implies that the posterior probability mass does not neces-
sarily converge to a single point, for example, σ2
ah = Θ(1) if γh < γVB
h
and
L < M. This is typical behavior of singular models with nonidentiﬁability. On
the other hand, the probability mass of the linear mapping U = BA⊤converges
to a single point.
Corollary 14.4
It holds that
*####BA⊤−BA
⊤####
2
Fro
+
rA(A)rB(B) = Op(N−1).
Proof
We have
*####BA⊤−BA
⊤####
2
Fro
+
rA(A)rB(B) = tr
/!
BA⊤−BA
⊤"⊤!
BA⊤−BA
⊤"0
rA(A)rB(B)
= tr
*
AB⊤BA⊤−2AB⊤BA
⊤+ AB
⊤BA
⊤+
rA(A)rB(B)
= tr
!
A⊤AB⊤B

rA(A)rB(B) −A
⊤AB
⊤B
"
= tr
!!
A
⊤A + MΣA
" !
B
⊤B + LΣB
"
−A
⊤AB
⊤B
"
=
H

h=1

a2
h + Mσ2
ah
 b2
h + Lσ2
bh
 
−a2
hb2
h
 
=
H

h=1

La2
hσ2
bh + Mb2
hσ2
ah + LMσ2
ahσ2
bh
 
.
(14.31)
Corollary 14.3 guarantees that all terms in Eq. (14.31) are of the order of
Θp(N−1) for any L, M, and {γh}, which completes the proof.
□
Now we derive an asymptotic form of the VB predictive distribution,
p (y|x, X, Y) = ⟨p(y|x, A, B)⟩rA(A)rB(B) .
(14.32)

394
14 Asymptotic VB Theory of Reduced Rank Regression
From Corollary 14.4, we expect that the predictive distribution is not very far
from the plug-in VB predictive distribution (see Section 1.1.3):
p(y|x, A, B) = GaussL(y; BA
⊤x, σ′2IL).
(14.33)
Indeed, we will show in the next section that both predictive distributions
(14.32) and (14.33) give the same generalization and training coefﬁcients. This
justiﬁes the use of the plug-in VB predictive distribution, which is easy to
compute from the optimal variational parameters.
By expanding the VB predictive distribution around the plug-in VB predic-
tive distribution, we have the following theorem:
Theorem 14.5
The VB predictive distribution (14.32) of the RRR model
(14.2) and (14.3) can be written as
p(y|x, X, Y) = GaussL(y; ΨBA
⊤x, σ′2Ψ) + Op(N−3/2)
(14.34)
for Ψ = IL + Op(N−1).
Proof
The VB predictive distribution can be written as follows:
p (y|x, X, Y) = ⟨p(y|x, A, B)⟩rA(A)rB(B)
= p(y|x, A, B)
/ p(y|x, A, B)
p(y|x, A, B)
0
rA(A)rB(B)
= p(y|x, A, B)
/
exp
⎛⎜⎜⎜⎜⎝−∥y−BA⊤x∥
2−
####y−BA
⊤x
####
2
2σ′2
⎞⎟⎟⎟⎟⎠
0
rA(A)rB(B)
= p(y|x, A, B)
/
exp
⎛⎜⎜⎜⎜⎜⎝−
!
y−BA⊤x+(y−BA
⊤x)
"⊤!
y−BA⊤x−(y−BA
⊤x)
"
2σ′2
⎞⎟⎟⎟⎟⎟⎠
0
rA(A)rB(B)
= p(y|x, A, B)
/
exp
⎛⎜⎜⎜⎜⎜⎝
!
y−(BA⊤−BA
⊤)x
"⊤
(BA⊤−BA
⊤)x
σ′2
⎞⎟⎟⎟⎟⎟⎠
0
rA(A)rB(B)
.
(14.35)
Corollary 14.4 implies that the exponent in Eq. (14.35) is of the order of N−1/2,
i.e.,
φ ≡
!
y −(BA⊤−BA
⊤)x
"⊤
(BA⊤−BA
⊤)x
σ′2
= Op(N−1/2).
(14.36)
By applying the Taylor expansion of the exponential function to Eq. (14.35),
we obtain an asymptotic expansion of the predictive distribution around the
plug-in predictive distribution:
p (y|x, X, Y) = p(y|x, A, B)

1 + ⟨φ⟩rA(A)rB(B) + 1
2

φ2
rA(A)rB(B) + Op(N−3/2)

.

14.1 Reduced Rank Regression
395
Focusing on the dependence on the random variable y, we can identify the
function form of the predictive distribution as follows:
p (y|x, X, Y) ∝exp

−
####y−BA
⊤x
####
2
2σ′2
+ log
!
1 + ⟨φ⟩rA(A)rB(B) + 1
2

φ2
rA(A)rB(B) + Op(N−3/2)
" 
= exp

−
####y−BA
⊤x
####
2
2σ′2
+ ⟨φ⟩rA(A)rB(B) + 1
2

φ2
rA(A)rB(B)
−1
2 ⟨φ⟩2
rA(A)rB(B) + Op(N−3/2)

∝exp
⎛⎜⎜⎜⎜⎝−
####y−BA
⊤x
####
2
2σ′2
+ 1
2

φ2
rA(A)rB(B) + Op(N−3/2)
⎞⎟⎟⎟⎟⎠
∝exp
⎛⎜⎜⎜⎜⎝−
####y−BA
⊤x
####
2
−y⊤Ψ1y
2σ′2
+ Op(N−3/2)
⎞⎟⎟⎟⎟⎠
∝exp
!
−∥y∥2−2y⊤BA
⊤x−y⊤Ψ1y
2σ′2
+ Op(N−3/2)
"
∝exp
⎛⎜⎜⎜⎜⎜⎝−
!
y−ΨBA
⊤x
"⊤
Ψ−1!
y−ΨBA
⊤x
"
2σ′2
+ Op(N−3/2)
⎞⎟⎟⎟⎟⎟⎠,
(14.37)
where
Ψ = (IL −Ψ1)−1 ,
(14.38)
Ψ1 =
/(BA⊤−BA
⊤)xx⊤(BA⊤−BA
⊤)⊤
σ′2
0
rA(A)rB(B)
.
(14.39)
Here we used
⟨φ⟩rA(A)rB(B) =
/ !
y−(BA⊤−BA
⊤)x
"⊤
(BA⊤−BA
⊤)x
σ′2
0
rA(A)rB(B)
=
/ ####(BA⊤−BA
⊤)x
####
2
σ′2
0
rA(A)rB(B)
= const.,

φ2
rA(A)rB(B) =
/ !
y−(BA⊤−BA
⊤)x
"⊤
(BA⊤−BA
⊤)xx⊤(BA⊤−BA
⊤)⊤
!
y−(BA⊤−BA
⊤)x
"
σ′4
0
rA(A)rB(B)
= y⊤Ψ1y
σ′2
+ Op(N−3/2).
Eq. (14.39) implies that Ψ1 is symmetric and Ψ1 = Op(N−1). Therefore,
Ψ, deﬁned by Eq. (14.38), is symmetric, positive deﬁnite, and can be written

396
14 Asymptotic VB Theory of Reduced Rank Regression
as Ψ = IL + Op(N−1). The function form of Eq. (14.37) implies that the VB
predictive distribution converges to the Gaussian distribution in the asymptotic
limit, and we thus have
p (y|x, X, Y) =
exp
⎛⎜⎜⎜⎜⎜⎝−(y−ΨBA⊤x)
⊤Ψ−1(y−ΨBA⊤x)
2σ′2
+Op(N−3/2)
⎞⎟⎟⎟⎟⎟⎠

exp
⎛⎜⎜⎜⎜⎜⎝−(y−ΨBA⊤x)
⊤Ψ−1(y−ΨBA⊤x)
2σ′2
+Op(N−3/2)
⎞⎟⎟⎟⎟⎟⎠dy
=
exp
⎛⎜⎜⎜⎜⎜⎝−(y−ΨBA⊤x)
⊤Ψ−1(y−ΨBA⊤x)
2σ′2
⎞⎟⎟⎟⎟⎟⎠(1+Op(N−3/2))

exp
⎛⎜⎜⎜⎜⎜⎝−(y−ΨBA⊤x)
⊤Ψ−1(y−ΨBA⊤x)
2σ′2
⎞⎟⎟⎟⎟⎟⎠(1+Op(N−3/2))dy
=
1
(2πσ′2)
L/2 det(Ψ)1/2 exp
⎛⎜⎜⎜⎜⎜⎝−
!
y−ΨBA
⊤x
"⊤
Ψ−1!
y−ΨBA
⊤x
"
2σ′2
⎞⎟⎟⎟⎟⎟⎠+ Op(N−3/2),
which completes the proof.
□
14.2 Generalization Properties
Let us analyze generalization properties of VB learning based on the posterior
distribution and the predictive distribution, derived in Section 14.1.2.
14.2.1 Assumption on True Distribution
We assume that the true distribution can be expressed by the model distribution
with the true parameter A∗and B∗with their rank H∗:
q(y|x) = GaussL

y, B∗A∗⊤x, σ′2IL
 
=

2πσ′2 −L/2 exp
⎛⎜⎜⎜⎜⎜⎜⎝−
###y −B∗A∗⊤x
###2
2σ′2
⎞⎟⎟⎟⎟⎟⎟⎠.
(14.40)
Let
U∗≡B∗A∗⊤=
min(L,M)

h=1
γ∗
hω∗
bhω∗⊤
ah
(14.41)
be the SVD of the true linear mapping B∗A∗⊤, where γ∗
h (≥0) is the hth
largest singular value, and ω∗
ah and ω∗
bh are the associated right and left singular
vectors. The assumption that the true linear mapping has rank H∗amounts to
γ∗
h =
⎧⎪⎪⎨⎪⎪⎩
Θ(1)
for
h = 1,. . . , H∗,
0
for
h = H∗+ 1,. . . , min(L, M).
(14.42)

14.2 Generalization Properties
397
14.2.2 Consistency of VB Estimator
Since the training samples are drawn from the true distribution (14.40), the
central limit theorem (Theorem 13.1) guarantees the following:
V
⎛⎜⎜⎜⎜⎜⎝≡1
N
N

n=1
y(n)x(n)⊤
⎞⎟⎟⎟⎟⎟⎠= 1
N
N

n=1

B∗A∗⊤x(n) + ε(n) 
x(n)⊤
= B∗A∗⊤+ Op(N−1/2),
(14.43)

xx⊤
q(x) = 1
N
N

n=1
x(n)x(n)⊤+ Op(N−1/2)
= IM + Op(N−1/2).
(14.44)
Here we used the assumption (14.7) that the input is prewhitened. Eq. (14.43)
is consistent with Eq. (14.9), which implies that the distribution of V is
given by
q(V) = MGaussL,M

V; B∗A∗⊤, σ′2
N IL ⊗IM

,
(14.45)
and therefore
⟨V⟩q(X,Y) = ⟨V⟩q(V) = B∗A∗⊤,
(14.46)
and for each (l, m),
*####Vl,m −

B∗A∗⊤ 
l,m
####
2
Fro
+
q(X,Y) = σ′2
N .
(14.47)
Eq. (14.43) implies that
γh = γ∗
h + Op(N−1/2),
(14.48)
where γh is the hth largest singular value of V (see Eq. (14.15)). Eq. (14.45)
also implies that, for any h,

h′:γ∗
h′=γ∗
h

γh′ωbh′ω⊤
ah′

q(X,Y) =

h′:γ∗
h′=γ∗
h
γ∗
h′ω∗
bh′ ω∗⊤
ah′,
(14.49)

h′:γ∗
h′=γ∗
h
γh′ωbh′ω⊤
ah′ =

h′:γ∗
h′=γ∗
h
γ∗
h′ω∗
bh′ ω∗⊤
ah′ + Op(N−1/2),
(14.50)
where 
h′:γ∗
h′=γ∗
h denotes the sum over all h′ such that γ∗
h′ = γ∗
h. Eq. (14.50)
implies that for any nonzero and nondegenerate singular component h (i.e.,
γ∗
h > 0 and γ∗
h  γ∗
h′∀h′  h), it holds that

398
14 Asymptotic VB Theory of Reduced Rank Regression
ωah = ω∗
ah + Op(N−1/2),
ωbh = ω∗
bh + Op(N−1/2).
Eq. (14.9) implies that the ML estimator is given by
!
BA
⊤"ML
=
H

h=1
γhωbhω⊤
ah.
(14.51)
Therefore, Eq. (14.43) guarantees the convergence of the ML estimator to the
true linear mapping B∗A∗⊤when H ≥H∗.
Lemma 14.6
(Consistency of ML estimator in RRR) It holds that
!
BA
⊤"ML
−B∗A∗⊤=
⎧⎪⎪⎨⎪⎪⎩
Θ(1)
if
H < H∗,
Op(N−1/2)
if
H ≥H∗.
We can also show the convergence of the VB estimator:
Lemma 14.7
(Consistency of VB estimator in RRR) It holds that
BA
⊤−B∗A∗⊤=
⎧⎪⎪⎨⎪⎪⎩
Θ(1)
if
H < H∗,
Op(N−1/2)
if
H ≥H∗.
Proof
The case where H < H∗is trivial because the rank H matrix BA
⊤can
never converge to the rank H∗matrix B∗A∗⊤. Assume that H ≥H∗. Theorem
14.1 implies that, when γh > γVB
h (= Θ(N−1/2)),
γVB
h
= ˘γVB
h
= γh
⎛⎜⎜⎜⎜⎝1 −max(L, M)σ′2
Nγ2
h
⎞⎟⎟⎟⎟⎠+ Op(N−1) = γh + Op(N−1/2),
and otherwise
γVB
h
= 0.
Since γh = Op(N−1/2) for h = H∗+ 1,. . . , min(L, M), the preceding two
equations lead to
BA
⊤=
H

h=1
γVB
h ωbhω⊤
ah =
min(L,M)

h=1
γhωbhω⊤
ah + Op(N−1/2) = V + Op(N−1/2).
(14.52)
Substituting Eq. (14.43) into Eq. (14.52) completes the proof.
□
14.2.3 Generalization Error
Now we analyze the asymptotic behavior of the generalization error. We
ﬁrst show the asymptotic equivalence between the VB predictive distribution,

14.2 Generalization Properties
399
given by Theorem 14.5, and the plug-in VB predictive distribution (14.33)—
both give the same leading term of the generalization error with Op(N−3/2)
difference. To this end, we use the following lemma:
Lemma 14.8
For any three sets of Gaussian parameters (μ∗, Σ∗), (μ, Σ), (´μ, ´Σ)
such that
μ = μ∗+ Op(N−1/2),
Σ = Σ∗+ Op(N−1/2),
(14.53)
´μ = μ + Op(N−1),
´Σ = Σ + Op(N−1),
(14.54)
it holds that
/
log
GaussL

y; ´μ, ´Σ
 
+ Op(N−3/2)
GaussL

y;μ, Σ
 
0
GaussL(y;μ∗,Σ∗)
= Op(N−3/2).
(14.55)
Proof
The (twice of the) left-hand side of Eq. (14.55) can be written as
ψ1 ≡2
/
log
GaussL

y; ´μ, ´Σ
 
+ Op(N−3/2)
GaussL

y;μ, Σ
 
0
GaussL(y;μ∗,Σ∗)
=
/
log
det
Σ
 
det( ´Σ) −(y −´μ)⊤´Σ
−1(y −´μ) + (y −μ)⊤Σ
−1(y −μ)
0
GaussL(y;μ∗,Σ∗)
+ Op(N−3/2)
= −log det
!
Σ∗Σ
−1Σ∗−1 ´Σ
"
−
*
(y −μ∗−(´μ −μ∗))⊤´Σ
−1 (y −μ∗−(´μ −μ∗))
+
GaussL(y;μ∗,Σ∗)
+
*$y −μ∗−(μ −μ∗)%⊤Σ
−1 $y −μ∗−(μ −μ∗)%+
GaussL(y;μ∗,Σ∗) + Op(N−3/2)
= tr
!
log
!
Σ∗´Σ
−1"
−log
!
Σ∗Σ
−1""
−tr
!
Σ∗´Σ
−1"
−(´μ −μ∗)⊤´Σ
−1 (´μ −μ∗)
+ tr
!
Σ∗Σ
−1"
+ $μ −μ∗%⊤Σ
−1 $μ −μ∗% + Op(N−3/2).
By using Eqs. (14.53) and (14.54) and the Taylor expansion of the logarithmic
function, we have
ψ1 = tr
 !
Σ∗´Σ
−1 −IL
"
−
!
Σ∗´Σ
−1−IL
"⊤!
Σ∗´Σ
−1−IL
"
2
−
!
Σ∗Σ
−1 −IL
"
+
!
Σ∗Σ
−1−IL
"⊤!
Σ∗Σ
−1−IL
"
2

−tr
!
Σ∗´Σ
−1"
−$μ −μ∗%⊤Σ
−1 $μ −μ∗%

400
14 Asymptotic VB Theory of Reduced Rank Regression
+ tr
!
Σ∗Σ
−1"
+ $μ −μ∗%⊤Σ
−1 $μ −μ∗% + Op(N−3/2)
= Op(N−3/2),
which completes the proof.
□
Given a test input x, Lemma 14.8 can be applied to the true distribution
(14.40), the plug-in VB predictive distribution (14.33), and the predictive
distribution (14.34) when H ≥H∗, where
μ∗= B∗A∗⊤x,
Σ∗= σ′2IL,
μ = BA
⊤x = μ∗+ Op(N−1/2),
Σ = σ′2IL = Σ∗,
´μ = ΨBA
⊤x = μ + Op(N−1),
´Σ = σ′2Ψ = Σ + Op(N−1),
for Ψ = IL+Op(N−1). Here, Lemma 14.7 was used in the equation forμ. Thus,
we have the following corollary:
Corollary 14.9
When H ≥H∗, it holds that
/
log p (y|x, X, Y)
p(y|x, A, B)
0
q(y|x)
= Op(N−3/2),
and therefore the difference between the generalization error (13.30) of the VB
predictive distribution (14.34) and the generalization error of the plug-in VB
predictive distribution (14.33) is of the order of N−3/2, i.e.,
GE(D) =
/
log
q(y|x)
p(y|x, X, Y)
0
q(y|x)q(x)
=
/
log
q(y|x)
p(y|x, A, B)
0
q(y|x)q(x)
+ Op(N−3/2).
Corollary 14.9 leads to the following theorem:
Theorem 14.10
The generalization error of the RRR model is written as
GE(D) =
⎧⎪⎪⎪⎨⎪⎪⎪⎩
Θ(1)
if
H < H∗,
####BA
⊤−B∗A∗⊤####
2
Fro
2σ′2
+ Op(N−3/2)
if
H ≥H∗.
(14.56)
Proof
When H < H∗, Theorem 14.5 implies that
GE(D) =
/
log
q(y|x)
p(y|x, A, B)
0
q(y|x)q(x)
+ Op(N−1)
=
####BA
⊤−B∗A∗⊤####
2
Fro
2σ′2
+ Op(N−1).

14.2 Generalization Properties
401
With Lemma 14.7, we have GE(D) = Θ(1). When H ≥H∗, we have
GE(D) =
/
log
q(y|x)
p(y|x, A, B)
0
q(y|x)q(x)
+ Op(N−3/2)
=
/
−∥y−B∗A∗⊤x∥
2−
####y−BA
⊤x
####
2
2σ′2
0
q(y|x)q(x)
+ Op(N−3/2)
=
/
−
∥y−B∗A∗⊤x∥
2−
#####y−B∗A∗⊤x−
!
BA
⊤−B∗A∗⊤"
x
#####
2
2σ′2
0
q(y|x)q(x)
+ Op(N−3/2)
=
/ #####
!
BA
⊤−B∗A∗⊤"
x
#####
2
2σ′2
0
q(x)
+ Op(N−3/2)
=
/ tr
)!
BA
⊤−B∗A∗⊤"
xx⊤
!
BA
⊤−B∗A∗⊤"⊤1
2σ′2
0
q(x)
+ Op(N−3/2).
By using Eq. (14.44) and Lemma 14.7, we obtain Eq. (14.56), which completes
the proof.
□
Next we compute the average generalization error (13.13) over the distri-
bution of training samples. As Theorem 14.10 states, the generalization error
never converges to zero if H < H∗, since a rank H∗matrix cannot be well
approximated by a rank H matrix. Accordingly, we hereafter focus on the
case where H ≥H∗. By WishartD(V, ν) we denote the D-dimensional Wishart
distribution with scale matrix V and degree of freedom ν. Then we have the
following theorem:
Theorem 14.11
The average generalization error of the RRR model for
H ≥H∗is asymptotically expanded as
GE(N) = ⟨GE(D)⟩q(D) = λVBN−1 + O(N−3/2),
where the generalization coefﬁcient is given by
2λVB = (H∗(L + M) −H∗2)
+
/H−H∗

h=1
θ

γ′2
h > max(L, M)
 !
1 −max(L,M)
γ′2
h
"2
γ′2
h
0
q(W)
.
(14.57)
Here γ′2
h is the hth largest eigenvalue of a random matrix W ∈Smin(L,M)
+
subject
to Wishartmin(L,M)−H∗(Imin(L,M)−H∗, max(L, M) −H∗), and θ(·) is the indicator
function such that θ(condition) = 1 if the condition is true and θ(condition) = 0
otherwise.

402
14 Asymptotic VB Theory of Reduced Rank Regression
Proof
Theorem 14.1 and Eqs. (14.42) and (14.48) imply that
γVB
h
=
⎧⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎩
γh + Op(N−1) = Op(1)
for h = 1,. . . , H∗,
max
!
0, γh
!
1 −max(L,M)σ′2
Nγ2
h
""
+ Op(N−1) = Op(N−1/2)
for h = H∗+ 1,. . . , H.
(14.58)
Therefore, we have
####BA
⊤−B∗A∗⊤####
2
Fro =
####
H
h=1γVB
h ωbhω⊤
ah −min(L,M)
h=1
γ∗
hω∗
bhω∗⊤
ah
####
2
Fro
=
####
H∗
h=1

γhωbhω⊤
ah −γ∗
hω∗
bhω∗⊤
ah
 
+ H
h=H∗+1γVB
h ωbhω⊤
ah + Op(N−1)
####
2
Fro
=
####
H∗
h=1

γhωbhω⊤
ah −γ∗
hω∗
bhω∗⊤
ah
 
+ H
h=H∗+1γVB
h ωbhω⊤
ah
####
2
Fro + Op(N−3/2)
=
####V −B∗A∗+ H
h=H∗+1γVB
h ωbhω⊤
ah −min(L,M)
h=H∗+1 γhωbhω⊤
ah
####
2
Fro
+ Op(N−3/2).
Here, in order to get the third equation, we used the fact that the ﬁrst two
terms in the norm in the second equation are of the order of Op(N−1/2). The
expectation over the distribution of training samples is given by
*####BA
⊤−B∗A∗⊤####
2
Fro
+
q(D)
=
*####V −B∗A∗+ H
h=H∗+1γVB
h ωbhω⊤
ah −min(L,M)
h=H∗+1 γhωbhω⊤
ah
####
2
Fro
+
q(D)
+ O(N−3/2)
=

∥V −B∗A∗∥2
Fro

q(D)
+ 2

(V −B∗A∗)⊤H
h=H∗+1γVB
h ωbhω⊤
ah −min(L,M)
h=H∗+1 γhωbhω⊤
ah
 
q(D)
+
H
h=H∗+1(γVB
h )2 −2 H
h=H∗+1 γhγVB
h
+ min(L,M)
h=H∗+1 γ2
h

q(D)
+ O(N−3/2)
=

∥V −B∗A∗∥2
Fro

q(D) + 2
H
h=H∗+1 γhγVB
h
−min(L,M)
h=H∗+1 γ2
h

q(D)
+
H
h=H∗+1(γVB
h )2 −2 H
h=H∗+1 γhγVB
h
+ min(L,M)
h=H∗+1 γ2
h

q(D) + O(N−3/2)
=

∥V −B∗A∗∥2
Fro

q(D) −
min(L,M)
h=H∗+1 γ2
h

q(D)
+
H
h=H∗+1(γVB
h )2
q(D) + O(N−3/2).
(14.59)
Here we used Eq. (14.49) and the orthonormality of the singular vectors.

14.2 Generalization Properties
403
Eq. (14.45) implies that the ﬁrst term in Eq. (14.59) is equal to

∥V −B∗A∗∥2
Fro

q(D) = LM σ′2
N .
(14.60)
The redundant components {γhωbhω⊤
ah}min(L,M)
h=H∗+1 are zero-mean (see Eq. (14.49))
Gaussian matrices capturing the Gaussian noise in the orthogonal space to
the necessary components {γhωbhω⊤
ah}H∗
h=1. Therefore, the distribution of the
corresponding singular values {γh}min(L,M)
h=H∗+1 coincides with the distribution of
the singular values of V′ ∈R(min(L,M)−H∗)×(max(L,M)−H∗) subject to
q(V′) = MGaussmin(L,M)−H∗,max(L,M)−H∗

V′; 0min(L,M)−H∗,max(L,M)−H∗, σ′2
N Imin(L,M)−H∗⊗Imax(L,M)−H∗

.
(14.61)
This leads to
/min(L,M)

h=H∗+1
γ2
h
0
q(D)
= (L −H∗)(M −H∗)σ′2
N .
(14.62)
Let {γ′
h}min(L,M)−H∗
h=1
be the singular values of
√
N
σ′ V′. Then, {γ′2
h }min(L,M)−H∗
h=1
are the eigenvalues of W =
N
σ′2 V′V′⊤, which is subject to Wishartmin(L,M)−H∗
(Imin(L,M)−H∗, max(L, M) −H∗). By substituting Eqs. (14.60), (14.62), and
(14.58) into Eq. (14.59), we have
*####BA
⊤−B∗A∗⊤####
2
Fro
+
q(D)
= σ′2
N {LM −(L −H∗)(M −H∗)}
+
/H
h=H∗+1
2
max
!
0, γh
!
1 −max(L,M)σ′2
Nγ2
h
""320
q(D)
+ O(N−3/2)
= σ′2
N
⎧⎪⎪⎨⎪⎪⎩(H∗(L + M) −H∗2) +
/H−H∗
h=1
2
max
!
0, 1 −max(L,M)
γ′2
h
"32
γ′2
h
0
q(W)
⎫⎪⎪⎬⎪⎪⎭
+ O(N−3/2).
Substituting the preceding into Eq. (14.56) completes the proof.
□
The ﬁrst and the second terms in Eq. (14.57) correspond to the contribution
from the necessary components h = 1,. . . , H∗and the contribution from the
redundant components h = H∗+ 1,. . . , H, respectively. If we focus on the
parameter space of the ﬁrst H∗components, i.e., {ah, bh}H∗
h=1, the true linear
mapping {a∗
h, b∗
h}H∗
h=1 lies at an (essentially) nonsingular point (after removing

404
14 Asymptotic VB Theory of Reduced Rank Regression
the trivial H∗2 redundancy). Therefore, as the regular learning theory states,
the contribution from the necessary components is equal to the (essential)
degree of freedom (see Eq. (14.4)) of the RRR model for H = H∗. On the
other hand, the regular learning theory cannot be applied to the redundant
components {ah, bh}H
h=H∗+1 since the true parameter is on the singularities
{ah = 0} ∪{bh = 0}, making the second term different from the degree of
freedom of the redundant parameters.
Assuming that L and M are large, we can approximate the second term in
Eq. (14.57) by using the random matrix theory (see Section 8.4.1). Consider
the large-scale limit when L, M, H, H∗go to inﬁnity with the same ratio, so that
α = min(L, M) −H∗
max(L, M) −H∗,
(14.63)
β =
H −H∗
min(L, M) −H∗,
(14.64)
κ =
max(L, M)
max(L, M) −H∗
(14.65)
are constant. Then Marˇcenko–Pastur law (Proposition 8.11) states that the
empirical distribution of the eigenvalues {y1,. . . , ymin(L,M)−H∗} of the random
matrix
NV′V′⊤
(max(L,M)−H∗)σ′2
∼
Wishartmin(L,M)−H∗(Imin(L,M)−H∗, 1) almost surely
converges to
p(y) →pMP(y) ≡
.
(y −y)(y −y)
2παy
θ

y < y < y
 
,
(14.66)
where
y = (1 + √α)2,
y = (1 −√α)2.
(14.67)
Let
(2πα)−1J′
s(u) =
 ∞
u
ysp(y)dy
(14.68)
be the sth order (incomplete) moment of the Marˇcenko–Pastur distribution
(14.66) with the lower bound u of the integration range. Then, the second term
of Eq. (14.57) can be written as
/H−H∗

h=1
θ(γ′2
h > max(L, M))
!
1 −max(L,M)
γ′2
h
"2
γ′2
h
0
q(W)
→(min(L, M) −H∗)(max(L, M) −H∗)
 ∞
uβ
θ (y > κ)

1 −κ
y
 2 yp(y)dy
= (min(L, M) −H∗)(max(L, M) −H∗)
 ∞
max(κ,uβ)

y −2κ + κ2y−1 
p(y)dy
= (min(L, M) −H∗)(max(L, M) −H∗)
2πα

J′
1(´u) −2κJ′
0(´u) + κ2J′
−1(´u)
 
,

14.2 Generalization Properties
405
where uβ is the β-percentile point of p(y), i.e.,
β =
 ∞
uβ
p(y)dy = (2πα)−1J′
0(uβ),
(14.69)
and
´u = max(κ, uβ).
(14.70)
Using the transformation z =

y −(y + y)/2
 
/(2 √α), we can derive analytic
forms of the moments (14.68) and thus obtain the following theorem:
Theorem 14.12
The VB generalization coefﬁcient of the RRR model in the
large-scale limit is given by
2λVB →(H∗(L + M) −H∗2)
+ (min(L, M) −H∗)(max(L, M) −H∗)
2πα
,
J1(´z) −2κJ0(´z) + κ2J−1(´z)
-
,
(14.71)
where
J1(z) = 2α(−z
C
1 −z2 + cos−1 z),
J0(z) = −2 √α
C
1 −z2 + (1 + α) cos−1 z −(1 −α) cos−1
√α(1 + α)z + 2α
2αz + √α(1 + α),
J−1(z) =
⎧⎪⎪⎪⎨⎪⎪⎪⎩
2 √α
√
1−z2
2 √αz+1+α −cos−1z+ 1+α
1−α cos−1
√α(1+α)z+2α
2αz+ √α(1+α)
(0 < α < 1),
2
.
1−z
1+z −cos−1 z
(α = 1),
and ´z = max

(κ −(1 + α))/2 √α, J−1
0 (2παβ)
 
. Here J−1
s (·) denotes the inverse
function of Js(z).
Theorem 14.12 allows us to compare the generalization error of VB learning
with those of ML (MAP) learning and Bayesian learning in Section 14.2.6.
14.2.4 Training Error
The training error can be analyzed in a similar way to the generalization error.
We ﬁrst prove the following lemma:
Lemma 14.13
Let U ∈RL,M and Σ ∈SL
+ be the ML estimators of the linear
regression model y = Ux + ε with Gaussian noise ε ∼Gauss(0, Σ). For any
two sets of parameters (U, Σ), ( ´U, ´Σ) such that
U = U + Op(N−1/2),
Σ = Σ + Op(N−1/2),
(14.72)
´U = U + Op(N−1),
´Σ = Σ + Op(N−1),
(14.73)

406
14 Asymptotic VB Theory of Reduced Rank Regression
it holds that
1
N
N

n=1
log
GaussL

y(n); ´Ux(n), ´Σ
 
+ Op(N−3/2)
GaussL

y(n); Ux(n), Σ
 
= Op(N−3/2).
(14.74)
Proof
The (twice of the) left-hand side of Eq. (14.74) can be written as
ψ2 ≡2
N
N

n=1
log
GaussL

y(n); ´Ux(n), ´Σ
 
+ Op(N−3/2)
GaussL

y(n); Ux(n), Σ
 
= log
det
Σ
 
det
 ´Σ
 + 1
N
N

n=1
!
−(y(n) −´Ux(n))⊤´Σ
−1(y(n) −´Ux(n))
+ (y(n) −Ux(n))⊤Σ
−1(y(n) −Ux(n))
"
+ Op(N−3/2)
= −log det
!
ΣΣ
−1Σ
−1 ´Σ
"
+ Op(N−3/2)
−1
N
N

n=1

y(n) −Ux(n) −( ´U −U)x(n) ⊤´Σ
−1 
y(n) −Ux(n) −( ´U −U)x(n) 
+ 1
N
N

n=1

y(n) −Ux(n) −(U −U)x(n) ⊤Σ
−1 
y(n) −Ux(n) −(U −U)x(n) 
= tr
!
log
!
Σ ´Σ
−1"
−log
!
ΣΣ
−1""
+ Op(N−3/2)
−tr
!
Σ ´Σ
−1"
−1
N
N

n=1

( ´U −U)x(n) ⊤´Σ
−1 
( ´U −U)x(n) 
+ tr
!
ΣΣ
−1"
+ 1
N
N

n=1

(U −U)x(n) ⊤Σ
−1 
(U −U)x(n) 
.
By using Eqs. (14.72) and (14.73) and the Taylor expansion of the logarithmic
function, we have
ψ2 = tr
⎛⎜⎜⎜⎜⎜⎝
!
Σ ´Σ
−1 −IL
"
−
!
Σ ´Σ
−1−IL
"⊤!
Σ ´Σ
−1−IL
"
2
−
!
ΣΣ
−1 −IL
"
+
!
ΣΣ
−1−IL
"⊤!
ΣΣ
−1−IL
"
2
⎞⎟⎟⎟⎟⎟⎠
−tr
!
Σ ´Σ
−1"
−1
N
N

n=1

(U −U)x(n) ⊤´Σ
−1 
(U −U)x(n) 
+ tr
!
ΣΣ
−1"
+ 1
N
N

n=1

(U −U)x(n) ⊤Σ
−1 
(U −U)x(n) 
+ Op(N−3/2)
= Op(N−3/2),
which completes the proof.
□

14.2 Generalization Properties
407
When H ≥H∗, Lemma 14.13 can be applied to the plug-in VB predictive
distribution (14.33) and the VB predictive distribution (14.34), where
U = V,
Σ = σ′2
N
N

n=1

y(n) −Vx(n) 
y(n) −Vx(n) ⊤
= σ′2IL + Op(N−1/2),
U = BA
⊤= U + Op(N−1/2),
Σ = σ′2IL = Σ + Op(N−1/2),
´U = ΨBA
⊤= U + Op(N−1),
´Σ = σ′2Ψ = Σ + Op(N−1),
for Ψ = IL + Op(N−1). Here Eq. (14.43) and Lemma 14.7 were used in the
equation for U. Thus, we have the following corollary:
Corollary 14.14
When H ≥H∗, it holds that
1
N
N

n=1
log
p

y(n)|x(n), X, Y
 
p

y(n)|x(n), A, B
 = Op(N−3/2),
and therefore the difference between the training error (13.31) of the VB
predictive distribution (14.34) and the training error of the plug-in VB
predictive distribution (14.33) is of the order of N−3/2, i.e.,
TE(D) = 1
N
N

n=1
log
q(y(n)|x(n))
p(y(n)|x(n), X, Y)
= 1
N
N

n=1
log
q(y(n)|x(n))
p(y(n)|x(n), A, B)
+ Op(N−3/2).
Corollary 14.14 leads to the following theorem:
Theorem 14.15
The training error of the RRR model is written as
TE(D) =
⎧⎪⎪⎪⎨⎪⎪⎪⎩
Θ(1)
if
H < H∗,
####V−BA
⊤####
2
Fro−∥V−B∗A∗⊤∥
2
Fro
2σ′2
+ Op(N−3/2)
if
H ≥H∗.
(14.75)
Proof
When H < H∗, Theorem 14.5 implies that
TE(D) = 1
N
N

n=1
log
q(y(n)|x(n))
p(y(n)|x(n), A, B)
+ Op(N−1)
=
####V −BA
⊤####
2
Fro −
###V −B∗A∗⊤###2
Fro
2σ′2
+ Op(N−1).

408
14 Asymptotic VB Theory of Reduced Rank Regression
With Lemma 14.7, we have TE(D) = Θ(1). When H ≥H∗, we have
TE(D) = −1
N
N
n=1
∥y(n)−B∗A∗⊤x(n)∥
2−
####y(n)−BA
⊤x(n)####
2
2σ′2
+ Op(N−3/2)
= −1
N
N
n=1
∥y(n)−Vx(n)−(B∗A∗⊤−V)x(n)∥
2−
#####y(n)−Vx(n)−
!
BA
⊤−V
"
x(n)
#####
2
2σ′2
+ Op(N−3/2)
= −1
N
N
n=1
∥(B∗A∗⊤−V)x(n)∥
2−
#####
!
BA
⊤−V
"
x(n)
#####
2
2σ′2
+ Op(N−3/2)
= −1
N
N
n=1
tr
)
(B∗A∗⊤−V)x(n)x(n)⊤(B∗A∗⊤−V)
⊤−
!
BA
⊤−V
"
x(n)x(n)⊤
!
BA
⊤−V
"⊤1
2σ′2
+ Op(N−3/2).
By using the prewhitening condition (14.7) and Lemma 14.7, we obtain
Eq. (14.75), which completes the proof.
□
Now we can derive an asymptotic form of the average training error:
Theorem 14.16
The average training error of the RRR model for H ≥H∗is
asymptotically expanded as
TE(N) = ⟨TE(D)⟩q(D) = νVBN−1 + O(N−3/2),
where the training coefﬁcient is given by
2νVB = −(H∗(L + M) −H∗2)
−
/H−H∗

h=1
θ

γ′2
h > max(L, M)
 !
1 −max(L,M)
γ′2
h
" !
1 + max(L,M)
γ′2
h
"
γ′2
h
0
q(W)
.
(14.76)
Here γ′2
h is the hth largest eigenvalue of a random matrix W ∈Smin(L,M)
+
subject
to Wishartmin(L,M)−H∗(Imin(L,M)−H∗, max(L, M) −H∗).
Proof
From Eq. (14.58), we have
####BA
⊤−V
####
2
Fro =
####
H
h=1γVB
h ωbhω⊤
ah −min(L,M)
h=1
γhωbhω⊤
ah
####
2
Fro
=
####
H
h=H∗+1(γVB
h
−γh)ωbhω⊤
ah −min(L,M)
h=H+1
γhωbhω⊤
ah + Op(N−1)
####
2
Fro
=
####
H
h=H∗+1(γVB
h
−γh)ωbhω⊤
ah −min(L,M)
h=H+1
γhωbhω⊤
ah
####
2
Fro + Op(N−3/2)
= H
h=H∗+1(γVB
h
−γh)2 + min(L,M)
h=H+1
γ2
h + Op(N−3/2)
= H
h=H∗+1
!
max
!
0, γh
!
1 −max(L,M)σ′2
Nγ2
h
""
−γh
"2
+ min(L,M)
h=H+1
γ2
h + Op(N−3/2)

14.2 Generalization Properties
409
= −H
h=H∗+1 θ

γ2
h > max(L,M)σ′2
N
 
·

γh −max(L,M)σ′2
Nγh
 
γh + max(L,M)σ′2
Nγh
 
+ min(L,M)
h=H∗+1 γ2
h + Op(N−3/2).
(14.77)
By using Eqs. (14.60), (14.62) and (14.77), we have
*####V −BA
⊤####
2
Fro −
###V −B∗A∗⊤###2
Fro
+
q(D)
= −σ′2
N
,
(H∗(L + M) −H∗2)
+ H
h=H∗+1 θ

γ2
h > max(L,M)σ′2
N
 
·

γh −max(L,M)σ′2
Nγh
 
γh + max(L,M)σ′2
Nγh
 -
+ Op(N−3/2).
Thus, by introducing the singular values {γ′
h}min(L,M)−H∗
h=1
of
√
N
σ′ V′, where V′ is
a random matrix subject to Eq. (14.61), and using Theorem 14.15, we obtain
Eq. (14.76), which completes the proof.
□
Finally, we apply the Marˇcenko–Pastur law (Proposition 8.11) for evaluat-
ing the second term in Eq. (14.76). In the large-scale limit when L, M, H, H∗
go to inﬁnity with the same ratio, so that Eqs. (14.63) through (14.65) are
constant, we have
/H−H∗

h=1
θ

γ′2
h > max(L, M)
 !
1 −max(L,M)
γ′2
h
" !
1 + max(L,M)
γ′2
h
"
γ′2
h
0
q(W)
→(min(L, M) −H∗)(max(L, M) −H∗)
 ∞
uβ
θ (y > κ)

1 −κ
y
 
1 + κ
y
 
yp(y)dy
= (min(L, M) −H∗)(max(L, M) −H∗)
 ∞
max(κ,uβ)

y −κ2y−1 
p(y)dy
= (min(L, M) −H∗)(max(L, M) −H∗)
2πα

J′
1(´u) −κ2J′
−1(´u)
 
,
where J′
s(u), β, and ´u are deﬁned in Eqs. (14.68), (14.69), and (14.70),
respectively. Thus, the transformation z =

y −(y + y)/2
 
/(2 √α) gives the
following theorem:
Theorem 14.17
The VB training coefﬁcient of the RRR model in the large
scale limit is given by
2νVB →−(H∗(L + M) −H∗2)
−(min(L, M) −H∗)(max(L, M) −H∗)
2πα
,
J1(´z) −κ2J−1(´z)
-
,
(14.78)
where J1(z), J−1(z), and ´z are deﬁned in Theorem 14.12.

410
14 Asymptotic VB Theory of Reduced Rank Regression
14.2.5 Free Energy
The VB free energy can be analyzed relatively easily based on the orders of
the variational parameters, given by Corollary 14.3:
Theorem 14.18
The relative VB free energy (13.150) of the RRR model for
H ≥H∗is asymptotically expanded as
FVB(D) = FVB(Y|X) −NS N(Y|X) = λ′VB log N + Op(1),
(14.79)
where the free energy coefﬁcient is given by
2λ′VB = H∗(L + M) + (H −H∗) min(L, M).
(14.80)
Proof
The VB free energy for the RRR model is given by Eq. (14.14), and
the empirical entropy is given by
2S N(Y|X) = −2
N
N

n=1
log q(y(n)|x(n))
= L log(2πσ′2) +
N
n=1
###y(n) −B∗A∗⊤x(n)###2
Nσ′2
= L log(2πσ′2) +
1
N
N
n=1
###y(n)###2 −2tr(V⊤B∗A∗⊤) +
###B∗A∗⊤###2
Fro
σ′2
= L log(2πσ′2) +
1
N
N
n=1
###y(n)###2 +
###V −B∗A∗⊤###2
Fro −∥V∥2
Fro
σ′2
.
Therefore, the relative VB free energy (14.79) is given as
2FVB(D) = N ·
####V−BA
⊤####
2
Fro−∥V−B∗A∗⊤∥
2
Fro
σ′2
+ M log det(CA)
det
ΣA
 + L log det(CB)
det
ΣB
 
−(L + M)H + tr
2
C−1
A
!
A
⊤A + MΣA
"
+ C−1
B
!
B
⊤B + LΣB
"
+ Nσ′−2 !
−A
⊤AB
⊤B +
!
A
⊤A + MΣA
" !
B
⊤B + LΣB
""3
.
(14.81)
Eqs. (14.52) and (14.43) imply that the ﬁrst term in Eq. (14.81) is Op(1).
Corollary 14.3 with Eqs. (14.42) and (14.48) implies that, for h = 1,. . . , H∗,
ah =Θp(1),
bh =Θp(1),
σ2
ah =Θp(N−1),
σ2
bh =Θp(N−1),
and, for h = H∗+ 1,. . . , H,
ah =Op(1),
bh =Op(N−1/2), σ2
ah =Θp(1),
σ2
bh =Θp(N−1),
(if L< M),
ah =Op(N−1/4), bh =Op(N−1/4), σ2
ah =Θp(N−1/2), σ2
bh =Θp(N−1/2), (if L= M),
ah =Op(N−1/2), bh =Op(1),
σ2
ah =Θp(N−1),
σ2
bh =Θp(1),
(if L> M).

14.2 Generalization Properties
411
These results imply that the most terms in Eq. (14.81) are Op(1), and we thus
have
2FVB(D) = M log det (CA)
det
ΣA
 + L log det (CB)
det
ΣB
 + Op(1)
= M log
H

h=1
σ−2
ah + L log
H

h=1
σ−2
bh + Op(1)
= {H∗(L + M) + (H −H∗) min(L, M)} log N + Op(1),
which completes the proof.
□
Clearly from the proof, the ﬁrst term and the second term in Eq. (14.80) cor-
respond to the contribution from the necessary components, h = 1,. . . , H∗, and
the contribution from the redundant components, h = H∗,. . . , H, respectively.
A remark is that the contribution from the necessary components contains the
trivial redundancy, i.e., it is H∗(L + M) instead of H∗(L + M) −H∗2. This
is because the independence between A and B prevents the VB posterior
distribution from extending along the trivial redundancy.
14.2.6 Comparison with Other Learning Algorithms
Theorems 14.12, 14.17, and 14.18 allow us to compute the generalization, the
training, and the free energy coefﬁcients of VB learning. We can now compare
those properties with those of ML learning and Bayesian learning, which have
been clariﬁed for the RRR model. Note that MAP learning with a smooth
and ﬁnite prior (e.g., the Gaussian prior (14.3)) with ﬁxed hyperparameters
is asymptotically equivalent to ML learning, and has the same generalization
and training coefﬁcients.
ML Learning
The generalization error of ML learning in the RRR model was analyzed
(Fukumizu, 1999), based on the Marˇcenko–Pastur law (Proposition 8.11). Let
γ′2
h be the hth largest eigenvalue of a random matrix W ∈Smin(L,M)
+
subject to
Wishartmin(L,M)−H∗(Imin(L,M)−H∗, max(L, M) −H∗).
Theorem 14.19
(Fukumizu, 1999) The average ML generalization error of
the RRR model for H ≥H∗is asymptotically expanded as
GE
ML(N) = λMLN−1 + O(N−3/2),
where the generalization coefﬁcient is given by
2λML = (H∗(L + M) −H∗2) +
/H−H∗

h=1
γ′2
h
0
q(W)
.
(14.82)

412
14 Asymptotic VB Theory of Reduced Rank Regression
Theorem 14.20
(Fukumizu, 1999) The ML generalization coefﬁcient of the
RRR model in the large-scale limit is given by
2λML →(H∗(L + M) −H∗2)
+ (min(L, M) −H∗)(max(L, M) −H∗)
2πα
J1(´z),
(14.83)
where J1(·) and ´z are deﬁned in Theorem 14.12.
Actually, Theorems 14.11 and 14.12 were derived by extending Theorems
14.19 and 14.20 to VB learning. We can derive Theorems 14.19 and 14.20 in
the same way as VB learning by replacing the VB estimator (14.58) with the
ML estimator γML
h
= γh.
The training error can be similarly analyzed.
Theorem 14.21
The average ML training error of the RRR model for H ≥H∗
is asymptotically expanded as
TE
ML(N) = νMLN−1 + O(N−3/2),
where the training coefﬁcient is given by
2νML = −(H∗(L + M) −H∗2) −
/H−H∗

h=1
γ′2
h
0
q(W)
.
(14.84)
Theorem 14.22
The ML training coefﬁcient of the RRR model in the large-
scale limit is given by
2νML →−(H∗(L + M) −H∗2)
−(min(L, M) −H∗)(max(L, M) −H∗)
2πα
J1(´z),
(14.85)
where J1(·) and ´z are deﬁned in Theorem 14.12.
A note is that Theorems 14.19 and 14.21 imply that the generalization
coefﬁcient and the training coefﬁcient are antisymmetric in ML learning,
i.e., λML = −νML, while they are not antisymmetric in VB learning, i.e.,
λVB  −νVB (see Theorems 14.11 and 14.16).
Bayesian Learning
The Bayes free energy in the RRR model was clariﬁed based on the singular
learning theory (see Section 13.5.4).

14.2 Generalization Properties
413
Theorem 14.23
(Aoyagi and Watanabe, 2005) The relative Bayes free energy
(13.32) in the RRR model is asymptotically expanded as
FBayes(D) = FBayes(Y|X) −NS N(Y|X)
= λ′Bayes log N −(m −1) log log N + Op(1),
where the free energy coefﬁcient, as well as the coefﬁcient of the second leading
term, is given as follows:
(i) When L + H∗≤M + H, M + H∗≤L + H, and H∗+ H ≤L + M:
(a) If L + M + H + H∗is even, then m = 1 and
2λ′Bayes = −(H∗+ H)2 −(L −M)2 + 2(H∗+ H)(L + M)
4
.
(b) If L + M + H + H∗is odd, then m = 2 and
2λ′Bayes = −(H∗+ H)2 −(L −M)2 + 2(H∗+ H)(L + M) + 1
4
.
(ii) When M + H < L + H∗, then m = 1 and
2λ′Bayes = HM −HH∗+ LH∗.
(iii) When L + H < M + H∗, then m = 1 and
2λ′Bayes = HL −HH∗+ MH∗.
(iv) When L + M < H + H∗, then m = 1 and
2λ′Bayes = LM.
Theorem 14.23 immediately informs us of the asymptotic behavior of the
Bayes generalization error, based on Corollary 13.14.
Theorem 14.24
(Aoyagi and Watanabe, 2005) The Bayes generalization
error of the RRR model for H ≥H∗is asymptotically expanded as
GE
Bayes(N) = λBayesN−1 −(m −1)(N log N)−1 + o

(N log N)−1 
,
where λBayes = λ′Bayes and m are given in Theorem 14.23.
Unfortunately, the Bayes training error has not been clariﬁed yet.
Numerical Comparison
Let us visually compare the theoretically clariﬁed generalization properties.
Figures 14.1 through 14.4 show the generalization coefﬁcients and the training
coefﬁcients of the RRR model under the following settings:

414
14 Asymptotic VB Theory of Reduced Rank Regression
–2
–1.5
–1
–0.5
0
0.5
1
1.5
2
0
5
10
15
20
25
30
35
40
VB
ML
Bayes
Regular
Figure 14.1 The generalization coefﬁcients (in the positive vertical region) and
the training coefﬁcients (in the negative vertical region) of VB learning, ML
learning, and Bayesian learning in the RRR model with max(L, M) = 50,
min(L, M) = 30, H = 1,. . . , 30, and H∗= 0.
–2
–1.5
–1
–0.5
0
0.5
1
1.5
2
0
20
40
60
80
100
VB
ML
Bayes
Regular
Figure 14.2 The generalization coefﬁcients and the training coefﬁcients
(max(L, M) = 80, min(L, M) = 1,. . . , 80, H = 1, and H∗= 0).
(i) max(L, M) = 50, min(L, M) = 30, H = 1,. . . , 30 (horizontal axis),
H∗= 0,
(ii) max(L, M) = 80, min(L, M) = 1,. . . , 80 (horizontal axis), H = 1, H∗= 0,
(iii) L = M = 80, H = 1,. . . , 80 (horizontal axis), H∗= 0,
(iv) max(L, M) = 50, min(L, M) = 30, H = 20, H∗= 1,. . . , 20
(horizontal axis).
The vertical axis indicates the coefﬁcient normalized by the half of the
essential parameter dimension D, given by Eq. (14.4). The curves in the
positive vertical region correspond to the generalization coefﬁcients of VB
learning, ML learning, and Bayesian learning, while the curves in the negative
vertical region correspond to the training coefﬁcients. As a guide, we depicted
the lines 2λ/D = 1 and 2ν/D = −1, which correspond to the generalization and
the training coefﬁcients (by ML learning and Bayesian learning) of the regular

14.2 Generalization Properties
415
–2
–1.5
–1
–0.5
0
0.5
1
1.5
2
0
20
40
60
80
100
VB
ML
Bayes
Regular
Figure 14.3 The generalization coefﬁcients and the training coefﬁcients
(L = M = 80, H = 1,. . . , 80, and H∗= 0).
–2
–1.5
–1
–0.5
0
0.5
1
1.5
2
0
5
10
15
20
25
30
VB
ML
Bayes
Regular
Figure 14.4 The generalization coefﬁcients and the training coefﬁcients
(max(L, M) = 50, min(L, M) = 30, H = 20, and H∗= 1,. . . , 20).
models with the same parameter dimensionality. The curves for ML learning
and VB learning were computed under the large-scale approximation, i.e., by
using Theorems 14.12, 14.17, 14.20, and 14.22.1
We see in Figures 14.1 through 14.4 that VB learning generally pro-
vides comparable generalization performance to Bayesian learning. However,
signiﬁcant differences are also observed. For example, we see in Figure
14.1 that VB learning provides much worse generalization performance than
Bayesian learning when H ≪min(L, M), and much better performance when
H ∼min(L, M).
Another ﬁnding is that, in Figures 14.1 and 14.3, the VB generalization
coefﬁcient depends on H similarly to the ML generalization coefﬁcient.
Moreover, we see that, when min(L, M) = 80 in Figure 14.2 and when H = 1
in Figure 14.3, the VB generalization coefﬁcient slightly exceeds the line
1 We conﬁrmed that numerical computation with Theorems 14.11, 14.16, 14.19, and 14.21 gives
visually indistinguishable results.

416
14 Asymptotic VB Theory of Reduced Rank Regression
2λ/D = 1—the VB generalization coefﬁcient per parameter dimension can
be larger than that in the regular models, which never happens for the Bayes
generalization coefﬁcient (see Eq. (13.125)).
Finally, Figure 14.4 shows that, for this particular RRR model with
max(L, M) = 50, min(L, M) = 30, and H = 20, VB learning always gives
smaller generalization error than Bayesian learning in the asymptotic limit,
regardless of the true rank H∗. This might be seen contradictory with the
proven optimality of Bayesian learning—Bayesian learning is never dominated
by any other method (see Appendix D for the optimality of Bayesin learning
and Appendix A for the deﬁnition of the term “domination”). We further
discuss this issue by considering subtle true singular values in Section 14.2.7.
Next we compare the VB free energy with the Bayes free energy, by using
Theorems 14.18 and 14.23. Figures 14.5 through 14.8 show the free energy
0
0.5
1
1.5
2
0
5
10
15
20
25
30
35
40
VB
Bayes
Regular
Figure 14.5 Free energy coefﬁcients (max(L, M)
=
50, min(L, M)
=
30,
H = 1,. . . , 30, and H∗= 0). The VB and the Bayes free energy coefﬁcients
are almost overlapped.
0
0.5
1
1.5
2
0
20
40
60
80
100
VB
Bayes
Regular
Figure 14.6 Free energy coefﬁcients (max(L, M) = 80, min(L, M) = 1,. . . , 80,
H = 1, and H∗= 0). The VB and the Bayes free energy coefﬁcients are almost
overlapped.

14.2 Generalization Properties
417
0
0.5
1
1.5
2
0
20
40
60
80
100
VB
Bayes
Regular
Figure 14.7 Free energy coefﬁcients (L = M = 80, H = 1,. . . , 80, and H∗= 0).
0
0.5
1
1.5
2
0
5
10
15
20
25
30
VB
Bayes
Regular
Figure 14.8 Free energy coefﬁcients (max(L, M) = 50, min(L, M) = 30, H = 20,
and H∗= 1,. . . , 20).
coefﬁcients of the RRR model with the same setting as Figures 14.1 through
14.4, respectively. As for the generalization and the training coefﬁcients, the
vertical axis indicates the free energy coefﬁcient normalized by the half of
the essential parameter dimensionality D, given by Eq. (14.4). The curves
correspond to the VB free energy coefﬁcient (Theorem 14.18), the Bayes
free energy coefﬁcient (Theorem 14.23), and the Bayes free energy coefﬁcient
2λ′Bayes
Regular = D of the regular models with the same parameter dimensionality.
We ﬁnd that the VB free energy almost coincides with the Bayes free energy in
Figures 14.5 and 14.6, while the VB free energy is much larger than the Bayes
free energy in Figures 14.7 and 14.8.
Since the gap between the VB free energy and the Bayes free energy
indicates how well the VB posterior approximates the Bayes posterior in terms
of the KL divergence (see Section 13.6), our observation is not exactly what
we would expect. For example, we see in Figure 14.1 that the generalization
performance of VB learning is signiﬁcantly different from Bayesian learning
(when H ≪min(L, M) and when H ∼min(L, M)), while the free energies in

418
14 Asymptotic VB Theory of Reduced Rank Regression
0
0.5
1
1.5
2
0
20
40
60
80
100
VB
Bayes
Regular
Figure 14.9 Free energy coefﬁcients (max(L, M) = 80, min(L, M) = 10,. . . , 80,
H = 10, and H∗= 0).
0
0.5
1
1.5
2
0
20
40
60
80
100
VB
Bayes
Regular
Figure 14.10 Free energy coefﬁcients (max(L, M) = 80, min(L, M) = 20,. . . , 80,
H = 20, and H∗= 0).
Figure 14.5 imply that the VB posterior well approximates the Bayes posterior.
Also, by comparing Figures 14.3 and 14.7, we observe that, when H ≪
min(L, M), VB learning provides much worse generalization performance than
Bayesian learning, while the VB free energy well approximates the Bayes free
energy; and that, when H ∼min(L, M), VB learning provides much better
generalization performance, while the VB free energy is signiﬁcantly larger
than the Bayes free energy. Further investigation is required to understand the
relation between the generalization performance and the gap between the VB
and the Bayes free energies.
Figures 14.9 through 14.11 show similar cases to Figure 14.6 but for
different ranks H = 10, 20, 40, respectively. From Figures 14.5 through 14.11,
we conclude that, in general, the VB free energy behaves similarly to the
Bayes free energy when L and M are signiﬁcantly different from each other
or H ≪min(L, M). In Figure 14.8, the VB free energy behaves strangely and
poorly approximates the Bayes free energy when H∗is large. This is because

14.2 Generalization Properties
419
0
0.5
1
1.5
2
0
20
40
60
80
100
VB
Bayes
Regular
Figure 14.11 Free energy coefﬁcients (max(L, M) = 80, min(L, M) = 40,. . . , 80,
H = 40, and H∗= 0).
of the trivial redundancy of the RRR model, of which VB learning with the
independence constraint cannot make use to reduce the free energy (see the
remark in the last paragraph of Section 14.2.5).
14.2.7 Analysis with Subtle True Singular Values
Here we conduct an additional analysis to explain the seemingly contradictory
observation in Figure 14.4—in the RRR model with max(L, M)
=
50,
min(L, M) = 30, H = 20, VB learning always gives smaller generalization
error than Bayesian learning, regardless of the true rank H∗. We show that this
does not mean the domination by VB learning over Bayesian learning, which
was proven to be never dominated by any other method (see Appendix D).
Distinct and Subtle Signal Assumptions
The contradictory observation was due to the assumption (14.42) on the true
singular values:
γ∗
h =
⎧⎪⎪⎨⎪⎪⎩
Θ(1)
for
h = 1,. . . , H∗,
0
for
h = H∗+ 1,. . . , min(L, M),
(14.86)
which we call the distinct signal assumption. This assumption seems to cover
any true linear mapping B∗A∗⊤= H
h=1 γ∗
hω∗
bhω∗
ah by classifying all singular
components such that γ∗
h > 0 to the necessary components h = 1,. . . , H∗, and
the other components such that γ∗
h = 0 to the redundant components h = H∗+
1,. . . , min(L, M). However, in the asymptotic limit, the assumption (14.86)
implicitly prohibits the existence of true singular values in the same order as
the noise contribution, i.e., γ∗
h = Θp(N−1/2). In other words, the distinct signal
assumption (14.86) considers all true singular values to be either inﬁnitely

420
14 Asymptotic VB Theory of Reduced Rank Regression
larger than the noise or exactly equal to zero. As a result, asymptotic analysis
under the distinct signal assumption reﬂects only the overﬁtting tendency of
a learning machine, and ignores the underﬁtting tendency, which happens
when the signal is not clearly separable from the noise. Since overﬁtting and
underﬁtting are in the trade-off relation, it is important to investigate both
tendencies when generalization performance is analyzed.
To relax the restriction discussed previously, we replace the assumption
(14.42) with
γ∗
h =
⎧⎪⎪⎨⎪⎪⎩
Θ(1)
for
h = 1,. . . , H∗,
O(N−1/2)
for
h = H∗+ 1,. . . , min(L, M),
(14.87)
which we call the subtle signal assumption, in the following analysis (Watan-
abe and Amari, 2003; Nakajima and Watanabe, 2007). Note that, with the
assumption (14.87), we do not intend to analyze the case where the true
singular values depend on N. Rather, we assume realistic situations where the
number of necessary components H∗depends on N. Let us keep in mind the
following two points, which are usually true when we analyze real-world data:
• The number N of samples is always ﬁnite.
Asymptotic theory is not to investigate what happens when N →∞, but
to approximate the situation where N is ﬁnite but large.
• It rarely happens that real-world data can be exactly expressed by a
low-rank model.
Statistical models are supposed to be simpler than the real-world data
generation process, but expected to approximate it with certain accuracy,
and the accuracy depends on the noise level and the number of samples.
Then we expect that, for most real-world data, it holds that γ∗
h > 0 for all
h = 1,. . . , min(L, M), but, given ﬁnite N, some of the true singular values
are comparable to the noise contribution γ∗
h = Θ(N−1/2), and some others are
negligible γ∗
h = o(N−1/2). The subtle signal assumption (14.87) covers such
realistic situations.
Generalization Error under Subtle Signal Assumption
Replacing the distinct signal assumption (14.86) with the subtle signal assump-
tion (14.87) does not affect the discussion up to Theorem 14.10, i.e., Theorems
14.1, 14.5, and 14.10, Lemmas 14.6 through 14.8, and their corollaries still
hold. Instead of Theorem 14.11, we have the following theorem:
Theorem 14.25
Under the subtle signal assumption (14.87), the average gen-
eralization error of the RRR model for H ≥H∗is asymptotically expanded as

14.2 Generalization Properties
421
GE(N) = λVBN−1 + O(N−3/2),
where the generalization coefﬁcient is given by
2λVB = (H∗(L + M) −H∗2) +
N
σ′2
min(L,M)
h=H∗+1 γ∗2
h
+
/ H−H∗
h=1
θ

γ′′2
h
> max(L, M)
 
·
)!
1 −max(L,M)
γ′′2
h
"2
γ′′2
h −2
!
1 −max(L,M)
γ′′2
h
"
γ′′
h ω′′⊤
bh V′′∗ω′′
ah
1 0
q(V′′)
.
(14.88)
Here,
V′′ =
min(L,M)−H∗

h=1
γ′′
h ω′′
bhω′′⊤
ah
(14.89)
is the SVD of a random matrix V′′ ∈R(min(L,M)−H∗)×(max(L,M)−H∗) subject to
q(V′′) = MGaussmin(L,M)−H∗,max(L,M)−H∗$V′′; V′′∗, Imin(L,M)−H∗⊗Imax(L,M)−H∗% ,
(14.90)
and V′′∗∈R(min(L,M)−H∗)×(max(L,M)−H∗) is a (nonsquare) diagonal matrix with
the diagonal entries given by V′′
h,h =
√
N
σ′ γ∗
H∗+h for h = 1,. . . , min(L, M) −H∗.
Proof
From Eq. (14.58), we have
####BA
⊤−B∗A∗⊤####
2
Fro =
###H
h=1γVB
h ωbhω⊤
ah −B∗A∗⊤###2
Fro
=
###H∗
h=1 γhωbhω⊤
ah −B∗A∗⊤+ H
h=H∗+1γVB
h ωbhω⊤
ah
###2
Fro + Op(N−3/2)
=
####V −B∗A∗+ H
h=H∗+1γVB
h ωbhω⊤
ah −min(L,M)
h=H∗+1 γhωbhω⊤
ah
####
2
Fro
+ Op(N−3/2),
and therefore,
*####BA
⊤−B∗A∗⊤####
2
Fro
+
q(D)
=
*####V −B∗A∗+ H
h=H∗+1γVB
h ωbhω⊤
ah −min(L,M)
h=H∗+1 γhωbhω⊤
ah
####
2
Fro
+
q(D)
+ O(N−3/2)
=

∥V −B∗A∗∥2
Fro

q(D)
+ 2

(V −B∗A∗)⊤H
h=H∗+1γVB
h ωbhω⊤
ah −min(L,M)
h=H∗+1 γhωbhω⊤
ah
 
q(D)

422
14 Asymptotic VB Theory of Reduced Rank Regression
+
H
h=H∗+1(γVB
h )2 −2 H
h=H∗+1 γhγVB
h
+ min(L,M)
h=H∗+1 γ2
h

q(D)
+ O(N−3/2)
=

∥V −B∗A∗∥2
Fro

q(D)
+ 2
/ H
h=H∗+1(γhωbhω⊤
ah −γ∗
hω∗
bhω∗⊤
ah )⊤γVB
h ωbhω⊤
ah
−min(L,M)
h=H∗+1 (γhωbhω⊤
ah −γ∗
hω∗
bhω∗⊤
ah )⊤γhωbhω⊤
ah
0
q(D)
+
H
h=H∗+1(γVB
h )2 −2 H
h=H∗+1 γhγVB
h
+ min(L,M)
h=H∗+1 γ2
h

q(D)
+ O(N−3/2)
=

∥V −B∗A∗∥2
Fro

q(D) −2
H
h=H∗+1(γ∗
hω∗
bhω∗⊤
ah )⊤γVB
h ωbhω⊤
ah

q(D)
+ 2
min(L,M)
h=H∗+1 (γ∗
hω∗
bhω∗⊤
ah )⊤γhωbhω⊤
ah

q(D)
+
H
h=H∗+1(γVB
h )2
q(D) −
min(L,M)
h=H∗+1 γ2
h

q(D) + O(N−3/2)
=

∥V −B∗A∗∥2
Fro

q(D) −
*min(L,M)
h=H∗+1
####γhωbhω⊤
ah −γ∗
hω∗
bhω∗⊤
ah
####
2
Fro
+
q(D)
−2
H
h=H∗+1(γ∗
hω∗
bhω∗⊤
ah )⊤γVB
h ωbhω⊤
ah

q(D)
+
H
h=H∗+1(γVB
h )2
q(D) + min(L,M)
h=H∗+1 γ∗2
h + O(N−3/2)
= σ′2
N

LM −(L −H∗)(M −H∗) +
N
σ′2
min(L,M)
h=H∗+1 γ∗2
h
 
+
H
h=H∗+1(γVB
h )2
q(D) −2
H
h=H∗+1(γ∗
hω∗
bhω∗⊤
ah )⊤γVB
h ωbhω⊤
ah

q(D)
+ O(N−3/2).
(14.91)
In
the
orthogonal
space
to
the
distinctly
necessary
components
{γh, ωah, ωbh}H∗
h=1, the distribution of {γh, ωah, ωbh}min(L,M)
h=H∗+1
coincides with
the distribution of { σ′2
√
N γ′′
h , ω′′
ah, ω′′
bh}min(L,M)−H∗
h=1
, deﬁned in Eq. (14.89),
with V′′∗as the true matrix for subtle or the redundant components,
h = H∗+ 1,. . . , min(L, M). By using Eq. (14.58), we thus have
*####BA
⊤−B∗A∗⊤####
2
Fro
+
q(D)
= σ′2
N

(H∗(L + M) −H∗2) +
N
σ′2
min(L,M)
h=H∗+1 γ∗2
h
+
/ H−H∗
h=1
θ

γ′′2
h
> max(L, M)
 

14.2 Generalization Properties
423
·
)!
1 −max(L,M)
γ′′2
h
"2
γ′′2
h −2
!
1 −max(L,M)
γ′′2
h
"
γ′′
h ω′′⊤
bh V′′∗ω′′
ah
1 0
q(V′′)

+ O(N−3/2),
which completes the proof.
□
Training Error under Subtle Signal Assumption
The training error can be analyzed more easily.
Theorem 14.26
Under the subtle signal assumption (14.87), the average
training error of the RRR model for H ≥H∗is asymptotically expanded as
TE(N) = νVBN−1 + O(N−3/2),
where the training coefﬁcient is given by
2νVB = −(H∗(L + M) −H∗2) +
N
σ′2
min(L,M)
h=H∗+1 γ∗2
h
+
/ H−H∗
h=1
θ

γ′′2
h
> max(L, M)
 
·
!
1 −max(L,M)
γ′′2
h
" !
1 + max(L,M)
γ′′2
h
"
γ′′2
h
0
q(V′′)
.
(14.92)
Here V′′ and {γ′′
h } are deﬁned in Theorem 14.25.
Proof
Theorem 14.15 and Eq. (14.77) still hold under the assumption (14.87).
Therefore,
####V −BA
⊤####
2
Fro
= −H
h=H∗+1 θ

γ2
h > max(L,M)σ′2
N
 
·

γh −max(L,M)σ′2
Nγh
 
γh + max(L,M)σ′2
Nγh
 
+ min(L,M)
h=H∗+1 (γh −γ∗
h)2 + min(L,M)
h=H∗+1 γ∗2
h + Op(N−3/2),
and
*####V −BA
⊤####
2
Fro −
###V −B∗A∗⊤###2
Fro
+
q(D)
= −σ′2
N
)
(H∗(L + M) −H∗2) −N
σ′2
min(L,M)

h=H∗+1
γ∗2
h
+ H
h=H∗+1 θ

γ2
h > max(L,M)σ′2
N
 
·

γh −max(L,M)σ′2
Nγh
 
γh + max(L,M)σ′2
Nγh
 1
+ Op(N−3/2).
Substituting the preceding equation into Eq. (14.75) and using the random
matrix V′′ and its singular values {γ′′
h }, deﬁned in Theorem 14.25, we obtain
Eq. (14.92).
□

424
14 Asymptotic VB Theory of Reduced Rank Regression
–2
–1.5
–1
–0.5
0
0.5
1
1.5
2
0
2
4
6
8
10
12
14
16
18
VB
ML
Regular
Figure 14.12 The generalization coefﬁcients and the training coefﬁcients under
the subtle signal assumption (14.87) in the RRR model with max(L, M) = 50,
min(L, M) = 30, H = 20, and H∗= 5.
Comparison with Other Learning Algorithms
Figure 14.12 shows the generalization coefﬁcients and the training coefﬁcients,
computed by using Theorems 14.25 and 14.26, respectively, as functions of a
rescaled subtle true singular value
√
Nγ∗
h/σ′. The considered RRR model is
with max(L, M) = 50, min(L, M) = 30, and H = 20, and the true linear
mapping is assumed to consist of H∗= 5 distinctly necessary components
(γ∗
h = Θ(1) for h = 1,. . . , 5), 10 subtle components (γ∗
h = Θ(N−1/2) for
h = 6,. . . , 15), and the other ﬁve null components (γ∗
h = 0 for h = 16,. . . , 20).
The subtle singular values are assumed to be identical, γ∗
h = γ∗for h =
6,. . . , 15, and the horizontal axis indicates
√
Nγ∗/σ′. The generalization
coefﬁcients and the training coefﬁcients of ML learning can be derived in the
same way as Theorems 14.25 and 14.26 with the VB estimator γVB
h
replaced
with the ML estimator γML
h
= γh. Unfortunately, the generalization error nor
the training error of Bayesian learning under the subtle signal assumption for
the general RRR model has not been clariﬁed.
Only in the case where L = H = 1, the Bayes generalization error under the
subtle signal assumption has been analyzed.
Theorem 14.27
(Watanabe and Amari, 2003) The Bayes generalization error
of the RRR model with M ≥2, L = H = 1 under the assumption that the true
mapping is b∗a∗= O(N−1/2) is asymptotically expanded as
GE
Bayes(N) = λBayesN−1 + o(N−1),
where the generalization coefﬁcient is given by
2λBayes = 1 +
*!####
√
N
σ′ b∗a∗####
2
+
√
N
σ′ b∗a∗⊤v
"
ΦM(v)
ΦM−2(v)
+
q(v) .
(14.93)

14.2 Generalization Properties
425
–2
–1.5
–1
–0.5
0
0.5
1
1.5
2
0
2
4
6
8
10
12
14
16
18
VB
ML(=Regular)
Bayes
Figure 14.13 The generalization coefﬁcients and the training coefﬁcients under
the subtle signal assumption (14.87) in the RRR model with M = 5, L = H = 1,
and H∗= 0.
Here,
ΦM(v) =
 π/2
0
sinM θ exp
!
−1
2
####
√
N
σ′ b∗a∗+ v
####
2
sin2 θ
"
dθ,
and v ∈RM is a random vector subject to q(v) = GaussM(v; 0, IM).
Figure 14.13 compares the generalization coefﬁcients when M = 5, L =
H = 1, and H∗= 0, where the horizontal axis corresponds to a rescaled
subtle true singular value
√
Nγ∗/σ′ =
√
N ∥b∗a∗∥/σ′.2 We see that the
generalization error of VB learning is smaller than that of Bayesian learning
when
√
Nγ∗/σ′ = 0, and identical when
√
Nγ∗/σ′ →∞. This means that,
under the distinct signal assumption (14.86), which considers only the case
where γ∗= 0 (i.e., H∗= 0) or γ∗= Θ(1) (i.e., H∗= 1), VB learning
always performs better than Bayesian learning. However, we can see in Figure
14.13 that, when
√
Nγ∗/σ′ ≈3, Bayesian learning outperforms VB learning.
Figure 14.13 simply implies that VB learning is more strongly regularized
than Bayesian learning, or in other words, VB learning tends to underﬁt subtle
signals such that γ∗= Θ(N−1/2), while Bayesian learning tends to overﬁt noise.
Knowing the proved optimality of Bayesian learning (Appendix D), we
would expect that the same happens in Figure 14.12, where the limits
√
Nγ∗/σ′ = 0 and
√
Nγ∗/σ′ →∞correspond to the cases with H∗= 5 and
H∗= 15, respectively, in Figure 14.4 under the distinct signal assumption
2 When L = H = 1, the parameter transformation ba →w makes the RRR model identiﬁable, and
therefore the ML generalization coefﬁcient is identical to that of the regular models. This is the
reason why only the integration effect or model induced-regularization was observed in the
one-dimensional matrix factorization model in Section 7.2 and Section 7.3.3. The basis
selection effect appears only when a singular model cannot be equivalently transformed to a
regular model (see the discussion in Section 14.3).

426
14 Asymptotic VB Theory of Reduced Rank Regression
(14.86). Namely, if we could depict the Bayes generalization coefﬁcient
in Figure 14.12, there should be some interval where Bayesian learning
outperforms VB learning.
14.3 Insights into VB Learning
In this chapter, we analyzed the generalization error, the training error, and the
free energy of VB learning in the RRR model, and derived their asymptotic
forms. We also introduced theoretical results providing those properties for
ML learning and Bayesian learning. As mentioned in Section 13.5, the RRR
model is the only singular model of which those three properties have been
theoretically clariﬁed for ML learning, Bayesian learning, and VB learning.
Accordingly, we here summarize our observations, and discuss effects of
singularities in VB learning and other learning algorithms.
(i) In the RRR model, the basis selection effect, explained in Section 13.5.1,
appears as a selection bias of largest singular values of a zero-mean
random matrix.
Theorem 14.19 gives an asymptotic expansion of the ML
generalization error. The second term in the generalization coefﬁcient
(14.82) is the expectation of the square of the (H −H∗) largest singular
values of a random matrix
√
N
σ′ V′, where V′ is subject to the zero-mean
Gaussian (14.61). This corresponds to the effect of basis selection: ML
learning chooses the singular components that best ﬁt the observation
noise. With the full-rank model, i.e., H = min(L, M), the second term in
the generalization coefﬁcient (14.82) is equal to
min(L,M)−H∗
h=1
γ′2
h

q(W) = (min(L, M) −H∗) (max(L, M) −H∗)
= (L −H∗) (M −H∗) ,
and therefore the generalization coefﬁcient becomes
2λML = (H∗(L + M) −H∗2) + (L −H∗) (M −H∗) = LM
= D,
which is the same as the generalization coefﬁcient of the regular models.
Indeed, the full-rank RRR model is equivalently transformed to a regular
model by BA⊤→U, where the domain for U is the whole RL×M space
(no low-rank restriction is imposed to U). In this case, no basis selection
occurs because all possible bases are supposed to be used.

14.3 Insights into VB Learning
427
(ii) In the RRR model, the integration effect, explained in Section 13.5.1,
appears as the James–Stein (JS) type shrinkage.
This was shown in Theorem 14.1 in the asymptotic limit: the VB
estimator converges to the positive-part JS estimator operated on each
singular component separately. By comparing Theorems 14.11 and
14.19, we see that ML learning and VB learning differ from each other
by the factor θ

γ′2
h > max(L, M)
 !
1 −max(L,M)
γ′2
h
"2
, which comes from the
postive-part JS shrinkage. Unlike the basis selection effect, the
integration effect appears even if the model can be equivalently
transformed to a regular model—the full-rank RRR model (with
H = min(L, M)) is still affected by the singularities. The relation
between VB learning and the JS shrinkage estimator was also observed
in nonasymptotic analysis in Chapter 7, where model-induced
regularization (MIR) was illustrated as a consequence of the integration
effect, by focusing on the one-dimensional matrix factorization model.
Note that basis selection effect does not appear in the one-dimensional
matrix factorization model (where L = M = H), because it can be
equivalently transformed to a regular model.
(iii) VB learning shows similarity both to ML learning and Bayesian
learning.
Figures 14.1 through 14.4 generally show that VB learning is
regularized as much as Bayesian learning, while its dependence on the
model size (H, L, M, etc.) is more like ML learning. Unlike Bayesian
learning, the integration effect does not always dominate the basis
selection effect in VB learning—a good property of Bayesian learning,
2λBayes ≤D, does not necessarily hold in VB learning, e.g., we
observe that 2λVB > D at min(L, M) = 80 in Figure 14.2, and H = 1 in
Figure 14.3.
(iv) In VB learning, the relation between the generalization error and the free
energy is not as simple as in Bayesian learning.
In Bayesian learning, the generalization coefﬁcient and the free energy
coefﬁcient coincide with each other, i.e., λBayes = λ′Bayes. This property
does not hold in VB learning even approximately, as seen by comparing
Figures 14.1 through 14.4 and Figures 14.5 through 14.8. In many
cases, the VB free energy well approximates the Bayes free energy,
while the VB generalization error signiﬁcantly differs from the
Bayes generalization error. Research on the relation between the free
energy and the generalization error in VB learning is ongoing (see
Section 17.4).

428
14 Asymptotic VB Theory of Reduced Rank Regression
(v) MIR in VB learning can be stronger than that in Bayesian learning.
By deﬁnition, the VB free energy is never less than the Bayes free
energy, and therefore it holds that λ′VB ≥λ′Bayes. On the other hand, such
a relation does not hold for the generalization error, i.e., λVB can be
larger or less than λBayes. However, even if λVB is less than or equal to
λBayes for any true rank H∗in some RRR model, it does not mean the
domination of VB learning over Bayesian learning. Since the optimality
of Bayesian learning was proved (see Appendix D), λVB < λBayes simply
means that VB learning is more strongly regularized than Bayesian
learning, or in other words, VB learning tends to underﬁt small signals
while Bayesian learning tends to overﬁt noise. Extending the analysis
under the subtle signal assumption (14.87) to the general RRR model
would clarify this point.
(vi) The generalization error depends on the dimensionality in an interesting
way.
The shrinkage factor is governed by max(L, M) and independent of
min(L, M) in the asymptotic limit (see Theorem 14.1). This is because
the shrinkage is caused by the VB posterior extending into the parameter
space with larger dimensional space (M-dimensional input space or
L-dimensional output space) for the redundant components, as seen in
Corollary 14.3. This choice was made by maximizing the entropy of the
VB posterior distribution when the free energy is minimized.
Consequently, when L  M, the shape of the VB posterior in the
asymptotic limit is similar to the partially Bayesian learning, where the
posterior of A or B is approximated by the Dirac delta function (see
Chapter 12). On the other hand, increase of the smaller dimensionality
min(L, M) broadens the variety of basis selection: as mentioned in (i),
the basis selection effect in the RRR model occurs by the redundant
components selecting the (H −H∗) largest singular components, and
(min(L, M) −H∗) corresponds to the dimensionality that the basis
functions can span. This phenomenon can be seen in Figure 8.3—the
Marˇcenko–Pastur distribution is diverse when α = (min(L, M) −H∗)/
(max(L, M) −H∗) is large. We can conclude that a large max(L, M)
enhances the integration effect, leading to strong regularization, while a
large min(L, M) enhances the basis selection effect, leading to
overﬁtting. As a result, VB learning tends to be strongly regularized
when L ≪M or L ≫M, and tends to overﬁt when L ≈M.

15
Asymptotic VB Theory of Mixture Models
In this chapter, we discuss the asymptotic behavior of the VB free energy of
mixture models, for which VB learning algorithms were introduced in Sections
4.1.1 and 4.1.2. We ﬁrst prepare basic lemmas commonly used in this and the
following chapters.
15.1 Basic Lemmas
Consider the latent variable model expressed as
p(D|w) =

H
p(D, H|w).
In this chapter, we analyze the VB free energy, which is the minimum of the
free energy under the constraint,
r(w, H) = rw(w)rH(H),
(15.1)
i.e.,
FVB(D) =
min
rw(w),rH(H) F(r),
(15.2)
where
F(r) =
/
log rw(w)rH(H)
p(w, H, D)
0
rw(w)rH(H)
= FBayes(D) + KL (rw(w)rH(H)||p(w, H|D)) .
(15.3)
Here,
FBayes(D) = −log p(D) = −log

H
p(D, H) = −log

H

p(D, H, w)dw
429

430
15 Asymptotic VB Theory of Mixture Models
is the Bayes free energy. Recall that the stationary condition of the free energy
yields
rw(w) = 1
Cw
p(w) exp log p(D, H|w)
rH(H),
(15.4)
rH(H) =
1
CH
exp log p(D, H|w)
rw(w).
(15.5)
For the minimizerrw(w) of F(r), let
w = ⟨w⟩rw(w)
(15.6)
be the VB estimator.
The following lemma shows that the free energy is decomposed into the
sum of two terms.
Lemma 15.1
It holds that
FVB(D) = min
rw(w){R + Q},
(15.7)
where
R = KL(rw(w)||p(w)),
Q = −logCH,
for CH = 
H exp log p(D, H|w)
rw(w).
Proof
From the restriction of the VB approximation in Eq. (15.1), F(r) can
be divided into two terms,
F(r) =
/
log rw(w)
p(w)
0
rw(w)
+
/
log
rH(H)
p(D, H|w)
0
rw(w)rH(H)
.
Since the optimal VB posteriors satisfy Eqs. (15.4) and (15.5), if the VB
posterior rH(H) is optimized, then
/
log
rH(H)
p(D, H|w)
0
rw(w)rH(H)
= −logCH
holds. Thus, we obtain Eq. (15.7).
□
The free energies of mixture models and other latent variable models
involve the di-gamma function Ψ(x) and the log-gamma function log Γ(x) (see,
e.g., Eq. (4.22)). To analyze the free energy, we will use the inequalities on
these functions in the following lemma:

15.1 Basic Lemmas
431
Lemma 15.2
(Alzer, 1997) For x > 0,
1
2x < log x −Ψ(x) < 1
x,
(15.8)
and
0 ≤log Γ(x) −
)
x −1
2

log x −x + 1
2 log 2π
1
≤
1
12x.
(15.9)
The inequalities (15.8) ensure that substituting log x for Ψ(x) only con-
tributes at most additive constant terms to the VB free energy. The substitution
for log Γ(x) is given by Eq. (15.9) as well.
For the i.i.d. latent variable models deﬁned as
p(x|w) =

z
p(x, z|w),
(15.10)
the likelihood for the observed data D = {x(n)}N
n=1 and the complete data
{D, H} = {x(n), z(n)}N
n=1 is given by
p(D|w) =
N

n=1
p(x(n)|w),
p(D, H|w) =
N

n=1
p(x(n), z(n)|w),
respectively. In the asymptotic analysis of the free energy for such a
model, when the free energy is minimized, the second term in Eq. (15.7),
Q = −logCH, is proved to be close to N times the empirical entropy (13.20),
S N(D) = −1
N
N

n=1
log p(x(n)|w∗),
(15.11)
where w∗is the true parameter generating the data. Thus, the ﬁrst term in
Eq. (15.7) shows the asymptotic behavior of the VB free energy, which is
analyzed with the inequalities in Lemma 15.2.
Let
Q = Q −NS N(D).
(15.12)
It follows from Jensen’s inequality that
Q = log p(D|w∗) −log

H
exp log p(D, H|w)
rw(w)
≥log
p(D|w∗)
⟨p(D|w)⟩rw(w)
(15.13)
≥NEN(wML),

432
15 Asymptotic VB Theory of Mixture Models
where wML is the maximum likelihood (ML) estimator, and
EN(w) = LN(w∗) −LN(w)
= 1
N
N

n=1
log p(x(n)|w∗)
p(x(n)|w)
(15.14)
is the empirical KL divergence. Note here that LN is deﬁned in Eq. (13.37), and
EN(w) corresponds to the training error of the plug-in predictive distribution
(deﬁned in Eq. (13.75) for regular models) with an estimator w.
If the domain of data X is discrete and with ﬁnite cardinality, # (X) = M,
Q in Eq. (15.7) can be analyzed in detail. In such a case, we can assume
without loss of generality that x ∈{e1,. . . , eM}, where em is the one-of-M
representation, i.e., only the mth entry is one and the other entries are zeros.
Let Nm be the number of output m in the sequence D, i.e., Nm = N
n=1 x(n)
m , and
deﬁne the strongly ε-typical set T N
ε (p∗) with respect to the probability mass
function,
p∗= (p∗
1,. . . , p∗
M)⊤= (p(x1 = 1|w∗),. . . , p(xM = 1|w∗))⊤∈ΔM−1
as follows:
T N
ε (p∗) =
⎧⎪⎪⎨⎪⎪⎩D ∈XN;

Nm
N −p∗
m
 ≤
p∗
m
log M ε, m = 1,. . . , M
⎫⎪⎪⎬⎪⎪⎭.
(15.15)
It is known that the probability that the observed data sequence is not strongly
ε-typical is upper-bounded as follows:
Lemma 15.3
(Han and Kobayashi, 2007) It holds that
Prob(D  T N
ε (p∗)) ≤κM
Nε2 ,
where
κ = $log M%2 max
m:p∗m0
1 −p∗
m
p∗m
.
Let
p = (p1,. . . , pM)⊤=

⟨p(x1 = 1|w)⟩rw(w) ,. . . , ⟨p(xM = 1|w)⟩rw(w)
 ⊤∈ΔM−1
be the probability mass function deﬁned by the predictive distribution
⟨p(x|w)⟩rw(w) with the VB posterior rw(w). For any ﬁxed δ > 0, deﬁne
R∗
δ =
,
p ∈ΔM−1; KL(p∗||p) ≤δ
-
,
(15.16)
where KL(p∗||p) = M
m=1 p∗
m log p∗
m
pm . Then the following lemma holds:

15.1 Basic Lemmas
433
Lemma 15.4
Suppose that the domain X is discrete and with ﬁnite cardinal-
ity, # (X) = M. For all ε > 0 and D ∈T N
ε (p∗), there exists a constant C > 0
such that if p  R∗
Cε2,
Q = Ωp(N).
(15.17)
Furthermore,
min
rw(w)
Q = Op(1).
(15.18)
Proof
From Eq. (15.13), we have
Q ≥log
p(D|w∗)
⟨p(D|w)⟩rw(w)
= N
M

m=1
Nm
N log p∗
m
pm
= N
,
KL(pML||p) −KL(pML||p∗)
-
,
(15.19)
where pML = (pML
1 ,. . . , pML
M )⊤= (N1/N,. . . , NM/N)⊤∈ΔM−1 is the type,
namely the empirical distribution of D. Thus, if KL(pML||p) > KL(pML||p∗),
the right-hand side of Eq. (15.19) grows in the order of N. If D ∈T N
ε (p∗),
KL(pML||p∗) = Op(ε2) since KL(pML||p∗) is well approximated by a quadratic
function ofpML −p∗. To prove the ﬁrst assertion of the lemma, it sufﬁces to see
that KL(p∗||p) ≤Cε2 is equivalent to KL(pML||p) ≤C′ε2 for a constant C′ > 0
if D ∈T N
ε (p∗). In fact, we have
KL(p∗||p) = KL(p∗||pML) + KL(pML||p) +
M

m=1
(p∗
m −pML
m )

log pML
m
−log pm
 
.
(15.20)
It follows from D ∈T N
ε (p∗) that KL(p∗||pML)/ε2 and |p∗
m −pML
m |/ε are bounded
by constants. Then KL(pML||p) ≤C′ε2 implies that |pML
m −pm|/ε is bounded by
a constant, and hence all the terms in Eq. (15.20) divided by ε2 are bounded
by constants.
It follows from Eq. (15.19) that
min
rw(w)
Q ≥−NKL(pML||p∗).
(15.21)
The standard asymptotic theory of the multinomial model implies that twice
the right-hand side of Eq. (15.21), with its sign ﬂipped, asymptotically follows
the chi-squared distribution with degree of freedom M −1 as discussed in
Section 13.4.5.
□

434
15 Asymptotic VB Theory of Mixture Models
This lemma is used for proving the consistency of the VB posterior and
evaluating lower-bounds of VB free energy for discrete models in Sections
15.3 and 15.4 and Chapter 16.
15.2 Mixture of Gaussians
In this section, we consider the following Gaussian mixture model (GMM)
introduced in Section 4.1.1 and give upper- and lower-bounds for the VB free
energy (Watanabe and Watanabe, 2004, 2006):
p(z|α) = MultinomialK,1(z; α),
(15.22)
p(x|z, {μk}K
k=1) =
K

k=1
6GaussM(x; μk, IM)7zk ,
(15.23)
p(α|φ) = DirichletK(α; (φ,. . . , φ)⊤),
(15.24)
p(μk|μ0, ξ) = GaussM(μk|μ0, (1/ξ)IM).
(15.25)
Under the constraint,
r(H, w) = rH(H)rw(w),
the VB posteriors are given as follows:
r({z(n)}N
n=1, α, {μk}K
k=1) = rz({z(n)}N
n=1)rα(α)rμ({μk}K
k=1),
rz({z(n)}N
n=1) =
N

n=1
MultinomialK,1

z(n);z(n) 
,
rα(α) = Dirichlet

α; (φ1,. . . ,φK)⊤ 
,
rμ({μk}K
k=1) =
K

k=1
GaussM

μk;μk, σ2
kIM
 
.
The variational parameters {z(n)}N
n=1, {φk}K
k=1, {μk, σ2
k}K
k=1 minimize the free
energy,
F = log
⎛⎜⎜⎜⎜⎜⎝
Γ(K
k=1 φk)
K
k=1 Γ(φk)
⎞⎟⎟⎟⎟⎟⎠−log
 Γ(Kφ)
(Γ(φ))K

−M
2
K

k=1
log

ξσ2
k
 
−KM
2
+
N

n=1
K

k=1
z(n)
k logz(n)
k +
K

k=1
φk −φ −Nk
 
Ψ(φk) −Ψ(K
k′=1 φk′)
 

15.2 Mixture of Gaussians
435
+
K

k=1
ξ

∥μk −μ0∥2 + Mσ2
k
 
2
+
K

k=1
Nk

M log(2π) + Mσ2
k
 
2
+
K

k=1
Nk∥xk −μk∥2 + N
n=1z(n)
k ∥x(n) −xk∥2
2
,
(15.26)
where
Nk =
N

n=1
z(n)
k ,
(15.27)
xk = 1
Nk
N

n=1
x(n)z(n)
k .
(15.28)
The stationary condition of the free energy yields
z(n)
k
=
z(n)
k
K
k′=1 z(n)
k′
,
(15.29)
φk = Nk + φ,
(15.30)
μk = Nkxk + ξμ0
Nk + ξ
,
(15.31)
σ2
k =
1
Nk + ξ
,
(15.32)
where
z(n)
k
∝exp

Ψ(φk) −1
2∥x(n) −μk∥2 + Mσ2
k

.
(15.33)
The following condition is assumed.
Assumption 15.1
The true distribution q(x) is an M-dimensional GMM
p(x|w∗), which has K0 components and parameter w∗= (α∗, {μ∗
k}K0
k=1):
q(x) = p(x|w∗) =
K0

k=1
α∗
kGaussM(x; μ∗
k, IM),
(15.34)
where x, μ∗
k ∈RM. Suppose that the true distribution can be realized by our
model in hand, i.e., K ≥K0 holds.
Under this condition, we prove the following theorem, which evaluates the
relative VB free energy,
FVB(D) = FVB(D) −NS N(D).
(15.35)
The proof will appear in the next section.

436
15 Asymptotic VB Theory of Mixture Models
Theorem 15.5
The relative VB free energy of the GMM satisﬁes
λ′VB
MM log N + NEN(w) + Op(1) ≤FVB(D) ≤λ
′VB
MM log N + Op(1),
(15.36)
where EN is the empirical KL divergence (15.14), and the coefﬁcients λ′VB
MM,
λ
′VB
MM are given by
λ′VB
MM =
⎧⎪⎪⎪⎨⎪⎪⎪⎩
(K −1)φ + M
2

φ < M+1
2
 
,
MK+K−1
2

φ ≥M+1
2
 
,
(15.37)
λ
′VB
MM =
⎧⎪⎪⎪⎨⎪⎪⎪⎩
(K −K0)φ + MK0+K0−1
2

φ < M+1
2
 
,
MK+K−1
2

φ ≥M+1
2
 
.
(15.38)
In this theorem, EN(w) is the training error of the VB estimator. Let wML be
the ML estimator. Then it immediately follows from Eq. (15.14) that
NEN(w) ≥NEN(wML),
(15.39)
where NEN(wML) = minw
N
n=1 log p(x(n)|w∗)
p(x(n)|w) is the (maximum) log-likelihood
ratio statistic with sign inversion. As discussed in Section 13.5.3, it is
conjectured for the GMM deﬁned by Eqs. (15.22) and (15.23) that the log-
likelihood ratio diverges in the order of log log N (Hartigan, 1985). If this
conjecture is proved, the statement of the theorem is simpliﬁed to
FVB(D) = λ′ log N + op(log N),
for λ′VB
MM ≤λ′ ≤λ
′VB
MM. Note, however, that even if NEN(wML) diverges to minus
inﬁnity, Eq. (15.39) does not necessarily mean NEN(w) diverges in the same
order. Also note that NEN(w) does not affect the upper-bound in Eq. (15.36).
Since the dimension of the parameter w is D = MK + K −1, the relative
Bayes free energy coefﬁcient of regular statistical models, on which the
Bayesian information criterion (BIC) (Schwarz, 1978) and the minimum
description length (MDL) (Rissanen, 1986) are based, is given by D/2. Note
that, unlike regular models, the advantage of Bayesian learning for singular
models is demonstrated by the asymptotic analysis as seen in Eqs. (13.123),
(13.124), and (13.125). Theorem 15.5 claims that the coefﬁcient λ
′VB
MM of log N
is smaller than D/2 when φ < (M + 1)/2. This means that the VB free energy
FVB becomes smaller than that of regular models, i.e., 2λ′VB ≤D holds.
Theorem 15.5 shows how the hyperparameters affect the learning process.
The coefﬁcients λ′VB
MM and λ
′VB
MM in Eqs. (15.37) and (15.38) are divided into

15.2 Mixture of Gaussians
437
two cases. These cases correspond to whether φ < M+1
2
holds, indicating that
the inﬂuence of the hyperparameter φ in the prior p(α|φ) appears depending
on the number M of parameters in each component. Let K be the number of
components satisfying Nk = Θp(N). Then the following corollary follows from
the proof of Theorem 15.5.
Corollary 15.6
The upper-bound in Eq. (15.36) is attained when K = K0 if
φ < M+1
2
and K = K if φ ≥M+1
2 .
This corollary implies that the phase transition of the VB posterior occurs
at φ = M+1
2 , i.e., only when φ < M+1
2 , the prior distribution reduces redundant
components; otherwise, it uses all the components. The phase transition of the
posterior occurs also in Bayesian learning while the phase transition point is
different from that of VB learning (Yamazaki and Kaji, 2013).
Theorem 15.5 also implies that the hyperparameter φ is the only hyperpa-
rameter on which the leading term of the VB free energy FVB depends. This
is due to the inﬂuence of the hyperparameters on the prior probability density
around the true parameters. Consider the case where K0 < K. In this case, for a
parameter that gives the true distribution, either of the followings holds: αk = 0
for some k or μi = μj for some pair (i, j). The prior distribution p(α|φ) given
by Eq. (15.24) can drastically change the probability density around the points
where αk = 0 for some k by changing the hyperparameter φ while the prior
distribution p(μk|μ0, ξ) given by Eq. (15.25) always takes positive values for
any values of the hyperparameters ξ and μ0. While the condition for the prior
density p(α|φ) to diverge at αk = 0 is αk < 1, and hence is independent of M,
the phase transition point of the VB posterior is φ = M+1
2 . As we will see in
Section 15.4 for the Bernoulli mixture model, if some of the components are
located at the boundary of the parameter space, the leading term of the relative
VB free energy depends also on the hyperparameter of the prior for component
parameters.
Theorem 15.5 is also extended to the case of the general Dirichlet prior
p(α|φ) = DirichletK(α; φ), where φ = (φ1,. . . , φK)⊤is the hyperparameter as
follows:
Theorem 15.7
(Nakamura and Watanabe, 2014) The relative VB free energy
of the GMM satisﬁes
K

k=1
λ′VB
k
log N + NEN(w) + Op(1) ≤FVB(D) ≤
K

k=1
λ
′VB
k
log N + Op(1),

438
15 Asymptotic VB Theory of Mixture Models
where the coefﬁcients λ′VB
k
, λ
′VB
k
are given by
λ′VB
k
=
⎧⎪⎪⎪⎨⎪⎪⎪⎩
φk −
1
2K

k  1 and φk < M+1
2
 
,
M+1
2
−
1
2K

k = 1 or φk ≥M+1
2
 
,
λ
′VB
k
=
⎧⎪⎪⎪⎨⎪⎪⎪⎩
φk −
1
2K

k > K0 and φk < M+1
2
 
,
M+1
2
−
1
2K

k ≤K0 or φk ≥M+1
2
 
.
The proof of this theorem is omitted. This theorem implies that the phase
transition of the VB posterior of each component occurs at the same transition
point φk = M+1
2
as Theorem 15.5.
Proof of Theorem 15.5
Before proving Theorem 15.5, we show two lemmas where the two terms,
R = KL(rw(w)||p(w)) and Q = −logCH, in Lemma 15.1 are respectively
evaluated.
Lemma 15.8
It holds that
R −
⎧⎪⎪⎨⎪⎪⎩G(α) + ξ
2
K

k=1
∥μk −μ0∥2
⎫⎪⎪⎬⎪⎪⎭
 ≤C,
where C is a constant, μk = μk

rμ({μk}K
k=1) = Nkxk+ξμ0
Nk+ξ
, and the function G(α) of
α =
2
αk = ⟨αk⟩rα(α) = Nk+φ
N+Kφ
3K
k=1 is deﬁned by
G(α) = MK + K −1
2
log N +
) M + 1
2
−φ
1
K

k=1
logαk.
(15.40)
Proof
Calculating the KL divergence between the posterior and the prior, we
obtain
KL(rα(α)||p(α|φ)) =
K

k=1
h(Nk) −NΨ(N + Kφ) + log Γ(N + Kφ) + log Γ(φ)K
Γ(Kφ),
(15.41)
where we use the notation h(x) = xΨ(x+φ)−log Γ(x+φ). Similarly, we obtain
KL(rμ({μk}K
k=1)||p({μk}K
k=1|μ0, ξ))
=
K

k=1
M
2 log Nk + ξ
ξ
−KM
2
+ ξ
2
K

k=1
)
M
Nk + ξ
+ ∥μk −μ0∥2
1
.
(15.42)

15.2 Mixture of Gaussians
439
By using Inequalities (15.8) and (15.9), we obtain
−1 + 12φ −1
12(x + φ) ≤h(x) +

φ −1
2

log(x + φ) −x −φ + 1
2 log 2π ≤0. (15.43)
Thus, from Eqs. (15.41), (15.42), (15.43), and
R = KL(rα(α)||p(α|φ)) + KL(rμ({μk}K
k=1)||p({μk}K
k=1|μ0, ξ)),
it follows that
R −
⎧⎪⎪⎨⎪⎪⎩G(α) + ξ
2
K

k=1
∥μk −μ0∥2
⎫⎪⎪⎬⎪⎪⎭

≤MK + K −1
2
log
!
1 + Kφ
N
"
+ (K −1)
φ −log 2π
2
 + K +
K

k=1
|12φ −1|
12(Nk + φ)
+
12N + 1
12(N + Kφ)
+
log Γ(φ)K
Γ(Kφ)
 +

K

k=1
log Nk + ξ
Nk + φ
−MK
2 (1 + log ξ) + ξ
2
K

k=1
M
Nk + ξ
 .
The right-hand side of the preceding inequality is bounded by a constant since
1
N + ξ <
1
Nk + ξ
< 1
ξ ,
and
1
N + φ <
1
Nk + φ
< 1
φ.
□
Lemma 15.9
It holds that
Q = −
N

n=1
log
⎛⎜⎜⎜⎜⎜⎝
K

k=1
1
√
2πM exp
⎛⎜⎜⎜⎜⎜⎝Ψ(Nk + φ) −Ψ(N + Kφ)
−∥x(n) −μk∥2
2
−M
2
1
Nk + ξ

,
(15.44)
and
NEN(w) −
N
N + Kφ ≤Q ≤NEN(w) −
N
2(N + Kφ),
(15.45)
where EN(w) is given by Eq. (15.14) and EN(w) is deﬁned by
EN(w) = 1
N
N

n=1
log
p(x(n)|w∗)
K
k=1
αk
√
2πM exp
!
−∥x(n)−μk∥2
2
−
M+2
2(Nk+min{φ,ξ})
".

440
15 Asymptotic VB Theory of Mixture Models
Proof
CH =
N

n=1

z(n)
exp

log p(x(n), z(n)|w)

rw(w)
=
N

n=1
K

k=1
1
√
2πM exp

Ψ(Nk + φ) −Ψ(N + Kφ)
−∥x(n) −μk∥2
2
−M
2
1
Nk + ξ

.
Thus, we have Eq. (15.44).
Using again Inequality (15.8), we obtain
Q ≤−
N

n=1
log
⎛⎜⎜⎜⎜⎜⎝
K

k=1
αk
√
2πM exp

−∥x(n) −μk∥2
2
−
M + 2
2(Nk + min{φ, ξ})
⎞⎟⎟⎟⎟⎟⎠
−
N
2(N + Kφ),
(15.46)
and
Q ≥−
N

n=1
log
⎛⎜⎜⎜⎜⎜⎝
K

k=1
αk
√
2πM exp

−∥x(n) −μk∥2
2
⎞⎟⎟⎟⎟⎟⎠−
N
N + Kφ,
which give upper- and lower-bounds in Eq. (15.45), respectively.
□
Now, from the preceding lemmas, we prove Theorem 15.5 by showing
upper- and lower-bounds, respectively. First, we show the upper-bound in
Eq. (15.36).
From Lemma 15.1, Lemma 15.8, and Lemma 15.9, it follows that
F −NS N(D) ≤min
w TN(w) + C,
(15.47)
where
TN(w) = G(α) + ξ
2
K

k=1
∥μk −μ0∥2 + NEN(w).
From Eq. (15.47), it is noted that the function values of TN(w) at speciﬁc
points of the variational parameter w give upper-bounds of the VB free energy
FVB(D). Hence, let us consider following two cases.
(I) Consider the case where all components, including redundant ones, are
used to learn K0 true components, i.e.,
αk = α∗
kN + φ
N + Kφ
(1 ≤k ≤K0 −1),

15.2 Mixture of Gaussians
441
αk =
α∗
K0N/(K −K0 + 1) + φ
N + Kφ
(K0 ≤k ≤K),
μk = μ∗
k
(1 ≤k ≤K0 −1),
μk = μ∗
K0
(K0 ≤k ≤K).
Then we obtain
NEN(w)
<
N

n=1
log p(x(n)|w∗) −
N

n=1
log N + φ
N + Kφ
−
N

n=1
log
⎛⎜⎜⎜⎜⎜⎜⎝
K0−1

k=1
α∗
k
√
2πM exp
⎛⎜⎜⎜⎜⎝−∥x(n) −μ∗
k∥2
2
−
M + 2
2(α∗
kN + min{ξ, φ})
⎞⎟⎟⎟⎟⎠
+
α∗
K0
√
2πM exp
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎝−∥x(n) −μ∗
k∥2
2
−
M + 2
2(
α∗
K0
K−K0+1N + min{ξ, φ})
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎠
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎠
<
N

n=1
log
N+Kφ
N+φ p(x(n)|w∗)
p(x(n)|w∗) exp
!
−
(M+2)(K−K0+1)
2(mink{α∗
k}N+min{ξ,φ}(K−K0+1))
"
< (K −1)φ
N + φ
+
(M + 2)(K −K0 + 1)N
2(mink{α∗
k}N + min{ξ, φ}(K −K0 + 1))
≤(K −1)φ +
 M + 2
2
 K −K0 + 1
mink{α∗
k} ,
where the ﬁrst inequality follows from
α∗
kN+φ
N+Kφ > α∗
k
N+φ
N+Kφ and the third
inequality follows from log(1 + x) ≤x for x > −1.
It follows that
TN(w) < MK + K −1
2
log N + C′,
(15.48)
where C′ is a constant.
(II) Consider the case where the redundant components are eliminated, i.e.,
αk = α∗
kN + φ
N + Kφ
(1 ≤k ≤K0),
αk =
φ
N + Kφ
(K0 + 1 ≤k ≤K),
μk = μ∗
k
(1 ≤k ≤K0),
μk = μ0
(K0 + 1 ≤k ≤K).

442
15 Asymptotic VB Theory of Mixture Models
Then it holds that
NEN(w)
<
N

n=1
log
p(x(n)|w∗)
N+φ
N+Kφ
K0
k=1
α∗
k
√
2πM exp
!
−
∥x(n)−μ∗
k∥2
2
−
M+2
2(α∗
kN+min{ξ,φ})
"
< (K −1)φN
N + φ
+
 M + 2
2

N
mink{α∗
k}N + min{ξ, φ}
≤(K −1)φ +
 M + 2
2

1
mink{α∗
k}.
(15.49)
The ﬁrst inequality follows from
α∗
kN+φ
N+Kφ > α∗
k
N+φ
N+Kφ and
K

k=K0+1
αk
√
2πM exp

−∥x(n) −μk∥2
2
−
M + 2
2(Nk + min{φ, ξ})

> 0.
The second inequality follows from log(1 + x) ≤x for x > −1.
It follows that
TN(w) <
)
(K −K0)φ + MK0 + K0 −1
2
1
log N + C′′,
(15.50)
where C′′ is a constant.
From Eqs. (15.47), (15.48), and (15.50), we obtain the upper-bound in
Eq. (15.36).
Next we show the lower-bound in Eq. (15.36). It follows from Lemma 15.1,
Lemma 15.8, and Lemma 15.9 that
F −NS N(D) ≥min
α {G(α)} + NEN(w) −C −1.
(15.51)
If φ ≥M+1
2 , then,
G(α) ≥MK + K −1
2
log N −
 M + 1
2
−φ

K log K,
(15.52)
since Jensen’s inequality yields that
K

k=1
logαk ≤K log
⎛⎜⎜⎜⎜⎜⎝
1
K
K

k=1
αk
⎞⎟⎟⎟⎟⎟⎠= K log
 1
K

.

15.3 Mixture of Exponential Family Distributions
443
If φ < M+1
2 , then
G(α) ≥
2
(K −1)φ + M
2
3
log N +
 M + 1
2
−φ

(K −1) log
φN
N + Kφ + C′′′
≥
2
(K −1)φ + M
2
3
log N +
 M + 1
2
−φ

(K −1) log
φ
1 + Kφ + C′′′,
(15.53)
where C′′′ is a constant. The ﬁrst inequality follows since
αk ≥
φ
N + Kφ
holds for every k, and the constraint
K

k=1
αk = 1
ensures that | logαk| is bounded by a constant independent of N for at least one
index k. From Eqs. (15.51), (15.52), and (15.53) we obtain the lower-bound in
Eq. (15.36).
15.3 Mixture of Exponential Family Distributions
The previous theorem for the GMM can be generalized to the mixture of
exponential family distributions (Watanabe and Watanabe, 2005, 2007). The
model that we consider is deﬁned by
p(z|α) = MultinomialK,1(z; α),
(15.54)
p(t|z, {ηk}K
k=1) =
K

k=1
,
exp

η⊤
k t −A(ηk) + B(t)
 -zk ,
(15.55)
p(α|φ) = DirichletK(α; (φ,. . . , φ)⊤),
(15.56)
p(ηk|ν0, ξ) =
1
C(ξ, ν0) exp

ξ(ν⊤
0 ηk −A(ηk))
 
.
(15.57)
As demonstrated in Section 4.1.2, under the constraint, r(H, w)
=
rH(H)rw(w), the VB posteriors are given as follows:
r({z(n)}N
n=1, α, {ηk}K
k=1) = rz({z(n)}N
n=1)rα(α)rη({ηk}K
k=1),
rz({z(n)}N
n=1) =
N

n=1
MultinomialK,1

z(n);z(n) 
,

444
15 Asymptotic VB Theory of Mixture Models
rα(α) = Dirichlet

α; (φ1,. . . ,φK)⊤ 
,
rη({ηk}K
k=1) =
K

k=1
1
C(ξk,νk)
exp
ξk(ν⊤
k ηk −A(ηk))
 
,
(15.58)
The variational parameters {z(n)}N
n=1, {φk}K
k=1, {νk,ξk}K
k=1 minimize the free
energy,
F = log
⎛⎜⎜⎜⎜⎜⎝
Γ(K
k=1 φk)
K
k=1 Γ(φk)
⎞⎟⎟⎟⎟⎟⎠−log
 Γ(Kφ)
(Γ(φ))K

−
K

k=1
logC(ξk,νk) + K logC(ξ, ν0)
+
N

n=1
K

k=1
z (n)
k
logz (n)
k
+
K

k=1
φk −φ −Nk
 
Ψ(φk) −Ψ(K
k′=1 φk′)
 
+
K

k=1
⎡⎢⎢⎢⎢⎣η⊤
k
,
ξ $νk −ν0
% + Nk

νk −tk
 -
+
ξk −ξ −Nk
 ∂logC(ξk,νk)
∂ξk
⎤⎥⎥⎥⎥⎦
−
N

n=1
B(t(n)),
(15.59)
where
Nk =
N

n=1
z (n)
k ,
tk = 1
Nk
N

n=1

z(n)
k

rH(H) t(n),
ηk = 1
ξk
∂logC(ξk,νk)
∂νk
.
The stationary condition of the free energy yields
z (n)
k
=
z(n)
k
K
k′=1 z(n)
k′
,
(15.60)
αk = Nk + φ,
(15.61)
νk = Nktk + ξν0
Nk + ξ
,
(15.62)
ξk = Nk + ξ.
(15.63)
where
z(n)
k
∝exp

Ψ(φk) +η⊤
k t(n) −A(ηk)
rη(ηk)
 
.
(15.64)
We assume the following conditions.

15.3 Mixture of Exponential Family Distributions
445
Assumption 15.2
The true distribution q(t) of sufﬁcient statistics is repre-
sented by a mixture of exponential family distributions p(t|w∗), which has K0
components and the parameter w∗= {α∗
k, η∗
k}K0
k=1:
q(t) = p(t|w∗) =
K0

k=1
α∗
k exp

η∗
k
⊤t −A(η∗
k) + B(t)
 
,
where η∗
k ∈RM and η∗
k  η∗
k′(k  k′). Also, assume that the true distribution
can be achieved with the model, i.e., K ≥K0 holds.
Assumption 15.3
The prior distribution p({ηk}K
k=1|ν0, ξ) deﬁned by Eq.
(15.57) satisﬁes 0 < p({ηk}K
k=1|ν0, ξ) < ∞.
Assumption 15.4
Regarding the distribution p(t|η) of each component, the
Fisher information matrix
F(η) = ∂2A(η)
∂η∂η⊤
satisﬁes 0 < det $F(η)% < +∞for an arbitrary η ∈H. The function ν⊤η −A(η)
has a stationary point at η in the interior of H for each ν ∈
, ∂A(η)
∂η ; η ∈H
-
.
The following theorem will be proven under these conditions. The proof
will appear in the next section. Here,
S N(D) = −1
N
N

n=1
log p(t(n)|w∗)
(15.65)
is the empirical entropy.
Theorem 15.10
The relative VB free energy of the mixture of exponential
family distributions satisﬁes
λ′VB
MM log N + NEN(w) + Op(1) ≤FVB(D) = FVB(D) −NS N(D)
≤λ
′VB
MM log N + Op(1),
(15.66)
where λ′VB
MM and λ
′VB
MM are given by
λ′VB
MM =
) (K −1)φ + M
2
(φ < M+1
2 ),
MK+K−1
2
(φ ≥M+1
2 ),
(15.67)
λ
′VB
MM =
) (K −K0)φ + MK0+K0−1
2
(φ < M+1
2 ),
MK+K−1
2
(φ ≥M+1
2 ).
(15.68)

446
15 Asymptotic VB Theory of Mixture Models
Again in this theorem,
EN(w) = 1
N
N

n=1
log p(t(n)|w∗)
p(t(n)|w)
(15.69)
is the training error, and
NEN(w) ≥NEN(wML),
(15.70)
holds for the (maximum) log-likelihood ratio statistic. As discussed in Section
13.5.3, the log-likelihood ratio statistics of some singular models diverge to
inﬁnity as N increases. Some known facts about the divergence of the log-
likelihood ratio are described in the following examples. Note again that even
if NEN(wML) diverges to minus inﬁnity, Eq. (15.70) does not necessarily mean
NEN(w) diverges in the same order.
If the domain of the sufﬁcient statistics t of the model p(t|w) is discrete and
ﬁnite, we obtain the following theorem by Lemmas 15.3 and 15.4:
Theorem 15.11
If the domain of the sufﬁcient statistics t is discrete and ﬁnite,
the relative VB free energy of the mixture of exponential family distributions
satisﬁes
FVB(D) = λ
′VB
MM log N + Op(1),
(15.71)
where the coefﬁcient λ
′VB
MM is given by Eq. (15.68).
The proof of this theorem follows the proof of the preceding theorem.
Examples
The following are examples where Theorems 15.10 and 15.11 apply.
Example 1 (Binomial)
Consider a mixture of binomial component distribu-
tions. Each component has a one-dimensional parameter ν ∈[0, 1]:
p(x = k|ν) = BinomialT(k; ν) =
T
k

νk(1 −ν)T−k,
(15.72)
where T is the number of Bernoulli trials and k = 0, 1, 2,. . . , T. Hence, M = 1
and the natural parameter is given by η = log
ν
1−ν. Theorem 15.11 applies with
M = 1.

15.3 Mixture of Exponential Family Distributions
447
Example 2 (Gamma)
Consider the gamma component with shape parameter
α > 0 and scale parameter β > 0:
p(x|α, β) = Gamma(x; α, β) =
βα
Γ(α) xα−1 exp (−βx) ,
(15.73)
where 0 ≤x < ∞. The natural parameter η is given by η1 = β and η2 = α −1.
Hence, Eq. (15.66) holds where λ′VB
MM and λ
′VB
MM are given by Eqs. (15.67) and
(15.68) with M = 2. When shape parameter α is known, the likelihood ratio in
ML learning diverges in the order of log log N (Liu et al., 2003). This implies
that NEN(w) = Op(log log N) from Eq. (15.70).
Example 3 (Gaussian)
Consider the L-dimensional Gaussian component
with mean μ and covariance matrix Σ:
p(x|μ, Σ) = GaussL(x; μ, Σ) =
1
(2π)L/2|Σ|1/2 exp

−1
2(x −μ)⊤Σ−1(x −μ)

.
The natural parameter η is given by μTΣ−1 and Σ−1. These are functions of the
elements of μ and the upper-right half of Σ−1. Hence, Eq. (15.66) holds where
λ′VB
MM and λ
′VB
MM are given by Eqs. (15.67) and (15.68) with M = L + L(L + 1)/2.
If the covariance matrix Σ is known and the parameter is restricted to mean μ,
it is conjectured that the likelihood ratio in ML learning diverges in the order of
log log N (Hartigan, 1985). This suggests that the likelihood ratio can diverge
in a higher order than log log N if the covariance matrices are also estimated.
Other than these examples, Theorems 15.10 and 15.11 apply to mixtures of
distributions such as multinomial, Poisson, and Weibull.
Proof of Theorem 15.10
Here Theorem 15.10 is proved in the same way as Theorem 15.5.
Since the VB posterior satisﬁes rw(w) = rα(α)rη({ηk}K
k=1), we have
R = KL(rw(w)||p(w))
= KL(rα(α)||p(α|φ)) +
K

k=1
KL(rη(ηk)||p(ηk|ν0, ξ)).
(15.74)
The following lemma is used for evaluating KL(rη(ηk)||p(ηk|ν0, ξ)) in the
case of the mixture of exponential family distributions.
Lemma 15.12
It holds that
KL(rη(ηk)||p(ηk|ν0, ξ)) = M
2 log(Nk + ξ) −log p(ηk|ν0, ξ) + Op(1),

448
15 Asymptotic VB Theory of Mixture Models
where
ηk = ηk

rη(ηk) = 1
ξk
∂logC(ξk,νk)
∂νk
.
(15.75)
Proof
Using the VB posterior, Eq. (15.58), we obtain
KL(rη(ηk)||p(ηk|ν0, ξ)) = −log C(ξk,νk)
C(ξ, ν0) + Nk
,
νk
ηk

rη(ηk) −A(ηk)
rη(ηk)
-
,
(15.76)
where we used ξk = Nk + ξ. Let us now evaluate the value of C(ξk,νk)
when ξk is sufﬁciently large. From Assumption 15.4, using the saddle point
approximation, we obtain
C(ξk,νk) = exp
ξk{ν⊤
k ηk −A(ηk)}
 ⎛⎜⎜⎜⎜⎝
2π
ξk
⎞⎟⎟⎟⎟⎠
M/2 .
det $F(ηk)%−1
⎧⎪⎨⎪⎩1 + Op
⎛⎜⎜⎜⎜⎝
1
ξk
⎞⎟⎟⎟⎟⎠
⎫⎪⎬⎪⎭,
(15.77)
where ηk is the maximizer of the functionν⊤ηk −A(ηk), that is,
∂A(ηk)
∂ηk
= νk.
Therefore, −logC(ξk,νk) is evaluated as
−logC(ξk,νk) = M
2 log
ξk
2π + 1
2 log det $F(ηk)% −ξk
,
ν⊤
k ηk −A(ηk)
-
+ Op
⎛⎜⎜⎜⎜⎝
1
ξk
⎞⎟⎟⎟⎟⎠.
(15.78)
Applying the saddle point approximation to
ηk −ηk =
1
C(ξk,νk)

(ηk −ηk) exp
ξk
,
ν⊤
k ηk −A(ηk)
- 
dηk,
we obtain
||ηk −ηk|| ≤A′
ξk
+ Op
ξ−3/2
k
 
,
(15.79)
where A′ is a constant. Since
A(ηk) −A(ηk) = (ηk −ηk)⊤νk + 1
2(ηk −ηk)⊤F(ηk)(ηk −ηk),
(15.80)

15.3 Mixture of Exponential Family Distributions
449
for some point ηk on the line segment between ηk and ηk, we have
A(ηk) −A(ηk) = (ηk −ηk)⊤νk + Op(ξ−2
k ),
(15.81)
and applying the saddle point approximation, we obtain
A(ηk)
rη(ηk) −A(ηk) = (ηk −ηk)⊤νk + M
2ξk
+ Op
ξ−3/2
k
 
.
(15.82)
From Eqs. (15.81) and (15.82), we have
A(ηk)
rη(ηk) −A(¯ηk) = M
2ξk
+ Op
ξ−3/2
k
 
,
(15.83)
Thus, from Eqs. (15.76), (15.78), (15.81), and (15.82), we obtain the
lemma.
□
Lemmas 15.8 and 15.9 are substituted by the following lemmas.
Lemma 15.13
It holds that
R −G(α) +
K

k=1
log p(ηk|ν0, ξ)
 ≤C,
(15.84)
where C is a constant and the function G(α) is deﬁned by Eq. (15.40).
Proof
From Eqs. (15.41), (15.43), and (15.74) and Lemma 15.12,
R −G(α) +
K

k=1
log p(ηk|ν0, ξ)

is upper-bounded by a constant since
1
N + ξ <
1
Nk + ξ
< 1
ξ .
□
Lemma 15.14
It holds that
NEN(w) + Op(1) ≤Q = −logCH −NS N(D) ≤NEN(w) + Op(1),
(15.85)
where the function EN(w) is deﬁned by Eq. (15.69) and
EN(w) = 1
N
N

n=1
log
p(t(n)|w∗)
K
k=1 αkp(t(n)|ηk) exp
!
−
A
Nk+min{φ,ξ}
",
where A is a constant.

450
15 Asymptotic VB Theory of Mixture Models
Proof
CH =
N

n=1
K

k=1
exp

log αkp(t(n)|ηk)

rw(w)
=
N

n=1
K

k=1
exp

Ψ(Nk + φ) −Ψ(N + Kφ) +η⊤
k t(n) −A(ηk)
rη(ηk) + B(t(n))
 
.
Again, using the inequalities in Eqs. (15.8) and (15.83), we obtain
Q ≤
N

n=1
log
⎛⎜⎜⎜⎜⎜⎝
K

k=1
αkp(t(n)|ηk) exp

−
M + 2
2(Nk + min{φ, ξ})
+ Op
!
N
−3
2
k
"⎞⎟⎟⎟⎟⎟⎠+ Op(1),
Q ≥−
N

n=1
log
⎛⎜⎜⎜⎜⎜⎝
K

k=1
αkp(t(n)|ηk)
⎞⎟⎟⎟⎟⎟⎠+ Op(1),
which give the upper- and lower-bounds in Eq. (15.85), respectively.
□
Since the prior distribution p({ηk}K
k=1|ν0, ξ) satisﬁes 0 < p({ηk}K
k=1|ν0, ξ)
< ∞, from Lemmas 15.13 and 15.14, we complete the proof of Theorem 15.10
in the same way as that of Theorem 15.5.
Proof of Theorem 15.11
The upper-bound follows from Theorem 15.10. From Lemmas 15.1 and 15.13
and the boundedness of the prior, we have the following lower-bound:
F −NS N(D) ≥G(α) + Q + Op(1).
Lemma 15.4 implies that for ε > 0, if D ∈T N
ε (p∗) andp  R∗
Cε2 for the constant
C in the lemma,
Q = Ωp(N).
Since G(α) = Op(log N), this means that if the free energy is minimized,
p ∈R∗
Cε2 for sufﬁciently large N, which implies that at least K0 components
are active and
| logαk| = Op(1)
holds for at least K0 components. By minimizing G(α) under this constraint
and the second assertion of Lemma 15.4, we have
FVB(D) −NS N(D) ≥λ
′VB
MM log N + Op(1),
for D ∈T N
ε (p∗). Because the probability that the observed data sequence is
strongly ε-typical tends to 1 as N →∞for any ε > 0 by Lemma 15.3, we
obtain the theorem.

15.4 Mixture of Bernoulli with Deterministic Components
451
15.4 Mixture of Bernoulli with Deterministic Components
In the previous sections, we assumed that all true component parameters are in
the interior of the parameter space. In this section, we consider the Bernoulli
mixture model when some components are at the boundary of the parameter
space (Kaji et al., 2010).
For an M-dimensional binary vector, x = (x1,. . . , xM)⊤∈{0, 1}M, we deﬁne
the Bernoulli distribution with parameter μ = (μ1,. . . , μM)⊤as
BernM(x|μ) =
M

m=1
μxm
m (1 −μm)(1−xm).
For each element of μ, its conjugate prior, the Beta distribution, is given by
Beta(μ; a, b) =
1
B(a, b)μa−1(1 −μ)b−1,
for a, b > 0.
The Bernoulli mixture model that we consider is given by
p(z|α) = MultinomialK,1(z; α),
(15.86)
p(x|z, {μk}K
k=1) =
K

k=1
6BernM(x; μk)7zk ,
(15.87)
p(α|φ) = DirichletK(α; (φ,. . . , φ)⊤),
(15.88)
p(μk|ξ) =
M

m=1
Beta(μkm; ξ, ξ),
(15.89)
where φ > 0 and ξ > 0 are hyperparameters. Under the constraint, r(H, w) =
rH(H)rw(w), the VB posteriors are given as follows:
r({z(n)}N
n=1, α, {μk}K
k=1) = rz({z(n)}N
n=1)rα(α)rμ({μk}K
k=1),
rz({z(n)}N
n=1) =
N

n=1
MultinomialK,1

z(n);z(n) 
,
rα(α) = Dirichlet

α; (φ1,. . . ,φK)⊤ 
,
rμ({μk}K
k=1) =
K

k=1
M

m=1
Beta

μkm;akm,bkm
 
.

452
15 Asymptotic VB Theory of Mixture Models
The variational parameters {z(n)}N
n=1, {φk}K
k=1, {{akm}M
m=1, {bkm}M
m=1}K
k=1 minimize
the free energy,
F =
N

n=1
K

k=1
z (n)
k
logz (n)
k
+ log
⎛⎜⎜⎜⎜⎜⎝
Γ(K
k=1 φk)
K
k=1 Γ(φk)
⎞⎟⎟⎟⎟⎟⎠−log
 Γ(Kφ)
(Γ(φ))K

+
K

k=1
φk −φ −Nk
 
Ψ(φk) −Ψ(K
k′=1 φk′)
 
+
K

k=1
M

m=1
⎧⎪⎪⎨⎪⎪⎩log
⎛⎜⎜⎜⎜⎝
Γ(akm +bkm)
Γ(akm)Γ(bkm)
⎞⎟⎟⎟⎟⎠−log
 Γ(2ξ)
(Γ(ξ))2

+

akm −ξ −Nkxkm
 
Ψ(akm) −Ψ(akm +bkm)
 
+
bkm −ξ −Nk(1 −xkm)
 
Ψ(bkm) −Ψ(akm +bkm)
 -
,
where
Nk =
N

n=1

z(n)
k

rH(H) ,
xkm = 1
Nk
N

n=1

z(n)
k

rH(H) x(n)
m ,
for k = 1,. . . , K and m = 1,. . . , M. The stationary condition of the free energy
yields
z (n)
k
=
z(n)
k
K
k′=1 z(n)
k′
,
(15.90)
φk = Nk + φ,
(15.91)
akm = Nkxkm + ξ,
(15.92)
bkm = Nk(1 −xkm) + ξ,
(15.93)
where
z(n)
k
= exp

Ψ(φk) −Ψ(K
k′=1 φk′) + M
m=1
,
x(n)
m

Ψ(akm) −Ψ(akm +bkm)
 
+ (1 −x(n)
m )

Ψ(bkm) −Ψ(akm +bkm)
 - 
.
(15.94)
We assume the following condition.
Assumption 15.5
For 0 ≤K∗
1 ≤K∗
0 ≤K, the true distribution q(x) =
p(x|w∗) is represented by K∗
0 components and the parameter is given by
w∗= {α∗
k, μ∗
k}
K∗
0
k=1:
q(x) = p(x|w∗) =
K∗
0

k=1
α∗
kBernM(x; μ∗
k),

15.4 Mixture of Bernoulli with Deterministic Components
453
where
0 < μ∗
km < 1 (1 ≤k ≤K∗
1),
μ∗
km = 0 or 1 (K∗
1 + 1 ≤k ≤K∗
0).
We deﬁne ΔK∗= K∗
0 −K∗
1.
Let K0 be the number of components satisfying Nk/N = Ωp(1) and K1
be the number of components satisfying xkm = Ωp(1) and 1 −xkm = Ωp(1)
for all m = 1, · · · , M. Then, for ΔK ≡K0 −K1 components, it holds that
Nk/N = Ωp(1) and xkm = op(1) or 1 −xkm = op(1). Hence, the K1 components
with Nk/N = Ωp(1) and xkm = Ωp(1) are said to be “nondeterministic” and
the ΔK components are said to be “deterministic,” respectively. We have the
following theorem.
Theorem 15.15
The relative free energy of the Bernoulli mixture model
satisﬁes
FVB(D) = FVB(D) −NS N(D)
=
) M + 1
2
−φ

K1+
1
2 −φ + Mξ

ΔK + Kφ −1
2
1
log N + Ωp

N J
 
+ Op(1),
where J = 1 if K1 < K∗
1 or ΔK < ΔK∗and otherwise J = 0.
The proof of Theorem 15.15 is shown after the next theorem. The following
theorem claims that the numbers of deterministic and nondeterministic com-
ponents are essentially determined by the hyperparameters.
Theorem 15.16
The estimated numbers of components K0 and K1 of the
Bernoulli mixture model are determined as follows:
(1) If M+1
2
−φ > 0 and 1
2 −φ + Mξ > 0, then K1 = K∗
1 and ΔK = ΔK∗.
(2) If M+1
2
−φ > 0 and 1
2 −φ + Mξ < 0, then K1 = K∗
1 and ΔK = K −K∗
1.
(3) If M+1
2
−φ < 0 and 1
2 −φ + Mξ > 0, then K1 = K −ΔK∗and ΔK = ΔK∗.
(4) If M+1
2
−φ < 0 and 1
2 −φ + Mξ < 0, and
(a) if ξ > 1
2, then K1 = K −ΔK∗and ΔK = ΔK∗.
(b) if ξ < 1
2, then K1 = K∗
1 and ΔK = K −K∗
1.
Proof
Minimizing the coefﬁcient of the relative free energy with respect to
K1 and ΔK under the constraint that the true distribution is realizable, i.e.,
K1 ≥K∗
1 and ΔK ≥ΔK∗, we obtain the theorem.
□

454
15 Asymptotic VB Theory of Mixture Models
Proof of Theorem 15.15
From Lemma 15.1, we ﬁrst evaluate R = KL(rw(w)||p(w)). The inequalities of
the di-gamma and log-gamma functions in Eqs. (15.8) and (15.9) yield that
R =
K

k=1
1
2 −φ

log(Nk + φ) +

Kφ −1
2

log(N + Kφ)
+
K

k=1
M

m=1
)1
2 −ξ

log(Nkxkm + ξ) +
1
2 −ξ

log(Nk(1 −xkm) + ξ)
1
+
K

k=1
M

2ξ −1
2

log(Nk + 2ξ) + Op(1).
We consider variational parameters in which K0 components are active, i.e.,
Nk = Ωp(N). Furthermore, without loss of generality, we can assume that
0 < xkm < 1 (1 ≤m ≤M) for K1 nondeterministic components and
xkm = Op(1/N) (1 ≤m ≤M) for ΔK deterministic components. Putting such
variational parameters into the preceding expression, we have the asymptotic
form in the theorem.
Lemmas 15.3 and 15.4 imply that if K1 < K∗
1 or ΔK < ΔK∗, Q = −logCH −
NS N(D) = Ωp(N), and otherwise Q = Op(1). Thus, we obtain the theorem.

16
Asymptotic VB Theory of Other Latent
Variable Models
In this chapter, we proceed to asymptotic analyses of VB learning in other
latent variable models discussed in Section 4.2, namely, Bayesian networks,
hidden Markov models, probabilistic context-free grammar, and latent
Dirichlet allocation.
16.1 Bayesian Networks
In this section, we analyze the VB free energy of the following Bayesian
network model (Watanabe et al., 2009), introduced in Section 4.2.1:
p(x|w) =

z∈Z
p(x, z|w),
(16.1)
p(x, z|w) = p(x|bz)
K

k=1
Tk

i=1
azk,i
(k,i),
p(x|bz) =
M

j=1
Y j

l=1
b
xj,l
(j,l|z),
p(w) =
⎧⎪⎪⎨⎪⎪⎩
K

k=1
p(ak|φ)
⎫⎪⎪⎬⎪⎪⎭
⎧⎪⎪⎪⎨⎪⎪⎪⎩

z∈Z
M

j=1
p(b j|z|ξ)
⎫⎪⎪⎪⎬⎪⎪⎪⎭,
p(ak|φ) = DirichletTk

ak; (φ,. . . , φ)⊤ 
,
p(b j|z|ξ) = DirichletY j

b j|z; (ξ,. . . , ξ)⊤ 
,
where φ > 0 and ξ > 0 are hyperparameters. Here, Z = {(z1,. . . , zK); zk ∈
{ei}Tk
i=1, k = 1,. . . , K}, and zk ∈{ei}Tk
i=1 is the one-of-K representation, i.e.,
zk,i = 1 for some i ∈{1,. . . , Tk} and zk, j = 0 for j  i. Also, x = (x1,. . . , xM)
for x j ∈{el}
Y j
l=1. The number of the parameters of this model is
455

456
16 Asymptotic VB Theory of Other Latent Variable Models
D = Mobs
K

k=1
Tk +
K

k=1
(Tk −1),
(16.2)
where
Mobs =
M

j=1
(Yj −1).
Under the constraint, r(H, w) = rH(H)rw(w), the VB posteriors are given by
rw(w) =
⎧⎪⎪⎨⎪⎪⎩
K

k=1
ra(ak)
⎫⎪⎪⎬⎪⎪⎭
⎧⎪⎪⎪⎨⎪⎪⎪⎩

z∈Z
M

j=1
rb(bj|z)
⎫⎪⎪⎪⎬⎪⎪⎪⎭,
ra(ak) = DirichletTk

ak;φk
 
,
(16.3)
rb(b j|z) = DirichletY j

b j|z;ξ j|z
 
,
(16.4)
rH(H) =
N

n=1
rz(z(n)),
where
rz(z(n) = z) ∝exp
⎛⎜⎜⎜⎜⎜⎝
K

k=1
2
Ψ(φ(k,ik)) −Ψ
!Tk
i′
k=1 φ(k,i′
k)
"3
+
M

j=1
2
Ψ(ξ(j,l(n)
j |z)) −Ψ
Y j
l′=1ξ(j,l′|z)
 3⎞⎟⎟⎟⎟⎟⎟⎠
(16.5)
for z = (ei1,. . . , eiK) and x(n)
j
= el(n)
j . The free energy is given by
F =
K

k=1
⎧⎪⎪⎨⎪⎪⎩log
⎛⎜⎜⎜⎜⎜⎝
Γ(Tk
i=1 φ(k,i))
Tk
i=1 Γ(φ(k,i))
⎞⎟⎟⎟⎟⎟⎠−log
 Γ(Tkφ)
(Γ(φ))Tk

+
Tk

i=1
φ(k,i) −φ −N
z
(k,i)
 
Ψ(φ(k,i)) −Ψ
Tk
i′=1 φ(k,i′)
  ⎫⎪⎪⎬⎪⎪⎭
+

z∈Z
M

j=1
⎧⎪⎪⎨⎪⎪⎩log
⎛⎜⎜⎜⎜⎜⎜⎝
Γ(Y j
l=1ξ(j,l|z))
Y j
l=1 Γ(ξ(j,l|z))
⎞⎟⎟⎟⎟⎟⎟⎠−log
 Γ(Yjξ)
(Γ(ξ))Y j

+
Y j

l=1
ξ(j,l|z) −ξ −N
x
(j,l|z)
 
Ψ(ξ(j,l|z)) −Ψ
Y j
l′=1ξ(j,l′|z)
  
⎫⎪⎪⎪⎬⎪⎪⎪⎭
+
N

n=1

z∈Z
rz(z(n) = z) log rz(z(n) = z),

16.1 Bayesian Networks
457
where
N
z
(k,ik) =
N

n=1

z(n)
k,ik

rH(H) ,
N
x
(j,lj|z) =
N

n=1
x(n)
j,ljrz(z(n) = z),
for
rz(z(n) = (ei1,. . . , eiK)) =
/ K

k=1
z(n)
k,ik
0
rH(H)
.
(16.6)
We assume the following condition:
Assumption 16.1
The true distribution q(x) can be expressed by a Bayesian
network with H hidden nodes, each of which has S k states, for H ≤K, i.e.,
q(x) = p(x|w∗) =

z∈Z∗
p(x, z|w∗) =

z∈Z∗
p(x|b∗
z)
H

k=1
S k

i=1
,
a∗
(k,i)
-zk,i ,
where
p(x|b∗
z) =
M

j=1
Y j

l=1
,
b∗
(j,l|z)
-xj,l
for z ∈Z∗= {(z1,. . . , zH); zk ∈{ei}S k
i=1, k = 1,. . . , H}. The true parameters
w∗= {{a∗
k}H
k=1, {b∗
z}z∈Z∗} are given by
a∗
k = {a∗
(k,i); 1 ≤i ≤S k}
(k = 1,. . . , H),
b∗
z = {b∗
j|z}M
j=1
(z ∈Z∗),
b∗
j|z = {b∗
(j,l|z); 1 ≤l ≤Yj}
( j = 1,. . . , M).
For k > H, we deﬁne S k = 1.
The true distribution can be realized by the model, i.e., the model is given
by Eq. (16.1), where Tk ≥S k holds for k = 1,. . . , H. We assume that the true
distribution is the smallest in the sense that it cannot be realized by any model
with a smaller number of hidden units and with a smaller number of the states
of each hidden unit.
Under this condition, we prove the following theorem, which evaluates the
relative VB free energy. The proof will appear in the next section.

458
16 Asymptotic VB Theory of Other Latent Variable Models
Theorem 16.1
The relative VB free energy of the Bayesian network model
satisﬁes
FVB(D) = FVB(D) −NS N(D) = λ′VB
BN log N + Op(1),
where
λ′VB
BN = φ
K

k=1
Tk −K
2 + min
{uk}
⎧⎪⎪⎨⎪⎪⎩
Mobs
2
K

k=1
uk −

φ −1
2

K

k=1
uk
⎫⎪⎪⎬⎪⎪⎭.
(16.7)
The minimum is taken over the set of positive integers {uk; S k ≤uk ≤Tk}K
k=1.
If K = 1, this is reduced to the case of the naive Bayesian networks whose
Bayes free energy or stochastic complexity has been evaluated (Yamazaki and
Watanabe, 2003a; Rusakov and Geiger, 2005). Bounds for their VB free energy
have also been obtained (Watanabe and Watanabe, 2004, 2005, 2006).
The coefﬁcient λ′VB
BN is given by the solution of the minimization problem
in Eq. (16.7). We present a few exemplary cases as corollaries in this section.
By taking uk = S k for 1 ≤k ≤H and uk = 1 for H + 1 ≤k ≤K, we obtain
the following upper-bound for the VB free energy (Watanabe et al., 2006). This
bound is tight if φ ≤(1 + Mobs min1≤k≤K{S k})/2.
Corollary 16.2
It holds that
FVB(D) ≤λ′VB
BN log N + Op(1),
(16.8)
where
λ′VB
BN = φ
K

k=1
Tk −φK +

φ −1
2

H +
1
2 −φ

H

k=1
S k + Mobs
2
H

k=1
S k.
(16.9)
If K = H = 2, that is, the true network and the model both have two hidden
nodes, solving the minimization problem in Eq. (16.7) gives the following
corollary. Suppose S 1 ≥S 2 and T1 ≥T2.
Corollary 16.3
If K = H = 2,
FVB(D) = λ′VB
BN log N + Op(1),
(16.10)
where
λ′VB
BN
=
⎧⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎩
(T1 −S 1 + T2 −S 2)φ + Mobs
2 S 1S 2 + S 1+S 2
2
−1
(0 < φ ≤1+S 2Mobs
2
),
(T2 −S 2)φ + Mobs
2 T1S 2 + T1+S 2
2
−1
( 1+S 2Mobs
2
< φ ≤1+T1Mobs
2
),
Mobs
2 T1T2 + T1+T2
2
−1
( 1+T1Mobs
2
< φ).
(16.11)

16.1 Bayesian Networks
459
The leading term of the relative Bayes free energy of regular statistical
models is given by (D/2) log N (Schwarz, 1978), where D is the number of
parameters in Eq. (16.2). Corollary 16.3 claims that the coefﬁcient λ′VB
BN of the
leading term is smaller than D/2 when φ ≤1+T1Mobs
2
.
Proof of Theorem 16.1
From Lemma 15.1, we can rewrite the free energy as follows:
FVB(D) = min
rw(w) [R + Q] ,
(16.12)
where
R = KL(rw(w)||p(w)),
Q = −logCH = −log

H
log p(D, H|w)
rw(w) .
From Eqs. (16.3), (16.4), and (16.5), we obtain Q and R in Eq. (16.12) as
follows:
Q = −
N

n=1
log

z(n)

log p(x(n), z(n)|w)

rw(w)
= −
N

n=1
log
⎛⎜⎜⎜⎜⎜⎜⎜⎝

z=(ei1,...,eiK )
exp
⎛⎜⎜⎜⎜⎜⎝
K

k=1
,
Ψ(N
z
(k,ik) + φ) −Ψ(N + Tkφ)
-
+
M

j=1
Y j

l=1
x(n)
j,l
,
Ψ(N
x
(j,l|z) + ξ) −Ψ(N
x
z + Yjξ)
-⎞⎟⎟⎟⎟⎟⎟⎠
⎞⎟⎟⎟⎟⎟⎟⎠,
(16.13)
and
R =
K

k=1
KL(ra(ak)||p(ak|φ)) +

z
M

j=1
KL(rb(b j|z)||p(b j|z|ξ))
=
K

k=1
⎡⎢⎢⎢⎢⎢⎢⎣
Tk

i=1
,
N
z
(k,i)Ψ(N
z
(k,i) + φ) −log Γ(N
z
(k,i) + φ)
-
−NΨ(N + Tkφ) + log Γ(N + Tkφ) + log Γ(φ)Tk
Γ(Tkφ)
H
+

z
M

j=1
⎡⎢⎢⎢⎢⎢⎢⎣
Y j

l=1
,
N
x
(j,l|z)Ψ(N
x
(j,l|z) + ξ) −log Γ(N
x
(j,l|z) + ξ)
-
−N
x
zΨ(N
x
z + Yjξ) + log Γ(N
x
z + Yjξ) + log Γ(ξ)Y j
Γ(Yjξ)
H
.
(16.14)

460
16 Asymptotic VB Theory of Other Latent Variable Models
Furthermore, by using the inequalities for the di-gamma and log-gamma
functions in Eqs. (15.8) and (15.9), we can bound Q as follows:
Q ≤−
N

n=1
log
⎛⎜⎜⎜⎜⎜⎜⎜⎝

z=(ei1,...,eiK )
exp
⎛⎜⎜⎜⎜⎜⎜⎝
K

k=1
⎧⎪⎪⎨⎪⎪⎩log
N
z
(k,ik) + φ
N + Tkφ −
1
N
z
(k,ik) + φ
+
1
2(N + Tkφ)
⎫⎪⎪⎬⎪⎪⎭
+
M

j=1
Y j

l=1
x(n)
j,l
⎧⎪⎪⎨⎪⎪⎩log
N
x
(j,l|z) + ξ
N
x
z + Yjξ
−
1
N
x
(j,l|z) + ξ
+
1
2(N
x
z + Yjξ)
⎫⎪⎪⎬⎪⎪⎭
⎞⎟⎟⎟⎟⎟⎟⎠
⎞⎟⎟⎟⎟⎟⎟⎠.
(16.15)
We can also evaluate R in Eq. (16.12) as follows:
R =
K

k=1
)
Tkφ −1
2

log (N + Tkφ)
1
−
K

k=1
Tk

i=1
)
φ −1
2

log

N
z
(k,i) + φ
 1
+

z
M

j=1
⎧⎪⎪⎪⎨⎪⎪⎪⎩

Yjξ −1
2

log

N
x
z + Yjξ
 
−
Y j

l=1

ξ −1
2

log

N
x
(j,l|z) + ξ
 
⎫⎪⎪⎪⎬⎪⎪⎪⎭
+ Op(1).
(16.16)
Since FVB(D) is given as the minimum value of the function of {N
x
(j,l|z)}, we
can obtain from Eq. (16.12) an upper-bound for FVB(D) by substituting each
N
x
(j,l|z) by any speciﬁc value. Therefore, let uk be a natural number such that
S k ≤uk ≤Tk for k = 1,. . . , K and consider the following N
x
(j,l|z) for each j
and l:
N
x
(j,l|z) = Nb∗
(j,l|z)
K

k=1
a(k,ik),
(16.17)
where z = (emin{l1,S 1}, emin{l2,S 2},. . . , emin{lH,S H}) and
a(k,ik) =
⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩
a∗
(k,ik)
(1 ≤ik ≤S k −1),
a∗
(k,S k)/(uk −S k + 1)
(S k ≤ik ≤uk),
0
(otherwise).
(16.18)
This corresponds to the case where uk (≥S k) states of the kth hidden node are
active for k = 1,. . . , H. Then we have N
x
z = N K
k=1 a(k,ik) and N
z
(k,i) = Na(k,i).
Substituting them into Eq. (16.16) yields
R =
⎧⎪⎪⎨⎪⎪⎩φ
K

k=1
Tk −K
2 + Mobs
2
K

k=1
uk −

φ −1
2

K

k=1
uk
⎫⎪⎪⎬⎪⎪⎭log N + Op(1).
(16.19)

16.2 Hidden Markov Models
461
From Eq. (16.15), we obtain
Q ≤−
N

n=1
log

p(x(n)|w∗) exp

Op
 1
N

= NS N(D) + Op(1).
(16.20)
From Eqs. (16.12), (16.19), and (16.20), we have proved that FVB(D) is upper-
bounded by the right-hand side of Eq. (16.19) for any data set D and {uk; S k ≤
uk ≤Tk}.
If the number of states such that N
z
(k,i) = Θp(N) is less than S k, i.e., uk < S k
for some k, the predictive distribution ⟨p(x|w)⟩rw(w) cannot approach the true
distribution p(x|w∗). Then Lemma 15.4 implies that Q −NS N(D) = Ωp(N).
Hence, minimizing the coefﬁcient of the leading term in Eq. (16.19) under the
constraints S k ≤uk ≤Tk for all k, we complete the proof.
16.2 Hidden Markov Models
Next we analyze the VB free energy of hidden Markov models (HMMs)
(Hosino et al., 2005, 2006b), introduced in Section 4.2.2. Suppose that we
observe N sequences, D = {X(1),. . . , X(N)}, where each sequence X(n) =
(x(n,1),. . . , x(n,T)) has length T. We consider the asymptotic analysis for the
VB free energy as the number of i.i.d. sample sequences tends to inﬁnity, i.e.,
N →∞while T is a ﬁxed constant.
The model for observed and hidden sequences X = (x(1),. . . , x(T)) and
Z = (z(1),. . . , z(T)) is given by
p(X|w) =

Z
p(X, Z|w),
(16.21)
p(X, Z|w) =
M

m=1
bx(1)
m
1,m
T

t=2
K

k=1
K

l=1
a
z(t)
l z(t−1)
k
k,l
M

m=1
b
z(t)
k x(t)
m
k,m ,
p(A|φ) =
K

k=1
DirichletK

ak; (φ,. . . , φ)⊤ 
,
p(B|ξ) =
K

k=1
DirichletM
bk; (ξ,. . . , ξ)⊤ 
,
where φ > 0 and ξ > 0 are hyperparameters. Let H = {Z(1),. . . , Z(N)} be the
set of hidden sequences. Under the constraint, r(H, w) = rH(H)rw(w), the VB
posteriors are given by

462
16 Asymptotic VB Theory of Other Latent Variable Models
rw(w) = rA(A)rB(B),
rA(A) =
K

k=1
DirichletK

ak; (φk,1,. . . ,φk,K)⊤ 
,
rB(B) =
K

k=1
DirichletM
bk; (ξk,1,. . . ,ξk,M)⊤ 
,
rH(H) =
N

n=1
rZ(Z(n)),
rZ(Z(n)) =
1
CZ(n) exp
⎛⎜⎜⎜⎜⎜⎝
T

t=2
K

k=1
K

l=1
z(n,t)
k
z(n,t−1)
l
⎧⎪⎪⎨⎪⎪⎩Ψ(φk,l) −Ψ
⎛⎜⎜⎜⎜⎜⎝
K

l′=1
φk,l′
⎞⎟⎟⎟⎟⎟⎠
⎫⎪⎪⎬⎪⎪⎭
+
T

t=1
K

k=1
M

m=1
z(n,t)
k
x(t)
m
⎧⎪⎪⎨⎪⎪⎩Ψ(ξk,m) −Ψ
⎛⎜⎜⎜⎜⎜⎝
M

m′=1
ξk,m′
⎞⎟⎟⎟⎟⎟⎠
⎫⎪⎪⎬⎪⎪⎭
⎞⎟⎟⎟⎟⎟⎠,
where CZ(n) is the normalizing constant. After the substitution of Eq. (15.5),
the free energy is given by
F =
K

k=1
⎧⎪⎪⎨⎪⎪⎩log
⎛⎜⎜⎜⎜⎜⎝
Γ(K
l=1 φk,l)
K
l=1 Γ(φk,l)
⎞⎟⎟⎟⎟⎟⎠+
K

l=1
φk,l −φ
 
Ψ(φk,l) −Ψ(K
l′=1 φk,l′)
 
+ log
⎛⎜⎜⎜⎜⎜⎝
Γ(M
m=1ξk,m)
M
m=1 Γ(ξk,m)
⎞⎟⎟⎟⎟⎟⎠+
M

m=1
ξk,m −ξ
 
Ψ(ξk,m) −Ψ(M
m′=1ξk,m′)
 ⎫⎪⎪⎬⎪⎪⎭
−K log
 Γ(Kφ)
(Γ(φ))K

−K log
 Γ(Mξ)
(Γ(ξ))M

−
N

n=1
logCZ(n).
The variational parameters satisfy
φk,l = N
[z]
k,l + φ,
ξk,m = N
[x]
k,m + ξ,
for the expected sufﬁcient statistics deﬁned by
N
[z]
k,l =
N

n=1
T

t=2

z(n,t)
l
z(n,t−1)
k

rH(H) ,
N
[x]
k,m =
N

n=1
T

t=1

z(n,t)
k

rH(H) x(n,t)
m .
We assume the following condition.

16.2 Hidden Markov Models
463
Assumption 16.2
The true distribution q(X) has K0 hidden states and emits
M-valued discrete symbols:
q(X) = p(X|w∗) =

Z
M

m=1
(b∗
1m)x(1)
m
T

t=2
K0

k=1
K0

l=1
(a∗
kl)z(t)
l z(t−1)
k
M

m=1
(b∗
km)z(t)
k x(t)
m ,
(16.22)
where 
Z is taken over all possible values of the hidden variables. Moreover,
the true parameter is deﬁned by
w∗= (A∗, B∗) = ((a∗
kl), (b∗
km)),
where A∗∈RK0×K0 and B∗∈RK0×m. The number of hidden states K0 of the
true distribution is the smallest under this parameterization (Ito et al., 1992)
and all parameters {a∗
kl, b∗
km} are strictly positive:
w∗= ((a∗
kl > 0), (b∗
km > 0)) (1 ≤k, l ≤K0, 1 ≤m ≤M).
The statistical model given by Eq. (16.21) can attain the true distribution, thus
the model has K (≥K0) hidden states.
Under this assumption, the next theorem evaluates the relative VB free
energy. Here S N(D) = −1
N
N
n=1 log p(X(n)|w∗) is the empirical entropy of the
true distribution (16.22).
Theorem 16.4
The relative VB free energy of HMMs satisﬁes
FVB(D) = FVB(D) −NS N(D) = λ′VB
HMM log N + Op(1),
where
λ′VB
HMM =
⎧⎪⎪⎪⎨⎪⎪⎪⎩
K0(K0−1)+K0(M−1)
2
+ K0(K −K0)φ

0 < φ ≤K0+K+M−2
2K0
 
,
K(K−1)+K(M−1)
2
 K0+K+M−2
2K0
< φ
 
.
(16.23)
Proof
As in the models discussed in the previous sections, we evaluate
the KL divergence from the posterior distribution to the prior distribution of
parameters:
R = KL(rw(w)||p(w))
=
K

k=1
⎡⎢⎢⎢⎢⎢⎣log Γ(Nk + Kφ) −NkΨ(Nk + Kφ)
−
K

l=1
2
log Γ(N
[z]
k,l + φ) −N
[z]
k,lΨ(N
[z]
k,l + φ)
3

464
16 Asymptotic VB Theory of Other Latent Variable Models
+ log Γ(Nk + Mξ) −NkΨ(Nk + Mξ)
−
M

m=1
2
log Γ(N
[x]
k,m + ξ) −N
[x]
k,mΨ(N
[x]
k,m + ξ)
3⎤⎥⎥⎥⎥⎥⎦+ Op(1).
(16.24)
Using the inequalities of the di-gamma and the log-gamma functions in
Eqs. (15.8) and (15.9), we have
R =
K

k=1
⎡⎢⎢⎢⎢⎢⎣

Kφ −1
2

log(Nk + Kφ) −
K

l=1

φ −1
2

log(N
[z]
k,l + φ)
+

Mξ −1
2

log(Nk + Mξ) −
M

m=1

ξ −1
2

log(N
[x]
k,m + ξ)
⎤⎥⎥⎥⎥⎥⎦+ Op(1).
(16.25)
We divide the sum over k and l in Eq. (16.25) to the necessary K0 and redundant
K −K0 terms. Moreover, we assume that additional l (0 ≤l ≤K −K0) hidden
states are used, i.e., having Nk = Θp(N).
R
log N =
K0

k=1
⎧⎪⎪⎨⎪⎪⎩
!
Kφ + M
2 −1
"
−
K0

l=1

φ −1
2
⎫⎪⎪⎬⎪⎪⎭+ g(l) + Op

1
log N

,
(16.26)
where g(l) is given by
g(l) =
!
Kφ + M
2 −1
"
l −

φ −1
2

(2K0l + l2).
If the number of states with Nk = Θp(N) is less than K0, Lemma 15.4 implies
that Q = −logCH −NS N(D) = Ωp(N) for data sequences in the strongly
ε-typical set. Otherwise, we can upper-bound FVB(D) −NS N(D) so that
Q = Op(1) similarly to the models in the previous sections. Hence, minimizing
the right-hand side of Eq. (16.26) with respect to l, we can evaluate the VB
free energy.
The minimum of g(l) is achieved by
⎧⎪⎪⎪⎨⎪⎪⎪⎩
l = 0

0 < φ ≤K0+K+M−2
2K0
 
,
l = K −K0
 K0+K+M−2
2K0
< φ
 
.
Putting this back into Eq. (16.26), we obtain the theorem.
□
Next we consider the simple left-to-right HMMs.
Assumption 16.3
In the simple left-to-right HMMs, transition from each
hidden state is constrained to itself or the next hidden state:
{ak,l = 0, l  {k, k + 1}}.
(16.27)

16.2 Hidden Markov Models
465
Figure 16.1 State transition diagram of a left-to-right HMM.
Thus, only ak,k+1 is a substantial parameter in the transition probability.
Figure 16.1 illustrates the state transition diagram of a left-to-right HMM.
The next theorem evaluates the relative VB free energy of the left-to-right
HMM.1
Theorem 16.5
The relative VB free energy of the left-to-right HMM satisﬁes
FVB(D) = λ′VB
LR−HMM log N + Op(1),
where
λ′VB
LR−HMM =
⎧⎪⎪⎪⎨⎪⎪⎪⎩
(K0−1)+K0(M−1)
2
+ φ
(φ ≤M(K−K0)
2
),
(K−1)+K(M−1)
2
(φ > M(K−K0)
2
).
(16.28)
Proof
From the constraints of the transition probabilities in Eq. (16.27), the
asymptotic form of the KL divergence from the VB posterior to the prior is
given by
R = KL(rw(w)||p(w))
=
K−1

k=1
I
2φ −1
2

log(Nk + 2φ) −

φ −1
2
 2
log(N
[z]
k,(k+1) + φ) + log(N
[z]
k,k + φ)
3H
+
K

k=1
⎡⎢⎢⎢⎢⎢⎣

Mξ −1
2

log(Nk + Mξ) −
M

m=1

ξ −1
2

log(N
[x]
k,m + ξ)
⎤⎥⎥⎥⎥⎥⎦
+ Op(1).
(16.29)
If K hidden states are used, all the variables, Nk, N
[z]
k,k, N
[z]
k,(k+1), and N
[x]
k,m are
in the order of N, which leads to the asymptotic form in the theorem. If
some states are not used, we assume that the (K0 + l)th state is the last state
that is effectively used. More speciﬁcally, if we consider the case where Nk,
N
[z]
k,k, N
[z]
k,(k+1), and N
[x]
k,m are Θp(N) for K0+l−1 states and N
[z]
(K0+l),(K0+l+1) = Op(1)
and N
[z]
(K0+l),(K0+l) = Θp(N) (and hence, NK0+l = Θp(N)), we obtain
R
log N = K0 −1
2
+ φ + K0
M −1
2
+ g(l) + Op

1
log N

,
1 This theorem is not obtained as a special case of Theorem 16.4 since some of the transition
probabilities are ﬁxed to zero and are no longer parameters.

466
16 Asymptotic VB Theory of Other Latent Variable Models
where
g(l) = M
2 l
for 0 ≤l ≤K −K0. Since the minimum of g(l) is obviously obtained by l = 0,
we obtain the theorem.
□
16.3 Probabilistic Context-Free Grammar
In this section, we asymptotically analyze the VB free energy of probabilistic
context-free grammar (PCFG), introduced in Section 4.2.3, as the number N
of the sequences in the training corpus D = {X(1),. . . , X(N)} goes to inﬁnity
(Hosino et al., 2006a). The PCFG model is deﬁned by
p(X|w) =

Z∈T(X)
p(X, Z|w),
(16.30)
p(X, Z|w) =
K

i, j,k=1

ai→jk
 cZ
i→jk
L

l=1
K

i=1
M

m=1
(bi→m)z(l)
i x(l)
m ,
w = {{ai}K
i=1, {bi}K
i=1},
ai = {ai→jk}K
j,k=1 (1 ≤i ≤K),
bi = {bi→m}M
m=1 (1 ≤i ≤K),
p({ai}K
i=1|φ) =
K

i=1
DirichletK2

ai; (φ,. . . , φ)⊤ 
,
p({bi}K
i=1|ξ) =
K

i=1
DirichletM

bi; (ξ,. . . , ξ)⊤ 
,
where φ > 0 and ξ > 0 are hyperparameters. Here T(X) is the set of derivation
sequences that generate X, cZ
i→jk is the count of the transition rule from the
nonterminal symbol i to the pair of nonterminal symbols (j, k) appearing in
the derivation sequence Z, and z(l) = (z(l)
1 ,. . . ,z(l)
K ) is the indicator of the
(nonterminal) symbol generating the lth output symbol of X.
Under the constraint, r(H, w) = rH(H)rw(w), the VB posteriors are
given by
rw(w) = ra({ai}K
i=1)rb({bi}K
i=1),
ra({ai}K
i=1) =
K

i=1
DirichletK2

ai; (φi→11,. . . ,φi→KK)⊤ 
,

16.3 Probabilistic Context-Free Grammar
467
rb({bi}K
i=1) =
K

i=1
DirichletM

bi; (ξi→1,. . . ,ξi→M)⊤ 
,
rH(H) =
N

n=1
rz(Z(n)),
rz(Z(n)) =
1
CZ(n) exp $γZ(n)% ,
(16.31)
γZ(n) =
K

i, j,k=1
cZ(n)
i→jk
,
Ψ
φi→jk
 
−Ψ
K
j′=1
K
k′=1 φi→j′k′
 -
+
L

l=1
K

i=1
M

m=1
z(n,l)
i
x(n,l)
m
,
Ψ
ξi→m
 
−Ψ
M
m′=1ξi→m′
 -
,
where CZ(n) = 
Z∈T(X(n)) exp(γZ) is the normalizing constant and T(X(n)) is
the set of derivation sequences that generate X(n). After the substitution of
Eq. (15.5), the free energy is given by
F =
K

i=1
⎧⎪⎪⎨⎪⎪⎩log
⎛⎜⎜⎜⎜⎜⎜⎝
Γ(K
j,k=1 φi→jk)
K
j,k=1 Γ(φi→jk)
⎞⎟⎟⎟⎟⎟⎟⎠
+
K

j,k=1
φi→jk −φ
 
Ψ
φi→jk
 
−Ψ
K
j′,k′=1 φi→j′k′)
  
+ log
⎛⎜⎜⎜⎜⎜⎜⎝
Γ
M
m=1ξi→m
 
M
m=1 Γ
ξi→m
 
⎞⎟⎟⎟⎟⎟⎟⎠+
M

m=1
ξi→m −ξ
 
Ψ
ξi→m
 
−Ψ
M
m′=1ξi→m′
  
⎫⎪⎪⎪⎬⎪⎪⎪⎭
−K log
 Γ(K2φ)
(Γ(φ))K2

−K log
 Γ(Mξ)
(Γ(ξ))M

−
N

n=1
logCZ(n).
The variational parameters satisfy
φi→jk = N
z
i→jk + φ,
ξi→m = N
x
i→m + ξ,
where
N
z
i→jk =
N

n=1
L

l=1

cZ(n)
i→jk

rz(Z(n)) ,
N
x
i→m =
N

n=1
L

l=1

z(n,l)
i

rz(Z(n)) x(n,l)
m .
We assume the following condition.

468
16 Asymptotic VB Theory of Other Latent Variable Models
Assumption 16.4
The true distribution q(X) has K0 nonterminal symbols
and M terminal symbols with parameter w∗:
q(X) = p(X|w∗) =

Z∈T(X)
p(X, Z|w∗).
(16.32)
The true parameters are
w∗= {{a∗
i }K0
i=1, {b∗
i }K0
i=1},
a∗
i = {a∗
i→jk}K0
j,k=1 (1 ≤i ≤K0),
b∗
i = {b∗
i→m}M
m=1 (1 ≤i ≤K0),
which satisfy the constraints
a∗
i→ii = 1 −

(j,k)(i,i)
a∗
i→jk, b∗
i→M = 1 −
M−1

m=1
b∗
i→m,
respectively. Since PCFG has nontrivial nonidentiﬁability as in HMM (Ito
et al., 1992), we assume that K0 is the smallest number of nonterminal
symbols under this parameterization. The statistical model given by Eq.
(16.30) includes the true distribution, namely, the number of nonterminal
symbols K satisﬁes the inequality K0 ≤K.
Under this assumption, the next theorem evaluates the relative VB free
energy. Here S N(D) = −1
N
N
n=1 log p(X(n)|w∗) is the empirical entropy of the
true distribution (16.32).
Theorem 16.6
The relative VB free energy of the PCFG model satisﬁes
FVB(D) = FVB(D) −NS N(D) = λ′VB
PCFG log N + Op(1),
where
λ′VB
PCFG =
⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩
K0(K2
0−1)+K0(M−1)
2
+ K0(K2 −K2
0)φ
!
0 < φ ≤
K2
0+KK0+K2+M−2
2(K2
0+KK0)
"
,
K(K2−1)+K(M−1)
2
!
K2
0+KK0+K2+M−2
2(K2
0+KK0)
< φ
"
.
(16.33)
Proof
Based on Lemma 15.1, similarly to the models in the previous sections,
we evaluate R = KL(rw(w)||p(w)). It is expressed by expected sufﬁcient
statistics as
R =
K

i=1
⎡⎢⎢⎢⎢⎢⎣log Γ(N
z
i + K2φ) −N
z
iΨ(N
z
i + K2φ)
−
K

j,k=1
,
log Γ(N
z
i→jk + φ) −N
z
i→jkΨ(N
z
i→jk + φ)
-

16.3 Probabilistic Context-Free Grammar
469
+ log Γ(N
x
i + Mξ) −N
x
i Ψ(N
x
i + Mξ)
−
M

m=1
,
log Γ(N
x
i→m + ξ) −N
x
i→mΨ(N
x
i→m + ξ)
-⎤⎥⎥⎥⎥⎥⎦+ Op(1).
Using the inequalities of the di-gamma and the log-gamma functions in
Eqs. (15.8) and (15.9), we have
R =
K

i=1
⎡⎢⎢⎢⎢⎢⎢⎣

K2φ −1
2

log(N
z
i + K2φ) −
K

j,k=1

φ −1
2

log(N
z
i→jk + φ)
+

Mξ −1
2

log(N
x
i + Mξ) −
M

m=1

ξ −1
2

log(N
x
i→m + ξ)
⎤⎥⎥⎥⎥⎥⎦+ Op(1).
(16.34)
We divide the sum over i, j, and k in Eq. (16.34) to the necessary K0
and redundant K −K0 terms. Moreover, we assume the trained model uses
redundant l
(0 ≤l ≤K −K0) nonterminal symbols, i.e., it holds that
N
z
i = Θp(N):
R
log N =
K0

i=1
⎧⎪⎪⎪⎨⎪⎪⎪⎩
!
K2φ + M
2 −1
"
−
K0

j,k=1

φ −1
2
⎫⎪⎪⎪⎬⎪⎪⎪⎭
+
K0

i=1
⎧⎪⎪⎪⎨⎪⎪⎪⎩
K0

j,k=1

φ −1
2

−
K0+l

j,k=1

φ −1
2
⎫⎪⎪⎪⎬⎪⎪⎪⎭
+
K0+l

i=K0+1
⎧⎪⎪⎪⎨⎪⎪⎪⎩
!
K2φ + M
2 −1
"
−
K0+l

j,k=1

φ −1
2
⎫⎪⎪⎪⎬⎪⎪⎪⎭+ Op

1
log N

=
!
K2φ + M
2 −1
"
−K2
0

φ −1
2

+ g(l) + Op

1
log N

,
(16.35)
where g(l) is given by
g(l) =
!
K2φ + M
2 −1
"
l −

φ −1
2
 ,
(K0 + l)3 −K3
0
-
.
By Lemma 15.4, similarly to the HMM, we can evaluate the VB free energy
by minimizing g(l). The minimum of g(l) is achieved by
⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩
l = 0
(0 < φ ≤
K2
0+KK0+K2+M−2
2(K2
0+KK0)
),
l = K −K0
(
K2
0+KK0+K2+M−2
2(K2
0+KK0)
< φ).
Putting this back into Eq. (16.35), we obtain the theorem.
□

470
16 Asymptotic VB Theory of Other Latent Variable Models
16.4 Latent Dirichlet Allocation
In this section, we investigate the VB free energy of the latent Dirichlet
allocation (LDA) introduced in Section 4.2.4. We also analyze the asymptotic
behavior of MAP learning and partially Bayesian learning, which are often
used alternatively to VB learning, and discuss similarities and dissimilarities
between those learning algorithms.
We consider the following LDA model:
p(w(n,m), z(n,m)|Θ, B) = p(w(n,m)|z(n,m), B)p(z(n,m)|Θ),
p(w(n,m)|z(n,m), B) =
L

l=1
H

h=1
(Bl,h)w(n,m)
l
z(n,m)
h
,
p(z(n,m)|Θ) =
H

h=1
(θm,h)z(n,m)
h
,
p(Θ|α) =
M

m=1
DirichletH(θm; (α,. . . , α)⊤),
p(B|η) =
H

h=1
DirichletL(βh; (η,. . . , η)⊤).
Here we have assumed that the priors are symmetric and have hyperparameters
α1 = . . . = αH = α > 0, η1 = . . . = ηL = η > 0, respectively.
Under the constraint, r(w, H) = rΘ,B(Θ, B)rz

{{z(n,m)}N(m)
n=1 }M
m=1
 
, the VB
posteriors are given by
rz

{{z(n,m)}N(m)
n=1 }M
m=1
 
=
M

m=1
N(m)

n=1
MultinomialH,1

z(n,m);z(n,m) 
,
rΘ,B(Θ, B) = rΘ(Θ)rB(B),
rΘ(Θ) =
M

m=1
Dirichlet
θm;αm
 
,
rB(B) =
H

h=1
Dirichlet $βh;ηh
% .
The free energy is given by
F =
M

m=1
⎛⎜⎜⎜⎜⎝log
⎛⎜⎜⎜⎜⎝
Γ(H
h=1 αm,h)
H
h=1 Γ(αm,h)
⎞⎟⎟⎟⎟⎠−log
Γ(Hα)
Γ(α)H
⎞⎟⎟⎟⎟⎠
+
H

h=1
⎛⎜⎜⎜⎜⎝log
⎛⎜⎜⎜⎜⎝
Γ(L
l=1ηl,h)
L
l=1 Γ(ηl,h)
⎞⎟⎟⎟⎟⎠−log
Γ(Lη)
Γ(η)L
⎞⎟⎟⎟⎟⎠

16.4 Latent Dirichlet Allocation
471
+
M

m=1
H

h=1
!
αm,h −(N
(m)
h
+ α)
" 
Ψ(αm,h) −Ψ(H
h′=1 αm,h′)
 
+
H

h=1
L

l=1

ηl,h −(Wl,h + η)
 
Ψ(ηl,h) −Ψ(L
l′=1ηl′,h)
 
+
M

m=1
N(m)

n=1
H

h=1
z(n,m)
h
logz(n,m)
h
,
where
N
(m)
h
=
N(m)

n=1

z(n,m)
h

rz

{{z(n,m)}N(m)
n=1 }M
m=1
 ,
Wl,h =
M

m=1
N(m)

n=1
w(n,m)
l

z(n,m)
h

rz

{{z(n,m)}N(m)
n=1 }M
m=1
 ,
for the observed data D = {{w(n,m)}N(m)
n=1 }M
m=1. The variational parameters satisfy
αm,h = N
(m)
h
+ α,
(16.36)
ηl,h = Wl,h + η,
(16.37)
z(n,m)
h
=
z(n,m)
h
H
h′=1 z(n,m)
h′
for
z(n,m)
h
= exp
⎛⎜⎜⎜⎜⎜⎝
,
Ψ(αm,h) −Ψ
H
h′=1 αm,h′
 -
+
L

l=1
w(n,m)
l
,
Ψ(ηl,h) −Ψ
L
l′=1ηl′,h
 -⎞⎟⎟⎟⎟⎟⎠.
Based on Lemma 15.1, we decompose the free energy as follows:
F = R + Q,
(16.38)
where
R = KL (rΘ(Θ)rB(B)||p(Θ|α)p(B|η))
=
M

m=1
⎛⎜⎜⎜⎜⎜⎝log Γ(H
h=1 αm,h)
H
h=1 Γ(αm,h)
Γ(α)H
Γ(Hα) +
H

h=1
$αm,h −α% 
Ψ(αm,h) −Ψ(H
h′=1 αm,h′)
 ⎞⎟⎟⎟⎟⎟⎠
+
H

h=1
⎛⎜⎜⎜⎜⎜⎝log Γ(L
l=1ηl,h)
L
l=1 Γ(ηl,h)
Γ(ηl)L
Γ(Lηl) +
L

l=1
$ηl,h −ηl
% 
Ψ(ηl,h) −Ψ(L
l′=1ηl′,h)
 ⎞⎟⎟⎟⎟⎟⎠,
(16.39)

472
16 Asymptotic VB Theory of Other Latent Variable Models
Q = −logCH
= −
M

m=1
N(m)
L

l=1
Vl,m log
⎛⎜⎜⎜⎜⎜⎜⎝
H

h=1
exp $Ψ(αm,h)%
exp

Ψ(H
h′=1 αm,h′)
 
exp $Ψ(ηl,h)%
exp

Ψ(L
l′=1ηl′,h)
 
⎞⎟⎟⎟⎟⎟⎟⎠.
(16.40)
Here, V ∈RL×M is the empirical word distribution matrix with its entries given
by Vl,m =
1
N(m)
N(m)
n=1 w(n,m)
l
.
16.4.1 Asymptotic Analysis of VB Learning
Here we analyze the VB free energy of LDA in the asymptotic limit when
N ≡minm N(m) →∞(Nakajima et al., 2014). Unlike the analyses for the
latent variable models in the previous sections, we do not assume L, M ≪N,
but 1 ≪L, M, N at this point. This amounts to considering the asymptotic
limit when L, M, N →∞with a ﬁxed mutual ratio, or equivalently, assuming
L, M ∼O(N).
We assume the following condition on the true distribution.
Assumption 16.5
The word distribution matrix V is a sample from the
multinomial distribution with the true parameter U∗∈RL×M whose rank is
H∗∼O(1), i.e., U∗= B∗Θ∗⊤where Θ∗∈RM×H∗and B∗∈RL×H∗.2 The
number of topics of the model H is set to H = min(L, M) (i.e., the matrix BΘ⊤
can express any multinomial distribution).
The stationary conditions, Eqs. (16.36) and (16.37), lead to the following
lemma:
Lemma 16.7
Let BΘ
⊤=

BΘ⊤
rΘ,B(Θ,B). Then it holds that
*
(BΘ⊤−BΘ
⊤)2
l,m
+
rΘ,B(Θ,B) = Op(N−2),
(16.41)
Q = −
M

m=1
N(m)
L

l=1
Vl,m log(BΘ
⊤)l,m + Op(M).
(16.42)
Proof
For the Dirichlet distribution p(a|˘a) ∝H
h=1 a˘ah−1
h
, the mean and the
variance are given as follows:
ah = ⟨ah⟩p(a|˘a) = ˘ah
˘a0
,
⟨(ah −ah)2⟩p(a|˘a) = ˘ah(˘a0 −˘ah)
˘a2
0(˘a0 + 1) ,
where ˘a0 = H
h=1 ˘ah.
2 More precisely, U∗= B∗Θ∗⊤+ O(N−1) is sufﬁcient.

16.4 Latent Dirichlet Allocation
473
For ﬁxed N, R, deﬁned by Eq. (16.39), diverges to +∞if αm,h →+0 for
any (m, h) or ηl,h →+0 for any (l, h). Therefore, the global minimizer of the
free energy (16.38) is in the interior of the domain, where the free energy is
differentiable. Consequently, the global minimizer is a stationary point. The
stationary conditions (16.36) and (16.37) imply that
αm,h ≥α,
ηl,h ≥η,
(16.43)
H

h=1
αm,h =
H

h=1
α + N(m),
L

l=1
ηl,h =
L

l=1
η +
M

m=1
(αm,h −α).
(16.44)
Therefore, we have

(Θm,h −Θm,h)2
rΘ(Θ) = Op(N−2)
for all (m, h),
(16.45)
!
max
m
Θm,h
"2 
(Bl,h −Bl,h)2
rB(B) = Op(N−2)
for all (l, h),
(16.46)
which leads to Eq. (16.41).
By using Eq. (15.8), Q is bounded as follows:
Q ≤Q ≤Q,
where
Q = −
M

m=1
N(m)
L

l=1
Vl,m log
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
H

h=1
αm,h
H
h′=1 αm,h′
ηl,h
L
l′=1 ηl′,h
exp

−
1
αm,h

exp
⎛⎜⎜⎜⎜⎜⎝−
1
2 H
h′=1 αm,h′
⎞⎟⎟⎟⎟⎟⎠
exp

−
1
ηl,h

exp
⎛⎜⎜⎜⎜⎜⎝−
1
2 L
l′=1 ηl′,h
⎞⎟⎟⎟⎟⎟⎠
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠,
Q = −
M

m=1
N(m)
L

l=1
Vl,m log
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
H

h=1
αm,h
H
h′=1 αm,h′
ηl,h
L
l′=1 ηl′,h
exp

−
1
2αm,h

exp
⎛⎜⎜⎜⎜⎜⎝−
1
H
h′=1 αm,h′
⎞⎟⎟⎟⎟⎟⎠
exp

−
1
2ηl,h

exp
⎛⎜⎜⎜⎜⎜⎝−
1
L
l′=1 ηl′,h
⎞⎟⎟⎟⎟⎟⎠
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠.
Using Eqs. (16.45) and (16.46), we have Eq. (16.42), which completes the
proof of Lemma 16.7.
□
Eq. (16.41) implies the convergence of the posterior. Let u∗
m = B∗(θ
∗
m)⊤be
the true probability mass function for the mth document and um = B(θm)⊤
be its predictive probability. Deﬁne a measure of how far the predictive
distributions are from the true distributions by
J =
M

m=1
N(m)
N KL(u∗
m||um).
(16.47)
Then, by the same arguments as the proof of Lemma 15.4, Eq. (16.42) leads to
the following lemma:

474
16 Asymptotic VB Theory of Other Latent Variable Models
Lemma 16.8
Q is minimized when J = Op(1/N), and it holds that
Q = NS N(D) + Op(JN + LM),
where
S N(D) = −1
N log p(D|Θ∗, B∗) = −
M

m=1
N(m)
N
L

l=1
Vl,m log(B∗Θ∗)l,m.
Lemma 16.8 simply states that Q/N converges to the empirical entropy
S N(D) of the true distribution if and only if the predictive distribution
converges to the true distribution (i.e., J = Op(1/N)).
Let 
H = H
h=1 θ( 1
M
M
m=1 Θm,h ∼Op(1)) be the number of topics used in the
whole corpus, 
M(h) = M
m=1 θ(Θm,h ∼Op(1)) be the number of documents that
contain the hth topic, and L(h) = L
l=1 θ(Bl,h ∼Op(1)) be the number of words
of which the hth topic consist. We have the following lemma:
Lemma 16.9
R is written as follows:
R =
⎧⎪⎪⎪⎨⎪⎪⎪⎩M

Hα −1
2

+ 
H

Lη −1
2

−

H

h=1


M(h)

α −1
2

+ L(h)

η −1
2
⎫⎪⎪⎪⎬⎪⎪⎪⎭log N
+ (H −
H)

Lη −1
2

log L + Op(H(M + L)).
(16.48)
Proof
By using the bounds (15.8) and (15.9), R can be bounded as
R ≤R ≤R,
(16.49)
where
R = −
M

m=1
log
Γ(Hα)
Γ(α)H

−
H

h=1
log
Γ(Lη)
Γ(η)
L
−M(H −1) + H(L −1)
2
log(2π)
+
M

m=1
⎧⎪⎪⎨⎪⎪⎩

Hα −1
2

log
H

h=1
αm,h −
H

h=1

α −1
2

logαm,h
⎫⎪⎪⎬⎪⎪⎭
+
H

h=1
⎧⎪⎪⎨⎪⎪⎩

Lη −1
2

log
L

l=1
ηl,h −
L

l=1

η −1
2

logηl,h
⎫⎪⎪⎬⎪⎪⎭
+
M

m=1
⎧⎪⎪⎨⎪⎪⎩−
H

h=1
1
12αm,h
−
H

h=1
$αm,h −α%  1
αm,h
−
1
2 H
h′=1 αm,h′
⎫⎪⎪⎬⎪⎪⎭
+
H

h=1
⎧⎪⎪⎨⎪⎪⎩−
L

l=1
1
12ηl,h
−
L

l=1
$ηl,h −η%  1
ηl,h
−
1
2 L
l′=1ηl′,h
⎫⎪⎪⎬⎪⎪⎭,
(16.50)

16.4 Latent Dirichlet Allocation
475
R = −
M

m=1
log
Γ(Hα)
Γ(α)H

−
H

h=1
log
Γ(Lη)
Γ(η)
L
−M(H −1) + H(L −1)
2
log(2π)
+
M

m=1
⎧⎪⎪⎨⎪⎪⎩

Hα −1
2

log
H

h=1
αm,h −
H

h=1

α −1
2

logαm,h
⎫⎪⎪⎬⎪⎪⎭
+
H

h=1
⎧⎪⎪⎨⎪⎪⎩

Lη −1
2

log
L

l=1
ηl,h −
L

l=1

η −1
2

logηl,h
⎫⎪⎪⎬⎪⎪⎭
+
M

m=1
⎧⎪⎪⎨⎪⎪⎩
1
12 H
h=1 αm,h
−
H

h=1
$αm,h −α% 
1
2αm,h
−
1
H
h′=1 αm,h′
⎫⎪⎪⎬⎪⎪⎭
+
H

h=1
⎧⎪⎪⎨⎪⎪⎩
1
12 L
l=1ηl,h
−
L

l=1
$ηl,h −η%  1
2ηl,h
−
1
L
l′=1ηl′,h
⎫⎪⎪⎬⎪⎪⎭.
(16.51)
Eqs. (16.43) and (16.44) imply that
R =
M

m=1
⎧⎪⎪⎨⎪⎪⎩

Hα −1
2

log
H

h=1
αm,h −
H

h=1

α −1
2

logαm,h
⎫⎪⎪⎬⎪⎪⎭
+
H

h=1
⎧⎪⎪⎨⎪⎪⎩

Lη −1
2

log
L

l=1
ηl,h −
L

l=1

η −1
2

logηl,h
⎫⎪⎪⎬⎪⎪⎭+ Op(H(M + L)),
which leads to Eq. (16.48). This completes the proof of Lemma 16.9.
□
Since we assumed that the true matrices Θ∗and B∗are of the rank of H∗,

H = H∗∼O(1) is sufﬁcient for the VB posterior to converge to the true distri-
bution. However, 
H can be much larger than H∗with

BΘ⊤
rΘ,B(Θ,B) unchanged
because of the nonidentiﬁability of matrix factorization—duplicating topics
with divided weights, for example, does not change the distribution.
Let
FVB(D) = FVB(D) −NS N(D)
(16.52)
be the relative free energy. Based on Lemmas 16.8 and 16.9, we obtain the
following theorem:
Theorem 16.10
In the limit when N →∞with L, M ∼O(1), it holds that
J = Op(1/N), and
FVB(D) = λ′VB
LDA log N + Op(1),
where
λ′VB
LDA =
⎧⎪⎪⎪⎨⎪⎪⎪⎩M

Hα −1
2

+ 
H

Lη −1
2

−

H

h=1


M(h)

α −1
2

+ L(h)

η −1
2
⎫⎪⎪⎪⎬⎪⎪⎪⎭.

476
16 Asymptotic VB Theory of Other Latent Variable Models
In the limit when N, M →∞with M
N , L ∼O(1), it holds that J = op(log N),
and
FVB(D) = λ′VB
LDA log N + op(N log N),
where
λ′VB
LDA =
⎧⎪⎪⎪⎨⎪⎪⎪⎩M

Hα −1
2

−

H

h=1

M(h)

α −1
2
⎫⎪⎪⎪⎬⎪⎪⎪⎭.
In the limit when N, L →∞with L
N , M ∼O(1), it holds that J = op(log N),
and
FVB(D) = λ′VB
LDA log N + op(N log N),
where
λ′VB
LDA = HLη.
In the limit when N, L, M →∞with
L
N , M
N
∼O(1), it holds that J =
op(N log N), and
FVB(D) = λ′VB
LDA log N + op(N2 log N),
where
λ′VB
LDA = H(Mα + Lη).
Proof
Lemmas 16.8 and 16.9 imply that the relative free energy can be
written as follows:
FVB(D)
=
⎧⎪⎪⎪⎨⎪⎪⎪⎩M

Hα −1
2

+ 
H

Lη −1
2

−

H

h=1


M(h)

α −1
2

+ L(h)

η −1
2
⎫⎪⎪⎪⎬⎪⎪⎪⎭log N
+ (H −
H)

Lη −1
2

log L + Op(JN + LM).
(16.53)
In the following subsection, we investigate the leading term of the relative free
energy (16.53) in different asymptotic limits.
In the Limit When N →∞with L, M ∼O(1)
In this case, the minimizer should satisfy
J = Op
 1
N

(16.54)

16.4 Latent Dirichlet Allocation
477
and the leading term of the relative free energy (16.52) is of the order of
Op(log N) as follows:
FVB(D)
=
⎧⎪⎪⎪⎨⎪⎪⎪⎩M

Hα −1
2

+ 
H

Lη −1
2

−

H

h=1


M(h)

α −1
2

+ L(h)

η −1
2
⎫⎪⎪⎪⎬⎪⎪⎪⎭log N
+ Op(1).
Note that Eq. (16.54) implies the consistency of the VB posterior.
In the Limit When N, M →∞with M
N , L ∼O(1)
In this case,
J = op(log N),
(16.55)
making the leading term of the relative free energy of the order of Op(N log N)
as follows:
FVB(D) =
⎧⎪⎪⎪⎨⎪⎪⎪⎩M

Hα −1
2

−

H

h=1

M(h)

α −1
2
⎫⎪⎪⎪⎬⎪⎪⎪⎭log N + op(N log N).
Eq. (16.55) implies that the VB posterior is not necessarily consistent.
In the Limit When N, L →∞with L
N, M ∼O(1)
In this case, Eq. (16.55) holds, and the leading term of the relative free energy
is of the order of Op(N log N) as follows:
FVB(D) = HLη log N + op(N log N).
In the Limit When N, L, M →∞with L
N, M
N ∼O(1)
In this case,
J = op(N log N),
(16.56)
and the leading term of the relative free energy is of the order of Op(N2 log N)
as follows:
FVB(D) = H (Mα + Lη) log N + op(N2 log N).
This completes the proof of Theorem 16.10.
□
Since Eq. (16.41) was shown to hold, the predictive distribution converges
to the true distribution if J = Op(1/N). Accordingly, Theorem 16.10 states that
the consistency holds in the limit when N →∞with L, M ∼O(1).

478
16 Asymptotic VB Theory of Other Latent Variable Models
Theorem 16.10 also implies that, in the asymptotic limits with small
L ∼O(1), the leading term depends on 
H, meaning that it dominates the topic
sparsity of the VB solution. We have the following corollary:
Corollary 16.11
Let M∗(h) = M
m=1 θ(Θ∗
m,h ∼O(1)) and L∗(h) = L
l=1 θ(B∗
l,h ∼
O(1)). Consider the limit when N →∞with L, M ∼O(1). When 0 < η ≤
1
2L,
the VB solution is sparse (i.e., 
H ≪H = min(L, M)) if α < 1
2 −
1
2 −Lη
minh M∗(h) , and
dense (i.e., 
H ≈H) if α > 1
2 −
1
2 −Lη
minh M∗(h) . When
1
2L < η ≤1
2, the VB solution is
sparse if α < 1
2 +
Lη−1
2
maxh M∗(h) , and dense if α > 1
2 +
Lη−1
2
maxh M∗(h) . When η > 1
2, the VB
solution is sparse if α < 1
2 +
L−1
2 maxh M∗(h) , and dense if α > 1
2 +
Lη−1
2
minh M∗(h) . In the
limit when N, M →∞with M
N , L ∼O(1), the VB solution is sparse if α < 1
2,
and dense if α > 1
2.
Proof
From the compact representation when 
H = H∗, 
M(h) = M∗(h), and
L(h) = L∗(h), we can decompose a singular component into two, keeping BΘ
⊤
unchanged, so that

H →
H + 1,
(16.57)
H

h=1

M(h) →
H∗

h=1

M(h) + ΔM
for
min
h
M∗(h) ≤ΔM ≤max
h
M∗(h),
(16.58)
H

h=1
L(h) →
H∗

h=1
L(h) + ΔL
for
0 ≤ΔL ≤max
h
L∗(h).
(16.59)
Here the lower-bound for ΔM in Eq. (16.58) corresponds to the case that
the least frequent topic is chosen to be decomposed, while the upper-bound
to the case that the most frequent topic is chosen. The lower-bound for
ΔL in Eq. (16.59) corresponds to the decomposition such that some of the
word-occurrences are moved to a new topic, while the upper-bound to the
decomposition such that the topic with the widest vocabulary is copied to a
new topic. Note that the bounds both for ΔM and ΔL are not always achievable
simultaneously, when we choose one topic to decompose.
In the following subsection, we investigate the relation between the sparsity
of the solution and the hyperparameter setting in different asymptotic limits.
In the Limit When N →∞with L, M ∼O(1)
The coefﬁcient of the leading term of the free energy is
λ′VB
LDA = M

Hα −1
2

+

H

h=1

Lη −1
2 −
M(h)

α −1
2

−L(h)

η −1
2

. (16.60)

16.4 Latent Dirichlet Allocation
479
Note that the solution is sparse if Eq. (16.60) is increasing with respect to 
H,
and dense if it is decreasing. Eqs. (16.57) through (16.59) imply the following:
(I) When 0 < η ≤
1
2L and α ≤1
2, the solution is sparse if
Lη −1
2 −min
h
M∗(h)

α −1
2

> 0, or equivalently,
α < 1
2 −
1
minh M∗(h)
1
2 −Lη

,
and dense if
α > 1
2 −
1
minh M∗(h)
1
2 −Lη

.
(II) When 0 < η ≤
1
2L and α > 1
2, the solution is sparse if
Lη −1
2 −max
h
M∗(h)

α −1
2

> 0, or equivalently,
α < 1
2 −
1
maxh M∗(h)
1
2 −Lη

,
and dense if
α > 1
2 −
1
maxh M∗(h)
1
2 −Lη

.
Therefore, the solution is always dense in this case.
(III) When
1
2L < η ≤1
2 and α < 1
2, the solution is sparse if
Lη −1
2 −min
h
M∗(h)

α −1
2

> 0, or equivalently,
α < 1
2 +
1
minh M∗(h)

Lη −1
2

,
and dense if
α > 1
2 +
1
minh M∗(h)

Lη −1
2

.
Therefore, the solution is always sparse in this case.
(IV) When
1
2L < η ≤1
2 and α ≥1
2, the solution is sparse if
Lη −1
2 −max
h
M∗(h)

α −1
2

> 0, or equivalently,
α < 1
2 +
1
maxh M∗(h)

Lη −1
2

,

480
16 Asymptotic VB Theory of Other Latent Variable Models
and dense if
α > 1
2 +
1
maxh M∗(h)

Lη −1
2

.
(V) When η > 1
2 and α < 1
2, the solution is sparse if
Lη −1
2 −max
h

M∗(h)

α −1
2

+ L∗(h)

η −1
2

> 0,
(16.61)
and dense if
Lη −1
2 −max
h

M∗(h)

α −1
2

+ L∗(h)

η −1
2

< 0.
(16.62)
Therefore, the solution is sparse if
Lη −1
2 −min
h
M∗(h)

α −1
2

−max
h
L∗(h)

η −1
2

> 0, or equivalently,
α < 1
2 +
1
minh M∗(h)

Lη −1
2 −max
h
L∗(h)

η −1
2

,
and dense if
Lη −1
2 −max
h
M∗(h)

α −1
2

−max
h
L∗(h)

η −1
2

< 0, or equivalently,
α > 1
2 +
1
maxh M∗(h)

Lη −1
2 −max
h
L∗(h)

η −1
2

.
Therefore, the solution is always sparse in this case.
(VI) When η > 1
2 and α ≥1
2, the solution is sparse if Eq. (16.61) holds, and
dense if Eq. (16.62) holds. Therefore, the solution is sparse if
Lη −1
2 −max
h
M∗(h)

α −1
2

−max
h
L∗(h)

η −1
2

> 0, or equivalently,
α < 1
2 +
1
maxh M∗(h)

Lη −1
2 −max
h
L∗(h)

η −1
2

,
and dense if
Lη −1
2 −min
h
M∗(h)

α −1
2

−max
h
L∗(h)

η −1
2

< 0, or equivalently,
α > 1
2 +
1
minh M∗(h)

Lη −1
2 −max
h
L∗(h)

η −1
2

.
Thus, we can conclude that, in this case, the solution is sparse if
α < 1
2 +
L −1
2 maxh M∗(h) ,

16.4 Latent Dirichlet Allocation
481
and dense if
α > 1
2 +
Lη −1
2
minh M∗(h) .
Summarizing the preceding, we have the following lemma:
Lemma 16.12
When 0 < η ≤
1
2L, the solution is sparse if α < 1
2 −
1
2 −Lη
minh M∗(h) ,
and dense if α >
1
2 −
1
2 −Lη
minh M∗(h) . When
1
2L < η ≤
1
2, the solution is sparse if
α < 1
2 +
Lη−1
2
maxh M∗(h) , and dense if α > 1
2 +
Lη−1
2
maxh M∗(h) . When η > 1
2, the solution is
sparse if α < 1
2 +
L−1
2 maxh M∗(h) , and dense if α > 1
2 +
Lη−1
2
minh M∗(h) .
In the Limit When N, M →∞with M
N , L ∼O(1)
The coefﬁcient of the leading term of the free energy is given by
λ′VB
LDA = M

Hα −1
2

−

H

h=1

M(h)

α −1
2

.
(16.63)
Although the predictive distribution does not necessarily converge to the true
distribution, we can investigate the sparsity of the solution by considering the
duplication rules (16.57) through (16.59) that keep BΘ
⊤unchanged. It is clear
that Eq. (16.63) is increasing with respect to 
H if α <
1
2, and decreasing
if α >
1
2. Combing this result with Lemma 16.12 completes the proof of
Corollary 16.11.
□
In the case when L, M ≪N and in the case when L ≪M, N, Corollary
16.11 provides information on the sparsity of the VB solution, which will be
compared with other methods in Section 16.4.2. On the other hand, although
we have successfully derived the leading term of the free energy also in the
case when M ≪L, N and in the case when 1 ≪L, M, N, it unfortunately
provides no information on sparsity of the solution.
16.4.2 Asymptotic Analysis of MAP Learning and
Partially Bayesian Learning
For training the LDA model, MAP learning and partially Bayesian (PB)
learning (see Section 2.2.2), where Θ and/or B are point-estimated, are also
popular choices. Although the differences in update equations is small, it
can affect the asymptotic behavior. In this subsection, we aim to clarify the
difference in the asymptotic behavior.

482
16 Asymptotic VB Theory of Other Latent Variable Models
MAP learning, PB-A learning, PB-B learning, and VB learning, respec-
tively, solve the following problem:
min
r
F,
s.t.
⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
rΘ,B(Θ, B) = δ(Θ; Θ)δ(B; B)
(for MAP learning),
rΘ,B(Θ, B) = rΘ(Θ)δ(B; B)
(for PB-A learning),
rΘ,B(Θ, B) = δ(Θ; Θ)rB(B)
(for PB-B learning),
rΘ,B(Θ, B) = rΘ(Θ)rB(B)
(for VB learning),
Similar analysis to Section 16.4.1 leads to the following theorem (the proof
is given in Section 16.4.5):
Theorem 16.13
In the limit when N →∞with L, M ∼O(1), the solution is
sparse if α < αsparse, and dense if α > αdense. In the limit when N, M →∞with
M
N , L ∼O(1), the solution is sparse if α < αM→∞, and dense if α > αM→∞.
Here, αsparse, αdense, and αM→∞are given in Table 16.1.
A notable ﬁnding from Table 16.1 is that the threshold that determines the
topic sparsity of PB-B learning is (most of the case exactly) 1
2 larger than the
threshold of VB learning. The same relation is observed between MAP learn-
ing and PB-A learning. From these, we can conclude that point-estimating Θ,
instead of integrating it out, increases the threshold by 1
2 in the LDA model.
We will validate this observation by numerical experiments in Section 16.4.4.
Table 16.1 Sparsity thresholds of VB, PB-A, PB-B, and MAP methods (see
Theorem 16.13). The ﬁrst four columns show the thresholds (αsparse, αdense), of
which the function forms depend on the range of η, in the limit when N →∞
with L, M ∼O(1). A single value is shown if αsparse = αdense. The last column
shows the threshold αM→∞in the limit when N, M →∞with M
N , L ∼O(1).

αsparse, αdense
 
αM→∞
η range
0 < η ≤
1
2L
1
2L < η ≤1
2
1
2 < η < 1
1 ≤η < ∞
0 < η < ∞
VB
1
2 −
1
2 −Lη
minh M∗(h)
1
2 +
Lη−1
2
maxh M∗(h)
!
1
2 +
L−1
2 maxh M∗(h) , 1
2 +
Lη−1
2
minh M∗(h)
"
1
2
PB-A
—
!
1
2, 1
2 +
L(η−1)
minh M∗(h)
"
1
2
PB-B
1
1 +
Lη−1
2
maxh M∗(h)
!
1 +
L−1
2 maxh M∗(h) , 1 +
Lη−1
2
minh M∗(h)
"
1
MAP
—
!
1, 1 +
L(η−1)
minh M∗(h)
"
1

16.4 Latent Dirichlet Allocation
483
16.4.3 Discussion
The preceding theoretical analysis (Thereom 16.13) showed that VB tends to
induce weaker sparsity than MAP in the LDA model,3 i.e., VB requires sparser
prior (smaller α) than MAP to give a sparse solution (mean of the posterior).
This phenomenon is opposite to other models such as mixture models (Chapter
15), Bayesian networks (Section 16.1), hidden Markov models (Section 16.2),
and fully observed matrix factorization (Chapter 7), where VB tends to induce
stronger sparsity than MAP. This phenomenon might be partly explained as
follows: in the case of mixture models, the sparsity threshold depends on the
degree of freedom of a single component (Theorem 15.5). This is reasonable
because adding a single component increases the model complexity by this
amount. Also, in the case of LDA, adding a single topic requires additional
L+1 parameters. However, the added topic is shared over M documents, which
could discount the increased model complexity relative to the increased data
ﬁdelity. Corollary 16.11, which implies the dependency of the threshold for α
on L and M, might support this conjecture. However, the same applies to the
matrix factorization, where VB was shown to give a sparser solution than MAP
(Chapter 7). Investigation on related models, e.g., Poisson MF (Gopalan et al.,
2013), would help us fully explain this phenomenon.
Unlike for the latent variable models in the previous sections, we derived a
general form of the asymptotic free energy for LDA, which can be applied
to different asymptotic limits and showed that the consistency does not
always hold (see Theorem 16.10). Speciﬁcally, the standard asymptotic theory
requires a large number N of words per document, compared to the number
M of documents and the vocabulary size L. Assuming such a situation
may be reasonable in some collaborative ﬁltering applications, e.g., in the
Last.FM data which will be used for numerical illustration in Section 16.4.4.
However, L and/or M are comparable to or larger than N in many text analysis
applications.
The general form of the asymptotic free energy also allowed us to elucidate
the behavior of the VB free energy when L and/or M diverges with the same
order as N. This attempt successfully revealed the sparsity of the solution
for the case when M diverges while L ∼O(1). However, when L diverges,
we found that the leading term of the free energy does not contain useful
information on sparsity of the solution. Higher-order asymptotic analysis will
be necessary to further understand the sparsity-inducing mechanism of the
LDA model with large vocabulary.
3 This tendency was pointed out (Asuncion et al., 2009) by using the approximation exp(Ψ(n)) ≈
n −1
2 and comparing the stationary condition. The theory here clariﬁed the sparsity behavior of
the global solution based on the asymptotic free energy analysis.

484
16 Asymptotic VB Theory of Other Latent Variable Models
16.4.4 Numerical Illustration
Here we conduct numerical experiments on artiﬁcial and real data for collabo-
rative ﬁltering.
The artiﬁcial data were created as follows: we ﬁrst sample the true
document matrix Θ∗of size M×H∗and the true topic matrix B∗of size L×H∗.
We assume that each row θ
∗
m of Θ∗follows the Dirichlet distribution with
α∗= 1/H∗, while each column β∗
h of B∗follows the Dirichlet distribution with
η∗= 1/L. The document length N(m) is sampled from the Poisson distribution
with mean N. The word histogram N(m)vm for each document is sampled from
the multinomial distribution with the parameter speciﬁed by the mth row vector
of B∗Θ∗⊤. Thus, we obtain the L × M matrix V, which corresponds to the
empirical word distribution over M documents.
As a real-world data set, we used the Last.FM data set.4 Last.FM is
a well-known social music web site, and the data set includes the triple
(“user,” “artist,” “Freq”), which was collected from the playlists of users in
0
0.5
1
0
0.5
1
0
20
40
60
80
100
(a) VB
0
0.5
1
0
0.5
1
0
20
40
60
80
100
(b) PB-A
0
0.5
1
0
0.5
1
0
20
40
60
80
100
(c) PB-B
0
0.5
1
0
0.5
1
0
20
40
60
80
100
(d) MAP
Figure 16.2 Estimated number 
H of topics by (a) VB learning, (b) PB-A learning,
(c) PB-B learning, and (d) MAP learning, on the artiﬁcial data with L = 100, M =
100, H∗= 20, and N ∼10000.
4 http://mtg.upf.edu/node/1671

16.4 Latent Dirichlet Allocation
485
0
0.5
1
0
0.5
1
0
20
40
60
80
100
(a) VB
0
0.5
1
0
0.5
1
0
20
40
60
80
100
(b) PB-A
0
0.5
1
0
0.5
1
0
20
40
60
80
100
(c) PB-B
0
0.5
1
0
0.5
1
0
20
40
60
80
100
(d) MAP
Figure 16.3 Estimated number 
H of topics on the Last.FM data with L =
100, M = 100, and N ∼700.
the community by using a plug-in in users’ media players. This triple means
that “user” played “artist” music “Freq” times, which indicates users’ preferred
artists. A user and a played artist are analogous to a document and a word,
respectively. We randomly chose L artists from the top 1, 000 frequent artists,
and M users who live in the United States. To ﬁnd a better local solution (which
hopefully is close to the global solution), we adopted a split and merge strategy
(Ueda et al., 2000), and chose the local solution giving the lowest free energy
among different initialization schemes.
Figure 16.2 shows the estimated number

H of topics by different
approximate Bayesian methods, i.e., VB, PB-A, PB-B, and MAP learning,
on the artiﬁcial data with L = 100, M = 100, H∗= 20, and N ∼10000. We
can clearly see that the sparsity threshold in PB-B and MAP learning, where
Θ is point-estimated, is larger than that in VB and PB-A learning, where Θ
is marginalized. This result supports the statement by Theorem 16.13. Figure
16.3 shows results on the Last.FM data with L = 100, M = 100, and N ∼700.
We see a similar tendency to Figure 16.2 except the region where η < 1 for
PB-A learning, in which our theory does not predict the estimated number
of topics.

486
16 Asymptotic VB Theory of Other Latent Variable Models
0
0.5
1
0
0.5
1
0
20
40
60
80
100
(a) L = 100, M = 100
0
0.5
1
0
0.5
1
0
20
40
60
80
100
(b) L = 100, M = 1000
0
0.5
1
0
0.5
1
0
20
40
60
80
100
(c) L = 500, M = 100
0
0.5
1
0
0.5
1
0
20
40
60
80
100
(d) L = 500, M = 1000
Figure 16.4 Estimated number 
H of topics by VB learning on the artiﬁcial data
with H∗= 20 and N ∼10000. For the case when L = 500, M = 1000, the
maximum estimated rank is limited to 100 for computational reason.
Finally, we investigate how different asymptotic settings affect the topic
sparsity. Figure 16.4 shows the sparsity dependence on L and M on the
artiﬁcial data. The graphs correspond to the four cases mentioned in Theorem
16.10, i.e, (a) L, M ≪N, (b) L ≪N, M, (c) M ≪N, L, and (d) 1 ≪N, L, M.
Corollary 16.11 explains the behavior in (a) and (b), and further analysis is
required to explain the behavior in (c) and (d).
16.4.5 Proof of Theorem 16.13
We analyze PB-A learning, PB-B learning, and MAP learning, and then
summarize the results, which proves Theorem 16.13.
PB-A Learning
The free energy for PB-A learning is given as follows:
FPB−A = χB + RPB−A + QPB−A,
(16.64)

16.4 Latent Dirichlet Allocation
487
where χB is a large constant corresponding to the negative entropy of the delta
functions (see Section 2.2.2), and
RPB−A =
/
log rΘ(Θ)rB(B)
p(Θ|α)p(B|η)
0
rPB−A(Θ,B)
=
M

m=1
⎛⎜⎜⎜⎜⎜⎝log
Γ(H
h=1 αPB−A
m,h
)
H
h=1 Γ(αPB−A
m,h
)
Γ(α)H
Γ(Hα)
+
H

h=1

αPB−A
m,h
−α
 ⎛⎜⎜⎜⎜⎜⎝Ψ(αPB−A
m,h
) −Ψ(
H

h′=1
αPB−A
m,h′ )
⎞⎟⎟⎟⎟⎟⎠
⎞⎟⎟⎟⎟⎟⎠
+
H

h=1
⎛⎜⎜⎜⎜⎜⎝log Γ(η)L
Γ(Lη) +
L

l=1
(1 −η)

log(ηPB−A
l,h
) −log(L
l′=1ηPB−A
l′,h
)
 ⎞⎟⎟⎟⎟⎟⎠,
(16.65)
QPB−A =
/
log
rz

{{z(n,m)}N(m)
n=1 }M
m=1
 
p({w(n,m)}, {z(n,m)}|Θ, B)
0
rPB−A(Θ,B,{z(n,m)})
= −
M

m=1
N(m)
L

l=1
Vl,m log
⎛⎜⎜⎜⎜⎜⎜⎝
H

h=1
exp

Ψ(αPB−A
m,h
)
 
exp

Ψ(H
h′=1 αPB−A
m,h′ )
 
ηPB−A
l,h
L
l′=1ηPB−A
l′,h
⎞⎟⎟⎟⎟⎟⎟⎠.
(16.66)
Let us ﬁrst consider the case when η < 1. In this case, F diverges to F →
−∞with ﬁxed N, when ηl,h = O(1) for any (l, h) and ηl′,h →+0 for all other
l′  l. Therefore, the solution is useless.
When η ≥1, the solution satisﬁes the following stationary condition:
αPB−A
m,h
= α +
N(m)

n=1
zPB−A(n,m)
h
,
ηPB−A
l,h
= η −1 +
M

m=1
N(m)

n=1
w(n,m)
l
zPB−A(n,m)
h
,
(16.67)
zPB−A(n,m)
h
=
exp

Ψ(αPB−A
m,h
)
 L
l=1(ηPB−A
l,h
)w(n,m)
l
H
h′=1

exp

Ψ(αPB−A
m,h′ )
 L
l=1(ηPB−A
l,h′
)w(n,m)
l
 .
(16.68)
In the same way as for VB learning, we can obtain the following lemma:
Lemma 16.14
Let B
PB−AΘ
PB−A⊤= ⟨BΘ⊤⟩rPB−A(Θ,B). Then it holds that
⟨(BΘ⊤−B
PB−AΘ
PB−A⊤)2
l,m⟩rPB−A(Θ,B) = Op(N−2),
(16.69)
QPB−A = −
M

m=1
N(m)
L

l=1
Vl,m log(B
PB−AΘ
PB−A⊤)l,m + Op(N−1).
(16.70)

488
16 Asymptotic VB Theory of Other Latent Variable Models
QPB−A is minimized when J = Op(N−1), and it holds that
QPB−A = NS N(D) + Op(JN + LM).
RPB−A is written as follows:
RPB−A =
⎧⎪⎪⎪⎨⎪⎪⎪⎩M

Hα−1
2

+ 
HL (η −1)−

H

h=1


M(h)

α−1
2

+ L(h) (η −1)
⎫⎪⎪⎪⎬⎪⎪⎪⎭log N
+ (H −
H)L (η −1) log L + Op(H(M + L)).
(16.71)
Taking the different asymptotic limits, we obtain the following theorem:
Theorem 16.15
When η < 1, each column vector of B
PB−A has only one
nonzero entry. Assume in the following that η ≥1. In the limit when N →∞
with L, M ∼O(1), it holds that J = Op(1/N) and
FPB−A(D) = λ′PB−A
LDA
log N + Op(1),
where
λ′PB−A
LDA
= M

Hα −1
2

+ 
HL (η −1) −

H

h=1


M(h)

α −1
2

+ L(h) (η −1)

.
In the limit when N, M →∞with M
N , L ∼O(1), it holds that J = op(log N),
and
FPB−A(D) = λ′PB−A
LDA
log N + op(N log N),
where
λ′PB−A
LDA
= M

Hα −1
2

−

H

h=1

M(h)

α −1
2

.
In the limit when N, L →∞with L
N , M ∼O(1), it holds that J = op(log N),
and
FPB−A(D) = λ′PB−A
LDA
log N + op(N log N),
where
λ′PB−A
LDA
= HL(η −1).
In the limit when N, L, M →∞with
L
N , M
N
∼O(1), it holds that J =
op(N log N), and
FPB−A(D) = λ′PB−A
LDA
log N + op(N2 log N),

16.4 Latent Dirichlet Allocation
489
where
λ′PB−A
LDA
= H(Mα + L(η −1)).
Note that Theorem 16.15 provides no information on the sparsity of the
PB-A solution for η < 1. In the following subsection, we investigate the
sparsity of the solution for η ≥1.
In the Limit When N →∞with L, M ∼O(1)
The coefﬁcient of the leading term of the free energy is
λ′PB−A
LDA
= M

Hα −1
2

+

H

h=1

L(η −1) −
M(h)

α −1
2

−L(h) (η −1)

.
The solution is sparse if λ′PB−A
LDA
is increasing with respect to 
H, and dense if it
is decreasing. We focus on the case where η ≥1. Eqs. (16.57) through (16.59)
imply the following:
(I) When α < 1
2, the solution is sparse if
L(η −1) −max
h

M∗(h)

α −1
2

+ L∗(h) (η −1)

> 0,
(16.72)
and dense if
L(η −1) −max
h

M∗(h)

α −1
2

+ L∗(h) (η −1)

< 0.
(16.73)
Therefore, the solution is sparse if
L(η −1) −min
h
M∗(h)

α −1
2

−max
h
L∗(h) (η −1) > 0, or equivalently,
α < 1
2 +

L −maxh L∗(h) 
(η −1)
minh M∗(h)
,
and dense if
L(η −1) −max
h
M∗(h)

α −1
2

−max
h
L∗(h) (η −1) < 0, or equivalently,
α > 1
2 +

L −maxh L∗(h) 
(η −1)
maxh M∗(h)
.
Therefore, the solution is always sparse in this case.

490
16 Asymptotic VB Theory of Other Latent Variable Models
(II) When α ≥1
2, the solution is sparse if Eq. (16.72) holds, and dense if Eq.
(16.73) holds. Therefore, the solution is sparse if
L(η −1) −max
h
M∗(h)

α −1
2

−max
h
L∗(h) (η −1) > 0, or equivalently,
α < 1
2 +

L −maxh L∗(h) 
(η −1)
maxh M∗(h)
,
and dense if
L(η −1) −min
h
M∗(h)

α −1
2

−max
h
L∗(h) (η −1) < 0, or equivalently,
α > 1
2 +

L −maxh L∗(h) 
(η −1)
minh M∗(h)
.
Thus, we can conclude that, in this case, the solution is sparse if
α < 1
2,
and dense if
α > 1
2 +
L(η −1)
minh M∗(h) .
Summarizing the preceding, we have the following lemma:
Lemma 16.16
Assume that η ≥1. The solution is sparse if α < 1
2, and dense
if α > 1
2 +
L(η−1)
minh M∗(h) .
In the Limit When N, M →∞with M
N , L ∼O(1)
The coefﬁcient of the leading term of the free energy is given by
λ′PB−A
LDA
= M

Hα −1
2

−

H

h=1

M(h)

α −1
2

.
(16.74)
Although the predictive distribution does not necessarily converge to the true
distribution, we can investigate the sparsity of the solution by considering the
duplication rules (16.57) through (16.59) that keep BΘ
⊤unchanged. It is clear
that Eq. (16.74) is increasing with respect to 
H if α <
1
2, and decreasing
if α >
1
2. Combing this result with Lemma 16.16, we obtain the following
corollary:
Corollary 16.17
Assume that η ≥1. In the limit when N →∞with L,
M ∼O(1), the PB-A solution is sparse if α < 1
2, and dense if α > 1
2 +
L(η−1)
minh M∗(h) .

16.4 Latent Dirichlet Allocation
491
In the limit when N, M →∞with M
N , L ∼O(1), the PB-A solution is sparse if
α < 1
2, and dense if α > 1
2.
PB-B Learning
The free energy for PB-B learning is given as follows:
FPB−B = χΘ + RPB−B + QPB−B,
(16.75)
where χΘ is a large constant corresponding to the negative entropy of the delta
functions, and
RPB−B =
/
log rΘ(Θ)rB(B)
p(Θ|α)p(B|η)
0
rPB−B(Θ,B)
=
M

m=1
⎛⎜⎜⎜⎜⎜⎝log Γ(α)H
Γ(Hα) +
H

h=1
(1 −α)
⎛⎜⎜⎜⎜⎜⎝log(αPB−B
m,h ) −log(
H

h′=1
αPB−B
m,h′ )
⎞⎟⎟⎟⎟⎟⎠
⎞⎟⎟⎟⎟⎟⎠
+
H

h=1
⎛⎜⎜⎜⎜⎜⎝log
Γ(L
l=1ηPB−B
l,h
)
L
l=1 Γ(ηPB−B
l,h
)
Γ(η)L
Γ(Lη)
+
L

l=1

ηPB−B
l,h
−η
 
Ψ(ηPB−B
l,h
) −Ψ(L
l′=1ηPB−B
l′,h
)
 ⎞⎟⎟⎟⎟⎟⎠,
(16.76)
QPB−B =
/
log
rz

{{z(n,m)}N(m)
n=1 }M
m=1
 
p({w(n,m)}, {z(n,m)}|Θ, B)
0
rPB−B(Θ,B,{z(n,m)})
= −
M

m=1
N(m)
L

l=1
Vl,m log
⎛⎜⎜⎜⎜⎜⎜⎝
H

h=1
αPB−B
m,h
H
h′=1 αPB−B
m,h′
exp

Ψ(ηPB−B
l,h
)
 
exp

Ψ(L
l′=1ηPB−B
l′,h
)
 
⎞⎟⎟⎟⎟⎟⎟⎠.
(16.77)
Let us ﬁrst consider the case when α < 1. In this case, F diverges to
F →−∞with ﬁxed N, when αm,h = O(1) for any (m, h) and αm,h′ →+0 for
all other h′  h. Therefore, the solution is sparse (so sparse that the estimator
is useless).
When α ≥1, the solution satisﬁes the following stationary condition:
αPB−B
m,h
= α −1 +
N(m)

n=1
zPB−B(n,m)
h
,
ηPB−B
l,h
= η +
M

m=1
N(m)

n=1
w(n,m)
l
zPB−B(n,m)
h
,
(16.78)
zPB−B(n,m)
h
=
αPB−B
m,h
exp
,L
l=1 w(n,m)
l

Ψ(ηPB−B
l,h
) −Ψ
L
l′=1ηPB−B
l′,h
  -
H
h′=1 αPB−B
m,h′ exp
,L
l=1 w(n,m)
l

Ψ(ηPB−B
l,h′
) −Ψ
L
l′=1ηPB−B
l′,h′
  -.
(16.79)

492
16 Asymptotic VB Theory of Other Latent Variable Models
In the same way as for VB and PB-A learning, we can obtain the following
lemma:
Lemma 16.18
Let B
PB−BΘ
PB−B⊤= ⟨BΘ⊤⟩rPB−B(Θ,B). Then it holds that
⟨(BΘ⊤−B
PB−BΘ
PB−B⊤)2
l,m⟩rPB−B(Θ,B) = Op(N−2),
(16.80)
QPB−B = −
M

m=1
N(m)
L

l=1
Vl,m log(B
PB−BΘ
PB−B⊤)l,m + Op(N−1).
(16.81)
QPB−B is minimized when J = Op(N−1), and it holds that
QPB−B = NS N(D) + Op(JN + LM).
RPB−B is written as follows:
RPB−B =
⎧⎪⎪⎪⎨⎪⎪⎪⎩MH (α −1)+ 
H

Lη −1
2

−

H

h=1


M(h) (α −1) +L(h)

η −1
2
⎫⎪⎪⎪⎬⎪⎪⎪⎭log N
+ (H −
H)

Lη −1
2

log L + Op(H(M + L)).
(16.82)
Taking the different asymptotic limits, we obtain the following theorem:
Theorem 16.19
When α < 1, each row vector of Θ
PB−B has only one nonzero
entry, and the PB-B solution is sparse. Assume in the following that α ≥1. In
the limit when N →∞with L, M ∼O(1), it holds that J = Op(1/N) and
FPB−B(D) = λ′PB−B
LDA
log N + Op(1),
where
λ′PB−B
LDA
= MH (α −1) + 
H

Lη −1
2

−

H

h=1


M(h) (α −1) + L(h)

η −1
2

.
In the limit when N, M →∞with M
N , L ∼O(1), it holds that J = op(log N),
and
FPB−B(D) = λ′PB−B
LDA
log N + op(N log N),
where
λ′PB−B
LDA
= MH (α −1) −

H

h=1

M(h) (α −1) .

16.4 Latent Dirichlet Allocation
493
In the limit when N, L →∞with L
N , M ∼O(1), it holds that J = op(log N),
and
FPB−B(D) = λ′PB−B
LDA
log N + op(N log N),
where
λ′PB−B
LDA
= HLη.
In the limit when N, L, M →∞with
L
N , M
N
∼O(1), it holds that J =
op(N log N), and
FPB−B(D) = λ′PB−B
LDA
log N + op(N2 log N),
where
λ′PB−B
LDA
= H(M(α −1) + Lη).
Theorem 16.19 states that the PB-B solution is sparse when α < 1. In the
following subsection, we investigate the sparsity of the solution for α ≥1.
In the Limit When N →∞with L, M ∼O(1)
The coefﬁcient of the leading term of the free energy is
λ′PB−B
LDA
= MH (α −1) +

H

h=1

Lη −1
2 −
M(h) (α −1) −L(h)

η −1
2

.
The solution is sparse if λ′PB−B
LDA
is increasing with respect to 
H, and dense if it
is decreasing. We focus on the case where α ≥1. Eqs. (16.57) through (16.59)
imply the following:
(I) When 0 < η ≤
1
2L, the solution is sparse if
Lη −1
2 −max
h
M∗(h) (α −1) > 0, or equivalently,
α < 1 −
1
maxh M∗(h)
1
2 −Lη

,
and dense if
α > 1 −
1
maxh M∗(h)
1
2 −Lη

.
Therefore, the solution is always dense in this case.
(II) When
1
2L < η ≤1
2, the solution is sparse if
Lη −1
2 −max
h
M∗(h) (α −1) > 0, or equivalently, α < 1 +
Lη −1
2
maxh M∗(h) ,

494
16 Asymptotic VB Theory of Other Latent Variable Models
and dense if
α > 1 +
Lη −1
2
maxh M∗(h) .
(III) When η > 1
2, the solution is sparse if
Lη −1
2 −max
h

M∗(h) (α −1) + L∗(h)

η −1
2

> 0,
(16.83)
and dense if
Lη −1
2 −max
h

M∗(h) (α −1) + L∗(h)

η −1
2

< 0.
(16.84)
Therefore, the solution is sparse if
Lη −1
2 −max
h
M∗(h) (α −1) −max
h
L∗(h)

η −1
2

> 0, or equivalently,
α < 1 +
1
maxh M∗(h)

Lη −1
2 −max
h
L∗(h)

η −1
2

,
and dense if
Lη −1
2 −min
h
M∗(h) (α −1) −max
h
L∗(h)

η −1
2

< 0, or equivalently,
α > 1 +
1
minh M∗(h)

Lη −1
2 −max
h
L∗(h)

η −1
2

.
Thus, we can conclude that, in this case, the solution is sparse if
α < 1 +
L −1
2 maxh M∗(h) ,
and dense if
α > 1 +
Lη −1
2
minh M∗(h) .
Summarizing the preceding, we have the following lemma:
Lemma 16.20
Assume that α ≥1. When 0 < η ≤
1
2L, the solution is always
dense. When
1
2L < η ≤1
2, the solution is sparse if α < 1 +
Lη−1
2
maxh M∗(h) , and dense
if α > 1 +
Lη−1
2
maxh M∗(h) . When η > 1
2, the solution is sparse if α < 1 +
L−1
2 maxh M∗(h) ,
and dense if α > 1 +
Lη−1
2
minh M∗(h) .

16.4 Latent Dirichlet Allocation
495
In the Limit When N, M →∞with M
N , L ∼O(1)
The coefﬁcient of the leading term of the free energy is given by
λ′PB−B
LDA
= M (Hα −1) −

H

h=1

M(h) (α −1) .
(16.85)
Although the predictive distribution does not necessarily converge to the true
distribution, we can investigate the sparsity of the solution by considering the
duplication rules (16.57) through (16.59) that keep BΘ
⊤unchanged. It is clear
that Eq. (16.85) is decreasing with respect to 
H if α > 1. Combing this result
with Theorem 16.19, which states that the PB-B solution is sparse when α < 1,
and Lemma 16.20, we obtain the following corollary:
Corollary 16.21
Consider the limit when N →∞with L, M ∼O(1). When
0 < η ≤
1
2L, the PB-B solution is sparse if α < 1, and dense if α > 1. When
1
2L < η ≤
1
2, the PB-B solution is sparse if α < 1 +
Lη−1
2
maxh M∗(h) , and dense if
α > 1+
Lη−1
2
maxh M∗(h) . When η > 1
2, the PB-B solution is sparse if α < 1+
L−1
2 maxh M∗(h) ,
and dense if α > 1 +
Lη−1
2
minh M∗(h) . In the limit when N, M →∞with M
N , L ∼O(1),
the PB-B solution is sparse if α < 1, and dense if α > 1.
MAP Learning
The free energy for MAP learning is given as follows:
FMAP = χΘ + χB + RMAP + QMAP,
(16.86)
where χΘ and χB are large constants corresponding to the negative entropies of
the delta functions, and
RMAP =
/
log rΘ(Θ)rB(B)
p(Θ|α)p(B|η)
0
rMAP(Θ,B)
=
M

m=1
⎛⎜⎜⎜⎜⎜⎝log Γ(α)H
Γ(Hα) +
H

h=1
(1 −α)
⎛⎜⎜⎜⎜⎜⎝log(αMAP
m,h ) −log(
H

h′=1
αMAP
m,h′ )
⎞⎟⎟⎟⎟⎟⎠
⎞⎟⎟⎟⎟⎟⎠
+
H

h=1
⎛⎜⎜⎜⎜⎜⎝log Γ(η)L
Γ(Lη) +
L

l=1
(1 −η)

log(ηMAP
l,h
) −log(L
l′=1ηMAP
l′,h )
 ⎞⎟⎟⎟⎟⎟⎠,
(16.87)
QMAP =
/
log
rz

{{z(n,m)}N(m)
n=1 }M
m=1
 
p({w(n,m)}, {z(n,m)}|Θ, B)
0
rMAP(Θ,B,{z(n,m)})
= −
M

m=1
N(m)
L

l=1
Vl,m log
⎛⎜⎜⎜⎜⎜⎝
H

h=1
αMAP
m,h
H
h′=1 αMAP
m,h′
ηMAP
l,h
L
l′=1ηMAP
l′,h
⎞⎟⎟⎟⎟⎟⎠.
(16.88)
Let us ﬁrst consider the case when α < 1. In this case, F diverges to
F →−∞with ﬁxed N, when αm,h = O(1) for any (h, m) and αm,h′ →+0 for

496
16 Asymptotic VB Theory of Other Latent Variable Models
all other h′  h. Therefore, the solution is sparse (so sparse that the estimator
is useless). Similarly, assume that η < 1. Then F diverges to F →−∞with
ﬁxed N, when ηl,h = O(1) for any (l, h) and ηl′,h →+0 for all other l′  l.
Therefore, the solution is useless.
When α ≥1 and η ≥1, the solution satisﬁes the following stationary
condition:
αMAP
m,h
= α −1 +
N(m)

n=1
zMAP(n,m)
h
,
ηMAP
l,h
= η −1 +
M

m=1
N(m)

n=1
w(n,m)
l
zMAP(n,m)
h
,
(16.89)
zMAP(n,m)
h
=
αMAP
m,h
L
l=1(ηMAP
l,h
)w(n,m)
l
H
h′=1

αMAP
m,h′
L
l=1(ηMAP
l,h′ )w(n,m)
l
 .
(16.90)
In the same way as for VB, PB-A, and PB-B learning, we can obtain the
following lemma:
Lemma 16.22
Let B
MAPΘ
MAP⊤= ⟨BΘ⊤⟩rMAP(Θ,B). Then QMAP is minimized
when J = Op(N−1), and it holds that
QMAP = NS N(D) + Op(JN + LM).
RMAP is written as follows:
RMAP =
⎧⎪⎪⎪⎨⎪⎪⎪⎩MH (α −1) + 
HL (η −1) −

H

h=1
 
M(h) (α −1) + L(h) (η −1)
 
⎫⎪⎪⎪⎬⎪⎪⎪⎭log N
+ (H −
H)L (η −1) log L + Op(H(M + L)).
(16.91)
Taking the different asymptotic limits, we obtain the following theorem:
Theorem 16.23
When α < 1, each row vector of Θ
MAP has only one nonzero
entry, and the MAP solution is sparse. When η < 1, each column vector of
B
MAP has only one nonzero entry. Assume in the following that α, η ≥1. In the
limit when N →∞with L, M ∼O(1), it holds that J = Op(1/N) and
FMAP(D) = λ′MAP
LDA log N + Op(1),
where
λ′MAP
LDA = MH (α −1) + 
HL (η −1) −

H

h=1
 
M(h) (α −1) + L(h) (η −1)
 
.

16.4 Latent Dirichlet Allocation
497
In the limit when N, M →∞with M
N , L ∼O(1), it holds that J = op(log N),
and
FMAP(D) = λ′MAP
LDA log N + op(N log N),
where
λ′MAP
LDA = MH (α −1) −

H

h=1

M(h) (α −1) .
In the limit when N, L →∞with L
N , M ∼O(1), it holds that J = op(log N),
and
FMAP(D) = λ′MAP
LDA log N + op(N log N),
where
λ′MAP
LDA = HL(η −1).
In the limit when N, L, M →∞with
L
N , M
N
∼O(1), it holds that J =
op(N log N), and
FMAP(D) = λ′MAP
LDA log N + op(N2 log N),
where
λ′MAP
LDA = H(M(α −1) + L(η −1)).
Theorem 16.23 states that the MAP solution is sparse when α < 1. However,
it provides no information on the sparsity of the MAP solution for η < 1. In
the following, we investigate the sparsity of the solution for α, η ≥1.
In the Limit When N →∞with L, M ∼O(1)
The coefﬁcient of the leading term of the free energy is
λ′MAP
LDA = MH (α −1) +

H

h=1

L(η −1) −
M(h) (α −1) −L(h) (η −1)
 
.
The solution is sparse if λ′MAP
LDA is increasing with respect to 
H, and dense if it is
decreasing. We focus on the case where α, η ≥1. Eqs. (16.57) through (16.59)
imply the following:

498
16 Asymptotic VB Theory of Other Latent Variable Models
The solution is sparse if
L(η −1) −max
h

M∗(h) (α −1) + L∗(h) (η −1)
 
> 0,
(16.92)
and dense if
L(η −1) −max
h

M∗(h) (α −1) + L∗(h) (η −1)
 
< 0.
(16.93)
Therefore, the solution is sparse if
L(η −1) −max
h
M∗(h) (α −1) −max
h
L∗(h) (η −1) > 0, or equivalently,
α < 1 + (L −maxh L∗(h))(η −1)
maxh M∗(h)
,
and dense if
L(η −1) −min
h
M∗(h) (α −1) −max
h
L∗(h) (η −1) < 0, or equivalently,
α > 1 + (L −maxh L∗(h))(η −1)
minh M∗(h)
.
Thus, we can conclude that the solution is sparse if
α < 1,
and dense if
α > 1 +
L(η −1)
minh M∗(h) .
Summarizing the preceding, we have the following lemma:
Lemma 16.24
Assume that η ≥1. The solution is sparse if α < 1, and dense
if α > 1 +
L(η−1)
minh M∗(h) .
In the Limit When N, M →∞with M
N , L ∼O(1)
The coefﬁcient of the leading term of the free energy is given by
λ′MAP
LDA = MH (α −1) −

H

h=1

M(h) (α −1) .
(16.94)
Although the predictive distribution does not necessarily converge to the true
distribution, we can investigate the sparsity of the solution by considering the
duplication rules (16.57) through (16.59) that keep BΘ
⊤unchanged. It is clear
that Eq. (16.94) is decreasing with respect to 
H if α > 1. Combing this result
with Theorem 16.23, which states that the MAP solution is sparse if α < 1,
and Lemma 16.24, we obtain the following corollary:

16.4 Latent Dirichlet Allocation
499
Corollary 16.25
Assume that η ≥1. In the limit when N →∞with L, M ∼
O(1), the MAP solution is sparse if α < 1, and dense if α > 1 +
L(η−1)
minh M∗(h) . In the
limit when N, M →∞with M
N , L ∼O(1), the MAP solution is sparse if α < 1,
and dense if α > 1.
Summary of Results
Summarizing Corollaries 16.11, 16.17, 16.21, and 16.25 completes the proof
of Theorem 16.13.
□

17
Uniﬁed Theory for Latent Variable Models
In this chapter, we present a formula for evaluating an asymptotic form of the
VB free energy of a general class of latent variable models by relating it to
the asymptotic theory of Bayesian learning (Watanabe, 2012). This formula is
applicable to all latent variable models discussed in Chapters 15 and 16.1 It also
explains relationships between these asymptotic analyses of VB free energy
and several previous works where the asymptotic Bayes free energy has been
analyzed for speciﬁc latent variable models. We apply this formula to Gaussian
mixture models (GMMs) as an example and demonstrate another proof of
the upper-bound of the VB free energy given in Section 15.2. Furthermore,
this analysis also provides a quantity that is related to the generalization
performance of VB learning. Analysis of generalization performance of VB
learning has been conducted only for limited cases, as discussed in Chapter
14. We show inequalities that relate the VB free energy to the generalization
errors of an approximate predictive distribution (Watanabe, 2012).
17.1 Local Latent Variable Model
Consider the joint model
p(x, z|w)
(17.1)
on the observed variable x and the local latent variable z with the parameter w.
The marginal distribution of the observed variable is2
1 The reduced rank regression (RRR) model discussed in Chapter 14 is not included in this class
of latent variable models.
2 The model is denoted as if the local latent variable is discrete, it can also be continuous. In this
case, the sum 
z is replaced by the integral

dz. The probabilistic principal component
analysis is an example with a continuous local latent variable.
500

17.1 Local Latent Variable Model
501
p(x|w) =

z
p(x, z|w).
(17.2)
For the complete data set {D, H} = {(x(n), z(1)),. . . , (x(N), z(N))}, we assume
the i.i.d. model
p(D, H|w) =
N

n=1
p(x(n), z(n)|w),
which implies
p(D|w) =
N

n=1
p(x(n)|w),
p(H|D, w) =
N

n=1
p(z(n)|x(n), w).
We assume that
p(x|w∗) =

z
p(x, z|w∗)
with the parameter w∗is the underlying distribution generating data D = {x(1),
. . . , x(N)}. Because of the nonidentiﬁability of the latent variable model, the set
of true parameters,
W∗≡
⎧⎪⎪⎨⎪⎪⎩w∗;

z
p(x, z|w∗) = p(x|w∗)
⎫⎪⎪⎬⎪⎪⎭,
(17.3)
is not generally a point but can be a union of several manifolds with
singularities as demonstrated in Section 13.5.
In the analysis in this chapter, we deﬁne and analyze quantities related to
generalization performance of a joint model, where the local latent variables
are treated as observed variables. Although we do not consider the case where
the local latent variables are observed, those quantities are useful for relating
generalization properties of VB learning to those of Bayesian learning, with
which we establish a uniﬁed theory connecting VB learning and Bayesian
learning of latent variable models.
Thus, consider for a moment the Bayesian learning of the joint model
(17.1), where the complete data set {D, H} is observed. For the prior distri-
bution p(w), the posterior distribution is given by
p(w|D, H) = p(D, H|w)p(w)
p(D, H)
.
(17.4)

502
17 Uniﬁed Theory for Latent Variable Models
The Bayes free energy of the joint model is deﬁned by
FBayes
Joint (D, H) = −log p(D, H) = −log

p(D, H|w)p(w)dw.
If w∗∈W∗is the true parameter, i.e., the complete data set {D, H} is
generated from q(x, z) = p(x, z|w∗) i.i.d., the relative Bayes free energy is
deﬁned by
FBayes
Joint (D, H) = FBayes
Joint (D, H) −NS N(D, H),
(17.5)
where
S N(D, H) = −1
N
N

n=1
log p(x(n), z(n)|w∗)
is the empirical joint entropy. Then the average relative Bayes free energy is
deﬁned by
F
Bayes
Joint (N) =
FBayes
Joint (D, H)

p(D,H|w∗) ,
and the average Bayes generalization error of the predictive distribution for the
joint model is deﬁned by
GE
Bayes
Joint (N) = KL(p(x, z|w∗)||p(x, z|D, H))
p(D,H|w∗) ,
where
p(x, z|D, H) =

p(x, z|w)p(w|D, H)dw.
These two quantities are related to each other as Eq. (13.24):
GE
Bayes
Joint (N) = F
Bayes
Joint (N + 1) −F
Bayes
Joint (N).
(17.6)
Furthermore, the average relative Bayes free energy for the joint model can be
approximated as (see Eq. (13.118))
F
Bayes
Joint (N) ≈−log

exp

−NE(w)
 
· p(w)dw,
(17.7)
where
E(w) = KL(p(x, z|w∗)||p(x, z|w)) =
/
log p(x, z|w∗)
p(x, z|w)
0
p(x,z|w∗)
.
(17.8)

17.1 Local Latent Variable Model
503
Since the log-sum inequality yields that3

z
p(x, z|w∗) log p(x, z|w∗)
p(x, z|w) ≥p(x|w∗) log p(x|w∗)
p(x|w) ,
we have
E(w) ≥E(w),
(17.10)
where
E(w) = KL(p(x|w∗)||p(x|w)) =
/
log p(x|w∗)
p(x|w)
0
p(x|w∗)
.
Hence, it follows from Eq. (13.118) that
F
Bayes(N) =
FBayes(D)

q(D)
≈−log

exp (−NE(w)) · p(w)dw
≤−log

exp $ −NE(w)% · p(w)dw ≈F
Bayes
Joint (N),
(17.11)
where
FBayes(D) = FBayes(D) −NS N(D)
is the relative Bayes free energy deﬁned by the Bayes free energy of the
original marginal model,
FBayes(D) = −log p(D) = −log

p(D|w)p(w)dw
and its empirical entropy,
S N(D) = −1
N log p(D|w∗) = −1
N
N

n=1
log p(x(n)|w∗),
(17.12)
as in Section 13.3.2.
The asymptotic theory of Bayesian learning (Theorem 13.13) shows that an
asymptotic form of F
Bayes
Joint (N) is given by
F
Bayes
Joint (N) = λ′Bayes
Joint
log N −(m′Bayes
Joint
−1) log log N + O(1),
(17.13)
3 The log-sum inequality is the following inequality satisﬁed for nonnegative reals ai ≥0 and
bi ≥0:

i
ai log ai
bi
≥
⎛⎜⎜⎜⎜⎜⎝

i
ai
⎞⎟⎟⎟⎟⎟⎠log
$
i ai
%
$
i bi
%.
(17.9)
This can be proved by subtracting the right-hand side from the left-hand side and applying the
nonnegativity of the KL divergence.

504
17 Uniﬁed Theory for Latent Variable Models
where −λ′Bayes
Joint
and m′Bayes
Joint
are respectively the largest pole and its order of the
zeta function deﬁned for a complex number z by
ζE(z) =

E(w)zp(w)dw.
(17.14)
This means that the asymptotic behavior of the free energy is characterized by
E(w), while that of the Bayes free energy FBayes is characterized by E(w) =
KL(p(x|w∗)||p(x|w)) and the zeta function ζE(z) in Eq. (13.122) as Theorem
13.13. The two functions, E and E, are related by the log-sum inequality
(17.10).
Then Corollary 13.14 implies the following asymptotic expansion of the
average generalization error:
GE
Bayes
Joint (N) =
λ′Bayes
Joint
N
−
m′Bayes
Joint
−1
N log N
+ o

1
N log N

.
(17.15)
With the preceding quantities, we ﬁrst provide a general upper-bound for the
VB free energy (Section 17.2), and then show inequalities relating the VB free
energy to the generalization errors of an approximate predictive distribution
for the joint model (Section 17.4).
17.2 Asymptotic Upper-Bound for VB Free Energy
Given the training data D = {x(1),. . . , x(N)}, consider VB learning for the latent
variable model (17.2) with the prior distribution p(w). Under the constraint,
r(w, H) = rw(w)rH(H),
the VB free energy is deﬁned by
FVB(D) =
min
rw(w),rH(H) F(r),
where
F(r) =
/
log
rw(w)rH(H)
p(D, H|w)p(w)
0
rw(w)rH(H)
(17.16)
= FBayes(D) + KL (rw(w)rH(H)||p(w, H|D)) .
(17.17)
The stationary condition of the free energy yields
rw(w) = 1
Cw
p(w) exp log p(D, H|w)
rH(H),
(17.18)
rH(H) =
1
CH
exp log p(D, H|w)
rw(w).
(17.19)

17.2 Asymptotic Upper-Bound for VB Free Energy
505
Let us deﬁne the relative VB free energy
FVB(D) = FVB(D) −NS N(D)
by the VB free energy and the empirical entropy (17.12). For arbitrary w∗∈
W∗, substituting Eq. (17.18) into Eq. (17.16), we have
FVB(D) = min
rH(H)
⎡⎢⎢⎢⎢⎣−log

p(w) exp
/
log p(D, H|w)
rH(H)
0
rH(H)
dw
⎤⎥⎥⎥⎥⎦+ log p(D|w∗)
(17.20)
≤−log

exp
⎧⎪⎪⎨⎪⎪⎩

H
p(H|D, w∗) log p(D, H|w)
p(D, H|w∗)
⎫⎪⎪⎬⎪⎪⎭p(w)dw (17.21)
≡FVB∗(D).
Here, we have substituted rH(H) ←p(H|D, w∗) =
p(D,H|w∗)

H p(D,H|w∗) to obtain the
upper-bound (17.21). The expression (17.20) of the free energy corresponds to
viewing the VB learning as a local variational approximation (Section 5.3.3),
where the variational parameter h(ξ) is the vector consisting of the elements
log p(D, H, ξ) for all possible H.4
By taking the expectation with respect to the distribution of training
samples, we deﬁne the average relative VB free energy and its upper-bound as
F
VB(N) =
FVB(D)

p(D|w∗) ,
(17.22)
F
VB∗(N) =
FVB∗(D)

p(D|w∗) .
(17.23)
From Eq. (17.7), we have
F
Bayes
Joint (N) ≈−log

e−NE(w)p(w)dw ≡FBayes
Joint (N),
where E(w) is deﬁned by Eq. (17.8). Then, the following theorem holds:
Theorem 17.1
It holds that
F
Bayes(N) ≤F
VB(N) ≤F
VB∗(N) ≤FBayes
Joint (N).
(17.24)
Proof
The left inequality follows from Eq. (17.17). Eq. (17.21) gives
F
VB(N) ≤F
VB∗(N)
=
FVB∗(D)

p(D|w∗)
4 The variational parameter h(ξ) has one-to-one correspondence with p(H|D, ξ), and is
substituted as h(ξ) ←h(w∗) in Eq. (17.21).

506
17 Uniﬁed Theory for Latent Variable Models
= −
/
log

exp
⎧⎪⎪⎨⎪⎪⎩

H
p(H|D, w∗) log p(D, H|w)
p(D, H|w∗)
⎫⎪⎪⎬⎪⎪⎭p(w)dw
0
p(D|w∗)
≤−log

exp
⎧⎪⎪⎪⎨⎪⎪⎪⎩
/
H
p(H|D, w∗) log p(D, H|w)
p(D, H|w∗)
0
p(D|w∗)
⎫⎪⎪⎪⎬⎪⎪⎪⎭p(w)dw
= −log

e−NE(w)p(w)dw = FBayes
Joint (N).
The ﬁrst and second equalities are deﬁnitions of F
VB∗(N) and FVB∗(D). We
have applied Jensen’s inequality to the convex function log

exp(·)p(w)dw to
obtain the last inequality. Finally, the last equality follows from the fact that
p(D|w∗)p(H|D, w∗) = p(D, H|w∗) and the i.i.d. assumption.
□
The following corollary is immediately obtained from Theorems 13.13
and 17.1:
Corollary 17.2
Let 0 > −λ1 > −λ2 > · · · be the sequence of the poles
of the zeta function (17.14) in the decreasing order, and m1, m2,. . . be the
corresponding orders of the poles. Then the average relative VB free energy
(17.22) can be asymptotically upper-bounded as
F
VB(N) ≤λ1 log N −(m1 −1) log log N + O(1).
(17.25)
It holds in Eqs. (17.13) and (17.15) that λ′Bayes
Joint
= λ1 and m′Bayes
Joint
= m1 for
λ1 and m1 deﬁned in Corollary 17.2. Note that E(w) depends on w∗∈W∗.
For different w∗, we have different values of λ1, which is determined by the
minimum over different w∗∈W∗in Eq. (17.25). Then m1 is determined by
the maximum of the order of the pole for the minimum λ1. Also note that unlike
for Bayesian learning, even if the largest pole of the zeta function is obtained,
Eq. (17.25) does not necessarily provide a lower-bound of the VB free energy.
If the joint model p(x, z|w), the true distribution p(x, z|w∗), and the prior
p(w) satisfy the regularity conditions (Section 13.4.1), it holds that
2λ′Bayes
Joint
= D,
where D is the number of parameters.
If the joint model p(x, z|w) is identiﬁable, even though the true parameter
is on the boundary of the parameter space or the prior does not satisfy 0 <
p(w) < ∞, λ′Bayes
Joint
can be analyzed similarly to the case of regular models. The
GMM with redundant components is an example of such a case, as will be
detailed in the next section.

17.3 Example: Average VB Free Energy of Gaussian Mixture Model
507
If the joint model p(x, z|w) is unidentiﬁable, we need the algebraic geomet-
rical technique to analyze λ′Bayes
Joint
as discussed in Section 13.5.4. This technique
is also applicable to identiﬁable cases as will be demonstrated in a part of the
analysis of λ′Bayes
Joint
for the GMM in the last part of the next section.
17.3 Example: Average VB Free Energy of
Gaussian Mixture Model
In this section, we derive an asymptotic upper-bound of the VB free energy
of GMMs. Although this upper-bound is immediately obtained from Theorem
15.5 in Section 15.2, it was derived by direct evaluation and minimization of
the free energy with respect to the expected sufﬁcient statistics. In this section,
we present another derivation through Theorem 17.1 in order to illustrate how
the general theory described in Section 17.2 is applied.
Let
g(x|μ) = GaussM(x; μ, IM)
be the M-dimensional uncorrelated Gaussian density and consider the GMM
with K components,
p(x|w) =

z
p(x, z|w) =
K

k=1
αkg(x|μk),
where x ∈RM and w = (α, {μk}K
k=1) denote the parameter vector consisting the
mixing weights and the mean vectors, respectively.
Assuming the same prior given by
p(α|φ) = DirichletK(α; (φ,. . . , φ)⊤),
(17.26)
p(μk|μ0, ξ) = GaussM(μk|μ0, (1/ξ)IM),
(17.27)
and the same true distribution
q(x) = p(x|w∗) =
K0

k=1
α∗
kg(x|μ∗
k),
(17.28)
as in Sections 4.1.1 and 15.2, we immediately obtain from the upper-bound in
Eq. (15.36) of Theorem 15.5 that
F
VB(N) ≤λ
′VB
MM log N + O(1),
(17.29)

508
17 Uniﬁed Theory for Latent Variable Models
where
λ
′VB
MM =
⎧⎪⎪⎨⎪⎪⎩
(K −K0)φ + MK0+K0−1
2
(φ < M+1
2 ),
MK+K−1
2
(φ ≥M+1
2 ).
In this section, we derive this upper-bound by using Theorem 17.1, which
provides an alternative proof to the one presented in Section 15.2. Similar
techniques were used for analyzing the Bayes free energy (13.19) in the
asymptotic limit (Yamazaki and Watanabe, 2003a,b, 2005). Here, we evaluate
the VB free energy and present the details of the proof for the speciﬁc choice
of the prior distribution.
First, in order to deﬁne p(x, z|w∗) for z with K elements, we extend and
redeﬁne the true parameter w∗denoting it as w∗= (α∗, {μ∗
k}K
k=1). Suppose that
the true distribution with parameter w∗has K nonzero mixing weights. For
example, we can assume that
α∗
k =
⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩
α∗
k
(1 ≤k ≤K0 −1),
α∗
K0/(K −K0 + 1)
(K0 ≤k ≤K),
0
(K + 1 ≤k ≤K),
μ∗
k =
) μ∗
k
(1 ≤k ≤K0),
μ∗
K0
(K0 + 1 ≤k ≤K).
Note that the marginal distribution of p(x, z|w∗) is reduced to Eq. (17.28). Then
we have
E(w) =
 
z
p(x, z|w∗) log p(x, z|w∗)
p(x, z|w) dx
=

K

k=1
α∗
kg(x|μ∗
k) log α∗
kg(x|μ∗
k)
αkg(x|μk)dx
=
K

k=1
α∗
k
)
log α∗
k
αk
+

g(x|μ∗
k) log g(x|μ∗
k)
g(x|μk)dx
1
=
K

k=1
α∗
k
)
log α∗
k
αk
+ ||μk −μ∗
k||2
2
1
.
Second, we divide the parameter w into three parts,
w1 = (α2, α3,. . . , αK),
(17.30)
w2 = (αK+1,. . . , αK),
(17.31)
w3 = (μ1, μ2,. . . , μK),
(17.32)

17.3 Example: Average VB Free Energy of Gaussian Mixture Model
509
and deﬁne
W1 = {w1; |αk −α∗
k| ≤ϵ, 2 ≤k ≤K},
W2 = {w2; |αk| ≤ϵ, K ≤k ≤K},
W3 = {w3; ||μk −μ∗
k|| ≤ϵ, 1 ≤k ≤K},
for a sufﬁciently small constant ϵ. For an arbitrary parameter w ∈W1 × W2 ×
W3 ≡W(ϵ), we can decompose E(w) as
E(w) = E1(w1) + E2(w2) + E3(w3),
(17.33)
where
E1(w1) =
K

k=2
α∗
k log α∗
k
αk
+
⎛⎜⎜⎜⎜⎜⎜⎜⎝1 −
K

k=2
α∗
k
⎞⎟⎟⎟⎟⎟⎟⎟⎠log 1 −K
k=2 α∗
k
1 −K
k=2 αk
,
E2(w2) =
1
1 −c
1 −K0
k=2 α∗
k
1 −K
k=2 αk
K

k=K+1
αk,
E3(w3) =
K

k=1
α∗
k
2 ||μk −μ∗
k||2.
(17.34)
Here we have used the mean value theorem −log(1 −t) =
1
1−ct for some c,
0 ≤c ≤t with t =
K
k=K+1 αk
1−K
k=2 αk . Furthermore, for w ∈W(ϵ), there exist positive
constants C1, C2, C3, and C4 such that
C1
K

k=2
(αk −α∗
k)2 ≤E1(w1) ≤C2
K

k=2
(αk −α∗
k)2,
(17.35)
C3
K

k=K+1
αk ≤E2(w2) ≤C4
K

k=K+1
αk.
(17.36)
Third, we evaluate the partial free energies deﬁned for i = 1, 2, 3 by
Fi = −log

Wi
exp(−NEi(wi))p(wi)dwi,
(17.37)
where p(wi) is the product of factors of the prior in Eqs. (17.26) and (17.27),
which involve wi deﬁned in Eqs. (17.30) through (17.32).
It follows from Eqs. (17.24), (17.33), and (17.37) that
F
VB(N) ≤F1 + F2 + F3 + O(1).
(17.38)

510
17 Uniﬁed Theory for Latent Variable Models
From Eqs. (17.35) and (17.34), as for F1 and F3, the Gaussian integration
yields
F1 =
K −1
2
log N + O(1),
(17.39)
F3 = MK
2
log N + O(1).
(17.40)
Since
Nφ
 ϵ
0
e−nαkαφ−1
k
dαk →Γ(φ)
(N →∞),
for k = K + 1,. . . , K, it follows from Eq. (17.36) that
F2 = (K −K)φ log N + O(1).
(17.41)
Finally, combining Eqs. (17.38) through (17.41), we obtain
F
VB(N) ≤
⎧⎪⎪⎨⎪⎪⎩(K −K)φ + MK + K −1
2
⎫⎪⎪⎬⎪⎪⎭log N + O(1).
Minimizing the right-hand side of the preceding expression over K (K0 ≤K ≤
K) leads to the upper-bound in Eq. (17.29).
Alternatively, the preceding evaluations of all the partial free energies,
F1, F2, and F3, are obtained by using the algebraic geometrical method based
on Corollary 17.2. For example, as for F2, the zeta function
ζE2(z) =

E2(w2)zp(w2)dw2
has a pole z = −(K −K)φ. This can be observed by the change of variables, the
so-called blow-up,
αk = α′
kα′
K
(k = K + 1,. . . , K −1),
αK = α′
K,
which yields that ζE2 has a term

α′
K
zα′
K
(K−K)φ−1ζE2(w′
2)dα′
K =
ζE2(w′
2)
z + (K −K)φ
,
where ζE2(w′
2) is a function proportional to
 ⎛⎜⎜⎜⎜⎜⎜⎜⎝
K−1

k=K+1
α′
k + 1
⎞⎟⎟⎟⎟⎟⎟⎟⎠
z
K−1

k=K+1
α′
k
φ−1
K−1

k=K+1
dα′
k.
Hence, we can see that ζE2 has a pole at z = −(K −K)φ.

17.4 Free Energy and Generalization Error
511
17.4 Free Energy and Generalization Error
In this section, we relate the VB free energy to the generalization performance
of VB learning. We denote a training data set by DN = {x(1), x(2),. . . , x(N)}
with the number N of training samples as a superscript in this section.
Let p(x, z|w∗) be the true distribution of the observed variable x and the
latent variable z, which has the marginal distribution p(x|w∗). We deﬁne the
generalization error of the predictive distribution for the joint distribution,
pVB∗(x, z|DN) = ⟨p(x, z|w)⟩r∗(w;w∗) =

p(x, z|w)r∗(w; w∗)dw,
(17.42)
by the Bayes generalization error (13.133)
GEVB∗
Joint(DN) = KL(p(x, z|w∗)||pVB∗(x, z|DN)),
(17.43)
and the Gibbs generalization error (13.136) by
GGEVB∗
Joint(DN) = KL(p(x, z|w∗)||p(x, z|w))
r∗(w;w∗) ,
(17.44)
where
r∗(w; w∗) ∝p(w)
N

n=1
exp
⎛⎜⎜⎜⎜⎜⎝−

z
p(z|x(n), w∗) log p(x(n), z|w∗)
p(x(n), z|w)
⎞⎟⎟⎟⎟⎟⎠
is the approximate posterior distribution (17.18) with p(H|DN, w∗) substituted
for rH(H). We denote their means by
GE
VB∗
Joint(N) =

GEVB∗
Joint(DN)

p(DN|w∗) ,
GGE
VB∗
Joint(N) =

GGEVB∗
Joint(DN)

p(DN|w∗) .
Then the following theorem holds:
Theorem 17.3
It holds that
GE
VB∗
Joint(N) ≤F
VB∗(N + 1) −F
VB∗(N) ≤GGE
VB∗
Joint(N),
(17.45)
where F
VB∗(N) is the upper-bound (17.23) of the average relative VB free
energy.
Proof
We have
FVB∗(DN+1) −FVB∗(DN)
= −log
 N+1
n=1 exp
!
z p(z|x(n), w∗) log p(x(n),z|w)
p(x(n),z|w∗)
"
p(w)dw
 N
n=1 exp

z p(z|x(n), w∗) log p(x(n),z|w)
p(x(n),z|w∗)
 
p(w)dw

512
17 Uniﬁed Theory for Latent Variable Models
= −log

exp
⎛⎜⎜⎜⎜⎜⎝

z
p(z|x(N+1), w∗) log p(x(N+1), z|w)
p(x(N+1), z|w∗)
⎞⎟⎟⎟⎟⎟⎠r∗(w; w∗)dw
=

z
p(z|x(N+1), w∗) log p(x(N+1), z|w∗)
−log

exp
!
log p(x(N+1), z|w)

p(z|x(N+1),w∗)
"
r∗(w; w∗)dw
(17.46)
≥

z
p(z|x(N+1), w∗) log
p(x(N+1), z|w∗)
p(x(N+1), z|w)
r∗(w;w∗)
.
In the last inequality, we have applied Jensen’s inequality to the convex
function
log

exp(·)p(w)dw.
Taking
the
expectation
with
respect
to
N+1
n=1 p(x(n)|w∗) in both sides of the preceding inequality yields the left
inequality in Eq. (17.45).
By applying Jensen’s inequality for the exponential function in Eq. (17.46),
and taking the expectation, we have the right inequality in Eq. (17.45).
□
The inequalities in Eq. (17.45) are analogous to Eq. (17.6). Let λ′VB∗be the
free energy coefﬁcient of F
VB∗(N), i.e.,
F
VB∗(N) = λ′VB∗log N + o(log N).
(17.47)
If its difference has the asymptotic form
F
VB∗(N + 1) −F
VB∗(N) = λ′VB∗
N
+ o
 1
N

,
the left inequality in Eq. (17.45) suggests that
GE
VB∗
Joint(N) ≤λ′VB∗
N
+ o
 1
N

.
This means that the free energy coefﬁcient λ′VB∗of F
VB∗(N) is directly related
to the generalization error of VB learning measured by Eq. (17.43). Theorem
17.1 implies that the free energy coefﬁcients satisfy λ′VB∗≤λ′Bayes
Joint , which in
turn implies from Eq. (17.6) that
GE
VB∗
Joint(N) ≤GE
Bayes
Joint (N)
(17.48)
holds asymptotically.
Let
rw(w) = argmin
rw(w)
min
rH(H) F(r)

17.5 Relation to Other Analyses
513
be the optimal VB posterior of the parameter that minimizes the free energy
(17.16). The average generalization errors of VB learning are naturally
deﬁned by
GE
VB
Joint(N) =

KL(p(x, z|w∗)||pVB(x, z|DN))

q(DN)
for the joint predictive distribution pVB(x, z|DN) = ⟨p(x, z|w)⟩rw(w), and by
GE
VB(N) =

KL(p(x|w∗)||pVB(x|DN))

q(DN)
for the marginal predictive distribution pVB(x|DN) = ⟨p(x|w)⟩rw(w). It follows
from the log-sum inequality (17.9) that
GE
VB(N) ≤GE
VB
Joint(N).
(17.49)
Since the predictive distribution (17.42) is derived from the approximate
posterior distribution r∗(w; w∗) consisting of p(H|DN, w∗) instead of the
minimizerrH(H) of the free energy, it is conjectured that GE
VB∗
Joint(N) provides
a lower-bound to GE
VB
Joint(N). At least, the inequalities in Eq. (17.45) imply
the afﬁnity of the VB free energy and the generalization error measured
by the KL divergence of the joint distributions. The generalization error
of the marginal predictive distribution is generally upper-bounded by that
of the joint predictive distribution as in Eq. (17.49). Although Eq. (17.48)
shows that the average generalization error GE
VB∗
Joint(N) of the approximate
predictive distribution of VB learning with r∗(w; w∗) is upper-bounded by that
of Bayesian learning in the joint model, the relationship between GE
VB∗
Joint(N)
and GE
VB
Joint(N) is still unknown.
17.5 Relation to Other Analyses
In this section, we discuss the relationships of the asymptotic formulae in
Sections 17.2 and 17.4 to the analyses of the Bayes free energy and the
generalization error.
17.5.1 Asymptotic Analysis of Free Energy Bounds
Asymptotic upper-bounds of the Bayes free energy were obtained for some
statistical models, including the GMM, HMM, and the Bayesian network
(Yamazaki and Watanabe, 2003a,b, 2005). The upper-bounds are given by the
following form:
F
Bayes(N) ≤λ′Bayes
Joint
log N + O(1),
(17.50)

514
17 Uniﬁed Theory for Latent Variable Models
where the coefﬁcient λ′Bayes
Joint
was identiﬁed for each model by analyzing the
largest pole of the zeta function ζE in Eq. (17.14) instead of ζE, by using the
log-sum inequality (17.10) (Yamazaki and Watanabe, 2003a,b, 2005). Since
the largest pole of ζE provides a lower-bound for that of ζE, their analyses
provided upper-bounds of F
Bayes(N) for the aforementioned models.
On the other hand, the asymptotic forms of the VB free energy were
analyzed also for the same models as discussed in Chapters 15 and 16, each of
which has the following form:
F
VB(N) ≤λ′VB log N + O(1).
(17.51)
In most cases, asymptotic upper-bounds of F
Bayes(N) and F
VB(N) coincide,
i.e., λ′Bayes
Joint
= λ′VB holds while Theorem 17.1 implies that λ′VB ≤λ′Bayes
Joint .
Hence, it is suggested that this upper-bound is tight in some cases. The
zeta function ζE was also analyzed by the algebraic geometrical technique
to evaluate the generalization error for estimating local latent variables
(Yamazaki, 2016).
Moreover, the previous analyses of the VB free energy are based on the
direct minimization of the free energy over the variational parameters (Chap-
ters 14 through 16). Hence, the analyses are highly dependent on the concrete
algorithm for the speciﬁc model and the choice of the prior distribution. In
other words, it is required to parameterize the free energy explicitly by a ﬁnite
number of variational parameters in such analyses. Analyzing the right-hand
side of Eq. (17.24) is more general and is independent of the concrete algorithm
for the speciﬁc model. It does not even require that the prior distribution p(w)
be conjugate since Theorem 17.1 holds for any prior. In such a case, the VB
learning algorithm should be implemented with techniques for nonconjugacy
such as the local variational approximation and the black-box variational
inference (Section 2.1.7). In fact, for mixture models, the upper-bound in
Theorem 15.10 averaged over the training samples can be obtained in more
general cases. The mixture component g(x|μ) can be generalized to any regular
models, while in Chapter 15 it was generalized only to the exponential family.
17.5.2 Accuracy of Approximation
For several statistical models, tighter bounds or exact evaluations of the
coefﬁcient λ′Bayes of the relative Bayes free energy in Eq. (17.5) have been
obtained (Aoyagi and Watanabe, 2005; Yamazaki et al., 2010). If the relative
Bayes free energy and VB free energy have the asymptotic forms, F
Bayes(N) =
λ′Bayes log N + o(log N) and F
VB(N) = λ′VB log N + o(log N), respectively,

17.5 Relation to Other Analyses
515
λ′Bayes ≤λ′VB holds, and the approximation accuracy of VB learning to
Bayesian learning can be evaluated by the gap between them:
F
VB(N) −F
Bayes(N) = (λ′VB −λ′Bayes) log N + o(log N).
From Eq. (17.17), this turns out to be the KL divergence from the approximate
posterior to the true posterior. Such a comparison was ﬁrst conducted for
GMMs (Watanabe and Watanabe, 2004, 2006; Aoyagi and Nagata, 2012).
A more detailed comparison was conducted for the Bernoulli mixture model
discussed in Section 15.4 (Yamazaki and Kaji, 2013; Kaji et al., 2010). Accord-
ing to the authors’ results, λ′VB can be strictly greater than λ′Bayes, while λ′VB
is not so large as D/2, where D is the number of parameters.5 The arguments
in Section 17.2 imply that such a comparison can be extended to general latent
variable models by examining the difference between minw∗∈W∗F
Bayes
Joint (N) and
F
Bayes(N), which is related to the difference between λ′Bayes
Joint
and λ′Bayes, i.e.,
the poles of ζE and ζE.
17.5.3 Average Generalization Error
Although the generalization performance of the VB learning was fully
analyzed in the RRR model as discussed in Chapter 14, little has been known
in other models. In Section 17.4, we derived an inequality that implies the
relationship between the generalization error and the VB free energy for
general latent variable models.
In the exact Bayesian learning, the universal relations (13.138) and (13.139)
among the quartet, Bayes and Gibbs generalization losses and Bayes and Gibbs
training losses, were proved as discussed in Section 13.5.5 (Watanabe, 2009). It
is an important future work to explore such relationships among the quantities
introduced in Section 17.4 for VB learning.
5 For the local latent variable model deﬁned in Section 17.1, Corollary 17.2 combined with
Eq. (13.125) implies that
2λ′VB ≤D.
However, this is not true in general as we discussed for the RRR model in Chapter 14 (see
Figure 14.8).

Appendix A
James–Stein Estimator
The James–Stein (JS) estimator (James and Stein, 1961), a shrinkage estimator known
to dominate the maximum likelihood (ML) estimator, has close relation to Bayesian
learning. More speciﬁcally, it can be derived as an empirical Bayesian (EBayes)
estimator (Efron and Morris, 1973).
Consider an M-dimensional Gaussian model with a Gaussian prior for the mean
parameter:
p(x|μ) = GaussM(x; μ, σ2IM) =

2πσ2 −M/2 exp

−∥x −μ∥2
2σ2

,
(A.1)
p(μ|c2) = GaussM(μ; 0, c2IM) =

2πc2 −M/2 exp

−∥μ∥2
2c2

,
(A.2)
where the variance σ2 of observation noise is assumed to be known. We perform
empirical Bayesian learning to estimate the mean parameter μ and the prior variance
c2 from observed samples D = {x(1),. . . , x(N)}. The joint distribution conditional to the
hyperparameter is
p(D, μ|c2) = p(μ|c2)
N

n=1
p(x(n)|μ)
=
1
(2πc2)M/2(2πσ2)NM/2 exp
⎛⎜⎜⎜⎜⎜⎜⎝−∥μ∥2
2c2 −
N

n=1
###x(n) −μ
###
2
2σ2
⎞⎟⎟⎟⎟⎟⎟⎠
=
1
(2πc2)M/2(2πσ2)NM/2 exp
⎛⎜⎜⎜⎜⎜⎜⎝−1
2σ2
N

n=1
###x(n)###
2 +
N2 ###x
###
2
2σ2(N + σ2/c2)
⎞⎟⎟⎟⎟⎟⎟⎠
· exp

−N + σ2/c2
2σ2
#####μ −
Nx
N + σ2/c2
#####
2
,
which implies that the posterior is Gaussian,
p(μ|D, c2) ∝p(D, μ|c2) ∝exp

−N + σ2/c2
2σ2
#####μ −
Nx
N + σ2/c2
#####
2
,
516

A James–Stein Estimator
517
with the mean given by
μ =
Nx
N + σ2/c2 =

1 −
σ2
Nc2 + σ2

x.
(A.3)
The marginal likelihood is computed as
p(D|c2) =

p(D, μ|c2)dμ
=
1
(2πc2)M/2(2πσ2)NM/2 exp
⎛⎜⎜⎜⎜⎜⎜⎝−1
2σ2
N

n=1
###x(n)###
2 +
N2 ###x
###
2
2σ2(N + σ2/c2)
⎞⎟⎟⎟⎟⎟⎟⎠
·

exp

−N + σ2/c2
2σ2
#####μ −
Nx
N + σ2/c2
#####
2
dμ
=
exp

−1
2σ2
N
n=1
###x(n)###
2 +
N2∥x∥
2
2σ2(N+σ2/c2)

(2πc2)M/2(2πσ2)(N−1)M/2(N + σ2/c2)M/2
=
exp

−1
2σ2
N
n=1
###x(n) −x
###
2 −
N∥x∥
2
2σ2
+
N2∥x∥
2
2σ2(N+σ2/c2)

(2π)M/2(2πσ2)(N−1)M/2(Nc2 + σ2)M/2
=
exp

−1
2σ2
N
n=1
###x(n) −x
###
2 −
Nσ2/c2∥x∥
2
2σ2(N+σ2/c2)

(2π)M/2(2πσ2)(N−1)M/2(Nc2 + σ2)M/2
=
exp

−1
2σ2
N
n=1
###x(n) −x
###
2 −
N∥x∥
2
2(Nc2+σ2)

(2π)M/2(2πσ2)(N−1)M/2(Nc2 + σ2)M/2
=
exp
!
−1
2σ2
N
n=1
###x(n) −x
###
2"
(2πσ2)(N−1)M/2
·
exp

−
N∥x∥
2
2(Nc2+σ2)

$2π(Nc2 + σ2)%M/2 .
(A.4)
This implies that v = x
C
N/(Nc2 + σ2) is a random variable subject to GaussM
(v; 0, IM). Since its (–2)nd order moment is equal to

∥v∥−2
GaussM(v;0,IM) = (M −2)−1,
we have
/ Nc2 + σ2
N
###x
###
2
0
p(D|c2)
=
1
M −2,
and therefore
/ M −2
N
###x
###
2
0
p(D|c2)
=
1
Nc2 + σ2 .
Accordingly, (M −2)/N
###x
###
2 is an unbiased estimator of the factor (Nc2 + σ2)−1.

518
A James–Stein Estimator
Replacing the factor (Nc2 + σ2)−1 in Eq. (A.3) with its unbiased estimator (M −2)/
N
###x
###
2, we obtain the JS estimator (with degree M −2):
μJS =
⎛⎜⎜⎜⎜⎜⎜⎝1 −(M −2)σ2
N
###x
###
2
⎞⎟⎟⎟⎟⎟⎟⎠x.
(A.5)
If we estimate c2 by maximizing the marginal likelihood (A.4), we obtain the positive-
part JS estimator (with degree M):
μPJS = max
⎛⎜⎜⎜⎜⎜⎜⎝0, 1 −Mσ2
N
###x
###
2
⎞⎟⎟⎟⎟⎟⎟⎠x.
(A.6)
The JS estimator has an interesting property. Let us ﬁrst introduce terminology.
Assume that we observed data D generated from a distribution p(D|w) with unknown
parameter w. Consider two estimators w1 = w1(D) and w2 = w2(D), and measure some
error criterion E(w, w∗) from the true parameter value w∗.
Deﬁnition A.1
(Domination) We say that the estimator w1 dominates the other
estimator w2 if
E(w1(D), w∗)
p(D|w∗) ≤E(w2(D), w∗)
p(D|w∗)
for arbitrary w∗,
and
E(w1(D), w∗)
p(D|w∗) < E(w2(D), w∗)
p(D|w∗)
for a certain w∗.
Deﬁnition A.2
(Efﬁciency) We say that an estimator is efﬁcient if no unbiased
estimator dominates it.
Deﬁnition A.3
(Admissibility) We say that an estimator is admissible if no estimator
dominates it.
Figure A.1 Generalization error of James–Stein estimator.

A James–Stein Estimator
519
Assume that p(D|w) = GaussM(x; μ, σ2IM). Then the ML estimator,
μML = x,
(A.7)
is known to be efﬁcient in terms of the mean squared error
E(μ, μ∗) =
###μ −μ∗###
2 .
However, the ML estimator was proven to be inadmissible when M ≥3, i.e., there
exists at least one biased estimator that dominates the ML estimator (Stein, 1956).
Subsequently, the JS estimator (A.5) was introduced as an estimator dominating the
ML estimator (James and Stein, 1961).
Figure A.1 shows the normalized squared loss N∥μ −μ∗∥2/(Mσ2) of the ML
estimator (A.7) and the JS estimator (A.5) as a function of a scaled true mean
√
N ∥μ∗∥/σ. The ML estimator always gives error equal to one, while the JS estimator
gives error dependent on the true value. We can see that the JS estimator dominates the
ML estimator for M ≥3. We can easily show that the positive-part JS estimator (A.6)
dominates the JS estimator with the same degree.

Appendix B
Metric in Parameter Space
In this appendix, we give a brief summary of the Kullback–Leibler (KL) divergence,
the Fisher information, and the Jeffreys prior. The KL divergence is a common (pseudo-
)distance measure between distributions, and the corresponding metric in the parameter
space is given by the Fisher information. The Jeffreys prior—the uniform prior when
the distance between distributions is measured by the KL divergence—is deﬁned so as
to reﬂect the nonuniformity of the density of the volume element in the parameter space.
B.1 Kullback–Leibler (KL) Divergence
The KL divergence between two distributions, q(x) and p(x), is deﬁned as
KL (q(x)∥p(x)) =

q(x) log
 q(x)
p(x)

dx
=

q(x) log
1
p(x)dx −

q(x) log
1
q(x)dx
≥0.
If q(x) is the true distribution, i.e., x ∼q(x), the ﬁrst term is the average information
gain for the one who has (possibly) wrong information (who believes x ∼p(x)), and
the second term is the average information gain, i.e., the entropy, for the one who has
the correct information (who believes x ∼q(x)). The KL divergence is not a proper
distance metric, since it is not symmetric, i.e., for general q(x) and p(x),
KL (q(x)∥p(x))  KL (p(x)∥q(x)) .
B.2 Fisher Information
The Fisher information of a parametric distribution p(x|w) with its parameter w ∈RD
is deﬁned as
SD
+ ∋F =
 ∂log p(x|w)
∂w
∂log p(x|w)
∂w
⊤
p(x|w)dx,
(B.1)
520

B.3 Metric and Volume Element
521
where ∂log p(x|w)
∂w
∈RD is the gradient (column) vector of log p(x|w). Under the regularity
conditions (see Section 13.4.1) on the statistical model p(x|w), the Fisher information
can be written as
F = −
 ∂2 log p(x|w)
∂w∂w⊤
p(x|w)dx,
(B.2)
where
∂2 log p(x|w)
∂w∂w⊤

i, j
= ∂2 log p(x|w)
∂wi∂w j
.
This is because
−
 ∂2 log p(x|w)
∂wi∂w j
p(x|w)dx
= −

∂
∂w j
⎛⎜⎜⎜⎜⎜⎜⎝
∂p(x|w)
∂wi
p(x|w)
⎞⎟⎟⎟⎟⎟⎟⎠p(x|w)dx
= −
 ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎝
∂2 p(x|w)
∂wi∂wj
p(x|w) −
∂p(x|w)
∂wi
∂p(x|w)
∂wj
p2(x|w)
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎠p(x|w)dx
= −
 ∂2p(x|w)
∂wi∂w j
dx +

∂p(x|w)
∂wi
∂p(x|w)
∂wj
p2(x|w)
p(x|w)dx
= −
∂2
∂wi∂w j

p(x|w)dx +
 ∂log p(x|w)
∂wi
∂log p(x|w)
∂w j
p(x|w)dx
=
 ∂log p(x|w)
∂wi
∂log p(x|w)
∂w j
p(x|w)dx.
B.3 Metric and Volume Element
For a small perturbation Δw of the parameter, the KL divergence between p(x|w) and
p(x|w + Δw) can be written as
KL (p(x|w)∥p(x|w + Δw)) =

p(x|w) log

p(x|w)
p(x|w + Δw)

dx
=

p(x|w) log
⎛⎜⎜⎜⎜⎜⎜⎝
p(x|w)
p(x|w) + ∂p(x|w)
∂w
⊤Δw + 1
2Δw⊤∂2 p(x|w)
∂w∂w Δw + O(∥Δw∥3)
⎞⎟⎟⎟⎟⎟⎟⎠dx
= −

p(x|w) log
⎛⎜⎜⎜⎜⎜⎜⎝1 +
∂p(x|w)
∂w
⊤Δw
p(x|w)
+ 1
2Δw⊤
∂2 p(x|w)
∂w∂w
p(x|w) Δw + O(∥Δw∥3)
⎞⎟⎟⎟⎟⎟⎟⎠dx
= −

p(x|w)
⎛⎜⎜⎜⎜⎜⎜⎝
∂p(x|w)
∂w
⊤
p(x|w) Δw + 1
2Δw⊤
⎛⎜⎜⎜⎜⎜⎜⎝
∂2 p(x|w)
∂w∂w
p(x|w) −
∂p(x|w)
∂w
∂p(x|w)
∂w
⊤
p2(x|w)
⎞⎟⎟⎟⎟⎟⎟⎠Δw
⎞⎟⎟⎟⎟⎟⎟⎠dx
+ O(∥Δw∥3)

522
B Metric in Parameter Space
= −
 ∂
∂w

p(x|w)dx
⊤
Δw −1
2Δw⊤

∂2
∂w∂w

p(x|w)dx

Δw
+ 1
2Δw⊤
 ∂log p(x|w)
∂w
∂log p(x|w)
∂w
⊤
p(x|w)dx

Δw + O(∥Δw∥3)
= 1
2Δw⊤FΔw + O(∥Δw∥3).
Therefore, the Fisher information corresponds to the metric of the space of distributions
when the distance is measured by the KL divergence (Jeffreys, 1946).
When we adopt the Fisher information as the metric, the volume element for
integrating functions is given by
dV =
1√
2
C
det (F)dw,
(B.3)
where
1√
2
√det (F) corresponds to the density.
B.4 Jeffreys Prior
The prior,
p(w) ∝
C
det (F),
(B.4)
proportional to the density of the volume element (B.3), is called the Jeffreys prior
(Jeffreys, 1946). The Jeffreys prior assigns the equal probability to the unit volume
element at any point in the parameter space, i.e., it is the uniform prior in the distribution
space. Since the uniformity is deﬁned not in the parameter space but in the distribution
space, the Jeffreys prior is invariant under parameter transformation. Accordingly, the
Jeffreys prior is said to be the parameterization invariant noninformative prior.
For singular models, the Fisher information can have zero eigenvalues, which makes
the Jeffreys prior zero. In some models, including the matrix factorization model, zero
eigenvalues appear everywhere in the parameter space (see Example B.2). In such cases,
we ignore the common zero eigenvalues and redeﬁne the (generalized) Jeffrey prior by
p(w) ∝
.D
d=1 λd,
(B.5)
where λd is the dth largest eigenvalue of the Fisher information F, and D is the
maximum number of positive eigenvalues over the whole parameter space.
Example B.1
(Jeffreys prior for one-dimensional Gaussian distribution) The Fisher
information of the Gaussian distribution,
p(x|μ, σ2) = Gauss1(x; μ, σ2) =

2πσ2 −1/2 exp

−(x −μ)2
2σ2

,
is calculated as follows. The derivatives of the log likelihood are
∂log p(x|μ, σ2)
∂μ
= ∂
∂μ

−(x −μ)2
2σ2

= x −μ
σ2 ,

B.4 Jeffreys Prior
523
∂log p(x|μ, σ2)
∂σ2
=
∂
∂σ2

−1
2 log σ2 −(x −μ)2
2σ2

= −1
2σ2 + (x −μ)2
2σ4
,
∂2 log p(x|μ, σ2)
∂μ2
= −1
σ2 ,
∂2 log p(x|μ, σ2)
∂μ∂σ2
= −x −μ
σ4 ,
∂2 log p(x|μ, σ2)
∂(σ2)2
=
1
2σ4 −(x −μ)2
σ6
,
and therefore
F =
/ ⎛⎜⎜⎜⎜⎜⎝
1
σ2
x−μ
σ4
x−μ
σ4
(x−μ)2
σ6
−
1
2σ4
⎞⎟⎟⎟⎟⎟⎠
0
p(x|μ,σ2)
=
 1
σ2
0
0
1
σ4 −
1
2σ4

=
 1
σ2
0
0
1
2σ4

.
Thus, the Jeffreys priors for p(x|μ), p(x|σ2), and p(x|μ, σ2) are
p(μ) ∝
C
Fμ,μ ∝1,
p(σ2) ∝
.
Fσ2,σ2 ∝1
σ2 ,
p(μ, σ2) ∝
C
det (F) ∝1
σ3 ,
respectively.
Example B.2
(Jeffreys prior for one-dimensional matrix factorization model) The
Fisher information of the one-dimensional matrix factorization (MF) model,
p(V|A, B) = Gauss1(V; BA, σ2) =

2πσ2 −1/2 exp

−(V −BA)2
2σ2

,
(B.6)
is calculated as follows. The derivatives of the log likelihood are
∂log p(V|A, B)
∂A
= σ−2(V −BA)B,
∂log p(V|A, B)
∂B
= σ−2(V −BA)A,
and therefore
F = 1
σ4
/(V −BA)2B2
(V −BA)2BA
(V −BA)2BA
(V −BA)2A2
0
Gauss1(V;BA,σ2)
= 1
σ2
 B2
BA
BA
A2

.

524
B Metric in Parameter Space
The Fisher information F has eigenvalues λ1 = σ−2(A2 + B2) and λ2 = 0, since
det

σ2F −λI2
 
= det
B2 −λ
BA
BA
A2 −λ

= (B2 −λ)(A2 −λ) −B2A2
= λ2 −(A2 + B2)λ
=

λ −(A2 + B2)
 
λ.
The common (over the whole parameter space) zero eigenvalue comes from the
invariance of the MF model under the transformation (A, B) →(sA, s−1B) for any
s  0. By adopting the generalized deﬁnition (B.5) of the Jeffreys prior, the distribution
proportional to
p(A, B) ∝
√
A2 + B2
(B.7)
is the parameterization invariant noninformative prior.
The Jeffreys prior is often improper, i.e., the integral of the unnormalized prior
over the parameter domain diverges, and therefore the normalization factor cannot be
computed, as in Examples B.1 and B.2.

Appendix C
Detailed Description
of Overlap Method
Let V ∈RL×M be the observed matrix, where L and M correspond to the dimensionality
D of the observation space and the number N of samples as follows:
L = D, M = N
if
D ≤N,
L = N, M = D
if
D > N.
(C.1)
Let
V =
L

h=1
γhωbhω⊤
ah
(C.2)
be the singular value decomposition (SVD) of V. The overlap (OL) method (Hoyle,
2008) computes the following approximation to the negative logarithm of the marginal
likelihood (8.90) over the hypothetical model rank H = 1,. . . , L:1
2FOL(H) ≈−2 log p(V)
= (LM −H(L −H −2)) log(2π) + L log π −2 H
h=1 log
 Γ((M−h+1)/2)
Γ(M−L−h+1)/2)
 
+ H(M −L) $1 −log (M −L)% + H
h=1
L
l=H+1 log

γ2
h −γ2
l
 
+ (M −L) H
h=1 log γ2
h + (M −H) H
h=1 log
!
1
σ2 OL −
1
λOL
h
"
−H
h=1
!
1
σ2 OL −
1
λOL
h
"
γ2
h + (L + 2)
H
h=1 logλOL
h
+ (M −H) log σ2 OL 
+ L
l=1
γ2
l
σ2 OL ,
(C.3)
where Γ(·) denotes the Gamma function, and {λOL
h }H
h=1 and σ2 OL are estimators for
{λh = b2
h + σ2}H
h=1 and σ2, respectively, computed by iterating the following updates
until convergence:
1 Our description is slightly different from Hoyle (2008), because the MF model (6.1) does not
have the mean parameter shared over the samples.
525

526
C Detailed Description of Overlap Method
Algorithm 23 Overlap method.
1: Prepare the observed matrix V ∈RL×M, following the rule (C.1).
2: Compute the SVD (C.2) of V.
3: Compute FOL(0) by Eq. (C.6).
4: for H = 1 to L do
5:
Initialize the noise variance to σ2 OL = 10−4 · L
h=1 γ2
h/(LM).
6:
Iterate Eq. (C.4) for h = 1,. . . , H, and Eq. (C.5) until convergence or any λOL
h
becomes a complex number.
7:
Compute FOL(H) by Eq. (C.3) if all {λOL
h }H
h=1 are real numbers. Otherwise, set
FOL(H) = ∞.
8: end for
9: Estimate the rank by 
HOL = minH∈{0,...,L} FOL(H).
λOL
h
=
γ2
h
2(L+2)

1 −(M−H−(L+2))σ2 OL
γ2
h
+
B!
1 −(M−H−(L+2))σ2 OL
γ2
h
"2
−4(L+2)σ2 OL
γ2
h

,
(C.4)
σ2 OL =
1
(M−H)
!L
l=1
γ2
l
L −H
h=1λOL
h
"
.
(C.5)
When iterating Eqs. (C.4) and (C.5), λOL
h
can become a complex number. In such a
case, the hypothetical H is rejected. Otherwise, Eq. (C.3) is evaluated after convergence.
For the null hypothesis, i.e., H = 0, the negative log likelihood is given by
2FOL(0) = −2 log p(V) = LM

log

2π
LM
L
l=1 γ2
l
 
+ 1
 
.
(C.6)
The estimated rank 
HOL is the minimizer of FOL(H) over H = 0,. . . , L.
Algorithm 23 summarizes the procedure.

Appendix D
Optimality of Bayesian Learning
Bayesian learning is deduced from the basic probability theory, and therefore it is
optimal in terms of generalization performance under the assumption that the model
and the prior are set reasonably.
Consider a distribution of problems where the true distribution is written as q(x) =
p(x|w∗) with the true parameter w∗subject to q(w∗). Although we usually omit the
dependency description on the true distribution or the true parameter, the average
generalization error, Eq. (13.13), naturally depends on the true distribution, so we here
denote the dependence explicitly as GE(N; w∗). Let
GE(N) =

GE(N; w∗)

q(w∗)
(D.1)
be the average of the average generalization error over the distribution q(w∗) of the true
parameter.
Theorem D.1
If we know the distribution q(w∗) of the true parameter and use it as
the prior distribution, i.e., p(w) = q(w), then Bayesian learning minimizes the average
generalization error (D.1) over q(w∗), i.e.,
GE
Bayes
(N) ≤GE
Other
(N),
(D.2)
where GE
Other
(N) denotes the average generalization error of any (other) learning
algorithm.
Proof
Let XN = (x(1),. . . , x(N)) be the N training samples. Regarding the new test
sample as the (N + 1)th sample, we can write the Bayes predictive distribution as
follows:
pBayes(x(N+1)|XN) =

p(x(N+1)|w)p(w|XN)dw
=

p(w) N+1
n=1 p(x(n)|w)dw

p(w′) N
n=1 p(x(n)|w′)dw′
= p(XN+1)
p(XN) ,
(D.3)
527

528
D Optimality of Bayesian Learning
where p(XN) =

p(w)p(XN|w)dw is the marginal likelihood. The average generaliza-
tion error (D.1) of a learning algorithm with its predictive distribution r(x) is given by
GE(N) =
/
log p(x(N+1)|w∗)
r(x(N+1))
0
p(XN+1|w∗)q(w∗)
= −
 
p(XN+1|w∗)q(w∗)dw∗

log r(x(N+1))dXN+1 −(N + 1)S ,
= −

q(XN+1) log r(x(N+1))dXN+1 −(N + 1)S ,
(D.4)
where
q(XN) =

p(XN|w∗)q(w∗)dw∗
is the marginal likelihood with the true prior distribution q(w∗), and
S = −log p(x|w∗)
p(x|w∗)q(w∗)
is the entropy, which does not depend on the predictive distribution r(x). Eq. (D.4) can
be written as
GE(N) = −

q(XN)q(XN+1)
q(XN) log r(x(N+1))dXN+1 −(N + 1)S
= −
/ q(XN+1)
q(XN) log r(x(N+1))dx(N+1)
0
q(XN)
−(N + 1)S
=
/ q(XN+1)
q(XN) log
q(XN+1)
q(XN)
r(x(N+1))dx(N+1)
0
q(XN)
+ const.
(D.5)
Since the ﬁrst term is the KL divergence between q(XN+1)/q(XN) and r(x(N+1)),
Eq. (D.5) is minimized when
r(x(N+1)) = q(XN+1)
q(XN) = qBayes(x(N+1)|XN),
(D.6)
where qBayes(x(N+1)|XN) is the Bayes predictive distribution (D.3) with the prior
distribution set to the distribution of the true parameter, i.e., p(w) = q(w). Thus, we
have proved that no other learning method can give better generalization error than
Bayesian learning with the true prior.
□
A remark is that, since we usually do not know the true distribution and the true
prior (the distribution of the true parameter), it is not surprising that an approximation
method, e.g., variational Bayesian learning, to Bayesian learning provides better
generalization performance than Bayesian learning with a nontrue prior in some
situations.

Bibliography
Akaho, S., and Kappen, H. J. 2000. Nonmonotonic Generalization Bias of Gaussian
Mixture Models. Neural Computation, 12, 1411–1427.
Akaike, H. 1974. A New Look at Statistical Model. IEEE Transactions on Automatic
Control, 19(6), 716–723.
Akaike, H. 1980. Likelihood and Bayes Procedure. Pages 143–166 of: Bernald, J. M.
(ed.), Bayesian Statistics. Valencia, Italy: University Press.
Alzer, H. 1997. On Some Inequalities for the Gamma and Psi Functions. Mathematics
of Computation, 66(217), 373–389.
Amari, S., Park, H., and Ozeki, T. 2002. Geometrical Singularities in the Neuroman-
ifold of Multilayer Perceptrons. Pages 343–350 of: Advances in NIPS, vol. 14.
Cambridge, MA: MIT Press.
Aoyagi, M., and Nagata, K. 2012. Learning Coefﬁcient of Generalization Error in
Bayesian Estimation and Vandermonde Matrix-Type Singularity. Neural Compu-
tation, 24(6), 1569–1610.
Aoyagi, M., and Watanabe, S. 2005. Stochastic Complexities of Reduced Rank
Regression in Bayesian Estimation. Neural Networks, 18(7), 924–933.
Asuncion, A., and Newman, D.J. 2007. UCI Machine Learning Repository.
www.ics.uci.edu/∼mlearn/MLRepository.html
Asuncion, A., Welling, M., Smyth, P., and Teh, Y. W. 2009. On Smoothing and
Inference for Topic Models. Pages 27–34 of: Proceedings of UAI. Stockholm,
Sweden: Morgan Kaufmann Publishers Inc.
Attias, H. 1999. Inferring Parameters and Structure of Latent Variable Models by
Variational Bayes. Pages 21–30 of: Proceedings of UAI. Stockholm, Sweden:
Morgan Kaufmann Publishers Inc.
Babacan, S. D., Nakajima, S., and Do, M. N. 2012a. Probabilistic Low-Rank Subspace
Clustering. Pages 2753–2761 of: Advances in Neural Information Processing
Systems 25. Lake Tahoe, NV: NIPS Foundation.
Babacan, S. D., Luessi, M., Molina, R., and Katsaggelos, A. K. 2012b. Sparse
Bayesian Methods for Low-Rank Matrix Estimation. IEEE Transactions on Signal
Processing, 60(8), 3964–3977.
Baik, J., and Silverstein, J. W. 2006. Eigenvalues of Large Sample Covariance Matrices
of Spiked Population Models. Journal of Multivariate Analysis, 97(6), 1382–1408.
529

530
Bibliography
Baldi, P. F., and Hornik, K. 1995. Learning in Linear Neural Networks: A Survey. IEEE
Transactions on Neural Networks, 6(4), 837–858.
Banerjee, A., Merugu, S., Dhillon, I. S., and Ghosh, J. 2005. Clustering with Bregman
Divergences. Journal of Machine Learning Research, 6, 1705–1749.
Beal, M. J. 2003. Variational Algorithms for Approximate Bayesian Inference. PhD
thesis, University College London.
Bicego, M., Lovato, P., Ferrarini, A., and Delledonne, M. 2010. Biclustering of Expres-
sion Microarray Data with Topic Models. Pages 2728–2731 of: Proceedings of
ICPR. Istanbul, Turkey: ICPR.
Bickel, P., and Chernoff, H. 1993. Asymptotic Distribution of the Likelihood Ratio
Statistic in a Prototypical Non Regular Problem. New Delhi, India: Wiley Eastern
Limited.
Bishop, C. M. 1999a. Bayesian Principal Components. Pages 382–388 of: Advances in
NIPS, vol. 11. Denver, CO: NIPS Foundation.
Bishop, C. M. 1999b. Variational Principal Components. Pages 514–509 of: Proceed-
ings of International Conference on Artiﬁcial Neural Networks, vol. 1. Edinburgh,
UK: Computing and Control Engineering Journal.
Bishop, C. M. 2006. Pattern Recognition and Machine Learning. New York: Springer.
Bishop, C. M., and Tipping, M. E. 2000. Variational Relevance Vector Machines.
Pages 46–53 of: Proceedings of the Sixteenth Conference Annual Conference
on Uncertainty in Artiﬁcial Intelligence. Stanford, CA: Morgan Kaufmann
Publishers Inc.
Blei, D. M., and Jordan, M. I. 2005. Variational Inference for Dirichlet Process
Mixtures. Bayesian Analysis, 1, 121–144.
Blei, D. M., Ng, A. Y., and Jordan, M. I. 2003. Latent Dirichlet Allocation. Journal of
Machine Learning Research, 3, 993–1022.
Bouchaud, J. P., and Potters, M. 2003. Theory of Financial Risk and Derivative
Pricing—From Statistical Physics to Risk Management, 2nd edn. Cambridge, UK:
University Press.
Brown, L. D. 1986. Fundamentals of Statistical Exponential Families. IMS Lecture
Notes–Monograph Series 9. Beachwood, OH: Institute of Mathematical Statistics.
Cand`es, E. J., Li, X., Ma, Y., and Wright, J. 2011. Robust Principal Component
Analysis? Journal of the ACM, 58(3), 1–37.
Carroll, J. D., and Chang, J. J. 1970. Analysis of Individual Differences in Multidimen-
sional Scaling via an N-way Generalization of “Eckart–Young” Decomposition.
Psychometrika, 35, 283–319.
Chen, X., Hu, X., Shen, X., and Rosen, G. 2010. Probabilistic Topic Modeling
for Genomic Data Interpretation. Pages 149–152 of: 2010 IEEE International
Conference on Bioinformatics and Biomedicine (BIBM).
Chib, S. 1995. Marginal Likelihood from the Gibbs Output. Journal of the American
Statistical Association, 90(432), 1313–1321.
Chu, W., and Ghahramani, Z. 2009. Probabilistic Models for Incomplete Multi-
dimensional Arrays. Pages 89–96. In: Proceedings of International Conference
on Artiﬁcial Intelligence and Statistics. Clearwater Beach, FL: Proceedings of
Machine Learning Research.

Bibliography
531
Courant, R., and Hilbert, D. 1953. Methods of Mathematical Physics, Volume 1. New
York: Wiley.
Cramer, H. 1949. Mathematical Methods of Statistics. Princeton, NJ: University Press.
Dacunha-Castelle, D., and Gassiat, E. 1997. Testing in Locally Conic Models, and
Application to Mixture Models. Probability and Statistics, 1, 285–317.
Dempster, A. P., Laird, N. M., and Rubin, D. B. 1977. Maximum Likelihood for
Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society,
39-B, 1–38.
Dharmadhikari, S., and Joag-Dev, K. 1988. Unimodality, Convexity, and Applications.
Cambridge, MA: Academic Press.
Ding, X., He, L., and Carin, L. 2011. Bayesian Robust Principal Component Analysis.
IEEE Transactions on Image Processing, 20(12), 3419–3430.
Drexler, F. J. 1978. A Homotopy Method for the Calculation of All Zeros of Zero-
Dimensional Polynomial Ideals. Pages 69–93 of: Wacker, H. J. (ed.), Continuation
Methods. New York: Academic Press.
D’Souza, A., Vijayakumar, S., and Schaal, S. 2004. The Bayesian Backﬁtting Rele-
vance Vector Machine. In: Proceedings of the 21st International Conference on
Machine Learning. Banff, AB: Association for Computing Machinery.
Durbin, R., Eddy, S., Krogh, A., and Mitchison, G. 1998. Biological Sequence Analysis:
Probabilistic Models of Proteins and Nucleic Acids. Cambridge: Cambridge
University Press.
Efron, B., and Morris, C. 1973. Stein’s Estimation Rule and its Competitors—An
Empirical Bayes Approach. Journal of the American Statistical Association, 68,
117–130.
Elhamifar, E., and Vidal, R. 2013. Sparse Subspace Clustering: Algorithm, Theory, and
Applications. IEEE Transactions on Pattern Analysis and Machine Intelligence,
35(11), 2765–2781.
Felzenszwalb, P. F., and Huttenlocher, D. P. 2004. Efﬁcient Graph-Based Image
Segmentation. International Journal of Computer Vision, 59(2), 167–181.
Fukumizu, K. 1999. Generalization Error of Linear Neural Networks in Unidentiﬁable
Cases. Pages 51–62 of: Proceedings of International Conference on Algorithmic
Learning Theory. Tokyo, Japan: Springer.
Fukumizu, K. 2003. Likelihood Ratio of Unidentiﬁable Models and Multilayer Neural
Networks. Annals of Statistics, 31(3), 833–851.
Garcia, C. B., and Zangwill, W. I. 1979. Determining All Solutions to Certain Systems
of Nonlinear Equations. Mathematics of Operations Research, 4, 1–14.
Gershman, S. J., and Blei, D. M. 2012. A Tutorial on Bayesian Nonparametric Models.
Journal of Mathematical Psychology, 56(1), 1–12.
Ghahramani, Z., and Beal, M. J. 2001. Graphical Models and Variational Methods.
Pages 161–177 of: Advanced Mean Field Methods. Cambridge, MA: MIT Press.
Girolami, M. 2001. A Variational Method for Learning Sparse and Overcomplete
Representations. Neural Computation, 13(11), 2517–2532.
Girolami, M., and Kaban, A. 2003. On an Equivalence between PLSI and LDA. Pages
433–434 of: Proceedings of SIGIR, New York and Toronto, ON: Association for
Computing Machinery.

532
Bibliography
Gopalan, P., Hofman, J. M., and Blei, D. M. 2013. Scalable Recommendation with
Poisson Factorization. arXiv:1311.1704 [cs.IR].
Grifﬁths, T. L., and Steyvers, M. 2004. Finding Scientiﬁc Topics. PNAS, 101, 5228–
5235.
Gunji, T., Kim, S., Kojima, M., Takeda, A., Fujisawa, K., and Mizutani, T. 2004.
PHoM—A Polyhedral Homotopy Continuation Method. Computing, 73, 57–77.
Gupta, A. K., and Nagar, D. K. 1999. Matrix Variate Distributions. London, UK:
Chapman and Hall/CRC.
Hagiwara, K. 2002. On the Problem in Model Selection of Neural Network Regression
in Overrealizable Scenario. Neural Computation, 14, 1979–2002.
Hagiwara, K., and Fukumizu, K. 2008. Relation between Weight Size and Degree of
Over-Fitting in Neural Network Regression. Neural Networks, 21(1), 48–58.
Han, T. S., and Kobayashi, K. 2007. Mathematics of Information and Coding. Provi-
dence, RI: American Mathematical Society.
Harshman, R. A. 1970. Foundations of the PARAFAC Procedure: Models and Condi-
tions for an “Explanatory” Multimodal Factor Analysis. UCLA Working Papers in
Phonetics, 16, 1–84.
Hartigan, J. A. 1985. A Failure of Likelihood Ratio Asymptotics for Normal Mixtures.
Pages 807–810 of: Proceedings of the Berkeley Conference in Honor of J. Neyman
and J. Kiefer. Berkeley, CA: Springer.
Hastie, T., and Tibshirani, R. 1986. Generalized Additive Models. Statistical Science,
1(3), 297–318.
Hinton, G. E., and van Camp, D. 1993. Keeping Neural Networks Simple by Minimiz-
ing the Description Length of the Weights. Pages 5–13 of: Proceedings of COLT.
Santa Cruz, CA.
Hoffman, M. D., Blei, D. M., Wang, C., and Paisley, J. 2013. Stochastic Variational
Inference. Journal of Machine Learning Research, 14, 1303–1347.
Hofmann, T. 2001. Unsupervised Learning by Probabilistic Latent Semantic Analysis.
Machine Learning, 42, 177–196.
Hosino, T., Watanabe, K., and Watanabe, S. 2005. Stochastic Complexity of Variational
Bayesian Hidden Markov Models. In: Proceedings of IJCNN. Montreal, QC.
Hosino, T., Watanabe, K., and Watanabe, S. 2006a. Free Energy of Stochastic Context
Free Grammar on Variational Bayes. Pages 407–416 of: Proceedings of ICONIP.
Hong Kong, China: Springer.
Hosino, T., Watanabe, K., and Watanabe, S. 2006b. Stochastic Complexity of Hidden
Markov Models on the Variational Bayesian Learning (in Japanese). IEICE
Transactions on Information and Systems, J89-D(6), 1279–1287.
Hotelling, H. 1933. Analysis of a Complex of Statistical Variables into Principal
Components. Journal of Educational Psychology, 24, 417–441.
Hoyle, D. C. 2008. Automatic PCA Dimension Selection for High Dimensional Data
and Small Sample Sizes. Journal of Machine Learning Research, 9, 2733–2759.
Hoyle, D. C., and Rattray, M. 2004. Principal-Component-Analysis Eigenvalue Spectra
from Data with Symmetry-Breaking Structure. Physical Review E, 69(026124).
Huynh, T., Mario, F., and Schiele, B. 2008. Discovery of Activity Patterns Using Topic
Models. Pages 9–10. In: International Conference on Ubiquitous Computing (Ubi-
Comp). New York and Seoul, South Korea: Association for Computer Machinery.

Bibliography
533
Hyv¨arinen, A., Karhunen, J., and Oja, E. 2001. Independent Component Analysis. New
York: Wiley.
Ibragimov, I. A. 1956. On the Composition of Unimodal Distributions. Theory of
Probability and Its Applications, 1(2), 255–260.
Ilin, A., and Raiko, T. 2010. Practical Approaches to Principal Component Analysis
in the Presence of Missing Values. Journal of Machine Learning Research, 11,
1957–2000.
Ito, H., Amari, S., and Kobayashi, K. 1992. Identiﬁability of Hidden Markov Infor-
mation Sources and Their Minimum Degrees of Freedom. IEEE Transactions on
Information Theory, 38(2), 324–333.
Jaakkola, T. S., and Jordan, M. I. 2000. Bayesian Parameter Estimation via Variational
Methods. Statistics and Computing, 10, 25–37.
James, W., and Stein, C. 1961. Estimation with Quadratic Loss. Pages 361–379
of: Proceedings of the 4th Berkeley Symposium on Mathematical Statistics and
Probability, vol. 1. Berkeley: University of California Press.
Jeffreys, H. 1946. An Invariant Form for the Prior Probability in Estimation Problems.
Pages 453–461 of: Proceedings of the Royal Society of London. Series A,
Mathematical and Physical Sciences, vol. 186. London, UK: Royal Society.
Jensen, F. V. 2001. Bayesian Networks and Decision Graphs. Springer.
Johnstone, I. M. 2001. On the Distribution of the Largest Eigenvalue in Principal
Components Analysis. Annals of Statistics, 29, 295–327.
Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., and Saul, L. 1999. Introduction to
Variational Methods for Graphical Models. Machine Learning, 37, 183–233.
Kaji, D., Watanabe, K., and Watanabe, S. 2010. Phase Transition of Variational Bayes
Learning in Bernoulli Mixture. Australian Journal of Intelligent Information
Processing Systems, 11(4), 35–40.
Khan, M. E., Babanezhad, R., Lin, W., Schmidt, M., and Sugiyama, M. 2016. Faster
Stochastic Variational Inference Using Proximal-Gradient Methods with General
Divergence Functions. Pages 309–318. In: Proceedings of UAI. New York: AUAI
Press.
Kim, Y. D., and Choi, S. 2014. Scalable Variational Bayesian Matrix Factorization with
Side Information. Pages 493–502 of: Proceedings of AISTATS. Reykjavik, Iceland:
Proceedings of Machine Learning Research.
Kingma, D. P., and Welling, M. 2014. Auto-Encoding Variational Bayes. In: Interna-
tional Conference on Learning Representations (ICLR). arXiv:1412.6980
Kolda, T. G., and Bader, B. W. 2009. Tensor Decompositions and Applications. SIAM
Review, 51(3), 455–500.
Krestel, R., Fankhauser, P., and Nejdl, W. 2009. Latent Dirichlet Allocation for Tag
Recommendation. Pages 61–68 of: Proceedings of the Third ACM Conference on
Recommender Systems. New York: Association for Computing Machinery.
Krizhevsky, A., Sutskever, I., and Hinton, G. E. 2012. ImageNet Classiﬁcation with
Deep Convolutional Neural Networks. Pages 1097–1105 of: Advances in NIPS.
Lake Tahoe, NV: NIPS Foundation.
Kurihara, K., and Sato, T. 2004. An application of the variational Bayesian Approach
to Probabilistic Context-Free Grammars. In: Proceedings of IJCNLP. Banff, AB.

534
Bibliography
Kurihara, K., Welling, M., and Teh, M. Y. W. 2007. Collapsed Variational Dirichlet
Process Mixture Models. In: Proceedings of IJCAI. Hyderabad, India.
Kuriki, S., and Takemura, A. 2001. Tail Probabilities of the Maxima of Multilinear
Forms and Their Applications. Annals of Statistics, 29(2), 328–371.
Lee, T. L., Li, T. Y., and Tsai, C. H. 2008. HOM4PS-2.0: A Software Package for
Solving Polynomial Systems by the Polyhedral Homotopy Continuation Method.
Computing, 83, 109–133.
Levin, E., Tishby, N., and Solla, S. A. 1990. A Statistical Approaches to Learning and
Generalization in Layered Neural Networks. Pages 1568–1674 of: Proceedings of
IEEE, vol. 78.
Li, F.-F., and Perona, P. 2005. A Bayesian Hierarchical Model for Learning Natural
Scene Categories. Pages 524–531 of: Proceedings of CVPR. San Diego, CA.
Lim, Y. J., and Teh, Y. W. 2007. Variational Bayesian Approach to Movie Rating
Prediction. In: Proceedings of KDD Cup and Workshop. New York and San Jose,
CA: Association for Computing Machinery.
Lin, Z., Chen, M., and Ma, Y. 2009. The Augmented Lagrange Multiplier Method for
Exact Recovery of Corrupted Low-Rank Matrices. UIUC Technical Report UILU-
ENG-09-2215.
Liu, G., and Yan, S. 2011. Latent Low-Rank Representation for Subspace Segmentation
and Feature Extraction. In: Proceedings of ICCV. Barcelona, Spain.
Liu, G., Lin, Z., and Yu, Y. 2010. Robust Subspace Segmentation by Low-Rank Repre-
sentation. Pages 663–670 of: Proceedings of ICML. Haifa, Israel: Omnipress.
Liu, G., Xu, H., and Yan, S. 2012. Exact Subspace Segmentation and Outlier Detection
by Low-Rank Representation. In: Proceedings of AISTATS. La Palma, Canary
Islands: Proceedings of Machine Learning Research.
Liu, X., Pasarica, C., and Shao, Y. 2003. Testing Homogeneity in Gamma Mixture
Models. Scandinavian Journal of Statistics, 30, 227–239.
Lloyd, S. P. 1982. Least Square Quantization in PCM. IEEE Transactions on Informa-
tion Theory, 28(2), 129–137.
MacKay, D. J. C. 1992. Bayesian Interpolation. Neural Computation, 4(2), 415–447.
MacKay, D. J. C. 1995. Developments in Probabilistic Modeling with Neural
Networks—Ensemble Learning. Pages 191–198 of: Proceedings of the 3rd Annual
Symposium on Neural Networks.
Mackay, D. J. C. 2001. Local Minima, Symmetry-Breaking, and Model Pruning in
Variational Free Energy Minimization. Available from www.inference.phy.cam
.ac.uk/mackay/minima.pdf.
MacKay, D. J. C. 2003. Information Theory, Inference, and Learning Algorithms.
Cambridge: Cambridge University Press. Available from www.inference.phy.cam
.ac.uk/mackay/itila/.
MacQueen, J. B. 1967. Some Methods for Classiﬁcation and Analysis of Multivariate
Observations. Pages 281–297 of: Proceedings of 5th Berkeley Symposium on
Mathematical Statistics and Probability, vol. 1. Berkeley: University of California
Press.
Marˇcenko, V. A., and Pastur, L. A. 1967. Distribution of Eigenvalues for Some Sets of
Random Matrices. Mathematics of the USSR-Sbornik, 1(4), 457–483.

Bibliography
535
Marshall, A. W., Olkin, I., and Arnold, B. C. 2009. Inequalities: Theory of Majorization
and Its Applications, 2d ed. Springer.
Minka, T. P. 2001a. Automatic Choice of Dimensionality for PCA. Pages 598–604 of:
Advances in NIPS, vol. 13. Cambridge, MA: MIT Press.
Minka, T. P. 2001b. Expectation Propagation for Approximate Bayesian Inference.
Pages 362–369 of: Proceedings of UAI. Seattle, WA: Morgan Kaufmann Publish-
ers Inc.
Mørup, M., and Hansen, L. R. 2009. Automatic Relevance Determination for Multi-
Way Models. Journal of Chemometrics, 23, 352–363.
Nakajima, S., and Sugiyama, M. 2011. Theoretical Analysis of Bayesian Matrix
Factorization. Journal of Machine Learning Research, 12, 2579–2644.
Nakajima, S., and Sugiyama, M. 2014. Analysis of Empirical MAP and Empirical
Partially Bayes: Can They Be Alternatives to Variational Bayes? Pages 20–28 of:
Proceedings of International Conference on Artiﬁcial Intelligence and Statistics,
vol. 33. Reykjavik, Iceland: Proceedings of Machine Learning Research.
Nakajima, S., and Watanabe, S. 2007. Variational Bayes Solution of Linear Neural
Networks and Its Generalization Performance. Neural Computation, 19(4), 1112–
1153.
Nakajima, S., Sugiyama, M., and Babacan, S. D. 2011 (June 28–July 2). On Bayesian
PCA: Automatic Dimensionality Selection and Analytic Solution. Pages 497–
504 of: Proceedings of 28th International Conference on Machine Learning
(ICML2011). Bellevue, WA: Omnipress.
Nakajima, S., Sugiyama, M., Babacan, S. D., and Tomioka, R. 2013a. Global Analytic
Solution of Fully-Observed Variational Bayesian Matrix Factorization. Journal of
Machine Learning Research, 14, 1–37.
Nakajima, S., Sugiyama, M., and Babacan, S. D. 2013b. Variational Bayesian Sparse
Additive Matrix Factorization. Machine Learning, 92, 319–1347.
Nakajima, S., Takeda, A., Babacan, S. D., Sugiyama, M., and Takeuchi, I. 2013c.
Global Solver and Its Efﬁcient Approximation for Variational Bayesian Low-Rank
Subspace Clustering. In: Advances in Neural Information Processing Systems 26.
Lake Tahoe, NV: NIPS Foundation.
Nakajima, S., Sato, I., Sugiyama, M., Watanabe, K., and Kobayashi, H. 2014. Analysis
of Variational Bayesian Latent Dirichlet Allocation: Weaker Sparsity Than MAP.
Pages 1224–1232 of: Advances in NIPS, vol. 27. Montreal, Quebec:: NIPS
Foundation.
Nakajima, S., Tomioka, R., Sugiyama, M., and Babacan, S. D. 2015. Condition
for Perfect Dimensionality Recovery by Variational Bayesian PCA. Journal of
Machine Learning Research, 16, 3757–3811.
Nakamura, F., and Watanabe, S. 2014. Asymptotic Behavior of Variational Free Energy
for Normal Mixtures Using General Dirichlet Distribution (in Japanese). IEICE
Transactions on Information and Systems, J97-D(5), 1001–1013.
Neal, R. M. 1996. Bayesian Learning for Neural Networks. New York: Springer.
Opper, M., and Winther, O. 1996. A Mean Field Algorithm for Bayes Learning in Large
Feed-Forward Neural Networks. Pages 225–231 of: Advances in NIPS. Denver,
CO: NIPS Foundation.

536
Bibliography
Pearson, K. 1914. Tables for Statisticians and Biometricians. Cambridge: Cambridge
University Press.
Purushotham, S., Liu, Y., and Kuo, C. C. J. 2012. Collaborative Topic Regression
with Social Matrix Factorization for Recommendation Systems. In: Proceedings
of ICML. Edinburgh, UK: Omnipress.
Rabiner, L. R. 1989. A Tutorial on Hidden Markov Models and Selected Applications
in Speech Recognition. Pages 257–286 of: Proceedings of the IEEE. Piscataway,
NJ: IEEE.
Ranganath, R., Gerrish, S., and Blei, D. M. 2013. Black Box Variational Inference.
In: Proceedings of AISTATS. Scottsdale, AZ: Proceedings of Machine Learning
Research.
Reinsel, G. R., and Velu, R. P. 1998. Multivariate Reduced-Rank Regression: Theory
and Applications. New York: Springer.
Rissanen, J. 1986. Stochastic Complexity and Modeling. Annals of Statistics, 14(3),
1080–1100.
Robbins, H., and Monro, S. 1951. A Stochastic Approximation Method. Annals of
Mathematical Statistics, 22(3), 400–407.
Ruhe, A. 1970. Perturbation Bounds for Means of Eigenvalues and Invariant Subspaces.
BIT Numerical Mathematics, 10, 343–354.
Rusakov, D., and Geiger, D. 2005. Asymptotic Model Selection for Naive Bayesian
Networks. Journal of Machine Learning Research, 6, 1–35.
Sakamoto, T., Ishiguro, M., and Kitagawa, G. 1986. Akaike Information Criterion
Statistics. Dordrecht: D. Reidel Publishing Company.
Salakhutdinov, R., and Mnih, A. 2008. Probabilistic Matrix Factorization. Pages 1257–
1264 of: Platt, J. C., Koller, D., Singer, Y., and Roweis, S. (eds), Advances in
Neural Information Processing Systems 20. Cambridge, MA: MIT Press.
Sato, I., Kurihara, K., and Nakagawa, H. 2012. Practical Collapsed Variational
Bayes Inference for Hierarchical Dirichlet Process. Pages 105–113. In: Pro-
ceedings of KDD. New York and Beijing, China: Association for Computing
Machinery.
Sato, M., Yoshioka, T., Kajihara, S., et al. 2004. Hierarchical Bayesian Estimation for
MEG Inverse Problem. NeuroImage, 23, 806–826.
Schwarz, G. 1978. Estimating the Dimension of a Model. Annals of Statistics, 6(2),
461–464.
Seeger, M. 2008. Bayesian Inference and Optimal Design for the Sparse Linear Model.
Journal of Machine Learning Research, 9, 759–813.
Seeger, M. 2009. Sparse Linear Models: Variational Approximate Inference and
Bayesian Experimental Design. In: Journal of Physics: Conference Series, vol.
197. Bristol, UK: IOP Publishing.
Seeger, M., and Bouchard, G. 2012. Fast Variational Bayesian Inference for Non-
Conjugate Matrix Factorization Models. Pages 1012–1018. In: Proceedings of
International Conference on Artiﬁcial Intelligence and Statistics. La Palma,
Canary Islands: Proceedings of Machine Learning Research.
Shi, J., and Malik, J. 2000. Normalized Cuts and Image Segmentation. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, 22(8), 888–905.

Bibliography
537
Soltanolkotabi, M., and Cand`es, E. J. 2011. A Geometric Analysis of Subspace
Clustering with Outliers. CoRR. arXiv:1112.4258 [cs.IT].
Spall, J. 2003. Introduction to Stochastic Search and Optimization: Estimation, Simu-
lation, and Control. New York: John Wiley and Sons.
Srebro, N., and Jaakkola, T. 2003. Weighted Low Rank Approximation. In: Fawcett, T.,
and Mishra, N. (eds), Proceedings of the Twentieth International Conference on
Machine Learning. Washington, DC: AAAI Press.
Srebro, N., Rennie, J., and Jaakkola, T. 2005. Maximum Margin Matrix Factorization.
In: Advances in Neural Information Processing Systems 17. Vancouver, BC: NIPS
Foundation.
Stein, C. 1956. Inadmissibility of the Usual Estimator for the Mean of a Multivariate
Normal Distribution. Pages 197–206 of: Proceedings of the 3rd Berkeley Symposi-
ium on Mathematics Statistics and Probability. Berkeley: University of California
Press.
Takemura, A., and Kuriki, S. 1997. Weights of Chi-Squared Distribution for Smooth or
Piecewise Smooth Cone Alternatives. Annals of Statistics, 25(6), 2368–2387.
Teh, Y. W., Newman, D., and Welling, M. 2007. A Collapsed Variational Bayesian
Inference Algorithm for Latent Dirichlet Allocation. In: Advances in NIPS.
Vancouver, BC: NIPS Foundation.
Tipping, M. E. 2001. Sparse Bayesian Learning and the Relevance Vector Machine.
Journal of Machine Learning Research, 1, 211–244.
Tipping, M. E., and Bishop, C. M. 1999. Probabilistic Principal Component Analysis.
Journal of the Royal Statistical Society, 61, 611–622.
Tomioka, R., Suzuki, T., Sugiyama, M., and Kashima, H. 2010. An Efﬁcient and
General Augmented Lagrangian Algorithm for Learning Low-Rank Matrices. In:
Proceedings of International Conference on Machine Learning. Haifa, Israel:
Omnipress.
Tron, R., and Vidal, R. 2007. A Benchmark for the Comparison of 3-D Motion
Segmentation Algorithms. Pages 1–8. In: Proceedings of CVPR. Minneapolis,
MN.
Tucker, L. R. 1996. Some Mathematical Notes on Three-Mode Factor Analysis.
Psychometrika, 31, 279–311.
Ueda, N., Nakano, R., Ghahramani, Z., and Hinton, G. E. 2000. SMEM Algorithm for
Mixture Models. Neural Computation, 12(9), 2109–2128.
van der Vaart, A. W. 1998. Asymptotic Statistics. Cambridge Series in Statistical
and Probabilistic Mathematics. Cambridge and New York: Cambridge University
Press.
Vidal, R., and Favaro, P. 2014. Low Rank Subspace Clustering. Pattern Recognition
Letters, 43(1), 47–61.
Wachter, K. W. 1978. The Strong Limits of Random Matrix Spectra for Sample
Matrices of Independent Elements. Annals of Probability, 6, 1–18.
Wainwright, M. J., and Jordan, M. I. 2008. Graphical Models, Exponential Families,
and Variational Inference. Foundations and Trends in Machine Learning, 1,
1–305.
Watanabe, K. 2012. An Alternative View of Variational Bayes and Asymptotic Approx-
imations of Free Energy. Machine Learning, 86(2), 273–293.

538
Bibliography
Watanabe, K., and Watanabe, S. 2004. Lower Bounds of Stochastic Complexities
in Variational Bayes Learning of Gaussian Mixture Models. Pages 99–104 of:
Proceedings of IEEE on CIS.
Watanabe, K., and Watanabe, S. 2005. Variational Bayesian Stochastic Complexity of
Mixture Models. Pages 99–104. In: Advances in NIPS, vol. 18. Vancouver, BC:
NIPS Foundation.
Watanabe, K., and Watanabe, S. 2006. Stochastic Complexities of Gaussian Mixtures
in Variational Bayesian Approximation. Journal of Machine Learning Research,
7, 625–644.
Watanabe, K., and Watanabe, S. 2007. Stochastic Complexities of General Mixture
Models in Variational Bayesian Learning. Neural Networks, 20(2), 210–219.
Watanabe, K., Shiga, M., and Watanabe, S. 2006. Upper Bounds for Variational
Stochastic Complexities of Bayesian Networks. Pages 139–146 of: Proceedings
of IDEAL. Burgos, Spain: Springer.
Watanabe, K., Shiga, M., and Watanabe, S. 2009. Upper Bound for Variational Free
Energy of Bayesian Networks. Machine Learning, 75(2), 199–215.
Watanabe, K., Okada, M., and Ikeda, K. 2011. Divergence Measures and a General
Framework for Local Variational Approximation. Neural Networks, 24(10), 1102–
1109.
Watanabe, S. 2001a. Algebraic Analysis for Nonidentiﬁable Learning Machines.
Neural Computation, 13(4), 899–933.
Watanabe, S. 2001b. Algebraic Information Geometry for Learning Machines with
Singularities. Pages 329–336 of: Advances in NIPS, vol. 13. Vancouver, BC: NIPS
Foundation.
Watanabe, S. 2009. Algebraic Geometry and Statistical Learning Theory. Cambridge:
Cambridge University Press.
Watanabe, S. 2010. Asymptotic Equivalence of Bayes Cross Validation and Widely
Applicable Information Criterion in Singular Learning Theory. Journal of
Machine Learning Research, 11, 3571–3594.
Watanabe, S. 2013. A Widely Applicable Bayesian Information Criterion. Journal of
Machine Learning Research, 14, 867–897.
Watanabe, S., and Amari, S. 2003. Learning Coefﬁcients of Layered Models When the
True Distribution Mismatches the Singularities. Neural Computation, 15, 1013–
1033.
Wei, X., and Croft, W. B. 2006. LDA-Based Document Models for Ad-Hoc Retrieval.
Pages 178–185 of: Proceedings of SIGIR. Seattle, WA: Association for Computing
Machinery New York.
Wingate, D., and Weber, T. 2013. Automated Variational Inference in Probabilistic
Programming. arXiv:1301.1299.
Yamazaki, K. 2016. Asymptotic Accuracy of Bayes Estimation for Latent Variables
with Redundancy. Machine Learning, 102(1), 1–28.
Yamazaki, K., and Kaji, D. 2013. Comparing Two Bayes Methods Based on the Free
Energy Functions in Bernoulli Mixtures. Neural Networks, 44, 36–43.
Yamazaki, K., and Watanabe, S. 2003a. Singularities in Mixture Models and Upper
Bounds Pages 1–8. of Stochastic Complexity. Neural Networks, 16(7), 1029–1038.

Bibliography
539
Yamazaki, K., and Watanabe, S. 2003b. Stochastic Complexity of Bayesian Networks.
Pages 592–599 of: Proceedings of the Nineteenth Conference on Uncertainty in
Artiﬁcial Intelligence. Acapulco, Mexico: Morgan Kaufmann.
Yamazaki, K., and Watanabe, S. 2004. Newton Diagram and Stochastic Complexity
in Mixture of Binomial Distributions. Pages 350–364. In: Proceedings of ALT.
Padova, Italy: Springer.
Yamazaki, K., and Watanabe, S. 2005. Algebraic Geometry and Stochastic Complexity
of Hidden Markov Models. Neurocomputing, 69, 62–84.
Yamazaki, K., Aoyagi, M., and Watanabe, S. 2010. Asymptotic Analysis of Bayesian
Generalization Error with Newton Diagram. Neural Networks, 23(1), 35–43.

Subject Index
N-mode tensor, 80
ℓ1-norm, 94, 291
ℓ1-regularizer, 88
n-mode tensor product, 80
activation function, 47
admissibility, 518
Akaike’s information criterion (AIC), 365
all temperatures method, 382
approximate global variational Bayesian
solver (AGVBS), 269
ARD Tucker, 332
asymptotic normality, 342, 352
asymptotic notation, 344
automatic relevance determination (ARD), 10,
36, 72, 204, 243, 294
average generalization error, 347
average training error, 347
Bachmann–Landau notation, 344
backﬁtting algorithm, 283
basis selection effect, 373, 426
Bayes free energy, 36, 194, 348
Bayes generalization loss, 379
Bayes posterior, 4
Bayes theorem, 4
Bayes training loss, 379
Bayesian estimator, 5
Bayesian information criterion (BIC), 366
Bayesian learning, 3, 5
Bayesian network, 115, 455
Bernoulli distribution, 11, 27, 248, 253
Bernoulli mixture model, 451
Beta distribution, 11, 27
Beta function, 27
binomial distribution, 11, 27
black-box variational inference, 50
burn-in, 59
calculus of variations, 44
centering, 73
central limit theorem, 344
chi-squared distribution, 356
classiﬁcation, 47
collaborative ﬁltering (CF), 74, 335
collapsed Gibbs sampling, 53
collapsed MAP learning, 53
collapsed variational Bayesian learning,
53
complete likelihood, 8
conditional conjugacy, 39, 42
conditional distribution, 3
conditionally conjugate prior, 42
conjugacy, 10, 12
consistency, 352
continuation method, 267
convergence in distribution, 344
convergence in law, 344
convergence in probability, 344
coordinate descent, 66
core tensor, 80
Cram´er–Rao lower-bound, 348
credible interval, 32
cross-validation, 9
cross-covariance, 73
density, 522
Dirac delta function, 37, 52
direct site bounding, 49, 132
Dirichlet distribution, 11, 26
Dirichlet process prior, 112
distinct signal assumption, 419
540

Subject Index
541
domination, 38, 518
doubly stochastic, 154
efﬁciency, 518
empirical Bayesian (EBayes) estimator, 38,
193, 516
empirical Bayesian (EBayes) learning, 9, 35
empirical entropy, 349
empirical MAP (EMAP) learning, 311
empirical PB (EPB) learning, 311
empirical variational Bayesian (EVB)
learning, 47
entropy, 380, 520
equivalence class, 187
error function, 47
Euler–Lagrange equation, 44
evidence, 36
evidence lower-bound (ELBO), 40, 341
exact global variational Bayesian solver
(EGVBS), 267
expectation propagation (EP), 53
expectation-maximization (EM) algorithm, 9,
53, 105
exponential family, 15, 108, 443
factor matrix, 80
ﬁnite mixture model, 103
Fisher information, 51, 197, 342, 520
foreground/background video separation,
97
forward–backward algorithm, 122
free energy, 40
free energy coefﬁcient, 349
Gamma distribution, 11, 16
Gamma function, 525
Gauss–Wishart distribution, 21
Gaussian distribution, 11
Gaussian mixture model, 104, 434
generalization coefﬁcient, 348
generalization error, 335, 347
generalized Bayesian learning, 378
generalized posterior distribution, 378
generalized predictive distribution, 379
Gibbs generalization loss, 379
Gibbs learning, 380
Gibbs sampling, 59
Gibbs training loss, 379
global latent variable, 7, 103
Hadamard product, 154
hard assignment, 9
hidden Markov model, 119, 461
hidden variable, 6
hierarchical model, 17
histogram, 26
homotopy method, 267
hyperparameter, 9
hyperprior, 9
identiﬁability, 342, 352
improper prior, 200
independent and identically distributed
(i.i.d.), 4
information criterion, 364
inside–outside algorithm, 126
integration effect, 374, 427
inverse temperature parameter, 379
isotropic Gauss-Gamma distribution, 17
isotropic Gaussian distribution, 11
iterative singular value shrinkage, 248, 252
James–Stein (JS) estimator, 38, 516
Jeffreys prior, 198, 522
joint distribution, 3
Kronecker delta, 130
Kronecker product, 66, 81, 331
Kronecker product covariance approximation
(KPCA), 93, 274
Kullback–Leibler (KL) divergence, 39, 197,
347, 520
Laplace approximation (LA), 51, 230
large-scale limit, 215, 319
latent Dirichlet allocation, 26, 127, 470
latent variable, 6
latent variable model, 103, 429
law of large numbers, 344
likelihood ratio, 347
linear neural network, 385
linear regression model, 22
link function, 253
local latent variable, 7, 103
local variational approximation, 49, 132
local-EMAP estimator, 317
local-EPB estimator, 317
local-EVB estimator, 182, 231, 242
log marginal likelihood, 36
log-concave distribution, 218
logistic regression, 132

542
Subject Index
low-rank representation, 88
low-rank subspace clustering (LRSC), 88, 255
Marˇcenko–Pastur (MP) distribution, 215
Marˇcenko–Pastur upper limit (MPUL), 216,
319
marginal likelihood, 5
Markov chain Monte Carlo (MCMC), 58
matrix factorization (MF), 63, 195
matrix variate Gaussian, 66
maximum a posteriori (MAP) estimator, 188
maximum a posteriori (MAP) learning, 5, 294
maximum likelihood (ML) estimator, 188, 432
maximum likelihood (ML) learning, 5, 105
maximum log-likelihood, 366
mean update (MU) algorithm, 283
mean value theorem, 353
metric, 522
Metropolis–Hastings sampling, 58
minimum description length (MDL), 366
mixing weight, 7
mixture coefﬁcient, 7
mixture model, 26, 196
mixture of Gaussians, 104
model distribution, 3
model likelihood, 3
model parameter, 3
model selection, 364
model-induced regularization (MIR), 72, 89,
94, 184, 195, 285, 308, 344, 373, 427
moment matching, 54
multilayer neural network, 196
multinomial distribution, 11, 26
multinomial parameter, 26
natural parameter, 15, 108
neural network, 47
Newton–Raphson method, 108
noise variance parameter, 22
noninformative prior, 522
nonsingular, 23
normalization constant, 10
normalized cuts, 88
Occam’s razor, 201, 366
one-of-K representation, 8, 104, 455
overﬁtting, 420
overlap (OL) method, 230, 242
Parafac, 80
partially Bayesian (PB) learning, 51, 131, 294
partitioned-and-rearranged (PR) matrix, 95,
279
plug-in predictive distribution, 6, 46
Poisson distribution, 248, 253
polygamma function, 108
polynomial system, 162, 257, 267
positive-part James–Stein (PJS) estimator,
185, 307, 390, 518
posterior covariance, 5
posterior distribution, 4
posterior mean, 5
predictive distribution, 6
prior distribution, 3
probabilistic context-free grammar, 123, 466
probabilistic latent semantic analysis (pLSA),
131
probabilistic principal component analysis
(probabilistic PCA), 63, 71, 230
quasiconvexity, 207, 305
radial basis function (RBF), 367
random matrix theory, 214, 375, 404
realizability, 346, 352
realizable, 375
rectiﬁed linear unit (ReLU), 47
reduced rank regression (RRR), 72, 385
regression parameter, 22
regular learning theory, 351
regular model, 198, 342
regularity condition, 342, 351
relative Bayes free energy, 348
relative variational Bayesian (VB) free energy,
383
resolution of singularities, 378
robust principal component analysis (robust
PCA), 93, 288
sample mean, 13
score function, 50
segmentation-based SAMF (sSAMF), 289
selecting the optimal basis function, 372
self-averaging, 216
sigmoid function, 47, 248, 253
simple variational Bayesian (SimpleVB)
learning, 71
singular learning theory (SLT), 376
singular model, 197, 342, 522
singularities, 197, 342
soft assignment, 9

Subject Index
543
sparse additive matrix factorization (SAMF),
94, 96, 204, 279
sparse matrix factorization (SMF) term, 94,
204, 279
sparse subspace clustering, 88
sparsity-inducing prior, 135
spectral clustering algorithm, 88
spiked covariance (SC) distribution, 217
spiked covariance model, 214
standard (K −1)-simplex, 7
state density, 376
stick-breaking process, 112
stochastic complexity, 36
stochastic gradient descent, 50
strictly quasiconxex, 207
strong unimodality, 218
subspace clustering, 87
subtle signal assumption, 420
sufﬁcient statistics, 15
superdiagonal, 80
Taylor approximation, 51
tensor, 80
tensor mode, 80
tensor rank, 80
trace norm, 88, 94, 291
training coefﬁcient, 348
training error, 347
trial distribution, 39
Tucker factorization (TF), 80, 294, 331, 336
underﬁtting, 420
unidentiﬁability, 184, 187, 195
uniform prior, 198, 522
unnormalized posterior distribution, 4
variation, 44
variational Bayesian (VB) estimator, 46
variational Bayesian (VB) free energy, 383
variational Bayesian (VB) learning, 39
variational Bayesian (VB) posterior, 43, 46
variational parameter, 46, 66
vectorization operator, 66, 81, 331
volume element, 197, 522
weak convergence, 344
whitening, 73, 386
widely applicable Bayesian information
criterion (WBIC), 60, 381
widely applicable information criterion
(WAIC), 380
Wishart distribution, 11, 20
zeta function, 376




Contents
Preface .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ix
1.
Linear Equations
. . . . . . . . . . . . . . 1
1.1
Introduction
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1
1.2
Gaussian Elimination and Matrices
.
.
.
.
.
.
.
.
3
1.3
Gauss–Jordan Method .
.
.
.
.
.
.
.
.
.
.
.
.
. 15
1.4
Two-Point Boundary Value Problems
.
.
.
.
.
.
. 18
1.5
Making Gaussian Elimination Work .
.
.
.
.
.
.
. 21
1.6
Ill-Conditioned Systems
.
.
.
.
.
.
.
.
.
.
.
.
. 33
2.
Rectangular Systems and Echelon Forms
. . .
41
2.1
Row Echelon Form and Rank .
.
.
.
.
.
.
.
.
.
. 41
2.2
Reduced Row Echelon Form
.
.
.
.
.
.
.
.
.
.
. 47
2.3
Consistency of Linear Systems
.
.
.
.
.
.
.
.
.
. 53
2.4
Homogeneous Systems .
.
.
.
.
.
.
.
.
.
.
.
.
. 57
2.5
Nonhomogeneous Systems
.
.
.
.
.
.
.
.
.
.
.
. 64
2.6
Electrical Circuits .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 73
3.
Matrix Algebra . . . . . . . . . . . . . .
79
3.1
From Ancient China to Arthur Cayley .
.
.
.
.
.
. 79
3.2
Addition and Transposition
.
.
.
.
.
.
.
.
.
.
. 81
3.3
Linearity .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 89
3.4
Why Do It This Way
.
.
.
.
.
.
.
.
.
.
.
.
.
. 93
3.5
Matrix Multiplication
.
.
.
.
.
.
.
.
.
.
.
.
.
. 95
3.6
Properties of Matrix Multiplication
.
.
.
.
.
.
.
105
3.7
Matrix Inversion
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
115
3.8
Inverses of Sums and Sensitivity
.
.
.
.
.
.
.
.
124
3.9
Elementary Matrices and Equivalence
.
.
.
.
.
.
131
3.10
The LU Factorization
.
.
.
.
.
.
.
.
.
.
.
.
.
141
4.
Vector Spaces . . . . . . . . . . . . . . . 159
4.1
Spaces and Subspaces
.
.
.
.
.
.
.
.
.
.
.
.
.
159
4.2
Four Fundamental Subspaces .
.
.
.
.
.
.
.
.
.
169
4.3
Linear Independence
.
.
.
.
.
.
.
.
.
.
.
.
.
181
4.4
Basis and Dimension
.
.
.
.
.
.
.
.
.
.
.
.
.
194

vi
Contents
4.5
More about Rank .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
210
4.6
Classical Least Squares
.
.
.
.
.
.
.
.
.
.
.
.
223
4.7
Linear Transformations
.
.
.
.
.
.
.
.
.
.
.
.
238
4.8
Change of Basis and Similarity
.
.
.
.
.
.
.
.
.
251
4.9
Invariant Subspaces .
.
.
.
.
.
.
.
.
.
.
.
.
.
259
5.
Norms, Inner Products, and Orthogonality
. . 269
5.1
Vector Norms
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
269
5.2
Matrix Norms
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
279
5.3
Inner-Product Spaces
.
.
.
.
.
.
.
.
.
.
.
.
.
286
5.4
Orthogonal Vectors
.
.
.
.
.
.
.
.
.
.
.
.
.
.
294
5.5
Gram–Schmidt Procedure
.
.
.
.
.
.
.
.
.
.
.
307
5.6
Unitary and Orthogonal Matrices .
.
.
.
.
.
.
.
320
5.7
Orthogonal Reduction .
.
.
.
.
.
.
.
.
.
.
.
.
341
5.8
Discrete Fourier Transform .
.
.
.
.
.
.
.
.
.
.
356
5.9
Complementary Subspaces .
.
.
.
.
.
.
.
.
.
.
383
5.10
Range-Nullspace Decomposition
.
.
.
.
.
.
.
.
394
5.11
Orthogonal Decomposition .
.
.
.
.
.
.
.
.
.
.
403
5.12
Singular Value Decomposition
.
.
.
.
.
.
.
.
.
411
5.13
Orthogonal Projection .
.
.
.
.
.
.
.
.
.
.
.
.
429
5.14
Why Least Squares? .
.
.
.
.
.
.
.
.
.
.
.
.
.
446
5.15
Angles between Subspaces
.
.
.
.
.
.
.
.
.
.
.
450
6.
Determinants . . . . . . . . . . . . . . . 459
6.1
Determinants .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
459
6.2
Additional Properties of Determinants .
.
.
.
.
.
475
7.
Eigenvalues and Eigenvectors . . . . . . . . 489
7.1
Elementary Properties of Eigensystems
.
.
.
.
.
489
7.2
Diagonalization by Similarity Transformations
.
.
505
7.3
Functions of Diagonalizable Matrices
.
.
.
.
.
.
525
7.4
Systems of Diﬀerential Equations
.
.
.
.
.
.
.
.
541
7.5
Normal Matrices
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
547
7.6
Positive Deﬁnite Matrices
.
.
.
.
.
.
.
.
.
.
.
558
7.7
Nilpotent Matrices and Jordan Structure
.
.
.
.
574
7.8
Jordan Form
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
587
7.9
Functions of Nondiagonalizable Matrices .
.
.
.
.
599

Contents
vii
7.10
Diﬀerence Equations, Limits, and Summability
.
.
616
7.11
Minimum Polynomials and Krylov Methods
.
.
.
642
8.
Perron–Frobenius Theory
. . . . . . . . . 661
8.1
Introduction
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
661
8.2
Positive Matrices
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
663
8.3
Nonnegative Matrices
.
.
.
.
.
.
.
.
.
.
.
.
.
670
8.4
Stochastic Matrices and Markov Chains
.
.
.
.
.
687
Index
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
705

Preface
Scaffolding
Reacting to criticism concerning the lack of motivation in his writings,
Gauss remarked that architects of great cathedrals do not obscure the beauty
of their work by leaving the scaﬀolding in place after the construction has been
completed. His philosophy epitomized the formal presentation and teaching of
mathematics throughout the nineteenth and twentieth centuries, and it is still
commonly found in mid-to-upper-level mathematics textbooks. The inherent ef-
ﬁciency and natural beauty of mathematics are compromised by straying too far
from Gauss’s viewpoint. But, as with most things in life, appreciation is gen-
erally preceded by some understanding seasoned with a bit of maturity, and in
mathematics this comes from seeing some of the scaﬀolding.
Purpose, Gap, and Challenge
The purpose of this text is to present the contemporary theory and applica-
tions of linear algebra to university students studying mathematics, engineering,
or applied science at the postcalculus level. Because linear algebra is usually en-
countered between basic problem solving courses such as calculus or diﬀerential
equations and more advanced courses that require students to cope with mathe-
matical rigors, the challenge in teaching applied linear algebra is to expose some
of the scaﬀolding while conditioning students to appreciate the utility and beauty
of the subject. Eﬀectively meeting this challenge and bridging the inherent gaps
between basic and more advanced mathematics are primary goals of this book.
Rigor and Formalism
To reveal portions of the scaﬀolding, narratives, examples, and summaries
are used in place of the formal deﬁnition–theorem–proof development. But while
well-chosen examples can be more eﬀective in promoting understanding than
rigorous proofs, and while precious classroom minutes cannot be squandered on
theoretical details, I believe that all scientiﬁcally oriented students should be
exposed to some degree of mathematical thought, logic, and rigor. And if logic
and rigor are to reside anywhere, they have to be in the textbook. So even when
logic and rigor are not the primary thrust, they are always available. Formal
deﬁnition–theorem–proof designations are not used, but deﬁnitions, theorems,
and proofs nevertheless exist, and they become evident as a student’s maturity
increases. A signiﬁcant eﬀort is made to present a linear development that avoids
forward references, circular arguments, and dependence on prior knowledge of the
subject. This results in some ineﬃciencies—e.g., the matrix 2-norm is presented

x
Preface
before eigenvalues or singular values are thoroughly discussed. To compensate,
I try to provide enough “wiggle room” so that an instructor can temper the
ineﬃciencies by tailoring the approach to the students’ prior background.
Comprehensiveness and Flexibility
A rather comprehensive treatment of linear algebra and its applications is
presented and, consequently, the book is not meant to be devoured cover-to-cover
in a typical one-semester course. However, the presentation is structured to pro-
vide ﬂexibility in topic selection so that the text can be easily adapted to meet
the demands of diﬀerent course outlines without suﬀering breaks in continuity.
Each section contains basic material paired with straightforward explanations,
examples, and exercises. But every section also contains a degree of depth coupled
with thought-provoking examples and exercises that can take interested students
to a higher level. The exercises are formulated not only to make a student think
about material from a current section, but they are designed also to pave the way
for ideas in future sections in a smooth and often transparent manner. The text
accommodates a variety of presentation levels by allowing instructors to select
sections, discussions, examples, and exercises of appropriate sophistication. For
example, traditional one-semester undergraduate courses can be taught from the
basic material in Chapter 1 (Linear Equations); Chapter 2 (Rectangular Systems
and Echelon Forms); Chapter 3 (Matrix Algebra); Chapter 4 (Vector Spaces);
Chapter 5 (Norms, Inner Products, and Orthogonality); Chapter 6 (Determi-
nants); and Chapter 7 (Eigenvalues and Eigenvectors). The level of the course
and the degree of rigor are controlled by the selection and depth of coverage in
the latter sections of Chapters 4, 5, and 7. An upper-level course might consist
of a quick review of Chapters 1, 2, and 3 followed by a more in-depth treatment
of Chapters 4, 5, and 7. For courses containing advanced undergraduate or grad-
uate students, the focus can be on material in the latter sections of Chapters 4,
5, 7, and Chapter 8 (Perron–Frobenius Theory of Nonnegative Matrices). A rich
two-semester course can be taught by using the text in its entirety.
What Does “Applied” Mean?
Most people agree that linear algebra is at the heart of applied science, but
there are divergent views concerning what “applied linear algebra” really means;
the academician’s perspective is not always the same as that of the practitioner.
In a poll conducted by SIAM in preparation for one of the triannual SIAM con-
ferences on applied linear algebra, a diverse group of internationally recognized
scientiﬁc corporations and government laboratories was asked how linear algebra
ﬁnds application in their missions. The overwhelming response was that the pri-
mary use of linear algebra in applied industrial and laboratory work involves the
development, analysis, and implementation of numerical algorithms along with
some discrete and statistical modeling. The applications in this book tend to
reﬂect this realization. While most of the popular “academic” applications are
included, and “applications” to other areas of mathematics are honestly treated,

Preface
xi
there is an emphasis on numerical issues designed to prepare students to use
linear algebra in scientiﬁc environments outside the classroom.
Computing Projects
Computing projects help solidify concepts, and I include many exercises
that can be incorporated into a laboratory setting. But my goal is to write a
mathematics text that can last, so I don’t muddy the development by marrying
the material to a particular computer package or language. I am old enough
to remember what happened to the FORTRAN- and APL-based calculus and
linear algebra texts that came to market in the 1970s. I provide instructors with a
ﬂexible environment that allows for an ancillary computing laboratory in which
any number of popular packages and lab manuals can be used in conjunction
with the material in the text.
History
Finally, I believe that revealing only the scaﬀolding without teaching some-
thing about the scientiﬁc architects who erected it deprives students of an im-
portant part of their mathematical heritage. It also tends to dehumanize mathe-
matics, which is the epitome of human endeavor. Consequently, I make an eﬀort
to say things (sometimes very human things that are not always complimentary)
about the lives of the people who contributed to the development and applica-
tions of linear algebra. But, as I came to realize, this is a perilous task because
writing history is frequently an interpretation of facts rather than a statement
of facts. I considered documenting the sources of the historical remarks to help
mitigate the inevitable challenges, but it soon became apparent that the sheer
volume required to do so would skew the direction and ﬂavor of the text. I can
only assure the reader that I made an eﬀort to be as honest as possible, and
I tried to corroborate “facts.” Nevertheless, there were times when interpreta-
tions had to be made, and these were no doubt inﬂuenced by my own views and
experiences.
Supplements
Included with this text is a solutions manual and a CD-ROM. The solutions
manual contains the solutions for each exercise given in the book. The solutions
are constructed to be an integral part of the learning process. Rather than just
providing answers, the solutions often contain details and discussions that are
intended to stimulate thought and motivate material in the following sections.
The CD, produced by Vickie Kearn and the people at SIAM, contains the entire
book along with the solutions manual in PDF format. This electronic version
of the text is completely searchable and linked. With a click of the mouse a
student can jump to a referenced page, equation, theorem, deﬁnition, or proof,
and then jump back to the sentence containing the reference, thereby making
learning quite eﬃcient. In addition, the CD contains material that extends his-
torical remarks in the book and brings them to life with a large selection of

xii
Preface
portraits, pictures, attractive graphics, and additional anecdotes. The support-
ing Internet site at MatrixAnalysis.com contains updates, errata, new material,
and additional supplements as they become available.
SIAM
I thank the SIAM organization and the people who constitute it (the in-
frastructure as well as the general membership) for allowing me the honor of
publishing my book under their name. I am dedicated to the goals, philosophy,
and ideals of SIAM, and there is no other company or organization in the world
that I would rather have publish this book. In particular, I am most thankful
to Vickie Kearn, publisher at SIAM, for the conﬁdence, vision, and dedication
she has continually provided, and I am grateful for her patience that allowed
me to write the book that I wanted to write. The talented people on the SIAM
staﬀwent far above and beyond the call of ordinary duty to make this project
special. This group includes Lois Sellers (art and cover design), Michelle Mont-
gomery and Kathleen LeBlanc (promotion and marketing), Marianne Will and
Deborah Poulson (copy for CD-ROM biographies), Laura Helfrich and David
Comdico (design and layout of the CD-ROM), Kelly Cuomo (linking the CD-
ROM), and Kelly Thomas (managing editor for the book). Special thanks goes
to Jean Anderson for her eagle-sharp editor’s eye.
Acknowledgments
This book evolved over a period of several years through many diﬀerent
courses populated by hundreds of undergraduate and graduate students. To all
my students and colleagues who have oﬀered suggestions, corrections, criticisms,
or just moral support, I oﬀer my heartfelt thanks, and I hope to see as many of
you as possible at some point in the future so that I can convey my feelings to
you in person. I am particularly indebted to Michele Benzi for conversations and
suggestions that led to several improvements. All writers are inﬂuenced by people
who have written before them, and for me these writers include (in no particular
order) Gil Strang, Jim Ortega, Charlie Van Loan, Leonid Mirsky, Ben Noble,
Pete Stewart, Gene Golub, Charlie Johnson, Roger Horn, Peter Lancaster, Paul
Halmos, Franz Hohn, Nick Rose, and Richard Bellman—thanks for lighting the
path. I want to oﬀer particular thanks to Richard J. Painter and Franklin A.
Graybill, two exceptionally ﬁne teachers, for giving a rough Colorado farm boy
a chance to pursue his dreams. Finally, neither this book nor anything else I
have done in my career would have been possible without the love, help, and
unwavering support from Bethany, my friend, partner, and wife. Her multiple
readings of the manuscript and suggestions were invaluable. I dedicate this book
to Bethany and our children, Martin and Holly, to our granddaughter, Margaret,
and to the memory of my parents, Carl and Louise Meyer.
Carl D. Meyer
April 19, 2000

CHAPTER 1
Linear
Equations
1.1
INTRODUCTION
A fundamental problem that surfaces in all mathematical sciences is that of
analyzing and solving m algebraic equations in n unknowns. The study of a
system of simultaneous linear equations is in a natural and indivisible alliance
with the study of the rectangular array of numbers deﬁned by the coeﬃcients of
the equations. This link seems to have been made at the outset.
The earliest recorded analysis of simultaneous equations is found in the
ancient Chinese book Chiu-chang Suan-shu (Nine Chapters on Arithmetic), es-
timated to have been written some time around 200 B.C. In the beginning of
Chapter VIII, there appears a problem of the following form.
Three sheafs of a good crop, two sheafs of a mediocre crop, and
one sheaf of a bad crop are sold for 39 dou. Two sheafs of
good, three mediocre, and one bad are sold for 34 dou; and one
good, two mediocre, and three bad are sold for 26 dou. What is
the price received for each sheaf of a good crop, each sheaf of a
mediocre crop, and each sheaf of a bad crop?
Today, this problem would be formulated as three equations in three un-
knowns by writing
3x + 2y + z = 39,
2x + 3y + z = 34,
x + 2y + 3z = 26,
where x, y, and z represent the price for one sheaf of a good, mediocre, and
bad crop, respectively. The Chinese saw right to the heart of the matter. They
placed the coeﬃcients (represented by colored bamboo rods) of this system in

2
Chapter 1
Linear Equations
a square array on a “counting board” and then manipulated the lines of the
array according to prescribed rules of thumb. Their counting board techniques
and rules of thumb found their way to Japan and eventually appeared in Europe
with the colored rods having been replaced by numerals and the counting board
replaced by pen and paper. In Europe, the technique became known as Gaussian
elimination in honor of the German mathematician Carl Gauss,
1 whose extensive
use of it popularized the method.
Because this elimination technique is fundamental, we begin the study of
our subject by learning how to apply this method in order to compute solutions
for linear equations. After the computational aspects have been mastered, we
will turn to the more theoretical facets surrounding linear systems.
1
Carl Friedrich Gauss (1777–1855) is considered by many to have been the greatest mathemati-
cian who has ever lived, and his astounding career requires several volumes to document. He
was referred to by his peers as the “prince of mathematicians.” Upon Gauss’s death one of
them wrote that “His mind penetrated into the deepest secrets of numbers, space, and nature;
He measured the course of the stars, the form and forces of the Earth; He carried within himself
the evolution of mathematical sciences of a coming century.” History has proven this remark
to be true.

1.2 Gaussian Elimination and Matrices
3
1.2
GAUSSIAN ELIMINATION AND MATRICES
The problem is to calculate, if possible, a common solution for a system of m
linear algebraic equations in n unknowns
a11x1 +
a12x2 + · · · +
a1nxn =
b1,
a21x1 +
a22x2 + · · · +
a2nxn =
b2,
...
am1x1 + am2x2 + · · · + amnxn = bm,
where the xi ’s are the unknowns and the aij ’s and the bi ’s are known constants.
The aij ’s are called the coeﬃcients of the system, and the set of bi ’s is referred
to as the right-hand side of the system. For any such system, there are exactly
three possibilities for the set of solutions.
Three Possibilities
•
UNIQUE SOLUTION:
There is one and only one set of values
for the xi ’s that satisﬁes all equations simultaneously.
•
NO SOLUTION:
There is no set of values for the xi ’s that
satisﬁes all equations simultaneously—the solution set is empty.
•
INFINITELY MANY SOLUTIONS:
There are inﬁnitely
many diﬀerent sets of values for the xi ’s that satisfy all equations
simultaneously. It is not diﬃcult to prove that if a system has more
than one solution, then it has inﬁnitely many solutions. For example,
it is impossible for a system to have exactly two diﬀerent solutions.
Part of the job in dealing with a linear system is to decide which one of these
three possibilities is true. The other part of the task is to compute the solution
if it is unique or to describe the set of all solutions if there are many solutions.
Gaussian elimination is a tool that can be used to accomplish all of these goals.
Gaussian elimination is a methodical process of systematically transform-
ing one system into another simpler, but equivalent, system (two systems are
called equivalent if they possess equal solution sets) by successively eliminating
unknowns and eventually arriving at a system that is easily solvable. The elimi-
nation process relies on three simple operations by which to transform one system
to another equivalent system. To describe these operations, let Ek denote the
kth equation
Ek :
ak1x1 + ak2x2 + · · · + aknxn = bk

4
Chapter 1
Linear Equations
and write the system as
S =







E1
E2
...
Em







.
For a linear system S , each of the following three elementary operations
results in an equivalent system S′.
(1)
Interchange the ith and jth equations. That is, if
S =























E1
...
Ei
...
Ej
...
Em























,
then
S′ =























E1
...
Ej
...
Ei
...
Em























.
(1.2.1)
(2)
Replace the ith equation by a nonzero multiple of itself. That is,
S′ =













E1
...
αEi
...
Em













,
where α ̸= 0.
(1.2.2)
(3)
Replace the jth equation by a combination of itself plus a multiple of
the ith equation. That is,
S′ =























E1
...
Ei
...
Ej + αEi
...
Em























.
(1.2.3)

1.2 Gaussian Elimination and Matrices
5
Providing explanations for why each of these operations cannot change the
solution set is left as an exercise.
The most common problem encountered in practice is the one in which there
are n equations as well as n unknowns—called a square system—for which
there is a unique solution. Since Gaussian elimination is straightforward for this
case, we begin here and later discuss the other possibilities. What follows is a
detailed description of Gaussian elimination as applied to the following simple
(but typical) square system:
2x +
y + z =
1,
6x + 2y + z = −1,
−2x + 2y + z =
7.
(1.2.4)
At each step, the strategy is to focus on one position, called the pivot po-
sition, and to eliminate all terms below this position using the three elementary
operations. The coeﬃcient in the pivot position is called a pivotal element (or
simply a pivot), while the equation in which the pivot lies is referred to as the
pivotal equation. Only nonzero numbers are allowed to be pivots. If a coef-
ﬁcient in a pivot position is ever 0, then the pivotal equation is interchanged
with an equation below the pivotal equation to produce a nonzero pivot. (This is
always possible for square systems possessing a unique solution.) Unless it is 0,
the ﬁrst coeﬃcient of the ﬁrst equation is taken as the ﬁrst pivot. For example,
the circled ⃝
2
in the system below is the pivot for the ﬁrst step:
⃝
2 x +
y + z =
1,
6x + 2y + z = −1,
−2x + 2y + z =
7.
Step 1. Eliminate all terms below the ﬁrst pivot.
•
Subtract three times the ﬁrst equation from the second so as to produce the
equivalent system:
⃝
2 x +
y +
z =
1,
−
y −2z = −4
(E2 −3E1),
−2x + 2y +
z =
7.
•
Add the ﬁrst equation to the third equation to produce the equivalent system:
⃝
2 x +
y +
z =
1,
−
y −2z = −4,
3y + 2z =
8
(E3 + E1).

6
Chapter 1
Linear Equations
Step 2. Select a new pivot.
•
For the time being, select a new pivot by moving down and to the right.
2 If
this coeﬃcient is not 0, then it is the next pivot. Otherwise, interchange
with an equation below this position so as to bring a nonzero number into
this pivotal position. In our example, −1 is the second pivot as identiﬁed
below:
2x +
y +
z =
1,
⃝
-1 y −2z = −4,
3y + 2z =
8.
Step 3. Eliminate all terms below the second pivot.
•
Add three times the second equation to the third equation so as to produce
the equivalent system:
2x +
y +
z =
1,
⃝
-1 y −2z = −4,
−4z = −4
(E3 + 3E2).
(1.2.5)
•
In general, at each step you move down and to the right to select the next
pivot, then eliminate all terms below the pivot until you can no longer pro-
ceed. In this example, the third pivot is −4, but since there is nothing below
the third pivot to eliminate, the process is complete.
At this point, we say that the system has been triangularized. A triangular
system is easily solved by a simple method known as back substitution in which
the last equation is solved for the value of the last unknown and then substituted
back into the penultimate equation, which is in turn solved for the penultimate
unknown, etc., until each unknown has been determined. For our example, solve
the last equation in (1.2.5) to obtain
z = 1.
Substitute z = 1 back into the second equation in (1.2.5) and determine
y = 4 −2z = 4 −2(1) = 2.
2
The strategy of selecting pivots in numerical computation is usually a bit more complicated
than simply using the next coeﬃcient that is down and to the right. Use the down-and-right
strategy for now, and later more practical strategies will be discussed.

1.2 Gaussian Elimination and Matrices
7
Finally, substitute z = 1 and y = 2 back into the ﬁrst equation in (1.2.5) to
get
x = 1
2(1 −y −z) = 1
2(1 −2 −1) = −1,
which completes the solution.
It should be clear that there is no reason to write down the symbols such
as “ x, ” “ y, ” “ z, ” and “ = ” at each step since we are only manipulating the
coeﬃcients. If such symbols are discarded, then a system of linear equations
reduces to a rectangular array of numbers in which each horizontal line represents
one equation. For example, the system in (1.2.4) reduces to the following array:


2
1
1
1
6
2
1
−1
−2
2
1
7

.
(The line emphasizes where = appeared.)
The array of coeﬃcients—the numbers on the left-hand side of the vertical
line—is called the coeﬃcient matrix for the system. The entire array—the
coeﬃcient matrix augmented by the numbers from the right-hand side of the
system—is called the augmented matrix associated with the system. If the
coeﬃcient matrix is denoted by A and the right-hand side is denoted by b ,
then the augmented matrix associated with the system is denoted by [A|b].
Formally, a scalar is either a real number or a complex number, and a
matrix is a rectangular array of scalars. It is common practice to use uppercase
boldface letters to denote matrices and to use the corresponding lowercase letters
with two subscripts to denote individual entries in a matrix. For example,
A =




a11
a12
· · ·
a1n
a21
a22
· · ·
a2n
...
...
...
...
am1
am2
· · ·
amn



.
The ﬁrst subscript on an individual entry in a matrix designates the row (the
horizontal line), and the second subscript denotes the column (the vertical line)
that the entry occupies. For example, if
A =


2
1
3
4
8
6
5
−9
−3
8
3
7

,
then
a11 = 2, a12 = 1, . . . , a34 = 7.
(1.2.6)
A submatrix of a given matrix A is an array obtained by deleting any
combination of rows and columns from A. For example, B =

2
4
−3
7

is a
submatrix of the matrix A in (1.2.6) because B is the result of deleting the
second row and the second and third columns of A.

8
Chapter 1
Linear Equations
Matrix A is said to have shape or size m × n —pronounced “m by n”—
whenever A has exactly m rows and n columns. For example, the matrix
in (1.2.6) is a 3 × 4 matrix. By agreement, 1 × 1 matrices are identiﬁed with
scalars and vice versa. To emphasize that matrix A has shape m × n, subscripts
are sometimes placed on A as Am×n. Whenever m = n (i.e., when A has the
same number of rows as columns), A is called a square matrix. Otherwise, A
is said to be rectangular. Matrices consisting of a single row or a single column
are often called row vectors or column vectors, respectively.
The symbol Ai∗is used to denote the ith row, while A∗j denotes the jth
column of matrix A . For example, if A is the matrix in (1.2.6), then
A2∗= ( 8
6
5
−9 )
and
A∗2 =


1
6
8

.
For a linear system of equations
a11x1 +
a12x2 + · · · +
a1nxn =
b1,
a21x1 +
a22x2 + · · · +
a2nxn =
b2,
...
am1x1 + am2x2 + · · · + amnxn = bm,
Gaussian elimination can be executed on the associated augmented matrix [A|b]
by performing elementary operations to the rows of [A|b]. These row operations
correspond to the three elementary operations (1.2.1), (1.2.2), and (1.2.3) used
to manipulate linear systems. For an m × n matrix
M =












M1∗
...
Mi∗
...
Mj∗
...
Mm∗












,
the three types of elementary row operations on M are as follows.
•
Type I:
Interchange rows i and j to produce












M1∗
...
Mj∗
...
Mi∗
...
Mm∗












.
(1.2.7)

1.2 Gaussian Elimination and Matrices
9
•
Type II:
Replace row i by a nonzero multiple of itself to produce







M1∗
...
αMi∗
...
Mm∗







,
where
α ̸= 0.
(1.2.8)
•
Type III:
Replace row j by a combination of itself plus a multiple of row
i to produce












M1∗
...
Mi∗
...
Mj∗+ αMi∗
...
Mm∗












.
(1.2.9)
To solve the system (1.2.4) by using elementary row operations, start with
the associated augmented matrix [A|b] and triangularize the coeﬃcient matrix
A by performing exactly the same sequence of row operations that corresponds
to the elementary operations executed on the equations themselves:


⃝
2
1
1
1
6
2
1
−1
−2
2
1
7

R2 −3R1
R3 + R1
−→


2
1
1
1
0
⃝
-1
−2
−4
0
3
2
8


R3 + 3R2
−→


2
1
1
1
0
−1
−2
−4
0
0
−4
−4

.
The ﬁnal array represents the triangular system
2x + y +
z =
1,
−y −2z = −4,
−4z = −4
that is solved by back substitution as described earlier. In general, if an n × n
system has been triangularized to the form




t11
t12
· · ·
t1n
c1
0
t22
· · ·
t2n
c2
...
...
...
...
...
0
0
· · ·
tnn
cn




(1.2.10)
in which each tii ̸= 0 (i.e., there are no zero pivots), then the general algorithm
for back substitution is as follows.

10
Chapter 1
Linear Equations
Algorithm for Back Substitution
Determine the xi ’s from (1.2.10) by ﬁrst setting xn = cn/tnn and then
recursively computing
xi = 1
tii
(ci −ti,i+1xi+1 −ti,i+2xi+2 −· · · −tinxn)
for i = n −1, n −2, . . . , 2, 1.
One way to gauge the eﬃciency of an algorithm is to count the number of
arithmetical operations required.
3 For a variety of reasons, no distinction is made
between additions and subtractions, and no distinction is made between multipli-
cations and divisions. Furthermore, multiplications/divisions are usually counted
separately from additions/subtractions. Even if you do not work through the de-
tails, it is important that you be aware of the operational counts for Gaussian
elimination with back substitution so that you will have a basis for comparison
when other algorithms are encountered.
Gaussian Elimination Operation Counts
Gaussian elimination with back substitution applied to an n × n system
requires
n3
3 + n2 −n
3
multiplications/divisions
and
n3
3 + n2
2 −5n
6
additions/subtractions.
As n grows, the n3/3 term dominates each of these expressions. There-
fore, the important thing to remember is that Gaussian elimination with
back substitution on an n × n system requires about n3/3 multiplica-
tions/divisions and about the same number of additions/subtractions.
3
Operation counts alone may no longer be as important as they once were in gauging the ef-
ﬁciency of an algorithm. Older computers executed instructions sequentially, whereas some
contemporary machines are capable of executing instructions in parallel so that diﬀerent nu-
merical tasks can be performed simultaneously. An algorithm that lends itself to parallelism
may have a higher operational count but might nevertheless run faster on a parallel machine
than an algorithm with a lesser operational count that cannot take advantage of parallelism.

1.2 Gaussian Elimination and Matrices
11
Example 1.2.1
Problem: Solve the following system using Gaussian elimination with back sub-
stitution:
v −
w =
3,
−2u + 4v −
w =
1,
−2u + 5v −4w = −2.
Solution: The associated augmented matrix is


0
1
−1
3
−2
4
−1
1
−2
5
−4
−2

.
Since the ﬁrst pivotal position contains 0, interchange rows one and two before
eliminating below the ﬁrst pivot:


⃝
0
1
−1
3
−2
4
−1
1
−2
5
−4
−2


Interchange R1 and R2
−−−−−−−−→


⃝
-2
4
−1
1
0
1
−1
3
−2
5
−4
−2


R3 −R1
−→


−2
4
−1
1
0
⃝
1
−1
3
0
1
−3
−3


R3 −R2
−→


−2
4
−1
1
0
1
−1
3
0
0
−2
−6

.
Back substitution yields
w = −6
−2 = 3,
v = 3 + w = 3 + 3 = 6,
u = 1
−2 (1 −4v + w) = 1
−2 (1 −24 + 3) = 10.
Exercises for section 1.2
1.2.1. Use Gaussian elimination with back substitution to solve the following
system:
x1 + x2 + x3 = 1,
x1 + 2x2 + 2x3 = 1,
x1 + 2x2 + 3x3 = 1.

12
Chapter 1
Linear Equations
1.2.2. Apply Gaussian elimination with back substitution to the following sys-
tem:
2x1 −x2
= 0,
−x1 + 2x2 −x3 = 0,
−x2 + x3 = 1.
1.2.3. Use Gaussian elimination with back substitution to solve the following
system:
4x2 −3x3 = 3,
−x1 + 7x2 −5x3 = 4,
−x1 + 8x2 −6x3 = 5.
1.2.4. Solve the following system:
x1 + x2 + x3 + x4 = 1,
x1 + x2 + 3x3 + 3x4 = 3,
x1 + x2 + 2x3 + 3x4 = 3,
x1 + 3x2 + 3x3 + 3x4 = 4.
1.2.5. Consider the following three systems where the coeﬃcients are the same
for each system, but the right-hand sides are diﬀerent (this situation
occurs frequently):
4x −8y + 5z = 1 0 0,
4x −7y + 4z = 0 1 0,
3x −4y + 2z = 0 0 1.
Solve all three systems at one time by performing Gaussian elimination
on an augmented matrix of the form

A
 b1
 b2
 b3

.
1.2.6. Suppose that matrix B is obtained by performing a sequence of row
operations on matrix A . Explain why A can be obtained by performing
row operations on B .
1.2.7. Find angles α, β, and γ such that
2 sin α −
cos β + 3 tan γ = 3,
4 sin α + 2 cos β −2 tan γ = 2,
6 sin α −3 cos β +
tan γ = 9,
where 0 ≤α ≤2π, 0 ≤β ≤2π, and 0 ≤γ < π.

1.2 Gaussian Elimination and Matrices
13
1.2.8. The following system has no solution:
−x1 + 3x2 −2x3 = 1,
−x1 + 4x2 −3x3 = 0,
−x1 + 5x2 −4x3 = 0.
Attempt to solve this system using Gaussian elimination and explain
what occurs to indicate that the system is impossible to solve.
1.2.9. Attempt to solve the system
−x1 + 3x2 −2x3 = 4,
−x1 + 4x2 −3x3 = 5,
−x1 + 5x2 −4x3 = 6,
using Gaussian elimination and explain why this system must have in-
ﬁnitely many solutions.
1.2.10. By solving a 3 × 3 system, ﬁnd the coeﬃcients in the equation of the
parabola y = α+βx+γx2 that passes through the points (1, 1), (2, 2),
and (3, 0).
1.2.11. Suppose that 100 insects are distributed in an enclosure consisting of
four chambers with passageways between them as shown below.
#1
#2
#3
#4
At the end of one minute, the insects have redistributed themselves.
Assume that a minute is not enough time for an insect to visit more than
one chamber and that at the end of a minute 40% of the insects in each
chamber have not left the chamber they occupied at the beginning of
the minute. The insects that leave a chamber disperse uniformly among
the chambers that are directly accessible from the one they initially
occupied—e.g., from #3, half move to #2 and half move to #4.

14
Chapter 1
Linear Equations
(a)
If at the end of one minute there are 12, 25, 26, and 37 insects
in chambers #1, #2, #3, and #4, respectively, determine what
the initial distribution had to be.
(b)
If the initial distribution is 20, 20, 20, 40, what is the distribution
at the end of one minute?
1.2.12. Show that the three types of elementary row operations discussed on
p. 8 are not independent by showing that the interchange operation
(1.2.7) can be accomplished by a sequence of the other two types of row
operations given in (1.2.8) and (1.2.9).
1.2.13. Suppose that [A|b] is the augmented matrix associated with a linear
system. You know that performing row operations on [A|b] does not
change the solution of the system. However, no mention of column oper-
ations was ever made because column operations can alter the solution.
(a)
Describe the eﬀect on the solution of a linear system when
columns A∗j and A∗k are interchanged.
(b)
Describe the eﬀect when column A∗j is replaced by αA∗j for
α ̸= 0.
(c)
Describe the eﬀect when A∗j is replaced by A∗j + αA∗k.
Hint: Experiment with a 2 × 2 or 3 × 3 system.
1.2.14. Consider the n × n Hilbert matrix deﬁned by
H =













1
1
2
1
3
· · ·
1
n
1
2
1
3
1
4
· · ·
1
n+1
1
3
1
4
1
5
· · ·
1
n+2
...
...
...
· · ·
...
1
n
1
n+1
1
n+2
· · ·
1
2n−1













.
Express the individual entries hij in terms of i and j.
1.2.15. Verify that the operation counts given in the text for Gaussian elimi-
nation with back substitution are correct for a general 3 × 3 system.
If you are up to the challenge, try to verify these counts for a general
n × n system.
1.2.16. Explain why a linear system can never have exactly two diﬀerent solu-
tions. Extend your argument to explain the fact that if a system has more
than one solution, then it must have inﬁnitely many diﬀerent solutions.

1.3 Gauss–Jordan Method
15
1.3
GAUSS–JORDAN METHOD
The purpose of this section is to introduce a variation of Gaussian elimination
that is known as the Gauss–Jordan method.
4 The two features that dis-
tinguish the Gauss–Jordan method from standard Gaussian elimination are as
follows.
•
At each step, the pivot element is forced to be 1.
•
At each step, all terms above the pivot as well as all terms below the pivot
are eliminated.
In other words, if




a11
a12
· · ·
a1n
b1
a21
a22
· · ·
a2n
b2
...
...
...
...
...
an1
an2
· · ·
ann
bn




is the augmented matrix associated with a linear system, then elementary row
operations are used to reduce this matrix to




1
0
· · ·
0
s1
0
1
· · ·
0
s2
...
...
...
...
...
0
0
· · ·
1
sn



.
The solution then appears in the last column (i.e., xi = si ) so that this procedure
circumvents the need to perform back substitution.
Example 1.3.1
Problem: Apply the Gauss–Jordan method to solve the following system:
2x1 + 2x2 + 6x3 =
4,
2x1 +
x2 + 7x3 =
6,
−2x1 −6x2 −7x3 = −1.
4
Although there has been some confusion as to which Jordan should receive credit for this
algorithm, it now seems clear that the method was in fact introduced by a geodesist named
Wilhelm Jordan (1842–1899) and not by the more well known mathematician Marie Ennemond
Camille Jordan (1838–1922), whose name is often mistakenly associated with the technique, but
who is otherwise correctly credited with other important topics in matrix analysis, the “Jordan
canonical form” being the most notable. Wilhelm Jordan was born in southern Germany,
educated in Stuttgart, and was a professor of geodesy at the technical college in Karlsruhe.
He was a proliﬁc writer, and he introduced his elimination scheme in the 1888 publication
Handbuch der Vermessungskunde. Interestingly, a method similar to W. Jordan’s variation
of Gaussian elimination seems to have been discovered and described independently by an
obscure Frenchman named Clasen, who appears to have published only one scientiﬁc article,
which appeared in 1888—the same year as W. Jordan’s Handbuch appeared.

16
Chapter 1
Linear Equations
Solution: The sequence of operations is indicated in parentheses and the pivots
are circled.


⃝
2
2
6
4
2
1
7
6
−2
−6
−7
−1


R1/2
−→


⃝
1
1
3
2
2
1
7
6
−2
−6
−7
−1

R2 −2R1
R3 + 2R1
−→


⃝
1
1
3
2
0
−1
1
2
0
−4
−1
3

(−R2) −→


1
1
3
2
0
⃝
1
−1
−2
0
−4
−1
3


R1 −R2
R3 + 4R2
−→


1
0
4
4
0
⃝
1
−1
−2
0
0
−5
−5


−R3/5
−→


1
0
4
4
0
1
−1
−2
0
0
⃝
1
1


R1 −4R3
R2 + R3
−→


1
0
0
0
0
1
0
−1
0
0
⃝
1
1

.
Therefore, the solution is


x1
x2
x3

=


0
−1
1

.
On the surface it may seem that there is little diﬀerence between the Gauss–
Jordan method and Gaussian elimination with back substitution because elimi-
nating terms above the pivot with Gauss–Jordan seems equivalent to performing
back substitution. But this is not correct. Gauss–Jordan requires more arithmetic
than Gaussian elimination with back substitution.
Gauss–Jordan Operation Counts
For an n × n system, the Gauss–Jordan procedure requires
n3
2 + n2
2
multiplications/divisions
and
n3
2 −n
2
additions/subtractions.
In other words, the Gauss–Jordan method requires about n3/2 multipli-
cations/divisions and about the same number of additions/subtractions.
Recall from the previous section that Gaussian elimination with back sub-
stitution requires only about n3/3 multiplications/divisions and about the same

1.3 Gauss–Jordan Method
17
number of additions/subtractions. Compare this with the n3/2 factor required
by the Gauss–Jordan method, and you can see that Gauss–Jordan requires about
50% more eﬀort than Gaussian elimination with back substitution. For small sys-
tems of the textbook variety (e.g., n = 3 ), these comparisons do not show a great
deal of diﬀerence. However, in practical work, the systems that are encountered
can be quite large, and the diﬀerence between Gauss–Jordan and Gaussian elim-
ination with back substitution can be signiﬁcant. For example, if n = 100, then
n3/3 is about 333,333, while n3/2 is 500,000, which is a diﬀerence of 166,667
multiplications/divisions as well as that many additions/subtractions.
Although the Gauss–Jordan method is not recommended for solving linear
systems that arise in practical applications, it does have some theoretical advan-
tages. Furthermore, it can be a useful technique for tasks other than computing
solutions to linear systems. We will make use of the Gauss–Jordan procedure
when matrix inversion is discussed—this is the primary reason for introducing
Gauss–Jordan.
Exercises for section 1.3
1.3.1. Use the Gauss–Jordan method to solve the following system:
4x2 −3x3 = 3,
−x1 + 7x2 −5x3 = 4,
−x1 + 8x2 −6x3 = 5.
1.3.2. Apply the Gauss–Jordan method to the following system:
x1 + x2 + x3 + x4 = 1,
x1 + 2x2 + 2x3 + 2x4 = 0,
x1 + 2x2 + 3x3 + 3x4 = 0,
x1 + 2x2 + 3x3 + 4x4 = 0.
1.3.3. Use the Gauss–Jordan method to solve the following three systems at
the same time.
2x1 −x2
= 1 0 0,
−x1 + 2x2 −x3 = 0 1 0,
−x2 + x3 = 0 0 1.
1.3.4. Verify that the operation counts given in the text for the Gauss–Jordan
method are correct for a general 3 × 3 system. If you are up to the
challenge, try to verify these counts for a general n × n system.

18
Chapter 1
Linear Equations
1.4
TWO-POINT BOUNDARY VALUE PROBLEMS
It was stated previously that linear systems that arise in practice can become
quite large in size. The purpose of this section is to understand why this often
occurs and why there is frequently a special structure to the linear systems that
come from practical applications.
Given an interval [a, b] and two numbers α and β, consider the general
problem of trying to ﬁnd a function y(t) that satisﬁes the diﬀerential equation
u(t)y′′(t)+v(t)y′(t)+w(t)y(t) = f(t),
where
y(a) = α and y(b) = β. (1.4.1)
The functions u, v, w, and f are assumed to be known functions on [a, b].
Because the unknown function y(t) is speciﬁed at the boundary points a and
b, problem (1.4.1) is known as a two-point boundary value problem. Such
problems abound in nature and are frequently very hard to handle because it is
often not possible to express y(t) in terms of elementary functions. Numerical
methods are usually employed to approximate y(t) at discrete points inside
[a, b]. Approximations are produced by subdividing the interval [a, b] into n+1
equal subintervals, each of length h = (b −a)/(n + 1) as shown below.
h
h
h


 





· · ·
· · ·
t0 = a
t1 = a + h
t2 = a + 2h
tn = a + nh
tn+1 = b
Derivative approximations at the interior nodes (grid points) ti = a + ih are
made by using Taylor series expansions y(t) = ∞
k=0 y(k)(ti)(t−ti)k/k! to write
y(ti + h) = y(ti) + y′(ti)h + y′′(ti)h2
2!
+ y′′′(ti)h3
3!
+ · · · ,
y(ti −h) = y(ti) −y′(ti)h + y′′(ti)h2
2!
−y′′′(ti)h3
3!
+ · · · ,
(1.4.2)
and then subtracting and adding these expressions to produce
y′(ti) = y(ti + h) −y(ti −h)
2h
+ O(h3)
and
y′′(ti) = y(ti −h) −2y(ti) + y(ti + h)
h2
+ O(h4),
where O(hp) denotes
5 terms containing pth and higher powers of h. The
5
Formally, a function f(h) is O(hp) if f(h)/hp remains bounded as h →0, but f(h)/hq
becomes unbounded if q > p. This means that f goes to zero as fast as hp goes to zero.

1.4 Two-Point Boundary Value Problems
19
resulting approximations
y′(ti) ≈y(ti+h) −y(ti−h)
2h
and y′′(ti) ≈y(ti−h) −2y(ti) + y(ti+h)
h2
(1.4.3)
are called centered diﬀerence approximations, and they are preferred over
less accurate one-sided approximations such as
y′(ti) ≈y(ti + h) −y(ti)
h
or
y′(ti) ≈y(t) −y(t −h)
h
.
The value h = (b −a)/(n + 1) is called the step size. Smaller step sizes pro-
duce better derivative approximations, so obtaining an accurate solution usually
requires a small step size and a large number of grid points. By evaluating the
centered diﬀerence approximations at each grid point and substituting the result
into the original diﬀerential equation (1.4.1), a system of n linear equations in
n unknowns is produced in which the unknowns are the values y(ti). A simple
example can serve to illustrate this point.
Example 1.4.1
Suppose that f(t) is a known function and consider the two-point boundary
value problem
y′′(t) = f(t)
on [0, 1] with y(0) = y(1) = 0.
The goal is to approximate the values of y at n equally spaced grid points
ti interior to [0, 1]. The step size is therefore h = 1/(n + 1). For the sake of
convenience, let yi = y(ti) and fi = f(ti). Use the approximation
yi−1 −2yi + yi+1
h2
≈y′′(ti) = fi
along with y0 = 0 and yn+1 = 0 to produce the system of equations
−yi−1 + 2yi −yi+1 ≈−h2fi
for i = 1, 2, . . . , n.
(The signs are chosen to make the 2’s positive to be consistent with later devel-
opments.) The augmented matrix associated with this system is shown below:










2
−1
0
· · ·
0
0
0
−h2f1
−1
2
−1
· · ·
0
0
0
−h2f2
0
−1
2
· · ·
0
0
0
−h2f3
...
...
...
...
...
...
...
...
0
0
0
· · ·
2
−1
0
−h2fn−2
0
0
0
· · ·
−1
2
−1
−h2fn−1
0
0
0
· · ·
0
−1
2
−h2fn










.
By solving this system, approximate values of the unknown function y at the
grid points ti are obtained. Larger values of n produce smaller values of h and
hence better approximations to the exact values of the yi ’s.

20
Chapter 1
Linear Equations
Notice the pattern of the entries in the coeﬃcient matrix in the above ex-
ample. The nonzero elements occur only on the subdiagonal, main-diagonal, and
superdiagonal lines—such a system (or matrix) is said to be tridiagonal. This
is characteristic in the sense that when ﬁnite diﬀerence approximations are ap-
plied to the general two-point boundary value problem, a tridiagonal system is
the result.
Tridiagonal systems are particularly nice in that they are inexpensive to
solve. When Gaussian elimination is applied, only two multiplications/divisions
are needed at each step of the triangularization process because there is at most
only one nonzero entry below and to the right of each pivot. Furthermore, Gaus-
sian elimination preserves all of the zero entries that were present in the original
tridiagonal system. This makes the back substitution process cheap to execute
because there are at most only two multiplications/divisions required at each
substitution step. Exercise 3.10.6 contains more details.
Exercises for section 1.4
1.4.1. Divide the interval [0, 1] into ﬁve equal subintervals, and apply the ﬁnite
diﬀerence method in order to approximate the solution of the two-point
boundary value problem
y′′(t) = 125t,
y(0) = y(1) = 0
at the four interior grid points. Compare your approximate values at
the grid points with the exact solution at the grid points. Note: You
should not expect very accurate approximations with only four interior
grid points.
1.4.2. Divide [0, 1] into n+1 equal subintervals, and apply the ﬁnite diﬀerence
approximation method to derive the linear system associated with the
two-point boundary value problem
y′′(t) −y′(t) = f(t),
y(0) = y(1) = 0.
1.4.3. Divide [0, 1] into ﬁve equal subintervals, and approximate the solution
to
y′′(t) −y′(t) = 125t,
y(0) = y(1) = 0
at the four interior grid points. Compare the approximations with the
exact values at the grid points.

1.5 Making Gaussian Elimination Work
21
1.5
MAKING GAUSSIAN ELIMINATION WORK
Now that you understand the basic Gaussian elimination technique, it’s time
to turn it into a practical algorithm that can be used for realistic applications.
For pencil and paper computations where you are doing exact arithmetic, the
strategy is to keep things as simple as possible (like avoiding messy fractions) in
order to minimize those “stupid arithmetic errors” we are all prone to make. But
very few problems in the real world are of the textbook variety, and practical
applications involving linear systems usually demand the use of a computer.
Computers don’t care about messy fractions, and they don’t introduce errors of
the “stupid” variety. Computers produce a more predictable kind of error, called
roundoﬀerror, and it’s important
6 to spend a little time up front to understand
this kind of error and its eﬀects on solving linear systems.
Numerical computation in digital computers is performed by approximating
the inﬁnite set of real numbers with a ﬁnite set of numbers as described below.
Floating-Point Numbers
A t -digit, base-β ﬂoating-point number has the form
f = ±.d1d2 · · · dt × βϵ
with
d1 ̸= 0,
where the base β, the exponent ϵ, and the digits 0 ≤di ≤β −1
are integers. For internal machine representation, β = 2 (binary rep-
resentation) is standard, but for pencil-and-paper examples it’s more
convenient to use β = 10. The value of t, called the precision, and
the exponent ϵ can vary with the choice of hardware and software.
Floating-point numbers are just adaptations of the familiar concept of sci-
entiﬁc notation where β = 10, which will be the value used in our examples. For
any ﬁxed set of values for t, β, and ϵ, the corresponding set F of ﬂoating-
point numbers is necessarily a ﬁnite set, so some real numbers can’t be found
in F. There is more than one way of approximating real numbers with ﬂoating-
point numbers. For the remainder of this text, the following common rounding
convention is adopted. Given a real number x, the ﬂoating-point approximation
fl(x) is deﬁned to be the nearest element in F to x, and in case of a tie we
round away from 0. This means that for t-digit precision with β = 10, we need
6
The computer has been the single most important scientiﬁc and technological development
of our century and has undoubtedly altered the course of science for all future time. The
prospective young scientist or engineer who passes through a contemporary course in linear
algebra and matrix theory and fails to learn at least the elementary aspects of what is involved
in solving a practical linear system with a computer is missing a fundamental tool of applied
mathematics.

22
Chapter 1
Linear Equations
to look at digit dt+1 in x = .d1d2 · · · dtdt+1 · · · × 10ϵ (making sure d1 ̸= 0) and
then set
fl(x) =
 .d1d2 · · · dt × 10ϵ
if dt+1 < 5,
([.d1d2 · · · dt] + 10−t) × 10ϵ
if dt+1 ≥5.
For example, in 2 -digit, base-10 ﬂoating-point arithmetic,
fl (3/80) = fl(.0375) = fl(.375 × 10−1) = .38 × 10−1 = .038.
By considering η = 1/3 and ξ = 3 with t -digit base-10 arithmetic, it’s
easy to see that
fl(η + ξ) ̸= fl(η) + fl(ξ)
and
fl(ηξ) ̸= fl(η)fl(ξ).
Furthermore, several familiar rules of real arithmetic do not hold for ﬂoating-
point arithmetic—associativity is one outstanding example. This, among other
reasons, makes the analysis of ﬂoating-point computation diﬃcult. It also means
that you must be careful when working the examples and exercises in this text
because although most calculators and computers can be instructed to display
varying numbers of digits, most have a ﬁxed internal precision with which all
calculations are made before numbers are displayed, and this internal precision
cannot be altered. Almost certainly, the internal precision of your calculator or
computer is greater than the precision called for by the examples and exercises
in this text. This means that each time you perform a t-digit calculation, you
should manually round the result to t signiﬁcant digits and reenter the rounded
number before proceeding to the next calculation. In other words, don’t “chain”
operations in your calculator or computer.
To understand how to execute Gaussian elimination using ﬂoating-point
arithmetic, let’s compare the use of exact arithmetic with the use of 3-digit
base-10 arithmetic to solve the following system:
47x + 28y = 19,
89x + 53y = 36.
Using Gaussian elimination with exact arithmetic, we multiply the ﬁrst equation
by the multiplier m = 89/47 and subtract the result from the second equation
to produce

47
28
19
0
−1/47
1/47

.
Back substitution yields the exact solution
x = 1
and
y = −1.
Using 3-digit arithmetic, the multiplier is
fl(m) = fl
89
47

= .189 × 101 = 1.89.

1.5 Making Gaussian Elimination Work
23
Since
fl

fl(m)fl(47)

= fl(1.89 × 47) = .888 × 102 = 88.8,
fl

fl(m)fl(28)

= fl(1.89 × 28) = .529 × 102 = 52.9,
fl

fl(m)fl(19)

= fl(1.89 × 19) = .359 × 102 = 35.9,
the ﬁrst step of 3-digit Gaussian elimination is as shown below:

47
28
19
fl(89 −88.8)
fl(53 −52.9)
fl(36 −35.9)

=

47
28
19
⃝
.2
.1
.1

.
The goal is to triangularize the system—to produce a zero in the circled
(2,1)-position—but this cannot be accomplished with 3-digit arithmetic. Unless
the circled value ⃝
.2
is replaced by 0, back substitution cannot be executed.
Henceforth, we will agree simply to enter 0 in the position that we are trying
to annihilate, regardless of the value of the ﬂoating-point number that might
actually appear. The value of the position being annihilated is generally not
even computed. For example, don’t even bother computing
fl

89 −fl

fl(m)fl(47)

= fl(89 −88.8) = .2
in the above example. Hence the result of 3-digit Gaussian elimination for this
example is

47
28
19
0
.1
.1

.
Apply 3-digit back substitution to obtain the 3-digit ﬂoating-point solution
y = fl
.1
.1

= 1,
x = fl
19 −28
47

= fl
−9
47

= −.191.
The vast discrepancy between the exact solution (1, −1) and the 3-digit
solution (−.191, 1) illustrates some of the problems we can expect to encounter
while trying to solve linear systems with ﬂoating-point arithmetic. Sometimes
using a higher precision may help, but this is not always possible because on
all machines there are natural limits that make extended precision arithmetic
impractical past a certain point. Even if it is possible to increase the precision, it

24
Chapter 1
Linear Equations
may not buy you very much because there are many cases for which an increase
in precision does not produce a comparable decrease in the accumulated roundoﬀ
error. Given any particular precision (say, t ), it is not diﬃcult to provide exam-
ples of linear systems for which the computed t-digit solution is just as bad as
the one in our 3-digit example above.
Although the eﬀects of rounding can almost never be eliminated, there are
some simple techniques that can help to minimize these machine induced errors.
Partial Pivoting
At each step, search the positions on and below the pivotal position for
the coeﬃcient of maximum magnitude. If necessary perform the appro-
priate row interchange to bring this maximal coeﬃcient into the pivotal
position. Illustrated below is the third step in a typical case:





∗
∗
∗
∗
∗
∗
0
∗
∗
∗
∗
∗
0
0
⃝
S
∗
∗
∗
0
0
S
∗
∗
∗
0
0
S
∗
∗
∗




.
Search the positions in the third column marked “ S ” for the coeﬃcient
of maximal magnitude and, if necessary, interchange rows to bring this
coeﬃcient into the circled pivotal position. Simply stated, the strategy
is to maximize the magnitude of the pivot at each step by using only
row interchanges.
On the surface, it is probably not apparent why partial pivoting should
make a diﬀerence. The following example not only shows that partial pivoting
can indeed make a great deal of diﬀerence, but it also indicates what makes this
strategy eﬀective.
Example 1.5.1
It is easy to verify that the exact solution to the system
−10−4x + y = 1,
x + y = 2,
is given by
x =
1
1.0001
and
y = 1.0002
1.0001.
If 3-digit arithmetic without partial pivoting is used, then the result is

1.5 Making Gaussian Elimination Work
25

−10−4
1
1
1
1
2

R2 + 104R1 −→

−10−4
1
1
0
104
104

because
fl(1 + 104) = fl(.10001 × 105) = .100 × 105 = 104
(1.5.1)
and
fl(2 + 104) = fl(.10002 × 105) = .100 × 105 = 104.
(1.5.2)
Back substitution now produces
x = 0
and
y = 1.
Although the computed solution for y is close to the exact solution for y, the
computed solution for x is not very close to the exact solution for x —the
computed solution for x is certainly not accurate to three signiﬁcant ﬁgures as
you might hope. If 3-digit arithmetic with partial pivoting is used, then the result
is

−10−4
1
1
1
1
2

−→

1
1
2
−10−4
1
1

R2 + 10−4R1
−→

1
1
2
0
1
1

because
fl(1 + 10−4) = fl(.10001 × 101) = .100 × 101 = 1
(1.5.3)
and
fl(1 + 2 × 10−4) = fl(.10002 × 101) = .100 × 101 = 1.
(1.5.4)
This time, back substitution produces the computed solution
x = 1
and
y = 1,
which is as close to the exact solution as one can reasonably expect—the com-
puted solution agrees with the exact solution to three signiﬁcant digits.
Why did partial pivoting make a diﬀerence? The answer lies in comparing
(1.5.1) and (1.5.2) with (1.5.3) and (1.5.4).
Without partial pivoting the multiplier is 104, and this is so large that it
completely swamps the arithmetic involving the relatively smaller numbers 1
and 2 and prevents them from being taken into account. That is, the smaller
numbers 1 and 2 are “blown away” as though they were never present so that
our 3-digit computer produces the exact solution to another system, namely,

−10−4
1
1
1
0
0

,

26
Chapter 1
Linear Equations
which is quite diﬀerent from the original system. With partial pivoting the mul-
tiplier is 10−4, and this is small enough so that it does not swamp the numbers
1 and 2. In this case, the 3-digit computer produces the exact solution to the
system
 0
1
1
1
1
2

, which is close to the original system.
7
In summary, the villain in Example 1.5.1 is the large multiplier that pre-
vents some smaller numbers from being fully accounted for, thereby resulting
in the exact solution of another system that is very diﬀerent from the original
system. By maximizing the magnitude of the pivot at each step, we minimize
the magnitude of the associated multiplier thus helping to control the growth
of numbers that emerge during the elimination process. This in turn helps cir-
cumvent some of the eﬀects of roundoﬀerror. The problem of growth in the
elimination procedure is more deeply analyzed on p. 348.
When partial pivoting is used, no multiplier ever exceeds 1 in magnitude. To
see that this is the case, consider the following two typical steps in an elimination
procedure:





∗
∗
∗
∗
∗
∗
0
∗
∗
∗
∗
∗
0
0
⃝
p
∗
∗
∗
0
0
q
∗
∗
∗
0
0
r
∗
∗
∗




R4 −(q/p)R3
R5 −(r/p)R3
−→





∗
∗
∗
∗
∗
∗
0
∗
∗
∗
∗
∗
0
0
⃝
p
∗
∗
∗
0
0
0
∗
∗
∗
0
0
0
∗
∗
∗




.
The pivot is p, while q/p and r/p are the multipliers. If partial pivoting has
been employed, then |p| ≥|q| and |p| ≥|r| so that
q
p
 ≤1
and
r
p
 ≤1.
By guaranteeing that no multiplier exceeds 1 in magnitude, the possibility
of producing relatively large numbers that can swamp the signiﬁcance of smaller
numbers is much reduced, but not completely eliminated. To see that there is
still more to be done, consider the following example.
Example 1.5.2
The exact solution to the system
−10x + 105y = 105,
x +
y = 2,
7
Answering the question, “What system have I really solved (i.e., obtained the exact solution
of), and how close is this system to the original system,” is called backward error analysis,
as opposed to forward analysis in which one tries to answer the question, “How close will a
computed solution be to the exact solution?”
Backward analysis has proven to be an eﬀective
way to analyze the numerical stability of algorithms.

1.5 Making Gaussian Elimination Work
27
is given by
x =
1
1.0001
and
y = 1.0002
1.0001.
Suppose that 3-digit arithmetic with partial pivoting is used. Since | −10| > 1,
no interchange is called for and we obtain

−10
105
105
1
1
2

R2 + 10−1R1 −→

−10
105
105
0
104
104

because
fl(1 + 104) = fl(.10001 × 105) = .100 × 105 = 104
and
fl(2 + 104) = fl(.10002 × 105) = .100 × 105 = 104.
Back substitution yields
x = 0
and
y = 1,
which must be considered to be very bad—the computed 3-digit solution for y
is not too bad, but the computed 3-digit solution for x is terrible!
What is the source of diﬃculty in Example 1.5.2? This time, the multi-
plier cannot be blamed. The trouble stems from the fact that the ﬁrst equation
contains coeﬃcients that are much larger than the coeﬃcients in the second
equation. That is, there is a problem of scale due to the fact that the coeﬃcients
are of diﬀerent orders of magnitude. Therefore, we should somehow rescale the
system before attempting to solve it.
If the ﬁrst equation in the above example is rescaled to insure that the
coeﬃcient of maximum magnitude is a 1, which is accomplished by multiplying
the ﬁrst equation by 10−5, then the system given in Example 1.5.1 is obtained,
and we know from that example that partial pivoting produces a very good
approximation to the exact solution.
This points to the fact that the success of partial pivoting can hinge on
maintaining the proper scale among the coeﬃcients. Therefore, the second re-
ﬁnement needed to make Gaussian elimination practical is a reasonable scaling
strategy. Unfortunately, there is no known scaling procedure that will produce
optimum results for every possible system, so we must settle for a strategy that
will work most of the time. The strategy is to combine row scaling—multiplying
selected rows by nonzero multipliers—with column scaling—multiplying se-
lected columns of the coeﬃcient matrix A by nonzero multipliers.
Row scaling doesn’t alter the exact solution, but column scaling does—see
Exercise 1.2.13(b). Column scaling is equivalent to changing the units of the
kth unknown. For example, if the units of the kth unknown xk in [A|b] are
millimeters, and if the kth column of A is multiplied by . 001, then the kth
unknown in the scaled system [ˆA | b] is ˆxi = 1000xi, and thus the units of the
scaled unknown ˆxk become meters.

28
Chapter 1
Linear Equations
Experience has shown that the following strategy for combining row scaling
with column scaling usually works reasonably well.
Practical Scaling Strategy
1.
Choose units that are natural to the problem and do not dis-
tort the relationships between the sizes of things. These natural
units are usually self-evident, and further column scaling past
this point is not ordinarily attempted.
2.
Row scale the system [A|b] so that the coeﬃcient of maximum
magnitude in each row of A is equal to 1. That is, divide each
equation by the coeﬃcient of maximum magnitude.
Partial pivoting together with the scaling strategy described above
makes Gaussian elimination with back substitution an extremely eﬀec-
tive tool. Over the course of time, this technique has proven to be reliable
for solving a majority of linear systems encountered in practical work.
Although it is not extensively used, there is an extension of partial pivoting
known as complete pivoting which, in some special cases, can be more eﬀective
than partial pivoting in helping to control the eﬀects of roundoﬀerror.
Complete Pivoting
If [A|b] is the augmented matrix at the kth step of Gaussian elimina-
tion, then search the pivotal position together with every position in A
that is below or to the right of the pivotal position for the coeﬃcient
of maximum magnitude. If necessary, perform the appropriate row and
column interchanges to bring the coeﬃcient of maximum magnitude into
the pivotal position. Shown below is the third step in a typical situation:





∗
∗
∗
∗
∗
∗
0
∗
∗
∗
∗
∗
0
0
⃝
S
S
S
∗
0
0
S
S
S
∗
0
0
S
S
S
∗





Search the positions marked “ S ” for the coeﬃcient of maximal magni-
tude. If necessary, interchange rows and columns to bring this maximal
coeﬃcient into the circled pivotal position. Recall from Exercise 1.2.13
that the eﬀect of a column interchange in A is equivalent to permuting
(or renaming) the associated unknowns.

1.5 Making Gaussian Elimination Work
29
You should be able to see that complete pivoting should be at least as eﬀec-
tive as partial pivoting. Moreover, it is possible to construct specialized exam-
ples where complete pivoting is superior to partial pivoting—a famous example
is presented in Exercise 1.5.7. However, one rarely encounters systems of this
nature in practice. A deeper comparison between no pivoting, partial pivoting,
and complete pivoting is given on p. 348.
Example 1.5.3
Problem:
Use 3-digit arithmetic together with complete pivoting to solve the
following system:
x −
y = −2,
−9x + 10y = 12.
Solution:
Since 10 is the coeﬃcient of maximal magnitude that lies in the
search pattern, interchange the ﬁrst and second rows and then interchange the
ﬁrst and second columns:

1
−1
−2
−9
10
12

−→

−9
10
12
1
−1
−2

−→

10
−9
12
−1
1
−2

−→

10
−9
12
0
.1
−.8

.
The eﬀect of the column interchange is to rename the unknowns to ˆx and ˆy,
where ˆx = y and ˆy = x. Back substitution yields ˆy = −8 and ˆx = −6 so that
x = ˆy = −8
and
y = ˆx = −6.
In this case, the 3-digit solution and the exact solution agree. If only partial
pivoting is used, the 3-digit solution will not be as accurate. However, if scaled
partial pivoting is used, the result is the same as when complete pivoting is used.
If the cost of using complete pivoting was nearly the same as the cost of using
partial pivoting, we would always use complete pivoting. However, it is not diﬃ-
cult to show that complete pivoting approximately doubles the cost over straight
Gaussian elimination, whereas partial pivoting adds only a negligible amount.
Couple this with the fact that it is extremely rare to encounter a practical system
where scaled partial pivoting is not adequate while complete pivoting is, and it
is easy to understand why complete pivoting is seldom used in practice. Gaus-
sian elimination with scaled partial pivoting is the preferred method for dense
systems (i.e., not a lot of zeros) of moderate size.

30
Chapter 1
Linear Equations
Exercises for section 1.5
1.5.1. Consider the following system:
10−3x −y = 1,
x + y = 0.
(a)
Use 3-digit arithmetic with no pivoting to solve this system.
(b)
Find a system that is exactly satisﬁed by your solution from
part (a), and note how close this system is to the original system.
(c)
Now use partial pivoting and 3-digit arithmetic to solve the
original system.
(d)
Find a system that is exactly satisﬁed by your solution from
part (c), and note how close this system is to the original system.
(e)
Use exact arithmetic to obtain the solution to the original sys-
tem, and compare the exact solution with the results of parts (a)
and (c).
(f)
Round the exact solution to three signiﬁcant digits, and compare
the result with those of parts (a) and (c).
1.5.2. Consider the following system:
x +
y = 3,
−10x + 105y = 105.
(a)
Use 4-digit arithmetic with partial pivoting and no scaling to
compute a solution.
(b)
Use 4-digit arithmetic with complete pivoting and no scaling to
compute a solution of the original system.
(c)
This time, row scale the original system ﬁrst, and then apply
partial pivoting with 4-digit arithmetic to compute a solution.
(d)
Now determine the exact solution, and compare it with the re-
sults of parts (a), (b), and (c).
1.5.3. With no scaling, compute the 3-digit solution of
−3x + y = −2,
10x −3y = 7,
without partial pivoting and with partial pivoting. Compare your results
with the exact solution.

1.5 Making Gaussian Elimination Work
31
1.5.4. Consider the following system in which the coeﬃcient matrix is the
Hilbert matrix:
x + 1
2y + 1
3z = 1
3,
1
2x + 1
3y + 1
4z = 1
3,
1
3x + 1
4y + 1
5z = 1
5.
(a)
First convert the coeﬃcients to 3-digit ﬂoating-point numbers,
and then use 3-digit arithmetic with partial pivoting but with
no scaling to compute the solution.
(b)
Again use 3-digit arithmetic, but row scale the coeﬃcients (after
converting them to ﬂoating-point numbers), and then use partial
pivoting to compute the solution.
(c)
Proceed as in part (b), but this time row scale the coeﬃcients
before each elimination step.
(d)
Now use exact arithmetic on the original system to determine
the exact solution, and compare the result with those of parts
(a), (b), and (c).
1.5.5. To see that changing units can aﬀect a ﬂoating-point solution, consider
a mining operation that extracts silica, iron, and gold from the earth.
Capital (measured in dollars), operating time (in hours), and labor (in
man-hours) are needed to operate the mine. To extract a pound of silica
requires $.0055, .0011 hours of operating time, and .0093 man-hours of
labor. For each pound of iron extracted, $.095, .01 operating hours, and
.025 man-hours are required. For each pound of gold extracted, $960,
112 operating hours, and 560 man-hours are required.
(a)
Suppose that during 600 hours of operation, exactly $5000 and
3000 man-hours are used. Let x, y, and z denote the number
of pounds of silica, iron, and gold, respectively, that are recov-
ered during this period. Set up the linear system whose solution
will yield the values for x, y, and z.
(b)
With no scaling, use 3-digit arithmetic and partial pivoting to
compute a solution (˜x, ˜y, ˜z) of the system of part (a). Then
approximate the exact solution (x, y, z) by using your machine’s
(or calculator’s) full precision with partial pivoting to solve the
system in part (a), and compare this with your 3-digit solution
by computing the relative error deﬁned by
er =
 
(x −˜x)2 + (y −˜y)2 + (z −˜z)2
 
x2 + y2 + z2
.

32
Chapter 1
Linear Equations
(c)
Using 3-digit arithmetic, column scale the coeﬃcients by chang-
ing units: convert pounds of silica to tons of silica, pounds of
iron to half-tons of iron, and pounds of gold to troy ounces of
gold (1 lb. = 12 troy oz.).
(d)
Use 3-digit arithmetic with partial pivoting to solve the column
scaled system of part (c). Then approximate the exact solution
by using your machine’s (or calculator’s) full precision with par-
tial pivoting to solve the system in part (c), and compare this
with your 3-digit solution by computing the relative error er as
deﬁned in part (b).
1.5.6. Consider the system given in Example 1.5.3.
(a)
Use 3-digit arithmetic with partial pivoting but with no scaling
to solve the system.
(b)
Now use partial pivoting with scaling. Does complete pivoting
provide an advantage over scaled partial pivoting in this case?
1.5.7. Consider the following well-scaled matrix:
Wn =












1
0
0
· · ·
0
0
1
−1
1
0
· · ·
0
0
1
−1
−1
1
...
0
0
1
...
...
...
...
...
...
...
−1
−1
−1
...
1
0
1
−1
−1
−1
· · ·
−1
1
1
−1
−1
−1
· · ·
−1
−1
1












.
(a)
Reduce Wn to an upper-triangular form using Gaussian elimi-
nation with partial pivoting, and determine the element of max-
imal magnitude that emerges during the elimination procedure.
(b)
Now use complete pivoting and repeat part (a).
(c)
Formulate a statement comparing the results of partial pivoting
with those of complete pivoting for Wn, and describe the eﬀect
this would have in determining the t -digit solution for a system
whose augmented matrix is [Wn | b].
1.5.8. Suppose that A is an n × n matrix of real numbers that has been scaled
so that each entry satisﬁes |aij| ≤1, and consider reducing A to tri-
angular form using Gaussian elimination with partial pivoting. Demon-
strate that after k steps of the process, no entry can have a magnitude
that exceeds 2k. Note: The previous exercise shows that there are cases
where it is possible for some elements to actually attain the maximum
magnitude of 2k after k steps.

1.6 Ill-Conditioned Systems
33
1.6
ILL-CONDITIONED SYSTEMS
Gaussian elimination with partial pivoting on a properly scaled system is perhaps
the most fundamental algorithm in the practical use of linear algebra. However,
it is not a universal algorithm nor can it be used blindly. The purpose of this
section is to make the point that when solving a linear system some discretion
must always be exercised because there are some systems that are so inordinately
sensitive to small perturbations that no numerical technique can be used with
conﬁdence.
Example 1.6.1
Consider the system
.835x + .667y = .168,
.333x + .266y = .067,
for which the exact solution is
x = 1
and
y = −1.
If b2 = .067 is only slightly perturbed to become ˆb2 = .066, then the exact
solution changes dramatically to become
ˆx = −666
and
ˆy = 834.
This is an example of a system whose solution is extremely sensitive to
a small perturbation. This sensitivity is intrinsic to the system itself and is
not a result of any numerical procedure. Therefore, you cannot expect some
“numerical trick” to remove the sensitivity. If the exact solution is sensitive to
small perturbations, then any computed solution cannot be less so, regardless of
the algorithm used.
Ill-Conditioned Linear Systems
A system of linear equations is said to be ill-conditioned when
some small perturbation in the system can produce relatively large
changes in the exact solution. Otherwise, the system is said to be well-
conditioned.
It is easy to visualize what causes a 2 × 2 system to be ill-conditioned.
Geometrically, two equations in two unknowns represent two straight lines, and
the point of intersection is the solution for the system. An ill-conditioned system
represents two straight lines that are almost parallel.

34
Chapter 1
Linear Equations
If two straight lines are almost parallel and if one of the lines is tilted only
slightly, then the point of intersection (i.e., the solution of the associated 2 × 2
linear system) is drastically altered.
L
L'
Original
Solution
Perturbed
Solution
Figure 1.6.1
This is illustrated in Figure 1.6.1 in which line L is slightly perturbed to
become line L′. Notice how this small perturbation results in a large change
in the point of intersection. This was exactly the situation for the system given
in Example 1.6.1. In general, ill-conditioned systems are those that represent
almost parallel lines, almost parallel planes, and generalizations of these notions.
Because roundoﬀerrors can be viewed as perturbations to the original coeﬃ-
cients of the system, employing even a generally good numerical technique—short
of exact arithmetic—on an ill-conditioned system carries the risk of producing
nonsensical results.
In dealing with an ill-conditioned system, the engineer or scientist is often
confronted with a much more basic (and sometimes more disturbing) problem
than that of simply trying to solve the system. Even if a minor miracle could
be performed so that the exact solution could be extracted, the scientist or
engineer might still have a nonsensical solution that could lead to totally incorrect
conclusions. The problem stems from the fact that the coeﬃcients are often
empirically obtained and are therefore known only within certain tolerances. For
an ill-conditioned system, a small uncertainty in any of the coeﬃcients can mean
an extremely large uncertainty may exist in the solution. This large uncertainty
can render even the exact solution totally useless.
Example 1.6.2
Suppose that for the system
.835x + .667y = b1
.333x + .266y = b2
the numbers b1 and b2 are the results of an experiment and must be read from
the dial of a test instrument. Suppose that the dial can be read to within a

1.6 Ill-Conditioned Systems
35
tolerance of ±.001, and assume that values for b1 and b2 are read as . 168 and
. 067, respectively. This produces the ill-conditioned system of Example 1.6.1,
and it was seen in that example that the exact solution of the system is
(x, y) = (1, −1).
(1.6.1)
However, due to the small uncertainty in reading the dial, we have that
.167 ≤b1 ≤.169
and
.066 ≤b2 ≤.068.
(1.6.2)
For example, this means that the solution associated with the reading (b1, b2) =
(.168, .067) is just as valid as the solution associated with the reading (b1, b2) =
(.167, .068), or the reading (b1, b2) = (.169, .066), or any other reading falling
in the range (1.6.2). For the reading (b1, b2) = (.167, .068), the exact solution is
(x, y) = (934, −1169),
(1.6.3)
while for the other reading (b1, b2) = (.169, .066), the exact solution is
(x, y) = (−932, 1167).
(1.6.4)
Would you be willing to be the ﬁrst to ﬂy in the plane or drive across the bridge
whose design incorporated a solution to this problem? I wouldn’t! There is just
too much uncertainty. Since no one of the solutions (1.6.1), (1.6.3), or (1.6.4)
can be preferred over any of the others, it is conceivable that totally diﬀerent
designs might be implemented depending on how the technician reads the last
signiﬁcant digit on the dial. Due to the ill-conditioned nature of an associated
linear system, the successful design of the plane or bridge may depend on blind
luck rather than on scientiﬁc principles.
Rather than trying to extract accurate solutions from ill-conditioned sys-
tems, engineers and scientists are usually better oﬀinvesting their time and re-
sources in trying to redesign the associated experiments or their data collection
methods so as to avoid producing ill-conditioned systems.
There is one other discomforting aspect of ill-conditioned systems. It con-
cerns what students refer to as “checking the answer” by substituting a computed
solution back into the left-hand side of the original system of equations to see
how close it comes to satisfying the system—that is, producing the right-hand
side. More formally, if
xc = ( ξ1
ξ2
· · ·
ξn )
is a computed solution for a system
a11x1 + a12x2 + · · · + a1nxn = b1,
a21x1 + a22x2 + · · · + a2nxn = b2,
...
an1x1 + an2x2 + · · · + annxn = bn,

36
Chapter 1
Linear Equations
then the numbers
ri = ai1ξ1 + ai2ξ2 + · · · + ainξn −bi
for i = 1, 2, . . . , n
are called the residuals. Suppose that you compute a solution xc and substitute
it back to ﬁnd that all the residuals are relatively small. Does this guarantee that
xc is close to the exact solution? Surprisingly, the answer is a resounding “no!”
whenever the system is ill-conditioned.
Example 1.6.3
For the ill-conditioned system given in Example 1.6.1, suppose that somehow
you compute a solution to be
ξ1 = −666
and
ξ2 = 834.
If you attempt to “check the error” in this computed solution by substituting it
back into the original system, then you ﬁnd—using exact arithmetic—that the
residuals are
r1 = .835ξ1 + .667ξ2 −.168 = 0,
r2 = .333ξ1 + .266ξ2 −.067 = −.001.
That is, the computed solution (−666, 834) exactly satisﬁes the ﬁrst equation
and comes very close to satisfying the second. On the surface, this might seem to
suggest that the computed solution should be very close to the exact solution. In
fact a naive person could probably be seduced into believing that the computed
solution is within ±.001 of the exact solution. Obviously, this is nowhere close
to being true since the exact solution is
x = 1
and
y = −1.
This is always a shock to a student seeing this illustrated for the ﬁrst time because
it is counter to a novice’s intuition. Unfortunately, many students leave school
believing that they can always “check” the accuracy of their computations by
simply substituting them back into the original equations—it is good to know
that you’re not among them.
This raises the question, “How can I check a computed solution for accu-
racy?” Fortunately, if the system is well-conditioned, then the residuals do indeed
provide a more eﬀective measure of accuracy (a rigorous proof along with more
insight appears in Example 5.12.2 on p. 416). But this means that you must be
able to answer some additional questions. For example, how can one tell before-
hand if a given system is ill-conditioned? How can one measure the extent of
ill-conditioning in a linear system?
One technique to determine the extent of ill-conditioning might be to exper-
iment by slightly perturbing selected coeﬃcients and observing how the solution

1.6 Ill-Conditioned Systems
37
changes. If a radical change in the solution is observed for a small perturbation
to some set of coeﬃcients, then you have uncovered an ill-conditioned situation.
If a given perturbation does not produce a large change in the solution, then
nothing can be concluded—perhaps you perturbed the wrong set of coeﬃcients.
By performing several such experiments using diﬀerent sets of coeﬃcients, a
feel (but not a guarantee) for the extent of ill-conditioning can be obtained. This
is expensive and not very satisfying. But before more can be said, more sophisti-
cated tools need to be developed—the topics of sensitivity and conditioning are
revisited on p. 127 and in Example 5.12.1 on p. 414.
Exercises for section 1.6
1.6.1. Consider the ill-conditioned system of Example 1.6.1:
.835x + .667y = .168,
.333x + .266y = .067.
(a)
Describe the outcome when you attempt to solve the system
using 5-digit arithmetic with no scaling.
(b)
Again using 5-digit arithmetic, ﬁrst row scale the system before
attempting to solve it. Describe to what extent this helps.
(c)
Now use 6-digit arithmetic with no scaling. Compare the results
with the exact solution.
(d)
Using 6-digit arithmetic, compute the residuals for your solution
of part (c), and interpret the results.
(e)
For the same solution obtained in part (c), again compute the
residuals, but use 7-digit arithmetic this time, and interpret the
results.
(f)
Formulate a concluding statement that summarizes the points
made in parts (a)–(e).
1.6.2. Perturb the ill-conditioned system given in Exercise 1.6.1 above so as to
form the following system:
.835x + .667y = .1669995,
.333x + .266y = .066601.
(a)
Determine the exact solution, and compare it with the exact
solution of the system in Exercise 1.6.1.
(b)
On the basis of the results of part (a), formulate a statement
concerning the necessity for the solution of an ill-conditioned
system to undergo a radical change for every perturbation of
the original system.

38
Chapter 1
Linear Equations
1.6.3. Consider the two straight lines determined by the graphs of the following
two equations:
.835x + .667y = .168,
.333x + .266y = .067.
(a)
Use 5-digit arithmetic to compute the slopes of each of the lines,
and then use 6-digit arithmetic to do the same. In each case,
sketch the graphs on a coordinate system.
(b)
Show by diagram why a small perturbation in either of these
lines can result in a large change in the solution.
(c)
Describe in geometrical terms the situation that must exist in
order for a system to be optimally well-conditioned.
1.6.4. Using geometric considerations, rank the following three systems accord-
ing to their condition.
(a)
1.001x −y = .235,
x + .0001y = .765.
(b)
1.001x −y = .235,
x + .9999y = .765.
(c)
1.001x + y = .235,
x + .9999y = .765.
1.6.5. Determine the exact solution of the following system:
8x + 5y + 2z = 15,
21x + 19y + 16z = 56,
39x + 48y + 53z = 140.
Now change 15 to 14 in the ﬁrst equation and again solve the system
with exact arithmetic. Is the system ill-conditioned?
1.6.6. Show that the system
v −w −x −y −z = 0,
w −x −y −z = 0,
x −y −z = 0,
y −z = 0,
z = 1,

1.6 Ill-Conditioned Systems
39
is ill-conditioned by considering the following perturbed system:
v −w −x −y −z = 0,
−1
15v + w −x −y −z = 0,
−1
15v + x −y −z = 0,
−1
15v + y −z = 0,
−1
15v + z = 1.
1.6.7. Let f(x) = sin πx on [0, 1]. The object of this problem is to determine
the coeﬃcients αi of the cubic polynomial
p(x) =
3
!
i=0
αixi
that is as close to f(x) as possible in the sense that
r =
" 1
0
[f(x) −p(x)]2dx
=
" 1
0
[f(x)]2dx −2
3
!
i=0
αi
" 1
0
xif(x)dx +
" 1
0
# 3
!
i=0
αixi
$2
dx
is as small as possible.
(a)
In order to minimize r, impose the condition that ∂r/∂αi = 0
for each i = 0, 1, 2, 3, and show this results in a system of linear
equations whose augmented matrix is [H4 | b], where H4 and
b are given by
H4 =









1
1
2
1
3
1
4
1
2
1
3
1
4
1
5
1
3
1
4
1
5
1
6
1
4
1
5
1
6
1
7









and
b =









2
π
1
π
1
π −
4
π3
1
π −
6
π3









.
Any matrix Hn that has the same form as H4 is called a
Hilbert matrix of order n.
(b)
Systems involving Hilbert matrices are badly ill-conditioned,
and the ill-conditioning becomes worse as the size increases. Use
exact arithmetic with Gaussian elimination to reduce H4 to tri-
angular form. Assuming that the case in which n = 4 is typical,
explain why a general system [Hn | b] will be ill-conditioned.
Notice that even complete pivoting is of no help.

CHAPTER 2
Rectangular Systems
and
Echelon Forms
2.1
ROW ECHELON FORM AND RANK
We are now ready to analyze more general linear systems consisting of m linear
equations involving n unknowns
a11x1 +
a12x2 + · · · +
a1nxn =
b1,
a21x1 +
a22x2 + · · · +
a2nxn =
b2,
...
am1x1 + am2x2 + · · · + amnxn = bm,
where m may be diﬀerent from n. If we do not know for sure that m and n
are the same, then the system is said to be rectangular. The case m = n is
still allowed in the discussion—statements concerning rectangular systems also
are valid for the special case of square systems.
The ﬁrst goal is to extend the Gaussian elimination technique from square
systems to completely general rectangular systems. Recall that for a square sys-
tem with a unique solution, the pivotal positions are always located along the
main diagonal—the diagonal line from the upper-left-hand corner to the lower-
right-hand corner—in the coeﬃcient matrix A so that Gaussian elimination
results in a reduction of A to a triangular matrix, such as that illustrated
below for the case n = 4:
T =



⃝
*
∗
∗
∗
0
⃝
*
∗
∗
0
0
⃝
*
∗
0
0
0
⃝
*


.

42
Chapter 2
Rectangular Systems and Echelon Forms
Remember that a pivot must always be a nonzero number. For square sys-
tems possessing a unique solution, it is a fact (proven later) that one can al-
ways bring a nonzero number into each pivotal position along the main diag-
onal.
8 However, in the case of a general rectangular system, it is not always
possible to have the pivotal positions lying on a straight diagonal line in the
coeﬃcient matrix. This means that the ﬁnal result of Gaussian elimination will
not be triangular in form. For example, consider the following system:
x1 + 2x2 + x3 + 3x4 + 3x5 = 5,
2x1 + 4x2
+ 4x4 + 4x5 = 6,
x1 + 2x2 + 3x3 + 5x4 + 5x5 = 9,
2x1 + 4x2
+ 4x4 + 7x5 = 9.
Focus your attention on the coeﬃcient matrix
A =



1
2
1
3
3
2
4
0
4
4
1
2
3
5
5
2
4
0
4
7


,
(2.1.1)
and ignore the right-hand side for a moment. Applying Gaussian elimination to
A yields the following result:



⃝
1
2
1
3
3
2
4
0
4
4
1
2
3
5
5
2
4
0
4
7


−→



1
2
1
3
3
0
⃝
0
−2
−2
−2
0
0
2
2
2
0
0
−2
−2
1


.
In the basic elimination process, the strategy is to move down and to the right
to the next pivotal position. If a zero occurs in this position, an interchange with
a row below the pivotal row is executed so as to bring a nonzero number into
the pivotal position. However, in this example, it is clearly impossible to bring
a nonzero number into the (2, 2) -position by interchanging the second row with
a lower row.
In order to handle this situation, the elimination process is modiﬁed as
follows.
8
This discussion is for exact arithmetic. If ﬂoating-point arithmetic is used, this may no longer
be true. Part (a) of Exercise 1.6.1 is one such example.

2.1 Row Echelon Form and Rank
43
Modiﬁed Gaussian Elimination
Suppose that U is the augmented matrix associated with the system
after i −1 elimination steps have been completed. To execute the ith
step, proceed as follows:
•
Moving from left to right in U , locate the ﬁrst column that contains
a nonzero entry on or below the ith position—say it is U∗j.
•
The pivotal position for the ith step is the (i, j) -position.
•
If necessary, interchange the ith row with a lower row to bring a
nonzero number into the (i, j) -position, and then annihilate all en-
tries below this pivot.
•
If row Ui∗as well as all rows in U below Ui∗consist entirely of
zeros, then the elimination process is completed.
Illustrated below is the result of applying this modiﬁed version of Gaussian
elimination to the matrix given in (2.1.1).
Example 2.1.1
Problem: Apply modiﬁed Gaussian elimination to the following matrix and
circle the pivot positions:
A =



1
2
1
3
3
2
4
0
4
4
1
2
3
5
5
2
4
0
4
7


.
Solution:



⃝
1
2
1
3
3
2
4
0
4
4
1
2
3
5
5
2
4
0
4
7


−→



⃝
1
2
1
3
3
0
0
⃝
-2
−2
−2
0
0
2
2
2
0
0
−2
−2
1



−→



⃝
1
2
1
3
3
0
0
⃝
-2
−2
−2
0
0
0
0
⃝
0
0
0
0
0
3


−→



⃝
1
2
1
3
3
0
0
⃝
-2
−2
−2
0
0
0
0
⃝
3
0
0
0
0
0


.

44
Chapter 2
Rectangular Systems and Echelon Forms
Notice that the ﬁnal result of applying Gaussian elimination in the above
example is not a purely triangular form but rather a jagged or “stair-step” type
of triangular form. Hereafter, a matrix that exhibits this stair-step structure will
be said to be in row echelon form.
Row Echelon Form
An m × n matrix E with rows Ei∗and columns E∗j is said to be in
row echelon form provided the following two conditions hold.
•
If Ei∗consists entirely of zeros, then all rows below Ei∗are also
entirely zero; i.e., all zero rows are at the bottom.
•
If the ﬁrst nonzero entry in Ei∗lies in the jth position, then all
entries below the ith position in columns E∗1, E∗2, . . . , E∗j are zero.
These two conditions say that the nonzero entries in an echelon form
must lie on or above a stair-step line that emanates from the upper-
left-hand corner and slopes down and to the right. The pivots are the
ﬁrst nonzero entries in each row. A typical structure for a matrix in row
echelon form is illustrated below with the pivots circled.







⃝
*
∗
∗
∗
∗
∗
∗
∗
0
0
⃝
*
∗
∗
∗
∗
∗
0
0
0
⃝
*
∗
∗
∗
∗
0
0
0
0
0
0
⃝
*
∗
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0







Because of the ﬂexibility in choosing row operations to reduce a matrix A
to a row echelon form E, the entries in E are not uniquely determined by A.
Nevertheless, it can be proven that the “form” of E is unique in the sense that
the positions of the pivots in E (and A) are uniquely determined by the entries
in A .
9 Because the pivotal positions are unique, it follows that the number of
pivots, which is the same as the number of nonzero rows in E, is also uniquely
determined by the entries in A . This number is called the rank
10 of A, and it
9
The fact that the pivotal positions are unique should be intuitively evident. If it isn’t, take the
matrix given in (2.1.1) and try to force some diﬀerent pivotal positions by a diﬀerent sequence
of row operations.
10
The word “rank” was introduced in 1879 by the German mathematician Ferdinand Georg
Frobenius (p. 662), who thought of it as the size of the largest nonzero minor determinant
in A. But the concept had been used as early as 1851 by the English mathematician James
J. Sylvester (1814–1897).

2.1 Row Echelon Form and Rank
45
is extremely important in the development of our subject.
Rank of a Matrix
Suppose Am×n is reduced by row operations to an echelon form E.
The rank of A is deﬁned to be the number
rank (A) = number of pivots
= number of nonzero rows in E
= number of basic columns in A,
where the basic columns of A are deﬁned to be those columns in A
that contain the pivotal positions.
Example 2.1.2
Problem: Determine the rank, and identify the basic columns in
A =


1
2
1
1
2
4
2
2
3
6
3
4

.
Solution: Reduce A to row echelon form as shown below:
A =


⃝
1
2
1
1
2
4
2
2
3
6
3
4

−→


⃝
1
2
1
1
0
0
0
⃝
0
0
0
0
1

−→


⃝
1
2
1
1
0
0
0
⃝
1
0
0
0
0

= E.
Consequently, rank (A) = 2. The pivotal positions lie in the ﬁrst and fourth
columns so that the basic columns of A are A∗1 and A∗4. That is,
Basic Columns =





1
2
3

,


1
2
4




.
Pay particular attention to the fact that the basic columns are extracted from
A and not from the row echelon form E .

46
Chapter 2
Rectangular Systems and Echelon Forms
Exercises for section 2.1
2.1.1. Reduce each of the following matrices to row echelon form, determine
the rank, and identify the basic columns.
(a)


1
2
3
3
2
4
6
9
2
6
7
6

(b)





1
2
3
2
6
8
2
6
0
1
2
5
3
8
6




(c)







2
1
1
3
0
4
1
4
2
4
4
1
5
5
2
1
3
1
0
4
3
6
3
4
8
1
9
5
0
0
3
−3
0
0
3
8
4
2
14
1
13
3







2.1.2. Determine which of the following matrices are in row echelon form:
(a)


1
2
3
0
0
4
0
1
0

.
(b)


0
0
0
0
0
1
0
0
0
0
0
1

.
(c)


2
2
3
−4
0
0
7
−8
0
0
0
−1

.
(d)



1
2
0
0
1
0
0
0
0
1
0
0
0
0
0
0
0
1
0
0
0
0
0
0


.
2.1.3. Suppose that A is an m × n matrix. Give a short explanation of why
each of the following statements is true.
(a)
rank (A) ≤min{m, n}.
(b)
rank (A) < m if one row in A is entirely zero.
(c)
rank (A) < m if one row in A is a multiple of another row.
(d)
rank (A) < m if one row in A is a combination of other rows.
(e)
rank (A) < n if one column in A is entirely zero.
2.1.4. Let A =


.1
.2
.3
.4
.5
.6
.7
.8
.901

.
(a)
Use exact arithmetic to determine rank (A).
(b)
Now use 3-digit ﬂoating-point arithmetic (without partial piv-
oting or scaling) to determine rank (A). This number might be
called the “3-digit numerical rank.”
(c)
What happens if partial pivoting is incorporated?
2.1.5. How many diﬀerent “forms” are possible for a 3 × 4 matrix that is in
row echelon form?
2.1.6. Suppose that [A|b] is reduced to a matrix [E|c].
(a)
Is [E|c] in row echelon form if E is?
(b)
If [E|c] is in row echelon form, must E be in row echelon form?

2.2 Reduced Row Echelon Form
47
2.2
REDUCED ROW ECHELON FORM
At each step of the Gauss–Jordan method, the pivot is forced to be a 1, and then
all entries above and below the pivotal 1 are annihilated. If A is the coeﬃcient
matrix for a square system with a unique solution, then the end result of applying
the Gauss–Jordan method to A is a matrix with 1’s on the main diagonal and
0’s everywhere else. That is,
A
Gauss–Jordan
−−−−−−−−→




1
0
· · ·
0
0
1
· · ·
0
...
...
...
...
0
0
· · ·
1



.
But if the Gauss–Jordan technique is applied to a more general m × n matrix,
then the ﬁnal result is not necessarily the same as described above. The following
example illustrates what typically happens in the rectangular case.
Example 2.2.1
Problem: Apply Gauss–Jordan elimination to the following 4 × 5 matrix and
circle the pivot positions. This is the same matrix used in Example 2.1.1:
A =



1
2
1
3
3
2
4
0
4
4
1
2
3
5
5
2
4
0
4
7


.
Solution:



⃝
1
2
1
3
3
2
4
0
4
4
1
2
3
5
5
2
4
0
4
7


→



⃝
1
2
1
3
3
0
0
⃝
-2
−2
−2
0
0
2
2
2
0
0
−2
−2
1


→



⃝
1
2
1
3
3
0
0
⃝
1
1
1
0
0
2
2
2
0
0
−2
−2
1



→



⃝
1
2
0
2
2
0
0
⃝
1
1
1
0
0
0
0
⃝
0
0
0
0
0
3


→



⃝
1
2
0
2
2
0
0
⃝
1
1
1
0
0
0
0
⃝
3
0
0
0
0
0


→



⃝
1
2
0
2
2
0
0
⃝
1
1
1
0
0
0
0
⃝
1
0
0
0
0
0



→



⃝
1
2
0
2
0
0
0
⃝
1
1
0
0
0
0
0
⃝
1
0
0
0
0
0


.

48
Chapter 2
Rectangular Systems and Echelon Forms
Compare the results of this example with the results of Example 2.1.1, and
notice that the “form” of the ﬁnal matrix is the same in both examples, which
indeed must be the case because of the uniqueness of “form” mentioned in the
previous section. The only diﬀerence is in the numerical value of some of the
entries. By the nature of Gauss–Jordan elimination, each pivot is 1 and all entries
above and below each pivot are 0. Consequently, the row echelon form produced
by the Gauss–Jordan method contains a reduced number of nonzero entries, so
it seems only natural to refer to this as a reduced row echelon form.
11
Reduced Row Echelon Form
A matrix Em×n is said to be in reduced row echelon form provided
that the following three conditions hold.
•
E is in row echelon form.
•
The ﬁrst nonzero entry in each row (i.e., each pivot) is 1.
•
All entries above each pivot are 0.
A typical structure for a matrix in reduced row echelon form is illustrated
below, where entries marked * can be either zero or nonzero numbers:







⃝
1
∗
0
0
∗
∗
0
∗
0
0
⃝
1
0
∗
∗
0
∗
0
0
0
⃝
1
∗
∗
0
∗
0
0
0
0
0
0
⃝
1
∗
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0







.
As previously stated, if matrix A is transformed to a row echelon form
by row operations, then the “form” is uniquely determined by A, but the in-
dividual entries in the form are not unique. However, if A is transformed by
row operations to a reduced row echelon form EA, then it can be shown
12 that
both the “form” as well as the individual entries in EA are uniquely determined
by A. In other words, the reduced row echelon form EA produced from A is
independent of whatever elimination scheme is used. Producing an unreduced
form is computationally more eﬃcient, but the uniqueness of EA makes it more
useful for theoretical purposes.
11
In some of the older books this is called the Hermite normal form
in honor of the French
mathematician Charles Hermite (1822–1901), who, around 1851, investigated reducing matrices
by row operations.
12
A formal uniqueness proof must wait until Example 3.9.2, but you can make this intuitively
clear right now with some experiments. Try to produce two diﬀerent reduced row echelon forms
from the same matrix.

2.2 Reduced Row Echelon Form
49
EA Notation
For a matrix A, the symbol EA will hereafter denote the unique re-
duced row echelon form derived from A by means of row operations.
Example 2.2.2
Problem: Determine EA, deduce rank (A), and identify the basic columns of
A =



1
2
2
3
1
2
4
4
6
2
3
6
6
9
6
1
2
4
5
3


.
Solution:



⃝
1
2
2
3
1
2
4
4
6
2
3
6
6
9
6
1
2
4
5
3


−→



⃝
1
2
2
3
1
0
0
⃝
0
0
0
0
0
0
0
3
0
0
2
2
2


−→



⃝
1
2
2
3
1
0
0
⃝
2
2
2
0
0
0
0
3
0
0
0
0
0



−→



⃝
1
2
2
3
1
0
0
⃝
1
1
1
0
0
0
0
3
0
0
0
0
0


−→



⃝
1
2
0
1
−1
0
0
⃝
1
1
1
0
0
0
0
⃝
3
0
0
0
0
0



−→



⃝
1
2
0
1
−1
0
0
⃝
1
1
1
0
0
0
0
⃝
1
0
0
0
0
0


−→



⃝
1
2
0
1
0
0
0
⃝
1
1
0
0
0
0
0
⃝
1
0
0
0
0
0



Therefore, rank (A) = 3, and {A∗1, A∗3, A∗5} are the three basic columns.
The above example illustrates another important feature of EA and ex-
plains why the basic columns are indeed “basic.” Each nonbasic column is ex-
pressible as a combination of basic columns. In Example 2.2.2,
A∗2 = 2A∗1
and
A∗4 = A∗1 + A∗3.
(2.2.1)
Notice that exactly the same set of relationships hold in EA. That is,
E∗2 = 2E∗1
and
E∗4 = E∗1 + E∗3.
(2.2.2)
This is no coincidence—it’s characteristic of what happens in general. There’s
more to observe. The relationships between the nonbasic and basic columns in a

50
Chapter 2
Rectangular Systems and Echelon Forms
general matrix A are usually obscure, but the relationships among the columns
in EA are absolutely transparent. For example, notice that the multipliers used
in the relationships (2.2.1) and (2.2.2) appear explicitly in the two nonbasic
columns in EA —they are just the nonzero entries in these nonbasic columns.
This is important because it means that EA can be used as a “map” or “key”
to discover or unlock the hidden relationships among the columns of A .
Finally, observe from Example 2.2.2 that only the basic columns to the left
of a given nonbasic column are needed in order to express the nonbasic column
as a combination of basic columns—e.g., representing A∗2 requires only A∗1
and not A∗3 or A∗5, while representing A∗4 requires only A∗1 and A∗3.
This too is typical. For the time being, we accept the following statements to be
true. A rigorous proof is given later on p. 136.
Column Relationships in A and EA
•
Each nonbasic column E∗k in EA is a combination (a sum of mul-
tiples) of the basic columns in EA to the left of E∗k. That is,
E∗k = µ1E∗b1 + µ2E∗b2 + · · · + µjE∗bj
= µ1









1
0
...
0
...
0









+ µ2









0
1
...
0
...
0









+ · · · + µj









0
0
...
1
...
0









=









µ1
µ2
...
µj
...
0









,
where the E∗bi’s are the basic columns to the left of E∗k and where
the multipliers µi are the ﬁrst j entries in E∗k.
•
The relationships that exist among the columns of A are exactly
the same as the relationships that exist among the columns of EA.
In particular, if A∗k is a nonbasic column in A , then
A∗k = µ1A∗b1 + µ2A∗b2 + · · · + µjA∗bj,
(2.2.3)
where the A∗bi’s are the basic columns to the left of A∗k, and
where the multipliers µi are as described above—the ﬁrst j entries
in E∗k.

2.2 Reduced Row Echelon Form
51
Example 2.2.3
Problem: Write each nonbasic column as a combination of basic columns in
A =


2
−4
−8
6
3
0
1
3
2
3
3
−2
0
0
8

.
Solution: Transform A to EA as shown below.


⃝
2
−4
−8
6
3
0
1
3
2
3
3
−2
0
0
8

→


⃝
1
−2
−4
3
3
2
0
1
3
2
3
3
−2
0
0
8

→


⃝
1
−2
−4
3
3
2
0
⃝
1
3
2
3
0
4
12
−9
7
2

→


⃝
1
0
2
7
15
2
0
⃝
1
3
2
3
0
0
0
−17
−17
2

→


⃝
1
0
2
7
15
2
0
⃝
1
3
2
3
0
0
0
⃝
1
1
2

→


⃝
1
0
2
0
4
0
⃝
1
3
0
2
0
0
0
⃝
1
1
2


The third and ﬁfth columns are nonbasic. Looking at the columns in EA reveals
E∗3 = 2E∗1 + 3E∗2
and
E∗5 = 4E∗1 + 2E∗2 + 1
2E∗4.
The relationships that exist among the columns of A must be exactly the same
as those in EA, so
A∗3 = 2A∗1 + 3A∗2
and
A∗5 = 4A∗1 + 2A∗2 + 1
2A∗4.
You can easily check the validity of these equations by direct calculation.
In summary, the utility of EA lies in its ability to reveal dependencies in
data stored as columns in an array A. The nonbasic columns in A represent
redundant information in the sense that this information can always be expressed
in terms of the data contained in the basic columns.
Although data compression is not the primary reason for introducing EA,
the application to these problems is clear. For a large array of data, it may be
more eﬃcient to store only “independent data” (i.e., the basic columns of A )
along with the nonzero multipliers µi obtained from the nonbasic columns in
EA. Then the redundant data contained in the nonbasic columns of A can
always be reconstructed if and when it is called for.
Exercises for section 2.2
2.2.1. Determine the reduced row echelon form for each of the following matri-
ces and then express each nonbasic column in terms of the basic columns:
(a)


1
2
3
3
2
4
6
9
2
6
7
6

,
(b)







2
1
1
3
0
4
1
4
2
4
4
1
5
5
2
1
3
1
0
4
3
6
3
4
8
1
9
5
0
0
3
−3
0
0
3
8
4
2
14
1
13
3







.

52
Chapter 2
Rectangular Systems and Echelon Forms
2.2.2. Construct a matrix A whose reduced row echelon form is
EA =







1
2
0
−3
0
0
0
0
0
1
−4
0
1
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0







.
Is A unique?
2.2.3. Suppose that A is an m × n matrix. Give a short explanation of why
rank (A) < n whenever one column in A is a combination of other
columns in A .
2.2.4. Consider the following matrix:
A =


.1
.2
.3
.4
.5
.6
.7
.8
.901

.
(a)
Use exact arithmetic to determine EA.
(b)
Now use 3-digit ﬂoating-point arithmetic (without partial piv-
oting or scaling) to determine EA and formulate a statement
concerning “near relationships” between the columns of A .
2.2.5. Consider the matrix
E =


1
0
−1
0
1
2
0
0
0

.
You already know that E∗3 can be expressed in terms of E∗1 and E∗2.
However, this is not the only way to represent the column dependencies
in E . Show how to write E∗1 in terms of E∗2 and E∗3 and then
express E∗2 as a combination of E∗1 and E∗3. Note:
This exercise
illustrates that the set of pivotal columns is not the only set that can
play the role of “basic columns.” Taking the basic columns to be the
ones containing the pivots is a matter of convenience because everything
becomes automatic that way.

2.3 Consistency of Linear Systems
53
2.3
CONSISTENCY OF LINEAR SYSTEMS
A system of m linear equations in n unknowns is said to be a consistent sys-
tem if it possesses at least one solution. If there are no solutions, then the system
is called inconsistent. The purpose of this section is to determine conditions
under which a given system will be consistent.
Stating conditions for consistency of systems involving only two or three
unknowns is easy. A linear equation in two unknowns represents a line in 2-space,
and a linear equation in three unknowns is a plane in 3-space. Consequently, a
linear system of m equations in two unknowns is consistent if and only if the m
lines deﬁned by the m equations have at least one common point of intersection.
Similarly, a system of m equations in three unknowns is consistent if and only
if the associated m planes have at least one common point of intersection.
However, when m is large, these geometric conditions may not be easy to verify
visually, and when n > 3, the generalizations of intersecting lines or planes are
impossible to visualize with the eye.
Rather than depending on geometry to establish consistency, we use Gaus-
sian elimination. If the associated augmented matrix [A|b] is reduced by row
operations to a matrix [E|c] that is in row echelon form, then consistency—or
lack of it—becomes evident. Suppose that somewhere in the process of reduc-
ing [A|b] to [E|c] a situation arises in which the only nonzero entry in a row
appears on the right-hand side, as illustrated below:
Row i −→







∗
∗
∗
∗
∗
∗
∗
0
0
0
∗
∗
∗
∗
0
0
0
0
∗
∗
∗
0
0
0
0
0
0
α
•
•
•
•
•
•
•
•
•
•
•
•
•
•






←−α ̸= 0.
If this occurs in the ith row, then the ith equation of the associated system is
0x1 + 0x2 + · · · + 0xn = α.
For α ̸= 0, this equation has no solution, and hence the original system must
also be inconsistent (because row operations don’t alter the solution set). The
converse also holds. That is, if a system is inconsistent, then somewhere in the
elimination process a row of the form
( 0
0
· · ·
0
|
α ) ,
α ̸= 0
(2.3.1)
must appear. Otherwise, the back substitution process can be completed and
a solution is produced. There is no inconsistency indicated when a row of the
form (0 0 · · · 0 | 0) is encountered. This simply says that 0 = 0, and although

54
Chapter 2
Rectangular Systems and Echelon Forms
this is no help in determining the value of any unknown, it is nevertheless a true
statement, so it doesn’t indicate inconsistency in the system.
There are some other ways to characterize the consistency (or inconsistency)
of a system. One of these is to observe that if the last column b in the augmented
matrix [A|b] is a nonbasic column, then no pivot can exist in the last column,
and hence the system is consistent because the situation (2.3.1) cannot occur.
Conversely, if the system is consistent, then the situation (2.3.1) never occurs
during Gaussian elimination and consequently the last column cannot be basic.
In other words, [A|b] is consistent if and only if b is a nonbasic column.
Saying that b is a nonbasic column in [A|b] is equivalent to saying that
all basic columns in [A|b] lie in the coeﬃcient matrix A . Since the number of
basic columns in a matrix is the rank, consistency may also be characterized by
stating that a system is consistent if and only if rank[A|b] = rank (A).
Recall from the previous section the fact that each nonbasic column in [A|b]
must be expressible in terms of the basic columns. Because a consistent system
is characterized by the fact that the right-hand side b is a nonbasic column,
it follows that a system is consistent if and only if the right-hand side b is a
combination of columns from the coeﬃcient matrix A.
Each of the equivalent
13 ways of saying that a system is consistent is sum-
marized below.
Consistency
Each of the following is equivalent to saying that [A|b] is consistent.
•
In row reducing [A|b], a row of the following form never appears:
( 0
0
· · ·
0
|
α ) ,
where
α ̸= 0.
(2.3.2)
•
b is a nonbasic column in [A|b].
(2.3.3)
•
rank[A|b] = rank (A).
(2.3.4)
•
b is a combination of the basic columns in A.
(2.3.5)
Example 2.3.1
Problem: Determine if the following system is consistent:
x1 + x2 + 2x3 + 2x4 + x5 = 1,
2x1 + 2x2 + 4x3 + 4x4 + 3x5 = 1,
2x1 + 2x2 + 4x3 + 4x4 + 2x5 = 2,
3x1 + 5x2 + 8x3 + 6x4 + 5x5 = 3.
13
Statements P and Q are said to be equivalent when (P implies Q) as well as its converse (Q
implies P) are true statements. This is also the meaning of the phrase “P if and only if Q.”

2.3 Consistency of Linear Systems
55
Solution: Apply Gaussian elimination to the augmented matrix [A|b] as shown:



⃝
1
1
2
2
1
1
2
2
4
4
3
1
2
2
4
4
2
2
3
5
8
6
5
3


−→



⃝
1
1
2
2
1
1
0
⃝
0
0
0
1
−1
0
0
0
0
0
0
0
2
2
0
2
0



−→



⃝
1
1
2
2
1
1
0
⃝
2
2
0
2
0
0
0
0
0
⃝
1
−1
0
0
0
0
0
0


.
Because a row of the form ( 0
0
· · ·
0
|
α ) with α ̸= 0 never emerges,
the system is consistent. We might also observe that b is a nonbasic column
in [A|b] so that rank[A|b] = rank (A). Finally, by completely reducing A to
EA, it is possible to verify that b is indeed a combination of the basic columns
{A∗1, A∗2, A∗5}.
Exercises for section 2.3
2.3.1. Determine which of the following systems are consistent.
(a)
x + 2y + z = 2,
2x + 4y
= 2,
3x + 6y + z = 4.
(b)
2x + 2y + 4z = 0,
3x + 2y + 5z = 0,
4x + 2y + 6z = 0.
(c)
x −y + z = 1,
x −y −z = 2,
x + y −z = 3,
x + y + z = 4.
(d)
x −y + z = 1,
x −y −z = 2,
x + y −z = 3,
x + y + z = 2.
(e)
2w + x + 3y + 5z = 1,
4w +
4y + 8z = 0,
w + x + 2y + 3z = 0,
x + y + z = 0.
(f)
2w + x + 3y + 5z = 7,
4w +
4y + 8z = 8,
w + x + 2y + 3z = 5,
x + y + z = 3.
2.3.2. Construct a 3 × 4 matrix A and 3 × 1 columns b and c such that
[A|b] is the augmented matrix for an inconsistent system, but [A|c] is
the augmented matrix for a consistent system.
2.3.3. If A is an m × n matrix with rank (A) = m, explain why the system
[A|b] must be consistent for every right-hand side b .

56
Chapter 2
Rectangular Systems and Echelon Forms
2.3.4. Consider two consistent systems whose augmented matrices are of the
form [A|b] and [A|c]. That is, they diﬀer only on the right-hand side.
Is the system associated with [A | b + c] also consistent? Explain why.
2.3.5. Is it possible for a parabola whose equation has the form y = α+βx+γx2
to pass through the four points (0, 1), (1, 3), (2, 15), and (3, 37)? Why?
2.3.6. Consider using ﬂoating-point arithmetic (without scaling) to solve the
following system:
.835x + .667y = .168,
.333x + .266y = .067.
(a)
Is the system consistent when 5-digit arithmetic is used?
(b)
What happens when 6-digit arithmetic is used?
2.3.7. In order to grow a certain crop, it is recommended that each square foot
of ground be treated with 10 units of phosphorous, 9 units of potassium,
and 19 units of nitrogen. Suppose that there are three brands of fertilizer
on the market— say brand X , brand Y , and brand Z . One pound of
brand X contains 2 units of phosphorous, 3 units of potassium, and 5
units of nitrogen. One pound of brand Y contains 1 unit of phosphorous,
3 units of potassium, and 4 units of nitrogen. One pound of brand Z
contains only 1 unit of phosphorous and 1 unit of nitrogen. Determine
whether or not it is possible to meet exactly the recommendation by
applying some combination of the three brands of fertilizer.
2.3.8. Suppose that an augmented matrix [A|b] is reduced by means of Gaus-
sian elimination to a row echelon form [E|c]. If a row of the form
( 0
0
· · ·
0
|
α ) ,
α ̸= 0
does not appear in [E|c], is it possible that rows of this form could have
appeared at earlier stages in the reduction process? Why?

2.4 Homogeneous Systems
57
2.4
HOMOGENEOUS SYSTEMS
A system of m linear equations in n unknowns
a11x1 + a12x2 + · · · + a1nxn = 0,
a21x1 + a22x2 + · · · + a2nxn = 0,
...
am1x1 + am2x2 + · · · + amnxn = 0,
in which the right-hand side consists entirely of 0’s is said to be a homogeneous
system. If there is at least one nonzero number on the right-hand side, then the
system is called nonhomogeneous. The purpose of this section is to examine
some of the elementary aspects concerning homogeneous systems.
Consistency is never an issue when dealing with homogeneous systems be-
cause the zero solution x1 = x2 = · · · = xn = 0 is always one solution regardless
of the values of the coeﬃcients. Hereafter, the solution consisting of all zeros is
referred to as the trivial solution. The only question is, “Are there solutions
other than the trivial solution, and if so, how can we best describe them?” As
before, Gaussian elimination provides the answer.
While reducing the augmented matrix [A|0] of a homogeneous system to
a row echelon form using Gaussian elimination, the zero column on the right-
hand side can never be altered by any of the three elementary row operations.
That is, any row echelon form derived from [A|0] by means of row operations
must also have the form [E|0]. This means that the last column of 0’s is just
excess baggage that is not necessary to carry along at each step. Just reduce the
coeﬃcient matrix A to a row echelon form E, and remember that the right-
hand side is entirely zero when you execute back substitution. The process is
best understood by considering a typical example.
In order to examine the solutions of the homogeneous system
x1 + 2x2 + 2x3 + 3x4 = 0,
2x1 + 4x2 + x3 + 3x4 = 0,
3x1 + 6x2 + x3 + 4x4 = 0,
(2.4.1)
reduce the coeﬃcient matrix to a row echelon form.
A =


1
2
2
3
2
4
1
3
3
6
1
4

−→


1
2
2
3
0
0
−3
−3
0
0
−5
−5

−→


1
2
2
3
0
0
−3
−3
0
0
0
0

= E.
Therefore, the original homogeneous system is equivalent to the following reduced
homogeneous system:
x1 + 2x2 + 2x3 + 3x4 = 0,
−3x3 −3x4 = 0.
(2.4.2)

58
Chapter 2
Rectangular Systems and Echelon Forms
Since there are four unknowns but only two equations in this reduced system,
it is impossible to extract a unique solution for each unknown. The best we can
do is to pick two “basic” unknowns—which will be called the basic variables
and solve for these in terms of the other two unknowns—whose values must
remain arbitrary or “free,” and consequently they will be referred to as the free
variables. Although there are several possibilities for selecting a set of basic
variables, the convention is to always solve for the unknowns corresponding to
the pivotal positions—or, equivalently, the unknowns corresponding to the basic
columns. In this example, the pivots (as well as the basic columns) lie in the ﬁrst
and third positions, so the strategy is to apply back substitution to solve the
reduced system (2.4.2) for the basic variables x1 and x3 in terms of the free
variables x2 and x4. The second equation in (2.4.2) yields
x3 = −x4
and substitution back into the ﬁrst equation produces
x1 = −2x2 −2x3 −3x4,
= −2x2 −2(−x4) −3x4,
= −2x2 −x4.
Therefore, all solutions of the original homogeneous system can be described by
saying
x1 = −2x2 −x4,
x2 is “free,”
x3 = −x4,
x4 is “free.”
(2.4.3)
As the free variables x2 and x4 range over all possible values, the above ex-
pressions describe all possible solutions. For example, when x2 and x4 assume
the values x2 = 1 and x4 = −2, then the particular solution
x1 = 0,
x2 = 1,
x3 = 2,
x4 = −2
is produced. When x2 = π and x4 =
√
2, then another particular solution
x1 = −2π −
√
2,
x2 = π,
x3 = −
√
2,
x4 =
√
2
is generated.
Rather than describing the solution set as illustrated in (2.4.3), future de-
velopments will make it more convenient to express the solution set by writing



x1
x2
x3
x4


=



−2x2 −x4
x2
−x4
x4


= x2



−2
1
0
0


+ x4



−1
0
−1
1



(2.4.4)

2.4 Homogeneous Systems
59
with the understanding that x2 and x4 are free variables that can range over
all possible numbers. This representation will be called the general solution
of the homogeneous system. This expression for the general solution emphasizes
that every solution is some combination of the two particular solutions
h1 =



−2
1
0
0



and
h2 =



−1
0
−1
1


.
The fact that h1 and h2 are each solutions is clear because h1 is produced
when the free variables assume the values x2 = 1 and x4 = 0, whereas the
solution h2 is generated when x2 = 0 and x4 = 1.
Now consider a general homogeneous system [A|0] of m linear equations
in n unknowns. If the coeﬃcient matrix is such that rank (A) = r, then it
should be apparent from the preceding discussion that there will be exactly r
basic variables—corresponding to the positions of the basic columns in A —and
exactly n −r free variables—corresponding to the positions of the nonbasic
columns in A . Reducing A to a row echelon form using Gaussian elimination
and then using back substitution to solve for the basic variables in terms of the
free variables produces the general solution, which has the form
x = xf1h1 + xf2h2 + · · · + xfn−rhn−r,
(2.4.5)
where xf1, xf2, . . . , xfn−r are the free variables and where h1, h2, . . . , hn−r are
n × 1 columns that represent particular solutions of the system. As the free
variables xfi range over all possible values, the general solution generates all
possible solutions.
The general solution does not depend on which row echelon form is used
in the sense that using back substitution to solve for the basic variables in
terms of the nonbasic variables generates a unique set of particular solutions
{h1, h2, . . . , hn−r}, regardless of which row echelon form is used. Without going
into great detail, one can argue that this is true because using back substitution
in any row echelon form to solve for the basic variables must produce exactly
the same result as that obtained by completely reducing A to EA and then
solving the reduced homogeneous system for the basic variables. Uniqueness of
EA guarantees the uniqueness of the hi’s.
For example, if the coeﬃcient matrix A associated with the system (2.4.1)
is completely reduced by the Gauss–Jordan procedure to EA
A =


1
2
2
3
2
4
1
3
3
6
1
4

−→


1
2
0
1
0
0
1
1
0
0
0
0

= EA,

60
Chapter 2
Rectangular Systems and Echelon Forms
then we obtain the following reduced system:
x1 + 2x2 + x4 = 0,
x3 + x4 = 0.
Solving for the basic variables x1 and x3 in terms of x2 and x4 produces
exactly the same result as given in (2.4.3) and hence generates exactly the same
general solution as shown in (2.4.4).
Because it avoids the back substitution process, you may ﬁnd it more con-
venient to use the Gauss–Jordan procedure to reduce A completely to EA
and then construct the general solution directly from the entries in EA. This
approach usually will be adopted in the examples and exercises.
As was previously observed, all homogeneous systems are consistent because
the trivial solution consisting of all zeros is always one solution. The natural
question is, “When is the trivial solution the only
solution?” In other words,
we wish to know when a homogeneous system possesses a unique solution. The
form of the general solution (2.4.5) makes the answer transparent. As long as
there is at least one free variable, then it is clear from (2.4.5) that there will
be an inﬁnite number of solutions. Consequently, the trivial solution is the only
solution if and only if there are no free variables. Because the number of free
variables is given by n −r, where r = rank (A), the previous statement can be
reformulated to say that a homogeneous system possesses a unique solution—the
trivial solution—if and only if rank (A) = n.
Example 2.4.1
The homogeneous system
x1 + 2x2 + 2x3 = 0,
2x1 + 5x2 + 7x3 = 0,
3x1 + 6x2 + 8x3 = 0,
has only the trivial solution because
A =


1
2
2
2
5
7
3
6
8

−→


1
2
2
0
1
3
0
0
2

= E
shows that rank (A) = n = 3. Indeed, it is also obvious from E that applying
back substitution in the system [E|0] yields only the trivial solution.
Example 2.4.2
Problem: Explain why the following homogeneous system has inﬁnitely many
solutions, and exhibit the general solution:
x1 + 2x2 + 2x3 = 0,
2x1 + 5x2 + 7x3 = 0,
3x1 + 6x2 + 6x3 = 0.

2.4 Homogeneous Systems
61
Solution:
A =


1
2
2
2
5
7
3
6
6

−→


1
2
2
0
1
3
0
0
0

= E
shows that rank (A) = 2 < n = 3. Since the basic columns lie in positions
one and two, x1 and x2 are the basic variables while x3 is free. Using back
substitution on [E|0] to solve for the basic variables in terms of the free variable
produces x2 = −3x3 and x1 = −2x2 −2x3 = 4x3, so the general solution is


x1
x2
x3

= x3


4
−3
1

,
where
x3 is free.
That is, every solution is a multiple of the one particular solution h1 =


4
−3
1

.
Summary
Let Am×n be the coeﬃcient matrix for a homogeneous system of m
linear equations in n unknowns, and suppose rank (A) = r.
•
The unknowns that correspond to the positions of the basic columns
(i.e., the pivotal positions) are called the basic variables, and the
unknowns corresponding to the positions of the nonbasic columns
are called the free variables.
•
There are exactly r basic variables and n −r free variables.
•
To describe all solutions, reduce A to a row echelon form using
Gaussian elimination, and then use back substitution to solve for
the basic variables in terms of the free variables. This produces the
general solution that has the form
x = xf1h1 + xf2h2 + · · · + xfn−rhn−r,
where the terms xf1, xf2, . . . , xfn−r are the free variables and where
h1, h2, . . . , hn−r are n × 1 columns that represent particular solu-
tions of the homogeneous system. The hi ’s are independent of which
row echelon form is used in the back substitution process. As the free
variables xfi range over all possible values, the general solution gen-
erates all possible solutions.
•
A homogeneous system possesses a unique solution (the trivial solu-
tion) if and only if rank (A) = n —i.e., if and only if there are no
free variables.

62
Chapter 2
Rectangular Systems and Echelon Forms
Exercises for section 2.4
2.4.1. Determine the general solution for each of the following homogeneous
systems.
(a)
x1 + 2x2 + x3 + 2x4 = 0,
2x1 + 4x2 + x3 + 3x4 = 0,
3x1 + 6x2 + x3 + 4x4 = 0.
(b)
2x + y + z = 0,
4x + 2y + z = 0,
6x + 3y + z = 0,
8x + 4y + z = 0.
(c)
x1 + x2 + 2x3
= 0,
3x1
+ 3x3 + 3x4 = 0,
2x1 + x2 + 3x3 + x4 = 0,
x1 + 2x2 + 3x3 −x4 = 0.
(d)
2x + y + z = 0,
4x + 2y + z = 0,
6x + 3y + z = 0,
8x + 5y + z = 0.
2.4.2. Among all solutions that satisfy the homogeneous system
x + 2y + z = 0,
2x + 4y + z = 0,
x + 2y −z = 0,
determine those that also satisfy the nonlinear constraint y −xy = 2z.
2.4.3. Consider a homogeneous system whose coeﬃcient matrix is
A =





1
2
1
3
1
2
4
−1
3
8
1
2
3
5
7
2
4
2
6
2
3
6
1
7
−3




.
First transform A to an unreduced row echelon form to determine the
general solution of the associated homogeneous system. Then reduce A
to EA, and show that the same general solution is produced.
2.4.4. If A is the coeﬃcient matrix for a homogeneous system consisting of
four equations in eight unknowns and if there are ﬁve free variables,
what is rank (A)?

2.4 Homogeneous Systems
63
2.4.5. Suppose that A is the coeﬃcient matrix for a homogeneous system of
four equations in six unknowns and suppose that A has at least one
nonzero row.
(a)
Determine the fewest number of free variables that are possible.
(b)
Determine the maximum number of free variables that are pos-
sible.
2.4.6. Explain why a homogeneous system of m equations in n unknowns
where m < n must always possess an inﬁnite number of solutions.
2.4.7. Construct a homogeneous system of three equations in four unknowns
that has
x2



−2
1
0
0


+ x4



−3
0
2
1



as its general solution.
2.4.8. If c1 and c2 are columns that represent two particular solutions of
the same homogeneous system, explain why the sum c1 + c2 must also
represent a solution of this system.

64
Chapter 2
Rectangular Systems and Echelon Forms
2.5
NONHOMOGENEOUS SYSTEMS
Recall that a system of m linear equations in n unknowns
a11x1 +
a12x2 + · · · +
a1nxn =
b1,
a21x1 +
a22x2 + · · · +
a2nxn =
b2,
...
am1x1 + am2x2 + · · · + amnxn = bm,
is said to be nonhomogeneous whenever bi ̸= 0 for at least one i. Unlike
homogeneous systems, a nonhomogeneous system may be inconsistent and the
techniques of §2.3 must be applied in order to determine if solutions do indeed
exist. Unless otherwise stated, it is assumed that all systems in this section are
consistent.
To describe the set of all possible solutions of a consistent nonhomogeneous
system, construct a general solution by exactly the same method used for homo-
geneous systems as follows.
•
Use Gaussian elimination to reduce the associated augmented matrix [A|b]
to a row echelon form [E|c].
•
Identify the basic variables and the free variables in the same manner de-
scribed in §2.4.
•
Apply back substitution to [E|c] and solve for the basic variables in terms
of the free variables.
•
Write the result in the form
x = p + xf1h1 + xf2h2 + · · · + xfn−rhn−r,
(2.5.1)
where xf1, xf2, . . . , xfn−r are the free variables and p, h1, h2, . . . , hn−r are
n × 1 columns. This is the general solution of the nonhomogeneous system.
As the free variables xfi range over all possible values, the general solu-
tion (2.5.1) generates all possible solutions of the system [A|b]. Just as in the
homogeneous case, the columns hi and p are independent of which row eche-
lon form [E|c] is used. Therefore, [A|b] may be completely reduced to E[A|b]
by using the Gauss–Jordan method thereby avoiding the need to perform back
substitution. We will use this approach whenever it is convenient.
The diﬀerence between the general solution of a nonhomogeneous system
and the general solution of a homogeneous system is the column p that appears

2.5 Nonhomogeneous Systems
65
in (2.5.1). To understand why p appears and where it comes from, consider the
nonhomogeneous system
x1 + 2x2 + 2x3 + 3x4 = 4,
2x1 + 4x2 + x3 + 3x4 = 5,
3x1 + 6x2 + x3 + 4x4 = 7,
(2.5.2)
in which the coeﬃcient matrix is the same as the coeﬃcient matrix for the
homogeneous system (2.4.1) used in the previous section. If [A|b] is completely
reduced by the Gauss–Jordan procedure to E[A|b]
[A|b] =


1
2
2
3
4
2
4
1
3
5
3
6
1
4
7

−→


1
2
0
1
2
0
0
1
1
1
0
0
0
0
0

= E[A|b],
then the following reduced system is obtained:
x1 + 2x2 + x4 = 2,
x3 + x4 = 1.
Solving for the basic variables, x1 and x3, in terms of the free variables, x2
and x4, produces
x1 = 2 −2x2 −x4,
x2 is “free,”
x3 = 1 −x4,
x4 is “free.”
The general solution is obtained by writing these statements in the form



x1
x2
x3
x4


=



2 −2x2 −x4
x2
1 −x4
x4


=



2
0
1
0


+ x2



−2
1
0
0


+ x4



−1
0
−1
1


.
(2.5.3)
As the free variables x2 and x4 range over all possible numbers, this generates
all possible solutions of the nonhomogeneous system (2.5.2). Notice that the
column



2
0
1
0


in (2.5.3) is a particular solution of the nonhomogeneous system
(2.5.2)—it is the solution produced when the free variables assume the values
x2 = 0 and x4 = 0.

66
Chapter 2
Rectangular Systems and Echelon Forms
Furthermore, recall from (2.4.4) that the general solution of the associated
homogeneous system
x1 + 2x2 + 2x3 + 3x4 = 0,
2x1 + 4x2 + x3 + 3x4 = 0,
3x1 + 6x2 + x3 + 4x4 = 0,
(2.5.4)
is given by



−2x2 −x4
x2
−x4
x4


= x2



−2
1
0
0


+ x4



−1
0
−1
1


.
That is, the general solution of the associated homogeneous system (2.5.4) is a
part of the general solution of the original nonhomogeneous system (2.5.2).
These two observations can be combined by saying that the general solution
of the nonhomogeneous system is given by a particular solution plus the general
solution of the associated homogeneous system.
14
To see that the previous statement is always true, suppose [A|b] represents
a general m × n consistent system where rank (A) = r. Consistency guarantees
that b is a nonbasic column in [A|b], and hence the basic columns in [A|b] are
in the same positions as the basic columns in [A|0] so that the nonhomogeneous
system and the associated homogeneous system have exactly the same set of basic
variables as well as free variables. Furthermore, it is not diﬃcult to see that
E[A|0] = [EA|0]
and
E[A|b] = [EA|c],
where c is some column of the form c =









ξ1
...
ξr
0
...
0









. This means that if you solve
the ith equation in the reduced homogeneous system for the ith basic variable
xbi in terms of the free variables xfi, xfi+1, . . . , xfn−r to produce
xbi = αixfi + αi+1xfi+1 + · · · + αn−rxfn−r,
(2.5.5)
then the solution for the ith basic variable in the reduced nonhomogeneous
system must have the form
xbi = ξi + αixfi + αi+1xfi+1 + · · · + αn−rxfn−r.
(2.5.6)
14
For those students who have studied diﬀerential equations, this statement should have a familiar
ring. Exactly the same situation holds for the general solution to a linear diﬀerential equation.
This is no accident—it is due to the inherent linearity in both problems. More will be said
about this issue later in the text.

2.5 Nonhomogeneous Systems
67
That is, the two solutions diﬀer only in the fact that the latter contains the
constant ξi. Consider organizing the expressions (2.5.5) and (2.5.6) so as to
construct the respective general solutions. If the general solution of the homoge-
neous system has the form
x = xf1h1 + xf2h2 + · · · + xfn−rhn−r,
then it is apparent that the general solution of the nonhomogeneous system must
have a similar form
x = p + xf1h1 + xf2h2 + · · · + xfn−rhn−r
(2.5.7)
in which the column p contains the constants ξi along with some 0’s—the ξi ’s
occupy positions in p that correspond to the positions of the basic columns, and
0’s occupy all other positions. The column p represents one particular solution
to the nonhomogeneous system because it is the solution produced when the free
variables assume the values xf1 = xf2 = · · · = xfn−r = 0.
Example 2.5.1
Problem: Determine the general solution of the following nonhomogeneous sys-
tem and compare it with the general solution of the associated homogeneous
system:
x1 + x2 + 2x3 + 2x4 + x5 = 1,
2x1 + 2x2 + 4x3 + 4x4 + 3x5 = 1,
2x1 + 2x2 + 4x3 + 4x4 + 2x5 = 2,
3x1 + 5x2 + 8x3 + 6x4 + 5x5 = 3.
Solution: Reducing the augmented matrix [A|b] to E[A|b] yields
A =



1
1
2
2
1
1
2
2
4
4
3
1
2
2
4
4
2
2
3
5
8
6
5
3


−→



1
1
2
2
1
1
0
0
0
0
1
−1
0
0
0
0
0
0
0
2
2
0
2
0



−→



1
1
2
2
1
1
0
2
2
0
2
0
0
0
0
0
1
−1
0
0
0
0
0
0


−→



1
1
2
2
1
1
0
1
1
0
1
0
0
0
0
0
1
−1
0
0
0
0
0
0



−→



1
0
1
2
0
1
0
1
1
0
1
0
0
0
0
0
1
−1
0
0
0
0
0
0


−→



1
0
1
2
0
1
0
1
1
0
0
1
0
0
0
0
1
−1
0
0
0
0
0
0


= E[A|b].

68
Chapter 2
Rectangular Systems and Echelon Forms
Observe that the system is indeed consistent because the last column is nonbasic.
Solve the reduced system for the basic variables x1, x2, and x5 in terms of the
free variables x3 and x4 to obtain
x1 = 1 −x3 −2x4,
x2 = 1 −x3,
x3 is “free,”
x4 is “free,”
x5 = −1.
The general solution to the nonhomogeneous system is
x =





x1
x2
x3
x4
x5




=





1 −x3 −2x4
1 −x3
x3
x4
−1




=





1
1
0
0
−1




+ x3





−1
−1
1
0
0




+ x4





−2
0
0
1
0




.
The general solution of the associated homogeneous system is
x =





x1
x2
x3
x4
x5




=





−x3 −2x4
−x3
x3
x4
0




= x3





−1
−1
1
0
0




+ x4





−2
0
0
1
0




.
You should verify for yourself that
p =





1
1
0
0
−1





is indeed a particular solution to the nonhomogeneous system and that
h3 =





−1
−1
1
0
0





and
h4 =





−2
0
0
1
0





are particular solutions to the associated homogeneous system.

2.5 Nonhomogeneous Systems
69
Now turn to the question, “When does a consistent system have a unique
solution?” It is known from (2.5.7) that the general solution of a consistent
m × n nonhomogeneous system [A|b] with rank (A) = r is given by
x = p + xf1h1 + xf2h2 + · · · + xfn−rhn−r,
where
xf1h1 + xf2h2 + · · · + xfn−rhn−r
is the general solution of the associated homogeneous system. Consequently, it
is evident that the nonhomogeneous system [A|b] will have a unique solution
(namely, p ) if and only if there are no free variables—i.e., if and only if r = n
(= number of unknowns)—this is equivalent to saying that the associated ho-
mogeneous system [A|0] has only the trivial solution.
Example 2.5.2
Consider the following nonhomogeneous system:
2x1 + 4x2 + 6x3 = 2,
x1 + 2x2 + 3x3 = 1,
x1
+ x3 = −3,
2x1 + 4x2
= 8.
Reducing [A|b] to E[A|b] yields
[A|b] =



2
4
6
2
1
2
3
1
1
0
1
−3
2
4
0
8


−→



1
0
0
−2
0
1
0
3
0
0
1
−1
0
0
0
0


= E[A|b].
The system is consistent because the last column is nonbasic. There are several
ways to see that the system has a unique solution. Notice that
rank (A) = 3 = number of unknowns,
which is the same as observing that there are no free variables. Furthermore,
the associated homogeneous system clearly has only the trivial solution. Finally,
because we completely reduced [A|b] to E[A|b], it is obvious that there is only
one solution possible and that it is given by p =


−2
3
−1

.

70
Chapter 2
Rectangular Systems and Echelon Forms
Summary
Let [A|b] be the augmented matrix for a consistent m × n nonhomo-
geneous system in which rank (A) = r.
•
Reducing [A|b] to a row echelon form using Gaussian elimination
and then solving for the basic variables in terms of the free variables
leads to the general solution
x = p + xf1h1 + xf2h2 + · · · + xfn−rhn−r.
As the free variables xfi range over all possible values, this general
solution generates all possible solutions of the system.
•
Column p is a particular solution of the nonhomogeneous system.
•
The expression xf1h1 + xf2h2 + · · · + xfn−rhn−r is the general so-
lution of the associated homogeneous system.
•
Column p as well as the columns hi are independent of the row
echelon form to which [A|b] is reduced.
•
The system possesses a unique solution if and only if any of the
following is true.
▷
rank (A) = n = number of unknowns.
▷
There are no free variables.
▷
The associated homogeneous system possesses only the trivial
solution.
Exercises for section 2.5
2.5.1. Determine the general solution for each of the following nonhomogeneous
systems.
(a)
x1 + 2x2 + x3 + 2x4 = 3,
2x1 + 4x2 + x3 + 3x4 = 4,
3x1 + 6x2 + x3 + 4x4 = 5.
(b)
2x + y + z = 4,
4x + 2y + z = 6,
6x + 3y + z = 8,
8x + 4y + z = 10.
(c)
x1 + x2 + 2x3
= 1,
3x1
+ 3x3 + 3x4 = 6,
2x1 + x2 + 3x3 + x4 = 3,
x1 + 2x2 + 3x3 −x4 = 0.
(d)
2x + y + z = 2,
4x + 2y + z = 5,
6x + 3y + z = 8,
8x + 5y + z = 8.

2.5 Nonhomogeneous Systems
71
2.5.2. Among the solutions that satisfy the set of linear equations
x1 + x2 + 2x3 + 2x4 + x5 = 1,
2x1 + 2x2 + 4x3 + 4x4 + 3x5 = 1,
2x1 + 2x2 + 4x3 + 4x4 + 2x5 = 2,
3x1 + 5x2 + 8x3 + 6x4 + 5x5 = 3,
ﬁnd all those that also satisfy the following two constraints:
(x1 −x2)2 −4x2
5 = 0,
x2
3 −x2
5 = 0.
2.5.3. In order to grow a certain crop, it is recommended that each square foot
of ground be treated with 10 units of phosphorous, 9 units of potassium,
and 19 units of nitrogen. Suppose that there are three brands of fertilizer
on the market—say brand X, brand Y, and brand Z. One pound of
brand X contains 2 units of phosphorous, 3 units of potassium, and 5
units of nitrogen. One pound of brand Y contains 1 unit of phosphorous,
3 units of potassium, and 4 units of nitrogen. One pound of brand Z
contains only 1 unit of phosphorous and 1 unit of nitrogen.
(a)
Take into account the obvious fact that a negative number of
pounds of any brand can never be applied, and suppose that
because of the way fertilizer is sold only an integral number of
pounds of each brand will be applied. Under these constraints,
determine all possible combinations of the three brands that can
be applied to satisfy the recommendations exactly.
(b)
Suppose that brand X costs $1 per pound, brand Y costs $6
per pound, and brand Z costs $3 per pound. Determine the
least expensive solution that will satisfy the recommendations
exactly as well as the constraints of part (a).
2.5.4. Consider the following system:
2x + 2y + 3z = 0,
4x + 8y + 12z = −4,
6x + 2y + αz = 4.
(a)
Determine all values of α for which the system is consistent.
(b)
Determine all values of α for which there is a unique solution,
and compute the solution for these cases.
(c)
Determine all values of α for which there are inﬁnitely many
diﬀerent solutions, and give the general solution for these cases.

72
Chapter 2
Rectangular Systems and Echelon Forms
2.5.5. If columns s1 and s2 are particular solutions of the same nonhomo-
geneous system, must it be the case that the sum s1 + s2 is also a
solution?
2.5.6. Suppose that [A|b] is the augmented matrix for a consistent system of
m equations in n unknowns where m ≥n. What must EA look like
when the system possesses a unique solution?
2.5.7. Construct a nonhomogeneous system of three equations in four un-
knowns that has



1
0
1
0


+ x2



−2
1
0
0


+ x4



−3
0
2
1



as its general solution.
2.5.8. Consider using ﬂoating-point arithmetic (without partial pivoting or
scaling) to solve the system represented by the following augmented
matrix:


.835
.667
.5
.168
.333
.266
.1994
.067
1.67
1.334
1.1
.436

.
(a)
Determine the 4-digit general solution.
(b)
Determine the 5-digit general solution.
(c)
Determine the 6-digit general solution.

2.6 Electrical Circuits
73
2.6
ELECTRICAL CIRCUITS
The theory of electrical circuits is an important application that naturally gives
rise to rectangular systems of linear equations. Because the underlying mathe-
matics depends on several of the concepts discussed in the preceding sections,
you may ﬁnd it interesting and worthwhile to make a small excursion into the
elementary mathematical analysis of electrical circuits. However, the continuity
of the text is not compromised by omitting this section.
In a direct current circuit containing resistances and sources of electromo-
tive force (abbreviated EMF) such as batteries, a point at which three or more
conductors are joined is called a node or branch point of the circuit, and a
closed conduction path is called a loop. Any part of a circuit between two ad-
joining nodes is called a branch of the circuit. The circuit shown in Figure 2.6.1
is a typical example that contains four nodes, seven loops, and six branches.
1
2
3
4
R1
R6
R5
R4
R3
R2
E4
E3
E2
E1
I2
I4
I1
I5
I6
I3
A
B
C
Figure 2.6.1
The problem is to relate the currents Ik in each branch to the resistances Rk
and the EMFs Ek.
15 This is accomplished by using Ohm’s law in conjunction
with Kirchhoﬀ’s rules to produce a system of linear equations.
Ohm’s Law
Ohm’s law states that for a current of I amps, the voltage drop (in
volts) across a resistance of R ohms is given by V = IR.
Kirchhoﬀ’s rules—formally stated below—are the two fundamental laws
that govern the study of electrical circuits.
15
For an EMF source of magnitude E and a current I, there is always a small internal resistance
in the source, and the voltage drop across it is V = E−I ×(internal resistance). But internal
source resistance is usually negligible, so the voltage drop across the source can be taken as
V = E. When internal resistance cannot be ignored, its eﬀects may be incorporated into
existing external resistances, or it can be treated as a separate external resistance.

74
Chapter 2
Rectangular Systems and Echelon Forms
Kirchhoff’s Rules
NODE RULE:
The algebraic sum of currents toward each node is zero.
That is, the total incoming current must equal the total
outgoing current. This is simply a statement of conser-
vation of charge.
LOOP RULE:
The algebraic sum of the EMFs around each loop must
equal the algebraic sum of the IR products in the same
loop. That is, assuming internal source resistances have
been accounted for, the algebraic sum of the voltage
drops over the sources equals the algebraic sum of the
voltage drops over the resistances in each loop. This is
a statement of conservation of energy.
Kirchhoﬀ’s rules may be used without knowing the directions of the currents
and EMFs in advance. You may arbitrarily assign directions. If negative values
emerge in the ﬁnal solution, then the actual direction is opposite to that assumed.
To apply the node rule, consider a current to be positive if its direction is toward
the node—otherwise, consider the current to be negative. It should be clear that
the node rule will always generate a homogeneous system. For example, applying
the node rule to the circuit in Figure 2.6.1 yields four homogeneous equations in
six unknowns—the unknowns are the Ik ’s:
Node 1:
I1 −I2 −I5 = 0,
Node 2:
−I1 −I3 + I4 = 0,
Node 3:
I3 + I5 + I6 = 0,
Node 4:
I2 −I4 −I6 = 0.
To apply the loop rule, some direction (clockwise or counterclockwise) must
be chosen as the positive direction, and all EMFs and currents in that direction
are considered positive and those in the opposite direction are negative. It is
possible for a current to be considered positive for the node rule but considered
negative when it is used in the loop rule. If the positive direction is considered
to be clockwise in each case, then applying the loop rule to the three indicated
loops A, B, and C in the circuit shown in Figure 2.6.1 produces the three non-
homogeneous equations in six unknowns—the Ik ’s are treated as the unknowns,
while the Rk’s and Ek’s are assumed to be known.
Loop A:
I1R1 −I3R3 + I5R5 = E1 −E3,
Loop B:
I2R2 −I5R5 + I6R6 = E2,
Loop C:
I3R3 + I4R4 −I6R6 = E3 + E4.

2.6 Electrical Circuits
75
There are 4 additional loops that also produce loop equations thereby mak-
ing a total of 11 equations (4 nodal equations and 7 loop equations) in 6 un-
knowns. Although this appears to be a rather general 11 × 6 system of equations,
it really is not. If the circuit is in a state of equilibrium, then the physics of the
situation dictates that for each set of EMFs Ek, the corresponding currents
Ik must be uniquely determined. In other words, physics guarantees that the
11 × 6 system produced by applying the two Kirchhoﬀrules must be consistent
and possess a unique solution.
Suppose that [A|b] represents the augmented matrix for the 11 × 6 system
generated by Kirchhoﬀ’s rules. From the results in §2.5, we know that the system
has a unique solution if and only if
rank (A) = number of unknowns = 6.
Furthermore, it was demonstrated in §2.3 that the system is consistent if and
only if
rank[A|b] = rank (A).
Combining these two facts allows us to conclude that
rank[A|b] = 6
so that when [A|b] is reduced to E[A|b], there will be exactly 6 nonzero rows
and 5 zero rows. Therefore, 5 of the original 11 equations are redundant in the
sense that they can be “zeroed out” by forming combinations of some particular
set of 6 “independent” equations. It is desirable to know beforehand which of
the 11 equations will be redundant and which can act as the “independent” set.
Notice that in using the node rule, the equation corresponding to node 4
is simply the negative sum of the equations for nodes 1, 2, and 3, and that the
ﬁrst three equations are independent in the sense that no one of the three can
be written as a combination of any other two. This situation is typical. For a
general circuit with n nodes, it can be demonstrated that the equations for
the ﬁrst n −1 nodes are independent, and the equation for the last node is
redundant.
The loop rule also can generate redundant equations. Only simple loops—
loops not containing smaller loops—give rise to independent equations. For ex-
ample, consider the loop consisting of the three exterior branches in the circuit
shown in Figure 2.6.1. Applying the loop rule to this large loop will produce
no new information because the large loop can be constructed by “adding” the
three simple loops A,
B, and C contained within. The equation associated
with the large outside loop is
I1R1 + I2R2 + I4R4 = E1 + E2 + E4,
which is precisely the sum of the equations that correspond to the three compo-
nent loops A, B, and C. This phenomenon will hold in general so that only
the simple loops need to be considered when using the loop rule.

76
Chapter 2
Rectangular Systems and Echelon Forms
The point of this discussion is to conclude that the more general 11 × 6
rectangular system can be replaced by an equivalent 6 × 6 square system that
has a unique solution by dropping the last nodal equation and using only the
simple loop equations. This is characteristic of practical work in general. The
physics of a problem together with natural constraints can usually be employed
to replace a general rectangular system with one that is square and possesses a
unique solution.
One of the goals in our study is to understand more clearly the notion of
“independence” that emerged in this application. So far, independence has been
an intuitive idea, but this example helps make it clear that independence is a
fundamentally important concept that deserves to be nailed down more ﬁrmly.
This is done in §4.3, and the general theory for obtaining independent equations
from electrical circuits is developed in Examples 4.4.6 and 4.4.7.
Exercises for section 2.6
2.6.1. Suppose that Ri = i ohms and Ei = i volts in the circuit shown in
Figure 2.6.1.
(a)
Determine the six indicated currents.
(b)
Select node number 1 to use as a reference point and ﬁx its
potential to be 0 volts. With respect to this reference, calculate
the potentials at the other three nodes. Check your answer by
verifying the loop rule for each loop in the circuit.
2.6.2. Determine the three currents indicated in the following circuit.
5Ω
8Ω
1Ω
1Ω
10Ω
9 volts
12 volts
I1
I2
I3
2.6.3. Determine the two unknown EMFs in the following circuit.
1 amp
2 amps
20 volts
E1
E2
6Ω
4Ω
2Ω

2.6 Electrical Circuits
77
2.6.4. Consider the circuit shown below and answer the following questions.
R1
R2
R3
R4
R5
R6
I
E
(a)
How many nodes does the circuit contain?
(b)
How many branches does the circuit contain?
(c)
Determine the total number of loops and then determine the
number of simple loops.
(d)
Demonstrate that the simple loop equations form an “indepen-
dent” system of equations in the sense that there are no redun-
dant equations.
(e)
Verify that any three of the nodal equations constitute an “in-
dependent” system of equations.
(f)
Verify that the loop equation associated with the loop containing
R1, R2, R3, and R4 can be expressed as the sum of the two
equations associated with the two simple loops contained in the
larger loop.
(g)
Determine the indicated current I if R1 = R2 = R3 = R4 = 1
ohm, R5 = R6 = 5 ohms, and E = 5 volts.

CHAPTER 3
Matrix
Algebra
3.1
FROM ANCIENT CHINA TO ARTHUR CAYLEY
The ancient Chinese appreciated the advantages of array manipulation in dealing
with systems of linear equations, and they possessed the seed that might have
germinated into a genuine theory of matrices. Unfortunately, in the year 213
B.C., emperor Shih Hoang-ti ordered that “all books be burned and all scholars
be buried.” It is presumed that the emperor wanted all knowledge and written
records to begin with him and his regime. The edict was carried out, and it will
never be known how much knowledge was lost. The book Chiu-chang Suan-shu
(Nine Chapters on Arithmetic), mentioned in the introduction to Chapter 1, was
compiled on the basis of remnants that survived.
More than a millennium passed before further progress was documented.
The Chinese counting board with its colored rods and its applications involving
array manipulation to solve linear systems eventually found its way to Japan.
Seki Kowa (1642–1708), whom many Japanese consider to be one of the greatest
mathematicians that their country has produced, carried forward the Chinese
principles involving “rule of thumb” elimination methods on arrays of numbers.
His understanding of the elementary operations used in the Chinese elimination
process led him to formulate the concept of what we now call the determinant.
While formulating his ideas concerning the solution of linear systems, Seki Kowa
anticipated the fundamental concepts of array operations that today form the
basis for matrix algebra. However, there is no evidence that he developed his
array operations to actually construct an algebra for matrices.
From the middle 1600s to the middle 1800s, while Europe was ﬂowering
in mathematical development, the study of array manipulation was exclusively

80
Chapter 3
Matrix Algebra
dedicated to the theory of determinants. Curiously, matrix algebra did not evolve
along with the study of determinants.
It was not until the work of the British mathematician Arthur Cayley (1821–
1895) that the matrix was singled out as a separate entity, distinct from the
notion of a determinant, and algebraic operations between matrices were deﬁned.
In an 1855 paper, Cayley ﬁrst introduced his basic ideas that were presented
mainly to simplify notation. Finally, in 1857, Cayley expanded on his original
ideas and wrote A Memoir on the Theory of Matrices. This laid the foundations
for the modern theory and is generally credited for being the birth of the subjects
of matrix analysis and linear algebra.
Arthur Cayley began his career by studying literature at Trinity College,
Cambridge (1838–1842), but developed a side interest in mathematics, which he
studied in his spare time. This “hobby” resulted in his ﬁrst mathematical paper
in 1841 when he was only 20 years old. To make a living, he entered the legal
profession and practiced law for 14 years. However, his main interest was still
mathematics. During the legal years alone, Cayley published almost 300 papers
in mathematics.
In 1850 Cayley crossed paths with James J. Sylvester, and between the two
of them matrix theory was born and nurtured. The two have been referred to
as the “invariant twins.” Although Cayley and Sylvester shared many mathe-
matical interests, they were quite diﬀerent people, especially in their approach
to mathematics. Cayley had an insatiable hunger for the subject, and he read
everything that he could lay his hands on. Sylvester, on the other hand, could
not stand the sight of papers written by others. Cayley never forgot anything
he had read or seen—he became a living encyclopedia. Sylvester, so it is said,
would frequently fail to remember even his own theorems.
In 1863, Cayley was given a chair in mathematics at Cambridge University,
and thereafter his mathematical output was enormous. Only Cauchy and Euler
were as proliﬁc. Cayley often said, “I really love my subject,” and all indica-
tions substantiate that this was indeed the way he felt. He remained a working
mathematician until his death at age 74.
Because the idea of the determinant preceded concepts of matrix algebra by
at least two centuries, Morris Kline says in his book Mathematical Thought from
Ancient to Modern Times that “the subject of matrix theory was well developed
before it was created.” This must have indeed been the case because immediately
after the publication of Cayley’s memoir, the subjects of matrix theory and linear
algebra virtually exploded and quickly evolved into a discipline that now occupies
a central position in applied mathematics.

3.2 Addition and Transposition
81
3.2
ADDITION AND TRANSPOSITION
In the previous chapters, matrix language and notation were used simply to for-
mulate some of the elementary concepts surrounding linear systems. The purpose
now is to turn this language into a mathematical theory.
16
Unless otherwise stated, a scalar is a complex number. Real numbers are
a subset of the complex numbers, and hence real numbers are also scalar quan-
tities. In the early stages, there is little harm in thinking only in terms of real
scalars. Later on, however, the necessity for dealing with complex numbers will
be unavoidable. Throughout the text, ℜwill denote the set of real numbers,
and C will denote the complex numbers. The set of all n -tuples of real numbers
will be denoted by ℜn, and the set of all complex n -tuples will be denoted
by Cn. For example, ℜ2 is the set of all ordered pairs of real numbers (i.e.,
the standard cartesian plane), and ℜ3 is ordinary 3-space. Analogously, ℜm×n
and Cm×n denote the m × n matrices containing real numbers and complex
numbers, respectively.
Matrices A = [aij] and B = [bij] are deﬁned to be equal matrices
when A and B have the same shape and corresponding entries are equal. That
is, aij = bij for each i = 1, 2, . . . , m and j = 1, 2, . . . , n. In particular, this
deﬁnition applies to arrays such as u =


1
2
3

and v = ( 1
2
3 ) . Even
though u and v describe exactly the same point in 3-space, we cannot consider
them to be equal matrices because they have diﬀerent shapes. An array (or
matrix) consisting of a single column, such as u, is called a column vector,
while an array consisting of a single row, such as v, is called a row vector.
Addition of Matrices
If A and B are m × n matrices, the sum of A and B is deﬁned to
be the m × n matrix A+B obtained by adding corresponding entries.
That is,
[A + B]ij = [A]ij + [B]ij
for each i and j.
For example,

−2
x
3
z + 3
4
−y

+

2
1 −x
−2
−3
4 + x
4 + y

=

0
1
1
z
8 + x
4

.
16
The great French mathematician Pierre-Simon Laplace (1749–1827) said that, “Such is the ad-
vantage of a well-constructed language that its simpliﬁed notation often becomes the source of
profound theories.” The theory of matrices is a testament to the validity of Laplace’s statement.

82
Chapter 3
Matrix Algebra
The symbol “+” is used two diﬀerent ways—it denotes addition between
scalars in some places and addition between matrices at other places. Although
these are two distinct algebraic operations, no ambiguities will arise if the context
in which “+” appears is observed. Also note that the requirement that A and
B have the same shape prevents adding a row to a column, even though the two
may contain the same number of entries.
The matrix (−A), called the additive inverse of A, is deﬁned to be
the matrix obtained by negating each entry of A. That is, if A = [aij], then
−A = [−aij]. This allows matrix subtraction to be deﬁned in the natural way.
For two matrices of the same shape, the diﬀerence A −B is deﬁned to be the
matrix A −B = A + (−B) so that
[A −B]ij = [A]ij −[B]ij
for each i and j.
Since matrix addition is deﬁned in terms of scalar addition, the familiar algebraic
properties of scalar addition are inherited by matrix addition as detailed below.
Properties of Matrix Addition
For m × n matrices A, B, and C, the following properties hold.
Closure property:
A + B is again an m × n matrix.
Associative property:
(A + B) + C = A + (B + C).
Commutative property:
A + B = B + A.
Additive identity:
The m × n matrix 0 consisting of all ze-
ros has the property that A + 0 = A.
Additive inverse:
The m × n matrix (−A) has the property
that A + (−A) = 0.
Another simple operation that is derived from scalar arithmetic is as follows.
Scalar Multiplication
The product of a scalar α times a matrix A, denoted by αA, is deﬁned
to be the matrix obtained by multiplying each entry of A by α. That
is, [αA]ij = α[A]ij for each i and j.
For example,
2


1
2
3
0
1
2
1
4
2

=


2
4
6
0
2
4
2
8
4


and


1
2
3
4
0
1

= 1
2


2
4
6
8
0
2

.
The rules for combining addition and scalar multiplication are what you
might suspect they should be. Some of the important ones are listed below.

3.2 Addition and Transposition
83
Properties of Scalar Multiplication
For m × n matrices A and B and for scalars α and β, the following
properties hold.
Closure property:
αA is again an m × n matrix.
Associative property:
(αβ)A = α(βA).
Distributive property:
α(A + B) = αA + αB. Scalar multiplica-
tion is distributed over matrix addition.
Distributive property:
(α + β)A = αA + βA. Scalar multiplica-
tion is distributed over scalar addition.
Identity property:
1A = A. The number 1 is an identity el-
ement under scalar multiplication.
Other properties such as αA = Aα could have been listed, but the prop-
erties singled out pave the way for the deﬁnition of a vector space on p. 160.
A matrix operation that’s not derived from scalar arithmetic is transposition
as deﬁned below.
Transpose
The transpose of Am×n is deﬁned to be the n × m matrix AT ob-
tained by interchanging rows and columns in A. More precisely, if
A = [aij], then [AT ]ij = aji. For example,


1
2
3
4
5
6


T
=

1
3
5
2
4
6

.
It should be evident that for all matrices,

AT 	T = A.
Whenever a matrix contains complex entries, the operation of complex con-
jugation almost always accompanies the transpose operation. (Recall that the
complex conjugate of z = a + ib is deﬁned to be z = a −ib.)

84
Chapter 3
Matrix Algebra
Conjugate Transpose
For A = [aij], the conjugate matrix is deﬁned to be A = [aij] , and
the conjugate transpose of A is deﬁned to be ¯AT = AT . From now
on, ¯AT will be denoted by A∗, so [A∗]ij = aji. For example,

1 −4i
i
2
3
2 + i
0
∗
=


1 + 4i
3
−i
2 −i
2
0

.
(A∗)∗= A for all matrices, and A∗= AT whenever A contains only
real entries. Sometimes the matrix A∗is called the adjoint of A.
The transpose (and conjugate transpose) operation is easily combined with
matrix addition and scalar multiplication. The basic rules are given below.
Properties of the Transpose
If A and B are two matrices of the same shape, and if α is a scalar,
then each of the following statements is true.
(A + B)T = AT + BT
and
(A + B)∗= A∗+ B∗.
(3.2.1)
(αA)T = αAT
and
(αA)∗= αA∗.
(3.2.2)
Proof.
17
We will prove that (3.2.1) and (3.2.2) hold for the transpose operation.
The proofs of the statements involving conjugate transposes are similar and are
left as exercises. For each i and j, it is true that
[(A + B)T ]ij = [A + B]ji = [A]ji + [B]ji = [AT ]ij + [BT ]ij = [AT + BT ]ij.
17
Computers can outperform people in many respects in that they do arithmetic much faster
and more accurately than we can, and they are now rather adept at symbolic computation and
mechanical manipulation of formulas. But computers can’t do mathematics—people still hold
the monopoly. Mathematics emanates from the uniquely human capacity to reason abstractly
in a creative and logical manner, and learning mathematics goes hand-in-hand with learning
how to reason abstractly and create logical arguments. This is true regardless of whether your
orientation is applied or theoretical. For this reason, formal proofs will appear more frequently
as the text evolves, and it is expected that your level of comprehension as well as your ability
to create proofs will grow as you proceed.

3.2 Addition and Transposition
85
This proves that corresponding entries in (A + B)T and AT + BT are equal,
so it must be the case that (A + B)T = AT + BT . Similarly, for each i and j,
[(αA)T ]ij = [αA]ji = α[A]ji = α[AT ]ij
=⇒
(αA)T = αAT .
Sometimes transposition doesn’t change anything. For example, if
A =


1
2
3
2
4
5
3
5
6

,
then
AT = A.
This is because the entries in A are symmetrically located about the main di-
agonal—the line from the upper-left-hand corner to the lower-right-hand corner.
Matrices of the form D =


λ1
0
· · ·
0
0
λ2
· · ·
0
...
...
...
...
0
0
· · ·
λn

are called diagonal matrices,
and they are clearly symmetric in the sense that D = DT . This is one of several
kinds of symmetries described below.
Symmetries
Let A = [aij] be a square matrix.
•
A is said to be a symmetric matrix whenever A = AT , i.e.,
whenever aij = aji.
•
A is said to be a skew-symmetric matrix whenever A = −AT ,
i.e., whenever aij = −aji.
•
A is said to be a hermitian matrix
whenever A = A∗, i.e.,
whenever aij = aji. This is the complex analog of symmetry.
•
A is said to be a skew-hermitian matrix when A = −A∗, i.e.,
whenever aij = −aji. This is the complex analog of skew symmetry.
For example, consider
A =


1
2 + 4i
1 −3i
2 −4i
3
8 + 6i
1 + 3i
8 −6i
5


and
B =


1
2 + 4i
1 −3i
2 + 4i
3
8 + 6i
1 −3i
8 + 6i
5

.
Can you see that A is hermitian but not symmetric, while B is symmetric but
not hermitian?
Nature abounds with symmetry, and very often physical symmetry manifests
itself as a symmetric matrix in a mathematical model. The following example is
an illustration of this principle.

86
Chapter 3
Matrix Algebra
Example 3.2.1
Consider two springs that are connected as shown in Figure 3.2.1.
x1
x2
x3
k1
k2
Node 1
Node 2
Node 3
F1
-F1
F3
-F3
Figure 3.2.1
The springs at the top represent the “no tension” position in which no force is
being exerted on any of the nodes. Suppose that the springs are stretched or
compressed so that the nodes are displaced as indicated in the lower portion
of Figure 3.2.1. Stretching or compressing the springs creates a force on each
node according to Hooke’s law
18 that says that the force exerted by a spring
is F = kx, where x is the distance the spring is stretched or compressed and
where k is a stiﬀness constant inherent to the spring. Suppose our springs have
stiﬀness constants k1 and k2, and let Fi be the force on node i when the
springs are stretched or compressed. Let’s agree that a displacement to the left
is positive, while a displacement to the right is negative, and consider a force
directed to the right to be positive while one directed to the left is negative.
If node 1 is displaced x1 units, and if node 2 is displaced x2 units, then the
left-hand spring is stretched (or compressed) by a total amount of x1 −x2 units,
so the force on node 1 is
F1 = k1(x1 −x2).
Similarly, if node 2 is displaced x2 units, and if node 3 is displaced x3 units,
then the right-hand spring is stretched by a total amount of x2 −x3 units, so
the force on node 3 is
F3 = −k2(x2 −x3).
The minus sign indicates the force is directed to the left. The force on the left-
hand side of node 2 is the opposite of the force on node 1, while the force on the
right-hand side of node 2 must be the opposite of the force on node 3. That is,
F2 = −F1 −F3.
18
Hooke’s law is named for Robert Hooke (1635–1703), an English physicist, but it was generally
known to several people (including Newton) before Hooke’s 1678 claim to it was made. Hooke
was a creative person who is credited with several inventions, including the wheel barometer,
but he was reputed to be a man of “terrible character.” This characteristic virtually destroyed
his scientiﬁc career as well as his personal life. It is said that he lacked mathematical sophis-
tication and that he left much of his work in incomplete form, but he bitterly resented people
who built on his ideas by expressing them in terms of elegant mathematical formulations.

3.2 Addition and Transposition
87
Organize the above three equations as a linear system:
k1x1
−k1x2
= F1,
−k1x1 + (k1 + k2)x2 −k2x3 = F2,
−k2x2 + k2x3 = F3,
and observe that the coeﬃcient matrix, called the stiﬀness matrix,
K =


k1
−k1
0
−k1
k1 + k2
−k2
0
−k2
k2

,
is a symmetric matrix. The point of this example is that symmetry in the physical
problem translates to symmetry in the mathematics by way of the symmetric
matrix K. When the two springs are identical (i.e., when k1 = k2 = k ), even
more symmetry is present, and in this case
K = k


1
−1
0
−1
2
−1
0
−1
1

.
Exercises for section 3.2
3.2.1. Determine the unknown quantities in the following expressions.
(a)
3X =

0
3
6
9

.
(b)
2

x + 2
y + 3
3
0

=

3
6
y
z
T
.
3.2.2. Identify each of the following as symmetric, skew symmetric, or neither.
(a)


1
−3
3
−3
4
−3
3
3
0

.
(b)


0
−3
−3
3
0
1
3
−1
0

.
(c)


0
−3
−3
−3
0
3
−3
3
1

.
(d)

1
2
0
2
1
0

.
3.2.3. Construct an example of a 3 × 3 matrix A that satisﬁes the following
conditions.
(a)
A is both symmetric and skew symmetric.
(b)
A is both hermitian and symmetric.
(c)
A is skew hermitian.

88
Chapter 3
Matrix Algebra
3.2.4. Explain why the set of all n × n symmetric matrices is closed under
matrix addition. That is, explain why the sum of two n × n symmetric
matrices is again an n × n symmetric matrix. Is the set of all n × n
skew-symmetric matrices closed under matrix addition?
3.2.5. Prove that each of the following statements is true.
(a)
If A = [aij] is skew symmetric, then ajj = 0 for each j.
(b)
If A = [aij] is skew hermitian, then each ajj is a pure imagi-
nary number—i.e., a multiple of the imaginary unit i.
(c)
If A is real and symmetric, then B = iA is skew hermitian.
3.2.6. Let A be any square matrix.
(a)
Show that A+AT is symmetric and A−AT is skew symmetric.
(b)
Prove that there is one and only one way to write A as the
sum of a symmetric matrix and a skew-symmetric matrix.
3.2.7. If A and B are two matrices of the same shape, prove that each of the
following statements is true.
(a)
(A + B)∗= A∗+ B∗.
(b)
(αA)∗= αA∗.
3.2.8. Using the conventions given in Example 3.2.1, determine the stiﬀness
matrix for a system of n identical springs, with stiﬀness constant k,
connected in a line similar to that shown in Figure 3.2.1.

3.3 Linearity
89
3.3
LINEARITY
The concept of linearity is the underlying theme of our subject. In elementary
mathematics the term “linear function” refers to straight lines, but in higher
mathematics linearity means something much more general. Recall that a func-
tion f is simply a rule for associating points in one set D —called the domain
of f —to points in another set R —the range of f. A linear function is a
particular type of function that is characterized by the following two properties.
Linear Functions
Suppose that D and R are sets that possess an addition operation as
well as a scalar multiplication operation—i.e., a multiplication between
scalars and set members. A function f that maps points in D to points
in R is said to be a linear function whenever f satisﬁes the conditions
that
f(x + y) = f(x) + f(y)
(3.3.1)
and
f(αx) = αf(x)
(3.3.2)
for every x and y in D and for all scalars α. These two conditions
may be combined by saying that f is a linear function whenever
f(αx + y) = αf(x) + f(y)
(3.3.3)
for all scalars α and for all x, y ∈D.
One of the simplest linear functions is f(x) = αx, whose graph in ℜ2 is a
straight line through the origin. You should convince yourself that f is indeed
a linear function according to the above deﬁnition. However, f(x) = αx + β
does not qualify for the title “linear function”—it is a linear function that has
been translated by a constant β. Translations of linear functions are referred to
as aﬃne functions. Virtually all information concerning aﬃne functions can
be derived from an understanding of linear functions, and consequently we will
focus only on issues of linearity.
In ℜ3, the surface described by a function of the form
f(x1, x2) = α1x1 + α2x2
is a plane through the origin, and it is easy to verify that f is a linear function.
For β ̸= 0, the graph of f(x1, x2) = α1x1 + α2x2 + β is a plane not passing
through the origin, and f is no longer a linear function—it is an aﬃne function.

90
Chapter 3
Matrix Algebra
In ℜ2 and ℜ3, the graphs of linear functions are lines and planes through
the origin, and there seems to be a pattern forming. Although we cannot visualize
higher dimensions with our eyes, it seems reasonable to suggest that a general
linear function of the form
f(x1, x2, . . . , xn) = α1x1 + α2x2 + · · · + αnxn
somehow represents a “linear” or “ﬂat” surface passing through the origin 0 =
(0, 0, . . . , 0) in ℜn+1. One of the goals of the next chapter is to learn how to
better interpret and understand this statement.
Linearity is encountered at every turn. For example, the familiar operations
of diﬀerentiation and integration may be viewed as linear functions. Since
d(f + g)
dx
= df
dx + dg
dx
and
d(αf)
dx
= α df
dx,
the diﬀerentiation operator Dx(f) = df/dx is linear. Similarly,

(f + g)dx =

fdx +

gdx
and

αfdx = α

fdx
means that the integration operator I(f) =

fdx is linear.
There are several important matrix functions that are linear. For example,
the transposition function f(Xm×n) = XT is linear because
(A + B)T = AT + BT
and
(αA)T = αAT
(recall (3.2.1) and (3.2.2)). Another matrix function that is linear is the trace
function presented below.
Example 3.3.1
The trace of an n × n matrix A = [aij] is deﬁned to be the sum of the entries
lying on the main diagonal of A. That is,
trace (A) = a11 + a22 + · · · + ann =
n

i=1
aii.
Problem: Show that f(Xn×n) = trace (X) is a linear function.
Solution: Let’s be eﬃcient by showing that (3.3.3) holds. Let A = [aij] and
B = [bij], and write
f(αA + B) = trace (αA + B) =
n

i=1
[αA + B]ii =
n

i=1
(αaii + bii)
=
n

i=1
αaii +
n

i=1
bii = α
n

i=1
aii +
n

i=1
bii = α trace (A) + trace (B)
= αf(A) + f(B).

3.3 Linearity
91
Example 3.3.2
Consider a linear system
a11x1 +
a12x2 + · · · +
a1nxn =
u1,
a21x1 +
a22x2 + · · · +
a2nxn =
u2,
...
am1x1 + am2x2 + · · · + amnxn = um,
to be a function u = f(x) that maps x =




x1
x2
...
xn



∈ℜn to u =




u1
u2
...
um



∈ℜm.
Problem: Show that u = f(x) is linear.
Solution: Let A = [aij] be the matrix of coeﬃcients, and write
f(αx + y) = f




αx1 + y1
αx2 + y2
...
αxn + yn



=
n

j=1
(αxj + yj)A∗j =
n

j=1
(αxjA∗j + yjA∗j)
=
n

j=1
αxjA∗j +
n

j=1
yjA∗j = α
n

j=1
xjA∗j +
n

j=1
yjA∗j
= αf(x) + f(y).
According to (3.3.3), the function f is linear.
The following terminology will be used from now on.
Linear Combinations
For scalars αj and matrices Xj, the expression
α1X1 + α2X2 + · · · + αnXn =
n

j=1
αjXj
is called a linear combination of the Xj’s.

92
Chapter 3
Matrix Algebra
Exercises for section 3.3
3.3.1. Each of the following is a function from ℜ2 into ℜ2. Determine which
are linear functions.
(a)
f

x
y

=

x
1 + y

.
(b)
f

x
y

=

y
x

.
(c)
f

x
y

=

0
xy

.
(d)
f

x
y

=

x2
y2

.
(e)
f

x
y

=

x
sin y

.
(f)
f

x
y

=

x + y
x −y

.
3.3.2. For x =




x1
x2
...
xn



, and for constants ξi, verify that
f(x) = ξ1x1 + ξ2x2 + · · · + ξnxn
is a linear function.
3.3.3. Give examples of at least two diﬀerent physical principles or laws that
can be characterized as being linear phenomena.
3.3.4. Determine which of the following three transformations in ℜ2 are linear.
θ
f(p)
p
f(p)
p
y = x
f(p)
p
Rotate counterclockwise
through an angle θ.
Reflect about
the x -axis.
Project onto
the line y = x.

3.4 Why Do It This Way
93
3.4
WHY DO IT THIS WAY
If you were given the task of formulating a deﬁnition for composing two ma-
trices A and B in some sort of “natural” multiplicative fashion, your ﬁrst
attempt would probably be to compose A and B by multiplying correspond-
ing entries—much the same way matrix addition is deﬁned. Asked then to defend
the usefulness of such a deﬁnition, you might be hard pressed to provide a truly
satisfying response. Unless a person is in the right frame of mind, the issue of
deciding how to best deﬁne matrix multiplication is not at all transparent, es-
pecially if it is insisted that the deﬁnition be both “natural” and “useful.” The
world had to wait for Arthur Cayley to come to this proper frame of mind.
As mentioned in §3.1, matrix algebra appeared late in the game. Manipula-
tion on arrays and the theory of determinants existed long before Cayley and his
theory of matrices. Perhaps this can be attributed to the fact that the “correct”
way to multiply two matrices eluded discovery for such a long time.
Around 1855, Cayley became interested in composing linear functions.
19 In
particular, he was investigating linear functions of the type discussed in Example
3.3.2. Typical examples of two such functions are
f(x) = f

x1
x2

=

ax1 + bx2
cx1 + dx2

and
g(x) = g

x1
x2

=

Ax1 + Bx2
Cx1 + Dx2

.
Consider, as Cayley did, composing f and g to create another linear function
h(x) = f

g(x)

= f

Ax1 + Bx2
Cx1 + Dx2

=

(aA + bC)x1 + (aB + bD)x2
(cA + dC)x1 + (cB + dD)x2

.
It was Cayley’s idea to use matrices of coeﬃcients to represent these linear
functions. That is, f, g, and h are represented by
F =

a
b
c
d

,
G =

A
B
C
D

,
and
H =

aA + bC
aB + bD
cA + dC
cB + dD

.
After making this association, it was only natural for Cayley to call H the
composition (or product) of F and G, and to write

a
b
c
d
 
A
B
C
D

=

aA + bC
aB + bD
cA + dC
cB + dD

.
(3.4.1)
In other words, the product of two matrices represents the composition of the
two associated linear functions. By means of this observation, Cayley brought to
life the subjects of matrix analysis and linear algebra.
19
Cayley was not the ﬁrst to compose linear functions. In fact, Gauss used these compositions
as early as 1801, but not in the form of an array of coeﬃcients. Cayley was the ﬁrst to make
the connection between composition of linear functions and the composition of the associated
matrices. Cayley’s work from 1855 to 1857 is regarded as being the birth of our subject.

94
Chapter 3
Matrix Algebra
Exercises for section 3.4
Each problem in this section concerns the following three linear transformations
in ℜ2.
Rotation:
Rotate points counterclockwise
through an angle θ.
θ
f(p)
p
Reﬂection:
Reﬂect points about the x -axis.
f(p)
p
Projection:
Project points onto the line
y = x in a perpendicular
manner.
y = x
f(p)
p
3.4.1. Determine the matrix associated with each of these linear functions.
That is, determine the aij ’s such that
f(p) = f

x1
x2

=

a11x1 + a12x2
a21x1 + a22x2

.
3.4.2. By using matrix multiplication, determine the linear function obtained
by performing a rotation followed by a reﬂection.
3.4.3. By using matrix multiplication, determine the linear function obtained
by ﬁrst performing a reﬂection, then a rotation, and ﬁnally a projection.

3.5 Matrix Multiplication
95
3.5
MATRIX MULTIPLICATION
The purpose of this section is to further develop the concept of matrix multipli-
cation as introduced in the previous section. In order to do this, it is helpful to
begin by composing a single row with a single column. If
R = ( r1
r2
· · ·
rn )
and
C =




c1
c2
...
cn



,
the standard inner product of R with C is deﬁned to be the scalar
RC = r1c1 + r2c2 + · · · + rncn =
n

i=1
rici.
For example,
( 2
4
−2 )


1
2
3

= (2)(1) + (4)(2) + (−2)(3) = 4.
Recall from (3.4.1) that the product of two 2 × 2 matrices
F =
	
a
b
c
d

and
G =
	
A
B
C
D

was deﬁned naturally by writing
FG =
	
a
b
c
d

 	
A
B
C
D

=
	
aA + bC
aB + bD
cA + dC
cB + dD

= H.
Notice that the (i, j) -entry in the product H can be described as the inner
product of the ith row of F with the jth column in G. That is,
h11 = F1∗G∗1 = ( a
b )
	
A
C

,
h12 = F1∗G∗2 = ( a
b )
	
B
D

,
h21 = F2∗G∗1 = ( c
d )
	
A
C

,
h22 = F2∗G∗2 = ( c
d )
	
B
D

.
This is exactly the way that the general deﬁnition of matrix multiplication is
formulated.

96
Chapter 3
Matrix Algebra
Matrix Multiplication
•
Matrices A and B are said to be conformable for multiplication
in the order AB whenever A has exactly as many columns as B
has rows—i.e., A is m × p and B is p × n.
•
For conformable matrices Am×p = [aij] and Bp×n = [bij], the
matrix product AB is deﬁned to be the m × n matrix whose
(i, j) -entry is the inner product of the ith row of A with the jth
column in B. That is,
[AB]ij = Ai∗B∗j = ai1b1j + ai2b2j + · · · + aipbpj =
p

k=1
aikbkj.
•
In case A and B fail to be conformable—i.e., A is m × p and B
is q × n with p ̸= q —then no product AB is deﬁned.
For example, if
A =

a11
a12
a13
a21
a22
a23

2×3
and
B =


b11
b12
b13
b14
b21
b22
b23
b24
b31
b32
b33
b34


3×4
↑
inside ones match
↑

shape of the product

then the product AB exists and has shape 2 × 4. Consider a typical entry of
this product, say, the (2,3)-entry. The deﬁnition says [AB]23 is obtained by
forming the inner product of the second row of A with the third column of B

a11
a12
a13
a21
a22
a23
 

b11
b21
b31
b12
b22
b32
b13
b23
b33
b14
b24
b34

,
so
[AB]23 = A2∗B∗3 = a21b13 + a22b23 + a23b33 =
3

k=1
a2kbk3.

3.5 Matrix Multiplication
97
For example,
A =

2
1
−4
−3
0
5

, B =


1
3
−3
2
2
5
−1
8
−1
2
0
2

=⇒AB =

8
3
−7
4
−8
1
9
4

.
Notice that in spite of the fact that the product AB exists, the product BA
is not deﬁned—matrix B is 3 × 4 and A is 2 × 3, and the inside dimensions
don’t match in this order. Even when the products AB and BA each exist
and have the same shape, they need not be equal. For example,
A=

1
−1
1
−1

, B=

1
1
1
1

=⇒AB=

0
0
0
0

, BA=

2
−2
2
−2

. (3.5.1)
This disturbing feature is a primary diﬀerence between scalar and matrix algebra.
Matrix Multiplication Is Not Commutative
Matrix multiplication is a noncommutative operation—i.e., it is possible
for AB ̸= BA, even when both products exist and have the same shape.
There are other major diﬀerences between multiplication of matrices and
multiplication of scalars. For scalars,
αβ = 0
implies
α = 0
or
β = 0.
(3.5.2)
However, the analogous statement for matrices does not hold—the matrices given
in (3.5.1) show that it is possible for AB = 0 with A ̸= 0 and B ̸= 0. Related
to this issue is a rule sometimes known as the cancellation law. For scalars,
this law says that
αβ = αγ
and
α ̸= 0
implies
β = γ.
(3.5.3)
This is true because we invoke (3.5.2) to deduce that α(β −γ) = 0 implies
β −γ = 0. Since (3.5.2) does not hold for matrices, we cannot expect (3.5.3) to
hold for matrices.
Example 3.5.1
The cancellation law (3.5.3) fails for matrix multiplication. If
A =

1
1
1
1

,
B =

2
2
2
2

,
and
C =

3
1
1
3

,
then
AB =

4
4
4
4

= AC
but
B ̸= C
in spite of the fact that A ̸= 0.

98
Chapter 3
Matrix Algebra
There are various ways to express the individual rows and columns of a
matrix product. For example, the ith row of AB is
[AB]i∗=

Ai∗B∗1 | Ai∗B∗2 | · · · | Ai∗B∗n

= Ai∗B
= ( ai1
ai2
· · ·
aip )




B1∗
B2∗
...
Bp∗



= ai1B1∗+ ai2B2∗+ · · · + aipBp∗.
As shown below, there are similar representations for the individual columns.
Rows and Columns of a Product
Suppose that A = [aij] is m × p and B = [bij] is p × n.
•
[AB]i∗= Ai∗B

( ith row of AB )=( ith row of A ) ×B

. (3.5.4)
•
[AB]∗j = AB∗j

( jth col of AB )= A× ( jth col of B )

.
(3.5.5)
•
[AB]i∗= ai1B1∗+ ai2B2∗+ · · · + aipBp∗= p
k=1 aikBk∗.
(3.5.6)
•
[AB]∗j = A∗1b1j + A∗2b2j + · · · + A∗pbpj = p
k=1 A∗kbkj.
(3.5.7)
These last two equations show that rows of AB are combinations of
rows of B, while columns of AB are combinations of columns of A.
For example, if A =

1
−2
0
3
−4
5

and B =


3
−5
1
2
−7
2
1
−2
0

, then the
second row of AB is
[AB]2∗= A2∗B = ( 3
−4
5 )


3
−5
1
2
−7
2
1
−2
0

= ( 6
3
−5 ) ,
and the second column of AB is
[AB]∗2 = AB∗2 =

1
−2
0
3
−4
5
 

−5
−7
−2

=

9
3

.
This example makes the point that it is wasted eﬀort to compute the entire
product if only one row or column is called for. Although it’s not necessary to
compute the complete product, you may wish to verify that
AB =

1
−2
0
3
−4
5
 

3
−5
1
2
−7
2
1
−2
0

=

−1
9
−3
6
3
−5

.

3.5 Matrix Multiplication
99
Matrix multiplication provides a convenient representation for a linear sys-
tem of equations. For example, the 3 × 4 system
2x1 + 3x2 + 4x3 + 8x4 = 7,
3x1 + 5x2 + 6x3 + 2x4 = 6,
4x1 + 2x2 + 4x3 + 9x4 = 4,
can be written as Ax = b, where
A3×4 =


2
3
4
8
3
5
6
2
4
2
4
9

,
x4×1 =



x1
x2
x3
x4


,
and
b3×1 =


7
6
4

.
And this example generalizes to become the following statement.
Linear Systems
Every linear system of m equations in n unknowns
a11x1 +
a12x2 + · · · +
a1nxn =
b1,
a21x1 +
a22x2 + · · · +
a2nxn =
b2,
...
am1x1 + am2x2 + · · · + amnxn = bm,
can be written as a single matrix equation Ax = b in which
A =




a11
a12
· · ·
a1n
a21
a22
· · ·
a2n
...
...
...
...
am1
am2
· · ·
amn



,
x =




x1
x2
...
xn



,
and
b =




b1
b2
...
bm



.
Conversely, every matrix equation of the form Am×nxn×1 = bm×1 rep-
resents a system of m linear equations in n unknowns.
The numerical solution of a linear system was presented earlier in the text
without the aid of matrix multiplication because the operation of matrix mul-
tiplication is not an integral part of the arithmetical process used to extract a
solution by means of Gaussian elimination. Viewing a linear system as a single
matrix equation Ax = b is more of a notational convenience that can be used to
uncover theoretical properties and to prove general theorems concerning linear
systems.

100
Chapter 3
Matrix Algebra
For example, a very concise proof of the fact (2.3.5) stating that a system
of equations Am×nxn×1 = bm×1 is consistent if and only if b is a linear
combination of the columns in A is obtained by noting that the system is
consistent if and only if there exists a column s that satisﬁes
b = As = ( A∗1
A∗2
· · ·
A∗n )




s1
s2
...
sn



= A∗1s1 + A∗2s2 + · · · + A∗nsn.
The following example illustrates a common situation in which matrix mul-
tiplication arises naturally.
Example 3.5.2
An airline serves ﬁve cities, say, A, B, C, D, and H, in which H is the “hub
city.” The various routes between the cities are indicated in Figure 3.5.1.
A
B
C
D
H
Figure 3.5.1
Suppose you wish to travel from city A to city B so that at least two connecting
ﬂights are required to make the trip. Flights (A →H) and (H →B) provide the
minimal number of connections. However, if space on either of these two ﬂights
is not available, you will have to make at least three ﬂights. Several questions
arise. How many routes from city A to city B require exactly three connecting
ﬂights? How many routes require no more than four ﬂights—and so forth? Since
this particular network is small, these questions can be answered by “eyeballing”
the diagram, but the “eyeball method” won’t get you very far with the large
networks that occur in more practical situations. Let’s see how matrix algebra
can be applied. Begin by creating a connectivity matrix C = [cij] (also known
as an adjacency matrix) in which
cij =
 1
if there is a ﬂight from city i to city j,
0
otherwise.

3.5 Matrix Multiplication
101
For the network depicted in Figure 3.5.1,
C =






A
B
C
D
H
A
0
0
1
0
1
B
1
0
0
0
1
C
0
0
0
1
1
D
0
1
0
0
1
H
1
1
1
1
0






.
The matrix C together with its powers C2, C3, C4, . . . will provide all of the
information needed to analyze the network. To see how, notice that since cik
is the number of direct routes from city i to city k, and since ckj is the
number of direct routes from city k to city j, it follows that cikckj must be
the number of 2-ﬂight routes from city i to city j that have a connection at
city k. Consequently, the (i, j) -entry in the product C2 = CC is
[C2]ij =
5

k=1
cikckj = the total number of 2-ﬂight routes from city i to city j.
Similarly, the (i, j) -entry in the product C3 = CCC is
[C3]ij =
5

k1,k2=1
cik1ck1k2ck2j = number of 3-ﬂight routes from city i to city j,
and, in general,
[Cn]ij =
5

k1,k2,···,kn−1=1
cik1ck1k2 · · · ckn−2kn−1ckn−1j
is the total number of n -ﬂight routes from city i to city j. Therefore, the total
number of routes from city i to city j that require no more than n ﬂights
must be given by
[C]ij + [C2]ij + [C3]ij + · · · + [Cn]ij = [C + C2 + C3 + · · · + Cn]ij.
For our particular network,
C2=





1
1
1
2
1
1
1
2
1
1
1
2
1
1
1
2
1
1
1
1
1
1
1
1
4




, C3=





2
3
2
2
5
2
2
2
3
5
3
2
2
2
5
2
2
3
2
5
5
5
5
5
4




, C4=





8
7
7
7
9
7
8
7
7
9
7
7
8
7
9
7
7
7
8
9
9
9
9
9
20




,

102
Chapter 3
Matrix Algebra
and
C + C2 + C3 + C4 =





11
11
11
11
16
11
11
11
11
16
11
11
11
11
16
11
11
11
11
16
16
16
16
16
28




.
The fact that [C3]12 = 3 means there are exactly 3 three-ﬂight routes from city
A to city B, and [C4]12 = 7 means there are exactly 7 four-ﬂight routes—try
to identify them. Furthermore, [C + C2 + C3 + C4]12 = 11 means there are 11
routes from city A to city B that require no more than 4 ﬂights.
Exercises for section 3.5
3.5.1. For A =


1
−2
3
0
−5
4
4
−3
8

, B =


1
2
0
4
3
7

, and C =


1
2
3

, compute
the following products when possible.
(a)
AB,
(b) BA,
(c) CB,
(d) CT B,
(e) A2,
(f) B2,
(g)
CT C,
(h) CCT ,
(i) BBT ,
(j) BT B,
(k) CT AC.
3.5.2. Consider the following system of equations:
2x1 +
x2 +
x3 =
3,
4x1
+ 2x3 =
10,
2x1 + 2x2
= −2.
(a)
Write the system as a matrix equation of the form Ax = b.
(b)
Write the solution of the system as a column s and verify by
matrix multiplication that s satisﬁes the equation Ax = b.
(c)
Write b as a linear combination of the columns in A.
3.5.3. Let E =


1
0
0
0
1
0
3
0
1

and let A be an arbitrary 3 × 3 matrix.
(a)
Describe the rows of EA in terms of the rows of A.
(b)
Describe the columns of AE in terms of the columns of A.
3.5.4. Let ej denote the jth unit column that contains a 1 in the jth
position and zeros everywhere else. For a general matrix An×n, describe
the following products.
(a)
Aej
(b)
eT
i A
(c)
eT
i Aej

3.5 Matrix Multiplication
103
3.5.5. Suppose that A and B are m × n matrices. If Ax = Bx holds for
all n × 1 columns x, prove that A = B. Hint: What happens when
x is a unit column?
3.5.6. For A =

1/2
α
0
1/2

, determine limn→∞An. Hint: Compute a few
powers of A and try to deduce the general form of An.
3.5.7. If Cm×1 and R1×n are matrices consisting of a single column and
a single row, respectively, then the matrix product Pm×n = CR is
sometimes called the outer product of C with R. For conformable
matrices A and B, explain how to write the product AB as a sum of
outer products involving the columns of A and the rows of B.
3.5.8. A square matrix U = [uij] is said to be upper triangular whenever
uij = 0 for i > j —i.e., all entries below the main diagonal are 0.
(a)
If A and B are two n × n upper-triangular matrices, explain
why the product AB must also be upper triangular.
(b)
If An×n and Bn×n are upper triangular, what are the diagonal
entries of AB?
(c)
L is lower triangular when ℓij = 0 for i < j. Is it true that
the product of two n × n lower-triangular matrices is again
lower triangular?
3.5.9. If A = [aij(t)] is a matrix whose entries are functions of a variable t,
the derivative
of A with respect to t is deﬁned to be the matrix of
derivatives. That is,
dA
dt =
daij
dt

.
Derive the product rule for diﬀerentiation
d(AB)
dt
= dA
dt B + AdB
dt .
3.5.10. Let Cn×n be the connectivity matrix associated with a network of n
nodes such as that described in Example 3.5.2, and let e be the n × 1
column of all 1’s. In terms of the network, describe the entries in each
of the following products.
(a)
Interpret the product Ce.
(b)
Interpret the product eT C.

104
Chapter 3
Matrix Algebra
3.5.11. Consider three tanks each containing V gallons of brine. The tanks are
connected as shown in Figure 3.5.2, and all spigots are opened at once.
As fresh water at the rate of r gal/sec is pumped into the top of the
ﬁrst tank, r gal/sec leaves from the bottom and ﬂows into the next
tank, and so on down the line—there are r gal/sec entering at the top
and leaving through the bottom of each tank.
r  gal / sec
r  gal / sec
r  gal / sec
r  gal / sec
Figure 3.5.2
Let xi(t) denote the number of pounds of salt in tank i at time t, and
let
x =



x1(t)
x2(t)
x3(t)



and
dx
dt =



dx1/dt
dx2/dt
dx3/dt


.
Assuming that complete mixing occurs in each tank on a continuous
basis, show that
dx
dt = Ax,
where
A = r
V


−1
0
0
1
−1
0
0
1
−1

.
Hint: Use the fact that
dxi
dt = rate of change = lbs
sec coming in −lbs
sec going out.

3.6 Properties of Matrix Multiplication
105
3.6
PROPERTIES OF MATRIX MULTIPLICATION
We saw in the previous section that there are some diﬀerences between scalar
and matrix algebra—most notable is the fact that matrix multiplication is not
commutative, and there is no cancellation law. But there are also some important
similarities, and the purpose of this section is to look deeper into these issues.
Although we can adjust to not having the commutative property, the situa-
tion would be unbearable if the distributive and associative properties were not
available. Fortunately, both of these properties hold for matrix multiplication.
Distributive and Associative Laws
For conformable matrices each of the following is true.
•
A(B + C) = AB + AC
(left-hand distributive law).
•
(D + E)F = DF + EF
(right-hand distributive law).
•
A(BC) = (AB)C
(associative law).
Proof.
To prove the left-hand distributive property, demonstrate the corre-
sponding entries in the matrices A(B + C) and AB + AC are equal. To this
end, use the deﬁnition of matrix multiplication to write
[A(B + C)]ij = Ai∗(B + C)∗j =

k
[A]ik[B + C]kj =

k
[A]ik ([B]kj + [C]kj)
=

k
([A]ik[B]kj + [A]ik[C]kj) =

k
[A]ik[B]kj +

k
[A]ik[C]kj
= Ai∗B∗j + Ai∗C∗j = [AB]ij + [AC]ij
= [AB + AC]ij.
Since this is true for each i and j, it follows that A(B + C) = AB + AC. The
proof of the right-hand distributive property is similar and is omitted. To prove
the associative law, suppose that B is p × q and C is q × n, and recall from
(3.5.7) that the jth column of BC is a linear combination of the columns in
B. That is,
[BC]∗j = B∗1c1j + B∗2c2j + · · · + B∗qcqj =
q

k=1
B∗kckj.

106
Chapter 3
Matrix Algebra
Use this along with the left-hand distributive property to write
[A(BC)]ij = Ai∗[BC]∗j = Ai∗
q

k=1
B∗kckj =
q

k=1
Ai∗B∗kckj
=
q

k=1
[AB]ikckj = [AB]i∗C∗j = [(AB)C]ij.
Example 3.6.1
Linearity of Matrix Multiplication. Let A be an m × n matrix, and f be
the function deﬁned by matrix multiplication
f(Xn×p) = AX.
The left-hand distributive property guarantees that f is a linear function be-
cause for all scalars α and for all n × p matrices X and Y,
f(αX + Y) = A(αX + Y) = A(αX) + AY = αAX + AY
= αf(X) + f(Y).
Of course, the linearity of matrix multiplication is no surprise because it was
the consideration of linear functions that motivated the deﬁnition of the matrix
product at the outset.
For scalars, the number 1 is the identity element for multiplication because
it has the property that it reproduces whatever it is multiplied by. For matrices,
there is an identity element with similar properties.
Identity Matrix
The n × n matrix with 1’s on the main diagonal and 0’s elsewhere
In =




1
0
· · ·
0
0
1
· · ·
0
...
...
...
...
0
0
· · ·
1




is called the identity matrix of order n. For every m × n matrix A,
AIn = A
and
ImA = A.
The subscript on In is neglected whenever the size is obvious from the
context.

3.6 Properties of Matrix Multiplication
107
Proof.
Notice that I∗j has a 1 in the jth position and 0’s elsewhere. Recall
from Exercise
3.5.4 that such columns were called unit columns, and they
have the property that for any conformable matrix A,
AI∗j = A∗j.
Using this together with the fact that [AI]∗j = AI∗j produces
AI = ( AI∗1
AI∗2
· · ·
AI∗n ) = ( A∗1
A∗2
· · ·
A∗n ) = A.
A similar argument holds when I appears on the left-hand side of A.
Analogous to scalar algebra, we deﬁne the 0th power of a square matrix to
be the identity matrix of corresponding size. That is, if A is n × n, then
A0 = In.
Positive powers of A are also deﬁned in the natural way. That is,
An = AA· · ·A



n times
.
The associative law guarantees that it makes no diﬀerence how matrices are
grouped for powering. For example, AA2 is the same as A2A, so that
A3 = AAA = AA2 = A2A.
Also, the usual laws of exponents hold. For nonnegative integers r and s,
ArAs = Ar+s
and
(Ar)s = Ars.
We are not yet in a position to deﬁne negative or fractional powers, and due to
the lack of conformability, powers of nonsquare matrices are never deﬁned.
Example 3.6.2
A Pitfall. For two n × n matrices, what is (A + B)2? Be careful! Because
matrix multiplication is not commutative, the familiar formula from scalar alge-
bra is not valid for matrices. The distributive properties must be used to write
(A + B)2 = (A + B)


(A + B) = (A + B)


 A + (A + B)


 B
= A2 + BA + AB + B2,
and this is as far as you can go. The familiar form A2+2AB+B2 is obtained only
in those rare cases where AB = BA. To evaluate (A + B)k, the distributive
rules must be applied repeatedly, and the results are a bit more complicated—try
it for k = 3.

108
Chapter 3
Matrix Algebra
Example 3.6.3
Suppose that the population migration between two geographical regions—say,
the North and the South—is as follows. Each year, 50% of the population in
the North migrates to the South, while only 25% of the population in the South
moves to the North. This situation is depicted by drawing a transition diagram
such as that shown in Figure 3.6.1.
N
S
.25
.5
.5
.75
Figure 3.6.1
Problem: If this migration pattern continues, will the population in the North
continually shrink until the entire population is eventually in the South, or will
the population distribution somehow stabilize before the North is completely
deserted?
Solution: Let nk and sk denote the respective proportions of the total popula-
tion living in the North and South at the end of year k and assume nk +sk = 1.
The migration pattern dictates that the fractions of the population in each region
at the end of year k + 1 are
nk+1 = nk(.5) + sk(.25),
sk+1 = nk(.5) + sk(.75).
(3.6.1)
If pT
k = (nk, sk) and pT
k+1 = (nk+1, sk+1) denote the respective population
distributions at the end of years k and k + 1, and if
T =
 N
S
N
.5
.5
S
.25
.75

is the associated transition matrix, then (3.6.1) assumes the matrix form
pT
k+1 = pT
k T. Inducting on pT
1 = pT
0 T, pT
2 = pT
1 T = pT
0 T2, pT
3 = pT
2 T =
pT
0 T3, etc., leads to
pT
k = pT
0 Tk.
(3.6.2)
Determining the long-run behavior involves evaluating limk→∞pT
k , and it’s clear
from (3.6.2) that this boils down to analyzing limk→∞Tk. Later, in Example

3.6 Properties of Matrix Multiplication
109
7.3.5, a more sophisticated approach is discussed, but for now we will use the
“brute force” method of successively powering P until a pattern emerges. The
ﬁrst several powers of P are shown below with three signiﬁcant digits displayed.
P2 =

.375
.625
.312
.687

P3 =

.344
.656
.328
.672

P4 =

.328
.672
.332
.668

P5 =

.334
.666
.333
.667

P6 =

.333
.667
.333
.667

P7 =

.333
.667
.333
.667

This sequence appears to be converging to a limiting matrix of the form
P∞= lim
k→∞Pk =

1/3
2/3
1/3
2/3

,
so the limiting population distribution is
pT
∞= lim
k→∞pT
k = lim
k→∞pT
0 Tk = pT
0 lim
k→∞Tk = ( n0
s0 )

1/3
2/3
1/3
2/3

=
 n0 + s0
3
2(n0 + s0)
3

= ( 1/3
2/3 ) .
Therefore, if the migration pattern continues to hold, then the population dis-
tribution will eventually stabilize with 1/3 of the population being in the North
and 2/3 of the population in the South. And this is independent of the initial
distribution! The powers of P indicate that the population distribution will be
practically stable in no more than 6 years—individuals may continue to move,
but the proportions in each region are essentially constant by the sixth year.
The operation of transposition has an interesting eﬀect upon a matrix
product—a reversal of order occurs.
Reverse Order Law for Transposition
For conformable matrices A and B,
(AB)T = BT AT .
The case of conjugate transposition is similar. That is,
(AB)∗= B∗A∗.

110
Chapter 3
Matrix Algebra
Proof.
By deﬁnition,
(AB)T
ij = [AB]ji = Aj∗B∗i.
Consider the (i, j)-entry of the matrix BT AT and write

BT AT 
ij =

BT 	
i∗

AT 	
∗j =

k

BT 
ik

AT 
kj
=

k
[B]ki[A]jk =

k
[A]jk[B]ki
= Aj∗B∗i.
Therefore, (AB)T
ij =

BT AT 
ij for all i and j, and thus (AB)T = BT AT .
The proof for the conjugate transpose case is similar.
Example 3.6.4
For every matrix Am×n, the products AT A and AAT are symmetric matrices
because

AT A
	T = AT AT T = AT A
and

AAT 	T = AT T AT = AAT .
Example 3.6.5
Trace of a Product. Recall from Example 3.3.1 that the trace of a square
matrix is the sum of its main diagonal entries. Although matrix multiplication
is not commutative, the trace function is one of the few cases where the order of
the matrices can be changed without aﬀecting the results.
Problem: For matrices Am×n and Bn×m, prove that
trace (AB) = trace (BA).
Solution:
trace (AB) =

i
[AB]ii =

i
Ai∗B∗i =

i

k
aikbki =

i

k
bkiaik
=

k

i
bkiaik =

k
Bk∗A∗k =

k
[BA]kk = trace (BA).
Note: This is true in spite of the fact that AB is m × m while BA is n × n.
Furthermore, this result can be extended to say that any product of conformable
matrices can be permuted cyclically without altering the trace of the product.
For example,
trace (ABC) = trace (BCA) = trace (CAB).
However, a noncyclical permutation may not preserve the trace. For example,
trace (ABC) ̸= trace (BAC).

3.6 Properties of Matrix Multiplication
111
Executing multiplication between two matrices by partitioning one or both
factors into submatrices—a matrix contained within another matrix—can be
a useful technique.
Block Matrix Multiplication
Suppose that A and B are partitioned into submatrices—often referred
to as blocks—as indicated below.
A =




A11
A12
· · ·
A1r
A21
A22
· · ·
A2r
...
...
...
...
As1
As2
· · ·
Asr



,
B =




B11
B12
· · ·
B1t
B21
B22
· · ·
B2t
...
...
...
...
Br1
Br2
· · ·
Brt



.
If the pairs (Aik, Bkj) are conformable, then A and B are said to
be conformably partitioned. For such matrices, the product AB is
formed by combining the blocks exactly the same way as the scalars are
combined in ordinary matrix multiplication. That is, the (i, j) -block in
AB is
Ai1B1j + Ai2B2j + · · · + AirBrj.
Although a completely general proof is possible, looking at some examples
better serves the purpose of understanding this technique.
Example 3.6.6
Block multiplication is particularly useful when there are patterns in the matrices
to be multiplied. Consider the partitioned matrices
A =




1
2
1
0
3
4
0
1
1
0
0
0
0
1
0
0



=

C
I
I
0

,
B =




1
0
0
0
0
1
0
0
1
2
1
2
3
4
3
4



=

I
0
C
C

,
where
I =

1
0
0
1

and
C =

1
2
3
4

.
Using block multiplication, the product AB is easily computed to be
AB =

C
I
I
0
 
I
0
C
C

=

2C
C
I
0

=




2
4
1
2
6
8
3
4
1
0
0
0
0
1
0
0



.

112
Chapter 3
Matrix Algebra
Example 3.6.7
Reducibility. Suppose that Tn×nx = b represents a system of linear equa-
tions in which the coeﬃcient matrix is block triangular. That is, T can be
partitioned as
T =

A
B
0
C

,
where
A is r × r and C is n −r × n −r.
(3.6.3)
If x and b are similarly partitioned as x =
 x1
x2

and b =
 b1
b2

, then block
multiplication shows that Tx = b reduces to two smaller systems
Ax1 + Bx2 = b1,
Cx2 = b2,
so if all systems are consistent, a block version of back substitution is possible—
i.e., solve Cx2 = b2 for x2, and substituted this back into Ax1 = b1 −Bx2,
which is then solved for x1. For obvious reasons, block-triangular systems of
this type are sometimes referred to as reducible systems, and T is said to
be a reducible matrix. Recall that applying Gaussian elimination with back
substitution to an n × n system requires about n3/3 multiplications/divisions
and about n3/3 additions/subtractions. This means that it’s more eﬃcient to
solve two smaller subsystems than to solve one large main system. For exam-
ple, suppose the matrix T in (3.6.3) is 100 × 100 while A and C are each
50 × 50. If Tx = b is solved without taking advantage of its reducibility, then
about 106/3 multiplications/divisions are needed. But by taking advantage of
the reducibility, only about (250 × 103)/3 multiplications/divisions are needed
to solve both 50 × 50 subsystems. Another advantage of reducibility is realized
when a computer’s main memory capacity is not large enough to store the entire
coeﬃcient matrix but is large enough to hold the submatrices.
Exercises for section 3.6
3.6.1. For the partitioned matrices
A =


1
0
0
3
3
3
1
0
0
3
3
3
1
2
2
0
0
0


and
B =









−1
−1
0
0
0
0
−1
−2
−1
−2
−1
−2









,
use block multiplication with the indicated partitions to form the prod-
uct AB.

3.6 Properties of Matrix Multiplication
113
3.6.2. For all matrices An×k and Bk×n, show that the block matrix
L =

I −BA
B
2A −ABA
AB −I

has the property L2 = I. Matrices with this property are said to be
involutory, and they occur in the science of cryptography.
3.6.3. For the matrix
A =







1
0
0
1/3
1/3
1/3
0
1
0
1/3
1/3
1/3
0
0
1
1/3
1/3
1/3
0
0
0
1/3
1/3
1/3
0
0
0
1/3
1/3
1/3
0
0
0
1/3
1/3
1/3







,
determine A300. Hint: A square matrix C is said to be idempotent
when it has the property that C2 = C. Make use of idempotent sub-
matrices in A.
3.6.4. For every matrix Am×n, demonstrate that the products A∗A and
AA∗are hermitian matrices.
3.6.5. If A and B are symmetric matrices that commute, prove that the
product AB is also symmetric. If AB ̸= BA, is AB necessarily sym-
metric?
3.6.6. Prove that the right-hand distributive property is true.
3.6.7. For each matrix An×n, explain why it is impossible to ﬁnd a solution
for Xn×n in the matrix equation
AX −XA = I.
Hint: Consider the trace function.
3.6.8. Let yT
1×m be a row of unknowns, and let Am×n and bT
1×n be known
matrices.
(a)
Explain why the matrix equation yT A = bT represents a sys-
tem of n linear equations in m unknowns.
(b)
How are the solutions for yT
in yT A = bT
related to the
solutions for x in AT x = b?

114
Chapter 3
Matrix Algebra
3.6.9. A particular electronic device consists of a collection of switching circuits
that can be either in an ON state or an OFF state. These electronic
switches are allowed to change state at regular time intervals called clock
cycles. Suppose that at the end of each clock cycle, 30% of the switches
currently in the OFF state change to ON, while 90% of those in the ON
state revert to the OFF state.
(a)
Show that the device approaches an equilibrium in the sense
that the proportion of switches in each state eventually becomes
constant, and determine these equilibrium proportions.
(b)
Independent of the initial proportions, about how many clock
cycles does it take for the device to become essentially stable?
3.6.10. Write the following system in the form Tn×nx = b, where T is block
triangular, and then obtain the solution by solving two small systems as
described in Example 3.6.7.
x1 +
x2 + 3x3 + 4x4 = −1,
2x3 + 3x4 =
3,
x1 + 2x2 + 5x3 + 6x4 = −2,
x3 + 2x4 =
4.
3.6.11. Prove that each of the following statements is true for conformable ma-
trices.
(a)
trace (ABC) = trace (BCA) = trace (CAB).
(b)
trace (ABC) can be diﬀerent from trace (BAC).
(c)
trace

AT B
	
= trace

ABT 	
.
3.6.12. Suppose that Am×n and xn×1 have real entries.
(a)
Prove that xT x = 0 if and only if x = 0.
(b)
Prove that trace

AT A
	
= 0 if and only if A = 0.

3.7 Matrix Inversion
115
3.7
MATRIX INVERSION
If α is a nonzero scalar, then for each number β the equation αx = β has a
unique solution given by x = α−1β. To prove that α−1β is a solution, write
α(α−1β) = (αα−1)β = (1)β = β.
(3.7.1)
Uniqueness follows because if x1 and x2 are two solutions, then
αx1 = β = αx2
=⇒
α−1(αx1) = α−1(αx2)
=⇒
(α−1α)x1 = (α−1α)x2
=⇒
(1)x1 = (1)x2
=⇒
x1 = x2.
(3.7.2)
These observations seem pedantic, but they are important in order to see how
to make the transition from scalar equations to matrix equations. In particular,
these arguments show that in addition to associativity, the properties
αα−1 = 1
and
α−1α = 1
(3.7.3)
are the key ingredients, so if we want to solve matrix equations in the same
fashion as we solve scalar equations, then a matrix analogue of (3.7.3) is needed.
Matrix Inversion
For a given square matrix An×n, the matrix Bn×n that satisﬁes the
conditions
AB = In
and
BA = In
is called the inverse of A and is denoted by B = A−1. Not all square
matrices are invertible—the zero matrix is a trivial example, but there
are also many nonzero matrices that are not invertible. An invertible
matrix is said to be nonsingular, and a square matrix with no inverse
is called a singular matrix.
Notice that matrix inversion is deﬁned for square matrices only—the con-
dition AA−1 = A−1A rules out inverses of nonsquare matrices.
Example 3.7.1
If
A =

a
b
c
d

,
where
δ = ad −bc ̸= 0,
then
A−1 = 1
δ

d
−b
−c
a

because it can be veriﬁed that AA−1 = A−1A = I2.

116
Chapter 3
Matrix Algebra
Although not all matrices are invertible, when an inverse exists, it is unique.
To see this, suppose that X1 and X2 are both inverses for a nonsingular matrix
A. Then
X1 = X1I = X1(AX2) = (X1A)X2 = IX2 = X2,
which implies that only one inverse is possible.
Since matrix inversion was deﬁned analogously to scalar inversion, and since
matrix multiplication is associative, exactly the same reasoning used in (3.7.1)
and (3.7.2) can be applied to a matrix equation AX = B, so we have the
following statements.
Matrix Equations
•
If A is a nonsingular matrix, then there is a unique solution for X
in the matrix equation An×nXn×p = Bn×p, and the solution is
X = A−1B.
(3.7.4)
•
A system of n linear equations in n unknowns can be written as a
single matrix equation An×nxn×1 = bn×1 (see p. 99), so it follows
from (3.7.4) that when A is nonsingular, the system has a unique
solution given by x = A−1b.
However, it must be stressed that the representation of the solution as
x = A−1b is mostly a notational or theoretical convenience. In practice, a
nonsingular system Ax = b is almost never solved by ﬁrst computing A−1 and
then the product x = A−1b. The reason will be apparent when we learn how
much work is involved in computing A−1.
Since not all square matrices are invertible, methods are needed to distin-
guish between nonsingular and singular matrices. There is a variety of ways to
describe the class of nonsingular matrices, but those listed below are among the
most important.
Existence of an Inverse
For an n × n matrix A, the following statements are equivalent.
•
A−1 exists
(A is nonsingular).
(3.7.5)
•
rank (A) = n.
(3.7.6)
•
A
Gauss–Jordan
−−−−−−−−→I.
(3.7.7)
•
Ax = 0 implies that x = 0.
(3.7.8)

3.7 Matrix Inversion
117
Proof.
The fact that (3.7.6) ⇐⇒(3.7.7) is a direct consequence of the deﬁ-
nition of rank, and (3.7.6) ⇐⇒(3.7.8) was established in §2.4. Consequently,
statements (3.7.6), (3.7.7), and (3.7.8) are equivalent, so if we establish that
(3.7.5) ⇐⇒(3.7.6), then the proof will be complete.
Proof of (3.7.5) =⇒(3.7.6).
Begin by observing that (3.5.5) guarantees
that a matrix X = [X∗1 | X∗2 | · · · | X∗n] satisﬁes the equation AX = I if and
only if X∗j is a solution of the linear system Ax = I∗j. If A is nonsingular,
then we know from (3.7.4) that there exists a unique solution to AX = I, and
hence each linear system Ax = I∗j has a unique solution. But in §2.5 we learned
that a linear system has a unique solution if and only if the rank of the coeﬃcient
matrix equals the number of unknowns, so rank (A) = n.
Proof of (3.7.6) =⇒(3.7.5).
If rank (A) = n, then (2.3.4) insures that
each system Ax = I∗j is consistent because rank[A | I∗j] = n = rank (A).
Furthermore, the results of §2.5 guarantee that each system Ax = I∗j has a
unique solution, and hence there is a unique solution to the matrix equation
AX = I. We would like to say that X = A−1, but we cannot jump to this
conclusion without ﬁrst arguing that XA = I. Suppose this is not true—i.e.,
suppose that XA −I ̸= 0. Since
A(XA −I) = (AX)A −A = IA −A = 0,
it follows from (3.5.5) that any nonzero column of XA−I is a nontrivial solution
of the homogeneous system Ax = 0. But this is a contradiction of the fact that
(3.7.6) ⇐⇒(3.7.8). Therefore, the supposition that XA −I ̸= 0 must be false,
and thus AX = I = XA, which means A is nonsingular.
The deﬁnition of matrix inversion says that in order to compute A−1, it is
necessary to solve both of the matrix equations AX = I and XA = I. These
two equations are necessary to rule out the possibility of nonsquare inverses. But
when only square matrices are involved, then any one of the two equations will
suﬃce—the following example elaborates.
Example 3.7.2
Problem: If A and X are square matrices, explain why
AX = I
=⇒
XA = I.
(3.7.9)
In other words, if A and X are square and AX = I, then X = A−1.
Solution: Notice ﬁrst that AX = I implies X is nonsingular because if X is
singular, then, by (3.7.8), there is a column vector x ̸= 0 such that Xx = 0,
which is contrary to the fact that x = Ix = AXx = 0. Now that we know X−1
exists, we can establish (3.7.9) by writing
AX = I
=⇒
AXX−1 = X−1
=⇒
A = X−1
=⇒
XA = I.
Caution!
The argument above is not valid for nonsquare matrices. When
m ̸= n, it’s possible that Am×nXn×m = Im, but XA ̸= In.

118
Chapter 3
Matrix Algebra
Although we usually try to avoid computing the inverse of a matrix, there are
times when an inverse must be found. To construct an algorithm that will yield
A−1 when An×n is nonsingular, recall from Example 3.7.2 that determining
A−1 is equivalent to solving the single matrix equation AX = I, and due to
(3.5.5), this in turn is equivalent to solving the n linear systems deﬁned by
Ax = I∗j
for
j = 1, 2, . . . , n.
(3.7.10)
In other words, if X∗1, X∗2, . . . , X∗n are the respective solutions to (3.7.10), then
X = [X∗1 | X∗2 | · · · | X∗n] solves the equation AX = I, and hence X = A−1.
If A is nonsingular, then we know from (3.7.7) that the Gauss–Jordan method
reduces the augmented matrix [A | I∗j] to [I | X∗j], and the results of §1.3 insure
that X∗j is the unique solution to Ax = I∗j. That is,
[A | I∗j]
Gauss–Jordan
−−−−−−−−→

I
   [A−1]∗j
!
.
But rather than solving each system Ax = I∗j separately, we can solve them
simultaneously by taking advantage of the fact that they all have the same
coeﬃcient matrix. In other words, applying the Gauss–Jordan method to the
larger augmented array [A | I∗1 | I∗2 | · · · | I∗n] produces
[A | I∗1 | I∗2 | · · · | I∗n]
Gauss–Jordan
−−−−−−−−→
"
I
   [A−1]∗1
   [A−1]∗2
   · · ·
   [A−1]∗n
#
,
or more compactly,
[A | I]
Gauss–Jordan
−−−−−−−−→[I | A−1].
(3.7.11)
What happens if we try to invert a singular matrix using this procedure?
The fact that (3.7.5) ⇐⇒(3.7.6) ⇐⇒(3.7.7) guarantees that a singular matrix
A cannot be reduced to I by Gauss–Jordan elimination because a zero row will
have to emerge in the left-hand side of the augmented array at some point during
the process. This means that we do not need to know at the outset whether A
is nonsingular or singular—it becomes self-evident depending on whether or not
the reduction (3.7.11) can be completed. A summary is given below.
Computing an Inverse
Gauss–Jordan elimination can be used to invert A by the reduction
[A | I]
Gauss–Jordan
−−−−−−−−→[I | A−1].
(3.7.12)
The only way for this reduction to fail is for a row of zeros to emerge
in the left-hand side of the augmented array, and this occurs if and only
if A is a singular matrix. A diﬀerent (and somewhat more practical)
algorithm is given Example 3.10.3 on p. 148.

3.7 Matrix Inversion
119
Although they are not included in the simple examples of this section, you
are reminded that the pivoting and scaling strategies presented in §1.5 need to
be incorporated, and the eﬀects of ill-conditioning discussed in §1.6 must be con-
sidered whenever matrix inverses are computed using ﬂoating-point arithmetic.
However, practical applications rarely require an inverse to be computed.
Example 3.7.3
Problem: If possible, ﬁnd the inverse of A =


1
1
1
1
2
2
1
2
3

.
Solution:
[A | I] =


1
1
1
1
0
0
1
2
2
0
1
0
1
2
3
0
0
1

−→


1
1
1
1
0
0
0
1
1
−1
1
0
0
1
2
−1
0
1


−→


1
0
0
2
−1
0
0
1
1
−1
1
0
0
0
1
0
−1
1

−→


1
0
0
2
−1
0
0
1
0
−1
2
−1
0
0
1
0
−1
1


Therefore, the matrix is nonsingular, and A−1 =


2
−1
0
−1
2
−1
0
−1
1

. If we wish
to check this answer, we need only check that AA−1 = I. If this holds, then the
result of Example 3.7.2 insures that A−1A = I will automatically be true.
Earlier in this section it was stated that one almost never solves a nonsin-
gular linear system Ax = b by ﬁrst computing A−1 and then the product
x = A−1b. To appreciate why this is true, pay attention to how much eﬀort is
required to perform one matrix inversion.
Operation Counts for Inversion
Computing A−1
n×n by reducing [A|I] with Gauss–Jordan requires
•
n3 multiplications/divisions,
•
n3 −2n2 + n additions/subtractions.
Interestingly, if Gaussian elimination with a back substitution process is
applied to [A|I] instead of the Gauss–Jordan technique, then exactly the same
operation count can be obtained. Although Gaussian elimination with back sub-
stitution is more eﬃcient than the Gauss–Jordan method for solving a single
linear system, the two procedures are essentially equivalent for inversion.

120
Chapter 3
Matrix Algebra
Solving a nonsingular system Ax = b by ﬁrst computing A−1 and then
forming the product x = A−1b requires n3 + n2 multiplications/divisions and
n3 −n2 additions/subtractions. Recall from §1.5 that Gaussian elimination with
back substitution requires only about n3/3 multiplications/divisions and about
n3/3 additions/subtractions. In other words, using A−1 to solve a nonsingular
system Ax = b requires about three times the eﬀort as does Gaussian elimina-
tion with back substitution.
To put things in perspective, consider standard matrix multiplication be-
tween two n × n matrices. It is not diﬃcult to verify that n3 multiplications
and n3−n2 additions are required. Remarkably, it takes almost exactly as much
eﬀort to perform one matrix multiplication as to perform one matrix inversion.
This fact always seems to be counter to a novice’s intuition—it “feels” like ma-
trix inversion should be a more diﬃcult task than matrix multiplication, but this
is not the case.
The remainder of this section is devoted to a discussion of some of the
important properties of matrix inversion. We begin with the four basic facts
listed below.
Properties of Matrix Inversion
For nonsingular matrices A and B, the following properties hold.
•

A−1	−1 = A.
(3.7.13)
•
The product AB is also nonsingular.
(3.7.14)
•
(AB)−1 = B−1A−1
(the reverse order law for inversion). (3.7.15)
•

A−1	T =

AT 	−1 and

A−1	∗= (A∗)−1.
(3.7.16)
Proof.
Property (3.7.13) follows directly from the deﬁnition of inversion. To
prove (3.7.14) and (3.7.15), let X = B−1A−1 and verify that (AB)X = I by
writing
(AB)X = (AB)B−1A−1 = A(BB−1)A−1 = A(I)A−1 = AA−1 = I.
According to the discussion in Example 3.7.2, we are now guaranteed that
X(AB) = I, and we need not bother to verify it. To prove property (3.7.16), let
X =

A−1	T and verify that AT X = I. Make use of the reverse order law for
transposition to write
AT X = AT 
A−1	T =

A−1A
	T = IT = I.
Therefore,

AT 	−1 = X =

A−1	T . The proof of the conjugate transpose case
is similar.

3.7 Matrix Inversion
121
In general the product of two rank-r matrices does not necessarily have to
produce another matrix of rank r. For example,
A =

1
2
2
4

and
B =

2
4
−1
−2

each has rank 1, but the product AB = 0 has rank 0. However, we saw in
(3.7.14) that the product of two invertible matrices is again invertible. That is, if
rank (An×n) = n and rank (Bn×n) = n, then rank (AB) = n. This generalizes
to any number of matrices.
Products of Nonsingular Matrices Are Nonsingular
If A1, A2, . . . , Ak are each n × n nonsingular matrices, then the prod-
uct A1A2 · · · Ak is also nonsingular, and its inverse is given by the
reverse order law. That is,
(A1A2 · · · Ak)−1 = A−1
k
· · · A−1
2 A−1
1 .
Proof.
Apply (3.7.14) and (3.7.15) inductively. For example, when k = 3 you
can write
(A1{A2A3})−1 = {A2A3}−1A−1
1
= A−1
3 A−1
2 A−1
1 .
Exercises for section 3.7
3.7.1. When possible, ﬁnd the inverse of each of the following matrices. Check
your answer by using matrix multiplication.
(a)

1
2
1
3

(b)

1
2
2
4

(c)


4
−8
5
4
−7
4
3
−4
2


(d)


1
2
3
4
5
6
7
8
9


(e)



1
1
1
1
1
2
2
2
1
2
3
3
1
2
3
4



3.7.2. Find the matrix X such that X = AX + B, where
A =


0
−1
0
0
0
−1
0
0
0


and
B =


1
2
2
1
3
3

.

122
Chapter 3
Matrix Algebra
3.7.3. For a square matrix A, explain why each of the following statements
must be true.
(a)
If A contains a zero row or a zero column, then A is singular.
(b)
If A contains two identical rows or two identical columns, then
A is singular.
(c)
If one row (or column) is a multiple of another row (or column),
then A must be singular.
3.7.4. Answer each of the following questions.
(a)
Under what conditions is a diagonal matrix nonsingular? De-
scribe the structure of the inverse of a diagonal matrix.
(b)
Under what conditions is a triangular matrix nonsingular? De-
scribe the structure of the inverse of a triangular matrix.
3.7.5. If A is nonsingular and symmetric, prove that A−1 is symmetric.
3.7.6. If A is a square matrix such that I −A is nonsingular, prove that
A(I −A)−1 = (I −A)−1A.
3.7.7. Prove that if A is m × n and B is n × m such that AB = Im and
BA = In, then m = n.
3.7.8. If A, B, and A + B are each nonsingular, prove that
A(A + B)−1B = B(A + B)−1A =

A−1 + B−1	−1.
3.7.9. Let S be a skew-symmetric matrix with real entries.
(a)
Prove that I −S is nonsingular. Hint: xT x = 0
=⇒
x = 0.
(b)
If A = (I + S)(I −S)−1, show that A−1 = AT .
3.7.10. For matrices Ar×r, Bs×s, and Cr×s such that A and B are nonsin-
gular, verify that each of the following is true.
(a)

A
0
0
B
−1
=

A−1
0
0
B−1

(b)

A
C
0
B
−1
=

A−1
−A−1CB−1
0
B−1


3.7 Matrix Inversion
123
3.7.11. Consider the block matrix

Ar×r
Cr×s
Rs×r
Bs×s

. When the indicated in-
verses exist, the matrices deﬁned by
S = B −RA−1C
and
T = A −CB−1R
are called the Schur complements
20 of A and B, respectively.
(a)
If A and S are both nonsingular, verify that

A
C
R
B
−1
=

A−1 + A−1CS−1RA−1
−A−1CS−1
−S−1RA−1
S−1

.
(b)
If B and T are nonsingular, verify that

A
C
R
B
−1
=

T−1
−T−1CB−1
−B−1RT−1
B−1 + B−1RT−1CB−1

.
3.7.12. Suppose that A, B, C, and D are n × n matrices such that ABT
and CDT are each symmetric and ADT −BCT = I. Prove that
AT D −CT B = I.
20
This is named in honor of the German mathematician Issai Schur (1875–1941), who ﬁrst studied
matrices of this type. Schur was a student and collaborator of Ferdinand Georg Frobenius
(p. 662). Schur and Frobenius were among the ﬁrst to study matrix theory as a discipline
unto itself, and each made great contributions to the subject. It was Emilie V. Haynsworth
(1916–1987)—a mathematical granddaughter of Schur—who introduced the phrase “Schur
complement” and developed several important aspects of the concept.

124
Chapter 3
Matrix Algebra
3.8
INVERSES OF SUMS AND SENSITIVITY
The reverse order law for inversion makes the inverse of a product easy to deal
with, but the inverse of a sum is much more diﬃcult. To begin with, (A + B)−1
may not exist even if A−1 and B−1 each exist. Moreover, if (A + B)−1 exists,
then, with rare exceptions, (A + B)−1 ̸= A−1 + B−1. This doesn’t even hold
for scalars (i.e., 1 × 1 matrices), so it has no chance of holding in general.
There is no useful general formula for (A+B)−1, but there are some special
sums for which something can be said. One of the most easily inverted sums is
I + cdT in which c and d are n × 1 nonzero columns such that 1 + dT c ̸= 0.
It’s straightforward to verify by direct multiplication that

I + cdT 	−1 = I −
cdT
1 + dT c.
(3.8.1)
If I is replaced by a nonsingular matrix A satisfying 1 + dT A−1c ̸= 0, then
the reverse order law for inversion in conjunction with (3.8.1) yields
(A + cdT )−1 =

A(I + A−1cdT )
−1
= (I + A−1cdT )−1A−1
=

I −
A−1cdT
1 + dT A−1c

A−1 = A−1 −A−1cdT A−1
1 + dT A−1c .
This is often called the Sherman–Morrison
21 rank-one update formula because
it can be shown (Exercise 3.9.9, p. 140) that rank (cdT ) = 1 when c ̸= 0 ̸= d.
Sherman–Morrison Formula
•
If An×n is nonsingular and if c and d are n × 1 columns such
that 1 + dT A−1c ̸= 0, then the sum A + cdT is nonsingular, and

A + cdT 	−1 = A−1 −A−1cdT A−1
1 + dT A−1c .
(3.8.2)
•
The Sherman–Morrison–Woodbury formula is a generalization. If C
and D are n × k such that (I + DT A−1C)−1 exists, then
(A + CDT )−1 = A−1 −A−1C(I + DT A−1C)−1DT A−1.
(3.8.3)
21
This result appeared in the 1949–1950 work of American statisticians J. Sherman and W. J.
Morrison, but they were not the ﬁrst to discover it. The formula was independently presented
by the English mathematician W. J. Duncan in 1944 and by American statisticians L. Guttman
(1946), Max Woodbury (1950), and M. S. Bartlett (1951). Since its derivation is so natural, it
almost certainly was discovered by many others along the way. Recognition and fame are often
not aﬀorded simply for introducing an idea, but rather for applying the idea to a useful end.

3.8 Inverses of Sums and Sensitivity
125
The Sherman–Morrison–Woodbury formula (3.8.3) can be veriﬁed with di-
rect multiplication, or it can be derived as indicated in Exercise 3.8.6.
To appreciate the utility of the Sherman–Morrison formula, suppose A−1
is known from a previous calculation, but now one entry in A needs to be
changed or updated—say we need to add α to aij. It’s not necessary to start
from scratch to compute the new inverse because Sherman–Morrison shows how
the previously computed information in A−1 can be updated to produce the
new inverse. Let c = ei and d = αej, where ei and ej are the ith and jth
unit columns, respectively. The matrix cdT has α in the (i, j)-position and
zeros elsewhere so that
B = A + cdT = A + αeieT
j
is the updated matrix. According to the Sherman–Morrison formula,
B−1 =

A + αeieT
j
	−1 = A−1 −α A−1eieT
j A−1
1 + αeT
j A−1ei
= A−1 −α[A−1]∗i[A−1]j∗
1 + α[A−1]ji
(recall Exercise 3.5.4).
(3.8.4)
This shows how A−1 changes when aij is perturbed, and it provides a useful
algorithm for updating A−1.
Example 3.8.1
Problem: Start with A and A−1 given below. Update A by adding 1 to a21,
and then use the Sherman–Morrison formula to update A−1 :
A =

1
2
1
3

and
A−1 =

3
−2
−1
1

.
Solution: The updated matrix is
B =

1
2
2
3

=

1
2
1
3

+

0
0
1
0

=

1
2
1
3

+

0
1

( 1
0 ) = A + e2eT
1 .
Applying the Sherman–Morrison formula yields the updated inverse
B−1 = A−1 −A−1e2eT
1 A−1
1 + eT
1 A−1e2
= A−1 −[A−1]∗2[A−1]1∗
1 + [A−1]12
=

3
−2
−1
1

−

−2
1

( 3
−2 )
1 −2
=

−3
2
2
−1

.

126
Chapter 3
Matrix Algebra
Another sum that often requires inversion is I −A, but we have to be
careful because (I−A)−1 need not always exist. However, we are safe when the
entries in A are suﬃciently small. In particular, if the entries in A are small
enough in magnitude to insure that limn→∞An = 0, then, analogous to scalar
algebra,
(I −A)(I + A + A2 + · · · + An−1) = I −An →I
as
n →∞,
so we have the following matrix version of a geometric series.
Neumann Series
If limn→∞An = 0, then I −A is nonsingular and
(I −A)−1 = I + A + A2 + · · · =
∞

k=0
Ak.
(3.8.5)
This is the Neumann series. It provides approximations of (I −A)−1
when A has entries of small magnitude. For example, a ﬁrst-order ap-
proximation is (I −A)−1 ≈I+A. More on the Neumann series appears
in Example 7.3.1, p. 527, and the complete statement is developed on
p. 618.
While there is no useful formula for (A + B)−1 in general, the Neumann
series allows us to say something when B has small entries relative to A, or
vice versa. For example, if A−1 exists, and if the entries in B are small enough
in magnitude to insure that limn→∞

A−1B
	n = 0, then
(A + B)−1 =

A

I −

−A−1B
	 −1
=

I −

−A−1B
 −1
A−1
=
 ∞

k=0

−A−1B
k

A−1,
and a ﬁrst-order approximation is
(A + B)−1 ≈A−1 −A−1BA−1.
(3.8.6)
Consequently, if A is perturbed by a small matrix B, possibly resulting from
errors due to inexact measurements or perhaps from roundoﬀerror, then the
resulting change in A−1 is about A−1BA−1. In other words, the eﬀect of a
small perturbation (or error) B is magniﬁed by multiplication (on both sides)
with A−1, so if A−1 has large entries, small perturbations (or errors) in A can
produce large perturbations (or errors) in the resulting inverse. You can reach

3.8 Inverses of Sums and Sensitivity
127
essentially the same conclusion from (3.8.4) when only a single entry is perturbed
and from Exercise 3.8.2 when a single column is perturbed.
This discussion resolves, at least in part, an issue raised in §1.6—namely,
“What mechanism determines the extent to which a nonsingular system Ax = b
is ill-conditioned?” To see how, an aggregate measure of the magnitude of the
entries in A is needed, and one common measure is
∥A∥= max
i

j
|aij| = the maximum absolute row sum.
(3.8.7)
This is one example of a matrix norm, a detailed discussion of which is given in
§5.1. Theoretical properties speciﬁc to (3.8.7) are developed on pp. 280 and 283,
and one property established there is the fact that ∥XY∥≤∥X∥∥Y∥for all
conformable matrices X and Y. But let’s keep things on an intuitive level for
the time being and defer the details. Using the norm (3.8.7), the approximation
(3.8.6) insures that if ∥B∥is suﬃciently small, then
$$A−1 −(A + B)−1$$ ≈
$$A−1BA−1$$ ≤
$$A−1$$ ∥B∥
$$A−1$$ ,
so, if we interpret x <∼y to mean that x is bounded above by something not
far from y, we can write
$$A−1 −(A + B)−1$$
∥A−1∥
<∼
$$A−1$$ ∥B∥=
$$A−1$$ ∥A∥
%∥B∥
∥A∥
&
.
The term on the left is the relative change in the inverse, and ∥B∥/ ∥A∥is the
relative change in A. The number κ =
$$A−1$$ ∥A∥is therefore the “magniﬁ-
cation factor” that dictates how much the relative change in A is magniﬁed.
This magniﬁcation factor κ is called a condition number for A. In other
words, if κ is small relative to 1 (i.e., if A is well conditioned), then a small
relative change (or error) in A cannot produce a large relative change (or error)
in the inverse, but if κ is large (i.e., if A is ill conditioned), then a small rela-
tive change (or error) in A can possibly (but not necessarily) result in a large
relative change (or error) in the inverse.
The situation for linear systems is similar. If the coeﬃcients in a nonsingular
system Ax = b are slightly perturbed to produce the system (A + B)˜x = b,
then x = A−1b and ˜x = (A + B)−1b so that (3.8.6) implies
x −˜x = A−1b −(A + B)−1b ≈A−1b −

A−1 −A−1BA−1	
b = A−1Bx.
For column vectors, (3.8.7) reduces to ∥x∥= maxi |xi|, and we have
∥x −˜x∥<∼
$$A−1$$ ∥B∥∥x∥,

128
Chapter 3
Matrix Algebra
so the relative change in the solution is
∥x −˜x∥
∥x∥
<∼
$$A−1$$ ∥B∥=
$$A−1$$ ∥A∥
%∥B∥
∥A∥
&
= κ
%∥B∥
∥A∥
&
.
(3.8.8)
Again, the condition number κ is pivotal because when κ is small, a small
relative change in A cannot produce a large relative change in x, but for larger
values of κ, a small relative change in A can possibly result in a large relative
change in x. Below is a summary of these observations.
Sensitivity and Conditioning
•
A nonsingular matrix A is said to be ill conditioned if a small
relative change in A can cause a large relative change in A−1.
The degree of ill-conditioning is gauged by a condition number
κ = ∥A∥∥A−1∥, where ∥⋆∥is a matrix norm.
•
The sensitivity of the solution of Ax = b to perturbations (or
errors) in A is measured by the extent to which A is an ill-
conditioned matrix. More is said in Example 5.12.1 on p. 414.
Example 3.8.2
It was demonstrated in Example 1.6.1 that the system
.835x + .667y = .168,
.333x + .266y = .067,
is sensitive to small perturbations. We can understand this in the current context
by examining the condition number of the coeﬃcient matrix. If the matrix norm
(3.8.7) is employed with
A =

.835
.667
.333
.266

and
A−1 =

−266000
667000
333000
−835000

,
then the condition number for A is
κ = κ = ∥A∥∥A−1∥= (1.502)(1168000) = 1, 754, 336 ≈1.7 × 106.
Since the right-hand side of (3.8.8) is only an estimate of the relative error in
the solution, the exact value of κ is not as important as its order of magnitude.
Because κ is of order 106, (3.8.8) holds the possibility that the relative change
(or error) in the solution can be about a million times larger than the relative

3.8 Inverses of Sums and Sensitivity
129
change (or error) in A. Therefore, we must consider A and the associated linear
system to be ill conditioned.
A Rule of Thumb. If Gaussian elimination with partial pivoting is used to
solve a well-scaled nonsingular system Ax = b using t -digit ﬂoating-point
arithmetic, then, assuming no other source of error exists, it can be argued that
when κ is of order 10p, the computed solution is expected to be accurate to
at least t −p signiﬁcant digits, more or less. In other words, one expects to
lose roughly p signiﬁcant ﬁgures. For example, if Gaussian elimination with 8-
digit arithmetic is used to solve the 2 × 2 system given above, then only about
t −p = 8 −6 = 2 signiﬁcant ﬁgures of accuracy should be expected. This
doesn’t preclude the possibility of getting lucky and attaining a higher degree of
accuracy—it just says that you shouldn’t bet the farm on it.
The complete story of conditioning has not yet been told. As pointed out ear-
lier, it’s about three times more costly to compute A−1 than to solve Ax = b,
so it doesn’t make sense to compute A−1 just to estimate the condition of A.
Questions concerning condition estimation without explicitly computing an in-
verse still need to be addressed. Furthermore, liberties allowed by using the ≈
and
<∼symbols produce results that are intuitively correct but not rigorous.
Rigor will eventually be attained—see Example 5.12.1on p. 414.
Exercises for section 3.8
3.8.1. Suppose you are given that
A =


2
0
−1
−1
1
1
−1
0
1


and
A−1 =


1
0
1
0
1
−1
1
0
2

.
(a)
Use the Sherman–Morrison formula to determine the inverse of
the matrix B that is obtained by changing the (3, 2)-entry in
A from 0 to 2.
(b)
Let C be the matrix that agrees with A except that c32 = 2
and c33 = 2. Use the Sherman–Morrison formula to ﬁnd C−1.
3.8.2. Suppose A and B are nonsingular matrices in which B is obtained
from A by replacing A∗j with another column b. Use the Sherman–
Morrison formula to derive the fact that
B−1 = A−1 −

A−1b −ej
	
[A−1]j∗
[A−1]j∗b
.

130
Chapter 3
Matrix Algebra
3.8.3. Suppose the coeﬃcient matrix of a nonsingular system Ax = b is up-
dated to produce another nonsingular system (A + cdT )z = b, where
b, c, d ∈ℜn×1, and let y be the solution of Ay = c. Show that
z = x −ydT x/(1 + dT y).
3.8.4.
(a)
Use the Sherman–Morrison formula to prove that if A is non-
singular, then A + αeieT
j
is nonsingular for a suﬃciently small
α.
(b)
Use part (a) to prove that I + E is nonsingular when all ϵij ’s
are suﬃciently small in magnitude. This is an alternative to using
the Neumann series argument.
3.8.5. For given matrices A and B, where A is nonsingular, explain why
A + ϵB is also nonsingular when the real number ϵ is constrained to
a suﬃciently small interval about the origin. In other words, prove that
small perturbations of nonsingular matrices are also nonsingular.
3.8.6. Derive the Sherman–Morrison–Woodbury formula. Hint: Recall Exer-
cise 3.7.11, and consider the product
 I
C
0
I
 A
C
DT
−I
 I
0
DT
I

.
3.8.7. Using the norm (3.8.7), rank the following matrices according to their
degree of ill-conditioning:
A =


100
0
−100
0
100
−100
−100
−100
300

,
B =


1
8
−1
−9
−71
11
1
17
18

,
C =


1
22
−42
0
1
−45
−45
−948
1

.
3.8.8. Suppose that the entries in A(t),
x(t), and b(t) are diﬀerentiable
functions of a real variable t such that A(t)x(t) = b(t).
(a)
Assuming that A(t)−1 exists, explain why
dA(t)−1
dt
= −A(t)−1A′(t)A(t)−1.
(b)
Derive the equation
x′(t) = A(t)−1b′(t) −A(t)−1A′(t)x(t).
This shows that A−1 magniﬁes both the change in A and the
change in b, and thus it conﬁrms the observation derived from
(3.8.8) saying that the sensitivity of a nonsingular system to
small perturbations is directly related to the magnitude of the
entries in A−1.

3.9 Elementary Matrices and Equivalence
131
3.9
ELEMENTARY MATRICES AND EQUIVALENCE
A common theme in mathematics is to break complicated objects into more
elementary components, such as factoring large polynomials into products of
smaller polynomials. The purpose of this section is to lay the groundwork for
similar ideas in matrix algebra by considering how a general matrix might be
factored into a product of more “elementary” matrices.
Elementary Matrices
Matrices of the form I−uvT , where u and v are n × 1 columns such
that vT u ̸= 1 are called elementary matrices, and we know from
(3.8.1) that all such matrices are nonsingular and

I −uvT 	−1 = I −
uvT
vT u −1.
(3.9.1)
Notice that inverses of elementary matrices are elementary matrices.
We are primarily interested in the elementary matrices associated with the
three elementary row (or column) operations hereafter referred to as follows.
•
Type I is interchanging rows (columns) i and j.
•
Type II is multiplying row (column) i by α ̸= 0.
•
Type III is adding a multiple of row (column) i to row (column) j.
An elementary matrix of Type I, II, or III is created by performing an elementary
operation of Type I, II, or III to an identity matrix. For example, the matrices
E1 =


0
1
0
1
0
0
0
0
1

, E2 =


1
0
0
0
α
0
0
0
1

,
and E3 =


1
0
0
0
1
0
α
0
1


(3.9.2)
are elementary matrices of Types I, II, and III, respectively, because E1 arises
by interchanging rows 1 and 2 in I3, whereas E2 is generated by multiplying
row 2 in I3 by α, and E3 is constructed by multiplying row 1 in I3 by α
and adding the result to row 3. The matrices in (3.9.2) also can be generated by
column operations. For example, E3 can be obtained by adding α times the
third column of I3 to the ﬁrst column. The fact that E1, E2, and E3 are of
the form (3.9.1) follows by using the unit columns ei to write
E1 = I−uuT , where u = e1−e2,
E2 = I−(1−α)e2eT
2 ,
and
E3 = I+αe3eT
1 .

132
Chapter 3
Matrix Algebra
These observations generalize to matrices of arbitrary size.
One of our objectives is to remove the arrows from Gaussian elimination
because the inability to do “arrow algebra” limits the theoretical analysis. For
example, while it makes sense to add two equations together, there is no mean-
ingful analog for arrows—reducing A →B and C →D by row operations does
not guarantee that A + C →B + D is possible. The following properties are
the mechanisms needed to remove the arrows from elimination processes.
Properties of Elementary Matrices
•
When used as a left-hand multiplier, an elementary matrix of Type
I, II, or III executes the corresponding row operation.
•
When used as a right-hand multiplier, an elementary matrix of Type
I, II, or III executes the corresponding column operation.
Proof.
A proof for Type III operations is given—the other two cases are left to
the reader. Using I + αejeT
i
as a left-hand multiplier on an arbitrary matrix A
produces

I + αejeT
i
	
A = A + αejAi∗= A + α







0
0
· · ·
0
...
...
...
ai1
ai2
· · ·
ain
...
...
...
0
0
· · ·
0







←jth row .
This is exactly the matrix produced by a Type III row operation in which the
ith row of A is multiplied by α and added to the jth row. When I + αejeT
i
is used as a right-hand multiplier on A, the result is
A

I + αejeT
i
	
= A + αA∗jeT
i = A + α






ith col
↓
0
· · ·
a1j
· · ·
0
0
· · ·
a2j
· · ·
0
...
...
...
0
· · ·
anj
· · ·
0






.
This is the result of a Type III column operation in which the jth column of A
is multiplied by α and then added to the ith column.

3.9 Elementary Matrices and Equivalence
133
Example 3.9.1
The sequence of row operations used to reduce A =


1
2
4
2
4
8
3
6
13

to EA is
indicated below.
A =


1
2
4
2
4
8
3
6
13

R2 −2R1
R3 −3R1
−→


1
2
4
0
0
0
0
0
1


Interchange R2 and R3
−−−−−−−−→


1
2
4
0
0
1
0
0
0


R1 −4R2
−→


1
2
0
0
0
1
0
0
0

= EA.
The reduction can be accomplished by a sequence of left-hand multiplications
with the corresponding elementary matrices as shown below.


1
−4
0
0
1
0
0
0
1




1
0
0
0
0
1
0
1
0




1
0
0
0
1
0
−3
0
1




1
0
0
−2
1
0
0
0
1

A = EA.
The product of these elementary matrices is P =


13
0
−4
−3
0
1
−2
1
0

, and you can
verify that it is indeed the case that PA = EA. Thus the arrows are eliminated
by replacing them with a product of elementary matrices.
We are now in a position to understand why nonsingular matrices are pre-
cisely those matrices that can be factored as a product of elementary matrices.
Products of Elementary Matrices
•
A is a nonsingular matrix if and only if A is the product
of elementary matrices of Type I, II, or III.
(3.9.3)
Proof.
If A is nonsingular, then the Gauss–Jordan technique reduces A to
I by row operations. If G1, G2, . . . , Gk is the sequence of elementary matrices
that corresponds to the elementary row operations used, then
Gk · · · G2G1A = I or, equivalently, A = G−1
1 G−1
2
· · · G−1
k .
Since the inverse of an elementary matrix is again an elementary matrix of the
same type, this proves that A is the product of elementary matrices of Type I,
II, or III. Conversely, if A = E1E2 · · · Ek is a product of elementary matrices,
then A must be nonsingular because the Ei ’s are nonsingular, and a product
of nonsingular matrices is also nonsingular.

134
Chapter 3
Matrix Algebra
Equivalence
•
Whenever B can be derived from A by a combination of elementary
row and column operations, we write A ∼B, and we say that A
and B are equivalent matrices. Since elementary row and column
operations are left-hand and right-hand multiplication by elementary
matrices, respectively, and in view of (3.9.3), we can say that
A ∼B ⇐⇒PAQ = B
for nonsingular P and Q.
•
Whenever B can be obtained from A by performing a sequence
of elementary row operations only, we write A
row
∼B, and we say
that A and B are row equivalent. In other words,
A
row
∼B ⇐⇒PA = B
for a nonsingular P.
•
Whenever B can be obtained from A by performing a sequence of
column operations only, we write A
col
∼B, and we say that A and
B are column equivalent. In other words,
A
col
∼B ⇐⇒AQ = B
for a nonsingular Q.
If it’s possible to go from A to B by elementary row and column oper-
ations, then clearly it’s possible to start with B and get back to A because
elementary operations are reversible—i.e., PAQ = B
=⇒
P−1BQ−1 = A. It
therefore makes sense to talk about the equivalence of a pair of matrices without
regard to order. In other words, A ∼B ⇐⇒B ∼A. Furthermore, it’s not
diﬃcult to see that each type of equivalence is transitive in the sense that
A ∼B
and
B ∼C
=⇒
A ∼C.
In §2.2 it was stated that each matrix A possesses a unique reduced row
echelon form EA, and we accepted this fact because it is intuitively evident.
However, we are now in a position to understand a rigorous proof.
Example 3.9.2
Problem: Prove that EA is uniquely determined by A.
Solution:
Without loss of generality, we may assume that A is square—
otherwise the appropriate number of zero rows or columns can be adjoined to A
without aﬀecting the results. Suppose that A
row
∼E1 and A
row
∼E2, where E1
and E2 are both in reduced row echelon form. Consequently, E1
row
∼E2, and
hence there is a nonsingular matrix P such that
PE1 = E2.
(3.9.4)

3.9 Elementary Matrices and Equivalence
135
Furthermore, by permuting the rows of E1 and E2 to force the pivotal 1’s to
occupy the diagonal positions, we see that
E1
row
∼T1
and
E2
row
∼T2,
(3.9.5)
where T1 and T2 are upper-triangular matrices in which the basic columns in
each Ti occupy the same positions as the basic columns in Ei. For example, if
E =


1
2
0
0
0
1
0
0
0

, then
T =


1
2
0
0
0
0
0
0
1

.
Each Ti has the property that T2
i = Ti because there is a permutation
matrix Qi (a product of elementary interchange matrices of Type I) such that
QiTiQT
i =

Iri
Ji
0
0

or, equivalently,
Ti = QT
i

Iri
Ji
0
0

Qi,
and QT
i = Q−1
i
(see Exercise 3.9.4) implies T2
i = Ti. It follows from (3.9.5)
that T1
row
∼T2, so there is a nonsingular matrix R such that RT1 = T2. Thus
T2 = RT1 = RT1T1 = T2T1
and
T1 = R−1T2 = R−1T2T2 = T1T2.
Because T1 and T2 are both upper triangular, T1T2 and T2T1 have the same
diagonal entries, and hence T1 and T2 have the same diagonal. Therefore, the
positions of the basic columns (i.e., the pivotal positions) in T1 agree with those
in T2, and hence E1 and E2 have basic columns in exactly the same positions.
This means there is a permutation matrix Q such that
E1Q =

Ir
J1
0
0

and
E2Q =

Ir
J2
0
0

.
Using (3.9.4) yields PE1Q = E2Q, or

P11
P12
P21
P22
 
Ir
J1
0
0

=

Ir
J2
0
0

,
which in turn implies that P11 = Ir and P11J1 = J2. Consequently, J1 = J2,
and it follows that E1 = E2.
In passing, notice that the uniqueness of EA implies the uniqueness of the
pivot positions in any other row echelon form derived from A. If A
row
∼U1
and A
row
∼U2, where U1 and U2 are row echelon forms with diﬀerent pivot
positions, then Gauss–Jordan reduction applied to U1 and U2 would lead to
two diﬀerent reduced echelon forms, which is impossible.
In §2.2 we observed the fact that the column relationships in a matrix A
are exactly the same as the column relationships in EA. This observation is a
special case of the more general result presented below.

136
Chapter 3
Matrix Algebra
Column and Row Relationships
•
If A
row
∼B, then linear relationships existing among columns of A
also hold among corresponding columns of B. That is,
B∗k =
n

j=1
αjB∗j
if and only if
A∗k =
n

j=1
αjA∗j.
(3.9.6)
•
In particular, the column relationships in A and EA must be iden-
tical, so the nonbasic columns in A must be linear combinations of
the basic columns in A as described in (2.2.3).
•
If A
col
∼B, then linear relationships existing among rows of A must
also hold among corresponding rows of B.
•
Summary. Row equivalence preserves column relationships, and col-
umn equivalence preserves row relationships.
Proof.
If A
row
∼B, then PA = B for some nonsingular P. Recall from (3.5.5)
that the jth column in B is given by
B∗j = (PA)∗j = PA∗j.
Therefore, if A∗k = 
j αjA∗j, then multiplication by P on the left produces
B∗k = 
j αjB∗j. Conversely, if B∗k = 
j αjB∗j, then multiplication on the
left by P−1 produces A∗k = 
j αjA∗j. The statement concerning column
equivalence follows by considering transposes.
The reduced row echelon form EA is as far as we can go in reducing A by
using only row operations. However, if we are allowed to use row operations in
conjunction with column operations, then, as described below, the end result of
a complete reduction is much simpler.
Rank Normal Form
If A is an m × n matrix such that rank (A) = r, then
A ∼Nr =

Ir
0
0
0

.
(3.9.7)
Nr is called the rank normal form for A, and it is the end product
of a complete reduction of A by using both row and column operations.

3.9 Elementary Matrices and Equivalence
137
Proof.
It is always true that A
row
∼EA so that there is a nonsingular matrix
P such that PA = EA. If rank (A) = r, then the basic columns in EA are
the r unit columns. Apply column interchanges to EA so as to move these r
unit columns to the far left-hand side. If Q1 is the product of the elementary
matrices corresponding to these column interchanges, then PAQ1 has the form
PAQ1 = EAQ1 =

Ir
J
0
0

.
Multiplying both sides of this equation on the right by the nonsingular matrix
Q2 =

Ir
−J
0
I

produces PAQ1Q2 =

Ir
J
0
0
 
Ir
−J
0
I

=

Ir
0
0
0

.
Thus A ∼Nr. because P and Q = Q1Q2 are nonsingular.
Example 3.9.3
Problem: Explain why rank
 A
0
0
B

= rank (A) + rank (B).
Solution:
If rank (A) = r and rank (B) = s, then A ∼Nr and B ∼Ns.
Consequently,

A
0
0
B

∼

Nr
0
0
Ns

=⇒
rank

A
0
0
B

= rank

Nr
0
0
Ns

= r + s.
Given matrices A and B, how do we decide whether or not A ∼B,
A
row
∼B, or A
col
∼B? We could use a trial-and-error approach by attempting to
reduce A to B by elementary operations, but this would be silly because there
are easy tests, as described below.
Testing for Equivalence
For m × n matrices A and B the following statements are true.
•
A ∼B if and only if rank (A) = rank (B).
(3.9.8)
•
A
row
∼B if and only if EA = EB.
(3.9.9)
•
A
col
∼B if and only if EAT = EBT .
(3.9.10)
Corollary. Multiplication by nonsingular matrices cannot change rank.

138
Chapter 3
Matrix Algebra
Proof.
To establish the validity of (3.9.8), observe that rank (A) = rank (B)
implies A ∼Nr and B ∼Nr. Therefore, A ∼Nr ∼B. Conversely, if A ∼B,
where rank (A) = r and rank (B) = s, then A ∼Nr and B ∼Ns, and
hence Nr ∼A ∼B ∼Ns. Clearly, Nr ∼Ns implies r = s. To prove (3.9.9),
suppose ﬁrst that A
row
∼B. Because B
row
∼EB, it follows that A
row
∼EB. Since
a matrix has a uniquely determined reduced echelon form, it must be the case
that EB = EA. Conversely, if EA = EB, then
A
row
∼EA = EB
row
∼B
=⇒
A
row
∼B.
The proof of (3.9.10) follows from (3.9.9) by considering transposes because
A
col
∼B ⇐⇒AQ = B ⇐⇒(AQ)T = BT
⇐⇒QT AT = BT ⇐⇒AT row
∼BT .
Example 3.9.4
Problem: Are the relationships that exist among the columns in A the same
as the column relationships in B, and are the row relationships in A the same
as the row relationships in B, where
A =


1
1
1
−4
−3
−1
2
1
−1


and
B =


−1
−1
−1
2
2
2
2
1
−1

?
Solution: Straightforward computation reveals that
EA = EB =


1
0
−2
0
1
3
0
0
0

,
and hence A
row
∼B. Therefore, the column relationships in A and B must be
identical, and they must be the same as those in EA. Examining EA reveals
that E∗3 = −2E∗1 + 3E∗2, so it must be the case that
A∗3 = −2A∗1 + 3A∗2
and
B∗3 = −2B∗1 + 3B∗2.
The row relationships in A and B are diﬀerent because EAT ̸= EBT .
On the surface, it may not seem plausible that a matrix and its transpose
should have the same rank. After all, if A is 3 × 100, then A can have as
many as 100 basic columns, but AT can have at most three. Nevertheless, we
can now demonstrate that rank (A) = rank

AT 	
.

3.9 Elementary Matrices and Equivalence
139
Transposition and Rank
Transposition does not change the rank—i.e., for all m × n matrices,
rank (A) = rank

AT 	
and
rank (A) = rank (A∗).
(3.9.11)
Proof.
Let rank (A) = r, and let P and Q be nonsingular matrices such that
PAQ = Nr =

Ir
0r×n−r
0m−r×r
0m−r×n−r

.
Applying the reverse order law for transposition produces QT AT PT = NT
r .
Since QT and PT are nonsingular, it follows that AT ∼NT
r , and therefore
rank

AT 	
= rank

NT
r
	
= rank

Ir
0r×m−r
0n−r×r
0n−r×m−r

= r = rank (A).
To prove rank (A) = rank (A∗), write Nr = Nr = PAQ = ¯P¯A¯Q, and use the
fact that the conjugate of a nonsingular matrix is again nonsingular (because
¯K−1 = K−1 ) to conclude that Nr ∼A, and hence rank (A) = rank
¯A
	
. It
now follows from rank (A) = rank

AT 	
that
rank (A∗) = rank
¯AT 	
= rank
¯A
	
= rank (A).
Exercises for section 3.9
3.9.1. Suppose that A is an m × n matrix.
(a)
If [A|Im] is row reduced to a matrix [B|P], explain why P
must be a nonsingular matrix such that PA = B.
(b)
If

A
In
!
is column reduced to

C
Q
!
, explain why Q must be a
nonsingular matrix such that AQ = C.
(c)
Find a nonsingular matrix P such that PA = EA, where
A =


1
2
3
4
2
4
6
7
1
2
3
6

.
(d)
Find nonsingular matrices P and Q such that PAQ is in rank
normal form.

140
Chapter 3
Matrix Algebra
3.9.2. Consider the two matrices
A =


2
2
0
−1
3
−1
4
0
0
−8
8
3


and
B =


2
−6
8
2
5
1
4
−1
3
−9
12
3

.
(a)
Are A and B equivalent?
(b)
Are A and B row equivalent?
(c)
Are A and B column equivalent?
3.9.3. If A
row
∼B, explain why the basic columns in A occupy exactly the
same positions as the basic columns in B.
3.9.4. A product of elementary interchange matrices—i.e., elementary matrices
of Type I—is called a permutation matrix. If P is a permutation
matrix, explain why P−1 = PT .
3.9.5. If An×n is a nonsingular matrix, which (if any) of the following state-
ments are true?
(a)
A ∼A−1.
(b)
A
row
∼A−1.
(c)
A
col
∼A−1.
(d)
A ∼I.
(e)
A
row
∼I.
(f)
A
col
∼I.
3.9.6. Which (if any) of the following statements are true?
(a)
A ∼B
=⇒
AT ∼BT .
(b)
A
row
∼B
=⇒
AT row
∼BT .
(c)
A
row
∼B
=⇒
AT col
∼BT .
(d)
A
row
∼B
=⇒
A ∼B.
(e)
A
col
∼B
=⇒
A ∼B.
(f)
A ∼B
=⇒
A
row
∼B.
3.9.7. Show that every elementary matrix of Type I can be written as a product
of elementary matrices of Types II and III. Hint: Recall Exercise 1.2.12
on p. 14.
3.9.8. If rank (Am×n) = r, show that there exist matrices Bm×r and Cr×n
such that A = BC, where rank (B) = rank (C) = r. Such a factor-
ization is called a full-rank factorization.
Hint: Consider the basic
columns of A and the nonzero rows of EA.
3.9.9. Prove that rank (Am×n) = 1 if and only if there are nonzero columns
um×1 and vn×1 such that
A = uvT .
3.9.10. Prove that if rank (An×n) = 1, then A2 = τA, where τ = trace (A).

3.10 The LU Factorization
141
3.10
THE LU FACTORIZATION
We have now come full circle, and we are back to where the text began—solving
a nonsingular system of linear equations using Gaussian elimination with back
substitution. This time, however, the goal is to describe and understand the
process in the context of matrices.
If Ax = b is a nonsingular system, then the object of Gaussian elimination
is to reduce A to an upper-triangular matrix using elementary row operations.
If no zero pivots are encountered, then row interchanges are not necessary, and
the reduction can be accomplished by using only elementary row operations of
Type III. For example, consider reducing the matrix
A =


2
2
2
4
7
7
6
18
22


to upper-triangular form as shown below:


2
2
2
4
7
7
6
18
22

R2 −2R1
R3 −3R1
−→


2
2
2
0
3
3
0
12
16


R3 −4R2
−→


2
2
2
0
3
3
0
0
4

= U.
(3.10.1)
We learned in the previous section that each of these Type III operations can be
executed by means of a left-hand multiplication with the corresponding elemen-
tary matrix Gi, and the product of all of these Gi ’s is
G3G2G1 =


1
0
0
0
1
0
0
−4
1




1
0
0
0
1
0
−3
0
1




1
0
0
−2
1
0
0
0
1

=


1
0
0
−2
1
0
5
−4
1

.
In other words, G3G2G1A = U, so that A = G−1
1 G−1
2 G−1
3 U = LU, where
L is the lower-triangular matrix
L = G−1
1 G−1
2 G−1
3
=


1
0
0
2
1
0
3
4
1

.
Thus A = LU is a product of a lower-triangular matrix L and an upper-
triangular matrix U. Naturally, this is called an LU factorization of A.

142
Chapter 3
Matrix Algebra
Observe that U is the end product of Gaussian elimination and has the
pivots on its diagonal, while L has 1’s on its diagonal. Moreover, L has the
remarkable property that below its diagonal, each entry ℓij is precisely the
multiplier used in the elimination (3.10.1) to annihilate the (i, j)-position.
This is characteristic of what happens in general. To develop the gen-
eral theory, it’s convenient to introduce the concept of an elementary lower-
triangular matrix, which is deﬁned to be an n × n triangular matrix of the
form
Tk = I −ckeT
k ,
where ck is a column with zeros in the ﬁrst k positions. In particular, if
ck =









0
0
...
µk+1
...
µn









,
then
Tk =











1
0
· · ·
0
0
· · ·
0
0
1
· · ·
0
0
· · ·
0
...
...
...
...
...
...
0
0
· · ·
1
0
· · ·
0
0
0
· · ·
−µk+1
1
· · ·
0
...
...
...
...
...
...
0
0
· · ·
−µn
0
· · ·
1











.
(3.10.2)
By observing that eT
k ck = 0, the formula for the inverse of an elementary matrix
given in (3.9.1) produces
T−1
k
= I + ckeT
k =











1
0
· · ·
0
0
· · ·
0
0
1
· · ·
0
0
· · ·
0
...
...
...
...
...
...
0
0
· · ·
1
0
· · ·
0
0
0
· · ·
µk+1
1
· · ·
0
...
...
...
...
...
...
0
0
· · ·
µn
0
· · ·
1











,
(3.10.3)
which is also an elementary lower-triangular matrix. The utility of elementary
lower-triangular matrices lies in the fact that all of the Type III row operations
needed to annihilate the entries below the kth pivot can be accomplished with
one multiplication by Tk. If
Ak−1 =











∗
∗
· · ·
α1
∗
· · ·
∗
0
∗
· · ·
α2
∗
· · ·
∗
...
...
...
...
...
...
0
0
· · ·
αk
∗
· · ·
∗
0
0
· · ·
αk+1
∗
· · ·
∗
...
...
...
...
...
...
0
0
· · ·
αn
∗
· · ·
∗












3.10 The LU Factorization
143
is the partially triangularized result after k −1 elimination steps, then
TkAk−1 =

I −ckeT
k
	
Ak−1 = Ak−1 −ckeT
k Ak−1
=











∗
∗
· · ·
α1
∗
· · ·
∗
0
∗
· · ·
α2
∗
· · ·
∗
...
...
...
...
...
...
0
0
· · ·
αk
∗
· · ·
∗
0
0
· · ·
0
∗
· · ·
∗
...
...
...
...
...
...
0
0
· · ·
0
∗
· · ·
∗











,
where
ck =











0
0
...
0
αk+1/αk
...
αn/αk











contains the multipliers used to annihilate those entries below αk. Notice that
Tk does not alter the ﬁrst k −1 columns of Ak−1 because eT
k [Ak−1]∗j = 0
whenever j ≤k−1. Therefore, if no row interchanges are required, then reducing
A to an upper-triangular matrix U by Gaussian elimination is equivalent to
executing a sequence of n −1 left-hand multiplications with elementary lower-
triangular matrices. That is, Tn−1 · · · T2T1A = U, and hence
A = T−1
1 T−1
2
· · · T−1
n−1U.
(3.10.4)
Making use of the fact that eT
j ck = 0 whenever j ≤k and applying (3.10.3)
reveals that
T−1
1 T−1
2
· · · T−1
n−1 =

I + c1eT
1
	 
I + c2eT
2
	
· · ·

I + cn−1eT
n−1
	
= I + c1eT
1 + c2eT
2 + · · · + cn−1eT
n−1.
(3.10.5)
By observing that
ckeT
k =











0
0
· · ·
0
0
· · ·
0
0
0
· · ·
0
0
· · ·
0
...
...
...
...
...
...
0
0
· · ·
0
0
· · ·
0
0
0
· · ·
ℓk+1,k
0
· · ·
0
...
...
...
...
...
...
0
0
· · ·
ℓnk
0
· · ·
0











,
where the ℓik ’s are the multipliers used at the kth stage to annihilate the entries
below the kth pivot, it now follows from (3.10.4) and (3.10.5) that
A = LU,

144
Chapter 3
Matrix Algebra
where
L = I + c1eT
1 + c2eT
2 + · · · + cn−1eT
n−1 =






1
0
0
· · ·
0
ℓ21
1
0
· · ·
0
ℓ31
ℓ32
1
· · ·
0
...
...
...
...
...
ℓn1
ℓn2
ℓn3
· · ·
1






(3.10.6)
is the lower-triangular matrix with 1’s on the diagonal, and where ℓij is precisely
the multiplier used to annihilate the (i, j) -position during Gaussian elimination.
Thus the factorization A = LU can be viewed as the matrix formulation of
Gaussian elimination, with the understanding that no row interchanges are used.
LU Factorization
If A is an n × n matrix such that a zero pivot is never encountered
when applying Gaussian elimination with Type III operations, then A
can be factored as the product A = LU, where the following hold.
•
L is lower triangular and U is upper triangular.
(3.10.7)
•
ℓii = 1 and uii ̸= 0 for each i = 1, 2, . . . , n.
(3.10.8)
•
Below the diagonal of L, the entry ℓij is the multiple of row j that
is subtracted from row i in order to annihilate the (i, j) -position
during Gaussian elimination.
•
U is the ﬁnal result of Gaussian elimination applied to A.
•
The matrices L and U are uniquely determined by properties
(3.10.7) and (3.10.8).
The decomposition of A into A = LU is called the LU factorization
of A, and the matrices L and U are called the LU factors of A.
Proof.
Except for the statement concerning the uniqueness of the LU fac-
tors, each point has already been established. To prove uniqueness, observe
that LU factors must be nonsingular because they have nonzero diagonals. If
L1U1 = A = L2U2 are two LU factorizations for A, then
L−1
2 L1 = U2U−1
1 .
(3.10.9)
Notice that L−1
2 L1 is lower triangular, while U2U−1
1
is upper triangular be-
cause the inverse of a matrix that is upper (lower) triangular is again up-
per (lower) triangular, and because the product of two upper (lower) trian-
gular matrices is also upper (lower) triangular. Consequently, (3.10.9) implies
L−1
2 L1 = D = U2U−1
1
must be a diagonal matrix. However, [L2]ii = 1 =
[L−1
2 ]ii, so it must be the case that L−1
2 L1 = I = U2U−1
1 , and thus L1 = L2
and U1 = U2.

3.10 The LU Factorization
145
Example 3.10.1
Once L and U are known, there is usually no need to manipulate with A. This
together with the fact that the multipliers used in Gaussian elimination occur in
just the right places in L means that A can be successively overwritten with the
information in L and U as Gaussian elimination evolves. The rule is to store
the multiplier ℓij in the position it annihilates—namely, the (i, j)-position of
the array. For a 3 × 3 matrix, the result looks like this:


a11
a12
a13
a21
a22
a23
a31
a32
a33


T ype III operations
−−−−−−−−→


u11
u12
u13
ℓ21
u22
u23
ℓ31
ℓ32
u33

.
For example, generating the LU factorization of
A =


2
2
2
4
7
7
6
18
22


by successively overwriting a single 3 × 3 array would evolve as shown below:


2
2
2
4
7
7
6
18
22

R2 −2R1
R3 −3R1
−→


2
2
2
⃝
2
3
3
⃝
3
12
16


R3 −4R2
−→


2
2
2
⃝
2
3
3
⃝
3
⃝
4
4

.
Thus
L =


1
0
0
2
1
0
3
4
1


and
U =


2
2
2
0
3
3
0
0
4

.
This is an important feature in practical computation because it guarantees that
an LU factorization requires no more computer memory than that required to
store the original matrix A.
Once the LU factors for a nonsingular matrix An×n have been obtained,
it’s relatively easy to solve a linear system Ax = b. By rewriting Ax = b as
L(Ux) = b
and setting
y = Ux,
we see that Ax = b is equivalent to the two triangular systems
Ly = b
and
Ux = y.
First, the lower-triangular system Ly = b is solved for y by forward substi-
tution. That is, if






1
0
0
· · ·
0
ℓ21
1
0
· · ·
0
ℓ31
ℓ32
1
· · ·
0
...
...
...
...
...
ℓn1
ℓn2
ℓn3
· · ·
1












y1
y2
y3
...
yn






=






b1
b2
b3
...
bn






,

146
Chapter 3
Matrix Algebra
set
y1 = b1,
y2 = b2 −ℓ21y1,
y3 = b3 −ℓ31y1 −ℓ32y2,
etc.
The forward substitution algorithm can be written more concisely as
y1 = b1
and
yi = bi −
i−1

k=1
ℓikyk
for
i = 2, 3, . . . , n.
(3.10.10)
After y is known, the upper-triangular system Ux = y is solved using the
standard back substitution procedure by starting with xn = yn/unn, and setting
xi = 1
uii

yi −
n

k=i+1
uikxk

for
i = n −1, n −2, . . . , 1.
(3.10.11)
It can be veriﬁed that only n2 multiplications/divisions and n2 −n addi-
tions/subtractions are required when (3.10.10) and (3.10.11) are used to solve
the two triangular systems Ly = b and Ux = y, so it’s relatively cheap to
solve Ax = b once L and U are known—recall from §1.2 that these operation
counts are about n3/3 when we start from scratch.
If only one system Ax = b is to be solved, then there is no signiﬁcant
diﬀerence between the technique of reducing the augmented matrix [A|b] to
a row echelon form and the LU factorization method presented here. However,
suppose it becomes necessary to later solve other systems Ax = ˜b with the
same coeﬃcient matrix but with diﬀerent right-hand sides, which is frequently
the case in applied work. If the LU factors of A were computed and saved
when the original system was solved, then they need not be recomputed, and
the solutions to all subsequent systems Ax = ˜b are therefore relatively cheap
to obtain. That is, the operation counts for each subsequent system are on the
order of n2, whereas these counts would be on the order of n3/3 if we would
start from scratch each time.
Summary
•
To solve a nonsingular system Ax = b using the LU factorization
A = LU, ﬁrst solve Ly = b for y with the forward substitution
algorithm (3.10.10), and then solve Ux = y for x with the back
substitution procedure (3.10.11).
•
The advantage of this approach is that once the LU factors for
A have been computed, any other linear system Ax = ˜b can
be solved with only n2 multiplications/divisions and n2 −n ad-
ditions/subtractions.

3.10 The LU Factorization
147
Example 3.10.2
Problem 1: Use the LU factorization of A to solve Ax = b, where
A =


2
2
2
4
7
7
6
18
22


and
b =


12
24
12

.
Problem 2: Suppose that after solving the original system new information is
received that changes b to
˜b =


6
24
70

.
Use the LU factors of A to solve the updated system Ax = ˜b.
Solution 1: The LU factors of the coeﬃcient matrix were determined in Example
3.10.1 to be
L =


1
0
0
2
1
0
3
4
1


and
U =


2
2
2
0
3
3
0
0
4

.
The strategy is to set Ux = y and solve Ax = L(Ux) = b by solving the two
triangular systems
Ly = b
and
Ux = y.
First solve the lower-triangular system Ly = b by using forward substitution:


1
0
0
2
1
0
3
4
1




y1
y2
y3

=


12
24
12


=⇒
y1 = 12,
y2 = 24 −2y1 = 0,
y3 = 12 −3y1 −4y2 = −24.
Now use back substitution to solve the upper-triangular system Ux = y:


2
2
2
0
3
3
0
0
4




x1
x2
x3

=


12
0
−24


=⇒
x1 = (12 −2x2 −2x3)/2 = 6,
x2 = (0 −3x3)/3 = 6,
x3 = −24/4 = −6.
Solution 2: To solve the updated system Ax = ˜b, simply repeat the forward
and backward substitution steps with b replaced by ˜b. Solving Ly = ˜b with
forward substitution gives the following:


1
0
0
2
1
0
3
4
1




y1
y2
y3

=


6
24
70


=⇒
y1 = 6,
y2 = 24 −2y1 = 12,
y3 = 70 −3y1 −4y2 = 4.
Using back substitution to solve Ux = y gives the following updated solution:


2
2
2
0
3
3
0
0
4




x1
x2
x3

=


6
12
4


=⇒
x1 = (6 −2x2 −2x3)/2 = −1,
x2 = (12 −3x3)/3 = 3,
x3 = 4/4 = 1.

148
Chapter 3
Matrix Algebra
Example 3.10.3
Computing A−1.
Although matrix inversion is not used for solving Ax = b,
there are a few applications where explicit knowledge of A−1 is desirable.
Problem: Explain how to use the LU factors of a nonsingular matrix An×n to
compute A−1 eﬃciently.
Solution: The strategy is to solve the matrix equation AX = I. Recall from
(3.5.5) that AA−1 = I implies A[A−1]∗j = ej, so the jth column of A−1
is the solution of a system Axj = ej. Each of these n systems has the same
coeﬃcient matrix, so, once the LU factors for A are known, each system Axj =
LUxj = ej can be solved by the standard two-step process.
(1)
Set yj = Uxj, and solve Lyj = ej for yj by forward substitution.
(2)
Solve Uxj = yj for xj = [A−1]∗j by back substitution.
This method has at least two advantages: it’s eﬃcient, and any code written to
solve Ax = b can also be used to compute A−1.
Note: A tempting alternate solution might be to use the fact A−1 = (LU)−1 =
U−1L−1. But computing U−1 and L−1 explicitly and then multiplying the
results is not as computationally eﬃcient as the method just described.
Not all nonsingular matrices possess an LU factorization. For example, there
is clearly no nonzero value of u11 that will satisfy

0
1
1
1

=

1
0
ℓ21
1
 
u11
u12
0
u22

.
The problem here is the zero pivot in the (1,1)-position. Our development of
the LU factorization using elementary lower-triangular matrices shows that if no
zero pivots emerge, then no row interchanges are necessary, and the LU factor-
ization can indeed be carried to completion. The converse is also true (its proof
is left as an exercise), so we can say that a nonsingular matrix A has an LU
factorization if and only if a zero pivot does not emerge during row reduction to
upper-triangular form with Type III operations.
Although it is a bit more theoretical, there is another interesting way to
characterize the existence of LU factors. This characterization is given in terms
of the leading principal submatrices of A that are deﬁned to be those
submatrices taken from the upper-left-hand corner of A. That is,
A1 =

a11

, A2 =

a11
a12
a21
a22

, . . . , Ak =




a11
a12
· · ·
a1k
a21
a22
· · ·
a2k
...
...
...
...
ak1
ak2
· · ·
akk



, . . . .

3.10 The LU Factorization
149
Existence of LU Factors
Each of the following statements is equivalent to saying that a nonsin-
gular matrix An×n possesses an LU factorization.
•
A zero pivot does not emerge during row reduction to upper-
triangular form with Type III operations.
•
Each leading principal submatrix Ak is nonsingular.
(3.10.12)
Proof.
We will prove the statement concerning the leading principal submatri-
ces and leave the proof concerning the nonzero pivots as an exercise. Assume
ﬁrst that A possesses an LU factorization and partition A as
A = LU =

L11
0
L21
L22
 
U11
U12
0
U22

=

L11U11
∗
∗
∗

,
where L11 and U11 are each k × k. Thus Ak = L11U11 must be nonsingular
because L11 and U11 are each nonsingular—they are triangular with nonzero
diagonal entries. Conversely, suppose that each leading principal submatrix in
A is nonsingular. Use induction to prove that each Ak possesses an LU fac-
torization. For k = 1, this statement is clearly true because if A1 = (a11) is
nonsingular, then A1 = (1)(a11) is its LU factorization. Now assume that Ak
has an LU factorization and show that this together with the nonsingularity
condition implies Ak+1 must also possess an LU factorization. If Ak = LkUk
is the LU factorization for Ak, then A−1
k
= U−1
k L−1
k
so that
Ak+1 =

Ak
b
cT
αk+1

=

Lk
0
cT U−1
k
1
 
Uk
L−1
k b
0
αk+1 −cT A−1
k b

, (3.10.13)
where cT and b contain the ﬁrst k components of Ak+1∗and A∗k+1, re-
spectively. Observe that this is the LU factorization for Ak+1 because
Lk+1 =

Lk
0
cT U−1
k
1

and
Uk+1 =

Uk
L−1
k b
0
αk+1 −cT A−1
k b

are lower- and upper-triangular matrices, respectively, and L has 1’s on its
diagonal while the diagonal entries of U are nonzero. The fact that
αk+1 −cT A−1
k b ̸= 0
follows because Ak+1 and Lk+1 are each nonsingular, so Uk+1 = L−1
k+1Ak+1
must also be nonsingular. Therefore, the nonsingularity of the leading principal

150
Chapter 3
Matrix Algebra
submatrices implies that each Ak possesses an LU factorization, and hence
An = A must have an LU factorization.
Up to this point we have avoided dealing with row interchanges because if
a row interchange is needed to remove a zero pivot, then no LU factorization is
possible. However, we know from the discussion in §1.5 that practical computa-
tion necessitates row interchanges in the form of partial pivoting. So even if no
zero pivots emerge, it is usually the case that we must still somehow account for
row interchanges.
To understand the eﬀects of row interchanges in the framework of an LU
decomposition, let Tk = I −ckeT
k be an elementary lower-triangular matrix as
described in (3.10.2), and let E = I −uuT with u = ek+i −ek+j be the Type I
elementary interchange matrix associated with an interchange of rows k +i and
k + j. Notice that eT
k E = eT
k because eT
k has 0’s in positions k + i and k + j.
This together with the fact that E2 = I guarantees
ETkE = E2 −EckeT
k E = I −˜ckeT
k ,
where
˜ck = Eck.
In other words, the matrix
˜Tk = ETkE = I −˜ckeT
k
(3.10.14)
is also an elementary lower-triangular matrix, and ˜Tk agrees with Tk in all
positions except that the multipliers µk+i and µk+j have traded places. As be-
fore, assume we are row reducing an n × n nonsingular matrix A, but suppose
that an interchange of rows k + i and k + j is necessary immediately after the
kth stage so that the sequence of left-hand multiplications ETkTk−1 · · · T1 is
applied to A. Since E2 = I, we may insert E2 to the right of each T to obtain
ETkTk−1 · · · T1 = ETkE2Tk−1E2 · · · E2T1E2
= (ETkE) (ETk−1E) · · · (ET1E) E
= ˜Tk ˜Tk−1 · · · ˜T1E.
In such a manner, the necessary interchange matrices E can be “factored” to
the far-right-hand side, and the matrices ˜T retain the desirable feature of be-
ing elementary lower-triangular matrices. Furthermore, (3.10.14) implies that
˜Tk ˜Tk−1 · · · ˜T1 diﬀers from TkTk−1 · · · T1 only in the sense that the multipli-
ers in rows k + i and k + j have traded places. Therefore, row interchanges in
Gaussian elimination can be accounted for by writing ˜Tn−1 · · · ˜T2 ˜T1PA = U,
where P is the product of all elementary interchange matrices used during the
reduction and where the ˜Tk ’s are elementary lower-triangular matrices in which
the multipliers have been permuted according to the row interchanges that were
implemented. Since all of the ˜Tk ’s are elementary lower-triangular matrices, we
may proceed along the same lines discussed in (3.10.4)—(3.10.6) to obtain
PA = LU,
where
L = ˜T−1
1
˜T−1
2
· · · ˜T−1
n−1.
(3.10.15)

3.10 The LU Factorization
151
When row interchanges are allowed, zero pivots can always be avoided when the
original matrix A is nonsingular. Consequently, we may conclude that for every
nonsingular matrix A, there exists a permutation matrix P (a product of
elementary interchange matrices) such that PA has an LU factorization. Fur-
thermore, because of the observation in (3.10.14) concerning how the multipliers
in Tk and ˜Tk trade places when a row interchange occurs, and because
˜T−1
k
=

I −˜ckeT
k
	−1 = I + ˜ckeT
k ,
it is not diﬃcult to see that the same line of reasoning used to arrive at (3.10.6)
can be applied to conclude that the multipliers in the matrix L in (3.10.15) are
permuted according to the row interchanges that are executed. More speciﬁcally,
if rows k and k+i are interchanged to create the kth pivot, then the multipliers
( ℓk1
ℓk2
· · ·
ℓk,k−1 )
and
( ℓk+i,1
ℓk+i,2
· · ·
ℓk+i,k−1 )
trade places in the formation of L.
This means that we can proceed just as in the case when no interchanges are
used and successively overwrite the array originally containing A with each mul-
tiplier replacing the position it annihilates. Whenever a row interchange occurs,
the corresponding multipliers will be correctly interchanged as well. The per-
mutation matrix P is simply the cumulative record of the various interchanges
used, and the information in P is easily accounted for by a simple technique
that is illustrated in the following example.
Example 3.10.4
Problem: Use partial pivoting on the matrix
A =



1
2
−3
4
4
8
12
−8
2
3
2
1
−3
−1
1
−4



and determine the LU decomposition PA = LU, where P is the associated
permutation matrix.
Solution: As explained earlier, the strategy is to successively overwrite the array
A with components from L and U. For the sake of clarity, the multipliers ℓij
are shown in boldface type. Adjoin a “permutation counter column” p that
is initially set to the natural order 1,2,3,4. Permuting components of p as the
various row interchanges are executed will accumulate the desired permutation.
The matrix P is obtained by executing the ﬁnal permutation residing in p to
the rows of an appropriate size identity matrix:
[A|p] =



1
2
−3
4
1
4
8
12
−8
2
2
3
2
1
3
−3
−1
1
−4
4


−→



4
8
12
−8
2
1
2
−3
4
1
2
3
2
1
3
−3
−1
1
−4
4




152
Chapter 3
Matrix Algebra
−→



4
8
12
−8
2
1/4
0
−6
6
1
1/2
−1
−4
5
3
−3/4
5
10
−10
4


−→



4
8
12
−8
2
−3/4
5
10
−10
4
1/2
−1
−4
5
3
1/4
0
−6
6
1



−→



4
8
12
−8
2
−3/4
5
10
−10
4
1/2
−1/5
−2
3
3
1/4
0
−6
6
1


−→



4
8
12
−8
2
−3/4
5
10
−10
4
1/4
0
−6
6
1
1/2
−1/5
−2
3
3



−→



4
8
12
−8
2
−3/4
5
10
−10
4
1/4
0
−6
6
1
1/2
−1/5
1/3
1
3


.
Therefore,
L=



1
0
0
0
−3/4
1
0
0
1/4
0
1
0
1/2
−1/5
1/3
1


, U=



4
8
12
−8
0
5
10
−10
0
0
−6
6
0
0
0
1


, P=



0
1
0
0
0
0
0
1
1
0
0
0
0
0
1
0


.
It is easy to combine the advantages of partial pivoting with the LU decom-
position in order to solve a nonsingular system Ax = b. Because permutation
matrices are nonsingular, the system Ax = b is equivalent to
PAx = Pb,
and hence we can employ the LU solution techniques discussed earlier to solve
this permuted system. That is, if we have already performed the factorization
PA = LU —as illustrated in Example 3.10.4—then we can solve Ly = Pb for
y by forward substitution, and then solve Ux = y by back substitution.
It should be evident that the permutation matrix P is not really needed.
All that is necessary is knowledge of the LU factors along with the ﬁnal permu-
tation contained in the permutation counter column p illustrated in Example
3.10.4. The column ˜b = Pb is simply a rearrangement of the components of
b according to the ﬁnal permutation shown in p. In other words, the strategy
is to ﬁrst permute b into ˜b according to the permutation p, and then solve
Ly = ˜b followed by Ux = y.
Example 3.10.5
Problem:
Use the LU decomposition obtained with partial pivoting to solve
the system Ax = b, where
A =



1
2
−3
4
4
8
12
−8
2
3
2
1
−3
−1
1
−4



and
b =



3
60
1
5


.

3.10 The LU Factorization
153
Solution:
The LU decomposition with partial pivoting was computed in Ex-
ample 3.10.4. Permute the components in b according to the permutation
p = ( 2
4
1
3 ) , and call the result ˜b. Now solve Ly = ˜b by applying
forward substitution:



1
0
0
0
−3/4
1
0
0
1/4
0
1
0
1/2
−1/5
1/3
1






y1
y2
y3
y4


=



60
5
3
1



=⇒
y =



y1
y2
y3
y4


=



60
50
−12
−15


.
Then solve Ux = y by applying back substitution:



4
8
12
−8
0
5
10
−10
0
0
−6
6
0
0
0
1






x1
x2
x3
x4


=



60
50
−12
−15



=⇒
x =



12
6
−13
−15


.
LU Factorization with Row Interchanges
•
For each nonsingular matrix A, there exists a permutation matrix
P such that PA possesses an LU factorization PA = LU.
•
To compute L, U, and P, successively overwrite the array origi-
nally containing A. Replace each entry being annihilated with the
multiplier used to execute the annihilation. Whenever row inter-
changes such as those used in partial pivoting are implemented, the
multipliers in the array will automatically be interchanged in the
correct manner.
•
Although the entire permutation matrix P is rarely called for, it
can be constructed by permuting the rows of the identity matrix
I according to the various interchanges used. These interchanges
can be accumulated in a “permutation counter column” p that is
initially in natural order ( 1, 2, . . . , n )—see Example 3.10.4.
•
To solve a nonsingular linear system Ax = b using the LU de-
composition with partial pivoting, permute the components in b to
construct ˜b according to the sequence of interchanges used—i.e.,
according to p —and then solve Ly = ˜b by forward substitution
followed by the solution of Ux = y using back substitution.

154
Chapter 3
Matrix Algebra
Example 3.10.6
The LDU factorization. There’s some asymmetry in an LU factorization be-
cause the lower factor has 1’s on its diagonal while the upper factor has a nonunit
diagonal. This is easily remedied by factoring the diagonal entries out of the up-
per factor as shown below:




u11
u12
· · ·
u1n
0
u22
· · ·
u2n
...
...
...
...
0
0
· · ·
unn



=




u11
0
· · ·
0
0
u22
· · ·
0
...
...
...
...
0
0
· · ·
unn








1 u12/u11
· · · u1n/u11
0
1
· · · u2n/u22
...
...
...
...
0
0
· · ·
1



.
Setting D = diag (u11, u22, . . . , unn) (the diagonal matrix of pivots) and redeﬁn-
ing U to be the rightmost upper-triangular matrix shown above allows any LU
factorization to be written as A = LDU, where L and U are lower- and upper-
triangular matrices with 1’s on both of their diagonals. This is called the LDU
factorization of A. It is uniquely determined, and when A is symmetric, the
LDU factorization is A = LDLT (Exercise 3.10.9).
Example 3.10.7
The Cholesky Factorization.
22 A symmetric matrix A possessing an LU fac-
torization in which each pivot is positive is said to be positive deﬁnite.
Problem: Prove that A is positive deﬁnite if and only if A can be uniquely
factored as A = RT R, where R is an upper-triangular matrix with positive
diagonal entries. This is known as the Cholesky factorization of A, and R is
called the Cholesky factor of A.
Solution: If A is positive deﬁnite, then, as pointed out in Example 3.10.6,
it has an LDU factorization A = LDLT
in which D = diag (p1, p2, . . . , pn)
is the diagonal matrix containing the pivots pi > 0. Setting R = D1/2LT
where D1/2 = diag
√p1, √p2, . . . , √pn
	
yields the desired factorization because
A = LD1/2D1/2LT = RT R, and R is upper triangular with positive diagonal
22
This is named in honor of the French military oﬃcer Major Andr´e-Louis Cholesky (1875–
1918). Although originally assigned to an artillery branch, Cholesky later became attached to
the Geodesic Section of the Geographic Service in France where he became noticed for his
extraordinary intelligence and his facility for mathematics. From 1905 to 1909 Cholesky was
involved with the problem of adjusting the triangularization grid for France. This was a huge
computational task, and there were arguments as to what computational techniques should be
employed. It was during this period that Cholesky invented the ingenious procedure for solving
a positive deﬁnite system of equations that is the basis for the matrix factorization that now
bears his name. Unfortunately, Cholesky’s mathematical talents were never allowed to ﬂower.
In 1914 war broke out, and Cholesky was again placed in an artillery group—but this time
as the commander. On August 31, 1918, Major Cholesky was killed in battle. Cholesky never
had time to publish his clever computational methods—they were carried forward by word-
of-mouth. Issues surrounding the Cholesky factorization have been independently rediscovered
several times by people who were unaware of Cholesky, and, in some circles, the Cholesky
factorization is known as the square root method.

3.10 The LU Factorization
155
entries. Conversely, if A = RRT , where R is lower triangular with a positive
diagonal, then factoring the diagonal entries out of R as illustrated in Example
3.10.6 produces R = LD, where L is lower triangular with a unit diagonal and
D is the diagonal matrix whose diagonal entries are the rii’s. Consequently,
A = LD2LT
is the LDU factorization for A, and thus the pivots must be
positive because they are the diagonal entries in D2. We have now proven that
A is positive deﬁnite if and only if it has a Cholesky factorization. To see why
such a factorization is unique, suppose A = R1RT
1 = R2RT
2 , and factor out
the diagonal entries as illustrated in Example 3.10.6 to write R1 = L1D1 and
R2 = L2D2, where each Ri is lower triangular with a unit diagonal and Di
contains the diagonal of Ri so that A = L1D2
1LT
1 = L2D2
2LT
2 . The uniqueness
of the LDU factors insures that L1 = L2 and D1 = D2, so R1 = R2. Note:
More is said about the Cholesky factorization and positive deﬁnite matrices on
pp. 313, 345, and 559.
Exercises for section 3.10
3.10.1. Let A =


1
4
5
4
18
26
3
16
30

.
(a)
Determine the LU factors of A.
(b)
Use the LU factors to solve Ax1 = b1 as well as Ax2 = b2,
where
b1 =


6
0
−6


and
b2 =


6
6
12

.
(c)
Use the LU factors to determine A−1.
3.10.2. Let A and b be the matrices
A =



1
2
4
17
3
6
−12
3
2
3
−3
2
0
2
−2
6



and
b =



17
3
3
4


.
(a)
Explain why A does not have an LU factorization.
(b)
Use partial pivoting and ﬁnd the permutation matrix P as well
as the LU factors such that PA = LU.
(c)
Use the information in P, L, and U to solve Ax = b.
3.10.3. Determine all values of ξ for which A =


ξ
2
0
1
ξ
1
0
1
ξ

fails to have an
LU factorization.

156
Chapter 3
Matrix Algebra
3.10.4. If A is a nonsingular matrix that possesses an LU factorization, prove
that the pivot that emerges after (k + 1) stages of standard Gaussian
elimination using only Type III operations is given by
pk+1 = ak+1,k+1 −cT A−1
k b,
where Ak and
Ak+1 =

Ak
b
cT
ak+1,k+1

are the leading principal submatrices of orders k and k + 1, respec-
tively. Use this to deduce that all pivots must be nonzero when an LU
factorization for A exists.
3.10.5. If A is a matrix that contains only integer entries and all of its pivots
are 1, explain why A−1 must also be an integer matrix. Note: This fact
can be used to construct random integer matrices that possess integer
inverses by randomly generating integer matrices L and U with unit
diagonals and then constructing the product A = LU.
3.10.6. Consider the tridiagonal matrix T =



β1
γ1
0
0
α1
β2
γ2
0
0
α2
β3
γ3
0
0
α3
β4


.
(a)
Assuming that T possesses an LU factorization, verify that it
is given by
L =



1
0
0
0
α1/π1
1
0
0
0
α2/π2
1
0
0
0
α3/π3
1


, U =



π1
γ1
0
0
0
π2
γ2
0
0
0
π3
γ3
0
0
0
π4


,
where the πi ’s are generated by the recursion formula
π1 = β1
and
πi+1 = βi+1 −αiγi
πi
.
Note:
This holds for tridiagonal matrices of arbitrary size
thereby making the LU factors of these matrices very easy to
compute.
(b)
Apply the recursion formula given above to obtain the LU fac-
torization of
T =



2
−1
0
0
−1
2
−1
0
0
−1
2
−1
0
0
−1
1


.

3.10 The LU Factorization
157
3.10.7. An×n is called a band matrix if aij = 0 whenever |i −j| > w for
some positive integer w, called the bandwidth. In other words, the
nonzero entries of A are constrained to be in a band of w diagonal lines
above and below the main diagonal. For example, tridiagonal matrices
have bandwidth one, and diagonal matrices have bandwidth zero. If
A is a nonsingular matrix with bandwidth w, and if A has an LU
factorization A = LU, then L inherits the lower band structure of A,
and U inherits the upper band structure in the sense that L has “lower
bandwidth” w, and U has “upper bandwidth” w. Illustrate why this
is true by using a generic 5 × 5 matrix with a bandwidth of w = 2.
3.10.8.
(a)
Construct an example of a nonsingular symmetric matrix that
fails to possess an LU (or LDU) factorization.
(b)
Construct an example of a nonsingular symmetric matrix that
has an LU factorization but is not positive deﬁnite.
3.10.9.
(a)
Determine the LDU factors for A =


1
4
5
4
18
26
3
16
30

(this is the
same matrix used in Exercise 3.10.1).
(b)
Prove that if a matrix has an LDU factorization, then the LDU
factors are uniquely determined.
(c)
If A is symmetric and possesses an LDU factorization, explain
why it must be given by A = LDLT .
3.10.10. Explain why A =


1
2
3
2
8
12
3
12
27

is positive deﬁnite, and then ﬁnd the
Cholesky factor R.

CHAPTER 4
Vector
Spaces
4.1
SPACES AND SUBSPACES
After matrix theory became established toward the end of the nineteenth century,
it was realized that many mathematical entities that were considered to be quite
diﬀerent from matrices were in fact quite similar. For example, objects such as
points in the plane ℜ2, points in 3-space ℜ3, polynomials, continuous functions,
and diﬀerentiable functions (to name only a few) were recognized to satisfy the
same additive properties and scalar multiplication properties given in §3.2 for
matrices. Rather than studying each topic separately, it was reasoned that it
is more eﬃcient and productive to study many topics at one time by studying
the common properties that they satisfy. This eventually led to the axiomatic
deﬁnition of a vector space.
A vector space involves four things—two sets V and F, and two algebraic
operations called vector addition and scalar multiplication.
•
V is a nonempty set of objects called vectors. Although V can be quite
general, we will usually consider V to be a set of n-tuples or a set of matrices.
•
F is a scalar ﬁeld—for us F is either the ﬁeld ℜof real numbers or the
ﬁeld C of complex numbers.
•
Vector addition (denoted by x + y ) is an operation between elements of V.
•
Scalar multiplication (denoted by αx ) is an operation between elements of
F and V.
The formal deﬁnition of a vector space stipulates how these four things relate
to each other. In essence, the requirements are that vector addition and scalar
multiplication must obey exactly the same properties given in §3.2 for matrices.

160
Chapter 4
Vector Spaces
Vector Space Deﬁnition
The set V is called a vector space over F when the vector addition
and scalar multiplication operations satisfy the following properties.
(A1)
x+y ∈V for all x, y ∈V. This is called the closure property
for vector addition.
(A2)
(x + y) + z = x + (y + z) for every x, y, z ∈V.
(A3)
x + y = y + x for every x, y ∈V.
(A4)
There is an element 0 ∈V such that x + 0 = x for every
x ∈V.
(A5)
For each x ∈V, there is an element (−x) ∈V such that
x + (−x) = 0.
(M1)
αx ∈V for all α ∈F and x ∈V. This is the closure
property for scalar multiplication.
(M2)
(αβ)x = α(βx) for all α, β ∈F and every x ∈V.
(M3)
α(x + y) = αx + αy for every α ∈F and all x, y ∈V.
(M4)
(α + β)x = αx + βx for all α, β ∈F and every x ∈V.
(M5)
1x = x for every x ∈V.
A theoretical algebraic treatment of the subject would concentrate on the
logical consequences of these deﬁning properties, but the objectives in this text
are diﬀerent, so we will not dwell on the axiomatic development.
23 Neverthe-
23
The idea of deﬁning a vector space by using a set of abstract axioms was contained in a general
theory published in 1844 by Hermann Grassmann (1808–1887), a theologian and philosopher
from Stettin, Poland, who was a self-taught mathematician. But Grassmann’s work was origi-
nally ignored because he tried to construct a highly abstract self-contained theory, independent
of the rest of mathematics, containing nonstandard terminology and notation, and he had a
tendency to mix mathematics with obscure philosophy. Grassmann published a complete re-
vision of his work in 1862 but with no more success. Only later was it realized that he had
formulated the concepts we now refer to as linear dependence, bases, and dimension. The
Italian mathematician Giuseppe Peano (1858–1932) was one of the few people who noticed
Grassmann’s work, and in 1888 Peano published a condensed interpretation of it. In a small
chapter at the end, Peano gave an axiomatic deﬁnition of a vector space similar to the one
above, but this drew little attention outside of a small group in Italy. The current deﬁnition is
derived from the 1918 work of the German mathematician Hermann Weyl (1885–1955). Even
though Weyl’s deﬁnition is closer to Peano’s than to Grassmann’s, Weyl did not mention his
Italian predecessor, but he did acknowledge Grassmann’s “epoch making work.” Weyl’s success
with the idea was due in part to the fact that he thought of vector spaces in terms of geometry,
whereas Grassmann and Peano treated them as abstract algebraic structures. As we will see,
it’s the geometry that’s important.

4.1 Spaces and Subspaces
161
less, it is important to recognize some of the more signiﬁcant examples and to
understand why they are indeed vector spaces.
Example 4.1.1
Because (A1)–(A5) are generalized versions of the ﬁve additive properties of
matrix addition, and (M1)–(M5) are generalizations of the ﬁve scalar multipli-
cation properties given in §3.2, we can say that the following hold.
•
The set ℜm×n of m × n real matrices is a vector space over ℜ.
•
The set Cm×n of m × n complex matrices is a vector space over C.
Example 4.1.2
The real coordinate spaces
ℜ1×n = {( x1
x2
· · ·
xn ) , xi ∈ℜ}
and
ℜn×1 =











x1
x2
...
xn



, xi ∈ℜ







are special cases of the preceding example, and these will be the object of most
of our attention. In the context of vector spaces, it usually makes no diﬀerence
whether a coordinate vector is depicted as a row or as a column. When the row or
column distinction is irrelevant, or when it is clear from the context, we will use
the common symbol ℜn to designate a coordinate space. In those cases where
it is important to distinguish between rows and columns, we will explicitly write
ℜ1×n or ℜn×1. Similar remarks hold for complex coordinate spaces.
Although the coordinate spaces will be our primary concern, be aware that
there are many other types of mathematical structures that are vector spaces—
this was the reason for making an abstract deﬁnition at the outset. Listed below
are a few examples.
Example 4.1.3
With function addition and scalar multiplication deﬁned by
(f + g)(x) = f(x) + g(x)
and
(αf)(x) = αf(x),
the following sets are vector spaces over ℜ:
•
The set of functions mapping the interval [0, 1] into ℜ.
•
The set of all real-valued continuous functions deﬁned on [0, 1].
•
The set of real-valued functions that are diﬀerentiable on [0, 1].
•
The set of all polynomials with real coeﬃcients.

162
Chapter 4
Vector Spaces
Example 4.1.4
Consider the vector space ℜ2, and let
L = {(x, y) | y = αx}
be a line through the origin. L is a subset of ℜ2, but L is a special kind
of subset because L also satisﬁes the properties (A1)–(A5) and (M1)–(M5)
that deﬁne a vector space. This shows that it is possible for one vector space to
properly contain other vector spaces.
Subspaces
Let S be a nonempty subset of a vector space V over F (symbolically,
S ⊆V). If S is also a vector space over F using the same addition
and scalar multiplication operations, then S is said to be a subspace of
V. It’s not necessary to check all 10 of the deﬁning conditions in order
to determine if a subset is also a subspace—only the closure conditions
(A1) and (M1) need to be considered. That is, a nonempty subset S
of a vector space V is a subspace of V if and only if
(A1)
x, y ∈S
=⇒
x + y ∈S
and
(M1)
x ∈S
=⇒
αx ∈S for all α ∈F.
Proof.
If S is a subset of V, then S automatically inherits all of the vector
space properties of V except (A1), (A4), (A5), and (M1). However, (A1)
together with (M1) implies (A4) and (A5). To prove this, observe that (M1)
implies (−x) = (−1)x ∈S for all x ∈S so that (A5) holds. Since x and (−x)
are now both in S, (A1) insures that x + (−x) ∈S, and thus 0 ∈S.
Example 4.1.5
Given a vector space V, the set Z = {0} containing only the zero vector is
a subspace of V because (A1) and (M1) are trivially satisﬁed. Naturally, this
subspace is called the trivial subspace.
Vector addition in ℜ2 and ℜ3 is easily visualized by using the parallelo-
gram law, which states that for two vectors u and v, the sum u + v is the
vector deﬁned by the diagonal of the parallelogram as shown in Figure 4.1.1.

4.1 Spaces and Subspaces
163
u = (u1,u2)
v = (v1,v2)
= (u
u+v
1+v1, u2+v2)
Figure 4.1.1
We have already observed that straight lines through the origin in ℜ2 are
subspaces, but what about straight lines not through the origin? No—they can-
not be subspaces because subspaces must contain the zero vector (i.e., they must
pass through the origin). What about curved lines through the origin—can some
of them be subspaces of ℜ2? Again the answer is “No!” As depicted in Figure
4.1.2, the parallelogram law indicates why the closure property (A1) cannot be
satisﬁed for lines with a curvature because there are points u and v on the
curve for which u + v (the diagonal of the corresponding parallelogram) is not
on the curve. Consequently, the only proper subspaces of ℜ2 are the trivial
subspace and lines through the origin.
u
v
u+v
Figure 4.1.2
u
v
u+v
αu
P
Figure 4.1.3
In ℜ3, the trivial subspace and lines through the origin are again subspaces,
but there is also another one—planes through the origin. If P is a plane through
the origin in ℜ3, then, as shown in Figure 4.1.3, the parallelogram law guarantees
that the closure property for addition (A1) holds—the parallelogram deﬁned by

164
Chapter 4
Vector Spaces
any two vectors in P is also in P so that if u, v ∈P, then u + v ∈P. The
closure property for scalar multiplication (M1) holds because multiplying any
vector by a scalar merely stretches it, but its angular orientation does not change
so that if u ∈P, then αu ∈P for all scalars α. Lines and surfaces in ℜ3 that
have curvature cannot be subspaces for essentially the same reason depicted in
Figure 4.1.2. So the only proper subspaces of ℜ3 are the trivial subspace, lines
through the origin, and planes through the origin.
The concept of a subspace now has an obvious interpretation in the visual
spaces ℜ2 and ℜ3 —subspaces are the ﬂat surfaces passing through the origin.
Flatness
Although we can’t use our eyes to see “ﬂatness” in higher dimensions,
our minds can conceive it through the notion of a subspace. From now on,
think of ﬂat surfaces passing through the origin whenever you encounter
the term “subspace.”
For a set of vectors S = {v1, v2, . . . , vr} from a vector space V, the set of
all possible linear combinations of the vi ’s is denoted by
span (S) = {α1v1 + α2v2 + · · · + αrvr | αi ∈F} .
Notice that span (S) is a subspace of V because the two closure properties
(A1) and (M1) are satisﬁed. That is, if x = 
i ξivi and y = 
i ηivi are two
linear combinations from span (S) , then the sum x + y = 
i(ξi + ηi)vi is also
a linear combination in span (S) , and for any scalar β,
βx = 
i(βξi)vi is
also a linear combination in span (S) .
u
v
αu
βv
αu + βv
Figure 4.1.4

4.1 Spaces and Subspaces
165
For example, if u ̸= 0 is a vector in ℜ3, then span {u} is the straight
line passing through the origin and u. If S = {u, v}, where u and v are two
nonzero vectors in ℜ3 not lying on the same line, then, as shown in Figure 4.1.4,
span (S) is the plane passing through the origin and the points u and v. As we
will soon see, all subspaces of ℜn are of the type span (S), so it is worthwhile
to introduce the following terminology.
Spanning Sets
•
For a set of vectors S = {v1, v2, . . . , vr} , the subspace
span (S) = {α1v1 + α2v2 + · · · + αrvr}
generated by forming all linear combinations of vectors from S is
called the space spanned by S.
•
If V is a vector space such that V = span (S) , we say S is a
spanning set for V. In other words, S spans V whenever each
vector in V is a linear combination of vectors from S.
Example 4.1.6
(i)
In Figure 4.1.4, S = {u, v} is a spanning set for the indicated plane.
(ii)
S =

1
1

,

2
2

spans the line y = x in ℜ2.
(iii)
The unit vectors


e1 =


1
0
0

, e2 =


0
1
0

, e3 =


0
0
1




span ℜ3.
(iv)
The unit vectors {e1, e2, . . . , en} in ℜn form a spanning set for ℜn.
(v)
The ﬁnite set

1, x, x2, . . . , xn
spans the space of all polynomials such
that deg p(x) ≤n, and the inﬁnite set

1, x, x2, . . .

spans the space of all
polynomials.
Example 4.1.7
Problem: For a set of vectors S = {a1, a2, . . . , an} from a subspace V ⊆ℜm×1,
let A be the matrix containing the ai ’s as its columns. Explain why S spans V
if and only if for each b ∈V there corresponds a column x such that Ax = b
(i.e., if and only if Ax = b is a consistent system for every b ∈V).

166
Chapter 4
Vector Spaces
Solution: By deﬁnition, S spans V if and only if for each b ∈V there exist
scalars αi such that
b = α1a1 + α2a2 + · · · + αnan =

a1 | a2 | · · · | an





α1
α2
...
αn



= Ax.
Note: This simple observation often is quite helpful. For example, to test whether
or not S = {( 1
1
1 ) , ( 1
−1
−1 ) , ( 3
1
1 )} spans ℜ3, place these
rows as columns in a matrix A, and ask, “Is the system


1
1
3
1
−1
1
1
−1
1




x1
x2
x3

=


b1
b2
b3


consistent for every b ∈ℜ3?” Recall from (2.3.4) that Ax = b is consis-
tent if and only if rank[A|b] = rank (A). In this case, rank (A) = 2, but
rank[A|b] = 3 for some b ’s (e.g., b1 = 0, b2 = 1, b3 = 0), so S doesn’t span
ℜ3. On the other hand, S′ = {( 1
1
1 ) , ( 1
−1
−1 ) , ( 3
1
2 )} is a
spanning set for ℜ3 because
A =


1
1
3
1
−1
1
1
−1
2


is nonsingular, so Ax = b is consistent for all b (the solution is x = A−1b ).
As shown below, it’s possible to “add” two subspaces to generate another.
Sum of Subspaces
If X and Y are subspaces of a vector space V, then the sum of X
and Y is deﬁned to be the set of all possible sums of vectors from X
with vectors from Y. That is,
X + Y = {x + y | x ∈X and y ∈Y}.
•
The sum X + Y is again a subspace of V.
(4.1.1)
•
If SX, SY span X, Y, then SX ∪SY spans X + Y.
(4.1.2)

4.1 Spaces and Subspaces
167
Proof.
To prove (4.1.1), demonstrate that the two closure properties (A1) and
(M1) hold for S = X +Y. To show (A1) is valid, observe that if u, v ∈S, then
u = x1 + y1 and v = x2 + y2, where x1, x2 ∈X and y1, y2 ∈Y. Because
X
and Y are closed with respect to addition, it follows that x1 + x2 ∈X
and y1 + y2 ∈Y, and therefore u + v = (x1 + x2) + (y1 + y2) ∈S. To
verify (M1), observe that X
and Y are both closed with respect to scalar
multiplication so that αx1 ∈X
and αy1 ∈Y for all α, and consequently
αu = αx1 + αy1 ∈S for all α. To prove (4.1.2), suppose SX = {x1, x2, . . . , xr}
and SY = {y1, y2, . . . , yt} , and write
z ∈span (SX ∪SY ) ⇐⇒z =
r

i=1
αixi +
t

i=1
βiyi = x + y with x ∈X, y ∈Y
⇐⇒z ∈X + Y.
Example 4.1.8
If X ⊆ℜ2 and Y ⊆ℜ2 are subspaces deﬁned by two diﬀerent lines through
the origin, then X + Y = ℜ2. This follows from the parallelogram law—sketch
a picture for yourself.
Exercises for section 4.1
4.1.1. Determine which of the following subsets of ℜn are in fact subspaces of
ℜn
(n > 2).
(a)
{x | xi ≥0},
(b)
{x | x1 = 0},
(c)
{x | x1x2 = 0},
(d)

x

n
j=1
xj = 0

,
(e)

x

n
j=1
xj = 1

,
(f)
{x | Ax = b, where Am×n ̸= 0 and bm×1 ̸= 0} .
4.1.2. Determine which of the following subsets of ℜn×n are in fact subspaces
of ℜn×n.
(a)
The symmetric matrices.
(b) The diagonal matrices.
(c)
The nonsingular matrices. (d) The singular matrices.
(e)
The triangular matrices.
(f) The upper-triangular matrices.
(g)
All matrices that commute with a given matrix A.
(h)
All matrices such that A2 = A.
(i)
All matrices such that trace (A) = 0.
4.1.3. If X is a plane passing through the origin in ℜ3 and Y is the line
through the origin that is perpendicular to X, what is X + Y ?

168
Chapter 4
Vector Spaces
4.1.4. Why must a real or complex nonzero vector space contain an inﬁnite
number of vectors?
4.1.5. Sketch a picture in ℜ3 of the subspace spanned by each of the following.
(a)





1
3
2

,


2
6
4

,


−3
−9
−6




, (b)





−4
0
0

,


0
5
0

,


1
1
0




,
(c)





1
0
0

,


1
1
0

,


1
1
1




.
4.1.6. Which of the following are spanning sets for ℜ3 ?
(a)
{( 1
1
1 )}
(b)
{( 1
0
0 ) , ( 0
0
1 )},
(c)
{( 1
0
0 ) , ( 0
1
0 ) , ( 0
0
1 ) , ( 1
1
1 )},
(d)
{( 1
2
1 ) , ( 2
0
−1 ) , ( 4
4
1 )},
(e)
{( 1
2
1 ) , ( 2
0
−1 ) , ( 4
4
0 )}.
4.1.7. For a vector space V, and for M, N ⊆V, explain why
span (M ∪N) = span (M) + span (N) .
4.1.8. Let X and Y be two subspaces of a vector space V.
(a)
Prove that the intersection X ∩Y is also a subspace of V.
(b)
Show that the union X ∪Y need not be a subspace of V.
4.1.9. For A ∈ℜm×n and S ⊆ℜn×1, the set A(S) = {Ax | x ∈S} contains
all possible products of A with vectors from S. We refer to A(S) as
the set of images of S under A.
(a)
If S is a subspace of ℜn, prove A(S) is a subspace of ℜm.
(b)
If s1, s2, . . . , sk spans S, show As1, As2, . . . , Ask spans A(S).
4.1.10. With the usual addition and multiplication, determine whether or not
the following sets are vector spaces over the real numbers.
(a)
ℜ,
(b)
C,
(c)
The rational numbers.
4.1.11. Let M = {m1, m2, . . . , mr} and N = {m1, m2, . . . , mr, v} be two sets
of vectors from the same vector space. Prove that span (M) = span (N)
if and only if v ∈span (M) .
4.1.12. For a set of vectors S = {v1, v2, . . . , vn} , prove that span (S) is the
intersection of all subspaces that contain S. Hint: For M =

S⊆V V,
prove that span (S) ⊆M and M ⊆span (S) .

4.2 Four Fundamental Subspaces
169
4.2
FOUR FUNDAMENTAL SUBSPACES
The closure properties (A1) and (M1) on p. 162 that characterize the notion
of a subspace have much the same “feel” as the deﬁnition of a linear function as
stated on p. 89, but there’s more to it than just a “similar feel.” Subspaces are
intimately related to linear functions as explained below.
Subspaces and Linear Functions
For a linear function f mapping ℜn into ℜm, let R(f) denote the
range of f. That is, R(f) = {f(x) | x ∈ℜn} ⊆ℜm is the set of all
“images” as x varies freely over ℜn.
•
The range of every linear function f : ℜn →ℜm is a subspace of
ℜm, and every subspace of ℜm is the range of some linear function.
For this reason, subspaces of ℜm are sometimes called linear spaces.
Proof.
If f : ℜn →ℜm is a linear function, then the range of f is a subspace
of ℜm because the closure properties (A1) and (M1) are satisﬁed. Establish
(A1) by showing that y1, y2 ∈R(f) ⇒y1 +y2 ∈R(f). If y1, y2 ∈R(f), then
there must be vectors x1, x2 ∈ℜn such that y1 = f(x1) and y2 = f(x2), so
it follows from the linearity of f that
y1 + y2 = f(x1) + f(x2) = f(x1 + x2) ∈R(f).
Similarly, establish (M1) by showing that if y ∈R(f), then αy ∈R(f) for all
scalars α by using the deﬁnition of range along with the linearity of f to write
y ∈R(f) =⇒y = f(x) for some x ∈ℜn =⇒αy = αf(x) = f(αx) ∈R(f).
Now prove that every subspace V of ℜm is the range of some linear function
f : ℜn →ℜm. Suppose that {v1, v2, . . . , vn} is a spanning set for V so that
V = {α1v1 + · · · + αnvn | αi ∈R}.
(4.2.1)
Stack the vi ’s as columns in a matrix Am×n =

v1 | v2 | · · · | vn

, and put the
αi ’s in an n × 1 column x = (α1, α2, . . . , αn)T to write
α1v1 + · · · + αnvn =

v1 | v2 | · · · | vn



α1
...
αn

= Ax.
(4.2.2)
The function f(x) = Ax is linear (recall Example 3.6.1, p. 106), and we have
that R(f) = {Ax | x ∈ℜn×1} = {α1v1 + · · · + αnvn | αi ∈R} = V.

170
Chapter 4
Vector Spaces
In particular, this result means that every matrix A ∈ℜm×n generates
a subspace of ℜm by means of the range of the linear function f(x) = Ax.
Likewise, the transpose
24 of A ∈ℜm×n deﬁnes a subspace of ℜn by means
of the range of f(y) = AT y. These two “range spaces” are two of the four
fundamental subspaces deﬁned by a matrix.
Range Spaces
The range of a matrix A ∈ℜm×n is deﬁned to be the subspace
R (A) of ℜm that is generated by the range of f(x) = Ax. That is,
R (A) = {Ax | x ∈ℜn} ⊆ℜm.
Similarly, the range of AT is the subspace of ℜn deﬁned by
R

AT 
= {AT y | y ∈ℜm} ⊆ℜn.
Because R (A) is the set of all “images” of vectors x ∈ℜm under
transformation by A, some people call R (A) the image space of A.
The observation (4.2.2) that every matrix–vector product Ax (i.e., every
image) is a linear combination of the columns of A provides a useful character-
ization of the range spaces. Allowing the components of x = (ξ1, ξ2, . . . , ξn)T to
vary freely and writing
Ax =

A∗1 | A∗2 | · · · | A∗n





ξ1
ξ2
...
ξn



=
n

j=1
ξjA∗j
shows that the set of all images Ax is the same as the set of all linear combi-
nations of the columns of A. Therefore, R (A) is nothing more than the space
spanned by the columns of A. That’s why R (A) is often called the column
space of A.
Likewise, R

AT 
is the space spanned by the columns of AT . But the
columns of AT are just the rows of A (stacked upright), so R

AT 
is simply
the space spanned by the rows
25 of A. Consequently, R

AT 
is also known as
the row space of A. Below is a summary.
24
For ease of exposition, the discussion in this section is in terms of real matrices and real spaces,
but all results have complex analogs obtained by replacing AT by A∗.
25
Strictly speaking, the range of AT
is a set of columns, while the row space of A is a set of
rows. However, no logical diﬃculties are encountered by considering them to be the same.

4.2 Four Fundamental Subspaces
171
Column and Row Spaces
For A ∈ℜm×n, the following statements are true.
•
R (A) = the space spanned by the columns of A (column space).
•
R

AT 
= the space spanned by the rows of A (row space).
•
b ∈R (A) ⇐⇒b = Ax for some x.
(4.2.3)
•
a ∈R

AT 
⇐⇒aT = yT A for some yT .
(4.2.4)
Example 4.2.1
Problem: Describe R (A) and R

AT 
for A =
 1
2
3
2
4
6

.
Solution: R (A) = span {A∗1, A∗2, A∗3} = {α1A∗1+α2A∗2+α3A∗3 | αi ∈ℜ},
but since A∗2 = 2A∗1 and A∗3 = 3A∗1, it’s clear that every linear combination
of A∗1, A∗2, and A∗3 reduces to a multiple of A∗1, so R (A) = span {A∗1} .
Geometrically, R (A) is the line in ℜ2 through the origin and the point (1, 2).
Similarly, R

AT 
= span {A1∗, A2∗} = {α1A1∗+ α2A2∗| α1, α2 ∈ℜ} . But
A2∗= 2A1∗implies that every combination of A1∗and A2∗reduces to a
multiple of A1∗, so R

AT 
= span {A1∗} , and this is a line in ℜ3 through
the origin and the point (1, 2, 3).
There are times when it is desirable to know whether or not two matrices
have the same row space or the same range. The following theorem provides the
solution to this problem.
Equal Ranges
For two matrices A and B of the same shape:
•
R

AT 
= R

BT 
if and only if A
row
∼B.
(4.2.5)
•
R (A) = R (B) if and only if A
col
∼B.
(4.2.6)
Proof.
To prove (4.2.5), ﬁrst assume A
row
∼B so that there exists a nonsingular
matrix P such that PA = B. To see that R

AT 
= R

BT 
, use (4.2.4) to
write
a ∈R

AT 
⇐⇒aT = yT A = yT P−1PA
for some yT
⇐⇒aT = zT B
for zT = yT P−1
⇐⇒a ∈R

BT 
.

172
Chapter 4
Vector Spaces
Conversely, if R

AT 
= R

BT 
, then
span {A1∗, A2∗, . . . , Am∗} = span {B1∗, B2∗, . . . , Bm∗} ,
so each row of B is a combination of the rows of A, and vice versa. On the
basis of this fact, it can be argued that it is possible to reduce A to B by using
only row operations (the tedious details are omitted), and thus A
row
∼B. The
proof of (4.2.6) follows by replacing A and B with AT and BT .
Example 4.2.2
Testing Spanning Sets. Two sets {a1, a2, . . . , ar} and {b1, b2, . . . , bs} in
ℜn span the same subspace if and only if the nonzero rows of EA agree with
the nonzero rows of EB, where A and B are the matrices containing the ai ’s
and bi ’s as rows. This is a corollary of (4.2.5) because zero rows are irrelevant
in considering the row space of a matrix, and we already know from (3.9.9) that
A
row
∼B if and only if EA = EB.
Problem: Determine whether or not the following sets span the same subspace:
A =








1
2
2
3


,



2
4
1
3


,



3
6
1
4








,
B =








0
0
1
1


,



1
2
3
4








.
Solution: Place the vectors as rows in matrices A and B, and compute
A =


1
2
2
3
2
4
1
3
3
6
1
4

→


1
2
0
1
0
0
1
1
0
0
0
0

= EA
and
B =

0
0
1
1
1
2
3
4

→

1
2
0
1
0
0
1
1

= EB.
Hence span {A} = span {B} because the nonzero rows in EA and EB agree.
We already know that the rows of A span R

AT 
, and the columns of A
span R (A), but it’s often possible to span these spaces with fewer vectors than
the full set of rows and columns.
Spanning the Row Space and Range
Let A be an m × n matrix, and let U be any row echelon form derived
from A. Spanning sets for the row and column spaces are as follows:
•
The nonzero rows of U span R

AT 
.
(4.2.7)
•
The basic columns in A span R (A).
(4.2.8)

4.2 Four Fundamental Subspaces
173
Proof.
Statement (4.2.7) is an immediate consequence of (4.2.5). To prove
(4.2.8), suppose that the basic columns in A are in positions b1, b2, . . . , br,
and the nonbasic columns occupy positions n1, n2, . . . , nt, and let Q1 be the
permutation matrix that permutes all of the basic columns in A to the left-hand
side so that AQ1 = ( Bm×r
Nm×t ) , where B contains the basic columns and
N contains the nonbasic columns. Since the nonbasic columns are linear com-
binations of the basic columns—recall (2.2.3)—we can annihilate the nonbasic
columns in N using elementary column operations. In other words, there is a
nonsingular matrix Q2 such that ( B
N ) Q2 = ( B
0 ) . Thus Q = Q1Q2 is
a nonsingular matrix such that AQ = AQ1Q2 = ( B
N ) Q2 = ( B
0 ) , and
hence A
col
∼( B
0 ). The conclusion (4.2.8) now follows from (4.2.6).
Example 4.2.3
Problem: Determine spanning sets for R (A) and R

AT 
, where
A =


1
2
2
3
2
4
1
3
3
6
1
4

.
Solution: Reducing A to any row echelon form U provides the solution—the
basic columns in A correspond to the pivotal positions in U, and the nonzero
rows of U span the row space of A. Using EA =
 1
2
0
1
0
0
1
1
0
0
0
0
	
produces
R (A) = span





1
2
3

,


2
1
1





and
R

AT 
= span








1
2
0
1


,



0
0
1
1








.
So far, only two of the four fundamental subspaces associated with each
matrix A ∈ℜm×n have been discussed, namely, R (A) and R

AT 
. To see
where the other two fundamental subspaces come from, consider again a general
linear function f mapping ℜn into ℜm, and focus on N(f) = {x | f(x) = 0}
(the set of vectors that are mapped to 0 ). N(f) is called the nullspace of f
(some texts call it the kernel of f), and it’s easy to see that N(f) is a subspace
of ℜn because the closure properties (A1) and (M1) are satisﬁed. Indeed, if
x1, x2 ∈N(f), then f(x1) = 0 and f(x2) = 0, so the linearity of f produces
f(x1 + x2) = f(x1) + f(x2) = 0 + 0 = 0
=⇒
x1 + x2 ∈N(f).
(A1)
Similarly, if α ∈ℜ, and if x ∈N(f), then f(x) = 0 and linearity implies
f(αx) = αf(x) = α0 = 0
=⇒
αx ∈N(f).
(M1)
By considering the linear functions f(x) = Ax and g(y) = AT y, the
other two fundamental subspaces deﬁned by A ∈ℜm×n are obtained. They are
N(f) = {xn×1 | Ax = 0} ⊆ℜn and N(g) =

ym×1 | ATy = 0

⊆ℜm.

174
Chapter 4
Vector Spaces
Nullspace
•
For an m × n matrix A, the set N (A) = {xn×1 | Ax = 0} ⊆ℜn
is called the nullspace of A. In other words, N (A) is simply the
set of all solutions to the homogeneous system Ax = 0.
•
The set N

AT 
=

ym×1 | AT y = 0

⊆ℜm is called the left-
hand nullspace of A because N

AT 
is the set of all solutions
to the left-hand homogeneous system yT A = 0T .
Example 4.2.4
Problem: Determine a spanning set for N (A), where A =
 1
2
3
2
4
6

.
Solution:
N (A) is merely the general solution of Ax = 0, and this is deter-
mined by reducing A to a row echelon form U. As discussed in §2.4, any such
U will suﬃce, so we will use EA =
 1
2
3
0
0
0

. Consequently, x1 = −2x2 −3x3,
where x2 and x3 are free, so the general solution of Ax = 0 is


x1
x2
x3

=


−2x2 −3x3
x2
x3

= x2


−2
1
0

+ x3


−3
0
1

.
In other words, N (A) is the set of all possible linear combinations of the vectors
h1 =


−2
1
0


and
h2 =


−3
0
1

,
and therefore span {h1, h2} = N (A). For this example, N (A) is the plane in
ℜ3 that passes through the origin and the two points h1 and h2.
Example 4.2.4 indicates the general technique for determining a spanning
set for N (A). Below is a formal statement of this procedure.

4.2 Four Fundamental Subspaces
175
Spanning the Nullspace
To determine a spanning set for N (A), where rank (Am×n) = r, row
reduce A to a row echelon form U, and solve Ux = 0 for the basic
variables in terms of the free variables to produce the general solution
of Ax = 0 in the form
x = xf1h1 + xf2h2 + · · · + xfn−rhn−r.
(4.2.9)
By deﬁnition, the set H = {h1, h2, . . . , hn−r} spans N (A). Moreover,
it can be proven that H is unique in the sense that H is independent
of the row echelon form U.
It was established in §2.4 that a homogeneous system Ax = 0 possesses a
unique solution (i.e., only the trivial solution x = 0 ) if and only if the rank of
the coeﬃcient matrix equals the number of unknowns. This may now be restated
using vector space terminology.
Zero Nullspace
If A is an m × n matrix, then
•
N (A) = {0} if and only if rank (A) = n;
(4.2.10)
•
N

AT 
= {0} if and only if rank (A) = m.
(4.2.11)
Proof.
We already know that the trivial solution x = 0 is the only solution to
Ax = 0 if and only if the rank of A is the number of unknowns, and this is
what (4.2.10) says. Similarly, AT y = 0 has only the trivial solution y = 0 if
and only if rank

AT 
= m. Recall from (3.9.11) that rank

AT 
= rank (A)
in order to conclude that (4.2.11) holds.
Finally, let’s think about how to determine a spanning set for N

AT 
. Of
course, we can proceed in the same manner as described in Example 4.2.4 by
reducing AT to a row echelon form to extract the general solution for AT x = 0.
However, the other three fundamental subspaces are derivable directly from EA
(or any other row echelon form U
row
∼A ), so it’s rather awkward to have to
start from scratch and compute a new echelon form just to get a spanning set
for N

AT 
. It would be better if a single reduction to echelon form could
produce all four of the fundamental subspaces. Note that EAT ̸= ET
A, so ET
A
won’t easily lead to N

AT 
. The following theorem helps resolve this issue.

176
Chapter 4
Vector Spaces
Left-Hand Nullspace
If rank (Am×n) = r, and if PA = U, where P is nonsingular and
U is in row echelon form, then the last m −r rows in P span the
left-hand nullspace of A. In other words, if P =
 P1
P2

, where P2 is
(m −r) × m, then
N

AT 
= R

PT
2

.
(4.2.12)
Proof.
If U =

C
0

, where Cr×n, then PA = U implies P2A = 0, and
this says R

PT
2

⊆N

AT 
. To show equality, demonstrate containment in
the opposite direction by arguing that every vector in N

AT 
must also be in
R

PT
2

. Suppose yT ∈N

AT 
, and let P−1 = ( Q1
Q2 ) to conclude that
0 = yT A = yT P−1U = yT Q1C
=⇒
0 = yT Q1
because N

CT 
= {0} by (4.2.11). Now observe that PP−1 = I = P−1P
insures P1Q1 = Ir and Q1P1 = Im −Q2P2, so
0 = yT Q1
=⇒
0 = yT Q1P1 = yT (I −Q2P2)
=⇒
yT = yT Q2P2 =

yT Q2

P2
=⇒
y ∈R

PT
2

=⇒
yT ∈R

PT
2

.
Example 4.2.5
Problem: Determine a spanning set for N

AT 
, where A =
 1
2
2
3
2
4
1
3
3
6
1
4

.
Solution: To ﬁnd a nonsingular matrix P such that PA = U is in row echelon
form, proceed as described in Exercise 3.9.1 and row reduce the augmented
matrix

A | I

to

U | P

. It must be the case that PA = U because P
is the product of the elementary matrices corresponding to the elementary row
operations used. Since any row echelon form will suﬃce, we may use Gauss–
Jordan reduction to reduce A to EA as shown below:


1
2
2
3
1
0
0
2
4
1
3
0
1
0
3
6
1
4
0
0
1

−→


1
2
0
1
−1/3
2/3
0
0
0
1
1
2/3
−1/3
0
0
0
0
0
1/3
−5/3
1


P =


−1/3
2/3
0
2/3
−1/3
0
1/3
−5/3
1

, so (4.2.12) implies N

AT 
= span





1/3
−5/3
1




.

4.2 Four Fundamental Subspaces
177
Example 4.2.6
Problem: Suppose rank (Am×n) = r, and let P =
 P1
P2

be a nonsingular
matrix such that PA = U =
 Cr×n
0

, where U is in row echelon form. Prove
R (A) = N (P2).
(4.2.13)
Solution: The strategy is to ﬁrst prove R (A) ⊆N (P2) and then show the
reverse inclusion N (P2) ⊆R (A). The equation PA = U implies P2A = 0, so
all columns of A are in N (P2), and thus R (A) ⊆N (P2) . To show inclusion
in the opposite direction, suppose b ∈N (P2), so that
Pb =

P1
P2

b =

P1b
P2b

=

dr×1
0

.
Consequently, P

A | b

=

PA | Pb

=
 C
d
0
0

, and this implies
rank[A|b] = r = rank (A).
Recall from (2.3.4) that this means the system Ax = b is consistent, and thus
b ∈R (A) by (4.2.3). Therefore, N (P2) ⊆R (A), and we may conclude that
N (P2) = R (A).
It’s often important to know when two matrices have the same nullspace (or
left-hand nullspace). Below is one test for determining this.
Equal Nullspaces
For two matrices A and B of the same shape:
•
N (A) = N (B) if and only if A
row
∼B.
(4.2.14)
•
N

AT 
= N

BT 
if and only if A
col
∼B.
(4.2.15)
Proof.
We will prove (4.2.15). If N

AT 
= N

BT 
, then (4.2.12) guarantees
R

PT
2

= N

BT 
, and hence P2B = 0. But this means the columns of B
are in N (P2). That is, R (B) ⊆N (P2) = R (A) by using (4.2.13). If A is
replaced by B in the preceding argument—and in (4.2.13)— the result is that
R (A) ⊆R (B), and consequently we may conclude that R (A) = R (B) . The
desired conclusion (4.2.15) follows from (4.2.6). Statement (4.2.14) now follows
by replacing A and B by AT and BT in (4.2.15).

178
Chapter 4
Vector Spaces
Summary
The four fundamental subspaces associated with Am×n are as follows.
•
The range or column space:
R (A) = {Ax} ⊆ℜm.
•
The row space or left-hand range:
R

AT 
=

AT y

⊆ℜn.
•
The nullspace:
N (A) = {x | Ax = 0} ⊆ℜn.
•
The left-hand nullspace:
N

AT 
=

y | AT y = 0

⊆ℜm.
Let P be a nonsingular matrix such that PA = U, where U is in row
echelon form, and suppose rank (A) = r.
•
Spanning set for R (A) = the basic columns in A.
•
Spanning set for R

AT 
= the nonzero rows in U.
•
Spanning set for N (A) =the hi’s in the general solution of Ax = 0.
•
Spanning set for N

AT 
= the last m −r rows of P.
If A and B have the same shape, then
•
A
row
∼B ⇐⇒N (A) = N (B) ⇐⇒R

AT 
= R

BT 
.
•
A
col
∼B ⇐⇒R (A) = R (B) ⇐⇒N

AT 
= N

BT 
.
Exercises for section 4.2
4.2.1. Determine spanning sets for each of the four fundamental subspaces
associated with
A =


1
2
1
1
5
−2
−4
0
4
−2
1
2
2
4
9

.
4.2.2. Consider a linear system of equations Am×nx = b.
(a)
Explain why Ax = b is consistent if and only if b ∈R (A).
(b)
Explain why a consistent system Ax = b has a unique solution
if and only if N (A) = {0}.

4.2 Four Fundamental Subspaces
179
4.2.3. Suppose that A is a 3 × 3 matrix such that
R =





1
2
3

,


1
−1
2





and
N =





−2
1
0





span R (A) and N (A), respectively, and consider a linear system
Ax = b, where b =

1
−7
0

.
(a)
Explain why Ax = b must be consistent.
(b)
Explain why Ax = b cannot have a unique solution.
4.2.4. If A =





−1
1
1
−2
1
−1
0
3
−4
2
−1
0
3
−5
3
−1
0
3
−6
4
−1
0
3
−6
4




and b =





−2
−5
−6
−7
−7




, is b ∈R (A) ?
4.2.5. Suppose that A is an n × n matrix.
(a)
If R (A) = ℜn, explain why A must be nonsingular.
(b)
If A is nonsingular, describe its four fundamental subspaces.
4.2.6. Consider the matrices A =


1
1
5
2
0
6
1
2
7

and B =


1
−4
4
4
−8
6
0
−4
5

.
(a)
Do A and B have the same row space?
(b)
Do A and B have the same column space?
(c)
Do A and B have the same nullspace?
(d)
Do A and B have the same left-hand nullspace?
4.2.7. If A =
 A1
A2

is a square matrix such that N (A1) = R

AT
2

, prove
that A must be nonsingular.
4.2.8. Consider a linear system of equations Ax = b for which yT b = 0
for every y ∈N

AT 
. Explain why this means the system must be
consistent.
4.2.9. For matrices Am×n and Bm×p, prove that
R (A | B) = R (A) + R (B).

180
Chapter 4
Vector Spaces
4.2.10. Let p be one particular solution of a linear system Ax = b.
(a)
Explain the signiﬁcance of the set
p + N (A) = {p + h | h ∈N (A)} .
(b)
If rank (A3×3) = 1, sketch a picture of p + N (A) in ℜ3.
(c)
Repeat part (b) for the case when rank (A3×3) = 2.
4.2.11. Suppose that Ax = b is a consistent system of linear equations, and
let a ∈R

AT 
. Prove that the inner product aT x is constant for all
solutions to Ax = b.
4.2.12. For matrices such that the product AB is deﬁned, explain why each of
the following statements is true.
(a)
R (AB) ⊆R (A).
(b)
N (AB) ⊇N (B).
4.2.13. Suppose that B = {b1, b2, . . . , bn} is a spanning set for R (B). Prove
that A(B) = {Ab1, Ab2, . . . , Abn} is a spanning set for R (AB).

4.3 Linear Independence
181
4.3
LINEAR INDEPENDENCE
For a given set of vectors S = {v1, v2, . . . , vn} there may or may not exist
dependency relationships in the sense that it may or may not be possible to
express one vector as a linear combination of the others. For example, in the set
A =





1
−1
2

,


3
0
−1

,


9
−3
4




,
the third vector is a linear combination of the ﬁrst two—i.e., v3 = 3v1 + 2v2.
Such a dependency always can be expressed in terms of a homogeneous equation
by writing
3v1 + 2v2 −v3 = 0.
On the other hand, it is evident that there are no dependency relationships in
the set
B =





1
0
0

,


0
1
0

,


0
0
1





because no vector can be expressed as a combination of the others. Another way
to say this is to state that there are no solutions for α1, α2, and α3 in the
homogeneous equation
α1v1 + α2v2 + α3v3 = 0
other than the trivial solution α1 = α2 = α3 = 0. These observations are the
basis for the following deﬁnitions.
Linear Independence
A set of vectors S = {v1, v2, . . . , vn} is said to be a linearly in-
dependent set whenever the only solution for the scalars αi in the
homogeneous equation
α1v1 + α2v2 + · · · + αnvn = 0
(4.3.1)
is the trivial solution α1 = α2 = · · · = αn = 0. Whenever there is a
nontrivial solution for the α ’s (i.e., at least one αi ̸= 0 ) in (4.3.1), the
set S is said to be a linearly dependent set. In other words, linearly
independent sets are those that contain no dependency relationships,
and linearly dependent sets are those in which at least one vector is a
combination of the others. We will agree that the empty set is always
linearly independent.

182
Chapter 4
Vector Spaces
It is important to realize that the concepts of linear independence and de-
pendence are deﬁned only for sets—individual vectors are neither linearly inde-
pendent nor dependent. For example consider the following sets:
S1 =

1
0

,

0
1

,
S2 =

1
0

,

1
1

,
S3 =

1
0

,

0
1

,

1
1

.
It should be clear that S1 and S2 are linearly independent sets while S3 is
linearly dependent. This shows that individual vectors can simultaneously belong
to linearly independent sets as well as linearly dependent sets. Consequently, it
makes no sense to speak of “linearly independent vectors” or “linearly dependent
vectors.”
Example 4.3.1
Problem: Determine whether or not the set
S =





1
2
1

,


1
0
2

,


5
6
7





is linearly independent.
Solution:
Simply determine whether or not there exists a nontrivial solution
for the α ’s in the homogeneous equation
α1


1
2
1

+ α2


1
0
2

+ α3


5
6
7

=


0
0
0


or, equivalently, if there is a nontrivial solution to the homogeneous system


1
1
5
2
0
6
1
2
7




α1
α2
α3

=


0
0
0

.
If A =
 1
1
5
2
0
6
1
2
7

, then EA =
 1
0
3
0
1
2
0
0
0

, and therefore there exist nontrivial
solutions. Consequently, S is a linearly dependent set. Notice that one particular
dependence relationship in S is revealed by EA because it guarantees that
A∗3 = 3A∗1 +2A∗2. This example indicates why the question of whether or not
a subset of ℜm is linearly independent is really a question about whether or not
the nullspace of an associated matrix is trivial. The following is a more formal
statement of this fact.

4.3 Linear Independence
183
Linear Independence and Matrices
Let A be an m × n matrix.
•
Each of the following statements is equivalent to saying that the
columns of A form a linearly independent set.
▷
N (A) = {0}.
(4.3.2)
▷
rank (A) = n.
(4.3.3)
•
Each of the following statements is equivalent to saying that the rows
of A form a linearly independent set.
▷
N

AT 
= {0}.
(4.3.4)
▷
rank (A) = m.
(4.3.5)
•
When A is a square matrix, each of the following statements is
equivalent to saying that A is nonsingular.
▷
The columns of A form a linearly independent set.
(4.3.6)
▷
The rows of A form a linearly independent set.
(4.3.7)
Proof.
By deﬁnition, the columns of A are a linearly independent set when
the only set of α ’s satisfying the homogeneous equation
0 = α1A∗1 + α2A∗2 + · · · + αnA∗n =

A∗1 | A∗2 | · · · | A∗n





α1
α2
...
αn




is the trivial solution α1 = α2 = · · · = αn = 0, which is equivalent to saying
N (A) = {0}. The fact that N (A) = {0} is equivalent to rank (A) = n was
demonstrated in (4.2.10). Statements (4.3.4) and (4.3.5) follow by replacing A
by AT in (4.3.2) and (4.3.3) and by using the fact that rank (A) = rank

AT 
.
Statements (4.3.6) and (4.3.7) are simply special cases of (4.3.3) and (4.3.5).
Example 4.3.2
Any set {ei1, ei2, . . . , ein} consisting of distinct unit vectors is a linearly indepen-
dent set because rank

ei1 | ei2 | · · · | ein

= n. For example, the set of unit vec-
tors {e1, e2, e4} in ℜ4 is linearly independent because rank


1
0
0
0
1
0
0
0
0
0
0
1

= 3.

184
Chapter 4
Vector Spaces
Example 4.3.3
Diagonal Dominance. A matrix An×n is said to be diagonally dominant
whenever
|aii| >
n

j=1
j̸=i
|aij|
for each i = 1, 2, . . . , n.
That is, the magnitude of each diagonal entry exceeds the sum of the magni-
tudes of the oﬀ-diagonal entries in the corresponding row. Diagonally dominant
matrices occur naturally in a wide variety of practical applications, and when
solving a diagonally dominant system by Gaussian elimination, partial pivoting
is never required—you are asked to provide the details in Exercise 4.3.15.
Problem: In 1900, Minkowski (p. 278) discovered that all diagonally dominant
matrices are nonsingular. Establish the validity of Minkowski’s result.
Solution: The strategy is to prove that if A is diagonally dominant, then
N (A) = {0}, so that (4.3.2) together with (4.3.6) will provide the desired
conclusion. Use an indirect argument—suppose there exists a vector x ̸= 0 such
that Ax = 0, and assume that xk is the entry of maximum magnitude in x.
Focus on the kth component of Ax, and write the equation Ak∗x = 0 as
akkxk = −
n

j=1
j̸=k
akjxj.
Taking absolute values of both sides and using the triangle inequality together
with the fact that |xj| ≤|xk| for each j produces
|akk| |xk| =

n

j=1
j̸=k
akjxj

≤
n

j=1
j̸=k
|akjxj| =
n

j=1
j̸=k
|akj| |xj| ≤

n

j=1
j̸=k
|akj|
 
|xk|.
But this implies that
|akk| ≤
n

j=1
j̸=k
|akj|,
which violates the hypothesis that A is diagonally dominant. Therefore, the
assumption that there exists a nonzero vector in N (A) must be false, so we
may conclude that N (A) = {0}, and hence A is nonsingular.
Note: An alternate solution is given in Example 7.1.6 on p. 499.

4.3 Linear Independence
185
Example 4.3.4
Vandermonde Matrices. Matrices of the form
Vm×n =




1
x1
x2
1
· · ·
xn−1
1
1
x2
x2
2
· · ·
xn−1
2
...
...
...
· · ·
...
1
xm
x2
m
· · ·
xn−1
m




in which xi ̸= xj for all i ̸= j are called Vandermonde
26 matrices.
Problem: Explain why the columns in V constitute a linearly independent set
whenever n ≤m.
Solution: According to (4.3.2), the columns of V form a linearly independent
set if and only if N (V) = {0}. If




1
x1
x2
1
· · ·
xn−1
1
1
x2
x2
2
· · ·
xn−1
2
...
...
...
· · ·
...
1
xm
x2
m
· · ·
xn−1
m








α0
α1
...
αn−1



=




0
0
...
0



,
(4.3.8)
then for each i = 1, 2, . . . , m,
α0 + xiα1 + x2
i α2 + · · · + xn−1
i
αn−1 = 0.
This implies that the polynomial
p(x) = α0 + α1x + α2x2 + · · · + αn−1xn−1
has m distinct roots—namely, the xi ’s. However, deg p(x) ≤n −1 and the
fundamental theorem of algebra guarantees that if p(x) is not the zero polyno-
mial, then p(x) can have at most n −1 distinct roots. Therefore, (4.3.8) holds
if and only if αi = 0 for all i, and thus (4.3.2) insures that the columns of V
form a linearly independent set.
26
This is named in honor of the French mathematician Alexandre-Theophile Vandermonde (1735–
1796). He made a variety of contributions to mathematics, but he is best known perhaps for
being the ﬁrst European to give a logically complete exposition of the theory of determinants.
He is regarded by many as being the founder of that theory. However, the matrix V (and
an associated determinant) named after him, by Lebesgue, does not appear in Vandermonde’s
published work. Vandermonde’s ﬁrst love was music, and he took up mathematics only after
he was 35 years old. He advocated the theory that all art and music rested upon a general
principle that could be expressed mathematically, and he claimed that almost anyone could
become a composer with the aid of mathematics.

186
Chapter 4
Vector Spaces
Example 4.3.5
Problem: Given a set of m points S = {(x1, y1),
(x2, y2), . . . , (xm, ym)} in
which the xi ’s are distinct, explain why there is a unique polynomial
ℓ(t) = α0 + α1t + α2t2 + · · · + αm−1tm−1
(4.3.9)
of degree m −1 that passes through each point in S.
Solution: The coeﬃcients αi must satisfy the equations
α0 + α1x1 + α2x2
1 + · · · + αm−1xm−1
1
= ℓ(x1) = y1,
α0 + α1x2 + α2x2
2 + · · · + αm−1xm−1
2
= ℓ(x2) = y2,
...
α0 + α1xm + α2x2
m + · · · + αm−1xm−1
m
= ℓ(xm) = ym.
Writing this m × m system as




1
x1
x2
1
· · ·
xm−1
1
1
x2
x2
2
· · ·
xm−1
2
...
...
...
· · ·
...
1
xm
x2
m
· · ·
xm−1
m








α0
α1
...
αm−1



=




y1
y2
...
ym




reveals that the coeﬃcient matrix is a square Vandermonde matrix, so the result
of Example 4.3.4 guarantees that it is nonsingular. Consequently, the system has
a unique solution, and thus there is one and only one possible set of coeﬃcients
for the polynomial ℓ(t) in (4.3.9). In fact, ℓ(t) must be given by
ℓ(t) =
m

i=1

yi
!m
j̸=i(t −xj)
!m
j̸=i(xi −xj)

.
Verify this by showing that the right-hand side is indeed a polynomial of degree
m −1 that passes through the points in S. The polynomial ℓ(t) is known as
the Lagrange
27 interpolation polynomial of degree m −1.
If rank (Am×n) < n, then the columns of A must be a dependent set—
recall (4.3.3). For such matrices we often wish to extract a maximal linearly
independent subset of columns—i.e., a linearly independent set containing as
many columns from A as possible. Although there can be several ways to make
such a selection, the basic columns in A always constitute one solution.
27
Joseph Louis Lagrange (1736–1813), born in Turin, Italy, is considered by many to be one
of the two greatest mathematicians of the eighteenth century—Euler is the other. Lagrange
occupied Euler’s vacated position in 1766 in Berlin at the court of Frederick the Great who
wrote that “the greatest king in Europe” wishes to have at his court “the greatest mathe-
matician of Europe.” After 20 years, Lagrange left Berlin and eventually moved to France.
Lagrange’s mathematical contributions are extremely wide and deep, but he had a particularly
strong inﬂuence on the way mathematical research evolved. He was the ﬁrst of the top-class
mathematicians to recognize the weaknesses in the foundations of calculus, and he was among
the ﬁrst to attempt a rigorous development.

4.3 Linear Independence
187
Maximal Independent Subsets
If rank (Am×n) = r, then the following statements hold.
•
Any maximal independent subset of columns from A con-
tains exactly r columns.
(4.3.10)
•
Any maximal independent subset of rows from A contains
exactly r rows.
(4.3.11)
•
In particular, the r basic columns in A constitute one
maximal independent subset of columns from A.
(4.3.12)
Proof.
Exactly the same linear relationships that exist among the columns of
A must also hold among the columns of EA —by (3.9.6). This guarantees that
a subset of columns from A is linearly independent if and only if the columns
in the corresponding positions in EA are an independent set. Let
C =

c1 | c2 | · · · | ck

be a matrix that contains an independent subset of columns from EA so that
rank (C) = k —recall (4.3.3). Since each column in EA is a combination of the
r basic (unit) columns in EA, there are scalars βij such that cj = r
i=1 βijei
for j = 1, 2, . . . , k. These equations can be written as the single matrix equation

c1 | c2 | · · · | ck

=

e1 | e2 | · · · | er





β11
β12
· · ·
β1k
β21
β22
· · ·
β2k
...
...
...
...
βr1
βr2
· · ·
βrk




or
Cm×k =

Ir
0

Br×k =

Br×k
0

,
where
B = [βij].
Consequently, r ≥rank (C) = k, and therefore any independent subset of
columns from EA —and hence any independent set of columns from A —cannot
contain more than r vectors. Because the r basic (unit) columns in EA form
an independent set, the r basic columns in A constitute an independent set.
This proves (4.3.10) and (4.3.12). The proof of (4.3.11) follows from the fact that
rank (A) = rank

AT 
—recall (3.9.11).

188
Chapter 4
Vector Spaces
Basic Facts of Independence
For a nonempty set of vectors S = {u1, u2, . . . , un} in a space V, the
following statements are true.
•
If S contains a linearly dependent subset, then S itself
must be linearly dependent.
(4.3.13)
•
If S is linearly independent, then every subset of S is
also linearly independent.
(4.3.14)
•
If S is linearly independent and if v ∈V, then the ex-
tension set Sext = S ∪{v} is linearly independent if and
only if v /∈span (S) .
(4.3.15)
•
If S ⊆ℜm and if n > m, then S must be linearly
dependent.
(4.3.16)
Proof of (4.3.13).
Suppose that S contains a linearly dependent subset, and,
for the sake of convenience, suppose that the vectors in S have been permuted
so that this dependent subset is Sdep = {u1, u2, . . . , uk} . According to the
deﬁnition of dependence, there must be scalars α1, α2, . . . , αk, not all of which
are zero, such that α1u1 +α2u2 +· · ·+αkuk = 0. This means that we can write
α1u1 + α2u2 + · · · + αkuk + 0uk+1 + · · · + 0un = 0,
where not all of the scalars are zero, and hence S is linearly dependent.
Proof of (4.3.14).
This is an immediate consequence of (4.3.13).
Proof of (4.3.15).
If Sext is linearly independent, then v /∈span (S) , for
otherwise v would be a combination of vectors from S thus forcing Sext to
be a dependent set. Conversely, suppose v /∈span (S) . To prove that Sext is
linearly independent, consider a linear combination
α1u1 + α2u2 + · · · + αnun + αn+1v = 0.
(4.3.17)
It must be the case that αn+1 = 0, for otherwise v would be a combination of
vectors from S. Consequently,
α1u1 + α2u2 + · · · + αnun = 0.
But this implies that
α1 = α2 = · · · = αn = 0
because S is linearly independent. Therefore, the only solution for the α ’s in
(4.3.17) is the trivial set, and hence Sext must be linearly independent.
Proof of (4.3.16).
This follows from (4.3.3) because if the ui ’s are placed as
columns in a matrix Am×n, then rank (A) ≤m < n.

4.3 Linear Independence
189
Example 4.3.6
Let V be the vector space of real-valued functions of a real variable, and let S =
{f1(x), f2(x), . . . , fn(x)} be a set of functions that are n−1 times diﬀerentiable.
The Wronski
28 matrix is deﬁned to be
W(x) =






f1(x)
f2(x)
· · ·
fn(x)
f ′
1(x)
f ′
2(x)
· · ·
f ′
n(x)
...
...
...
...
f (n−1)
1
(x)
f (n−1)
2
(x)
· · ·
f (n−1)
n
(x)






.
Problem: If there is at least one point x = x0 such that W(x0) is nonsingular,
prove that S must be a linearly independent set.
Solution: Suppose that
0 = α1f1(x) + α2f2(x) + · · · + αnfn(x)
(4.3.18)
for all values of x. When x = x0, it follows that
0 = α1f1(x0) + α2f2(x0) + · · · + αnfn(x0),
0 = α1f ′
1(x0) + α2f ′
2(x0) + · · · + αnf ′
n(x0),
...
0 = α1f (n−1)
1
(x0) + α2f (n−1)
2
(x0) + · · · + αnf (n−1)
n
(x0),
which means that v =



α1
α2
...
αn


∈N

W(x0)

. But N

W(x0)

= {0} because
W(x0) is nonsingular, and hence v = 0. Therefore, the only solution for the
α ’s in (4.3.18) is the trivial solution α1 = α2 = · · · = αn = 0 thereby insuring
that S is linearly independent.
28
This matrix is named in honor of the Polish mathematician Jozef Maria H¨oen´e Wronski
(1778–1853), who studied four special forms of determinants, one of which was the deter-
minant of the matrix that bears his name. Wronski was born to a poor family near Poznan,
Poland, but he studied in Germany and spent most of his life in France. He is reported to have
been an egotistical person who wrote in an exhaustively wearisome style. Consequently, almost
no one read his work. Had it not been for his lone follower, Ferdinand Schweins (1780–1856)
of Heidelberg, Wronski would probably be unknown today. Schweins preserved and extended
Wronski’s results in his own writings, which in turn received attention from others. Wronski
also wrote on philosophy. While trying to reconcile Kant’s metaphysics with Leibniz’s calculus,
Wronski developed a social philosophy called “Messianism” that was based on the belief that
absolute truth could be achieved through mathematics.

190
Chapter 4
Vector Spaces
For example, to verify that the set of polynomials P =

1, x, x2, . . . , xn
is
linearly independent, observe that the associated Wronski matrix
W(x) =






1
x
x2
· · ·
xn
0
1
2x
· · ·
nxn−1
0
0
2
· · ·
n(n −1)xn−2
...
...
...
...
...
0
0
0
· · ·
n!






is triangular with nonzero diagonal entries. Consequently, W(x) is nonsingular
for every value of x, and hence P must be an independent set.
Exercises for section 4.3
4.3.1. Determine which of the following sets are linearly independent. For those
sets that are linearly dependent, write one of the vectors as a linear
combination of the others.
(a)





1
2
3

,


2
1
0

,


1
5
9




,
(b)
{( 1
2
3 ) , ( 0
4
5 ) , ( 0
0
6 ) , ( 1
1
1 )} ,
(c)





3
2
1

,


1
0
0

,


2
1
0




,
(d)
{( 2
2
2
2 ) , ( 2
2
0
2 ) , ( 2
0
2
2 )} ,
(e)


























1
2
0
4
0
3
0









,









0
2
0
4
1
3
0









,









0
2
1
4
0
3
0









,









0
2
0
4
0
3
1


























.
4.3.2. Consider the matrix A =
 2
1
1
0
4
2
1
2
6
3
2
2

.
(a)
Determine a maximal linearly independent subset of columns
from A.
(b)
Determine the total number of linearly independent subsets that
can be constructed using the columns of A.

4.3 Linear Independence
191
4.3.3. Suppose that in a population of a million children the height of each one
is measured at ages 1 year, 2 years, and 3 years, and accumulate this
data in a matrix








1 yr
2 yr
3 yr
#1
h11
h12
h13
#2
h21
h22
h23
...
...
...
...
#i
hi1
hi2
hi3
...
...
...
...








= H.
Explain why there are at most three “independent children” in the sense
that the heights of all the other children must be a combination of these
“independent” ones.
4.3.4. Consider a particular species of wildﬂower in which each plant has several
stems, leaves, and ﬂowers, and for each plant let the following hold.
S = the average stem length (in inches).
L = the average leaf width (in inches).
F = the number of ﬂowers.
Four particular plants are examined, and the information is tabulated
in the following matrix:
A =




S
L
F
#1
1
1
10
#2
2
1
12
#3
2
2
15
#4
3
2
17



.
For these four plants, determine whether or not there exists a linear rela-
tionship between S, L, and F. In other words, do there exist constants
α0, α1, α2, and α3 such that α0 + α1S + α2L + α3F = 0 ?
4.3.5. Let S = {0} be the set containing only the zero vector.
(a)
Explain why S must be linearly dependent.
(b)
Explain why any set containing a zero vector must be linearly
dependent.
4.3.6. If T is a triangular matrix in which each tii ̸= 0, explain why the rows
and columns of T must each be linearly independent sets.

192
Chapter 4
Vector Spaces
4.3.7. Determine whether or not the following set of matrices is a linearly
independent set:

1
0
0
0

,

1
1
0
0

,

1
1
1
0

,

1
1
1
1

.
4.3.8. Without doing any computation, determine whether the following ma-
trix is singular or nonsingular:
A =






n
1
1
· · ·
1
1
n
1
· · ·
1
1
1
n
· · ·
1
...
...
...
...
...
1
1
1
· · ·
n






n×n
.
4.3.9. In theory, determining whether or not a given set is linearly independent
is a well-deﬁned problem with a straightforward solution. In practice,
however, this problem is often not so well deﬁned because it becomes
clouded by the fact that we usually cannot use exact arithmetic, and con-
tradictory conclusions may be produced depending upon the precision
of the arithmetic. For example, let
S =





.1
.4
.7

,


.2
.5
.8

,


.3
.6
.901




.
(a)
Use exact arithmetic to determine whether or not S is linearly
independent.
(b)
Use 3-digit arithmetic (without pivoting or scaling) to determine
whether or not S is linearly independent.
4.3.10. If Am×n is a matrix such that n
j=1 aij = 0 for each i = 1, 2, . . . , m
(i.e., each row sum is 0), explain why the columns of A are a linearly
dependent set, and hence rank (A) < n.
4.3.11. If S = {u1, u2, . . . , un} is a linearly independent subset of ℜm×1, and
if Pm×m is a nonsingular matrix, explain why the set
P(S) = {Pu1, Pu2, . . . , Pun}
must also be a linearly independent set. Is this result still true if P is
singular?

4.3 Linear Independence
193
4.3.12. Suppose that S = {u1, u2, . . . , un} is a set of vectors from ℜm. Prove
that S is linearly independent if and only if the set
S′ =

u1,
2

i=1
ui,
3

i=1
ui, . . . ,
n

i=1
ui

is linearly independent.
4.3.13. Which of the following sets of functions are linearly independent?
(a)
{sin x, cos x, x sin x} .
(b)

ex, xex, x2ex
.
(c)

sin2 x, cos2 x, cos 2x

.
4.3.14. Prove that the converse of the statement given in Example 4.3.6 is false
by showing that S =

x3, |x|3
is a linearly independent set, but the
associated Wronski matrix W(x) is singular for all values of x.
4.3.15. If AT is diagonally dominant, explain why partial pivoting is not needed
when solving Ax = b by Gaussian elimination. Hint: If after one step
of Gaussian elimination we have
A =

α
dT
c
B

one step
−−−−−−−−→
 α
dT
0
B −cdT
α

,
show that AT being diagonally dominant implies X =

B −cdT
α
T
must also be diagonally dominant.

194
Chapter 4
Vector Spaces
4.4
BASIS AND DIMENSION
Recall from §4.1 that S is a spanning set for a space V if and only if every
vector in V is a linear combination of vectors in S. However, spanning sets
can contain redundant vectors. For example, a subspace L deﬁned by a line
through the origin in ℜ2 may be spanned by any number of nonzero vectors
{v1, v2, . . . , vk} in L, but any one of the vectors {vi} by itself will suﬃce.
Similarly, a plane P through the origin in ℜ3 can be spanned in many diﬀerent
ways, but the parallelogram law indicates that a minimal spanning set need only
be an independent set of two vectors from P. These considerations motivate the
following deﬁnition.
Basis
A linearly independent spanning set for a vector space V is called a
basis for V.
It can be proven that every vector space V possesses a basis—details for
the case when V ⊆ℜm are asked for in the exercises. Just as in the case of
spanning sets, a space can possess many diﬀerent bases.
Example 4.4.1
•
The unit vectors S = {e1, e2, . . . , en} in ℜn are a basis for ℜn. This is
called the standard basis for ℜn.
•
If A is an n × n nonsingular matrix, then the set of rows in A as well as
the set of columns from A constitute a basis for ℜn. For example, (4.3.3)
insures that the columns of A are linearly independent, and we know they
span ℜn because R (A) = ℜn —recall Exercise 4.2.5(b).
•
For the trivial vector space Z = {0}, there is no nonempty linearly indepen-
dent spanning set. Consequently, the empty set is considered to be a basis
for Z.
•
The set

1, x, x2, . . . , xn
is a basis for the vector space of polynomials
having degree n or less.
•
The inﬁnite set

1, x, x2, . . .

is a basis for the vector space of all polynomi-
als. It should be clear that no ﬁnite basis is possible.

4.4 Basis and Dimension
195
Spaces that possess a basis containing an inﬁnite number of vectors are
referred to as inﬁnite-dimensional spaces, and those that have a ﬁnite basis
are called ﬁnite-dimensional spaces. This is often a line of demarcation in
the study of vector spaces. A complete theoretical treatment would include the
analysis of inﬁnite-dimensional spaces, but this text is primarily concerned with
ﬁnite-dimensional spaces over the real or complex numbers. It can be shown that,
in eﬀect, this amounts to analyzing ℜn or Cn and their subspaces.
The original concern of this section was to try to eliminate redundancies
from spanning sets so as to provide spanning sets containing a minimal number
of vectors. The following theorem shows that a basis is indeed such a set.
Characterizations of a Basis
Let V be a subspace of ℜm, and let B = {b1, b2, . . . , bn} ⊆V. The
following statements are equivalent.
•
B is a basis for V.
(4.4.1)
•
B is a minimal spanning set for V.
(4.4.2)
•
B is a maximal linearly independent subset of V.
(4.4.3)
Proof.
First argue that (4.4.1) =⇒(4.4.2) =⇒(4.4.1), and then show (4.4.1)
is equivalent to (4.4.3).
Proof of (4.4.1) =⇒(4.4.2). First suppose that B is a basis for V, and
prove that B is a minimal spanning set by using an indirect argument—i.e.,
assume that B is not minimal, and show that this leads to a contradiction. If
X = {x1, x2, . . . , xk} is a basis for V in which k < n, then each bj can be
written as a combination of the xi ’s. That is, there are scalars αij such that
bj =
k

i=1
αijxi
for j = 1, 2, . . . , n.
(4.4.4)
If the b ’s and x ’s are placed as columns in matrices
Bm×n =

b1 | b2 | · · · | bn

and
Xm×k =

x1 | x2 | · · · | xk

,
then (4.4.4) can be expressed as the matrix equation
B = XA,
where,
Ak×n = [αij] .
Since the rank of a matrix cannot exceed either of its size dimensions, and since
k < n, we have that rank (A) ≤k < n, so that N (A) ̸= {0} —recall (4.2.10).
If z ̸= 0 is such that Az = 0, then Bz = 0. But this is impossible because

196
Chapter 4
Vector Spaces
the columns of B are linearly independent, and hence N (B) = {0} —recall
(4.3.2). Therefore, the supposition that there exists a basis for V containing
fewer than n vectors must be false, and we may conclude that B is indeed a
minimal spanning set.
Proof of (4.4.2) =⇒(4.4.1). If B is a minimal spanning set, then B must
be a linearly independent spanning set. Otherwise, some bi would be a linear
combination of the other b ’s, and the set
B′ = {b1, . . . , bi−1, bi+1, . . . , bn}
would still span V, but B′ would contain fewer vectors than B, which is im-
possible because B is a minimal spanning set.
Proof of (4.4.3) =⇒(4.4.1). If B is a maximal linearly independent subset
of V, but not a basis for V, then there exists a vector v ∈V such that
v /∈span (B) . This means that the extension set
B ∪{v} = {b1, b2, . . . , bn, v}
is linearly independent—recall (4.3.15). But this is impossible because B is a
maximal linearly independent subset of V. Therefore, B is a basis for V.
Proof of (4.4.1) =⇒(4.4.3). Suppose that B is a basis for V, but not a
maximal linearly independent subset of V, and let
Y = {y1, y2, . . . , yk} ⊆V,
where
k > n
be a maximal linearly independent subset—recall that (4.3.16) insures the ex-
istence of such a set. The previous argument shows that Y must be a basis
for V. But this is impossible because we already know that a basis must be a
minimal spanning set, and B is a spanning set containing fewer vectors than Y.
Therefore, B must be a maximal linearly independent subset of V.
Although a space V can have many diﬀerent bases, the preceding result
guarantees that all bases for V contain the same number of vectors. If B1 and
B2 are each a basis for V, then each is a minimal spanning set, and thus they
must contain the same number of vectors. As we are about to see, this number
is quite important.
Dimension
The dimension of a vector space V is deﬁned to be
dim V = number of vectors in any basis for V
= number of vectors in any minimal spanning set for V
= number of vectors in any maximal independent subset of V.

4.4 Basis and Dimension
197
Example 4.4.2
•
If Z = {0} is the trivial subspace, then dim Z = 0 because the basis for
this space is the empty set.
•
If L is a line through the origin in ℜ3, then dim L = 1 because a basis for
L consists of any nonzero vector lying along L.
•
If P is a plane through the origin in ℜ3, then dim P = 2 because a minimal
spanning set for P must contain two vectors from P.
•
dim ℜ3 = 3 because the three unit vectors
 1
0
0

,
 0
1
0

,
 0
0
1

constitute
a basis for ℜ3.
•
dim ℜn = n because the unit vectors {e1, e2, . . . , en} in ℜn form a basis.
Example 4.4.3
Problem:
If V is an n -dimensional space, explain why every independent
subset S = {v1, v2, . . . , vn} ⊂V containing n vectors must be a basis for V.
Solution: dim V = n means that every subset of V that contains more than n
vectors must be linearly dependent. Consequently, S is a maximal independent
subset of V, and hence S is a basis for V.
Example 4.4.2 shows that in a loose sense the dimension of a space is a
measure of the amount of “stuﬀ” in the space—a plane P in ℜ3 has more
“stuﬀ” in it than a line L, but P contains less “stuﬀ” than the entire space
ℜ3. Recall from the discussion in §4.1 that subspaces of ℜn are generalized
versions of ﬂat surfaces through the origin. The concept of dimension gives us a
way to distinguish between these “ﬂat” objects according to how much “stuﬀ”
they contain—much the same way we distinguish between lines and planes in ℜ3.
Another way to think about dimension is in terms of “degrees of freedom.” In
the trivial space Z, there are no degrees of freedom—you can move nowhere—
whereas on a line there is one degree of freedom—length; in a plane there are
two degrees of freedom—length and width; in ℜ3 there are three degrees of
freedom—length, width, and height; etc.
It is important not to confuse the dimension of a vector space V with the
number of components contained in the individual vectors from V. For example,
if P is a plane through the origin in ℜ3, then dim P = 2, but the individual
vectors in P each have three components. Although the dimension of a space V
and the number of components contained in the individual vectors from V need
not be the same, they are nevertheless related. For example, if V is a subspace of
ℜn, then (4.3.16) insures that no linearly independent subset in V can contain
more than n vectors and, consequently, dim V ≤n. This observation generalizes
to produce the following theorem.

198
Chapter 4
Vector Spaces
Subspace Dimension
For vector spaces M and N such that M ⊆N, the following state-
ments are true.
•
dim M ≤dim N.
(4.4.5)
•
If dim M = dim N, then M = N.
(4.4.6)
Proof.
Let dim M = m and dim N = n, and use an indirect argument to
prove (4.4.5). If it were the case that m > n, then there would exist a linearly
independent subset of N (namely, a basis for M ) containing more than n vec-
tors. But this is impossible because dim N is the size of a maximal independent
subset of N. Thus m ≤n. Now prove (4.4.6). If m = n but M ̸= N, then
there exists a vector x such that x ∈N but x /∈M. If B is a basis for M,
then x /∈span (B) , and the extension set E = B ∪{x} is a linearly independent
subset of N —recall (4.3.15). But E contains m + 1 = n + 1 vectors, which is
impossible because dim N = n is the size of a maximal independent subset of
N. Hence M = N.
Let’s now ﬁnd bases and dimensions for the four fundamental subspaces
of an m × n matrix A of rank r, and let’s start with R (A). The entire set
of columns in A spans R (A), but they won’t form a basis when there are
dependencies among some of the columns. However, the set of basic columns in
A is also a spanning set—recall (4.2.8)—and the basic columns always constitute
a linearly independent set because no basic column can be a combination of other
basic columns (otherwise it wouldn’t be basic). So, the set of basic columns is a
basis for R (A), and, since there are r of them, dim R (A) = r = rank (A).
Similarly, the entire set of rows in A spans R

AT 
, but the set of all rows
is not a basis when dependencies exist. Recall from (4.2.7) that if U =
 Cr×n
0

is any row echelon form that is row equivalent to A, then the rows of C span
R

AT 
. Since rank (C) = r, (4.3.5) insures that the rows of C are linearly
independent. Consequently, the rows in C are a basis for R

AT 
, and, since
there are r of them, dim R

AT 
= r = rank (A). Older texts referred to
dim R

AT 
as the row rank of A, while dim R (A) was called the column rank
of A, and it was a major task to prove that the row rank always agrees with the
column rank. Notice that this is a consequence of the discussion above where it
was observed that dim R

AT 
= r = dim R (A).
Turning to the nullspaces, let’s ﬁrst examine N

AT 
. We know from
(4.2.12) that if P is a nonsingular matrix such that PA = U is in row echelon
form, then the last m −r rows in P span N

AT 
. Because the set of rows
in a nonsingular matrix is a linearly independent set, and because any subset

4.4 Basis and Dimension
199
of an independent set is again independent—see (4.3.7) and (4.3.14)—it follows
that the last m −r rows in P are linearly independent, and hence they con-
stitute a basis for N

AT 
. And this implies dim N

AT 
= m −r (i.e., the
number of rows in A minus the rank of A). Replacing A by AT shows that
dim N

AT T 
= dim N (A) is the number of rows in AT minus rank

AT 
.
But rank

AT 
= rank (A) = r, so dim N (A) = n−r. We deduced dim N (A)
without exhibiting a speciﬁc basis, but a basis for N (A) is easy to describe.
Recall that the set H containing the hi ’s appearing in the general solution
(4.2.9) of Ax = 0 spans N (A). Since there are exactly n −r vectors in H,
and since dim N (A) = n −r, H is a minimal spanning set, so, by (4.4.2), H
must be a basis for N (A). Below is a summary of facts uncovered above.
Fundamental Subspaces—Dimension and Bases
For an m × n matrix of real numbers such that rank (A) = r,
•
dim R (A) = r,
(4.4.7)
•
dim N (A) = n −r,
(4.4.8)
•
dim R

AT 
= r,
(4.4.9)
•
dim N

AT 
= m −r.
(4.4.10)
Let P be a nonsingular matrix such that PA = U is in row echelon
form, and let H be the set of hi ’s appearing in the general solution
(4.2.9) of Ax = 0.
•
The basic columns of A form a basis for R (A).
(4.4.11)
•
The nonzero rows of U form a basis for R

AT 
.
(4.4.12)
•
The set H is a basis for N (A).
(4.4.13)
•
The last m −r rows of P form a basis for N

AT 
.
(4.4.14)
For matrices with complex entries, the above statements remain valid
provided that AT is replaced with A∗.
Statements (4.4.7) and (4.4.8) combine to produce the following theorem.
Rank Plus Nullity Theorem
•
dim R (A) + dim N (A) = n for all m × n matrices.
(4.4.15)

200
Chapter 4
Vector Spaces
In loose terms, this is a kind of conservation law—it says that as the amount
of “stuﬀ” in R (A) increases, the amount of “stuﬀ” in N (A) must decrease,
and vice versa. The phrase rank plus nullity is used because dim R (A) is the
rank of A, and dim N (A) was traditionally known as the nullity of A.
Example 4.4.4
Problem: Determine the dimension as well as a basis for the space spanned by
S =





1
2
1

,


1
0
2

,


5
6
7




.
Solution 1: Place the vectors as columns in a matrix A, and reduce
A =


1
1
5
2
0
6
1
2
7

−→EA =


1
0
3
0
1
2
0
0
0

.
Since span (S) = R (A), we have
dim

span (S)

= dim R (A) = rank (A) = 2.
The basic columns B =
 1
2
1

,
 1
0
2

are a basis for R (A) = span (S) .
Other bases are also possible. Examining EA reveals that any two vectors in S
form an independent set, and therefore any pair of vectors from S constitutes
a basis for span (S) .
Solution 2: Place the vectors from S as rows in a matrix B, and reduce B
to row echelon form:
B =


1
2
1
1
0
2
5
6
7

−→U =


1
2
1
0
−2
1
0
0
0

.
This time we have span (S) = R

BT 
, so that
dim

span (S)

= dim R

BT 
= rank (B) = rank (U) = 2,
and a basis for span (S) = R

BT 
is given by the nonzero rows in U.

4.4 Basis and Dimension
201
Example 4.4.5
Problem:
If Sr = {v1, v2, . . . , vr} is a linearly independent subset of an
n -dimensional space V, where r < n, explain why it must be possible to ﬁnd
extension vectors {vr+1, . . . , vn} from V such that
Sn = {v1, . . . , vr, vr+1, . . . , vn}
is a basis for V.
Solution 1:
r < n means that span (Sr) ̸= V, and hence there exists a vector
vr+1 ∈V such that vr+1 /∈span (Sr) . The extension set Sr+1 = Sr ∪{vr+1} is
an independent subset of V containing r+1 vectors—recall (4.3.15). Repeating
this process generates independent subsets Sr+2, Sr+3, . . . , and eventually leads
to a maximal independent subset Sn ⊂V containing n vectors.
Solution 2:
The ﬁrst solution shows that it is theoretically possible to ﬁnd
extension vectors, but the argument given is not much help in actually computing
them. It is easy to remedy this situation. Let {b1, b2, . . . , bn} be any basis for
V, and place the given vi ’s along with the bi ’s as columns in a matrix
A =

v1 | · · · | vr | b1 | · · · | bn

.
Clearly, R (A) = V so that the set of basic columns from A is a basis for V.
Observe that {v1, v2, . . . , vr} are basic columns in A because no one of these is
a combination of preceding ones. Therefore, the remaining n −r basic columns
must be a subset of {b1, b2, . . . , bn} —say they are

bj1, bj2, . . . , bjn−r

. The
complete set of basic columns from A, and a basis for V, is the set
B =

v1, . . . , vr, bj1, . . . , bjn−r

.
For example, to extend the independent set
S =








1
0
−1
2


,



0
0
1
−2








to a basis for ℜ4, append the standard basis {e1, e2, e3, e4} to the vectors in
S, and perform the reduction
A =



1
0
1
0
0
0
0
0
0
1
0
0
−1
1
0
0
1
0
2
−2
0
0
0
1


−→EA =



1
0
1
0
0
0
0
1
1
0
0
−1/2
0
0
0
1
0
0
0
0
0
0
1
1/2


.
This reveals that {A∗1, A∗2, A∗4, A∗5} are the basic columns in A, and there-
fore
B =








1
0
−1
2


,



0
0
1
−2


,



0
1
0
0


,



0
0
1
0








is a basis for ℜ4 that contains S.

202
Chapter 4
Vector Spaces
Example 4.4.6
Rank and Connectivity. A set of points (or nodes), {N1, N2, . . . , Nm} , to-
gether with a set of paths (or edges), {E1, E2, . . . , En} , between the nodes is
called a graph. A connected graph is one in which there is a sequence of edges
linking any pair of nodes, and a directed graph is one in which each edge has been
assigned a direction. For example, the graph in Figure 4.4.1 is both connected
and directed.
E6
E5
E4
E3
E2
E1
1
2
3
4
Figure 4.4.1
The connectivity of a directed graph is independent of the directions assigned
to the edges—i.e., changing the direction of an edge doesn’t change the connec-
tivity. (Exercise 4.4.20 presents another type of connectivity in which direction
matters.) On the surface, the concepts of graph connectivity and matrix rank
seem to have little to do with each other, but, in fact, there is a close relationship.
The incidence matrix associated with a directed graph containing m nodes
and n edges is deﬁned to be the m × n matrix E whose (k, j) -entry is
ekj =



1
if edge Ej is directed toward node Nk.
−1
if edge Ej is directed away from node Nk.
0
if edge Ej neither begins nor ends at node Nk.
For example, the incidence matrix associated with the graph in Figure 4.4.1 is
E =




E1
E2
E3
E4
E5
E6
N1
1
−1
0
0
−1
0
N2
−1
0
−1
1
0
0
N3
0
0
1
0
1
1
N4
0
1
0
−1
0
−1



.
(4.4.16)
Each edge in a directed graph is associated with two nodes—the nose and the tail
of the edge—so each column in E must contain exactly two nonzero entries—a
(+1) and a (−1). Consequently, all column sums are zero. In other words, if
eT = ( 1
1
· · ·
1 ) , then eT E = 0, so e ∈N

ET 
, and
rank (E) = rank

ET 
= m −dim N

ET 
≤m −1.
(4.4.17)
This inequality holds regardless of the connectivity of the associated graph, but
marvelously, equality is attained if and only if the graph is connected.

4.4 Basis and Dimension
203
Rank and Connectivity
Let G be a graph containing m nodes. If G is undirected, arbitrarily
assign directions to the edges to make G directed, and let E be the
corresponding incidence matrix.
•
G is connected if and only if rank (E) = m −1.
(4.4.18)
Proof.
Suppose G is connected. Prove rank (E) = m −1 by arguing that
dim N

ET 
= 1, and do so by showing e = ( 1
1
· · ·
1 )T is a basis N

ET 
.
To see that e spans N

ET 
, consider an arbitrary x ∈N

ET 
, and focus on
any two components xi and xk in x along with the corresponding nodes Ni
and Nk in G. Since G is connected, there must exist a subset of r nodes,
{Nj1, Nj2, . . . , Njr} ,
where
i = j1
and
k = jr,
such that there is an edge between Njp and Njp+1 for each p = 1, 2, . . . , r −1.
Therefore, corresponding to each of the r −1 pairs

Njp, Njp+1

, there must
exist a column cp in E (not necessarily the pth column) such that components
jp and jp+1 in cp are complementary in the sense that one is (+1) while the
other is (−1) (all other components are zero). Because xT E = 0, it follows that
xT cp = 0, and hence xjp = xjp+1. But this holds for every p = 1, 2, . . . , r −1,
so xi = xk for each i and k, and hence x = αe for some scalar α. Thus {e}
spans N

ET 
. Clearly, {e} is linearly independent, so it is a basis N

ET 
,
and, therefore, dim N

ET 
= 1 or, equivalently, rank (E) = m−1. Conversely,
suppose rank (E) = m−1, and prove G is connected with an indirect argument.
If G is not connected, then G is decomposable into two nonempty subgraphs
G1 and G2 in which there are no edges between nodes in G1 and nodes in G2.
This means that the nodes in G can be ordered so as to make E have the form
E =

E1
0
0
E2

,
where E1 and E2 are the incidence matrices for G1 and G2, respectively. If
G1 and G2 contain m1 and m2 nodes, respectively, then (4.4.17) insures that
rank (E)=rank

E1 0
0 E2

=rank (E1)+rank (E1)≤(m1−1)+(m2−1)=m−2.
But this contradicts the hypothesis that rank (E) = m −1, so the supposition
that G is not connected must be false.

204
Chapter 4
Vector Spaces
Example 4.4.7
An Application to Electrical Circuits. Recall from the discussion on p. 73
that applying Kirchhoﬀ’s node rule to an electrical circuit containing m nodes
and n branches produces m homogeneous linear equations in n unknowns (the
branch currents), and Kirchhoﬀ’s loop rule provides a nonhomogeneous equation
for each simple loop in the circuit. For example, consider the circuit in Figure
4.4.2 along with its four nodal equations and three loop equations—this is the
same circuit appearing on p. 73, and the equations are derived there.
1
2
3
4
R1
R6
R5
R4
R3
R2
E4
E3
E2
E1
I2
I4
I1
I5
I6
I3
A
B
C
Node 1:
I1 −I2 −I5 = 0
Node 2:
−I1 −I3 + I4 = 0
Node 3:
I3 + I5 + I6 = 0
Node 4:
I2 −I4 −I6 = 0
Loop A:
I1R1 −I3R3 + I5R5 = E1 −E3
Loop B:
I2R2 −I5R5 + I6R6 = E2
Loop C:
I3R3 + I4R4 −I6R6 = E3 + E4
Figure 4.4.2
The directed graph and associated incidence matrix E deﬁned by this circuit
are the same as those appearing in Example 4.4.6 in Figure 4.4.1 and equation
(4.4.16), so it’s apparent that the 4 × 3 homogeneous system of nodal equations
is precisely the system Ex = 0. This observation holds for general circuits. The
goal is to compute the six currents I1, I2, . . . , I6 by selecting six independent
equations from the entire set of node and loop equations. In general, if a circuit
containing m nodes is connected in the graph sense, then (4.4.18) insures that
rank (E) = m −1, so there are m independent nodal equations. But Example
4.4.6 also shows that 0 = eT E = E1∗+ E2∗+ · · · + Em∗, which means that
any row can be written in terms of the others, and this in turn implies that
every subset of m −1 rows in E must be independent (see Exercise 4.4.13).
Consequently, when any nodal equation is discarded, the remaining ones are
guaranteed to be independent. To determine an n × n nonsingular system that
has the n branch currents as its unique solution, it’s therefore necessary to ﬁnd
n−m+1 additional independent equations, and, as shown in §2.6, these are the
loop equations. A simple loop in a circuit is now seen to be a connected subgraph
that does not properly contain other connected subgraphs. Physics dictates that
the currents must be uniquely determined, so there must always be n −m + 1
simple loops, and the combination of these loop equations together with any
subset of m −1 nodal equations will be a nonsingular n × n system that yields
the branch currents as its unique solution. For example, any three of the nodal
equations in Figure 4.4.2 can be coupled with the three simple loop equations to
produce a 6 × 6 nonsingular system whose solution is the six branch currents.

4.4 Basis and Dimension
205
If X and Y are subspaces of a vector space V, then the sum of X and
Y was deﬁned in §4.1 to be
X + Y = {x + y | x ∈X and y ∈Y},
and it was demonstrated in (4.1.1) that X + Y is again a subspace of V. You
were asked in Exercise
4.1.8 to prove that the intersection X ∩Y is also a
subspace of V. We are now in a position to exhibit an important relationship
between dim (X + Y) and dim (X ∩Y) .
Dimension of a Sum
If X and Y are subspaces of a vector space V, then
dim (X + Y) = dim X + dim Y −dim (X ∩Y) .
(4.4.19)
Proof.
The strategy is to construct a basis for X + Y and count the number
of vectors it contains. Let S = {z1, z2, . . . , zt} be a basis for X ∩Y. Since
S ⊆X and S ⊆Y, there must exist extension vectors {x1, x2, . . . , xm} and
{y1, y2, . . . , yn} such that
BX = {z1, . . . , zt, x1, . . . , xm} = a basis for X
and
BY = {z1, . . . , zt, y1, . . . , yn} = a basis for Y.
We know from (4.1.2) that B = BX ∪BY spans X + Y, and we wish show that
B is linearly independent. If
t

i=1
αizi +
m

j=1
βjxj +
n

k=1
γkyk = 0,
(4.4.20)
then
n

k=1
γkyk = −


t

i=1
αizi +
m

j=1
βjxj

∈X.
Since it is also true that 
k γkyk ∈Y, we have that 
k γkyk ∈X ∩Y, and
hence there must exist scalars δi such that
n

k=1
γkyk =
t

i=1
δizi
or, equivalently,
n

k=1
γkyk −
t

i=1
δizi = 0.

206
Chapter 4
Vector Spaces
Since BY
is an independent set, it follows that all of the γk ’s (as well as all
δi ’s) are zero, and (4.4.20) reduces to t
i=1 αizi + m
j=1 βjxj = 0. But BX is
also an independent set, so the only way this can hold is for all of the αi ’s as
well as all of the βj ’s to be zero. Therefore, the only possible solution for the
α ’s, β ’s, and γ ’s in the homogeneous equation (4.4.20) is the trivial solution,
and thus B is linearly independent. Since B is an independent spanning set, it
is a basis for X + Y and, consequently,
dim (X + Y) = t+m+n = (t+m)+(t+n)−t = dim X +dim Y−dim (X ∩Y) .
Example 4.4.8
Problem: Show that rank (A + B) ≤rank (A) + rank (B).
Solution: Observe that
R (A + B) ⊆R (A) + R (B)
because if b ∈R (A + B), then there is a vector x such that
b = (A + B)x = Ax + Bx ∈R (A) + R (B).
Recall from (4.4.5) that if M and N are vector spaces such that M ⊆N, then
dim M ≤dim N. Use this together with formula (4.4.19) for the dimension of a
sum to conclude that
rank (A + B) = dim R (A + B) ≤dim

R (A) + R (B)

= dim R (A) + dim R (B) −dim

R (A) ∩R (B)

≤dim R (A) + dim R (B) = rank (A) + rank (B).
Exercises for section 4.4
4.4.1. Find the dimensions of the four fundamental subspaces associated with
A =


1
2
2
3
2
4
1
3
3
6
1
4

.
4.4.2. Find a basis for each of the four fundamental subspaces associated with
A =


1
2
0
2
1
3
6
1
9
6
2
4
1
7
5

.

4.4 Basis and Dimension
207
4.4.3. Determine the dimension of the space spanned by the set
S =








1
2
−1
3


,



1
0
0
2


,



2
8
−4
8


,



1
1
1
1


,



3
3
0
6








.
4.4.4. Determine the dimensions of each of the following vector spaces:
(a)
The space of polynomials having degree n or less.
(b)
The space ℜm×n of m × n matrices.
(c)
The space of n × n symmetric matrices.
4.4.5. Consider the following matrix and column vector:
A =


1
2
2
0
5
2
4
3
1
8
3
6
1
5
5


and
v =





−8
1
3
3
0




.
Verify that v ∈N (A), and then extend {v} to a basis for N (A).
4.4.6. Determine whether or not the set
B =





2
3
2

,


1
1
−1





is a basis for the space spanned by the set
A =





1
2
3

,


5
8
7

,


3
4
1




.
4.4.7. Construct a 4 × 4 homogeneous system of equations that has no zero
coeﬃcients and three linearly independent solutions.
4.4.8. Let B = {b1, b2, . . . , bn} be a basis for a vector space V. Prove that
each v ∈V can be expressed as a linear combination of the bi ’s
v = α1b1 + α2b2 + · · · + αnbn,
in only one way—i.e., the coordinates αi are unique.

208
Chapter 4
Vector Spaces
4.4.9. For A ∈ℜm×n and a subspace S of ℜn×1, the image
A(S) = {Ax | x ∈S}
of S under A is a subspace of ℜm×1 —recall Exercise
4.1.9. Prove
that if S ∩N (A) = 0, then dim A(S) = dim(S). Hint: Use a basis
{s1, s2, . . . , sk} for S to determine a basis for A(S).
4.4.10. Explain why
rank (A) −rank (B)
 ≤rank (A −B).
4.4.11. If rank (Am×n) = r and rank (Em×n) = k ≤r, explain why
r −k ≤rank (A + E) ≤r + k.
In words, this says that a perturbation of rank k can change the rank
by at most k.
4.4.12. Explain why every nonzero subspace V ⊆ℜn must possess a basis.
4.4.13. Explain why every set of m −1 rows in the incidence matrix E of a
connected directed graph containing m nodes is linearly independent.
4.4.14. For the incidence matrix E of a directed graph, explain why
"
EET #
ij =

number of edges at node i
when i = j,
−(number of edges between nodes i and j)
when i ̸= j.
4.4.15. If M and N are subsets of a space V, explain why
dim

span (M ∪N)

= dim

span (M)

+ dim

span (N)

−dim

span (M) ∩span (N)

.
4.4.16. Consider two matrices Am×n and Bm×k.
(a)
Explain why
rank (A | B) = rank (A) + rank (B) −dim

R (A) ∩R (B)

.
Hint: Recall Exercise 4.2.9.
(b)
Now explain why
dim N (A | B) = dim N (A)+dim N (B)+dim

R (A)∩R (B)

.
(c)
Determine dim

R (C) ∩N (C)

and dim

R (C) + N (C)

for
C =





−1
1
1
−2
1
−1
0
3
−4
2
−1
0
3
−5
3
−1
0
3
−6
4
−1
0
3
−6
4




.

4.4 Basis and Dimension
209
4.4.17. Suppose that A is a matrix with m rows such that the system Ax = b
has a unique solution for every b ∈ℜm. Explain why this means that
A must be square and nonsingular.
4.4.18. Let S be the solution set for a consistent system of linear equations
Ax = b.
(a)
If Smax = {s1, s2, . . . , st} is a maximal independent subset of
S, and if p is any particular solution, prove that
span (Smax) = span {p} + N (A).
Hint:
First show that x ∈S implies x ∈span (Smax) , and
then demonstrate set inclusion in both directions with the aid
of Exercise 4.2.10.
(b)
If b ̸= 0 and rank (Am×n) = r, explain why Ax = b has
n −r + 1 “independent solutions.”
4.4.19. Let rank (Am×n) = r, and suppose Ax = b with b ̸= 0 is a consistent
system. If H = {h1, h2, . . . , hn−r} is a basis for N (A), and if p is a
particular solution to Ax = b, show that
Smax = {p, p + h1, p + h2, . . . , p + hn−r}
is a maximal independent set of solutions.
4.4.20. Strongly Connected Graphs. In Example 4.4.6 we started with a
graph to construct a matrix, but it’s also possible to reverse the situation
by starting with a matrix to build an associated graph. The graph of
An×n (denoted by G(A)) is deﬁned to be the directed graph on n
nodes {N1, N2, . . . , Nn} in which there is a directed edge leading from
Ni to Nj if and only if aij ̸= 0. The directed graph G(A) is said to
be strongly connected provided that for each pair of nodes (Ni, Nk)
there is a sequence of directed edges leading from Ni to Nk. The matrix
A is said to be reducible if there exists a permutation matrix P such
that PT AP =
 X
Y
0
Z

, where X and Z are both square matrices.
Otherwise, A is said to be irreducible. Prove that G(A) is strongly
connected if and only if A is irreducible. Hint: Prove the contrapositive:
G(A) is not strongly connected if and only if A is reducible.

210
Chapter 4
Vector Spaces
4.5
MORE ABOUT RANK
Since equivalent matrices have the same rank, it follows that if P and Q are
nonsingular matrices such that the product PAQ is deﬁned, then
rank (A) = rank (PAQ) = rank (PA) = rank (AQ).
In other words, rank is invariant under multiplication by a nonsingular matrix.
However, multiplication by rectangular or singular matrices can alter the rank,
and the following formula shows exactly how much alteration occurs.
Rank of a Product
If A is m × n and B is n × p, then
rank (AB) = rank (B) −dim N (A) ∩R (B).
(4.5.1)
Proof.
Start with a basis S = {x1, x2, . . . , xs} for N (A) ∩R (B), and no-
tice N (A) ∩R (B) ⊆R (B). If dim R (B) = s + t, then, as discussed in
Example 4.4.5, there exists an extension set Sext = {z1, z2, . . . , zt} such that
B = {x1, . . . , xs, z1, . . . , zt} is a basis for R (B). The goal is to prove that
dim R (AB) = t, and this is done by showing T = {Az1, Az2, . . . , Azt} is a
basis for R (AB). T
spans R (AB) because if b ∈R (AB), then b = ABy
for some y, but By ∈R (B) implies By = s
i=1 ξixi + t
i=1 ηizi, so
b = A
 s

i=1
ξixi +
t

i=1
ηizi
 
=
s

i=1
ξiAxi +
t

i=1
ηiAzi =
t

i=1
ηiAzi.
T
is linearly independent because if 0 = t
i=1 αiAzi = A t
i=1 αizi, then
t
i=1 αizi ∈N (A) ∩R (B), so there are scalars βj such that
t

i=1
αizi =
s

j=1
βjxj
or, equivalently,
t

i=1
αizi −
s

j=1
βjxj = 0,
and hence the only solution for the αi ’s and βi ’s is the trivial solution because
B is an independent set. Thus T is a basis for R (AB), so t = dim R (AB) =
rank (AB), and hence
rank (B) = dim R (B) = s + t = dim N (A) ∩R (B) + rank (AB).
It’s sometimes necessary to determine an explicit basis for N (A) ∩R (B).
In particular, such a basis is needed to construct the Jordan chains that are
associated with the Jordan form that is discussed on pp. 582 and 594. The
following example outlines a procedure for ﬁnding such a basis.

4.5 More about Rank
211
Basis for an Intersection
If A is m × n and B is n × p, then a basis for N (A) ∩R (B) can
be constructed by the following procedure.
▷
Find a basis {x1, x2, . . . , xr} for R (B).
▷
Set Xn×r =

x1 | x2 | · · · | xr

.
▷
Find a basis {v1, v2, . . . , vs} for N (AX).
▷
B = {Xv1, Xv2, . . . , Xvs} is a basis for N (A) ∩R (B).
Proof.
The strategy is to argue that B is a maximal linear independent sub-
set of N (A) ∩R (B). Since each Xvj belongs to R (X) = R (B), and since
AXvj = 0, it’s clear that B ⊂N (A) ∩R (B). Let Vr×s =

v1 | v2 | · · · | vs

,
and notice that V and X each have full column rank. Consequently, N (X) = 0
so, by (4.5.1),
rank (XV)n×s = rank (V) −dim N (X) ∩R (V) = rank (V) = s,
which insures that B is linearly independent. B is a maximal independent
subset of N (A) ∩R (B) because (4.5.1) also guarantees that
s = dim N (AX) = dim N (X) + dim N (A) ∩R (X)
(see Exercise 4.5.10)
= dim N (A) ∩R (B).
The utility of (4.5.1) is mitigated by the fact that although rank (A) and
rank (B) are frequently known or can be estimated, the term dim N (A)∩R (B)
can be costly to obtain. In such cases (4.5.1) still provides us with useful upper
and lower bounds for rank (AB) that depend only on rank (A) and rank (B).
Bounds on the Rank of a Product
If A is m × n and B is n × p, then
•
rank (AB) ≤min {rank (A), rank (B)} ,
(4.5.2)
•
rank (A) + rank (B) −n ≤rank (AB).
(4.5.3)

212
Chapter 4
Vector Spaces
Proof.
In words, (4.5.2) says that the rank of a product cannot exceed the rank
of either factor. To prove rank (AB) ≤rank (B), use (4.5.1) and write
rank (AB) = rank (B) −dim N (A) ∩R (B) ≤rank (B).
This says that the rank of a product cannot exceed the rank of the right-hand
factor. To show that rank (AB) ≤rank (A), remember that transposition does
not alter rank, and use the reverse order law for transposes together with the
previous statement to write
rank (AB) = rank (AB)T = rank

BT AT 
≤rank

AT 
= rank (A).
To prove (4.5.3), notice that N (A)∩R (B) ⊆N (A), and recall from (4.4.5) that
if M and N are spaces such that M ⊆N, then dim M ≤dim N. Therefore,
dim N (A) ∩R (B) ≤dim N (A) = n −rank (A),
and the lower bound on rank (AB) is obtained from (4.5.1) by writing
rank (AB) = rank (B) −dim N (A) ∩R (B) ≥rank (B) + rank (A) −n.
The products AT A and AAT and their complex counterparts A∗A and
AA∗deserve special attention because they naturally appear in a wide variety
of applications.
Products AT A and AAT
For A ∈ℜm×n, the following statements are true.
•
rank

AT A

= rank (A) = rank

AAT 
.
(4.5.4)
•
R

AT A

= R

AT 
and
R

AAT 
= R (A).
(4.5.5)
•
N

AT A

= N (A)
and
N

AAT 
= N

AT 
.
(4.5.6)
For A ∈Cm×n, the transpose operation (⋆)T must be replaced by the
conjugate transpose operation (⋆)∗.

4.5 More about Rank
213
Proof.
First observe that N

AT 
∩R (A) = {0} because
x ∈N

AT 
∩R (A)
=⇒
AT x = 0 and x = Ay for some y
=⇒
xT x = yT AT x = 0
=⇒

x2
i = 0
=⇒
x = 0.
Formula (4.5.1) for the rank of a product now guarantees that
rank

AT A

= rank (A) −dim N

AT 
∩R (A) = rank (A),
which is half of (4.5.4)—the other half is obtained by reversing the roles of A
and AT . To prove (4.5.5) and (4.5.6), use the facts R (AB) ⊆R (A) and
N (B) ⊆N (AB) (see Exercise
4.2.12) to write R

AT A

⊆R

AT 
and
N (A) ⊆N

AT A

. The ﬁrst half of (4.5.5) and (4.5.6) now follows because
dim R

AT A

= rank

AT A

= rank (A) = rank

AT 
= dim R

AT 
,
dim N (A) = n −rank (A) = n −rank

AT A

= dim N

AT A

.
Reverse the roles of A and AT to get the second half of (4.5.5) and (4.5.6).
To see why (4.5.4)—(4.5.6) might be important, consider an m × n system
of equations Ax = b that may or may not be consistent. Multiplying on the
left-hand side by AT produces the n × n system
AT Ax = AT b
called the associated system of normal equations, which has some ex-
tremely interesting properties. First, notice that the normal equations are always
consistent, regardless of whether or not the original system is consistent because
(4.5.5) guarantees that AT b ∈R

AT 
= R

AT A

(i.e., the right-hand side is
in the range of the coeﬃcient matrix), so (4.2.3) insures consistency. However, if
Ax = b happens to be consistent, then Ax = b and AT Ax = AT b have the
same solution set because if p is a particular solution of the original system,
then Ap = b implies AT Ap = AT b (i.e., p is also a particular solution of
the normal equations), so the general solution of Ax = b is S = p + N (A),
and the general solution of AT Ax = AT b is
p + N

AT A

= p + N (A) = S.
Furthermore, if Ax = b is consistent and has a unique solution, then the same
is true for AT Ax = AT b, and the unique solution common to both systems is
x =

AT A
−1AT b.
(4.5.7)

214
Chapter 4
Vector Spaces
This follows because a unique solution (to either system) exists if and only if
0 = N (A) = N

AT A

, and this insures (AT A)n×n must be nonsingular (by
(4.2.11)), so (4.5.7) is the unique solution to both systems. Caution!
When
A is not square, A−1 does not exist, and the reverse order law for inversion
doesn’t apply to

AT A
−1, so (4.5.7) cannot be further simpliﬁed.
There is one outstanding question—what do the solutions of the normal
equations AT Ax = AT b represent when the original system Ax = b is not
consistent? The answer, which is of fundamental importance, will have to wait
until §4.6, but let’s summarize what has been said so far.
Normal Equations
•
For an m × n system Ax = b, the associated system of normal
equations is deﬁned to be the n × n system AT Ax = AT b.
•
AT Ax = AT b is always consistent, even when Ax = b is not
consistent.
•
When Ax = b is consistent, its solution set agrees with that of
AT Ax = AT b. As discussed in §4.6, the normal equations provide
least squares solutions to Ax = b when Ax = b is inconsistent.
•
AT Ax = AT b has a unique solution if and only if rank (A) = n,
in which case the unique solution is x =

AT A
−1AT b.
•
When Ax = b is consistent and has a unique solution, then the
same is true for AT Ax = AT b, and the unique solution to both
systems is given by x =

AT A
−1AT b.
Example 4.5.1
Caution!
Use of the product AT A or the normal equations is not recom-
mended for numerical computation. Any sensitivity to small perturbations that
is present in the underlying matrix A is magniﬁed by forming the product
AT A. In other words, if Ax = b is somewhat ill-conditioned, then the asso-
ciated system of normal equations AT Ax = AT b will be ill-conditioned to an
even greater extent, and the theoretical properties surrounding AT A and the
normal equations may be lost in practical applications. For example, consider
the nonsingular system Ax = b, where
A =

3
6
1
2.01

and
b =

9
3.01

.
If Gaussian elimination with 3-digit ﬂoating-point arithmetic is used to solve
Ax = b, then the 3-digit solution is (1, 1), and this agrees with the exact

4.5 More about Rank
215
solution. However if 3-digit arithmetic is used to form the associated system of
normal equations, the result is

10
20
20
40
 
x1
x2

=

30
60.1

.
The 3-digit representation of AT A is singular, and the associated system of
normal equations is inconsistent. For these reasons, the normal equations are
often avoided in numerical computations. Nevertheless, the normal equations
are an important theoretical idea that leads to practical tools of fundamental
importance such as the method of least squares developed in §4.6 and §5.13.
Because the concept of rank is at the heart of our subject, it’s important to
understand rank from a variety of diﬀerent viewpoints. The statement below is
one more way to think about rank.
29
Rank and the Largest Nonsingular Submatrix
The rank of a matrix Am×n is precisely the order of a maximal square
nonsingular submatrix of A. In other words, to say rank (A) = r
means that there is at least one r × r nonsingular submatrix in A,
and there are no nonsingular submatrices of larger order.
Proof.
First demonstrate that there exists an r × r nonsingular submatrix in
A, and then show there can be no nonsingular submatrix of larger order. Begin
with the fact that there must be a maximal linearly independent set of r rows
in A as well as a maximal independent set of r columns, and prove that the
submatrix Mr×r lying on the intersection of these r rows and r columns is
nonsingular. The r independent rows can be permuted to the top, and the
remaining rows can be annihilated using row operations, so
A
row
∼

Ur×n
0

.
Now permute the r independent columns containing M to the left-hand side,
and use column operations to annihilate the remaining columns to conclude that
A
row
∼

Ur×n
0

col
∼

Mr×r
N
0
0

col
∼

Mr×r
0
0
0

.
29
This is the last characterization of rank presented in this text, but historically this was the
essence of the ﬁrst deﬁnition (p. 44) of rank given by Georg Frobenius (p. 662) in 1879.

216
Chapter 4
Vector Spaces
Rank isn’t changed by row or column operations, so r = rank (A) = rank (M),
and thus M is nonsingular. Now suppose that W is any other nonsingu-
lar submatrix of A, and let P and Q be permutation matrices such that
PAQ =
 W
X
Y
Z

. If
E =

I
0
−YW−1
I

,
F =

I
−W−1X
0
I

,
and
S = Z −YW−1X,
then
EPAQF =

W
0
0
S

=⇒
A ∼

W
0
0
S

,
(4.5.8)
and hence r = rank (A) = rank (W) + rank (S) ≥rank (W) (recall Example
3.9.3). This guarantees that no nonsingular submatrix of A can have order
greater than r = rank (A).
Example 4.5.2
Problem: Determine the rank of A =
 1
2
1
2
4
1
3
6
1

.
Solution: rank (A) = 2 because there is at least one 2 × 2 nonsingular sub-
matrix (e.g., there is one lying on the intersection of rows 1 and 2 with columns
2 and 3), and there is no larger nonsingular submatrix (the entire matrix is sin-
gular). Notice that not all 2 × 2 matrices are nonsingular (e.g., consider the one
lying on the intersection of rows 1 and 2 with columns 1 and 2).
Earlier in this section we saw that it is impossible to increase the rank by
means of matrix multiplication—i.e., (4.5.2) says rank (AE) ≤rank (A). In
a certain sense there is a dual statement for matrix addition that says that it
is impossible to decrease the rank by means of a “small” matrix addition—i.e.,
rank (A + E) ≥rank (A) whenever E has entries of small magnitude.
Small Perturbations Can’t Reduce Rank
If A and E are m × n matrices such that E has entries of suﬃciently
small magnitude, then
rank (A + E) ≥rank (A).
(4.5.9)
The term “suﬃciently small” is further clariﬁed in Exercise 5.12.4.

4.5 More about Rank
217
Proof.
Suppose rank (A) = r, and let P and Q be nonsingular matrices
that reduce A to rank normal form—i.e., PAQ =
 Ir
0
0
0

. If P and Q are
applied to E to form PEQ =
 E11
E12
E21
E22

, where E11 is r × r, then
P(A + E)Q =

Ir + E11
E12
E21
E22

.
(4.5.10)
If the magnitude of the entries in E are small enough to insure that Ek
11 →0
as k →∞, then the discussion of the Neumann series on p. 126 insures that
I + E11 is nonsingular. (Exercise 4.5.14 gives another condition on the size of
E11 to insure this.) This allows the right-hand side of (4.5.10) to be further
reduced by writing

I
0
−E21(I + E11)−1 I

I + E11 E12
E21
E22

I
−(I + E11)−1E12
0
I

=

I −E11 0
0
S

,
where S = E22 −E21 (I + E11)−1 E12. In other words,
A + E ∼

I −E11
0
0
S

,
and therefore
rank (A + E) = rank (Ir + E11) + rank (S)
(recall Example 3.9.3)
= rank (A) + rank (S)
≥rank (A).
(4.5.11)
Example 4.5.3
A Pitfall in Solving Singular Systems.
Solving Ax = b with ﬂoating-
point arithmetic produces the exact solution of a perturbed system whose coeﬃ-
cient matrix is A+E. If A is nonsingular, and if we are using a stable algorithm
(an algorithm that insures that the entries in E have small magnitudes), then
(4.5.9) guarantees that we are ﬁnding the exact solution to a nearby system that
is also nonsingular. On the other hand, if A is singular, then perturbations of
even the slightest magnitude can increase the rank, thereby producing a system
with fewer free variables than the original system theoretically demands, so even
a stable algorithm can result in a signiﬁcant loss of information. But what are
the chances that this will actually occur in practice? To answer this, recall from
(4.5.11) that
rank (A + E) = rank (A) + rank (S),
where
S = E22 −E21 (I + E11)−1 E12.

218
Chapter 4
Vector Spaces
If the rank is not to jump, then the perturbation E must be such that S = 0,
which is equivalent to saying E22 = E21 (I + E11)−1 E12. Clearly, this requires
the existence of a very speciﬁc (and quite special) relationship among the entries
of E, and a random perturbation will almost never produce such a relation-
ship. Although rounding errors cannot be considered to be truly random, they
are random enough so as to make the possibility that S = 0 very unlikely.
Consequently, when A is singular, the small perturbation E due to roundoﬀ
makes the possibility that rank (A + E) > rank (A) very likely. The moral is
to avoid ﬂoating-point solutions of singular systems. Singular problems can often
be distilled down to a nonsingular core or to nonsingular pieces, and these are
the components you should be dealing with.
Since no more signiﬁcant characterizations of rank will be given, it is ap-
propriate to conclude this section with a summary of all of the diﬀerent ways we
have developed to say “rank.”
Summary of Rank
For A ∈ℜm×n, each of the following statements is true.
•
rank (A) = The number of nonzero rows in any row echelon form
that is row equivalent to A.
•
rank (A) = The number of pivots obtained in reducing A to a row
echelon form with row operations.
•
rank (A) = The number of basic columns in A (as well as the num-
ber of basic columns in any matrix that is row equivalent
to A ).
•
rank (A) = The number of independent columns in A —i.e., the size
of a maximal independent set of columns from A.
•
rank (A) = The number of independent rows in A —i.e., the size of
a maximal independent set of rows from A.
•
rank (A) = dim R (A).
•
rank (A) = dim R

AT 
.
•
rank (A) = n −dim N (A).
•
rank (A) = m −dim N

AT 
.
•
rank (A) = The size of the largest nonsingular submatrix in A.
For A ∈Cm×n, replace (⋆)T with (⋆)∗.

4.5 More about Rank
219
Exercises for section 4.5
4.5.1. Verify that rank

AT A

= rank (A) = rank

AAT 
for
A =


1
3
1
−4
−1
−3
1
0
2
6
2
−8

.
4.5.2. Determine dim N (A) ∩R (B) for
A =


−2
1
1
−4
2
2
0
0
0


and
B =


1
3
1
−4
−1
−3
1
0
2
6
2
−8

.
4.5.3. For the matrices given in Exercise
4.5.2, use the procedure described
on p. 211 to determine a basis for N (A) ∩R (B).
4.5.4. If A1A2 · · · Ak is a product of square matrices such that some Ai is
singular, explain why the entire product must be singular.
4.5.5. For A ∈ℜm×n, explain why AT A = 0 implies A = 0.
4.5.6. Find rank (A) and all nonsingular submatrices of maximal order in
A =


2
−1
1
4
−2
1
8
−4
1

.
4.5.7. Is it possible that rank (AB) < rank (A) and rank (AB) < rank (B)
for the same pair of matrices?
4.5.8. Is rank (AB) = rank (BA) when both products are deﬁned? Why?
4.5.9. Explain why rank (AB) = rank (A) −dim N

BT 
∩R

AT 
.
4.5.10. Explain why dim N (Am×nBn×p) = dim N (B) + dim R (B) ∩N (A).

220
Chapter 4
Vector Spaces
4.5.11. Sylvester’s law of nullity, given by James J. Sylvester in 1884, states
that for square matrices A and B,
max {ν(A), ν(B)} ≤ν(AB) ≤ν(A) + ν(B),
where ν(⋆) = dim N (⋆) denotes the nullity.
(a)
Establish the validity of Sylvester’s law.
(b)
Show Sylvester’s law is not valid for rectangular matrices be-
cause ν(A) > ν(AB) is possible. Is ν(B) > ν(AB) possible?
4.5.12. For matrices Am×n and Bn×p, prove each of the following statements:
(a) rank (AB) = rank (A) and R (AB) = R (A) if rank (B) = n.
(b) rank (AB) = rank (B) and N (AB) = N (B) if rank (A) = n.
4.5.13. Perform the following calculations using the matrices:
A =


1
2
2
4
1
2.01


and
b =


1
2
1.01

.
(a)
Find rank (A), and solve Ax = b using exact arithmetic.
(b)
Find rank

AT A

, and solve AT Ax=AT b exactly.
(c)
Find rank (A), and solve Ax = b with 3-digit arithmetic.
(d)
Find AT A,
AT b, and the solution of AT Ax = AT b with
3-digit arithmetic.
4.5.14. Prove that if the entries of Fr×r satisfy r
j=1 |fij| < 1 for each i (i.e.,
each absolute row sum < 1), then I + F is nonsingular. Hint: Use the
triangle inequality for scalars |α+β| ≤|α|+|β| to show N (I + F) = 0.
4.5.15. If A =
 W
X
Y
Z

, where rank (A) = r = rank (Wr×r), show that
there are matrices B and C such that
A =

W
WC
BW
BWC

=

I
B

W

I | C

.
4.5.16. For a convergent sequence {Ak}∞
k=1 of matrices, let A = limk→∞Ak.
(a)
Prove that if each Ak is singular, then A is singular.
(b)
If each Ak is nonsingular, must A be nonsingular? Why?

4.5 More about Rank
221
4.5.17. The Frobenius Inequality. Establish the validity of Frobenius’s 1911
result that states that if ABC exists, then
rank (AB) + rank (BC) ≤rank (B) + rank (ABC).
Hint: If M = R (BC)∩N (A) and N = R (B)∩N (A), then M ⊆N.
4.5.18. If A is n × n, prove that the following statements are equivalent:
(a)
N (A) = N

A2
.
(b)
R (A) = R

A2
.
(c)
R (A) ∩N (A) = {0}.
4.5.19. Let A and B be n × n matrices such that A = A2, B = B2, and
AB = BA = 0.
(a)
Prove that rank (A + B) = rank (A) + rank (B). Hint: Con-
sider
 A
B

(A + B)(A | B).
(b)
Prove that rank (A) + rank (I −A) = n.
4.5.20. Moore–Penrose Inverse. For A ∈ℜm×n such that rank (A) = r,
let A = BC be the full rank factorization of A in which Bm×r is the
matrix of basic columns from A and Cr×n is the matrix of nonzero
rows from EA (see Exercise 3.9.8). The matrix deﬁned by
A† = CT 
BT ACT −1 BT
is called the Moore–Penrose
30 inverse of A. Some authors refer to
A† as the pseudoinverse or the generalized inverse of A. A more elegant
treatment is given on p. 423, but it’s worthwhile to introduce the idea
here so that it can be used and viewed from diﬀerent perspectives.
(a)
Explain why the matrix BT ACT is nonsingular.
(b)
Verify that x = A†b solves the normal equations AT Ax = AT b (as
well as Ax = b when it is consistent).
(c)
Show that the general solution for AT Ax = AT b (as well as Ax = b
when it is consistent) can be described as
x = A†b +

I −A†A

h,
30
This is in honor of Eliakim H. Moore (1862–1932) and Roger Penrose (a famous contemporary
English mathematical physicist). Each formulated a concept of generalized matrix inversion—
Moore’s work was published in 1922, and Penrose’s work appeared in 1955. E. H. Moore is
considered by many to be America’s ﬁrst great mathematician.

222
Chapter 4
Vector Spaces
where h is a “free variable” vector in ℜn×1.
Hint: Verify AA†A = A, and then show R

I −A†A

= N (A).
(d)
If rank (A) = n, explain why A† =

AT A
−1AT .
(e)
If A is square and nonsingular, explain why A† = A−1.
(f)
Verify that A† = CT 
BT ACT −1 BT satisﬁes the Penrose equations:
AA†A = A,

AA†T = AA†,
A†AA† = A†,

A†A
T = A†A.
Penrose originally deﬁned A† to be the unique solution to these four
equations.

4.6 Classical Least Squares
223
4.6
CLASSICAL LEAST SQUARES
The following problem arises in almost all areas where mathematics is applied.
At discrete points ti (often points in time), observations bi of some phenomenon
are made, and the results are recorded as a set of ordered pairs
D = {(t1, b1),
(t2, b2), . . . , (tm, bm)} .
On the basis of these observations, the problem is to make estimations or predic-
tions at points (times) ˆt that are between or beyond the observation points ti.
A standard approach is to ﬁnd the equation of a curve y = f(t) that closely ﬁts
the points in D so that the phenomenon can be estimated at any nonobservation
point ˆt with the value ˆy = f(ˆt).
Let’s begin by ﬁtting a straight line to the points in D. Once this is under-
stood, it will be relatively easy to see how to ﬁt the data with curved lines.
f (t)= α + β t
(t1,b1)

t1,f (t1)

(t2,b2)

t2,f (t2)

(tm ,bm )

tm ,f (tm )

t
b
ε1
ε2
εm
•
•
•
•
•
•
•
•
•
•
•
•
Figure 4.6.1
The strategy is to determine the coeﬃcients α and β in the equation of the
line f(t) = α + βt that best ﬁts the points (ti, bi) in the sense that the sum
of the squares of the vertical
31 errors ε1, ε2, . . . , εm indicated in Figure 4.6.1 is
31
We consider only vertical errors because there is a tacit assumption that only the observations
bi are subject to error or variation. The ti ’s are assumed to be errorless constants—think of
them as being exact points in time (as they often are). If the ti ’s are also subject to variation,
then horizontal as well as vertical errors have to be considered in Figure 4.6.1, and a more
complicated theory known as total least squares (not considered in this text) emerges. The
least squares line L obtained by minimizing only vertical deviations will not be the closest
line to points in D in terms of perpendicular distance, but L is the best line for the purpose
of linear estimation—see §5.14 (p. 446).

224
Chapter 4
Vector Spaces
minimal. The distance from (ti, bi) to a line f(t) = α + βt is
εi = |f(ti) −bi| = |α + βti −bi|,
so that the objective is to ﬁnd values for α and β such that
m

i=1
ε2
i =
m

i=1
(α + βti −bi)2
is minimal.
Minimization techniques from calculus tell us that the minimum value must
occur at a solution to the two equations
0 =
∂
m
i=1 (α + βti −bi)2
∂α
= 2
m

i=1
(α + βti −bi) ,
0 =
∂
m
i=1 (α + βti −bi)2
∂β
= 2
m

i=1
(α + βti −bi) ti.
Rearranging terms produces two equations in the two unknowns α and β
 m

i=1
1
 
α +
 m

i=1
ti
 
β =
m

i=1
bi,
 m

i=1
ti
 
α +
 m

i=1
t2
i
 
β =
m

i=1
tibi.
(4.6.1)
By setting
A =




1
t1
1
t2
...
...
1
tm



,
b =




b1
b2
...
bm



,
and
x =

α
β

,
we see that the two equations (4.6.1) have the matrix form AT Ax = AT b.
In other words, (4.6.1) is the system of normal equations associated with the
system Ax = b (see p. 213). The ti ’s are assumed to be distinct numbers,
so rank (A) = 2, and (4.5.7) insures that the normal equations have a unique
solution given by
x =

AT A
−1AT b
=
1
m  t2
i −( ti)2

 t2
i
− ti
− ti
m
   bi
 tibi

=
1
m  t2
i −( ti)2
  t2
i
 bi − ti
 tibi
m  tibi − ti
 bi

=

α
β

.
Finally, notice that the total sum of squares of the errors is given by
m

i=1
ε2
i =
m

i=1
(α + βti −bi)2 = (Ax −b)T (Ax −b).

4.6 Classical Least Squares
225
Example 4.6.1
Problem: A small company has been in business for four years and has recorded
annual sales (in tens of thousands of dollars) as follows.
Year
1
2
3
4
Sales
23
27
30
34
When this data is plotted as shown in Figure 4.6.2, we see that although the
points do not exactly lie on a straight line, there nevertheless appears to be a
linear trend. Predict the sales for any future year if this trend continues.
0
22
23
24
25
26
27
28
29
30
31
32
33
34
4
3
2
1
Year
Sales
Figure 4.6.2
Solution: Determine the line f(t) = α + βt that best ﬁts the data in the sense
of least squares. If
A =



1
1
1
2
1
3
1
4


,
b =



23
27
30
34


,
and
x =

α
β

,
then the previous discussion guarantees that x is the solution of the normal
equations AT Ax = AT b. That is,

4
10
10
30
 
α
β

=

114
303

.
The solution is easily found to be α = 19.5 and β = 3.6, so we predict that the
sales in year t will be f(t) = 19.5 + 3.6t. For example, the estimated sales for
year ﬁve is $375,000. To get a feel for how close the least squares line comes to

226
Chapter 4
Vector Spaces
passing through the data points, let ε = Ax −b, and compute the sum of the
squares of the errors to be
m

i=1
ε2
i = εT ε = (Ax −b)T (Ax −b) = .2.
General Least Squares Problem
For A ∈ℜm×n and b ∈ℜm, let ε = ε(x) = Ax −b. The general
least squares problem is to ﬁnd a vector x that minimizes the quantity
m

i=1
ε2
i = εT ε = (Ax −b)T (Ax −b).
Any vector that provides a minimum value for this expression is called
a least squares solution.
•
The set of all least squares solutions is precisely the set of solutions
to the system of normal equations AT Ax = AT b.
•
There is a unique least squares solution if and only if rank (A) = n,
in which case it is given by x =

AT A
−1AT b.
•
If Ax = b is consistent, then the solution set for Ax = b is the
same as the set of least squares solutions.
Proof.
32 First prove that if x minimizes εT ε, then x must satisfy the normal
equations. Begin by using xT AT b = bT Ax (scalars are symmetric) to write
m

i=1
ε2
i = εT ε = (Ax −b)T (Ax −b) = xT AT Ax −2xT AT b + bT b.
(4.6.2)
To determine vectors x that minimize the expression (4.6.2), we will again use
minimization techniques from calculus and diﬀerentiate the function
f(x1, x2, . . . , xn) = xT AT Ax −2xT AT b + bT b
(4.6.3)
with respect to each xi. Diﬀerentiating matrix functions is similar to diﬀer-
entiating scalar functions (see Exercise 3.5.9) in the sense that if U = [uij],
then
$∂U
∂x
%
ij
= ∂uij
∂x ,
∂[U + V]
∂x
= ∂U
∂x + ∂V
∂x ,
and
∂[UV]
∂x
= ∂U
∂x V + U∂V
∂x .
32
A more modern development not relying on calculus is given in §5.13 on p. 437, but the more
traditional approach is given here because it’s worthwhile to view least squares from both
perspectives.

4.6 Classical Least Squares
227
Applying these rules to the function in (4.6.3) produces
∂f
∂xi
= ∂xT
∂xi
AT Ax + xT AT A ∂x
∂xi
−2∂xT
∂xi
AT b.
Since ∂x/∂xi = ei (the ith unit vector), we have
∂f
∂xi
= eT
i AT Ax + xT AT Aei −2eT
i AT b = 2eT
i AT Ax −2eT
i AT b.
Using eT
i AT =

AT 
i∗and setting ∂f/∂xi = 0 produces the n equations

AT 
i∗Ax =

AT 
i∗b
for i = 1, 2, . . . , n,
which can be written as the single matrix equation AT Ax = AT b. Calculus
guarantees that the minimum value of f occurs at some solution of this system.
But this is not enough—we want to know that every solution of AT Ax = AT b
is a least squares solution. So we must show that the function f in (4.6.3) attains
its minimum value at each solution to AT Ax = AT b. Observe that if z is a
solution to the normal equations, then f(z) = bT b −zT AT b. For any other
y ∈ℜn×1, let u = y −z, so y = z + u, and observe that
f(y) = f(z) + vT v,
where
v = Au.
Since vT v = 
i v2
i ≥0, it follows that f(z) ≤f(y) for all y ∈ℜn×1, and
thus f attains its minimum value at each solution of the normal equations. The
remaining statements in the theorem follow from the properties established on
p. 213.
The classical least squares problem discussed at the beginning of this sec-
tion and illustrated in Example 4.6.1 is part of a broader topic known as linear
regression, which is the study of situations where attempts are made to express
one variable y as a linear combination of other variables t1, t2, . . . , tn. In prac-
tice, hypothesizing that y is linearly related to t1, t2, . . . , tn means that one
assumes the existence of a set of constants {α0, α1, . . . , αn} (called parameters)
such that
y = α0 + α1t1 + α2t2 + · · · + αntn + ε,
where ε is a “random function” whose values “average out” to zero in some
sense. Practical problems almost always involve more variables than we wish to
consider, but it is frequently fair to assume that the eﬀect of variables of lesser
signiﬁcance will indeed “average out” to zero. The random function ε accounts
for this assumption. In other words, a linear hypothesis is the supposition that
the expected (or mean) value of y at each point where the phenomenon can be
observed is given by a linear equation
E(y) = α0 + α1t1 + α2t2 + · · · + αntn.

228
Chapter 4
Vector Spaces
To help seat these ideas, consider the problem of predicting the amount of
weight that a pint of ice cream loses when it is stored at very low temperatures.
There are many factors that may contribute to weight loss—e.g., storage tem-
perature, storage time, humidity, atmospheric pressure, butterfat content, the
amount of corn syrup, the amounts of various gums (guar gum, carob bean gum,
locust bean gum, cellulose gum), and the never-ending list of other additives and
preservatives. It is reasonable to believe that storage time and temperature are
the primary factors, so to predict weight loss we will make a linear hypothesis of
the form
y = α0 + α1t1 + α2t2 + ε,
where y = weight loss (grams), t1 = storage time (weeks), t2 = storage tem-
perature ( oF ), and ε is a random function to account for all other factors. The
assumption is that all other factors “average out” to zero, so the expected (or
mean) weight loss at each point (t1, t2) is
E(y) = α0 + α1t1 + α2t2.
(4.6.4)
Suppose that we conduct an experiment in which values for weight loss are
measured for various values of storage time and temperature as shown below.
Time (weeks)
1
1
1
2
2
2
3
3
3
Temp (oF)
−10
−5
0
−10
−5
0
−10
−5
0
Loss (grams)
.15
.18
.20
.17
.19
.22
.20
.23
.25
If
A =













1
1
−10
1
1
−5
1
1
0
1
2
−10
1
2
−5
1
2
0
1
3
−10
1
3
−5
1
3
0













,
x =


α0
α1
α2

,
and
b =













.15
.18
.20
.17
.19
.22
.20
.23
.25













,
and if we were lucky enough to exactly observe the mean weight loss each time
(i.e., if bi = E(yi) ), then equation (4.6.4) would insure that Ax = b is a
consistent system, so we could solve for the unknown parameters α0, α1, and
α2. However, it is virtually impossible to observe the exact value of the mean
weight loss for a given storage time and temperature, and almost certainly the
system deﬁned by Ax = b will be inconsistent—especially when the number
of observations greatly exceeds the number of parameters. Since we can’t solve
Ax = b to ﬁnd exact values for the αi ’s, the best we can hope for is a set of
“good estimates” for these parameters.

4.6 Classical Least Squares
229
The famous Gauss–Markov theorem (developed on p. 448) states that under
certain reasonable assumptions concerning the random error function ε, the
“best” estimates for the αi ’s are obtained by minimizing the sum of squares
(Ax −b)T (Ax −b). In other words, the least squares estimates are the “best”
way to estimate the αi ’s.
Returning to our ice cream example, it can be veriﬁed that b /∈R (A), so, as
expected, the system Ax = b is not consistent, and we cannot determine exact
values for α0, α1, and α2. The best we can do is to determine least squares esti-
mates for the αi ’s by solving the associated normal equations AT Ax = AT b,
which in this example are


9
18
−45
18
42
−90
−45
−90
375




α0
α1
α2

=


1.79
3.73
−8.2

.
The solution is


α0
α1
α2

=


.174
.025
.005

,
and the estimating equation for mean weight loss becomes
ˆy = .174 + .025t1 + .005t2.
For example, the mean weight loss of a pint of ice cream that is stored for nine
weeks at a temperature of −35oF is estimated to be
ˆy = .174 + .025(9) + .005(−35) = .224 grams.
Example 4.6.2
Least Squares Curve Fitting Problem:
Find a polynomial
p(t) = α0 + α1t + α2t2 + · · · + αn−1tn−1
with a speciﬁed degree that comes as close as possible in the sense of least squares
to passing through a set of data points
D = {(t1, b1),
(t2, b2), . . . , (tm, bm)} ,
where the ti ’s are distinct numbers, and n ≤m.

230
Chapter 4
Vector Spaces
p(t)
(t1,b1)
(t2,b2)

t2,p (t2)


t1,p (t1)

(tm ,bm )

tm ,p (tm )

t
b
ε1
ε2
εm
•
•
•
•
•
•
•
•
•
•
•
•
Figure 4.6.3
Solution: For the εi ’s indicated in Figure 4.6.3, the objective is to minimize
the sum of squares
m

i=1
ε2
i =
m

i=1
(p(ti) −bi)2 = (Ax −b)T (Ax −b),
where
A =




1
t1
t2
1
· · ·
tn−1
1
1
t2
t2
2
· · ·
tn−1
2
...
...
...
· · ·
...
1
tm
t2
m
· · ·
tn−1
m



,
x =




α0
α1
...
αn−1



,
and
b =




b1
b2
...
bm



.
In other words, the least squares polynomial of degree n−1 is obtained from the
least squares solution associated with the system Ax = b. Furthermore, this
least squares polynomial is unique because Am×n is the Vandermonde matrix
of Example 4.3.4 with n ≤m, so rank (A) = n, and Ax = b has a unique
least squares solution given by x =

AT A
−1AT b.
Note: We know from Example 4.3.5 on p. 186 that the Lagrange interpolation
polynomial ℓ(t) of degree m−1 will exactly ﬁt the data—i.e., it passes through
each point in D. So why would one want to settle for a least squares ﬁt when
an exact ﬁt is possible? One answer stems from the fact that in practical work
the observations bi are rarely exact due to small errors arising from imprecise

4.6 Classical Least Squares
231
measurements or from simplifying assumptions. For this reason, it is the trend
of the observations that needs to be ﬁtted and not the observations themselves.
To hit the data points, the interpolation polynomial ℓ(t) is usually forced to
oscillate between or beyond the data points, and as m becomes larger the oscil-
lations can become more pronounced. Consequently, ℓ(t) is generally not useful
in making estimations concerning the trend of the observations—Example 4.6.3
drives this point home. In addition to exactly hitting a prescribed set of data
points, an interpolation polynomial called the Hermite polynomial (p. 607) can
be constructed to have speciﬁed derivatives at each data point. While this helps,
it still is not as good as least squares for making estimations on the basis of
observations.
Example 4.6.3
A missile is ﬁred from enemy territory, and its position in ﬂight is observed by
radar tracking devices at the following positions.
Position down range (miles)
0
250
500
750
1000
Height (miles)
0
8
15
19
20
Suppose our intelligence sources indicate that enemy missiles are programmed
to follow a parabolic ﬂight path—a fact that seems to be consistent with the
diagram obtained by plotting the observations on the coordinate system shown
in Figure 4.6.4.
1000
750
500
250
0
0
5
10
15
20
t = Range
b = Height
Figure 4.6.4
Problem: Predict how far down range the missile will land.

232
Chapter 4
Vector Spaces
Solution:
Determine the parabola f(t) = α0 + α1t + α2t2 that best ﬁts the
observed data in the least squares sense. Then estimate where the missile will
land by determining the roots of f (i.e., determine where the parabola crosses
the horizontal axis). As it stands, the problem will involve numbers having rela-
tively large magnitudes in conjunction with relatively small ones. Consequently,
it is better to ﬁrst scale the data by considering one unit to be 1000 miles. If
A =





1
0
0
1
.25
.0625
1
.5
.25
1
.75
.5625
1
1
1




,
x =


α0
α1
α2

,
and
b =





0
.008
.015
.019
.02




,
and if ε = Ax −b, then the object is to ﬁnd a least squares solution x that
minimizes
5

i=1
ε2
i = εT ε = (Ax −b)T (Ax −b).
We know that such a least squares solution is given by the solution to the system
of normal equations AT Ax = AT b, which in this case is


5
2.5
1.875
2.5
1.875
1.5625
1.875
1.5625
1.3828125




α0
α1
α2

=


.062
.04375
.0349375

.
The solution (rounded to four signiﬁcant digits) is
x =


−2.286 × 10−4
3.983 × 10−2
−1.943 × 10−2

,
and the least squares parabola is
f(t) = −.0002286 + .03983t −.01943t2.
To estimate where the missile will land, determine where this parabola crosses
the horizontal axis by applying the quadratic formula to ﬁnd the roots of f(t)
to be t = .005755 and t = 2.044. Therefore, we estimate that the missile will
land 2044 miles down range. The sum of the squares of the errors associated with
the least squares solution is
5

i=1
ε2
i = εT ε = (Ax −b)T (Ax −b) = 4.571 × 10−7.

4.6 Classical Least Squares
233
Least Squares vs. Lagrange Interpolation. Instead of using least squares,
ﬁt the observations exactly with the fourth-degree Lagrange interpolation poly-
nomial
ℓ(t) = 11
375t +
17
750000t2 −
1
18750000t3 +
1
46875000000t4
described in Example 4.3.5 on p. 186 (you can verify that ℓ(ti) = bi for each
observation). As the graph in Figure 4.6.5 indicates, ℓ(t) has only one real
nonnegative root, so it is worthless for predicting where the missile will land.
This is characteristic of Lagrange interpolation.
y = ℓ(t)
Figure 4.6.5
Computational Note: Theoretically, the least squares solutions of Ax = b
are exactly the solutions of the normal equations AT Ax = AT b, but form-
ing and solving the normal equations to compute least squares solutions with
ﬂoating-point arithmetic is not recommended. As pointed out in Example 4.5.1
on p. 214, any sensitivities to small perturbations that are present in the under-
lying problem are magniﬁed by forming the normal equations. In other words, if
the underlying problem is somewhat ill-conditioned, then the system of normal
equations will be ill-conditioned to an even greater extent. Numerically stable
techniques that avoid the normal equations are presented in Example 5.5.3 on
p. 313 and Example 5.7.3 on p. 346.
Epilogue
While viewing a region in the Taurus constellation on January 1, 1801, Giuseppe
Piazzi, an astronomer and director of the Palermo observatory, observed a small
“star” that he had never seen before. As Piazzi and others continued to watch
this new “star”—which was really an asteroid—they noticed that it was in fact
moving, and they concluded that a new “planet” had been discovered. However,
their new “planet” completely disappeared in the autumn of 1801. Well-known
astronomers of the time joined the search to relocate the lost “planet,” but all
eﬀorts were in vain.

234
Chapter 4
Vector Spaces
In September of 1801 Carl F. Gauss decided to take up the challenge of
ﬁnding this lost “planet.” Gauss allowed for the possibility of an elliptical or-
bit rather than constraining it to be circular—which was an assumption of the
others—and he proceeded to develop the method of least squares. By December
the task was completed, and Gauss informed the scientiﬁc community not only
where the lost “planet” was located, but he also predicted its position at fu-
ture times. They looked, and it was exactly where Gauss had predicted it would
be! The asteroid was named Ceres, and Gauss’s contribution was recognized by
naming another minor asteroid Gaussia.
This extraordinary feat of locating a tiny and distant heavenly body from
apparently insuﬃcient data astounded the scientiﬁc community. Furthermore,
Gauss refused to reveal his methods, and there were those who even accused
him of sorcery. These events led directly to Gauss’s fame throughout the entire
European community, and they helped to establish his reputation as a mathe-
matical and scientiﬁc genius of the highest order.
Gauss waited until 1809, when he published his Theoria Motus Corporum
Coelestium In Sectionibus Conicis Solem Ambientium, to systematically develop
the theory of least squares and his methods of orbit calculation. This was in
keeping with Gauss’s philosophy to publish nothing but well-polished work of
lasting signiﬁcance. When criticized for not revealing more motivational aspects
in his writings, Gauss remarked that architects of great cathedrals do not obscure
the beauty of their work by leaving the scaﬀolds in place after the construction
has been completed. Gauss’s theory of least squares approximation has indeed
proven to be a great mathematical cathedral of lasting beauty and signiﬁcance.
Exercises for section 4.6
4.6.1. Hooke’s law says that the displacement y of an ideal spring is propor-
tional to the force x that is applied—i.e., y = kx for some constant k.
Consider a spring in which k is unknown. Various masses are attached,
and the resulting displacements shown in Figure 4.6.6 are observed. Us-
ing these observations, determine the least squares estimate for k.
x (lb)
y (in)
5
11.1
7
15.4
8
17.5
10
22.0
12
26.3
x
y
Figure 4.6.6

4.6 Classical Least Squares
235
4.6.2. Show that the slope of the line that passes through the origin in ℜ2 and
comes closest in the least squares sense to passing through the points
{(x1, y1), (x2, y2), . . . , (xn, yn)} is given by m = 
i xiyi/ 
i x2
i .
4.6.3. A small company has been in business for three years and has recorded
annual proﬁts (in thousands of dollars) as follows.
Year
1
2
3
Sales
7
4
3
Assuming that there is a linear trend in the declining proﬁts, predict the
year and the month in which the company begins to lose money.
4.6.4. An economist hypothesizes that the change (in dollars) in the price of a
loaf of bread is primarily a linear combination of the change in the price
of a bushel of wheat and the change in the minimum wage. That is, if B
is the change in bread prices, W is the change in wheat prices, and M
is the change in the minimum wage, then B = αW +βM. Suppose that
for three consecutive years the change in bread prices, wheat prices, and
the minimum wage are as shown below.
Year 1
Year 2
Year 3
B
+$1
+$1
+$1
W
+$1
+$2
0$
M
+$1
0$
−$1
Use the theory of least squares to estimate the change in the price of
bread in Year 4 if wheat prices and the minimum wage each fall by $1.
4.6.5. Suppose that a researcher hypothesizes that the weight loss of a pint of
ice cream during storage is primarily a linear function of time. That is,
y = α0 + α1t + ε,
where y = the weight loss in grams, t = the storage time in weeks, and
ε is a random error function whose mean value is 0. Suppose that an
experiment is conducted, and the following data is obtained.
Time (t)
1
2
3
4
5
6
7
8
Loss (y)
.15
.21
.30
.41
.49
.59
.72
.83
(a)
Determine the least squares estimates for the parameters α0
and α1.
(b)
Predict the mean weight loss for a pint of ice cream that is stored
for 20 weeks.

236
Chapter 4
Vector Spaces
4.6.6. After studying a certain type of cancer, a researcher hypothesizes that
in the short run the number (y) of malignant cells in a particular tissue
grows exponentially with time (t). That is, y = α0eα1t. Determine least
squares estimates for the parameters α0 and α1 from the researcher’s
observed data given below.
t (days)
1
2
3
4
5
y (cells)
16
27
45
74
122
Hint: What common transformation converts an exponential function
into a linear function?
4.6.7. Using least squares techniques, ﬁt the following data
x
−5
−4
−3
−2
−1
0
1
2
3
4
5
y
2
7
9
12
13
14
14
13
10
8
4
with a line y = α0 + α1x and then ﬁt the data with a quadratic y =
α0 +α1x+α2x2. Determine which of these two curves best ﬁts the data
by computing the sum of the squares of the errors in each case.
4.6.8. Consider the time (T) it takes for a runner to complete a marathon (26
miles and 385 yards). Many factors such as height, weight, age, previous
training, etc. can inﬂuence an athlete’s performance, but experience has
shown that the following three factors are particularly important:
x1 = Ponderal index =
height (in.)
[weight (lbs.)]
1
3 ,
x2 = Miles run the previous 8 weeks,
x3 = Age (years).
A linear model hypothesizes that the time T (in minutes) is given by
T = α0 + α1x1 + α2x2 + α3x3 + ε, where ε is a random function
accounting for all other factors and whose mean value is assumed to
be zero. On the basis of the ﬁve observations given below, estimate the
expected marathon time for a 43-year-old runner of height 74 in., weight
180 lbs., who has run 450 miles during the previous eight weeks.
T
x1
x2
x3
181
13.1
619
23
193
13.5
803
42
212
13.8
207
31
221
13.1
409
38
248
12.5
482
45
What is your personal predicted mean marathon time?

4.6 Classical Least Squares
237
4.6.9. For A ∈ℜm×n and b ∈ℜm, prove that x2 is a least squares solution
for Ax = b if and only if x2 is part of a solution to the larger system
 Im×m
A
AT
0n×n
  x1
x2

=
 b
0

.
(4.6.5)
Note: It is not uncommon to encounter least squares problems in which
A is extremely large but very sparse (mostly zero entries). For these
situations, the system (4.6.5) will usually contain signiﬁcantly fewer
nonzero entries than the system of normal equations, thereby helping to
overcome the memory requirements that plague these problems. Using
(4.6.5) also eliminates the undesirable need to explicitly form the prod-
uct AT A —recall from Example 4.5.1 that forming AT A can cause
loss of signiﬁcant information.
4.6.10. In many least squares applications, the underlying data matrix Am×n
does not have independent columns—i.e., rank (A) < n —so the corre-
sponding system of normal equations AT Ax = AT b will fail to have
a unique solution. This means that in an associated linear estimation
problem of the form
y = α1t1 + α2t2 + · · · + αntn + ε
there will be inﬁnitely many least squares estimates for the parameters
αi, and hence there will be inﬁnitely many estimates for the mean value
of y at any given point (t1, t2, . . . , tn) —which is clearly an undesirable
situation. In order to remedy this problem, we restrict ourselves to mak-
ing estimates only at those points (t1, t2, . . . , tn) that are in the row
space of A. If
t =




t1
t2
...
tn



∈R

AT 
,
and if
x =




ˆα1
ˆα2
...
ˆαn




is any least squares solution (i.e., AT Ax = AT b ), prove that the esti-
mate deﬁned by
ˆy = tT x =
n

i=1
tiˆαi
is unique in the sense that ˆy is independent of which least squares
solution x is used.

238
Chapter 4
Vector Spaces
4.7
LINEAR TRANSFORMATIONS
The connection between linear functions and matrices is at the heart of our sub-
ject. As explained on p. 93, matrix algebra grew out of Cayley’s observation that
the composition of two linear functions can be represented by the multiplication
of two matrices. It’s now time to look deeper into such matters and to formalize
the connections between matrices, vector spaces, and linear functions deﬁned on
vector spaces. This is the point at which linear algebra, as the study of linear
functions on vector spaces, begins in earnest.
Linear Transformations
Let U and V be vector spaces over a ﬁeld F ( ℜor C for us).
•
A linear transformation from U into V is deﬁned to be a linear
function T mapping U into V. That is,
T(x + y) = T(x) + T(y)
and
T(αx) = αT(x)
(4.7.1)
or, equivalently,
T(αx + y) = αT(x) + T(y) for all x, y ∈U, α ∈F.
(4.7.2)
•
A linear operator on U is deﬁned to be a linear transformation
from U into itself—i.e., a linear function mapping U back into U.
Example 4.7.1
•
The function 0(x) = 0 that maps all vectors in a space U to the zero
vector in another space V is a linear transformation from U into V, and,
not surprisingly, it is called the zero transformation.
•
The function I(x) = x that maps every vector from a space U back to itself
is a linear operator on U. I is called the identity operator on U.
•
For A ∈ℜm×n and x ∈ℜn×1, the function T(x) = Ax is a linear
transformation from ℜn into ℜm because matrix multiplication satisﬁes
A(αx + y) = αAx + Ay. T is a linear operator on ℜn if A is n × n.
•
If W is the vector space of all functions from ℜto ℜ, and if V is the space
of all diﬀerentiable functions from ℜto ℜ, then the mapping D(f) = df/dx
is a linear transformation from V into W because
d(αf + g)
dx
= α df
dx + dg
dx.
•
If V is the space of all continuous functions from ℜinto ℜ, then the
mapping deﬁned by T(f) =
& x
0 f(t)dt is a linear operator on V because
' x
0
[αf(t) + g(t)] dt = α
' x
0
f(t)dt +
' x
0
g(t)dt.

4.7 Linear Transformations
239
•
The rotator Q that rotates vectors u in ℜ2 counterclockwise through an
angle θ, as shown in Figure 4.7.1, is a linear operator on ℜ2 because the
“action” of Q on u can be described by matrix multiplication in the sense
that the coordinates of the rotated vector Q(u) are given by
Q(u) =

x cos θ −y sin θ
x sin θ + y cos θ

=

cos θ
−sin θ
sin θ
cos θ
 
x
y

.
•
The projector P that maps each point v = (x, y, z) ∈ℜ3 to its orthogonal
projection (x, y, 0) in the xy -plane, as depicted in Figure 4.7.2, is a linear
operator on ℜ3 because if u = (u1, u2, u3) and v = (v1, v2, v3), then
P(αu + v)=(αu1+v1, αu2+v2, 0)=α(u1, u2, 0)+(v1, v2, 0)=αP(u)+P(v).
•
The reﬂector R that maps each vector v = (x, y, z) ∈ℜ3 to its reﬂection
R(v) = (x, y, −z) about the xy -plane, as shown in Figure 4.7.3, is a linear
operator on ℜ3.
θ
Q(u) = (x cos θ  -  y sin θ,  x sin θ  +  y cos θ)
u = (x, y)
y = x
P(v)
v
v = (x, y, z)
R(v) = (x, y, -z)
Figure 4.7.1
Figure 4.7.2
Figure 4.7.3
•
Just as the rotator Q is represented by a matrix [Q] =
 cos θ
−sin θ
sin θ
cos θ

, the
projector P and the reﬂector R can be represented by matrices
[P] =


1
0
0
0
1
0
0
0
0


and
[R] =


1
0
0
0
1
0
0
0
−1


in the sense that the “action” of P and R on v = (x, y, z) can be accom-
plished with matrix multiplication using [P] and [R] by writing


1
0
0
0
1
0
0
0
0




x
y
z

=


x
y
0


and


1
0
0
0
1
0
0
0
−1




x
y
z

=


x
y
−z

.

240
Chapter 4
Vector Spaces
It would be wrong to infer from Example 4.7.1 that all linear transformations
can be represented by matrices (of ﬁnite size). For example, the diﬀerential and
integral operators do not have matrix representations because they are deﬁned
on inﬁnite-dimensional spaces. But linear transformations on ﬁnite-dimensional
spaces will always have matrix representations. To see why, the concept of “co-
ordinates” in higher dimensions must ﬁrst be understood.
Recall that if B = {u1, u2, . . . , un} is a basis for a vector space U, then
each v ∈U can be written as v = α1u1 + α2u2 + · · · + αnun. The αi ’s in this
expansion are uniquely determined by v because if v = 
i αiui = 
i βiui,
then 0 = 
i(αi −βi)ui, and this implies αi −βi = 0 (i.e., αi = βi) for each
i because B is an independent set.
Coordinates of a Vector
Let B = {u1, u2, . . . , un} be a basis for a vector space U, and let v ∈U.
The coeﬃcients αi in the expansion v = α1u1 +α2u2 +· · ·+αnun are
called the coordinates of v with respect to B, and, from now on,
[v]B will denote the column vector
[v]B =




α1
α2
...
αn



.
Caution! Order is important. If B′ is a permutation of B, then [v]B′
is the corresponding permutation of [v]B.
From now on, S = {e1, e2, . . . , en} will denote the standard basis of unit
vectors (in natural order) for ℜn (or Cn). If no other basis is explicitly men-
tioned, then the standard basis is assumed. For example, if no basis is mentioned,
and if we write
v =


8
7
4

,
then it is understood that this is the representation with respect to S in the
sense that v = [v]S = 8e1 + 7e2 + 4e3. The standard coordinates of a vector
are its coordinates with respect to S. So, 8, 7, and 4 are the standard coordinates
of v in the above example.
Example 4.7.2
Problem: If v is a vector in ℜ3 whose standard coordinates are

4.7 Linear Transformations
241
v =


8
7
4

,
determine the coordinates of v with respect to the basis
B =


u1 =


1
1
1

, u2 =


1
2
2

, u3 =


1
2
3




.
Solution: The object is to ﬁnd the three unknowns α1, α2, and α3 such that
α1u1 + α2u2 + α3u3 = v. This is simply a 3 × 3 system of linear equations


1
1
1
1
2
2
1
2
3




α1
α2
α3

=


8
7
4


=⇒
[v]B =


α1
α2
α3

=


9
2
−3

.
The general rule for making a change of coordinates is given on p. 252.
Linear transformations possess coordinates in the same way vectors do be-
cause linear transformations from U to V also form a vector space.
Space of Linear Transformations
•
For each pair of vector spaces U and V over F, the set L(U, V) of
all linear transformations from U to V is a vector space over F.
•
Let B = {u1, u2, . . . , un} and B′ = {v1, v2, . . . , vm} be bases for U
and V, respectively, and let Bji be the linear transformation from
U into V deﬁned by Bji(u) = ξjvi, where (ξ1, ξ2, . . . , ξn)T = [u]B.
That is, pick oﬀthe jth coordinate of u, and attach it to vi.
▷
BL = {Bji}i=1...m
j=1...n is a basis for L(U, V).
▷
dim L(U, V) = (dim U) (dim V) .
Proof.
L(U, V) is a vector space because the deﬁning properties on p. 160 are
satisﬁed—details are omitted. Prove BL is a basis by demonstrating that it is a
linearly independent spanning set for L(U, V). To establish linear independence,
suppose 
j,i ηjiBji = 0 for scalars ηji, and observe that for each uk ∈B,
Bji(uk)=

vi
if j = k
0
if j ̸= k =⇒0=
 
j,i
ηjiBji

(uk)=

j,i
ηjiBji(uk)=
m

i=1
ηkivi.
For each k, the independence of B′ implies that ηki = 0 for each i, and thus
BL is linearly independent. To see that BL spans L(U, V), let T ∈L(U, V),

242
Chapter 4
Vector Spaces
and determine the action of T on any u ∈U by using u = n
j=1 ξjuj and
T(uj) = m
i=1 αijvi to write
T(u) = T

n

j=1
ξjuj
 
=
n

j=1
ξjT(uj) =
n

j=1
ξj
m

i=1
αijvi
=

i,j
αijξjvi =

i,j
αijBji(u).
(4.7.3)
This holds for all u ∈U, so T = 
i,j αijBji, and thus BL spans L(U, V).
It now makes sense to talk about the coordinates of T ∈L(U, V) with
respect to the basis BL. In fact, the rule for determining these coordinates is
contained in the proof above, where it was demonstrated that T = 
i,j αijBji
in which the coordinates αij are precisely the scalars in
T(uj) =
m

i=1
αijvi or, equivalently, [T(uj)]B′ =




α1j
α2j
...
αmj



,
j = 1, 2, . . . , n.
This suggests that rather than listing all coordinates αij in a single column
containing mn entries (as we did with coordinate vectors), it’s more logical to
arrange the αij ’s as an m × n matrix in which the jth column contains the
coordinates of T(uj) with respect to B′. These ideas are summarized below.
Coordinate Matrix Representations
Let B = {u1, u2, . . . , un} and B′ = {v1, v2, . . . , vm} be bases for U
and V, respectively. The coordinate matrix of T ∈L(U, V) with
respect to the pair (B, B′) is deﬁned to be the m × n matrix
[T]BB′ =

[T(u1)]B′
 [T(u2)]B′
 · · ·
 [T(un)]B′

.
(4.7.4)
In other words, if T(uj) = α1jv1 + α2jv2 + · · · + αmjvm, then
[T(uj)]B′ =




α1j
α2j
...
αmj



and [T]BB′ =




α11
α12
· · ·
α1n
α21
α22
· · ·
α2n
...
...
...
...
αm1
αm2
· · ·
αmn



.
(4.7.5)
When T is a linear operator on U, and when there is only one basis
involved, [T]B is used in place of [T]BB to denote the (necessarily
square) coordinate matrix of T with respect to B.

4.7 Linear Transformations
243
Example 4.7.3
Problem: If P is the projector deﬁned in Example 4.7.1 that maps each point
v = (x, y, z) ∈ℜ3 to its orthogonal projection P(v) = (x, y, 0) in the xy -plane,
determine the coordinate matrix [P]B with respect to the basis
B =


u1 =


1
1
1

, u2 =


1
2
2

, u3 =


1
2
3




.
Solution: According to (4.7.4), the jth column in [P]B is [P(uj)]B. Therefore,
P(u1) =


1
1
0

= 1u1 + 1u2 −1u3
=⇒
[P(u1)]B =


1
1
−1

,
P(u2) =


1
2
0

= 0u1 + 3u2 −2u3
=⇒
[P(u2)]B =


0
3
−2

,
P(u3) =


1
2
0

= 0u1 + 3u2 −2u3
=⇒
[P(u3)]B =


0
3
−2

,
so that [P]B =


1
0
0
1
3
3
−1
−2
−2

.
Example 4.7.4
Problem: Consider the same problem given in Example 4.7.3, but use diﬀerent
bases—say,
B =


u1 =


1
0
0

,
u2 =


1
1
0

,
u3 =


1
1
1





and
B′ =


v1 =


−1
0
0

,
v2 =


0
1
0

,
v3 =


0
1
−1




.
For the projector deﬁned by P(x, y, z) = (x, y, 0), determine [P]BB′.
Solution:
Determine the coordinates of each P(uj) with respect to B′, as

244
Chapter 4
Vector Spaces
shown below:
P(u1) =


1
0
0

= −1v1 + 0v2 + 0v3
=⇒
[P(u1)]B′ =


−1
0
0

,
P(u2) =


1
1
0

= −1v1 + 1v2 + 0v3
=⇒
[P(u2)]B′ =


−1
1
0

,
P(u3) =


1
1
0

= −1v1 + 1v2 + 0v3
=⇒
[P(u3)]B′ =


−1
1
0

.
Therefore, according to (4.7.4), [P]BB′ =
 −1
−1
−1
0
1
1
0
0
0

.
At the heart of linear algebra is the realization that the theory of ﬁnite-
dimensional linear transformations is essentially the same as the theory of ma-
trices. This is due primarily to the fundamental fact that the action of a linear
transformation T on a vector u is precisely matrix multiplication between the
coordinates of T and the coordinates of u.
Action as Matrix Multiplication
Let T ∈L(U, V), and let B and B′ be bases for U and V, respectively.
For each u ∈U, the action of T on u is given by matrix multiplication
between their coordinates in the sense that
[T(u)]B′ = [T]BB′[u]B.
(4.7.6)
Proof.
Let B = {u1, u2, . . . , un} and B′ = {v1, v2, . . . , vm} . If u = n
j=1 ξjuj
and T(uj) = m
i=1 αijvi, then
[u]B =




ξ1
ξ2
...
ξn




and [T]BB′ =




α11
α12
· · ·
α1n
α21
α22
· · ·
α2n
...
...
...
...
αm1
αm2
· · ·
αmn



,
so, according to (4.7.3),
T(u) =

i,j
αijξjvi =
m

i=1

n

j=1
αijξj
 
vi.

4.7 Linear Transformations
245
In other words, the coordinates of T(u) with respect to B′ are the terms
n
j=1 αijξj for i = 1, 2, . . . , m, and therefore
[T(u)]B′ =






j α1jξj

j α2jξj
...

j αmjξj




=




α11
α12
· · ·
α1n
α21
α22
· · ·
α2n
...
...
...
...
αm1
αm2
· · ·
αmn








ξ1
ξ2
...
ξn



= [T]BB′[u]B.
Example 4.7.5
Problem: Show how the action of the operator D

p(t)

= dp/dt on the space
P3 of polynomials of degree three or less is given by matrix multiplication.
Solution: The coordinate matrix of D with respect to the basis B = {1, t, t2, t3}
is
[D]B =



0
1
0
0
0
0
2
0
0
0
0
3
0
0
0
0


.
If p = p(t) = α0 + α1t + α2t2 + α3t3, then D(p) = α1 + 2α2t + 3α3t2 so that
[p]B =



α0
α1
α2
α3



and
[D(p)]B =



α1
2α2
3α3
0


.
The action of D is accomplished by means of matrix multiplication because
[D(p)]B =



α1
2α2
3α3
0


=



0
1
0
0
0
0
2
0
0
0
0
3
0
0
0
0






α0
α1
α2
α3


= [D]B[p]B.
For T ∈L(U, V) and L ∈L(V, W), the composition of L with T is
deﬁned to be the function C : U →W such that C(x) = L

T(x)

, and this
composition, denoted by C = LT, is also a linear transformation because
C(αx + y) = L

T(αx + y)

= L

αT(x) + T(y)

= αL

T(x)

+ L

T(y)

= αC(x) + C(y).
Consequently, if B,
B′, and B′′ are bases for U,
V, and W, respectively,
then C must have a coordinate matrix representation with respect to (B, B′′),
so it’s only natural to ask how [C]BB′′ is related to [L]B′B′′ and [T]BB′. Re-
call that the motivation behind the deﬁnition of matrix multiplication given on
p. 93 was based on the need to represent the composition of two linear trans-
formations, so it should be no surprise to discover that [C]BB′′ = [L]B′B′′[T]BB′.
This, along with the other properties given below, makes it clear that studying
linear transformations on ﬁnite-dimensional spaces amounts to studying matrix
algebra.

246
Chapter 4
Vector Spaces
Connections with Matrix Algebra
•
If T, L ∈L(U, V), and if B and B′ are bases for U and V, then
▷
[αT]BB′ = α[T]BB′ for scalars α,
(4.7.7)
▷
[T + L]BB′ = [T]BB′ + [L]BB′.
(4.7.8)
•
If T ∈L(U, V) and L ∈L(V, W), and if B, B′, and B′′ are bases
for U, V, and W, respectively, then LT ∈L(U, W), and
▷
[LT]BB′′ = [L]B′B′′[T]BB′.
(4.7.9)
•
If T ∈L(U, U) is invertible in the sense that TT−1 = T−1T = I
for some T−1 ∈L(U, U), then for every basis B of U,
▷
[T−1]B = [T]−1
B .
(4.7.10)
Proof.
The ﬁrst three properties (4.7.7)–(4.7.9) follow directly from (4.7.6). For
example, to prove (4.7.9), let u be any vector in U, and write
[LT]BB′′[u]B =
"
LT(u)
#
B′′ =
"
L

T(u)
#
B′′ =[L]B′B′′"
T(u)
#
B′ =[L]B′B′′[T]BB′[u]B.
This is true for all u ∈U, so [LT]BB′′ = [L]B′B′′[T]BB′ (see Exercise 3.5.5).
Proving (4.7.7) and (4.7.8) is similar—details are omitted. To prove (4.7.10),
note that if dim U = n, then [I]B = In for all bases B, so property (4.7.9)
implies In = [I]B = [TT−1]B = [T]B[T−1]B, and thus [T−1]B = [T]−1
B .
Example 4.7.6
Problem: Form the composition C = LT of the two linear transformations
T : ℜ3 →ℜ2 and L : ℜ2 →ℜ2 deﬁned by
T(x, y, z) = (x + y, y −z)
and
L(u, v) = (2u −v, u),
and then verify (4.7.9) and (4.7.10) using the standard bases S2 and S3 for ℜ2
and ℜ3, respectively.
Solution: The composition C : ℜ3 →ℜ2 is the linear transformation
C(x, y, z) = L

T(x, y, z)

= L(x + y, y −z) = (2x + y + z, x + y).
The coordinate matrix representations of C, L, and T are
[C]S3S2 =

2
1
1
1
1
0

,
[L]S2 =

2
−1
1
0

,
and
[T]S3S2 =

1
1
0
0
1
−1

.

4.7 Linear Transformations
247
Property (4.7.9) is veriﬁed because [LT]S3S2 = [C]S3S2 = [L]S2[T]S3S2. Find
L−1 by looking for scalars βij in L−1(u, v) = (β11u + β12v, β21u + β22v) such
that LL−1 = L−1L = I or, equivalently,
L

L−1(u, v)

= L−1
L(u, v)

= (u, v)
for all (u, v) ∈ℜ2.
Computation reveals L−1(u, v) = (v, 2v −u), and (4.7.10) is veriﬁed by noting
[L−1]S2 =

0
1
−1
2

=

2
−1
1
0
−1
= [L]−1
S2 .
Exercises for section 4.7
4.7.1. Determine which of the following functions are linear operators on ℜ2.
(a)
T(x, y) = (x, 1 + y),
(b)
T(x, y) = (y, x),
(c)
T(x, y) = (0, xy),
(d)
T(x, y) = (x2, y2),
(e)
T(x, y) = (x, sin y),
(f)
T(x, y) = (x + y, x −y).
4.7.2. For A ∈ℜn×n, determine which of the following functions are linear
transformations.
(a)
T(Xn×n) = AX −XA,
(b)
T(xn×1) = Ax + b for b ̸= 0,
(c)
T(A) = AT ,
(d)
T(Xn×n) = (X + XT )/2.
4.7.3. Explain why T(0) = 0 for every linear transformation T.
4.7.4. Determine which of the following mappings are linear operators on Pn,
the vector space of polynomials of degree n or less.
(a)
T = ξkDk + ξk−1Dk−1 + · · · + ξ1D + ξ0I, where Dk is the
kth-order diﬀerentiation operator (i.e., Dkp(t) = dkp/dtk).
(b)
T

p(t)

= tnp′(0) + t.
4.7.5. Let v be a ﬁxed vector in ℜn×1 and let T : ℜn×1 →ℜbe the mapping
deﬁned by T(x) = vTx (i.e., the standard inner product).
(a)
Is T a linear operator?
(b)
Is T a linear transformation?
4.7.6. For the operator T : ℜ2 →ℜ2 deﬁned by T(x, y) = (x + y, −2x + 4y),
determine [T]B, where B is the basis B =
( 1
1

,
 1
2
)
.

248
Chapter 4
Vector Spaces
4.7.7. Let T : ℜ2 →ℜ3 be the linear transformation deﬁned by
T(x, y) = (x + 3y, 0, 2x −4y).
(a)
Determine [T]SS′, where S and S′ are the standard bases for
ℜ2 and ℜ3, respectively.
(b)
Determine [T]SS′′, where S′′ is the basis for ℜ3 obtained by
permuting the standard basis according to S′′ = {e3, e2, e1}.
4.7.8. Let T be the operator on ℜ3 deﬁned by T(x, y, z) = (x−y, y−x, x−z)
and consider the vector
v =


1
1
2


and the basis
B =





1
0
1

,


0
1
1

,


1
1
0




.
(a)
Determine [T]B and [v]B.
(b)
Compute [T(v)]B, and then verify that [T]B[v]B = [T(v)]B.
4.7.9. For A ∈ℜn×n, let T be the linear operator on ℜn×1 deﬁned by
T(x) = Ax. That is, T is the operator deﬁned by matrix multiplica-
tion. With respect to the standard basis S, show that [T]S = A.
4.7.10. If T is a linear operator on a space V with basis B, explain why
[Tk]B = [T]k
B for all nonnegative integers k.
4.7.11. Let P be the projector that maps each point v ∈ℜ2 to its orthogonal
projection on the line y = x as depicted in Figure 4.7.4.
y = x
P(v)
v
Figure 4.7.4
(a)
Determine the coordinate matrix of P with respect to the stan-
dard basis.
(b)
Determine the orthogonal projection of v =
 α
β

onto the line
y = x.

4.7 Linear Transformations
249
4.7.12. For the standard basis S =

1
0
0
0

,

0
1
0
0

,

0
0
1
0

,

0
0
0
1

of ℜ2×2, determine the matrix representation [T]S for each of the fol-
lowing linear operators on ℜ2×2, and then verify [T(U)]S = [T]S[U]S
for U =
 a
b
c
d

.
(a)
T(X2×2) = X + XT
2
.
(b)
T(X2×2) = AX −XA, where A =

1
1
−1
−1

.
4.7.13. For P2 and P3 (the spaces of polynomials of degrees less than or
equal to two and three, respectively), let S : P2 →P3 be the linear
transformation deﬁned by S(p) =
& t
0 p(x)dx. Determine [S]BB′, where
B = {1, t, t2} and B′ = {1, t, t2, t3}.
4.7.14. Let Q be the linear operator on ℜ2 that rotates each point counter-
clockwise through an angle θ, and let R be the linear operator on ℜ2
that reﬂects each point about the x -axis.
(a)
Determine the matrix of the composition [RQ]S relative to the
standard basis S.
(b)
Relative to the standard basis, determine the matrix of the lin-
ear operator that rotates each point in ℜ2 counterclockwise
through an angle 2θ.
4.7.15. Let P : U →V and Q : U →V be two linear transformations, and let
B and B′ be arbitrary bases for U and V, respectively.
(a)
Provide the details to explain why [P+Q]BB′ = [P]BB′+[Q]BB′.
(b)
Provide the details to explain why [αP]BB′ = α[P]BB′, where
α is an arbitrary scalar.
4.7.16. Let I be the identity operator on an n -dimensional space V.
(a)
Explain why
[I]B =




1
0
· · ·
0
0
1
· · ·
0
...
...
...
...
0
0
· · ·
1




regardless of the choice of basis B.
(b)
Let B = {xi}n
i=1 and B′ = {yi}n
i=1 be two diﬀerent bases for
V, and let T be the linear operator on V that maps vectors
from B′ to vectors in B according to the rule T(yi) = xi for
i = 1, 2, . . . , n. Explain why
[I]BB′ = [T]B = [T]B′ =

[x1]B′
 [x2]B′
 · · ·
 [xn]B′

.

250
Chapter 4
Vector Spaces
(c)
When V = ℜ3, determine [I]BB′ for
B =





1
0
0

,


0
1
0

,


0
0
1




,
B′ =





1
0
0

,


1
1
0

,


1
1
1




.
4.7.17. Let T : ℜ3 →ℜ3 be the linear operator deﬁned by
T(x, y, z) = (2x −y, −x + 2y −z, z −y).
(a)
Determine T−1(x, y, z).
(b)
Determine [T−1]S, where S is the standard basis for ℜ3.
4.7.18. Let T be a linear operator on an n -dimensional space V. Show that
the following statements are equivalent.
(1)
T−1 exists.
(2)
T is a one-to-one mapping (i.e., T(x) = T(y)
=⇒
x = y ).
(3)
N (T) = {0}.
(4)
T is an onto mapping (i.e., for each v ∈V, there is an x ∈V
such that T(x) = v ).
Hint:
Show that (1) =⇒(2) =⇒(3) =⇒(4) =⇒(2),
and then show (2) and (4)
=⇒
(1).
4.7.19. Let V be an n -dimensional space with a basis B = {ui}n
i=1.
(a)
Prove that a set of vectors {x1, x2, . . . , xr} ⊆V is linearly
independent if and only if the set of coordinate vectors
(
[x1]B, [x2]B, . . . , [xr]B
)
⊆ℜn×1
is a linearly independent set.
(b)
If T is a linear operator on V, then the range of T is the set
R (T) = {T(x) | x ∈V}.
Suppose that the basic columns of [T]B
occur in positions
b1, b2, . . . , br. Explain why

T(ub1), T(ub2), . . . , T(ubr)

is a
basis for R (T).

4.8 Change of Basis and Similarity
251
4.8
CHANGE OF BASIS AND SIMILARITY
By their nature, coordinate matrix representations are basis dependent. However,
it’s desirable to study linear transformations without reference to particular bases
because some bases may force a coordinate matrix representation to exhibit
special properties that are not present in the coordinate matrix relative to other
bases. To divorce the study from the choice of bases it’s necessary to somehow
identify properties of coordinate matrices that are invariant among all bases—
these are properties intrinsic to the transformation itself, and they are the ones
on which to focus. The purpose of this section is to learn how to sort out these
basis-independent properties.
The discussion is limited to a single ﬁnite-dimensional space V and to linear
operators on V. Begin by examining how the coordinates of v ∈V change as
the basis for V changes. Consider two diﬀerent bases
B = {x1, x2, . . . , xn}
and
B′ = {y1, y2, . . . , yn} .
It’s convenient to regard B as an old basis for V and B′ as a new basis for V.
Throughout this section T will denote the linear operator such that
T(yi) = xi for i = 1, 2, . . . , n.
(4.8.1)
T is called the change of basis operator because it maps the new basis vectors
in B′ to the old basis vectors in B. Notice that [T]B = [T]B′ = [I]BB′. To see
this, observe that
xi =
n

j=1
αjyj
=⇒
T(xi) =
n

j=1
αjT(yj) =
n

j=1
αjxj,
which means [xi]B′ = [T(xi)]B , so, according to (4.7.4),
[T]B =

[T(x1)]B [T(x2)]B · · · [T(xn)]B

=

[x1]B′ [x2]B′ · · · [xn]B′

= [T]B′.
The fact that [I]BB′ = [T]B follows because [I(xi)]B′ = [xi]B′ . The matrix
P = [I]BB′ = [T]B = [T]B′
(4.8.2)
will hereafter be referred to as a change of basis matrix. Caution! [I]BB′ is
not necessarily the identity matrix—see Exercise 4.7.16—and [I]BB′ ̸= [I]B′B.
We are now in a position to see how the coordinates of a vector change as
the basis for the underlying space changes.

252
Chapter 4
Vector Spaces
Changing Vector Coordinates
Let B = {x1, x2, . . . , xn} and B′ = {y1, y2, . . . , yn} be bases for V,
and let T and P be the associated change of basis operator and change
of basis matrix, respectively—i.e., T(yi) = xi, for each i, and
P = [T]B = [T]B′ = [I]BB′ =

[x1]B′
 [x2]B′
 · · ·
 [xn]B′

.
(4.8.3)
•
[v]B′ = P[v]B for all v ∈V.
(4.8.4)
•
P is nonsingular.
•
No other matrix can be used in place of P in (4.8.4).
Proof.
Use (4.7.6) to write [v]B′ = [I(v)]B′ = [I]BB′[v]B = P[v]B, which is
(4.8.4). P is nonsingular because T is invertible (in fact, T−1(xi) = yi), and
because (4.7.10) insures [T−1]B = [T]−1
B
= P−1. P is unique because if W is
another matrix satisfying (4.8.4) for all v ∈V, then (P −W)[v]B = 0 for all
v. Taking v = xi yields (P −W)ei = 0 for each i, so P −W = 0.
If we think of B as the old basis and B′ as the new basis, then the change
of basis operator T acts as
T(new basis) = old basis,
while the change of basis matrix P acts as
new coordinates = P(old coordinates).
For this reason, T should be referred to as the change of basis operator from
B′ to B, while P is called the change of basis matrix from B to B′.
Example 4.8.1
Problem: For the space P2 of polynomials of degree 2 or less, determine the
change of basis matrix P from B to B′, where
B = {1, t, t2}
and
B′ = {1, 1 + t, 1 + t + t2},
and then ﬁnd the coordinates of q(t) = 3 + 2t + 4t2 relative to B′.
Solution: According to (4.8.3), the change of basis matrix from B to B′ is
P =

[x1]B′
 [x2]B′
 [x3]B′

.

4.8 Change of Basis and Similarity
253
In this case, x1 = 1,
x2 = t, and x3 = t2, and y1 = 1,
y2 = 1 + t, and
y3 = 1 + t + t2, so the coordinates [xi]B′ are computed as follows:
1 =
1(1) + 0(1 + t) + 0(1 + t + t2) =
1y1 + 0y2 + 0y3,
t = −1(1) + 1(1 + t) + 0(1 + t + t2) = −1y1 + 1y2 + 0y3,
t2 =
0(1) −1(1 + t) + 1(1 + t + t2) =
0y1 −1y2 + 1y3.
Therefore,
P =

[x1]B′
 [x2]B′
 [x3]B′

=


1
−1
0
0
1
−1
0
0
1

,
and the coordinates of q = q(t) = 3 + 2t + 4t2 with respect to B′ are
[q]B′ = P[q]B =


1
−1
0
0
1
−1
0
0
1




3
2
4

=


1
−2
4

.
To independently check that these coordinates are correct, simply verify that
q(t) = 1(1) −2(1 + t) + 4(1 + t + t2).
It’s now rather easy to describe how the coordinate matrix of a linear oper-
ator changes as the underlying basis changes.
Changing Matrix Coordinates
Let A be a linear operator on V, and let B and B′ be two bases for
V. The coordinate matrices [A]B and [A]B′ are related as follows.
[A]B = P−1[A]B′P,
where
P = [I]BB′
(4.8.5)
is the change of basis matrix from B to B′. Equivalently,
[A]B′ = Q−1[A]BQ,
where
Q = [I]B′B = P−1
(4.8.6)
is the change of basis matrix from B′ to B.

254
Chapter 4
Vector Spaces
Proof.
Let B = {x1, x2, . . . , xn} and B′ = {y1, y2, . . . , yn} , and observe that
for each j, (4.7.6) can be used to write
*
A(xj)
+
B′ = [A]B′ [xj]B′ = [A]B′P∗j =
*
[A]B′P
+
∗j.
Now use the change of coordinates rule (4.8.4) together with the fact that
"
A(xj)
#
B =
"
[A]B
#
∗j (see (4.7.4)) to write
*
A(xj)
+
B′ = P
*
A(xj)
+
B = P
*
[A]B
+
∗j =
*
P[A]B
+
∗j.
Consequently,
"
[A]B′P
#
∗j =
"
P[A]B
#
∗j for each j, so [A]B′P = P[A]B. Since
the change of basis matrix P is nonsingular, it follows that [A]B = P−1[A]B′P,
and (4.8.5) is proven. Setting Q = P−1 in (4.8.5) yields [A]B′ = Q−1[A]BQ.
The matrix Q = P−1 is the change of basis matrix from B′ to B because if T
is the change of basis operator from B′ to B (i.e., T(yi) = xi ), then T−1 is
the change of basis operator from B to B′ (i.e., T−1(xi) = yi ), and according
to (4.8.3), the change of basis matrix from B′ to B is
[I]B′B =

[y1]B
 [y2]B
 · · ·
 [yn]B

= [T−1]B = [T]−1
B
= P−1 = Q.
Example 4.8.2
Problem: Consider the linear operator A(x, y) = (y, −2x + 3y) on ℜ2 along
with the two bases
S =

1
0

,

0
1

and
S′ =

1
1

,

1
2

.
First compute the coordinate matrix [A]S as well as the change of basis matrix
Q from S′ to S, and then use these two matrices to determine [A]S′.
Solution: The matrix of A relative to S is obtained by computing
A(e1) =A(1, 0) = (0, −2) = (0)e1 + (−2)e2,
A(e2) =A(0, 1) = (1, 3) = (1)e1 + (3)e2,
so that [A]S =

[A(e1)]S
 [A(e2)]S

=

0
1
−2
3

. According to (4.8.6), the
change of basis matrix from S′ to S is
Q =

[y1]S
 [y2]S

=

1
1
1
2

,

4.8 Change of Basis and Similarity
255
and the matrix of A with respect to S′ is
[A]S′ = Q−1[A]SQ =

2
−1
−1
1
 
0
1
−2
3
 
1
1
1
2

=

1
0
0
2

.
Notice that [A]S′ is a diagonal matrix, whereas [A]S is not. This shows that
the standard basis is not always the best choice for providing a simple matrix
representation. Finding a basis so that the associated coordinate matrix is as
simple as possible is one of the fundamental issues of matrix theory. Given an
operator A, the solution to the general problem of determining a basis B so
that [A]B is diagonal is summarized on p. 520.
Example 4.8.3
Problem: Consider a matrix Mn×n to be a linear operator on ℜn by deﬁning
M(v) = Mv (matrix–vector multiplication). If S is the standard basis for ℜn,
and if S′ = {q1, q2, . . . , qn} is any other basis, describe [M]S and [M]S′.
Solution: The jth column in [M]S is [Mej]S = [M∗j]S = M∗j, and hence
[M]S = M. That is, the coordinate matrix of M with respect to S is M itself.
To ﬁnd [M]S′, use (4.8.6) to write [M]S′ = Q−1[M]SQ = Q−1MQ, where
Q = [I]S′S =

[q1]S
 [q2]S
 · · ·
 [qn]S

=

q1
 q2
 · · ·
 qn

.
Conclusion: The matrices M and Q−1MQ represent the same linear operator
(namely, M), but with respect to two diﬀerent bases (namely, S and S′). So,
when considering properties of M (as a linear operator), it’s legitimate to replace
M by Q−1MQ. Whenever the structure of M obscures its operator properties,
look for a basis S′ = {Q∗1, Q∗2, . . . , Q∗n} (or, equivalently, a nonsingular matrix
Q) such that Q−1MQ has a simpler structure. This is an important theme
throughout linear algebra and matrix theory.
For a linear operator A, the special relationships between [A]B and [A]B′
that are given in (4.8.5) and (4.8.6) motivate the following deﬁnitions.
Similarity
•
Matrices Bn×n and Cn×n are said to be similar matrices when-
ever there exists a nonsingular matrix Q such that B = Q−1CQ.
We write B ≃C to denote that B and C are similar.
•
The linear operator f : ℜn×n →ℜn×n deﬁned by f(C) = Q−1CQ
is called a similarity transformation.

256
Chapter 4
Vector Spaces
Equations (4.8.5) and (4.8.6) say that any two coordinate matrices of a
given linear operator must be similar. But must any two similar matrices be
coordinate matrices of the same linear operator? Yes, and here’s why. Suppose
C = Q−1BQ, and let A(v) = Bv be the linear operator deﬁned by matrix–
vector multiplication. If S is the standard basis, then it’s straightforward to see
that [A]S = B (Exercise 4.7.9). If B′ = {Q∗1, Q∗2, . . . , Q∗n} is the basis con-
sisting of the columns of Q, then (4.8.6) insures that [A]B′ = [I]−1
B′S[A]S[I]B′S,
where
[I]B′S =

[Q∗1]S
 [Q∗2]S
 · · ·
 [Q∗n]S

= Q.
Therefore, B = [A]S and C = Q−1BQ = Q−1[A]SQ = [A]B′, so B and
C are both coordinate matrix representations of A. In other words, similar
matrices represent the same linear operator.
As stated at the beginning of this section, the goal is to isolate and study
coordinate-independent properties of linear operators. They are the ones de-
termined by sorting out those properties of coordinate matrices that are ba-
sis independent. But, as (4.8.5) and (4.8.6) show, all coordinate matrices for a
given linear operator must be similar, so the coordinate-independent properties
are exactly the ones that are similarity invariant (invariant under similarity
transformations). Naturally, determining and studying similarity invariants is an
important part of linear algebra and matrix theory.
Example 4.8.4
Problem: The trace of a square matrix C was deﬁned in Example 3.3.1 to be
the sum of the diagonal entries
trace (C) =

i
cii.
Show that trace is a similarity invariant, and explain why it makes sense to talk
about the trace of a linear operator without regard to any particular basis. Then
determine the trace of the linear operator on ℜ2 that is deﬁned by
A(x, y) = (y, −2x + 3y).
(4.8.7)
Solution: As demonstrated in Example 3.6.5, trace (BC) = trace (CB), when-
ever the products are deﬁned, so
trace

Q−1CQ

= trace

CQQ−1
= trace (C),
and thus trace is a similarity invariant. This allows us to talk about the trace of
a linear operator A without regard to any particular basis because trace ([A]B)
is the same number regardless of the choice of B. For example, two coordinate
matrices of the operator A in (4.8.7) were computed in Example 4.8.2 to be
[A]S =

0
1
−2
3

and
[A]S′ =

1
0
0
2

,
and it’s clear that trace ([A]S) = trace ([A]S′) = 3. Since trace ([A]B) = 3 for
all B, we can legitimately deﬁne trace (A) = 3.

4.8 Change of Basis and Similarity
257
Exercises for section 4.8
4.8.1. Explain why rank is a similarity invariant.
4.8.2. Explain why similarity is transitive in the sense that A ≃B and B ≃C
implies A ≃C.
4.8.3. A(x, y, z) = (x + 2y −z, −y, x + 7z) is a linear operator on ℜ3.
(a)
Determine [A]S, where S is the standard basis.
(b)
Determine [A]S′ as well as the nonsingular matrix Q such that
[A]S′ = Q−1[A]SQ for S′ =
 1
0
0

,
 1
1
0

,
 1
1
1

.
4.8.4. Let A =
 1
2
0
3
1
4
0
1
5

and B =
 1
1
1

,
 1
2
2

,
 1
2
3

. Consider A
as a linear operator on ℜn×1 by means of matrix multiplication A(x) =
Ax, and determine [A]B.
4.8.5. Show that C =
 4
6
3
4

and B =
 −2
−3
6
10

are similar matrices, and
ﬁnd a nonsingular matrix Q such that C = Q−1BQ. Hint: Consider
B as a linear operator on ℜ2, and compute [B]S and [B]S′, where S
is the standard basis, and S′ =
(
2
−1

,
 −3
2
)
.
4.8.6. Let T be the linear operator T(x, y) = (−7x −15y, 6x + 12y). Find
a basis B such that [T]B =
 2
0
0
3

, and determine a matrix Q such
that [T]B = Q−1[T]SQ, where S is the standard basis.
4.8.7. By considering the rotator P(x, y) = (x cos θ −y sin θ, x sin θ + y cos θ)
described in Example 4.7.1 and Figure 4.7.1, show that the matrices
R =

cos θ
−sin θ
sin θ
cos θ

and
D =

eiθ
0
0
e−iθ

are similar over the complex ﬁeld. Hint: In case you have forgotten (or
didn’t know), eiθ = cos θ + i sin θ.

258
Chapter 4
Vector Spaces
4.8.8. Let λ be a scalar such that (C −λI)n×n is singular.
(a)
If B ≃C, prove that (B −λI) is also singular.
(b)
Prove that (B −λiI) is singular whenever Bn×n is similar to
D =




λ1
0
· · ·
0
0
λ2
· · ·
0
...
...
...
...
0
0
· · ·
λn



.
4.8.9. If A ≃B, show that Ak ≃Bk for all nonnegative integers k.
4.8.10. Suppose B = {x1, x2, . . . , xn} and B′ = {y1, y2, . . . , yn} are bases for
an n -dimensional subspace V ⊆ℜm×1, and let Xm×n and Ym×n be
the matrices whose columns are the vectors from B and B′, respectively.
(a)
Explain why YT Y is nonsingular, and prove that the change
of basis matrix from B to B′ is P =

YT Y
−1YT X.
(b)
Describe P when m = n.
4.8.11.
(a)
N is nilpotent of index k when Nk = 0 but Nk−1 ̸= 0. If N
is a nilpotent operator of index n on ℜn, and if Nn−1(y) ̸= 0,
show B =

y, N(y), N2(y), . . . , Nn−1(y)

is a basis for ℜn,
and then demonstrate that
[N]B = J =






0
0
· · ·
0
0
1
0
· · ·
0
0
0
1
· · ·
0
0
...
...
...
...
...
0
0
· · ·
1
0






.
(b)
If A and B are any two n × n nilpotent matrices of index n,
explain why A ≃B.
(c)
Explain why all n × n nilpotent matrices of index n must have
a zero trace and be of rank n −1.
4.8.12. E is idempotent when E2 = E. For an idempotent operator E on ℜn,
let X = {xi}r
i=1 and Y = {yi}n−r
i=1
be bases for R (E) and N (E),
respectively.
(a)
Prove that B = X ∪Y is a basis for ℜn. Hint: Show Exi = xi
and use this to deduce that B is linearly independent.
(b)
Show that [E]B =
 Ir
0
0
0

.
(c)
Explain why two n × n idempotent matrices of the same rank
must be similar.
(d)
If F is an idempotent matrix, prove that rank (F) = trace (F).

4.9 Invariant Subspaces
259
4.9
INVARIANT SUBSPACES
For a linear operator T on a vector space V, and for X ⊆V,
T(X) = {T(x) | x ∈X}
is the set of all possible images of vectors from X under the transformation T.
Notice that T(V) = R (T). When X is a subspace of V, it follows that T(X)
is also a subspace of V, but T(X) is usually not related to X. However, in
some special cases it can happen that T(X) ⊆X, and such subspaces are the
focus of this section.
Invariant Subspaces
•
For a linear operator T on V, a subspace X ⊆V is said to be an
invariant subspace under T whenever T(X) ⊆X.
•
In such a situation, T can be considered as a linear operator on X
by forgetting about everything else in V and restricting T to act
only on vectors from X. Hereafter, this restricted operator will
be denoted by T/X .
Example 4.9.1
Problem: For
A =


4
4
4
−2
−2
−5
1
2
5

,
x1 =


2
−1
0

,
and
x2 =


−1
2
−1

,
show that the subspace X spanned by B = {x1, x2} is an invariant subspace
under A. Then describe the restriction A/X
and determine the coordinate
matrix of A/X relative to B.
Solution: Observe that Ax1 = 2x1 ∈X and Ax2 = x1 + 2x2 ∈X, so the
image of any x = αx1 + βx2 ∈X is back in X because
Ax = A(αx1+βx2) = αAx1+βAx2 = 2αx1+β(x1+2x2) = (2α+β)x1+2βx2.
This equation completely describes the action of A restricted to X, so
A/X (x) = (2α + β)x1 + 2βx2
for each
x = αx1 + βx2 ∈X.
Since A/X (x1) = 2x1 and A/X (x2) = x1 + 2x2, we have
*
A/X
+
B =
 *
A/X (x1)
+
B

*
A/X (x2)
+
B
 
=

2
1
0
2

.

260
Chapter 4
Vector Spaces
The invariant subspaces for a linear operator T are important because they
produce simpliﬁed coordinate matrix representations of T. To understand how
this occurs, suppose X is an invariant subspace under T, and let
BX = {x1, x2, . . . , xr}
be a basis for X that is part of a basis
B = {x1, x2, . . . , xr, y1, y2, . . . , yq}
for the entire space V. To compute [T]B, recall from the deﬁnition of coordinate
matrices that
[T]B =

[T(x1)]B
 · · ·
 [T(xr)]B
 [T(y1)]B
 · · ·
 [T(yq)]B

.
(4.9.1)
Because each T(xj) is contained in X, only the ﬁrst r vectors from B are
needed to represent each T(xj), so, for j = 1, 2, . . . , r,
T(xj) =
r

i=1
αijxi
and
[T(xj)]B =









α1j
...
αrj
0
...
0









.
(4.9.2)
The space
Y = span {y1, y2, . . . , yq}
(4.9.3)
may not be an invariant subspace for T, so all the basis vectors in B may be
needed to represent the T(yj) ’s. Consequently, for j = 1, 2, . . . , q,
T(yj) =
r

i=1
βijxi +
q

i=1
γijyi
and
[T(yj)]B =










β1j
...
βrj
γ1j
...
γqj










.
(4.9.4)
Using (4.9.2) and (4.9.4) in (4.9.1) produces the block-triangular matrix
[T]B =










α11
· · ·
α1r
β11
· · ·
β1q
...
...
...
...
...
...
αr1
· · ·
αrr
βr1
· · ·
βrq
0
· · ·
0
γ11
· · ·
γ1q
...
...
...
...
...
...
0
· · ·
0
γq1
· · ·
γqq










.
(4.9.5)

4.9 Invariant Subspaces
261
The equations T(xj) = r
i=1 αijxi in (4.9.2) mean that
*
T/X (xj)
+
BX
=




α1j
α2j
...
αrj



,
so
*
T/X
+
BX
=




α11
α12
· · ·
α1r
α21
α22
· · ·
α2r
...
...
...
...
αr1
αr2
· · ·
αrr



,
and thus the matrix in (4.9.5) can be written as
[T]B =
 *
T/X
+
BX
Br×q
0
Cq×q
 
.
(4.9.6)
In other words, (4.9.6) says that the matrix representation for T can be made
to be block triangular whenever a basis for an invariant subspace is available.
The more invariant subspaces we can ﬁnd, the more tools we have to con-
struct simpliﬁed matrix representations. For example, if the space Y in (4.9.3)
is also an invariant subspace for T, then T(yj) ∈Y for each j = 1, 2, . . . , q,
and only the yi ’s are needed to represent T(yj) in (4.9.4). Consequently, the
βij ’s are all zero, and [T]B has the block-diagonal form
[T]B =

Ar×r
0
0
Cq×q

=


*
T/X
+
Bx
0
0
*
T/Y
+
By

.
This notion easily generalizes in the sense that if B = BX ∪BY ∪· · ·∪BZ is a basis
for V, where BX , BY, . . . , BZ are bases for invariant subspaces under T that
have dimensions r1, r2, . . . , rk, respectively, then [T]B has the block-diagonal
form
[T]B =




Ar1×r1
0
· · ·
0
0
Br2×r2
· · ·
0
...
...
...
...
0
0
· · ·
Crk×rk



,
where
A =
*
T/X
+
Bx
,
B =
*
T/Y
+
By
,
. . . ,
C =
*
T/Z
+
Bz
.
The situations discussed above are also reversible in the sense that if the
matrix representation of T has a block-triangular form
[T]B =

Ar×r
Br×q
0
Cq×q

relative to some basis
B = {u1, u2, . . . , ur, w1, w2, . . . , wq},

262
Chapter 4
Vector Spaces
then the r -dimensional subspace U = span {u1, u2, . . . , ur} spanned by the
ﬁrst r vectors in B must be an invariant subspace under T. Furthermore, if
the matrix representation of T has a block-diagonal form
[T]B =

Ar×r
0
0
Cq×q

relative to B, then both
U = span {u1, u2, . . . , ur}
and
W = span {w1, w2, . . . , wq}
must be invariant subspaces for T. The details are left as exercises.
The general statement concerning invariant subspaces and coordinate ma-
trix representations is given below.
Invariant Subspaces and Matrix Representations
Let T be a linear operator on an n-dimensional space V, and let
X, Y, . . . , Z be subspaces of V with respective dimensions r1, r2, . . . , rk
and bases BX , BY, . . . , BZ. Furthermore, suppose that 
i ri = n and
B = BX ∪BY ∪· · · ∪BZ is a basis for V.
•
The subspace X is an invariant subspace under T if and only if
[T]B has the block-triangular form
[T]B =

Ar1×r1
B
0
C

,
in which case
A =
*
T/X
+
BX
.
(4.9.7)
•
The subspaces X, Y, . . . , Z are all invariant under T if and only if
[T]B has the block-diagonal form
[T]B =




Ar1×r1
0
· · ·
0
0
Br2×r2
· · ·
0
...
...
...
...
0
0
· · ·
Crk×rk



,
(4.9.8)
in which case
A =
*
T/X
+
Bx
,
B =
*
T/Y
+
By
,
. . . ,
C =
*
T/Z
+
Bz
.
An important corollary concerns the special case in which the linear operator
T is in fact an n × n matrix and T(v) = Tv is a matrix–vector multiplication.

4.9 Invariant Subspaces
263
Triangular and Diagonal Block Forms
When T is an n × n matrix, the following two statements are true.
•
Q is a nonsingular matrix such that
Q−1TQ =

Ar×r
Br×q
0
Cq×q

(4.9.9)
if and only if the ﬁrst r columns in Q span an invariant subspace
under T.
•
Q is a nonsingular matrix such that
Q−1TQ =




Ar1×r1
0
· · ·
0
0
Br2×r2
· · ·
0
...
...
...
...
0
0
· · ·
Crk×rk




(4.9.10)
if and only if Q =

Q1
 Q2
 · · ·
 Qk

in which Qi is n × ri, and
the columns of each Qi span an invariant subspace under T.
Proof.
We know from Example 4.8.3 that if B = {q1, q2, . . . , qn} is a basis for
ℜn, and if Q =

q1
 q2
 · · ·
 qn

is the matrix containing the vectors from B
as its columns, then [T]B = Q−1TQ. Statements (4.9.9) and (4.9.10) are now
direct consequences of statements (4.9.7) and (4.9.8), respectively.
Example 4.9.2
Problem: For
T =



−1
−1
−1
−1
0
−5
−16
−22
0
3
10
14
4
8
12
14


,
q1 =



2
−1
0
0


,
and
q2 =



−1
2
−1
0


,
verify that X = span {q1, q2} is an invariant subspace under T, and then ﬁnd
a nonsingular matrix Q such that Q−1TQ has the block-triangular form
Q−1TQ =




∗
∗
∗
∗
∗
∗
∗
∗
0
0
∗
∗
0
0
∗
∗



.

264
Chapter 4
Vector Spaces
Solution: X is invariant because Tq1 = q1+3q2 and Tq2 = 2q1+4q2 insure
that for all α and β, the images
T(αq1 + βq2) = (α + 2β)q1 + (3α + 4β)q2
lie in X. The desired matrix Q is constructed by extending {q1, q2} to a basis
B = {q1, q2, q3, q4} for ℜ4. If the extension technique described in Solution 2
of Example 4.4.5 is used, then
q3 =



1
0
0
0



and
q4 =



0
0
0
1


,
and
Q =

q1
 q2
 q3
 q4

=



2
−1
1
0
−1
2
0
0
0
−1
0
0
0
0
0
1


.
Since the ﬁrst two columns of Q span a space that is invariant under T, it
follows from (4.9.9) that Q−1TQ must be in block-triangular form. This is easy
to verify by computing
Q−1 =



0
−1
−2
0
0
0
−1
0
1
2
3
0
0
0
0
1



and
Q−1TQ =




1
2
0
−6
3
4
0
−14
0
0
−1
−3
0
0
4
14



.
In passing, notice that the upper-left-hand block is
*
T/X
+
{q1,q2} =

1
2
3
4

.
Example 4.9.3
Consider again the matrices of Example 4.9.2:
T =



−1
−1
−1
−1
0
−5
−16
−22
0
3
10
14
4
8
12
14


,
q1 =



2
−1
0
0


,
and
q2 =



−1
2
−1
0


.
There are inﬁnitely many extensions of {q1, q2} to a basis B = {q1, q2, q3, q4}
for ℜ4 —the extension used in Example 4.9.2 is only one possibility. Another
extension is
q3 =



0
−1
2
−1



and
q4 =



0
0
−1
1


.

4.9 Invariant Subspaces
265
This extension might be preferred over that of Example 4.9.2 because the spaces
X = span {q1, q2} and Y = span {q3, q4} are both invariant under T, and
therefore it follows from (4.9.10) that Q−1TQ is block diagonal. Indeed, it is
not diﬃcult to verify that
Q−1TQ =



1
1
1
1
1
2
2
2
1
2
3
3
1
2
3
4






−1
−1
−1
−1
0
−5
−16
−22
0
3
10
14
4
8
12
14






2
−1
0
0
−1
2
−1
0
0
−1
2
−1
0
0
−1
1



=




1
2
0
0
3
4
0
0
0
0
5
6
0
0
7
8



.
Notice that the diagonal blocks must be the matrices of the restrictions in the
sense that

1
2
3
4

=
*
T/X
+
{q1,q2}
and

5
6
7
8

=
*
T/Y
+
{q3,q4} .
Example 4.9.4
Problem: Find all subspaces of ℜ2 that are invariant under
A =

0
1
−2
3

.
Solution: The trivial subspace {0} is the only zero-dimensional invariant sub-
space, and the entire space ℜ2 is the only two-dimensional invariant subspace.
The real problem is to ﬁnd all one-dimensional invariant subspaces. If M is a
one-dimensional subspace spanned by x ̸= 0 such that A(M) ⊆M, then
Ax ∈M
=⇒
there is a scalar λ such that Ax = λx
=⇒
(A −λI) x = 0.
In other words, M ⊆N (A −λI) . Since dim M = 1, it must be the case that
N (A −λI) ̸= {0}, and consequently λ must be a scalar such that (A −λI) is
a singular matrix. Row operations produce
A −λI =

−λ
1
−2
3 −λ

−→

−2
3 −λ
−λ
1

−→

−2
3 −λ
0
1 + (λ2 −3λ)/2

,
and it is clear that (A −λI) is singular if and only if 1 + (λ2 −3λ)/2 = 0 —i.e.,
if and only if λ is a root of
λ2 −3λ + 2 = 0.

266
Chapter 4
Vector Spaces
Thus λ = 1 and λ = 2, and straightforward computation yields the two one-
dimensional invariant subspaces
M1 = N (A −I) = span

1
1

and
M2 = N (A −2I) = span

1
2

.
In passing, notice that B =
( 1
1

,
 1
2
)
is a basis for ℜ2, and
[A]B = Q−1AQ =

1
0
0
2

,
where
Q =

1
1
1
2

.
In general, scalars λ for which (A −λI) is singular are called the eigenvalues
of A, and the nonzero vectors in N (A −λI) are known as the associated
eigenvectors for A. As this example indicates, eigenvalues and eigenvectors
are of fundamental importance in identifying invariant subspaces and reducing
matrices by means of similarity transformations. Eigenvalues and eigenvectors
are discussed at length in Chapter 7.
Exercises for section 4.9
4.9.1. Let T be an arbitrary linear operator on a vector space V.
(a)
Is the trivial subspace {0} invariant under T?
(b)
Is the entire space V invariant under T?
4.9.2. Describe all of the subspaces that are invariant under the identity oper-
ator I on a space V.
4.9.3. Let T be the linear operator on ℜ4 deﬁned by
T(x1, x2, x3, x4) = (x1 + x2 + 2x3 −x4,
x2 + x4,
2x3 −x4,
x3 + x4),
and let X = span {e1, e2} be the subspace that is spanned by the ﬁrst
two unit vectors in ℜ4.
(a)
Explain why X is invariant under T.
(b)
Determine
"
T/X
#
{e1,e2}.
(c)
Describe the structure of [T]B, where B is any basis obtained
from an extension of {e1, e2} .

4.9 Invariant Subspaces
267
4.9.4. Let T and Q be the matrices
T =



−2
−1
−5
−2
−9
0
−8
−2
2
3
11
5
3
−5
−13
−7



and
Q =



1
0
0
−1
1
1
3
−4
−2
0
1
0
3
−1
−4
3


.
(a)
Explain why the columns of Q are a basis for ℜ4.
(b)
Verify that X = span {Q∗1, Q∗2} and Y = span {Q∗3, Q∗4}
are each invariant subspaces under T.
(c)
Describe the structure of Q−1TQ without doing any compu-
tation.
(d)
Now compute the product Q−1TQ to determine
*
T/X
+
{Q∗1,Q∗2}
and
*
T/Y
+
{Q∗3,Q∗4} .
4.9.5. Let T be a linear operator on a space V, and suppose that
B = {u1, . . . , ur, w1, . . . , wq}
is a basis for V such that [T]B has the block-diagonal form
[T]B =

Ar×r
0
0
Cq×q

.
Explain why U = span {u1, . . . , ur} and W = span {w1, . . . , wq} must
each be invariant subspaces under T.
4.9.6. If Tn×n and Pn×n are matrices such that
P−1TP =

Ar×r
0
0
Cq×q

,
explain why
U = span {P∗1, . . . , P∗r}
and
W = span {P∗r+1, . . . , P∗n}
are each invariant subspaces under T.
4.9.7. If A is an n × n matrix and λ is a scalar such that (A −λI) is
singular (i.e., λ is an eigenvalue), explain why the associated space of
eigenvectors N (A −λI) is an invariant subspace under A.
4.9.8. Consider the matrix A =
 −9
4
−24
11

.
(a)
Determine the eigenvalues of A.
(b)
Identify all subspaces of ℜ2 that are invariant under A.
(c)
Find a nonsingular matrix Q such that Q−1AQ is a diagonal
matrix.

CHAPTER 5
Norms,
Inner Products,
and Orthogonality
5.1
VECTOR NORMS
A signiﬁcant portion of linear algebra is in fact geometric in nature because
much of the subject grew out of the need to generalize the basic geometry of
ℜ2 and ℜ3 to nonvisual higher-dimensional spaces. The usual approach is to
coordinatize geometric concepts in ℜ2 and ℜ3, and then extend statements
concerning ordered pairs and triples to ordered n-tuples in ℜn and Cn.
For example, the length of a vector u ∈ℜ2 or v ∈ℜ3 is obtained from
the Pythagorean theorem by computing the length of the hypotenuse of a right
triangle as shown in Figure 5.1.1.
x
y
u = (x,y)
x2 + y2
||u|| =
x
y
z
v = (x,y,z)
x2 + y2 + z2
||v|| =
Figure 5.1.1
This measure of length,
∥u∥=

x2 + y2
and
∥v∥=

x2 + y2 + z2,

270
Chapter 5
Norms, Inner Products, and Orthogonality
is called the euclidean norm in ℜ2 and ℜ3, and there is an obvious extension
to higher dimensions.
Euclidean Vector Norm
For a vector xn×1, the euclidean norm of x is deﬁned to be
•
∥x∥=
 n
i=1 x2
i
1/2
=
√
xT x whenever x ∈ℜn×1,
•
∥x∥=
 n
i=1 |xi|21/2
=
√
x∗x whenever x ∈Cn×1.
For example, if u =



0
−1
2
−2
4


and v =



i
2
1 −i
0
1 + i


, then
∥u∥=

u2
i =
√
uT u =
√
0 + 1 + 4 + 4 + 16 = 5,
∥v∥=

|vi|2 =
√
v∗v =
√
1 + 4 + 2 + 0 + 2 = 3.
There are several points to note.
33
•
The complex version of ∥x∥includes the real version as a special case because
|z|2 = z2 whenever z is a real number. Recall that if z = a + ib, then
¯z = a −ib, and the magnitude of z is |z| = √¯zz =
√
a2 + b2. The fact that
|z|2 = ¯zz = a2 + b2 is a real number insures that ∥x∥is real even if x has
some complex components.
•
The deﬁnition of euclidean norm guarantees that for all scalars α,
∥x∥≥0,
∥x∥= 0 ⇐⇒x = 0,
and
∥αx∥= |α| ∥x∥.
(5.1.1)
•
Given a vector x ̸= 0, it’s frequently convenient to have another vector
that points in the same direction as x (i.e., is a positive multiple of x) but
has unit length. To construct such a vector, we normalize x by setting
u = x/ ∥x∥. From (5.1.1), it’s easy to see that
∥u∥=

x
∥x∥
 =
1
∥x∥∥x∥= 1.
(5.1.2)
33
By convention, column vectors are used throughout this chapter. But there is nothing special
about columns because, with the appropriate interpretation, all statements concerning columns
will also hold for rows.

5.1 Vector Norms
271
•
The distance between vectors in ℜ3 can be visualized with the aid of the
parallelogram law as shown in Figure 5.1.2, so for vectors in ℜn and Cn,
the distance between u and v is naturally deﬁned to be ∥u −v∥.
v
u
u - v
||u - v||
Figure 5.1.2
Standard Inner Product
The scalar terms deﬁned by
xT y =
n

i=1
xiyi ∈ℜ
and
x∗y =
n

i=1
¯xiyi ∈C
are called the standard inner products for ℜn and Cn, respectively.
The Cauchy–Bunyakovskii–Schwarz (CBS) inequality
34 is one of the most
important inequalities in mathematics. It relates inner product to norm.
34
The Cauchy–Bunyakovskii–Schwarz inequality is named in honor of the three men who played
a role in its development. The basic inequality for real numbers is attributed to Cauchy in 1821,
whereas Schwarz and Bunyakovskii contributed by later formulating useful generalizations of
the inequality involving integrals of functions.
Augustin-Louis Cauchy (1789–1857) was a French mathematician who is generally regarded
as being the founder of mathematical analysis—including the theory of complex functions.
Although deeply embroiled in political turmoil for much of his life (he was a partisan of the
Bourbons), Cauchy emerged as one of the most proliﬁc mathematicians of all time. He authored
at least 789 mathematical papers, and his collected works ﬁll 27 volumes—this is on a par with
Cayley and second only to Euler. It is said that more theorems, concepts, and methods bear
Cauchy’s name than any other mathematician.
Victor Bunyakovskii (1804–1889) was a Russian professor of mathematics at St. Petersburg, and
in 1859 he extended Cauchy’s inequality for discrete sums to integrals of continuous functions.
His contribution was overlooked by western mathematicians for many years, and his name is
often omitted in classical texts that simply refer to the Cauchy–Schwarz inequality.
Hermann Amandus Schwarz (1843–1921) was a student and successor of the famous German
mathematician Karl Weierstrass at the University of Berlin. Schwarz independently generalized
Cauchy’s inequality just as Bunyakovskii had done earlier.

272
Chapter 5
Norms, Inner Products, and Orthogonality
Cauchy–Bunyakovskii–Schwarz (CBS) Inequality
|x∗y| ≤∥x∥∥y∥
for all x, y ∈Cn×1.
(5.1.3)
Equality holds if and only if y = αx for α = x∗y/x∗x.
Proof.
Set α = x∗y/x∗x = x∗y/ ∥x∥2 (assume x ̸= 0 because there is nothing
to prove if x = 0) and observe that x∗(αx −y) = 0, so
0 ≤∥αx −y∥2 = (αx −y)∗(αx −y) = ¯αx∗(αx −y) −y∗(αx −y)
= −y∗(αx −y) = y∗y −αy∗x = ∥y∥2 ∥x∥2 −(x∗y) (y∗x)
∥x∥2
.
(5.1.4)
Since y∗x = x∗y, it follows that (x∗y) (y∗x) = |x∗y|2 , so
0 ≤∥y∥2 ∥x∥2 −|x∗y|2
∥x∥2
.
Now, 0 < ∥x∥2 implies 0 ≤∥y∥2 ∥x∥2 −|x∗y|2 , and thus the CBS inequality
is obtained. Establishing the conditions for equality is Exercise 5.1.9.
One reason that the CBS inequality is important is because it helps to
establish that the geometry in higher-dimensional spaces is consistent with the
geometry in the visual spaces ℜ2 and ℜ3. In particular, consider the situation
depicted in Figure 5.1.3.
x
y
x + y
||x||
||y||
||x + y||
Figure 5.1.3
Imagine traveling from the origin to the point x and then moving from x to the
point x+y. Clearly, you have traveled a distance that is at least as great as the
direct distance from the origin to x+y along the diagonal of the parallelogram.
In other words, it’s visually evident that ∥x + y∥≤∥x∥+∥y∥. This observation

5.1 Vector Norms
273
is known as the triangle inequality. In higher-dimensional spaces we do not
have the luxury of visualizing the geometry with our eyes, and the question of
whether or not the triangle inequality remains valid has no obvious answer. The
CBS inequality is precisely what is required to prove that, in this respect, the
geometry of higher dimensions is no diﬀerent than that of the visual spaces.
Triangle Inequality
∥x + y∥≤∥x∥+ ∥y∥
for every x, y ∈Cn.
Proof.
Consider x and y to be column vectors, and write
∥x + y∥2 = (x + y)∗(x + y) = x∗x + x∗y + y∗x + y∗y
= ∥x∥2 + x∗y + y∗x + ∥y∥2 .
(5.1.5)
Recall that if z = a + ib, then z + ¯z = 2a = 2 Re (z) and |z|2 = a2 + b2 ≥a2,
so that |z| ≥Re (z) . Using the fact that y∗x = x∗y together with the CBS
inequality yields
x∗y + y∗x = 2 Re (x∗y) ≤2 |x∗y| ≤2 ∥x∥∥y∥.
Consequently, we may infer from (5.1.5) that
∥x + y∥2 ≤∥x∥2 + 2 ∥x∥∥y∥+ ∥y∥2 = (∥x∥+ ∥y∥)2 .
It’s not diﬃcult to see that the triangle inequality can be extended to any
number of vectors in the sense that
 
i xi
 ≤
i ∥xi∥. Furthermore, it follows
as a corollary that for real or complex numbers,
 
i αi
 ≤
i |αi| (the triangle
inequality for scalars).
Example 5.1.1
Backward Triangle Inequality. The triangle inequality produces an upper
bound for a sum, but it also yields the following lower bound for a diﬀerence:
 ∥x∥−∥y∥
 ≤∥x −y∥.
(5.1.6)
This is a consequence of the triangle inequality because
∥x∥= ∥x −y + y∥≤∥x −y∥+ ∥y∥
=⇒
∥x∥−∥y∥≤∥x −y∥
and
∥y∥= ∥x −y −x∥≤∥x −y∥+ ∥x∥
=⇒
−(∥x∥−∥y∥) ≤∥x −y∥.

274
Chapter 5
Norms, Inner Products, and Orthogonality
There are notions of length other than the euclidean measure. For example,
urban dwellers navigate on a grid of city blocks with one-way streets, so they are
prone to measure distances in the city not as the crow ﬂies but rather in terms
of lengths on a directed grid. For example, instead of than saying that “it’s a
one-half mile straight-line (euclidean) trip from here to there,” they are more
apt to describe the length of the trip by saying, “it’s two blocks north on Dan
Allen Drive, four blocks west on Hillsborough Street, and ﬁve blocks south on
Gorman Street.” In other words, the length of the trip is 2 + | −4| + | −5| = 11
blocks—absolute value is used to insure that southerly and westerly movement
does not cancel the eﬀect of northerly and easterly movement, respectively. This
“grid norm” is better known as the 1-norm because it is a special case of a more
general class of norms deﬁned below.
p-Norms
For p ≥1, the p-norm of x ∈Cn is deﬁned as ∥x∥p = (n
i=1 |xi|p)1/p.
It can be proven that the following properties of the euclidean norm are in
fact valid for all p-norms:
∥x∥p ≥0
and
∥x∥p = 0 ⇐⇒x = 0,
∥αx∥p = |α| ∥x∥p
for all scalars α,
∥x + y∥p ≤∥x∥p + ∥y∥p
(see Exercise 5.1.13).
(5.1.7)
The generalized version of the CBS inequality (5.1.3) for p-norms is H¨older’s
inequality (developed in Exercise 5.1.12), which states that if p > 1 and q > 1
are real numbers such that 1/p + 1/q = 1, then
|x∗y| ≤∥x∥p ∥y∥q .
(5.1.8)
In practice, only three of the p-norms are used, and they are
∥x∥1 =
n

i=1
|xi|
(the grid norm),
∥x∥2 =
 n

i=1
|xi|2
1/2
(the euclidean norm),
and
∥x∥∞= lim
p→∞∥x∥p = lim
p→∞
 n

i=1
|xi|p
1/p
= max
i
|xi|
(the max norm).
For example, if x = (3, 4−3i, 1), then ∥x∥1 = 9, ∥x∥2 =
√
35, and ∥x∥∞= 5.

5.1 Vector Norms
275
To see that limp→∞∥x∥p = maxi |xi| , proceed as follows. Relabel the en-
tries of x by setting ˜x1 = maxi |xi| , and if there are other entries with this
same maximal magnitude, label them ˜x2, . . . , ˜xk. Label any remaining coordi-
nates as ˜xk+1 · · · ˜xn. Consequently, |˜xi/˜x1| < 1 for i = k + 1, . . . , n, so, as
p →∞,
∥x∥p =
 n

i=1
|˜xi|p
1/p
= |˜x1|

k +

˜xk+1
˜x1

p
+ · · · +

˜xn
˜x1

p1/p
→|˜x1| .
Example 5.1.2
To get a feel for the 1-, 2-, and ∞-norms, it helps to know the shapes and relative
sizes of the unit p-spheres Sp = {x | ∥x∥p = 1} for p = 1, 2, ∞. As illustrated
in Figure 5.1.4, the unit 1-, 2-, and ∞-spheres in ℜ3 are an octahedron, a ball,
and a cube, respectively, and it’s visually evident that S1 ﬁts inside S2, which
in turn ﬁts inside S∞. This means that ∥x∥1 ≥∥x∥2 ≥∥x∥∞for all x ∈ℜ3.
In general, this is true in ℜn (Exercise 5.1.8).
S1
S2
S∞
Figure 5.1.4
Because the p-norms are deﬁned in terms of coordinates, their use is limited
to coordinate spaces. But it’s desirable to have a general notion of norm that
works for all vector spaces. In other words, we need a coordinate-free deﬁnition
of norm that includes the standard p-norms as a special case. Since all of the p-
norms satisfy the properties (5.1.7), it’s natural to use these properties to extend
the concept of norm to general vector spaces.
General Vector Norms
A norm for a real or complex vector space V is a function ∥⋆∥mapping
V into ℜthat satisﬁes the following conditions.
∥x∥≥0
and
∥x∥= 0 ⇐⇒x = 0,
∥αx∥= |α| ∥x∥
for all scalars α,
∥x + y∥≤∥x∥+ ∥y∥.
(5.1.9)

276
Chapter 5
Norms, Inner Products, and Orthogonality
Example 5.1.3
Equivalent Norms. Vector norms are basic tools for deﬁning and analyzing
limiting behavior in vector spaces V. A sequence {xk} ⊂V is said to converge
to x (write xk →x ) if ∥xk −x∥→0. This depends on the choice of the norm,
so, ostensibly, we might have xk →x with one norm but not with another.
Fortunately, this is impossible in ﬁnite-dimensional spaces because all norms are
equivalent in the following sense.
Problem: For each pair of norms, ∥⋆∥a , ∥⋆∥b , on an n-dimensional space V,
exhibit positive constants α and β (depending only on the norms) such that
α ≤∥x∥a
∥x∥b
≤β
for all nonzero vectors in V.
(5.1.10)
Solution: For Sb = {y | ∥y∥b = 1}, let µ = miny∈Sb ∥y∥a > 0,
35 and write
x
∥x∥b
∈Sb
=⇒
∥x∥a = ∥x∥b

x
∥x∥b

a
≥∥x∥b min
y∈Sb ∥y∥a = ∥x∥b µ.
The same argument shows there is a ν > 0 such that ∥x∥b ≥ν ∥x∥a , so
(5.1.10) is produced with α = µ and β = 1/ν. Note that (5.1.10) insures that
∥xk −x∥a →0 if and only if ∥xk −x∥b →0. Speciﬁc values for α and β are
given in Exercises 5.1.8 and 5.12.3.
Exercises for section 5.1
5.1.1. Find the 1-, 2-, and ∞-norms of x =


2
1
−4
−2

and x =


1 + i
1 −i
1
4i

.
5.1.2. Consider the euclidean norm with u =


2
1
−4
−2

and v =


1
−1
1
−1

.
(a)
Determine the distance between u and v.
(b)
Verify that the triangle inequality holds for u and v.
(c)
Verify that the CBS inequality holds for u and v.
5.1.3. Show that (α1 + α2 + · · · + αn)2 ≤n

α2
1 + α2
2 + · · · + α2
n

for αi ∈ℜ.
35
An important theorem from analysis states that a continuous function mapping a closed and
bounded subset K ⊂V
into ℜattains a minimum and maximum value at points in K.
Unit spheres in ﬁnite-dimensional spaces are closed and bounded, and every norm on V is
continuous (Exercise 5.1.7), so this minimum is guaranteed to exist.

5.1 Vector Norms
277
5.1.4. (a)
Using the euclidean norm, describe the solid ball in ℜn centered
at the origin with unit radius.
(b)
Describe a solid ball centered at
the point c = ( ξ1
ξ2
· · ·
ξn ) with radius ρ.
5.1.5. If x, y ∈ℜn such that ∥x −y∥2 = ∥x + y∥2 , what is xT y?
5.1.6. Explain why ∥x −y∥= ∥y −x∥is true for all norms.
5.1.7. For every vector norm on Cn, prove that ∥v∥depends continuously on
the components of v in the sense that for each ϵ > 0, there corresponds
a δ > 0 such that
 ∥x∥−∥y∥
 < ϵ whenever |xi −yi| < δ for each i.
5.1.8.
(a)
For x ∈Cn×1, explain why ∥x∥1 ≥∥x∥2 ≥∥x∥∞.
(b)
For x ∈Cn×1, show that ∥x∥i ≤α ∥x∥j , where α is the (i, j)-
entry in the following matrix. (See Exercise 5.12.3 for a similar
statement regarding matrix norms.)


1
2
∞
1
∗
√n
n
2
1
∗
√n
∞
1
1
∗

.
5.1.9. For x, y ∈Cn, x ̸= 0, explain why equality holds in the CBS inequality
if and only if y = αx, where α = x∗y/x∗x. Hint: Use (5.1.4).
5.1.10. For nonzero vectors x, y ∈Cn with the euclidean norm, prove that
equality holds in the triangle inequality if and only if y = αx, where α
is real and positive. Hint: Make use of Exercise 5.1.9.
5.1.11. Use H¨older’s inequality (5.1.8) to prove that if the components of
x ∈ℜn×1 sum to zero (i.e., xT e = 0 for eT = (1, 1, . . . , 1) ), then
|xT y| ≤∥x∥1
ymax −ymin
2

for all y ∈ℜn×1.
Note: For “zero sum” vectors x, this is at least as sharp and usually
it’s sharper than (5.1.8) because (ymax −ymin)/2 ≤maxi |yi| = ∥y∥∞.

278
Chapter 5
Norms, Inner Products, and Orthogonality
5.1.12. The classical form of H¨older’s inequality
36 states that if p > 1 and
q > 1 are real numbers such that 1/p + 1/q = 1, then
n

i=1
|xiyi| ≤
 n

i=1
|xi|p
1/p  n

i=1
|yi|q
1/q
.
Derive this inequality by executing the following steps:
(a)
By considering the function f(t) = (1 −λ) + λt −tλ for 0 < λ < 1,
establish the inequality
αλβ1−λ ≤λα + (1 −λ)β
for nonnegative real numbers α and β.
(b)
Let ˆx = x/ ∥x∥p and ˆy = y/ ∥y∥q , and apply the inequality of part (a)
to obtain
n

i=1
|ˆxiˆyi| ≤1
p
n

i=1
|ˆxi|p + 1
q
n

i=1
|ˆyi|q = 1.
(c)
Deduce the classical form of H¨older’s inequality, and then explain why
this means that
|x∗y| ≤∥x∥p ∥y∥q .
5.1.13. The triangle inequality ∥x + y∥p ≤∥x∥p + ∥y∥p for a general p-norm
is really the classical Minkowski inequality,
37 which states that for
p ≥1,
 n

i=1
|xi + yi|p
1/p
≤
 n

i=1
|xi|p
1/p
+
 n

i=1
|yi|p
1/p
.
Derive Minkowski’s inequality. Hint: For p > 1, let q be the number
such that 1/q = 1 −1/p. Verify that for scalars α and β,
|α + β|p = |α + β| |α + β|p/q ≤|α| |α + β|p/q + |β| |α + β|p/q,
and make use of H¨older’s inequality in Exercise 5.1.12.
36
Ludwig Otto H¨older (1859–1937) was a German mathematician who studied at G¨ottingen and
lived in Leipzig. Although he made several contributions to analysis as well as algebra, he is
primarily known for the development of the inequality that now bears his name.
37
Hermann Minkowski (1864–1909) was born in Russia, but spent most of his life in Germany
as a mathematician and professor at K¨onigsberg and G¨ottingen. In addition to the inequality
that now bears his name, he is known for providing a mathematical basis for the special theory
of relativity. He died suddenly from a ruptured appendix at the age of 44.

5.2 Matrix Norms
279
5.2
MATRIX NORMS
Because Cm×n is a vector space of dimension mn, magnitudes of matrices
A ∈Cm×n can be “measured” by employing any vector norm on Cmn. For
example, by stringing out the entries of A =

2
−1
−4
−2

into a four-component
vector, the euclidean norm on ℜ4 can be applied to write
∥A∥=

22 + (−1)2 + (−4)2 + (−2)21/2 = 5.
This is one of the simplest notions of a matrix norm, and it is called the Frobenius
(p. 662) norm (older texts refer to it as the Hilbert–Schmidt norm or the Schur
norm). There are several useful ways to describe the Frobenius matrix norm.
Frobenius Matrix Norm
The Frobenius norm of A ∈Cm×n is deﬁned by the equations
∥A∥2
F =

i,j
|aij|2 =

i
∥Ai∗∥2
2 =

j
∥A∗j∥2
2 = trace (A∗A).
(5.2.1)
The Frobenius matrix norm is ﬁne for some problems, but it is not well suited
for all applications. So, similar to the situation for vector norms, alternatives need
to be explored. But before trying to develop diﬀerent recipes for matrix norms, it
makes sense to ﬁrst formulate a general deﬁnition of a matrix norm. The goal is
to start with the deﬁning properties for a vector norm given in (5.1.9) on p. 275
and ask what, if anything, needs to be added to that list.
Matrix multiplication distinguishes matrix spaces from more general vector
spaces, but the three vector-norm properties (5.1.9) say nothing about products.
So, an extra property that relates ∥AB∥to ∥A∥and ∥B∥is needed. The
Frobenius norm suggests the nature of this extra property. The CBS inequality
insures that ∥Ax∥2
2 = 
i |Ai∗x|2 ≤
i ∥Ai∗∥2
2 ∥x∥2
2 = ∥A∥2
F ∥x∥2
2 . That is,
∥Ax∥2 ≤∥A∥F ∥x∥2 ,
(5.2.2)
and we express this by saying that the Frobenius matrix norm ∥⋆∥F and the
euclidean vector norm ∥⋆∥2 are compatible. The compatibility condition (5.2.2)
implies that for all conformable matrices A and B,
∥AB∥2
F =

j
∥[AB]∗j∥2
2 =

j
∥AB∗j∥2
2 ≤

j
∥A∥2
F ∥B∗j∥2
2
= ∥A∥2
F

j
∥B∗j∥2
2 = ∥A∥2
F ∥B∥2
F
=⇒
∥AB∥F ≤∥A∥F ∥B∥F .
This suggests that the submultiplicative property ∥AB∥≤∥A∥∥B∥should be
added to (5.1.9) to deﬁne a general matrix norm.

280
Chapter 5
Norms, Inner Products, and Orthogonality
General Matrix Norms
A matrix norm is a function ∥⋆∥from the set of all complex matrices
(of all ﬁnite orders) into ℜthat satisﬁes the following properties.
∥A∥≥0
and
∥A∥= 0 ⇐⇒A = 0.
∥αA∥= |α| ∥A∥
for all scalars α.
∥A + B∥≤∥A∥+ ∥B∥
for matrices of the same size.
∥AB∥≤∥A∥∥B∥
for all conformable matrices.
(5.2.3)
The Frobenius norm satisﬁes the above deﬁnition (it was built that way),
but where do other useful matrix norms come from? In fact, every legitimate
vector norm generates (or induces) a matrix norm as described below.
Induced Matrix Norms
A vector norm that is deﬁned on Cp for p = m, n induces a matrix
norm on Cm×n by setting
∥A∥= max
∥x∥=1 ∥Ax∥
for A ∈Cm×n, x ∈Cn×1.
(5.2.4)
The footnote on p. 276 explains why this maximum value must exist.
•
It’s apparent that an induced matrix norm is compatible with its
underlying vector norm in the sense that
∥Ax∥≤∥A∥∥x∥.
(5.2.5)
•
When A is nonsingular,
min
∥x∥=1 ∥Ax∥=
1
∥A−1∥.
(5.2.6)
Proof.
Verifying that max∥x∥=1 ∥Ax∥satisﬁes the ﬁrst three conditions in
(5.2.3) is straightforward, and (5.2.5) implies ∥AB∥≤∥A∥∥B∥(see Exercise
5.2.5). Property (5.2.6) is developed in Exercise 5.2.7.
In words, an induced norm ∥A∥represents the maximum extent to which
a vector on the unit sphere can be stretched by A, and 1/
A−1 measures the
extent to which a nonsingular matrix A can shrink vectors on the unit sphere.
Figure 5.2.1 depicts this in ℜ3 for the induced matrix 2-norm.

5.2 Matrix Norms
281
1
max
∥x∥=1 ∥Ax∥= ∥A∥
min
∥x∥=1 ∥Ax∥=
1
∥A-1∥
A
Figure 5.2.1. The induced matrix 2-norm in ℜ3.
Intuition might suggest that the euclidean vector norm should induce the
Frobenius matrix norm (5.2.1), but something surprising happens instead.
Matrix 2-Norm
•
The matrix norm induced by the euclidean vector norm is
∥A∥2 = max
∥x∥2=1 ∥Ax∥2 =

λmax,
(5.2.7)
where λmax is the largest number λ such that A∗A −λI is singular.
•
When A is nonsingular,
A−1
2 =
1
min
∥x∥2=1 ∥Ax∥2
=
1
√λmin
,
(5.2.8)
where λmin is the smallest number λ such that A∗A−λI is singular.
Note: If you are already familiar with eigenvalues, these say that λmax
and λmin are the largest and smallest eigenvalues of A∗A (Example
7.5.1, p. 549), while (λmax)1/2 = σ1 and (λmin)1/2 = σn are the largest
and smallest singular values of A (p. 414).
Proof.
To prove (5.2.7), assume that Am×n is real (a proof for complex ma-
trices is given in Example 7.5.1 on p. 549). The strategy is to evaluate ∥A∥2
2 by
solving the problem
maximize f(x) = ∥Ax∥2
2 = xT AT Ax
subject to g(x) = xT x = 1

282
Chapter 5
Norms, Inner Products, and Orthogonality
using the method of Lagrange multipliers. Introduce a new variable λ (the
Lagrange multiplier), and consider the function h(x, λ) = f(x) −λg(x). The
points at which f is maximized are contained in the set of solutions to the
equations ∂h/∂xi = 0
(i = 1, 2, . . . , n) along with g(x) = 1. Diﬀerentiating
h with respect to the xi ’s is essentially the same as described on p. 227, and
the system generated by ∂h/∂xi = 0 (i = 1, 2, . . . , n) is (AT A −λI)x = 0. In
other words, f is maximized at a vector x for which (AT A −λI)x = 0 and
∥x∥2 = 1. Consequently, λ must be a number such that AT A −λI is singular
(because x ̸= 0 ). Since
xT AT Ax = λxT x = λ,
it follows that
∥A∥2 = max
∥x∥=1 ∥Ax∥= max
∥x∥2=1 ∥Ax∥=

max
xT x=1 xT AT Ax
1/2
=

λmax,
where λmax is the largest number λ for which AT A −λI is singular. A similar
argument applied to (5.2.6) proves (5.2.8). Also, an independent development of
(5.2.7) and (5.2.8) is contained in the discussion of singular values on p. 412.
Example 5.2.1
Problem: Determine the induced norm ∥A∥2 as well as ∥A−1∥2 for the non-
singular matrix
A =
1
√
3
 3
−1
0
√
8

.
Solution: Find the values of λ that make AT A −λI singular by applying
Gaussian elimination to produce
AT A −λI =

3 −λ
−1
−1
3 −λ

−→

−1
3 −λ
3 −λ
−1

−→

−1
3 −λ
0
−1 + (3 −λ)2

.
This shows that AT A−λI is singular when −1+(3−λ)2 = 0 or, equivalently,
when λ = 2 or λ = 4, so λmin = 2 and λmax = 4. Consequently, (5.2.7) and
(5.2.8) say that
∥A∥2 =

λmax = 2
and
∥A−1∥2 =
1
√λmin
=
1
√
2.
Note: As mentioned earlier, the values of λ that make AT A −λI singular
are called the eigenvalues of AT A, and they are the focus of Chapter 7 where
their determination is discussed in more detail. Using Gaussian elimination to
determine the eigenvalues is not practical for larger matrices.
Some useful properties of the matrix 2-norm are stated below.

5.2 Matrix Norms
283
Properties of the 2-Norm
In addition to the properties shared by all induced norms, the 2-norm
enjoys the following special properties.
•
∥A∥2 = max
∥x∥2=1 max
∥y∥2=1 |y∗Ax|.
(5.2.9)
•
∥A∥2 = ∥A∗∥2.
(5.2.10)
•
∥A∗A∥2 = ∥A∥2
2 .
(5.2.11)
•

 A
0
0
B

2 = max

∥A∥2 , ∥B∥2

.
(5.2.12)
•
∥U∗AV∥2 = ∥A∥2 when UU∗= I and V∗V = I.
(5.2.13)
You are asked to verify the validity of these properties in Exercise 5.2.6
on p. 285. Furthermore, some additional properties of the matrix 2-norm are
developed in Exercise 5.6.9 and on pp. 414 and 417.
Now that we understand how the euclidean vector norm induces the matrix
2-norm, let’s investigate the nature of the matrix norms that are induced by the
vector 1-norm and the vector ∞-norm.
Matrix 1-Norm and Matrix ∞-Norm
The matrix norms induced by the vector 1-norm and ∞-norm are as
follows.
•
∥A∥1 = max
∥x∥1=1 ∥Ax∥1 = max
j

i
|aij|
= the largest absolute column sum.
(5.2.14)
•
∥A∥∞=
max
∥x∥∞=1 ∥Ax∥∞= max
i

j
|aij|
= the largest absolute row sum.
(5.2.15)
Proof of (5.2.14).
For all x with ∥x∥1 = 1, the scalar triangle inequality yields
∥Ax∥1 =

i
Ai∗x
 =

i


j
aijxj
 ≤

i

j
|aij| |xj| =

j

|xj|

i
|aij|

≤
 
j
|xj|

max
j

i
|aij|

= max
j

i
|aij| .

284
Chapter 5
Norms, Inner Products, and Orthogonality
Equality can be attained because if A∗k is the column with largest absolute sum,
set x = ek, and note that ∥ek∥1 = 1 and ∥Aek∥1 = ∥A∗k∥1 = maxj

i |aij| .
Proof of (5.2.15).
For all x with ∥x∥∞= 1,
∥Ax∥∞= max
i


j
aijxj
 ≤max
i

j
|aij| |xj| ≤max
i

j
|aij| .
Equality can be attained because if Ak∗is the row with largest absolute sum,
and if x is the vector such that
xj =

1
if akj ≥0,
−1
if akj < 0,
then

|Ai∗x| = | 
j aijxj| ≤
j |aij| for all i,
|Ak∗x| = 
j |akj| = maxi

j |aij| ,
so ∥x∥∞= 1, and ∥Ax∥∞= maxi |Ai∗x| = maxi

j |aij| .
Example 5.2.2
Problem: Determine the induced matrix norms ∥A∥1 and ∥A∥∞for
A =
1
√
3

3
−1
0
√
8

,
and compare the results with ∥A∥2 (from Example 5.2.1) and ∥A∥F .
Solution: Equation (5.2.14) says that ∥A∥1 is the largest absolute column sum
in A, and (5.2.15) says that ∥A∥∞is the largest absolute row sum, so
∥A∥1 = 1/
√
3 +
√
8/
√
3 ≈2.21
and
∥A∥∞= 4/
√
3 ≈2.31.
Since ∥A∥2 = 2 (Example 5.2.1) and ∥A∥F =

trace (AT A) =
√
6 ≈2.45, we
see that while ∥A∥1, ∥A∥2, ∥A∥∞, and ∥A∥F are not equal, they are all in
the same ballpark. This is true for all n × n matrices because it can be shown
that ∥A∥i ≤α ∥A∥j , where α is the (i, j)-entry in the following matrix




1
2
∞
F
1
∗
√n
n
√n
2
√n
∗
√n
1
∞
n
√n
∗
√n
F
√n
√n
√n
∗




(see Exercise 5.1.8 and Exercise 5.12.3 on p. 425). Since it’s often the case that
only the order of magnitude of ∥A∥is needed and not the exact value (e.g.,
recall the rule of thumb in Example 3.8.2 on p. 129), and since ∥A∥2 is diﬃcult
to compute in comparison with ∥A∥1, ∥A∥∞, and ∥A∥F , you can see why any
of these three might be preferred over ∥A∥2 in spite of the fact that ∥A∥2 is
more “natural” by virtue of being induced by the euclidean vector norm.

5.2 Matrix Norms
285
Exercises for section 5.2
5.2.1. Evaluate the Frobenius matrix norm for each matrix below.
A =

1
−2
−1
2

,
B =


0
1
0
0
0
1
1
0
0

,
C =


4
−2
4
−2
1
−2
4
−2
4

.
5.2.2. Evaluate the induced 1-, 2-, and ∞-matrix norm for each of the three
matrices given in Exercise 5.2.1.
5.2.3.
(a)
Explain why ∥I∥= 1 for every induced matrix norm (5.2.4).
(b)
What is ∥In×n∥F ?
5.2.4. Explain why ∥A∥F = ∥A∗∥F for Frobenius matrix norm (5.2.1).
5.2.5. For matrices A and B and for vectors x, establish the following com-
patibility properties between a vector norm deﬁned on every Cp and
the associated induced matrix norm.
(a)
Show that ∥Ax∥≤∥A∥∥x∥.
(b)
Show that ∥AB∥≤∥A∥∥B∥.
(c)
Explain why ∥A∥= max∥x∥≤1 ∥Ax∥.
5.2.6. Establish the following properties of the matrix 2-norm.
(a)
∥A∥2 = max
∥x∥2=1
∥y∥2=1
|y∗Ax|,
(b)
∥A∥2 = ∥A∗∥2,
(c)
∥A∗A∥2 = ∥A∥2
2 ,
(d)

 A
0
0
B

2 = max

∥A∥2 , ∥B∥2

(take A, B to be real),
(e)
∥U∗AV∥2 = ∥A∥2 when UU∗= I and V∗V = I.
5.2.7. Using the induced matrix norm (5.2.4), prove that if A is nonsingular,
then
∥A∥=
1
min
∥x∥=1
A−1x

or, equivalently,
A−1 =
1
min
∥x∥=1 ∥Ax∥.
5.2.8. For A ∈Cn×n and a parameter z ∈C, the matrix R(z) = (zI −A)−1
is called the resolvent of A. Prove that if |z| > ∥A∥for any induced
matrix norm, then
∥R(z)∥≤
1
|z| −∥A∥.

286
Chapter 5
Norms, Inner Products, and Orthogonality
5.3
INNER-PRODUCT SPACES
The euclidean norm, which naturally came ﬁrst, is a coordinate-dependent con-
cept. But by isolating its important properties we quickly moved to the more
general coordinate-free deﬁnition of a vector norm given in (5.1.9) on p. 275. The
goal is to now do the same for inner products. That is, start with the standard
inner product, which is a coordinate-dependent deﬁnition, and identify proper-
ties that characterize the basic essence of the concept. The ones listed below are
those that have been distilled from the standard inner product to formulate a
more general coordinate-free deﬁnition.
General Inner Product
An inner product on a real (or complex) vector space V is a function
that maps each ordered pair of vectors x, y to a real (or complex) scalar
⟨x y⟩such that the following four properties hold.
⟨x x⟩is real with ⟨x x⟩≥0, and ⟨x x⟩= 0 if and only if x = 0,
⟨x αy⟩= α ⟨x y⟩for all scalars α,
(5.3.1)
⟨x y + z⟩= ⟨x y⟩+ ⟨x z⟩,
⟨x y⟩= ⟨y x⟩
(for real spaces, this becomes ⟨x y⟩= ⟨y x⟩).
Notice that for each ﬁxed value of x, the second and third properties
say that ⟨x y⟩is a linear function of y.
Any real or complex vector space that is equipped with an inner product
is called an inner-product space.
Example 5.3.1
•
The standard inner products, ⟨x y⟩= xT y for ℜn×1 and ⟨x y⟩= x∗y
for Cn×1, each satisfy the four deﬁning conditions (5.3.1) for a general inner
product—this shouldn’t be a surprise.
•
If An×n is a nonsingular matrix, then ⟨x y⟩= x∗A∗Ay is an inner product
for Cn×1. This inner product is sometimes called an A-inner product or
an elliptical inner product.
•
Consider the vector space of m × n matrices. The functions deﬁned by
⟨A B⟩= trace

AT B

and
⟨A B⟩= trace (A∗B)
(5.3.2)
are inner products for ℜm×n and Cm×n, respectively. These are referred to
as the standard inner products for matrices. Notice that these reduce
to the standard inner products for vectors when n = 1.

5.3 Inner-Product Spaces
287
•
If V is the vector space of real-valued continuous functions deﬁned on the
interval (a, b), then
⟨f|g⟩=
 b
a
f(t)g(t)dt
is an inner product on V.
Just as the standard inner product for Cn×1 deﬁnes the euclidean norm on
Cn×1 by ∥x∥2 =
√
x∗x, every general inner product in an inner-product space
V deﬁnes a norm on V by setting
∥⋆∥=

⟨⋆⋆⟩.
(5.3.3)
It’s straightforward to verify that this satisﬁes the ﬁrst two conditions in (5.2.3)
on p. 280 that deﬁne a general vector norm, but, just as in the case of euclidean
norms, verifying that (5.3.3) satisﬁes the triangle inequality requires a generalized
version of CBS inequality.
General CBS Inequality
If V is an inner-product space, and if we set ∥⋆∥=

⟨⋆⋆⟩, then
| ⟨x y⟩| ≤∥x∥∥y∥
for all x, y ∈V.
(5.3.4)
Equality holds if and only if y = αx for α = ⟨x y⟩/ ∥x∥2 .
Proof.
Set α = ⟨x y⟩/ ∥x∥2 (assume x ̸= 0, for otherwise there is nothing to
prove), and observe that ⟨x αx −y⟩= 0, so
0 ≤∥αx −y∥2 = ⟨αx −y αx −y⟩
= ¯α ⟨x αx −y⟩−⟨y αx −y⟩
(see Exercise 5.3.2)
= −⟨y αx −y⟩= ⟨y y⟩−α ⟨y x⟩= ∥y∥2 ∥x∥2 −⟨x y⟩⟨y x⟩
∥x∥2
.
Since ⟨y x⟩= ⟨x y⟩, it follows that ⟨x y⟩⟨y x⟩= |⟨x y⟩|2 , so
0 ≤∥y∥2 ∥x∥2 −|⟨x y⟩|2
∥x∥2
=⇒
| ⟨x y⟩| ≤∥x∥∥y∥.
Establishing the conditions for equality is the same as in Exercise 5.1.9.
Let’s now complete the job of showing that ∥⋆∥=

⟨⋆⋆⟩is indeed a vector
norm as deﬁned in (5.2.3) on p. 280.

288
Chapter 5
Norms, Inner Products, and Orthogonality
Norms in Inner-Product Spaces
If V is an inner-product space with an inner product ⟨x y⟩, then
∥⋆∥=

⟨⋆⋆⟩
deﬁnes a norm on V.
Proof.
The fact that ∥⋆∥=

⟨⋆⋆⟩satisﬁes the ﬁrst two norm properties in
(5.2.3) on p. 280 follows directly from the deﬁning properties (5.3.1) for an inner
product. You are asked to provide the details in Exercise 5.3.3. To establish the
triangle inequality, use ⟨x y⟩≤| ⟨x y⟩| and ⟨y x⟩= ⟨x y⟩≤| ⟨x y⟩| together
with the CBS inequality to write
∥x + y∥2 = ⟨x + y x + y⟩= ⟨x x⟩+ ⟨x y⟩+ ⟨y x⟩+ ⟨y y⟩
≤∥x∥2 + 2| ⟨x y⟩| + ∥y∥2 ≤(∥x∥+ ∥y∥)2.
Example 5.3.2
Problem: Describe the norms that are generated by the inner products pre-
sented in Example 5.3.1.
•
Given a nonsingular matrix A ∈Cn×n, the A-norm (or elliptical norm)
generated by the A-inner product on Cn×1 is
∥x∥A =

⟨x x⟩=
√
x∗A∗Ax = ∥Ax∥2 .
(5.3.5)
•
The standard inner product for matrices generates the Frobenius matrix
norm because
∥A∥=

⟨A A⟩=

trace (A∗A) = ∥A∥F .
(5.3.6)
•
For the space of real-valued continuous functions deﬁned on (a, b), the norm
of a function f generated by the inner product ⟨f|g⟩=
 b
a f(t)g(t)dt is
∥f∥=

⟨f|f⟩=
 b
a
f(t)2dt
1/2
.

5.3 Inner-Product Spaces
289
Example 5.3.3
To illustrate the utility of the ideas presented above, consider the proposition
trace

AT B
2 ≤trace

AT A

trace

BT B

for all A, B ∈ℜm×n.
Problem: How would you know to formulate such a proposition and, second,
how do you prove it?
Solution: The answer to both questions is the same. This is the CBS inequality
in ℜm×n equipped with the standard inner product ⟨A B⟩= trace

AT B

and
associated norm ∥A∥F =

⟨A A⟩=

trace (AT A) because CBS says
⟨A B⟩2 ≤∥A∥2
F ∥B∥2
F
=⇒
trace

AT B
2 ≤trace

AT A

trace

BT B

.
The point here is that if your knowledge is limited to elementary matrix manip-
ulations (which is all that is needed to understand the statement of the propo-
sition), formulating the correct inequality might be quite a challenge to your
intuition. And then proving the proposition using only elementary matrix ma-
nipulations would be a signiﬁcant task—essentially, you would have to derive a
version of CBS. But knowing the basic facts of inner-product spaces makes the
proposition nearly trivial to conjecture and prove.
Since each inner product generates a norm by the rule ∥⋆∥=

⟨⋆⋆⟩, it’s
natural to ask if the reverse is also true. That is, for each vector norm ∥⋆∥
on a space V, does there exist a corresponding inner product on V such that

⟨⋆⋆⟩= ∥⋆∥2 ? If not, under what conditions will a given norm be generated by
an inner product? These are tricky questions, and it took the combined eﬀorts
of Maurice R. Fr´echet
38 (1878–1973) and John von Neumann (1903–1957) to
provide the answer.
38
Maurice Ren´e Fr´echet began his illustrious career by writing an outstanding Ph.D. dissertation
in 1906 under the direction of the famous French mathematician Jacques Hadamard (p. 469)
in which the concepts of a metric space and compactness were ﬁrst formulated. Fr´echet devel-
oped into a versatile mathematical scientist, and he served as professor of mechanics at the
University of Poitiers (1910–1919), professor of higher calculus at the University of Strasbourg
(1920–1927), and professor of diﬀerential and integral calculus and professor of the calculus of
probabilities at the University of Paris (1928–1948).
Born in Budapest, Hungary, John von Neumann was a child prodigy who could divide eight-
digit numbers in his head when he was only six years old. Due to the political unrest in
Europe, he came to America, where, in 1933, he became one of the six original professors
of mathematics at the Institute for Advanced Study at Princeton University, a position he
retained for the rest of his life. During his career, von Neumann’s genius touched mathematics
(pure and applied), chemistry, physics, economics, and computer science, and he is generally
considered to be among the best scientists and mathematicians of the twentieth century.

290
Chapter 5
Norms, Inner Products, and Orthogonality
Parallelogram Identity
For a given norm ∥⋆∥on a vector space V, there exists an inner product
on V such that ⟨⋆⋆⟩= ∥⋆∥2 if and only if the parallelogram identity
∥x + y∥2 + ∥x −y∥2 = 2

∥x∥2 + ∥y∥2 
(5.3.7)
holds for all x, y ∈V.
Proof.
Consider real spaces—complex spaces are discussed in Exercise 5.3.6. If
there exists an inner product such that ⟨⋆⋆⟩= ∥⋆∥2 , then the parallelogram
identity is immediate because ⟨x + y x + y⟩+⟨x −y x −y⟩= 2 ⟨x x⟩+2 ⟨y y⟩.
The diﬃcult part is establishing the converse. Suppose ∥⋆∥satisﬁes the paral-
lelogram identity, and prove that the function
⟨x y⟩= 1
4

∥x + y∥2 −∥x −y∥2 
(5.3.8)
is an inner product for V such that ⟨x x⟩= ∥x∥2 for all x by showing the four
deﬁning conditions (5.3.1) hold. The ﬁrst and fourth conditions are immediate.
To establish the third, use the parallelogram identity to write
∥x + y∥2 + ∥x + z∥2 = 1
2

∥x + y + x + z∥2 + ∥y −z∥2 
,
∥x −y∥2 + ∥x −z∥2 = 1
2

∥x −y + x −z∥2 + ∥z −y∥2 
,
and then subtract to obtain
∥x + y∥2−∥x −y∥2+∥x + z∥2−∥x −z∥2 = ∥2x + (y + z)∥2 −∥2x −(y + z)∥2
2
.
Consequently,
⟨x y⟩+ ⟨x z⟩= 1
4

∥x + y∥2 −∥x −y∥2 + ∥x + z∥2 −∥x −z∥2 
= 1
8

∥2x + (y + z)∥2 −∥2x −(y + z)∥2 
= 1
2
x + y + z
2

2
−
x −y + z
2

2
= 2

x y + z
2

,
(5.3.9)
and setting z = 0 produces the statement that ⟨x y⟩= 2 ⟨x y/2⟩for all y ∈V.
Replacing y by y + z yields ⟨x y + z⟩= 2 ⟨x (y + z)/2⟩, and thus (5.3.9)

5.3 Inner-Product Spaces
291
guarantees that ⟨x y⟩+ ⟨x z⟩= ⟨x y + z⟩. Now prove that ⟨x αy⟩= α ⟨x y⟩
for all real α. This is valid for integer values of α by the result just established,
and it holds when α is rational because if β and γ are integers, then
γ2

x β
γ y

= ⟨γx βy⟩= βγ ⟨x y⟩
=⇒

x β
γ y

= β
γ ⟨x y⟩.
Because ∥x + αy∥and ∥x −αy∥are continuous functions of α (Exercise
5.1.7), equation (5.3.8) insures that ⟨x αy⟩is a continuous function of α. There-
fore, if α is irrational, and if {αn} is a sequence of rational numbers such that
αn →α, then ⟨x αny⟩→⟨x αy⟩and ⟨x αny⟩= αn ⟨x y⟩→α ⟨x y⟩, so
⟨x αy⟩= α ⟨x y⟩.
Example 5.3.4
We already know that the euclidean vector norm on Cn is generated by the stan-
dard inner product, so the previous theorem guarantees that the parallelogram
identity must hold for the 2-norm. This is easily corroborated by observing that
∥x + y∥2
2 + ∥x −y∥2
2 = (x + y)∗(x + y) + (x −y)∗(x −y)
= 2 (x∗x + y∗y) = 2(∥x∥2
2 + ∥y∥2
2).
The parallelogram identity is so named because it expresses the fact that the
sum of the squares of the diagonals in a parallelogram is twice the sum of the
squares of the sides. See the following diagram.
x
y
x + y
||x||
||y||
||x + y||
||x - y||
Example 5.3.5
Problem: Except for the euclidean norm, is any other vector p-norm generated
by an inner product?
Solution: No, because the parallelogram identity (5.3.7) doesn’t hold when
p ̸= 2. To see that ∥x + y∥2
p + ∥x −y∥2
p = 2

∥x∥2
p + ∥y∥2
p

is not valid for
all x, y ∈Cn when p ̸= 2, consider x = e1 and y = e2. It’s apparent that
∥e1 + e2∥2
p = 22/p = ∥e1 −e2∥2
p , so
∥e1 + e2∥2
p + ∥e1 −e2∥2
p = 2(p+2)/p
and
2

∥e1∥2
p + ∥e2∥2
p

= 4.

292
Chapter 5
Norms, Inner Products, and Orthogonality
Clearly, 2(p+2)/p = 4 only when p = 2. Details for the ∞-norm are asked for
in Exercise 5.3.7.
Conclusion: For applications that are best analyzed in the context of an inner-
product space (e.g., least squares problems), we are limited to the euclidean
norm or else to one of its variation such as the elliptical norm in (5.3.5).
Virtually all important statements concerning ℜn or Cn with the standard
inner product remain valid for general inner-product spaces—e.g., consider the
statement and proof of the general CBS inequality. Advanced or more theoretical
texts prefer a development in terms of general inner-product spaces. However,
the focus of this text is matrices and the coordinate spaces ℜn and Cn, so
subsequent discussions will usually be phrased in terms of ℜn or Cn and their
standard inner products. But remember that extensions to more general inner-
product spaces are always lurking in the background, and we will not hesitate
to use these generalities or general inner-product notation when they serve our
purpose.
Exercises for section 5.3
5.3.1. For x =
 x1
x2
x3

, y =
 y1
y2
y3

, determine which of the following are inner
products for ℜ3×1.
(a)
⟨x y⟩= x1y1 + x3y3,
(b)
⟨x y⟩= x1y1 −x2y2 + x3y3,
(c)
⟨x y⟩= 2x1y1 + x2y2 + 4x3y3,
(d)
⟨x y⟩= x2
1y2
1 + x2
2y2
2 + x2
3y2
3.
5.3.2. For a general inner-product space V, explain why each of the following
statements must be true.
(a)
If ⟨x y⟩= 0 for all x ∈V, then y = 0.
(b)
⟨αx y⟩= α ⟨x y⟩for all x, y ∈V and for all scalars α.
(c)
⟨x + y z⟩= ⟨x z⟩+ ⟨y z⟩for all x, y, z ∈V.
5.3.3. Let V be an inner-product space with an inner product ⟨x y⟩. Explain
why the function deﬁned by ∥⋆∥=

⟨⋆⋆⟩satisﬁes the ﬁrst two norm
properties in (5.2.3) on p. 280.
5.3.4. For a real inner-product space with ∥⋆∥2 = ⟨⋆⋆⟩, derive the inequality
⟨x y⟩≤∥x∥2 + ∥y∥2
2
.
Hint: Consider x −y.

5.3 Inner-Product Spaces
293
5.3.5. For n × n matrices A and B, explain why each of the following in-
equalities is valid.
(a)
|trace (B)|2 ≤n [trace (B∗B)] .
(b)
trace

B2
≤trace

BT B

for real matrices.
(c)
trace

AT B

≤trace

AT A

+ trace

BT B

2
for real matrices.
5.3.6. Extend the proof given on p. 290 concerning the parallelogram identity
(5.3.7) to include complex spaces. Hint: If V is a complex space with
a norm ∥⋆∥that satisﬁes the parallelogram identity, let
⟨x y⟩r = ∥x + y∥2 −∥x −y∥2
4
,
and prove that
⟨x y⟩= ⟨x y⟩r + i ⟨ix y⟩r
(the polarization identity)
(5.3.10)
is an inner product on V.
5.3.7. Explain why there does not exist an inner product on Cn (n ≥2) such
that ∥⋆∥∞=

⟨⋆⋆⟩.
5.3.8. Explain why the Frobenius matrix norm on Cn×n must satisfy the par-
allelogram identity.
5.3.9. For n ≥2, is either the matrix 1-, 2-, or ∞-norm generated by an inner
product on Cn×n?

294
Chapter 5
Norms, Inner Products, and Orthogonality
5.4
ORTHOGONAL VECTORS
Two vectors in ℜ3 are orthogonal (perpendicular) if the angle between them is
a right angle (90◦). But the visual concept of a right angle is not at our disposal in
higher dimensions, so we must dig a little deeper. The essence of perpendicularity
in ℜ2 and ℜ3 is embodied in the classical Pythagorean theorem,
u
|| u ||
v
|| v  ||
|| u - v ||
which says that u and v are orthogonal if and only if ∥u∥2 +∥v∥2 = ∥u −v∥2 .
But
39 ∥u∥2 = uT u for all u ∈ℜ3, and uT v = vT u, so we can rewrite the
Pythagorean statement as
0 = ∥u∥2 + ∥v∥2 −∥u −v∥2 = uT u + vT v −(u −v)T (u −v)
= uT u + vT v −

uT u −uT v −vT u + vT v

= 2uT v.
Therefore, u and v are orthogonal vectors in ℜ3 if and only if uT v = 0. The
natural extension of this provides us with a deﬁnition in more general spaces.
Orthogonality
In an inner-product space V, two vectors x, y ∈V are said to be
orthogonal (to each other) whenever ⟨x y⟩= 0, and this is denoted
by writing x ⊥y.
•
For ℜn with the standard inner product, x ⊥y ⇐⇒xT y = 0.
•
For Cn with the standard inner product, x ⊥y ⇐⇒x∗y = 0.
Example 5.4.1
x =


1
−2
3
−1

is orthogonal to y =


4
1
−2
−4

because xT y = 0.
39
Throughout this section, only norms generated by an underlying inner product ∥⋆∥2 = ⟨⋆⋆⟩
are used, so distinguishing subscripts on the norm notation can be omitted.

5.4 Orthogonal Vectors
295
In spite of the fact that uT v = 0, the vectors u =
 i
3
1

and v =
 i
0
1

are
not orthogonal because u∗v ̸= 0.
Now that “right angles” in higher dimensions make sense, how can more
general angles be deﬁned? Proceed just as before, but use the law of cosines
rather than the Pythagorean theorem. Recall that
u
|| u ||
v
|| v  ||
|| u - v ||
θ
the law of cosines in ℜ2 or ℜ3 says ∥u −v∥2 = ∥u∥2+∥v∥2−2 ∥u∥∥v∥cos θ.
If u and v are orthogonal, then this reduces to the Pythagorean theorem. But,
in general,
cos θ = ∥u∥2 + ∥v∥2 −∥u −v∥2
2 ∥u∥∥v∥
= uT u + vT v −(u −v)T (u −v)
2 ∥u∥∥v∥
=
2uT v
2 ∥u∥∥v∥=
uT v
∥u∥∥v∥.
This easily extends to higher dimensions because if x, y are vectors from any real
inner-product space, then the general CBS inequality (5.3.4) on p. 287 guarantees
that ⟨x y⟩/ ∥x∥∥y∥is a number in the interval [−1, 1], and hence there is a
unique value θ in [0, π] such that cos θ = ⟨x y⟩/ ∥x∥∥y∥.
Angles
In a real inner-product space V, the radian measure of the angle be-
tween nonzero vectors x, y ∈V is deﬁned to be the number θ ∈[0, π]
such that
cos θ =
⟨x y⟩
∥x∥∥y∥.
(5.4.1)

296
Chapter 5
Norms, Inner Products, and Orthogonality
Example 5.4.2
In ℜn,
cos θ = xT y/ ∥x∥∥y∥. For example, to determine the angle between
x =


−4
2
1
2

and y =


1
0
2
2

, compute cos θ = 2/(5)(3) = 2/15, and use the
inverse cosine function to conclude that θ = 1.437 radians (rounded).
Example 5.4.3
Linear Correlation. Suppose that an experiment is conducted, and the result-
ing observations are recorded in two data vectors
x =




x1
x2
...
xn



, y =




y1
y2
...
yn



,
and let e =




1
1
...
1



.
Problem: Determine to what extent the yi ’s are linearly related to the xi ’s.
That is, measure how close y is to being a linear combination β0e + β1x.
Solution: The cosine as deﬁned in (5.4.1) does the job. To understand how, let
µx and σx be the mean and standard deviation of the data in x. That is,
µx =

i xi
n
= eT x
n
and
σx =

i(xi −µx)2
n
= ∥x −µxe∥2
√n
.
The mean is a measure of central tendency, and the standard deviation mea-
sures the extent to which the data is spread. Frequently, raw data from diﬀerent
sources is diﬃcult to compare because the units of measure are diﬀerent—e.g.,
one researcher may use the metric system while another uses American units. To
compensate, data is almost always ﬁrst “standardized” into unitless quantities.
The standardization of a vector x for which σx ̸= 0 is deﬁned to be
zx = x −µxe
σx
.
Entries in zx are often referred to as standard scores or z-scores. All stan-
dardized vectors have the properties that ∥z∥= √n,
µz = 0, and σz = 1.
Furthermore, it’s not diﬃcult to verify that for vectors x and y such that
σx ̸= 0 and σy ̸= 0, it’s the case that
zx = zy ⇐⇒∃constants β0, β1 such that y = β0e + β1x,
where
β1 > 0,
zx = −zy ⇐⇒∃constants β0, β1 such that y = β0e + β1x,
where
β1 < 0.
•
In other words, y = β0e+β1x for some β0 and β1 if and only if zx = ±zy,
in which case we say y is perfectly linearly correlated with x.

5.4 Orthogonal Vectors
297
Since zx varies continuously with x, the existence of a “near” linear relationship
between x and y is equivalent to zx being “close” to ±zy in some sense. The
fact that ∥zx∥= ∥±zy∥= √n means zx and ±zy diﬀer only in orientation,
so a natural measure of how close zx is to ±zy is cos θ, where θ is the angle
between zx and zy. The number
ρxy = cos θ =
zxT zy
∥zx∥∥zy∥= zxT zy
n
= (x −µxe)T (y −µye)
∥x −µxe∥∥y −µye∥
is called the coeﬃcient of linear correlation, and the following facts are now
immediate.
•
ρxy = 0 if and only if x and y are orthogonal, in which case we say that
x and y are completely uncorrelated.
•
|ρxy| = 1 if and only if y is perfectly correlated with x. That is, |ρxy| = 1
if and only if there exists a linear relationship y = β0e + β1x.
▷
When β1 > 0, we say that y is positively correlated with x.
▷
When β1 < 0, we say that y is negatively correlated with x.
•
|ρxy| measures the degree to which y is linearly related to x. In other
words, |ρxy| ≈1 if and only if y ≈β0e + β1x for some β0 and β1.
▷
Positive correlation is measured by the degree to which ρxy ≈1.
▷
Negative correlation is measured by the degree to which ρxy ≈−1.
If the data in x and y are plotted in ℜ2 as points (xi, yi), then, as depicted in
Figure 5.4.1, ρxy ≈1 means that the points lie near a straight line with positive
slope, while ρxy ≈−1 means that the points lie near a line with negative slope,
and ρxy ≈0 means that the points do not lie near a straight line.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
..
. .
. .
... .
.
.. .
.
.
....
.
. .
. .
.. .
.
..
. .
.
...
. ..
.
..
.
.
....
.
.
. .
.
..
.
.
.
Positive Correlation
No Correlation
Negative Correlation
ρxy ≈1
ρxy ≈−1
ρxy ≈0
Figure 5.4.1
If |ρxy| ≈1, then the theory of least squares as presented in §4.6 can be used
to determine a “best-ﬁtting” straight line.

298
Chapter 5
Norms, Inner Products, and Orthogonality
Orthonormal Sets
B = {u1, u2, . . . , un} is called an orthonormal set whenever ∥ui∥= 1
for each i, and ui ⊥uj for all i ̸= j. In other words,
⟨ui uj⟩=

1
when i = j,
0
when i ̸= j.
•
Every orthonormal set is linearly independent.
(5.4.2)
•
Every orthonormal set of n vectors from an n-dimensional space V
is an orthonormal basis for V.
Proof.
The second point follows from the ﬁrst. To prove the ﬁrst statement,
suppose B = {u1, u2, . . . , un} is orthonormal. If 0 = α1u1 +α2u2 +· · ·+αnun,
use the properties of an inner product to write
0 = ⟨ui 0⟩= ⟨ui α1u1 + α2u2 + · · · + αnun⟩
= α1 ⟨ui u1⟩+ · · · + αi ⟨ui ui⟩+ · · · + αn ⟨ui un⟩= αi ∥ui∥2
= αi
for each i.
Example 5.4.4
The set B′ =

u1 =

1
−1
0

, u2 =
 1
1
1

, u3 =
 −1
−1
2
!
is a set of mutually
orthogonal vectors because uT
i uj = 0 for i ̸= j, but B′ is not an orthonormal
set—each vector does not have unit length. However, it’s easy to convert an
orthogonal set (not containing a zero vector) into an orthonormal set by simply
normalizing each vector. Since ∥u1∥=
√
2,
∥u2∥=
√
3, and ∥u3∥=
√
6, it
follows that B =

u1/
√
2, u2/
√
3, u3/
√
6

is orthonormal.
The most common orthonormal basis is S = {e1, e2, . . . , en} , the stan-
dard basis for ℜn and Cn, and, as illustrated below for ℜ2 and ℜ3, these
orthonormal vectors are directed along the standard coordinate axes.
e1
e2
x
y
e1
e2
e3
x
y
z

5.4 Orthogonal Vectors
299
Another orthonormal basis B need not be directed in the same way as S, but
that’s the only signiﬁcant diﬀerence because it’s geometrically evident that B
must amount to some rotation of S. Consequently, we should expect general
orthonormal bases to provide essentially the same advantages as the standard
basis. For example, an important function of the standard basis S for ℜn is to
provide coordinate representations by writing
x = [x]S =




x1
x2
...
xn




to mean
x = x1e1 + x2e2 + · · · + xnen.
With respect to a general basis B = {u1, u2, . . . , un} , the coordinates of x
are the scalars ξi in the representation x = ξ1u1 + ξ2u2 + · · · + ξnun, and, as
illustrated in Example 4.7.2, ﬁnding the ξi ’s requires solving an n × n system,
a nuisance we would like to avoid. But if B is an orthonormal basis, then the
ξi ’s are readily available because ⟨ui x⟩= ⟨ui ξ1u1 + ξ2u2 + · · · + ξnun⟩=
n
j=1 ξj ⟨ui uj⟩= ξi ∥ui∥2 = ξi. This yields the Fourier
40 expansion of x.
Fourier Expansions
If B = {u1, u2, . . . , un} is an orthonormal basis for an inner-product
space V, then each x ∈V can be expressed as
x = ⟨u1 x⟩u1 + ⟨u2 x⟩u2 + · · · + ⟨un x⟩un.
(5.4.3)
This is called the Fourier expansion of x. The scalars ξi = ⟨ui x⟩
are the coordinates of x with respect to B, and they are called the
Fourier coeﬃcients. Geometrically, the Fourier expansion resolves x
into n mutually orthogonal vectors ⟨ui x⟩ui, each of which represents
the orthogonal projection of x onto the space (line) spanned by ui.
(More is said in Example 5.13.1 on p. 431 and Exercise 5.13.11.)
40
Jean Baptiste Joseph Fourier (1768–1830) was a French mathematician and physicist who,
while studying heat ﬂow, developed expansions similar to (5.4.3). Fourier’s work dealt with
special inﬁnite-dimensional inner-product spaces involving trigonometric functions as discussed
in Example 5.4.6. Although they were apparently used earlier by Daniel Bernoulli (1700–1782)
to solve problems concerned with vibrating strings, these orthogonal expansions became known
as Fourier series, and they are now a fundamental tool in applied mathematics. Born the son
of a tailor, Fourier was orphaned at the age of eight. Although he showed a great aptitude for
mathematics at an early age, he was denied his dream of entering the French artillery because
of his “low birth.” Instead, he trained for the priesthood, but he never took his vows. However,
his talents did not go unrecognized, and he later became a favorite of Napoleon. Fourier’s work
is now considered as marking an epoch in the history of both pure and applied mathematics.
The next time you are in Paris, check out Fourier’s plaque on the ﬁrst level of the Eiﬀel Tower.

300
Chapter 5
Norms, Inner Products, and Orthogonality
Example 5.4.5
Problem: Determine the Fourier expansion of x =
 −1
2
1

with respect to the
standard inner product and the orthonormal basis given in Example 5.4.4
B =


u1 =
1
√
2


1
−1
0

, u2 =
1
√
3


1
1
1

, u3 =
1
√
6


−1
−1
2




.
Solution: The Fourier coeﬃcients are
ξ1 = ⟨u1 x⟩= −3
√
2,
ξ2 = ⟨u2 x⟩=
2
√
3,
ξ3 = ⟨u3 x⟩=
1
√
6,
so
x = ξ1u1 + ξ2u2 + ξ3u3 = 1
2


−3
3
0

+ 1
3


2
2
2

+ 1
6


−1
−1
2

.
You may ﬁnd it instructive to sketch a picture of these vectors in ℜ3.
Example 5.4.6
Fourier Series.
Let V be the inner-product space of real-valued functions
that are integrable on the interval (−π, π) and where the inner product and
norm are given by
⟨f|g⟩=
 π
−π
f(t)g(t)dt
and
∥f∥=
 π
−π
f 2(t)dt
1/2
.
It’s straightforward to verify that the set of trigonometric functions
B′ = {1, cos t, cos 2t, . . . , sin t, sin 2t, sin 3t, . . .}
is a set of mutually orthogonal vectors, so normalizing each vector produces the
orthonormal set
B =

1
√
2π , cos t
√π , cos 2t
√π , . . . , sin t
√π , sin 2t
√π , sin 3t
√π , . . .
!
.
Given an arbitrary f ∈V, we construct its Fourier expansion
F(t) = α0
1
√
2π +
∞

k=1
αk
cos kt
√π
+
∞

k=1
βk
sin kt
√π ,
(5.4.4)

5.4 Orthogonal Vectors
301
where the Fourier coeﬃcients are given by
α0 =

1
√
2π f

=
1
√
2π
 π
−π
f(t)dt ,
αk =
cos kt
√π
f

=
1
√π
 π
−π
f(t) cos kt dt
for k = 1, 2, 3, . . . ,
βk =
sin kt
√π
f

=
1
√π
 π
−π
f(t) sin kt dt
for k = 1, 2, 3, . . . .
Substituting these coeﬃcients in (5.4.4) produces the inﬁnite series
F(t) = a0
2 +
∞

n=1
(an cos nt + bn sin nt) ,
(5.4.5)
where
an = 1
π
 π
−π
f(t) cos nt dt
and
bn = 1
π
 π
−π
f(t) sin nt dt.
(5.4.6)
The series F(t) in (5.4.5) is called the Fourier series expansion for f(t), but,
unlike the situation in ﬁnite-dimensional spaces, F(t) need not agree with the
original function f(t). After all, F is periodic, so there is no hope of agreement
when f is not periodic. However, the following statement is true.
•
If f(t) is a periodic function with period 2π that is sectionally continu-
ous
41 on the interval (−π, π), then the Fourier series F(t) converges to
f(t) at each t ∈(−π, π), where f is continuous. If f is discontinuous
at t0 but possesses left-hand and right-hand derivatives at t0, then F(t0)
converges to the average value
F(t0) = f(t−
0 ) + f(t+
0 )
2
,
where f(t−
0 ) and f(t+
0 ) denote the one-sided limits f(t−
0 ) = limt→t−
0 f(t)
and f(t+
0 ) = limt→t+
0 f(t).
For example, the square wave function deﬁned by
f(t) =

−1
when −π < t < 0,
1
when
0 < t < π,
41
A function f is sectionally continuous on (a, b) when f has only a ﬁnite number of discon-
tinuities in (a, b) and the one-sided limits exist at each point of discontinuity as well as at the
end points a and b.

302
Chapter 5
Norms, Inner Products, and Orthogonality
and illustrated in Figure 5.4.2, satisﬁes these conditions. The value of f at t = 0
is irrelevant—it’s not even necessary that f(0) be deﬁned.
π
1
−π
−1
Figure 5.4.2
To ﬁnd the Fourier series expansion for f, compute the coeﬃcients in (5.4.6) as
an = 1
π
 π
−π
f(t) cos nt dt = 1
π
 0
−π
−cos nt dt + 1
π
 π
0
cos nt dt
= 0,
bn = 1
π
 π
−π
f(t) sin nt dt = 1
π
 0
−π
−sin nt dt + 1
π
 π
0
sin nt dt
= 2
nπ (1 −cos nπ) =

0
when n is even,
4/nπ
when n is odd,
so that
F(t) = 4
π sin t + 4
3π sin 3t + 4
5π sin 5t + · · · =
∞

n=1
4
(2n −1)π sin(2n −1)t.
For each t ∈(−π, π), except t = 0, it must be the case that F(t) = f(t), and
F(0) = f(0−) + f(0+)
2
= 0.
Not only does F(t) agree with f(t) everywhere f is deﬁned, but F also pro-
vides a periodic extension of f in the sense that the graph of F(t) is the entire
square wave depicted in Figure 5.4.2—the values at the points of discontinuity
(the jumps) are F(±nπ) = 0.

5.4 Orthogonal Vectors
303
Exercises for section 5.4
5.4.1. Using the standard inner product, determine which of the following pairs
are orthogonal vectors in the indicated space.
(a)
x =


1
−3
4


and
y =


−2
2
2


in ℜ3,
(b)
x =



i
1 + i
2
1 −i



and
y =



0
1 + i
−2
1 −i



in C4,
(c)
x =



1
−2
3
4



and
y =



4
2
−1
1



in ℜ4,
(d)
x =


1 + i
1
i


and
y =


1 −i
−3
−i


in C3,
(e)
x =




0
0
...
0




and
y =




y1
y2
...
yn




in ℜn.
5.4.2. Find two vectors of unit norm that are orthogonal to u =

3
−2

.
5.4.3. Consider the following set of three vectors.





x1 =



1
−1
0
2


, x2 =



1
1
1
0


, x3 =



−1
−1
2
0








.
(a)
Using the standard inner product in ℜ4, verify that these vec-
tors are mutually orthogonal.
(b)
Find a nonzero vector x4 such that {x1, x2, x3, x4} is a set
of mutually orthogonal vectors.
(c)
Convert the resulting set into an orthonormal basis for ℜ4.
5.4.4. Using the standard inner product, determine the Fourier expansion of
x with respect to B, where
x =


1
0
−2


and
B =



1
√
2


1
−1
0

,
1
√
3


1
1
1

,
1
√
6


−1
−1
2




.

304
Chapter 5
Norms, Inner Products, and Orthogonality
5.4.5. With respect to the inner product for matrices given by (5.3.2), verify
that the set
B =
 1
√
2

0
1
1
0

,
1
√
2

1
0
0
−1

,
1
2

1
−1
1
1

,
1
2

1
1
−1
1
!
is an orthonormal basis for ℜ2×2, and then compute the Fourier expan-
sion of A =
 1
1
1
1

with respect to B.
5.4.6. Determine the angle between x =

2
−1
1

and y =
 1
1
2

.
5.4.7. Given an orthonormal basis B for a space V, explain why the Fourier
expansion for x ∈V is uniquely determined by B.
5.4.8. Explain why the columns of Un×n are an orthonormal basis for Cn if
and only if U∗= U−1. Such matrices are said to be unitary—their
properties are studied in a later section.
5.4.9. Matrices with the property A∗A = AA∗are said to be normal. No-
tice that hermitian matrices as well as real symmetric matrices are in-
cluded in the class of normal matrices. Prove that if A is normal, then
R (A) ⊥N (A)—i.e., every vector in R (A) is orthogonal to every vec-
tor in N (A). Hint: Recall equations (4.5.5) and (4.5.6).
5.4.10. Using the trace inner product described in Example 5.3.1, determine the
angle between the following pairs of matrices.
(a)
I =

1
0
0
1

and
B =

1
1
1
1

.
(b)
A =

1
3
2
4

and
B =

2
−2
2
0

.
5.4.11. Why is the deﬁnition for cos θ given in (5.4.1) not good for Cn? Explain
how to deﬁne cos θ so that it makes sense in Cn.
5.4.12. If {u1, u2, . . . , un} is an orthonormal basis for an inner-product space
V, explain why
⟨x y⟩=

i
⟨x ui⟩⟨ui y⟩
holds for every x, y ∈V.

5.4 Orthogonal Vectors
305
5.4.13. Consider a real inner-product space, where ∥⋆∥2 = ⟨⋆⋆⟩.
(a)
Prove that if ∥x∥= ∥y∥, then (x + y) ⊥(x −y).
(b)
For the standard inner product in ℜ2, draw a picture of this.
That is, sketch the location of x + y and x −y for two vectors
with equal norms.
5.4.14. Pythagorean Theorem.
Let V be a general inner-product space in
which ∥⋆∥2 = ⟨⋆⋆⟩.
(a)
When V is a real space, prove that x ⊥y if and only if
∥x + y∥2 = ∥x∥2 + ∥y∥2 . (Something would be wrong if this
were not true because this is where the deﬁnition of orthogonal-
ity originated.)
(b)
Construct an example to show that one of the implications in
part (a) does not hold when V is a complex space.
(c)
When V is a complex space, prove that x ⊥y if and only if
∥αx + βy∥2 = ∥αx∥2 + ∥βy∥2 for all scalars α and β.
5.4.15. Let B = {u1, u2, . . . , un} be an orthonormal basis for an inner-product
space V, and let x = 
i ξiui be the Fourier expansion of x ∈V.
(a)
If V is a real space, and if θi is the angle between ui and x,
explain why
ξi = ∥x∥cos θi.
Sketch a picture of this in ℜ2 or ℜ3 to show why the com-
ponent ξiui represents the orthogonal projection of x onto
the line determined by ui, and thus illustrate the fact that a
Fourier expansion is nothing more than simply resolving x into
mutually orthogonal components.
(b)
Derive Parseval’s identity,
42 which says n
i=1 |ξi|2 = ∥x∥2 .
5.4.16. Let B = {u1, u2, . . . , uk} be an orthonormal set in an n-dimensional
inner-product space V. Derive Bessel’s inequality,
43 which says that
if x ∈V and ξi = ⟨ui x⟩, then
k

i=1
|ξi|2 ≤∥x∥2 .
Explain why equality holds if and only if x ∈span {u1, u2, . . . , uk} .
Hint: Consider ∥x −k
i=1 ξiui∥2.
42
This result appeared in the second of the ﬁve mathematical publications by Marc-Antoine
Parseval des Chˆenes (1755–1836). Parseval was a royalist who had to ﬂee from France when
Napoleon ordered his arrest for publishing poetry against the regime.
43
This inequality is named in honor of the German astronomer and mathematician Friedrich
Wilhelm Bessel (1784–1846), who devoted his life to understanding the motions of the stars.
In the process he introduced several useful mathematical ideas.

306
Chapter 5
Norms, Inner Products, and Orthogonality
5.4.17. Construct an example using the standard inner product in ℜn to show
that two vectors x and y can have an angle between them that is close
to π/2 without xT y being close to 0. Hint: Consider n to be large,
and use the vector e of all 1’s for one of the vectors.
5.4.18. It was demonstrated in Example 5.4.3 that y is linearly correlated with
x in the sense that y ≈β0e + β1x if and only if the standardization
vectors zx and zy are “close” in the sense that they are almost on the
same line in ℜn. Explain why simply measuring ∥zx −zy∥2 does not
always gauge the degree of linear correlation.
5.4.19. Let θ be the angle between two vectors x and y from a real inner-
product space.
(a)
Prove that cos θ = 1 if and only if y = αx for α > 0.
(b)
Prove that cos θ = −1 if and only if y = αx for α < 0.
Hint: Use the generalization of Exercise 5.1.9.
5.4.20. With respect to the orthonormal set
B =

1
√
2π , cos t
√π , cos 2t
√π , . . . , sin t
√π , sin 2t
√π , sin 3t
√π , . . .
!
,
determine the Fourier series expansion of the saw-toothed function
deﬁned by f(t) = t for −π < t < π. The periodic extension of this
function is depicted in Figure 5.4.3.
π
π
−π
−π
Figure 5.4.3

5.5 Gram–Schmidt Procedure
307
5.5
GRAM–SCHMIDT PROCEDURE
As discussed in §5.4, orthonormal bases possess signiﬁcant advantages over bases
that are not orthonormal. The spaces ℜn and Cn clearly possess orthonormal
bases (e.g., the standard basis), but what about other spaces? Does every ﬁnite-
dimensional space possess an orthonormal basis, and, if so, how can one be
produced? The Gram–Schmidt
44 orthogonalization procedure developed below
answers these questions.
Let B = {x1, x2, . . . , xn} be an arbitrary basis (not necessarily orthonormal)
for an n-dimensional inner-product space S, and remember that ∥⋆∥= ⟨⋆⋆⟩1/2.
Objective:
Use B to construct an orthonormal basis O = {u1, u2, . . . , un}
for S.
Strategy:
Construct O sequentially so that Ok = {u1, u2, . . . , uk} is an or-
thonormal basis for Sk = span {x1, x2, . . . , xk} for k = 1, . . . , n.
For k = 1, simply take u1 = x1/ ∥x1∥. It’s clear that O1 = {u1} is an
orthonormal set whose span agrees with that of S1 = {x1} . Now reason in-
ductively. Suppose that Ok = {u1, u2, . . . , uk} is an orthonormal basis for
Sk = span {x1, x2, . . . , xk} , and consider the problem of ﬁnding one additional
vector uk+1 such that Ok+1 = {u1, u2, . . . , uk, uk+1} is an orthonormal basis
for Sk+1 = span {x1, x2, . . . , xk, xk+1} . For this to hold, the Fourier expansion
(p. 299) of xk+1 with respect to Ok+1 must be
xk+1 =
k+1

i=1
⟨ui xk+1⟩ui,
which in turn implies that
uk+1 = xk+1 −k
i=1 ⟨ui xk+1⟩ui
⟨uk+1 xk+1⟩
.
(5.5.1)
Since ∥uk+1∥= 1, it follows from (5.5.1) that
| ⟨uk+1 xk+1⟩| =
xk+1 −
k

i=1
⟨ui xk+1⟩ui
,
44
Jorgen P. Gram (1850–1916) was a Danish actuary who implicitly presented the essence of or-
thogonalization procedure in 1883. Gram was apparently unaware that Pierre-Simon Laplace
(1749–1827) had earlier used the method. Today, Gram is remembered primarily for his de-
velopment of this process, but in earlier times his name was also associated with the matrix
product A∗A that historically was referred to as the Gram matrix of A.
Erhard Schmidt (1876–1959) was a student of Hermann Schwarz (of CBS inequality fame) and
the great German mathematician David Hilbert. Schmidt explicitly employed the orthogonal-
ization process in 1907 in his study of integral equations, which in turn led to the development
of what are now called Hilbert spaces. Schmidt made signiﬁcant use of the orthogonalization
process to develop the geometry of Hilbert Spaces, and thus it came to bear Schmidt’s name.

308
Chapter 5
Norms, Inner Products, and Orthogonality
so ⟨uk+1 xk+1⟩= eiθxk+1 −k
i=1 ⟨ui xk+1⟩ui
 for some 0 ≤θ < 2π, and
uk+1 =
xk+1 −k
i=1 ⟨ui xk+1⟩ui
eiθ
xk+1 −k
i=1 ⟨ui xk+1⟩ui

.
Since the value of θ in the scalar eiθ neither aﬀects span {u1, u2, . . . , uk+1} nor
the facts that ∥uk+1∥= 1 and ⟨uk+1 ui⟩= 0 for all i ≤k, we can arbitrarily
deﬁne uk+1 to be the vector corresponding to the θ = 0 or, equivalently,
eiθ = 1. For the sake of convenience, let
νk+1 =
xk+1 −
k

i=1
⟨ui xk+1⟩ui

so that we can write
u1 =
x1
∥x1∥
and
uk+1 = xk+1 −k
i=1 ⟨ui xk+1⟩ui
νk+1
for k > 0.
(5.5.2)
This sequence of vectors is called the Gram–Schmidt sequence. A straight-
forward induction argument proves that Ok = {u1, u2, . . . , uk} is indeed an or-
thonormal basis for span {x1, x2, . . . , xk} for each k = 1, 2, . . . . Details are
called for in Exercise 5.5.7.
The orthogonalization procedure deﬁned by (5.5.2) is valid for any inner-
product space, but if we concentrate on subspaces of ℜm or Cm with the stan-
dard inner product and euclidean norm, then we can formulate (5.5.2) in terms
of matrices. Suppose that B = {x1, x2, . . . , xn} is a basis for an n-dimensional
subspace S of Cm×1 so that the Gram–Schmidt sequence (5.5.2) becomes
u1 =
x1
∥x1∥
and
uk =
xk −k−1
i=1 (u∗
i xk) ui
xk −k−1
i=1 (u∗
i xk) ui

for k = 2, 3, . . . , n. (5.5.3)
To express this in matrix notation, set
U1 = 0m×1
and
Uk =

u1 | u2 | · · · | uk−1

m×k−1
for k > 1,
and notice that
U∗
kxk =




u∗
1xk
u∗
2xk
...
u∗
k−1xk




and
UkU∗
kxk =
k−1

i=1
ui (u∗
i xk) =
k−1

i=1
(u∗
i xk) ui.
Since
xk −
k−1

i=1
(u∗
i xk) ui = xk −UkU∗
kxk = (I −UkU∗
k) xk,
the vectors in (5.5.3) can be concisely written as
uk =
(I −UkU∗
k) xk
∥(I −UkU∗
k) xk∥
for k = 1, 2, . . . , n.
Below is a summary.

5.5 Gram–Schmidt Procedure
309
Gram–Schmidt Orthogonalization Procedure
If B = {x1, x2, . . . , xn} is a basis for a general inner-product space S,
then the Gram–Schmidt sequence deﬁned by
u1 =
x1
∥x1∥
and
uk =
xk −k−1
i=1 ⟨ui xk⟩ui
xk −k−1
i=1 ⟨ui xk⟩ui

for k = 2, . . . , n
is an orthonormal basis for S. When S is an n-dimensional subspace
of Cm×1, the Gram–Schmidt sequence can be expressed as
uk =
(I −UkU∗
k) xk
∥(I −UkU∗
k) xk∥
for
k = 1, 2, . . . , n
(5.5.4)
in which U1 = 0m×1 and Uk =

u1 | u2 | · · · | uk−1

m×k−1 for k > 1.
Example 5.5.1
Classical Gram–Schmidt Algorithm. The following formal algorithm is the
straightforward or “classical” implementation of the Gram–Schmidt procedure.
Interpret a ←b to mean that “a is deﬁned to be (or overwritten by) b.”
For k = 1:
u1 ←
x1
∥x1∥
For k > 1:
uk ←xk −
k−1

i=1
(u∗
i xk)ui
uk ←
uk
∥uk∥
(See Exercise 5.5.10 for other formulations of the Gram–Schmidt algorithm.)
Problem: Use the classical formulation of the Gram–Schmidt procedure given
above to ﬁnd an orthonormal basis for the space spanned by the following three
linearly independent vectors.
x1 =



1
0
0
−1


,
x2 =



1
2
0
−1


,
x3 =



3
1
1
−1


.

310
Chapter 5
Norms, Inner Products, and Orthogonality
Solution:
k = 1:
u1 ←
x1
∥x1∥=
1
√
2



1
0
0
−1



k = 2:
u2 ←x2 −(uT
1 x2)u1 =



0
2
0
0


,
u2 ←
u2
∥u2∥=



0
1
0
0



k = 3:
u3 ←x3 −(uT
1 x3)u1 −(uT
2 x3)u2 =



1
0
1
1


,
u3 ←
u3
∥u3∥=
1
√
3



1
0
1
1



Thus
u1 =
1
√
2



1
0
0
−1


,
u2 =



0
1
0
0


,
u3 =
1
√
3



1
0
1
1



is the desired orthonormal basis.
The Gram–Schmidt process frequently appears in the disguised form of a
matrix factorization. To see this, let Am×n =

a1 | a2 | · · · | an

be a matrix with
linearly independent columns. When Gram–Schmidt is applied to the columns
of A, the result is an orthonormal basis {q1, q2, . . . , qn} for R (A), where
q1 = a1
ν1
and
qk = ak −k−1
i=1 ⟨qi ak⟩qi
νk
for k = 2, 3, . . . , n,
where ν1 = ∥a1∥and νk =
ak −k−1
i=1 ⟨qi ak⟩qi
 for k > 1. The above
relationships can be rewritten as
a1 = ν1q1
and
ak = ⟨q1 ak⟩q1 + · · · + ⟨qk−1 ak⟩qk−1 + νkqk
for k > 1,
which in turn can be expressed in matrix form by writing

a1 | a2 | · · · | an

=

q1 | q2 | · · · | qn







ν1
⟨q1 a2⟩
⟨q1 a3⟩
· · ·
⟨q1 an⟩
0
ν2
⟨q2 a3⟩
· · ·
⟨q2 an⟩
0
0
ν3
· · ·
⟨q3 an⟩
...
...
...
...
...
0
0
0
· · ·
νn






.
This says that it’s possible to factor a matrix with independent columns as
Am×n = Qm×nRn×n, where the columns of Q are an orthonormal basis for
R (A) and R is an upper-triangular matrix with positive diagonal elements.

5.5 Gram–Schmidt Procedure
311
The factorization A = QR is called the QR factorization for A, and it is
uniquely determined by A (Exercise 5.5.8). When A and Q are not square,
some authors emphasize the point by calling A = QR the rectangular QR
factorization—the case when A and Q are square is further discussed on p. 345.
Below is a summary of the above observations.
QR Factorization
Every matrix Am×n with linearly independent columns can be uniquely
factored as A = QR in which the columns of Qm×n are an orthonor-
mal basis for R (A) and Rn×n is an upper-triangular matrix with
positive diagonal entries.
•
The QR factorization is the complete “road map” of the Gram–
Schmidt process because the columns of Q =

q1 | q2 | · · · | qn

are
the result of applying the Gram–Schmidt procedure to the columns
of A =

a1 | a2 | · · · | an

and R is given by
R =









ν1
q∗
1a2
q∗
1a3
· · ·
q∗
1an
0
ν2
q∗
2a3
· · ·
q∗
2an
0
0
ν3
· · ·
q∗
3an
...
...
...
...
...
0
0
0
· · ·
νn









,
where ν1 = ∥a1∥and νk =
ak −k−1
i=1 ⟨qi ak⟩qi
 for k > 1.
Example 5.5.2
Problem: Determine the QR factors of
A =


0
−20
−14
3
27
−4
4
11
−2

.
Solution: Using the standard inner product for ℜn, apply the Gram–Schmidt
procedure to the columns of A by setting
q1 = a1
ν1
and
qk = ak −k−1
i=1

qT
i ak

qi
νk
for k = 2, 3,
where ν1 = ∥a1∥and νk =
ak −k−1
i=1

qT
i ak

qi
. The computation of these
quantities can be organized as follows.

312
Chapter 5
Norms, Inner Products, and Orthogonality
k = 1:
r11 ←∥a1∥= 5
and
q1 ←a1
r11
=


0
3/5
4/5


k = 2:
r12 ←qT
1 a2 = 25
q2 ←a2 −r12q1 =


−20
12
−9


r22 ←∥q2∥= 25 and q2 ←q2
r22
= 1
25


−20
12
−9


k = 3:
r13 ←qT
1 a3 = −4 and r23 ←qT
2 a3 = 10
q3 ←a3 −r13q1 −r23q2 = 2
5


−15
−16
12


r33 ←∥q3∥= 10 and q3 ←q3
r33
= 1
25


−15
−16
12


Therefore,
Q = 1
25


0
−20
−15
15
12
−16
20
−9
12


and
R =


5
25
−4
0
25
10
0
0
10

.
We now have two important matrix factorizations, namely, the LU factor-
ization, discussed in §3.10 on p. 141 and the QR factorization. They are not the
same, but some striking analogies exist.
•
Each factorization represents a reduction to upper-triangular form—LU by
Gaussian elimination, and QR by Gram–Schmidt. In particular, the LU fac-
torization is the complete “road map” of Gaussian elimination applied to a
square nonsingular matrix, whereas QR is the complete road map of Gram–
Schmidt applied to a matrix with linearly independent columns.
•
When they exist, both factorizations A = LU and A = QR are uniquely
determined by A.
•
Once the LU factors (assuming they exist) of a nonsingular matrix A are
known, the solution of Ax = b is easily computed—solve Ly = b by
forward substitution, and then solve Ux = y by back substitution (see
p. 146). The QR factors can be used in a similar manner. If A ∈ℜn×n is
nonsingular, then QT = Q−1 (because Q has orthonormal columns), so
Ax = b
⇐⇒
QRx = b
⇐⇒
Rx = QT b, which is also a triangular
system that is solved by back substitution.

5.5 Gram–Schmidt Procedure
313
While the LU and QR factors can be used in more or less the same way to
solve nonsingular systems, things are diﬀerent for singular and rectangular cases
because Ax = b might be inconsistent, in which case a least squares solution
as described in §4.6, (p. 223) may be desired. Unfortunately, the LU factors of
A don’t exist when A is rectangular. And even if A is square and has an
LU factorization, the LU factors of A are not much help in solving the system
of normal equations AT Ax = AT b that produces least squares solutions. But
the QR factors of Am×n always exist as long as A has linearly independent
columns, and, as demonstrated in the following example, the QR factors provide
the least squares solution of an inconsistent system in exactly the same way as
they provide the solution of a consistent system.
Example 5.5.3
Application to the Least Squares Problem. If Ax = b is a possibly in-
consistent (real) system, then, as discussed on p. 226, the set of all least squares
solutions is the set of solutions to the system of normal equations
AT Ax = AT b.
(5.5.5)
But computing AT A and then performing an LU factorization of AT A to solve
(5.5.5) is generally not advisable. First, it’s ineﬃcient and, second, as pointed
out in Example 4.5.1, computing AT A with ﬂoating-point arithmetic can result
in a loss of signiﬁcant information. The QR approach doesn’t suﬀer from either
of these objections. Suppose that rank (Am×n) = n (so that there is a unique
least squares solution), and let A = QR be the QR factorization. Because the
columns of Q are an orthonormal set, it follows that QT Q = In, so
AT A = (QR)T (QR) = RT QT QR = RT R.
(5.5.6)
Consequently, the normal equations (5.5.5) can be written as
RT Rx = RT QT b.
(5.5.7)
But RT is nonsingular (it is triangular with positive diagonal entries), so (5.5.7)
simpliﬁes to become
Rx = QT b.
(5.5.8)
This is just an upper-triangular system that is eﬃciently solved by back substi-
tution. In other words, most of the work involved in solving the least squares
problem is in computing the QR factorization of A. Finally, notice that
x = R−1QT b =

AT A
−1AT b
is the solution of Ax = b when the system is consistent as well as the least
squares solution when the system is inconsistent (see p. 214). That is, with the
QR approach, it makes no diﬀerence whether or not Ax = b is consistent
because in both cases things boil down to solving the same equation—namely,
(5.5.8). Below is a formal summary.

314
Chapter 5
Norms, Inner Products, and Orthogonality
Linear Systems and the QR Factorization
If rank (Am×n) = n, and if A = QR is the QR factorization, then the
solution of the nonsingular triangular system
Rx = QT b
(5.5.9)
is either the solution or the least squares solution of Ax = b depending
on whether or not Ax = b is consistent.
It’s worthwhile to reemphasize that the QR approach to the least squares prob-
lem obviates the need to explicitly compute the product AT A. But if AT A is
ever needed, it is retrievable from the factorization AT A = RT R. In fact, this
is the Cholesky factorization of AT A as discussed in Example 3.10.7, p. 154.
The Gram–Schmidt procedure is a powerful theoretical tool, but it’s not a
good numerical algorithm when implemented in the straightforward or “classi-
cal” sense. When ﬂoating-point arithmetic is used, the classical Gram–Schmidt
algorithm applied to a set of vectors that is not already close to being an orthog-
onal set can produce a set of vectors that is far from being an orthogonal set. To
see this, consider the following example.
Example 5.5.4
Problem: Using 3-digit ﬂoating-point arithmetic, apply the classical Gram–
Schmidt algorithm to the set
x1 =


1
10−3
10−3

,
x2 =


1
10−3
0

,
x3 =


1
0
10−3

.
Solution:
k = 1:
fl ∥x1∥= 1, so u1 ←x1.
k = 2:
fl

uT
1 x2

= 1, so
u2 ←x2 −

uT
1 x2

u1 =


0
0
−10−3


and
u2 ←fl
 u2
∥u2∥

=


0
0
−1

.
k = 3:
fl

uT
1 x3

= 1 and fl

uT
2 x3

= −10−3, so
u3←x3−

uT
1 x3

u1−

uT
2 x3

u2=


0
−10−3
−10−3

and u3←fl
 u3
∥u3∥

=


0
−.709
−.709

.

5.5 Gram–Schmidt Procedure
315
Therefore, classical Gram–Schmidt with 3-digit arithmetic returns
u1 =


1
10−3
10−3

,
u2 =


0
0
−1

,
u3 =


0
−.709
−.709

,
(5.5.10)
which is unsatisfactory because u2 and u3 are far from being orthogonal.
It’s possible to improve the numerical stability of the orthogonalization pro-
cess by rearranging the order of the calculations. Recall from (5.5.4) that
uk =
(I −UkU∗
k) xk
∥(I −UkU∗
k) xk∥,
where
U1 = 0 and Uk =

u1 | u2 | · · · | uk−1

.
If E1 = I and Ei = I −ui−1u∗
i−1 for i > 1, then the orthogonality of the ui ’s
insures that
Ek · · · E2E1 = I −u1u∗
1 −u2u∗
2 −· · · −uk−1u∗
k−1 = I −UkU∗
k,
so the Gram–Schmidt sequence can also be expressed as
uk =
Ek · · · E2E1xk
∥Ek · · · E2E1xk∥
for k = 1, 2, . . . , n.
This means that the Gram–Schmidt sequence can be generated as follows:
{x1, x2, . . . , xn}
Normalize 1-st
−−−−−−−−−→{u1, x2, . . . , xn}
Apply E2
−−−−−−−−−→{u1, E2x2, E2x3, . . . , E2xn}
Normalize 2-nd
−−−−−−−−−→{u1, u2, E2x3, . . . , E2xn}
Apply E3
−−−−−−−−−→{u1, u2, E3E2x3, . . . , E3E2xn}
Normalize 3-rd
−−−−−−−−−→{u1, u2, u3, E3E2x4, . . . , E3E2xn} ,
etc.
While there is no theoretical diﬀerence, this “modiﬁed” algorithm is numerically
more stable than the classical algorithm when ﬂoating-point arithmetic is used.
The kth step of the classical algorithm alters only the kth vector, but the kth
step of the modiﬁed algorithm “updates” all vectors from the kth through the
last, and conditioning the unorthogonalized tail in this way makes a diﬀerence.

316
Chapter 5
Norms, Inner Products, and Orthogonality
Modiﬁed Gram–Schmidt Algorithm
For a linearly independent set {x1, x2, . . . , xn} ⊂Cm×1, the Gram–
Schmidt sequence given on p. 309 can be alternately described as
uk =
Ek · · · E2E1xk
∥Ek · · · E2E1xk∥with E1 = I, Ei = I −ui−1u∗
i−1 for i > 1,
and this sequence is generated by the following algorithm.
For k = 1:
u1 ←x1/ ∥x1∥
and
uj ←xj for j = 2, 3, . . . , n
For k > 1:
uj ←Ekuj = uj −

u∗
k−1uj

uk−1 for j = k, k + 1, . . . , n
uk ←uk/ ∥uk∥
(An alternate implementation is given in Exercise 5.5.10.)
To see that the modiﬁed version of Gram–Schmidt can indeed make a dif-
ference when ﬂoating-point arithmetic is used, consider the following example.
Example 5.5.5
Problem: Use 3-digit ﬂoating-point arithmetic, and apply the modiﬁed Gram–
Schmidt algorithm to the set given in Example 5.5.4 (p. 314), and then compare
the results of the modiﬁed algorithm with those of the classical algorithm.
Solution: x1 =


1
10−3
10−3

,
x2 =


1
10−3
0

,
x3 =


1
0
10−3

.
k = 1:
fl ∥x1∥= 1, so {u1, u2, u3} ←{x1, x2, x3} .
k = 2:
fl

uT
1 u2

= 1 and fl

uT
1 u3

= 1, so
u2 ←u2 −

uT
1 u2

u1 =


0
0
−10−3

,
u3 ←u3 −

uT
1 u3

u1 =


0
−10−3
0

,
and
u2 ←
u2
∥u2∥=


0
0
−1

.
k = 3:
uT
2 u3 = 0, so
u3 ←u3 −

uT
2 u3

u2 =


0
−10−3
0


and
u3 ←
u3
∥u3∥=


0
−1
0

.

5.5 Gram–Schmidt Procedure
317
Thus the modiﬁed Gram–Schmidt algorithm produces
u1 =


1
10−3
10−3

,
u2 =


0
0
−1

,
u3 =


0
−1
0

,
(5.5.11)
which is as good as one can expect using 3-digit arithmetic. Comparing (5.5.11)
with the result (5.5.10) obtained in Example 5.5.4 illuminates the advantage
possessed by modiﬁed Gram–Schmidt algorithm over the classical algorithm.
Below is a summary of some facts concerning the modiﬁed Gram–Schmidt
algorithm compared with the classical implementation.
Summary
•
When the Gram–Schmidt procedures (classical or modiﬁed) are ap-
plied to the columns of A using exact arithmetic, each produces an
orthonormal basis for R (A).
•
For computing a QR factorization in ﬂoating-point arithmetic, the
modiﬁed algorithm produces results that are at least as good as and
often better than the classical algorithm, but the modiﬁed algorithm
is not unconditionally stable—there are situations in which it fails
to produce a set of columns that are nearly orthogonal.
•
For solving the least square problem with ﬂoating-point arithmetic,
the modiﬁed procedure is a numerically stable algorithm in the sense
that the method described in Example 5.5.3 returns a result that is
the exact solution of a nearby least squares problem. However, the
Householder method described on p. 346 is just as stable and needs
slightly fewer arithmetic operations.
Exercises for section 5.5
5.5.1. Let S = span





x1 =



1
1
1
−1


,
x2 =



2
−1
−1
1


,
x3 =



−1
2
2
1








.
(a)
Use the classical Gram–Schmidt algorithm (with exact arith-
metic) to determine an orthonormal basis for S.
(b)
Verify directly that the Gram–Schmidt sequence produced in
part (a) is indeed an orthonormal basis for S.
(c)
Repeat part (a) using the modiﬁed Gram–Schmidt algorithm,
and compare the results.

318
Chapter 5
Norms, Inner Products, and Orthogonality
5.5.2. Use the Gram–Schmidt procedure to ﬁnd an orthonormal basis for the
four fundamental subspaces of A =
 1
−2
3
−1
2
−4
6
−2
3
−6
9
−3

.
5.5.3. Apply the Gram–Schmidt procedure with the standard inner product
for C3 to
 i
i
i

,
 0
i
i

,
 0
0
i
!
.
5.5.4. Explain what happens when the Gram–Schmidt process is applied to an
orthonormal set of vectors.
5.5.5. Explain what happens when the Gram–Schmidt process is applied to a
linearly dependent set of vectors.
5.5.6. Let A =


1
0
−1
1
2
1
1
1
−3
0
1
1

and b =


1
1
1
1

.
(a)
Determine the rectangular QR factorization of A.
(b)
Use the QR factors from part (a) to determine the least squares
solution to Ax = b.
5.5.7. Given a linearly independent set of vectors S = {x1, x2, . . . , xn} in an
inner-product space, let Sk = span {x1, x2, . . . , xk} for k = 1, 2, . . . , n.
Give an induction argument to prove that if Ok = {u1, u2, . . . , uk} is
the Gram–Schmidt sequence deﬁned in (5.5.2), then Ok is indeed an or-
thonormal basis for Sk = span {x1, x2, . . . , xk} for each k = 1, 2, . . . , n.
5.5.8. Prove that if rank (Am×n) = n, then the rectangular QR factorization
of A is unique. That is, if A = QR, where Qm×n has orthonormal
columns and Rn×n is upper triangular with positive diagonal entries,
then Q and R are unique. Hint: Recall Example 3.10.7, p. 154.
5.5.9.
(a)
Apply classical Gram–Schmidt with 3-digit ﬂoating-point arith-
metic to

x1 =

1
0
10−3

, x2 =
 1
0
0

, x3 =

1
10−3
0
!
. You may
assume that fl
√
2

= 1.41.
(b)
Again using 3-digit ﬂoating-point arithmetic, apply the modiﬁed
Gram–Schmidt algorithm to {x1, x2, x3} , and compare the re-
sult with that of part (a).

5.5 Gram–Schmidt Procedure
319
5.5.10. Depending on how the inner products rij are deﬁned, verify that the fol-
lowing code implements both the classical and modiﬁed Gram–Schmidt
algorithms applied to a set of vectors {x1, x2, . . . , xn} .
For j = 1 to n
uj ←−xj
For i = 1 to j −1
rij ←−
 ⟨ui xj⟩
(classical Gram–Schmidt)
⟨ui uj⟩
(modiﬁed Gram–Schmidt)
uj ←−uj −rijui
End
rjj ←−∥uj∥
If rjj = 0
quit
(because xj ∈span {x1, x2, . . . , xj−1} )
Else uj ←−uj/rjj
End
If exact arithmetic is used, will the inner products rij be the same for
both implementations?
5.5.11. Let V be the inner-product space of real-valued continuous functions
deﬁned on the interval [−1, 1], where the inner product is deﬁned by
⟨f g⟩=
 1
−1
f(x)g(x)dx,
and let S be the subspace of V that is spanned by the three linearly
independent polynomials q0 = 1,
q1 = x,
q2 = x2.
(a)
Use the Gram–Schmidt process to determine an orthonormal set
of polynomials {p0, p1, p2} that spans S. These polynomials
are the ﬁrst three normalized Legendre
45 polynomials.
(b)
Verify that pn satisﬁes Legendre’s diﬀerential equation
(1 −x2)y′′ −2xy′ + n(n + 1)y = 0
for n = 0, 1, 2. This equation and its solutions are of consider-
able importance in applied mathematics.
45
Adrien–Marie Legendre (1752–1833) was one of the most eminent French mathematicians of
the eighteenth century. His primary work in higher mathematics concerned number theory
and the study of elliptic functions. But he was also instrumental in the development of the
theory of least squares, and some people believe that Legendre should receive the credit that
is often aﬀorded to Gauss for the introduction of the method of least squares. Like Gauss and
many other successful mathematicians, Legendre spent substantial time engaged in diligent
and painstaking computation. It is reported that in 1824 Legendre refused to vote for the
government’s candidate for Institut National, so his pension was stopped, and he died in
poverty.

320
Chapter 5
Norms, Inner Products, and Orthogonality
5.6
UNITARY AND ORTHOGONAL MATRICES
The purpose of this section is to examine square matrices whose columns (or
rows) are orthonormal. The standard inner product and the euclidean 2-norm
are the only ones used in this section, so distinguishing subscripts are omitted.
Unitary and Orthogonal Matrices
•
A unitary matrix is deﬁned to be a complex matrix Un×n whose
columns (or rows) constitute an orthonormal basis for Cn.
•
An orthogonal matrix is deﬁned to be a real matrix Pn×n whose
columns (or rows) constitute an orthonormal basis for ℜn.
Unitary and orthogonal matrices have some nice features, one of which is
the fact that they are easy to invert. To see why, notice that the columns of
Un×n =

u1 | u2 | · · · |un

are an orthonormal set if and only if
[U∗U]ij = u∗
i uj =

1
when i = j,
0
when i ̸= j, ⇐⇒U∗U = I ⇐⇒U−1 = U∗.
Notice that because U∗U = I ⇐⇒UU∗= I, the columns of U are orthonor-
mal if and only if the rows of U are orthonormal, and this is why the deﬁnitions
of unitary and orthogonal matrices can be stated either in terms of orthonormal
columns or orthonormal rows.
Another nice feature is that multiplication by a unitary matrix doesn’t
change the length of a vector. Only the direction can be altered because
∥Ux∥2 = x∗U∗Ux = x∗x = ∥x∥2
∀x ∈Cn.
(5.6.1)
Conversely, if (5.6.1) holds, then U must be unitary. To see this, set x = ei
in (5.6.1) to observe u∗
i ui = 1 for each i, and then set x = ej + ek for j ̸= k
to obtain 0 = u∗
juk + u∗
kuj = 2 Re (u∗
juk) . By setting x = ej + iek in (5.6.1)
it also follows that 0 = 2 Im (u∗
juk) , so u∗
juk = 0 for each j ̸= k, and thus
(5.6.1) guarantees that U is unitary.
In the case of orthogonal matrices, everything is real so that (⋆)∗can be
replaced by (⋆)T . Below is a summary of these observations.

5.6 Unitary and Orthogonal Matrices
321
Characterizations
•
The following statements are equivalent to saying that a complex
matrix Un×n is unitary.
▷
U has orthonormal columns.
▷
U has orthonormal rows.
▷
U−1 = U∗.
▷
∥Ux∥2 = ∥x∥2 for every x ∈Cn×1.
•
The following statements are equivalent to saying that a real matrix
Pn×n is orthogonal.
▷
P has orthonormal columns.
▷
P has orthonormal rows.
▷
P−1 = PT .
▷
∥Px∥2 = ∥x∥2 for every x ∈ℜn×1.
Example 5.6.1
•
The identity matrix I is an orthogonal matrix.
•
All permutation matrices (products of elementary interchange matrices) are
orthogonal—recall Exercise 3.9.4.
•
The matrix
P =


1/
√
2
1/
√
3
−1/
√
6
−1/
√
2
1/
√
3
−1/
√
6
0
1/
√
3
2/
√
6


is an orthogonal matrix because PT P = PPT = I or, equivalently, because
the columns (and rows) constitute an orthonormal set.
•
The matrix U =
1
2
 1 + i
−1 + i
1 + i
1 −i

is unitary because U∗U = UU∗= I or,
equivalently, because the columns (and rows) are an orthonormal set.
•
An orthogonal matrix can be considered to be unitary, but a unitary matrix
is generally not orthogonal.
In general, a linear operator T on a vector space V with the property that
∥Tx∥= ∥x∥for all x ∈V is called an isometry on V. The isometries on ℜn
are precisely the orthogonal matrices, and the isometries on Cn are the unitary
matrices. The term “isometry” has an advantage in that it can be used to treat
the real and complex cases simultaneously, but for clarity we will often revert
back to the more cumbersome “orthogonal” and “unitary” terminology.

322
Chapter 5
Norms, Inner Products, and Orthogonality
The geometrical concepts of projection, reﬂection, and rotation are among
the most fundamental of all linear transformations in ℜ2 and ℜ3 (see Example
4.7.1 for three simple examples), so pursuing these ideas in higher dimensions
is only natural. The reﬂector and rotator given in Example 4.7.1 are isometries
(because they preserve length), but the projector is not. We are about to see
that the same is true in more general settings.
Elementary Orthogonal Projectors
For a vector u ∈Cn×1 such that ∥u∥= 1, a matrix of the form
Q = I −uu∗
(5.6.2)
is called an elementary orthogonal projector. More general projec-
tors are discussed on pp. 386 and 429.
To understand the nature of elementary projectors consider the situation in
ℜ3. Suppose that ∥u3×1∥= 1, and let u⊥denote the space (the plane through
the origin) consisting of all vectors that are perpendicular to u —we call u⊥the
orthogonal complement of u (a more general deﬁnition appears on p. 403).
The matrix Q = I−uuT is the orthogonal projector onto u⊥in the sense that
Q maps each x ∈ℜ3×1 to its orthogonal projection in u⊥as shown in Figure
5.6.1.
u ⊥
x
Qx = (I - uuT)x
u
(I - Q)x = uuTx
0
Figure 5.6.1
To see this, observe that each x can be resolved into two components
x = (I −Q)x + Qx,
where
(I −Q)x ⊥Qx.
The vector (I −Q)x = u(uT x) is on the line determined by u, and Qx is in
the plane u⊥because uT Qx = 0.

5.6 Unitary and Orthogonal Matrices
323
The situation is exactly as depicted in Figure 5.6.1. Notice that (I−Q)x =
uuT x is the orthogonal projection of x onto the line determined by u and
uuT x
 = |uT x|. This provides a nice interpretation of the magnitude of the
standard inner product. Below is a summary.
Geometry of Elementary Projectors
For vectors u, x ∈Cn×1 such that ∥u∥= 1,
•
(I −uu∗)x is the orthogonal projection of x onto the orthogonal
complement u⊥, the space of all vectors orthogonal to u;
(5.6.3)
•
uu∗x is the orthogonal projection of x onto the one-dimensional
space span {u} ;
(5.6.4)
•
|u∗x| represents the length of the orthogonal projection of x onto
the one-dimensional space span {u} .
(5.6.5)
In passing, note that elementary projectors are never isometries—they can’t
be because they are not unitary matrices in the complex case and not orthogonal
matrices in the real case. Furthermore, isometries are nonsingular but elementary
projectors are singular.
Example 5.6.2
Problem: Determine the orthogonal projection of x onto span {u} , and then
ﬁnd the orthogonal projection of x onto u⊥for x =
 2
0
1

and u =

2
−1
3

.
Solution: We cannot apply (5.6.3) and (5.6.4) directly because ∥u∦= 1, but
this is not a problem because

u
∥u∥
 = 1,
span {u} = span
 u
∥u∥
!
,
and
u⊥=
 u
∥u∥
⊥
.
Consequently, the orthogonal projection of x onto span {u} is given by
 u
∥u∥
  u
∥u∥
T
x = uuT
uT ux = 1
2


2
−1
3

,
and the orthogonal projection of x onto u⊥is

I −uuT
uT u

x = x −uuT x
uT u = 1
2


2
1
−1

.

324
Chapter 5
Norms, Inner Products, and Orthogonality
There is nothing special about the numbers in this example. For every nonzero
vector u ∈Cn×1, the orthogonal projectors onto span {u} and u⊥are
Pu = uu∗
u∗u
and
Pu⊥= I −uu∗
u∗u.
(5.6.6)
Elementary Reﬂectors
For un×1 ̸= 0, the elementary reﬂector about u⊥is deﬁned to be
R = I −2uu∗
u∗u
(5.6.7)
or, equivalently,
R = I −2uu∗
when
∥u∥= 1.
(5.6.8)
Elementary reﬂectors are also called Householder transformations,
46 and
they are analogous to the simple reﬂector given in Example 4.7.1. To understand
why, suppose u ∈ℜ3×1 and ∥u∥= 1 so that Q = I −uuT is the orthogonal
projector onto the plane u⊥. For each x ∈ℜ3×1, Qx is the orthogonal pro-
jection of x onto u⊥as shown in Figure 5.6.1. To locate Rx = (I −2uuT )x,
notice that Q(Rx) = Qx. In other words, Qx is simultaneously the orthogo-
nal projection of x onto u⊥as well as the orthogonal projection of Rx onto
u⊥. This together with ∥x −Qx∥= |uT x| = ∥Qx −Rx∥implies that Rx
is the reﬂection of x about the plane u⊥, exactly as depicted in Figure 5.6.2.
(Reﬂections about more general subspaces are examined in Exercise 5.13.21.)
x
Rx
Qx
|| x - Qx ||
|| Qx - Rx ||
0
u ⊥
u
Figure 5.6.2
46
Alston Scott Householder (1904–1993) was one of the ﬁrst people to appreciate and promote
the use of elementary reﬂectors for numerical applications. Although his 1937 Ph.D. disserta-
tion at University of Chicago concerned the calculus of variations, Householder’s passion was
mathematical biology, and this was the thrust of his career until it was derailed by the war
eﬀort in 1944. Householder joined the Mathematics Division of Oak Ridge National Labora-
tory in 1946 and became its director in 1948. He stayed at Oak Ridge for the remainder of his
career, and he became a leading ﬁgure in numerical analysis and matrix computations. Like
his counterpart J. Wallace Givens (p. 333) at the Argonne National Laboratory, Householder
was one of the early presidents of SIAM.

5.6 Unitary and Orthogonal Matrices
325
Properties of Elementary Reﬂectors
•
All elementary reﬂectors R are unitary, hermitian, and involutory
( R2 = I ). That is,
R = R∗= R−1.
(5.6.9)
•
If xn×1 is a vector whose ﬁrst entry is x1 ̸= 0, and if
u = x ± µ ∥x∥e1,
where
µ =

1
if x1 is real,
x1/|x1|
if x1 is not real,
(5.6.10)
is used to build the elementary reﬂector R in (5.6.7), then
Rx = ∓µ ∥x∥e1.
(5.6.11)
In other words, this R “reﬂects” x onto the ﬁrst coordinate axis.
Computational Note: To avoid cancellation when using ﬂoating-
point arithmetic for real matrices, set u = x + sign(x1) ∥x∥e1.
Proof of (5.6.9). It is clear that R = R∗, and the fact that R = R−1 is
established simply by verifying that R2 = I.
Proof of (5.6.10). Observe that R = I −2ˆuˆu∗, where ˆu = u/ ∥u∥.
Proof of (5.6.11). Write Rx = x −2uu∗x/u∗u = x −(2u∗x/u∗u)u and verify
that 2u∗x = u∗u to conclude Rx = x −u = ∓µ ∥x∥e1.
Example 5.6.3
Problem: Given x ∈Cn×1 such that ∥x∥= 1, construct an orthonormal basis
for Cn that contains x.
Solution: An eﬃcient solution is to build a unitary matrix that contains x as
its ﬁrst column. Set u = x±µe1 in R = I−2(uu∗/u∗u) and notice that (5.6.11)
guarantees Rx = ∓µe1, so multiplication on the left by R (remembering that
R2 = I) produces x = ∓µRe1 = [∓µR]∗1 . Since | ∓µ| = 1,
U = ∓µR
is a unitary matrix with U∗1 = x, so the columns of U provide the desired
orthonormal basis. For example, to construct an orthonormal basis for ℜ4 that
includes x = (1/3) ( −1
2
0 −2 )T , set
u = x −e1 = 1
3



−4
2
0
−2


and compute R = I −2uuT
uT u = 1
3



−1
2
0
−2
2
2
0
1
0
0
3
0
−2
1
0
2


.
The columns of R do the job.

326
Chapter 5
Norms, Inner Products, and Orthogonality
Now consider rotation, and begin with a basic problem in ℜ2. If a nonzero
vector u = (u1, u2) is rotated counterclockwise through an angle θ to produce
v = (v1, v2), how are the coordinates of v related to the coordinates of u? To
answer this question, refer to Figure 5.6.3, and use the fact that ∥u∥= ν = ∥v∥
(rotation is an isometry) together with some elementary trigonometry to obtain
v1 = ν cos(φ + θ) = ν(cos θ cos φ −sin θ sin φ),
v2 = ν sin(φ + θ) = ν(sin θ cos φ + cos θ sin φ).
(5.6.12)
u = ( u1 , u2 )
v = ( v1 , v2 )
θ
φ
Figure 5.6.3
Substituting cos φ = u1/ν and sin φ = u2/ν into (5.6.12) yields
v1 = (cos θ)u1 −(sin θ)u2,
v2 = (sin θ)u1 + (cos θ)u2,
or

v1
v2

=

cos θ
−sin θ
sin θ
cos θ
 
u1
u2

. (5.6.13)
In other words, v = Pu, where P is the rotator (rotation operator)
P =

cos θ
−sin θ
sin θ
cos θ

.
(5.6.14)
Notice that P is an orthogonal matrix because PT P = I. This means that if
v = Pu, then u = PT v, and hence PT is also a rotator, but in the opposite
direction of that associated with P. That is, PT is the rotator associated with
the angle −θ. This is conﬁrmed by the fact that if θ is replaced by −θ in
(5.6.14), then PT is produced.
Rotating vectors in ℜ3 around any one of the coordinate axes is similar.
For example, consider rotation around the z-axis. Suppose that v = (v1, v2, v3)
is obtained by rotating u = (u1, u2, u3) counterclockwise
47 through an angle
θ around the z-axis. Just as before, the goal is to determine the relationship
between the coordinates of u and v. Since we are rotating around the z-axis,
47
This is from the perspective of looking down the z-axis onto the xy-plane.

5.6 Unitary and Orthogonal Matrices
327
it is evident (see Figure 5.6.4) that the third coordinates are unaﬀected—i.e.,
v3 = u3. To see how the xy-coordinates of u and v are related, consider the
orthogonal projections
up = (u1, u2, 0)
and
vp = (v1, v2, 0)
of u and v onto the xy-plane.
x
y
z
v = (v1, v2, v3)
vp = (v1, v2, 0)
u = (u1, u2, u3)
up = (u1, u2, 0)
θ
θ
Figure 5.6.4
It’s apparent from Figure 5.6.4 that the problem has been reduced to rotation
in the xy-plane, and we already know how to do this. Combining (5.6.13) with
the fact that v3 = u3 produces the equation


v1
v2
v3

=


cos θ
−sin θ
0
sin θ
cos θ
0
0
0
1




u1
u2
u3

,
so
Pz =


cos θ
−sin θ
0
sin θ
cos θ
0
0
0
1


is the matrix that rotates vectors in ℜ3 counterclockwise around the z-axis
through an angle θ. It is easy to verify that Pz is an orthogonal matrix and
that P−1
z
= PT
z rotates vectors clockwise around the z-axis.
By using similar techniques, it is possible to derive orthogonal matrices that
rotate vectors around the x-axis or around the y-axis. Below is a summary of
these rotations in ℜ3.

328
Chapter 5
Norms, Inner Products, and Orthogonality
Rotations in R3
A vector u ∈ℜ3 can be rotated counterclockwise through an angle θ
around a coordinate axis by means of a multiplication P⋆u in which
P⋆is an appropriate orthogonal matrix as described below.
Rotation around the x-Axis
Px =


1
0
0
0
cos θ
−sin θ
0
sin θ
cos θ


x
y
z
θ
Rotation around the y-Axis
Py =


cos θ
0
sin θ
0
1
0
−sin θ
0
cos θ


θ
x
y
z
Rotation around the z-Axis
Pz =


cos θ
−sin θ
0
sin θ
cos θ
0
0
0
1


x
y
z
θ
Note: The minus sign appears above the diagonal in Px and Pz, but
below the diagonal in Py. This is not a mistake—it’s due to the orien-
tation of the positive x-axis with respect to the yz-plane.
Example 5.6.4
3-D Rotational Coordinates. Suppose that three counterclockwise rotations
are performed on the three-dimensional solid shown in Figure 5.6.5. First rotate
the solid in View (a) 90◦around the x-axis to obtain the orientation shown
in View (b). Then rotate View (b) 45◦around the y-axis to produce View (c)
and, ﬁnally, rotate View (c) 60◦around the z-axis to end up with View (d).
You can follow the process by watching how the notch, the vertex v, and the
lighter shaded face move.

5.6 Unitary and Orthogonal Matrices
329
x
y
z
View (a)
π/2
v
x
y
z
View (b)
π/4
v
View (c)
y
z
x
π/3
v
x
y
z
View (d)
v
Figure 5.6.5
Problem: If the coordinates of each vertex in View (a) are speciﬁed, what are
the coordinates of each vertex in View (d)?
Solution: If Px is the rotator that maps points in View (a) to corresponding
points in View (b), and if Py and Pz are the respective rotators carrying View
(b) to View (c) and View (c) to View (d), then
Px =


1
0
0
0
0
−1
0
1
0

, Py =
1
√
2


1
0
1
0
√
2
0
−1
0
1

, Pz =


1/2
−
√
3/2
0
√
3/2
1/2
0
0
0
1

,
so
P = PzPyPx =
1
2
√
2


1
1
√
6
√
3
√
3
−
√
2
−2
2
0


(5.6.15)
is the orthogonal matrix that maps points in View (a) to their corresponding
images in View (d). For example, focus on the vertex labeled v in View (a), and
let va, vb, vc, and vd denote its respective coordinates in Views (a), (b), (c),
and (d). If va = ( 1
1
0 )T , then vb = Pxva = ( 1
0
1 )T ,
vc = Pyvb = PyPxva=


√
2
0
0

,
and
vd = Pzvc = PzPyPxva=


√
2/2
√
6/2
0

.

330
Chapter 5
Norms, Inner Products, and Orthogonality
More generally, if the coordinates of each of the ten vertices in View (a) are
placed as columns in a vertex matrix,
Va =



v1
↓
v2
↓
v10
↓
x1
x2
· · ·
x10
y1
y2
· · ·
y10
z1
z2
· · ·
z10


, then
Vd = PzPyPxVa =



ˆv1
↓
ˆv2
↓
ˆv10
↓
ˆx1
ˆx2
· · ·
ˆx10
ˆy1
ˆy2
· · ·
ˆy10
ˆz1
ˆz2
· · ·
ˆz10



is the vertex matrix for the orientation shown in View (d). The polytope in
View (d) is drawn by identifying pairs of vertices (vi, vj) in Va that have an
edge between them, and by drawing an edge between the corresponding vertices
(ˆvi, ˆvj) in Vd.
Example 5.6.5
3-D Computer Graphics. Consider the problem of displaying and manipu-
lating views of a three-dimensional solid on a two-dimensional computer display
monitor. One simple technique is to use a wire-frame representation of the solid
consisting of a mesh of points (vertices) on the solid’s surface connected by
straight line segments (edges). Once these vertices and edges have been deﬁned,
the resulting polytope can be oriented in any desired manner as described in
Example 5.6.4, so all that remains are the following problems.
Problem: How should the vertices and edges of a three-dimensional polytope
be plotted on a two-dimensional computer monitor?
Solution: Assume that the screen represents the yz-plane, and suppose the
x-axis is orthogonal to the screen so that it points toward the viewer’s eye as
shown in Figure 5.6.6.
z
y
x
Figure 5.6.6
A solid in the xyz-coordinate system appears to the viewer as the orthogonal
projection of the solid onto the yz-plane, and the projection of a polytope is
easy to draw. Just set the x-coordinate of each vertex to 0 (i.e., ignore the
x-coordinates), plot the (y, z)-coordinates on the yz-plane (the screen), and

5.6 Unitary and Orthogonal Matrices
331
draw edges between appropriate vertices. For example, suppose that the vertices
of the polytope in Figure 5.6.5 are numbered as indicated below in Figure 5.6.7,
x
y
z
2
3
4
5
6
10
7
8
9
1
Figure 5.6.7
and suppose that the associated vertex matrix is
V =


v1
v2
v3
v4
v5
v6
v7
v8
v9
v10
x
0
1
1
0
0
1
1
1
.8
0
y
0
0
1
1
0
0
.8
1
1
1
z
0
0
0
0
1
1
1
.8
1
1

.
There are 15 edges, and they can be recorded in an edge matrix
E =
 e1
e2
e3
e4
e5
e6
e7
e8
e9
e10
e11
e12
e13
e14
e15
1
2
3
4
1
2
3
4
5
6
7
7
8
9
10
2
3
4
1
5
6
8
10
6
7
8
9
9
10
5

in which the kth column represents an edge between the indicated pair of ver-
tices. To display the image of the polytope in Figure 5.6.7 on a monitor, (i) drop
the ﬁrst row from V, (ii) plot the remaining yz-coordinates on the screen, (iii)
draw edges between appropriate vertices as dictated by the information in the
edge matrix E. To display the image of the polytope after it has been rotated
counterclockwise around the x-, y-, and z-axes by 90◦,
45◦, and 60◦, re-
spectively, use the orthogonal matrix P = PzPyPx determined in (5.6.15) and
compute the product
PV =


0
.354
.707
.354
.866
1.22
1.5
1.4
1.5
1.22
0
.612
1.22
.612
−.5
.112
.602
.825
.602
.112
0
−.707
0
.707
0
−.707
−.141
0
.141
.707

.
Now proceed as before—(i) ignore the ﬁrst row of PV, (ii) plot the points in
the second and third row of PV as yz-coordinates on the monitor, (iii) draw
edges between appropriate vertices as indicated by the edge matrix E.

332
Chapter 5
Norms, Inner Products, and Orthogonality
Problem: In addition to rotation, how can a polytope (or its image on a com-
puter monitor) be translated?
Solution: Translation of a polytope to a diﬀerent point in space is accom-
plished by adding a constant to each of its coordinates. For example, to trans-
late the polytope shown in Figure 5.6.7 to the location where vertex 1 is at
pT = (x0, y0, z0) instead of at the origin, just add p to every point. In partic-
ular, if e is the column of 1’s, the translated vertex matrix is
Vtrans = Vorig +


x0
x0
· · ·
x0
y0
y0
· · ·
y0
z0
z0
· · ·
z0

= Vorig + peT
(a rank-1 update).
Of course, the edge matrix is not aﬀected by translation.
Problem: How can a polytope (or its image on a computer monitor) be scaled?
Solution: Simply multiply every coordinate by the desired scaling factor. For
example, to scale an image by a factor α, form the scaled vertex matrix
Vscaled = αVorig,
and then connect the scaled vertices with appropriate edges as dictated by the
edge matrix E.
Problem: How can the faces of a polytope that are hidden from the viewer’s
perspective be detected so that they can be omitted from the drawing on the
screen?
Solution: A complete discussion of this tricky problem would carry us too far
astray, but one clever solution relying on the cross product of vectors in ℜ3 is
presented in Exercise 5.6.21 for the case of convex polytopes.
Rotations in higher dimensions are straightforward generalizations of rota-
tions in ℜ3. Recall from p. 328 that rotation around any particular axis in ℜ3
amounts to rotation in the complementary plane, and the associated 3 × 3 ro-
tator is constructed by embedding a 2 × 2 rotator in the appropriate position
in a 3 × 3 identity matrix. For example, rotation around the y-axis is rotation
in the xz-plane, and the corresponding rotator is produced by embedding

cos θ
sin θ
−sin θ
cos θ

in the “ xz-position” of I3×3 to form
Py =


cos θ
0
sin θ
0
1
0
−sin θ
0
cos θ

.
These observations directly extend to higher dimensions.

5.6 Unitary and Orthogonal Matrices
333
Plane Rotations
Orthogonal matrices of the form
col i
↓
col j
↓
Pij =















1
...
c
s
1
...
−s
c
1
...
1















←−row i
←−row j
in which c2 +s2 = 1 are called plane rotation matrices because they
perform a rotation in the (i, j)-plane of ℜn. The entries c and s are
meant to suggest cosine and sine, respectively, but designating a rotation
angle θ as is done in ℜ2 and ℜ3 is not useful in higher dimensions.
Plane rotations matrices Pij are also called Givens
48 rotations. Applying
Pij to 0 ̸= x ∈ℜn rotates the (i, j)-coordinates of x in the sense that
Pijx =










x1
...
cxi + sxj
...
−sxi + cxj
...
xn










←−i
←−j
.
If xi and xj are not both zero, and if we set
c =
xi

x2
i + x2
j
and
s =
xj

x2
i + x2
j
,
(5.6.16)
48
J. Wallace Givens, Jr. (1910–1993) pioneered the use of plane rotations in the early days
of automatic matrix computations. Givens graduated from Lynchburg College in 1928, and
he completed his Ph.D. at Princeton University in 1936. After spending three years at the
Institute for Advanced Study in Princeton as an assistant of O. Veblen, Givens accepted an
appointment at Cornell University but later moved to Northwestern University. In addition to
his academic career, Givens was the Director of the Applied Mathematics Division at Argonne
National Laboratory and, like his counterpart A. S. Householder (p. 324) at Oak Ridge National
Laboratory, Givens served as an early president of SIAM.

334
Chapter 5
Norms, Inner Products, and Orthogonality
then
Pijx =











x1
...

x2
i + x2
j
...
0
...
xn











←−i
←−j
.
This means that we can selectively annihilate any component—the jth in this
case—by a rotation in the (i, j)-plane without aﬀecting any entry except xi and
xj. Consequently, plane rotations can be applied to annihilate all components
below any particular “pivot.” For example, to annihilate all entries below the
ﬁrst position in x, apply a sequence of plane rotations as follows:
P12x=






√
x2
1+x2
2
0
x3
x4
...
xn






, P13P12x=






√
x2
1+x2
2+x2
3
0
0
x4
...
xn






, . . . , P1n· · ·P13P12x=






∥x∥
0
0
0
...
0






.
The product of plane rotations is generally not another plane rotation, but
such a product is always an orthogonal matrix, and hence it is an isometry. If
we are willing to interpret “rotation in ℜn ” as a sequence of plane rotations,
then we can say that it is always possible to “rotate” each nonzero vector onto
the ﬁrst coordinate axis. Recall from (5.6.11) that we can also do this with a
reﬂection. More generally, the following statement is true.
Rotations in ℜn
Every nonzero vector x ∈ℜn can be rotated to the ith coordinate
axis by a sequence of n −1 plane rotations. In other words, there is an
orthogonal matrix P such that
Px = ∥x∥ei,
(5.6.17)
where P has the form
P = Pin · · · Pi,i+1Pi,i−1 · · · Pi1.

5.6 Unitary and Orthogonal Matrices
335
Example 5.6.6
Problem: If x ∈ℜn is a vector such that ∥x∥= 1, explain how to use plane
rotations to construct an orthonormal basis for ℜn that contains x.
Solution: This is almost the same problem as that posed in Example 5.6.3, and,
as explained there, the goal is to construct an orthogonal matrix Q such that
Q∗1 = x. But this time we need to use plane rotations rather than an elementary
reﬂector. Equation (5.6.17) asserts that we can build an orthogonal matrix from
a sequence of plane rotations P = P1n · · · P13P12 such that Px = e1. Thus
x = PT e1 = PT
∗1, and hence the columns of Q = PT serve the purpose. For
example, to extend
x = 1
3



−1
2
0
−2



to an orthonormal basis for ℜ4, sequentially annihilate the second and fourth
components of x by using (5.6.16) to construct the following plane rotations:
P12x =



−1/
√
5
2/
√
5
0
0
−2/
√
5
−1/
√
5
0
0
0
0
1
0
0
0
0
1


1
3



−1
2
0
−2


= 1
3



√
5
0
0
−2


,
P14

P12x

=



√
5/3
0
0
−2/3
0
1
0
0
0
0
1
0
2/3
0
0
√
5/3


1
3



√
5
0
0
−2


=



1
0
0
0


.
Therefore, the columns of
Q = (P14P12)T = PT
12PT
14 =



−1/3
−2/
√
5
0
−2/3
√
5
2/3
−1/
√
5
0
4/3
√
5
0
0
1
0
−2/3
0
0
√
5/3



are an orthonormal set containing the speciﬁed vector x.
Exercises for section 5.6
5.6.1. Determine which of the following matrices are isometries.
(a)


1/
√
2
−1/
√
2
0
1/
√
6
1/
√
6
−2/
√
6
1/
√
3
1/
√
3
1/
√
3

.
(b)


1
0
1
1
0
−1
0
1
0

.
(c)



0
0
1
0
1
0
0
0
0
0
0
1
0
1
0
0


.
(d)




eiθ1
0
· · ·
0
0
eiθ2
· · ·
0
...
...
...
...
0
0
· · ·
eiθn



.

336
Chapter 5
Norms, Inner Products, and Orthogonality
5.6.2. Is




1 + i
√
3
1 + i
√
6
i
√
3
−2 i
√
6



a unitary matrix?
5.6.3.
(a)
How many 3 × 3 matrices are both diagonal and orthogonal?
(b)
How many n × n matrices are both diagonal and orthogonal?
(c)
How many n × n matrices are both diagonal and unitary?
5.6.4.
(a)
Under what conditions on the real numbers α and β will
P =

α + β
β −α
α −β
β + α

be an orthogonal matrix?
(b)
Under what conditions on the real numbers α and β will
U =



0
α
0
iβ
α
0
iβ
0
0
iβ
0
α
iβ
0
α
0



be a unitary matrix?
5.6.5. Let U and V be two n × n unitary (orthogonal) matrices.
(a)
Explain why the product UV must be unitary (orthogonal).
(b)
Explain why the sum U+V need not be unitary (orthogonal).
(c)
Explain why
 Un×n
0
0
Vm×m

must be unitary (orthogonal).
5.6.6. Cayley Transformation. Prove, as Cayley did in 1846, that if A is
skew hermitian (or real skew symmetric), then
U = (I −A)(I + A)−1 = (I + A)−1(I −A)
is unitary (orthogonal) by ﬁrst showing that (I + A)−1 exists for skew-
hermitian matrices, and (I −A)(I + A)−1 = (I + A)−1(I −A) (recall
Exercise 3.7.6). Note: There is a more direct approach, but it requires
the diagonalization theorem for normal matrices—see Exercise 7.5.5.
5.6.7. Suppose that R and S are elementary reﬂectors.
(a)
Is
 I
0
0
R

an elementary reﬂector?
(b)
Is
 R
0
0
S

an elementary reﬂector?

5.6 Unitary and Orthogonal Matrices
337
5.6.8.
(a)
Explain why the standard inner product is invariant under a uni-
tary transformation. That is, if U is any unitary matrix, and if
u = Ux and v = Uy, then
u∗v = x∗y.
(b)
Given any two vectors x, y ∈ℜn, explain why the angle between
them is invariant under an orthogonal transformation. That is, if
u = Px and v = Py, where P is an orthogonal matrix, then
cos θu,v = cos θx,y.
5.6.9. Let Um×r be a matrix with orthonormal columns, and let Vk×n be a
matrix with orthonormal rows. For an arbitrary A ∈Cr×k, solve the
following problems using the matrix 2-norm (p. 281) and the Frobenius
matrix norm (p. 279).
(a)
Determine the values of ∥U∥2 , ∥V∥2 , ∥U∥F , and ∥V∥F .
(b)
Show that ∥UAV∥2 = ∥A∥2 . (Hint: Start with ∥UA∥2 . )
(c)
Show that ∥UAV∥F = ∥A∥F .
Note: In particular, these properties are valid when U and V are
unitary matrices. Because of parts (b) and (c), the 2-norm and the F -
norm are said to be unitarily invariant norms.
5.6.10. Let u =


−2
1
3
−1

and v =


1
4
0
−1

.
(a)
Determine the orthogonal projection of u onto span {v} .
(b)
Determine the orthogonal projection of v onto span {u} .
(c)
Determine the orthogonal projection of u onto v⊥.
(d)
Determine the orthogonal projection of v onto u⊥.
5.6.11. Consider elementary orthogonal projectors Q = I −uu∗.
(a)
Prove that Q is singular.
(b)
Now prove that if Q is n × n, then rank (Q) = n −1.
Hint: Recall Exercise 4.4.10.
5.6.12. For vectors u, x ∈Cn such that ∥u∥= 1, let p be the orthogonal
projection of x onto span {u} . Explain why ∥p∥≤∥x∥with equality
holding if and only if x is a scalar multiple of u.

338
Chapter 5
Norms, Inner Products, and Orthogonality
5.6.13. Let x = (1/3)

1
−2
−2

.
(a)
Determine an elementary reﬂector R such that Rx lies on the
x-axis.
(b)
Verify by direct computation that your reﬂector R is symmet-
ric, orthogonal, and involutory.
(c)
Extend x to an orthonormal basis for ℜ3 by using an elemen-
tary reﬂector.
5.6.14. Let R = I −2uu∗, where ∥un×1∥= 1. If x is a ﬁxed point for R in
the sense that Rx = x, and if n > 1, prove that x must be orthogonal
to u, and then sketch a picture of this situation in ℜ3.
5.6.15. Let x, y ∈ℜn×1 be vectors such that ∥x∥= ∥y∥but x ̸= y. Explain
how to construct an elementary reﬂector R such that Rx = y.
Hint: The vector u that deﬁnes R can be determined visually in ℜ3
by considering Figure 5.6.2.
5.6.16. Let xn×1 be a vector such that ∥x∥= 1, and partition x as
x =

x1
˜x

,
where ˜x is n −1 × 1.
(a)
If the entries of x are real, and if x1 ̸= 1, show that
P =

x1
˜xT
˜x
I −α˜x˜xT

,
where
α =
1
1 −x1
is an orthogonal matrix.
(b)
Suppose that the entries of x are complex. If |x1| ̸= 1, and if
µ is the number deﬁned in (5.6.10), show that the matrix
U =

x1
µ2˜x∗
˜x
µ(I −α˜x˜x∗)

,
where
α =
1
1 −|x1|
is unitary. Note: These results provide an easy way to extend
a given vector to an orthonormal basis for the entire space ℜn
or Cn.

5.6 Unitary and Orthogonal Matrices
339
5.6.17. Perform the following sequence of rotations in ℜ3 beginning with
v0 =


1
1
−1

.
1.
Rotate v0 counterclockwise 45◦around the x-axis to produce v1.
2.
Rotate v1 clockwise 90◦around the y-axis to produce v2.
3.
Rotate v2 counterclockwise 30◦around the z-axis to produce v3.
Determine the coordinates of v3 as well as an orthogonal matrix Q
such that Qv0 = v3.
5.6.18. Does it matter in what order rotations in ℜ3 are performed? For ex-
ample, suppose that a vector v ∈ℜ3 is ﬁrst rotated counterclockwise
around the x-axis through an angle θ, and then that vector is rotated
counterclockwise around the y-axis through an angle φ. Is the result
the same as ﬁrst rotating v counterclockwise around the y-axis through
an angle φ followed by a rotation counterclockwise around the x-axis
through an angle θ?
5.6.19. For each nonzero vector u ∈Cn, prove that dim u⊥= n −1.
5.6.20. A matrix satisfying A2 = I is said to be an involution or an involu-
tory matrix, and a matrix P satisfying P2 = P is called a projector
or is said to be an idempotent matrix—properties of such matrices
are developed on p. 386. Show that there is a one-to-one correspondence
between the set of involutions and the set of projectors in Cn×n. Hint:
Consider the relationship between the projectors in (5.6.6) and the re-
ﬂectors (which are involutions) in (5.6.7) on p. 324.
5.6.21. When using a computer to generate and display a three-dimensional
convex polytope such as the one in Example 5.6.4, it is desirable to not
draw those faces that should be hidden from the perspective of a viewer
positioned as shown in Figure 5.6.6. The operation of cross product in
ℜ3 (usually introduced in elementary calculus courses) can be used to
decide which faces are visible and which are not. Recall that if
u =


u1
u2
u3

and v =


v1
v2
v3

,
then
u × v =


u2v3 −u3v2
u3v1 −u1v3
u1v2 −u2v1

,

340
Chapter 5
Norms, Inner Products, and Orthogonality
and u × v is a vector orthogonal to both u and v. The direction of
u × v is determined from the so-called right-hand rule as illustrated in
Figure 5.6.8.
Figure 5.6.8
Assume the origin is interior to the polytope, and consider a particular
face and three vertices p0, p1, and p2 on the face that are positioned
as shown in Figure 5.6.9. The vector n = (p1 −p0) × (p2 −p1) is
orthogonal to the face, and it points in the outward direction.
Figure 5.6.9
Explain why the outside of the face is visible from the perspective indi-
cated in Figure 5.6.6 if and only if the ﬁrst component of the outward
normal vector n is positive. In other words, the face is drawn if and
only if n1 > 0.

5.7 Orthogonal Reduction
341
5.7
ORTHOGONAL REDUCTION
We know that a matrix A can be reduced to row echelon form by elementary row
operations. This is Gaussian elimination, and, as explained on p. 143, the basic
“Gaussian transformation” is an elementary lower triangular matrix Tk whose
action annihilates all entries below the kth pivot at the kth elimination step.
But Gaussian elimination is not the only way to reduce a matrix. Elementary
reﬂectors Rk can be used in place of elementary lower triangular matrices Tk
to annihilate all entries below the kth pivot at the kth elimination step, or a
sequence of plane rotation matrices can accomplish the same purpose.
When reﬂectors are used, the process is usually called Householder re-
duction, and it proceeds as follows. For Am×n = [A∗1 | A∗2 | · · · | A∗n] , use
x = A∗1 in (5.6.10) to construct the elementary reﬂector
R1 = I −2uu∗
u∗u,
where
u = A∗1 ± µ ∥A∗1∥e1,
(5.7.1)
so that
R1A∗1 = ∓µ ∥A∗1∥e1 =




t11
0
...
0



.
(5.7.2)
Applying R1 to A yields
R1A=[R1A∗1 | R1A∗2 | · · · | R1A∗n]=





t11
t12
· · ·
t1n
0
∗
· · ·
∗
...
...
...
0
∗
· · ·
∗




=

t11
tT
1
0
A2

,
where A2 is m −1 × n −1. Thus all entries below the (1, 1)-position are an-
nihilated. Now apply the same procedure to A2 to construct an elementary
reﬂector ˆR2 that annihilates all entries below the (1, 1)-position in A2. If we
set R2 =
 1
0
0
ˆR2

, then R2R1 is an orthogonal matrix (Exercise 5.6.5) such
that
R2R1A =

t11
tT
1
0
ˆR2A2

=








t11
t12
t13
· · ·
t1n
0
t22
t23
· · ·
t2n
0
0
∗
· · ·
∗
...
...
...
...
0
0
∗
· · ·
∗








.
The result after k −1 steps is Rk−1 · · · R2R1A =

Tk−1
˜Tk−1
0
Ak

. At step
k an elementary reﬂector
ˆRk is constructed in a manner similar to (5.7.1)

342
Chapter 5
Norms, Inner Products, and Orthogonality
to annihilate all entries below the (1, 1)-position in Ak, and Rk is deﬁned
as Rk =
 Ik−1
0
0
ˆRk

, which is another elementary reﬂector (Exercise 5.6.7).
Eventually, all of the rows or all of the columns will be exhausted, so the ﬁnal
result is one of the two following upper-trapezoidal forms:
Rn · · · R2R1Am×n =











∗
∗
· · ·
∗
0
∗
· · ·
∗
...
...
...
0
0
· · ·
∗
0
0
· · ·
0
...
...
...
0
0
· · ·
0




















n×n
when
m > n,
Rm−1 · · · R2R1Am×n =




∗
∗
· · ·
∗
∗
· · ·
∗
0
∗
· · ·
∗
∗
· · ·
∗
...
...
...
...
...
0
0
· · ·
∗
∗
· · ·
∗




when
m < n.
)
*+
,
m×m
If m = n, then the ﬁnal form is an upper-triangular matrix.
A product of elementary reﬂectors is not necessarily another elementary re-
ﬂector, but a product of unitary (orthogonal) matrices is again unitary (orthogo-
nal) (Exercise 5.6.5). The elementary reﬂectors Ri described above are unitary
(orthogonal in the real case) matrices, so every product RkRk−1 · · · R2R1 is a
unitary matrix, and thus we arrive at the following important conclusion.
Orthogonal Reduction
•
For every A ∈Cm×n, there exists a unitary matrix P such that
PA = T
(5.7.3)
has an upper-trapezoidal form. When P is constructed as a prod-
uct of elementary reﬂectors as described above, the process is called
Householder reduction.
•
If A is square, then T is upper triangular, and if A is real, then
the P can be taken to be an orthogonal matrix.

5.7 Orthogonal Reduction
343
Example 5.7.1
Problem:
Use Householder reduction to ﬁnd an orthogonal matrix P such
that PA = T is upper triangular with positive diagonal entries, where
A =


0
−20
−14
3
27
−4
4
11
−2

.
Solution: To annihilate the entries below the (1, 1)-position and to guarantee
that t11 is positive, equations (5.7.1) and (5.7.2) dictate that we set
u1 = A∗1 −∥A∗1∥e1 = A∗1 −5e1 =


−5
3
4


and
R1 = I −2u1uT
1
uT
1 u1
.
To compute a reﬂector-by-matrix product RA = [RA∗1 | RA∗2 | · · · | RA∗n] ,
it’s wasted eﬀort to actually determine the entries in R = I−2uuT /uT u. Simply
compute uT A∗j and then
RA∗j = A∗j −2
uT A∗j
uT u

u
for each j = 1, 2, . . . , n.
(5.7.4)
By using this observation we obtain
R1A = [R1A∗1 | R1A∗2 | R1A∗3] =


5
25
−4
0
0
−10
0
−25
−10

.
To annihilate the entry below the (2, 2)-position, set
A2 =

0
−10
−25
−10

and
u2 = [A2]∗1 −
 [A2]∗1
e1 = 25

−1
−1

.
If ˆR2 = I −2u2uT
2 /uT
2 u2 and R2 =
 1
0
0
ˆR2

(neither is explicitly computed),
then
ˆR2A2 =

25
10
0
10

and
R2R1A = T =


5
25
−4
0
25
10
0
0
10

.
If ˆRk = I −2ˆuˆuT /ˆuT ˆu is an elementary reﬂector, then so is
Rk =

I
0
0
ˆRk

= I −2uuT
uT u
with
u =

0
ˆu

,
and consequently the product of any sequence of these Rk ’s can be formed by
using the observation (5.7.4). In this example,
P = R2R1 = 1
25


0
15
20
−20
12
−9
−15
−16
12

.
You may wish to check that P really is an orthogonal matrix and PA = T.

344
Chapter 5
Norms, Inner Products, and Orthogonality
Elementary reﬂectors are not the only type of orthogonal matrices that can
be used to reduce a matrix to an upper-trapezoidal form. Plane rotation matrices
are also orthogonal, and, as explained on p. 334, plane rotation matrices can be
used to selectively annihilate any component in a given column, so a sequence of
plane rotations can be used to annihilate all elements below a particular pivot.
This means that a matrix A ∈ℜm×n can be reduced to an upper-trapezoidal
form strictly by using plane rotations—such a process is usually called a Givens
reduction.
Example 5.7.2
Problem: Use Givens reduction (i.e., use plane rotations) to reduce the matrix
A =


0
−20
−14
3
27
−4
4
11
−2


to upper-triangular form. Also compute an orthogonal matrix P such that
PA = T is upper triangular.
Solution: The plane rotation that uses the (1,1)-entry to annihilate the (2,1)-
entry is determined from (5.6.16) to be
P12 =


0
1
0
−1
0
0
0
0
1


so that
P12A =


3
27
−4
0
20
14
4
11
−2

.
Now use the (1,1)-entry in P12A to annihilate the (3,1)-entry in P12A. The
plane rotation that does the job is again obtained from (5.6.16) to be
P13 = 1
5


3
0
4
0
5
0
−4
0
3


so that
P13P12A =


5
25
−4
0
20
14
0
−15
2

.
Finally, using the (2,2)-entry in P13P12A to annihilate the (3,2)-entry produces
P23 = 1
5


5
0
0
0
4
−3
0
3
4


so that
P23P13P12A = T =


5
25
−4
0
25
10
0
0
10

.
Since plane rotation matrices are orthogonal, and since the product of orthogonal
matrices is again orthogonal, it must be the case that
P = P23P13P12 = 1
25


0
15
20
−20
12
−9
−15
−16
12


is an orthogonal matrix such that PA = T.

5.7 Orthogonal Reduction
345
Householder and Givens reductions are closely related to the results pro-
duced by applying the Gram–Schmidt process (p. 307) to the columns of A.
When A is nonsingular, Householder, Givens, and Gram–Schmidt each pro-
duce an orthogonal matrix Q and an upper-triangular matrix R such that
A = QR (Q = PT in the case of orthogonal reduction). The upper-triangular
matrix R produced by the Gram–Schmidt algorithm has positive diagonal en-
tries, and, as illustrated in Examples 5.7.1 and 5.7.2, we can also force this to be
true using the Householder or Givens reduction. This feature makes Q and R
unique.
QR Factorization
For each nonsingular A ∈ℜn×n, there is a unique orthogonal matrix Q
and a unique upper-triangular matrix R with positive diagonal entries
such that
A = QR.
This “square” QR factorization is a special case of the more general
“rectangular” QR factorization discussed on p. 311.
Proof.
Only uniqueness needs to be proven. If there are two QR factorizations
A = Q1R1 = Q2R2,
let U = QT
2 Q1 = R2R−1
1 . The matrix R2R−1
1
is upper triangular with positive
diagonal entries (Exercises 3.5.8 and 3.7.4) while QT
2 Q1 is an orthogonal matrix
(Exercise 5.6.5), and therefore U is an upper-triangular matrix whose columns
are an orthonormal set and whose diagonal entries are positive. Considering the
ﬁrst column of U we see that





u11
0
...
0





= 1
=⇒
u11 = ±1
and
u11 > 0
=⇒
u11 = 1,
so that U∗1 = e1. A similar argument together with the fact that the columns
of U are mutually orthogonal produces
UT
∗1U∗2 = 0
=⇒
u12 = 0
=⇒
u22 = 1
=⇒
U∗2 = e2.
Proceeding inductively establishes that U∗k = ek for each k (i.e., U = I ), and
therefore Q1 = Q2 and R1 = R2.

346
Chapter 5
Norms, Inner Products, and Orthogonality
Example 5.7.3
Orthogonal Reduction and Least Squares. Orthogonal reduction can be
used to solve the least squares problem associated with an inconsistent system
Ax = b in which A ∈ℜm×n and m ≥n (the most common case). If ε
denotes the diﬀerence ε = Ax −b, then, as described on p. 226, the general
least squares problem is to ﬁnd a vector x that minimizes the quantity
m

i=1
ε2
i = εT ε = ∥ε∥2 ,
where ∥⋆∥is the standard euclidean vector norm. Suppose that A is reduced
to an upper-trapezoidal matrix T by an orthogonal matrix P, and write
PA = T =

Rn×n
0

and
Pb =

cn×1
d

in which R is an upper-triangular matrix. An orthogonal matrix is an isometry—
recall (5.6.1)—so that
∥ε∥2 = ∥Pε∥2 = ∥P(Ax −b)∥2 =


R
0

x −

c
d

2
=


Rx −c
d

2
= ∥Rx −c∥2 + ∥d∥2 .
Consequently, ∥ε∥2 is minimized when x is a vector such that ∥Rx −c∥2 is
minimal or, in other words, x is a least squares solution for Ax = b if and only
if x is a least squares solution for Rx = c.
Full-Rank Case.
In a majority of applications the coeﬃcient matrix A has
linearly independent columns so rank (Am×n) = n. Because multiplication by
a nonsingular matrix P does not change the rank,
n = rank (A) = rank (PA) = rank (T) = rank (Rn×n).
Thus R is nonsingular, and we have established the following fact.
•
If A has linearly independent columns, then the (unique) least squares so-
lution for Ax = b is obtained by solving the nonsingular triangular system
Rx = c for x.
As pointed out in Example 4.5.1, computing the matrix product AT A is to be
avoided when ﬂoating-point computation is used because of the possible loss of
signiﬁcant information. Notice that the method based on orthogonal reduction
sidesteps this potential problem because the normal equations AT Ax = AT b
are avoided and the product AT A is never explicitly computed. Householder
reduction (or Givens reduction for sparse problems) is a numerically stable algo-
rithm (see the discussion following this example) for solving the full-rank least
squares problem, and, if the computations are properly ordered, it is an attrac-
tive alternative to the method of Example 5.5.3 that is based on the modiﬁed
Gram–Schmidt procedure.

5.7 Orthogonal Reduction
347
We now have four diﬀerent ways to reduce a matrix to an upper-triangular
(or trapezoidal) form. (1) Gaussian elimination; (2) Gram–Schmidt procedure;
(3) Householder reduction; and (4) Givens reduction. It’s natural to try to com-
pare them and to sort out the advantages and disadvantages of each.
First consider numerical stability. This is a complicated issue, but you can
nevertheless gain an intuitive feel for the situation by considering the eﬀect of
applying a sequence of “elementary reduction” matrices to a small perturbation
of A. Let E be a matrix such that ∥E∥F
is small relative to ∥A∥F
(the
Frobenius norm was introduced on p. 279), and consider
Pk · · · P2P1(A + E) = (Pk · · · P2P1A) + (Pk · · · P2P1E) = PA + PE.
If each Pi is an orthogonal matrix, then the product P = Pk · · · P2P1 is also an
orthogonal matrix (Exercise 5.6.5), and consequently ∥PE∥F = ∥E∥F (Exercise
5.6.9). In other words, a sequence of orthogonal transformations cannot magnify
the magnitude of E, and you might think of E as representing the eﬀects of
roundoﬀerror. This suggests that Householder and Givens reductions should be
numerically stable algorithms. On the other hand, if the Pi ’s are elementary
matrices of Type I, II, or III, then the product P = Pk · · · P2P1 can be any
nonsingular matrix—recall (3.9.3). Nonsingular matrices are not generally norm
preserving (i.e., it is possible that ∥PE∥F > ∥E∥F ), so the possibility of E
being magniﬁed is generally present in elimination methods, and this suggests
the possibility of numerical instability.
Strictly speaking, an algorithm is considered to be numerically stable
if, under ﬂoating-point arithmetic, it always returns an answer that is the exact
solution of a nearby problem. To give an intuitive argument that the Householder
or Givens reduction is a stable algorithm for producing the QR factorization of
An×n, suppose that Q and R are the exact QR factors, and suppose that
ﬂoating-point arithmetic produces an orthogonal matrix Q + E and an upper-
triangular matrix R + F that are the exact QR factors of a diﬀerent matrix
˜A = (Q + E)(R + F) = QR + QF + ER + EF = A + QF + ER + EF.
If E and F account for the roundoﬀerrors, and if their entries are small relative
to those in A, then the entries in EF are negligible, and
˜A ≈A + QF + ER.
But since Q is orthogonal, ∥QF∥F = ∥F∥F and ∥A∥F = ∥QR∥F = ∥R∥F ,
and this means that neither QF nor ER can contain entries that are large
relative to those in A. Hence ˜A ≈A, and this is what is required to conclude
that the algorithm is stable.
Gaussian elimination is not a stable algorithm because, as alluded to in §1.5,
problems arise due to the growth of the magnitude of the numbers that can occur

348
Chapter 5
Norms, Inner Products, and Orthogonality
during the process. To see this from a heuristic point of view, consider the LU
factorization of A = LU, and suppose that ﬂoating-point Gaussian elimination
with no pivoting returns matrices L + E and U + F that are the exact LU
factors of a somewhat diﬀerent matrix
˜A = (L + E)(U + F) = LU + LF + EU + EF = A + LF + EU + EF.
If E and F account for the roundoﬀerrors, and if their entries are small relative
to those in A, then the entries in EF are negligible, and
˜A ≈A + LF + EU
(using no pivoting).
However, if L or U contains entries that are large relative to those in A
(and this is certainly possible), then LF or EU can contain entries that are
signiﬁcant. In other words, Gaussian elimination with no pivoting can return the
LU factorization of a matrix
˜A that is not very close to the original matrix
A, and this is what it means to say that an algorithm is unstable. We saw on
p. 26 that if partial pivoting is employed, then no multiplier can exceed 1 in
magnitude, and hence no entry of L can be greater than 1 in magnitude (recall
that the subdiagonal entries of L are in fact the multipliers). Consequently,
L cannot greatly magnify the entries of F, so, if the rows of A have been
reordered according to the partial pivoting strategy, then
˜A ≈A + EU
(using partial pivoting).
Numerical stability requires that ˜A ≈A, so the issue boils down to the degree
to which U magniﬁes the entries in E —i.e., the issue rests on the magnitude of
the entries in U. Unfortunately, partial pivoting may not be enough to control
the growth of all entries in U. For example, when Gaussian elimination with
partial pivoting is applied to
Wn =












1
0
0
· · ·
0
0
1
−1
1
0
· · ·
0
0
1
−1
−1
1
...
0
0
1
...
...
...
...
...
...
...
−1
−1
−1
...
1
0
1
−1
−1
−1
· · ·
−1
1
1
−1
−1
−1
· · ·
−1
−1
1












,
the largest entry in U is unn = 2n−1. However, if complete pivoting is used on
Wn, then no entry in the process exceeds 2 in magnitude (Exercises 1.5.7 and
1.5.8). In general, it has been proven that if complete pivoting is used on a well-
scaled matrix An×n for which max |aij| = 1, then no entry of U can exceed

5.7 Orthogonal Reduction
349
γ = n1/2 
2131/241/3 · · · n1/n−11/2 in magnitude. Since γ is a slow growing
function of n, the entries in U won’t greatly magnify the entries of E, so
˜A ≈A
(using complete pivoting).
In other words, Gaussian elimination with complete pivoting is stable, but Gaus-
sian elimination with partial pivoting is not. Fortunately, in practical work it is
rare to encounter problems such as the matrix Wn in which partial pivoting
fails to control the growth in the U factor, so scaled partial pivoting is generally
considered to be a “practically stable” algorithm.
Algorithms based on the Gram–Schmidt procedure are more complicated.
First, the Gram–Schmidt algorithms diﬀer from Householder and Givens reduc-
tions in that the Gram–Schmidt procedures are not a sequential application of
elementary orthogonal transformations. Second, as an algorithm to produce the
QR factorization even the modiﬁed Gram–Schmidt technique can return a Q
factor that is far from being orthogonal, and the intuitive stability argument
used earlier is not valid. As an algorithm to return the QR factorization of A,
the modiﬁed Gram–Schmidt procedure has been proven to be unstable, but as
an algorithm used to solve the least squares problem (see Example 5.5.3), it is
stable—i.e., stability of modiﬁed Gram–Schmidt is problem dependent.
Summary of Numerical Stability
•
Gaussian elimination with scaled partial pivoting is theoretically un-
stable, but it is “practically stable”—i.e., stable for most practical
problems.
•
Complete pivoting makes Gaussian elimination unconditionally sta-
ble.
•
For the QR factorization, the Gram–Schmidt procedure (classical
or modiﬁed) is not stable. However, the modiﬁed Gram–Schmidt
procedure is a stable algorithm for solving the least squares problem.
•
Householder and Givens reductions are unconditionally stable algo-
rithms for computing the QR factorization.
For the algorithms under consideration, the number of multiplicative oper-
ations is about the same as the number of additive operations, so computational
eﬀort is gauged by counting only multiplicative operations. For the sake of com-
parison, lower-order terms are not signiﬁcant, and when they are neglected the
following approximations are obtained.

350
Chapter 5
Norms, Inner Products, and Orthogonality
Summary of Computational Effort
The approximate number of multiplications/divisions required to reduce
an n × n matrix to an upper-triangular form is as follows.
•
Gaussian elimination (scaled partial pivoting) ≈n3/3.
•
Gram–Schmidt procedure (classical and modiﬁed) ≈n3.
•
Householder reduction ≈2n3/3.
•
Givens reduction ≈4n3/3.
It’s not surprising that the unconditionally stable methods tend to be more
costly—there is no free lunch. No one triangularization technique can be con-
sidered optimal, and each has found a place in practical work. For example, in
solving unstructured linear systems, the probability of Gaussian elimination with
scaled partial pivoting failing is not high enough to justify the higher cost of using
the safer Householder or Givens reduction, or even complete pivoting. Although
much the same is true for the full-rank least squares problem, Householder re-
duction or modiﬁed Gram–Schmidt is frequently used as a safeguard against
sensitivities that often accompany least squares problems. For the purpose of
computing an orthonormal basis for R (A) in which A is unstructured and
dense (not many zeros), Householder reduction is preferred—the Gram–Schmidt
procedures are unstable for this purpose and Givens reduction is too costly.
Givens reduction is useful when the matrix being reduced is highly structured
or sparse (many zeros).
Example 5.7.4
Reduction to Hessenberg Form. For reasons alluded to in §4.8 and §4.9, it
is often desirable to triangularize a square matrix A by means of a similarity
transformation—i.e., ﬁnd a nonsingular matrix P such that P−1AP = T is
upper triangular. But this is a computationally diﬃcult task, so we will try to do
the next best thing, which is to ﬁnd a similarity transformation that will reduce
A to a matrix in which all entries below the ﬁrst subdiagonal are zero. Such a
matrix is said to be in upper-Hessenberg form—illustrated below is a 5 × 5
Hessenberg form.
H =





∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
0
∗
∗
∗
∗
0
0
∗
∗
∗
0
0
0
∗
∗




.

5.7 Orthogonal Reduction
351
Problem: Reduce A ∈ℜn×n to upper-Hessenberg form by means of an orthog-
onal similarity transformation—i.e., construct an orthogonal matrix P such that
PT AP = H is upper Hessenberg.
Solution: At each step, use Householder reduction on entries below the main
diagonal. Begin by letting ˆA∗1 denote the entries of the ﬁrst column that are
below the (1,1)-position—this is illustrated below for n = 5:
A =






∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗






=
 a11
ˆA1∗
ˆA∗1
A1

.
If
ˆR1 is an elementary reﬂector determined according to (5.7.1) for which
ˆR1 ˆA∗1 =


∗
0
0
0

, then R1 =
 1
0
0
ˆR1

is an orthogonal matrix such that
R1AR1 =
 1
0
0
ˆR1
 
a11
ˆA1∗
ˆA∗1
A1
  1
0
0
ˆR1

=

a11
ˆA1∗ˆR1
ˆR1 ˆA∗1
ˆR1A1 ˆR1

=






∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
0
∗
∗
∗
∗
0
∗
∗
∗
∗
0
∗
∗
∗
∗






.
At the second step, repeat the process on A2 = ˆR1A1 ˆR1 to obtain an orthogo-
nal matrix ˆR2 such that ˆR2A2 ˆR2 =



∗
∗
∗
∗
∗
∗
∗
∗
0
∗
∗
∗
0
∗
∗
∗


. Matrix R2 =
 I2
0
0
ˆR2

is an orthogonal matrix such that
R2R1AR1R2 =







∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
0
∗
∗
∗
∗
0
0
∗
∗
∗
0
0
∗
∗
∗







.
After n −2 of these steps, the product P = R1R2 · · · Rn−2 is an orthogonal
matrix such that PT AP = H is in upper-Hessenberg form.

352
Chapter 5
Norms, Inner Products, and Orthogonality
Note:
If A is a symmetric matrix, then HT = (PT AP)T = PT AT P = H,
so H is symmetric. But as illustrated below for n = 5, a symmetric Hessenberg
form is a tridiagonal matrix,
H = PT AP =





∗
∗
0
0
0
∗
∗
∗
0
0
0
∗
∗
∗
0
0
0
∗
∗
∗
0
0
0
∗
∗




,
so the following useful corollary is obtained.
•
Every real-symmetric matrix is orthogonally similar to a tridiagonal matrix,
and Householder reduction can be used to compute this tridiagonal matrix.
However, the Lanczos technique discussed on p. 651 can be much more eﬃ-
cient.
Example 5.7.5
Problem: Compute the QR factors of a nonsingular upper-Hessenberg matrix
H ∈ℜn×n.
Solution: Due to its smaller multiplication count, Householder reduction is
generally preferred over Givens reduction. The exception is for matrices that
have a zero pattern that can be exploited by the Givens method but not by
the Householder method. A Hessenberg matrix H is such an example. The
ﬁrst step of Householder reduction completely destroys most of the zeros in
H, but applying plane rotations does not. This is illustrated below for a 5 × 5
Hessenberg form—remember that the action of Pk,k+1 aﬀects only the kth and
(k + 1)st rows.





∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
0
∗
∗
∗
∗
0
0
∗
∗
∗
0
0
0
∗
∗





P12
−−−→





∗
∗
∗
∗
∗
0
∗
∗
∗
∗
0
∗
∗
∗
∗
0
0
∗
∗
∗
0
0
0
∗
∗





P23
−−−→





∗
∗
∗
∗
∗
0
∗
∗
∗
∗
0
0
∗
∗
∗
0
0
∗
∗
∗
0
0
0
∗
∗





P34
−−−→





∗
∗
∗
∗
∗
0
∗
∗
∗
∗
0
0
∗
∗
∗
0
0
0
∗
∗
0
0
0
∗
∗





P45
−−−→





∗
∗
∗
∗
∗
0
∗
∗
∗
∗
0
0
∗
∗
∗
0
0
0
∗
∗
0
0
0
0
∗




.
In general, Pn−1,n · · · P23P12H = R is upper triangular in which all diagonal
entries, except possibly the last, are positive—the last diagonal can be made pos-
itive by the technique illustrated in Example 5.7.2. Thus we obtain an orthogonal
matrix P such that PH = R, or H = QR in which Q = PT .

5.7 Orthogonal Reduction
353
Example 5.7.6
Jacobi Reduction.
49 Given a real-symmetric matrix A, the result of Example
5.7.4 shows that Householder reduction can be used to construct an orthogonal
matrix P such that PT AP = T is tridiagonal. Can we do better?—i.e., can we
construct an orthogonal matrix P such that PT AP = D is a diagonal matrix?
Indeed we can, and much of the material in Chapter 7 concerning eigenvalues and
eigenvectors is devoted to this problem. But in the present context, this fact can
be constructively established by means of Jacobi’s diagonalization algorithm.
Jacobi’s Idea. If A ∈ℜn×n is symmetric, then a plane rotation matrix can
be applied to reduce the magnitude of the oﬀ-diagonal entries. In particular,
suppose that aij ̸= 0 is the oﬀ-diagonal entry of maximal magnitude, and let
A′ denote the matrix obtained by setting each akk = 0. If Pij is the plane
rotation matrix described on p. 333 in which c = cos θ and s = sin θ, where
cot 2θ = (aii −ajj)/2aij, and if B = PT
ijAPij, then
(1)
bij = bji = 0
(i.e., aij is annihilated),
(2)
∥B′∥2
F = ∥A′∥2
F −2a2
ij,
(3)
∥B′∥2
F ≤

1 −
2
n2 −n

∥A′∥2
F .
Proof.
The entries of B = PT
ijAPij that lay on the intersection of the ith and
jth rows with the ith and jth columns can be described by
ˆB =

bii
bij
bji
bjj

=

cos θ
sin θ
−sin θ
cos θ
 
aii
aij
aij
ajj
 
cos θ
−sin θ
sin θ
cos θ

= PT ˆAP.
Use the identities cos 2θ = cos2 θ −sin2 θ and sin 2θ = 2 cos θ sin θ to verify
bij = bji = 0, and recall that ∥ˆB∥F = ∥PT ˆAP∥F = ∥ˆA∥F (recall Exercise
49
Karl Gustav Jacob Jacobi (1804–1851) ﬁrst presented this method in 1846, and it was popular
for a time. But the twentieth-century development of electronic computers sparked tremendous
interest in numerical algorithms for diagonalizing symmetric matrices, and Jacobi’s method
quickly fell out of favor because it could not compete with newer procedures—at least on the
traditional sequential machines. However, the emergence of multiprocessor parallel computers
has resurrected interest in Jacobi’s method because of the inherent parallelism in the algorithm.
Jacobi was born in Potsdam, Germany, educated at the University of Berlin, and employed as
a professor at the University of K¨onigsberg. During his proliﬁc career he made contributions
that are still important facets of contemporary mathematics. His accomplishments include the
development of elliptic functions; a systematic development and presentation of the theory
of determinants; contributions to the theory of rotating liquids; and theorems in the areas of
diﬀerential equations, calculus of variations, and number theory. In contrast to his great con-
temporary Gauss, who disliked teaching and was anything but inspiring, Jacobi was regarded
as a great teacher (the introduction of the student seminar method is credited to him), and
he advocated the view that “the sole end of science is the honor of the human mind, and that
under this title a question about numbers is worth as much as a question about the system
of the world.” Jacobi once defended his excessive devotion to work by saying that “Only cab-
bages have no nerves, no worries. And what do they get out of their perfect wellbeing?” Jacobi
suﬀered a breakdown from overwork in 1843, and he died at the relatively young age of 46.

354
Chapter 5
Norms, Inner Products, and Orthogonality
5.6.9) to produce the conclusion b2
ii + b2
jj = a2
ii + 2a2
ij + a2
jj. Now use the fact
that bkk = akk for all k ̸= i, j together with ∥B∥F = ∥A∥F to write
∥B′∥2
F = ∥B∥2
F −

k
b2
kk = ∥B∥2
F −

k̸=i,j
b2
kk −

b2
ii + b2
jj

= ∥A∥2
F −

k̸=i,j
a2
kk −

a2
ii + 2a2
ij + a2
jj

= ∥A∥2
F −

k
a2
kk −2a2
ij
= ∥A′∥2
F −2a2
ij.
Furthermore, since a2
pq ≤a2
ij for all p ̸= q,
∥A′∥2
F =

p̸=q
a2
pq ≤

p̸=q
a2
ij = (n2 −n)a2
ij
=⇒
−a2
ij ≤−∥A′∥2
F
n2 −n,
so
∥B′∥2
F = ∥A′∥2
F −2a2
ij ≤∥A′∥2
F −2∥A′∥2
F
n2 −n =

1 −
2
n2 −n

∥A′∥2
F .
Jacobi’s Diagonalization Algorithm. Start with A0 = A, and produce a
sequence of matrices Ak = PT
k Ak−1Pk, where at the kth step Pk is a plane
rotation constructed to annihilate the maximal oﬀ-diagonal entry in Ak−1. In
particular, if aij is the entry of maximal magnitude in Ak−1, then Pk is the
rotator in the (i, j)-plane deﬁned by setting
s =
1
√
1 + σ2
and
c =
σ
√
1 + σ2 =

1 −s2,
where
σ = (aii −ajj)
2aij
.
For n > 2 we have
∥A′
k∥2
F ≤

1 −
2
n2 −n
k
∥A′∥2
F →0
as
k →∞.
Therefore, if P(k) is the orthogonal matrix deﬁned by P(k) = P1P2 · · · Pk, then
lim
k→∞P(k)T AP(k) = lim
k→∞Ak = D
is a diagonal matrix.
Exercises for section 5.7
5.7.1.
(a)
Using Householder reduction, compute the QR factors of
A =


1
19
−34
−2
−5
20
2
8
37

.
(b)
Repeat part (a) using Givens reduction.

5.7 Orthogonal Reduction
355
5.7.2. For A ∈ℜm×n, suppose that rank (A) = n, and let P be an orthog-
onal matrix such that
PA = T =

Rn×n
0

,
where R is an upper-triangular matrix. If PT is partitioned as
PT = [Xm×n | Y] ,
explain why the columns of X constitute an orthonormal basis for
R (A).
5.7.3. By using Householder reduction, ﬁnd an orthonormal basis for R (A),
where
A =



4
−3
4
2
−14
−3
−2
14
0
1
−7
15


.
5.7.4. Use Householder reduction to compute the least squares solution for
Ax = b, where
A =



4
−3
4
2
−14
−3
−2
14
0
1
−7
15



and
b =



5
−15
0
30


.
Hint: Make use of the factors you computed in Exercise 5.7.3.
5.7.5. If A = QR is the QR factorization for A, explain why ∥A∥F = ∥R∥F ,
where ∥⋆∥F is the Frobenius matrix norm introduced on p. 279.
5.7.6. Find an orthogonal matrix P such that PT AP = H is in upper-
Hessenberg form, where
A =


−2
3
−4
3
−25
50
−4
50
25

.
5.7.7. Let H be an upper-Hessenberg matrix, and suppose that H = QR,
where R is a nonsingular upper-triangular matrix. Prove that Q as
well as the product RQ must also be in upper-Hessenberg form.
5.7.8. Approximately how many multiplications are needed to reduce an n × n
nonsingular upper-Hessenberg matrix to upper-triangular form by using
plane rotations?

356
Chapter 5
Norms, Inner Products, and Orthogonality
5.8
DISCRETE FOURIER TRANSFORM
For a positive integer n, the complex numbers

1, ω, ω2, . . . , ωn−1
, where
ω = e2πi/n = cos 2π
n + i sin 2π
n
are called the n throots of unity because they represent all solutions to zn = 1.
Geometrically, they are the vertices of a regular polygon of n sides as depicted
in Figure 5.8.1 for n = 3 and n = 6.
1
1
ω
ω
ω2
ω2
ω3
ω4
ω5
n = 6
n = 3
Figure 5.8.1
The roots of unity are cyclic in the sense that if k ≥n, then ωk = ωk (mod n),
where k (mod n) denotes the remainder when k is divided by n—for example,
when n = 6, ω6 = 1, ω7 = ω, ω8 = ω2, ω9 = ω3, . . . .
The numbers

1, ξ, ξ2, . . . , ξn−1
, where
ξ = e−2πi/n = cos 2π
n −i sin 2π
n = ω
are also the nth roots of unity, but, as depicted in Figure 5.8.2 for n = 3 and
n = 6, they are listed in clockwise order around the unit circle rather than
counterclockwise.
1
ξ5
ξ4
ξ3
ξ2
ξ
n = 6
1
ξ2
ξ
n = 3
Figure 5.8.2
The following identities will be useful in our development. If k is an integer,
then 1 = |ξk|2 = ξkξk implies that
ξ−k = ξk = ωk.
(5.8.1)

5.8 Discrete Fourier Transform
357
Furthermore, the fact that
ξk 
1 + ξk + ξ2k + · · · + ξ(n−2)k + ξ(n−1)k
= ξk + ξ2k + · · · + ξ(n−1)k + 1
implies

1 + ξk + ξ2k + · · · + ξ(n−1)k 
1 −ξk
= 0 and, consequently,
1 + ξk + ξ2k + · · · + ξ(n−1)k = 0
whenever
ξk ̸= 1.
(5.8.2)
Fourier Matrix
The n × n matrix whose (j, k)-entry is ξjk = ω−jk for 0 ≤j, k ≤n−1
is called the Fourier matrix of order n, and it has the form
Fn =






1
1
1
· · ·
1
1
ξ
ξ2
· · ·
ξn−1
1
ξ2
ξ4
· · ·
ξn−2
...
...
...
...
...
1
ξn−1
ξn−2
· · ·
ξ






n×n
.
Note. Throughout this section entries are indexed from 0 to n −1.
For example, the upper left-hand entry of Fn is considered to be in the
(0, 0) position (rather than the (1, 1) position), and the lower right-
hand entry is in the (n −1, n −1) position. When the context makes it
clear, the subscript n on Fn is omitted.
The Fourier matrix
50 is a special case of the Vandermonde matrix introduced
in Example 4.3.4. Using (5.8.1) and (5.8.2), we see that the inner product of any
two columns in Fn, say, the rth and sth, is
F∗
∗rF∗s =
n−1

j=0
ξjrξjs =
n−1

j=0
ξ−jrξjs =
n−1

j=0
ξj(s−r) = 0.
In other words, the columns in Fn are mutually orthogonal. Furthermore, each
column in Fn has norm √n because
∥F∗k∥2
2 =
n−1

j=0
|ξjk|2 =
n−1

j=0
1 = n,
50
Some authors deﬁne the Fourier matrix using powers of ω rather than powers of ξ, and some
include a scalar multiple 1/n or 1/√n. These diﬀerences are superﬁcial, and they do not
aﬀect the basic properties. Our deﬁnition is the discrete counterpart of the integral operator
F(f) =  ∞
−∞x(t)e−i2πftdt that is usually taken as the deﬁnition of the continuous Fourier
transform.

358
Chapter 5
Norms, Inner Products, and Orthogonality
and consequently every column of Fn can be normalized by multiplying by the
same scalar—namely, 1/√n. This means that (1/√n )Fn is a unitary matrix.
Since it is also true that FT
n = Fn, we have
 1
√nFn
−1
=
 1
√nFn
∗
=
1
√nFn,
and therefore F−1
n
= Fn/n. But (5.8.1) says that ξk = ωk, so it must be the
case that
F−1
n
= 1
nFn = 1
n






1
1
1
· · ·
1
1
ω
ω2
· · ·
ωn−1
1
ω2
ω4
· · ·
ωn−2
...
...
...
...
...
1
ωn−1
ωn−2
· · ·
ω






n×n
.
Example 5.8.1
The Fourier matrices of orders 2 and 4 are given by
F2 =

1
1
1
−1

and
F4 =



1
1
1
1
1
−i
−1
i
1
−1
1
−1
1
i
−1
−i


,
and their inverses are
F−1
2
= 1
2F2 = 1
2

1
1
1
−1

and
F−1
4
= 1
4F4 = 1
4



1
1
1
1
1
i
−1
−i
1
−1
1
−1
1
−i
−1
i


.
Discrete Fourier Transform
Given a vector xn×1, the product Fnx is called the discrete Fourier
transform of x, and F−1
n x is called the inverse transform of x.
The kth entries in Fnx and F−1
n x are given by
[Fnx]k =
n−1

j=0
xjξjk
and
[F−1
n x]k = 1
n
n−1

j=0
xjωjk.
(5.8.3)

5.8 Discrete Fourier Transform
359
Example 5.8.2
Problem: Computing the Inverse Transform. Explain why any algorithm
or program designed to compute the discrete Fourier transform of a vector x
can also be used to compute the inverse transform of x.
Solution: Call such an algorithm FFT (see p. 373 for a speciﬁc example). The
fact that
F−1
n x = Fnx
n
= Fnx
n
means that FFT will return the inverse transform of x by executing the following
three steps:
(1)
x ←−x
(compute x ).
(2)
x ←−FFT(x)
(compute Fnx ).
(3)
x ←−(1/n)x
(compute n−1Fnx = F−1
n x ).
For example, computing the inverse transform of x = ( i
0
−i
0 )T is ac-
complished as follows—recall that F4 was given in Example 5.8.1.
x =



−i
0
i
0


,
F4x =



0
−2i
0
−2i


,
1
4F4x = 1
4



0
2i
0
2i


= F−1
4 x.
You may wish to check that this answer agrees with the result obtained by
directly multiplying F−1
4
times x, where F−1
4
is given in Example 5.8.1.
Example 5.8.3
Signal Processing.
Suppose that a microphone is placed under a hovering
helicopter, and suppose that Figure 5.8.3 represents the sound signal that is
recorded during 1 second of time.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
-6
-4
-2
0
2
4
6
Figure 5.8.3

360
Chapter 5
Norms, Inner Products, and Orthogonality
It seems reasonable to expect that the signal should have oscillatory components
together with some random noise contamination. That is, we expect the signal
to have the form
y(τ) =

k
αk cos 2πfkτ + βk sin 2πfkτ

+ Noise.
But due to the noise contamination, the oscillatory nature of the signal is only
barely apparent—the characteristic “chop-a chop-a chop-a” is not completely
clear. To reveal the oscillatory components, the magic of the Fourier transform
is employed. Let x be the vector obtained by sampling the signal at n equally
spaced points between time τ = 0 and τ = 1 ( n = 512 in our case), and let
y = (2/n)Fnx = a + ib,
where
a = (2/n)Re (Fnx) and b = (2/n)Im (Fnx) .
Using only the ﬁrst n/2 = 256 entries in a and ib, we plot the points in
{(0, a0), (1, a1), . . . , (255, a255)}
and
{(0, ib0), (1, ib1), . . . , (255, ib255)}
to produce the two graphs shown in Figure 5.8.4.
0
50
100
150
200
250
300
-0.5
0
0.5
1
1.5
Real Axis
Frequency
0
50
100
150
200
250
300
-2
-1.5
-1
-0.5
0
0.5
Imaginary Axis
Frequency
Figure 5.8.4
Now there are some obvious characteristics—the plot of a in the top graph of
Figure 5.8.4 has a spike of height approximately 1 at entry 80, and the plot of
ib in the bottom graph has a spike of height approximately −2 at entry 50.
These two spikes indicate that the signal is made up primarily of two oscillatory

5.8 Discrete Fourier Transform
361
components—the spike in the real vector a indicates that one of the oscillatory
components is a cosine of frequency 80 Hz (or period = 1/80 ) whose amplitude
is approximately 1, and the spike in the imaginary vector ib indicates there is a
sine component with frequency 50 Hz and amplitude of about 2. In other words,
the Fourier transform indicates that the signal is
y(τ) = cos 2π(80τ) + 2 sin 2π(50τ) + Noise.
In truth, the data shown in Figure 5.8.3 was artiﬁcially generated by contami-
nating the function y(τ) = cos 2π(80τ) + 2 sin 2π(50τ) with some normally dis-
tributed zero-mean noise, and therefore the plot of (2/n)Fnx shown in Figure
5.8.4 does indeed accurately reﬂect the true nature of the signal. To understand
why Fn reveals the hidden frequencies, let cos 2πft and sin 2πft denote the
discrete cosine and discrete sine vectors
cos 2πft =








cos

2πf · 0
n

cos

2πf · 1
n

cos

2πf · 2
n

...
cos

2πf · n−1
n









and
sin 2πft =








sin

2πf · 0
n

sin

2πf · 1
n

sin

2πf · 2
n

...
sin

2πf · n−1
n









,
where t = ( 0/n
1/n
2/n
· · ·
n−1/n )T is the discrete time vector. If
the discrete exponential vectors ei2πft and e−i2πft are deﬁned in the natural
way as ei2πft = cos 2πft + i sin 2πft and e−i2πft = cos 2πft −i sin 2πft, and
if 0 ≤f < n is an integer frequency,
51 then
ei2πft =






ω0f
ω1f
ω2f
...
ω(n−1)f






= n

F−1
n

∗f = nF−1
n ef,
where ef is the n × 1 unit vector with a 1 in the f th component—remember
that components of vectors are indexed from 0 to n−1 throughout this section.
Similarly, the fact that
ξkf = ω−kf = 1ω−kf = ωknω−kf = ωk(n−f)
for
k = 0, 1, 2, . . .
allows us to conclude that if 0 ≤n −f < n, then
e−i2πft =






ξ0f
ξ1f
ξ2f
...
ξ(n−1)f






=






ω0(n−f)
ω1(n−f)
ω2(n−f)
...
ω(n−1)(n−f)






= n

F−1
n

∗n−f = nF−1
n en−f.
51
The assumption that frequencies are integers is not overly harsh because the Fourier series for
a periodic function requires only integer frequencies—recall Example 5.4.6.

362
Chapter 5
Norms, Inner Products, and Orthogonality
Therefore, if 0 < f < n, then
Fnei2πft = nef
and
Fne−i2πft = nen−f.
(5.8.4)
Because cos θ = (eiθ +e−iθ)/2 and sin θ = (eiθ −e−iθ)/2i, it follows from (5.8.4)
that for any scalars α and β,
Fn(α cos 2πft) = αFn
ei2πft + e−i2πft
2

= nα
2 (ef + en−f)
and
Fn(β sin 2πft) = βFn
ei2πft −e−i2πft
2i

= nβ
2i (ef −en−f) ,
so that
2
nFn(α cos 2πft) = αef + αen−f
(5.8.5)
and
2
nFn(β sin 2πft) = −βief + βien−f.
(5.8.6)
The trigonometric functions α cos 2πfτ and β sin 2πfτ have amplitudes α and
β, respectively, and their frequency is f (their period is 1/f ). The discrete
vectors α cos 2πft and β sin 2πft are obtained by evaluating α cos 2πfτ and
β sin 2πfτ at the discrete points in t = ( 0
1/n
2/n
· · ·
(n −1)/n )T . As
depicted in Figure 5.8.5 for n = 32 and f = 4, the vectors αef and αen−f
are interpreted as two pulses of magnitude α at frequencies f and n −f.
28
4
32
16
8
24
α
Frequency
 n = 32      f = 4
0
(1/16)F( 
)
Time
-α
0
α
1
αcos πt
αcos πt
Figure 5.8.5

5.8 Discrete Fourier Transform
363
The vector α cos 2πft is said to be in the time domain, while the pulses
αef and αen−f are said to be in the frequency domain. The situation for
β sin 2πft is similarly depicted in Figure 5.8.6 in which −βief and βien−f are
considered two pulses of height −β and β, respectively.
1
Time
-β
0
β
28
4
32
16
8
24
β
- βi
i
Frequency
 n = 32      f = 4
0
(1/16)F( 
)
sin πt
β
sin πt
β
Figure 5.8.6
Therefore, if a waveform is given by a ﬁnite sum
x(τ) =

k
(αk cos 2πfkτ + βk sin 2πfkτ)
in which the fk ’s are integers, and if x is the vector containing the values of
x(τ) at n equally spaced points between time τ = 0 and τ = 1, then, provided
that n is suﬃciently large,
2
nFnx = 2
nFn

k
αk cos 2πfkt + βk sin 2πfkt

=

k
2
nFn (αk cos 2πfkt) +

k
2
nFn (βk sin 2πfkt)
=

k
αk (efk + en−fk) + i

k
βk (−efk + en−fk) ,
(5.8.7)
and this exposes the frequency and amplitude of each of the components. If n is
chosen so that max{fk} < n/2, then the pulses represented by ef and en−f are

364
Chapter 5
Norms, Inner Products, and Orthogonality
symmetric about the point n/2 in the frequency domain, and the information in
just the ﬁrst (or second) half of the frequency domain completely characterizes
the original waveform—this is why only 512/2=256 points are plotted in the
graphs shown in Figure 5.8.4. In other words, if
y = 2
nFnx =

k
αk (efk + en−fk) + i

k
βk (−efk + en−fk) ,
(5.8.8)
then the information in
yn/2 =

k
αkefk −i

k
βkefk
(the ﬁrst half of y )
is enough to reconstruct the original waveform. For example, the equation of the
waveform shown in Figure 5.8.7 is
x(τ) = 3 cos 2πτ + 5 sin 2πτ,
(5.8.9)
Amplitude
1
. 2 5
. 5
. 7 5
1
0
2
3
4
5
6
- 1
- 2
- 3
- 4
- 5
- 6
Time
Figure 5.8.7
and it is completely determined by the four values in
x =



x(0)
x(1/4)
x(1/2)
x(3/4)


=



3
5
−3
−5


.
To capture equation (5.8.9) from these four values, compute the vector y deﬁned
by (5.8.8) to be
y = 2
4F4x =



1
1
1
1
1
−i
−1
i
1
−1
1
−1
1
i
−1
−i






3
5
−3
−5


=



0
3 −5i
0
3 + 5i



=



0
3
0
3


+ i



0
−5
0
5


= 3(e1 + e3) + 5i(−e1 + e3).

5.8 Discrete Fourier Transform
365
The real part of y tells us there is a cosine component with amplitude = 3 and
frequency = 1, while the imaginary part of y says there is a sine component
with amplitude = 5 and frequency = 1. This is depicted in the frequency
domain shown in Figure 5.8.8.
Imaginary Axis
4
1
2
3
1
0
2
3
4
5
6
- 1
- 2
- 3
- 4
- 5
- 6
Frequency
Real Axis
4
1
2
3
1
0
2
3
4
5
6
Frequency
Figure 5.8.8
Putting this information together allows us to conclude that the equation of the
waveform must be x(τ) = 3 cos 2πτ + 5 sin 2πτ. Since
1 = max{fk} < n
2 = 4
2 = 2,
the information in just the ﬁrst half of y
yn/2 =

0
3

+ i

0
−5

= 3e1 −5ie1
suﬃces to completely characterize x(τ).
These elementary ideas help explain why applying F to a sample from a
signal can reveal the oscillatory components of the signal. But there is still a
signiﬁcant amount of theory that is well beyond the scope of this example. The
purpose here is to just hint at how useful the discrete Fourier transform is and
why it is so important in analyzing the nature of complicated waveforms.

366
Chapter 5
Norms, Inner Products, and Orthogonality
If
a =




α0
α1
...
αn−1




n×1
and
b =




β0
β1
...
βn−1




n×1
,
then the vector
a ⊙b=













α0β0
α0β1 + α1β0
α0β2 + α1β1 + α2β0
...
αn−2βn−1 + αn−1βn−2
αn−1βn−1
0













2n×1
(5.8.10)
is called the convolution of a and b. The 0 in the last position is for con-
venience only—it makes the size of the convolution twice the size of the origi-
nal vectors, and this provides a balance in some of the formulas involving con-
volution. Furthermore, it is sometimes convenient to pad a and b with n
additional zeros to consider them to be vectors with 2n components. Setting
αn = · · · = α2n−1 = βn = · · · = β2n−1 = 0 allows us to write the kth entry in
a ⊙b as
[a ⊙b]k =
k

j=0
αjβk−j
for
k = 0, 1, 2, . . . , 2n −1.
A visual way to form a ⊙b is to “slide” the reversal of b “against” a as
depicted in Figure 5.8.9, and then sum the resulting products.
α0
α1
...
αn−1
×
βn−1
...
β1
β0
α0
α1
...
αn−1
×
×
βn−1
...
β1
β0
α0
α1
α2
...
αn−1
×
×
×
βn−1
...
β2
β1
β0
· · ·
α0
...
αn−2
αn−1
×
×
βn−1
βn−2
...
β0
α0
...
αn−2
αn−1 ×βn−1
βn−2
...
β0
Figure 5.8.9
The convolution operation is a natural occurrence in a variety of situations,
and polynomial multiplication is one such example.

5.8 Discrete Fourier Transform
367
Example 5.8.4
Polynomial Multiplication. For p(x) = n−1
k=0 αkxk, q(x) = n−1
k=0 βkxk, let
a = ( α0
α1
· · ·
αn−1 )T
and b = ( β0
β1
· · ·
βn−1 )T . The product
p(x)q(x) = γ0 + γ1x + γ2x2 + · · · + γ2n−2x2n−2 is a polynomial of degree 2n −2
in which γk is simply the kth component of the convolution a ⊙b because
p(x)q(x) =
2n−2

k=0


k

j=0
αjβk−j

xk =
2n−2

k=0
[a ⊙b]kxk.
(5.8.11)
In other words, polynomial multiplication and convolution are equivalent opera-
tions, so if we can devise an eﬃcient way to perform a convolution, then we can
eﬃciently multiply two polynomials, and conversely.
There are two facets involved in eﬃciently performing a convolution. The
ﬁrst is the realization that the discrete Fourier transform has the ability to
convert a convolution into an ordinary product, and vice versa. The second is
the realization that it’s possible to devise a fast algorithm to compute a discrete
Fourier transform. These two facets are developed below.
Convolution Theorem
Let a × b denote the entry-by-entry product
a × b =




α0
α1
...
αn−1



×




β0
β1
...
βn−1



=




α0β0
α1β1
...
αn−1βn−1




n×1
,
and let ˆa and ˆb be the padded vectors
ˆa =









α0
...
αn−1
0
...
0









2n×1
and
ˆb =









β0
...
βn−1
0
...
0









2n×1
.
If F = F2n is the Fourier matrix of order 2n, then
F(a ⊙b) = (Fˆa) × (Fˆb) and a ⊙b = F−1
(Fˆa) × (Fˆb)

.
(5.8.12)

368
Chapter 5
Norms, Inner Products, and Orthogonality
Proof.
Observe that the tth component in F∗j × F∗k is
[F∗j × F∗k]t = ξtjξtk = ξt(j+k) = [F∗j+k]t ,
so that the columns of F have the property that
F∗j × F∗k = F∗j+k
for each
j, k = 0, 1, . . . , (n −1).
This means that if Fˆa, Fˆb, and F(a ⊙b) are expressed as combinations of
columns of F as indicated below,
Fˆa =
n−1

k=0
αkF∗k,
Fˆb =
n−1

k=0
βkF∗k,
and
F(a ⊙b) =
2n−2

k=0
[a ⊙b]kF∗k,
then the computation of (Fˆa)×(Fˆb) is exactly the same as forming the product
of two polynomials in the sense that
(Fˆa) × (Fˆb) =
n−1

k=0
αkF∗k
 n−1

k=0
βkF∗k

=
2n−2

k=0


k

j=0
αjβk−j

F∗k
=
2n−2

k=0
[a ⊙b]kF∗k = F(a ⊙b).
According to the convolution theorem, the convolution of two n × 1 vectors
can be computed by executing three discrete Fourier transforms of order 2n
an×1 ⊙bn×1 = F−1
2n

(F2nˆa) × (F2nˆb)

.
(5.8.13)
The fact that one of them is an inverse transform is not a source of diﬃculty—
recall Example 5.8.2. But it is still not clear that much has been accomplished.
Performing a convolution by following the recipe called for in deﬁnition (5.8.10)
requires n2 scalar multiplications (you are asked to verify this in the exercises).
Performing a discrete Fourier transform of order 2n by standard matrix–vector
multiplication requires 4n2 scalar multiplications, so using matrix–vector multi-
plication to perform the computations on the right-hand side of (5.8.13) requires
at least 12 times the number of scalar multiplications demanded by the deﬁnition
of convolution. So, if there is an advantage to be gained by using the convolution
theorem, then it is necessary to be able to perform a discrete Fourier transform
in far fewer scalar multiplications than that required by standard matrix–vector
multiplication. It was not until 1965 that this hurdle was overcome. Two Ameri-
cans, J. W. Cooley and J. W. Tukey, introduced a fast Fourier transform (FFT)
algorithm that requires only on the order of (n/2) log2 n scalar multiplications
to compute Fnx. Using the FFT together with the convolution theorem requires

5.8 Discrete Fourier Transform
369
only about 3n log2 n multiplications to perform a convolution of two n × 1 vec-
tors, and when n is large, this is signiﬁcantly less than the n2 factor demanded
by the deﬁnition of convolution.
The magic of the fast Fourier transform algorithm emanates from the fact
that if n is a power of 2, then a discrete Fourier transform of order n can be
executed by performing two transforms of order n/2. To appreciate exactly how
this comes about, observe that when n = 2r we have

ξjn =

ξ2jn/2 , so

1, ξ, ξ2, ξ3, . . . , ξn−1
= the nth roots of unity
if and only if

1, ξ2, ξ4, ξ6, . . . , ξn−2
= the (n/2)th roots of unity.
This means that the (j, k)-entries in the Fourier matrices Fn and Fn/2 are
[Fn]jk = ξjk
and
[Fn/2]jk = (ξ2)jk = ξ2jk.
(5.8.14)
If the columns of Fn are permuted so that columns with even subscripts are
listed before those with odd subscripts, and if PT
n is the corresponding permu-
tation matrix, then we can partition FnPT
n as
FnPT
n = [F∗0 F∗2 · · · F∗n−2 | F∗1 F∗3 · · · F∗n−1] =
 A n
2×n
2
B n
2×n
2
C n
2×n
2
G n
2×n
2

.
By using (5.8.14) together with the facts that
ξnk = 1
and
ξn/2 = cos 2π(n/2)
n
−i sin 2π(n/2)
n
= −1,
we see that the entries in A, B, C, and G are
Ajk = Fj,2k = ξ2jk = [Fn/2]jk,
Bjk = Fj,2k+1 = ξj(2k+1) = ξjξ2jk = ξj[Fn/2]jk,
Cjk = F n
2 +j, 2k = ξ( n
2 +j)2k = ξnkξ2jk = ξ2jk = [Fn/2]jk,
Gjk = F n
2 +j, 2k+1 = ξ( n
2 +j)(2k+1) = ξnkξn/2ξjξ2jk = −ξjξ2jt = −ξj[Fn/2]jk.
In other words, if Dn/2 is the diagonal matrix
Dn/2 =




1
ξ ξ2
...
ξ
n
2 −1



,
then
FnPT
n =
 A(n/2)×(n/2)
B(n/2)×(n/2)
C(n/2)×(n/2)
G(n/2)×(n/2)

=
 Fn/2
Dn/2Fn/2
Fn/2
−Dn/2Fn/2

.
This fundamental feature of the discrete Fourier transform is summarized below.

370
Chapter 5
Norms, Inner Products, and Orthogonality
Decomposing the Fourier Matrix
If n = 2r, then
Fn =
 Fn/2
Dn/2Fn/2
Fn/2
−Dn/2Fn/2

Pn,
(5.8.15)
where
Dn/2 =




1
ξ
ξ2
...
ξ
n
2 −1




contains half of the nth roots of unity and Pn is the “even–odd” per-
mutation matrix deﬁned by
PT
n = [e0 e2 e4 · · · en−2 | e1 e3 e5 · · · en−1] .
The decomposition (5.8.15) says that a discrete Fourier transform of order
n = 2r can be accomplished by two Fourier transforms of order n/2 = 2r−1, and
this leads to the FFT algorithm. To get a feel for how the FFT works, consider
the case when n = 8, and proceed to “divide and conquer.” If
x8 =











x0
x1
x2
x3
x4
x5
x6
x7











,
then
P8x8 =












x0
x2
x4
x6
x1
x3
x5
x7












=


x(0)
4
x(1)
4

,
so
F8x8 =
 F4
D4F4
F4
−D4F4
 
x(0)
4
x(1)
4

=

F4x(0)
4
+ D4F4x(1)
4
F4x(0)
4
−D4F4x(1)
4

.
(5.8.16)
But
P4x(0)
4
=




x0
x4
x2
x6



=


x(0)
2
x(1)
2


and
P4x(1)
4
=




x1
x5
x3
x7



=


x(2)
2
x(3)
2

,

5.8 Discrete Fourier Transform
371
so
F4x(0)
4
=
 F2
D2F2
F2
−D2F2
 
x(0)
2
x(1)
2

=

F2x(0)
2
+ D2F2x(1)
2
F2x(0)
2
−D2F2x(1)
2


and
(5.8.17)
F4x(1)
4
=
 F2
D2F2
F2
−D2F2
 
x(2)
2
x(3)
2

=

F2x(2)
2
+ D2F2x(3)
2
F2x(2)
2
−D2F2x(3)
2

.
Now, since F2 =
 1
1
1
−1

, it is a trivial matter to compute the terms
F2x(0)
2 ,
F2x(1)
2 ,
F2x(2)
2 ,
F2x(3)
2 .
Of course, to actually carry out the computation, we need to work backward
through the preceding sequence of steps. That is, we start with
˜x8 =













x(0)
2
x(1)
2
x(2)
2
x(3)
2













=













x0
x4
x2
x6
x1
x5
x3
x7













,
(5.8.18)
and use (5.8.17) followed by (5.8.16) to work downward in the following tree.
F2x(0)
2
F2x(1)
2
↘
↙
F4x(0)
4
↘
↘
F2x(2)
2
F2x(3)
2
↘
↙
F4x(1)
4
↙
↙
F8x8
But there appears to be a snag. In order to work downward through this
tree, we cannot start directly with x8—we must start with the permutation ˜x8
shown in (5.8.18). So how is this initial permutation determined? Looking back
reveals that the entries in ˜x8 were obtained by ﬁrst sorting the xj ’s into two
groups—the entries in the even positions were separated from those in the odd

372
Chapter 5
Norms, Inner Products, and Orthogonality
positions. Then each group was broken into two more groups by again separating
the entries in the even positions from those in the odd positions.

0
1
2
3
4
5
6
7

↙
↘

0
2
4
6


1
3
5
7

↙
↘
↙
↘

0
4


2
6


1
5


3
7

(5.8.19)
In general, this even–odd sorting process (sometimes called a perfect shuﬄe)
produces the permutation necessary to initiate the algorithm. A clever way to
perform a perfect shuﬄe is to use binary representations and observe that the
ﬁrst level of sorting in (5.8.19) is determined according to whether the least
signiﬁcant bit is 0 or 1, the second level of sorting is determined by the second
least signiﬁcant bit, and so on—this is illustrated in Table 5.8.1 for n = 8.
Table 5.8.1
Natural order
First level
Second level
0 ↔000
0 ↔000
0 ↔000
1 ↔001
2 ↔010
4 ↔100
2 ↔010
4 ↔100
2 ↔010
3 ↔011
6 ↔110
6 ↔110
4 ↔100
1 ↔001
1 ↔001
5 ↔101
3 ↔011
5 ↔101
6 ↔110
5 ↔101
3 ↔011
7 ↔111
7 ↔111
7 ↔111
But all intermediate levels in this sorting process can be eliminated because
something very nice occurs. Examination of the last column in Table 5.8.1 reveals
that the binary bits in the perfect shuﬄe ordering are exactly the reversal of the
binary bits in the natural ordering. In other words,
•
to generate the perfect shuﬄe of the numbers 0, 1, 2, . . . , n−1, simply reverse
the bits in the binary representation of each number.
We can summarize the fast Fourier transform by the following implementa-
tion that utilizes array operations.
52
52
There are a variety of diﬀerent ways to implement the FFT, and choosing a practical imple-
mentation frequently depends on the hardware being used as well as the application under
consideration. The FFT ranks high on the list of useful algorithms because it provides an ad-
vantage in a large variety of applications, and there are many more facets of the FFT than
those presented here (e.g., FFT when n is not a power of 2). In fact, there are entire texts
devoted to these issues, so the interested student need only go as far as the nearest library to
ﬁnd more details.

5.8 Discrete Fourier Transform
373
Fast Fourier Transform
For a given input vector x containing n = 2r components, the discrete
Fourier transform Fnx is the result of successively creating the following
arrays.
X1×n ←−rev(x)
(bit reverse the subscripts)
For j = 0, 1, 2, 3, . . . , r −1
D ←−








1
e−πi/2j
e−2πi/2j
e−3πi/2j
...
e−(2j−1)πi/2j








2j×1
(Half of the (2j+1)th roots of 1,
perhaps from a lookup table)
X(0) ←−

X∗0
X∗2
X∗4
· · ·
X∗2r−j−2
	
2j×2r−j−1
X(1) ←−

X∗1
X∗3
X∗5
· · ·
X∗2r−j−1
	
2j×2r−j−1
X ←−

 X(0) + D × X(1)
X(0) −D × X(1)

2j+1×2r−j−1

 Deﬁne × to mean
[D × M]ij = dimij

Example 5.8.5
Problem: Perform the FFT on x =


x0
x1
x2
x3

.
Solution: Start with X ←−rev(x) = ( x0
x2
x1
x3 ) .
For j = 0 :
D ←−(1)
(Half of the square roots of 1)
X(0) ←−( x0
x1 )
X(1) ←−( x2
x3 )
and
D × X(1) ←−( x2
x3 )
X ←−

 X(0) + D × X(1)
X(0) −D × X(1)

=

 x0 + x2
x1 + x3
x0 −x2
x1 −x3


374
Chapter 5
Norms, Inner Products, and Orthogonality
For j = 1 :
D ←−

1
−i

(Half of the 4th roots of 1)
X(0) ←−

x0 + x2
x0 −x2

X(1) ←−

x1 + x3
x1 −x3

and
D × X(1) ←−

x1 + x3
−ix1 + ix3

X ←−
 X(0) + D × X(1)
X(0) −D × X(1)

=



x0 + x2 + x1 + x3
x0 −x2 −ix1 + ix3
x0 + x2 −x1 −x3
x0 −x2 + ix1 −ix3


= F4x
Notice that this agrees with the result obtained by using direct matrix–vector
multiplication with F4 given in Example 5.8.1.
To understand why it is called the “fast” Fourier transform, simply count
the number of multiplications the FFT requires. Observe that the jth iteration
requires 2j multiplications for each column in X(1), and there are 2r−j−1
columns, so 2r−1 multiplications are used for each iteration.
53 Since r iterations
are required, the total number of multiplications used by the FFT does not exceed
2r−1r = (n/2) log2 n.
FFT Multiplication Count
If n is a power of 2, then applying the FFT to a vector of n components
requires at most (n/2) log2 n multiplications.
The (n/2) log2 n count represents a tremendous advantage over the n2
factor demanded by a direct matrix–vector product. To appreciate the magnitude
of the diﬀerence between n2 and (n/2) log2 n, look at Figure 5.8.10.
53
Actually, we can get by with slightly fewer multiplications if we take advantage of the fact that
the ﬁrst entry in D is always 1 and if we observe that no multiplications are necessary when
j = 0. But when n is large, these savings are relatively insigniﬁcant, so they are ignored in
the multiplication count.

5.8 Discrete Fourier Transform
375
f(n) = n2
512
256
128
0
0
100000
200000
300000
n
f(n) = (n/2) log2n
Figure 5.8.10
The small dark portion at the bottom of the graph is the area under the curve
f(n) = (n/2) log2 n—it is tiny in comparison to the area under f(n) = n2.
For example, if n = 512, then n2 = 262, 144, but (n/2) log2 n = 2304. In
other words, for n = 512, the FFT is on the order of 100 times faster than
straightforward matrix–vector multiplication, and for larger values of n, the
gap is even wider—Figure 5.8.10 illustrates just how fast the diﬀerence between
n2 and (n/2) log2 n grows as n increases. Since Cooley and Tukey introduced
the FFT in 1965, it has risen to a position of fundamental importance. The FFT
and the convolution theorem are extremely powerful tools, and they have been
principal components of the computational revolution that now touches our lives
countless times each day.
Example 5.8.6
Problem: Fast Integer Multiplication. Consider two positive integers whose
base-b representations are
c = (γn−1γn−2 · · · γ1γ0)b
and
d = (δn−1δn−2 · · · δ1δ0)b.
Use the convolution theorem together with the FFT to compute the product cd.
Solution: If we let
p(x) =
n−1

k=0
γkxk,
q(x) =
n−1

k=0
δkxk,
c =




γ0
γ1
...
γn−1



,
and
d =




δ0
δ1
...
δn−1



,

376
Chapter 5
Norms, Inner Products, and Orthogonality
then
c = γn−1bn−1 + γn−2bn−2 + · · · + γ1b1 + γ0b0 = p(b),
d = δn−1bn−1 + δn−2bn−2 + · · · + δ1b1 + δ0b0 = q(b),
and it follows from (5.8.11) that the product cd is given by
cd = p(b)q(b) = [c⊙d]2n−2b2n−2 +[c⊙d]2n−3b2n−3 +· · ·+[c⊙d]1b1 +[c⊙d]0b0.
It looks as though the convolution c ⊙d provides the base-b representation for
cd, but this is not quite the case because it is possible to have some [c⊙d]k ≥b.
For example, if c = 20110 and d = 42510, then
c ⊙d =


1
0
2

⊙


5
2
4

=







5
2
14
4
8
0







,
so
cd = (8×104) + (4×103) + (14×102) + (2×101) + (5×100).
(5.8.20)
But when numbers like 14 (i.e., greater than or equal to the base) appear in c⊙d,
it is relatively easy to decompose them by writing 14 = (1×101) + (4×100), so
14×102 =

(1×101) + (4×100)

×102 = (1×103) + (4×102).
Substituting this in (5.8.20) and combining coeﬃcients of like powers produces
the base-10 representation of the product
cd = (8×104) + (5×103) + (4×102) + (2×101) + (5×100) = 8542510.
Computing c ⊙d directly demands n2 multiplications, but using the FFT in
conjunction with the convolution theorem requires only about 3n log2 n mul-
tiplications, which is considerably less than n2 for large values of n. Thus it
is possible to multiply very long base-b integers much faster than by using di-
rect methods. Most digital computers have binary integer multiplication (usually
64-bit multiplication not requiring the FFT) built into their hardware, but for
ultra-high-precision multiplication or for more general base-b multiplication, the
FFT is a viable tool.
Exercises for section 5.8
5.8.1. Evaluate the following convolutions.
(a)


1
2
3

⊙


4
5
6

.
(b)


−1
0
1

⊙


1
0
−1

.
(c)


1
1
1

⊙


α0
α1
α2

.

5.8 Discrete Fourier Transform
377
5.8.2.
(a)
Evaluate the discrete Fourier transform of



1
−i
−1
i


.
(b)
Evaluate the inverse transform of



1
i
−1
−i


.
5.8.3. Verify directly that F4 =
 F2
D2F2
F2
−D2F2

P4, where the F4, P4, and
D2, are as deﬁned in (5.8.15).
5.8.4. Use the following vectors to perform the indicated computations:
a =
 α0
α1

,
b =
 β0
β1

,
ˆa =



α0
α1
0
0


,
ˆb =



β0
β1
0
0


.
(a)
Compute a ⊙b, F4(a ⊙b), and (F4ˆa) × (F4ˆb).
(b)
By using F−1
4
as given in Example 5.8.1, compute
F−1
4

(F4ˆa) × (F4ˆb)

.
Compare this with the results guaranteed by the convolution
theorem.
5.8.5. For p(x) = 2x −3 and q(x) = 3x −4, compute the product p(x)q(x)
by using the convolution theorem.
5.8.6. Use convolutions to form the following products.
(a)
4310 × 2110.
(b)
1238 × 6018.
(c)
10102 × 11012.
5.8.7. Let a and b be n × 1 vectors, where n is a power of 2.
(a)
Show that the number of multiplications required to form a⊙b
by using the deﬁnition of convolution is n2.
Hint: 1 + 2 + · · · + k = k(k + 1)/2.
(b)
Show that the number of multiplications required to form a⊙b
by using the FFT in conjunction with the convolution theorem
is 3n log2 n + 7n. Sketch a graph of 3n log2 n (the 7n factor is
dropped because it is not signiﬁcant), and compare it with the
graph of n2 to illustrate why the FFT in conjunction with the
convolution theorem provides such a big advantage.

378
Chapter 5
Norms, Inner Products, and Orthogonality
5.8.8. A waveform given by a ﬁnite sum
x(τ) =

k
(αk cos 2πfkτ + βk sin 2πfkτ)
in which the fk ’s are integers and max{fk} ≤3 is sampled at eight
equally spaced points between τ = 0 and τ = 1. Let
x =











x(0/8)
x(1/8)
x(2/8)
x(3/8)
x(4/8)
x(5/8)
x(6/8)
x(7/8)











,
and suppose that
y = 1
4F8x =











0
−5i
1 −3i
4
0
4
1 + 3i
5i











.
What is the equation of the waveform?
5.8.9. Prove that a ⊙b = b ⊙a for all a, b ∈Cn—i.e., convolution is a
commutative operation.
5.8.10. For p(x) = n−1
k=0 αkxk and the nth roots of unity ξk, let
a = ( α0 α1 α2 · · · αn−1 )T
and
p = ( p(1) p(ξ) p(ξ2) · · · p(ξn−1) )T .
Explain why Fna = p and a = F−1
n p. This says that the discrete
Fourier transform allows us to go from the representation of a polynomial
p in terms of its coeﬃcients αk to the representation of p in terms of its
values p(ξk), and the inverse transform takes us in the other direction.
5.8.11. For two polynomials p(x) = n−1
k=0 αkxk and q(x) = n−1
k=0 βkxk, let
p =




p(1)
p(ξ)
...
p(ξ2n−1)




and
q =




q(1)
q(ξ)
...
q(ξ2n−1)



,
where

1, ξ, ξ2, . . . , ξ2n−1
are now the 2nth roots of unity. Explain
why the coeﬃcients in the product
p(x)q(x) = γ0 + γ1x + γ2x2 + · · · + γ2n−2x2n−2
must be given by




γ0
γ1
γ2
...



= F−1
2n




p(1)q(1)
p(ξ)q(ξ)
p(ξ2)q(ξ2)
...



.
This says that the product p(x)q(x) is completely determined by the
values of p(x) and q(x) at the 2nth roots of unity.

5.8 Discrete Fourier Transform
379
5.8.12. A circulant matrix is deﬁned to be a square matrix that has the form
C =






c0
cn−1
cn−2
· · ·
c1
c1
c0
cn−1
· · ·
c2
c2
c1
c0
· · ·
c3
...
...
...
...
...
cn−1
cn−2
cn−3
· · ·
c0






n×n
.
In other words, the entries in each column are the same as the previous
column, but they are shifted one position downward and wrapped around
at the top—the (j, k)-entry in C can be described as cjk = cj−k (mod n).
(Some authors use CT
rather than C as the deﬁnition—it doesn’t
matter.)
(a)
If Q is the circulant matrix deﬁned by
Q =






0
0
· · ·
0
1
1
0
· · ·
0
0
0
1
· · ·
0
0
...
...
...
...
...
0
0
· · ·
1
0






n×n
,
and if p(x) = c0 + c1x + · · · + cn−1xn−1, verify that
C = p(Q) = c0I + c1Q + · · · + cn−1Qn−1.
(b)
Explain why the Fourier matrix of order n diagonalizes Q in
the sense that
FQF−1 = D =




1
0
· · ·
0
0
ξ
· · ·
0
...
...
...
...
0
0
· · ·
ξn−1



,
where the ξk ’s are the nth roots of unity.
(c)
Prove that the Fourier matrix of order n diagonalizes every
n × n circulant in the sense that
FCF−1 =




p(1)
0
· · ·
0
0
p(ξ)
· · ·
0
...
...
...
...
0
0
· · ·
p(ξn−1)



,
where p(x) = c0 + c1x + · · · + cn−1xn−1.
(d)
If C1 and C2 are any pair of n × n circulants, explain why
C1C2 = C2C1—i.e., all circulants commute with each other.

380
Chapter 5
Norms, Inner Products, and Orthogonality
5.8.13. For a nonsingular circulant Cn×n, explain how to use the FFT algo-
rithm to eﬃciently perform the following operations.
(a)
Solve a system Cx = b.
(b)
Compute C−1.
(c)
Multiply two circulants C1C2.
5.8.14. For the vectors
a=




α0
α1
...
αn−1



, b=




β0
β1
...
βn−1



, ˆa=









α0
...
αn−1
0
...
0









2n×1
, and ˆb=









β0
...
βn−1
0
...
0









2n×1
,
let C be the 2n × 2n circulant matrix (see Exercise 5.8.12) whose ﬁrst
column is ˆa.
(a)
Show that the convolution operation can be described as a
matrix–vector product by demonstrating that
a ⊙b = Cˆb.
(b)
Use this relationship to give an alternate proof of the convolu-
tion theorem. Hint: Use the diagonalization result of Exercise
5.8.12 together with the result of Exercise 5.8.10.
5.8.15. The Kronecker product of two matrices Am×n and Bp×q is deﬁned
to be the mp × nq matrix
A ⊗B =




a11B
a12B
· · ·
a1nB
a21B
a22B
· · ·
a2nB
...
...
...
...
am1B
am2B
· · ·
amnB



.
This is also known as the tensor product or the direct product. Al-
though there is an extensive list of properties that the tensor product
satisﬁes, this exercise requires only the following two elementary facts
(which you need not prove unless you feel up to it). The complete list of
properties is given in Exercise 7.8.11 (p. 597) along with remarks about
Kronecker, and another application appears in Exercise 7.6.10 (p. 573).
A ⊗(B ⊗C) = (A ⊗B) ⊗C.
(A⊗B)(C⊗D) = AC⊗BD (if AC and BD exist).

5.8 Discrete Fourier Transform
381
(a)
If n = 2r, and if Pn is the even–odd permutation matrix
described in (5.8.15), explain why
Rn = (I2r−1 ⊗P21)(I2r−2 ⊗P22) · · · (I21 ⊗P2r−1)(I20 ⊗P2r)
is the permutation matrix associated with the bit reversing (or
perfect shuﬄe) permutation described in (5.8.19) and Table
5.8.1. Hint: Work it out for n = 8 by showing
R8











x0
x1
x2
x3
x4
x5
x6
x7











=











x0
x4
x2
x6
x1
x5
x3
x7











,
and you will see why it holds in general.
(b)
Suppose n = 2r, and set
Bn =
 In/2
Dn/2
In/2
−Dn/2

.
According to (5.8.15), the Fourier matrix can be written as
Fn = Bn(I2 ⊗Fn/2)Pn.
Expand on this idea by proving that Fn can be factored as
Fn = LnRn
in which
Ln = (I20 ⊗B2r)(I21 ⊗B2r−1) · · · (I2r−2 ⊗B22)(I2r−1 ⊗B21),
and where Rn is the bit reversing permutation
Rn = (I2r−1 ⊗P21)(I2r−2 ⊗P22) · · · (I21 ⊗P2r−1)(I20 ⊗P2r).
Notice that this says Fnx = LnRnx, so the discrete Fourier
transform of x is obtained by ﬁrst performing the bit revers-
ing permutation to x followed by r applications of the terms
(I2r−k ⊗B2k) from Ln. This in fact is the FFT algorithm in
factored form. Hint: Deﬁne two sequences by the rules
L2k = (I2r−k ⊗B2k) L2k−1
and
R2k = R2k−1 (I2r−k ⊗P2k) ,
where
L1 = 1,
R1 = In,
B2 = F2,
P2 = I2,
and use induction on k to prove that
I2r−k ⊗F2k = L2kR2k
for
k = 1, 2, . . . , r.

382
Chapter 5
Norms, Inner Products, and Orthogonality
5.8.16. For p(x) = α0 + α1x + α2x2 + · · · + αn−1xn−1, prove that
1
n
n−1

k=0
p(ξk)
2 = |α0|2 + |α1|2 + · · · + |αn−1|2,
where

1, ξ, ξ2, . . . , ξn−1
are the nth roots of unity.
5.8.17. Consider a waveform that is given by the ﬁnite sum
x(τ) =

k
(αk cos 2πfkτ + βk sin 2πfkτ)
in which the fk ’s are distinct integers, and let
x =

k
(αk cos 2πfkt + βk sin 2πfkt)
be the vector containing the values of x(τ) at n > 2 max{fk} equally
spaced points between τ = 0 and τ = 1 as described in Example 5.8.3.
Use the discrete Fourier transform to prove that
∥x∥2
2 = n
2

k

α2
k + β2
k

.
5.8.18. Let η be an arbitrary scalar, and let
c =






1
η
η2
...
η2n−1






and
a =




α0
α1
...
αn−1



.
Prove that cT (a ⊙a) =

cT ˆa
2 .
5.8.19. Apply the FFT algorithm to the vector x8 =



x0
x1
...
x7


, and then verify
that your answer agrees with the result obtained by computing F8x8
directly.

5.9 Complementary Subspaces
383
5.9
COMPLEMENTARY SUBSPACES
The sum of two subspaces X and Y of a vector space V was deﬁned on p. 166
to be the set X + Y = {x + y | x ∈X and y ∈Y}, and it was established that
X + Y is another subspace of V. For example, consider the two subspaces of
ℜ3 shown in Figure 5.9.1 in which X is a plane through the origin, and Y is a
line through the origin.
Figure 5.9.1
Notice that X and Y are disjoint in the sense that X ∩Y = 0. The paral-
lelogram law for vector addition makes it clear that X + Y = ℜ3 because each
vector in ℜ3 can be written as “something from X plus something from Y. ”
Thus ℜ3 is resolved into a pair of disjoint components X and Y. These ideas
generalize as described below.
Complementary Subspaces
Subspaces X, Y of a space V are said to be complementary whenever
V = X + Y
and
X ∩Y = 0,
(5.9.1)
in which case V is said to be the direct sum of X and Y, and this is
denoted by writing V = X ⊕Y.
•
For a vector space V with subspaces X, Y having respective bases
BX and BY, the following statements are equivalent.
▷
V = X ⊕Y.
(5.9.2)
▷
For each v ∈V there are unique vectors x ∈X and y ∈Y
such that v = x + y.
(5.9.3)
▷
BX ∩BY = φ and BX ∪BY is a basis for V.
(5.9.4)

384
Chapter 5
Norms, Inner Products, and Orthogonality
Prove these by arguing (5.9.2)
=⇒
(5.9.3)
=⇒
(5.9.4)
=⇒
(5.9.2).
Proof of (5.9.2) =⇒(5.9.3).
First recall from (4.4.19) that
dim V = dim (X + Y) = dim X + dim Y −dim (X ∩Y) .
If V = X ⊕Y, then X ∩Y = 0, and thus dim V = dim X + dim Y. To prove
(5.9.3), suppose there are two ways to represent a vector v ∈V as “something
from X plus something from Y. ” If v = x1 + y1 = x2 + y2, where x1, x2 ∈X
and y1, y2 ∈Y, then
x1 −x2 = y2 −y1
=⇒



x1 −x2 ∈X
and
x1 −x2 ∈Y



=⇒
x1 −x2 ∈X ∩Y.
But X ∩Y = 0, so x1 = x2 and y1 = y2.
Proof of (5.9.3) =⇒(5.9.4).
The hypothesis insures that V = X + Y, and we
know from (4.1.2) that BX ∪BY spans X +Y, so BX ∪BY must be a spanning
set for V. To prove BX ∪BY is linearly independent, let BX = {x1, x2, . . . , xr}
and BY = {y1, y2, . . . , ys} , and suppose that
0 =
r

i=1
αixi +
s

j=1
βjyj.
This is one way to express 0 as “something from X plus something from Y, ”
while 0 = 0 + 0 is another way. Consequently, (5.9.3) guarantees that
r

i=1
αixi = 0
and
s

j=1
βjyj = 0,
and hence α1 = α2 = · · · = αr = 0 and β1 = β2 = · · · = βs = 0 because
BX
and BY are both linearly independent. Therefore, BX ∪BY is linearly
independent, and hence it is a basis for V.
Proof of (5.9.4) =⇒(5.9.2).
If BX ∪BY is a basis for V, then BX ∪BY is a
linearly independent set. This together with the fact that BX ∪BY always spans
X + Y means BX ∪BY is a basis for X + Y as well as for V. Consequently,
V = X + Y, and hence
dim X + dim Y = dim V = dim(X + Y) = dim X + dim Y −dim (X ∩Y) ,
so dim (X ∩Y) = 0 or, equivalently, X ∩Y = 0.
If V = X ⊕Y, then (5.9.3) says there is one and only one way to resolve
each v ∈V into an “X -component” and a “Y-component” so that v = x + y.
These two components of v have a deﬁnite geometrical interpretation. Look
back at Figure 5.9.1 in which ℜ3 = X ⊕Y, where X is a plane and Y is a line
outside the plane, and notice that x (the X -component of v ) is the result of
projecting v onto X along a line parallel to Y, and y (the Y-component of
v ) is obtained by projecting v onto Y along a line parallel to X. This leads
to the following formal deﬁnition of a projection.

5.9 Complementary Subspaces
385
Projection
Suppose that V = X ⊕Y so that for each v ∈V there are unique
vectors x ∈X and y ∈Y such that v = x + y.
•
The vector x is called the projection of v onto X along Y.
•
The vector y is called the projection of v onto Y along X.
It’s clear that if X ⊥Y in Figure 5.9.1, then this notion of projection
agrees with the concept of orthogonal projection that was discussed on p. 322.
The phrase “oblique projection” is sometimes used to emphasize the fact that
X and Y are not orthogonal subspaces. In this text the word “projection” is
synonymous with the term “oblique projection.” If it is known that X ⊥Y, then
we explicitly say “orthogonal projection.” Orthogonal projections are discussed
in detail on p. 429.
Given a pair of complementary subspaces X and Y of ℜn and an arbitrary
vector v ∈ℜn = X ⊕Y, how can the projection of v onto X be computed?
One way is to build a projector (a projection operator) that is a matrix Pn×n
with the property that for each v ∈ℜn, the product Pv is the projection of
v onto X along Y. Let BX = {x1, x2, . . . , xr} and BY = {y1, y2, . . . , yn−r}
be respective bases for X and Y so that BX ∪BY is a basis for ℜn—recall
(5.9.4). This guarantees that if the xi ’s and yi ’s are placed as columns in
Bn×n =

x1 x2 · · · xr | y1 y2 · · · yn−r

=

Xn×r | Yn×(n−r)

,
then B is nonsingular. If Pn×n is to have the property that Pv is the pro-
jection of v onto X
along Y for every v ∈ℜn, then (5.9.3) implies that
Pxi = xi, i = 1, 2, . . . , r and Pyj = 0, j = 1, 2, . . . , n −r, so
PB = P

X | Y

=

PX | PY

=

X | 0

and, consequently,
P =

X | 0

B−1 = B

Ir
0
0
0

B−1.
(5.9.5)
To argue that Pv is indeed the projection of v onto X along Y, set x = Pv
and y = (I −P)v and observe that v = x + y, where
x = Pv =

X | 0

B−1v ∈R (X) = X
(5.9.6)
and
y = (I −P)v = B

0
0
0
In−r

B−1v =

0 | Y

B−1v ∈R (Y) = Y.
(5.9.7)

386
Chapter 5
Norms, Inner Products, and Orthogonality
Is it possible that there can be more than one projector onto X
along
Y ? No, P is unique because if P1 and P2 are two such projectors, then for
i = 1, 2, we have PiB = Pi

X | Y

=

PiX | PiY

=

X | 0

, and this implies
P1B = P2B, which means P1 = P2. Therefore, (5.9.5) is the projector onto
X along Y, and this formula for P is independent of which pair of bases for X
and Y is selected. Notice that the argument involving (5.9.6) and (5.9.7) also
establishes that the complementary projector—the projector onto Y along
X —must be given by
Q = I −P =

0 | Y

B−1 = B

0
0
0
In−r

B−1.
Below is a summary of the basic properties of projectors.
Projectors
Let X and Y be complementary subspaces of a vector space V so that
each v ∈V can be uniquely resolved as v = x + y, where x ∈X and
y ∈Y. The unique linear operator P deﬁned by Pv = x is called the
projector onto X along Y, and P has the following properties.
•
P2 = P
( P is idempotent).
(5.9.8)
•
I −P is the complementary projector onto Y along X.
(5.9.9)
•
R (P) = {x | Px = x} (the set of “ﬁxed points” for P ).
(5.9.10)
•
R (P) = N (I −P) = X and R (I −P) = N (P) = Y.
(5.9.11)
•
If V = ℜn or Cn, then P is given by
P =

X | 0

X | Y
−1 =

X | Y
 
I
0
0
0
 
X | Y
−1,
(5.9.12)
where the columns of X and Y are respective bases for X and Y.
Other formulas for P are given on p. 634.
Proof.
Some of these properties have already been derived in the context of
ℜn. But since the concepts of projections and projectors are valid for all vector
spaces, more general arguments that do not rely on properties of ℜn will be
provided. Uniqueness is evident because if P1 and P2 both satisfy the deﬁning
condition, then P1v = P2v for every v ∈V, and thus P1 = P2. The linearity
of P follows because if v1 = x1 +y1 and v2 = x2 +y2, where x1, x2 ∈X and
y1, y2 ∈Y, then P(αv1 + v2) = αx1 + x2 = αPv1 + Pv2. To prove that P is
idempotent, write
P2v = P(Pv) = Px = x = Pv for every v ∈V
=⇒
P2 = P.

5.9 Complementary Subspaces
387
The validity of (5.9.9) is established by observing that v = x + y = Pv + y
implies y = v −Pv = (I −P)v. The properties in (5.9.11) and (5.9.10) are
immediate consequences of the deﬁnition. Formula (5.9.12) is the result of the
arguments that culminated in (5.9.5), but it can be more elegantly derived by
making use of the material in §4.7 and §4.8. If BX and BY are bases for X
and Y, respectively, then B = BX ∪BY = {x1, x2, . . . , xr, y1, y2, . . . , yn−r} is
a basis for V, and (4.7.4) says that the matrix of P with respect to B is
[P]B =
1
[Px1]B
 · · ·
 [Pxr]B
 [Py1]B
 · · ·
 [Pyn−r]B
2
=
1
[x1]B
 · · ·
 [xr]B
 [0]B
 · · ·
 [0]B
2
=
1
e1
 · · ·
 er
 0
 · · ·
 0
2
=

Ir
0
0
0

.
If S is the standard basis, then (4.8.5) says that [P]B = B−1[P]SB in which
B = [I]BS =
1
[x1]S
 · · ·
 [xr]S
 [y1]S · · ·
 [yn−r]S
2
=

X | Y

,
and therefore [P]S = B[P]BB−1 =

X | Y
 Ir
0
0
0

X | Y
−1.
In the language of §4.8, statement (5.9.12) says that P is similar to the
diagonal matrix
 I
0
0
0

. In the language of §4.9, this means that P must be
the matrix representation of the linear operator that when restricted to X is
the identity operator and when restricted to Y is the zero operator.
Statement (5.9.8) says that if P is a projector, then P is idempotent
( P2 = P ). But what about the converse—is every idempotent linear operator
necessarily a projector? The following theorem says, “Yes.”
Projectors and Idempotents
A linear operator P on V is a projector if and only if P2 = P. (5.9.13)
Proof.
The fact that every projector is idempotent was proven in (5.9.8). The
proof of the converse rests on the fact that
P2 = P
=⇒
R (P) and N (P) are complementary subspaces.
(5.9.14)
To prove this, observe that V = R (P) + N (P) because for each v ∈V,
v = Pv + (I −P)v,
where
Pv ∈R (P) and (I −P)v ∈N (P).
(5.9.15)

388
Chapter 5
Norms, Inner Products, and Orthogonality
Furthermore, R (P) ∩N (P) = 0 because
x ∈R (P) ∩N (P)
=⇒
x = Py and Px = 0
=⇒
x = Py = P2y = 0,
and thus (5.9.14) is established. Now that we know R (P) and N (P) are com-
plementary, we can conclude that P is a projector because each v ∈V can be
uniquely written as v = x + y, where x ∈R (P) and y ∈N (P), and (5.9.15)
guarantees Pv = x.
Notice that there is a one-to-one correspondence between the set of idem-
potents (or projectors) deﬁned on a vector space V and the set of all pairs of
complementary subspaces of V in the following sense.
•
Each idempotent P deﬁnes a pair of complementary spaces—namely, R (P)
and N (P).
•
Every pair of complementary subspaces X and Y deﬁnes an idempotent—
namely, the projector onto X along Y.
Example 5.9.1
Problem: Let X and Y be the subspaces of ℜ3 that are spanned by
BX =





1
−1
−1

,


0
1
−2





and
BY =





1
−1
0




,
respectively. Explain why X and Y are complementary, and then determine
the projector onto X along Y. What is the projection of v = ( −2
1
3 )T
onto X along Y? What is the projection of v onto Y along X?
Solution: BX and BY are linearly independent, so they are bases for X and
Y, respectively. The spaces X and Y are complementary because
rank [X | Y] = rank


1
0
1
−1
1
−1
−1
−2
0

= 3
insures that BX ∪BY is a basis for ℜ3—recall (5.9.4). The projector onto X
along Y is obtained from (5.9.12) as
P =

X | 0

X | Y
−1 =


1
0
0
−1
1
0
−1
−2
0




−2
−2
−1
1
1
0
3
2
1

=


−2
−2
−1
3
3
1
0
0
1

.
You may wish to verify that P is indeed idempotent. The projection of v onto
X along Y is Pv, and, according to (5.9.9), the projection of v onto Y along
X is (I −P)v.

5.9 Complementary Subspaces
389
Example 5.9.2
Angle between Complementary Subspaces.
The angle between nonzero
vectors u and v in ℜn was deﬁned on p. 295 to be the number 0 ≤θ ≤
π/2 such that cos θ = vT u/ ∥v∥2 ∥u∥2 . It’s natural to try to extend this idea
to somehow make sense of angles between subspaces of ℜn. Angles between
completely general subspaces are presently out of our reach—they are discussed
in §5.15—but the angle between a pair of complementary subspaces is within
our grasp. When ℜn = R ⊕N with R ̸= 0 ̸= N, the angle (also known as the
minimal angle) between R and N is deﬁned to be the number 0 < θ ≤π/2
that satisﬁes
cos θ = max
u∈R
v∈N
vT u
∥v∥2 ∥u∥2
=
max
u∈R, v∈N
∥u∥2=∥v∥2=1
vT u.
(5.9.16)
While this is a good deﬁnition, it’s not easy to use—especially if one wants to
compute the numerical value of cos θ. The trick in making θ more accessible
is to think in terms of projections and sin θ = (1 −cos2 θ)1/2. Let P be the
projector such that R (P) = R and N (P) = N, and recall that the matrix
2-norm (p. 281) of P is
∥P∥2 = max
∥x∥2=1 ∥Px∥2 .
(5.9.17)
In other words, ∥P∥2 is the length of a longest vector in the image of the unit
sphere under transformation by P. To understand how sin θ is related to ∥P∥2 ,
consider the situation in ℜ3. The image of the unit sphere under P is obtained
by projecting the sphere onto R along lines parallel to N. As depicted in
Figure 5.9.2, the result is an ellipse in R.
x
v
θ
θ
=
max
∥x∥=1
∥Px
∥∥
P
=
∥∥
v
∥
Figure 5.9.2
The norm of a longest vector v on this ellipse equals the norm of P. That is,
∥v∥2 = max∥x∥2=1 ∥Px∥2 = ∥P∥2 , and it is apparent from the right triangle in

390
Chapter 5
Norms, Inner Products, and Orthogonality
Figure 5.9.2 that
sin θ = ∥x∥2
∥v∥2
=
1
∥v∥2
=
1
∥P∥2
.
(5.9.18)
A little reﬂection on the geometry associated with Figure 5.9.2 should convince
you that in ℜ3 a number θ satisﬁes (5.9.16) if and only if θ satisﬁes (5.9.18)—a
completely rigorous proof validating this fact in ℜn is given in §5.15.
Note: Recall from p. 281 that ∥P∥2 = √λmax, where λmax is the largest
number λ such that PT P −λI is a singular matrix. Consequently,
sin θ =
1
∥P∥2
=
1
√λmax
.
Numbers λ such that PT P −λI is singular are called eigenvalues of PT P
(they are the main topic of discussion in Chapter 7, p. 489), and the numbers
√
λ are the singular values of P discussed on p. 411.
Exercises for section 5.9
5.9.1. Let X and Y be subspaces of ℜ3 whose respective bases are
BX =





1
1
1

,


1
2
2





and
BY =





1
2
3




.
(a)
Explain why X and Y are complementary subspaces of ℜ3.
(b)
Determine the projector P onto X along Y as well as the
complementary projector Q onto Y along X.
(c)
Determine the projection of v =

2
−1
1

onto Y along X.
(d)
Verify that P and Q are both idempotent.
(e)
Verify that R (P) = X = N (Q) and N (P) = Y = R (Q).
5.9.2. Construct an example of a pair of nontrivial complementary subspaces
of ℜ5, and explain why your example is valid.
5.9.3. Construct an example to show that if V = X + Y but X ∩Y ̸= 0, then
a vector v ∈V can have two diﬀerent representations as
v = x1 + y1
and
v = x2 + y2,
where x1, x2 ∈X and y1, y2 ∈Y, but x1 ̸= x2 and y1 ̸= y2.

5.9 Complementary Subspaces
391
5.9.4. Explain why ℜn×n = S ⊕K, where S and K are the subspaces of
n × n symmetric and skew-symmetric matrices, respectively. What is
the projection of A =
 1
2
3
4
5
6
7
8
9

onto S along K? Hint: Recall
Exercise 3.2.6.
5.9.5. For a general vector space, let X and Y be two subspaces with respec-
tive bases BX = {x1, x2, . . . , xm} and BY = {y1, y2, . . . , yn} .
(a)
Prove that X ∩Y = 0 if and only if {x1, . . . , xm, y1, . . . , yn}
is a linearly independent set.
(b)
Does BX ∪BY being linear independent imply X ∩Y = 0?
(c)
If BX ∪BY is a linearly independent set, does it follow that X
and Y are complementary subspaces? Why?
5.9.6. Let P be a projector deﬁned on a vector space V. Prove that (5.9.10)
is true—i.e., prove that the range of a projector is the set of its “ﬁxed
points” in the sense that R (P) = {x ∈V | Px = x}.
5.9.7. Suppose that V = X ⊕Y, and let P be the projector onto X along
Y. Prove that (5.9.11) is true—i.e., prove
R (P) = N (I −P) = X
and
R (I −P) = N (P) = Y.
5.9.8. Explain why ∥P∥2 ≥1 for every projector P ̸= 0. When is ∥P∥2 = 1?
5.9.9. Explain why ∥I −P∥2 = ∥P∥2 for all projectors that are not zero and
not equal to the identity.
5.9.10. Prove that if u, v ∈ℜn×1 are vectors such that vT u = 1, then
I −uvT 
2 =
uvT 
2 = ∥u∥2 ∥v∥2 =
uvT 
F ,
where ∥⋆∥F is the Frobenius matrix norm deﬁned in (5.2.1) on p. 279.
5.9.11. Suppose that X and Y are complementary subspaces of ℜn, and let
B = [X | Y] be a nonsingular matrix in which the columns of X and
Y constitute respective bases for X and Y. For an arbitrary vector
v ∈ℜn×1, explain why the projection of v onto X along Y can be
obtained by the following two-step process.
(1)
Solve the system Bz = v for z.
(2)
Partition z as z =
 z1
z2

, and set p = Xz1.

392
Chapter 5
Norms, Inner Products, and Orthogonality
5.9.12. Let P and Q be projectors.
(a)
Prove R (P) = R (Q) if and only if PQ = Q and QP = P.
(b)
Prove N (P) = N (Q) if and only if PQ = P and QP = Q.
(c)
Prove that if E1, E2, . . . , Ek are projectors with the same range,
and if α1, α2, . . . , αk are scalars such that 
j αj = 1, then

j αjEj is a projector.
5.9.13. Prove that rank (P) = trace (P) for every projector P deﬁned on ℜn.
Hint: Recall Example 3.6.5 (p. 110).
5.9.14. Let {Xi}k
i=1 be a collection of subspaces from a vector space V, and
let Bi denote a basis for Xi. Prove that the following statements are
equivalent.
(i)
V = X1 + X2 + · · · + Xk and Xj ∩(X1 + · · · + Xj−1) = 0 for
each j = 2, 3, . . . , k.
(ii)
For each vector v ∈V, there is one and only one way to write
v = x1 + x2 + · · · + xk, where xi ∈Xi.
(iii)
B = B1 ∪B2 ∪· · · ∪Bk with Bi ∩Bj = φ for i ̸= j is a basis
for V.
Whenever any one of the above statements is true, V is said to be the
direct sum of the Xi ’s, and we write V = X1 ⊕X2 ⊕· · · ⊕Xk. Notice
that for k = 2, (i) and (5.9.1) say the same thing, and (ii) and (iii)
reduce to (5.9.3) and (5.9.4), respectively.
5.9.15. For complementary subspaces X and Y of ℜn, let P be the projec-
tor onto X along Y, and let Q = [X | Y] in which the columns of
X and Y constitute bases for X and Y, respectively. Prove that if
Q−1An×nQ is partitioned as Q−1AQ =
 A11
A12
A21
A22

, then
Q

A11
0
0
0

Q−1=PAP,
Q

0
A12
0
0

Q−1 = PA(I −P),
Q

0
0
A21
0

Q−1= (I −P)AP, Q

0
0
0
A12

Q−1=(I −P)A(I −P).
This means that if A is considered as a linear operator on ℜn, and if
B = BX ∪BY, where BX and BY are the respective bases for X and
Y deﬁned by the columns of X and Y, then, in the context of §4.8, the
matrix representation of A with respect to B is [A]B =
 A11
A12
A21
A22


5.9 Complementary Subspaces
393
in which the blocks are matrix representations of restricted operators as
shown below.
A11 =
1
PAP/X
2
BX
.
A12 =
1
PA(I −P)/Y
2
BYBX
.
A21 =
1
(I −P)AP/X
2
BX BY
.
A22 =
1
(I −P)A(I −P)/Y
2
BY
.
5.9.16. Suppose that ℜn = X ⊕Y, where dim X = r, and let P be the
projector onto X
along Y. Explain why there exist matrices Xn×r
and Ar×n such that P = XA, where rank (X) = rank (A) = r and
AX = Ir. This is a full-rank factorization for P (recall Exercise 3.9.8).
5.9.17. For either a real or complex vector space, let E be the projector onto
X1 along Y1, and let F be the projector onto X2 along Y2. Prove
that E + F is a projector if and only if EF = FE = 0, and under this
condition, prove that R (E + F) = X1 ⊕X2 and N (E + F) = Y1 ∩Y2.
5.9.18. For either a real or complex vector space, let E be the projector onto
X1 along Y1, and let F be the projector onto X2 along Y2. Prove
that E −F is a projector if and only if EF = FE = F, and under this
condition, prove that R (E −F) = X1 ∩Y2 and N (E −F) = Y1 ⊕X2.
Hint: P is a projector if and only if I −P is a projector.
5.9.19. For either a real or complex vector space, let E be the projector onto
X1 along Y1, and let F be the projector onto X2 along Y2. Prove that
if EF = P = FE, then P is the projector onto X1∩X2 along Y1+Y2.
5.9.20. An inner pseudoinverse for Am×n is a matrix Xn×m such that
AXA = A, and an outer pseudoinverse for A is a matrix X satis-
fying XAX = X. When X is both an inner and outer pseudoinverse,
X is called a reﬂexive pseudoinverse.
(a)
If Ax = b is a consistent system of m equations in n un-
knowns, and if A−is any inner pseudoinverse for A, explain
why the set of all solutions to Ax = b can be expressed as
A−b + R

I −A−A

= {A−b + (I −A−A)h | h ∈ℜn}.
(b)
Let M and L be respective complements of R (A) and N (A)
so that Cm = R (A) ⊕M and Cn = L ⊕N (A). Prove that
there is a unique reﬂexive pseudoinverse X for A such that
R (X) = L and N (X) = M. Show that X = QA−P, where
A−is any inner pseudoinverse for A, P is the projector onto
R (A) along M, and Q is the projector onto L along N (A).

394
Chapter 5
Norms, Inner Products, and Orthogonality
5.10
RANGE-NULLSPACE DECOMPOSITION
Since there are inﬁnitely many diﬀerent pairs of complementary subspaces in
ℜn (or Cn ),
54 is some pair more “natural” than the rest? Without reference to
anything else the question is hard to answer. But if we start with a given matrix
An×n, then there is a very natural direct sum decomposition of ℜn deﬁned
by fundamental subspaces associated with powers of A. The rank plus nullity
theorem on p. 199 says that dim R (A)+dim N (A) = n, so it’s reasonable to ask
about the possibility of R (A) and N (A) being complementary subspaces. If A
is nonsingular, then it’s trivially true that R (A) and N (A) are complementary,
but when A is singular, this need not be the case because R (A) and N (A)
need not be disjoint. For example,
A =

0
1
0
0

=⇒

1
0

∈R (A) ∩N (A).
But all is not lost if we are willing to consider powers of A.
Range-Nullspace Decomposition
For every singular matrix An×n, there exists a positive integer k such
that R

Ak
and N

Ak
are complementary subspaces. That is,
ℜn = R

Ak
⊕N

Ak
.
(5.10.1)
The smallest positive integer k for which (5.10.1) holds is called the
index of A. For nonsingular matrices we deﬁne index(A) = 0.
Proof.
First observe that as A is powered the nullspaces grow and the ranges
shrink—recall Exercise 4.2.12.
N

A0
⊆N (A) ⊆N

A2
⊆· · · ⊆N

Ak
⊆N

Ak+1
⊆· · ·
R

A0
⊇R (A) ⊇R

A2
⊇· · · ⊇R

Ak
⊇R

Ak+1
⊇· · · .
(5.10.2)
The proof of (5.10.1) is attained by combining the four following properties.
Property 1.
There is equality at some point in each of the chains (5.10.2).
Proof.
If there is strict containment at each link in the nullspace chain in
(5.10.2), then the sequence of inequalities
dim N

A0
< dim N (A) < dim N

A2
< dim N

A3
< · · ·
54
All statements and arguments in this section are phrased in terms of ℜn, but everything we
say has a trivial extension to Cn.

5.10 Range-Nullspace Decomposition
395
holds, and this forces n < dim N

An+1
, which is impossible. A similar
argument proves equality exists somewhere in the range chain.
Property 2.
Once equality is attained, it is maintained throughout the rest of
both chains in (5.10.2). In other words,
N

A0
⊂N (A) ⊂· · · ⊂N

Ak
= N

Ak+1
= N

Ak+2
= · · ·
R

A0
⊃R (A) ⊃· · · ⊃R

Ak
= R

Ak+1
= R

Ak+2
= · · · .
(5.10.3)
To prove this for the range chain, observe that if k is the smallest nonneg-
ative integer such that R

Ak
= R

Ak+1
, then for all i ≥1,
R

Ai+k
= R

AiAk
= AiR

Ak
= AiR

Ak+1
= R

Ai+k+1
.
The nullspace chain stops growing at exactly the same place the ranges
stop shrinking because the rank plus nullity theorem (p. 199) insures that
dim N (Ap) = n −dim R (Ap).
Property 3.
If k is the value at which the ranges stop shrinking and the
nullspaces stop growing in (5.10.3), then R

Ak
∩N

Ak
= 0.
Proof.
If x ∈R

Ak
∩N

Ak
, then Aky = x for some y ∈ℜn, and
Akx = 0. Hence A2ky = Akx = 0 ⇒y ∈N

A2k
= N

Ak
⇒x = 0.
Property 4.
If k is the value at which the ranges stop shrinking and the
nullspaces stop growing in (5.10.3), then R

Ak
+ N

Ak
= ℜn.
Proof.
Use Property 3 along with (4.4.19), (4.4.15), and (4.4.6), to write
dim

R

Ak
+ N

Ak
= dim R

Ak
+ dim N

Ak
−dim R

Ak
∩N

Ak
= dim R

Ak
+ dim N

Ak
= n
=⇒
R

Ak
+ N

Ak
= ℜn.
Below is a summary of our observations concerning the index of a square matrix.
Index
The index of a square matrix A is the smallest nonnegative integer k
such that any one of the three following statements is true.
•
rank

Ak
= rank

Ak+1
.
•
R

Ak
= R

Ak+1
—i.e., the point where R

Ak
stops shrinking.
•
N

Ak
= N

Ak+1
—i.e., the point where N

Ak
stops growing.
For nonsingular matrices,
index (A)
=
0.
For singular matrices,
index (A) is the smallest positive integer k such that either of the fol-
lowing two statements is true.
•
R

Ak
∩N

Ak
= 0.
(5.10.4)
•
ℜn = R

Ak
⊕N

Ak
.

396
Chapter 5
Norms, Inner Products, and Orthogonality
Example 5.10.1
Problem: Determine the index of A =
 2
0
0
0
1
1
0
−1
−1

.
Solution: A is singular (because rank (A) = 2), so index(A) > 0. Since
A2 =


4
0
0
0
0
0
0
0
0


and
A3 =


8
0
0
0
0
0
0
0
0

,
we see that rank (A) > rank

A2
= rank

A3
, so index(A) = 2. Alternately,
R (A) = span





2
0
0

,


0
1
−1




, R

A2
= span


4
0
0

, R

A3
= span


8
0
0

,
so R (A) ⊃R

A2
= R

A3
implies index(A) = 2.
Nilpotent Matrices
•
Nn×n is said to be nilpotent whenever Nk = 0 for some positive
integer k.
•
k = index(N) is the smallest positive integer such that Nk = 0.
(Some authors refer to index(N) as the index of nilpotency.)
Proof.
To prove that k = index(N) is the smallest positive integer such that
Nk = 0, suppose p is a positive integer such that Np = 0, but Np−1 ̸= 0.
We know from (5.10.3) that R

N0
⊃R (N) ⊃· · · ⊃R

Nk
= R

Nk+1
=
R

Nk+2
= · · · , and this makes it clear that it’s impossible to have p < k or
p > k, so p = k is the only choice.
Example 5.10.2
Problem: Verify that
N =


0
1
0
0
0
1
0
0
0


is a nilpotent matrix, and determine its index.
Solution: Computing the powers
N2 =


0
0
1
0
0
0
0
0
0


and
N3 =


0
0
0
0
0
0
0
0
0

,
reveals that N is indeed nilpotent, and it shows that index(N) = 3 because
N3 = 0, but N2 ̸= 0.

5.10 Range-Nullspace Decomposition
397
Anytime ℜn can be written as the direct sum of two complementary sub-
spaces such that one of them is an invariant subspace for a given square matrix A
we have a block-triangular representation for A according to formula (4.9.9) on
p. 263. And if both complementary spaces are invariant under A, then (4.9.10)
says that this block-triangular representation is actually block diagonal.
Herein lies the true value of the range-nullspace decomposition (5.10.1) be-
cause it turns out that if k = index(A), then R

Ak
and N

Ak
are both
invariant subspaces under A. R

Ak
is invariant under A because
A

R

Ak
= R

Ak+1
= R

Ak
,
and N

Ak
is invariant because
x ∈A

N

Ak
=⇒
x = Aw for some w ∈N

Ak
= N

Ak+1
=⇒
Akx = Ak+1w = 0
=⇒
x ∈N

Ak
=⇒
A

N

Ak
⊆N

Ak
.
This brings us to a matrix decomposition that is an important building
block for developments that culminate in the Jordan form on p. 590.
Core-Nilpotent Decomposition
If A is an n × n singular matrix of index k such that rank

Ak
= r,
then there exists a nonsingular matrix Q such that
Q−1AQ =

Cr×r
0
0
N

(5.10.5)
in which C is nonsingular, and N is nilpotent of index k. In other
words, A is similar to a 2 × 2 block-diagonal matrix containing a non-
singular “core” and a nilpotent component. The block-diagonal matrix
in (5.10.5) is called a core-nilpotent decomposition of A.
Note: When A is nonsingular, k = 0 and r = n, so N is not present,
and we can set Q = I and C = A (the nonsingular core is everything).
So (5.10.5) says absolutely nothing about nonsingular matrices.
Proof.
Let Q =

X | Y

, where the columns of Xn×r and Yn×n−r constitute
bases for R

Ak
and N

Ak
, respectively. Equation (4.9.10) guarantees that
Q−1AQ must be block diagonal in form, and thus (5.10.5) is established. To see
that N is nilpotent, let
Q−1 =

U
V

,

398
Chapter 5
Norms, Inner Products, and Orthogonality
and write

Ck
0
0
Nk

= Q−1AkQ =

U
V

Ak
X | Y

=

UAkX
0
VAkX
0

.
Therefore, Nk = 0 and Q−1AkQ =
 Ck
0
0
0
	
. Since Ck is r × r and r =
rank

Ak
= rank

Q−1AkQ

= rank

Ck
, it must be the case that Ck is
nonsingular, and hence C is nonsingular. Finally, notice that index(N) = k
because if index(N) ̸= k, then Nk−1 = 0, so
rank

Ak−1
=rank

Q−1Ak−1Q

=rank

Ck−1
0
0
Nk−1

=rank

Ck−1
0
0
0

= rank

Ck−1
= r = rank

Ak
,
which is impossible because index(A) = k is the smallest integer for which there
is equality in ranks of powers.
Example 5.10.3
Problem: Let An×n have index k with rank

Ak
= r, and let
Q−1AQ =

Cr×r
0
0
N

with
Q =

Xn×r | Y

and Q−1 =
 Ur×n
V

be the core-nilpotent decomposition described in (5.10.5). Explain why
Q

Ir
0
0
0

Q−1 = XU = the projector onto R

Ak
along N

Ak
and
Q

0
0
0
In−r

Q−1 = YV = the projector onto N

Ak
along R

Ak
.
Solution: Because R

Ak
and N

Ak
are complementary subspaces, and
because the columns of X and Y constitute respective bases for these spaces,
it follows from the discussion concerning projectors on p. 386 that
P =

X | Y
 
I
0
0
0
 
X | Y
−1 = Q

Ir
0
0
0

Q−1 = XU
must be the projector onto R

Ak
along N

Ak
, and
I −P =

X | Y
 
0
0
0
I
 
X | Y
−1 = Q

0
0
0
In−r

Q−1 = YV
is the complementary projector onto N

Ak
along R

Ak
.

5.10 Range-Nullspace Decomposition
399
Example 5.10.4
Problem: Explain how each noninvertible linear operator deﬁned on an n-
dimensional vector space V can be decomposed as the “direct sum” of an in-
vertible operator and a nilpotent operator.
Solution: Let T be a linear operator of index k deﬁned on V = R ⊕N,
where R = R

Tk
and N = N

Tk
, and let E = T/R and F = T/N be
the restriction operators as described in §4.9. Since R and N are invariant
subspaces for T, we know from the discussion of matrix representations on
p. 263 that the right-hand side of the core-nilpotent decomposition in (5.10.5)
must be the matrix representation of T with respect to a basis BR ∪BN , where
BR and BN are respective bases for R and N. Furthermore, the nonsingular
matrix C and the nilpotent matrix N are the matrix representations of E and
F with respect to BR and BN , respectively. Consequently, E is an invertible
operator on R, and F is a nilpotent operator on N. Since V = R ⊕N, each
x ∈V can be expressed as x = r + n with r ∈R and n ∈N. This allows
us to formulate the concept of the direct sum of E and F by deﬁning E ⊕F
to be the linear operator on V such that (E ⊕F)(x) = E(r) + F(n) for each
x ∈V. Therefore,
T(x) = T(r + n) = T(r) + T(n) = (T/R)(r) + (T/N )(n)
= E(r) + F(n) = (E ⊕F)(x)
for each
x ∈V.
In other words, T = E ⊕F in which E = T/R is invertible and F = T/N is
nilpotent.
Example 5.10.5
Drazin Inverse.
Inverting the nonsingular core C and neglecting the nilpo-
tent part N in the core-nilpotent decomposition (5.10.5) produces a natural
generalization of matrix inversion. More precisely, if
A = Q

C
0
0
N

Q−1,
then
AD = Q

C−1
0
0
0

Q−1
(5.10.6)
deﬁnes the Drazin inverse of A. Even though the components in a core-
nilpotent decomposition are not uniquely deﬁned by A, it can be proven that
AD is unique and has the following properties.
•
AD = A−1 when A is nonsingular (the nilpotent part is not present).
•
ADAAD = AD, AAD = ADA, Ak+1AD = Ak, where k = index(A).
55
55
These three properties served as Michael P. Drazin’s original deﬁnition in 1968. Initially,

400
Chapter 5
Norms, Inner Products, and Orthogonality
•
If Ax = b is a consistent system of linear equations in which b ∈R

Ak
,
then x = ADb is the unique solution that belongs to R

Ak
(Exercise
5.10.9).
•
AAD is the projector onto R

Ak
along N

Ak
, and I −AAD is the
complementary projector onto N

Ak
along R

Ak
(Exercise 5.10.10).
•
If A is considered as a linear operator on ℜn, then, with respect to a basis
BR for R

Ak
, C is the matrix representation for the restricted operator
A/R(Ak) (see p. 263). Thus A/R(Ak) is invertible. Moreover,
1
AD
/R(Ak)
2
BR
= C−1 =
3
A/R(Ak)
−14
BR
,
so
AD
/R(Ak) =

A/R(Ak)
−1
.
In other words, AD is the inverse of A on R

Ak
, and AD is the zero
operator on N

Ak
, so, in the context of Example 5.10.4,
A = A/R(Ak) ⊕A/N(Ak)
and
AD =

A/R(Ak)
−1
⊕0/N(Ak).
Exercises for section 5.10
5.10.1. If A is a square matrix of index k > 0, prove that index(Ak) = 1.
5.10.2. If A is a nilpotent matrix of index k, describe the components in a
core-nilpotent decomposition of A.
5.10.3. Prove that if A is a symmetric matrix, then index(A) ≤1.
5.10.4. A ∈Cn×n is said to be a normal matrix whenever AA∗= A∗A.
Prove that if A is normal, then index(A) ≤1.
Note: All symmetric matrices are normal, so the result of this exercise
includes the result of Exercise 5.10.3 as a special case.
Drazin’s concept attracted little interest—perhaps due to Drazin’s abstract algebraic pre-
sentation. But eventually Drazin’s generalized inverse was recognized to be a useful tool for
analyzing nonorthogonal types of problems involving singular matrices. In this respect, the
Drazin inverse is complementary to the Moore–Penrose pseudoinverse discussed in Exercise
4.5.20 and on p. 423 because the Moore–Penrose pseudoinverse is more useful in applications
where orthogonality is somehow wired in (e.g., least squares).

5.10 Range-Nullspace Decomposition
401
5.10.5. Find a core-nilpotent decomposition and the Drazin inverse of
A =


−2
0
−4
4
2
4
3
2
2

.
5.10.6. For a square matrix A, any scalar λ that makes A −λI singular
is called an eigenvalue for A. The index of an eigenvalue λ is de-
ﬁned to be the index of the associated matrix A −λI. In other words,
index(λ) = index(A −λI). Determine the eigenvalues and the index of
each eigenvalue for the following matrices:
(a)
J =





1
0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
2
0
0
0
0
0
2




.
(b)
J =





1
1
0
0
0
0
1
1
0
0
0
0
1
0
0
0
0
0
2
1
0
0
0
0
2




.
5.10.7. Let P be a projector diﬀerent from the identity.
(a)
Explain why index(P) = 1. What is the index of I?
(b)
Determine the core-nilpotent decomposition for P.
5.10.8. Let N be a nilpotent matrix of index k, and suppose that x is a vector
such that Nk−1x ̸= 0. Prove that the set
C = {x, Nx, N2x, . . . , Nk−1x}
is a linearly independent set. C is sometimes called a Jordan chain or
a Krylov sequence.
5.10.9. Let A be a square matrix of index k, and let b ∈R

Ak
.
(a)
Explain why the linear system Ax = b must be consistent.
(b)
Explain why x = ADb is the unique solution in R

Ak
.
(c)
Explain why the general solution is given by ADb + N (A).
5.10.10. Suppose that A is a square matrix of index k, and let AD be the
Drazin inverse of A as deﬁned in Example 5.10.5. Explain why AAD
is the projector onto R

Ak
along N

Ak
. What does I −AAD
project onto and along?

402
Chapter 5
Norms, Inner Products, and Orthogonality
5.10.11. An algebraic group is a set G together with an associative operation
between its elements such that G is closed with respect to this operation;
G possesses an identity element E (which can be proven to be unique);
and every member A ∈G has an inverse A# (which can be proven to
be unique). These are essentially the axioms (A1), (A2), (A4), and (A5)
in the deﬁnition of a vector space given on p. 160. A matrix group is
a set of square matrices that forms an algebraic group under ordinary
matrix multiplication.
(a)
Show that the set of n × n nonsingular matrices is a matrix
group.
(b)
Show that the set of n × n unitary matrices is a subgroup of
the n × n nonsingular matrices.
(c)
Show that the set G =
5 α
α
α
α
  α ̸= 0
6
is a matrix group.
In particular, what does the identity element E ∈G look like,
and what does the inverse A# of A ∈G look like?
5.10.12. For singular matrices, prove that the following statements are equivalent.
(a)
A is a group matrix (i.e., A belongs to a matrix group).
(b)
R (A) ∩N (A) = 0.
(c)
R (A) and N (A) are complementary subspaces.
(d)
index(A) = 1.
(e)
There are nonsingular matrices Qn×n and Cr×r such that
Q−1AQ =

Cr×r
0
0
0

,
where
r = rank (A).
5.10.13. Let A ∈G for some matrix group G.
(a)
Show that the identity element E ∈G is the projector onto
R (A) along N (A) by arguing that E must be of the form
E = Q

Ir×r
0
0
0

Q−1.
(b)
Show that the group inverse of A (the inverse of A in G )
must be of the form
A# = Q

C−1
0
0
0

Q−1.

5.11 Orthogonal Decomposition
403
5.11
ORTHOGONAL DECOMPOSITION
The orthogonal complement of a single vector x was deﬁned on p. 322 to be the
set of all vectors orthogonal to x. Below is the natural extension of this idea.
Orthogonal Complement
For a subset M of an inner-product space V, the orthogonal com-
plement M⊥(pronounced “M perp”) of M is deﬁned to be the set
of all vectors in V that are orthogonal to every vector in M. That is,
M⊥=

x ∈V
 ⟨m x⟩= 0 for all m ∈M

.
For example, if M = {x} is a single vector in ℜ2, then, as illustrated in
Figure 5.11.1, M⊥is the line through the origin that is perpendicular to x. If
M is a plane through the origin in ℜ3, then M⊥is the line through the origin
that is perpendicular to the plane.
Figure 5.11.1
Notice that M⊥is a subspace of V even if M is not a subspace because M⊥is
closed with respect to vector addition and scalar multiplication (Exercise 5.11.4).
But if M is a subspace, then M and M⊥decompose V as described below.
Orthogonal Complementary Subspaces
If M is a subspace of a ﬁnite-dimensional inner-product space V, then
V = M ⊕M⊥.
(5.11.1)
Furthermore, if N is a subspace such that V = M ⊕N and N ⊥M
(every vector in N is orthogonal to every vector in M ), then
N = M⊥.
(5.11.2)

404
Chapter 5
Norms, Inner Products, and Orthogonality
Proof.
Observe that M ∩M⊥= 0 because if x ∈M and x ∈M⊥, then
x must be orthogonal to itself, and ⟨x x⟩= 0 implies x = 0. To prove that
M ⊕M⊥= V, suppose that BM and BM⊥are orthonormal bases for M
and M⊥, respectively. Since M and M⊥are disjoint, BM ∪BM⊥is an
orthonormal basis for some subspace S = M ⊕M⊥⊆V. If S ̸= V, then
the basis extension technique of Example 4.4.5 followed by the Gram–Schmidt
orthogonalization procedure of §5.5 yields a nonempty set of vectors E such that
BM ∪BM⊥∪E is an orthonormal basis for V. Consequently,
E ⊥BM
=⇒
E ⊥M
=⇒
E ⊆M⊥
=⇒
E ⊆span (BM⊥) .
But this is impossible because BM∪BM⊥∪E is linearly independent. Therefore,
E is the empty set, and thus V = M ⊕M⊥. To prove statement (5.11.2),
note that N ⊥M implies N ⊆M⊥, and coupling this with the fact that
M ⊕M⊥= V = M ⊕N together with (4.4.19) insures
dim N = dim V −dim M = dim M⊥.
Example 5.11.1
Problem: Let Um×m =

U1 | U2

be a partitioned orthogonal matrix. Explain
why R (U1) and R (U2) must be orthogonal complements of each other.
Solution: Statement (5.9.4) insures that ℜm = R (U1) ⊕R (U2), and we know
that R (U1) ⊥R (U2) because the columns of U are an orthonormal set.
Therefore, (5.11.2) guarantees that R (U2) = R (U1)⊥.
Perp Operation
If M is a subspace of an n-dimensional inner-product space, then the
following statements are true.
•
dim M⊥= n −dim M.
(5.11.3)
•
M⊥⊥= M.
(5.11.4)
Proof.
Property (5.11.3) follows from the fact that M and M⊥are comple-
mentary subspaces—recall (4.4.19). To prove (5.11.4), ﬁrst show that M⊥⊥⊆
M. If x ∈M⊥⊥, then (5.11.1) implies x = m + n, where m ∈M and
n ∈M⊥, so
0 = ⟨n x⟩= ⟨n m + n⟩= ⟨n m⟩+ ⟨n n⟩= ⟨n n⟩
=⇒
n = 0
=⇒
x ∈M,
and thus M⊥⊥⊆M. We know from (5.11.3) that dim M⊥= n −dim M and
dim M⊥⊥= n−dim M⊥, so dim M⊥⊥= dim M. Therefore, (4.4.6) guarantees
that M⊥⊥= M.

5.11 Orthogonal Decomposition
405
We are now in a position to understand why the four fundamental subspaces
associated with a matrix A ∈ℜm×n are indeed “fundamental.” First consider
R (A)⊥, and observe that for all y ∈ℜn,
x ∈R (A)⊥
⇐⇒
⟨Ay x⟩= 0
⇐⇒
yT AT x = 0
⇐⇒
7
y AT x
8
= 0
⇐⇒
AT x = 0
(Exercise 5.3.2)
⇐⇒
x ∈N

AT 
.
Therefore, R (A)⊥= N

AT 
. Perping both sides of this equation and replac-
ing
56 A by AT produces R

AT 
= N (A)⊥. Combining these observations
produces one of the fundamental theorems of linear algebra.
Orthogonal Decomposition Theorem
For every A ∈ℜm×n,
R (A)⊥= N

AT 
and
N (A)⊥= R

AT 
.
(5.11.5)
In light of (5.11.1), this means that every matrix A ∈ℜm×n produces
an orthogonal decomposition of ℜm and ℜn in the sense that
ℜm = R (A) ⊕R (A)⊥= R (A) ⊕N

AT 
,
(5.11.6)
and
ℜn = N (A) ⊕N (A)⊥= N (A) ⊕R

AT 
.
(5.11.7)
Theorems without hypotheses tend to be extreme in the sense that they
either say very little or they reveal a lot. The orthogonal decomposition theorem
has no hypothesis—it holds for all matrices—so, does it really say something
signiﬁcant? Yes, it does, and here’s part of the reason why.
In addition to telling us how to decompose ℜm and ℜn in terms of the
four fundamental subspaces of A, the orthogonal decomposition theorem also
tells us how to decompose A itself into more basic components. Suppose that
rank (A) = r, and let
BR(A) = {u1, u2, . . . , ur}
and
BN(AT) = {ur+1, ur+2, . . . , um}
be orthonormal bases for R (A) and N

AT 
, respectively, and let
BR(AT) = {v1, v2, . . . , vr}
and
BN(A) = {vr+1, vr+2, . . . , vn}
56
Here, as well as throughout the rest of this section, (⋆)T
can be replaced by (⋆)∗whenever
ℜm×n is replaced by Cm×n.

406
Chapter 5
Norms, Inner Products, and Orthogonality
be orthonormal bases for R

AT 
and N (A), respectively. It follows that
BR(A) ∪BN(AT) and BR(AT) ∪BN(A) are orthonormal bases for ℜm and ℜn,
respectively, and hence
Um×m =

u1 | u2 | · · · | um

and
Vn×n =

v1 | v2 | · · · | vn

(5.11.8)
are orthogonal matrices. Now consider the product R = UT AV, and notice
that rij = uT
i Avj. However, uT
i A = 0 for i = r + 1, . . . , m and Avj = 0 for
j = r + 1, . . . , n, so
R = UT AV =










uT
1 Av1
· · ·
uT
1 Avr
0
· · ·
0
...
...
...
...
...
uT
r Av1
· · ·
uT
r Avr
0
· · ·
0
0
· · ·
0
0
· · ·
0
...
...
...
...
...
0
· · ·
0
0
· · ·
0










.
(5.11.9)
In other words, A can be factored as
A = URVT = U

Cr×r
0
0
0

VT .
(5.11.10)
Moreover, C is nonsingular because it is r × r and
rank (C) = rank

C
0
0
0

= rank

UT AV

= rank (A) = r.
For lack of a better name, we will refer to (5.11.10) as a URV factorization.
We have just observed that every set of orthonormal bases for the four
fundamental subspaces deﬁnes a URV factorization. The situation is also re-
versible in the sense that every URV factorization of A deﬁnes an orthonor-
mal basis for each fundamental subspace. Starting with orthogonal matrices
U =

U1 | U2

and V =

V1 | V2

together with a nonsingular matrix Cr×r
such that (5.11.10) holds, use the fact that right-hand multiplication by a non-
singular matrix does not alter the range (Exercise 4.5.12) to observe
R (A) = R (UR) = R (U1C | 0) = R (U1C) = R (U1).
By (5.11.5) and Example 5.11.1, N

AT 
= R (A)⊥= R (U1)⊥= R (U2).
Similarly, left-hand multiplication by a nonsingular matrix does not change the
nullspace, so the second equation in (5.11.5) along with Example 5.11.1 yields
N (A) = N

RVT 
= N

CVT
1
0

= N

CVT
1

= N

VT
1

= R (V1)⊥= R (V2),
and R

AT 
= N (A)⊥= R (V2)⊥= R (V1). A summary is given below.

5.11 Orthogonal Decomposition
407
URV Factorization
For each A ∈ℜm×n of rank r, there are orthogonal matrices Um×m
and Vn×n and a nonsingular matrix Cr×r such that
A = URVT = U

Cr×r
0
0
0

m×n
VT .
(5.11.11)
•
The ﬁrst r columns in U are an orthonormal basis for R (A).
•
The last m−r columns of U are an orthonormal basis for N

AT 
.
•
The ﬁrst r columns in V are an orthonormal basis for R

AT 
.
•
The last n −r columns of V are an orthonormal basis for N (A).
Each diﬀerent collection of orthonormal bases for the four fundamental
subspaces of A produces a diﬀerent URV factorization of A. In the
complex case, replace (⋆)T by (⋆)∗and “orthogonal” by “unitary.”
Example 5.11.2
Problem: Explain how to make C lower triangular in (5.11.11).
Solution: Apply Householder (or Givens) reduction to produce an orthogonal
matrix Pm×m such that PA =
 B
0

, where B is r × n of rank r. House-
holder (or Givens) reduction applied to BT
results in an orthogonal matrix
Qn×n and a nonsingular upper-triangular matrix T such that
QBT =

Tr×r
0

=⇒
B =

TT | 0

Q
=⇒

B
0

=

TT
0
0
0

Q,
so A = PT  B
0

= PT  TT
0
0
0

Q is a URV factorization.
Note: C can in fact be made diagonal—see (p. 412).
Have you noticed the duality that has emerged concerning the use of fun-
damental subspaces of A to decompose ℜn (or Cn )? On one hand there is
the range-nullspace decomposition (p. 394), and on the other is the orthogo-
nal decomposition theorem (p. 405). Each produces a decomposition of A. The
range-nullspace decomposition of ℜn produces the core-nilpotent decomposition
of A (p. 397), and the orthogonal decomposition theorem produces the URV
factorization. In the next section, the URV factorization specializes to become

408
Chapter 5
Norms, Inner Products, and Orthogonality
the singular value decomposition (p. 412), and in a somewhat parallel manner,
the core-nilpotent decomposition paves the way to the Jordan form (p. 590).
These two parallel tracks constitute the backbone for the theory of modern linear
algebra, so it’s worthwhile to take a moment and reﬂect on them.
The range-nullspace decomposition decomposes ℜn with square matrices
while the orthogonal decomposition theorem does it with rectangular matrices.
So does this mean that the range-nullspace decomposition is a special case of,
or somehow weaker than, the orthogonal decomposition theorem? No! Even for
square matrices they are not very comparable because each says something that
the other doesn’t. The core-nilpotent decomposition (and eventually the Jordan
form) is obtained by a similarity transformation, and, as discussed in §§4.8–4.9,
similarity is the primary mechanism for revealing characteristics of A that are
independent of bases or coordinate systems. The URV factorization has little
to say about such things because it is generally not a similarity transforma-
tion. Orthogonal decomposition has the advantage whenever orthogonality is
naturally built into a problem—such as least squares applications. And, as dis-
cussed in §5.7, orthogonal methods often produce numerically stable algorithms
for ﬂoating-point computation, whereas similarity transformations are generally
not well suited for numerical computations. The value of similarity is mainly on
the theoretical side of the coin.
So when do we get the best of both worlds—i.e., when is a URV factoriza-
tion also a core-nilpotent decomposition? First, A must be square and, second,
(5.11.11) must be a similarity transformation, so U = V. Surprisingly, this
happens for a rather large class of matrices described below.
Range Perpendicular to Nullspace
For rank (An×n) = r, the following statements are equivalent:
•
R (A) ⊥N (A),
(5.11.12)
•
R (A) = R

AT 
,
(5.11.13)
•
N (A) = N

AT 
,
(5.11.14)
•
A = U

Cr×r
0
0
0

UT
(5.11.15)
in which U is orthogonal and C is nonsingular. Such matrices will
be called RPN matrices, short for“range perpendicular to nullspace.”
Some authors call them range-symmetric or EP matrices. Nonsingular
matrices are trivially RPN because they have a zero nullspace. For com-
plex matrices, replace (⋆)T by (⋆)∗and “orthogonal” by “unitary.”
Proof.
The fact that (5.11.12) ⇐⇒(5.11.13) ⇐⇒(5.11.14) is a direct conse-
quence of (5.11.5). It suﬃces to prove (5.11.15) ⇐⇒(5.11.13). If (5.11.15) is a

5.11 Orthogonal Decomposition
409
URV factorization with V = U =

U1 | U2), then R (A) = R (U1) = R (V1) =
R

AT 
. Conversely, if R (A) = R

AT 
, perping both sides and using equation
(5.11.5) produces N (A) = N

AT 
, so (5.11.8) yields a URV factorization with
U = V.
Example 5.11.3
A ∈Cn×n is called a normal matrix whenever AA∗= A∗A. As illustrated
in Figure 5.11.2, normal matrices ﬁll the niche between hermitian and (complex)
RPN matrices in the sense that real-symmetric ⇒hermitian ⇒normal ⇒RPN,
with no implication being reversible—details are called for in Exercise 5.11.13.
RPN
Normal
Hermitian
Real-Symmetric
Nonsingular
Figure 5.11.2
Exercises for section 5.11
5.11.1. Verify the orthogonal decomposition theorem for A=

2
1
1
−1
−1
0
−2
−1
−1

.
5.11.2. For an inner-product space V, what is V⊥? What is 0⊥?
5.11.3. Find a basis for the orthogonal complement of M=span





1
2
0
3

,


2
4
1
6




.
5.11.4. For every inner-product space V, prove that if M ⊆V, then M⊥is
a subspace of V.
5.11.5. If M and N are subspaces of an n-dimensional inner-product space,
prove that the following statements are true.
(a)
M ⊆N
=⇒
N ⊥⊆M⊥.
(b)
(M + N)⊥= M⊥∩N ⊥.
(c)
(M ∩N)⊥= M⊥+ N ⊥.

410
Chapter 5
Norms, Inner Products, and Orthogonality
5.11.6. Explain why the rank plus nullity theorem on p. 199 is a corollary of the
orthogonal decomposition theorem.
5.11.7. Suppose A = URVT is a URV factorization of an m × n matrix of
rank r, and suppose U is partitioned as U =

U1 | U2

, where U1
is m × r. Prove that P = U1UT
1
is the projector onto R (A) along
N

AT 
. In this case, P is said to be an orthogonal projector because its
range is orthogonal to its nullspace. What is the orthogonal projector
onto N

AT 
along R (A)? (Orthogonal projectors are discussed in
more detail on p. 429.)
5.11.8. Use the Householder reduction method as described in Example 5.11.2
to compute a URV factorization as well as orthonormal bases for the
four fundamental subspaces of A =
 −4
−2
−4
−2
2
−2
2
1
−4
1
−4
−2

.
5.11.9. Compute a URV factorization for the matrix given in Exercise 5.11.8 by
using elementary row operations together with Gram–Schmidt orthogo-
nalization. Are the results the same as those of Exercise 5.11.8?
5.11.10. For the matrix A of Exercise
5.11.8, ﬁnd vectors x ∈R (A) and
y ∈N

AT 
such that v = x + y, where v = ( 3
3
3 )T . Is there
more than one choice for x and y?
5.11.11. Construct a square matrix such that R (A) ∩N (A) = 0, but R (A) is
not orthogonal to N (A).
5.11.12. For An×n singular, explain why R (A) ⊥N (A) implies index(A) = 1,
but not conversely.
5.11.13. Prove that real-symmetric matrix ⇒hermitian ⇒normal ⇒(com-
plex) RPN. Construct examples to show that none of the implications
is reversible.
5.11.14. Let A be a normal matrix.
(a)
Prove that R (A −λI) ⊥N (A −λI) for every scalar λ.
(b)
Let λ and µ be scalars such that A −λI and A −µI are
singular matrices—such scalars are called eigenvalues of A.
Prove that if λ ̸= µ, then N (A −λI) ⊥N (A −µI).

5.12 Singular Value Decomposition
411
5.12
SINGULAR VALUE DECOMPOSITION
For an m × n matrix A of rank r, Example 5.11.2 shows how to build a URV
factorization
A = URVT = U

Cr×r
0
0
0

m×n
VT
in which C is triangular. The purpose of this section is to prove that it’s possible
to do even better by showing that C can be made to be diagonal. To see how,
let σ1 = ∥A∥2 = ∥C∥2 (Exercise 5.6.9), and recall from the proof of (5.2.7) on
p. 281 that ∥C∥2 = ∥Cx∥2 for some vector x such that
(CT C −λI)x = 0,
where
∥x∥2 = 1 and λ = xT CT Cx = σ2
1.
(5.12.1)
Set y = Cx/∥Cx∥2 = Cx/σ1, and let Ry =

y | Y

and Rx =

x | X

be
elementary reﬂectors having y and x as their ﬁrst columns, respectively—recall
Example 5.6.3. Reﬂectors are orthogonal matrices, so xT X = 0 and YT y = 0,
and these together with (5.12.1) yield
yT CX = xT CT CX
σ1
= λxT X
σ1
= 0
and
YT Cx = σ1YT y = 0.
Coupling these facts with yT Cx = yT (σ1y) = σ1 and Ry = RT
y produces
RyCRx =
 yT
YT

C

x | X

=

yT Cx
yT CX
YT Cx
YT CX

=

σ1
0
0
C2

with σ1 ≥∥C2∥2 (because σ1 = ∥C∥2 = max{σ1, ∥C2∥} by (5.2.12)). Repeat-
ing the process on C2 yields reﬂectors Sy, Sx such that
SyC2Sx =

σ2
0
0
C3

,
where σ2 ≥∥C3∥2 .
If P2 and Q2 are the orthogonal matrices
P2 =

1
0
0
Sy

Ry,
Q2 = Rx

1
0
0
Sx

, then P2CQ2 =


σ1
0
0
0
σ2
0
0
0
C3


in which σ1 ≥σ2 ≥∥C3∥2 . Continuing for r −1 times produces orthogonal
matrices Pr−1 and Qr−1 such that Pr−1CQr−1 = diag (σ1, σ2, . . . , σr) = D,
where σ1 ≥σ2 ≥· · · ≥σr. If ˜UT and ˜V are the orthogonal matrices
˜UT =

Pr−1
0
0
I

UT and ˜V = V

Qr−1
0
0
I

, then ˜UT A ˜V =

D
0
0
0

,
and thus the singular value decomposition (SVD) is derived.
57
57
The SVD has been independently discovered and rediscovered several times. Those credited
with the early developments include Eugenio Beltrami (1835–1899) in 1873, M. E. Camille
Jordan (1838–1922) in 1875, James J. Sylvester (1814–1897) in 1889, L. Autonne in 1913, and
C. Eckart and G. Young in 1936.

412
Chapter 5
Norms, Inner Products, and Orthogonality
Singular Value Decomposition
For each A ∈ℜm×n of rank r, there are orthogonal matrices Um×m,
Vn×n and a diagonal matrix Dr×r = diag (σ1, σ2, . . . , σr) such that
A = U

D
0
0
0

m×n
VT
with
σ1 ≥σ2 ≥· · · ≥σr > 0.
(5.12.2)
The
σi ’s are called the nonzero singular values of
A.
When
r < p = min{m, n}, A is said to have p −r additional zero singular
values. The factorization in (5.12.2) is called a singular value decom-
position of A, and the columns in U and V are called left-hand and
right-hand singular vectors for A, respectively.
While the constructive method used to derive the SVD can be used as an
algorithm, more sophisticated techniques exist, and all good matrix computation
packages contain numerically stable SVD implementations. However, the details
of a practical SVD algorithm are too complicated to be discussed at this point.
The SVD is valid for complex matrices when (⋆)T is replaced by (⋆)∗, and
it can be shown that the singular values are unique, but the singular vectors
are not. In the language of Chapter 7, the σ2
i ’s are the eigenvalues of AT A,
and the singular vectors are specialized sets of eigenvectors for AT A—see the
summary on p. 555. In fact, the practical algorithm for computing the SVD is
an implementation of the QR iteration (p. 535) that is cleverly applied to AT A
without ever explicitly computing AT A.
Singular values reveal something about the geometry of linear transforma-
tions because the singular values σ1 ≥σ2 ≥· · · ≥σn of a matrix A tell us how
much distortion can occur under transformation by A. They do so by giving us
an explicit picture of how A distorts the unit sphere. To develop this, suppose
that A ∈ℜn×n is nonsingular (Exercise 5.12.5 treats the singular and rectangu-
lar case), and let S2 = {x | ∥x∥2 = 1} be the unit 2-sphere in ℜn. The nature
of the image A(S2) is revealed by considering the singular value decompositions
A = UDVT
and
A−1 = VD−1UT
with
D = diag (σ1, σ2, . . . , σn) ,
where U and V are orthogonal matrices. For each y ∈A(S2) there is an
x ∈S2 such that y = Ax, so, with w = UT y,
1 = ∥x∥2
2 =
A−1Ax
2
2 =
A−1y
2
2 =
VD−1UT y
2
2 =
D−1UT y
2
2
=
D−1w
2
2 = w2
1
σ2
1
+ w2
2
σ2
2
+ · · · + w2
r
σ2r
.
(5.12.3)

5.12 Singular Value Decomposition
413
This means that UT A(S2) is an ellipsoid whose kth semiaxis has length
σk. Because orthogonal transformations are isometries (length preserving trans-
formations), UT can only aﬀect the orientation of A(S2) , so A(S2) is also an
ellipsoid whose kth semiaxis has length σk. Furthermore, (5.12.3) implies that
the ellipsoid UT A(S2) is in standard position—i.e., its axes are directed along
the standard basis vectors ek. Since U maps UT A(S2) to A(S2), and since
Uek = U∗k, it follows that the axes of A(S2) are directed along the left-hand
singular vectors deﬁned by the columns of U. Therefore, the kth semiaxis of
A(S2) is σkU∗k. Finally, since AV = UD implies AV∗k = σkU∗k, the right-
hand singular vector V∗k is a point on S2 that is mapped to the kth semiaxis
vector on the ellipsoid A(S2). The picture in ℜ3 looks like Figure 5.12.1.
1
A
σ1U∗1
σ2U∗2
σ3U∗3
V∗1
V∗2
V∗3
Figure 5.12.1
The degree of distortion of the unit sphere under transformation by A
is therefore measured by κ2 = σ1/σn, the ratio of the largest singular value
to the smallest singular value. Moreover, from the discussion of induced ma-
trix norms (p. 280) and the unitary invariance of the 2-norm (Exercise 5.6.9),
max
∥x∥2=1 ∥Ax∥2 = ∥A∥2 =
UDVT 
2 = ∥D∥2 = σ1
and
min
∥x∥2=1 ∥Ax∥2 =
1
∥A−1∥2
=
1
∥VD−1UT ∥2
=
1
∥D−1∥2
= σn.
In other words, longest and shortest vectors on A(S2) have respective lengths
σ1 = ∥A∥2 and σn = 1/
A−1
2 (this justiﬁes Figure 5.2.1 on p. 281), so
κ2 = ∥A∥2
A−1
2 . This is called the 2-norm condition number of A. Diﬀer-
ent norms result in condition numbers with diﬀerent values but with more or
less the same order of magnitude as κ2 (see Exercise 5.12.3), so the qualitative
information about distortion is the same. Below is a summary.

414
Chapter 5
Norms, Inner Products, and Orthogonality
Image of the Unit Sphere
For a nonsingular An×n having singular values σ1 ≥σ2 ≥· · · ≥σn
and an SVD A = UDVT with D = diag (σ1, σ2, . . . , σn) , the image
of the unit 2-sphere is an ellipsoid whose kth semiaxis is given by σkU∗k
(see Figure 5.12.1). Furthermore, V∗k is a point on the unit sphere such
that AV∗k = σkU∗k. In particular,
•
σ1 = ∥AV∗1∥2 = max
∥x∥2=1 ∥Ax∥2 = ∥A∥2,
(5.12.4)
•
σn = ∥AV∗n∥2 = min
∥x∥2=1 ∥Ax∥2 = 1/∥A−1∥2.
(5.12.5)
The degree of distortion of the unit sphere under transformation by A
is measured by the 2-norm condition number
•
κ2 = σ1
σn
= ∥A∥2
A−1
2 ≥1.
(5.12.6)
Notice that κ2 = 1 if and only if A is an orthogonal matrix.
The amount of distortion of the unit sphere under transformation by A
determines the degree to which uncertainties in a linear system Ax = b can be
magniﬁed. This is explained in the following example.
Example 5.12.1
Uncertainties in Linear Systems. Systems of linear equations Ax = b aris-
ing in practical work almost always come with built-in uncertainties due to mod-
eling errors (because assumptions are almost always necessary), data collection
errors (because inﬁnitely precise gauges don’t exist), and data entry errors (be-
cause numbers like
√
2,
π, and 2/3 can’t be entered exactly). In addition,
roundoﬀerror in ﬂoating-point computation is a prevalent source of uncertainty.
In all cases it’s important to estimate the degree of uncertainty in the solution
of Ax = b. This is not diﬃcult when A is known exactly and all uncertainty
resides in the right-hand side. Even if this is not the case, it’s sometimes possible
to aggregate uncertainties and shift all of them to the right-hand side.
Problem: Let Ax = b be a nonsingular system in which A is known exactly
but b is subject to an uncertainty e, and consider A˜x = b −e = ˜b. Estimate
the relative uncertainty
58 ∥x −˜x∥/ ∥x∥in x in terms of the relative uncertainty
∥b −˜b∥/ ∥b∥= ∥e∥/ ∥b∥in b. Use any vector norm and its induced matrix
norm (p. 280).
58
Knowing the absolute uncertainty ∥x −˜x∥by itself may not be meaningful. For example, an
absolute uncertainty of a half of an inch might be ﬁne when measuring the distance between
the earth and the moon, but it’s not good in the practice of eye surgery.

5.12 Singular Value Decomposition
415
Solution: Use ∥b∥= ∥Ax∥≤∥A∥∥x∥with x −˜x = A−1e to write
∥x −˜x∥
∥x∥
=
A−1e

∥x∥
≤∥A∥
A−1 ∥e∥
∥b∥
= κ ∥e∥
∥b∥,
(5.12.7)
where κ = ∥A∥
A−1 is a condition number as discussed earlier (κ = σ1/σn
if the 2-norm is used). Furthermore, ∥e∥= ∥A(x −˜x)∥≤∥A∥∥(x −˜x)∥and
∥x∥≤
A−1 ∥b∥imply
∥x −˜x∥
∥x∥
≥
∥e∥
∥A∥∥x∥≥
∥e∥
∥A∥∥A−1∥∥b∥= 1
κ
∥e∥
∥b∥.
This with (5.12.7) yields the following bounds on the relative uncertainty:
κ−1 ∥e∥
∥b∥≤∥x −˜x∥
∥x∥
≤κ ∥e∥
∥b∥,
where
κ = ∥A∥
A−1 .
(5.12.8)
In other words, when A is well conditioned (i.e., when κ is small—see the rule
of thumb in Example 3.8.2 to get a feeling of what “small” and “large” might
mean), (5.12.8) insures that small relative uncertainties in b cannot greatly
aﬀect the solution, but when A is ill conditioned (i.e., when κ is large), a
relatively small uncertainty in b might result in a relatively large uncertainty
in x. To be more sure, the following problem needs to be addressed.
Problem: Can equality be realized in each bound in (5.12.8) for every nonsin-
gular A, and if so, how?
Solution: Use the 2-norm, and let A = UDVT be an SVD so AV∗k = σkU∗k
for each k. If b and e are directed along left-hand singular vectors associated
with σ1 and σn, respectively—say, b = βU∗1 and e = ϵU∗n, then
x = A−1b = A−1(βU∗1) = βV∗1
σ1
and
x−˜x = A−1e = A−1(ϵU∗n) = ϵV∗n
σn
,
so
∥x −˜x∥2
∥x∥2
=
 σ1
σn
 |ϵ|
|β| = κ2
∥e∥2
∥b∥2
when b = βU∗1 and e = ϵU∗n.
Thus the upper bound (the worst case) in (5.12.8) is attainable for all A. The
lower bound (the best case) is realized in the opposite situation when b and e
are directed along U∗n and U∗1, respectively. If b = βU∗n and e = ϵU∗1,
then the same argument yields x = σ−1
n βV∗n and x −˜x = σ−1
1 ϵV∗1, so
∥x −˜x∥2
∥x∥2
=
σn
σ1
 |ϵ|
|β| = κ−1
2
∥e∥2
∥b∥2
when b = βU∗n and e = ϵU∗1.

416
Chapter 5
Norms, Inner Products, and Orthogonality
Therefore, if A is well conditioned, then relatively small uncertainties in b can’t
produce relatively large uncertainties in x. But when A is ill conditioned, it’s
possible for relatively small uncertainties in b to have relatively large eﬀects on
x, and it’s also possible for large uncertainties in b to have almost no eﬀect on
x. Since the direction of e is almost always unknown, we must guard against the
worst case and proceed with caution when dealing with ill-conditioned matrices.
Problem: What if there are uncertainties in both sides of Ax = b?
Solution: Use calculus to analyze the situation by considering the entries of
A = A(t) and b = b(t) to be diﬀerentiable functions of a variable t, and
compute the relative size of the derivative of x = x(t) by diﬀerentiating b = Ax
to obtain b′ = (Ax)′ = A′x + Ax′ (with ⋆′ denoting d ⋆/dt ), so
∥x′∥=
A−1b′ −A−1A′x
 ≤
A−1b′ +
A−1A′x

≤
A−1 ∥b′∥+
A−1 ∥A′∥∥x∥.
Consequently,
∥x′∥
∥x∥≤
A−1 ∥b′∥
∥x∥
+
A−1 ∥A′∥
≤∥A∥
A−1
∥b′∥
∥A∥∥x∥+ ∥A∥
A−1 ∥A′∥
∥A∥
≤κ∥b′∥
∥b∥+ κ∥A′∥
∥A∥= κ
∥b′∥
∥b∥+ ∥A′∥
∥A∥

.
In other words, the relative sensitivity of the solution is the sum of the relative
sensitivities of A and b magniﬁed by κ = ∥A∥
A−1 . A discrete analog of
the above inequality is developed in Exercise 5.12.12.
Conclusion: In all cases, the credibility of the solution to Ax = b in the face
of uncertainties must be gauged in relation to the condition of A.
As the next example shows, the condition number is pivotal also in deter-
mining whether or not the residual r = b −A˜x is a reliable indicator of the
accuracy of an approximate solution ˜x.
Example 5.12.2
Checking an Answer. Suppose that ˜x is a computed (or otherwise approxi-
mate) solution for a nonsingular system Ax = b, and suppose the accuracy of
˜x is “checked” by computing the residual r = b −A˜x. If r = 0, exactly,
then ˜x must be the exact solution. But if r is not exactly zero—say, ∥r∥2 is
zero to t signiﬁcant digits—are we guaranteed that ˜x is accurate to roughly t
signiﬁcant ﬁgures? This question was brieﬂy examined in Example 1.6.3, but it’s
worth another look.
Problem: To what extent does the size of the residual reﬂect the accuracy of
an approximate solution?

5.12 Singular Value Decomposition
417
Solution: Without realizing it, we answered this question in Example 5.12.1.
To bound the accuracy of ˜x relative to the exact solution x, write r = b −A˜x
as A˜x = b −r, and apply (5.12.8) with e = r to obtain
κ−1 ∥r∥2
∥b∥2
≤∥x −˜x∥
∥x∥
≤κ ∥r∥2
∥b∥2
,
where
κ = ∥A∥2
A−1
2 .
(5.12.9)
Therefore, for a well-conditioned A, the residual r is relatively small if and
only if ˜x is relatively accurate. However, as demonstrated in Example 5.12.1,
equality on either side of (5.12.9) is possible, so, when A is ill conditioned, a
very inaccurate approximation ˜x can produce a small residual r, and a very
accurate approximation can produce a large residual.
Conclusion: Residuals are reliable indicators of accuracy only when A is well
conditioned—if A is ill conditioned, residuals are nearly meaningless.
In addition to measuring the distortion of the unit sphere and gauging the
sensitivity of linear systems, singular values provide a measure of how close A
is to a matrix of lower rank.
Distance to Lower-Rank Matrices
If σ1 ≥σ2 ≥· · · ≥σr are the nonzero singular values of Am×n, then
for each k < r, the distance from A to the closest matrix of rank k is
σk+1 =
min
rank(B)=k ∥A −B∥2.
(5.12.10)
Proof.
Suppose rank (Bm×n) = k, and let A = U
 D
0
0
0

VT be an SVD
for A with D = diag (σ1, σ2, . . . , σr) . Deﬁne S = diag (σ1, . . . , σk+1), and
partition V =

Fn×k+1 | G

. Since rank (BF) ≤rank (B) = k (by (4.5.2)),
dim N (BF) = k+1−rank (BF) ≥1, so there is an x ∈N (BF) with ∥x∥2 = 1.
Consequently, BFx = 0 and
AFx = U

D
0
0
0

VT Fx = U


S
0
0
0
⋆
0
0
0
0




x
0
0

= U


Sx
0
0

.
Since ∥A −B∥2 = max∥y∥2=1 ∥(A −B)y∥2 , and since ∥Fx∥2 = ∥x∥2 = 1
(recall (5.2.4), p. 280, and (5.2.13), p. 283),
∥A −B∥2
2 ≥∥(A −B)Fx∥2
2 = ∥Sx∥2
2 =
k+1

i=1
σ2
i x2
i ≥σ2
k+1
k+1

i=1
x2
i = σ2
k+1.
Equality holds for Bk = U
 Dk
0
0
0

VT with Dk = diag (σ1, . . . , σk), and thus
(5.12.10) is proven.

418
Chapter 5
Norms, Inner Products, and Orthogonality
Example 5.12.3
Filtering Noisy Data. The SVD can be a useful tool in applications involving
the need to sort through noisy data and lift out relevant information. Suppose
that Am×n is a matrix containing data that are contaminated with a certain
level of noise—e.g., the entries A might be digital samples of a noisy video or
audio signal such as that in Example 5.8.3 (p. 359). The SVD resolves the data
in A into r mutually orthogonal components by writing
A = U

Dr×r
0
0
0

VT =
r

i=1
σiuivT
i =
r

i=1
σiZi,
(5.12.11)
where Zi = uivT
i
and σ1 ≥σ2 ≥· · · ≥σr > 0. The matrices {Z1, Z2, . . . , Zr}
constitute an orthonormal set because
⟨Zi Zj⟩= trace

ZT
i Zj

=

0
if i ̸= j,
1
if i = j.
In other words, the SVD (5.12.11) can be regarded as a Fourier expansion as
described on p. 299 and, consequently, σi = ⟨Zi A⟩can be interpreted as the
proportion of A lying in the “direction” of Zi. In many applications the noise
contamination in A is random (or nondirectional) in the sense that the noise
is distributed more or less uniformly across the Zi’s. That is, there is about as
much noise in the “direction” of one Zi as there is in the “direction” of any
other. Consequently, we expect each term σiZi to contain approximately the
same level of noise. This means that if SNR(σiZi) denotes the signal-to-noise
ratio in σiZi, then
SNR(σ1Z1) ≥SNR(σ2Z2) ≥· · · ≥SNR(σrZr),
more or less. If some of the singular values, say, σk+1, . . . , σr, are small relative to
(total noise)/r, then the terms σk+1Zk+1, . . . , σrZr have small signal-to-noise
ratios. Therefore, if we delete these terms from (5.12.11), then we lose a small part
of the total signal, but we remove a disproportionately large component of the
total noise in A. This explains why a truncated SVD Ak = k
i=1 σiZi can, in
many instances, ﬁlter out some of the noise without losing signiﬁcant information
about the signal in A. Determining the best value of k often requires empirical
techniques that vary from application to application, but looking for obvious
gaps between large and small singular values is usually a good place to start.
The next example presents an interesting application of this idea to building an
Internet search engine.

5.12 Singular Value Decomposition
419
Example 5.12.4
Search Engines. The ﬁltering idea presented in Example 5.12.3 is widely used,
but a particularly novel application is the method of latent semantic indexing
used in the areas of information retrieval and text mining. You can think of
this in terms of building an Internet search engine. Start with a dictionary of
terms T1, T2, . . . , Tm. Terms are usually single words, but sometimes a term
may contain more that one word such as “landing gear.” It’s up to you to decide
how extensive your dictionary should be, but even if you use the entire English
language, you probably won’t be using more than a few hundred-thousand terms,
and this is within the capacity of existing computer technology. Each document
(or web page) Dj of interest is scanned for key terms (this is called indexing the
document), and an associated document vector dj = (freq1j, freq2j, . . . , freqmj)T
is created in which
freqij = number of times term Ti occurs in document Dj.
(More sophisticated search engines use weighted frequency strategies.) After a
collection of documents D1, D2, . . . , Dn has been indexed, the associated docu-
ment vectors dj are placed as columns in a term-by-document matrix
Am×n =

d1 | d2 · · · | dn

=





D1
D2
· · ·
Dn
T1
freq11
freq12
· · ·
freq1n
T2
freq21
freq22
· · ·
freq2n
...
...
...
...
Tm
freqm1
freqm2
· · ·
freqmn




.
Naturally, most entries in each document vector dj will be zero, so A is a
sparse matrix—this is good because it means that sparse matrix technology can
be applied. When a query composed of a few terms is submitted to the search
engine, a query vector qT = (q1, q2, . . . , qn) is formed in which
qi =
5 1
if term Ti appears in the query,
0
otherwise.
(The qi ’s might also be weighted.) To measure how well a query q matches a
document Dj, we check how close q is to dj by computing the magnitude of
cos θj =
qT dj
∥q∥2 ∥dj∥2
=
qT Aej
∥q∥2 ∥Aej∥2
.
(5.12.12)
If | cos θj| ≥τ for some threshold tolerance τ, then document Dj is con-
sidered relevant and is returned to the user. Selecting τ is part art and part
science that’s based on experimentation and desired performance criteria. If the
columns of A along with q are initially normalized to have unit length, then

420
Chapter 5
Norms, Inner Products, and Orthogonality
|qT A| =

| cos θ1|, | cos θ2|, . . . , | cos θn|

provides the information that allows
the search engine to rank the relevance of each document relative to the query.
However, due to things like variation and ambiguity in the use of vocabulary,
presentation style, and even the indexing process, there is a lot of “noise” in
A, so the results in |qT A| are nowhere near being an exact measure of how
well query q matches the various documents. To ﬁlter out some of this noise,
the techniques of Example 5.12.3 are employed. An SVD A = r
i=1 σiuivT
i
is
judiciously truncated, and
Ak = UkDkVT
k =

u1 | · · · | uk



σ1
...
σk





vT
1
...
vT
k


=
k

i=1
σiuivT
i
is used in place of A in (5.12.12). In other words, instead of using cos θj, query
q is compared with document Dj by using the magnitude of
cos φj =
qT Akej
∥q∥2 ∥Akej∥2
.
To make this more suitable for computation, set Sk = DkVT
k =

s1 | s2 | · · · | sk

,
and use
∥Akej∥2 =
UkDkVT
k ej

2 = ∥Uksj∥2 = ∥sj∥2
to write
cos φj =
qT Uksj
∥q∥2 ∥sj∥2
.
(5.12.13)
The vectors in Uk and Sk only need to be computed once (and they can be
determined without computing the entire SVD), so (5.12.13) requires very little
computation to process each new query. Furthermore, we can be generous in the
number of SVD components that are dropped because variation in the use of
vocabulary and the ambiguity of many words produces signiﬁcant noise in A.
Coupling this with the fact that numerical accuracy is not an important issue
(knowing a cosine to two or three signiﬁcant digits is suﬃcient) means that we
are more than happy to replace the SVD of A by a low-rank truncation Ak,
where k is signiﬁcantly less than r.
Alternate Query Matching Strategy.
An alternate way to measuring how
close a given query q is to a document vector dj is to replace the query vector
q in (5.12.12) by the projected query 9q = PR(A)q, where PR(A) = UrUT
r is the
orthogonal projector onto R (A) along R (A)⊥(Exercise 5.12.15) to produce
cos 9θj =
9qT Aej
∥9q∥2 ∥Aej∥2
.
(5.12.14)

5.12 Singular Value Decomposition
421
It’s proven on p. 435 that 9q = PR(A)q is the vector in R (A) (the document
space) that is closest to q, so using 9q in place of q has the eﬀect of using the
best approximation to q that is a linear combination of the document vectors
di. Since 9qT A = qT A and ∥9q∥2 ≤∥q∥2 , it follows that cos 9θj ≥cos θj, so
more documents are deemed relevant when the projected query is used. Just as
in the unprojected query matching strategy, the noise is ﬁltered out by replacing
A in (5.12.14) with a truncated SVD Ak = k
i=1 σiuivT
i . The result is
cos 9φj =
qT Uksj
UT
k q

2 ∥sj∥2
and, just as in (5.12.13), cos 9φj is easily and quickly computed for each new
query q because Uk and sj need only be computed once.
The next example shows why singular values are the primary mechanism
for numerically determining the rank of a matrix.
Example 5.12.5
Perturbations and Numerical Rank. For A ∈ℜm×n with p = min{m, n},
let {σ1, σ2, . . . , σp} and {β1, β2, . . . , βp} be all singular values (nonzero as well
as any zero ones) for A and A + E, respectively.
Problem: Prove that
|σk −βk| ≤∥E∥2
for each
k = 1, 2, . . . , p.
(5.12.15)
Solution: If the SVD for A given in (5.12.2) is written in the form
A =
p

i=1
σiuivT
i ,
and if we set
Ak−1 =
k−1

i=1
σiuivT
i ,
then
σk = ∥A −Ak−1∥2 = ∥A + E −Ak−1 −E∥2
≥∥A + E −Ak−1∥2 −∥E∥2
(recall (5.1.6) on p. 273)
≥βk −∥E∥2
by (5.12.10).
Couple this with the observation that
σk = min
rank(B)=k−1 ∥A −B∥2 = min
rank(B)=k−1 ∥A + E −B −E∥2
≤min
rank(B)=k−1 ∥A + E −B∥2 + ∥E∥2 = βk + ∥E∥2
to conclude that |σk −βk| ≤∥E∥2.

422
Chapter 5
Norms, Inner Products, and Orthogonality
Problem: Explain why this means that computing the singular values of A
with any stable algorithm (one that returns the exact singular values βk of a
nearby matrix A + E) is a good way to compute rank (A).
Solution: If rank (A) = r, then p −r of the σk ’s are exactly zero, so the
perturbation result (5.12.15) guarantees that p−r of the computed βk ’s cannot
be larger than ∥E∥2. So if
β1 ≥· · · ≥β˜r > ∥E∥2 ≥β˜r+1 ≥· · · ≥βp,
then it’s reasonable to consider ˜r to be the numerical rank of A. For most
algorithms, ∥E∥2 is not known exactly, but adequate estimates of ∥E∥2 often
can be derived. Considerable eﬀort has gone into the development of stable al-
gorithms for computing singular values, but such algorithms are too involved
to discuss here—consult an advanced book on matrix computations. Gener-
ally speaking, good SVD algorithms have ∥E∥2 ≈5 × 10−t∥A∥2 when t-digit
ﬂoating-point arithmetic is used.
Just as the range-nullspace decomposition was used in Example 5.10.5 to
deﬁne the Drazin inverse of a square matrix, a URV factorization or an SVD
can be used to deﬁne a generalized inverse for rectangular matrices. For a URV
factorization
Am×n = U

C
0
0
0

m×n
VT ,
we deﬁne
A†
n×m = V

C−1
0
0
0

n×m
UT
to be the Moore–Penrose inverse (or the pseudoinverse) of A. (Replace
(⋆)T by (⋆)∗when A ∈Cm×n. ) Although the URV factors are not uniquely
deﬁned by A, it can be proven that A† is unique by arguing that A† is the
unique solution to the four Penrose equations
AA†A = A,
A†AA† = A†,

AA†T = AA†,

A†A
T = A†A,
so A† is the same matrix deﬁned in Exercise 4.5.20. Since it doesn’t matter
which URV factorization is used, we can use the SVD (5.12.2), in which case
C = D = diag (σ1, . . . , σr). Some “inverselike” properties that relate A† to
solutions and least squares solutions for linear systems are given in the following
summary. Other useful properties appear in the exercises.

5.12 Singular Value Decomposition
423
Moore–Penrose Pseudoinverse
•
In terms of URV factors, the Moore–Penrose pseudoinverse of
Am×n= U

Cr×r
0
0
0

VT
is A†
n×m= V

C−1
0
0
0

UT . (5.12.16)
•
When Ax = b is consistent, x = A†b is the solution
of minimal euclidean norm.
(5.12.17)
•
When Ax = b is inconsistent, x = A†b is the least
squares solution of minimal euclidean norm.
(5.12.18)
•
When an SVD is used, C = D = diag (σ1, . . . , σr), so
A† = V

D−1
0
0
0

UT =
r

i=1
viuT
i
σi
and
A†b =
r

i=1

uT
i b

σi
vi.
Proof.
To prove (5.12.17), suppose Ax0 = b, and replace A by AA†A to
write b = Ax0 = AA†Ax0 = AA†b. Thus A†b solves Ax = b when it is
consistent. To see that A†b is the solution of minimal norm, observe that the
general solution is A†b+N (A) (a particular solution plus the general solution of
the homogeneous equation), so every solution has the form z = A†b+n, where
n ∈N (A). It’s not diﬃcult to see that A†b ∈R

A†
= R

AT 
(Exercise
5.12.16), so A†b ⊥n. Therefore, by the Pythagorean theorem (Exercise 5.4.14),
∥z∥2
2 =
A†b + n
2
2 =
A†b
2
2 + ∥n∥2
2 ≥
A†b
2
2 .
Equality is possible if and only if n = 0, so A†b is the unique minimum
norm solution. When Ax = b is inconsistent, the least squares solutions are the
solutions of the normal equations AT Ax = AT b, and it’s straightforward to
verify that A†b is one such solution (Exercise 5.12.16(c)). To prove that A†b
is the least squares solution of minimal norm, apply the same argument used in
the consistent case to the normal equations.
Caution! Generalized inverses are useful in formulating theoretical statements
such as those above, but, just as in the case of the ordinary inverse, generalized
inverses are not practical computational tools. In addition to being computation-
ally ineﬃcient, serious numerical problems result from the fact that A† need

424
Chapter 5
Norms, Inner Products, and Orthogonality
not be a continuous function of the entries of A. For example,
A(x) =

1
0
0
x

=⇒
A†(x) =








1
0
0
1/x

for x ̸= 0,

1
0
0
0

for x = 0.
Not only is A†(x) discontinuous in the sense that limx→0 A†(x) ̸= A†(0), but
it is discontinuous in the worst way because as A(x) comes closer to A(0) the
matrix A†(x) moves farther away from A†(0). This type of behavior translates
into insurmountable computational diﬃculties because small errors due to round-
oﬀ(or anything else) can produce enormous errors in the computed A†, and as
errors in A become smaller the resulting errors in A† can become greater. This
diabolical fact is also true for the Drazin inverse (p. 399). The inherent numeri-
cal problems coupled with the fact that it’s extremely rare for an application to
require explicit knowledge of the entries of A† or AD constrains them to being
theoretical or notational tools. But don’t underestimate this role—go back and
read Laplace’s statement quoted in the footnote on p. 81.
Example 5.12.6
Another way to view the URV or SVD factorizations in relation to the Moore–
Penrose inverse is to consider A/R(AT ) and A†
/R(A), the restrictions of A and
A† to R

AT 
and R (A), respectively. Begin by making the straightforward
observations that R

A†
= R

AT 
and N

A†
= N

AT 
(Exercise 5.12.16).
Since ℜn = R

AT 
⊕N (A) and ℜm = R (A) ⊕N

AT 
, it follows that
R (A) = A(ℜn) = A(R

AT 
) and R

AT 
= R

A†
= A†(ℜm) = A†(R (A)).
In other words, A/R(AT ) and A†
/R(A) are linear transformations such that
A/R(AT ) : R

AT 
→R (A)
and
A†
/R(A) : R (A) →R

AT 
.
If B = {u1, u2, . . . , ur} and B′ = {v1, v2, . . . , vr} are the ﬁrst r columns
from U =

U1 | U2

and V =

V1 | V2

in (5.11.11), then AV1 = U1C and
A†U1 = V1C−1 implies (recall (4.7.4)) that
1
A/R(AT )
2
B′B = C
and
1
A†
/R(A)
2
BB′ = C−1.
(5.12.19)
If left-hand and right-hand singular vectors from the SVD (5.12.2) are used in
B and B′, respectively, then C = D = diag (σ1, . . . , σr). Thus (5.12.19) reveals
the exact sense in which A and A† are “inverses.” Compare these results with
the analogous statements for the Drazin inverse in Example 5.10.5 on p. 399.

5.12 Singular Value Decomposition
425
Exercises for section 5.12
5.12.1. Following the derivation in the text, ﬁnd an SVD for
C =

−4
−6
3
−8

.
5.12.2. If σ1 ≥σ2 ≥· · · ≥σr are the nonzero singular values of A, then it can
be shown that the function νk(A) =

σ2
1 + σ2
2 + · · · + σ2
k
1/2 deﬁnes a
unitarily invariant norm (recall Exercise 5.6.9) for ℜm×n (or Cm×n)
for each k = 1, 2, . . . , r. Explain why the 2-norm and the Frobenius
norm (p. 279) are the extreme cases in the sense that ∥A∥2
2 = σ2
1 and
∥A∥2
F = σ2
1 + σ2
2 + · · · + σ2
r.
5.12.3. Each of the four common matrix norms can be bounded above and below
by a constant multiple of each of the other matrix norms. To be precise,
∥A∥i ≤α ∥A∥j , where α is the (i, j)-entry in the following matrix.




1
2
∞
F
1
∗
√n
n
√n
2
√n
∗
√n
1
∞
n
√n
∗
√n
F
√n
√n
√n
∗



.
For analyzing limiting behavior, it therefore makes no diﬀerence which
of these norms is used, so they are said to be equivalent matrix norms. (A
similar statement for vector norms was given in Exercise 5.1.8.) Explain
why the (2, F) and the (F, 2) entries are correct.
5.12.4. Prove that if σ1 ≥σ2 ≥· · · ≥σr are the nonzero singular values of a
rank r matrix A, and if ∥E∥2 < σr, then rank (A + E) ≥rank (A).
Note: This clariﬁes the meaning of the term “suﬃciently small” in the
assertion on p. 216 that small perturbations can’t reduce rank.
5.12.5. Image of the Unit Sphere. Extend the result on p. 414 concerning
the image of the unit sphere to include singular and rectangular matrices
by showing that if σ1 ≥σ2 ≥· · · ≥σr > 0 are the nonzero singular
values of Am×n, then the image A(S2) ⊂ℜm of the unit 2-sphere
S2 ⊂ℜn is an ellipsoid (possibly degenerate) in which the kth semiaxis
is σkU∗k = AV∗k, where U∗k and V∗k are respective left-hand and
right-hand singular vectors for A.

426
Chapter 5
Norms, Inner Products, and Orthogonality
5.12.6. Prove that if σr is the smallest nonzero singular value of Am×n, then
σr =
min
∥x∥2=1
x∈R(AT )
∥Ax∥2 = 1/
A†
2,
which is the generalization of (5.12.5).
5.12.7. Generalized Condition Number. Extend the bound in (5.12.8) to
include singular and rectangular matrices by showing that if x and
˜x are the respective minimum 2-norm solutions of consistent systems
Ax = b and A˜x = ˜b = b −e, then
κ−1 ∥e∥
∥b∥≤∥x −˜x∥
∥x∥
≤κ ∥e∥
∥b∥,
where
κ = ∥A∥
A† .
Can the same reasoning given in Example 5.12.1 be used to argue that
for ∥⋆∥2, the upper and lower bounds are attainable for every A?
5.12.8. Prove that if |ϵ| < σ2
r for the smallest nonzero singular value of Am×n,
then (AT A + ϵI)−1 exists, and limϵ→0(AT A + ϵI)−1AT = A†.
5.12.9. Consider a system Ax = b in which
A =

.835
.667
.333
.266

,
and suppose b is subject to an uncertainty e. Using ∞-norms, deter-
mine the directions of b and e that give rise to the worst-case scenario
in (5.12.8) in the sense that ∥x −˜x∥∞/ ∥x∥∞= κ∞∥e∥∞/ ∥b∥∞.
5.12.10. An ill-conditioned matrix is suspected when a small pivot uii emerges
during the LU factorization of A because

U−1
ii = 1/uii is then
large, and this opens the possibility of A−1 = U−1L−1 having large
entries. Unfortunately, this is not an absolute test, and no guarantees
about conditioning can be made from the pivots alone.
(a)
Construct an example of a matrix that is well conditioned but
has a small pivot.
(b)
Construct an example of a matrix that is ill conditioned but has
no small pivots.

5.12 Singular Value Decomposition
427
5.12.11. Bound the relative uncertainty in the solution of a nonsingular system
Ax = b for which there is some uncertainty in A but not in b by
showing that if (A−E)˜x = b, where α =
A−1E
 < 1 for any matrix
norm such that ∥I∥= 1, then
∥x −˜x∥
∥x∥
≤
κ
1 −α
∥E∥
∥A∥,
where
κ = ∥A∥
A−1 .
Note: If the 2-norm is used, then ∥E∥2 < σn insures α < 1.
Hint: If B = A−1E, then A −E = A(I −B), and α = ∥B∥< 1
=⇒
Bk ≤∥B∥k →0
=⇒
Bk →0, so the Neumann series
expansion (p. 126) yields (I −B)−1 = ∞
i=0 Bi.
5.12.12. Now bound the relative uncertainty in the solution of a nonsingular
system Ax = b for which there is some uncertainty in both A and b
by showing that if (A −E)˜x = b −e, where α =
A−1E
 < 1 for any
matrix norm such that ∥I∥= 1, then
∥x −˜x∥
∥x∥
≤
κ
1 −κ ∥E∥/ ∥A∥
 ∥e∥
∥b∥+ ∥E∥
∥A∥

,
where κ = ∥A∥
A−1 .
Note: If the 2-norm is used, then ∥E∥2 < σn insures α < 1. This
exercise underscores the conclusion of Example 5.12.1 stating that if A
is well conditioned, and if the relative uncertainties in A and b are
small, then the relative uncertainty in x must be small.
5.12.13. Consider the matrix A =
 −4
−2
−4
−2
2
−2
2
1
−4
1
−4
−2

.
(a)
Use the URV factorization you computed in Exercise 5.11.8 to
determine A†.
(b)
Now use the URV factorization you obtained in Exercise 5.11.9
to determine A†. Do your results agree with those of part (a)?
5.12.14. For matrix A in Exercise 5.11.8, and for b = ( −12
3
−9 )T , ﬁnd
the solution of Ax = b that has minimum euclidean norm.
5.12.15. Suppose A = URVT is a URV factorization (so it could be an SVD)
of an m × n matrix of rank r, and suppose U is partitioned as U =

U1 | U2

, where U1 is m × r. Prove that P = U1UT
1 = AA† is the
projector onto R (A) along N

AT 
. In this case, P is said to be an or-
thogonal projector because its range is orthogonal to its nullspace. What
is the orthogonal projector onto N

AT 
along R (A)? (Orthogonal
projectors are discussed in more detail on p. 429.)

428
Chapter 5
Norms, Inner Products, and Orthogonality
5.12.16. Establish the following properties of A†.
(a)
A† = A−1 when A is nonsingular.
(b)
(A†)
† = A.
(c)
(A†)
T = (AT )
† .
(d)
A† =

(AT A)−1AT
when rank (Am×n) = n,
AT (AAT )−1
when rank (Am×n) = m.
(e)
AT = AT AA† = A†AAT for all A ∈ℜm×n.
(f)
A† = AT (AAT )† = (AT A)†AT for all A ∈ℜm×n.
(g)
R

A†
= R

AT 
= R

A†A

, and
N

A†
= N

AT 
= N

AA†
.
(h)
(PAQ)† = QT A†PT when P and Q are orthogonal matrices,
but in general (AB)† ̸= B†A† (the reverse-order law fails).
(i)
(AT A)† = A†(AT )† and (AAT )† = (AT )†A†.
5.12.17. Explain why A† = AD if and only if A is an RPN matrix.
5.12.18. Let X, Y ∈ℜm×n be such that R (X) ⊥R (Y).
(a)
Establish the Pythagorean theorem for matrices by proving
∥X + Y∥2
F = ∥X∥2
F + ∥Y∥2
F .
(b)
Give an example to show that the result of part (a) does not
hold for the matrix 2-norm.
(c)
Demonstrate that A† is the best approximate inverse for A
in the sense that A† is the matrix of smallest Frobenius norm
that minimizes ∥I −AX∥F .

5.13 Orthogonal Projection
429
5.13
ORTHOGONAL PROJECTION
As discussed in §5.9, every pair of complementary subspaces deﬁnes a projector.
But when the complementary subspaces happen to be orthogonal complements,
the resulting projector has some particularly nice properties, and the purpose of
this section is to develop this special case in more detail. Discussions are in the
context of real spaces, but generalizations to complex spaces are straightforward
by replacing (⋆)T by (⋆)∗and “orthogonal matrix” by “unitary matrix.”
If M is a subspace of an inner-product space V, then V = M ⊕M⊥
by (5.11.1), and each v ∈V can be written uniquely as v = m + n, where
m ∈M and n ∈M⊥by (5.9.3). The vector m was deﬁned on p. 385 to be
the projection of v onto M along M⊥, so the following deﬁnitions are natural.
Orthogonal Projection
For v ∈V, let v = m + n, where m ∈M and n ∈M⊥.
•
m is called the orthogonal projection of v onto M.
•
The projector PM onto M along M⊥is called the orthogonal
projector onto M.
•
PM is the unique linear operator such that PMv = m (see p. 386).
These ideas are illustrated illustrated in Figure 5.13.1 for V = ℜ3.
Figure 5.13.1
Given an arbitrary pair of complementary subspaces M, N of ℜn, formula
(5.9.12) on p. 386 says that the projector P onto M along N is given by
P =

M | N
 
I
0
0
0
 
M | N
−1 =

M | 0

M | N
−1,
(5.13.1)
where the columns of M and N constitute bases for M and N, respectively.
So, how does this expression simplify when N = M⊥? To answer the question,

430
Chapter 5
Norms, Inner Products, and Orthogonality
observe that if N = M⊥, then NT M = 0 and MT N = 0. Furthermore, if
dim M = r, then MT M is r × r, and rank

MT M

= rank (M) = r by
(4.5.4), so MT M is nonsingular. Therefore, if the columns of N are chosen to
be an orthonormal basis for M⊥, then



MT M
−1 MT
NT


M | N

=

I
0
0
I

=⇒

M | N
−1=



MT M
−1 MT
NT

.
This together with (5.13.1) says the orthogonal projector onto M is given by
PM =

M | 0




MT M
−1 MT
NT

= M

MT M
−1 MT .
(5.13.2)
As discussed in §5.9, the projector associated with any given pair of com-
plementary subspaces is unique, and it doesn’t matter which bases are used to
form M and N in (5.13.1). Consequently, formula PM = M

MT M
−1 MT
is independent of the choice of M —just as long as its columns constitute some
basis for M. In particular, the columns of M need not be an orthonormal basis
for M. But if they are, then MT M = I, and (5.13.2) becomes PM = MMT .
Moreover, if the columns of M and N constitute orthonormal bases for M and
M⊥, respectively, then U =

M | N

is an orthogonal matrix, and (5.13.1) be-
comes
PM = U

Ir
0
0
0

UT .
In other words, every orthogonal projector is orthogonally similar to a diagonal
matrix in which the diagonal entries are 1’s and 0’s.
Below is a summary of the formulas used to build orthogonal projectors.
Constructing Orthogonal Projectors
Let M be an r-dimensional subspace of ℜn, and let the columns of
Mn×r and Nn×n−r be bases for M and M⊥, respectively. The or-
thogonal projectors onto M and M⊥are
•
PM = M

MT M
−1 MT and PM⊥= N

NT N
−1 NT .
(5.13.3)
If M and N contain orthonormal bases for M and M⊥, then
•
PM = MMT and PM⊥= NNT .
(5.13.4)
•
PM = U

Ir
0
0
0

UT , where U =

M | N

.
(5.13.5)
•
PM⊥= I −PM in all cases.
(5.13.6)
Note: Extensions of (5.13.3) appear on p. 634.

5.13 Orthogonal Projection
431
Example 5.13.1
Problem: Let un×1 ̸= 0, and consider the line L = span {u} . Construct the
orthogonal projector onto L, and then determine the orthogonal projection of
a vector xn×1 onto L.
Solution: The vector u by itself is a basis for L, so, according to (5.13.3),
PL = u

uT u
−1 uT = uuT
uT u
is the orthogonal projector onto L. The orthogonal projection of a vector x
onto L is therefore given by
PLx = uuT
uT ux =
uT x
uT u

u.
Note: If ∥u∥2 = 1, then PL = uuT , so PLx = uuT x = (uT x)u, and
∥PLx∥2 = |uT x| ∥u∥2 = |uT x|.
This yields a geometrical interpretation for the magnitude of the standard inner
product. It says that if u is a vector of unit length in L, then, as illustrated
in Figure 5.13.2, |uT x| is the length of the orthogonal projection of x onto the
line spanned by u.
x
u
L
0
PLx
|uT x|
Figure 5.13.2
Finally, notice that since PL = uuT is the orthogonal projector onto L, it must
be the case that PL⊥= I −PL = I −uuT is the orthogonal projection onto
L⊥. This was called an elementary orthogonal projector on p. 322—go back
and reexamine Figure 5.6.1.
Example 5.13.2
Volume, Gram–Schmidt, and QR. A solid in ℜm with parallel opposing
faces whose adjacent sides are deﬁned by vectors from a linearly independent set
{x1, x2, . . . , xn} is called an n-dimensional parallelepiped. As shown in the
shaded portions of Figure 5.13.3, a two-dimensional parallelepiped is a parallel-
ogram, and a three-dimensional parallelepiped is a skewed rectangular box.

432
Chapter 5
Norms, Inner Products, and Orthogonality
x1
x2
∥x1∥
∥(I −P2)x2∥
x1
x2
x3
∥(I −P3)x3∥
Figure 5.13.3
Problem: Determine the volumes of a two-dimensional and a three-dimensional
parallelepiped, and then make the natural extension to deﬁne the volume of an
n-dimensional parallelepiped.
Solution: In the two-dimensional case, volume is area, and it’s evident from
Figure 5.13.3 that the area of the shaded parallelogram is the same as the area
of the dotted rectangle. The width of the dotted rectangle is ν1 = ∥x1∥2 , and
the height is ν2 = ∥(I −P2)x2∥2 , where P2 is the orthogonal projector onto
the space (line) spanned by x1, and I −P2 is the orthogonal projector onto
span {x1}⊥. In other words, the area, V2, of the parallelogram is the length of
its base times its projected height, ν2, so
V2 = ∥x1∥2 ∥(I −P2)x2∥2 = ν1ν2.
Similarly, the volume of a three-dimensional parallelepiped is the area of its
base times its projected height. The area of the base was just determined to be
V2 = ∥x1∥2 ∥(I −P2)x2∥2 = ν1ν2, and it’s evident from Figure 5.13.3 that the
projected height is ν3 = ∥(I −P3)x3∥2 , where P3 is the orthogonal projector
onto span {x1, x2} . Therefore, the volume of the parallelepiped generated by
{x1, x2, x3} is
V3 = ∥x1∥2 ∥(I −P2)x2∥2 ∥(I −P3)x3∥2 = ν1ν2ν3.
It’s now clear how to inductively deﬁne V4, V5, etc. In general, the volume of
the parallelepiped generated by a linearly independent set {x1, x2, . . . , xn} is
Vn = ∥x1∥2 ∥(I −P2)x2∥2 ∥(I −P3)x3∥2 · · · ∥(I −Pn)xn∥2 = ν1ν2 · · · νn,
where Pk is the orthogonal projector onto span {x1, x2, . . . , xk−1} , and where
ν1 = ∥x1∥2
and
νk = ∥(I −Pk)xk∥2
for
k > 1.
(5.13.7)
Note that if {x1, x2, . . . , xn} is an orthogonal set, Vn = ∥x1∥2 ∥x2∥2 · · · ∥xn∥2 ,
which is what we would expect.

5.13 Orthogonal Projection
433
Connections with Gram–Schmidt and QR. Recall from (5.5.4) on p. 309
that the vectors in the Gram–Schmidt sequence generated from a linearly inde-
pendent set {x1, x2, . . . , xn} ⊂ℜm are u1 = x1/ ∥x1∥2 and
uk =

I −UkUT
k

xk

I −UkUT
k

xk

2
,
where
Uk =

u1 | u2 | · · · | uk−1

for k > 1.
Since {u1, u2, . . . , uk−1} is an orthonormal basis for span {x1, x2, . . . , xk−1} ,
it follows from (5.13.4) that UkUT
k
must be the orthogonal projector onto
span {x1, x2, . . . , xk−1} . Hence UkUT
k = Pk and (I−Pk)xk = (I−UkUT
k )xk,
so

I −UkUT
k

xk

2 = νk is the kth projected height in (5.13.7). This means
that when the Gram–Schmidt equations are written in the form of a QR fac-
torization as explained on p. 311, the diagonal elements of the upper-triangular
matrix R are the νk ’s. Consequently, the product of the diagonal entries in R
is the volume of the parallelepiped generated by the xk ’s. But the QR factor-
ization of A =

x1 | x2 | · · · | xn

is unique (Exercise 5.5.8), so it doesn’t matter
whether Gram–Schmidt or another method is used to determine the QR factors.
Therefore, we arrive at the following conclusion.
•
If Am×n = Qm×nRn×n is the (rectangular) QR factorization of a matrix
with linearly independent columns, then the volume of the n-dimensional
parallelepiped generated by the columns of A is Vn = ν1ν2 · · · νn, where
the νk ’s are the diagonal elements of R. We will see on p. 468 what this
means in terms of determinants.
Of course, not all projectors are orthogonal projectors, so a natural question
to ask is, “What characteristic features distinguish orthogonal projectors from
more general oblique projectors?” Some answers are given below.
Orthogonal Projectors
Suppose that P ∈ℜn×n is a projector—i.e., P2 = P. The following
statements are equivalent to saying that P is an orthogonal projector.
•
R (P) ⊥N (P).
(5.13.8)
•
PT = P
(i.e., orthogonal projector ⇐⇒P2 = P = PT ). (5.13.9)
•
∥P∥2 = 1 for the matrix 2-norm (p. 281).
(5.13.10)
Proof.
Every projector projects vectors onto its range along (parallel to) its
nullspace, so statement (5.13.8) is essentially a restatement of the deﬁnition of
an orthogonal projector. To prove (5.13.9), note that if P is an orthogonal
projector, then (5.13.3) insures that P is symmetric. Conversely, if a projector

434
Chapter 5
Norms, Inner Products, and Orthogonality
P is symmetric, then it must be an orthogonal projector because (5.11.5) on
p. 405 allows us to write
P = PT
=⇒
R (P) = R

PT 
=⇒
R (P) ⊥N (P).
To see why (5.13.10) characterizes projectors that are orthogonal, refer back
to Example 5.9.2 on p. 389 (or look ahead to (5.15.3)) and note that ∥P∥2 =
1/ sin θ, where θ is the angle between R (P) and N (P). This makes it clear
that ∥P∥2 ≥1 for all projectors, and ∥P∥2 = 1 if and only if θ = π/2, (i.e., if
and only if R (P) ⊥N (P) ).
Example 5.13.3
Problem: For A ∈ℜm×n such that rank (A) = r, describe the orthogonal
projectors onto each of the four fundamental subspaces of A.
Solution 1: Let Bm×r and Nn×n−r be matrices whose columns are bases for
R (A) and N (A), respectively—e.g., B might contain the basic columns of
A. The orthogonal decomposition theorem on p. 405 says R (A)⊥= N

AT 
and N (A)⊥= R

AT 
, so, by making use of (5.13.3) and (5.13.6), we can write
PR(A) = B

BT B
−1 BT ,
PN(AT ) = PR(A)⊥= I −PR(A) = I −B

BT B
−1 BT ,
PN(A) = N

NT N
−1 NT ,
PR(AT ) = PN(A)⊥= I −PN(A) = I −N

NT N
−1 NT .
Note: If rank (A) = n, then all columns of A are basic and
PR(A) = A

AT A
−1 AT .
(5.13.11)
Solution 2: Another way to describe these projectors is to make use of the
Moore–Penrose pseudoinverse A† (p. 423). Recall that if A has a URV factor-
ization
A = U

C
0
0
0

VT ,
then
A† = V

C−1
0
0
0

UT ,
where U =

U1 | U2

and V =

V1 | V2

are orthogonal matrices in which the
columns of U1 and V1 constitute orthonormal bases for R (A) and R

AT 
,
respectively, and the columns of U2 and V2 are orthonormal bases for N

AT 
and N (A), respectively. Computing the products AA† and A†A reveals
AA† = U

I
0
0
0

UT = U1UT
1
and
A†A = V

I
0
0
0

VT = V1VT
1 ,

5.13 Orthogonal Projection
435
so, according to (5.13.4),
PR(A) = U1UT
1 = AA†,
PN(AT ) = I −PR(A) = I −AA†,
PR(AT ) = V1VT
1 = A†A,
PN(A) = I −PR(AT ) = I −A†A.
(5.13.12)
The notion of orthogonal projection in higher-dimensional spaces is consis-
tent with the visual geometry in ℜ2 and ℜ3. In particular, it is visually evident
from Figure 5.13.4 that if M is a subspace of ℜ3, and if b is a vector outside
of M, then the point in M that is closest to b is p = PMb, the orthogonal
projection of b onto M.
M
p = PMb
b
0
min
m∈M ∥b −m∥2
Figure 5.13.4
The situation is exactly the same in higher dimensions. But rather than using
our eyes to understand why, we use mathematics—it’s surprising just how easy
it is to “see” such things in abstract spaces.
Closest Point Theorem
Let M be a subspace of an inner-product space V, and let b be a
vector in V. The unique vector in M that is closest to b is p = PMb,
the orthogonal projection of b onto M. In other words,
min
m∈M ∥b −m∥2 = ∥b −PMb∥2 = dist (b, M).
(5.13.13)
This is called the orthogonal distance between b and M.
Proof.
If p = PMb, then p −m ∈M for all m ∈M, and
b −p = (I −PM)b ∈M⊥,
so (p −m) ⊥(b −p). The Pythagorean theorem says ∥x + y∥2 = ∥x∥2 + ∥y∥2
whenever x ⊥y (recall Exercise 5.4.14), and hence
∥b −m∥2
2 = ∥b −p + p −m∥2
2 = ∥b −p∥2
2 + ∥p −m∥2
2 ≥∥p −m∥2
2 .

436
Chapter 5
Norms, Inner Products, and Orthogonality
In other words, minm∈M ∥b −m∥2 = ∥b −p∥2 . Now argue that there is not
another point in M that is as close to b as p is. If
:m ∈M such that
∥b −:m∥2 = ∥b −p∥2 , then by using the Pythagorean theorem again we see
∥b −:m∥2
2 = ∥b −p + p −:m∥2
2 = ∥b −p∥2
2 + ∥p −:m∥2
2
=⇒
∥p −:m∥2 = 0,
and thus :m = p.
Example 5.13.4
To illustrate some of the previous ideas, consider ℜn×n with the inner product
⟨A B⟩= trace

AT B

. If Sn is the subspace of n × n real-symmetric matrices,
then each of the following statements is true.
•
S⊥
n = the subspace Kn of n × n skew-symmetric matrices.
▷Sn ⊥Kn because for all S ∈Sn and K ∈Kn,
⟨S K⟩= trace

ST K

= −trace

SKT 
= −trace

SKT T
= −trace

KST 
= −trace

ST K

= −⟨S K⟩
=⇒
⟨S K⟩= 0.
▷ℜn×n = Sn ⊕Kn because every A ∈ℜn×n can be uniquely expressed
as the sum of a symmetric and a skew-symmetric matrix by writing
A = A + AT
2
+ A −AT
2
(recall (5.9.3) and Exercise 3.2.6).
•
The orthogonal projection of A ∈ℜn×n onto Sn is P(A) = (A + AT )/2.
•
The closest symmetric matrix to A ∈ℜn×n is P(A) = (A + AT )/2.
•
The distance from A ∈ℜn×n to Sn (the deviation from symmetry) is
dist(A, Sn) = ∥A−P(A)∥F =
(A−AT )/2

F =

trace (AT A)−trace (A2)
2
.
Example 5.13.5
Aﬃne Projections.
If v ̸= 0 is a vector in a space V, and if M is a
subspace of V, then the set of points A = v + M is called an aﬃne space in
V. Strictly speaking, A is not a subspace (e.g., it doesn’t contain 0 ), but, as
depicted in Figure 5.13.5, A is the translate of a subspace—i.e., A is just a copy
of M that has been translated away from the origin through v. Consequently,
notions such as projection onto A and points closest to A are analogous to the
corresponding concepts for subspaces.

5.13 Orthogonal Projection
437
Problem: For b ∈V, determine the point p in A = v + M that is closest to
b. In other words, explain how to project b orthogonally onto A.
Solution: The trick is to subtract v from b as well as from everything in A
to put things back into the context of subspaces where we already know the
answers. As illustrated in Figure 5.13.5, this moves A back down to M, and it
translates v →0, b →(b −v), and p →(p −v).
0
p - v
b
v
p
A = v + M
b −v
p −v
M
0
Figure 5.13.5
If p is to be the orthogonal projection of b onto A, then p −v must be the
orthogonal projection of b −v onto M, so
p −v = PM(b −v)
=⇒
p = v + PM(b −v),
(5.13.14)
and thus p is the point in A that is closest to b. Applications to the solution
of linear systems are developed in Exercises 5.13.17–5.13.22.
We are now in a position to replace the classical calculus-based theory of
least squares presented in §4.6 with a more modern vector space development.
In addition to being straightforward, the modern geometrical approach puts
the entire least squares picture in much sharper focus. Viewing concepts from
more than one perspective generally produces deeper understanding, and this is
particularly true for the theory of least squares.
Recall from p. 226 that for an inconsistent system Am×nx = b, the object
of the least squares problem is to ﬁnd vectors x that minimize the quantity
(Ax −b)T (Ax −b) = ∥Ax −b∥2
2 .
(5.13.15)
The classical development in §4.6 relies on calculus to argue that the set of vectors
x that minimize (5.13.15) is exactly the set that solves the (always consistent)
system of normal equations AT Ax = AT b. In the context of the closest point
theorem the least squares problem asks for vectors x such that Ax is as close

438
Chapter 5
Norms, Inner Products, and Orthogonality
to b as possible. But Ax is always a vector in R (A), and the closest point
theorem says that the vector in R (A) that is closest to b is PR(A)b, the
orthogonal projection of b onto R (A). Figure 5.13.6 illustrates the situation
in ℜ3.
b
0
R (A)
PR(A)b
PR(A)b
∥
−b∥2
=
min
x∈ℜn ∥Ax −b∥2
Figure 5.13.6
So the least squares problem boils down to ﬁnding vectors x such that
Ax = PR(A)b.
But this system is equivalent to the system of normal equations because
Ax = PR(A)b ⇐⇒PR(A)Ax = PR(A)b
⇐⇒PR(A)(Ax −b) = 0
⇐⇒(Ax −b) ∈N

PR(A)

= R (A)⊥= N

AT 
⇐⇒AT (Ax −b) = 0
⇐⇒AT Ax = AT b.
Characterizing the set of least squares solutions as the solutions to Ax = PR(A)b
makes it obvious that x = A†b is a particular least squares solution because
(5.13.12) insures AA† = PR(A), and thus
A(A†b) = PR(A)b.
Furthermore, since A†b is a particular solution of Ax = PR(A)b, the general
solution—i.e., the set of all least squares solutions—must be the aﬃne space
S = A†b + N (A). Finally, the fact that A†b is the least squares solution of
minimal norm follows from Example 5.13.5 together with
R

A†
= R

AT 
= N (A)⊥
(see part (g) of Exercise 5.12.16)
because (5.13.14) insures that the point in S that is closest to the origin is
p = A†b + PN(A)(0 −A†b) = A†b.
The classical development in §4.6 based on partial diﬀerentiation is not easily
generalized to cover the case of complex matrices, but the vector space approach
given in this example trivially extends to complex matrices by simply replacing
(⋆)T by (⋆)∗.
Below is a summary of some of the major points concerning the theory of
least squares.

5.13 Orthogonal Projection
439
Least Squares Solutions
Each of the following four statements is equivalent to saying that :x is a
least squares solution for a possibly inconsistent linear system Ax = b.
•
∥A:x −b∥2 = min
x∈ℜn ∥Ax −b∥2 .
(5.13.16)
•
A:x = PR(A)b.
(5.13.17)
•
AT A:x = AT b
( A∗A:x = A∗b when A ∈Cm×n ).
(5.13.18)
•
:x ∈A†b + N (A) ( A†b is the minimal 2-norm LSS).
(5.13.19)
Caution! These are valuable theoretical characterizations, but none is
recommended for ﬂoating-point computation. Directly solving (5.13.17)
or (5.13.18) or explicitly computing A† can be ineﬃcient and numeri-
cally unstable. Computational issues are discussed in Example 4.5.1 on
p. 214; Example 5.5.3 on p. 313; and Example 5.7.3 on p. 346.
The least squares story will not be complete until the following fundamental
question is answered: “Why is the method of least squares the best way to make
estimates of physical phenomena in the face of uncertainty?” This is the focal
point of the next section.
Exercises for section 5.13
5.13.1. Find the orthogonal projection of b onto M = span {u} , and then de-
termine the orthogonal projection of b onto M⊥, where b = ( 4
8 )T
and u = ( 3
1 )T .
5.13.2. Let A =


1
2
0
2
4
1
1
2
0

and b =


1
1
1

.
(a)
Compute the orthogonal projectors onto each of the four funda-
mental subspaces associated with A.
(b)
Find the point in N (A)⊥that is closest to b.
5.13.3. For an orthogonal projector P, prove that ∥Px∥2 = ∥x∥2 if and only
if x ∈R (P).
5.13.4. Explain why AT PR(A) = AT for all A ∈ℜm×n.

440
Chapter 5
Norms, Inner Products, and Orthogonality
5.13.5. Explain why PM = r
i=1 uiuiT whenever B = {u1, u2, . . . , ur} is an
orthonormal basis for M ⊆ℜn×1.
5.13.6. Explain how to use orthogonal reduction techniques to compute the
orthogonal projectors onto each of the four fundamental subspaces of a
matrix A ∈ℜm×n.
5.13.7. (a)
Describe all 2 × 2 orthogonal projectors in ℜ2×2.
(b)
Describe all 2 × 2 projectors in ℜ2×2.
5.13.8. The line L in ℜn passing through two distinct points u and v is
L = u + span {u −v} . If u ̸= 0 and v ̸= αu, then L is a line not
passing through the origin—i.e., L is not a subspace. Sketch a picture
in ℜ2 or ℜ3 to visualize this, and then explain how to project a vector
b orthogonally onto L.
5.13.9. Explain why :x is a least squares solution for Ax = b if and only if
∥A:x −b∥2 =
PN(AT )b

2 .
5.13.10. Prove that if ε = A:x −b, where :x is a least squares solution for
Ax = b, then ∥ε∥2
2 = ∥b∥2
2 −
PR(A)b
2
2 .
5.13.11. Let M be an r-dimensional subspace of ℜn. We know from (5.4.3)
that if B = {u1, u2, . . . , ur} is an orthonormal basis for M, and if
x ∈M, then x is equal to its Fourier expansion with respect to B.
That is, x = r
i=1(uiT x)ui. However, if x /∈M, then equality is not
possible (why?), so the question that arises is, “What does the Fourier
expansion on the right-hand side of this expression represent?” Answer
this question by showing that the Fourier expansion r
i=1(uiT x)ui is
the point in M that is closest to x in the euclidean norm. In other
words, show that r
i=1(uiT x)ui = PMx.
5.13.12. Determine the orthogonal projection of b onto M, where
b =



5
2
5
3



and
M = span








−3/5
0
4/5
0


,



0
0
0
1


,



4/5
0
3/5
0








.
Hint: Is this spanning set in fact an orthonormal basis?

5.13 Orthogonal Projection
441
5.13.13. Let M and N be subspaces of a vector space V, and consider the
associated orthogonal projectors PM and PN .
(a)
Prove that PMPN = 0 if and only if M ⊥N.
(b)
Is it true that PMPN = 0 if and only if PN PM = 0? Why?
5.13.14. Let M and N be subspaces of the same vector space, and let PM
and PN be orthogonal projectors onto M and N, respectively.
(a)
Prove that R (PM + PN ) = R (PM) + R (PN ) = M + N.
Hint: Use Exercise 4.2.9 along with (4.5.5).
(b)
Explain why M ⊥N if and only if PMPN = 0.
(c)
Explain why PM + PN is an orthogonal projector if and only
if PMPN = 0, in which case R (PM + PN ) = M ⊕N and
M ⊥N. Hint: Recall Exercise 5.9.17.
5.13.15. Anderson–Duﬃn Formula.
59 Prove that if M and N are subspaces
of the same vector space, then the orthogonal projector onto M ∩N
is given by PM∩N = 2PM(PM + PN )†PN . Hint: Use (5.13.12) and
Exercise 5.13.14 to show PM(PM + PN )†PN = PN (PM + PN )†PM.
Argue that if Z = 2PM(PM+PN )†PM, then Z = PM∩N Z = PM∩N .
5.13.16. Given a square matrix X, the matrix exponential eX is deﬁned as
eX = I + X + X2
2! + X3
3! + · · · =
∞

n=0
Xn
n! .
It can be shown that this series converges for all X, and it is legitimate
to diﬀerentiate and integrate it term by term to produce the statements
deAt/dt = AeAt = eAtA and

eAtA dt = eAt.
(a)
Use the fact that limt→∞e−AT At = 0 for all A ∈ℜm×n to
show A† =
 ∞
0
e−AT AtAT dt.
(b)
If limt→∞e−Ak+1t = 0, show AD =
 ∞
0
e−Ak+1tAkdt, where
k = index(A).
60
(c)
For nonsingular matrices, show that if limt→∞e−At = 0, then
A−1 =
 ∞
0
e−Atdt.
59
W. N. Anderson, Jr., and R. J. Duﬃn discovered this formula for the orthogonal projector onto
an intersection in 1969. They called PM(PM + PN )†PN the parallel sum of PM and PN
because it is the matrix generalization of the scalar function r1r2/(r1 + r2) = r1(r1 + r2)−1r2
that is the resistance of a circuit composed of two resistors r1 and r2 connected in parallel.
The simple elegance of the Anderson–Duﬃn formula makes it one of the innumerable little
sparkling facets in the jewel that is linear algebra.
60
A more useful integral representation for AD is given in Exercise 7.9.22 (p. 615).

442
Chapter 5
Norms, Inner Products, and Orthogonality
5.13.17. An aﬃne space v + M ⊆ℜn for which dim M = n −1 is called a
hyperplane. For example, a hyperplane in ℜ2 is a line (not necessarily
through the origin), and a hyperplane in ℜ3 is a plane (not necessarily
through the origin). The ith equation Ai∗x = bi in a linear system
Am×nx = b is a hyperplane in ℜn, so the solutions of Ax = b occur
at the intersection of the m hyperplanes deﬁned by the rows of A.
(a)
Prove that for a given scalar β and a nonzero vector u ∈ℜn,
the set H = {x | uT x = β} is a hyperplane in ℜn.
(b)
Explain why the orthogonal projection of b ∈ℜn onto H is
p = b −

uT b −β/uT u

u.
5.13.18. For u, w ∈ℜn such that uT w ̸= 0, let M = u⊥and W = span {w} .
(a)
Explain why ℜn = M ⊕W.
(b)
For b ∈ℜn×1, explain why the oblique projection of b onto
M along W is given by p = b −uT b/uT ww.
(c)
For a given scalar β, let H be the hyperplane in ℜn deﬁned
by H = {x | uT x = β}—see Exercise 5.13.17. Explain why the
oblique projection of b onto H along W should be given by
p = b −

uT b −β/uT w

w.
5.13.19. Kaczmarz’s
61 Projection Method. The solution of a nonsingular
system

a11
a12
a21
a22
 
x1
x2

=

b1
b2

is the intersection of the two hyperplanes (lines in this case) deﬁned by
H1={(x1, x2) | a11x1+a12x2 = b1} , H2={(x1, x2) | a21x1+a22x2 = b2}.
It’s visually evident that by starting with an arbitrary point p0 and
alternately projecting orthogonally onto H1 and H2 as depicted in
Figure 5.13.7, the resulting sequence of projections {p1, p2, p3, p4, . . . }
converges to H1 ∩H2, the solution of Ax = b.
61
Although this idea has probably occurred to many people down through the ages, credit is
usually given to Stefan Kaczmarz, who published his results in 1937. Kaczmarz was among a
school of bright young Polish mathematicians who were beginning to ﬂower in the ﬁrst part
of the twentieth century. Tragically, this group was decimated by Hitler’s invasion of Poland,
and Kaczmarz himself was killed in military action while trying to defend his country.

5.13 Orthogonal Projection
443
Figure 5.13.7
This idea can be generalized by using Exercise 5.13.17. For a consis-
tent system An×rx = b with rank (A) = r, scale the rows so that
∥Ai∗∥2 = 1 for each i, and let Hi = {x | Ai∗x = bi} be the hyperplane
deﬁned by the ith equation. Begin with an arbitrary vector p0 ∈ℜr×1,
and successively perform orthogonal projections onto each hyperplane
to generate the following sequence:
p1 = p0 −(A1∗p0 −b1) (A1∗)T
(project p0 onto H1 ),
p2 = p1 −(A2∗p1 −b2) (A2∗)T
(project p1 onto H2 ),
...
...
pn = pn−1 −(An∗pn−1 −bn) (An∗)T
(project pn−1 onto Hn ).
When all n hyperplanes have been used, continue by repeating the
process. For example, on the second pass project pn onto H1; then
project pn+1 onto H2, etc. For an arbitrary p0, the entire Kaczmarz
sequence is generated by executing the following double loop:
For k = 0, 1, 2, 3, . . .
For i = 1, 2, . . . , n
pkn+i = pkn+i−1 −(Ai∗pkn+i−1 −bi) (Ai∗)T
Prove that the Kaczmarz sequence converges to the solution of Ax = b
by showing ∥pkn+i −x∥2
2 = ∥pkn+i−1 −x∥2
2 −(Ai∗pkn+i−1 −bi)2 .
5.13.20. Oblique Projection Method.
Assume that a nonsingular system
An×nx = b has been row scaled so that ∥Ai∗∥2 = 1 for each i, and let
Hi = {x | Ai∗x = bi} be the hyperplane deﬁned by the ith equation—
see Exercise 5.13.17. In theory, the system can be solved by making n−1
oblique projections of the type described in Exercise 5.13.18 because if
an arbitrary point p1 in H1 is projected obliquely onto H2 along H1
to produce p2, then p2 is in H1∩H2. If p2 is projected onto H3 along
H1 ∩H2 to produce p3, then p3 ∈H1 ∩H2 ∩H3, and so forth until
pn ∈∩n
i=1Hi. This is similar to Kaczmarz’s method given in Exercise
5.13.19, but here we are projecting obliquely instead of orthogonally.
However, projecting pk onto Hk+1 along ∩k
i=1Hi is diﬃcult because

444
Chapter 5
Norms, Inner Products, and Orthogonality
∩k
i=1Hi is generally unknown. This problem is overcome by modifying
the procedure as follows—use Figure 5.13.8 with n = 3 as a guide.
Figure 5.13.8
Step 0.
Begin with any set

p(1)
1 , p(1)
2 , . . . , p(1)
n

⊂H1 such that

p(1)
1
−p(1)
2

,

p(1)
1
−p(1)
3

, . . . ,

p(1)
1
−p(1)
n

is linearly independent
and A2∗

p(1)
1
−p(1)
k

̸= 0 for k = 2, 3, . . . , n.
Step 1.
In turn, project p(1)
1
onto H2 through p(1)
2 , p(1)
3 , . . . , p(1)
n
to
produce

p(2)
2 , p(2)
3 , . . . , p(2)
n

⊂H1 ∩H2 (see Figure 5.13.8).
Step 2.
Project p(2)
2
onto H3 through p(2)
3 , p(2)
4 , . . . , p(2)
n
to produce

p(3)
3 , p(3)
4 , . . . , p(3)
n

⊂H1 ∩H2 ∩H3. And so the process continues.
Step n−1.
Project p(n−1)
n−1
through p(n−1)
n
to produce p(n)
n
∈∩n
i=1Hi.
Of course, x = p(n)
n
is the solution of the system.
For any initial set {x1, x2, . . . , xn} ⊂H1 satisfying the properties
described in Step 0, explain why the following algorithm performs the
computations described in Steps 1, 2, . . . , n −1.
For i = 2 to n
For j = i to n
xj ←xj −(Ai∗xi−1 −bi)
Ai∗(xi−1 −xj)(xi−1 −xj)
x ←xn
(the solution of the system)
5.13.21. Let M be a subspace of ℜn, and let R = I −2PM. Prove that the
orthogonal distance between any point x ∈ℜn and M⊥is the same as
the orthogonal distance between Rx and M⊥. In other words, prove
that R reﬂects everything in ℜn about M⊥. Naturally, R is called
the reﬂector about M⊥. The elementary reﬂectors I −2uuT /uT u
discussed on p. 324 are special cases—go back and look at Figure 5.6.2.

5.13 Orthogonal Projection
445
5.13.22. Cimmino’s Reﬂection Method. In 1938 the Italian mathematician
Gianfranco Cimmino used the following elementary observation to con-
struct an iterative algorithm for solving linear systems. For a 2 × 2 sys-
tem Ax = b, let H1 and H2 be the two lines (hyperplanes) deﬁned
by the two equations. For an arbitrary guess r0, let r1 be the reﬂection
of r0 about the line H1, and let r2 be the reﬂection of r0 about the
line H2. As illustrated in Figure 5.13.9, the three points r0, r1, and
r2 lie on a circle whose center is H1 ∩H2 (the solution of the system).
Figure 5.13.9
The mean value m = (r1 + r2)/2 is strictly inside the circle, so m is a
better approximation to the solution than r0. It’s visually evident that
iteration produces a sequence that converges to the solution of Ax = b.
Prove this in general by using the following blueprint.
(a)
For a scalar β and a vector u ∈ℜn such that ∥u∥2 = 1,
consider the hyperplane H = {x | uT x = β} (Exercise 5.13.17).
Use (5.6.8) to show that the reﬂection of a vector b about H
is r = b −2(uT b −β)u.
(b)
For a system Ax = b in which the rows of A ∈ℜn×r have been
scaled so that ∥Ai∗∥2 = 1 for each i, let Hi = {x | Ai∗x = bi}
be the hyperplane deﬁned by the ith equation. If r0 ∈ℜr×1 is
an arbitrary vector, and if ri is the reﬂection of r0 about Hi,
explain why the mean value of the reﬂections {r1, r2, . . . , rn} is
m = r0 −(2/n)AT ε, where ε = Ar0 −b.
(c)
Iterating part (b) produces mk = mk−1 −(2/n)AT εk−1, where
εk−1 = Amk−1 −b. Show that if A is nonsingular, and if
x = A−1b, then x−mk =

I −(2/n)AT A
k (x−m0). Note:
It can be proven that

I −(2/n)AT A
k →0 as k →∞, so
mk →x for all m0. In fact, mk converges even if A is rank
deﬁcient—if consistent, it converges to a solution, and, if incon-
sistent, the limit is a least squares solution. Cimmino’s method
also works with weighted means. If W = diag (w1, w2, . . . , wn),
where wi > 0 and  wi = 1, then mk = mk−1 −ωAT Wεk−1
is a convergent sequence in which 0 < ω < 2 is a “relaxation
parameter” that can be adjusted to alter the rate of convergence.

446
Chapter 5
Norms, Inner Products, and Orthogonality
5.14
WHY LEAST SQUARES?
Drawing inferences about natural phenomena based upon physical observations
and estimating characteristics of large populations by examining small samples
are fundamental concerns of applied science. Numerical characteristics of a phe-
nomenon or population are often called parameters, and the goal is to design
functions or rules called estimators that use observations or samples to estimate
parameters of interest. For example, the mean height h of all people is a pa-
rameter of the world’s population, and one way of estimating h is to observe
the mean height of a sample of k people. In other words, if hi is the height of
the ith person in a sample, the function ˆh deﬁned by
ˆh(h1, h2, . . . , hk) = 1
k
 k

i=1
hi

is an estimator for h. Moreover, ˆh is a linear estimator because ˆh is a linear
function of the observations.
Good estimators should possess at least two properties—they should be un-
biased and they should have minimal variance. For example, consider estimating
the center of a circle drawn on a wall by asking Larry, Moe, and Curly to each
throw one dart at the circle. To decide which estimator is best, we need to know
more about each thrower’s style. While being able to throw a tight pattern, it is
known that Larry tends to have a left-hand bias in his style. Moe doesn’t suﬀer
from a bias, but he tends to throw a rather large pattern. However, Curly can
throw a tight pattern without a bias. Typical patterns are shown below.
Larry
Moe
Curly
Although Larry has a small variance, he is an unacceptable estimator be-
cause he is biased in the sense that his average is signiﬁcantly diﬀerent than
the center. Moe and Curly are each unbiased estimators because they have an
average that is the center, but Curly is clearly the preferred estimator because
his variance is much smaller than Moe’s. In other words, Curly is the unbiased
estimator of minimal variance.
To make these ideas more formal, let’s adopt the following standard no-
tation and terminology from elementary probability theory concerning random
variables X and Y.

5.14 Why Least Squares?
447
•
E[X] = µX denotes the mean (or expected value) of X.
•
Var[X] = E

(X −µX)2
= E[X2] −µ2
X is the variance of X.
•
Cov[X, Y ] = E[(X −µX)(Y −µY )] = E[XY ] −µXµY is the covariance of
X and Y.
Minimum Variance Unbiased Estimators
An estimator ˆθ (consider as a random variable) for a parameter θ is
said to be unbiased when E[ˆθ] = θ, and ˆθ is called a minimum
variance unbiased estimator for θ whenever Var[ˆθ] ≤Var[ˆφ] for
all unbiased estimators ˆφ of θ.
These ideas make it possible to precisely articulate why the method of least
squares is the best way to ﬁt observed data. Let Y be a variable that is known
(or assumed) to be linearly related to other variables X1, X2, . . . , Xn according
to the equation
62
Y = β1X1 + · · · + βnXn,
(5.14.1),
where the βi ’s are unknown constants (parameters). Suppose that the values
assumed by the Xi ’s are not subject to error or variation and can be exactly
observed or speciﬁed, but, due perhaps to measurement error, the values of Y
cannot be exactly observed. Instead, we observe
y = Y + ε = β1X1 + · · · + βnXn + ε,
(5.14.2)
where ε is a random variable accounting for the measurement error. For exam-
ple, consider the problem of determining the velocity v of a moving object by
measuring the distance D it has traveled at various points in time T by using
the linear relation D = vT. Time can be prescribed at exact values such as
T1 = 1 second, T2 = 2 seconds, etc., but observing the distance traveled at the
prescribed values of T will almost certainly involve small measurement errors so
that in reality the observed distances satisfy d = D + ε = vT + ε. Now consider
the general problem of determining the parameters βk in (5.14.1) by observing
(or measuring) values of Y at m diﬀerent points Xi∗= (xi1, xi2, . . . , xin) ∈ℜn,
where xij is the value of Xj to be used when making the ith observation. If yi
denotes the random variable that represents the outcome of the ith observation
of Y, then according to (5.14.2),
yi = β1xi1 + · · · + βnxin + εi,
i = 1, 2, . . . , m,
(5.14.3)
62
Equation (5.14.1) is called a no-intercept model, whereas the slightly more general equation
Y = β0 + β1X1 + · · · + βnXn is known as an intercept model. Since the analysis for an
intercept model is not signiﬁcantly diﬀerent from the analysis of the no-intercept case, we deal
only with the no-intercept case and leave the intercept model for the reader to develop.

448
Chapter 5
Norms, Inner Products, and Orthogonality
where εi is a random variable accounting for the ith observation (or mea-
surement) error.
63 It is generally valid to assume that observation errors are not
correlated with each other but have a common variance (not necessarily known)
and a zero mean. In other words, we assume that
E[εi] = 0 for each i
and
Cov[εi, εj] =

σ2
when i = j,
0
when i ̸= j.
If y =




y1
y2
...
ym



, X =




x11
x12
· · ·
x1n
x21
x22
· · ·
x2n
...
...
...
...
xm1
xm2
· · ·
xmn



, β =




β1
β2
...
βn



, ε =




ε1
ε2
...
εm



,
then the equations in (5.14.3) can be written as y = Xm×nβ + ε. In practice,
the points Xi∗at which observations yi are made can almost always be selected
to insure that rank (Xm×n) = n, so the complete statement of the standard
linear model is
y = Xm×nβ + ε
such that





rank (X) = n,
E[ε] = 0,
Cov[ε] = σ2I,
(5.14.4)
where we have adopted the conventions
E[ε]=




E[ε1]
E[ε2]
...
E[εm]



and Cov[ε]=




Cov[ε1, ε1]
Cov[ε1, ε2]
· · ·
Cov[ε1, εm]
Cov[ε2, ε1]
Cov[ε2, ε2]
· · ·
Cov[ε2, εm]
...
...
...
...
Cov[εm, ε1]
Cov[εm, ε2]
· · ·
Cov[εm, εm]



.
The problem is to determine the best (minimum variance) linear (linear function
of the yi ’s) unbiased estimators for the components of β. Gauss realized in 1821
that this is precisely what the least squares solution provides.
Gauss–Markov Theorem
For the standard linear model (5.14.4), the minimum variance linear
unbiased estimator for βi is given by the ith component
ˆβi in the
vector ˆβ =

XT X
−1XT y = X†y. In other words, the best linear
unbiased estimator for β is the least squares solution of Xˆβ = y.
63
In addition to observation and measurement errors, other errors such as modeling errors or
those induced by imposing simplifying assumptions produce the same kind of equation—recall
the discussion of ice cream on p. 228.

5.14 Why Least Squares?
449
Proof.
It is clear that ˆβ = X†y is a linear estimator of β because each com-
ponent ˆβi = 
k[X†]ik yk is a linear function of the observations. The fact that
ˆβ is unbiased follows by using the linear nature of expected value to write
E[y] = E[Xβ + ε] = E[Xβ] + E[ε] = Xβ + 0 = Xβ,
so that
E
ˆβ

= E

X†y

= X†E[y] = X†Xβ =

XT X
−1XT Xβ = β.
To argue that ˆβ = X†y has minimal variance among all linear unbiased estima-
tors for β, let β∗be an arbitrary linear unbiased estimator for β. Linearity of
β∗implies the existence of a matrix Ln×m such that β∗= Ly, and unbiased-
ness insures β = E[β∗] = E[Ly] = LE[y] = LXβ. We want β = LXβ to hold
irrespective of the values of the components in β, so it must be the case that
LX = In (recall Exercise 3.5.5). For i ̸= j we have
0 = Cov[εi, εj] = E[εiεj] −µεiµεj
=⇒
E[εiεj] = E[εi]E[εj] = 0,
so that
Cov[yi, yj] =

E[(yi −µyi)2] = E[ε2
i ] = Var[εi] = σ2
when i = j,
E[(yi −µyi)(yj −µyj)] = E[εiεj] = 0
when i ̸= j.
(5.14.5)
This together with the fact that Var[aW +bZ] = a2Var[W]+b2Var[Z] whenever
Cov[W, Z] = 0 allows us to write
Var[β∗
i ] = Var[Li∗y] = Var
; m

k=1
likyk
<
= σ2
m

k=1
l2
ik = σ2 ∥Li∗∥2
2 .
Since LX = I, it follows that Var[β∗
i ] is minimal if and only if Li∗is the
minimum norm solution of the system zT X = eT
i . We know from (5.12.17) that
the (unique) minimum norm solution is given by zT = eT
i X† = X†
i∗, so Var[β∗
i ]
is minimal if and only if Li∗= X†
i∗. Since this holds for i = 1, 2, . . . , m, it follows
that L = X†. In other words, the components of ˆβ = X†y are the (unique)
minimal variance linear unbiased estimators for the parameters in β.
Exercises for section 5.14
5.14.1. For a matrix Zm×n = [zij], of random variables, E[Z] is deﬁned to be
the m × n matrix whose (i, j)-entry is E[zij]. Consider the standard
linear model described in (5.14.4), and let ˆe denote the vector of random
variables deﬁned by ˆe = y −Xˆβ in which ˆβ =

XT X
−1XT y = X†y.
Demonstrate that
ˆσ2 =
ˆeT ˆe
m −n
is an unbiased estimator for σ2. Hint: dT c = trace(cdT ) for column
vectors c and d, and, by virtue of Exercise 5.9.13,
trace

I −XX†
= m −trace

XX†
= m −rank

XX†
= m −n.

450
Chapter 5
Norms, Inner Products, and Orthogonality
5.15
ANGLES BETWEEN SUBSPACES
Consider the problem of somehow gauging the separation between a pair of
nontrivial but otherwise general subspaces M and N of ℜn. Perhaps the ﬁrst
thing that comes to mind is to measure the angle between them. But deﬁning the
“angle” between subspaces in ℜn is not as straightforward as the visual geometry
of ℜ2 or ℜ3 might suggest. There is just too much “wiggle room” in higher
dimensions to make any one deﬁnition completely satisfying, and the “correct”
deﬁnition usually varies with the speciﬁc application under consideration.
Before exploring general angles, recall what has already been said about
some special cases beginning with the angle between a pair of one-dimensional
subspaces. If M and N are spanned by vectors u and v, respectively, and if
∥u∥= 1 = ∥v∥, then the angle between M and N is deﬁned by the expression
cos θ = vT u (p. 295). This idea was carried one step further on p. 389 to deﬁne
the angle between two complementary subspaces, and an intuitive connection to
norms of projectors was presented. These intuitive ideas are now made rigorous.
Minimal Angle
The minimal angle between nonzero subspaces M, N ⊆ℜn is deﬁned
to be the number 0 ≤θmin ≤π/2 for which
cos θmin =
max
u∈M, v∈N
∥u∥2=∥v∥2=1
vT u.
(5.15.1)
•
If PM and PN are the orthogonal projectors onto M and N,
respectively, then
cos θmin = ∥PN PM∥2 .
(5.15.2)
•
If M and N are complementary subspaces, and if PMN is the
oblique projector onto M along N, then
sin θmin =
1
∥PMN ∥2
.
(5.15.3)
•
M and N are complementary subspaces if and only if PM −PN
is invertible, and in this case
sin θmin =
1
∥(PM −PN )−1∥2
.
(5.15.4)
Proof of (5.15.2).
If f : V →ℜis a function deﬁned on a space V such that
f(αx) = αf(x) for all scalars α ≥0, then
max
∥x∥=1 f(x) = max
∥x∥≤1 f(x)
(see Exercise 5.15.8).
(5.15.5)

5.15 Angles between Subspaces
451
This together with (5.2.9) and the fact that PMx ∈M and PN y ∈N means
cos θmin =
max
u∈M, v∈N
∥u∥2=∥v∥2=1
vT u =
max
u∈M, v∈N
∥u∥2≤1, ∥v∥2≤1
vT u
=
max
∥x∥2≤1, ∥y∥2≤1 yT PN PMx = ∥PN PM∥2 .
Proof of (5.15.3).
Let U =

U1 | U2

and V =

V1 | V2

be orthogonal
matrices in which the columns of U1 and U2 constitute orthonormal bases for
M and M⊥, respectively, and V1 and V2 are orthonormal bases for N ⊥
and N, respectively, so that UT
i Ui = I and VT
i Vi = I for i = 1, 2, and
PM = U1UT
1 , I −PM = U2UT
2 , PN = V2VT
2 , I −PN = V1VT
1 .
As discussed on p. 407, there is a nonsingular matrix C such that
PMN = U

C
0
0
0

VT = U1CVT
1 .
(5.15.6)
Notice that P2
MN = PMN
implies C = CVT
1 U1C, which in turn insures
C−1 = VT
1 U1. Recall that ∥XAY∥2 = ∥A∥2 whenever X has orthonormal
columns and Y has orthonormal rows (Exercise 5.6.9). Consequently,
∥PMN ∥2 = ∥C∥2 =
1
min
∥x∥2=1
C−1x

2
=
1
min
∥x∥2=1
VT
1 U1x

2
(recall (5.2.6)).
Combining this with (5.15.2) produces (5.15.3) by writing
sin2 θmin = 1 −cos2 θmin = 1 −∥PN PM∥2
2 = 1 −
V2VT
2 U1UT
1
2
2
= 1 −
(I −V1VT
1 )U1
2
2 = 1 −max
∥x∥2=1
(I −V1VT
1 )U1x
2
2
= 1 −max
∥x∥2=1 xT UT
1 (I −V1VT
1 )U1x = 1 −max
∥x∥2=1

1 −
VT
1 U1x
2
2

= 1 −

1 −min
∥x∥2=1
VT
1 U1x
2
2

=
1
∥PMN ∥2
2
.
Proof of (5.15.4).
Observe that
UT (PM −PN )V =
 UT
1
UT
2

(U1UT
1 −V2VT
2 )

V1 | V2

=

UT
1 V1
0
0
−UT
2 V2

,
(5.15.7)

452
Chapter 5
Norms, Inner Products, and Orthogonality
where UT
1 V1 = (C−1)T is nonsingular. To see that UT
2 V2 is also nonsingular,
suppose dim M = r so that dim N = n −r and UT
2 V2 is n −r × n −r. Use
the formula for the rank of a product (4.5.1) to write
rank

UT
2 V2

= rank

UT
2

−dim N

UT
2

∩R (V2) = n−r−dim M∩N = n−r.
It now follows from (5.15.7) that PM −PN is nonsingular, and
VT (PM −PN )−1U =

(UT
1 V1)−1
0
0
−(UT
2 V2)−1

.
(Showing that PM −PN
is nonsingular implies M ⊕N = ℜn is Exercise
5.15.6.) Formula (5.2.12) on p. 283 for the 2-norm of a block-diagonal matrix
can now be applied to yield
(PM −PN )−1
2 = max
5 (UT
1 V1)−1
2 ,
(UT
2 V2)−1
2
6
.
(5.15.8)
But
(UT
1 V1)−1
2 =
(UT
2 V2)−1
2 because we can again use (5.2.6) to write
1
(UT
1 V1)−12
2
= min
∥x∥2=1
UT
1 V1x
2
2 = min
∥x∥2=1 xT VT
1 U1UT
1 V1x
= min
∥x∥2=1 xT VT
1 (I −U2UT
2 )V1x
= min
∥x∥2=1(1 −xT VT
1 U2UT
2 V1x)
= 1 −max
∥x∥2=1
UT
2 V1x
2
2 = 1 −
UT
2 V1
2
2 .
By a similar argument, 1/
(UT
2 V2)−12
2 = 1−
UT
2 V1
2
2 (Exercise 5.15.11(a)).
Therefore,
(PM −PN )−1
2 =
(UT
1 V1)−1
2 =
CT 
2 = ∥C∥2 = ∥PMN ∥2 .
While the minimal angle works ﬁne for complementary spaces, it may not
convey much information about the separation between noncomplementary sub-
spaces. For example, θmin = 0 whenever M and N have a nontrivial inter-
section, but there nevertheless might be a nontrivial “gap” between M and
N —look at Figure 5.15.1. Rather than thinking about angles to measure such a
gap, consider orthogonal distances as discussed in (5.13.13). Deﬁne
δ(M, N) = max
m∈M
∥m∥2=1
dist (m, N) = max
m∈M
∥m∥2=1
∥(I −PN )m∥2

5.15 Angles between Subspaces
453
to be the directed distance from M to N, and notice that δ(M, N) ≤1
because (5.2.5) and (5.13.10) can be combined to produce
dist (m, N) = ∥(I −PN )m∥2 = ∥PN ⊥m∥2 ≤∥PN ⊥∥2 ∥m∥2 = 1.
Figure 5.15.1 illustrates δ(M, N) for two planes in ℜ3.
M
m
N
δ(M, N) = max
m∈M
∥m∥2=1
dist (m, N)
Figure 5.15.1
This picture is a bit misleading because δ(M, N) = δ(N, M) for this particular
situation. However, δ(M, N) and δ(N, M) need not always agree—that’s why
the phrase directed distance is used. For example, if M is the xy-plane in ℜ3
and N = span {(0, 1, 1)} , then δ(N, M) = 1/
√
2 while δ(M, N) = 1. Con-
sequently, using orthogonal distance to gauge the degree of maximal separation
between an arbitrary pair of subspaces requires that both values of δ be taken
into account. Hence we make the following deﬁnition.
Gap Between Subspaces
The gap between subspaces M, N ⊆ℜn is deﬁned to be
gap (M, N) = max

δ(M, N), δ(N, M)

,
(5.15.9)
where δ(M, N) = max
m∈M
∥m∥2=1
dist (m, N).
Evaluating the gap between a given pair of subspaces requires knowing some
properties of directed distance. Observe that (5.15.5) together with the fact that
∥AT ∥2 = ∥A∥2 can be used to write
δ(M, N) = max
m∈M
∥m∥2=1
dist (m, N) = max
m∈M
∥m∥2=1
∥(I −PN )m∥2
= max
m∈M
∥m∥2≤1
∥(I −PN )m∥2 = max
∥x∥2=1 ∥(I −PN )PMx∥2
= ∥(I −PN )PM∥2 = ∥PM(I −PN )∥2 .
(5.15.10)

454
Chapter 5
Norms, Inner Products, and Orthogonality
Similarly, δ(N, M) = ∥(I −PM)PN ∥2 = ∥PN (I −PM)∥2 . If U =

U1 | U2

and V =

V1 | V2

are the orthogonal matrices introduced on p. 451, then
δ(M, N) = ∥PM(I −PN )∥2 =
U1UT
1 V1VT
1

2 =
UT
1 V1

2
and
(5.15.11)
δ(N, M) = ∥(I −PM)PN ∥2 =
U2UT
2 V2VT
2

2 =
UT
2 V2

2 .
Combining these observations with (5.15.7) leads us to conclude that
∥PM −PN ∥2 = max
5 UT
1 V1

2 ,
UT
2 V2

2
6
= max

δ(M, N), δ(N, M)

= gap (M, N).
(5.15.12)
Below is a summary of these and other properties of the gap measure.
Gap Properties
The following statements are true for subspaces M, N ⊆ℜn.
•
gap (M, N) = ∥PM −PN ∥2 .
•
gap (M, N) = max

∥(I −PN )PM∥2 , ∥(I −PM)PN ∥2

.
•
gap (M, N) = 1 whenever dim M ̸= dim N.
(5.15.13)
•
If dim M = dim N, then δ(M, N) = δ(N, M), and
▷
gap (M, N) = 1 when M⊥∩N (or M ∩N ⊥) ̸= 0, (5.15.14)
▷
gap (M, N) < 1 when M⊥∩N (or M ∩N ⊥) = 0. (5.15.15)
Proof of (5.15.13).
Suppose that dim M = r and dim N = k, where r < k.
Notice that this implies that M⊥∩N ̸= 0, for otherwise the formula for the
dimension of a sum (4.4.19) yields
n ≥dim(M⊥+ N) = dim M⊥+ dim N = n −r + k > n,
which is impossible. Thus there exists a nonzero vector x ∈M⊥∩N, and by
normalization we can take ∥x∥2 = 1. Consequently, (I−PM)x = x = PN x, so
∥(I −PM)PN x∥2 = 1. This insures that ∥(I −PM)PN ∥2 = 1, which implies
δ(N, M) = 1.
Proof of (5.15.14).
Assume dim M = dim N = r, and use the formula for the
dimension of a sum along with (M ∩N ⊥)⊥= M⊥+ N (Exercise 5.11.5) to
conclude that
dim

M⊥∩N

= dim M⊥+ dim N −dim

M⊥+ N

= (n −r) + r −dim

M ∩N ⊥⊥= dim

M ∩N ⊥
.

5.15 Angles between Subspaces
455
When dim

M ∩N ⊥
= dim

M⊥∩N

> 0, there are vectors x ∈M⊥∩N
and y ∈M ∩N ⊥such that ∥x∥2 = 1 = ∥y∥2 . Hence, ∥(I −PM)PN x∥2 =
∥x∥2 = 1, and ∥(I −PN )PMy∥2 = ∥y∥2 = 1, so
δ(N, M) = ∥(I −PM)PN ∥2 = 1 = ∥(I −PN )PM∥2 = δ(M, N).
Proof of (5.15.15).
If dim

M ∩N ⊥
= dim

M⊥∩N

= 0, then UT
2 V1 is
nonsingular because it is r × r and has rank r—apply the formula (4.5.1) for
the rank of a product. From (5.15.11) we have
δ2(M, N) =
UT
1 V1
2
2 =
U1UT
1 V1
2
2 =
(I −U2UT
2 )V1
2
2
= max
∥x∥2=1 xT VT
1 (I −U2UT
2 )V1x = max
∥x∥2=1

1 −
UT
2 V1x
2
2

= 1 −min
∥x∥2=1
UT
2 V1x
2
2 = 1 −
1
(UT
2 V1)−12
2
< 1 (recall (5.2.6)).
A similar argument shows δ2(N, M) =
UT
2 V2
2
2 = 1 −1/
(UT
2 V1)−12
2 (Ex-
ercise 5.15.11(b)), so δ(N, M) = δ(M, N) < 1.
Because 0 ≤gap (M, N) ≤1, the gap measure deﬁnes another angle be-
tween M and N.
Maximal Angle
The maximal angle between subspaces M, N ⊆ℜn is deﬁned to be
the number 0 ≤θmax ≤π/2 for which
sin θmax = gap (M, N) = ∥PM −PN ∥2 .
(5.15.16)
For applications requiring knowledge of the degree of separation between
a pair of nontrivial complementary subspaces, the minimal angle does the job.
Similarly, the maximal angle adequately handles the task for subspaces of equal
dimension. However, neither the minimal nor maximal angle may be of much
help for more general subspaces. For example, if M and N
are subspaces
of unequal dimension that have a nontrivial intersection, then θmin = 0 and
θmax = π/2, but neither of these numbers might convey the desired information.
Consequently, it seems natural to try to formulate deﬁnitions of “intermediate”
angles between θmin and θmax. There are a host of such angles known as the
principal or canonical angles, and they are derived as follows.

456
Chapter 5
Norms, Inner Products, and Orthogonality
Let k = min{dim M, dim N}, and set M1 = M, N1 = N, and θ1 = θmin.
Let u1 and v1 be vectors of unit 2-norm such that the following maximum is
attained when u = u1 and v = v1 :
cos θmin =
max
u∈M, v∈N
∥u∥2=∥v∥2=1
vT u = vT
1 u1.
Set
M2 = u⊥
1 ∩M1
and
N2 = v⊥
1 ∩N1,
and deﬁne the second principal angle θ2 to be the minimal angle between M2
and N2. Continue in this manner—e.g., if u2 and v2 are vectors such that
∥u2∥2 = 1 = ∥v2∥2 and
cos θ2 =
max
u∈M2, v∈N2
∥u∥2=∥v∥2=1
vT u = vT
2 u2,
set
M3 = u⊥
2 ∩M2
and
N3 = v⊥
2 ∩N2,
and deﬁne the third principal angle θ3 to be the minimal angle between M3
and N3. This process is repeated k times, at which point one of the subspaces
is zero. Below is a summary.
Principal Angles
For nonzero subspaces M, N ⊆ℜn with k = min{dim M, dim N},
the principal angles between M = M1 and N = N1 are recursively
deﬁned to be the numbers 0 ≤θi ≤π/2 such that
cos θi =
max
u∈Mi, v∈Ni
∥u∥2=∥v∥2=1
vT u = vT
i ui,
i = 1, 2, . . . , k,
where ∥ui∥2 = 1 = ∥vi∥2 , Mi = u⊥
i−1∩Mi−1, and Ni = v⊥
i−1∩Ni−1.
•
It’s possible to prove that θmin = θ1 ≤θ2 ≤· · · ≤θk ≤θmax, where
θk = θmax when dim M = dim N.
•
The vectors ui and vi are not uniquely deﬁned, but the θi ’s are
unique. In fact, it can be proven that the sin θi ’s are singular values
(p. 412) for PM −PN . Furthermore, if dim M ≥dim N = k,
then the cos θi ’s are the singular values of VT
2 U1, and the sin θi ’s
are the singular values of VT
2 U2UT
2 , where U =

U1 | U2

and
V =

V1 | V2

are the orthogonal matrices from p. 451.

5.15 Angles between Subspaces
457
Exercises for section 5.15
5.15.1. Determine the angles θmin and θmax between the following subspaces
of ℜ3.
(a)
M = xy-plane,
N = span {(1, 0, 0), (0, 1, 1)} .
(b)
M = xy-plane,
N = span {(0, 1, 1)} .
5.15.2. Determine the principal angles between the following subspaces of ℜ3.
(a)
M = xy-plane,
N = span {(1, 0, 0), (0, 1, 1)} .
(b)
M = xy-plane,
N = span {(0, 1, 1)} .
5.15.3. Let θmin be the minimal angle between nonzero subspaces M, N ⊆ℜn.
(a)
Explain why θmax = 0 if and only if M = N.
(b)
Explain why θmin = 0 if and only if M ∩N ̸= 0.
(c)
Explain why θmin = π/2 if and only if M ⊥N.
5.15.4. Let θmin be the minimal angle between nonzero subspaces M, N ⊂ℜn,
and let θ⊥
min denote the minimal angle between M⊥and N ⊥. Prove
that if M ⊕N = ℜn, then θmin = θ⊥
min.
5.15.5. For nonzero subspaces M, N ⊂ℜn, let ˜θmin denote the minimal angle
between M and N ⊥, and let θmax be the maximal angle between M
and N. Prove that if M ⊕N ⊥= ℜn, then cos ˜θmin = sin θmax.
5.15.6. For subspaces M, N ⊆ℜn, prove that PM −PN is nonsingular if and
only if M and N are complementary.
5.15.7. For complementary spaces M, N ⊂ℜn, let P = PMN be the oblique
projector onto M along N, and let Q = PM⊥N ⊥be the oblique
projector onto M⊥along N ⊥.
(a)
Prove that (PM −PN )−1 = P −Q.
(b)
If θmin is the minimal angle between M and N, explain why
sin θmin =
1
∥P −Q∥2
.
(c)
Explain why ∥P −Q∥2 = ∥P∥2 .

458
Chapter 5
Norms, Inner Products, and Orthogonality
5.15.8. Prove that if f : V →ℜis a function deﬁned on a space V such that
f(αx) = αf(x) for scalars α ≥0, then
max
∥x∥=1 f(x) = max
∥x∥≤1 f(x).
5.15.9. Let M and N be nonzero complementary subspaces of ℜn.
(a)
Explain why PMN =

(I −PN )PM
†, where PM and PN
are the orthogonal projectors onto M and N, respectively,
and PMN is the oblique projector onto M along N.
(b)
If θmin is the minimal angle between M and N, explain why
sin θmin =


(I −PN )PM
†
−1
2
=


PM(I −PN )
†
−1
2
=


(I −PM)PN
†
−1
2
=


PN (I −PM)
†
−1
2
.
5.15.10. For complementary subspaces M, N ⊂ℜn, let θmin be the minimal
angle between M and N, and let ¯θmin denote the minimal angle be-
tween M and N ⊥.
(a)
If PMN is the oblique projector onto M along N, prove that
cos ¯θmin =
P†
MN

2 .
(b)
Explain why sin θmin ≤cos ¯θmin.
5.15.11. Let U =

U1 | U2

and V =

V1 | V2

be the orthogonal matrices
deﬁned on p. 451.
(a)
Prove that if UT
2 V2 is nonsingular, then
1
(UT
2 V2)−12
2
= 1 −
UT
2 V1
2
2 .
(b)
Prove that if UT
2 V1 is nonsingular, then
UT
2 V2
2
2 = 1 −
1
(UT
2 V1)−12
2
.

CHAPTER 6
Determinants
6.1
DETERMINANTS
At the beginning of this text, reference was made to the ancient Chinese counting
board on which colored bamboo rods were manipulated according to prescribed
“rules of thumb” in order to solve a system of linear equations. The Chinese
counting board is believed to date back to at least 200 B.C., and it was used
more or less in the same way for a millennium. The counting board and the “rules
of thumb” eventually found their way to Japan where Seki Kowa (1642–1708),
a great Japanese mathematician, synthesized the ancient Chinese ideas of array
manipulation. Kowa formulated the concept of what we now call the determinant
to facilitate solving linear systems—his deﬁnition is thought to have been made
some time before 1683.
About the same time—somewhere between 1678 and 1693—Gottfried W.
Leibniz (1646–1716), a German mathematician, was independently developing
his own concept of the determinant together with applications of array manipu-
lation to solve systems of linear equations. It appears that Leibniz’s early work
dealt with only three equations in three unknowns, whereas Seki Kowa gave a
general treatment for n equations in n unknowns. It seems that Kowa and
Leibniz both developed what later became known as Cramer’s rule (p. 476), but
not in the same form or notation. These men had something else in common—
their ideas concerning the solution of linear systems were never adopted by the
mathematical community of their time, and their discoveries quickly faded into
oblivion.
Eventually the determinant was rediscovered, and much was written on the
subject between 1750 and 1900. During this era, determinants became the ma-
jor tool used to analyze and solve linear systems, while the theory of matrices
remained relatively undeveloped. But mathematics, like a river, is everchanging

460
Chapter 6
Determinants
in its course, and major branches can dry up to become minor tributaries while
small trickling brooks can develop into raging torrents. This is precisely what
occurred with determinants and matrices. The study and use of determinants
eventually gave way to Cayley’s matrix algebra, and today matrix and linear
algebra are in the main stream of applied mathematics, while the role of deter-
minants has been relegated to a minor backwater position. Nevertheless, it is still
important to understand what a determinant is and to learn a few of its funda-
mental properties. Our goal is not to study determinants for their own sake, but
rather to explore those properties that are useful in the further development of
matrix theory and its applications. Accordingly, many secondary properties are
omitted or conﬁned to the exercises, and the details in proofs will be kept to a
minimum.
Over the years there have evolved various “slick” ways to deﬁne the determi-
nant, but each of these “slick” approaches seems to require at least one “sticky”
theorem in order to make the theory sound. We are going to opt for expedience
over elegance and proceed with the classical treatment.
A permutation p = (p1, p2, . . . , pn) of the numbers (1, 2, . . . , n) is simply
any rearrangement. For example, the set
{(1, 2, 3)
(1, 3, 2)
(2, 1, 3)
(2, 3, 1)
(3, 1, 2)
(3, 2, 1)}
contains the six distinct permutations of (1, 2, 3). In general, the sequence
(1, 2, . . . , n) has n! = n(n −1)(n −2) · · · 1 diﬀerent permutations. Given a per-
mutation, consider the problem of restoring it to natural order by a sequence
of pairwise interchanges. For example, (1, 4, 3, 2) can be restored to natural or-
der with a single interchange of 2 and 4 or, as indicated in Figure 6.1.1, three
adjacent interchanges can be used.
( 1,  2,  3,  4 )
( 1,  4,  3  2)
( 1,  2,  3,  4 )
( 1,  4,  3,  2 )
( 1,  4,  2,  3 )
( 1,  2,  4,  3 )
Figure 6.1.1
The important thing here is that both 1 and 3 are odd. Try to restore
(1, 4, 3, 2) to natural order by using an even number of interchanges, and you
will discover that it is impossible. This is due to the following general rule that is
stated without proof. The parity of a permutation is unique—i.e., if a permuta-
tion p can be restored to natural order by an even (odd) number of interchanges,
then every other sequence of interchanges that restores p to natural order must

6.1 Determinants
461
also be even (odd). Accordingly, the sign of a permutation p is deﬁned to be
the number
σ(p) =







+1
if p can be restored to natural order by an
even number of interchanges,
−1
if p can be restored to natural order by an
odd number of interchanges.
For example, if p = (1, 4, 3, 2), then σ(p) = −1, and if p = (4, 3, 2, 1), then
σ(p) = +1. The sign of the natural order p = (1, 2, 3, 4) is naturally σ(p) = +1.
The general deﬁnition of the determinant can now be given.
Deﬁnition of Determinant
For an n × n matrix A = [aij], the determinant of A is deﬁned to
be the scalar
det (A) =

p
σ(p)a1p1a2p2 · · · anpn,
(6.1.1)
where the sum is taken over the n! permutations p = (p1, p2, . . . , pn)
of (1, 2, . . . , n). Observe that each term a1p1a2p2 · · · anpn in (6.1.1) con-
tains exactly one entry from each row and each column of A. The de-
terminant of A can be denoted by det (A) or |A|, whichever is more
convenient.
Note: The determinant of a nonsquare matrix is not deﬁned.
For example, when A is 2 × 2 there are 2! = 2 permutations of (1,2),
namely, {(1, 2)
(2, 1)}, so det (A) contains the two terms
σ(1, 2)a11a22
and
σ(2, 1)a12a21.
Since σ(1, 2) = +1 and σ(2, 1) = −1, we obtain the familiar formula

a11
a12
a21
a22
 = a11a22 −a12a21.
(6.1.2)
Example 6.1.1
Problem: Use the deﬁnition to compute det (A), where A =
 1
2
3
4
5
6
7
8
9
	
.
Solution: The 3! = 6 permutations of (1, 2, 3) together with the terms in the
expansion of det (A) are shown in Table 6.1.1.

462
Chapter 6
Determinants
Table 6.1.1
p = (p1, p2, p3)
σ(p)
a1p1a2p2a3p3
(1, 2, 3)
+
1 × 5 × 9 = 45
(1, 3, 2)
−
1 × 6 × 8 = 48
(2, 1, 3)
−
2 × 4 × 9 = 72
(2, 3, 1)
+
2 × 6 × 7 = 84
(3, 1, 2)
+
3 × 4 × 8 = 96
(3, 2, 1)
−
3 × 5 × 7 = 105
Therefore,
det (A) =

p
σ(p)a1p1a2p2a3p3 = 45 −48 −72 + 84 + 96 −105 = 0.
Perhaps you have seen rules for computing 3 × 3 determinants that involve
running up, down, and around various diagonal lines. These rules do not easily
generalize to matrices of order greater than three, and in case you have forgotten
(or never knew) them, do not worry about it. Remember the 2 × 2 rule given
in (6.1.2) as well as the following statement concerning triangular matrices and
let it go at that.
Triangular Determinants
The determinant of a triangular matrix is the product of its diagonal
entries. In other words,

t11
t12
· · ·
t1n
0
t22
· · ·
t2n
...
...
...
...
0
0
· · ·
tnn

= t11t22 · · · tnn.
(6.1.3)
Proof.
Recall from the deﬁnition (6.1.1) that each term t1p1t2p2 · · · tnpn con-
tains exactly one entry from each row and each column. This means that there
is only one term in the expansion of the determinant that does not contain an
entry below the diagonal, and this term is t11t22 · · · tnn.

6.1 Determinants
463
Transposition Doesn’t Alter Determinants
•
det

AT 
= det (A) for all n × n matrices.
(6.1.4)
Proof.
As p = (p1, p2, . . . , pn) varies over all permutations of (1, 2, . . . , n), the
set of all products {σ(p)a1p1a2p2 · · · anpn} is the same as the set of all products
{σ(p)ap11ap22 · · · apnn} . Explicitly construct both of these sets for n = 3 to
convince yourself.
Equation (6.1.4) insures that it’s not necessary to distinguish between rows
and columns when discussing properties of determinants, so theorems concern-
ing determinants that involve row manipulations will remain true when the word
“row” is replaced by “column.” For example, it’s essential to know how elemen-
tary row and column operations alter the determinant of a matrix, but, by virtue
of (6.1.4), it suﬃces to limit the discussion to elementary row operations.
Effects of Row Operations
Let B be the matrix obtained from An×n by one of the three elemen-
tary row operations:
Type I:
Interchange rows i and j.
Type II:
Multiply row i by α ̸= 0.
Type III:
Add α times row i to row j.
The value of det (B) is as follows:
•
det (B) = −det (A) for Type I operations.
(6.1.5)
•
det (B) = α det (A) for Type II operations.
(6.1.6)
•
det (B) = det (A) for Type III operations.
(6.1.7)
Proof of (6.1.5).
If B agrees with A except that Bi∗= Aj∗and Bj∗= Ai∗,
then for each permutation p = (p1, p2, . . . , pn) of (1, 2, . . . , n),
b1p1 · · · bipi · · · bjpj · · · bnpn = a1p1 · · · ajpi · · · aipj · · · anpn
= a1p1 · · · aipj · · · ajpi · · · anpn.
Furthermore, σ(p1, . . . , pi, . . . , pj, . . . , pn) = −σ(p1, . . . , pj, . . . , pi, . . . , pn) be-
cause the two permutations diﬀer only by one interchange. Consequently, deﬁni-
tion (6.1.1) of the determinant guarantees that det (B) = −det (A).

464
Chapter 6
Determinants
Proof of (6.1.6).
If B agrees with A except that Bi∗= αAi∗, then for each
permutation p = (p1, p2, . . . , pn),
b1p1 · · · bipi · · · bnpn = a1p1 · · · αaipi · · · anpn = α(a1p1 · · · aipi · · · anpn),
and therefore the expansion (6.1.1) yields det (B) = α det (A).
Proof of (6.1.7).
If B agrees with A except that Bj∗= Aj∗+ αAi∗, then
for each permutation p = (p1, p2, . . . , pn),
b1p1 · · · bipi · · · bjpj · · · bnpn = a1p1 · · · aipi · · · (ajpj + αaipj) · · · anpn
= a1p1 · · · aipi · · · ajpj · · · anpn + α(a1p1 · · · aipi · · · aipj · · · anpn),
so that
det (B) =

p
σ(p)a1p1 · · · aipi · · · ajpj · · · anpn
+α

p
σ(p)a1p1 · · · aipi · · · aipj · · · anpn.
(6.1.8)
The ﬁrst sum on the right-hand side of (6.1.8) is det (A), while the second sum is
the expansion of the determinant of a matrix ˜A in which the ith and jth rows
are identical. For such a matrix, det( ˜A) = 0 because (6.1.5) says that the sign
of the determinant is reversed whenever the ith and jth rows are interchanged,
so det( ˜A) = −det( ˜A). Consequently, the second sum on the right-hand side of
(6.1.8) is zero, and thus det (B) = det (A).
It is now possible to evaluate the determinant of an elementary matrix as-
sociated with any of the three types of elementary operations. Let E, F, and
G be elementary matrices of Types I, II, and III, respectively, and recall from
the discussion in §3.9 that each of these elementary matrices can be obtained by
performing the associated row (or column) operation to an identity matrix of ap-
propriate size. The result concerning triangular determinants (6.1.3) guarantees
that det (I) = 1 regardless of the size of I, so if E is obtained by interchanging
any two rows (or columns) in I, then (6.1.5) insures that
det (E) = −det (I) = −1.
(6.1.9)
Similarly, if F is obtained by multiplying any row (or column) in I by α ̸= 0,
then (6.1.6) implies that
det (F) = α det (I) = α,
(6.1.10)
and if G is the result of adding a multiple of one row (or column) in I to
another row (or column) in I, then (6.1.7) guarantees that
det (G) = det (I) = 1.
(6.1.11)

6.1 Determinants
465
In particular, (6.1.9)–(6.1.11) guarantee that the determinants of elementary
matrices of Types I, II, and III are nonzero.
As discussed in §3.9, if P is an elementary matrix of Type I, II, or III,
and if A is any other matrix, then the product PA is the matrix obtained by
performing the elementary operation associated with P to the rows of A. This,
together with the observations (6.1.5)–(6.1.7) and (6.1.9)–(6.1.11), leads to the
conclusion that for every square matrix A,
det (EA)
= −det (A) = det (E)det (A),
det (FA)
= α det (A) = det (F)det (A),
det (GA) =
det (A)
= det (G)det (A).
In other words, det (PA) = det (P)det (A) whenever P is an elementary matrix
of Type I, II, or III. It’s easy to extend this observation to any number of these
elementary matrices, P1, P2, . . . , Pk, by writing
det (P1P2 · · · PkA) = det (P1)det (P2 · · · PkA)
= det (P1)det (P2)det (P3 · · · PkA)
...
= det (P1)det (P2) · · · det (Pk)det (A).
(6.1.12)
This leads to a characterization of invertibility in terms of determinants.
Invertibility and Determinants
•
An×n is nonsingular if and only if det (A) ̸= 0
(6.1.13)
or, equivalently,
•
An×n is singular if and only if det (A) = 0.
(6.1.14)
Proof.
Let P1, P2, . . . , Pk be a sequence of elementary matrices of Type I, II,
or III such that P1P2 · · · PkA = EA, and apply (6.1.12) to conclude
det (P1)det (P2) · · · det (Pk)det (A) = det (EA).
Since elementary matrices have nonzero determinants,
det (A) ̸= 0 ⇐⇒det (EA) ̸= 0 ⇐⇒there are no zero pivots
⇐⇒every column in EA (and in A) is basic
⇐⇒A is nonsingular.

466
Chapter 6
Determinants
Example 6.1.2
Caution! Small Determinants
/
⇐⇒Near Singularity. Because of (6.1.13)
and (6.1.14), it might be easy to get the idea that det (A) is somehow a measure
of how close A is to being singular, but this is not necessarily the case. Nearly
singular matrices need not have determinants of small magnitude. For example,
An =
 n
0
0
1/n

is nearly singular when n is large, but det (An) = 1 for all
n. Furthermore, small determinants do not necessarily signal nearly singular
matrices. For example,
An =




.1
0
· · ·
0
0
.1
· · ·
0
...
...
...
...
0
0
· · ·
.1




n×n
is not close to any singular matrix—see (5.12.10) on p. 417—but det (An) =
(.1)n is extremely small for large n.
A minor determinant (or simply a minor) of Am×n is deﬁned to be the
determinant of any k × k submatrix of A. For example,

1
2
4
5
 = −3 and

2
3
8
9
 = −6 are 2 × 2 minors of A =


1
2
3
4
5
6
7
8
9

.
An individual entry of A can be regarded as a 1 × 1 minor, and det (A) itself
is considered to be a 3 × 3 minor of A.
We already know that the rank of any matrix A is the size of the largest
nonsingular submatrix in A (p. 215). But (6.1.13) guarantees that the nonsingu-
lar submatrices of A are simply those submatrices with nonzero determinants,
so we have the following characterization of rank.
Rank and Determinants
•
rank (A) = the size of the largest nonzero minor of A.
Example 6.1.3
Problem: Use determinants to compute the rank of A =
 1
2
3
1
4
5
6
1
7
8
9
1
	
.
Solution: Clearly, there are 1 × 1 and 2 × 2 minors that are nonzero, so
rank (A) ≥2. In order to decide if the rank is three, we must see if there

6.1 Determinants
467
are any 3 × 3 nonzero minors. There are exactly four 3 × 3 minors, and they
are

1
2
3
4
5
6
7
8
9

= 0,

1
2
1
4
5
1
7
8
1

= 0,

1
3
1
4
6
1
7
9
1

= 0,

2
3
1
5
6
1
8
9
1

= 0.
Since all 3 × 3 minors are 0, we conclude that rank (A) = 2. You should be
able to see from this example that using determinants is generally not a good
way to compute the rank of a matrix.
In (6.1.12) we observed that the determinant of a product of elementary
matrices is the product of their respective determinants. We are now in a position
to extend this observation.
Product Rules
•
det (AB) = det (A)det (B) for all n × n matrices.
(6.1.15)
•
det

A
B
0
D
	
= det (A)det (D) if A and D are square.
(6.1.16)
Proof of (6.1.15).
If A is singular, then AB is also singular because (4.5.2)
says that rank (AB) ≤rank (A). Consequently, (6.1.14) implies that
det (AB) = 0 = det (A)det (B),
so (6.1.15) is trivially true when A is singular. If A is nonsingular, then A
can be written as a product of elementary matrices A = P1P2 · · · Pk that are
of Type I, II, or III—recall (3.9.3). Therefore, (6.1.12) can be applied to produce
det (AB) = det (P1P2 · · · PkB) = det (P1)det (P2) · · · det (Pk)det (B)
= det (P1P2 · · · Pk) det (B) = det (A)det (B).
Proof of (6.1.16).
First consider the special case X =
 Ar×r
0
0
I

, and use the
deﬁnition to write det (X) = 
σ(p) x1j1x2j2 · · · xrjrxr+1,jr+1 · · · xn,jn. But
xrjrxr+1,jr+1 · · · xn,jn =



1
when p =

1
· · ·
r
r + 1
· · ·
n
j1
· · ·
jr
r + 1
· · ·
n
	
,
0
for all other permutations,
so, if pr denotes permutations of only the ﬁrst r positive integers, then
det (X) =

σ(p)
x1j1x2j2 · · · xrjrxr+1,jr+1 · · · xn,jn =

σ(pr)
x1j1x2j2 · · · xrjr = det (A).

468
Chapter 6
Determinants
Thus
 A
0
0
I
 = det (A). Similarly,
 I
0
0
D
 = det (D), so, by (6.1.15),

A
0
0
D
 = det

A
0
0
I
	 
I
0
0
D
	
=

A
0
0
I


I
0
0
D
 = det (A)det (D).
If A = QARA and D = QDRD are the respective QR factorizations (p. 345) of
A and D, then
 A
B
0
D

=
 QA
0
0
QD
 RA
QT
AB
0
RD

is also a QR factorization.
By (6.1.3), the determinant of a triangular matrix is the product of its diagonal
entries, and this together with the previous results yield

A
B
0
D
 =

QA
0
0
QD


RA
QT
AB
0
RD
 = det (QA)det (QD)det (RA)det (RD)
= det (QARA)det (QDRD) = det (A)det (D).
Example 6.1.4
Volume and Determinants. The deﬁnition of a determinant is purely al-
gebraic, but there is a concrete geometrical interpretation. A solid in ℜm with
parallel opposing faces whose adjacent sides are deﬁned by vectors from a linearly
independent set {x1, x2, . . . , xn} is called an n-dimensional parallelepiped. As
depicted in Figure 6.1.2, a two-dimensional parallelepiped is a parallelogram, and
a three-dimensional parallelepiped is a skewed rectangular box.
x1
x2
x1
x2
x3
Figure 6.1.2
Problem: When A ∈ℜm×n has linearly independent columns, explain why
the volume of the n-dimensional parallelepiped generated by the columns of A
is Vn =

det

AT A
1/2. In particular, if A is square, then Vn = |det (A)|.
Solution: Recall from Example 5.13.2 on p. 431 that if Am×n = Qm×nRn×n is
the (rectangular) QR factorization of A, then the volume of the n-dimensional
parallelepiped generated by the columns of A is Vn = ν1ν2 · · · νn = det (R),
where the νk ’s are the diagonal elements of the upper-triangular matrix R. Use

6.1 Determinants
469
QT Q = I together with the product rule (6.1.15) and the fact that transposition
doesn’t aﬀect determinants (6.1.4) to write
det

AT A

= det

RT QT QR

= det

RT R

= det

RT 
det (R)
= (det (R))2 = (ν1ν2 · · · νn)2 = V 2
n .
(6.1.17)
If A is square, det

AT A

= det

AT 
det (A) = (det (A))2, so Vn = |det (A)|.
Hadamard’s Inequality: Recall from (5.13.7) that if
A =

x1 | x2 | · · · | xn

n×n
and
Aj =

x1 | x2 | · · · | xj

n×j,
then ν1 = ∥x1∥2 and νk = ∥(I −Pk)xk∥2 (the projected height of xk ) for
k > 1, where Pk is the orthogonal projector onto R (Ak−1). But
ν2
k = ∥(I −Pk)xk∥2
2 ≤∥(I −Pk)∥2
2 ∥xk∥2
2 = ∥xk∥2
2
(recall (5.13.10)),
so, by (6.1.17), det

AT A

≤∥x1∥2
2 ∥x2∥2
2 · · · ∥xn∥2
2 or, equivalently,
|det (A)| ≤
n

k=1
∥xk∥2 =
n

j=1
 n

i=1
|aij|2
1/2
,
(6.1.18)
with equality holding if and only if the xk ’s are mutually orthogonal. This
is Hadamard’s inequality.
64 In light of the preceding discussion, it simply
asserts that the volume of the parallelepiped P generated by the columns of A
can’t exceed the volume of a rectangular box whose sides have length ∥xk∥2 , a
fact that is geometrically evident because P is a skewed rectangular box with
sides of length ∥xk∥2 .
The product rule (6.1.15) provides a practical way to compute determinants.
Recall from §3.10 that for every nonsingular matrix A, there is a permutation
matrix P (which is a product of elementary interchange matrices) such that
PA = LU in which L is lower triangular with 1’s on its diagonal, and U is
upper triangular with the pivots on its diagonal. The product rule guarantees
64
Jacques Hadamard (1865–1963), a leading French mathematician of the ﬁrst half of the twenti-
eth century, discovered this inequality in 1893. Inﬂuenced in part by the tragic death of his sons
in World War I, Hadamard became a peace activist whose politics drifted far left to the extent
that the United States was reluctant to allow him to enter the country to attend the Interna-
tional Congress of Mathematicians held in Cambridge, Massachusetts, in 1950. Due to support
from inﬂuential mathematicians, Hadamard was made honorary president of the congress, and
the resulting visibility together with pressure from important U.S. scientists forced oﬃcials to
allow him to attend.

470
Chapter 6
Determinants
that det (P)det (A) = det (L)det (U), and we know from (6.1.9) that if E is an
elementary interchange matrix, then det (E) = −1, so
det (P) =
 +1
if P is the product of an even number of interchanges,
−1
if P is the product of an odd number of interchanges.
The result concerning triangular determinants (6.1.3) shows that det (L) = 1
and det (U) = u11u22 · · · unn, where the uii ’s are the pivots, so, putting these
observations together yields det (A) = ±u11u22 · · · unn, where the sign depends
on the number of row interchanges used. Below is a summary.
Computing a Determinant
If PAn×n = LU is an LU factorization obtained with row interchanges
(use partial pivoting for numerical stability), then
det (A) = σu11u22 · · · unn.
The uii ’s are the pivots, and σ is the sign of the permutation. That is,
σ =
 +1
if an even number of row interchanges are used,
−1
if an odd number of row interchanges are used.
If a zero pivot emerges that cannot be removed (because all entries below
the pivot are zero), then A is singular and det (A) = 0. Exercise 6.2.18
discusses orthogonal reduction to compute det (A).
Example 6.1.5
Problem: Use partial pivoting to determine an LU decomposition PA = LU,
and then evaluate the determinant of A =


1
2
−3
4
4
8
12
−8
2
3
2
1
−3
−1
1
−4

.
Solution: The LU factors of A were computed in Example 3.10.4 as follows.
L=



1
0
0
0
−3/4
1
0
0
1/4
0
1
0
1/2
−1/5
1/3
1


, U=



4
8
12
−8
0
5
10
−10
0
0
−6
6
0
0
0
1


, P=



0
1
0
0
0
0
0
1
1
0
0
0
0
0
1
0


.
The only modiﬁcation needed is to keep track of how many row interchanges are
used. Reviewing Example 3.10.4 reveals that the pivoting process required three
interchanges, so σ = −1, and hence det (A) = (−1)(4)(5)(−6)(1) = 120.
It’s sometimes necessary to compute the derivative of a determinant whose
entries are diﬀerentiable functions. The following formula shows how this is done.

6.1 Determinants
471
Derivative of a Determinant
If the entries in An×n = [aij(t)] are diﬀerentiable functions of t, then
d

det (A)

dt
= det (D1) + det (D2) + · · · + det (Dn),
(6.1.19)
where Di is identical to A except that the entries in the ith row are
replaced by their derivatives—i.e., [Di]k∗=
 Ak∗
if i ̸= k,
d Ak∗/dt
if i = k.
Proof.
This follows directly from the deﬁnition of a determinant by writing
d

det (A)

dt
= d
dt

p
σ(p)a1p1a2p2 · · · anpn =

p
σ(p)d

a1p1a2p2 · · · anpn

dt
=

p
σ(p)

a′
1p1a2p2 · · · anpn + a1p1a′
2p2 · · · anpn + · · · + a1p1a2p2 · · · a′
npn

=

p
σ(p)a′
1p1a2p2 · · · anpn +

p
σ(p)a1p1a′
2p2 · · · anpn
+ · · · +

p
σ(p)a1p1a2p2 · · · a′
npn
= det (D1) + det (D2) + · · · + det (Dn).
Example 6.1.6
Problem: Evaluate the derivative d

det (A)

/dt for A =
 et
e−t
cos t
sin t

.
Solution: Applying formula (6.1.19) yields
d

det (A)

dt
=

et
−e−t
cos t
sin t
 +

et
e−t
−sin t
cos t
 =

et + e−t
(cos t + sin t) .
Check this by ﬁrst expanding det (A) and then computing the derivative.

472
Chapter 6
Determinants
Exercises for section 6.1
6.1.1. Use the deﬁnition to evaluate det (A) for each of the following matrices.
(a)
A =


3
−2
1
−5
4
0
2
1
6

.
(b)
A =


2
1
1
6
2
1
−2
2
1

.
(c)
A =


0
0
α
0
β
0
γ
0
0

.
(d)
A =


a11
a12
a13
a21
a22
a23
a31
a32
a33

.
6.1.2. What is the volume of the parallelepiped generated by the three vectors
x1 = (3, 0, −4, 0)T , x2 = (0, 2, 0, −2)T , and x3 = (0, 1, 0, 1)T ?
6.1.3. Using Gaussian elimination to reduce A to an upper-triangular matrix,
evaluate det (A) for each of the following matrices.
(a) A =


1
2
3
2
4
1
1
4
4

.
(b) A =


1
3
5
−1
4
2
3
−2
4

.
(c) A =



1
2
−3
4
4
8
12
−8
2
3
2
1
−3
−1
1
−4


.
(d) A =



0
0
−2
3
1
0
1
2
−1
1
2
1
0
2
−3
0


.
(e) A =





2
−1
0
0
0
−1
2
−1
0
0
0
−1
2
−1
0
0
0
−1
2
−1
0
0
0
−1
1




. (f) A =






1
1
1
· · ·
1
1
2
1
· · ·
1
1
1
3
· · ·
1
...
...
...
...
...
1
1
1
· · ·
n






.
6.1.4. Use determinants to compute the rank of A =



1
3
−2
0
1
2
−1
−1
6
2
5
−6


.
6.1.5. Use determinants to ﬁnd the values of α for which the following system
possesses a unique solution.


1
α
0
0
1
−1
α
0
1




x1
x2
x3

=


−3
4
7

.

6.1 Determinants
473
6.1.6. If A is nonsingular, explain why det

A−1
= 1/det (A).
6.1.7. Explain why determinants are invariant under similarity transforma-
tions. That is, show det

P−1AP

= det (A) for all nonsingular P.
6.1.8. Explain why det (A∗) = det (A).
6.1.9.
(a)
Explain why |det (Q)| = 1 when Q is unitary. In particular,
det (Q) = ±1 if Q is an orthogonal matrix.
(b)
How are the singular values of A ∈Cn×n related to det (A)?
6.1.10. Prove that if A is m × n, then det (A∗A) ≥0, and explain why
det (A∗A) > 0 if and only if rank (A) = n.
6.1.11. If A is n × n, explain why det (αA) = αndet (A) for all scalars α.
6.1.12. If A is an n × n skew-symmetric matrix, prove that A is singular
whenever n is odd. Hint: Use Exercise 6.1.11.
6.1.13. How can you build random integer matrices with det (A) = 1?
6.1.14. If the kth row of An×n is written as a sum Ak∗= xT + yT + · · · + zT ,
where xT, yT, . . . , zT are row vectors, explain why
det (A) = det







A1∗
...
xT
...
An∗







+ det







A1∗
...
yT
...
An∗







+ · · · + det







A1∗
...
zT
...
An∗







.
6.1.15. The CBS inequality (p. 272) says that |x∗y| ≤∥x∥2
2 ∥y∥2
2 for vectors
x, y ∈Cn×1. Use Exercise 6.1.10 to give an alternate proof of the CBS
inequality along with an alternate explanation of why equality holds if
and only if y is a scalar multiple of x.

474
Chapter 6
Determinants
6.1.16. Determinant Formula for Pivots. Let Ak be the k × k leading
principal submatrix of An×n (p. 148). Prove that if A has an LU
factorization A = LU, then det (Ak) = u11u22 · · · ukk, and deduce
that the kth pivot is ukk =
 det (A1) = a11
for k = 1,
det (Ak)/det (Ak−1)
for k = 2, 3, . . . , n.
6.1.17. Prove that if rank (Am×n) = n, then AT A has an LU factorization
with positive pivots—i.e., AT A is positive deﬁnite (pp. 154 and 559).
6.1.18. Let A(x) =


2 −x
3
4
0
4 −x
−5
1
−1
3 −x

.
(a)
First evaluate det (A), and then compute d

det (A)

/dx.
(b)
Use formula (6.1.19) to evaluate d

det (A)

/dx.
6.1.19. When the entries of A = [aij(x)] are diﬀerentiable functions of x,
we deﬁne d A/dx = [d aij/dx] (the matrix of derivatives). For square
matrices, is it always the case that d

det (A)

/dx = det (dA/dx)?
6.1.20. For a set of functions S = {f1(x), f2(x), . . . , fn(x)} that are n−1 times
diﬀerentiable, the determinant
w(x) =

f1(x)
f2(x)
· · ·
fn(x)
f ′
1(x)
f ′
2(x)
· · ·
f ′
n(x)
...
...
...
...
f (n−1)
1
(x)
f (n−1)
2
(x)
· · ·
f (n−1)
n
(x)

is called the Wronskian of S. If S is a linearly dependent set, explain
why w(x) = 0 for every value of x. Hint: Recall Example 4.3.6 (p. 189).
6.1.21. Consider evaluating an n × n determinant from the deﬁnition (6.1.1).
(a)
How many multiplications are required?
(b)
Assuming a computer will do 1,000,000 multiplications per sec-
ond, and neglecting all other operations, what is the largest
order determinant that can be evaluated in one hour?
(c)
Under the same conditions of part (b), how long will it take to
evaluate the determinant of a 100 × 100 matrix?
Hint: 100! ≈9.33 × 10157.
(d)
If all other operations are neglected, how many multiplications
per second must a computer perform if the task of evaluating
the determinant of a 100 × 100 matrix is to be completed in
100 years?

6.2 Additional Properties of Determinants
475
6.2
ADDITIONAL PROPERTIES OF DETERMINANTS
The purpose of this section is to present some additional properties of determi-
nants that will be helpful in later developments.
Block Determinants
If A and D are square matrices, then
det

A
B
C
D
	
=

det (A)det

D −CA−1B

when A−1 exists,
det (D)det

A −BD−1C

when D−1 exists.
(6.2.1)
The matrices D −CA−1B and A −BD−1C are called the Schur
complements of A and D, respectively—see Exercise 3.7.11 on p. 123.
Proof.
If A−1 exists, then
 A
B
C
D

=

I
0
CA−1
I
  A
B
0
D −CA−1B

, and
the product rules (p. 467) produce the ﬁrst formula in (6.2.1). The second formula
follows by using a similar trick.
Since the determinant of a product is equal to the product of the deter-
minants, it’s only natural to inquire if a similar result holds for sums. In other
words, is det (A + B) = det (A)+det (B)? Almost never! Try a couple of exam-
ples to convince yourself. Nevertheless, there are still some statements that can
be made regarding the determinant of certain types of sums. In a loose sense, the
result of Exercise
6.1.14 was a statement concerning determinants and sums,
but the following result is a little more satisfying.
Rank-One Updates
If An×n is nonsingular, and if c and d are n × 1 columns, then
•
det

I + cdT 
= 1 + dT c,
(6.2.2)
•
det

A + cdT 
= det (A)

1 + dT A−1c

.
(6.2.3)
Exercise 6.2.7 presents a generalized version of these formulas.
Proof.
The proof of (6.2.2) follows by applying the product rules (p. 467) to

I
0
dT
1
	 
I + cdT
c
0
1
	 
I
0
−dT
1
	
=

I
c
0
1 + dT c
	
.
To prove (6.2.3), write A + cdT = A

I + A−1cdT 
, and apply the product
rule (6.1.15) along with (6.2.2).

476
Chapter 6
Determinants
Example 6.2.1
Problem: For A =




1 + λ1
1
· · ·
1
1
1 + λ2
· · ·
1
...
...
...
...
1
1
· · ·
1 + λn



, λi ̸= 0, ﬁnd det (A).
Solution: Express A as a rank-one updated matrix A = D + eeT , where
D = diag (λ1, λ2, . . . , λn) and eT = ( 1
1
· · ·
1 ) . Apply (6.2.3) to produce
det (D + eeT ) = det (D)

1 + eT D−1e

=
 n

i=1
λi
 
1 +
n

i=1
1
λi

.
The classical result known as Cramer’s rule
65 is a corollary of the rank-one
update formula (6.2.3).
Cramer’s Rule
In a nonsingular system An×nx = b, the ith unknown is
xi = det (Ai)
det (A) ,
where Ai =

A∗1
 · · ·
 A∗i−1
 b
 A∗i+1
 · · ·
 A∗n

. That is, Ai is
identical to A except that column A∗i has been replaced by b.
Proof.
Since Ai = A + (b −A∗i) eT
i , where ei is the ith unit vector, (6.2.3)
may be applied to yield
det (Ai) = det (A)

1 + eT
i A−1 (b −A∗i)

= det (A)

1 + eT
i (x −ei)

= det (A) (1 + xi −1) = det (A) xi.
Thus xi = det (Ai)/det (A) because A being nonsingular insures det (A) ̸= 0
by (6.1.13).
65
Gabriel Cramer (1704–1752) was a mathematician from Geneva, Switzerland. As mentioned
in §6.1, Cramer’s rule was apparently known to others long before Cramer rediscovered and
published it in 1750. Nevertheless, Cramer’s recognition is not undeserved because his work
was responsible for a revived interest in determinants and systems of linear equations. After
Cramer’s publication, Cramer’s rule met with instant success, and it quickly found its way
into the textbooks and classrooms of Europe. It is reported that there was a time when stu-
dents passed or failed the exams in the schools of public service in France according to their
understanding of Cramer’s rule.

6.2 Additional Properties of Determinants
477
Example 6.2.2
Problem: Determine the value of t for which x3(t) is minimized in


t
0
1/t
0
t
t2
1
t2
t3




x1(t)
x2(t)
x3(t)

=


1
1/t
1/t2

.
Solution: Only one component of the solution is required, so it’s wasted eﬀort
to solve the entire system. Use Cramer’s rule to obtain
x3(t) =

t
0
1
0
t
1/t
1
t2
1/t2


t
0
1/t
0
t
t2
1
t2
t3

= 1 −t −t2
−1
= t2 + t −1,
and set
d x3(t)
dt
= 0
to conclude that x3(t) is minimized at t = −1/2.
Recall that minor determinants of A are simply determinants of subma-
trices of A. We are now in a position to see that in an n × n matrix the
n −1 × n −1 minor determinants have a special signiﬁcance.
Cofactors
The cofactor of An×n associated with the (i, j)-position is deﬁned as
˚Aij = (−1)i+jMij,
where Mij is the n −1 × n −1 minor obtained by deleting the ith row
and jth column of A. The matrix of cofactors is denoted by ˚
A.
Example 6.2.3
Problem: For A =

1
−1
2
2
0
6
−3
9
1
	
, determine the cofactors ˚A21 and ˚A13.
Solution:
˚A21=(−1)2+1M21 = (−1)(−19)= 19
and
˚A13=(−1)1+3M13=(+1)(18) = 18.
The entire matrix of cofactors is ˚
A =
 −54
−20
18
19
7
−6
−6
−2
2
	
.

478
Chapter 6
Determinants
The cofactors of a square matrix A appear naturally in the expansion of
det (A). For example,

a11
a12
a13
a21
a22
a23
a31
a32
a33

= a11a22a33 + a12a23a31 + a13a21a32
−a11a23a32 −a12a21a33 −a13a22a31
= a11 (a22a33 −a23a32) + a12 (a23a31 −a21a33)
+ a13 (a21a32 −a22a31)
= a11˚A11 + a12˚A12 + a13˚A13.
(6.2.4)
Because this expansion is in terms of the entries of the ﬁrst row and the corre-
sponding cofactors, (6.2.4) is called the cofactor expansion of det (A) in terms
of the ﬁrst row. It should be clear that there is nothing special about the ﬁrst
row of A. That is, it’s just as easy to write an expression similar to (6.2.4) in
which entries from any other row or column appear. For example, the terms in
(6.2.4) can be rearranged to produce
det (A) = a12 (a23a31 −a21a33) + a22 (a11a33 −a13a31) + a32 (a13a21 −a11a23)
= a12˚A12 + a22˚A22 + a32˚A32.
This is called the cofactor expansion for det (A) in terms of the second column.
The 3 × 3 case is typical, and exactly the same reasoning can be applied to a
more general n × n matrix in order to obtain the following statements.
Cofactor Expansions
•
det (A) = ai1˚Ai1 + ai2˚Ai2 + · · · + ain˚Ain (about row i).
(6.2.5)
•
det (A) = a1j˚A1j +a2j˚A2j +· · ·+anj˚Anj (about column j). (6.2.6)
Example 6.2.4
Problem: Use cofactor expansions to evaluate det (A) for
A =



0
0
0
2
7
1
6
5
3
7
2
0
0
3
−1
4


.
Solution: To minimize the eﬀort, expand det (A) in terms of the row or column
that contains a maximal number of zeros. For this example, the expansion in
terms of the ﬁrst row is most eﬃcient because
det (A) = a11˚A11 + a12˚A12 + a13˚A13 + a14˚A14 = a14˚A14 = (2)(−1)

7
1
6
3
7
2
0
3
−1

.

6.2 Additional Properties of Determinants
479
Now expand this remaining 3 × 3 determinant either in terms of the ﬁrst column
or the third row. Using the ﬁrst column produces

7
1
6
3
7
2
0
3
−1

= (7)(+1)

7
2
3
−1
 + (3)(−1)

1
6
3
−1
 = −91 + 57 = −34,
so det (A) = (2)(−1)(−34) = 68. You may wish to try an expansion using
diﬀerent rows or columns, and verify that the ﬁnal result is the same.
In the previous example, we were able to take advantage of the fact that
there were zeros in convenient positions. However, for a general matrix An×n
with no zero entries, it’s not diﬃcult to verify that successive application of
cofactor expansions requires n!

1 + 1
2! + 1
3! + · · · +
1
(n−1)!

multiplications to
evaluate det (A). Even for moderate values of n, this number is too large for
the cofactor expansion to be practical for computational purposes. Neverthe-
less, cofactors can be useful for theoretical developments such as the following
determinant formula for A−1.
Determinant Formula for A
−1
The adjugate of An×n is deﬁned to be adj (A) = ˚
A
T , the transpose of
the matrix of cofactors—some older texts call this the adjoint matrix.
If A is nonsingular, then
A−1 =
˚
A
T
det (A) = adj (A)
det (A).
(6.2.7)
Proof.

A−1
ij is the ith component in the solution to Ax = ej, where ej
is the jth unit vector. By Cramer’s rule, this is

A−1
ij = xi = det (Ai)
det (A) ,
where Ai is identical to A except that the ith column has been replaced by
ej, and the cofactor expansion in terms of the ith column implies that
det (Ai) =
ith
↓

a11
· · ·
0
· · ·
a1n
...
· · ·
...
· · ·
...
aj1
· · ·
1
· · ·
ajn
...
· · ·
...
· · ·
...
an1
· · ·
0
· · ·
ann

= ˚Aji.

480
Chapter 6
Determinants
Example 6.2.5
Problem: Use determinants to compute

A−1
12 and

A−1
31 for the matrix
A =


1
−1
2
2
0
6
−3
9
1

.
Solution: The cofactors ˚A21 and ˚A13 were determined in Example 6.2.3 to be
˚A21 = 19 and ˚A13 = 18, and it’s straightforward to compute det (A) = 2, so

A−1
12 =
˚A21
det (A) = 19
2
and

A−1
31 =
˚A13
det (A) = 18
2 = 9.
Using the matrix of cofactors ˚
A computed in Example 6.2.3, we have that
A−1 = adj (A)
det (A) =
˚
A
T
det (A) = 1
2


−54
19
−6
−20
7
−2
18
−6
2

.
Example 6.2.6
Problem: For A =
 a
b
c
d

, determine a general formula for A−1.
Solution: adj (A) = ˚AT =

d
−b
−c
a

, and det (A) = ad −bc, so
A−1 = adj (A)
det (A) =
1
ad −bc

d
−b
−c
a
	
.
Example 6.2.7
Problem: Explain why the entries in A−1 vary continuously with the entries in
A when A is nonsingular. This is in direct contrast with the lack of continuity
exhibited by pseudoinverses (p. 423).
Solution: Recall from elementary calculus that the sum, the product, and the
quotient of continuous functions are each continuous functions. In particular,
the sum and the product of any set of numbers varies continuously as the num-
bers vary, so det (A) is a continuous function of the aij ’s. Since each entry in
adj (A) is a determinant, each quotient [A−1]ij = [adj (A)]ij/det (A) must be
a continuous function of the aij ’s.
The Moral: The formula A−1 = adj (A) /det (A) is nearly worthless for actu-
ally computing the value of A−1, but, as this example demonstrates, the formula
is nevertheless a useful mathematical tool. It’s not uncommon for applied ori-
ented students to fall into the trap of believing that the worth of a formula or
an idea is tied to its utility for computing something. This example makes the
point that things can have signiﬁcant mathematical value without being compu-
tationally important. In fact, most of this chapter is in this category.

6.2 Additional Properties of Determinants
481
Example 6.2.8
Problem: Explain why the inner product of one row (or column) in An×n with
the cofactors of a diﬀerent row (or column) in A must always be zero.
Solution: Let ˜A be the result of replacing the jth column in A by the kth
column of A. Since ˜A has two identical columns, det ( ˜A) = 0. Furthermore, the
cofactor associated with the (i, j)-position in ˜A is ˚Aij, the cofactor associated
with the (i, j) in A, so expansion of det ( ˜A) in terms of the jth column yields
0 = det ( ˜A) =
jth
↓
kth
↓

a11
· · ·
a1k
· · ·
a1k
· · ·
a1n
...
...
...
ai1
· · ·
aik
· · ·
aik
· · ·
ain
...
...
...
an1
· · ·
ank
· · ·
ank
· · ·
ann

=
n

i=1
aik˚Aij.
Thus the inner product of the kth column of An×n with the cofactors of the
jth column of A is zero. A similar result holds for rows. Combining these
observations with (6.2.5) and (6.2.6) produces
n

j=1
akj˚Aij =

det (A)
if k = i,
0
if k ̸= i,
and
n

i=1
aik˚Aij =

det (A)
if k = j,
0
if k ̸= j,
which is equivalent to saying that A[adj (A)] = [adj (A)]A = det (A) I.
Example 6.2.9
Diﬀerential Equations and Determinants. A system of n homogeneous
ﬁrst-order linear diﬀerential equations
d xi(t)
dt
= ai1(t)x1(t) + ai2(t)x2(t) + · · · + ain(t)xn(t),
i = 1, 2, . . . , n
can be expressed in matrix notation by writing




x′
1(t)
x′
2(t)
...
x′
n(t)



=




a11(t)
a12(t)
· · ·
a1n(t)
a21(t)
a22(t)
· · ·
a2n(t)
...
...
...
...
an1(t)
an2(t)
· · ·
ann(t)








x1(t)
x2(t)
...
xn(t)




or, equivalently, x′ = Ax. Let S = {w1(t), w2(t), . . . , wn(t)} be a set of n × 1
vectors that are solutions to x′ = Ax, and place these solutions as columns in
a matrix W(t)n×n = [w1(t) | w2(t) | · · · | wn(t)] so that W′ = AW.
Problem: Prove that if w(t) = det (W), (called the Wronskian (p. 474)), then
w(t) = w(ξ0) e
 t
ξ0 trace A(ξ) dξ,
where ξ0 is an arbitrary constant.
(6.2.8)

482
Chapter 6
Determinants
Solution: By (6.1.19), d w(t)/dt = n
i=1 det (Di), where
Di =







w11
w12
· · ·
w1n
...
...
· · ·
...
w′
i1
w′
i2
· · ·
w′
in
...
...
· · ·
...
wn1
wn2
· · ·
wnn







= W + eieT
i W′ −eieT
i W.
Notice that

−eieT
i W

subtracts Wi∗from the ith row while

+eieT
i W′
adds W′
i∗to the ith row. Use the fact that W′ = AW to write
Di = W+eieT
i W′−eieT
i W = W+eieT
i AW−eieT
i W =

I+ei

eT
i A −eT
i
 
W,
and apply formula (6.2.2) for the determinant of a rank-one updated matrix
together with the product rule (6.1.15) to produce
det (Di) =

1 + eT
i Aei −eT
i ei

det (W) = aii(t) w(t),
so
d w(t)
dt
=
n

i=1
det (Di) =
 n

i=1
aii(t)

w(t) = trace A(t) w(t).
In other words, w(t) satisﬁes the ﬁrst-order diﬀerential equation w′ =τ w, where
τ = trace A(t), and the solution of this equation is w(t)=w(ξ0) e
 t
ξ0
τ(ξ) dξ.
Consequences: In addition to its aesthetic elegance, (6.2.8) is a useful result
because it is the basis for the following theorems.
•
If x′ = Ax has a set of solutions S = {w1(t), w2(t), . . . , wn(t)} that is
linearly independent at some point ξ0 ∈(a, b), and if
 t
ξ0 τ(ξ) dξ is ﬁnite for
t ∈(a, b), then S must be linearly independent at every point t ∈(a, b).
•
If A is a constant matrix, and if S is a set of n solutions that is linearly
independent at some value t = ξ0, then S must be linearly independent for
all values of t.
Proof.
If S is linearly independent at ξ0, then W(ξ0) is nonsingular, so
w(ξ0) ̸= 0. If
 t
ξ0 τ(ξ) dξ is ﬁnite when t ∈(a, b), then e
 t
ξ0
τ(ξ) dξ is ﬁnite
and nonzero on (a, b), so, by (6.2.8), w(t) ̸= 0 on (a, b). Therefore, W(t) is
nonsingular for t ∈(a, b), and thus S is linearly independent at each t ∈(a, b).
Exercises for section 6.2
6.2.1. Use a cofactor expansion to evaluate each of the following determinants.
(a)

2
1
1
6
2
1
−2
2
1

,
(b)

0
0
−2
3
1
0
1
2
−1
1
2
1
0
2
−3
0

,
(c)

0
1
1
1
1
0
1
1
1
1
0
1
1
1
1
0

.

6.2 Additional Properties of Determinants
483
6.2.2. Use determinants to compute the following inverses.
(a)


2
1
1
6
2
1
−2
2
1


−1
.
(b)



0
0
−2
3
1
0
1
2
−1
1
2
1
0
2
−3
0



−1
.
6.2.3.
(a)
Use Cramer’s rule to solve
x1 + x2 + x3 = 1,
x1 + x2
= α,
x2 + x3 = β.
(b)
Evaluate limt→∞x2(t), where x2(t) is deﬁned by the system
x1 +
tx2 + t2x3 = t4,
t2x1 +
x2 +
tx3 = t3,
tx1 + t2x2 +
x3 = 0.
6.2.4. Is the following equation a valid derivation of Cramer’s rule for solving
a nonsingular system Ax = b, where Ai is as described on p. 476?
det (Ai)
det (A) = det

A−1Ai

= det

e1 · · · ei−1 x ei+1 · · · en

= xi.
6.2.5.
(a)
By example, show that det (A + B) ̸= det (A) + det (B).
(b)
Using square matrices, construct an example that shows that
det

A
B
C
D
	
̸= det (A)det (D) −det (B)det (C).
6.2.6. Suppose rank (Bm×n) = n, and let Q be the orthogonal projector onto
N

BT 
. For A = [B | cn×1] , prove cT Qc = det

AT A

/det

BT B

.
6.2.7. If An×n is a nonsingular matrix, and if D and C are n × k matrices,
explain how to use (6.2.1) to derive the formula
det

A + CDT 
= det (A)det

Ik + DT A−1C

.
Note: This is a generalization of (6.2.3) because if ci and di are the
ith columns of C and D, respectively, then
A + CDT = A + c1dT
1 + c2dT
2 + · · · + ckdT
k .

484
Chapter 6
Determinants
6.2.8. Explain why A is singular if and only if A[adj (A)] = 0.
6.2.9. For a nonsingular linear system Ax = b, explain why each component
of the solution must vary continuously with the entries of A.
6.2.10. For scalars α, explain why adj (αA) = αn−1adj (A) . Hint: Recall
Exercise 6.1.11.
6.2.11. For an n × n matrix A, prove that the following statements are true.
(a)
If rank (A) < n −1, then adj (A) = 0.
(b)
If rank (A) = n −1, then rank (adj (A)) = 1.
(c)
If rank (A) = n, then rank (adj (A)) = n.
6.2.12. In 1812, Cauchy discovered the formula that says that if A is n × n,
then det (adj (A)) = [det (A)]n−1. Establish Cauchy’s formula.
6.2.13. For the following tridiagonal matrix, An, let Dn = det (An), and de-
rive the formula Dn = 2Dn−1 −Dn−2 to deduce that Dn = n + 1.
An =






2
−1
0
· · ·
0
−1
2
−1
· · ·
0
...
...
...
0
· · ·
−1
2
−1
0
· · ·
0
−1
2






n×n
.
6.2.14. By considering rank-one updated matrices, derive the following formulas.
(a)

1+α1
α1
1
· · ·
1
1
1+α2
α2
· · ·
1
...
...
...
...
1
1
· · ·
1+αn
αn

= 1 +  αi
 αi
.
(b)

α
β
β
· · ·
β
β
α
β
· · ·
β
β
β
α
· · ·
β
...
...
...
...
...
β
β
β
· · ·
α

n×n
=

(α −β)n 
1 +
nβ
α−β

if α ̸= β,
0
if α = β.
(c)

1 + α1
α2
· · ·
αn
α1
1 + α2
· · ·
αn
...
...
...
...
α1
α2
· · ·
1 + αn

= 1 + α1 + α2 + · · · + αn.

6.2 Additional Properties of Determinants
485
6.2.15. A bordered matrix has the form B =
 A
x
yT
α

in which An×n is
nonsingular, x is a column, yT is a row, and α is a scalar. Explain
why the following statements must be true.
(a)

A
x
yT
−1
 = −det

A + xyT 
.
(b)

A
x
yT
0
 = −yT adj (A) x.
6.2.16. If B is m × n and C is n × m, explain why (6.2.1) guarantees that
λmdet (λIn −CB) = λndet (λIm −BC) is true for all scalars λ.
6.2.17. For a square matrix A and column vectors c and d, derive the fol-
lowing two extensions of formula (6.2.3).
(a)
If Ax = c, then det

A + cdT 
= det (A)

1 + dT x

.
(b)
If yT A = dT , then det

A + cdT 
= det (A)

1 + yT c

.
6.2.18. Describe the determinant of an elementary reﬂector (p. 324) and a plane
rotation (p. 333), and then explain how to ﬁnd det (A) using House-
holder reduction (p. 341) and Givens reduction (Example 5.7.2).
6.2.19. Suppose that A is a nonsingular matrix whose entries are integers.
Prove that the entries in A−1 are integers if and only if det (A) = ±1.
6.2.20. Let A = I −2uvT be a matrix in which u and v are column vectors
with integer entries.
(a)
Prove that A−1 has integer entries if and only if vT u = 0 or 1.
(b)
A matrix is said to be involutory whenever A−1 = A. Explain
why A = I −2uvT is involutory when vT u = 1.
6.2.21. Use induction to argue that a cofactor expansion of det (An×n) requires
c(n) = n!

1 + 1
2! + 1
3! + · · · +
1
(n −1)!
	
multiplications for n ≥2. Assume a computer will do 1,000,000 multi-
plications per second, and neglect all other operations to estimate how
long it will take to evaluate the determinant of a 100 × 100 matrix using
cofactor expansions. Hint: Recall the series expansion for ex, and use
100! ≈9.33 × 10157.

486
Chapter 6
Determinants
6.2.22. Determine all values of λ for which the matrix A−λI is singular, where
A =


0
−3
−2
2
5
2
−2
−3
0

.
Hint: If p(λ) = λn + αn−1λn−1 + · · · + α1λ + α0 is a monic polynomial
with integer coeﬃcients, then the integer roots of p(λ) are a subset of
the factors of α0.
6.2.23. Suppose that f1(t), f2(t), . . . , fn(t) are solutions of nth-order linear
diﬀerential equation y(n) + p1(t)y(n−1) + · · · + pn−1(t)y′ + pn(t)y = 0,
and let w(t) be the Wronskian
w(t) =

f1(t)
f2(t)
· · ·
fn(t)
f ′
1(t)
f ′
2(t)
· · ·
f ′
n(t)
...
...
...
...
f (n−1)
1
(t)
f (n−1)
2
(t)
· · ·
f (n−1)
n
(t)

.
By converting the nth-order equation into a system of n ﬁrst-order
equations with the substitutions x1 = y, x2 = y′, . . . , xn = y(n−1),
show that w(t) = w(ξ0) e
− t
ξ0
p1(ξ) dξ for an arbitrary constant ξ0.
6.2.24. Evaluate the Vandermonde determinant by showing

1
x1
x2
1
· · ·
xn−1
1
1
x2
x2
2
· · ·
xn−1
2
...
...
...
· · ·
...
1
xn
x2
n
· · ·
xn−1
n

=

j>i
(xj −xi).
When is this nonzero (compare with Example 4.3.4)? Hint: For the
polynomial p(λ) =

1
λ
λ2
· · ·
λk−1
1
x2
x2
2
· · ·
xk−1
2
...
...
...
· · ·
...
1
xk
x2
k
· · ·
xk−1
k

k×k
, use induction to ﬁnd the
degree of p(λ), the roots of p(λ), and the coeﬃcient of λk−1 in p(λ).
6.2.25. Suppose that each entry in An×n = [aij(x)] is a diﬀerentiable function
of a real variable x. Use formula (6.1.19) to derive the formula
d

det (A)

dx
=
n

j=1
n

i=1
d aij
dx
˚Aij.

6.2 Additional Properties of Determinants
487
6.2.26. Consider the entries of A to be independent variables, and use formula
(6.1.19) to derive the formula
∂det (A)
∂aij
= ˚Aij.
6.2.27. Laplace’s Expansion. In 1772, the French mathematician Pierre-Simon
Laplace (1749–1827) presented the following generalized version of the
cofactor expansion. For an n × n matrix A, let
A(i1i2 · · · ik | j1j2 · · · jk) = the k × k submatrix of A that lies on
the intersection of rows i1, i2, . . . , ik
with columns j1, j2, . . . , jk,
and let
M(i1i2 · · · ik | j1j2 · · · jk) = the n −k × n −k minor determinant
obtained by deleting rows i1, i2, . . . , ik
and columns j1, j2, . . . , jk from A.
The cofactor of A(i1 · · · ik | j1 · · · jk) is deﬁned to be the signed minor
˚A(i1 · · · ik | j1 · · · jk) = (−1)i1+···+ik+j1+···+jkM(i1 · · · ik | j1 · · · jk).
This is consistent with the deﬁnition of cofactor given earlier because if
A(i | j) = aij, then ˚A(i | j) = (−1)i+jM(i | j) = (−1)i+jMij = ˚Aij. For
each ﬁxed set of row indices 1 ≤i1 < · · · < ik ≤n,
det (A) =

1≤j1<···<jk≤n
det A(i1 · · · ik | j1 · · · jk)˚A(i1 · · · ik | j1 · · · jk).
Similarly, for each ﬁxed set of column indices 1 ≤j1 < · · · < jk ≤n,
det (A) =

1≤i1<···<ik≤n
det A(i1 · · · ik | j1 · · · jk)˚A(i1 · · · ik | j1 · · · jk).
Each of these sums contains

n
k

terms. Use Laplace’s expansion to
evaluate the determinant of
A =



0
0
−2
3
1
0
1
2
−1
1
2
1
0
2
−3
0



in terms of the ﬁrst and third rows.

CHAPTER 7
Eigenvalues
and
Eigenvectors
7.1
ELEMENTARY PROPERTIES OF EIGENSYSTEMS
Up to this point, almost everything was either motivated by or evolved from the
consideration of systems of linear algebraic equations. But we have come to a
turning point, and from now on the emphasis will be diﬀerent. Rather than being
concerned with systems of algebraic equations, many topics will be motivated
or driven by applications involving systems of linear diﬀerential equations and
their discrete counterparts, diﬀerence equations.
For example, consider the problem of solving the system of two ﬁrst-order
linear diﬀerential equations, du1/dt = 7u1 −4u2 and du2/dt = 5u1 −2u2. In
matrix notation, this system is

u′
1
u′
2

=

7
−4
5
−2
 
u1
u2

or, equivalently,
u′ = Au,
(7.1.1)
where u′ =
 u′
1
u′
2

, A =
 7
−4
5
−2

, and u =
 u1
u2

. Because solutions of a single
equation u′ = λu have the form u = αeλt, we are motivated to seek solutions
of (7.1.1) that also have the form
u1 = α1eλt
and
u2 = α2eλt.
(7.1.2)
Diﬀerentiating these two expressions and substituting the results in (7.1.1) yields
α1λeλt = 7α1eλt −4α2eλt
α2λeλt = 5α1eλt −2α2eλt ⇒
α1λ = 7α1 −4α2
α2λ = 5α1 −2α2
⇒
 7
−4
5
−2
  α1
α2

=λ
 α1
α2

.

490
Chapter 7
Eigenvalues and Eigenvectors
In other words, solutions of (7.1.1) having the form (7.1.2) can be constructed
provided solutions for λ and x =
 α1
α2

in the matrix equation Ax = λx can
be found. Clearly, x = 0 trivially satisﬁes Ax = λx, but x = 0 provides no
useful information concerning the solution of (7.1.1). What we really need are
scalars λ and nonzero vectors x that satisfy Ax = λx. Writing Ax = λx
as (A −λI) x = 0 shows that the vectors of interest are the nonzero vectors in
N (A −λI) . But N (A −λI) contains nonzero vectors if and only if A −λI
is singular. Therefore, the scalars of interest are precisely the values of λ that
make A −λI singular or, equivalently, the λ ’s for which det (A −λI) = 0.
These observations motivate the deﬁnition of eigenvalues and eigenvectors.
66
Eigenvalues and Eigenvectors
For an n × n matrix A, scalars λ and vectors xn×1 ̸= 0 satisfying
Ax = λx are called eigenvalues and eigenvectors of A, respectively,
and any such pair, (λ, x), is called an eigenpair for A. The set of
distinct eigenvalues, denoted by σ (A) , is called the spectrum of A.
•
λ ∈σ (A) ⇐⇒A −λI is singular ⇐⇒det (A −λI) = 0.
(7.1.3)
•

x ̸= 0
 x ∈N (A −λI)

is the set of all eigenvectors associated
with λ. From now on, N (A −λI) is called an eigenspace for A.
•
Nonzero row vectors y∗such that y∗(A −λI) = 0 are called left-
hand eigenvectors for A (see Exercise 7.1.18 on p. 503).
Geometrically, Ax = λx says that under transformation by A, eigenvec-
tors experience only changes in magnitude or sign—the orientation of Ax in ℜn
is the same as that of x. The eigenvalue λ is simply the amount of “stretch”
or “shrink” to which the eigenvector x is subjected when transformed by A.
Figure 7.1.1 depicts the situation in ℜ2.
Ax = λx
x
Figure 7.1.1
66
The words eigenvalue and eigenvector are derived from the German word eigen, which means
owned by or peculiar to. Eigenvalues and eigenvectors are sometimes called characteristic values
and characteristic vectors, proper values and proper vectors, or latent values and latent vectors.

7.1 Elementary Properties of Eigensystems
491
Let’s now face the problem of ﬁnding the eigenvalues and eigenvectors of
the matrix A =
 7
−4
5
−2

appearing in (7.1.1). As noted in (7.1.3), the eigen-
values are the scalars λ for which det (A −λI) = 0. Expansion of det (A −λI)
produces the second-degree polynomial
p(λ) = det (A −λI) =

7 −λ
−4
5
−2 −λ
 = λ2 −5λ + 6 = (λ −2)(λ −3),
which is called the characteristic polynomial for A. Consequently, the eigen-
values for A are the solutions of the characteristic equation p(λ) = 0 (i.e.,
the roots of the characteristic polynomial), and they are λ = 2 and λ = 3.
The eigenvectors associated with λ = 2 and λ = 3 are simply the nonzero
vectors in the eigenspaces N (A −2I) and N (A −3I), respectively. But deter-
mining these eigenspaces amounts to nothing more than solving the two homo-
geneous systems, (A −2I) x = 0 and (A −3I) x = 0.
For λ = 2,
A −2I =

5
−4
5
−4

−→

1
−4/5
0
0

=⇒
x1 = (4/5)x2
x2 is free
=⇒
N (A −2I) =
	
x
 x = α

4/5
1

.
For λ = 3,
A −3I =

4
−4
5
−5

−→

1
−1
0
0

=⇒
x1 = x2
x2 is free
=⇒
N (A −3I) =
	
x
 x = β

1
1

.
In other words, the eigenvectors of A associated with λ = 2 are all nonzero
multiples of x = ( 4/5
1 )T , and the eigenvectors associated with λ = 3 are
all nonzero multiples of y = ( 1
1 )T . Although there are an inﬁnite number of
eigenvectors associated with each eigenvalue, each eigenspace is one dimensional,
so, for this example, there is only one independent eigenvector associated with
each eigenvalue.
Let’s complete the discussion concerning the system of diﬀerential equations
u′ = Au in (7.1.1). Coupling (7.1.2) with the eigenpairs (λ1, x) and (λ2, y) of
A computed above produces two solutions of u′ = Au, namely,
u1 = eλ1tx = e2t

4/5
1

and
u2 = eλ2ty = e3t

1
1

.
It turns out that all other solutions are linear combinations of these two particular
solutions—more is said in §7.4 on p. 541.
Below is a summary of some general statements concerning features of the
characteristic polynomial and the characteristic equation.

492
Chapter 7
Eigenvalues and Eigenvectors
Characteristic Polynomial and Equation
•
The characteristic polynomial of An×n is p(λ) = det (A −λI).
The degree of p(λ) is n, and the leading term in p(λ) is (−1)nλn.
•
The characteristic equation for A is p(λ) = 0.
•
The eigenvalues of A are the solutions of the characteristic equation
or, equivalently, the roots of the characteristic polynomial.
•
Altogether, A has n eigenvalues, but some may be complex num-
bers (even if the entries of A are real numbers), and some eigenval-
ues may be repeated.
•
If A contains only real numbers, then its complex eigenvalues must
occur in conjugate pairs—i.e., if λ ∈σ (A) , then λ ∈σ (A) .
Proof.
The fact that det (A −λI) is a polynomial of degree n whose leading
term is (−1)nλn follows from the deﬁnition of determinant given in (6.1.1). If
δij =
	
1
if i = j,
0
if i ̸= j,
then
det (A −λI) =

p
σ(p)(a1p1 −δ1p1λ)(a2p2 −δ2p2λ) · · · (anpn −δnpnλ)
is a polynomial in λ. The highest power of λ is produced by the term
(a11 −λ)(a22 −λ) · · · (ann −λ),
so the degree is n, and the leading term is (−1)nλn. The discussion given
earlier contained the proof that the eigenvalues are precisely the solutions of the
characteristic equation, but, for the sake of completeness, it’s repeated below:
λ ∈σ (A) ⇐⇒Ax = λx for some x ̸= 0 ⇐⇒(A −λI) x = 0 for some x ̸= 0
⇐⇒A −λI is singular ⇐⇒det (A −λI) = 0.
The fundamental theorem of algebra is a deep result that insures every poly-
nomial of degree n with real or complex coeﬃcients has n roots, but some
roots may be complex numbers (even if all the coeﬃcients are real), and some
roots may be repeated. Consequently, A has n eigenvalues, but some may be
complex, and some may be repeated. The fact that complex eigenvalues of real
matrices must occur in conjugate pairs is a consequence of the fact that the roots
of a polynomial with real coeﬃcients occur in conjugate pairs.

7.1 Elementary Properties of Eigensystems
493
Example 7.1.1
Problem: Determine the eigenvalues and eigenvectors of A =
 1
−1
1
1

.
Solution: The characteristic polynomial is
det (A −λI) =

1 −λ
−1
1
1 −λ
 = (1 −λ)2 + 1 = λ2 −2λ + 2,
so the characteristic equation is λ2 −2λ + 2 = 0. Application of the quadratic
formula yields
λ = 2 ± √−4
2
= 2 ± 2√−1
2
= 1 ± i,
so the spectrum of A is σ (A) = {1 + i, 1 −i}. Notice that the eigenvalues are
complex conjugates of each other—as they must be because complex eigenvalues
of real matrices must occur in conjugate pairs. Now ﬁnd the eigenspaces.
For λ = 1 + i,
A −λI =

−i
−1
1
−i

−→

1
−i
0
0

=⇒
N (A −λI) = span
	
i
1

.
For λ = 1 −i,
A −λI =

i
−1
1
i

−→

1
i
0
0

=⇒
N (A −λI) = span
	
−i
1

.
In other words, the eigenvectors associated with λ1 = 1 + i are all nonzero
multiples of x1 = ( i
1 )T , and the eigenvectors associated with λ2 = 1 −i
are all nonzero multiples of x2 = ( −i
1 )T . In previous sections, you could
be successful by thinking only in terms of real numbers and by dancing around
those statements and issues involving complex numbers. But this example makes
it clear that avoiding complex numbers, even when dealing with real matrices,
is no longer possible—very innocent looking matrices, such as the one in this
example, can possess complex eigenvalues and eigenvectors.
As we have seen, computing eigenvalues boils down to solving a polynomial
equation. But determining solutions to polynomial equations can be a formidable
task. It was proven in the nineteenth century that it’s impossible to express
the roots of a general polynomial of degree ﬁve or higher using radicals of the
coeﬃcients. This means that there does not exist a generalized version of the
quadratic formula for polynomials of degree greater than four, and general poly-
nomial equations cannot be solved by a ﬁnite number of arithmetic operations
involving
+,−,×,÷, n√. Unlike solving Ax = b, the eigenvalue problem gener-
ally requires an inﬁnite algorithm, so all practical eigenvalue computations are
accomplished by iterative methods—some are discussed later.

494
Chapter 7
Eigenvalues and Eigenvectors
For theoretical work, and for textbook-type problems, it’s helpful to express
the characteristic equation in terms of the principal minors. Recall that an r × r
principal submatrix of An×n is a submatrix that lies on the same set of r
rows and columns, and an r × r principal minor is the determinant of an r × r
principal submatrix. In other words, r × r principal minors are obtained by
deleting the same set of n−r rows and columns, and there are
n
r

= n!/r!(n−r)!
such minors. For example, the 1 × 1 principal minors of
A =


−3
1
−3
20
3
10
2
−2
4


(7.1.4)
are the diagonal entries −3, 3, and 4. The 2 × 2 principal minors are

−3
1
20
3
 = −29,

−3
−3
2
4
 = −6,
and

3
10
−2
4
 = 32,
and the only 3 × 3 principal minor is det (A) = −18.
Related to the principal minors are the symmetric functions of the eigenval-
ues. The kth symmetric function of λ1, λ2, . . . , λn is deﬁned to be the sum
of the product of the eigenvalues taken k at a time. That is,
sk =

1≤i1<···<ik≤n
λi1 · · · λik.
For example, when n = 4,
s1 = λ1 + λ2 + λ3 + λ4,
s2 = λ1λ2 + λ1λ3 + λ1λ4 + λ2λ3 + λ2λ4 + λ3λ4,
s3 = λ1λ2λ3 + λ1λ2λ4 + λ1λ3λ4 + λ2λ3λ4,
s4 = λ1λ2λ3λ4.
The connection between symmetric functions, principal minors, and the coeﬃ-
cients in the characteristic polynomial is given in the following theorem.
Coefﬁcients in the Characteristic Equation
If λn + c1λn−1 + c2λn−2 + · · · + cn−1λ + cn = 0 is the characteristic
equation for An×n, and if sk is the kth symmetric function of the
eigenvalues λ1, λ2, . . . , λn of A, then
•
ck = (−1)k (all k × k principal minors),
(7.1.5)
•
sk = (all k × k principal minors),
(7.1.6)
•
trace (A) = λ1 + λ2 + · · · + λn = −c1,
(7.1.7)
•
det (A) = λ1λ2 · · · λn = (−1)ncn.
(7.1.8)

7.1 Elementary Properties of Eigensystems
495
Proof.
At least two proofs of (7.1.5) are possible, and although they are concep-
tually straightforward, each is somewhat tedious. One approach is to successively
use the result of Exercise 6.1.14 to expand det (A −λI). Another proof rests on
the observation that if
p(λ) = det(A −λI) = (−1)nλn + a1λn−1 + a2λn−2 + · · · + an−1λ + an
is the characteristic polynomial for A, then the characteristic equation is
λn + c1λn−1 + c2λn−2 + · · · + cn−1λ + cn = 0,
where
ci = (−1)nai.
Taking the rth derivative of p(λ) yields p(r)(0) = r!an−r, and hence
cn−r = (−1)n
r!
p(r)(0).
(7.1.9)
It’s now a matter of repeatedly applying the formula (6.1.19) for diﬀerentiating
a determinant to p(λ) = det (A −λI). After r applications of (6.1.19),
p(r)(λ) =

ij̸=ik
Di1···ir(λ),
where Di1···ir(λ) is the determinant of the matrix identical to A −λI except
that rows i1, i2, . . . , ir have been replaced by −eT
i1, −eT
i2, . . . , −eT
ir, respectively.
It follows that Di1···ir(0) = (−1)rdet (Ai1···ir), where Ai1i2···ir is identical to
A except that rows i1, i2, . . . , ir have been replaced by eT
i1, eT
i2, . . . , eT
ir, re-
spectively, and det (Ai1···ir) is the n −r × n −r principal minor obtained by
deleting rows and columns i1, i2, . . . , ir from A. Consequently,
p(r)(0) =

ij̸=ik
Di1···ir(0) = (−1)r 
ij̸=ik
det (Ai1···ir)
= r! × (−1)r 
(all n −r × n −r principal minors).
The factor r! appears because each of the r! permutations of the subscripts on
Ai1···ir describes the same matrix. Therefore, (7.1.9) says
cn−r = (−1)n
r!
p(r)(0) = (−1)n−r 
(all n −r × n −r principal minors).
To prove (7.1.6), write the characteristic equation for A as
(λ −λ1)(λ −λ2) · · · (λ −λn) = 0,
(7.1.10)
and expand the left-hand side to produce
λn −s1λn−1 + · · · + (−1)kskλn−k + · · · + (−1)nsn = 0.
(7.1.11)
(Using n = 3 or n = 4 in (7.1.10) makes this clear.) Comparing (7.1.11)
with (7.1.5) produces the desired conclusion. Statements (7.1.7) and (7.1.8) are
obtained from (7.1.5) and (7.1.6) by setting k = 1 and k = n.

496
Chapter 7
Eigenvalues and Eigenvectors
Example 7.1.2
Problem: Determine the eigenvalues and eigenvectors of
A =


−3
1
−3
20
3
10
2
−2
4

.
Solution: Use the principal minors computed in (7.1.4) along with (7.1.5) to
obtain the characteristic equation
λ3 −4λ2 −3λ + 18 = 0.
A result from elementary algebra states that if the coeﬃcients αi in
λn + αn−1λn−1 + · · · + α1λ + α0 = 0
are integers, then every integer solution is a factor of α0. For our problem, this
means that if there exist integer eigenvalues, then they must be contained in the
set S = {±1, ±2, ±3, ±6, ±9, ±18}. Evaluating p(λ) for each λ ∈S reveals
that p(3) = 0 and p(−2) = 0, so λ = 3 and λ = −2 are eigenvalues for A.
To determine the other eigenvalue, deﬂate the problem by dividing
λ3 −4λ2 −3λ + 18
λ −3
= λ2 −λ −6 = (λ −3)(λ + 2).
Thus the characteristic equation can be written in factored form as
(λ −3)2(λ + 2) = 0,
so the spectrum of A is σ (A) = {3, −2} in which λ = 3 is repeated—we say
that the algebraic multiplicity of λ = 3 is two. The eigenspaces are obtained
as follows.
For λ = 3,
A −3I −→


1
0
1/2
0
1
0
0
0
0


=⇒
N (A −3I) = span





−1
0
2




.
For λ = −2,
A + 2I −→


1
0
1
0
1
−2
0
0
0


=⇒
N (A + 2I) = span





−1
2
1




.
Notice that although the algebraic multiplicity of λ = 3 is two, the dimen-
sion of the associated eigenspace is only one—we say that A is deﬁcient in
eigenvectors. As we will see later, deﬁcient matrices pose signiﬁcant diﬃculties.

7.1 Elementary Properties of Eigensystems
497
Example 7.1.3
Continuity of Eigenvalues. A classical result (requiring complex analysis)
states that the roots of a polynomial vary continuously with the coeﬃcients. Since
the coeﬃcients of the characteristic polynomial p(λ) of A can be expressed
in terms of sums of principal minors, it follows that the coeﬃcients of p(λ)
vary continuously with the entries of A. Consequently, the eigenvalues of A
must vary continuously with the entries of A. Caution! Components of an
eigenvector need not vary continuously with the entries of A —e.g., consider
x = (ϵ−1, 1) as an eigenvector for A =
 0
1
0
ϵ

, and let ϵ →0.
Example 7.1.4
Spectral Radius. For square matrices A, the number
ρ(A) = max
λ∈σ(A) |λ|
is called the spectral radius of A. It’s not uncommon for applications to
require only a bound on the eigenvalues of A. That is, precise knowledge of
each eigenvalue may not called for, but rather just an upper bound on ρ(A)
is all that’s often needed. A rather crude (but cheap) upper bound on ρ(A)
is obtained by observing that ρ(A) ≤∥A∥for every matrix norm. This is
true because if (λ, x) is any eigenpair, then X =

x | 0 | · · · | 0

n×n ̸= 0, and
λX = AX implies |λ| ∥X∥= ∥λX∥= ∥AX∥≤∥A∥∥X∥, so
|λ| ≤∥A∥
for all λ ∈σ (A) .
(7.1.12)
This result is a precursor to a stronger relationship between spectral radius
and norm that is hinted at in Exercise 7.3.12 and developed in Example 7.10.1
(p. 619).
The eigenvalue bound (7.1.12) given in Example 7.1.4 is cheap to compute,
especially if the 1-norm or ∞-norm is used, but you often get what you pay
for. You get one big circle whose radius is usually much larger than the spectral
radius ρ(A). It’s possible to do better by using a set of Gerschgorin
67 circles as
described below.
67
S. A. Gerschgorin illustrated the use of Gerschgorin circles for estimating eigenvalues in 1931,
but the concept appears earlier in work by L. L´evy in 1881, by H. Minkowski (p. 278) in 1900,
and by J. Hadamard (p. 469) in 1903. However, each time the idea surfaced, it gained little
attention and was quickly forgotten until Olga Taussky (1906–1995), the premier woman of
linear algebra, and her fellow German emigr`e Alfred Brauer (1894–1985) became captivated
by the result. Taussky (who became Olga Taussky-Todd after marrying the numerical analyst
John Todd) and Brauer devoted signiﬁcant eﬀort to strengthening, promoting, and popularizing
Gerschgorin-type eigenvalue bounds. Their work during the 1940s and 1950s ended the periodic
rediscoveries, and they made Gerschgorin (who might otherwise have been forgotten) famous.

498
Chapter 7
Eigenvalues and Eigenvectors
Gerschgorin Circles
•
The eigenvalues of A ∈Cn×n are contained the union Gr of the n
Gerschgorin circles deﬁned by
|z −aii| ≤ri,
where ri =
n

j=1
j̸=i
|aij| for i = 1, 2, . . . , n.
(7.1.13)
In other words, the eigenvalues are trapped in the collection of circles
centered at aii with radii given by the sum of absolute values in Ai∗
with aii deleted.
•
Furthermore, if a union U of k Gerschgorin circles does not touch
any of the other n −k circles, then there are exactly k eigenvalues
(counting multiplicities) in the circles in U.
(7.1.14)
•
Since σ(AT ) = σ (A) , the deleted absolute row sums in (7.1.13)
can be replaced by deleted absolute column sums, so the eigenvalues
of A are also contained in the union Gc of the circles deﬁned by
|z −ajj| ≤cj,
where cj =
n

i=1
i̸=j
|aij| for j = 1, 2, . . . , n.
(7.1.15)
•
Combining (7.1.13) and (7.1.15) means that the eigenvalues of A
are contained in the intersection Gr ∩Gc.
(7.1.16)
Proof.
Let (λ, x) be an eigenpair for A, and assume x has been normalized
so that ∥x∥∞= 1. If xi is a component of x such that |xi| = 1, then
λxi = [λx]i = [Ax]i =
n

j=1
aijxj
=⇒
(λ −aii)xi =
n

j=1
j̸=i
aijxj,
and hence
|λ −aii| =|λ −aii| |xi| =


j̸=i
aijxj
 ≤

j̸=i
|aij| |xj| ≤

j̸=i
|aij| = ri.
Thus λ is in one of the Gerschgorin circles, so the union of all such circles
contains σ (A) . To establish (7.1.14), let D = diag (a11, a22, . . . , ann) and
B = A−D, and set C(t) = D+tB for t ∈[0, 1]. The ﬁrst part shows that the
eigenvalues of λi(t) of C(t) are contained in the union of the Gerschgorin circles
Ci(t) deﬁned by |z−aii| ≤t ri. The circles Ci(t) grow continuously with t from
individual points aii when t = 0 to the Gerschgorin circles of A when t = 1,

7.1 Elementary Properties of Eigensystems
499
so, if the circles in the isolated union U are centered at ai1i1, ai2i2, . . . , aikik,
then for every t ∈[0, 1] the union U(t) = Ci1(t) ∪Ci2(t) ∪· · · ∪Cik(t) is dis-
joint from the union U(t) of the other n −k Gerschgorin circles of C(t). Since
(as mentioned in Example 7.1.3) each eigenvalue λi(t) of C(t) also varies con-
tinuously with t, each λi(t) is on a continuous curve Γi having one end at
λi(0) = aii and the other end at λi(1) ∈σ (A) . But since U(t) ∩U(t) = φ for
all t ∈[0, 1], the curves Γi1, Γi2, . . . , Γik are entirely contained in U, and hence
the end points λi1(1), λi2(1), . . . , λik(1) are in U. Similarly, the other n −k
eigenvalues of A are in the union of the complementary set of circles.
Example 7.1.5
Problem: Estimate the eigenvalues of A =
 5
1
1
0
6
1
1
0
−5

.
•
A crude estimate is derived from the bound given in Example 7.1.4 on p. 497.
Using the ∞-norm, (7.1.12) says that |λ| ≤∥A∥∞= 7 for all λ ∈σ (A) .
•
Better estimates are produced by the Gerschgorin circles in Figure 7.1.2 that
are derived from row sums. Statements (7.1.13) and (7.1.14) guarantee that
one eigenvalue is in (or on) the circle centered at −5, while the remaining
two eigenvalues are in (or on) the larger circle centered at +5.
1
2
3
4
5
6
7
-1
-2
-3
-4
-5
-6
-7
Figure 7.1.2.
Gerschgorin circles derived from row sums.
•
The best estimate is obtained from (7.1.16) by considering Gr ∩Gc.
1
2
3
4
5
6
7
-1
-2
-3
-4
-5
-6
-7
Figure 7.1.3.
Gerschgorin circles derived from Gr ∩Gc.
In other words, one eigenvalue is in the circle centered at −5, while the other
two eigenvalues are in the union of the other two circles in Figure 7.1.3. This is
corroborated by computing σ (A)={5, (1±5
√
5)/2} ≈{5, 6.0902, −5.0902}.
Example 7.1.6
Diagonally Dominant Matrices Revisited. Recall from Example 4.3.3 on
p. 184 that An×n is said to be diagonally dominant (some authors say strictly
diagonally dominant) whenever

500
Chapter 7
Eigenvalues and Eigenvectors
|aii| >
n

j=1
j̸=i
|aij|
for each i = 1, 2, . . . , n.
Gerschgorin’s theorem (7.1.13) guarantees that diagonally dominant matrices
cannot possess a zero eigenvalue. But 0 /∈σ (A) if and only if A is nonsingular
(Exercise 7.1.6), so Gerschgorin’s theorem provides an alternative to the argu-
ment used in Example 4.3.3 to prove that all diagonally dominant matrices are
nonsingular.
68 For example, the 3 × 3 matrix A in Example 7.1.5 is diagonally
dominant, and thus A is nonsingular. Even when a matrix is not diagonally
dominant, Gerschgorin estimates still may be useful in determining whether or
not the matrix is nonsingular simply by observing if zero is excluded from σ (A)
based on the conﬁguration of the Gerschgorin circles given in (7.1.16).
Exercises for section 7.1
7.1.1. Determine the eigenvalues and eigenvectors for the following matrices.
A =

−10
−7
14
11

.
B =


2
16
8
4
14
8
−8
−32
−18

.
C =


3
−2
5
0
1
4
0
−1
5

.
D =


0
6
3
−1
5
1
−1
2
4

.
E =


3
0
0
0
3
0
0
0
3

.
Which, if any, are deﬁcient in eigenvectors in the sense that there fails
to exist a complete linearly independent set?
7.1.2. Without doing an eigenvalue–eigenvector computation, determine which
of the following are eigenvectors for
A =



−9
−6
−2
−4
−8
−6
−3
−1
20
15
8
5
32
21
7
12


,
and for those which are eigenvectors, identify the associated eigenvalue.
(a)



−1
1
0
1


.
(b)



1
0
−1
0


.
(c)



−1
0
2
2


.
(d)



0
1
−3
0


.
68
In fact, this result was the motivation behind the original development of Gerschgorin’s circles.

7.1 Elementary Properties of Eigensystems
501
7.1.3. Explain why the eigenvalues of triangular and diagonal matrices
T =




t11
t12
· · ·
t1n
0
t22
· · ·
t2n
...
...
...
...
0
0
· · ·
tnn




and
D =




λ1
0
· · ·
0
0
λ2
· · ·
0
...
...
...
...
0
0
· · ·
λn




are simply the diagonal entries—the tii ’s and λi ’s.
7.1.4. For T =
 A
B
0
C

, prove det (T −λI) = det (A −λI)det (C −λI) to
conclude that σ
 A
B
0
C

= σ (A) ∪σ (C) for square A and C.
7.1.5. Determine the eigenvectors of D = diag (λ1, λ2, . . . , λn) . In particular,
what is the eigenspace associated with λi?
7.1.6. Prove that 0 ∈σ (A) if and only if A is a singular matrix.
7.1.7. Explain why it’s apparent that An×n =




n
1
1
· · ·
1
1
n
1
· · ·
1
1
1
n
· · ·
1
...
...
...
...
...
1
1
1
· · ·
n



doesn’t
have a zero eigenvalue, and hence why A is nonsingular.
7.1.8. Explain why the eigenvalues of A∗A and AA∗are real and nonneg-
ative for every A ∈Cm×n. Hint: Consider ∥Ax∥2
2 / ∥x∥2
2 . When are
the eigenvalues of A∗A and AA∗strictly positive?
7.1.9.
(a)
If A is nonsingular, and if (λ, x) is an eigenpair for A, show
that

λ−1, x

is an eigenpair for A−1.
(b)
For all α /∈σ(A), prove that x is an eigenvector of A if and
only if x is an eigenvector of (A −αI)−1.
7.1.10.
(a)
Show that if (λ, x) is an eigenpair for A, then (λk, x) is an
eigenpair for Ak for each positive integer k.
(b)
If p(x) = α0 + α1x + α2x2 + · · · + αkxk is any polynomial, then
we deﬁne p(A) to be the matrix
p(A) = α0I + α1A + α2A2 + · · · + αkAk.
Show that if (λ, x) is an eigenpair for A, then (p(λ), x) is an
eigenpair for p(A).

502
Chapter 7
Eigenvalues and Eigenvectors
7.1.11. Explain why (7.1.14) in Gerschgorin’s theorem on p. 498 implies that
A =


1
0
−2
0
0
12
0
−4
1
0
−1
0
0
5
0
0

must have at least two real eigenvalues. Cor-
roborate this fact by computing the eigenvalues of A.
7.1.12. If A is nilpotent ( Ak = 0 for some k ), explain why trace (A) = 0.
Hint: What is σ (A)?
7.1.13. If x1, x2, . . . , xk are eigenvectors of A associated with the same eigen-
value λ, explain why every nonzero linear combination
v = α1x1 + α2x2 + · · · + αnxn
is also an eigenvector for A associated with the eigenvalue λ.
7.1.14. Explain why an eigenvector for a square matrix A cannot be associated
with two distinct eigenvalues for A.
7.1.15. Suppose σ (An×n) = σ (Bn×n) . Does this guarantee that A and B
have the same characteristic polynomial?
7.1.16. Construct 2 × 2 examples to prove the following statements.
(a)
λ ∈σ (A) and µ ∈σ (B)
̸=⇒
λ + µ ∈σ (A + B) .
(b)
λ ∈σ (A) and µ ∈σ (B)
̸=⇒
λµ ∈σ (AB) .
7.1.17. Suppose that {λ1, λ2, . . . , λn} are the eigenvalues for An×n, and let
(λk, c) be a particular eigenpair.
(a)
For λ /∈σ (A) , explain why (A −λI)−1c = c/(λk −λ).
(b)
For an arbitrary vector dn×1, prove that the eigenvalues of
A + cdT agree with those of A except that λk is replaced by
λk + dT c.
(c)
How can d be selected to guarantee that the eigenvalues of
A+cdT and A agree except that λk is replaced by a speciﬁed
number µ?

7.1 Elementary Properties of Eigensystems
503
7.1.18. Suppose that A is a square matrix.
(a)
Explain why A and AT have the same eigenvalues.
(b)
Explain why λ ∈σ (A) ⇐⇒λ ∈σ (A∗) .
Hint: Recall Exercise 6.1.8.
(c)
Do these results imply that λ ∈σ (A) ⇐⇒λ ∈σ (A) when A
is a square matrix of real numbers?
(d)
A nonzero row vector y∗is called a left-hand eigenvector for
A whenever there is a scalar µ ∈C such that y∗(A −µI) = 0.
Explain why µ must be an eigenvalue for A in the “right-hand”
sense of the term when A is a square matrix of real numbers.
7.1.19. Consider matrices Am×n and Bn×m.
(a)
Explain why AB and BA have the same characteristic poly-
nomial if m = n. Hint: Recall Exercise 6.2.16.
(b)
Explain why the characteristic polynomials for AB and BA
can’t be the same when m ̸= n, and then explain why σ (AB)
and σ (BA) agree, with the possible exception of a zero eigen-
value.
7.1.20. If AB = BA, prove that A and B have a common eigenvector.
Hint: For λ ∈σ (A) , let the columns of X be a basis for N (A −λI)
so that (A −λI)BX = 0. Explain why there exists a matrix P such
that BX = XP, and then consider any eigenpair for P.
7.1.21. For ﬁxed matrices Pm×m and Qn×n, let T be the linear operator on
Cm×n deﬁned by T(A) = PAQ.
(a)
Show that if x is a right-hand eigenvector for P and y∗is a
left-hand eigenvector for Q, then xy∗is an eigenvector for T.
(b)
Explain why trace (T) = trace (P) trace (Q).
7.1.22. Let D = diag (λ1, λ2, . . . , λn) be a diagonal real matrix such that
λ1 < λ2 < · · · < λn, and let vn×1 be a column of real nonzero numbers.
(a)
Prove that if α is real and nonzero, then λi is not an eigenvalue
for D + αvvT . Show that the eigenvalues of D + αvvT are in
fact given by the solutions of the secular equation f(ξ) = 0
deﬁned by
f(ξ) = 1 + α
n

i=1
v2
i
λi −ξ .
For n = 4 and α > 0, verify that the graph of f(ξ) is as de-
picted in Figure 7.1.4, and thereby conclude that the eigenvalues
of D + αvvT interlace with those of D.

504
Chapter 7
Eigenvalues and Eigenvectors
ξ1 ξ2
ξ3
ξ4
λ1
λ2
λ3
λ4
λ4 + α
1
1
Figure 7.1.4
(b)
Verify that (D −ξiI)−1v is an eigenvector for D + αvvT that
is associated with the eigenvalue ξi.
7.1.23. Newton’s Identities.
Let λ1, . . . , λn be the roots of the polynomial
p(λ) = λn +c1λn−1 +c2λn−2 +· · ·+cn, and let τk = λk
1 +λk
2 +· · ·+λk
n.
Newton’s identities say ck = −(τ1ck−1 + τ2ck−2 + · · · + τk−1c1 + τk)/k.
Derive these identities by executing the following steps:
(a)
Show p′(λ) = p(λ) n
i=1(λ−λi)−1 (logarithmic diﬀerentiation).
(b)
Use the geometric series expansion for (λ −λi)−1 to show that
for |λ| > maxi|λi|,
n

i=1
1
(λ −λi) = n
λ + τ1
λ2 + τ2
λ3 + · · · .
(c)
Combine these two results, and equate like powers of λ.
7.1.24. Leverrier–Souriau–Frame Algorithm.
69 Let the characteristic equa-
tion for A be given by λn +c1λn−1 +c2λn−2 +· · ·+cn = 0, and deﬁne
a sequence by taking B0 = I and
Bk = −trace (ABk−1)
k
I + ABk−1
for
k = 1, 2, . . . , n.
Prove that for each k,
ck = −trace (ABk−1)
k
.
Hint: Use Newton’s identities, and recall Exercise 7.1.10(a).
69
This algorithm has been rediscovered and modiﬁed several times. In 1840, the Frenchman U.
J. J. Leverrier provided the basic connection with Newton’s identities. J. M. Souriau, also from
France, and J. S. Frame, from Michigan State University, independently modiﬁed the algo-
rithm to its present form—Souriau’s formulation was published in France in 1948, and Frame’s
method appeared in the United States in 1949. Paul Horst (USA, 1935) along with Faddeev
and Sominskii (USSR, 1949) are also credited with rediscovering the technique. Although the
algorithm is intriguingly beautiful, it is not practical for ﬂoating-point computations.

7.2 Diagonalization by Similarity Transformations
505
7.2
DIAGONALIZATION BY SIMILARITY TRANSFORMATIONS
The correct choice of a coordinate system (or basis) often can simplify the form
of an equation or the analysis of a particular problem. For example, consider the
obliquely oriented ellipse in Figure 7.2.1 whose equation in the xy -coordinate
system is
13x2 + 10xy + 13y2 = 72.
By rotating the xy -coordinate system counterclockwise through an angle of 45◦
x
y
u
v
Figure 7.2.1
into a uv -coordinate system by means of (5.6.13) on p. 326, the cross-product
term is eliminated, and the equation of the ellipse simpliﬁes to become
u2
9 + v2
4 = 1.
It’s shown in Example 7.6.3 on p. 567 that we can do a similar thing for quadratic
equations in ℜn.
Choosing or changing to the most appropriate coordinate system (or basis)
is always desirable, but in linear algebra it is fundamental. For a linear operator
L on a ﬁnite-dimensional space V, the goal is to ﬁnd a basis B for V such
that the matrix representation of L with respect to B is as simple as possible.
Since diﬀerent matrix representations A and B of L are related by a similarity
transformation P−1AP = B (recall §4.8),
70 the fundamental problem for linear
operators is strictly a matrix issue—i.e., ﬁnd a nonsingular matrix P such that
P−1AP is as simple as possible. The concept of similarity was ﬁrst introduced
on p. 255, but in the interest of continuity it is reviewed below.
70
While it is helpful to have covered the topics in §§4.7–4.9, much of the subsequent development
is accessible without an understanding of this material.

506
Chapter 7
Eigenvalues and Eigenvectors
Similarity
•
Two n × n matrices A and B are said to be similar whenever
there exists a nonsingular matrix P such that P−1AP = B. The
product P−1AP is called a similarity transformation on A.
•
A Fundamental Problem. Given a square matrix A, reduce it to
the simplest possible form by means of a similarity transformation.
Diagonal matrices have the simplest form, so we ﬁrst ask, “Is every square
matrix similar to a diagonal matrix?” Linear algebra and matrix theory would
be simpler subjects if this were true, but it’s not. For example, consider
A =

0
1
0
0

,
(7.2.1)
and observe that A2 = 0 ( A is nilpotent). If there exists a nonsingular matrix
P such that P−1AP = D, where D is diagonal, then
D2 = P−1APP−1AP = P−1A2P = 0
=⇒
D = 0
=⇒
A = 0,
which is false. Thus A, as well as any other nonzero nilpotent matrix, is not sim-
ilar to a diagonal matrix. Nonzero nilpotent matrices are not the only ones that
can’t be diagonalized, but, as we will see, nilpotent matrices play a particularly
important role in nondiagonalizability.
So, if not all square matrices can be diagonalized by a similarity transforma-
tion, what are the characteristics of those that can? An answer is easily derived
by examining the equation
P−1An×nP = D =




λ1
0
· · ·
0
0
λ2
· · ·
0
...
...
...
...
0
0
· · ·
λn



,
which implies A [P∗1 | · · · | P∗n] = [P∗1 | · · · | P∗n]


λ1
· · ·
0
...
...
...
0
· · ·
λn

or, equiva-
lently, [AP∗1 | · · · | AP∗n] = [λ1P∗1 | · · · | λnP∗n] . Consequently, AP∗j = λjP∗j
for each j, so each (λj, P∗j) is an eigenpair for A. In other words, P−1AP = D
implies that P must be a matrix whose columns constitute n linearly indepen-
dent eigenvectors, and D is a diagonal matrix whose diagonal entries are the
corresponding eigenvalues. It’s straightforward to reverse the above argument to
prove the converse—i.e., if there exists a linearly independent set of n eigenvec-
tors that are used as columns to build a nonsingular matrix P, and if D is the
diagonal matrix whose diagonal entries are the corresponding eigenvalues, then
P−1AP = D. Below is a summary.

7.2 Diagonalization by Similarity Transformations
507
Diagonalizability
•
A square matrix A is said to be diagonalizable whenever A is
similar to a diagonal matrix.
•
A complete set of eigenvectors for An×n is any set of n lin-
early independent eigenvectors for A. Not all matrices have com-
plete sets of eigenvectors—e.g., consider (7.2.1) or Example 7.1.2.
Matrices that fail to possess complete sets of eigenvectors are some-
times called deﬁcient or defective matrices.
•
An×n is diagonalizable if and only if A possesses a complete set of
eigenvectors. Moreover, P−1AP = diag (λ1, λ2, . . . , λn) if and only
if the columns of P constitute a complete set of eigenvectors and
the λj ’s are the associated eigenvalues—i.e., each (λj, P∗j) is an
eigenpair for A.
Example 7.2.1
Problem: If possible, diagonalize the following matrix with a similarity trans-
formation:
A =


1
−4
−4
8
−11
−8
−8
8
5

.
Solution: Determine whether or not A has a complete set of three linearly
independent eigenvectors. The characteristic equation—perhaps computed by
using (7.1.5)—is
λ3 + 5λ2 + 3λ −9 = (λ −1)(λ + 3)2 = 0.
Therefore, λ = 1 is a simple eigenvalue, and λ = −3 is repeated twice (we
say its algebraic multiplicity is 2). Bases for the eigenspaces N (A −1I) and
N (A + 3I) are determined in the usual way to be
N (A −1I) = span





1
2
−2





and
N (A + 3I) = span





1
1
0

,


1
0
1




,
and it’s easy to check that when combined these three eigenvectors constitute a
linearly independent set. Consequently, A must be diagonalizable. To explicitly
exhibit the similarity transformation that diagonalizes A, set
P =


1
1
1
2
1
0
−2
0
1

,
and verify
P−1AP =


1
0
0
0
−3
0
0
0
−3

= D.

508
Chapter 7
Eigenvalues and Eigenvectors
Since not all square matrices are diagonalizable, it’s natural to inquire about
the next best thing—i.e., can every square matrix be triangularized by similarity?
This time the answer is yes, but before explaining why, we need to make the
following observation.
Similarity Preserves Eigenvalues
Row reductions don’t preserve eigenvalues (try a simple example). How-
ever, similar matrices have the same characteristic polynomial, so they
have the same eigenvalues with the same multiplicities. Caution! Sim-
ilar matrices need not have the same eigenvectors—see Exercise 7.2.3.
Proof.
Use the product rule for determinants in conjunction with the fact that
det

P−1
= 1/det (P) (Exercise 6.1.6) to write
det (A −λI) = det

P−1BP −λI

= det

P−1(B −λI)P

= det

P−1
det (B −λI)det (P) = det (B −λI).
In the context of linear operators, this means that the eigenvalues of a matrix
representation of an operator L are invariant under a change of basis. In other
words, the eigenvalues are intrinsic to L in the sense that they are independent
of any coordinate representation.
Now we can establish the fact that every square matrix can be triangularized
by a similarity transformation. In fact, as Issai Schur (p. 123) realized in 1909,
the similarity transformation always can be made to be unitary.
Schur’s Triangularization Theorem
Every square matrix is unitarily similar to an upper-triangular matrix.
That is, for each An×n, there exists a unitary matrix U (not unique)
and an upper-triangular matrix T (not unique) such that U∗AU = T,
and the diagonal entries of T are the eigenvalues of A.
Proof.
Use induction on n, the size of the matrix. For n = 1, there is nothing
to prove. For n > 1, assume that all n −1 × n −1 matrices are unitarily similar
to an upper-triangular matrix, and consider an n × n matrix A. Suppose that
(λ, x) is an eigenpair for A, and suppose that x has been normalized so that
∥x∥2 = 1. As discussed on p. 325, we can construct an elementary reﬂector
R = R∗= R−1 with the property that Rx = e1 or, equivalently, x = Re1
(set R = I if x = e1). Thus x is the ﬁrst column in R, so R =

x | V

, and
RAR = RA

x | V

= R

λx | AV

=

λe1 | RAV

=

λ
x∗AV
0
V∗AV

.

7.2 Diagonalization by Similarity Transformations
509
Since V∗AV is n −1 × n −1, the induction hypothesis insures that there exists
a unitary matrix Q such that Q∗(V∗AV)Q = ˜T is upper triangular. If U =
R
 1
0
0
Q

, then U is unitary (because U∗= U−1), and
U∗AU =

λ
x∗AVQ
0
Q∗V∗AVQ

=

λ
x∗AVQ
0
˜T

= T
is upper triangular. Since similar matrices have the same eigenvalues, and since
the eigenvalues of a triangular matrix are its diagonal entries (Exercise 7.1.3),
the diagonal entries of T must be the eigenvalues of A.
Example 7.2.2
The Cayley–Hamilton
71 theorem asserts that every square matrix satisﬁes
its own characteristic equation p(λ) = 0. That is, p(A) = 0.
Problem: Show how the Cayley–Hamilton theorem follows from Schur’s trian-
gularization theorem.
Solution: Schur’s theorem insures the existence of a unitary U such that
U∗AU = T is triangular, and the development allows for the eigenvalues A to
appear in any given order on the diagonal of T. So, if σ (A) = {λ1, λ2, . . . , λk}
with λi repeated ai times, then there is a unitary U such that
U∗AU=T=




T1
⋆
· · ·
⋆
T2
· · ·
⋆
...
...
Tk



,
where Ti =




λi
⋆
· · ·
⋆
λi
· · ·
⋆
...
...
λi




ai×ai
.
Consequently, (Ti −λiI)ai = 0, so (T −λiI)ai has the form
(T −λiI)ai =







⋆
· · ·
⋆
· · ·
⋆
...
...
...
0
· · ·
⋆
...
...
⋆







←−ith row of blocks.
71
William Rowan Hamilton (1805–1865), an Irish mathematical astronomer, established this
result in 1853 for his quaternions, matrices of the form

a + bi
c + di
−c + di
a −bi

that resulted
from his attempt to generalize complex numbers. In 1858 Arthur Cayley (p. 80) enunciated
the general result, but his argument was simply to make direct computations for 2 × 2 and
3 × 3 matrices. Cayley apparently didn’t appreciate the subtleties of the result because he
stated that a formal proof “was not necessary.” Hamilton’s quaternions took shape in his mind
while walking with his wife along the Royal Canal in Dublin, and he was so inspired that he
stopped to carve his idea in the stone of the Brougham Bridge. He believed quaternions would
revolutionize mathematical physics, and he spent the rest of his life working on them. But the
world did not agree. Hamilton became an unhappy man addicted to alcohol who is reported
to have died from a severe attack of gout.

510
Chapter 7
Eigenvalues and Eigenvectors
This form insures that (T −λ1I)a1(T −λ2I)a2 · · · (T −λkI)ak = 0. The charac-
teristic equation for A is p(λ) = (λ −λ1)a1(λ −λ2)a2 · · · (λ −λk)ak = 0, so
U∗p(A)U = U∗(A −λ1I)a1(A −λ2I)a2 · · · (A −λkI)akU
= (T −λ1I)a1(T −λ2I)a2 · · · (T −λkI)ak = 0,
and thus p(A) = 0. Note: A completely diﬀerent approach to the Cayley–
Hamilton theorem is discussed on p. 532.
Schur’s theorem is not the complete story on triangularizing by similarity.
By allowing nonunitary similarity transformations, the structure of the upper-
triangular matrix T can be simpliﬁed to contain zeros everywhere except on
the diagonal and the superdiagonal (the diagonal immediately above the main
diagonal). This is the Jordan form developed on p. 590, but some of the seeds
are sown here.
Multiplicities
For λ ∈σ (A) = {λ1, λ2, . . . , λs} , we adopt the following deﬁnitions.
•
The algebraic multiplicity of λ is the number of times it is re-
peated as a root of the characteristic polynomial. In other words,
alg multA (λi) = ai if and only if (x −λ1)a1 · · · (x −λs)as = 0 is
the characteristic equation for A.
•
When alg multA (λ) = 1, λ is called a simple eigenvalue.
•
The geometric multiplicity of λ is dim N (A −λI). In other
words, geo multA (λ) is the maximal number of linearly independent
eigenvectors associated with λ.
•
Eigenvalues such that alg multA (λ) = geo multA (λ) are called
semisimple eigenvalues of A. It follows from (7.2.2) on p. 511
that a simple eigenvalue is always semisimple, but not conversely.
Example 7.2.3
The algebraic and geometric multiplicity need not agree. For example, the nilpo-
tent matrix A =
 0
1
0
0

in (7.2.1) has only one distinct eigenvalue, λ = 0,
that is repeated twice, so alg multA (0) = 2. But
dim N (A −0I) = dim N (A) = 1
=⇒
geo multA (0) = 1.
In other words, there is only one linearly independent eigenvector associated with
λ = 0 even though λ = 0 is repeated twice as an eigenvalue.
Example 7.2.3 shows that geo multA (λ) < alg multA (λ) is possible. How-
ever, the inequality can never go in the reverse direction.

7.2 Diagonalization by Similarity Transformations
511
Multiplicity Inequality
For every A ∈Cn×n, and for each λ ∈σ(A),
geo multA (λ) ≤alg multA (λ) .
(7.2.2)
Proof.
Suppose alg multA (λ) = k. Schur’s triangularization theorem (p. 508)
insures the existence of a unitary U such that U∗An×nU =
 T11
T12
0
T22

,
where T11 is a k × k upper-triangular matrix whose diagonal entries are equal
to λ, and T22 is an n −k × n −k upper-triangular matrix with λ /∈σ (T22) .
Consequently, T22 −λI is nonsingular, so
rank (A −λI) = rank (U∗(A −λI)U) = rank

T11 −λI
T12
0
T22 −λI

≥rank (T22 −λI) = n −k.
The inequality follows from the fact that the rank of a matrix is at least as great
as the rank of any submatrix—recall the result on p. 215. Therefore,
alg multA (λ) = k ≥n −rank (A −λI) = dim N (A −λI) = geo multA (λ) .
Determining whether or not An×n is diagonalizable is equivalent to deter-
mining whether or not A has a complete linearly independent set of eigenvectors,
and this can be done if you are willing and able to compute all of the eigenvalues
and eigenvectors for A. But this brute force approach can be a monumental
task. Fortunately, there are some theoretical tools to help determine how many
linearly independent eigenvectors a given matrix possesses.
Independent Eigenvectors
Let {λ1, λ2, . . . , λk} be a set of distinct eigenvalues for A.
•
If {(λ1, x1), (λ2, x2), . . . , (λk, xk)} is a set of eigenpairs for
A, then S = {x1, x2, . . . , xk} is a linearly independent set.
(7.2.3)
•
If Bi is a basis for N (A −λiI), then B = B1∪B2∪· · ·∪Bk
is a linearly independent set.
(7.2.4)

512
Chapter 7
Eigenvalues and Eigenvectors
Proof of (7.2.3).
Suppose S is a dependent set. If the vectors in S are arranged
so that M = {x1, x2, . . . , xr} is a maximal linearly independent subset, then
xr+1 =
r

i=1
αixi,
and multiplication on the left by A −λr+1I produces
0 =
r

i=1
αi (Axi −λr+1xi) =
r

i=1
αi (λi −λr+1) xi.
Because M is linearly independent, αi (λi −λr+1) = 0 for each i. Conse-
quently, αi = 0 for each i (because the eigenvalues are distinct), and hence
xr+1 = 0. But this is impossible because eigenvectors are nonzero. Therefore,
the supposition that S is a dependent set must be false.
Proof of (7.2.4).
The result of Exercise 5.9.14 guarantees that B is linearly
independent if and only if
Mj = N (A −λjI) ∩

N (A −λ1I) + N (A −λ2I) + · · · + N (A −λj−1I)

= 0
for each j = 1, 2, . . . , k. Suppose we have 0 ̸= x ∈Mj for some j. Then
Ax = λjx and x = v1 + v2 + · · · + vj−1 for vi ∈N (A −λiI), which implies
j−1

i=1
(λi −λj)vi =
j−1

i=1
λivi −λj
j−1

i=1
vi = Ax −λjx = 0.
By (7.2.3), the vi ’s are linearly independent, and hence λi −λj = 0 for each
i = 1, 2, . . . , j −1. But this is impossible because the eigenvalues are distinct.
Therefore, Mj = 0 for each j, and thus B is linearly independent.
These results lead to the following characterization of diagonalizability.
Diagonalizability and Multiplicities
A matrix An×n is diagonalizable if and only if
geo multA (λ) = alg multA (λ)
(7.2.5)
for each λ ∈σ(A) —i.e., if and only if every eigenvalue is semisimple.

7.2 Diagonalization by Similarity Transformations
513
Proof.
Suppose geo multA (λi) = alg multA (λi) = ai for each eigenvalue λi.
If there are k distinct eigenvalues, and if Bi is a basis for N (A −λiI), then
B = B1 ∪B2 ∪· · · ∪Bk contains k
i=1 ai = n vectors. We just proved in (7.2.4)
that B is a linearly independent set, so B represents a complete set of linearly
independent eigenvectors of A, and we know this insures that A must be
diagonalizable. Conversely, if A is diagonalizable, and if λ is an eigenvalue for
A with alg multA (λ) = a, then there is a nonsingular matrix P such that
P−1AP = D =

λIa×a
0
0
B

,
where λ /∈σ(B). Consequently,
rank (A −λI) = rank P

0
0
0
B −λI

P−1 = rank (B −λI) = n −a,
and thus
geo multA (λ) = dim N (A −λI) = n −rank (A −λI) = a = alg multA (λ) .
Example 7.2.4
Problem: Determine if either of the following matrices is diagonalizable:
A =


−1
−1
−2
8
−11
−8
−10
11
7

,
B =


1
−4
−4
8
−11
−8
−8
8
5

.
Solution: Each matrix has exactly the same characteristic equation
λ3 + 5λ2 + 3λ −9 = (λ −1)(λ + 3)2 = 0,
so σ (A) = {1, −3} = σ (B) , where λ = 1 has algebraic multiplicity 1 and
λ = −3 has algebraic multiplicity 2. Since
geo multA (−3) = dim N (A + 3I) = 1 < alg multA (−3) ,
A is not diagonalizable. On the other hand,
geo multB (−3) = dim N (B + 3I) = 2 = alg multB (−3) ,
and geo multB (1) = 1 = alg multB (1) , so B is diagonalizable.
If An×n happens to have n distinct eigenvalues, then each eigenvalue is
simple. This means that geo multA (λ) = alg multA (λ) = 1 for each λ, so
(7.2.5) produces the following corollary guaranteeing diagonalizability.

514
Chapter 7
Eigenvalues and Eigenvectors
Distinct Eigenvalues
If no eigenvalue of A is repeated, then A is diagonalizable.
(7.2.6)
Caution! The converse is not true—see Example 7.2.4.
Example 7.2.5
Toeplitz
72 matrices have constant entries on each diagonal parallel to the main
diagonal. For example, a 4 × 4 Toeplitz matrix T along with a tridiagonal
Toeplitz matrix A are shown below:
T =



t0
t1
t2
t3
t−1
t0
t1
t2
t−2
t−1
t0
t1
t−3
t−2
t−1
t0


,
A =



t0
t1
0
0
t−1
t0
t1
0
0
t−1
t0
t1
0
0
t−1
t0


.
Toeplitz structures occur naturally in a variety of applications, and tridiago-
nal Toeplitz matrices are commonly the result of discretizing diﬀerential equa-
tion problems—e.g., see §1.4 (p. 18) and Example 7.6.1 (p. 559). The Toeplitz
structure is rich in special properties, but tridiagonal Toeplitz matrices are par-
ticularly nice because they are among the few nontrivial structures that admit
formulas for their eigenvalues and eigenvectors.
Problem: Show that the eigenvalues and eigenvectors of
A =






b
a
c
b
a
...
...
...
c
b
a
c
b






n×n
with
a ̸= 0 ̸= c
are given by
λj = b + 2a

c/a cos
 jπ
n + 1

and
xj =






(c/a)1/2 sin (1jπ/(n + 1))
(c/a)2/2 sin (2jπ/(n + 1))
(c/a)3/2 sin (3jπ/(n + 1))
...
(c/a)n/2 sin (njπ/(n + 1))






72
Otto Toeplitz (1881–1940) was a professor in Bonn, Germany, but because of his Jewish back-
ground he was dismissed from his chair by the Nazis in 1933. In addition to the matrix that
bears his name, Toeplitz is known for his general theory of inﬁnite-dimensional spaces devel-
oped in the 1930s.

7.2 Diagonalization by Similarity Transformations
515
for j = 1, 2, . . . , n, and conclude that A is diagonalizable.
Solution: For an eigenpair (λ, x), the components in (A −λI) x = 0 are
cxk−1+(b−λ)xk+axk+1 = 0, k = 1, . . . , n with x0 = xn+1 = 0 or, equivalently,
xk+2+
b −λ
a

xk+1+
 c
a

xk = 0
for k = 0, . . . , n −1 with x0 = xn+1 = 0.
These are second-order homogeneous diﬀerence equations, and solving them is
similar to solving analogous diﬀerential equations. The technique is to seek solu-
tions of the form xk = ξrk for constants ξ and r. This produces the quadratic
equation r2 + (b −λ)r/a + c/a = 0 with roots r1 and r2, and it can be argued
that the general solution of xk+2 + ((b −λ)/a)xk+1 + (c/a)xk = 0 is
xk =
	 αrk
1 + βrk
2
if r1 ̸= r2,
αρk + βkρk
if r1 = r2 = ρ,
where α and β are arbitrary constants.
For the eigenvalue problem at hand, r1 and r2 must be distinct—otherwise
xk = αρk + βkρk, and x0 = xn+1 = 0 implies each xk = 0, which is impossible
because x is an eigenvector. Hence xk = αrk
1 + βrk
2, and x0 = xn+1 = 0 yields
	
0 = α + β
0 = αrn+1
1
+ βrn+1
2

=⇒
r1
r2
n+1
= −β
α = 1
=⇒
r1
r2
= ei2πj/(n+1),
so r1 = r2ei2πj/(n+1) for some 1 ≤j ≤n. Couple this with
r2 + (b −λ)r
a
+ c
a = (r −r1)(r −r2)
=⇒
	
r1r2 = c/a
r1 + r2 = −(b −λ)/a
to conclude that r1 =

c/a eiπj/(n+1), r2 =

c/a e−iπj/(n+1), and
λ = b + a

c/a

eiπj/(n+1) + e−iπj/(n+1)
= b + 2a

c/a cos
 jπ
n + 1

.
Therefore, the eigenvalues of A must be given by
λj = b + 2a

c/a cos
 jπ
n + 1

,
j = 1, 2, . . . , n.
Since these λj ’s are all distinct (cos θ is a strictly decreasing function of θ on
(0, π), and a ̸= 0 ̸= c), A must be diagonalizable—recall (7.2.6). Finally, the
kth component of any eigenvector associated with λj satisﬁes xk = αrk
1 + βrk
2
with α + β = 0, so
xk = α
 c
a
k/2
eiπjk/(n+1) −e−iπjk/(n+1)
= 2iα
 c
a
k/2
sin
 jkπ
n + 1

.

516
Chapter 7
Eigenvalues and Eigenvectors
Setting α = 1/2i yields a particular eigenvector associated with λj as
xj =






(c/a)1/2 sin (1jπ/(n + 1))
(c/a)2/2 sin (2jπ/(n + 1))
(c/a)3/2 sin (3jπ/(n + 1))
...
(c/a)n/2 sin (njπ/(n + 1))






.
Because the λj ’s are distinct, {x1, x2, . . . , xn} is a complete linearly indepen-
dent set—recall (7.2.3)—so P =

x1 | x2 | · · · | xn

diagonalizes A.
It’s often the case that a right-hand and left-hand eigenvector for some
eigenvalue is known. Rather than starting from scratch to ﬁnd additional eigen-
pairs, the known information can be used to reduce or “deﬂate” the problem to
a smaller one as described in the following example.
Example 7.2.6
Deﬂation. Suppose that right-hand and left-hand eigenvectors x and y∗for an
eigenvalue λ of A ∈ℜn×n are already known, so Ax = λx and y∗A = λy∗.
Furthermore, suppose y∗x ̸= 0 —such eigenvectors are guaranteed to exist if λ
is simple or if A is diagonalizable (Exercises 7.2.23 and 7.2.22).
Problem: Use x and y∗to deﬂate the size of the remaining eigenvalue problem.
Solution: Scale x and y∗so that y∗x = 1, and construct Xn×n−1 so that its
columns are an orthonormal basis for y⊥. An easy way of doing this is to build
a reﬂector R =
˜y | X

having ˜y = y/ ∥y∥2 as its ﬁrst column as described on
p. 325. If P =

x | X

, then straightforward multiplication shows that
P−1 =

y∗
X∗(I −xy∗)

and
P−1AP =

λ
0
0
B

,
where B = X∗AX is n −1 × n −1. The eigenvalues of B constitute the re-
maining eigenvalues of A (Exercise 7.1.4), and thus an n × n eigenvalue prob-
lem is deﬂated to become one of size n −1 × n −1.
Note: When A is symmetric, we can take x = y to be an eigenvector with
∥x∥2 = 1, so P = R = R−1, and RAR =
 λ
0
0
B

in which B = BT .
An elegant and more geometrical way of expressing diagonalizability is now
presented to help simplify subsequent analyses and pave the way for extensions.

7.2 Diagonalization by Similarity Transformations
517
Spectral Theorem for Diagonalizable Matrices
A matrix An×n with spectrum σ(A) = {λ1, λ2, . . . , λk} is diagonaliz-
able if and only if there exist matrices {G1, G2, . . . , Gk} such that
A = λ1G1 + λ2G2 + · · · + λkGk,
(7.2.7)
where the Gi ’s have the following properties.
•
Gi is the projector onto N (A −λiI) along R (A −λiI).
(7.2.8)
•
GiGj = 0 whenever i ̸= j.
(7.2.9)
•
G1 + G2 + · · · + Gk = I.
(7.2.10)
The expansion (7.2.7) is known as the spectral decomposition of A,
and the Gi ’s are called the spectral projectors associated with A.
Proof.
If A is diagonalizable, and if Xi is a matrix whose columns form a
basis for N (A −λiI), then P =

X1 | X2 | · · · | Xk

is nonsingular. If P−1 is
partitioned in a conformable manner, then we must have
A = PDP−1 =

X1 | X2 | · · · | Xk





λ1I
0
· · ·
0
0
λ2I
· · ·
0
...
...
...
...
0
0
· · ·
λkI













YT
1
YT
2
...
YT
k









= λ1X1YT
1 + λ2X2YT
2 + · · · + λkXkYT
k
= λ1G1 + λ2G2 + · · · + λkGk.
(7.2.11)
For Gi = XiYT
i , the statement PP−1 = I translates to k
i=1 Gi = I, and
P−1P = I
=⇒
YT
i Xj =
	
I
when i = j,
0
when i ̸= j,
=⇒
	
G2
i = Gi,
GiGj = 0
when i ̸= j.
To establish that R (Gi) = N (A −λiI), use R (AB) ⊆R (A) (Exercise 4.2.12)
and YT
i Xi = I to write
R (Gi) = R(XiYT
i ) ⊆R (Xi) = R(XiYT
i Xi) = R(GiXi) ⊆R (Gi).
Thus R (Gi) = R (Xi) = N (A −λiI). To show N (Gi) = R (A −λiI), use
A = k
j=1 λjGj with the already established properties of the Gi ’s to conclude
Gi(A −λiI) = Gi


k

j=1
λjGj −λi
k

j=1
Gj

= 0
=⇒
R (A −λiI) ⊆N (Gi).

518
Chapter 7
Eigenvalues and Eigenvectors
But we already know that N (A −λiI) = R (Gi), so
dim R (A −λiI) = n −dim N (A −λiI) = n −dim R (Gi) = dim N (Gi),
and therefore, by (4.4.6), R (A −λiI) = N (Gi). Conversely, if there exist ma-
trices Gi satisfying (7.2.8)–(7.2.10), then A must be diagonalizable. To see
this, note that (7.2.8) insures dim R (Gi) = dim N (A −λiI) = geo multA (λi) ,
while (7.2.9) implies R (Gi) ∩R (Gj) = 0 and R
 k
i=1 Gi

= k
i=1 R (Gi)
(Exercise 5.9.17). Use these with (7.2.10) in the formula for the dimension of a
sum (4.4.19) to write
n = dim R (I) = dim R (G1 + G2 + · · · + Gk)
= dim [R (G1) + R (G2) + · · · + R (Gk)]
= dim R (G1) + dim R (G2) + · · · + dim R (Gk)
= geo multA (λ1) + geo multA (λ2) + · · · + geo multA (λk) .
Since geo multA (λi) ≤alg multA (λi) and k
i=1 alg multA (λi) = n, the above
equation insures that geo multA (λi) = alg multA (λi) for each i, and, by
(7.2.5), this means A is diagonalizable.
Simple Eigenvalues and Projectors
If x and y∗are respective right-hand and left-hand eigenvectors asso-
ciated with a simple eigenvalue λ ∈σ (A) , then
G = xy∗/y∗x
(7.2.12)
is the projector onto N (A −λI) along R (A −λI). In the context
of the spectral theorem (p. 517), this means that G is the spectral
projector associated with λ.
Proof.
It’s not diﬃcult to prove y∗x ̸= 0 (Exercise 7.2.23), and it’s clear that
G is a projector because G2 = x(y∗x)y∗/(y∗x)2 = G. Now determine R (G).
The image of any z is Gz = αx with α = y∗z/y∗x, so
R (G) ⊆span {x} = N (A −λI)
and
dim R (G) = 1 = dim N (A −λI).
Thus R (G) = N (A −λI). To ﬁnd N (G), recall N (G) = R (I −G) (see
(5.9.11), p. 386), and observe that y∗(A −λI) = 0
=⇒
y∗(I −G) = 0, so
R (A −λI)⊥⊆R (I −G)⊥= N (G)⊥=⇒N (G) ⊆R (A −λI) (Exercise 5.11.5).
But dim N (G) = n−dim R (G) =n−1 =n−dim N (A −λI) = dim R (A −λI),
so N (G) = R (A −λI).

7.2 Diagonalization by Similarity Transformations
519
Example 7.2.7
Problem: Determine the spectral projectors for A =

1
−4
−4
8
−11
−8
−8
8
5

.
Solution: This is the diagonalizable matrix from Example 7.2.1 (p. 507). Since
there are two distinct eigenvalues, λ1 = 1 and λ2 = −3, there are two spectral
projectors,
G1 = the projector onto N (A −1I) along R (A −1I),
G2 = the projector onto N (A + 3I) along R (A + 3I).
There are several diﬀerent ways to ﬁnd these projectors.
1.
Compute bases for the necessary nullspaces and ranges, and use (5.9.12).
2.
Compute Gi = XiYT
i
as described in (7.2.11). The required computations
are essentially the same as those needed above. Since much of the work has
already been done in Example 7.2.1, let’s complete the arithmetic. We have
P =


1
1
1
2
1
0
−2
0
1

=

X1 | X2

,
P−1 =


1
−1
−1
−2
3
2
2
−2
−1

=
 YT
1
YT
2
!
,
so
G1 = X1YT
1 =


1
−1
−1
2
−2
−2
−2
2
2

,
G2 = X2YT
2 =


0
1
1
−2
3
2
2
−2
−1

.
Check that these are correct by conﬁrming the validity of (7.2.7)–(7.2.10).
3.
Since λ1 = 1 is a simple eigenvalue, (7.2.12) may be used to compute G1
from any pair of associated right-hand and left-hand eigenvectors x and yT .
Of course, P and P−1 are not needed to determine such a pair, but since P
and P−1 have been computed above, we can use X1 and YT
1 to make the
point that any right-hand and left-hand eigenvectors associated with λ1 = 1
will do the job because they are all of the form x = αX1 and yT = βYT
1
for α ̸= 0 ̸= β. Consequently,
G1 = xyT
yT x =
α


1
2
−2

β ( 1
−1
−1 )
αβ
=


1
−1
−1
2
−2
−2
−2
2
2

.
Invoking (7.2.10) yields the other spectral projector as G2 = I −G1.
4.
An even easier solution is obtained from the spectral theorem by writing
A −I = (1G1 −3G2) −(G1 + G2) = −4G2,
A + 3I = (1G1 −3G2) + 3 (G1 + G2) = 4G1,

520
Chapter 7
Eigenvalues and Eigenvectors
so that
G1 = (A + 3I)
4
and
G2 = −(A −I)
4
.
Can you see how to make this rather ad hoc technique work in more general
situations?
5.
In fact, the technique above is really a special case of a completely general
formula giving each Gi as a function A and λi as
Gi =
k"
j=1
j̸=i
(A −λjI)
k"
j=1
j̸=i
(λi −λj)
.
This “interpolation formula” is developed on p. 529.
Below is a summary of the facts concerning diagonalizability.
Summary of Diagonalizability
For an n × n matrix A with spectrum σ(A) = {λ1, λ2, . . . , λk} , the
following statements are equivalent.
•
A is similar to a diagonal matrix—i.e., P−1AP = D.
•
A has a complete linearly independent set of eigenvectors.
•
Every λi is semisimple—i.e., geo multA (λi) = alg multA (λi) .
•
A = λ1G1 + λ2G2 + · · · + λkGk, where
▷
Gi is the projector onto N (A −λiI) along R (A −λiI),
▷
GiGj = 0 whenever i ̸= j,
▷
G1 + G2 + · · · + Gk = I,
▷
Gi =
k"
j=1
j̸=i
(A −λjI)
# k"
j=1
j̸=i
(λi −λj)
(see (7.3.11) on p. 529).
▷If λi is a simple eigenvalue associated with right-hand and left-
hand eigenvectors x and y∗, respectively, then Gi = xy∗/y∗x.
Exercises for section 7.2
7.2.1. Diagonalize A =
 −8
−6
12
10

with a similarity transformation, or else
explain why A can’t be diagonalized.

7.2 Diagonalization by Similarity Transformations
521
7.2.2. (a)
Verify that alg multA (λ) = geo multA (λ) for each eigenvalue of
A =


−4
−3
−3
0
−1
0
6
6
5

.
(b)
Find a nonsingular P such that P−1AP is a diagonal matrix.
7.2.3. Show that similar matrices need not have the same eigenvectors by
giving an example of two matrices that are similar but have diﬀerent
eigenspaces.
7.2.4. λ = 2 is an eigenvalue for A =

3
2
1
0
2
0
−2
−3
0

. Find alg multA (λ) as
well as geo multA (λ) . Can you conclude anything about the diagonal-
izability of A from these results?
7.2.5. If B = P−1AP, explain why Bk = P−1AkP.
7.2.6. Compute limn→∞An for A =
 7/5
1/5
−1
1/2

.
7.2.7. Let {x1, x2, . . . , xt} be a set of linearly independent eigenvectors for
An×n associated with respective eigenvalues {λ1, λ2, . . . , λt} , and let
X be any n × (n −t) matrix such that Pn×n =

x1 | · · · | xt | X

is
nonsingular. Prove that if P−1 =




y∗
1
...
y∗
t
Y∗



, where the y∗
i ’s are rows
and Y∗is (n −t) × n, then {y∗
1, y∗
2, . . . , y∗
t } is a set of linearly inde-
pendent left-hand eigenvectors associated with {λ1, λ2, . . . , λt} , respec-
tively (i.e., y∗
i A = λiy∗
i ).
7.2.8. Let A be a diagonalizable matrix, and let ρ(⋆) denote the spectral
radius (recall Example 7.1.4 on p. 497). Prove that limk→∞Ak = 0 if
and only if ρ(A) < 1. Note: It is demonstrated on p. 617 that this
result holds for nondiagonalizable matrices as well.
7.2.9. Apply the technique used to prove Schur’s triangularization theorem
(p. 508) to construct an orthogonal matrix P such that PT AP is
upper triangular for A =
 13
−9
16
−11

.

522
Chapter 7
Eigenvalues and Eigenvectors
7.2.10. Verify the Cayley–Hamilton theorem for A =

1
−4
−4
8
−11
−8
−8
8
5

.
Hint: This is the matrix from Example 7.2.1 on p. 507.
7.2.11. Since each row sum in the following symmetric matrix A is 4, it’s clear
that x = (1, 1, 1, 1)T
is both a right-hand and left-hand eigenvector
associated with λ = 4 ∈σ (A) . Use the deﬂation technique of Example
7.2.6 (p. 516) to determine the remaining eigenvalues of
A =



1
0
2
1
0
2
1
1
2
1
1
0
1
1
0
2


.
7.2.12. Explain why AGi = GiA = λiGi for the spectral projector Gi asso-
ciated with the eigenvalue λi of a diagonalizable matrix A.
7.2.13. Prove that A = cn×1dT
1×n is diagonalizable if and only if dT c ̸= 0.
7.2.14. Prove that A =
 W
0
0
Z

is diagonalizable if and only if Ws×s and
Zt×t are each diagonalizable.
7.2.15. Prove that if AB = BA, then A and B can be simultaneously tri-
angularized by a unitary similarity transformation—i.e., U∗AU = T1
and U∗BU = T2 for some unitary matrix U. Hint: Recall Exercise
7.1.20 (p. 503) along with the development of Schur’s triangularization
theorem (p. 508).
7.2.16. For diagonalizable matrices, prove that AB = BA if and only if A
and B can be simultaneously diagonalized—i.e., P−1AP = D1 and
P−1BP = D2 for some P. Hint: If A and B commute, then so do
P−1AP =
 λ1I
0
0
D

and P−1BP =
 W
X
Y
Z

.
7.2.17. Explain why the following “proof” of the Cayley–Hamilton theorem is
not valid. p(λ) = det (A −λI) =⇒p(A) = det (A −AI) = det (0) = 0.
7.2.18. Show that the eigenvalues of the ﬁnite diﬀerence matrix (p. 19)
A =






2
−1
−1
2
−1
...
...
...
−1
2
−1
−1
2






n×n
are λj = 4 sin2
jπ
2(n + 1), 1 ≤j ≤n.

7.2 Diagonalization by Similarity Transformations
523
7.2.19. Let N =




0
1
...
...
...
1
0




n×n
.
(a)
Show that λ ∈σ

N + NT 
if and only if iλ ∈σ

N −NT 
.
(b)
Explain why N + NT is nonsingular if and only if n is even.
(c)
Evaluate det

N −NT 
/det

N + NT 
when n is even.
7.2.20. A Toeplitz matrix having the form
C =






c0
cn−1
cn−2
· · ·
c1
c1
c0
cn−1
· · ·
c2
c2
c1
c0
· · ·
c3
...
...
...
...
...
cn−1
cn−2
cn−3
· · ·
c0






n×n
is called a circulant matrix. If p(x) = c0 + c1x + · · · + cn−1xn−1,
and if {1, ξ, ξ2, . . . , ξn−1} are the nth roots of unity, then the results
of Exercise 5.8.12 (p. 379) insure that
FnCF−1
n
=




p(1)
0
· · ·
0
0
p(ξ)
· · ·
0
...
...
...
...
0
0
· · ·
p(ξn−1)




in which Fn is the Fourier matrix of order n. Verify these facts for the
circulant below by computing its eigenvalues and eigenvectors directly:
C =



1
0
1
0
0
1
0
1
1
0
1
0
0
1
0
1


.
7.2.21. Suppose that (λ, x) and (µ, y∗) are right-hand and left-hand eigen-
pairs for A ∈ℜn×n —i.e., Ax = λx and y∗A = µy∗. Explain why
y∗x = 0 whenever λ ̸= µ.
7.2.22. Consider A ∈ℜn×n.
(a)
Show that if A is diagonalizable, then there are right-hand and
left-hand eigenvectors x and y∗associated with λ ∈σ (A)
such that y∗x ̸= 0 so that we can make y∗x = 1.
(b)
Show that not every right-hand and left-hand eigenvector x and
y∗associated with λ ∈σ (A) must satisfy y∗x ̸= 0.
(c)
Show that (a) need not be true when A is not diagonalizable.

524
Chapter 7
Eigenvalues and Eigenvectors
7.2.23. Consider A ∈ℜn×n with λ ∈σ (A) .
(a)
Prove that if λ is simple, then y∗x ̸= 0 for every pair of respec-
tive right-hand and left-hand eigenvectors x and y∗associated
with λ regardless of whether or not A is diagonalizable. Hint:
Use the core-nilpotent decomposition on p. 397.
(b)
Show that y∗x = 0 is possible when λ is not simple.
7.2.24. For A ∈ℜn×n with σ (A) = {λ1, λ2, . . . , λk} , show A is diagonaliz-
able if and only if ℜn = N (A −λ1I)⊕N (A −λ2I)⊕· · ·⊕N (A −λkI).
Hint: Recall Exercise 5.9.14.
7.2.25. The Real Schur Form. Schur’s triangularization theorem (p. 508)
insures that every square matrix A is unitarily similar to an upper-
triangular matrix—say, U∗AU = T. But even when A is real, U
and T may have to be complex if A has some complex eigenvalues.
However, the matrices (and the arithmetic) can be constrained to be real
by settling for a block-triangular result with 2 × 2 or scalar entries on
the diagonal. Prove that for each A ∈ℜn×n there exists an orthogonal
matrix P ∈ℜn×n and real matrices Bij such that
PT AP =



B11
B12
· · ·
B1k
0
B22
· · ·
B2k
...
...
...
...
0
0
· · ·
Bkk


,
where Bjj is 1 × 1 or 2 × 2.
If Bjj = [λj] is 1 × 1, then λj ∈σ (A) , and if Bjj is 2 × 2, then
σ (Bjj) = {λj, λj} ⊆σ (A) .
7.2.26. When A ∈ℜn×n is diagonalizable by a similarity transformation S,
then S may have to be complex if A has some complex eigenvalues.
Analogous to Exercise 7.2.25, we can stay in the realm of real numbers
by settling for a block-diagonal result with 1 × 1 or 2 × 2 entries on the
diagonal. Prove that if A ∈ℜn×n is diagonalizable with real eigenvalues
{ρ1, . . . , ρr} and complex eigenvalues {λ1, λ1, λ2, λ2, . . . , λt, λt} with
2t+r = n, then there exists a nonsingular P ∈ℜn×n and Bj ’s ∈ℜ2×2
such that
P−1AP =



D
0
· · ·
0
0
B1
· · ·
0
...
...
...
...
0
0
· · ·
Bt


,
where
D =



ρ1
0
· · ·
0
0
ρ2
· · ·
0
...
...
...
...
0
0
· · ·
ρr


,
and where Bj has eigenvalues λj and λj.
7.2.27. For A ∈Cn×n, prove that x∗Ax = 0 for all x ∈Cn×1 ⇒A = 0. Show
that xT Ax = 0 for all x ∈ℜn×1 ̸⇒A = 0, even if A is real.

7.3 Functions of Diagonalizable Matrices
525
7.3
FUNCTIONS OF DIAGONALIZABLE MATRICES
For square matrices A, what should it mean to write sin A, eA, ln A, etc.?
A naive approach might be to simply apply the given function to each entry of
A such as
sin

a11
a12
a21
a22

?=

sin a11
sin a12
sin a21
sin a22

.
(7.3.1)
But doing so results in matrix functions that fail to have the same properties as
their scalar counterparts. For example, since sin2 x + cos2 x = 1 for all scalars
x, we would like our deﬁnitions of sin A and cos A to result in the analogous
matrix identity sin2 A + cos2 A = I for all square matrices A. The entrywise
approach (7.3.1) clearly fails in this regard.
One way to deﬁne matrix functions possessing properties consistent with
their scalar counterparts is to use inﬁnite series expansions. For example, consider
the exponential function
ez =
∞

k=0
zk
k! = 1 + z + z2
2! + z3
3! · · · .
(7.3.2)
Formally replacing the scalar argument z by a square matrix A ( z0 = 1 is
replaced with A0 = I ) results in the inﬁnite series of matrices
eA = I + A + A2
2! + A3
3! · · · ,
(7.3.3)
called the matrix exponential. While this results in a matrix that has properties
analogous to its scalar counterpart, it suﬀers from the fact that convergence must
be dealt with, and then there is the problem of describing the entries in the limit.
These issues are handled by deriving a closed form expression for (7.3.3).
If A is diagonalizable, then A = PDP−1 = P diag (λ1, . . . , λn) P−1, and
Ak = PDkP−1 = P diag

λk
1, . . . , λk
n

P−1, so
eA =
∞

k=0
Ak
k! =
∞

k=0
PDkP−1
k!
= P
 ∞

k=0
Dk
k!
!
P−1 = P diag

eλ1, . . . , eλn
P−1.
In other words, we don’t have to use the inﬁnite series (7.3.3) to deﬁne eA.
Instead, deﬁne eD = diag (eλ1, eλ2, . . . , eλn), and set
eA = PeDP−1 = P diag (eλ1, eλ2, . . . , eλn) P−1.
This idea can be generalized to any function f(z) that is deﬁned on the
eigenvalues λi of a diagonalizable matrix A = PDP−1 by deﬁning f(D) to
be f(D) = diag (f(λ1), f(λ2), . . . , f(λn)) and by setting
f(A) = Pf(D)P−1 = P diag (f(λ1), f(λ2), . . . , f(λn)) P−1.
(7.3.4)

526
Chapter 7
Eigenvalues and Eigenvectors
At ﬁrst glance this deﬁnition seems to have an edge over the inﬁnite series ap-
proach because there are no convergence issues to deal with. But convergence
worries have been traded for uniqueness worries. Because P is not unique, it’s
not apparent that (7.3.4) is well deﬁned. The eigenvector matrix P you compute
for a given A need not be the same as the eigenvector matrix I compute, so what
insures that your f(A) will be the same as mine? The spectral theorem (p. 517)
does. Suppose there are k distinct eigenvalues that are grouped according to
repetition, and expand (7.3.4) just as (7.2.11) is expanded to produce
f(A) = PDP−1 =

X1|X2| · · · |Xk





f(λ1)I
0
· · ·
0
0
f(λ2)I
· · ·
0
...
...
...
...
0
0
· · ·
f(λk)I













YT
1
YT
2
...
YT
k









=
k

i=1
f(λi)XiYT
i =
k

i=1
f(λi)Gi.
Since Gi is the projector onto N (A −λiI) along R (A −λiI), Gi is uniquely
determined by A. Therefore, (7.3.4) uniquely deﬁnes f(A) regardless of the
choice of P. We can now make a formal deﬁnition.
Functions of Diagonalizable Matrices
Let A = PDP−1 be a diagonalizable matrix where the eigenvalues in
D = diag (λ1I, λ2I, . . . , λkI) are grouped by repetition. For a function
f(z) that is deﬁned at each λi ∈σ (A) , deﬁne
f(A) = Pf(D)P−1 = P




f(λ1)I
0
· · ·
0
0
f(λ2)I
· · ·
0
...
...
...
...
0
0
· · ·
f(λk)I



P−1 (7.3.5)
= f(λ1)G1 + f(λ2)G2 + · · · + f(λk)Gk,
(7.3.6)
where Gi is the ith spectral projector as described on pp. 517, 529.
The generalization to nondiagonalizable matrices is on p. 603.
The discussion of matrix functions was initiated by considering inﬁnite se-
ries, so, to complete the circle, a formal statement connecting inﬁnite series with
(7.3.5) and (7.3.6) is needed. By replacing A by PDP−1 in ∞
n=0 cn(A−z0I)n
and expanding the result, the following result is established.

7.3 Functions of Diagonalizable Matrices
527
Inﬁnite Series
If f(z) = ∞
n=0 cn(z −z0)n converges when |z −z0| < r, and if
|λi −z0| < r for each eigenvalue λi of a diagonalizable matrix A, then
f(A) =
∞

n=0
cn(A −z0I)n.
(7.3.7)
It can be argued that the matrix series on the right-hand side of (7.3.7)
converges if and only if |λi−z0| < r for each λi, regardless of whether or
not A is diagonalizable. So (7.3.7) serves to deﬁne f(A) for functions
with series expansions regardless of whether or not A is diagonalizable.
More is said in Example 7.9.3 (p. 605).
Example 7.3.1
Neumann Series Revisited. The function f(z) = (1−z)−1 has the geometric
series expansion (1−z)−1 = ∞
k=1 zk that converges if and only if |z| < 1. This
means that the associated matrix function f(A) = (I −A)−1 is given by
(I −A)−1 =
∞

k=0
Ak
if and only if |λ| < 1 for all λ ∈σ (A) .
(7.3.8)
This is the Neumann series discussed on p. 126, where it was argued that
if limn→∞An = 0, then (I −A)−1 = ∞
k=0 Ak. The two approaches are the
same because it turns out that limn→∞An = 0 ⇐⇒|λ| < 1 for all λ ∈σ (A) .
This is immediate for diagonalizable matrices, but the nondiagonalizable case is
a bit more involved—the complete statement is developed on p. 618. Because
maxi |λi| ≤∥A∥for all matrix norms (Example 7.1.4, p. 497), a corollary of
(7.3.8) is that (I −A)−1 exists and
(I −A)−1 =
∞

k=0
Ak
when ∥A∥< 1 for any matrix norm.
(7.3.9)
Caution! (I −A)−1 can exist without the Neumann series expansion being
valid because all that’s needed for I −A to be nonsingular is 1 /∈σ (A) , while
convergence of the Neumann series requires each |λ| < 1.

528
Chapter 7
Eigenvalues and Eigenvectors
Example 7.3.2
Eigenvalue Perturbations. It’s often important to understand how the eigen-
values of a matrix are aﬀected by perturbations. In general, this is a complicated
issue, but for diagonalizable matrices the problem is more tractable.
Problem: Suppose B = A+E, where A is diagonalizable, and let β ∈σ (B) .
If P−1AP = D = diag (λ1, λ2, . . . , λn) , explain why
min
λi∈σ(A) |β −λi| ≤κ(P) ∥E∥,
where
κ(P) = ∥P∥∥P−1∥
(7.3.10)
for matrix norms satisfying ∥D∥= maxi |λi| (e.g., any standard induced norm).
Solution: Assume β ̸∈σ (A) —(7.3.10) is trivial if β ∈σ (A) —and observe
that
(βI −A)−1(βI −B) = (βI −A)−1(βI −A −E) = I −(βI −A)−1E
implies that 1 ≤∥(βI−A)−1E∥—otherwise I−(βI−A)−1E is nonsingular by
(7.3.9), which is impossible because (βI −B) (and hence (βI −A)−1(βI −B)
is singular). Consequently,
1 ≤∥(βI −A)−1E∥= ∥P(βI −D)−1P−1E∥≤∥P∥∥(βI −D)−1∥∥P−1∥∥E∥
= κ(P) ∥E∥max
i
|β −λi|−1 = κ(P) ∥E∥
1
mini |β −λi|,
and this produces (7.3.10). Similar to the case of linear systems (Example 5.12.1,
p. 414), the expression κ(P) is a condition number in the sense that if κ(P) is
relatively small, then the λi ’s are relatively insensitive, but if κ(P) is relatively
large, we must be suspicious. Note: Because it’s a corollary of their 1960 results,
the bound (7.3.10) is often referred to as the Bauer–Fike bound.
Inﬁnite series representations can always be avoided because every func-
tion of An×n can be expressed as a polynomial in A. In other words, when
f(A) exists, there is a polynomial p(z) such that p(A) = f(A). This is
true for all matrices, but the development here is limited to diagonalizable
matrices—nondiagonalizable matrices are treated in Exercise 7.3.7. In the di-
agonalizable case, f(A) exists if and only if f(λi) exists for each λi ∈σ (A) =
{λ1, λ2, . . . , λk} , and, by (7.3.6), f(A) = k
i=1 f(λi)Gi, where Gi is the ith
spectral projector. Any polynomial p(z) agreeing with f(z) on σ (A) does the
job because if p(λi) = f(λi) for each λi ∈σ (A) , then
p(A) =
k

i=1
p(λi)Gi =
k

i=1
f(λi)Gi = f(A).

7.3 Functions of Diagonalizable Matrices
529
But is there always a polynomial satisfying p(λi) = f(λi) for each λi ∈σ (A)?
Sure—that’s what the Lagrange interpolating polynomial from Example 4.3.5
(p. 186) does. It’s given by
p(z)=
k

i=1






f(λi)
k"
j=1
j̸=i
(z −λj)
k"
j=1
j̸=i
(λi −λj)






, so f(A)=p(A)=
k

i=1






f(λi)
k"
j=1
j̸=i
(A −λjI)
k"
j=1
j̸=i
(λi −λj)






.
Using the function gi(z) =
	
1
if z = λi,
0
if z ̸= λi, with this representation as well as
that in (7.3.6) yields
k"
j=1
j̸=i
(A −λjI)
# k"
j=1
j̸=i
(λi −λj) = gi(A) = Gi. For example,
if σ (An×n) = {λ1, λ2, λ3}, then f(A) = f(λ1)G1 + f(λ2)G2 + f(λ3)G3 with
G1 = (A−λ2I)(A−λ3I)
(λ1−λ2)(λ1−λ3) ,
G2 = (A−λ1I)(A−λ3I)
(λ2−λ1)(λ2−λ3) ,
G3 = (A−λ1I)(A−λ2I)
(λ3−λ1)(λ3−λ2) .
Below is a summary of these observations.
Spectral Projectors
If A is diagonalizable with σ (A) = {λ1, λ2, . . . , λk} , then the spectral
projector onto N (A −λiI) along R (A −λiI) is given by
Gi =
k
$
j=1
j̸=i
(A −λjI)
# k
$
j=1
j̸=i
(λi −λj)
for i = 1, 2, . . . , k.
(7.3.11)
Consequently, if f(z) is deﬁned on σ (A) , then f(A) = k
i=1 f(λi)Gi
is a polynomial in A of degree at most k −1.
Example 7.3.3
Problem: For a scalar t, determine the matrix exponential eAt, where
A =

−α
β
α
−β

with α + β ̸= 0.
Solution 1: The characteristic equation for A is λ2 + (α + β)λ = 0, so the
eigenvalues of A are λ1 = 0 and λ2 = −(α+β). Note that A is diagonalizable

530
Chapter 7
Eigenvalues and Eigenvectors
because no eigenvalue is repeated—recall (7.2.6). Using the function f(z) = ezt,
the spectral representation (7.3.6) says that
eAt = f(A) = f(λ1)G1 + f(λ2)G2 = eλ1tG1 + eλ2tG2.
The spectral projectors G1 and G2 are determined from (7.3.11) to be
G1 = A −λ2I
−λ2
=
1
α + β

β
β
α
α

and
G2 = A
λ2
=
1
α + β

α
−β
−α
β

,
so
eAt = G1 + e−(α+β)tG2 =
1
α + β
%
β
β
α
α

+ e−(α+β)t

α
−β
−α
β
&
.
Solution 2: Compute eigenpairs (λ1, x1) and (λ2, x2), construct P =

x1 | x2

,
and compute
eAt = P

f(λ1)
0
0
f(λ2)

P−1 = P

eλ1t
0
0
eλ2t

P−1.
The computational details are called for in Exercise 7.3.2.
Example 7.3.4
Problem: For T =
 1/2
1/2
1/4
3/4

, evaluate limk→∞Tk.
Solution 1: Compute two eigenpairs, λ1 = 1, x1 = (1, 1)T , and λ2 = 1/4,
x2 = (−2, 1)T . If P = [x1 | x2], then T = P
 1
0
0
1/4

P−1, so
Tk = P

1k
0
0
1/4k

P−1 →P

1
0
0
0

P−1 = 1
3

1
2
1
2

.
(7.3.12)
Solution 2: We know from (7.3.6) that Tk = 1kG1 + (1/4)kG2 →G1. Since
λ1 = 1 is a simple eigenvalue, formula (7.2.12) on p. 518 can be used to compute
G1 = x1yT
1 /yT
1 x1, where x1 and yT
1 are any right- and left-hand eigenvectors
associated with λ1 = 1. A right-hand eigenvector x1 was computed above.
Computing a left-hand eigenvector yT
1 = (1, 2) yields
Tk →G1 = x1yT
1
yT
1 x1
= 1
3

1
2
1
2

.
(7.3.13)

7.3 Functions of Diagonalizable Matrices
531
Example 7.3.5
Population Migration. Suppose that the population migration between two
geographical regions—say, the North and the South—is as follows. Each year,
50% of the population in the North migrates to the South, while only 25% of
the population in the South moves to the North. This situation is depicted by
drawing a transition diagram as shown below in Figure 7.3.1.
N
S
.25
.5
.5
.75
Figure 7.3.1
Problem: If this migration pattern continues, will the population in the North
continually shrink until the entire population is eventually in the South, or will
the population distribution somehow stabilize before the North is completely
deserted?
Solution: Let nk and sk denote the respective proportions of the total popula-
tion living in the North and South at the end of year k, and assume nk+sk = 1.
The migration pattern dictates that the fractions of the population in each region
at the end of year k + 1 are
'
nk+1 = nk(.5) + sk(.25)
sk+1 = nk(.5) + sk(.75)
(
or, equivalently,
pT
k+1 = pT
k T,
(7.3.14)
where pT
k = ( nk
sk ) and pT
k+1 = ( nk+1
sk+1 ) are the respective population
distributions at the end of years k and k + 1, and where
T =
 N
S
N
.5
.5
S
.25
.75

is the associated transition matrix (recall Example 3.6.3). Inducting on
pT
1 = pT
0 T,
pT
2 = pT
1 T = pT
0 T2,
pT
3 = pT
2 T = pT
0 T3,
· · ·
leads to pT
k = pT
0 Tk, which indicates that the powers of T determine how the
process evolves. Determining the long-run population distribution
73 is therefore
73
The long-run distribution goes by a lot of diﬀerent names. It’s also called the limiting distri-
bution, the steady-state distribution, and the stationary distribution.

532
Chapter 7
Eigenvalues and Eigenvectors
accomplished by analyzing limk→∞Tk. The results of Example 7.3.4 together
with n0 + s0 = 1 yield the long-run (or limiting) population distribution as
pT
∞= lim
k→∞pT
k = lim
k→∞pT
0 Tk = pT
0 lim
k→∞Tk = ( n0
s0 )

1/3
2/3
1/3
2/3

=
 n0 + s0
3
2(n0 + s0)
3

=
 1
3
2
3

.
So if the migration pattern continues to hold, then the population distribution
will eventually stabilize with 1/3 of the population being in the North and 2/3 of
the population in the South. And this is independent of the initial distribution!
Observations: This is an example of a broader class of evolutionary processes
known as Markov chains (p. 687), and the following observations are typical.
•
It’s clear from (7.3.12) or (7.3.13) that the rate at which the population
distribution stabilizes is governed by how fast (1/4)k →0. In other words,
the magnitude of the largest subdominant eigenvalue of T determines the
rate of evolution.
•
For the dominant eigenvalue λ1 = 1, the column, x1, of 1’s is a right-
hand eigenvector (because T has unit row sums). This forces the limiting
distribution pT
∞to be a particular left-hand eigenvector associated with
λ1 = 1 because for an arbitrary left-hand eigenvector yT
1
associated with
λ1 = 1, equation (7.3.13) in Example 7.3.4 insures that
pT
∞= lim
k→∞pT
0 Tk = pT
0 lim
k→∞Tk = pT
0 G1 = (pT
0 x1)yT
1
yT
1 x1
=
yT
1
yT
1 x1
.
(7.3.15)
The fact that pT
0 Tk converges to an eigenvector is a special case of the
power method discussed in Example 7.3.7.
•
Equation (7.3.15) shows why the initial distribution pT
0 always drops away
in the limit. But pT
0
is not completely irrelevant because it always aﬀects
the transient behavior—i.e., the behavior of pT
k = pT
0 Tk for smaller k ’s.
Example 7.3.6
Cayley–Hamilton Revisited. The Cayley–Hamilton theorem (p. 509) says
that if p(λ) = 0 is the characteristic equation for A, then p(A) = 0. This is
evident for diagonalizable A because p(λi) = 0 for each λi ∈σ (A) , so, by
(7.3.6), p(A) = p(λ1)G1 + p(λ2)G2 + · · · + p(λk)Gk = 0.
Problem: Establish the Cayley–Hamilton theorem for nondiagonalizable matri-
ces by using the diagonalizable result together with a continuity argument.
Solution: Schur’s triangularization theorem (p. 508) insures An×n = UTU∗
for a unitary U and an upper triangular T having the eigenvalues of A on the

7.3 Functions of Diagonalizable Matrices
533
diagonal. For each ϵ ̸= 0, it’s possible to ﬁnd numbers ϵi such that (λ1 + ϵ1),
(λ2 + ϵ2), . . . , (λn + ϵn) are distinct and  ϵ2
i = |ϵ|. Set
D(ϵ) = diag (ϵ1, ϵ2, . . . , ϵn)
and
B(ϵ) = U

T + D(ϵ)

U∗= A + E(ϵ),
where E(ϵ) = UD(ϵ)U∗. The (λi + ϵi) ’s are the eigenvalues of B(ϵ) and
they are distinct, so B(ϵ) is diagonalizable—by (7.2.6). Consequently, B(ϵ)
satisﬁes its own characteristic equation 0 = pϵ(λ) = det (A + E(ϵ) −λI) for
each ϵ ̸= 0. The coeﬃcients of pϵ(λ) are continuous functions of the entries in
E(ϵ) (recall (7.1.6)) and hence are continuous functions of the ϵi ’s. Combine
this with limϵ→0 E(ϵ) = 0 to obtain 0 = limϵ→0 pϵ(B(ϵ)) = p(A).
Note:
Embedded in the above development is the fact that every square com-
plex matrix is arbitrarily close to some diagonalizable matrix because for each
ϵ ̸= 0, we have ∥A −B(ϵ)∥F = ∥E(ϵ)∥F = ϵ (recall Exercise 5.6.9).
Example 7.3.7
Power method
74 is an iterative technique for computing a dominant eigenpair
(λ1, x) of a diagonalizable A ∈ℜm×m with eigenvalues
|λ1| > |λ2| ≥|λ3| ≥· · · ≥|λk|.
Note that this implies λ1 is real—otherwise λ1 is another eigenvalue with the
same magnitude as λ1. Consider f(z) = (z/λ1)n, and use the spectral repre-
sentation (7.3.6) along with |λi/λ1| < 1 for i = 2, 3, . . . , k to conclude that
 A
λ1
n
= f(A) = f(λ1)G1 + f(λ2)G2 + · · · + f(λk)Gk
= G1 +
λ2
λ1
n
G2 + · · · +
λk
λ1
n
Gk →G1
(7.3.16)
as n →∞. Consequently, (Anx0/λn
1) →G1x0 ∈N (A −λ1I) for all x0. So if
G1x0 ̸= 0 or, equivalently, x0 /∈R (A −λ1I), then Anx0/λn
1 converges to an
eigenvector associated with λ1. This means that the direction of Anx0 tends
toward the direction of an eigenvector because λn
1 acts only as a scaling factor
to keep the length of Anx0 under control. Rather than using λn
1, we can scale
Anx0 with something more convenient. For example, ∥Anx0∥(for any vector
norm) is a reasonable scaling factor, but there are even better choices. For vectors
v, let m(v) denote the component of maximal magnitude, and if there is more
74
While the development of the power method was considered to be a great achievement when
R. von Mises introduced it in 1929, later algorithms relegated its computational role to that of
a special purpose technique. Nevertheless, it’s still an important idea because, in some way or
another, most practical algorithms for eigencomputations implicitly rely on the mathematical
essence of the power method.

534
Chapter 7
Eigenvalues and Eigenvectors
than one maximal component, let m(v) be the ﬁrst maximal component—e.g.,
m(1, 3, −2) = 3, and m(−3, 3, −2) = −3. It’s clear that m(αv) = αm(v) for
all scalars α. Suppose m(Anx0/λn
1) →γ. Since (An/λn
1) →G1, we see that
lim
n→∞
Anx0
m(Anx0) = lim
n→∞
(An/λn
1)x0
m(Anx0/λn
1) = G1x0
γ
= x
is an eigenvector associated with λ1. But rather than successively powering
A, the sequence Anx0/m(Anx0) is more eﬃciently generated by starting with
x0 /∈R (A −λ1I) and setting
yn = Axn,
νn = m(yn),
xn+1 = yn
νn
,
for n = 0, 1, 2, . . . .
(7.3.17)
Not only does xn →x, but as a bonus we get νn →λ1 because for all n,
Axn+1 = A2xn/νn, so if νn →ν as n →∞, the limit on the left-hand side
is Ax = λ1x, while the limit on the right-hand side is A2x/ν = λ2
1x/ν. Since
these two limits must agree, λ1x = (λ2
1/ν)x, and this implies ν = λ1.
Summary. The sequence (νn, xn) deﬁned by (7.3.17) converges to an eigenpair
(λ1, x) for A provided that G1x0 ̸= 0 or, equivalently, x0 /∈R (A −λ1I).
▷
Advantages. Each iteration requires only one matrix–vector product, and
this can be exploited to reduce the computational eﬀort when A is large
and sparse—assuming that a dominant eigenpair is the only one of interest.
▷
Disadvantages. Only a dominant eigenpair is determined—something else
must be done if others are desired. Furthermore, it’s clear from (7.3.16) that
the rate at which (7.3.17) converges depends on how fast (λ2/λ1)n →0, so
convergence is slow when |λ1| is close to |λ2|.
Example 7.3.8
Inverse Power Method. Given a real approximation α /∈σ(A) to any real
λ ∈σ(A), this algorithm (also called the inverse iteration) determines an
eigenpair (λ, x) for a diagonalizable matrix A ∈ℜm×m by applying the power
method
75 to B = (A −αI)−1. Recall from Exercise 7.1.9 that
x is an eigenvector for A ⇐⇒x is an eigenvector for B,
λ ∈σ(A) ⇐⇒(λ −α)−1 ∈σ(B).
(7.3.18)
If |λ −α| < |λi −α| for all other λi ∈σ(A), then (λ −α)−1 is the dominant
eigenvalue of B because |λ −α|−1 > |λi −α|−1. Therefore, applying the power
75
The relation between the power method and inverse iteration is clear to us now, but it originally
took 15 years to make the connection. Inverse iteration was not introduced until 1944 by the
German mathematician Helmut Wielandt (1910–2001).

7.3 Functions of Diagonalizable Matrices
535
method to B produces an eigenpair

(λ −α)−1, x

for B from which the
eigenpair (λ, x) for A is determined. That is, if x0 /∈R (B −λI), and if
yn = Bxn = (A −αI)−1xn,
νn = m(yn),
xn+1 = yn
νn
for n = 0, 1, 2, . . . ,
then (νn, xn) →

(λ −α)−1, x

, an eigenpair for B, so (7.3.18) guarantees that
(ν−1
n +α, xn) →(λ, x), an eigenpair for A. Rather than using matrix inversion
to compute yn = (A −αI)−1xn, it’s more eﬃcient to solve the linear system
(A −αI)yn = xn for yn. Because this is a system in which the coeﬃcient
matrix remains the same from step to step, the eﬃciency is further enhanced by
computing an LU factorization of (A −αI) at the outset so that at each step
only one forward solve and one back solve (as described on pp. 146 and 153) are
needed to determine yn.
▷
Advantages. Striking results are often obtained (particularly in the case of
symmetric matrices) with only one or two iterations, even when x0 is nearly
in R (B −λI) = R (A −λI). For α close to λ, computing an accurate
ﬂoating-point solution of (A −αI)yn = xn is diﬃcult because A −αI is
nearly singular, and this almost surely guarantees that (A−αI)yn = xn is an
ill-conditioned system. But only the direction of the solution is important,
and the direction of a computed solution is usually reasonable in spite of
conditioning problems. Finally, the algorithm can be adapted to compute
approximations of eigenvectors associated with complex eigenvalues.
▷
Disadvantages. Only one eigenpair at a time is computed, and an approxi-
mate eigenvalue must be known in advance. Furthermore, the rate of conver-
gence depends on how fast [(λ −α)/(λi −α)]n →0, and this can be slow
when there is another eigenvalue λi close to the desired λ. If λi is too
close to λ, roundoﬀerror can divert inverse iteration toward an eigenvector
associated with λi instead of λ in spite of a theoretically correct α.
Note: In the standard version of inverse iteration a constant value of α is used at
each step to approximate an eigenvalue λ, but there is variation called Rayleigh
quotient iteration that uses the current iterate xn to improve the value of α
at each step by setting α = xT
nAxn/xT
nxn. The function R(x) = xT Ax/xT x is
called the Rayleigh quotient. It can be shown that if x is a good approximation to
an eigenvector, then R(x) is a good approximation of the associated eigenvalue.
More is said about this in Example 7.5.1 (p. 549).
Example 7.3.9
The QR Iteration algorithm for computing the eigenvalues of a general ma-
trix came from an elegantly simple idea that was proposed by Heinz Rutishauser
in 1958 and reﬁned by J. F. G. Francis in 1961-1962. The underlying concept is
to alternate between computing QR factors (Rutishauser used LU factors) and

536
Chapter 7
Eigenvalues and Eigenvectors
reversing their order as shown below. Starting with A1 = A ∈ℜn×n,
Factor:
A1 = Q1R1,
Set:
A2 = R1Q1,
Factor:
A2 = Q2R2,
Set:
A3 = R2Q2,
...
In general, Ak+1 = RkQk, where Qk and Rk are the QR factors of Ak.
Notice that if Pk = Q1Q2 · · · Qk, then each Pk is an orthogonal matrix such
that
PT
1 AP1 = QT
1 Q1R1Q1 = A2,
PT
2 AP2 = QT
2 QT
1 AQ1Q2 = QT
2 A2Q2 = A3,
...
PT
k APk = Ak+1.
In other words, A2, A3, A4, . . . are each orthogonally similar to A, and hence
σ (Ak) = σ (A) for each k. But the process does more than just create a matrix
that is similar to A at each step. The magic lies in the fact that if the process
converges, then limk→∞Ak = R is an upper-triangular matrix in which the
diagonal entries are the eigenvalues of A. Indeed, if Pk →P, then
Qk = PT
k−1Pk →PT P = I
and
Rk = Ak+1QT
k →R I = R,
so
lim
k→∞Ak = lim
k→∞QkRk = R,
which is necessarily upper triangular having diagonal entries equal to the eigen-
values of A. However, as is often the case, there is a big gap between theory
and practice, and turning this clever idea into a practical algorithm requires sig-
niﬁcant eﬀort. For example, one obvious hurdle that needs to be overcome is the
fact that the R factor in a QR factorization has positive diagonal entries, so,
unless modiﬁcations are made, the “vanilla” version of the QR iteration can’t
converge for matrices with complex or nonpositive eigenvalues. Laying out all of
the details and analyzing the rigors that constitute the practical implementation
of the QR iteration is tedious and would take us too far astray, but the basic
principals are within our reach.
•
Hessenberg Matrices.
A big step in turning the QR iteration into a prac-
tical method is to realize that everything can be done with upper-Hessenberg
matrices. As discussed in Example 5.7.4 (p. 350), Householder reduction
can be used to produce an orthogonal matrix P such that PT AP = H1,
and Example 5.7.5 (p. 352) shows that Givens reduction easily produces

7.3 Functions of Diagonalizable Matrices
537
the QR factors of any Hessenberg matrix. Givens reduction on H1 pro-
duces the Q factor of H1 as the transposed product of plane rotations
Q1 = PT
12PT
23 · · · PT
(n−1)n, and this is also upper Hessenberg (constructing a
4 × 4 example will convince you). Since multiplication by an upper-triangular
matrix can’t alter the upper-Hessenberg structure, the matrix R1Q1 = H2
at the second step of the QR iteration is again upper Hessenberg, and so
on for each successive step. Being able to iterate with Hessenberg matrices
results in a signiﬁcant reduction of arithmetic. Note that if A = AT , then
Hk = HT
k for each k, which means that each Hk is tridiagonal in structure.
•
Convergence.
When the Hk ’s converge, the entries at the bottom of the
ﬁrst subdiagonal tend to die ﬁrst—i.e., a typical pattern might be
Hk =



∗
∗
∗
∗
∗
∗
∗
∗
0
∗
∗
∗
0
0
ϵ
⋆


.
When ϵ is satisfactorily small, take ⋆(the (n, n)-entry) to be an eigenvalue,
and deﬂate the problem. An even nicer state of aﬀairs is to have a zero (or a
satisfactorily small) entry in row n −1 and column 2 (illustrated below for
n = 4)
Hk =



∗
∗
∗
∗
∗
∗
∗
∗
0
ϵ
⋆
⋆
0
0
⋆
⋆



(7.3.19)
because the trailing 2 × 2 block
 ⋆
⋆
⋆
⋆

will yield two eigenvalues by the
quadratic formula, and thus complex eigenvalues can be revealed.
•
Shifts.
Instead of factoring Hk at the kth step, factor a shifted matrix
Hk −αkI = QkRk, and set Hk+1 = RkQk + αkI, where αk is an ap-
proximate real eigenvalue—a good candidate is αk = [Hk]nn. Notice that
σ (Hk+1) = σ (Hk) because Hk+1 = QT
k HkQk. The inverse power method
is now at work. To see how, drop the subscripts, and write H −αI = QR
as QT = R(H −αI)−1. If α ≈λ ∈σ (H) = σ (A) (say, |λ −α| = ϵ with
α, λ ∈ℜ), then the discussion concerning the inverse power method in Exam-
ple 7.3.8 insures that the rows in QT are close to being left-hand eigenvectors
of H associated with λ. In particular, if qT
n is the last row in QT , then
rnneT
n = eT
nR = qT
nQR = qT
n(H −αI) = qT
nH −αqT
n ≈(λ −α)qT
n,
so rnn = |rnn| ≈
))(λ −α)qT
n
))
2 = ϵ and qT
n ≈±eT
n. The signiﬁcance of this

538
Chapter 7
Eigenvalues and Eigenvectors
is revealed by looking at a generic 4 × 4 pattern for
Hk+1 = RQ + αI
≈


∗
∗
∗
∗
0
∗
∗
∗
0
0
∗
∗
0
0
0
ϵ




∗
∗
∗
0
∗
∗
∗
0
0
∗
∗
0
0
0
⋆
±1

+


α
α
α
α


=


∗
∗
∗
∗
∗
∗
∗
∗
0
∗
∗
∗
0
0
ϵ⋆
α ± ϵ

≈


∗
∗
∗
∗
∗
∗
∗
∗
0
∗
∗
∗
0
0
0
α ± ϵ

.
The strength of the last approximation rests not only on the size of ϵ, but
it is also reinforced by the fact that ⋆≈0 because the 2-norm of the last
row of Q must be 1. This indicates why this technique (called the single
shifted QR iteration) can provide rapid convergence to a real eigenvalue. To
extract complex eigenvalues, a double shift strategy is employed in which the
eigenvalues αk and βk of the lower 2 × 2 block of Hk are used as shifts
as indicated below:
Factor:
Hk −αkI = QkRk,
Set:
Hk+1 = RkQk + αkI
(so Hk+1 = QT
k HkQk),
Factor:
Hk+1 −βkI = Qk+1Rk+1,
Set:
Hk+2 = Rk+1Qk+1 + βkI
(so Hk+2 = QT
k+1QT
k HkQkQk+1),
...
The nice thing about the double shift strategy is that even when αk is
complex (so that βk = αk) the matrix QkQk+1 (and hence Hk+2) is
real, and there are eﬃcient ways to form QkQk+1 by computing only the
ﬁrst column of the product. The double shift method typically requires very
few iterations (using only real arithmetic) to produce a small entry in the
(n −2, 2)-position as depicted in (7.3.19) for a generic 4 × 4 pattern.
Exercises for section 7.3
7.3.1. Determine cos A for A =
 −π/2
π/2
π/2
−π/2

.
7.3.2. For the matrix A in Example 7.3.3, verify with direct computation that
eλ1tG1 + eλ2tG2 = P
 eλ1t
0
0
eλ2t

P−1 = eAt.
7.3.3. Explain why sin2 A + cos2 A = I for a diagonalizable matrix A.

7.3 Functions of Diagonalizable Matrices
539
7.3.4. Explain e0 = I for every square zero matrix.
7.3.5. The spectral mapping property for diagonalizable matrices says that
if f(A) exists, and if {λ1, λ2, . . . , λn} are the eigenvalues of An×n
(including multiplicities), then {f(λ1), . . . , f(λn)} are the eigenvalues
of f(A).
(a)
Establish this for diagonalizable matrices.
(b)
Establish this when an inﬁnite series f(z) = ∞
n=0 cn(z −z0)n
deﬁnes f(A) = ∞
n=0 cn(A −z0I)n as discussed in (7.3.7).
7.3.6. Explain why det

eA
= etrace(A).
7.3.7. Suppose that for nondiagonalizable matrices Am×m an inﬁnite series
f(z) = ∞
n=0 cn(z −z0)n is used to deﬁne f(A) = ∞
n=0 cn(A −z0I)n
as suggested in (7.3.7). Neglecting convergence issues, explain why there
is a polynomial p(z) of at most degree m −1 such that f(A) = p(A).
7.3.8. If f(A) exists for a diagonalizable A, explain why Af(A) = f(A)A.
What can you say when A is not diagonalizable?
7.3.9. Explain why eA+B = eAeB whenever AB = BA. Give an example
to show that eA+B, eAeB, and eBeA all can diﬀer when AB ̸= BA.
Hint: Exercise 7.2.16 can be used for the diagonalizable case. For the
general case, consider F(t) = e(A+B)t −eAteBt and F′(t).
7.3.10. Show that eA is an orthogonal matrix whenever A is skew symmetric.
7.3.11. A particular electronic device consists of a collection of switching circuits
that can be either in an ON state or an OFF state. These electronic
switches are allowed to change state at regular time intervals called clock
cycles. Suppose that at the end of each clock cycle, 30% of the switches
currently in the OFF state change to ON, while 90% of those in the ON
state revert to the OFF state.
(a)
Show that the device approaches an equilibrium in the sense
that the proportion of switches in each state eventually becomes
constant, and determine these equilibrium proportions.
(b)
Independent of the initial proportions, about how many clock
cycles does it take for the device to become essentially stable?

540
Chapter 7
Eigenvalues and Eigenvectors
7.3.12. The spectral radius of A is ρ(A) = maxλi∈σ(A) |λi| (p. 497). Prove
that if A is diagonalizable, then
ρ(A) = lim
n→∞∥An∥1/n
for every matrix norm.
This result is true for nondiagonalizable matrices as well, but the proof
at this point in the game is more involved. The full development is given
in Example 7.10.1 (p. 619).
7.3.13. Find a dominant eigenpair for A=

7
2
3
0
2
0
−6
−2
−2

by the power method.
7.3.14. Apply the inverse power method (Example 7.3.8, p. 534) to ﬁnd an eigen-
vector for each of the eigenvalues of the matrix A in Exercise 7.3.13.
7.3.15. Explain why the function m(v) used in the development of the power
method in Example 7.3.7 is not a continuous function, so statements
like m(xn) →m(x) when xn →x are not valid. Nevertheless, if
limn→∞xn ̸= 0, then limn→∞m(xn) ̸= 0.
7.3.16. Let H =

1
0
0
−1
−2
−1
0
2
1

.
(a)
Apply the “vanilla” QR iteration to H.
(b)
Apply the the single shift QR iteration on H.
7.3.17. Show that the QR iteration can fail to converge using H =
 0
0
1
1
0
0
0
1
0

.
(a)
First use the “vanilla” QR iteration on H to see what happens.
(b)
Now try the single shift QR iteration on H.
(c)
Finally, execute the double shift QR iteration on H.

7.4 Systems of Diﬀerential Equations
541
7.4
SYSTEMS OF DIFFERENTIAL EQUATIONS
Systems of ﬁrst-order linear diﬀerential equations with constant coeﬃcients were
used in §7.1 to motivate the introduction of eigenvalues and eigenvectors, but
now we can delve a little deeper. For constants aij, the goal is to solve the
following system for the unknown functions ui(t).
u′
1 = a11u1 + a12u2 + · · · + a1nun,
u′
2 = a21u1 + a22u2 + · · · + a2nun,
...
u′
n = an1u1 + an2u2 + · · · + annun,
with
u1(0) = c1,
u2(0) = c2,
...
un(0) = cn.
(7.4.1)
Since the scalar exponential provides the unique solution to a single diﬀerential
equation u′(t) = αu(t) with u(0) = c as u(t) = eαtc, it’s only natural to try to
use the matrix exponential in an analogous way to solve a system of diﬀerential
equations. Begin by writing (7.4.1) in matrix form as u′ = Au, u(0) = c, where
u =




u1(t)
u2(t)
...
un(t)



,
A =




a11
a12
· · ·
a1n
a21
a22
· · ·
a2n
...
...
...
...
an1
an2
· · ·
ann



,
and
c =




c1
c2
...
cn



.
If A is diagonalizable with σ(A) = {λ1, λ2, . . . , λk} , then (7.3.6) guarantees
eAt = eλ1tG1 + eλ2tG2 + · · · + eλktGk.
(7.4.2)
The following identities are derived from properties of the Gi ’s given on p. 517.
•
deAt/dt = k
i=1 λieλitGi =
k
i=1 λiGi
 k
i=1 eλitGi

= AeAt.
(7.4.3)
•
AeAt = eAtA
(by a similar argument).
(7.4.4)
•
e−AteAt = eAte−At = I = e0
(by a similar argument).
(7.4.5)
Equation (7.4.3) insures that u = eAtc is one solution to u′ = Au, u(0) = c.
To see that u = eAtc is the only solution, suppose v(t) is another solution so
that v′ = Av with v(0) = c. Diﬀerentiating e−Atv produces
d

e−Atv

dt
= e−Atv′ −e−AtAv = 0,
so
e−Atv is constant for all t.

542
Chapter 7
Eigenvalues and Eigenvectors
At t = 0 we have e−Atv

t=0 = e0v(0) = Ic = c, and hence e−Atv = c for
all t. Multiply both sides of this equation by eAt and use (7.4.5) to conclude
v = eAtc. Thus u = eAtc is the unique solution to u′ = Au with u(0) = c.
Finally, notice that vi = Gic ∈N (A −λiI) is an eigenvector associated
with λi, so that the solution to u′ = Au, u(0) = c, is
u = eλ1tv1 + eλ2tv2 + · · · + eλktvk,
(7.4.6)
and this solution is completely determined by the eigenpairs (λi, vi). It turns
out that u also can be expanded in terms of any complete set of independent
eigenvectors—see Exercise 7.4.1. Let’s summarize what’s been said so far.
Differential Equations
If An×n is diagonalizable with σ (A) = {λ1, λ2, . . . , λk} , then the
unique solution of u′ = Au, u(0) = c, is given by
u = eAtc = eλ1tv1 + eλ2tv2 + · · · + eλktvk
(7.4.7)
in which vi is the eigenvector vi = Gic, where Gi is the ith spectral
projector. (See Exercise 7.4.1 for an alternate eigenexpansion.) Nonho-
mogeneous systems as well as the nondiagonalizable case are treated in
Example 7.9.6 (p. 608).
Example 7.4.1
An Application to Diﬀusion.
Important issues in medicine and biology in-
volve the question of how drugs or chemical compounds move from one cell to
another by means of diﬀusion through cell walls. Consider two cells, as depicted
in Figure 7.4.1, which are both devoid of a particular compound. A unit amount
of the compound is injected into the ﬁrst cell at time t = 0, and as time proceeds
the compound diﬀuses according to the following assumption.
Cell  1
Cell  2
α
β
Figure 7.4.1

7.4 Systems of Diﬀerential Equations
543
At each point in time the rate (amount per second) of diﬀusion from one cell to
the other is proportional to the concentration (amount per unit volume) of the
compound in the cell giving up the compound—say the rate of diﬀusion from
cell 1 to cell 2 is α times the concentration in cell 1, and the rate of diﬀusion
from cell 2 to cell 1 is β times the concentration in cell 2. Assume α, β > 0.
Problem: Determine the concentration of the compound in each cell at any
given time t, and, in the long run, determine the steady-state concentrations.
Solution: If uk = uk(t) denotes the concentration of the compound in cell k at
time t, then the statements in the above assumption are translated as follows:
du1
dt = rate in −rate out = βu2 −αu1,
where
u1(0) = 1,
du2
dt = rate in −rate out = αu1 −βu2,
where
u2(0) = 0.
In matrix notation this system is u′ = Au, u(0) = c, where
A =

−α
β
α
−β

,
u =

u1
u2

,
and
c =

1
0

.
Since A is the matrix of Example 7.3.3 we can use the results from Example
7.3.3 to write the solution as
u(t) = eAtc =
1
α + β
%
β
β
α
α

+ e−(α+β)t

α
−β
−α
β
& 
1
0

,
so that
u1(t) =
β
α + β +
α
α + β e−(α+β)t
and
u2(t) =
α
α + β

1 −e−(α+β)t
.
In the long run, the concentrations in each cell stabilize in the sense that
lim
t→∞u1(t) =
β
α + β
and
lim
t→∞u2(t) =
α
α + β .
An innumerable variety of physical situations can be modeled by u′ = Au,
and the form of the solution (7.4.6) makes it clear that the eigenvalues and
eigenvectors of A are intrinsic to the underlying physical phenomenon being
investigated. We might say that the eigenvalues and eigenvectors of A act as its
genes and chromosomes because they are the basic components that either dic-
tate or govern all other characteristics of A along with the physics of associated
phenomena.

544
Chapter 7
Eigenvalues and Eigenvectors
For example, consider the long-run behavior of a physical system that can be
modeled by u′ = Au. We usually want to know whether the system will even-
tually blow up or will settle down to some sort of stable state. Might it neither
blow up nor settle down but rather oscillate indeﬁnitely? These are questions
concerning the nature of the limit
lim
t→∞u(t) = lim
t→∞eAtc = lim
t→∞

eλ1tG1 + eλ2tG2 + · · · + eλktGk

c,
and the answers depend only on the eigenvalues. To see how, recall that for a
complex number λ = x + iy and a real parameter t > 0,
eλt = e(x+iy)t = exteiyt = ext (cos yt + i sin yt) .
(7.4.8)
The term eiyt = (cos yt + i sin yt) is a point on the unit circle that oscillates as a
function of t, so |eiyt| = |cos yt + i sin yt| = 1 and
eλt = |exteiyt| = |ext| = ext.
This makes it clear that if Re (λi) < 0 for each i, then, as t →∞, eAt →0,
and u(t) →0 for every initial vector c. Thus the system eventually settles down
to zero, and we say the system is stable. On the other hand, if Re (λi) > 0 for
some i, then components of u(t) may become unbounded as t →∞, and
we say the system is unstable. Finally, if
Re (λi) ≤0 for each i, then the
components of u(t) remain ﬁnite for all t, but some may oscillate indeﬁnitely,
and this is called a semistable situation. Below is a summary of stability.
Stability
Let u′ = Au, u(0) = c, where A is diagonalizable with eigenvalues
λi.
•
If Re (λi) < 0 for each i, then
lim
t→∞eAt = 0, and
lim
t→∞u(t) = 0
for every initial vector c. In this case u′ = Au is said to be a stable
system, and A is called a stable matrix.
•
If Re (λi) > 0 for some i, then components of u(t) can become
unbounded as t →∞, in which case the system u′ = Au as well
as the underlying matrix A are said to be unstable.
•
If Re (λi) ≤0 for each i, then the components of u(t) remain
ﬁnite for all t, but some can oscillate indeﬁnitely. This is called a
semistable situation.
Example 7.4.2
Predator–Prey Application.
Consider two species of which one is the preda-
tor and the other is the prey, and assume there are initially 100 in each popula-
tion. Let u1(t) and u2(t) denote the respective population of the predator and

7.4 Systems of Diﬀerential Equations
545
prey species at time t, and suppose their growth rates are given by
u′
1 = u1 + u2,
u′
2 = −u1 + u2.
Problem: Determine the size of each population at all future times, and decide
if (and when) either population will eventually become extinct.
Solution: Write the system as u′ = Au, u(0) = c, where
A =

1
1
−1
1

,
u =

u1
u2

,
and
c =

100
100

.
The characteristic equation for A is p(λ) = λ2 −2λ + 2 = 0, so the eigenvalues
for A are λ1 = 1 + i and λ2 = 1 −i. We know from (7.4.7) that
u(t) = eλ1tv1 + eλ2tv2
(where vi = Gic )
(7.4.9)
is the solution to u′ = Au, u(0) = c. The spectral theorem on p. 517 implies
A −λ2I = (λ1 −λ2)G1 and I = G1 + G2, so (A −λ2I)c = (λ1 −λ2)v1 and
c = v1 + v2, and consequently
v1 = (A −λ2I)c
(λ1 −λ2) = 50

λ2
λ1

and
v2 = c −v1 = 50

λ1
λ2

.
With the aid of (7.4.8) we obtain the solution components from (7.4.9) as
u1(t) = 50

λ2eλ1t + λ1eλ2t
= 100et(cos t + sin t)
and
u2(t) = 50

λ1eλ1t + λ2eλ2t
= 100et(cos t −sin t).
The system is unstable because Re (λi) > 0 for each eigenvalue. Indeed, u1(t)
and u2(t) both become unbounded as t →∞. However, a population cannot
become negative–once it’s zero, it’s extinct. Figure 7.4.2 shows that the graph
of u2(t) will cross the horizontal axis before that of u1(t).
u1(t)
u2(t)
   t
0.2
0.4
0.6
0.8
1
-200
-100
100
200
300
400
0
Figure 7.4.2

546
Chapter 7
Eigenvalues and Eigenvectors
Therefore, the prey species will become extinct at the value of t for which
u2(t) = 0 —i.e., when
100et(cos t −sin t) = 0
=⇒
cos t = sin t
=⇒
t = π
4 .
Exercises for section 7.4
7.4.1. Suppose that An×n is diagonalizable, and let P = [x1 | x2 | · · · | xn]
be a matrix whose columns are a complete set of linearly independent
eigenvectors corresponding to eigenvalues λi. Show that the solution to
u′ = Au, u(0) = c, can be written as
u(t) = ξ1eλ1tx1 + ξ2eλ2tx2 + · · · + ξneλntxn
in which the coeﬃcients ξi satisfy the algebraic system Pξ = c.
7.4.2. Using only the eigenvalues, determine the long-run behavior of the so-
lution to u′ = Au, u(0) = c for each of the following matrices.
(a)
A =

−1
−2
0
−3

. (b) A =

1
−2
0
3

. (c) A =

1
−2
1
−1

.
7.4.3. Competing Species.
Consider two species that coexist in the same
environment but compete for the same resources. Suppose that the pop-
ulation of each species increases proportionally to the number of its
own kind but decreases proportionally to the number in the competing
species—say that the population of each species increases at a rate equal
to twice its existing number but decreases at a rate equal to the number
in the other population. Suppose that there are initially 100 of species I
and 200 of species II.
(a)
Determine the number of each species at all future times.
(b)
Determine which species is destined to become extinct, and com-
pute the time to extinction.
7.4.4. Cooperating Species.
Consider two species that survive in a sym-
biotic relationship in the sense that the population of each species de-
creases at a rate equal to its existing number but increases at a rate
equal to the existing number in the other population.
(a)
If there are initially 200 of species I and 400 of species II, deter-
mine the number of each species at all future times.
(b)
Discuss the long-run behavior of each species.

7.5 Normal Matrices
547
7.5
NORMAL MATRICES
A matrix A is diagonalizable if and only if A possesses a complete independent
set of eigenvectors, and if such a complete set is used for columns of P, then
P−1AP = D is diagonal (p. 507). But even when A possesses a complete in-
dependent set of eigenvectors, there’s no guarantee that a complete orthonormal
set of eigenvectors can be found. In other words, there’s no assurance that P
can be taken to be unitary (or orthogonal). And the Gram–Schmidt procedure
(p. 309) doesn’t help—Gram–Schmidt can turn a basis of eigenvectors into an
orthonormal basis but not into an orthonormal basis of eigenvectors. So when (or
how) are complete orthonormal sets of eigenvectors produced? In other words,
when is A unitarily similar to a diagonal matrix?
Unitary Diagonalization
A ∈Cn×n is unitarily similar to a diagonal matrix (i.e., A has a com-
plete orthonormal set of eigenvectors) if and only if A∗A = AA∗, in
which case A is said to be a normal matrix.
•
Whenever U∗AU = D with U unitary and D diagonal, the
columns of U must be a complete orthonormal set of eigenvectors
for A, and the diagonal entries of D are the associated eigenvalues.
Proof.
If A is normal with σ (A) = {λ1, λ2, . . . , λk} , then A−λkI is also nor-
mal. All normal matrices are RPN (range is perpendicular to nullspace, p. 409),
so there is a unitary matrix Uk such that
U∗
k(A −λkI)Uk =

Ck
0
0
0

(by (5.11.15) on p. 408)
or, equivalently
U∗
kAUk =

Ck+λkI
0
0
λkI

=

Ak−1
0
0
λkI

,
where Ck is nonsingular and Ak−1 = Ck+λkI. Note that λk /∈σ (Ak−1) (oth-
erwise Ak−1 −λkI = Ck would be singular), so σ (Ak−1) = {λ1, λ2, . . . , λk−1}
(Exercise 7.1.4). Because Ak−1 is also normal, the same argument can be re-
peated with Ak−1 and λk−1 in place A and λk to insure the existence of a
unitary matrix Uk−1 such that
U∗
k−1Ak−1Uk−1 =

Ak−2
0
0
λk−1I

,

548
Chapter 7
Eigenvalues and Eigenvectors
where Ak−2 is normal and σ (Ak−2) = {λ1, λ2, . . . , λk−2} . After k such rep-
etitions, Uk
 Uk−1
0
0
I

· · ·
 U1
0
0
I

= U is a unitary matrix such that
U∗AU =




λ1Ia1
0
· · ·
0
0
λ2Ia2
· · ·
0
...
...
...
...
0
0
· · ·
λkIak



= D,
ai = alg multA (λi) .
(7.5.1)
Conversely, if there is a unitary matrix U such that U∗AU = D is diagonal,
then A∗A = UD∗DU∗= U = UDD∗U∗= AA∗, so A is normal.
Caution! While it’s true that normal matrices possess a complete orthonormal
set of eigenvectors, not all complete independent sets of eigenvectors of a normal
A are orthonormal (or even orthogonal)—see Exercise 7.5.6. Below are some
things that are true.
Properties of Normal Matrices
If A is a normal matrix with σ (A) = {λ1, λ2, . . . , λk} , then
•
A is RPN—i.e., R (A) ⊥N (A) (see p. 408).
•
Eigenvectors corresponding to distinct eigenvalues are orthogonal. In
other words,
N (A −λiI) ⊥N (A −λjI)
for λi ̸= λj.
(7.5.2)
•
The spectral theorems (7.2.7) and (7.3.6) on pp. 517 and 526 hold,
but the spectral projectors Gi on p. 529 specialize to become orthog-
onal projectors because R (A −λiI) ⊥N (A −λiI) for each λi.
Proof of (7.5.2).
If A is normal, so is A −λjI, and hence A −λjI is RPN.
Consequently, N (A −λjI)∗= N (A −λjI) —recall (5.11.14) from p. 408. If
(λi, xi) and (λj, xj) are distinct eigenpairs, then (A −λjI)∗xj = 0, and 0 =
x∗
j(A −λjI)xi = x∗
jAxi −x∗
jλjxi = (λi −λj)x∗
jxi implies 0 = x∗
jxi.
Several common types of matrices are normal. For example, real-symmetric
and hermitian matrices are normal, real skew-symmetric and skew-hermitian
matrices are normal, and orthogonal and unitary matrices are normal. By virtue
of being normal, these kinds of matrices inherit all of the above properties, but
it’s worth looking a bit closer at the real-symmetric and hermitian matrices
because they have some special eigenvalue properties.
If A is real symmetric or hermitian, and if (λ, x) is an eigenpair for A,
then x∗x ̸= 0, and λx = Ax implies λx∗= x∗A∗, so
x∗x(λ −λ) = x∗(λ −λ)x = x∗Ax −x∗A∗x = 0
=⇒
λ = λ.

7.5 Normal Matrices
549
In other words, eigenvalues of real-symmetric and hermitian matrices are real.
A similar argument (Exercise 7.5.4) shows that the eigenvalues of a real skew-
symmetric or skew-hermitian matrix are pure imaginary numbers.
Eigenvectors for a hermitian A ∈Cn×n may have to involve complex num-
bers, but a real-symmetric matrix possesses a complete orthonormal set of real
eigenvectors. Consequently, the real-symmetric case can be distinguished by ob-
serving that A is real symmetric if and only if A is orthogonally similar to a
real-diagonal matrix D. Below is a summary of these observations.
Symmetric and Hermitian Matrices
In addition to the properties inherent to all normal matrices,
•
Real-symmetric and hermitian matrices have real eigenvalues. (7.5.3)
•
A is real symmetric if and only if A is orthogonally similar to a
real-diagonal matrix D —i.e., PT AP = D for some orthogonal P.
•
Real skew-symmetric and skew-hermitian matrices have pure imag-
inary eigenvalues.
Example 7.5.1
Largest and Smallest Eigenvalues. Since the eigenvalues of a hermitian ma-
trix An×n are real, they can be ordered as λ1 ≥λ2 ≥· · · ≥λn.
Problem: Explain why the largest and smallest eigenvalues can be described as
λ1 = max
∥x∥2=1 x∗Ax
and
λn = min
∥x∥2=1 x∗Ax.
(7.5.4)
Solution: There is a unitary U such that U∗AU = D = diag (λ1, λ2, . . . , λn)
or, equivalently, A = UDU∗. Since ∥x∥2 = 1 ⇐⇒∥y∥2 = 1 for y = U∗x,
max
∥x∥2=1 x∗Ax = max
∥y∥2=1 y∗Dy = max
∥y∥2=1
n

i=1
λi|yi|2 ≤max
∥y∥2=1 λ1
n

i=1
|yi|2 = λ1
with equality being attained when x is an eigenvector of unit norm associated
with λ1. The expression for the smallest eigenvalue λn is obtained by writing
min
∥x∥2=1 x∗Ax =
min
∥y∥2=1 y∗Dy =
min
∥y∥2=1
n

i=1
λi|yi|2 ≥
min
∥y∥2=1 λn
n

i=1
|yi|2 = λn,
where equality is attained at an eigenvector of unit norm associated with λn.
Note: The characterizations in (7.5.4) often appear in the equivalent forms
λ1 = max
x̸=0
x∗Ax
x∗x
and
λn = min
x̸=0
x∗Ax
x∗x .

550
Chapter 7
Eigenvalues and Eigenvectors
Consequently, λ1 ≥(x∗Ax/x∗x) ≥λn for all x ̸= 0. The term x∗Ax/x∗x
is referred to as a Rayleigh quotient in honor of the famous English physicist
John William Strutt (1842–1919) who became Baron Rayleigh in 1873.
It’s only natural to wonder if the intermediate eigenvalues of a hermitian
matrix have representations similar to those for the extreme eigenvalues as de-
scribed in (7.5.4). Ernst Fischer (1875–1954) gave the answer for matrices in 1905,
and Richard Courant (1888–1972) provided extensions for inﬁnite-dimensional
operators in 1920.
Courant–Fischer Theorem
The eigenvalues λ1 ≥λ2 ≥· · · ≥λn of a hermitian matrix An×n are
λi = max
dim V=i min
x∈V
∥x∥2=1
x∗Ax
and
λi =
min
dim V=n−i+1 max
x∈V
∥x∥2=1
x∗Ax.
(7.5.5)
When i = 1 in the min-max formula and when i = n in the max-
min formula, V = Cn, so these cases reduce to the equations in (7.5.4).
Alternate max-min and min-max formulas are given in Exercise 7.5.12.
Proof.
Only the min-max characterization is proven—the max-min proof is
analogous (Exercise 7.5.11). As shown in Example 7.5.1, a change of coordinates
y = U∗x with a unitary U such that U∗AU = D = diag (λ1, λ2, . . . , λn) has
the eﬀect of replacing A by D, so we need only establish that
λi =
min
dim V=n−i+1 max
y∈V
∥y∥2=1
y∗Dy.
For a subspace V of dimension n −i + 1, let SV = {y ∈V, ∥y∥2 = 1}, and let
S′
V = {y ∈V ∩F, ∥y∥2 = 1},
where
F = span {e1, e2, . . . , ei} .
Note that V ∩F ̸= 0, for otherwise dim(V + F) = dim V + dim F = n + 1,
which is impossible. In other words, S′
V contains those vectors of SV of the
form y = (y1, . . . , yi, 0, . . . , 0)T with i
j=1 |yj|2 = 1. So for each subspace V
with dim V = n −i + 1,
y∗Dy =
i

j=1
λj|yj|2 ≥λi
i

j=1
|yj|2 = λi
for all y ∈S′
V.
Since S′
V ⊆SV, it follows that maxSV y∗Dy ≥maxS′
V y∗Dy ≥λi, and hence
min
V
max
SV
y∗Dy ≥λi.

7.5 Normal Matrices
551
But this inequality is reversible because if ˜V = {e1, e2, . . . , ei−1}⊥, then every
y ∈˜V has the form y = (0, . . . , 0, yi, . . . , yn)T , and hence
y∗Dy =
n

j=i
λj|yj|2 ≤λi
n

j=i
|yj|2 = λi
for all y ∈S˜V.
So min
V
max
SV
y∗Dy ≤max
S ˜
V
y∗Dy ≤λi, and thus min
V
max
SV
y∗Dy = λi.
The value of the Courant–Fischer theorem is its ability to produce inequal-
ities concerning eigenvalues of hermitian matrices without involving the associ-
ated eigenvectors. This is illustrated in the following two important examples.
Example 7.5.2
Eigenvalue Perturbations. Let λ1 ≥λ2 ≥· · · ≥λn be the eigenvalues of
a hermitian A ∈Cn×n, and suppose A is perturbed by a hermitian E with
eigenvalues ϵ1 ≥ϵ2 ≥· · · ≥ϵn to produce B = A + E, which is also hermitian.
Problem: If β1 ≥β2 ≥· · · ≥βn are the eigenvalues of B, explain why
λi + ϵ1 ≥βi ≥λi + ϵn
for each i.
(7.5.6)
Solution: If U is a unitary matrix such that U∗AU = D = diag (λ1, . . . , λn),
then ˜B = U∗BU and ˜E = U∗EU have the same eigenvalues as B and E,
respectively, and ˜B = D+ ˜E. For x ∈F = span {e1, e2, . . . , ei} with ∥x∥2 = 1,
x = (x1, . . . , xi, 0, . . . , 0)T
and
x∗Dx =
i

j=1
λj|xj|2 ≥λi
i

j=1
|xj|2 = λi,
so applying the max-min part of the Courant–Fischer theorem to ˜B yields
βi = max
dim V=i min
x∈V
∥x∥2=1
x∗˜Bx ≥min
x∈F
∥x∥2=1
x∗˜Bx = min
x∈F
∥x∥2=1

x∗Dx + x∗˜Ex

≥min
x∈F
∥x∥2=1
x∗Dx + min
x∈F
∥x∥2=1
x∗˜Ex ≥λi + min
x∈Cn
∥x∥2=1
x∗˜Ex = λi + ϵn,
where the last equality is the result of the “min” part of (7.5.4). Similarly, for
x ∈T = span {ei, . . . , en} with ∥x∥2 = 1, we have x∗Dx ≤λi, and
βi =
min
dim V=n−i+1 max
x∈V
∥x∥2=1
x∗˜Bx ≤max
x∈T
∥x∥2=1
x∗˜Bx = max
x∈T
∥x∥2=1

x∗Dx + x∗˜Ex

≤max
x∈T
∥x∥2=1
x∗Dx + max
x∈T
∥x∥2=1
x∗˜Ex ≤λi + max
x∈Cn
∥x∥2=1
x∗˜Ex = λi + ϵ1.

552
Chapter 7
Eigenvalues and Eigenvectors
Note: Because E often represents an error, only ∥E∥(or an estimate thereof)
is known. But for every matrix norm, |ϵj| ≤∥E∥for each j (Example 7.1.4,
p. 497). Since the ϵj ’s are real, −∥E∥≤ϵj ≤∥E∥, so (7.5.6) guarantees that
λi −∥E∥≤βi ≤λi + ∥E∥.
(7.5.7)
In other words,
•
the eigenvalues of a hermitian matrix A are perfectly conditioned because a
hermitian perturbation E changes no eigenvalue of A by more than ∥E∥.
It’s interesting to compare (7.5.7) with the Bauer–Fike bound of Example 7.3.2
(p. 528). When A is hermitian, (7.3.10) reduces to minλi∈σ(A) |β −λi| ≤∥E∥
because P can be made unitary, so, for induced matrix norms, κ(P) = 1. The
two results diﬀer in that Bauer–Fike does not assume E and B are hermitian.
Example 7.5.3
Interlaced Eigenvalues. For a hermitian matrix A ∈Cn×n with eigenvalues
λ1 ≥λ2 ≥· · · ≥λn, and for c ∈Cn×1, let B be the bordered matrix
B =

A
c
c∗
α

n+1×n+1
with eigenvalues
β1 ≥β2 ≥· · · ≥βn ≥βn+1.
Problem: Explain why the eigenvalues of A interlace with those of B in that
β1 ≥λ1 ≥β2 ≥λ2 ≥· · · ≥βn ≥λn ≥βn+1.
(7.5.8)
Solution: To see that βi ≥λi ≥βi+1 for 1 ≤i ≤n, let U be a unitary
matrix such that UT AU = D = diag (λ1, λ2, . . . , λn) . Since V =
 U
0
0
1

is
also unitary, the eigenvalues of B agree with those of
˜B = V∗BV =

D
y
y∗
α

,
where
y = U∗c.
For x ∈F = span {e1, e2, . . . , ei} ⊂Cn+1×1 with ∥x∥2 = 1,
x = (x1, . . . , xi, 0, . . . , 0)T
and
x∗˜Bx =
n

j=1
λj|xj|2 ≥λi
n

j=1
|xj|2 = λi,
so applying the max-min part of the Courant–Fisher theorem to ˜B yields
βi = max
dim V=i min
x∈V
∥x∥2=1
x∗˜Bx ≥min
x∈F
∥x∥2=1
x∗˜Bx ≥λi.

7.5 Normal Matrices
553
For x ∈T = span{ei−1, ei, . . . , en} ⊂Cn+1×1 with ∥x∥2 = 1,
x = (0, . . . , 0, xi−1, . . . , xn, 0)T and x∗˜Bx=
n

j=i−1
λj|xj|2 ≤λi−1
n

j=i
|xj|2 = λi−1,
so the min-max part of the Courant–Fisher theorem produces
βi =
min
dim V=n−i+2 max
x∈V
∥x∥2=1
x∗˜Bx ≤max
x∈F
∥x∥2=1
x∗˜Bx ≤λi−1.
Note: If A is any n × n principal submatrix of B, then (7.5.8) still holds
because each principal submatrix can be brought to the upper-left-hand corner
by a similarity transformation PT BP, where P is a permutation matrix. In
other words,
•
the eigenvalues of an n + 1 × n + 1 hermitian matrix are interlaced with the
eigenvalues of each of its n × n principal submatrices.
For A ∈Cm×n (or ℜm×n), the products A∗A and AA∗(or AT A and
AAT ) are hermitian (or real symmetric), so they are diagonalizable by a uni-
tary (or orthogonal) similarity transformation, and their eigenvalues are nec-
essarily real. But in addition to being real, the eigenvalues of these matrices
are always nonnegative. For example, if (λ, x) is an eigenpair of A∗A, then
λ = x∗A∗Ax/x∗x = ∥Ax∥2
2 / ∥x∥2
2 ≥0, and similarly for the other products. In
fact, these λ ’s are the squares of the singular values for A developed in §5.12
(p. 411) because if
A = U

Dr×r
0
0
0

m×n
V∗
is a singular value decomposition, where D = diag (σ1, σ2, . . . , σr) contains the
nonzero singular values of A, then
V∗A∗AV =

D2
0
0
0

,
(7.5.9)
and this means that (σ2
i , vi) for i = 1, 2, . . . , r is an eigenpair for A∗A. In
other words, the nonzero singular values of A are precisely the positive square
roots of the nonzero eigenvalues of A∗A, and right-hand singular vectors vi of
A are particular eigenvectors of A∗A. Note that this establishes the uniqueness
of the σi ’s (but not the vi ’s), and pay attention to the fact that the number
of zero singular values of A need not agree with the number of zero eigenvalues
of A∗A —e.g., A1×2 = (1, 1) has no zero singular values, but A∗A has one
zero eigenvalue. The same game can be played with AA∗in place of A∗A to
argue that the nonzero singular values of A are the positive square roots of

554
Chapter 7
Eigenvalues and Eigenvectors
the nonzero eigenvalues of AA∗, and left-hand singular vectors ui of A are
particular eigenvectors of AA∗.
Caution! The statement that right-hand singular vectors vi of A are eigenvec-
tors of A∗A and left-hand singular vectors ui of A are eigenvectors of AA∗
is a one-way street—it doesn’t mean that just any orthonormal sets of eigen-
vectors for A∗A and AA∗can be used as respective right-hand and left-hand
singular vectors for A. The columns vi of any unitary matrix V that diago-
nalizes A∗A as in (7.5.9) can serve as right-hand singular vectors for A, but
corresponding left-hand singular vectors ui are constrained by the relationships
Avi = σiui,
i = 1, 2, . . . , r
=⇒
ui = Avi
σi
=
Avi
∥Avi∥2
,
i = 1, 2, . . . , r,
u∗
i A = 0,
i = r + 1, . . . , m
=⇒
span {ur+1, ur+2, . . . , um} = N (A∗).
In other words, the ﬁrst r left-hand singular vectors for A are uniquely deter-
mined by the ﬁrst r right-hand singular vectors, while the last m −r left-hand
singular vectors can be any orthonormal basis for N (A∗). If U is constructed
from V as described above, then U is guaranteed to be unitary because for
U=

u1 · · · ur|ur+1 · · · um

=

U1|U2

and V=

v1 · · · vr|vr+1 · · · vn

=

V1|V2

,
U1 and U2 each contain orthonormal columns, and, by using (7.5.9),
R (U1) = R

AV1D−1
= R (AV1) = R (AV1D) = R

[AV1D][AV1D]∗
= R (AA∗AA∗) = R (AA∗) = R (A) = N (A∗)⊥= R (U2)⊥.
The matrix V is unitary to start with, but, in addition,
R (V1) = R (V1D) = R ([V1D][V1D]∗) = R (A∗A) = R (A∗) and
R (V2) = R (A∗)⊥= N (A).
These observations are consistent with those established on p. 407 for any
URV factorization. Otherwise something would be terribly wrong because an
SVD is just a special kind of a URV factorization. Finally, notice that there
is nothing special about starting with V to build a U —we can also take the
columns of any unitary U that diagonalizes AA∗as left-hand singular vectors
for A and build corresponding right-hand singular vectors in a manner similar
to that described above. Below is a summary of the preceding developments
concerning singular values together with an additional observation connecting
singular values with eigenvalues.

7.5 Normal Matrices
555
Singular Values and Eigenvalues
For A ∈Cm×n with rank (A) = r, the following statements are valid.
•
The nonzero eigenvalues of A∗A and AA∗are equal and positive.
•
The nonzero singular values of A are the positive square roots of
the nonzero eigenvalues of A∗A (and AA∗).
•
If A is normal with nonzero eigenvalues {λ1, λ2, . . . , λr} , then the
nonzero singular values of A are {|λ1|, |λ2|, . . . , |λr|}.
•
Right-hand and left-hand singular vectors for A are special eigen-
vectors for A∗A and AA∗, respectively.
•
Any complete orthonormal set of eigenvectors vi for A∗A can serve
as a complete set of right-hand singular vectors for A, and a cor-
responding complete set of left-hand singular vectors is given by
ui = Avi/ ∥Avi∥2, i = 1, 2, . . . , r, together with any orthonormal
basis {ur+1, ur+2, . . . , um} for N (A∗). Similarly, any complete or-
thonormal set of eigenvectors for AA∗can be used as left-hand sin-
gular vectors for A, and corresponding right-hand singular vectors
can be built in an analogous way.
•
The hermitian matrix B =
 0m×m
A
A∗
0n×n

of order m + n has
nonzero eigenvalues {±σ1, ±σ2, . . . , ±σr} in which {σ1, σ2, . . . , σr}
are the nonzero singular values of A.
Proof.
Only the last point requires proof, and this follows by observing that if
λ is an eigenvalue of B, then

0
A
A∗
0
 
x1
x2

= λ

x1
x2

=⇒
	
Ax2 = λx1
A∗x1 = λx2

=⇒
A∗Ax2 = λ2x2,
so each eigenvalue of B is the square of a singular value of A. But B is
hermitian with rank (B) = 2r, so there are exactly 2r nonzero eigenvalues of
B. Therefore, each pair ±σi, i = 1, 2, . . . , r, must be an eigenvalue for B.
Example 7.5.4
Min-Max Singular Values. Since the singular values of A are the positive
square roots of the eigenvalues of A∗A, and since ∥Ax∥2 = (x∗A∗Ax)1/2, it’s
a corollary of the Courant–Fischer theorem (p. 550) that if σ1 ≥σ2 ≥· · · ≥σn
are the singular values for Am×n (n ≤m), then
σi = max
dim V=i min
x∈V
∥x∥2=1
∥Ax∥2
and
σi =
min
dim V=n−i+1 max
x∈V
∥x∥2=1
∥Ax∥2 .

556
Chapter 7
Eigenvalues and Eigenvectors
These expressions provide intermediate values between the extremes
σ1 = max
∥x∥2=1 ∥Ax∥2
and
σn = min
∥x∥2=1 ∥Ax∥2
(see p. 414).
Exercises for section 7.5
7.5.1. Is A =
 5 + i
−2 i
2
4 + 2 i

a normal matrix?
7.5.2. Give examples of two distinct classes of normal matrices that are real
but not symmetric.
7.5.3. Show that A ∈ℜn×n is normal and has real eigenvalues if and only if
A is symmetric.
7.5.4. Prove that the eigenvalues of a real skew-symmetric or skew-hermitian
matrix must be pure imaginary numbers (i.e., multiples of i ).
7.5.5. When trying to decide what’s true about matrices and what’s not, it
helps to think in terms of the following associations.
Hermitian matrices
←→
Real numbers (z = z).
Skew-hermitian matrices
←→
Imaginary numbers (z = −z).
Unitary matrices
←→
Points on the unit circle (z = eiθ).
For example, the complex function f(z) = (1 −z)(1 + z)−1 maps the
imaginary axis in the complex plane to points on the unit circle because
|f(z)|2 = 1 whenever z = −z. It’s therefore reasonable to conjecture
(as Cayley did in 1846) that if A is skew hermitian (or real skew sym-
metric), then
f(A) = (I −A)(I + A)−1 = (I + A)−1(I −A)
(7.5.10)
is unitary (or orthogonal). Prove this is indeed correct. Note: Expression
(7.5.10) has come to be known as the Cayley transformation.
7.5.6. Show by example that a normal matrix can have a complete independent
set of eigenvectors that are not orthonormal, and then explain how every
complete independent set of eigenvectors for a normal matrix can be
transformed into a complete orthonormal set of eigenvectors.

7.5 Normal Matrices
557
7.5.7. Construct an example to show that the converse of (7.5.2) is false. In
other words, show that it is possible for N (A −λiI) ⊥N (A −λjI)
whenever i ̸= j without A being normal.
7.5.8. Explain why a triangular matrix is normal if and only if it is diagonal.
7.5.9. Use the result of Exercise 7.5.8 to give an alternate proof of the unitary
diagonalization theorem given on p. 547.
7.5.10. For a normal matrix A, explain why (λ, x) is an eigenpair for A if
and only if (λ, x) is an eigenpair for A∗.
7.5.11. To see if you understand the proof of the min-max part of the Courant–
Fischer theorem (p. 550), construct an analogous proof for the max-min
part of (7.5.5).
7.5.12. The Courant–Fischer theorem has the following alternate formulation.
λi =
max
v1,...,vn−i∈Cn
min
x⊥v1,...,vn−i
∥x∥2=1
x∗Ax
and
λi =
min
v1,...,vi−1∈Cn
max
x⊥v1,...,vi−1
∥x∥2=1
x∗Ax
for 1 < i < n. To see if you really understand the proof of the min-
max part of (7.5.5), adapt it to prove the alternate min-max formulation
given above.
7.5.13.
(a)
Explain why every unitary matrix is unitarily similar to a diag-
onal matrix of the form
D =




eiθ1
0
· · ·
0
0
eiθ2
· · ·
0
...
...
...
...
0
0
· · ·
eiθn



.
(b)
Prove that every orthogonal matrix is orthogonally similar to a
real block-diagonal matrix of the form
B =












±1
...
±1
cos θ1 sin θ1
−sin θ1 cos θ1
...
cos θt sin θt
−sin θt cos θt












.

558
Chapter 7
Eigenvalues and Eigenvectors
7.6
POSITIVE DEFINITE MATRICES
Since the symmetric structure of a matrix forces its eigenvalues to be real, what
additional property will force all eigenvalues to be positive (or perhaps just non-
negative)? To answer this, let’s deal with real-symmetric matrices—the hermi-
tian case follows along the same lines. If A ∈ℜn×n is symmetric, then, as
observed above, there is an orthogonal matrix P such that A = PDPT , where
D = diag (λ1, λ2, . . . , λn) is real. If λi ≥0 for each i, then D1/2 exists, so
A = PDPT = PD1/2D1/2PT = BT B
for
B = D1/2PT ,
and λi > 0 for each i if and only if B is nonsingular. Conversely, if A can be
factored as A = BT B, then all eigenvalues of A are nonnegative because for
any eigenpair (λ, x),
λ = xT Ax
xT x
= xT BT Bx
xT x
= ∥Bx∥2
2
∥x∥2
2
≥0.
Moreover, if B is nonsingular, then N (B) = 0 =⇒Bx ̸= 0, so λ > 0. In other
words, a real-symmetric matrix A has nonnegative eigenvalues if and only if A
can be factored as A = BT B, and all eigenvalues are positive if and only if B
is nonsingular. A symmetric matrix A whose eigenvalues are positive is called
positive deﬁnite, and when the eigenvalues are just nonnegative, A is said to
be positive semideﬁnite.
The use of this terminology is consistent with that introduced in Exam-
ple 3.10.7 (p. 154), where the term “positive deﬁnite” was used to designate
symmetric matrices possessing an LU factorization with positive pivots. It was
demonstrated in Example 3.10.7 that possessing positive pivots is equivalent to
the existence of a Cholesky factorization A = RT R, where R is upper trian-
gular with positive diagonal entries. By the result of the previous paragraph this
means that all eigenvalues of a symmetric matrix A are positive if and only if
A has an LU factorization with positive pivots.
But the pivots are intimately related to the leading principal minor deter-
minants. Recall from Exercise 6.1.16 (p. 474) that if Ak is the kth leading
principal submatrix of An×n, then the kth pivot is given by
ukk =
	 det (A1) = a11
for k = 1,
det (Ak)/det (Ak−1)
for k = 2, 3, . . . , n.
Consequently, a symmetric matrix is positive deﬁnite if and only if each of its
leading principal minors is positive. However, if each leading principal minor
is positive, then all principal minors must be positive because if Pk is any
principal submatrix of A, then there is a permutation matrix Q such that

7.6 Positive Deﬁnite Matrices
559
Pk is a leading principal submatrix in C = QT AQ =
 Pk
⋆
⋆
⋆

, and, since
σ (A) = σ (C) , we have, with some obvious shorthand notation,
A ’s leading pm’s > 0 ⇒A pd ⇒C pd ⇒det (Pk) > 0 ⇒all of A ’s pm’s > 0.
Finally, observe that A is positive deﬁnite if and only if xT Ax > 0 for
every nonzero x ∈ℜn×1. If A is positive deﬁnite, then A = BT B for a
nonsingular B, so xT Ax = xT BT Bx = ∥Bx∥2
2 ≥0 with equality if and only if
Bx = 0 or, equivalently, x = 0. Conversely, if xT Ax > 0 for all x ̸= 0, then
for every eigenpair (λ, x) we have λ = (xT Ax/xT x) > 0.
Below is a formal summary of the results for positive deﬁnite matrices.
Positive Deﬁnite Matrices
For real-symmetric matrices A, the following statements are equivalent,
and any one can serve as the deﬁnition of a positive deﬁnite matrix.
•
xT Ax > 0 for every nonzero x ∈ℜn×1 (most commonly used as
the deﬁnition).
•
All eigenvalues of A are positive.
•
A = BT B for some nonsingular B.
▷While B is not unique, there is one and only one upper-triangular
matrix R with positive diagonals such that A = RT R. This is
the Cholesky factorization of A (Example 3.10.7, p. 154).
•
A has an LU (or LDU) factorization with all pivots being positive.
▷The LDU factorization is of the form A = LDLT = RT R, where
R = D1/2LT is the Cholesky factor of A (also see p. 154).
•
The leading principal minors of A are positive.
•
All principal minors of A are positive.
For hermitian matrices, replace (⋆)T by (⋆)∗and ℜby C.
Example 7.6.1
Vibrating Beads on a String. Consider n small beads, each having mass
m, spaced at equal intervals of length L on a very tightly stretched string
or wire under a tension T as depicted in Figure 7.6.1. Each bead is initially
displaced from its equilibrium position by a small vertical distance—say bead k
is displaced by an amount ck at t = 0. The beads are then released so that
they can vibrate freely.

560
Chapter 7
Eigenvalues and Eigenvectors
m
m
L
Equilibrium Position
A Typical Initial Position
Figure 7.6.1
Problem: For small vibrations, determine the position of each bead at time
t > 0 for any given initial conﬁguration.
Solution: The small vibration hypothesis validates the following assumptions.
•
The tension T remains constant for all time.
•
There is only vertical motion (the horizontal forces cancel each other).
•
Only small angles are involved, so the approximation sin θ ≈tan θ is valid.
Let yk(t) = yk be the vertical distance of the kth bead from equilibrium at
time t, and set y0 = 0 = yn+1.
yk–1
yk
yk+1
k–1
k
k+1
θk–1
θk+1
θk
Figure 7.6.2
If θk is the angle depicted in Figure 7.6.2, the diagram above, then the upward
force on the kth bead at time t is Fu = T sin θk, while the downward force is
Fd = T sin θk−1, so the total force on the kth bead at time t is
F = Fu −Fd = T(sin θk −sin θk−1) ≈T(tan θk −tan θk−1)
= T
yk+1 −yk
L
−yk −yk−1
L

= T
L(yk−1 −2yk + yk+1).
Newton’s second law says force = mass × acceleration, so we set
my′′
k = T
L(yk−1 −2yk + yk+1)
=⇒
y′′
k + T
mL(−yk−1 + 2yk −yk+1) = 0 (7.6.1)
together with yk(0) = ck and y′
k(0) = 0 to model the motion of the kth
bead. Altogether, equations (7.6.1) represent a system of n second-order linear
diﬀerential equations, and each is coupled to its neighbors so that no single

7.6 Positive Deﬁnite Matrices
561
equation can be solved in isolation. To extract solutions, the equations must
somehow be uncoupled, and here’s where matrix diagonalization works its magic.
Write equations (7.6.1) in matrix form as






y′′
1
y′′
2
y′′
3
...
y′′
n






+ T
mL







2
−1
−1
2
−1
−1
2
...
...
...
−1
−1
2













y1
y2
y3
...
yn






=






0
0
0
...
0






,
or
y′′ + Ay = 0,
(7.6.2)
with y(0) = c = (c1c2 · · · cn)T and y′(0) = 0. Since A is symmetric, there is
an orthogonal matrix P such that PT AP = D = diag (λ1, λ2, . . . , λn), where
the λi ’s are the eigenvalues of A. By making the substitution y = Pz (or,
equivalently, by changing the coordinate system), (7.6.2) is transformed into
z′′ + Dz = 0,
z(0) = PT c = ˜c,
z′(0) = 0,
or




z′′
1
z′′
2
...
z′′
n



+




λ1
0
· · ·
0
0
λ2
· · ·
0
...
...
...
...
0
0
· · ·
λn








z1
z2
...
zn



=




0
0
...
0



.
In other words, by changing to a coordinate system deﬁned by a complete set of
orthonormal eigenvectors for A, the original system (7.6.2) is completely uncou-
pled so that each equation z′′
k +λkzk = 0 with zk(0) = ˜ck and z′
k(0) = 0 can be
solved independently. This helps reveal why diagonalizability is a fundamentally
important concept. Recall from elementary diﬀerential equations that
z′′
k + λkzk = 0
=⇒
zk(t) =
'
αket√−λk + βke−t√−λk
when λk < 0,
αk cos

t
√
λk

+ βk sin

t
√
λk

when λk ≥0.
Vibrating beads suggest sinusoidal solutions, so we expect each λk > 0. In other
words, the mathematical model would be grossly inconsistent with reality if the
symmetric matrix A in (7.6.2) were not positive deﬁnite. It turns out that A
is positive deﬁnite because there is a Cholesky factorization A = RT R with
R =
*
T
mL






r1
−1/r1
r2
−1/r2
...
...
rn−1
−1/rn−1
rn






with
rk =
*
2 −k −1
k
,
and thus we are insured that each λk > 0. In fact, since A is a tridiagonal
Toeplitz matrix, the results of Example 7.2.5 (p. 514) can be used to show that
λk = 2T
mL

1 −cos
kπ
n + 1

= 4T
mL sin2
kπ
2(n + 1)
(see Exercise 7.2.18).

562
Chapter 7
Eigenvalues and Eigenvectors
Therefore,



zk
= αk cos

t
√
λk

+ βk sin

t
√
λk

zk(0) = ˜ck
z′
k(0) = 0



=⇒
zk = ˜ck cos

t
√
λk

, (7.6.3)
and for P =

x1 | x2 | · · · | xn

,
y = Pz = z1x1 + z2x2 + · · · + znxn =
n

j=1

˜cj cos

t
√
λk

xj.
(7.6.4)
This means that every possible mode of vibration is a combination of modes
determined by the eigenvectors xj. To understand this more clearly, suppose
that the beads are initially positioned according to the components of xj —i.e.,
c = y(0) = xj. Then ˜c = PT c = PT xj = ej, so (7.6.3) and (7.6.4) reduce to
zk =

cos

t
√
λk

if k = j
0
if k ̸= j
=⇒
y =

cos

t
√
λk

xj.
(7.6.5)
In other words, when y(0) = xj, the jth eigenpair (λj, xj) completely deter-
mines the mode of vibration because the amplitudes are determined by xj, and
each bead vibrates with a common frequency f =

λj/2π. This type of motion
(7.6.5) is called a normal mode of vibration. In these terms, equation (7.6.4)
translates to say that every possible mode of vibration is a combination of the
normal modes. For example, when n = 3, the matrix in (7.6.2) is
A =
T
mL


2
−1
0
−1
2
−1
0
−1
2


with



λ1 = (T/mL)(2)
λ2 = (T/mL)(2 −
√
2)
λ3 = (T/mL)(2 +
√
2)


,
and a complete orthonormal set of eigenvectors is
x1 =
1
√
2


1
0
−1

,
x2 = 1
2


1
√
2
1

,
x3 = 1
2


1
−
√
2
1

.
The three corresponding normal modes are shown in Figure 7.6.3.
Mode for (λ1, x1)
Mode for (λ2, x2)
Mode for (λ3, x3)
Figure 7.6.3

7.6 Positive Deﬁnite Matrices
563
Example 7.6.2
Discrete Laplacian. According to the laws of physics, the temperature at time
t at a point (x, y, z) in a solid body is a function u(x, y, z, t) satisfying the
diﬀusion equation
∂u
∂t = K∇2u,
where
∇2u = ∂2u
∂x2 + ∂2u
∂y2 + ∂2u
∂z2
is the Laplacian of u and K is a constant of thermal diﬀusivity. At steady
state the temperature at each point does not vary with time, so ∂u/∂t = 0 and
u = u(x, y, z) satisfy Laplace’s equation ∇2u = 0. Solutions of this equation
are often called harmonic functions. The nonhomogeneous equation ∇2u = f
(Poisson’s equation) is addressed in Exercise 7.6.9. To keep things simple, let’s
conﬁne our attention to the following two-dimensional problem.
Problem: For a square plate as shown in Figure 7.6.4(a), explain how to nu-
merically determine the steady-state temperature at interior grid points when
the temperature around the boundary is prescribed to be u(x, y) = g(x, y) for
a given function g. In other words, explain how to extract a numerical solution
to ∇2u = 0 in the interior of the square when u(x, y) = g(x, y) on the square’s
boundary. This is called a Dirichlet problem.
76
Solution: Discretize the problem by overlaying the plate with a square mesh
containing n2 interior points at equally spaced intervals of length h. As il-
lustrated in Figure 7.6.4(b) for n = 4, label the grid points using a rowwise
ordering scheme—i.e., label them as you would label matrix entries.
∇2u = 0 in the interior
u(x, y) = g(x, y) on the boundary
u(x, y) = g(x, y) on the boundary
u(x, y) = g(x, y) on the boundary
u(x, y) = g(x, y) on the boundary
00
01
02
03
04
05
10
11
12
13
14
15
20
21
22
23
24
25
30
31
32
33
34
35
40
41
42
43
44
45
50
51
52
53
54
55
+
,-
.
+
,-
.
h
h
(a)
(b)
Figure 7.6.4
76
Johann Peter Gustav Lejeune Dirichlet (1805–1859) held the chair at G¨ottingen previously
occupied by Gauss. Because of his work on the convergence of trigonometric series, Dirichlet
is generally considered to be the founder of the theory of Fourier series, but much of the
groundwork was laid by S. D. Poisson (p. 572) who was Dirichlet’s Ph.D. advisor.

564
Chapter 7
Eigenvalues and Eigenvectors
Approximate ∂2u/∂x2 and ∂2u/∂y2 at the interior grid points (xi, yj) by using
the second-order centered diﬀerence formula (1.4.3) developed on p. 19 to write
∂2u
∂x2

(xi,yj) = u(xi −h, yj) −2u(xi, yj) + u(xi + h, yj)
h2
+ O(h2),
∂2u
∂y2

(xi,yj) = u(xi, yj −h) −2u(xi, yj) + u(xi, yj + h)
h2
+ O(h2).
(7.6.6)
Adopt the notation uij = u(xi, yj), and add the expressions in (7.6.6) using
∇2u|(xi,yj) = 0 for interior points (xi, yj) to produce
4uij = (ui−1,j + ui+1,j + ui,j−1 + ui,j+1) + O(h4)
for
i, j = 1, 2, . . . , n.
In other words, the steady-state temperature at an interior grid point is approxi-
mately the average of the steady-state temperatures at the four neighboring grid
points as illustrated in Figure 7.6.5.
ij
i −1, j
i + 1, j
i, j + 1
i, j −1
uij = ui−1,j + ui+1,j + ui,j−1 + ui,j+1
4
+ O(h4)
Figure 7.6.5
If the O(h4) terms are neglected, the resulting ﬁve-point diﬀerence equations,
4uij −(ui−1,j + ui+1,j + ui,j−1 + ui,j+1) = 0
for
i, j = 1, 2, . . . , n,
constitute an n2 × n2 linear system Lu = g in which the unknowns are the
uij’s, and the right-hand side contains boundary values. For example, a mesh
with nine interior points produces the 9 × 9 system in Figure 7.6.6.
00
01
02
03
04
10
11
12
13
14
20
21
22
23
24
30
31
32
33
34
40
41
42
43
44















4
−1
0
−1
0
0
0
0
0
−1
4
−1
0
−1
0
0
0
0
0
−1
4
0
0
−1
0
0
0
−1
0
0
4
−1
0
−1
0
0
0
−1
0
−1
4
−1
0
−1
0
0
0
−1
0
−1
4
0
0
−1
0
0
0
−1
0
0
4
−1
0
0
0
0
0
−1
0
−1
4
−1
0
0
0
0
0
−1
0
−1
4






























u11
u12
u13
u21
u22
u23
u31
u32
u33















=















g01 + g10
g02
g03 + g14
g20
0
g24
g30 + g41
g42
g43 + g34















Figure 7.6.6

7.6 Positive Deﬁnite Matrices
565
The coeﬃcient matrix of this system is the discrete Laplacian, and in general
it has the symmetric block-tridiagonal form
L=






T
−I
−I
T
−I
...
...
...
−I
T
−I
−I
T






n2×n2
with
T=






4
−1
−1
4
−1
...
...
...
−1
4
−1
−1
4






n×n
.
In addition, L is positive deﬁnite. In fact, the discrete Laplacian is a primary
example of how positive deﬁnite matrices arise in practice. Note that L is the
two-dimensional version of the one-dimensional ﬁnite-diﬀerence matrix in Exam-
ple 1.4.1 (p. 19).
Problem: Show L is positive deﬁnite by explicitly exhibiting its eigenvalues.
Solution: Example 7.2.5 (p. 514) insures that the n eigenvalues of T are
λi = 4 −2 cos

iπ
n + 1

,
i = 1, 2, . . . , n.
(7.6.7)
If U is an orthogonal matrix such that UT TU = D = diag (λ1, λ2, . . . , λn) ,
and if B is the n2 × n2 block-diagonal orthogonal matrix
B =




U
0
· · ·
0
0
U
· · ·
0
...
...
...
...
0
0
· · ·
U



,
then
BT LB = ˜L =






D
−I
−I
D
−I
...
...
...
−I
D
−I
−I
D






.
Consider the permutation obtained by placing the numbers 1, 2, . . . , n2 rowwise
in a square matrix, and then reordering them by listing the entries columnwise.
For example, when n = 3 this permutation is generated as follows:
v = (1, 2, 3, 4, 5, 6, 7, 8, 9) →A =


1
2
3
4
5
6
7
8
9

→(1, 4, 7, 2, 5, 8, 3, 6, 9) = ˜v.
Equivalently, this can be described in terms of wrapping and unwrapping rows by
writing v
wrap
−−−→A −→AT unwrap
−−−−→˜v. If P is the associated n2 × n2 permutation
matrix, then
PT ˜LP=




T1
0
· · ·
0
0
T2
· · ·
0
...
...
...
...
0
0
· · ·
Tn




with
Ti =






λi
−1
−1
λi
−1
...
...
...
−1
λi
−1
−1
λi






n×n
.

566
Chapter 7
Eigenvalues and Eigenvectors
If you try it on the 9 × 9 case, you will see why it works. Now, Ti is another
tridiagonal Toeplitz matrix, so Example 7.2.5 (p. 514) again applies to yield
σ (Ti) = {λi −2 cos (jπ/n + 1) , j = 1, 2, . . . , n} . This together with (7.6.7) pro-
duces the n2 eigenvalues of L as
λij = 4 −2
%
cos

iπ
n + 1

+ cos
 jπ
n + 1
&
,
i, j = 1, 2, . . . , n,
or, by using the identity 1 −cos θ = 2 sin2(θ/2),
λij = 4
%
sin2

iπ
2(n + 1)

+ sin2

jπ
2(n + 1)
&
,
i, j = 1, 2, . . . , n.
(7.6.8)
Since each λij is positive, L must be positive deﬁnite. As a corollary, L is
nonsingular, and hence Lu = g yields a unique solution for the steady-state
temperatures on the square plate (otherwise something would be amiss).
At ﬁrst glance it’s tempting to think that statements about positive deﬁnite
matrices translate to positive semideﬁnite matrices simply by replacing the word
“positive” by “nonnegative,” but this is not always true. When A has zero
eigenvalues (i.e., when A is singular) there is no LU factorization, and, unlike the
positive deﬁnite case, having nonnegative leading principal minors doesn’t insure
that A is positive semideﬁnite—e.g., consider A =
 0
0
0
−1

. The positive
deﬁnite properties that have semideﬁnite analogues are listed below.
Positive Semideﬁnite Matrices
For real-symmetric matrices such that rank (An×n) = r, the following
statements are equivalent, so any one of them can serve as the deﬁnition
of a positive semideﬁnite matrix.
•
xT Ax ≥0 for all x ∈ℜn×1 (the most common deﬁnition). (7.6.9)
•
All eigenvalues of A are nonnegative.
(7.6.10)
•
A = BT B for some B with rank (B) = r.
(7.6.11)
•
All principal minors of A are nonnegative.
(7.6.12)
For hermitian matrices, replace (⋆)T by (⋆)∗and ℜby C.
Proof of (7.6.9) =⇒(7.6.10). The hypothesis insures xT Ax ≥0 for eigenvectors
of A. If (λ, x) is an eigenpair, then λ = xT Ax/xT x = ∥Bx∥2
2 / ∥x∥2
2 ≥0.
Proof of (7.6.10) =⇒(7.6.11). Similar to the positive deﬁnite case, if each λi ≥0,
write A = PD1/2D1/2PT = BT B, where B = D1/2PT has rank r.

7.6 Positive Deﬁnite Matrices
567
Proof of (7.6.11) =⇒(7.6.12). If Pk is a principal submatrix of A, then

Pk
⋆
⋆
⋆

= QT AQ = QT BT BQ =

FT
⋆
 
F | ⋆

=⇒
Pk = FT F
for a permutation matrix Q. Thus det (Pk) = det

FT F

≥0 (Exercise 6.1.10).
Proof of (7.6.12) =⇒(7.6.9). If Ak is the leading k × k principal submatrix
of A, and if {µ1, µ2, . . . , µk} are the eigenvalues (including repetitions) of Ak,
then ϵI + Ak has eigenvalues {ϵ + µ1, ϵ + µ2, . . . , ϵ + µk}, so, for every ϵ > 0,
det (ϵI + Ak) = (ϵ + µ1)(ϵ + µ2) · · · (ϵ + µk) = ϵk + s1ϵk−1 + · · · + ϵsk−1 + sk > 0
because sj is the jth symmetric function of the µi ’s (p. 494), and, by (7.1.6),
sj is the sum of the j × j principal minors of Ak, which are principal minors
of A. In other words, each leading principal minor of ϵI + A is positive, so
ϵI+A is positive deﬁnite by the results on p. 559. Consequently, for each nonzero
x ∈ℜn×1, we must have xT (ϵI + A)x > 0 for every ϵ > 0. Let ϵ →0+ (i.e.,
through positive values) to conclude that xT Ax ≥0 for each x ∈ℜn×1.
Quadratic Forms
For a vector x ∈ℜn×1 and a matrix A ∈ℜn×n, the scalar function
deﬁned by
f(x) = xT Ax =
n

i=1
n

j=1
aijxixj
(7.6.13)
is called a quadratic form. A quadratic form is said to be positive def-
inite whenever A is a positive deﬁnite matrix. In other words, (7.6.13)
is a positive deﬁnite form if and only if f(x) > 0 for all 0 ̸= x ∈ℜn×1.
Because xT Ax = xT 
(A+AT )/2

x, and because (A+AT )/2 is symmet-
ric, the matrix of a quadratic form can always be forced to be symmetric. For
this reason it is assumed that the matrix of every quadratic form is symmetric.
When x ∈Cn×1,
A ∈Cn×n, and A is hermitian, the expression xHAx is
known as a complex quadratic form.
Example 7.6.3
Diagonalization of a Quadratic Form. A quadratic form f(x) = xT Dx
is said to be a diagonal form whenever Dn×n is a diagonal matrix, in which
case xT Dx = n
i=1 diix2
i (there are no cross-product terms). Every quadratic
form xT Ax can be diagonalized by making a change of variables (coordinates)

568
Chapter 7
Eigenvalues and Eigenvectors
y = QT x. This follows because A is symmetric, so there is an orthogonal ma-
trix Q such that QT AQ = D = diag (λ1, λ2, . . . , λn) , where λi ∈σ (A) , and
setting y = QT x (or, equivalently, x = Qy) gives
xT Ax = yT QT AQy = yT Dy =
n

i=1
λiy2
i .
(7.6.14)
This shows that the nature of the quadratic form is determined by the eigenvalues
of A (which are necessarily real). The eﬀect of diagonalizing a quadratic form in
this way is to rotate the standard coordinate system so that in the new coordinate
system the graph of xT Ax = α is in “standard form.” If A is positive deﬁnite,
then all of its eigenvalues are positive (p. 559), so (7.6.14) makes it clear that the
graph of xT Ax = α for a constant α > 0 is an ellipsoid centered at the origin.
Go back and look at Figure 7.2.1 (p. 505), and see Exercise 7.6.4 (p. 571).
Example 7.6.4
Congruence. It’s not necessary to solve an eigenvalue problem to diagonalize
a quadratic form because a congruence transformation CT AC in which C
is nonsingular (but not necessarily orthogonal) can be found that will do the
job. A particularly convenient congruence transformation is produced by the
LDU factorization for A, which is A = LDLT because A is symmetric—see
Exercise 3.10.9 (p. 157). This factorization is relatively cheap, and the diagonal
entries in D = diag (p1, p2, . . . , pn) are the pivots that emerge during Gaussian
elimination (p. 154). Setting y = LT x (or, equivalently, x = (LT )−1y) yields
xT Ax = yT Dy =
n

i=1
piy2
i .
The inertia of a real-symmetric matrix A is deﬁned to be the triple (ρ, ν, ζ)
in which ρ,
ν, and ζ are the respective number of positive, negative, and
zero eigenvalues, counting algebraic multiplicities. In 1852 J. J. Sylvester (p. 80)
discovered that the inertia of A is invariant under congruence transformations.
Sylvester’s Law of Inertia
Let A ∼= B denote the fact that real-symmetric matrices A and B
are congruent (i.e., CT AC = B for some nonsingular C). Sylvester’s
law of inertia states that:
A ∼= B
if and only if A and B have the same inertia.

7.6 Positive Deﬁnite Matrices
569
Proof.
77 Observe that if An×n is real and symmetric with inertia (p, j, s), then
A ∼=


Ip×p
−Ij×j
0s×s

= E,
(7.6.15)
because if {λ1, . . . , λp, −λp+1, . . . , −λp+j, 0, . . . , 0} are the eigenvalues of A
(counting multiplicities) with each λi > 0, there is an orthogonal matrix P
such that PT AP = diag (λ1, . . . , λp, −λp+1, . . . , −λp+j, 0, . . . , 0) , so C = PD,
where D = diag

λ−1/2
1
, . . . , λ−1/2
p+j , 1, . . . , 1

, is nonsingular and CT AC = E.
Let B be a real-symmetric matrix with inertia (q, k, t) so that
B ∼=


Iq×q
−Ik×k
0t×t

= F.
If B ∼= A, then F ∼= E (congruence is transitive), so rank (F) = rank (E), and
hence s = t. To show that p = q, assume to the contrary that p > q, and write
F = KT EK for some nonsingular K =

Xn×q | Yn×n−q

. If M = R (Y) ⊆ℜn
and N = span {e1, . . . , ep} ⊆ℜn, then using the formula (4.4.19) for the dimen-
sion of a sum (p. 205) yields
dim(M∩N) = dim M+dim N −dim(M+N) = (n−q)+p−dim(M+N) > 0.
Consequently, there exists a nonzero vector x ∈M ∩N. For such a vector,
x ∈M
=⇒
x = Yy = K

0
y

=⇒
xT Ex =

0T | yT 
F

0
y

≤0,
and
x ∈N
=⇒
x = (x1, . . . , xp, 0, . . . , 0)T
=⇒
xT Ex > 0,
which is impossible. Therefore, we can’t have p > q. A similar argument shows
that it’s also impossible to have p < q, so p = q. Thus it is proved that if
A ∼= B, then A and B have the same inertia. Conversely, if A and B have in-
ertia (p, j, s), then the argument that produced (7.6.15) yields A ∼= E ∼= B.
77
The fact that inertia is invariant under congruence is also a corollary of a deeper theo-
rem stating that the eigenvalues of A vary continuously with the entries. The argument
is as follows. Assume A is nonsingular (otherwise consider A + ϵI for small ϵ), and set
X(t) = tQ + (1 −t)QR for t ∈[0, 1], where C = QR is the QR factorization. Both X(t)
and Y(t) = XT (t)AX(t) are nonsingular on [0, 1], so continuity of eigenvalues insures that
no eigenvalue Y(t) can cross the origin as t goes from 0 to 1. Hence Y(0) = CT AC has
the same number of positive (and negative) eigenvalues as Y(1) = QT AQ, which is similar
to A. Thus CT AC and A have the same inertia.

570
Chapter 7
Eigenvalues and Eigenvectors
Example 7.6.5
Taylor’s theorem in ℜn says that if f is a smooth real-valued function deﬁned
on ℜn, and if x0 ∈ℜn×1, then the value of f at x ∈ℜn×1 is given by
f(x) = f(x0) + (x −x0)T g(x0) + (x −x0)T H(x0)(x −x0) + O(∥x −x0∥3),
where g(x0) = ∇f(x0) (the gradient of f evaluated at x0) has components
gi = ∂f/∂xi

x0
, and where H(x0) is the Hessian matrix whose entries are
given by hij = ∂2f/∂xi∂xj

x0
. Just as in the case of one variable, the vector
x0 is called a critical point when g(x0) = 0. If x0 is a critical point, then
Taylor’s theorem shows that (x −x0)T H(x0)(x −x0) governs the behavior of
f at points x near to x0. This observation yields the following conclusions
regarding local maxima or minima.
•
If x0 is a critical point such that H(x0) is positive deﬁnite, then f has a
local minimum at x0.
•
If x0 is a critical point such that H(x0) is negative deﬁnite (i.e., zT Hz < 0
for all z ̸= 0 or, equivalently, −H is positive deﬁnite), then f has a local
maximum at x0.
Exercises for section 7.6
7.6.1. Which of the following matrices are positive deﬁnite?
A =


1
−1
−1
−1
5
1
−1
1
5

.
B =


20
6
8
6
3
0
8
0
8

.
C =


2
0
2
0
6
2
2
2
4

.
7.6.2. Spring-Mass Vibrations.
Two masses m1 and m2 are suspended
between three identical springs (with spring constant k) as shown in
Figure 7.6.7. Each mass is initially displaced from its equilibrium posi-
tion by a horizontal distance and released to vibrate freely (assume there
is no vertical displacement).
m1
m2
m1
m2
x1
x2
Figure 7.6.7

7.6 Positive Deﬁnite Matrices
571
(a) If xi(t) denotes the horizontal displacement of mi from equilibrium at
time t, show that Mx′′ = Kx, where
M =

m1
0
0
m2

,
x =

x1(t)
x2(t)

,
and
K = k

2
−1
−1
2

.
(Consider a force directed to the left to be positive.) Notice that the
mass-stiﬀness equation Mx′′ = Kx is the matrix version of Hooke’s
law F = kx, and K is positive deﬁnite.
(b) Look for a solution of the form x = eiθtv for a constant vector v, and
show that this reduces the problem to solving an algebraic equation of
the form Kv = λMv (for λ = −θ2). This is called a generalized
eigenvalue problem because when M = I we are back to the ordi-
nary eigenvalue problem. The generalized eigenvalues λ1 and λ2 are
the roots of the equation det (K −λM) = 0—ﬁnd them when k = 1,
m1 = 1, and m2 = 2, and describe the two modes of vibration.
(c) Take m1 = m2 = m, and apply the technique used in the vibrating
beads problem in Example 7.6.1 (p. 559) to determine the normal modes.
Compare the results with those of part (b).
7.6.3. Three masses m1, m2, and m3 are suspended on three identical springs
(with spring constant k) as shown below. Each mass is initially displaced
from its equilibrium position by a vertical distance and then released to
vibrate freely.
(a)
If yi(t) denotes the displacement of mi from equilibrium
at time t, show that the mass-stiﬀness equation is My′′ = Ky,
where
M=


m1
0
0
0
m2
0
0
0
m3

, y=


y1(t)
y2(t)
y3(t)

, K=k


2
−1
0
−1
2
−1
0
−1
1


(k33 = 1 is not a mistake!).
(b)
Show that K is positive deﬁnite.
(c)
Find the normal modes when m1 = m2 = m3 = m.
7.6.4. By diagonalizing the quadratic form 13x2 + 10xy + 13y2, show that the
rotated graph of 13x2 +10xy +13y2 = 72 is an ellipse in standard form
as shown in Figure 7.2.1 on p. 505.
7.6.5. Suppose that A is a real-symmetric matrix. Explain why the signs of
the pivots in the LDU factorization for A reveal the inertia of A.

572
Chapter 7
Eigenvalues and Eigenvectors
7.6.6. Consider the quadratic form
f(x) = 1
9(−2x2
1 + 7x2
2 + 4x2
3 + 4x1x2 + 16x1x3 + 20x2x3).
(a)
Find a symmetric matrix A so that f(x) = xT Ax.
(b)
Diagonalize the quadratic form using the LDLT factorization
as described in Example 7.6.4, and determine the inertia of A.
(c)
Is this a positive deﬁnite form?
(d)
Verify the inertia obtained above is correct by computing the
eigenvalues of A.
(e)
Verify Sylvester’s law of inertia by making up a congruence
transformation C and then computing the inertia of CT AC.
7.6.7. Polar Factorization. Explain why each nonsingular A ∈Cn×n can be
uniquely factored as A = RU, where R is hermitian positive deﬁnite
and U is unitary. This is the matrix analog of the polar form of a
complex number z = reiθ,
r > 0, because 1 × 1 hermitian positive
deﬁnite matrices are positive real numbers, and 1 × 1 unitary matrices
are points on the unit circle. Hint: First explain why R = (AA∗)1/2.
7.6.8. Explain why trying to produce better approximations to the solution
of the Dirichlet problem in Example 7.6.2 by using ﬁner meshes with
more grid points results in an increasingly ill-conditioned linear system
Lu = g.
7.6.9. For a given function f the equation ∇2u = f is called Poisson’s
78
equation. Consider Poisson’s equation on a square in two dimensions
with Dirichlet boundary conditions. That is,
∂2u
∂x2 + ∂2u
∂y2 = f(x, y)
with
u(x, y) = g(x, y)
on the boundary.
78
Sim´eon Denis Poisson (1781–1840) was a proliﬁc French scientist who was originally encouraged
to study medicine but was seduced by mathematics. While he was still a teenager, his work
attracted the attention of the reigning scientiﬁc elite of France such as Legendre, Laplace, and
Lagrange. The latter two were originally his teachers (Lagrange was his thesis director) at the
´Ecole Polytechnique, but they eventually became his friends and collaborators. It is estimated
that Poisson published about 400 scientiﬁc articles, and his 1811 book Trait´e de m´ecanique was
the standard reference for mechanics for many years. Poisson began his career as an astronomer,
but he is primarily remembered for his impact on applied areas such as mechanics, probability,
electricity and magnetism, and Fourier series. This seems ironic because he held the chair of
“pure mathematics” in the Facult´e des Sciences. The next time you ﬁnd yourself on the streets
of Paris, take a stroll on the Rue Denis Poisson, or you can check out Poisson’s plaque, along
with those of Lagrange, Laplace, and Legendre, on the ﬁrst stage of the Eiﬀel Tower.

7.6 Positive Deﬁnite Matrices
573
Discretize the problem by overlaying the square with a regular mesh con-
taining n2 interior points at equally spaced intervals of length h as ex-
plained in Example 7.6.2 (p. 563). Let fij = f(xi, yj), and deﬁne f to be
the vector f = (f11, f12, . . . , f1n|f21, f22 . . . , f2n| · · · |fn1, fn2, . . . , fnn)T .
Show that the discretization of Poisson’s equation produces a system
of linear equations of the form Lu = g −h2f, where L is the discrete
Laplacian and where u and g are as described in Example 7.6.2.
7.6.10. As deﬁned in Exercise 5.8.15 (p. 380) and discussed in Exercise 7.8.11
(p. 597) the Kronecker product (sometimes called tensor product, or
direct product) of matrices Am×n and Bp×q is the mp × nq matrix
A ⊗B =




a11B
a12B
· · ·
a1nB
a21B
a22B
· · ·
a2nB
...
...
...
...
am1B
am2B
· · ·
amnB



.
Verify that if In is the n × n identity matrix, and if
An =






2
−1
−1
2
−1
...
...
...
−1
2
−1
−1
2






n×n
is the nth-order ﬁnite diﬀerence matrix of Example 1.4.1 (p. 19), then
the discrete Laplacian is given by
Ln2×n2 = (In ⊗An) + (An ⊗In).
Thus we have an elegant matrix connection between the ﬁnite diﬀerence
approximations of the one-dimensional and two-dimensional Laplacians.
This formula leads to a simple alternate derivation of (7.6.8)—see Exer-
cise 7.8.12 (p. 598). As you might guess, the discrete three-dimensional
Laplacian is
Ln3×n3 = (In ⊗In ⊗An) + (In ⊗An ⊗In) + (An ⊗In ⊗In).

574
Chapter 7
Eigenvalues and Eigenvectors
7.7
NILPOTENT MATRICES AND JORDAN STRUCTURE
While it’s not always possible to diagonalize a matrix A ∈Cm×m with a similar-
ity transformation, Schur’s theorem (p. 508) guarantees that every A ∈Cm×m
is unitarily similar to an upper-triangular matrix—say U∗AU = T. But other
than the fact that the diagonal entries of T are the eigenvalues of A, there is
no pattern to the nonzero part of T. So to what extent can this be remedied by
giving up the unitary nature of U? In other words, is there a nonunitary P for
which P−1AP has a simpler and more predictable pattern than that of T? We
have already made the ﬁrst step in answering this question. The core-nilpotent
decomposition (p. 397) says that for every singular matrix A of index k and
rank r, there is a nonsingular matrix Q such that
Q−1AQ =

Cr×r
0
0
L

, where rank (C) = r and L is nilpotent of index k.
Consequently, any further simpliﬁcation by means of similarity transformations
can revolve around C and L. Let’s begin by examining the degree to which
nilpotent matrices can be reduced by similarity transformations.
In what follows, let Ln×n be a nilpotent matrix of index k so that Lk = 0
but Lk−1 ̸= 0. The ﬁrst question is, “Can L be diagonalized by a similarity
transformation?” To answer this, notice that λ = 0 is the only eigenvalue of L
because
Lx = λx
=⇒
Lkx = λkx
=⇒
0 = λkx
=⇒
λ = 0
(since x ̸= 0 ).
So if L is to be diagonalized by a similarity transformation, it must be the case
that P−1LP = D = 0 (diagonal entries of D must be eigenvalues of L ), and
this forces L = 0. In other words, the only nilpotent matrix that is similar to a
diagonal matrix is the zero matrix.
Assume L ̸= 0 from now on so that L is not diagonalizable. Since L
can always be triangularized (Schur’s theorem again), our problem boils down
to ﬁnding a nonsingular P such that P−1LP is an upper-triangular matrix
possessing a simple and predictable form. This turns out to be a fundamental
problem, and the rest of this section is devoted to its solution. But before diving
in, let’s set the stage by thinking about some possibilities.
If P−1LP = T is upper triangular, then the diagonal entries of T must
be the eigenvalues of L, so T must have the form
T =





0
⋆
· · ·
⋆
...
...
...
...
⋆
0




.

7.7 Nilpotent Matrices and Jordan Structure
575
One way to simplify the form of T is to allow nonzero entries only on the
superdiagonal (the diagonal immediately above the main diagonal) of T, so we
might try to construct a nonsingular P such that T has the form
T =





0
⋆
...
...
...
⋆
0




.
To gain some insight on how this might be accomplished, let L be a 3 × 3
nilpotent matrix for which L3 = 0 and L2 ̸= 0, and search for a P such that
P−1LP =


0
1
0
0
0
1
0
0
0

⇐⇒L[ P∗1 P∗2 P∗3 ] = [ P∗1 P∗2 P∗3 ]


0
1
0
0
0
1
0
0
0


⇐⇒LP∗1 = 0,
LP∗2 = P∗1,
LP∗3 = P∗2.
Since L3 = 0, we can set P∗1 = L2x for any x3×1 such that L2x ̸= 0. This
in turn allows us to set P∗2 = Lx and P∗3 = x. Because J = {L2x, Lx, x}
is a linearly independent set (Exercise 5.10.8), P = [ L2x | Lx | x ] will do the
job. J is called a Jordan chain, and it is characterized by the fact that its
ﬁrst vector is a somewhat special eigenvector for L while the other vectors are
built (or “chained”) on top of this eigenvector to form a special basis for C3.
There are a few more wrinkles in the development of a general theory for n × n
nilpotent matrices, but the features illustrated here illuminate the path.
For a general nilpotent matrix Ln×n ̸= 0 of index k, we know that λ = 0 is
the only eigenvalue, so the set of eigenvectors of L is N (L) (excluding the zero
vector of course). Realizing that L is not diagonalizable is equivalent to realizing
that L does not possess a complete linearly independent set of eigenvectors or,
equivalently, dim N (L) < n. As in the 3 × 3 example above, the strategy for
building a similarity transformation P that reduces L to a simple triangular
form is as follows.
(1)
Construct a somewhat special basis B for N (L).
(2)
Extend B to a basis for Cn by building Jordan chains on top of the
eigenvectors in B.
To accomplish (1), consider the subspaces deﬁned by
Mi = R

Li
∩N (L)
for i = 0, 1, . . . , k,
(7.7.1)
and notice (Exercise 7.7.4) that these subspaces are nested as
0 = Mk ⊆Mk−1 ⊆Mk−2 ⊆· · · ⊆M1 ⊆M0 = N (L).

576
Chapter 7
Eigenvalues and Eigenvectors
Use these nested spaces to construct a basis for N (L) = M0 by starting with
any basis Sk−1 for Mk−1 and by sequentially extending Sk−1 with addi-
tional sets Sk−2, Sk−3, . . . , S0 such that Sk−1 ∪Sk−2 is a basis for Mk−2,
Sk−1 ∪Sk−2 ∪Sk−3 is a basis for Mk−3, etc. In general, Si is a set of vectors
that extends Sk−1 ∪Sk−2 ∪· · · ∪Si−1 to a basis for Mi. Figure 7.7.1 is a
heuristic diagram depicting an example of k = 5 nested subspaces Mi along
with some typical extension sets Si that combine to form a basis for N (L).
Figure 7.7.1
Now extend the basis B = Sk−1 ∪Sk−2 ∪· · · ∪S0 = {b1, b2, . . . , bt} for
N (L) to a basis for Cn by building Jordan chains on top of each b ∈B. If
b ∈Si, then there exists a vector x such that Lix = b because each b ∈Si
belongs to Mi = R

Li
∩N (L) ⊆R

Li
. A Jordan chain is built on top of
each b ∈Si by solving the system Lix = b for x and by setting
Jb = {Lix, Li−1x, . . . , Lx, x}.
(7.7.2)
Notice that chains built on top of vectors from Si each have length i + 1. The
heuristic diagram in Figure 7.7.2 depicts Jordan chains built on top of the basis
vectors illustrated in Figure 7.7.1—the chain that is labeled is built on top of a
vector b ∈S3.
Figure 7.7.2

7.7 Nilpotent Matrices and Jordan Structure
577
The collection of vectors in all of these Jordan chains is a basis for Cn.
To demonstrate this, ﬁrst it must be argued that the total number of vectors
in all Jordan chains is n, and then it must be proven that this collection is a
linearly independent set. To count the number of vectors in all Jordan chains
Jb, ﬁrst recall from (4.5.1) that the rank of a product is given by the formula
rank (AB) = rank (B) −dim N (A) ∩R (B), and apply this to conclude that
dim Mi = dim R

Li
∩N (L) = rank

Li
−rank

LLi
. In other words, if we
set di = dim Mi and ri = rank

Li
, then
di = dim Mi = rank

Li
−rank

Li+1
= ri −ri+1,
(7.7.3)
so the number of vectors in Si is
νi = di −di+1 = ri −2ri+1 + ri+2.
(7.7.4)
Since every chain emanating from a vector in Si contains i + 1 vectors, and
since dk = 0 = rk, the total number of vectors in all Jordan chains is
total =
k−1

i=0
(i + 1)νi =
k−1

i=0
(i + 1)(di −di+1)
= d0 −d1 + 2(d1 −d2) + 3(d2 −d3) + · · · + k(dk−1 −dk)
= d0 + d1 + · · · + dk−1
= (r0 −r1) + (r1 −r2) + (r2 −r3) + · · · + (rk−1 −rk)
= r0 = n.
To prove that the set of all vectors from all Jordan chains is linearly independent,
place these vectors as columns in a matrix Qn×n and show that N (Q) = 0.
The trick in doing so is to arrange the vectors from the Jb ’s in just the right
order. Begin by placing the vectors at the top level in chains emanating from Si
as columns in a matrix Xi as depicted in the heuristic diagram in Figure 7.7.3.
Figure 7.7.3

578
Chapter 7
Eigenvalues and Eigenvectors
The matrix LXi contains all vectors at the second highest level of those chains
emanating from Si, while L2Xi contains all vectors at the third highest level
of those chains emanating from Si, and so on. In general, LjXi contains all
vectors at the (j+1)st highest level of those chains emanating from Si. Proceed
by ﬁlling in Q = [ Q0 | Q1 | · · · | Qk−1 ] from the bottom up by letting Qj be
the matrix whose columns are all vectors at the jth level from the bottom in all
chains. For the example illustrated in Figures 7.7.1–7.7.3 with k = 5,
Q0 = [ X0 | LX1 | L2X2 | L3X3 | L4X4 ] = vectors at level 0 = basis B for N (L),
Q1 = [ X1 | LX2 | L2X3 | L3X4 ] = vectors at level 1 (from the bottom),
Q2 = [ X2 | LX3 | L2X4 ] = vectors at level 2 (from the bottom),
Q3 = [ X3 | LX4 ] = vectors at level 3 (from the bottom),
Q4 = [ X4 ] = vectors at level 4 (from the bottom).
In general, Qj = [ Xj | LXj+1 | L2Xj+2 | · · · | Lk−1−jXk−1 ]. Since the columns
of LjXj are all on the bottom level (level 0), they are part of the basis B for
N (L). This means that the columns of LjQj are also part of the basis B for
N (L), so they are linearly independent, and thus N

LjQj

= 0. Furthermore,
since the columns of LjQj are in N (L), we have L

LjQj

= 0, and hence
Lj+hQj = 0 for all h ≥1. Now use these observations to prove N (Q) = 0. If
Qz = 0, then multiplication by Lk−1 yields
0 = Lk−1Qz = [ Lk−1Q0 | Lk−1Q1 | · · · | Lk−1Qk−1 ] z
= [ 0 | 0 | · · · | Lk−1Qk−1 ]




z0
z2
...
zk−1




=⇒
zk−1 ∈N

Lk−1Qk−1

=⇒
zk−1 = 0.
This conclusion with the same argument applied to 0 = Lk−2Qz produces
zk−2 = 0. Similar repetitions show that zi = 0 for each i, and thus N (Q) = 0.
It has now been proven that if B = Sk−1 ∪Sk−2 ∪· · ·∪S0 = {b1, b2, . . . , bt}
is the basis for N (L) derived from the nested subspaces Mi, then the set of
all Jordan chains J = Jb1 ∪Jb2 ∪· · · ∪Jbt is a basis for Cn. If the vectors
from J are placed as columns (in the order in which they appear in J ) in a
matrix Pn×n = [ J1 | J2 | · · · | Jt ], then P is nonsingular, and if bj ∈Si, then
Jj = [ Lix | Li−1x | · · · | Lx | x ] for some x such that Lix = bj so that
LJj = [ 0 | Lix | · · · | Lx ] = [ Lix | · · · | Lx | x ]





0
1
...
...
...
1
0




= JjNj,

7.7 Nilpotent Matrices and Jordan Structure
579
where Nj is an (i + 1) × (i + 1) matrix whose entries are equal to 1 along the
superdiagonal and zero elsewhere. Therefore,
LP = [ LJ1 | LJ2 | · · · | LJt ] = [ J1 | J2 | · · · | Jt ]




N1
0
· · ·
0
0
N2
· · ·
0
...
...
...
0
0
· · ·
Nt




or, equivalently,
P−1LP = N =




N1 0 · · ·
0
0
N2· · ·
0
...
...
...
0
0 · · · Nt



, where Nj =





0
1
...
...
...
1
0




. (7.7.5)
Each Nj is a nilpotent matrix whose index is given by its size. The Nj ’s are
called nilpotent Jordan blocks, and the block-diagonal matrix N is called the
Jordan form for L. Below is a summary.
Jordan Form for a Nilpotent Matrix
Every nilpotent matrix Ln×n of index k is similar to a block-diagonal
matrix
P−1LP = N =




N1 0 · · ·
0
0
N2· · ·
0
...
...
...
0
0 · · · Nt




(7.7.6)
in which each Nj is a nilpotent matrix having ones on the superdiagonal
and zeros elsewhere—see (7.7.5).
•
The number of blocks in N is given by t = dim N (L).
•
The size of the largest block in N is k × k.
•
The number of i × i blocks in N is νi = ri−1 −2ri + ri+1, where
ri = rank

Li
—this follows from (7.7.4).
•
If B = Sk−1 ∪Sk−2 ∪· · ·∪S0 = {b1, b2, . . . , bt} is a basis for N (L)
derived from the nested subspaces Mi = R

Li
∩N (L), then
▷
the set of vectors J = Jb1 ∪Jb2 ∪· · · ∪Jbt from all Jordan
chains is a basis for Cn;
▷
Pn×n = [ J1 | J2 | · · · | Jt ] is the nonsingular matrix containing
these Jordan chains in the order in which they appear in J .

580
Chapter 7
Eigenvalues and Eigenvectors
The following theorem demonstrates that the Jordan structure (the num-
ber and the size of the blocks in N ) is uniquely determined by L, but P is
not. In other words, the Jordan form is unique up to the arrangement of the
individual Jordan blocks.
Uniqueness of the Jordan Structure
The structure of the Jordan form for a nilpotent matrix Ln×n of index
k is uniquely determined by L in the sense that whenever L is similar
to a block-diagonal matrix B = diag (B1, B2, . . . , Bt) in which each
Bi has the form
Bi =






0
ϵi
0
· · ·
0
0
0
ϵi
0
...
...
...
...
0
0
· · ·
0
ϵi
0
0
· · ·
0
0






ni×ni
for
ϵi ̸= 0,
then it must be the case that t = dim N (L), and the number of
blocks having size i × i must be given by ri−1 −2ri + ri+1, where
ri = rank

Li
.
Proof.
Suppose that L is similar to both B and N, where B is as described
above and N is as described in (7.7.6). This implies that B and N are similar,
and hence rank

Bi
= rank

Li
= ri for every nonnegative integer i. In
particular, index (B) = index (L). Each time a block Bi is powered, the line of
ϵi ’s moves to the next higher diagonal level so that
rank (Bp
i ) =
	
ni −p
if p < ni,
0
if p ≥ni.
Since rp = rank (Bp) = t
i=1 rank (Bp
i ), it follows that if ωi is the number of
i × i blocks in B, then
rk−1 = ωk,
rk−2 = ωk−1 + 2ωk,
rk−3 = ωk−2 + 2ωk−1 + 3ωk,
...
and, in general, ri = ωi+1 + 2ωi+2 + · · · + (k −i)ωk. It’s now straightforward
to verify that ri−1 −2ri + ri+1 = ωi. Finally, using this equation together with
(7.7.4) guarantees that the number of blocks in B must be
t =
k

i=1
ωi =
k

i=1
(ri−1 −2ri + ri+1) =
k

i=1
νi = dim N (L).

7.7 Nilpotent Matrices and Jordan Structure
581
The manner in which we developed the Jordan theory spawned 1’s on the su-
perdiagonals of the Jordan blocks Ni in (7.7.5). But it was not necessary to
do so—it was simply a matter of convenience. In fact, any nonzero value can be
forced onto the superdiagonal of any Ni —see Exercise 7.7.9. In other words,
the fact that 1’s appear on the superdiagonals of the Ni ’s is artiﬁcial and is not
important to the structure of the Jordan form for L. What’s important, and
what constitutes the “Jordan structure,” is the number and sizes of the Jordan
blocks (or chains) and not the values appearing on the superdiagonals of these
blocks.
Example 7.7.1
Problem: Determine the Jordan forms for 3 × 3 nilpotent matrices L1, L2,
and L3 that have respective indices k = 1, 2, 3.
Solution: The size of the largest block must be k × k, so
N1 =


0
0
0
0
0
0
0
0
0

,
N2 =


0
1
0
0
0
0
0
0
0

,
N3 =


0
1
0
0
0
1
0
0
0

.
Example 7.7.2
For a nilpotent matrix L, the theoretical development relies on a complicated
basis for N (L) to derive the structure of the Jordan form N as well as the
Jordan chains that constitute a nonsingular matrix P such that P−1LP = N.
But, after the dust settled, we saw that a basis for N (L) is not needed to
construct N because N is completely determined simply by ranks of powers of
L. A basis for N (L) is only required to construct the Jordan chains in P.
Question:
For the purpose of constructing Jordan chains in P, can we use an
arbitrary basis for N (L) instead of the complicated basis built from the Mi ’s?
Answer:
No!
Consider the nilpotent matrix
L =


2
0
1
−4
0
−2
−4
0
−2


and its Jordan form
N =


0
1
0
0
0
0
0
0
0

.
If P−1LP = N, where P = [ x1 | x2 | x3 ], then LP = PN implies that
Lx1 = 0,
Lx2 = x1, and Lx3 = 0. In other words, B = {x1, x3} must
be a basis for N (L), and Jx1 = {x1, x2} must be a Jordan chain built on top
of x1. If we try to construct such vectors by starting with the naive basis
x1 =


1
0
−2


and
x3 =


0
1
0


(7.7.7)

582
Chapter 7
Eigenvalues and Eigenvectors
for N (L) obtained by solving Lx = 0 with straightforward Gaussian elimi-
nation, we immediately hit a brick wall because x1 ̸∈R (L) means Lx2 = x1
is an inconsistent system, so x2 cannot be determined. Similarly, x3 ̸∈R (L)
insures that the same diﬃculty occurs if x3 is used in place of x1. In other
words, even though the vectors in (7.7.7) constitute an otherwise perfectly good
basis for N (L), they can’t be used to build P.
Example 7.7.3
Problem: Let Ln×n be a nilpotent matrix of index k. Provide an algorithm
for constructing the Jordan chains that generate a nonsingular matrix P such
that P−1LP = N is in Jordan form.
Solution:
1.
Start with the fact that Mk−1 = R

Lk−1
(Exercise 7.7.5), and deter-
mine a basis {y1, y2, . . . , yq} for R

Lk−1
.
2.
Extend {y1, y2, . . . , yq} to a basis for Mk−2 = R

Lk−2
∩N (L) as
follows.
▷
Find a basis {v1, v2, . . . , vs} for N (LB), where B is a matrix con-
taining a basis for R

Lk−2
—e.g., the basic columns of Lk−2. The
set {Bv1, Bv2, . . . , Bvs} is a basis for Mk−2 (see p. 211).
▷
Find the basic columns in [ y1 | y2 | · · · | yq | Bv1 | Bv2 | · · · | Bvs ]. Say
they are {y1, . . . , yq, Bvβ1, . . . , Bvβj} (all of the yj ’s are basic be-
cause they are a leading linearly independent subset). This is a basis
for Mk−2 that contains a basis for Mk−1. In other words,
Sk−1 = {y1, y2, . . . , yq}
and
Sk−2 = {Bvβ1, Bvβ2, . . . , Bvβj}.
3.
Repeat the above procedure k −1 times to construct a basis for N (L)
that is of the form B = Sk−1 ∪Sk−2 ∪· · · ∪S0 = {b1, b2, . . . , bt}, where
Sk−1 ∪Sk−2 ∪· · · ∪Si is a basis for Mi for each i = k −1, k −2, . . . , 0.
4.
Build a Jordan chain on top of each bj ∈B. If bj ∈Si, then we solve
Lixj = bj and set Jj = [ Lixj | Li−1xj | · · · | Lxj | xj ]. The desired simi-
larity transformation is Pn×n = [ J1 | J2 | · · · | Jt ].
Example 7.7.4
Problem: Find P and N such that P−1LP = N is in Jordan form, where
L =







1
1
−2
0
1
−1
3
1
5
1
−1
3
−2
−1
0
0
−1
0
2
1
0
0
1
0
−5
−3
−1
−1
−1
−1
−3
−2
−1
−1
0
−1







.

7.7 Nilpotent Matrices and Jordan Structure
583
Solution: First determine the Jordan form for L. Computing ri = rank

Li
reveals that r1 = 3, r2 = 1, and r3 = 0, so the index of L is k = 3, and
the number of 3 × 3 blocks = r2 −2r3 + r4 = 1,
the number of 2 × 2 blocks = r1 −2r2 + r3 = 1,
the number of 1 × 1 blocks = r0 −2r1 + r2 = 1.
Consequently, the Jordan form of L is
N =









0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0









.
Notice that three Jordan blocks were found, and this agrees with the fact that
dim N (L) = 6 −rank (L) = 3. Determine P by following the procedure de-
scribed in Example 7.7.3.
1.
Since rank

L2
= 1, any nonzero column from L2 will be a basis for
M2 = R

L2
, so set y1 = [L2]∗1 = (6, −6, 0, 0, −6, −6)T .
2.
To extend y1 to a basis for M1 = R (L) ∩N (L), use
B = [L∗1 | L∗2 | L∗3 ] =







1
1
−2
3
1
5
−2
−1
0
2
1
0
−5
−3
−1
−3
−2
−1







=⇒LB =







6
3
3
−6
−3
−3
0
0
0
0
0
0
−6
−3
−3
−6
−3
−3







,
and determine a basis for N (LB) to be
	
v1 =
 −1
2
0

, v2 =
 −1
0
2

.
Reducing [ y1 | Bv1 | Bv2 ] to echelon form shows that its basic columns
are in the ﬁrst and third positions, so {y1, Bv2} is a basis for M1 with
S2 =




















6
−6
0
0
−6
−6







= b1













and
S1 =




















−5
7
2
−2
3
1







= b2













.

584
Chapter 7
Eigenvalues and Eigenvectors
3.
Now extend S2 ∪S1 = {b1, b2} to a basis for M0 = N (L). This time,
B = I, and a basis for N (LB) = N (L) can be computed to be
v1 =







2
−4
−1
3
0
0







,
v2 =







−4
5
2
0
3
0







,
and
v3 =







1
−2
−2
0
0
3







,
and {Bv1, Bv2, Bv3} = {v1, v2, v3}. Reducing [ b1 | b2 | v1 | v2 | v3 ] to
echelon form reveals that its basic columns are in positions one, two, and
three, so v1 is the needed extension vector. Therefore, the complete nested
basis for N (L) is
b1 =







6
−6
0
0
−6
−6







∈S2,
b2 =







−5
7
2
−2
3
1







∈S1,
and
b3 =







2
−4
−1
3
0
0







∈S0.
4.
Complete the process by building a Jordan chain on top of each bj ∈Si
by solving Lixj = bj and by setting Jj = [Lixj | · · · | Lxj | xj ]. Since
x1 = e1 solves L2x1 = b1, we have J1 = [ L2e1 | Le1 | e1 ]. Solving
Lx2 = b2 yields x2 = (−1, 0, 2, 0, 0, 0)T , so J2 = [ Lx2 | x2 ]. Finally,
J3 = [ b3 ]. Putting these chains together produces
P = [ J1 | J2 | J3 ] =







6
1
1
−5
−1
2
−6
3
0
7
0
−4
0
−2
0
2
2
−1
0
2
0
−2
0
3
−6
−5
0
3
0
0
−6
−3
0
1
0
0







.
It can be veriﬁed by direct multiplication that P−1LP = N.
It’s worthwhile to pay attention to how the results in this section translate into
the language of direct sum decompositions of invariant subspaces as discussed
in §4.9 (p. 259) and §5.9 (p. 383). For a linear nilpotent operator L of index
k deﬁned on a ﬁnite-dimensional vector space V, statement (7.7.6) on p. 579
means that V can be decomposed as a direct sum V = V1 ⊕V2 ⊕· · ·⊕Vt, where
Vj = span(Jbj) is the space spanned by a Jordan chain emanating from the
basis vector bj ∈N (L) and where t = dim N (L). Furthermore, each Vj is an

7.7 Nilpotent Matrices and Jordan Structure
585
invariant subspace for L, and the matrix representation of L with respect to
the basis J = Jb1 ∪Jb2 ∪· · · ∪Jbt is
[L]J =




N1
0
· · ·
0
0
N2
· · ·
0
...
...
...
...
0
0
· · ·
Nt




in which
Nj =

L/Vj

Jbj
.
(7.7.8)
Exercises for section 7.7
7.7.1. Can the index of an n × n nilpotent matrix ever exceed n?
7.7.2. Determine all possible Jordan forms N for a 4 × 4 nilpotent matrix.
7.7.3. Explain why the number of blocks of size i × i or larger in the Jordan
form for a nilpotent matrix is given by rank

Li−1
−rank

Li
.
7.7.4. For a nilpotent matrix L of index k, let Mi = R

Li
∩N (L). Prove
that Mi ⊆Mi−1 for each i = 0, 1, . . . , k.
7.7.5. Prove that R

Lk−1
∩N (L) = R

Lk−1
for all nilpotent matrices L
of index k > 1. In other words, prove Mk−1 = R

Lk−1
.
7.7.6. Let L be a nilpotent matrix of index k > 1. Prove that if the columns
of B are a basis for R

Li
for i ≤k −1, and if {v1, v2, . . . , vs} is a
basis for N (LB), then {Bv1, Bv2, . . . , Bvs} is a basis for Mi.
7.7.7. Find P and N such that P−1LP = N is in Jordan form, where
L =



3
3
2
1
−2
−1
−1
−1
1
−1
0
1
−5
−4
−3
−2


.
7.7.8. Determine the Jordan form for the following 8 × 8 nilpotent matrix.
L =











41
30
15
7
4
6
1
3
−54
−39
−19
−9
−6
−8
−2
−4
9
6
2
1
2
1
0
1
−6
−5
−3
−2
1
−1
0
0
−32
−24
−13
−6
−2
−5
−1
−2
−10
−7
−2
0
−3
0
3
−2
−4
−3
−2
−1
0
−1
−1
0
17
12
6
3
2
3
2
1











.

586
Chapter 7
Eigenvalues and Eigenvectors
7.7.9. Prove that if N is the Jordan form for a nilpotent matrix L as described
in (7.7.5) and (7.7.6) on p. 579, then for any set of nonzero scalars
{ϵ1, ϵ2, . . . , ϵt} , the matrix L is similar to a matrix ˜N of the form
˜N =




ϵ1N1
0
· · ·
0
0
ϵ2N2· · ·
0
...
...
...
0
0
· · · ϵtNt



.
In other words, the 1’s on the superdiagonal of the Ni ’s in (7.7.5) are
artiﬁcial because any nonzero value can be forced onto the superdiagonal
of any Ni. What’s important in the “Jordan structure” of L is the
number and sizes of the nilpotent Jordan blocks (or chains) and not the
values appearing on the superdiagonals of these blocks.

7.8 Jordan Form
587
7.8
JORDAN FORM
The goal of this section is to do for general matrices A ∈Cn×n what was done for
nilpotent matrices in §7.7—reduce A by means of a similarity transformation
to a block-diagonal matrix in which each block has a simple triangular form.
The two major components for doing this are now in place—they are the core-
nilpotent decomposition (p. 397) and the Jordan form for nilpotent matrices. All
that remains is to connect these two ideas. To do so, it is convenient to adopt
the following terminology.
Index of an Eigenvalue
The index of an eigenvalue λ for a matrix A ∈Cn×n is deﬁned to
be the index of the matrix (A −λI) . In other words, from the charac-
terizations of index given on p. 395, index (λ) is the smallest positive
integer k such that any one of the following statements is true.
•
rank

(A −λI)k
= rank

(A −λI)k+1
.
•
R

(A −λI)k
= R

(A −λI)k+1
.
•
N

(A −λI)k
= N

(A −λI)k+1
.
•
R

(A −λI)k
∩N

(A −λI)k
= 0.
•
Cn = R

(A −λI)k
⊕N

(A −λI)k
.
It is understood that index (µ) = 0 if and only if µ ̸∈σ (A) .
The Jordan form for A ∈Cn×n is derived by digesting the distinct eigen-
values in σ (A) = {λ1, λ2, . . . , λs} one at a time with a core-nilpotent decom-
position as follows. If index (λ1) = k1, then there is a nonsingular matrix X1
such that
X−1
1 (A −λ1I)X1 =

L1
0
0
C1

,
(7.8.1)
where L1 is nilpotent of index k1 and C1 is nonsingular (it doesn’t matter
whether C1 or L1 is listed ﬁrst, so, for the sake of convenience, the nilpotent
block is listed ﬁrst). We know from the results on nilpotent matrices (p. 579)
that there is a nonsingular matrix Y1 such that
Y−1
1 L1Y1 = N(λ1) =




N1(λ1)
0
· · ·
0
0
N2(λ1)
· · ·
0
...
...
...
...
0
0
· · ·
Nt1(λ1)




is a block-diagonal matrix that is characterized by the following features.

588
Chapter 7
Eigenvalues and Eigenvectors
▷
Every block in N(λ1) has the form N⋆(λ1) =




0
1
...
...
...
1
0



.
▷
There are t1 = dim N (L1) = dim N (A −λ1I) such blocks in N(λ1).
▷
The number of i × i blocks of the form N⋆(λ1) contained in N(λ1) is
νi(λ1) = rank

Li−1
1

−2 rank

Li
1

+ rank

Li+1
1

. But C1 in (7.8.1) is
nonsingular, so rank (Lp
1) = rank ((A −λ1I)p) −rank (C1), and thus the
number of i × i blocks N⋆(λ1) contained in N(λ1) can be expressed as
νi(λ1) = ri−1(λ1) −2ri(λ1) + ri+1(λ1),
where
ri(λ1) = rank

(A −λ1I)i
.
Now, Q1=X1
 Y1 0
0
I

is nonsingular, and Q−1
1 (A −λ1I)Q1 =
 N(λ1)
0
0
C1

or,
equivalently,
Q−1
1 AQ1 =

N(λ1) + λ1I
0
0
C1 + λ1I

=

J(λ1)
0
0
A1

.
(7.8.2)
The upper-left-hand segment J(λ1) = N(λ1) + λ1I has the block-diagonal form
J(λ1) =




J1(λ1)
0
· · ·
0
0
J2(λ1) · · ·
0
...
...
...
...
0
0
· · · Jt1(λ1)




with
J⋆(λ1) = N⋆(λ1) + λ1I.
The matrix J(λ1) is called the Jordan segment associated with the eigenvalue
λ1, and the individual blocks J⋆(λ1) contained in J(λ1) are called Jordan
blocks associated with the eigenvalue λ1. The structure of the Jordan segment
J(λ1) is inherited from Jordan structure of the associated nilpotent matrix L1.
▷
Each Jordan block looks like J⋆(λ1) = N⋆(λ1) + λ1I =




λ1
1
...
...
...
1
λ1



.
▷
There are t1 = dim N (A −λ1I) such Jordan blocks in the segment J(λ1).
▷
The number of i × i Jordan blocks J⋆(λ1) contained in J(λ1) is
νi(λ1) = ri−1(λ1) −2ri(λ1) + ri+1(λ1),
where
ri(λ1) = rank

(A −λ1I)i
.
Since the distinct eigenvalues of A are σ (A) = {λ1, λ2, . . . , λs} , the distinct
eigenvalues of A −λ1I are
σ (A −λ1I) = {0, (λ2 −λ1), (λ3 −λ1), . . . , (λs −λ1)}.

7.8 Jordan Form
589
Couple this with the fact that the only eigenvalue for the nilpotent matrix L1
in (7.8.1) is zero to conclude that
σ (C1) = {(λ2 −λ1), (λ3 −λ1), . . . , (λs −λ1)}.
Therefore, the spectrum of A1 = C1+λ1I in (7.8.2) is σ (A1) = {λ2, λ3, . . . , λs}.
This means that the core-nilpotent decomposition process described above can
be repeated on A1 −λ2I to produce a nonsingular matrix Q2 such that
Q−1
2 A1Q2 =

J(λ2)
0
0
A2

,
where
σ (A2) = {λ3, λ4, . . . , λs},
(7.8.3)
and where J(λ2) = diag (J1(λ2), J2(λ2), . . . , Jt2(λ2)) is a Jordan segment com-
posed of Jordan blocks J⋆(λ2) with the following characteristics.
▷
Each Jordan block in J(λ2) has the form J⋆(λ2) =




λ2
1
...
...
...
1
λ2



.
▷
There are t2 = dim N (A −λ2I) Jordan blocks in segment J(λ2).
▷
The number of i × i Jordan blocks in segment J(λ2) is
νi(λ2) = ri−1(λ2) −2ri(λ2) + ri+1(λ2), where ri(λ2) = rank

(A −λ2I)i
.
If we set P2 = Q1
 I
0
0
Q2

, then P2 is a nonsingular matrix such that
P−1
2 AP2 =


J(λ1)
0
0
0
J(λ2)
0
0
0
A2

,
where
σ (A2) = {λ3, λ4, . . . , λs}.
Repeating this process until all eigenvalues have been depleted results in a
nonsingular matrix Ps such that P−1
s APs = J = diag (J(λ1), J(λ2), . . . , J(λs))
in which each J(λj) is a Jordan segment containing tj = dim N (A −λjI) Jor-
dan blocks. The matrix J is called the Jordan form
79 for A (some texts refer
to J as the Jordan canonical form or the Jordan normal form). The Jordan
structure of A is deﬁned to be the number of Jordan segments in J along
with the number and sizes of the Jordan blocks within each segment. The proof
of uniqueness of the Jordan form for a nilpotent matrix (p. 580) can be extended
to all A ∈Cn×n. In other words, the Jordan structure of a matrix is uniquely
determined by its entries. Below is a formal summary of these developments.
79
Marie Ennemond Camille Jordan (1838–1922) discussed this idea (not over the complex num-
bers but over a ﬁnite ﬁeld) in 1870 in Trait´e des substitutions et des ´equations algebraique
that earned him the Poncelet Prize of the Acad´emie des Science. But Jordan may not have
been the ﬁrst to develop these concepts. It has been reported that the German mathematician
Karl Theodor Wilhelm Weierstrass (1815–1897) had previously formulated results along these
lines. However, Weierstrass did not publish his ideas because he was fanatical about rigor, and
he would not release his work until he was sure it was on a ﬁrm mathematical foundation.
Weierstrass once said that “a mathematician who is not also something of a poet will never be
a perfect mathematician.”

590
Chapter 7
Eigenvalues and Eigenvectors
Jordan Form
For every A ∈Cn×n with distinct eigenvalues σ (A) = {λ1, λ2, . . . , λs} ,
there is a nonsingular matrix P such that
P−1AP = J =




J(λ1)
0
· · ·
0
0
J(λ2) · · ·
0
...
...
...
...
0
0
· · · J(λs)



.
(7.8.4)
•
J has one Jordan segment J(λj) for each eigenvalue λj ∈σ (A) .
•
Each segment J(λj) is made up of tj = dim N (A −λjI) Jordan
blocks J⋆(λj) as described below.
J(λj)=




J1(λj)
0
· · ·
0
0
J2(λj) · · ·
0
...
...
...
...
0
0
· · · Jtj(λj)



with J⋆(λj) =





λj
1
...
...
...
1
λj




.
•
The largest Jordan block in J(λj) is kj × kj, where kj = index (λj).
•
The number of i × i Jordan blocks in J(λj) is given by
νi(λj)= ri−1(λj) −2ri(λj) + ri+1(λj) with ri(λj)=rank

(A −λjI)i
.
•
Matrix J in (7.8.4) is called the Jordan form for A. The structure
of this form is unique in the sense that the number of Jordan seg-
ments in J as well as the number and sizes of the Jordan blocks in
each segment is uniquely determined by the entries in A. Further-
more, every matrix similar to A has the same Jordan structure—i.e.,
A, B ∈Cn×n are similar if and only if A and B have the same
Jordan structure. The matrix P is not unique—see p. 594.
Example 7.8.1
Problem: Find the Jordan form for A =







5
4
0
0
4
3
2
3
1
0
5
1
0
−1
2
0
2
0
−8
−8
−1
2
−12
−7
0
0
0
0
−1
0
−8
−8
−1
0
−9
−5







.

7.8 Jordan Form
591
Solution: Computing the eigenvalues (which is the hardest part) reveals two
distinct eigenvalues λ1 = 2 and λ2 = −1, so there are two Jordan segments in
the Jordan form J =
 J(2)
0
0
J(−1)

. Computing ranks ri(2) = rank

(A −2I)i
and ri(−1) = rank

(A + I)i
until rk(⋆) = rk+1(⋆) yields
r1(2) = rank (A −2I)
= 4,
r1(−1) = rank (A + I)
= 4,
r2(2) = rank

(A −2I)2
= 3,
r2(−1) = rank

(A + I)2
= 4,
r3(2) = rank

(A −2I)3
= 2,
r4(2) = rank

(A −2I)4
= 2,
so k1 = index (λ1) = 3 and k2 = index (λ2) = 1. This tells us that the largest
Jordan block in J(2) is 3 × 3, while the largest Jordan block in J(−1) is 1 × 1
so that J(−1) is a diagonal matrix (the associated eigenvalue is semisimple
whenever this happens). Furthermore,
ν3(2)
= r2(2) −2r3(2) + r4(2) = 1
=⇒
one 3 × 3 block in J(2),
ν2(2)
= r1(2) −2r2(2) + r3(2) = 0
=⇒
no 2 × 2 blocks in J(2),
ν1(2)
= r0(2) −2r1(2) + r2(2) = 1
=⇒
one 1 × 1 block in J(2),
ν1(−1) = r0(−1) −2r1(−1) + r2(−1) = 2
=⇒
two 1 × 1 blocks in J(−1).
Therefore, J(2) =



2
1
0
0
0
2
1
0
0
0
2
0
0
0
0
2


and J(−1) =
 −1
0
0
−1

so that
J =

J(2)
0
0
J(−1)

=










2
1
0
0
0
2
1
0
0
0
2
0
0
0
0
2
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
−1
0
0
−1










.
The above example suggests that determining the Jordan form for An×n
is straightforward, and perhaps even easy. In theory, it is—just ﬁnd σ (A) , and
calculate some ranks. But, in practice, both of these tasks can be diﬃcult. To
begin with, the rank of a matrix is a discontinuous function of its entries, and rank
computed with ﬂoating-point arithmetic can vary with the algorithm used and is
often diﬀerent than rank computed with exact arithmetic (recall Exercise 2.2.4).

592
Chapter 7
Eigenvalues and Eigenvectors
Furthermore, computing higher-index eigenvalues with ﬂoating-point arithmetic
is fraught with peril. To see why, consider the matrix
L(ϵ) =





0
1
...
...
...
1
ϵ
0





n×n
whose characteristic equation is
λn −ϵ = 0.
For ϵ = 0, zero is the only eigenvalue (and it has index n ), but for all ϵ > 0,
there are n distinct eigenvalues given by ϵ1/ne2kπi/n for k = 0, 1, . . . , n−1. For
example, if n = 32, and if ϵ changes from 0 to 10−16, then the eigenvalues
of L(ϵ) change in magnitude from 0 to 10−1/2 ≈.316, which is substantial for
such a small perturbation. Sensitivities of this kind present signiﬁcant problems
for ﬂoating-point algorithms. In addition to showing that high-index eigenvalues
are sensitive to small perturbations, this example also shows that the Jordan
structure is highly discontinuous. L(0) is in Jordan form, and there is just one
Jordan block of size n, but for all ϵ ̸= 0, the Jordan form of L(ϵ) is a diagonal
matrix—i.e., there are n Jordan blocks of size 1 × 1. Lest you think that this
example somehow is an isolated case, recall from Example 7.3.6 (p. 532) that
every matrix in Cn×n is arbitrarily close to a diagonalizable matrix.
All of the above observations make it clear that it’s hard to have faith in
a Jordan form that has been computed with ﬂoating-point arithmetic. Conse-
quently, numerical computation of Jordan forms is generally avoided.
Example 7.8.2
The Jordan form of A conveys complete information about the eigenvalues of
A. For example, if the Jordan form for A is
J =

















4
1
0
4
1
4
4
1
0
4
3
1
0
3
2
2

















,
then we know that
▷
A9×9 has three distinct eigenvalues, namely σ (A) = {4, 3, 2};
▷
alg mult (4) = 5, alg mult (3) = 2, and alg mult (2) = 2;
▷
geo mult (4) = 2, geo mult (3) = 1, and geo mult (2) = 2;

7.8 Jordan Form
593
▷
index (4) = 3, index (3) = 2, and index (2) = 1;
▷
λ = 2 is a semisimple eigenvalue, so, while A is not diagonalizable, part of
it is; i.e., the restriction A/N(A−2I) is a diagonalizable linear operator.
Of course, if both P and J are known, then A can be completely reconstructed
from (7.8.4), but the point being made here is that only J is needed to reveal
the eigenstructure along with the other similarity invariants of A.
Now that the structure of the Jordan form J is known, the structure of the
similarity transformation P such that P−1AP = J is easily revealed. Focus
on a single p × p Jordan block J⋆(λ) contained in the Jordan segment J(λ)
associated with an eigenvalue λ, and let P⋆= [ x1 x2 · · · xp ] be the portion of
P = [ · · · | P⋆| · · ·] that corresponds to the position of J⋆(λ) in J. Notice that
AP = PJ implies AP⋆= P⋆J⋆(λ) or, equivalently,
A[ x1 x2 · · · xp ] = [ x1 x2 · · · xp ]





λ
1
...
...
...
1
λ





p×p
,
so equating columns on both sides of this equation produces
Ax1 = λx1
=⇒
x1 is an eigenvector
=⇒
(A −λI) x1 = 0,
Ax2 = x1 + λx2
=⇒
(A −λI) x2 = x1
=⇒
(A −λI)2 x2 = 0,
Ax3 = x2 + λx3
=⇒
(A −λI) x3 = x2
=⇒
(A −λI)3 x3 = 0,
...
...
...
Axp = xp−1 + λxp
=⇒
(A −λI) xp = xp−1
=⇒
(A −λI)p xp = 0.
In other words, the ﬁrst column x1 in P⋆is a eigenvector for A associated with
λ. We already knew there had to be exactly one independent eigenvector for each
Jordan block because there are t = dim N (A −λI) Jordan blocks J⋆(λ), but
now we know precisely where these eigenvectors are located in P.
Vectors x such that x ∈N

(A−λI)g
but x ̸∈N

(A−λI)g−1
are called
generalized eigenvectors of order g associated with λ. So P⋆consists of an
eigenvector followed by generalized eigenvectors of increasing order. Moreover,
the columns of P⋆form a Jordan chain analogous to (7.7.2) on p. 576; i.e.,
xi = (A −λI)p−i xp implies P⋆must have the form
P⋆=

(A −λI)p−1 xp | (A −λI)p−2 xp | · · · | (A −λI) xp | xp

.
(7.8.5)
A complete set of Jordan chains associated with a given eigenvalue λ is de-
termined in exactly the same way as Jordan chains for nilpotent matrices are

594
Chapter 7
Eigenvalues and Eigenvectors
determined except that the nested subspaces Mi deﬁned in (7.7.1) on p. 575
are redeﬁned to be
Mi = R

(A −λI)i
∩N (A −λI)
for i = 0, 1, . . . , k,
(7.8.6)
where k = index (λ). Just as in the case of nilpotent matrices, it follows that
0 = Mk ⊆Mk−1 ⊆· · · ⊆M0 = N (A −λI) (see Exercise 7.8.8). Since
(A −λI)/N((A−λI)k) is a nilpotent linear operator of index k (Example 5.10.4,
p. 399), it can be argued that the same process used to build Jordan chains for
nilpotent matrices can be used to build Jordan chains for a general eigenvalue
λ. Below is a summary of the process adapted to the general case.
Constructing Jordan Chains
For each λ ∈σ (An×n) , set Mi = R

(A −λI)i
∩N (A −λI) for
i = k −1, k −2, . . . , 0, where k = index (λ).
•
Construct a basis B for N (A −λI).
▷Starting with any basis Sk−1 for Mk−1 (see p. 211), sequentially
extend Sk−1 with sets Sk−2, Sk−3, . . . , S0 such that
Sk−1
is a basis for
Mk−1,
Sk−1 ∪Sk−2
is a basis for
Mk−2,
Sk−1 ∪Sk−2 ∪Sk−3
is a basis for
Mk−3,
etc., until a basis B = Sk−1 ∪Sk−2 ∪· · · ∪S0 = {b1, b2, . . . , bt}
for M0 = N (A −λI) is obtained (see Example 7.7.3 on p. 582).
•
Build a Jordan chain on top of each eigenvector b⋆∈B.
▷For each eigenvector b⋆∈Si, solve (A −λI)i x⋆= b⋆(a neces-
sarily consistent system) for x⋆, and construct a Jordan chain on
top of b⋆by setting
P⋆=

(A −λI)i x⋆
 (A −λI)i−1 x⋆
 · · ·
 (A −λI) x⋆
 x⋆

(i+1)×n.
▷Each such P⋆corresponds to one Jordan block J⋆(λ) in the Jor-
dan segment J(λ) associated with λ.
▷The ﬁrst column in P⋆is an eigenvector, and subsequent columns
are generalized eigenvectors of increasing order.
•
If all such P⋆’s for a given λj ∈σ (A) = {λ1, λ2, . . . , λs} are put in
a matrix Pj, and if P =

P1 | P2 | · · · | Ps

, then P is a nonsingu-
lar matrix such that P−1AP = J = diag (J(λ1), J(λ2), . . . , J(λs))
is in Jordan form as described on p. 590.

7.8 Jordan Form
595
Example 7.8.3
Caution!
Not every basis for N(A −λI) can be used to build Jordan chains
associated with an eigenvalue λ ∈σ (A) . For example, the Jordan form of
A =


3
0
1
−4
1
−2
−4
0
−1


is
J =


1
1
0
0
1
0
0
0
1


because σ (A) = {1} and index (1) = 2. Consequently, if P = [ x1 | x2 | x3 ]
is a nonsingular matrix such that P−1AP = J, then the derivation beginning
on p. 593 leading to (7.8.5) shows that {x1, x2} must be a Jordan chain such
that (A −I)x1 = 0 and (A −I)x2 = x1, while x3 is another eigenvector (not
dependent on x1 ). Suppose we try to build the Jordan chains in P by starting
with the eigenvectors
x1 =


1
0
−2


and
x3 =


0
1
0


(7.8.7)
obtained by solving (A −I)x = 0 with straightforward Gauss–Jordan elimina-
tion. This naive approach fails because x1 ̸∈R (A −I) means (A −I)x2 = x1 is
an inconsistent system, so x2 cannot be determined. Similarly, x3 ̸∈R (A −I)
insures that the same diﬃculty occurs if x3 is used in place of x1. In other
words, even though the vectors in (7.8.7) constitute an otherwise perfectly good
basis for N (A −I), they are not suitable for building Jordan chains. You are
asked in Exercise 7.8.2 to ﬁnd the correct basis for N (A −I) that will yield the
Jordan chains that constitute P.
Example 7.8.4
Problem: What do the results concerning the Jordan form for A ∈Cn×n say
about the decomposition of Cn into invariant subspaces?
Solution: Consider P−1AP = J = diag (J(λ1), J(λ2), . . . , J(λs)) , where the
J(λj) ’s are the Jordan segments and P =

P1 | P2 | · · · | Ps

is a matrix of
Jordan chains as described in (7.8.5) and on p. 594. If A is considered as a
linear operator on Cn, and if the set of columns in Pi is denoted by Ji, then
the results in §4.9 (p. 259) concerning invariant subspaces together with those
in §5.9 (p. 383) about direct sum decompositions guarantee that each R (Pi) is
an invariant subspace for A such that
Cn = R (P1) ⊕R (P2) ⊕· · · ⊕R (Ps)
and
J(λi) =

A/R(Pi)

Ji
.
More can be said. If alg mult (λi) = mi and index (λi) = ki, then Ji is a
linearly independent set containing mi vectors, and the discussion surrounding

596
Chapter 7
Eigenvalues and Eigenvectors
(7.8.5) insures that each column in Ji belongs to N

(A−λiI)ki
. This coupled
with the fact that dim N

(A −λiI)ki
) = mi (Exercise 7.8.7) implies that Ji
is a basis for
R (Pi) = N

(A −λiI)ki
.
Consequently, each N

(A −λiI)ki
is an invariant subspace for A such that
Cn = N

(A −λ1I)k1
⊕N

(A −λ2I)k2
⊕· · · ⊕N

(A −λsI)ks
and
J(λi) =
%
A/N

(A−λiI)ki
&
Ji
.
Of course, an even ﬁner direct sum decomposition of Cn is possible because
each Jordan segment is itself a block-diagonal matrix containing the individual
Jordan blocks—the details are left to the interested reader.
Exercises for section 7.8
7.8.1. Find the Jordan form of the following matrix whose distinct eigenvalues
are σ (A) = {0, −1, 1}. Don’t be frightened by the size of A.
A =







−4
−5
−3
1
−2
0
1
−2
4
7
3
−1
3
0
−1
2
0
−1
0
0
0
0
0
0
−1
1
2
−4
2
0
−3
1
−8
−14
−5
1
−6
0
1
−4
4
7
4
−3
3
−1
−3
4
2
−2
−2
5
−3
0
4
−1
6
7
3
0
2
0
0
3







.
7.8.2. For the matrix
A =
 
3
0
1
−4
1
−2
−4
0
−1
!
that was used in Example 7.8.3, use
the technique described on p. 594 to construct a nonsingular matrix P
such that P−1AP = J is in Jordan form.
7.8.3. Explain why index (λ) ≤alg mult (λ) for each λ ∈σ (An×n) .
7.8.4. Explain why index (λ) = 1 if and only if λ is a semisimple eigenvalue.
7.8.5. Prove that every square matrix is similar to its transpose. Hint: Con-
sider the “reversal matrix” R =



1
1
...
1


obtained by reversing the
order of the rows (or the columns) of the identity matrix I.

7.8 Jordan Form
597
7.8.6. Cayley–Hamilton Revisited. Prove the the Cayley–Hamilton theo-
rem (pp. 509, 532) by means of the Jordan form; i.e., prove that every
A ∈Cn×n satisﬁes its own characteristic equation.
7.8.7. Prove that if λ is an eigenvalue of A ∈Cn×n such that index (λ) = k
and alg multA (λ) = m, then dim N

(A −λI)k
= m. Is it also true
that dim N

(A −λI)m
= m?
7.8.8. Let λj be an eigenvalue of A with index (λj) = kj. Prove that if
Mi(λj) = R

(A −λjI)i
∩N (A −λjI), then
0 = Mkj(λj) ⊆Mkj−1(λj) ⊆· · · ⊆M0(λj) = N (A −λjI).
7.8.9. Explain why (A−λjI)ix = b(λj) must be a consistent system whenever
λj ∈σ (A) and b(λj) ∈Si(λj), where b(λj) and Si(λj) are as deﬁned
on p. 594.
7.8.10. Does the result of Exercise 7.7.5 extend to nonnilpotent matrices? That
is, if λ ∈σ (A) with index (λ) = k > 1, is Mk−1 = R

(A −λI)k−1
?
7.8.11. As deﬁned in Exercise 5.8.15 (p. 380) and mentioned in Exercise 7.6.10
(p. 573), the Kronecker
80 product (sometimes called tensor product,
80
Leopold Kronecker (1823–1891) was born in Liegnitz, Prussia (now Legnica, Poland), to a
wealthy business family that hired private tutors to educate him until he enrolled at Gymna-
sium at Liegnitz where his mathematical talents were recognized by Eduard Kummer (1810–
1893), who became his mentor and lifelong colleague. Kronecker went to Berlin University
in 1841 to earn his doctorate, writing on algebraic number theory, under the supervision of
Dirichlet (p. 563). Rather than pursuing a standard academic career, Kronecker returned to
Liegnitz to marry his cousin and become involved in his uncle’s banking business. But he never
lost his enjoyment of mathematics. After estate and business interests were left to others in
1855, Kronecker joined Kummer in Berlin who had just arrived to occupy the position vacated
by Dirichlet’s move to G¨ottingen. Kronecker didn’t need a salary, so he didn’t teach or hold a
university appointment, but his research activities led to his election to the Berlin Academy in
1860. He declined the oﬀer of the mathematics chair in G¨ottingen in 1868, but he eventually
accepted the chair in Berlin that was vacated upon Kummer’s retirement in 1883. Kronecker
held the unconventional view that mathematics should be reduced to arguments that involve
only integers and a ﬁnite number of steps, and he questioned the validity of nonconstructive
existence proofs, so he didn’t like the use of irrational or transcendental numbers. Kronecker be-
came famous for saying that “God created the integers, all else is the work of man.” Kronecker’s
signiﬁcant inﬂuence led to animosity with people of diﬀering philosophies such as Georg Cantor
(1845–1918), whose publications Kronecker tried to block. Kronecker’s small physical size was
another sensitive issue. After Hermann Schwarz (p. 271), who was Kummer’s son-in-law and
a student of Weierstrass (p. 589), tried to make a joke involving Weierstrass’s large physique
by stating that “he who does not honor the Smaller, is not worthy of the Greater,” Kronecker
had no further dealings with Schwarz.

598
Chapter 7
Eigenvalues and Eigenvectors
or direct product) of Am×n and Bp×q is the mp × nq matrix
A ⊗B =




a11B
a12B
· · ·
a1nB
a21B
a22B
· · ·
a2nB
...
...
...
...
am1B
am2B
· · ·
amnB



.
(a)
Assuming conformability, establish the following properties.
◦
A ⊗(B ⊗C) = (A ⊗B) ⊗C.
◦
A ⊗(B + C) = (A ⊗B) + (A ⊗C).
◦
(A + B) ⊗C = (A ⊗C) + (B ⊗C).
◦
(A1 ⊗B1)(A2 ⊗B2) · · · (Ak ⊗Bk) = (A1 · · · Ak) ⊗(B1 · · · Bk).
◦
(A ⊗B)∗= A∗+ B∗.
◦
rank (A ⊗B) = (rank (A))(rank (B)).
Assume A is m × m and B is n × n for the following.
◦
trace (A ⊗B) = (trace (A))(trace (B)).
◦
(A ⊗In)(Im ⊗B) = A ⊗B = (Im ⊗B)(A ⊗In).
◦
det (A ⊗B) = (det (A))m(det (B))n.
◦
(A ⊗B)−1 = A−1 ⊗B−1.
(b)
Let the eigenvalues of Am×m be denoted by λi and let the eigenvalues
of Bn×n be denoted by µj. Prove the following.
◦The eigenvalues of A ⊗B are the mn numbers {λiµj} m
i=1
n
j=1.
◦The eigenvalues of (A ⊗In) + (Im ⊗B) are {λi + µj} m
i=1
n
j=1.
7.8.12. Use part (b) of Exercise 7.8.11 along with the result of Exercise 7.6.10
(p. 573) to construct an alternate derivation of (7.6.8) on p. 566. That
is, show that the n2 eigenvalues of the discrete Laplacian Ln2×n2 de-
scribed in Example 7.6.2 (p. 563) are given by
λij = 4
%
sin2

iπ
2(n + 1)

+ sin2

jπ
2(n + 1)
&
,
i, j = 1, 2, . . . , n.
Hint: Recall Exercise 7.2.18 (p. 522).
7.8.13. Determine the eigenvalues of the three-dimensional discrete Laplacian
by using the formula from Exercise 7.6.10 (p. 573) that states
Ln3×n3 = (In ⊗In ⊗An) + (In ⊗An ⊗In) + (An ⊗In ⊗In).

7.9 Functions of Nondiagonalizable Matrices
599
7.9
FUNCTIONS OF NONDIAGONALIZABLE MATRICES
The development of functions of nondiagonalizable matrices parallels the devel-
opment for functions of diagonal matrices that was presented in §7.3 except that
the Jordan form is used in place of the diagonal matrix of eigenvalues. Recall
from the discussion surrounding (7.3.5) on p. 526 that if A ∈Cn×n is diago-
nalizable, say A = PDP−1, where D = diag (λ1I, λ2I, . . . , λsI) , and if f(λi)
exists for each λi, then f(A) is deﬁned to be
f(A) = Pf(D)P−1 = P



f(λ1)I
0
· · ·
0
0
f(λ2)I
· · ·
0
...
...
...
...
0
0
· · ·
f(λs)I


P−1.
The Jordan decomposition A = PJP−1 described on p. 590 easily provides a
generalization of this idea to nondiagonalizable matrices. If J is the Jordan form
for A, it’s natural to deﬁne f(A) by writing f(A) = Pf(J)P−1. However,
there are a couple of wrinkles that need to be ironed out before this notion
actually makes sense. First, we have to specify what we mean by f(J)—this is
not as clear as f(D) is for diagonal matrices. And after this is taken care of
we need to make sure that Pf(J)P−1 is a uniquely deﬁned matrix. This also
is not clear because, as mentioned on p. 590, the transforming matrix P is not
unique—it would not be good if for a given A you used one P, and I used
another, and this resulted in your f(A) being diﬀerent than mine.
Let’s ﬁrst make sense of f(J). Assume throughout that A = PJP−1 ∈Cn×n
with σ (A) = {λ1, λ2, . . . , λs} and where J = diag (J(λ1), J(λ2), . . . , J(λs)) is
the Jordan form for A in which each segment J(λj) is a block-diagonal matrix
containing one or more Jordan blocks. That is,
J(λj) =



J1(λj)
0
· · ·
0
0
J2(λj)· · ·
0
...
...
...
...
0
0
· · · Jtj(λj)



with
J⋆(λj) =




λj
1
...
...
...
1
λj



.
We want to deﬁne f(J) to be
f(J) =



f
J(λ1)
...
f
J(λs)



with
f

J(λj)

=



...
f
J⋆(λj)
...


,
but doing so requires that we give meaning to f

J⋆(λj)

. To keep the notation
from getting out of hand, let J⋆=
 λ
1
...
...
λ
!
denote a generic k × k Jordan

600
Chapter 7
Eigenvalues and Eigenvectors
block, and let’s develop a deﬁnition of f(J⋆). Suppose for a moment that f(z)
is a function from C into C that has a Taylor series expansion about λ. That
is, for some r > 0,
f(z) = f(λ)+f ′(λ)(z−λ)+f ′′(λ)
2!
(z−λ)2+f ′′′(λ)
3!
(z−λ)3+ · · ·
for
|z−λ| < r.
The representation (7.3.7) on p. 527 suggests that f(J⋆) should be deﬁned as
f(J⋆) = f(λ)I + f ′(λ)(J⋆−λI) + f ′′(λ)
2!
(J⋆−λI)2 + f ′′′(λ)
3!
(J⋆−λI)3 + · · · .
But since N = J⋆−λI is nilpotent of index k, this series is just the ﬁnite sum
f(J⋆) =
k−1

i=0
f (i)(λ)
i!
Ni,
(7.9.1)
and this means that only f(λ), f ′(λ), . . . , f (k−1)(λ) are required to exist. Also,
N=



0
1
...
...
...
1
0


,
N2=





0
0
1
...
...
...
...
...
1
0
0
0




, . . . , Nk−1=



0
0
· · ·
1
...
...
...
0
0


,
so the representation of f(J⋆) in (7.9.1) can be elegantly expressed as follows.
Functions of Jordan Blocks
For a k × k Jordan block J⋆with eigenvalue λ, and for a function
f(z) such that f(λ), f ′(λ), . . . , f (k−1)(λ) exist, f(J⋆) is deﬁned to be
f(J⋆) = f



λ
1
...
...
...
1
λ


=












f(λ)
f ′(λ)
f ′′(λ)
2!
· · · f (k−1)(λ)
(k −1)!
f(λ)
f ′(λ)
...
...
...
...
f ′′(λ)
2!
f(λ)
f ′(λ)
f(λ)












.
(7.9.2)

7.9 Functions of Nondiagonalizable Matrices
601
Every Jordan form J =
 
... J⋆...
!
is a block-diagonal matrix composed of
various Jordan blocks J⋆, so (7.9.2) allows us to deﬁne
f(J) =
 
...f(J⋆)...
!
as
long as we pay attention to the fact that a suﬃcient number of derivatives of f
are required to exist at the various eigenvalues. More precisely, if the size of the
largest Jordan block associated with an eigenvalue λ is k (i.e., if index (λ) = k),
then f(λ), f ′(λ), . . . , f (k−1)(λ) must exist in order for f(J) to make sense.
Matrix Functions
For A ∈Cn×n with σ (A) = {λ1, λ2, . . . , λs} , let ki = index (λi).
•
A function f : C →C is said to be deﬁned (or to exist) at A when
f(λi), f ′(λi), . . . , f (ki−1)(λi) exist for each λi ∈σ (A) .
•
Suppose that A = PJP−1, where J =
 
... J⋆...
!
is in Jordan form
with the J⋆’s representing the various Jordan blocks described on
p. 590. If f exists at A, then the value of f at A is deﬁned to be
f(A) = Pf(J)P−1 = P


...
f(J⋆)...

P−1,
(7.9.3)
where the f(J⋆) ’s are as deﬁned in (7.9.2).
We still need to explain why (7.9.3) produces a uniquely deﬁned matrix.
The following argument will not only accomplish this purpose, but it will also
establish an alternate expression for f(A) that involves neither the Jordan form
J nor the transforming matrix P. Begin by partitioning J into its s Jordan
segments as described on p. 590, and partition P and P−1 conformably as
P =

P1 | · · · | Ps

,
J =


J(λ1)
...
J(λs)

,
and
P−1 =



Q1...
Qs


.
Deﬁne Gi = PiQi, and observe that if ki = index (λi), then Gi is the pro-
jector onto N

(A −λiI)ki
along R

(A −λiI)ki
. To see this, notice that
Li = J(λi) −λiI is nilpotent of index ki, but J(λj) −λiI is nonsingular when

602
Chapter 7
Eigenvalues and Eigenvectors
i ̸= j, so
(A −λiI) = P(J −λiI)P−1 = P





J(λ1) −λiI
...
Li
...
J(λs) −λiI




P−1
(7.9.4)
is a core-nilpotent decomposition as described on p. 397 (reordering the eigenval-
ues can put the nilpotent block Li on the bottom to realize the form in (5.10.5)).
Consequently, the results in Example 5.10.3 (p. 398) insure that PiQi = Gi is
the projector onto N

(A −λiI)ki
along R

(A −λiI)ki
, and this is true for
all similarity transformations that reduce A to J. If A happens to be diago-
nalizable, then ki = 1 for each i, and the matrices Gi = PiQi are precisely
the spectral projectors deﬁned on p. 517. For this reason, there is no ambigu-
ity in continuing to use the Gi notation, and we will continue to refer to the
Gi ’s as spectral projectors. In the diagonalizable case, Gi projects onto the
eigenspace associated with λi, and in the nondiagonalizable case Gi projects
onto the generalized eigenspace associated with λi.
Now consider
f(A) = Pf(J)P−1 = P



f
J(λ1)
...
f
J(λs)


P−1 =
s

i=1
Pif

J(λi)

Qi.
(7.9.5)
Since f

J(λi)

=


...
f

J⋆(λi)

...

, where the J⋆(λi) ’s are the Jordan blocks
associated with λi, (7.9.2) insures that if ki = index (λi), then
f

J(λi)

= f(λi)I + f ′(λi)Li + f ′′(λi)
2!
L2
i + · · · + f (ki−1)(λi)
(ki −1)! Lki−1
i
,
where Li = J(λi) −λiI, and thus (7.9.5) becomes
f(A) =
s

i=1
Pif

J(λi)

Qi =
s

i=1
ki−1

j=0
f (j)(λi)
j!
PiLj
iQi.
(7.9.6)
The terms PiLj
iQi can be simpliﬁed by noticing that
P−1P = I =⇒QiPj =
	
I
if i = j,
0
if i ̸= j, =⇒P−1Gi =



Q1
...
Qi...
Qs


PiQi =



0...
Qi...0


,

7.9 Functions of Nondiagonalizable Matrices
603
and by using this with (7.9.4) to conclude that
(A −λiI)jGi = P








J(λ1) −λiIj
...
Lj
i
...

J(λs) −λiIj







P−1Gi = PiLj
iQi.
(7.9.7)
Thus (7.9.6) can be written as
f(A) =
s

i=1
ki−1

j=0
f (j)(λi)
j!
(A −λiI)jGi,
(7.9.8)
and this expression is independent of which similarity is used to reduce A to J.
Not only does (7.9.8) prove that f(A) is uniquely deﬁned, but it also provides
a generalization of the spectral theorems for diagonalizable matrices given on
pp. 517 and 526 because if A is diagonalizable, then each ki = 1 so that (7.9.8)
reduces to (7.3.6) on p. 526. Below is a formal summary along with some related
properties.
Spectral Resolution of f(A)
For A ∈Cn×n with σ (A) = {λ1, λ2, . . . , λs} such that ki = index (λi),
and for a function f : C →C such that f(λi), f ′(λi), . . . , f (ki−1)(λi)
exist for each λi ∈σ (A) , the value of f(A) is
f(A) =
s

i=1
ki−1

j=0
f (j)(λi)
j!
(A −λiI)jGi,
(7.9.9)
where the spectral projectors Gi ’s have the following properties.
•
Gi is the projector onto the generalized eigenspace N

(A −λiI)ki
along R

(A −λiI)ki
.
•
G1 + G2 + · · · + Gs = I.
(7.9.10)
•
GiGj = 0 when i ̸= j.
(7.9.11)
•
Ni = (A −λiI)Gi = Gi(A −λiI) is nilpotent of index ki. (7.9.12)
•
If A is diagonalizable, then (7.9.9) reduces to (7.3.6) on p. 526, and
the spectral projectors reduce to those described on p. 517.

604
Chapter 7
Eigenvalues and Eigenvectors
Proof of (7.9.10)–(7.9.12).
Property (7.9.10) results from using (7.9.9) with the
function f(z) = 1, and property (7.9.11) is a consequence of
I = P−1P
=⇒
QiPj =
	
I
if i = j,
0
if i ̸= j.
(7.9.13)
To prove (7.9.12), establish that (A −λiI)Gi = Gi(A −λiI) by noting that
(7.9.13) implies P−1Gi =

0 · · · Qi · · · 0
T and GiP =

0 · · · Pi · · · 0

. Use this
with (7.9.4) to observe that (A −λiI)Gi = PiLiQi = Gi(A −λiI). Now
Nj
i = (PiLiQi)j = PiLj
iQi
for j = 1, 2, 3, . . . ,
and thus Ni is nilpotent of index ki because Li is nilpotent of index ki.
Example 7.9.1
A coordinate-free version of the representation in (7.9.3) results by separating
the ﬁrst-order terms in (7.9.9) from the higher-order terms to write
f(A) =
s

i=1

f(λi)Gi +
ki−1

j=1
f (j)(λi)
j!
Nj
i

.
Using the identity function f(z) = z produces a coordinate-free version of the
Jordan decomposition of A in the form
A =
s

i=1

λiGi + Ni

,
and this is the extension of (7.2.7) on p. 517 to the nondiagonalizable case.
Another version of (7.9.9) results from lumping things into one matrix to write
f(A) =
s

i=1
ki−1

j=0
f (j)(λi)Zij,
where
Zij = (A −λiI)jGi
j!
.
(7.9.14)
The Zij ’s are often called the component matrices or the constituent matrices.
Example 7.9.2
Problem: Describe f(A) for functions f deﬁned at A =

6
2
8
−2
2
−2
0
0
2

.
Solution: A is block triangular, so it’s easy to see that λ1 = 2 and λ2 = 4
are the two distinct eigenvalues with index (λ1) = 1 and index (λ2) = 2. Thus
f(A) exists for all functions such that f(2), f(4), and f ′(4) exist, in which case
f(A) = f(2)G1 + f(4)G2 + f ′(4)(A −4I)G2.
The spectral projectors could be computed directly, but things are easier if some
judicious choices of f are made. For example,
	
f(z) = 1
⇒
I
= f(A) = G1 + G2
f(z) = (z −4)2 ⇒(A −4I)2 = f(A) = 4G1

=⇒
G1 = (A −4I)2/4,
G2 = I −G1.

7.9 Functions of Nondiagonalizable Matrices
605
Now that the spectral projectors are known, any function deﬁned at A can be
evaluated. For example, if f(z) = z1/2, then
f(A) =
√
A =
√
2G1 +
√
4G2 + (1/2
√
4)(A −4I)G2 = 1
2


5
1
7 −2
√
2
−1
3
5 −4
√
2
0
0
2
√
2

.
This technique illustrated above is rather ad hoc, but it always works if a suf-
ﬁcient number of appropriate functions are used. For example, using f(z) = zp
for p = 0, 1, 2, . . . will always produce a system of equations that will yield the
component matrices Zij given in (7.9.14) because
for f(z) = 1:
I =  Zi0,
for f(z) = z :
A =  λiZi0 +  Zi1,
for f(z) = z2 :
A2 =  λ2
i Zi0 +  2λiZi1 +  2Zi2,
...
and this can be considered as a generalized Vandermonde linear system (p. 185)






1
· · ·
1
λ1
· · ·
λs
1
· · ·
1
λ2
1
· · ·
λ2
s
2λ1
· · ·
2λs
2
· · ·
2
...
...
...
...
...
...
· · ·
· · ·
· · ·
· · ·














Z10
...
Zs0
Z11
...
Zs1
Z21
...








=






I
A
A2
A3
...






that can be solved for the Zij ’s. Other sets of polynomials such as
{1, (z −λ1)k1, (z −λ1)k2(z −λ2)k2, . . . (z −λ1)k1 · · · (z −λs)ks}
will generate other linear systems that yield solutions containing the Zij ’s.
Example 7.9.3
Series Representations. Suppose that ∞
j=0 cj(z −z0)j converges to f(z) at
each point inside a circle |z −z0| = r, and suppose that A is a matrix such
that |λi −z0| < r for each eigenvalue λi ∈σ (A) .
Problem: Explain why ∞
j=0 cj(A −z0I)j converges to f(A).
Solution: If P−1AP = J is in Jordan form as described on p. 601, then it’s
not diﬃcult to argue that ∞
j=0 cj(A −z0I)j converges if and only if
P−1
∞

j=0
cj(A−z0I)j
P=
∞

j=0
cjP−1(A−z0I)jP=
∞

j=0
cj(J−z0I)j =




...
∞

j=0
cj(J⋆−z0I)j
...





606
Chapter 7
Eigenvalues and Eigenvectors
converges. Consequently, it suﬃces to prove that ∞
j=0 cj(J⋆−z0I)j converges
to f(J⋆) for a generic k × k Jordan block
J⋆=
 λ
1
...
...
λ
!
= λI + N,
where
N =
 0
1
...
...
0
!
k×k
.
A standard theorem from analysis states that if ∞
j=0 cj(z −z0)j converges to
f(z) when |z −z0| < r, then the series may be diﬀerentiated term by term
to yield series that converge to derivatives of f at points inside the circle of
convergence. Consequently, for each i = 0, 1, 2, . . . ,
f (i)(z)
i!
=
∞

j=0
cj
j
i

(z −z0)j−i
when
|z −z0| < r.
(7.9.15)
We know from (7.9.1) (with f(z) = zj) that
(J⋆−z0I)j = (λ−z0)jI+
j
1

(λ−z0)j−1N+· · ·+

j
k −1

(λ−z0)j−(k−1)Nk−1,
so this together with (7.9.15) produces
∞

j=0
cj(J⋆−z0I)j =


∞

j=0
cj(λ −z0)j

I +


∞

j=0
cj
j
1

(λ −z0)j−1

N
+ · · · +


∞

j=0
cj

j
k −1

(λ −z0)j−(k−1)

Nk−1
= f(λ)I + f ′(λ)N + · · · + f (k−1)
(k −1)!(λ)Nk−1 = f(J∗).
Note: The result of this example validates the statements made on p. 527.
Example 7.9.4
All Matrix Functions Are Polynomials.
It was pointed out on p. 528
that if A is diagonalizable, and if f(A) exists, then there is a polynomial
p(z) such that f(A) = p(A), and you were asked in Exercise
7.3.7 (p. 539)
to use the Cayley–Hamilton theorem (pp. 509, 532) to extend this property to
nondiagonalizable matrices for functions that have an inﬁnite series expansion.
We can now see why this is true in general.
Problem: For a function f deﬁned at A ∈Cn×n, exhibit a polynomial p(z)
such that f(A) = p(A).

7.9 Functions of Nondiagonalizable Matrices
607
Solution: Suppose that σ (A) = {λ1, λ2, . . . , λs} with index (λi) = ki. The
trick is to ﬁnd a polynomial p(z) such that for each i = 1, 2, . . . , s,
p(λi) = f(λi),
p′(λi) = f ′(λi),
. . . ,
p(ki−1)(λi) = f (ki−1)(λi)
(7.9.16)
because if such a polynomial exists, then (7.9.9) guarantees that
p(A) =
s

i=1
ki−1

j=0
p(j)(λi)
j!
(A −λiI)jGi =
s

i=1
ki−1

j=0
f (j)(λi)
j!
(A −λiI)jGi = f(A).
Since there are k = s
i=1 ki equations in (7.9.16) to be satisﬁed, let’s look for
a polynomial of the form
p(z) = α0 + α1z + α2z2 + · · · + αk−1zk−1
by writing the equations in (7.9.16) as the following k × k linear system Hx = f :
p(λ1) = f(λ1)
...
p(λs) = f(λs)
...
p′(λi) = f ′(λi)
...
...
p′′(λi) = f ′′(λi)
...
...
⇒
⇒
⇒
⇒


































1
λ1
λ2
1
λ3
1
· · ·
λk−1
1
...
...
...
...
...
1
λs
λ2
s
λ3
s
· · ·
λk−1
s
...
...
...
...
...
0
1
2λi
3λ2
i
· · ·
(k −1)λk−2
i
...
...
...
...
...
...
...
...
...
...
0
0
2
6λi
· · ·
(k −1)(k −2)λ(k−3)
i
...
...
...
...
...
...
...
...
...
...




































































α0
α1
α2
α3
...
...
...
αk−1


































=


































f(λ1)
...
f(λs)
...
f ′(λi)
...
...
f ′′(λi)
...
...


































.
The coeﬃcient matrix H can be proven to be nonsingular because the rows in
each segment of H are linearly independent. The rows in the top segment of
H are a subset of rows from a Vandermonde matrix (p. 185), while the nonzero
portion of each succeeding segment has the form VD, where the rows of V are
a subset of rows from a Vandermonde matrix and D is a nonsingular diagonal
matrix. Consequently, Hx = f has a unique solution, and thus there is a unique
polynomial p(z) = α0 + α1z + α2z2 + · · · + αk−1zk−1 that satisﬁes the condi-
tions in (7.9.16). This polynomial p(z) is called the Hermite interpolation
polynomial, and it has the property that f(A) = p(A).

608
Chapter 7
Eigenvalues and Eigenvectors
Example 7.9.5
Functional Identities.
Scalar functional identities generally extend to the
matrix case. For example, the scalar identity sin2 z + cos2 z = 1 extends to
matrices as sin2 Z + cos2 Z = I, and this is valid for all Z ∈Cn×n. While
it’s possible to prove such identities on a case-by-case basis by using (7.9.3) or
(7.9.9), there is a more robust approach that is described below.
For two functions f1 and f2 from C into C and for a polynomial p(x, y) in
two variables, let h be the composition deﬁned by h(z) = p

f1(z), f2(z)

. If
An×n has eigenvalues σ (A) = {λ1, λ2, . . . , λs} with index (λi) = ki, and if h
is deﬁned at A, then we are allowed to assert that h(A) = p

f1(A), f2(A)

because Example 7.9.4 insures that there are polynomials g(z) and q(z) such
that h(A) = g(A) and p

f1(A), f2(A)

= q(A), where for each λi ∈σ (A) ,
g(j)(λi) = h(j)(λi) = dj
p

f1(z), f2(z)

dzj

z=λi
= q(j)(λi)
for j = 0, 1, . . . , ki −1,
so g(A) = q(A), and thus h(A) = p

f1(A), f2(A)

. To build functional iden-
tities for A, choose f1 and f2 in h(z) = p

f1(z), f2(z)

that will make
h(λi) = h′(λi) = h′′(λi) = · · · = h(ki−1)(λi) = 0
for each
λi ∈σ (A) ,
thereby insuring that 0 = h(A) = p

f1(A), f2(A)

. This technique produces a
plethora of functional identities. For example, using



f1(z) = sin2 z
f2(z) = cos2 z
p(x, y) = x2 + y2 −1


produces h(z) = p

f1(z), f2(z)

= sin2 z + cos2 z −1.
Since h(z) = 0 for all z ∈C, it follows that h(Z) = 0 for all Z ∈Cn×n, and
thus sin2 Z+cos2 Z = I for all Z ∈Cn×n. It’s evident that this technique can be
extended to include any number of functions f1, f2, . . . , fm with a polynomial
p(x1, x2, . . . , xm) to produce even more complicated relationships.
Example 7.9.6
Systems of Diﬀerential Equations Revisited.
The purpose here is to ex-
tend the discussion in §7.4 to cover the nondiagonalizable case. Write the system
of diﬀerential equations in (7.4.1) on p. 541 in matrix form as
u′(t) = An×nu(t)
with
u(0) = c,
(7.9.17)
but this time don’t assume that An×n is diagonalizable—suppose instead that
σ (A) = {λ1, λ2, . . . , λs} with index (λi) = ki. The development parallels that

7.9 Functions of Nondiagonalizable Matrices
609
for the diagonalizable case, but eAt is now a little more complicated than (7.4.2).
Using f(z) = ezt in (7.9.3) and (7.9.2) yields
eAt = P
 
...eJ⋆t
...
!
P−1 with eJ⋆t =













eλt
teλt
t2eλt
2!
· · · tk−1eλt
(k −1)!
eλt
teλt
...
...
...
...
t2eλt
2!
eλt
teλt
eλt













, (7.9.18)
while setting f(z) = ezt in (7.9.9) produces
eAt =
s

i=1
ki−1

j=0
tjeλit
j!
(A −λiI)jGi.
(7.9.19)
Either of these can be used to show that the three properties (7.4.3)–(7.4.5)
on p. 541 still hold. In particular, d eAt/dt = AeAt = eAtA, so, just as in
the diagonalizable case, u(t) = eAtc is the unique solution of (7.9.17) (the
uniqueness argument given in §7.4 remains valid). In the diagonalizable case,
the solution of (7.9.17) involves only the eigenvalues and eigenvectors of A as
described in (7.4.7) on p. 542, but generalized eigenvectors are needed for the
nondiagonalizable case. Using (7.9.19) yields the solution to (7.9.17) as
u(t) = eAtc =
s

i=1
ki−1

j=0
tjeλit
j!
vj(λi),
where vj(λi) = (A −λiI)jGic. (7.9.20)
Each vki−1(λi) is an eigenvector associated with λi because (A −λiI)kiGi = 0,
and {vki−2(λi), . . . , v1(λi), v0(λi)} is an associated chain of generalized eigen-
vectors. The behavior of the solution (7.9.20) as t →∞is similar but not
identical to that discussed on p. 544 because for λ = x + iy and t > 0,
tjeλt = tjext (cos yt + i sin yt) →









0
if x < 0,
unbounded
if x ≥0 and j > 0,
oscillates indeﬁnitely
if x = j = 0 and y ̸= 0,
1
if x = y = j = 0.
In particular, if Re (λi) < 0 for every λi ∈σ (A) , then u(t) →0 for every
initial vector c, in which case the system is said to be stable.
•
Nonhomogeneous Systems.
It can be veriﬁed by direct manipulation
that the solution of u′(t) = Au(t) + f(t) with u(t0) = c is given by
u(t) = eA(t−t0)c +
4 t
t0
eA(t−τ)f(τ)dτ.

610
Chapter 7
Eigenvalues and Eigenvectors
Example 7.9.7
Nondiagonalizable Mixing Problem.
To make the point that even simple
problems in nature can be nondiagonalizable, consider three V gallon tanks as
shown in Figure 7.9.1 that are initially full of polluted water in which the ith
tank contains ci lbs of a pollutant. In an attempt to ﬂush the pollutant out, all
spigots are opened at once allowing fresh water at the rate of r gal/sec to ﬂow
into the top of tank #3, while r gal/sec ﬂow from its bottom into the top of
tank #2, and so on.
r  gal/sec
r  gal/sec
r  gal/sec
r  gal/sec
3
2
1
Fresh
Figure 7.9.1
Problem: How many pounds of the pollutant are in each tank at any ﬁnite time
t > 0 when instantaneous and continuous mixing occurs?
Solution: If ui(t) denotes the number of pounds of pollutant in tank i at time
t > 0, then the concentration of pollutant in tank i at time t is ui(t)/V lbs/gal,
so the model u′
i(t) = (lbs/sec) coming in−(lbs/sec) going out produces the non-
diagonalizable system:



u′
1(t)
u′
2(t)
u′
3(t)


= r
V



−1
1
0
0
−1
1
0
0
−1






u1(t)
u2(t)
u3(t)


, or u′ =Au with u(0)=c=


c1
c2
c3

.
This setup is almost the same as that in Exercise 3.5.11 (p. 104). Notice that
A is simply a scalar multiple of a single Jordan block
J⋆=
 −1
1
0
0
−1
1
0
0
−1
!
, so
eAt is easily determined by replacing t by rt/V and λ by −1 in the second
equation of (7.9.18) to produce
eAt = e(rt/V )J⋆= e−rt/V



1
rt/V
(rt/V )2 /2
0
1
rt/V
0
0
1


.

7.9 Functions of Nondiagonalizable Matrices
611
Therefore,
u(t) = eAtc = e−rt/V


c1 + c2(rt/V ) + c3 (rt/V )2 /2
c2 + c3(rt/V )
c3

,
and, just as common sense dictates, the pollutant is never completely ﬂushed
from the tanks in ﬁnite time. Only in the limit does each ui →0, and it’s clear
that the rate at which u1 →0 is slower than the rate at which u2 →0, which
in turn is slower than the rate at which u3 →0.
Example 7.9.8
The Cauchy integral formula is an elegant result from complex analysis
stating that if f : C →C is analytic in and on a simple closed contour Γ ⊂C
with positive (counterclockwise) orientation, and if ξ0 is interior to Γ, then
f(ξ0) =
1
2πi
4
Γ
f(ξ)
ξ −ξ0
dξ
and
f (j)(ξ0) = j!
2πi
4
Γ
f(ξ)
(ξ −ξ0)j+1 dξ.
(7.9.21)
These formulas produce analogous representations of matrix functions. Suppose
that A ∈Cn×n with σ (A) = {λ1, λ2, . . . , λs} and index (λi) = ki. For a
complex variable ξ, the resolvent of A ∈Cn×n is deﬁned to be the matrix
R(ξ) = (ξI −A)−1.
If ξ ̸∈σ (A) , then r(z) = (ξ −z)−1 is deﬁned at A with r(A) = R(ξ), so the
spectral resolution theorem (p. 603) can be used to write
R(ξ) =
s

i=1
ki−1

j=0
r(j)(λi)
j!
(A −λiI)jGi =
s

i=1
ki−1

j=0
1
(ξ −λi)j+1 (A −λiI)jGi.
If σ (A) is in the interior of a simple closed contour Γ, and if the contour
integral of a matrix is deﬁned by entrywise integration, then (7.9.21) produces
1
2πi
4
Γ
f(ξ)(ξI −A)−1dξ =
1
2πi
4
Γ
f(ξ)R(ξ)dξ
=
1
2πi
4
Γ
s

i=1
ki−1

j=0
f(ξ)
(ξ −λi)j+1 (A −λiI)jGidξ
=
s

i=1
ki−1

j=0
% 1
2πi
4
Γ
f(ξ)
(ξ −λi)j+1 dξ
&
(A −λiI)jGi
=
s

i=1
ki−1

j=0
f (j)(λi)
j!
(A −λiI)jGi = f(A).

612
Chapter 7
Eigenvalues and Eigenvectors
•
In other words, if Γ is a simple closed contour containing σ (A) in its
interior, then
f(A) =
1
2πi
4
Γ
f(ξ)(ξI −A)−1dξ
(7.9.22)
whenever f is analytic in and on Γ. Since this formula makes sense for
general linear operators, it is often adopted as a deﬁnition for f(A) in more
general settings.
•
Furthermore, if Γi is a simple closed contour enclosing λi but excluding all
other eigenvalues of A, then the ith spectral projector is given by
Gi =
1
2πi
4
Γi
R(ξ)dξ =
1
2πi
4
Γi
(ξI −A)−1dξ
(Exercise 7.9.19).
Exercises for section 7.9
7.9.1. Lake #i in a closed system of three lakes of equal volume V initially
contains ci lbs of a pollutant. If the water in the system is circulated
at rates (gal/sec) as indicated in Figure 7.9.2, ﬁnd the amount of pollu-
tant in each lake at time t > 0 (assume continuous mixing), and then
determine the pollution in each lake in the long run.
4r
2r
3r
r
2r
#3
#2
#1
Figure 7.9.2
7.9.2. Suppose that A ∈Cn×n has eigenvalues λi with index (λi) = ki. Ex-
plain why the ith spectral projector is given by
Gi = fi(A),
where
fi(z) =
5 1
when z = λi,
0
otherwise.
7.9.3. Explain why each spectral projector Gi can be expressed as a polyno-
mial in A.
7.9.4. If σ (An×n) = {λ1, λ2, . . . , λs} with ki = index (λi), explain why
Ak =
s

i=1
ki−1

j=0
k
j

λk−j
i
(A −λiI)jGi.

7.9 Functions of Nondiagonalizable Matrices
613
7.9.5. With the convention that
k
j

= 0 for j > k, explain why


λ
1
...
...
λ


k
m×m
=












λk
k
1

λk−1
k
2

λk−2 · · · 
k
m−1

λk−m+1
λk
k
1

λk−1 ...
...
...
...
k
2

λk−2
λk
k
1

λk−1
λk












.
7.9.6. Determine eA for A =

6
2
8
−2
2
−2
0
0
2

.
7.9.7. For f(z) = 4√z −1, determine f(A) when A =
 −3
−8
−9
5
11
9
−1
−2
1

.
7.9.8.
(a)
Explain why every nonsingular A ∈Cn×n has a square root.
(b)
Give necessary and suﬃcient conditions for the existence of
√
A
when A is singular.
7.9.9. Spectral Mapping Property.
Prove that if (λ, x) is an eigenpair
for A, then (f(λ), x) is an eigenpair for f(A) whenever f(A) exists.
Does it also follow that alg multA (λ) = alg multf(A) (f(λ))?
7.9.10. Let f be deﬁned at A, and let λ ∈σ (A) . Give an example or an
explanation of why the following statements are not necessarily true.
(a)
f(A) is similar to A.
(b)
geo multA (λ) = geo multf(A) (f(λ)) .
(c)
indexA(λ) = indexf(A)(f(λ)).
7.9.11. Explain why Af(A) = f(A)A whenever f(A) exists.
7.9.12. Explain why a function f is deﬁned at A ∈Cn×n if and only if f
is deﬁned at AT , and then prove that f(AT ) =

f(A)
T . Why can’t
(⋆)∗be used in place of (⋆)T ?

614
Chapter 7
Eigenvalues and Eigenvectors
7.9.13. Use the technique of Example 7.9.5 (p. 608) to establish the following
identities.
(a)
eAe−A = I for all A ∈Cn×n.
(b)
eαA =

eAα for all α ∈C and A ∈Cn×n.
(c)
eiA = cos A + i sin A for all A ∈Cn×n.
7.9.14. (a)
Show that if AB = BA, then eA+B = eAeB.
(b)
Give an example to show that eA+B ̸= eAeB in general.
7.9.15. Find the Hermite interpolation polynomial p(z) as described in Exam-
ple 7.9.4 such that p(A) = eA for A =

3
2
1
−3
−2
−1
−3
−2
−1

.
7.9.16. The Cayley–Hamilton theorem (pp. 509, 532) says that every A ∈Cn×n
satisﬁes its own characteristic equation, and this guarantees that An+j
(j = 0, 1, 2, . . .) can be expressed as a polynomial in A of at most
degree n −1. Since f(A) is always a polynomial in A, the Cayley–
Hamilton theorem insures that f(A) can be expressed as a polynomial
in A of at most degree n −1. Such a polynomial can be determined
whenever f (j)(λi), j = 0, 1, . . . , ai −1 exists for each λi ∈σ (A) ,
where ai = alg mult (λi) . The strategy is the same as that in Example
7.9.4 except that ai is used in place of ki . If we can ﬁnd a polynomial
p(z) = α0 + α1z + · · · + αn−1zn−1 such that for each λi ∈σ (A) ,
p(λi) = f(λi),
p′(λi) = f ′(λi),
. . . ,
p(ai−1)(λi) = f (ai−1)(λi),
then p(A) = f(A). Why? These equations are an n × n linear system
with the αi ’s as the unknowns, and, for the same reason outlined in
Example 7.9.4, a solution is always possible.
(a)
What advantages and disadvantages does this approach have
with respect to the approach in Example 7.9.4?
(b)
Use this method to ﬁnd a polynomial p(z) such that p(A) = eA
for A =

3
2
1
−3
−2
−1
−3
−2
−1

. Compare with Exercise 7.9.15.
7.9.17. Show that if f is a function deﬁned at
A =


α
β
γ
0
α
β
0
0
α

= αI + βN + γN2,
where
N =


0
1
0
0
0
1
0
0
0

,
then f(A) = f(α)I + βf ′(α)N +

γf ′(α) + β2f ′′(α)
2!

N2.

7.9 Functions of Nondiagonalizable Matrices
615
7.9.18. Composition of Matrix Functions. If h(z) = f(g(z)), where f
and g are functions such that g(A) and f

g(A)

each exist, then
h(A) = f

g(A)

. However, it’s not legal to prove this simply by saying
“replace z by A.” One way to prove that h(A) = f

g(A)

is to
demonstrate that h(J⋆) = f

g(J⋆)

for a generic Jordan block and then
invoke (7.9.3). Do this for a 3 × 3 Jordan block—the generalization to
k × k blocks is similar. That is, let h(z) = f(g(z)), and use Exercise
7.9.17 to prove that if g(J⋆) and f

g(J⋆)

each exist, then
h(J⋆) = f

g(J⋆)

for
J⋆=


λ
1
0
0
λ
1
0
0
λ

.
7.9.19. Prove that if Γi is a simple closed contour enclosing λi ∈σ (A) but
excluding all other eigenvalues of A, then the ith spectral projector is
Gi =
1
2πi
4
Γi
(ξI −A)−1dξ =
1
2πi
4
Γi
R(ξ)dξ.
7.9.20. For f(z) = z−1, verify that f(A) = A−1 for every nonsingular A.
7.9.21. If Γ is a simple closed contour enclosing all eigenvalues of a nonsingular
matrix A, what is the value of
1
2πi
4
Γ
ξ−1(ξI −A)−1dξ ?
7.9.22. Generalized Inverses.
The inverse function f(z) = z−1 is not de-
ﬁned at singular matrices, but the generalized inverse function
g(z) =
	
z−1
if z ̸= 0,
0
if z = 0,
is deﬁned on all square matrices. It’s clear from Exercise 7.9.20 that
if A is nonsingular, then g(A) = A−1, so g(A) is a natural way to
extend the concept of inversion to include singular matrices. Explain why
g(A) = AD is the Drazin inverse of Example 5.10.5 (p. 399) and not
necessarily the Moore–Penrose pseudoinverse A† described on p. 423.
7.9.23. Drazin Is “Natural.”
Suppose that A is a singular matrix, and let
Γ be a simple closed contour that contains all eigenvalues of A except
λ1 = 0, which is neither in nor on Γ. Prove that
1
2πi
4
Γ
ξ−1(ξI −A)−1dξ = AD
is the Drazin inverse for A as deﬁned in Example 5.10.5 (p. 399). Hint:
The Cauchy–Goursat theorem states that if a function f is analytic at
all points inside and on a simple closed contour Γ, then
6
Γ f(z)dz = 0.

616
Chapter 7
Eigenvalues and Eigenvectors
7.10
DIFFERENCE EQUATIONS, LIMITS, AND SUMMABILITY
A linear diﬀerence equation of order m with constant coeﬃcients has the form
y(k + 1) = αmy(k) + αm−1y(k −1) · · · + α1y(k −m + 1) + α0
(7.10.1)
in which α0, α1, . . . , αm along with initial conditions y(0), y(1), . . . , y(m −1)
are known constants, and y(m), y(m + 1), y(m + 2) . . . are unknown. Diﬀerence
equations are the discrete analogs of diﬀerential equations, and, among other
ways, they arise by discretizing diﬀerential equations. For example, discretizing
a second-order linear diﬀerential equation results in a system of second-order
diﬀerence equations as illustrated in Example 1.4.1, p 19. The theory of linear
diﬀerence equations parallels the theory for linear diﬀerential equations, and
a technique similar to the one used to solve linear diﬀerential equations with
constant coeﬃcients produces the solution of (7.10.1) as
y(k) =
α0
1 −α1 −· · · −αm
+
m

i=1
βiλk
i ,
for k = 0, 1, . . .
(7.10.2)
in which the λi ’s are the roots of λm −αmλm−1 −· · · −α0 = 0, and the βi ’s
are constants determined by the initial conditions y(0), y(1), . . . , y(m −1). The
ﬁrst term on the right-hand side of (7.10.2) is a particular solution of (7.10.1),
and the summation term in (7.10.2) is the general solution of the associated
homogeneous equation deﬁned by setting α0 = 0.
This section focuses on systems of ﬁrst-order linear diﬀerence equations with
constant coeﬃcients, and such systems can be written in matrix form as
x(k + 1) = Ax(k)
(a homogeneous system)
or
x(k + 1) = Ax(k) + b(k)
(a nonhomogeneous system),
(7.10.3)
where matrix An×n, the initial vector x(0), and vectors b(k), k = 0, 1, . . . , are
known. The problem is to determine the unknown vectors x(k), k = 1, 2, . . . ,
along with an expression for the limiting vector limk→∞x(k). Such systems are
used to model linear discrete-time evolutionary processes, and the goal is usually
to predict how (or to where) the process eventually evolves given the initial state
of the process. For example, the population migration problem in Example 7.3.5
(p. 531) produces a 2 × 2 system of homogeneous linear diﬀerence equations
(7.3.14), and the long-run (or steady-state) population distribution is obtained
by ﬁnding the limiting solution. More sophisticated applications are given in
Example 7.10.8 (p. 635) and Example 8.3.7 (p. 683).

7.10 Diﬀerence Equations, Limits, and Summability
617
Solving the equations in (7.10.3) is easy. Direct substitution veriﬁes that
x(k) = Akx(0)
for
k = 1, 2, 3, . . .
and
(7.10.4)
x(k) = Akx(0) +
k−1

j=0
Ak−j−1b(j)
for
k = 1, 2, 3, . . .
are respective solutions to (7.10.3). So rather than ﬁnding x(k) for any ﬁ-
nite k, the real problem is to understand the nature of the limiting solution
limk→∞x(k), and this boils down to analyzing limk→∞Ak. We begin this anal-
ysis by establishing conditions under which Ak →0.
For scalars α we know that αk →0 if and only if |α| < 1, so it’s natural
to ask if there is an analogous statement for matrices. The ﬁrst inclination is to
replace | ⋆| by a matrix norm ∥⋆∥, but this doesn’t work for the standard
norms. For example, if A =
 0
2
0
0

, then Ak →0 but ∥A∥= 2 for all of the
standard matrix norms. Although it’s possible to construct a rather goofy-looking
matrix norm ∥⋆∥g such that ∥A∥g < 1 when limk→∞Ak = 0, the underlying
mechanisms governing convergence to zero are better understood and analyzed
by using eigenvalues and the Jordan form rather than norms. In particular, the
spectral radius of A deﬁned as ρ(A) = maxλ∈σ(A) |λ| (Example 7.1.4, p. 497)
plays a central role.
Convergence to Zero
For A ∈Cn×n,
lim
k→∞Ak = 0
if and only if
ρ(A) < 1.
(7.10.5)
Proof.
If P−1AP = J is the Jordan form for A, then
Ak = PJkP−1 = P



...
Jk
⋆
...


P−1,
where
J⋆=


λ
1
...
...
λ

(7.10.6)
denotes a generic Jordan block in J. Clearly, Ak →0 if and only if Jk
⋆→0
for each Jordan block, so it suﬃces to prove that Jk
⋆→0 if and only if |λ| <
1. Using the function f(z) = zn in formula (7.9.2) on p. 600 along with the
convention that
k
j

= 0 for j > k produces

618
Chapter 7
Eigenvalues and Eigenvectors
Jk
⋆=












λk
k
1

λk−1
k
2

λk−2 · · · 
k
m−1

λk−m+1
λk
k
1

λk−1 ...
...
...
...
k
2

λk−2
λk
k
1

λk−1
λk












m×m
.
(7.10.7)
It’s clear from the diagonal entries that if Jk
⋆→0, then λk →0, so |λ| < 1.
Conversely, if |λ| < 1 then limk→∞
k
j

λk−j = 0 for each ﬁxed value of j
because
k
j

= k(k −1) · · · (k −j + 1)
j!
≤kj
j!
=⇒

k
j

λk−j
 ≤kj
j! |λ|k−j →0.
You can see that the last term on the right-hand side goes to zero as k →∞
either by applying l’Hopital’s rule or by realizing that kj goes to inﬁnity with
polynomial speed while |λ|k−j is going to zero with exponential speed. There-
fore, if |λ| < 1, then Jk
⋆→0, and thus (7.10.5) is proven.
Intimately related to the question of convergence to zero is the convergence
of the Neumann series ∞
k=0 Ak. It was demonstrated in (3.8.5) on p. 126
that if limn→∞An = 0, then the Neumann series converges, and it was argued
in Example 7.3.1 (p. 527) that the converse holds for diagonalizable matrices.
Now we are in a position to prove that the converse is true for all square matrices
and thereby produce the following complete statement regarding the convergence
of the Neumann series.
Neumann Series
For A ∈Cn×n, the following statements are equivalent.
•
The Neumann series I + A + A2 + · · · converges.
(7.10.8)
•
ρ(A) < 1.
(7.10.9)
•
lim
k→∞Ak = 0.
(7.10.10)
In which case, (I −A)−1 exists and ∞
k=0 Ak = (I −A)−1. (7.10.11)
Proof.
We know from (7.10.5) that (7.10.9) and (7.10.10) are equivalent, and it
was argued on p. 126 that (7.10.10) implies (7.10.8), so the theorem can be estab-
lished by proving that (7.10.8) implies (7.10.9). If ∞
k=0 Ak converges, it follows
that ∞
k=0 Jk
∗must converge for each Jordan block J∗in the Jordan form for A.
This together with (7.10.7) implies that
∞
k=0 Jk
∗

ii = ∞
k=0 λk converges for

7.10 Diﬀerence Equations, Limits, and Summability
619
each λ ∈σ (A) , and this scalar geometric series converges if and only if |λ| < 1.
Thus the convergence of ∞
k=0 Ak implies ρ(A) < 1. When it converges,
∞
k=0 Ak = (I −A)−1 because (I −A)(I + A + A2 + · · · + Ak−1) = I −Ak →
I as k →∞.
The following examples illustrate the utility of the previous results for es-
tablishing some useful (and elegant) statements concerning spectral radius.
Example 7.10.1
Spectral Radius as a Limit. It was shown in Example 7.1.4 (p. 497) that
if A ∈Cn×n, then ρ (A) ≤∥A∥for every matrix norm. But this was just the
precursor to the following elegant relationship between spectral radius and norm.
Problem: Prove that for every matrix norm,
ρ(A) = lim
k→∞
))Ak))1/k .
(7.10.12)
Solution: First note that ρ (A)k = ρ

Ak
≤
))Ak))
=⇒
ρ (A) ≤
))Ak))1/k .
Next, observe that ρ

A/(ρ (A) + ϵ)

< 1 for every ϵ > 0, so, by (7.10.5),
lim
k→∞

A
ρ (A) + ϵ
k
= 0
=⇒
lim
k→∞
))Ak))
(ρ (A) + ϵ)k = 0.
Consequently, there is a positive integer Kϵ such that
))Ak)) /(ρ (A) + ϵ)k < 1
for all k ≥Kϵ, so
))Ak))1/k < ρ (A) + ϵ for all k ≥Kϵ, and thus
ρ (A) ≤
))Ak))1/k < ρ (A) + ϵ
for k ≥Kϵ.
Because this holds for each ϵ > 0, it follows that limk→∞
))Ak))1/k = ρ(A).
Example 7.10.2
For A ∈Cn×n let |A| denote the matrix having entries |aij|, and for matrices
B, C ∈ℜn×n deﬁne B ≤C to mean bij ≤cij for each i and j.
Problem: Prove that if |A| ≤B, then
ρ (A) ≤ρ (|A|) ≤ρ (B) .
(7.10.13)
Solution: The triangle inequality yields |Ak| ≤|A|k for every positive integer
k. Furthermore, |A| ≤B implies that |A|k ≤Bk. This with (7.10.12) produces
))Ak))
∞=
)) |Ak|
))
∞≤
)) |A|k ))
∞≤
))Bk))
∞
=⇒
))Ak))1/k
∞≤
)) |A|k ))1/k
∞≤
))Bk))1/k
∞
=⇒
lim
k→∞
))Ak))1/k
∞≤lim
k→∞≤
)) |A|k ))1/k
∞≤lim
k→∞≤
))Bk))1/k
∞
=⇒
ρ (A) ≤ρ (|A|) ≤ρ (B) .

620
Chapter 7
Eigenvalues and Eigenvectors
Example 7.10.3
Problem: Prove that if 0 ≤Bn×n, then
ρ (B) < r if and only if (rI −B)−1 exists and (rI −B)−1 ≥0.
(7.10.14)
Solution: If ρ (B) < r, then ρ(B/r) < 1, so (7.10.8)–(7.10.11) imply that
rI −B = r

I −B
r

is nonsingular and (rI −B)−1 = 1
r
∞

k=0
B
r
k
≥0.
To prove the converse, it’s convenient to adopt the following notation. For any
P ∈ℜm×n, let |P| =

|pij|

denote the matrix of absolute values, and notice
that the triangle inequality insures that |PQ| ≤|P| |Q| for all conformable P
and Q. Now assume that rI−B is nonsingular and (rI−B)−1 ≥0, and prove
ρ (B) < r. Let (λ, x) be any eigenpair for B, and use B ≥0 together with
(rI −B)−1 ≥0 to write
λx = Bx
=⇒
|λ| |x| = |λx| = |Bx| ≤|B| |x| = B |x|
=⇒
(rI −B)|x| ≤(r −|λ|) |x|
=⇒
0 ≤|x| ≤(r −|λ|) (rI −B)−1|x|
(7.10.15)
=⇒
r −|λ| ≥0.
But |λ| ̸= r; otherwise (7.10.15) would imply that |x| (and hence x) is zero,
which is impossible. Thus |λ| < r for all λ ∈σ (B) , which means ρ (B) < r.
Iterative algorithms are often used in lieu of direct methods to solve large
sparse systems of linear equations, and some of the traditional iterative schemes
fall into the following class of nonhomogeneous linear diﬀerence equations.
Linear Stationary Iterations
Let Ax = b be a linear system that is square but otherwise arbitrary.
•
A splitting of A is a factorization A = M−N, where M−1 exists.
•
Let H = M−1N (called the iteration matrix), and set d = M−1b.
•
For an initial vector x(0)n×1, a linear stationary iteration is
x(k) = Hx(k −1) + d,
k = 1, 2, 3, . . . .
(7.10.16)
•
If ρ(H) < 1, then A is nonsingular and
lim
k→∞x(k) = x = A−1b
for every initial vector x(0).
(7.10.17)

7.10 Diﬀerence Equations, Limits, and Summability
621
Proof.
To prove (7.10.17), notice that if A = M−N = M(I−H) is a splitting
for which ρ(H) < 1, then (7.10.11) guarantees that (I −H)−1 exists, and thus
A is nonsingular. Successive substitution applied to (7.10.16) yields
x(k) = Hkx(0) + (I + H + H2 + · · · + Hk−1)d,
so if ρ(H) < 1, then (7.10.9)–(7.10.11) insures that for all x(0),
lim
k→∞x(k) = (I −H)−1d = (I −H)−1M−1b = A−1b = x.
(7.10.18)
It’s clear that the convergence rate of (7.10.16) is governed by the size
of ρ(H) along with the index of its associated eigenvalue (go back and look
at (7.10.7)). But what really is needed is an indication of how many digits of
accuracy can be expected to be gained per iteration. So as not to obscure the
simple underlying idea, assume that Hn×n is diagonalizable with
σ (H) = {λ1, λ2, . . . , λs} ,
where
1 > |λ1| > |λ2| ≥|λ3| ≥· · · ≥|λs|
(which is frequently the case in applications), and let ϵ(k) = x(k) −x denote
the error after the kth iteration. Subtracting x = Hx + d (a consequence of
(7.10.18)) from x(k) = Hx(k −1) + d produces (for large k)
ϵ(k) = Hϵ(k −1) = Hkϵ(0) = (λk
1G1 + λk
2G2 + · · · + λk
sGs)ϵ(0) ≈λk
1G1ϵ(0),
where the Gi ’s are the spectral projectors occurring in the spectral decomposi-
tion (pp. 517 and 520) of Hk. Similarly, ϵ(k −1) ≈λk−1
1
G1ϵ(0), so comparing
the ith components of ϵ(k −1) and ϵ(k) reveals that after several iterations,

ϵi(k −1)
ϵi(k)
 ≈
1
|λ1| =
1
ρ (H)
for each
i = 1, 2, . . . , n.
To understand the signiﬁcance of this, suppose for example that
|ϵi(k −1)| = 10−q
and
|ϵi(k)| = 10−p
with
p ≥q > 0,
so that the error in each entry is reduced by p −q digits per iteration. Since
p −q = log10

ϵi(k −1)
ϵi(k)
 ≈−log10 ρ (H) ,
we see that −log10 ρ (H) provides us with an indication of the number of digits
of accuracy that can be expected to be eventually gained on each iteration. For
this reason, the number R = −log10 ρ (H) (or, alternately, R = −ln ρ (H)) is
called the asymptotic rate of convergence, and this is the primary tool for
comparing diﬀerent linear stationary iterative algorithms.
The trick is to ﬁnd splittings that guarantee rapid convergence while insuring
that H = M−1N and d = M−1b can be computed easily. The following three
examples present the classical splittings.

622
Chapter 7
Eigenvalues and Eigenvectors
Example 7.10.4
Jacobi’s method
81 is produced by splitting A = D −N, where D is the
diagonal part of A (we assume each aii ̸= 0 ), and −N is the matrix containing
the oﬀ-diagonal entries of A. Clearly, both H = D−1N and d = D−1b can be
formed with little eﬀort. Notice that the ith component in the Jacobi iteration
x(k) = D−1Nx(k −1) + D−1b is given by
xi(k) =

bi −
j̸=i aijxj(k −1)

/aii.
(7.10.19)
This shows that the order in which the equations are considered is irrelevant
and that the algorithm can process equations independently (or in parallel).
For this reason, Jacobi’s method was referred to in the 1940s as the method of
simultaneous displacements.
Problem: Explain why Jacobi’s method is guaranteed to converge for all initial
vectors x(0) and for all right-hand sides b when A is diagonally dominant as
deﬁned and discussed in Examples 4.3.3 (p. 184) and 7.1.6 (p. 499).
Solution: According to (7.10.17), it suﬃces to show that ρ(H) < 1. This follows
by combining |aii| > 
j̸=i |aij| for each i with the fact that ρ(H) ≤∥H∥∞
(Example 7.1.4, p. 497) to write
ρ(H) ≤∥H∥∞= max
i

j
|aij|
|aii| = max
i

j̸=i
|aij|
|aii| < 1.
Example 7.10.5
The Gauss–Seidel method
82 is the result of splitting A = (D−L)−U, where
D is the diagonal part of A (aii ̸= 0 is assumed) and where −L and −U
contain the entries occurring below and above the diagonal of A, respectively.
The iteration matrix is H = (D−L)−1U, and d = (D−L)−1b. The ith entry
in the Gauss–Seidel iteration x(k) = (D −L)−1Ux(k −1) + (D −L)−1b is
xi(k) =

bi −
j<i aijxj(k) −
j>i aijxj(k −1)

/aii.
(7.10.20)
This shows that Gauss–Seidel determines xi(k) by using the newest possible
information—namely, x1(k), x2(k), . . . , xi−1(k) in the current iterate in con-
junction with xi+1(k −1), xi+2(k −1), . . . , xn(k −1) from the previous iterate.
81
Karl Jacobi (p. 353) considered this method in 1845, but it seems to have been independently
discovered by others. In addition to being called the method of simultaneous displacements in
1945, Jacobi’s method was referred to as the Richardson iterative method in 1958.
82
Ludwig Philipp von Seidel (1821–1896) studied with Dirichlet in Berlin in 1840 and with
Jacobi (and others) in K¨onigsberg. Seidel’s involvement in transforming Jacobi’s method into
the Gauss–Seidel scheme is natural, but the reason for attaching Gauss’s name is unclear.
Seidel went on to earn his doctorate (1846) in Munich, where he stayed as a professor for the
rest of his life. In addition to mathematics, Seidel made notable contributions in the areas of
optics and astronomy, and in 1970 a lunar crater was named for Seidel.

7.10 Diﬀerence Equations, Limits, and Summability
623
This diﬀers from Jacobi’s method because Jacobi relies strictly on the old data
in x(k −1). The Gauss–Seidel algorithm was known in the 1940s as the method
of successive displacements (as opposed to the method of simultaneous displace-
ments, which is Jacobi’s method). Because Gauss–Seidel computes xi(k) with
newer data than that used by Jacobi, it appears at ﬁrst glance that Gauss–Seidel
should be the superior algorithm. While this is often the case, it is not universally
true—see Exercise 7.10.7.
Other Comparisons. Another major diﬀerence between Gauss–Seidel and Ja-
cobi is that the order in which the equations are processed is irrelevant for Ja-
cobi’s method, but the value (not just the position) of the components xi(k) in
the Gauss–Seidel iterate can change when the order of the equations is changed.
Since this ordering feature can aﬀect the performance of the algorithm, it was the
object of much study at one time. Furthermore, when core memory is a concern,
Gauss–Seidel enjoys an advantage because as soon as a new component xi(k) is
computed, it can immediately replace the old value xi(k −1), whereas Jacobi
requires all old values in x(k −1) to be retained until all new values in x(k)
have been determined. Something that both algorithms have in common is that
diagonal dominance in A guarantees global convergence of each method.
Problem: Explain why diagonal dominance in A is suﬃcient to guarantee
convergence of the Gauss–Seidel method for all initial vectors x(0) and for all
right-hand sides b .
Solution: Show ρ (H) < 1. Let (λ, z) be any eigenpair for H, and suppose
that the component of maximal magnitude in z occurs in position m. Write
(D −L)−1Uz = λz as λ(D −L)z = Uz, and write the mth row of this latter
equation as λ(d −l) = u, where
d = ammzm,
l = −

j<m
amjzj,
and
u = −

j>m
amjzj.
Diagonal dominance |amm| > 
j̸=m |amj| and |zj| ≤|zm| for all j yields
|u| + |l| =


j<m
amjzj
 +


j>m
amjzj
 ≤|zm|
 
j<m
|amj| +

j>m
|amj|

< |zm||amm| = |d|
=⇒
|u| < |d| −|l|.
This together with λ(d −l) = u and the backward triangle inequality (Example
5.1.1, p. 273) produces the conclusion that
|λ| =
|u|
|d −l| ≤
|u|
|d| −|l| < 1,
and thus
ρ(H) < 1.
Note: Diagonal dominance in A guarantees convergence for both Jacobi and
Gauss–Seidel, but diagonal dominance is a rather severe condition that is often

624
Chapter 7
Eigenvalues and Eigenvectors
not present in applications. For example the linear system in Example 7.6.2
(p. 563) that results from discretizing Laplace’s equation on a square is not
diagonally dominant (e.g., look at the ﬁfth row in the 9 × 9 system on p. 564).
But such systems are always positive deﬁnite (Example 7.6.2), and there is a
classical theorem stating that if A is positive deﬁnite, then the Gauss–Seidel
iteration converges to the solution of Ax = b for every initial vector x(0). The
same cannot be said for Jacobi’s method, but there are matrices (the M-matrices
of Example 7.10.7, p. 626) having properties resembling positive deﬁniteness for
which Jacobi’s method is guaranteed to converge—see (7.10.29).
Example 7.10.6
The successive overrelaxation (SOR) method improves on Gauss–Seidel
by introducing a real number ω ̸= 0, called a relaxation parameter, to form
the splitting A = M −N, where M = ω−1D −L and N = (ω−1 −1)D + U.
As before, D is the diagonal part of A ( aii ̸= 0 is assumed) and −L and −U
contain the entries occurring below and above the diagonal of A, respectively.
Since M−1 = ω(D −ωL)−1 = ω(I −ωD−1L)−1, the SOR iteration matrix is
Hω =M−1N=(D−ωL)−1
(1−ω)D+ωU

=(I−ωD−1L)−1
(1−ω)I+ωD−1U

,
and the kth SOR iterate emanating from (7.10.16) is
x(k) = Hωx(k −1) + ω(I −ωD−1L)−1D−1b.
(7.10.21)
This is the Gauss–Seidel iteration when ω = 1. Using ω > 1 is called overrelax-
ation, while taking ω < 1 is referred to as underrelaxation. Writing (7.10.21) in
the form (I −ωD−1L)x(k) =

(1 −ω)I + ωD−1U

x(k −1) + ωD−1b and con-
sidering the ith component on both sides of this equality produces
xi(k) = (1 −ω)xi(k −1) + ω
aii

bi −

j<i
aijxj(k) −

j>i
aijxj(k −1)

. (7.10.22)
The matrix splitting approach is elegant and unifying, but it obscures the simple
idea behind SOR. To understand the original motivation, write the Gauss–Seidel
iterate in (7.10.20) as 7xi(k) = 7xi(k −1) + ck, where ck is the “correction term”
ck = 1
aii

bi −

j<i
aij7xj(k) −
n

j=i
aij7xj(k −1)

.
This clearly suggests that the performance of the iteration can be aﬀected by
adjusting (or “relaxing”) the correction term—i.e., by replacing ck with ωck.
The resulting algorithm, 7xi(k) = 7xi(k −1) + ωck, is in fact (7.10.22), which
produces (7.10.21). Moreover, it was observed early on that Gauss–Seidel applied
to ﬁnite diﬀerence approximations for elliptic partial diﬀerential equations, such

7.10 Diﬀerence Equations, Limits, and Summability
625
as the one in Example 7.6.2 (p. 563), often produces successive corrections ck
that have the same sign, so it was reasoned that convergence might be accelerated
for these applications by increasing the magnitude of the correction factor at each
step (i.e., by setting ω > 1). Thus the technique became known as “successive
overrelaxation” rather than simply “successive relaxation.” It’s not hard to see
that ρ (Hω) < 1 only if 0 < ω < 2 (Exercise 7.10.9), and it can be proven
that positive deﬁniteness of A is suﬃcient to guarantee ρ (Hω) < 1 whenever
0 < ω < 2. But determining ω to minimize ρ (Hω) is generally a diﬃcult task.
Nevertheless, there is one famous special case
83 for which the optimal value
of ω can be explicitly given. If det (αD −L −U) = det

αD −βL −β−1U

for
all real α and β ̸= 0, and if the iteration matrix HJ for Jacobi’s method has
real eigenvalues with ρ (HJ) < 1, then the eigenvalues λJ for HJ are related
to the eigenvalues λω of Hω by
(λω + ω −1)2 = ω2λ2
Jλω.
(7.10.23)
From this it can be proven that the optimum value of ω for SOR is
ωopt =
2
1 +

1 −ρ2(HJ)
and
ρ

Hωopt

= ωopt −1.
(7.10.24)
Furthermore, setting ω = 1 in (7.10.23) yields ρ (HGS) = ρ2(HJ), where HGS
is the Gauss–Seidel iteration matrix. For example, the discrete Laplacian Ln2×n2
in Example 7.6.2 (p. 563) satisﬁes the special case conditions, and the spectral
radii of the iteration matrices associated with L are
Jacobi:
ρ (HJ)
= cos πh
≈1 −(π2h2/2)
(see Exercise 7.10.10),
Gauss–Seidel:
ρ (HGS)
= cos2 πh
≈1 −π2h2,
SOR:
ρ

Hωopt

= 1 −sin πh
1 + sin πh ≈1 −2πh,
where we have set h = 1/(n + 1). Examining asymptotic rates of convergence
reveals that Gauss–Seidel is twice as fast as Jacobi on the discrete Laplacian
because RGS = −log10 cos2 πh = −2 log10 cos πh = 2RJ. However, optimal
SOR is much better because 1 −2πh is signiﬁcantly smaller than 1 −π2h2 for
even moderately small h. The point is driven home by looking at the asymptotic
rates of convergence for h = .02 (n = 49) as shown below:
Jacobi:
RJ ≈.000858,
Gauss–Seidel:
RGS = 2RJ ≈.001716,
SOR:
Ropt ≈.054611 ≈32RGS = 64RJ.
83
This special case was developed by the contemporary numerical analyst David M. Young, Jr.,
who produced much of the SOR theory in his 1950 Ph.D. dissertation that was directed by
Garrett Birkhoﬀat Harvard University. The development of SOR is considered to be one of the
major computational achievements of the ﬁrst half of the twentieth century, and it motivated
at least two decades of intense eﬀort in matrix computations.

626
Chapter 7
Eigenvalues and Eigenvectors
In other words, after things settle down, a single SOR step on L (for h = .02)
is equivalent to about 32 Gauss–Seidel steps and 64 Jacobi steps!
Note: In spite of the preceding remarks, SOR has limitations. Special cases
for which the optimum ω can be explicitly determined are rare, so adaptive
computational procedures are generally necessary to approximate a good ω, and
the results are often not satisfying. While SOR was a big step forward over the
algorithms of the nineteenth century, the second half of the twentieth century saw
the development of more robust methods—such as the preconditioned conjugate
gradient method (p. 657) and GMRES (p. 655)—that have relegated SOR to a
secondary role.
Example 7.10.7
M-matrices
84 are real nonsingular matrices An×n such that aij ≤0 for all
i ̸= j and A−1 ≥0 (each entry of A−1 is nonnegative). They arise naturally in
a broad variety of applications ranging from economics (Example 8.3.6, p. 681)
to hard-core engineering problems, and, as shown in (7.10.29), they are partic-
ularly relevant in formulating and analyzing iterative methods. Some important
properties of M-matrices are developed below.
•
A is an M-matrix if and only if there exists a matrix B ≥0 and a real
number r > ρ(B) such that A = rI −B.
(7.10.25)
•
If A is an M-matrix, then Re (λ) > 0 for all λ ∈σ (A) . Conversely, all
matrices with nonpositive oﬀ-diagonal entries whose spectrums are in the
right-hand halfplane are M-matrices.
(7.10.26)
•
Principal submatrices of M-matrices are also M-matrices.
(7.10.27)
•
If A is an M-matrix, then all principal minors in A are positive. Conversely,
all matrices with nonpositive oﬀ-diagonal entries whose principal minors are
positive are M-matrices.
(7.10.28)
•
If A = M −N is a splitting of an M-matrix for which M−1 ≥0, then the
linear stationary iteration (7.10.16) is convergent for all initial vectors x(0)
and for all right-hand sides b. In particular, Jacobi’s method in Example
7.10.4 (p. 622) converges for all M-matrices.
(7.10.29)
Proof of (7.10.25).
Suppose that A is an M-matrix, and let r = maxi |aii| so
that B = rI −A ≥0. Since A−1 = (rI −B)−1 ≥0, it follows from (7.10.14)
in Example 7.10.3 (p. 620) that r > ρ(B). Conversely, if A is any matrix of
84
This terminology was introduced in 1937 by the twentieth-century mathematician Alexan-
der Markowic Ostrowski, who made several contributions to the analysis of classical iterative
methods. The “M” is short for “Minkowski” (p. 278).

7.10 Diﬀerence Equations, Limits, and Summability
627
the form A = rI −B, where B ≥0 and r > ρ (B) , then (7.10.14) guarantees
that A−1 exists and A−1 ≥0, and it’s clear that aij ≤0 for each i ̸= j, so
A must be an M-matrix.
Proof of (7.10.26).
If A is an M-matrix, then, by (7.10.25), A = rI −B,
where r > ρ (B) . This means that if λA ∈σ (A) , then λA = r −λB for some
λB ∈σ (B) . If λB = α + iβ, then r > ρ (B) ≥|λB| =

α2 + β2 ≥|α| ≥α
implies that Re (λA) = r−α ≥0. Now suppose that A is any matrix such that
aij ≤0 for all i ̸= j and Re (λA) > 0 for all λA ∈σ (A) . This means that
there is a real number γ such that the circle centered at γ and having radius
equal to γ contains σ (A)—see Figure 7.10.1. Let r be any real number such
that r > max{2γ, maxi |aii|}, and set B = rI −A. It’s apparent that B ≥0,
and, as can be seen from Figure 7.10.1, the distance |r −λA| between r and
every point in σ (A) is less than r.
σ(A)
r
γ
x
iy
Figure 7.10.1
All eigenvalues of B look like λB = r −λA, and |λB| = |r −λA| < r, so
ρ (B) < r. Since A = rI −B is nonsingular (because 0 /∈σ (A) ) with B ≥0
and r > ρ (B) , it follows from (7.10.14) in Example 7.10.3 (p. 620) that
A−1 ≥0, and thus A is an M-matrix.
Proof of (7.10.27).
If Ak×k is the principal submatrix lying on the intersection
of rows and columns i1, . . . , ik in an M-matrix A = rI −B, where B ≥0 and
r > ρ (B) , then A = rI −B, where B ≥0 is the corresponding principal
submatrix of B. Let P be a permutation matrix such that
PT BP =
 B
X
Y
Z

, or B = P
 B
X
Y
Z

PT , and let C = P
 B
0
0
0

PT .
Clearly, 0 ≤C ≤B, so, by (7.10.13) on p. 619, ρ(B) = ρ (C) ≤ρ (B) < r.
Consequently, (7.10.25) insures that A is an M-matrix.
Proof of (7.10.28).
If A is an M-matrix, then det (A) > 0 because the eigenval-
ues of a real matrix appear in complex conjugate pairs, so (7.10.26) and (7.1.8),

628
Chapter 7
Eigenvalues and Eigenvectors
p. 494, guarantee that det (A) = "n
i=1 λi > 0. It follows that each principal
minor is positive because each submatrix of an M-matrix is again an M-matrix.
Now prove that if An×n is a matrix such that aij ≤0 for i ̸= j and each prin-
cipal minor is positive, then A must be an M-matrix. Proceed by induction on
n. For n = 1, the assumption of positive principal minors implies that A = [ρ]
with ρ > 0, so A−1 = 1/ρ > 0. Suppose the result is true for n = k, and
consider the LU factorization
A(k+1)×(k+1) =
 7Ak×k
c
dT
α
!
=

I
0
dT 7A−1
1
  7A
c
0
α −dT 7A−1c
!
= LU.
We know that A is nonsingular (det (A) is a principal minor) and α > 0 (it’s
a 1 × 1 principal minor), and the induction hypothesis insures that 7A−1 ≥0.
Combining these facts with c ≤0 and dT ≤0 produces
A−1 = U−1L−1 =





7A−1
−7A−1c
α −dT 7A−1c
0
1
α −dT 7A−1c





 
I
0
−dT 7A−1
1
!
≥0,
and thus the induction argument is completed.
Proof of (7.10.29).
If A = M−N is an M-matrix, and if M−1 ≥0 and N ≥0,
then the iteration matrix H = M−1N is clearly nonnegative. Furthermore,
(I −H)−1 −I = (I −H)−1H = A−1N ≥0
=⇒
(I −H)−1 ≥I ≥0,
so (7.10.14) in Example 7.10.3 (p. 620) insures that ρ (H) < 1. Convergence of
Jacobi’s method is a special case because the Jacobi splitting is A = D −N,
where D = diag (a11, a22, . . . , ann) , and (7.10.28) implies that each aii > 0.
Note: Comparing properties of M-matrices with those of positive deﬁnite ma-
trices reveals many parallels, and, in a rough sense, an M-matrix often plays the
role of “a poor man’s positive deﬁnite matrix.” Only a small sample of M-matrix
theory has been presented here, but there is in fact enough to ﬁll a monograph
on the subject. For example, there are at least 50 known equivalent conditions
that can be imposed on a real matrix with nonpositive oﬀ-diagonal entries (often
called a Z-matrix) to guarantee that it is an M-matrix—see Exercise 7.10.12 for
a sample of such conditions in addition to those listed above.

7.10 Diﬀerence Equations, Limits, and Summability
629
We now focus on broader issues concerning when limk→∞Ak exists but may
be nonzero. Start from the fact that limk→∞Ak exists if and only if limk→∞Jk
⋆
exists for each Jordan block in (7.10.6). It’s clear from (7.10.7) that limk→∞Jk
⋆
cannot exist when |λ| > 1, and we already know the story for |λ| < 1, so we
only have to examine the case when |λ| = 1. If |λ| = 1 with λ ̸= 1 (i.e., λ = eiθ
with 0 < θ < 2π ), then the diagonal terms λk oscillate indeﬁnitely, and this
prevents Jk
⋆(and Ak ) from having a limit. When λ = 1,
Jk
⋆=





1
k
1

· · ·

k
m−1

...
...
...
...
k
1

1





m×m
(7.10.30)
has a limiting value if and only if m = 1, which is equivalent to saying that
λ = 1 is a semisimple eigenvalue. But λ = 1 may be repeated p times so that
there are p Jordan blocks of the form J⋆= [1]1×1. Consequently, limk→∞Ak
exists if and only if the Jordan form for A has the structure
J = P−1AP =

Ip×p
0
0
K

, where p = alg mult (1) and ρ(K) < 1.
(7.10.31)
Now that we know when limk→∞Ak exists, let’s describe what limk→∞Ak
looks like. We already know the answer when p = 0—it’s 0 (because ρ (A) < 1).
But when p is nonzero, limk→∞Ak ̸= 0, and it can be evaluated in a couple of
diﬀerent ways. One way is to partition P =

P1 | P2

and P−1 =
 Q1
Q2

, and
use (7.10.5) and (7.10.31) to write
lim
k→∞Ak
n×n = lim
k→∞P

Ip×p
0
0
Kk

P−1 = P

Ip×p
0
0
0

P−1
=

P1 | P2
 
Ip×p
0
0
0
 
Q1
Q2

= P1Q1 = G.
(7.10.32)
Another way is to use f(z) = zk in the spectral resolution theorem on p. 603. If
σ (A) = {λ1, λ2, . . . , λs} with 1 = λ1 > |λ2| ≥· · · ≥|λs|, and if index (λi) = ki,
where k1 = 1, then limk→∞
k
j

λk−j
i
= 0 for i ≥2 (see p. 618), and
Ak =
s

i=1
ki−1

j=0
k
j

λk−j
i
(A −λiI)jGi
= G1 +
s

i=2
ki−1

j=0
k
j

λk−j
i
(A −λiI)jGi →G1
as
k →∞.

630
Chapter 7
Eigenvalues and Eigenvectors
In other words, limk→∞Ak = G1 = G is the spectral projector associated
with λ1 = 1. Since index (λ1) = 1, we know from the discussion on p. 603 that
R (G) = N (I −A) and N (G) = R (I −A). Notice that if ρ(A) < 1, then
I −A is nonsingular, and N (I −A) = {0}. So regardless of whether the limit
is zero or nonzero, limk→∞Ak is always the projector onto N (I −A) along
R (I −A). Below is a summary of the above observations.
Limits of Powers
For A ∈Cn×n, limk→∞Ak exists if and only if
ρ(A) < 1
or else
(7.10.33)
ρ(A) = 1,
where λ = 1 is the only eigenvalue on the
unit circle, and λ = 1 is semisimple.
When it exists,
lim
k→∞Ak = the projector onto N (I −A) along R (I −A).
(7.10.34)
With each scalar sequence {α1, α2, α3, . . .} there is an associated sequence
of averages {µ1, µ2, µ3, . . .} in which
µ1 = α1,
µ2 = α1 + α2
2
,
. . . ,
µn = α1 + α2 + · · · + αn
n
.
This sequence of averages is called the associated Ces`aro sequence,
85 and when
limn→∞µn = α, we say that {αn} is Ces`aro summable (or merely summable)
to α. It can be proven (Exercise 7.10.11) that if {αn} converges to α, then
{µn} converges to α, but not conversely. In other words, convergence implies
summability, but summability doesn’t insure convergence. To see that a sequence
can be summable without being convergent, notice that the oscillatory sequence
{0, 1, 0, 1, . . .} doesn’t converge, but it is Ces`aro summable to 1/2, which is the
mean value of {0, 1}. This is typical because averaging has a smoothing eﬀect
so that oscillations that prohibit convergence of the original sequence tend to be
smoothed away or averaged out in the Ces`aro sequence.
85
Ernesto Ces`aro (1859–1906) was an Italian mathematician who worked mainly in diﬀerential
geometry but also contributed to number theory, divergent series, and mathematical physics.
After studying in Naples, Li`ege, and Paris, Ces`aro received his doctorate from the University
of Rome in 1887, and he went on to occupy the chair of mathematics at Palermo. Ces`aro’s
most important contribution is considered to be his 1890 book Lezione di geometria intrinseca,
but, in large part, his name has been perpetuated because of its attachment to the concept of
Ces`aro summability.

7.10 Diﬀerence Equations, Limits, and Summability
631
Similar statements hold for general sequences of vectors and matrices (Ex-
ercise 7.10.11), but Ces`aro summability is particularly interesting when it is
applied to the sequence P = {Ak}∞
k=0 of powers of a square matrix A. We
know from (7.10.33) and (7.10.34) under what conditions sequence P converges
as well as the nature of the limit, so let’s now suppose that P doesn’t converge,
and decide when P is summable, and what P is summable to.
From now on, we will say that An×n is a convergent matrix when
limk→∞Ak exists, and we will say that A is a summable matrix when
limk→∞(I+A+A2 +· · ·+Ak−1)/k exists. As in the scalar case, if A is conver-
gent to G, then A is summable to G, but not conversely (Exercise 7.10.11).
To analyze the summability of A in the absence of convergence, begin with the
observation that A is summable if and only if the Jordan form J = P−1AP
for A is summable, which in turn is equivalent to saying that each Jordan
block J⋆in J is summable. Consequently, A cannot be summable whenever
ρ(A) > 1 because if J⋆=
 λ
1
...
...
λ
!
is a Jordan block in which |λ| > 1, then
each diagonal entry of

I + J⋆+ · · · + Jk−1
⋆

/k is
δ(λ, k) = 1 + λ + · · · + λk−1
k
= 1
k
1 −λk
1 −λ

=
1
1 −λ
1
k −λk
k

,
(7.10.35)
and this becomes unbounded as k →∞. In other words, it’s necessary that
ρ(A) ≤1 for A to be summable. Since we already know that A is convergent
(and hence summable) to 0 when ρ(A) < 1, we need only consider the case
when A has eigenvalues on the unit circle.
If λ ∈σ (A) such that |λ| = 1, λ ̸= 1, and if index (λ) > 1, then there
is an associated Jordan block J⋆=
 λ
1
...
...
λ
!
that is larger than 1 × 1. Each
entry on the ﬁrst superdiagonal of

I + J⋆+ · · · + Jk−1
⋆

/k is the derivative
∂δ/∂λ of the expression in (7.10.35), and it’s not hard to see that ∂δ/∂λ oscil-
lates indeﬁnitely as k →∞. In other words, A cannot be summable if there
are eigenvalues λ ̸= 1 on the unit circle such that index (λ) > 1.
Similarly, if λ = 1 is an eigenvalue of index greater than one, then A can’t
be summable because each entry on the ﬁrst superdiagonal of
I + J⋆+ · · · + Jk−1
⋆
k
is
1 + 2 + · · · + (k −1)
k
= k(k −1)
2k
= k −1
2
→∞.
Therefore, if A is summable and has eigenvalues λ such that |λ| = 1, then it’s
necessary that index (λ) = 1. The condition also is suﬃcient—i.e., if ρ(A) = 1
and each eigenvalue on the unit circle is semisimple, then A is summable. This
follows because each Jordan block associated with an eigenvalue µ such that
|µ| < 1 is convergent (and hence summable) to 0 by (7.10.5), and for semisimple

632
Chapter 7
Eigenvalues and Eigenvectors
eigenvalues λ such that |λ| = 1, the associated Jordan blocks are 1 × 1 and
hence summable because (7.10.35) implies
1 + λ + · · · + λk−1
k
=





1
1 −λ
1
k −λk
k

→0
for |λ| = 1, λ ̸= 1,
1
for λ = 1.
In addition to providing a necessary and suﬃcient condition for A to be
Ces`aro summable, the preceding analysis also reveals the nature of the Ces`aro
limit because if A is summable, then each Jordan block J⋆=
 λ
1
...
...
λ
!
in
the Jordan form for A is summable, in which case we have established that
lim
k→∞
I + J⋆+ · · · + Jk−1
⋆
k
=








1

1×1
if λ = 1 and index (λ) = 1,

0

1×1
if |λ| = 1, λ ̸= 1, and index (λ) = 1,
0
if |λ| < 1.
Consequently, if A is summable, then the Jordan form for A must look like
J = P−1AP =

Ip×p
0
0
C

,
where
p = alg multA (λ = 1) ,
and the eigenvalues of C are such that |λ| < 1 or else |λ| = 1,
λ ̸= 1,
index (λ) = 1. So C is summable to 0, J is summable to
 Ip×p
0
0
0

, and
I + A + · · · + Ak−1
k
= P
I + J + · · · + Jk−1
k

P−1 →P

Ip×p
0
0
0

P−1 = G.
Comparing this expression with that in (7.10.32) reveals that the Ces`aro limit
is exactly the same as the ordinary limit, had it existed. In other words, if A is
summable, then regardless of whether or not A is convergent, A is summable
to the projector onto N (I −A) along R (I −A). Below is a formal summary
of our observations concerning Ces`aro summability.

7.10 Diﬀerence Equations, Limits, and Summability
633
Ces`aro Summability
•
A ∈Cn×n is Ces`aro summable if and only if ρ(A) < 1 or else
ρ(A) = 1 with each eigenvalue on the unit circle being semisimple.
•
When it exists, the Ces`aro limit
lim
k→∞
I + A + · · · + Ak−1
k
= G
(7.10.36)
is the projector onto N (I −A) along R (I −A).
•
G ̸= 0 if and only if 1 ∈σ (A) , in which case G is the spectral
projector associated with λ = 1.
•
If A is convergent to G, then A is summable to G, but not
conversely.
Since the projector G onto N (I −A) along R (I −A) plays a prominent
role, let’s consider how G might be computed. Of course, we could just iterate
on Ak or (I + A + · · · + Ak−1)/k, but this is ineﬃcient and, depending on the
proximity of the eigenvalues relative to the unit circle, convergence can be slow—
averaging in particular can be extremely slow. The Jordan form is the basis for
the theoretical development, but using it to compute G would be silly (see
p. 592). The formula for a projector given in (5.9.12) on p. 386 is a possibility,
but using a full-rank factorization of I −A is an attractive alternative.
A full-rank factorization of a matrix Mm×n of rank r is a factorization
M = Bm×rCr×n,
where
rank (B) = rank (C) = r = rank (M).
(7.10.37)
All of the standard reduction techniques produce full-rank factorizations. For
example, Gaussian elimination can be used because if B is the matrix of basic
columns of M, and if C is the matrix containing the nonzero rows in the
reduced row echelon form EM, then M = BC is a full-rank factorization
(Exercise 3.9.8, p. 140). If orthogonal reduction (p. 341) is used to produce a
unitary matrix P =
 P1
P2

and an upper-trapezoidal matrix T =
 T1
0

such
that PA = T, where P1 is r × m and T1 contains the nonzero rows, then
M = P∗
1T1 is a full-rank factorization. If
M = U

D
0
0
0

V∗= (U1 | U2)

D
0
0
0
  V∗
1
V∗
2
!
= U1DV∗
1
(7.10.38)

634
Chapter 7
Eigenvalues and Eigenvectors
is the singular value decomposition (5.12.2) on p. 412 (a URV factorization
(p. 407) could also be used), then M = U1(DV∗
1) = (U1D)V∗
1 are full-rank
factorizations. Projectors, in general, and limiting projectors, in particular, are
nicely described in terms of full-rank factorizations.
Projectors
If Mn×n = Bn×rCr×n is any full-rank factorization as described in
(7.10.37), and if R (M) and N (M) are complementary subspaces of
Cn, then the projector onto R (M) along N (M) is given by
P = B(CB)−1C
(7.10.39)
or
P = U1(V∗
1U1)−1V∗
1
when (7.10.38) is used.
(7.10.40)
If A is convergent or summable to G as described in (7.10.34) and
(7.10.36), and if I −A = BC is a full-rank factorization, then
G = I −B(CB)−1C
(7.10.41)
or
G = I −U1(V∗
1U1)−1V∗
1
when (7.10.38) is used.
(7.10.42)
Note: Formulas (7.10.39) and (7.10.40) are extensions of (5.13.3) on
p. 430.
Proof.
It’s always true (Exercise 4.5.12, p. 220) that
R (Xm×nYn×p) = R (X)
when
rank (Y) = n,
N (Xm×nYn×p) = N (Y)
when
rank (X) = n.
(7.10.43)
If Mn×n = Bn×rCr×n is a full-rank factorization, and if R (M) and N (M)
are complementary subspaces of CN, then rank (M) = rank

M2
(Exercise
5.10.12, p. 402), so combining this with the ﬁrst part of (7.10.43) produces
r = rank (BC) = rank (BCBC) = rank (CB)r×r
=⇒
(CB)−1 exists.
P = B(CB)−1C is a projector because P2 = P (recall (5.9.8), p. 386), and
(7.10.43) insures that R (P) = R (B) = R (M) and N (P) = N (C) = N (M).
Thus (7.10.39) is proved. If (7.10.38) is used to produce a full-rank factorization
M = U1(DV∗
1), then, because D is nonsingular,
P = (U1D)(V∗
1(U1D))−1V∗
1 = U1(V∗
1U1)−1V∗
1.
Equations (7.10.41) and (7.10.42) follow from (5.9.11), p. 386.
Formulas (7.10.40) and (7.10.42) are useful because all good matrix com-
putation packages contain numerically stable SVD implementations from which
U1 and V∗
1 can be obtained. But, of course, the singular values are not needed
in this application.

7.10 Diﬀerence Equations, Limits, and Summability
635
Example 7.10.8
Shell Game. As depicted in Figure 7.10.2, a pea is placed under one of four
shells, and an agile manipulator quickly rearranges them by a sequence of discrete
moves. At the end of each move the shell containing the pea has been shifted
either to the left or right by only one position according to the following rules.
#1
#2
#3
#4
1
1/2
1/2
1
1/2
1/2
Figure 7.10.2
When the pea is under shell #1, it is moved to position #2, and if the pea is
under shell #4, it is moved to position #3. When the pea is under shell #2 or
#3, it is equally likely to be moved one position to the left or to the right.
Problem 1: Given that we know something about where the pea starts, what
is the probability of ﬁnding the pea in any given position after k moves?
Problem 2: In the long run, what proportion of time does the pea occupy each
of the four positions?
Solution to Problem 1: Let pj(k) denote the probability that the pea is in
position j after the kth move, and translate the given information into four
diﬀerence equations by writing
p1(k) = p2(k−1)
2
p2(k) = p1(k−1) + p3(k−1)
2
p3(k) = p2(k−1)
2
+ p4(k−1)
p4(k) = p3(k−1)
2
or








p1(k)
p2(k)
p3(k)
p4(k)








=








0
1/2
0
0
1
0
1/2
0
0
1/2
0
1
0
0
1/2
0
















p1(k−1)
p2(k−1)
p3(k−1)
p4(k−1)








.
The matrix equation on the right-hand side is a homogeneous diﬀerence equation
p(k) = Ap(k −1) whose solution, from (7.10.4), is p(k) = Akp(0), and thus
Problem 1 is solved. For example, if you know that the pea is initially under
shell #2, then p(0) = e2, and after six moves the probability that the pea is
in the fourth position is p4(6) =

A6e2

4 = 21/64. If you don’t know exactly
where the pea starts, but you assume that it is equally likely to start under any
one of the four shells, then p(0) = (1/4, 1/4, 1/4, 1/4)T , and the probabilities

636
Chapter 7
Eigenvalues and Eigenvectors
for occupying the four positions after six moves are given by p(6) = A6p(0), or



p1(6)
p2(6)
p3(6)
p4(6)


=



11/32
0
21/64
0
0
43/64
0
21/32
21/32
0
43/64
0
0
21/64
0
11/32






1/4
1/4
1/4
1/4


=
1
256



43
85
85
43


.
Solution to Problem 2: There is a straightforward solution when A is a con-
vergent matrix because if Ak →G as k →∞, then p(k) →Gp(0) = p, and
the components in this limiting (or steady-state) vector p provide the answer.
Intuitively, if p(k) →p, then after awhile p(k) is practically constant, so the
probability that the pea occupies a particular position remains essentially the
same move after move. Consequently, the components in limk→∞p(k) reveal
the proportion of time spent in each position over the long run. For example,
if limk→∞p(k) = (1/6, 1/3, 1/3, 1/6)T , then, as the game runs on indeﬁnitely,
the pea is expected to be under shell #1 for about 16.7% of the time, under shell
#2 for about 33.3% of the time, etc.
A Fly in the Ointment: Everything above rests on the assumption that A
is convergent. But A is not convergent for the shell game because a bit of
computation reveals that σ (A) = {±1, ±(1/2)}. That is, there is an eigenvalue
other than 1 on the unit circle, so (7.10.33) guarantees that limk→∞Ak does
not exist. Consequently, there’s no limiting solution p to the diﬀerence equation
p(k) = Ap(k −1), and the intuitive analysis given above does not apply.
Ces`aro to the Rescue: However, A is summable because ρ(A) = 1, and
every eigenvalue on the unit circle is semisimple—these are the conditions in
(7.10.36). So as k →∞,
I + A + · · · + Ak−1
k

p(0) →Gp(0) = p.
The job now is to interpret the meaning of this Ces`aro limit in the context of
the shell game. To do so, focus on a particular position—say the jth one—and
set up “counting functions” (random variables) deﬁned as
X(0) =
	
1
if the pea starts under shell j,
0
otherwise,
and
X(i) =
	
1
if the pea is under shell j after the ith move,
0
otherwise,
i = 1, 2, 3, . . . .
Notice that X(0) + X(1) + · · · + X(k −1) counts the number of times the pea
occupies position j before the kth move, so

X(0) + X(1) + · · · + X(k −1)

/k

7.10 Diﬀerence Equations, Limits, and Summability
637
represents the fraction of times that the pea is under shell j before the kth
move. Since the expected (or mean) value of X(i) is, by deﬁnition,
E[X(i)] = 1 × P

X(i) = 1

+ 0 × P

X(i) = 0

= pj(i),
and since expectation is linear ( E[αX(i) + X(h)] = αE[X(i)] + E[X(h)] ), the
expected fraction of times that the pea occupies position j before move k is
E
%X(0) + X(1) + · · · + X(k −1)
k
&
= E[X(0)] + E[X(1)] + · · · + E[X(k −1)]
k
= pj(0) + pj(1) + · · · + pj(k −1)
k
=
%p(0) + p(1) + · · · + p(k −1)
k
&
j
=
%p(0) + Ap(0) + · · · + Ak−1p(0)
k
&
j
=
%I + A + · · · + Ak−1
k

p(0)
&
j
→[Gp(0)]j .
In other words, as the game progresses indeﬁnitely, the components of the Ces`aro
limit p = Gp(0) provide the expected proportion of times that the pea is under
each shell, and this is exactly what we wanted to know.
Computing the Limiting Vector. Of course, p can be determined by ﬁrst
computing G with a full-rank factorization of I −A as described in (7.10.41),
but there is some special structure in this problem that can be exploited to make
the task easier. Recall from (7.2.12) on p. 518 that if λ is a simple eigenvalue for
A, and if x and y∗are respective right-hand and left-hand eigenvectors associ-
ated with λ, then xy∗/y∗x is the projector onto N (λI −A) along R (λI −A).
We can use this because, for the shell game, λ = 1 is a simple eigenvalue for
A. Furthermore, we get an associated left-hand eigenvector for free—namely,
eT = (1, 1, 1, 1) —because each column sum of A is one, so eT A = eT . Con-
sequently, if x is any right-hand eigenvector of A associated with λ = 1, then
(by noting that eT p(0) = p1(0) + p2(0) + p3(0) + p4(0) = 1) the limiting vector
is given by
p = Gp(0) = xeT p(0)
eT x
=
x
eT x =
x
 xi
.
(7.10.44)
In other words, the limiting vector is obtained by normalizing any nonzero so-
lution of (I −A)x = 0 to make the components sum to one. Not only does
(7.10.44) show how to compute the limiting proportions, it also shows that the
limiting proportions are independent of the initial values in p(0). For example, a
simple calculation reveals that x = (1, 2, 2, 1)T is one solution of (I−A)x = 0,
so the vector of limiting proportions is p = (1/6, 1/3, 1/3, 1/6)T . Therefore, if
many moves are made, then, regardless of where the pea starts, we expect the
pea to end up under shell #1 in about 16.7% of the moves, under #2 for about

638
Chapter 7
Eigenvalues and Eigenvectors
33.3% of the moves, under #3 for about 33.3% of the moves, and under shell #4
for about 16.7% of the moves.
Note: The shell game (and its analysis) is a typical example of a random walk
with reﬂecting barriers, and these problems belong to a broader classiﬁcation
of stochastic processes known as irreducible, periodic Markov chains. (Markov
chains are discussed in detail in §8.4 on p. 687.) The shell game is irreducible
in the sense of Exercise 4.4.20 (p. 209), and it is periodic because the pea can
return to given position only at deﬁnite periods, as reﬂected in the periodicity
of the powers of A. More details are given in Example 8.4.3 on p. 694.
Exercises for section 7.10
7.10.1. Which of the following are convergent, and which are summable?
A=


−1/2
3/2
−3/2
1
0
−1/2
1
−1
1/2

. B=


0
1
0
0
0
1
1
0
0

. C=


−1
−2
−3/2
1
2
1
1
1
3/2

.
7.10.2. For the matrices in Exercise 7.10.1, evaluate the limit of each convergent
matrix, and evaluate the Ces`aro limit for each summable matrix.
7.10.3. Verify that the expressions in (7.10.4) are indeed the solutions to the
diﬀerence equations in (7.10.3).
7.10.4. Determine the limiting vector for the shell game in Example 7.10.8 by
ﬁrst computing the Ces`aro limit G with a full-rank factorization.
7.10.5. Verify that the expressions in (7.10.4) are indeed the solutions to the
diﬀerence equations in (7.10.3).
7.10.6. Prove that if there exists a matrix norm such that ∥A∥< 1, then
limk→∞Ak = 0.
7.10.7. By examining the iteration matrix, compare the convergence of Jacobi’s
method and the Gauss–Seidel method for each of the following coeﬃcient
matrices with an arbitrary right-hand side. Explain why this shows that
neither method can be universally favored over the other.
A1 =


1
2
−2
1
1
1
2
2
1

.
A2 =


2
−1
1
2
2
2
−1
−1
2

.

7.10 Diﬀerence Equations, Limits, and Summability
639
7.10.8. Let A =

2
−1
0
−1
2
−1
0
−1
2

(the ﬁnite-diﬀerence Example 1.4.1, p. 19).
(a)
Verify that A satisﬁes the special case conditions given in Ex-
ample 7.10.6 that guarantee the validity of (7.10.24).
(b)
Determine the optimum SOR relaxation parameter.
(c)
Find the asymptotic rates of convergence for Jacobi, Gauss–
Seidel, and optimum SOR.
(d)
Use x(0) = (1, 1, 1)T and b = (2, 4, 6)T to run through sev-
eral steps of Jacobi, Gauss–Seidel, and optimum SOR to solve
Ax = b until you can see a convergence pattern.
7.10.9. Prove that if ρ (Hω) < 1, where Hω is the iteration matrix for the SOR
method, then 0 < ω < 2. Hint: Use det (Hω) to show |λk| ≥|1 −ω|
for some λk ∈σ (Hω) .
7.10.10. Show that the spectral radius of the Jacobi iteration matrix for the
discrete Laplacian Ln2×n2 described in Example 7.6.2 (p. 563) is
ρ (HJ) = cos π/(n + 1).
7.10.11. Consider a scalar sequence {α1, α2, α3, . . .} and the associated Ces`aro
sequence of averages {µ1, µ2, µ3, . . .}, where µn = (α1+α2+· · ·+αn)/n.
Prove that if {αn} converges to α, then {µn} also converges to α.
Note: Like scalars, a vector sequence {vn} in a ﬁnite-dimensional space
converges to v if and only if for each ϵ > 0 there is a natural number
N = N(ϵ) such that ∥vn −v∥< ϵ for all n ≥N, and, by virtue of
Example 5.1.3 (p. 276), it doesn’t matter which norm is used. Therefore,
your proof should also be valid for vectors (and matrices).
7.10.12. M-matrices Revisited. For matrices with nonpositive oﬀ-diagonal en-
tries (Z-matrices), prove that the following statements are equivalent.
(a)
A is an M-matrix.
(b)
All leading principal minors of A are positive.
(c)
A has an LU factorization, and both L and U are M-matrices.
(d)
There exists a vector x > 0 such that Ax > 0.
(e)
Each aii > 0 and AD is diagonally dominant for some diago-
nal matrix D with positive diagonal entries.
(f)
Ax ≥0 implies x ≥0.

640
Chapter 7
Eigenvalues and Eigenvectors
7.10.13. Index by Full-Rank Factorization.
Suppose that λ ∈σ (A) , and
let M1 = A−λI. The following procedure yields the value of index (λ).
Factor M1 = B1C1 as a full-rank factorization.
Set M2 = C1B1.
Factor M2 = B2C2 as a full-rank factorization.
Set M3 = C2B2.
...
In general, Mi = Ci−1Bi−1, where Mi−1 = Bi−1Ci−1 is a full-rank
factorization.
(a)
Explain why this procedure must eventually produce a matrix
Mk that is either nonsingular or zero.
(b)
Prove that if k is the smallest positive integer such that M−1
k
exists or Mk = 0, then
index (λ) =
	
k −1
if Mk is nonsingular,
k
if Mk = 0.
7.10.14. Use the procedure in Exercise 7.10.13 to ﬁnd the index of each eigenvalue
of A =
 −3
−8
−9
5
11
9
−1
−2
1

. Hint: σ (A) = {4, 1}.
7.10.15. Let A be the matrix given in Exercise 7.10.14.
(a)
Find the Jordan form for A.
(b)
For any function f deﬁned at A, ﬁnd the Hermite interpolation
polynomial that is described in Example 7.9.4 (p. 606), and
describe f(A).
7.10.16. Limits and Group Inversion. Given a matrix Bn×n of rank r such
that index (B) ≤1 (i.e., index (λ = 0) ≤1 ), the Jordan form for B
looks like
 0
0
0
Cr×r

= P−1BP, so B = P
 0
0
0
C

P−1, where C
is nonsingular. This implies that B belongs to an algebraic group G
with respect to matrix multiplication, and the inverse of B in G is
B# = P
 0
0
0
C−1

P−1. Naturally, B# is called the group inverse of
B. The group inverse is a special case of the Drazin inverse discussed in
Example 5.10.5 on p. 399, and properties of group inversion are devel-
oped in Exercises 5.10.11–5.10.13 on p. 402. Prove that if limk→∞Ak
exists, and if B = I −A, then
lim
k→∞Ak = I −BB#.
In other words, the limiting matrix can be characterized as the diﬀer-
ence of two identity elements— I is the identity in the multiplicative
group of nonsingular matrices, and BB# is the identity element in the
multiplicative group containing B.

7.10 Diﬀerence Equations, Limits, and Summability
641
7.10.17. If Mn×n is a group matrix (i.e., if index (M) ≤1 ), then the group
inverse of M can be characterized as the unique solution M# of the
equations MM#M = M, M#MM# = M#, and MM# = M#M.
In fact, some authors use these equations to deﬁne M#. Use this char-
acterization to show that if M = BC is any full-rank factorization of
M, then M# = B(CB)−2C. In particular, if M = U1DV∗
1 is the
full-rank factorization derived from the singular value decomposition as
described in (7.10.38), then
M# = U1D−1/2(V∗
1U1)−2D−1/2V∗
1
= U1D−1(V∗
1U1)−2V∗
1
= U1(V∗
1U1)−2D−1V∗
1.

642
Chapter 7
Eigenvalues and Eigenvectors
7.11
MINIMUM POLYNOMIALS AND KRYLOV METHODS
The characteristic polynomial plays a central role in the theoretical development
of linear algebra and matrix analysis, but it is not alone in this respect. There
are other polynomials that occur naturally, and the purpose of this section is to
explore some of them.
In this section it is convenient to consider the characteristic polynomial of
A ∈Cn×n to be c(x) = det (xI −A). This diﬀers from the deﬁnition given on
p. 492 only in the sense that the coeﬃcients of c(x) = det (xI −A) have diﬀerent
signs than the coeﬃcients of ˆc(x) = det (A −xI). In particular, c(x) is a monic
polynomial (i.e., its leading coeﬃcient is 1), whereas the leading coeﬃcient of ˆc(x)
is (−1)n. (Of course, the roots of c and ˆc are identical.)
Monic polynomials p(x) such that p(A) = 0 are said to be annihilating
polynomials for A. For example, the Cayley–Hamilton theorem (pp. 509, 532)
guarantees that c(x) is an annihilating polynomial of degree n.
Minimum Polynomial for a Matrix
There is a unique annihilating polynomial for A of minimal degree, and
this polynomial, denoted by m(x), is called the minimum polynomial
for A. The Cayley–Hamilton theorem guarantees that deg[m(x)] ≤n.
Proof.
Only uniqueness needs to be proven. Let k be the smallest degree of
any annihilating polynomial for A. There is a unique annihilating polynomial
for A of degree k because if there were two diﬀerent annihilating polynomials
p1(x) and p2(x) of degree k, then d(x) = p1(x) −p2(x) would be a nonzero
polynomial such that d(A) = 0 and deg[d(x)] < k. Dividing d(x) by its leading
coeﬃcient would produce an annihilating polynomial of degree less than k, the
minimal degree, and this is impossible.
The ﬁrst problem is to describe what the minimum polynomial m(x) for
A ∈Cn×n looks like, and the second problem is to uncover the relationship
between m(x) and the characteristic polynomial c(x). The Jordan form for
A reveals everything. Suppose that A = PJP−1, where J is in Jordan form.
Since p(A) = 0 if and only if p(J) = 0 or, equivalently, p(J⋆) = 0 for each
Jordan block J⋆, it’s clear that m(x) is the monic polynomial of smallest degree
that annihilates all Jordan blocks. If J⋆is a k × k Jordan block associated
with an eigenvalue λ, then (7.9.2) on p. 600 insures that p(J⋆) = 0 if and
only if p(i)(λ) = 0 for i = 0, 2, . . . , k −1, and this happens if and only if
p(x) = (x −λ)kq(x) for some polynomial q(x). Since this must be true for
all Jordan blocks associated with λ, it must be true for the largest Jordan
block associated with λ, and thus the minimum degree monic polynomial that

7.11 Minimum Polynomials and Krylov Methods
643
annihilates all Jordan blocks associated with λ is
pλ(x) = (x −λ)kλ,
where
kλ = index (λ).
Since the minimum polynomial for A must annihilate the largest Jordan block
associated with each λj ∈σ (A) , it follows that
m(x) = (x −λ1)k1(x −λ2)k2 · · · (x −λs)ks,
where
kj = index (λj) (7.11.1)
is the minimum polynomial for A.
Example 7.11.1
Minimum Polynomial, Gram–Schmidt, and QR.
If you are willing to
compute the eigenvalues λj and their indicies kj for a given A ∈Cn×n, then,
as shown in (7.11.1), the minimum polynomial for A ∈Cn×n is obtained by
setting m(x) = (x −λ1)k1(x −λ2)k2 · · · (x −λs)ks. But ﬁnding the eigenvalues
and their indicies can be a substantial task, so let’s consider how we might
construct m(x) without computing eigenvalues. An approach based on ﬁrst
principles is to determine the ﬁrst matrix Ak for which {I, A, A2, . . . , Ak} is
linearly dependent. In other words, if k is the smallest positive integer such that
Ak = k−1
j=0 αjAj, then the minimum polynomial for A is
m(x) = xk −
k−1

j=0
αjxj.
The Gram–Schmidt orthogonalization procedure (p. 309) with the standard in-
ner product ⟨A B⟩= trace (A∗B) (p. 286) is the perfect theoretical tool for
determining k and the αj ’s. Gram–Schmidt applied to {I, A, A2, . . .} begins
by setting U0 = I/ ∥I∥F = I/√n, and it proceeds by sequentially computing
Uj =
Aj −j−1
i=0
8
Ui Aj9
Ui
∥Aj −j−1
i=0 ⟨Ui Aj⟩Ui∥F
for
j = 1, 2, . . .
(7.11.2)
until Ak −k−1
i=0
8
Ui Ak9
Ui = 0. The ﬁrst such k is the smallest positive in-
teger such that Ak ∈span {U0, U1, . . . , Uk−1} = span

I, A, . . . , Ak−1
. The
coeﬃcients αj such that Ak = k−1
j=0 αjAj are easily determined from the
upper-triangular matrix R in the QR factorization produced by the Gram–
Schmidt process. To see how, extend the notation in the discussion on p. 311 in
an obvious way to write (7.11.2) in matrix form as

I | A | · · · | Ak
=

U0 | U1 | · · · | Uk








ν0
r01
· · ·
r0k−1
r0k
0
ν1
· · ·
r1k−1
r1k
...
...
...
...
...
0
0
νk−1
rk−1k
0
0
· · ·
0
0







, (7.11.3)

644
Chapter 7
Eigenvalues and Eigenvectors
where ν0 = ∥I∥F = √n , νj =
)))Aj −j−1
i=0
8
Ui Aj9
Ui
)))
F, and rij =
8
Ui Aj9
.
If we set R =


ν0
· · ·
r0k−1
...
...
νk−1

and c =


r0k
...
rk−1k

, then (7.11.3) implies that
Ak =

U0| · · · |Uk−1

c =

I| · · · |Ak−1
R−1c, so R−1c =


α0
...
αk−1

contains
the coeﬃcients such that Ak = k−1
j=0 αjAj, and thus the coeﬃcients in the
minimum polynomial are determined.
Caution!
While Gram–Schmidt works ﬁne to produce m(x) in exact arith-
metic, things are not so nice in ﬂoating-point arithmetic. For example, if A
has a dominant eigenvalue, then, as explained in the power method (Example
7.3.7, p. 533), Ak asymptotically approaches the dominant spectral projector
G1, so, as k grows, Ak becomes increasingly close to span

I, A, . . . , Ak−1
.
Consequently, ﬁnding the ﬁrst Ak that is truly in span

I, A, . . . , Ak−1
is
an ill-conditioned problem, and Gram–Schmidt may not work well in ﬂoating-
point arithmetic—the modiﬁed Gram–Schmidt algorithm (p. 316), or a version
of Householder reduction (p. 341), or Arnoldi’s method (p. 653) works better.
Fortunately, explicit knowledge of the minimum polynomial often is not needed
in applied work.
The relationship between the characteristic polynomial c(x) and the mini-
mum polynomial m(x) for A is now transparent. Since
c(x) = (x −λ1)a1(x −λ2)a2 · · · (x −λs)as,
where
aj = alg mult (λj),
and
m(x) = (x −λ1)k1(x −λ2)k2 · · · (x −λs)ks,
where
kj = index (λj),
it’s clear that m(x) divides c(x). Furthermore, m(x) = c(x) if and only if
alg mult (λj) = index (λj) for each λj ∈σ (A) . Matrices for which m(x) = c(x)
are said to be nonderogatory matrices, and they are precisely the ones for
which geo mult (λj) = 1 for each eigenvalue λj because
m(x) = c(x) ⇐⇒alg mult (λj) = index (λj) for each j
⇐⇒there is only one Jordan block for each λj
⇐⇒there is only one independent eigenvector for each λj
⇐⇒geo mult (λj) = 1 for each λj.
In addition to dividing the characteristic polynomial c(x), the minimum
polynomial m(x) divides all other annihilating polynomials p(x) for A be-
cause deg[m(x)] ≤deg[p(x)] insures the existence of polynomials q(x) and
r(x) (quotient and remainder) such that
p(x) = m(x)q(x) + r(x),
where
deg[r(x)] < deg[m(x)].

7.11 Minimum Polynomials and Krylov Methods
645
Since
0 = p(A) = m(A)q(A) + r(A) = r(A),
it follows that r(x) = 0; otherwise r(x), when normalized to be monic, would
be an annihilating polynomial having degree smaller than the degree of the min-
imum polynomial.
The structure of the minimum polynomial for A is related to the diago-
nalizability of A. By combining the fact that kj = index (λj) is the size of
the largest Jordan block for λj with the fact that A is diagonalizable if and
only if all Jordan blocks are 1 × 1, it follows that A is diagonalizable if and
only if kj = 1 for each j, which, by (7.11.1), is equivalent to saying that
m(x) = (x −λ1)(x −λ2) · · · (x −λs). In other words, A is diagonalizable if and
only if its minimum polynomial is the product of distinct linear factors.
Below is a summary of the preceding observations about properties of m(x).
Properties of the Minimum Polynomial
Let A ∈Cn×n with σ (A) = {λ1, λ2, . . . , λs} .
•
The minimum polynomial of A is the unique monic polynomial
m(x) of minimal degree such that m(A) = 0.
•
m(x) = (x −λ1)k1(x −λ2)k2 · · · (x −λs)ks, where kj = index (λj).
•
m(x) divides every polynomial p(x) such that p(A) = 0. In par-
ticular, m(x) divides the characteristic polynomial c(x).
(7.11.4)
•
m(x) = c(x) if and only if geo mult (λj) = 1 for each λj or,
equivalently, alg mult (λj) = index (λj) for each j, in which case
A is called a nonderogatory matrix.
•
A is diagonalizable if and only if m(x) = (x−λ1)(x−λ2) · · · (x−λs)
(i.e., if and only if m(x) is a product of distinct linear factors).
The next immediate aim is to extend the concept of the minimum polyno-
mial for a matrix to formulate the notion of a minimum polynomial for a vector.
To do so, it’s helpful to introduce Krylov
86 sequences, subspaces, and matrices.
86
Aleksei Nikolaevich Krylov (1863–1945) showed in 1931 how to use sequences of the form
{b, Ab, A2b, . . .} to construct the characteristic polynomial of a matrix (see Example 7.11.3
on p. 649). Krylov was a Russian applied mathematician whose scientiﬁc interests arose from
his early training in naval science that involved the theories of buoyancy, stability, rolling
and pitching, vibrations, and compass theories. Krylov served as the director of the Physics–
Mathematics Institute of the Soviet Academy of Sciences from 1927 until 1932, and in 1943
he was awarded a “state prize” for his work on compass theory. Krylov was made a “hero of

646
Chapter 7
Eigenvalues and Eigenvectors
Krylov Sequences, Subspaces, and Matrices
For A ∈Cn×n and 0 ̸= b ∈Cn×1, we adopt the following terminology.
•
{b, Ab, A2b, . . . , Aj−1b} is called a Krylov sequence.
•
Kj = span

b, Ab, . . . , Aj−1b

is called a Krylov subspace.
•
Kn×j =

b | Ab | · · · | Aj−1b

is called a Krylov matrix.
Since dim(Kj) ≤n (because Kj ⊆Cn×1 ), there is a ﬁrst vector Akb in
the Krylov sequence that is a linear combination of preceding Krylov vectors. If
Akb =
k−1

j=0
αjAjb,
then we deﬁne
v(x) = xk −
k−1

j=0
αjxj,
and we say that v(x) is an annihilating polynomial for b relative to A
because v(x) is a monic polynomial such that v(A)b = 0. The argument on
p. 642 that establishes uniqueness of the minimum polynomial for matrices can
be reapplied to prove that for each matrix–vector pair (A, b) there is a unique
annihilating polynomial of b relative to A that has minimal degree. These
observations are formalized below.
Minimum Polynomial for a Vector
•
The minimum polynomial for b ∈Cn×1 relative to A ∈Cn×n
is deﬁned to be the monic polynomial v(x) of minimal degree such
that v(A)b = 0.
•
If Akb is the ﬁrst vector in the Krylov sequence {b, Ab, A3b, . . .}
that is a linear combination of preceding Krylov vectors (say
Akb = k−1
j=0 αjAjb ), then v(x) = xk −k−1
j=0 αjxj (or v(x) = 1
when b = 0 ) is the minimum polynomial for b relative to A.
socialist labor,” and he is one of a few mathematicians to have a lunar feature named in his
honor—on the moon there is the “Crater Krylov.”

7.11 Minimum Polynomials and Krylov Methods
647
So is the minimum polynomial for a matrix related to minimum polynomials
for vectors? It seems intuitive that knowing the minimum polynomial of b rela-
tive to A for enough diﬀerent vectors b should somehow lead to the minimum
polynomial for A. This is indeed the case, and here is how it’s done. Recall that
the least common multiple (LCM) of polynomials v1(x), . . . , vn(x) is the unique
monic polynomial l(x) such that
(i)
each vi(x) divides l(x);
(ii)
if each vi(x) also divides q(x), then l(x) divides q(x).
Minimum Polynomial as LCM
Let A ∈Cn×n, and let B = {b1, b2, . . . , bn} be any basis for Cn×1.
If vi(x) is the minimum polynomial for bi relative to A, then the
minimum polynomial m(x) for A is the least common multiple of
v1(x), v2(x), . . . , vn(x).
(7.11.5)
Proof.
The strategy ﬁrst is to prove that if l(x) is the LCM of the vi(x) ’s,
then m(x) divides l(x). Then prove the reverse by showing that l(x) also
divides m(x). Since each vi(x) divides l(x), it follows that l(A)bi = 0 for
each i. In other words, B ⊂N(l(A)), so dim N(l(A)) = n or, equivalently,
l(A) = 0. Therefore, by property (7.11.4) on p. 645, m(x) divides l(x). Now
show that l(x) divides m(x) . Since m(A)bi = 0 for every bi, it follows that
deg[vi(x)] < deg[m(x)] for each i, and hence there exist polynomials qi(x) and
ri(x) such that m(x) = qi(x)vi(x) + ri(x), where deg[ri(x)] < deg[vi(x)]. But
0 = m(A)bi = qi(A)vi(A)bi + ri(A)bi = ri(A)bi
insures ri(x) = 0, for otherwise ri(x) (when normalized to be monic) would be
an annihilating polynomial for bi of degree smaller than the minimum polyno-
mial for bi, which is impossible. In other words, each vi(x) divides m(x), and
this implies l(x) must also divide m(x). Therefore, since m(x) and l(x) are
divisors of each other, it must be the case that m(x) = l(x).
The utility of this result is illustrated in the following development. We
already know that associated with n × n matrix A is an nth-degree monic
polynomial—namely, the characteristic polynomial c(x) = det (xI −A). But
the reverse is also true. That is, every nth-degree monic polynomial is the char-
acteristic polynomial of some n × n matrix.

648
Chapter 7
Eigenvalues and Eigenvectors
Companion Matrix of a Polynomial
For each monic polynomial p(x) = xn + αn−1xn−1 + · · · + α1x + α0,
the companion matrix of p(x) is deﬁned (by G. Frobenius) to be
C =




0
0
· · ·
0
−α0
1
0
· · ·
0
−α1
...
...
...
...
0
· · ·
1
0
−αn−2
0
0
· · ·
1
−αn−1




n×n
.
(7.11.6)
•
The polynomial p(x) is both the characteristic and minimum poly-
nomial for C (i.e., C is nonderogatory).
Proof.
To prove that det (xI −C) = p(x), write C = N −ceT
n, where
N =




0
1
...
...
...
1
0




and
c =



α0
α1
...
αn−1


,
and use (6.2.3) on p. 475 to conclude that
det (xI −C) = det (xI −N)(1 + eT
ndet (xI −N)−1c)
= xn

1 + eT
n
 I
x + N
x2 + N2
x3 + · · · + Nn−1
xn

c

= xn + αn−1xn−1 + αn−2xn−2 + · · · + α0
= p(x).
The fact that p(x) is also the minimum polynomial for C is a consequence of
(7.11.5). Set B = {e1, e2, . . . , en} , and let vi(x) be the minimum polynomial
of ei with respect to C. Observe that v1(x) = p(x) because Cej = ej+1 for
j = 1, . . . , n −1, so
{e1, Ce1, C2e1, . . . , Cn−1e1} = {e1, e2, e3, . . . , en}
and
Cne1 = Cen = C∗n = −
n−1

j=0
αjej+1 = −
n−1

j=0
αjCje1
=⇒
v1(x) = p(x).
Since v1(x) divides the LCM of all vi(x) ’s (which we know from (7.11.5) to be
the minimum polynomial m(x) for C ), we conclude that p(x) divides m(x).
But m(x) always divides p(x) —recall (7.11.4)—so m(x) = p(x).

7.11 Minimum Polynomials and Krylov Methods
649
Example 7.11.2
Poor Man’s Root Finder. The companion matrix is the source of what is
often called the poor man’s root ﬁnder because any general purpose algorithm
designed to compute eigenvalues (e.g., the QR iteration on p. 535) can be applied
to the companion matrix for a polynomial p(x) to compute the roots of p(x).
When used in conjunction with (7.1.12) on p. 497, the companion matrix is also
a poor man’s root bounder. For example, it follows that if λ is a root of p(x),
then
|λ| ≤∥C∥∞= max{|α0|, 1 + |α1|, . . . , 1 + |αn−1|} ≤1 + max |αi|.
The results on p. 647 insure that the minimum polynomial v(x) for every
nonzero vector b relative to A ∈Cn×n divides the minimum polynomial m(x)
for A, which in turn divides the characteristic polynomial c(x) for A, so it
follows that every v(x) divides c(x). This suggests that it might be possible to
construct c(x) as a product of vi(x) ’s. In fact, this is what Krylov did in 1931,
and the following example shows how he did it.
Example 7.11.3
Krylov’s method for constructing the characteristic polynomial for A ∈Cn×n
as a product of minimum polynomials for vectors is as follows.
Starting with any nonzero vector bn×1, let v1(x) = xk−k−1
j=0 αjxj be the min-
imum polynomial for b relative to A, and let K1 =

b | Ab | · · · | Ak−1b

n×k
be the associated Krylov matrix. Notice that rank (K1) = k (by deﬁnition of
the minimum polynomial for b ). If C1 is the k × k companion matrix of v(x)
as described in (7.11.6), then direct multiplication shows that
K1C1 = AK1.
(7.11.7)
If k = n, then K−1
1 AK1 = C1, so v1(x) must be the characteristic polynomial
for A, and there is nothing more to do. If k < n, then use any n × (n −k)
matrix 7K1 such that K2 =

K1 | 7K1

n×n is nonsingular, and use (7.11.7) to
write
AK2 =

AK1 | A 7K1

=

K1 | 7K1
 
C1
X
0
A2

,
where

X
A2

= K−1
2 A 7K1.
Therefore, K−1
2 AK2 =
 C1
X
0
A2

, and hence
c(x) = det (xI −A) = det (xI −C1)det (xI −A2) = v1(x) det (xI −A2).

650
Chapter 7
Eigenvalues and Eigenvectors
Repeat the process on A2. If the Krylov matrix on the second time around is
nonsingular, then c(x) = v1(x)v2(x); otherwise c(x) = v1(x)v2(x) det (xI −A3)
for some matrix A3. Continuing in this manner until a nonsingular Krylov
matrix is obtained—say at the mth step—produces a nonsingular matrix K
such that
K−1AK=


C1
· · ·
⋆
...
...
Cm

= H,
(7.11.8)
where the Cj ’s are companion matrices, and thus c(x) = v1(x)v2(x) · · · vm(x).
Note: All companion matrices are upper-Hessenberg matrices as described in
Example 5.7.4 (p. 350)—e.g., a 5 × 5 Hessenberg form is
H5 =


∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
0
∗
∗
∗
∗
0
0
∗
∗
∗
0
0
0
∗
∗

.
Since the matrix H in (7.11.8) is upper Hessenberg, we see that Krylov’s method
boils down to a recipe for using Krylov sequences to build a similarity transfor-
mation that will reduce A to upper-Hessenberg form. In eﬀect, this means that
most information about A can be derived from Krylov sequences and the asso-
ciated Hessenberg form H. This is the real message of this example.
Deriving information about A by using a Hessenberg form and a Krylov
similarity transformation as shown in (7.11.8) has some theoretical appeal, but
it’s not a practical idea as far as computation is concerned. Krylov sequences tend
to be nearly linearly dependent sets because, as the power method of Example
7.3.7 (p. 533) indicates, the directions of the vectors Akb want to converge to
the direction of an eigenvector for A, so, as k grows, the vectors in a Krylov
sequence become ever closer to being multiples of each other. This means that
Krylov matrices tend to be ill conditioned. Putting conditioning issues aside,
there is still a problem with computational eﬃciency because K is usually a
dense matrix (one with a preponderance of nonzero entries) even when A is
sparse (which it often is in applied work), so the amount of arithmetic involved
in the reduction (7.11.8) is prohibitive.
However, these objections often can be overcome by replacing a Krylov
matrix K =

b | Ab | · · · | Ak−1b

with its QR factorization K = Qn×kRk×k.
Doing so in (7.11.7) (and dropping the subscript) produces
AK = KC
=⇒
AQR = QRC
=⇒
Q∗AQ = RCR−1 = H.
(7.11.9)
While H = RCR−1 is no longer a companion matrix, it’s still in upper-
Hessenberg form (convince yourself by writing out the pattern for the 4 × 4
case). In other words, an orthonormal basis for a Krylov subspace can reduce a

7.11 Minimum Polynomials and Krylov Methods
651
matrix to upper-Hessenberg form. Since matrices with orthonormal columns are
perfectly conditioned, the ﬁrst objection raised above is overcome. The second
objection concerning computational eﬃciency is dealt with in Examples 7.11.4
and 7.11.5.
If k < n, then Q is not square, and Q∗AQ = H is not a similarity
transformation, so it would be wrong to conclude that A and H have the same
spectral properties. Nevertheless, it’s often the case that the eigenvalues of H,
which are called the Ritz values for A, are remarkably good approximations to
the extreme eigenvalues of A, especially when A is hermitian. This is somewhat
intuitive because Q∗AQ can be viewed as a generalization of (7.5.4) on p. 549
that says λmax = max∥x∥2=1 x∗Ax and λmin = min∥x∥2=1 x∗Ax. The results
of Exercise 5.9.15 (p. 392) can be used to argue the point further.
Example 7.11.4
Lanczos
87 Tridiagonalization Algorithm. The fact that the matrix H in
(7.11.9) is upper Hessenberg is particularly nice when A is real and symmetric
because AT = A implies HT = (QT AQ)T = H, and symmetric Hessenberg
matrices are tridiagonal in structure. That is,
H =






α1
β1
β1
α2
β2
β2
α3
...
...
...
βn−1
βn−1
αn






when A = AT .
(7.11.10)
This makes Q particularly easy to determine. While the matrix Q in (7.11.9)
was only n × k, let’s be greedy and look for an n × n orthogonal matrix Q
such that AQ = QH, where H is tridiagonal as depicted in (7.11.10). If we
set Q =

q1 | q2 | · · · | qn

, and if we agree to let β0 = 0 and qn+1 = 0, then
87
Cornelius Lanczos (1893–1974) was born Korn´el L¨owy in Budapest, Hungary, to Jewish par-
ents, but he changed his name to avoid trouble during the dangerous times preceding World
War II. After receiving his doctorate from the University of Budapest in 1921, Lanczos moved
to Germany where he became Einstein’s assistant in Berlin in 1928. After coming home to
Germany from a visit to Purdue University in Lafayette, Indiana, in 1931, Lanczos decided
that the political climate in Germany was unacceptable, and he returned to Purdue in 1932 to
continue his work in mathematical physics. The development of electronic computers stimu-
lated Lanczos’s interest in numerical analysis, and this led to positions at the Boeing Company
in Seattle and at the Institute for Numerical Analysis of the National Bureau of Standards
in Los Angeles. When senator Joseph R. McCarthy led a crusade against communism in the
1950s, Lanczos again felt threatened, so he left the United States to accept an oﬀer from the
famous Nobel physicist Erwin Schr¨odinger (1887–1961) to head the Theoretical Physics De-
partment at the Dublin Institute for Advanced Study in Ireland where Lanczos returned to his
ﬁrst love—the theory of relativity. Lanczos was aware of the fast Fourier transform algorithm
(p. 373) 25 years before the heralded work of J. W. Cooley and J. W. Tukey (p. 368) in 1965,
but 1940 was too early for applications of the FFT to be realized. This is yet another instance
where credit and fame are accorded to those who ﬁrst make good use of an idea rather than
to those who ﬁrst conceive it.

652
Chapter 7
Eigenvalues and Eigenvectors
equating the jth column of AQ to the jth column of QH tells us that we
must have
Aqj = βj−1qj−1 + αjqj + βjqj+1
for j = 1, 2, . . . , n
or, equivalently,
βjqj+1 = vj,
where
vj = Aqj −αjqj −βj−1qj−1
for j = 1, 2, . . . , n.
By observing that αj = qT
j Aqj and βj = ∥vj∥2 , we are led to Lanczos’s
algorithm.
•
Start with an arbitrary b ̸= 0, set β0 = 0, q0 = 0, q1 = b/ ∥b∥2 , and
iterate as indicated below.
For j = 1 to n
v
←
Aqj
αj
←
qT
j v
v
←
v −αjqj −βj−1qj−1
βj
←
∥v∥2
If βj = 0, then quit
qj+1 ←
v/βj
End
After the kth step we have an n × (k + 1) matrix Qk+1 =

q1 | q2 | · · · | qk+1

of orthonormal columns such that
AQk = Qk+1

Tk
βkeT
k

, where Tk is the k × k tridiagonal form (7.11.10).
If the iteration terminates prematurely because βj = 0 for j < n, then restart
the algorithm with a new initial vector b that is orthogonal to q1, q2, . . . , qj.
When a full orthonormal set {q1, q2, . . . , qn} has been computed and turned
into an orthogonal matrix Q, we will have
QT AQ =



T1
0
· · ·
0
0
T2
· · ·
0
...
...
...
...
0
0
· · ·
Tm


,
where each Ti is tridiagonal
(7.11.11)
with the splits occurring at rows where the βj ’s are zero. Of course, having these
splits is generally a desirable state of aﬀairs, especially when the objective is to
compute the eigenvalues of A.
Note: The Lanczos algorithm is computationally eﬃcient because if each row of
A has ν nonzero entries, then each matrix–vector product uses νn multiplica-
tions, so each step of the process uses only νn + 4n multiplications (and about

7.11 Minimum Polynomials and Krylov Methods
653
the same number of additions). This can be a tremendous savings over what
is required by Householder (or Givens) reduction as discussed in Example 5.7.4
(p. 350). Once the form (7.11.11) has been determined, spectral properties of A
usually can be extracted by a variety of standard methods such as the QR iter-
ation (p. 535). An alternative to computing the full tridiagonal decomposition
is to stop the Lanczos iteration before completion, accept the Ritz values (the
eigenvalues Hk×k = QT
k×nAQn×k) as approximations to a portion of σ (A) ,
deﬂate the problem, and repeat the process on the smaller result.
Even when A is not symmetric, the same logic that produces the Lanc-
zos algorithm can be applied to obtain an orthogonal matrix Q such that
QT AQ = H is upper Hessenberg. But we can’t expect to obtain the eﬃciency
that Lanczos provides because the tridiagonal structure is lost. The more general
algorithm is called Arnoldi’s
88 method, and it’s presented below.
Example 7.11.5
Arnoldi Orthogonalization Algorithm. Given A ∈Cn×n, the goal is to
compute an orthogonal matrix Q =

q1 | q2 | · · · | qn

such that QT AQ = H
is upper Hessenberg. Proceed in the manner that produced the Lanczos algorithm
by equating the jth column of AQ to the jth column of QH to obtain
Aqj =
j+1

i=1
qihij
=⇒
qT
k Aqj =
j+1

i=1
qT
k qihij = hkj
for each 1 ≤k ≤j
=⇒
hj+1,jqj+1 = Aqj −
j

i=1
qihij.
By observing that hj+1,j = ∥vj∥2 for vj = Aqj −j
i=1 qihij, we are led to
Arnoldi’s algorithm.
•
Start with an arbitrary b ̸= 0, set q1 = b/ ∥b∥2 , and then iterate as
indicated below.
88
Walter Edwin Arnoldi (1917–1995) was an American engineer who published this technique in
1951, not far from the time that Lanczos’s algorithm emerged. Arnoldi received his undergrad-
uate degree in mechanical engineering from Stevens Institute of Technology, Hoboken, New
Jersey, in 1937 and his MS degree at Harvard University in 1939. He spent his career working
as an engineer in the Hamilton Standard Division of the United Aircraft Corporation where he
eventually became the division’s chief researcher. He retired in 1977. While his research con-
cerned mechanical and aerodynamic properties of aircraft and aerospace structures, Arnoldi’s
name is kept alive by his orthogonalization procedure.

654
Chapter 7
Eigenvalues and Eigenvectors
For j = 1 to n
v
←
Aqj
For i = 1 to j
hij ←
qT
i v
v
←
v −hijqi
End For
hj+1,j ←
∥v∥2
If hj+1,j = 0, then quit
qj+1
←
v/hj+1,j
End For
(7.11.12)
After the kth step we have an n × (k + 1) matrix Qk+1 =

q1 | q2 | · · · | qk+1

of orthonormal columns such that
AQk = Qk+1

Hk
hk+1,keT
k

,
(7.11.13)
where Hk is a k × k upper-Hessenberg matrix.
Note: Remarks similar to those made about the Lanczos algorithm also hold
for Arnoldi’s algorithm, but the computational eﬃciency of Arnoldi is not as
great as that of Lanczos. Close examination of Arnoldi’s method reveals that it
amounts to a modiﬁed Gram–Schmidt process (p. 316).
Krylov methods are a natural way to solve systems of linear equations. To
see why, suppose that An×nx = b with b ̸= 0 is a nonsingular system, and let
v(x) = xk −k−1
j=0 αjxj be the minimum polynomial of b with respect to A.
Since α0 ̸= 0 (otherwise v(x)/x would be an annihilating polynomial for b of
degree less than deg v), we have
Akb −
k−1

j=0
αjAjb = 0
=⇒
A
%Ak−1b −αk−1Ak−2b −· · · −α1b
α0
&
= b.
In other words, the solution of Ax = b is somewhere in the Krylov space Kk.
A technique for sorting through Kk to ﬁnd the solution (or at least an
acceptable approximate solution) of Ax = b is to sequentially consider the
subspaces A(K1), A(K2), . . . , A(Kk), where at the jth step of the process the
vector xj ∈A(Kj) that is closest to b is used as an approximation to x. If
Qj is an n × j orthogonal matrix whose columns constitute a basis for Kj,
then R (AQj) = A(Kj), so the vector xj ∈A(Kj) that is closest to b is the
orthogonal projection of b onto R (AQj). This means that xj is the least
squares solution of AQjz = b (p. 439). If the solution of this least squares
problem yields a vector xj such that the residual rj = b −AQjxj is zero
(or satisfactorily small), then set x = Qjxj, and quit. Otherwise move up one

7.11 Minimum Polynomials and Krylov Methods
655
dimension, and compute the least squares solution xj+1 of AQj+1z = b. Since
x ∈Kk, the process is guaranteed to terminate in k ≤n steps or less (when
exact arithmetic is used). When Arnoldi’s method is used to implement this idea,
the resulting algorithm is known as GMRES (an acronym for the generalized
minimal residual algorithm that was formulated by Yousef Saad and Martin H.
Schultz in 1986).
Example 7.11.6
GMRES Algorithm. To implement the idea discussed above by employing
Arnoldi’s algorithm, recall from (7.11.13) that after j steps of the Arnoldi pro-
cess we have matrices Qj and Qj+1 with orthonormal columns that span Kj
and Kj+1, respectively, along with a j × j upper-Hessenberg matrix Hj such
that
AQj = Qj+1 7Hj,
where
7Hj =

Hj
hj+1,jeT
j

.
Consequently the least squares solution of AQjz = b is the same as the least
squares solution of Qj+1 7Hjz = b, which in turn is the same as the least squares
solution of 7Hjz = QT
j+1b. But QT
j+1b = ∥b∥2 e1 (because the ﬁrst column in
Qj+1 is b/ ∥b∥2), so the GMRES algorithm is as follows.
•
To compute the solution to a nonsingular linear system An×nx = b ̸= 0,
start with q1 = b/ ∥b∥2 , and iterate as indicated below.
For j = 1 to n
execute the jth Arnoldi step in (7.11.12)
compute the least squares solution of 7Hjz = ∥b∥2 e1 by using a QR
factorization of 7Hj (see Note at the end of the example)
If ∥b −AQjz∥2 = 0 (or is satisfactorily small)
set x = Qjz, and quit (see Note at the end of the example)
End If
End For
The structure of the 7Hj ’s allows us to update the QR factors of 7Hj to produce
the QR factors of 7Hj+1 with a single plane rotation (p. 333). To see how this
is done, consider what happens when moving from the third step to the fourth
step of the process. Let U3 =
 QT
vT

be the 4 × 4 orthogonal matrix that was
previously accumulated (as a product of plane rotations) to give U3 7H3 =
 R3
0

with R3 being upper triangular so that 7H3 = QR3. Since

656
Chapter 7
Eigenvalues and Eigenvectors

U3
0
0
1

7H4 =

U3
0
0
1





⋆
⋆
7H3
⋆
⋆
0
0
0
⋆



=




⋆
⋆
U3 7H3
⋆
⋆
0
0
0
⋆



=



⋆
⋆
⋆
⋆
0
⋆
⋆
⋆
0
0
⋆
⋆
0
0
0
⋆
0
0
0
⋆


,
a plane rotation of the form P45 =


1
1
1
c
s
−s c

will annihilate the entry in
the lower-right-hand corner of this last array. Consequently, U4 = P45
 U3
0
0
1

is an orthogonal matrix such that U4 7H4 =
 R4
0

, where R4 is upper triangu-
lar, and this produces the QR factors of 7H4.
Note: The value of the residual norm ∥b −AQjz∥2 at each step of GMRES is
available at almost no cost. To see why, notice that the previous discussion shows
that at the jth step there is a (j + 1) × (j + 1) orthogonal matrix U =
 QT
vT

(that exists as an accumulation of plane rotations) such that U 7Hj =
 R
0

,
and this produces 7Hj = QR. The least squares solution of 7Hjz = ∥b∥2 e1 is
obtained by solving Rz = QT ∥b∥2 e1 (p. 314), so
∥b −AQjz∥2 =
)))∥b∥2 e1 −7Hjz
)))
2 =
)))∥b∥2 Ue1 −
 R
0

z
)))
2
=
)))∥b∥2
 QT
vT

e1 −
 R
0

z
)))
2 =
)))

0
∥b∥2 vT e1
)))
2
= ∥b∥2 |uj+1,1|.
Since uj+1,1 is just the last entry in the accumulation of the various plane
rotations applied to e1, the cost of producing these values as the algorithm
proceeds is small, so deciding on the acceptability of an approximate solution at
each step in the GMRES algorithm is cheap.
When solving nonsingular symmetric systems Ax = b, a strategy similar
to the one that produced the GMRES algorithm can be adopted except that
the Lanczos procedure (p. 651) is used in place of the Arnoldi process (p. 653).
When this is done, the resulting algorithm is called MINRES (an acronym for
minimal residual algorithm), and, as you might guess, there is an increase in
computational eﬃciency when Lanczos replaces Arnoldi. Historically, MINRES
preceded GMRES.
Another Krylov method that deserves mention is the conjugate gradient
algorithm, presented by Magnus R. Hestenes and Eduard Stiefel in 1952, that
is used to solve positive deﬁnite systems.

7.11 Minimum Polynomials and Krylov Methods
657
Example 7.11.7
Conjugate Gradient Algorithm. Suppose that An×nx = b ̸= 0 is a (real)
positive deﬁnite system, and suppose that the minimum polynomial of b with
respect to A is v(x) = xk −k−1
j=0 αjxj so that the solution x is somewhere
in the Krylov space Kk (p. 654). The conjugate gradient algorithm emanated
from the observation that if A is positive deﬁnite, then the quadratic function
f(x) = xT Ax
2
−xT b
has as its gradient
∇f(x) = Ax −b,
and there is a unique minimizer for f that happens to be the solution of Ax = b.
Consequently, any technique that attempts to minimize f is a technique that
attempts to solve Ax = b. Since the x is somewhere in Kk, it makes sense
to try to minimize f over Kk. One approach for doing this is the method of
steepest descent in which a current approximation xj is updated by adding a
correction term directed along the negative gradient −∇f(xj) = b −Axj = rj
(the jth residual). In other words, let
xj+1 = xj + αjrj,
and set
αj =
rT
j rj
rT
j Arj
because this αj minimizes f(xj+1). In spite of the fact that successive residuals
are orthogonal (rT
j+1rj = 0), the rate of convergence can be slow because as the
ratio of eigenvalues λmax(A)/λmin(A) becomes larger, the surface deﬁned by f
becomes more distorted, and a negative gradient rj need not point in a direction
aimed anywhere near the lowest point on the surface. An ingenious mechanism
for overcoming this diﬃculty is to replace the search directions rj by directions
deﬁned by vectors q1, q2, . . . that are conjugate to each other in the sense that
qT
i Aqj = 0 for all i ̸= j (some authors say “A-orthogonal”). Starting with
x0 = 0, the idea is to begin by moving in the direction of steepest descent with
x1 = α1q1,
where q1 = r0 = b and α1 =
rT
0 r0
rT
0 Ar0
,
but at the second step use a direction vector
q2 = r1 + β1q1,
where β1 is chosen to force qT
2 Aq1 = 0.
With a bit of eﬀort you can see that β1 = rT
1 r1/rT
0 r0 does the job. Then set
x2 = x1 + α2q2, and recycle the process. The formal algorithm is as follows.

658
Chapter 7
Eigenvalues and Eigenvectors
Formal Conjugate Gradient Algorithm. To compute the solution to a pos-
itive deﬁnite linear system An×nx = b, start with x0 = 0,
r0 = b, and
q1 = b, and iterate as indicated below.
For j = 1 to n
αj
←
rT
j−1rj−1/qT
j Aqj
(step size)
xj
←
xj−1 + αjqj
(approximate solution)
rj
←
rj−1 −αjAqj
(residual)
If ∥rj∥2 = 0 (or is satisfactorily small)
set x = xj, and quit
End If
βj
←
rT
j rj/rT
j−1rj−1
(conjugation factor)
qj+1
←
rj + βjqj
(search direction)
End For
It can be shown that vectors produced by this algorithm after j steps are such
that (in exact arithmetic)
span {x1, . . . , xj} = span {q1, . . . , qj} = span {r0, r1, . . . , rj−1} = Kj,
and, in addition to having qiAqj = 0 for i < j, the residuals are orthogonal—
i.e., rT
i rj = 0 for i < j. Furthermore, the algorithm will ﬁnd the solution in
k ≤n steps.
As mentioned earlier, Krylov solvers such as GMRES and the conjugate
gradient algorithm produce the solution of Ax = b in k ≤n steps (in exact
arithmetic), so, at ﬁrst glance, this looks like good news. But in practice n can
be prohibitively large, and it’s not rare to have k = n. Consequently, Krylov
algorithms are often viewed as iterative methods that are terminated long before
n steps have been completed. The challenge in applying Krylov solvers (as well as
iterative methods in general) revolves around the issue of how to replace Ax = b
with an equivalent preconditioned system M−1Ax = M−1b that requires
only a small number of iterations to deliver a reasonably accurate approximate
solution. Building eﬀective preconditioners M−1 is part science and part art,
and the techniques vary from algorithm to algorithm.
Classical linear stationary iterative methods (p. 620) are formed by splitting
A = M −N and setting x(k) = Hx(k −1) + d, where H = M−1N and
d = M−1b. This is a preconditioning technique because the eﬀect is to replace
Ax = b by M−1Ax = M−1b, where M−1A = I −H such that ρ (H) < 1.
The goal is to ﬁnd an easily inverted M (in the sense that Md = b is easily
solved) that drives the value of ρ (H) down far enough to insure a satisfactory
rate of convergence, and this is a delicate balancing act.

7.11 Minimum Polynomials and Krylov Methods
659
The goal in preconditioning Krylov solvers is somewhat diﬀerent. For ex-
ample, if k = deg v(x) is the degree of the minimum polynomial of b with
respect to A, then GMRES sorts through Kk to ﬁnd the solution of Ax = b
in k steps. So the aim of preconditioning GMRES might be to manipulate the
interplay between M−1b and M−1A to insure that the degree of minimum
polynomial 7v(x) of M−1b with respect to M−1A is signiﬁcantly smaller than
k. Since this is diﬃcult to do, an alternate goal is to try to reduce the degree
of the minimum polynomial 7m(x) for M−1A because driving down deg 7m(x)
also drives down deg 7v(x)—remember, 7v(x) is a divisor of 7m(x) (p. 647). If a
preconditioner M−1 can be found to force M−1A to be diagonalizable with
only a few distinct eigenvalues (say j of them), then deg 7m(x) = j (p. 645),
and GMRES will ﬁnd the solution in no more than j steps. But this too is an
overly ambitious goal for practical problems. In reality this objective is compro-
mised by looking for a preconditioner such that M−1A is diagonalizable whose
eigenvalues fall into a few small clusters—say j of them. The hope is that if
M−1A is diagonalizable, and if the diameters of the clusters are small enough,
then M−1A will behave numerically like a diagonalizable matrix with j distinct
eigenvalues, so GMRES is inclined to produce reasonably accurate approxima-
tions in no more than j steps. While the intuition is simple, subtleties involving
the magnitudes of eigenvalues, separation of clusters, and the meaning of “small
diameter” complicate the picture to make deﬁnitive statements and rigorous ar-
guments diﬃcult to formulate. Constructing good preconditioners and proving
they actually work as advertised remains an active area of research in the ﬁeld
of numerical analysis.
Only the tip of the iceberg concerning practical applications of Krylov meth-
ods is revealed in this section. The analysis required to more fully understand the
numerical behavior of various Krylov methods can be found in several excellent
advanced texts specializing in matrix computations.
Exercises for section 7.11
7.11.1. Determine the minimum polynomial for A =

5
1
2
−4
0
−2
−4
−1
−1

.
7.11.2. Find the minimum polynomial of b = (−1, 1, 1)T with respect to the
matrix A given in Exercise 7.11.1.
7.11.3. Use Krylov’s method to determine the characteristic polynomial for the
matrix A given in Exercise 7.11.1.
7.11.4. What is the Jordan form for a matrix whose minimum polynomial
is m(x) = (x −λ)(x −µ)2 and whose characteristic polynomial is
c(x) = (x −λ)2(x −µ)4?

660
Chapter 7
Eigenvalues and Eigenvectors
7.11.5. Use the technique described in Example 7.11.1 (p. 643) to determine the
minimum polynomial for A =


−7
−4
8
−8
−4
−1
4
−4
−16
−8
17
−16
−6
−3
6
−5

.
7.11.6. Explain why similar matrices have the same minimum and characteristic
polynomials.
7.11.7. Show that two matrices can have the same minimum and characteristic
polynomials without being similar by considering A =
 N
0
0
N

and
B =
 N
0
0
0

, where N =
 0
1
0
0

.
7.11.8. Prove that if A and B are nonderogatory matrices that have the same
characteristic polynomial, then A is similar to B.
7.11.9. Use the Lanczos algorithm to ﬁnd an orthogonal matrix P such that
PT AP = T is tridiagonal, where A =
 2
1
1
1
2
1
1
1
2

.
7.11.10. Starting with x0 = 0, apply the conjugate gradient algorithm to solve
Ax = b, where A =
 2
1
1
1
2
1
1
1
2

and b =
 4
0
0

.
7.11.11. Use Arnoldi’s algorithm to ﬁnd an orthogonal matrix Q such that
QT AQ = H is upper Hessenberg, where A =

5
1
2
−4
0
−2
−4
−1
−1

.
7.11.12. Use GMRES to solve Ax = b for A =

5
1
2
−4
0
−2
−4
−1
−1

and b =
 1
2
1

.

CHAPTER 8
Perron–Frobenius
Theory of
Nonnegative Matrices
8.1
INTRODUCTION
A ∈ℜm×n is said to be a nonnegative matrix whenever each aij ≥0, and
this is denoted by writing A ≥0. In general, A ≥B means that each aij ≥bij.
Similarly, A is a positive matrix when each aij > 0, and this is denoted by
writing A > 0. More generally, A > B means that each aij > bij.
Applications abound with nonnegative and positive matrices. In fact, many
of the applications considered in this text involve nonnegative matrices. For
example, the connectivity matrix C in Example 3.5.2 (p. 100) is nonnegative.
The discrete Laplacian L from Example 7.6.2 (p. 563) leads to a nonnegative
matrix because (4I −L) ≥0. The matrix eAt that deﬁnes the solution of
the system of diﬀerential equations in the mixing problem of Example 7.9.7
(p. 610) is nonnegative for all t ≥0. And the system of diﬀerence equations
p(k) = Ap(k −1) resulting from the shell game of Example 7.10.8 (p. 635) has
a nonnegative coeﬃcient matrix A.
Since nonnegative matrices are pervasive, it’s natural to investigate their
properties, and that’s the purpose of this chapter. A primary issue concerns
the extent to which the properties A > 0 or A ≥0 translate to spectral
properties—e.g., to what extent does A have positive (or nonnegative) eigen-
values and eigenvectors?
The topic is called the “Perron–Frobenius theory” because it evolved from
the contributions of the German mathematicians Oskar (or Oscar) Perron
89 and
89
Oskar Perron (1880–1975) originally set out to fulﬁll his father’s wishes to be in the family busi-

662
Chapter 8
Perron–Frobenius Theory of Nonnegative Matrices
Ferdinand Georg Frobenius.
90 Perron published his treatment of positive matri-
ces in 1907, and in 1912 Frobenius contributed substantial extensions of Perron’s
results to cover the case of nonnegative matrices.
In addition to saying something useful, the Perron–Frobenius theory is ele-
gant. It is a testament to the fact that beautiful mathematics eventually tends
to be useful, and useful mathematics eventually tends to be beautiful.
ness, so he only studied mathematics in his spare time. But he was eventually captured by the
subject, and, after studying at Berlin, T¨ubingen, and G¨ottingen, he completed his doctorate,
writing on geometry, at the University of Munich under the direction of Carl von Lindemann
(1852–1939) (who ﬁrst proved that π was transcendental). Upon graduation in 1906, Perron
held positions at Munich, T¨ubingen, and Heidelberg. Perron’s career was interrupted in 1915
by World War I in which he earned the Iron Cross. After the war he resumed work at Hei-
delberg, but in 1922 he returned to Munich to accept a chair in mathematics, a position he
occupied for the rest of his career. In addition to his contributions to matrix theory, Perron’s
work covered a wide range of other topics in algebra, analysis, diﬀerential equations, continued
fractions, geometry, and number theory. He was a man of extraordinary mental and physical
energy. In addition to being able to climb mountains until he was in his midseventies, Perron
continued to teach at Munich until he was 80 (although he formally retired at age 71), and he
maintained a remarkably energetic research program into his nineties. He published 18 of his
218 papers after he was 84.
90
Ferdinand Georg Frobenius (1849–1917) earned his doctorate under the supervision of Karl
Weierstrass (p. 589) at the University of Berlin in 1870. As mentioned earlier, Frobenius was
a mentor to and a collaborator with Issai Schur (p. 123), and, in addition to their joint work
in group theory, they were among the ﬁrst to study matrix theory as a discipline unto itself.
Frobenius in particular must be considered along with Cayley and Sylvester when thinking
of core developers of matrix theory. However, in the beginning, Frobenius’s motivation came
from Kronecker (p. 597) and Weierstrass, and he seemed oblivious to Cayley’s work (p. 80).
It was not until 1896 that Frobenius became aware of Cayley’s 1857 work, A Memoir on
the Theory of Matrices, and only then did the terminology “matrix” appear in Frobenius’s
work. Even though Frobenius was the ﬁrst to give a rigorous proof of the Cayley–Hamilton
theorem (p. 509), he generously attributed it to Cayley in spite of the fact that Cayley had
only discussed the result for 2 × 2 and 3 × 3 matrices. But credit in this regard is not overly
missed because Frobenius’s extension of Perron’s results are more substantial, and they alone
may keep Frobenius’s name alive forever.

8.2 Positive Matrices
663
8.2
POSITIVE MATRICES
The purpose of this section is to focus on matrices An×n > 0 with positive en-
tries, and the aim is to investigate the extent to which this positivity is inherited
by the eigenvalues and eigenvectors of A.
There are a few elementary observations that will help along the way, so
let’s begin with them. First, notice that
A > 0
=⇒
ρ (A) > 0
(8.2.1)
because if σ (A) = {0}, then the Jordan form for A, and hence A itself, is
nilpotent, which is impossible when each aij > 0. This means that our discus-
sions can be limited to positive matrices having spectral radius 1 because A
can always be normalized by its spectral radius—i.e., A > 0 ⇐⇒A/ρ (A) > 0,
and ρ (A) = r ⇐⇒ρ(A/r) = 1. Other easily veriﬁed observations are
P > 0, x ≥0, x ̸= 0
=⇒Px > 0,
(8.2.2)
N ≥0, u ≥v ≥0
=⇒Nu ≥Nv,
(8.2.3)
N ≥0, z > 0, Nz = 0
=⇒N = 0,
(8.2.4)
N ≥0, N ̸= 0, u > v > 0 =⇒Nu > Nv.
(8.2.5)
In all that follows, the bar notation | ⋆| is used to denote a matrix of
absolute values—i.e., |M| is the matrix having entries |mij|. The bar notation
will never denote a determinant in the sequel. Finally, notice that as a simple
consequence of the triangle inequality, it’s always true that |Ax| ≤|A| |x|.
Positive Eigenpair
If An×n > 0, then the following statements are true.
•
ρ (A) ∈σ (A) .
(8.2.6)
•
If Ax = ρ (A) x, then A|x| = ρ (A) |x| and |x| > 0.
(8.2.7)
In other words, A has an eigenpair of the form (ρ (A) , v) with v > 0.
Proof.
As mentioned earlier, it can be assumed that ρ (A) = 1 without any
loss of generality. If (λ, x) is any eigenpair for A such that |λ| = 1, then
|x| = |λ| |x| = |λx| = |Ax| ≤|A| |x| = A |x|
=⇒
|x| ≤A |x|.
(8.2.8)
The goal is to show that equality holds. For convenience, let z = A |x| and
y = z −|x|, and notice that (8.2.8) implies y ≥0. Suppose that y ̸= 0—i.e.,

664
Chapter 8
Perron–Frobenius Theory of Nonnegative Matrices
suppose that some yi > 0. In this case, it follows from (8.2.2) that Ay > 0 and
z > 0, so there must exist a number ϵ > 0 such that Ay > ϵ z or, equivalently,
A
1 + ϵz > z.
Writing this inequality as Bz > z, where B = A/(1 + ϵ), and successively
multiplying both sides by B while using (8.2.5) produces
B2z > Bz > z,
B3z > B2z > z,
. . .
=⇒
Bkz > z
for all k = 1, 2, . . . .
But limk→∞Bk = 0 because ρ (B) = σ

A/(1 + ϵ)

= 1/(1 + ϵ) < 1 (recall
(7.10.5) on p. 617), so, in the limit, we have 0 > z, which contradicts the fact
that z > 0. Since the supposition that y ̸= 0 led to this contradiction, the
supposition must be false and, consequently, 0 = y = A |x| −|x|. Thus |x| is
an eigenvector for A associated with the eigenvalue 1 = ρ (A) . The proof is
completed by observing that |x| = A |x| = z > 0.
Now that it’s been established that ρ (A) > 0 is in fact an eigenvalue for
A > 0, the next step is to investigate the index of this special eigenvalue.
Index of ρ (A)
If An×n > 0, then the following statements are true.
•
ρ (A) is the only eigenvalue of A on the spectral circle.
•
index (ρ (A)) = 1. In other words, ρ (A) is a semisimple eigenvalue.
Recall Exercise 7.8.4 (p. 596).
Proof.
Again, assume without loss of generality that ρ (A) = 1. We know from
(8.2.7) on p. 663 that if (λ, x) is an eigenpair for A such that |λ| = 1, then
0 < |x| = A |x|, so 0 < |xk| =

A |x|

k = n
j=1 akj|xj|. But it’s also true that
|xk| = |λ| |xk| = |(λx)k| = |(Ax)k| =
 n
j=1 akjxj
,
and thus


j
akjxj
 =

j
akj|xj| =

j
|akjxj|.
(8.2.9)
For nonzero vectors {z1, . . . , zn} ⊂Cn, it’s a fact that ∥
j zj∥2 = 
j ∥zj∥2
(equality in the triangle inequality) if and only if each zj = αjz1 for some
αj > 0 (Exercise 5.1.10, p. 277). In particular, this holds for scalars, so (8.2.9)
insures the existence of numbers αj > 0 such that
akjxj = αj(ak1x1)
or, equivalently,
xj = πjx1
with πj = αjak1
akj
> 0.

8.2 Positive Matrices
665
In other words, if |λ| = 1, then x = x1p, where p = (1, π2, . . . , πn)T > 0, so
λx = Ax
=⇒
λp = Ap = |Ap| = |λp| = |λ|p = p
=⇒
λ = 1,
and thus 1 is the only eigenvalue of A on the spectral circle. Now suppose that
index (1) = m > 1. It follows that
Ak
∞→∞as k →∞because there is
an m × m Jordan block J⋆in the Jordan form J = P−1AP that looks like
(7.10.30) on p. 629, so
Jk
⋆

∞→∞, which in turn means that
Jk
∞→∞
and, consequently,
Jk
∞=
P−1AkP

∞≤
P−1
∞
Ak
∞∥P∥∞implies
Ak
∞≥
Jk
∞
∥P−1∥∞∥P∥∞
→∞.
Let Ak =

a(k)
ij
	
, and let ik denote the row index for which
Ak
∞= 
j a(k)
ikj.
We know that there exists a vector p > 0 such that p = Ap, so for such an
eigenvector,
∥p∥∞≥pik =

j
a(k)
ikjpj ≥

 
j
a(k)
ikj

(min
i
pi) =
Ak
∞(min
i
pi) →∞.
But this is impossible because p is a constant vector, so the supposition that
index (1) > 1 must be false, and thus index (1) = 1.
Establishing that ρ (A) is a semisimple eigenvalue of A > 0 was just a
steppingstone (but an important one) to get to the following theorem concerning
the multiplicities of ρ (A) .
Multiplicities of ρ (A)
If An×n > 0, then alg multA (ρ (A)) = 1. In other words, the spectral
radius of A is a simple eigenvalue of A.
So dim N (A −ρ (A) I) = geo multA (ρ (A)) = alg multA (ρ (A)) = 1.
Proof.
As before, assume without loss of generality that ρ (A) = 1, and sup-
pose that alg multA (λ = 1) = m > 1. We already know that λ = 1 is a
semisimple eigenvalue, which means that alg multA (1) = geo multA (1) (p. 510),
so there are m linearly independent eigenvectors associated with λ = 1. If x
and y are a pair of independent eigenvectors associated with λ = 1, then
x ̸= αy for all α ∈C. Select a nonzero component from y, say yi ̸= 0,
and set z = x −(xi/yi)y. Since Az = z, we know from (8.2.7) on p. 663
that A|z| = |z| > 0. But this contradicts the fact that zi = xi −(xi/yi)yi = 0.
Therefore, the supposition that m > 1 must be false, and thus m = 1.

666
Chapter 8
Perron–Frobenius Theory of Nonnegative Matrices
Since N (A −ρ (A) I) is a one-dimensional space that can be spanned by
some v > 0, there is a unique eigenvector p ∈N (A −ρ (A) I) such that p > 0
and 
j pj = 1 (it’s obtained by the normalization p = v/ ∥v∥1—see Exercise
8.2.3). This special eigenvector p is called the Perron vector for A > 0, and
the associated eigenvalue r = ρ (A) is called the Perron root of A.
Since A > 0 ⇐⇒AT > 0, and since ρ(A) = ρ(AT ), it’s clear that
if A > 0, then in addition to the Perron eigenpair (r, p) for A there is a
corresponding Perron eigenpair (r, q) for AT . Because qT A = rqT , the vector
qT > 0 is called the left-hand Perron vector for A.
While eigenvalues of A > 0 other than ρ (A) may or may not be positive,
it turns out that no eigenvectors other than positive multiples of the Perron
vector can be positive—or even nonnegative.
No Other Positive Eigenvectors
There are no nonnegative eigenvectors for An×n > 0 other than the
Perron vector p and its positive multiples.
(8.2.10)
Proof.
If (λ, y) is an eigenpair for A such that y ≥0, and if x > 0 is the
Perron vector for AT , then xT y > 0 by (8.2.2), so
ρ (A) xT = xT A
=⇒
ρ (A) xT y = xT Ay = λxT y
=⇒
ρ (A) = λ.
In 1942 the German mathematician Lothar Collatz (1910–1990) discovered
the following formula for the Perron root, and in 1950 Helmut Wielandt (p. 534)
used it to develop the Perron–Frobenius theory.
Collatz–Wielandt Formula
The Perron root of An×n > 0 is given by r = maxx∈N f(x), where
f(x) =
min
1≤i≤n
xi̸=0
[Ax]i
xi
and
N = {x | x ≥0 with x ̸= 0}.
Proof.
If ξ = f(x) for x ∈N, then 0 ≤ξx ≤Ax. Let p and qT be the
respective the right-hand and left-hand Perron vectors for A associated with
the Perron root r, and use (8.2.3) along with qT x > 0 (by (8.2.2)) to write
ξx ≤Ax
=⇒
ξqT x ≤qT Ax = rqT x
=⇒
ξ ≤r
=⇒
f(x) ≤r ∀x ∈N.
Since f(p) = r and p ∈N, it follows that r = maxx∈N f(x).
Below is a summary of the results obtained in this section.

8.2 Positive Matrices
667
Perron’s Theorem
If An×n > 0 with r = ρ (A) , then the following statements are true.
•
r > 0.
(8.2.11)
•
r ∈σ (A)
(r is called the Perron root).
(8.2.12)
•
alg multA (r) = 1.
(8.2.13)
•
There exists an eigenvector x > 0 such that Ax = rx.
(8.2.14)
•
The Perron vector is the unique vector deﬁned by
Ap = rp,
p > 0,
and
∥p∥1 = 1,
and, except for positive multiples of p, there are no other nonneg-
ative eigenvectors for A, regardless of the eigenvalue.
•
r is the only eigenvalue on the spectral circle of A.
(8.2.15)
•
r = maxx∈N f(x) (the Collatz–Wielandt formula),
where f(x) =
min
1≤i≤n
xi̸=0
[Ax]i
xi
and N = {x | x ≥0 with x ̸= 0}.
Note: Our development is the reverse of that of Wielandt and others in the
sense that we ﬁrst proved the existence of the Perron eigenpair (r, p) without
reference to f(x) , and then we used the Perron eigenpair to established the
Collatz-Wielandt formula. Wielandt’s approach is to do things the other way
around—ﬁrst prove that f(x) attains a maximum value on N, and then es-
tablish existence of the Perron eigenpair by proving that maxx∈N f(x) = ρ(A)
with the maximum value being attained at a positive eigenvector p.
Exercises for section 8.2
8.2.1. Verify Perron’s theorem by by computing the eigenvalues and eigenvec-
tors for
A =


7
2
3
1
8
3
1
2
9

.
Find the right-hand Perron vector p as well as the left-hand Perron
vector qT .

668
Chapter 8
Perron–Frobenius Theory of Nonnegative Matrices
8.2.2. Convince yourself that (8.2.2)–(8.2.5) are indeed true.
8.2.3. Provide the details that explain why the Perron vector is uniquely de-
ﬁned.
8.2.4. Find the Perron root and the Perron vector for
A =

1 −α
β
α
1 −β

,
where α + β = 1 with α, β > 0.
8.2.5. Suppose that An×n > 0 has ρ(A) = r.
(a)
Explain why limk→∞(A/r)k exists.
(b)
Explain why limk→∞(A/r)k = G > 0 is the projector onto
N(A −rI) along R(A −rI).
(c)
Explain why rank (G) = 1.
8.2.6. Prove that if every row (or column) sum of An×n > 0 is equal to ρ,
then ρ (A) = ρ.
8.2.7. Prove that if An×n > 0, then
min
i
n

j=1
aij ≤ρ (A) ≤max
i
n

j=1
aij.
Hint: Recall Example 7.10.2 (p. 619).
8.2.8. To show the extent to which the hypothesis of positivity cannot be re-
laxed in Perron’s theorem, construct examples of square matrices A
such that A ≥0, but A ̸> 0 (i.e., A has at least one zero entry),
with r = ρ (A) ∈σ (A) that demonstrate the validity of the following
statements. Diﬀerent examples may be used for the diﬀerent statements.
(a)
r can be 0.
(b)
alg multA (r) can be greater than 1.
(c)
index (r) can be greater than 1.
(d)
N(A −rI) need not contain a positive eigenvector.
(e)
r need not be the only eigenvalue on the spectral circle.

8.2 Positive Matrices
669
8.2.9. Establish the min-max version of the Collatz–Wielandt formula that
says the Perron root for A > 0 is given by r = minx∈P g(x), where
g(x) = max
1≤i≤n
[Ax]i
xi
and
P = {x | x > 0}.
8.2.10. Notice that N = {x | x ≥0 with x ̸= 0} is used in the max-min version
of the Collatz–Wielandt formula on p. 666, but P = {x | x > 0} is used
in the min-max version in Exercise 8.2.9. Give an example of a matrix
A > 0 that shows r ̸= minx∈N g(x) when g(x) is deﬁned as
g(x) = max
1≤i≤n
xi̸=0
[Ax]i
xi
.

670
Chapter 8
Perron–Frobenius Theory of Nonnegative Matrices
8.3
NONNEGATIVE MATRICES
Now let zeros creep into the picture and investigate the extent to which Perron’s
results generalize to nonnegative matrices containing at least one zero entry. The
ﬁrst result along these lines shows how to extend the statements on p. 663 to
nonnegative matrices by sacriﬁcing the existence of a positive eigenvector for a
nonnegative one.
Nonnegative Eigenpair
For An×n ≥0 with r = ρ (A) , the following statements are true.
•
r ∈σ (A) , (but r = 0 is possible).
(8.3.1)
•
Az = rz for some z ∈N = {x | x ≥0 with x ̸= 0}.
(8.3.2)
•
r = maxx∈N f(x), where f(x) =
min
1≤i≤n
xi̸=0
[Ax]i
xi
(8.3.3)
(i.e., the Collatz–Wielandt formula remains valid).
Proof.
Consider the sequence of positive matrices Ak = A + (1/k)E > 0,
where E is the matrix of all 1 ’s, and let rk > 0 and pk > 0 denote the
Perron root and Perron vector for Ak, respectively. Observe that {pk}∞
k=1 is
a bounded set because it’s contained in the unit 1-sphere in ℜn. The Bolzano–
Weierstrass theorem states that each bounded sequence in ℜn has a convergent
subsequence. Therefore, {pk}∞
k=1 has convergent subsequence
{pki}∞
i=1 →z, where z ≥0 with z ̸= 0 (because pki > 0 and ∥pki∥1 = 1).
Since A1 > A2 > · · · > A, the result in Example 7.10.2 (p. 619) guarantees
that r1 ≥r2 ≥· · · ≥r, so {rk}∞
k=1 is a monotonic sequence of positive numbers
that is bounded below by r. A standard result from analysis guarantees that
lim
k→∞rk = r⋆exists, and r⋆≥r.
In particular,
lim
i→∞rki = r⋆≥r.
But limk→∞Ak = A implies limi→∞Aki →A, so, by using the easily estab-
lished fact that the limit of a product is the product of the limits (provided that
all limits exist), it’s also true that
Az = lim
i→∞Akipki = lim
i→∞rkipki = r⋆z
=⇒
r⋆∈σ (A)
=⇒
r⋆≤r.
Consequently, r⋆= r, and Az = rz with z ≥0 and z ̸= 0. Thus (8.3.1) and
(8.3.2) are proven. To prove (8.3.3), let qT
k > 0 be the left-hand Perron vector
of Ak. For every x ∈N and k > 0 we have qT
k x > 0 (by (8.2.2)), and
0 ≤f(x)x ≤Ax ≤Akx
=⇒
f(x)qT
k x ≤qT
k Akx = rkqT
k x
=⇒
f(x) ≤rk
=⇒
f(x) ≤r
(because rk →r∗=r).
Since f(z) = r and z ∈N, it follows that maxx∈N f(x) = r.

8.3 Nonnegative Matrices
671
This is as far as Perron’s theorem can be generalized to nonnegative matrices
without additional hypothesis. For example, A =
 0
1
0
0

shows that properties
(8.2.11), (8.2.13), and (8.2.14) on p. 667 do not hold for general nonnegative ma-
trices, and A =
 0
1
1
0

shows that (8.2.15) is also lost. Rather than accepting
that the major issues concerning spectral properties of nonnegative matrices had
been settled, Frobenius had the insight to look below the surface and see that
the problem doesn’t stem just from the existence of zero entries, but rather from
the positions of the zero entries. For example, (8.2.13) and (8.2.14) are false for
A =

1
0
1
1

, but they are true for A =

1
1
1
0

.
(8.3.4)
Frobenius’s genius was to see the diﬀerence between A and A in terms of re-
ducibility and to relate these ideas to spectral properties of nonnegative matrices.
Reducibility and graphs were discussed in Example 4.4.6 (p. 202) and Exercise
4.4.20 (p. 209), but for the sake of continuity they are reviewed below.
Reducibility and Graphs
•
An×n is said to be a reducible matrix when there exists a permu-
tation matrix P such that
PT AP =

X
Y
0
Z

,
where X and Z are both square.
Otherwise A is said to be an irreducible matrix.
•
PT AP is called a symmetric permutation of A. The eﬀect is to
interchange rows in the same way as columns are interchanged.
•
The graph G(A) of A is deﬁned to be the directed graph on n
nodes {N1, N2, . . . , Nn} in which there is a directed edge leading
from Ni to Nj if and only if aij ̸= 0.
•
G(PT AP) = G(A) whenever P is a permutation matrix—the eﬀect
is simply a relabeling of nodes.
•
G(A) is called strongly connected if for each pair of nodes (Ni, Nk)
there is a sequence of directed edges leading from Ni to Nk.
•
A is an irreducible matrix if and only if G(A) is strongly connected
(see Exercise 4.4.20 on p. 209).

672
Chapter 8
Perron–Frobenius Theory of Nonnegative Matrices
For example, the matrix A in (8.3.4) is reducible because
PT AP =

1
1
0
1

for
P =

0
1
1
0

,
and, as can be seen from Figure 8.3.1, G(A) is not strongly connected because
there is no sequence of paths leading from node 1 to node 2. On the other
hand, A is irreducible, and as shown in Figure 8.3.1, G( A) is strongly connected
because each node is accessible from the other.
1
2
1
2
G(A)
G( A)
Figure 8.3.1
This discussion suggests that some of Perron’s properties given on p. 667
extend to nonnegative matrices when the zeros are in just the right positions to
insure irreducibility. To prove that this is in fact the case, the following lemma is
needed. It shows how to convert a nonnegative irreducible matrix into a positive
matrix in a useful fashion.
Converting Nonnegativity & Irreducibility to Positivity
If An×n ≥0 is irreducible, then (I + A)n−1 > 0.
(8.3.5)
Proof.
Let a(k)
ij
denote the (i, j)-entry in Ak, and observe that
a(k)
ij =

h1,...,hk−1
aih1ah1h2 · · · ahk−1j > 0
if and only if there exists a set of indicies h1, h2, . . . , hk−1 such that
aih1 > 0
and
ah1h2 > 0
and
· · ·
and
ahk−1j > 0.
In other words, there is a sequence of k paths Ni →Nh1 →Nh2 →· · · →Nj
in G(A) that lead from node Ni to node Nj if and only if a(k)
ij
> 0. The
irreducibility of A insures that G(A) is strongly connected, so for any pair of
nodes (Ni, Nj) there is a sequence of k paths (with k < n) from Ni to Nj.
This means that for each position (i, j), there is some 0 ≤k ≤n −1 such that
a(k)
ij > 0, and this guarantees that for each i and j,

(I + A)n−1
ij =
n−1

k=0
n −1
k

Ak

ij
=
n−1

k=0
n −1
k

a(k)
ij > 0.

8.3 Nonnegative Matrices
673
With the exception of the Collatz–Wielandt formula, we have seen that
ρ (A) ∈σ (A) is the only property in the list of Perron properties on p. 667
that extends to nonnegative matrices without additional hypothesis. The next
theorem shows how adding irreducibility to nonnegativity recovers the Perron
properties (8.2.11), (8.2.13), and (8.2.14).
Perron–Frobenius Theorem
If An×n ≥0 is irreducible, then each of the following is true.
•
r = ρ (A) ∈σ (A) and r > 0.
(8.3.6)
•
alg multA (r) = 1.
(8.3.7)
•
There exists an eigenvector x > 0 such that Ax = rx.
(8.3.8)
•
The unique vector deﬁned by
Ap = rp,
p > 0,
and
∥p∥1 = 1,
(8.3.9)
is called the Perron vector. There are no nonnegative eigenvectors
for A except for positive multiples of p, regardless of the eigenvalue.
•
The Collatz–Wielandt formula r = maxx∈N f(x),
where f(x) =
min
1≤i≤n
xi̸=0
[Ax]i
xi
and N = {x | x ≥0 with x ̸= 0}
was established in (8.3.3) for all nonnegative matrices, but it is in-
cluded here for the sake of completeness.
Proof.
We already know from (8.3.2) that r = ρ (A) ∈σ (A) . To prove that
alg multA (r) = 1, let B = (I + A)n−1 > 0 be the matrix in (8.3.5). It fol-
lows from (7.9.3) that λ ∈σ (A) if and only if (1 + λ)n−1 ∈σ (B) , and
alg multA (λ) = alg multB

(1 + λ)n−1
. Consequently, if µ = ρ (B) , then
µ = max
λ∈σ(A) |(1 + λ)|n−1 =

max
λ∈σ(A) |(1 + λ)|
n−1
= (1 + r)n−1
because when a circular disk |z| ≤ρ is translated one unit to the right, the point
of maximum modulus in the resulting disk |z + 1| ≤ρ is z = 1 + ρ (it’s clear if
you draw a picture). Therefore, alg multA (r) = 1; otherwise alg multB (µ) > 1,
which is impossible because B > 0. To see that A has a positive eigenvector

674
Chapter 8
Perron–Frobenius Theory of Nonnegative Matrices
associated with r, recall from (8.3.2) that there exists a nonnegative eigenvector
x ≥0 associated with r. It’s a simple consequence of (7.9.9) that if (λ, x) is an
eigenpair for A, then (f(λ), x) is an eigenpair for f(A) (Exercise 7.9.9, p. 613),
so (r, x) being an eigenpair for A implies that (µ, x) is an eigenpair for B.
Hence (8.2.10) insures that x must be a positive multiple of the Perron vector
of B, and thus x must in fact be positive. Now, r > 0; otherwise Ax = 0,
which is impossible because A ≥0 and x > 0 forces Ax > 0. The argument
used to prove (8.2.10) also proves (8.3.9).
Example 8.3.1
Problem: Suppose that An×n ≥0 is irreducible with r = ρ (A) , and suppose
that rz ≤Az for z ≥0. Explain why rz = Az, and z > 0.
Solution: If rz < Az, then by using the Perron vector q > 0 for AT we have
(A −rI)z ≥0
=⇒
qT (A −rI)z > 0,
which is impossible since qT (A −rI) = 0. Thus rz = Az, and since z must
be a multiple of the Perron vector for A by (8.3.9), we also have that z > 0.
The only property in the list on p. 667 that irreducibility is not able to
salvage is (8.2.15), which states that there is only one eigenvalue on the spectral
circle. Indeed, A =
 0
1
1
0

is nonnegative and irreducible, but the eigenvalues
±1 are both on the unit circle. The property of having (or not having) only
one eigenvalue on the spectral circle divides the set of nonnegative irreducible
matrices into two important classes.
Primitive Matrices
•
A nonnegative irreducible matrix A having only one eigenvalue,
r = ρ (A) , on its spectral circle is said to be a primitive matrix.
•
A nonnegative irreducible matrix having h > 1 eigenvalues on its
spectral circle is called imprimitive, and h is referred to as index
of imprimitivity.
•
A nonnegative irreducible matrix A with r = ρ (A) is primitive if
and only if limk→∞(A/r)k exists, in which case
lim
k→∞
A
r
k
= G = pqT
qT p > 0,
(8.3.10)
where p and q are the respective Perron vectors for A and AT .
G is the (spectral) projector onto N(A −rI) along R(A −rI).

8.3 Nonnegative Matrices
675
Proof of (8.3.10).
The Perron–Frobenius theorem insures that 1 = ρ(A/r) is a
simple eigenvalue for A/r, and it’s clear that A is primitive if and only if A/r
is primitive. In other words, A is primitive if and only if 1 = ρ(A/r) is the only
eigenvalue on the unit circle, which is equivalent to saying that limk→∞(A/r)k
exists by the results on p. 630. The structure of the limit as described in (8.3.10)
is the result of (7.2.12) on p. 518.
The next two results, discovered by Helmut Wielandt (p. 534) in 1950,
establish the remarkable fact that the eigenvalues on the spectral circle of an
imprimitive matrix are in fact the hth roots of the spectral radius.
Wielandt’s Theorem
If |B| ≤An×n, where A is irreducible, then ρ (B) ≤ρ (A) . If equality
holds (i.e., if µ = ρ (A) eiφ ∈σ (B) for some φ), then
B = eiφDAD−1
for some
D =




eiθ1
eiθ2
...
eiθn



,
(8.3.11)
and conversely.
Proof.
We already know that ρ (B) ≤ρ (A) by Example 7.10.2 (p. 619). If
ρ (B) = r = ρ (A) , and if (µ, x) is an eigenpair for B such that |µ| = r, then
r|x| = |µ| |x| = |µx| = |Bx| ≤|B| |x| ≤A|x|
=⇒
|B| |x| = r|x|
because the result in Example 8.3.1 insures that A|x| = r|x|, and |x| > 0.
Consequently, (A −|B|)|x| = 0. But A −|B| ≥0, and |x| > 0, so A = |B|
by (8.2.4). Since xk/|xk| is on the unit circle, xk/|xk| = eiθk for some θk. Set
D =




eiθ1
eiθ2
...
eiθn



, and notice that x = D|x|.
Since |µ| = r, there is a φ ∈ℜsuch that µ = reiφ, and hence
BD|x|=Bx=µx=reiφx=reiφD|x| ⇒e−iφD−1BD|x|=r|x|=A|x|. (8.3.12)
For convenience, let C = e−iφD−1BD, and note that |C| = |B| = A to write
(8.3.12) as 0 = (|C| −C)|x|. Considering only the real part of this equation

676
Chapter 8
Perron–Frobenius Theory of Nonnegative Matrices
yields 0 =

|C| −Re (C)

|x|. But |C| ≥Re (C) , and |x| > 0, so it follows
from (8.2.4) that Re (C) = |C|, and hence
Re (cij) = |cij| =

Re (cij) 2 + Im (cij) 2
=⇒Im (cij) = 0 =⇒Im (C) = 0.
Therefore, C = Re (C) = |C| = A, which implies B = eiφDAD−1. Conversely,
if B = eiφDAD−1, then similarity insures that ρ (B) = ρ

eiφA

= ρ (A) .
hth Roots of ρ (A) on Spectral Circle
If An×n ≥0 is irreducible and has h eigenvalues {λ1, λ2, . . . , λh} on
its spectral circle, then each of the following statements is true.
•
alg multA (λk) = 1 for k = 1, 2, . . . , h.
(8.3.13)
•
{λ1, λ2, . . . , λh} are the hth roots of r = ρ (A) given by
{r, rω, rω2, . . . , rωh−1},
where
ω = e2πi/h.
(8.3.14)
Proof.
Let S = {r, reiθ1, . . . , reiθh−1} denote the eigenvalues on the spectral
circle of A. Applying (8.3.11) with B = A and µ = reiθk insures the existence
of a diagonal matrix Dk such that A = eiθkDkAD−1
k , thus showing that eiθkA
is similar to A. Since r is a simple eigenvalue of A (by the Perron–Frobenius
theorem), reiθk must be a simple eigenvalue of eiθkA. But similarity transfor-
mations preserve eigenvalues and algebraic multiplicities (because the Jordan
structure is preserved), so reiθk must be a simple eigenvalue of A, thus estab-
lishing (8.3.13). To prove (8.3.14), consider another eigenvalue reiθs ∈S. Again,
we can write A = eiθsDsAD−1
s
for some Ds, so
A = eiθkDkAD−1
k
= eiθkDk(eiθsDsAD−1
s )D−1
k
= ei(θk+θr)(DkDs)A(DkDs)−1
and, consequently, rei(θk+θr) is also an eigenvalue on the spectral circle of A.
In other words, S = {r, reiθ1, . . . , reiθh−1} is closed under multiplication. This
means that G = {1, eiθ1, . . . , eiθh−1} is closed under multiplication, and it follows
that G is a ﬁnite commutative group of order h. A standard result from algebra
states that the hth power of every element in a ﬁnite group of order h must be
the identity element in the group. Therefore, (eiθk)h = 1 for each 0 ≤k ≤h−1,
so G is the set of the hth roots of unity e2πki/h ( 0 ≤k ≤h −1), and thus S
must be the hth roots of r.
Combining the preceding results reveals just how special the spectrum of an
imprimitive matrix is.

8.3 Nonnegative Matrices
677
Rotational Invariance
If A is imprimitive with h eigenvalues on its spectral circle, then σ (A)
is invariant under rotation about the origin through an angle 2π/h. No
rotation less than 2π/h can preserve σ (A) .
(8.3.15)
Proof.
Since λ ∈σ (A) ⇐⇒λe2πi/h ∈σ(e2πi/hA), it follows that σ(e2πi/hA)
is σ (A) rotated through 2π/h. But (8.3.11) and (8.3.14) insure that A and
e2πi/hA are similar and, consequently, σ (A) = σ(e2πi/hA). No rotation less
than 2π/h can keep σ (A) invariant because (8.3.14) makes it clear that the
eigenvalues on the spectral circle won’t go back into themselves for rotations less
than 2π/h.
Example 8.3.2
The Spectral Projector Is Positive. We already know from (8.3.10) that
if A is a primitive matrix, and if G is the spectral projector associated with
r = ρ (A) , then G > 0.
Problem: Explain why this is also true for an imprimitive matrix. In other
words, establish the fact that if G is the spectral projector associated with
r = ρ (A) for any nonnegative irreducible matrix A, then G > 0.
Solution: Being imprimitive means that A is nonnegative and irreducible with
more than one eigenvalue on the spectral circle. However, (8.3.13) says that
each eigenvalue on the spectral circle is simple, so the results concerning Ces`aro
summability on p. 633 can be applied to A/r to conclude that
lim
k→∞
I + (A/r) + · · · + (A/r)k−1
k
= G,
where G is the spectral projector onto N((A/r) −I) = N(A −rI) along
R((A/r) −I) = R(A −rI). Since r is a simple eigenvalue the same argument
used to establish (8.3.10) (namely, invoking (7.2.12) on p. 518) shows that
G = pqT
qT p > 0,
where p and q are the respective Perron vectors for A and AT .
Trying to determine if an irreducible matrix A ≥0 is primitive or imprim-
itive by ﬁnding the eigenvalues is generally a diﬃcult task, so it’s natural to ask
if there’s another way. It turns out that there is, and, as the following example
shows, determining primitivity can sometimes be trivial.

678
Chapter 8
Perron–Frobenius Theory of Nonnegative Matrices
Example 8.3.3
Suﬃcient Condition for Primitivity. If a nonnegative irreducible matrix A
has at least one positive diagonal element, then A is primitive.
Proof.
Suppose there are h > 1 eigenvalues on the spectral circle. We know
from (8.3.15) that if λ0 ∈σ (A) , then λk = λ0e2πik/h ∈σ (A) for k =
0, 1, . . . , h −1, so
h−1

k=0
λk = λ0
h−1

k=0
e2πik/h = 0
(roots of unity sum to 1—see p. 357).
This implies that the sum of all of the eigenvalues is zero. In other words,
•
if A is imprimitive, then trace (A) = 0.
(Recall (7.1.7) on p. 494.)
Therefore, if A has a positive diagonal entry, then A must be primitive.
Another of Frobenius’s contributions was to show how the powers of a non-
negative matrix determine whether or not the matrix is primitive. The exact
statement is as follows.
Frobenius’s Test for Primitivity
A ≥0 is primitive if and only if Am > 0 for some m > 0.
(8.3.16)
Proof.
First assume that Am > 0 for some m. This implies that A is irre-
ducible; otherwise there exists a permutation matrix such that
A = P

X
Y
0
Z

PT
=⇒
Am = P

Xm
⋆
0
Zm

PT
has zero entries.
Suppose that A has h eigenvalues {λ1, λ2, . . . , λh} on its spectral circle so
that r = ρ (A) = |λ1| = · · · = |λh| > |λh+1| > · · · > |λn|. Since λ ∈σ (A)
implies λm ∈σ(Am) with alg multA (λ) = alg multAm (λm) (consider the Jor-
dan form—Exercise 7.9.9 on p. 613), it follows that λm
k (1 ≤k ≤h) is on the
spectral circle of Am with alg multA (λk) = alg multAm (λm
k ) . Perron’s theo-
rem (p. 667) insures that Am has only one eigenvalue (which must be rm) on
its spectral circle, so rm = λm
1 = λm
2 = · · · = λm
h . But this means that
alg multA (r) = alg multAm (rm) = h,
and therefore h = 1 by (8.3.7). Conversely, if A is primitive with r = ρ (A) ,
then limk→∞(A/r)k > 0 by (8.3.10). Hence there must be some m such that
(A/r)m > 0, and thus Am > 0.

8.3 Nonnegative Matrices
679
Example 8.3.4
Suppose that we wish to decide whether or not a nonnegative matrix A is
primitive by computing the sequence of powers A, A2, A3, . . . . Since this can be
a laborious task, it would be nice to know when we have computed enough powers
of A to render a judgement. Unfortunately there is nothing in the statement or
proof of Frobenius’s test to help us with this decision. But Wielandt provided
an answer by proving that a nonnegative matrix An×n is primitive if and only
if An2−2n+2 > 0. Furthermore, n2 −2n + 2 is the smallest such exponent
that works for the class of n × n primitive matrices having all zeros on the
diagonal—see Exercise 8.3.9.
Problem: Determine whether or not A =
 0
1
0
0
0
2
3
4
0

is primitive.
Solution: Since A has zeros on the diagonal, the result in Example 8.3.3 doesn’t
apply, so we are forced into computing powers of A. This job is simpliﬁed by
noticing that if B = β(A) is the Boolean matrix that results from setting
bij =
 1
if aij > 0,
0
if aij = 0,
then [Bk]ij > 0 if and only if [Ak]ij > 0 for every k > 0. This means that
instead of using A, A2, A3, . . . to decide on primitivity, we need only compute
B1 = β(A),
B2 = β(B1B1),
B3 = β(B1B2),
B4 = β(B1B3), . . . ,
going no further than Bn2−2n+2, and these computations require only Boolean
operations AND and OR. The matrix A in this example is primitive because
B1 =

 0
1
0
0
0
1
1
1
0

, B2 =

 0
0
1
1
1
0
0
1
1

, B3 =

 1
1
0
0
1
1
1
1
1

, B4 =

 0
1
1
1
1
1
1
1
1

, B5 =

 1
1
1
1
1
1
1
1
1

.
The powers of an irreducible matrix A ≥0 can tell us if A has more
than one eigenvalue on its spectral circle, but the powers of A provide no clue
to the number of such eigenvalues. The next theorem shows how the index of
imprimitivity can be determined without explicitly calculating the eigenvalues.
Index of Imprimitivity
If c(x) = xn + ck1xn−k1 + ck2xn−k2 + · · · + cksxn−ks = 0 is the char-
acteristic equation of an imprimitive matrix An×n in which only the
terms with nonzero coeﬃcients are listed (i.e., each ckj ̸= 0, and
n > (n −k1) > · · · > (n −ks)), then the index of imprimitivity h
is the greatest common divisor of {k1, k2, . . . , ks}.

680
Chapter 8
Perron–Frobenius Theory of Nonnegative Matrices
Proof.
We know from (8.3.15) that if {λ1, λ2, . . . , λn} are the eigenvalues of
A (including multiplicities), then {ωλ1, ωλ2, . . . , ωλn} are also the eigenvalues
of A, where ω = e2πi/h. It follows from the results on p. 494 that
ckj = (−1)kj 
1≤i1<···<ikj ≤n
λi1 · · · λikj = (−1)kj 
1≤i1<···<ikj ≤n
ωλi1 · · · ωλikj = ωkjckj =⇒ωkj = 1.
Therefore, h must divide each kj. If d divides each kj with d > h, then
γ−kj = 1 for γ = e2πi/d. Hence γλ ∈σ (A) for each λ ∈σ (A) because
c(γλ) = 0. But this means that σ (A) is invariant under a rotation through
an angle (2π/d) < (2π/h), which, by (8.3.15), is impossible.
Example 8.3.5
Problem: Find the index of imprimitivity of A =


0
1
0
0
2
0
1
0
0
1
0
2
0
0
1
0

.
Solution: Using the principal minors to compute the characteristic equation as
illustrated in Example 7.1.2 (p. 496) produces the characteristic equation
c(x) = x4 −5x2 + 4 = 0,
so that k1 = 2 and k2 = 4. Since gcd{2, 4} = 2, it follows that h = 2. The
characteristic equation is relatively simple in this example, so the eigenvalues
can be explicitly determined to be {±2, ±1}. This corroborates the fact that
h = 2. Notice also that this illustrates the property that σ (A) is invariant
under rotation through an angle 2π/h = π.
More is known about nonnegative matrices than what has been presented
here—in fact, there are entire books on the subject. But before moving on to
applications, there is a result that Frobenius discovered in 1912 that is worth
mentioning because it completely reveals the structure of an imprimitive matrix.
Frobenius Form
For each imprimitive matrix A with index of imprimitivity h > 1,
there exists a permutation matrix P such that
PT AP=




0
A12
0
· · ·
0
0
0
A23
· · ·
0
...
...
...
...
...
0
0
· · ·
0
Ah−1,h
Ah1
0
· · ·
0
0



,
where the zero blocks on the main diagonal are square.

8.3 Nonnegative Matrices
681
Example 8.3.6
Leontief’s
91 Input–Output Economic Model. Suppose that n major indus-
tries in a closed economic system each make one commodity, and let a J-unit be
what industry J produces that sells for $1. For example, the Boeing Company
makes airplanes, and the Champion Company makes rivets, so a BOEING-unit
is only a tiny fraction of an airplane, but a CHAMPION-unit might be several
rivets. If
0 ≤sj = # J-units produced by industry J each year, and if
0 ≤aij = # I-units needed to produce one J-unit,
then
aijsj = # I-units consumed by industry J each year, and
n

j=1
aijsj = # I-units consumed by all industries each year,
so
di = si −
n

j=1
aijsj = # I-units available to the public (nonindustry) each year.
Consider d = (d1, d2, . . . , dn)T to be the public demand vector, and think of
s = (s1, s2, . . . , sn)T as the industrial supply vector.
Problem: Determine the supply s ≥0 that is required to satisfy a given
demand d ≥0.
Solution: At ﬁrst glance the problem seems to be trivial because the equations
di = si −n
j=1 aijsj translate to (I−A)s = d, so if I−A is nonsingular, then
s = (I−A)−1d. The catch is that this solution may have negative components in
spite of the fact that A ≥0. So something must be added. It’s not unreasonable
to assume that major industries are strongly connected in the sense that the
commodity of each industry is either directly or indirectly needed to produce
all commodities in the system. In other words, it’s reasonable to assume that
91
Wassily Leontief (1906–1999) was the 1973 Nobel Laureate in Economics. He was born in St.
Petersburg (now Leningrad), where his father was a professor of economics. After receiving his
undergraduate degree in economics at the University of Leningrad in 1925, Leontief went to
the University of Berlin to earn a Ph.D. degree. He migrated to New York in 1931 and moved
to Harvard University in 1932, where he became Professor of Economics in 1946. Leontief spent
a signiﬁcant portion of his career developing and applying his input–output analysis, which
eventually led to the famous “Leontief paradox.” In the U.S. economy of the 1950s, labor was
considered to be scarce while capital was presumed to be abundant, so the prevailing thought
was that U.S. foreign trade was predicated on trading capital-intensive goods for labor-intensive
goods. But Leontief’s input–output tables revealed that just the opposite was true, and this
contributed to his fame. One of Leontief’s secret weapons was the computer. He made use
of large-scale computing techniques (relative to the technology of the 1940s and 1950s), and
he was among the ﬁrst to put the Mark I (one of the ﬁrst electronic computers) to work on
nonmilitary projects in 1943.

682
Chapter 8
Perron–Frobenius Theory of Nonnegative Matrices
G(A) is a strongly connected graph so that in addition to being nonnegative,
A is an irreducible matrix. Furthermore, it’s not unreasonable to assume that
ρ (A) < 1. To understand why, notice that the jth column sum of A is
cj =
n

i=1
aij = total number of all units required to make one J-unit
= total number of dollars spent by J to create $1 of revenue.
In a healthy economy all major industries should have cj ≤1, and there should
be at least one major industry such that cj < 1. This means that there exists a
matrix E ≥0, but E ̸= 0, such that each column sum of A + E is 1, so
eT (A + E) = eT ,
where
eT is the row of all 1 ’s.
This forces ρ (A) < 1; otherwise the Perron vector p > 0 for A can be used
to write
1 = eT p = eT (A + E)p = 1 + eT Ep > 1
because
E ≥0, E ̸= 0, p > 0
=⇒
Ep > 0.
(Conditions weaker than the column-sum condition can also force ρ (A) < 1—see
Example 7.10.3 on p. 620.) The assumption that A is a nonnegative irreducible
matrix whose spectral radius is ρ (A) < 1 combined with the Neumann series
(p. 618) provides the conclusion that
(I −A)−1 =
∞

k=0
Ak > 0.
Positivity is guaranteed by the irreducibility of A because the same argu-
ment given on p. 672 that is to prove (8.3.5) also applies here. Therefore, for
each demand vector d ≥0, there exists a unique supply vector given by
s = (I −A)−1d, which is necessarily positive. The fact that (I −A)−1 > 0
and s > 0 leads to the interesting conclusion that an increase in public demand
by just one unit from a single industry will force an increase in the output of all
industries.
Note: The matrix I −A is an M-matrix as deﬁned and discussed in Example
7.10.7 (p. 626). The realization that M-matrices are naturally present in economic
models provided some of the motivation for studying M-matrices during the ﬁrst
half of the twentieth century. Some of the M-matrix properties listed on p. 626
were independently discovered and formulated in economic terms.

8.3 Nonnegative Matrices
683
Example 8.3.7
Leslie Population Age Distribution Model. Divide a population of females
into age groups G1, G2, . . . , Gn, where each group covers the same number of
years. For example,
G1 = all females under age 10,
G2 = all females from age 10 up to 20,
G1 = all females from age 20 up to 30,
...
Consider discrete points in time, say t = 0, 1, 2, . . . years, and let bk and sk
denote the birth rate and survival rate for females in Gk. That is, let
bk = Expected number of daughters produced by a female in Gk,
sk = Proportion of females in Gk at time t that are in Gk+1 at time t + 1.
If
fk(t) = Number of females in Gk at time t,
then it follows that
f1(t + 1) = f1(t)b1 + f2(t)b2 + · · · + fn(t)bn
and
(8.3.17)
fk(t + 1) = fk−1(t)sk−1
for k = 2, 3, . . . , n.
Furthermore,
Fk(t) =
fk(t)
f1(t) + f2(t) + · · · + fn(t) = % of population in Gk at time t.
The vector F(t) = (F1(t), F2(t), . . . , Fn(t))T represents the population age dis-
tribution at time t, and, provided that it exists, F⋆= limt→∞F(t) is the
long-run (or steady-state) age distribution.
Problem: Assuming that s1, . . . , sn and b2, . . . , bn are positive, explain why
the population age distribution approaches a steady state, and then describe it.
In other words, show that F⋆= limt→∞F(t) exists, and determine its value.
Solution: The equations in (8.3.17) constitute a system of homogeneous diﬀer-
ence equations that can be written in matrix form as
f(t + 1) = Lf(t),
where
L =






b1
b2
· · ·
bn−1
bn
s1
0
· · ·
· · ·
0
0
s2
0
0
...
...
...
...
0
0
· · ·
sn
0






n×n
.
(8.3.18)

684
Chapter 8
Perron–Frobenius Theory of Nonnegative Matrices
The matrix L is called the Leslie matrix in honor of P. H. Leslie who used this
model in 1945. Notice that in addition to being nonnegative, L is also irreducible
when s1, . . . , sn and b2, . . . , bn are positive because the graph G(L) is strongly
connected. Moreover, L is primitive. This is obvious if in addition to s1, . . . , sn
and b2, . . . , bn being positive we have b1 > 0 (recall Example 8.3.3 on p. 678).
But even if b1 = 0, L is still primitive because Ln+2 > 0 (recall (8.3.16) on
p. 678). The technique on p. 679 also can be used to show primitivity (Exercise
8.3.11). Consequently, (8.3.10) on p. 674 guarantees that
lim
t→∞
L
r
t
= G = pqT
qT p > 0,
where p > 0 and q > 0 are the respective Perron vectors for L and LT . If we
combine this with the fact that the solution to the system of diﬀerence equations
in (8.3.18) is f(t) = Ltf(0) (p. 617), and if we assume that f(0) ̸= 0, then we
arrive at the conclusion that
lim
t→∞
f(t)
rt
= Gf(0) = p

qT f(0)
qT p

and
lim
t→∞

f(t)
rt

1
= qT f(0)
qT p
> 0
(8.3.19)
(because ∥⋆∥1 is a continuous function—Exercise 5.1.7 on p. 277). Now
Fk(t) =
fk(t)
∥f(t)∥1
= % of population that is in Gk at time t
is the quantity of interest, and (8.3.19) allows us to conclude that
F⋆= lim
t→∞F(t) = lim
t→∞
f(t)
∥f(t)∥1
= lim
t→∞
f(t)/rt
∥f(t)∥1 /rt
=
limt→∞f(t)/rt
limt→∞∥f(t)∥1 /rt = p
(the Perron vector!).
In other words, while the numbers in the various age groups may increase or
decrease, depending on the value of r (Exercise 8.3.10), the proportion of in-
dividuals in each age group becomes stable as time increases. And because the
steady-state age distribution is given by the Perron vector of L, each age group
must eventually contain a positive fraction of the population.
Exercises for section 8.3
8.3.1. Let A =
 0
1
0
3
0
3
0
2
0

.
(a)
Show that A is irreducible.
(b)
Find the Perron root and Perron vector for A.
(c)
Find the number of eigenvalues on the spectral circle of A.

8.3 Nonnegative Matrices
685
8.3.2. Suppose that the index of imprimitivity of a 5 × 5 nonnegative irre-
ducible matrix A is h = 3. Explain why A must be singular with
alg multA (0) = 2.
8.3.3. Suppose that A is a nonnegative matrix that possesses a positive spec-
tral radius and a corresponding positive eigenvector. Does this force A
to be irreducible?
8.3.4. Without computing the eigenvalues or the characteristic polynomial,
explain why σ (Pn) = {1, ω, ω2, . . . , ωn−1}, where ω = e2πi/n for
Pn=




0
1
0
· · ·
0
0
0
1
· · ·
0
...
...
...
...
...
0
0
· · ·
0
1
1
0
0
· · ·
0



.
8.3.5. Determine whether A =


0
1
2
0
0
0
0
0
7
0
2
0
0
0
0
0
9
2
0
4
0
0
0
1
0

is reducible or irreducible.
8.3.6. Determine whether the matrix A in Exercise 8.3.5 is primitive or im-
primitive.
8.3.7. A matrix Sn×n ≥0 having row sums less than or equal to 1 with at
least one row sum less than 1 is called a substochastic matrix.
(a)
Explain why ρ (S) ≤1 for every substochastic matrix.
(b)
Prove that ρ (S) < 1 for every irreducible substochastic matrix.
8.3.8. A nonnegative matrix for which each row sum is 1 is called a stochastic
matrix (some say row-stochastic). Prove that if An×n is nonnegative
and irreducible with r = ρ (A) , then A is similar to rP for some ir-
reducible stochastic matrix P. Hint: Consider D=



p1
0
· · ·
0
0
p2
· · ·
0
...
...
...
...
0
0
· · ·
pn


,
where the pk ’s are the components of the Perron vector for A.
8.3.9. Wielandt constructed the matrix Wn=




0
1
0
· · ·
0
0
0
1
· · ·
0
...
...
...
...
...
0
0
· · ·
0
1
1
1
0
· · ·
0



to show
that Wn2−2n+2 > 0, but [Wn2−2n+1]11 = 0. Verify that this is true
for n = 4.

686
Chapter 8
Perron–Frobenius Theory of Nonnegative Matrices
8.3.10. In the Leslie population model on p. 683, explain what happens to the
vector f(t) as t →∞depending on whether r < 1, r = 1, or r > 1.
8.3.11. Use the characteristic equation as described on p. 679 to show that the
Leslie matrix in (8.3.18) is primitive even if b1 = 0 (assuming all other
bk ’s and sk ’s are positive).
8.3.12. A matrix A ∈ℜn×n is said to be essentially positive if A is irre-
ducible and aij ≥0 for every i ̸= j. Prove that each of the following
statements is equivalent to saying that A is essentially positive.
(a)
There exists some α ∈ℜsuch that A + αI is primitive.
(b)
etA > 0 for all t > 0.
8.3.13. Let A be an essentially positive matrix as deﬁned in Exercise 8.3.12.
Prove that each of the following statements is true.
(a)
A has an eigenpair (ξ, x), where ξ is real and x > 0.
(b)
If λ is any eigenvalue for A other than ξ, then Re (λ) < ξ.
(c)
ξ increases when any entry in A is increased.
8.3.14. Let A ≥0 be an irreducible matrix, and let a(k)
ij
denote entries in Ak.
Prove that A is primitive if and only if
ρ (A) = lim
k→∞

a(k)
ij
1/k
.

8.4 Stochastic Matrices and Markov Chains
687
8.4
STOCHASTIC MATRICES AND MARKOV CHAINS
One of the most elegant applications of the Perron–Frobenius theory is the al-
gebraic development of the theory of ﬁnite Markov chains. The purpose of this
section is to present some of the aspects of this development.
A stochastic matrix is a nonnegative matrix Pn×n in which each row
sum is equal to 1. Some authors say “row-stochastic” to distinguish this from
the case when each column sum is 1.
A Markov
92chain is a stochastic process (a set of random variables {Xt}∞
t=0
in which Xt has the same range {S1, S2, . . . , Sn}, called the state space) that
satisﬁes the Markov property
P(Xt+1 = Sj | Xt = Sit, Xt−1 = Sit−1, . . . , X0 = Si0) = P(Xt+1 = Sj | Xt = Sit)
for each t = 0, 1, 2, . . . . Think of a Markov chain as a random chain of events
that occur at discrete points t = 0, 1, 2, . . . in time, where Xt represents the
state of the event that occurs at time t. For example, if a mouse moves randomly
through a maze consisting of chambers S1, S2, . . . , Sn, then Xt might represent
the chamber occupied by the mouse at time t. The Markov property asserts that
the process is memoryless in the sense that the state of the chain at the next
time period depends only on the current state and not on the past history of the
chain. In other words, the mouse moving through the maze obeys the Markov
property if its next move doesn’t depend on where in the maze it has been in
the past—i.e., the mouse is not using its memory (if it has one).
To emphasize that time is considered discretely rather than continuously the
phrase “discrete-time Markov chain” is often used, and the phrase “ﬁnite-state
Markov chain” might be used to emphasize that the state space is ﬁnite rather
than inﬁnite.
92
Andrei Andreyevich Markov (1856–1922) was born in Ryazan, Russia, and he graduated from
Saint Petersburg University in 1878 where he later became a professor. Markov’s early interest
was number theory because this was the area of his famous teacher Pafnuty Lvovich Chebyshev
(1821–1894). But when Markov discovered that he could apply his knowledge of continued frac-
tions to probability theory, he embarked on a new course that would make him famous—enough
so that there was a lunar crater named in his honor in 1964. In addition to being involved with
liberal political movements (he once refused to be decorated by the Russian Czar), Markov
enjoyed poetry, and in his spare time he studied poetic style. Therefore, it was no accident
that led him to analyze the distribution of vowels and consonants in Pushkin’s work, Eugene
Onegin, by constructing a simple model based on the assumption that the probability that a
consonant occurs at a given position in any word should depend only on whether the preceding
letter is a vowel or a consonant and not on any prior history. This was the birth of the “Markov
chain.” Markov was wrong in one regard—he apparently believed that the only real examples
of his chains were to be found in literary texts. But Markov’s work in 1907 has grown to be-
come an indispensable tool of enormous power. It launched the theory of stochastic processes
that is now the foundation for understanding, explaining, and predicting phenomena in diverse
areas such as atomic physics, quantum theory, biology, genetics, social behavior, economics,
and ﬁnance. Markov’s chains serve to underscore the point that the long-term applicability of
mathematical research is impossible to predict.

688
Chapter 8
Perron–Frobenius Theory of Nonnegative Matrices
Every Markov chain deﬁnes a stochastic matrix, and conversely. Let’s see
how this happens. The value pij(t) = P(Xt = Sj | Xt−1 = Si) is the probability
of being in state Sj at time t given that the chain is in state Si at time t −1,
so pij(t) is called the transition probability of moving from Si to Sj at
time t. The matrix of transition probabilities Pn×n(t) = [pij(t)] is clearly a
nonnegative matrix, and a little thought should convince you that each row sum
must be 1. Thus P(t) is a stochastic matrix. When the transition probabilities
don’t vary with time (say pij(t) = pij for all t), the chain is said to be stationary
(or homogeneous), and the transition matrix is the constant stochastic matrix
P = [pij]. We will make the assumption of stationarity throughout. Conversely,
every stochastic matrix Pn×n deﬁnes an n -state Markov chain because the
entries pij deﬁne a set of transition probabilities, which can be interpreted as a
stationary Markov chain on n states.
A probability distribution vector is deﬁned to be a nonnegative vector
pT = (p1, p2, . . . , pn) such that 
k pk = 1. (Every row in a stochastic ma-
trix is such a vector.) For an n -state Markov chain, the kth step probability
distribution vector is deﬁned to be
pT (k) =

p1(k), p2(k), . . . , pn(k)

,
k = 1, 2, . . . ,
where pj(k) = P(Xk = Sj).
In other words, pj(k) is the probability of being in the jth state after the kth
step, but before the (k + 1)st step. The initial distribution vector is
pT (0) =

p1(0), p2(0), . . . , pn(0)

,
where
pj(0) = P(X0 = Sj)
is the probability that the chain starts in Sj.
For example, consider the Markov chain deﬁned by placing a mouse in the
3-chamber box with connecting doors as shown in Figure 8.4.1, and suppose that
the mouse moves from the chamber it occupies to another chamber by picking a
door at random—say that the doors open each minute, and when they do, the
mouse is forced to move by electrifying the ﬂoor of the occupied chamber.
#1
#2
#3
Figure 8.4.1
If the mouse is initially placed in chamber #2, then the initial distribution vector
is pT (0) = (0, 1, 0) = eT
2 . But if the process is started by tossing the mouse into
the air so that it randomly lands in one of the chambers, then a reasonable

8.4 Stochastic Matrices and Markov Chains
689
initial distribution is pT (0) = (.5, .25, .25) because the area of chamber #1 is
50% of the box, while chambers #2 and #3 each constitute 25% of the box. The
transition matrix for this Markov chain is the stochastic matrix
M =


0
1/2
1/2
1/3
0
2/3
1/3
2/3
0

.
(8.4.1)
A standard eigenvalue calculation reveals that σ (M) = {1, −1/3, /, −2/3}, so
it’s apparent that M is a nonnegative matrix having spectral radius ρ (M) = 1.
This is a feature that is shared by all stochastic matrices Pn×n because having
row sums equal to 1 means that ∥P∥∞= 1 or, equivalently, Pe = e, where e
is the column of all 1’s. Because (1, e) is an eigenpair for every stochastic matrix,
and because ρ (⋆) ≤∥⋆∥for every matrix norm (recall (7.1.12) on p. 497), it
follows that
1 ≤ρ (P) ≤∥P∥∞= 1
=⇒
ρ (P) = 1.
Furthermore, e is a positive eigenvector associated with ρ (P) = 1. But be
careful! This doesn’t mean that you necessarily can call e the Perron vector for
P because P might not be irreducible—consider P =
 .5
.5
0
1

.
Two important issues that arise in Markovian analysis concern the transient
behavior of the chain as well as the limiting behavior. In other words, we want
to accomplish the following goals.
•
Describe the kth step distribution pT (k) for any given initial distribution
vector pT (0).
•
Determine whether or not limk→∞pT (k) exists, and if it exists, determine
the value of limk→∞pT (k).
•
If there is no limiting distribution, then determine the possibility of having
a Ces`aro limit
lim
k→∞
pT (0) + pT (1) + · · · + pT (k −1)
k

.
If such a limit exists, interpret its meaning, and determine its value.
The kth step distribution is easily described by using the laws of elementary
probability—in particular, recall that P(E ∨F) = P(E) + P(F) when E and
F are mutually exclusive events, and the conditional probability of E occurring
given that F occurs is P(E | F) = P(E ∧F)/P(F) (it’s convenient to use ∧
and ∨to denote AND and OR, respectively). To determine the jth component

690
Chapter 8
Perron–Frobenius Theory of Nonnegative Matrices
pj(1) in pT (1) for a given pT (0), write
pj(1) = P(X1=Sj) = P

X1=Sj ∧(X0=S1 ∨X0=S2 ∨· · · ∨X0=Sn)

= P

(X1=Sj ∧X0=S1) ∨(X1=Sj ∧X0=S2) ∨· · · ∨(X1=Sj ∧X0=Sn)

=
n

i=1
P

X1=Sj ∧X0=Si

=
n

i=1
P

X0 = Si

P

X1 = Sj | X0 = Si

=
n

i=1
pi(0)pij
for
j = 1, 2, . . . , n.
Consequently, pT (1) = pT (0)P. This tells us what to expect after one step when
we start with pT (0). But the “no memory” Markov property tells us that the
state of aﬀairs at the end of two steps is determined by where we are at the end of
the ﬁrst step—it’s like starting over but with pT (1) as the initial distribution.
In other words, it follows that pT (2) = pT (1)P, and pT (3) = pT (2)P, etc.
Therefore, successive substitution yields
pT (k) = pT (k −1)P = pT (k −2)P2 = · · · = pT (0)Pk,
and thus the kth step distribution is determined from the initial distribution
and the transition matrix by the vector–matrix product
pT (k) = pT (0)Pk.
(8.4.2)
Notice that if we adopt the notation Pk =

p(k)
ij
	
, and if we set pT (0) = eT
i
in
(8.4.2), then we get pj(k) = p(k)
ij
for each i = 1, 2, . . . , n, and thus we arrive at
the following conclusion.
•
The (i, j)-entry in Pk represents the probability of moving from Si to Sj
in exactly k steps. For this reason, Pk is often called the k-step transition
matrix.
Example 8.4.1
Let’s go back to the mouse-in-the-box example, and, as suggested earlier, toss
the mouse into the air so that it randomly lands somewhere in the box in Fig-
ure 8.4.1—i.e., take the initial distribution to be pT (0) = (1/2, 1/4, 1/4). The
transition matrix is given by (8.4.1), so the probability of ﬁnding the mouse in
chamber #1 after three moves is
[pT (3)]1 = [pT (0)M3]1 = 13/54.
In fact, the entire third step distribution is pT (3) = ( 13/54, 41/108, 41/108 ) .

8.4 Stochastic Matrices and Markov Chains
691
To analyze limiting properties of Markov chains, divide the class of stochas-
tic matrices (and hence the class of stationary Markov chains) into four mutually
exclusive categories as described below.
(1)
Irreducible with limk→∞Pk existing
(i.e., P is primitive).
(2)
Irreducible with limk→∞Pk not existing
(i.e., P is imprimitive).
(3)
Reducible
with limk→∞Pk existing.
(4)
Reducible
with limk→∞Pk not existing.
In case (1), where P is primitive, we know exactly what limk→∞Pk looks
like. The Perron vector for P is e/n (the uniform distribution vector), so if
π = (π1, π2, . . . , πn)T is the Perron vector for PT , then
lim
k→∞Pk = (e/n)πT
πT (e/n) = eπT
πT e = eπT =




π1
π2
· · ·
πn
π1
π2
· · ·
πn
...
...
...
π1
π2
· · ·
πn



> 0
(8.4.3)
by (8.3.10) on p. 674. Therefore, if P is primitive, then a limiting probability
distribution exists, and it is given by
lim
k→∞pT (k) = lim
k→∞pT (0)Pk = pT (0)eπT = πT .
(8.4.4)
Notice that because 
k pk(0) = 1, the term pT (0)e drops away, so we have the
conclusion that the value of the limit is independent of the value of the initial
distribution pT (0), which isn’t too surprising.
Example 8.4.2
Going back to the mouse-in-the-box example, it’s easy to conﬁrm that the transi-
tion matrix M in (8.4.1) is primitive, so limk→∞Mk as well as limk→∞pT (0)
must exist, and their values are determined by the left-hand Perron vector of
M that can be found by calculating any nonzero vector v ∈N
	
I −MT 
and
normalizing it to produce πT = vT / ∥v∥1 . Routine computation reveals that
the one solution of the homogeneous equation (I−MT )v = 0 is vT = (2, 3, 3),
so πT = (1/8)(2, 3, 3), and thus
lim
k→∞Mk = 1
8


2
3
3
2
3
3
2
3
3


and
lim
k→∞pT (k) = 1
8(2, 3, 3).
This limiting distribution can be interpreted as meaning that in the long run the
mouse will occupy chamber #1 one-fourth of the time, while 37.5% of the time it’s
in chamber #2, and 37.5% of the time it’s in chamber #3, and this is independent
of where (or how) the process started. The mathematical justiﬁcation for this
statement is on p. 693.

692
Chapter 8
Perron–Frobenius Theory of Nonnegative Matrices
Now consider the imprimitive case. We know that if P is irreducible and has
h > 1 eigenvalues on the unit (spectral) circle, then limk→∞Pk cannot exist
(p. 674), and hence limk→∞pT (k) cannot exist (otherwise taking pT (0) = eT
i
for each i would insure that Pk has a limit). However, each eigenvalue on the
unit circle is simple (p. 676), and this means that P is Ces`aro summable (p. 633).
Moreover, e/n is the Perron vector for P, and, as pointed out in Example 8.3.2
(p. 677), if πT = (π1, π2, . . . , πn) is the left-hand Perron vector, then
lim
k→∞
I + P + · · · + Pk−1
k
= (e/n)πT
πT (e/n) = eπT
πT e = eπT =




π1
π2
· · ·
πn
π1
π2
· · ·
πn
...
...
...
π1
π2
· · ·
πn



,
which is exactly the same form as the limit (8.4.3) for the primitive case. Con-
sequently, the kth step distributions have a Ces`aro limit given by
lim
k→∞
pT (0) + pT (1) + · · · + pT (k −1)
k

= lim
k→∞pT (0)
I + P + · · · + Pk−1
k

= pT (0)eπT = πT ,
and, just as in the primitive case (8.4.4), this Ces`aro limit is independent of the
initial distribution.
Let’s interpret the meaning of this Ces`aro limit. The analysis is essentially
the same as the description outlined in the shell game in Example 7.10.8 (p. 635),
but for the sake of completeness we will duplicate some of the logic here. The
trick is to focus on one state, say Sj, and deﬁne a sequence of random variables
{Zk}∞
k=0 that count the number of visits to Sj. Let
Z0 =

1
if the chain starts in Sj,
0
otherwise,
and for i > 1,
(8.4.5)
Zi =

1
if the chain is in Sj after the ith move,
0
otherwise.
Notice that Z0 + Z1 + · · · + Zk−1 counts the number of visits to Sj before the
kth move, so (Z0 + Z1 + · · · + Zk−1)/k represents the fraction of times that Sj
is hit before the kth move. The expected (or mean) value of each Zi is
E[Zi] = 1 · P(Zi=1) + 0 · P(Zi=0) = P(Zi=1) = pj(i),
and, since expectation is linear, the expected fraction of times that Sj is hit
before move k is
E
Z0 + Z1 + · · · + Zk−1
k

= E[Z0] + E[Z1] + · · · + E[Zk−1]
k
= pj(0) + pj(1) + · · · + pj(k −1)
k
=
pT (0) + pT (1) + · · · + pT (k −1)
k

j
→πj.

8.4 Stochastic Matrices and Markov Chains
693
In other words, the long-run fraction of time that the chain spends in Sj is
πj, which is the jth component of the Ces`aro limit or, equivalently, the jth
component of the left-hand Perron vector for P.
When limk→∞pT (k) exists, it must be the case that
lim
k→∞pT (k) = lim
k→∞
pT (0)+pT (1)+· · ·+pT (k−1)
k

(Exercise 7.10.11, p. 639),
and therefore the interpretation of the limiting distribution limk→∞pT (k) for
the primitive case is exactly the same as the interpretation of the Ces`aro limit
in the imprimitive case.
Below is a summary of our ﬁndings for irreducible chains.
Irreducible Markov Chains
Let P be the transition probability matrix for an irreducible Markov
chain on states
{S1, S2, . . . , Sn}
(i.e.,
P
is an
n × n
irreducible
stochastic matrix), and let πT denote the left-hand Perron vector for P.
The following statements are true for every initial distribution pT (0).
•
The kth step transition matrix is Pk because the (i, j) -entry in
Pk is the probability of moving from Si to Sj in exactly k steps.
•
The kth step distribution vector is given by pT (k) = pT (0)Pk.
•
If P is primitive, and if e denotes the column of all 1’s, then
lim
k→∞Pk = eπT
and
lim
k→∞pT (k) = πT .
•
If P is imprimitive, then
lim
k→∞
I + P + · · · + Pk−1
k
= eπT
and
lim
k→∞
pT (0)+pT (1)+· · ·+pT (k−1)
k

= πT .
•
Regardless of whether P is primitive or imprimitive, the jth com-
ponent πj of πT represents the long-run fraction of time that the
chain is in Sj.
•
πT is often called the stationary distribution vector for the chain
because it is the unique distribution vector satisfying πT P = πT .

694
Chapter 8
Perron–Frobenius Theory of Nonnegative Matrices
Example 8.4.3
Periodic Chains. Consider an electronic switch that can be in one of three
states {S1, S2, S3}, and suppose that the switch changes states on regular clock
cycles. If the switch is in either S1 or S3, then it must change to S2 on the
next clock cycle, but if the switch is in S2, then there is an equal likelihood that
it changes to S1 or S3 on the next clock cycle. The transition matrix is
P =


0
1
0
.5
0
.5
0
1
0

,
and it’s not diﬃcult to see that P is irreducible (because G(P) is strongly con-
nected) and imprimitive (because σ (P) = {±1, 0}). Since the left-hand Perron
vector is πT = (.25, .5, .25), the long-run expectation is that the switch should
be in S1 25% of the time, in S2 50% of the time, and in S3 25% of the time,
and this agrees with what common sense tells us. Furthermore, notice that the
switch cannot be in just any position at any given clock cycle because if the
chain starts in either S1 or S3, then it must be in S2 on every odd-numbered
cycle, and it can occupy S1 or S3 only on even-numbered cycles. The situation
is similar, but with reversed parity, when the chain starts in S2. In other words,
the chain is periodic in the sense that the states can be occupied only at peri-
odic points in time. In this example the period of the chain is 2, and this is the
same as the index of imprimitivity. This is no accident. The Frobenius form for
imprimitive matrices on p. 680 can be used to prove that this is true in general.
Consequently, an irreducible Markov chain is said to be a periodic chain when
its transition matrix P is imprimitive (with the period of the chain being the
index of imprimitivity for P), and an irreducible Markov chain for which P
is primitive is called an aperiodic chain. The shell game in Example 7.10.8
(p. 635) is a periodic Markov chain that is similar to the one in this example.
Because the Perron–Frobenius theorem is not directly applicable to reducible
chains (chains for which P is a reducible matrix), the strategy for analyzing
reducible chains is to deﬂate the situation, as much as possible, back to the
irreducible case as described below.
If P is reducible, then, by deﬁnition, there exists a permutation matrix Q
and square matrices X and Z such that
QT PQ =
 X
Y
0
Z

. For convenience, denote this by writing P ∼
 X
Y
0
Z

.
If X or Z is reducible, then another symmetric permutation can be performed
to produce
 X
Y
0
Z

∼
 R
S
T
0
U
V
0
0
W

,
where R, U, and W are square.

8.4 Stochastic Matrices and Markov Chains
695
Repeating this process eventually yields
P ∼



X11
X12
· · ·
X1k
0
X22
· · ·
X2k
...
...
...
0
0
· · ·
Xkk


,
where each Xii is irreducible or Xii = [0]1×1.
Finally, if there exist rows having nonzero entries only in diagonal blocks, then
symmetrically permute all such rows to the bottom to produce
P ∼












P11
P12
· · ·
Prr
P1,r+1
P1,r+2
· · ·
P1m
0
P22
· · ·
P2r
P2,r+1
P2,r+2
· · ·
P2m
...
...
...
...
...
· · ·
...
0
0
· · ·
Prr
Pr,r+1
Pr,r+2
· · ·
Prm
0
0
· · ·
0
Pr+1,r+1
0
· · ·
0
0
0
· · ·
0
0
Pr+2,r+2
· · ·
0
...
...
· · ·
...
...
...
...
...
0
0
· · ·
0
0
0
· · ·
Pmm












,
(8.4.6)
where each P11, . . . , Prr is either irreducible or [0]1×1, and Pr+1,r+1, . . . , Pmm
are irreducible (they can’t be zero because each has row sums equal to 1). As
mentioned on p. 671, the eﬀect of a symmetric permutation is simply to relabel
nodes in G(P) or, equivalently, to reorder the states in the chain. When the
states of a chain have been reordered so that P assumes the form on the right-
hand side of (8.4.6), we say that P is in the canonical form for reducible
matrices. When P is in canonical form, the subset of states corresponding
to Pkk for 1 ≤k ≤r is called the kth transient class (because once left,
a transient class can’t be reentered), and the subset of states corresponding to
Pr+j,r+j for j ≥1 is called the jth ergodic class. Each ergodic class is an
irreducible Markov chain unto itself that is imbedded in the larger reducible
chain. From now on, we will assume that the states in our reducible chains have
been ordered so that P is in canonical form.
The results on p. 676 guarantee that if an irreducible stochastic matrix P
has h eigenvalues on the unit circle, then these h eigenvalues are the hth roots
of unity, and each is a simple eigenvalue for P. The same can’t be said for
reducible stochastic matrices, but the canonical form (8.4.6) allows us to prove
the next best thing as discussed below.

696
Chapter 8
Perron–Frobenius Theory of Nonnegative Matrices
Unit Eigenvalues
The unit eigenvalues for a stochastic matrix are deﬁned to be those
eigenvalues that are on the unit circle. For every stochastic matrix Pn×n,
the following statements are true.
•
Every unit eigenvalue of P is semisimple.
•
Every unit eigenvalue has form λ = e2kπi/h for some k < h ≤n.
•
In particular, ρ (P) = 1 is always a semisimple eigenvalue of P.
Proof.
If P is irreducible, then there is nothing to prove because, as proved on
p. 676, the unit eigenvalues are roots of unity, and each unit eigenvalue is simple.
If P is reducible, suppose that a symmetric permutation has been performed so
that P is in the canonical form (8.4.6), and observe that
ρ (Pkk) < 1
for each k = 1, 2, . . . , r.
(8.4.7)
This is certainly true when Pkk = [0]1×1, so suppose that Pkk (1 ≤k ≤r)
is irreducible. Because there must be blocks Pkj,
j ̸= k, that have nonzero
entries, it follows that
Pkke ≤e
and
Pkke ̸= e,
where e is the column of all 1’s.
If ρ (Pkk) = 1, then the observation in Example 8.3.1 (p. 674) forces Pkke = e,
which is impossible, and thus ρ (Pkk) < 1. Consequently, the unit eigenval-
ues for P are the collection of the unit eigenvalues of the irreducible matrices
Pr+1,r+1, . . . , Pmm. But each unit eigenvalue of Pr+i,r+i is simple and is a
root of unity. Consequently, if λ is a unit eigenvalue for P, then it must be
some root of unity, and although it might be repeated because it appears in
the spectrum of more than one Pr+i,r+i, it must nevertheless be the case that
alg multP (λ) = geo multP (λ) , so λ is a semisimple eigenvalue of P.
We know from the discussion on p. 633 that a matrix A ∈Cn×n is Ces`aro
summable if and only if ρ(A) < 1 or ρ(A) = 1 with each eigenvalue on the unit
circle being semisimple. We just proved that the latter holds for all stochastic
matrices P, so we have in fact established the following powerful statement
concerning all stochastic matrices.

8.4 Stochastic Matrices and Markov Chains
697
All Stochastic Matrices Are Summable
Every stochastic matrix P is Ces`aro summable. That is,
lim
k→∞
I + P + · · · + Pk−1
k
exists for all stochastic matrices P,
and, as discussed on p. 633, the value of the limit is the (spectral) pro-
jector G onto N (I −P) along R (I −P).
Since we already know the structure and interpretation of the Ces`aro limit
when P is an irreducible stochastic matrix (p. 693), all that remains in order to
complete the picture is to analyze the nature of limk→∞(I + P + · · · + Pk−1)/k
for the reducible case.
Suppose that P =
 T11
T12
0
T22

is a reducible stochastic matrix that is in
the canonical form (8.4.6), where
T11 =


P11
· · ·
Prr
...
...
Prr

,
T12 =


P1,r+1 · · · P1m
...
...
Pr,r+1 · · · Prm

,
and
T22 =


Pr+1,r+1
...
Pmm

.
(8.4.8)
We know from (8.4.7) that ρ (Pkk) < 1 for each k = 1, 2, . . . , r, so it follows
that ρ (T11) < 1, and hence
lim
k→∞
I + T11 + · · · + Tk−1
11
k
= lim
k→∞Tk
11 = 0
(recall Exercise 7.10.11 on p. 639).
Furthermore, Pr+1,r+1, . . . , Pmm are each irreducible stochastic matrices, so if
πT
j
is the left-hand Perron vector for Pjj, r + 1 ≤j ≤m, then our previous
results (p. 693) tell us that
lim
k→∞
I + T22 + · · · + Tk−1
22
k
=



eπT
r+1
...
eπT
m


= E.
(8.4.9)
Furthermore, it’s clear from the results on p. 674 that limk→∞Tk
22 exists if and
only if Pr+1,r+1, . . . , Pmm are each primitive, in which case limk→∞Tk
22 = E.

698
Chapter 8
Perron–Frobenius Theory of Nonnegative Matrices
Therefore, the limits, be they Ces`aro or ordinary (if it exists), all have the form
lim
k→∞
I + P + · · · + Pk−1
k
=

0
Z
0
E

= G = lim
k→∞Pk (when it exists).
To determine the precise nature of Z, use the fact that R (G) = N (I −P)
(because G is the projector onto N (I −P) along R (I −P)) to write
(I−P)G = 0
=⇒
 I −T11
−T12
0
I −T22
  0
Z
0
E

= 0
=⇒
(I−T11)Z = T12E.
Since I −T11 is nonsingular (because ρ (T11) < 1 by (8.4.7)), it follows that
Z = (I −T11)−1T12E,
and thus the following results concerning limits of reducible chains are produced.
Reducible Markov Chains
If the states in a reducible Markov chain have been ordered to make the
transition matrix assume the canonical form
P =

T11
T12
0
T22

that is described in (8.4.6) and (8.4.8), and if πT
j is the left-hand Perron
vector for Pjj
(r + 1 ≤j ≤m), then I −T11 is nonsingular, and
lim
k→∞
I + P + · · · + Pk−1
k
=
 0
(I −T11)−1T12E
0
E

,
where
E =


eπT
r+1
...
eπT
m

.
Furthermore, limk→∞Pk exists if and only if the stochastic matrices
Pr+1,r+1, . . . , Pmm in (8.4.6) are each primitive, in which case
lim
k→∞Pk =
 0
(I −T11)−1T12E
0
E

.
(8.4.10)

8.4 Stochastic Matrices and Markov Chains
699
The preceding analysis shows that every reducible chain eventually gets
absorbed (trapped) into one of the ergodic classes—i.e., into a subchain deﬁned
by Pr+j,r+j for some j ≥1. If Pr+j,r+j is primitive, then the chain settles
down to a steady-state deﬁned by the left-hand Perron vector of Pr+j,r+j, but
if Pr+j,r+j is imprimitive, then the process will oscillate in the jth ergodic class
forever. There is not much more that can be said about the limit, but there are
still important questions concerning which ergodic class the chain will end up in
and how long it takes to get there. This time the answer depends on where the
chain starts—i.e., on the initial distribution.
For convenience, let Ti denote the ith transient class, and let Ej be the jth
ergodic class. Suppose that the chain starts in a particular transient state—say
we start in the pth state of Ti. Since the question at hand concerns only which
ergodic class is hit but not what happens after it’s entered, we might as well
convert every state in each ergodic class into a trap by setting Pr+j,r+j = I
for each j ≥1 in (8.4.6). The transition matrix for this modiﬁed chain is
P =
 T11
T12
0
I

, and it follows from (8.4.10) that limk→∞Pk exists and has
the form
lim
k→∞
Pk =
 0
(I −T11)−1T12
0
I

=












0
0
· · ·
0
L1,1
L1,2
· · ·
L1s
0
0
· · ·
0
L2,1
L2,2
· · ·
L2s
...
...
...
...
...
· · ·
...
0
0
· · ·
0
Lr,1
Lr,2
· · ·
Lrs
0
0
· · ·
0
I
0
· · ·
0
0
0
· · ·
0
0
I
· · ·
0
...
...
· · ·
...
...
...
...
...
0
0
· · ·
0
0
0
· · ·
I












.
Consequently, the (p, q)-entry in block Lij represents the probability of even-
tually hitting the qth state in Ej given that we start from the pth state in
Ti. Therefore, if e is the vector of all 1 ’s, then the probability of eventually
entering somewhere in Ej is given by
•
P(absorption into Ej | start in pth state of Ti) = 
k

Lij
	
pk =

Lije
	
p.
If pT
i (0) is an initial distribution for starting in the various states of Ti, then
•
P

absorption into Ej | pT
i (0)

= pT
i (0)Lije.
To determine the expected number of steps required to ﬁrst hit an ergodic
state, proceed as follows. Count the number of times the chain is in transient
state Sj given that it starts in transient state Si by reapplying the argument
given in (8.4.5) on p. 692. That is, given that the chain starts in Si, let
Z0 =

1
if Si = Sj,
0
otherwise,
and
Zk =
 1
if the chain is in Sj after step k,
0
otherwise.

700
Chapter 8
Perron–Frobenius Theory of Nonnegative Matrices
Since
E[Zk] = 1 · P(Zk=1) + 0 · P(Zk=0) = P(Zk=1) =

Tk
11
	
ij,
and since ∞
k=0 Zk is the total number of times the chain is in Sj, we have
E[# times in Sj| start in Si] = E
 ∞

k=0
Zk

=
∞

k=0
E [Zk] =
∞

k=0

Tk
11
	
ij
=

(I −T11)−1	
ij
(because ρ (T11) < 1).
Summing this over all transient states produces the expected number of times
the chain is in some transient state, which is the same as the expected number
of times before ﬁrst hitting an ergodic state. In other words,
•
E[# steps until absorption | start in ith transient state] =

(I −T11)−1e
	
i.
Example 8.4.4
Absorbing Markov Chains. It’s often the case in practical applications that
there is only one transient class, and the ergodic classes are just single absorbing
states (states such that once they are entered, they are never left). If the single
transient class contains r states, and if there are s absorbing states, then the
canonical form for the transition matrix is
P =









p11
· · ·
p1r
p1,r+1
· · ·
p1s
...
...
...
...
pr1
· · ·
prr
pr,r+1
· · ·
prs
0
· · ·
0
1
· · ·
0
...
...
...
...
...
0
· · ·
0
0
· · ·
1









and
Lij =

(I −T11)−1T12
	
ij.
The preceding analysis specializes to say that every absorbing chain must even-
tually reach one of its absorbing states. The probability of being absorbed into
the jth absorbing state (which is state Sr+j) given that the chain starts in the
ith transient state (which is Si) is
P(absorption into Sr+j| start in Si for 1 ≤i ≤r) =

(I −T11)−1T12
	
ij,
while the expected time until absorption is
E[# steps until absorption | start in Si] =

(I −T11)−1e
	
i ,
and the amount of time spent in Sj is
E[# times in Sj| start in Si] =

(I −T11)−1	
ij .

8.4 Stochastic Matrices and Markov Chains
701
Example 8.4.5
Fail-Safe System. Consider a system that has two independent controls, A and
B, that can prevent the system from being destroyed. The system is activated at
discrete points in time t1, t2, t3, . . . , and the system is considered to be “under
control” if either control A or B holds at the time of activation. The system is
destroyed if A and B fail simultaneously.
▷
For example, an automobile has two independent braking systems—one is
operated by a foot pedal, whereas the “emergency brake” is operated by a
hand lever. The automobile is “under control” if at least one braking system
is operative when you try to stop, but a crash occurs if both braking systems
fail simultaneously.
If one of the controls fails at some activation point but the other control holds,
then the defective control is repaired before the next activation. If a control holds
at time t = tk, then it is considered to be 90% reliable at t = tk+1, but if a
control fails at time t = tk, then its untested replacement is considered to be
only 60% reliable at t = tk+1.
Problem: Can the system be expected to run indeﬁnitely without every being
destroyed? If not, how long is the system expected to run before destruction
occurs?
Solution: This is a four-state Markov chain with the states being the controls
that hold at any particular time of activation. In other words the state space is
the set of pairs (a, b) in which
a =

1
if A holds,
0
if A fails,
and
b =
 1
if B holds,
0
if B fails.
State (0, 0) is absorbing, and the transition matrix (in canonical form) is
P =




(1, 1)
(1, 0)
(0, 1)
(0, 0)
(1, 1)
.81
.09
.09
.01
(1, 0)
.54
.36
.06
.04
(0, 1)
.54
.06
.36
.04
(0, 0)
0
0
0
1




with
T11 =


.81
.09
.09
.54
.36
.06
.54
.06
.36


and
T12 =


.01
.04
.04

.
The fact that limk→∞Pk exists and is given by
lim
k→∞Pk =
 0
(I −T11)−1T12
0
1


702
Chapter 8
Perron–Frobenius Theory of Nonnegative Matrices
makes it clear that the absorbing state must eventually be reached. In other
words, this proves the validity of the popular belief that “if something can go
wrong, then it eventually will.” Rounding to three signiﬁcant ﬁgures produces
(I −T11)−1 =


44.6
6.92
6.92
41.5
8.02
6.59
41.5
6.59
8.02


and
(I −T11)−1e =


58.4
56.1
56.1

,
so the mean time to failure starting with two proven controls is slightly more
than 58 steps, while the mean time to failure starting with one untested control
and one proven control is just over 56 steps. The diﬀerence here doesn’t seem
signiﬁcant, but consider what happens when only one control is used in the
system. In this case, there are only two states in the chain, 1 (meaning that the
control holds) and 0 (meaning that it doesn’t). The transition matrix is
P =
 1
0
1
.9
.1
0
0
1

,
so now the mean time to failure is only (I −T11)−1e = 10 steps. It’s interesting
to consider what happens when three independent control are used. How much
more security does your intuition tell you that you should have? See Exercise
8.4.8.
Exercises for section 8.4
8.4.1. Find the stationary distribution for P =


1/4
0
0
3/4
3/8
1/4
3/8
0
1/3
1/6
1/6
1/3
0
0
1/2
1/2

. Does
this stationary distribution represent a limiting distribution in the reg-
ular sense or only in the Ces`aro sense?
8.4.2. A doubly-stochastic matrix is a nonnegative matrix Pn×n having
all row sums as well as all column sums equal to 1. For an irreducible
n -state Markov chain whose transition matrix is doubly stochastic,
what is the long-run proportion of time spent in each state? What form
do limk→∞(I + P + · · · + Pk−1)/k and limk→∞Pk (if it exists) have?
Note: The purpose of this exercise is to show that doubly-stochastic
matrices are not very interesting from a Markov-chain point of view.
However, there is an interesting theoretical result (due to G. Birkhoﬀ
in 1946) that says the set of n × n doubly-stochastic matrices forms
a convex polyhedron in ℜn×n with the permutation matrices as the
vertices.

8.4 Stochastic Matrices and Markov Chains
703
8.4.3. Explain why rank (I −P) = n −1 for every irreducible stochastic ma-
trix Pnn. Give an example to show that this need not be the case for
reducible stochastic matrices.
8.4.4. Prove that the left-hand Perron vector for an irreducible stochastic ma-
trix Pn×n (n > 1) is given by
πT =
1
n
i=1 Pi

P1, P2, . . . , Pn

,
where Pi is the ith principal minor determinant of order n−1 in I−P.
Hint: What is [adj (A)]A if A is singular?
8.4.5. Let Pn×n be an irreducible stochastic matrix, and let Qk×k be a prin-
cipal submatrix of I −P, where 1 ≤k < n. Prove that ρ (Q) < 1.
8.4.6. Let Pn×n be an irreducible stochastic matrix, and let Qk×k be a prin-
cipal submatrix of I −P, where 1 ≤k < n. Explain why Q is an
M-matrix as deﬁned and discussed on p. 626.
8.4.7. Let Pn×n (n > 1) be an irreducible stochastic matrix. Explain why all
principal minors of order 1 ≤k < n in I −P are positive.
8.4.8. Use the same assumptions that are used for the fail-safe system described
in Example 8.4.5, but use three controls, A, B, and C, instead of two.
Determine the mean time to failure starting with three proven controls,
two proven but one untested control, and three untested controls.
8.4.9. A mouse is placed in one chamber of the box shown in Figure 8.4.1 on
p. 688, and a cat is placed in another chamber. Each minute the doors to
the chambers are opened just long enough to allow movement from one
chamber to an adjacent chamber. Half of the time when the doors are
opened, the cat doesn’t leave the chamber it occupies. The same is true
for the mouse. When either the cat or mouse moves, a door is chosen at
random to pass through.
(a)
Explain why the cat and mouse must eventually end up in the
same chamber, and determine the expected number of steps for
this to occur.
(b)
Determine the probability that the cat will catch the mouse in
chamber #j for each j = 1, 2, 3.

Solutions for Chapter 1
Solutions for exercises in section 1. 2
1.2.1. (1, 0, 0)
1.2.2. (1, 2, 3)
1.2.3. (1, 0, −1)
1.2.4. (−1/2, 1/2, 0, 1)
1.2.5.


2
−4
3
4
−7
4
5
−8
4


1.2.6. Every row operation is reversible. In particular the “inverse” of any row operation
is again a row operation of the same type.
1.2.7.
π
2 , π, 0
1.2.8. The third equation in the triangularized form is 0x3 = 1, which is impossible
to solve.
1.2.9. The third equation in the triangularized form is 0x3 = 0, and all numbers are
solutions. This means that you can start the back substitution with any value
whatsoever and consequently produce inﬁnitely many solutions for the system.
1.2.10. α = −3, β = 11
2 , and γ = −3
2
1.2.11. (a) If xi = the number initially in chamber #i, then
.4x1 + 0x2 + 0x3 + .2x4 = 12
0x1 + .4x2 + .3x3 + .2x4 = 25
0x1 + .3x2 + .4x3 + .2x4 = 26
.6x1 + .3x2 + .3x3 + .4x4 = 37
and the solution is x1 = 10, x2 = 20, x3 = 30, and x4 = 40.
(b)
16, 22, 22, 40
1.2.12. To interchange rows i and j, perform the following sequence of Type II and
Type III operations.
Rj ←Rj + Ri
(replace row j by the sum of row j and i)
Ri ←Ri −Rj
(replace row i by the diﬀerence of row i and j)
Rj ←Rj + Ri
(replace row j by the sum of row j and i)
Ri ←−Ri
(replace row i by its negative)
1.2.13. (a)
This has the eﬀect of interchanging the order of the unknowns— xj and
xk are permuted.
(b)
The solution to the new system is the same as the

2
Solutions
solution to the old system except that the solution for the jth unknown of the
new system is ˆxj = 1
αxj. This has the eﬀect of “changing the units” of the jth
unknown.
(c) The solution to the new system is the same as the solution for
the old system except that the solution for the kth unknown in the new system
is ˆxk = xk −αxj.
1.2.14. hij =
1
i+j−1
1.2.16. If x =




x1
x2
...
xm




and
y =




y1
y2
...
ym



are two diﬀerent solutions, then
z = x + y
2
=





x1+y1
2
x2+y2
2
...
xm+ym
2





is a third solution diﬀerent from both x and y.
Solutions for exercises in section 1. 3
1.3.1. (1, 0, −1)
1.3.2. (2, −1, 0, 0)
1.3.3.


1
1
1
1
2
2
1
2
3


Solutions for exercises in section 1. 4
1.4.2. Use y′(tk) = y′
k ≈yk+1 −yk−1
2h
and y′′(tk) = y′′
k ≈yk−1 −2yk + yk+1
h2
to write
f(tk) = fk = y′′
k −y′
k ≈2yk−1 −4yk + 2yk+1
2h2
−hyk+1 −hyk−1
2h2
,
k = 1, 2, . . . , n,
with y0 = yn+1 = 0. These discrete approximations form the tridiagonal system






−4
2 −h
2 + h
−4
2 −h
...
...
...
2 + h
−4
2 −h
2 + h
−4












y1
y2
...
yn−1
yn






= 2h2






f1
f2
...
fn−1
fn






.

Solutions
3
Solutions for exercises in section 1. 5
1.5.1. (a)
(0, −1)
(c)
(1, −1)
(e)

1
1.001,
−1
1.001
	
1.5.2. (a)
(0, 1)
(b)
(2, 1)
(c)
(2, 1)
(d)

2
1.0001, 1.0003
1.0001
	
1.5.3. Without PP: (1.01, 1.03)
With PP: (1, 1)
Exact: (1, 1)
1.5.4. (a)


1
.500
.333
.333
.500
.333
.250
.333
.333
.250
.200
.200

−→


1
.500
.333
.333
0
.083
.083
.166
0
.083
.089
.089


−→


1
.500
.333
.333
0
.083
.083
.166
0
0
.006
−.077


z = −.077/.006 = −12.8,
y = (.166 −.083z)/.083 = 14.8,
x = .333 −(.5y + .333z) = −2.81
(b)


1
.500
.333
.333
.500
.333
.250
.333
.333
.250
.200
.200

−→


1
.500
.333
.333
1
.666
.500
.666
1
.751
.601
.601


−→


1
.500
.333
.333
0
.166
.167
.333
0
.251
.268
.268

−→


1
.500
.333
.333
0
.251
.268
.268
0
.166
.167
.333


−→


1
.500
.333
.333
0
.251
.268
.268
0
0
−.01
.156


z = −.156/.01 = −15.6,
y = (.268 −.268z)/.251 = 17.7,
x = .333 −(.5y + .333z) = −3.33
(c)


1
.500
.333
.333
.500
.333
.250
.333
.333
.250
.200
.200

−→


1
.500
.333
.333
1
.666
.500
.666
1
.751
.601
.601


−→


1
.500
.333
.333
0
.166
.167
.333
0
.251
.268
.268

−→


1
.500
.333
.333
0
.994
1
1.99
0
.937
1
1


−→


1
.500
.333
.333
0
.994
1
1.99
0
0
.057
−.880


z = −.88/.057 = −15.4,
y = (1.99 −z)/.994 = 17.5,
x = .333 −(.5y + .333z) = −3.29
(d)
x = −3,
y = 16,
z = −14
1.5.5. (a)
.0055x + .095y + 960z = 5000
.0011x + . 01y + 112z = 600
.0093x + .025y + 560z = 3000

4
Solutions
(b)
3-digit solution = (55, 900 lbs. silica, 8, 600 lbs. iron, 4.04 lbs. gold).
Exact solution (to 10 digits) = (56, 753.68899, 8, 626.560726, 4.029511918). The
relative error (rounded to 3 digits) is er = 1.49 × 10−2.
(c)
Let u = x/2000, v = y/1000, and w = 12z to obtain the system
11u + 95v +
80w = 5000
2.2u + 10v + 9.33w = 600
18.6u + 25v + 46.7w = 3000.
(d)
3-digit solution = (28.5 tons silica, 8.85 half-tons iron, 48.1 troy oz. gold).
Exact solution (to 10 digits) = (28.82648317, 8.859282804, 48.01596023). The
relative error (rounded to 3 digits) is er = 5.95 × 10−3. So, partial pivoting
applied to the column-scaled system yields higher relative accuracy than partial
pivoting applied to the unscaled system.
1.5.6. (a)
(−8.1, −6.09) = 3-digit solution with partial pivoting but no scaling.
(b) No! Scaled partial pivoting produces the exact solution—the same as with
complete pivoting.
1.5.7. (a)
2n−1
(b)
2
(c)
This is a famous example that shows that there are indeed cases where par-
tial pivoting will fail due to the large growth of some elements during elimination,
but complete pivoting will be successful because all elements remain relatively
small and of the same order of magnitude.
1.5.8. Use the fact that with partial pivoting no multiplier can exceed 1 together with
the triangle inequality |α + β| ≤|α| + |β| and proceed inductively.
Solutions for exercises in section 1. 6
1.6.1. (a) There are no 5-digit solutions. (b) This doesn’t help—there are now inﬁnitely
many 5-digit solutions. (c) 6-digit solution = (1.23964, −1.3) and exact solution
= (1, −1) (d) r1 = r2 = 0 (e) r1 = −10−6 and r2 = 10−7 (f) Even if computed
residuals are 0, you can’t be sure you have the exact solution.
1.6.2. (a)
(1, −1.0015) (b) Ill-conditioning guarantees that the solution will be very
sensitive to some small perturbation but not necessarily to every small perturba-
tion. It is usually diﬃcult to determine beforehand those perturbations for which
an ill-conditioned system will not be sensitive, so one is forced to be pessimistic
whenever ill-conditioning is suspected.
1.6.3. (a)
m1(5) = m2(5) = −1.2519,
m1(6) = −1.25187, and m2(6) = −1.25188
(c)
An optimally well-conditioned system represents orthogonal (i.e., perpen-
dicular) lines, planes, etc.
1.6.4. They rank as (b) = Almost optimally well-conditioned. (a) = Moderately well-
conditioned. (c) = Badly ill-conditioned.
1.6.5. Original solution = (1, 1, 1). Perturbed solution = (−238, 490, −266). System
is ill-conditioned.

Solutions for Chapter 2
Solutions for exercises in section 2. 1
2.1.1. (a)


1
2
3
3
0
2
1
0
0
0
0
3

is one possible answer. Rank = 3 and the basic columns
are {A∗1, A∗2, A∗4}. (b)





1
2
3
0
2
2
0
0
−8
0
0
0
0
0
0




is one possible answer. Rank = 3 and
every column in A is basic.
(c)







2
1
1
3
0
4
1
0
0
2
−2
1
−3
3
0
0
0
0
−1
3
−1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0







is one possible answer. The rank is 3, and
the basic columns are {A∗1, A∗3, A∗5}.
2.1.2. (c) and (d) are in row echelon form.
2.1.3. (a) Since any row or column can contain at most one pivot, the number of pivots
cannot exceed the number of rows nor the number of columns. (b) A zero row
cannot contain a pivot. (c) If one row is a multiple of another, then one of
them can be annihilated by the other to produce a zero row. Now the result
of the previous part applies. (d) One row can be annihilated by the associated
combination of row operations. (e) If a column is zero, then there are fewer than
n basic columns because each basic column must contain a pivot.
2.1.4. (a) rank (A) = 3 (b) 3-digit rank (A) = 2 (c) With PP, 3-digit rank (A) = 3
2.1.5. 15
2.1.6. (a) No, consider the form


∗
∗
∗
∗
0
0
0
0
0
0
0
∗

(b) Yes—in fact, E is a row
echelon form obtainable from A .
Solutions for exercises in section 2. 2
2.2.1. (a)


1
0
2
0
0
1
1
2
0
0
0
0
1


and
A∗3 = 2A∗1 + 1
2A∗2

6
Solutions
(b)







1
1
2
0
2
0
2
0
0
0
1
−1
0
0
1
0
0
0
0
1
−3
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0







and
A∗2 = 1
2A∗1,
A∗4 = 2A∗1−A∗3,
A∗6 = 2A∗1−3A∗5,
A∗7 = A∗3+A∗5
2.2.2. No.
2.2.3. The same would have to hold in EA, and there you can see that this means not
all columns can be basic. Remember, rank (A) = number of basic columns.
2.2.4. (a)


1
0
0
0
1
0
0
0
1


(b)


1
0
−1
0
1
2
0
0
0

A∗3 is almost a combination of A∗1
and A∗2. In particular, A∗3 ≈−A∗1 + 2A∗2.
2.2.5. E∗1 = 2E∗2 −E∗3 and E∗2 = 1
2E∗1 + 1
2E∗3
Solutions for exercises in section 2. 3
2.3.1. (a), (b)—There is no need to do any arithmetic for this one because the right-
hand side is entirely zero so that you know (0,0,0) is automatically one solution.
(d), (f)
2.3.3. It is always true that rank (A) ≤rank[A|b] ≤m. Since rank (A) = m, it
follows that rank[A|b] = rank (A).
2.3.4. Yes—Consistency implies that b and c are each combinations of the basic
columns in A . If b = 
 βiA∗bi and c = 
 γiA∗bi where the A∗bi ’s are the
basic columns, then b + c = 
(βi + γi)A∗bi = 
 ξiA∗bi, where ξi = βi + γi
so that b + c is also a combination of the basic columns in A .
2.3.5. Yes—because the 4 × 3 system α + βxi + γx2
i = yi obtained by using the four
given points (xi, yi) is consistent.
2.3.6. The system is inconsistent using 5-digits but consistent when 6-digits are used.
2.3.7. If x, y, and z denote the number of pounds of the respective brands applied,
then the following constraints must be met.
total # units of phosphorous = 2x + y + z = 10
total # units of potassium = 3x + 3y
= 9
total # units of nitrogen = 5x + 4y + z = 19
Since this is a consistent system, the recommendation can be satisﬁed exactly.
Of course, the solution tells how much of each brand to apply.
2.3.8. No—if one or more such rows were ever present, how could you possibly eliminate
all of them with row operations? You could eliminate all but one, but then there
is no way to eliminate the last remaining one, and hence it would have to appear
in the ﬁnal form.

Solutions
7
Solutions for exercises in section 2. 4
2.4.1. (a)
x2



−2
1
0
0


+ x4



−1
0
−1
1



(b)
y


−1
2
1
0


(c)
x3



−1
−1
1
0


+ x4



−1
1
0
1



(d) The trivial solution is the only solution.
2.4.2.


0
0
0


and


1
−1
2
0


2.4.3. x2





−2
1
0
0
0




+ x4





−2
0
−1
1
0





2.4.4. rank (A) = 3
2.4.5. (a) 2—because the maximum rank is 4.
(b) 5—because the minimum rank is
1.
2.4.6. Because r = rank (A) ≤m < n
=⇒
n −r > 0.
2.4.7. There are many diﬀerent correct answers. One approach is to answer the question
“What must EA look like?” The form of the general solution tells you that
rank (A) = 2 and that the ﬁrst and third columns are basic. Consequently,
EA =


1
α
0
β
0
0
1
γ
0
0
0
0

so that x1 = −αx2 −βx4 and x3 = −γx4 gives rise
to the general solution x2



−α
1
0
0


+ x4



−β
0
−γ
1


. Therefore, α = 2,
β = 3,
and γ = −2. Any matrix A obtained by performing row operations to EA
will be the coeﬃcient matrix for a homogeneous system with the desired general
solution.
2.4.8. If 
i xfihi is the general solution, then there must exist scalars αi and βi such
that c1 = 
i αihi and c2 = 
i βihi. Therefore, c1 + c2 = 
i(αi + βi)hi,
and this shows that c1 + c2 is the solution obtained when the free variables xfi
assume the values xfi = αi + βi.
Solutions for exercises in section 2. 5
2.5.1. (a)



1
0
2
0


+ x2



−2
1
0
0


+ x4



−1
0
−1
1



(b)


1
0
2

+ y


−1
2
1
0



8
Solutions
(c)



2
−1
0
0


+ x3



−1
−1
1
0


+ x4



−1
1
0
1



(d)


3
−3
−1


2.5.2. From Example 2.5.1, the solutions of the linear equations are:
x1 = 1 −x3 −2x4
x2 = 1 −x3
x3 is free
x4 is free
x5 = −1
Substitute these into the two constraints to get x3 = ±1 and x4 = ±1. Thus
there are exactly four solutions:














−2
0
1
1
−1




,





2
0
1
−1
−1




,





0
2
−1
1
−1




,





4
2
−1
−1
−1














2.5.3. (a) {(3, 0, 4), (2, 1, 5), (1, 2, 6), (0, 3, 7)} See the solution to Exercise 2.3.7 for
the underlying system.
(b)
(3, 0, 4) costs $15 and is least expensive.
2.5.4. (a) Consistent for all α.
(b)
α ̸= 3, in which case the solution is (1, −1, 0).
(c)
α = 3, in which case the general solution is


1
−1
0

+ z


0
−3
2
1

.
2.5.5. No
2.5.6.
EA =











1
0
· · ·
0
0
1
· · ·
0
...
...
...
...
0
0
· · ·
1
0
0
· · ·
0
...
...
· · ·
...
0
0
· · ·
0











m×n
2.5.7. See the solution to Exercise 2.4.7.
2.5.8. (a)


−.3976
0
1

+ y


−.7988
1
0


(b)
There are no solutions in this case.
(c)


1.43964
−2.3
1



Solutions
9
Solutions for exercises in section 2. 6
2.6.1. (a)
(1/575)(383, 533, 261, 644, −150, −111)
2.6.2. (1/211)(179, 452, 36)
2.6.3. (18, 10)
2.6.4. (a)
4
(b)
6
(c)
7 loops but only 3 simple loops.
(d)
Show that
rank ([A|b]) = 3
(g)
5/6

10
Solutions
I fear explanations explanatory of things explained.
— Abraham Lincoln (1809–1865)

Solutions for Chapter 3
Solutions for exercises in section 3. 2
3.2.1. (a)
X =

0
1
2
3

(b)
x = −1
2, y = −6, and z = 0
3.2.2. (a) Neither
(b) Skew symmetric
(c) Symmetric
(d) Neither
3.2.3. The 3 × 3 zero matrix trivially satisﬁes all conditions, and it is the only pos-
sible answer for part (a). The only possible answers for (b) are real symmetric
matrices. There are many nontrivial possibilities for (c).
3.2.4. A = AT and B = BT
=⇒
(A + B)T = AT + BT = A + B. Yes—the
skew-symmetric matrices are also closed under matrix addition.
3.2.5. (a)
A = −AT
=⇒
aij = −aji. If i = j, then ajj = −ajj
=⇒
ajj = 0.
(b) A = −A∗
=⇒
aij = −aji. If i = j, then ajj = −ajj. Write ajj = x+iy
to see that ajj = −ajj
=⇒
x + iy = −x + iy
=⇒
x = 0
=⇒
ajj is pure
imaginary.
(c)
B∗= (iA)∗= −iA∗= −iA
T = −iAT = −iA = −B.
3.2.6. (a) Let S = A+AT and K = A−AT . Then ST = AT +AT T = AT +A = S.
Likewise, KT = AT −AT T = AT −A = −K.
(b) A = S
2 + K
2 is one such decomposition. To see it is unique, suppose A = X+
Y, where X = XT and Y = −YT . Thus, AT = XT +YT = X −Y =⇒A+
AT = 2X, so that X =
A+AT
2
=
S
2 . A similar argument shows that Y =
A−AT
2
= K
2 .
3.2.7. (a)
[(A + B)∗]ij = [A + B]ji = [A + B]ji = [A]ji + [B]ji = [A∗]ij + [B∗]ij =
[A∗+ B∗]ij
(b)
[(αA)∗]ij = [αA]ji = [¯αA]ji = ¯α[A]ji = ¯α[A∗]ij
3.2.8. k








1
−1
0
· · ·
0
0
−1
2
−1
· · ·
0
0
0
−1
2
· · ·
0
0
...
...
...
...
...
...
0
0
0
· · ·
2
−1
0
0
0
· · ·
−1
1








Solutions for exercises in section 3. 3
3.3.1. Functions (b) and (f) are linear. For example, to check if (b) is linear, let
A =

a1
a2

and B =

b1
b2

, and check if f(A + B) = f(A) + f(B) and

12
Solutions
f(αA) = αf(A). Do so by writing
f(A + B) = f

a1 + b1
a2 + b2

=

a2 + b2
a1 + b1

=

a2
a1

+

b2
b1

= f(A) + f(B),
f(αA) = f

αa1
αa2

=

αa2
αa1

= α

a2
a1

= αf(A).
3.3.2. Write f(x) = 
n
i=1 ξixi. For all points x =




x1
x2
...
xn



and y =




y1
y2
...
yn



, and for
all scalars α, it is true that
f(αx + y) =
n

i=1
ξi(αxi + yi) =
n

i=1
ξiαxi +
n

i=1
ξiyi
= α
n

i=1
ξixi +
n

i=1
ξiyi = αf(x) + f(y).
3.3.3. There are many possibilities. Two of the simplest and most common are Hooke’s
law for springs that says that F = kx (see Example 3.2.1) and Newton’s second
law that says that F = ma (i.e., force = mass × acceleration).
3.3.4. They are all linear. To see that rotation is linear, use trigonometry to deduce
that if p =

x1
x2

, then f(p) = u =

u1
u2

, where
u1 = (cos θ)x1 −(sin θ)x2
u2 = (sin θ)x1 + (cos θ)x2.
f is linear because this is a special case of Example 3.3.2. To see that reﬂection
is linear, write p =

x1
x2

and f(p) =

x1
−x2

. Veriﬁcation of linearity is
straightforward. For the projection function, use the Pythagorean theorem to
conclude that if p =

x1
x2

, then f(p) = x1+x2
2

1
1

. Linearity is now easily
veriﬁed.

Solutions
13
Solutions for exercises in section 3. 4
3.4.1. Refer to the solution for Exercise 3.3.4. If Q, R, and P denote the matrices
associated with the rotation, reﬂection, and projection, respectively, then
Q =

cos θ
−sin θ
sin θ
cos θ

,
R =

1
0
0
−1

,
and
P =
 1
2
1
2
1
2
1
2

.
3.4.2. Refer to the solution for Exercise 3.4.1 and write
RQ =

1
0
0
−1
 
cos θ
−sin θ
sin θ
cos θ

=

cos θ
−sin θ
−sin θ
−cos θ

.
If Q(x) is the rotation function and R(x) is the reﬂection function, then the
composition is
R

Q(x)

=

(cos θ)x1 −(sin θ)x2
−(sin θ)x1 −(cos θ)x2

.
3.4.3. Refer to the solution for Exercise 3.4.1 and write
PQR =

a11x1 + a12x2
a21x1 + a22x2
 
cos θ
−sin θ
sin θ
cos θ
 
1
0
0
−1

= 1
2

cos θ + sin θ
sin θ −cos θ
cos θ + sin θ
sin θ −cos θ

.
Therefore, the composition of the three functions in the order asked for is
P

Q

R(x)

= 1
2

(cos θ + sin θ)x1 + (sin θ −cos θ)x2
(cos θ + sin θ)x1 + (sin θ −cos θ)x2

.
Solutions for exercises in section 3. 5
3.5.1. (a) AB =


10
15
12
8
28
52


(b) BA does not exist
(c) CB does not exist
(d) CT B = ( 10
31 )
(e) A2 =


13
−1
19
16
13
12
36
−17
64


(f) B2 does not exist
(g) CT C = 14
(h) CCT =


1
2
3
2
4
6
3
6
9


(i) BBT =


5
8
17
8
16
28
17
28
58


(j) BT B =

10
23
23
69

(k) CT AC = 76

14
Solutions
3.5.2. (a)
A =


2
1
1
4
0
2
2
2
0

, x =


x1
x2
x3

, b =


3
10
−2


(b)
s =


1
−2
3


(c)
b = A∗1 −2A∗2 + 3A∗3
3.5.3. (a)
EA =


A1∗
A2∗
3A1∗+ A3∗


(b)
AE = ( A∗1 + 3A∗3
A∗2
A∗3 )
3.5.4. (a)
A∗j
(b)
Ai∗
(c)
aij
3.5.5. Ax = Bx ∀x
=⇒
Aej = Bej ∀ej
=⇒
A∗j = B∗j ∀j
=⇒
A = B.
(The symbol ∀is mathematical shorthand for the phrase “for all.”)
3.5.6. The limit is the zero matrix.
3.5.7. If A is m × p and B is p × n, write the product as
AB = ( A∗1
A∗2
· · ·
A∗p )




B1∗
B2∗
...
Bp∗



= A∗1B1∗+ A∗2B2∗+ · · · + A∗pBp∗
=
p

k=1
A∗kBk∗.
3.5.8. (a)
[AB]ij = Ai∗B∗j = ( 0
· · ·
0
aii
· · ·
ain )









b1j
...
bjj
0
...
0









is 0 when i > j.
(b)
When i = j, the only nonzero term in the product Ai∗B∗i is aiibii.
(c) Yes.
3.5.9. Use [AB]ij = 
k aikbkj along with the rules of diﬀerentiation to write
d[AB]ij
dt
= d (
k aikbkj)
dt
=

k
d(aikbkj)
dt
=

k
daik
dt bkj + aik
dbkj
dt

=

k
daik
dt bkj +

k
aik
dbkj
dt
=
dA
dt B

ij
+

AdB
dt

ij
=
dA
dt B + AdB
dt

ij
.
3.5.10. (a)
[Ce]i = the total number of paths leaving node i.
(b)
[eT C]i = the total number of paths entering node i.

Solutions
15
3.5.11. At time t, the concentration of salt in tank i is
xi(t)
V
lbs/gal. For tank 1,
dx1
dt = lbs
sec coming in −lbs
sec going out = 0lbs
sec −

r gal
sec × x1(t)
V
lbs
gal

= −r
V x1(t)lbs
sec.
For tank 2,
dx2
dt = lbs
sec coming in −lbs
sec going out = r
V x1(t)lbs
sec −

r gal
sec × x2(t)
V
lbs
gal

= r
V x1(t)lbs
sec −r
V x2(t)lbs
sec = r
V

x1(t) −x2(t)

,
and for tank 3,
dx3
dt = lbs
sec coming in −lbs
sec going out = r
V x2(t)lbs
sec −

r gal
sec × x3(t)
V
lbs
gal

= r
V x2(t)lbs
sec −r
V x3(t)lbs
sec = r
V

x2(t) −x3(t)

.
This is a system of three linear ﬁrst-order diﬀerential equations
dx1
dt
= r
V

−x1(t)

dx2
dt
= r
V

x1(t) −x2(t)

dx3
dt
= r
V

x2(t) −x3(t)

that can be written as a single matrix diﬀerential equation



dx1/dt
dx2/dt
dx3/dt


= r
V



−1
0
0
1
−1
0
0
1
−1






x1(t)
x2(t)
x3(t)


.

16
Solutions
Solutions for exercises in section 3. 6
3.6.1.
AB =

A11
A12
A13
A21
A22
A23
 

B1
B2
B3

=

A11B1 + A12B2 + A13B3
A21B1 + A22B2 + A23B3

=


−10
−19
−10
−19
−1
−1


3.6.2. Use block multiplication to verify L2 = I —be careful not to commute any of
the terms when forming the various products.
3.6.3. Partition the matrix as A =

I
C
0
C

, where C = 1
3


1
1
1
1
1
1
1
1
1

and observe
that C2 = C. Use this together with block multiplication to conclude that
Ak =

I
C + C2 + C3 + · · · + Ck
0
Ck

=

I
kC
0
C

.
Therefore, A300 =







1
0
0
100
100
100
0
1
0
100
100
100
0
0
1
100
100
100
0
0
0
1/3
1/3
1/3
0
0
0
1/3
1/3
1/3
0
0
0
1/3
1/3
1/3







.
3.6.4. (A∗A)∗= A∗A∗∗= A∗A and (AA∗)∗= A∗∗A∗= AA∗.
3.6.5. (AB)T = BT AT = BA = AB. It is easy to construct a 2 × 2 example to show
that this need not be true when AB ̸= BA.
3.6.6.
[(D + E)F]ij = (D + E)i∗F∗j =

k
[D + E]ik[F]kj =

k
([D]ik + [E]ik) [F]kj
=

k
([D]ik[F]kj + [E]ik[F]kj) =

k
[D]ik[F]kj +

k
[E]ik[F]kj
= Di∗F∗j + Ei∗F∗j = [DF]ij + [EF]ij
= [DF + EF]ij.
3.6.7. If a matrix X did indeed exist, then
I = AX −XA
=⇒
trace (I) = trace (AX −XA)
=⇒
n = trace (AX) −trace (XA) = 0,

Solutions
17
which is impossible.
3.6.8. (a)
yT A = bT
=⇒
(yT A)
T = bT T
=⇒
AT y = b. This is an n × m
system of equations whose coeﬃcient matrix is AT .
(b) They are the same.
3.6.9. Draw a transition diagram similar to that in Example 3.6.3 with North and South
replaced by ON and OFF, respectively. Let xk be the proportion of switches in
the ON state, and let yk be the proportion of switches in the OFF state after
k clock cycles have elapsed. According to the given information,
xk = xk−1(.1) + yk−1(.3)
yk = xk−1(.9) + yk−1(.7)
so that pk = pk−1P, where
pk = ( xk
yk )
and
P =

.1
.9
.3
.7

.
Just as in Example 3.6.3, pk = p0Pk. Compute a few powers of P to ﬁnd
P2 =

.280
.720
.240
.760

,
P3 =

.244
.756
.252
.748

P4 =

.251
.749
.250
.750

,
P5 =

.250
.750
.250
.750

and deduce that P∞= limk→∞Pk =

1/4
3/4
1/4
3/4

. Thus
pk →p0P∞= ( 1
4(x0 + y0)
3
4(x0 + y0) ) = ( 1
4
3
4 ) .
For practical purposes, the device can be considered to be in equilibrium after
about 5 clock cycles—regardless of the initial proportions.
3.6.10. ( −4
1
−6
5 )
3.6.11. (a)
trace (ABC) = trace (A{BC}) = trace ({BC}A) = trace (BCA). The
other equality is similar.
(b) Use almost any set of 2 × 2 matrices to con-
struct an example that shows equality need not hold.
(c) Use the fact that
trace

CT 	
= trace (C) for all square matrices to conclude that
trace

AT B
	
=trace

(AT B)
T 
= trace

BT AT T 
=trace

BT A
	
= trace

ABT 	
.
3.6.12. (a)
xT x = 0 ⇐⇒
n
k=1 x2
i = 0 ⇐⇒xi = 0 for each i ⇐⇒x = 0.
(b) trace

AT A
	
= 0 ⇐⇒

i
[AT A]ii = 0 ⇐⇒

i
(AT )i∗A∗i = 0
⇐⇒

i

k
[AT ]ik[A]ki = 0 ⇐⇒

i

k
[A]ki[A]ki = 0
⇐⇒

i

k
[A]2
ki = 0
⇐⇒[A]ki = 0 for each k and i ⇐⇒A = 0

18
Solutions
Solutions for exercises in section 3. 7
3.7.1. (a)

3
−2
−1
1

(b) Singular
(c)


2
−4
3
4
−7
4
5
−8
4


(d) Singular
(e)



2
−1
0
0
−1
2
−1
0
0
−1
2
−1
0
0
−1
1



3.7.2. Write the equation as (I −A)X = B and compute
X = (I −A)−1B =


1
−1
1
0
1
−1
0
0
1




1
2
2
1
3
3

=


2
4
−1
−2
3
3

.
3.7.3. In each case, the given information implies that rank (A) < n —see the solution
for Exercise 2.1.3.
3.7.4. (a) If D is diagonal, then D−1 exists if and only if each dii ̸= 0, in which case




d11
0
· · ·
0
0
d22
· · ·
0
...
...
...
...
0
0
· · ·
dnn




−1
=




1/d11
0
· · ·
0
0
1/d22
· · ·
0
...
...
...
...
0
0
· · ·
1/dnn



.
(b)
If T is triangular, then T−1 exists if and only if each tii ̸= 0. If T
is upper (lower) triangular, then T−1 is also upper (lower) triangular with
[T−1]ii = 1/tii.
3.7.5.

A−1	T =

AT 	−1 = A−1.
3.7.6. Start with A(I −A) = (I −A)A and apply (I −A)−1 to both sides, ﬁrst on
one side and then on the other.
3.7.7. Use the result of Example 3.6.5 that says that trace (AB) = trace (BA) to
write
m = trace (Im) = trace (AB) = trace (BA) = trace (In) = n.
3.7.8. Use the reverse order law for inversion to write

A(A + B)−1B
−1 = B−1(A + B)A−1 = B−1 + A−1
and

B(A + B)−1A
−1 = A−1(A + B)B−1 = B−1 + A−1.
3.7.9. (a)
(I −S)x = 0
=⇒
xT (I −S)x = 0
=⇒
xT x = xT Sx. Taking trans-
poses on both sides yields xT x = −xT Sx, so that xT x = 0, and thus x = 0

Solutions
19
(recall Exercise 3.6.12). The conclusion follows from property (3.7.8).
(b)
First notice that Exercise 3.7.6 implies that A = (I + S)(I −S)−1 =
(I −S)−1(I + S). By using the reverse order laws, transposing both sides yields
exactly the same thing as inverting both sides.
3.7.10. Use block multiplication to verify that the product of the matrix with its inverse
is the identity matrix.
3.7.11. Use block multiplication to verify that the product of the matrix with its inverse
is the identity matrix.
3.7.12. Let M =

A
B
C
D

and X =

DT
−BT
−CT
AT

. The hypothesis implies that
MX = I, and hence (from the discussion in Example 3.7.2) it must also be
true that XM = I, from which the conclusion follows. Note: This problem
appeared on a past Putnam Exam—a national mathematics competition for
undergraduate students that is considered to be quite challenging. This means
that you can be proud of yourself if you solved it before looking at this solution.
Solutions for exercises in section 3. 8
3.8.1. (a)
B−1 =


1
2
−1
0
−1
1
1
4
−2


(b) Let c =


0
0
1

and dT = ( 0
2
1 ) to obtain C−1 =


0
−2
1
1
3
−1
−1
−4
2


3.8.2. A∗j needs to be removed, and b needs to be inserted in its place. This is
accomplished by writing B = A+(b−A∗j)eT
j . Applying the Sherman–Morrison
formula with c = b −A∗j and dT = eT
j
yields
B−1 = A−1 −A−1(b −A∗j)eT
j A−1
1 + eT
j A−1(b −A∗j) = A−1 −A−1beT
j A−1 −ejeT
j A−1
1 + eT
j A−1b −eT
j ej
= A−1 −A−1b[A−1]j∗−ej[A−1]j∗
[A−1]j∗b
= A−1 −

A−1b −ej
	
[A−1]j∗
[A−1]j∗b
.
3.8.3. Use the Sherman–Morrison formula to write
z = (A + cdT )−1b =

A−1 −A−1cdT A−1
1 + dT A−1c

b = A−1b −A−1cdT A−1b
1 + dT A−1c
= x −
ydT x
1 + dT y.
3.8.4. (a)
For a nonsingular matrix A, the Sherman–Morrison formula guarantees
that A + αeieT
j
is also nonsingular when 1 + α

A−1
ji ̸= 0, and this certainly
will be true if α is suﬃciently small.

20
Solutions
(b)
Write Em×m = [ϵij] = 
m
i,j=1 ϵijeieT
j
and successively apply part (a) to
I + E =

I + ϵ11e1eT
1
	
+ ϵ12e1eT
2

+ · · · + ϵmmemeT
m

to conclude that when the ϵij ’s are suﬃciently small,
I + ϵ11e1eT
1 ,

I + ϵ11e1eT
1
	
+ ϵ12e1eT
2

,
. . . ,
I + E
are each nonsingular.
3.8.5. Write A + ϵB = A(I + A−1ϵB). You can either use the Neumann series result
(3.8.5) or Exercise 3.8.4 to conclude that (I + A−1ϵB) is nonsingular whenever
the entries of A−1ϵB are suﬃciently small in magnitude, and this can be insured
by restricting ϵ to a small enough interval about the origin. Since the product
of two nonsingular matrices is again nonsingular—see (3.7.14)—it follows that
A + ϵB = A(I + A−1ϵB) must be nonsingular.
3.8.6. Since

I
C
0
I
 
A
C
DT
−I
 
I
0
DT
I

=

A + CDT
0
0
−I

,
we can use R = DT and B = −I in part (a) of Exercise 3.7.11 to obtain

I
0
−DT
I
 
A−1 + A−1CS−1DT A−1
−A−1CS−1
−S−1DT A−1
S−1
 
I
−C
0
I

=
 
A + CDT 	−1
0
0
−I

,
where S = −

I + DT A−1C
	
. Comparing the upper-left-hand blocks produces

A + CDT 	−1 = A−1 −A−1C

I + DT A−1C
	−1 DT A−1.
3.8.7. The ranking from best to worst condition is A, B, C, because
A−1 =
1
100


2
1
1
1
2
1
1
1
1


=⇒
κ(A) = 20 = 2 × 101
B−1 =


−1465
−161
17
173
19
−2
−82
−9
1


=⇒
κ(B) = 149, 513 ≈1.5 × 105
C−1 =


−42659
39794
−948
2025
−1889
45
45
−42
1


=⇒
κ(C) = 82, 900, 594 ≈8.2 × 107.
3.8.8. (a)
Diﬀerentiate A(t)A(t)−1 = I with the product rule for diﬀerentiation (re-
call Exercise 3.5.9).
(b)
Use the product rule for diﬀerentiation together with part (a) to diﬀeren-
tiate A(t)x(t) = b(t).

Solutions
21
Solutions for exercises in section 3. 9
3.9.1. (a) If G1, G2, . . . , Gk is the sequence of elementary matrices that corresponds
to the elementary row operations used in the reduction [A|I] −→[B|P], then
Gk · · · G2G1[A|I] = [B|P]
=⇒
[Gk · · · G2G1A | Gk · · · G2G1I] = [B|P]
=⇒
Gk · · · G2G1A = B
and
Gk · · · G2G1 = P.
(b) Use the same argument given above, but apply it on the right-hand side.
(c)
[A|I]
Gauss–Jordan
−−−−−−−−→[EA|P] yields


1
2
3
4
1
0
0
2
4
6
7
0
1
0
1
2
3
6
0
0
1

−→


1
2
3
0
−7
4
0
0
0
0
1
2
−1
0
0
0
0
0
−5
2
1

.
Thus P =


−7
4
0
2
−1
0
−5
2
1


is the product of the elementary matrices corre-
sponding to the operations used in the reduction, and PA = EA.
(d) You already have P such that PA = EA. Now ﬁnd Q such that EAQ =
Nr by column reducing EA. Proceed using part (b) to accumulate Q.
EA
I4

−→










1
2
3
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1










−→










1
0
2
3
0
1
0
0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
1
0
1
0
0










−→










1
0
0
0
0
1
0
0
0
0
0
0
1
0
−2
−3
0
0
1
0
0
0
0
1
0
1
0
0










3.9.2. (a)
Yes—because rank (A) = rank (B).
(b)
Yes—because EA = EB.
(c) No—because EAT ̸= EBT .
3.9.3. The positions of the basic columns in A correspond to those in EA. Because
A
row
∼B ⇐⇒EA = EB, it follows that the basic columns in A and B must
be in the same positions.
3.9.4. An elementary interchange matrix (a Type I matrix) has the form E = I −uuT ,
where u = ei −ej, and it follows from (3.9.1) that E = ET = E−1. If
P = E1E2 · · · Ek is a product of elementary interchange matrices, then the re-
verse order laws yield
P−1 = (E1E2 · · · Ek)−1 = E−1
k
· · · E−1
2 E−1
1
= ET
k · · · ET
2 ET
1 = (E1E2 · · · Ek)T = PT .

22
Solutions
3.9.5. They are all true! A ∼I ∼A−1 because rank (A) = n = rank

A−1	
,
A
row
∼
A−1 because PA = A−1 with P =

A−1	2 = A−2,
and A
col
∼A−1 because
AQ = A−1 with Q = A−2. The fact that A
row
∼I and A
col
∼I follows since
A−1A = AA−1 = I.
3.9.6. (a), (c), (d), and (e) are true.
3.9.7. Rows i and j can be interchanged with the following sequence of Type II and
Type III operations—this is Exercise 1.2.12 on p. 14.
Rj ←Rj + Ri
(replace row j by the sum of row j and i)
Ri ←Ri −Rj
(replace row i by the diﬀerence of row i and j)
Rj ←Rj + Ri
(replace row j by the sum of row j and i)
Ri ←−Ri
(replace row i by its negative)
Translating these to elementary matrices (remembering to build from the right
to the left) produces
(I −2eieT
i )(I + ejeT
i )(I −eieT
j )(I + ejeT
i ) = I −uuT ,
where
u = ei −ej.
3.9.8. Let Bm×r = [A∗b1A∗b2 · · · A∗br] contain the basic columns of A, and let Cr×n
contain the nonzero rows of EA. If A∗k is basic—say A∗k = A∗bj —then
C∗k = ej, and
(BC)∗k = BC∗k = Bej = B∗j = A∗bj = A∗k.
If A∗k is nonbasic, then C∗k is nonbasic and has the form
C∗k =









µ1
µ2
...
µj
...
0









= µ1









1
0
...
0
...
0









+ µ2









0
1
...
0
...
0









+ · · · + µj









0
0
...
1
...
0









= µ1e1 + µ2e2 + · · · + µjej,
where the ei ’s are the basic columns to the left of C∗k. Because A
row
∼EA,
the relationships that exist among the columns of A are exactly the same as
the relationships that exist among the columns of EA. In particular,
A∗k = µ1A∗b1 + µ2A∗b2 + · · · + µjA∗bj,
where the A∗bi ’s are the basic columns to the left of A∗k. Therefore,
(BC)∗k = BC∗k = B (µ1e1 + µ2e2 + · · · + µjej)
= µ1B∗1 + µ2B∗2 + · · · + µjB∗j
= µ1A∗b1 + µ2A∗b2 + · · · + µjA∗bj
= A∗k.

Solutions
23
3.9.9. If A = uvT , where um×1 and vn×1 are nonzero columns, then
u
row
∼e1
and
vT col
∼eT
1
=⇒
A = uvT ∼e1eT
1 = N1
=⇒
rank (A) = 1.
Conversely, if rank (A) = 1, then the existence of u and v follows from Exer-
cise 3.9.8. If you do not wish to rely on Exercise 3.9.8, write PAQ = N1 = e1eT
1 ,
where e1 is m × 1 and eT
1 is 1 × n so that
A = P−1e1eT
1 Q−1 =

P−1	
∗1

Q−1	
1∗= uvT .
3.9.10. Use Exercise 3.9.9 and write
A = uvT
=⇒
A2 =

uvT 	 
uvT 	
= u

vT u
	
vT = τuvT = τA,
where τ = vT u. Recall from Example 3.6.5 that trace (AB) = trace (BA),
and write
τ = trace(τ) = trace

vT u
	
= trace

uvT 	
= trace (A).
Solutions for exercises in section 3. 10
3.10.1. (a)
L =


1
0
0
4
1
0
3
2
1

and U =


1
4
5
0
2
6
0
0
3


(b)
x1 =


110
−36
8

and
x2 =


112
−39
10


(c)
A−1 = 1
6


124
−40
14
−42
15
−6
10
−4
2


3.10.2. (a)
The second pivot is zero.
(b)
P is the permutation matrix associated
with the permutation p = ( 2
4
1
3 ) . P is constructed by permuting the
rows of I in this manner.
L =



1
0
0
0
0
1
0
0
1/3
0
1
0
2/3
−1/2
1/2
1



and
U =



3
6
−12
3
0
2
−2
6
0
0
8
16
0
0
0
−5



(c)
x =



2
−1
0
1




24
Solutions
3.10.3. ξ = 0, ±
√
2, ±
√
3
3.10.4. A possesses an LU factorization if and only if all leading principal submatrices
are nonsingular. The argument associated with equation (3.10.13) proves that

Lk
0
cT U−1
k
1
 
Uk
L−1
k b
0
ak+1,k+1 −cT A−1
k b

= Lk+1Uk+1
is the LU factorization for Ak+1. The desired conclusion follows from the fact
that the k + 1th pivot is the (k + 1, k + 1) -entry in Uk+1. This pivot must be
nonzero because Uk+1 is nonsingular.
3.10.5. If L and U are both triangular with 1’s on the diagonal, then L−1 and U−1
contain only integer entries, and consequently A−1 = U−1L−1 is an integer
matrix.
3.10.6. (b)
L =



1
0
0
0
−1/2
1
0
0
0
−2/3
1
0
0
0
−3/4
1


and U =



2
−1
0
0
0
3/2
−1
0
0
0
4/3
−1
0
0
0
1/4



3.10.7. Observe how the LU factors evolve from Gaussian elimination. Following the
procedure described in Example 3.10.1 where multipliers ℓij are stored in the
positions they annihilate (i.e., in the (i, j) -position), and where ⋆’s are put
in positions that can be nonzero, the reduction of a 5 × 5 band matrix with
bandwidth w = 2 proceeds as shown below.





⋆
⋆
⋆
0
0
⋆
⋆
⋆
⋆
0
⋆
⋆
⋆
⋆
⋆
0
⋆
⋆
⋆
⋆
0
0
⋆
⋆
⋆




−→





⋆
⋆
⋆
0
0
l21
⋆
⋆
⋆
0
l31
⋆
⋆
⋆
⋆
0
⋆
⋆
⋆
⋆
0
0
⋆
⋆
⋆




−→





⋆
⋆
⋆
0
0
l21
⋆
⋆
⋆
0
l31
l32
⋆
⋆
⋆
0
l42
⋆
⋆
⋆
0
0
⋆
⋆
⋆





−→





⋆
⋆
⋆
0
0
l21
⋆
⋆
⋆
0
l31
l32
⋆
⋆
⋆
0
l42
l43
⋆
⋆
0
0
l53
⋆
⋆




−→





⋆
⋆
⋆
0
0
l21
⋆
⋆
⋆
0
l31
l32
⋆
⋆
⋆
0
l42
l43
⋆
⋆
0
0
l53
l54
⋆





Thus L =





1
0
0
0
0
l21
1
0
0
0
l31
l32
1
0
0
0
l42
l43
1
0
0
0
l53
l54
1




and U =





⋆
⋆
⋆
0
0
0
⋆
⋆
⋆
0
0
0
⋆
⋆
⋆
0
0
0
⋆
⋆
0
0
0
0
⋆




.
3.10.8. (a)
A =

0
1
1
0

(b)
A =

1
0
0
−1

3.10.9. (a)
L =


1
0
0
4
1
0
3
2
1

,
D =


1
0
0
0
2
0
0
0
3

, and U =


1
4
5
0
1
3
0
0
1



Solutions
25
(b)
Use the same argument given for the uniqueness of the LU factorization
with minor modiﬁcations.
(c)
A = AT
=⇒
LDU = UT DT LT = UT DLT . These are each LDU
factorizations for A, and consequently the uniqueness of the LDU factorization
means that U = LT .
3.10.10. A is symmetric with pivots 1, 4, 9. The Cholesky factor is R =


1
0
0
2
2
0
3
3
3

.

26
Solutions
It is unworthy of excellent men to lose hours
like slaves in the labor of calculations.
— Baron Gottfried Wilhelm von Leibnitz (1646–1716)

Solutions for Chapter 4
Solutions for exercises in section 4. 1
4.1.1. Only (b) and (d) are subspaces.
4.1.2. (a), (b), (f), (g), and (i) are subspaces.
4.1.3. All of ℜ3.
4.1.4. If v ∈V is a nonzero vector in a space V, then all scalar multiples αv must
also be in V.
4.1.5. (a) A line.
(b) The (x,y)-plane.
(c)
ℜ3
4.1.6. Only (c) and (e) span ℜ3. To see that (d) does not span ℜ3, ask whether
or not every vector (x, y, z) ∈ℜ3 can be written as a linear combination of
the vectors in (d). It’s convenient to think in terms columns, so rephrase the
question by asking if every b =


x
y
z

can be written as a linear combination
of


v1 =


1
2
1

, v2 =


2
0
−1

, v3 =


4
4
1




. That is, for each b ∈ℜ3, are
there scalars α1, α2, α3 such that α1v1 + α2v2 + α3v3 = b or, equivalently, is


1
2
4
2
0
4
1
−1
1




α1
α2
α3

=


x
y
z


consistent for all


x
y
z

?
This is a system of the form Ax = b, and it is consistent for all b if and only
if rank ([A|b]) = rank (A) for all b. Since


1
2
4
x
2
0
4
y
1
−1
1
z

→


1
2
4
x
0
−4
−4
y −2x
0
−3
−3
z −x


→


1
2
4
x
0
−4
−4
y −2x
0
0
0
(x/2) −(3y/4) + z

,
it’s clear that there exist b ’s (e.g., b = (1, 0, 0)T ) for which Ax = b is not
consistent, and hence not all b ’s are a combination of the vi ’s. Therefore, the
vi ’s don’t span ℜ3.
4.1.7. This follows from (4.1.2).

28
Solutions
4.1.8. (a)
u, v ∈X ∩Y
=⇒
u, v ∈X and u, v ∈Y. Because X and Y are
closed with respect to addition, it follows that u + v ∈X and u + v ∈Y,
and therefore u + v ∈X ∩Y. Because X and Y are both closed with respect
to scalar multiplication, we have that αu ∈X and αu ∈Y for all α, and
consequently αu ∈X ∩Y for all α.
(b) The union of two diﬀerent lines through the origin in ℜ2 is not a subspace.
4.1.9. (a)
(A1) holds because x1, x2 ∈A(S)
=⇒
x1 = As1 and x2 = As2 for
some s1, s2 ∈S
=⇒
x1 + x2 = A(s1 + s2). Since S is a subspace, it is
closed under vector addition, so s1 + s2 ∈S. Therefore, x1 + x2 is the image of
something in S —namely, s1+s2 —and this means that x1+x2 ∈A(S). To see
that (M1) holds, consider αx, where α is an arbitrary scalar and x ∈A(S).
Now, x ∈A(S)
=⇒
x = As for some s ∈S
=⇒
αx = αAs = A(αs).
Since S is a subspace, we are guaranteed that αs ∈S, and therefore αx is the
image of something in S. This is what it means to say αx ∈A(S).
(b)
Prove equality by demonstrating that span {As1, As2, . . . , Ask} ⊆A(S)
and A(S) ⊆span {As1, As2, . . . , Ask} . To show span {As1, As2, . . . , Ask} ⊆
A(S), write
x ∈span {As1, As2, . . . , Ask}
=⇒
x =
k

i=1
αi(Asi) = A
 k

i=1
αisi

∈A(S).
Inclusion in the reverse direction is established by saying
x ∈A(S)
=⇒
x = As for some s ∈S
=⇒
s =
k

i=1
βisi
=⇒x = A
 k

i=1
βisi

=
k

i=1
βi(Asi) ∈span {As1, As2, . . . , Ask} .
4.1.10. (a) Yes, all of the deﬁning properties are satisﬁed.
(b) Yes, this is essentially ℜ2.
(c) No, it is not closed with respect to scalar multiplication.
4.1.11. If span (M) = span (N) , then every vector in N must be a linear combination
of vectors from M. In particular, v must be a linear combination of the mi ’s,
and hence v ∈span (M) . To prove the converse, ﬁrst notice that span (M) ⊆
span (N) . The desired conclusion will follow if it can be demonstrated that
span (M) ⊇span (N) . The hypothesis that v ∈span (M) guarantees that
v = 
r
i=1 βimi. If z ∈span (N) , then
z =
r

i=1
αimi + αr+1v =
r

i=1
αimi + αr+1
r

i=1
βimi
=
r

i=1

αi + αr+1βi

mi,

Solutions
29
which shows z ∈span (M) , and therefore span (M) ⊇span (N) .
4.1.12. To show span (S) ⊆M, observe that x ∈span (S)
=⇒
x = 
i αivi.
If V is any subspace containing S, then 
i αivi ∈V because V is closed
under addition and scalar multiplication, and therefore x ∈M. The fact that
M ⊆span (S) follows because if x ∈M, then x ∈span (S) because span (S)
is one particular subspace that contains S.
Solutions for exercises in section 4. 2
4.2.1. R (A) = span





1
−2
1

,


1
0
2




,
N

AT 	
= span





4
1
−2




,
N (A) = span














−2
1
0
0
0




,





2
0
−3
1
0




,





−1
0
−4
0
1














,
R

AT 	
= span














1
2
0
−2
1




,





0
0
1
3
4














.
4.2.2. (a) This is simply a restatement of equation (4.2.3).
(b)
Ax = b has a unique solution if and only if rank (A) = n (i.e., there are
no free variables—see §2.5), and (4.2.10) says rank (A) = n ⇐⇒N (A) = {0}.
4.2.3. (a)
It is consistent because b ∈R (A).
(b)
It is nonunique because N (A) ̸= {0} —see Exercise 4.2.2.
4.2.4. Yes, because rank[A|b] = rank (A) = 3
=⇒
∃x such that Ax = b —i.e.,
Ax = b is consistent.
4.2.5. (a)
If R (A) = ℜn, then
R (A) = R (In)
=⇒
A
col
∼In
=⇒
rank (A) = rank (In) = n.
(b)
R (A) = R

AT 	
= ℜn and N (A) = N

AT 	
= {0}.
4.2.6. EA ̸= EB means that R

AT 	
̸= R

BT 	
and N (A) ̸= N (B). However,
EAT = EBT implies that R (A) = R (B) and N

AT 	
= N

BT 	
.
4.2.7. Demonstrate that rank (An×n) = n by using (4.2.10). If x ∈N (A), then
Ax = 0
=⇒
A1x = 0
and
A2x = 0
=⇒
x ∈N (A1) = R

AT
2
	
=⇒
∃yT such that xT = yT A2
=⇒
xT x = yT A2x = 0
=⇒

i
x2
i = 0
=⇒
x = 0.

30
Solutions
4.2.8. yT b = 0 ∀y ∈N

AT 	
= R

PT
2
	
=⇒
P2b = 0
=⇒
b ∈N (P2) = R (A)
4.2.9. x ∈R

A | B
	
⇐⇒∃y such that x =

A | B
	
y =

A | B
	 
y1
y2

= Ay1 +
By2 ⇐⇒x ∈R (A) + R (B)
4.2.10. (a) p+N (A) is the set of all possible solutions to Ax = b. Recall from (2.5.7)
that the general solution of a nonhomogeneous equation is a particular solution
plus the general solution of the homogeneous equation Ax = 0. The general
solution of the homogeneous equation is simply a way of describing all possible
solutions of Ax = 0, which is N (A).
(b)
rank (A3×3) = 1 means that N (A) is spanned by two vectors, and hence
N (A) is a plane through the origin. From the parallelogram law, p + N (A) is
a plane parallel to N (A) passing through the point deﬁned by p.
(c) This time N (A) is spanned by a single vector, and p + N (A) is a line
parallel to N (A) passing through the point deﬁned by p.
4.2.11. a ∈R

AT 	
⇐⇒∃y such that aT = yT A. If Ax = b, then
aT x = yT Ax = yT b,
which is independent of x.
4.2.12. (a)
b ∈R (AB)
=⇒
∃x such that b = ABx = A(Bx)
=⇒
b ∈R (A)
because b is the image of Bx.
(b)
x ∈N (B)
=⇒
Bx = 0
=⇒
ABx = 0
=⇒
x ∈N (AB).
4.2.13. Given any z ∈R (AB), the object is to show that z can be written as some
linear combination of the Abi ’s. Argue as follows. z ∈R (AB)
=⇒
z = ABy
for some y. But it is always true that By ∈R (B), so
By = α1b1 + α2b2 + · · · + αnbn,
and therefore z = ABy = α1Ab1 + α2Ab2 + · · · + αnAbn.
Solutions for exercises in section 4. 3
4.3.1. (a) and (b) are linearly dependent—all others are linearly independent. To write
one vector as a combination of others in a dependent set, place the vectors as
columns in A and ﬁnd EA. This reveals the dependence relationships among
columns of A.
4.3.2. (a) According to (4.3.12), the basic columns in A always constitute one maximal
linearly independent subset.
(b) Ten—5 sets using two vectors, 4 sets using one vector, and the empty set.
4.3.3. rank (H) ≤3, and according to (4.3.11), rank (H) is the maximal number of
independent rows in H.

Solutions
31
4.3.4. The question is really whether or not the columns in
ˆA =




S
L
F
#1
1
1
1
10
#2
1
2
1
12
#3
1
2
2
15
#4
1
3
2
17




are linearly independent. Reducing ˆA to E ˆ
A shows that 5 + 2S + 3L −F = 0.
4.3.5. (a) This follows directly from the deﬁnition of linear dependence because there
are nonzero values of α such that α0 = 0.
(b) This is a consequence of (4.3.13).
4.3.6. If each tii ̸= 0, then T is nonsingular, and the result follows from (4.3.6) and
(4.3.7).
4.3.7. It is linearly independent because
α1

1
0
0
0

+ α2

1
1
0
0

+ α3

1
1
1
0

+ α4

1
1
1
1

=

0
0
0
0

⇐⇒α1



1
0
0
0


+ α2



1
1
0
0


+ α3



1
1
1
0


+ α4



1
1
1
1


=



0
0
0
0



⇐⇒



1
1
1
1
0
1
1
1
0
0
1
1
0
0
0
1






α1
α2
α3
α4


=



0
0
0
0


⇐⇒



α1
α2
α3
α4


=



0
0
0
0


.
4.3.8. A is nonsingular because it is diagonally dominant.
4.3.9. S is linearly independent using exact arithmetic, but using 3-digit arithmetic
yields the conclusion that S is dependent.
4.3.10. If e is the column vector of all 1’s, then Ae = 0, so that N (A) ̸= {0}.
4.3.11. (Solution 1.)

i αiPui = 0
=⇒
P 
i αiui = 0
=⇒

i αiui =
0
=⇒
each αi = 0 because the ui ’s are linearly independent.
(Solution 2.)
If Am×n is the matrix containing the ui ’s as columns, then
PA = B is the matrix containing the vectors in P(S) as its columns. Now,
A
row
∼B
=⇒
rank (B) = rank (A) = n,
and hence (4.3.3) insures that the columns of B are linearly independent. The
result need not be true if P is singular—take P = 0 for example.
4.3.12. If Am×n is the matrix containing the ui ’s as columns, and if
Qn×n =




1
1
· · ·
1
0
1
· · ·
1
...
...
...
...
0
0
· · ·
1



,

32
Solutions
then the columns of B = AQ are the vectors in S′. Clearly, Q is nonsingular
so that A
col
∼B, and thus rank (A) = rank (B). The desired result now follows
from (4.3.3).
4.3.13. (a) and (b) are linearly independent because the Wronski matrix W(0) is non-
singular in each case. (c) is dependent because sin2 x −cos2 x + cos 2x = 0.
4.3.14. If S were dependent, then there would exist a constant α such that x3 = α|x|3
for all values of x. But this would mean that
α = x3
|x|3 =

1
if x > 0,
−1
if x < 0,
which is clearly impossible since α must be constant. The associated Wronski
matrix is
W(x) =








x3
x3
3x2
3x2

when x ≥0,

x3
−x3
3x2
−3x2

when x < 0,
which is singular for all values of x.
4.3.15. Start with the fact that
AT diag. dom.
=⇒
|bii| > |di| +

j̸=i
|bji|
and
|α| >

j
|cj|
=⇒

j̸=i
|bji| < |bii| −|di|
and
1
|α|

j̸=i
|cj| < 1 −|ci|
|α| ,
and then use the forward and backward triangle inequality to write

j̸=i
|xij| =

j̸=i
bji −dicj
α
 ≤

j̸=i
|bji| + |di|
|α|

j̸=i
|cj|
<

|bii| −|di|
	
+ |di|

1 −|ci|
|α|

= |bii| −|di| |ci|
|α|
≤
bii −dici
α
 = |xii|.
Now, diagonal dominance of AT insures that α is the entry of largest magni-
tude in the ﬁrst column of A, so no row interchange is needed at the ﬁrst step
of Gaussian elimination. After one step, the diagonal dominance of X guar-
antees that the magnitude of the second pivot is maximal with respect to row
interchanges. Proceeding by induction establishes that no step requires partial
pivoting.

Solutions
33
Solutions for exercises in section 4. 4
4.4.1. dim R (A) = dim R

AT 	
= rank (A) = 2,
dim N (A) = n −r = 4 −2 = 2,
and dim N

AT 	
= m −r = 3 −2 = 1.
4.4.2. BR(A) =





1
3
2

,


0
1
1




,
BN(AT ) =





1
−1
1





BR(AT ) =














1
2
0
2
1




,





0
0
1
3
3














,
BN(A) =














−2
1
0
0
0




,





−2
0
−3
1
0




,





−1
0
−3
0
1














4.4.3. dim

span (S)
	
= 3
4.4.4. (a)
n + 1 (See Example 4.4.1)
(b)
mn
(c)
n2+n
2
4.4.5. Use the technique of Example 4.4.5. Find EA to determine









h1 =





−2
1
0
0
0




, h2 =





−2
0
1
1
0




, h3 =





−1
0
−2
0
1














is a basis for N (A). Reducing the matrix

v, h1, h2, h3

to row echelon
form reveals that its ﬁrst, second, and fourth columns are basic, and hence
{v, h1, h3} is a basis for N (A) that contains v.
4.4.6. Placing the vectors from A and B as rows in matrices and reducing
A =


1
2
3
5
8
7
3
4
1

−→EA =


1
0
−5
0
1
4
0
0
0


and
B =

2
3
2
1
1
−1

−→EB =

1
0
−5
0
1
4

shows A and B have the same row space (recall Example 4.2.2), and hence A
and B span the same space. Because B is linearly independent, it follows that
B is a basis for span (A) .
4.4.7. 3 = dim N (A) = n −r = 4 −rank (A)
=⇒
rank (A) = 1. Therefore, any
rank-one matrix with no zero entries will do the job.
4.4.8. If v = α1b1 + α2b2 + · · · + αnbn and v = β1b1 + β2b2 + · · · + βnbn, then
subtraction produces
0 = (α1 −β1)b1 + (α2 −β2)b2 + · · · + (αn −βn)bn.

34
Solutions
But B is a linearly independent set, so this equality can hold only if (αi−βi) = 0
for each i = 1, 2, . . . , n, and hence the αi ’s are unique.
4.4.9. Prove that if {s1, s2, . . . , sk} is a basis for S, then {As1, As2, . . . , Ask} is a
basis for A(S). The result of Exercise 4.1.9 insures that
span {As1, As2, . . . , Ask} = A(S),
so we need only establish the independence of {As1, As2, . . . , Ask}. To do this,
write
k

i=1
αi (Asi) = 0
=⇒
A
 k

i=1
αisi

= 0
=⇒
k

i=1
αisi ∈N (A)
=⇒
k

i=1
αisi = 0
because
S ∩N (A) = 0
=⇒
α1 = α2 = · · · = αk = 0
because {s1, s2, . . . , sk} is linearly independent. Since {As1, As2, . . . , Ask} is
a basis for A(S), it follows that dim A(S) = k = dim(S).
4.4.10. rank (A) = rank (A −B + B) ≤rank (A −B) + rank (B) implies that
rank (A) −rank (B) ≤rank (A −B).
Furthermore, rank (B) = rank (B −A + A) ≤rank (B −A) + rank (A) =
rank (A −B) + rank (A) implies that
−

rank (A) −rank (B)

≤rank (A −B).
4.4.11. Example 4.4.8 guarantees that rank (A + E) ≤rank (A) + rank (E) = r + k.
Use Exercise 4.4.10 to write
rank (A + E) = rank (A −(−E)) ≥rank (A) −rank (−E) = r −k.
4.4.12. Let v1 ∈V such that v1 ̸= 0. If span {v1} = V, then S1 = {v1} is an
independent spanning set for V, and we are ﬁnished. If span {v1} ̸= V, then
there is a vector
v2 ∈V such that v2 /∈span {v1} , and hence the extension set
S2 = {v1, v2} is independent. If span (S2) = V, then we are ﬁnished. Otherwise,
we can proceed as described in Example 4.4.5 and continue to build independent
extension sets S3, S4, . . . . Statement (4.3.16) guarantees that the process must
eventually yield a linearly independent spanning set Sk with k ≤n.
4.4.13. Since 0 = eT E = E1∗+E2∗+· · ·+Em∗, any row can be written as a combination
of the other m −1 rows, so any set of m −1 rows from E spans N

ET 	
.
Furthermore, rank (E) = m −1 insures that no fewer than m −1 vectors

Solutions
35
can span N

ET 	
, and therefore any set of m −1 rows from E is a minimal
spanning set, and hence a basis.
4.4.14.

EET 
ij = Ei∗

ET 	
∗j = Ei∗(Ej∗)T = 
k eikejk. Observe that edge Ek
touches node Ni if and only if eik = ±1 or, equivalently, e2
ik = 1. Thus

EET 
ii = 
k e2
ik = the number of edges touching Ni. If i ̸= j, then
eikejk =
 −1
if Ek is between Ni and Nj
0
if Ek is not between Ni and Nj
so that

EET 
ij = 
k eikejk = −(the number of edges between Ni and Nj ).
4.4.15. Apply (4.4.19) to span (M ∪N) = span (M) + span (N) (see Exercise 4.1.7).
4.4.16. (a) Exercise 4.2.9 says R (A | B) = R (A) + R (B). Since rank is the same as
dimension of the range, (4.4.19) yields
rank (A | B) = dim R (A | B) = dim

R (A) + R (B)

= dim R (A) + dim R (B) −dim

R (A) ∩R (B)

= rank (A) + rank (B) −dim

R (A) ∩R (B)

.
(b) Use the results of part (a) to write
dim N (A | B) = n + k −rank (A | B)
=

n −rank (A)

+

k −rank (B)

+ dim

R (A) ∩R (B)

= dim N (A) + dim N (B) + dim

R (A) ∩R (B)

.
(c) Let A =





−1
1
−2
−1
0
−4
−1
0
−5
−1
0
−6
−1
0
−6




and B =





3
−2
2
−1
1
0
0
1
0
1




contain bases for R (C)
and N (C), respectively, so that R (A) = R (C) and R (B) = N (C). Use
either part (a) or part (b) to obtain
dim

R (C) ∩N (C)

= dim

R (A) ∩R (B)

= 2.
Using R (A | B) = R (A) + R (B) produces
dim

R (C) + N (C)

= dim

R (A) + R (B)

= rank (A | B) = 3.
4.4.17. Suppose A is m × n. Existence of a solution for every b implies R (A) = ℜm.
Recall from §2.5 that uniqueness of the solution implies rank (A) = n. Thus
m = dim R (A) = rank (A) = n so that A is m × m of rank m.

36
Solutions
4.4.18. (a) x ∈S
=⇒x ∈span (Smax) —otherwise, the extension set E = Smax∪{x}
would be linearly independent—which is impossible because E would contain
more independent solutions than Smax. Now show span (Smax) ⊆span {p} +
N (A). Since S = p + N (A) (see Exercise 4.2.10), si ∈S means there must
exist a corresponding vector ni ∈N (A) such that si = p + ni, and hence
x ∈span (Smax)
=⇒
x =
t

i=1
αisi =
t

i=1
αi (p + ni) =
t

i=1
αip +
t

i=1
αini
=⇒
x ∈span {p} + N (A)
=⇒
span (Smax) ⊆span {p} + N (A).
To prove the reverse inclusion, observe that if x ∈span {p}+N (A), then there
exists a scalar α and a vector n ∈N (A) such that
x = αp + n = (α −1)p + (p + n).
Because p and (p + n) are both solutions, S ⊆span(Smax) guarantees that
p and (p + n) each belong to span (Smax) , and the closure properties of a
subspace insure that x ∈span (Smax) . Thus span {p}+N (A) ⊆span (Smax) .
(b) The problem is really to determine the value of t in Smax. The fact that
Smax is a basis for span (Smax) together with (4.4.19) produces
t = dim

span (Smax)
	
= dim

span {p} + N (A)

= dim

span {p}
	
+ dim N (A) −dim

span {p} ∩N (A)

= 1 + (n −r) −0.
4.4.19. To show Smax is linearly independent, suppose
0 = α0p +
n−r

i=1
αi (p + hi) =
n−r

i=0
αi

p +
n−r

i=1
αihi.
Multiplication by A yields 0 =

n−r
i=0 αi

b, which implies 
n−r
i=0 αi = 0,
and hence 
n−r
i=1 αihi = 0. Because H is independent, we may conclude that
α1 = α2 = · · · = αn−r = 0. Consequently, α0p = 0, and therefore α0 = 0
(because p ̸= 0 ), so that Smax is an independent set. By Exercise 4.4.18, it
must also be maximal because it contains n −r + 1 vectors.
4.4.20. The proof depends on the observation that if B = PT AP, where P is a per-
mutation matrix, then the graph G(B) is the same as G(A) except that the
nodes in G(B) have been renumbered according to the permutation deﬁning
P. This follows because PT = P−1 implies A = PBPT , so if the rows (and

Solutions
37
columns) in P are the unit vectors that appear according to the permutation
π =

1
2
· · ·
n
π1
π2
· · ·
πn

, then
aij =

PBPT 
ij =





eT
π1
...
eT
πn


B ( eπ1
· · ·
eπn )


ij
= eT
πiBeπj = bπiπj.
Consequently, aij ̸= 0 if and only if bπiπj ̸= 0, and thus G(A) and G(B)
are the same except for the fact that node Nk in G(A) is node Nπk in G(B)
for each k = 1, 2, . . . , n. Now we can prove G(A) is not strongly connected
⇐⇒A is reducible. If A is reducible, then there is a permutation matrix such
that PT AP = B =

X
Y
0
Z

, where X is r × r and Z is n −r × n −r.
The zero pattern in B indicates that the nodes {N1, N2, . . . , Nr} in G(B) are
inaccessible from nodes {Nr+1, Nr+2, . . . , Nn} , and hence G(B) is not strongly
connected—e.g., there is no sequence of edges leading from Nr+1 to N1. Since
G(B) is the same as G(A) except that the nodes have diﬀerent numbers, we
may conclude that G(A) is also not strongly connected. Conversely, if G(A)
is not strongly connected, then there are two nodes in G(A) such that one
is inaccessible from the other by any sequence of directed edges. Relabel the
nodes in G(A) so that this pair is N1 and Nn, where N1 is inaccessible
from Nn. If there are additional nodes—excluding Nn itself—which are also
inaccessible from Nn, label them N2, N3, . . . , Nr so that the set of all nodes that
are inaccessible from Nn —with the possible exception of Nn itself—is Nn =
{N1, N2, . . . , Nr} (inaccessible nodes). Label the remaining nodes—which are
all accessible from Nn —as Nn = {Nr+1, Nr+2, . . . , Nn−1} (accessible nodes).
It follows that no node in Nn can be accessible from any node in Nn, for
otherwise nodes in Nn would be accessible from Nn through nodes in Nn.
In other words, if Nr+k ∈Nn and Nr+k →Ni ∈Nn, then Nn →Nr+k →
Ni, which is impossible. This means that if π =

1
2
· · ·
n
π1
π2
· · ·
πn

is the
permutation generated by the relabeling process, then aπiπj = 0 for each i =
r+1, r+2, . . . , n−1 and j = 1, 2, . . . , r. Therefore, if B = PT AP, where P is
the permutation matrix corresponding to the permutation π, then bij = aπiπj,
so PT AP = B =

X
Y
0
Z

, where X is r × r and Z is n −r × n −r, and
thus A is reducible.
Solutions for exercises in section 4. 5
4.5.1. rank

AT A
	
= rank (A) = rank

AAT 	
= 2
4.5.2. dim N (A) ∩R (B) = rank (B) −rank (AB) = 2 −1 = 1.

38
Solutions
4.5.3. Gaussian elimination yields X =

1
1
−1
1
2
2

,
V =
 1
1

, and XV =
 2
0
4

.
4.5.4. Statement (4.5.2) says that the rank of a product cannot exceed the rank of any
factor.
4.5.5. rank (A) = rank

AT A
	
= 0
=⇒
A = 0.
4.5.6. rank (A) = 2, and there are six 2 × 2 nonsingular submatrices in A.
4.5.7. Yes. A =

1
1
1
1

and B =

1
1
−1
−1

is one of many examples.
4.5.8. No—it is not diﬃcult to construct a counterexample using two singular matrices.
If either matrix is nonsingular, then the statement is true.
4.5.9. Transposition does not alter rank, so (4.5.1) says
rank (AB) = rank(AB)T = rank

BT AT 	
= rank

AT 	
−dim N

BT 	
∩R

AT 	
= rank (A) −dim N

BT 	
∩R

AT 	
.
4.5.10. This follows immediately from (4.5.1) because dim N (AB) = p −rank (AB)
and dim N (B) = p −rank (B).
4.5.11. (a)
First notice that N (B) ⊆N (AB) (Exercise 4.2.12) for all conformable A
and B, so, by (4.4.5), dim N (B) ≤dim N (AB), or ν(B) ≤ν(AB), is always
true—this also answers the second half of part (b). If A and B are both n × n,
then the rank-plus-nullity theorem together with (4.5.2) produces
ν(A) = dim N (A) = n −rank (A) ≤n −rank (AB) = dim N (AB) = ν(AB),
so, together with the ﬁrst observation, we have max {ν(A), ν(B)} ≤ν(AB).
The rank-plus-nullity theorem applied to (4.5.3) yields ν(AB) ≤ν(A) + ν(B).
(b)
To see that ν(A) > ν(AB) is possible for rectangular matrices, consider
A = ( 1
1 ) and B =

1
1

.
4.5.12. (a)
rank (Bn×p) = n
=⇒
R (B) = ℜn
=⇒
N (A)∩R (B) = N (A)
=⇒
rank (AB) = rank (B) −dim N (A) ∩R (B) = n −dim N (A)
= n −(n −rank (A)) = rank (A)
It’s always true that R (AB) ⊆R (A). When dim R (AB) = dim R (A) (i.e.,
when rank (AB) = rank (A) ), (4.4.6) implies R (AB) = R (A).
(b)
rank (Am×n) = n =⇒N (A) = {0} =⇒N (A) ∩R (B) = {0} =⇒
rank (AB) = rank (B) −dim N (A) ∩R (B) = rank (B)
Assuming the product exists, it is always the case that N (B) ⊆N (AB). Use
rank (B) = rank (AB) =⇒p−rank (B) = p−rank (AB) =⇒dim N (B) =
dim N (AB) together with (4.4.6) to conclude that N (B) = N (AB).

Solutions
39
4.5.13. (a)
rank (A) = 2, and the unique exact solution is (−1, 1).
(b)
Same as part (a).
(c)
The 3-digit rank is 2, and the unique 3-digit solution is (−1, 1).
(d)
The 3-digit normal equations

6
12
12
24
 
x1
x2

=

6.01
12

have inﬁnitely
many 3-digit solutions.
4.5.14. Use an indirect argument. Suppose x ∈N (I + F) in which xi ̸= 0 is a compo-
nent of maximal magnitude. Use the triangle inequality together with x = −Fx
to conclude
|xi| =

r

j=1
fijxj
 ≤
r

j=1
|fijxj| =
r

j=1
|fij| |xj| ≤

r

j=1
|fij|

|xi| < |xi|,
which is impossible. Therefore, N (I + F) = 0, and hence I + F is nonsingular.
4.5.15. Follow the approach used in (4.5.8) to write
A ∼

W
0
0
S

,
where
S = Z −YW−1X.
rank (A) = rank (W)
=⇒
rank (S) = 0
=⇒
S = 0, so Z = YW−1X.
The desired conclusion now follows by taking B = YW−1 and C = W−1X.
4.5.16. (a)
Suppose that A is nonsingular, and let Ek = Ak−A so that lim
k→∞Ek = 0.
This together with (4.5.9) implies there exists a suﬃciently large value of k such
that
rank (Ak) = rank (A + Ek) ≥rank (A) = n,
which is impossible because each Ak is singular. Therefore, the supposition that
A is nonsingular must be false.
(b)
No!—consider
 1
k

1×1 →[0].
4.5.17. M ⊆N
because R (BC) ⊆R (B), and therefore dim M ≤dim N. For-
mula (4.5.1) guarantees dim M = rank (BC) −rank (ABC) and dim N =
rank (B) −rank (AB), so the desired conclusion now follows.
4.5.18. N (A) ⊆N

A2	
and R

A2	
⊆R (A) always hold, so (4.4.6) insures
N (A) = N

A2	
⇐⇒dim N (A) = dim N

A2	
⇐⇒n −rank (A) = n −rank

A2	
⇐⇒rank (A) = rank

A2	
⇐⇒R (A) = R

A2	
.
Formula (4.5.1) says rank

A2	
= rank (A) −dim R (A) ∩N (A), so
R

A2	
= R (A) ⇐⇒rank

A2	
= rank (A) ⇐⇒dim R (A) ∩N (A) = 0.

40
Solutions
4.5.19. (a)
Since

A
B

(A + B)(A | B) =

A
B

(A | B) =

A
0
0
B

,
the result of Example 3.9.3 together with (4.5.2) insures
rank (A) + rank (B) ≤rank (A + B).
Couple this with the fact that rank (A + B) ≤rank (A) + rank (B) (see Ex-
ample 4.4.8) to conclude rank (A + B) = rank (A) + rank (B).
(b)
Verify that if B = I −A, then B2 = B and AB = BA = 0, and apply
the result of part (a).
4.5.20. (a)
BT ACT = BT BCCT . The products BT B and CCT are each nonsin-
gular because they are r × r with
rank

BT B
	
= rank (B) = r
and
rank

CCT 	
= rank (C) = r.
(b)
Notice that A† = CT 
BT BCCT 	−1BT = CT 
CCT 	−1
BT B
	−1BT , so
AT AA†b = CT BT BCCT 
CCT 	−1
BT B
	−1BT b = CT BT b = AT b.
If Ax = b is consistent, then its solution set agrees with the solution set for the
normal equations.
(c)
AA†A = BCCT 
CCT 	−1
BT B
	−1BT BC = BC = A. Now,
x ∈R

I −A†A
	
=⇒
x =

I −A†A
	
y
for some y
=⇒
Ax =

A −AA†A
	
y = 0
=⇒
x ∈N (A).
Conversely,
x ∈N (A)
=⇒
Ax = 0
=⇒
x =

I −A†A
	
x
=⇒
x ∈R

I −A†A
	
,
so R

I −A†A
	
= N (A). As h ranges over all of ℜn×1, the expression

I −A†A
	
h generates R

I −A†A
	
= N (A). Since A†b is a particular solu-
tion of AT Ax = AT b, the general solution is
x = A†b + N (A) = A†b +

I −A†A
	
h.
(d)
If r = n, then B = A and C = In.
(e)
If A is nonsingular, then so is AT , and
A† =

AT A
	−1AT = A−1
AT 	−1AT = A−1.
(f)
Follow along the same line as indicated in the solution to part (c) for the
case AA†A = A.

Solutions
41
Solutions for exercises in section 4. 6
4.6.1. A =





5
7
8
10
12




and b =





11.1
15.4
17.5
22.0
26.3




, so AT A = 382 and AT b = 838.9. Thus the
least squares estimate for k is 838.9/382 = 2.196.
4.6.2. This is essentially the same problem as Exercise 4.6.1. Because it must pass
through the origin, the equation of the least squares line is y = mx, and hence
A =




x1
x2
...
xn



and b =




y1
y2
...
yn



, so AT A = 
i x2
i and AT b = 
i xiyi.
4.6.3. Look for the line p = α + βt that comes closest to the data in the least squares
sense. That is, ﬁnd the least squares solution for the system Ax = b, where
A =


1
1
1
2
1
3

,
x =

α
β

,
and
b =


7
4
3

.
Set up normal equations AT Ax = AT b to get

3
6
6
14
 
α
β

=

14
24

=⇒

α
β

=

26/3
−2

=⇒
p = (26/3) −2t.
Setting p = 0 gives t = 13/3. In other words, we expect the company to begin
losing money on May 1 of year ﬁve.
4.6.4. The associated linear system Ax = b is
Year 1:
α + β = 1
Year 2:
2α = 1
Year 3:
−β = 1
or


1
1
2
0
0
−1



α
β

=


1
1
1

.
The least squares solution to this inconsistent system is obtained from the system
of normal equations AT Ax = AT b that is

5
1
1
2
 
α
β

=

3
0

. The unique
solution is

α
β

=

2/3
−1/3

, so the least squares estimate for the increase in
bread prices is
B = 2
3W −1
3M.
When W = −1 and M = −1, we estimate that B = −1/3.
4.6.5. (a)
α0 = .02 and α1 = .0983.
(b)
1.986 grams.

42
Solutions
4.6.6. Use ln y = ln α0 + α1t to obtain the least squares estimates α0 = 9.73 and
α1 = .507.
4.6.7. The least squares line is y = 9.64 + .182x and for εi = 9.64 + .182xi −yi, the
sum of the squares of these errors is 
i ε2
i = 162.9. The least squares quadratic
is y = 13.97 + .1818x −.4336x2, and the corresponding sum of squares of the
errors is 
 ε2
i = 1.622. Therefore, we conclude that the quadratic provides a
much better ﬁt.
4.6.8. 230.7 min. (α0 = 492.04, α1 = −23.435, α2 = −.076134, α3 = 1.8624)
4.6.9. x2 is a least squares solution
=⇒
AT Ax2 = AT b
=⇒
0 = AT (b −Ax2).
If we set x1 = b −Ax2, then
 Im×m
A
AT
0n×n
  x1
x2

=
 Im×m
A
AT
0n×n
  b −Ax2
x2

=
 b
0

.
The converse is true because
 Im×m
A
AT
0n×n
  x1
x2

=
 b
0

=⇒
Ax2 = b −x1
and
AT x1 = 0
=⇒
AT Ax2 = AT b −AT x1 = AT b.
4.6.10. t ∈R

AT 	
= R

AT A
	
=⇒tT = zT AT A for some z. For each x satisfying
AT Ax = AT b, write
ˆy = tT x = zT AT Ax = zT AT b,
and notice that zT AT b is independent of x.
Solutions for exercises in section 4. 7
4.7.1. (b) and (f)
4.7.2. (a), (c), and (d)
4.7.3. Use any x to write T(0) = T(x −x) = T(x) −T(x) = 0.
4.7.4. (a)
4.7.5. (a) No
(b) Yes
4.7.6. T(u1) = (2, 2) = 2u1 + 0u2 and T(u2) = (3, 6) = 0u1 + 3u2 so that [T]B =

2
0
0
3

.
4.7.7. (a) [T]SS′ =


1
3
0
0
2
−4


(b) [T]SS′′ =


2
−4
0
0
1
3


4.7.8. [T]B =


1
−3/2
1/2
−1
1/2
1/2
0
1/2
−1/2

and [v]B =


1
1
0

.

Solutions
43
4.7.9. According to (4.7.4), the jth column of [T]S is
[T(ej)]S = [Aej]S = [A∗j]S = A∗j.
4.7.10. [Tk]B = [TT · · · T]B = [T]B[T]B · · · [T]B = [T]k
B
4.7.11. (a)
Sketch a picture to observe that P(e1) =

x
x

= P(e2) and that the
vectors e1,
P(e1), and 0 are vertices of a 45◦right triangle (as are e2,
P(v2), and 0 ). So, if ∥⋆∥denotes length, the Pythagorean theorem may be
applied to yield 1 = 2 ∥P(e1)∥2 = 4x2 and 1 = 2 ∥P(e2)∥2 = 4x2. Thus







P(e1) =

1/2
1/2

= (1/2)e1 + (1/2)e2
P(e2) =

1/2
1/2

= (1/2)e1 + (1/2)e2







=⇒
[P]S =

1/2
1/2
1/2
1/2

.
(b)
P(v) =
 α+β
2
α+β
2

4.7.12. (a)
If U1 =

1
0
0
0

,
U2 =

0
1
0
0

,
U3 =

0
0
1
0

,
U4 =

0
0
0
1

,
then
T(U1) = U1 + 0U2 + 0U3 + 0U4,
T(U2) = 1
2

0
1
1
0

= 0U1 + 1/2U2 + 1/2U3 + 0U4,
T(U3) = 1
2

0
1
1
0

= 0U1 + 1/2U2 + 1/2U3 + 0U4,
T(U4) = 0U1 + 0U2 + 0U3 + U4,
so [T]S =



1
0
0
0
0
1/2
1/2
0
0
1/2
1/2
0
0
0
0
1


. To verify [T(U)]S = [T]S[U]S, observe that
T(U) =

a
(b + c)/2
(b + c)/2
d

, [T(U)]S =



a
(b + c)/2
(b + c)/2
d


, [U]S =



a
b
c
d


.
(b)
For U1, U2, U3, and U4 as deﬁned above,

44
Solutions
T(U1) =

0
−1
−1
0

= 0U1 −U2 −U3 + 0U4,
T(U2) =

1
2
0
−1

= U1 + 2U2 + 0U3 −U4,
T(U3) =

1
0
−2
−1

= U1 + 0U2 −2U3 −1U4,
T(U4) =

0
1
1
0

= 0U1 + U2 + U3 + 0U4,
so [T]S =



0
1
1
0
−1
2
0
1
−1
0
−2
1
0
−1
−1
0


. To verify [T(U)]S = [T]S[U]S, observe that
T(U) =

c + b
−a + 2b + d
−a −2c + d
−b −c

and [T(U)]S =



c + b
−a + 2b + d
−a −2c + d
−b −c


.
4.7.13. [S]BB′ =



0
0
0
1
0
0
0
1/2
0
0
0
1/3



4.7.14. (a) [RQ]S = [R]S[Q]S =

1
0
0
−1
 
cos θ
−sin θ
sin θ
cos θ

=

cos θ
−sin θ
−sin θ
−cos θ

(b)
[QQ]S = [Q]S[Q]S =

cos2 θ −sin2 θ
−2 cos θ sin θ
2 cos θ sin θ
cos2 θ −sin2 θ

=

cos 2θ
−sin 2θ
sin 2θ
cos 2θ

4.7.15. (a)
Let B = {ui}n
i=1,
B′ = {vi}m
i=1. If [P]BB′ = [αij] and [Q]BB′ = [βij],
then P(uj) = 
i αijvi and Q(uj) = 
i βijvi. Thus (P+Q)(uj) = 
i(αij +
βij)vi and hence [P + Q]BB′ = [αij +βij] = [αij]+[βij] = [P]BB′ +[Q]BB′. The
proof of part (b) is similar.
4.7.16. (a)
If B = {xi}n
i=1 is a basis, then I(xj) = 0x1 + 0x2 + · · · + 1xj + · · · + 0xn
so that the jth column in [I]B is just the jth unit column.
(b)
Suppose xj = 
i βijyi so that [xj]B′ =



β1j
...
βnj


. Then
I(xj) = xj =

i
βijyi
=⇒
[I]BB′ = [βij] =

[x1]B′
 [x2]B′
 · · ·
 [xn]B′

.
Furthermore, T(yj) = xj = 
i βijyi
=⇒
[T]B′ = [βij], and
T(xj) = T
 
i
βijyi

=

i
βijT(yi) =

i
βijxi
=⇒
[T]B = [βij].

Solutions
45
(c)


1
−1
0
0
1
−1
0
0
1


4.7.17. (a)
T−1(x, y, z) = (x + y + z, x + 2y + 2z, x + 2y + 3z)
(b)
[T−1]S =


1
1
1
1
2
2
1
2
3

= [T]−1
S
4.7.18. (1) =⇒(2) :
T(x) = T(y)
=⇒
T(x−y) = 0
=⇒
(y−x) = T−1(0) = 0.
(2)
=⇒
(3) :
T(x) = 0 and T(0) = 0
=⇒
x = 0.
(3)
=⇒
(4) :
If {ui}n
i=1 is a basis for V, show that N (T) = {0} implies
{T(ui)}n
i=1 is also a basis. Consequently, for each v ∈V there are coordinates
ξi such that
v =

i
ξiT(ui) = T
 
i
ξiui

.
(4)
=⇒
(2) :
For each basis vector ui, there is a vi such that T(vi) = ui.
Show that {vi}n
i=1 is also a basis. If T(x) = T(y), then T(x −y) = 0.
Let x −y = 
i ξivi so that 0 = T(x −y) = T
 
i ξivi

= 
i ξiT(vi) =

i ξiui
=⇒
each ξi = 0
=⇒
x −y = 0
=⇒
x = y.
(4) and (2)
=⇒
(1) :
For each y ∈V, show there is a unique x such
that T(x) = y. Let ˆT be the function deﬁned by the rule ˆT(y) = x. Clearly,
TˆT = ˆTT = I. To show that ˆT is a linear function, consider αy1 +y2, and let
x1 and x2 be such that T(x1) = y1, T(x2) = y2. Now, T(αx1+x2) = αy1+y2
so that ˆT(αy1 + y2) = αx1 + x2. However, x1 = ˆT(y1), x2 = ˆT(y2) so that
α ˆT(y1) + ˆT(y2) = αx1 + x2 = ˆT(αy1 + y2). Therefore ˆT = T−1.
4.7.19. (a)
0 = 
i αixi ⇐⇒


0
...
0

= [0]B =
% 
i αixi
&
B = 
i[αixi]B = 
i αi[xi]B
(b)
G =
'
T(u1), T(u2), . . . , T(un)
(
spans R (T). From part (a), the set
'
T(ub1), T(ub2), . . . , T(ubr)
(
is a maximal independent subset of G if and only if the set
'
[T(ub1)]B, [T(ub2)]B, . . . , [T(ubr)]B
(
is a maximal linearly independent subset of
)
[T(u1)]B, [T(u2)]B, . . . , [T(un)]B
*
,
which are the columns of [T]B.

46
Solutions
Solutions for exercises in section 4. 8
4.8.1. Multiplication by nonsingular matrices does not change rank.
4.8.2. A = Q−1BQ
and
B = P−1CP
=⇒
A = (PQ)−1C(PQ).
4.8.3. (a) [A]S =


1
2
−1
0
−1
0
1
0
7


(b) [A]S′ =


1
4
3
−1
−2
−9
1
1
8

and Q =


1
1
1
0
1
1
0
0
1


4.8.4. Put the vectors from B into a matrix Q and compute
[A]B = Q−1AQ =


−2
−3
−7
7
9
12
−2
−1
0

.
4.8.5. [B]S = B and [B]S′ = C. Therefore, C = Q−1BQ, where Q =

2
−3
−1
2

is the change of basis matrix from S′ to S.
4.8.6. If B = {u, v} is such a basis, then T(u) = 2u and T(v) = 3v. For u =
(u1, u2), T(u) = 2u implies
−7u1 −15u2 = 2u1
6u1 + 12u2 = 2u2,
or
−9u1 −15u2 = 0
6u1 + 10u2 = 0,
so u1 = (−5/3)u2 with u2 being free. Letting u2 = −3 produces u = (5, −3).
Similarly, a solution to T(v) = 3v is v = (−3, 2). [T]S =

−7
−15
6
12

and
[T]B =

2
0
0
3

. For Q =

5
−3
−3
2

, [T]B = Q−1[T]SQ.
4.8.7. If sin θ = 0, the result is trivial. Assume sin θ ̸= 0. Notice that with respect
to the standard basis S, [P]S = R. This means that if R and D are to be
similar, then there must exist a basis B = {u, v} such that [P]B = D, which
implies that P(u) = eiθu and P(v) = e−iθv. For u = (u1, u2), P(u) = eiθu
implies
u1 cos θ −u2 sin θ = eiθu1 = u1 cos θ + iu1 sin θ
u1 sin θ + u2 cos θ = eiθu2 = u2 cos θ + iu2 sin θ,
or
iu1 + u2 = 0
u1 −iu2 = 0,

Solutions
47
so u1 = iu2 with u2 being free. Letting u2 = 1 produces u = (i, 1). Similarly,
a solution to P(v) = e−iθv is v = (1, i). Now, [P]S = R and [P]B = D so
that R and D must be similar. The coordinate change matrix from B to S
is Q =

i
1
1
i

, and therefore D = Q−1RQ.
4.8.8. (a) B = Q−1CQ
=⇒
(B −λI) = Q−1CQ−λQ−1Q = Q−1 (C −λI) Q. The
result follows because multiplication by nonsingular matrices does not change
rank.
(b) B = P−1DP
=⇒
B −λiI = P−1(D −λiI)P and (D −λiI) is singular
for each λi. Now use part (a).
4.8.9. B = P−1AP
=⇒
Bk = P−1APP−1AP · · · P−1AP = P−1AA · · · AP =
P−1AkP
4.8.10. (a) YT Y is nonsingular because rank

YT Y
	
n×n = rank (Y) = n. If
[v]B =


α1
...
αn


and
[v]B′ =



β1
...
βn


,
then
v =

i
αixi = X[v]B
and
v =

i
βiyi = Y[v]B′
=⇒
X[v]B = Y[v]B′
=⇒
YT X[v]B = YT Y[v]B′
=⇒
(YT Y)−1YT X[v]B = [v]B′.
(b) When m = n, Y is square and (YT Y)−1YT = Y−1 so that P = Y−1X.
4.8.11. (a) Because B contains n vectors, you need only show that B is linearly in-
dependent. To do this, suppose 
n−1
i=0 αiNi(y) = 0 and apply Nn−1 to both
sides to get α0Nn−1(y) = 0
=⇒
α0 = 0. Now 
n−1
i=1 αiNi(y) = 0. Apply
Nn−2 to both sides of this to conclude that α1 = 0. Continue this process until
you have α0 = α1 = · · · = αn−1 = 0.
(b) Any n × n nilpotent matrix of index n can be viewed as a nilpotent operator
of index n on ℜn. Furthermore, A = [A]S and B = [B]S, where S is the
standard basis. According to part (a), there are bases B and B′ such that
[A]B = J and [B]B′ = J. Since [A]S ≃[A]B, it follows that A ≃J. Similarly
B ≃J, and hence A ≃B by Exercise 4.8.2.
(c) Trace and rank are similarity invariants, and part (a) implies that every
n × n nilpotent matrix of index n is similar to J, and trace (J) = 0 and
rank (J) = n −1.
4.8.12. (a) xi ∈R (E)
=⇒
xi = E(vi) for some vi
=⇒
E(xi) = E2(vi) =
E(vi) = xi. Since B contains n vectors, you need only show that B is linearly
independent. 0 = 
i αixi + βiyi
=⇒
0 = E(0) = 
i αiE(xi) + βiE(yi) =

i αixi
=⇒
αi ’s = 0
=⇒

i βiyi = 0
=⇒
βi ’s = 0.

48
Solutions
(b) Let B = X ∪Y = {b1, b2, . . . , bn}. For j = 1, 2, . . . , r, the jth column
of [E]B is [E(bj)]B = [E(xj)]B = ej. For j = r + 1, r + 2, . . . , n, [E(bj)]B =
[E(yj−r)]B = [0]B = 0.
(c) Suppose that B and C are two idempotent matrices of rank r. If you
regard them as linear operators on ℜn, then, with respect to the standard basis,
[B]S = B and [C]S = C. You know from part (b) that there are bases U and
V such that [B]U = [C]V =

Ir
0
0
0

= P. This implies that B ≃P, and
P ≃C. From Exercise 4.8.2, it follows that B ≃C.
(d) It follows from part (c) that F ≃P =

Ir
0
0
0

. Since trace and rank are
similarity invariants, trace (F) = trace (P) = r = rank (P) = rank (F).
Solutions for exercises in section 4. 9
4.9.1. (a)
Yes, because T(0) = 0.
(b)
Yes, because x ∈V
=⇒
T(x) ∈V.
4.9.2. Every subspace of V is invariant under I.
4.9.3. (a)
X is invariant because x ∈X ⇐⇒x = (α, β, 0, 0) for α, β ∈ℜ, so
T(x) = T(α, β, 0, 0) = (α + β, β, 0, 0) ∈X.
(b)
%
T/X
&
{e1,e2} =

1
1
0
1

(c)
[T]B =




1
1
∗
∗
0
1
∗
∗
0
0
∗
∗
0
0
∗
∗




4.9.4. (a)
Q is nonsingular.
(b)
X is invariant because
T(α1Q∗1 + α2Q∗2) = α1



1
1
−2
3


+ α2



1
2
−2
2


= α1Q∗1 + α2(Q∗1 + Q∗2)
= (α1 + α2)Q∗1 + α2Q∗2 ∈span {Q∗1, Q∗2} .
Y is invariant because
T(α3Q∗3 + α4Q∗4) = α3



0
0
0
0


+ α4



0
3
1
−4


= α4Q∗3 ∈span {Q∗3, Q∗4} .
(c)
According to (4.9.10), Q−1TQ should be block diagonal.

Solutions
49
(d)
Q−1TQ =




1
1
0
0
0
1
0
0
0
0
0
1
0
0
0
0



=


%
T/X
&
{Q∗1,Q∗2}
0
0
%
T/Y
&
{Q∗3,Q∗4}


4.9.5. If A = [αij] and C = [γij], then
T(uj) =
r

i=1
αijui ∈U
and
T(wj) =
q

i=1
γijwi ∈W.
4.9.6. If S is the standard basis for ℜn×1, and if B is the basis consisting of the
columns of P, then
[T]B = P−1[T]SP = P−1TP =

A
0
0
C

.
(Recall Example 4.8.3.) The desired conclusion now follows from the result of
Exercise 4.9.5.
4.9.7. x ∈N (A −λI)
=⇒
(A −λI) x = 0
=⇒
Ax = λx ∈N (A −λI)
4.9.8. (a)
(A −λI) is singular when λ = −1 and λ = 3.
(b)
There are four invariant subspaces—the trivial space {0}, the entire space
ℜ2, and the two one-dimensional spaces
N (A + I) = span

1
2
+
and
N (A −3I) = span

1
3
+
.
(c)
Q =

1
1
2
3


50
Solutions
Clearly spoken, Mr. Fogg; you explain English by Greek.
— Benjamin Franklin (1706–1790)

Solutions for Chapter 5
Solutions for exercises in section 5. 1
5.1.1. (a)
∥x∥1 = 9,
∥x∥2 = 5,
∥x∥∞= 4
(b)
∥x∥1 = 5 + 2
√
2,
∥x∥2 =
√
21,
∥x∥∞= 4
5.1.2. (a)
∥u −v∥=
√
31
(b)
∥u + v∥=
√
27 ≤7 = ∥u∥+ ∥v∥
(c)
|uT v| = 1 ≤10 = ∥u∥∥v∥
5.1.3. Use the CBS inequality with x =




α1
α2
...
αn



and y =




1
1
...
1



.
5.1.4. (a)
)
x ∈ℜn  ∥x∥2 ≤1
*
(b)
)
x ∈ℜn  ∥x −c∥2 ≤ρ
*
5.1.5. ∥x −y∥2 = ∥x + y∥2
=⇒
−2xT y = 2xT y
=⇒
xT y = 0.
5.1.6. ∥x −y∥= ∥(−1)(y −x)∥= |(−1)| ∥y −x∥= ∥y −x∥
5.1.7. x−y = 
n
i=1(xi −yi)ei
=⇒
∥x −y∥≤
n
i=1 |xi −yi| ∥ei∥≤ν 
n
i=1 |xi −yi|,
where ν = maxi ∥ei∥. For each ϵ > 0, set δ = ϵ/nν. If |xi −yi| < δ for each
i, then, using (5.1.6),
 ∥x∥−∥y∥
 ≤∥x −y∥< νnδ = ϵ.
5.1.8. To show that ∥x∥1 ≤√n ∥x∥2 , apply the CBS inequality to the standard inner
product of a vector of all 1’s with a vector whose components are the |xi| ’s.
5.1.9. If y = αx, then |x∗y| = |α| ∥x∥2 = ∥x∥∥y∥. Conversely, if |x∗y| = ∥x∥∥y∥,
then (5.1.4) implies that ∥αx −y∥= 0, and hence αx −y = 0 —recall (5.1.1).
5.1.10. If y = αx for α > 0, then ∥x + y∥= ∥(1 + α)x∥= (1 + α) ∥x∥= ∥x∥+ ∥y∥.
Conversely, ∥x + y∥= ∥x∥+ ∥y∥
=⇒
(∥x∥+ ∥y∥)2 = ∥x + y∥2
=⇒
∥x∥2 + 2 ∥x∥∥y∥+ ∥y∥2 = (x∗+ y∗) (x + y)
= x∗x + x∗y + y∗x + y∗y
= ∥x∥2 + 2 Re(x∗y) + ∥y∥2 ,
and hence ∥x∥∥y∥= Re (x∗y) . But it’s always true that Re (x∗y) ≤
x∗y
,
so the CBS inequality yields
∥x∥∥y∥= Re (x∗y) ≤
x∗y
 ≤∥x∥∥y∥.
In other words,
x∗y
 = ∥x∥∥y∥. We know from Exercise 5.1.9 that equality
in the CBS inequality implies y = αx, where α = x∗y/x∗x. We now need to
show that this α is real and positive. Using y = αx in the equality ∥x + y∥=

52
Solutions
∥x∥+ ∥y∥produces |1 + α| = 1 + |α|, or |1 + α|2 = (1 + |α|)2 . Expanding this
yields
(1 + ¯α)(1 + α) = 1 + 2|α| + |α|2
=⇒
1 + 2 Re(α) + ¯αα = 1 + 2|α| + ¯αα
=⇒
Re(α) = |α|,
which implies that α must be real. Furthermore, α = Re (α) = |α| ≥0. Since
y = αx and y ̸= 0, it follows that α ̸= 0, and therefore α > 0.
5.1.11. This is a consequence of H¨older’s inequality because
|xT y| = |xT (y −αe)| ≤∥x∥1 ∥y −αe∥∞
for all α, and minα ∥y −αe∥∞= (ymax −ymin)/2 (with the minimum being
attained at α = (ymax + ymin)/2 ).
5.1.12. (a)
It’s not diﬃcult to see that f ′(t) < 0 for t < 1, and f ′(t) > 0 for t > 1,
so we can conclude that f(t) > f(1) = 0 for t ̸= 1. The desired inequality
follows by setting t = α/β.
(b)
This inequality follows from the inequality of part (a) by setting
α = |ˆxi|p,
β = |ˆyi|q,
λ = 1/p,
and
(1 −λ) = 1/q.
(c)
H¨older’s inequality results from part (b) by setting ˆxi = xi/ ∥x∥p and
ˆyi = yi/ ∥y∥q . To obtain the “vector form” of the inequality, use the triangle
inequality for complex numbers to write
|x∗y| =

n

i=1
xiyi
 ≤
n

i=1
|xi| |yi| =
n

i=1
|xiyi| ≤
 n

i=1
|xi|p
1/p  n

i=1
|yi|q
1/q
= ∥x∥p ∥y∥q .
5.1.13. For p = 1, Minkowski’s inequality is a consequence of the triangle inequality
for scalars. The inequality in the hint follows from the fact that p = 1 + p/q
together with the scalar triangle inequality, and it implies that
n

i=1
|xi +yi|p =
n

i=1
|xi +yi| |xi +yi|p/q ≤
n

i=1
|xi| |xi +yi|p/q +
n

i=1
|yi| |xi +yi|p/q.
Application of H¨older’s inequality produces
n

i=1
|xi| |xi + yi|p/q ≤
 n

i=1
|xi|p
1/p  n

i=1
|xi + yi|p
1/q
=
 n

i=1
|xi|p
1/p  n

i=1
|xi + yi|p
(p−1)/p
= ∥x∥p ∥x + y∥p−1
p
.

Solutions
53
Similarly,
n

i=1
|yi| |xi + yi|p/q ≤∥y∥p ∥x + y∥p−1
p
, and therefore
∥x + y∥p
p ≤

∥x∥p + ∥y∥p

∥x + y∥p−1
p
=⇒
∥x + y∥p ≤∥x∥p + ∥y∥p .
Solutions for exercises in section 5. 2
5.2.1. ∥A∥F =
%
i,j |aij|2&1/2
= [trace (A∗A)]1/2 =
√
10,
∥B∥F =
√
3, and ∥C∥F =
√
9.
5.2.2. (a)
∥A∥1 = max absolute column sum = 4, and ∥A∥∞= max absolute
row sum = 3. ∥A∥2 = √λmax, where λmax is the largest value of λ for which
AT A −λI is singular. Determine these λ ’s by row reduction.
AT A −λI =

2 −−λ
−4
−4
8 −λ

−→

−4
8 −λ
2 −λ
−4

−→

−4
8 −λ
0
−4 + 2−λ
4 (8 −λ)

This matrix is singular if and only if the second pivot is zero, so we must have
(2 −λ)(8 −λ) −16 = 0
=⇒
λ2 −10λ = 0
=⇒
λ = 0, λ = 10, and therefore
∥A∥2 =
√
10.
(b)
Use the same technique to get ∥B∥1 = ∥B∥2 = ∥B∥∞= 1, and
(c)
∥C∥1 = ∥C∥∞= 10 and ∥C∥2 = 9.
5.2.3. (a)
∥I∥= max∥x∥=1 ∥Ix∥= max∥x∥=1 ∥x∥= 1.
(b)
∥In×n∥F =

trace

IT I
	1/2 = √n.
5.2.4. Use the fact that trace (AB) = trace (BA) (recall Example 3.6.5) to write
∥A∥2
F = trace (A∗A) = trace (AA∗) = ∥A∗∥2
F .
5.2.5. (a)
For x = 0, the statement is trivial. For x ̸= 0, we have ∥(x/ ∥x∥)∥= 1,
so for any particular x0 ̸= 0,
∥A∥= max
∥x∥=1 ∥Ax∥= max
x̸=0
,,,,A x
∥x∥
,,,, ≥∥Ax0∥
∥x0∥
=⇒
∥Ax0∥≤∥A∥∥x0∥.
(b)
Let x0 be a vector such that ∥x0∥= 1 and
∥ABx0∥= max
∥x∥=1 ∥ABx∥= ∥AB∥.
Make use of the result of part (a) to write
∥AB∥= ∥ABx0∥≤∥A∥∥Bx0∥≤∥A∥∥B∥∥x0∥= ∥A∥∥B∥.

54
Solutions
(c)
∥A∥= max
∥x∥=1 ∥Ax∥≤max
∥x∥≤1 ∥Ax∥because {x | ∥x∥= 1} ⊂{x | ∥x∥≤1} .
If there would exist a vector x0 such that ∥x0∥< 1 and ∥A∥< ∥Ax0∥,
then part (a) would insure that ∥A∥< ∥Ax0∥≤∥A∥∥x0∥< ∥A∥, which is
impossible.
5.2.6. (a)
Applying the CBS inequality yields
|y∗Ax| ≤∥y∥2 ∥Ax∥2
=⇒
max
∥x∥2=1
∥y∥2=1
|y∗Ax| ≤max
∥x∥2=1 ∥Ax∥2 = ∥A∥2 .
Now show that equality is actually attained for some pair x and y on the unit
2-sphere. To do so, notice that if x0 is a vector of unit length such that
∥Ax0∥2 = max
∥x∥2=1 ∥Ax∥2 = ∥A∥2 ,
and if
y0 =
Ax0
∥Ax0∥2
= Ax0
∥A∥2
,
then
y∗
0Ax0 = x∗
0A∗Ax0
∥A∥2
= ∥Ax0∥2
2
∥A∥2
= ∥A∥2
2
∥A∥2
= ∥A∥2 .
(b)
This follows directly from the result of part (a) because
∥A∥2 = max
∥x∥2=1
∥y∥2=1
|y∗Ax| = max
∥x∥2=1
∥y∥2=1
|(y∗Ax)∗| = max
∥x∥2=1
∥y∥2=1
|x∗A∗y| = ∥A∗∥2 .
(c)
Use part (a) with the CBS inequality to write
∥A∗A∥2 = max
∥x∥2=1
∥y∥2=1
|y∗A∗Ax| ≤max
∥x∥2=1
∥y∥2=1
∥Ay∥2 ∥Ax∥2 = ∥A∥2
2 .
To see that equality is attained, let x = y = x0, where x0 is a vector of unit
length such that ∥Ax0∥2 = max∥x∥2=1 ∥Ax∥2 = ∥A∥2 , and observe
|x∗
0A∗Ax0| = x∗
0A∗Ax0 = ∥Ax0∥2
2 = ∥A∥2
2 .
(d)
Let D =

A
0
0
B

. We know from (5.2.7) that ∥D∥2
2 is the largest value
λ such that DT D −λI is singular. But DT D −λI is singular if and only if
AT A −λI or BT B −λI is singular, so λmax(D) = max {λmax(A), λmax(B)} .
(e)
If UU∗= I, then ∥U∗Ax∥2
2 = x∗A∗UU∗Ax = x∗A∗Ax = ∥Ax∥2
2, so
∥U∗A∥2 = max∥x∥2=1 ∥U∗Ax∥2 = max∥x∥2=1 ∥Ax∥2 = ∥A∥2. Now, if V∗V =
I, use what was just established with part (b) to write
∥AV∥2 = ∥(AV)∗∥2 = ∥V∗A∗∥2 = ∥A∗∥2 = ∥A∥2
=⇒
∥U∗AV∥2 = ∥A∥2.

Solutions
55
5.2.7. Proceed as follows.
1
min
∥x∥=1
,,A−1x
,, = max
∥x∥=1

1
∥A−1x∥
+
= max
y̸=0



1
,,,A−1 (Ay)
∥Ay∥
,,,



= max
y̸=0
∥Ay∥
∥A−1(Ay)∥= max
y̸=0
∥Ay∥
∥y∥
= max
y̸=0
,,,,A
 y
∥y∥
,,,,
= max
∥x∥=1 ∥Ax∥= ∥A∥
5.2.8. Use (5.2.6) on p. 280 to write ∥(zI−A)−1∥= (1/ min∥x∥=1 ∥(zI −A)x∥), and let
w be a vector for which ∥w∥= 1 and ∥(zI −A)w∥= min∥x∥=1 ∥(zI −A)x∥.
Use ∥Aw∥≤∥A∥< |z| together with the “backward triangle inequality” from
Example 5.1.1 (p. 273) to write
∥(zI −A)w∥= ∥zw −Aw∥≥
∥zw∥−∥Aw∥
 =
|z| −∥Aw∥

= |z| −∥Aw∥≥|z| −∥A∥.
Consequently, min∥x∥=1 ∥(zI −A)x∥= ∥(zI −A)w∥≥|z| −∥A∥implies that
∥(zI −A)−1∥=
1
min
∥x∥=1 ∥(zI −A)x∥≤
1
|z| −∥A∥.
Solutions for exercises in section 5. 3
5.3.1. Only (c) is an inner product. The expressions in (a) and (b) each fail the ﬁrst
condition of the deﬁnition (5.3.1), and (d) fails the second.
5.3.2. (a)
⟨x y⟩= 0
∀x ∈V
=⇒
⟨y y⟩= 0
=⇒
y = 0.
(b)
⟨αx y⟩= ⟨y αx⟩= α ⟨y x⟩= α⟨y x⟩= α ⟨x y⟩
(c)
⟨x + y z⟩= ⟨z x + y⟩= ⟨z x⟩+ ⟨z y⟩= ⟨z x⟩+ ⟨z y⟩= ⟨x z⟩+ ⟨y z⟩
5.3.3. The ﬁrst property in (5.2.3) holds because ⟨x x⟩≥0 for all x ∈V implies
∥x∥=
-
⟨x x⟩≥0, and ∥x∥= 0 ⇐⇒⟨x x⟩= 0 ⇐⇒x = 0. The second
property in (5.2.3) holds because
∥αx∥2 = ⟨αx αx⟩= α ⟨αx x⟩= α⟨x αx⟩= αα⟨x x⟩= |α|2 ⟨x x⟩= |α|2 ∥x∥2 .
5.3.4. 0 ≤∥x −y∥2 = ⟨x −y x −y⟩= ⟨x x⟩−2 ⟨x y⟩+⟨y y⟩= ∥x∥2−2 ⟨x y⟩+∥y∥2
5.3.5. (a)
Use the CBS inequality with the Frobenius matrix norm and the standard
inner product as illustrated in Example 5.3.3, and set A = I.
(b)
Proceed as in part (a), but this time set A = BT (recall from Example
3.6.5 that trace

BT B
	
= trace

BBT 	
).

56
Solutions
(c) Use the result of Exercise 5.3.4 with the Frobenius matrix norm and the inner
product for matrices.
5.3.6. Suppose that parallelogram identity holds, and verify that (5.3.10) satisﬁes the
four conditions in (5.3.1). The ﬁrst condition follows because ⟨x x⟩r = ∥x∥2 and
⟨ix x⟩r = 0 combine to yield ⟨x x⟩= ∥x∥2 . The second condition (for real α )
and third condition hold by virtue of the argument for (5.3.7). We will prove the
fourth condition and then return to show that the second holds for complex α.
By observing that ⟨x y⟩r = ⟨y x⟩r and ⟨ix iy⟩r = ⟨x y⟩r , we have
⟨iy x⟩r =
.
iy −i2x
/
r = ⟨y −ix⟩r = −⟨y ix⟩r = −⟨ix y⟩r ,
and hence
⟨y x⟩= ⟨y x⟩r + i ⟨iy x⟩r = ⟨y x⟩r −i ⟨ix y⟩r = ⟨x y⟩r −i ⟨ix y⟩r = ⟨x y⟩.
Now prove that ⟨x αy⟩= α ⟨x y⟩for all complex α. Begin by showing it is
true for α = i.
⟨x iy⟩= ⟨x iy⟩r + i ⟨ix iy⟩r = ⟨x iy⟩r + i ⟨x y⟩r = ⟨iy x⟩r + i ⟨x y⟩r
= −⟨ix y⟩r + i ⟨x y⟩r = i (⟨x y⟩r + i ⟨ix y⟩r)
= i ⟨x y⟩
For α = ξ + iη,
⟨x αy⟩= ⟨x ξy + iηy⟩= ⟨x ξy⟩+ ⟨x iηy⟩= ξ ⟨x y⟩+ iη ⟨x y⟩= α ⟨x y⟩.
Conversely, if ⟨⋆⋆⟩is any inner product on V, then with ∥⋆∥2 = ⟨⋆⋆⟩we have
∥x + y∥2 + ∥x −y∥2 = ⟨x + y x + y⟩+ ⟨x −y x −y⟩
= ∥x∥2 + 2Re ⟨x y⟩+ ∥y∥2 + ∥x∥2 −2Re ⟨x y⟩+ ∥y∥2
= 2

∥x∥2 + ∥y∥2
.
5.3.7. The parallelogram identity (5.3.7) fails to hold for all x, y ∈Cn. For example,
if x = e1 and y = e2, then
∥e1 + e2∥2
∞+ ∥e1 −e2∥2
∞= 2,
but
2

∥e1∥2
∞+ ∥e2∥2
∞
	
= 4.
5.3.8. (a) As shown in Example 5.3.2, the Frobenius matrix norm Cn×n is generated
by the standard matrix inner product (5.3.2), so the result on p. 290 guarantees
that ∥⋆∥F satisﬁes the parallelogram identity.
5.3.9. No, because the parallelogram inequality (5.3.7) doesn’t hold. To see that
∥X + Y∥2 + ∥X −Y∥2 = 2

∥X∥2 + ∥Y∥2 	
is not valid for all X, Y ∈Cn×n,
let X = diag (1, 0, . . . , 0) and Y = diag (0, 1, . . . , 0) . For ⋆= 1, 2, or ∞,
∥X + Y∥2
⋆+ ∥X −Y∥2
⋆= 1 + 1 = 2,
but
2

∥X∥2
⋆+ ∥Y∥2
⋆
	
= 4.

Solutions
57
Solutions for exercises in section 5. 4
5.4.1. (a), (b), and (e) are orthogonal pairs.
5.4.2. First ﬁnd v =

α1
α2

such that 3α1 −2α2 = 0, and then normalize v. The
second must be the negative of v.
5.4.3. (a)
Simply verify that xT
i xj = 0 for i ̸= j.
(b)
Let xT
4 = ( α1
α2
α3
α4 ) , and notice that xT
i x4 = 0 for i = 1, 2, 3
is three homogeneous equations in four unknowns


1
−1
0
2
1
1
1
0
−1
−1
2
0





α1
α2
α3
α4


=


0
0
0


=⇒



α1
α2
α3
α4


= β



−1
1
0
1


.
(c)
Simply normalize the set by dividing each vector by its norm.
5.4.4. The Fourier coeﬃcients are
ξ1 = ⟨u1 x⟩=
1
√
2,
ξ2 = ⟨u2 x⟩= −1
√
3,
ξ3 = ⟨u3 x⟩= −5
√
6,
so
x = ξ1u1 + ξ2u2 + ξ3u3 = 1
2


1
−1
0

−1
3


1
1
1

−5
6


−1
−1
2

.
5.4.5. If U1, U2, U3, and U4 denote the elements of B, verify they constitute an
orthonormal set by showing that
⟨Ui Uj⟩= trace(UT
i Uj) = 0 for i ̸= j
and
∥Ui∥=
0
trace(UT
i Ui) = 1.
Consequently, B is linearly independent—recall (5.4.2)—and therefore B is a
basis because it is a maximal independent set—part (b) of Exercise 4.4.4 insures
dim ℜ2×2 = 4. The Fourier coeﬃcients ⟨Ui A⟩= trace(UT
i A) are
⟨U1 A⟩=
2
√
2,
⟨U2 A⟩= 0,
⟨U3 A⟩= 1,
⟨U4 A⟩= 1,
so the Fourier expansion of A is A = (2/
√
2)U1 + U3 + U4.
5.4.6. cos θ = xT y/ ∥x∥∥y∥= 1/2, so θ = π/3.
5.4.7. This follows because each vector has a unique representation in terms of a basis—
see Exercise 4.4.8 or the discussion of coordinates in §4.7.
5.4.8. If the columns of U = [u1 | u2 | · · · | un] are an orthonormal basis for Cn, then
[U∗U]ij = u∗
i uj =

1
when i = j,
0
when i ̸= j,
(‡)

58
Solutions
and, therefore, U∗U = I. Conversely, if U∗U = I, then ( ‡ ) holds, so the
columns of U are orthonormal—they are a basis for Cn because orthonormal
sets are always linearly independent.
5.4.9. Equations (4.5.5) and (4.5.6) guarantee that
R (A) = R (AA∗)
and
N (A) = N (A∗A),
and consequently r ∈R (A) = R (AA∗)
=⇒
r = AA∗x for some x, and
n ∈N (A) = N (A∗A)
=⇒
A∗An = 0. Therefore,
⟨r n⟩= r∗n = x∗AA∗n = x∗A∗An = 0.
5.4.10. (a) π/4
(b) π/2
5.4.11. The number xT y or x∗y will in general be complex. In order to guarantee that
we end up with a real number, we should take
cos θ = |Re (x∗y) |
∥x∥∥y∥.
5.4.12. Use the Fourier expansion y = 
i ⟨ui y⟩ui together with the various properties
of an inner product to write
⟨x y⟩=
1
x

i
⟨ui y⟩ui
2
=

i
⟨x ⟨ui y⟩ui⟩=

i
⟨ui y⟩⟨x ui⟩.
5.4.13. In a real space, ⟨x y⟩= ⟨y x⟩, so the third condition in the deﬁnition (5.3.1)
of an inner product and Exercise 5.3.2(c) produce
⟨x + y x −y⟩= ⟨x + y x⟩−⟨x + y y⟩
= ⟨x x⟩+ ⟨y x⟩−⟨x y⟩−⟨y y⟩
= ∥x∥2 −∥y∥2 = 0.
5.4.14. (a)
In a real space, ⟨x y⟩= ⟨y x⟩, so the third condition in the deﬁnition
(5.3.1) of an inner product and Exercise 5.3.2(c) produce
∥x + y∥2 = ⟨x + y x + y⟩= ⟨x + y x⟩+ ⟨x + y y⟩
= ⟨x x⟩+ ⟨y x⟩+ ⟨x y⟩+ ⟨y y⟩
= ∥x∥2 + 2 ⟨x y⟩+ ∥y∥2 ,
and hence ⟨x y⟩= 0 if and only if ∥x + y∥2 = ∥x∥2 + ∥y∥2 .
(b)
In a complex space, x ⊥y
=⇒
∥x + y∥2 = ∥x∥2 + ∥y∥2 , but the
converse is not valid—e.g., consider C2 with the standard inner product, and
let x =

−i
1

and y =

1
i

.

Solutions
59
(c)
Again, using the properties of a general inner product, derive the expansion
∥αx + βy∥2 = ⟨αx + βy αx + βy⟩
= ⟨αx αx⟩+ ⟨αx βy⟩+ ⟨βy αx⟩+ ⟨βy βy⟩
= ∥αx∥2 + αβ ⟨x y⟩+ βα ⟨y x⟩+ ∥βy∥2 .
Clearly, x ⊥y
=⇒
∥αx + βy∥2 = ∥αx∥2 + ∥βy∥2 ∀α, β. Conversely, if
∥αx + βy∥2 = ∥αx∥2 + ∥βy∥2 ∀α, β, then αβ ⟨x y⟩+ βα ⟨y x⟩= 0 ∀α, β.
Letting α = ⟨x y⟩and β = 1 produces the conclusion that 2| ⟨x y⟩|2 = 0,
and thus ⟨x y⟩= 0.
5.4.15. (a)
cos θi = ⟨ui x⟩/ ∥ui∥∥x∥= ⟨ui x⟩/ ∥x∥= ξi/ ∥x∥
(b)
Use the Pythagorean theorem (Exercise 5.4.14) to write
∥x∥2 = ∥ξ1u1 + ξ2u2 + · · · + ξnun∥2
= ∥ξ1u1∥2 + ∥ξ2u2∥2 + · · · + ∥ξnun∥2
= |ξ1|2 + |ξ2|2 + · · · + |ξn|2.
5.4.16. Use the properties of an inner product to write
,,,,,x −
k

i=1
ξiui
,,,,,
2
=
1
x −
k

i=1
ξiui x −
k

i=1
ξiui
2
= ⟨x x⟩−2

i
|ξi|2 +
1 k

i=1
ξiui
k

i=1
ξiui
2
= ∥x∥2 −2

i
|ξi|2 +
,,,,,
k

i=1
ξiui
,,,,,
2
,
and then invoke the Pythagorean theorem (Exercise 5.4.14) to conclude
,,,,,
k

i=1
ξiui
,,,,,
2
=

i
∥ξiui∥2 =

i
|ξi|2.
Consequently,
0 ≤
,,,,,x −
k

i=1
ξiui
,,,,,
2
= ∥x∥2 −

i
|ξi|2
=⇒
k

i=1
|ξi|2 ≤∥x∥2 .
(‡)
If x ∈span {u1, u2, . . . , uk} , then the Fourier expansion of x with respect
to the ui ’s is x = 
k
i=1 ξiui, and hence equality holds in (‡). Conversely, if
equality holds in (‡), then x −
k
i=1 ξiui = 0.

60
Solutions
5.4.17. Choose any unit vector ei for y. The angle between e and ei approaches π/2
as n →∞, but eT ei = 1 for all n.
5.4.18. If y is negatively correlated to x, then zx = −zy, but ∥zx −zy∥2 = 2√n
gives no indication of the fact that zx and zy are on the same line. Continuity
therefore dictates that when y ≈β0e + β1x with β1 < 0, then zx ≈−zy, but
∥zx −zy∥2 ≈2√n gives no hint that zx and zy are almost on the same line.
If we want to use norms to gauge linear correlation, we should use
min
)
∥zx −zy∥2 , ∥zx + zy∥2
*
.
5.4.19. (a) cos θ = 1
=⇒
⟨x y⟩= ∥x∥∥y∥> 0, and the straightforward extension of
Exercise 5.1.9 guarantees that
y = ⟨x y⟩
∥x∥2 x,
and clearly
⟨x y⟩
∥x∥2 > 0.
Conversely, if y = αx for α > 0, then ⟨x y⟩= α ∥x∥2
=⇒
cos θ = 1.
(b)
cos θ = −1
=⇒
⟨x y⟩= −∥x∥∥y∥< 0, so the generalized version of
Exercise 5.1.9 guarantees that
y = ⟨x y⟩
∥x∥2 x,
and in this case
⟨x y⟩
∥x∥2 < 0.
Conversely, if y = αx for α < 0, then ⟨x y⟩= α ∥x∥2 , so
cos θ = α ∥x∥2
|α| ∥x∥2 = −1.
5.4.20. F(t) = 
∞
n (−1)n 2
n sin nt.
Solutions for exercises in section 5. 5
5.5.1. (a)
u1 = 1
2



1
1
1
−1


,
u2 =
1
2
√
3



3
−1
−1
1


,
u3 =
1
√
6



0
1
1
2



(b)
First verify this is an orthonormal set by showing uT
i uj =

1
when i = j,
0
when i ̸= j.
To show that the xi ’s and the ui ’s span the same space, place the xi ’s as rows
in a matrix A, and place the ui ’s as rows in a matrix B, and then verify that
EA = EB—recall Example 4.2.2.
(c)
The result should be the same as in part (a).

Solutions
61
5.5.2. First reduce A to EA to determine a “regular” basis for each space.
R (A) = span





1
2
3





N

AT 	
= span





−2
1
0

,


−3
0
1





R

AT 	
= span








1
−2
3
−1








N (A) = span








2
1
0
0


,



−3
0
1
0


,



1
0
0
1








Now apply Gram–Schmidt to each of these.
R (A) = span



1
√
14


1
2
3





N

AT 	
= span



1
√
5


−2
1
0

,
1
√
70


−3
−6
5





R

AT 	
= span





1
√
15



1
−2
3
−1








N (A) = span





1
√
5



2
1
0
0


,
1
√
70



−3
6
5
0


,
1
√
210



1
−2
3
14








5.5.3.
u1 =
1
√
3


i
i
i

,
u2 =
1
√
6


−2i
i
i

,
u3 =
1
√
2


0
−i
i


5.5.4. Nothing! The resulting orthonormal set is the same as the original.
5.5.5. It breaks down at the ﬁrst vector such that xk ∈span {x1, x2, . . . , xk−1} because
if
xk ∈span {x1, x2, . . . , xk−1} = span {u1, u2, . . . , uk−1} ,
then the Fourier expansion of xk with respect to span {u1, u2, . . . , uk−1} is
xk =
k−1

i=1
⟨ui xk⟩ui,
and therefore
uk =

xk −
k−1
i=1 ⟨ui xk⟩ui

,,,

xk −
k−1
i=1 ⟨ui xk⟩ui
,,,
=
0
∥0∥

62
Solutions
is not deﬁned.
5.5.6. (a)
The rectangular QR factors are
Q =



1/
√
3
−1/
√
3
1/
√
6
1/
√
3
1/
√
3
1/
√
6
1/
√
3
0
−2/
√
6
0
1/
√
3
0



and
R =


√
3
√
3
−
√
3
0
√
3
√
3
0
0
√
6

.
(b)
Following Example 5.5.3, solve Rx = QT b to get x =


2/3
1/3
0

.
5.5.7. For k = 1, there is nothing to prove. For k > 1, assume that Ok is an
orthonormal basis for Sk. First establish that Ok+1 must be an orthonormal
set. Orthogonality follows because for each j < k + 1,
⟨uj uk+1⟩=
1
uj
1
νk+1

xk+1 −
k

i=1
⟨ui xk+1⟩ui
2
=
1
νk+1

⟨uj xk+1⟩−
1
uj
k

i=1
⟨ui xk+1⟩ui
2
=
1
νk+1

⟨uj xk+1⟩−
k

i=1
⟨ui xk+1⟩⟨uj ui⟩

=
1
νk+1
(⟨uj xk+1⟩−⟨uj xk+1⟩) = 0.
This together with the fact that each ui has unit norm means that Ok+1 is an
orthonormal set. Now assume Ok is a basis for Sk, and prove that Ok+1 is a
basis for Sk+1. If x ∈Sk+1, then x can be written as a combination
x =
k+1

i=1
αixi =
 k

i=1
αixi

+ αk+1xk+1,
where 
k
i=1 αixi ∈Sk = span (Ok) ⊂span (Ok+1) . Couple this together with
the fact that
xk+1 = νk+1uk+1 +
k

i=1
⟨ui xk+1⟩ui ∈span (Ok+1)
to conclude that x ∈span (Ok+1) . Consequently, Ok+1 spans Sk+1, and there-
fore Ok+1 is a basis for Sk+1 because orthonormal sets are always linearly
independent.

Solutions
63
5.5.8. If A = Q1R1 = Q2R2 are two rectangular QR factorizations, then (5.5.6)
implies AT A = RT
1 R1 = RT
2 R2. It follows from Example 3.10.7 that AT A is
positive deﬁnite, and R1 = R2 because the Cholesky factorization of a positive
deﬁnite matrix is unique. Therefore, Q1 = AR−1
1
= AR−1
2
= Q2.
5.5.9. (a)
Step 1:
fl ∥x1∥= 1, so u1 ←x1.
Step 2:
uT
1 x2 = 1, so
u2 ←x2 −

uT
1 x2
	
u1 =


0
0
−10−3


and
u2 ←
u2
∥u2∥=


0
0
−1

.
Step 3:
uT
1 x3 = 1 and uT
2 x3 = 0, so
u3 ←x3 −

uT
1 x3
	
u1 −

uT
2 x3
	
u2 =


0
10−3
−10−3

and u3 ←
u3
∥u3∥=


0
.709
−.709

.
Therefore, the result of the classical Gram–Schmidt algorithm using 3-digit arith-
metic is
u1 =


1
0
10−3

,
u2 =


0
0
−1

,
u3 =


0
.709
−.709

,
which is not very good because u2 and u3 are not even close to being orthog-
onal.
(b)
Step 1:
fl ∥x1∥= 1, so
{u1, u2, u3} ←{x1, x2, x3} .
Step 2:
uT
1 u2 = 1 and uT
1 u3 = 1, so
u2 ←u2 −

uT
1 u2
	
u1 =


0
0
−10−3

,
u3 ←u3 −

uT
1 u3
	
u1 =


0
10−3
−10−3

,
and then
u2 ←
u2
∥u2∥=


0
0
−1

.
Step 3:
uT
2 u3 = 10−3, so
u3 ←u3 −

uT
2 u3
	
u2 =


0
10−3
0


and
u3 ←
u3
∥u3∥=


0
1
0

.

64
Solutions
Thus the modiﬁed Gram–Schmidt algorithm produces
u1 =


1
0
10−3

,
u2 =


0
0
−1

,
u3 =


0
1
0

,
which is as close to being an orthonormal set as one could reasonably hope to
obtain by using 3-digit arithmetic.
5.5.10. Yes. In both cases rij is the (i, j)-entry in the upper-triangular matrix R in the
QR factorization.
5.5.11. p0(x) = 1/
√
2,
p1(x) =
-
3/2 x,
p2(x) =
-
5/8 (3x2 −1)
Solutions for exercises in section 5. 6
5.6.1. (a), (c), and (d).
5.6.2. Yes, because U∗U =

1
0
0
1

.
5.6.3. (a)
Eight: D =


±1
0
0
0
±1
0
0
0
±1


(b)
2n : D =




±1
0
· · ·
0
0
±1
· · ·
0
...
...
...
...
0
0
· · ·
±1




(c) There are inﬁnitely many because each diagonal entry can be any point on the
unit circle in the complex plane—these matrices have the form given in part (d)
of Exercise 5.6.1.
5.6.4. (a)
When α2 + β2 = 1/2.
(b)
When α2 + β2 = 1.
5.6.5. (a)
(UV)∗(UV) = V∗U∗UV = V∗V = I.
(b)
Consider I + (−I) = 0.
(c)

U
0
0
V
∗
U
0
0
V

=

U∗
0
0
V∗
 
U
0
0
V

=

U∗U
0
0
V∗V

=

I
0
0
I

.
5.6.6. Recall from (3.7.8) or (4.2.10) that (I+A)−1 exists if and only if N (I + A) = 0,
and write x ∈N (I + A)
=⇒
x = −Ax
=⇒
x∗x = −x∗Ax. But
taking the conjugate transpose of both sides yields x∗x = −x∗A∗x = x∗Ax,
so x∗x = 0, and thus x = 0. Replacing A by −A in Exercise 3.7.6 gives
A(I + A)−1 = (I + A)−1A, so
(I −A)(I + A)−1 = (I + A)−1 −A(I + A)−1
= (I + A)−1 −(I + A)−1A = (I + A)−1(I −A).

Solutions
65
These results together with the fact that A is skew hermitian produce
U∗U = (I + A)−1∗(I −A)∗(I −A)(I + A)−1
= (I + A)∗−1(I −A)∗(I −A)(I + A)−1
= (I −A)−1(I + A)(I −A)(I + A)−1 = I.
5.6.7. (a)
Yes—because if R = I −2uu∗, where ∥u∥= 1, then

I
0
0
R

= I −2

0
u

( 0
u∗)
and
,,,,

0
u
,,,, = 1.
(b)
No—Suppose R = I −2uu∗and S = I −2vv∗, where ∥u∥= 1 and
∥v∥= 1 so that

R
0
0
S

= I −2

uu∗
0
0
vv∗

.
If we could ﬁnd a vector w such that ∥w∥= 1 and

R
0
0
S

= I −2ww∗,
then
ww∗=

uu∗
0
0
vv∗

.
But this is impossible because (recall Example 3.9.3)
rank (ww∗) = 1
and
rank

uu∗
0
0
vv∗

= 2.
5.6.8. (a)
u∗v = (Ux)∗Uy = x∗U∗Uy = x∗y
(b)
The fact that P is an isometry means ∥u∥= ∥x∥and ∥v∥= ∥y∥. Use
this together with part (a) and the deﬁnition of cosine given in (5.4.1) to obtain
cos θu,v =
uT v
∥u∥∥v∥=
xT y
∥x∥∥y∥= cos θx,y.
5.6.9. (a) Since Um×r has orthonormal columns, we have U∗U = Ir so that
∥U∥2
2 = max
∥x∥2=1 x∗U∗Ux = max
∥x∥2=1 x∗x = 1.
This together with ∥A∥2 = ∥A∗∥2—recall (5.2.10)—implies ∥V∥2 = 1. For the
Frobenius norm we have
∥U∥F = [trace (U∗U)]1/2 = [trace (I)]1/2 = √r.
trace (AB) = trace (BA) (Example 3.6.5) and VV∗= Ik
=⇒
∥V∥F =
√
k.

66
Solutions
(b)
First show that ∥UA∥2 = ∥A∥2 by writing
∥UA∥2
2 = max
∥x∥2=1 ∥UAx∥2
2 = max
∥x∥2=1 x∗A∗U∗UAx = max
∥x∥2=1 x∗A∗Ax
= max
∥x∥2=1 ∥Ax∥2
2 = ∥A∥2
2 .
Now use this together with ∥A∥2 = ∥A∗∥2 to observe that
∥AV∥2 = ∥V∗A∗∥2 = ∥A∗∥2 = ∥A∥2 .
Therefore, ∥UAV∥2 = ∥U(AV)∥2 = ∥AV∥2 = ∥A∥2 .
(c)
Use trace (AB) = trace (BA) with U∗U = Ir and VV∗= Ik to write
∥UAV∥2
F = trace

(UAV)∗UAV
	
= trace (V∗A∗U∗UAV)
= trace (V∗A∗AV) = trace (A∗AVV∗)
= trace (A∗A) = ∥A∥2
F .
5.6.10. Use (5.6.6) to compute the following quantities.
(a)
vvT
vT vu =
vT u
vT v

v = 1
6v = 1
6



1
4
0
−1



(b)
uuT
uT uv =
uT v
uT u

u = 1
5u = 1
5



−2
1
3
−1



(c)

I −vvT
vT v

u = u −
vT u
vT v

v = u −1
6v = 1
6



−13
2
18
−5



(d)

I −uuT
uT u

v = v −
uT v
uT u

u = v −1
5u = 1
5



7
19
−3
−4



5.6.11. (a)
N (Q) ̸= {0} because Qu = 0 and ∥u∥= 1
=⇒
u ̸= 0, so Q must
be singular by (4.2.10).
(b)
The result of Exercise 4.4.10 insures that n−1 ≤rank (Q), and the result
of part (a) says rank (Q) ≤n −1, and therefore rank (Q) = n −1.
5.6.12. Use (5.6.5) in conjunction with the CBS inequality given in (5.1.3) to write
∥p∥= |u∗x| ≤∥u∥∥x∥= ∥x∥.

Solutions
67
The fact that equality holds if and only if x is a scalar multiple of u follows
from the result of Exercise 5.1.9.
5.6.13. (a)
Set u = x −∥x∥e1 = −2/3


1
1
1

, and compute
R = I −2uuT
uT u = 1
3


1
−2
−2
−2
1
−2
−2
−2
1

.
(You could also use u = x + ∥x∥e1. )
(b)
Verify that R = RT , RT R = I, and R2 = I.
(c)
The columns of the reﬂector R computed in part (a) do the job.
5.6.14. Rx = x
=⇒
2uu∗x = 0
=⇒
u∗x = 0 because u ̸= 0.
5.6.15. If Rx = y in Figure 5.6.2, then the line segment between x −y is parallel to
the line determined by u, so x −y itself must be a scalar multiple of u. If
x −y = αu, then
u = x −y
α
=
x −y
∥x −y∥.
It is straightforward to verify that this choice of u produces the desired reﬂector.
5.6.16. You can verify by direct multiplication that PT P = I and U∗U = I, but you
can also recognize that P and U are elementary reﬂectors that come from
Example 5.6.3 in the sense that
P = I −2uuT
uT u,
where
u = x −e1 =

x1 −1
˜x

and
U = µ

I −2uu∗
u∗u

,
where
u = x −µe1 =

x1 −µ
˜x

.
5.6.17. The ﬁnal result is
v3 =


−
√
2/2
√
6/2
1


and
Q = Pz(π/6)Py(−π/2)Px(π/4) = 1
4


0
−
√
6 −
√
2
−
√
6 +
√
2
0
√
6 −
√
2
−
√
6 −
√
2
4
0
0

.
5.6.18. It matters because the rotation matrices given on p. 328 generally do not com-
mute with each other (this is easily veriﬁed by direct multiplication). For exam-
ple, this means that it is generally the case that
Py(φ)Px(θ)v ̸= Px(θ)Py(φ)v.

68
Solutions
5.6.19. As pointed out in Example 5.6.2, u⊥= (u/ ∥u∥)⊥, so we can assume without
any loss of generality that u has unit norm. We also know that any vector of
unit norm can be extended to an orthonormal basis for Cn—Examples 5.6.3 and
5.6.6 provide two possible ways to accomplish this. Let {u, v1, v2, . . . , vn−1}
be such an orthonormal basis for Cn.
Claim: span {v1, v2, . . . , vn−1} = u⊥.
Proof. x ∈span {v1, v2, . . . , vn−1}
=⇒
x = 
i αivi
=⇒
u∗x =

i αiu∗vi = 0
=⇒
x ∈u⊥, and thus span {v1, v2, . . . , vn−1} ⊆u⊥.
To establish the reverse inclusion, write x = α0u + 
i αivi, and then note
that x ⊥u
=⇒
0 = u∗x = α0
=⇒
x ∈span {v1, v2, . . . , vn−1} , and
hence
=⇒
u⊥⊆span {v1, v2, . . . , vn−1} .
Consequently, {v1, v2, . . . , vn−1} is a basis for u⊥because it is a spanning set
that is linearly independent—recall (4.3.14)—and thus dim u⊥= n −1.
5.6.20. The relationship between the matrices in (5.6.6) and (5.6.7) on p. 324 suggests
that if P is a projector, then A = I −2P is an involution—and indeed this
is true because A2 = (I −2P)2 = I −4P + 4P2 = I. Similarly, if A is an
involution, then P = (I −A)/2 is easily veriﬁed to be a projector. Thus each
projector uniquely deﬁnes an involution, and vice versa.
5.6.21. The outside of the face is visible from the perspective indicated in Figure 5.6.6
if and only if the angle θ between n and the positive x-axis is between −90◦
and +90◦. This is equivalent to saying that the cosine between n and e1 is
positive, so the desired conclusion follows from the fact that
cos θ > 0 ⇐⇒
nT e1
∥n∥∥e1∥> 0 ⇐⇒nT e1 > 0 ⇐⇒n1 > 0.
Solutions for exercises in section 5. 7
5.7.1. (a)
Householder reduction produces
R2R1A =


1
0
0
0
−3/5
4/5
0
4/5
3/5




1/3
−2/3
2/3
−2/3
1/3
2/3
2/3
2/3
1/3




1
19
−34
−2
−5
20
2
8
37


=


3
15
0
0
15
−30
0
0
45

= R,
so
Q = (R2R1)T =


1/3
14/15
−2/15
−2/3
1/3
2/3
2/3
−2/15
11/15

.

Solutions
69
(b)
Givens reduction produces P23P13P12A = R, where
P12 =


1/
√
5
−2/
√
5
0
2/
√
5
1/
√
5
0
0
0
1


P13 =


√
5/3
0
2/3
0
1
0
−2/3
0
√
5/3


P23 =


1
0
0
0
11/5
√
5
−2/5
√
5
0
2/5
√
5
11/5
√
5


5.7.2. Since P is an orthogonal matrix, so is PT , and hence the columns of X are
an orthonormal set. By writing
A = PT T = [X | Y]

R
0

= XR,
and by using the fact that rank (A) = n
=⇒
rank (R) = n, it follows that
R (A) = R (XR) = R (X)—recall Exercise 4.5.12. Since every orthonormal set
is linearly independent, the columns of X are a linearly independent spanning
set for R (A), and thus the columns of X are an orthonormal basis for R (A).
Notice that when the diagonal entries of R are positive, A = XR is the
“rectangular” QR factorization for A introduced on p. 311, and the columns of
X are the same columns as those produced by the Gram–Schmidt procedure.
5.7.3. According to (5.7.1), set u = A∗1 −∥A∗1∥e1 =



−1
2
−2
1


, so
R1 = I−2uu∗
u∗u = 1
5



4
2
−2
1
2
1
4
−2
−2
4
1
2
1
−2
2
4



and
R1A =



5
−15
5
0
10
−5
0
−10
2
0
5
14


.
Next use u =


10
−10
5

−


15
0
0

=


−5
−10
5

to build
ˆR2 = I −2uu∗
u∗u = 1
3


2
−2
1
−2
−1
2
1
2
2


and
R2 = 1
3



3
0
0
0
0
2
−2
1
0
−2
−1
2
0
1
2
2


,
so
R2R1A =



5
−15
5
0
15
0
0
0
12
0
0
9


.

70
Solutions
Finally, with u =

12
9

−

15
0

=

−3
9

, build
ˆR3 = 1
5

4
3
3
−4

and
R3 = 1
5



5
0
0
0
0
5
0
0
0
0
4
3
0
0
3
−4


,
so that
R3R2R1A =



5
−15
5
0
15
0
0
0
15
0
0
0


.
Therefore, PA = T =

R
0

, where
P = R3R2R1 = 1
15



12
6
−6
3
9
−8
8
−4
0
−5
2
14
0
−10
−11
−2



and
R =


5
−15
5
0
15
0
0
0
15

.
The result of Exercise 5.7.2 insures that the ﬁrst three columns in
PT = R1R2R3 = 1
15



12
9
0
0
6
−8
−5
−10
−6
8
2
−11
3
−4
14
−2



are an orthonormal basis for R (A). Since the diagonal entries of R are positive,
1
15



12
9
0
6
−8
−5
−6
8
2
3
−4
14





5
−15
5
0
15
0
0
0
15

= A
is the “rectangular” QR factorization for A discussed on p. 311.
5.7.4. If A has full column rank, and if P is an orthogonal matrix such that
PA = T =

R
0

and
Pb =

c
d

,
where R is an upper-triangular matrix, then the results of Example 5.7.3 insure
that the least squares solution of Ax = b can be obtained by solving the
triangular system Rx = c. The matrices P and R were computed in Exercise
5.7.3, so the least squares solution of Ax = b is the solution to


5
−15
5
0
15
0
0
0
15




x1
x2
x3

=


4
3
33


=⇒
x = 1
5


−4
1
11

.

Solutions
71
5.7.5. ∥A∥F = ∥QR∥F = ∥R∥F
because orthogonal matrices are norm preserving
transformations—recall Exercise 5.6.9.
5.7.6. Follow the procedure outlined in Example 5.7.4 to compute the reﬂector
ˆR =

−3/5
4/5
4/5
3/5

,
and then set
R =


1
0
0
0
−3/5
4/5
0
4/5
3/5

.
Since A is 3 × 3, there is only one step, so P = R and
PT AP = H =


−2
−5
0
−5
−41
38
0
38
41

.
5.7.7. First argue that the product of an upper-Hessenberg matrix with an upper-
triangular matrix must be upper Hessenberg—regardless of which side the tri-
angular factor appears. This implies that Q is upper Hessenberg because Q =
HR−1 and R−1 is upper triangular—recall Exercise 3.7.4. This in turn means
that RQ must be upper Hessenberg.
5.7.8. From the structure of the matrices in Example 5.7.5, it can be seen that P12
requires 4n multiplications, P23 requires 4(n−1) multiplications, etc. Use the
formula 1 + 2 + · · · + n = n(n + 1)/2 to obtain the total as
4[n + (n −1) + (n −2) + · · · + 2] = 4
n2 + n
2
−1

≈2n2.
Solutions for exercises in section 5. 8
5.8.1. (a)







4
13
28
27
18
0







(b)







−1
0
2
0
−1
0







(c)







α0
α0 + α1
α0 + α1 + α2
α1 + α2
α2
0







5.8.2. The answer to both parts is



0
0
0
4


.
5.8.3. F2 =

1
1
1
−1

, D2 =

1
0
0
−i

, and

72
Solutions
F4PT
4 =



1
1
1
1
1
−i
−1
i
1
−1
1
−1
1
i
−1
−i


PT
4 =



1
1
1
1
1
−1
−i
i
1
1
−1
−1
1
−1
i
−i



=





1
1
1
−1

1
0
0
−i
 
1
1
1
−1

1
1
1
−1
−

1
0
0
−i
 
1
1
1
−1





=
 F2
D2F2
F2
−D2F2

.
5.8.4. (a)
a ⊙b =



α0β0
α0β1 + α1β0
α1β1
0



F4(a ⊙b) =



α0β0 + α0β1 + α1β0 + α1β1
α0β0 −iα0β1 −iα1β0 −α1β1
α0β0 −α0β1 −α1β0 + α1β1
α0β0 + iα0β1 + iα1β0 −α1β1


= (F4ˆa) × (F4ˆb)
(b)
F−1
4

(F4ˆa) × (F4ˆb)

= a ⊙b
5.8.5. p(x)q(x) = γ0 + γ1x + γ2x2 + γ3x3, where



γ0
γ1
γ2
γ3


= F−1
4

(F4ˆa) × (F4ˆb)

= F−1
4





1
1
1
1
1
−i
−1
i
1
−1
1
−1
1
i
−1
−i






−3
2
0
0


×



1
1
1
1
1
−i
−1
i
1
−1
1
−1
1
i
−1
−i






−4
3
0
0





= F−1
4





−1
−3 −2i
−5
−3 + 2i


×



−1
−4 −3i
−7
−4 + 3i





= 1
4



1
1
1
1
1
i
−1
−i
1
−1
1
−1
1
−i
−1
i






1
6 + 17i
35
6 −17i


=



12
−17
6
0


.
5.8.6. (a)

3
4

⊙

1
2

=



3
10
8
0


, so
4310 × 2110 = (8 × 102) + (10 × 101) + (3 × 100)
= (9 × 102) + (0 × 101) + (3 × 100) = 903.

Solutions
73
(b)


1
2
3

⊙


6
0
1

=







3
2
19
12
6
0







, so
1238 × 6018 = (6 × 84) + (12 × 83) + (19 × 82) + (2 × 81) + (3 × 80).
Since
12 = 8 + 4
=⇒
12 × 83 = (8 + 4) × 83 = 84 + (4 × 83)
19 = (2 × 8) + 3
=⇒
19 × 82 = (2 × 83) + (3 × 82),
we have that
1238 × 6018 = (7 × 84) + (6 × 83) + (3 × 82) + (2 × 81) + (3 × 80) = 763238.
(c)



1
0
1
0


⊙



1
1
0
1


=











0
1
0
2
1
1
1
0











, so
10102×11012 = (1×26)+(1×25)+(1×24)+(2×23)+(0×22)+(1×21)+(0×20).
Substituting 2 × 23 = 1 × 24 in this expression and simplifying yields
10102 × 11012 = (1 × 27) + (0 × 26) + (0 × 25) + (0 × 24)
+ (0 × 23) + (0 × 22) + (1 × 21) + (0 × 20)
= 100000102.
5.8.7. (a)
The number of multiplications required by the deﬁnition is
1 + 2 + · · · + (n −1) + n + (n −1) + · · · + 2 + 1
= 2

1 + 2 + · · · + (n −1)

+ n
= (n −1)n + n = n2.
(b)
In the formula an×1 ⊙bn×1 = F−1
2n

(F2nˆa) × (F2nˆb)

, using the FFT to
compute F2nˆa and F2nˆb requires (2n/2) log2 2n = n(1 + log2 n) multiplica-
tions for each term, and an additional 2n multiplications are needed to form
the product (F2nˆa)×(F2nˆb). Using the FFT in conjunction with the procedure

74
Solutions
described in Example 5.8.2 to apply F−1 to (F2nˆa) × (F2nˆb) requires another
(2n/2) log2 2n = n(1 + log2 n) multiplications to compute F2nx followed by
2n more multiplications to produce (1/2n)F2nx = F−1
2n x . Therefore, the total
count is 3n(1 + log2 n) + 4n = 3n log2 n + 7n.
5.8.8. Recognize that y is of the form
y = 1(e2 + e6) + 4(e3 + e5) + 5i(−e1 + e7) + 3i(−e2 + e6).
The real part says that there are two cosines—one with amplitude 1 and fre-
quency 2, and the other with amplitude 4 and frequency 3. The imaginary
part says there are two sines—one with amplitude 5 and frequency 1, and the
other with amplitude 3 and frequency 2. Therefore,
x(τ) = cos 4πτ + 4 cos 6πτ + 5 sin 2πτ + 3 sin 4πτ.
5.8.9. Use (5.8.12) to write a ⊙b = F−1
(Fˆa) × (Fˆb)

= F−1
(Fˆb) × (Fˆa)

= a ⊙b.
5.8.10. This is a special case of the result given in Example 4.3.5. The Fourier matrix
Fn is a special case of the Vandermonde matrix—simply let xk ’s that deﬁne
the Vandermonde matrix be the nth roots of unity.
5.8.11. The result of Exercise 5.8.10 implies that if
ˆa =









α0
...
αn−1
0
...
0









2n×1
and
ˆb =









β0
...
βn−1
0
...
0









2n×1
,
then F2nˆa = p and F2nˆb = q, and we know from (5.8.11) that the γk ’s are
given by γk = [a ⊙b]k. Therefore, the convolution theorem guarantees




γ0
γ1
γ2
...



= a ⊙b = F−1
2n

(F2nˆa) × (F2nˆb)

= F−1
2n

p × q

= F−1
2n




p(1)q(1)
p(ξ)q(ξ)
p(ξ2)q(ξ2)
...



.
5.8.12. (a)
This follows from the observation that Qk has 1’s on the kth subdiagonal
and 1’s on the (n −k)th superdiagonal. For example, if n = 8, then
Q3 =











0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0











.

Solutions
75
(b)
If the rows of F are indexed from 0 to n −1, then they satisfy the
relationships Fk∗Q = ξkFk∗for each k (verifying this for n = 4 will indicate
why it is true in general). This means that FQ = DF, which in turn implies
FQF−1 = D.
(c)
Couple parts (a) and (b) with FQkF−1 = (FQF−1)k = Dk to write
FCF−1 = Fp(Q)F−1
= F(c0I + c1Q + · · · + cn−1Qn−1)F−1
= c0I + c1FQF−1 + · · · + cn−1FQn−1F−1
= c0I + c1D + · · · + cn−1Dn−1
=




p(1)
0
· · ·
0
0
p(ξ)
· · ·
0
...
...
...
...
0
0
· · ·
p(ξn−1)



.
(d)
FC1F−1 = D1 and FC2F−1 = D2, where D1 and D2 are diagonal
matrices, and therefore
C1C2 = F−1D1FF−1D2F = F−1D1D2F = F−1D2D1F = F−1D2FF−1D1F
= C2C1.
5.8.13. (a)
According to Exercise 5.8.12,
C=




σ0
σn−1
· · ·
σ1
σ1
σ0
· · ·
σ2
...
...
...
...
σn−1
σn−2
· · ·
σ0



=F−1




p(1)
0
· · ·
0
0
p(ξ)
· · ·
0
...
...
...
...
0
0
· · ·
p(ξn−1)



F=F−1DF
in which p(x) = σ0+σ1x+· · ·+σn−1xn−1. Therefore, x = C−1b = F−1D−1Fb,
so we can execute the following computations.
(i)




p(0)
p(ξ)
...
p(ξn−1)



←−F




σ0
σ1
...
σn−1




using the FFT
(ii)
x ←−Fb
using the FFT
(iii)
xk ←−xk/p(ξk)
for
k = 0, 1, . . . , n −1
(iv)
x ←−F−1x
using the FFT as described in Example 5.8.2

76
Solutions
(b)
Use the same techniques described in part (a) to compute the kth column
of C−1 from the formula
[C−1]∗k = C−1ek = F−1D−1Fek
= F−1 
D−1[F]∗k
	
= F−1






1/p(1)
ξk/p(ξ)
ξ2k/p(ξ2)
...
ξn−k/p(ξn−1)






.
(c)
The kth column of P = C1C2 is given by
P∗k = Pek = F−1D1FF−1D2Fek = F−1
D1D2[F]∗k

.
If ( σ0
σ1
· · ·
σn−1 ) and ( η0
η1
· · ·
ηn−1 ) are the ﬁrst rows in C1 and
C2, respectively, and if p(x) = 
n−1
k=0 σkxk and q(x) = 
n−1
k=0 ηkxk , then ﬁrst
compute
p =




p(0)
p(ξ)
...
p(ξn−1)



←−F




σ0
σ1
...
σn−1




and
q =




q(0)
q(ξ)
...
q(ξn−1)



←−F




η0
η1
...
ηn−1



.
The kth column of the product can now be obtained from
P∗k ←−F−1
p × q × F∗k

for
k = 0, 1, . . . , n −1.
5.8.14. (a)
For n = 3 we have
Cˆb =







α0
0
0
0
α2
α1
α1
α0
0
0
0
α2
α2
α1
α0
0
0
0
0
α2
α1
α0
0
0
0
0
α2
α1
α0
0
0
0
0
α2
α1
α0














β0
β1
β2
0
0
0







=







α0β0
α1β0 + α0β1
α2β0 + α1β1 + α0β2
α2β1 + α1β2
α2β2
0







.
Use this as a model to write the expression for Cˆb, where n is arbitrary.
(b)
We know from part (c) of Exercise 5.8.12 that if F is the Fourier matrix
of order 2n, then FCF−1 = D, where
D =




p(1)
0
· · ·
0
0
p(ξ)
· · ·
0
...
...
...
...
0
0
· · ·
p(ξ2n−1)




(the ξk ’s are the 2nth roots of unity)

Solutions
77
in which p(x) = α0 + α1x + · · · + αn−1xn−1. Therefore, from part (a),
F(a ⊙b) = FCˆb = FCF−1Fˆb = DFˆb.
According to Exercise 5.8.10, we also know that
Fˆa =




p(1)
p(ξ)
...
p(ξ2n−1)



,
and hence
F(a ⊙b) = DFˆb = (Fˆa) × (Fˆb).
5.8.15. (a) Pnx performs an even–odd permutation to all components of x. The matrix
(I2 ⊗Pn/2) =

Pn/2
0
0
Pn/2

x
performs an even–odd permutation to the top half of x and then does the same
to the bottom half of x. The matrix
(I4 ⊗Pn/4) =



Pn/4
0
0
0
0
Pn/4
0
0
0
0
Pn/4
0
0
0
0
Pn/4


x
performs an even–odd permutation to each individual quarter of x. As this
pattern is continued, the product
Rn = (I2r−1 ⊗P21)(I2r−2 ⊗P22) · · · (I21 ⊗P2r−1)(I20 ⊗P2r)x
produces the bit-reversing permutation. For example, when n = 8,

78
Solutions
R8x = (I4 ⊗P2)(I2 ⊗P4)(I1 ⊗P8)x
=



P2
0
0
0
0
P2
0
0
0
0
P2
0
0
0
0
P2




P4
0
0
P4

P8











x0
x1
x2
x3
x4
x5
x6
x7











=



P2
0
0
0
0
P2
0
0
0
0
P2
0
0
0
0
P2




P4
0
0
P4













x0
x2
x4
x6
x1
x3
x5
x7












=



P2
0
0
0
0
P2
0
0
0
0
P2
0
0
0
0
P2















x0
x4
x2
x6
x1
x5
x3
x7












=











x0
x4
x2
x6
x1
x5
x3
x7











because
P2 =

1
0
0
1

.
(b)
To prove that I2r−k ⊗F2k = L2kR2k using induction, note ﬁrst that for
k = 1 we have
L2 = (I2r−1 ⊗B2)1 = I2r−1 ⊗F2
and
R2 = In(I2r−1 ⊗P2) = InIn = In,
so L2R2 = I2r−1 ⊗F2. Now assume that the result holds for k = j—i.e., assume
I2r−j ⊗F2j = L2jR2j.
Prove that the result is true for k = j + 1—i.e., prove
I2r−(j+1) ⊗F2j+1 = L2j+1R2j+1.
Use the fact that F2j+1 = B2j+1(I2 ⊗Fj)P2j+1 along with the two basic prop-

Solutions
79
erties of the tensor product given in the introduction of this exercise to write
I2r−(j+1) ⊗F2j+1 = I2r−(j+1) ⊗B2j+1(I2 ⊗F2j)P2j+1
=

I2r−(j+1) ⊗B2j+1(I2 ⊗F2j)

I2r−(j+1) ⊗P2j+1

= (I2r−(j+1) ⊗B2j+1)(I2r−(j+1) ⊗I2 ⊗F2j)(I2r−(j+1) ⊗P2j+1)
= (I2r−(j+1) ⊗B2j+1)(I2r−j ⊗F2j)(I2r−(j+1) ⊗P2j+1)
= (I2r−(j+1) ⊗B2j+1)L2jR2j(I2r−(j+1) ⊗P2j+1)
= L2j+1R2j+1.
Therefore, I2r−k ⊗F2k = L2kR2k for k = 1, 2, . . . , r, and when k = r we have
that Fn = LnRn.
5.8.16. According to Exercise 5.8.10,
Fna = b,
where
a =






α0
α1
α2
...
αn−1






and
b =






p(1)
p(ξ)
p(ξ2)
...
p(ξn−1)






.
By making use of the fact that (1/√n)Fn is unitary we can write
n−1

k=0
p(ξk)
2 = b∗b = (Fna)∗(Fna) = a∗F∗
nFna = a∗(nI)a = n
n−1

k=0
|αk|2 .
5.8.17. Let y = (2/n)Fx, and use the result in (5.8.7) to write
∥y∥2 =
,,,,,

k

(αk −iβk)efk + (αk + iβk)en−fk
,,,,,
=

k

|αk −iβk|2 + |αk + iβk|2
= 2

k

α2
k + β2
k

.
But because F∗F = nI, it follows that
∥y∥2 =
,,,,
2
nFx
,,,,
2
= 4
n2 x∗F∗Fx = 4
n ∥x∥2 ,
so combining these two statements produces the desired conclusion.

80
Solutions
5.8.18. We know from (5.8.11) that if p(x) = 
n−1
k=0 αkxk, then
p2(x) =
2n−2

k=0
[a ⊙a]kxk.
The last component of a ⊙a is zero, so we can write
cT (a ⊙a) =
2n−2

k=0
[a ⊙a]kηk = p2(η) =
n−1

k=0
αkηk
2
=

cT ˆa
	2 .
5.8.19. Start with X ←−rev(x) = (x0 x4 x2 x6 x1 x5 x3 x7).
For j = 0 :
D ←−(1)
X(0) ←−( x0
x2
x1
x3 )
X(1) ←−( x4
x6
x5
x7 )
X ←−
 X(0) + D × X(1)
X(0) −D × X(1)

=
 x0 + x4
x2 + x6
x1 + x5
x3 + x7
x0 −x4
x2 −x6
x1 −x5
x3 −x7

2×8
For j = 1 :
D ←−

1
e−πi/2

=

1
ξ2

X(0) ←−

x0 + x4
x1 + x5
x0 −x4
x1 −x5

X(1) ←−

x2 + x6
x3 + x7
x2 −x6
x3 −x7

X ←−
 X(0) + D × X(1)
X(0) −D × X(1)

=



x0 + x4 +
x2 +
x6
x1 + x5 +
x3 +
x7
x0 −x4 + ξ2x2 −ξ2x6
x1 −x5 + ξ2x3 −ξ2x7
x0 + x4 −
x2 −
x6
x1 + x5 −
x3 −
x7
x0 −x4 −ξ2x2 + ξ2x6
x1 −x5 −ξ2x3 + ξ2x7



4×2
For j = 2 :
D ←−



1
e−πi/4
e−2πi/4
e−3πi/4


=



1
ξ
ξ2
ξ3




Solutions
81
X(0) ←−



x0 + x4 +
x2 +
x6
x0 −x4 + ξ2x2 −ξ2x6
x0 + x4 −
x2 −
x6
x0 −x4 −ξ2x2 + ξ2x6



X(1) ←−



x1 + x5 +
x3 +
x7
x1 −x5 + ξ2x3 −ξ2x7
x1 + x5 −
x3 −
x7
x1 −x5 −ξ2x3 + ξ2x7



X ←−
 X(0) + D × X(1)
X(0) −D × X(1)

=











x0 + x4 +
x2 +
x6 +
x1 +
x5 +
x3 +
x7
x0 −x4 + ξ2x2 −ξ2x6 + ξ x1 −ξx5 + ξ3x3 −ξ3x7
x0 + x4 −
x2 −
x6 + ξ2x1 + ξ2x5 −ξ2x3 −ξ2x7
x0 −x4 −ξ2x2 + ξ2x6 + ξ3x1 −ξ3x5 −ξ5x3 + ξ5x7
x0 + x4 +
x2 +
x6 −
x1 −
x5 −
x3 −
x7
x0 −x4 + ξ2x2 −ξ2x6 −ξ x1 + ξ x5 −ξ3x3 + ξ3x7
x0 + x4 −
x2 −
x6 −ξ2x1 −ξ2x5 + ξ2x3 + ξ2x7
x0 −x4 −ξ2x2 + ξ2x6 −ξ3x1 + ξ3x5 + ξ5x3 −ξ5x7











8×1
To verify that this is the same as F8x8, use the fact that ξ = −ξ5, ξ2 = −ξ6,
ξ3 = −ξ7, and ξ4 = −1.
Solutions for exercises in section 5. 9
5.9.1. (a)
The fact that
rank (B) = rank

X | Y

= rank


1
1
1
1
2
2
1
2
3

= 3
implies BX ∪BY is a basis for ℜ3, so (5.9.4) guarantees that X and Y are
complementary.
(b)
According to (5.9.12), the projector onto X along Y is
P =

X | 0

X | Y
−1 =


1
1
0
1
2
0
1
2
0




1
1
1
1
2
2
1
2
3


−1
=


1
1
0
1
2
0
1
2
0




2
−1
0
−1
2
−1
0
−1
1

=


1
1
−1
0
3
−2
0
3
−2

,
and (5.9.9) insures that the complementary projector onto Y along X is
Q = I −P =


0
−1
1
0
−2
2
0
−3
3

.

82
Solutions
(c)
Qv =


2
4
6


(d)
Direct multiplication shows P2 = P and Q2 = Q.
(e)
To verify that R (P) = X = N (Q), you can use the technique of Example
4.2.2 to show that the basic columns of P (or the columns in a basis for N (Q) )
span the same space generated by BX . To verify that N (P) = Y, note that
P


1
2
3

=


0
0
0

together with the fact that dim N (P) = 3 −rank (P) = 1.
5.9.2. There are many ways to do this. One way is to write down any basis for ℜ5—say
B = {x1, x2, x3, x4, x5}—and set
X = span {x1, x2}
and
Y = span {x3, x4, x5} .
Property (5.9.4) guarantees that X and Y are complementary.
5.9.3. Let X = {(α, α) | α ∈ℜ} and Y = ℜ2 so that ℜ2 = X + Y, but X ∩Y ̸= 0.
For each vector in ℜ2 we can write
(x, y) = (x, x) + (0, y −x)
and
(x, y) = (y, y) + (x −y, 0).
5.9.4. Exercise 3.2.6 says that each A ∈ℜn×n can be uniquely written as the sum of
a symmetric matrix and a skew-symmetric matrix according to the formula
A = A + AT
2
+ A −AT
2
,
so (5.9.3) guarantees that ℜn×n = S ⊕K. By deﬁnition, the projection of A
onto S along K is the S -component of A —namely (A + AT )/2. For the
given matrix, this is
A + AT
2
=


1
3
5
3
5
7
5
7
9

.
5.9.5. (a)
Assume that X ∩Y = 0. To prove BX ∪BY is linearly independent, write
m

i=1
αixi +
n

j=1
βjyj = 0
=⇒
m

i=1
αixi = −
n

j=1
βjyj
=⇒
m

i=1
αixi ∈X ∩Y = 0
=⇒
m

i=1
αixi = 0
and
n

j=1
βjyj = 0
=⇒
α1 = · · · = αm = β1 = · · · = βn = 0
(because BX and BY are both independent).

Solutions
83
Conversely, if BX ∪BY is linearly independent, then
v ∈X ∩Y
=⇒
v =
m

i=1
αixi
and
v =
n

j=1
βjyj
=⇒
m

i=1
αixi −
n

j=1
βjyj = 0
=⇒
α1 = · · · = αm = β1 = · · · = βn = 0
(because BX ∪BY is independent)
=⇒
v = 0.
(b)
No. Take X to be the xy-plane and Y to be the yz-plane in ℜ3 with
BX = {e1, e2} and BY = {e2, e3}. We have BX ∪BY = {e1, e2, e3}, but
X ∩Y ̸= 0.
(c)
No, the fact that BX ∪BY is linearly independent is no guarantee that
X + Y is the entire space—e.g., consider two distinct lines in ℜ3.
5.9.6. If x is a ﬁxed point for P, then Px = x implies x ∈R (P). Conversely, if
x ∈R (P), then x = Py for some y ∈V
=⇒
Px = P2y = Py = x.
5.9.7. Use (5.9.10) (which you just validated in Exercise 5.9.6) in conjunction with the
deﬁnition of a projector onto X to realize that
x ∈X ⇐⇒Px = x ⇐⇒x ∈R (P),
and
x ∈R (P) ⇐⇒Px = x ⇐⇒(I −P)x = 0 ⇐⇒x ∈N (I −P).
The statements concerning the complementary projector I −P are proven in a
similar manner.
5.9.8. If θ is the angle between R (P) and N (P), it follows from (5.9.18) that ∥P∥2 =
(1/ sin θ) ≥1. Furthermore, ∥P∥2 = 1 if and only if sin θ = 1 (i.e., θ = π/2 ),
which is equivalent to saying R (P) ⊥N (P).
5.9.9. Let θ be the angle between R (P) and N (P). We know from (5.9.11) that
R (I −P) = N (P) and N (I −P) = R (P), so θ is also the angle between
R (I −P) and N (I −P). Consequently, (5.9.18) says that
∥I −P∥2 =
1
sin θ = ∥P∥2 .
5.9.10. The trick is to observe that P = uvT is a projector because vT u = 1 implies
P2 = uvT uvT = uvT = P, so the result of Exercise 5.9.9 insures that
,,I −uvT ,,
2 =
,,uvT ,,
2 .

84
Solutions
To prove that
,,uvT ,,
2 = ∥u∥2 ∥v∥2 , start with the deﬁnition of an induced
matrix given in (5.2.4) on p. 280, and write
,,uvT ,,
2 = max∥x∥2=1
,,uvT x
,,
2 . If
the maximum occurs at x = x0 with ∥x0∥2 = 1, then
,,uvT ,,
2 =
,,uvT x0
,,
2 = ∥u∥2 |vT x0|
≤∥u∥2 ∥v∥2 ∥x0∥2 by CBS inequality
= ∥u∥2 ∥v∥2 .
But we can also write
∥u∥2 ∥v∥2 = ∥u∥2
∥v∥2
2
∥v∥2
= ∥u∥2
(vT v)
∥v∥2
=
,,uvT v
,,
2
∥v∥2
=
,,,,uvT

v
∥v∥2
,,,,
2
≤max
∥x∥2=1
,,uvT x
,,
2
=
,,uvT ,,
2 ,
so
,,uvT ,,
2 = ∥u∥2 ∥v∥2 . Finally, if P = uvT , use Example 3.6.5 to write
∥P∥2
F = trace

PT P
	
= trace(vuT uvT ) = trace(uT uvT v) = ∥u∥2
2 ∥v∥2
2 .
5.9.11. p = Pv = [X | 0][X | Y]−1v = [X | 0]z = Xz1
5.9.12. (a)
Use (5.9.10) to conclude that
R (P) = R (Q)
=⇒
PQ∗j = Q∗j
and
QP∗j = P∗j
∀j
=⇒
PQ = Q
and
QP = P.
Conversely, use Exercise 4.2.12 to write
 PQ = Q
=⇒
R (Q) ⊆R (P)
QP = P
=⇒
R (P) ⊆R (Q)
+
=⇒
R (P) = R (Q).
(b)
Use N (P) = N (Q) ⇐⇒R (I −P) = R (I −Q) together with part (a).
(c)
From part (a), EiEj = Ej so that
 
j
αjEj
2
=

i

j
αiαjEiEj =

i

j
αiαjEj
=
 
i
αi
 
j
αjEj

=

j
αjEj.
5.9.13. According to (5.9.12), the projector onto X along Y is P = B

Ir
0
0
0

B−1,
where B = [X | Y] in which the columns of X and Y form bases for X

Solutions
85
and Y, respectively. Since multiplication by nonsingular matrices does not alter
the rank, it follows that rank (P) = rank

Ir
0
0
0

= r. Using the result of
Example 3.6.5 produces
trace (P) = trace

B

Ir
0
0
0

B−1

= trace

Ir
0
0
0

B−1B

= trace

Ir
0
0
0

= r = rank (P).
5.9.14. (i) =⇒
(ii) : If v = x1 + · · · + xk and v = y1 + · · · + yk, where xi, yi ∈Xi,
then
k

i=1
(xi −yi) = 0
=⇒
(xk −yk) = −
k−1

i=1
(xi −yi)
=⇒
(xk −yk) ∈Xk ∩(X1 + · · · + Xk−1) = 0
=⇒
xk = yk
and
k−1

i=1
(xi −yi) = 0.
Now repeat the argument—to be formal, use induction.
(ii)
=⇒
(iii) : The proof is essentially the same argument as that used to
establish (5.9.3) =⇒(5.9.4).
(iii) =⇒(i) : B always spans X1 + X2 + · · · + Xk, and since the hypothesis is
that B is a basis for V, it follows that B is a basis for both V and X1+· · ·+Xk.
Consequently V = X1 + X2 + · · · + Xk. Furthermore, the set B1 ∪· · · ∪Bk−1 is
linearly independent (each subset of an independent set is independent), and it
spans Vk−1 = X1+· · ·+Xk−1, so B1∪· · ·∪Bk−1 must be a basis for Vk−1. Now,
since (B1∪· · ·∪Bk−1)∪Bk is a basis for V = (X1+· · ·+Xk−1)+Xk, it follows from
(5.9.2)–(5.9.4) that V = (X1 + · · · + Xk−1) ⊕Xk, so Xk ∩(X1 + · · · + Xk−1) = 0.
The same argument can now be repeated on Vk−1—to be formal, use induction.
5.9.15. We know from (5.9.12) that P = Q

I
0
0
0

Q−1 and I−P = Q

0
0
0
I

Q−1,
so
PAP = Q

I
0
0
0

Q−1Q

A11
A12
A21
A22

Q−1Q

I
0
0
0

Q−1
= Q

A11
0
0
0

Q−1.
The other three statements are derived in an analogous fashion.
5.9.16. According to (5.9.12), the projector onto X along Y is P =

X | 0

X | Y
−1,
where the columns of X and Y are bases for X
and Y, respectively. If

86
Solutions

Xn×r | Y
−1 =

Ar×n
C

, then
P =

Xn×r | 0
 
Ar×n
C

= Xn×rAr×n.
The nonsingularity of

X | Y

and

A
C

insures that X has full column rank
and A has full row rank. The fact that AX = Ir is a consequence of

Ir
0
0
I

=

X | Y
−1
X | Y

=

Ar×n
C
 
Xn×r | Y

=

AX
AY
CX
CY

.
5.9.17. (a)
Use the fact that a linear operator P is a projector if and only if P is
idempotent. If EF = FE = 0, then (E + F)2 = E + F. Conversely, if E + F is
a projector, then
(E + F)2 = E + F
=⇒
EF + FE = 0
=⇒
E(EF + FE) = 0
and
(EF + FE)E = 0
=⇒
EF = FE
=⇒
EF = 0 = FE
(because EF + FE = 0).
Thus P = E + F is a projector if and only if EF = FE = 0. Now prove that
under this condition R (P) = X1⊕X2. Start with the fact that z ∈R (P) if and
only if Pz = z, and write each such vector z as z = x1 + y1 and z = x2 + y2,
where xi ∈Xi and yi ∈Yi so that Ex1 = x1,
Ey1 = 0,
Fx2 = x2, and
Fy2 = 0. To prove that R (P) = X1 + X2, write
z ∈R (P)
=⇒
Pz = z
=⇒
(E + F)z = z
=⇒
(E + F)(x2 + y2) = (x2 + y2)
=⇒
Ez = y2
=⇒
x1 = y2
=⇒
z = x1 + x2 ∈X1 + X2
=⇒
R (P) ⊆X1 + X2.
Conversely, X1 + X2 ⊆R (P) because
z ∈X1 + X2
=⇒
z = x1 + x2,
where
x1 ∈X1 and x2 ∈X2
=⇒
x1 = Ex1
and
x2 = Fx2
=⇒
Fx1 = FEx1 = 0 and Ex2 = EFx2 = 0
=⇒
Pz = (E + F)(x1 + x2) = x1 + x2 = z
=⇒
z ∈R (P).
The fact that X1 and X2 are disjoint follows by writing
z ∈X1 ∩X2
=⇒
Ez = z = Fz
=⇒
z = EFz = 0,

Solutions
87
and thus R (P) = X1 ⊕X2 is established. To prove that N (P) = Y1 ∩Y2, write
Pz = 0
=⇒
(E + F)z = 0
=⇒
Ez = −Fz
=⇒
Ez = −EFz
and
FEz = −Fz
=⇒
Ez = 0
and
0 = Fz
=⇒
z ∈Y1 ∩Y2.
5.9.18. Use the hint together with the result of Exercise 5.9.17 to write
E −F is a projector ⇐⇒I −(E −F) is a projector
⇐⇒(I −E) + F is a projector
⇐⇒(I −E)F = 0 = F(I −E)
⇐⇒EF = F = FE.
Under this condition, Exercise 5.9.17 says that
R (I −E + F) = R (I −E) ⊕R (F) and N (I −E + F) = N (I −E) ∩N (F),
so (5.9.11) guarantees
R (E −F) = N (I −E + F) = N (I −E) ∩N (F) = R (E) ∩N (F) = X1 ∩Y2
N (E −F) = R (I −E + F) = R (I −E) ⊕R (F) = N (E) ⊕R (F) = Y1 ⊕X2.
5.9.19. If EF = P = FE, then P is idempotent, and hence P is a projector. To prove
that R (P) = X1 ∩X2, write
z ∈R (P)
=⇒
Pz = z
=⇒
E(Fz) = z
and
F(Ez) = z
=⇒
z ∈R (E) ∩R (F) = X1 ∩X2
=⇒
R (P) ⊆X1 ∩X2.
Conversely,
z ∈X1 ∩X2
=⇒
Ez = z = Fz
=⇒
Pz = z
=⇒
X1 ∩X2 ⊆R (P),
and hence R (P) = X1 ∩X2. To prove that N (P) = Y1 + Y2, ﬁrst notice that
z ∈N (P)
=⇒
FEz = 0
=⇒
Ez ∈N (F).
This together with the fact that (I −E)z ∈N (E) allows us to conclude that
z = (I −E)z + Ez ∈N (E) + N (F) = Y1 + Y2
=⇒
N (P) ⊆Y1 + Y2.

88
Solutions
Conversely,
z ∈Y1 + Y2
=⇒
z = y1 + y2, where yi ∈Yi for i = 1, 2
=⇒
Ey1 = 0
and
Fy2 = 0
=⇒
Pz = 0
=⇒
Y1 + Y2 ⊆N (P).
Thus N (P) = Y1 + Y2.
5.9.20. (a)
For every inner pseudoinverse, AA−is a projector onto R (A), and I −
A−A is a projector onto N (A). The system being consistent means that
b ∈R (A) = R

AA−	
=⇒
AA−b = b,
so A−b is a particular solution. Therefore, the general solution of the system is
A−b + N (A) = A−b + R

I −A−A
	
.
(b)
A−A is a projector along N (A), so Exercise 5.9.12 insures Q(A−A) = Q
and (A−A)Q = (A−A). This together with the fact that PA = A allows us
to write
AXA = AQA−PA = AQA−A = AQ = AA−AQ = AA−A = A.
Similarly,
XAX = (QA−P)A(QA−P) = QA−(PA)QA−P = QA−AQA−P
= Q(A−AQ)A−P = Q(A−A)A−P = QA−P = X,
so X is a reﬂexive pseudoinverse for A. To show X has the prescribed range
and nullspace, use the fact that XA is a projector onto R (X) and AX is a
projector along N (X) to write
R (X) = R (XA) = R

QA−PA
	
= R

QA−A
	
= R (Q) = L
and
N (X) = N (AX) = N

AQA−P
	
= N

AA−AQA−P
	
= N

AA−AA−P
	
= N

AA−P
	
= N (P) = M.
To prove uniqueness, suppose that X1 and X2 both satisfy the speciﬁed con-
ditions. Then
N (X2) = M = R (I −AX1)
=⇒
X2(I −AX1) = 0
=⇒
X2 = X2AX1
and
R (X2A) = R (X2) = L = R (X1)
=⇒
X2AX1 = X1,
so X2 = X1.

Solutions
89
Solutions for exercises in section 5. 10
5.10.1. Since index(A) = k, we must have that
rank

Ak−1	
> rank

Ak	
= rank

Ak+1	
= · · · = rank

A2k	
= · · · ,
so rank

Ak	
= rank(Ak)2, and hence index(Ak) ≤1. But Ak is singular
(because A is singular) so that index(Ak) > 0. Consequently, index(Ak) = 1.
5.10.2. In this case, R

Ak	
= 0 and N

Ak	
= ℜn. The nonsingular component C
in (5.10.5) is missing, and you can take Q = I, thereby making A its own
core-nilpotent decomposition.
5.10.3. If A is nonsingular, then index(A) = 0, regardless of whether or not A is
symmetric. If A is singular and symmetric, we want to prove index(A) = 1.
The strategy is to show that R (A) ∩N (A) = 0 because this implies that
R (A) ⊕N (A) = ℜn. To do so, start with
x ∈R (A) ∩N (A)
=⇒
Ax = 0
and
x = Ay
for some
y.
Now combine this with the symmetry of A to obtain
xT = yT AT = yT A
=⇒
xT x = yT Ax = 0
=⇒
∥x∥2
2 = 0
=⇒
x = 0.
5.10.4. index(A) = 0 when A is nonsingular. If A is singular and normal we want
to prove index(A) = 1. The strategy is to show that R (A) ∩N (A) = 0
because this implies that R (A)⊕N (A) = Cn. Recall from (4.5.6) that N (A) =
N (A∗A) and N (A∗) = N (AA∗), so N (A) = N (A∗). Start with
x ∈R (A) ∩N (A)
=⇒
Ax = 0
and
x = Ay
for some
y,
and combine this with N (A) = N (A∗) to obtain
A∗x = 0 and x = Ay
=⇒
x∗x = y∗A∗x = 0
=⇒
∥x∥2
2 = 0
=⇒
x = 0.
5.10.5. Compute rank

A0	
= 3, rank (A) = 2, rank

A2	
= 1, and rank

A3	
= 1,
to see that k = 2 is the smallest integer such that rank

Ak	
= rank

Ak+1	
,
so index(A) = 2. The matrix Q = [X | Y] is a matrix in which the columns of
X are a basis for R

A2	
, and the columns of Y are a basis for N

A2	
. Since
EA2 =


1
1
0
0
0
0
0
0
0

,
we have
X =


−8
12
8


and
Y =


−1
0
1
0
0
1

,
so
Q =


−8
−1
0
12
1
0
8
0
1

.

90
Solutions
It can now be veriﬁed that
Q−1AQ =


1/4
1/4
0
−3
−2
0
−2
−2
1




−2
0
−4
4
2
4
3
2
2




−8
−1
0
12
1
0
8
0
1

=


2
0
0
0
−2
4
0
−1
2

,
where
C = [2]
and
N =

−2
4
−1
2

,
and N2 = 0. Finally, AD = Q

C−1
0
0
0

Q−1 =


−1
−1
0
3/2
3/2
0
1
1
0

.
5.10.6. (a)
Because
J −λI =





1 −λ
0
0
0
0
0
1 −λ
0
0
0
0
0
1 −λ
0
0
0
0
0
2 −λ
0
0
0
0
0
2 −λ




,
and because a diagonal matrix is singular if and only if it has a zero-diagonal
entry, it follows that J−λI is singular if and only if λ = 1 or λ = 2, so λ1 = 1
and λ2 = 2 are the two eigenvalues of J. To ﬁnd the index of λ1, use block
multiplication to observe that
J −I =

0
0
0
I2×2

=⇒
rank (J −I) = 2 = rank (J −I)2.
Therefore, index(λ1) = 1. Similarly,
J −2I =

−I3×3
0
0
0

and
rank (J −2I) = 3 = rank (J −2I)2,
so index(λ2) = 1.
(b)
Since
J −λI =





1 −λ
1
0
0
0
0
1 −λ
1
0
0
0
0
1 −λ
0
0
0
0
0
2 −λ
1
0
0
0
0
2 −λ




,
and since a triangular matrix is singular if and only if there exists a zero-diagonal
entry (i.e., a zero pivot), it follows that J −λI is singular if and only if λ = 1

Solutions
91
or λ = 2, so λ1 = 1 and λ2 = 2 are the two eigenvalues of J. To ﬁnd the
index of λ1, use block multiplication to compute
J −I =





0
1
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
1
1
0
0
0
0
1




,
(J −I)2 =





0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
2
0
0
0
0
1




,
(J −I)3 =





0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
3
0
0
0
0
1




,
(J −I)4 =





0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
4
0
0
0
0
1




.
Since
rank (J −I) > rank (J −I)2 > rank (J −I)3 = rank (J −I)4,
it follows that index(λ1) = 3. A similar computation using λ2 shows that
rank (J −2I) > rank (J −2I)2 = rank (J −2I)3,
so index(λ2) = 2. The fact that eigenvalues associated with diagonal matrices
have index 1 while eigenvalues associated with triangular matrices can have
higher indices is no accident. This will be discussed in detail in §7.8 (p. 587).
5.10.7. (a)
If P is a projector, then, by (5.9.13), P = P2, so rank (P) = rank

P2	
,
and hence index(P) ≤1. If P ̸= I, then P is singular, and thus index(P) = 1.
If P = I, then index(P) = 0. An alternate argument could be given on the
basis of the observation that ℜn = R (P) ⊕N (P).
(b)
Recall from (5.9.12) that if the columns of X and Y constitute bases for
R (P) and N (P), respectively, then for Q =

X | Y

,
Q−1PQ =

I
0
0
0

,
and it follows that

I
0
0
0

is the core-nilpotent decomposition for P.
5.10.8. Suppose that 
k−1
i=0 αiNix = 0, and multiply both sides by Nk−1 to ob-
tain α0Nk−1x = 0. By assumption, Nk−1x ̸= 0, so α0 = 0, and hence

k−1
i=1 αiNix = 0. Now multiply both sides of this equation by Nk−2 to pro-
duce α1Nk−1x = 0, and conclude that α1 = 0. Continuing in this manner (or
by making a formal induction argument) gives α0 = α1 = α2 = · · · = αk−1 = 0.
5.10.9. (a)
b ∈R

Ak	
⊆R (A)
=⇒
b ∈R (A)
=⇒
Ax = b is consistent.

92
Solutions
(b)
We saw in Example 5.10.5 that when considered as linear operators re-
stricted to R

Ak	
, both A and AD are invertible, and in fact they are
true inverses of each other. Consequently, A and AD are one-to-one map-
pings on R

Ak	
(recall Exercise 4.7.18), so for each b ∈R

Ak	
there is
a unique x ∈R

Ak	
such that Ax = b, and this unique x is given by
x =

A/R(Ak)
−1
b = ADb.
(c)
Part (b) shows that ADb is a particular solution. The desired result follows
because the general solution is any particular solution plus the general solution
of the associated homogeneous equation.
5.10.10. Notice that AAD = Q

I
0
0
0

Q−1, and use the results from Example 5.10.3
(p. 398). I −AAD is the complementary projector, so it projects onto N

Ak	
along R

Ak	
.
5.10.11. In each case verify that the axioms (A1), (A2), (A4), and (A5) in the deﬁnition
of a vector space given on p. 160 hold for matrix multiplication (rather than
+). In parts (a) and (b) the identity element is the ordinary identity matrix,
and the inverse of each member is the ordinary inverse. In part (c), the identity
element is E =

1/2
1/2
1/2
1/2

because AE = A = EA for each A ∈G, and

α
α
α
α
#
= 1
4α

1
1
1
1

because AA# = E = A#A.
5.10.12. (a)
=⇒
(b) : If A belongs to a matrix group G in which the identity element
is E, and if A# is the inverse of A in G, then A#A2 = EA = A, so
x ∈R (A) ∩N (A)
=⇒
x = Ay for some y and Ax = 0
=⇒
Ay = A#A2y = A#Ax = 0
=⇒
x = 0.
(b)
=⇒
(c) : Suppose A is n × n, and let BR and BN be bases for R (A)
and N (A), respectively. Verify that B = R (A) ∩N (A) = 0 implies BR ∩BN
is a linearly independent set, and use the fact that there are n vectors in B to
conclude that B is a basis for ℜn. Statement (c) now follows from (5.9.4).
(c)
=⇒
(d) : Use the fact that R

Ak	
∩N

Ak	
= 0.
(d)
=⇒
(e) : Use the result of Example 5.10.5 together with the fact that
the only nilpotent matrix of index 1 is the zero matrix.
(e)
=⇒
(a) : It is straightforward to verify that the set
G =

Q

Xr×r
0
0
0

Q−1  X is nonsingular
+
is a matrix group, and it’s clear that A ∈G.

Solutions
93
5.10.13. (a)
Use part (e) of Exercise 5.10.12 to write A = Q

Cr×r
0
0
0

Q−1. For the
given E, verify that EA = AE = A for all A ∈G. The fact that E is the
desired projector follows from (5.9.12).
(b)
Simply verify that AA# = A#A = E. Notice that the group inverse
agrees with the Drazin inverse of A described in Example 5.10.5. However, the
Drazin inverse exists for all square matrices, but the concept of a group inverse
makes sense only for group matrices—i.e., when index(A) = 1.
Solutions for exercises in section 5. 11
5.11.1. Proceed as described on p. 199 to determine the following bases for each of the
four fundamental subspaces.
R (A) = span





2
−1
−2

,


1
−1
−1





N

AT 	
= span





1
0
1





N (A) = span





−1
1
1





R

AT 	
= span





1
0
1

,


0
1
−1





Since each vector in a basis for R (A) is orthogonal to each vector in a basis
for N

AT 	
, it follows that R (A) ⊥N

AT 	
. The same logic also explains
why N (A) ⊥R

AT 	
. Notice that R (A) is a plane through the origin in ℜ3,
and N

AT 	
is the line through the origin perpendicular to this plane, so it
is evident from the parallelogram law that R (A) ⊕N

AT 	
= ℜ3. Similarly,
N (A) is the line through the origin normal to the plane deﬁned by R

AT 	
, so
N (A) ⊕R

AT 	
= ℜ3.
5.11.2. V⊥= 0, and 0⊥= V.
5.11.3. If A =



1
2
2
4
0
1
3
6


, then R (A) = M, so (5.11.5) insures M⊥= N

AT 	
. Using
row operations, a basis for N

AT 	
is computed to be








−2
1
0
0


,



−3
0
0
1








.
5.11.4. Verify that M⊥is closed with respect to vector addition and scalar multipli-
cation. If x, y ∈M⊥, then ⟨m x⟩= 0 = ⟨m y⟩for each m ∈M so that
⟨m x + y⟩= 0 for each m ∈M, and thus x + y ∈M⊥. Similarly, for every
scalar α we have ⟨m αx⟩= α ⟨m x⟩= 0 for each m ∈M, so αx ∈M⊥.
5.11.5. (a)
x ∈N ⊥=⇒x ⊥N ⊇M =⇒x ⊥M =⇒x ∈M⊥.

94
Solutions
(b)
Simply observe that
x ∈(M + N)⊥⇐⇒x ⊥(M + N)
⇐⇒x ⊥M and x ⊥N
⇐⇒x ∈

M⊥∩N ⊥	
.
(c)
Use part (b) together with (5.11.4) to write

M⊥+ N ⊥	⊥= M⊥⊥∩N ⊥⊥= M ∩N,
and then perp both sides.
5.11.6. Use the fact that dim R

AT 	
= rank

AT 	
= rank (A) = dim R (A) together
with (5.11.7) to conclude that
n = dim N (A) + dim R

AT 	
= dim N (A) + dim R (A).
5.11.7. U is a unitary matrix in which the columns of U1 are an orthonormal basis for
R (A) and the columns of U2 are an orthonormal basis for N

AT 	
, so setting
X = U1,
Y = U2, and

X | Y
−1 = UT in (5.9.12) produces P = U1UT
1 .
According to (5.9.9), the projector onto N

AT 	
along R (A) is I −P = I −
U1UT
1 = U2UT
2 .
5.11.8. Start with the ﬁrst column of A, and set u = A∗1 + 6e1 = ( 2
2
−4 )T to
obtain
R1 = I−2uuT
uT u = 1
3


2
−1
2
−1
2
2
2
2
−1


and
R1A =


−6
0
−6
−3
0
0
0
0
0
−3
0
0

.
Now set u =

0
−3

+ 3e1 =

3
−3

to get
ˆR2 = I −2uuT
uT u =

0
1
1
0

and
R2 =

1
0
0
ˆR2

=


1
0
0
0
0
1
0
1
0

,
so
P = R2R1 = 1
3


2
−1
2
2
2
−1
−1
2
2

and PA =


−6
0
−6
−3
0
−3
0
0
0
0
0
0

=

B
0

.

Solutions
95
Therefore, rank (A) = 2, and orthonormal bases for R (A) and N

AT 	
are
extracted from the columns of U = PT as shown below.
R (A) = span





2/3
−1/3
2/3

,


2/3
2/3
−1/3





and
N

AT 	
= span





−1/3
2/3
2/3





Now work with BT , and set u = (B1∗)T + 9e1 = ( 3
0
−6
−3 )T to get
Q = I−2uuT
uT u = 1
3



2
0
2
1
0
3
0
0
2
0
−1
−2
1
0
−2
2



and
QBT =



−9
0
0
−3
0
0
0
0


=

T
0

.
Orthonormal bases for R

AT 	
and N (A) are extracted from the columns of
V = QT = Q as shown below.
R

AT 	
=span








2/3
0
2/3
1/3


,



0
1
0
0








and N (A)=span








2/3
0
−1/3
−2/3


,



1/3
0
−2/3
2/3








A URV factorization is obtained by setting U = PT , V = QT , and
R =

TT
0
0
0

=


−9
0
0
0
0
−3
0
0
0
0
0
0

.
5.11.9. Using EA =


1
0
1
1/2
0
1
0
0
0
0
0
0

along with the standard methods of Chapter 4,
we have
R (A) = span





−4
2
−4

,


−2
−2
1





and
N

AT 	
= span





−1
2
2




,
R

AT 	
= span








1
0
1
1/2


,



0
1
0
0








and N (A) = span








−1
0
1
0


,



−1/2
0
0
1








.
Applying the Gram–Schmidt procedure to each of these sets produces the fol-
lowing orthonormal bases for the four fundamental subspaces.
BR(A) =



1
3


−2
1
−2

, 1
3


−2
−2
1





BN(AT) =



1
3


−1
2
2






96
Solutions
BR(AT) =





1
3



2
0
2
1


,



0
1
0
0








BN(A) =





1
√
2



−1
0
1
0


,
1
3
√
2



−1
0
−1
4








The matrices U and V were deﬁned in (5.11.8) to be
U =

BR(A) ∪BN(AT)

= 1
3


−2
−2
−1
1
−2
2
−2
1
2


and
V =

BR(AT) ∪BN(A)

= 1
3



2
0
−3/
√
2
−1/
√
2
0
3
0
0
2
0
3/
√
2
−1/
√
2
1
0
0
4/
√
2


.
Direct multiplication now produces
R = UT AV =


9
0
0
0
0
3
0
0
0
0
0
0

.
5.11.10. According to the discussion of projectors on p. 386, the unique vectors satisfying
v = x+y, x ∈R (A), and y ∈N

AT 	
are given by x = Pv and y = (I−P)v,
where P is the projector onto R (A) along N

AT 	
. Use the results of Exercise
5.11.7 and Exercise 5.11.8 to compute
P = U1UT
1 = 1
9


8
2
2
2
5
−4
2
−4
5

, x = Pv =


4
1
1

, y = (I −P)v =


−1
2
2

.
5.11.11. Observe that
R (A) ∩N (A) = 0
=⇒
index(A) ≤1,
R (A) ̸⊥N (A)
=⇒
A is singular,
R (A) ̸⊥N (A)
=⇒
R

AT 	
̸= R (A).
It is now trial and error to build a matrix that satisﬁes the three conditions on
the right-hand side. One such matrix is A =

1
2
1
2

.
5.11.12. R (A) ⊥N (A) =⇒R (A)∩N (A) = 0 =⇒index(A) = 1 by using (5.10.4).
The example in the solution to Exercise 5.11.11 shows that the converse is false.
5.11.13. The facts that real symmetric
=⇒
hermitian
=⇒
normal are direct conse-
quences of the deﬁnitions. To show that normal =⇒RPN, use (4.5.5) to write
R (A) = R (AA∗) = R (A∗A) = R (A∗). The matrix

1
i
−i
2

is hermitian
but not symmetric. To construct a matrix that is normal but not hermitian or

Solutions
97
real symmetric, try to ﬁnd an example with real numbers. If A =

a
b
c
d

,
then
AAT =

a2 + b2
ac + bd
ac + bd
c2 + d2

and
AT A =

a2 + c2
ab + cd
ab + cd
b2 + d2

,
so we need to have b2 = c2. One such matrix is A =

1
−1
1
1

. To construct
a singular matrix that is RPN but not normal, try again to ﬁnd an example with
real numbers. For any orthogonal matrix P and nonsingular matrix C, the
matrix A = P

C
0
0
0

PT is RPN. To prevent A from being normal, simply
choose C to be nonnormal. For example, let C =

1
2
3
4

and P = I.
5.11.14. (a)
A∗A = AA∗
=⇒
(A −λI)∗(A −λI) = (A −λI) (A −λI)∗
=⇒
(A −λI) is normal
=⇒
(A −λI) is RPN
=⇒
R (A −λI) ⊥N (A −λI) .
(b)
Suppose x ∈N (A −λI) and y ∈N (A −µI), and use the fact that
N (A −λI) = N (A −λI)∗to write
(A −λI) x = 0
=⇒
0 = x∗(A −λI)
=⇒
0 = x∗(A −λI) y
= x∗(µy −λy) = x∗y(µ −λ)
=⇒
x∗y = 0.
Solutions for exercises in section 5. 12
5.12.1. Since CT C =

25
0
0
100

, σ2
1 = 100, and it’s clear that x = e2 is a vector
such that (CT C −100I)x = 0 and ∥x∥2 = 1. Let y = Cx/σ1 =

−3/5
−4/5

.
Following the procedure in Example 5.6.3, set ux = x −e1 and uy = y −e1,
and construct
Rx = I −2uxuT
x
uTx ux
=

0
1
1
0

and
Ry = I −2uyuT
y
uTy uy
=

−3/5
−4/5
−4/5
3/5

.
Since RyCRx =

10
0
0
5

= D, it follows that C = RyDRx is a singular
value decomposition of C.
5.12.2. ν2
1(A) = σ2
1 = ∥A∥2
2 needs no proof—it’s just a restatement of (5.12.4). The
fact that ν2
r(A) = ∥A∥2
F amounts to observing that
∥A∥2
F = trace

AT A
	
= traceV

D2
0
0
0

VT = trace

D2	
= σ2
1 + · · · + σ2
r.

98
Solutions
5.12.3. If σ1 ≥· · · ≥σr are the nonzero singular values for A, then it follows from
Exercise 5.12.2 that ∥A∥2
2 = σ2
1 ≤σ2
1 + σ2
2 + · · · + σ2
r = ∥A∥2
F ≤nσ2
1 = n∥A∥2
2.
5.12.4. If rank (A + E) = k < r, then (5.12.10) implies that
∥E∥2 = ∥A −(A + E)∥2 ≥
min
rank(B)=k ∥A −B∥2 = σk+1 ≥σr,
which is impossible. Hence rank (A + E) ≥r = rank (A).
5.12.5. The argument is almost identical to that given for the nonsingular case except
that A† replaces A−1. Start with SVDs
A = U

D
0
0
0

VT
and
A† = V

D−1
0
0
0

UT ,
where D = diag (σ1, σ2, . . . , σr) , and note that
,,A†Ax
,,
2 ≤
,,A†A
,,
2 ∥x∥2 = 1
with equality holding when A†A = I (i.e., when r = n ). For each y ∈A(S2)
there is an x ∈S2 such that y = Ax, so, with w = UT y,
1 ≥
,,A†Ax
,,2
2 =
,,A†y
,,2
2 =
,,VD−1UT y
,,2
2 =
,,D−1UT y
,,2
2
=
,,D−1w
,,2
2 = w2
1
σ2
1
+ w2
2
σ2
2
+ · · · + w2
r
σ2r
with equality holding when r = n. In other words, the set UT A(S2) is an
ellipsoid (degenerate if r < n ) whose kth semiaxis has length σk. To resolve
the inequality with what it means for points to be on an ellipsoid, realize that the
surface of a degenerate ellipsoid (one having some semiaxes with zero length) is
actually the set of all points in and on a smaller dimension ellipsoid. For example,
visualize an ellipsoid in ℜ3, and consider what happens as one of its semiaxes
shrinks to zero. The skin of the three-dimensional ellipsoid degenerates to a solid
planar ellipse. In other words, all points on a degenerate ellipsoid with semiaxes
of length σ1 ̸= 0, σ2 ̸= 0, σ3 = 0 are actually points on and inside a planar
ellipse with semiaxes of length σ1 and σ2. Arguing that the kth semiaxis of
A(S2) is σkU∗k = AV∗k is the same as the nonsingular case given in the text.
5.12.6. If A = U

D
0
0
0

VT and A†
n×m = V

D−1
0
0
0

UT are SVDs in which
V =

V1 | V2
	
, then the columns of V1 are an orthonormal basis for R

AT 	
,
so x ∈R

AT 	
and ∥x∥2 = 1 if and only if x = V1y with ∥y∥2 = 1. Since
the 2-norm is unitarily invariant (Exercise 5.6.9),
min
∥x∥2=1
x∈R(AT )
∥Ax∥2 =
min
∥y∥2=1 ∥AV1y∥2 =
min
∥y∥2=1 ∥Dy∥2 =
1
∥D−1∥2
= σr =
1
∥A†∥2
.

Solutions
99
5.12.7. x = A†b and ˜x = A†(b −e) are the respective solutions of minimal 2-norm of
Ax = b and A˜x = ˜b = b −e. The development of the more general bound is
the same as for (5.12.8).
∥x −˜x∥= ∥A†(b −˜b)∥≤∥A†∥∥b −˜b∥,
b = Ax
=⇒
∥b∥≤∥A∥∥x∥
=⇒
1/∥x∥≤∥A∥/∥b∥,
so
∥x −˜x∥
∥x∥
≤

∥A†∥∥b −˜b∥
 ∥A∥
∥b∥= κ ∥e∥
∥b∥.
Similarly,
∥b −˜b∥= ∥A(x −˜x)∥≤∥A∥∥x −˜x∥,
x = A†b
=⇒
∥x∥≤∥A†∥∥b∥
=⇒
1/∥b∥≤∥A†∥/∥x∥,
so
∥b −˜b∥
∥b∥
≤(∥A∥∥x −˜x∥) ∥A†∥
∥x∥= κ∥x −˜x∥
∥x∥
.
Equality was attained in Example 5.12.1 by choosing b and e to point in
special directions. But for these choices, Ax = b and A˜x = ˜b = b −e cannot
be guaranteed to be consistent for all singular or rectangular matrices A, so
the answer to the second part is “no.” However, the argument of Example 5.12.1
proves equality for all A such that AA† = I (i.e., when rank (Am×n) = m ).
5.12.8. If A = U

D
0
0
0

VT is an SVD, then AT A+ϵI = U

D2 + ϵI
0
0
ϵI

VT is
an SVD with no zero singular values, so it’s nonsingular. Furthermore,
(AT A + ϵI)−1AT = U

(D2 + ϵI)−1D
0
0
0

VT →U

D−1
0
0
0

VT = A†.
5.12.9. Since A−1 =

−266000
667000
333000
−835000

,
κ∞= ∥A∥∞
,,A−1,,
∞= 1, 754, 336.
Similar to the 2-norm situation discussed in Example 5.12.1, the worst case is
realized when b is in the direction of a maximal vector in A(S∞) while e is
in the direction of a minimal vector in A(S∞). Sketch A(S∞) as shown below
to see that v = ( 1.502
.599 )T is a maximal vector in A(S∞).

100
Solutions
(.168, .067)
(-.168, -.067)
(-1.502, -.599)
(1.502, .599)
(1, -1)
(-1, -1)
(-1, 1)
(1, 1)
A
It’s not clear which vector is minimal—don’t assume ( .168
.067 )T is. A min-
imal vector y in A(S∞) satisﬁes ∥y∥∞= min∥x∥∞=1 ∥Ax∥∞= 1/
,,A−1,,
∞
(see (5.2.6) on p. 280), so, for y = Ax0 with ∥x0∥∞= 1,
,,,,A−1
 y
∥y∥∞
,,,,
∞
= ∥x0∥∞
∥y∥∞
=
1
∥y∥∞
=
,,A−1,,
∞=
max
∥z∥∞=1
,,A−1z
,,
∞.
In other words, ˆy = y/ ∥y∥∞must be a vector in S∞that receives maximal
stretch under A−1. You don’t have to look very hard to ﬁnd such a vector
because its components are ±1—recall the proof of (5.2.15) on p. 283. Notice
that ˆy = ( 1
−1 )T ∈S∞, and ˆy receives maximal stretch under A−1 because
,,A−1y
,,
∞= 1, 168, 000 =
,,A−1,,
∞, so setting
b = αv = α

1.502
.599

and
e = βˆy = β

1
−1

produces equality in (5.12.8), regardless of α and β. You may wish to compu-
tationally verify that this is indeed the case.
5.12.10. (a)
Consider A =

ϵ
−1
1
0

or A =

ϵ
ϵn
0
ϵ

for small ϵ ̸= 0.
(b)
For α > 1, consider
A =






1
−α
0
· · ·
0
0
1
−α
· · ·
0
...
...
...
...
...
0
0
· · ·
1
−α
0
0
· · ·
0
1






n×n
and A−1 =






1
α
· · ·
αn−2
αn−1
0
1
· · ·
αn−3
αn−2
...
...
...
...
...
0
0
· · ·
1
α
0
0
· · ·
0
1






.
Regardless of which norm is used, ∥A∥> α and
,,A−1,, > αn−1, so κ > αn
exhibits exponential growth. Even for moderate values of n and α > 1, κ can
be quite large.

Solutions
101
5.12.11. For B = A−1E, write (A −E) = A(I −B), and use the Neumann series
expansion to obtain
˜x = (A−E)−1b = (I−B)−1A−1b = (I+B+B2+· · ·)x = x+B(I+B+B2+· · ·)x.
Therefore, ∥x −˜x∥≤∥B∥
∞
n=0 ∥B∥n ∥x∥≤
,,A−1,, ∥E∥∥x∥
∞
n=0 αn, so
∥x −˜x∥
∥x∥
≤
,,A−1,, ∥E∥
1
1 −α = ∥A∥
,,A−1,, ∥E∥
∥A∥
1
1 −α =
κ
1 −α
∥E∥
∥A∥.
5.12.12. Begin with
x −˜x = x −(I −B)−1A−1(b −e) =

I −(I −B)−1	
x + (I −B)−1A−1e.
Use the triangle inequality with b = Ax ⇒1/ ∥x∥≤∥A∥/ ∥b∥to obtain
∥x −˜x∥
∥x∥
≤
,,I −(I −B)−1,, +
,,(I −B)−1,, κ ∥e∥
∥b∥.
Write (I−B)−1 = 
∞
i=0 Bi, and use the identity I−(I −B)−1 = −B(I −B)−1
to produce
,,(I −B)−1,, ≤
∞

i=0
∥B∥i =
1
1 −∥B∥
and
,,I −(I −B)−1,, ≤
∥B∥
1 −∥B∥.
Now combine everything above with ∥B∥≤
,,A−1,, ∥E∥= κ ∥E∥/ ∥A∥.
5.12.13. Even though the URV factors are not unique, A† is, so in each case you should
arrive at the same matrix
A† = VR†UT = 1
81



−4
2
−4
−18
−18
9
−4
2
−4
−2
1
−2


.
5.12.14. By (5.12.17), the minimum norm solution is A†b = (1/9) ( 10
9
10
5 )T .
5.12.15. U is a unitary matrix in which the columns of U1 are an orthonormal basis for
R (A) and the columns of U2 are an orthonormal basis for N

AT 	
, so setting
X = U1,
Y = U2, and

X | Y
−1 = UT in (5.9.12) produces P = U1UT
1 .
Furthermore,
AA† = U

C
0
0
0

VT V

C−1
0
0
0

UT = U

I
0
0
0

UT = U1UT
1 .
According to (5.9.9), the projector onto N

AT 	
along R (A) is I −P = I −
U1UT
1 = U2UT
2 = I −AA†.

102
Solutions
5.12.16. (a)
When A is nonsingular, U = V = I and R = A, so A† = A−1.
(b)
If A = URVT is as given in (5.12.16), where R =

C
0
0
0

, it is clear
that (R†)
† = R, and hence (A†)
† = (VR†UT )† = U(R†)
† VT = URVT =
A.
(c)
For R as above, it is easy to see that (R†)
T = (RT )
† , so an argument
similar to that used in part (b) leads to (A†)
T = (AT )
† .
(d)
When rank (Am×n) = n, an SVD must have the form
A = Um×m

Dn×n
0m−n×n

In×n,
so
A† = I ( D−1
0 ) UT .
Furthermore, AT A = D2, and (AT A)−1AT = I ( D−1
0 ) UT = A†. The
other part is similar.
(e)
AT AA† = V

CT
0
0
0

UT U

Cr×r
0
0
0

VT V

C−1
0
0
0

UT = AT .
The other part is similar.
(f)
Use an SVD to write
AT (AAT )† = V

DT
0
0
0

UT U

D−2
0
0
0

UT = V

D−1
0
0
0

UT = A†.
The other part is similar.
(g)
The URV factorization insures that rank

A†	
= rank (A) = rank

AT 	
,
and part (f) implies R

A†	
⊆R

AT 	
, so R

A†	
= R

AT 	
. Argue that
R

AT 	
= R

A†A
	
by using Exercise 5.12.15. The other parts are similar.
(h)
If A = URVT is a URV factorization for A, then (PU)R(QT V)T is a
URV factorization for B = PAQ. So, by (5.12.16), we have
B† = QT V

C−1
0
0
0

UT PT = QT A†PT .
Almost any two singular or rectangular matrices can be used to build a coun-
terexample to show that (AB)† is not always the same as B†A†.
(i)
If A = URVT , then (AT A)† = (VRT UT URV)† = VT (RT R)†VT . Sim-
ilarly, A†(AT )† = VR†UT URT †VT = VR†RT †VT = VT (RT R)†VT . The
other part is argued in the same way.
5.12.17. If A is RPN, then index(A) = 1, and the URV decomposition (5.11.15) is a
similarity transformation of the kind (5.10.5). That is, N = 0 and Q = U, so
AD as deﬁned in (5.10.6) is the same as A† as deﬁned by (5.12.16). Conversely,
if A† = AD, then
AAD = ADA
=⇒
A†A = AA†
=⇒
R (A) = R

AT 	
.

Solutions
103
5.12.18. (a)
Recall that ∥B∥2
F = trace

BT B
	
, and use the fact that R (X) ⊥R (Y)
implies XT Y = 0 = YT X to write
∥X + Y∥2
F = trace

(X + Y)T (X + Y)

= trace

XT X + XT Y + YT X + YT Y
	
= trace

XT X
	
+ trace

YT Y
	
= ∥X∥2
F + ∥Y∥2
F .
(b)
Consider X =

2
0
0
0

and Y =

0
0
0
3

.
(c)
Use the result of part (a) to write
∥I −AX∥2
F =
,,I −AA† + AA† −AX
,,2
F
=
,,I −AA†,,2
F +
,,AA† −AX
,,2
F
≥
,,I −AA†,,2
F ,
with equality holding if and only if AX = AA†—i.e., if and only if X = A†+Z,
where R (Z) ⊆N (A) ⊥R

AT 	
= R

A†	
. Moreover, for any such X,
∥X∥2
F =
,,A† + Z
,,2
F =
,,A†,,2
F + ∥Z∥2
F ≥
,,A†,,2
F
with equality holding if and only if Z = 0.
Solutions for exercises in section 5. 13
5.13.1. PM = uuT /(uT u) = (1/10)
 9
3
3
1

, and PM⊥= I −PM = (1/10)

1
−3
−3
9

,
so PMb =

6
2

, and PM⊥b =

−2
6

.
5.13.2. (a)
Use any of the techniques described in Example 5.13.3 to obtain the fol-
lowing.
PR(A) =


.5
0
.5
0
1
0
.5
0
.5


PN(A) =


.8
−.4
0
−.4
.2
0
0
0
0


PR(AT ) =


.2
.4
0
.4
.8
0
0
0
1


PN(AT ) =


.5
0
−.5
0
0
0
−.5
0
.5


(b)
The point in N (A)⊥that is closest to b is
PN(A)⊥b = PR(AT )b =


.6
1.2
1

.

104
Solutions
5.13.3. If x ∈R (P), then Px = x—recall (5.9.10)—so ∥Px∥2 = ∥x∥2 . Conversely,
suppose ∥Px∥2 = ∥x∥2 , and let x = m+n, where m ∈R (P) and n ∈N (P)
so that m ⊥n. The Pythagorean theorem (Exercise 5.4.14) guarantees that
∥x∥2
2 = ∥m + n∥2
2 = ∥m∥2
2 + ∥n∥2
2 . But we also have
∥x∥2
2 = ∥Px∥2
2 = ∥P(m + n)∥2
2 = ∥Pm∥2
2 = ∥m∥2
2 .
Therefore, n = 0, and thus x = m ∈R (P).
5.13.4. (AT PR(A))T = PT
R(A)A = PR(A)A = A.
5.13.5. Equation (5.13.4) says that PM = UUT = 
r
i=1 uiuiT , where U contains the
ui ’s as columns.
5.13.6. The Householder (or Givens) reduction technique can be employed as described
in Example 5.11.2 on p. 407 to compute orthogonal matrices U =

U1 | U2
	
and V =

V1 | V2
	
, which are factors in a URV factorization of A. Equation
(5.13.12) insures that
PR(A) = U1UT
1 ,
PN(AT ) = PR(A)⊥= I −U1UT
1 = U2UT
2 ,
PR(AT ) = V1VT
1 ,
PN(A) = PR(AT )⊥= I −V1VT
1 = V2VT
2 .
5.13.7. (a)
The only nonsingular orthogonal projector (i.e., the only nonsingular sym-
metric idempotent matrix) is the identity matrix. Consequently, for all other or-
thogonal projectors P, we must have rank (P) = 0 or rank (P) = 1, so P = 0
or, by Example 5.13.1, P = (uuT )/uT u. In other words, the 2 × 2 orthogonal
projectors are P = I, P = 0, and, for a nonzero vector uT = ( α
β ) ,
P = uuT
uT u =
1
α2 + β2

α2
αβ
αβ
β2

.
(b)
P = I, P = 0, and, for nonzero vectors u, v ∈ℜ2×1, P = (uvT )/uT v.
5.13.8. If either u or v is the zero vector, then L is a one-dimensional subspace, and
the solution is given in Example 5.13.1. Suppose that neither u nor v is the
zero vector, and let p be the orthogonal projection of b onto L. Since L is the
translate of the subspace span {u −v} , subtracting u from everything moves
the situation back to the origin—the following picture illustrates this in ℜ2.

Solutions
105
b
p
u
v
L
L −u
b −u
u −v
p −u
In other words, L is translated back down to span {u −v} , b →b −u, and
p →p −u, so that p −u must be the orthogonal projection of b −u onto
span {u −v} . Example 5.13.1 says that
p −u = Pspan{u−v}(b −u) = (u −v)(u −v)T
(u −v)T (u −v)(b −u),
and thus
p = u +
(u −v)T (b −u)
(u −v)T (u −v)

(u −v).
5.13.9. ∥A3x −b∥2 =
,,PR(A)b −b
,,
2 =
,,(I −PR(A))b
,,
2 =
,,PN(AT )b
,,
2
5.13.10. Use (5.13.17) with PR(A) = PT
R(A) = P2
R(A), to write
∥ε∥2
2 = (b −PR(A)b)T (b −PR(A)b)
= bT b −bT PT
R(A)b −bT PR(A)b + bT PT
R(A)PR(A)b
= bT b −bT PR(A)b = ∥b∥2
2 −
,,PR(A)b
,,2
2 .
5.13.11. According to (5.13.13) we must show that 
r
i=1(uiT x)ui = PMx. It follows
from (5.13.4) that if Un×r is the matrix containing the vectors in B as columns,
then
PM = UUT =
r

i=1
uiui
T
=⇒
PMx =
r

i=1
uiui
T x =
r

i=1
(ui
T x)ui.
5.13.12. Yes, the given spanning set {u1, u2, u3} is an orthonormal basis for M, so, by
Exercise 5.13.11,
PMb =
3

i=1
(ui
T b)ui = u1 + 3u2 + 7u3 =



5
0
5
3


.

106
Solutions
5.13.13. (a)
Combine the fact that PMPN = 0 if and only if R (PN ) ⊆N (PM) with
the facts R (PN ) = N and N (PM) = M⊥to write
PMPN = 0 ⇐⇒N ⊆M⊥⇐⇒N ⊥M.
(b)
Yes—this is a direct consequence of part (a). Alternately, you could say
0 = PMPN ⇐⇒0 = (PMPN )T = PT
N PT
M = PN PM.
5.13.14. (a)
Use Exercise 4.2.9 along with (4.5.5) to write
R (PM) + R (PN ) = R (PM | PN ) = R

(PM | PN )
 PM
PN
T 

= R

PMPM
T + PN PN
T 
= R

P2
M + P2
N
	
= R (PM + PN ).
(b)
PMPN = 0 ⇐⇒R (PN ) ⊆N (PM) ⇐⇒N ⊆M⊥⇐⇒M ⊥N.
(c)
Exercise 5.9.17 says PM + PN is idempotent if and only if PMPN = 0 =
PN PM. Because PM and PN are symmetric, PMPN = 0 if and only if
PMPN = PN PM = 0 (via the reverse order law for transposition). The fact
that R (PM + PN ) = R (PM) ⊕R (PN ) = M ⊕N was established in Exercise
5.9.17, and M ⊥N follows from part (b).
5.13.15. First notice that PM +PN is symmetric, so (5.13.12) and the result of Exercise
5.13.14, part (a), can be combined to conclude that
(PM + PN )(PM + PN )† = (PM + PN )†(PM + PN ) = PR(PM+PN ) = PM+N .
Now, M ⊆M + N implies PM+N PM = PM, and the reverse order law for
transposition yields PMPM+N = PM so that PM+N PM = PMPM+N . In
other words, (PM + PN )(PM + PN )†PM = PM(PM + PN )†(PM + PN ), or
PM(PM + PN )†PM + PN (PM + PN )†PM
= PM(PM + PN )†PM + PM(PM + PN )†PN .
Subtracting PM(PM + PN )†PM from both sides of this equation produces
PM(PM + PN )†PN = PN (PM + PN )†PM.
Let Z = 2PM(PM+PN )†PN = 2PN (PM+PN )†PM, and notice that R (Z) ⊆
R (PM) = M and R (Z) ⊆R (PN ) = N implies R (Z) ⊆M∩N. Furthermore,
PMPM∩N = PM∩N = PN PM∩N , and PM+N PM∩N = PM∩N , so, by the

Solutions
107
reverse order law for transposition, PM∩N PM = PM∩N = PM∩N PN
and
PM∩N PM+N = PM∩N . Consequently,
Z = PM∩N Z = PM∩N

PM(PM + PN )†PN + PN (PM + PN )†PM

= PM∩N (PM + PN )†(PM + PN ) = PM∩N PM+N = PM∩N .
5.13.16. (a)
Use the fact that AT = AT PR(A) = AT AA† (see Exercise 5.13.4) to write
4 ∞
0
e−AT AtAT dt =
4 ∞
0
e−AT AtAT AA†dt =
4 ∞
0
e−AT AtAT Adt

A†
=
%
−e−AT At&∞
0 A† = [0 −(−I)]A† = A†.
(b)
Recall from Example 5.10.5 that Ak = Ak+1AD = AkAAD, and write
4 ∞
0
e−Ak+1tAkdt =
4 ∞
0
e−Ak+1tAkAADdt =
4 ∞
0
e−Ak+1tAk+1Adt

AD
=
%
−e−Ak+1t&∞
0 AD = [0 −(−I)]AD = AD.
(c)
This is just a special case of the formula in part (b) with k = 0. However,
it is easy to derive the formula directly by writing
4 ∞
0
e−Atdt =
4 ∞
0
e−AtAA−1dt =
4 ∞
0
e−AtAdt

A−1
=
%
e−At&∞
0 A−1 = [0 −(−I)]A−1 = A−1.
5.13.17. (a)
The points in H are just solutions to a linear system uT x = β. Using the
fact that the general solution of any linear system is a particular solution plus
the general solution of the associated homogeneous equation produces
H = βu
uT u + N(uT ) = βu
uT u + [R(u)]⊥= βu
uT u + u⊥,
where u⊥denotes the orthogonal complement of the one-dimensional space
spanned by the vector u. Thus H = v+M, where v = βu/uT u and M = u⊥.
The fact that dim (u⊥) = n −1 follows directly from (5.11.3).
(b)
Use (5.13.14) with part (a) and the fact that Pu⊥= I −uuT /uT u to write
p = βu
uT u +

I −uuT
uT u
 
b −βu
uT u

= βu
uT u + b −uuT b
uT u = b −
uT b −β
uT u

u.

108
Solutions
5.13.18. (a)
uT w ̸= 0 implies M ∩W = 0 so that
dim (M + W) = dim M + dim W = (n −1) + 1 = n.
Therefore, M+W = ℜn. This together with M∩W = 0 means ℜn = M⊕W.
(b)
Write
b = b −uT b
uT ww + uT b
uT ww = p + uT b
uT ww,
and observe that p ∈M (because uT p = 0 ) and (uT b/uT w)w ∈W. By
deﬁnition, p is the projection of b onto M along W.
(c)
We know from Exercise 5.13.17, part (a), that H = v + M, where v =
βu/uT u and M = u⊥, so subtracting v = βu/uT u from everything in H
as well as from b translates the situation back to the origin. Sketch a picture
similar to that of Figure 5.13.5 to see that this moves H back to M, it translates
b to b−v, and it translates p to p−v. Now, p−v should be the projection
of b −v onto M along W, so by the result of part (b),
p−v = b−v−uT (b −v)
uT w
w
=⇒
p = b−uT (b −v)
uT w
w = b−
uT b −β
uT w

w.
5.13.19. For convenience, set β = Ai∗pkn+i−1 −bi so that pkn+i = pkn+i−1 −β(Ai∗)T .
Use the fact that
Ai∗(pkn+i−1 −x) = Ai∗pkn+i−1 −bi = β
together with ∥Ai∗∥2 = 1 to write
∥pkn+i −x∥2
2 =
,,,pkn+i−1 −β(Ai∗)T −x
,,,
2
2
=
,,,(pkn+i−1 −x) −β(Ai∗)T ,,,
2
2
= (pkn+i−1 −x)T (pkn+i−1 −x)
−2βAi∗(pkn+i−1 −x) + β2Ai∗(Ai∗)T
= ∥pkn+i−1 −x∥2
2 −β2.
Consequently, ∥pkn+i −x∥2 ≤∥pkn+i−1 −x∥2 , with equality holding if and
only if β = 0 or, equivalently, if and only if pkn+i−1 ∈Hi−1 ∩Hi. Therefore,
the sequence of norms ∥pkn+i −x∥2 is monotonically decreasing, and hence it
must have a limiting value. This implies that the sequence of the β ’s deﬁned
above must approach 0, and thus the sequence of the pkn+i ’s converges to x.
5.13.20. Refer to Figure 5.13.8, and notice that the line passing from p(1)
1
to p(1)
2
is
parallel to V = span

p(1)
1
−p(1)
2

, so projecting p(1)
1
through p(1)
2
onto H2

Solutions
109
is exactly the same as projecting p(1)
1
onto H2 along (i.e., parallel to) V.
According to part (c) of Exercise 5.13.18, this projection is given by
p(2)
2
= p(1)
1
−
A2∗

p(1)
1
−b1AT
2∗

A2∗

p(1)
1
−p(1)
2


p(1)
1
−p(1)
2

= p(1)
1
−

A2∗p(1)
1
−b1

A2∗

p(1)
1
−p(1)
2


p(1)
1
−p(1)
2

.
All other projections are similarly derived. It is now straightforward to verify
that the points created by the algorithm are exactly the same points described
in Steps 1, 2, . . . , n −1.
Note:
The condition that
'
p(1)
1
−p(1)
2

,

p(1)
1
−p(1)
3

, . . . ,

p(1)
1
−p(1)
n
(
is independent insures that
'
p(2)
2
−p(2)
3

,

p(2)
2
−p(2)
4

, . . . ,

p(2)
2
−p(2)
n
(
is also independent. The same holds at each subsequent step. Furthermore,
A2∗

p(1)
1
−p(1)
k

̸= 0 for k > 1 implies that Vk = span

p(1)
1
−p(1)
k

is
not parallel to H2, so all projections onto H2 along Vk are well deﬁned. It can
be argued that the analogous situation holds at each step of the process—i.e.,
the initial conditions insure Ai+1∗

p(i)
i
−p(i)
k

̸= 0 for k > i.
5.13.21. Equation (5.13.13) says that the orthogonal distance between x and M⊥is
dist (x, M⊥) = ∥x −PM⊥x∥2 = ∥(I −PM⊥)x∥2 = ∥PMx∥2 .
Similarly,
dist (Rx, M⊥) = ∥PMRx∥2 = ∥−PMx∥2 = ∥PMx∥2 .
5.13.22. (a)
We know from Exercise 5.13.17 that H = v + u⊥, where v = βu, so
subtracting v from everything in H as well as from b translates the situation
back to the origin. As depicted in the diagram below, this moves H down to
u⊥, and it translates b to b −v and r to r −v.

110
Solutions
v
u
u⊥
0
b
p
b - v
p - v
H
Now, we know from (5.6.8) that the reﬂection of b −v about u⊥is
r −v = R(b −v) = (I −2uuT )(b −v) = b + (β −2uT b)u,
and therefore the reﬂection of b about H is
r = R(b −v) + v = b −2(uT b −β)u.
(b)
From part (a), the reﬂection of r0 about Hi is
ri = r0 −2(Ai∗r0 −bi) (Ai∗)T ,
and therefore the mean value of all of the reﬂections {r1, r2, . . . , rn} is
m = 1
n
n

i=1
ri = 1
n
n

i=1

r0 −2(Ai∗r0 −bi) (Ai∗)T 
= r0 −2
n
n

i=1
(Ai∗r0 −bi)(Ai∗)T
= r0 −2
nAT (Ar0 −b) = r0 −2
nAT ε.
Note: If weights wi > 0 such that 
 wi = 1 are used, then the weighted mean
is
m =
n

i=1
wiri =
n

i=1
wi

r0 −2(Ai∗r0 −bi) (Ai∗)T 
= r0 −2
n

i=1
wi(Ai∗r0 −bi)(Ai∗)T
= r0 −2
nAT W (Ar0 −b) = r0 −2
nAT Wε,

Solutions
111
where W = diag {w1, w2, . . . , wn} .
(c)
First observe that
x −mk = x −mk−1 + 2
nAT εk−1
= x −mk−1 + 2
nAT (Amk−1 −b)
= x −mk−1 + 2
nAT (Amk−1 −Ax)
= x −mk−1 + 2
nAT A(mk−1 −x)
=

I −2
nAT A

(x −mk−1),
and then use successive substitution to conclude that
x −mk =

I −2
nAT A
k
(x −m0).
Solutions for exercises in section 5. 14
5.14.1. Use (5.14.5) to observe that
E[yiyj] = Cov[yi, yj] + µyiµyj =

σ2 + (Xi∗β)2
if i = j,
(Xi∗β)(Xj∗β)
if i ̸= j,
so that
E[yyT ] = σ2I + (Xβ)(Xβ)T = σ2I + XββT XT .
Write ˆe = y −Xˆβ = (I−XX†)y, and use the fact that I−XX† is idempotent
to obtain
ˆeT ˆe = yT (I −XX†)y = trace

(I −XX†)yyT 	
.
Now use the linearity of trace and expectation together with the result of Exercise
5.9.13 and the fact that (I −XX†)X = 0 to write
E[ˆeT ˆe] = E

trace

(I −XX†)yyT 	
= trace

E[(I −XX†)yyT ]
	
= trace

(I −XX†)E[yyT ]
	
= trace

(I −XX†)(σ2I + XββT XT )

= σ2trace

I −XX†	
= σ2 
m −trace

XX†		
= σ2 
m −rank

XX†		
= σ2(m −n).

112
Solutions
Solutions for exercises in section 5. 15
5.15.1. (a)
θmin = 0, and θmax = θ = φ = π/4.
(b)
θmin = θ = φ = π/4, and θmax = 1.
5.15.2. (a)
The ﬁrst principal angle is θ1 = θmin = 0, and we can take u1 = v1 = e1.
This means that
M2 = u⊥
1 ∩M = span {e2}
and
N2 = v⊥
1 ∩N = span {(0, 1, 1)} .
The second principal angle is the minimal angle between M2 and N2, and this
is just the angle between e2 and (0, 1, 1), so θ2 = π/4.
(b)
This time the ﬁrst principal angle is θ1 = θmin = π/4, and we can take
u1 = e1 and v1 = (0, 1/
√
2, 1/
√
2). There are no more principal angles because
N2 = v⊥
1 ∩N = 0.
5.15.3. (a)
This follows from (5.15.16) because PM = PN if and only if M = N.
(b)
If 0 ̸= x ∈M ∩N, then (5.15.1) evaluates to 1 with the maximum being
attained at u = v = x/ ∥x∥2 . Conversely, cos θmin = 1 =⇒vT u = 1 for some
u ∈M and v ∈N such that ∥u∥2 = 1 = ∥v∥2 . But vT u = 1 = ∥u∥2 ∥v∥2
represents equality in the CBS inequality (5.1.3), and we know this occurs if and
only if v = αu for α = vT u/u∗u = 1/1 = 1. Thus u = v ∈M ∩N.
(c)
max
u∈M, v∈N
∥u∥2=∥v∥2=1
vT u = 0 ⇐⇒vT u = 0 ∀u ∈M, v ∈V ⇐⇒M ⊥N.
5.15.4. You can use either (5.15.3) or (5.15.4) to arrive at the result. The latter is used
by observing
,,(PM⊥−PN ⊥)−1,,
2 =
,,,

(I −PM) −(I −PN )
	−1,,,
2
=
,,(PN −PM)−1,,
2 =
,,(PM −PN )−1,,
2 .
5.15.5. M ⊕N ⊥= ℜn
=⇒
dim M = dim N
=⇒
sin θmax = δ(M, N) = δ(N, M),
so cos ˜θmin = ∥PMPN ⊥∥2 = ∥PM(I −PN )∥2 = δ(M, N) = sin θmax.
5.15.6. It was argued in the proof of (5.15.4) that PM −PN is nonsingular whenever
M and N are complementary, so we need only prove the converse. Suppose
dim M = r > 0 and dim N = k > 0 (the problem is trivial if r = 0 or
k = 0 ) so that UT
1 V1 is r × n −k and UT
2 V2 is n −r × k. If PM −PN is
nonsingular, then (5.15.7) insures that the rows as well as the columns in each of
these products must be linearly independent. That is, UT
1 V1 and UT
2 V2 must
both be square and nonsingular, so r + k = n. Combine this with the formula
for the rank of a product (4.5.1) to conclude
k = rank

UT
2 V2
	
= rank

UT
2
	
−dim N

UT
2
	
∩R (V2)
= n −r −dim M ∩N = k −dim M ∩N.
It follows that M ∩N = 0, and hence M ⊕N = ℜn.

Solutions
113
5.15.7. (a)
This can be derived from (5.15.7), or it can be veriﬁed by direct multipli-
cation by using PN (I −P) = I −P
=⇒
P −PN P = I −PN to write
(PM −PN )(P −Q) = PMP −PMQ −PN P + PN Q
= P −0 −PN P + PN Q = I −PN + PN Q
= I −PN (I −Q) = I.
(b) and (c) follow from (a) in conjunction with (5.15.3) and (5.15.4).
5.15.8. Since we are maximizing over a larger set, max∥x∥=1 f(x) ≤max∥x∥≤1 f(x). A
strict inequality here implies the existence of a nonzero vector x0 such that
∥x0∥< 1 and f(x) < f(x0) for all vectors such that ∥x∥= 1. But then
f(x0) > f(x0/ ∥x0∥) = f(x0)/ ∥x0∥
=⇒
∥x0∥f(x0) > f(x0),
which is impossible because ∥x0∥< 1.
5.15.9. (a)
We know from equation (5.15.6) that PMN = U

C
0
0
0

VT in which
C is nonsingular and C−1 = VT
1 U1. Consequently,
P†
MN = V

C−1
0
0
0

UT = V1C−1UT
1 = V1VT
1 U1UT
1 = PN ⊥PM.
(b)
Use the fact
,,(UT
1 V1)−1,,
2 =
,,(VT
1 U1)−1,,
2 =
,,U1(VT
1 U1)−1VT
1
,,
2
=
,,(V1VT
1 U1UT
1 )†,,
2 =
,,,

(I −PN )PM
†,,,
2
(and similarly for the other term) to show that
,,,

(I −PN )PM
†,,,
2 =
,,(UT
1 V1)−1,,
2 =
,,,

PM(I −PN )
†,,,
2 ,
and
,,,

(I −PM)PN
†,,,
2 =
,,(UT
2 V2)−1,,
2 =
,,,

PN (I −PM)
†,,,
2 .
It was established in the proof of (5.15.4) that
,,(UT
1 V1)−1,,
2 =
,,(UT
2 V2)−1,,
2 ,
so combining this with the result of part (a) and (5.15.3) produces the desired
conclusion.
5.15.10. (a)
We know from (5.15.2) that cos ¯θmin = ∥PN ⊥PM∥2 = ∥(I −PN )PM∥2 ,
and we know from Exercise 5.15.9 that PMN =

(I −PN )PM
†, so taking the
pseudoinverse of both sides of this yields the desired result.

114
Solutions
(b)
Use (5.15.3) together with part (a), (5.13.10), and (5.13.12) to write
1 =
,,,PMN P†
MN
,,,
2 ≤
,,,PMN
,,,
2
,,,P†
MN
,,,
2 = cos ¯θmin
sin θmin
.
5.15.11. (a)
Use the facts that ∥A∥2 = ∥AT ∥2 and (AT )−1 = (A−1)T to write
1
,,(UT
2 V2)−1,,2
2
=
1
,,(VT
2 U2)−1,,2
2
= min
∥x∥2=1
,,VT
2 U2x
,,2
2
= min
∥x∥2=1 xT UT
2 V2VT
2 U2x
= min
∥x∥2=1 xT UT
2 (I −V1VT
1 )U2x = min
∥x∥2=1

1 −
,,VT
1 U2x
,,2
2

= 1 −max
∥x∥2=1
,,VT
1 U2x
,,2
2 = 1 −
,,VT
1 U2
,,2
2 = 1 −
,,UT
2 V1
,,2
2 .
(b)
Use a similar technique to write
,,UT
2 V2
,,2
2 =
,,UT
2 V2VT
2
,,2
2 =
,,UT
2 (I −V1VT
1 )
,,2
2
=
,,(I −V1VT
1 )U2
,,2
2 = max
∥x∥2=1 xT UT
2 (I −V1VT
1 )U2x
= 1 −min
∥x∥2=1
,,VT
1 U2x
,,2
2 = 1 −
1
,,(VT
1 U2)−1,,2
2
= 1 −
1
,,(UT
2 V1)−1,,2
2
.

Solutions for Chapter 6
Solutions for exercises in section 6. 1
6.1.1. (a)
−1
(b)
8
(c)
−αβγ
(d)
a11a22a33 + a12a23a31 + a13a21a32 −(a11a23a32 + a12a21a33 + a13a22a31)
(This is where the “diagonal rule” you learned in high school comes from.)
6.1.2. If A = [x1 | x2 | x3], then V3 =

det

AT A
	1/2 = 20 (recall Example 6.1.4).
But you could also realize that the xi ’s are mutually orthogonal to conclude
that V3 = ∥x1∥2 ∥x2∥2 ∥x3∥2 = 20.
6.1.3. (a)
10
(b)
0
(c)
120
(d)
39
(e)
1
(f)
(n −1)!
6.1.4. rank (A) = 2
6.1.5. A square system has a unique solution if and only if its coeﬃcient matrix is
nonsingular—recall the discussion in §2.5. Consequently, (6.1.13) guarantees that
a square system has a unique solution if and only if the determinant of the
coeﬃcient matrix is nonzero. Since

1
α
0
0
1
−1
α
0
1

= 1 −α2,
it follows that there is a unique solution if and only if α ̸= ±1.
6.1.6. I = A−1A
=⇒
det (I) = det

A−1A
	
= det

A−1	
det (A)
=⇒
1 = det

A−1	
det (A)
=⇒
det

A−1	
= 1/det (A).
6.1.7. Use the product rule (6.1.15) to write
det

P−1AP
	
= det

P−1	
det (A)det (P) = det

P−1	
det (P)det (A)
= det

P−1P
	
det (A) = det (I)det (A) = det (A).
6.1.8. Use (6.1.4) together with the fact that z1z2 = ¯z1¯z2 and z1 + z2 = ¯z1 + ¯z2 for
all complex numbers to write
det (A∗) = det
¯AT 	
= det
¯A
	
=

p
σ(p)a1p1 · · · anpn
=

p
σ(p)a1p1 · · · anpn =

p
σ(p)a1p1 · · · anpn = det (A).
6.1.9. (a)
I = Q∗Q
=⇒
1 = det (Q∗Q) = det (Q∗)det (Q) = [det (Q)]2 by
Exercise 6.1.8.

116
Solutions
(b)
If A = UDV∗is an SVD, then, by part (a),
|det (A)| = |det (UDV∗)| = |det (U)| |det (D)| |det (V∗)|
= det (D) = σ1σ2 · · · σn.
6.1.10. Let r = rank (A), and let σ1 ≥· · · ≥σr be the nonzero singular values of A.
If A = Um×m

Dr×r
0
0
0

m×n
(V∗)n×n is an SVD, then, by Exercises 6.1.9 and
6.1.8, det (V)det (V∗) = |det (V)|2 = 1, so
det (A∗A) = det (VD∗DV∗) = det (V)

(D∗D)r×r
0
0
0

n×n
det (V∗)
= σ2
1σ2
2 · · · σ2
r 0 · · · 0
5 67 8,
n−r
and this is
' = 0
when r < n,
> 0
when r = n.
Note: You can’t say det (A∗A) = det (A)det (A) = |det (A)|2 ≥0 because A
need not be square.
6.1.11. αA = (αI)A
=⇒
det (αA) = det (αI)det (A) = αndet (A).
6.1.12. A = −AT
=⇒
det (A) = det

−AT 	
= det (−A) = (−1)ndet (A) (by
Exercise 6.1.11)
=⇒
det (A) = −det (A) when n is odd
=⇒
det (A) = 0.
6.1.13. If A = LU, where L is lower triangular and U is upper triangular where each
has 1’s on its diagonal and random integers in the remaining nonzero positions,
then det (A) = det (L)det (U) = 1 × 1 = 1, and the entries of A are rather
random integers.
6.1.14. According to the deﬁnition,
det (A) =

p
σ(p)a1p1 · · · akpk · · · anpn
=

p
σ(p)a1p1 · · · (xpk + ypk + · · · + zpk) · · · anpn
=

p
σ(p)a1p1 · · · xpk · · · anpn +

p
σ(p)a1p1 · · · ypk · · · anpn
+ · · · +

p
σ(p)a1p1 · · · zpk · · · anpn
= det







A1∗
...
xT
...
An∗







+ det







A1∗
...
yT
...
An∗







+ · · · + det







A1∗
...
zT
...
An∗







.

Solutions
117
6.1.15. If An×2 = [x | y] , then the result of Exercise 6.1.10 implies
0 ≤det (A∗A) =

x∗x
x∗y
y∗x
y∗y
 = (x∗x) (y∗y) −(x∗y) (y∗x)
= ∥x∥2
2 ∥y∥2
2 −(x∗y) (x∗y)
= ∥x∥2
2 ∥y∥2
2 −|x∗y|2,
with equality holding if and only if rank (A) < 2 —i.e., if and only if y is a
scalar multiple of x.
6.1.16. Partition A as
A = LU =

Lk
0
L21
L22
 
Uk
U12
0
U22

=

LkUk
∗
∗
∗

to deduce that Ak can be written in the form
Ak = LkUk =

Lk−1
0
dT
1
 
Uk−1
c
0
ukk

and
Ak−1 = Lk−1Uk−1.
The product rule (6.1.15) shows that
det (Ak) = det (Uk−1) × ukk = det (Ak−1) × ukk,
and the desired conclusion follows.
6.1.17. According to (3.10.12), a matrix has an LU factorization if and only if each
leading principal submatrix is nonsingular. The leading k × k principal subma-
trix of AT A is given by Pk = AT
k Ak, where Ak = [A∗1 | A∗2 | · · · | A∗k] . If
A has full column rank, then any nonempty subset of columns is linearly in-
dependent, so rank (Ak) = k. Therefore, the results of Exercise 6.1.10 insure
that det (Pk) = det

AT
k Ak
	
> 0 for each k, and hence AT A has an LU
factorization. The fact that each pivot is positive follows from Exercise 6.1.16.
6.1.18. (a)
To evaluate det (A), use Gaussian elimination as shown below.


2 −x
3
4
0
4 −x
−5
1
−1
3 −x

−→


1
−1
3 −x
0
4 −x
−5
2 −x
3
4


−→


1
−1
3 −x
0
4 −x
−5
0
5 −x
−x2 + 5x −2

−→


1
−1
3 −x
0
4 −x
−5
0
0
x3−9x2+17x+17
4−x

= U.
Since one interchange was used, det (A) is (−1) times the product of the diag-
onal entries of U, so
det (A) = −x3 + 9x2 −17x −17
and
d

det (A)

dx
= −3x2 + 18x −17.

118
Solutions
(b)
Using formula (6.1.19) produces
d

det (A)

dx
=

−1
0
0
0
4 −x
−5
1
−1
3 −x

+

2 −x
3
4
0
−1
0
1
−1
3 −x

+

2 −x
3
4
0
4 −x
−5
0
0
−1

= (−x2 + 7x −7) + (−x2 + 5x −2) + (−x2 + 6x −8)
= −3x2 + 18x −17.
6.1.19. No—almost any 2 × 2 example will show that this cannot hold in general.
6.1.20. It was argued in Example 4.3.6 that if there is at least one value of x for which
the Wronski matrix
W(x) =






f1(x)
f2(x)
· · ·
fn(x)
f ′
1(x)
f ′
2(x)
· · ·
f ′
n(x)
...
...
...
...
f (n−1)
1
(x)
f (n−1)
2
(x)
· · ·
f (n−1)
n
(x)






is nonsingular, then S is a linearly independent set. This is equivalent to saying
that if S is a linearly dependent set, then the Wronski matrix W(x) is singular
for all values of x. But (6.1.14) insures that a matrix is singular if and only
if its determinant is zero, so, if S is linearly dependent, then the Wronskian
w(x) must vanish for every value of x. The converse of this statement is false
(Exercise 4.3.14).
6.1.21. (a)
(n!)(n−1)
(b)
11 × 11
(c)
About 9.24×10153 sec ≈3×10146
years
(d)
About 3 × 10150 mult/sec. (Now this would truly be a “super
computer.”)
Solutions for exercises in section 6. 2
6.2.1. (a)
8
(b)
39
(c)
−3
6.2.2. (a)
A−1 = adj (A)
det (A) = 1
8


0
1
−1
−8
4
4
16
−6
−2


(b)
A−1 = adj (A)
det (A) = 1
39



−12
25
−14
7
−9
9
9
15
−6
6
6
−3
9
4
4
−2



6.2.3. (a)
x1 = 1 −β,
x2 = α + β −1,
x3 = 1 −α

Solutions
119
(b)
Cramer’s rule yields
x2(t) =

1
t4
t2
t2
t3
t
t
0
1


1
t
t2
t2
1
t
t
t2
1

=
t

t4
t2
t3
t
 +

1
t4
t2
t3


1
t
t2
1
 −t

t2
t
t
1
 + t2

t2
1
t
t2

=
t3 −t6
(t3 −1)(t3 −1) =
−t3
(t3 −1),
and hence
lim
t→∞x2(t) = lim
t→∞
−1
1 −1/t3 = −1.
6.2.4. Yes.
6.2.5. (a)
Almost any two matrices will do the job. One example is A = I and
B = −I.
(b)
Again, almost anything you write down will serve the purpose. One example
is A = D = 02×2, B = C = I2×2.
6.2.6. Recall from Example 5.13.3 that Q = I −BBT B−1BT . According to (6.2.1),
det

AT A
	
= det

BT B
BT c
cT B
cT c

= det

BT B
	 
cT Qc
	
.
Since det

BT B
	
> 0 (by Exercise 6.1.10), cT Qc = det

AT A
	
/det

BT B
	
.
6.2.7. Expand

A
−C
DT
Ik
 both of the ways indicated in (6.2.1).
6.2.8. The result follows from Example 6.2.8, which says A[adj (A)] = det (A) I, to-
gether with the fact that A is singular if and only if det (A) = 0.
6.2.9. The solution is x = A−1b, and Example 6.2.7 says that the entries in A−1 are
continuous functions of the entries in A. Since xi = 
k[A−1]ikbk, and since
the sum of continuous functions is again continuous, it follows that each xi is a
continuous function of the aij ’s.
6.2.10. If B = αA, then Exercise 6.1.11 implies B˚ij = αn−1˚Aij, so B
˚ = αn−1˚
A, and
hence adj (B) = αn−1adj (A) .
6.2.11. (a)
We saw in §6.1 that rank (A) is the order of the largest nonzero minor of
A. If rank (A) < n −1, then every minor of order n −1 (as well as det (A)
itself) must be zero. Consequently, ˚
A = 0, and thus adj (A) = ˚
A
T = 0.
(b)
rank (A) = n −1
=⇒
at least one minor of order n −1 is nonzero
=⇒
some ˚Aij ̸= 0
=⇒
adj (A) ̸= 0
=⇒
rank (adj (A)) ≥1.

120
Solutions
Also,
rank (A) = n −1
=⇒
det (A) = 0
=⇒
A[adj (A)] = 0
(by Exercise 6.2.8)
=⇒
R (adj (A)) ⊆N (A)
=⇒
dim R (adj (A)) ≤dim N (A)
=⇒
rank (adj (A)) ≤n −rank (A) = 1.
(c)
rank (A) = n
=⇒
det (A) ̸= 0
=⇒
adj (A) = det (A) A−1
=⇒
rank (adj (A)) = n
6.2.12. If det (A) = 0, then Exercise 6.2.11 insures that rank (adj (A)) ≤1. Conse-
quently, det (adj (A)) = 0, and the result is trivially true because both sides
are zero. If det (A) ̸= 0, apply the product rule (6.1.15) to A[adj (A)] =
det (A) I (from Example 6.2.8) to obtain det (A)det (adj (A)) = [det (A)]n ,
so that det (adj (A)) = [det (A)]n−1 .
6.2.13. Expanding in terms of cofactors of the ﬁrst row produces Dn = 2˚A11 −˚A12. But
˚A11 = Dn−1 and expansion using the ﬁrst column yields
˚A12 = (−1)

−1
−1
0
· · ·
0
0
2
−1
· · ·
0
0
−1
2
· · ·
0
...
...
...
...
...
0
0
0
· · ·
2

= (−1)(−1)Dn−2,
so Dn = 2Dn−1 −Dn−2. By recursion (or by direct substitution), it is easy to
see that the solution of this equation is Dn = n + 1.
6.2.14. (a)
Use the results of Example 6.2.1 with λi = 1/αi.
(b)
Recognize that the matrix A is a rank-one updated matrix in the sense
that
A = (α −β)I + βeeT ,
where
e =


1
...
1

.
If α = β, then A is singular, so det (A) = 0. If α ̸= β, then (6.2.3) may be
applied to obtain
det (A) = det

(α −β)I
 
1 + βeT e
α −β

= (α −β)n

1 +
nβ
α −β

.
(c)
Recognize that the matrix is I + edT , where
e =




1
1
...
1




and
d =




α1
α2
...
αn



.

Solutions
121
Apply (6.2.2) to produce the desired formula.
6.2.15. (a)
Use the second formula in (6.2.1).
(b)
Apply the ﬁrst formula in (6.2.1) along with (6.2.7).
6.2.16. If λ = 0, then the result is trivially true because both sides are zero. If λ ̸= 0,
then expand

λIm
λB
C
λIn
 both of the ways indicated in (6.2.1).
6.2.17. (a)
Use the product rule (6.1.15) together with (6.2.2) to write
A + cdT = A + AxdT = A

I + xdT 	
.
(b)
Apply the same technique used in part (a) to obtain
A + cdT = A + cyT A =

I + cyT 	
A.
6.2.18. For an elementary reﬂector R = I −2uuT /uT u, (6.2.2) insures det (R) = −1.
If An×n is reduced to upper-triangular form (say PA = T ) by Householder
reduction as explained on p. 341, then det (P)det (A) = det (T) = t11 · · · tnn.
Since P is the product of elementary reﬂectors, det (A) = (−1)kt11 · · · tnn,
where k is the number of reﬂections used in the reduction process. In general,
one reﬂection is required to annihilate entries below a diagonal position, so, if
no reduction steps can be skipped, then det (A) = (−1)n−1t11 · · · tnn. If Pij is
a plane rotation, then there is a permutation matrix (a product of interchange
matrices) B such that Pij = BT

Q
0
0
I

B, where Q =

c
s
−s
c

with
c2 + s2 = 1. Consequently, det (Pij) = det

BT 	 
Q
0
0
I
 det (B) = det (Q) = 1
because det (B)det

BT 	
= det (B)2 = 1 by (6.1.9). Since Givens reduction
produces PA = T, where P is a product of plane rotations and T is upper
triangular, the product rule (6.1.15) insures det (P) = 1, so det (A) = det (T) =
t11 · · · tnn.
6.2.19. If det (A) = ±1, then (6.2.7) implies A−1 = ±adj (A) , and thus A−1 is
an integer matrix because the cofactors are integers. Conversely, if A−1 is an
integer matrix, then det

A−1	
and det (A) are both integers. Since
AA−1 = I
=⇒
det (A)det

A−1	
= 1,
it follows that det (A) = ±1.
6.2.20. (a)
Exercise 6.2.19 guarantees that A−1 has integer entries if and only if
det (A) = ±1, and (6.2.2) says that det (A) = 1 −2vT u, so A−1 has inte-
ger entries if and only if vT u is either 0 or 1.
(b)
According to (3.9.1),
A−1 =

I −2uvT 	−1 = I −
2uvT
2vT u −1,

122
Solutions
and thus A−1 = A when vT u = 1.
6.2.21. For n = 2, two multiplications are required, and c(2) = 2. Assume c(k) mul-
tiplications are required to evaluate any k × k determinant by cofactors. For a
k + 1 × k + 1 matrix, the cofactor expansion in terms of the ith row is
det (A) = ai1˚Ai1 + · · · + aik˚Aik + aik+1˚Aik+1.
Each ˚Aij requires c(k) multiplications, so the above expansion contains
(k + 1) + (k + 1)c(k) = (k + 1) + (k + 1)k!

1 + 1
2! + 1
3! + · · · +
1
(k −1)!

= (k + 1)!
 1
k! +

1 + 1
2! + 1
3! + · · · +
1
(k −1)!

= c(k + 1)
multiplications. Remember that ex = 1+x+x2/2!+x3/3!+· · · , so for n = 100,
1 + 1
2! + 1
3! + · · · + 1
99! ≈e −1,
and c(100) ≈100!(e−1). Consequently, approximately 1.6×10152 seconds (i.e.,
5.1 × 10144 years) are required.
6.2.22. A −λI is singular if and only if det (A −λI) = 0. The cofactor expansion in
terms of the ﬁrst row yields
det (A −λI) = −λ

5 −λ
2
−3
−λ
 + 3

2
2
−2
−λ
 −2

2
5 −λ
−2
−3

= −λ3 + 5λ2 −8λ + 4,
so A −λI is singular if and only if λ3 −5λ2 + 8λ −4 = 0. According to the
hint, the integer roots of p(λ) = λ3 −5λ2 +8λ−4 are a subset of {±4, ±2, ±1}.
Evaluating p(λ) at these points reveals that λ = 2 is a root, and either ordinary
or synthetic division produces
p(λ)
λ −2 = λ2 −3λ + 2 = (λ −2)(λ −1).
Therefore, p(λ) = (λ −2)2(λ −1), so λ = 2 and λ = 1 are the roots of p(λ),
and these are the values for which A −λI is singular.
6.2.23. The indicated substitutions produce the system






x′
1
x′
2
...
x′
n−1
x′
n






=






0
1
0
· · ·
0
0
0
1
· · ·
0
...
...
...
...
...
0
0
0
· · ·
1
−pn
−pn−1
−pn−2
· · ·
−p1












x1
x2
...
xn−1
xn






.

Solutions
123
Each of the n vectors wi =

fi(t)
f ′
i(t)
· · ·
f (n−1)
i
	T for i = 1, 2, . . . , n
satisﬁes this system, so (6.2.8) may be applied to produce the desired conclusion.
6.2.24. The result is clearly true for n = 2. Assume the formula holds for n = k −1,
and prove that it must also hold for n = k. According to the cofactor expansion
in terms of the ﬁrst row, deg p(λ) = k −1, and it’s clear that
p(x2) = p(x3) = · · · = p(xk) = 0,
so x2, x3, . . . , xk are the k −1 roots of p(λ). Consequently,
p(λ) = α(λ −x2)(λ −x3) · · · (λ −xk),
where α is the coeﬃcient of λk−1. But the coeﬃcient of λk−1 is the cofactor
associated with the (1, k) -entry, so the induction hypothesis yields
α = (−1)k−1

1
x2
x2
2
· · ·
xk−2
2
1
x3
x2
3
· · ·
xk−2
3
...
...
...
· · ·
...
1
xk
x2
k
· · ·
xk−2
k

k−1×k−1
= (−1)k−1 9
j>i≥2
(xj −xi).
Therefore,
det (Vk) = p(x1) = (x1 −x2)(x1 −x3) · · · (x1 −xk)α
= (x1 −x2)(x1 −x3) · · · (x1 −xk)

(−1)k−1 9
j>i≥2
(xj −xi)
	
= (x2 −x1)(x3 −x1) · · · (xk −x1)
9
j>i≥2
(xj −xi)
=
9
j>i
(xj −xi),
and the formula is proven. The determinant is nonzero if and only if the xi ’s
are distinct numbers, and this agrees with the conclusion in Example 4.3.4.
6.2.25. According to (6.1.19),
d

det (A)

dx
= det (D1) + det (D2) + · · · + det (Dn),
where Di is the matrix
Di =







a11
a12
· · ·
a1n
...
...
· · ·
...
a′
i1
a′
i2
· · ·
a′
in
...
...
· · ·
...
an1
an2
· · ·
ann







.

124
Solutions
Expanding det (Di) in terms of cofactors of the ith row yields
det (Ai) = a′
i1˚Ai1 + a′
i2˚Ai2 + · · · + a′
in˚Ain,
so the desired conclusion is obtained.
6.2.26. According to (6.1.19),
∂det (A)
∂aij
= det (Di) =

a11
· · ·
a1j
· · ·
a1n
...
· · ·
...
· · ·
...
0
· · ·
1
· · ·
0
...
· · ·
...
· · ·
...
an1
· · ·
anj
· · ·
ann

←row i = ˚Aij.
6.2.27. The
4
2
	
= 6 ways to choose pairs of column indices are
(1, 2)
(1, 3)
(1, 4)
(2, 3)
(2, 4)
(3, 4)
so that the Laplace expansion using i1 = 1 and i2 = 3 is
det (A) = det A(1, 3 | 1, 2) ˚A(1, 3 | 1, 2) + det A(1, 3 | 1, 3) ˚A(1, 3 | 1, 3)
+ det A(1, 3 | 1, 4) ˚A(1, 3 | 1, 4) + det A(1, 3 | 2, 3) ˚A(1, 3 | 2, 3)
+ det A(1, 3 | 2, 4) ˚A(1, 3 | 2, 4) + det A(1, 3 | 3, 4) ˚A(1, 3 | 3, 4)
= 0 + (−2)(−4) + (−1)(3)(−2) + 0 + (−3)(−3) + (−1)(−8)(2)
= 39.

Solutions for Chapter 7
Solutions for exercises in section 7. 1
7.1.1. σ (A) = {−3, 4}
N (A + 3I) = span

−1
1
+
and
N (A −4I) = span

−1/2
1
+
σ (B) = {−2, 2} in which the algebraic multiplicity of λ = −2 is two.
N (B + 2I) = span





−4
1
0

,


−2
0
1




and N (B −2I) = span





−1/2
−1/2
1





σ (C) = {3} in which the algebraic multiplicity of λ = 3 is three.
N (C −3I) = span





1
0
0





σ (D) = {3} in which the algebraic multiplicity of λ = 3 is three.
N (D −3I) = span





2
1
0

,


1
0
1





σ (E) = {3} in which the algebraic multiplicity of λ = 3 is three.
N (E −3I) = span





1
0
0

,


0
1
0

,


0
0
1





Matrices C and D are deﬁcient in eigenvectors.
7.1.2. Form the product Ax, and answer the question, “Is Ax some multiple of x ?”
When the answer is yes, then x is an eigenvector for A, and the multiplier
is the associated eigenvalue. For this matrix, (a), (c), and (d) are eigenvectors
associated with eigenvalues 1, 3, and 3, respectively.

126
Solutions
7.1.3. The characteristic polynomial for T is
det (T −λI) = (t11 −λ) (t22 −λ) · · · (tnn −λ) ,
so the roots are the tii ’s.
7.1.4. This follows directly from (6.1.16) because
det (T −λI) =

A −λI
B
0
C −λI
 = det (A −λI)det (C −λI).
7.1.5. If λi is not repeated, then N (A −λiI) = span {ei} . If the algebraic multiplic-
ity of λi is k, and if λi occupies positions i1, i2, . . . , ik in D, then
N (A −λiI) = span {ei1, ei2, . . . , eik} .
7.1.6. A singular ⇐⇒det (A) = 0 ⇐⇒0 solves det (A −λI) = 0 ⇐⇒0 ∈σ (A) .
7.1.7. Zero is not in or on any Gerschgorin circle. You could also say that A is non-
singular because it is diagonally dominant—see Example 7.1.6 on p. 499.
7.1.8. If (λ, x) is an eigenpair for A∗A, then ∥Ax∥2
2 / ∥x∥2
2 = x∗A∗Ax/x∗x = λ is
real and nonnegative. Furthermore, λ > 0 if and only if A∗A is nonsingular or,
equivalently, n = rank (A∗A) = rank (A). Similar arguments apply to AA∗.
7.1.9. (a)
Ax = λx
=⇒
x = λA−1x
=⇒
(1/λ)x = A−1x.
(b)
Ax = λx ⇐⇒(A −αI)x = (λ −α)x ⇐⇒(λ −α)−1x = (A −αI)−1x.
7.1.10. (a)
Successively use A as a left-hand multiplier to produce
Ax = λx
=⇒A2x = λAx = λ2x
=⇒A3x = λ2Ax = λ3x
=⇒A4x = λ3Ax = λ4x
etc.
(b)
Use part (a) to write
p(A)x =

i
αiAi

x =

i
αiAix =

i
αiλix =

i
αiλi

x = p(λ)x.
7.1.11. Since one Geschgorin circle (derived from row sums and shown below) is isolated
2
4
6
8
10
12
14
16
-2
-4
-6

Solutions
127
from the union of the other three circles, statement (7.1.14) on p. 498 insures
that there is one eigenvalue in the isolated circle and three eigenvalues in the
union of the other three. But, as discussed on p. 492, the eigenvalues of real
matrices occur in conjugate pairs. So, the root in the isolated circle must be real
and there must be at least one real root in the union of the other three circles.
Computation reveals that σ (A) = {±i, 2, 10}.
7.1.12. Use Exercise 7.1.10 to deduce that
λ ∈σ (A)
=⇒
λk ∈σ

Ak	
=⇒
λk = 0
=⇒
λ = 0.
Therefore, (7.1.7) insures that trace (A) = 
i λi = 0.
7.1.13. This is true because N (A −λI) is a subspace—recall that subspaces are closed
under vector addition and scalar multiplication.
7.1.14. If there exists a nonzero vector x that satisﬁes Ax = λ1x and Ax = λ2x,
where λ1 ̸= λ2, then
0 = Ax −Ax = λ1x −λ2x = (λ1 −λ2)x.
But this implies x = 0, which is impossible. Consequently, no such x can exist.
7.1.15. No—consider A =


1
0
0
0
1
0
0
0
2

and B =


1
0
0
0
2
0
0
0
2

.
7.1.16. Almost any example with rather random entries will do the job, but avoid diag-
onal or triangular matrices—they are too special.
7.1.17. (a)
c = (A −λI)−1(A−λI)c = (A −λI)−1(Ac−λc) = (A −λI)−1(λk −λ)c.
(b)
Use (6.2.3) to compute the characteristic polynomial for A + cdT to be
det

A + cdT −λI
	
= det

A −λI + cdT 	
= det (A −λI)

1 + dT (A −λI)−1c
	
=

±
n
9
i=1
(λj −λ)
 
1 +
dT c
λk −λ

=

±
9
j̸=k
(λj −λ)


λk + dT c −λ
	
.
The roots of this polynomial are λ1, . . . , λk−1, λk + dT c, λk+1, . . . , λn.
(c)
d = (µ −λk)c
cT c
will do the job.
7.1.18. (a)
The transpose does not alter the determinant—recall (6.1.4)—so that
det (A −λI) = det

AT −λI
	
.

128
Solutions
(b)
We know from Exercise 6.1.8 that det (A) = det (A∗), so
λ ∈σ (A) ⇐⇒0 = det (A −λI)
⇐⇒0 = det (A −λI) = det ((A −λI)∗) = det

A∗−λI
	
⇐⇒λ ∈σ (A∗) .
(c)
Yes.
(d)
Apply the reverse order law for conjugate transposes to obtain
y∗A = µy∗
=⇒
A∗y = µy
=⇒
AT y = µy
=⇒
µ ∈σ

AT 	
= σ (A) ,
and use the conclusion of part (c) insuring that the eigenvalues of real matrices
must occur in conjugate pairs.
7.1.19. (a)
When m = n, Exercise 6.2.16 insures that
λndet (AB −λI) = λndet (BA −λI)
for all λ,
so det (AB −λI) = det (BA −λI).
(b)
If m ̸= n, then the characteristic polynomials of AB and BA are of
degrees m and n, respectively, so they must be diﬀerent. When m and n are
diﬀerent—say m > n —Exercise 6.2.16 implies that
det (AB −λI) = (−λ)m−ndet (BA −λI).
Consequently, AB has m −n more zero eigenvalues than BA.
7.1.20. Suppose that A and B are n × n, and suppose X is n × g. The equation
(A −λI)BX = 0 says that the columns of BX are in N (A −λI), and hence
they are linear combinations of the basis vectors in X. Thus
[BX]∗j =

i
pijX∗j
=⇒
BX = XP,
where Pg×g = [pij] .
If (µ, z) is any eigenpair for P, then
B(Xz) = XPz = µ(Xz)
and
AX = λX
=⇒
A(Xz) = λ(Xz),
so Xz is a common eigenvector.
7.1.21. (a)
If Px = λx and y∗Q = µy∗, then T(xy∗) = Pxy∗Q = λµxy∗.
(b)
Since dim Cm×n = mn, the operator T (as well as any coordinate ma-
trix representation of T ) must have exactly mn eigenvalues (counting mul-
tiplicities), and since there are exactly mn products λµ, where λ ∈σ (P) ,
µ ∈σ (Q) , it follows that σ (T) = {λµ | λ ∈σ (P) , µ ∈σ (Q)}. Use the fact

Solutions
129
that the trace is the sum of the eigenvalues (recall (7.1.7)) to conclude that
trace (T) = 
i,j λiµj = 
i λi

j µj = trace (P) trace (Q).
7.1.22. (a)
Use (6.2.3) to compute the characteristic polynomial for D + αvvT to be
p(λ) = det

D + αvvT −λI

= det

D −λI + αvvT 
= det (D −λI)

1 + αvT (D −λI)−1v

(‡)
=


n

j=1
(λ −λj)



1 + α
n

i=1
v2
i
λi −λ

=
n

j=1
(λ −λj) + α
n

i=1

vi

j̸=i
(λ −λj)

.
For each λk, it is true that
p(λk) = αvk

j̸=k
(λk −λj) ̸= 0,
and hence no λk can be an eigenvalue for D + αvvT . Consequently, if ξ is an
eigenvalue for D + αvvT , then det (D −ξI) ̸= 0, so p(ξ) = 0 and (‡) imply
that
0 = 1 + αvT (D −ξI)−1v = 1 + α
n

i=1
v2
i
λi −ξ = f(ξ).
(b)
Use the fact that f(ξi) = 1 + αvT (D −ξiI)−1v = 0 to write

D + αvvT 
(D −ξiI)−1v = D(D −ξiI)−1v + v

αvT (D −ξiI)−1v

= D(D −ξiI)−1v −v
=

D −(D −ξiI)

(D −ξiI)−1v
= ξi(D −ξiI)−1v.
7.1.23. (a)
If p(λ) = (λ −λ1) (λ −λ2) · · · (λ −λn) , then
ln p(λ) =
n

i=1
ln (λ −λi)
=⇒
p′(λ)
p(λ) =
n

i=1
1
(λ −λi).
(b)
If |λi/λ| < 1, then we can write
(λ −λi)−1 =

λ

1 −λi
λ
−1
= 1
λ

1 −λi
λ
−1
= 1
λ

1 + λi
λ + λ2
i
λ2 + · · ·

.

130
Solutions
Consequently,
n

i=1
1
(λ −λi) =
n

i=1
 1
λ + λi
λ2 + λ2
i
λ3 + · · ·

= n
λ + τ1
λ2 + τ2
λ3 + · · · .
(c) Combining these two results yields
nλn−1 + (n −1)c1λn−2 + (n −2)c2λn−3 + · · · + cn−1
=

λn + c1λn−1 + c2λn−2 + · · · + cn
	 n
λ + τ1
λ2 + τ2
λ3 + · · ·

= nλn−1 + (nc1 + τ1) λn−2 + (nc2 + τ1c1 + τ2) λn−3
+ · · · + (ncn−1 + τ1cn−2 + τ2cn−3 + · · · + τn−1)
+ (ncn + τ1cn−1 + τ2cn−2 · · · + τn) 1
λ + · · · ,
and equating like powers of λ produces the desired conclusion.
7.1.24. We know from Exercise 7.1.10 that λ ∈σ (A)
=⇒
λk ∈σ

Ak	
, so (7.1.7)
guarantees that trace

Ak	
= 
i λk
i = τk. Proceed by induction. The result is
true for k = 1 because (7.1.7) says that c1 = −trace (A). Assume that
ci = −trace (ABi−1)
i
for
i = 1, 2, . . . , k −1,
and prove the result holds for i = k. Recursive application of the induction
hypothesis produces
B1 = c1I + A
B2 = c2I + c1A + A2
...
Bk−1 = ck−1I + ck−2A + · · · + c1Ak−2 + Ak−1,
and therefore we can use Newton’s identities given in Exercise 7.1.23 to obtain
trace (ABk−1) = trace

ck−1A + ck−2A2 + · · · + c1Ak−1 + Ak	
= ck−1τ1 + ck−2τ2 + · · · + c1τk−1 + τk
= −kck.

Solutions
131
Solutions for exercises in section 7. 2
7.2.1. The characteristic equation is λ2−2λ−8 = (λ+2)(λ−4) = 0, so the eigenvalues
are λ1 = −2 and λ2 = 4. Since no eigenvalue is repeated, (7.2.6) insures A
must be diagonalizable. A similarity transformation P that diagonalizes A is
constructed from a complete set of independent eigenvectors. Compute a pair of
eigenvectors associated with λ1 and λ2 to be
x1 =

−1
1

, x2 =

−1
2

,
and set
P =

−1
−1
1
2

.
Now verify that
P−1AP =

−2
−1
1
1
 
−8
−6
12
10
 
−1
−1
1
2

=

−2
0
0
4

= D.
7.2.2. (a)
The characteristic equation is λ3 −3λ −2 = (λ −2)(λ + 1)2 = 0, so the
eigenvalues are λ = 2 and λ = −1. By reducing A −2I and A + I to echelon
form, compute bases for N (A −2I) and N (A + I). One set of bases is
N (A −2I) = span





−1
0
2





and
N (A + I) = span





−1
1
0

,


−1
0
1




.
Therefore,
geo multA (2)
= dim N (A −2I) = 1 = alg multA (2) ,
geo multA (−1) = dim N (A + I)
= 2 = alg multA (−1) .
In other words, λ = 2 is a simple eigenvalue, and λ = −1 is a semisimple
eigenvalue.
(b)
A similarity transformation P that diagonalizes A is constructed from a
complete set of independent eigenvectors, and these are obtained from the above
bases. Set P =


−1
−1
−1
0
1
0
2
0
1

, and compute P−1 =


1
1
1
0
1
0
−2
−2
−1

and
verify that P−1AP =


2
0
0
0
−1
0
0
0
−1

.
7.2.3. Consider the matrix A of Exercise 7.2.1. We know from its solution that A
is similar to D =

−2
0
0
4

, but the two eigenspaces for A are spanned by

−1
1

and

−1
2

, whereas the eigenspaces for D are spanned by the unit
vectors e1 and e2.

132
Solutions
7.2.4. The characteristic equation of A is p(λ) = (λ−1)(λ−2)2, so alg multA (2) = 2.
To ﬁnd geo multA (2) , reduce A −2I to echelon form to ﬁnd that
N (A −2I) = span





−1
0
1




,
so geo multA (2) = dim N (A −2I) = 1. Since there exists at least one eigen-
value such that geo multA (λ) < alg multA (λ) , it follows (7.2.5) on p. 512 that
A cannot be diagonalized by a similarity transformation.
7.2.5. A formal induction argument can be given, but it suﬃces to “do it with dots”
by writing
Bk = (P−1AP)(P−1AP) · · · (P−1AP)
= P−1A(PP−1)A(PP−1) · · · (PP−1)AP = P−1AA · · · AP = P−1AkP.
7.2.6. limn→∞An =

5
2
−10
−4

. Of course, you could compute A, A2, A3, . . . in
hopes of seeing a pattern, but this clumsy approach is not deﬁnitive. A better
technique is to diagonalize A with a similarity transformation, and then use the
result of Exercise 7.2.5. The characteristic equation is 0 = λ2−(19/10)λ+(1/2) =
(λ−1)(λ−(9/10)), so the eigenvalues are λ = 1 and λ = .9. By reducing A−I
and A −.9I to echelon form, we see that
N (A −I) = span

−1
2
+
and
N (A −.9I) = span

−2
5
+
,
so A is indeed diagonalizable, and P =

−1
−2
2
5

is a matrix such that
P−1AP =

1
0
0
.9

= D or, equivalently, A = PDP−1. The result of Exer-
cise 7.2.5 says that An = PDnP−1 = P

1
0
0
.9n

P−1, so
lim
n→∞An =P

1
0
0
0

P−1 =

−1
−2
2
5
 
1
0
0
0
 
−5
−2
2
1

=

5
2
−10
−4

.
7.2.7. It follows from P−1P = I that y∗
i xj =

1
if i = j,
0
if i ̸= j, as well as y∗
i X = 0 and
Y∗xi = 0 for each i = 1, . . . , t, so
P−1AP =





y∗
1
...
y∗
t
Y∗




A

x1 | · · · | xt | X

=




λ1
· · ·
0
0
...
...
...
...
0
· · ·
λt
0
0
· · ·
0
Y∗AX



= B.

Solutions
133
Therefore, examining the ﬁrst t rows on both sides of P−1A = BP−1 yields
y∗
i A = λiy∗
i for i = 1, . . . , t.
7.2.8. If P−1AP = diag (λ1, λ2, . . . , λn) , then P−1AkP = diag

λk
1, λk
2, . . . , λk
n
	
for
k = 0, 1, 2, . . . or, equivalently, Ak = P diag

λk
1, λk
2, . . . , λk
n
	
P−1. Therefore,
Ak →0 if and only if each λk
i →0, which is equivalent to saying that |λi| < 1
for each i. Since ρ(A) = maxλi∈σ(A) |λi| (recall Example 7.1.4 on p. 497), it
follows that Ak →0 if and only if ρ(A) < 1.
7.2.9. The characteristic equation for A is λ2 −2λ + 1, so λ = 1 is the only distinct
eigenvalue. By reducing A −I to echelon form, we see that

3
4

is a basis for
N (A −I), so x = (1/5)

3
4

is an eigenvector of unit length. Following the
procedure on p. 325, we ﬁnd that R =

3/5
4/5
4/5
−3/5

is an elementary reﬂector
having x as its ﬁrst column, and RT AR = RAR =

1
25
0
1

.
7.2.10. From Example 7.2.1 on p. 507 we see that the characteristic equation for A is
p(λ) = λ3 + 5λ2 + 3λ −9 = (λ −1)(λ + 3)2 = 0. Straightforward computation
shows that
p(A) = (A −I)(A + 3I)2 =


0
−4
−4
8
−12
−8
−8
8
4




16
−16
−16
32
−32
−32
−32
32
32

= 0.
7.2.11. Rescale the observed eigenvector as x = (1/2)(1, 1, 1, 1)T = y so that xT x = 1.
Follow the procedure described in Example 5.6.3 (p. 325), and set u = x −e1
to construct
R = I −2uuT
uT u = 1
2



1
1
1
1
1
1
−1
−1
1
−1
1
−1
1
−1
−1
1


= P =

x | X

(since x = y ).
Consequently, B = XT AX =


−1
0
−1
0
2
0
−1
0
1

, and σ (B) = {2,
√
2, −
√
2}.
7.2.12. Use the spectral theorem with properties GiGj = 0 for i ̸= j and G2
i = Gi
to write AGi = (λ1G1 + λ2G2 + · · · + λkGk)Gi = λiG2
i = λiGi. A similar
argument shows GiA = λiGi.
7.2.13. Use (6.2.3) to show that λn−1(λ−dT c) = 0 is the characteristic equation for A.
Thus λ = 0 and λ = dT c are the eigenvalues of A. We know from (7.2.5) that
A is diagonalizable if and only if the algebraic and geometric multiplicities agree
for each eigenvalue. Since geo multA (0) = dim N (A) = n −rank (A) = n −1,
and since
alg multA (0) =

n −1
if
dT c ̸= 0,
n
if
dT c = 0,

134
Solutions
it follows that A is diagonalizable if and only if dT c ̸= 0.
7.2.14. If W and Z are diagonalizable—say P−1WP and Q−1ZQ are diagonal—
then

P
0
0
Q

diagonalizes A. Use an indirect argument for the converse.
Suppose A is diagonalizable but W (or Z ) is not. Then there is an eigenvalue
λ ∈σ (W) with geo multW (λ) < alg multW (λ) . Since σ (A) = σ (W) ∪σ (Z)
(Exercise 7.1.4), this would mean that
geo multA (λ) = dim N (A −λI) = (s + t) −rank (A −λI)
= (s −rank (W −λI)) + (t −rank (Z −λI))
= dim N (W −λI) + dim N (Z −λI)
= geo multW (λ) + geo multZ (λ)
< alg multW (λ) + alg multZ (λ)
< alg multA (λ) ,
which contradicts the fact that A is diagonalizable.
7.2.15. If AB = BA, then, by Exercise 7.1.20 (p. 503), A and B have a common
eigenvector—say Ax = λx and Bx = µx, where x has been scaled so that
∥x∥2 = 1. If R =

x | X

is a unitary matrix having x as its ﬁrst column
(Example 5.6.3, p. 325), then
R∗AR =

λ
x∗AX
0
X∗AX

and
R∗BR =

µ
x∗BX
0
X∗BX

.
Since A and B commute, so do R∗AR and R∗BR, which in turn implies
A2 = X∗AX and B2 = X∗BX commute. Thus the problem is deﬂated, so the
same argument can be applied inductively in a manner similar to the development
of Schur’s triangularization theorem (p. 508).
7.2.16. If P−1AP = D1 and P−1BP = D2 are both diagonal, then D1D2 = D2D1
implies that AB = BA. Conversely, suppose AB = BA. Let λ ∈σ (A) with
alg multA (λ) = a, and let P be such that P−1AP =

λIa
0
0
D

, where D
is a diagonal matrix with λ ̸∈σ (D) . Since A and B commute, so do P−1AP
and P−1BP. Consequently, if P−1BP =

W
X
Y
Z

, then

λIa
0
0
D
 
W
X
Y
Z

=

W
X
Y
Z
 
λIa
0
0
D

=⇒

λX = XD,
DY = λY,
so (D−λI)X = 0 and (D−λI)Y = 0. But (D−λI) is nonsingular, so X = 0
and Y = 0, and thus P−1BP =

W
0
0
Z

. Since B is diagonalizable, so is

Solutions
135
P−1BP, and hence so are W and Z (Exercise 7.2.14). If Q =

Qw
0
0
Qz

,
where Qw and Qz are such that Q−1
w WQw = Dw and Q−1
z ZQz = Dz are
each diagonal, then
(PQ)−1A(PQ) =

λIa
0
0
Q−1
z DQz

and
(PQ)−1B(PQ) =

Dw
0
0
Dz

.
Thus the problem is deﬂated because A2 = Q−1
z DQz and B2 = Dz commute
and are diagonalizable, so the same argument can be applied to them. If A has k
distinct eigenvalues, then the desired conclusion is attained after k repetitions.
7.2.17. It’s not legitimate to equate p(A) with det (A −AI) because the former is a
matrix while the latter is a scalar.
7.2.18. This follows from the eigenvalue formula developed in Example 7.2.5 (p. 514) by
using the identity 1 −cos θ = 2 sin2(θ/2).
7.2.19. (a)
The result in Example 7.2.5 (p. 514) shows that the eigenvalues of N+NT
and N−NT are λj = 2 cos (jπ/n + 1) and λj = 2i cos (jπ/n + 1) , respectively.
(b)
Since N −NT is skew symmetric, it follows from Exercise 6.1.12 (p. 473)
that N−NT is nonsingular if and only if n is even, which is equivalent to saying
N −NT has no zero eigenvalues (recall Exercise 7.1.6, p. 501), and hence, by
part (a), the same is true for N + NT .
(b: Alternate)
Since the eigenvalues of N+NT are λj = 2 cos (jπ/n + 1) you
can argue that N+NT has a zero eigenvalue (and hence is singular) if and only
if n is odd by showing that there exists an integer α such that jπ/n+1 = απ/2
for some 1 ≤j ≤n if and only if n is odd.
(c)
Since a determinant is the product of eigenvalues (recall (7.1.8), p. 494),
det

N −NT 	
/det

N + NT 	
= (iλ1 · · · iλn)/(λ1 · · · λn) = in = (−1)n/2.
7.2.20. The eigenvalues are {2, 0, 2, 0}. The columns of F4 =



1
1
1
1
1
−i
−1
i
1
−1
1
−1
1
i
−1
−i


are
corresponding eigenvectors.
7.2.21. Ax = λx
=⇒
y∗Ax = λy∗x and y∗A = µy∗
=⇒
y∗Ax = µy∗x.
Therefore, λy∗x = µy∗x
=⇒
(λ −µ)y∗x = 0
=⇒
y∗x = 0 when λ ̸= µ.
7.2.22. (a)
Suppose P is a nonsingular matrix such that P−1AP = D is diagonal,
and suppose that λ is the kth diagonal entry in D. If x and y∗are the kth
column and kth row in P and P−1, respectively, then x and y∗must be
right-hand and left-hand eigenvectors associated with λ such that y∗x = 1.
(b)
Consider A = I with x = ei and y = ej for i ̸= j.
(c)
Consider A =

0
1
0
0

.
7.2.23. (a)
Suppose not—i.e., suppose y∗x = 0. Then
x ⊥span (y) = N (A −λI)∗
=⇒
x ∈N (A −λI)∗⊥= R (A −λI).

136
Solutions
Also, x ∈N (A −λI), so x ∈R (A −λI)∩N (A −λI). However, because λ is
a simple eigenvalue, the the core-nilpotent decomposition on p. 397 insures that
A −λI is similar to a matrix of the form

C
0
0
01×1

, and this implies that
R (A −λI)∩N (A −λI) = 0 (Exercise 5.10.12, p. 402), which is a contradiction.
Thus y∗x ̸= 0.
(b)
Consider A = I with x = ei and y = ej for i ̸= j.
7.2.24. Let Bi be a basis for N (A −λiI), and suppose A is diagonalizable. Since
geo multA (λi) = alg multA (λi) for each i, (7.2.4) implies B = B1 ∪B2 ∪· · ·∪Bk
is a set of n independent vectors—i.e., B is a basis for ℜn. Exercise 5.9.14
now guarantees that ℜn = N (A −λ1I) ⊕N (A −λ2I) ⊕· · · ⊕N (A −λkI).
Conversely, if this equation holds, then Exercise 5.9.14 says B = B1∪B2∪· · ·∪Bk
is a basis for ℜn, and hence A is diagonalizable because B is a complete
independent set of eigenvectors.
7.2.25. Proceed inductively just as in the development of Schur’s triangularization the-
orem. If the ﬁrst eigenvalue λ is real, the reduction is exactly the same as
described on p. 508 (with everything being real). If λ is complex, then (λ, x)
and (λ, x) are both eigenpairs for A, and, by (7.2.3), {x, x} is linearly indepen-
dent. Consequently, if x = u + iv, with u, v ∈ℜn×1, then {u, v} is linearly
independent—otherwise, u = ξv implies x = (1 + iξ)u and x = (1 −iξ)u,
which is impossible. Let λ = α + iβ,
α, β ∈ℜ, and observe that Ax = λx
implies Au = αu −βv and Av = βu + αv, so AW = W

α
β
−β
α

, where
W =

u | v

. Let W = Qn×2R2×2 be a rectangular QR factorization (p. 311),
and let B = R

α
β
−β
α

R−1 so that σ (B) = σ

α
β
−β
α

= {λ, λ}, and
AW = AQR = QR

α
β
−β
α

=⇒
QT AQ = R

α
β
−β
α

R−1 = B.
If Xn×n−2 is chosen so that P =

Q | X

is an orthogonal matrix (i.e., the
columns of X complete the two columns of Q to an orthonormal basis for
ℜn ), then XT AQ = XT QB = 0, and
PT AP =

QT AQ
QT AX
XT AQ
XT AX

=

B
QT AX
0
XT AX

.
Now repeat the argument on the n −2 × n −2 matrix XT AX. Continuing in
this manner produces the desired conclusion.
7.2.26. Let the columns Rn×r be linearly independent eigenvectors corresponding to
the real eigenvalues ρj, and let {x1, x1, x2, x2, . . . , xt, xt} be a set of linearly
independent eigenvectors associated with {λ1, λ1, λ2, λ2, . . . , λt, λt} so that the
matrix Q =

R | x1 | x1 | · · · | xt | xt

is nonsingular. Write xj = uj + ivj for

Solutions
137
uj, vj ∈ℜn×1 and λj = αj + iβj for α, β ∈ℜ, and let P be the real matrix
P =

R | u1 | v1 | u2 | v2 | · · · | ut | vt

. This matrix is nonsingular because Exer-
cise 6.1.14 can be used to show that det (P) = 2t(−i)t det (Q). For example, if
t = 1, then P =

R | u1 | v1

and
det (Q) = det

R | x1 | x1] = det

R | u1 + iv1 | u1 −iv1

= det

R | u1 | u1

+ det

R | u1 | −iv1

+ det

R | iv1 | u1

+ det

R | iv1 | iv1

= −i det

R | u1 |v1

+ i det

R | v1 | u1

= −i det

R | u1 |v1

−i det

R | u1 |v1

= 2(−i) det (P).
Induction can now be used. The equations A(uj + ivj) = (αj + iβj)(uj + ivj)
yield Auj = αjuj −βjvj and Avj = βjuj + αjvj. Couple these with the fact
that AR = RD to conclude that
AP =

RD | · · · | αjuj −βjvj | βjuj + αjvj | · · ·

= P




D
0
· · ·
0
0
B1
· · ·
0
...
...
...
...
0
0
· · ·
Bt



,
where
D =




ρ1
0
· · ·
0
0
ρ2
· · ·
0
...
...
...
...
0
0
· · ·
ρr




and
Bj =

αj
βj
−βj
αj

.
7.2.27. Schur’s triangularization theorem says U∗AU = T where U is unitary and T
is upper triangular. Setting x = Uei in x∗Ax = 0 yields that tii = 0 for each
i, so tij = 0 for all i ≥j. Now set x = U(ei +ej) with i < j in x∗Ax = 0 to
conclude that tij = 0 whenever i < j. Consequently, T = 0, and thus A = 0.
To see that xT Ax = 0 ∀x ∈ℜn×1 ̸⇒A = 0, consider A =
 0
−1
1
0

.
Solutions for exercises in section 7. 3
7.3.1. cos A =

0
1
1
0

. The characteristic equation for A is λ2 + πλ = 0, so the
eigenvalues of A are λ1 = 0 and λ2 = −π. Note that A is diagonalizable
because no eigenvalue is repeated. Associated eigenvectors are computed in the
usual way to be
x1 =

1
1

and
x2 =

−1
1

,
so
P =

1
−1
1
1

and
P−1 = 1
2

1
1
−1
1

.

138
Solutions
Thus
cos A = P

cos (0)
0
0
cos (−π)

P−1 = 1
2

1
−1
1
1
 
1
0
0
−1
 
1
1
−1
1

=

0
1
1
0

.
7.3.2. From Example 7.3.3, the eigenvalues are λ1 = 0 and λ2 = −(α + β), and
associated eigenvectors are computed in the usual way to be
x1 =

β/α
1

and
x2 =

−1
1

,
so
P =

β/α
−1
1
1

and
P−1 =
1
1 + β/α

1
1
−1
β/α

.
Thus
P

eλ1t
0
0
eλ2t

P−1 =
α
α + β

β/α
−1
1
1
 
1
0
0
e−(α+β)t
 
1
1
−1
β/α

=
1
α + β

β
β
α
α

+ e−(α+β)t

α
−β
−α
β

= eλ1tG1 + eλ2tG2.
7.3.3. Solution 1: If A = PDP−1, where D = diag (λ1, λ2, . . . , λn) , then
sin2 A = P

sin2 D
	
P−1 = P




sin2 λ1
0
· · ·
0
0
sin2 λ2
· · ·
0
...
...
...
...
0
0
· · ·
sin2 λn



P−1.
Similarly for cos2A, so sin2A + cos2A = P

sin2D + cos2D
	
P−1=PIP−1 = I.
Solution 2: If σ (A) = {λ1, λ2, . . . , λk} , use the spectral representation (7.3.6)
to write sin2 A = 
k
i=1(sin2 λi)Gi and cos2 A = 
k
i=1(cos2 λi)Gi, so that
sin2 A + cos2 A = 
k
i=1(sin2 λi + cos2 λi)Gi = 
k
i=1 Gi = I.
7.3.4. The inﬁnite series representation of eA readily yields this.
7.3.5. (a)
Eigenvalues are invariant under a similarity transformation, so the eigen-
values of f(A) = Pf(D)P−1 are the eigenvalues of f(D), which are given by
{f(λ1), f(λ2), . . . , f(λn)}.
(b)
If (λ, x) is an eigenpair for A, then (A −z0I)nx = (λ −z0)nx implies
that (f(λ), x) is an eigenpair for f(A).

Solutions
139
7.3.6. If {λ1, λ2, . . . , λn} are the eigenvalues of An×n, then {eλ1, eλ2, . . . , eλn} are the
eigenvalues of eA by the spectral mapping property from Exercise 7.3.5. The
trace is the sum of the eigenvalues, and the determinant is the product of the
eigenvalues (p. 494), so det

eA	
= eλ1eλ2 · · · eλn = eλ1+λ2+···+λn = etrace(A).
7.3.7. The Cayley–Hamilton theorem says that each Am×m satisﬁes its own charac-
teristic equation, 0 = det (A −λI) = λm +c1λm−1 +c2λm−2 +· · ·+cm−1λ+cm,
so Am = −c1Am−1 −· · · −cm−1A −cmI. Consequently, Am and every higher
power of A is a polynomial in A of degree at most m−1, and thus any expres-
sion involving powers of A can always be reduced to an expression involving at
most I, A, . . . , Am−1.
7.3.8. When A is diagonalizable, (7.3.11) insures f(A) = p(A) is a polynomial in
A, and Ap(A) = p(A)A. If f(A) is deﬁned by the series (7.3.7) in the non-
diagonalizable case, then, by Exercise 7.3.7, it’s still true that f(A) = p(A) is
a polynomial in A, and thus Af(A) = f(A)A holds in the nondiagonalizable
case also.
7.3.9. If A and B are diagonalizable with AB = BA, Exercise 7.2.16 insures A and
B can be simultaneously diagonalized. If P−1AP = DA = diag (λ1, λ2, . . . , λn)
and P−1BP=DB =diag (µ1, µ2, . . . , µn) , then A + B = P(DA + DB)P−1, so
eA+B = P

eDA+DB	
P−1 = P




eλ1+µ1
0
· · ·
0
0
eλ2+µ2
· · ·
0
...
...
...
...
0
0
· · ·
eλn+µn



P−1
= P




eλ1
0
· · ·
0
0
eλ2
· · ·
0
...
...
...
...
0
0
· · ·
eλn



P−1P




eµ1
0
· · ·
0
0
eµ2
· · ·
0
...
...
...
...
0
0
· · ·
eµn



P−1
= eAeB.
In general, the same brute force multiplication of scalar series that yields
ex+y =
∞

n=0
(x + y)n
n!
=
 ∞

n=0
xn
n!
  ∞

n=0
yn
n!

= exey
holds for matrix series when AB = BA, but this is quite messy. A more elegant
approach is to set F(t) = eAt+Bt −eAteBt and note that F′(t) = 0 for all t
when AB = BA, so F(t) must be a constant matrix for all t. Since F(0) = 0,
it follows that e(A+B)t = eAteBt for all t. To see that eA+B, eAeB, and eBeA
can be diﬀerent when AB ̸= BA, consider A =

1
0
0
0

and B =

0
1
1
0

.
7.3.10. The inﬁnite series representation of eA shows that if A is skew symmetric,
then

eA	T = eAT = e−A, and hence eA 
eA	T = eA−A = e0 = I.

140
Solutions
7.3.11. (a)
Draw a transition diagram similar to that in Figure 7.3.1 with North and
South replaced by ON and OFF, respectively. Let xk be the fraction of switches
in the ON state and let yk be the fraction of switches in the OFF state after k
clock cycles have elapsed. According to the given information,
xk = xk−1(.1) + yk−1(.3)
yk = xk−1(.9) + yk−1(.7)
so that pT
k+1 = pT
k T, where pT
k = ( xk
yk ) and T =

.1
.9
.3
.7

. Compute
σ(T) = {1, −1/5}, and use the methods of Example 7.3.4 to determine the
steady-state (or limiting) distribution as
pT
∞= lim
k→∞pT
k = lim
k→∞pT
0 Tk = pT
0 lim
k→∞Tk = ( x0
y0 )

1/4
3/4
1/4
3/4

=
 x0 + y0
4
3(x0 + y0)
4

= ( 1/4
3/4 ) .
Alternately, (7.3.15) can be used with x1 =

1
1

and y1 = ( 1
3 ) to obtain
pT
∞= pT
0 lim
k→∞Tk = pT
0 lim
k→∞G1 = (pT
0 x1)yT
1
yT
1 x1
=
yT
1
yT
1 x1
= ( 1/4
3/4 ) .
(b)
Computing a few powers of T reveals that
T2 =

.280
.720
.240
.760

,
T3 =

.244
.756
.252
.748

,
T4 =

.251
.749
.250
.750

,
T5 =

.250
.750
.250
.750

,
so, for practical purposes, the device can be considered to be in equilibrium after
about 5 clock cycles, regardless of the initial conﬁguration.
7.3.12. Let σ (A) = {λ1, λ2, . . . , λk} with |λ1| ≥|λ2| ≥· · · ≥|λk|, and assume λ1 ̸= 0;
otherwise A = 0 and there is nothing to prove. Set
νn = ∥An∥
|λn
1| = ∥λn
1G1 + λn
2G2 + · · · + λn
kGk∥
|λn
1|
=
,,,,
λn
1G1 + λn
2G2 + · · · + λn
kGk
λn
1
,,,,
=
,,,,G1 +
λ2
λ1
n
G2 + · · · +
λk
λ1
n
Gk
,,,,
and let
ν =
k

i=1
∥Gi∥.

Solutions
141
Observe that 1 ≤νn ≤ν for every positive integer n —the ﬁrst inequality
follows because λn
1 ∈σ (An) implies |λn
1| ≤∥An∥by (7.1.12) on p. 497, and
the second is the result of the triangle inequality. Consequently,
11/n ≤ν1/n
n
≤ν1/n =⇒1 ≤lim
n→∞ν1/n
n
≤1 =⇒1 = lim
n→∞ν1/n
n
= lim
n→∞
∥An∥1/n
|λ1|
.
7.3.13. The dominant eigenvalue is λ1 = 4, and all corresponding eigenvectors are
multiples of (−1, 0, 1)T .
7.3.15. Consider
xn =

1 −1/n
−1

→x =

1
−1

,
but m(xn) = −1 for all n = 1, 2, . . . , and m(x) = 1, so m(xn) ̸→m(x).
Nevertheless, if limn→∞xn ̸= 0, then limn→∞m(xn) ̸= 0 because the function
˜m(v) = |m(v)| = ∥v∥∞is continuous.
7.3.16. (a)
The “vanilla” QR iteration fails to converge.
(b)
H −I = QR =

0
0
1
−1
0
0
0
1
0
  1
3
1
0
2
0
0
0
0

and RQ + I =
 −2
1
1
−2
1
0
0
0
1

.
Solutions for exercises in section 7. 4
7.4.1. The unique solution to u′ = Au, u(0) = c, is
u = eAtc = P




eλ1t
0
· · ·
0
0
eλ2t
· · ·
0
...
...
...
...
0
0
· · ·
eλnt



P−1c
= [x1 | x2 | · · · | xn]




eλ1t
0
· · ·
0
0
eλ2t
· · ·
0
...
...
...
...
0
0
· · ·
eλnt








ξ1
ξ2
...
ξn




= ξ1eλ1tx1 + ξ2eλ2tx2 + · · · + ξneλntxn.
7.4.2. (a)
All eigenvalues in σ (A) = {−1, −3} are negative, so the system is stable.
(b)
All eigenvalues in σ (A) = {1, 3} are positive, so the system is unstable.
(c)
σ (A) = {±i}, so the system is semistable. If c ̸= 0, then the components
in u(t) will oscillate indeﬁnitely.
7.4.3. (a)
If uk(t) denotes the number in population k at time t, then
u′
1 = 2u1 −u2,
u′
2 = −u1 + 2u2,
u1(0) = 100,
u2(0) = 200,

142
Solutions
or u′ = Au,
u(0) = c, where A =

2
−1
−1
2

and c =

100
200

. The
characteristic equation for A is p(λ) = λ2 −4λ + 3 = (λ −1)(λ −3) = 0, so
the eigenvalues for A are λ1 = 1 and λ2 = 3. We know from (7.4.7) that
u(t) = eλ1tv1 + eλ2tv2
(where vi = Gic )
is the solution to u′ = Au, u(0) = c. The spectral theorem on p. 517 implies
A −λ2I = (λ1 −λ2)G1 and I = G1 + G2, so (A −λ2I)c = (λ1 −λ2)v1 and
c = v1 + v2, and consequently
v1 = (A −λ2I)c
(λ1 −λ2) =

150
150

and
v2 = c −v1 =

−50
50

,
so
u1(t) = 150et −50e3t
and
u2(t) = 150et + 50e3t.
(b)
As t →∞,
u1(t) →−∞and u2(t) →+∞. But a population can’t
become negative, so species I is destined to become extinct, and this occurs at
the value of t for which u1(t) = 0 —i.e., when
et 
e2t −3
	
= 0
=⇒
e2t = 3
=⇒
t = ln 3
2 .
7.4.4. If uk(t) denotes the number in population k at time t, then the hypothesis
says
u′
1 = −u1 + u2,
u′
2 = u1 −2u2,
u1(0) = 200,
u2(0) = 400,
or u′ = Au,
u(0) = c, where A =

−1
1
1
−1

and c =

200
400

. The
characteristic equation for A is p(λ) = λ2+2λ = λ(λ+2) = 0, so the eigenvalues
for A are λ1 = 0 and λ2 = −2. We know from (7.4.7) that
u(t) = eλ1tv1 + eλ2tv2
(where vi = Gic )
is the solution to u′ = Au, u(0) = c. The spectral theorem on p. 517 implies
A −λ2I = (λ1 −λ2)G1 and I = G1 + G2, so (A −λ2I)c = (λ1 −λ2)v1 and
c = v1 + v2, and consequently
v1 = (A −λ2I)c
(λ1 −λ2) =

300
300

and
v2 = c −v1 =

−100
100

,
so
u1(t) = 300 −100e−2t
and
u2(t) = 300 + 100e−2t.
As t →∞, u1(t) →300 and u2(t) →300, so both populations will stabilize
at 300.

Solutions
143
Solutions for exercises in section 7. 5
7.5.1. Yes, because A∗A = AA∗=

30
6 −6 i
6 + 6 i
24

.
7.5.2. Real skew-symmetric and orthogonal matrices are examples.
7.5.3. We already know from (7.5.3) that real-symmetric matrices are normal and have
real eigenvalues, so only the converse needs to be proven. If A is real and
normal with real eigenvalues, then there is a complete orthonormal set of real
eigenvectors, so using them as columns in P ∈ℜn×n results in an orthogonal
matrix such that PT AP = D is diagonal or, equivalently, A = PDPT , and
thus A = AT .
7.5.4. If (λ, x) is an eigenpair for A = −A∗then x∗x ̸= 0, and λx = Ax implies
λx∗= x∗A∗, so
x∗x(λ + λ) = x∗(λ + λ)x = x∗Ax + x∗A∗x = 0 =⇒λ = −λ =⇒ℜe(λ) = 0.
7.5.5. If A is skew hermitian (real skew symmetric), then A is normal, and hence
A is unitarily (orthogonally) similar to a diagonal matrix—say A = UDU∗.
Moreover, the eigenvalues λj in D = diag (λ1, λ2, . . . , λn) are pure imaginary
numbers (Exercise 7.5.4). Since f(z) = (1 −z)(1 + z)−1 maps the imaginary
axis in the complex plane to points on the unit circle, each f(λj) is on the unit
circle, so there is some θj such that f(λj) = eiθj = cos θj+i sin θj. Consequently,
f(A) = U




f(λ1)
0
· · ·
0
0
f(λ2)
· · ·
0
...
...
...
...
0
0
· · ·
f(λn)



U∗= U




eiθ1
0
· · ·
0
0
eiθ2
· · ·
0
...
...
...
...
0
0
· · ·
eiθn



U∗
together with eiθjeiθj = eiθje−iθj = 1 yields f(A)∗f(A) = I. Note: The fact
that (I −A)(I + A)−1 = (I + A)−1(I −A) follows from Exercise 7.3.8. See the
solution to Exercise 5.6.6 for an alternate approach.
7.5.6. Consider the identity matrix—every nonzero vector is an eigenvector, so not ev-
ery complete independent set of eigenvectors needs to be orthonormal. Given
a complete independent set of eigenvectors for a normal A with σ (A) =
{λ1, λ2, . . . , λk} , use the Gram–Schmidt procedure to form an orthonormal basis
for N (A −λiI) for each i. Since N (A −λiI) ⊥N (A −λjI) for λi ̸= λj (by
(7.5.2)), the union of these orthonormal bases will be a complete orthonormal
set of eigenvectors for A.
7.5.7. Consider A =


0
1
0
0
0
0
0
0
1

.
7.5.8. Suppose Tn×n is an upper-triangular matrix such that T∗T = TT∗. The (1,1)-
entry of T∗T is |t11|2, and the (1,1)-entry of TT∗is 
n
k=1 |t1k|2. Equating

144
Solutions
these implies t12 = t13 = · · · = t1n = 0. Now use this and compare the (2,2)-
entries to get t23 = t24 = · · · = t2n = 0. Repeating this argument for each row
produces the conclusion that T must be diagonal. Conversely, if T is diagonal,
then T is normal because T∗T = diag (|t11|2 · · · |tnn|2) = TT∗.
7.5.9. Schur’s triangularization theorem on p. 508 says every square matrix is unitarily
similar to an upper-triangular matrix—say U∗AU = T. If A is normal, then
so is T. Exercise 7.5.8 therefore insures that T must be diagonal. Conversely,
if T is diagonal, then it is normal, and thus so is A.
7.5.10. If A is normal, so is A −λI. Consequently, A −λI is RPN, and hence
N (A −λI) = N (A −λI)∗(p. 408), so (A −λI) x = 0 ⇐⇒(A∗−λI)x = 0.
7.5.11. Just as in the proof of the min-max part, it suﬃces to prove
λi = max
dim V=i min
y∈V
∥y∥2=1
y∗Dy.
For each subspace V of dimension i, let SV = {y ∈V, ∥y∥2 = 1}, and let
S′
V = {y ∈V ∩F⊥, ∥y∥2 = 1},
where
F = {e1, e2, . . . , ei−1} .
( V ∩F⊥̸= 0 —otherwise dim(V + F⊥) = dim V + dim F⊥= n + 1, which is
impossible.) So S′
V contains vectors of SV of the form y = (0, . . . , 0, yi, . . . , yn)T
with 
n
j=i |yj|2 = 1, and for each subspace V with dim V = i,
y∗Dy =
n

j=i
λj|yj|2 ≤λi
n

j=i
|yj|2 = λi
for all y ∈S′
V.
Since S′
V ⊆SV, it follows that min
SV y∗Dy ≤min
S′
V
y∗Dy ≤λi, and hence
max
V
min
SV y∗Dy ≤λi.
To reverse this inequality, let ˜V = span {e1, e2, . . . , ei} , and observe that
y∗Dy =
i

j=1
λj|yj|2 ≥λi
i

j=1
|yj|2 = λi
for all y ∈S˜V,
so max
V
min
SV y∗Dy ≥max
S ˜
V
y∗Dy ≥λi.
7.5.12. Just as before, it suﬃces to prove λi =
min
v1,...,vi−1∈Cn
max
y⊥v1,...,vi−1
∥y∥2=1
y∗Dy. For each set
V = {v1, v2, . . . , vi−1} , let SV = {y ∈V⊥, ∥y∥2 = 1}, and let
S′
V = {y ∈V⊥∩T ⊥, ∥y∥2 = 1},
where
T = {ei+1, . . . , en}

Solutions
145
( V⊥∩T ⊥̸= 0 —otherwise dim(V⊥+T ⊥) = dim V⊥+dim T ⊥= n+1, which is
impossible.) So S′
V contains vectors of SV of the form y= (y1, . . . , yi, 0, . . . , 0)T
with 
i
j=1 |yj|2 = 1, and for each V = {v1, . . . , vi−1},
y∗Dy =
i

j=1
λj|yj|2 ≥λi
i

j=1
|yj|2 = λi
for all y ∈S′
V.
Since S′
V ⊆SV, it follows that max
SV
y∗Dy ≥max
S′
V
y∗Dy ≥λi, and hence
min
V max
SV
y∗Dy ≥λi.
This inequality is reversible because if ˜V = {e1, e2, . . . , ei−1} , then every y ∈˜V
has the form y = (0, . . . , 0, yi, . . . , yn)T , so
y∗Dy =
n

j=i
λj|yj|2 ≤λi
n

j=i
|yj|2 = λi
for all y ∈S˜V,
and thus min
V max
SV
y∗Dy ≤max
S ˜
V
y∗Dy ≤λi. The solution for Exercise 7.5.11
can be adapted in a similar fashion to prove the alternate max-min expression.
7.5.13. (a)
Unitary matrices are unitarily diagonalizable because they are normal. Fur-
thermore, if (λ, x) is an eigenpair for a unitary U, then
∥x∥2
2 = ∥Ux∥2
2 = ∥λx∥2
2 = |λ|2 ∥x∥2
2
=⇒
|λ| = 1
=⇒
λ = cos θ+i sin θ = eiθ.
(b)
This is a special case of Exercise 7.2.26 whose solution is easily adapted to
provide the solution for the case at hand.
Solutions for exercises in section 7. 6
7.6.1. Check the pivots in the LDLT factorization to see that A and C are positive
deﬁnite. B is positive semideﬁnite.
7.6.2. (a)
Examining Figure 7.6.7 shows that the force on m1 to the left, by Hooke’s
law, is F (l)
1 =kx1, and the force to the right is F (r)
1
=k(x2 −x1), so the total
force on m1 is F1 = F (l)
1
−F (r)
1
= k(2x1 −x2). Similarly, the total force on
m2 is F2 = k(−x1 + 2x2). Using Newton’s laws F1 = m1a1 = m1x′′
1 and
F2 = m2a2 = m2x′′
2 yields the two second-order diﬀerential equations
m1x′′
1(t) = k(2x1 −x2)
m2x′′
2(t) = k(−x1 + 2x2)
=⇒
Mx′′ = Kx,
where M =

m1
0
0
m2

, and K = k

−2
1
1
−2

.

146
Solutions
(b)
λ = (3±
√
3)/2, and the normal modes are determined by the corresponding
eigenvectors, which are found in the usual way by solving
(K −λM)v = 0.
They are
v1 =

−1 −
√
3
1

and
v2 =

−1 +
√
3
1

(c)
This part is identical to that in Example 7.6.1 (p. 559) except a 2 × 2
matrix is used in place of a 3 × 3 matrix.
7.6.3. Each mass “feels” only the spring above and below it, so
m1y′′
1 = Force up −Force down = ky1 −k(y2 −y1) = k(2y1 −y2)
m2y′′
2 = Force up −Force down = k(y2 −y1) −k(y3 −y2) = k(−y1 + 2y2 −y3)
m3y′′
3 = Force up −Force down = k(y3 −y2)
(b)
Gerschgorin’s theorem (p. 498) shows that the eigenvalues are nonnegative,
as since det (K) ̸= 0, it follows that K is positive deﬁnite.
(c)
The same technique used in the vibrating beads problem in Example 7.6.1
(p. 559) shows that modes are determined by the eigenvectors. Some computation
is required to produce λ1 ≈.198, λ2 ≈1.55, and λ3 ≈3.25. The modes are
deﬁned by the associated eigenvectors
x1 =


γ
α
β

≈


.328
.591
.737

,
x2 =


−β
−γ
α

,
and
x3 =


−α
β
−γ

.
7.6.4. Write the quadratic form as 13x2+10xy+13y2 = ( x
y )
 13
5
5
13
	  x
y
	
= zT Az.
We know from Example 7.6.3 on p. 567 that if Q is an orthogonal matrix such
that QT AQ = D =
 λ1
0
0
λ2
	
, and if w = QT z =
 u
v
	
, then
13x2 + 10xy + 13y2 = zT Az = wT Dw = λ1u2 + λ2v2.
Computation reveals that λ1 = 8,
λ2 = 18, and Q =
1
√
2

1
1
−1
1
	
, so the
graph of 13x2 + 10xy + 13y2 = 72 is the same as that for 18u2 + 8v2 = 72
or, equivalently, u2/9 + v2/4 = 1. It follows from (5.6.13) on p. 326 that the
uv-coordinate system results from rotating the standard xy-coordinate system
counterclockwise by 45◦.
7.6.5. Since A is symmetric, the LDU factorization is really A = LDLT (see Exercise
3.10.9 on p. 157). In other words, A ∼= D, so Sylvester’s law of inertia guarantees
that the inertia of A is the same as the inertia of D.

Solutions
147
7.6.6. (a)
Notice that, in general, when xT Ax is expanded, the coeﬃcient of xixj
is given by (aij + aji)/2. Therefore, for the problem at hand, we can take
A = 1
9


−2
2
8
2
7
10
8
10
4

.
(b)
Gaussian elimination provides A = LDLT , where
L =


1
0
0
−1
1
0
−4
2
1


and
D =


−2/9
0
0
0
1
0
0
0
0

,
so the inertia of A is (1, 1, 1). Setting y = LT x (or, x = (LT )−1y) yields
xT Ax = yT Dy = −2
9y2
1 + y2
2.
(c)
No, the form is indeﬁnite.
(d)
The eigenvalues of A are {2, −1, 0}, and hence the inertia is (1, 1, 1).
7.6.7. AA∗is positive deﬁnite (because A is nonsingular), so its eigenvalues λi are
real and positive. Consequently, the spectral decomposition (p. 517) allows us to
write AA∗= 
k
i=1 λiGi. Use the results on (p. 526), and set
R = (AA∗)1/2 =
k

i=1
λ1/2
i
Gi,
and
R−1 = (AA∗)−1/2 =
k

i=1
λ−1/2
i
Gi.
It now follows that R is positive deﬁnite, and A = R(R−1A) = RU, where
U = R−1A. Finally, U is unitary because
UU∗= (AA∗)−1/2AA∗(AA∗)−1/2 = I.
If R1U1 = A = R2U2, then R−1
2 R1 = U2U∗
1, which is unitary, so
R−1
2 R1R1R−1
2
= I
=⇒
R2
1 = R2
2
=⇒
R1 = R2 (because the Ri ’s are pd).
7.6.8. The 2-norm condition number is the ratio of the largest to smallest singular
values. Since L is symmetric and positive deﬁnite, the singular values are the
eigenvalues, and, by (7.6.8), max λij →8 and min λij →0 as n →∞.
7.6.9. The procedure is essentially identical to that in Example 7.6.2. The only diﬀer-
ence is that when (7.6.6) is applied, the result is
−4uij + (ui−1,j + ui+1,j + ui,j−1 + ui,j+1)
h2
+ O(h2) = fij

148
Solutions
or, equivalently,
4uij −(ui−1,j +ui+1,j +ui,j−1+ui,j+1)+O(h4) = −h2fij
for
i, j = 1, 2, . . . , n.
If the O(h4) terms are neglected, and if the boundary values gij are taken to
the right-hand side, then, with the same ordering as indicated in Example 7.6.2,
the system Lu = g −h2f is produced.
7.6.10. In⊗An =




An
0
· · ·
0
0
An
· · ·
0
...
...
...
...
0
0
· · ·
An



, An⊗In =






2In
−In
−In
2In
−In
...
...
...
−In
2In
−In
−In
2In






,
An + 2In = Tn =






4
−1
−1
4
−1
...
...
...
−1
4
−1
−1
4






n×n
, so
(In ⊗An) + (An ⊗In) =






Tn
−In
−In
Tn
−In
...
...
...
−In
Tn
−In
−In
Tn






= Ln2×n2.
Solutions for exercises in section 7. 7
7.7.1. No. This can be deduced directly from the deﬁnition of index given on p. 395,
or it can be seen by looking at the Jordan form (7.7.6) on p. 579.
7.7.2. Since the index k of a 4 × 4 nilpotent matrix cannot exceed 4, consider the
diﬀerent possibilities for k = 1, 2, 3, 4. For k = 1,
N = 04×4 is the only
possibility. If k = 2, the largest Jordan block in N is 2 × 2, so
N =




0
1
0
0
0
0
0
0
0
0
0
1
0
0
0
0




and
N =





0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0





are the only possibilities. If k = 3 or k = 4, then the largest Jordan block is
3 × 3 or 4 × 4, so
N =




0
1
0
0
0
0
1
0
0
0
0
0
0
0
0
0




and
N =



0
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0




Solutions
149
are the only respective possibilities.
7.7.3. Let k = index (L), and let ζi denote the number of blocks of size i × i or
larger. This number is determined by the number of chains of length i or larger,
and such chains emanate from the vectors in Sk−1 ∪Sk−2 ∪· · · ∪Si−1 = Bi−1.
Since Bi−1 is a basis for Mi−1, it follows that ζi = dim Mi−1 = ri−1 −ri,
where ri = rank

Li	
—recall (7.7.3).
7.7.4. x ∈Mi = R

Li	
∩N (L)
=⇒
x = Liy for some y and Lx = 0
=⇒
x =
Li−1(Ly)y and Lx = 0
=⇒
x ∈R

Li−1	
∩N (L) = Mi−1.
7.7.5. It suﬃces to prove that R

Lk−1	
⊆N (L), and this is accomplished by writing
x ∈R

Lk−1	
=⇒x = Lk−1y for some y
=⇒Lx = Lky = 0 =⇒x ∈N (L).
7.7.6. This follows from the result on p. 211.
7.7.7. L2 = 0 means that L is nilpotent of index k = 2. Consequently, the size of
the largest Jordan block in N is 2 × 2. Since r1 = 2 and ri = 0 for i ≥2,
the number of 2 × 2 blocks is r1 −2r2 + r3 = 2, so the Jordan form is
N =




0
1
0
0
0
0
0
0
0
0
0
1
0
0
0
0



.
In this case, M0 = N (L) = R (L) = M1 because L2 = 0
=⇒
R (L) ⊆N (L)
and dim R (L) = 2 = dim R (L). Now, S1 = {L∗1, L∗2} (the basic columns in
L ) is a basis for M1 = R (L), and S0 = φ. Since x1 = e1 and x2 = e2 are
solutions for Lx1 = L∗1 and Lx2 = L∗1, respectively, there are two Jordan
chains, namely {Lx1, x1} = {L∗1, e1} and {Lx2, x2} = {L∗2, e2}, so
P = [ L∗1 | e1 | L∗2 | e2 ] =



3
1
3
0
−2
0
−1
1
1
0
−1
0
−5
0
−4
0


.
Use direct computation to verify that P−1LP = N.
7.7.8. Computing ri = rank

Li	
reveals that r1 = 4, r2 = 1, and r3 = 0, so the
index of L is k = 3, and
the number of 3 × 3 blocks = r2 −2r3 + r4 = 1,
the number of 2 × 2 blocks = r1 −2r2 + r3 = 2,
the number of 1 × 1 blocks = r0 −2r1 + r2 = 1.

150
Solutions
Consequently, the Jordan form of L is
N =














0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0














.
Notice that four Jordan blocks were found, and this agrees with the fact that
dim N (L) = 8 −rank (L) = 4.
7.7.9. If Ni is an ni × ni, nilpotent block as described in (7.7.5), and if Di is the
diagonal matrix Di = diag

1, ϵi, . . . , ϵni−1	
, then D−1
i NiDi = ϵiNi. There-
fore, if P−1LP = N is in Jordan form, and if Q = PD, where D is the
block-diagonal matrix D = diag (D1, D2, . . . , Dt) , then Q−1LQ = ˜N.
Solutions for exercises in section 7. 8
7.8.1. Since rank (A) = 7, rank

A2	
= 6, and rank

A3	
= 5 = rank

A3+i	
, there
is one 3 × 3 Jordan block associates with λ = 0. Since rank (A + I) = 6 and
rank

(A + I)2	
= 5 = rank

(A + I)2+i	
, there is one 1 × 1 and one 2 × 2
Jordan block associated with λ = −1. Finally, rank (A−I) = rank

(A−I)1+i	
implies there are two 1 × 1 blocks associated with λ = 1 —i.e., λ = 1 is a
semisimple eigenvalue. Therefore, the Jordan form for A is
J =















0
1
0
0
1
0
−1
1
0
−1
−1
1
1















.
7.8.2. As noted in Example 7.8.3, σ (A) = {1} and k = index (1) = 2. Use the pro-
cedure on p. 211 to determine a basis for Mk−1 = M1 = R(A −I) ∩N(A −I)
to be S1 =

1
−2
−2

= b1
+
. (You might also determine this just by inspection.)
A basis for N (A −I) is easily found to be
 0
1
0

,

1
0
−2
+
, so examining

Solutions
151
the basic columns of

1
0
1
−2
1
0
−2
0
−2

yields the extension set S0 =
 0
1
0

= b2
+
.
Solving (A −I)x = b1 produces x = e3, so the associated Jordan chain is
{b1, e3}, and thus P =

b1 | e3 | b2
	
=

1
0
0
−2
0
1
−2
1
0

. It’s easy to check that
P−1AP =
 1
1
0
0
1
0
0
0
1

is indeed the Jordan form for A.
7.8.3. If k = index (λ), then the size of the largest Jordan block associated with λ
is k × k. This insures that λ must be repeated at least k times, and thus
index (λ) ≤alg mult (λ) .
7.8.4. index (λ) = 1 if and only if every Jordan block is 1 × 1, which happens if
and only if the number of eigenvectors associated with λ in P such that
P−1AP = J is the same as the number of Jordan blocks, and this is just an-
other way to say that alg multA (λ) = geo multA (λ) , which is the deﬁnition of
λ being a semisimple eigenvalue (p. 510).
7.8.5. Notice that R2 = I, so R−1 = R = RT , and if J⋆=
 λ
1
...
...
λ

is a generic
Jordan block, then R−1J⋆R = RJ⋆R = JT
⋆. Thus every Jordan block is similar
to its transpose. Given any Jordan form, reversal matrices of appropriate sizes
can be incorporated into a block-diagonal matrix :R such that :R−1J :R = JT
showing that J is similar to JT . Consequently, if A = PJP−1, then
AT = P−1T JT PT = P−1T :R−1J :RPT = P−1T :R−1P−1AP :RPT = Q−1AQ,
where Q = P :RPT .
7.8.6. If σ (A) = {λ1, λ2, . . . , λs} , where alg mult (λi) = ai, then the characteristic
equation for A is 0 = (x −λ1)a1(x −λ2)a2 · · · (x −λs)as = c(x). If
J =


...
J⋆...

= P−1AP is in Jordan form with J⋆=
 λi
1
...
...
λi

representing a generic Jordan block, then
c(A) = c(PJP−1) = Pc(J)P−1 = P


...
c(J⋆)...

P−1.
Notice that if J⋆is r × r , then (J⋆−λiI)r =
 0
1
...
...
0
r
= 0. Since the size of
the largest Jordan block associated with λi is ki × ki, where ki = index (λi) ≤
alg mult (λi) = ai, it follows that (J⋆−λiI)ai = 0. Consequently c(J⋆) = 0 for
every Jordan block, and thus c(A) = 0.

152
Solutions
7.8.7. By using the Jordan form for A, one can ﬁnd a similarity transformation P
such that P−1 (A −λI) P =

Lm×m
0
0
C

with Lk = 0 and C nonsingular.
Therefore, P−1 (A −λI)k P =

0m×m
0
0
Ck

, and thus
dim N

(A −λI)k	
= n −rank

(A −λI)k 	
= n −rank

Ck	
= m.
It is also true that dim N

(A −λI)m	
= m because the nullspace remains the
same for all powers beyond the index (p. 395).
7.8.8. To prove Mkj(λj) = 0, suppose x ∈Mkj(λj) so that (A −λjI)x = 0 and
x = (A −λjI)kjz for some z. Combine these with the properties of index (λj)
(p. 587) to obtain
(A −λjI)kj+1z = 0
=⇒
(A −λjI)kjz = 0
=⇒
x = 0.
The fact that the subspaces are nested follows from the observation that if x ∈
Mi+1(λj), then x = (A −λjI)i+1z and (A −λjI)x = 0 implies x = (A −
λjI)i
(A −λjI)z
	
and (A −λjI)x = 0, so Mi+1(λj) ⊆Mi(λj).
7.8.9. b(λj) ∈Si(λj) ⊆Mi(λj) = R

(A −λjI)i	
∩N (A −λjI) ⊆R

(A −λjI)i	
.
7.8.10. No—consider A =


1
1
0
0
1
0
0
0
2

and λ = 1.
7.8.11. (a)
All of these facts are established by straightforward arguments using ele-
mentary properties of matrix algebra, so the details are omitted here.
(b)
To show that the eigenvalues of A⊗B are {λiµj} m
i=1
n
j=1, let JA = P−1AP
and JB = Q−1BQ be the respective Jordan forms for A and B, and use
properties from (a) to establish that A ⊗B is similar to JA ⊗JB by writing
JA ⊗JB = (P−1AP) ⊗(Q−1BQ) = (P−1 ⊗Q−1)(A ⊗B)(P ⊗Q)
= (P ⊗Q)−1(A ⊗B)(P ⊗Q)
Thus the eigenvalues of A ⊗B are the same as those of JA ⊗JB, and because
JA and JB are upper triangular with the λi ’s and µi ’s on the diagonal, it’s
clear that JA ⊗JB is also upper triangular with diagonal entries being λiµj.
To show that the eigenvalues of (A⊗In)+(Im ⊗B) are {λi +µj} m
i=1
n
j=1, show
that (A ⊗In) + (Im ⊗B) is similar to (JA ⊗In) + (Im ⊗JB) by writing
(JA ⊗In) + (Im ⊗JB) = (P−1AP) ⊗(Q−1IQ) + (P−1IP) ⊗(Q−1BQ)
= (P−1 ⊗Q−1)(A ⊗I)(P ⊗Q)
+ (P−1 ⊗Q−1)(I ⊗B)(P ⊗Q)
= (P−1 ⊗Q−1)

(A ⊗I) + (I ⊗B)

(P ⊗Q)
= (P ⊗Q)−1
(A ⊗I) + (I ⊗B)

(P ⊗Q).

Solutions
153
Thus (A⊗In)+(Im⊗B) and (JA ⊗In) + (Im ⊗JB) have the same eigenvalues,
and the latter matrix is easily seen to be an upper-triangular matrix whose
diagonal entries are {λi + µj} m
i=1
n
j=1.
7.8.12. It was established in Exercise 7.6.10 (p. 573) that Ln2×n2 = (In⊗An)+(An⊗In),
where
An =






2
−1
−1
2
−1
...
...
...
−1
2
−1
−1
2






n×n
is the ﬁnite diﬀerence matrix of Example 1.4.1 (p. 19). The eigenvalues of An
were determined in Exercise 7.2.18 (p. 522) to be µj = 4 sin2[jπ/2(n + 1)] for
j = 1, 2, . . . , n, so it follows from the last property in Exercise 7.8.11 that the
n2 eigenvalues of Ln2×n2 = (In ⊗An) + (An ⊗In) are
λij = µi + µj = 4

sin2

iπ
2(n + 1)

+ sin2

jπ
2(n + 1)

,
i, j = 1, 2, . . . , n.
7.8.13. The same argument given in the solution of the last part of Exercise 7.8.11
applies to show that if J is the Jordan form for A, then L is similar to
(I ⊗I ⊗J) + (I ⊗J ⊗I) + (J ⊗I ⊗I), and since J is upper triangular with the
eigenvalues µj = 4 sin2[jπ/2(n + 1)] of A (recall Exercise 7.2.18 (p. 522)) on
the diagonal of J, it follows that the eigenvalues of Ln3×n3 are the n3 numbers
λijk = µi +µj +µk = 4

sin2

iπ
2(n + 1)

+ sin2

jπ
2(n + 1)

+ sin2

kπ
2(n + 1)

for i, j, k = 1, 2, . . . , n.
Solutions for exercises in section 7. 9
7.9.1. If ui(t) denotes the number of pounds of pollutant in lake i at time t > 0,
then the concentration of pollutant in lake i at time t is ui(t)/V lbs/gal, so
the model u′
i(t) = (lbs/sec) coming in−(lbs/sec) going out produces the system
u′
1 = 4r
V u2 −4r
V u1
u′
2 = 2r
V u1 + 3r
V u3 −5r
V u2
u′
3 = 2r
V u1 + r
V u2 −3r
V u3
or



u′
1
u′
2
u′
3


= r
V



−4
4
0
2
−5
3
2
1
−3






u1(t)
u2(t)
u3(t)


.
The solution of u′ = Au with u(0) = c is u = eAtc. The eigenvalues of A
are λ1 = 0 with alg mult (λ1) = 1 and λ2 = −6r/V with index (λ2) = 2, so
u = eAtc = eλ1tG1c + eλ2tG2c + teλ2t(A −λ2I)G2c.

154
Solutions
Since λ1 = 0 is a simple eigenvalue, it follows from (7.2.12) on p. 518 that
G1 = xyT /yT x, where x and yT are any pair of respective right-hand and
left-hand eigenvectors associated with λ1 = 0. By observing that Ae = 0 and
eT A = 0T for e = (1, 1, 1)T (this is a result of being a closed system), and by
using G1 + G2 = I, we have (by using x = y = e)
G1 = eeT
3 ,
G2 = I −eeT
3 ,
and
(A −λ2I)G2 = A −λ2I + λ2
3 eeT .
If α = (c1 + c2 + c3)/3 = eT c/3 denotes the average of the initial values, then
G1c = αe and G2c = c −αe, so
u(t) = αe + eλ2t(c −αe) + teλ2t
Ac −λ2(c −αe)

for
λ2 = −6r/V.
Since λ2 < 0, it follows that u(t) →αe as t →∞. In other words, the long-run
amount of pollution in each lake is the same—namely α lbs—and this is what
common sense would dictate.
7.9.2. It follows from (7.9.9) that fi(A) = Gi.
7.9.3. We know from Exercise 7.9.2 that Gi = fi(A) for fi(z) =

1
when z = λi,
0
otherwise,
and from Example 7.9.4 (p. 606) we know that every function of A is a poly-
nomial in A.
7.9.4. Using f(z) = zk in (7.9.9) on p. 603 produces the desired result.
7.9.5. Using f(z) = zn in (7.9.2) on p. 600 produces the desired result.
7.9.6. A is the matrix in Example 7.9.2, so the results derived there imply that
eA = e2G1 + e4G2 + e4(A −4I)G2 =


3e4
2e4
e4 −e2
−2e4
−e4
−4e4 −2e2
0
0
e2

.
7.9.7. The eigenvalues of A are λ1 = 1 and λ2 = 4 with alg mult (1) = 1 and
index (4) = 2, so
f(A) = f(1)G1 + f(4)G2 + f ′(4)(A −4I)G2
Since λ1 = 1 is a simple eigenvalue, it follows from formula (7.2.12) on p. 518
that G1 = xyT /yT x, where x and yT are any pair of respective right-hand
and left-hand eigenvectors associated with λ1 = 1. Using x = (−2, 1, 0)T and
y = (1, 1, 1)T produces
G1 =


2
2
2
−1
−1
−1
0
0
0


and
G2 = I −G1 =


−1
−2
−2
1
2
1
0
0
1



Solutions
155
Therefore,
f(A) = 4
√
A −I = 3G1 + 7G2 + (A −4I)G2 =


−2
−10
−11
6
15
10
−1
−2
4

.
7.9.8. (a)
The only point at which derivatives of f(z) = z1/2 fail to exist are at
λ = 0, so as long as A is nonsingular, f(A) =
√
A is deﬁned.
(b)
If A is singular so that 0 ∈σ (A) it’s clear from (7.9.9) that A1/2 exists
if and only if derivatives of f(z) = z1/2 need not be evaluated at λ = 0, and
this is the case if and only if index (0) = 1.
7.9.9. If 0 ̸= xh ∈N(A −λhI), then (7.9.11) guarantees that
Gixh =

0
if h ̸= i,
xh
if h = i,
so (7.9.9) can be used to conclude that f(A)xh = f(λh)xh. It’s an immediate
consequence of (7.9.3) that alg multA (λ) = alg multf(A) (f(λ)) .
7.9.10. (a)
If Ak×k (with k > 1) is a Jordan block associated with λ = 0, and if
f(z) = zk, then f(A) = 0 is not similar to A ̸= 0.
(b)
Also, geo multA (0) = 1 but geo multf(A) (f(0)) = geo mult0 (0) = k.
(c)
And indexA(0) = k while indexf(A)(f(λ)) = index0(0) = 1.
7.9.11. This follows because, as explained in Example 7.9.4 (p. 606), there is always a
polynomial p(z) such that f(A) = p(A), and A commutes with p(A).
7.9.12. Because every square matrix is similar to its transpose (recall Exercise 7.8.5 on
p. 596), and because similar matrices have the same Jordan structure, transpo-
sition doesn’t change the eigenvalues or their indicies. So f(A) exists if and
only if f(AT ) exists. As proven in Example 7.9.4 (p. 606), there is a polyno-
mial p(z) such that f(A) = p(A), so

f(A)
T =

p(A)
T = p(AT ) = f(AT ).
While transposition doesn’t change eigenvalues, conjugate transposition does—it
conjugates them—so it’s possible that f can exist at A but not at A∗. Fur-
thermore, you can’t replace (⋆)T by (⋆)∗in the above argument because if p(z)
has some complex coeﬃcients, then

p(A)
∗̸= p(A∗).
7.9.13. (a)
If f1(z) = ez, f2(z) = e−z, and p(x, y) = xy −1, then
h(z) = p

f1(z), f2(z)
	
= eze−z −1 = 0
for all
z ∈C,
so h(A) = p

f1(A), f2(A)
	
= 0 for all A ∈Cn×n, and thus eAe−A −I = 0.
(b)
Use f1(z) = eαz, f2(z) =

ez	α, and p(x, y) = x −y. Since
h(z) = p

f1(z), f2(z)
	
= eαz −

ez	α = 0
for all z ∈C,
h(A) = p

f1(A), f2(A)
	
= 0 for all A ∈Cn×n, and thus eαA =

eA	α.

156
Solutions
(c)
Using f1(z) = eiz, f2(z) = cos z + i sin z, and p(x, y) = x −y produces
h(z) = p

f1(z), f2(z)
	
, which is zero for all z, so h(A) = 0 for all A ∈Cn×n.
7.9.14. (a)
The representation ez = 
∞
n=0 zn/n! together with AB = BA yields
eA+B =
∞

n=0
(A + B)n
n!
=
∞

n=0
1
n!
n

j=0
n
j

AjBn−j =
∞

n=0
n

j=0
AjBn−j
j!(n −j)!
=
∞

r=0
∞

s=0
1
r!
1
s!ArBs =
 ∞

r=0
Ar
r!
  ∞

s=0
Bs
s!

= eAeB.
(b)
If A =

0
1
0
0

and B =

0
0
1
0

, then
eAeB =

1
1
0
1
 
1
0
1
1

=

2
1
1
1

,
but
eA+B = 1
2

e + e−1
e −e−1
e −e−1
e + e−1

.
7.9.15. The characteristic equation for A is λ3 = 0, and the number of 2 × 2 Jordan
blocks associated with λ = 0 is ν2 = rank (A) −2 rank

A2	
+ rank

A3	
= 1
(from the formula on p. 590), so index (λ = 0) = 2. Therefore, for f(z) = ez we
are looking for a polynomial p(z) = α0 + α1z such that p(0) = f(0) = 1 and
p′(0) = f ′(0) = 1. This yields the Hermite interpolation polynomial as
p(z) = 1 + z,
so
eA = p(A) = I + A.
Note: Since A2 = 0, this agrees with the inﬁnite series representation for eA.
7.9.16. (a)
The advantage is that the only the algebraic multiplicity and not the index
of each eigenvalue is required—determining index generally requires more eﬀort.
The disadvantage is that a higher-degree polynomial might be required, so a
larger system might have to be solved. Another disadvantage is the fact that f
may not have enough derivatives deﬁned at some eigenvalue for this method to
work in spite of the fact that f(A) exists.
(b)
The characteristic equation for A is λ3 = 0, so, for f(z) = ez, we are
looking for a polynomial p(z) = α0 + α1z + α2z2 such that p(0) = f(0) = 1,
p′(0) = f ′(0) = 1, and p′′(0) = f ′′(0) = 1. This yields
p(z) = 1 + z + z2
2 ,
so
eA = p(A) = I + A + A2
2 .
Note: Since A2 = 0, this agrees with the result in Exercise 7.9.15.
7.9.17. Since σ (A) = {α} with index (α) = 3, it follows from (7.9.9) that
f(A) = f(α)G1 + f ′(α)(A −αI)G1 + f ′′(α)
2!
(A −αI)2G1.

Solutions
157
The desired result is produced by using G1 = I (because of (7.9.10)), and
A −αI = βN + γN2, and N3 = 0.
7.9.18. Since
g(J⋆) =


g(λ)
g′(λ)
g′′(λ)/2!
0
g(λ)
g′(λ)
0
0
g(λ)

,
using Exercise 7.9.17 with α = g(λ), β = g′(λ), and γ = g′′(λ)/2! yields
f

g(J⋆)
	
= f(g(λ))I+g′(λ)f ′(g(λ))N+
;
g′′(λ)f ′(g(λ))
2!
+

g′(λ)
2f ′′(g(λ))
2!
<
N2.
Observing that
h(J⋆) =


h(λ)
h′(λ)
h′′(λ)/2!
0
h(λ)
h′(λ)
0
0
h(λ)

= h(λ)I + h′(λ)N + h′′(λ)
2!
N2,
where h(λ) = f(g(λ)), h′(λ) = f ′(g(λ))g′(λ), and
h′′(λ) = g′′(λ)f ′(g(λ)) + f ′′(g(λ))

g′(λ)
2
proves that h(J⋆) = f

g(J⋆)
	
.
7.9.19. For the function
fi(z) =

1
in a small circle about λi that is interior to Γi,
0
elsewhere,
it follows, just as in Exercise 7.9.2, that fi(A) = Gi. But using fi in (7.9.22)
produces fi(A) =
1
2πi
=
Γi(ξI −A)−1dξ, and thus the result is proven.
7.9.20. For a k × k Jordan block J⋆=



λ
1
...
...
λ


, it’s straightforward to verify that
J−1
⋆
=









λ−1
−λ−2
λ−3
· · · −1(k−1)λ−k
λ−1
−λ−2 ...
...
...
...
λ−3
λ−1
−λ−2
λ−1









=












f(λ)
f ′(λ)
f ′′(λ)
2!
· · · f (k−1)(λ)
(k −1)!
f(λ)
f ′(λ)
...
...
...
...
f ′′(λ)
2!
f(λ)
f ′(λ)
f(λ)












for f(z) = z−1, and thus if J =

... J⋆...

is the Jordan form for A = PJP−1,
then the representation of A−1 as A−1 = PJ−1P−1 agrees with the expression
for f(A) = Pf(J)P−1 given in (7.9.3) when f(z) = z−1.

158
Solutions
7.9.21.
1
2πi
4
Γ
ξ−1(ξI −A)−1dξ = A−1.
7.9.22. Partition the Jordan form for A as J =

C
0
0
N

in which C contains all
Jordan segments associated with nonzero eigenvalues and N contains all Jordan
segments associated with the zero eigenvalue (if one exists). Observe that N is
nilpotent, so g(N) = 0, and consequently
A = P

C
0
0
N

P−1 ⇒g(A) = P

g(C)
0
0 g(N)

P−1 = P

C−1
0
0
0

P−1 = AD.
It follows from Exercise 5.12.17 (p. 428) that g(A) is the Moore–Penrose pseu-
doinverse A† if and only if A is an RPN matrix.
7.9.23. Use the Cauchy–Goursat theorem to observe that
=
Γ ξ−jdξ = 0 for j = 2, 3, . . . ,
and follow the argument given in Example 7.9.8 (p. 611) with λ1 = 0 along with
the result of Exercise 7.9.22 to write
1
2πi
4
Γ
ξ−1(ξI −A)−1dξ =
1
2πi
4
Γ
ξ−1R(ξ)dξ
=
1
2πi
4
Γ
s

i=1
ki−1

j=0
ξ−1
(ξ −λi)j+1 (A −λiI)jGidξ
=
s

i=1
ki−1

j=0
 1
2πi
4
Γ
ξ−1
(ξ −λi)j+1 dξ

(A −λiI)jGi
=
s

i=1
ki−1

j=0
g(j)(λi)
j!
(A −λiI)jGi = g(A) = AD.
Solutions for exercises in section 7. 10
7.10.1. The characteristic equation for A is 0 = x3−(3/4)x−(1/4) = (x−1)(x−1/2)2,
so (7.10.33) guarantees that A is convergent (and hence also summable).
The characteristic equation for B is x3 −1 = 0, so the eigenvalues are the three
cube roots of unity, and thus (7.10.33) insures B is not convergent, but B is
summable because ρ(B) = 1 and each eigenvalue on the unit circle is semisimple
(in fact, each eigenvalue is simple).
The characteristic equation for C is
0 = x3 −(5/2)x2 + 2x −(1/2) = (x −1)2(x −1/2),
but index (λ = 1) = 2 because rank (C −I) = 2 while 1 = rank (C −I)2 =
rank (C −I)3 = · · · , so C is neither convergent nor summable.

Solutions
159
7.10.2. Since A is convergent, (7.10.41) says that a full-rank factorization I −A = BC
can be used to compute limk→∞Ak = G = I −B(CB)−1C. One full-rank
factorization is obtained by placing the basic columns of I −A in B and the
nonzero rows of E(I−A) in C. This yields
B =


−3/2
−3/2
1
−1/2
1
−1/2

,
C =

1
−1
0
0
0
1

,
and
G =


0
1
−1
0
1
−1
0
0
0

.
Alternately, since λ = 1 is a simple eigenvalue, the limit G can also be de-
termined by computing right- and left-hand eigenvectors, x = (1, 1, 0)T
and
yT = (0, −1, 1), associated with λ = 1 and setting G = xyT /(yT x) as de-
scribed in (7.2.12) on p. 518. The matrix B is not convergent but it is summable,
and since the unit eigenvalue is simple, the Ces`aro limit G can be determined
as described in (7.2.12) on p. 518 by computing right- and left-hand eigenvec-
tors, x = (1, 1, 1)T
and yT = (1, 1, 1), associated with λ = 1 and setting
G = xyT /(yT x) = 1
3


1
1
1
1
1
1
1
1
1

.
7.10.3. To see that x(k) = Akx(0) solves x(k+1) = Ax(k), use successive substitution
to write x(1) = Ax(0),
x(2) = Ax(1) = A2x(0),
x(3) = Ax(2) = A3x(0),
etc. Of course you could build a formal induction argument, but it’s not necessary.
To see that x(k) = Akx(0) + 
k−1
j=0 Ak−j−1b(j) solves the nonhomogeneous
equation x(k + 1) = Ax(k) + b(k), use successive substitution to write
x(1) = Ax(0) + b(0),
x(2) = Ax(1) + b(1) = A2x(0) + Ab(0) + b(0),
x(3) = Ax(2) + b(2) = A3x(0) + A2b(0) + Ab(0) + b(0),
etc.
7.10.4. Put the basic columns of I −A in B and the nonzero rows of the reduced row
echelon form E(I−A) in C to build a full-rank factorization of I −A = BC
(Exercise 3.9.8, p. 140), and use (7.10.41).
p=Gp(0)=(I −B(CB)−1C)p(0)=



1/6
1/6
1/6
1/6
1/3
1/3
1/3
1/3
1/3
1/3
1/3
1/3
1/6
1/6
1/6
1/6






p1(0)
p2(0)
p3(0)
p4(0)


=



1/6
1/3
1/3
1/6


.
7.10.5. To see that x(k) = Akx(0) solves x(k+1) = Ax(k), use successive substitution
to write x(1) = Ax(0),
x(2) = Ax(1) = A2x(0),
x(3) = Ax(2) = A3x(0),
etc. Of course you could build a formal induction argument, but it’s not necessary.

160
Solutions
To see that x(k) = Akx(0) + 
k−1
j=0 Ak−j−1b(j) solves the nonhomogeneous
equation x(k + 1) = Ax(k) + b(k), use successive substitution to write
x(1) = Ax(0) + b(0),
x(2) = Ax(1) + b(1) = A2x(0) + Ab(0) + b(0),
x(3) = Ax(2) + b(2) = A3x(0) + A2b(0) + Ab(0) + b(0),
etc.
7.10.6. Use (7.1.12) on p. 497 along with (7.10.5) on p. 617.
7.10.7. For A1, the respective iteration matrices for Jacobi and Gauss–Seidel are
HJ =


0
−2
2
−1
0
−1
−2
−2
0


and
HGS =


0
−2
2
0
2
−3
0
0
2

.
HJ is nilpotent of index three, so σ (HJ) = {0} , and hence ρ(HJ) = 0 < 1.
Clearly, HGS is triangular, so ρ(HGS) = 2. > 1 Therefore, for arbitrary right-
hand sides, Jacobi’s method converges after two steps, whereas the Gauss–Seidel
method diverges. On the other hand, for A2,
HJ = 1
2


0
1
−1
−2
0
−2
1
1
0


and
HGS = 1
2


0
1
−1
0
−1
−1
0
0
−1

,
and a little computation reveals that σ (HJ) =
)
±i
√
5/2
*
, so ρ(HJ) > 1, while
ρ(HGS) = 1/2 < 1. These examples show that there is no universal superiority
enjoyed by either method because there is no universal domination of ρ(HJ) by
ρ(HGS), or vise versa.
7.10.8. (a)
det (αD −L −U) = det

αD −βL −β−1U
	
= 8α3 −4α for all real α
and β ̸= 0. Furthermore, the Jacobi iteration matrix is
HJ =


0
1/2
0
1/2
0
1/2
0
1/2
0

,
and Example 7.2.5, p. 514, shows σ (HJ) = {cos(π/4), cos(2π/4), cos(3π/4)}.
Clearly, these eigenvalues are real and ρ (HJ) = (1/
√
2) ≈.707 < 1.
(b)
According to (7.10.24),
ωopt =
2
1 +
-
1 −ρ2(HJ)
≈1.172,
and
ρ

Hωopt
	
= ωopt −1 ≈.172.
(c)
RJ = −log10 ρ (HJ) = log10(
√
2) ≈.1505,
RGS = 2RJ ≈.301, and
Ropt = −log10 ρ (Hopt) ≈.766.

Solutions
161
(d)
I used standard IEEE 64-bit ﬂoating-point arithmetic (i.e., about 16 deci-
mal digits of precision) for all computations, but I rounded the results to 3 places
to report the answers given below. Depending on your own implementation, your
answers may vary slightly.
Jacobi with 21 iterations:
1
1.5
2.5
3.25
3.75
4.12
4.37
4.56
4.69
4.78
4.84
4.89
4.92
4.95
4.96
4.97
4.98
4.99
4.99
4.99
5
5
1
3
4.5
5.5
6.25
6.75
7.12
7.37
7.56
7.69
7.78
7.84
7.89
7.92
7.95
7.96
7.97
7.98
7.99
7.99
7.99
8
1
3.5
4.5
5.25
5.75
6.12
6.37
6.56
6.69
6.78
6.84
6.89
6.92
6.95
6.96
6.97
6.98
6.99
6.99
6.99
7
7
Gauss–Seidel with 11 iterations:
1
1.5
2.62
3.81
4.41
4.7
4.85
4.93
4.96
4.98
4.99
5
1
3.25
5.62
6.81
7.41
7.7
7.85
7.93
7.96
7.98
7.99
8
1
4.62
5.81
6.41
6.7
6.85
6.93
6.96
6.98
6.99
7
7
SOR (optimum) with 6 iterations:
1
1.59
3.06
4.59
4.89
4.98
5
1
3.69
6.73
7.69
7.93
7.99
8
1
5.5
6.51
6.9
6.98
7
7
7.10.9. The product rule for determinants produces
det (Hω)=det

(D−ωL)−1
det

(1−ω)D+ωU

=
n
9
i=1
a−1
ii
n
9
i=1
(1−ω)aii =(1−ω)n.
But it’s also true that det (Hω) = >n
i=1 λi, where the λi ’s are the eigenvalues
of Hω. Consequently, |λk| ≥|1 −ω| for some k because if |λi| < |1 −ω| for
all i, then |1 −ω|n = |det (Hω)| = >
i |λi| < |1 + ω|n, which is impossible.
Therefore, |1 −ω| ≤|λk| ≤ρ (Hω) < 1 implies 0 < ω < 2.
7.10.10. Observe that HJ is the block-triangular matrix
HJ = 1
4






K
I
I
K
I
...
...
...
I
K
I
I
K






n2×n2
with
K =






0
1
1
0
1
...
...
...
1
0
1
1
0






n×n
.
Proceed along the same lines as in Example 7.6.2, to argue that HJ is similar
to the block-diagonal matrix




T1
0
· · ·
0
0
T2
· · ·
0
...
...
...
...
0
0
· · ·
Tn



,
where
Ti =






κi
1
1
κi
1
...
...
...
1
κi
1
1
κi






n×n

162
Solutions
in which κi ∈σ (K) . Use the result of Example 7.2.5 (p. 514) to infer that the
eigenvalues of Ti are κi + 2 cos jπ/(n + 1) for j = 1, 2, . . . , n and, similarly,
the eigenvalues of K are κi = 2 cos iπ/(n+1) for i = 1, 2, . . . , n. Consequently
the n2 eigenvalues of HJ are λij = (1/4)

2 cos iπ/(n + 1) + 2 cos jπ/(n + 1)

,
so ρ (HJ) = maxi,j λij = cos π/(n + 1).
7.10.11. If limn→∞αn = α, then for each ϵ > 0 there is a natural number N = N(ϵ)
such that |αn −α| < ϵ/2 for all n ≥N. Furthermore, there exists a real number
β such that |αn −α| < β for all n. Consequently, for all n ≥N,
|µn −α| =

α1 + α2 + · · · + αn
n
−α
 = 1
n

N

k=1
(αk −α) +
n

k=N+1
(αk −α)

≤1
n
N

k=1
|αk −α| + 1
n
n

k=N+1
|αk −α| < Nβ
n
+ n −N
n
ϵ
2 ≤Nβ
n
+ ϵ
2.
When n is suﬃciently large, Nβ/n ≤ϵ/2 so that |µn −α| < ϵ, and therefore,
limn→∞µn = α. Note: The same proof works for vectors and matrices by
replacing | ⋆| with a vector or matrix norm.
7.10.12. Prove that (a) ⇒(b) ⇒(c) ⇒(d) ⇒(e) ⇒(f) ⇒(a).
(a) ⇒(b):
This is a consequence of (7.10.28).
(b) ⇒(c):
Use induction on the size of An×n. For n = 1, the result is trivial.
Suppose the result holds for n = k —i.e., suppose positive leading minors insures
the existence of LU factors which are M-matrices when n = k. For n = k + 1,
use the induction hypothesis to write
A(k+1)×(k+1) =
 :A
c
dT
α

=
 :L :U
c
dT
α

=

:L
0
dT :U−1
1
 :U
:L−1c
0
σ

= LU,
where :L and
:U are M-matrices. Notice that σ > 0 because det( :U) > 0
and 0 < det (A) = σ det(:L) det( :U). Consequently, L and U are M-matrices
because
L−1 =

:L−1
0
−dT :U−1:L−1
1

≥0
and
U−1 =
 :U−1
−σ−1 :U−1:L−1c
0
σ−1

≥0.
(c) ⇒(d):
A = LU with L and U M-matrices implies A−1 = U−1L−1 ≥0,
so if x = A−1e, where e = (1, 1, . . . , 1)T , then x > 0 (otherwise A−1 would
have a zero row, which would force A to be singular), and Ax = e > 0.
(d) ⇒(e):
If x > 0 is such that Ax > 0, deﬁne D = diag (x1, x2, . . . , xn)
and set B = AD, which is clearly another Z-matrix. For e = (1, 1, . . . , 1)T ,
notice that Be = ADe = Ax > 0 says each row sum of B = AD is positive.
In other words, for each i = 1, 2, . . . , n,
0 <

j
bij =

j̸=i
bij +bii ⇒bii >

j̸=i
−bij =

j̸=i
|bij|
for each i = 1, 2, . . . , n.

Solutions
163
(e) ⇒(f):
Suppose that AD is diagonally dominant for a diagonal matrix D
with positive entries, and suppose each aii > 0. If E = diag (a11, a22, . . . , ann)
and −N is the matrix containing the oﬀ-diagonal entries of A, then A = E−N
is the Jacobi splitting for A as described in Example 7.10.4 on p. 622, and
AD = ED + ND is the Jacobi splitting for AD with the iteration matrix
H = D−1E−1ND. It was shown in Example 7.10.4 that diagonal dominance
insures convergence of Jacobi’s method (i.e., ρ (H) < 1 ), so, by (7.10.14), p. 620,
A = ED(I −H)D−1
=⇒
A−1 = D(I −H)−1D−1E−1 ≥0,
and this guarantees that if Ax ≥0, then x ≥0.
(f) ⇒(a):
Let r ≥max |aii| so that B = rI −A ≥0, and ﬁrst show that the
condition (Ax ≥0 ⇒x ≥0) insures the existence of A−1. For any x ∈N (A),
(rI−B)x = 0 ⇒rx = Bx ⇒r|x| ≤|B|x| ⇒A(−|x|) ≥0 ⇒−|x| ≥0 ⇒x = 0,
so N (A) = 0. Now, A[A−1]∗i = ei ≥0 ⇒[A−1]∗i ≥0, and thus A−1 ≥0.
7.10.13. (a)
If Mi is ni × ni with rank (Mi) = ri, then Bi is ni × ri and Ci is
ri × ni with rank (Bi) = rank (Ci) = ri. This means that Mi+1 = CiBi is
ri × ri, so if ri < ni, then Mi+1 has smaller size than Mi. Since this can’t
happen indeﬁnitely, there must be a point in the process at which rk = nk or
rk = 0 and thus some Mk is either nonsingular or zero.
(b)
Let M = M1 = A −λI, and notice that
M2 = B1C1B1C1 = B1M2C1,
M3 = B1C1B1C1B1C1 = B1(B2C2)(B2C2)C1 = B1B2M3C2C1,
...
Mi = B1B2 · · · Bi−1MiCi−1 · · · C2C1.
In general, it’s true that rank (XYZ) = rank (Y) whenever X has full col-
umn rank and Z has full row rank (Exercise 4.5.12, p. 220), so applying this
yields rank

Mi	
= rank (Mi) for each i = 1, 2, . . . . Suppose that some
Mi = Ci−1Bi−1 is ni × ni and nonsingular. For this to happen, we must have
Mi−1 = Bi−1Ci−1, where Bi−1 is ni−1 × ni, Ci−1 is ni × ni−1, and
rank (Mi−1) = rank (Bi−1) = rank (Ci−1) = ni = rank (Mi).
Therefore, if k is the smallest positive integer such that M−1
k
exists, then k
is the smallest positive integer such that rank (Mk−1) = rank (Mk), and thus
k is the smallest positive integer such that rank

Mk−1	
= rank

Mk	
, which
means that index (M) = k −1 or, equivalently, index (λ) = k −1. On the other
hand, if some Mi = 0, then rank

Mi	
= rank (Mi) insures that Mi = 0.

164
Solutions
Consequently, if k is the smallest positive integer such that Mk = 0, then k
is the smallest positive integer such that Mk = 0. Therefore, M is nilpotent of
index k, and this implies that index (λ) = k.
7.10.14. M = A −4I =


−7
−8
−9
5
7
9
−1
−2
−3

−→


1
0
−1
0
1
2
0
0
0

⇒B1 =


−7
−8
5
7
−1
−2


and C1 =

1
0
−1
0
1
2

, so M2 = C1B1 =

−6
−6
3
3

−→

1
1
0
0

⇒
B2 =

−6
3

and C2 =

1
1
	
, so M3 = C2B2 = −3. Since M3 is the ﬁrst
Mi to be nonsingular, index (4) = 3 −1 = 2. Now, index (1) if forced to be 1
because 1 = alg mult (1) ≥index (1) ≥1.
7.10.15. (a)
Since σ (A) = {1, 4} with index (1) = 1 and index (4) = 2, the Jordan
form for A is J =


1
0
0
0
4
1
0
0
4

.
(b)
The Hermite interpolation polynomial p(z) = α0+α1z+α2z2 is determined
by solving p(1) = f(1), p(4) = f(4), and p′(4) = f ′(4) for αi ’s. So


1
1
1
1
4
16
0
1
8




α0
α1
α2

=


f(1)
f(4)
f ′(4)


=⇒


α0
α1
α2

=


1
1
1
1
4
16
0
1
8


−1 

f(1)
f(4)
f ′(4)


= −1
9


−16
7
−12
8
−8
15
−1
1
−3




f(1)
f(4)
f ′(4)


= −1
9


−16f(1) + 7f(4) −12f ′(4)
8f(1) −8f(4) + 15f ′(4)
−f(1) + f(4) −3f ′(4)

.
Writing f(A) = p(A) produces
f(A) =
−16I + 8A −A2
−9

f(1) +
7I −8A + A2
−9

f(4)
+
−12I + 15A −3A2
−9

f ′(4).
7.10.16. Suppose that limk→∞Ak exists and is nonzero. It follows from (7.10.33) that
λ = 1 is a semisimple eigenvalue of A, so the Jordan form for B looks like
B = I −A = P

0
0
0
I −K

P−1, where I −K is nonsingular. Therefore, B
belongs to a matrix group and
B# = P

0
0
0
(I −K)−1

P−1
=⇒
I −BB# = P

I
0
0
0

P−1.

Solutions
165
Comparing I −BB# with (7.10.32) shows that limk→∞Ak = I −BB#. If
limk→∞Ak = 0, then ρ(A) < 1, and hence B is nonsingular, so B# = B−1
and I −BB# = 0. In other words, it’s still true that limk→∞Ak = I −BB#.
7.10.17. We already know from the development of (7.10.41) that if rank (M) = r, then
CB and V∗
1U1 are r × r nonsingular matrices. It’s a matter of simple algebra
to verify that MM#M = M, M#MM# = M#, and MM# = M#M.
Solutions for exercises in section 7. 11
7.11.1. m(x) = x2 −3x + 2
7.11.2. v(x) = x −2
7.11.3. c(x) = (x −1)(x −2)2
7.11.4. J =





λ
λ
λ
µ
µ
µ
1
µ





7.11.5. Set ν0 = ∥I∥F = 2, U0 = I/2, and generate the sequence (7.11.2).
r01 = ⟨U0 A⟩= 2,
ν1 = ∥A −r01U0∥F =
√
1209,
U1 = A −r01U0
ν1
= A −I
√
1209,
r02 =
.
U0 A2/
= 2,
r12 =
.
U1 A2/
= 2
√
1209,
ν2 = ∥A2 −r02U0 −r12U1∥F = 0,
so that
R =
 2
2
0
√
1209

,
c =

2
2
√
1209

, and R−1c =
 −1
2

=
 α0
α1

.
Consequently, the minimum polynomial is m(x) = x2 −2x + 1 = (x −1)2. As a
by-product, we see that λ = 1 is the only eigenvalue of A, and index (λ) = 2,
so the Jordan form for A must be J =



1
1
0
0
0
1
0
0
0
0
1
0
0
0
0
1


.
7.11.6. Similar matrices have the same minimum polynomial because similar matrices
have the same Jordan form, and hence they have the same eigenvalues with the
same indicies.
7.11.10. x = (3, −1, −1)T
7.11.12. x = (−3, 6, 5)T

Solutions for Chapter 8
Solutions for exercises in section 8. 2
8.2.1. The eigenvalues are σ (A) = {12, 6} with alg multA (6) = 2, and it’s clear that
12 = ρ(A) ∈σ (A) . The eigenspace N(A−12I) is spanned by e = (1, 1, 1)T , so
the Perron vector is p = (1/3)(1, 1, 1)T . The left-hand eigenspace N(AT −12I)
is spanned by (1, 2, 3)T , so the left-hand Perron vector is qT = (1/6)(1, 2, 3).
8.2.3. If p1 and p2 are two vectors satisfying Ap = ρ (A) p, p > 0, and ∥p∥1 = 1,
then dim N (A −ρ (A) I) = 1 implies that p1 = αp2 for some α < 0. But
∥p1∥1 = ∥p2∥1 = 1 insures that α = 1.
8.2.4. σ (A) = {0, 1}, so ρ (A) = 1 is the Perron root, and the Perron vector is
p = (α + β)−1(β, α).
8.2.5. (a)
ρ(A/r) = 1 is a simple eigenvalue of A/r, and it’s the only eigenvalue on
the spectral circle of A/r, so (7.10.33) on p. 630 guarantees that limk→∞(A/r)k
exists.
(b)
This follows from (7.10.34) on p. 630.
(c)
G is the spectral projector associated with the simple eigenvalue λ = r,
so formula (7.2.12) on p. 518 applies.
8.2.6. If e is the column of all 1 ’s, then Ae = ρe. Since e > 0, it must be a positive
multiple of the Perron vector p, and hence p = n−1e. Therefore, Ap = ρp
implies that ρ = ρ (A) . The result for column sums follows by considering AT .
8.2.7. Since ρ = maxi

j aij is the largest row sum of A, there must exist a matrix
E ≥0 such that every row sum of B = A + E is ρ. Use Example 7.10.2
(p. 619) together with Exercise 8.2.7 to obtain ρ (A) ≤ρ (B) = ρ. The lower
bound follows from the Collatz–Wielandt formula. If e is the column of ones,
then e ∈N, so
ρ (A) = max
x∈N f(x) ≥f(e) = min
1≤i≤n
[Ae]i
ei
= min
i
n

j=1
aij.
8.2.8. (a), (b), (c), and (d) are illustrated by using the nilpotent matrix A =

0
1
0
0

.
(e)
A =

0
1
1
0

has eigenvalues ±1.
8.2.9. If ξ = g(x) for x ∈P, then ξx ≥Ax > 0. Let p and qT be the respective
the right-hand and left-hand Perron vectors for A associated with the Perron
root r, and use (8.2.3) along with qT x > 0 to write
ξx ≥Ax > 0
=⇒
ξqT x ≥qT Ax = rqT x
=⇒
ξ ≥r,

168
Solutions
so g(x) ≥r for all x ∈P. Since g(p) = r and p ∈P, it follows that
r = minx∈P g(x).
8.2.10. A =
 1
2
2
4

=⇒
ρ(A) = 5, but g(e1) = 1
=⇒
minx∈N g(x) < ρ(A).
Solutions for exercises in section 8. 3
8.3.1. (a)
The graph is strongly connected.
(b)
ρ (A) = 3, and p = (1/6, 1/2, 1/3)T .
(c)
h = 2 because A is imprimitive and singular.
8.3.2. If A is nonsingular then there are either one or two distinct nonzero eigenvalues
inside the spectral circle. But this is impossible because σ (A) has to be invariant
under rotations of 120◦by the result on p. 677. Similarly, if A is singular with
alg multA (0) = 1, then there is a single nonzero eigenvalue inside the spectral
circle, which is impossible.
8.3.3. No! The matrix A =

1
1
0
2

has ρ (A) = 2 with a corresponding eigenvector
e = (1, 1)T , but A is reducible.
8.3.4. Pn is nonnegative and irreducible (its graph is strongly connected), and Pn
is imprimitive because Pn
n = I insures that every power has zero entries. Fur-
thermore, if λ ∈σ (Pn) , then λn ∈σ(Pn
n) = {1}, so all eigenvalues of Pn
are roots of unity. Since all eigenvalues on the spectral circle are simple (re-
call (8.3.13) on p. 676) and uniformly distributed, it must be the case that
σ (Pn) = {1, ω, ω2, . . . , ωn−1}.
8.3.5. A is irreducible because the graph G(A) is strongly connected—every node is
accessible by some sequence of paths from every other node.
8.3.6. A is imprimitive. This is easily seen by observing that each A2n for n > 1 has
the same zero pattern (and each A2n+1 for n > 0 has the same zero pattern),
so every power of A has zero entries.
8.3.7. (a)
Having row sums less than or equal to 1 means that ∥P∥∞≤1. Because
ρ (⋆) ≤∥⋆∥for every matrix norm (recall (7.1.12) on p. 497), it follows that
ρ (S) ≤∥S∥1 ≤1.
(b)
If e denotes the column of all 1’s, then the hypothesis insures that Se ≤e,
and Se ̸= e. Since S is irreducible, the result in Example 8.3.1 (p. 674) implies
that it’s impossible to have ρ (S) = 1 (otherwise Se = e), and therefore ρ (S) <
1 by part (a).
8.3.8. If p is the Perron vector for A, and if e is the column of 1 ’s, then
D−1ADe = D−1Ap = rD−1p = re
shows that every row sum of D−1AD is r, so we can take P = r−1D−1AD
because the Perron–Frobenius theorem guarantees that r > 0.
8.3.9. Construct the Boolean matrices as described in Example 8.3.5 (p. 680), and show
that B9 has a zero in the (1, 1) position, but B10 > 0.

Solutions
169
8.3.10. According to the discussion on p. 630, f(t) →0 if r < 1. If r = 1, then
f(t) →Gf(0) = p

qT f(0)/qT p

> 0, and if r > 1, the results of the Leslie
analysis imply that fk(t) →∞for each k.
8.3.11. The only nonzero coeﬃcient in the characteristic equation for L is c1, so
gcd{2, 3, . . . , n} = 1.
8.3.12. (a)
Suppose that A is essentially positive. Since we can always ﬁnd a β > 0
such that βI + diag (a11, a22, . . . , ann) ≥0, and since aij ≥0 for i ̸= j, it
follows that A + βI is a nonnegative irreducible matrix, so (8.3.5) on p. 672
can be applied to conclude that (A + (1 + β)I)n−1 > 0, and thus A + αI is
primitive with α = β +1. Conversely, if A+αI is primitive, then A+αI must
be nonnegative and irreducible, and hence aij ≥0 for every i ̸= j, and A must
be irreducible (diagonal entries don’t aﬀect the reducibility or irreducibility).
(b)
If A is essentially positive, then A + αI is primitive for some α (by the
ﬁrst part), so (A + αI)k > 0 for some k. Consequently, for all t > 0,
0 <
∞

k=0
tk(A + αI)k
k!
= et(A+αI) = etAetαI = B
=⇒
0 < e−αtB = etA.
Conversely, if 0 < etA = ∞
k=0 tkAk/k! for all t > 0, then aij ≥0 for every
i ̸= j, for if aij < 0 for some i ̸= j, then there exists a suﬃciently small t > 0
such that [I + tA + t2A2/2 + · · ·]ij < 0, which is impossible. Furthermore, A
must be irreducible; otherwise
A ∼

X
Y
0
Z

=⇒
etA =
∞

k=0
tkAk/k! ∼

⋆
⋆
0
⋆

,
which is impossible.
8.3.13. (a)
Being essentially positive implies that there exists some α ∈ℜsuch that
A+αI is nonnegative and irreducible (by Exercise 8.3.12). If (r, x) is the Perron
eigenpair for A + αI, then for ξ = r −α, (ξ, x) is an eigenpair for A.
(b)
Every eigenvalue of A + αI has the form z = λ + α, where λ ∈σ (A) ,
so if r is the Perron root of A + αI, then for z ̸= r,
|z| < r
=⇒
Re (z) < r
=⇒
Re (λ + α) < r
=⇒
Re (λ) < r −α = ξ.
(c)
If A ≤B, then A + αI ≤B + αI, so Wielandt’s theorem (p. 675) insures
that rA = ρ (A + αI) ≤ρ (B + αI) = rB, and hence ξA = rA −α ≤rB −α = ξB.
8.3.14. If A is primitive with r = ρ (A) , then, by (8.3.10) on p. 674,
A
r
k
→G > 0
=⇒
∃k0 such that
A
r
m
> 0
∀m ≥k0
=⇒
a(m)
ij
rm > 0
∀m ≥k0
=⇒
lim
m→∞

a(m)
ij
rm
	1/m
→1
=⇒
lim
m→∞

a(m)
ij
1/m
= r.

170
Solutions
Conversely, we know from the Perron–Frobenius theorem that r > 0, so if
limk→∞

a(k)
ij
1/k
= r, then ∃k0 such that ∀m ≥k0,

a(m)
ij
1/m
> 0, which
implies that Am > 0, and thus A is primitive by Frobenius’s test (p. 678).
Solutions for exercises in section 8. 4
8.4.1. The left-hand Perron vector for P is πT = (10/59, 4/59, 18/59, 27/59). It’s
the limiting distribution in the regular sense because P is primitive (it has a
positive diagonal entry—recall Example 8.3.3 (p. 678)).
8.4.2. The left-hand Perron vector is πT = (1/n)(1, 1, . . . , 1). Thus the limiting dis-
tribution is the uniform distribution, and in the long run, each state is occupied
an equal proportion of the time. The limiting matrix is G = (1/n)eeT .
8.4.3. If P is irreducible, then ρ (P) = 1 is a simple eigenvalue for P, so
rank (I −P) = n−dim N (I −P) = n−geo multP (1) = n−alg multP (1) = n−1.
8.4.4. Let A = I−P, and recall that rank (A) = n−1 (Exercise 8.4.3). Consequently,
A singular =⇒A[adj (A)] = 0 = [adj (A)]A
(Exercise 6.2.8, p. 484),
and
rank (A) = n −1 =⇒rank (adj (A)) = 1
(Exercises 6.2.11).
It follows from A[adj (A)] = 0 and the Perron–Frobenius theorem that each col-
umn of [adj (A)] must be a multiple of e (the column of 1 ’s or, equivalently,
the right-hand Perron vector for P), so [adj (A)] = evT for some vector v.
But [adj (A)]ii = Pi forces vT = (P1, P2, . . . , Pn). Similarly, [adj (A)]A = 0
insures that each row in [adj (A)] is a multiple of πT (the left-hand Perron vec-
tor of P), and hence vT = απT for some α. This scalar α can’t be zero; other-
wise [adj (A)] = 0, which is impossible because rank (adj (A)) = 1. Therefore,
vT e = α ̸= 0, and vT /(vT e) = vT /α = πT .
8.4.5. If Qk×k (1 ≤k < n) is a principal submatrix of P, then there is a permutation
matrix H such that HT PH =

Q
X
Y
Z

= P. If B =

Q
0
0
0

, then
B ≤P, and we know from Wielandt’s theorem (p. 675) that ρ (B) ≤ρ

P

= 1,
and if ρ (B) = ρ

P

= 1, then there is a number φ and a nonsingular diagonal
matrix D such that B = eiφDPD−1 or, equivalently, P = e−iφDBD−1. But
this implies that X = 0, Y = 0, and Z = 0, which is impossible because P
is irreducible. Therefore, ρ (B) < 1, and thus ρ (Q) < 1.
8.4.6. In order for I −Q to be an M-matrix, it must be the case that [I −Q]ij ≤0
for i ̸= j, and I −Q must be nonsingular with (I −Q)−1 ≥0. It’s clear that
[I −Q]ij ≤0 because 0 ≤qij ≤1. Exercise 8.4.5 says that ρ (Q) < 1, so

Solutions
171
the Neumann series expansion (p. 618) insures that I −Q is nonsingular and
(I −Q)−1 = ∞
j=1 Qj ≥0. Thus I −Q is an M-matrix.
8.4.7. We know from Exercise 8.4.6 that every principal submatrix of order 1 ≤k <
n is an M-matrix, and M-matrices have positive determinants by (7.10.28) on
p. 626.
8.4.8. You can consider an absorbing chain with eight states
{(1, 1, 1), (1, 1, 0), (1, 0, 1), (0, 1, 1), (1, 0, 0), (0, 1, 0), (0, 0, 1), (0, 0, 0)}
similar to what was described in Example 8.4.5, or you can use a four-state
chain in which the states are deﬁned to be the number of controls that hold at
each activation of the system. Using the eight-state chain yields the following
mean-time-to-failure vector.










(1, 1, 1)
368.4
(1, 1, 0)
366.6
(1, 0, 1)
366.6
(0, 1, 1)
366.6
(1, 0, 0)
361.3
(0, 1, 0)
361.3
(0, 0, 1)
361.3










= (I −T11)−1e.
8.4.9. This is a Markov chain with nine states (c, m) in which c is the chamber
occupied by the cat, and m is the chamber occupied by the mouse. There are
three absorbing states—namely (1, 1), (2, 2), (3, 3). The transition matrix is
P = 1
72














(1, 2)
(1, 3)
(2, 1)
(2, 3)
(3, 1)
(3, 2)
(1, 1)
(2, 2)
(3, 3)
(1, 2)
18
12
3
6
3
9
6
9
6
(1, 3)
12
18
3
9
3
6
6
6
9
(2, 1)
3
3
18
9
12
6
6
9
6
(2, 3)
4
6
6
18
4
8
2
12
12
(3, 1)
3
3
12
6
18
9
6
6
9
(3, 2)
6
4
4
8
6
18
2
12
12
(1, 1)
0
0
0
0
0
0
72
0
0
(2, 2)
0
0
0
0
0
0
0
72
0
(3, 3)
0
0
0
0
0
0
0
0
72














The expected number of steps until absorption and absorption probabilities are
(I −T11)−1e=





(1, 2)
3.24
(1, 3)
3.24
(2, 1)
3.24
(2, 3)
2.97
(3, 1)
3.24
(3, 2)
2.97





and
(I −T11)−1T12=





(1, 1)
(2, 2)
(3, 3)
0.226
0.41
0.364
0.226
0.364
0.41
0.226
0.41
0.364
0.142
0.429
0.429
0.226
0.364
0.41
0.142
0.429
0.429






Index
A
absolute uncertainty or error,
414
absorbing Markov chains,
700
absorbing states,
700
addition, properties of,
82
additive identity,
82
additive inverse,
82
adjacency matrix,
100
adjoint,
84, 479
adjugate,
479
aﬃne functions,
89
aﬃne space,
436
algebraic group,
402
algebraic multiplicity,
496, 510
amplitude,
362
Anderson-Duﬃn formula,
441
Anderson, Jean,
xii
Anderson, W. N., Jr.,
441
angle,
295
canonical,
455
between complementary spaces,
389, 450
maximal,
455
principal,
456
between subspaces,
450
annihilating polynomials,
642
aperiodic Markov chain,
694
Arnoldi’s algorithm,
653
Arnoldi, Walter Edwin,
653
associative property
of addition,
82
of matrix multiplication,
105
of scalar multiplication,
83
asymptotic rate of convergence,
621
augmented matrix,
7
Autonne, L.,
411
B
back substitution,
6, 9
backward error analysis,
26
backward triangle inequality,
273
band matrix,
157
Bartlett, M. S.,
124
base-b,
375
basic columns,
45, 61, 178, 218
combinations of,
54
basic variables,
58, 61
in nonhomogeneous systems,
70
basis,
194, 196
change of,
253
characterizations,
195
orthonormal,
355
basis for
direct sum,
383
intersection of spaces,
211
space of linear transformations,
241
Bauer–Fike bound,
528
beads on a string,
559
Bellman, Richard,
xii
Beltrami, Eugenio,
411
Benzi, Michele,
xii
Bernoulli, Daniel,
299
Bessel, Friedrich W.,
305
Bessel’s inequality,
305
best approximate inverse,
428
biased estimator,
446
binary representation,
372
Birkhoﬀ, Garrett,
625
Birkhoﬀ’s theorem,
702
bit reversal,
372
bit reversing permutation matrix,
381
block diagonal,
261–263
rank of,
137
using real arithmetic,
524
block matrices,
111
determinant of,
475
linear operators,
392
block matrix multiplication,
111
block triangular,
112, 261–263
determinants,
467
eigenvalues of,
501
Bolzano–Weierstrass theorem,
670
Boolean matrix,
679
bordered matrix,
485
eigenvalues of,
552
branch,
73, 204
Brauer, Alfred,
497
Bunyakovskii, Victor,
271
C
cancellation law,
97
canonical angles,
455
canonical form, reducible matrix,
695
Cantor, Georg,
597
Cauchy, Augustin-Louis,
271
determinant formula,
484
integral formula,
611
Cauchy–Bunyakovskii–Schwarz inequality,
272
Cauchy–Goursat theorem,
615
Cauchy–Schwarz inequality,
287
Cayley, Arthur,
80, 93, 158, 460
Cayley–Hamilton theorem,
509, 532, 597
to determine f(A) ,
614
Cayley transformation,
336, 556
CBS inequality,
272, 277, 473
general form,
287
centered diﬀerence approximations,
19
Ces`aro, Ernesto,
630
Ces`aro sequence,
630
Ces`aro summability,
630, 633, 677
for stochastic matrix,
697
chain, Jordan,
575
change of basis,
251, 253, 258

706
Index
change of coordinates,
252
characteristic equation,
491, 492
coeﬃcients,
494, 504
characteristic polynomial,
491, 492
of a product,
503
characteristic values and vectors,
490
Chebyshev, Pafnuty Lvovich,
40, 687
checking an answer,
35, 416
Cholesky, Andre-Louis,
154
Cholesky factorization,
154, 314, 558, 559
Cimmino, Gianfranco,
445
Cimmino’s reﬂection method,
445
circuit,
204
circulant matrix,
379
with convolution,
380
eigenvalues, eigenvectors,
523
classical Gram–Schmidt algorithm,
309
classical least squares,
226
clock cycles,
539, 694
closest point
to an aﬃne space,
436
with Fourier expansion,
440
theorem,
435
closure property
of addition,
82, 160
of multiplication, 83, 160
coeﬃcient matrix,
7, 42
coeﬃcient of linear correlation,
297
cofactor,
477, 487
expansion,
478, 481
Collatz, Lothar,
666
Collatz–Wielandt formula,
666, 673, 686
column,
7
equivalence,
134
and nullspace,
177
operations,
14, 134
rank,
198
relationships,
50, 136
scaling,
27
space,
170, 171, 178
spanning set for,
172
vector,
8, 81
Comdico, David,
xii
commutative law,
97
commutative property of addition,
82
commuting matrices, eigenvectors,
503, 522
companion matrix,
648
compatibility of norms,
285
compatible norms,
279, 280
competing species model,
546
complementary projector,
386
complementary subspaces,
383, 403
angle between,
389, 450
complete pivoting,
28
numerical stability,
349
complete set of eigenvectors,
507
complex conjugate,
83
complex exponential,
362, 544
complex numbers, the set of,
81
component matrices,
604
component vectors,
384
composition
of linear functions,
93
of linear transformations,
245, 246
of matrix functions,
608, 615
computer graphics,
328, 330
condition number
for eigenvalues,
528
generalized,
426
for matrices, 127, 128, 414, 415
condition of
eigenvalues, hermitian matrices,
552
linear system,
128
conditioning and pivots,
426
conformable,
96
conformably partitioned,
111
congruence transformation,
568
conjugate, complex,
83
conjugate gradient algorithm,
657
conjugate matrix,
84
conjugate transpose,
84
reverse order law,
109
conjugate vectors,
657
connected graph,
202
connectivity and linear dependence,
208
connectivity matrix,
100
consistent system,
53, 54
constituent matrices,
604
continuity
of eigenvalues,
497
of inversion,
480
of norms,
277
continuous Fourier transform,
357
continuous functions, max and min,
276
convergence,
276, 277
convergent matrix,
631
converse of a statement,
54
convolution
with circulants,
380
deﬁnition,
366
operation count,
377
theorem,
367, 368, 377
Cooley, J. W.,
368, 375, 651
cooperating species model,
546
coordinate matrix,
242
coordinates,
207, 240, 299
change of,
252
of a vector,
240
coordinate spaces,
161
core-nilpotent decomposition,
397
correlation,
296
correlation coeﬃcient,
297
cosine
of angle,
295
minimal angle,
450
discrete,
361

Index
707
Courant–Fischer theorem,
550
alternate,
557
for singular values,
555
Courant, Richard,
550
covariance,
447
Cramer, Gabriel,
476
Cramer’s rule,
459, 476
critical point,
570
cross product,
332, 339
Cuomo, Kelly,
xii
curve ﬁtting,
186, 229
D
defective,
507
deﬁcient,
496, 507
deﬁnite matrices,
559
deﬂation, eigenvalue problems,
516
dense matrix,
350
dependent set,
181
derivative
of a determinant,
471, 474, 486
of a linear system,
130
of a matrix,
103, 226
operator,
245
determinant,
461
computing,
470
of a product,
467
as product of eigenvalues,
494
of a sum,
485
and volume,
468
deviation from symmetry,
436
diagonal dominance,
639
diagonal matrix,
85
eigenvalues of,
501
inverse of,
122
diagonalizability,
507
being arbitrarily close to,
533
in terms of minimum polynomial,
645
in terms of multiplicities,
512
summary,
520
diagonalization
of circulants,
379
Jacobi’s method,
353
of normal matrices,
547
simultaneous,
522
diagonally dominant,
184, 499, 622, 623, 639
systems,
193
diﬀerence equations,
515, 616
diﬀerence of matrices,
82
diﬀerence of projectors,
393
diﬀerential equations,
489, 541, 542
independent solutions,
481
nonhomogeneous,
609
solution of,
546
stability,
544, 609
systems,
608
uncoupling,
559
diﬀusion equation,
563
diﬀusion model,
542
dimension,
196
of direct sum,
383
of fundamental subspaces,
199
of left-hand nullspace,
218
of nullspace,
218
of orthogonal complement,
339
of range,
218
of row space,
218
of space of linear transformations,
241
of subspace,
198
of sum,
205
direct product,
380, 597
direct sum,
383
of linear operators,
399
of several subspaces,
392
of symmetric and skew-symmetric matrices,
391
directed distance between subspaces,
453
directed graph,
202
Dirichlet, Johann P. G. L,
563, 597
Dirichlet problem,
563
discrete Fourier transform,
356, 358
discrete Laplacian,
563
eigenvalues of,
598
discrete sine, cosine, and exponential,
361
disjoint subspaces,
383
distance,
271
to lower-rank matrices,
417
between subspaces,
450
to symmetric matrices,
436
between a vector and a subspace,
435
distinct eigenvalues,
514
distributions,
532
distributive property
of matrix multiplication,
105
of scalar multiplication,
83
domain,
89
doubly stochastic,
702
Drazin generalized inverse,
399, 401, 422, 640
Cauchy formula,
615
integral representation,
441
Drazin, M. P.,
399
Duﬃn, R. J.,
441
Duncan, W. J.,
124
E
Eckart, C.,
411
economic input–output model,
681
edge matrix,
331
edges,
202
eigenpair,
490
eigenspace,
490

708
Index
eigenvalues,
266, 410, 490
bounds for,
498
continuity of,
497
determinant and trace,
494
distinct,
514
generalized,
571
index of,
401, 587, 596
perturbations and condition of,
528, 551
semisimple,
596
sensitivity, hermitian matrices,
552
unit,
696
eigenvalues of
bordered matrices,
552
discrete Laplacian,
566, 598
triangular and diagonal matrices,
501
tridiagonal Toeplitz matrices,
514
eigenvectors,
266, 490
of commuting matrices,
503
generalized,
593, 594
independent,
511
of tridiagonal Toeplitz matrices,
514
electrical circuits,
73, 204
elementary matrices,
131–133
interchange matrices,
135, 140
elementary orthogonal projector,
322, 431
rank of,
337
elementary reﬂector,
324, 444
determinant of,
485
elementary row and column operations,
4, 8
and determinants,
463
elementary triangular matrix,
142
ellipsoid,
414
degenerate,
425
elliptical inner product,
286
elliptical norm,
288
EP matrices,
408
equal matrices,
81
equivalence, row and column,
134
testing for,
137
equivalent norms
matrices,
425
vectors,
276
equivalent statements,
54
equivalent systems,
3
ergodic class,
695
error, absolute and relative,
414
essentially positive matrix,
686
estimators,
446
euclidean norm,
270
unitary invariance of,
321
evolutionary processes,
616
exponential
complex,
544
discrete,
361
matrix,
441, 525
inverse of,
614
products of,
539
sums of,
614
extending to a basis,
201
extending to an orthonormal basis,
325, 335, 338, 404
extension set,
188
F
Faddeev and Sominskii,
504
fail-safe system,
701
fast Fourier transform (FFT),
368
FFT algorithm,
368, 370, 373, 381, 651
FFT operation count,
377
fast integer multiplication,
375
ﬁltering random noise,
418
ﬁnite diﬀerence matrix,
522, 639
ﬁnite-dimensional spaces,
195
ﬁnite group,
676
Fischer, Ernst,
550
ﬁve-point diﬀerence equations,
564
ﬁxed points,
386, 391
of a reﬂector,
338
ﬂatness,
164
ﬂoating-point number,
21
forward substitution,
145
four fundamental subspaces,
169
summary,
178
Fourier coeﬃcients,
299
Fourier expansion,
299
and projection,
440
Fourier, Jean Baptiste Joseph,
299
Fourier matrix,
357
Fourier series,
299, 300
Fourier transform
continuous,
357
discrete,
356, 358
Frame, J. S.,
504
Francis, J. F. G.,
535
Fr´echet, Maurice, R.,
289
free variables,
58, 61
in nonhomogeneous systems,
70
frequency,
362
frequency domain,
363
Frobenius, Ferdinand Georg,
44, 123, 215, 662
Frobenius form,
680
Frobenius inequality,
221
Frobenius matrix norm,
279, 425, 428
and inner product,
288
of rank-one matrices,
391
unitary invariance of,
337
Frobenius test for primitivity,
678
full-rank factorization,
140, 221, 633
for determining index,
640
of a projector,
393
function
aﬃne,
89
composition of,
93, 615, 608
domain of,
89
linear,
89, 238
norm of,
288
range of,
89

Index
709
functional matrix identities,
608
functions of
diagonalizable matrices,
526
of Jordan blocks,
600
matrices,
601
using Cauchy integral formula,
611
using Cayley–Hamilton theorem,
614
nondiagonalizable matrices,
603
fundamental (normal) mode of vibration,
562
fundamental problem of matrix theory,
506
fundamental subspaces,
169
dimension of,
199
orthonormal bases for,
407
projector onto,
434
fundamental theorem of algebra,
185, 492
fundamental theorem of linear algebra,
405
G
gap,
453, 454
Gauss, Carl F.,
ix, 2, 93, 234, 488
as a teacher,
353
Gaussian elimination,
2, 3
and LU factorization,
141
eﬀects of roundoﬀ,
129
modiﬁed,
43
numerical stability,
348
operation counts,
10
Gaussian transformation,
341
Gauss–Jordan method,
15, 47, 48
for computing a matrix inverse,
118
operation counts,
16
Gauss–Markov theorem,
229, 448
Gauss–Seidel method,
622
general solution
algebraic equations
homogeneous systems,
59, 61,
nonhomogeneous systems,
64, 66, 70, 180, 221
diﬀerence equations,
616
diﬀerential equations,
541, 609
generalized condition number,
426
generalized eigenvalue problem,
571
generalized eigenvectors,
593, 594
generalized inverse,
221, 393, 422, 615
Drazin,
399
group,
402
and orthogonal projectors,
434
generalized minimal residual (GMRES),
655
genes and chromosomes,
543
geometric multiplicity,
510
geometric series,
126, 527, 618
Gerschgorin circles,
498
Gerschgorin, S. A.,
497
Givens reduction,
344
and determinants,
485
numerical stability,
349
Givens rotations,
333
Givens, Wallace,
333
GMRES,
655
Golub, Gene H.,
xii
gradient,
570
Gram, Jorgen P.,
307
Gram matrix,
307
Gram–Schmidt algorithm
classical version,
309
implementations of,
319
and minimum polynomial,
643
modiﬁed version,
316
numerical stability of,
349
and volume,
431
Gram–Schmidt process,
345
Gram–Schmidt sequence,
308, 309
graph,
202
of a matrix,
209, 671
graphics, 3-D rotations,
328, 330
Grassmann, Hermann G.,
160
Graybill, Franklin A.,
xii
grid norm,
274
grid points,
18
group, ﬁnite,
676
group inverse,
402, 640, 641
growth in Gaussian elimination,
26
Guttman, L.,
124
H
Hadamard, Jacques,
469, 497
Hadamard’s inequality,
469
Halmos, Paul,
xii, 268
Hamilton, William R.,
509
harmonic functions,
563
Haynsworth, Emilie V.,
123
heat equation,
563
Helfrich, Laura,
xii
Hermite, Charles,
48
Hermite interpolation polynomial,
607
Hermite normal form,
48
Hermite polynomial,
231
hermitian matrix,
85, 409, 410
condition of eigenvalues,
552
eigen components of,
549
Hessenberg matrices
350
QR factorization of,
352
Hessian matrix,
570
Hestenes, Magnus R.,
656
hidden surfaces,
332, 339
Hilbert, David,
307
Hilbert matrix,
14, 31, 39
Hilbert–Schmidt norm,
279
Hohn, Franz,
xii
H¨older, Ludwig O.,
278
H¨older’s inequality,
274, 277, 278
homogeneous systems,
57, 61
Hooke, Robert,
86
Hooke’s law,
86
Horn, Roger,
xii

710
Index
Horst, Paul,
504
Householder, Alston S.,
324
Householder reduction,
341, 342
and determinants,
485
and fundamental subspaces,
407
numerical stability,
349
Householder transformations,
324
hyperplane,
442
I
idempotent,
113, 258, 339, 386
and projectors,
387
identity matrix,
106
identity operator,
238
ill-conditioned matrix,
127, 128, 415
ill-conditioned system,
33, 535
normal equations,
214
image and image space,
168, 170
dimension of,
208
image of unit sphere,
417
imaginary, pure,
556
imprimitive matrices,
674
maximal root of,
676
spectrum of,
677
test for,
678
imprimitivity, index of,
679, 680
incidence matrix,
202
inconsistent system,
53
independent columns,
218
independent eigenvectors,
511
independent rows,
218
independent set,
181
basic facts,
188
maximal,
186
independent solutions
for algebraic equations,
209
for diﬀerential equations,
481
index
of an eigenvalue,
401, 587, 596
of imprimitivity,
674, 679, 680
of nilpotency,
396
of a square matrix,
394, 395
by full-rank factorization,
640
induced matrix norm,
280, 389
of A−1,
285
elementary properties,
285
of rank-one matrices,
391
unitary invariance of,
337
inertia,
568
inﬁnite-dimensional spaces,
195
inﬁnite series and matrix functions,
527
inﬁnite series of matrices,
605
information retrieval,
419
inner product,
286
geometric interpretation,
431
input–output economic model,
681
integer matrices,
156, 473, 485
integer multiplication,
375
integral formula
for generalized inverses,
441
for matrix functions,
611
intercept model,
447
interchange matrices,
135, 140
interlacing of eigenvalues,
552
interpolation
formula for f(A),
529
Hermite polynomial,
607
Lagrange polynomial,
186
intersection of subspaces
basis for,
211
projection onto,
441
invariant subspace,
259, 262, 263
inverse Fourier transform,
358
inverse iteration,
534
inverse matrix,
115
best approximation to,
428
Cauchy formula for,
615
computation of,
118
operation counts,
119
continuity of,
480
determinants,
479
eigenvalues of,
501
existence of,
116
generalized,
615
integral representation of,
441
norm of,
285
properties of,
120
of a sum,
220
invertible operators,
246, 250
invertible part of an operator,
399
involutory,
113, 325, 339, 485
irreducible Markov chain, limits,
693
irreducible matrix,
209, 671
isometry,
321
iteration matrix,
620
iterative methods,
620
J
Jacobi’s diagonalization method,
353
Jacobi’s iterative method,
622, 626
Jacobi, Karl G. J.,
353
Johnson, Charlie,
xii
Jordan blocks,
588, 590
functions of,
600
nilpotent,
579
Jordan chains,
210, 401, 575, 576, 593
construction of,
594
Jordan form,
397, 408, 589, 590
for nilpotent matrices,
579
preliminary version,
397
Jordan, Marie Ennemond Camille,
15, 411, 589
Jordan segment,
588, 590
Jordan structure of matrices,
580, 581, 586, 589
uniqueness of,
580

Index
711
Jordan, Wilhelm,
15
K
Kaczmarz’s projection method,
442, 443
Kaczmarz, Stefan,
442
Kaplansky, Irving,
268
Kearn, Vickie,
xi, 12
kernel,
173
Kirchhoﬀ’s rules,
73
loop rule,
204
Kline, Morris,
80
Kowa, Seki,
459
Kronecker, Leopold,
597
Kronecker product,
380, 597
and the Laplacian,
573
Krylov, Aleksei Nikolaevich,
645
Krylov
method,
649
sequence,
401
subspaces, sequences, matrices,
646
Kummer, Ernst Eduard,
597
L
Lagrange interpolating polynomial,
186, 230, 233, 529
Lagrange, Joseph-Louis,
186, 572
Lagrange multipliers,
282
Lancaster, Peter,
xii
Lanczos algorithm,
651
Lanczos, Cornelius,
651
Laplace’s determinant expansion,
487
Laplace’s equation,
624
Laplace, Pierre-Simon,
81, 307, 487, 572
Laplacian,
563
latent semantic indexing,
419
latent values and vectors,
490
law of cosines,
295
LDU factorization,
154
leading principal minor,
558
leading principal submatrices,
148, 156
least common multiple,
647
least squares,
226, 439
and Gram–Schmidt,
313
and orthogonal projection,
437
and polynomial ﬁtting,
230
and pseudoinverse,
438
and QR factorization,
346
total least squares,
223
why least squares?,
446
LeBlanc, Kathleen,
xii
left-hand eigenvectors,
490, 503, 516, 523, 524
in inverses,
521
and projectors,
518
left-hand nullspace,
174, 178, 199
spanning set for,
176
Legendre, Adrien–Marie,
319, 572
Legendre polynomials,
319
Legendre’s diﬀerential equation,
319
Leibniz, Gottfried W.,
459
length of a projection,
323
Leontief’s input–output model,
681
Leontief, Wassily,
681
Leslie, P. H.,
684
Leslie population model,
683
Leverrier–Souriau–Frame Algorithm,
504
Leverrier, U. J. J.,
504
L´evy, L.,
497
limiting distribution,
531, 636
limits
and group inversion,
640
in Markov chains
irreducible Markov chains,
693
reducible Markov chains,
698
of powers of matrices,
630
and spectral radius,
617
of vector sequences,
639
in vector spaces,
276, 277
Lindemann, Carl Louis Ferdinand von,
662
linear
algebra,
238
combination,
91
correlation,
296, 306
dependence and connectivity,
208
estimation,
446
functions,
89, 238
deﬁned by matrix multiplication,
106
deﬁned by systems of equations,
99
models,
448
operators,
238
and block matrices,
392
regression,
227, 446
spaces,
169
stationary iterations,
620
transformation,
238
linearly independent and dependent sets,
181
basic facts,
188
maximal,
186
and rank,
183
linearly independent eigenvectors,
511
lines in ℜn not through the origin,
440
lines, projection onto,
440
long-run distribution,
531
loop,
73
equations,
204
rule,
74
simple,
75
lower triangular,
103
LU factorization,
141, 144
existence of,
149
with interchanges,
148
operation counts,
146
summary,
153

712
Index
M
main diagonal,
41, 85
Markov, Andrei Andreyevich,
687
Markov chains,
532, 638, 687
absorbing,
700
periodic,
694
mass-stiﬀness equation,
571
matrices, the set of,
81
matrix,
7
diagonal
85
exponential,
441, 525, 529
and diﬀerential equations,
541, 546, 608
inverse of,
614
products,
539
sums,
614
functions,
526, 601
as inﬁnite series,
527
as polynomials,
606
group,
402
multiplication,
96
by blocks,
111
as a linear function,
106
properties of,
105
relation to linear transformations,
244
norms,
280
1-norm,
283
2-norm,
281, 425
∞-norm,
127, 283
Frobenius norm,
425
induced norm,
285
polynomials,
501
product,
96
representation of a projector,
387
representations,
262
triangular
41
maximal angle,
455
maximal independent set,
218
maximal linearly independent subset,
186, 196
maximum and minimum of continuous functions,
276
McCarthy, Joseph R.,
651
mean,
296, 447
Meyer
Bethany B.,
xii
Carl, Sr.,
xii
Holly F.,
xii
Louise,
xii
Margaret E.,
xii
Martin D.,
xii
min-max theorem,
550
alternate formulation,
557
for singular values,
555
minimal angle,
450
minimal spanning set,
196, 197
minimum norm least squares solution,
438
minimum norm solution,
426
minimum polynomial,
642
determination of,
643
of a vector,
646
minimum variance estimator,
446
Minkowski, Hermann,
184, 278, 497, 626
Minkowski inequality,
278
minor determinant, principal,
559, 466
MINRES algorithm,
656
Mirsky, Leonid,
xii
M-matrix,
626, 639, 682, 703
modern least squares,
437
modiﬁed gaussian elimination,
43
modiﬁed Gram–Schmidt algorithm,
316
monic polynomial,
642
Montgomery, Michelle,
xii
Moore, E. H.,
221
Moore–Penrose generalized inverse,
221, 422, 400
best approximate inverse,
428
integral representation,
441
and orthogonal projectors,
434
Morrison, W. J.,
124
multiplication
of integers,
375
of matrices,
96
of polynomials,
367
multiplicities,
510
and diagonalizability,
512
multiplier,
22, 25
in partial pivoting,
26
N
negative deﬁnite,
570
Neumann series,
126, 527, 618
Newton,
86
Newton’s identities,
504
Newton’s second law,
560
nilpotent,
258, 396, 502, 510
Jordan blocks,
579
part of an operator,
399
Noble, Ben,
xii
node,
18, 73, 202, 204
rule,
74, 204
no-intercept model,
447
noise removal with SVD,
418
nonbasic columns,
50, 61
nonderogatory matrices,
644, 648
nondiagonalizable, spectral resolution,
603
nonhomogeneous diﬀerential equations,
609
nonhomogeneous systems,
57, 64
general solution,
64, 66, 70
summary,
70
nonnegative matrices,
661, 670
nonsingular matrices,
115
and determinants,
465
and elementary matrices,
133
products of,
121
sequences of,
220

Index
713
norm,
269
compatibility,
279, 280, 285
elliptical,
288
equivalent,
276, 425
of a function,
288
on a grid,
274
of an inverse,
285
for matrices,
280
1-, 2-, and ∞-norms,
281, 283
Frobenius,
279, 337
induced,
280, 285, 337
of a projection,
323
for vectors,
275
1-, 2-, and ∞-norms,
274
p-norms,
274
of a waveform,
382
normal equations,
213, 214, 221, 226, 313, 437
normal modes of vibration,
562, 571
normalized vector,
270
normal matrix,
304, 400, 409, 547
nullity,
200, 220
nullspace,
173, 174, 178, 199
equality,
177
of an orthogonal projector,
434
of a partitioned matrix,
208
of a product,
180, 220
spanning set for,
175
and transpose,
177
number of pivots,
218
numerical stability,
347
O
oblique projection,
385
method for linear systems,
443
oblique projectors from SVD,
634
Ohm’s law,
73
Oh notation O(hp) ,
18
one-to-one mapping,
250
onto mapping,
250
operation counts
for convolution,
377
for Gaussian elimination,
10
for Gauss–Jordan method,
16
for LU factorization,
146
for matrix inversion,
119
operator, linear,
238
operator norm,
280
Ortega, James,
xii
orthogonal complement,
322, 403
dimension of,
339
involving range and nullspace,
405
orthogonal decomposition theorem,
405, 407
orthogonal diagonalization,
549
orthogonal distance,
435
orthogonal matrix,
320
determinant of,
473
orthogonal projection,
239, 243, 248, 299, 305, 385, 429
and 3-D graphics,
330
onto an aﬃne space,
436
and least squares,
437
orthogonal projectors,
322, 410, 427, 429
elementary,
431
formulas for,
430
onto an intersection,
441
and pseudoinverses,
434
sums of,
441
orthogonal reduction,
341
to determine full-rank factorization,
633
to determine fundamental subspaces,
407
orthogonal triangularization,
342
orthogonal vectors,
294
orthonormal basis,
298
extending to,
325, 335, 38
for fundamental subspaces,
407
by means of orthogonal reduction,
355
orthonormal set,
298
Ostrowski, Alexander,
626
outer product,
103
overrelaxation,
624
P
Painter, Richard J.,
xii
parallelepiped,
431, 468
parallelogram identity,
290, 291
parallelogram law,
162
parallel sum,
441
parity of a permutation,
460
Parseval des Chˆenes, M.,
305
Parseval’s identity,
305
partial pivoting,
24
and diagonal dominance,
193
and LU factorization,
148
and numerical stability,
349
particular solution,
58, 65–68, 70, 180, 213
partitioned matrix,
111
and linear operators,
392
rank and nullity of,
208
Peano, Giuseppe,
160
Penrose equations,
422
Penrose, Roger,
221
perfect shuﬄe,
372, 381
period of trigonometric functions,
362
periodic extension,
302
periodic function,
301
periodic Markov chain,
694
permutation,
460
symmetric,
671
permutation counter,
151
permutation matrix,
135, 140, 151
perpendicular,
294
perp, properties of,
404, 409
Perron–Frobenius theory,
661, 673
Perron, Oskar,
661
Perron root,
666, 668

714
Index
Perron vector,
665, 668, 673
perturbations
aﬀecting rank,
216
eigenvalues,
528
hermitian eigenvalues,
551
in inverses,
128
in linear systems,
33, 128, 217
rank-one update,
208
singular values,
421
Piazzi, Giuseppe,
233
pivot
conditioning,
426
determinant formula for,
474, 558
elements and equations,
5
positions,
5, 58, 61
in partial pivoting,
24
uniqueness,
44
pivoting
complete,
28
partial,
24
plane rotation,
333
determinant of,
485
p-norm,
274
Poisson’s equation,
563, 572
Poisson, Sim´eon D.,
78, 572
polar factorization,
572
polarization identity,
293
polynomial
equations,
493
in a matrix,
501
and matrix functions,
606
minimum,
642
multiplication and convolution,
367
polytope,
330, 339
ponderal index,
236
poor man’s root ﬁnder,
649
population distribution,
532
population migration,
531
population model, Leslie,
683
positive deﬁnite form,
567
positive deﬁnite matrix,
154, 474, 558, 559
positive matrix,
661, 663
positive semideﬁnite matrix,
558, 566
Poulson, Deborah ,
xii
power method,
532, 533
powers of a matrix,
107
limiting values,
530
powers of linear transformations,
248
precision,
21
preconditioned system,
658
predator–prey model,
544
primitive matrices,
674
test for,
678
principal angles,
456
principal minors,
494, 558
in an M-matrix,
626, 639
nonnegative,
566
positive,
559
principal submatrix,
494, 558
and interlaced eigenvalues,
553
of an M-matrix,
626
of a stochastic,
703
products
of matrices,
96
of nonsingular matrices,
121
of orthogonal projectors,
441
of projectors,
393
product rule for determinants,
467
projection,
92, 94, 322, 385, 429
and Fourier expansion,
440
method for solving linear systems,
442, 443
onto
aﬃne spaces,
436
fundamental subspaces,
434
hyperplanes,
442
lines,
440, 431
oblique subspaces,
385
orthogonal subspaces,
429
symmetric matrices,
436
projectors,
239, 243, 339, 385, 386
complementary,
386
from core-nilpotent decomposition,
398
diﬀerence of,
393
from full-rank factorization,
633, 634
as idempotents,
387
induced norm of,
389
matrix representation of,
387
oblique,
386
orthogonal,
429
product of,
393
spectral,
517, 603
sum of,
393
proper values and vectors,
490
pseudoinverse,
221, 422, 615
as best approximate inverse,
428
Drazin,
399
group,
402
inner, outer, reﬂexive,
393
integral representation of,
441, 615
and least squares,
438
Moore–Penrose,
422
and orthogonal projectors,
434
pure imaginary,
556
Pythagorean theorem,
294, 305, 423
and closest point theorem,
435
for matrices with Frobenius norm,
428
Q
QR factorization,
345, 535
and Hessenberg matrices,
352
and least squares,
346
and minimum polynomial,
643
rectangular version of,
311
and volume,
431
quadratic form,
567

Index
715
quaternions,
509
R
random integer matrices,
156
random walk,
638
range
of a function,
89, 169
of a matrix,
170, 171, 178, 199
of an operator,
250
of an orthogonal projector,
434
of a partitioned matrix,
179
of a product,
180, 220
of a projector,
391
of a sum,
206
range-nullspace decomposition,
394, 407
range-symmetric matrices,
408
rank,
45, 139
of a block diagonal matrix,
137
and consistency,
54
and determinants,
466
of a diﬀerence,
208
of an elementary projector,
337
and free variables,
61
of an incidence matrix,
203
and independent sets,
183
and matrix inverses,
116
and nonhomogeneous systems,
70
and nonsingular submatrices,
218
numerical determination,
421
of a partitioned matrix,
208
of a perturbed matrix,
216
of a product,
210, 211, 219
of a projector,
392
and submatrices,
215
of a sum,
206, 221
summary,
218
and trivial nullspaces,
175
rank normal form,
136
rank-one matrices
characterization of,
140
diagonalizability of,
522
perturbations of,
208
rank-one updates
determinants of,
475
eigenvalues of,
503
rank plus nullity theorem,
199, 410
Rayleigh, Lord,
550
Rayleigh quotient,
550
iteration,
535
real numbers, the set of,
81
real Schur form,
524
real-symmetric matrix,
409, 410
rectangular matrix,
8
rectangular QR factorization,
311
rectangular systems,
41
reduced row echelon form,
48
reducible Markov chain,
698
reducible matrices,
209, 671
canonical form for,
695
in linear systems,
112
reﬂection,
92, 94
about a hyperplane,
445
method for solving linear systems,
445
reﬂector,
239, 324, 444
determinant of,
485
reﬂexive pseudoinverse,
393
regression,
227, 446
relative uncertainty or error,
414
relaxation parameter,
445, 624
residual,
36, 416
resolvent,
285, 611
restricted operators,
259, 393, 399
restricted transformations,
424
reversal matrix,
596
reverse order law
for inversion,
120, 121
for transpose and conjugate transpose,
109
reversing binary bits,
372
Richardson iterative method,
622
right angle,
294
right-hand rule,
340
right-hand side,
3
Ritz values,
651
roots of unity,
356
and imprimitive matrices,
676
Rose, Nick,
xii
rotation,
92, 94
determinant of,
485
plane (Givens rotations),
333
in ℜ2,
326
in ℜ3,
328
in ℜn,
334
rotator,
239, 326
rounding convention,
21
roundoﬀerror,
21, 129, 347
row,
7
echelon form,
44
reduced,
48
equivalence,
134, 218
and nullspace,
177
operations,
134
rank,
198
relationships,
136
scaling,
27
space,
170, 171, 178, 199
spanning set for,
172
vector,
8, 81
RPN matrices,
408
Rutishauser, Heinz,
535
S
Saad, Yousef,
655
saw-toothed function,
306
scalar,
7, 81

716
Index
scalar multiplication,
82, 83
scale,
27
scaling a linear system,
27, 28
scaling in 3-D graphics,
332
Schmidt, Erhard,
307
Schr¨odinger, Erwin,
651
Schultz, Martin H.,
655
Schur complements,
123, 475
Schur form for real matrices,
524
Schur, Issai,
123, 508, 662
Schur norm,
279
Schur triangularization theorem,
508
Schwarz, Hermann A.,
271, 307
search engine,
418, 419
sectionally continuous,
301
secular equation,
503
Seidel, Ludwig,
622
Sellers, Lois,
xii
semiaxes,
414
semideﬁnite,
566
semisimple eigenvalue,
510, 591, 593, 596
semistable,
544
sensitivity,
128
minimum norm solution,
426
sequence
limit of,
639
of matrices,
220
series for f(A) ,
605
shape,
8
shell game,
635
Sherman, J.,
124
Sherman–Morrison formula,
124, 130
SIAM,
324, 333
signal processing,
359
signal-to-noise ratio,
418
sign of a permutation,
461
similar matrices,
255, 473, 506
similarity,
505
and block-diagonal matrices,
263
and block-triangular matrices,
263
and eigenvalues,
508
invariant,
256
and orthogonal matrices,
549
transformation,
255, 408, 506
and transpose,
596
unitary,
547
simple eigenvalue,
510
simple loops,
75
simultaneous diagonalization, triangularization,
522
simultaneous displacements,
622
sine, discrete,
361
singular matrix,
115
eigenvalues of,
501
sequences of,
220
singular systems, practical solution of,
217
singular values,
553
Courant–Fischer theorem,
555
and determinants,
473
as eigenvalues,
555
and the SVD,
412
size,
8
skew-hermitian matrices,
85, 88
skew-symmetric matrices,
85, 88, 391, 473
eigenvalues of,
549, 556
as exponentials,
539
vector space of,
436
SOR method,
624
Souriau, J. M.,
504
spanning sets,
165
for column space,
172
for four fundamental subspaces,
178
for left-hand nullspace,
176
minimal,
197
for nullspace,
175
for row space,
172
test for,
172
sparse least squares,
237
sparse matrix,
350
spectral circle, imprimitive matrices,
676
spectral mapping property,
539, 613
spectral projectors,
517, 602, 603
commuting property,
522
interpolation formula for,
529
positivity of,
677
in terms of eigenvectors,
518
spectral radius,
497, 521, 540
Collatz–Wielandt formula,
666, 673, 686
as a limit,
619
and limits,
617
spectral representation of matrix functions,
526
spectral resolution of f(A) ,
603
spectral theorem for diagonalizable matrices,
517
spectrum,
490
of imprimitive matrix,
677
spheres,
275
splitting,
620
spring-mass vibrations,
570
springs,
86
square
matrix,
8
system,
5
wave function,
301
stable,
544
algorithm,
217, 317, 347, 422
matrix,
544
system,
544, 609
standard
basis,
194, 240, 299
coordinates,
240
deviation,
296
inner product,
95, 271
scores,
296
standardization of data,
296
stationary distribution,
531, 693

Index
717
steady-state distribution,
531, 636
steepest descent,
657
step size,
19
Stewart, G. W.,
xii
Stiefel, Eduard,
656
stiﬀness
constant,
86
matrix,
87
stochastic matrix,
685, 687
doubly,
702
summability of,
697
unit eigenvalues of,
696
Strang, Gilbert,
xii
strongly connected graph,
209, 671
Strutt, John W.,
550
stuﬀin a vector space,
197, 200
subgroup,
402
submatrix,
7
as a block matrix,
111
and rank,
215
subscripts,
7
subset,
162
subspace,
162
angles or gaps between,
450
dimension of,
198
directed distance between,
453
four fundamental,
169
invariant,
259, 262, 263
maximal angle between,
455
sum of,
205
substochastic matrix,
685
successive displacements,
623
successive overrelaxation method,
624
sum
of matrices,
81
of orthogonal projectors,
441
of projectors,
393
of vector spaces,
166, 383
dimension of,
205
summable matrix and summability,
631, 633, 677
stochastic matrices,
697
superdiagonal,
575
SVD,
412
and full-rank factorization,
634
and oblique projectors,
634
switching circuits,
539
Sylvester, James J.,
44, 80, 411
Sylvester’s law of inertia,
568
Sylvester’s law of nullity,
220
symmetric
functions,
494
matrices,
85
diagonalization and eigen components of,
549
reduction to tridiagonal form,
352
space of,
436
permutation,
671
T
Taussky-Todd, Olga,
497
Taylor series,
18, 570, 600
t-digit arithmetic,
21
tensor product,
380, 597
and the Laplacian,
573
term-by-document matrix,
419
text mining,
419
three-dimensional rotations,
328, 330
time domain,
363
Todd, John,
497
Toeplitz matrices,
514
Toeplitz, Otto,
514
total least squares,
223
trace,
90
and characteristic equation,
504
of imprimitive matrices,
678
inequalities,
293
of a linear operator,
256
of a product,
110, 114
of a projector,
392
as sum of eigenvalues,
494
transformation, linear,
238
transient behavior,
532
transient class,
695
transition diagram,
108, 531
transition matrix,
108, 531, 688
transitive operations,
257
translation, in 3-D graphics,
332
transpose,
83
and determinants,
463
nullspace,
177
properties of,
84
reverse order law for,
109
and similarity,
596
trapezoidal form,
342
trend of observations,
231
triangle inequality,
220, 273, 277
backward version,
273
triangular matrices,
41, 103
block versions,
112
determinant of,
462
eigenvalues of,
501
elementary,
142
inverses of,
122
triangularization, simultaneous,
522
triangularization using elementary reﬂectors,
342
triangular system,
6
tridiagonal matrix,
20, 156, 352
Toeplitz matrices,
514
trivial
nullspaces,
175
solution,
57, 60, 69
and nonhomogeneous systems,
70
and nonsingular matrices,
116
subspace,
162, 197
Tukey, J. W.,
368, 375, 651

718
Index
two-point boundary value problem,
18
U
unbiased estimator for variance,
449, 446
uncertainties in linear systems,
414
underrelaxation,
624
unique solution
for diﬀerential equations,
541
and free variables,
61
for homogeneous systems,
61
for nonhomogeneous systems,
70
unitarily invariant norm,
425, 337
unitary diagonalization,
547
unitary matrices,
304, 320
determinant of,
473
unit columns,
102, 107
unit eigenvalues of stochastic matrices,
696
units,
27
unit sphere,
275
image of,
414, 425
unstable,
544
upper-trapezoidal form,
342, 344
upper triangular,
103
URV factorization,
406, 407
and full-rank factorization,
634
V
Vandermonde, Alexandre-Theophile,
185
Vandermonde determinant,
486
Vandermonde matrices,
185, 230, 357
Van Loan, Charlie,
xii
variance,
447
vector,
159
norms,
274
spaces,
160
vertex matrix,
330
vibrations, small,
559
volume
by determinants,
468
by Gram–Schmidt, and QR,
431
von Mises, R.,
533
von Neumann, John,
289
W
Weierstrass, Karl Theodor Wilhelm,
589, 662
well conditioned,
33, 127, 415
Weyl, Hermann,
160
why least squares?,
446
Wielandt, Helmut,
534, 666, 675, 679
Wielandt’s matrix,
685
Wielandt’s theorem,
675
Will, Marianne,
xii
wire frame ﬁgure,
330
Woodbury, M.,
124
Wronskian,
474, 481, 486
Wronski, Jozef M.,
189
Wronski matrix,
189, 190
X, Y, Z
Young, David M.,
625
Young, G.,
411
Zeeman, E. Christopher,
704
zero nullspace,
175
zero transformation,
238
Z-matrix,
628, 639, 296
z-scores,
296

